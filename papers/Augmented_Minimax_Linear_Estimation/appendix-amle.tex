\section{Proof of Finite Sample Results}
\label{sec:finite-sample-proofs}
In this section, we prove the finite sample bounds on which Theorem~\ref{theo:simple-rate} is based.
Here and throughout the appendix we will write $\Pn f$ and $\P f$ for averages of the function $f$ over the empirical and population distributions of $Z$ respectively in accordance with convention in the empirical process literature \citep[see e.g.][]{vandervaart-wellner1996:weak-convergence},
As a slight abuse of notation, we also write $\Pn$ to indicate a sample average in other contexts. We will write $\gapprox$
with the same meaning as $\tilde\riesz$ in Theorem~\ref{theo:simple-rate}, as it will be helpful to distinguish between vectors of weights $\gamma$
and functions $g$ which, when evaluated, give those weights.

\subsection{Setting}
\label{sec:appendix-setting}
We observe iid $(Y_1,Z_1) \ldots (Y_n,Z_n)$ with $Y_i \in \R$ and $Z_i$ in an arbitrary set $\zz$
and define $m(z) = \E[Y_i \mid Z_i=z]$ and $v(z) = \Var{Y_i \mid Z_i=z}$. We assume that $m$ is in a closed subspace $\mm$ 
of the $\P$-square integrable functions. Our estimand is defined as $\psi(m) = \P h(Z,m)$ in terms of a family $\set{h(z,\cdot) : z \in \zz}$ of linear functionals on $\mm$, and we assume that $\psi(\cdot) = \P h(Z,\cdot)$ is continuous on $\mm$.


\subsection{Consistency of the Minimax Linear Weights}
\label{sec:consistency}

In this section, we will prove the following consistency result.
It is stated as a deterministic consequence of two empirical process bounds
that will be shown to hold with high probability in Section~\ref{sec:putting-it-all-together}.

\begin{lemm}
\label{lemma:abstract-finite-sample}
Let $\F \subset \mm$ be absolutely convex with the property that the linear functionals $f \to f(z)$ and $f \to h(z,f)$ for $z \in \set{Z_1 \ldots Z_n}$ are continuous with respect to its gauge $\norm{\cdot}_{\F}$.\footnotemark\  Let $\riesz[\psi]$ be the Riesz representer for $\psi$ on the span of $\F$,
and consider, for $Q \in \set{\P,\Pn}$,
\begin{align*}
\hgamma &= \argmin_{\gamma \in \R^n}\sup_{f \in \F}  \sqb{\Pn h(Z_i, f) - \gamma_i f(Z_i)}^2 + (\sigma^2/n^2)\norm{\gamma}^2, \\
\tilde g  &= \argmin_{g} \ \norm{g - \riesz[\psi]}_{L_2(Q)}^2 + (\sigma^2/n)\norm{g}_{\F}^2. 
\end{align*}
These minimizers exist, are unique, and satisfy
\[ \Pn (\hgamma_i - \tilde g)^2 \le 2\alpha \eta_M r^2 \text{ for } \alpha = \max\set{3(\norm{\gapprox}_{\ff} + \eta_M r^2 n/\sigma^2),\ 2\eta_M/\eta_Q} \]
if $\F$ is $\norm{\cdot}_{L_2(Q)}$-closed and bounded and for all $f \in \F$, 
\begin{equation}
\label{eq:ratio-process-bounds}
\begin{aligned}
& \Pn f^2 \ge \eta_Q \P f^2 
&&\text{ if }\  \P f^2 \ge r^2 &&, \\
&\abs{(\Pn-\P)[ h(\cdot,f) - \gapprox f]} \le \eta_M r^2\
&&\text{ if  }\ \P f^2 \le r^2 && \text{ for }\ Q=\P \\
 &\abs{(\Pn-\P)[ h(\cdot,f) - \riesz[\psi] f]} \le \eta_M r^2\
&&\text{ if  }\ \P f^2 \le r^2 && \text{ for }\ Q=\Pn \\
\end{aligned}
\end{equation}
\end{lemm}
\footnotetext{Gauge-continuity is a convenient rephrasing of the pointwise boundedness assumption
of Theorems~\ref{theo:simple}-\ref{theo:simple-rate}.}
We begin by showing existence and uniqueness. It suffices to show that the functions minimized are lower-semicontinuous,
as they are proper and strongly convex and minimized over reflexive spaces $(\R_n, \norm{\cdot}_2)$ 
and $(\vspan \F, \norm{\cdot}_{L_2(Q)})$ respectively \citep[Corollary 2.20]{peypouquet2015convex}. The first is continuous, as 
a convex function is continuous if it is bounded on an open set \citep[Theorem 5.43]{aliprantis2006infinite},
and it is bounded on any bounded subset of $\R^n$. And the second is lower-semicontinuous,
as it is the sum of the continuous function mapping \smash{$g \to \norm{g - \riesz[\psi]}_{L_2(Q)}^2$} and 
the square of the gauge of the absorbing closed convex set $(\sqrt{n}/\sigma)\F$, which is lower-semicontinuous \citep[Theorem 5.52]{aliprantis2006infinite}.

To show that our weights converge to $\gapprox$, we will characterize them as the solution to a least squares problem for estimating $\gapprox$.
This least squares problem is the dual of the problem \eqref{eq:aml} solved by our weights $\hgamma$.
We use the following lemma to establish duality.
\begin{lemm}
\label{lemma:duality}
Let $\S$ be a normed vector space with norm $\norm{\cdot}$   
and $L_0:\S \to \R$ and $\bar L:\S \to \R^n$ be continuous linear maps.
Define a primal $p:\R^n \to R$ and dual $d:\S \to \R$ by
\begin{align*}
p(\gamma) &=  \frac{1}{2}\sup_{\norm{f} \le 1}\sqb{ L_0(f) - \gamma^T \bar L(f)}^2 +\frac{1}{2} \norm{\gamma}_2^2, \\
d(g)      &= \frac{1}{2} \norm{\bar L(g)}_2^2  - L_0(g)  + \frac{1}{2}\norm{g}^2
\end{align*}
Then:
\begin{enumerate}
\item $\min_{\gamma \in \R^n} p(\gamma) = -\inf_{g} d(g)$. 
\item $p$ has a unique minimum at a vector $\hgamma \in \R^n$.
\item For every sequence $\hat g^j$ along which $d(\hat g^j) \to \inf_{g} d(g)$, \\
      $\bar L(\hat g^j) \to \hgamma$.
\end{enumerate}
\end{lemm}
\noindent In our estimator \eqref{eq:aml}, we use the weights $\hriesz$ that minimize 
\smash{$(2\sigma^2/n^2) p(\gamma)$} where \smash{$L_0(f) = \sum_{i=1}^n h(Z_i, f)$}, \smash{$\gamma^T \bar L(f) = \sum_{i=1}^n \gamma_i f(Z_i)$},
and $\norm{\cdot}$ is $\sigma$ times the gauge of $\F$, and we can characterize our weights as the limit of a minimizing sequence for the corresponding dual $d(g)$.
\begin{equation}
\label{eq:duality-explicit}
\begin{aligned}  
&\hgamma_i = \lim_{j \to \infty} \hg_j(Z_i) \quad \text{ if } \quad d(\hat g_j) \to \inf_{g}d(g) \quad \text{ for } \\
&(2/n) d(g) = \Pn g^2  - 2\Pn h(\cdot,g)  + (\sigma^2/n)\norm{g}_{\F}^2.
\end{aligned}
\end{equation}
We will show that $g_j \approx \tilde g$ whenever $d(g_j) \le d(\tilde g)$.
This characterizes $\hgamma$, as each of its coordinates $\hgamma_i$ 
is the limit of $g_j(Z_i)$ for a sequence of functions with this property.


To do this, we will show that the excess loss $d(g) - d(\tilde g)$ is large
unless $g \approx \tilde g$. We begin by lower bounding the excess loss.
Via the elementary identity $g^2 - \tilde g^2 = (g-\tilde g)^2 + 2\gapprox (g-\gapprox)$,
\begin{equation}
\label{eq:concrete-dual}
\begin{aligned}
(2/n)[d(g) - d(\tilde g)] 
&= \Pn (g-\tilde g)^2 + 2\Pn \tilde g (g - \tilde g) - 2 \Pn h(\cdot, g - \tilde g)\\
& + (\sigma^2/n)[\norm{g}_{\F}^2 - \norm{\tilde g}_{\F}^2].
\end{aligned} 
\end{equation}
To lower bound this, we use a convenient property of our approximation $\tilde g$
\begin{equation}
\label{eq:projection-theorem}
0 \le  Q (\tilde g - \riesz[\psi]) (g - \tilde g) + (\sigma^2/n)\norm{\tilde g}_{\F}(\norm{g}_{\F} - \norm{\tilde g}_{\F})
 \ \text{ for all }\  g \in L_2(Q). 
\end{equation}
This is implied by the following generalization of the Hilbert space projection theorem.
The relevant Hilbert space is $\vspan\F \subseteq L_2(Q)$ 
and we take $\phi(x)=\sigma^2 x^2 /(2n)$ and $\rho(g) = \norm{g}_{\F}$.
\begin{lemm}
\label{lemma:projection-theorem}
Let $\phi$ be a nondecreasing convex differentiable function on the nonnegative reals;
$\rho$ be a proper, nonnegative, convex, and lower-semicontinuous function on a Hilbert space;
and $g_{\star}$ be a vector in that space.
Letting $\tilde g = \argmin_{g}  (1/2)\norm{g - g_{\star}}^2 + \phi(\rho(g))$,
$\inner{  \tilde g - g_{\star}, g - \tilde g} + \phi'(\rho(\tilde g))(\rho(g) - \rho(\tilde g))$
is nonnegative for all $g$.
\end{lemm}
Subtracting from the excess loss twice the non-negative right side of \eqref{eq:projection-theorem} yields a simple lower bound.
It is the sum of the empirical mean squared error, a mean-zero empirical process, and a regularization term:
\begin{equation*}
\begin{aligned}
&\Pn (g-\tilde g)^2 + 2(\Pn-Q) \tilde g (g - \tilde g) - 2[\Pn h(\cdot, g - \tilde g) -  Q \riesz[\psi](g-\tilde g)] && \\
& + (\sigma^2/n)[\norm{g}_{\F}^2 - \norm{\tilde g}_{\F}^2 - 2\norm{\tilde g}_{\F}(\norm{g}_{\F}-\norm{\tilde g}_{\F}) ] && \\
&= \Pn (g-\tilde g)^2 + 2(\Pn-\P) [\tilde g (g - \tilde g) \  - \ h(\cdot, g - \tilde g)] + (\sigma^2/n)(\norm{g}_{\F} - \norm{\tilde g}_{\F})^2 && \text{ for }\ Q=\P, \\
&= \Pn (g-\tilde g)^2 + 2(\Pn-\P) [\riesz[\psi](g-\tilde g) - h(\cdot, g - \tilde g)] + (\sigma^2/n)(\norm{g}_{\F} - \norm{\tilde g}_{\F})^2 && \text{ for }\ Q=\Pn. \\
\end{aligned} 
\end{equation*}
Here we've used the Riesz representation property $\P h(\cdot, f) = \P \riesz[\psi] f$ to simplify the first expression
in the two cases $Q=\P$ and $Q = \Pn$. We will use another lower bound that is a function of $\delta = g-\tilde g$,
\begin{equation}
\label{eq:excess-riesz}
\begin{aligned}
&\excess(\delta) 
= \Pn \delta^2 - 2 \abs{M(\delta)} + (\sigma^2/n) (\norm{\delta}_{\ff} - 2\norm{\gapprox}_{\ff})_+^2 \quad \text{ where }\\
&M(\delta) = \begin{cases} 
(\Pn-\P) [h(\cdot,\delta) - \tilde g \delta] & \text{ for } Q = \P, \\
(\Pn-\P) [h(\cdot,\delta) - \riesz[\psi] \delta] & \text{ for } Q = \Pn.
\end{cases} \quad \text{ and }\quad x_+^2 := x^2 1(x \ge 0).
\end{aligned}
\end{equation} 
This bound is derived from the previous one by (i) replacing the second term with its negated absolute value
and (ii) substituting a lower bound on the third term implied by the triangle inequality 
$\norm{\delta}_{\ff} -  \norm{\tilde g}_{\ff} \le \norm{g}_{\ff}$, the increasingness of $x_+^2$,
and the bound $x_+^2 \le x^2$.

By the lemma below, this excess loss lower bound $\excess(\delta)$ can be zero or negative only if 
$\Pn \delta^2 \le 2\alpha \eta_M r^2$. And because $\hgamma_i - \gapprox(Z_i)$ is the limit of a sequence $\delta_j(Z_i)$ 
with $\excess(\delta_j) \le 0$, it follows that $\Pn (\hgamma_i - \gapprox(Z_i))^2 \le 2\alpha\eta_M r^2.$

\begin{lemm}
\label{lemma:consistency-deterministic}
Let $\F$ be a class of functions that is star-shaped around zero,
define $\excess$ as in \eqref{eq:excess-riesz},
and suppose that for all $\delta \in \ff$,
\begin{equation}
\label{eq:ratio-bounds}
\begin{aligned}
& \Pn  \delta^2 \ge \eta_Q \P \delta^2
&& \text{ if }\ \P \delta^2 \ge r^2 \\
&\abs{M(\delta)} \le \eta_M r^2
&& \text{ if }\ \P \delta^2 \le r^2. 
\end{aligned}
\end{equation}
Let $\alpha = \max\set{3(\norm{\gapprox}_{\ff} + \eta_M r^2 n/\sigma^2),\ 2\eta_M/\eta_Q}$. 
Then $\excess(\delta) \le 0$ only if $\norm{\delta}_{\ff} \le \alpha$, 
$\P \delta^2 \le (\alpha r)^2$, and $\Pn \delta^2 \le 2\alpha \eta_M r^2$.
Furthermore, $\excess(\delta) \le \xi$ only if $\norm{\delta}_{\ff} \le \alpha + (\xi n)^{1/2}/\sigma$.
\end{lemm}
\noindent We conclude our proof of Lemma~\ref{lemma:abstract-finite-sample}
by proving Lemmas~\ref{lemma:duality}-\ref{lemma:consistency-deterministic}.
 


\begin{proof}[Proof of Lemma~\ref{lemma:duality}]
Because $p$ is a proper, strictly convex, coercive, and lower-semicontinuous function on the reflexive space $\R^n$,
it has a unique minimum $\hat p$ at some vector $\hgamma \in \R^n$ \citep[Corollary 2.20]{peypouquet2015convex}. 
Letting $A:\R^n \to \S^{\star}$ be the linear map $A \gamma := -\gamma^T\bar L$,
our primal $p:\R^n \to \R$ has the form of a primal in Fenchel-Rockafellar duality,
\begin{align*}
p(\gamma) &= s(\gamma) + r(A \gamma) \quad \text{ where } \\
s(\gamma) &=  (1/2)\norm{\gamma}^2 \\
r(L) &= (1/2)\norm{L_0 + L}_{\S^{\star}}^2,
\end{align*}
so its dual,
\[  d:\S^{\star\star} \to \R\ \text{ by }\ d(L^{\star}) := s^{\star}(-A^{\star}L^{\star}) + r^{\star}(L^{\star}), \]
has a minimum, and each argmin $\hat L^{\star}$ satisfies $-A^{\star}\hat L^{\star} \in \partial s(\hat \gamma) = \set{ \hat \gamma }$ \citep[Theorem 3.51]{peypouquet2015convex}. Here $s^{\star}$ and $r^{\star}$ are the convex conjugates of $s$ and $r$; $\partial s(\hat \gamma)$ is the subgradient of $s$ at $\hat \gamma$; and 
$A^{\star}$ is the adjoint of $A$, i.e., $A^{\star} L^{\star}$ is the vector in $\R^n$  
satisfying  $\inner{A^{\star} L^{\star}, e_i} = \inner{L^{\star}, A e_i}$ for the standard basis vectors $e_1 \ldots e_n$.


We will now characterize $\hat L^{\star}$ more explicitly, as a minimizer of 
\[ d(L^{\star}) = \frac{1}{2}\sum_{i=1}^n \inner{L^{\star}, A e_i}^2  - \inner{L^{\star}, L_0}  + \frac{1}{2} \norm{L^{\star}}_{\S^{\star\star}}^2. \]
To do this, we first calculate $r^{\star}$ and $s^{\star}$.
\begin{align*}
r^{\star}(L^{\star}) 
&= \sup_{L \in \S^{\star}} \inner{L^{\star}, L} - (1/2)\norm{L_0 + L}_{\S^{\star}}^2 \\
&= \sup_{L' \in \S^{\star}} \inner{L^{\star}, L' - L_0} - (1/2)\norm{L'}_{\S^{\star}}^2 \\ 
&= -\inner{L^{\star}, L_0} + \sup_{t \in \R } \sup_{\norm{L''}_{\S^{\star}}=1} \inner{L^{\star},  t L''} - (1/2)\norm{tL''}_{\S^{\star}}^2 \\
&= -\inner{L^{\star}, L_0} + \sup_{t \in \R} t \norm{L^{\star}}_{\S^{\star\star}} - t^2/2 \\
&= -\inner{L^{\star}, L_0} + (1/2)\norm{L^{\star}}_{\S^{\star\star}}^2.
\end{align*}
In the first step, we reparameterize in terms of $L' = L_0 + L$; in the second, we reparameterize again in terms of $tL''=L'$; 
in the third we substitute $\norm{\cdot}_{\S^{\star\star}}$ for its definition; and in the fourth we use the identity $\max_{t \in \R} at - t^2/2 = a^2/2$.
Similarly, for $y \in \R^n$, 
\begin{align*}
s^{\star}(y) 
&= \sup_{x \in \R^n} \inner{y,x} - \norm{x}^2/2 \\
&= \sup_{t \in \R}\sup_{x' \in \R^n : \norm{x'}=1} \inner{y,tx'} - t^2/2 \\
&= \sup_{t \in \R}t\norm{y} - t^2/2 \\
&= \norm{y}^2/2.
\end{align*}
Taking $y=-A^{\star}L^{\star}$ and establishes our claimed characterization of $d(L^{\star})$.

Now suppose that $L^{\star}$ is an evaluation functional $J_{g} \in \S^{\star\star}$, defined $J_{g}(L) := L(g)$.
Then for any $x \in \R^n$ and any $g \in \S$, $\inner{J_{g}, -Ax} =  \inner{ J_{g}, x^T \bar L } = x^T \bar L(g)$,
and it follows that $\sum_{i=1}^n \inner{J_{g}, A e_i}^2 = \norm{\bar L(g)}^2$. Thus,
\[ d(J_{g}) = \frac{1}{2}\norm{\bar L(g)}^2  - L_0(g)  + \frac{1}{2} \norm{g}_{\S}. \]
If an argmin $\hat L^{\star}$ of $d$ were the evaluation functional $J_{g}$, 
then $g$ would minimize the right side above. When every $L^{\star} \in \S^{\star\star}$ is an evaluation functional,
i.e. when $\S$ is reflexive, because $d$ has a minimum $\hat L^{\star}$ over $\S^{\star\star}$ 
it follows that the right side above has a minimum $\hat g$ over $g \in \S$. 
Furthermore, recalling our first-order optimality condition $-A^{\star}\hat L^{\star} =  \hat \gamma$, 
$\hat \gamma = \bar L(\hat g)$.


This is essentially true whether $\S$ is reflexive or not because evaluation functionals are dense in the bidual $\S^{\star\star}$ in an appropriate sense.
By Goldstine's theorem, for every $L^{\star} \in \S^{\star\star}$, there is a sequence $g_j \in \S$ satisfying $\norm{g_j}_{\S} \le \norm{L^{\star}}_{\S^{\star\star}}$
for all $j$ and $L(g_j) \to L^{\star}(L)$ pointwise for each $L \in \S^{\star}$ \citep[e.g.,][Theorem 2.6.26]{megginson2012introduction}.
Consider such a sequence $\hat g_j$ for an argmin $\hat L^{\star}$ of $d$. We can characterize $\hat \gamma$ as $\lim_{j \to \infty} \bar L(g_j)$,
as $\hat \gamma = -A^{\star}\hat L^{\star} = \lim_{j \to \infty} -A^{\star} J_{g_j}$: $A^{\star}\hat L^{\star}$ is the solution to finitely many linear equations 
\smash{$\inner{A^{\star}\hat L^{\star}, y_k} = \inner{\hat L^{\star}, Ay_k}$} $= \lim_{j \to \infty} \inner{J_{\hat g_j}, Ay_k}$ 
for $\set{ y_k }$ forming a basis for $\R^n$, and pointwise convergence is sufficient to imply convergence 
of the finite dimensional vector with elements $\inner{J_{\hat g_j}, Ay_k}$. 
Furthermore, because $d$ is continuous and depends only on $\norm{\cdot}_{\S^{\star\star}}$ and the value of finitely many functionals, in particular \smash{$L_0$} 
and a basis for the image of $A$, $d(J_{\hat g_j}) \to d(\hat L^{\star})$, and it follows that
$d(J_{\hat g_j}) \to \inf_{g \in \S}d(J_{g})$.

We conclude our proof by showing that every sequence $g_j$ along which $d(J_{g_j})$ converges to its infimum has the 
same limiting value of $\bar L(g_j)$, which therefore must converge to \smash{$\lim_{j \to \infty} \bar L(\hat g_j) = \hat \gamma$}.
This is the case because every term in $d(J_{g})$ is convex in $g$ and 
there is a term that is uniformly convex in $\bar L(g)$: 
if there were two minimizing sequences $g_j$ and $\tilde g_j$ with different limits 
\smash{$\lim \bar L(g_j) \neq \lim \bar L(\tilde g_j)$}, 
their average $(g_j + \tilde g_j) / 2$ would be a sequence along which $d$ converges to 
something strictly smaller than the average of the limit along $g_j$ or $\tilde g_j$, which is its infimum.
\end{proof}

\begin{proof}[Proof of Lemma~\ref{lemma:projection-theorem}]
Let $a(g) = (1/2) \norm{g - g_{\star}}^2 + \phi(\rho(g))$.  
Because it is proper, convex, coercive, and lower-semicontinuous, $a$ has a minimizer $\tilde g$ \citep[Theorem 2.19]{peypouquet2015convex}.
Zero is in its subdifferential $\partial a(\tilde g)$ at its minimizer,
and by a chain rule for subdifferentials \citep[Corollary 3.5]{combari1996note} and 
the Moreau-Rockafellar theorem for subdifferentials of sums \citep[Theorem 3.30]{peypouquet2015convex},
$\partial a(\tilde g)$ is the set of maps $v_a(f) = \inner{  \tilde g - g_{\star}, f} + \phi'(\rho(\tilde g))v_{\rho}(f)$
for $v_{\rho} \in \partial \rho(\tilde g)$. And by definition, $v_{\rho}(g-\tilde g) \le \rho(g) - \rho(\tilde g)$,
so all functionals $v_a \in \partial a(\tilde g)$ satisfy $v_a(g-\tilde g) \le \inner{\tilde g - g_{\star}, g-\tilde g} + \phi'(\rho(\tilde g))(\rho(g) - \rho(\tilde g))$.
This bound implies the claimed nonnegativity property, as $0 = v_a$ for some $a$.
\end{proof}


We prove Lemma~\ref{lemma:consistency-deterministic} with the aid of the following scaling result.
\begin{lemm}
\label{lemm:rescaling}
Let $\ff$ be a set that is star-shaped around zero, $L$ be a homogeneous functional on $\vspan \ff$, 
and $\norm{\cdot}$ be a norm on $\vspan \ff$. If
$L(f) \le \eta r^2$ for all $f \in \ff$ with $\norm{f} \le r$, then
$L(f) \le (\eta/\alpha)\max\set{\norm{f},\ \alpha r}^2$ for all $f \in \alpha \ff$
for every $\alpha > 0$.
\end{lemm}
\begin{proof}
For $f \in \alpha \ff$ with $\norm{f} \le \alpha r$, consider $f' = f/\alpha$. 
Because $f' \in \ff$ and $\norm{f'} \le r$, our assumed bound implies that 
$L(f)=\alpha L(f') \le \eta \alpha r^2=(\eta/\alpha)(\alpha r)^2$.
For $f \in \alpha \ff$ with $\norm{f} \ge \alpha r$, consider $f' = r f / \norm{f}$.
Because $f' \in \ff$ and $\norm{f'} \le r$, our assumed bound implies that 
$L(f) = L(f') \norm{f}/r \le \eta  r \norm{f} \le (\eta/\alpha) \norm{f}^2$,
using in the last step the property $\norm{f} \ge \alpha r$.
\end{proof}

\begin{proof}[Proof of Lemma~\ref{lemma:consistency-deterministic}]
Given our assumed bounds, if $\delta \in \alpha \F$, 
\begin{equation}
\label{eq:rewritten-ratio-bounds}
\begin{aligned}
& \Pn \delta^2 \ge \eta_Q \P \delta^2 && \quad \text{ when }\ \P \delta^2 \ge (\alpha r)^2 \\ 
&\abs{M(\delta)} \le (\eta_M/\alpha) \P \delta^2  && \quad \text{ when }\ \P \delta^2 \ge (\alpha r)^2  \\
&\abs{M(\delta)} \le \eta_M \alpha r^2 && \quad \text{ when }\ \P \delta^2 \le (\alpha r)^2  \\
\end{aligned}
\end{equation}
The first of these is an immediate consequence of the invariance of the ratio $\Pn f^2 / \P f^2$ to scaling
and the second and third follow from Lemma~\ref{lemm:rescaling} with $L(\cdot)=\abs{M(\cdot)}$.  We will now prove our claims using these bounds.

We begin by showing that $\excess(\delta) > 0$ for all $\delta$ with $\norm{\delta}_{\ff} \ge \alpha$.
It suffices to consider $\delta$ with $\norm{\delta}_{\ff} = \alpha$,
as we can write the others as $\delta=s\delta'$ for $s > 1$ and $\norm{\delta'}_{\ff} = \alpha$,
and $\excess(s\delta') \ge s \excess(\delta')$ for $s \ge 1$ when $\alpha \ge 2\norm{\gapprox}_{\ff}$: 
for such $s$, $\delta'$, and $\alpha$,
\begin{align*}
\excess(s\delta') - s \excess(\delta') &= (s^2-s)\Pn (\delta')^2 + (\sigma^2/n)[(s^2-s)\alpha^2 + (1-s)4\norm{\gapprox}^2] \\  
				       &= (s^2-s)\Pn (\delta')^2 + (\sigma^2/n)(s-1)(s\alpha^2 - 4\norm{\gapprox}^2) \ge 0.
\end{align*}
If $\P \delta^2 \ge (\alpha r)^2$, then 
$\excess(\delta) \ge (\eta_Q - 2\eta_M/\alpha) \P \delta^2 + (\sigma^2/n)(\alpha - 2\norm{\gapprox}_{\ff})_+^2$.
If instead $\P \delta^2 \le (\alpha r)^2$, then 
$\excess(\delta) \ge -2\eta_M \alpha r^2 + (\sigma^2/n)(\alpha - 2\norm{\gapprox}_{\ff})_+^2$.
Thus, $\excess(\delta) > \xi$ for all $\delta$ with $\norm{\delta}_{\ff} \ge \alpha$ 
so long as $\eta_Q - 2\eta_M/\alpha \ge 0$ and $(\sigma^2/n)(\alpha - 2\norm{\gapprox}_{\ff})_+^2 > 2\eta_M \alpha r^2 + \xi$.
These conditions hold for $\alpha > \alpha_0 + (\xi n/\sigma^2)^{1/2}$  where
$\alpha_0 = \max\set{2\eta_M/\eta_Q,\ 3(\norm{\gapprox}_{\ff} + \eta_M r^2 n/\sigma^2)}$.
To see that this lower bound implies the latter condition,
observe that for $\alpha \ge 2\norm{\gapprox}_{\ff}$, it expands to
\[ 0 < \alpha^2 - \alpha \p{4\norm{\gapprox}_{\ff} + 2\eta_M r^2 \lambda} + 4\norm{\gapprox}_{\ff}^2 - \xi \lambda \ \text{ for }\ \lambda = n/\sigma^2,\] 
which holds for $\alpha$ exceeding the larger root of the right side,
\begin{align*} 
&2\norm{\gapprox}_{\ff} + \eta_M  r^2 \lambda 
+ \sqrt{ \p{2\norm{\gapprox}_{\ff} + \eta_M  r^2 \lambda}^2 -  4\norm{\gapprox}_{\ff}^2 + \xi \lambda } \\
&= 2\norm{\gapprox}_{\ff} + \eta_M  r^2 \lambda 
+ \sqrt{ 4\norm{\gapprox}_{\ff}\eta_M  r^2 \lambda + \p{\eta_M  r^2 \lambda}^2 + \xi \lambda } \\
&\le 2\p{\norm{\gapprox}_{\ff} + \eta_M  r^2 \lambda}
+ 2 \sqrt{\norm{\gapprox}_{\ff}\eta_M  r^2 \lambda } + \sqrt{\xi \lambda } \\
& \le 3\p{\norm{\gapprox}_{\ff} + \eta_M r^2 \lambda} + \sqrt{\xi\lambda}
\end{align*}
Here the second expression is derived by
expanding and canceling terms under the square root in the first,
the third is follows via the inequality $\sqrt{a+b+c} \le \sqrt{a}+\sqrt{b} + \sqrt{c}$,
and the fourth follows via the inequality $a+b \ge 2\sqrt{ab}$ relating the arithmetic and geometric means.




Now take $\xi=0$ and consider $\delta \in \alpha\F$ for $\alpha > \alpha_0$.
If $\P \delta^2 \ge (\alpha r)^2$, then 
$\excess(\delta) \ge (\eta_Q - 2 \eta_M/\alpha) \P \delta^2 > 0$.
Otherwise, $\excess(\delta) \ge \Pn \delta^2 - 2 \eta_M \alpha r^2$,
which is positive if $\Pn \delta^2 > 2\eta_M \alpha r^2$.

In summary, we've shown that for $\alpha > \alpha_0$, (i) $\excess(\delta) > 0$ if $\norm{g}_{\ff} \ge \alpha$, $\P \delta^2 \ge (\alpha r)^2$,
or $\Pn \delta^2 > 2\eta_M \alpha r^2$,  and (ii) $\excess(\delta) > \xi$ if $\norm{\delta}_{\ff} \ge \alpha + (\xi n)^{1/2}/\sigma$. 
Taking contrapositives, (i)  $\excess(\delta) \le 0$ only if 
$\norm{\delta}_{\ff} < \alpha$, $\P \delta^2 < (\alpha r)^2$, and $\Pn \delta^2 \le 2\eta_M \alpha r^2$,
and (ii) $\excess(\delta) \le \xi$ only if $\norm{\delta}_{\ff} < \alpha + (\xi n)^{1/2}/\sigma$. 
It follows that for $\alpha=\alpha_0$, nonstrict variants of these bounds hold.
\end{proof}


\subsection{Convergence of the noise term} 
\label{sec:convergence-of-the-noise-term}

In this section, we bound the difference between the noise term in the decomposition \eqref{eq:error-decomp}
and the iid sum $\Pn \gapprox(Z_i) \varepsilon_i,\ \varepsilon_i = Y_i - m(Z_i)$. Because $\hriesz$ is a function of $Z_1 \ldots Z_n$,
we can apply Chebyshev's inequality conditionally on $Z_1 \ldots Z_n$ to the difference between our noise term and this sum. 
With conditional and therefore unconditional probability $1-\delta$,
\begin{equation}
\label{eq:noise-term-deviation}
\begin{aligned}
\abs*{ \Pn (\hriesz_i - \gapprox(Z_i)) \varepsilon_i } 
&\le \delta^{-1/2} n^{-1/2} \sqrt{\Pn [\hriesz_i - \gapprox(Z_i)]^2 v(Z_i)}.  \\
&\le \delta^{-1/2} n^{-1/2} \norm{v}_{\infty} \norm{\hriesz - \gapprox}_{L_2(\Pn)}.
\end{aligned}
\end{equation}
The second bound follows from the first via H\"older's inequality.

\subsection{Bounding the bias term}
\label{sec:bias-term-bound}

In this section, we bound the bias term in the decomposition \eqref{eq:error-decomp}. 
As we work with two function classes $\F$ and $\F'$, to avoid ambiguity we indicate the class with a sub or superscript:
\begin{equation}
\label{eq:weight-def-explicit}
\begin{aligned}
 \hgamma^{\GG} &= \argmin_{\gamma \in \R^n} I_{h,\GG}^2(\gamma) + \frac{\sigma_{\GG}^2}{n}\norm{\gamma}_{L_2(\Pn)}^2, \\
 I_{h,\GG}(\gamma) &= \sup_{f \in \GG} \Pn h(Z_i, f) - \gamma_i f(Z_i).
\end{aligned}
\end{equation}
For any absolutely convex set $\GG$, our bias term satisfies the bound
\[ \abs{\Pn h(Z_i, \hm - m) - \hgamma^{\F}_i (\hm - m)} \le \norm{\hm - m}_{\GG} I_{h,\GG}(\hgamma^{\F}). \]
Rather than using this bound for $\GG=\F$, we use it for \smash{$\F' = \{ f : \norm{f}_{\F}^2 + \rho^{-2}\norm{f}_{L_2(\Pn)}^2 \le 1\}$},
a subset of $\F$ containing only small functions. We control the latter factor as follows.
\begin{equation}
\label{eq:bias-bound-abstract}
\begin{aligned}
&I_{h,\F'}(\hgamma^{\F}) \le \rho\norm{\hgamma^{\F'} - \hgamma^{\F}}_{L_2(\Pn)} + I_{h,\F'}(\hgamma^{\F'}) \\
&\le \rho\norm{\hgamma^{\F'} - \hgamma^{\F}}_{L_2(\Pn)} +  \sqb{I_{h,\F'}^2(\riesz[\psi]) +
\frac{\sigma_{\F'}^2}{n}\p{\norm{\riesz[\psi]}_{L_2(\Pn)}^2 - \norm{\hgamma^{\F'}}_{L_2(\Pn)}^2}}^{1/2} \\
&\le \rho\norm{\hgamma^{\F'} - \hgamma^{\F}}_{L_2(\Pn)} + I_{h,\F'}(\riesz[\psi]) \\
&+ \sqb{\frac{\sigma_{\F'}^2}{n}\cb{\norm{\riesz[\psi]}_{L_2(\Pn)}^2 \wedge 2\norm{\riesz[\psi]}_{L_2(\Pn)}\p{\norm{\hgamma^{\F}-\riesz[\psi]}_{L_2(\Pn)} + \norm{\hgamma^{\F'} - \hgamma^{\F}}_{L_2(\Pn)}}}}^{1/2}
\end{aligned}
\end{equation}
The first bound, via the Cauchy-Schwarz inequality, is implied by the property that $\norm{f}_{L_2(\Pn)} \le \rho$ for all $f \in \F'$.
\begin{align*} 
I_{h,\F'}(\hgamma^{\F}) 
&=   \sup_{f \in \F'} \Pn[ h(Z_i, f) - \hgamma^{\F'} f + (\hgamma^{\F'}-\hgamma^{\F})f] \\
&\le \sup_{f \in \F'} \Pn[ h(Z_i, f) - \hgamma^{\F'} f] + \sup_{f \in \F'} \Pn (\hgamma^{\F'}-\hgamma^{\F})f \\
&\le I_{h,\F'}(\hgamma^{\F'}) + \rho \norm{\hgamma^{\F'}-\hgamma^{\F}}_{L_2(\Pn)}. 
\end{align*}
The second bound is implied by the optimality of the weights $\hgamma^{\F'}$. It is a rearrangement of the 
condition that the function minimized by $\hgamma^{\F'}$ is smaller at its minimizer than at the weights $\gamma_i = \riesz[\psi](Z_i)$.
The third bound follows from the second by some elementary arithmetic. As $a^2-b^2=2a(a-b) - (a-b)^2 \le 2a\abs{a-b}$,
using this bound termwise and then taking Cauchy-Schwarz and triangle inequality bounds,
\begin{align*}
\norm{\riesz[\psi]}_{L_2(\Pn)}^2 - \norm{\hgamma^{\F'}}_{L_2(\Pn)}^2 
&\le 2\Pn \riesz[\psi](Z_i) \abs{\riesz[\psi](Z_i) - \hgamma^{\F'}_i} \\
&\le 2\norm{\riesz[\psi]}_{L_2(\Pn)}\norm{\riesz[\psi]-\hgamma^{\F'}}_{L_2(\Pn)} \\
&\le 2\norm{\riesz[\psi]}_{L_2(\Pn)}\p{\norm{\riesz[\psi] - \hgamma^{\F}}_{L_2(\Pn)}  + \norm{\hgamma^{\F} - \hgamma^{\F'}}_{L_2(\Pn)}}.
\end{align*}

Having established the abstract bound \eqref{eq:bias-bound-abstract}, we will derive a concrete version
by controlling $\norm{\hgamma^{\F'}-\hgamma^{\F}}_{L_2(\Pn)}$.
We will take $\sigma_{\F'}^2 = \sigma_{\F}^2 / (1 - \eta)$ with $\eta = \sigma_{\F}^2/(\rho^2 n)$,
as this allows us to control it well. To do this, we recall that the weights $\hgamma_{\GG}$ satisfy
$\hgamma_i^{\GG}=\lim_{j \to \infty}g_j(Z_i)$ where $g_j$ is a minimizing sequence for the dual $d_{\GG}$ \eqref{eq:concrete-dual},
use the similarity of $d_{\F'}$ and $(\sigma_{\F'}/\sigma_{\F})^2 d_{\F}$ to show that a minimizing sequence for the latter
is almost a minimizing sequence for the former, and use the strong convexity of $d_{\F'}$ 
to show that this implies $\hgamma^{\F'} \approx \hgamma^{\F}$. 

\begin{lemm}
\label{lemma:weight-similarity}
For an absolutely convex set $\F$ and $\sigma_{\F} \ge 0$, 
let 
\[ \F' = \set{ f : \norm{f}_{\F}^2 + \rho^{-2}\norm{f}_{L_2(\Pn)}^2 \le 1} \ \text{ and }\ 
\sigma_{\F'}^2 = \sigma_{\F}^2 / (1-\eta) \ \text{ for } \eta = \sigma_{\F}^2 / (\rho^2 n). \]
Define $\hgamma^{\F}$ and $\hgamma^{\F'}$ as in \eqref{eq:weight-def-explicit}
and corresponding duals $d_{\F}$ amd $d_{\F'}$ as in \eqref{eq:concrete-dual}
and suppose that for some $\gapprox$, 
$\excess_{\F}(\delta) = (2/n)[d_{\F}(\gapprox + \delta) - d_{\F}(\gapprox)]$
has the property that for every $\xi \ge 0$, $\excess_{\F}(\delta) \le \xi$ 
only if $\norm{\delta}_{\F} \le \alpha + (\xi n)^{1/2}/\sigma_{\F}$. For any $\gamma \in \R^n$, 

\begin{align*}
&\rho\norm{\hgamma_{\F} - \hgamma_{\F'}}_{L_2(\Pn)} 
\le 6\eta\sqb{I_{h,\F'}(\gamma) + \rho\norm{\gamma}_{L_2(\Pn)}} \\
&\vee   \sqb{\p{4\rho^2\alpha  + 2I_{h,\F'}(\gamma) 
      + 4\rho^2c_{\star} \norm{\hgamma_{\F} - \gapprox}_{L_2(\Pn)}^{1/2}} 6\eta I_{h,\F'}(\gamma) }^{1/2} \\
&\vee   \sqb{24 \eta \rho^{3/2} c_{\star} I_{h,\F'}(\gamma)}^{2/3} \ \ \text{ with }\ \ c_{\star}^2 = 2\rho^{-3}I_{h,\F'}(\gamma) + 2\rho^{-2} \norm{\gamma}_{L_2(\Pn)}. 
\end{align*}
\end{lemm}

To establish our claim of oracle behavior, in the sense that we get essentially the same bias bound with the weights $\hgamma^{\F}$ 
as we would with $\hgamma^{\F'}$, we need to show that \smash{$\rho \norm{\hgamma_{\F}' - \hgamma_{\F}}_{L_2(\Pn)}$} is small relative to $I_{h,\F'}(\hgamma^{\F'})$.
By working with the bound above, we show that subject to some limits on the range of $\rho$, this is the case.

\begin{coro}
\label{coro:weight-similarity}
Under the assumptions of Lemma~\ref{lemma:weight-similarity},
for $\phi \ge I_{h,\F'}(\gamma)$,
$\rho \norm{\hgamma_{\F}' - \hgamma_{\F}}_{L_2(\Pn)} \le \epsilon \phi$ 
if the bounds below are satisfied.
\begin{equation}
\label{eq:rho-lb-implicit}
\begin{aligned}
\rho^2                   &\ge \p{\epsilon^{-1}12 \vee \epsilon^{-2} 36 \vee \epsilon^{-3/2}48 } \sigma_{\F}^2/n, \\
\phi       &\ge \p{\epsilon^{-2} 72 \alpha } \sigma_{\F}^2/n, \\
\rho \phi   &\ge  \p{\epsilon^{-1} 64 \norm{\riesz}_{L_2(\Pn)} 
    			     \vee \epsilon^{-2} 144\norm{\hriesz_{\F} - \gapprox}_{L_2(\Pn)}^{1/2} \norm{\riesz}_{L_2(\Pn)}^{1/2}} \sigma_{\F}^2/n, \\
\rho^3 \phi &\ge \p{\epsilon^{-4} 144^2 \norm{\hriesz_{\F} - \gapprox}_{L_2(\Pn)}} \sigma_{\F}^4/n^2.
\end{aligned}
\end{equation}
\end{coro}


Each of these conditions is a lower bound on an increasing function of $\rho$, as $I_{h,\F'}(\gamma)$ is increasing in $\rho$,
so this is implictly a lower bound on $\rho$. We can simplify these conditions if we can bound
\smash{$\norm{\hriesz_{\F} - \gapprox}_{L_2(\Pn)}$} in terms of $\alpha$ as in Lemma~\ref{lemma:abstract-finite-sample}.

\begin{coro}
\label{coro:weight-similarity-simplified}
Under the assumptions of Corollary~\ref{coro:weight-similarity},
if $\phi \ge I_{h,\F'}(\gamma) \vee \epsilon^{-2} 72 \alpha_{\phi} \sigma_{\F}^2/n$, $\epsilon \le 9/16$,
and $\norm{\hriesz_{\F} - \gapprox}_{L_2(\Pn)}^2 \le \alpha_{\phi} s^2$ and $\alpha_{\phi} \ge n s^2 / \sigma_{\F}^2$
for some $s$ and $\alpha_{\phi} \ge \alpha$, the bounds \eqref{eq:rho-lb-implicit}
are satisfied and therefore $\rho \norm{\hgamma_{\F}' - \hgamma_{\F}}_{L_2(\Pn)} \le \epsilon \phi$ if 
\begin{align*}
\rho &\ge  \epsilon^{-1} 6 \sigma_\F/n^{1/2} \vee (1/2) \norm{\riesz}_{L_2(\Pn)}  \sigma_\F^2/(s^2 n). 
\end{align*}
\end{coro}
\noindent Using the bound on $\norm{\hriesz_{\F} - \gapprox}_{L_2(\Pn)}^2$ from Lemma~\ref{lemma:abstract-finite-sample}, 
we can take $\alpha_{\phi} = \alpha$, $s^2=2\eta_Mr^2$.
Taking $\gamma=\riesz[\psi]$ in \eqref{eq:bias-bound-abstract}
and substituting $\phi \ge I_{h,\F'}(\riesz[\psi]) \vee \epsilon^{-2} 72 \alpha \sigma_{\F}^2/n$
for $I_{h,\F'}(\riesz[\psi])$, when $\rho$ satisfies the lower bound from Corollary~\ref{coro:weight-similarity-simplified}, we get the following oracle bias bound. 
\begin{equation}
\label{eq:bias-bound-concrete}
\begin{aligned}
&I_{h,\F'}(\hgamma^{\F}) \le (1+\epsilon)\phi \\
&+ \sqb{\frac{\sigma_{\F'}^2}{n}\cb{ \norm{\riesz[\psi]}^2_{L_2(\Pn)} \wedge 2\norm{\riesz[\psi]}_{L_2(\Pn)}\p{\norm{\hgamma^{\F}-\riesz[\psi]}_{L_2(\Pn)} + \frac{\epsilon \phi}{\rho} }}}^{1/2} \\
&\le (1+\epsilon + \epsilon') \phi \\ 
&+ \sqb{\frac{\sigma_{\F}^2}{(1-\eta) n}\cb{\norm{\riesz[\psi]}^2_{L_2(\Pn)} \wedge 2\norm{\riesz[\psi]}_{L_2(\Pn)}\norm{\hgamma^{\F}-\riesz[\psi]}_{L_2(\Pn)}}}^{1/2}, \\
(\epsilon')^2 
&= \frac{2\sigma_{\F'}^2\norm{\riesz[\psi]}_{L_2(\Pn)}\epsilon}{n \rho \phi}
\le \frac{2\sigma_{\F'}^2 \norm{\riesz[\psi]}_{L_2(\Pn)} \epsilon }{128\epsilon^{-1}\norm{\riesz[\psi]}_{L_2(\Pn)}\sigma_{\F}^2} = \frac{\epsilon^2}{64(1-\eta)}.
\end{aligned}
\end{equation}
This definition of $(\epsilon')^2$  equates the bracketed term involving $\phi$ and $(\epsilon')^2\phi^2$, 
so the second bound follows by the elementary inequality $\sqrt{a+b}\le \sqrt{a} + \sqrt{b}$.
To bound $(\epsilon')^2$, we've substituted in the denominator one of the lower bounds on $\rho \phi$ from Corollary~\ref{coro:weight-similarity}. 
We conclude the section by proving our lemma and corollaries.


\begin{proof}[Proof of Lemma~\ref{lemma:weight-similarity}]

The bulk of our proof will be devoted to bounding $\norm{\hg_{\F'}- \hg_{\F}}_{L_2(\Pn)}$ where
each $\hg_{\GG}$ is an approximate minimizer of the dual $d_{\GG}$.
We will consider $\hg_{\GG}$ satisfing $d_{\GG}(\hg_{\GG}) \le \min(d_{\GG}(\tilde g), \inf_g d_{\GG}(g) + \epsilon)$
for $\epsilon > 0$.  To simplify our notation, we will work with $d_n := (2/n)d_{\F}$, $d_n' := (2/n)d_{\F'}$, and $\epsilon_n = (2/n)\epsilon$,
and let \smash{$\hg = \hg_{\F}$} and \smash{$\hg' = \hg_{\F'}$} and \smash{$\sigma = \sigma_{\F}$} and \smash{$\sigma' = \sigma_{\F'}$}.

We define $\sigma'$ as we do because it allows us to write $d_n'(g)$ as the sum of $(\sigma'/\sigma)^2 d_n(g)$ and a small remainder.
Observe that  
\[ 
\frac{(\sigma')^2}{\sigma^2} =  \frac{1}{1-\eta} = \frac{\rho^2 n}{\rho^2 n - \sigma^2} = 1 + \frac{\sigma^2}{\rho^2 n - \sigma^2} =  1+\frac{\sigma^2}{\rho^2 n(1-\eta)} = 1+\frac{(\sigma')^2}{\rho^2 n}, \] 
so expanding $\norm{\cdot}_{\F'}^2 = \norm{\cdot}_{\F}^2 + \norm{\cdot}_{L_2(\Pn)}^2/\rho^2$ in the definition \eqref{eq:concrete-dual} of $d_n'$, 
\begin{equation*}
\begin{aligned} 
d_n'(g) 
&= \Pn g^2 - 2\Pn h(\cdot, g)  + ((\sigma')^2/n)(\norm{g}_{\F}^2 + \norm{g}_{L_2(\Pn)}^2/\rho^2) \\
&= (1+(\sigma')^2/(\rho^2 n)) \Pn g^2 - 2\Pn h(\cdot, g)  + ((\sigma')^2/n)\norm{g}_{\F}^2 \\
&= (\sigma'/\sigma)^2 \sqb{\Pn g^2 - 2\Pn h(\cdot, g) +  (\sigma^2/n)\norm{g}_{\F}^2} + 2[(\sigma'/\sigma)^2 - 1]\Pn h(\cdot, g) \\
&= (\sigma'/\sigma)^2 d_n(g) + 2[(\sigma'/\sigma)^2 - 1]\Pn h(\cdot, g).
\end{aligned} 
\end{equation*}
As $\hg'$ and $\hg$ approximately minimize $d_n'$ and $d_n' - 2[(\sigma'/\sigma)^2 - 1]\Pn h(\cdot, g)$,
\begin{align*}
&d_n'(\hat g')  \le d_n'(\hat g) + \epsilon_n, \\
&d_n'(\hat g) - 2[(\sigma'/\sigma)^2 - 1]\Pn h(\cdot, \hat g) \le d_n'(\hat g') - 2[(\sigma'/\sigma)^2 - 1]\Pn h(\cdot, \hat g') + \epsilon_n, \text{ and therefore } \\ 
&d_n'(\hat g')  \le d_n'(\hat g) + \epsilon_n \le d_n'(\hat g') + 2[(\sigma'/\sigma)^2 - 1]\Pn h(\cdot, \hat g - \hat g') + 2\epsilon_n. 
\end{align*}
This implies a bound on the suboptimality of $\hat g$,
\[ d_n'(\hat g) - d_n'(\hat g') \le 2[(\sigma'/\sigma)^2 - 1]\Pn h(\cdot, \hat g - \hat g') + 2\epsilon_n. \]
Furthermore, because $d_n'$ is $2(\sigma'/\sigma)^2$-strongly convex with respect to $\norm{\cdot}_{L_2(\Pn)}$,
\begin{align*}
(1/2)(\sigma'/\sigma)^2 \norm{\hat g - \hat g'}_{L_2(\Pn)}^2 
&\le (1/2)d_n'(\hat g) + (1/2)d_n'(\hat g') - d_n'((\hat g + \hat g')/2) \\
&= (1/2)[d_n'(\hat g) - d_n'(\hat g')] + [d_n'(\hat g') - d_n'((\hat g + \hat g')/2)] \\
&\le [(\sigma'/\sigma)^2 - 1]\Pn h(\cdot, \hat g - \hat g') + 2\epsilon_n. 
\end{align*}
Here we've used the suboptimality bound above and our assumption that $\hat g'$ approximately
minimizes $d_n'$. As $(\sigma/\sigma')^{2}[(\sigma'/\sigma)^2 - 1] = 1- (\sigma/\sigma')^2 = \eta$,
it follows that
\[ \norm{\hat g - \hat g'}_{L_2(\Pn)}^2 \le 2\eta  \Pn h(\cdot, \hat g - \hat g') + 4 (\sigma/\sigma')^2\epsilon_n, \]
and as $\sigma/\sigma' \le 1$,
\begin{equation}
\label{eq:two-gs-l2-bound}
\begin{aligned}
&\norm{\hat g - \hat g'}_{L_2(\Pn)}^2/(2\eta) - (2/\eta)\epsilon_n\\ 
&\le \Pn h(\cdot, \hat g - \hat g')  \\
&=   \sqb{ \Pn[ h(\cdot, \hat g - \hat g') - \gamma (\hat g - \hat g')] 
		   			         	+ \Pn \gamma (\hat g - \hat g') }  \\
&\le I_{h,\F'}(\gamma)\norm{\hat g - \hat g'}_{\F'} + \norm{\gamma}_{L_2(\Pn)} \norm{\hat g - \hat g'}_{L_2(\Pn)} \\ 
&= I_{h,\F'}(\gamma)\sqrt{\norm{\hat g - \hat g'}_{\F}^2 + \rho^{-2}\norm{\hat g - \hat g'}_{L_2(\Pn)}^2} + \norm{\gamma}_{L_2(\Pn)} \norm{\hat g - \hat g'}_{L_2(\Pn)} \\
&\le I_{h,\F'}(\gamma)\norm{\hat g - \hat g'}_{\F} + ( \rho^{-1} I_{h,\F'}(\gamma) + \norm{\gamma}_{L_2(\Pn)}) \norm{\hat g - \hat g'}_{L_2(\Pn)}.
\end{aligned}
\end{equation}

We eliminate the dependence of this bound on $\hg$ by substituting a bound on $\norm{\hat g - \hat g'}_{\F}$.
By the triangle inequality, $\norm{\hat g - \hat g'}_{\F} \le \norm{\hat g - \tilde g}_{\F} + \norm{\hat g' - \tilde g}_{\F}$,
and as $d_n(\hg) \le d_n(\tilde g)$, our assumption about $\excess_{\F}$
implies that the first term is bounded by $\alpha$ and the second by $\alpha + \sqrt{\xi n/\sigma^2}$ if $d_n(\hg') - d_n(\gapprox) \le \xi$.
To establish a bound like this, we use the similarity of
$d_n$ and $d_n'$ like we did above. As $d_n(g) = q d_n'(g) - 2\eta \Pn h(\cdot,g)$ for $q=(\sigma/\sigma')^2$
and $d_n'(\hg') \le d_n'(\gapprox)$, either $d_n(\hg') \le d_n(\gapprox)$ or
\[ \underset{d_n(\gapprox)}{ q d_n'(\gapprox) - 2\eta \Pn h(\cdot, \gapprox)}  \le
   \underset{d_n(\hg')}{q d_n'(\hg') - 2\eta \Pn h(\cdot,\hg')} \le    
   q d_n'(\gapprox) - 2\eta \Pn h(\cdot,\hg'). \]
Consequently, $d_n(\hg') - d_n(\gapprox) \le \xi$ for $\xi= 2\eta\max(0, \Pn h(\cdot, \gapprox - \hg'))$.
It follows that $\norm{\hg' - \tilde g}_{\F} \le \alpha + \sqrt{\xi n/\sigma^2}$,
and the bound remains valid if we subsititute an upper bound on $\sqrt{\xi}$.
We derive an upper bound as follows.
\begin{align*} 
\abs{\Pn h(\cdot, \gapprox - \hg')} 
&\le \abs{\Pn h(\cdot, \gapprox - \hg') - \gamma (\gapprox - \hg')} + \abs{\Pn \gamma (\gapprox - \hg')} \\
&\le \norm{\hg' - \gapprox}_{\F'} I_{h,\F'}(\gamma) + \norm{\gamma}_{L_2(\Pn)} \norm{\hg'- \gapprox}_{L_2(\Pn)} \\
&\le \norm{\hg' - \gapprox}_{\F} I_{h,\F'}(\gamma) + (\rho^{-1}I_{h,\F'}(\gamma) + \norm{\gamma}_{L_2(\Pn)}) \norm{\hg'- \gapprox}_{L_2(\Pn)},
\end{align*}
so 
\[ \norm{\hg' - \tilde g}_{\F} \le \alpha 
+ \sqrt{2\eta n/\sigma^2}\sqb{\norm{\hg' - \gapprox}_{\F}^{1/2} I_{h,\F'}^{1/2}(\gamma) 
	   + \sqrt{\rho^{-1}I_{h,\F'}(\gamma) + \norm{\gamma}_{L_2(\Pn)}} \norm{\hg'- \gapprox}_{L_2(\Pn)}^{1/2}}.
\]
Here $\sqrt{2\eta n/\sigma^2} = \sqrt{2/\rho^2}$, and this 
is a quadratic inequality $y^2 \le  by + c$ for 
\begin{align*} 
y&=\norm{\hg' - \tilde g}_{\F}^{1/2},\\
b&=\sqrt{2} \rho^{-1} I_{h,\F'}^{1/2}(\gamma),\\
c&= \alpha + c_{\star}\norm{\hg'- \gapprox}_{L_2(\Pn)}^{1/2}, 
\quad  c_{\star}^2 = 2\rho^{-3}I_{h,\F'}(\gamma) + 2\rho^{-2} \norm{\gamma}_{L_2(\Pn)}. 
\end{align*}
Its solutions satisfy $y^2 \le (b + \sqrt{b^2+4c})^2/4 \le b^2 + 4c$, so
\begin{align*}
\norm{\hg' - \tilde g}_{\F} 
&\le 2\rho^{-2} I_{h,\F'}(\gamma) + 4\alpha + 4 c_{\star} \p{\norm{\hg-\gapprox}_{L_2(\Pn)}^{1/2} + \norm{\hg'-\hg}_{L_2(\Pn)}^{1/2}}.
\end{align*}
Substituting this in \eqref{eq:two-gs-l2-bound},
\begin{align*}
&\norm{\hat g - \hat g'}_{L_2(\Pn)}^2/(2\eta) \\ 
&\le \sqb{\rho^{-1}I_{h,\F'}(\gamma) + \norm{\gamma}_{L_2(\Pn)}} \norm{\hg' - \hg}_{L_2(\Pn)} \\
&+ \sqb{(2/\eta)\epsilon_n + \p{4\alpha  + 2\rho^{-2}I_{h,\F'}(\gamma) 
      + 4c_{\star} \norm{\hg - \gapprox}_{L_2(\Pn)}^{1/2}} I_{h,\F'}(\gamma)} \\
&+ \sqb{4c_{\star} I_{h,\F'}(\gamma)} \norm{\hg'-\hg}_{L_2(\Pn)}^{1/2}.
\end{align*}
This is $ax^2 \le bx + c + dx^{1/2}$ for $x=\norm{\hat g - \hat g'}_{L_2(\Pn)}$, $a=1/(2\eta)$, and successive bracketed factors $b$,$c$,$d$.
This implies that $ax^2 \le 3\max(bx, c, dx^{1/2})$ and therefore that $x \le \max(3b/a, (3c/a)^{1/2}, (3d/a)^{2/3})$. 
Expanding $a$,$b$,$c$,$d$,
\begin{align*}
&\norm{\hat g - \hat g'}_{L_2(\Pn)} 
\le 6\eta\sqb{\rho^{-1}I_{h,\F'}(\gamma) + \norm{\gamma}_{L_2(\Pn)}} \\
&\vee   \sqb{12\epsilon_n + 6\eta\p{4\alpha  + 2\rho^{-2}I_{h,\F'}(\gamma) 
      + 4c_{\star} \norm{\hg - \gapprox}_{L_2(\Pn)}^{1/2}} I_{h,\F'}(\gamma) }^{1/2} \\
&\vee   \sqb{24 \eta c_{\star} I_{h,\F'}(\gamma)}^{2/3}. 
\end{align*}
This bound is satisfied with $\hgamma$ and $\hgamma'$ in place of $\hg$ and $\hg'$ and $\epsilon_n=0$, 
as $\hgamma_i=\lim_{j\to\infty}\hg_j(Z_i)$ and $\hgamma_i'=\lim_{j \to \infty}\hg_j'(Z_i)$
for approximate minimizers $\hg_j$ and $\hg_j'$ satisying our conditions for $\epsilon^j \to 0$. 
We derive our claimed bound by multiplying by $\rho$.
\end{proof}

\begin{proof}[Proof of Corollary~\ref{coro:weight-similarity}]
Throughout this proof, we will write $\sigma$ meaning $\sigma_\F$.
We work with the bound from Lemma~\ref{lemma:weight-similarity},
which we relax by substituting the upper bound $\phi$ for $I_{h,\F}(\gamma)$.
Then, within each branch of the maximum, we will allocate to each term in our bound a fraction of $\epsilon \phi$.


Consider the first branch. Recalling that $\eta = \sigma^2/(\rho^2 n)$,
\begin{align*}
&6\eta \phi \le \epsilon_{1,1} \phi 
    && \text{ if } 6\sigma^2/n \le \epsilon_{1,1} \rho^2, \\
&6\eta\rho\norm{\gamma}_{L_2(\Pn)} \le \epsilon_{1,2}  \phi
    && \text{ if } 6\norm{\gamma}_{L_2(\Pn)}\sigma^2/n \le \epsilon_{1,2} \rho \phi
\end{align*}
It is bounded by $\epsilon_1 \phi$ for $\epsilon_1 = \epsilon_{1,1}+\epsilon_{1,2}$ if these conditions are satisfied.


Now consider the second branch. $(a\phi)^{1/2} \le \epsilon_2 \phi$ if
$a \le \epsilon_2^2 \phi$, so we will show that each term $a_j$ in $a$ satifies
$a_j \le \epsilon_{2,j}^2 \phi$. It will follow that their sum satisfies
$a \le \epsilon_2^2 \phi$ for $\epsilon_2^2 = \sum_{j}\epsilon_{2,j}^2$,
and therefore that the second branch is bounded by $\epsilon_2 \phi$.
We now bound each term $a_j$.
\begin{align*} 
&24\eta\rho^2\alpha \le \epsilon_{2,1}^2 \phi
    && \text{ if } 24\alpha \sigma^2/n \le \epsilon_{2,1}^2 \phi, \\
&12\eta \phi \le \epsilon_{2,2}^2 \phi
    && \text{ if } 12\sigma^2/n \le \epsilon_{2,2}^2 \rho^2, \\
&24\eta \rho^2 c_{\star} \norm{\hriesz_{\F} - \gapprox}_{L_2(\Pn)}^{1/2} \le \epsilon_{2,3}^2  \phi
    && \text{ if }\ \  48\norm{\hriesz_{\F} - \gapprox}_{L_2(\Pn)}^{1/2}\sigma^2/n \le \epsilon_{2,3}^2 \rho^{3/2} \phi^{1/2},  \\
& && \text{and }    48\norm{\hriesz_{\F} - \gapprox}_{L_2(\Pn)}^{1/2}\norm{\gamma}_{L_2(\Pn)}^{1/2}\sigma^2/n \le \epsilon_{2,3}^2 \rho \phi.
\end{align*}
For the third term, we've used the bound $c_{\star} \le \max(4\rho^{-3}\phi,\ 4\rho^{-2}\norm{\gamma}_{L_2(\Pn)})^{1/2}$.


Finally, consider the third branch. $(a\phi)^{2/3} \le \epsilon_3 \phi$ if
$a^2 \le \epsilon_3^3 \phi$, so we will show that 
we will show that the two terms $a_j^2$ in $a^2=48^2\eta^2\rho^3c_{\star}^2$ 
satisfy $a_j^2 \le \epsilon_{3,j}^3 \phi$ for $\epsilon_3^3 = \sum_{j}\epsilon_{3,j}^3$.
It will follow that the third term is bounded by $\epsilon_3 \phi$.
We now bound these two terms.
\begin{align*}
& 24^2 \eta^2 \rho^3 \cdot 2 \rho^{-3} \phi \le \epsilon_{3,1}^3 \phi
&&\text{ if } 1152\sigma^4/n^2 \le \epsilon_{3,1}^3 \rho^4, \\
& 24^2 \eta^2 \rho^3 \cdot 2 \rho^{-2} \norm{\gamma}_{L_2(\Pn)} \le \epsilon_{3,2}^3 \phi
&&\text{ if } 1152\norm{\gamma}_{L_2(\Pn)}\sigma^4/n^2 \le \epsilon_{3,2}^3 \rho^3 \phi. 
\end{align*}


Bounding the maximum over the three branches by the maximum of our bounds,
$\rho\norm{\hriesz_{\F} - \hriesz_{\F'}}_{L_2(\Pn)} \le \epsilon \phi$
for $\epsilon=\max_{i \in 1 \ldots 3}\epsilon_{i}$ if 
\begin{align*}
\rho^2 &\ge \p{\epsilon_{1,1}^{-1}6 \vee \epsilon_{2,2}^{-2} 12 \vee \epsilon_{3,1}^{-3/2}1152^{1/2} } \sigma^2/n, \\
\phi      &\ge \p{\epsilon_{2,1}^{-2} 24\alpha } \sigma^2/n, \\
\rho \phi &\ge  \p{\epsilon_{1,2}^{-1} 6 \norm{\riesz}_{L_2(\Pn)} 
    			\vee \epsilon_{2,3}^{-2} 48\norm{\hriesz_{\F} - \gapprox}_{L_2(\Pn)}^{1/2} \norm{\riesz}_{L_2(\Pn)}^{1/2}} \sigma^2/n, \\
\rho^3 \phi &\ge \p{\epsilon_{2,3}^{-4} 48^2 \norm{\hriesz_{\F} - \gapprox}_{L_2(\Pn)} \vee \epsilon_{3,2}^{-3} 1152\norm{\gamma}_{L_2(\Pn)}} \sigma^4/n^2.
\end{align*}

To simplify these conditions, first set $\epsilon=\epsilon_1=\epsilon_2=\epsilon_3$ and equally divide
contributions to the $\epsilon_{i}$ between the $\epsilon_{i,j}$ respectively,
taking $\epsilon_{1,j}=\epsilon/2$, $\epsilon_{2,j}^2 = \epsilon^2/3$, and $\epsilon_{3,j}^3 = \epsilon^3/2$.
\begin{align*}
\rho^2 &\ge \p{\epsilon^{-1}12 \vee \epsilon^{-2} 36 \vee \epsilon^{-3/2}48 } \sigma^2/n, \\
\phi      &\ge \p{\epsilon^{-2} 72 \alpha } \sigma^2/n, \\
\rho \phi &\ge  \p{\epsilon^{-1} 12 \norm{\riesz}_{L_2(\Pn)} 
    			\vee \epsilon^{-2} 144\norm{\hriesz_{\F} - \gapprox}_{L_2(\Pn)}^{1/2} \norm{\riesz}_{L_2(\Pn)}^{1/2}} \sigma^2/n, \\
\rho^3 \phi &\ge \p{\epsilon^{-4} 144^2 \norm{\hriesz_{\F} - \gapprox}_{L_2(\Pn)} \vee \epsilon^{-3} 48^2 \norm{\gamma}_{L_2(\Pn)}} \sigma^4/n^2.
\end{align*}
In our lemma statement, we increase the first lower bound on $\rho \phi$ to \smash{$\epsilon^{-1} 64 \norm{\riesz}_{L_2(\Pn)}$}
and then drop the second lower bound on \smash{$\rho^3 \phi$.} The dropped bound is implied by multiplying 
this lower bound on $\rho \phi$ and our lower bound $\epsilon^{-2} 36$ on $\rho^2$.
\end{proof}


\begin{proof}[Proof of Corollary~\ref{coro:weight-similarity-simplified}]
Throughout this proof, we will write $\sigma$ meaning $\sigma_{\F}$.
We will choose $\rho$ so that the bounds \eqref{eq:rho-lb-implicit} are satisfied.
As $\phi \ge \epsilon^{-2}72\alpha_\phi \sigma^2/n$,
we have the lower bounds $\rho^3 \phi \ge \rho^3 \epsilon^{-2}72\alpha_\phi \sigma^2/n$
and $\rho \phi \ge  \rho \epsilon^{-2}72\alpha_\phi \sigma^2/n$. 
These exceed the corresponding bounds
from Corollary~\ref{coro:weight-similarity} as follows. 
\begin{align*} 
&\rho^3 \epsilon^{-2}72\alpha_\phi \sigma^2/n \ge \epsilon^{-4} 144^2 \norm{\hriesz_{\F} - \gapprox}_{L_2(\Pn)} \sigma^4/n^2 \\
&\qquad \text{ if } \quad  \rho^3 \ge \epsilon^{-2} (144^2/72) \norm{\hriesz_{\F} - \tilde g}_{L_2(\Pn)}\sigma^2/(\alpha_\phi n). \\
&\rho \epsilon^{-2}72\alpha_\phi \sigma^2/n \ge \epsilon^{-1} 64 \norm{\riesz}_{L_2(\Pn)} \sigma^2/n \\
&\qquad \text{ if } \quad  \rho   \ge \epsilon (72/64) \norm{\riesz}_{L_2(\Pn)}/\alpha_\phi, \\
&\rho \epsilon^{-2}72\alpha_\phi \sigma^2/n \ge \epsilon^{-2} 144\norm{\hriesz_{\F} - \gapprox}_{L_2(\Pn)}^{1/2} \norm{\riesz}_{L_2(\Pn)}^{1/2} \sigma^2/n \\
&\qquad \text{ if } \quad \rho   \ge (144/72) \norm{\hriesz_{\F} - \gapprox}_{L_2(\Pn)}^{1/2}\norm{\riesz}_{L_2(\Pn)}^{1/2} /\alpha_\phi. 
\end{align*}
Simplifying fractions and substituting the upper bound $\alpha_\phi^{1/2}s \ge \norm{\hriesz_{\F} - \tilde g}_{L_2(\Pn)}$
in the numerator, these bounds hold if
\begin{align*} 
\rho^3 &\ge \epsilon^{-2} 288 s \sigma^2 / (\alpha_\phi^{1/2} n), \\
\rho   &\ge \epsilon (8/9) \norm{\riesz}_{L_2(\Pn)}/\alpha_\phi, \\
\rho   &\ge 2 s^{1/2} \norm{\riesz}_{L_2(\Pn)}^{1/2} /\alpha_\phi^{3/4}, \\
\end{align*} 
And substituting the lower bound $\alpha_\phi \ge n s^2 / \sigma^2$ in the denominators, these hold if
\begin{align*} 
\rho   &\ge \epsilon^{-2/3} 288^{1/3}  \sigma / n^{1/2} && \text{ or equivalently }  \rho^3 \ge \epsilon^{-2} 288  \sigma^3 / n^{3/2}, \\
\rho   &\ge \epsilon (8/9) \norm{\riesz}_{L_2(\Pn)} \sigma^2/(s^2 n), &&\\
\rho   &\ge 2 \norm{\riesz}_{L_2(\Pn)}^{1/2} \sigma^{3/2}/(s n^{3/4}), && \\
\end{align*} 
As $\phi \ge \epsilon^{-2}72\alpha \sigma^2/n$ by construction, 
the bounds \eqref{eq:rho-lb-implicit} from Corollary~\ref{coro:weight-similarity}
hold if the bounds above and the explicit lower bounds on $\rho^2$ from \eqref{eq:rho-lb-implicit} do.
That is, if 
\begin{align*} 
\rho &\ge  \p{\epsilon^{-1/2}\sqrt{12} \vee \epsilon^{-2/3} 288^{1/3} \vee \epsilon^{-3/4}\sqrt{48} \vee \epsilon^{-1} \sqrt{36} }\sigma/n^{1/2} \\
     &\vee \epsilon (8/9) \norm{\riesz}_{L_2(\Pn)} \sigma^2/(s^2 n) \\
     &\vee  2 \norm{\riesz}_{L_2(\Pn)}^{1/2} \sigma^{3/2}/(s n^{3/4}).
\end{align*}

The first term in this lower bound will be $6\epsilon^{-1}\sigma/\sqrt{n}$ 
if $\epsilon \le (36/48)^2 \wedge 36^{3/2}/288 \wedge 36/12 = (36/48)^2 = (3/4)^2$. 
And when this holds, $(1/2)\norm{\riesz}_{L_2(\Pn)}\sigma^2/(s^2 n)$ exceeds the second term.
This yields the simplified bound 
\begin{align*} 
\rho &\ge  \epsilon^{-1} 6 \sigma/n^{1/2} \vee (1/2) \norm{\riesz}_{L_2(\Pn)} \sigma^2/(s^2 n) \vee  2 \norm{\riesz}_{L_2(\Pn)}^{1/2} \sigma^{3/2}/(s n^{3/4}).
\end{align*}
In our stated bound, we drop the third term.
It is not maximal, as it is smaller than the geometric mean of the first two,
which is $\epsilon^{-1/2} \sqrt{3} \norm{\riesz}_{L_2(\Pn)}^{1/2} \sigma^{3/2}/(s n^{3/4})$
with $\epsilon^{-1/2} \sqrt{3} \ge (4/3)\sqrt{3} \ge 2$.

\end{proof}



\subsection{Putting it all together}
\label{sec:putting-it-all-together}
The assumptions of Lemma~\ref{lemma:consistency-deterministic} imply the assumption of
Lemma~\ref{lemma:weight-similarity} concerning $\excess_{\F}$ with the same values of $\tilde g$ and $\alpha$.
Thus, on the intersection of an event of probability $1-\delta$, on which our noise term bound \eqref{eq:noise-term-deviation} holds,
and an event on which the ratio process bounds \eqref{eq:ratio-process-bounds} hold for some $r > 0$,

\begin{equation}
\label{eq:asymptotic-linearity-abstract}
\begin{aligned}
&\abs{\hpsi - \tilde\psi(m) - \sum_{i=1}^n \gapprox(Z_i)(Y_i - m(Z_i))} \le \delta^{-1/2} n^{-1/2} \norm{v}_{\infty} \norm{\hriesz - \gapprox}_{L_2(\Pn)} \\
&+ \norm{\hm-m}_{\F'_{\rho}}\sqb{1+\frac{2\epsilon}{\sqrt{1-\eta_{\rho}}}}  \phi(\rho) \\
&+ \norm{\hm-m}_{\F'_{\rho}}\sqb{\frac{2\sigma^2}{(1-\eta_{\rho}) n}\cb{\norm{\riesz[\psi]}_{L_2(\Pn)}^2 \wedge \norm{\riesz[\psi]}_{L_2(\Pn)}\norm{\hgamma-\riesz[\psi]}_{L_2(\Pn)}}}^{1/2} \\
&\text{for \quad} \eta_\rho = \sigma^2/(\rho^2 n), \\ 
&\text{\hphantom{for \quad}} \F'_{\rho} = \set{f : \norm{f}_{\F}^2 + \rho^{-2}\norm{f}_{L_2(\Pn)}^2 \le 1}, \\
&\text{\hphantom{for \quad}} \phi(\rho) \ge I_{h,\F_{\rho}'}(\riesz[\psi]) \vee \epsilon^{-2} 72 \alpha_{\phi} \sigma_{\F}^2/n.
\end{aligned}
\end{equation}
Here we've used the bounds \eqref{eq:noise-term-deviation} and \eqref{eq:bias-bound-concrete}
on the noise and bias terms in our error decomposition \eqref{eq:error-decomp},
substituting $2\epsilon/\sqrt{1-\eta_{\rho}} \ge \epsilon + \epsilon'$ into \eqref{eq:bias-bound-concrete}.
It holds, with $\epsilon \le 9/16$ and $\alpha_{\phi} = \alpha \vee 2 \eta_M r_{\phi}^2 n / \sigma^2$ for $\alpha$ as in Lemma~\ref{lemma:consistency-deterministic},
when $\rho$ satisfies the lower bound of Corollary~\ref{coro:weight-similarity-simplified} 
with \smash{$s^2=2\eta_Mr_{\phi}^2$} for $r_{\phi} \ge r$, as these are sufficient conditions for the bound \eqref{eq:bias-bound-concrete} to hold
as a consequence of Lemma~\ref{lemma:abstract-finite-sample} and Corollary~\ref{coro:weight-similarity-simplified}.


To complete our proof of Theorem~\ref{theo:simple-rate}, 
we show in Section~\ref{sec:ratio-process-bounds} that the ratio process bounds \eqref{eq:ratio-process-bounds} are satisfied with high probability,
show in Section~\ref{sec:bounding-I} that a certain function $\phi$ bounds $I_{h,\F_{\rho}'}(\riesz[\psi])$ with high probability,
and \ldots in Section \ldots.
In the first two steps, we will use the assumption that $\F$ is uniformly bounded, giving bounds that depend on 
$M_{\infty}(\F)=\sup_{f\in\F}\norm{f}_{\infty}$. 
After we have concluded our proof, in Section~\ref{sec:not-uniformly-bounded}, we will briefly discuss techniques for relaxing this assumption.

\subsubsection{Ratio Process Bounds}
\label{sec:ratio-process-bounds}
Our first bound in \eqref{eq:ratio-process-bounds}, 
a uniform lower bound on the ratio $\Pn f^2/ \P f^2$, holds under a wide range of conditions. These are summarized in \citet{mendelson2017extending}, where
Corollary 3.6 addresses the uniformly bounded case we consider here. 
It establishes that for any $\eta_Q < 1$,
the bound $\Pn f^2 \ge \eta_Q \P f^2$ holds for all $f \in \F$
satisfying $\P f^2 \ge r^2$ with probability $1-2\exp(-c_2nr^2/M_{\infty}^2(\F))$ if 
$R_n(\F_{c_0r}) \le c_1 r^2/M_{\infty}(\F)$ for constants $c_0,c_1,c_2$ that depend only on $\eta_Q$.
And by a scaling argument of \citet*[Lemmas 3.2, 3.4]{bartlett2005local}, 
there is a unique positive $r_Q$ that satisfies the fixed point condition $R_n(\F_{c_0r}) \le c_1 r^2/M_{\infty}(\F)$ 
with equality, and it is satisfied for all $r \ge r_Q$.


For $g=\gapprox$ when $Q=\P$ and $g = \riesz[\psi]$ when $Q=\Pn$, our second bound in \eqref{eq:ratio-process-bounds} is 
on the supremum of the mean-zero empirical process indexed by the 
image $h_{g}(\cdot, \F_r)$ of $\F_r$ under the function $h_{g}(z,f) = h(z,f) - g(z)f(z)$.
By Markov's inequality, this is bounded by 
$\delta^{-1} \E \sup_{h \in h_{g}(\cdot,\F_r)}\abs{(\Pn-\P) h}$ with probability $1-\delta$.
Furthermore, if we prefer to state our bounds in terms of Rademacher complexities,
via symmetrization this is bounded by $2\delta^{-1}R_n(h_{g}(\cdot,\F_r))$ 
\citep[Lemma 2.3.1]{vandervaart-wellner1996:weak-convergence}. 
By the aforementioned scaling argument, 
there is a unique positive $r_M$ satisfying the fixed point condition $2\delta^{-1}R_n(h_{g}(\cdot,\F_r)) \le \eta_M r^2$
with equality, and it is satisfied for all $r \ge r_M$.

In summary, our ratio process bounds \eqref{eq:ratio-process-bounds} hold 
on an event of probability $1-\delta-2\exp(-c_2nr^2/M_{\infty}^2(\F))$ 
for $r \ge r_Q \vee r_M$ where 
\begin{align*}
r_Q &= \inf\set{ r > 0 : R_n(\F_{c_0r}) \le c_1 r^2/M_{\infty}(\F)}, & \\
r_M &= \begin{cases}
    \inf \set{ r > 0 : R_n(h_{\gapprox}(\cdot,\F_r)\ \le \ \delta \eta_M r^2/2} & \text{ for }\quad Q=\P, \\
    \inf \set{ r > 0 : R_n(h_{\riesz[\psi]}(\cdot,\F_r) \le \delta \eta_M r^2/2} & \text{ for }\quad Q=\Pn. \end{cases}
\end{align*}
Here $\eta_Q \in [0,1)$ and $\eta_M > 0$ are arbitrary
and $c_0 \ldots c_2$ are constants dependening on only on $\eta_Q$.
It follows that for such $r$,
the bound \eqref{eq:asymptotic-linearity-abstract} holds with probability $1-2\delta-2\exp(-c_2nr_Q^2/M_{\infty}^2(\F))$
for all $\rho$ satisfying the lower bound from Corollary~\ref{coro:weight-similarity-simplified}.

\subsubsection{Bounding $I_{h,\F_{\rho}'}(\riesz[\psi])$} 
\label{sec:bounding-I}

To bound $I_{h,\F'_{\rho}}(\riesz[\psi])$, we first observe that it is smaller than $I_{h,\GG}(\riesz[\psi])$ 
for any $\GG \supseteq \F'_{\rho}$. As $\F'_{\rho} \subseteq \set{ f \in \F : \Pn f^2 \le \rho^2}$, 
it is contained in $\F_{\sqrt{2}\rho} = \set{ f \in \F : \P f^2 \le 2\rho^2}$ 
for $\rho^2 \ge 20M_{\infty}(\F)R_n(\F_{\rho}) + 26M_{\infty}^2(\F)\log(1/\delta')/n$ on an event of probability $1-\delta'$ \citep[Lemma 3.6]{bartlett2005local}.
Setting $\log(1/\delta') = (20/26)nR_n(\F_{\rho})/M_{\infty}(\F)$, the two terms in this lower bound on $\rho^2$ are equal,
so this containment holds for $\rho^2 \ge 40M_{\infty}(\F)R_n(\F_{\rho})$
on an event of probability $1-\exp\set{- (20/26)nR_n(\F_{\rho})/M_{\infty}(\F)}$.
Using a constant $c_1 \le 1/40$ in the fixed point condition $R_n(\F_{c_0r}) \le c_1 r^2/M_{\infty}(\F)$ 
in the previous section, this condition on $\rho$ is satisfied for $\rho \ge c_0 r_Q$, and for such $\rho$,
$R_n(\F_{\rho}) \ge R_n(\F_{c_0 r_Q}) = c_1 r_Q^2 / M_{\infty}(\F)$, so the probability of this event is
at least \smash{$1-\exp\set{- (20/26)nc_1 r_Q^2/M_{\infty}^2(\F)}$}. By the union bound, it follows
that this containment and 
\eqref{eq:asymptotic-linearity-abstract} hold on an event of probability \smash{$1-2\delta-3\exp(-c_2nr_Q^2/M_{\infty}^2(\F))$},
taking $c_2$ to be no larger than $(20/26)c_1$.



On the intersection of this event and the probability $1-\delta$ event on which Markov's inequality implies 
$I_{h,\F_{\sqrt{2}\rho}}(\riesz[\psi]) \le \delta^{-1}\E I_{h,\F_{\sqrt{2}\rho}}(\riesz[\psi])$,
it follows that $I_{h,\F'_{\rho}}(\riesz[\psi]) \le  \delta^{-1}\E I_{h,\F_{\sqrt{2}\rho}}(\riesz[\psi])$
and a variant of \eqref{eq:asymptotic-linearity-abstract} in which 
$I_{h,\F'_{\rho}}(\riesz[\psi])$ is replaced with the upper bound \smash{$\delta^{-1}\E I_{h,\F_{\sqrt{2}\rho}}(\riesz[\psi])$} holds
 for $\rho$ equal to or exceeding both $c_0 r_Q$ and the lower bound from Corollary~\ref{coro:weight-similarity-simplified}.
We use a deterministic variant of the latter in which $\norm{\riesz[\psi]}_{L_2(\Pn)}$ is replaced with the probability $1-\delta$
Markov's inequality bound \smash{$\delta^{-1/2} \norm{\riesz[\psi]}_{L_2(\P)}$.} Recalling that 
we take $s^2=2\eta_M r_{\phi}^2$ in Corollary~\ref{coro:weight-similarity-simplified}, our bound on $\rho$ is
\begin{equation}
\label{eq:rho-lb}
\rho \ge  c_0 r_Q \vee \frac{6 \epsilon^{-1} \sigma}{n^{1/2}} 
     \vee \frac{\norm{\riesz[\psi]}_{L_2(\P)} \sigma^2}{4\delta^{1/2} \eta_M r_\phi^2 n}.
\end{equation}
The intersection of these events has probability at least
$1-4\delta-3\exp(-c_2nr_Q^2/M_{\infty}^2(\F))$ by the union bound.


\subsubsection{A concrete bound}
We state a bound summarizing the results above. Let
$\gapprox = \argmin_{g} \norm{\riesz[\psi] - g}_{L_2(Q)}^2 + (\sigma^2/n)\norm{g}_{\F}^2$. With probability $1-4\delta-3\exp(-c_2nr_Q^2/M_{\infty}^2(\F))$, 
\begin{equation}
\label{eq:asymptotic-linearity-concrete}
\begin{aligned}
&\norm{\hriesz - \gapprox}_{L_2(\Pn)}^2 \le 2\alpha r^2 \quad \text{ for } \alpha = 3\p{\norm{\gapprox}_{\ff} + r^2 n/\sigma^2} \vee 4, \\
&\sqrt{n}\abs{\hpsi - \tilde\psi(m) - \sum_{i=1}^n \gapprox(Z_i)(Y_i - m(Z_i))} \le \delta^{-1/2} \norm{v}_{\infty} \norm{\hriesz - \gapprox}_{L_2(\Pn)} \\
&+ \norm{\hm-m}_{\F'_{\rho}}\sqrt{n}\phi(\rho) \sqb{1+\frac{2\epsilon}{\sqrt{1-\epsilon^2/36}}}  \\
&+ \norm{\hm-m}_{\F'_{\rho}}\sqb{\frac{2\sigma^2}{\set{1-\epsilon^2/36}}\cb{\norm{\riesz[\psi]}_{L_2(\Pn)}^2 \wedge \norm{\riesz[\psi]}_{L_2(\Pn)}\norm{\hgamma-\riesz[\psi]}_{L_2(\Pn)}}}^{1/2}, \\
& \text{ \quad with \quad }            r = r_Q \vee r_M, \\
& \text{\hphantom{ \quad with \quad }} r_Q = \inf\set{ r > 0 : R_n(\F_{c_0r}) \le c_1 r^2/M_{\infty}(\F)}, \\
& \text{\hphantom{ \quad with \quad }} r_M = \begin{cases} \inf\set{ r > 0 : R_n(h_{\gapprox}(\cdot,\F_r)  \ \le \ \delta r^2/2} & \text{ for } \quad Q=\P,  \\
							   \inf\set{ r > 0 : R_n(h_{\riesz[\psi]}(\cdot,\F_r) \le \delta r^2/2} & \text{ for } \quad Q=\Pn, \end{cases}  \\
& \text{\hphantom{ \quad with \quad }} \phi(\rho) = \delta^{-1}\E I_{h,\F_{\sqrt{2}\rho}}(\riesz[\psi]) \vee \epsilon^{-2} 72 \alpha_{\phi} \sigma^2/n, \\
& \text{\hphantom{ \quad with \quad }} \alpha_{\phi} = \alpha \vee 2nr_{\phi}^2/\sigma^2 \quad \text{ for any } \quad r_{\phi} \ge r \\
& \text{ \quad for any \quad } \rho \ge c_0 r_Q \vee \frac{6 \sigma}{\epsilon \sqrt{n}} 
     \vee \frac{\norm{\riesz[\psi]}_{L_2(\P)} \sigma^2}{4\sqrt{\delta}n r_\phi^2}. 
\end{aligned}
\end{equation}
Here $h_{\gamma}(z,f) = h(z,f) - \gamma(z)f(z)$, $c_0 \ldots c_2$ are universal constants, and $\epsilon \le 9/16$.
To derive this bound, we have taken $\eta_Q=1/2$ and $\eta_M=1$, 
used Lemma~\ref{lemma:abstract-finite-sample} to bound \smash{$\norm{\hriesz - \gapprox}_{L_2(\Pn)}$},
and substituted into \eqref{eq:asymptotic-linearity-abstract} the bounds discussed in the subsections above,
as well as the bound \smash{$\epsilon^2/36 \ge \eta_{\rho}$} implied by the condition $\rho \ge 6 \epsilon^{-1} \sigma / n^{1/2}$. 


To simplify our lower bound on $\rho$, we set \smash{$r_\phi^2 = \norm{\riesz[\psi]}_{L_2(\P)} \sigma^2 / (4c_0\sqrt{\delta}n r)$}
to equate $c_0 r$ and \smash{$\norm{\riesz[\psi]}_{L_2(\P)} \sigma^2 / (4\sqrt{\delta}n r_\phi^2)$}. Taking $c_0 \ge 1$, 
this satisfies our assumption $r \ge r_{\phi}$, and by design our lower bound on $\rho$ simplifies 
to $c_0 r \vee  6 \sigma/(\epsilon \sqrt{n})$. For this $r_{\phi}$,
\smash{$\alpha_{\phi}= 3\p{\norm{\gapprox}_{\ff} + r^2 n/\sigma^2} \vee $} \smash{$ \norm{\riesz[\psi]}_{L_2(\P)} / (2c_0\sqrt{\delta} r) \vee 4$},
so the bound above holds for \smash{$\rho \ge c_0 r \vee  6 \sigma/(\epsilon \sqrt{n})$} and
\[
\phi(\rho) = \delta^{-1}\E I_{h,\F_{\sqrt{2}\rho}}(\riesz[\psi]) 
	  \vee \frac{216}{\epsilon^2}\p{\frac{\norm{\gapprox}_{\ff}\sigma^2}{n} + r^2} 
          \vee \frac{36\norm{\riesz[\psi]}_{L_2(\P)}\sigma^2}{\epsilon^2\sqrt{\delta} c_0 nr} \vee \frac{288\sigma^2}{\epsilon^2 n}. 
\]
In our definition of $\phi(\rho)$ in Theorem~\ref{theo:simple-rate}, we substitute the bound \smash{$2R_n(h_{\riesz[\psi]}(\cdot, \F_{\sqrt{2}\rho}))\ge$} \smash{$\E I_{h,\F_{\sqrt{2}\rho}}(\riesz[\psi])$} implied by symmetrization \citep[Lemma 2.3.1]{vandervaart-wellner1996:weak-convergence}.

\paragraph*{Approximately optimizing over $\rho$}
Rather than including $\rho$ explicitly in our bound, 
we approximately optimize over $\rho$ exceeding the lower bound above, which we will call $\rho_{\phi}$.
To do this, we will work with bounds $\norm{\hm - m}_{\F} \le s_{\F}$ and $\norm{\hm-m}_{L_2(\Pn)} \le s_{L_2(\Pn)}$.
Subject to the additional constraint \smash{$\rho \ge s_{L_2(\Pn)}/s_{\F}$}, 
we increase our bound by substituting \smash{$\sqrt{2}s_{\F}$} for $\norm{\hm - m}_{\F_{\rho}'}$, as
\[ \norm{\hm - m}_{\F'_{\rho}}^2 = \norm{\hm - m}_{\F}^2 + \rho^{-2}\norm{\hm - m}_{L_2(\Pn)}^2 
			         \le s_{\F}^2\p{1+ \rho^{-2}s_{L_2(\Pn)}^2/s_{\F}^2}.\]
Thus, our bound \eqref{eq:asymptotic-linearity-concrete} holds for
\smash{$\rho = \rho_{\phi} \vee (s_{L_2(\Pn)}/s_{\F})$}, 
and for this $\rho$, \smash{$\sqrt{2}s_{\F} \ge \norm{\hm - m}_{\F'_{\rho}}$.}
Making these substitutions yields the claim of Theorem~\ref{theo:simple-rate}. 

\subsection{Doing without uniform boundedness}
\label{sec:not-uniformly-bounded}
In Section~\ref{sec:ratio-process-bounds},
we show that the ratio process bounds \eqref{eq:ratio-process-bounds}
hold with high probability when $\F$ is uniformly bounded.
Lower bounds on the ratio process $\Pn f^2 / \P f^2$, like 
our first bound in \eqref{eq:ratio-process-bounds},
hold for classes with $M_p(\F) = \sup_{f \in \F} \norm{f}_{L_p(\P)}$ finite
for $p > 2$. In this case, the fixed point condition determining $r$ is $R_n(\F_{c_0r}) \le c_1 r (r /M_{p}(\F))^{p/(p-2)}$ 
\citep[Corollary 3.6]{mendelson2017extending}. 
The approach we use to establish the second bound in \eqref{eq:ratio-process-bounds}
is based on Markov's inequality and holds without uniform boundedness.
However, if it were known that the class $h_{g}(\cdot,\F_r)$ were uniformly bounded or otherwise had well behaved tails,
a sharper concentration inequality like Talagrand's \citep[e.g.,][Theorem 3.3.9]{gine2015mathematical}
could be used to establish bounds that do not depend strongly on the tail probability $\delta$.

In Section~\ref{sec:bounding-I}, we bound the supremum of the mean-zero empirical process
$(\Pn - \P)h_{\riesz[\psi]}f$ indexed by the random set $\F'_{\rho} \subseteq \set{f \in \F : \Pn f^2 \le \rho^2}$. 
Our approach is based on showing that with high probability, $\F'_{\rho}$ is contained in the 
deterministic set $\set {f \in \F : \P f^2 \le 2\rho^2}$,
and involves the use of bounds based on contraction principle arguments 
that do not generalize well to the unbounded case. In the unbounded case,
it is probably more natural to work with the random set $\F'_{\rho}$ directly, 
for example by using symmetrization to introduce Rademacher multipliers 
and analyzing the resulting Rademacher average conditional on $Z_1 \ldots Z_n$ using
bounds on $L_2(\Pn)$ metric entropy \citep[see e.g.,][Theorem 3.5.1]{gine2015mathematical}.



\section{Asymptotics}
\label{sec:asymptotics}

We will now prove our simple asymptotic result, Theorem~\ref{theo:simple}, using Theorem~\ref{theo:simple-rate} for $Q=\Pn$.
Our assumptions that $\F$ is pointwise closed and therefore $L_2(\Pn)$-closed, that $h(Z,f)$ is pointwise bounded, 
and that $f(Z)$ is uniformly bounded justify the application of the latter.
The following lemma will be used to show that our Rademacher complexity fixed points are $o(n^{-1/4})$.
\begin{lemm}
\label{lemma:fixed-point-fourth-root}
Let $\tau_n(r)$ be a sequence of positive functions, each increasing in $r$, and satisfying $\tau_n(s_n) = o(n^{-1/2})$
for all positive sequences $s_n \to 0$. For any $\eta > 0$, there exists a positive sequence $r_n$ satisfying $r_n = o(n^{-1/4})$ and $\tau_n(r_n) \le \eta r_n^2$ for sufficiently large $n$.
\end{lemm}
\begin{proof}
Let $r_n = \sqrt{\tau_n(n^{-1/4})/\eta}$. Then $r_n = o(n^{-1/4})$ and $\tau(r_n) \le \eta r_n^2 = \tau(n^{-1/4})$ for $n$ sufficiently large that $r_n \le n^{-1/4}$.
\end{proof}


\begin{proof}[Proof of Theorem~\ref{theo:simple}]
We will prove asymptotic linearity \eqref{eq:asymptotic-linearity} here, deferring our claims about regularity  
and efficiency to Section~\ref{sec:proof-of-efficiency} below. 
We begin by showing that $\sqrt{n}R_n(\chi(\F) \cap \omega_{\chi}(r_n)B) \to 0$ 
whenever $r_n \to 0$ for $\chi \in \set{ f \to f,\ f \to \riesz[\psi] f,\ f \to h(\cdot, f) }$
and $\omega_{\chi}(r) = \sup_{f \in \F \cap r B} \norm{f}_{L_2(\P)}$. 

Because each set $\chi(\F)$ is Donsker, the
corresponding Rademacher processes are asymptotically equicontinuous \citep[e.g.,][Theorem 14.6]{ledoux1991probability}
in the sense that $\sqrt{n}R_n(\chi(\F) \cap s_n B) \to 0$ whenever $s_n \to 0$.
Thus,  $\sqrt{n}R_n(\chi(\F) \cap \omega_{\chi}(r_n)B) \to 0$ whenever $r_n \to 0$ 
if $\lim_{r \to 0}\omega_{\chi}(r) = 0$. For $\chi(f)=f$, this holds tautologically; for $\chi(f)=h(\cdot,f)$, this is assumed;
and for $\chi(f)=\riesz[\psi]f$, this follows from the uniform boundedness of $\F$ and square integrability of $\riesz[\psi]$
via a truncation argument: if $\P f^2 \le r^2$,
\[ \P \riesz[\psi]^2 f^2 
    = \P \riesz[\psi]^2 1(\riesz[\psi]^2 \le 1/r) f^2 +
      \P \riesz[\psi]^2 1(\riesz[\psi]^2 > 1/r) f^2 
    \le r + \norm{f}_{\infty} \P \riesz[\psi]^2 1(\riesz[\psi]^2 > 1/r), \]
and this goes to zero as $r \to 0$. And this implies that 
$R_n(h_{\riesz[\psi]}(\cdot, \F \cap r_n B)) \to 0$ as $r_n \to 0$,
as $R_n(h_{\riesz[\psi]}(\cdot, \GG)) \le R_n(h(\cdot,\GG)) + R_n(\riesz[\psi]\GG)$ for any set $\GG$.
Thus, on an event of arbitarily high probability,
$r = o(n^{-1/4})$ via Lemma~\ref{lemma:fixed-point-fourth-root} and $\sqrt{n}\phi(s_n) \to 0$ for any $s_n \to 0$. 
The remainder of our proof is based on these two rates.


As a consequence of our assumed tightness and consistency properties \eqref{eq:consistency-properties},
to establish the asymptotic linearity property 
\[ \sqrt{n}(\hpsi_{AML} - \psi(m) - n^{-1}\sum_{i=1}^n \influence_{\tilde\riesz}(Y_i,Z_i)) \to_P 0, \]
it suffices to show that for any $\delta > 0$, the three-term remainder bound \eqref{eq:remainder-rate} goes to zero
for any constant $s_{\F}$ and with any sequence $s_n \to 0$ in place of $s_{L_2(\Pn)}$.
\begin{enumerate}
\item The first term of our bound goes to zero if $\norm{\hriesz - \tilde\riesz}_{L_2(\Pn)}$ does.
This happens because $\sqrt{n}r^2 \to 0$ and $\norm{\tilde\riesz}_{\F} \le (\sqrt{n}/\sigma)\norm{\riesz[\psi]}_{L_2(\Pn)} = O_P(\sqrt{n})$.
The latter bound holds because 
\[ \norm{\riesz[\psi] - \riesz}_{L_2(\Pn)}^2 + (\sigma^2/n)\norm{\riesz}_\F^2 \ \text{ is smaller at is minimizer }\ 
\riesz=\tilde\riesz \ \text{ than at }\ \riesz=0. \]
\item The second term goes to zero because $\sqrt{n}\phi(s_n) \to 0$ when $s_n \to 0$.
\item The third term goes to zero if \smash{$\norm{\hriesz - \riesz[\psi]}_{L_2(\Pn)}$} does.
By the triangle inequality, this happens if both $\norm{\hriesz - \tilde\riesz}_{L_2(\Pn)}$
and $\norm{\tilde\riesz - \riesz[\psi]}_{L_2(\Pn)}$ do. We have established that the first does.
To show that the second does, observe that there is a sequence of approximations $\tilde\riesz_j \in \vspan \F$ 
converging to any element in its closure, and therefore to $\riesz[\psi]$, 
and it has a convergent subsequence $\tilde\riesz_{j_n}$ satisfying $\norm{\tilde\riesz_{j_n}}_{\F}/\sqrt{n} \to 0$.
It follows that $\norm{\tilde\riesz - \riesz[\psi]}_{L_2(\Pn)} \to 0$ on an event of probability $1-\delta$, as 
\begin{align*} 
    \norm{\tilde\riesz - \riesz[\psi]}_{L_2(\Pn)}^2 + (\sigma^2/n)\norm{\tilde\riesz}_\F^2 
&\le \norm{\tilde\riesz_{j_n} - \riesz[\psi]}_{L_2(\Pn)}^2 + (\sigma^2/n)\norm{\tilde\riesz_{j_n}}_\F^2 \\
&\le \delta^{-1}\norm{\tilde\riesz_{j_n} - \riesz[\psi]}_{L_2(\P)}^2 + (\sigma^2/n)\norm{\tilde\riesz_{j_n}}_\F^2 \to 0
\end{align*} 
Our first comparison is via the optimality of $\tilde\riesz$ and our second on Markov's inequality.
\end{enumerate}
This establishes asymptotic linearity in the sense stated above.
The form of asymptotic linearity we want to prove \eqref{eq:asymptotic-linearity} 
differs in that it has $\influence_{\riesz[\psi]}$ in place of $\influence_{\tilde\riesz}$.
By the triangle inequality, these are equivalent if $\sqrt{n}\Pn( \influence_{\riesz[\psi]} - \influence_{\tilde\riesz}) = o_p(1)$.
And as $\Pn( \influence_{\riesz[\psi]} - \influence_{\tilde\riesz}) = \Pn (\riesz[\psi] - \tilde \riesz)\varepsilon_i$ for $\varepsilon_i = Y_i- m(Z_i)$,
via Chebyshev's inequality as in the derivation of our noise term bound in Section~\ref{sec:convergence-of-the-noise-term},
this goes to zero because $\norm{\riesz[\psi] - \tilde \riesz}_{L_2(\Pn)} \to 0$.
\end{proof}

\subsection{Theorem~\ref{theo:simple-hull}}
\label{sec:appendix-flexible}
We turn our focus to Theorem~\ref{theo:simple-hull}, a variant of the theorem proven above
in which $\F$ is defined as the absolutely convex hull of $\set{m_1 \ldots m_{K_n}} - \GG$ 
for a Donsker class $\GG$. Our claim that this theorem justifies the use of $K=o(n^{1/(2+\alpha)})$ candidates
in ideal conditions, for example when $\sup_{n} \norm{\chi(\F_n)}_{\infty} < \infty$ and $\omega_{\chi,\F_n}(r) \lesssim r$ for all $\chi$
and $\norm{\hm - m}_{L_2(\Pn)} =$ \smash{$ O_p(n^{-1/4})$}, follows from a straightforward covering number bound.

For large $K$, when $\log \hat N(\HH,\tau) \le \tau^{-\alpha}$ for $\alpha < 2$,
\[ \hat N(\HH,\log(K+1)^{-1/2}) \le \exp(\log(K+1)^{\alpha/2}) \le \exp((\alpha/2)\log(K+1)) = (K+1)^{\alpha/2}. \]
In the second comparison, we've used the property that for $a=\log(K+1)$ and $b=\alpha/2 < 1$,
$a^b \le ab \iff a^{b-1} \le b$, and $a^{b-1} \to 0$ as $a=\log(K+1) \to \infty$ whereas $b$ remains constant.
Thus, if we could take $a_n=1$ in condition $(2c)$, it would suffice that $(K_n+1)^{1+\alpha/2} = o(n^{1/2})$,
which would imply our claim. Modification for $a_n \to 0$ slowly is straightforward. 


We will now prove Theorem~\ref{theo:simple-hull} and a related claim from Remark~\ref{rema:hull-k-constant}.
Throughout, we will write $rB$ and $r\hat B$ for the radius-$r$ balls in $L_2(\P)$ and $L_2(\Pn)$
and $R_n(\HH)$ and \smash{$\hat R_n(\HH)$} for $\E \sup_{h \in \HH}\abs{\Pn \varepsilon_i h(Z_i) }$ 
and $\E_{\varepsilon} \sup_{h \in \HH}\abs{\Pn \varepsilon_i h(Z_i)}$, with the latter
denoting expectation conditional on $Z_1 \ldots Z_n$. 
Here $\varepsilon_1 \ldots \varepsilon_n$ is a sequence of independent Rademacher random variables independent of $Z_1 \ldots Z_n$,
and we will write $G_n$ and $\hat G_n$ for analogs of $R_n$ and $\hat R_n$ in which 
a sequence of standard normals $\xi_1 \ldots \xi_n$ replaces the Rademacher sequence.
The lemmas below, which we will use in our proof, will be proven afterward.

\begin{lemm}
\label{lemma:empirical-vs-pop-local-complexity}
Let $\F \subseteq L_2(\P)$ be star-shaped around zero, with finite $M_p := \sup_{f \in \F} \norm{f}_{L_p(P)}$ for $p \in (2,\infty]$,
let $\omega(r)$ be a non-decreasing function on the positive reals, and for any $r_L \ge 0$, let 
\begin{align*}
&r_{\star} = \inf\set{ r > r_L : R_n(\F \cap \omega'(r) B) \le \eta r^2} && \text{ and } \\
&\hat r_{\star} = \inf \set{ r > r_L : \hat R_n(\F \cap \omega'(r) \hat B) \le \delta \eta r^2/2} && \text{ for } \\
&\omega'(r) = \omega(r) \vee c_0 r^{2/(1+q)}, \ \ q=p/(p-2), \ \ \eta < c_1/M_p^q.
\end{align*} 
On an event of probability $1-\delta - 2\exp(-c_2 n r_\star^{4q/(1+q)}/M_p^{2q})$, $\hat r_{\star} \ge r_{\star}$.
Here $c_0$ is a universal constant and $c_1,c_2$ depend only on $p$.
\end{lemm}

\begin{lemm}
\label{lemma:non-increasing-modulus}
Let $\F$ be a subset of a space with norm $\norm{\cdot}$ that is star-shaped around zero and $\chi$ be a linear map from $\F$ into a space with norm $\norm{\cdot}'$. For the continuity modulus $\omega(r) = \sup_{f \in \F : \norm{f} \le r} \norm{\chi(g)}'$, $\omega(r)/r$ is nonincreasing. 
\end{lemm}

\begin{coro}
\label{cor:empirical-vs-pop-local-complexity}
Let $\F_n \subseteq L_2(\P)$ be a sequence of sets, each star-shaped around zero, 
let $\chi$ be a linear map from $\cup_{n}\F_n \to L_2(\P)$ 
with $\sup_n \sup_{f \in \ff_n} \norm{f}_{L_p(\P)} < \infty$ for $p \in (2,\infty]$,
and let 
\[ \omega_n'(r) = \sup_{f \in \F \cap r B} \norm{\chi(f)}_{L_2(\P)} \vee c_0 r^{(p-2)/(p-1)}
\ \text{ for a universal constant }\ c_0.
 \]
Let $\eta > 0$ be a constant and $r_n$ and $r_n'$ be deterministic sequences with $r_n' \ge r_n$.
If $\hat R_n(\chi(\F_n) \cap \omega'(\hat r) \hat B) \le (\eta/4) \hat r^2$ with $\hat r = o_P(r_n)$,
then $R_n(\chi(\F) \cap \omega'(r)B) \le \eta r^2$ with $r=o(r_n)$ 
and furthermore $R_n(\chi(\F) \cap \omega'(r_n')B) = O_P( \hat R_n(\chi(\F) \cap \omega'(r_n') \hat B))$.
\end{coro}

\begin{lemm}
\label{lemma:gaussian-comparison-convex-hull}
Let $\F$ be the absolutely convex hull of $\set{m_1 \ldots m_{K}} - \GG$. For any $r, s > 0$,
\[ \hat G_n\p{\F \cap r \hat B} \le 2 \hat G_n\p{[\GG - \GG] \cap s \hat B} + cn^{-1/2} s \sqrt{\log(K+1)} +  n^{-1/2} r \sqrt{(K+1)\hat N(\GG, s)}. \]
Here $c$ is a universal constant and $\hat N(\GG, s)$ is the minimal size of a cover of $\GG$ by $\norm{\cdot}_{L_2(\Pn)}$-balls of radius $s$.
\end{lemm}

\begin{proof}[Proof of Theorem~\ref{theo:simple-hull} and Remark~\ref{rema:hull-k-constant}]
As in the proof of Theorem~\ref{theo:simple} above, 
it suffices to show two rate bounds: 
$R_n(\chi(\F) \cap \omega_{\chi}(r)B) \le \eta r^2$ with $r=o(n^{-1/4})$ for arbitarily small $\eta > 0$
and $\sqrt{n}R_n(\chi(\F) \cap \omega_{\chi}(s_n)B) \to 0$ for all $\chi$
and $\norm{\hm - m}_{L_2(\Pn)} = O_p(s_n)$. 
And by Corollary~\ref{cor:empirical-vs-pop-local-complexity} for $r_n = n^{-1/4}$ and $r_n'=n^{-1/4} \vee s_n$,
it suffices that\footnotemark 
\begin{align}
&\hat R_n(\chi(\F) \omega_{\chi}'(\hat r)\hat B) \le (\eta/4) \hat r^2 \quad \text{ with } \quad \hat r = o_p(n^{-1/4}), \label{eq:hull-fixed-pt}\\
&\sqrt{n}\hat R_n(\chi(\F) \cap \omega_{\chi}'(s_n \vee n^{-1/4})\hat B) = o_P(1). \label{eq:hull-localized}
\end{align}
\footnotetext{In the statement of Theorem~\ref{theo:simple-hull}, we use a simplified definition of $\omega_{\chi'}$ in which the universal constant
$c_0$ from \eqref{cor:empirical-vs-pop-local-complexity} is taken to be one. This does not affect our proof,
which depends on the order of $\omega_{\chi'}(r)$ but not constant factors.}



By a contraction principle for Rademacher averages \citep[Lemma 4.5]{ledoux1991probability}, 
we can bound each Rademacher complexity by a multiple of the analogous Gaussian complexity:
\[ \sqrt{n}\hat R_n\p{\chi(\F) \cap \omega'_{\chi}(r) \hat B} \le (\pi/2)^{1/2} \sqrt{n}\hat G_n\p{\chi(\F) \cap \omega'_{\chi}(r) \hat B}. \] 
And as  $\chi(\F)$ is the absolutely convex hull of $\set{\chi(m_1) \ldots \chi(m_K)} - \chi(\GG)$,
by Lemma~\ref{lemma:gaussian-comparison-convex-hull}, 
\begin{align*} \sqrt{n}\hat G_n\p{\chi(\F) \cap \omega_{\chi}'(r) \hat B)} 
&\le 2\sqrt{n}\hat G_n\p{[\chi(\GG)-\chi(\GG)] \cap s \hat B} 
+ c s \sqrt{\log(K+1)}  \\
&+ \omega_{\chi}'(r) \sqrt{(K+1)\hat N(\chi(\GG), s)} \quad \text{ for any } s. 
\end{align*}
 The first term in this bound is $o_P(1)$ when $s \to 0$ as $n \to \infty$, 
as (i) $\HH := \chi(\GG) - \chi(\GG)$ is Donsker when $\chi(\GG)$ is Donsker with $\sup_{f \in \chi(\GG)}\abs{\P f} < \infty$ \citep[Example 2.10.7]{vandervaart-wellner1996:weak-convergence},
(ii) $\sqrt{n}G_n(\HH \cap t B) \to 0$ 
when $t \to 0$  for Donsker $\HH$ \citep[e.g.,][Theorem 14.6]{ledoux1991probability},
(iii) $\hat G_n(\HH \cap t B) = O_P(G_n(\HH \cap t B))$ by Markov's inequality,
(iv) for any $s \to 0$, there exists $t \to 0$ for which
    $\HH \cap s \hat B \subseteq \HH \cap t B$ with arbitrarily high probability. This last property
holds because $\HH^2 = \set{h^2 : h \in \HH}$ is Glivenko-Cantelli when $\HH$ is Donsker with $\sup_{h\in\HH}\abs{\P h} < \infty$ \citep[Lemma 2.10.14]{vandervaart-wellner1996:weak-convergence}, so $\sup_{h \in \HH \cap s \hat B} \P h^2 \le \sup_{h \in \HH \cap s \hat B} \Pn h^2 + \sup_{h \in \HH}\abs{(\P - \Pn)h^2} \le s^2 + o_P(1)$.
Thus, taking $s=a_n/\sqrt{\log(K+1)}$ with $a_n \to 0$,
\[  \sqrt{n}\hat R_n\p{\chi(\F) \cap \omega'_{\chi}(r) B} = o_P(1) + O_P\p{\omega_{\chi}'(r) \sqrt{(K+1)\hat N(\chi(\GG), a_n/\sqrt{\log(K+1)})}}. \]
We will use this bound to check the aforementioned sufficient conditions. 

For condition \eqref{eq:hull-fixed-pt}, it suffices
that this bound is less than $\sqrt{n}(\eta/4) r^2$ for $r=o_P(n^{-1/4})$. This happens if each term satisfies this condition individually,
and the leading $o_P(1)$ term does, so this condition reduces to
\[ \sqrt{(K+1)\hat N(\chi(\GG), a_n/\sqrt{\log(K+1)})} \le \sqrt{n}(\eta/4) r^2 /\omega_{\chi}'(r)\ \text{ for }\ r=n^{-1/4}a_n',\ a_n' = o_P(1). \]
And as $\omega_{\chi}'$ is increasing, it suffices that
\[ \sqrt{(K+1)\hat N(\chi(\GG), a_n/\sqrt{\log(K+1)})} \le \sqrt{n}(\eta/4) r^2 / \omega_{\chi}'(n^{-1/4}) 
							= (\eta/4)(a_n')^2 / \omega_{\chi}'(n^{-1/4}) \]
or equivalently that
\[ (K+1)\hat N(\chi(\GG), a_n/\sqrt{\log(K+1)}) = o_P(1/\omega_{\chi}'(n^{-1/4})^2). \]
For condition \eqref{eq:hull-localized}, taking $r=n^{-1/4} \vee s_n$  in our bound above,
it suffices that a variant of this condition holds with $\omega_{\chi}'(n^{-1/4} \vee s_n)$ 
in place of $\omega_{\chi}'(n^{-1/4})$, so our assumption involving \smash{$n^{-1/4} \vee s_n$} implies both 
conditions \eqref{eq:hull-fixed-pt} and \eqref{eq:hull-localized}. 
This concludes our proof of Theorem~\ref{theo:simple-hull}.
 

We will now prove our claim from Remark~\ref{rema:hull-k-constant}. For $K=O(1)$, 
the condition above reduces to \smash{$\omega_{\chi}'(s_n \vee n^{-1/4}) \to 0$}, as by taking $a_n \to 0$ slowly we can take \smash{$\hat N(\chi(\GG),a_n) \to \infty$} arbitrarily slowly ---
in particular, slowly enough that \smash{$\hat N(\chi(\GG),a_n) = o_P(b_n)$} for any sequence $b_n \to \infty$.
That the growth of \smash{$\hat N(\chi(\GG), a_n)$} is bounded as a function of $a_n$ 
in this sense is implied by Sudakov minoration \citep[e.g.,][Theorem 3.18]{ledoux1991probability}:
\[ \sqrt{\log \hat N(\chi(\GG), s)} = O(s^{-1}\hat G_n(\chi(\GG)) = O_P(s^{-1}n^{-1/2}), \]
where in the second comparison we've used the tightness of $\sqrt{n}G_n(\HH)$ for Donsker $\HH$.
And it suffices to assume this condition
on $\omega_{\chi}$ for $\chi(f)=h(\cdot,f)$ only ---
this is clearly satisfied for $\chi(f)=f$, and it was shown that it is satisfied for $\chi(f)=\riesz[\psi]f$
when $\sup_n\sup_{f \in \ff_n}\norm{f}_{\infty} < \infty$ in the proof of Theorem~\ref{theo:simple}.
\end{proof}

\begin{proof}[Proof of Lemma~\ref{lemma:empirical-vs-pop-local-complexity}]
Our proof is based on that of Theorem 4.1 in \citet{bartlett2005local}.
By \citet[Corollary 3.6]{mendelson2017extending}, if $c_1^{-1} M_p^q R_n(\F \cap c_0 s B) \le s^{1+q}$,
then with probability $1-2\exp(-c_2n (s/M_p)^{2q})$,
$\Pn f^2 \ge (1/4)\P f^2$ for all $f \in \F$ with $\P f^2 \ge s^2$. We can assume $c_0 \ge 1$ here,
as if it is true for $c_0 < 1$ it remains true for $c_0=1$.
Taking $s=r^{2/(1+q)}$, if $c_1^{-1} M_p^q R_n(\F \cap \omega''(r) B) \le r^2$
for $\omega''(r)=c_0 r^{2/(1+q)}$, then on an event of probability $1-2\exp(-c_2 n  r^{4q/(1+q)}/M_p^{2q})$,
$\Pn f^2 \ge (1/4)\P f^2$ for all $f \in \F$ with $\P f^2 \ge r^{4/(1+q)}$. And on this event, 
$\F \cap 2s\hat B \supseteq \F \cap sB$ for all $s \ge r^{2/(1+q)}$. 
Furthermore, $R_n(\F \cap sB) < \delta^{-1}\hat R_n(\F \cap s B)$ with probability $1-\delta$
by Markov's inequality. And by the union bound, 
both hold on an event of probability $1-\delta - 2\exp(-c_2 n r^{4q/(1+q)}/M_p^{2q})$.
For the remainder of our argument, we work on this event, and let 
$\psi_0(r) := c_1^{-1} M_p^q R_n(\F \cap \omega'(r) B)$ for $\omega'(r) = \omega(r) \vee \omega''(r)$.
If $r^2 \ge \psi_0(r)$, because $\omega''(r) \ge r^{2/(1+q)}$,
\begin{align*} 
\psi(r) &:= \eta^{-1} R_n(\F \cap \omega''(r)B) \\
&\le  (\delta \eta)^{-1} \hat R_n(\F \cap \omega''(r) B) \\
&\le  (\delta \eta)^{-1} \hat R_n(\F \cap 2 \omega''(r) \hat B)  \\
&\le  \hat\psi(r) := 2(\delta \eta)^{-1} \hat R_n(\F \cap \omega''(r) \hat B).
\end{align*}
It follows that the bound $\psi(r) \le \hat \psi(r)$ holds for any $r$ satisfying $\psi(r) \le r^2$, 
as $\psi_0(r) \le \kappa \psi(r)$ for $\kappa = (c_1^{-1} M_p^q)/\eta^{-1} < 1$. Now suppose that $\psi(r)/r$ is non-increasing --- we will show this below.
This means that this bound holds for any $r > r_{\star}$, as if $\psi(r)/r \le r$ then $\psi(r_+)/r_+ \le r_+$
for all $r_+ \ge r$. Furthermore, our bound above holds for 
$r=r_-$ slightly smaller than $r_{\star}$, as for $r=r_+ := r_-/\sqrt{\kappa} $ slightly larger,
$\psi_0(r_-) \le \psi_0(r_+) \le \kappa \psi(r_+) \le \kappa r_+^2 = r_-^2$.


It follows that the bound $\psi(r_-) \le \hat \psi(r_-)$ holds for $r_-$ smaller than, but sufficiently close to, $r_{\star}$.
If $\hat r_{\star}$ were less than $r_{\star}$, then this bound would hold for some $r_- \in (\hat r_{\star}, r_{\star})$,
and implying that $\psi(r_-) \le \hat \psi(r_-) \le r_-^2$. As by definition $r_{\star}$ is a lower bound on 
the set $\set{r > 0 : \psi(r) \le r^2}$, our premise cannot be true --- it must be the case that $\hat r_{\star} \ge r_{\star}$.


We conclude by showing that $\psi(r)/r$ is non-increasing.
Because $\psi(r)/r = \eta^{-1} R_n(r^{-1}\F \cap [\omega''(r)/r]B)$
and $R_n$ is increasing in the order of inclusion in the sense that $A \subseteq B \implies R_n(A) \le R_n(B)$,
it suffices to show that $r_+^{-1}\F \cap [\omega''(r_+)/r_+] B \subseteq r^{-1}\F \cap [\omega''(r)/r] B$
when $r_+ \ge r$. This holds because $\F$ and $B$ are star-shaped around zero
and $r^{-1}$ and $\omega(r)/r$ are non-increasing.
\end{proof}

\begin{proof}[Proof of Lemma~\ref{lemma:non-increasing-modulus}]
If $s \ge r$ and a sequence $f_j \in \set{\F : \norm{f} \le s}$ satisfy $\lim_{j}\norm{\chi(f_j)}' = \omega(s)$,
then $(r/s)f_j \in \set{\F : \norm{f} \le r }$, so $\omega(r) \ge \lim_{j} \norm{\chi((r/s)f_j)}' = (r/s)\omega(s)$ and equivalently $\omega(r)/r \ge \omega(s)/s$.
\end{proof}

\begin{proof}[Proof of Corollary~\ref{cor:empirical-vs-pop-local-complexity}]
Define $r_{\star}$ and $\hat r_{\star}$ as in Lemma~\ref{lemma:empirical-vs-pop-local-complexity} with $\F = \chi(\F_n)$,
$\omega(r)=\sup_{f \in \F_n \cap r B} \norm{\chi(f)}_{L_2(\P)}$, $\delta=1/2$, and $r_L$ satisfying $n^{(1+q)/4q}r_L \to 0$
for $q=p/(p-2)$. The definition of $\omega'$ that we use here agrees with that of Lemma~\ref{lemma:empirical-vs-pop-local-complexity},
as 
\[ \frac{2}{1 + p/(p-2)} = \frac{2}{2(p-1)/(p-2)} = \frac{p-2}{p-1}. \] 
And as the restriction $r_{\star} \ge r_L$ implies that $n r_\star^{4q/(1+q)} \to \infty$,
$P(r_\star \ge \hat r_{\star}) \to \delta$ by Lemma~\ref{lemma:empirical-vs-pop-local-complexity}.
Furthermore, as $\hat r_{\star} = o_P(r_n)$ by assumption,
$P(\hat r_{\star} \ge \epsilon r_n) \to 0$ for any $\epsilon > 0$, and by the union bound
$P(r_\star \ge \epsilon r_n) \to \delta$. As $r_\star \le \epsilon r_n$ is a deterministic comparison,
and it is true with probability tending to the nonzero limit $1 - \delta=1/2$, 
it follows that it is true deterministically: $r_\star = o(r_n)$. 

Now consider the latter claim. By Markov's inequality, $R_n(\chi(\F_n) \cap \omega_n'(s_n)B) = O_P( \hat R_n(\chi(\F_n) \cap \omega_n'(s_n) B))$,
and using a property used in the proof of Lemma~\ref{lemma:empirical-vs-pop-local-complexity}
--- the property that $\F \cap 2s\hat B \supseteq \F \cap sB$ for all $s > \omega_n'(r_{\star})$ 
with probability tending to one --- it follows that so long as $r_n' > r_{\star}$,
$R_n(\chi(\F_n) \cap \omega_n'(r_n')B) = O_P(R_n(\chi(\F_n) \cap 2\omega_n'(r_n')\hat B))$.
This implies our claim, as $r_n' \ge r_n > r_{\star}$ with probability tending to one, and 
$\chi(\F_n) \cap 2\omega_n'(r_n')\hat B \subseteq  2[ \chi(\F_n) \cap \omega_n'(r_n')\hat B)]$.
\end{proof}


\begin{proof}[Proof of Lemma~\ref{lemma:gaussian-comparison-convex-hull}]
Observe that $\F$ is contained in the Minkowski sum $\conv(\F_0)-\conv(\F_0)$
where $\F_0 := \MM - \GG$ for $\MM = \set{0, m_1, m_2, \ldots, m_K}$. Thus, for any $s$,
\begin{align*} 
\hat G_n(\F \cap r \hat B) 
&\le \hat G_n\p{(\conv(\F_0) - \conv(\F_0)) \cap r \hat B} \\
&\le 2\hat G_n\p{(\F_0 - \F_0] \cap s \hat B} + r \sqrt{\hat N(\F_0, s)/n} \\
&\le 2\hat G_n\p{(\F_0 - \F_0) \cap s \hat B} + r \sqrt{(K+1)\hat N(\GG, s)/n}. 
\end{align*}
The first comparison holds because of this containment, the second via a bound of \citet[Theorem 1]{bousquet2002some}
relating the moduli of continuity of the isonormal gaussian processes $f \to n^{1/2}\Pn \xi_i f(Z_i)$
indexed by a set $\F_0$ and its convex hull, and the third because given any $s$-cover $\GG^s$ 
of $\GG$, $\MM -  \GG^s$ is an $s$-cover of $\F_0$.

To bound the first term here, observe that every function $f \in \F_0-\F_0$
can be written as $m - g$ for $m \in \mathcal M - \mathcal M$ and $g \in \GG-\GG$.
Letting $\Pi$ be the $L_2(\Pn)$-orthogonal projection onto the convex set $\GG-\GG$, we can write this as
$(m - \Pi m) + (\Pi m - g)$, and by the Hilbert space projection theorem \citep[Proposition 1.37]{peypouquet2015convex},
$\Pn (m-\Pi m) (g-\Pi m) \le 0$ for all $g \in \GG-\GG$, so
\begin{align*} 
\norm{f}_{L_2(\Pn)}^2 &= \norm{m - \Pi m}_{L_2(\Pn)}^2 + \norm{\Pi m - g}_{L_2(\Pn)}^2 + 2 \Pn (m-\Pi m)(\Pi m - g) \\
		    &\ge \norm{m - \Pi m}_{L_2(\Pn)}^2 + \norm{\Pi m - g}_{L_2(\Pn)}^2. 
\end{align*}
Thus, every $f \in [\F_0 - \F_0] \cap s\hat B$ can be written as a sum $(m-\Pi m) + (\Pi m - g)$ 
where $\Pi m - g \subseteq [2(\GG-\GG)] \cap s\hat B$ and 
$m-\Pi m \in \MM' \cap s \hat B$ for $\MM' := \set{m_i - m_j - \Pi (\hat m_i - \hat m_j) : i,j \in 0 \ldots K}$ with $m_0=0$.
Thus, for some universal constant $c$, 
\begin{align*}
\hat G_n\p{(\F_0 - \F_0) \cap s \hat B} 
&\le \hat G_n\p{\MM' \cap s B} + \hat G_n\p{[2(\GG-\GG)] \cap s \hat B} \\
&\le c n^{-1/2} s \sqrt{\log(K+1)} +  2 \hat G_n\p{(\GG-\GG) \cap s \hat B}. 
\end{align*}
Here we bound $\hat G_n([2(\GG-\GG)] \cap s \hat B)$ using the inclusion $[2(\GG-\GG)] \cap s \hat B \subseteq 2[(\GG - \GG) \cap s \hat B]$
and $\hat G_n(\MM' \cap s \hat B)$ using the finite class bound 
$\hat G_n(\set{f_1 \ldots f_k}) \le cn^{-1/2}\sup_{j}\norm{f_j}_{L_2(\Pn)} \sqrt{\log(k)}$ \citep[e.g.,][Exercise 7.5.10]{vershynin2018high}.
\end{proof}



\subsection{Regularity and Efficiency}
\label{sec:proof-of-efficiency} 

In this section, we will prove the claims about regularity and efficiency in Theorem~\ref{theo:simple}. We express our estimand
as a functional $\chi(P)$ of the distribution of the observed data, defined by $\chi(P)=\psi_P(m_P)$ for $\psi_P(m) = \E_P h(Z,m)$ and $m_P(z)=E_P[Y \mid Z=z]$.
The first step of our proof is characterizing the tangent space at $\P$ in our model. 
Having done this, we calculate the derivative $\dot\chi_P$ of $\chi$ at $\P$ on this tangent space.
An estimator $\hat \chi$ for a differentiable functional $\chi(\P)$ with the asymptotic characterization 
$\hat \chi - \chi(\P) = \Pn \influence(Y,Z) + o_P(1/\sqrt{n})$ 
is regular iff $\E_P \influence(Y,Z)g(Y,Z) = \dot\chi_P(g)$ for all scores $g$ in the tangent space and
asymptotically efficient iff it is regular and $\influence(y,z)$ is in the closure of the tangent space \citep[Section 1.2 and Example 4.6]{van2000semiparametric}.

\subsubsection{The tangent space}
We will show that the tangent space at $\P_0=\P$ to the set of all submodels $\P_t$ 
for which the regression functions satisfy $m_{\P_t} \in \mm$
and $\norm{m_{\P_t} - m_{\P}}_{L_2(\P)} \to 0$ as $t \to 0$ and the squares of $\epsilon_t=Y_t-m_{P_t}(Z_t)$ for $Y_t,Z_t \sim \P_t$ are uniformly integrable is
\[ \tangent = \set*{ a(z) + b(y,z) \in L_2(\P) : \E_P[a(Z)]=0, \E_P[b(Y,Z) \mid Z]=0, \E_P[Y b(Y,Z) \mid Z=z] \in \mm}. \]
To show that $\tangent$ contains the tangent space, we will show that
the score $g$ of every such submodel $P_t$ is in $\tangent$.
To show that $\tangent$ is contained in the tangent space, we construct such a submodel
for each score $g \in \tangent$. 

\paragraph*{Containment of the tangent space in $\tangent$}
We'll begin with a non-rigorous argument. 
Consider a submodel $P_t$ with factored density $p_t(y,z)=p_t(z) p_t(y \mid z)$ with respect to a product measure $\mu_y \times \mu_z$
and suppose that it is differentiable both pointwise and in quadratic mean, so its score
function is the derivative at $t=0$ of the log likelihood $\log p_t (z) + \log p_t(y \mid z)$.
Call the first term $a(z)$ and the second term $b(y,z)$ --- it is well known that given the score 
$g(y,z)=a(z)+b(y,z)$, we can uniquely recover $a(z)=\E_P[g(Y,Z) \mid Z=z]$ and $b(y,z)=g(y,z)-a(z)$.
A submodel must satisfy $\int y p_t(y \mid z)d\mu_y =\E_{P_t}[Y \mid Z=z] = m_{P_t}(z)$ for $m_{P_t} \in \mm$, 
and assuming we can interchange differentiation and integration,
this implies  
\begin{align*} 
\lim_{t=0}t^{-1}(m_{P_t}(z) - m_P(z))
&= \int y \frac{\partial}{\partial t}\mid_{t=0} p_t(y \mid z) d\mu_y \\
&= \int y \frac{\partial}{\partial t}\mid_{t=0} \log p_t(y \mid z) p_0(y \mid z) d\mu_y \\
&= \E_{P}[Y b(Y,Z) \mid Z=z]. 
\end{align*}
And as $m_{P_t} - m_P \in \mm$ for all $t$, this implies that $\E_{P}[Y b(Y,Z) \mid Z=z] \in \mm$,
so $g \in \tangent$.


To prove this rigorously, we must show that that for any quadratic-mean differentiable submodel $P_t$,
its score $g$ is in $\tangent$. To do this, we begin by simplifying the condition $\E_P[Y b(Y,Z) \mid Z=z] \in \mm$ 
characterizing $\tangent$. This condition is equivalent to the condition $\E_P[f(Z) \E_P[Y b(Y,Z) \mid Z]]=0$
for all $f \in \mm_{\perp}$, the $L_2(\P)$-orthogonal complement of $\mm$. Furthermore, for all bounded $f$,
this is equivalent to the condition $\E_P[f(Z) Y  b(Y,Z)]=0$. Now suppose that this condition holds for all bounded $f$
and recall that we've assumed that $\mm_{\perp}$ has a $\norm{\cdot}_{L_2(P)}$-dense subset of bounded functions.
Each unbounded $f \in \mm_{\perp}$ is the limit of a sequence $f_j \in \mm_{\perp}$ of bounded functions satisfying
$\E_P[f_j(Z) \E_P[Y b(Y,Z) \mid Z]]=0$, and by the Cauchy-Schwarz inequality, $\E_P[(f(Z)-f_j(Z)) \E_P[Y b(Y,Z) \mid Z]] \to 0$ 
and therefore $\E_P[f(Z) \E_P[Y b(Y,Z) \mid Z]] = 0$. Thus, it suffices to show that $\E_P[f(Z) Y b(Y,Z)]=0$ 
for all bounded $f \in \mm_{\perp}$. We will use this to formalize the argument above. 


Let $P_t$ be any one-dimensional parametric submodel with score $g$. For a sequence $t_j \to 0$, let $p_t$ and $p$ be densities of $P_{t}$ and $P$ 
with respect to a dominating probability measure $\mu$, so $t^{-1}(\sqrt{p_t}-\sqrt{p}) \to (1/2)g\sqrt{p}$ in $L_2(\mu)$.
Letting $h_t = f(z)(y-m_{P_t}(z))$, 
\begin{equation}
\label{eq:bounded-score-argument}
\begin{aligned} 
&t^{-1}(\E_{P_{t}}[h_{t}(Y,Z)] - \E_{P}[h_{t}(Y,Z)]) - \E_{P}[h_0(Y,Z)g(Y,Z)] \\
&= \int h_t \cdot t^{-1}(p_t - p) d\mu - \int h_0 g p d\mu \\
&= \int h_t \cdot [t^{-1}(\sqrt{p_t} - \sqrt{p})(\sqrt{p_t} + \sqrt{p}) - gp]d\mu + \int (h_t-h_0) g p d\mu.
\end{aligned}
\end{equation}
If this difference goes to zero for every $f \in \mm_{\perp}$, 
this would imply that $g \in \tangent$,
as $\E_{P_{t}}[h_{t}(Y,Z)] - \E_{P}[h_{t}(Y,Z)]=0$ for all $t$ and $\E_{P}[h_0(Y,Z)g(Y,Z)]=\E_P[f(Z)Yb(Y,Z)]$.
To see this, observe that for every $f \in \mm_{\perp}$,
$\E_{P_t}[f(Z)(Y-m_{P_t}(Z))] = 0$ because $\E_{P_t}[Y-m_{P_t}(Z)\mid Z]=0$,
$\E_{P}[f(Z)(Y-m_{P_t}(Z))] = \E_{P}[f(Z)(m_P-m_{P_t})(Z)] = 0$ 
because $f \in \mm_{\perp}$ and $m_P - m_{P_t} \in \mm$ for any submodel,
and $\E_P[f(Z)(Y-m_P(Z))a(Z)] = 0$ and $\E_P[f(Z)m_P(Z)b(Y,Z)]=0$
because $\E_P[Y-m_{P}(Z)\mid Z]=0$ and $\E_P[b(Y,Z) \mid Z]=0$.

If $h_t$ were bounded, this difference would go to zero.
Because $t^{-1}(\sqrt{p_t} - \sqrt{p}) \to (1/2)g\sqrt{p}$ in $L_2(\mu)$,
$\sqrt{p_t} + \sqrt{p} \to 2\sqrt{p}$ in $L_2(\mu)$ and it follows that $t^{-1}(\sqrt{p_t} - \sqrt{p})(\sqrt{p_t} + \sqrt{p}) \to (1/2)g\sqrt{p} \cdot 2\sqrt{p} = gp$ in $L_1(\mu)$.
Thus, by H\"older's inequality, the first term in the last line of \eqref{eq:bounded-score-argument} would go to zero. And because 
$h_t-h_0 = f(z)(m_P - m_{P_t})$ converges to zero in $L_2(\P)$, the Cauchy-Schwarz bound on the second term converges to zero. 
We conclude by using a truncation argument to show that this happens though $h_t$ is not, in general, bounded.

Let $h_t^K = h_t 1(\abs{y - m_{P_t}}  \le K)$, so $\norm{h_t^K}_{\infty} \le K\norm{f}_{\infty}$.  
\begin{equation}
\label{eq:bounded-score-argument-trunc}
\begin{aligned} 
&t^{-1}(\E_{P_{t}}[h_{t}(Y,Z)] - \E_{P}[h_{t}(Y,Z)]) - \E_{P}[h_0(Y,Z)g(Y,Z)] \\
&= \int (h_t - h_t^K) \cdot t^{-1}(p_t - p) d\mu\ +\  \int h_t^K \cdot [t^{-1}(\sqrt{p_t} - \sqrt{p})(\sqrt{p_t} + \sqrt{p}) - gp]d\mu \\
&+ \int (h_t^K-h_0^K) g p d\mu \ +\  \int (h_0^K - h_0) g p d\mu.
\end{aligned}
\end{equation}
Calling these terms $\delta^k_{t,K}$ for $j \in 1\ldots 4$,
we will show that $\lim_{K \to \infty}\lim_{t \to 0}\delta^j_{t_K}=0$.
By H\"older's inequality as described above, for any finite $K$, the second term goes to zero as $t \to 0$.
And as $\abs{h_t^K - h_0^K} \le \abs{h_t - h_0}$, it follows that the Cauchy-Schwarz bound on the third term
is smaller than $\norm{h_t - h_0}_{L_2(\P)}\norm{g}_{L_2(\P)}$, which goes to zero as $t \to 0$. 
This leaves the first and fourth terms. The Cauchy-Schwarz bound on the fourth goes to zero as $K \to \infty$ if $\norm{h_0^K - h_0}_{L_2(\P)} \to 0$,
and we will now show the relevant limit of the first term is zero if, in addition, \smash{$\lim_{K \to\infty}\lim_{t \to 0}\norm{h_t^K - h_t}_{L_2(\P_t)} = 0$.}
We use the following decomposition.
\begin{align*}
\int (h_t-h_t^K) \cdot t^{-1}(p_t - p) d\mu 
&= \int (h_t-h_t^K)(\sqrt{p_t} + \sqrt{p}) \cdot t^{-1}(\sqrt{p_t} - \sqrt{p} - (t/2)g\sqrt{p}) d\mu \\
&+ \int (h_t-h_t^K)(\sqrt{p_t} + \sqrt{p}) \cdot (1/2)g\sqrt{p} d\mu \\
&\le \norm{(h_t-h_t^K)(\sqrt{p_t} + \sqrt{p})}_{L_2(\mu)} \cdot t^{-1}\norm{\sqrt{p_t} - \sqrt{p} - (t/2)g\sqrt{p}}_{L_2(\mu)} \\
& +  \norm{(h_t-h_t^K)(\sqrt{p_t} + \sqrt{p})}_{L_2(\mu)} \cdot (1/2)\norm{g\sqrt{p}}_{L_2(\mu)}. 
\end{align*}
As $t^{-1}\norm{\sqrt{p_t} - \sqrt{p} - (t/2)g\sqrt{p}}_{L_2(\mu)} \to 0$ and $\norm{g\sqrt{p}}_{L_2(\mu)} = \norm{g}_{L_2(\P)} < \infty$,
this is bounded by a constant (in $t$) multiple of $\norm{(h_t-h_t^K)(\sqrt{p_t} + \sqrt{p})}_{L_2(\mu)}$. Furthermore, 
because $(\sqrt{p_t} + \sqrt{p_t})^2 \le 2p_t + 2p$, this is bounded by a constant multiple of $\norm{h_t-h_t^K}_{L_2(\P_t)} + \norm{h_t - h_t^K}_{L_2(\P)}$.
And the relevant limit of this sum goes to zero if 
\begin{equation}
\label{eq:two-limits}
 \lim_{K \to \infty}\lim_{t \to 0}\norm{h_t-h_t^K}_{L_2(\P_t)}=0 \ \text{ and }\ \lim_{K \to \infty}\norm{h_0 - h_0^K}_{L_2(\P)}=0,
\end{equation}
as the second and third terms in the following bound go to zero as $t \to 0$ for any $K$.
\begin{align*} 
\norm{h_t - h_t^K}_{L_2(\P)}
&=\norm{ (h_0 - h_0^K) + (h_t - h_0) + (h_t^K-h_0^K) }_{L_2(\P)} \\
&\le \norm{ h_0 - h_0^K}_{L_2(\P)} + \norm{h_t - h_0}_{L_2(\P)} + \norm{h_t^K-h_0^K}_{L_2(\P)}. 
\end{align*}
All that is left is to show that \eqref{eq:two-limits} holds. It suffices to consider the case of $h_t = y-m_{P_t}$, as
\[ \norm{h_t - h_t^K}_{L_2(\P_t)} = \norm{f \cdot (y-m_{P_t}) 1(\abs{y - m_{P_t}}  > K)}_{L_2(\P_t)} \le \norm{f}_{\infty} \norm{(y-m_{P_t}) 1(\abs{y - m_{P_t}}  > K)}_{L_2(\P_t)}. \]
And for this $h_t$, \eqref{eq:two-limits} is implied by the uniform integrability property we impose on our paths.


\paragraph*{Containment of $\tangent$ in the tangent space}

We will show that each element $a(z)+b(y,z)$ of $\tangent$ 
is the score of a one-dimensional parametric submodel $P_t$
with $E_{P_t}[Y \mid Z=z] = m_P(z) + t m'(z)$ for $m'(z) = \E[ Y b(Y,Z) \mid Z=z]$.
To do this, we use a tilting construction \citep[see e.g.,][Section 4.5]{tsiatis2007semiparametric}. 
Factor $P$ into the product of a regular conditional distribution $P(\cdot \mid Z=z)$ on $Y$ and a marginal $P^M$ on $Z$.
We define this submodel $P_t$ by  
choosing a nonnegative continuously differentiable function $k(x)$ satisfying $k(0)=k'(0)=1$ with $k,k'$ bounded in a neighborhood of zero, 
e.g., $k(x)=2(1+e^{-2x})^{-1}$ \citep[Example 1.12]{van2000semiparametric},
and taking
\begin{equation}
\label{eq:submodels}
\begin{aligned}
&d\P_t(y \mid Z=z)/d\P(y \mid Z=z) = \frac{k(c_t(z)y + tb(y,z))}{\E_P[ k(c_t(Z)Y + tb(Y,Z)) \mid Z=z]}, \\
&d\P_t^M(z) / d\P^M(z) = \frac{k(ta(z))}{\E_P k(ta(Z))}, 
\end{aligned}
\end{equation}
for $c_t$ satisfying 
\[ \E_{P_t}[Y \mid Z=z] = \frac{\E_P[Y k(c_t(Z)Y + tb(Y,Z)) \mid Z=z]}{\E_P[k(c_t(Z)Y + tb(Y,Z)) \mid Z=z]} = m_P(z) + tm'(z). \] 
If we take $c_t(0)=0$, this condition is satisfied for $t=0$, and we will use the implicit function theorem 
to characterize $c_t$ for which it holds on a neighborhood of zero. This requires that the function 
\[ f(t,c) = \frac{\E_P[Y k(cY + tb(Y,Z)) \mid Z=z]}{\E_P[k(c Y + tb(Y,Z)) \mid Z=z]} - \sqb{m_P(z) + tm'(z)} \]
be continuously differentiable and that its partial derivative with respect to $c$ be nonzero at $(c,t)=0$,
and it implies that the solution $c_t$ is continuously differentiable with 
$c_t'(z) = -(\partial f/\partial t)(t, c_t(z)) / (\partial f/\partial c)(t, c_t(z))$.
By the sum and quotient rules of calculus, $f$ is continuously differentiable if the numerator and denominator of
the first term of $f$ are, and this reduces to continuous differentiability of the integrands in the numerator and denominator, 
as we can interchange integration and differentiation because the derivatives of the integrand are dominated.
In particular, because $k'$ is bounded, the partials with respect to $c$ and $t$ 
of the integrand in the numerator are dominated by multiples of $Y^2$ and $Yb(Y,Z)$ respectively;
in the denominator the same goes for $\abs{Y}$ and $\abs{b(Y,Z)}$. Thus, we calculate
\begin{align*} 
(\partial f/\partial t)(t,0) \mid_{t=0} &= \E_P[Yb(Y,Z) \mid Z=z] - \E_P[Y \mid Z=z]\E_P[b(Y,Z) \mid Z=z] - m'(z) \\
                                        &= \E_P[Yb(Y,Z) \mid Z=z] - m'(z) = 0, \\
(\partial f/\partial c)(0,c) \mid_{c=0} &= \E_P[Y^2 \mid Z=z] - (\E_P[Y \mid Z=z])^2 \\
                                        &= \var_P[Y \mid Z=z], \\
c_0'(z) &= -0 \ /\ \var_P[Y \mid Z=z] = 0.
\end{align*}
We will check that this yields a valid submodel. By construction, our densities are nonnegative and integrate to one
and $m_{P_t}(z) = m(z) + tm'(z)$ is in $\mm$ for all $t$ and satisfies $\norm{m_{P_t} - m}_{L_2(\P)} = t\norm{m'}_{L_2(\P)} \to 0$ as $t \to 0$.
The remaining condition is the uniform integrability property $\lim_{K \to \infty}\E_{P_t}(Y-m_{P_t})^2 1((Y-m_{P_t})^2 \ge K)=0$ for $t$ in a neighborhood of zero.
Because the denominators in \eqref{eq:submodels} are near one in a neighborhood of $0$, it suffices that
\[ \lim_{K \to \infty}\E_P[ k(c_t(Z) + t b(Y,Z))k(t(a(Z))) (Y - m(Z)-tm'(Z))^2 1((Y-m(Z) - tm'(Z))^2 \ge K) = 0. \]
Furthermore, because $k$ is bounded this is equivalent to the uniform integrability of $(Y - m(Z)-tm'(Z))^2$ under $\P$,
and this holds because for $t \le 1$, these are dominated by the integrable quantity $2(Y-m(Z))^2 + 2 m'(Z)^2$.

Finally, we check that this submodel is differentiable in quadratic mean with the intended score  $a(Z) + b(Y,Z)$.
By design, this intended score is the derivative at zero of the log of the density $p_t = d\P_t/d\P$.
\begin{align*}
&\frac{d}{dt}\mid_{t=0} \log p_t =\frac{d}{dt}\mid_{t=0} \log \p{ d\P_t^M / d\P^M } +\frac{d}{dt}\mid_{t=0} \log\p{ d\P_t(\cdot \mid Z)/d\P(\cdot \mid Z) } \\ 
&= \frac{d}{dt}\mid_{t=0} \set*{\log k(ta(Z)) - \log \E_P[ k(ta(Z))]} \\
& + \frac{d}{dt}\mid_{t=0}\set*{\log k(c_t(Z)Y + tb(Y,Z)) - \log \E_P[ k(c_t(Z)Y + t b(Y,Z))  \mid Z] } \\ 
&=a(Z) - \E_P[ a(Z) ] + c_0'(Z)Y + b(Y,Z) - \E_P[ c_0'(Z)Y + b(Y,Z)  \mid Z]  \\ 
&=a(Z) + b(Y,Z).
\end{align*}
Here we've used the property $c'_0=0$ derived above. All that is left is to show that our submodel is differentiable in
quadratic mean. Via \citet[Lemma 1.8]{van2000semiparametric}, 
it suffices to show that $\sqrt{p_t}$ is continuously differentiable near zero and
$\int [(p'_t)^2 / p_t] d\P$ is finite. These properties follow from our assumptions on $k$ and
and our characterization of $c_t$ as continuously differentiable in a neighborhood of zero.

\subsubsection{The Pathwise Derivative of $\chi$, Regularity, and Efficiency}
\label{sec:pathwise-derivative}
We will calculate the derivative of our functional $\chi(P)$ on the tangent space $\tangent$.
As discussed above, for any score $g \in \tangent$, there is a submodel $P_t$ of the form defined in \eqref{eq:submodels}, with regression function $m_{P_t}(z) = m_P(z) + t m'(z)$ for $m' \in \mm$. Furthermore, its score satisfies $m'(z) = \E_P[Y b(Y,Z) \mid Z=z]=\E_P[(Y-m_P(Z))g(Y,Z) \mid Z=z]$. 
The form of our path makes it easy to calculate the derivative of $\chi(P_t)$, as $\chi(P_t)$ depends only on $m_{P_t}$ and the marginal distribution of $Z$.

\begin{align*}
&(\partial/\partial t)\mid_{t=0} \chi(P_t) \\ 
&= (\partial/\partial t) \mid_{t=0}   \E_{P_t}[ h(Z,m_{P_t}) ]  \\
&= (\partial/\partial t) \mid_{t=0} \p{  \E_{P}[ k(ta(Z)) h(Z,m_{P_t})] / \E_{P}[ k(ta(Z))] } \\
&=  \p{ (\partial/\partial t) \mid_{t=0} \E_{P}[ k(ta(Z)) h(Z,m_{P_t}) ] } -  \p{(\partial/\partial t)\mid_{t=0}\E_P[ k(ta(Z))]}\E_{P}[ h(Z,m_{P}) ] \\
&= \E_{P}[ (\partial/\partial t)\mid_{t=0}  k(ta(Z)) h(Z,m_P + t m') ] - \E_P[ (\partial/\partial t)\mid_{t=0} k(ta(Z))]\E_{P}[ h(Z,m_{P}) ] \\
&= \E_P[ a(Z) h(Z, m_P)] + \E_P[ h(Z, m') ] - \E_P[a(Z)]\E_{P}[ h(Z,m_{P}) ]
\end{align*}
The third term here is zero, as $\E_P[a(Z)]=0$, and we rely on dominated convergence to interchange integration and differentiation.
Dominatedness follows, via the mean value theorem, from the boundedness of $k'(\cdot)$ and the square integrability of $a(z)$,
 $h(z,m)$, and $h(z,m')$. 

Recalling that $g(Y,Z)=a(Z) + b(Y,Z)$ where $\E_P[b(Y,Z) \mid Z] = 0$ and $\E_P[a(Z)]=0$, 
the first term above is equal to $\E_P[ g(Y,Z) (h(Z,m_P) - \psi_P(m_P))]$.
Furthermore, if $\riesz[\psi]$ is the Riesz representer for $\psi_P(\cdot) = \E_P[ h(Z, \cdot)]$ on a superset of $\mm$, we can write the second term as 
$\E_P[\riesz[\psi](Z) m'(Z)]= \E_P[\riesz[\psi](Z) \E_P[(Y - m_P(Z)) g(Y,Z) \mid Z]]$. 
Thus, $\influence(y,z) = h(z,m_P) - \psi_P(m_P) + \riesz[\psi](z)(y-m_P(z))$ is an influence function, as 
$\E_P[ \influence(Y,Z) g(Y,Z)]=(\partial/\partial t)\mid_{t=0}$ $\chi(P_t)$. This establishes our regularity claim.

Furthermore, $\influence$ is in the closure of the tangent space $\tangent$, and therefore the efficient influence function,
 if and only if $\E_P[ (Y-m_P(Z)) \influence(Y,Z) \mid Z=z]$ is in the closure of $\mm$.
As $\E_P[ (Y-m_P(Z)) \influence(Y,Z) \mid Z=z] = \E_P[(Y-m_P(Z))^2 \mid Z=z] \riesz[\psi](z)$,
this happens if and only if $v \riesz[\psi]$ is in the closure of $\mm$ for $v(z) = \E_P[(Y-m_P(Z))^2 \mid Z=z]$. 
This completes our proof.

Note that if $\psi_P$ is not continuous on $\mm$, $\chi$ is not differentiable at $P$:
for a submodel $P_t$ with constant marginal distribution on $Z$, $t^{-1}(\chi(P_t)-\chi(P)) = \psi_P(m')$.
Thus, as the existence of a regular estimator for $\chi(P)$ implies the differentiability of $\chi$ at $\P$ \citep[Theorem 2.1]{van1991differentiable}, 
it implies the continuity of $\psi_P$ on $\mm$. 

\subsection{Estimating $\riesz[\psi]$ at the optimal rate}
\label{sec:estimating-riesz-optimal}
Here we consider the optimality of the rate $\norm{\hriesz - \riesz[\psi]}_{L_2(P)} = O_P(r)$ discussed in Section~\ref{sec:tuning-sigma}.
We use the notation of Theorem~\ref{theo:simple-rate}. 

If $\ff$ is a class of uniformly bounded functions with empirical metric entropy 
\smash{$\log N(\ff; $} \smash{$\L{2}{\Pn}; \epsilon) = O(\epsilon^{-2\rho})$} for $\rho > 1$, it can be shown that $r_Q = $ \smash{$O(n^{-1/(2+2\rho)})$}
\citep[see e.g.,][Equation 2.4]{koltchinskii2006local}. Furthermore, if $\riesz[\psi]$ is bounded
and the map $f \to h(\cdot,f)$ is well-behaved, $\riesz[\psi]\ff$ and $h(\cdot,\ff)$ will satisfy the same entropy bound, and
$r$ will have this rate as well. When $\ff$ is the unit ball of a H\"older space $C^s$ of functions on $[0,1]^d$
with $s>d/2$, we have such an entropy bound with $\rho = d/(2s)$ \citep{tikhomirov1993varepsilon, van1994bracketing},
and we get the minimax rate $n^{-1/(2+d/s)}$ for estimating a function $f$ with \smash{$\norm{f}_{\ff} < \infty$}
from direct observations of $f(Z_i) + \xi_i$ with gaussian noise $\xi_i$ \citep[Theorem 3.2]{gyorfi2006distribution}.

While the general problem of estimating a Riesz representer is nonstandard, one point of reference is Example~\ref{exam:mar},
in which $\riesz[\psi](x,w)=w/e(x)$ for $e(x)=\E[W_i \mid X_i=x]$. If $e(x)$ is bounded away from zero,
$\riesz[\psi]$ and $e(x)$ are estimable at the same rate, and in the case that \smash{$\norm{e}_{C^s} < \infty$},
the minimax rate for estimating directly observed $e(x)$ is \smash{$n^{-1/(2+d/s)}$.}
In this example, our estimator $\hriesz[\psi]$ for \smash{$\ff = \set{ w g : \norm{g}_{C^s}  \le 1}$} 
attains the minimax rate, as $\norm{\riesz[\psi]}_{\ff} < \infty$ and $\ff$ has entropy comparable to that of the unit ball of our H\"older space.

\section{Simulation Study: Details}
\label{sec:simu_details}

Here we describe the cross-fitting scheme used to estimate 
$\htau(X_i)$ and $\hmu(X_i)$ in the simulation study discussed in Section~\ref{sec:simu}.
Ten-fold cross-fitting is used throughout: where $\htau(X_i)$ and $\hmu(X_i)$ appear in
\eqref{eq:ape_aml} and \eqref{eq:ape_dr}, we use estimators $\htau^{(-i)}$ and $\hmu^{(-i)}$
trained on the folds that do not include unit $i$. This 
reduces dependence on $(Y_i,X_i,W_i)$ and therefore mitigates potential own-observation bias 
in $\hpsi_{DR}$ \citep[see e.g.,][]{chernozhukov2016double}. However, we do get some dependence
through the estimates of $\hat r$ and $\he$ used to train $\htau$ and
through lasso tuning parameters, which are chosen once for all $i$ by cross-validation.
This dependence can be eliminated using a computationally demanding nested sample splitting scheme;
we follow the \texttt{grf} package of \citet*{athey2016generalized} in using the following simplified scheme.

\begin{enumerate}
\item Partition the indices $1 \ldots n$ into $K=10$ folds of equal size, associating each index with a fold $f_i \in 1 \ldots K$.
\item For each fold $j \in 1 \ldots K$, train $\hat r_{j,\lambda_r}$ and $\hat e_{j,\lambda_e}$ on observations for $\set{ i : f_i \neq j}$
      for fixed values $\lambda_r, \lambda_e$ of the lasso penalty parameter. 
      Choose values $\hat\lambda_r, \hat\lambda_e$  by cross-validation, solving
     \[ \hat\lambda_r = \argmin_{\lambda_r} \sum_{i=1}^n (Y_i - \hat r_{f_i, \lambda_r}(X_i))^2, \quad  \hat\lambda_e = \argmin_{\lambda_e}\sum_{i=1}^n (W_i - \hat e_{f_i, \lambda_e}(X_i))^2. \] 
\item For each fold $j \in 1 \ldots K$, train $\htau_{j,\lambda_{\tau}}(x) = \phi(x)^T \hat\beta_j$ for fixed $\lambda_{\tau}$ by  
\[ \hbeta_{j} = 
        \argmin_{\tau} \sum_{i : f_i \neq j} (Y_i - \hat r_{f_i,\hat\lambda_r}(X_i) - (W_i - \hat e_{f_i,\hat\lambda_e}(X_i)) \phi(X_i)^T \beta_j)^2 + \lambda_{\tau}\norm{\beta_j}_{\ell_1}. \]
Choose a value $\hat\lambda_{\tau}$ by cross-validation, solving
\[ \hat\lambda_{\tau} = \argmin_{\lambda_\tau}\sum_{i=1}^n (Y_i - \hat r_{f_i,\hat\lambda_r}(X_i) - (W_i - \hat e_{f_i,\hat\lambda_e}(X_i)) \htau_{f_i,\lambda_{\tau}}(X_i))^2. \] 
\item Define $\htau^{(-i)}=\htau_{f_i, \hat\lambda_{\tau}}$ and $\hmu^{(-i)}=\hat r_{f_i,\hat\lambda_r}(x)- \htau_{f_i, \hat\lambda_\tau}(x) \he_{f_i,\hat\lambda_e}(x)$.
\end{enumerate}


\section{Computing the Weights}
\label{sec:computation}

The optimization problem \eqref{eq:aml-primal} that defines our weights $\hriesz$, 
\[ \hriesz = \argmin_{\gamma \in \R^n} I_{h,\ff}^2(\gamma) + \frac{\sigma^2}{n^2}\norm{\gamma}^2,
\ \ I_{h,\ff}(\gamma) = \sup_{f \in \ff} \frac{1}{n} \sum_{i = 1}^n [\gamma_i f(Z_i) - h(Z_i,f)]. \] 
is strongly convex and not extremely high dimensional, so it is often fairly tractable.
However, because the objective function involves a supremum over a set $\ff$,
 it is helpful to reformulate the problem for implementation. We will discuss the case
that $\ff$ is the absolutely convex hull $\absconv\set{\phi_1,\phi_2, \ldots}$.
We will assume, in addition, that we have decay in $\phi_j$ and $h(\cdot,\phi_j)$ that justifies working with a finite
dimensional approximation to $\ff$, the absolutely convex hull
$\ff_p$ of the first $p$ basis functions. When we do this, our optimization problem above can be expressed as a finite-dimensional quadratic program,
\begin{align*}
&\argmin_{(t, \gamma) \in \RR^{n+1}} t^2 + \frac{\sigma^2}{n^2}\norm{\gamma}^2 && \text{subject to}\ \\
& \abs*{n^{-1}\sum_{i=1}^n[ \gamma_i \phi_j(Z_i) - h(Z_i,\phi_j)] } \le t && \text{for }\ j \in 1 \ldots p,
\end{align*} 
as H\"older's inequality is sharp on $\ell_1$, i.e.,
\begin{align*} 
I_{h,\ff_p}(\gamma)&=\sup_{\norm{\beta}_{\ell_1} \le 1} \sum_{j=1}^p\beta_j \p{n^{-1}\sum_{i=1}^n[\gamma_i \phi_j(Z_i) - h(Z_i,\phi_j)]} \\
    &= \max_{j \le p} \abs*{n^{-1}\sum_{i=1}^n [\gamma_i \phi_j(Z_i) - h(Z_i,\phi_j)]}.
\end{align*} 

Our implementation of the average partial effect estimator described in Section~\ref{sec:simu},  included in the
\texttt{R} package \texttt{amlinear}, uses a variant of this formulation appropriate to the class
$\ff_{\hh}$ \eqref{eq:F_ape}:
\begin{align*}
&\argmin_{(t_{\mu},t_{\tau}, \gamma) \in \RR^{n+2}} t_{\mu}^2+t_{\tau}^2 + \frac{\sigma^2}{n^2}\norm{\gamma}^2 && \text{subject to}\ \\
& \abs*{n^{-1}\sum_{i=1}^n \gamma_i \phi_j(Z_i) } \le t_{\mu} && \text{for }\ j \in 1 \ldots p, \\
& \abs*{n^{-1}\sum_{i=1}^n (W_i \gamma_i - 1) \phi_j(Z_i) } \le t_{\tau} && \text{for }\ j \in 1 \ldots p.
\end{align*} 
%This and \eqref{eq:ape_gamma} are equivalent because H\"older's inequality is sharp on $\ell_1$, and therefore 
%\begin{align*}
%&\sup_{\mu \in \hh_p} \abs*{n^{-1}\sum_{i=1}^n \gamma_i \mu(Z_i)}  
% = \max_{j \le p} \abs*{ n^{-1}\sum_{i=1}^n \gamma_i \phi_j(Z_i) }, \\
%&\sup_{\tau \in \hh_p} \abs*{n^{-1}\sum_{i=1}^n (W_i\gamma_i - 1) \tau(Z_i)} 
% = \max_{j \le p} \abs*{ n^{-1}\sum_{i=1}^n (W_i \gamma_i - 1) \phi_j(Z_i) }.
%\end{align*}

\paragraph*{Finite-dimensional approximation}
To determine the number of basis functions we need to include in $\ff_p$, 
we consider the conditional bias term in \eqref{eq:error-decomp}. For any weights $\gamma$, it is bounded by 
\[ \norm{\hm - m}_{\ff}I_{h,\ff}(\gamma) = \norm{\hm - m}_{\ff}\sqb{ I_{h,\ff_p}(\gamma) + I_{h,\ff}(\gamma) - I_{h,\ff_p}(\gamma)}. \]
In particular, for 
\[ \hgamma  = \argmin_{\gamma \in \R^n} I_{h,\ff}^2(\gamma) + \frac{\sigma^2}{n^2}\norm{\gamma}^2,\quad
   \hgamma_p = \argmin_{\gamma \in \R^n} I_{h,\ff_p}^2(\gamma) + \frac{\sigma^2}{n^2}\norm{\gamma}^2, \]
we have the bound
\begin{align*} 
&\norm{\hm - m}_{\ff} I_{h,\ff_p}(\hgamma_p) + \norm{\hm - m}_{\ff}\p{I_{h,\ff}(\hgamma_p) - I_{h,\ff_p}(\hgamma_p)} \\
&\le \norm{\hm - m}_{\ff} I_{h,\ff}(\hgamma) + \norm{\hm - m}_{\ff}\p{I_{h,\ff}(\hgamma_p) - I_{h,\ff_p}(\hgamma_p)}.
\end{align*}
The excess that results from the use of $\hgamma_p$ instead of $\hgamma$ is bounded by the latter term. In this term, the difference 
$I_{h,\ff}(\hgamma_p) - I_{h,\ff_p}(\hgamma_p)$ is bounded by 
\begin{align*} 
&\sup_{f \in \ff}\inf_{f \in \ff_p}\p{\norm{\hgamma_p}\norm{f-f'}_{L_2(\Pn)} + \norm{h(\cdot,f-f')}_{L_2(\Pn)}} \\
&\le \norm{\hgamma_p}\sup_{j > p}\norm{\phi_j}_{L_2(\Pn)} + \sup_{j > p}\norm{h(\cdot,\phi_j)}_{L_2(\Pn)},
\end{align*}
so we choose $p$ to control these suprema. To ensure that this excess is negligible relative to variance, so our estimator is
asymptotically linear under the assumptions of Theorem~\ref{theo:simple}, these suprema must be $o_P(n^{-1/2})$.

\subsection{A dual approach}
It is also possible to use the dual characterization of $\hgamma$ given by Lemma~\ref{lemma:duality},
\[ \hriesz_i = \hg(Z_i) \ \text{ for }\ \hg = \argmin_g n^{-1}\sum_{i=1}^n [g(Z_i)^2 - 2 h(Z_i, g)] + \sigma^2 n^{-1}\norm{g}_{\ff_p}^2, \]
i.e., $\hriesz_i = \phi(Z_i)^T \hat\beta$ for 
\begin{align*}
&\hbeta = \argmin_{\beta \in \R^p} \beta^T \Phi \beta - 2h^T \beta + \sigma^2 n^{-1}\norm{\beta}_{\ell_1}^2, \\ 
&\Phi = n^{-1}\sum_{i=1}^n \phi(Z_i)\phi(Z_i)^T, \ h_j = n^{-1}\sum_{i=1}^n h(Z_i, \phi_j).
\end{align*}
We can solve this by splitting $\beta$ into positive and negative parts, which gives an equivalent second order cone program: $\hbeta=\hbeta^+ - \hbeta^-$ where the latter solve 
\begin{align*}
&\argmin_{(s,t,\beta^+, \beta^-) \in \R^{2p+2}} s - 2\begin{pmatrix} h & -h\end{pmatrix}\begin{pmatrix}\beta^+ \\ \beta^-\end{pmatrix} + \sigma^2 n^{-1} t \quad \text{ subject to} \\
&\beta^+,\ \beta^- \ge 0, \\
&\begin{pmatrix}
\beta^+ \\
\beta^-
\end{pmatrix}^T
\begin{pmatrix}
\hphantom{-}\Phi &\ -\Phi \\ 
-\Phi            &\ \hphantom{-}\Phi
\end{pmatrix}
\begin{pmatrix}
\beta^+ \\
\beta^-
\end{pmatrix} \le s, \\
&\p{ \sum_{j=1}^p \beta^+_j + \sum_{j=1}^p \beta^{-}_j}^2 \le t, \\
\end{align*}

\subsection{Computation in Hilbert Spaces}
The approaches discussed above rely on finite-dimensional approximation and efficient solvers for quadratic and second order cone programs.
In contrast, when $\ff$ is the unit ball of a Reproducing Kernel Hilbert Space, we can often solve the dual 
\[ \hg = \argmin_g n^{-1}\sum_{i=1}^n [g(Z_i)^2 - 2 h(Z_i, g)] + \sigma^2 n^{-1}\norm{g}_{\ff}^2 \]
without approximation by solving a $n \times n$ linear system. In particular, if $h(Z_i, g)$ is a function of $g(Z_i)$,
a well-known Representer theorem  states that the solution to this problem
has the form $\hg(z) = \sum_{j=1}^n \hat\alpha_j K(Z_j,z)$, where $K(z',z)$ is the kernel 
associated with our space \citep{scholkopf2001generalized}.
 When $\hg$ is known to have this form, we can calculate $\hat\alpha$ by substituting $g(z)=\sum_{j=1}^n \alpha_j K(Z_j, z)$
into our dual problem above and solving the resulting unconstrained quadratic optimization problem over $\alpha$,
\begin{align*} 
&\hat \alpha = \argmin_\alpha \alpha^T \bar K \alpha - 2 \bar h^T \alpha + \sigma^2 n^{-1} \alpha^T G \alpha, \\
&\bar{K}_{ij} = n^{-1}\sum_{k=1}^n K(Z_i, Z_k) K(Z_j,Z_k), \\ 
&\bar h_i = n^{-1}\sum_{k=1}^n h(Z_k, K(Z_i,\cdot)),\\
& G_{ij} = K(Z_i,Z_j).
\end{align*}
This approach works in Example~\ref{exam:mar}, as $h(Z_i,g)$ has the required form,
and variations apply in our other examples, 
as similar representer theorems hold under appropriate conditions \citep[see e.g.][]{argyriou2014unifying}.


\section{Consistency of penalized least squares estimators}
\label{appendix:penalized-least-squares}
In this section, we state and prove a consistency result for penalized least squares 
relevant to our claims in Remark~\ref{rema:consistency}. We base our presentation 
on that of \citet{lecue2017regularization}.


\begin{theo}
\label{theo:donsker-consistency}
Let $(Y_1,Z_1)\ldots(Y_n,Z_n) \sim \P$ be independent and identically distributed, 
let $m_{\star} = \argmin_{m \in \mm}\P (Y - m(Z))^2$ for closed convex $\mm \subseteq L_2(\P)$,
and let $\ell(m) =  \Pn (Y_i - m(Z_i))^2 + \lambda \norm{m}_{\F}$ for some norm $\norm{\cdot}_{\F}$.
If $\ell(\hm) \le \ell(m_{\star})$ for $\hm \in \mm$, then 
$\norm{\hm-m}_{\F} \le \alpha$, $\norm{\hm-m}_{L_2(\P)} \le \alpha r$, and 
$\norm{\hm-m}_{L_2(\Pn)} \le (2\eta_M \alpha r^2 + 2\lambda \norm{m_{\star}}_{\F})^{1/2}$   
\[ \text{ where }\ \alpha = \max\set{ 2\eta_M/\eta_Q + \sqrt{2\lambda \norm{m_{\star}}_{\F}/(\eta_Q r^2)},\ 2\norm{m_{\star}}_{\F}/(1-2\eta_M r^2/\lambda)}, \]
on an event on which, for all $f \in \F = \set{f : \norm{f}_{\F} \le 1}$,
\begin{equation}
\label{eq:least-squares-ratio-process}
\begin{aligned}
&\Pn f^2 \ge \eta_Q \P f^2          	       && \text{ if }\quad \P f^2 \ge r^2, \\
&\abs{(\P - \Pn)(Y-m_{\star})f} \le \eta_M r^2  && \text{ if }\quad \P f^2 \le r^2.
\end{aligned}
\end{equation}
\end{theo}


Taking $\mm = L_2(\P)$, this characterizes the consistency of an estimator $\hm$ of 
the regression function $m_{\star}(z) = \E[Y \mid Z=z]$. And with appropriate tuning, this simplifies.

\begin{coro}
Under the assumptions of Theorem~\ref{theo:donsker-consistency}, 
taking $\lambda=c\eta_Mr^2$ for any constant $c > 2$,  
$\norm{\hm-m}_{\F} \lesssim \alpha$, 
$\norm{\hm - m}_{L_2(\P)} \lesssim \alpha r$,
and $\norm{\hm - m}_{L_2(\Pn)} \lesssim \sqrt{\eta_M \alpha}r$ 
for $\alpha = \max(\eta_M/\eta_Q, \norm{m_{\star}})$.
\end{coro} 

The first condition in \eqref{eq:least-squares-ratio-process},
a uniform lower bound, holds with high probability for $r$ satisfying 
$R_n(\F_{c_0r}) \le c_1 r^2/\sup_{f \in \F}\norm{f}_{\infty}$ for constants $c_0$ and $c_1$ dependent on $\eta_Q$,
and variants apply to unbounded classes \citep{mendelson2017extending}.
The second, if the `noise' $\xi_i = Y_i - m_{\star}(Z_i)$ is in some bounded interval $[-b,b]$,
holds with high probability if $R_n(\F_{r}) \le \eta_M r^2/(2b)$
by symmetrization and contraction, and similar claims hold for unbounded but relatively well behaved noise
via multiplier inequalities \cite[see e.g.,][Section 3.14]{gine2015mathematical}.



\begin{proof}
The Hilbert space projection theorem implies that the minimizer $m_{\star}$ exists
and satisfies $\P(Y-m_{\star})(m-m_{\star}) \le 0$ for all $m \in \mm$ \citep[Proposition 1.37]{peypouquet2015convex}.
We use this property to lower bound the excess loss $\ell(m) - \ell(m_{\star})$. 
\begin{align*}
\ell(m) - \ell(m_{\star}) 
&= \Pn [(Y - m)^2 - (Y - m_{\star})^2] + \lambda (\norm{m}_{\F}- \norm{m_{\star}}_{\F}) \\
&= \Pn (m-m_{\star})^2 - 2\Pn (Y-m_{\star})(m-m_{\star}) + \lambda (\norm{m}_{\F}- \norm{m_{\star}}_{\F}) \\
&\ge \Pn (m-m_{\star})^2 + 2(\P - \Pn)(Y-m_{\star})(m-m_{\star})  + \lambda (\norm{m}_{\F} - \norm{m_{\star}}_{\F}) \\
&\ge \Pn (m-m_{\star})^2 - 2\abs{(\P - \Pn)(Y-m_{\star})(m-m_{\star})}  + \lambda (\norm{m-m_{\star}}_{\F} - 2 \norm{m_{\star}}_{\F}).
\end{align*}
In the last step, we use the triangle inequality bound $\norm{a-b} \ge \norm{a} - \norm{b}$ for $a=m-m_{\star}$ and $b=-m_{\star}$.
The last bound is a function of $\delta = m-m_{\star}$, which we will call $\ell'(\delta)$.

Our argument will be based on scaled versions of our assumed bounds \eqref{eq:least-squares-ratio-process}.
Using the scale invariance of the first and Lemma~\ref{lemm:rescaling} for the second, for all $f \in \alpha \F$,
\begin{equation}
\label{eq:erm-ratio-bounds-scaled}
\begin{aligned}
& \Pn f^2 \ge \eta_Q \P f^2                                  & \text{ if }\ \P f^2 \ge (\alpha r)^2, \\ 
&\abs{(\P-\Pn) (Y-m_{\star})f} \le \eta_M \P f^2 / \alpha    & \text{ if }\ \P f^2 \ge (\alpha r)^2, \\
&\abs{(\P-\Pn) (Y-m_{\star})f} \le \eta_M \alpha r^2         & \text{ if }\ \P f^2 \le (\alpha r)^2. \\
\end{aligned}
\end{equation}

We will show that $\ell'(f) > 0$ if $\norm{f}_{\F} \ge \alpha$. 
It suffices to do so for $\norm{f}_{\F}=\alpha$, as if $\norm{f}_{\F} \ge \alpha$,
$f=tg$ for $\norm{g}_{\F} = \alpha$ and $t \ge 1$,
and $\ell'(f) = \ell'(tg) \ge t\ell'(g)$, as  $\ell'(tg) - t\ell'(g)= (t^2-t)\Pn g^2 - 2\lambda (1-t)\norm{m_{\star}}_{\F}$.
When $\norm{f}_{\F}=\alpha$, 
$\ell'(f) \ge (\eta_Q - 2\eta_M/\alpha) \P f^2 +  \lambda (\alpha - 2 \norm{m_{\star}}_{\F})$ if $\P f^2 \ge (\alpha r)^2$
and $\ell'(f) \ge -2\eta_M\alpha r^2 + \lambda (\alpha - 2 \norm{m_{\star}}_{\F})$ otherwise,
so it will be positive if $\eta_Q - 2\eta_M/\alpha \ge 0$ and  $-2\eta_M\alpha r^2 + \lambda (\alpha - 2 \norm{m_{\star}}_{\F}) > 0$.
This holds for $\alpha \ge 2\max\set{\eta_M/\eta_Q, \norm{m_{\star}}_{\F}/(1-2\eta_M r^2/\lambda)}$.

We will now consider the case that $\norm{f}_{\F} \le \alpha$. In this case,
$\ell'(f) \ge (\eta_Q - 2\eta_M/\alpha) \P f^2 -  2 \lambda \norm{m_{\star}}_{\F}$ if $\P f^2 \ge (\alpha r)^2$
and $\ell'(f) \ge \Pn  f^2 - 2\eta_M\alpha r^2 - 2 \lambda \norm{m_{\star}}_{\F}$ otherwise. 
Thus, $\ell'(f) > 0$ if $\P f^2 > \max\set{(\alpha r)^2, 2 \lambda \norm{m_{\star}}_{\F}/(\eta_Q - 2\eta_M/\alpha)}$
or if $\P f^2 \le (\alpha r)^2$ and $\Pn f^2 > 2(\eta_M \alpha r^2 + \lambda \norm{m_{\star}}_{\F})$.
And when $(\alpha r)^2 \ge 2 \lambda \norm{m_{\star}}_{\F}/(\eta_Q - 2\eta_M/\alpha)$, 
and therefore for $\alpha \ge 2\eta_M/\eta_Q + \sqrt{2\lambda \norm{m_{\star}}_{\F}/\eta_Q r^2}$,
this conclusion simplifies to $\ell'(f) > 0$ if $\P f^2 > (\alpha r)^2$
or if $\Pn f^2 > 2(\eta_M \alpha r^2 + \lambda \norm{m_{\star}}_{\F})^2$.
Our claim follows, as $\ell'(\hm-m) \le 0$.
\end{proof}

