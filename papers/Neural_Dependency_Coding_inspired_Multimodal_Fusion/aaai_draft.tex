\def\year{2022}\relax
%File: formatting-instructions-latex-2021.tex
%release 2021.1
\documentclass[letterpaper]{article} % DO NOT CHANGE THIS
\usepackage{aaai22}  % DO NOT CHANGE THIS
\usepackage{times}  % DO NOT CHANGE THIS
\usepackage{helvet} % DO NOT CHANGE THIS
\usepackage{courier}  % DO NOT CHANGE THIS
\usepackage[hyphens]{url}  % DO NOT CHANGE THIS
\usepackage{graphicx} % DO NOT CHANGE THIS
\urlstyle{rm} % DO NOT CHANGE THIS
\def\UrlFont{\rm}  % DO NOT CHANGE THIS
\usepackage{natbib}  % DO NOT CHANGE THIS AND DO NOT ADD ANY OPTIONS TO IT
\usepackage{caption} % DO NOT CHANGE THIS AND DO NOT ADD ANY OPTIONS TO IT

\usepackage[switch]{lineno}  %

\frenchspacing  % DO NOT CHANGE THIS
\setlength{\pdfpagewidth}{8.5in}  % DO NOT CHANGE THIS
\setlength{\pdfpageheight}{11in}  % DO NOT CHANGE THIS
%\nocopyright
%PDF Info Is REQUIRED.
% For /Author, add all authors within the parentheses, separated by commas. No accents or commands.
% For /Title, add Title in Mixed Case. No accents or commands. Retain the parentheses.
\pdfinfo{
/Title ( Neural Dependency Coding inspired Multimodal Fusion)
/Author (Shiv Shankar)
/TemplateVersion (2021.1)
} %Leave this
% /Title ()
% Put your actual complete title (no codes, scripts, shortcuts, or LaTeX commands) within the parentheses in mixed case
% Leave the space between \Title and the beginning parenthesis alone
% /Author ()
% Put your actual complete list of authors (no codes, scripts, shortcuts, or LaTeX commands) within the parentheses in mixed case.
% Each author should be only by a comma. If the name contains accents, remove them. If there are any LaTeX commands,
% remove them.

% Get better typography
% \usepackage[protrusion=true,expansion=true]{microtype}		

% Small margins
%\usepackage[top=2cm, bottom=2cm, left = 1cm, right = 1cm,columnsep=20pt]{geometry}


% For basic math, align, fonts, etc.
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amssymb}
\usepackage{bbm}
\usepackage{mathtools}
\usepackage{mathrsfs}
\usepackage{dsfont}
\mathtoolsset{showonlyrefs}

\newtheorem{defn}{Definition}[]
\newtheorem{thm}{Theorem}[]
\newtheorem{claim}{Claim}[]
\newtheorem{lemma}{Lemma}[]
\newtheorem{prop}{Property}[]
\newtheorem{ass}{Assumption}[]
\newtheorem{cor}{Corollary}[]
\newtheorem{rem}{Remark}[]


% For creating sub-groups of Assumptions/Theorems etc.
\newtheorem{assumpA}{Assumption}
\renewcommand\theassumpA{A\arabic{assumpA}}

\usepackage{courier} % For \texttt{foo} to put foo in Courier (for code / variables)

\usepackage{lipsum} % For dummy text

% For images
% \usepackage{graphicx}
% \usepackage{subcaption}
\usepackage[space]{grffile} % For spaces in image names

% For bibliography
% \usepackage[round]{natbib}

% For links (e.g., clicking a reference takes you to the phy)
% \usepackage{hyperref}

\usepackage{theoremref}
% For color
\usepackage{graphicx}
\usepackage{wrapfig}
\usepackage{xcolor}
%\usepackage{equation}
\usepackage{colortbl}
\definecolor{dark-red}{rgb}{0.4,0.15,0.15}
\definecolor{dark-blue}{rgb}{0,0,0.7}
% \hypersetup{
%     colorlinks, linkcolor={dark-blue},
%     citecolor={dark-blue}, urlcolor={dark-blue}
% }


% Macros
\newcommand{\todo}[1]{\textcolor{red}{[TODO: #1]}}
% \newcommand{\topic}[1]{\textcolor{violet}{[Topic: #1]}}
\newcommand{\topic}[1]{\textcolor{violet}{}}


\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\argmax}{arg\,max}


\usepackage{algpseudocode}
% \usepackage[noend,linesnumbered,ruled]{algorithm2e}
\usepackage[vlined,linesnumbered,ruled,algo2e]{algorithm2e}
% \usepackage[algo2e]{algorithm2e} 
\SetKwProg{Fn}{Function}{}{}
\let\oldnl\nl% Store \nl in \oldnl
\newcommand{\nonl}{\renewcommand{\nl}{\let\nl\oldnl}}% Remove line number for one line

% Miscelaneous headers
\usepackage{multicol}
\usepackage{enumitem}
\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
% \usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{nicefrac}       % compact symbols for 1/2, etc.

% DISALLOWED PACKAGES
% \usepackage{authblk} -- This package is specifically forbidden
% \usepackage{balance} -- This package is specifically forbidden
% \usepackage{color (if used in text)
% \usepackage{CJK} -- This package is specifically forbidden
% \usepackage{float} -- This package is specifically forbidden
% \usepackage{flushend} -- This package is specifically forbidden
% \usepackage{fontenc} -- This package is specifically forbidden
% \usepackage{fullpage} -- This package is specifically forbidden
% \usepackage{geometry} -- This package is specifically forbidden
% \usepackage{grffile} -- This package is specifically forbidden
% \usepackage{hyperref} -- This package is specifically forbidden
% \usepackage{navigator} -- This package is specifically forbidden
% (or any other package that embeds links such as navigator or hyperref)
% \indentfirst} -- This package is specifically forbidden
% \layout} -- This package is specifically forbidden
% \multicol} -- This package is specifically forbidden
% \nameref} -- This package is specifically forbidden
% \usepackage{savetrees} -- This package is specifically forbidden
% \usepackage{setspace} -- This package is specifically forbidden
% \usepackage{stfloats} -- This package is specifically forbidden
% \usepackage{tabu} -- This package is specifically forbidden
% \usepackage{titlesec} -- This package is specifically forbidden
% \usepackage{tocbibind} -- This package is specifically forbidden
% \usepackage{ulem} -- This package is specifically forbidden
% \usepackage{wrapfig} -- This package is specifically forbidden
% DISALLOWED COMMANDS
% \nocopyright -- Your paper will not be published if you use this command
% \addtolength -- This command may not be used
% \balance -- This command may not be used
% \baselinestretch -- Your paper will not be published if you use this command
% \clearpage -- No page breaks of any kind may be used for the final version of your paper
% \columnsep -- This command may not be used
% \newpage -- No page breaks of any kind may be used for the final version of your paper
% \pagebreak -- No page breaks of any kind may be used for the final version of your paperr
% \pagestyle -- This command may not be used
% \tiny -- This is not an acceptable font size.
% \vspace{- -- No negative value may be used in proximity of a caption, figure, table, section, subsection, subsubsection, or reference
% \vskip{- -- No negative value may be used to alter spacing above or below a caption, figure, table, section, subsection, subsubsection, or reference

\setcounter{secnumdepth}{0} %May be changed to 1 or 2 if section numbers are desired.

% The file aaai22.sty is the style file for AAAI Press
% proceedings, working notes, and technical reports.
%

% Title

% Your title must be in mixed case, not sentence case.
% That means all verbs (including short verbs like be, is, using,and go),
% nouns, adverbs, adjectives should be capitalized, including both words in hyphenated terms, while
% articles, conjunctions, and prepositions are lower case unless they
% directly follow a colon or long dash
\iffalse
%Example, Single Author, ->> remove \iffalse,\fi and place them surrounding AAAI title to use it
\title{My Publication Title --- Single Author}
\author {
    % Author
    Author Name \\
}

\affiliations{
    Affiliation \\
    Affiliation Line 2 \\
    name@example.com
}
\fi

% \iffalse
%Example, Multiple Authors, ->> remove \iffalse,\fi and place them surrounding AAAI title to use it
\title{ Neural Dependency Coding inspired Multimodal Fusion}
\author {
    % Authors
        
       % Shiv Shankar, \,\, Ina Fiterau\\
        Shiv Shankar
}
\affiliations {
    % Affiliations
    %University of Massachusetts \\
    \{sshankar\}@cics.umass.edu
    % \\\textcolor{red}{Working Draft}
}
% \fi

% REMOVE THIS: bibentry
% This is only needed to show inline citations in the guidelines document. You should not need it and can safely delete it.
%\usepackage{bibentry}
% END REMOVE bibentry

\begin{document}

\maketitle


\begin{abstract}
Information integration from different modalities is an active area of research. Human beings and, in general, biological neural systems are quite adept at using a multitude of signals from different sensory perceptive fields to interact with the environment and each other.
Recent work in deep fusion models via neural networks has led to substantial improvements over unimodal approaches in areas like speech recognition, emotion recognition and analysis, captioning and image description. However, such research has mostly focused on architectural changes allowing for fusion of different modalities while keeping the model complexity manageable.
Inspired by recent neuroscience ideas about multisensory integration and processing, we investigate the effect of introducing neural dependencies in the loss functions. Experiments on multimodal sentiment analysis tasks:  CMU-MOSI and CMU-MOSEI with different models show that our approach provides a consistent performance boost.
\end{abstract}

\section{Introduction}


Human beings perceive the world as a unified whole, not in individual sensory modalities. While traditionally different sensory models have been studied in isolation, it has been well recognized that perception operates via the integration of information from multiple sensory modalities. Research in multimodal fusion aims to achieve a similar goal in artificial models: integrate different unimodal representations into a unified common representation.



Heterogeneities across different modalities mean that learning multimodal representations face challenges like feature shifts, distributional effects, nuisance variation and other related challenges \citep{baltruvsaitis2018multimodal}. Effective fusion still is an open and challenging task. While in some areas like opinion analysis \citep{garcia2019multimodal, soleymani2017survey}, speech recognition, image processing \citep{xu2015multi, xu2013survey} and trait analysis \citep{park2014computational} fusion models have achieved substantial improvements over their unimodal counterparts ;in other areas like sentiment analysis \cite{rahman2020integrating} the improvements have been uninspiring.

Current research in deep multimodal fusion primarily deals with architectural improvements to create complex feature rich yet efficient representations \cite{zadeh2017tensor, liu2018efficient, hazarika2020misa}. Recently \cite{rahman2020integrating} used pre-trained transformer \citep{tsai2019multimodal, siriwardhana2020jointly} based models to achieve state-of the-art results on multimodal sentiment benchmark MOSI \cite{wollmer2013youtube} and MOSEI \cite{zadeh2018multimodal}.

Unlike these prior works, we do not focus not on architectural changes. Inspiring from work in multisensory neural processing, we utilize a concept known as \emph{synergy} or \emph{dependency} to train these models. Synergy is an information-theoretic concept related to mutual information (MI). The synergy between random variables $X$ and $Y$ refers to the unique mutual information that $X$ provides about $Y$. As it is based on conditional mutual information, synergy is able to capture relations between modalities that are difficult to capture via techniques like subspace alignment which implicitly assume linear dependencies.









%studied multimodal information encoding in a small sensorimotor system of the crustacean stomatogastric nervous system that drives rhythmic motor activity for the processing of food. This system is experimentally advantageous, as it produces a fictive behavioral output in vitro, and distinct sensory modalities can be selectively activated. It has the additional advantage that all sensory information is routed through a hub ganglion, the commissural ganglion, a structure with fewer than 220 neurons.
%Using optical imaging of a population of commissural neurons to track each individual neuron's response across sensory modalities, we provide evidence that multimodal information is encoded via a combinatorial code of recruited neurons. By selectively stimulating chemosensory and mechanosensory inputs that are functionally important for processing of food, we find that these two modalities were processed in a distributed network comprising the majority of commissural neurons imaged. In a total of 12 commissural ganglia, we show that 98% of all imaged neurons were involved in sensory processing, with the two modalities being processed by a highly overlapping set of neurons. Of these, 80% were multimodal, 18% were unimodal, and only 2% of the neurons did not respond to either modality. 
%Differences between modalities were represented by the identities of the neurons participating in each sensory condition and by differences in response sign (excitation versus inhibition), with 46% changing their responses in the other modality. Consistent with the hypothesis that the commissural network encodes different sensory conditions in the combination of activated neurons, a new combination of excitation and inhibition was found when both pathways were activated simultaneously. The responses to this bimodal condition were distinct from either unimodal condition, and for 30% of the neurons, they were not predictive from the individual unimodal responses. Thus, in a sensorimotor network, different sensory modalities are encoded using a combinatorial code of neurons that are activated or inhibited









%Now, it could be the case that the neural convergence that results in multisensory neurons is set up in a way that ignores the locations of the input neurons’ receptive fields. Amazingly, however, these crossmodal receptive fields overlap. For example, a multisensory neuron in the superior colliculus might receive input from two unimodal neurons: one with a visual receptive field and one with an auditory receptive field. It has been found that the unimodal receptive fields refer to the same locations in space—that is, the two unimodal neurons respond to stimuli in the same region of space. Crucially, the overlap in the crossmodal receptive fields plays a vital role in the integration of crossmodal stimuli. When the information from the separate modalities is coming from within these overlapping receptive fields, then it is treated as having come from the same location—and the neuron responds with a superadditive (enhanced) response. So, part of the information that is used by the brain to combine multimodal inputs is the location in space from which the stimuli came.

%This pattern is common across many multisensory neurons in multiple regions of the brain. Because of this, researchers have defined the spatial principle of multisensory integration: Multisensory enhancement is observed when the sources of stimulation are spatially related to one another. A related phenomenon concerns the timing of crossmodal stimuli. Enhancement effects are observed in multisensory neurons only when the inputs from different senses arrive within a short time of one another (e.g., Recanzone, 2003).
%Multimodal Processing in Unimodal Cortex

%Multisensory neurons have also been observed outside of multisensory convergence zones, in areas of the brain that were once thought to be dedicated to the processing of a single modality (unimodal cortex). For example, the primary visual cortex was long thought to be devoted to the processing of exclusively visual information. The primary visual cortex is the first stop in the cortex for information arriving from the eyes, so it processes very low-level information like edges. Interestingly, neurons have been found in the primary visual cortex that receives information from the primary auditory cortex (where sound information from the auditory pathway is processed) and from the superior temporal sulcus (a multisensory convergence zone mentioned above). This is remarkable because it indicates that the processing of visual information is, from a very early stage, influenced by auditory information.


%Empirical evidence demonstrate that a stimulus for local excitatory neurons at a cellular level can be etiologically associated with large-scale brain activity, which may propagate through numerous neuronal interconnections (Beggs and Plenz, 2003; Beggs, 2008; Lee et al., 2010; Fenno et al., 2011; Tagliazucchi et al., 2012). Over the years, studying task evoked brain activity via whole-brain imaging has been successful in mapping specific cognitive functions onto distinct regions of the human brain (e.g., Kanwisher et al., 1997).





%multisensory integration is not yet fully understood, hence, questions remain: How does the brain integrate the incoming multisensory signals with respect to different external environments? How are the roles of these multisensory signals defined to adhere to the anticipated behavioral-constraint of the environment?

%multisensory integration (CMI) that addresses the aforementioned research challenges. Specifically, the well-established contextual field (CF) in pyramidal cells and coherent infomax theory (Kay et al., 1998; Kay and Phillips, 2011) is split into two functionally distinctive integrated input fields: local contextual field (LCF) and universal contextual field (UCF). LCF defines the modulatory sensory signal coming from some other parts of the brain (in principle from anywhere in space-time) and UCF defines the outside environment and anticipated behavior (based on past learning and reasoning). Both LCF and UCF are integrated with the receptive field (RF) to develop a new class of contextually-adaptive neuron (CAN), which adapts to changing environments. 













%The physiological studies in Phillips and Singer (1997) have suggested that biological neurons, in addition to the classical excitatory and inhibitory signals, do receive contextual inputs. These contextual inputs possibly fulfill the gain-controlling RC role (Fox and Daw, 1992). The authors in Kepecs and Raghavachari (2002) used a two-compartment model of pyramidal neurons to capture the spatial extent of neuronal morphology. Their study simulated three neurons, each receiving the same α-amino-3-hydroxy-5-methyl-4-isoxazolepropionic acid (AMPA), representing the informational input i.e., a word “green.” The three neurons also received distinct contextual input via NMDA receptors, representing specific noun groups: objects, people and fruits. It is to be noted that the word “green,” when expressed with a contextual input, varies in meaning, e.g., a color green or an unripe fruit. The simulation results showed that even though each neuron received the same strong AMPA input, their firing was uncorrelated and context-dependent.

%An overlay of a coherent infomax neural processor on layer 5 pyramidal cells is shown in Figure 3A (Wibral et al., 2017), highlighting potential parallels to existing physiological mechanisms. In two sites of integration, one is at the soma and the other at the top of the apical trunk. The driving excitatory (Re) or inhibitory (Ri) signals arrive via basal and perisomatic synapses, whereas the modulatory excitatory (Ce) or inhibitory (Ci) signals arrive via synapses on the tuft dendrites at the top of the apical trunk. Na+ at the somatic integration site initiates sodium spikes that backpropagate up to the apical trunk. Ca2+ at the apical integration site initiates calcium spikes, which amplify the neural response (Phillips et al., 2015).



%follman 
%Multimodal sensory information is represented by a combinatorial code 


%A ubiquitous feature of the nervous system is the processing of simultaneously arriving sensory inputs from different modalities

%studied multimodal information encoding in a small sensorimotor system of the crustacean stomatogastric nervous system that drives rhythmic motor activity for the processing of food. This system is experimentally advantageous, as it produces a fictive behavioral output in vitro, and distinct sensory modalities can be selectively activated. It has the additional advantage that all sensory information is routed through a hub ganglion, the commissural ganglion, a structure with fewer than 220 neurons. Using optical imaging of a population of commissural neurons to track each individual neuron's response across sensory modalities, we provide evidence that multimodal information is encoded via a combinatorial code of recruited neurons. By selectively stimulating chemosensory and mechanosensory inputs that are functionally important for processing of food, we find that these two modalities were processed in a distributed network comprising the majority of commissural neurons imaged. In a total of 12 commissural ganglia, we show that 98% of all imaged neurons were involved in sensory processing, with the two modalities being processed by a highly overlapping set of neurons. Of these, 80% were multimodal, 18% were unimodal, and only 2% of the neurons did not respond to either modality. Differences between modalities were represented by the identities of the neurons participating in each sensory condition and by differences in response sign (excitation versus inhibition), with 46% changing their responses in the other modality. Consistent with the hypothesis that the commissural network encodes different sensory conditions in the combination of activated neurons, a new combination of excitation and inhibition was found when both pathways were activated simultaneously. The responses to this bimodal condition were distinct from either unimodal condition, and for 30% of the neurons, they were not predictive from the individual unimodal responses. Thus, in a sensorimotor network, different sensory modalities are encoded using a combinatorial code of neurons that are activated or inhibited









%Now, it could be the case that the neural convergence that results in multisensory neurons is set up in a way that ignores the locations of the input neurons’ receptive fields. Amazingly, however, these crossmodal receptive fields overlap. For example, a multisensory neuron in the superior colliculus might receive input from two unimodal neurons: one with a visual receptive field and one with an auditory receptive field. It has been found that the unimodal receptive fields refer to the same locations in space—that is, the two unimodal neurons respond to stimuli in the same region of space. Crucially, the overlap in the crossmodal receptive fields plays a vital role in the integration of crossmodal stimuli. When the information from the separate modalities is coming from within these overlapping receptive fields, then it is treated as having come from the same location—and the neuron responds with a superadditive (enhanced) response. So, part of the information that is used by the brain to combine multimodal inputs is the location in space from which the stimuli came.

%This pattern is common across many multisensory neurons in multiple regions of the brain. Because of this, researchers have defined the spatial principle of multisensory integration: Multisensory enhancement is observed when the sources of stimulation are spatially related to one another. A related phenomenon concerns the timing of crossmodal stimuli. Enhancement effects are observed in multisensory neurons only when the inputs from different senses arrive within a short time of one another (e.g., Recanzone, 2003).
%Multimodal Processing in Unimodal Cortex

%Multisensory neurons have also been observed outside of multisensory convergence zones, in areas of the brain that were once thought to be dedicated to the processing of a single modality (unimodal cortex). For example, the primary visual cortex was long thought to be devoted to the processing of exclusively visual information. The primary visual cortex is the first stop in the cortex for information arriving from the eyes, so it processes very low-level information like edges. Interestingly, neurons have been found in the primary visual cortex that receives information from the primary auditory cortex (where sound information from the auditory pathway is processed) and from the superior temporal sulcus (a multisensory convergence zone mentioned above). This is remarkable because it indicates that the processing of visual information is, from a very early stage, influenced by auditory information.



%Examples of premotor multisensory integration come from single-neuron studies in the superior colliculus [8,15–18], showing that multiple sensory modalities are processed in a distributed fashion throughout this brain stem region, with some neurons being exclusively unimodal and other being multimodal. Hypotheses of encoding of multimodal information include changes in neuronal firing rates (e.g., a rate code) [19], activation of distinct network components, or distinct activation and inhibition of neurons within a shared population (e.g., a combinatorial code) [20,21].



%multisensory integration is not yet fully understood, hence, questions remain: How does the brain integrate the incoming multisensory signals with respect to different external environments? How are the roles of these multisensory signals defined to adhere to the anticipated behavioral-constraint of the environment?

%multisensory integration (CMI) that addresses the aforementioned research challenges. Specifically, the well-established contextual field (CF) in pyramidal cells and coherent infomax theory (Kay et al., 1998; Kay and Phillips, 2011) is split into two functionally distinctive integrated input fields: local contextual field (LCF) and universal contextual field (UCF). LCF defines the modulatory sensory signal coming from some other parts of the brain (in principle from anywhere in space-time) and UCF defines the outside environment and anticipated behavior (based on past learning and reasoning). Both LCF and UCF are integrated with the receptive field (RF) to develop a new class of contextually-adaptive neuron (CAN), which adapts to changing environments. 








%Empirical evidence demonstrate that a stimulus for local excitatory neurons at a cellular level can be etiologically associated with large-scale brain activity, which may propagate through numerous neuronal interconnections (Beggs and Plenz, 2003; Beggs, 2008; Lee et al., 2010; Fenno et al., 2011; Tagliazucchi et al., 2012). Over the years, studying task evoked brain activity via whole-brain imaging has been successful in mapping specific cognitive functions onto distinct regions of the human brain (e.g., Kanwisher et al., 1997).





%The physiological studies in Phillips and Singer (1997) have suggested that biological neurons, in addition to the classical excitatory and inhibitory signals, do receive contextual inputs. These contextual inputs possibly fulfill the gain-controlling RC role (Fox and Daw, 1992). The authors in Kepecs and Raghavachari (2002) used a two-compartment model of pyramidal neurons to capture the spatial extent of neuronal morphology. Their study simulated three neurons, each receiving the same %α-amino-3-hydroxy-5-methyl-4-isoxazolepropionic acid (AMPA), representing the informational input i.e., a word “green.” The three neurons also received distinct contextual input via NMDA receptors, representing specific noun groups: objects, people and fruits. It is to be noted that the word “green,” when expressed with a contextual input, varies in meaning, e.g., a color green or an unripe fruit. The simulation results showed that even though each neuron received the same strong AMPA input, their firing was uncorrelated and context-dependent.

%An overlay of a coherent infomax neural processor on layer 5 pyramidal cells is shown in Figure 3A (Wibral et al., 2017), highlighting potential parallels to existing physiological mechanisms. In two sites of integration, one is at the soma and the other at the top of the apical trunk. The driving excitatory (Re) or inhibitory (Ri) signals arrive via basal and perisomatic synapses, whereas the modulatory excitatory (Ce) or inhibitory (Ci) signals arrive via synapses on the tuft dendrites at the top of the apical trunk. Na+ at the somatic integration site initiates sodium spikes that backpropagate up to the apical trunk. Ca2+ at the apical integration site initiates calcium spikes, which amplify the neural response (Phillips et al., 2015).




%The hypothesis reviewed in Phillips et al. (2015) suggests that this is mostly achieved by a widely distributed process of contextual modulation, which amplifies and suppresses the transmission of signals that are relevant and irrelevant to current circumstances, respectively. Nevertheless, selective modulation (amplification/attenuation) of incoming multisensory information with respect to the outside world is poorly understood



%At a granular level, evidence gathered in the literature suggests that the multisensory interaction emerges at the primary cortical level (Stein and Stanford, 2008; Stein et al., 2009). The divisive/multiplicative gain modulations are widely spread in mammalian neocortex with an indication of amplification or attenuation via contextual modulation (Galletti and Battaglini, 1989; Salinas and Sejnowski, 2001; Phillips et al., 2018).



%multimodal integration jerath

%the thalamus and the thalamocortical oscillations are additional key coordinators of oscillatory activity among the cortex and among the cortex and sensory receptors [our novel proposition (Jerath et al., 2016)], serving to coordinate the binding of multi-modal qualia to the sensory frameworks and these frameworks to the underlying virtual space. In this way, expectations may directly modulate the activity of sensory receptors (Jerath and Beveridge, 2018). There is increasing evidence that the thalamus may integrate different sensory stimuli and influence cortical multimodal processing via corticothalamic connections (Tyll et al., 2011). The sensory specific nuclei of the thalamus have also been evidenced to integrate different modalities and feed this multisensory information to primary sensory specific-cortices (Noesselt et al., 2007; Kayser et al., 2008). As the thalamus has vast reentrant and resonant connections with the cortex (Jones, 2007), it is a prime candidate for an integrator of diverse information across widely dispersed cortical sites (Cappe et al., 2012) providing a means of large-scale, simultaneous synchronized events which may conjoin in time all sensory inputs (Llinás et al., 2002). We assert the thalamus in tandem with the ultra-slow oscillations provide the means for metastable operations of binding and global unity of neural activity to occur by coordinating the integration of the vast and varied mental operations creating qualia into the virtual 3D matrix




%---------------------


%can induce crossmodal changes within putative unimodal sensory cortex even in the absence of its archetypal substrate. Our results also suggest that multimodal learning is associated with network wide activity within the conditioned neural system. 


%Cortical Processing of Multimodal Sensory Learning in Human Neonates





\section{Preliminaries}
In this section, we give an overview of mutual information and existing work on deep multimodal fusion.

\subsection{Multimodal Fusion}

The problem in the most abstract terms is a supervised learning problem. We are provided with a dataset of $N$ observations $\mathcal{D} = { (x_i,y_i)_{i=1}^{N}} $. All $x_i$ come from a space $\mathcal{X}$ and $y_i$ from $\mathcal{Y}$. We are provided a loss function $L : \mathcal{Y} \times \mathcal{Y} \rightarrow \mathbb{R}$ which is the task loss. Our goal is to learn a model $\mathcal{F}_{\theta}: \mathcal{X} \rightarrow \mathcal{Y}$ such that the total loss  $\mathcal{L} =  \sum_i L(\mathcal{F}(x_i),y_i)$ is minimized.
In multimodal fusion the space of inputs $\mathcal{X}$ naturally decomposes into $K$ different modalities $\mathcal{X} = \prod_{j=1}^K \mathcal{X}_j$.  We use $X_j$ to represent random variables which form the individual modality specific components of the input random variable $X$.

The standard way to learn such a multimodal function is to decompose it into two components: a) an embedding component $E$ which fuses information into a high dimensional vector in $\mathbb{R}^d$ and b) a predictive component $P$ which maps vector from  $\mathbb{R}^d$ to $\mathcal{Y}$. Furthermore since the different modalities are often no directly compatible with each other (for eg text and image), $E$ itself is decomposed into a) modality specific readers $F_i \mathcal{X}_i \rightarrow \mathbb{R}^{d_i}$ which are specifically designed for each individual modality $\mathcal{X}_i$ and b) a fusion component $F : \prod_i \mathbb{R}^{d_i} \rightarrow \mathbb{R}^d$ which fuses information from eah individual modality embedding. $F$ is provided with uni-modal representations of the inputs $X_i = (X_1,X_2, \dots X_K)$ obtained through embedding networks $f_i$. $F$ has to retain both unimodal dependencies (i.e relations between features that span only one modality) and multi-modal dependency (i.e relationships between features across multiple modalities).

This decomposition has two advantages a) the individual modality reader can be pre-trained on the task at hand or even from a larger dataset (for example  BERT \citep{devlin2018bert} for language, Resnet \citep{he2016identity} for images ) which allows us to leverage wider modality specific information and b) often but not always each individual modality is in principle enough to correctly predict the output


Plethora of neural architectures have been proposed to learn multimodal representations for sentiment classification. Models often rely on a fusion mechanism \citep{khan2012color}, tensor factorisation \citep{liu2018efficient, zadeh2019factorized} or complex attention mechanisms \citep{zadeh2018memory} that is fed with modality specific representations. 


\subsection{Mutual Information}
Mutual information (MI) measures the dependence between two random variable $X$ and $Y$ and is able to incorporate all forms of relationships between the two. MI between $X$ and $Y$, is given by 
$$
I(X ; Y) = \mathbb{E}_{X Y}\left[\log \frac{p_{X Y}(x, y)}{p_{X}(x) p_{Y}(y)}\right]
$$
where $p_{X Y}$ is the joint probability density of the pair $(X, Y)$, and $p_{X}, p_{Y}$ are the marginal probability densities of $X,Y$ respectively \cite{cover1999elements}. It is also clear from the above expression that it equals the KL divergence of the joint density of $(X, Y)$  relative to the product of marginals of $X$ and $Y$.
%
\begin{align}
I(X ; Y) = KL\left[ p_{X Y}(x, y) \| p_{X}(x) p_{Y}(y)\right ] {\label{eqn:MIKL}}
\end{align}
%
Since under the assumption of independence, the distribution of the pair $(X,Y)$ is given by the product of their individual distributions; the MI between $X,Y$ is 0 if and only if $X,Y$ are independent. As such mutual information can be understood as a measure of the inherent dependence expressed in the joint distribution of $X,Y$. 

\paragraph{Estimation of Mutual Information}
The computation of MI and other mutual dependency measures based only on samples is difficult. “Reliably estimating mutual information from finite continuous data remains a significant and unresolved problem” \citep{kinney2014equitability}.
Recently several estimators have been proposed based on optimization of variational bounds with flexible neural networks as proposed witness functions. For example \citet{belghazi2018mine} maximize the Donsker-Varadhan bound \citep{donsker1985large} to estimate KL divergence. 

\subsection{Alternative Dependence Measures}
Given the interpretation of the mutual information between variables as a divergence between their joint distribution and the independence implied distribution, other measures of mutual dependence can and have been proposed by replacing the KL divergence in with other proper discrepancy measures \cite{lopez2018generalization,belghazi2018mine,griffith2014intersection}

We will focus on the Hilbert-Schmidt criteria (also knowns as MMD criteria) which is also used in our experiments
\begin{itemize}
    %\item 
    
    \item
    Hilbert-Schmidt Dependence \citep{gretton2005measuring} is obtained by using the Maximum Mean Discrepancy or MMD \citep{gretton2012mmd} instead of KL divergence in \eqref{eqn:MIKL}. Further extensions to MMD have been developed based on neural networks which provide non-universal but more powerful kernel based tests \citep{liu2020learning}. 
    $$I_{\mathcal{MMD}} = \mathcal{MMD}\left(p_{X Y}(x, y) || p_{X}(x) p_{Y}(y)\right)$$
    
\end{itemize}


\subsection{Neural Synergy}
The aforementioned measures have been developed for the case of two random variables. Extension of mutual information to the multivariate case is an active area of research in information theory \cite{griffith2014quantifying, ince2017measuring}. 
There is a vast literature on partial information decomposition (PID) \cite{williams2010nonnegative} which aims to characterize and analyze different divisions of mutual information into a set of non-negative terms. Several axiomatic frameworks have been proposed and analysed  to analyze the concepts of redundancy \citet{williams2010nonnegative, kolchinsky2019novel} and surprise \cite{ince2017measuring}. Variants of MI based on non-Shannon divergences \cite{james2017multivariate,gacs1973common} are less common but have also been analyzed in the PID framework. We will focus on a specific measure known as dual correlation TSE measure \cite{tononi1994measure}. This has been used by \citet{rosas2019quantifying,barrett2011practical} to analyze neural complexity and brain functional connectivity. It is sometimes also known by the name of synergy in the neuroscience literature \citep{mediano2019measuring} and we shall use TSE and synergy interchangeably



%\subsubsection{Other works on multimodal fusion}


\section{Neural Dependency Coding in Multisensory Processing}
A common and vital feature of nervous systems is the integration of information arriving simultaneously from multiple sensory pathways.
The underlying neural structures have been found to be related in both vertebrates and invertebrates. The classic understanding of this process is that different sensory modalities are processed individually and then combined in various multimodal convergence zones, including cortical and subcortical regions \citep{ghazanfar2006neocortex}, as well as multimodal association areas \citep{rauschecker1995processing}. 
Studies in the superior colliculus \citep{meredith1987determinants} showed that multiple sensory modalities are processed in this brain stem region, with some neurons being exclusively unimodal and others being multimodal. Hypotheses of encoding of multimodal information include changes in neuronal firing rates \citep{pennartz2009identification} or a combinatorial code in population of neurons \citep{osborne2008neural,rohe2016distinct}.




%Empirical evidence demonstrate that a stimulus for local excitatory neurons at a cellular level can be etiologically associated with large-scale brain activity, which may propagate through numerous neuronal interconnections (Beggs and Plenz, 2003; Beggs, 2008; Lee et al., 2010; Fenno et al., 2011; Tagliazucchi et al., 2012). Over the years, studying task evoked brain activity via whole-brain imaging has been successful in mapping specific cognitive functions onto distinct regions of the human brain (e.g., Kanwisher et al., 1997).



%the thalamus and the thalamocortical oscillations are additional key coordinators of oscillatory activity among the cortex and among the cortex and sensory receptors [our novel proposition (Jerath et al., 2016)], serving to coordinate the binding of multi-modal qualia to the sensory frameworks and these frameworks to the underlying virtual space. In this way, expectations may directly modulate the activity of sensory receptors (Jerath and Beveridge, 2018). 



Studies of multisensory collicular neurons suggest that their crossmodal receptive fields (RF) often overlap \cite{spence2004crossmodal}. This pattern is also found in multisensory neurons present in other brain regions. As such, a spatiotemporal hypothesis of multisensory integration has been suggested: superadditive multimodal processing is observed when information from different modalities comes from spatiotemporally overlapping receptive fields \citep{recanzone2003auditory,wallace2004visual, stanford2005evaluating}. Since multimodal cortical neurons are generally downstream of modality-specific regions, the information about RF overlap is present in their input unimodal neural representations. Recent observations have led to the discovery of multimodal neurons in the generally modality-specific regions suggesting that a non-trivial part of the process happens in distributed circuits \citep{schroeder2005multisensory,stein2008multisensory}. \citet{tyll2011thalamic} provide evidence that that cortical multimodal processing is influenced via corticothalamic connections. Moreover, the sensory-specific nuclei of the thalamus have been shown to feed multisensory information to primary sensory specific-cortices \citep{kayser2008visual}


Other evidence also shows that while multimodal representations are distinct from unimodal ones, there is sufficient overlap between the set of neurons that process different sensory modalities. For example, \citet{follmann2018multimodal} show that even in a simple crustacean organism, more than half the neurons in the commissural ganglion are multimodal. Moreover, they show that in 30\% of these multimodal neurons, responses to one modality were predictive of responses to other modalities. Both these facts suggest that the neural representations across different modalities have high information about each other.


Cortical and subcortical networks often contain clusters of strongly connected neurons. Functionally the existence of such cliques imply highly integrated pyramidal cells that handle a disproportionately large amount of traffic \citep{harriger2012rich}. In cortical circuits, around 20\% of the neurons account for 80\% of the information propagation \citep{nigam2016rich, van2011rich}. \citet{timme2016high, faber2019computation} demonstrate that multimodal computation tends to concentrate in such local cortical clusters. They also found significantly elevated synergy in such clusters and that the amount of synergy was proportional to the amount of information flow, suggesting that neural synergy emerges 
where there is more significant cognitive processing and information flow. 

High synergy has also been hypothesized to facilitate inter-circuit communication and intra-circuit processing. Correlated neural representation, especially synergic activity, are indicative of the recurrent oscillations that are believed to underlie multisensory cognition\citep{ hasselmo2002proposed, hernandez2020medial, honey2017switching}. \citet{fries2015rhythms, yu2008small} show the importance of synergy for organizing information in cortical circuits. The synchronization of neuronal circuits is related to higher-order processing \citep{vinck2015arousal} especially in thalamic and cortical circuits. Moreover, \citet{sherrill2021synergistic} recently have shown that recurrent information flow in cortical circuits leads to an increase in neural synergy and neural complexity. 

\citet{sherrill2020correlated} show that correlated activity and neural synergy were positively related when multiple external correlated stimuli are provided. Thus, synergy in neural firings is a representation of the similarity between inputs. However, synergy is also low when the stimuli are very highly correlated. This suggests that correlated neural firings are a means of combining both redundancy and complementarity when faced with multiple inputs. This also points to the possibility that multimodal perception is most efficient when inputs produce maximal synergy. 




\section{Model}

For our purposes we will limit ourselves to talk about tasks similar to the MOSI dataset. In this setting the input has three modalities viz audio ($a$), visal ($v$) and text ($t$) The fusion problem involves learning a representation $E_{\theta}$ that combined the uni-modal representations of the inputs $X_{a, v, t}=(X_{a}, X_{v}, X_{t})$.

To train our neural architecture we need to estimate the previously defined synergy measures. Multivarite synergy is defined in terms of mutual information between different random variables in the combined distribution. We extend synergy to also include an MMD based measure of dependence as well as defined earlier. For estimation of these dependence measures, we use estimators of the following distribution divergences for this purpose in our experiments. 

\begin{itemize}
    \item Kullback Liebler Dependence/Mutual Information -  We use the method of \citet{belghazi2018mine} who use the Donsker Varadhan bound \citep{donsker1985large,belghazi2018mine} to estimate the KL divergence between the requisite distribution. This version corresponds to the standard definition of synergy.
    \item Neural Hilbert Schmidt Dependence - We use neural kernel augmented MMD \citep{liu2020learning} to estimate the divergence between the requisite joint distributions to estimate synergy.
\end{itemize}
\section{Experiments}

\subsection{Datasets}
We empirically evaluate our methods on two commonly used datasets for multimodal training viz CMU-MOSI and CMU-MOSEI.

CMU-MOSI \citep{wollmer2013youtube} is sentiment prediction tasks on a set of short youtube video clips. CMU-MOSEI \citep{zadeh2018multi} is a similar dataset consisting of around 23k review videos taken from YouTube. The output in both cases is a sentiment score in $[-3,3]$. For each dataset, three modalities are available; audio, video, and language text.


\subsection{Models}
We run our experiments with the following architectures:
\begin{itemize}
    \item Tensor Fusion Network or TFN \cite{zadeh2017tensor} combined information via pooling of a high dimensional tensor representation of multimodal features. More specifically it does a multimodal Hadamard product of the aggregated features with RNN based language features.
    \item Memory Fusion Network or MFN \citep{zadeh2018memory} incorporate gated memory-units to store multiview representations. It then performs an attention augmented readout over the memory units to combine information into a single representation.
    \item MAGBERT \citep{rahman2020integrating} is a transformer based architecture that uses the Wang gate \citep{wang2019words}. The multimodal information is send to the multimodal gate to compute modified embeddings which are passed to a BERT \citep{devlin2018bert} based model.
\end{itemize}


\subsection{Evaluation}
For our experiments we evaluate and report both the Mean Absolute Error (MAE) and the correlation of model predictions with true labels. Existing works \citep{rahman2020integrating} also use the output of the regression model, to predict a positive or negative sentiment on the data, using it as binary classifier. Using the same approach we report the accuracy or our models. In the results Accuracy $Acc_{7}$ denotes accuracy on 7 classes and $Acc_{2}$ denotes the binary accuracy. We also report the correlation of model intensity predictions with true values.


\subsection{Results}


Results on MOSI are presented in Table \ref{tab:mosi} while Table \ref{tab:mosei} present results for MOSEI dataset.
We trained each of the models with the standard cross-entropy loss (labeled as MLE) and with cross-entropy loss regularized with the synergy penalty discussed earlier. On both datasets our model performs improve on their respective baselines; sometimes by more than 3 points.
Also, note that just changing the training loss provides a reasonable improvement over the state-of-the-art model MAGBERT \citep{rahman2020integrating}. This shows that our approach is generalizable across architectures.

Our results also show that in general MMD based models tends to be better than the KL divergence based models. The greater efficacy of MMD based synergy might be due to the inherent behavior of the MMD dependency, which is always well defined, or it might reflect the hardness of information estimation. For example, it is well known that reasonable bounds on standard mutual information are challenging to obtain \citep{kinney2014equitability}; while MMD estimators do not suffer from the entropy estimation issue and are consistent \citep{gretton2012mmd}

Recently \citet{colombo2021improving} has conducted experiments on similar lines. The main differences between the our method and their method are a) our method focuses on synergy terms whereas their proposal is optimizing joint mutual information between different unimodal representations; and b) they experiment with variational measures of information whereas we use exact measures in the MMD criteria. We replicate our experiments with their best performing model and present the results in our Tables \ref{tab:mosi, tab:mosei} with the label $\text{MI}_{Was}$. It is clear that our proposal is better than the Waserstein model used by \citet{colombo2021improving}. Such an approach was also used by \citet{han2021improving}; however their proposal includes architectural changes as well which makes the exact comparison unclear. However these results present further evidence of the utility of some of form of dependence driver optimization for multimodal fusion.


\begin{table}[htb]
\begin{tabular}{|l|llll|}
\hline
% MOSI
   & $Acc_{7}$   & $Acc_{2}$   & MAE  & CORR \\
\hline
\hline
   & & \multicolumn{2}{c}{MFN} &  \\
\hline
MLE & 31.3 & 76.6 & 1.01 & 0.62 \\
MLE+$\text{S}_{KL}$ & 34.5 & 76.9 & 0.94 & 0.65 \\
MLE+$\text{S}_{MMD}$ & 35.9 & 77.4 & 0.95 & 0.66 \\
$\text{MI}_{Was}$ & 35.1 & 77.1 & 0.97 & 0.63 \\
\hline
   & & \multicolumn{2}{c}{LFN} &  \\
\hline
MLE & 31.9 & 76.9 & 1.01 & 0.64 \\
MLE+$\text{S}_{KL}$ & 32.6 & 77.6 & 0.97 & 0.64 \\
MLE+$\text{S}_{MMD}$ & 35.4 & 77.9 & 0.97 & 0.67 \\
$\text{MI}_{Was}$ & 32.4 & 77.6 & 0.97 & 0.64 \\
\hline
   & &\multicolumn{2}{c}{MAGBERT} & \\
\hline
MLE & 40.2 & 83.7 & 0.79 & 0.80 \\
MLE+$\text{S}_{KL}$ & 41.9 & 84.3 & 0.76 & 0.82 \\
MLE+$\text{S}_{MMD}$ & 41.9 & 85.6 & 0.76 & 0.82 \\
$\text{MI}_{Was}$ & 41.8 & 84.2 & 0.76 & 0.82 \\
\hline
\end{tabular}
\caption{Results on sentiment analysis on CMU-MOSI. $Acc_{7}$ denotes accuracy on 7 classes and $Acc_{2}$ the binary accuracy. $MAE$ denotes the Mean Absolute Error and Corr is the Pearson correlation \label{tab:mosi}}
\end{table}



\begin{table}[htb]
\begin{tabular}{|l|llll|}
\hline
%  MOSEI
   &  $Acc_{7}$   & $Acc_{2}$   & MAE  & CORR \\
\hline
\hline
   & & \multicolumn{2}{c}{MFN} & \\
\hline
MLE &  44.3 & 74.7 & 0.72 & 0.52 \\
MLE+$\text{S}_{KL}$ &  45.3 & 74.8 & 0.72 & 0.56 \\
MLE+$\text{S}_{MMD}$ &  46.2 & 75.1 & 0.69 & 0.56 \\
$\text{MI}_{Was}$ &  45.1 & 75.2 & 0.72 & 0.54 \\
\hline
   & & \multicolumn{2}{c}{LFN} & \\
\hline
MLE &  45.2 & 74.3 & 0.70 & 0.54 \\
MLE+$\text{S}_{KL}$ &  46.1 & 75.3 & 0.69 & 0.56 \\
MLE+$\text{S}_{MMD}$ &  46.3 & 75.3 & 0.67 & 0.56 \\
$\text{MI}_{Was}$ &  45.9 & 75.1 & 0.69 & 0.55 \\
\hline
   & & \multicolumn{2}{c}{MAGBERT} & \\
\hline
MLE &  46.9 & 84.8 & 0.59 & 0.77 \\
MLE+$\text{S}_{KL}$ &  47.4 & 85.3 & 0.59 & 0.79 \\
MLE+$\text{S}_{MMD}$ &  47.9 & 85.4 & 0.59 & 0.79 \\
$\text{MI}_{Was}$ &  47.2 & 85.0 & 0.59 & 0.78 \\
\hline
\end{tabular}
\caption{Results on sentiment analysis on CMU-MOSEI. $Acc_{7}$ denotes accuracy on 7 classes and $Acc_{2}$ the binary accuracy. $MAE$ denotes the Mean Absolute Error and Corr is the Pearson correlation \label{tab:mosei}}
\end{table}



\section{Conclusions}
In this paper, we introduced the idea of synergy maximization. We experimented with different measures of synergy based on discrepancy measures such as KL divergene and MMD distance. We show that training with synergy can produce benefit on even state-of-the-art architectures. Similar experiments have been performed recently by \citet{han2021improving} and \citet{colombo2021improving} suggesting mutual information based approaches can be used to improve multimodal fusion.


\bibliography{mybib}
%\bibliographystyle{abbrvnat}

 
%\input{appendix2}



%%% \appendix


\end{document}
