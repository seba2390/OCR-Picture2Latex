We give the derivations for the examples considered in \Cref{sub:examples} and prove the results for goodness-of-fit testing in \Cref{sub:goodness}.

\subsection{Examples}
\label{sub:append:examples}

\begin{example}[Generalized linear models]
    Let $Z := (X, Y)$ be a pair of input and output, where $X \in \calX \subset \reals^\tau$ and $Y \in \calY \subset \reals$.
    Let $t: \calX \times \calY \rightarrow \reals^d$ and $\mu$ be a measure on $\calY$.
    Consider the statistical model
    \begin{align*}
        p(y \mid x) \sim \frac{\exp(\ip{\theta, t(x, y)})}{\int \exp(\ip{\theta, t(x, \bar y)}) \D \mu(\bar y)} \D \mu(y)
    \end{align*}
    with $\norm{t(x, Y)}_2 \le M$ a.s.~under $p(y \mid x)$ for all $x$.
    It induces the loss function
    \begin{align*}
        \score(\theta; z) := -\ip{\theta, t(x, y)} + \log{\int \exp(\ip{\theta, t(x, \bar y)}) \D \mu(\bar y)}.
    \end{align*}
    
    We first verify \Cref{asmp:self_concordance}, i.e., show that it is generalized self-concordant for $\nu = 2$ and $R = 2M$.
    We denote by $\Expect_{Y\mid x}$ the expectation w.r.t.~$p(y \mid x)$.
    Note that $\log{\int \ip{\theta, t(x, \bar y)} \D \mu(\bar y)}$ is the cumulant generating function.
    It follows from some computation that
    \begin{align*}
        D_\theta \score(\theta; z) [u] &= -\ip{u, t(x, y)} + \Expect_{Y\mid x}\ip{u, t(x, Y)} \\
        D_\theta^2 \score(\theta; z) [u, u] &= \Expect_{Y \mid x}[\ip{u, t(x, Y)}^2] - [\Expect_{Y \mid x}\ip{u, t(x, Y)}]^2 \\
        D_\theta^3 \score(\theta; z)[u, u, v] &= \Expect_{Y \mid x}[\ip{u, t(x, Y)}^2 \ip{v, t(x, Y}] - \Expect_{Y \mid x}[\ip{u, t(x, Y)}^2] \Expect_{Y \mid x}\ip{v, t(x, Y)} \\
        &\quad - 2\Expect[\ip{u, t(x, Y)} \ip{v, t(x, Y)}] \Expect[\ip{u, t(x, Y)}] - 2[\Expect\ip{u, t(x, Y)}]^2 \Expect\ip{v, t(x, Y)}.
    \end{align*}
    As a result,
    \begin{align*}
        \abs{D_\theta^3 \score(\theta; z)[u, u, v]}
        &= \abs{\Expect_{Y \mid x}\left\{ \left[ \ip{u, t(x, Y)} - \Expect_{Y\mid x}\ip{u, t(x, Y)} \right]^2 \left[ \ip{v, t(x, Y)} - \Expect_{Y\mid x}\ip{v, t(x, Y)} \right] \right\}} \\
        &\le 2M \norm{v}_2 \Expect_{Y \mid x}\left\{ \left[ \ip{u, t(x, Y)} - \Expect_{Y\mid x}\ip{u, t(x, Y)} \right]^2 \right\}, \quad \mbox{by } \norm{t(x, Y)}_2 \txtover{a.s.}{\le} M \\
        &= 2M \norm{v}_2 D_\theta^2 \ell(\theta; z)[u, u],
    \end{align*}
    which completes the proof.
    
    We then verify \Cref{asmp:sub_gaussian} and \Cref{asmp:subG_local}.
    By \Cref{lem:bounded_subG}, it suffices to show that $\norm{S(\theta_\star; Z)}_2$ is a.s.~bounded.
    In fact,
    \begin{align*}
        S(\theta_\star; z) = -t(x, y) + \Expect_{p_{\theta_\star}(Y \mid x)}[t(x, Y)].
    \end{align*}
    Since $\abs{t(X, Y)}_2 \txtover{a.s.}{\le} M$, we get $\norm{S(\theta_\star; Z)}_2 \txtover{a.s.}{\le} 2M$ and thus the claim follows.
    \Cref{asmp:subG_local} can be verified similarly.
    
    Next, we verify \Cref{asmp:bernstein}.
    According to \Cref{lem:bounded_bernstein}, it is enough to prove that $\norm{H(\theta; Z)}_2$ is a.s.~bounded.
    In fact,
    \begin{align*}
        H(\theta; z) = \Expect_{Y \mid x}[t(x, Y) t(x, Y)^\top] - \Expect_{Y \mid x}[t(x, Y)] \Expect_{Y \mid x}[t(x, Y)]^\top.
    \end{align*}
    Since $\norm{t(X, Y) t(X, Y)^\top}_2 \le \norm{t(X, Y)}_2^2 \txtover{a.s.}{\le} M^2$, it follows that $\norm{H(\theta, Z)}_2 \txtover{a.s.}{\le} M^2$.
    
    Finally, we verify \Cref{asmp:lip}.
    It suffices to show that $\norm{G(\theta_1; Z) - G(\theta_2; Z)}_2 / \norm{\theta_1 - \theta_2}_2$ is a.s.~bounded.
    Note that
    \begin{align*}
        G(\theta_1; z) - G(\theta_2; z)
        &= \Expect_{p_{\theta_1}(Y \mid x)}[t(x, Y)] \Expect_{p_{\theta_1}(Y \mid x)}[t(x, Y)]^\top - \Expect_{p_{\theta_2}(Y \mid x)}[t(x, Y)] \Expect_{p_{\theta_2}(Y \mid x)}[t(x, Y)]^\top \\
        &\quad - 2t(x, y)\left\{ \Expect_{p_{\theta_1}(Y \mid x)}[t(x, Y)] - \Expect_{p_{\theta_2}(Y \mid x)}[t(x, Y)] \right\}^\top.
    \end{align*}
    For the second term, we have
    \begin{align*}
        \norm{-2t(x, y)\left\{ \Expect_{p_{\theta_1}(Y \mid x)}[t(x, Y)] - \Expect_{p_{\theta_2}(Y \mid x)}[t(x, Y)] \right\}^\top}_2
        \le 2\norm{t(x, y)}_2 \norm{\Expect_{p_{\theta_1}(Y \mid x)}[t(x, Y)] - \Expect_{p_{\theta_2}(Y \mid x)}[t(x, Y)]}_2.
    \end{align*}
    Note that
    \begin{align*}
        \Expect_{p_{\theta_1}(Y \mid x)}[t(x, Y)] - \Expect_{p_{\theta_2}(Y \mid x)}[t(x, Y)]
        &= \frac{\int t(x, y) \exp(\ip{\theta_1, t(x, y)}) \D \mu(y)}{\int \exp(\ip{\theta_1, t(x, y)}) \D \mu(y)} - \frac{\int t(x, y) \exp(\ip{\theta_2, t(x, y)}) \D \mu(y)}{\int \exp(\ip{\theta_2, t(x, y)}) \D \mu(y)}.
    \end{align*}
    Since $\abs{\ip{\theta, t(X, Y)}} \txtover{a.s.}{\le} [\norm{\theta - \theta_\star}_2 + \norm{\theta_\star}_2]M \le [\lambda_\star^{-1/2} r + \norm{\theta_\star}_2] M$ for all $\theta \in \Theta_{r}(\theta_\star)$, it holds that $\int \exp(\ip{\theta, t(X, y)}) \D \mu(y) \txtover{a.s.}{\ge} c$ for some $c > 0$ and $\theta \in \{\theta_1, \theta_2\}$.
    Now it remains to control
    \begin{align*}
        A_1 &:= \Big \Vert \int t(x, y) \exp(\ip{\theta_1, t(x, y)}) \D \mu(y) \int \exp(\ip{\theta_2, t(x, y)}) \D \mu(y) \\
        &\qquad - \int t(x, y) \exp(\ip{\theta_2, t(x, y)}) \D \mu(y) \int \exp(\ip{\theta_1, t(x, y)}) \D \mu(y) \Big \Vert_2.
    \end{align*}
    By the triangle inequality, we get $A_1 \le B_1 + B_2$ where
    \begin{align*}
        B_1 &:= \norm{\left[\int t(x, y) \exp(\ip{\theta_1, t(x, y)}) \D \mu(y) - \int t(x, y) \exp(\ip{\theta_2, t(x, y)}) \D \mu(y) \right] \int \exp(\ip{\theta_2, t(x, y)}) \D \mu(y)}_2 \\
        B_2 &:= \norm{\int t(x, y) \exp(\ip{\theta_2, t(x, y)}) \D \mu(y) \left[ \int \exp(\ip{\theta_2, t(x, y)}) \D \mu(y) - \int \exp(\ip{\theta_1, t(x, y)}) \D \mu(y) \right]}_2.
    \end{align*}
    Since $\abs{\ip{\theta_2, t(X, Y)}}$ and $d$ is a.s.~bounded, 
\end{example}

\myparagraph{Remark}
As a special case, the negative log-likelihood of the softmax regression with $\calX \subset \{x \in \reals^\tau: \norm{x} \le M\}$ and $\calY = \{1, \dots, K\}$ is generalized self-concordant with $\nu = 2$ and $R = 2M$.
In fact, the statistical model of the softmax regression is
\begin{align*}
    p(y = k \mid x) \sim \frac{\exp{\ip{w_k, x}}}{\sum_{j=1}^K \exp{\ip{w_j, x}}}.
\end{align*}
Define $\theta^\top := (w_1^\top, \dots, w_K^\top)$ and $t(x, y)^\top := (0_\tau^\top, \dots, x^\top, \dots, 0_\tau^\top)$ whose elements from $(y-1)\tau + 1$ to $y\tau$ are given by $x^\top$ and 0 elsewhere.
Then we have
\begin{align*}
    p(y=k \mid x) \sim \frac{\exp{\ip{\theta, t(x, k)}}}{\sum_{y=1}^K \exp{\ip{\theta, t(x, y)}}}.
\end{align*}
The claim then follows from the example above and $\norm{t(x, Y)}_2 = \norm{x}_2 \le M$.

\myparagraph{Remark}
The conditional random fields \citep{lafferty2001conditional} also fall into the category of generalized linear models.
For simplicity, we consider a conditional random field on a chain, i.e., for $x = (x_t)_{t=1}^T$ and $y = (y_t)_{t=1}^T$,
\begin{align*}
    p(y \mid x) \propto \exp\left\{ \sum_{t=1}^{T-1} \lambda_t f_t(x, y_t, y_{t+1}) + \sum_{t=1}^T \mu_t g_t(x, y_t) \right\}.
\end{align*}
Define $\theta^\top := (\lambda_1, \dots, \lambda_{T-1}, \mu_1, \dots, \mu_T)$ and
\begin{align*}
    t(x, y)^\top := \left( f_1(x, y_1, y_2), \dots, f_{T-1}(x, y_{T-1}, y_T), g_1(x, y_1), \dots, g_T(x, y_T) \right).
\end{align*}
Then we have
\begin{align*}
    p(y \mid x) \sim \frac{\exp\ip{\theta, t(x, y)}}{\int \exp\ip{\theta, t(x, \bar y)} \D \bar y}.
\end{align*}

\begin{example}[Score matching with exponential families]
    Assume that $\bbZ = \reals^p$.
    Consider an exponential family on $\reals^d$ with densities
    \begin{align*}
        \log{p_\theta(z)} = \theta^\top t(z) + h(z) - \Lambda(\theta).
    \end{align*}
    The non-normalized density $q_\theta$ then reads $\log{q_\theta(z)} = \theta^\top t(z) + h(z)$.
    As a result, the score matching loss becomes
    \begin{align*}
        \score(\theta; z) &= \sum_{k=1}^p \left[ \theta^\top \frac{\partial^2 t(z)}{\partial z_k^2} + \frac{\partial^2 h(z)}{\partial z_k^2} + \frac12 \left( \theta^\top \frac{\partial t(z)}{\partial z_k} + \frac{\partial h(z)}{\partial z_k} \right)^2 \right] + \text{const} \\
        &= \frac12 \theta^\top A(z) \theta - b(z)^\top \theta + c(z) + \text{const},
    \end{align*}
    where $A(z) := \sum_{k=1}^p \frac{\partial t(z)}{\partial z_k} \big(\frac{\partial t(z)}{\partial z_k}\big)^\top$ is p.s.d, $b(z) := \sum_{k=1}^p \left[ \frac{\partial^2 t(z)}{\partial z_k^2} + \frac{\partial h(z)}{\partial z_k} \frac{\partial t(z)}{\partial z_k} \right]$, and $c(z) := \sum_{k=1}^p \left[ \frac{\partial^2 h(z)}{\partial z_k^2} + \big(\frac{\partial h(z)}{\partial z_k}\big)^2 \right]$.
    Therefore, the score matching loss $\score(\theta; z)$ is convex.
    Moreover, since the third derivatives of $\ell(\cdot; z)$ is zero, the score matching loss is generalized self-concordant for all $\nu \ge 2$ and $R \ge 0$.
    When the true distribution $\Prob$ is supported on the non-negative orthant $\reals^p_+$, the score matching loss does not apply.
    Fortunately, a generalized score matching \citep{hyvarinen2007some,yu2019generalized} loss can be used to address this issue.
    Let $w_1, \dots, w_m: \reals_+ \to \reals_+$ be functions that are absolutely continuous in every bounded sub-interval of $\reals_+$.
    Then the generalized score matching loss reads
    \begin{align}\label{eq:score_matching}
        \score(\theta; z) = \sum_{j=1}^d \left[ w_j'(z_j) \partial_j \log{q(z)} + w_j(z_j) \partial_{jj} \log{q(z)} + \frac12 w_j(z_j) (\partial_j \log{q(z)})^2 \right] + \text{const},
    \end{align}
    which consists of a weighted version of the original score matching loss with weights $\{w_j(x_j)\}_{j=1}^d$ (the last two terms in \eqref{eq:score_matching}) and an additional term (the first term in \eqref{eq:score_matching}).
    According to \cite[Theorem 5]{yu2019generalized}, the loss \eqref{eq:score_matching} admits a quadratic form:
    \begin{align*}
        \score(z, Q_\theta) = \frac12 \theta^\top \bar A(z) \theta - \bar b(z)^\top \theta + \bar c(z) + \text{const},
    \end{align*}
    where $\bar A(z)$ is p.s.d.
    Hence, it is generalized self-concordant.
    Note that a particular example is the pairwise graphical models studies in \citep{yu2016statistical,yu2020simultaneous}.
\end{example}

\begin{example}[Generalized score matching with exponential families]
    When the true distribution $\Prob$ is supported on the non-negative orthant, $\reals^d_+$, the Hyv\"arinen score does not apply.
    Hyv\"arinen \citep{hyvarinen2007some} proposed the non-negative score matching to address this issue, which is later generalized in \cite[Section 2.2]{yu2019generalized}.
    Let $h_1, \dots, h_m: \reals_+ \to \reals_+$ be positive functions that are absolutely continuous in every bounded sub-interval of $\reals_+$.
    Then the generalized Hyv\"arinen score reads
    \begin{align}\label{eq:general_score_matching}
        \score(z, Q) = \sum_{j=1}^d \left[ h_j'(z_j) \partial_j \log{q(z)} + h_j(z_j) \partial_{jj} \log{q(z)} + \frac12 h_j(z_j) (\partial_j \log{q(z)})^2 \right],
    \end{align}
    which is a weighted version of the original Hyv\"arinen score with weights $\{h_j(x_j)\}_{j=1}^d$ (the last two terms in \eqref{eq:general_score_matching}) with an additional term (the first term in \eqref{eq:general_score_matching}).
    
    We then consider an exponential family on $\reals_+^d$ with densities
    \begin{align*}
        \log{q_\theta(z)} = \theta^\top t(z) - S(\theta) + b(z).
    \end{align*}
    According to \cite[Theorem 5]{yu2019generalized}, the score \eqref{eq:general_score_matching} admits the quadratic form:
    \begin{align*}
        \score(z, Q_\theta) = \frac12 \theta^\top \Gamma(z) \theta - g(z)^\top \theta + C,
    \end{align*}
    where $\Gamma(z)$ is p.s.d.
    Hence, this score is self-concordant.
    Note that a particular example is the pairwise graphical models studies in \citep{yu2016statistical,yu2020simultaneous}.
\end{example}

\subsection{Applications to goodness-of-fit testing}
\label{sub:appendix:application}

Before we start, we note that a simple modification to the confidence bound in \Cref{thm:conf_set} leads to the following risk bound that can be utilized to analyze the likelihood ratio test.

\begin{corollary}\label{cor:risk_bound}
    Under the same assumptions in \Cref{thm:conf_set}, we have, with probability at least $1 - \delta$,
    \begin{align*}
        L(\theta_n) - L(\theta_\star) \lesssim K_{1}^2 \omega_\nu^2(\varepsilon) \log{(e/\delta)} \frac{d_\star}{n}
    \end{align*}
    whenever $n$ satisfies \eqref{eq:n_large_enough}.
\end{corollary}
\begin{proof}
    By Taylor's expansion, we have
    \begin{align*}
        L(\theta_n) - L(\theta_\star) = S(\theta_\star)^\top (\theta_n - \theta_\star) + \frac12 \norm{\theta_n - \theta_\star}_{H_n(\bar \theta_n)}^2
    \end{align*}
    for some $\bar \theta_n \in \mbox{Conv}\{\theta_n, \theta_\star\} \subset \Theta_{\varepsilon / R_\nu^\star}(\theta_\star)$.
    By $S(\theta_\star) = 0$ and \Cref{thm:conf_set}, we get
    \begin{align*}
        L(\theta_n) - L(\theta_\star) \lesssim K_1^2 \omega_\nu^2(\varepsilon) \log{(e/\delta)} \frac{d_\star}{n}.
    \end{align*}
\end{proof}

We begin with the type I error rates of Rao's score test, the likelihood ratio test, and the Wald test.
Note that $d_\star = d$ under $\hnull$.

\begin{customprop}{\ref{prop:typeI}}
    Suppose that \Cref{asmp:sub_gaussian,asmp:bernstein} with $r = 0$ hold true.
    Under $\hnull$, we have, with probability at least $1 - \delta$,
    \begin{align*}
        \rao \lesssim K_1^2 \log{(e/\delta)} \frac{d}n
    \end{align*}
    whenever $n \ge 4(K_2 + 2\sigma_H^2) \log{(4d/\delta)}$.
    Furthermore, if \Cref{asmp:self_concordance,asmp:bernstein} with $r = K_\nu/R_\nu^\star$ hold true, we have, with probability at least $1 - \delta$,
    \begin{align*}
        \lr, \wald \lesssim K_1^2 \omega_\nu^2(\varepsilon) \log{(e/\delta)} \frac{d}{n}
    \end{align*}
    whenever $n$ satisfies \eqref{eq:n_large_enough}.
\end{customprop}

\begin{proof}
    Under $\hnull$, we have $\theta_\star = \theta_0$.
    It then follows from \Cref{prop:score} that, with probability at least $1 - \delta$,
    \begin{align*}
        \rao := \norm{S_n(\theta_0)}_{H_n^{-1}(\theta_0)}^2 \lesssim \frac1n K_1^2 \log{(e/\delta)} d
    \end{align*}
    whenever $n \ge 4(K_2 + 2\sigma_H^2) \log{(4d/\delta)}$.
    
    By Taylor's theorem, there exists $\bar \theta_n \in \mbox{Conv}\{\theta_n, \theta_\star\}$ such that
    \begin{align*}
        \lr = 2S_n^\top(\theta_n) (\theta_0 - \theta_n) + \norm{\theta_0 - \theta_n}_{H_n(\bar \theta_n)}^2 = \norm{\theta_0 - \theta_n}_{H_n(\bar \theta_n)}^2.
    \end{align*}
    Following a similar argument as \Cref{thm:conf_set}, we obtain, with probability at least $1 - \delta$,
    \begin{align*}
        \lr \lesssim K_1^2 \omega_\nu^2(\varepsilon) \log{(e/\delta)} \frac{d}{n}
    \end{align*}
    whenever $n$ satisfies \eqref{eq:n_large_enough}.
    The statement for $\wald$ follows directly from \Cref{thm:conf_set}.
\end{proof}

We then prove the result for statistical power given in \Cref{prop:power}.

\begin{customprop}{\ref{prop:power}}[Statistical power]
    Let $\theta_\star \neq \theta_0$ that may depend on $n$.
    The following statements are true for sufficiently large $n$.
    \begin{enumerate}
        \item[(a)] Suppose that $S(\theta_0) \neq 0$, $H(\theta_0) \succ 0$, and \Cref{asmp:self_concordance,asmp:sub_gaussian,asmp:bernstein} hold true with $r=0$.
        When $\theta_\star - \theta_0 = O(n^{-1/2})$ and $\tau_n := t_n(\alpha)/4 - \norm{S(\theta_0)}_{H(\theta_0)^{-1}}^2 - \Tr(\Omega(\theta_0))/n > 0$, we have
        \begin{align*}
            \Prob(\rao > t_n(\alpha)) &\le 2d \exp\left( - \frac{n}{4(K_2 + 2\sigma_H^2)} \right) + \exp\left( -c\min\left\{ \frac{n^2 \tau_n^2}{K_1^2 \norm{\Omega(\theta_0)}_2^2}, \frac{n\tau_n}{K_1 \norm{\Omega(\theta_0)}_\infty} \right\} \right).
        \end{align*}
        When $\theta_* - \theta_n = \omega(n^{-1/2})$, we have
        \begin{align*}
            \Prob(\rao > t_n(\alpha)) \ge 1 - 2d \exp\left( - \frac{n}{4(K_2 + 2\sigma_H^2)} \right) - \exp\left( -c\min\left\{ \frac{n^2 \bar \tau_n^2}{K_1^2 \norm{\Omega(\theta_0)}_2^2}, \frac{n\bar \tau_n}{K_1 \norm{\Omega(\theta_0)}_\infty} \right\} \right),
        \end{align*}
        where $\bar \tau_n := \left[ \norm{S(\theta_0)}_{H(\theta_0)^{-1}} - \sqrt{3t_n(\alpha)/4} \right]^2 - \Tr(\Omega(\theta_0))/n$.
        
        \item[(b)] Suppose that the assumptions in \Cref{thm:conf_set} hold true.
        When $\theta_\star - \theta_0 = O(n^{-1/2})$ and $\tau_n' := t_n(\alpha)/384 - \norm{\theta_\star - \theta_0}_{H(\theta_\star)}^2/64 - d/n > 0$, we have
        \begin{align*}
            \Prob(\lr > t_n(\alpha))
            \le \exp\left( -c\min\left\{ \frac{n^2 (\tau_n')^2}{K_1^2 \norm{\Omega(\theta_\star)}_2^2}, \frac{n\tau_n'}{K_1 \norm{\Omega(\theta_\star)}_\infty} \right\} \right) + \exp\left(-c \frac{\varepsilon^2 n^{3-\nu}}{(R_\nu^\star)^2 K_1^2 d} \right).
        \end{align*}
        When $\theta_* - \theta_n = \omega(n^{-1/2})$, we have
        \begin{align*}
            \Prob(\lr > t_n(\alpha)) \ge 1 - \exp\left( -c\min\left\{ \frac{n^2 (\bar \tau_n')^2}{K_1^2 \norm{\Omega(\theta_\star)}_2^2}, \frac{n\bar \tau_n'}{K_1 \norm{\Omega(\theta_\star)}_\infty} \right\} \right) - \exp\left(-c \frac{\varepsilon^2 n^{3-\nu}}{(R_\nu^\star)^2 K_1^2 d} \right),
        \end{align*}
        where
        \begin{align*}
            \bar \tau_n' := \left[ \norm{\theta_\star - \theta_0}_{H(\theta_\star)}/8 - \sqrt{t_n(\alpha)}/4 \right]^2 - d/n.
        \end{align*}
        
        \item[(c)] Suppose that the assumptions in \Cref{thm:conf_set} hold true.
        When $\theta_\star - \theta_0 = O(n^{-1/2})$ and $\tau_n' := t_n(\alpha)/384 - \norm{\theta_\star - \theta_0}_{H(\theta_\star)}^2/64 - d/n > 0$, we have
        \begin{align*}
            \Prob(\wald > t_n(\alpha))
            \le \exp\left( -c\min\left\{ \frac{n^2 (\tau_n')^2}{K_1^2 \norm{\Omega(\theta_\star)}_2^2}, \frac{n\tau_n'}{K_1 \norm{\Omega(\theta_\star)}_\infty} \right\} \right) + \exp\left(-c \frac{\varepsilon^2 n^{3-\nu}}{(R_\nu^\star)^2 K_1^2 d} \right).
        \end{align*}
        When $\theta_* - \theta_n = \omega(n^{-1/2})$, we have
        \begin{align*}
            \Prob(\wald > t_n(\alpha)) \ge 1 - \exp\left( -c\min\left\{ \frac{n^2 (\bar \tau_n')^2}{K_1^2 \norm{\Omega(\theta_\star)}_2^2}, \frac{n\bar \tau_n'}{K_1 \norm{\Omega(\theta_\star)}_\infty} \right\} \right) - \exp\left(-c \frac{\varepsilon^2 n^{3-\nu}}{(R_\nu^\star)^2 K_1^2 d} \right),
        \end{align*}
        where
        \begin{align*}
            \bar \tau_n' := \left[ \norm{\theta_\star - \theta_0}_{H(\theta_\star)}/8 - \sqrt{t_n(\alpha)}/4 \right]^2 - d/n.
        \end{align*}
    \end{enumerate}
\end{customprop}

\begin{proof}[Proof of \Cref{prop:power}]
    We are mostly interested in local alternatives, i.e., $\theta_\star \rightarrow \theta_0$ as $n \rightarrow \infty$.

    \textbf{Rao's score test}. Define four events
    \begin{align*}
        \calA &:= \{\rao > t_n(\alpha)\} \\
        \calB &:= \left\{ \frac12 H(\theta_0) \preceq H_n(\theta_0) \preceq \frac32 H(\theta_0) \right\} \\
        \calC &:= \left\{ 4 \norm{S_n(\theta_0) - S(\theta_0)}_{H(\theta_0)^{-1}}^2 > t_n(\alpha) - 4 \norm{S(\theta_0)}_{H(\theta_0)^{-1}}^2 \right\} \\
        \calD &:= \left\{ \norm{S_n(\theta_0) - S(\theta_0)}_{H(\theta_0)^{-1}} < \norm{S(\theta_0)}_{H(\theta_0)^{-1}} - \sqrt{3t_n(\alpha)/4} \right\}.
    \end{align*}
    Note that
    \begin{align*}
        S(\theta_0) = S(\theta_0) - S(\theta_\star) = H(\bar \theta) (\theta_0 - \theta_\star),
    \end{align*}
    where $\bar \theta \in \mbox{Conv}\{\theta_0, \theta_\star\}$.
    Due to \Cref{asmp:self_concordance}, we have
    \begin{align}\label{eq:hessian_order}
        e^{-R \norm{\bar \theta - \theta_0}_2} H(\theta_0) \preceq H(\bar \theta) \preceq e^{R \norm{\bar \theta - \theta_0}_2} H(\theta_0).
    \end{align}
    Therefore, we conclude that, as $n \rightarrow \infty$,
    \begin{align}\label{eq:score_order}
        S(\theta_0) = H(\bar \theta) (\theta_0 - \theta_\star) = \Theta(\theta_\star - \theta_0).
    \end{align}
    
    We first consider the case when $\theta_\star - \theta_0 = O(n^{-1/2})$.
    On the event $\calB$, it holds that
    \begin{align*}
        \rao
        \le 2 \norm{S_n(\theta_0)}_{H(\theta_0)^{-1}}^2
        \le 4 \norm{S_n(\theta_0) - S(\theta_0)}_{H(\theta_0)^{-1}}^2 + 4 \norm{S(\theta_0)}_{H(\theta_0)^{-1}}^2.
    \end{align*}
    This implies $\calA \calB \subset \calA \calC$ and thus
    \begin{align*}
        \Prob(\calA) = \Prob(\calA \calB) + \Prob(\calA \calB^c) \le \Prob(\calA \calC) + \Prob(\calB^c) \le \Prob(\calC) + \Prob(\calB^c)
    \end{align*}
    It follows from \Cref{thm:bernstein_matrix} that, when $n$ is large enough,
    \begin{align*}
        \Prob(\calB^c) \le 2d \exp\left( - \frac{n}{4(K_2 + 2\sigma_H^2)} \right).
    \end{align*}
    Moreover, note that $\calC = \{ \norm{S_n(\theta_0) - S(\theta_0)}_{H^{-1}(\theta_0)}^2 - \Tr(\Omega(\theta_0))/n \ge \tau_n \}$, where
    \begin{align*}
        \tau_n = t_n(\alpha)/4 - \norm{S(\theta_0)}_{H(\theta_0)^{-1}}^{2} - \Tr(\Omega(\theta_0))/n.
    \end{align*}
    By \Cref{thm:isotropic_tail}, we have, whenever $\tau_n > 0$,
    \begin{align*}
        \Prob(\calC) \le \exp\left( -c\min\left\{ \frac{n^2 \tau_n^2}{K_1^2 \norm{\Omega(\theta_0)}_2^2}, \frac{n\tau_n}{K_1 \norm{\Omega(\theta_0)}_\infty} \right\} \right).
    \end{align*}
    Consequently, it holds that, whenever $\tau_n > 0$ and $n$ is large enough,
    \begin{align*}
        \Prob(\calA) \le 2d \exp\left( - \frac{n}{4(K_2 + 2\sigma_H^2)} \right) + \exp\left( -c\min\left\{ \frac{n^2 \tau_n^2}{K_1^2 \norm{\Omega(\theta_0)}_2^2}, \frac{n\tau_n}{K_1 \norm{\Omega(\theta_0)}_\infty} \right\} \right).
    \end{align*}
    Note that, for large enough $n$, it holds that $\Tr(\Omega(\theta_0)) \rightarrow d$ and thus $t_n(\alpha) > \Tr(\Omega(\theta_0))/n$.
    Hence, it follows from \eqref{eq:score_order} that, as long as $\theta_\star - \theta_0 = o(n^{-1/2})$, $\tau_n > 0$ for sufficiently large $n$.
    
    We then consider the case when $\theta_\star - \theta_0 = \omega(n^{-1/2})$.
    On the event $\calB$, it holds that
    \begin{align*}
        \rao
        \ge 2 \norm{S_n(\theta_0)}_{H(\theta_0)^{-1}}^2 / 3
        \ge 4 [\norm{S(\theta_0)}_{H(\theta_0)^{-1}} -  \norm{S_n(\theta_0) - S(\theta_0)}_{H(\theta_0)^{-1}}]^2 / 3.
    \end{align*}
    By \Cref{thm:isotropic_tail}, it holds that $\norm{S_n(\theta_0) - S(\theta_0)}_{H(\theta_0)^{-1}} = O(n^{-1/2})$.
    By \eqref{eq:score_order}, we know that $\norm{S(\theta_0)}_{H(\theta_0)^{-1}} = \omega(n^{-1/2})$ and thus, for sufficiently large $n$,
    \begin{align*}
        \norm{S(\theta_0)}_{H(\theta_0)^{-1}} > \norm{S_n(\theta_0) - S(\theta_0)}_{H(\theta_0)^{-1}} + \sqrt{t_n(\alpha)}.
    \end{align*}
    This implies that $\calB \calD \subset \calA \calB$ and hence
    \begin{align*}
        \Prob(\calA) \ge \Prob(\calA \calB) \ge \Prob(\calB \calD) \ge 1 - \Prob(\calB^c) - \Prob(\calD^c).
    \end{align*}
    Following a similar argument as above, we have, whenever $n$ is large enough,
    \begin{align*}
        \Prob(\calA) \ge 1 - 2d \exp\left( - \frac{n}{4(K_2 + 2\sigma_H^2)} \right) - \exp\left( -c\min\left\{ \frac{n^2 \bar \tau_n^2}{K_1^2 \norm{\Omega(\theta_0)}_2^2}, \frac{n\bar \tau_n}{K_1 \norm{\Omega(\theta_0)}_\infty} \right\} \right),
    \end{align*}
    where
    \begin{align*}
        \bar \tau_n := \left[ \norm{S(\theta_0)}_{H(\theta_0)^{-1}} - \sqrt{3t_n(\alpha)/4} \right]^2 - \Tr(\Omega(\theta_0))/n.
    \end{align*}
    
    \textbf{The Wald test}.
    Notice that $d_\star = d$ since the model is well-specified.
    Fix $\varepsilon = \varepsilon_\nu$ so that $\omega_\nu(\varepsilon) \le 2$.
    Let $\delta := \exp\left(-c \frac{\varepsilon^2 n^{3-\nu}}{(R_\nu^\star)^2 K_1^2 d} \right)$.
    Define the following events
    \begin{align}\label{eq:events_wald}
        \begin{split}
            \calA &:= \left\{ \norm{S_n(\theta_\star)}_{H^{-1}(\theta_\star)}^{2} \le C K_{1}^2 \log{(e / \delta)} \frac{d}{n} \right\} \\
            \calB &:= \left\{ \frac12 H(\theta_\star) \preceq H_n(\theta_\star) \preceq \frac32 H(\theta_\star) \right\} \\
            \calC &:= \left\{ \frac1{2 \omega_\nu^2(\varepsilon)} H(\theta_\star) \preceq H_n(\theta) \preceq \frac{3}{2} \omega_\nu^2(\varepsilon) H(\theta_\star), \quad \mbox{for all } \theta \in \Theta_{\varepsilon/R_\nu^\star}(\theta_\star) \right\} \\
            \calD &:= \left\{ \wald > t_n(\alpha) \right\} \\
            \calE &:= \left\{ \norm{S_n(\theta_\star)}_{H(\theta_\star)^{-1}}^2 >  t_n(\alpha)/384 - \norm{\theta_\star - \theta_0}_{H(\theta_\star)}^2/64 \right\} \\
            \calF &:= \left\{ \norm{S_n(\theta_\star)}_{H(\theta_\star)^{-1}}^2 < \norm{\theta_\star - \theta_0}_{H(\theta_\star)}/8 - \sqrt{t_n(\alpha)}/4 \right\}.
        \end{split}
    \end{align}
    Following the proof of \Cref{thm:conf_set}, we get $\Prob(\calA \calB \calC) \ge 1 - \delta$ and,
    on the event $\calA \calB \calC$, we have, for sufficiently large $n$,
    \begin{align*}
        \frac1{4} H(\theta_\star) \preceq \frac1{2 \omega_\nu^2(\varepsilon)} H(\theta_\star) \preceq H_n(\theta_n) \preceq \frac32 \omega_\nu^2(\varepsilon) H(\theta_\star) \preceq 3 H(\theta_\star)
    \end{align*}
    and
    \begin{align}\label{eq:alt_local}
        \norm{\theta_n - \theta_\star}_{H(\theta_\star)} \le 4\sqrt{2} \norm{S_n(\theta_\star)}_{H_n(\theta_\star)^{-1}} \le 8 \norm{S_n(\theta_\star)}_{H(\theta_\star)^{-1}}.
    \end{align}
    
    We first consider the case $\theta_\star - \theta_0 = O(n^{-1/2})$.
    On the event $\calA \calB \calC$, it holds that
    \begin{align*}
        \norm{\theta_n - \theta_0}_{H_n(\theta_n)}^2
        &\le 3\norm{\theta_n - \theta_0}_{H(\theta_\star)}^2
        \le 6\norm{\theta_n - \theta_\star}_{H(\theta_\star)}^2 + 6\norm{\theta_\star - \theta_0}_{H(\theta_\star)}^2 \\
        &\le 384\norm{S_n(\theta_\star)}_{H_n(\theta_\star)^{-1}}^2 + 6\norm{\theta_\star - \theta_0}_{H(\theta_\star)}^2
    \end{align*}
    This implies that $\calA \calB \calC \calD \subset \calA \calB \calC \calE$ and thus
    \begin{align*}
        \Prob(\calD)
        = \Prob(\calA \calB \calC \calD) + \Prob((\calA \calB \calC)^c \calD)
        \le \Prob(\calE) + \Prob((\calA \calB \calC)^c).
    \end{align*}
    Moreover, note that $\calE = \{ \norm{S_n(\theta_0) - S(\theta_0)}_{H^{-1}(\theta_0)}^2 - d/n \ge \tau_n' \}$, where
    \begin{align*}
        \tau_n' = t_n(\alpha)/384 - \norm{\theta_\star - \theta_0}_{H(\theta_\star)}^2/64 - d/n.
    \end{align*}
    By \Cref{thm:isotropic_tail}, we have, whenever $\tau_n' > 0$,
    \begin{align*}
        \Prob(\calE) \le \exp\left( -c\min\left\{ \frac{n^2 (\tau_n')^2}{K_1^2 \norm{\Omega(\theta_\star)}_2^2}, \frac{n\tau_n'}{K_1 \norm{\Omega(\theta_\star)}_\infty} \right\} \right).
    \end{align*}
    Since $\Prob((\calA \calB \calC)^c) \le \delta = \exp\left(-c \frac{\varepsilon^2 n^{3-\nu}}{(R_\nu^\star)^2 K_1^2 d} \right)$, it holds that
    \begin{align*}
        \Prob(\calD)
        \le \exp\left( -c\min\left\{ \frac{n^2 (\tau_n')^2}{K_1^2 \norm{\Omega(\theta_\star)}_2^2}, \frac{n\tau_n'}{K_1 \norm{\Omega(\theta_\star)}_\infty} \right\} \right) + \exp\left(-c \frac{\varepsilon^2 n^{3-\nu}}{(R_\nu^\star)^2 K_1^2 d} \right).
    \end{align*}

    We then consider the case $\theta_\star - \theta_0 = \omega(n^{-1/2})$.
    On the event $\calA \calB \calC$, we get
    \begin{align*}
        \norm{\theta_n - \theta_0}_{H_n(\theta_n)}^2
        \ge \norm{\theta_n - \theta_0}_{H(\theta_\star)}^2 / 4
        \ge [\norm{\theta_\star - \theta_0}_{H(\theta_\star)} - \norm{\theta_n - \theta_\star}_{H(\theta_\star)}]^2 / 4.
    \end{align*}
    According to \eqref{eq:alt_local} and the event $\calA$, we have $\norm{\theta_n - \theta_\star}_{H(\theta_\star)} = O(n^{-1})$ and thus $\norm{\theta_n - \theta_\star}_{H(\theta_\star)} < \norm{\theta_\star - \theta_0}_{H(\theta_\star)}$ for sufficiently large $n$.
    As a result, it holds that
    \begin{align*}
        \norm{\theta_n - \theta_0}_{H_n(\theta_n)}^2
        \ge \left[ \norm{\theta_\star - \theta_0}_{H(\theta_\star)} - 8 \norm{S_n(\theta_\star)}_{H(\theta_\star)^{-1}} \right]^2/4.
    \end{align*}
    This implies that $\calA \calB \calC \calF \subset \calA \calB \calC \calD$ and thus
    \begin{align*}
        \Prob(\calD) \ge \Prob(\calA \calB \calC \calD) \ge \Prob(\calA \calB \calC \calF) \ge 1 - \Prob((\calA \calB \calC)^c) - \Prob(\calF^c).
    \end{align*}
    Let
    \begin{align*}
        \bar \tau_n' := \left[ \norm{\theta_\star - \theta_0}_{H(\theta_\star)}/8 - \sqrt{t_n(\alpha)}/4 \right]^2 - d/n.
    \end{align*}
    It is positive for sufficiently large $n$ since $\theta_\star - \theta_0 = \omega(n^{-1/2})$.
    By \Cref{thm:isotropic_tail} and $\Prob((\calA \calB \calC)^c) \le \delta$, it holds that
    \begin{align*}
        \Prob(\calD) \ge 1 - \exp\left( -c\min\left\{ \frac{n^2 (\bar \tau_n')^2}{K_1^2 \norm{\Omega(\theta_\star)}_2^2}, \frac{n\bar \tau_n'}{K_1 \norm{\Omega(\theta_\star)}_\infty} \right\} \right) - \exp\left(-c \frac{\varepsilon^2 n^{3-\nu}}{(R_\nu^\star)^2 K_1^2 d} \right).
    \end{align*}
    
    \textbf{The likelihood ratio test}.
    Note that
    \begin{align*}
        \ell_n(\theta_0) - \ell_n(\theta_n) = \norm{\theta_n - \theta_0}_{H_n(\bar \theta)}^2
    \end{align*}
    for some $\bar \theta \in \mbox{Conv}\{\theta_n, \theta_0\}$.
    The claim can be proved with the same argument as the one for the Wald test.
\end{proof}
