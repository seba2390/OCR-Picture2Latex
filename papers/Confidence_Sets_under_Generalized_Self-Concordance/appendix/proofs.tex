Our proof techniques rely on a self-concordance property to localize the estimator and control the Hessian and related quantities. This property was, up to our knowledge, first put to use in machine learning by~\citet{abernethy2008efficient} in the context of sequential allocation of experiments and multi-armed bandits. The key observation is that, within the Dikin ellipsoid, the variation of the Hessian
can be easily controlled. More recently,~\citet{ostrovskii2021finite} obtained risk bounds for generalized linear models based on this observation.
Our results and proof techniques also rely on this observation. We show how to leverage this observation to obtain confidence sets for a broad class of statistical models under a generalized self-concordance assumption owing to the use of the matrix Bernstein inequality. For instance, we obtain confidence bounds for parameter estimation using score matching and generalized linear statistical models under possible model misspecification as provided in \Cref{sec:application}.

Our proofs are inspired by \citet{ostrovskii2021finite}.
However, there are two key differences.
First, since they focus on loss functions of the form $\ell(Y, \theta^\top X)$, the Hessian is $\ell''(Y, \theta^\top X) XX^\top$ where $\ell''(y, \bar y) := \D^2 \ell(y, \bar y) / \D \bar y^2$.
As a result, they can control the deviation of the empirical Hessian using inequalities for sample second-moment matrices of sub-Gaussian random vectors \citep[Thm.~A.2]{ostrovskii2021finite}.
In contrast, we use matrix Bernstein inequality which allows us to work with a larger class of loss functions.
Second, we extend their localization result from pseudo self-concordant losses to generalized self-concordant losses (\Cref{prop:localization}).
This is enabled by a new property on the existence of a unique minimizer for generalized self-concordant functions (\Cref{prop:self_concordance_local}). We also establish the concentration of the effective dimension. 

In the remainder of this section, we first prove the localization result \Cref{prop:localization} and the score bound \Cref{prop:score} in \Cref{sub:appendix:local}.
It not only guarantees the existence and uniqueness of $\theta_n$ but also localizes it.
We then, in \Cref{sub:appendix:thm}, control the empirical Hessian at $\theta_n$ as in \Cref{prop:emp_hess_est} using a covering number argument.
Finally, we prove \Cref{thm:risk_bound_generalized}, \Cref{thm:conf_set}, and \Cref{prop:d_n}.

We use the notation $C$ to denote a constant which may change from line to line, where subscripts are used to emphasize the dependency on other quantities.
For instance, $C_d$ represents a quantity depending only on $d$.

\subsection{Localization}
\label{sub:appendix:local}

We start by showing that the empirical risk $L_n$ is generalized self-concordant.
\begin{lemma}\label{lem:emp_risk_self_concordance}
    Under \Cref{asmp:self_concordance}, the empirical risk $L_n$ is $(n^{\nu/2-1} R, \nu)$-generalized self-concordant.
\end{lemma}
\begin{proof}
    By \Cref{asmp:self_concordance}, the loss $\ell(\cdot; Z_i)$ is $(R, \nu)$-generalized self-concordant for every $i \in [n] := \{1, \dots, n\}$.
    Note that $L_n$ is the empirical average of $\{\ell(\cdot; Z_i)\}_{i=1}^n$.
    Hence, it follows from \cite[Prop.~1]{sun2019generalized} that $L_n$ is $(n^{\nu/2-1}R, \nu)$-generalized self-concordant
\end{proof}

Applying \Cref{prop:self_concordance_local} to $L_n$ leads to the localization result.
Let $\lambda_{n,\star} := \lambda_{\min}(H_n(\theta_\star))$ and $\lambda_n^\star := \lambda_{\min}(H_n(\theta_\star))$.
Recall $K_\nu$ from \Cref{cor:K_nu}.
Define
\begin{align}\label{eq:R_n_nu_star}
    R_{n, \nu}^\star :=
    \begin{cases}
        \lambda_{n,\star}^{-1/2} R & \mbox{if } \nu = 2 \\
        (\nu/2 -1) \lambda_{n,\star}^{(\nu - 3)/2} n^{\nu/2-1} R & \mbox{if } \nu \in (2, 3] \\
        (\nu/2 - 1) (\lambda_{n}^\star)^{(\nu - 3)/2} n^{\nu/2-1} R & \mbox{if } \nu > 3.
    \end{cases}
\end{align}

We can then prove \Cref{prop:localization}.
\begin{customprop}{\ref{prop:localization}}
    Under \Cref{asmp:self_concordance},
    whenever $R_{n,\nu}^\star \norm{\grad_n(\theta_\star)}_{H_n^{-1}(\theta_\star)} \le K_\nu$,
    the estimator $\theta_n$ uniquely exists and satisfies
    \begin{align*}
        \norm{\theta_n - \theta_\star}_{H_n(\theta_\star)} \le 4 \norm{\grad_n(\theta_\star)}_{H_n^{-1}(\theta_\star)}.
    \end{align*}
\end{customprop}
\begin{proof}
    The claim follows directly from \Cref{lem:emp_risk_self_concordance,prop:self_concordance_local}.
\end{proof}

\Cref{prop:localization} implies that the empirical risk minimizer uniquely exists if $\norm{\grad_n(\theta_\star)}_{H_n^{-1}(\theta_\star)}$ is small.
Hence, it remains to bound $\norm{\grad_n(\theta_\star)}_{H_n^{-1}(\theta_\star)}$, which can be achieved by controlling $\norm{\grad_n(\theta_\star)}_{H_\star^{-1}}$ and $H_n(\theta_\star)$.
Let $\Omega(\theta) := G(\theta)^{1/2} H(\theta)^{-1} G(\theta)^{1/2}$ and $\Omega_\star := \Omega(\theta_\star)$
Recall from \Cref{def:effective_dim} that $d_\star = \Tr(\Omega_\star)$.
\begin{lemma}\label{lem:score}
    Under \Cref{asmp:sub_gaussian}, it holds that, with probability at least $1 - \delta$,
    \begin{align*}
        \norm{S_n(\theta_\star)}_{H_\star^{-1}}^{2} \le \frac{d_\star}n + C K_{1}^2 \log{(e / \delta)} \frac{\norm{\Omega_\star}_2}{n}.
    \end{align*}
\end{lemma}
\begin{proof}
    By the first order optimality condition, we have $S(\theta_\star) = 0$.
    As a result,
    \begin{align*}
        X := \sqrt{n} G^{-1/2}(\theta_\star) S_n(\theta_\star; Z)
    \end{align*}
    is an isotropic random vector.
    Moreover, it follows from \Cref{lem:sum_subg} that $\norm{X}_{\psi_2} \lesssim K_{1}$.
    Define $J := G^{1/2}_\star H_\star^{-1} G^{1/2}_\star / n$.
    Then we have
    \begin{align*}
        \norm{S_n(\theta_\star)}_{H_\star^{-1}}^{2} = \norm{X}_{J}^2.
    \end{align*}
    Invoking \Cref{thm:isotropic_tail} yields the claim.
\end{proof}

The next result characterizes the concentration of $H_n(\theta_\star)$.
Let
\begin{align}\label{eq:t_n}
    t_n := t_n(\delta) := \frac{2\sigma_H^2}{-K_2 + \sqrt{K_2^2 + 2\sigma_H^2 n/\log{(4d/\delta)}}}.
\end{align}
Note that it decays to 0 at rate $O(n^{-1/2})$ as $n \rightarrow \infty$.
\begin{lemma}\label{lem:hessian}
    Under \Cref{asmp:bernstein} with $r = 0$, it holds that, with probability at least $1 - \delta$,
    \begin{align*}
        (1 - t_n) H_\star \preceq H_n(\theta_\star) \preceq (1 + t_n) H_\star.
    \end{align*}
    Furthermore, if $n \ge 4(K_2 + 2\sigma_H^2) \log{(2d/\delta)}$, we have $t_n \le 1/2$ and thus
    \begin{align*}
        \frac12 H_\star \preceq H_n(\theta_\star) \preceq \frac32 H_\star.
    \end{align*}
\end{lemma}
\begin{proof}
    Due to \Cref{asmp:bernstein}, the standardized Hessian at $\theta_\star$
    \begin{align*}
        H_\star^{-1/2} H(\theta_\star; Z) H_\star^{-1/2} - I_d
    \end{align*}
    satisfies a Bernstein condition with parameter $K_2$.
    It then follows from \Cref{thm:bernstein_matrix} that
    \begin{align*}
        \Prob\left( \anorm{H_\star^{-1/2} H_n(\theta_\star) H_\star^{-1/2} - I_d}_2 \ge t \right) \le 2d \exp\left\{-\frac{nt^2}{2(\sigma_H^2 + K_2 t)} \right\}.
    \end{align*}
    As a result, it holds that, with probability at least $1 - \delta$,
    $(1 - t_n)I_d \preceq H_\star^{-1/2} H_n(\theta_\star) H_\star^{-1/2} \le (1 + t_n)I_d$, or equivalently,
    \begin{align*}
        (1 - t_n) H_\star \preceq H_n(\theta_\star) \preceq (1 + t_n) H_\star.
    \end{align*}
    Hence, whenever $n \ge 4(K_2 + 2\sigma_H^2) \log{(2d/\delta)}$, we have
    \begin{align*}
        \frac12 H_\star \preceq H_n(\theta_\star) \preceq \frac32 H_\star.
    \end{align*}
\end{proof}

We then prove \Cref{prop:score}.
Recall $t_n$ from \eqref{eq:t_n}.
\begin{customprop}{\ref{prop:score}}
    Under \Cref{asmp:sub_gaussian,asmp:bernstein} with $r = 0$, if $n \ge 4(K_2 + 2\sigma_H^2) \log{(4d/\delta)}$, then we have $t_n \le 1/2$ and, with probability at least $1 - \delta$,
    \begin{align*}
        \norm{S_n(\theta_\star)}_{H_n^{-1}(\theta_\star)}^2
        \le \frac{d_\star}{n(1-t_n)} + C K_{1}^2 \log{(e / \delta)} \frac{\norm{\Omega_\star}_2}{n(1-t_n)}.
    \end{align*}
\end{customprop}

\begin{proof}
    Define two events
    \begin{align*}
        \calA := \left\{ \norm{S_n(\theta_\star)}_{H_\star^{-1}}^{2} \le \frac{d_\star}{n} + C K_{1}^2 \log{(2e / \delta)} \frac{\norm{\Omega_\star}_2}{n} \right\} \quad \mbox{and} \quad \calB := \left\{ (1-t_n) H_\star \preceq H_n(\theta_\star) \preceq (1+t_n) H_\star \right\}.
    \end{align*}
    According to \Cref{lem:score,lem:hessian}, we have $\Prob(\calA) \ge 1 - \delta/2$ and $\Prob(\calB) \ge 1 - \delta/2$.
    On the event $\calA \calB$, we have
    \begin{align*}
        \norm{\grad_n(\theta_\star)}_{H_n^{-1}(\theta_\star)}^2
        \le \frac1{1-t_n} \norm{S_n(\theta_\star)}_{H_\star^{-1}}^2
        \le \frac{d_\star}{n(1-t_n)} + C K_{1}^2 \log{(2e / \delta)} \frac{\norm{\Omega_\star}_2}{n(1-t_n)}.
    \end{align*}
    Since $\Prob(\calA \calB) \ge 1 - \Prob(\calA^c) - \Prob(\calB^c) \ge 1 - \delta$, we have, with probability at least $1 - \delta$,
    \begin{align*}
        \norm{S_n(\theta_\star)}_{H_n^{-1}(\theta_\star)}^2 \le \frac{d_\star}{n(1-t_n)} + C K_{1}^2 \log{(e / \delta)} \frac{\norm{\Omega_\star}_2}{n(1-t_n)}.
    \end{align*}
    If $n \ge 4(K_2 + 2\sigma_H^2) \log{(4d/\delta)}$, then $t_n \le 1/2$ and thus
    \begin{align*}
        \norm{\grad_n(\theta_\star)}_{H_n^{-1}(\theta_\star)}^2 \le \frac{2d_\star}{n} + C K_{1}^2 \log{(e / \delta)} \frac{\norm{\Omega_\star}_2}{n}.
    \end{align*}
\end{proof}


\subsection{Proof of the main theorems}
\label{sub:appendix:thm}

Before we prove the main theorem, we control the empirical Hessian as in \Cref{prop:emp_hess_est}.
A na\"ive approach is to invoke \Cref{lem:hessian} to bound $H_n(\theta)$ by $H_n(\theta_\star)$.
However, this would not work since the generalized self-concordance parameter of $L_n$, i.e., $n^{\nu/2-1} R$, is diverging as $n \rightarrow \infty$.
Hence, we use a covering number argument: 1) we take a covering with radius $O(n^{1-\nu/2})$; 2) we bound $H_n(\theta)$ by $H_n(\pi(\theta))$ where $\pi(\theta)$ is the projection of $\theta$ onto the covering. The factor $n^{1-\nu/2}$ in the radius will cancel out with the factor $n^{\nu/2-1}$ in the generalized self-concordance parameter; 3) we bound $H_n(\pi(\theta))$ by $H(\pi(\theta))$ using matrix concentration; 4) we bound $H(\pi(\theta))$ by $H(\theta_\star)$ where the generalized self-concordance parameter of $L$ is $R$.
Recall $t_n$ from \eqref{eq:t_n}, $\lambda_\star := \lambda_{\min}(H_\star)$ and $\lambda^\star := \lambda_{\max}(H_\star)$.
Let $\omega_\nu(\tau) := e^{\tau}$ if $\nu = 2$ and $(1 - \tau)^{-2/(\nu-2)}$ if $\nu > 2$.
\begin{align}\label{eq:R_nu_star}
    R_{\nu}^\star :=
    \begin{cases}
        \lambda_{\star}^{-1/2} R & \mbox{if } \nu = 2 \\
        (\nu/2 -1) \lambda_{\star}^{(\nu - 3)/2} R & \mbox{if } \nu \in (2, 3] \\
        (\nu/2 - 1) (\lambda^\star)^{(\nu - 3)/2} R & \mbox{if } \nu > 3.
    \end{cases}
\end{align}

\begin{customprop}{\ref{prop:emp_hess_est}}
    Fix $\varepsilon \in (0, K_\nu]$ and let $s_n := t_n\big( 3^{-d}[1.5 \omega_\nu(\varepsilon) n]^{d(1-\nu/2)} \delta/2 \big)$.
    Under \Cref{asmp:self_concordance,asmp:bernstein} with $r = K_\nu  / R_\nu^\star$, it holds that, with probability at least $1 - \delta$,
    \begin{align*}
        \frac1{2\omega_\nu^2(\varepsilon)} H_\star \preceq \frac{1-s_n}{\omega_\nu^2(\varepsilon)} H_\star \preceq H_n(\theta) \preceq (1 + s_n) \omega_\nu^2(\varepsilon) H_\star \preceq \frac32 \omega_\nu^2(\varepsilon) H_\star, \;\mbox{for all } \theta \in \Theta_{\varepsilon/R_\nu^\star}(\theta_\star),
    \end{align*}
    whenever $n \ge 4(K_2 + 2\sigma_H^2) \left\{ \log{(4d/\delta)} + d \log{[3(1.5 \omega_\nu(\varepsilon)n)^{\nu/2-1}]}\right\}$.
\end{customprop}

\begin{proof}
    We prove the result in the following steps.

    \emph{Step 1. Take a $\tau$-covering and relate $H_n(\theta)$ to $H_n(\bar \theta)$ for some $\bar \theta$ in the covering.}
    Let $\tau := \varepsilon / R_\nu^\star [1.5 \omega_\nu(\varepsilon) n]^{\nu/2-1}$. Take an $\tau$-covering $\calN_\tau$ of $\Theta_{\varepsilon/R_\nu^\star}(\theta_\star)$ w.r.t.~$\norm{\cdot}_{H_\star}$, and let $\pi(\theta)$ be the projection of $\theta$ onto $\calN_\tau$.
    Let
    \begin{align*}
        d_{n, \nu}(\theta_1, \theta_2) :=
        \begin{cases}
            n^{\nu/2-1} R \norm{\theta_2 - \theta_1}_2 & \mbox{if } \nu = 2 \\
            (\nu/2-1) n^{(\nu/2-1)} R \norm{\theta_2 - \theta_1}_2^{3 - \nu} \norm{\theta_2 - \theta_1}_{H_n(\theta_1)}^{\nu-2} & \mbox{otherwise}.
        \end{cases}
    \end{align*}
    By \Cref{lem:emp_risk_self_concordance} and \Cref{prop:hessian}, we have, for all $\theta \in \Theta_{\varepsilon/R_\nu^\star}(\theta_\star)$,
    \begin{align}\label{eq:emp_hess_sandwich}
        \frac1{\omega_\nu(d_{n,\nu}(\pi(\theta), \theta))} H_n(\pi(\theta)) \preceq H_n(\theta) \preceq \omega_\nu(d_{n,\nu}(\pi(\theta), \theta)) H_n(\pi(\theta)),
    \end{align}
    where it holds if $d_{n,\nu}(\pi(\theta), \theta) < 1$ for the case $\nu > 2$.
    
    \emph{Step 2. Relate $H_n(\theta)$ to $H_\star$ for all $\theta$ in the covering.} Fix an arbitrary $\theta \in \calN_\tau$. Following the same argument as \Cref{lem:hessian}, we have, with probability at least $1 - \delta$,
    \begin{align}\label{eq:concentration_Hn}
        (1-t_n) H(\theta) \preceq H_n(\theta) \preceq (1+t_n) H(\theta).
    \end{align}
    It follows from \Cref{asmp:self_concordance} and \Cref{lem:bound_d_nu} that
    \begin{align}\label{eq:pop_hess_bound}
        \frac1{\omega_\nu(R_\nu^\star \norm{\theta - \theta_\star}_{H_\star})} H_\star \preceq H(\theta) \preceq \omega_\nu(R_\nu^\star \norm{\theta - \theta_\star}_{H_\star}) H_\star,
    \end{align}
    since $R_\nu^\star \norm{\theta - \theta_\star}_{H_\star} \le \varepsilon \le K_\nu < 1$.
    By the monotonicity of $\omega_\nu$, we get
    \begin{align*}
        \frac1{\omega_\nu(\varepsilon)} H_\star \preceq H(\theta) \preceq \omega_\nu(\varepsilon) H_\star,
    \end{align*}
    and thus, with probability at least $1 - \delta$,
    \begin{align*}
        \frac{1-t_n(\delta/2)}{\omega_\nu(\varepsilon)} H_\star \preceq H_n(\theta) \preceq [1 + t_n(\delta/2)] \omega_\nu(\varepsilon) H_\star.
    \end{align*}
    Let $s_n := t_n\big( (\tau R_\nu^\star/3\varepsilon)^d \delta/2 \big)$ and
    \begin{align*}
        \calA := \left\{ \frac{1-s_n}{\omega_\nu(\varepsilon)} H_\star \preceq H_n(\pi(\theta)) \preceq (1+s_n) \omega_\nu(\varepsilon) H_\star, \mbox{ for all } \theta \in \Theta_{\varepsilon/R_{\nu}^\star}(\theta_\star) \right\}.
    \end{align*}
    Since $\abs{\calN_\tau} \le (3\varepsilon/\tau R_\nu^\star)^d$ \citep{ostrovskii2021finite}, by a union bound, we have $\Prob(\calA) \ge 1 - \delta$.
    
    \emph{Step 3. Combine the previous two steps.}
    On the event $\calA$, we have $H_n(\pi(\theta)) \preceq (1+s_n) \omega_\nu(\varepsilon) H_\star$ for all $\theta \in \Theta_{\varepsilon/R_\nu^\star}(\theta_\star)$.
    A similar argument as \Cref{lem:bound_d_nu} shows that
    \begin{align*}
        d_{n, \nu}(\pi(\theta), \theta) \le
        \begin{cases}
            \lambda_\star^{-1/2} R \tau & \mbox{if } \nu = 2 \\
            (\nu/2-1) \lambda_\star^{(\nu-3)/2} [(1+s_n)\omega_\nu(\varepsilon)]^{(\nu-2)/2} n^{\nu/2-1} R \tau & \mbox{if } \nu \in (2, 3] \\
            (\nu/2-1) (\lambda^\star)^{(\nu - 3)/2} [(1+s_n)\omega_\nu(\varepsilon)]^{(\nu-2)/2} n^{\nu/2-1} R \tau & \mbox{otherwise},
        \end{cases}
    \end{align*}
    which is equal to $[(1+s_n)\omega_\nu(\varepsilon)]^{\nu/2-1} n^{\nu/2-1}R_\nu^\star \tau$.
    When $n \ge 4(K_2 + 2\sigma_H^2) \left\{ \log{(4d/\delta)} + d \log{[3(1.5 \omega_\nu(\varepsilon)n)^{\nu/2-1}]}\right\}$, we have $s_n \le 1/2$, and thus
    substituting $\tau$ gives $d_{n, \nu}(\pi(\theta), \theta) \le \varepsilon \le K_\nu < 1$.
    Hence, by \eqref{eq:emp_hess_sandwich}, we obtain
    \begin{align*}
        \frac{1-s_n}{\omega_\nu^2(\varepsilon)} H_\star \preceq H_n(\theta) \preceq (1+s_n) \omega_\nu^2(\varepsilon) H_\star, \quad \mbox{for all } \theta \in \Theta_{\varepsilon/R_\nu^\star}(\theta_\star).
    \end{align*}
    on the event $\calA$.
\end{proof}

We give below the precise version of \Cref{thm:risk_bound_generalized}.
Recall $K_\nu$ and $R_\nu^\star$ from \Cref{cor:K_nu} and \eqref{eq:R_nu_star}.
\begin{customthm}{\ref{thm:risk_bound_generalized}}
    Let $\nu \in [2, 3)$.
    Under \Cref{asmp:self_concordance,asmp:sub_gaussian,asmp:bernstein} with $r = 0$, we have,
    whenever
    \begin{align*}
        n \ge \max\left\{ 4(K_2 + 2\sigma_H^2) \log{(4d/\delta)}, C\left[\frac{(R_\nu^\star)^2 K_1^2 d_\star \log{(e/\delta)}}{K_\nu^2}\right]^{1/(3-\nu)} \right\},
    \end{align*}
    the empirical risk minimizer $\theta_n$ uniquely exists and satisfies, with probability at least $1 - \delta$,
    \begin{align*}
        \norm{\theta_n - \theta_\star}^2_{H_\star} \le  \frac{16 d_\star}{n} + C K_{1}^2 \log{(e / \delta)} \frac{\norm{\Omega_\star}_2}{n}.
    \end{align*}
\end{customthm}

\begin{proof}
    Similar to the proof of \Cref{prop:score}, we define two events
    \begin{align*}
        \begin{split}
            \calA := \left\{ \norm{S_n(\theta_\star)}_{H_\star^{-1}}^{2} \le \frac{d_\star}n + CK_{1}^2 \log{(2e / \delta)} \frac{\norm{\Omega_\star}_2}{n} \right\} \quad \mbox{and} \quad \calB := \left\{ \frac12 H_\star \preceq H_n(\theta_\star) \preceq \frac32 H_\star \right\}.
        \end{split}
    \end{align*}
    In the following, we let
    \begin{align*}
        n \gtrsim \max\left\{ 4(K_2 + 2\sigma_H^2) \log{(4d/\delta)}, \left[\frac{(R_\nu^\star)^2 K_1^2 d_\star \log{(e/\delta)}}{K_\nu^2}\right]^{1/(3-\nu)} \right\}.
    \end{align*}
    Following the same argument as \Cref{prop:score}, we have $\Prob(\calA \calB) \ge 1 - \delta$ and
    \begin{align*}
        \norm{\grad_n(\theta_\star)}_{H_n^{-1}(\theta_\star)}^2 \le \frac{2 d_\star}{n} + C K_{1}^2 \log{(e / \delta)} \frac{\norm{\Omega_\star}_2}{n} \le C K_{1}^2 \log{(e / \delta)} \frac{d_\star}{n}.
    \end{align*}
    Now, it suffices to prove, on the event $\calA \calB$,
    \begin{align*}
        \norm{\theta_n - \theta_\star}_{H_\star}^2 \le \frac{16 d_\star}{n} + C K_{1}^2 \log{(e / \delta)} \frac{\norm{\Omega_\star}_2}{n}.
    \end{align*}
    
    Recall $R_{n, \nu}^\star$ and $R_{\nu}^\star$ from \eqref{eq:R_n_nu_star} and \eqref{eq:R_nu_star}.
    It is straightforward to check that $R_{n, \nu}^\star \le \sqrt{2} n^{\nu/2-1} R_\nu^\star$ for all $\nu \in [2, 3]$.
    Consequently, it holds that
    \begin{align*}
        R_{n,\nu}^\star \norm{S_n(\theta_\star)}_{H_n^{-1}(\theta_\star)} \lesssim R_\nu^\star n^{(\nu - 3)/2} \sqrt{K_{1}^2 \log{(e / \delta)} d_\star} \le K_\nu
    \end{align*}
    since $n^{3-\nu} \gtrsim (R_\nu^\star)^2 K_1^2 \log{(e / \delta)} d_\star/ K_\nu^2$.
    As a result, by \Cref{prop:localization}, we have that $\theta_n$ uniquely exists and satisfies
    \begin{align*}
        \norm{\theta_n - \theta_\star}_{H_n(\theta_\star)} \le 4 \norm{S_n(\theta_\star)}_{H_n^{-1}(\theta_\star)},
    \end{align*}
    and thus, using the event $\calB$,
    \begin{align*}
        \norm{\theta_n - \theta_\star}_{H_\star}^2
        \le 2\norm{\theta_n - \theta_\star}_{H_n(\theta_\star)}^2
        \le \frac{16 d_\star}{n} + C K_{1}^2 \log{(e / \delta)} \frac{\norm{\Omega_\star}_2}{n}.
    \end{align*}
\end{proof}


We give below the precise version of \Cref{thm:conf_set}.
\begin{customthm}{\ref{thm:conf_set}}
    Let $\nu \in [2, 3)$ and $r_n := \sqrt{CK_1^2 \log{(e/\delta)} d_\star / n}$.
    Suppose the same assumptions in \Cref{thm:risk_bound_generalized} hold true.
    Furthermore, suppose that \Cref{asmp:bernstein} holds with $r = K_\nu / R_\nu^\star$.
    Let
    \begin{align*}
        \calC_n(\delta) := \left\{\theta \in \Theta: \norm{\theta_n - \theta}_{H_n(\theta_n)}^2 \le 24 \omega_\nu^2(r_n R_\nu^\star) \frac{d_\star}{n} + C K_1^2 \omega_\nu^2(r_n R_\nu^\star) \log{(e/\delta)} \frac{\norm{\Omega_\star}_2}{n} \right\}.
    \end{align*}
    Then we have $\Prob(\theta_\star \in \calC_n(\delta)) \ge 1 - \delta$ whenever $n$ satisfies
    \begin{align*}
        n \ge C \max\left\{(K_2 + \sigma_H^2)\left[\log(2d/\delta) + d\log{(\omega_{\nu}(K_\nu) n)} \right], \left[ \frac{(R_\nu^\star)^2 K_1^2 d_\star \log{(e/\delta)}}{K_\nu^2} \right]^{1/(3-\nu)} \right\}.
    \end{align*}
    Here $C$ is an absolute constant which may change from line to line.
\end{customthm}

\begin{proof}
    We start by defining some events:
    \begin{align}\label{eq:def_events}
        \begin{split}
            \calA &:= \left\{ \norm{S_n(\theta_\star)}_{H_\star^{-1}}^{2} \le \frac{d_\star}n + C K_{1}^2 \log{(3e / \delta)} \frac{\norm{\Omega_\star}_2}{n} \right\} \\
            \calB &:= \left\{ \frac12 H_\star \preceq H_n(\theta_\star) \preceq \frac32 H_\star \right\} \\
            \calC &:= \left\{ \frac1{2 \omega_\nu^2(r_n R_\nu^\star)} H_\star \preceq H_n(\theta) \preceq \frac{3}{2} \omega_\nu^2(r_n R_\nu^\star) H_\star, \quad \mbox{for all } \theta \in \Theta_{r_n}(\theta_\star) \right\}.
        \end{split}
    \end{align}
    In the following, we let
    \begin{align*}
        n \ge C \max\left\{(K_2 + \sigma_H^2)\left[\log(2d/\delta) + d\log{(\omega_{\nu}(K_\nu) n)} \right], \left[ \frac{(R_\nu^\star)^2 K_1^2 d_\star \log{(e/\delta)}}{K_\nu^2} \right]^{1/(3-\nu)} \right\}.
    \end{align*}
    It then follows that $r_n R_{\nu}^\star \le K_\nu$.
    According to \Cref{lem:score}, \Cref{lem:hessian}, and \Cref{prop:emp_hess_est} (with $\varepsilon = r_n R_\nu^\star$), it holds that $\Prob(\calA) \ge 1 - \delta/3$, $\Prob(\calB) \ge 1 - \delta/3$, and $\Prob(\calC) \ge 1 - \delta/3$.
    This implies that $\Prob(\calA \calB \calC) \ge 1 - \delta$.
    Now, it suffices to prove, on the event $\calA \calB \calC$,
    \begin{align*}
        \norm{\theta_n - \theta_\star}_{H_n(\theta_n)}^2 \le 24 \omega_\nu^2(r_n R_\nu^\star) \frac{d_\star}{n} + C K_1^2 \omega_\nu^2(r_n R_\nu^\star) \log{(e/\delta)} \frac{\norm{\Omega_\star}_2}{n}.
    \end{align*}
    
    Following the same argument as \Cref{thm:risk_bound_generalized}, we obtain
    \begin{align*}
        \norm{\theta_n - \theta_\star}_{H_\star}^2
        \le 2\norm{\theta_n - \theta_\star}_{H_n(\theta_\star)}^2
        \le \frac{16d_\star}{n} + CK_1^2 \log{(e/\delta)} \frac{\norm{\Omega_\star}_2}{n}
        \le r_n^2.
    \end{align*}
    Therefore, using the event $\calC$, we have
    \begin{align*}
        \norm{\theta_n - \theta_\star}_{H_n(\theta_n)}^2
        \le \frac32 \omega_\nu^2(r_n R_\nu^\star) \norm{\theta_n - \theta_\star}_{H_\star}^2
        \le 24 \omega_\nu^2(r_n R_\nu^\star) \frac{d_\star}{n} + C K_1^2 \omega_\nu^2(r_n R_\nu^\star) \log{(e/\delta)} \frac{\norm{\Omega_\star}_2}{n},
    \end{align*}
    which completes the proof.
\end{proof}

\subsection{Consistency of $d_n$}
\label{sub:appendix:consist_dn}

Now we are ready to prove \Cref{prop:d_n}.
Recall $t_n$ from \eqref{eq:t_n} and $r_n$ from \Cref{thm:conf_set}.
\begin{customprop}{\ref{prop:d_n}}
    Let $\nu \in [2, 3)$ and $s_n := CMr_n + CK_1^2(1 + Mr_n) (d/n) \log{(Mnr_n/\delta)}$.
    Under Asms.~\ref{asmp:self_concordance}, \ref{asmp:subG_local}, \ref{asmp:bernstein}, and \ref{asmp:lip} with $r = K_\nu/R_\nu^\star$, it holds that, with probability at least $1 - \delta$,
    \begin{align*}
        \frac{1 - t_n}{\omega_\nu^2(r_n R_\nu^\star)(1 + s_n)} d_n \le d_\star \le \frac{(1 + t_n)\omega_\nu^2(r_n R_\nu^\star)}{1 - s_n} d_n
    \end{align*}
    whenever $n$ satisfies
    \begin{align*}
        n \ge C \max\left\{ (K_2 + \sigma_H^2 + K_1^2) \left[ \log{(2d/\delta)} + d\log{(\omega_{\nu}(K_\nu) n/\delta)} \right], \left[ (M + R_\nu^\star/K_\nu)^2 K_1^2 d_\star \log{(e/\delta)} \right]^{1/(3-\nu)} \right\}.
    \end{align*}
\end{customprop}

\begin{proof}
    Let $\tau := \delta / (Mn)$.
    Take a $\tau$-covering of $\calN_\tau$ of $\Theta_{r_n}(\theta_\star)$ w.r.t.~$\norm{\cdot}_{H_\star}$, and let $\pi(\theta)$ be the projection of $\theta$ onto $\calN_\tau$.
    For simplicity of the notation, we define
    \begin{align*}
        \norm{A}_{B} := \norm{B^{1/2} A B^{1/2}}
    \end{align*}
    for a symmetric matrix $A$ and a psd matrix $B$.
    We start by defining some events.
    Let
    \begin{equation*}
        \begin{split}
            \calA &:= \left\{ \norm{S_n(\theta_\star)}_{H_\star^{-1}}^{2} \lesssim \frac1n K_{1}^2 \log{(5e / \delta)} d_\star \right\} \\
            \calB &:= \left\{ (1-t_n) H_\star \preceq H_n(\theta_\star) \preceq (1+t_n) H_\star \right\} \\
            \calC &:= \left\{ \frac{1 - t_n}{\omega_\nu^2(r_n R_\nu^\star)} H_\star \preceq H_n(\theta) \preceq (1 + t_n) \omega_\nu^2(r_n R_\nu^\star) H_\star, \quad \mbox{for all } \theta \in \Theta_{r_n}(\theta_\star) \right\} \\
            \calD &:= \left\{ \sup_{\theta \in \Theta_{r_n}(\theta_\star)} \norm{G_n(\theta) - G_n(\pi(\theta))}_{G_\star^{-1}} \le 5M\tau/\delta \right\} \\
            \calE &:= \left\{ \sup_{\theta \in \Theta_{r_n}(\theta_\star)} \norm{G_n(\pi(\theta)) - G(\pi(\theta))}_{G_\star^{-1}} \lesssim K_1^2 (1 + Mr_n) h\left( \frac{d\log{(36 r_n/\tau)} + \log{(10/\delta)}}{n} \right) \right\},
        \end{split}
    \end{equation*}
    where $h(t) := \max\{t^2, t\}$.
    In the following, we let
    \begin{align}\label{eq:n_constraint}
        n \ge C \max\left\{ (K_2 + \sigma_H^2 + K_1^2) \left[ \log{(2d/\delta)} + d\log{(\omega_{\nu}(K_\nu) n/\delta)} \right], \left[ (M + R_\nu^\star/K_\nu)^2 K_1^2 d_\star \log{(e/\delta)} \right]^{1/(3-\nu)} \right\}.
    \end{align}
    It then follows that $t_n \le 1/2$, $r_n \le K_\nu / R_\nu^\star = r$ and $s_n < 1$.
    According to \Cref{lem:score}, \Cref{lem:hessian}, and \Cref{prop:emp_hess_est}, it holds that $\Prob(\calA) \ge 1 - \delta/5$, $\Prob(\calB) \ge 1 - \delta/5$, and $\Prob(\calC) \ge 1 - \delta/5$.
    In the following, we prove the claim in three steps.
    
    \emph{Step 1. Control the probability of $\calD$.}
    By Markov's inequality, it holds that
    \begin{align*}
        \Prob(\calD^c)
        &\le \frac{\delta}{5M\tau} \Expect\left[ \sup_{\theta \in \Theta_{r_n}(\theta_\star)} \norm{G_n(\theta) - G_n(\pi(\theta))}_{G_\star^{-1}} \right]
        \txtover{Jensen's}{\le} \frac{\delta}{5M\tau} \sup_{\theta \in \Theta_{r_n}(\theta_\star)} \norm{G(\theta) - G(\pi(\theta))}_{G_\star^{-1}}.
    \end{align*}
    According to \Cref{asmp:lip}, we have
    \begin{align}\label{eq:lip_reformulation}
        M \norm{\theta_1 - \theta_2}_{H_\star} \ge \Expect[\norm{G(\theta_1; Z) - G(\theta_2; Z)}_{G_\star^{-1}}], \quad \mbox{for all } \theta_1, \theta_2 \in \Theta_r(\theta_\star).
    \end{align}
    It follows from Jensen's inequality that
    \begin{align}\label{eq:lip_jensen}
        M \norm{\theta_1 - \theta_2}_{H_\star} \ge \norm{G(\theta_1) - G(\theta_2)}_{G_\star^{-1}}, \quad \mbox{for all } \theta_1, \theta_2 \in \Theta_r(\theta_\star).
    \end{align}
    As a result,
    \begin{align*}
        \Prob(\calD^c) \le \frac{\delta}{5\tau} \norm{\theta - \pi(\theta)}_{H_\star} \le \frac{\delta}{5}.
    \end{align*}
    
    \emph{Step 2. Control the probability of $\calE$.}
    According to \citet[Exercise 4.4.3]{vershynin2018high}, we have
    \begin{align}\label{eq:matrix_norm_cover}
        \norm{G_n(\pi(\theta)) - G(\pi(\theta))}_{G_\star^{-1}} \le \frac12 \sup_{v \in \calV_{1/4}} \abs{v^\top G_\star^{-1/2} [G_n(\pi(\theta)) - G(\pi(\theta))] G_\star^{-1/2} v},
    \end{align}
    where $\calV_{1/4}$ is a $1/4$-covering of the unit ball in $\reals^d$.
    Note that
    \begin{align*}
        v^\top G_\star^{-1/2} (G_n(\pi(\theta)) - G(\pi(\theta))) G_\star^{-1/2} v = \frac1n \sum_{i=1}^n [W_i - \Expect[W_i]],
    \end{align*}
    where $W_i := [v^\top G_\star^{-1/2} S(\pi(\theta); Z_i)]^2$.
    Let $\bar v := G(\pi(\theta))^{1/2} G_\star^{-1/2} v$.
    By \Cref{asmp:subG_local},
    \begin{align*}
        \norm{v^\top G_\star^{-1/2} S(\pi(\theta); Z_i)}_{\psi_2}
        &= \norm{\bar v^\top G(\pi(\theta))^{-1/2} S(\pi(\theta); Z_i)}_{\psi_2} \\
        &\le \norm{\bar v}_2 K_1 \le \norm{G(\pi(\theta))^{1/2} G_\star^{-1/2}} K_1.
    \end{align*}
    Since $\pi(\theta) \in \Theta_{r_n}(\theta_\star) \subset \Theta_{r}(\theta_\star)$, it follows from \eqref{eq:lip_jensen} that 
    \begin{align*}
        \norm{G_\star^{-1/2} G(\pi(\theta)) G_\star^{-1/2} - I_d} \le M \norm{\pi(\theta) - \theta_\star}_{H_\star} \le M r_n.
    \end{align*}
    and thus
    \begin{align*}
        \norm{v^\top G_\star^{-1/2} S(\pi(\theta); Z_i)}_{\psi_2} \le \sqrt{1 + Mr_n} K_1.
    \end{align*}
    This implies, by \citet[Lemma 2.7.6]{vershynin2018high}, $W_i$ is sub-Exponential with $\norm{W_i}_{\psi_1} \le K_1^2 (1 + Mr_n)$.
    It then follows from the Bernstein inequality that
    \begin{align*}
        \Prob\left( \abs{\frac1n \sum_{i=1}^n [W_i - \Expect[W_i]]} > t \right) \le 2 \exp\left(- c \min\left\{\frac{t^2}{K_1^4 (1 + Mr_n)^2}, \frac{t}{K_1^2 (1 + Mr_n)} \right\} \right).
    \end{align*}
    Since $\abs{\calN_\tau} \le (3r_n/\tau)^d$ and $\calV_{1/4} \le 12^d$,
    by a union bound, we get
    \begin{align*}
        &\quad \Prob\left( \frac12 \sup_{\theta \in \Theta_{r_n}(\theta_\star)} \sup_{v \in \calV_{1/4}} \abs{v^\top G_\star^{-1/2} [G_n(\pi(\theta)) - G(\pi(\theta))] G_\star^{-1/2} v} > t \right) \\
        &\le 2 \abs{\calN_\tau} \abs{\calV_{1/4}} \exp\left(- c \min\left\{\frac{4t^2}{K_1^4 (1 + Mr_n)^2}, \frac{2t}{K_1^2 (1 + Mr_n)} \right\} \right) \\
        &\le 2 (36r_n/\tau)^d \exp\left(- c \min\left\{\frac{4t^2}{K_1^4 (1 + Mr_n)^2}, \frac{2t}{K_1^2 (1 + Mr_n)} \right\} \right).
    \end{align*}
    Hence, it follows from \eqref{eq:matrix_norm_cover} that $\Prob(\calE^c) \le \delta/5$.
    
    \emph{Step 3. Prove the bound on the event $\calA \calB \calC \calD \calE$.}
    Following the same argument as \Cref{thm:risk_bound_generalized}, we obtain
    \begin{align}\label{eq:local}
        \norm{\theta_n - \theta_\star}_{H_\star} \lesssim \norm{\theta_n - \theta_\star}_{H_n(\theta_\star)} \lesssim n^{-1/2} \sqrt{K_{1}^2 \log{(e / \delta)} d_\star} = r_n.
    \end{align}
    Using the event $\calC$, we have
    \begin{align*}
        \frac1{(1 + t_n)\omega_\nu^2(r_n R_\nu^\star)} H_n(\theta_n) \preceq H_\star \preceq \frac{\omega_\nu^2(r_n R_\nu^\star)}{1 - t_n} H_n(\theta_n),
    \end{align*}
    and thus
    \begin{equation}\label{eq:d_star_first_bound}
    \begin{split}
        d_\star &\le (1+t_n) \omega_\nu^2(r_n R_\nu^\star) \Tr\left( H_n(\theta_n)^{-1/2} G_\star H_n(\theta_n)^{-1/2} \right) \\
        d_\star &\ge \frac{1-t_n}{\omega_\nu^2(r_n R_\nu^\star)} \Tr\left( H_n(\theta_n)^{-1/2} G_\star H_n(\theta_n)^{-1/2} \right).
    \end{split}
    \end{equation}
    Now it remains to control
    \begin{align*}
        \norm{G_n(\theta_n) - G_\star}_{G_\star^{-1}} \le \norm{G(\theta_n) - G_\star}_{G_\star^{-1}} + \norm{G_n(\theta_n) - G(\theta_n)}_{G_\star^{-1}}.
    \end{align*}
    We first control $\norm{G(\theta_n) - G_\star}_{G_\star^{-1}}$.
    It follows from \eqref{eq:lip_jensen} and \eqref{eq:local} that
    \begin{align*}
        \norm{G(\theta_n) - G_\star}_{G_\star^{-1}}
        \le M \norm{\theta_n - \theta_\star}_{H_\star}
        \lesssim M r_n.
    \end{align*}
    We then control $\norm{G_n(\theta_n) - G(\theta_n)}_{G_\star^{-1}}$.
    By \eqref{eq:local}, we have
    \begin{align*}
        \norm{G_n(\theta_n) - G(\theta_n)}_{G_\star^{-1}}
        \le \sup_{\theta \in \Theta_{r_n}(\theta_\star)} \norm{G_n(\theta) - G(\theta)}_{G_\star^{-1}}.
    \end{align*}
    It then follows from the triangle inequality that
    \begin{align*}
        \sup_{\theta \in \Theta_{r_n}(\theta_\star)} \norm{G_n(\theta) - G(\theta)}_{G_\star^{-1}} \le A_1 + A_2 + A_3,
    \end{align*}
    where
    \begin{align*}
        A_1 &:= \sup_{\theta \in \Theta_{r_n}(\theta_\star)} \norm{G(\pi(\theta)) - G(\theta)}_{G_\star^{-1}} \\
        A_2 &:= \sup_{\theta \in \Theta_{r_n}(\theta_\star)} \norm{G_n(\pi(\theta)) - G(\pi(\theta))}_{G_\star^{-1}} \\
        A_3 &:= \sup_{\theta \in \Theta_{r_n}(\theta_\star)} \norm{G_n(\theta) - G_n(\pi(\theta))}_{G_\star^{-1}}.
    \end{align*}
    
    To control $A_1$, note that, for all $\theta \in \Theta_{r_n}(\theta_\star)$,
    \begin{align*}
        \norm{G(\pi(\theta)) - G(\theta)}_{G_\star^{-1}} \txtover{\eqref{eq:lip_jensen}}{\le} M \norm{\pi(\theta) - \theta}_{H_\star}
        \le M \tau.
    \end{align*}
    Consequently, we obtain $A_1 \le M \tau$.
    To control $A_2$, we use the event $\calE$ to obtain
    \begin{align*}
        A_2 \lesssim K_1^2 (1 + Mr_n) h\left( \frac{d\log{(36 r_n/\tau)} + \log{(10/\delta)}}{n} \right).
    \end{align*}
    To control $A_3$, we use the event $\calD$ to obtain $A_3 \le 5M\tau/\delta$.
    Therefore,
    \begin{align*}
        \norm{G_n(\theta_n) - G_\star}_{G_\star^{-1}}
        &\le CM r_n + M\tau + 5M\tau / \delta + CK_1^2 (1 + Mr_n) h\left( \frac{d\log{(36 r_n/\tau)} + \log{(10/\delta)}}{n} \right) \\
        &= CM r_n + \frac{5+\delta}{n} + CK_1^2 (1 + Mr_n) h\left( \frac{d\log{(36M n r_n/\delta)} + \log{(10/\delta)}}{n} \right).
    \end{align*}
    This yields that
    \begin{align*}
        (1 - s_n) G_\star \preceq G_n(\theta_n) \preceq (1 + s_n) G_\star,
    \end{align*}
    and thus
    \begin{align*}
        \frac{1 - t_n}{\omega_\nu^2(r_n R_\nu^\star)(1 + s_n)} d_n \le d_\star \le \frac{(1 + t_n)\omega_\nu^2(r_n R_\nu^\star)}{1 - s_n} d_n.
    \end{align*}
\end{proof}


\subsection{Effective dimension}
\label{sub:appendix:effect_dim}

To gain a better understanding on the effective dimension $d_\star$, we summarize it in \Cref{tab:decay} under different regimes of eigendecay, assuming that $G_\star$ and $H_\star$ share the same eigenvectors.

\begin{table}[t]
    \caption{Comparison between the effective dimension $d_\star$ and the parameter dimension $d$ in different regimes of eigendecays of $G_\star$ and $H_\star$ assuming they share the same eigenvectors.}
    \label{tab:decay}
    \centering
    \renewcommand{\arraystretch}{1.2}
    \begin{tabular}{lccccc}
        \addlinespace[0.4em]
        \toprule
        & \multicolumn{2}{c}{\textbf{Eigendecay}} & \multicolumn{2}{c}{\textbf{Dimension Dependency}} & \textbf{Ratio} \\
        & $G_\star$ & $H_\star$ & $d_\star$ & $d$ & $d_\star/d$ \\
        \midrule
        Poly-Poly & $i^{-\alpha}$ & $i^{-\beta}$ & $d^{(\beta - \alpha + 1) \vee 0}$ & $d$ & $d^{(\beta - \alpha)\vee(-1)}$ \\
        Poly-Exp & $i^{-\alpha}$ & $ e^{-\nu i}$ & $d^{1-\alpha} e^{\nu d}$ & $d$ & $d^{-\alpha} e^{\nu d}$ \\
        Exp-Poly & $e^{-\mu i}$ & $i^{-\beta}$ & $1$ & $d$ & $d^{-1}$ \\
        Exp-Exp & $e^{-\mu i}$ & $e^{-\nu i}$ & \begin{tabular}{@{}c@{}} \qquad $d$ \mbox{ if }  $\mu = \nu$ \\ \qquad $1$ \mbox{ if } $\mu > \nu$ \\ $e^{(\nu - \mu) d}$ \mbox{ if } $\mu < \nu$ \end{tabular} & $d$ & \begin{tabular}{@{}c@{}} $1$ \mbox{ if }  $\mu = \nu$ \\ $d^{-1}$ \mbox{ if } $\mu > \nu$ \\ $d^{-1} e^{(\nu - \mu) d}$ \mbox{ if } $\mu < \nu$ \end{tabular} \\
        \bottomrule
    \end{tabular}
\end{table}
