In this section, we first recall and prove some key properties of generalized self-concordance.
We then review some key results regarding the concentration of random vectors and matrices.

\subsection{Properties of generalized self-concordant functions}
\label{sub:appendix:self_concordance}

Throughout this section, we let $f: \reals^d \rightarrow \reals$ be $(R, \nu)$-generalized self-concordant as in \Cref{def:general_self_concordance}, where $R > 0$ and $\nu \ge 2$.
For simplicity of the notation, we denote $\norm{\cdot}_x := \norm{\cdot}_{\nabla^2 f(x)}$.
Let
\begin{align}\label{eq:d_nu}
    d_\nu(x, y) :=
    \begin{cases}
        R \norm{y - x}_2 & \mbox{if } \nu = 2 \\
        (\nu/2 - 1) R \norm{y - x}_2^{3-\nu} \norm{y - x}_x^{\nu-2} & \mbox{if } \nu > 2
    \end{cases}
\end{align}
and
\begin{align}\label{eq:omega_nu}
    \omega_\nu(\tau) :=
    \begin{cases}
        (1 - \tau)^{-2/(\nu-2)} & \mbox{if } \nu > 2 \\
        e^{\tau} & \mbox{if } \nu = 2
    \end{cases}
\end{align}
with $\dom(\omega_\nu) = \reals$ if $\nu = 2$ and $\dom(\omega_\nu) = (-\infty, 1)$ if $\nu > 2$.

The next proposition gives bounds for the Hessian of $f$.
\begin{proposition}[\citet{sun2019generalized}, Prop.~8]
\label{prop:hessian}
    For any $x, y \in \dom(f)$, we have
    \begin{align*}
        \frac{1}{\omega_\nu(d_\nu(x, y))} \nabla^2 f(x) \preceq \nabla^2 f(y) \preceq \omega_\nu(d_\nu(x,y)) \nabla^2 f(x),
    \end{align*}
    where it holds if $d_\nu(x, y) < 1$ for the case $\nu > 2$.
\end{proposition}

We then give the bounds for function values.
Define two functions
\begin{align}\label{eq:bar_omega}
    \bar \omega_\nu(\tau) := \int_0^1 \omega_\nu(t \tau) \D t =
    \begin{cases}
        \tau^{-1} (e^\tau - 1) & \mbox{if } \nu = 2 \\
        -\tau^{-1} \log{(1 - \tau)} & \mbox{if } \nu = 4 \\
        \frac{\nu - 2}{\nu - 4} \frac{1 - (1 - \tau)^{(\nu-4)/(\nu-2)}}{\tau} & \mbox{otherwise}
    \end{cases}
\end{align}
and
\begin{align}
    \bbar{\omega}_\nu(\tau) := \int_0^1 t \bar \omega_\nu(t \tau) \D t =
    \begin{cases}
        \tau^{-2} (e^\tau - \tau - 1) & \mbox{if } \nu = 2 \\
        -\tau^{-2} [\tau + \log{(1 - \tau)}] & \mbox{if } \nu = 3 \\
        \tau^{-2} [(1 - \tau) \log{(1 - \tau)} + \tau] & \mbox{if } \nu = 4 \\
        \frac{\nu - 2}{\nu - 4} \frac1\tau \left[ \frac{\nu - 2}{2(3 - \nu) \tau} \left( (1 - \tau)^{2(3-\nu)/(2-\nu)} - 1 \right) - 1 \right] & \mbox{otherwise}.
    \end{cases}
\end{align}

\begin{proposition}[\citet{sun2019generalized}, Prop.~10]
\label{prop:function_value}
    For any $x, y \in \dom(f)$, we have
    \begin{align*}
        \bbar{\omega}_\nu(-d_\nu(x,y)) \norm{y - x}_x^2 \le f(y) - f(x) - \ip{\nabla f(x), y - x} \le \bbar{\omega}_\nu(d_\nu(x, y)) \norm{y - x}_x^2,
    \end{align*}
    where it holds if $d_\nu(x, y) < 1$ for the case $\nu > 2$.
\end{proposition}


In the following, we fix $x \in \dom(f)$ and assume $\nabla^2 f(x) \succ 0$.
We denote $\lambda_{\min} := \lambda_{\min}(\nabla^2 f(x))$ and $\lambda_{\max} := \lambda_{\max}(\nabla^2 f(x))$.
The next lemma bounds $d_\nu(x, y)$ with the local norm $\norm{y - x}_x$.
Let
\begin{align}\label{eq:R_nu}
    R_\nu :=
    \begin{cases}
        \lambda_{\min}^{-1/2} R & \mbox{if } \nu = 2 \\
        (\nu/2 -1) \lambda_{\min}^{(\nu - 3)/2} R & \mbox{if } \nu \in (2, 3] \\
        (\nu/2 - 1) \lambda_{\max}^{(\nu - 3)/2} R & \mbox{if } \nu > 3.
    \end{cases}
\end{align}
\begin{lemma}\label{lem:bound_d_nu}
    For any $\nu \ge 2$ and $y \in \dom(f)$, we have
    \begin{align}
        d_\nu(x, y) \le R_\nu \norm{y - x}_x.
    \end{align}
    Moreover, it holds that
    \begin{align*}
        \frac1{\omega_\nu(R_\nu \norm{y - x}_x)} \nabla^2 f(x) \preceq \nabla^2 f(y) \preceq \omega_\nu(R_\nu \norm{y - x}_x) \nabla^2 f(x),
    \end{align*}
    where it holds if $R_\nu \norm{y - x}_x < 1$ for the case $\nu > 2$.
\end{lemma}
\begin{proof}
    Recall the definition of $d_\nu$ in \eqref{eq:d_nu}.
    If $\nu = 2$, then, by the Cauchy-Schwarz inequality,
    \begin{align*}
        d_\nu(x, y) = R \norm{y - x}_2 \le \norm{[\nabla^2 f(x)]^{-1/2}}_2 R \norm{y - x}_x \le \lambda_{\min}^{-1/2} R \norm{y - x}_x.
    \end{align*}
    The case $\nu > 2$ can be proved similarly.
\end{proof}

We then prove some useful properties for the function $\bbar{\omega}$.
\begin{lemma}
\label{lem:monotonicity}
    For any $\nu \ge 2$, the following statements hold true:
    \begin{enumerate}[label=(\alph*)]
        \item\label{item:varphi} The function $\varphi(\tau) := \bbar{\omega}_\nu(-\tau)$ is strictly decreasing on $[0, \infty)$ with $\varphi(0) = 1/2$ and $\varphi(\tau) \ge 0$ for all $\tau \ge 0$.
        \item\label{item:psi} The function $\psi(\tau) := \bbar{\omega}_\nu(-\tau) \tau$ is strictly increasing on $[0, \infty)$ with $\psi(0) = 0$.
    \end{enumerate}
\end{lemma}
\begin{proof}
    \textbf{\ref{item:varphi}}.
    By definition, $\omega_\nu$ is strictly increasing on $(-\infty, 1)$.
    As a result, for any $\tau \in (-\infty, 1)$,
    \begin{align*}
        \bar \omega_\nu'(\tau) = \int_0^1 t \omega_\nu'(t \tau) \D t > 0.
    \end{align*}
    It then follows that, for any $\tau \ge 0$,
    \begin{align*}
        \varphi'(\tau)
        &= - \bbar{\omega}_\nu'(-\tau)
        = -\int_0^1 t^2 \bar \omega_\nu'(-t\tau) \D t < 0,
    \end{align*}
    and thus $\varphi$ is strictly decreasing on $[0, \infty)$.
    Note that $\omega_\nu(0) = 1$ and $\omega_\nu(\tau) > 0$ for all $\tau \in (-\infty, 1)$.
    It is straightforward to check that $\varphi(0) = 1/2$ and $\varphi(\tau) > 0$ for all $\tau \ge 0$.
    
    \textbf{\ref{item:psi}} Due to \eqref{eq:bar_omega}, it is clear that $\tau \mapsto \tau \bar \omega_\nu(-\tau)$ is strictly increasing on $[0, \infty)$ and equals 0 at $\tau = 0$.
    Note that, for any $\tau \ge 0$,
    \begin{align*}
        \psi(\tau) = \int_0^1 t\tau \bar \omega_\nu(-t\tau) \D t = \frac1\tau \int_0^\tau t \bar \omega_\nu(-t) \D t.
    \end{align*}
    We get
    \begin{align*}
        \psi'(\tau) = \frac1{\tau^2} \left[ \tau^2 \bar \omega_\nu(-\tau) - \int_0^\tau t \bar \omega_\nu(-t) \D t \right].
    \end{align*}
    By the monotonicity of $\tau \mapsto \tau \bar \omega_\nu(-\tau)$, it follows that $\psi'(\tau) > 0$.
\end{proof}

\begin{corollary}\label{cor:K_nu}
    Let $\tau \ge 0$.
    For any $\nu \ge 2$, there exists $K_\nu \in (0, 1/2]$ such that
    \begin{align*}
        \bbar{\omega}_\nu(-\tau) \tau \le K_\nu \Rightarrow \tau < 1 + \ind\{\nu = 2\} \mbox{ and } \bbar{\omega}_\nu(-\tau) \ge 1/4.
    \end{align*}
    In particular, $K_\nu = 1/2$ if $\nu = 2$ and $K_\nu = 1/4$ if $\nu = 3$.
\end{corollary}
\begin{proof}
    The existence of $K_\nu$ follows directly from the strict monotonicity of $\varphi$ and $\psi$ shown in \Cref{lem:monotonicity}.
    For $\nu = 2$,
    \begin{align*}
        \bbar{\omega}_\nu(-\tau) \tau = \frac{e^{-\tau} + \tau - 1}{\tau} \le 1/2 \Rightarrow \tau < 2.
    \end{align*}
    As a result, we have $\bbar{\omega}_\nu(-\tau) \ge 1/4$.
    The case for $\nu = 3$ can be proved similarly.
\end{proof}

The next result shows that the local distance between the minimizer of $f$ and $x$ only depends on the geometry at $x$.
It can be used to localize the empirical risk minimizer as in \Cref{prop:localization}.
\begin{proposition}
\label{prop:self_concordance_local}
    Whenever $R_\nu \norm{\nabla f(x)}_{\nabla^2 f(x)^{-1}} \le K_\nu$, the function $f$ has a unique minimizer $\bar x$ and
    \begin{align*}
        \norm{\bar x - x}_x \le 4 \norm{\nabla f(x)}_{\nabla^2 f(x)^{-1}}.
    \end{align*}
\end{proposition}
\begin{proof}
    Consider the level set
    \begin{align*}
        \calL_f(f(x)) := \{y \in \calX: f(y) \le f(x)\} \neq \emptyset.
    \end{align*}
    Take an arbitrary $y \in \calL_f(f(x))$.
    According to \Cref{prop:function_value}, we have
    \begin{align*}
        0 \ge f(y) - f(x) \ge \ip{\nabla f(x), y - x} + \bbar{\omega}_\nu(-d_\nu(x, y)) \norm{y-x}_x^2.
    \end{align*}
    By the Cauchy-Schwarz inequality and \Cref{lem:bound_d_nu,lem:monotonicity}, we get
    \begin{align*}
        \bbar{\omega}_\nu(-R_\nu \norm{y - x}_x) \norm{y - x}_x^2 \le \norm{\nabla f(x)}_{H^{-1}(x)} \norm{y - x}_x
    \end{align*}
    This implies
    \begin{align*}
        \bbar{\omega}_\nu(-R_\nu \norm{y - x}_x) R_\nu \norm{y - x}_x \le R_\nu \norm{\nabla f(x)}_{H^{-1}(x)} \le K_\nu.
    \end{align*}
    Due to \Cref{cor:K_nu}, it holds that $R_\nu \norm{y - x}_x < 1 + \ind\{\nu = 2\}$ and $\bbar{\omega}_\nu(-R_\nu \norm{y - x}_x) \ge 1/4$.
    It follows that $d_\nu(x, y) < 1 + \ind\{\nu = 2\}$ and
    \begin{align*}
        \norm{y - x}_x \le 4 \norm{\nabla f(x)}_{\nabla^2 f(x)^{-1}}.
    \end{align*}
    Hence, the level set $\calL_f(f(x))$ is compact so that $f$ has a minimizer $\bar x$.
    Moreover, by \Cref{prop:hessian} and $\nabla^2 f(x) \succ 0$, we obtain $\nabla^2 f(y) \succ 0$ for all $y \in \calL_f(f(x))$.
    This yields that $\bar x$ is the unique minimizer of $f$ and it satisfies
    \begin{align*}
        \norm{\bar x - x}_x \le 4 \norm{\nabla f(x)}_{\nabla^2 f(x)^{-1}}.
    \end{align*}
\end{proof}

\begin{remark}
    A similar result also appears in \cite[Prop.~B.4]{ostrovskii2021finite}.
    We extend their result from $\nu \in \{2, 3\}$ to $\nu \ge 2$.
\end{remark}

\subsection{Concentration of random vectors and matrices}
\label{sub:appendix:concentration}

We start with the precise definition of sub-Gaussian random vectors \cite[Chapter 3.4]{vershynin2018high}.

\begin{definition}[Sub-Gaussian vector]\label{def:subg_vec}
    Let $S \in \reals^d$ be a random vector.
    We say $S$ is sub-Gaussian if $\ip{S, s}$ is sub-Gaussian for every $s \in \reals^d$.
    Moreover, we define the sub-Gaussian norm of $S$ as
    \begin{align*}
        \norm{S}_{\psi_2} := \sup_{\norm{s}_2 = 1} \norm{\ip{S, s}}_{\psi_2}.
    \end{align*}
    Note that $\norm{\cdot}_{\psi_2}$ is a norm and satisfies, e.g., the triangle inequality.
\end{definition}

\begin{remark}\label{rmk:centering}
    When $S$ is not mean-zero, we have
    \begin{align*}
        \norm{S - \Expect[S]}_{\psi_2}
        = \sup_{\norm{s}_2 = 1} \norm{\ip{S - \Expect[S], s}}_{\psi_2}
        = \sup_{\norm{s}_2 = 1} \norm{s^\top S - \Expect[s^\top S]}_{\psi_2}.
    \end{align*}
    According to \citet[Lemma 2.6.8]{vershynin2018high}, we obtain
    \begin{align*}
        \norm{S - \Expect[S]}_{\psi_2} \le C \sup_{\norm{s}_2 = 1} \norm{s^\top S}_{\psi_2} = C \norm{S}_{\psi_2},
    \end{align*}
    where $C$ is an absolute constant.
\end{remark}

It follows from \citet[Eq.~(2.17)]{vershynin2018high} that a bounded random vector is sub-Gaussian.
\begin{lemma}\label{lem:bounded_subG}
    Let $S$ be a random vector such that $\norm{S}_2 \le M$ for some constant $M > 0$.
    Then $X$ is sub-Gaussian with $\norm{X}_{\psi_2} \le M/\sqrt{\log{2}}$.
\end{lemma}

As a direct consequence of \citet[Prop.~2.6.1]{vershynin2018high}, the sum of i.i.d.~sub-Gaussian random vectors is also sub-Gaussian.
\begin{lemma}\label{lem:sum_subg}
Let $S_1, \dots, S_n$ be i.i.d.~random vectors, then we have $\norm{\sum_{i=1}^n S_i}_{\psi_2}^2 \lesssim \sum_{i=1}^n \norm{S_i}_{\psi_2}^2$.
\end{lemma}

We call a random vector $S \in \reals^d$ isotropic if $\Expect[S] = 0$ and $\Expect[SS^\top] = I_d$.
The following theorem is a tail bound for quadratic forms of isotropic sub-Gaussian random vectors.
\begin{theorem}[\citet{ostrovskii2021finite}, Theorem A.1]\label{thm:isotropic_tail}
Let $S \in \reals^d$ be an isotropic random vector with $\norm{S}_{\psi_2} \le K$, and let $J \in \reals^{d \times d}$ be positive semi-definite.
Then,
\begin{align*}
    \Prob(\norm{S}_{J}^2 - \text{Tr}(J) \ge t) \le \exp\left(-c\min\left\{ \frac{t^2}{K^2 \norm{J}_2^2}, \frac{t}{K\norm{J}_\infty} \right\} \right).
\end{align*}
In other words, with probability at least $1 - \delta$, it holds that
\begin{align}
  \norm{S}_{J}^2 - \text{Tr}(J) \lesssim K^2\left( \norm{J}_2 \sqrt{\log{(e/\delta)}} + \norm{J}_{\infty} \log{(1/\delta)} \right).
\end{align}
\end{theorem}

We then give the definition of the matrix Bernstein condition \cite[Chapter 6.4]{wainwright2019high}.

\begin{definition}[Matrix Bernstein condition]\label{def:matrix_bernstein}
    Let $H \in \reals^{d \times d}$ be a zero-mean symmetric random matrix.
    We say $H$ satisfies a Bernstein condition with parameter $b > 0$ if, for all $j \ge 3$,
    \begin{align*}
        \Expect[H^j] \preceq \frac12 j! b^{j-2} \Var(H).
    \end{align*}
\end{definition}

The next lemma, which follows from \citet[Eq.~(6.30)]{wainwright2019high}, shows that a matrix with bounded spectral norm satisfies the matrix Bernstein condition.
\begin{lemma}\label{lem:bounded_bernstein}
    Let $H$ be a zero-mean random matrix such that $\norm{H}_2 \le M$ for some constant $M > 0$.
    Then $H$ satisfies the matrix Bernstein condition with $b = M$ and $\sigma_H^2 = \norm{\Var(H)}_2$.
    Moreover, $\sigma_H^2 \le 2M^2$.
\end{lemma}

The next theorem is the Bernstein bound for random matrices.

\begin{theorem}[\citet{wainwright2019high}, Theorem 6.17]\label{thm:bernstein_matrix}
Let $\{H_i\}_{i=1}^n$ be a sequence of zero-mean independent symmetric random matrices that satisfies the Bernstein condition with parameter $b > 0$.
Then, for all $\delta > 0$, it holds that
\begin{align}
  \Prob\left( \anorm{\frac1n \sum_{i=1}^n H_i}_2 \ge \delta \right) \le 2 \Rank\left(\sum_{i=1}^n \Var(H_i)\right) \exp\left\{ -\frac{n\delta^2}{2(\sigma^2 + b\delta)} \right\},
\end{align}
where $\sigma^2 := \frac1n \anorm{\sum_{i=1}^n \Var(H_i)}_2$.
\end{theorem}
