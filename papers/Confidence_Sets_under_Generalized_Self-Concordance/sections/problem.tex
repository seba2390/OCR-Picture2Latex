We briefly recall the framework of statistical inference via empirical risk minimization.
Let $(\bbZ, \calZ)$ be a measurable space.
Let $Z \in \bbZ$ be a random element following some unknown distribution $\Prob$.
Consider a parametric family of distributions $\calP_\Theta := \{P_\theta: \theta \in \Theta \subset \reals^d\}$ which may or may not contain $\Prob$.
We are interested in finding the parameter $\theta_\star$ so that the model $P_{\theta_\star}$ best approximates the underlying distribution $\Prob$.
For this purpose, we choose a \emph{loss function} $\score$ and minimize the \emph{population risk} $\risk(\theta) := \Expect_{Z \sim \Prob}[\score(\theta; Z)]$.
Throughout this paper, we assume that
\begin{align*}
     \theta_\star = \argmin_{\theta \in \Theta} L(\theta)
\end{align*}
uniquely exists and satisfies $\theta_\star \in \text{int}(\Theta)$, $\nabla_\theta L(\theta_\star) = 0$, and $\nabla_\theta^2 L(\theta_\star) \succ 0$.

\myparagraph{Consistent loss function}
We focus on loss functions that are consistent in the following sense.

\begin{customasmp}{0}\label{asmp:proper_loss}
    When the model is \emph{well-specified}, i.e., there exists $\theta_0 \in \Theta$ such that $\Prob = P_{\theta_0}$, it holds that $\theta_0 = \theta_\star$.
    We say such a loss function is \emph{consistent}.
\end{customasmp}

In the statistics literature, such loss functions are known as proper scoring rules \citep{dawid2016scoring}.
We give below two popular choices of consistent loss functions.

\begin{example}[Maximum likelihood estimation]
    A widely used loss function in statistical machine learning is the negative log-likelihood $\score(\theta; z) := -\log{p_\theta(z)}$ where $p_\theta$ is the probability mass/density function for the discrete/continuous case.
    When $\Prob = P_{\theta_0}$ for some $\theta_0 \in \Theta$,
    we have $L(\theta) = \Expect[-\log{p_\theta(Z)}] = \kl(p_{\theta_0} \Vert p_\theta) - \Expect[\log{p_{\theta_0}(Z)}]$ where $\kl$ is the Kullback-Leibler divergence.
    As a result, $\theta_0 \in \argmin_{\theta \in \Theta} \kl(p_{\theta_0} \Vert p_\theta) = \argmin_{\theta \in \Theta} L(\theta)$.
    Moreover, if there is no $\theta$ such that $p_\theta \txtover{a.s.}{=} p_{\theta_0}$, then $\theta_0$ is the unique minimizer of $L$.
    We give in \Cref{tab:glms} a few examples from the class of generalized linear models (GLMs) proposed by \citet{nelder1972generalized}.
\end{example}

\begin{example}[Score matching estimation]
    Another important example appears in \emph{score matching} \citep{hyvarinen2005estimation}.
    Let $\bbZ = \reals^\tau$.
    Assume that $\Prob$ and $P_\theta$ have densities $p$ and $p_\theta$ w.r.t the Lebesgue measure, respectively.
    Let $p_\theta(z) = q_\theta(z) / \Lambda(\theta)$ where $\Lambda(\theta)$ is an unknown normalizing constant. We can choose the loss
    \begin{align*}
        \score(\theta; z) := \Delta_z \log{q_\theta(z)} + \frac12 \norm{\nabla_z \log{q_\theta(z)}}^2 + \text{const}.
    \end{align*}
    Here $\Delta_z := \sum_{k=1}^p \partial^2/\partial z_k^2$ is the Laplace operator.
    Since \cite[Thm.~1]{hyvarinen2005estimation}
    \begin{align*}
        L(\theta) = \frac12 \Expect\left[ \norm{\nabla_z q_\theta(z) - \nabla_z p(z)}^2 \right],
    \end{align*}
    we have, when $p = p_{\theta_0}$, that $\theta_0 \in \argmin_{\theta \in \Theta} L(\theta)$.
    In fact, when $q_\theta > 0$ and there is no $\theta$ such that $p_\theta \txtover{a.s.}{=} p_{\theta_0}$, the true parameter $\theta_0$ is the unique minimizer of $L$ \cite[Thm.~2]{hyvarinen2005estimation}.
\end{example}

\myparagraph{Empirical risk minimization}
Assume now that we have an i.i.d.~sample $\{Z_i\}_{i=1}^n$ from $\Prob$.
To learn the parameter $\theta_\star$ from the data, we minimize the empirical risk to obtain the \emph{empirical risk minimizer}
\begin{align*}
    \theta_n \in \argmin_{\theta \in \Theta} \left[ L_n(\theta) := \frac1n \sum_{i=1}^n \score(\theta; Z_i) \right].
\end{align*}
This applies to both maximum likelihood estimation and score matching estimation. 
In \Cref{sec:main_results}, we will prove that, with high probability, the estimator $\theta_n$ exists and is unique under a generalized self-concordance assumption.

\begin{figure}
    \centering
    \includegraphics[width=0.45\textwidth]{graphs/logistic-dikin} %0.4
    \caption{Dikin ellipsoid and Euclidean ball.}
    \label{fig:logistic_dikin}
\end{figure}

\myparagraph{Confidence set}
In statistical inference, it is of great interest to quantify the uncertainty in the estimator $\theta_n$.
In classical asymptotic theory, this is achieved by constructing an asymptotic confidence set.
We review here two commonly used ones, assuming the model is well-specified.
We start with the \emph{Wald confidence set}.
It holds that $n(\theta_n - \theta_\star)^\top H_n(\theta_n) (\theta_n - \theta_\star) \rightarrow_d \chi_d^2$, where $H_n(\theta) := \nabla^2 L_n(\theta)$.
Hence, one may consider a confidence set $\{\theta: n(\theta_n - \theta)^\top H_n(\theta_n) (\theta_n - \theta) \le q_{\chi_d^2}(\delta) \}$ where $q_{\chi_d^2}(\delta)$ is the upper $\delta$-quantile of $\chi_d^2$.
The other is the \emph{likelihood-ratio (LR) confidence set} constructed from the limit $2n [L_n(\theta_\star) - L_n(\theta_n)] \rightarrow_d \chi_d^2$, which is known as the Wilks' theorem \citep{wilks1938large}.
These confidence sets enjoy two merits: 1) their shapes are an ellipsoid (known as the \emph{Dikin ellipsoid}) which is adapted to the optimization landscape induced by the population risk; 2) they are asymptotically valid, i.e., their coverages are exactly $1 - \delta$ as $n \rightarrow \infty$.
However, due to their asymptotic nature, it is unclear how large $n$ should be in order for it to be valid.

Non-asymptotic theory usually focuses on developing finite-sample bounds for the \emph{excess risk}, i.e., $\Prob(L(\theta_n) - L(\theta_\star) \le C_n(\delta)) \ge 1 - \delta$.
To obtain a confidence set, one may assume that the population risk is twice continuously differentiable and $\lambda$-strongly convex.
Consequently, we have $\lambda \norm{\theta_n - \theta_\star}_2^2 / 2 \le L(\theta_n) - L(\theta_\star)$ and thus we can consider the confidence set $\calC_{\text{finite}, n}(\delta) := \{\theta: \norm{\theta_n - \theta}_2^2 \le 2C_n(\delta)/\lambda\}$.
Since it originates from a finite-sample bound, it is valid for fixed $n$, i.e., $\Prob(\theta_\star \in \calC_{\text{finite}, n}(\delta)) \ge 1 - \delta$ for all $n$; however, it is usually conservative, meaning that the coverage is strictly larger than $1 - \delta$.
Another drawback is that its shape is a Euclidean ball which remains the same no matter which loss function is chosen.
We illustrate this phenomenon in \Cref{fig:logistic_dikin}.
Note that a similar observation has also been made in the bandit literature \citep{faury2020improved}.

We are interested in developing finite-sample confidence sets.
However, instead of using excess risk bounds and strong convexity, we construct in \Cref{sec:main_results} the Wald and LR confidence sets in a non-asymptotic fashion, under a generalized self-concordance condition.
These confidence sets have the same shape as their asymptotic counterparts while maintaining validity for fixed $n$.
These new results are achieved by characterizing the critical sample size enough to enter the asymptotic regime.
