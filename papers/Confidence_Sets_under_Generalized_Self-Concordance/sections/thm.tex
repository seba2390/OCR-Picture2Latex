\begin{figure}
    \centering
    \includegraphics[width=0.6\textwidth]{graphs/convex-concordance} %0.45
    \caption{Strong convexity v.s.~self-concordance. Black curve: population risk; colored dot: reference point; colored dashed curve: quadratic approximation at the corresponding reference point.}
    \label{fig:convex_concordance}
\end{figure}

\subsection{Preliminaries}
\label{sub:preliminary}

\myparagraph{Notation}
We denote by $\grad(\theta; z) := \nabla_\theta \score(\theta; z)$ the gradient of the loss at $z$ and $H(\theta; z) := \nabla_\theta^2 \score(\theta; z)$ the Hessian at $z$.
Their population versions are $\grad(\theta) := \Expect[\grad(\theta; Z)]$ and $H(\theta) := \Expect[H(\theta; Z)]$, respectively.
We assume standard regularity assumptions so that $\grad(\theta) = \nabla_\theta L(\theta)$ and $H(\theta) = \nabla_\theta^2 L(\theta)$.
We write $H_\star := H(\theta_\star)$.
Note that the two optimality conditions then read $\grad(\theta_\star) = 0$ and $H_\star \succ 0$.
It follows that $\lambda_\star := \lambda_{\min}(H_\star) > 0$ and $\lambda^\star := \lambda_{\max}(H_\star) > 0$.
Furthermore, we let $G(\theta; z) := S(\theta; z) S(\theta; z)^\top$ and $G(\theta) := \Expect[\grad(\theta; Z)\grad(\theta; Z)^\top]$ be the autocorrelation matrices of the gradient.
We write $G_\star := G(\theta_\star)$.
We define their empirical quantities as $L_n(\theta) := n^{-1} \sum_{i=1}^n \score(\theta; Z_i)$, $\grad_n(\theta) := n^{-1} \sum_{i=1}^n \grad(\theta; Z_i)$, $H_n(\theta) := n^{-1} \sum_{i=1}^n H(\theta; Z_i)$, and $G_n(\theta) := n^{-1} \sum_{i=1}^n G(\theta; Z_i)$.
The first step of our analysis is to localize the estimator to a \emph{Dikin ellipsoid} at $\theta_\star$ of radius $r$, i.e.,
\begin{align*}
    \Theta_r(\theta_\star) := \left\{\theta \in \Theta: \norm{\theta - \theta_\star}_{H_\star} < r \right\},
\end{align*}
where, given a positive semi-definite matrix $J$, we let $\norm{x}_J := \norm{J^{1/2} x}_2 = \sqrt{x^\top J x}$.

\myparagraph{Effective dimension}
A quantity that plays a central role in our analysis is the \emph{effective dimension}.
\begin{definition}
\label{def:effective_dim}
    We define the effective dimension to be
    \begin{align}
        d_\star := \Tr( H_\star^{-1/2} G_\star H_\star^{-1/2} ).
    \end{align}
\end{definition}
The effective dimension appears recently in non-asymptotic analyses of (penalized) M-estimation; see, e.g., \citep{spokoiny2017penalized,ostrovskii2021finite}.
It better characterizes the complexity of the parameter space $\Theta$ than the parameter dimension $d$.
When the model is well-specified, it can be shown that $H_\star = G_\star$ and thus $d_\star = d$.
When the model is misspecified, it can be much smaller than $d$ depending on the spectra of $H_\star$ and $G_\star$.
Moreover, it is closely connected to classical asymptotic theory of M-estimation under model misspecification---it is the trace of the limiting covariance matrix of $\sqrt{n}H_n(\theta_n)^{1/2}(\theta_n - \theta_\star)$;
see \Cref{sub:discussion} for a thorough discussion.

\myparagraph{Generalized self-concordance}
We will use the notion of \emph{self-concordance} from convex optimization in our analysis.
Self-concordance originated from the analysis of the interior-point and Newton-type convex optimization methods \citep{yurii1994interior}.
It was later modified by \citet{bach2010self}, which we call the \emph{pseudo self-concordance}, to derive finite-sample bounds for the generalization properties of the logistic regression.
Recently, \citet{sun2019generalized} proposed the \emph{generalized self-concordance} which unifies these two notions.
For a function $f: \reals^d \to \reals$, we define $D_x f(x)[u] := \frac{\D}{\D t} f(x + tu) |_{t = 0}$, $D_x^2 f(x)[u, v] := D_x (D_x f(x)[u])[v]$ for $x, u, v \in \reals^d$, and $D_x^3 f(x)[u, v, w]$ similarly.
\begin{definition}[Generalized self-concordance]
\label{def:general_self_concordance}
    Let $\calX \subset \reals^d$ be open and $f: \calX \rightarrow \reals$ be a closed convex function.
    For $R > 0$ and $\nu > 0$, we say $f$ is $(R, \nu)$-generalized self-concordant on $\calX$ if
    \begin{align*}
        \abs{D_x^3 f(x) [u, u, v]} \le R \norm{u}_{\nabla^2 f(x)}^2 \norm{v}_{\nabla^2 f(x)}^{\nu-2} \norm{v}_2^{3-\nu}
    \end{align*}
    with the convention $0/0 = 0$ for the case $\nu < 2$ and $\nu > 3$.
    Recall that $\norm{u}_{\nabla^2 f(x)}^2 := u^\top \nabla^2 f(x) u$.
\end{definition}

\myparagraph{Remark}
When $\nu = 2$ and $\nu = 3$, this definition recovers the pseudo self-concordance and the standard self-concordance, respectively.

In contrast to strong convexity which imposes a gross lower bound on the Hessian, generalized self-concordance specifies the rate at which the Hessian can vary, leading to a finer control on the Hessian.
Concretely, it allows us to bound the Hessian in a neighborhood of $\theta_\star$ with the Hessian at $\theta_\star$, which is key to controlling $H_n(\theta_n)$.
We illustrate the difference between them in \Cref{fig:convex_concordance}.
As we will see in \Cref{sub:main_results}, thanks to the generalized self-concordance, we are able to remove the direct dependency on $\lambda_\star$ in our confidence set.
To the best of our knowledge, this is the first work extending classical results for M-estimation to generalized self-concordant losses.

\myparagraph{Concentration of Hessian}
One key result towards deriving our bounds is the concentration of empirical Hessian, i.e., $(1 - c_n(\delta))H(\theta) \preceq H_n(\theta) \preceq (1 + c_n(\delta)) H(\theta)$ with probability at least $1 - \delta$.
When the loss function is of the form $\ell(\theta; z) := \ell(y, \theta^\top x)$ (e.g., GLMs), the empirical Hessian reads $H_n(\theta) = n^{-1} \sum_{i=1}^n \ell''(Y_i, \theta^\top X_i) X_i X_i^\top$ where $\ell''(y, \bar y) := \D^2 \ell(y, \bar y) / \D \bar y^2$, which is of the form of a sample covariance.
Assuming $X$ to be sub-Gaussian, \citet{ostrovskii2021finite} obtained a concentration bound for $H_n(\theta_\star)$ with $c_n(\delta) = O(\sqrt{(d + \log{(1/\delta)})/n})$ via the concentration bound for sample covariance \citep[Thm.~5.39]{vershynin2010introduction}.
For general loss functions, such a special structure cannot be exploited.
We overcame this challenge by the matrix Bernstein inequality \citep[Thm.~6.17]{wainwright2019high}, obtaining a sharper concentration bound with $c_n(\delta) := O(\sqrt{\log{(d/\delta)}/n})$.
Note that the matrix Bernstein inequality has been used to control the empirical Hessian of kernel ridge regression with random features \citep[Prop.~6]{rudi2017generalization} and later extended to regularized empirical risk minimization \citep[Lem.~30]{marteau2019beyond}.
However, their results require the regularization parameter to be strictly positive (otherwise the bounds are vacuous) and the sample Hessian to be bounded.
On the contrary, our technique allows for zero regularization and unbounded Hessian as long as the Hessian satisfies a matrix Bernstein condition.
Moreover, combining generalized self-concordance with matrix Bernstein, we are able to show the concentration of $H_n(\theta_n)$ around $H_\star$ for general losses, which is itself a novel result.

\subsection{Assumptions}
\label{sub:assumption}

Our key assumption is the generalized self-concordance of the loss function.
\begin{assumption}[Generalized self-concordance]
\label{asmp:self_concordance}
    For any $z \in \calZ$, the scoring rule $\score(\cdot; z)$ is $(R, \nu)$-generalized self-concordant for some $R > 0$ and $\nu \ge 2$.
    Moreover, $\risk(\cdot)$ is also $(R, \nu)$-generalized self-concordant.
\end{assumption}

\myparagraph{Remark}
If $\score(\cdot; z)$ is generalized self-concordant with $\nu = 2$, so is $\risk(\cdot)$.

Many loss functions in statistical machine learning satisfy this assumption.
We give in \Cref{sub:examples} examples from generalized linear models and score matching.


In order to control the empirical gradient $\grad_n(\theta)$, we assume that the normalized gradient at $\theta_\star$ is sub-Gaussian.
\begin{assumption}[Sub-Gaussian gradient]
\label{asmp:sub_gaussian}
    There exists a constant $K_1 > 0$ such that the normalized gradient at $\theta_\star$ is sub-Gaussian with parameter $K_1$, i.e., $\lVert G_\star^{-1/2} \grad(\theta_\star; Z) \rVert_{\psi_2} \le K_1$.
    Here $\norm{\cdot}_{\psi_2}$ is the sub-Gaussian norm whose definition is recalled in \Cref{sec:tools}.
\end{assumption}

When the loss function is of the form $\ell(\theta; z) = \ell(y, \theta^\top x)$, we have $S(\theta; Z) = \ell'(Y, \theta^\top X) X$.
As a result, \Cref{asmp:sub_gaussian} holds true if (i) $\ell'(Y, \theta_\star^\top X)$ is sub-Gaussian and $X$ is bounded or (ii) $\ell'(Y, \theta_\star^\top X)$ is bounded and $X$ is sub-Gaussian.
For least squares with $\ell(y, \theta^\top x) = \frac12 (y - \theta^\top x)^2$, the derivative $\ell'(Y, \theta_\star^\top X) = \theta_\star^\top X - Y$ is the negative residual.
\Cref{asmp:sub_gaussian} is guaranteed if the residual is sub-Gaussian and $X$ is bounded.
For logistic regression with $\ell(y, \theta^\top x) = -\log{\sigma(y\cdot \theta^\top x)}$ where $\sigma(u) = (1 + e^{-u})^{-1}$, the derivative $\ell'(Y, \theta_\star^\top X) = [\sigma(Y \cdot \theta_\star^\top X) - 1]Y \in [-1, 1]$ is bounded.
Thus, \Cref{asmp:sub_gaussian} is guaranteed if $X$ is sub-Gaussian.

In order to control the empirical Hessian, we assume that the Hessian of the loss function satisfies the matrix Bernstein condition in a neighborhood of $\theta_\star$.

\begin{assumption}[Matrix Bernstein of Hessian]
\label{asmp:bernstein}
    There exist constants $K_2, r > 0$ such that, for any $\theta \in \Theta_{r}(\theta_\star)$, the standardized Hessian
    \begin{align*}
        H(\theta)^{-1/2} H(\theta; Z) H(\theta)^{-1/2} - I_d
    \end{align*}
    satisfies a Bernstein condition (defined in \Cref{sec:tools}) with parameter $K_2$. Moreover,
    \begin{align*}
        \sigma_H^2 := \sup_{\theta \in \Theta_{r}(\theta_\star)} \norm{\Var\left( H(\theta)^{-\frac12}H(\theta; Z)H(\theta)^{-\frac12} \right)}_2 < \infty,
    \end{align*}
    where $\norm{\cdot}_2$ is the spectral norm and $\Var(J) := \Expect[JJ^\top] - \Expect[J] \Expect[J]^\top$.
    By convention, we let $\Theta_0(\theta_\star) = \{\theta_\star\}$.
\end{assumption}

\subsection{Main Results}
\label{sub:main_results}

We now give simplified versions of our main theorems.
We use $C_\nu$ to represent a constant depending only on $\nu$ that may change from line to line; and $C_{K_1, \nu}$ similarly.
We use $\lesssim$ and $\gtrsim$ to hide constants depending only on $K_1, K_2, \sigma_H, \nu$.
The precise versions can be found in \Cref{sec:proofs}.
Recall that $\lambda_\star := \lambda_{\min}(H_\star)$ and $\lambda^\star := \lambda_{\max}(H_\star)$.
\begin{theorem}\label{thm:risk_bound_generalized}
    Let $\nu \in [2, 3)$.
    Under \Cref{asmp:self_concordance,asmp:sub_gaussian,asmp:bernstein} with $r = 0$, it holds that,
    whenever
    \begin{align*}
        n \gtrsim \log{(2d/\delta)} + \lambda_\star^{-1} \left[ R^2 d_\star \log{(e/\delta)} \right]^{1/(3-\nu)},
    \end{align*}
    the empirical risk minimizer $\theta_n$ uniquely exists and satisfies, with probability at least $1 - \delta$,
    \begin{align}\label{eq:conf_bound}
        \norm{\theta_n - \theta_\star}^2_{H_\star} \lesssim \log{(e/\delta)} \frac{d_\star}{n}.
    \end{align}
\end{theorem}

With a local matrix Bernstein condition, we can replace $H_\star$ by $H_n(\theta_n)$ in \eqref{eq:conf_bound} and obtain a finite-sample version of the Wald confidence set.
\begin{theorem}\label{thm:conf_set}
    Let $\nu \in [2, 3)$.
    Suppose the same assumptions in \Cref{thm:risk_bound_generalized} hold true.
    Furthermore, suppose that \Cref{asmp:bernstein} holds with $r = C_\nu \lambda_\star^{(3-\nu)/2} / R$.
    Let $\calC_{\text{Wald}, n}(\delta)$ be
    \begin{align}\label{eq:my_conf_set}
        \left\{\theta \in \Theta: \norm{\theta - \theta_n}_{H_n(\theta_n)}^2 \le C_{K_1,\nu} \frac{d_\star}{n} \log{\frac{e}{\delta}} \right\}.
    \end{align}
    Then we have $\Prob(\theta_\star \in \calC_{\text{Wald}, n}(\delta)) \ge 1 - \delta$ whenever
    \begin{align}\label{eq:n_large_enough}
        n \gtrsim \log{\frac{2d}{\delta}} + d\log{n} + \lambda_\star^{-1}\left[ R^2 d_\star \log{\frac{e}{\delta}} \right]^{\frac1{3-\nu}}.
    \end{align}
\end{theorem}

\myparagraph{Remark}
In the precise versions of \Cref{thm:risk_bound_generalized,thm:conf_set}, the term $d_\star \log{(e/\delta)}$ in the bounds \eqref{eq:conf_bound} and \eqref{eq:my_conf_set} should be replaced by $d_\star + \log{(e/\delta)} \lVert G_\star^{1/2} H_\star^{-1} G_\star^{1/2} \rVert_2$, which almost match the misspecified Cram\'er-Rao lower bound \citep[e.g.,][Thm.~1]{fortunati2016misspecified} up to a constant factor.

\Cref{thm:conf_set} suggests that the tail probability of $\norm{\theta_n - \theta_\star}_{H_n(\theta_n)}^2$ is governed by a $\chi^2$ distribution with $d_\star$ degrees of freedom, which coincides with the asymptotic result.
In fact, according to \citet{huber1967under}, under suitable regularity assumptions, it holds that $\sqrt{n} H_n(\theta_n)^{1/2}(\theta_n - \theta_\star) \rightarrow_d W \sim \mathcal{N}(0, H_\star^{-1/2} G_\star H_\star^{-1/2})$ which implies that
\begin{align*}
    n(\theta_n - \theta_\star)^\top H_n(\theta_n) (\theta_n - \theta_\star) \rightarrow_d W^\top W.
\end{align*}
This induces an asymptotic confidence set with a similar form of \eqref{eq:my_conf_set} and radius $O(\Expect[W^\top W] / n) = O(d_\star / n)$.
Our result characterizes the \emph{critical sample size} enough to enter the asymptotic regime.

From \Cref{thm:conf_set} we can also derive a finite-sample version of the LR confidence set.
\begin{corollary}\label{cor:lr_conf_set}
    Let $\nu \in [2, 3)$.
    Suppose the same assumptions in \Cref{thm:conf_set} hold true.
    Let $\calC_{\text{LR}, n}(\delta)$ be
    \begin{align}\label{eq:lr_conf_set}
        \left\{\theta \in \Theta: 2[L_n(\theta) - L_n(\theta_n)] \le C_{K_1,\nu} \frac{d_\star}{n} \log{\frac{e}{\delta}} \right\}.
    \end{align}
    Then we have $\Prob(\theta_\star \in \calC_{\text{LR}, n}(\delta)) \ge 1 - \delta$ whenever
    \begin{align*}
        n \gtrsim \log{\frac{2d}{\delta}} + d\log{n} + \lambda_\star^{-1}\left[ R^2 d_\star \log{\frac{e}{\delta}} \right]^{\frac1{3-\nu}}.
    \end{align*}
\end{corollary}

We give the proof sketches of \Cref{thm:risk_bound_generalized}, \Cref{thm:conf_set}, and \Cref{cor:lr_conf_set} here and defer their full proofs to \Cref{sec:proofs}.
We discuss in~\Cref{sub:discussion} 
how our proof techniques and theoretical results complement and improve on previous works.

We start by showing the existence and uniqueness of $\theta_n$.
The next result shows that $\theta_n$ exists and is unique whenever the quadratic form $\grad_n(\theta_\star)^\top H_n^{-1}(\theta_\star) \grad_n(\theta_\star)$ is small.
Note that this quantity is also known as Rao's score statistic for goodness-of-fit testing.
This result also localizes $\theta_n$ to a neighborhood of the target parameter $\theta_\star$.
\begin{proposition}\label{prop:localization}
    Under \Cref{asmp:self_concordance},
    if $\norm{\grad_n(\theta_\star)}_{H_n^{-1}(\theta_\star)} \le C_{\nu} [\lambda_{\min}(H_n(\theta_\star))]^{(3-\nu)/2} / (R n^{\nu/2-1})$,
    then the estimator $\theta_n$ uniquely exists and satisfies
    \begin{align*}
        \norm{\theta_n - \theta_\star}_{H_n(\theta_\star)} \le 4 \norm{\grad_n(\theta_\star)}_{H_n^{-1}(\theta_\star)}.
    \end{align*}
\end{proposition}

The main tool used in the proof of \Cref{prop:localization} is a strong convexity type result for generalized self-concordant functions recalled in \Cref{sec:tools}.
In order to apply \Cref{prop:localization}, we need to control $\norm{\grad_n(\theta_\star)}_{H_n^{-1}(\theta_\star)}$.
This result is summarized in the following proposition.

\begin{proposition}\label{prop:score}
    Under \Cref{asmp:sub_gaussian,asmp:bernstein} with $r = 0$, it holds that, with probability at least $1 - \delta$,
    \begin{align*}
        \norm{S_n(\theta_\star)}_{H_n^{-1}(\theta_\star)}^2 \lesssim \frac{d_\star}n \log{(e/\delta)}
    \end{align*}
    whenever $n \gtrsim \log{(2d/\delta)}$.
\end{proposition}

The proof of \Cref{prop:score} consists of two steps: (a) lower bound $H_n(\theta_\star)$ by $H_\star$ up to a constant using the Bernstein inequality and (b) upper bound $\norm{\grad_n(\theta_\star)}_{H^{-1}(\theta_\star)}$ using a concentration inequality for isotropic random vectors, where the tools are recalled in \Cref{sec:tools}.
Combining them implies that $\norm{\grad_n(\theta_\star)}_{H^{-1}(\theta_\star)}$ can be arbitrarily small and thus satisfies the requirement in \Cref{prop:localization} for sufficiently large $n$.
This not only proves the existence and uniqueness of the empirical risk minimizer $\theta_n$ but also provides an upper bound for $\norm{\theta_n - \theta_\star}_{H_n(\theta_\star)}$ through $\norm{\grad_n(\theta_\star)}_{H_n^{-1}(\theta_\star)}$.

In order to prove \Cref{thm:conf_set}, it remains to upper bound $H_n(\theta_n)$ by $H_\star$ up to a constant factor.
This can be achieved by the following result.
\begin{proposition}\label{prop:emp_hess_est}
    Under \Cref{asmp:self_concordance,asmp:bernstein} with $r = C_\nu \lambda_\star^{(\nu-3)/2} / R$, it holds that, with probability at least $1 - \delta$,
    \begin{align*}
        \frac1{2C_\nu} H_\star \preceq H_n(\theta) \preceq \frac32 C_\nu H_\star, \;\mbox{for all } \theta \in \Theta_{r}(\theta_\star),
    \end{align*}
    whenever $n \gtrsim \left\{ \log{(2d/\delta)} + d (\nu/2-1) \log{n}\right\}$.
\end{proposition}

Finally, \Cref{cor:lr_conf_set} follows from \Cref{thm:conf_set} and the Taylor expansion: there exists $\bar \theta_n \in \mbox{Conv}\{\theta_n, \theta_\star\}$ such that
\begin{align*}
    2[L_n(\theta_\star) - L_n(\theta_n)] = \norm{\theta_n - \theta_\star}_{H_n(\bar \theta_n)},
\end{align*}
where we have used $\nabla L_n(\theta_n) = 0$.

\subsection{Approximating the effective dimension}

One downside of \Cref{thm:conf_set,cor:lr_conf_set} is that $d_\star$ depends on the unknown data distribution.
Alternatively, we use the following empirical counterpart
\begin{align*}
    d_n := \Tr\left(H_n(\theta_n)^{-1/2} G_n(\theta_n) H_n(\theta_n)^{-1/2} \right).
\end{align*}
The next result implies that we do not lose much if we replace $d_\star$ by $d_n$.
This result is novel and of independent interest since one also needs to estimate $d_\star$ in order to construct asymptotic confidence sets under model misspecification.

\begin{customasmp}{2'}\label{asmp:subG_local}
    There exist constants $r, K_1 > 0$ such that, for any $\theta \in \Theta_r(\theta_\star)$, we have $\norm{G(\theta)^{-1/2} S(\theta; Z)}_{\psi_2} \le K_1$.
\end{customasmp}

\begin{assumption}\label{asmp:lip}
    There exists $r > 0$ such that $M := \Expect[M(Z)] < \infty$, where $M(z)$ is defined as
    \begin{align*}
        \sup_{\theta_1 \neq \theta_2 \in \Theta_r(\theta_\star)} \frac{\norm{G_\star^{-1/2} [G(\theta_1; z) - G(\theta_2; z)] G_\star^{-1/2}}_2}{\norm{\theta_1 - \theta_2}_{H_\star}}.
    \end{align*}
\end{assumption}

\myparagraph{Remark}
\Cref{asmp:lip} is a Lipschitz-type condition for $G(\theta; z)$. This assumption was previously used by \citep[Assumption 3]{mei2018landscape} to analyze non-convex risk landscapes. 

\begin{proposition}\label{prop:d_n}
    Let $\nu \in [2, 3)$.
    Under Asms.~\ref{asmp:self_concordance}, \ref{asmp:subG_local}, \ref{asmp:bernstein} and \ref{asmp:lip} with $r = C_\nu \lambda_\star^{(3-\nu)/2}/R$, it holds that
    \begin{align*}
        \frac1{C_\nu} d_\star \le d_n \le C_\nu d_\star,
    \end{align*}
    with probability at least $1 - \delta$,
    whenever $n$ is large enough (see \Cref{sub:appendix:consist_dn} for the precise condition).
\end{proposition}

\myparagraph{Remark}
The precise version of $\Cref{prop:d_n}$ in \Cref{sub:appendix:consist_dn} implies that $d_n$ is a consistent estimator of $d$.

With \Cref{prop:d_n} at hand, we can obtain finite-sample confidence sets involving $d_n$, which can be computed from data.
We illustrate it with the Wald confidence set.
\begin{corollary}\label{cor:wald_conf_set}
    Suppose the same assumptions in \Cref{prop:d_n} hold true.
    Let $\calC_{\text{Wald}, n}'(\delta)$ be
    \begin{align*}
        \left\{ \theta \in \Theta: \norm{\theta - \theta_\star}_{H_n(\theta_n)}^2 \le C_{K_1, \nu} \log{(e/\delta)} \frac{d_n}{n} \right\}.
    \end{align*}
    Then we have $\Prob(\theta_\star \in \calC_{\text{Wald}, n}'(\delta)) \ge 1 - \delta$ whenever $n$ satisfies the same condition as in \Cref{prop:d_n}.
\end{corollary}

\subsection{Discussion}
\label{sub:discussion}

\myparagraph{Fisher information and model misspecification}
When the model is well-specified, the autocorrelation matrix $G(\theta)$ coincides with the well-known Fisher information $\mathcal{I}(\theta) := \Expect_{Z \sim P_\theta}[S(\theta; Z)S(\theta; Z)^\top]$ at $\theta_\star$.
The Fisher information plays a central role in mathematical statistics and, in particular, M-estimation; see \citep{pennington2018spectrum,kunstner2019limitations,ash2021gone,soen2021variance} for recent developments in this line of research.
It quantifies the amount of information a random variable carries about the model parameter.
Under a well-specified model, it also coincides with the Hessian matrix $H(\theta)$ at the optimum which captures the local curvature of the population risk.
When the model is misspecified, the Fisher information deviates from the Hessian matrix.
In the asymptotic regime, this discrepancy is reflected in the limiting covariance of the weighted M-estimator which admits a sandwich form $H_\star^{-1/2} G_\star H_\star^{-1/2}$; see, e.g., \cite[Sec.~4]{huber1967under}.

\myparagraph{Effective dimension}
The counterpart of the sandwich covariance in the non-asymptotic regime is the effective dimension $d_\star$; see, e.g., \citep{spokoiny2017penalized,ostrovskii2021finite}.
Our bounds also enjoy the same merit---its dimension dependency is via the effective dimension.
When the model is well-specified, the effective dimension reduces to $d$, recovering the same rate of convergence $O(d/n)$ as in classical linear regression; see, e.g., \cite[Prop.~3.5]{bach2021learning}.
When the model is misspecified, the effective dimension provides a characterization of the problem complexity which is adapted to both the data distribution and the loss function via the matrix $H_\star^{-1/2} G_\star H_\star^{-1/2}$.
To gain a better understanding of the effective dimension $d_\star$, we summarize it in \Cref{tab:decay} in \Cref{sec:proofs} under different regimes of eigendecay, assuming that $G_\star$ and $H_\star$ share the same eigenvectors.
It is clear that, when the spectrum of $G_\star$ decays faster than the one of $H_\star$, the dimension dependency can be better than $O(d)$.
In fact, it can be as good as $O(1)$ when the spectrum of $G_\star$ and $H_\star$ decay exponentially and polynomially, respectively.

\myparagraph{Comparison to classical asymptotic theory}
Classical asymptotic theory of M-estimation is usually based on two assumptions: (a) the model is well-specified and (b) the sample size $n$ is much larger than the parameter dimension $d$.
These assumptions prevent it from being applicable to many real applications where the parametric family is only an approximation to the unknown data distribution and the data is of high dimension involving a large number of parameters.
On the contrary, our results do not require a well-specified model, and the dimension dependency is replaced by the effective dimension $d_\star$ which captures the complexity of the parameter space.
Moreover, they are of non-asymptotic nature---they hold true for any $n$ as long as it exceeds some constant factor of $d_\star$.
This allows the number of parameters to potentially grow with the same size.

\myparagraph{Comparison to recent non-asymptotic theory}
Recently, \citet{spokoiny2012parametric} achieved a breakthrough in finite-sample analysis of parametric M-estimation.
Although fully general, their results require strong global assumptions on the deviation of the empirical risk process and are built upon advanced tools from empirical process theory.
Restricting ourselves to generalized self-concordant losses, we are able to provide a more transparent analysis with neater assumptions only in a neighborhood of the optimum parameter $\theta_\star$.
Moreover, our results maintain some generality, covering several interesting examples in statistical machine learning as provided in \Cref{sub:examples}.

\citet{ostrovskii2021finite} also considered self-concordant losses for M-estimation.
However, their results are limited to generalized linear models whose loss is (pseudo) self-concordant and admits the form $\ell(\theta; Z) := \ell(Y, \theta^\top X)$.
While sharing the same rate $O(d_\star / n)$, our results are more general than theirs in two aspects.
First, the loss need not be of the form $\ell(Y, \theta^\top X)$, encompassing the score matching loss in \Cref{ex:score_matching} below.
Second, we go beyond pseudo self-concordance via the notion of generalized self-concordance.
Moreover, they focus on bounding the excess risk rather than providing confidence sets, and they do not study the estimation of $d_\star$.

Pseudo self-concordant losses have been considered for semi-parametric models \citep{liu2022orthogonal}.
However, they focus on bounding excess risk and require a localization assumption on $\theta_n$. Here we prove the localization result in \Cref{prop:localization} and we focus on confidence sets.

\myparagraph{Regularization}
Our results can also be applied to regularized empirical risk minimization by including the regularization term in the loss function.
Let $\theta_{n}^\lambda$ and $\theta_{\star}^\lambda$ be the minimizers of the \emph{regularized} empirical and population risk, respectively.
Let $d_\star^\lambda := \Tr\big((H_\star^\lambda)^{-1/2} G_\star^{\lambda} (H_\star^\lambda)^{-1/2}\big)$ where $H_\star^{\lambda}$ and $G_\star^{\lambda}$ are the regularized Hessian and the autocorrelation matrix of the regularized gradient at $\theta_\star^\lambda$, respectively.
Then our results characterize the concentration of $\theta_{n}^\lambda$ around $\theta_{\star}^\lambda$:
\begin{align*}
    \norm{\theta_n^{\lambda} - \theta_\star^\lambda}_{H_\star^\lambda}^2 \le O(d_\star^\lambda / n).
\end{align*}
This result coincides with \citet[Thm.~2.1]{spokoiny2017penalized}.
If the goal is to estimate the unregularized population risk minimizer $\theta_\star$, then we need to pay an additional error $\norm{\theta_\star^\lambda - \theta_\star}_{H_\star^\lambda}^2$ which is referred to as the modeling bias \citep[Sec.~2.5]{spokoiny2017penalized}.
One can invoke a so-called \emph{source condition} to bound the modeling bias and a \emph{capacity condition} to bound $d_\star^\lambda$.
An optimal value of $\lambda$ can be obtained by balancing between these two terms \cite[see, e.g.,][]{marteau2019beyond}.

For instance, let $Z := (X, Y)$ where $X \in \reals^d$ with $\Expect[XX^\top] = I_d$ and $Y \in \reals$.
Consider the regularized squared loss
$ \score^\lambda(\theta; z) := 1/2\, (y - \theta^\top x)^2 + 1/2\, \theta^\top U \theta$
where $U = \diag\{\mu_1, \dots, \mu_d\}$.
The regularized effective dimension is then~\citep[Sec.~2.1]{spokoiny2017penalized} of order 
$ O\big( \sum_{k=1}^d 1/(1 + \mu_k) \big)$
which can be much smaller than $d$ if $\{\mu_k\}$ is increasing.
