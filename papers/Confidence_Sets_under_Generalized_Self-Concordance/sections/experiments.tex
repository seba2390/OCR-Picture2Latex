\begin{figure}
  \centering
  \includegraphics[width=0.6\textwidth]{graphs/est-effective-dim}
  \caption{Absolute error of the empirical effective dimension. \textbf{(Left)}: least squares; \textbf{(Right)}: logistic regression.}
  \label{fig:est_effective_dim}
\end{figure}

We run simulation studies to illustrate our theoretical results.
We start by demonstrating the consistency of $d_n$ and the shape of the Wald confidence set defined in \Cref{cor:wald_conf_set}, i.e.,
\begin{align*}
  \calC_{\text{Wald}, n}'(\delta) = \left\{\theta \in \Theta: \norm{\theta - \theta_n}_{H_n(\theta_n)}^2 \le C_{K_1,\nu} \frac{d_n}{n} \log{(e/\delta)} \right\}.
\end{align*}
Note that the oracle Wald confidence set should be constructed from $\norm{\theta_n - \theta_\star}_{H_\star}$ and $d_\star$; however, \Cref{cor:wald_conf_set} suggests that we can replace $H_\star$ and $d_\star$ by $H_n(\theta_n)$ and $d_n$ without losing too much.
To empirically verify our theoretical results, we calibrate the Wald confidence set based on $\norm{\theta_n - \theta_\star}_{H_n(\theta_n)}$ with the threshold from the oracle Wald confidence set and compare its coverage with the one calibrated by the multiplier bootstrap---a popular resampling-based approach for calibration.
Finally, we compare the coverage of the Wald and LR confidence sets calibrated by the multiplier bootstrap.
In all the experiments, we generate $n$ i.i.d.~pairs by sampling $X$ and then sampling $Y \mid X$.

\subsection{Numerical Illustrations}

\paragraph{Approximation of the effective dimension.}
By \Cref{prop:d_n}, we know that $d_n$ is a consistent estimator of $d_\star$.
We verify it with simulations.
We consider two models.
For least squares, the data are generated from $X \sim \calN(0, I_d)$ and $Y | X \sim \calN(\ones^\top X, 1)$.
For logistic regression, the data are generated from $X \sim \calN(0, I_d)$ and $Y \mid X \sim p(Y \mid X) = \sigma(Y \ones^\top X)$ for $Y \in \{-1, 1\}$ where $\sigma(u) := (1 + e^{-u})^{-1}$.
We then estimate $d_\star = d$ (since the model is well-specified) by $d_n$ and quantify its estimation error by $\Expect\abs{d_n / d_\star - 1}$.
We vary $n \in [2000, 10000]$ and $d \in \{5, 10, 15, 20\}$, and give the plots in \Cref{fig:est_effective_dim}.
For a fixed $d$, the absolute error decays to zero as the sample size increases as predicted by \Cref{prop:d_n}.
For a fixed $n$, the absolute error raises as the dimension becomes larger in logistic regression, but it remains similar in least squares.

\begin{figure}
  \centering
  \includegraphics[width=0.7\textwidth]{graphs/logistic-conf-set}
  \caption{Confidence set in \Cref{cor:wald_conf_set} under a logistic regression model. \textbf{Left:} $\Sigma = (2, 0; 0, 1)$; \textbf{Middle:} $\Sigma = (2, 1; 1, 1)$; \textbf{Right:} $\Sigma = (2, -1; -1, 1)$.}
  \label{fig:logistic_conf_set}
\end{figure}

\paragraph{Shape of the Wald confidence set.}
Recall that the Wald confidence set in \Cref{thm:conf_set} is an ellipsoid whose shape is determined by the empirical Hessian $H_n(\theta_n)$ and thus can effectively handles the local curvature of the empirical risk.
We illustrate this feature on a logistic regression example.
We generate data from $X \sim \calN(0, \Sigma)$ with different $\Sigma$'s and $Y \mid X \sim p(Y \mid X) = \sigma(Y \theta_0^\top X)$ for $Y \in \{-1, 1\}$ where $\theta_0 = (-1, 2)^\top$.
We then construct the confidence set with $d_\star = d$.
As shown in \Cref{fig:logistic_conf_set}, the shape of the confidence set varies with $\Sigma$ and captures the curvature of the empirical risk at $\theta_0$.

\subsection{Calibration}
\label{sub:bootstrap}

We investigate two calibration schemes.
Inspired by the setting in \citet[Sec.~5.1]{chen2020robust},
we generate $n = 100$ i.i.d.~observations from three models with true parameter $\theta_0$ whose elements are equally spaced between $[0, 1]$---1) \emph{well-specified least squares} with $X \sim \calN(0, I_d)$ and $Y \mid X \sim \calN(\theta_0^\top X, 1)$, 2) \emph{misspecified least squares} with $X \sim \calN(0, I_d)$ and $Y \mid X \sim \theta_0^\top X + t_{3.5}$, and 3) \emph{well-specified logistic regression} with $X \sim \calN(0, I_d)$ and $Y \mid X \sim p(Y \mid X) = \sigma(Y \theta_0^\top X)$ for $Y \in \{-1, 1\}$.
For each $\delta \in \{0.95, 0.9, 0.85, 0.8, 0.75\}$, we construct a confidence set using either \emph{oracle calibration} or \emph{multiplier bootstrap}.
We repeat the whole process for $1000$ times and report the coverage of each confidence set in \Cref{tab:bootstrap}.

\begin{table*}[t]
  \caption{Coverage of the oracle and bootstrap confidence sets.}
  \label{tab:bootstrap}
  \centering
  \renewcommand{\arraystretch}{1.2}
  \begin{tabular}{llccccc}
      \addlinespace[0.4em]
      \toprule
      \multicolumn{1}{c}{\textbf{Model}} & \multicolumn{1}{c}{\textbf{Confidence set}} & \multicolumn{1}{c}{$\delta = 0.95$} & \multicolumn{1}{c}{$\delta = 0.9$} & \multicolumn{1}{c}{$\delta = 0.85$} & \multicolumn{1}{c}{$\delta = 0.8$} & \multicolumn{1}{c}{$\delta = 0.75$}  \\
      \midrule
      \multirow{3}{*}{Well-specified least squares} & Oracle & 0.957 & 0.908 & 0.868 & 0.792 & 0.770 \\
      & BootWald & 0.947 & 0.908 & 0.855 & 0.791 & 0.735 \\
      & BootLR & 0.949 & 0.906 & 0.852 & 0.792 & 0.737 \\
      \midrule
      \multirow{3}{*}{Misspecified least squares} & Oracle & 0.972 & 0.916 & 0.882 & 0.841 & 0.764 \\
      & BootWald & 0.968 & 0.924 & 0.865 & 0.779 & 0.727 \\
      & BootLR & 0.972 & 0.923 & 0.865 & 0.784 & 0.727 \\
      \midrule
      \multirow{3}{*}{Well-specified logistic regression} & Oracle & 0.961 & 0.915 & 0.868 & 0.809 & 0.776 \\
      & BootWald & 0.938 & 0.885 & 0.826 & 0.781 & 0.706 \\
      & BootLR & 0.976 & 0.948 & 0.901 & 0.866 & 0.791 \\
      \bottomrule
  \end{tabular}
\end{table*}

\paragraph{Oracle calibration.}
According to \Cref{thm:risk_bound_generalized}, if we have access to $H_\star$ and $d_\star$, we can construct a confidence set of the form $\calC_\star(\delta) := \{\theta: \norm{\theta_n - \theta}_{H_\star} \le d_\star/n + c_n(\delta)\}$.
Now \Cref{cor:wald_conf_set} suggests that $H_\star$ and $d_\star$ can be accurately estimated by $H_n(\theta_n)$ and $d_n$, respectively, leading the confidence set $\calC_n(\delta) := \{\theta: \norm{\theta_n - \theta}_{H_n(\theta_n)} \le d_n/n + c_n(\delta)\}$.
To calibrate $\calC_n(\delta)$, we use the data generating distribution to estimate $c_n(\delta)$ so that $\Prob(\theta_\star \in \calC_\star(\delta)) \approx 1 - \delta$, and then plug it into $\calC_n(\delta)$.
We call it the \emph{oracle Wald confidence set}.
As shown in \Cref{tab:bootstrap}, its coverage is very close to the prescribed confidence level in the well-specified case and it tends to be more conservative in the misspecified case.

\paragraph{Multiplier bootstrap.}
To further evaluate the oracle calibration, we compare its coverage with the one calibrated by the multiplier bootstrap \citep[e.g.,][]{chen2020robust}---a popular resampling-based calibration approach that is widely used in practice.
We construct a \emph{bootstrap Wald confidence set} (BootWald) with $B = 2000$ bootstrap samples in the following steps.
For each $b \in \{1, \dots, B\}$, we 1) generate weights $\{W_i^b\}_{i=1}^n \txtover{i.i.d.}{\sim} \calN(1, 1)$, 2) compute the bootstrap estimator
\begin{align*}
  \theta_n^b = \argmin_{\theta} \left[ L_n^b(\theta) := \frac1n \sum_{i=1}^n W_i^b \ell(\theta; Z_i) \right],
\end{align*}
3) compute the bootstrap Wald statistic $T_{\text{Wald}}^b := \norm{\theta_n^b - \theta_n}_{H_n^b(\theta_n^b)}^2$ where $H_n^b(\theta) := \nabla_\theta^2 L_n^b(\theta)$.
Finally, we compare $\norm{\theta_n - \theta_0}_{H_n(\theta_n)}^2$ with the upper $\delta$ quantile of $\{T_{\text{Wald}}^b\}_{b=1}^B$ to decide if the Wald confidence set covers the true parameter.
It is clear that the bootstrap Wald confidence set performs similarly as the oracle Wald confidence set in least squares, but it is more liberal in logistic regression.

For comparison purposes, we also describe the procedure to construct a \emph{bootstrap likelihood ratio confidence set} (BootLR).
The first two steps are the same as the bootstrap Wald confidence set, while the third step is to compute the bootstrap LR statistic $T_{\text{LR}}^b := 2[L_n^b(\theta_n) - L_n^b(\theta_n^b)]$.
And we compare $2[L_n(\theta_0) - L_n(\theta_n)]$ with the upper $\delta$ quantile of $\{T_{\text{LR}}^b\}_{b=1}^B$ to decide if the bootstrap LR confidence set covers the true parameter.
For the well-specified least squares, the two bootstrap confidence sets perform similarly with coverages close to the target ones.
However, when the target coverage is small (i.e., $0.75$), they tend to be liberal.
For the misspecified least squares, the bootstrap two confidence sets perform similarly.
When the target coverage is large, they tend to be conservative; when the target coverage is small, they tend to be liberal.
For the well-specified logistic regression, the bootstrap Wald confidence set tends to be liberal and the bootstrap LR one tends to be conservative.
