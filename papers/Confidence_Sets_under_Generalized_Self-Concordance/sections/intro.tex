The problem of statistical inference on learned parameters is regaining the importance it deserves
as machine learning and data science are increasingly impacting humanity and society through an increasingly large range of successful applications from transportation to healthcare \citep[see, e.g.,][]{fan2020statistical,efron2021computer}.
The classical asymptotic theory of M-estimation is well established in a rather general setting under the assumption that the parametric model is well-specified, i.e., the underlying data distribution belongs to the parametric family.
Two types of confidence sets can be constructed from this theory: (a) the Wald-type one which relies on the weighted difference between the estimator and the target parameter, and (b) the likelihood-ratio-type one based on the log-likelihood ratio between the estimator and the target parameter.
The main tool is the local asymptotic normality (LAN) condition introduced by \citet{lecam1960locally}.
We mention here, among many of them, the monographs \citep{ibragimov1981statistical,van2000asymptotic,geer2000empirical}.

In many real problems, the parametric model is usually an approximation to the data distribution, so it is too restrictive to assume that the model is well-specified.
To relax this restriction, model misspecification has been considered in the asymptotic regime; see, e.g., \citep{huber1967under,wakefield2013bayesian,dawid2016scoring}.
Another limitation of classical asymptotic theory is its asymptotic regime where $n \rightarrow \infty$ and the parameter dimension $d$ is fixed.
This is inapplicable in the modern context where the data are of a rather high dimension involving a huge number of parameters.

The non-asymptotic viewpoint has been fruitful to address high dimensional problems---the results are developed for all fixed $n$ so that it also captures the asymptotic regime where $d$ grows with $n$.
Early works in this line of research focus on specific models such as Gaussian models~\citep{beran1996confidence,beran1998modulation,laurent2000adaptive,baraud2004confidence}, ridge regression~\citep{hsu2012random}, logistic regression \citep{bach2010self}, and robust M-estimation~\citep{zhou2018huber,chen2020robust}; see~\citet{bach2021learning} for a survey.~\citet{spokoiny2012parametric} addressed the finite-sample regime in full generality in a spirit similar to the classical LAN theory.
The approach of~\cite{spokoiny2012parametric} relies on heavy empirical process machinery and requires strong global assumptions on the deviation of the empirical risk process. More recently,~\citet{ostrovskii2021finite} focused on risk bounds, specializing their discussion to linear models with (pseudo) self-concordant losses and obtained a more transparent analysis under neater assumptions.

\begin{table*}[t]
    \caption{Loss function of generalized linear models.}
    \label{tab:glms}
    \centering
    \renewcommand{\arraystretch}{1.2}
    \begin{tabular}{lccc}
        \addlinespace[0.4em]
        \toprule
        & \multicolumn{1}{c}{\textbf{Data}} & \multicolumn{1}{c}{\textbf{Model}} & \textbf{Loss} \\
        \midrule
        Linear & $(X, Y)$ & $Y \mid X \sim \mathcal{N}(\theta^\top X, \sigma^2)$ & $\frac12 (y - \theta^\top x)^2$ \\
        Logistic & $(X, Y)$ & $Y \mid X \sim \mbox{Bernoulli}\big((1 + e^{-\theta^\top X})^{-1}\big)$ & $\log{\big(1 + \exp(-y(\theta^\top x))\big)}$\\
        Poisson & $(X, Y)$ & $Y \mid X \sim \mbox{Poisson}(\exp(\theta^\top X))$ & $-y(\theta^\top x) + \exp(\theta^\top x)$ \\
        \bottomrule
    \end{tabular}
\end{table*}

A critical tool arising from this line of research is the so-called \emph{Dikin ellipsoid}, a geometric object identified in the theory of convex optimization~\citep{yurii1994interior,ben2001lectures,boyd2004convex,tunccel2010self,bubeck2016black,bubeck:2019}.
The Dikin ellipsoid corresponds to the distance measured by the Euclidean distance weighted by the Hessian matrix at the optimum.
This weighted Euclidean distance is adapted to the geometry near the target parameter and thus leads to sharper bounds that do not depend on the minimum eigenvalue of the Hessian. This important property has been used fruitfully in various problems of learning theory and mathematical statistics \citep{zhang2015disco,yang2016optimistic,faury2020improved}.

\myparagraph{Outline}
We review in \Cref{sec:problem} the empirical risk minimization framework and the two types of confidence sets from classical asymptotic theory.
We establish finite-sample bounds to characterize these two confidences sets, whose sizes are controlled by the \emph{effective dimension}, in a non-asymptotic fashion in \Cref{sec:main_results}.
Our results hold for a general class of models characterized by the notion of \emph{generalized self-concordance}.
Along the way, we show how the effective dimension can be estimated from data and provide its estimation accuracy.
This is a novel result and is of independent interest.
We apply our results to compare Rao's score test, the likelihood ratio test, and the Wald test for goodness-of-fit testing in \Cref{sec:application}.
Finally, in \Cref{sec:experiments}, we illustrate the interest of our results on synthetic data.
