We give several examples whose loss function is generalized self-concordant so that our results can be applied.
We also provide finite-sample analysis for Rao's score test, the likelihood ratio test, and the Wald test in goodness-of-fit testing.
All the proofs and derivations are deferred to \Cref{sec:example}.

\subsection{Examples}
\label{sub:examples}

\begin{example}[Generalized linear models]\label{ex:glm}
    Let $Z := (X, Y)$ be a pair of input and output, where $X \in \calX \subset \reals^d$ and $Y \in \calY \subset \reals$.
    Let $t: \calX \times \calY \rightarrow \reals^d$ and $\mu$ be a measure on $\calY$.
    Consider the statistical model
    \begin{align*}
        p_\theta(y \mid x) \sim \frac{\exp(\theta^\top t(x, y))}{\int \exp(\theta^\top t(x, \bar y)) \D \mu(\bar y)} \D \mu(y)
    \end{align*}
    with $\norm{t(X, Y)}_2 \le_{a.s.} M$.
    It induces the loss function
    \begin{align*}
        \score(\theta; z) := -\theta^\top t(x, y) + \log{\int \exp(\theta^\top t(x, \bar y)) \D \mu(\bar y)},
    \end{align*}
    which is generalized self-concordant for $\nu = 2$ and $R = 2M$.
    Moreover, this model satisfies \Cref{asmp:sub_gaussian,asmp:bernstein,asmp:lip} and \ref{asmp:subG_local}.
\end{example}


\begin{example}[Score matching with exponential families]\label{ex:score_matching}
    Assume that $\bbZ = \reals^p$.
    Consider an exponential family on $\reals^d$ with densities
    \begin{align*}
        \log{p_\theta(z)} = \theta^\top t(z) + h(z) - \Lambda(\theta).
    \end{align*}
    The non-normalized density $q_\theta$ then reads $\log{q_\theta(z)} = \theta^\top t(z) + h(z)$.
    As a result, the score matching loss becomes
    \begin{align*}
        \score(\theta; z) = \frac12 \theta^\top A(z) \theta - b(z)^\top \theta + c(z) + \text{const},
    \end{align*}
    where $A(z) := \sum_{k=1}^p \frac{\partial t(z)}{\partial z_k} \big(\frac{\partial t(z)}{\partial z_k}\big)^\top$ is positive semi-definite, $b(z) := \sum_{k=1}^p \left[ \frac{\partial^2 t(z)}{\partial z_k^2} + \frac{\partial h(z)}{\partial z_k} \frac{\partial t(z)}{\partial z_k} \right]$, and $c(z) := \sum_{k=1}^p \left[ \frac{\partial^2 h(z)}{\partial z_k^2} + \big(\frac{\partial h(z)}{\partial z_k}\big)^2 \right]$.
    Therefore, the score matching loss $\score(\theta; z)$ is convex.
    Moreover, since the third derivatives of $\ell(\cdot; z)$ is zero, the score matching loss is generalized self-concordant for all $\nu \ge 2$ and $R \ge 0$.
\end{example}

\subsection{Rao's Score Test and Its Relatives}
\label{sub:goodness}

We discuss how our results can be applied to analyze three classical goodness-of-fit tests.
In this subsection, we will assume that the model is well-specified.
Due to \Cref{asmp:proper_loss}, we will use $\theta_\star$ to denote the true parameter of $\Prob$ and reserve $\theta_0$ for the parameter under the null hypothesis.

Given a subset $\Theta_0 \subset \Theta$, a goodness-of-fit testing problem is to test the hypotheses
\begin{align*}
    \hnull: \theta_\star \in \Theta_0 \leftrightarrow \halt: \theta_\star \notin \Theta_0.
\end{align*}
We focus on a simple null hypothesis where $\Theta_0 := \{\theta_0\}$ is a singleton.
A statistical test consists of a test statistic $T := T(Z_1, \dots, Z_n)$ and a prescribed critical value $t$, and we reject the null hypothesis if $T > t$.
Its performance is quantified by the \emph{type I error rate} $\Prob(T > t \mid \hnull)$ and \emph{statistical power} $\Prob(T > t \mid \halt)$.
Classical goodness-of-fit tests include Rao's score test, the likelihood ratio test (LRT), and the Wald test.
Their test statistics are $\rao := \norm{\grad_n(\theta_0)}_{H_n^{-1}(\theta_0)}^2$, $\lr := 2[\score_n(\theta_0) - \score_n(\theta_n)]$, and $\wald := \norm{\theta_n - \theta_0}_{H_n(\theta_n)}^2$,
respectively.

Our approach can be applied to analyze the type I error rate of these tests as summarized in the following proposition.
\begin{proposition}[Type I error rate]\label{prop:typeI}
    Suppose that \Cref{asmp:sub_gaussian,asmp:bernstein} with $r = 0$ hold true.
    Under $\hnull$, we have, with probability at least $1 - \delta$,
    \begin{align*}
        \rao \lesssim \log{(e/\delta)} \frac{d}n
    \end{align*}
    whenever $n \gtrsim \log{(2d/\delta)}$.
    Furthermore, if \Cref{asmp:self_concordance,asmp:sub_gaussian,asmp:bernstein} with $r = C_\nu \lambda_\star^{(\nu-3)/2}/R$ hold true, we have, with probability at least $1 - \delta$,
    \begin{align*}
        \lr, \wald \lesssim \log{(e/\delta)} \frac{d}{n}
    \end{align*}
    whenever $n$ satisfies \eqref{eq:n_large_enough}.
\end{proposition}

This result implies that the three test statistics all scale as $O(d / n)$ under the null hypothesis.
Consequently, for a fixed significance level $\alpha \in (0, 1)$, we can choose the critical value $t = t_n(\alpha) = O(d/n)$ so that their type I error rates are below $\alpha$.
With this choice, we can then characterize the statistical powers of these tests under alternative hypotheses $\theta_\star \neq \theta_0$ where $\theta_\star$ may depend on $n$.
Let $\Omega(\theta) := G(\theta)^{1/2} H(\theta)^{-1} G(\theta)^{1/2}$ and $h(\tau) := \min\{\tau^2, \tau\}$.

\begin{proposition}[Statistical power]
\label{prop:power}
    Let $\theta_\star \neq \theta_0$.
    The following statements are true for sufficiently large $n$.
    \begin{enumerate}
        \item[(a)] Suppose that \Cref{asmp:self_concordance,asmp:sub_gaussian,asmp:bernstein} hold true with $r=0$.
        When $\theta_\star - \theta_0 = O(n^{-1/2})$ and $\tau_n := t_n(\alpha)/4 - \norm{S(\theta_0)}_{H(\theta_0)^{-1}}^2 - \Tr(\Omega(\theta_0))/n > 0$, we have
        \begin{align*}
            &\quad \Prob(\rao > t_n(\alpha)) \\
            &\le 2d e^{- C_{K_2, \sigma_H} n} + e^{-C_{K_1} h(n \tau_n/\norm{\Omega(\theta_0)}_2)}.
        \end{align*}
        When $\theta_* - \theta_n = \omega(n^{-1/2})$, we have
        \begin{align*}
            &\quad \Prob(\rao > t_n(\alpha)) \\
            &\ge 1 - 2d e^{-C_{K_2, \sigma_H} n} - e^{-C_{K_1} n \bar \tau_n/\norm{\Omega(\theta_0)}_2},
        \end{align*}
        where $\bar \tau_n = \Theta(\norm{\theta_\star - \theta_n}^2)$.
        
        \item[(b)] Suppose that the assumptions in \Cref{thm:conf_set} hold true.
        When $\theta_\star - \theta_0 = O(n^{-1/2})$ and $\tau_n' := t_n(\alpha)/384 - \norm{\theta_\star - \theta_0}_{H(\theta_\star)}^2/64 - d/n > 0$, we have
        \begin{align*}
            &\quad \Prob(\lr > t_n(\alpha)) \\
            &\le e^{-C_{K_1} h(n\tau_n'/\norm{\Omega(\theta_\star)}_2)} + e^{-C_{K_1, \nu} (\lambda_\star n)^{3-\nu}/(R^2 d)}.
        \end{align*}
        When $\theta_* - \theta_n = \omega(n^{-1/2})$, we have
        \begin{align*}
            &\quad \Prob(\lr > t_n(\alpha)) \\
            &\ge 1 - e^{-C_{K_1} \frac{n \bar \tau_n'}{\norm{\Omega(\theta_\star)}_2}} - e^{-\frac{C_{K_1, \nu} (\lambda_\star n)^{3-\nu}}{R^2 d}},
        \end{align*}
        where $\bar \tau_n' = \Theta(\norm{\theta_\star - \theta_n}^2)$.
        
        \item[(c)] The same statements replacing $\lr$ by $\wald$.
    \end{enumerate}
\end{proposition}


According to \Cref{prop:power}, when $\theta_\star - \theta_0 = O(n^{-1/2})$, the powers of the three tests are asymptotically upper bounded; when $\theta_\star - \theta_0 = \omega(n^{-1/2})$, the power of Rao's score test tends to one at rate $O(e^{-n \norm{\theta_\star - \theta_0}^2})$ and the ones of the other two tests tend to one at rate $O(e^{-n \norm{\theta_\star - \theta_0}^2 \wedge n^{3-\nu}})$.
