\section{Introduction}
\label{sec:intro}
ASV refers to verifying whether a utterance is uttered by a certain person, and has been adopted in a wide range of security-critical applications
% ASV is an essential biometric identification technology, as a particular person’s voice carry inherent characteristics of identity that is relatively stable and non-reproducible.  
Recently, deep learning has dramatically boosted advancements in ASV, resulting in a variety of high-performance ASV models \cite{dehak2010front,kenny2012small,snyder2018x}.
% So ASV has been adopted in a wide range of security-critical applications, including transaction authentication and access control.
However, ASV is susceptible to the recently emerged adversarial attacks \cite{kreuk2018fooling,das2020attacker}, causing serious security problems.
% Given that ASV systems have gained wide usage in safety-critical environments, their security is of high priority.
% Since adversarial attacks against ASV only emerged very recently, defense methods that can counter them are limited. 
% Hence, this paper focuses on mitigating adversarial attacks for ASV.
% Hence, this paper focuses on mitigating adversarial attacks for ASV.

% The concept of adversarial attacks was first proposed in \cite{szegedy2013intriguing}. 
Using adversarial samples to attack machine learning models is called adversarial attack \cite{szegedy2013intriguing}.
Adversarial samples are similar to their genuine counterparts according to human perception, but can can fool high-performance models, which is surprising.
Speech processing models, including automatic speech recognition (ASR) \cite{carlini2018audio,qin2019imperceptible} and anti-spoofing for ASV \cite{liu2019adversarial,wu2020defense_2,wu2020defense}, are also susceptible to adversarial attacks.
ASV models are no exception \cite{das2020attacker,jati2021adversarial} – the first illustration of the vulnerability of ASV models to adversarial attacks was presented in \cite{kreuk2018fooling}. 
Also, state-of-the-art ASV models, including i-vector and x-vector models, can be manipulated by adversarial samples \cite{villalba2020x,li2020adversarial}.
This is followed by efforts that investigate more malicious adversarial attacks from the perspectives of universality \cite{marras2019adversarial}, in-the-air transferability \cite{li2020practical}, and imperceptibility \cite{wang2020inaudible}.
% Countering adversarial attacks for ASV is of utmost importance, due to the widespread usage of ASV.


However, due to limited research efforts \cite{wang2019adversarial,li2020investigating,zhang2020adversarial,wu2021adversarialasv,wu2021improving,joshi2021adversarial,wu2021voting} in adversarial defense on ASV, effective strategies remain an open question. 
Wang et al. \cite{wang2019adversarial} adopts adversarial training to mitigate adversarial attacks for ASV by injecting adversarial data into the training set.
Li et al. \cite{li2020investigating} proposes a detection model for adversarial samples by training it on a mixture of adversarial samples and genuine samples.
Zhang et al. \cite{zhang2020adversarial} harnesses an independent DNN filter trained with adversarial samples and applies it to purify the adversarial samples.
However, the above methods \cite{wang2019adversarial,li2020investigating,zhang2020adversarial} require the knowledge of the attack algorithms used by attackers.
It is impractical to assume that the ASV system designers know in advance which attack algorithms will be implemented by attackers in-the-wild, not to mention that such methods \cite{wang2019adversarial,li2020investigating,zhang2020adversarial} may overfit to a specific adversarial attack algorithm.
Self-supervised learning models have also been proposed \cite{wu2021adversarialasv,wu2021improving} as a filter to purify the adversarial noise.
Josshi et al. \cite{joshi2021adversarial} proposes four pre-processing defenses, and \cite{wu2021voting} introduces the idea of voting to prevent risky decisions of ASV when encountering adversarial samples.

We propose neural vocoders to detect adversarial samples and use Parallel WaveGAN \cite{yamamoto2020parallel} as a case study. 
Vocoders are usually adopted to attack ASV systems by generating spoofing audios \cite{todisco2019asvspoof}.
We propose the contrary to harness the vocoders to defend ASV systems. 
Defense aims at purifying the adversarial noise, while detection aims at spotting adversarial samples and filtering them away.
In contrast to the approaches which need to know the adversarial attack methods \cite{wang2019adversarial,li2020investigating,zhang2020adversarial}, the proposed approach does not require such information.
Compared to approaches that defend ASV in the frequency domain \cite{wu2021adversarialasv,wu2021improving}, the proposed approach directly detects adversarial samples in the time domain.
So the proposed method can serve as a complement of \cite{wu2021adversarialasv,wu2021improving}.
The vocoder in \cite{joshi2021adversarial} is for defense, while the proposed approach is for detection.
Wu et al. \cite{wu2021voting} focuses on defense, yet this paper aims at detection. 
To the best of our knowledge, this is the first paper to adopt neural vocoders to detect time-domain adversarial samples for ASV, and our results demonstrate effectiveness over the traditional Griffin-Lim vocoder.
% We also propose the traditional vocoder, Griffin-Lim, to do detection.
% The proposed approach achieves effective detection performance.