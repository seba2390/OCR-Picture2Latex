\section{Background}
\subsection{Automatic speaker verification}
The objective of ASV is to authenticate the claimed identity of a speaker by a piece of his/her speech and some enrolled speaker records.
The procedure of ASV can be divided into feature engineering, speaker embedding extraction, and similarity scoring. 
Feature engineering aims at transforming a piece of utterance in waveform representation, into acoustic features, such as Mel-frequency cepstral coefficients (MFCCs), filter-banks, and spectrograms. 
The speaker embedding extraction procedure of recently ASV models \cite{dehak2010front,kenny2012small,snyder2018x} usually extracts utterance-level speaker embedding from acoustic features.
Then similarity scoring will measure the similarity between the testing speaker embedding and the enrolled speaker embedding.
% \cite{dehak2010cosine, kenny2013plda}
The higher the score, the more likely that the enrolment utterance and the testing utterance belong to the same speaker, and vice versa. 
Let us denote the testing utterance and the enroll utterance as $x_{t}$ and $x_{e}$ respectively.
For simplicity, we combine the above three procedures and view ASV as an end-to-end function $f$: 
\begin{align}
    &s = f(x_{t}, x_{e}), 
\end{align}
where $s$ is the similarity score between $x_{t}$ and $x_{e}$.


\subsection{Adversarial attack}
Attackers deliberately incorporate a tiny perturbation, which is indistinguishable from human perception, and combine it with the original sample to generate the new sample, which will manipulate the model give wrong prediction.
The new sample and the tiny perturbation are denoted as the adversarial sample and adversarial noise, respectively.
Suppose that the attackers in the wild have access to the internals of the ASV system, including structures, parameters and gradients, and have the access to the testing utterance $x_{t}$.
They aim at crafting such an adversarial utterance by finding an adversarial perturbation.
Different searching strategies for elaborating adversarial noise result in different attack algorithms.
In this work, we adopt a powerful attack method, the basic iterative method (BIM) \cite{kurakin2016adversarial}.
During BIM attack, attackers will start from $x_{t}^{0}=x_{t}$, then iteratively update it to find the adversarial sample:
\begin{equation}
\begin{aligned}
    x_{t}^{k+1}=clip\left(x_{t}^{k} + \alpha \cdot (-1)^{is\_tgt} \cdot sign\left(\nabla_{x_{t}^{k}}f(x_{t}^{k}, x_{e}) \right)\right), 
    \\ for \, k=0,1, \ldots, K-1,
\end{aligned}
\end{equation}
where $clip(.)$ is the clipping function which make sure that $||x_{t}^{k+1} - x_{t}||_{\infty}\leq \epsilon$,
$\epsilon$, denotes the attack budget or intensity predefined by the attackers,
$\epsilon \geq 0 \in \mathbb{R}$,
$\alpha$ is the step size,
$is\_tgt=1$ and $is\_tgt=0$ for the target trial and the non-target trial respectively,
$K$ is the number of iterations and we define $K = \lceil \epsilon / \alpha \rceil$, where $\lceil.\rceil$ denotes the ceiling function.
In target trials, the testing and enrolment utterances are pronounced by the same speaker.  In non-target trials, they belong to different speakers.
% In the target trial, the testing and enroll utterance are pronounced by the same speaker, yet they belong to different speakers in the non-target trial.
Take the non-target trial as an example -- after the BIM attack, the similarity score between the testing and enrolment utterances will be high, which will mislead the ASV system to falsely accept the imposter.
We recommend that our readers listen to the demo of the deliberately crafted adversarial samples \footnote[2]{https://haibinwu666.github.io/adv-audio-demo/index.html}, which tend to be indistinguishable from their genuine counterparts.

\subsection{Vocoder}
Due to the lack of phase information, speech waveforms cannot be restored directly from acoustic features, such as linear spectrograms and mel-spectrograms.
The traditional vocoder, Griffin-Lim, \cite{griffin1984signal} is usually used to reconstruct phase information.
However, it inevitably introduces distortion during reconstruction, resulting in reduced speech quality.
We argue that the introduced distortion may also degrade the effect of the attack.
Another approach, the neural vocoder, takes acoustic features as conditions and uses a neural network to generate speech signals.
Since a neural vocoder is trained to maximize the likelihood of real speech in training data, we expect that when given distorted or attacked acoustic features, the neural vocoder can generate their genuine counterparts. 
In contrast to Griffin-Lim, a neural vocoder is a data-driven method, which can model the manifolds of genuine data, and thus generates waveform with lowered distortion. %without too much distortion given the real acoustic feature.

% Common neural vocoders, such as WaveNet \cite{oord2016wavenet} and WaveRNN \cite{kalchbrenner2018efficient}, can restore high-quality speech but with slow inference speed due to the autoregressive architecture.
Neural vocoders can restore high-quality speech but with slow inference speed due to the autoregressive architecture.
Parallel WaveGAN \cite{yamamoto2020parallel} adopted a model based on dilated CNN, which can generate audio samples in parallel.
They jointly trained the model using the adversarial loss in GAN and the proposed loss on the frequency domain.
Parallel synthesis improves the efficiency of speech generation, while the GAN architecture can make the Parallel WaveGAN effectively model the distribution of real speech.
Thus in this work, we adopt Parallel WaveGAN for spotting adversarial samples.



\section{Neural vocoder is all you need}
\label{sec:method}
\begin{figure}[ht]
  \centering
  \centerline{\includegraphics[width=\linewidth]{figures/defense.png}}
  \vspace{-5pt}
  \caption{Proposed detection framework. $s$ and $s'$ are the ASV scores for $x$ and $x'$. $|s-s'|$ is the absolute value between $s$ and $s'$.}
  \label{fig:method}
  \vspace{-5pt}
\end{figure}

\subsection{The detection procedure}
\label{subsec:vocoder based detection}
We first detail the detection procedure, followed by the reason why it works.
The vocoder\footnote[3]{Unless specified otherwise, the use of “vocoder” refers to the “neural vocoder” in the following sections.}-based detection framework is shown in Fig.~\ref{fig:method}.
For brevity, we omit the enrollment utterance $x_{e}$.
The subscript of $x_{t}$ is also omitted, and we use $x$ to denote the testing utterance.
We use $x'$ to denote the testing utterance after feature extraction and vocoder preprocessing (yellow block and gray block in Fig.~\ref{fig:method}).
We follow the procedure in Fig.~\ref{fig:method}, and get $|s-s'|$ for a piece of testing utterance $x$.
Denote the score variation $d=|s-s'|$.
Denote $\mathbb{T}_{gen} = \{x_{gen}^{1}, x_{gen}^{2},...,x_{gen}^{I}\}$ is the set of genuine testing utterances, and $\vert \mathbb{T}_{gen} \vert $ denotes the number of elements in set $\mathbb{T}_{gen}$.
Then we derive $\{d_{gen}^{1}=|s_{gen}^{1}-{s_{gen}^{1}}'|,d_{gen}^{2}=|s_{gen}^{2}-{s_{gen}^{2}}'|, ...,d_{gen}^{I}=|s_{gen}^{I}-{s_{gen}^{I}}'|\}$ for $\mathbb{T}_{gen}$ as shown in Fig.~\ref{fig:method}, where $s_{gen}^{i}$ and ${s_{gen}^{i}}'$ are the ASV scores for $x_{gen}^{i}$ before and after vocoder preprocessing respectively.
Given a false positive rate for detection ($FPR_{given}$, a real number), such that $FPR_{given} \in [0,1]$, for genuine samples, we derive a detection threshold $\tau_{det}$:
\begin{align}
    &FPR_{det}(\tau) = \frac{\vert \{ d_{gen}^{i} > \tau : x_{gen}^{i} \in \mathbb{T}_{gen} \} \vert}{\vert \mathbb{T}_{gen} \vert} \label{eq:det-far} \\
    &\tau_{det} = \{ \tau \in \mathbb{R} : FPR_{det}(\tau) =FPR_{given} \} \label{eq:det-threshold} 
\end{align}
where $FPR_{det}(\tau)$ is the false positive rate for genuine samples given a threshold $\tau$, $d_{gen}^{i}$ is derived by $x_{gen}^{i}$ as shown in Fig.~\ref{fig:method}.
In realistic conditions, the ASV system designer is unaware of adversarial samples, not to mention which exact adversarial attack algorithm will be adopted.
So the detection threshold $\tau_{det}$ is determined based on genuine samples.
Hence the detection method does not require knowledge of adversarial sample generation.

Given a testing utterance, be it adversarial or genuine, $|s-s'|$ will be derived, and the system will label it adversarial if $|s-s'| > \tau_{det}$, and vice versa.
The detection rate ($DR_{\tau_{det}}$) under $\tau_{det}$, which is determined by Eq.~\ref{eq:det-threshold}, for adversarial data can be derived as:
\begin{align}
    &DR_{\tau_{det}} = \frac{\vert \{ d_{adv}^{i} > \tau_{det} : x_{adv}^{i} \in \mathbb{T}_{adv} \} \vert}{\vert \mathbb{T}_{adv} \vert} \label{eq:det-rate} 
\end{align}
where $\mathbb{T}_{adv}$ denotes the set of adversarial testing utterances, and $d_{adv}^{i}$ is derived by $x_{adv}^{i}$ as the procedure illustrated in Fig.~\ref{fig:method}.

\subsection{Rationale behind the detection framework}
As the vocoder is data-driven and trained with genuine data during training, it models the distribution of genuine data, resulting in less distortion when generating genuine waveforms.
Thus, during inference, the vocoder's preprocessing will not affect the ASV scores of genuine samples too much, as reflected by the EER in the second row and last column of Table~\ref{tab:EER}.
However, suppose the inputs are adversarial samples. In that case, the vocoder will try to pull it back towards the manifold of their genuine counterparts to some extent, resulting in purifying the adversarial noise.

Take a non-target trial as an example, in which an ASV system should give the genuine sample a score below the threshold.
And after the nearly lossless reconstruction procedure (i.e., the yellow block and gray block in Fig.~\ref{fig:method}), the genuine sample will not change much, and the ASV score will remain largely unchanged.
In contrast to the genuine sample, the ASV score for the adversarial one is higher than the threshold.
And the reconstruction procedure will try to counter the adversarial noise, purify the adversarial sample, and decrease the ASV score for the adversarial sample. 
Then we can adopt the discrepancy of the score variations, $d_{adv}$ and $d_{gen}$, to discriminate between them, as shown in Fig.~\ref{fig:Hist-ep5}.
The transform, which makes $d_{gen}$ as small as possible while makes $d_{adv}$ as large as possible, is suitable for adversarial detection.

Also, the Griffin-Lim can be regarded as an imperfect transform as well, and it will also introduce distortion to affect the adversarial noise.
However, for genuine data, the distortion introduced by the Griffin-Lim is more significant than the vocoder, as it is not a data-driven method and can not be customized for a specific dataset, resulting in larger $d_{gen}$ and inferior detection performance.
