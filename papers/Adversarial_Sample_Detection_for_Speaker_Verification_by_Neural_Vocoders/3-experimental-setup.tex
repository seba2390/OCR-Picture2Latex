\section{Experimental setup}

\begin{table}[t]
\centering
\caption{EER with different $\epsilon$}
\scalebox{0.8}{
\begin{tabular}{cccccc}
\toprule
\multirow{2}{*}{Method} & \multicolumn{5}{c}{EER with different $\epsilon$ (\%)} \\
        & 20      & 15      & 10      & 5       & 0 (no attack)     \\ \hline
None    & 99.33   & 95.66   & 90.57   & 74.04   & 2.88     \\
Vocoder & 87.58    & 65.75   & 52.20   & 30.37   & 3.39    \\
GL-lin  & 95.23   & 80.83   & 66.73   & 39.49   & 3.93              \\
GL-mel  & 88.41   &65.39   & 49.76   & 26.67   & 3.81   \\ \bottomrule
\end{tabular}
}
\label{tab:EER}
\end{table}

\subsection{ASV setup}
The adopted system is a variation of X-vector system, and is modified from \cite{chung2020defence}.
We adopt the dev sets of Voxceleb1 \cite{nagrani2017voxceleb} and Voxceleb2 \cite{chung2018voxceleb2} for training.
Spectrograms are extracted with a Hamming window of width 25ms and step 10ms, and 64-dimensional fbanks are extracted as input features.
No further data augmentation and voice activity detection are adopted during training.
% The model is trained for only 50 epochs rather than 500 epochs in \cite{chung2018voxceleb2} because the focus here is to evaluate the performance of detection method rather than the performance of ASV.
Cosine similarity is used for back-end scoring.
We adopt the trials provided in VoxCeleb1 test set for generating adversarial samples, evaluating the ASV performance and detection performance.
% The equal error rate (EER) for genuine samples is 2.88\% as shown in Table~\ref{tab:EER}.

\subsection{Griffin-Lim and Parallel WaveGAN}
We use Griffin-Lim and Parallel WaveGAN in our experiments. The Griffin-Lim method, denoted as ``GL-lin", uses 100 iterations to reconstruct speech from linear spectrograms. ``GL-mel" denotes that linear spectrograms are first estimated from Mel-spectrograms using the pseudo inverse. Our Parallel WaveGAN method, denoted as "Vocoder", is modified from the public implementation\footnote[4]{https://github.com/kan-bayashi/ParallelWaveGAN}.
% \footnote[4]{\href{https://github.com/kan-bayashi/ParallelWaveGAN}{\texttt{Parallel WaveGAN}}}.
We use 80-dimension, band-limited (80-7600 kHz), and normalized log-mel spectrograms as conditional features. The window and hop sizes are set to 50 ms and 12.5 ms.
The architectures of the generator and discriminator follow those in \cite{yamamoto2020parallel}. We trained the model on the dev set of VoxCeleb1 ~\cite{nagrani2017voxceleb} for 1000k iterations, which takes around 5 days. 
Note that there is no overlap between the training data of Voxceleb1 for neural vocoder and the evaluation data of speaker verification.
To further show that the vocoder adopted in the proposed method is dataset independent, we also trained a universal vocoder~\cite{hsu2019towards} with the same structure as Vocoder, but on Lrg dataset \cite{hsu2019towards}, which is a large speech dataset containing 6 languages and more than 600 speakers.
The vocoder trained on Lrg is denoted as "Vocoder-L".

\subsection{ASV performance with genuine and adversarial inputs}
\label{subsec:ASV performance}
To evaluate the performance, we use the trials provide in VoxCeleb1 test set, which contains 37,720 enrollment-testing pairs.
During adversarial samples generation, $\alpha$ is set as $1$, attack budget $\epsilon$ is set as $5,10,15,20$. 
The adversarial attack is conducted in the time domain.
Also, note that it is time-consuming to generate adversarial samples.
We first evaluate the performance of our ASV system on genuine and adversarial samples.
The results are shown in the first row and the last column of Table~\ref{tab:EER}. "None" denotes that utterances are passed directly to the ASV system. 
We find that: 
(1) When testing on genuine samples, the ASV system achieved an EER of 2.88\%, comparable to recent ASV models.
When using generated speech as input, we found that the EER slightly increased.
(2) While introducing the adversarial attack, the EER increased from 2.88\% to over 70\%, which shows the effectiveness of the attack method.
The larger the attack budget $\epsilon$ is, the higher the attack intensity is.
One may question why the EER is over 50\%. The threshold of ASV is fixed, and the attackers try their best to do adversarial attack to make the score over and below the threshold for non-target and target trials respectively, resulting in the decisions for the trials reversed.
