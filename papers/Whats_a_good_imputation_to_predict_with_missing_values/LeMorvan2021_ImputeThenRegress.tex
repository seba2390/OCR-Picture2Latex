\documentclass{article}

% if you need to pass options to natbib, use, e.g.:
%     \PassOptionsToPackage{numbers, compress}{natbib}
% before loading neurips_2021

% ready for submission
% \usepackage{neurips_2021}

% to compile a preprint version, e.g., for submission to arXiv, add add the
% [preprint] option:
% \usepackage[preprint]{neurips_2021}

% to compile a camera-ready version, add the [final] option, e.g.:
\usepackage[final]{neurips_2021}

% to avoid loading the natbib package, add option nonatbib:
%    \usepackage[nonatbib]{neurips_2021}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{dsfont}         % for indicator variable
\usepackage[dvipsnames]{xcolor}

% My packages
\usepackage{comment}
\usepackage{enumitem}
\usepackage{dsfont}
%\usepackage{natbib}
\usepackage{graphicx}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{amstext}
\usepackage{amsthm} % for newtheorem
\usepackage{mathtools}
\usepackage{thm-restate}
\usepackage{bm}
\usepackage{nicematrix}
\usepackage{stmaryrd} % for bbr
\usepackage[normalem]{ulem} % for sout (striketrough) 
\usepackage[textsize=tiny]{todonotes}
\usepackage[parfill]{parskip}   % to remove indents at the beginning of new paragraphs
\usepackage{subfiles}
\usepackage{pgfplots}
\usetikzlibrary{intersections}
\usetikzlibrary{arrows, positioning, calc}
\usetikzlibrary{shapes,fit}
\usetikzlibrary{decorations.pathreplacing, patterns}
% \pgfplotsset{compat = 1.3}
\pgfplotsset{compat = newest}


\newcommand{\RR}{\mathbb{R}}
\newcommand{\E}{\mathbb{E}}
\newcommand{\Bcal}{\mathcal{B}}
\newcommand{\Ccal}{\mathcal{C}}
\newcommand{\Dcal}{\mathcal{D}}
\newcommand{\Fcal}{\mathcal{F}}
\newcommand{\Mcal}{\mathcal{M}}
\newcommand{\Ncal}{\mathcal{N}}
\newcommand{\Ocal}{\mathcal{O}}
\newcommand{\Pcal}{\mathcal{P}}
\newcommand{\Rcal}{\mathcal{R}}
\newcommand{\Scal}{\mathcal{S}}
\newcommand{\Vcal}{\mathcal{V}}
\newcommand{\Xcal}{\mathcal{X}}
\renewcommand{\P}{\mathds{P}}
\newcommand{\NA}{\mathtt{NA}}


\newcommand{\ML}[1]{\textcolor{blue}{#1}}
\newcommand{\GV}[1]{\textcolor{red}{#1}}
\newcommand{\ES}[1]{\textcolor{purple}{#1}}

\newcommand{\JJ}[1]{\textcolor{green}{#1}}

\newcommand{\tmu}{\widetilde{\mu}}
\newcommand{\tsigma}{\widetilde{\sigma}}
\newcommand{\tSigma}{\widetilde{\Sigma}}
\newcommand{\cond}{{mis^\prime|j, obs}}
\newcommand{\Ximp}{X^{imp}}

\newcommand{\br}[1]{\left(#1\right)}
\newcommand{\sqb}[1]{\left[#1\right]}
\newcommand{\cbr}[1]{\left\{#1\right\}}
\newcommand{\bbr}[1]{\left\llbracket#1\right\rrbracket}
\newcommand{\nm}[1]{\left\Vert\, #1 \,\right\Vert}
\newcommand{\abs}[1]{\left\vert\, #1 \,\right\vert}

\theoremstyle{plain}
\newtheorem{proposition}{Proposition}[section]
\newtheorem{theorem}{Theorem}[section]
\newtheorem{corollary}{Corollary}[section]
\newtheorem{assumption}{Assumption}
\newtheorem{lemma}{Lemma}[section]
\newtheorem{remark}{Remark}
\newtheorem{definition}{Definition}

\def\toptitlebar{\hrule height4pt \vskip .25in}
\def\bottomtitlebar{ \vskip .25in \hrule height1pt \vskip .25in}

\def\mytitle{What's a good imputation to predict with missing values?}
\title{\mytitle}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to LaTeX to determine where to break the
% lines. Using \AND forces a line break at that point. So, if LaTeX puts 3 of 4
% authors names on the first line, and the last on the second line, try using
% \AND instead of \And before the third author name.

% \author{%
%   David S.~Hippocampus\thanks{Use footnote for providing further information
%     about author (webpage, alternative address)---\emph{not} for acknowledging
%     funding agencies.} \\
%   Department of Computer Science\\
%   Cranberry-Lemon University\\
%   Pittsburgh, PA 15213 \\
%   \texttt{hippo@cs.cranberry-lemon.edu} \\
%   % examples of more authors
%   % \And
%   % Coauthor \\
%   % Affiliation \\
%   % Address \\
%   % \texttt{email} \\
%   % \AND
%   % Coauthor \\
%   % Affiliation \\
%   % Address \\
%   % \texttt{email} \\
%   % \And
%   % Coauthor \\
%   % Affiliation \\
%   % Address \\
%   % \texttt{email} \\
%   % \And
%   % Coauthor \\
%   % Affiliation \\
%   % Address \\
%   % \texttt{email} \\
% }

\author{Marine Le Morvan\textsuperscript{\textnormal{1,2}}\hspace{0.5em}
Julie Josse\textsuperscript{\textnormal{4}} \hspace{0.5em}
Erwan Scornet\textsuperscript{\textnormal{3}}\hspace{0.5em}
Gaël Varoquaux\textsuperscript{\textnormal{1}}
\AND
\\[-1.5em]
\textsuperscript{1}  Université Paris-Saclay, Inria, CEA, Palaiseau, 91120, France\\
\textsuperscript{2}  Université Paris-Saclay, CNRS/IN2P3, IJCLab, 91405 Orsay, France\\
\textsuperscript{3}  CMAP, UMR7641, Ecole Polytechnique, IP Paris, 91128 Palaiseau, France\\
\textsuperscript{4}  Inria Sophia-Antipolis, Montpellier, France\\
\AND
\\[-1.5em]
\texttt{\{marine.le-morvan, julie.josse, gael.varoquaux\}@inria.fr}\\
\texttt{erwan.scornet@polytechnique.edu}
}

\begin{document}

\maketitle

\begin{abstract}
    How to learn a good predictor on data with missing values? Most
    efforts focus on first imputing as well as possible
    and second learning on the completed data to predict the outcome.
    Yet, this widespread practice has no theoretical grounding.
    Here we show that for almost all imputation functions, an
    impute-then-regress procedure with a powerful learner is Bayes optimal. This result holds for all missing-values mechanisms, in
    contrast with the classic statistical results that require
    missing-at-random settings to use imputation in probabilistic
    modeling.
    Moreover, it implies that perfect conditional imputation is not needed for good prediction asymptotically. In fact, we show that on perfectly imputed data the best regression function will generally be discontinuous, which makes it hard to learn.
    Crafting instead the
    imputation so as to leave the regression function unchanged
    simply shifts the problem to learning discontinuous imputations.
    Rather, we suggest that it is easier to learn imputation and
    regression jointly. We propose such a procedure, adapting NeuMiss, a
    neural network capturing the conditional links across observed and
    unobserved variables whatever the missing-value pattern.
    Experiments confirm
    that joint imputation and regression through NeuMiss is better than various
    two step procedures in our experiments with finite number of samples. 
\end{abstract}


\section{Introduction}

Data with missing values are ubiquitous in many applications, as in health or business: some observations come with missing features. There is a rich statistical literature on imputation as well as inference with missing values \citep{rubin1976inference,little2002statistical}. Most of the theory and practices build upon the \emph{Missing At Random} (MAR) assumption that allows to maximize the likelihood of observed data while ignoring the missing-values mechanism, for instance using expectation maximization \citep{dempster1977maximum}. On the contrary, Missing Not At Random settings, where missingness depends on the unobserved values, may not be identifiable and require dedicated methods with a model of the missing-values mechanism.



Learning predictive models with missing values poses distinct challenges compared to inference tasks \citep{josse2019consistency}. Indeed, when the input is an arbitrary subset of variables in dimension $d$, there are $2^d$ potential missing data patterns and as many sub-models to learn. Consequently, even simple data-generating mechanisms lead to complex decision rules \citep{LeMorvan2020Linear}. To date, there are few supervised-learning models natively suited for partially-observed data. A notable exception is found with tree-based models \citep{Twala2008_mia,chen2016xgboost}, widely used in data-science practice. 

The most common practice however remains by far to use off-the-shelf methods first for imputation of missing values and second for supervised-learning on the resulting completed data. Such a procedure may benefit from progress in missing-value imputation with machine learning [\citealt{VanBuuren2018Flexible}, \mbox{\citealt{yoon2018gain}}, \citealt{Mattei2019MIWAE}]. 
However, there is a lack of learning theory to support such Impute-then-Regress procedures: Under what conditions are they Bayes consistent? Which aspects of the imputation are important?

There is empirical realization that the choice of imputation matters for predictive performance. The NADIA R package \citep{nadia_R} can select an imputation method to minimize a prediction error on a test set.
Auto-ML is used to optimize full pipelines, including imputation \cite[eg][]{jarrett2021clairvoyance}.
\citet{ipsen2020deal} optimize a constant imputation for supervised learning. However, the imputation is only weakly guided by the target in these approaches, it is set either from a family of black-box methods using gradient-free model selection, or from trivial imputation functions. In addition, there is a lack of insight on what drives a good imputation for prediction.

We contribute a systematic analysis of Impute-the-Regress procedures in a general setting: non-linear response function and any missingness mechanism (no MAR assumptions).
We show that:
\begin{itemize}[leftmargin=1.5em, itemsep=.5ex, topsep=-.5ex, parsep=.5ex, partopsep=-.5ex]
    \item Impute-then-Regress procedures are Bayes optimal for \emph{all missing data mechanisms} and for \emph{almost all imputation functions}, whatever the number of variables that may be missing. This very general result gives theoretical grounding to such widespread procedures.
    
    \item We study ``natural'' choices of imputation and regression functions: the oracle imputation by the conditional expectation and oracle regression function on the complete data. We show that chaining these oracles is not Bayes optimal in general and quantify its excess risk. We show that in both cases, choosing an oracle for one step, imputation or regression, imposes discontinuities on the other step, thus making it harder to learn.
    
    \item As these results suggest that imputation and regression should be adapted to one another, we contribute a method that jointly optimizes imputation and regression, using NeuMiss networks \citep{LeMorvan2020NeuMiss} as a differentiable imputation procedure.
    
    \item We compare empirically a number of Impute-then-Regress procedures on simulated non-linear regression tasks. Joint optimization of both steps provides the best performance.

\end{itemize}

\section{Problem setting}
% $\Dcal_n = \cbr{(\bm x^{(i)}, \bm m^{(i)}, y^{(i)})}_{i=1..n}$
\paragraph{Notations} We consider a dataset of i.i.d.\ realizations of the random variable $(X, M, Y) \in \RR^d \times \cbr{0, 1}^d \times \RR$ where $X$ are the complete covariates, $M$ a missingness indicator, and $Y$ a response of interest. For each realization $(x, m, y)$, $m_j=1$ indicates that $x_j$ is missing, and $m_j=0$ that it is observed. We denote by $mis(m) \subset \bbr{1, d}$ the indices corresponding to the missing covariates (and similarly $obs(m)$ the observed indices), so that $x_{obs(m)}$ corresponds to the entries actually observed. We define the incomplete covariate vector $\widetilde{X} \in \br{\RR \cup \br{\NA}}^d$ as $\widetilde{X}_j = X_j$ if $M_j = 0$ and $\widetilde{X}_j = \NA$ otherwise, where $\NA$ represents a missing value.

\paragraph{Assumptions} We assume that $X$ admits a density on $\RR^d$ and that, for all $j \in \bbr{1, d},$ each component $X_j$ has finite expectation and variance, that is $ \E\sqb{X_j^2}<\infty$.  Moreover, we assume that the response $Y$ is generated according to:
\begin{equation}
    \label{eq:generation}
    Y = f^\star(X) + \epsilon, \qquad \textrm{with}~~\E\sqb{\epsilon|X_{obs(M)}, M} = 0 \quad \textrm{and}~~\E\sqb{Y^2} < \infty.
\end{equation}%
where $f^\star: \RR^d \to \RR$ is a function of the complete input data $X$, $\epsilon \in \RR$ is a random noise variable.

\subsection{Supervised learning with missing values}
\paragraph{Optimization problem} In practice, in the presence of missing values, we do not have access to the complete data $(X, M, Y)$ but only to the subset of it that is observed, i.e, $(X_{obs(M)}, M, Y)$. Thus instead of learning a mapping from $\RR^d$ to $\RR$, we need to learn a mapping from $\br{\RR \cup \br{\NA}}^d$ to $\RR$, where the set of observed covariates can be any subset of $\bbr{1, d}$. It is this unusual input space, partly discrete, that makes supervised learning with missing values challenging and different from classical supervised learning problems. Formally, the optimization problem we wish to solve is:
\begin{equation}
    \label{eq:minimization_pb}
    \min_{f: \br{\RR \cup \br{\NA}}^d \mapsto \RR} \Rcal(f):= \E\sqb{\br{Y - f(\widetilde X)}^2}
\end{equation}


\paragraph{Bayes predictor} The function which minimizes \eqref{eq:minimization_pb}, called the \emph{Bayes predictor}, is given by:
\begin{equation}
    \tilde{f}^{\star}(\widetilde X) = \E \sqb{Y | X_{obs(M)}, M}
     = \E \sqb{f^\star(X) | X_{obs(M)}, M}. \label{eq:BP}
\end{equation}


As $\widetilde X$ is a function of $X_{obs}$ and $M$, we will sometimes slightly abuse notations and write $\tilde f^\star(\widetilde X) = \tilde f^\star(X_{obs}, M)$. The risk of the Bayes predictor is called the \emph{Bayes risk}, which we denote as $\Rcal^{\star}$. It is the lowest achievable risk for a given supervised learning problem.

\begin{definition}[Bayes optimality]
A \emph{Bayes optimal} function $f$ achieves the Bayes rate, i.e, $\Rcal(f) = \Rcal^{\star}$.
\end{definition}




As can be seen from \eqref{eq:BP}, the Bayes predictor is a function of $M$, a discrete random variable that can take one of $2^d$ values since $M \in \cbr{0, 1}^d$. The function  $\tilde{f}^{\star}$ can thus be viewed as $2^d$ different functions, one for each possible subset of variables. This view raises questions that are central to this paper: How should we parametrize functions on such input domains? And which function families should we consider to approximate $\tilde{f}^{\star}$? These questions have been studied in the case where $f^\star$ is assumed to be a linear function, and $X$ follows a Gaussian distribution. Indeed, under these assumptions, \citet{LeMorvan2020Linear,LeMorvan2020NeuMiss} have derived analytical expressions for the Bayes predictor and deduced appropriate parametric estimators. However, aside from specific cases, it is impossible to derive an analytical expression for the Bayes predictor, and novel arguments are needed to understand which classes of functions should be considered in general.

\section{Asymptotic analysis of Impute-then-regress procedures}

\subsection{Impute-then-regress procedures}
Let $|mis(m)|$ (resp. $|obs(m)|$) be the number of missing entries (resp. observed) for any missing data pattern $m$. For each $m \in \{0,1\}^d$, we define an \emph{imputation function} $\phi^{(m)}: \RR^{|obs(m)|} \to \RR^{|mis(m)|}$ which outputs values for the missing entries based on the observed ones. We denote by $\phi_j^{(m)}: \RR^{|obs(m)|} \to \RR$ the component function of $\phi^{(m)}$ that imputes the $j$-th component in $X$ if it is missing. Classical choices of imputation functions include constant functions or linear functions. Finally, we introduce the family of functions $\Fcal^I$ that transform an incomplete vector into a complete one, precisely:
\begin{equation}
    \label{eq:imputation}
    \Fcal^I = \cbr{\Phi: \br{\RR \cup \cbr{\NA}}^d \to \RR^d: \forall j \in \bbr{1, d},\: \Phi_j(\widetilde X) =
    \begin{cases}
        X_j \: &\text{if} \: M_j = 0\\
        \phi^{(M)}_j(X_{obs(M)}) \: &\text{if} \: M_j = 1
    \end{cases}
    }.
\end{equation}
Let us define $\Fcal^I_{\infty}$ in the exact same way but for imputation functions $\phi^{(m)} \in \Ccal^{\infty}$, for all $m \in \{0,1\}^d$. Here we study \emph{Impute-then-regress procedures}, which we define as two-step procedures where the data is first imputed using a function $\Phi \in \Fcal^I$, and then a regression is performed on the imputed data. Such a procedure is quite natural to deal with arbitrary subsets of inputs variables. It embeds the data into $\RR^d$ to reduce the problem to a classical one. In practice, impute-then-regress procedures are widely used. However, the choice of function class is so far mostly ad-hoc and raises multiple questions: How close to the Bayes rate can functions obtained via such procedures be? Should we prefer some choices of imputation functions over others? What happens when the missing data mechanism is missing not at random? In this section, we will give answers to these questions.

Below, we write $obs$ (resp.~$mis$) instead of $obs(M)$ (resp.~$mis(M)$) to lighten notations.

\subsection{Impute-then-regress procedures are Bayes optimal}

\begin{definition}[Universal consistency]
    An estimator $f_n$ is \emph{Bayes consistent} if $\lim_{n \to \infty}\Rcal(f_n) =  \Rcal^{\star}$. It is said to be \emph{universally consistent} if the previous statement holds for all distributions of $(X,Y)$.
\end{definition}

The following theorem shows that Impute-then-regress procedures are Bayes optimal for almost all imputation functions. In other words, it means that a universal learner trained on imputed data provides optimal performances asymptotically for almost all imputation functions. Let us now define, for all imputation functions $\Phi \in \Fcal^I$, the function $g^\star_{\Phi} \in \underset{g: \RR^d \mapsto \RR}{\text{argmin}} \quad \E\sqb{\br{Y-g \circ \Phi(\widetilde X)}^2}$.

\begin{restatable}[Bayes consistency of Impute-then-regress procedures]{theorem}{fbp}
\label{th:fbp}
    Assume the data is generated according to \eqref{eq:generation}.
    Then, for almost all imputation function $\Phi \in \Fcal^I_{\infty}$, the function $g^\star_{\Phi} \circ \Phi$ is Bayes optimal.
    %Let $\Phi \in \Fcal^I$ and infinitely differentiable. 
    In other words, for almost all imputation functions $\Phi\in \Fcal^I_{\infty}$, a universally consistent algorithm trained on the imputed data $\Phi(\widetilde X)$ is Bayes consistent.
    
\end{restatable}

Appendix~\ref{ss:bayes_optimality} gives the proof. Theorem~\ref{th:fbp} states a very general result: Impute-then-regress procedures are Bayes consistent for all missing data mechanisms, almost all imputation functions, regardless of the distribution of $(X,Y)$ and the number of missing covariates. Since Theorem~\ref{th:fbp} holds for almost all imputation functions, it implies that good imputations are not required to obtain good predictive performances, at least asymptotically. Note that here, the notion of \emph{almost all} is to be understood in its topological sense, and not in its measure theory sense. Moreover, this theorem does not make any assumption on the missing data mechanism, and is therefore valid for Missing Not At Random (MNAR) data. This contrasts with most methods for inference and imputation with missing values, valid only for MAR data. Finally, the theorem remains valid for any configuration of variables that may contain missing values, including the case in which all variables may contain missing values. 
Bayes consistency of Impute-the-Regress procedures has already been
studied, but in much more restricted settings.
\citet{josse2019consistency} proved that such procedures are Bayes consistent under the MAR assumption, for constant imputations functions and for only one potentially missing variable. 
\citet{Bertsimas2021Prediction} refined this result to almost surely continuous imputation functions. While these two prior works build on very similar proofs, we use here very different arguments summarized in the next paragraph.

\begin{figure}[b]
    \begin{minipage}{.52\linewidth}
    \caption{\textbf{Example - Imputation manifolds in three dimensions} --- 3-dimensional Gaussian data after imputation. Data points are colored according to their missing data pattern prior to imputation. Red, brown and purple (resp. orange, blue, and green) correspond to missing data patterns with two (resp. one) missing value(s). Completely observed points are not represented to ease the visualization of manifolds.}
    \label{fig:example_manifolds}
    \end{minipage}%
    \hspace{2em}
    \begin{minipage}{.46\linewidth}
        \includegraphics[scale=0.54]{manifolds_corrected_points.png}
    \end{minipage}
\end{figure}

The first key idea of the proof is that, after imputation, all data points with a given missing data pattern $m$ are mapped to a manifold $\Mcal^{(m)}$ of dimension $|obs(m)|$. For example in 3D, data points are mapped to $\RR^3$ when completely observed, to 2D manifolds when they have one value missing, to 1D manifolds when they have two values missing, and to one point when all values are missing (see Figure\,\ref{fig:example_manifolds}). Thus, Impute-then-Regress procedures first map data points to various manifolds depending on their missing data patterns and then apply a prediction function defined on the whole space including manifolds. The second key idea of the proof is to ensure that the original missing data patterns of imputed points can almost surely be identified. For this, the proof requires that all manifolds of the same dimension are pairwise transverse. This assumption is sufficient, though not necessary, to ensure that the intersection of two manifolds of dimension $|obs(m)|$ cannot itself be of dimension $|obs(m)|$. Transversality is a weak assumption. In fact, Thom's transversality theorem, (which we rely on in our proof) says that it is a generic property: it holds for ``typical examples'', i.e \emph{almost all} imputation functions will lead to transverse manifolds. To clarify this concept, we provide a particular case in 2D where 1D manifolds are not transverse in Appendix~\ref{ss:2D_manifolds}.

The proof is constructive and exhibits a function $g^\star_{\Phi}$ which achieves the Bayes rate for a given set of imputation functions. For each manifold $\Mcal^{(m)}$, ordered from smallest dimension to largest, we require that $g^\star_{\Phi}$ on $\Mcal^{(m)}$ equals the Bayes predictor for missing data pattern $m$ except on points for which $g^\star_{\Phi}$ has already been defined, i.e, the points where $\Mcal^{(m)}$ intersects with the manifolds ranked before it. Thus, we obtain a function $g^\star_{\Phi}$ that does not depend on $m$, and which for each manifold, equals the Bayes predictor except on subsets of measure zero under the assumption that manifolds of the same dimension are pairwise transverse. Refer to appendix~\ref{ss:bayes_optimality} for more details.

While this theorem is a very general result, it does not say what the optimal function associated to a given imputation looks like. In fact, depending on the imputation function it may be non-continuous, vary widely, and require a very large number of samples to be approximated.

\paragraph{Note on Impute-then-classify procedures -} Theorem~\ref{th:fbp} applies to regression problems. However, it can easily be shown that a similar result holds in binary classification settings. Indeed, in a binary classification setting, the Bayes predictor predicts class 1 if $P(Y=1|X)>0.5$ and -1 otherwise. Thus, it suffices to consider that the function of interest $f^\star(X)$ is the posterior probability $P(Y=1|X)$. Then the same arguments as those used to prove Theorem~\ref{th:fbp} can be used to show that Impute-then-classify procedures are Bayes optimal for almost all imputation functions. 

% Then Theorem~\ref{th:fbp} (actually not exactly the theorem but the main arguments that we use to prove it) tells us that for almost all imputation functions, there exists prediction functions so that they are equal to $P(Y=1|X)$ except on a set of measure zero. As a result the Bayes predictor in the classification setting will also be Bayes optimal (because the 0-1 loss is also an expectation and so we do not care about the set of measure zero).


% \begin{figure}
%     \begin{minipage}{.7\linewidth}
%     \caption{\textbf{Example - Imputation manifolds in two dimensions} --- Manifolds represented for linear imputation functions. $\Mcal^{(0, 0)}$ is the whole plane. Note that $\Mcal^{(1, 1)}$ need not be at the intersection of the two lines, it depends on the imputation function chosen. Here, $\Mcal^{(0, 1)}$ and $\Mcal^{(1, 0)}$ are transverse if and only if the two lines are not coincident.}
%     \label{fig:example_manifolds}
%     \end{minipage}%
%     \hfill
%     \begin{minipage}{.28\linewidth}
%     \scalebox{.21}{\subfile{tikz_manifolds}}
%     \end{minipage}
% \end{figure}

\section{Imputation versus regression: choosing one may break the other}

Theorem~\ref{th:fbp} gives a theoretical grounding to Impute-then-regress procedures. As it holds for almost any imputation function, one could very well choose simple and cheap imputations such as imputing by a constant. However, the difficulty of the ensuing learning problem will depend on the choice of imputation function. Indeed, the function $g^\star_{\Phi}$ that achieves the Bayes rate depends on the imputation function $\Phi$. In general, it may not be continuous or smooth. Thus $g^\star_{\Phi}$ can be more or less difficult to approximate by machine learning algorithms depending on the chosen imputation function.

\citet{LeMorvan2020Linear} showed that even if $Y$ is a linear function of $X$, imputing by a constant leads to a complicated Bayes predictor: piecewise affine but with $2^d$ regions. This result highlights how imputations neglecting the structure of covariates can result in additional complexity for the regression function $g^\star_{\Phi}$. Rather, another common practice is to impute by the conditional expectation: it minimizes the mean squared error between the imputed matrix and the complete one and is the target of most imputation methods. One hope may be that if $f^\star$ has desirable properties, such as smoothness, conditional imputation will lead to a function $g^\star_{\Phi}$ which inherits from these properties.

In this section we first show that replacing missing values by their conditional expectation in the oracle regression function $f^\star$ gives a small but non-zero risk. Characterizing the optimal function on the conditionally-imputed data, we find that it suffers from discontinuities and thus forms a difficult estimation problem. Rather, we study whether the imputation can be corrected for $f^\star$ to form the Bayes predictor on partially-observed data.

\subsection{Applying $f^\star$ on conditional imputations: chaining oracles isn't without risks.}

The conditional imputation function $\Phi^{CI}: \br{\RR \cup \cbr{\NA}}^d \to \RR^d$ is defined as follows:
\begin{equation*}
     \forall j \in \bbr{1, d},\: \Phi^{CI}_j(\widetilde X) =
    \begin{cases}
        X_j \: &\text{if} \: M_j = 0\\
        \E \sqb{X_j | X_{obs}, M} \: &\text{if} \: M_j = 1
    \end{cases}
\end{equation*}
Note that $\Phi^{CI} \in \Fcal^I$. To lighten notations, we will write $X^{CI}:= \Phi^{CI}(\widetilde X)$ to denote the conditionally imputed data.




\begin{restatable}[First order approximation]{lemma}{fimpth}
\label{le:fimp}
    Assume that the data is generated according to \eqref{eq:generation}. Moreover assume that (i) $f^\star \in \Ccal^2(\Scal, \RR)$ where $\Scal \subset \RR^d$ is the support of the data, and that (ii) there exists positive semidefnite matrices $\bar{H}^+ \in P_d^+$ and $\bar{H}^- \in P_d^+$ such that for all $X$ in $\Scal$, $\bar{H}^- \preccurlyeq H(X) \preccurlyeq \bar{H}^+$ with $H(X)$ the Hessian of $f^\star$ at $X$. Then for all $X$ in $\Scal$ and for all missing data patterns:
    \begin{equation}
        \frac{1}{2}\text{tr}\left(\bar{H}^-_{mis, mis} \Sigma_{mis|obs, M}\right) \leq \tilde f^\star( \widetilde X) - f^\star(X^{CI}) \leq \frac{1}{2}\text{tr}\left(\bar{H}^+_{mis, mis} \Sigma_{mis|obs, M}\right)
    \end{equation}
    
    where $\Sigma_{mis|obs, M}$ is the covariance matrix of the distribution of $X_{mis}$ given $X_{obs}$ and $M$.
\end{restatable}

Appendix \ref{ss:proof_fimp} gives the proof. The assumption that $\bar{H}^- \preccurlyeq H(X) \preccurlyeq \bar{H}^+$ for any $X$ means that the minimum and maximum curvatures of $f^\star$ in any direction are uniformly bounded over the entire space. Lemma~\ref{le:fimp} shows that applying $f^\star$ to the conditionally imputed (CI) data is a good approximation of the Bayes predictor when there is no direction in which both the curvature of $f^\star$ and the conditional variance of the missing data given the observed one are high. Intuitively, if a low quality imputation is compensated by a flat function, or conversely, if a fast varying function is compensated by a high quality imputation, then $f^\star$ applied to the CI data approximates well the Bayes predictor.


\begin{restatable}[(Non-)Consistency of chaining oracles]{proposition}{propchain}
\label{prop:chain}
    The function $f^\star \circ \Phi^{CI}$ is Bayes optimal if and only if the function $f^{\star}$ and the imputed data $X^{CI}$ satisfy:
    \begin{equation}
        \label{eq:bo_cond}
        \forall M \; \text{s.t.} \; P(M)>0, \quad \E\sqb{f^\star(X)|X_{obs}, M} = f^\star(X^{CI}) \quad \text{almost everywhere}.
    \end{equation}
    Besides, 
    %Moreover, if $f^\star(X^{CI})$ is not Bayes optimal, then 
    under the assumptions of Lemma~\ref{le:fimp}, the excess risk of chaining oracles compared to the Bayes risk $\Rcal^\star$ is upper-bounded by:
    \begin{equation}
    \Rcal(f^\star \circ \Phi^{CI}) - \Rcal^\star \leq \frac{1}{4} \E_M \sqb{ \max \br{tr \br{\bar{H}^-_{mis, mis} \Sigma_{mis|obs, M}}^2, tr\br{\bar{H}^+_{mis, mis} \Sigma_{mis|obs, M}}^2}}
    \label{eq:excess_risk}
    \end{equation}
\end{restatable}



Appendix \ref{ss:proof_prop_chain} gives the proof. 
Condition~\eqref{eq:bo_cond} for Bayes optimality is clearly stringent.
Indeed, if one variable is missing, condition~\eqref{eq:bo_cond} says that the expectation of the regression function should be equal to the regression function applied at the expected entry. Although such functions are difficult to characterize precisely, it is clear that condition~\eqref{eq:bo_cond} is difficult to fulfill for generic regression functions (linear functions are among the few examples that do satisfy it). Therefore, for most functions $f^\star$, $f^\star \circ \Phi^{CI}$ is not Bayes optimal. Proposition~\ref{prop:chain} also gives an upper bound for the excess risk of the predictor $f^\star(X^{CI})$ compared to the Bayes rate, showing here again that if there is no direction in which both the curvature and the variance of the missing data given the observed one are high, the excess risk is small.

\emph{The special case of linear regression:} When $f^\star$ is a linear function, the curvature is 0, hence eq.\,(\ref{eq:excess_risk}) implies no excess risk. This is also visible from the expression of the Bayes predictor \eqref{eq:BP}, where the expectation on unobserved data can be pushed inside $f^\star$ as it is linear. The Bayes predictor can thus be exactly written as $f^\star$ applied to conditionally-imputed data.

\subsection{Regressing on conditional imputations, a good idea?}

\begin{restatable}[Regression function discontinuities]{proposition}{propdisc}
\label{prop:disc}
    Suppose that $f^\star \circ \Phi^{CI}$ is not Bayes optimal, and that the probability of observing all variables is strictly positive, i.e., for all $x$, $P(M=(0, \dots , 0), X=x)>0$. Then there is no \underline{continuous} function $g$ such that $g \circ \Phi^{CI}$ is Bayes optimal.
\end{restatable}
In other words, when conditional imputation is used, the optimal regression function experiences discontinuities unless it is $f^\star$. The proof is given in appendix \ref{ss:proof_prop_disc}. From a finite-sample learning standpoint, discontinuous functions are in general harder to learn: in the general case, non-parametric regression requires more samples to achieve a given error on functions without specific regularities as opposed to functions with a form of smoothness
\citep[see e.g.,][chap 3]{gyorfi2006distribution}. Hence, while regression on conditional imputation may be consistent (\autoref{th:fbp}), it can require an inordinate number of samples.


\subsection{Fasten your seat belt: corrected imputations may experience discontinuities.}


\begin{figure}[t]
    \includegraphics[width=.4\linewidth, clip, trim={1.9cm 1.9cm .2cm 2.6cm}]{corrected_imputation_bowl.png}%
    \llap{\raisebox{.3\linewidth}{\rlap{\sffamily Bowl}\hspace*{.4\linewidth}}}
        \hfill%
    \includegraphics[width=.4\linewidth, clip, trim={1.9cm 1.9cm .2cm 2.6cm}]{corrected_imputation_wave.png}%
    \llap{\raisebox{.3\linewidth}{\rlap{\sffamily Wave}\hspace*{.4\linewidth}}}%
    \caption{\textbf{Left: corrected imputation} The regression function is $f^\star(x_1, x_2) \mapsto x_1^2 + x_2^2$. When $x_2$ is missing, chaining perfect conditional imputation with the regression function ($f^\star \circ \Phi^{CI}$) gives a biased predictor, shown in red, as the unexplained variance in $x_2$ is turned into bias. However, using as an imputation $\Phi(x_1) = \sqrt{\rho^2 x_1^2 + (1-\rho^2)}$ corrects this bias, with $\rho$ the correlation between $x_1$ and $x_2$.
    \textbf{Right: no continuous corrected imputation exists}. The
function is defined as $f^\star(x_1, x_2) \mapsto x_2^2 -
3\,x_2$. No continuous corrected imputation is possible because the Bayes
predictor on the partially-observed data $\mathbb{E}[Y|X_1]$ is
monotonous, while the regression function $f^\star$ is not.}
    \label{fig:2d_examples}
\end{figure}


Another possible route is to find \emph{corrected imputations} which we
define as imputation functions $\Phi$ such that, if $f^\star$ is used as
regression function, the impute-then-regress procedure $f^\star \circ
\Phi$ is Bayes optimal. Intuitively, given a
fixed regression function $f^\star$, the question is: can we "correct" an
imputation function and thus the manifold that it describes so that
$f^\star$ restricted to this manifold is equal to the Bayes
predictor?
Assuming $f^\star$ is continuous, the intermediate value theorem gives a first answer to this question by ensuring the existence of imputations functions satisfying 
\begin{align*}
f^\star \circ
\Phi (X_{obs(M)}, M) = \E \sqb{f^\star(X) | X_{obs(M)}, M}.
\end{align*}
For the same reasons as above, determining that such imputations not only exist but are \emph{continuous} is important from a practical perspective.  Indeed, assuming $f^\star$ is continuous, the Bayes predictor with missing values could then be tackled as the composition of two continuous functions, with an Impute-then-Regress strategy. Intuitively in 2D, the existence of a continuous corrected imputation can be seen as the existence of a continuous path in the 2D plane whose value by $f^\star$ equals the Bayes predictor. Figure~\ref{fig:2d_examples} (left) gives a simple example in 2D for which a continuous corrected imputation exists. Here if one chooses the imputation function of $X_2$ given $X_1$ as the black function denoted as $\Phi_{corrected}$, then its composition by the green paraboloid $f^\star$ gives the Bayes Predictor depicted on the right in black. By contrast, if one imputes by the conditional expectation, then its composition with $f^\star$ gives the red curve which is different from the Bayes Predictor. Note that the manifolds in Figure~\ref{fig:example_manifolds} were obtained using (continuous) corrected imputations functions for the same setting as Figure~\ref{fig:2d_examples} (left) but with 3-dimensional data. However, as illustrated in Figure~\ref{fig:2d_examples} (right), \emph{continuous} corrected imputations do not always exist. Indeed, on this example the Bayes predictor is non-decreasing but there is no continuous path in the 2D plane on which $f^\star$ is non-decreasing and maps at some point to both the 'purple' and 'yellow values' (proof in Appendix~\ref{ss:no_cc}). It is thus of interest to clarify when continuous corrected imputations exist. Proposition~\ref{prop:cont_imp} establishes such conditions.


\begin{restatable}[Existence of continuous corrected imputations]{proposition}{prop:cont_imp}
    \label{prop:cont_imp}
        Assume that $f^{\star}$ is uniformly continuous,  twice continuously differentiable and that, for all missing patterns $m$ and all $x_{obs}$, the support of $X_{mis} | X_{obs}=x_{obs}, M=m$ is connected. 
    Additionally, assume that for all missing patterns $m$, and all $(x_{obs}, x_{mis})$, the gradient of $f^{\star}$ with respect to the missing coordinates is nonzero:
    \begin{equation}
        \nabla_{x_{mis}} f^{\star} (x_{obs}, x_{mis}) \neq 0. \label{eq:regularity_assumption}
    \end{equation}
    Then, for all $m$, theres exist continuous imputation functions $\phi^{(m)} : \RR^{|obs(m)|} \to \RR^{|mis(m)|}$ such that $f^{\star} \circ \Phi$ is Bayes optimal. 
\end{restatable}

Appendix~\ref{app:continuous_imputations} gives a proof based on a global implicit function theorem. Assumption~\ref{eq:regularity_assumption} is restrictive: it is for instance not met for our example in Figure~\ref{fig:2d_examples} (left), which still admits continuous corrected imputations. This highlights the fact that continuous corrected imputations also exist under weaker conditions, but it is difficult to conclude on ``how often'' it is the case.

\section{Jointly optimizing an impute-n-regress procedure: NeuMiss+MLP}

The above suggests that it is beneficial to adapt the regression function to the imputation procedure and vice versa. Hence, we introduce a method for the joint optimization of these two steps by chaining a NeuMiss network with an MLP (multi-layer perceptron).

% NeuMiss \citep{LeMorvan2020NeuMiss} is a neural-network architecture originally designed to approximatethe Bayes predictor for a linear model with missing values.  It contains a theoretically-groundedNeumann block that can efficiently approximate the conditional expectation of the missing valuesgiven the observed ones.  Here, we reuse the Neumann block as it outputs a learned imputation:each observed coordinate is mapped to its observed value and each missing coordinate is mappedto a function of the observed ones. The whole architecture can be seen as an Impute-then-Regressarchitecture, but that can be jointly optimized.

NeuMiss \citep{LeMorvan2020NeuMiss} is a neural-network architecture originally designed to approximate the Bayes predictor for linear models with missing values. It contains a Neumann block that has the particularity of using element-wise multiplications by the missingness indicator as non-linearities. %Here we reuse this block, chained with an MLP, to create a new architecture suited for missing values.
Here we reuse this block to play the role of an imputation layer. This choice is motivated by two key reasons. First, the Neumann block is a theoretically grounded layer for missing values: it can approximate the conditional expectation of the missing values given the observed ones with an error that decays exponentially fast with its depth. As explained in Prop. 4.1, this property is desirable in some cases. Second, it is a differentiable block, which allows it to be chained with a MLP and learned jointly with the regression function. The resulting architecture can thus be seen as an Impute-then-Regress architecture, but that can be jointly optimized.

We performed one minor improvement on the NeuMiss architecture compared to the original paper. Though the theory behind NeuMiss points to using shared weights in the Neumann block as well as residual connections going from the input to each hidden layer of the Neumann block, \citet{LeMorvan2020NeuMiss} used neither. We found empirically that shared weights in the Neumann block as well as residual connections improved performance. Therefore, we used both in all our experiments. %
%Moreover, while \citet{LeMorvan2020NeuMiss} initialized the weights of the Neumann block randomly, we chose to initialize them with sample estimates of quantities that should be targeted to perform well according to \citet{LeMorvan2020NeuMiss}.
For clarity, the (non-linear) NeuMiss architecture %
% and its initialization strategy are
is described in detail in Appendix~\ref{ss:NeuMiss_archi}.

\section{Empirical study of impute-n-regress procedures}

\subsection{Experimental setup}\label{sec:experimental_setup}

\paragraph{Data generation}
The data $X \in \RR^{n \times d}$ are generated according to a multivariate Gaussian distribution $\Ncal(\mu, \Sigma)$ where the mean is drawn from a standard Gaussian and the covariance is generated as $\Sigma = B B^\top + D$. $B \in \RR^{d \times q}$ is a matrix with entries drawn from a standard normal Gaussian distribution, and $D$ is a diagonal matrix with small entries that ensures that the covariance matrix is full rank. We study two correlation settings called \emph{high} and \emph{low} corresponding respectively to $q=\texttt{int}(0.3*d)$ and $q=\texttt{int}(0.7*d)$. The experiments are run with $d=50$.

\paragraph{Choice of $f^\star$} The response $Y$ is generated according to $Y = f^\star(X) + \epsilon$ with three choices of $f^\star$ named \emph{bowl}, \emph{wave}, and \emph{break} depicted in Figure~\ref{fig:f_star} (exact expression in appendix \ref{ss:f_star}). $\beta$ is a vector of ones normalized such that the quantity $z = \beta^\top X + \beta_0$ follows a Gaussian distribution centered on 1 with variance 1. Note that $f^\star_{bowl}$, $f^\star_{wave}$ and $f^\star_{break}$ were designed so that the desired variations occur over the support of the data. The noise $\epsilon$ is chosen so as to have a signal-to-noise ratio of 10.
\begin{figure}[h]
    \begin{minipage}{.32\linewidth}
	\caption{\textbf{Bowl, wave and break functions} used for $f^\star$ in the empirical study.}
	\label{fig:f_star}
    \end{minipage}%
    \hfill%
    \begin{minipage}{.18\linewidth}
	\includegraphics[width=\linewidth]{f_star_bowl.pdf}
    \end{minipage}%
    \hfill%
    \begin{minipage}{.18\linewidth}
	\includegraphics[width=\linewidth]{f_star_wave.pdf}
    \end{minipage}%
    \hfill%
    \begin{minipage}{.18\linewidth}
	\includegraphics[width=\linewidth]{f_star_break.pdf}
    \end{minipage}%
\end{figure}

\vspace{-0.5em}

\paragraph{Missing values} 50\% of the entries of $X$ were deleted according to one of two missing data mechanisms: Missing Completely At Random (MCAR) or Gaussian self-masking \citep[GSM, see ][]{LeMorvan2020NeuMiss}. Gaussian self-masking is a Missing Not At Random (MNAR) mechanism, where the probability that a variable $j$ is missing depends on $X_j$ via a Gaussian function.

\paragraph{Baseline methods benchmarked}
For each level of correlation (\emph{low} or \emph{high}), for each function $f^\star$ (\emph{bowl}, \emph{wave} or \emph{break}), and each missing data mechanism (MCAR or GSM), we compare a number of methods. First, for reference, we compute various oracle predictors:
\begin{itemize}[itemsep=0ex, topsep=0ex, partopsep=0.5ex, parsep=0.5ex, leftmargin=2.5ex]
    \item \textbf{Bayes predictor}: This is the function that achieves the lowest achievable risk. In general cases, its expression cannot be derived analytically. However, we show that it can be derived for ridge functions, i.e. functions of the form $x \mapsto g(\beta^\top x)$, for some choices of $g$ including polynomials, the Gaussian cdf and piecewise constant functions. We thus built $f^\star_{bowl}$, $f^\star_{wave}$ and $f_{break}$ as combination of these base functions which allows us to compute their corresponding Bayes predictors.  Appendix~\ref{ss:f_star} gives their expressions.
    \item \textbf{Chained oracles}: $f^\star \circ \Phi^{CI}$ consists in imputing by the conditional expectation and then applying $f^\star$. The analytical expression of $\Phi^{CI}$ can be derived analytically for both MCAR and GSM, and we thus use this analytical expression to impute the missing values.
    \item \textbf{Oracle + MLP}: The data is imputed using the analytical expression of the conditional expectation, and then a MLP is fitted to the completed data.
\end{itemize}
These three predictors all use ground truth information (parameters $\mu$, $\Sigma$ of the data distribution, expression of $f^\star$ or of the missing data mechanism) which are unavailable in practice. They are mainly useful as reference points. We then compare the NeurMiss+MLP architecture and a number of classic Impute-then-Regress methods as well as gradient boosted regression trees:
\begin{itemize}[itemsep=0ex, topsep=0ex, partopsep=0.5ex, parsep=0.5ex, leftmargin=2.5ex]
    \item \textbf{Mean + MLP} The data is imputed by the mean, and a multilayer perceptron (MLP) is fitted to the completed data.
    \item \textbf{MICE + MLP} The data is imputed using Scikit-learn's \citep[BSD licensed]{Pedregosa2011Scikit} conditional imputer \texttt{IterativeImputer} that adapts the popular Multivariate Imputation by Chained Equations  \citep[MICE,][]{VanBuuren2018Flexible} to be able to impute a test set. A multilayer perceptron (MLP) is then fitted to the completed data.

    \item \textbf{GBRT}: Gradient boosted regression trees  (Scikit-learn's \texttt{HistGradientBoostingRegressor} with default parameters). This predictor readily supports missing values: during training, missing values on the decision variable for a given split are sent to the left or right child depending on which provides the largest gain. This is know as the Missing Incorporated Attribute strategy \citep{Twala2008_mia}.
\end{itemize}
Finally, we also run \textbf{Mean + mask + MLP} as well as \textbf{MICE + mask + MLP} in which the mask is concatenated to the imputed data before fitting a MLP. Concatenating the mask is a widespread pratice to account for MNAR data.


All MLPs are implemented with PyTorch \citep{paszke2019pytorch}. A validation set is used to choose MLPs' depth (1, 2 or 5), width ($1d$, $5d$ or $10d$), initial learning rate (ranging from $5.10^{-4}$ to $10^{-2}$) and weight decay (ranging from $10^{-6}$ to $10^{-3}$). Adam is used with an adaptive learning rate: the learning rate is divided by 5 each time 10 consecutive epochs fail to decrease the training loss by at least 1e-4. Early stopping is triggered when the validation score does not improve by at least 1e-4 for 12 consecutive epochs. The batch size is set to 100, and ReLUs are used as activation functions. Finally for NeuMiss the depth is set to 20. Note that since the weights of NeuMiss are shared, increasing its depth does not increase its number of parameters. %
For gradient boosted regression trees, several hyperparameters are chosen using the validation set including the maximum number of leaves for each tree (from 50 to 600), the maximum number of iterations for the boosting process (from 100 to 300), as well as the minimum number of samples per leaf (from 10 to 50). 

The experiments use training sets of size $n=100\,000$ and validation and test sets of size $n=10\,000$. The code for all experiments is available at \url{https://github.com/marineLM/Impute\_then\_Regress}.

\subsection{Experimental results}

The results are presented in Figure~\ref{fig:boxplots} as well as in Figure~\ref{fig:boxplots_discontinuous_linear} (Appendix~\ref{ss:boxplots_discontinuous_linear}).

\begin{figure}[b!]
    \includegraphics[width=\linewidth]{boxplots_neurips2021.pdf}
    \caption{Performances (R2 score on a test set) compared to that of the Bayes predictor across 10 repeated experiments.}
    \label{fig:boxplots}
\end{figure}

\paragraph{Chaining oracles fails when both curvature is high and correlation is low.} The chained oracle has a performance close to that of the Bayes predictor in all cases except when the wave or break functions are applied to low correlation data. This observation illustrates well Proposition~\ref{prop:chain}. Intuitively, the Bayes predictor for each missing data pattern is a smoothed version of $f^\star$, and it is all the more smoothed that there is uncertainty around the likely values of the missing data. In the low correlation setting, the uncertainty is such that $f^\star$ is not a good proxy anymore for the Bayes predictor.

\paragraph{Regressing on oracle conditional imputation provide excellent performances.} Contrary to the chained oracles, \emph{Oracle + MLP} is close to the Bayes rate in all cases. This result should be put into perspective with Proposition~\ref{prop:disc}, which states that there is no \emph{continuous} regression function $g$ such that $g \circ \Phi^{CI}$ is Bayes optimal unless it is $f^\star$. Indeed, as the MLP can only learn continuous functions, it shows that there are continuous functions $g$ such that $g \circ \Phi^{CI}$, even though it is not Bayes optimal, performs very well.

\paragraph{Adding the mask is critical in MNAR settings with \emph{mean} and \emph{MICE} imputations} In MNAR settings, missingness carries information that can be useful for prediction. However, both the mean and iterative conditional imputation output an imputed dataset in which the missingness information is more difficult to retrieve. For this reason, it is common practice to concatenate the mask with the imputed data to expose the missingness information to the predictor. Our experiments show that under self-masking (MNAR), adding the mask to the mean or iteratively imputed data markedly improves performances. Note that NeuMiss does not require adding the mask as an input since the missingness information is already incorporated via the non-linearities.

\paragraph{NeuMiss+MLP performs best among Impute-then-Regress predictors.} In \emph{all}
settings, NeuMiss performs best. GBRT performs poorly here possibly because they are not well adapted to approximate smooth functions. Finally, note that when the difficulty of the problem increases, for example with a lower correlation, then (i) the performance of the Bayes predictor decreases and (ii) the differences in performance among methods is reduced, as in the lower right panel.


\section{Conclusion}

Impute-then-regress procedures assemble standard statistical routines to build predictors suited for data with missing values. However, we have shown that seeking the best prediction of the outcome leads to different tradeoffs compared to inferential purposes. Given a powerful learner, \emph{almost all imputations} lead asymptotically to the optimal prediction, \emph{whatever the missingness mechanism}. A good choice of imputation can however reduce the complexity of the function to learn.
Though conditional expectation can lead to discontinuous optimal regression functions, our experiments show that it still leads to easier learning problems compared to simpler imputations. 
In order to  adapt the imputation to the regression function, we proposed to jointly learn these two steps by chaining a trainable imputation via the NeuMiss networks and a classical MLP. An empirical study of non-linear regression shows that it outperforms impute-then-regress procedures built on standard imputation methods as well as gradient-boosted trees with incorporated handling of missing values. In further work, it would be useful to theoretically characterize the learning behaviors of Impute-then-Regress methods in finite sample regimes. 

\begin{ack}
MLM, JJ, and GV acknowledge funding via DataIA MissingBigData. MlM and GV acknowledge funding via ANR-17-CE23-0018 DirtyData and GV acknowledges funding via ANR-20-CHIA-0026 LearnI.
JJ acknowledges funding via ANR-16-IDEX-0006.
\end{ack}

\bibliographystyle{plainnat}
\bibliography{Reports-Neurips2021}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \section*{Checklist}

% \begin{enumerate}

% \item For all authors...
% \begin{enumerate}
%   \item Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope?
%     \answerYes{}
%   \item Did you describe the limitations of your work?
%     \answerYes{}\\
%     The conclusion gives the main shortcomings of our study.
%   \item Did you discuss any potential negative societal impacts of your work?
%     \answerNA{}\\
%     Our work is very fundamental and not attached to a specific application. Hence, it does not have per se direct societal impact other than that classic of machine learning.
%   \item Have you read the ethics review guidelines and ensured that your paper conforms to them?
%     \answerYes{}\\
%     Our paper is not applied to real-world data or application.
% \end{enumerate}

% \item If you are including theoretical results...
% \begin{enumerate}
%   \item Did you state the full set of assumptions of all theoretical results?
%     \answerYes{}\\
%     All theorems, propositions, and lemma clearly state the formal assumptions needed.
% 	\item Did you include complete proofs of all theoretical results?
%     \answerYes{}\\
%     All proofs are in the appendices, and the relevant appendix is referenced next to the statement of the corresponding result.
% \end{enumerate}

% \item If you ran experiments...
% \begin{enumerate}
%   \item Did you include the code, data, and instructions needed to reproduce the main experimental results (either in the supplemental material or as a URL)?
%     \answerYes{}\\
%     The methods are described in  \autoref{sec:experimental_setup} and in appendix \label{ss:NeuMiss_archi}.
%   \item Did you specify all the training details (e.g., data splits, hyperparameters, how they were chosen)?
%     \autoref{sec:experimental_setup} and in appendix \label{ss:NeuMiss_archi}.
%     \answerYes{}
% 	\item Did you report error bars (e.g., with respect to the random seed after running experiments multiple times)?
%     \answerYes{}
%     Figure \autoref{fig:boxplots} gives the full distribution of scores across 10 runs of the experiments.
% 	\item Did you include the total amount of compute and the type of resources used (e.g., type of GPUs, internal cluster, or cloud provider)?
%     \answerYes{}
%     Computing time is mentioned at the end of \autoref{sec:experimental_setup}.
% \end{enumerate}

% \item If you are using existing assets (e.g., code, data, models) or curating/releasing new assets...
% \begin{enumerate}
%   \item If your work uses existing assets, did you cite the creators?
%     \answerYes{}\\
%     Our experiments build upon scikit-learn, for reference methods, Pytorch to implement the deep-learning based methods, and the code of NeuMiss networks. All are appropriately cited.
%   \item Did you mention the license of the assets?
%     \answerYes{}
%   \item Did you include any new assets either in the supplemental material or as a URL?
%     \answerNA{}
%   \item Did you discuss whether and how consent was obtained from people whose data you're using/curating?
%     \answerNA{}
%   \item Did you discuss whether the data you are using/curating contains personally identifiable information or offensive content?
%     \answerNA{}
% \end{enumerate}

% \item If you used crowdsourcing or conducted research with human subjects...
% \begin{enumerate}
%   \item Did you include the full text of instructions given to participants and screenshots, if applicable?
%     \answerNA{}
%   \item Did you describe any potential participant risks, with links to Institutional Review Board (IRB) approvals, if applicable?
%     \answerNA{}
%   \item Did you include the estimated hourly wage paid to participants and the total amount spent on participant compensation?
%     \answerNA{}
% \end{enumerate}

% \end{enumerate}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\clearpage
\appendix

\toptitlebar
{\centering{\Large{\bfseries  Supplementary materials} -- \mytitle \par}}
\bottomtitlebar


\section{Proofs}

\subsection{Proof of Lemma~\ref{lem:manifold}}

\begin{lemma}
\label{lem:manifold}
    Let $\phi^{(m)}\in \Ccal^\infty\br{\RR^{|obs(m)|}, \RR^{|mis(m)|}}$ be the imputation function for missing data pattern $m$, and let $\Mcal^{(m)} = \cbr{x \in \RR^d: x_{mis} = \phi^{(m)}(x_{obs})}$. For all $m$, $\Mcal^{(m)}$ is an $|obs|-$dimensional manifold.
\end{lemma}

\begin{proof}
    Let:
    \begin{align*}
        h^{(m)}: \RR^d & \to \RR^{|mis|}\\
        x & \mapsto x_{mis} - \phi^{(m)}(x_{obs})
    \end{align*}
    \underline{Regular value:} We will show that $\bm 0_{mis}$ is a regular value of $h^{(m)}$. By definition \citep[see p21 in][]{GP_book}, a point $y \in \RR^{|mis|}$ is a regular value of $h^{(m)}$ if $dh^{(m)}_x$ is surjective at every point $x$ such that $h^{(m)}(x) = y$. The mapping $dh^{(m)}_x$ is linear and can be represented by the Jacobian of $h^{(m)}$ at $x$:
    
    \newcommand{\BigA}{\vrule width 0pt height 17pt depth 15pt A}
    \begin{equation*}
        J_{h^{(m)}}(x) =
            \left(
                \begin{matrix} & \BigA & \vline & Id & \end{matrix}
            \right), \quad
            A \in \RR^{|mis|\times|obs|},\, Id \in \RR^{|mis|\times|mis|}.
    \end{equation*}
    Given the structure of $J_{h^{(m)}}(x)$, it is obviously of rank $|mis|$ at every point $x$. Thus $dh^{(m)}_x$ is surjective at every point $x$, and it is true in particular for the points $x$ such that $h^{(m)}(x) = \bm 0$. We conclude that by definition, $\bm 0_{mis}$ is a regular value of $h^{(m)}$.
    
    \underline{Preimage theorem:} By the Preimage theorem (\citep{GP_book}, p.21), since $\bm 0 \in \RR_{mis}$ is a regular value of $h^{(m)}: \RR^d \to \RR^{|mis|}$, then the the preimage $\br{h^{(m)}}^{-1}(\bm 0)$ is a submanifold of $\RR^d$ of dimension $d - |mis| = |obs|$.
    
    Since by definition, $\br{h^{(m)}}^{-1}(\bm 0) = \Mcal^{(m)}$, we have that $\Mcal^{(m)}$ is a $|obs|-$dimensional mainfold.
\end{proof}



\subsection{Proof of Lemma~\ref{lem:manifold_intersection}}

\begin{lemma}
\label{lem:manifold_intersection}
    Let $m$ and $m^\prime$ be two distinct missing data patterns with the same number of missing values $|mis|$. Let $\phi^{(m)}\in \Ccal^\infty\br{\RR^{|obs(m)|}, \RR^{|mis(m)|}}$ be the imputation function for missing data pattern $m$, and let $\Mcal^{(m)} = \cbr{x \in \RR^d: x_{mis} = \phi^{(m)}(x_{obs})}$. We define similarly $\phi^{(m^\prime)}$ and $\Mcal^{(m^\prime)}$.
    For almost all imputation functions $\phi^{(m)}$ and $\phi^{(m^\prime)}$,
    \begin{equation}
        dim\br{ \Mcal^{(m)} \cap \Mcal^{(m^\prime)} } =
        \begin{cases}
            0 \quad & if \; |mis| > \frac{d}{2}\\
            d - 2|mis| \quad &otherwise.
        \end{cases}
    \end{equation}
\end{lemma}

\begin{proof}
    According to Thom Transversality theorem (\citep{Stable_mappings_book}, p.54) with:
    \begin{itemize}
        \item $W = \Mcal^{(m^\prime)}$,
        \item $f = \phi^{(m)}$,
        \item $k = 0$ (note that as stated p.37, $J^0(X, Y) = X \times Y$ and $j^0f(x) = \text{graph}(f)$),
    \end{itemize}
    we have that $\cbr{\phi^{(m)} \in \Ccal^{\infty}(\RR^{|obs|}, \RR^{|mis|})\, |\, \text{graph}(\phi^{(m)}) \pitchfork \Mcal^{(m^\prime)}}$ is a residual subset of $\Ccal^{\infty}(\RR^{|obs|}, \RR^{|mis|})$ in the $\Ccal^\infty$ topology. In other words, the fact that $\text{graph}(\phi^{(m)})$ is transverse to $\Mcal^{(m^\prime)}$ is a generic property. Put differently, almost all functions $\phi^{(m)}$ have their graph transverse to $\Mcal^{(m^\prime)}$. Note that here the notion of \emph{almost all} has to be understood in its topological sense, and not in its measure theory sense.\\
    
    Suppose that $|obs| < \frac{d}{2}$. According to Lemma~\ref{lem:manifold}, $\Mcal^{(m^\prime)}$ is a $|obs|-$dimensional manifold. Moreover we just showed that for almost all $\phi^{(m)}$, $\text{graph}(\phi^{(m)}) \pitchfork \Mcal^{(m^\prime)}$. Applying Proposition 4.2 of \citep{Stable_mappings_book} (p.51) with $W = \Mcal^{(m^\prime)}$ and $f = \text{graph}(\phi^{(m)})$, we obtain that $\Mcal^{(m)}$ and $\Mcal^{(m^\prime)}$ are disjoint, since, by definition, $\Mcal^{(m)}$ is the image of $\text{graph}(\phi^{(m)})$. Consequently, the dimension of their intersection is 0.
    
    Suppose that $|obs| \geq \frac{d}{2}$. According to the theorem p.30 of \citep{GP_book}, since $\Mcal^{(m)}$ and $\Mcal^{(m^\prime)}$ are transverse submanifolds of $\RR^d$, their intersection is again a manifold with $\text{codim}(\Mcal^{(m)} \cap \Mcal^{(m^\prime)}) = \text{codim}(\Mcal^{(m)}) + \text{codim}(\Mcal^{(m^\prime)})$. This implies that $\text{dim}(\Mcal^{(m)} \cap \Mcal^{(m^\prime)}) = 2|obs| - d$.
\end{proof}



\subsection{Proof of Theorem~\ref{th:fbp}}
\label{ss:bayes_optimality}

\fbp*

\begin{proof}
    Let $\phi^{(m)}\in \Ccal^\infty\br{\RR^{|obs(m)|}, \RR^{|mis(m)|}}$ be the imputation function for missing data pattern $m$, and let $\Mcal^{(m)} = \cbr{x \in \RR^d: x_{mis} = \phi^{(m)}(x_{obs})}$. According to Lemma~\ref{lem:manifold}, for all $m$, $\Mcal^{(m)}$ is an $|obs|-$dimensional manifold. $\Mcal^{(m)}$ corresponds to the subspace where all points with missing data pattern $m$ are mapped after imputation.
    
    Let us order missing data patterns according to their number of missing values, with the pattern of all missing entries ranked first and the pattern of all observed entries ranked last. Two patterns with the same number of missing values are ordered arbitrarily. We use $m(i)$ to refer to the missing data pattern ranked in $i^{th}$ position.
    
    Let $g^\star$ be the function defined as follows: for all $i$, 
    \begin{equation*}
        \forall Z=\Phi(\widetilde X) \in \Mcal^{(m(i))} \setminus \bigcup\limits_{m(k) < m(i)}  \Mcal^{(m(k))}, \qquad g^\star(Z) = \tilde f^{\star}(\widetilde X).
    \end{equation*}
    
    For a given missing data pattern $m(i)$, by distributivity of intersections across unions, we have:
    \begin{equation*}
        \Mcal^{(m(i))} \bigcap \br{ \bigcup\limits_{m(k) < m(i)}  \Mcal^{(m(k))}} = \underset{m(k) < m(i)} {\bigcup}\br{\Mcal^{(m(i))}  \bigcap \Mcal^{(m(k))}} 
    \end{equation*}
    
    If $m(k)$ has strictly more missing values than $m(i)$, then by Lemma~\ref{lem:manifold} $\text{dim}(\Mcal^{(m(k))}) <  \text{dim}(\Mcal^{(m(i))})$, and thus $\text{dim}(\Mcal^{(m(k))} \cap \Mcal^{(m(i))}) < \text{dim}(\Mcal^{(m(i))})$. Moreover, If $m(k)$ has the same number of missing values as $m(i)$, then by Lemma~\ref{lem:manifold_intersection}, for almost all imputation functions $\phi^{(m(k))}$ and $\phi^{(m(i))}$, $\text{dim}(\Mcal^{(m(k))} \cap \Mcal^{(m(i))}) < \text{dim}(\Mcal^{(m(i))})$. We conclude that for all $m(k) < m(i)$, $\Mcal^{(m(k))} \cap \Mcal^{(m(i))}$ is a subset of measure zero in $\Mcal^{(m(i))}$. Finally, since a countable union of sets of measure zero has measure zero, we obtain that $\underset{m(k) < m(i)} {\cup}\br{\Mcal^{(m(i))} \cap \Mcal^{(m(k))}}$ has measure zero in $\Mcal^{(m(i))}$.
    
    Let's now compute the risk of $g^\star \circ \Phi$:
    \begin{equation}
        \Rcal(g^\star \circ \Phi) = \sum_{M = m} P(M=m) \int_{X_{obs}} P(X_{obs}| M=m) \br{ \tilde f^\star(\widetilde X) - g^\star \circ \Phi(\widetilde X)}^2
    \end{equation}
    
    For a given missing data pattern $m$, $\Phi(\widetilde X) \in \Mcal^{(m)}$. Moreover, we constructed $g^\star$ such that $g^\star \circ \Phi(\widetilde X) = \tilde f^\star (\widetilde X)$ for all $\Phi(\widetilde X) \in \Mcal^{(m)}$ except on a set that we just showed to be of measure zero for almost all imputation functions. As a result, the function $X_{obs} \mapsto \tilde f^\star(\widetilde X) - g^\star \circ \Phi(\widetilde X)$ is zero almost everywhere for a given $m$, and the function $X_{obs} \mapsto P(X_{obs}| M=m) \br{ \tilde f^\star(\widetilde X) - g^\star \circ \Phi(\widetilde X)}^2$ is also zero almost everywhere. Since the integral of a function that vanishes almost everywhere is equal to 0, we conclude that $\Rcal(g^\star \circ \Phi) = 0$. Since the risk cannot be negative, $g^\star \circ \Phi$ is a minimizer of the risk and thus it is Bayes optimal.
\end{proof}

\subsection{Examples of transverse and nontransverse manifolds in 2D.}
\label{ss:2D_manifolds}
Theorem~\ref{th:fbp} is true for \emph{almost all} imputation functions and not \emph{all} of them. Thus, we can construct examples with particular choices of imputation functions that lead to nontransverse manifolds, and consequently for which Impute-then-Regress procedures are not Bayes optimal. We provide such an example below.

Consider a dataset with points $x \in \mathbb R^2$, and let $a \in \mathbb R$. Let $\Phi^{(0, 1)}_2(x_1) = a*x_1$ be the imputation function for $x_2$ when only $x_1$ is observed. And let $\Phi^{(1, 0)}_1(x_2) = \frac{1}{a}x_2$ be the imputation function for $x_1$ when only $x_2$ is observed. In this particular case shown in Figure~\ref{fig:2d_linear_manifolds} (bottom), the manifolds on which the data with either $x_1$ missing or $x_2$ missing are projected are exactly the same (the same line in the 2D space). Thus they are nontransverse and consequently Theorem~\ref{th:fbp} does not hold.

However according to the Thom transversality theorem, almost all imputation functions will lead to transverse manifolds.

\begin{figure}[h]
    \begin{minipage}{.28\linewidth}
    \scalebox{.21}{\subfile{tikz_linear_manifolds_transversal}}\\
    \scalebox{.21}{\subfile{tikz_linear_manifolds_nontransversal}}
    \end{minipage}%
        \hfill%
    \begin{minipage}{.65\linewidth}
        \caption{\textbf{Example - Linear imputation manifolds in two dimensions} Manifolds represented for linear imputation functions. $\Mcal^{(0, 0)}$ is the whole plane. Note that $\Mcal^{(1, 1)}$ need not be at the intersection of the two lines, it depends on the imputation function chosen. With linear imputation functions, $\Mcal^{(0, 1)}$ and $\Mcal^{(1, 0)}$ are transverse if and only if the two lines are not coincident.\textbf{Top:} Transverse manifolds. \textbf{Bottom:} Nontransverse manifolds.}
         \label{fig:2d_linear_manifolds}
    \end{minipage}
\end{figure}


\subsection{Proof of Lemma~\ref{lem:lem1}}
\begin{lemma}
\label{lem:lem1}
\begin{equation*}
    \forall X \in \RR^p,\, \forall mis \subseteq \bbr{1, p},\, H(X) \preccurlyeq \bar{H}^+ \implies H_{mis, mis}(X) \preccurlyeq \bar{H}^+_{mis, mis}
\end{equation*}
\end{lemma}

\begin{proof}
Let $X \in \RR^p$, and let $m$ be a missing data pattern with observed (resp. missing) indices $obs$ (resp. $mis$).
$H(X) \preccurlyeq \bar{H}^+$ is equivalent to:
\begin{equation}
\label{eq:psd}
    \forall u \in \RR^p,\, u^\top \br{\bar{H}^+ - H(X)}u \geq 0.
\end{equation}
Let $\Vcal \subseteq \RR^p$ be a subspace such that for any $v$ in $\Vcal$, $v_{obs}=0$. Since $\Vcal \subseteq \RR^p$, \eqref{eq:psd} implies:
\begin{align*}
    & \forall v \in \Vcal, \, v^\top \br{\bar{H}^+ - H(X)} v \geq 0\\
    \iff & \forall v_{mis} \in \RR^{\abs{mis}}, \, v_{mis}^\top \br{\bar{H}_{mis, mis}^+ - H_{mis, mis}(X)} v_{mis} \geq 0\\
    \iff & H_{mis, mis}(X) \preccurlyeq \bar{H}_{mis, mis}^+
\end{align*}
\end{proof}


\subsection{Proof of Lemma~\ref{le:fimp}}
\label{ss:proof_fimp}

\fimpth*

\begin{proof}
Without loss of generality, suppose that we reorder variables such that we can write $X = (X_{obs}, X_{mis})$. Consider the function
\begin{align*}
    f^\star_{mis}: \RR^{|mis|} & \to \RR\\
    X_{mis} &\mapsto f^\star(X_{obs}, X_{mis})
\end{align*}
Since $f^\star \in \Ccal^2\br{\RR^d, \RR}$, we have $f^\star_{mis} \in \Ccal^2 \br{\RR^{|mis|}, \RR}$. Therefore, we can write the first order Taylor expansion (see Theorem 2.68 in \cite{Folland2002advanced}) of $f^\star_{mis}$ around $E\sqb{X_{mis}|X_{obs}, M}$:
    \begin{align}
        \begin{split}
            \label{eq:Taylor}
            f^\star_{mis}(X_{mis}) =& f^\star(X_{obs}, \E\sqb{X_{mis}|X_{obs}, M})\\
            & + \nabla f^\star_{mis}(X_{obs}, \E\sqb{X_{mis}|X_{obs}, M})^\top \br{X_{mis} - \E\sqb{X_{mis}|X_{obs}, M}}\\
            & + R\br{X_{mis} - \E\sqb{X_{mis}|X_{obs}, M}},
        \end{split}
    \end{align}
    where $R$ is the Lagrange remainder satisfying
    \begin{align*}
        & R\br{X_{mis} - \E\sqb{X_{mis}| X_{obs}, M}} = \\
        & \hspace{4em} \frac{1}{2}\br{X_{mis} - \E\sqb{X_{mis}|X_{obs}, M}}^\top H_{mis, mis}(c) \br{X_{mis} - \E\sqb{X_{mis}|X_{obs}, M}},
    \end{align*}
    for some $c$ in the ball $\Bcal\br{\E\sqb{X_{mis}|X_{obs}, M}, \nm{X_{mis} - \E\sqb{X_{mis}|X_{obs}, M}}_2}$. By assumption, for all $X$, $H(X) \preccurlyeq \bar{H}^+$. Therefore, according to Lemma~\ref{lem:lem1},  we have $H_{mis, mis}(X) \preccurlyeq \bar{H}^+_{mis, mis}$ for any missing data pattern, which leads to:
    \begin{align*}
        & R\br{X_{mis} - \E\sqb{X_{mis}|X_{obs}, M}} \leq\\
        & \hspace{6em} \frac{1}{2}\br{X_{mis} - \E\sqb{X_{mis}|X_{obs}, M}}^\top \bar H^+_{mis, mis} \br{X_{mis} - \E\sqb{X_{mis}|X_{obs}, M}}.
    \end{align*}
    Using equality~\eqref{eq:Taylor}, we get:
    \begin{align*}
        & f^\star(X_{obs}, X_{mis})  - f^\star(X_{obs}, \E\sqb{X_{mis}|X_{obs}, M})\\
        & \hspace{4em} - \nabla f^\star_{mis}(X_{obs}, \E\sqb{X_{mis}|X_{obs}, M})^\top \br{X_{mis} - \E\sqb{X_{mis}|X_{obs}, M}} \\ 
        & \hspace{10em} \leq \frac{1}{2}\br{X_{mis} - \E\sqb{X_{mis}|X_{obs}, M}}^\top \bar H^+_{mis, mis} \br{X_{mis} - \E\sqb{X_{mis}|X_{obs}, M}}
    \end{align*}
    
    Finally, taking the expectation with regards to $P(X_{mis}|X_{obs}, M)$ on both sides, we obtain
    \begin{equation}
    \label{eq:maj}
        \E\sqb{f^\star(X_{obs}, X_{mis})|X_{obs}, M} - f^\star(X_{obs}, \E\sqb{X_{mis}|X_{obs}, M}) \leq \frac{1}{2} tr(H_{mis, mis}^{+ \top} \Sigma_{mis|obs, M}),
    \end{equation}
    where we have used the fact that, for any vector $X \in \RR^d$ and for any $H \in P_d^+$,
    \begin{align*}
     X^\top H X = tr(X^\top H X) = tr( H X X^\top).
    \end{align*}
    
    Following a similar reasoning, we can show that:
    \begin{equation}
    \label{eq:min}
        \E\sqb{f^\star(X_{obs}, X_{mis})|X_{obs}, M} - f^\star(X_{obs}, \E\sqb{X_{mis}|X_{obs}, M}) \geq \frac{1}{2} tr(H_{mis, mis}^{- \top} \Sigma_{mis|obs, M})
    \end{equation}
    
    Together, inequalities~\eqref{eq:maj} and \eqref{eq:min} conclude the proof.
\end{proof}


\subsection{Proof of Proposition~\ref{prop:chain}}
\label{ss:proof_prop_chain}

\propchain*

\begin{proof}
    \begin{align}
        Y - f^\star(X^{CI}) =& (Y - \tilde f^\star(\widetilde X)) + (\tilde f^\star(\widetilde X) - f^\star(X^{CI}))\\
        \br{Y - f(X^{CI})}^2 =& (Y - \tilde f^\star(\widetilde X))^2 + (\tilde f^\star(\widetilde X) - f^\star(X^{CI}))^2\\
        & + 2(Y - \tilde f^\star(\widetilde X))(\tilde f^\star(\widetilde X) - f^\star(X^{CI})\\
        =& (Y - \tilde f^\star(\widetilde X))^2 + (\tilde f^\star(\widetilde X) - f^\star(X^{CI}))^2\\
        & + 2(f^\star(X) - \tilde f^\star(\widetilde X))(\tilde f^\star(\widetilde X) - f^\star(X^{CI})) \label{eq:term2}\\
        & + 2\epsilon(\tilde f^\star(\widetilde X) - f^\star(X^{CI})) \label{eq:term3}\\
        \E\sqb{\br{Y - f^\star(X^{CI})}^2} =& \Rcal^\star + \E \sqb{\br{\tilde f^\star(\widetilde X) - f^\star(X^{CI})}^2} \label{eq:cofimp1}
    \end{align}
    where we used the definition of the Bayes rate. Moreover, term~\eqref{eq:term3} vanishes when taking the expectation w.r.t $\epsilon$ because $\E\sqb{\epsilon | X_{obs}, M} = 0$ and $\epsilon$ in uncorrelated with $X$ or $M$, and term~\eqref{eq:term2} vanishes when taking the expectation w.r.t $X_{mis}|X_{obs}, M$ because by definition $\E_{X_{mis}|X_{obs}, M} \sqb{f^\star(X_{obs}, X_{mis})}= \tilde f^\star(\widetilde X)$.
    
    Clearly, $f^\star \odot \Phi^{CI}$ is Bayes optimal if ans only if:
    \begin{align}
        &\quad \E \sqb{\br{\tilde f^\star(\widetilde X) - f^\star(X^{CI})}^2} = 0\\
        \iff &\quad \sum_{M} \int P(X_{obs}, M)\br{\tilde f^\star(\widetilde X) - f^\star(X^{CI})}^2 dX_{obs} = 0\\
        \iff &\quad \forall M, X_{obs}: P(X_{obs}, M)>0,\; \tilde f^\star(\widetilde X) = f^\star(X^{CI}) \quad \text{almost everywhere}. \label{eq:positive_terms}
    \end{align}
    where equality~\ref{eq:positive_terms} is true since all terms are positive.
    
    Besides, by Lemma~\ref{le:fimp}, we have:
    \begin{equation}
        \frac{1}{2}\text{tr}\left(\bar{H}^-_{mis, mis} \Sigma_{mis|obs, M}\right) \leq \tilde f^\star(\widetilde X) - f^\star(X^{CI}) \leq \frac{1}{2}\text{tr}\left(\bar{H}^+_{mis, mis} \Sigma_{mis|obs, M}\right).
    \end{equation}
    
    By convexity of the square function, it follows that:
    \begin{equation}
        \br{\tilde f^\star(\widetilde X) - f^\star(X^{CI})}^2 \leq \frac{1}{2} \max \br{\text{tr}\br{ \bar{H}^-_{mis, mis} \Sigma_{mis|obs, M}}^2,  \text{tr}\br{\bar{H}^+_{mis, mis} \Sigma_{mis|obs, M}}^2}.
    \end{equation}
    
    Finally, by taking the expectation on both sides:
    \begin{align}
        \begin{split}
            \label{eq:cofimp2}
            &\E \sqb{\br{\tilde f^\star(\widetilde X) - f^\star(X^{CI})}^2} \leq \\
            & \hspace{4em} \frac{1}{2} \E_M \sqb{\max \br{\text{tr}\br{ \bar{H}^-_{mis, mis} \Sigma_{mis|obs, M}}^2,  \text{tr}\br{\bar{H}^+_{mis, mis} \Sigma_{mis|obs, M}}^2}}.
        \end{split}
    \end{align}

    Combining equation~\eqref{eq:cofimp1} with inequality~\eqref{eq:cofimp2} concludes the proof.
\end{proof}


\subsection{Proof of Proposition~\ref{prop:disc}}
\label{ss:proof_prop_disc}

\propdisc*

\begin{proof}
    We will prove this result by contradiction. Suppose that (i) $f^\star \circ \Phi^{CI}$ is not Bayes optimal, (ii) the probability of observing all variables is strictly positive, (iii) there exists a function $g$ continuous such that $g \circ \Phi^{CI}$ is Bayes optimal.
    
    Following a reasoning similar to the one in the proof of proposition~\ref{prop:chain}, we can show that $g \circ \Phi^{CI}$ is Bayes optimal if and only if:
    \begin{equation*}
        \forall M, X_{obs}: P(X_{obs}, M)>0, \quad \E\sqb{f^\star(X)|X_{obs}, M} = g(X^{CI}) \quad \text{almost everywhere}.
    \end{equation*}
    
    In particular since for all $x$, the joint probability $P(M = (0, \dots, 0), X=x)$ of observing all variables is strictly positive, $g$ should satisfy this equality for $M = (0, \dots, 0)$, i.e.:
    \begin{equation*}
        f^\star(X) = g(X) \quad \text{almost everywhere}.
    \end{equation*}
    Since $g$ is continuous, it implies $g = f^\star$.
    Since by assumption, $f^\star$ is not Bayes optimal, then $g$ is not either, which is a contradiction.
\end{proof}



\subsection{Example of a case where no continuous corrected imputation exists.}
\label{ss:no_cc}
Let:
\begin{align*}
    f^\star: \RR^2 &\to \RR\\
    (X_1, X_2) & \mapsto X_2^3 - 3X_2
\end{align*}
and let:
\begin{align*}
    X_2 = X_1 + \epsilon \quad \text{with} \quad & \E \sqb{\epsilon | X_1, M=(0, 1)} = 0\\
    & \E \sqb{\epsilon^2|X_1, M=(0, 1)} = \sigma^2, \, \sigma^2 > 1\\
    & \E \sqb{\epsilon^3|X_1, M=(0, 1)} = 0
\end{align*}

Suppose that $X_2$ is missing. Then the Bayes predictor is given by:
\begin{align*}
    \tilde f^\star(X_1, M=(0, 1)) &= \E \sqb{f^\star(X) | X_1, M=(0, 1)}\\
    &= \E \sqb{X_2^3 - 3X_2| X_1, M=(0, 1)}\\
    &= \E \sqb{\br{X_1 + \epsilon}^3 - 3\br{X_1 + \epsilon} | X_1, M=(0, 1)}\\
    &= \E \sqb{X_1^3 + \epsilon^3 + 3X_1\epsilon^2 + 3X_1^2\epsilon - 3X_1 -3\epsilon) | X_1, M=(0, 1)}\\
    &= X_1^3 + 3X_1(\sigma^2 - 1)
\end{align*}
Clearly, the Bayes predictor for $M = (0, 1)$ is:
\begin{itemize}
    \item continuous,
    \item non-decreasing since $\sigma^2>1$,
    \item $\underset{X1 \to +\infty}{\lim} \tilde f^\star(X_1, M=(0, 1)) = +\infty$ and $\underset{X1 \to -\infty}{\lim} \tilde f^\star(X_1, M=(0, 1)) = -\infty$.
\end{itemize}

\underline{Proof by contradiction}: Suppose that there exists a function $\Phi: \RR \to \RR$ (i) continuous and (ii) such that for all $X_1$, $f^\star(X_1, \Phi(X_1)) = \tilde f^\star(X_1, M=(0, 1))$.

\begin{figure}
    \centering
    \includegraphics[scale=0.65]{f_star_cc.pdf}
    \caption{Graph of $X_2 \mapsto f^\star(X_1, X_2)$}
    \label{fig:f_star_cc}
\end{figure}

Let $x_1^+ \in \RR$ such that $\tilde f^\star(X_1 = x_1^+, M=(0, 1)) > 2$. $x_1^+$ exists since $\underset{X1 \to +\infty}{\lim} \tilde f^\star(X_1, M=(0, 1)) = +\infty$. Clearly,
$$ f^\star(x_1^+, X_2) = \tilde f^\star(x_1^+, M=(0, 1)) \iff X_2 = x_2^+ \quad \text{with} \quad x_2^+ > 2.$$

Similarly, let $x_1^- \in \RR$ such that $\tilde f^\star(X_1 = x_1^-, M=(0, 1)) < -2$. $x_1^-$ exists since $\underset{X1 \to -\infty}{\lim} \tilde f^\star(X_1, M=(0, 1)) = -\infty$. Clearly,
$$ f^\star(x_1^-, X_2) = \tilde f^\star(x_1^-, M=(0, 1)) \iff X_2 = x_2^- \quad \text{with} \quad x_2^- < -2.$$

So $\Phi$ must satisfy:
\begin{align*}
    \Phi(x_1^-) &= x_2^- < -2\\
    \Phi(x_1^+) &= x_2^+ > 2
\end{align*}
Note that since the Bayes predictor is non-decreasing, we have $x_1^- < x_1^+$. Since $\Phi$ is continuous, there exists $\check x_1 \in \sqb{x_1^-, x_1^+}$ and $\hat x_1 \in \sqb{x_1^-, x_1^+}$ such that $\check x_1 < \hat x_1$
and $\Phi(\check x_1) = -1$ and $\Phi(\hat x_1) = 1$. It implies that:
$$ f^\star(\check x_1, \Phi(\check x_1)) = f^\star(\check x_1, -1) = 2 > -2 = f^\star(\hat x_1, 1) = f^\star(\hat x_1, \Phi(\hat x_1)).$$
This implies that the function $X_1 \mapsto f^\star(X_1, \Phi(X_1))$ cannot be non-decreasing. Since the Bayes predictor is non-decreasing, the two cannot be equal. CONTRADICTION.



\subsection{Proof of Proposition~\ref{prop:cont_imp}}
\label{app:continuous_imputations}

We start by proving the result for a given missing pattern $m \in \{0,1\}^d$. Take $r \in \{1, \hdots, d-1\}$ and consider a missing pattern $m$ such that $|obs(m)| = r$. 
We let $F : \RR^r \times \RR^{d-r} \to \RR$ defined, for all $(x_{obs}, x_{mis})$ as 
\begin{align}
    F(x_{obs}, x_{mis}) = f^{\star} (x_{obs}, x_{mis}) - \tilde{f}^{\star}(x_{obs}, m).
\end{align}
Our aim is to find, for all $x_{obs}$, a value $x_{mis}$ (depending continuously on $x_{obs}$) satisfying
\begin{align}
 F(x_{obs}, x_{mis}) = 0.   
\end{align}
To this aim, we check the assumptions of Theorem~6 in \cite{arutyunov2019application} for the function $F$. The desired conclusion will follow. 

Since $f^{\star}$ is uniformly continuous and twice continuously differentiable, condition $1-3$ of Theorem~6 in \cite{arutyunov2019application} are satisfied. 
%
To verify the next condition, we have to prove that there exists $(x_{obs, 0}, x_{mis, 0})$ such that $F(x_{obs, 0}, x_{mis, 0}) =0$. 
Note that this is equivalent to finding $(x_{obs, 0}, x_{mis, 0})$ satisfying 
\begin{align}
    f^{\star} (x_{obs, 0}, x_{mis, 0}) = \tilde{f}^{\star}(x_{obs, 0}, m) = \E \left[ f^{\star}(X)| X_{obs} = x_{obs, 0}, M=m\right], 
    \label{proof:eq_existence_pair}
\end{align}
by definition of the regression function $\tilde{f}^{\star}$. By assumption, the support of $X_{mis} | X_{obs} = x_{obs, 0}, M=m$ is connected. Therefore, the intermediate value theorem can be applied and proves the existence of a pair $(x_{obs, 0}, x_{mis, 0})$ satisfying equation~\eqref{proof:eq_existence_pair}. Finally, by assumption, the regularity condition (GR1) in \cite{arutyunov2019application} is satisfied. This proves that there exists a continuous mapping $\phi^{(m)}: \RR^r \to \RR^{d-r}$ such that 
\begin{align}
    F(x_{obs}, \phi^{(m)}(x_{obs})) = 0.   
\end{align}
The previous reasoning holds for all missing patterns $m$, such that $|mis(m)| \geq 1$. Besides the result is clear for $r=0$ since the imputation function is reduced to a constant in this case (no components of $X$ are observed). On the contrary, in the case where all covariates are observed ($r=d$), no imputation function is needed. Therefore, the result holds for all $0 \leq r \leq d$, which concludes the proof. 



\clearpage
\section{Additional results}

\subsection{NeuMiss+MLP architecture}
\label{ss:NeuMiss_archi}
\begin{figure}[h]
    \centering
    \scalebox{.85}{\subfile{tikz_neumann}}
    \caption{\textbf{(Non-linear) NeuMiss+MLP network architecture with a Neumann block of depth 3} --- $\bar{m} = 1-m$. MLP stands for a standard multi-layer perceptron with ReLU activations.}
    \label{fig:NeuMiss}
\end{figure}

% \paragraph{Initialization strategy for the Neumann block} \citep{LeMorvan2020NeuMiss} designed the Neumann block so that the network may be able to approximate well the conditional expectation $\E \sqb{X_{mis}|X_{obs}, M}$. In particular, if the weights of the Neumann block are chosen equal to certain quantities depending on $\mu$ and $\Sigma$ (the mean and covariance matrix of the data distribution), then the network computes an approximation of the conditional expectation whose error tends to zero exponentially fast with the depth of the Neumann block. Since $\mu$ and $\Sigma$ are unknown in practice, the weights of the network cannot be set to the desired quantities and are instead learned by gradient descent.

% In our experiments, we computed estimates $\hat \mu$ and $\hat \Sigma$ as the sample mean and sample covariance matrix over the observed entries only, i.e, if variable $j$ is observed in $n_j$ samples, then $\hat \mu_j$ is computed as the mean over these $n_j$ values only. Similarly, the covariance between variables $i$ and $j$ is estimated using only the samples for which both are observed. Based on these estimates, we initialized the weights of the network to their theoretical values given in \cite{LeMorvan2020NeuMiss}:
% \begin{align*}
%     \mu &= \hat \mu\\
%     W^{(0)} &= Id - \frac{2}{\hat L} \hat \Sigma
% \end{align*}
% where $\hat L$ is the largest eigenvalue of $\Sigma$.

% We use a pytorch-based \citep{paszke2019pytorch} implementation of NeuMiss as well as the MLP with which we combine it.



\subsection{Expressions of $f^\star_{bowl}$, $f^\star_{wave}$ and $f^\star_{break}$ and the corresponding Bayes predictors.}
\label{ss:f_star}

\paragraph{Expressions of $f^\star_{bowl}$, $f^\star_{wave}$ and $f^\star_{break}$.}
The functions $f^\star$ used in the experimental study are defined as:
\begin{align*}
    f^\star_{bowl}(X) &= \br{\beta^\top X + \beta_0 -1}^2\\
    f^\star_{wave}(X) &= (\beta^\top X +\beta_0 -1) + \sum_{(a_i, b_i) \in S} a_i\,\Phi\br{\gamma \br{\beta^\top X + \beta_0 + b_i}}\\
    f^\star_{break}(X) &= \br{\beta^\top X + \beta_0} + 3 \times \mathds{1}_{\beta^\top X + \beta_0 > 1}
\end{align*}
where $\Phi$ the standard Gaussian cdf, $\gamma = 20 \sqrt{\frac{\pi}{8}}$ and $S = \cbr{(2, -0.8), (-4, -1), (2, -1.2)}$. $\beta$ is chosen as a vector of ones rescaled so that $\text{var}(\beta^\top X) = 1$. These functions are depicted in Figure~\ref{fig:f_star}. 


\paragraph{Expressions of the Bayes predictors.}
The expressions of the corresponding Bayes predictors are given by:
\begin{align}
    \tilde f^\star_{bowl}(\widetilde X) &= \E \sqb{f^\star_{bowl}(X) | X_{obs}, M}\\
    &= \br{\beta_{obs}^\top X_{obs} + \beta_{mis}^\top \mu_{mis|obs, M} + \beta_0 - 1}^2 + \beta_{mis}^\top \Sigma_{mis|obs, M} \beta_{mis}\\[1em]
    \tilde f^\star_{wave}(\widetilde X) &= \E \sqb{f^\star_{wave}(X) | X_{obs}, M}\\
    &= \beta_{obs}^\top X_{obs} + \beta_{mis}^\top \mu_{mis|obs, M} + \beta_0 - 1\\
    & \quad + \sum_{(a_i, b_i) \in S} a_i\,\Phi\br{\frac{\beta_{obs}^\top X_{obs} + \beta_{mis}^\top \mu_{mis|obs, M} + \beta_0 + b_i}{\sqrt{1/\gamma^2 + \beta_{mis}^\top \Sigma_{mis|obs, M} \beta_{mis}}}}\\[1em]
    \tilde f^\star_{break}(\widetilde X) &= \E \sqb{f^\star_{break}(X) | X_{obs}, M}\\
    &= \beta_{obs}^\top X_{obs} + \beta_{mis}^\top \mu_{mis|obs, M} + \beta_0 + 3\br{1-\Phi\br{\frac{1-\mu_{mis|obs, M}}{\beta_{mis}^\top \Sigma_{mis|obs, M} \beta_{mis}}}}
\end{align}
with $\mu_{mis|obs, M}$ and $\Sigma_{mis|obs, M}$ the mean and covariance matrix of the conditional distribution $P(X_{mis}|X_{obs}, M)$. Below, we give the expression of these parameters for the MCAR and Gaussian self-masking missing data mechanisms. Let $\mu_{mis|obs}$ and $\Sigma_{mis|obs}$ the mean and covariance matrix of the conditional distribution $P(X_{mis}|X_{obs})$. Since the data is generated according to a multivariate Gaussian distribution $\Ncal \br{\mu, \Sigma}$, we have:
\begin{align*}
    \mu_{mis|obs} &= \mu_{mis} + \Sigma_{mis|obs} \Sigma_{obs}^{-1}(X_{obs} - \mu_{obs})\\
    \Sigma_{mis|obs} &= \Sigma_{mis, mis} - \Sigma_{mis, obs} \Sigma_{obs}^{-1} \Sigma_{obs, mis}
\end{align*}

In the MCAR case, we simply have $\Sigma_{mis|obs, M} = \Sigma_{mis|obs}$ and $\mu_{mis|obs, M} = \mu_{mis|obs}$. In the Gaussian self-masking case, it has been shown in \cite{LeMorvan2020NeuMiss} that $P(X_{mis}|X_{obs}, M)$ is again Gaussian but with parameters:
\begin{align*}
    \Sigma_{mis|obs, M} = \br{D_{mis, mis}^{-1} + \Sigma_{mis|obs}^{-1}}^{-1} \\
    \mu_{mis|obs, M} = \Sigma_{mis|obs, M} \br{D_{mis, mis}^{-1}\tmu_{mis} + \Sigma_{mis|obs}^{-1}\mu_{mis|obs}}
\end{align*}
where $\tilde \mu$ and $D$ are parameters of the Gaussian self-masking missing data mechanism. Finally, we detail below the derivations to obtain the expression of the Bayes predictors.


\paragraph{Derivation of the Bayes predictor for $f^\star_{bowl}$.}
\begin{align}
    f^\star_{bowl}(X) &= \br{\beta^\top X + \beta_0 -1}^2\\
    &= \br{\beta_{obs}^\top X_{obs} + \beta_{mis}^\top X_{mis} + \beta_0 - 1}^2\\
    &= \br{\beta_{obs}^\top X_{obs} + \beta_{mis}^\top (X_{mis}-\mu_{mis|obs, M}) + \beta_{mis}^\top \mu_{mis|obs, M} + \beta_0 - 1}^2\\
    &= \br{\beta_{obs}^\top X_{obs} + \beta_{mis}^\top \mu_{mis|obs, M} + \beta_0 - 1}^2 + \br{\beta_{mis}^\top (X_{mis}-\mu_{mis|obs, M})}^2\\
    & \quad + 2\beta_{mis}^\top (X_{mis}-\mu_{mis|obs, M})\br{\beta_{obs}^\top X_{obs} + \beta_{mis}^\top \mu_{mis|obs, M} + \beta_0 - 1}
\end{align}
Now taking the expectation with regards to $P(X_{mis}|X_{obs}, M)$, the last term vanishes and we get:
\begin{equation}
    \E \sqb{f^\star_{bowl}(X) | X_{obs}, M} = \br{\beta_{obs}^\top X_{obs} + \beta_{mis}^\top \mu_{mis|obs, M} + \beta_0 - 1}^2 + \beta_{mis}^\top \Sigma_{mis|obs, M} \beta_{mis}
\end{equation}

\paragraph{Derivation of the Bayes predictor for $f^\star_{wave}$.}
\begin{align}
    f^\star_{wave}(X) &= (\beta^\top X +\beta_0 -1) + \sum_{(a_i, b_i) \in S} a_i\,\Phi\br{\gamma \br{\beta^\top X + \beta_0 + b_i}}\\
    &= (\beta_{obs}^\top X_{obs} + \beta_{mis}^\top X_{mis} +\beta_0 -1)\\
    & \quad + \sum_{(a_i, b_i) \in S} a_i\,\Phi\br{\gamma \br{\beta_{obs}^\top X_{obs} + \beta_{mis}^\top X_{mis} + \beta_0 + b_i}}
\end{align}
Define $T^{(m)} = \beta_{mis}^\top X_{mis}$. Since $P(X_{mis}|X_{obs}, M)$ is Gaussian in both the MCAR and Gaussian self-masking cases, $P(T^{(m)}|X_{obs}, M)$ is also Gaussian with mean and variance given by:
\begin{align}
    \mu_{T^{(m)|X_{obs, M}}} &= \beta_{mis}^\top \mu_{mis|obs, M}\\
    \sigma^2_{T^{(m)|X_{obs, M}}} &= \beta_{mis}^\top  \Sigma_{mis|obs, M} \beta_{mis}
\end{align}
To compute the Bayes predictor, we now need to compute the quantity:
\begin{equation*}
    \E_{T^{(m)}|X_{obs, M}} \sqb{\Phi\br{\gamma \br{\beta_{obs}^\top X_{obs} + T^{(m)} + \beta_0 + b_i}}}
\end{equation*}
This expectation can then be computed following \citep{Bishop} (section 4.5.2) which gives the result.

\paragraph{Derivation of the Bayes predictor for $f^\star_{break}$.}
\begin{align}
    f^\star_{break}(X) &= \br{\beta^\top X + \beta_0} + 3 \times \mathds{1}_{\beta^\top X + \beta_0 > 1}\\
    \E \sqb{f^\star_{break}(X) | X_{obs}, M} &= \beta_{obs}^\top X_{obs} + \beta_{mis}^\top \mu_{mis|obs, M} + \beta_0\\
    & \quad + 3 \times \int P(X_{mis}|X_{obs}, M) \mathds{1}_{\beta_{obs}^\top X_{obs} + \beta_{mis}^\top X_{mis} + \beta_0 > 1} dX_{mis}
\end{align}

Let $U^{(m)} = \beta_{obs} X_{obs} + \beta_{mis} X_{mis} + \beta_0$. Since $P(X_{mis}|X_{obs}, M)$ is Gaussian in both the MCAR and Gaussian self-masking cases, $P(U^{(m)}|X_{obs}, M)$ is also Gaussian with mean and variance given by:
\begin{align}
    \mu_{U^{(m)|X_{obs, M}}} &= \beta_{obs}^\top X_{obs} + \beta_{mis}^\top \mu_{mis|obs, M} +\beta_0\\
    \sigma^2_{U^{(m)|X_{obs, M}}} &= \beta_{mis}^\top  \Sigma_{mis|obs, M} \beta_{mis}
\end{align}
Using the law of the unconscious statistician, we get:
\begin{align}
    \E \sqb{f^\star_{break}(X) | X_{obs}, M} &= \beta_{obs}^\top X_{obs} + \beta_{mis}^\top \mu_{mis|obs, M} + \beta_0\\
    & \quad + 3 \times \int P(U^{(m)}|X_{obs}, M) \mathds{1}_{U^{(m)} > 1} dU^{(m)}\\
    &= \beta_{obs}^\top X_{obs} + \beta_{mis}^\top \mu_{mis|obs, M} + \beta_0\\
    & \quad + 3 \times \sqb{1- \mathbb{P}\br{U^{(m)} \leq 1 | X_{obs}, M}}\\
    &= \beta_{obs}^\top X_{obs} + \beta_{mis}^\top \mu_{mis|obs, M} + \beta_0\\
    & \quad + 3 \times \sqb{1- \Phi_{U^{(m)}|X_{obs}, M} (1)}
\end{align}


\subsection{Supplementary experiments with $f^*_{break}$.}
\label{ss:boxplots_discontinuous_linear}
\begin{figure}[h]
    \centering
    \includegraphics[scale=0.6]{boxplots_discontinuous_linear.pdf}
    \caption{Performances (R2 score on a test set) compared to that of the Bayes predictor across 10 repeated experiments.}
    \label{fig:boxplots_discontinuous_linear}
\end{figure}

\end{document}
