\begin{table*}[t]
\centering
\scalebox{1.0}{
\begin{tabular}{c|m{0.4\linewidth}|m{0.4\linewidth}}
\Xhline{2\arrayrulewidth}
  & \multicolumn{1}{c|}{Wrong supporting facts} & \multicolumn{1}{c}{Wrong inference} \bigstrut[t] \bigstrut[b] \\ \hline
Question & Was Lil Jon's top ranked Billboard song a collaboration with a member of The Lox?
 & Would the top of Mount Fuji stick out of the Sea of Japan? 
 \bigstrut[t] \bigstrut[b] \\ \hline
Answer & \multicolumn{1}{c|}{No} & \multicolumn{1}{c}{Yes} \bigstrut[t] \bigstrut[b]  \\ \hline
Facts & Lil Jon's highest ranked billboard song was Yeah; Yeah was a collaboration between Lil Jon, Usher, and Ludacris; The Lox is a rap trio consisting of: Styles P, Sheek Louch, and Jadakiss. & The average depth of the Sea of Japan is  5,748 feet (1,752 metres) and its maximum depth is 12,276 feet (3,742 metres); Mount Fuji is 3,776.24 metres (12,389.2 ft) tall.  \bigstrut[t] \bigstrut[b]  \\ \hline
GPT-3 & \textcolor{red}{Lil Jon's top ranked Billboard song was "Get Low"} with the East Side Boyz. The Lox is not a member of the East Side Boyz. Thus, Lil Jon's top ranked Billboard song was not a collaboration with a member of The Lox. So the answer is no.  & Mount Fuji is 3,776 meters tall. The Sea of Japan is about 3,741 meters deep. \textcolor{red}{Thus, the top of Mount Fuji would not stick out of the Sea of Japan.} So the answer is no.
 \bigstrut[t] \bigstrut[b]  \\ \Xhline{2\arrayrulewidth}
\end{tabular}}
\caption{Examples of incorrect outputs from GPT-3 with CoT prompting.} 
\label{table:gpt-3-outputs}
\end{table*}




\section{Analysis}

In this section, we perform a thorough analysis to gain a deeper understanding of \NAME{}.

\subsection{Limitations of LLMs in Reasoning}
\label{subsec:limitations}
In this subsection, we present an analysis of GPT-3 with CoT prompting on the StrategyQA dataset. Upon closer examination of the outputs of GPT-3, we observed that it can provide reasonable explanations and correct predictions for a number of questions. For example, when given the question ``\textit{Will the Albany in Georgia reach a hundred thousand occupants before the one in New York?}'', GPT-3 produced the following output:

\begin{quote}
\textcolor{cyan}{The Albany in New York has a population of about 98,000. The Albany in Georgia has a population of about 77,000.} \textcolor{YellowOrange}{Thus, the Albany in New York is more populous than the Albany in Georgia.} \textcolor{LimeGreen}{So the answer is no.}
\end{quote}

The above output consists of three components: (1) supporting facts (in cyan) that are based on a particular perspective, (2) chaining arguments (in orange), and (3) a prediction (in green). Components (1) and (2) contribute to the explanation. Overall, the output exhibits a high level of quality. However, we also observed that GPT-3 may occasionally produce incorrect supporting facts for its explanations or make incorrect inferences for its predictions, despite generally being able to identify suitable perspectives. 

\paragraph{Wrong supporting facts.} As shown in Table \ref{table:gpt-3-outputs}, GPT-3 provides the incorrect supporting fact for Lil Jon's top-ranked Billboard song, stating that it was ``Get Low'' instead of the correct answer, ``Yeah''. However, it does have the correct perspective on how to answer the question, ``\textit{Was Lil Jonâ€™s top ranked Billboard song a collaboration with a member of
The Lox?}''.

\paragraph{Wrong inference.} As shown in Table \ref{table:gpt-3-outputs}, GPT-3 makes an incorrect inference, stating that the top of Mount Fuji ``would not stick out'' of the Sea of Japan, rather than the correct answer, ``would stick out''. However, it does provide correct supporting facts based on the appropriate perspective for the question, ``\textit{Would the top of Mount Fuji stick out of the Sea of Japan?}''. 

\subsection{Ablation Study}


\begin{table}[t]
\centering
\scalebox{0.92}{
\begin{tabular}{c|c|c}
\hline
Retrieval & Commonsense & Tabular  \bigstrut[t]  \\ \hline
Query-based & 73.36 & 36.69 \bigstrut[t] \bigstrut[b] \\ \hline
Decomposition-based & {\bf 77.73} & {\bf 39.05}  \bigstrut[t] \bigstrut[b] \\ \hline
\end{tabular}}
\caption{Comparison of query-based and decomposition-based retrieval on commonsense and tabular reasoning.
}
\label{table:retrieval-analysis}
\end{table} 

\paragraph{Importance of decomposition-based retrieval.} In our proposed method, we retrieve relevant external knowledge based on the decomposed reasoning steps rather than the original query. To further investigate the impact of this choice, we conducted additional experiments in which we used the original query for knowledge retrieval while keeping other aspects of our method unchanged. As shown in Table \ref{table:retrieval-analysis}, the results for these experiments are poor for both commonsense and temporal reasoning, indicating the importance of using decomposition-based retrieval in our approach.

\begin{table}[t]
\centering
\scalebox{1.0}{
\begin{tabular}{c|c}
\hline
Knowledge & Tabular  \bigstrut[t]  \\ \hline
External & 79.92 \bigstrut[t] \bigstrut[b] \\ \hline
Background &  84.75 \bigstrut[t] \bigstrut[b] \\ \hline
Background + External & {\bf 84.83}  \bigstrut[t] \bigstrut[b] \\ \hline
\end{tabular}}
\caption{Performance of \NAME{} with different types of knowledge on tabular reasoning: external only, background only, and a combination of both. External knowledge refers to WordNet and ConceptNet, while background knowledge refers to the tables.
}
\label{table:knowledge-analysis}
\end{table} 
\begin{figure*}[t]
		\centering
        \hspace{0.01in}
		\subfigure[Accuracy of predictions]{
			\centering
			\includegraphics[scale=0.35]{figures/answers_accuracy_model_sizes.pdf}
			\label{fig:answer-accuracy-model-size}}
        \hspace{0.01in} 
        	\subfigure[Faithfulness of explanations]{
			\centering
		\includegraphics[scale=0.35]{figures/reasons_factuality_model_sizes.pdf}
			\label{fig:reason-factuality-model-size}}
		\caption{The effect of LM size on the performance of our proposed method (Variant II) and CoT prompting. We use various sizes of OPT models, with the exception of the 175B model, which is GPT-3.}
\label{fig:model-size}
\end{figure*}
\paragraph{The impact of different types of knowledge.} For tabular reasoning, we use both external knowledge (WordNet and ConceptNet) and background knowledge (tables) in our experiments. In this section, we further examine the effect of different types of knowledge on the performance of our proposed method. As shown in Table \ref{table:knowledge-analysis}, the additional improvement gained by incorporating Wikidata and ConceptNet in addition to tables is limited, indicating that GPT-3 already captures many word-level relations in these external knowledge sources. In addition, the observed significant improvement in tabular reasoning from using tables alone suggests that our proposed method can also effectively leverage background knowledge.

\subsection{Variations of the Proposed Approach}
\label{subsec:variations}
\paragraph{Basic approach: Weighting outputs.} In Section \ref{sec:framework}, we present a basic version of our proposal for taking advantage of external knowledge. Our basic approach involves \textit{weighting outputs as individual units} and using a \textit{voting} mechanism to select the best-supported prediction. We can also directly choose the best-supported output, which includes both an explanation and a prediction, without using voting. For example, in the running example of ``\textit{Did Aristotle use a laptop?}'' (see more in Section \ref{sec:framework}), the third reasoning path $R_3$ is the output most supported by the knowledge paragraphs $K_1$ and $K_2$.

\paragraph{Variant \uppercase\expandafter{\romannumeral1}: Fact selection.} The first variant of our approach involves selecting facts from the outputs of LLMs based on external knowledge. For example, consider the running example of ``\textit{Did Aristotle use a laptop?}'', where we only have access to the first two reasoning paths, $R_1$ and $R_2$. In this case, the first sentence in $R_2$ and the second sentence in $R_1$ are supported by knowledge $K_1$ and $K_2$, respectively. Therefore, the first variant would output the first sentence in $R_2$ and the second sentence in $R_1$ as the supporting facts.

\paragraph{Variant \uppercase\expandafter{\romannumeral2}: Fact generation.} The second variant of our approach involves generating facts based on both the outputs of LLMs and external knowledge. For example, consider the running example of ``\textit{Did Aristotle use a laptop?}'', where we only have access to the first reasoning path $R_1$. The second sentence in $R_1$ is supported by the second knowledge paragraph $K_2$. However, the first sentence is not supported by any evidence paragraphs. We can generate questions about the first sentence, such as ``When did Aristotle die?'' and use the first knowledge paragraph $K_1$ to generate a new fact: ``Aristotle died in 322BC.''. As a result, the second variant would output the generated fact ``Aristotle died in 322 BC.'' and the second sentence in $R_1$ as the supporting facts.

\paragraph{Inference with supporting facts.} For the two variants of our approach, we only have the supporting facts and need to perform a final inference step to obtain the corresponding prediction. One option for this inference is to use LLMs, but they can be costly \cite{brown2020language} or difficult to use \cite{zhang2022opt}. An alternative is to use an off-the-shelf model for inference with supporting facts, such as UnifiedQA \cite{khashabi2020unifiedqa, khashabi2022unifiedqa}. As discussed in Appendix \ref{subsec:inference-comparison}, UnifiedQA is more robust to noisy supporting facts than GPT-3. We thus use the second version of UnifiedQA, UnifiedQA-v2 \cite{khashabi2022unifiedqa}, for the final step of inference.

\paragraph{Experimental settings.} In this part, we focus on commonsense reasoning and use the \textit{evidence paragraphs} provided in StrategyQA as the relevant knowledge, rather than the retrieved paragraphs discussed in Section \ref{subsec:commonsense}. To evaluate the quality of the explanations, we adopt the best metric for factual consistency evaluation in \citet{honovich2022true}. For simplicity, we use the pre-trained NLI model released by \citet{nie2020adversarial} to compute the NLI-based metric, rather than fine-tuning T5-11B \cite{raffel2020exploring} ourselves. The implementation details of the two variants can be found in Appendix \ref{subsec:variants-implementation}.

\begin{table}[t]
\centering
\scalebox{0.88}{
\begin{tabular}{c|c|c}
\hline
Methods & Accuracy (\%) & Faithfulness (\%) \bigstrut[t]  \\ \hline
CoT prompting & 65.94 & 38.73 \bigstrut[t] \bigstrut[b] \\ \hline \hline
Basic (w/o voting) & 76.86 & 50.02 \bigstrut[t] \bigstrut[b] \\ \hline
Variant \uppercase\expandafter{\romannumeral1} & {\bf 78.60} & 54.11 \bigstrut[t] \bigstrut[b] \\ \hline
Variant \uppercase\expandafter{\romannumeral2} & {\bf 78.60} & {\bf 54.54} \bigstrut[t] \bigstrut[b] \\ \hline
\end{tabular}}
\caption{Comparison of various variations of \NAME{} and the CoT prompting baseline on StrategyQA using evidence paragraphs.
}
\label{table:proposal-variants}
\end{table} 



\paragraph{Results.} Table \ref{table:proposal-variants} illustrates that the fact selection and fact generation variants of our proposal improve the faithfulness of the supporting facts in explanations, leading to increased prediction accuracy compared to the basic approach without voting. Across all variations of our proposal, we observe significant improvements in both prediction accuracy and the faithfulness of explanations when compared to the CoT prompting baseline.

The incorporation of a voting mechanism leads to an increased prediction accuracy of $79.91\%$ for the basic approach. Comparison with the performance (i.e., $77.73\%$) of the same approach using retrieved paragraphs rather than evidence paragraphs in Table \ref{table:gpt3-results} demonstrates that retrieved paragraphs are also effective for our proposal, as both significantly outperform the voting baseline, self-consistency (i.e., $73.36\%$), as shown in Table \ref{table:gpt3-results}.

It is noteworthy that UnifiedQA performs poorly on StrategyQA, achieving an accuracy of only $58.95\%$. However, when provided with gold supporting facts in StrategyQA, UnifiedQA demonstrates excellent performance with an accuracy of $90.83\%$. This suggests that UnifiedQA is suitable for last-step inference, but not effective for answering questions in StrategyQA.

\subsection{Impact of the Size of LMs}
\label{subsec:model-size}
In this subsection, we examine the effect of the size of LMs on the performance of our proposed method, specifically in the context of the fact generation variant. We compare the performance of our method using various sizes of OPT models \cite{zhang2022opt} in addition to GPT-3 (175B) using the same experimental setup as in Section \ref{subsec:variations}. As shown in Figure~\ref{fig:model-size}, our proposed method (Variant II) consistently outperforms CoT prompting in terms of both prediction accuracy and the faithfulness of explanations, even when using smaller LMs.

