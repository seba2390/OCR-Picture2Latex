
\section{Experiments}
\label{sec:experiments}
In this section, we present the evaluation of our proposed method, \NAME{}, on three complex reasoning tasks: commonsense reasoning, temporal reasoning, and tabular reasoning. 
%We first describe the baselines for our experiments in Section \ref{subsec:baselines}. We then provide details on the specific experimental settings for each of the reasoning tasks we consider in Sections \ref{subsec:commonsense}-\ref{subsec:tabular}. The evaluation of different methods is presented in Section \ref{subsec:evaluation}.

\subsection{Baselines}
\label{subsec:baselines}
We compare with the following baselines.

\paragraph{Zero-shot/few-shot prompting.} In our experiments, we consider GPT-3 with standard zero-shot/few-shot prompting as baselines, following the approach described in \citet{brown2020language}, in which zero or few in-context exemplars of input-output pairs are provided in the prompt.

\paragraph{Chain-of-thought prompting.} In addition to the standard zero-shot/few-shot prompting, we also consider GPT-3 with the CoT prompting proposed in \cite{wei2022chain} as a baseline in our experiments. This approach involves feeding LLMs step-by-step reasoning examples instead of standard input-output examples.

\paragraph{Self-consistency.} In addition, we also consider self-consistency \cite{wang2022self} as a baseline in our experiments. This approach, proposed as an alternative to the naive greedy decoding used in CoT prompting \cite{wei2022chain}, involves sampling a diverse set of reasoning paths and selecting the most consistent answer by marginalizing the sampled paths. 

\subsection{Commonsense Reasoning}
\label{subsec:commonsense}

\paragraph{Dataset description.} For commonsense reasoning, we consider the StrategyQA dataset \cite{geva2021did}, which includes questions that require implicit reasoning strategies. For example, the question ``\textit{Did Aristotle use a laptop?}'' requires \textit{implicit} decomposition into reasoning steps, while the question ``\textit{Was Aristotle alive when the laptop was invented?}'' explicitly specifies the reasoning process. The StrategyQA dataset includes $2,290$ training examples, each consisting of a question (Q), a yes/no answer (A), a decomposition (D), evidence paragraphs (E), and supporting facts (F). On average, each question requires about $2.93$ reasoning steps and $2.33$ evidence paragraphs. In addition, a development set is constructed by randomly sampling $10\%$ of the training examples (i.e., $229$ examples). The answer distribution is roughly balanced, with approximately $47\%$ "yes" questions in both the training and development sets. Unless otherwise specified, the models are evaluated on the development set\footnote{As the annotations for the test set are not publicly available, we use the development set for evaluation. This allows us to perform a more comprehensive analysis.} for StrategyQA.

\begin{table*}[t]
\centering
\scalebox{1.0}{
\begin{tabular}{c|c|c|c|c}
\Xhline{2\arrayrulewidth}
 & Methods & Commonsense & Temporal & Tabular  \bigstrut[t] \bigstrut[b]   \\ \hline
 \multirow{5}{*}{GPT-3} & Zero-shot prompting & 58.08 &  28.40 & 82.00  \bigstrut[t]  \\
 & Few-shot prompting & 63.32 & 29.59  & 83.08 \\
 & Chain-of-thought prompting & 65.94  & 33.14 & 83.33 \\ 
 & Self-consistency & 73.36 & 37.28 & 84.00   \\
  & Rethinking with retrieval & {\bf 77.73} & {\bf 39.05} & {\bf 84.83} \bigstrut[b]  \\
\Xhline{2\arrayrulewidth}
\end{tabular}}
\caption{Performance of different methods using GPT-3 on three reasoning tasks.
}
\label{table:gpt3-results}
\end{table*}

\paragraph{Implementation details.} In this part, we utilize Wikipedia as the external knowledge base $\mathcal{KB}$. For each sentence in the explanation of every reasoning path, we first apply BM25 \cite{robertson2009probabilistic} to retrieve the top 10 most relevant paragraphs from Wikipedia. In particular, we use the re-implementation of the sparse retrieval BM25\footnote{We also experimented with DPR and BM25+DPR, and found that BM25 outperformed these methods in our experiments. More details can be found in Appendix \ref{subsec:retrieval-comparison}.} in \citet{karpukhin2020dense} from Pyserini \cite{Lin_etal_SIGIR2021_Pyserini}. Subsequently, we use the pre-trained MPNet model \cite{song2020mpnet} to select the most similar paragraph based on the cosine similarity between the sentence embeddings of the retrieved paragraph and the sentence. We then employ a pre-trained natural language inference (NLI) model \cite{nie2020adversarial} to obtain the entailment and contradiction scores for the sentence, treating the most similar paragraph as the premise. The faithfulness of each reasoning path is then calculated using $f_{\mathcal{KB}}(\cdot)$ based on the entailment scores, contradiction scores, and MPNet similarities of all sentences in the explanation of the reasoning path. The final prediction for each question is obtained through faithful inference (Equation \ref{eq:inference}). More details about $f_{\mathcal{KB}}(\cdot)$ can be found in Appendix \ref{subsec:faithfulness-functions}.


\subsection{Temporal Reasoning}
\label{subsec:temporal}

\paragraph{Dataset description.} In this experiment, we use the TempQuestions dataset \cite{jia2018tempquestions} to investigate temporal reasoning. This dataset includes $1,271$ temporal questions that are divided into four classes: explicit temporal, implicit temporal, temporal answer, and ordinal constraints. The questions are paired with their answers from Freebase \cite{bollacker2008freebase}. To examine the most challenging aspect of temporal reasoning, we focus on the set of \textit{implicit} temporal questions, which contain implicit temporal expressions, including free-text temporal expressions. For example, the question ``\textit{who was governor of oregon when shanghai noon was released?}'' is an implicit temporal question. To facilitate our analysis, we only consider questions with a single answer, resulting in a total of $175$ examples. Of these examples, the first $6$ are used for prompting, and the remaining $169$ are used for evaluation.

\paragraph{Implementation details.} In this part, we utilize Wikidata \cite{vrandevcic2014wikidata} as the external knowledge base $\mathcal{KB}$, as it is the largest publicly available knowledge graph, and the data from Freebase has been migrated to Wikidata. To incorporate this knowledge into our system, we apply an entity linking system\footnote{We use the spacy entity linker: \url{https://pypi.org/project/spacy-entity-linker/}. } to each sentence in the explanation of each reasoning path to identify the corresponding Wikidata pages for all entities in the sentence. Next, we extract all temporal relations from these relevant Wikidata pages and use templates to convert these temporal relations into sentences. This step generates a set of relevant knowledge sentences for each sentence in the explanation of each reasoning path. The final prediction is then obtained by applying the procedure described in Section \ref{subsec:commonsense}, in which the retrieved paragraphs are replaced with the relevant knowledge sentences from the current part.

\subsection{Tabular Reasoning}
\label{subsec:tabular}

\paragraph{Dataset description.} We consider the \textsc{INFOTABS} dataset \cite{gupta2020infotabs} for tabular reasoning, which consists of $23,738$ human-written textual hypotheses based on premises in the form of tables extracted from $2,540$ unique Wikipedia info-boxes. We focus on the development set, which includes $1,800$ hypotheses based on $200$ tables, and only consider entailed and contradictory hypotheses as it is tricky to write CoT demonstrations for neutral hypotheses. This results in a total of $1,200$ hypotheses based on $200$ tables for evaluation, with an equal number of entailed and contradictory hypotheses.

\paragraph{Implementation details.} In this part, we utilize WordNet \cite{miller1995wordnet} and ConceptNet \cite{speer2017conceptnet} as external knowledge bases. To convert tables into textual premises, we follow the same technique as in \citet{varun2022trans}. For each premise-hypothesis pair, we follow the procedure outlined in \citet{varun2022trans} to retrieve relevant word relation triples that connect the premise and hypothesis words, such as ``married''$ \xleftrightarrow{\text{RelatedTo}}$ ``spouse''. These triples are then converted into sentences using some simple templates. The resulting sentences, along with the textual premises from the tables, serve as relevant knowledge for each sentence in the explanation of each reasoning path. To obtain the final prediction, the procedure described in Section \ref{subsec:commonsense} is applied, whereby the retrieved paragraphs in Section \ref{subsec:commonsense} are replaced with the relevant knowledge from the current part.

\subsection{Evaluation}
\label{subsec:evaluation}

\paragraph{Experimental settings.} In all experiments, we utilize GPT-3 \texttt{text-davinci-002} unless otherwise stated. The maximum number of tokens for generation during completion is set to $256$. For zero-shot, few-shot, and chain-of-thought prompting, the temperature is fixed at $0$. For self-consistency and rethinking with retrieval, we randomly sample $10$ outputs\footnote{For commonsense reasoning, we sample $9$ outputs, as we have found that odd numbers of outputs tend to yield better voting performance for self-consistency on StrategyQA.} with temperature $0.7$. Detailed prompts can be found in Appendix \ref{subsec:prompts}. We evaluate the performance of different methods on commonsense and tabular reasoning using accuracy, and on temporal reasoning using the exact match metric as defined in \citet{rajpurkar2016squad}.

\paragraph{Results.} As shown in Table \ref{table:gpt3-results}, our proposed method, rethinking with retrieval, consistently outperforms all baselines on all three reasoning tasks without requiring additional training or fine-tuning. The results highlight the effectiveness of our approach in leveraging external knowledge to improve the performance of LLMs. 

