%% ****** Start of file apstemplate.tex ****** %
%%
%%
%%   This file is part of the APS files in the REVTeX 4.2 distribution.
%%   Version 4.2a of REVTeX, January, 2015
%%
%%
%%   Copyright (c) 2015 The American Physical Society.
%%
%%   See the REVTeX 4 README file for restrictions and more information.
%%
%
% This is a template for producing manuscripts for use with REVTEX 4.2
% Copy this file to another name and then work on that file.
% That way, you always have this original template file to use.
%
% Group addresses by affiliation; use superscriptaddress for long
% author lists, or if there are many overlapping affiliations.
% For Phys. Rev. appearance, change preprint to twocolumn.
% Choose pra, prb, prc, prd, pre, prl, prstab, prstper, or rmp for journal
%  Add 'draft' option to mark overfull boxes with black boxes
%  Add 'showkeys' option to make keywords appear
%\documentclass[aps,prl,preprint,groupedaddress]{revtex4-2}
%\documentclass[aps,prl,twocolumn,groupedaddress]{revtex4-2}
\documentclass[preprint,aps,pre,letterpaper,onecolumn,superscriptaddress]{revtex4-2} %PRE numbers sections if you want that

%\documentclass[aps,prl,preprint,superscriptaddress]{revtex4-2}
%\documentclass[aps,prl,reprint,groupedaddress]{revtex4-2}

% You should use BibTeX and apsrev.bst for references
% Choosing a journal automatically selects the correct APS
% BibTeX style file (bst file), so only uncomment the line
% below if necessary.
%\bibliographystyle{apsrev4-2}
\bibliographystyle{abbrv}

%Import standard packages
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage[margin=1in]{geometry}
\usepackage{graphicx}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{mathrsfs}
\usepackage{placeins}
\usepackage{caption3}
\usepackage{wrapfig}
\usepackage{caption} 
\usepackage{subcaption}
\usepackage{xcolor}
\usepackage{contour}
\usepackage{color}
\usepackage{outlines}
\usepackage[bookmarksnumbered=true,breaklinks=true,colorlinks,citecolor=blue,linkcolor=blue]{hyperref}

\usepackage[normalem]{ulem}
%\renewcommand{\sout}[1]{} % uncomment to remove struck-out text
\newcommand{\MDG}[1]{\textcolor{magenta}{*** #1 ***}}
%\renewcommand{\MDG}[1]{}% uncomment to remove MDG comments
\newcommand{\MDGrevise}[1]{\textcolor{blue}{#1}}
%\newcommand{\MDGrevise}[1]{\textcolor{black}{#1}}
\definecolor{ao(english)}{rgb}{0.0, 0.5, 0.0}
\newcommand{\AL}[1]{\textcolor{ao(english)}{*** #1 ***}}
\newcommand{\ALrevise}[1]{\textcolor{ao(english)}{#1}}
%\newcommand{\ALrevise}[1]{\textcolor{black}{#1}}

\newcommand{\IM}{\mathcal{M}}
\newcommand{\chitilde}{\check{\chi}}
% to put in notes for internal consumption
%\newcommand{\comment}[1]{\textcolor{red}{*** #1 ***}} % phase this out
\newcommand{\note}[1]{\textcolor{red}{*** #1 ***}} % phase this in
\newcommand{\sanote}[1]{~\\ \noindent\note{#1}\\~\\} % standalone note

% some custom headers and bullets to get around issues with itemize environment e.g.
\newcommand{\pheader}[1]{\noindent {\bf #1}}
\newcommand{\pbullet}{\pheader{$\bullet$~}}

\newenvironment{inprogress}{\color{blue} ~~\\ \noindent IN PROGRESS: START --- }{--- IN PROGRESS:END \\ \color{black}}
% blank versions to generate output without visible notes
%\newcommand{\comment}[1]{} % phase this out
%\newcommand{\note}[1]{} % phase this in
%\newcommand{\inprogress}[1]{}

\newcommand{\mbf}[1]{\boldsymbol{#1}}
\newcommand{\bsf}[1]{\mbf{\sf{#1}}}

%cmr vs txfonts
%\newcommand{\varv}{v} % comment out if using txfonts
% don't need to use \varv if using [varg] option in newtxmath
\newcommand{\reals}{\mathbb{R}} %this requires amssymb if not using txfonts

\newcommand{\Karman}{K\'arm\'an~}



%% commands for applying the following convention: Vectors are italic alphabetic characters. Second order tensors and higher are roman upper case alphabetic or greek. (The exception is angular velocity \index{angular velocity}/vorticity, which is greek vector (but it's actually a representation of an antisymmetric tensor so this violation of the convention is ok.) Concatenated vectors and tensors like for $N$-body problems are bold sans serif. I could try to reserve lower case for vector fields and upper case for point vectors (like point force, point dipole etc.)
\newcommand{\tA}{\mathbf{A}}
\newcommand{\tF}{\mathbf{F}}
\newcommand{\tB}{\mathbf{B}}
\newcommand{\tL}{\mathbf{L}}
\newcommand{\tE}{\mathbf{E}}
\newcommand{\tC}{\mathbf{C}}
\newcommand{\tG}{\mathbf{G}}
\newcommand{\tT}{\mathbf{T}}
\newcommand{\tP}{\mathbf{P}}
\newcommand{\tD}{\mathbf{D}}
\newcommand{\tR}{\mathbf{R}}
\newcommand{\tM}{\mathbf{M}}
\newcommand{\tW}{\mathbf{W}}
\newcommand{\tQ}{\mathbf{Q}}
\newcommand{\tS}{\mathbf{S}}
\newcommand{\tsigma}{\mbf{\sigma}}
\newcommand{\tzeta}{\mbf{\zeta}}

%%

%% finite fourier transform. We'll use $\hat$ for infinite-domain Fourier transform.   
\newcommand{\fft}[1]{\check{#1}}

%% Fourier transform
\newcommand{\FTt}[1]{F\left\{#1\right\}} %Fourier transform in time -- as yet we don't have symbols to distinguish between Fourier transforms in space and time
\newcommand{\LTt}[1]{L\left\{#1\right\}} %Laplace transform

%% complex conjugate
\newcommand{\cc}[1]{\bar{#1}}
%\newcommand{\cc}[1]{{#1}^{*}}

% transpose
\newcommand{\trans}{^\mathrm{T}}
% symmetric part
\newcommand{\sym}{\mathrm{S}}
%antisymmetric part
\newcommand{\anti}{\mathrm{A}}

\newcommand{\lambdav}{\lambda_{\mathrm{v}}}
\newcommand{\bv}{\mbf{\varv}}
\newcommand{\bx}{\mbf{x}}
\newcommand{\by}{\mbf{y}}
\newcommand{\br}{\mbf{r}}
\newcommand{\bb}{\mbf{b}}
\newcommand{\bX}{\mbf{X}}
\newcommand{\bY}{\mbf{Y}}
\newcommand{\bZ}{\mbf{Z}}
\newcommand{\bn}{\mbf{n}}
\newcommand{\bg}{\mbf{g}}
\newcommand{\bh}{\mbf{h}}
\newcommand{\bj}{\mbf{j}}
\newcommand{\bG}{\mbf{G}}
\newcommand{\bC}{\mbf{C}}
\newcommand{\bL}{\mbf{L}}
\newcommand{\bI}{\mbf{I}}
\newcommand{\bD}{\mbf{D}}
\newcommand{\bE}{\mbf{E}}
\newcommand{\bJ}{\mbf{J}}
\newcommand{\bM}{\mbf{M}}
\newcommand{\bT}{\mbf{T}}
\newcommand{\torque}{\bT}
\newcommand{\bR}{\mbf{R}}
\newcommand{\bW}{\mbf{W}}
\newcommand{\bU}{\mbf{U}}
\newcommand{\bff}{\mbf{f}}
\newcommand{\bfF}{\mbf{F}}
\newcommand{\Fc}{\mbf{F}_c} % connector force
\newcommand{\bc}{\mbf{c}}
\newcommand{\bp}{\mbf{p}}
\newcommand{\bd}{\mbf{d}}
%  \newcommand{\bm}{\mbf{m}}
\newcommand{\ba}{\mbf{a}}
\newcommand{\be}{\mbf{e}}
\newcommand{\bk}{\mbf{k}}
\newcommand{\bu}{\mbf{u}}
\newcommand{\bq}{\mbf{q}}
\newcommand{\bt}{\mbf{t}}
\newcommand{\bw}{\mbf{w}}
\newcommand{\grad}{\mbf{\nabla}} %grad
\newcommand{\lap}{\nabla^2} %Laplacian
\DeclareMathOperator{\tr}{Tr} %trace
\newcommand{\divg}{\grad\cdot} %divergence
\newcommand{\curl}{\grad\times} %curl
\newcommand{\erf}{\mathrm{erf}} %error function
\newcommand{\Rey}{\mathrm{Re}}
\newcommand{\El}{\mathrm{El}}
\newcommand{\Ex}{\mathrm{Ex}}
\newcommand{\Pe}{\textrm{P\'e}}
\newcommand{\Sr}{\mathrm{Sr}}
\newcommand{\De}{\mathrm{De}}
\newcommand{\Wi}{\mathrm{Wi}}
\newcommand{\Tr}{\mathrm{Tr}}
\newcommand{\cgrad}[2]{\frac{\partial{#1}}{\partial x_{#2}}}
\newcommand{\bsigma}{\boldsymbol{\sigma}}
\newcommand{\btau}{\boldsymbol{\tau}}
\newcommand{\bdelta}{\boldsymbol{\delta}}
\newcommand{\bomega}{\boldsymbol{\omega}}
\newcommand{\bphi}{\boldsymbol{\phi}}
\newcommand{\bOmega}{\boldsymbol{\Omega}}
\newcommand{\bzeta}{\boldsymbol{\zeta}}
\newcommand{\beps}{\boldsymbol{\epsilon}}
\newcommand{\bPhi}{\boldsymbol{\Phi}}
\newcommand{\bSigma}{\boldsymbol{\Sigma}}
\newcommand{\abs}[1]{\left|#1\right|}
\newcommand{\llangle}{\left\langle}
\newcommand{\rrangle}{\right\rangle}
\newcommand{\rangleQ}{\rangle_Q}
\newcommand{\submax}{\textrm{max}}

%% ensemble average
\newcommand{\Eavg}[1]{\llangle #1 \rrangle}
\newcommand{\cov}{\textrm{cov}}
\newcommand{\var}{\textrm{var}}
\newcommand{\Prob}{\textrm{Pr}}


\newcommand{\bzero}{\mbf{0}}
\newcommand{\gammadot}{\dot{\gamma}}
\newcommand{\epsilondot}{\dot{\epsilon}}
\newcommand{\nhat}{\hat{n}}
\newcommand{\sgn}{\textrm{sgn}}
\newcommand{\expe}{\mathcal{E}}
\newcommand{\vperp}{\varv^\perp}
% short course notes

%\newcommand{\bff}{\mathbf{f}}
%\newcommand{\bv}{\mathbf{v}}
%\newcommand{\br}{\mathbf{r}}
%\newcommand{\bx}{\mathbf{x}}
%\newcommand{\bu}{\mathbf{u}}
%\newcommand{\bq}{\mathbf{q}}
%\newcommand{\bR}{\mathbf{R}}
\newcommand{\bF}{\mathbf{F}}
\newcommand{\bV}{\mathbf{V}}
%\newcommand{\bJ}{\mathbf{J}}
%\newcommand{\bW}{\mathbf{W}}
\newcommand{\Acal}{\mathcal{A}}
\newcommand{\Dcal}{\mathcal{D}}
\newcommand{\Bcal}{\mathcal{B}}
\newcommand{\bAcal}{\boldsymbol{\mathcal{A}}}
\newcommand{\bDcal}{\boldsymbol{\mathcal{D}}}
%\newcommand{\bD}{\boldsymbol{D}}
\newcommand{\bB}{\boldsymbol{B}}
\newcommand{\bA}{\boldsymbol{A}}
\newcommand{\bBcal}{\boldsymbol{\mathcal{B}}}
\newcommand{\balpha}{\boldsymbol{\alpha}}

%% CONVENTIONS
% fluid velocity is \mbf{v}
% force density will be \mbf{f}
% force (e.g. on a particle) will be upper case: \mbf{F} 
% velocities of rigid particles will be upper case \mbf{U}
% N-particle positions, velocities etc will be represented with sans serif characters
% I could represent 3x3 matrices (tensor) by bold roman (\boldsymbol) with vectors as bold italic (\mathbf) 
\newcommand{\Up}{\mbf{U}} % particle velocity
\newcommand{\Omp}{\bOmega} % particle angular velocity \index{angular velocity} 
\newcommand{\Omf}{\bomega} % fluid angular velocity \index{angular velocity}
\newcommand{\tVort}{\tW} % vorticity tensor 
\newcommand{\vort}{\bw} % vorticity vector
\newcommand{\Fp}{\bfF} %force on a particle
\newcommand{\Rp}{\mbf{R}} %position of a particle
\newcommand{\Qp}{\mbf{Q}} %connector vector
\newcommand{\bQ}{\mbf{Q}} %connector vector
\newcommand{\gradRp}{\frac{\partial}{\partial\Rp}} %gradient with respect to particle position
\newcommand{\gradR}{\frac{\partial}{\partial R}} %gradient with respect to particle position in 1D


%%sub and superscripts
\newcommand{\dom}{\mathrm{D}} %to signify a bounded domain 
\newcommand{\mat}{\mathrm{M}} %to signify a material volume 
\newcommand{\grav}{\mathrm{g}} %to signify a gravitational effect 
\newcommand{\wall}{\mathrm{W}} %to signify a wall effect 
\newcommand{\di}{\mathrm{d}} % to signify a dipole 
\newcommand{\particle}{\mathrm{P}} % to signify a particle
\newcommand{\loc}{\mathrm{l}} % to signify a local part
\newcommand{\glob}{\mathrm{g}} % to signify a global part
\newcommand{\ro}{\mathrm{r}} % to signify a rotational quantity
\newcommand{\tot}{\mathrm{t}} % to signify a total quantity
\newcommand{\hor}{\mathrm{H}} % to signify a horizonal quantity (e.g. in lubrication theory)
\newcommand{\bdd}{\mathrm{B}} % to signify a bounded domain (e.g. in Green's function)
\newcommand{\am}{\mathrm{a}} % to signify added mass. 
\newcommand{\hist}{\mathrm{h}} % to signify history. 
\newcommand{\stk}{\mathrm{s}} % to signify steady stokes mass. 
\newcommand{\lift}{\mathrm{lift}} % to signify lift  
\newcommand{\chr}{\mathrm{c}} % to signify ``characteristic''  
\newcommand{\Kuhn}{\mathrm{K}} % to signify ``Kuhn''  
\newcommand{\pol}{\mathrm{p}} % to signify ``polymer''  
\newcommand{\sol}{\mathrm{s}} % to signify ``polymer''  
\newcommand{\STR}{\mathrm{STR}} % to signify ``stresslet''
\newcommand{\ROT}{\mathrm{ROT}} % to signify ``rotlet''



\newcommand{\eq}{\mathrm{eq}} % to represent equilibrium
\newcommand{\mol}{\mathrm{mol}} % molecules
\newcommand{\fluc}{\textrm{fluc}} % for fluctuating components
\newcommand{\ext}{\mathrm{ext}} % for external forces
\newcommand{\ev}{\mathrm{ev}} % for excluded volume forces
\newcommand{\equil}{\mathrm{eq}} % for properties evaluated at equilbrium
\newcommand{\equilnull}{{~}} % for properties evaluated at equilbrium<==if we decide not to use the equil subscript
\newcommand{\eff}{\mathrm{eff}} % for effective properties
\newcommand{\Brown}{\mathrm{Brown}} % for ensemble-averaged Brownian forces
\newcommand{\os}{\mathrm{os}} % for osmotic forces
\newcommand{\bend}{\mathrm{bend}} % for bending
\newcommand{\kin}{\mathrm{kin}} % for effects related to ensemble-averaged kinetic energy (e.g. osmotic pressure)
\newcommand{\spring}{\mathrm{spring}} % for  spring forces
\newcommand{\sub}{\mathrm{sub}} % for forces between ends of a subchain
\newcommand{\drag}{\mathrm{drag}} % for  drag forces
\newcommand{\con}{\mathrm{con}} % for  constraint forces
\newcommand{\rf}{\mathrm{frame}} % for moving reference frames
\newcommand{\Wp}{\mbf{W}} % 3D Wiener process
\newcommand{\UN}{\bsf{U}} %N-particle velocity
\newcommand{\VN}{\bsf{V}} %N-particle fluid velocity
\newcommand{\FN}{\bsf{F}} %N-particle force
\newcommand{\LN}{\bsf{L}} %N-particle torque
\newcommand{\SN}{\bsf{S}} %N-particle stresslet
\newcommand{\RN}{\bsf{R}} %N-particle position
\newcommand{\BN}{\bsf{B}} %N-particle Brownian term
\newcommand{\CN}{\bsf{C}} %N-particle Brownian term
\newcommand{\DN}{\bsf{D}} %N-particle diffusion tensor
\newcommand{\WN}{\bsf{W}} % N-particle Wiener process
\newcommand{\MN}{\bsf{M}} % N-particle mobility
\newcommand{\QN}{\bsf{Q}} % N-particle connector vector
\newcommand{\TN}{\bsf{T}} % N-particle torque
\newcommand{\XN}{\bsf{X}} % N-particle vector of independent Gaussians N(0,1)
\newcommand{\gradRN}{\frac{\partial}{\partial\RN}} %N-particle gradient with respect to particle position
\newcommand{\lc}{L} % contour length
\newcommand{\lk}{L_{K}} % Kuhn length
\newcommand{\kT}{k_{B}T} % Boltzmann const times temp

%% operators
\newcommand{\oproj}{\left(\bdelta-\bu\bu\right)\cdot}

%% UNITS
%\newcommand{\nm}{~\textrm{nm}}
\newcommand{\micron}{\;\mu\mathrm{m}}
\newcommand{\nm}{\;\mathrm{nm}}
\newcommand{\mPas}{\;\mathrm{mPa~s}}
\newcommand{\invsec}{\;\mathrm{s}^{-1}}
\newcommand{\degree}[1]{#1\; ^\circ\mathrm{C}}

%% variables for graphics
\newlength{\figwidth}
\figwidth=4in
\newlength{\SCwidth}
\SCwidth=2.2in


% OTHER
% CAUCHY PRINCIPAL VALUE INTEGRAL \dashint
% From The LATEX Mathematics Companion, Helin Gai, Trinity College of Arts and Sciences Duke University
\def\Xint#1{\mathchoice
	{\XXint\displaystyle\textstyle{#1}}%
	{\XXint\textstyle\scriptstyle{#1}}%
	{\XXint\scriptstyle\scriptscriptstyle{#1}}%
	{\XXint\scriptscriptstyle\scriptscriptstyle{#1}}%
	\!\int}
\def\XXint#1#2#3{{\setbox0=\hbox{$#1{#2#3}{\int}$}
		\vcenter{\hbox{$#2#3$}}\kern-.5\wd0}}
\def\ddashint{\Xint=}
\def\dashint{\Xint-}

\begin{document}

% Use the \preprint command to place your local institutional report
% number in the upper righthand corner of the title page in preprint mode.
% Multiple \preprint commands are allowed.
% Use the 'preprintnumbers' class option to override journal defaults
% to display numbers if necessary
%\preprint{}

%Title of paper
\title{Data-Driven Reduced-Order Modeling of Spatiotemporal Chaos with Neural Ordinary Differential Equations}
%\title{Minimal Dimensional Evolution of the Kuramoto-Sivashinsky Equation with Neural ODEs} %MDG

% repeat the \author .. \affiliation  etc. as needed
% \email, \thanks, \homepage, \altaffiliation all apply to the current
% author. Explanatory text should go in the []'s, actual e-mail
% address or url should go in the {}'s for \email and \homepage.
% Please use the appropriate macro foreach each type of information

% \affiliation command applies to all authors since the last
% \affiliation command. The \affiliation command should follow the
% other information
% \affiliation can be followed by \email, \homepage, \thanks as well.
\author{Alec J. Linot}
\affiliation{Department of Chemical and Biological Engineering, University of Wisconsin-Madison, Madison WI 53706, USA}

\author{Michael D. Graham}
\email{mdgraham@wisc.edu}
\affiliation{Department of Chemical and Biological Engineering, University of Wisconsin-Madison, Madison WI 53706, USA}

%Collaboration name if desired (requires use of superscriptaddress
%option in \documentclass). \noaffiliation is required (may also be
%used with the \author command).
%\collaboration can be followed by \email, \homepage, \thanks as well.
%\collaboration{}
%\noaffiliation

\date{\today}

%\section{Outline}
%
%\begin{outline}[enumerate]
%	\1 Introduction
%		\2 We want to generate models of systems from data
% 			\3 Allows us to predict future states
% 			\3 Useful when used to design control schemes
% 			\3 Useful if you want to know the state between data samples
% 		\2 Generating models from data is beneficial when we don't have the governing eq or when the governing equations are expensive to integrate
% 			\3 We will consider data that comes from measuring the state at given times $u(t_i)\in \mathbb{R}^d$
% 			\3 $u$ can also come from time delays or be homeomorphic to the state
% 		\2 For many systems the data can be high dimensional and chaotic
% 			\3 high d state space examples
% 			\3 nonlinear dynamics leads to the chaos
% 			\3 many systems esp. dissipative long-time dynamics live on a manifold of dimension dm<d
% 		\2 Due to the high dimension it is natural to find a low dimensional representation $h(t_i)\in \mathbb{R}^{d_h}$
%  			\3 Frequently the reduction in dimension is linear, but this can limit how far the dimension can be reduced
%  				\4 The dimension being subminimal is due to Whitney's
%  				\4 \cite{Cunningham2015} gives an overview of linear dimension reduction techniques.
%  				\4 One of the most common methods is PCA, which finds the matrix $U\in\mathbb{R}^{d \times d_h}$ that minimizes $||u-Uh|||$, where $h=U^Tu$.
% 			\3 Nonlinear dimension reduction expands the possible state representations
% 				\4 \cite{VanDerMaaten2009} gives an overview of linear and nonlinear dimension reduction techniques.
% 				\4 Cite autoencoders, diffusion maps, LLE, tSNE
% 		\2 Time evolution can either be discrete or continuous and either markov or non-markov
% 			\3 A Markovian represenatation can use a time-delayed state or measurements of the state
% 		\2 Linear methods like DMD and autoregressive models (e.g. ARMAX) do not work when modeling chaotic dynamics
% 		\2 Methods that have shown success at predicting chaotic dynamics without the governing eq
% 			\3 Reservoir computing 
% 				\4 \cite{Pathak2018a} used reservoir computing to predict chaotic dynamics of the KSE
% 			\3 Recurrent NN (e.g. LSTM)
% 				\4 In \cite{Vlachas2018a} they show an LSTM can predict the short-time behavior of the chaotic dynamics of the Lorenz-96, the KSE, and a prototype climate model
% 				\4 In \cite{Vlachas2019} LSTM and Reservoir computing are compared on the Lorenz-96 and KSE. Reservoir computing is better when the full state is known and LSTMs are better for reduced order data. 
% 			\3 A drawback of these methods are they are not Markov so the state at any given time depends on multiple past states. This means the dimension of the state at any time is still high and starting up these processes requires multiple known states.
% 			\3 Dense NN 
% 				\4 \cite{Gonzalez-Garcia1998} used a dense NN to output an estimate of $du/dt$ that was used in a RK4 timestepper to evolve a periodic orbit of the KSE
% 				\4 \cite{Raissi2018} also used a dense NN to output $du/dt$ but estimated it from data using a multistep method
% 				\4 In \cite{Linot2020} dense NN are used to predict the discrete dynamics of the KSE
% 			\3 Neural ODE
% 		\2 We are going to do both dimension reduction and time evolution of high-dim problems with chaotic dynamics. These dynamics will come from the KSE
% 		
%	\1 Framework
%		\2 Describe framework
%			\3 Find a transformation to the manifold coordinates from the data and back ($\chi$ and $\check{\chi}$)
%			\3 Use the data in the manifold coordinates to find a the time evolution ($dw/dt=v(w)$) where $w$ is either $u$ or $h$ and $v$ is either $f$ or $g$
%			\3 Take new IC, transform them to the manifold coordinates, evolve them forward in time, and put them back in the full coordinates
%		\2 Dimension Reduction with autoencoders
%			\3 The objective is to minimize a reconstruction loss 
%			\3 Weights are optimized using a stochastic gradient descent method
%			\3 Autoencoders have been used to reduce dimension for flow around a cylinder, flow around a flat plate \cite{Nair2020}, and Kolmogrov flow \cite{Page2020}
%		\2 Time Evolution
%			\3 We assume that the dynamics of our data can be reconstructed by an ODE $du/dt=f(u)$
%			\3 The ODE in the reduced space can be written $dh/dt=g(h)$
%			\3 We use a NN to model this ODE
%				\4 \cite{Chen2019} showed how the derivative can be estimated using the adjoint method
%				\4 \cite{Gholami2019} showed how estimates of the gradient can be improved
%				\4 We use backpropagation instead of the adjoint because memory constraints are not an issue here
%			\3 Neural ODEs have shown success for problems that don't exhibit chaos
%				\4 \cite{Rojas2021} used NODE to predict the periodic dynamics of flow around a cylinder
%				\4 \cite{portwood2019turbulence} used NODE to predict the kinetic energy and dissipation as a function of time for decaying turbulence.
%			\3 We investigate the effectiveness of this method for chaotic systems
%				
%\end{outline}

\begin{abstract}
Dissipative partial differential equations that exhibit chaotic dynamics tend to evolve to attractors that exist on finite-dimensional manifolds. We present a data-driven reduced order modeling method that capitalizes on this fact by finding the coordinates of this manifold and finding an ordinary differential equation (ODE) describing the dynamics in this coordinate system. The manifold coordinates are discovered using an undercomplete autoencoder -- a neural network (NN) that reduces then expands dimension. Then the ODE, in these coordinates, is approximated by a NN using the neural ODE framework. Both of these methods only require snapshots of data to learn a model, and the data can be widely and/or unevenly spaced. We apply this framework to the Kuramoto-Sivashinsky for different domain sizes that exhibit chaotic dynamics. With this system, we find that dimension reduction improves performance relative to predictions in the ambient space, where artifacts arise. Then, with the low-dimensional model, we vary the training data spacing and find excellent short- and long-time statistical recreation of the true dynamics for widely spaced data (spacing of $\sim$0.7 Lyapunov times). We end by comparing performance with various degrees of dimension reduction, and find a ``sweet spot" in terms of performance vs.\ dimension.
\end{abstract}

% insert suggested keywords - APS authors don't need to do this
%\keywords{}

%\maketitle must follow title, authors, abstract, and keywords
\maketitle

% body of paper here - Use proper section commands
%2

\section{Introduction} \label{sec:Intro}

A common question in many applications is: given a time series of measurements on a system how can a predictive model be generated to estimate future states of the system? For problems like weather forecasting, just knowing the future state of the system is useful. In other problems, like minimizing turbulent drag on an aircraft, models are desirable for creating control policies. These models can sometimes be generated from first principles, but insufficient information on the system often limits the ability to write them explicitly. Even when a model can be written out explicitly, it may be very high-dimensional and computationally expensive to simulate. Thus it is often desirable to generate a low-dimensional model from data.

We consider data sets $\left\{u(t_1), u(t_2), \ldots, u(t_N)\right\}$, where $u(t_i)\in \mathbb{R}^d$  comes from measuring the state of a system at a given time $t_i$. The full state at a given time can be represented by direct measurements (e.g.\ position and velocity of a pendulum) or by a representation that is diffeomorphic to the state. One such representation is time delay measurements of the system, as shown by Takens \citep{Takens}. In the case of the pendulum, for example, this could correspond to writing the state as $u(t)=[\theta(t),\theta(t-\tau),\theta(t-2\tau),\theta(t-3\tau)]$, where $\theta$ is the angle of the pendulum and $\tau$ is the delay time. If $u$ lies on a $d$-manifold then a time delay representation in $\mathbb{R}^{2d+1}$ is diffeomorphic to u \citep{Takens}.  %\MDG{add -- if $u\in \reals^d$, then how many scalar time delay measurements are required? } 

For many systems of interest the state is high-dimensional and comes from a chaotic time series \cite{Ivancevic2007}. 
%Some examples of systems that can exhibit this behavior include turbulence, molecular dynamics, and the weather \cite{Ivancevic2007}.
%cardiac arrhythmias
%Some examples of high-dimensional systems include turbulence, plasmas, and nonlinear optics \cite{Strogatz1994}.
%Furthermore, many of these systems include nonlinearities that result in chaotic behavior. 
%\MDG{rewrite the following sentence -- it's awfully bold, especially since you haven't cited any references} 
Despite this complex behavior, when these systems are dissipative the long-time dynamics often lie on a smooth invariant finite-dimensional inertial manifold $\mathcal{M} \subset \mathbb{R}^d$ of a much lower dimension ($d_\mathcal{M}$) than that of the state \cite{Temam1990}. 
%Many dissipative systems that exhibit chaotic dynamics are known to collapse at long times to a smooth invariant finite-dimensional manifold called an inertial manifold (IM) \cite{Temam1990}. %These manifolds provide an integer dimension, as opposed to the fractal dimension of the global attractor. 
Two such systems are the Kuramoto-Sivashinsky equation (KSE) \cite{Foias1988a,Temam1994,Jolly2000,Zelik2014} and the complex Ginzburg-Landau equation \cite{Doering1988}. Similarly, for the Navier-Stokes in two and three spatial dimensions, it has been shown that there is an approximate inertial manifold \cite{Foias1988,Temam1989}. It is approximate in the sense that there are small variations in many dimensions that can be assumed to be zero and still provide an accurate approximation of the state.

With a mapping to the manifold coordinate system the state can be represented (at least locally) in the manifold coordinates $h(t)\in\mathbb{R}^{d_\IM}$. That is, mappings $\chi$ and $\check{\chi}$ exist such that  $h=\chi(u)$ and $u=\check{\chi}(h)$. 
%In the model reduction literature $\chi$ and $\check{\chi}$ are sometimes called ``restriction" and ``lifting" operators, respectively \MDG{need a citation}. 
In the machine learning literature, $\chi$ and $\check{\chi}$ correspond to the the encoder and decoder, respectively, of a so-called undercomplete autoencoder structure \cite{Linot2020}, as we further discuss below. It should be noted that  there is no guarantee that $\IM$ can be \emph{globally} represented with a cartesian representation in $d_\IM$ dimensions. Indeed in general this cannot be done, and an ``atlas" of overlapping local representations, or ``charts", must be used \cite{lee2003}.  The application considered here will not require this more general formalism, but for related work using charts, see \cite{floryan2021charts}. Our aim here is to use data from a spatiotemporally chaotic system to learn the mappings  $\chi(u)$ and $\check{\chi}(h)$ back and forth between $\reals^d$ and a coordinate system on $\IM$ and then to learn the evolution equation for the dynamics in this coordinate system. This will be a minimal-dimensional representation for the dynamics of the system in question. We first introduce some background for the dimension-reduction problem, and then for the time-evolution problem. 

%\sout{Finding this coordinate transformation is a difficult problem, and many approaches have been considered.} 
Dimension-reduction is a challenging and widely-studied problem, and many approaches have been considered.  Frequently a linear dimension reduction technique is used (i.e.\ $\IM$ is taken to exist in a linear subspace of $\mathbb{R}^d$). This choice can be rationalized by invocation of Whitney's theorem \cite{Whitney1944}: any smooth $d_\IM$-manifold can be embedded into $\mathbb{R}^{2d_\IM}$.  Sauer et al.\ later refined this result by showing this embedding can be performed by almost every smooth map from a $d_\IM$-manifold to $\mathbb{R}^{n}$ for $n>2d_b$ \cite{Sauer1991}, where $d_b$ is the box-counting dimension of the attractor that lies on the manifold $\IM$. The box-counting dimension is a fractal dimension that must be less than the manifold dimension.
	%, so it also holds that almost every linear transformation is an embedding from $\IM$ to $\mathbb{R}^{2d_\mathcal{M}+1}$. 
	Thus, for almost every smooth map, including linear ones, $h \in \mathbb{R}^{2d_\mathcal{M}+1}$ contains the same information as $u$ (i.e. a map exists for reconstructing $u$ from $h$). 
%\AL{Rewrote to try and clarify. The conclusion that a manifold can be embedded in $\mathbb{R}^{2d_\IM}$ is different than saying a linear mapping is an embedding from $\IM$ to $\mathbb{R}^{2d_\IM}$. I reference Sauer because it sharpens the result for fractal dimensions, and more explicitly shows almost any map (including linear ones) is an embedding.}

%\MDG{I don't understand the following sentence. The way I understand Whitney is that for any $d_\mathcal{M}$-manifold $\mathcal{M}$, there is a diffeomorphism to $\reals^{2d_\IM}$. The original theorem was for $d>2d_\IM$, but it was later refined to $d\geq 2d_\IM$ -- rewrite the following  accordingly} 
%\AL{$d_h$ and $d_\IM$ need to be separated here. The confusion is that just because a diffeomorphism exists when $d_h\geq 2d_\IM$ doesn't mean anything unless you can actually find that mapping. In Whitney there is a more generic form of F, but in Sauer they show that almost every smooth map satisfies this. So linear mappings must also satisfy this.}
%Whitney \cite{Whitney1936} showed a generic smooth map is a diffeomorphism for $d>2d_\mathcal{M}$. Sauer et al. extended Whitney's work, showing that almost every smooth map for $d>2d_b$ is one-to-one \cite{Sauer2008}, where $d_b$ is the box-counting dimension of the attractor the data lies on. The box-counting dimension is a fractal dimension that must be less than the manifold dimension, so it also holds that almost every linear transformation is one-to-one for $d>2d_\mathcal{M}$. Thus, for almost any map, including linear ones, $h \in \mathbb{R}^{2d_\mathcal{M}+1}$ contains the same information as $u$ (i.e. a map exists for reconstructing $u$ from $h$). 

Cunningham and Ghahramani \cite{Cunningham2015} give an overview of linear dimension techniques from a machine learning perspective. Many of these collapse to one of the most common methods -- principal component analysis (PCA). In PCA the linear transformation that minimizes the reconstruction error or maximizes the variance in the data is found. This transformation comes from projecting data onto the leading left singular vectors $U\in\mathbb{R}^{d\times d_h}$ of the centered snapshot matrix $X=[u(t_0)-\left<u\right>,...,u(t_N)-\left<u\right>]$ (here $\left<.\right>$ denotes the mean). This is equivalent to finding the eigenvectors of the covariance matrix $X X^T$. Similarly, the eigenvectors of $X^T X$ can be found and related to the eigenvectors of $X X^T$ through the singular value decomposition. This approach is sometimes known as classical scaling \cite{VanDerMaaten2009}. The projection onto these modes is $\tilde{u}=U U^T u$, where $\sim$ denotes the approximation of the state $u$. In this projection, $h=U^T u$ is the low-dimensional representation. Although this projection is the optimal linear transformation for minimizing the reconstruction error, $h$ may still require as many as $2d_\mathcal{M}+1$ dimensions to contain the same information as $u$.

Finding a \emph{minimal} representation in general requires a nonlinear transformation. Many different techniques exist for nonlinear dimension reduction, often with the goal of preserving some property of the original dataset in the lower dimension. Some popular methods include kernel PCA, diffusion maps, local linear embedding (LLE), and t-distributed stochastic neighbor embedding (tSNE) \cite{VanDerMaaten2009}. In kernel PCA the matrix $X^T X$ is replaced with a kernel matrix $K(u_i,u_j)$. This can be viewed as an application of the ``kernel trick" on the the covariance matrix between data that has been mapped into an higher-dimensional (often infinite-dimensional) ``feature space" \cite{VanDerMaaten2009}. The low-dimensional representation is then computed using the eigenvectors such that $h=[\sum_i^d v_{i,1} K(u_i,u),...,\sum_i^d v_{i,d_h} K(u_i,u)]$, where $v_{i,k}$ is the \emph{i}th element of the \emph{k}th eigenvector of $K(u_i,u_j)$.

Similarly, diffusion maps and LLE can be viewed as extensions of kernel PCA with special kernels \cite{VanDerMaaten2009}. In diffusion maps a Gaussian kernel is applied to the data giving the matrix $A=\exp (-||u_i-u_j||/2\epsilon)$, where $\epsilon$ is a tuning parameter that represents the local connectivity \cite{Ferguson2011}. Then the dimension reduction is performed by computing the eigenvalue decomposition of this matrix (normalized so columns add to one) giving $h_i=[v_{i,2},...,v_{i,d_h+1}]$. The first eigenvector is the trivial all-ones vector \cite{Ferguson2011}.
In LLE, a linear combination of the $k$ nearest neighbors to a data point are used to minimize $\sum_i||u_i-\sum_jW_{i,j}u_j||^2$, where $W_{i,j}=0$ if $u_j$ is not a neighbor of $u_i$. Then the low-dimensional representation $h$ is calculated to minimize $\sum_i||h_i-\sum_jW_{i,j}h_j||^2$ using the $W$ calculated in the first step \cite{Roweis2323}. The solution that minimizes this cost function can be found by computing the eigenvectors of $(I-W)^T(I-W)$, and letting $h_i=[v_{i,1},...,v_{i,d_h}]$.

%In LLE the $d_h$ nearest neighbors of a data point $u_i$ are found and the optimal linear projection of this neighborhood is used to reconstruct the data. This means the dimension reduction in this case is $h_i=W_iX$ where the weight matrix $W_i\in \mathbb{R}^{d\times d_h}$ 
The last algorithm we mention here is tSNE \cite{Hinton2003}. Unlike the previous methods which use the eigenvalue decompositions to solve the optimization problem, tSNE uses gradient descent. In tSNE, the objective is to match the distribution of the data in the high-dimensional space to the data in the low-dimensional space. This is done by finding $h$ that minimizes the Kullback-Leibler (KL) divergence $\sum_i \sum_j p_{i,j} \log p_{i,j}/q_{i,j}$, where $p$ and $q$ are the distributions of the high- and low-dimensional data respectively. In tSNE these distributions are approximated by $p=\exp (-||u_i-u_j||^2/2\sigma^2)/\sum_{k\neq l}\exp (-||u_k-u_l||^2/2\sigma^2)$ and $q=(1+||h_i-h_j||^2)^{-1}/\sum_{k\neq l}(1+||h_k-h_l||^2)^{-1}$. 
%\sout{$h$ is found by randomly initializing values and using gradient descent to minimize the KL divergence.}
%The KL divergence is minimized by randomly initializing values of $h$ and using gradient descent. 
There is no guarantee in finding a global minimum or in finding the same solution every time. tSNE is frequently used for visualizing high-dimensional data in two or three dimensions, because it often separates complex data sets out into visually distinct clusters.

A few major drawbacks of these nonlinear dimension reduction techniques are they do not provide the function $\check{\chi}$, which reconstructs $u$ from $h$, they do not provide a method for adding out-of-sample data, and they do not scale well for large amounts of data. Except for tSNE, the Nystr$\ddot{\text{o}}$m extension can be used to resolve the out-of-sample problem for these methods \cite{Bengio2004}. However, using the Nystr$\ddot{\text{o}}$m extension results in different functions for $\chi$ depending on whether the data is in-sample or out-of-sample.

Because of these limitations, instead of using one of the above techniques, we tackle the dimension reduction problem directly, by approximating the mappings to the manifold coordinates $\chi$ and back $\check{\chi}$ as NNs. I.e., we use an undercomplete autoencoder \cite{Hinton2006,IanGoodfellowYoshuaBengio2017}. 
%\sout{In an undercomplete autoencoder, the mappings to the manifold coordinates $\chi$ and back $\check{\chi}$ are directly approximated by neural networks. } 
We describe autoencoders in more detail in Section \ref{sec:Framework}.

We now turn to developing, from data, dynamical models for the evolution of a state $u(t)$.  We consider systems that display deterministic, Markovian dynamics, so if $u(t)$ is the full state (either directly or diffeomorphically through embedding of a sufficient number of time delays), then the
%\sout{After determining an appropriate state representation, we now want a method for predicting the time evolution. Time evolution can be performed on $u$ or $h$. First we described situations where the time evolution involves $u$. The} 
dynamics can either be represented by a discrete time map 
\begin{equation}\label{eq:discrete}
	u(t+\tau)=F(u(t)),%\sout{,u(t-\tau),...)}
\end{equation}
or 
%\sout{ with continuous time with the}
an ordinary differential equation (ODE)
\begin{equation}\label{eq:ODE}
	\dfrac{d u}{d t}=f(u).
\end{equation} 
The learning goal is an accurate representation of $F$ or $f$, or their lower-dimensional analogues.
%\sout{If the evolution depends on the current and previous states (i.e.~ $u(t-\tau),u(t-2\tau),...$) then the process is not Markov, and if it depends on only the current state it is Markov. Our focus is on Markov processes.}

We consider first the discrete-time problem. This is easier because that is the format in which we usually have data. For simple systems that decay to a fixed point or display quasiperiodic dynamics with discrete frequencies, linear discrete-time methods, like dynamic mode decomposition (DMD) \cite{DMDBook},
%and autoregressive models (e.g.~ ARMA)\MDG{citation}, 
can predict dynamics. However, these models are unable to predict the long-time behavior of chaotic systems, which is our interest here.

Predicting chaotic dynamics requires a nonlinear model. Among the most successful methods for predicting chaotic dynamics are reservoir networks \cite{Pathak2018a} and recurrent neural networks (RNN) \cite{Vlachas2018,Vlachas2019}. These methods use discrete time maps, are non-Markovian, and typically increase the dimension of the state.
Reservoir networks work by evolving forward a high-dimensional reservoir state $r(t)\in \mathbb{R}^{d_r}$, and finding a mapping from $r$ to $u$. This reservoir state is evolved forward in time by some function $r(t+\tau)=G(r(t),W_\text{in}u(t))$, where $G$ and $W_\text{in}$ are chosen a priori. Then, the task is finding the optimal parameters $p$ in $\tilde{u}(t+\tau)=W_\text{out}(r(t+\tau);p)$ that minimize $\left<||u(t+\tau)-\tilde{u}(t+\tau)||^2\right>$. This is non-Markovian because the prediction of the state $u(t+\tau)$ depends on the previous $u(t)$ and $r(t)$.
 In Pathak et al.\ \cite{Pathak2018a} a reservoir network was used to predict the chaotic dynamics of the KSE. For data with a state dimension $d=64$, they choose a reservoir dimension $d_r=5000$.  That is, here  the dimension of the representation has not been reduced, but rather expanded, by two orders of magnitude.
 %Then a the state is recreated from the reservoir state with an equation that is linear in parameters $p$ giving $\tilde{u}(t+\tau)=W_\text{out}(r(t),p)$. For example, Pathak et al. use $u(t+\tau)=P_1r(t)+P_2r(t)^2$. With this method $G$ and $W_\text{in}$ are assigned and $W_\text{out}$ is optimized so that the difference between the prediction and the data is minimized $\left<||u(t)-\tilde{u}(t)||^2\right>$. 

Similar to reservoir networks, RNNs have a hidden state $h_r\in \mathbb{R}^{d_{h_r}}$ that is evolved forward in time. The hidden state is evolved forward by $h_r(t+\tau)=\sigma_h(u(t),h_r(t);W_h)$, and the future state is estimated from the hidden state $\tilde{u}(t+\tau)=\sigma_u(h_r(t+\tau);W_u)$. The functions $\sigma_h$ and $\sigma_u$ take different forms depending upon the type of RNN -- two examples are the long short-term memory (LSTM) \cite{Hochreiter1997} and the gated recurrent unit \cite{cho-etal-2014-properties}. Regardless of architecture, the functions $\sigma_h$ and $\sigma_u$ are constructed from NN with parameters $W_h$ and $W_u$. 
%\sout{NNs are parametric function approximators whose parameters are optimized by stochastic gradient descent to minimize a loss.} 
These parameters come from minimizing the same loss as in reservoir computing $\left<||u(t+\tau)-\tilde{u}(t+\tau)||^2\right>$. 
%For RNN, the loss that is minimized to find $W_h$ and $W_u$ is given by $\left<||u(t+\tau)-\tilde{u}(t+\tau)||^2\right>$. 
Vlachas et al.\ \cite{Vlachas2019} provide a comparison between reservoir networks and RNN for chaotic dynamics of the KSE. Both provide predictive capabilities for multiple Lyapunov times, but, as noted, are high-dimensional and non-Markovian. Additionally, these methods typically require evenly spaced data for training, and start up often requires multiple known states, instead of a single initial condition. 

%\MDG{Why are you not citing your 2020 PRE?! Compare and contrast with the non-Markovian approaches} 
In contrast to these high-dimensional non-Markovian approaches to learning a discrete time mapping we have shown in prior work \cite{Linot2020} that a dense NN can approximate $F(u)$. This approach is advantageous because, like the underlying system, it is Markovian -- predictions of the next state only depends on the current state. Unlike the previous methods, this approach also drastically reduced the dimension of the state by representing it in terms of the manifold coordinates. For example, at a domain size of $L=22$, for the KSE, we showed that the state $u\in\mathbb{R}^{64}$ could be represented in $h\in\mathbb{R}^8$. We found this coordinate system using an undercomplete autoencoder that corrected on PCA, as described in more detail in Section \ref{sec:autoencoders}. Representing the state in this minimal fashion allows for faster prediction, and may provide insight due to limiting the system to only the essential degrees of freedom.

Rather than approximating $F(u)$ in Eq.\ \ref{eq:discrete}, the discrete-time representation, one can, in principle, approximate $f(u)$ in Eq.\ \ref{eq:ODE} the continuous-time representation.
 This is more challenging, because one does not generally have access to data for $du/dt$. This can be estimated, of course, from data closely spaced in time.  
%Instead of predicting discrete timesteps $f$ can also be computed.
 When the time derivative is known (or estimated) for all of the states ($f(u_i)$), Gonzalez Garcia et al.\ \cite{Gonzalez-Garcia1998} showed that a dense NN can provide an accurate functional form of $f$ for the KSE in a regime that exhibits a periodic orbit. In their work they input the state and its derivatives into the NN.
When $f(u_i)$ is unknown, Raissi et al.\ \cite{Raissi2018} 
%\MDG{is there an updated version of this reference?} \AL{I think it only exists on arxiv. I can't find it anywhere else.} 
showed it can be approximated from data with a multistep time-integration scheme such as the second-order Adams-Bashforth formula, and a NN can be trained to match this approximation. %Approximating $f$ from data in this fashion requires closely spaced to be accurate.

To estimate $f$ from data that is not closely spaced, Chen et al.\ \cite{Chen2019} have introduced an approach, which they call ``neural ODEs", where they use a NN to compute $f$ at each step in an ODE solver. This allows them to compute the solution at arbitrary points in time. We further describe, and apply, this approach in Section \ref{sec:Framework}. To determine the parameters of $f$, the difference between data and predictions are minimized. To determine the derivatives of $f$ with respect to the NN parameters, either an adjoint problem can be solved, or an automatic differentiation method used. 

Neural ODEs have been applied to a number of time series problems.  %It has 
%This can be done using Neural ODEs which use a NN approximation of $f$ to integrate initial conditions forward in time so that they match a set of training data \cite{Chen2018}. Neural ODEs have
% been applied to multiple systems including the viscous Burgers \MDG{note: the name is Burgers, so it's not Burger's equation} 
%equation \cite{MAULIK2020} and homogeneous isotropic turbulence \cite{portwood2019turbulence}. 
Maulik et al.\ \cite{MAULIK2020} used neural ODEs and LSTMs for Burgers equation, and showed both outperform a Galerkin projection approach. %neural ODEs and using LSTMs %\MDG{what do you need an LSTM for if you're using n-ODES? Or are you saying two different approaches, one with n-ODES and one with LSTM both outperformed Galerkin?} 
%outperforms a Galerkin projection approach. 
Portwood et al.\ \cite{portwood2019turbulence} used the neural ODE approach to find an evolution equation for dissipation in decaying isotropic turbulence, showing that it outperformed a heuristic model. Neural ODEs have also been applied to flow around a cylinder in the time-periodic regime, where velocity field simulation data were projected onto 8 PCA modes, and the time evolution of these modes was determined \cite{rojas2021reducedorder}.

The present work combines nonlinear dimension reduction using autoencoders with the neural ODE method to model the dynamics of a system displaying spatiotemporal chaos, the Kuramoto-Sivashinsky equation, over a range of parameter values.  In Section \ref{sec:Framework}, we introduce the framework of our methodology. 
Section \ref{sec:autoencoders} briefly describes the results for the dimension reduction problem alone, then Section \ref{sec:dimredev} uses the reduced-dimensional descriptions to illustrate performance of the neural ODE approximation for closely spaced data.  An important conclusion here is that dimension reduction can improve neural ODE performance relative to predictions in the ambient space, where artifacts arise. Section \ref{sec:dataspacing} examines the role of data spacing on neural ODE performance, showing that even for  chaotic systems, widely spaced data can be used, within a fairly well-defined limit.  Finally, Section \ref{sec:dimdependence} shows comparisons of neural ODE performance with various degrees of dimension reduction, finding a ``sweet spot" in terms of performance vs.\ dimension. We summarize in Section \ref{sec:Conclusion}.
%\MDG{much of this could be used verbatim in the abstract} 

%\sout{In this work, we will show a framework for data-driven dimension reduction and time evolution to a problem that exhibits sustained spatiotemporal chaos. We will reduced dimension with an undercomplete autoencoder, and generate an evolution model using neural ODEs.} 
%In this work, we use autoencoders to map data to the coordinates of the manifold on which it lies and neural ODEs to problems with sustained chaos and show that they allow us to predict the RHS with data spaced much further apart than other methods until we hit a critical Lyapunov time.
		
%For a nonlinear change of basis a popular approach is using an autoencoder \cite{Hinton:2006,IanGoodfellowYoshuaBengio2017}.  

%Whitney showed that a $d_\mathcal{M}$-manifold can be embedded in $\mathbb{R}^{2d_\mathcal{M}}$, and Sauer et al. showed that almost every linear transformation for $d>2d_\mathcal{M}$ is one to one\cite{Sauer2008}. 
	
%Many ways exist to estimate $\chi$. Some of these approaches are linear. These include projecting onto the leading eigenfunctions of the linear operator of the PDE \cite{Temam1990}, performing a discrete Fourier transform, or principal component analysis (PCA) \cite{Strang2019}. These approaches represent the state with a sufficient number of modes \cite{Sauer2008}, but restricting the change of coordinates to linear operations may not provide a minimal representation. For a nonlinear change of basis a popular approach is using an autoencoder \cite{Hinton:2006,IanGoodfellowYoshuaBengio2017}. 

%For some systems the full state is measurable, like a pendulum, where the full state consists of the position and the velocity. For other systems, in particular spatial distributed systems, the data comes from discrete measurements of a state represented by a continuous function. 

%This then introduces an additional problem of how to reduce the dimension with a reduced-order model (ROM) to speed up computation.

%Instead, a desirable approach is generating the model strictly from data or to incorporate knowledge of the system into a data-driven framework. For linear systems or nonlinear systems that decay to a simple low-dimensional solutions many approaches work (e.g. DMD, SINDY, NARMAX etc.). When the dynamics become more complex and exhibit spatiotemporal chaos this problem becomes much harder. Modeling complex chaotic systems tends to require a high-dimensional state representation and finely spaced data. 

%Instead, a desirable approach is generating a ROM from the data. This type of approach overcomes the need to determine a form of the equation and a mapping to a lower dimension a priori. 
%Instead, a desirable approach is generating the model strictly data or to incorporate knowledge of the system into a data-driven framework. 
%For linear systems or nonlinear systems that decay to a simple low-dimensional solutions many approaches work (e.g. DMD). When the dynamics become more complex and exhibit spatiotemporal chaos this problem becomes much harder. Modeling complex chaotic systems tends to require a high-dimensional state representation and finely spaced data. 

%Here we show a method for data-driven modeling of spatiotemporal chaos that come from partial differential equations (PDEs). We leverage the fact that solutions to PDEs often lie on a finite dimensional manifold to represent the state with a minimal ``exact" representation. To find this changed of coordinates, we use an autoencoder. Then we show, in this coordinate system, that Neural ODEs provide an effective way for learning a model for time evolution. This method allows us to use data spaced more sparsely in time than other methods.

%that approximating the dynamics under this coordinate transformation works well even with sparse temporal data.

%% The data we consider is from a chaotic system that lies on a finite dimensional manifold
%Many dissipative systems that exhibit chaotic dynamics are known to collapse at long times to a finite dimensional manifold called an inertial manifold (IM) \cite{Temam1990}. %These manifolds provide an integer dimension, as opposed to the fractal dimension of the global attractor. 
%Two of these systems include the Kuramoto-Sivashinsky equation (KSE) \cite{Foias1988a,Temam1994,Jolly2000} and the complex Ginzburg-Landau equation (CGLE) \cite{Doering1988}, and although no proof exists for the Navier-Stokes equation Foias et al. showed that higher dimensions are bound to a thin region \cite{Foias1988,Temam1989} for a so called approximate inertial manifold. When solutions to PDEs exist on a finite-dimensional manifold the state at any given time can be exactly represented in a finite number of dimensions, as opposed to the infinite dimensional representation needed for solutions to a PDE. This is computationally advantageous as solutions can now be evolved forward in this low dimensional representation. Additionally, this approach may be informative because the state is represented in its fundamental coordinate system. Unfortunately, determining the change of coordinates and the evolution equation in this coordinate system is non-obvious.

%% The two things we are interested in are finding a change of coordinates and time evolution

% The typical approaches for finding change of coordinates are linear. Some of these include projecting onto the leading eigenfunctions of the PDEs linear operator \cite{Temam1990}, performing a discrete Fourier transform, or principal component analysis (PCA) \cite{Strang2019}. These approaches represent the state with a sufficient number of modes \cite{Sauer2008}, but restricting the change of coordinates to linear operations may not provide a minimal representation. For a nonlinear change of basis a popular approach is using an autoencoder \cite{Hinton:2006,IanGoodfellowYoshuaBengio2017}. 

%Autoencoders consist of two NN, an encoder and a decoder. 

%In either the original coordinates, or a change of basis, multiple ways exist to find the dynamics. Some of the most successful methods for predicting chaotic dynamics are reservoir networks \cite{Pathak2018a} and recurrent neural networks (e.g. LSTM) \cite{Vlachas2018,Vlachas2019}. Both of these methods have been shown to provide predictive capabilities for multiple Lyapunov times, but they both require high-dimensional state representations and are non-Markovian. Furthermore, these methods typically require evenly spaced data for training and perform discrete-time mapping when predicting. Similarly, in \cite{Linot2020} we show that dense neural networks (NN) can be trained for discrete-time mapping of chaotic dynamics when data is closely spaced.

%A more desirable approach is learning the right hand side to an ordinary differential equation (ODE). If the equations of motion are known and the dimension reduction is linear one method for learning the dynamics is the nonlinear Galerkin approach, which has been applied to the KSE \cite{Jolly2001}, two-dimensional turbulence \cite{Jauberteau1990a}, and three-dimensional homogeneous isotropic turbulence \cite{Akram2020}. This method requires no data for training. 

%\cite{Gonzalez-Garcia1998}
%Lui and Wolf show that when the RHS is known that training a NN to output the RHS and use that in time integration can predict different fluid flows \cite{Lui2019}. Alternatively, if a linear representation is non-minimal then the manifold Galerkin method, shown in \cite{Lee2020}, works with autoencoders or any change of basis where the Jacobian can be calculated. 

%When the RHS is unknown Raissi et al. \cite{Raissi2018} show approximating the derivative with a multistep method and training a NN to recreate this approximation works for time prediction. Using a multistep method still requires closely spaced data. The other method for approximating the right hand side, called Neural ODEs or ODENet, is to use a NN and integrate initial conditions forward in time so that they match given data \cite{Chen2018}. 

%With Neural ODEs a NN estimates the time derivative by minimizing the difference between a trajectory evolved by the true equations of motion and a trajectory evolved with the NN estimate in an ODE solver. The difficulty in this approach comes from calculating the gradient of this loss with respect to the parameters of the NN. The gradient normally is found by back-propagating through multiple layers of a NN. In this case, all of the data must be stored at each integration for calculating the gradient leading to memory issues. This problem is overcome in Chen et al. \cite{Chen2018} by solving an adjoint problem backwards in time. Further improvements upon this algorithm have been made for calculating more accurate gradients by Gholami et al. \cite{Gholami2019}

% Discuss papers where neural odes have been used
%Neural ODEs have been applied to multiple systems including the viscous Burger's equation \cite{MAULIK2020} and homogeneous isotropic turbulence \cite{portwood2019turbulence}. In Maulik et al. neural ODEs and long short term memory (LSTM) networks were shown to outperform the Galkerin projection approach. In the case of turbulence it was shown that Neural ODEs can accurately calculate turbulent dissipation better than a heuristic model \cite{portwood2019turbulence}. Neural ODEs have also been applied to flow around a cylinder where dimension reduction was performed with 8 PCA modes and the neural ODE predicted the evolution of these modes for a low Reynolds number periodic flow \cite{rojas2021reducedorder}. 

%In this work, we apply neural ODEs to problems with sustained chaos and show that they allow us to predict the RHS with data spaced much further apart than other methods until we hit a critical Lyapunov time.

%3
\section{Framework} \label{sec:Framework}
 
%Our modeling approach involves finding a coordinate transformation that parameterizes the manifold on which data lies, and finding the evolution on this manifold. 

We consider data $u\in\reals^d$ that lies on a $d_\IM$-dimensional manifold $\mathcal{M}$ that is embedded in the ambient space $\mathbb{R}^d$ of the data. With the data, we find a coordinate transformation $\chi : \mathbb{R}^d \rightarrow \mathbb{R}^{d_\mathcal{M}}$ giving the state in the manifold coordinates, $h\in\mathbb{R}^{d_\mathcal{M}}$. We also find the inverse $\check{\chi} :\mathbb{R}^{d_\mathcal{M}} \rightarrow \mathbb{R}^d$ to reconstruct $u$ from $h$. Then we describe the dynamics on $\IM$ with the differential equation
	\begin{equation}\label{eq:ODE_red}
		\dfrac{dh}{dt}=g(h).
	\end{equation}
If we do not do any dimension reduction then Eq.\ \ref{eq:ODE_red} is the same as Eq.\ \ref{eq:ODE}. Of course, for an arbitrary data set, we do not know $d_\IM$ a priori, so in Section \ref{sec:dimdependence} we present results with various choices for $d_h$, where $h\in\mathbb{R}^{d_h}$. Using the mapping to the manifold coordinates, the evolution in the manifold coordinates, and the mapping back, we evolve new initial conditions forward in time. So, our task is to approximate $\chi$, $\check{\chi}$, and $g$. We also consider the case with no dimension reduction, where we determine $f(u)$, the right hand side (RHS) of the ODE in the ambient space. In general, there can be no expectation that any of these functions have a simple (e.g.\ polynomial) form, so here we approximate them with NNs, as detailed below.  

Figure \ref{fig:Framework} illustrates this framework on data from a solution of the Lorenz equation that we embedded in four dimensions by mapping the $z$ coordinate of the Lorenz equation to the Archimedean spiral. The change of coordinates for this embedding is given by $[u_1,u_2,u_3,u_4]=[x,y,\alpha z\cos \alpha z,\alpha z\sin \alpha z]$ where $x,y,z$ are the standard variables in the Lorenz equation and $\alpha=0.02$. In Fig.\ \ref{fig:Frameworka} we show an example of learning the manifold coordinates for this system. The three spatial dimensions for the embedded data are $u_1$, $u_3$, and $u_4$ and the color is $u_2$. For a trajectory to be chaotic it must lie on at least a three-dimensional manifold, so the minimum embedding dimension for a chaotic trajectory that requires all the steps in this framework is  four dimensions. Figure \ref{fig:Frameworkb} illustrates learning the vector field $g$ in the manifold coordinates. In Fig.\ \ref{fig:Frameworkc} we show how, after learning these functions, new initial conditions can be mapped to the manifold coordinates, evolved forward in time, and then mapped back to the ambient space.

%\MDG{You need to describe the various parts of the figure here in the text, not in the caption -- rewrite this paragraph accordingly} 
%In Figure \ref{fig:Framework} the three spatial dimensions for the embedded data are $u_1$, $u_3$, and $u_4$ and the color is $u_2$. After reducing the dimension, the color and the $y$ spatial dimension are redundant. For a trajectory to be chaotic it must lie on at least a three-dimensional manifold, so the minimum embedding dimension for a chaotic trajectory that requires all the steps in this framework is  four dimensions.
 
% For our method we consider data that comes from some PDE
% 	\begin{equation}\label{eq:PDE}
%		\dfrac{\partial v}{\partial t}=f_\partial(v,\dfrac{\partial v}{\partial x},...).
%	\end{equation}
%	Here $f_\partial$ is a nonlinear function that takes $v$ and any number of its derivatives as inputs.  By projecting $v$ onto a finite number of basis functions such that $v(x,t)=\sum_i^d u_i(t)\phi_i(x)$ we can turn this PDE into an ordinary differential equation (ODE) using the method of lines giving 
% 	\begin{equation}\label{eq:ODE}
%		\dfrac{du}{dt}=f(u).
%	\end{equation}
%	At long-times we assume that the solutions to the PDEs we consider collapse onto a finite-dimensional manifold, suggesting that $v$ can be exactly represented by a finite number of basis functions. This means $u(t)\in \mathbb{R}^d$ is exact and we refer to it as the full-state. By evolving forward Eq \ref{eq:ODE} with the appropriate boundary conditions we generate a time series of data $[u(t_1),u(t_2),...,u(t_M)]$. Now we seek to create an evolution equation for $u$ given the data $u(t_i)$ without knowing Eq \ref{eq:ODE}. Although we use data from the full-state to learn a model any representation that is homeomorphic to the full-state will work. For example, the state could be represented by measurements of the state or by time-delays.
	
%	Two natural approaches to modeling this system from data are approximating discrete time maps $u(t+\tau)=F(u(t))$ or $f$. Some of the most successful methods for predicting chaotic dynamics with disrete time maps are reservoir networks \cite{Pathak2018a} and recurrent neural networks (e.g. LSTM) \cite{Vlachas2018,Vlachas2019}. Both of these methods have been shown to provide predictive capabilities for multiple Lyapunov times, but they both typically use high-dimensional state representations and are non-Markovian. Addtionally, these methods typically require evenly spaced data for training. 
%	
%	Instead of predicting discrete timesteps $f$ can also be computed. When the time derivative is known for all of that states ($f(u_i)$) Gonzalez Garcia et al. \cite{Gonzalez-Garcia1998} showed a dense neural network (NN) can predict this output. In their work they input the state and its derivatives into the NN. When $f(u_i)$ is unknown Raissi et al. \cite{Raissi2018} show it can be approximated with a multistep method. Then they also use a NN to recreate this approximation and show it works for time prediction. Approximating $f$ from data in this fashion requires closely spaced to be accurate.
%	
%	To use more widely spaced data computing $f$ directly from the data should be avoided. This can be done using Neural ODEs which use a NN approximation of $f$ to integrate initial conditions forward in time so that they match a set of training data \cite{Chen2018}. Neural ODEs have been applied to multiple systems including the viscous Burger's equation \cite{MAULIK2020} and homogeneous isotropic turbulence \cite{portwood2019turbulence}. In Maulik et al. neural ODEs and long short term memory (LSTM) networks were shown to outperform the Galkerin projection approach. In the case of turbulence it was shown that Neural ODEs can accurately calculate turbulent dissipation better than a heuristic model \cite{portwood2019turbulence}.
%		
%	A drawback of learning either $f$ or $F$ directly is both can be high-dimensional making them costly to approximate and use for time evolution. In \cite{Linot2020} we show improved performance of discrete time mapping using dense neural networks (NN) when using a low-dimensional representation of the data. Neural ODEs have also been applied to reduced dimensional states. In \cite{rojas2021reducedorder} neural ODEs were used to predict flow around a cylinder for a low Reynolds number periodic flow using 8 PCA modes.
	
	%flow around a cylinder where dimension reduction was performed with 8 PCA modes and the neural ODE predicted the evolution of these modes for a low Reynolds number periodic flow \cite{rojas2021reducedorder}. 
	
	%% Write about how people approximate f here
	
	%However, $f$ may be high-dimensional making it costly to approximate and use in time evolution. Instead we leverage the fact that we consider systems where $u$ falls on a lower-dimensional manifold $\mathcal{M}$ at long-times.
	
%	A low-dimensional representation of the data is efficient because it allows us to leverage the fact that $u$ falls on a lower-dimensional manifold $\mathcal{M}$ at long-times. This means that $u\in \mathcal{M} \subset \mathbb{R}^d$ and an exact coordinate transformation $\chi : \mathcal{M} \rightarrow \mathbb{R}^{d_\mathcal{M}}$ gives the state in the manifold coordinates $h\in\mathbb{R}^{d_\mathcal{M}}$. In this new coordinate system the evolution equation is 
%		\begin{equation}\label{eq:ODE_red}
%			\dfrac{dh}{dt}=g(h).
%		\end{equation}
%		If we know $\chi$ then instead of approximating $f$ we can approximate $g$. Approximating $g$ requires less computational cost than approximating $f$ and evolving states forward is less computationally expensive. Also, these coordinates may provide insight as only the fewest relevant parameters remain. After evolving $h$ forward in time, we also need a mapping back to the $u$ given by $\check{\chi} :\mathbb{R}^{d_\mathcal{M}} \rightarrow \mathcal{M}$. 
		
%Many ways exist to estimate $\chi$. Some of these approaches are linear. These include projecting onto the leading eigenfunctions of the linear operator of the PDE \cite{Temam1990}, performing a discrete Fourier transform, or principal component analysis (PCA) \cite{Strang2019}. These approaches represent the state with a sufficient number of modes \cite{Sauer2008}, but restricting the change of coordinates to linear operations may not provide a minimal representation. For a nonlinear change of basis a popular approach is using an autoencoder \cite{Hinton:2006,IanGoodfellowYoshuaBengio2017}. 
		
%Our approach to generate a model involves approximating $g$, $\chi$, and $\check{\chi}$ from data. First we approximate $\chi$ and $\check{\chi}$ using the original data $u(t_i)$. Then we perform the change of coordinates to get data $h(t_i)$ and approximate $g$. With these three functions new initial conditions can be evolved forward to make predictions.
	
	%We use series of snapshots of this data $[u(t_1),u(t_2),...,u(t_M)]$ to generate a model, but the method described works with any representation of that state that is homeomorphic to $u(t)$. For example, the state could be represented by measurements of the state or by time-delays.
	

	%Although these states are embedded in an ambient dimension $\mathbb{R}^d$ we assume that the long-time solutions relax to some lower-dimensional manifold $\mathcal{M}$. %Although this representation of the state is exact it is not necessarily minimal.
	
% Our modeling approach involves finding a coordinate transformation that parameterizes the manifold on which data lies and then finding the evolution under this coordinate transformation strictly from data. The data we consider comes from some partial differential equations (PDE) 
%	\begin{equation}\label{eq:PDE}
%		\dfrac{\partial v}{\partial t}=f(v,\dfrac{\partial v}{\partial x},...).
%	\end{equation}
%	This equation can be converted to a system of ordinary differential equations (ODE) using the method of lines 
%	
%	At long-times we assume that the solutions to this PDE collapse onto a finite-dimensional manifold, in which case the state at any time can be exactly described by a finite number
%Using the method of lines this partial differential equation can be turned into a an ordinary differential equation given by 
%	\begin{equation}\label{eq:ODE}
%		\dfrac{du}{dt}=f(u),
%	\end{equation}
%where $u(t)\in \mathbb{R}^d$.

\begin{figure*}
	\centering
	\captionsetup[subfigure]{labelformat=empty}
	\begin{subfigure}[b]{17.2 cm}
		\includegraphics[trim=0 0 0 0,width=\textwidth,clip]{ODENet_Fig.png}
		\caption{}
		\vspace{-10mm}
		\label{fig:Frameworka}
	\end{subfigure}
	\begin{subfigure}[b]{0\textwidth}\caption{}\vspace{-10mm}\label{fig:Frameworkb}\end{subfigure}
	\begin{subfigure}[b]{0\textwidth}\caption{}\vspace{-10mm}\label{fig:Frameworkc}\end{subfigure}
	\captionsetup{justification=raggedright}
	\vspace{-10mm}
	\caption{(a) Learning the three-dimensional manifold coordinates of the four-dimensional Lorenz butterfly wrapped on the Archimedean spiral (color is the fourth dimension). (b) Learning the vector field in the manifold coordinates. (c) Transforming new initial conditions (black dots) into the manifold coordinates, evolving them according to the learned vector field g (black curves), and transforming back into the original space.}
	\label{fig:Framework}
\end{figure*}   

%\begin{figure*}
%	\includegraphics[trim=0 0 0 0,width=17.2 cm,clip]{ODENet_Fig.png}
%	
%	%\captionsetup{justification=raggedright}
%	\caption{(a) Learning the three-dimensional manifold coordinates of the four-dimensional Lorenz butterfly wrapped on the Archimedean spiral (color is the fourth dimension). (b) Learning the vector field in the manifold coordinates. (c) Transforming new initial conditions (black dots) into the manifold coordinates, evolving them according to the learned vector field g (black curves), and transforming back into the original space. }
%	\label{fig:Framework}
%	%\vspace{-5mm}
%\end{figure*} 

%Our modeling approach involves finding a coordinate transformation that parameterizes the manifold on which data lies and then finding the evolution under this coordinate transformation. We consider systems of ODEs described by 
%	\begin{equation}\label{eq:ODE}
%		\dfrac{du}{dt}=f(u),
%	\end{equation}
%where $u\in \mathcal{M} \subset \mathbb{R}^d$ is the full-state at long-times after relaxing to the manifold $\mathcal{M}$ that is embedded in the ambient space $\mathbb{R}^d$. We take this state and approximate an exact coordinate transformation $\chi : \mathcal{M} \rightarrow \mathbb{R}^{d_\mathcal{M}}$ giving the state in this coordinate system $h\in\mathbb{R}^{d_\mathcal{M}}$, and approximate the inverse of this map ($\check{\chi}$). Then, with this lower-dimensional state representation, we approximate the dynamics given by 
%	\begin{equation}\label{eq:ODE_red}
%		\dfrac{dh}{dt}=g(h).
%	\end{equation}
%With the mapping to the manifold coordinates, the evolution in the manifold coordinates, and the mapping back we can evolve new initial conditions forward in time.

%Figure \ref{fig:Framework} illustrates these steps on a solution of the Lorenz equation that we embedded in 4-dimensions. This embedding is done by mapping the $z$ coordinate of the Lorenz equation to the Archimedean spiral. The change of coordinates for this embedding is given by $[u_1,u_2,u_3,u_4]=[x,y,\alpha z\cos \alpha z,\alpha z\sin \alpha z]$ where $x,y,z$ are the standard variables in the Lorenz equation and $\alpha$ is some scaling factor. In Figure \ref{fig:Framework} the three spatial dimensions for the embedded data are $u_1$, $u_3$, and $u_4$ and the color is $u_2$. After reducing the dimension, the color and the $y$ spatial dimension are redundant. For a trajectory to be chaotic it must lie on at least a 3-dimensional manifold, so the minimum embedding dimension for a chaotic trajectory that requires all the steps in this framework is 4-dimensions.

%Figure \ref{fig:Framework} illustrates these steps with the Lorenz butterfly wrapped around the Archimedean spiral. This change of coordinates is given by $[u_1,u_2,u_3,u_4]=[x,y,\alpha z\cos \alpha z,\alpha z\sin \alpha z]$ where $x,y,z$ are the standard variables in the Lorenz equation and $\alpha$ is some scaling factor. %This set is embedded in $\mathbb{R}^4$ by taking the $z$ component of the Lorenz equation and mapping it to two dimensions with $(x,y)=(zcos(z),zsin(z))$.

%The advantage of taking this approach is using the most compact exact representation of the data lowers computational cost both in training and integrating and may provide insight as only the fewest relevant parameters remain. Also, by learning the dynamics, as opposed to discrete-time mapping, evolving solutions forward to arbitrary times is simple. Along with these practical benefits, we find later that approximating the full-space dynamics with Neural ODEs results in high-frequency behavior that pushes trajectories off the attractor at long-times.

%So far the approach is agnostic to the method of function approximation for $\chi$, $\check{\chi}$, and $g$. In general, selecting the best method for approximating these functions depends upon various factors including dimension, amount, quality, and physical constraints of data. We use NNs for approximating all of these functions because we have plenty of high-dimensional and high-quality simulation data.


To find $\chi$ and $\check{\chi}$, we represent them as NNs and find parameters that minimize a reconstruction loss averaged over a batch of data given by 
	\begin{equation}\label{eq:Auto}
		L=\left<||u-\check{\chi}(\chi(u;\theta_1);\theta_2)||^2\right>,
	\end{equation}
where $\theta_1$ and $\theta_2$ are the weights of $\chi$ and $\check{\chi}$, respectively. Here, and elsewhere, we use stochastic gradient descent methods to determine the parameters. Further details are given in Section \ref{sec:autoencoders}.
	This architecture is known as an undercomplete autoencoder, where $\chi$ is an encoder and $\check{\chi}$ is a decoder \cite{IanGoodfellowYoshuaBengio2017}. Autoencoders are widely used for dimension reduction; in the fluid mechanics context they have been used for many examples including flow around an airfoil \cite{Omata2019}, flow around a flat plate \cite{Nair2020}, Kolmogorov flow \cite{Page2020}, and channel flow \cite{Milano2002,Fukami2020}.
	



%Next, we need to estimate $g$ with a NN. If the data is spaced very close together, one could estimate $g$ by differencing \cite{Raissi2018}, as described above.

%% Put finite difference stuff here

As noted above, to approximate $g$ in Eq.\ \ref{eq:ODE_red}, we use the neural ODE approach of Chen et al.\ \cite{Chen2019}.  %Denoting $\tilde{g}$ as a neural network approximation to $g$, 
In the neural ODE framework we use $g$ to estimate $\tilde{h}(t_i+\tau)$, an approximation of the reduced state $h(t_i+\tau)$, at some time $t_i+\tau$ by integrating 
\begin{equation}\label{eq:ODENet_Int}
	\tilde{h}(t_i+\tau)=h(t_i)+\int_{t_i}^{t_i+\tau}g(h(t);\theta_3) dt,
\end{equation}
where $\theta_3$ is the set of weights of the NN. Then $g$ is learned by minimizing the difference between the prediction of the state ($\tilde{h}(t_i+\tau)$) and the known state ($h(t_i+\tau)$), in the manifold coordinates, at that time. Specifically, the loss we minimize is
\begin{equation}\label{eq:ODENet}
	J=\left<||h(t_i+\tau)-\tilde{h}(t_i+\tau)||_1\right>.
\end{equation}
Here $||.||_1$ denotes the $L_1$-norm -- of course other norms can be used.
By taking this approach we can estimate $g$ from data spaced further apart in time than when $g$ is estimated directly from finite differences.
%Let $\tilde{h}(t)$ represent an estimate of the reduced state at time $t$ and $g$ a neural-network approximation, with parameters $\theta_3$. Now
%%\begin{equation}%\frac{d\tilde{h}}{dt}=g(t;\theta_3).%\end{equation}%With initial condition $\tilde{h}(t_i}=h(t_i)$, where $h(t_i)$ is determined from data vector  $u(t_i)$ via the mapping $\chi$, we can formally write
%	
%
%Instead of using closely spaced data to approximate $g$ we integrate initial conditions forward with $g$ to predict future states. Then $g$ is optimized to minimize the difference between the prediction of the state ($\tilde{h}(t_i+\tau)$) and the known state ($h(t_i+\tau)$) in the manifold coordinates at that time. This gives the loss
%	\begin{equation}\label{eq:ODENet}
%		J=\left<||h(t_i+\tau)-\tilde{h}(t_i+\tau)||^2\right>,
%	\end{equation}
%	where $\tilde{h}(t_i+\tau)$ comes from integrating $\tilde{g}$ 
%	\begin{equation}\label{eq:ODENet_Int}
%		\tilde{h}(t_i+\tau)=h(t_i)+\int_{t_i}^{t_i+\tau}\tilde{g}(h(t);\theta_3) dt.
%	\end{equation}
%Here $\tilde{g}$ is a NN with weights $\theta_3$. This is the Neural ODE approach \cite{Chen2019}.
 The difficulty comes in determining the gradient of the loss with respect to the weights of the NN, $\partial J/\partial \theta_3$. 

One approach is to use automatic differentiation to back-propagate through all of the steps of the time integration scheme used to solve \ref{eq:ODENet_Int}. Another approach involves solving an adjoint problem backwards in time. A drawback of back-propagating through the solver is that all of the data must be stored at each time-step to calculate the gradient, which can lead to memory issues \cite{Chen2019}. However, we consider a chaotic system which puts an implicit constraint on how far apart data can feasibly be sampled and still yield a good estimate of $g$. We reach this limit before the memory limit of back-propagation becomes an issue. In our trials, the adjoint method yielded results in good agreement with back-propagation, but required longer computation times for training, so we chose to use the back-propagation approach.

%Neural ODEs have been successfully used for systems that do not exhibit spatiotemporal chaos. For example, in Rojas et al. \cite{Rojas2021} the periodic dynamics of flow around a cylinder was predicting using neural ODEs, and in Portwood et al. \cite{portwood2019turbulence} they predicted the kinetic energy and dissipation as a function of time for decaying turbulence with neural ODEs. We extend this work by showing the efficacy of using neural ODEs on data from a system that exhibits chaotic dynamics.

%Many dissipative systems that exhibit chaotic dynamics are known to collapse at long times to a smooth invariant finite-dimensional manifold called an inertial manifold (IM) \cite{Temam1990}. %These manifolds provide an integer dimension, as opposed to the fractal dimension of the global attractor. 
%Two of these systems include the Kuramoto-Sivashinsky equation (KSE) \cite{Foias1988a,Temam1994,Jolly2000,Zelik2014} and the complex Ginzburg-Landau equation \cite{Doering1988}. This can also be shown for the Navier-Stokes in two dimensions (CITE), but not yet in three dimensions. In three dimensions is has been shown that there is an approximate inertial manifold \cite{Foias1988,Temam1989}. This means there are small variations in many dimension that can be assumed to be zero and still provide an accurate approximation of the state.

The data sets we consider are numerical solutions of the 1D Kuramoto-Sivashinsky equation (KSE),
\begin{equation}\label{eq:KSE}
	\dfrac{\partial v}{\partial t}=-v\dfrac{\partial v}{\partial x}-\dfrac{\partial^2 v}{\partial x^2}-\dfrac{\partial^4 v}{\partial x^4}, 
\end{equation}
in a domain of length $L$ with periodic boundary conditions. The domain size determines the types of dynamics this system exhibits. For the domain sizes we consider, trajectories exhibit sustained chaos. The dimension of the manifold that contains the global attractor has been computationally approximated using several approaches \cite{Yang2009,Ding2016,Linot2020}. We generate high-dimensional state representations ($u\in\mathbb{R}^d$) via a Galerkin projection  of $v$ onto Fourier modes. Then, we use an exponential time differencing method \cite{Kassam2005} to integrate the resulting system of ordinary differential equations forward in time. The code used is available from Cvitanovi\'c et al.\ \cite{ChaosBook}. The data vectors $u$ that we use are solutions $v$ of the KSE on $d=64$ equally-spaced grid points in the domain.

%Instead we approximate the $g$ with a NN and then evolve trajectories forward in time with this estimate. minimize the difference between the state in the manifold coordinates at some future time and the integration of some state forward in time with a NN approximation of $g$ ($\tilde{g}$) given by 
%	\begin{equation}\label{eq:ODENet}
%		J=\left<||h(t_i+\tau)-\int_{t_i}^{t_i+\tau}\tilde{g}(h(t);\theta_3) dt||^2\right>.
%	\end{equation}
%This is the Neural ODE approach \cite{Chen2019}.

%With Neural ODEs a NN estimates the time derivative by minimizing the difference between a trajectory evolved by the true equations of motion and a trajectory evolved with the NN estimate in an ODE solver. The difficulty in this approach comes from calculating the gradient of this loss with respect to the parameters of the NN. The gradient normally is found by back-propagating through multiple layers of a NN. In this case, all of the data must be stored at each integration for calculating the gradient leading to memory issues. This problem is overcome in Chen et al. \cite{Chen2018} by solving an adjoint problem backwards in time. Further improvements upon this algorithm have been made for calculating more accurate gradients by Gholami et al. \cite{Gholami2019}
%
% As mention earlier to reduce memory cost a backwards in time adjoint problem is typically solved. In our case, we consider a chaotic system which puts an implicit constraint on how far apart data can feasibly be sampled for training. We reach this limit before the memory limit of back-propagation.  The adjoint method performed the same as back-propagation here, and we choose to back-propagate due to empirically faster training.

%4
\section{Results} \label{sec:Results}

Section \ref{sec:autoencoders} briefly describes the dimension reduction (manifold representation results).  Section \ref{sec:dimredev} considers neural ODE predictions, with and without dimension reduction, for closely spaced data. Section \ref{sec:dataspacing} shows the effect of data spacing on the results, and Section \ref{sec:dimdependence} illustrates the effect of the degree of dimension reduction. We summarize in Section \ref{sec:Conclusion}.

%\sout{First, we show the importance of dimension reduction when using neural ODEs on a chaotic dataset. Then, we show, with this dimension reduction, how sparse the training data can be in time before approximating ODEs fails. For both of these sections we assume we know the intrinsic dimension of the manifold on which the data lies. Finally, we show the effect of varying the dimension on the performance.}

We consider datasets for three domain sizes, $L=22$, $44$, and $66$, where we estimate the manifold dimension to be $d_\mathcal{M}=8$, $18$, and $28$, as explained in the following section. The relevant timescale for each of these system is the Lyapunov timescale (inverse of the largest Lyapunov exponent), which we previously found to be be very close to the integral time for KSE data \cite{Linot2020}. For these domain sizes, the Lyapunov times are $\tau_L=22.2$, $12.3$, and $11.6$, respectively \cite{Ding2016,Edson2019}. We use $10^5$ time units of data for training at each domain size, and vary how frequently we sample this dataset in time. 
For training the NNs we use Keras \cite{chollet2015keras} for the autoencoders and PyTorch \cite{Paszke2019} for the neural ODEs. The neural ODE code is a modification on the code used in \cite{Chen2019}.

\subsection{Dimension reduction with autoencoders}\label{sec:autoencoders}

In each section we use autoencoders to approximate the map to the manifold coordinates $\chi$ and back $\check{\chi}$. We found in \cite{Linot2020} that a useful way to represent the map to the manifold coordinates ($h=\chi(u)$) is as a difference from a linear projection of the data onto the leading $d_h$ PCA modes:
	\begin{equation} \label{eq:hidden}
		h=E(U^T u;\theta_1)+P_{d_h}U^T u.
	\end{equation}
Here $E$ is a NN, $U$ is the full PCA matrix, and $P_{d_h}$ is the projection onto the leading $d_h$ modes. Similarly, we can learn a difference for the decoder ($\tilde{u}=\check{\chi}(h)$)
\begin{equation} \label{eq:Autoencoder}
	\tilde{u}=UD(h;\theta_2)+U\begin{bmatrix}
		{h} \\ 0
	\end{bmatrix},
\end{equation}
where $D$ is a NN.
By taking this approach we simplify the problem such that the NN only needs to correct upon PCA. We refer to this as a hybrid NN (HNN). 
%Setting $P_{d_h}U\trans u=0$ corresponds to a standard encoder and setting $E(U\trans u)=0$ corresponds to PCA.

\begin{figure} 
	\includegraphics[trim=0 0 0 0,width=8.6 cm,clip]{Drop.png}
	%\captionsetup{justification=raggedright}
	\caption{MSE of test data as a function of dimension for domain sizes $L=22$, $44$, and $66$. The expected manifold dimensions are $d_\IM=8$, $18$, and $28$ for these domain sizes, respectively.}
	\label{fig:Dimension}
	%\vspace{-5mm}
\end{figure} 

In \cite{Linot2020} we trained these HNN using an Adam optimizer with a learning rate of 0.001 at multiple dimensions. Figure \ref{fig:Dimension} shows the mean squared reconstruction error (MSE) of a test data set not used in training at various dimensions for each of the domain sizes. When using this hybrid approach we see a significant drop in the MSE at dimensions of $d_\mathcal{M}=8$, $18$, and $28$ for the domain sizes $L=22$, $44$, and $66$, respectively. The observed approximately linear relationship between dimension and domain size is in agreement with the scaling of physical modes in \cite{Yang2009}, and the dimension for $L=22$ is in agreement with estimates by Ding et al.\ \cite{Ding2016}.

These results guide our selection of dimension in the next two sections. Furthermore, we find that at the manifold dimension that the same MSE is achieved whether we use the full HNN or simply project on to $d_\IM$ PCA modes ($E(U\trans u)=0$). This suggests, for the KSE with periodic boundary conditions, that the leading PCA coefficients fully parameterize the state. So, in the next two sections, $\chi$ is the projection onto $d_\IM$ PCA modes and $\check{\chi}$ is a NN that approximates the remaining PCA coefficients from the leading ones. Table \ref{Table} shows the architectures for the autoencoders and for the NNs used for time evolution in Sections \ref{sec:dimredev} and \ref{sec:dataspacing}.

\subsection{Effect of Dimension Reduction on Time Evolution}\label{sec:dimredev}

Now that we have a map to the manifold coordinates and back, the next step is approximating the ODE. Here we train three different types of ODEs to evaluate the impact of mapping the data to the manifold coordinates. We learn the RHS of the ODE in the manifold coordinates: $\dot{h}=g(h)$, in physical space: $\dot{u}=f(u)$, and in the space of the spatial Fourier coefficients: $\dot{\hat{u}}=\hat{f}(\hat{u})$, where $\hat{u}=\mathcal{F}(u)$ and $\mathcal{F}$ is the discrete Fourier transform. For the last two cases there is no dimension reduction, so $\chi$ is the identity in the first case and the discrete Fourier transform in the second, with $\check{\chi}$ being, again, the identity in the first case and the discrete inverse Fourier transform in the second.

%\begin{table}
%	\captionsetup{justification=raggedright}
%	\caption{Architectures of NNs and matrices used for dimension reduction and time evolution. ``Shape" indicates the dimension of each layer, and ``activation" the corresponding activation functions (S is the sigmoid activation) \cite{IanGoodfellowYoshuaBengio2017}.}
%	\resizebox{.65\textwidth}{!}{%
%		\begin{tabular}{l*{6}{c}r}
%			              & Function & Shape & Activation \\
%			\hline
%			Linear				& $\chi$ 			& $d:d_\mathcal{M}$ & linear  \\
%			Encoder $L=22$		& $\chi$ 			& $d:500:d_h$  & S:linear  \\
%			Encoder	$L=44,66$	& $\chi$ 			& $d:500:500:d_h$  & S:S:linear  \\
%			Decoder $L=22$		& $\check{\chi}$ 	& $d_h:500:d$  & S:linear  \\
%			Decoder $L=44,66$	& $\check{\chi}$ 	& $d_h:500:500:d$  & S:S:linear  \\
%			ODE					& $f$ or $g$ 		& $d_h:200:200:200:d_h$ & S:S:S:linear  \\
%			Discrete Map 		& $G$ 				& $d_\mathcal{M}:200:200:200:d_\mathcal{M}$ & S:S:S:linear  \\
%		\label{Table}
%		\end{tabular}}
%	%\vspace{-5mm}
%\end{table}

\begin{table}
	\captionsetup{justification=raggedright}
	\caption{Architectures of NNs and matrices used in Sections \ref{sec:autoencoders}-\ref{sec:dataspacing}. ``Shape" indicates the dimension of each layer, and ``activation" the corresponding activation functions (S is the sigmoid activation) \cite{IanGoodfellowYoshuaBengio2017}.}
	\resizebox{.65\textwidth}{!}{%
		\begin{tabular}{l*{6}{c}r}
			              & Function & Shape & Activation \\
			\hline
			Encoder $L=22,44,66$	& $\chi$ 			& $d:d_\mathcal{M}$ & linear  \\
			Decoder $L=22,44$	& $\check{\chi}$ 	& $d_\mathcal{M}:500:d$  & S:linear  \\
			Decoder $L=66$		& $\check{\chi}$ 	& $d_\mathcal{M}:500:500:d$  & S:S:linear  \\
			ODE					& $f/g$ 		& $d/d_\mathcal{M}:200:200:200:d/d_\mathcal{M}$ & S:S:S:linear  \\
			Discrete Map 		& $G$ 				& $d_\mathcal{M}:200:200:200:d_\mathcal{M}$ & S:S:S:linear  \\
		\label{Table}
		\end{tabular}}
	%\vspace{-5mm}
\end{table}


%In Table \ref{Table} the NN architectures are shown for each of these approximations of the RHS. 
We train each of the NN with data spaced 0.25 time units in these trials. This timescale is substantially shorter than the Lyapunov time, which decouples the issue of dimension from that of time resolution. Each NN is trained until the loss levels off using an Adam optimizer with a learning rate of $10^{-3}$ that we drop to $10^{-4}$ halfway through training. First the autoencoder is trained, then the neural ODE. To avoid spurious results, we train 5 autoencoders then 10 ODEs, and select the combination of autoencoder and ODE that provides the best short-time tracking. Due to random weight initialization and the stochastic nature of the optimization, we train multiple models and use the one that performs best. 

In Fig.\ \ref{fig:Fullspace} we compare the short and long-time trajectories between the data at $L=22$ (Fig.\ \ref{fig:Traja}) and the NN models (Fig.\ \ref{fig:Trajb}-\ref{fig:Trajd}). The same initial condition, which is on the attractor, is used for all cases. The first model prediction, shown in Fig.\ \ref{fig:Trajb}, comes from $g$ (the low-dimensional ODE), the second model prediction, shown in Fig.\ \ref{fig:Trajc}, comes from $f$ (the full physical space ODE), and the third model prediction, shown in Fig.\ \ref{fig:Trajd}, comes from $\hat{f}$ (the full Fourier space ODE). 
%We use the same initial condition for all four trials, which comes from evolving the true system forward in time until the dynamics relaxed onto the manifold $\mathcal{M}$ (i.e. are statistically stationary). 
For each of these figures, the first 50 time units are shown on the left and 450-500 time units are shown on the right. 

For this domain size, the Lyapunov time is $\tau_L=22.2$. All three of the models are able to generate good predictions for times up to about $t=30$; this is a reasonable quantitative prediction horizon for a data-driven model of chaotic dynamics. At longer times, a good model should still yield behavior consistent with the data, and indeed the reduced model, Fig.\ \ref{fig:Trajb}, no longer matches the true solution, but continues to exhibit realistic dynamics. In the next section we discuss the long-time behavior in more detail. However, the full state model predictions in Figs. \ref{fig:Trajc} and \ref{fig:Trajd} develop high wavenumber striations at long times, and are thus not faithful to the dynamics. Training high-dimensional neural ODEs is computationally more expensive, and the predictions from these models are worse.
%All methods match the true trajectory well for around 30 time units, with high wavenumber striations starting to appear in the approximations of the models without dimension reduction, \ref{fig:Trajc} and \ref{fig:Trajd}. For this domain size, the Lyapunov time $\tau_L$ is 22.2 time units, past which small differences in the initial condition lead to divergence of solutions.

%At long times we expect model solutions to diverge from the true solution, but remain on the attractor. In the reduced model, Fig. \ref{fig:Trajb}, we see that this example trajectory no longer matches the true solution, but continues to exhibit dynamics like the true system, as expected. Unlike the reduced model, the high wavenumber excitations for the full-space models (\ref{fig:Trajc} and \ref{fig:Trajd}) and continue to increase in time resulting in long-time trajectories far from any state on the attractor of the KSE.

 %Figure \ref{fig:Fullspace} shows the recreation of a long trajectory for a domain size of $L=22$ starting with a trajectory on the manifold $\mathcal{M}$. The recreation comes from providing the model with a single initial condition and evolving forward in time with each of the methods.  
 \begin{figure}
	\centering
	\captionsetup[subfigure]{labelformat=empty}
	\begin{subfigure}[b]{8.6 cm}
		\includegraphics[trim=0 0 0 0,width=\textwidth,clip]{Traj_Comparison_ODENetRHS.png}
		\begin{picture}(0,0)
		\put(-115,263){\contour{white}{ \textcolor{black}{a)}}}
		\put(-115,206){\contour{white}{ \textcolor{black}{b)}}}
		\put(-115,149){\contour{white}{ \textcolor{black}{c)}}}
		\put(-115,92){\contour{white}{ \textcolor{black}{d)}}}
		\end{picture}
		\caption{}
		\vspace{-10mm}
		\label{fig:Traja}
	\end{subfigure}
	\begin{subfigure}[b]{0\textwidth}\caption{}\vspace{-10mm}\label{fig:Trajb}\end{subfigure}\begin{subfigure}[b]{0\textwidth}\caption{}\vspace{-10mm}\label{fig:Trajc}\end{subfigure}\begin{subfigure}[b]{0\textwidth}\caption{}\vspace{-10mm}\label{fig:Trajd}\end{subfigure}    
	\vspace{-1.5\baselineskip}
	\captionsetup{justification=raggedright}
	\caption{Examples of trajectory predictions for different models. (a) is the true trajectory, (b) is the predicted trajectory when approximating $g$ for $d_\mathcal{M}=8$. (c) is the predicted trajectory when approximating $f$. (d) is the predicted trajectory when approximating $\mathcal{F}(f)$.} 
	\label{fig:Fullspace} 
	\vspace{-5mm}
\end{figure}

% \begin{figure} 
%	\includegraphics[trim=0 0 0 0,width=8.6 cm,clip]{Traj_Comparison_ODENetRHS.png}
%	%\captionsetup{justification=raggedright}
%	\caption{Examples of trajectory predictions for different models. From top to bottom the plots are the true trajectory, the predicted trajectory when approximating $g$ for $d_\mathcal{M}=8$, the predicted trajectory when approximating $f$, and the predicted trajectory when approximating $\mathcal{F}(f)$.}
%	%\MDG{I don't see where $\mathcal{F}$ is defined. Label more carefully.  Don't make the reader guess that the left and right plots are short time and long time. Here and everywhere, put in labels (a)-(d).  That makes it easier to refer to specific plots in a figure.}\AL{I didn't have $\mathcal{F}$ defined before. I added that it is the Fourier transform above.}}
%	\label{fig:Fullspace}
%	%\vspace{-5mm}
%\end{figure} 

%Approximating the evolution with a discrete-time map ($u(t_i+T)=F(u(t_i))$) does not result in this high wavenumber behavior. 

To understand this high wavenumber behavior, we plot the magnitude of the Fourier modes as a function of time in Fig.\ \ref{fig:Wavenumbers_Both}. In both cases, there is a linear growth in the high wavenumber modes despite the sigmoid activation functions used in the NN being bounded. This appears to arise because perturbations off the attractor cause the sigmoids to go to either 0 or 1 resulting in a constant bias term on the RHS of the neural ODE. However, this phenomena is not exclusive to our choice of sigmoid activations. When we use other activation functions, like hyperbolic tangent and rectified linear units, we also see a bias term on the RHS.
 %If we pick other bounded activation functions (e.g. hyperbolic tangent) this problem remains, and if we pick other unbounded activation functions (e.g. rectified linear unit) the RHS continues to grow, also resulting in erroneous behavior. 

%the model of the full Fourier space ODE $\mathcal{F}(f)$. We see a linear growth in the high wavenumber modes. The high wavenumber behavior in neural ODEs appears to happen due to a constant in the high wavenumbers of approximating $\mathcal{F}(f)$ accumulating at long times. In other words, when approximating ODEs, if the NN outputs a constant $\dot{x} \approx C$ we expect solutions $x(t) \approx Ct+x(0)$, which agrees with Fig. \ref{fig:Wavenumbers}. This constant appears because the NN architecture uses sigmoid activations that bound the output. When the state leaves the attractor the neural ODE sees states far from the training data causing the activations to max out. In Fig. \ref{fig:Wavenumbers_Real} we show the magnitudes of the Fourier modes for the model of the full physical-space ODE $f$. This too shows growth in high wavenumbers.
 
%In figure \ref{fig:Wavenumbers} we plot the evolution of the magnitude of the Fourier modes as a function of time for the model of the full-space Fourier ODE $\mathcal{F}(f)$. As expected, we see a linear growth in the high wavenumbers. 


In the true system these high wavenumbers are strongly damped due to hyperdiffusivity in the KSE, so the data used to train these models contains little content in these wavenumbers. Furthermore, the prediction horizon in the training of the neural ODE is short  (0.25 here), so small errors in high wavenumbers have a negligible effect on the loss. %Thus, the model never trains on perturbations to the high wavenumbers and thus exhibits erroneous behavior. 
The combination of these factors is why the trajectories of the high-dimensional models leave the attractor, while the trajectories of the low-dimensional model do not. This same behavior appears when modeling the full state for the larger domain sizes $L=44$ and $66$.
%In other words, training finds suboptimal NN parameters for recreating high wavenumber behavior. In this case, we train the ODE with such finely spaced data that the error due to high wavenumbers contributes little to the short-time training loss, Equation \ref{eq:ODENet}.

%\AL{I haven't verified that L=44 and L=66 also show this behavior, but after doing that I will add a comment here about that.}

%Figure \ref{fig:Wavenumbers} shows that the high-wavenumber modes grow linearly in time when learning $\mathcal{F}(f)$, supporting this claim. Unlike with neural ODEs, for discrete-time maps a small constant in high wavenumbers does not grow rapidly, but in neural ODEs the linear growth likely comes from $\dot{x} \approx C$ giving solutions $x(t) \approx Ct+x(0)$. When learning the dynamics in physical space ($f$) we see similar growth (Fig. \ref{fig:Wavenumbers_Real}). The growth in high-frequencies here comes from different constants leading to different rates of linear growth depending upon domain location.
 
 
 %This error contributes little to the short-time training loss resulting in suboptimal NN parameters for recreating high-wavenumber behavior. To overcome this one might think using data spaced further in time could solve the problem, however, in the next section we show the chaotic nature of the system limits the time spacing between data points. It turns out that even using the largest spacing we expect to work does not prevent the high-wavenumber behavior from approximating $f$ in \ref{fig:Fullspace}. 

%\begin{figure*}
%	\includegraphics[trim=0 0 0 0,width=17.2 cm,clip]{Fourier_Comb.png}
%	%\captionsetup{justification=raggedright}
%	\caption{Magnitude of Fourier modes over time when evolving an initial condition with a neural ODE approximation of the full-space dynamics in Fourier space ($\mathcal{F}(f)$) for $L=22$.}
%	\label{fig:Wavenumbers}
%	%\vspace{-5mm}
%\end{figure*}

\begin{figure*}
	\centering
	\captionsetup[subfigure]{labelformat=empty}
	\begin{subfigure}[b]{17.2 cm}
		\includegraphics[trim=0 0 0 0,width=\textwidth,clip]{Fourier_Comb.png}
		\begin{picture}(0,0)
		\put(-240,170){a)}
		\put(-10,170){b)}
		\end{picture}
		\caption{}
		\vspace{-10mm}
		\label{fig:Wavenumbers_Real}
	\end{subfigure}
	\begin{subfigure}[b]{0\textwidth}\caption{}\vspace{-10mm}\label{fig:Wavenumbers}\end{subfigure}
	\captionsetup{justification=raggedright}
	\caption{Magnitude of Fourier modes over time when evolving an initial condition with a neural ODE approximation of (a) the full-space dynamics in real space ($f$) and of (b) the full-space dynamics in Fourier space ($\hat{f}$) for $L=22$.}
	\label{fig:Wavenumbers_Both}
	\vspace{-5mm}
\end{figure*}   

% \begin{figure} 
%	\includegraphics[trim=0 0 0 0,width=8.6 cm,clip]{FullSpace_Fourier.png}
%	%\captionsetup{justification=raggedright}
%	\caption{Magnitude of Fourier modes over time when evolving an initial condition with a neural ODE approximation of the full-space dynamics in Fourier space ($\mathcal{F}(f)$) for $L=22$.}
%	\label{fig:Wavenumbers}
%	%\vspace{-5mm}
%\end{figure} 
%
% \begin{figure} 
%	\includegraphics[trim=0 0 0 0,width=8.6 cm,clip]{FullSpace_Fourier_Real.png}
%	%\captionsetup{justification=raggedright}
%	\caption{Magnitude of Fourier modes over time when evolving an initial condition with a neural ODE approximation of the physical space dynamics ($f$) for $L=22$.}
%	\label{fig:Wavenumbers_Real}
%	%\vspace{-5mm}
%\end{figure} 

\subsection{Effect of Temporal Data Spacing on Time Evolution Prediction}\label{sec:dataspacing}

\begin{figure}
	\centering
	\captionsetup[subfigure]{labelformat=empty}
	\begin{subfigure}[b]{8.6 cm}
		\includegraphics[trim=0 0 0 0,width=\textwidth,clip]{Example.png}
		\begin{picture}(0,0)
		\put(-115,263){\contour{white}{ \textcolor{black}{a)}}}
		\put(-115,206){\contour{white}{ \textcolor{black}{b)}}}
		\put(-115,149){\contour{white}{ \textcolor{black}{c)}}}
		\put(-115,96){\contour{white}{ \textcolor{black}{d)}}}
		\end{picture}
		\caption{}
		\vspace{-10mm}
		\label{fig:Exa}
	\end{subfigure}
	\begin{subfigure}[b]{0\textwidth}\caption{}\vspace{-10mm}\label{fig:Exb}\end{subfigure}\begin{subfigure}[b]{0\textwidth}\caption{}\vspace{-10mm}\label{fig:Exc}\end{subfigure}\begin{subfigure}[b]{0\textwidth}\caption{}\vspace{-10mm}\label{fig:Exd}\end{subfigure}    
	\vspace{-1.5\baselineskip}
	\captionsetup{justification=raggedright}
	\caption{Example of trajectory and predictions for data spaced $\tau=10$ apart: (a) true trajectory; (b) predicted trajectory; (c) absolute error between the two trajectories; (d) norm of the error. The norm of the error is shown for the raw data, and for the spatial shift that minimizes this error at each time.}
	\label{fig:Example} 
	\vspace{-5mm}
\end{figure}


Now we consider only reduced-dimension ODE models, and address the dependence of model quality on the temporal spacing $\tau$ between data points. For $L=22$, $44$, and $66$, we select the manifold dimensions mentioned in Section \ref{sec:autoencoders} -- $d_\IM=8$, $18$, and $28$, respectively. We judge the model performance based on, first, short-time tracking for $L=22$ and then long-time statistical agreement for all domain sizes. In these cases autoencoders and ODEs were both trained with $\tau$ much larger than the value of 0.25 used in Section \ref{sec:dimredev}, while keeping all other training parameters the same (e.g. architecture, optimizer, dimension). Figure \ref{fig:Example} compares a true short-time trajectory (\ref{fig:Exa}) to the model prediction (with $\tau=10$) of this trajectory (\ref{fig:Exb}) starting from the same initial condition. Qualitatively the space-time plots exhibit similar behavior over around 80 time units. The magnitude of the difference is shown in \ref{fig:Exc}. Here we see the difference growing at around 40 time units due primarily to a growing spatial phase difference between the true and predicted results. Figure \ref{fig:Exd} shows the norm of this difference and the minimum value of the norm when shifting $\tilde{u}$ to any position in the domain. The minimum translated difference, in \ref{fig:Exd}, remains much smaller than the standard Euclidean distance indicating that, in this case, the phase difference causes most of the error.

We now consider ensemble average quantities to better understand the reconstruction quality of the models as a function of $\tau$. In addition to the ODE models, we also consider discrete-time maps $h(t_i+\tau)=G(h(t_i))$. 
 %In these trials we compare neural ODEs predictions to discrete-time maps. The discrete-time maps are NN trained to approximate the mapping $h(t_i+\tau)=G(h(t_i))$. 
 Details of the discrete-time map NN architecture is given in Table \ref{Table}. 
 %We do not compare to RNNs or reservoir networks because using previous states for prediction effectively increases the dimension of our state, so $h$ would not longer be minimal.

\begin{figure} 
	\includegraphics[trim=0 0 0 0,width=8.6 cm,clip]{Difference.png}
	%\captionsetup{justification=raggedright}
	\caption{Normalized difference between trajectory and prediction as a function of time for different training data spacing.}
	\label{fig:Difference}
	%\vspace{-5mm}
\end{figure} 

Figure \ref{fig:Difference} shows the root mean squared difference between the exact trajectory and ODE/discrete-time map models as a function of time, averaged over 1000 initial conditions and normalized by the root mean squared difference between random training data, which is denoted with $D$. In this figure, the ODE models and discrete time maps use data spaced $\tau=10-16$. For $\tau<10$, there is little improvement in the ODE models' performance.
%This quantity is the same as in Fig. \ref{fig:Exd}, but averaged over 1000 different initial conditions and normalized. 
ODE predictions at and below $\tau=15$ all track well for short times, with the error being halfway to random at around $t\sim 1.5\tau_L$, and diverge at long times, with the error leveling off at around $t\sim 3\tau_L$. %At around 30 time units is when the model error is about half that of random signals. 
Then, performance degrades sharply at $\tau=16$ (yellow curve). 
%Discrete-time maps perform worse at the lowest data-spacing and gradually deteriorate. 
In all cases, the discrete-time map performs worse than the ODE with the same data spacing. Furthermore, although we do not show it here, the performance for discrete-time maps becomes even worse when trying to interpolate between prediction times. This is a significant drawback not seen when using ODEs.

%Even the best performing discrete-time mapping method is problematic for determining what happens between timesteps. For data spaced this far apart all interpolation methods we considered (linear, cubic, spline) showed large peaks in error between data points. This is a significant drawback to discrete time-mapping with sparse data because, even with excellent approximation of the state at discrete times, between discrete times there is no good way to approximate the state.

\begin{figure} 
	\includegraphics[trim=0 0 0 0,width=8.6 cm,clip]{TempCor.png}
	%\captionsetup{justification=raggedright}
	\caption{Temporal autocorrelation for models learned with different data spacing.}
	\label{fig:TempCor}
	%\vspace{-5mm}
\end{figure} 


A similar trend appears in the temporal autocorrelation. Figure \ref{fig:TempCor} shows the temporal autocorrelation of $u$ at a given gridpoint averaged over space and 1000 initial conditions.
% Reconsider how to explain why you used 10-16
%We generate this plot by averaging the temporal autocorrelation over 1000 initial conditions and for each of the grid points in the domain. 
The temporal autocorrelation of the neural ODE matches the exact temporal autocorrelation well for data spaced up to $\tau=15$. Then, there is an abrupt deviation from the true solution at $\tau=16$. 
%As solutions diverge exponentially fast, it is expected that at some data separation, approximating the model becomes impossible. 
Also, in Fig.\ \ref{fig:TempCor} we show the temporal autocorrelation for discrete-time maps. At $\tau=10$ discrete-time maps perform worse than all ODEs below $\tau=15$, and predictions worsen when increasing $\tau$.

%A similar trend appears in the average difference, Figure \ref{fig:Difference}. Neural ODEs at and below 15 time units all track well for short-times and diverge at long-times, where the error is the same as selecting random signals on the attractor. %At around 30 time units is when the model error is about half that of random signals. 
%As with the temporal autocorrelation the performance remains steady regardless of data-spacing until the sharp change at 16 time units where the error rapidly increases at short-times. Discrete-time maps again perform worse at the lowest data-spacing and gradually deteriorate. Even selecting the best performing discrete-time mapping method is problematic for determining what happens between timesteps. For data spaced this far apart all interpolation methods we considered (linear, cubic, spline) showed large peaks in error between data points. This is a significant drawback to discrete time-mapping with sparse data because, even with excellent approximation of the state at discrete times, between discrete times there is no good way to approximate the state.

\begin{figure*}
	\centering
	\captionsetup[subfigure]{labelformat=empty}
	\begin{subfigure}[b]{17.2 cm}
		\includegraphics[trim=0 0 0 0,width=\textwidth,clip]{PDF.png}
		\begin{picture}(0,0)
		\put(-235,305){a)}
		\put(-45,305){b)}
		\put(98,305){c)}
		\put(-235,165){d)}
		\put(-45,165){e)}
		\put(98,165){f)}
		\end{picture}
		\caption{}
		\vspace{-10mm}
		\label{fig:PDF_True}
	\end{subfigure}
	\begin{subfigure}[b]{0\textwidth}\caption{}\vspace{-10mm}\label{fig:PDFg10}\end{subfigure}\begin{subfigure}[b]{0\textwidth}\caption{}\vspace{-10mm}\label{fig:PDFF10}\end{subfigure}\begin{subfigure}[b]{0\textwidth}\caption{}\vspace{-10mm}\label{fig:PDFg15}\end{subfigure}\begin{subfigure}[b]{0\textwidth}\caption{}\vspace{-10mm}\label{fig:PDFg16}\end{subfigure}\begin{subfigure}[b]{0\textwidth}\caption{}\vspace{-10mm}\label{fig:PDFg20}\end{subfigure}
	\captionsetup{justification=raggedright}
	\caption{(a) joint PDF at $L=22$. (b) and (c) joint PDFs when approximating the ODE and the discrete-time map, respectively, at $\tau=10$. (d)-(f) joint PDFs when approximating the ODE for $\tau=15$, $16$, and $20$.}
	\label{fig:PDF}
	\vspace{-5mm}
\end{figure*} 

The other half of evaluating a model is determining if trajectories stay on the attractor at long times. For this purpose, we consider the long-time joint PDF of the first ($u_x$) and second ($u_{xx}$) spatial derivatives of the solution. We select these quantities because both are relevant to the energy balance for the KSE ($\left<u_x^2\right>$ is the energy production and $\left<u_{xx}^2\right>$ the dissipation), and joint PDFs are more difficult to reconstruct than single PDFs. %or PDFs of the signal alone.
 In Fig.\ \ref{fig:PDF} the joint PDF for data is compared to various models. The colormap is a log scale, with low probability excursions in black, and no data in white regions. When $\tau=10$, the ODE model matches well, with differences primarily in the low probability excursions, while at $\tau=10$ the discrete-time mapping appears more diffuse in the high probability region and matches poorly. At $\tau=15$, the joint PDF for the ODE prediction still matches well, degrading once $\tau\geq 16$. This deterioration becomes more evident at $\tau=20$.

%\begin{figure*}
%	\includegraphics[trim=0 0 0 0,width=17.2 cm,clip]{PDF.png}
%	%\captionsetup{justification=raggedright}
%	\caption{Compares the joint PDF at $L=22$ with neural ODEs trained with different data spacing and discrete time-mapping.}
%	\label{fig:PDF}
%	%\vspace{-5mm}
%\end{figure*}   

\begin{figure} 
	\includegraphics[trim=0 0 0 0,width=8.6 cm,clip]{JointPDF_LT.png}
	%\captionsetup{justification=raggedright}
	\caption{Relative error in Joint PDFs for models trained with different data spacing. Here time is scaled with Lyapunov time for the corresponding domain size.}
	\label{fig:JointPDFError_Temp}
	%\vspace{-5mm}
\end{figure} 


We quantify the difference between true and predicted PDFs by considering the relative error $||P-\tilde{P}||/||P||$ where $P$ corresponds to the true joint PDF and $\tilde{P}$ corresponds to the prediction. This relative error appears in Fig.\ \ref{fig:JointPDFError_Temp} for various $\tau$, where we normalize by the Lyapunov time $\tau_L$. For all domain sizes, the joint PDF reconstruction error is small for neural ODE models when data is spaced below $\sim$0.7 Lyapunov times. In contrast, good reconstruction of the joint PDF with the discrete-time model, for $L=22$, requires $\tau/\tau_L\lesssim 0.4$.

%As expected, at low data spacing the relative error is low and upon reaching a spacing the relative error becomes high. In the case of $L=22$ Neural ODEs the 16 time unit case that failed for short-time statistics corresponds to slightly over 0.7 Lyapunov times. 

%For all cases, $L=22$, $44$ , and $66$ the error is small when data is spaced below \~0.7 Lyapunov times.  
%When we consider larger domain sizes, $L=44$ and $66$, we find the lines collapse when plotted in Lyapunov time indicating models all succeed when data is spaced below around 0.7 Lyapunov times. This highlights that the exponential divergence of initial conditions hinders training. For these larger domain sizes Figure \ref{fig:Dimension} shows how the manifold dimension was determined by the drop in the MSE using the HNN model described in \cite{Linot2020}. The Lyapunov time is 22.2, 12.3, and 11.6 time units for domain sizes 22, 44, and 66, respectively \citep{Ding2016,Edson2019}. 
%\MDG{maybe a table is appropriate, with $L$, $d_\IM$, and Lyapunov time $\tau_L$. } \AL{Added.} 
%In all of these cases, excellent statistical reconstruction is shown up until around 0.7 Lyapunov times after which the models for different domain sizes fail in different ways. In contrast, good reconstruction of this PDF with the discrete-time model requires $\tau/\tau_L\lesssim 0.4$. %However, as shown above, even at 0.4 Lyapunov times the short-time statistics of discrete-time maps perform poorly compared to the Neural ODE method at 0.7 Lyapunov times.

%\begin{table}
%	%\captionsetup{justification=raggedright}
%	\caption{Manifold dimension and Lyapunov time for different domain sizes of the KSE.}
%	\resizebox{.25\textwidth}{!}{%
%		\begin{tabular}{l*{6}{c}r}
%			 $L$ & $d_\IM$ & $\tau_L$ \\
%			\hline
%			22		& 8 & 22.2  \\
%			44		& 18 & 12.3  \\
%			66		& 28 & 11.6  \\
%		\label{Table2}
%		\end{tabular}}
%	\vspace{-5mm}
%\end{table}

%\MDG{Did you report $d_\IM$ vs L?   It's ok if we repeat some plots from the PRE.  I think we should also show some ODEnet results with dimensions other than $d_\IM$.   What we would like to show is that if you choose a higher dimension, the results do not necessarily get better (though maybe they do). And maybe a brief preliminary study of what happens with $d< d_\IM$ -- what do we get right, what do we get wrong?}
%\AL{I can add the $d_\IM$ vs L plot. When $d> d_\IM$ if I use the hybrid model it appears the long-time statistics stay the same, but if I use a standard autoencoder the results begin to deteriorate. With the caveat that Fig 1 shows if I were to keep too many degrees of freedom the model would likely fail. I think the reason this happens is for $d>d_\IM$ the standard autoencoder doesn't improve predictive capability, but now we introduce more variables where error can be introduced from the neural ODE (i.e. the additional degrees of freedom no longer help). On the flip side, if $d< d_\IM$ the hybrid model performs poorly, while the standard autoencoder performs well. The hybrid model is falling on a local minimum close to a linear projection, when for too few dimensions a linear projection does a poor job of representing the state.}

%Along with sparse temporal data, we find that this framework also performs well with sparse spatial data
%
%\begin{figure} 
%	\includegraphics[trim=0 0 0 0,width=8.6 cm,clip]{JointPDF_Spat.png}
%	%\captionsetup{justification=raggedright}
%	\caption{Relative Error in Joint-PDFs for states with different sampling.}
%	\label{fig:JointPDFError_Spat}
%	%\vspace{-5mm}
%\end{figure} 

\subsection{Effect of Model Dimension on Time Evolution Predictions}\label{sec:dimdependence}

\begin{table}
	\captionsetup{justification=raggedright}
	\caption{Architectures of NNs used in Section \ref{sec:dimdependence}. Labels are the same as in Table \ref{Table}.}
	\resizebox{.65\textwidth}{!}{%
		\begin{tabular}{l*{6}{c}r}
			              & Function & Shape & Activation \\
			\hline
			Encoder $L=22$		& $\chi$ 			& $d:500:d_h$  & S:linear  \\
			Encoder	$L=44,66$	& $\chi$ 			& $d:500:500:d_h$  & S:S:linear  \\
			Decoder $L=22$		& $\check{\chi}$ 	& $d_h:500:d$  & S:linear  \\
			Decoder $L=44,66$	& $\check{\chi}$ 	& $d_h:500:500:d$  & S:S:linear  \\
			ODE					& $g$ 				& $d_h:200:200:200:d_h$ & S:S:S:linear  \\
		\label{Table2}
		\end{tabular}}
	%\vspace{-5mm}
\end{table}

In the previous sections, the model dimension was fixed at values assumed to be the correct inertial manifold dimensions based on other studies \cite{Linot2020,Yang2009,Ding2016}. Here we examine model performance as a function of dimension.

%In the last cases we assumed we knew the dimension of the manifold based on previous studies. Now we assume we do not know the manifold dimension and investigate how the dimension of the system affects short-time tracking and long-time statistics.
%When using the correct number of dimensions with neural ODEs the results perform well for widely spaced data, but the results are less clear with the incorrect number of dimensions. 
%At fewer dimension, we find that autoencoders with nonlinear encoding give better statistical recreation than a linear encoder, as shown in \ref{fig:Lin_vs_nonlin}. Due to these better results at fewer dimensions the following results use NNs for encoding, as opposed to the linear encoding in the previous trials. Table \ref{Table} shows the architectures for these models. 
For training we again use $10^5$ time units of data spaced apart 0.25 time units and train 5 autoencoders and 15 neural ODEs. Our first result here is the observation that, for models with a lower dimension than the ``correct" one, reasonable approximations of the joint PDF of $u_x$ and $u_{xx}$ can be obtained, but only if a nonlinear encoder is used for dimension reduction. This point is illustrated in Figure \ref{fig:Lin_vs_nonlin_PDF}, for the case $L=22$, where we see good reconstruction of the joint PDF when using a nonlinear encoder. Accordingly, the results below all use a nonlinear encoder; architectures are reported in Table \ref{Table2}.  However, even though this statistic is reconstructed well at low dimensions, Fig.\ \ref{fig:Nonlin_Auto} shows the mean squared error for the autoencoder, with a nonlinear encoder, still drops significantly at $d_h=8$.

\begin{figure*}
	\centering
	\captionsetup[subfigure]{labelformat=empty}
	\begin{subfigure}[b]{17.2 cm}
		\includegraphics[trim=0 0 0 0,width=\textwidth,clip]{JointPDF_LinNonlin_Final.png}
		\begin{picture}(0,0)
		\put(-240,160){a)}
		\put(20,160){b)}
		\end{picture}
		\caption{}
		\vspace{-10mm}
		\label{fig:Lin_vs_nonlin_PDF}
	\end{subfigure}
	\begin{subfigure}[b]{0\textwidth}\caption{}\vspace{-10mm}\label{fig:Nonlin_Auto}\end{subfigure}
	\captionsetup{justification=raggedright}
	\caption{(a) is the relative error in the joint PDF with the best autoencoder and neural ODE pair at each dimension. (b) is the mean squared error of reconstruction for the nonlinear autoencoder at each dimension. Both are for a domain size of $L=22$.}
	\label{fig:Lin_vs_nonlin}
	\vspace{-5mm}
\end{figure*}   

%\begin{figure} 
%	\includegraphics[trim=0 0 0 0,width=8.6 cm,clip]{JointPDF_LinNonlin_Final.png}
%	%\captionsetup{justification=raggedright}
%	\caption{Joint PDF relative error with the best autoencoder and neural ODE pairs at various dimensions for $L=22$.}
%	\label{fig:Lin_vs_nonlin}
%	%\vspace{-5mm}
%\end{figure} 


\begin{figure*}
	\centering
	\captionsetup[subfigure]{labelformat=empty}
	\begin{subfigure}[b]{17.2 cm}
		\includegraphics[trim=0 0 0 0,width=\textwidth,clip]{Trajectory_VaryDim_Comb.png}
		\begin{picture}(0,0)
		\put(-235,150){a)}
		\put(-65,150){b)}
		\put(90,150){c)}
		\end{picture}
		\caption{}
		\vspace{-10mm}
		\label{fig:Trajectory_VaryDim}
	\end{subfigure}
	\begin{subfigure}[b]{0\textwidth}\caption{}\vspace{-10mm}\label{fig:Trajectory_VaryDim44}\end{subfigure}
	\begin{subfigure}[b]{0\textwidth}\caption{}\vspace{-10mm}\label{fig:Trajectory_VaryDim66}\end{subfigure}
	\captionsetup{justification=raggedright}
	\caption{Short-time error at various dimensions. Domain sizes are $L=22,44,66$ for (a)-(c).}
	\label{fig:Trajectory_VaryDim_Comb}
	\vspace{-5mm}
\end{figure*}   

%\begin{figure} 
%	\includegraphics[trim=0 0 0 0,width=8.6 cm,clip]{Trajectory_VaryDim.png}
%	%\captionsetup{justification=raggedright}
%	\caption{Short-time error at various dimensions for $L=22$.}
%	\label{fig:Trajectory_VaryDim}
%	%\vspace{-5mm}
%\end{figure} 

Figure \ref{fig:Trajectory_VaryDim_Comb} shows the ensemble-averaged short-time tracking error of the models with the best tracking as the dimension varies. %We select the best models in this case for clarity, but the difference between models here is small. 
For all domain sizes the recreation gradually improves and then becomes dimension-independent as dimension increases. This happens because the short-time tracking is directly related to the loss minimized in training the neural ODEs, and $h$ contains the same information as $u$ if $d_h$ is large enough. As mentioned before, for $L=22$, $44$, and $66$, respectively, the ``correct" manifold dimensions are $8$, $18$, and $28$, and Fig.\ \ref{fig:Trajectory_VaryDim_Comb} shows the tracking error becoming dimension-independent near these values. 

%In Fig. \ref{fig:Trajectory_VaryDim} we show the short-time tracking for $L=22$. For this domain size there is a noticeable difference in the tracking once the intrinsic dimension of the manifold ($d_h=8$) is reached. For larger domain sizes $L=44$ and $L=66$, Fig. \ref{fig:Trajectory_VaryDim44} and \ref{fig:Trajectory_VaryDim66}, again we see short-time tracking improvement as dimension increases. However, the improvement in performance at the intrinsic dimension of the manifold, $d_h=18$ and $d_h=28$, is less evident.

%This follows from the fact that the dynamics are learned by minimizing the error in recreating closely spaced data and the autoencoder recreation tends to perform better with more dimensions. 

%For $L=22$ there there is a noticeable difference between performance after reaching the intrinsic dimension. 

%\begin{figure} 
%	\includegraphics[trim=0 0 0 0,width=8.6 cm,clip]{Trajectory_VaryDim44.png}
%	%\captionsetup{justification=raggedright}
%	\caption{Short-time error at various dimensions for $L=44$.}
%	\label{fig:Trajectory_VaryDim44}
%	%\vspace{-5mm}
%\end{figure} 
%
%\begin{figure} 
%	\includegraphics[trim=0 0 0 0,width=8.6 cm,clip]{Trajectory_VaryDim66.png}
%	%\captionsetup{justification=raggedright}
%	\caption{Short-time error at various dimensions for $L=66$.}
%	\label{fig:Trajectory_VaryDim66}
%	%\vspace{-5mm}
%\end{figure} 

Now we turn to long-time statistical recreation upon varying dimension. Figure \ref{fig:JointPDFError_VaryDim_Comb} shows the relative error in the joint PDFs of $u_x$ and $u_{xx}$ for all the autoencoder and neural ODE pairs trained for each dimension and domain size. Unlike short-time tracking, the relative error of the best models does not monotonically drop with dimension. Also, Fig.\ \ref{fig:JointPDFError_VaryDim_Comb} shows there is a large differences in model performance at a given dimension, despite all models at a given dimension reaching a similar loss during training. These large differences come from trajectories of many of the models leaving the attractor, which can happen because the loss is directly tied to short-time tracking, not long-time statistics. 
%These large differences in performance appear to happen because this statistic is not directly tied to the loss. 
In all the cases, when the dimension is low the models do a poor job of reconstructing the joint PDF. When we are near the expected manifold dimension, the average model performance improves and the variance in performance between the models decreases. However, after further increasing the dimension there is an increase in the variance in performance between models, despite little change in the performance of the best models. Like the short-time tracking, the joint PDF of the best models become dimension-independent, but here we see that issues arise when training the neural ODE with unnecessary dimensions.

 %an initial improvement in the joint PDF, and then a decrease in the accuracy. The training loss is not directly tied to these long-time statistics, so it is possible for the short-time statistics to improve with dimension while the long-time statistics degrade. This agrees with the conclusion in Section \ref{sec:dimredev} that training neural ODEs without dimension reduction leads to erroneous long-time behavior.

%In Fig. \ref{fig:JointPDFError_VaryDim} we show the relative error in the joint PDF for $L=22$. When there are fewer dimensions than the expected manifold dimension there is a large variability in reconstruction, and the error of the best models is also high. At the expected dimension, both the relative error and the variability in the relative error are low. Then, for higher dimensions, the variability becomes even larger than at few dimensions, with most models performing poorly. However, the a few models at these higher dimensions perform nearly as well as the best model at the intrinsic dimension.

\begin{figure*}
	\centering
	\captionsetup[subfigure]{labelformat=empty}
	\begin{subfigure}[b]{17.2 cm}
		\includegraphics[trim=0 0 0 0,width=\textwidth,clip]{JointPDF_VaryDim_Comb.png}
		\begin{picture}(0,0)
		\put(-230,145){a)}
		\put(-55,145){b)}
		\put(95,145){c)}
		\end{picture}
		\caption{}
		\vspace{-10mm}
		\label{fig:JointPDFError_VaryDim}
	\end{subfigure}
	\begin{subfigure}[b]{0\textwidth}\caption{}\vspace{-10mm}\label{fig:JointPDFError_VaryDim44}\end{subfigure}
	\begin{subfigure}[b]{0\textwidth}\caption{}\vspace{-10mm}\label{fig:JointPDFError_VaryDim66}\end{subfigure}
	\captionsetup{justification=raggedright}
	\caption{Relative error in Joint PDFs for all trained models at different dimensions. Domain sizes are $L=22,44,66$ for (a)-(c).}
	\label{fig:JointPDFError_VaryDim_Comb}
	\vspace{-5mm}
\end{figure*}   

%\begin{figure} 
%	\includegraphics[trim=0 0 0 0,width=8.6 cm,clip]{JointPDF_VaryDim.png}
%	%\captionsetup{justification=raggedright}
%	\caption{Relative error in $L=22$ Joint PDFs for all trained models at different dimensions.}
%	\label{fig:JointPDFError_VaryDim}
%	%\vspace{-5mm}
%\end{figure} 

%We show the larger domain sizes $L=44$ and $L=66$ in Figs. \ref{fig:JointPDFError_VaryDim44} and \ref{fig:JointPDFError_VaryDim66}. For $L=44$ at few dimensions the model performance is poor and the variability is high. Upon increasing the dimension, the models improve and show similar performance between $d_h\sim 15-18$. Then, at higher dimension the variability in the models again becomes large. Unlike for $L=22$, the joint PDF is captured well slightly below the expected intrinsic dimension, and there is little variability in these models. 

%$L=66$ shows similar trends as $L=44$. These models perform poorly at few dimensions, then there is a range of dimensions $d_h\sim 21-26$ where there is little variability and the reconstruction is accurate, and at higher dimensions the variability is high, but the best models perform well. In the case of $L=66$, this increase in the variability of the models happens before the expected intrinsic dimension of $d_h=28$. The large variability in these trials likely comes from the variability in the different representations of $h$ for the different autoencoders -- a problem that did not appear in Sections \ref{sec:dimredev} and \ref{sec:dataspacing} because of the different $\chi$. Further investigation needs to be put into what methods may alleviate the large variability in training.  

%above the intrinsic dimension of $d_\IM=18$. Unlike with $L=22$, there is less variability with too few dimensions. Upon increasing the domain size to $L=66$, Fig. \ref{fig:JointPDFError_VaryDim66}, we see similar trends to the other domain sizes, but performance decreases before the expected intrinsic dimension of 28. 

%\begin{figure} 
%	\includegraphics[trim=0 0 0 0,width=8.6 cm,clip]{JointPDF_VaryDim44.png}
%	%\captionsetup{justification=raggedright}
%	\caption{Relative error in $L=44$ Joint PDFs for all trained models at different dimensions.}
%	\label{fig:JointPDFError_VaryDim44}
%	%\vspace{-5mm}
%\end{figure} 

%\begin{figure} 
%	\includegraphics[trim=0 0 0 0,width=8.6 cm,clip]{JointPDF_VaryDim66.png}
%	%\captionsetup{justification=raggedright}
%	\caption{Relative error in $L=66$ Joint PDFs for all trained models at different dimensions.}
%	\label{fig:JointPDFError_VaryDim66}
%	%\vspace{-5mm}
%\end{figure} 

%Unlike with the full dynamics, learning the dynamics slightly over the intrinsic dimensions occasionally lands on solutions with good agreement. The addition of redundant dimensions hurts training, but good solutions still exist. Surprisingly, many of the lower dimension cases also generate joint PDFs that agree well with the data. This result is unexpected because information ought to be lost upon using too few dimensions.


%To find out what information may be lost we consider a more difficult statistic to recreate -- the correlation dimension. The correlation dimension is the log of the correlation integral divided by the log of $r$ as $r$ goes to zero. The correlation integral is given by
%\begin{equation}\label{eq:Corr}
%	C(N, r)=\frac{1}{N(N-1)} \sum_{i \neq j} \Theta\left(r-d(u_{i},u_{j})\right),
%\end{equation}
%where $\Theta$ is the indicator function, $u$ is a snapshot of data, and $N$ is the number of snapshots. This quantity measures how many snapshots are below a certain distance between one another. Due to the translational invariance of the system we find the minimum translated distance between snapshots for calculating the correlation integral (i.e. $d(u_{i},u_{j})=\displaystyle \min_s\left\|u(x)_{i}-u(x+s)_{j}\right\|$). 
%
%In Figure \ref{fig:Corrdim} we show the correlation integral for $L=22$. When the dimension of the reduced space is too low the recreation is poor especially when the data separation $r$ becomes small. This indicates that with too few dimensions small separations between signals are collapsed to the same point, a fact that would not necessarily appear in the joint PDF. 
%
%\begin{figure} 
%	\includegraphics[trim=0 0 0 0,width=8.6 cm,clip]{CorrDim.png}
%	%\captionsetup{justification=raggedright}
%	\caption{Correlation integral at various dimensions for $L=22$.}
%	\label{fig:Corrdim}
%	%\vspace{-5mm}
%\end{figure} 
%
%With a domain size of $L=44$, \ref{fig:Corrdim44}, again the results are less clear. All of the models push trajectories further away from one another resulting in fewer data points with small separations. The models with too few dimensions never meet back up with the data, but the model at the intrinsic dimension does.The deviation for the intermediate $r$ values could possibly be because more data is needed to resolve small separations when the dynamics become more complex.
%
%\begin{figure} 
%	\includegraphics[trim=0 0 0 0,width=8.6 cm,clip]{CorrDim44.png}
%	%\captionsetup{justification=raggedright}
%	\caption{Correlation integral at various dimensions for $L=44$.}
%	\label{fig:Corrdim44}
%	%\vspace{-5mm}
%\end{figure} 
%%5

\section{Conclusion} \label{sec:Conclusion}

Neural ODEs, in conjunction with a low-dimensional approximation of the state in the manifold coordinates, provide a means of accurate data-driven time-evolution near the expected manifold dimension using data that is widely spaced in time. Dimension reduction is a vital step in the process. Not only is training and evolution of Neural ODEs more expensive with more dimensions, but we find that trying to approximate the dynamics with too many dimensions leads to the excitation of high-wavenumber modes that push trajectories off the attractor. With dimension reduction, we find that training with data spaced up to $\sim0.7$ Lyapunov times gives accurate short-time tracking and long-time statistical reconstruction. Finally, we investigate the impact of varying the dimension and find that performance improves with dimension, and then levels off. However, keeping too many dimensions hurts training, resulting in many poor models.

%and can lead to the excitation of high frequencies found when learning the full dynamics. 

\section*{Data Availability}
The data that support the findings of this study are available from the corresponding author upon reasonable request.

\begin{acknowledgments}
This work was supported by AFOSR  FA9550-18-1-0174 and ONR N00014-18-1-2865 (Vannevar Bush Faculty Fellowship).
\end{acknowledgments}

%6
% Overview key results (Dimensionality reduction, energy conservation, invariances)


% References should be done using the \cite, \ref, and \label commands
%\section{}
% Put \label in argument of \section for cross-referencing
%\section{\label{}}
%\subsection{}
%\subsubsection{}

% If in two-column mode, this environment will change to single-column
% format so that long equations can be displayed. Use
% sparingly.
%\begin{widetext}
% put long equation here
%\end{widetext}

% figures should be put into the text as floats.
% Use the graphics or graphicx packages (distributed with LaTeX2e)
% and the \includegraphics macro defined in those packages.
% See the LaTeX Graphics Companion by Michel Goosens, Sebastian Rahtz,
% and Frank Mittelbach for instance.
%
% Here is an example of the general form of a figure:
% Fill in the caption in the braces of the \caption{} command. Put the label
% that you will use with \ref{} command in the braces of the \label{} command.
% Use the figure* environment if the figure should span across the
% entire page. There is no need to do explicit centering.

% \begin{figure}
% \includegraphics{}%
% \caption{\label{}}
% \end{figure}

% Surround figure environment with turnpage environment for landscape
% figure
% \begin{turnpage}
% \begin{figure}
% \includegraphics{}%
% \caption{\label{}}
% \end{figure}
% \end{turnpage}

% tables should appear as floats within the text
%
% Here is an example of the general form of a table:
% Fill in the caption in the braces of the \caption{} command. Put the label
% that you will use with \ref{} command in the braces of the \label{} command.
% Insert the column specifiers (l, r, c, d, etc.) in the empty braces of the
% \begin{tabular}{} command.
% The ruledtabular enviroment adds doubled rules to table and sets a
% reasonable default table settings.
% Use the table* environment to get a full-width table in two-column
% Add \usepackage{longtable} and the longtable (or longtable*}
% environment for nicely formatted long tables. Or use the the [H]
% placement option to break a long table (with less control than 
% in longtable).
% \begin{table}%[H] add [H] placement to break table across pages
% \caption{\label{}}
% \begin{ruledtabular}
% \begin{tabular}{}
% Lines of table here ending with \\
% \end{tabular}
% \end{ruledtabular}
% \end{table}

% Surround table environment with turnpage environment for landscape
% table
% \begin{turnpage}
% \begin{table}
% \caption{\label{}}
% \begin{ruledtabular}
% \begin{tabular}{}
% \end{tabular}
% \end{ruledtabular}
% \end{table}
% \end{turnpage}

% Specify following sections are appendices. Use \appendix* if there
% only one appendix.
%\appendix
%\section{}

% If you have acknowledgments, this puts in the proper section head.
%\begin{acknowledgments}
% put your acknowledgments here.
%\end{acknowledgments}
\begin{thebibliography}{10}

\bibitem{Bengio2004}
Y.~Bengio, J.~F. Paiement, P.~Vincent, O.~Delalleau, N.~{Le Roux}, and
  M.~Ouimet.
\newblock {Out-of-sample extensions for LLE, Isomap, MDS, Eigenmaps, and
  spectral clustering}.
\newblock {\em Advances in Neural Information Processing Systems}, 2004.

\bibitem{Chen2019}
R.~T.~Q. Chen, Y.~Rubanova, J.~Bettencourt, and D.~Duvenaud.
\newblock Neural ordinary differential equations.
\newblock {\em arXiv preprint arXiv:1806.07366}, 2019.

\bibitem{cho-etal-2014-properties}
K.~Cho, B.~van Merri{\"e}nboer, D.~Bahdanau, and Y.~Bengio.
\newblock On the properties of neural machine translation: Encoder{--}decoder
  approaches.
\newblock In {\em Proceedings of {SSST}-8, Eighth Workshop on Syntax, Semantics
  and Structure in Statistical Translation}, pages 103--111, Doha, Qatar, Oct.
  2014. Association for Computational Linguistics.

\bibitem{chollet2015keras}
F.~Chollet et~al.
\newblock Keras.
\newblock \url{https://keras.io}, 2015.

\bibitem{Cunningham2015}
J.~P. Cunningham and Z.~Ghahramani.
\newblock {Linear dimensionality reduction: Survey, insights, and
  generalizations}.
\newblock {\em Journal of Machine Learning Research}, 16:2859--2900, 2015.

\bibitem{ChaosBook}
P.~Cvitanovi{\'c}, R.~Artuso, R.~Mainieri, G.~Tanner, and G.~Vattay.
\newblock {\em Chaos: Classical and Quantum}.
\newblock Niels Bohr Inst., Copenhagen, 2016.

\bibitem{Ding2016}
X.~Ding, H.~Chat{\'{e}}, P.~Cvitanovi{\'{c}}, E.~Siminos, and K.~A. Takeuchi.
\newblock {Estimating the Dimension of an Inertial Manifold from Unstable
  Periodic Orbits}.
\newblock {\em Physical Review Letters}, 117(2):1--5, 2016.

\bibitem{Doering1988}
C.~R. Doering, J.~D. Gibbon, D.~D. Holm, and B.~Nicolaenko.
\newblock {Low-dimensional behaviour in the complex Ginzburg-Landau equation}.
\newblock {\em Nonlinearity}, 1(2):279--309, 1988.

\bibitem{Edson2019}
R.~A. Edson, J.~E. Bunder, T.~W. Mattner, and A.~J. Roberts.
\newblock Lyapunov exponents of the {K}uramoto{S}ivashinsky {PDE}.
\newblock {\em The ANZIAM Journal}, 61(3):270285, 2019.

\bibitem{Ferguson2011}
A.~L. Ferguson, A.~Z. Panagiotopoulos, I.~G. Kevrekidis, and P.~G. Debenedetti.
\newblock {Nonlinear dimensionality reduction in molecular simulation: The
  diffusion map approach}.
\newblock {\em Chemical Physics Letters}, 509(1-3):1--11, 2011.

\bibitem{floryan2021charts}
D.~Floryan and M.~D. Graham.
\newblock Charts and atlases for nonlinear data-driven models of dynamics on
  manifolds.
\newblock {\em arXiv preprint arXiv:2108.05928}, 2021.

\bibitem{Foias1988}
C.~Foias, O.~Manley, and R.~Temam.
\newblock Modelling of the interaction of small and large eddies in two
  dimensional turbulent flows.
\newblock {\em ESAIM: Mathematical Modelling and Numerical Analysis -
  Mod\'elisation Math\'ematique et Analyse Num\'erique}, 22(1):93--118, 1988.

\bibitem{Foias1988a}
C.~Foias, B.~Nicolaenko, G.~R. Sell, and R.~Temam.
\newblock {Inertial manifold for the Kuramoto-Sivashinsky equation and an
  estimate of their lowest dimension}.
\newblock {\em J. Math. Pure Appl.}, 67:197--226, 1988.

\bibitem{Fukami2020}
K.~Fukami, T.~Nakamura, and K.~Fukagata.
\newblock Convolutional neural network based hierarchical autoencoder for
  nonlinear mode decomposition of fluid field data.
\newblock {\em Physics of Fluids}, 32(9):095110, 2020.

\bibitem{Gonzalez-Garcia1998}
R.~Gonzalez-Garcia, R.~Rico-Martinez, and I.~G. Kevrekidis.
\newblock Identification of distributed parameter systems: A neural net based
  approach.
\newblock {\em Computers \& Chemical EngineeringComputers \& Chemical
  Engineering}, 22:S965--S968, 1998.

\bibitem{Hinton2003}
G.~Hinton and S.~Roweis.
\newblock Stochastic neighbor embedding.
\newblock {\em Advances in neural information processing systems}, 15:833--840,
  2003.

\bibitem{Hinton2006}
G.~E. Hinton and R.~R. Salakhutdinov.
\newblock Reducing the dimensionality of data with neural networks.
\newblock {\em Science}, 313(5786):504--507, 2006.

\bibitem{Hochreiter1997}
S.~Hochreiter and J.~Schmidhuber.
\newblock {Long Short-Term Memory}.
\newblock {\em Neural Computation}, 9(8):1735--1780, 1997.

\bibitem{IanGoodfellowYoshuaBengio2017}
A.~C. {Ian Goodfellow, Yoshua Bengio}.
\newblock {\em {The Deep Learning Book}}, volume 521.
\newblock 2017.

\bibitem{Ivancevic2007}
V.~G. Ivancevic.
\newblock {\em High-Dimensional Chaotic and Attractor Systems}.
\newblock Springer Publishing Company, Incorporated, 2007.

\bibitem{Jolly2000}
M.~S. Jolly, R.~Rosa, and R.~Temam.
\newblock {Evaluating the dimension of an inertial manifold for the
  Kuramoto-Sivashinsky equation}.
\newblock {\em Advances in Differential Equations}, 5(1-3):31--66, 2000.

\bibitem{Kassam2005}
A.-K. Kassam and L.~N. Trefethen.
\newblock Fourth-order time-stepping for stiff {PDE}s.
\newblock {\em SIAM Journal on Scientific Computing}, 26(4):1214--1233, 2005.

\bibitem{DMDBook}
J.~N. Kutz, S.~L. Brunton, B.~W. Brunton, and J.~L. Proctor.
\newblock {\em Dynamic Mode Decomposition}.
\newblock Society for Industrial and Applied Mathematics, Philadelphia, PA,
  2016.

\bibitem{lee2003}
J.~Lee and J.~Lee.
\newblock {\em Introduction to Smooth Manifolds}.
\newblock Graduate Texts in Mathematics. Springer, 2003.

\bibitem{Linot2020}
A.~J. Linot and M.~D. Graham.
\newblock Deep learning to discover and predict dynamics on an inertial
  manifold.
\newblock {\em Phys. Rev. E}, 101:062209, 2020.

\bibitem{MAULIK2020}
R.~Maulik, A.~Mohan, B.~Lusch, S.~Madireddy, P.~Balaprakash, and D.~Livescu.
\newblock Time-series learning of latent-space dynamics for reduced-order model
  closure.
\newblock {\em Physica D: Nonlinear Phenomena}, 405:132368, 2020.

\bibitem{Milano2002}
M.~Milano and P.~Koumoutsakos.
\newblock {Neural network modeling for near wall turbulent flow}.
\newblock {\em Journal of Computational Physics}, 182(1):1--26, 2002.

\bibitem{Nair2020}
N.~J. Nair and A.~Goza.
\newblock {Leveraging reduced-order models for state estimation using deep
  learning}.
\newblock {\em Journal of Fluid Mechanics}, 897:1--13, 2020.

\bibitem{Omata2019}
N.~Omata and S.~Shirayama.
\newblock A novel method of low-dimensional representation for temporal
  behavior of flow fields using deep autoencoder.
\newblock {\em AIP Advances}, 9(1):015006, 2019.

\bibitem{Page2020}
J.~Page, M.~P. Brenner, and R.~R. Kerswell.
\newblock Revealing the state space of turbulence using machine learning.
\newblock {\em Phys. Rev. Fluids}, 6:034402, 2021.

\bibitem{Paszke2019}
A.~Paszke, S.~Gross, F.~Massa, A.~Lerer, J.~Bradbury, G.~Chanan, T.~Killeen,
  Z.~Lin, N.~Gimelshein, L.~Antiga, A.~Desmaison, A.~Kopf, E.~Yang, Z.~DeVito,
  M.~Raison, A.~Tejani, S.~Chilamkurthy, B.~Steiner, L.~Fang, J.~Bai, and
  S.~Chintala.
\newblock Pytorch: An imperative style, high-performance deep learning library.
\newblock In {\em Advances in Neural Information Processing Systems 32}, pages
  8024--8035. Curran Associates, Inc., 2019.

\bibitem{Pathak2018a}
J.~Pathak, B.~Hunt, M.~Girvan, Z.~Lu, and E.~Ott.
\newblock {Model-Free Prediction of Large Spatiotemporally Chaotic Systems from
  Data: A Reservoir Computing Approach}.
\newblock {\em Physical Review Letters}, 120(2):24102, 2018.

\bibitem{portwood2019turbulence}
G.~D. Portwood, P.~P. Mitra, M.~D. Ribeiro, T.~M. Nguyen, B.~T. Nadiga, J.~A.
  Saenz, M.~Chertkov, A.~Garg, A.~Anandkumar, A.~Dengel, R.~Baraniuk, and D.~P.
  Schmidt.
\newblock Turbulence forecasting via neural {ODE}.
\newblock {\em arXiv preprint arXiv:1911.05180}, 2019.

\bibitem{Raissi2018}
M.~Raissi, P.~Perdikaris, and G.~E. Karniadakis.
\newblock {Multistep Neural Networks for Data-driven Discovery of Nonlinear
  Dynamical Systems}.
\newblock {\em arXiv preprint arXiv:1801.01236}, pages 1--19, 2018.

\bibitem{rojas2021reducedorder}
C.~J.~G. Rojas, A.~Dengel, and M.~D. Ribeiro.
\newblock Reduced-order model for fluid flows via neural ordinary differential
  equations.
\newblock {\em arXiv preprint arXiv:2102.02248}, 2021.

\bibitem{Roweis2323}
S.~T. Roweis and L.~K. Saul.
\newblock Nonlinear dimensionality reduction by locally linear embedding.
\newblock {\em Science}, 290(5500):2323--2326, 2000.

\bibitem{Sauer1991}
T.~Sauer, J.~A. Yorke, and M.~Casdagli.
\newblock Embedology.
\newblock {\em Journal of Statistical Physics}, 65(3):579--616, 1991.

\bibitem{Takens}
F.~Takens.
\newblock Detecting strange attractors in turbulence.
\newblock In D.~Rand and L.-S. Young, editors, {\em Dynamical Systems and
  Turbulence, Warwick 1980}, pages 366--381, Berlin, Heidelberg, 1981. Springer
  Berlin Heidelberg.

\bibitem{Temam1989}
R.~Temam.
\newblock {Do inertial manifolds apply to turbulence?}
\newblock {\em Physica D: Nonlinear Phenomena}, 37(1-3):146--152, 1989.

\bibitem{Temam1990}
R.~Temam.
\newblock {Inertial manifolds}.
\newblock {\em The Mathematical Intelligencer}, 12(4):68--74, 1990.

\bibitem{Temam1994}
R.~Temam and X.~Wang.
\newblock {Estimates on the lowest dimension of inertial manifolds for the
  Kuramoto-Sivasbinsky equation in the general case}.
\newblock {\em Differential and Integral Equations}, 7(3-4):1095--1108, 1994.

\bibitem{VanDerMaaten2009}
L.~J.~P. {Van Der Maaten}, E.~O. Postma, and H.~J. {Van Den Herik}.
\newblock {Dimensionality Reduction: A Comparative Review}.
\newblock {\em Journal of Machine Learning Research}, 10:1--41, 2009.

\bibitem{Vlachas2019}
P.~Vlachas, J.~Pathak, B.~Hunt, T.~Sapsis, M.~Girvan, E.~Ott, and
  P.~Koumoutsakos.
\newblock Backpropagation algorithms and reservoir computing in recurrent
  neural networks for the forecasting of complex spatiotemporal dynamics.
\newblock {\em Neural Networks}, 126:191--217, 2020.

\bibitem{Vlachas2018}
P.~R. Vlachas, W.~Byeon, Z.~Y. Wan, T.~P. Sapsis, and P.~Koumoutsakos.
\newblock Data-driven forecasting of high-dimensional chaotic systems with long
  short-term memory networks.
\newblock {\em Proceedings of the Royal Society A: Mathematical, Physical and
  Engineering Sciences}, 474(2213):20170844, 2018-05.

\bibitem{Whitney1944}
H.~Whitney.
\newblock The self-intersections of a smooth n-manifold in 2n-space.
\newblock {\em Annals of Mathematics}, 45(2):220--246, 1944.

\bibitem{Yang2009}
H.~L. Yang, K.~A. Takeuchi, F.~Ginelli, H.~Chat{\'{e}}, and G.~Radons.
\newblock {Hyperbolicity and the effective dimension of spatially extended
  dissipative systems}.
\newblock {\em Physical Review Letters}, 102(7):1--4, 2009.

\bibitem{Zelik2014}
S.~Zelik.
\newblock {Inertial manifolds and finite-dimensional reduction for dissipative
  PDEs}.
\newblock {\em Proceedings of the Royal Society of Edinburgh Section A:
  Mathematics}, 144(6):1245--1327, 2013.

\end{thebibliography}

\end{document}
%
% ****** End of file apstemplate.tex ******

