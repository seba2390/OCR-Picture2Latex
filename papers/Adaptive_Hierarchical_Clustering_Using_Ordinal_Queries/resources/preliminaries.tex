%% Definitions and Preliminaries

\section{Definitions and Preliminaries} \label{sec:preliminaries}

\subsection{Hierarchical Clusterings}

Let \AllElements be a set of $n \geq 2$ \elements.
A \emph{hierarchical clustering}
of \AllElements can
be equivalently characterized in two ways:
\begin{enumerate}
\item A laminar family \SetFam of subsets\footnote{%
Recall that a family of sets is
\emph{laminar} if $A, B \in \SetFam$ and $A \cap B \neq \emptyset$
implies that $A \subseteq B$ or $B \subseteq A$.}
(called \emph{clusters}) of \AllElements,
with the properties that $\emptyset \notin \SetFam$, 
$\AllElements \in \SetFam$ and
$\Set{\ElS} \in \SetFam$ for all $\ElS \in \AllElements$.
\item A rooted tree \Tree
whose leaves are the \elements $\ElS \in \AllElements$.
\end{enumerate}

The equivalence is obvious:
the clusters in \SetFam exactly correspond to vertices \Vertex in \Tree,
or more specifically, to the set of leaves in the subtree rooted at \Vertex.
We call the set of all clusters corresponding to rooted subtrees of \Tree
the \emph{hierarchical clustering corresponding to \Tree},
and denote it by \IndClust{\Tree}.
When there is no risk of confusion,
we also refer to \Tree itself as a hierarchical clustering.

We primarily use the representation in terms of the tree \Tree.
In keeping with much of the prior literature,
(\cite{dasgupta:2016:hierarchy-cost,%
clauset-moore-newman:2008:link-prediction,%
felsenstein:2004:phylogenies}, for example)
we focus
(except in Section~\ref{sec:generalized-hierarchical-clustering})
on the case when \Tree is binary.
In terms of the representation \SetFam, this means that for each
$\ElSet \in \SetFam$ with $\SetCard{\ElSet} \geq 2$,
there exists a set $A \subsetneq \ElSet$
with $A, \ElSet \setminus A \in \SetFam$.
We assume without loss of generality that \Tree has no internal node
with one child,
since such a node could be removed without changing
the corresponding family of sets.

There is an unknown ground truth hierarchical clustering \TreeOpt,
which an algorithm is to recover using ordinal queries.
The algorithm succeeds if it finds a rooted binary tree \Tree
with $\IndClust{\Tree} = \IndClust{\TreeOpt}$;
this corresponds to the rooted binary trees \Tree and \TreeOpt being isomorphic
(when the ordering of left vs.~right subtrees is immaterial).
In this case, we also write $\Tree \equiv \TreeOpt$.

For a set $S \subseteq \AllElements$ with
$\SetCard{S} \geq 2$,
the \emph{induced hierarchy} \SetFam[S] is defined in the obvious way:
$\SetFam[S] = \SpecSet{\ElSet \cap S}{\ElSet \in \SetFam}$.
We define the \emph{induced tree} \TreeI{S} as follows:
remove from \Tree all leaves not in $S$,
then repeatedly remove all internal nodes without children,
and contract all nodes with a single child.\footnote{%
If the root has one child, it is removed,
and its child becomes the root of the tree.}

\begin{proposition} \label{prop:induced}
\SetFam[S] is the hierarchy corresponding to the tree induced by $S$,
i.e., $\SetFam[S] = \IndClust{\TreeI{S}}$.
\end{proposition}

In reasoning about trees, we use the following notation.
For an \emph{internal} vertex \Vertex,
let $\Subtree[L]{\Vertex}, \Subtree[R]{\Vertex}$
be the subtrees rooted at \Vertex's left and right children, and let
$\Subtree{\Vertex} = \Tree \setminus (\Subtree[L]{\Vertex} \cup \Subtree[R]{\Vertex})$
be the set of all other vertices (so $\Vertex \in \Subtree{\Vertex}$).
As is standard for \emph{rooted} trees, we consider
the \emph{degree} of a node to be its number of children,
i.e., we do not count its parent.

\subsection{Ordinal Queries} \label{sec:ordinal-def}

An \emph{ordinal} query is a set of three distinct \elements
$\Set{\ElS, \ElSP, \ElSPP} \subseteq \AllElements$.
The response to the query reveals which two \elements
are ``closest'' to each other.
We say that \ElS and \ElSP are closer to each other than to \ElSPP
(with respect to \Tree) 
if there exists a cluster (subtree rooted at an internal node)
in \Tree which contains \ElS and \ElSP, but not \ElSPP.
In other words, for every set of three \elements,
the two \elements which have the lowest common ancestor are closest.
Because \Tree is a binary tree,
such a cluster --- and hence a valid response --- always exists
and is unique.
% the query response for each triplet is unique.

\begin{proposition} \label{prop:tree-unique}
Two hierarchical clusterings \Tree, \TreeP of the same \elements \AllElements
satisfy $\Tree \equiv \TreeP$ if and only if
for every triplet $\Set{\ElS, \ElSP, \ElSPP}$,
the query responses with respect to \Tree and \TreeP are the same.
\end{proposition}

Thus, using at most $\binom{n}{3} = \Theta(n^3)$ ordinal queries,
an algorithm can uniquely recover the underlying hierarchy \TreeOpt.
(The actual algorithm is a simple bottom-up algorithm.)
For non-adaptive algorithms, the following easy theorem
(proved in Appendix~\ref{sec:non-adaptive-lower-bound})
gives a matching lower bound.

\begin{theorem} \label{thm:hardness-non-adaptive}
Every non-adaptive algorithm requires $\Omega(n^3)$ ordinal queries
to learn the hierarchical clustering over $n$ \elements
(even in the absence of noise).
\end{theorem}

Our goal is to reduce the number of queries to $O(n \log n)$
using adaptive ordinal queries.
The algorithm has access to an oracle (for example, a human user) that
will answer ordinal queries based on the (otherwise unknown)
ground truth hierarchy \TreeOpt.
For the next two sections, we assume that these answers are correct;
in Section~\ref{sec:insertion-noisy},
we turn our attention to the  \emph{independent noisy} model.
In this model, there is a known constant $p > \Half$
such that each query response is correct \emph{independently}
with probability $p$,
and adversarially incorrect\footnote{Of course, the adversary could
give a correct answer if an algorithm were to rely on it
always answering incorrectly.} with probability $1 - p$.
Because the query responses are independent,
if an algorithm queries the same triplet multiple times,
it may get different responses.

In several of our algorithms, we will try to locate where within an
existing (partial) hierarchy \Tree another \element \ElS
should be inserted.
In doing so, it is particularly useful to ``query'' an internal
vertex \Vertex, and learn whether \ElS should be in the left subtree
of \Vertex, the right subtree of \Vertex, or neither.
This can be accomplished as follows.
Let $\ElSL \in \Subtree[L]{\Vertex}, \ElSR \in \Subtree[R]{\Vertex}$
be arbitrary \elements in the left and right subtrees of \Vertex.
(\ElSL, \ElSR exist because \Vertex is an internal vertex,
and thus has exactly two children.)
We call the query $\Set{\ElSL, \ElSR, \ElS}$ an 
\emph{ordinal query for \ElS with pivot \Vertex}.
Its possible outcomes can be interpreted as follows.

\begin{proposition} \label{prop:pivot-query}
If the response to \Set{\ElSL, \ElSR, \ElS} is that
\ElSL and \ElS are closest,
then \ElS's correct location is in \Subtree[L]{\Vertex}
(the left subtree of \Vertex).
If the response to $\Set{\ElSL, \ElSR, \ElS}$ is that
\ElSR and \ElS are closest,
then \ElS's correct location is in \Subtree[R]{\Vertex}.
If \ElSR and \ElSL are closest, then \ElS's correct location is not
in the subtree rooted at \Vertex, i.e., \ElS belongs in \Subtree{\Vertex}.
\end{proposition}

Ordinal queries with pivot \Vertex are only defined when \Vertex is an
internal node, and we will only use them in that case.
When notationally convenient, we will consider the left/right child
of \Vertex itself as the response to an ordinal query with pivot \Vertex.

\subsection{Vertex Queries} \label{sec:vertex-queries}

Ordinal queries with pivot \Vertex provide ``directional'' information
regarding the location of \ElS with respect to \Vertex.
As such, they are similar to (though subtly different from)
\emph{vertex queries} defined in past work
\cite{onak-parys:2006:tree-vertex-linear,2016:binary-search}.
In the vertex query model, the goal is to identify an a priori unknown
\emph{target node} \Target in a known
(undirected, not necessarily rooted) tree \Tree,
by repeatedly querying nodes \Vertex of \Tree.
In response to a vertex query, the algorithm is told that
$\Target = \Vertex$, or otherwise is given the unique neighbor that is
closer to the target.

Vertex queries are subtly more powerful than ordinal queries with a pivot,
because they can identify \Vertex as the target.
We will show later how to simulate (for our purposes)
vertex queries with a constant number of ordinal queries with pivots.
In fact, our analysis of the algorithm inspired by \InsertionSort is
based on ideas from prior work in the vertex query model,
and the analysis for the independent noisy model explicitly uses a
reduction from the ordinal query model to the vertex query model.

