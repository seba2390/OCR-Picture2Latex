%% Introduction

\section{Introduction} \label{sec:introduction}

One of the most useful and frequent computational tasks
in interacting with a set of \elements is to \emph{cluster} them,
inferring groups (\emph{clusters}) of \elements
that are ``similar to each other.''
Clustering is often an essential step in ``learning'' facts
about the items, inferring generative models,
deriving scientific or other insights,
or letting users interact with large data sets meaningfully.
The wide range of applications has led to an enormous amount of research
spanning many disciplines, driven by different applications,
different notions of ``similarity,'' different constraints
on allowable groups of \elements,
and different models of user interaction,
to name just a few.

Two particularly common restrictions
on the allowable groups are \emph{flat clustering},
in which each item belongs to exactly (or at most) one cluster,
and \emph{hierarchical clustering},
in which the groups form a nested structure.
Both restrictions have myriad natural applications.
Among the prototypical applications of hierarchical clusterings 
are hierarchies of animal or plant species
\cite{felsenstein:2004:phylogenies},
ontologies of words or other entities,
or social networks, viewed at different levels of granularity.
Hierarchical clusterings are particularly well suited
for human data exploration, in that they allow a person
to zoom in/out to understand the \elements and their relationships
at different levels. In this paper, we are interested in
inferring a hierarchical clustering
on a set \AllElements of $n$ \elements.

A second key question is how the similarities between \elements
are defined and obtained.
In much of the work on clustering, the similarities are derived
from observable features of the \elements
(such as their position in an observable metric space),
and finding a clustering is cast as
an optimization problem for a suitably chosen objective function.
Such an approach frequently identifies clusters quite similar
--- but not identical --- to the ground truth.
An alternative approach is to interact with human users,
and obtain input to guide the search for the true clustering.
These interactions are typically expensive;
thus, minimizing the number of interactions with human users
is often a primary goal in designing algorithms. 
When interacting with humans, it is also important to keep in mind
that even when there is an objective meaning to numerical similarity values,
humans are notoriously bad at estimating such numerical quantities.
This has led to a search for suitable models of interaction,
advocated recently in
\cite{balcan-blum:2008:split-merge,%
awasthi-zadeh:2010:supervised-clustering,%
awasthi-balcan-voevodski:2017:local-algorithm-journal,%
vikram-dasgupta:2016:interactive-hierarchical-clustering},
in which human users are only given queries
they may reasonably be in a position to answer.

Inspired by the equivalence query model for learning a classifier
\cite{angluin:1988:queries-concept}, 
Balcan and Blum~\cite{balcan-blum:2008:split-merge}
and Awasthi et al.~\cite{awasthi-zadeh:2010:supervised-clustering,%
awasthi-balcan-voevodski:2017:local-algorithm-journal}
defined a model of interaction (specifically for flat clustering)
in which an algorithm repeatedly proposes a clustering,
and a user can request corrections, such as splitting or merging clusters.
In this model, the user has a lot of leeway in determining
what feedback the algorithm receives.
The \emph{ordinal query} model
(\cite{%aho-sagiv-szymanski-ullman:1981:inferring-tree,%
schultz-joachims:2003:relative-comparisons,%
jain-jamieson-nowak:2016:ordinal-embedding,%
vikram-dasgupta:2016:interactive-hierarchical-clustering,%
kendall-gibbons:1990:rank-correlation}, for example)
is more reminiscent of traditional ``active learning'' models.
Here, a user is given queries comprising triplets of \elements,
and asked to identify the pair from each triplet that is closest.
For example, a user may be asked which pair out of
\{lion, dog, shark\} is most similar.

One well-known application of this model
is to discover the phylogeny trees of a set of species.
A phylogeny tree over a set of species
is a rooted binary tree whose leaves correspond to the species,
while internal nodes capture the evolutionary history.
It is often assumed that there is a black box
which can answer ordinal queries on a set of three DNA sequences
and infer which two are biologically closer to each other;
under this assumption,
the problem of finding the underlying phylogeny
is similar to the problem we study in this paper~%
\cite{kannan-lawler-warnow:1996:phylogeny-triplet,%
brown-truszkowski:2011:phylogeny-quartet,%
brown-truszkowski:2011:phylogeny-quartet-practical}.
Other topics such as social computing and a computational lens
on political and other decision making processes have led to
increasing interest in ordinal query responses as well.
As a result, ordinal queries have
been analyzed in the contexts of clustering or inferring distances
\cite{%aho-sagiv-szymanski-ullman:1981:inferring-tree,%
schultz-joachims:2003:relative-comparisons,%
agarwal-wills-cayton-lanckriet-kriegman-belongie:2007:embedding,%
tamuz-liu-belongie-shamir-kalai:2011:learning-kernel,%
jain-jamieson-nowak:2016:ordinal-embedding,%
vikram-dasgupta:2016:interactive-hierarchical-clustering},
in social choice scenarios such as voting
\cite{anshelevich-bhardwaj-postl:2015:social-choice,%
anshelevich:2016:ordinal},
causal inference~\cite{pearl-tarsi:1986:strucuting},
and political and group decision making
\cite{goel-lee:2012:triadic,%
goel-lee:2014:small-group-interaction}.
\emph{Learning of a hierarchical clustering
from ordinal queries is the topic of the present work.}

We assume that there is
an unknown ground truth hierarchical clustering to be learned.
It is represented by an (unknown) tree \TreeOpt
whose $n$ leaves are the \elements.
For each node \Vertex of the tree,
the leaves in the subtree rooted at \Vertex
form a cluster in the hierarchical clustering.
For most of the paper
(except Section~\ref{sec:generalized-hierarchical-clustering}),
we assume that \TreeOpt is binary.
This implies that for each triplet \Set{\ElS, \ElSP, \ElSPP}
of leaves/\elements,
there is a unique pair that shares a closer common ancestor
than any other pair:\footnote{This does not hold in trees of higher degree.}
this pair is the correct \emph{response} to the query \Set{\ElS, \ElSP, \ElSPP}.
The goal is to learn \TreeOpt (or an isomorphic tree)
using few ordinal queries.

For some of this paper,
we assume that the answers to all ordinal queries are correct.
However, in particular in light of the motivation
of interaction with humans,
it is natural to assume that responses may be noisy.
In the \emph{independent noise mode}
(which we use in Section~\ref{sec:insertion-noisy}),
each response is correct independently with
probability $p > \Half$, and adversarially incorrect otherwise.

It is not difficult to show (see Section~\ref{sec:ordinal-def})
that even without incorrect responses,
when the ordinal queries are non-adaptive
(i.e., have to be chosen ahead of time),
$\Theta(n^3)$ queries are necessary and sufficient.
We therefore focus on adaptive queries.

For adaptive queries, when \TreeOpt has height $O(\log n)$,
there is a simple top-down algorithm to learn \TreeOpt
using $O(n \log n)$ adaptive ordinal queries
(with no incorrect responses).
This algorithm can be considered as a special case of an
algorithm of Pearl and Tarsi~\cite{pearl-tarsi:1986:strucuting}.
Their algorithm was designed for a different context, namely,
discovering underlying causal trees over visible random variables.
However, as a preliminary step, they studied a model essentially
identical to the ordinal query model.
Their algorithm learns the underlying tree (of any height)
using $O(n \log n)$ queries.
In Section~\ref{sec:insertion-clean}, we will review their algorithm
as a point of departure for our noise-tolerant algorithm in
Section~\ref{sec:insertion-noisy}.
A matching lower bound of $\Omega(n \log n)$
follows easily from a counting argument
like the one for sorting (see Section~\ref{sec:quick-sort}).

\subsection*{Our Results and Techniques}

The analogy with sorting is also useful in the design of algorithms
for inferring a hierarchy.
We begin (in Sections~\ref{sec:quick-sort} and \ref{sec:insertion-clean})
by studying the model without incorrect responses.

Our first (and simplest) algorithm (in Section~\ref{sec:quick-sort})
is modeled after randomized \QuickSort.
It picks \emph{two} random pivot \elements \PivotA, \PivotB,
and partitions the \elements into those closest to \PivotA,
those closest to \PivotB,
and those which are further from both pivots
than the two pivots are from each other.
We prove that with constant probability,
each of the three sets constitutes
only a constant fraction of the original set.
Then, we present an algorithm that, 
once the hierarchies for all three sets have been (recursively) computed,
can merge them using a linear number of queries.
As a result, the algorithm uses $O(n \log n)$ ordinal queries.

In preparation for the noise-tolerant algorithm in
Section~\ref{sec:insertion-noisy},
in Section~\ref{sec:insertion-clean},
we review\footnote{We would like to thank Daniel Hsu for
pointing us to the article \cite{pearl-tarsi:1986:strucuting}.}
the algorithm of Pearl and Tarsi
\cite{pearl-tarsi:1986:strucuting},
which can be considered as a variant of \InsertionSort. 
% and to improve the constant in the number of queries,
The main observation
guiding the algorithm in Section~\ref{sec:insertion-clean}
is that, given the true hierarchy over $i$ \elements,
even when the tree is arbitrarily unbalanced, 
a new \element can be inserted using at most
$1 + \log_2 i$ ordinal queries,
giving an overall query complexity of $n \log_2 n$.
The algorithm to insert an \element is a close variant
of the generalization of \binarysearch to trees or arbitrary graphs
\cite{onak-parys:2006:tree-vertex-linear,%
mozes-onak-weimann:2008:tree-edge-linear,%
2016:binary-search}.
While the work on \binarysearch in trees/graphs
assumes a different query model called \emph{vertex queries}
(see Section~\ref{sec:vertex-queries}),
the algorithm is easily adapted.

Next, we turn our attention to the noisy model.
It is easy to see that any algorithm
that learns the true hierarchy in the absence of noise
can be made robust to noise by repeating each ordinal query
$\Theta(\log n)$ times and using a majority output.
Then, Hoeffding's inequality and union bounds guarantee that
the algorithm succeeds with high probability.
However, this approach increases the
number of queries by a factor of $\Theta(\log n)$.
In Section~\ref{sec:insertion-noisy},
we show how to avoid the worse dependence on $n$
for the \InsertionSort-based algorithm,
and only increase the number of queries by a constant factor
depending solely on the correctness probability $p$.

The robustness is achieved by replacing
the \binarysearch subroutine with a robust version.
While robust \binarysearch algorithms
in a vertex query model in the presence of noise
had been given in work of
Feige et al.~\cite{feige-raghavan-peleg-upfal:1994:noisy}
and Emamjomeh-Zadeh et al.~\cite{2016:binary-search},
neither algorithm itself has strong enough guarantees for our purposes:
the algorithm of \cite{feige-raghavan-peleg-upfal:1994:noisy} 
requires a number of queries linear in the diameter of the tree,
while the algorithm of \cite{2016:binary-search} has a dependency on
the desired error guarantee that would lead to a query complexity of
$\Theta(\log ^2 n)$ in our context.
Our main contribution in Section~\ref{sec:insertion-noisy}
is a hybrid algorithm combining the first stage of the algorithm
from \cite{2016:binary-search} with the algorithm of
\cite{feige-raghavan-peleg-upfal:1994:noisy} to obtain a number of
queries that is logarithmic in $n$.
Specifically, we show how, given the tree over a subset of \elements,
to correctly insert a new \element into the tree
with probability $1 - \Err$, using $O(\log n + \log (1/\Err))$ queries.
This leads to an algorithm that learns the correct hierarchy \TreeOpt
with probability at least $1-\Err$ using
$O(n (\log n + \log(1/\Err)))$ ordinal queries.

\subsection*{Related Work}

The algorithmic problem of inferring the evolutionary history
(known as \emph{phylogeny}) of a set of animal or plant species
(see, e.g., \cite{aho-sagiv-szymanski-ullman:1981:inferring-tree,%
kannan-lawler-warnow:1996:phylogeny-triplet,%
erdos-steel-szekely-warnow:1999:phylogeny-quartet-a,%
brodal-fagerberg-pedersen-ostlin:2001:phylogeny-higher-degree,%
felsenstein:2004:phylogenies,%
gronau-moran-snir:2008:phylogeny-short-edges,%
wu-kao-lin-you:2008:pylogeny-noisy,%
truszkowski:2013:thesis})
is closely related to the problem of learning an underlying hierarchy
over a set of elements.
A (rooted) phylogeny over $n$ species is defined as a rooted binary tree
with $n$ leaves corresponding to the species.
The internal nodes represent
past extinct ancestors or hypothesized ones.
Given (the DNA sequence of) three species,
an ``experiment'' determines which two
are (biologically) closer to each other than to the third one~%
\cite{kannan-lawler-warnow:1996:phylogeny-triplet,%
wu-kao-lin-you:2008:pylogeny-noisy}.
Hence, discovering the phylogeny using a minimum number of experiments
is equivalent to the problem we study here.
Alternatively, the phylogeny of $n$ species
is sometimes defined as an unrooted tree with $n$ leaves
in which each internal node has degree exactly $3$.
In this model, queries are in the form of \emph{quartet} queries:
each query consists of $4$ species,
and the response to it indicates which pair can be separated
from the other pair by removing one edge in the underlying phylogeny.
Although there may not be a direct (algorithmic) reduction
between the two models, most ideas and algorithms that are proposed
in the literature for one of the models
directly carry over for the other one as well.

Assuming that all the query responses are correct,
Kannan, Lawler and Warnow~\cite{kannan-lawler-warnow:1996:phylogeny-triplet}
discuss the trade-off between the query complexity of the algorithm
and its running time. 
All three algorithms they present
achieve an $O(n \log n)$ upper bound on the number of queries;
they show that by increasing the query complexity
by a constant factor, one can achieve faster algorithms in return.
Their algorithms are similar to
the algorithm we present in Section~\ref{sec:insertion-clean}.
In this paper, we only focus on the query complexity
and do not analyze the running time of our algorithms
(except for noting that they are obviously polynomial-time).

The algorithms which are used in computational biology
to answer (triplet or quartet) queries
are not fully reliable;
hence, it is necessary to account for noise in the 
query responses.
Wu et al.~\cite{wu-kao-lin-you:2008:pylogeny-noisy}
studied the problem of recovering an underlying unrooted phylogeny
using quartet queries under an independent noise model.
They presented an algorithm that achieves an $O(n^3 \log n)$ upper
bound on the number of quartet queries.

Brown and Truszkowski%
~\cite{brown-truszkowski:2011:phylogeny-quartet,%
brown-truszkowski:2011:phylogeny-quartet-practical}
improved this result by designing a randomized algorithm
with a query complexity of $O(n \log n)$,
both in expectation and with high probability.
The running time of their algorithm
is also $O(n \log n)$ in expectation and with high probability.
Their algorithm is very similar%
\footnote{The original version of our paper was submitted and accepted
before we learned about
\cite{brown-truszkowski:2011:phylogeny-quartet} and
\cite{brown-truszkowski:2011:phylogeny-quartet-practical}.
We thank the authors for informing us about these publications.}
to our algorithm in Section~\ref{sec:insertion-noisy}:
They insert the elements one by one into an existing tree,
using an idea inspired by \cite{feige-raghavan-peleg-upfal:1994:noisy},
and guarantee (as we do) that each insertion is correct
with high enough probability.
They prove that if the elements are
randomly permuted at the beginning of the algorithm
(i.e., the order in which they are inserted is uniformly random),
then each insertion takes $O(\log n)$ queries
both in expectation and with high probability.
Our algorithm, in contrast, is deterministic
and ensures that each insertion uses
no more than $O(\log n)$ queries.
\cite{brown-truszkowski:2011:phylogeny-quartet}
assume that noise is \emph{permanent},
which means that if the algorithm asks the same query multiple times,
the responses are the same.
This assumption is necessary for their application,
because query responses are obtained
based on fixed DNA sequences,
using fixed algorithms in computational biology.
The algorithm of \cite{brown-truszkowski:2011:phylogeny-quartet}
is designed to recover the exact phylogeny
without ever querying the same quartet more than once.
In our model, on the other hand, each query response is incorrect
with probability $1 - p$, independently of all previous queries
(even if the same query has been asked before).
When queries are triplet (rather than quartet) queries,
if the noise is permanent, it is impossible
to recover the exact structure of the underlying rooted phylogeny
(or hierarchical clustering) with high probability.
As pointed out by~%
\cite{brown-truszkowski:2011:phylogeny-quartet,%
brown-truszkowski:2011:phylogeny-quartet-practical},
the independent noise model is not, in general,
a natural model for inferring phylogenetic trees.
The reason is that query responses are based on the DNA
sequences of the species, and hence, they are not independent.

Vikram and Dasgupta~\cite{vikram-dasgupta:2016:interactive-hierarchical-clustering}
addressed the problem of incorporating hard ordinal constraints
into algorithms for hierarchical clusterings.
However, contrary to our model, they assume that
the algorithm is provided information regarding the geometry of the data,
and the hard constraints given by the user are
additional information to improve the accuracy of the outcome.

A hierarchical clustering over $n$ \elements
can be seen as a metric (or in fact, ultra-metric)
over the \elements.\footnote{For instance,
let the distance between every pair of \elements
be the size of the smallest cluster in the hierarchy
containing both of them.}
In this sense, learning a hierarchical clustering is related
to a recent line of work (mostly in the machine learning community)
\cite{agarwal-wills-cayton-lanckriet-kriegman-belongie:2007:embedding,%
jamieson-nowak:2011:embedding,%
jamieson-nowak:2011:active-ranking,%
McFee-lanckriet:2011:multi-model-similarity,%
tamuz-liu-belongie-shamir-kalai:2011:learning-kernel,%
VanDerMaaten-weinberger:2012:stochastic-triplet-embedding,%
kleindessner-vonLuxburg:2014:embedding-uniqueness,%
jain-jamieson-nowak:2016:ordinal-embedding}
on constructing a geometric interpretation of data
based on ordinal information.
In particular, the ``non-metric multidimensional scaling'' problem
\cite{agarwal-wills-cayton-lanckriet-kriegman-belongie:2007:embedding,%
jamieson-nowak:2011:embedding,%
kleindessner-vonLuxburg:2014:embedding-uniqueness,%
jain-jamieson-nowak:2016:ordinal-embedding} is formulated as
finding an embedding of the \elements
into a (low-dimensional) Euclidean space,
such that the distances between the points
satisfy a given set of ordinal constraints.
Many of the algorithms on the non-metric multidimensional scaling
problem are in the non-adaptive model (e.g.,
\cite{agarwal-wills-cayton-lanckriet-kriegman-belongie:2007:embedding,%
jain-jamieson-nowak:2016:ordinal-embedding}).
Jamieson and Nowak~\cite{jamieson-nowak:2011:embedding}
point out that an adaptive algorithm may be able to
find an embedding of the \elements into the low-dimensional space
using only a few ordinal queries,
in a way that the embedding satisfies
all the $\Theta(n^3)$ possible queries.
They proved a lower bound of $\Omega(\Dimension n \log n)$
where \Dimension is the dimension,
and they conjectured that this lower bound is tight.

In the context of the \emph{multinomial logit model},
a finite set of choices \CCC satisfies the
``Independence of Irrelevant Alternatives'' (IIA) property%
~\cite{McFadden-tye-train:1977:multinomial-logit}
if for every two choices $c_1, c_2 \in \CCC$,
the ratio of the probabilities of being chosen
is a constant, regardless of which other choices (from \CCC)
are available or unavailable. That is,
for every two subsets of choices $\CCC', \CCC'' \subseteq \CCC$
with $\Set{c_1, c_2} \subseteq \CCC', \CCC''$,
\begin{align*}
\frac{\Pr[c_1 \text{ is chosen } | \; \CCC']}
{\Pr[c_2 \text{ is chosen } | \; \CCC']} =
\frac{\Pr[c_1 \text{ is chosen } | \; \CCC'']}
{\Pr[c_2 \text{ is chosen } | \; \CCC'']}.
\end{align*}
In many applications where this strong assumption is not met,
the \emph{nested logistic regression} model%
~\cite{benson-kumar-tomkins:2016:nested-IIA} is used.
In the nested logistic regression model,
the choices are the leaves of an underlying (usually unknown) rooted tree
(which need not be binary).
A pair of choices $c_1, c_2$ is independent
of a third choice $c_3$
if there is a (rooted) subtree containing $c_1$ and $c_2$, but not $c_3$.
Using the terminology of this paper,
the pair $c_1, c_2$ is independent of $c_3$ if
$c_1$ and $c_2$ are closer to each other than to $c_3$.

In a recent paper,
Benson et al.~\cite{benson-kumar-tomkins:2016:nested-IIA}
considered the problem of algorithmically recovering
the underlying tree structure from an observed set of choices.
In particular, they showed that using $O(n^2)$ adaptive ordinal queries,
one can recover the tree.
In fact, their algorithm is a deterministic version
of our \QuickSort-based algorithm from Section~\ref{sec:quick-sort}.
As we discuss in Section~\ref{sec:generalized-hierarchical-clustering},
for trees of very high degree,
the performance of the generalization of our algorithm
deteriorates to $\Theta(n^2)$,
matching the guarantee of
\cite{benson-kumar-tomkins:2016:nested-IIA}.
However, for low-degree trees, and in particular,
for binary trees, our algorithm is provably more efficient.

Yet another application of the algorithmic problem
is \emph{inferring underlying causal structures}.
Pearl and Tarsi~\cite{pearl-tarsi:1986:strucuting}
introduced a tree-like model
where the leaves are observable random variables
and the internal nodes are hidden causes.
They considered a statistical test over the observable variables
(i.e., leaves) which is equivalent to the notation of ordinal query
we discuss in this article.
They showed that using $O(n \log n)$ statistical tests,
one can discover the underlying causal tree
if the degrees are bounded by a constant.
We discuss their algorithm in Section~\ref{sec:insertion-clean}
as the foundation of our main result in Section~\ref{sec:insertion-noisy}.

Feige et al.~\cite{feige-raghavan-peleg-upfal:1994:noisy}
studied several problems under the independent noise model.
In particular, they showed that for the classical \binarysearch problem,
if comparisons are noisy
(i.e., each comparison is correct independently with probability $p > \Half$
and flipped otherwise),
there exists an algorithm that uses $O(\log n)$ queries
and succeeds with high probability.
Ben-Or and Hassidim~\cite{BenOr-hassidim:2008:noisy-binary-search}
improved this result and obtained
the information-theoretically optimal query complexity. 
Braverman and Mossel~\cite{braverman-mossel:2008:noisy-sorting}
studied the sorting problem under an independent noise model
in a non-adaptive setting.
They proposed an algorithm that,
given the noisy comparisons for every pair of \elements,
finds the maximum-likelihood permutation.
They also showed that the maximum-likelihood permutation
is ``close'' to the ground truth.

