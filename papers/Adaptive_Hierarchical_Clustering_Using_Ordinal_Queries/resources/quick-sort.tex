%% A \QuickSort-like Randomized Algorithm

\section{A \QuickSort-like Randomized Algorithm}
\label{sec:quick-sort}

In this section, we propose a simple randomized algorithm
(Algorithm~\ref{alg:quick-sort}),
reminiscent of \QuickSort.
The algorithm uses $O(n \log n)$ ordinal queries in expectation
to learn the hierarchy over $n$ \elements.
Algorithm~\ref{alg:quick-sort} 
randomly partitions the \elements into three \partitions
\PartitionA, \PartitionB, and \PartitionC
(lines~\ref{line:partitioning-begin}--\ref{line:partitioning-end}),
until all of the partitions are small enough.
Once they are, the algorithm recursively determines the hierarchy for
each partition, and finally merges them.

\InsertAlgorithmInOneColumn{\QuickClustering$(\ElSet)$}{alg:quick-sort}{%
\IF{$\SetCard{\ElSet} \leq 2$}
	\RETURN{the obvious hierarchy.}
\ENDIF
\REPEAT
	\STATE{Let $(\PivotA, \PivotB)$
	be a pair of distinct \elements chosen uniformly at random.}
	\label{line:partitioning-begin}
	\FOR{each $\ElS \in \ElSet \setminus \Set{\PivotA, \PivotB}$}
		\STATE{Make an ordinal query of \Set{\PivotA, \PivotB, \ElS}.}
	\ENDFOR
	\STATE{Let $\PartitionA \AlgAssign \Set{\PivotA} \cup
	\SpecSet{\ElS \in \ElSet}{\text{\ElS and \PivotA are closer to each other
	than to 	\PivotB}}$.}
	\STATE{Let $\PartitionB \AlgAssign \Set{\PivotB} \cup
	\SpecSet{\ElS \in \ElSet}{\text{\ElS and \PivotB are closer to each other
	than to \PivotA}}$.}
	\STATE{Let $\PartitionC \AlgAssign 
  	\SpecSet{\ElS \in \ElSet}{\text{\PivotA and \PivotB are closer to each other
  	than to \ElS}}$.}
\UNTIL{$\max(\SetCard{\PartitionA}, \SetCard{\PartitionB}, \SetCard{\PartitionC})
\leq \frac{15}{16} \cdot \SetCard{\ElSet}$.}
\label{line:partitioning-end}
% partitioning ends
\STATE{$\TreeA \AlgAssign \QuickClustering(\PartitionA)$.}
\STATE{$\TreeB \AlgAssign \QuickClustering(\PartitionB)$.}
\STATE{$\TreeC \AlgAssign \QuickClustering(\PartitionC)$.}
\RETURN{\Merge $(\TreeA, \TreeB, \TreeC)$.}
}

The partitioning is akin to the partitioning stage in \QuickSort: 
it draws a pair of distinct \emph{pivot \elements}
$\PivotA, \PivotB \in \ElSet$, uniformly at random.
It then partitions the \elements of \ElSet into three sets.
These sets can be characterized as follows:
Let \VertexBar be the lowest common ancestor of \PivotA, \PivotB
in the ground truth hierarchy \TreeOpt,
with children \Vertex[A], \Vertex[B].
Then, \PartitionA (\PartitionB) consists of all \elements in the
subtree of \TreeOpt rooted at \Vertex[A] (\Vertex[B]),
while \PartitionC consists of all remaining \elements:
exactly the \elements outside of the subtree of \TreeOpt rooted at \VertexBar.
Therefore, after the recursive calls,
\TreeA and \TreeB are the subtrees of \TreeOpt rooted at \Vertex[A]
and \Vertex[B], while \TreeC is the result of removing \VertexBar
and its subtree from \TreeOpt (and contracting its parent).

The role of the \Merge function (Algorithm~\ref{alg:merge}) is then to
insert \VertexBar and its subtree in the correct place in \TreeC;
the primary task here is to correctly identify the sibling \Vertex of \VertexBar.
The algorithm does this by starting at the root and repeatedly using
ordinal queries for a representative \element \ElS of
$\TreeA \cup \TreeB$ with \Vertex as a pivot,
and following the direction returned by the ordinal query.
That inserting \VertexBar as a sibling of \Vertex
(with a new parent \VertexP) is correct follows from
Proposition~\ref{prop:induced}.

\InsertAlgorithmInOneColumn{\Merge $(\TreeA, \TreeB, \TreeC)$}{alg:merge}{%
\STATE{Add a vertex \VertexBar whose children are the roots of \TreeA and \TreeB.}
\STATE{Let \ElS be an arbitrary \element (i.e., leaf) in \TreeA or \TreeB.}
\STATE{Let \Vertex be the root of \TreeC.}
\WHILE{\Vertex is not a leaf, and the ordinal query for \ElS
with pivot \Vertex does not return \Subtree{\Vertex}}
	\STATE{Update \Vertex to the child of \Vertex
returned by that ordinal query for \ElS}
\ENDWHILE
\STATE{Insert a new vertex \VertexP as parent of \Vertex, and make
\VertexBar its other child.}
}

We next analyze the expected number of ordinal queries required
by the \QuickClustering algorithm.
The number of queries required for \Merge is at most\footnote{%
In fact, using the techniques from Section~\ref{sec:insertion-clean},
the bound could be improved to $1 + \log_2 \SetCard{\ElSet}$;
but this improvement is drowned out by the number of queries
for the partitioning.}
$\SetCard{\ElSet}$,
and the number of queries for each iteration of the partitioning loop
(lines \ref{line:partitioning-begin}--\ref{line:partitioning-end}
in Algorithm~\ref{alg:quick-sort})
is $\SetCard{\ElSet} - 2$.
The key observation is that in expectation,
the loop is executed only a constant number of times
before succeeding, captured by Lemma~\ref{lem:separator}.

\begin{lemma} \label{lem:separator}
The probability that in line~\ref{line:partitioning-end}
of Algorithm~\ref{alg:quick-sort},
at least one $\PartitionA, \PartitionB, \PartitionC$
is larger than $\frac{15}{16} \cdot \SetCard{\ElSet}$
is at most $\frac{125}{128}$.
\end{lemma}

\begin{proof}
We first prove that there exist clusters
\Cluster and \ClusterP with
$\ClusterP \subseteq \Cluster$ in \IndClust{\Tree}
such that $\frac{n}{16} < \SetCard{\ClusterP} \leq \frac{n}{8}$
and $\frac{n}{4} < \SetCard{\Cluster} \leq \frac{n}{2}$.
This follows because \Tree is binary:
starting from $\Cluster = \AllElements$,
we can repeatedly consider a bipartition of
\Cluster into two disjoint subclusters.
While \Cluster has more than $\frac{n}{2}$ \elements,
we can replace it by the larger of the two subclusters,
which must have at least half the size of \Cluster.
We end up with a set \Cluster of the required size.
Continuing from $\ClusterP = \Cluster$ in the same way until
\ClusterP has size at most $\frac{n}{8}$ gives us the claimed sets.

The probability that one of $\PivotA, \PivotB$ is in \ClusterP and the
other in $\Cluster \setminus \ClusterP$ is at least
\[
\frac{\SetCard{\ClusterP} \cdot \SetCard{\Cluster \setminus \ClusterP}}{\binom{n}{2}}
\; > \; 2 \cdot \frac{1}{16} \cdot \frac{3}{16}
\; = \; \frac{3}{128}.
\]

When this event happens, we assume w.l.o.g.~that
$\PivotA \in \ClusterP$ and
$\PivotB \in \Cluster \setminus \ClusterP$.
Then, $\ClusterP \subseteq \PartitionA \subseteq \Cluster$,
and therefore
$\frac{n}{16} \leq \SetCard{\PartitionA} \leq \frac{n}{2} \leq \frac{15n}{16}$.
Because $\SetCard{\PartitionA} \geq \frac{n}{16}$,
we also get that
$\SetCard{\PartitionB}, \SetCard{\PartitionC} \leq \frac{15n}{16}$, 
completing the proof.
\end{proof}

By Lemma~\ref{lem:separator} and the preceding discussion,
the total number of ordinal queries
for both partitioning and merging
is $O(\SetCard{\ElSet})$.
Letting \QC{n} denote the expected number of comparisons for
\QuickClustering on $n$ \elements,
we obtain the recurrence
%\begin{align*}
$\QC{n}
\; \leq \; \QC{\SetCard{\PartitionA}}
         + \QC{\SetCard{\PartitionB}}
         + \QC{\SetCard{\PartitionC}}
         + O(n)$.
%\end{align*}
Because
$\SetCard{\PartitionA} + \SetCard{\PartitionB} + \SetCard{\PartitionC} = n$
and
$\max (\SetCard{\PartitionA}, \SetCard{\PartitionB}, \SetCard{\PartitionC})
\leq \frac{15}{16} \cdot n$,
the solution of this recurrence is $\QC{n} = O(n \log n)$.
Hence, we have proved the following theorem.

\begin{theorem} \label{thm:quickclustering}
Algorithm~\ref{alg:quick-sort} learns the hierarchy
using $O(n \log n)$ ordinal queries in expectation.
\end{theorem}

The query complexity of 
Algorithm~\ref{alg:quick-sort} is asymptotically optimal.
Because the hierarchical clusterings include all permutations
of \AllElements,
there are at least $2^{\Omega(n \log n)}$ different hierarchical
clusterings of $n$ \elements (see also
\cite{vikram-dasgupta:2016:interactive-hierarchical-clustering}).
More precisely, as pointed out in \cite{felsenstein:2004:phylogenies},
there are $(2n-3) \cdot (2n-5)!! = \frac{(2n-3)!}{(n-2)! \cdot 2^{n-2}}$
hierarchical clusterings.
By Stirling's approximation,
at least $n \log_2 n - O(n)$ bits of information
are necessary to uniquely identify \TreeOpt.
Since each query response reveals at most $\log_2 (3)$ bits of
information, the number of queries required is at least
$n \log_3 n - O(n)$.

% \InsertAlgorithm{\Partition$(\ElS[1], \ldots, \ElS[n])$}%
% {alg:partition}{
% \STATE{$(\PivotA, \PivotB) \AlgAssign
% \text{two distinc \elements chosen uniformly at random}.$}
% \label{line:partitioning-begin}
% \STATE{$\PartitionA \AlgAssign \Set{\PivotA},
% \PartitionB \AlgAssign \Set{\PivotB},
% \PartitionC \AlgAssign \emptyset$.}
% \FOR{each $\ElS \in \ElSet \setminus \Set{\PivotA, \PivotB}$}
% %\STATE{$\Response \AlgAssign
% %\text{response to query of }\Set{\PivotA, \PivotB, \ElS}$.}
% \STATE{Query \Set{\PivotA, \PivotB, \ElS}.}
% \IF{\ElS and \PivotA are closer to each other than to \PivotB}
% \STATE{$\PartitionA \AlgAssign \PartitionA \cup \Set{\ElS}$.}
% \ENDIF
% \IF{\ElS and \PivotB are closer to each other than to \PivotA}
% \STATE{$\PartitionB \AlgAssign \PartitionB \cup \Set{\ElS}$.}
% \ENDIF
% \IF{\PivotA and \PivotB are closer to each other than to \ElS}
% \STATE{$\PartitionC \AlgAssign \PartitionC \cup \Set{\ElS}$.}
% \ENDIF
% \ENDFOR
% \RETURN{$(\PartitionA, \PartitionB, \PartitionC)$.}
% }

