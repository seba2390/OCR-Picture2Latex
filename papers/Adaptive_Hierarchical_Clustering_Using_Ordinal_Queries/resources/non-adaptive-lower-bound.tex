%% Proof of Theorem~\ref{thm:hardness-non-adaptive}

\section{Proof of Theorem~\ref{thm:hardness-non-adaptive}}
\label{sec:non-adaptive-lower-bound}

Here, we prove Theorem~\ref{thm:hardness-non-adaptive},
restated for convenience.

\begin{rtheorem}{Theorem}{\ref{thm:hardness-non-adaptive}}
Any non-adaptive algorithm requires $\Omega(n^3)$ ordinal queries
to learn the hierarchical clustering over $n$ \elements
(even in the absence of noise).
\end{rtheorem}

\begin{proof}
A (possibly randomized) non-adaptive algorithm poses \NumQuery queries
and receives their responses all at once.
Let $n$ be a power of $2$ and generate a ground truth
hierarchical clustering \TreeOpt as a full binary tree
in which the leaves form
a uniformly random permutation of the \elements. 
For every constant $\Err > 0$,
we prove that if the algorithm uses fewer than
$n (n - 1)(n - 2) (1 - \Err) / 24 = \Omega(n^3)$ ordinal queries,
then it fails to uniquely identify the clustering
with probability at least $1 - \Err$.

%% figure
\begin{figure}[htb]
\begin{center}
\includegraphics{figures/complete-perfect-four.pdf}
\end{center}
\caption{A full binary tree and one of its clusters of size 4. \label{fig:complete-perfect-four}}
\end{figure}
%% end of figure

Because \TreeOpt is a full binary tree,
there are exactly $n/4$ disjoint clusters of size 4 each
(Figure~\ref{fig:complete-perfect-four}).
Let \Cluster be one of them.
The 4 \elements in \Cluster are partitioned into two clusters
of size 2 each.
The algorithm can learn this internal structure of \Cluster
only if one of its queries consists of 3 (out of the 4) \elements in \Cluster.
Because the \elements are uniformly randomly shuffled,
the probability that the 3 \elements of any one query all lie in
\Cluster is $\frac{24}{n (n - 1)(n - 2)}$.

Because there are $n/4$ such clusters \Cluster, 
if the algorithm queries \NumQuery triplets,
in expectation, it learns the internal structure of at most
$\NumQuery \cdot \frac{n}{4} \cdot \frac{24}{n (n - 1)(n - 2)}
= \frac{6\NumQuery}{(n-1)(n-2)}$
clusters of size 4.
If the algorithm succeeds with probability at least $1 - \Err$,
then it learns the internal structure of at least
$(1 - \Err)\frac{n}{4}$ size-4 clusters in expectation.
Thus, $\frac{6\NumQuery}{(n-1)(n-2)} \geq (1 - \Err)\frac{n}{4}$
which implies $\NumQuery \geq (1 - \Err) n (n - 1) (n - 2) / 24$.
\end{proof}

