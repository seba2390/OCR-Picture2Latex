\documentclass[sigconf,10pt]{acmart}

%\usepackage[•]{•}{microtype}

\pdfoutput=1
\usepackage{booktabs} % For formal tables
%\usepackage[subtle]{savetrees}
%\settopmatter{printacmref=true}
% Copyright
%\setcopyright{none}
%\setcopyright{acmcopyright}
\setcopyright{acmlicensed}
%\setcopyright{rightsretained}
%\setcopyright{usgov}
%\setcopyright{usgovmixed}
%\setcopyright{cagov}
%\setcopyright{cagovmixed}
\newcommand{\minihead}[1]{{\vspace{.5em}\noindent\textbf{#1} }}
\newcommand{\red}[1]{{\color{black}#1}}
\newcommand{\mvar}{\red{d}}
\newcommand{\dvar}{\red{n}}

\newcommand\code[1]{\lstinline$#1$}

%%%%%%%%%%%%
\renewcommand\paragraph{\@startsection{paragraph}{4}{\z@}%
                                    {0.5ex \@plus 0.5ex \@minus .2ex}%
                                    {-0.5em}%
                                    {\normalfont\normalsize\bfseries}}
\usepackage[labelfont=bf]{caption}
\setlength{\intextsep}{5pt plus 1.0pt minus 2.0pt}
\setlength{\abovedisplayskip}{1pt}
\setlength{\belowdisplayskip}{1pt}
%\usepackage[small,compact]{titlesec}

%\newenvironment{denseitemize}{
%\begin{itemize}[topsep=2pt, partopsep=0pt, leftmargin=1.5em]
%  \setlength{\itemsep}{4pt}
%  \setlength{\parskip}{0pt}
%  \setlength{\parsep}{0pt}
%}{\end{itemize}}
%
%\newenvironment{denseenum}{
%\begin{enumerate}[topsep=2pt, partopsep=0pt, leftmargin=1.5em]
%  \setlength{\itemsep}{4pt}
%  \setlength{\parskip}{0pt}
%  \setlength{\parsep}{0pt}
%}{\end{enumerate}}

%\newcommand{\subparagraph}{}
%\usepackage[small,compact]{titlesec}
%\renewcommand{\paragraph}[1]{\vspace{1mm}\noindent \textbf{#1}}
%\usepackage[labelfont=bf,skip=2pt,belowskip=2pt]{caption}
%%%%%%%%%%%

\usepackage{enumitem}


\usepackage{algorithm}
\usepackage[noend]{algpseudocode}
\algdef{SE}[DOWHILE]{Do}{doWhile}{\algorithmicdo}[1]{\algorithmicwhile\ #1}%

\theoremstyle{problem}
\newtheorem{problem}{Problem}[section]




\begin{document}
\title{DROP: A Workload-Aware Optimizer for Dimensionality Reduction}


\author{Sahaana Suri, Peter Bailis}
\affiliation{
  \institution{Stanford University}
}

\renewcommand{\shortauthors}{S. Suri and P. Bailis}


\copyrightyear{2019} 
\acmYear{2019} 
\setcopyright{acmlicensed}
\acmConference[DEEM'19]{International Workshop on Data Management for End-to-End Machine Learning}{June 30, 2019}{Amsterdam, Netherlands}
\acmBooktitle{International Workshop on Data Management for End-to-End Machine Learning (DEEM'30), June 30, 2019, Amsterdam, Netherlands}
\acmPrice{15.00}
\acmDOI{10.1145/3329486.3329490}
\acmISBN{978-1-4503-6797-4/19/06}


\begin{abstract}
Dimensionality reduction (DR) is critical in scaling machine learning pipelines: by reducing input dimensionality in exchange for a preprocessing overhead, DR enables faster end-to-end runtime. Principal component analysis (PCA) is a DR standard, but can be computationally expensive: classically $O(dn^2 + n^3)$ for an $n$-dimensional dataset of $d$ points. 
Theoretical work has optimized PCA via iterative, sample-based stochastic methods. 
However, these methods execute for a fixed number of iterations or to convergence, sampling too many or too few datapoints for end-to-end runtime improvements. 
We show how accounting for downstream analytics operations during DR via PCA allows stochastic methods to efficiently terminate after processing small (e.g., 1\%) samples of data. 
Leveraging this, we propose DROP, a DR optimizer that enables speedups of up to \red{$5\times$} over \red{Singular-Value-Decomposition (SVD)-based} PCA, and \red{$16\times$} over conventional DR methods in end-to-end nearest neighbor workloads.
\end{abstract}


\begin{CCSXML}
<ccs2012>
<concept>
<concept_id>10010147.10010257.10010258.10010262.10010277</concept_id>
<concept_desc>Computing methodologies~Transfer learning</concept_desc>
<concept_significance>500</concept_significance>
</concept>
<concept>
<concept_id>10002951.10003227.10003351</concept_id>
<concept_desc>Information systems~Data mining</concept_desc>
<concept_significance>300</concept_significance>
</concept>
</ccs2012>
\end{CCSXML}

\maketitle

%\input{abstract}
\input{intro}
\input{relwork}
\input{background}
%\input{sampling}
\input{algo}
\input{ear}
\input{extensions}
%\input{conclusion}

\section*{Acknowledgements}
We thank the members of the Stanford InfoLab as well as Aaron Sidford, Mary Wootters, and Moses Charikar for valuable feedback.  
We also thank the creators of the UCR classification archive for their diverse set of time series.
This research was supported in part by affiliate members and other supporters of the Stanford DAWN project---Ant Financial, Facebook, Google, Intel, Microsoft, NEC, SAP, Teradata, and VMware---as well as Toyota Research Institute, Keysight Technologies, Northrop Grumman, Hitachi, and the NSF Graduate Research Fellowship grant DGE-1656518.


\bibliographystyle{ACM-Reference-Format}
{\footnotesize
\bibliography{drop}}
%\input{appendices}
\end{document}
