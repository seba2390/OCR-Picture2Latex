\section*{abstract}
Dimensionality reduction is a critical step in scaling machine learning pipelines. Principal component analysis (PCA) is a standard tool for dimensionality reduction, but performing PCA over a full dataset can be prohibitively expensive. 
As a result, theoretical work has studied the effectiveness of iterative, stochastic PCA methods that operate over data samples. 
However, termination conditions for stochastic PCA either execute for a predetermined number of iterations, or until convergence of the solution, frequently sampling too many or too few datapoints for end-to-end runtime improvements. We show how accounting for downstream analytics operations during DR via PCA allows stochastic methods to efficiently terminate after operating over small (e.g., 1\%) subsamples of input data, reducing whole workload runtime. 
Leveraging this, we propose DROP, a DR optimizer that enables speedups of up to \red{$5\times$} over \red{Singular-Value-Decomposition-based} PCA techniques, and exceeds conventional approaches like FFT and PAA by up to \red{$16\times$} in end-to-end workloads.




%This enables end-to-end optimization over both dimensionality reduction and analytics tasks.
%By combining techniques spanning progressive sampling, approximate query processing, and cost-based optimization, we propose a dimensionality reduction optimizer that enables speedups of up to \red{$5\times$} over \red{Singular-Value-Decomposition-based} PCA techniques, and achieves parity with or exceeds conventional approaches like FFT and PAA by up to \red{$16\times$} in end-to-end workloads.

%As time series analytics workloads grow, we see an increase in time series tools and DB stuff to enable more efficient TS analytics routines. 
%In order to facilitate this, indexing of time series is important---and DR plays a critical step in this.





%DROP uses the key insight that structured data such as time series can be sufficiently characterized by a small subset of data, which permits aggressive sampling during dimensionality reduction.  
%Sampling allows DROP to uncover high quality low-dimensional bases in runtime proportional to the dataset's intrinsic dimensionality---\textit{independent} of the actual dataset size.
%Additionally, DROP uses online progress estimation to predict DROP and downstream runtime costs, enabling a trade-off between degree of dimensionality reduction and pipeline runtime, and obviating the need for the intrinsic data dimension to be specified by the user. 
%At a high level, DROP progressively samples its input, computes a candidate transformation, and terminates once it finds a low dimensional representation of suitable size and quality quickly enough to not bottleneck downstream tasks. DROP provides speedups of up to $50\times$ over the na\"ive, SVD-based approach and $33\times$  in end-to-end analytics pipelines.
%A dataset's intrinsic dimensionality is also typically unknown a priori (i.e., to what extent a dataset can be compressed), a necessary parameter for dimensionality reduction via PCA.
%Further, based on downstream analytics tasks, it may be crucial to quickly return a result even if it is of dimension higher than the intrinsic dimension, but sufficient for the task at hand.
%To address these challenges, we present DROP, a new dynamic optimizer for high-dimensional analytics pipelines that efficiently computes high quality results (low-dimensional bases) via approximate PCA, without PCA's expensive runtime.
%dimensionality reduction optimizer that efficiently performs PCA in high-dimensional analytics pipelines.
%new dynamic optimizer that efficiently computes high quality results (i.e., low-dimensional basis) without the expensive runtime of PCA
%DROP reduces the cost of PCA over structured datasets, and automatically identifies a low dimensional data representation by taking the runtime of downstream analytics tasks into consideration.




%DROP enables downstream-operation-aware optimization via online progress estimation, trading-off degree of dimensionality reduction with the combined runtime of DROP and downstream analytics tasks. 

%In this paper, we revisit a now-classic study of time series dimensionality reduction operators and find that for a given quality constraint, Principal Component Analysis (PCA) uncovers representations that are over $2\times$ smaller than those obtained via alternative techniques favored in the time series literature. 
