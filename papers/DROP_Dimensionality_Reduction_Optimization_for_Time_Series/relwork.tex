\section{Related Work}
\label{sec:relwork}
\label{sec:relatedwork}

\minihead{Dimensionality Reduction} DR is a well-studied operation~\cite{dr-survey1,dr-survey2,nonlinear-dr} in the
database~\cite{keogh-indexing,local-dr,charu-ss}, data
mining~\cite{sax,paa}, statistics and machine
learning~\cite{alecton,shamir,bernstein} communities.
In this paper, our focus is on DR via PCA.
While classic PCA via SVD is inefficient, stochastic~\cite{re-new, shamir} and randomized~\cite{tropp} methods provide scalable alternatives.
DROP draws from both the former to tackle the challenge of how much data to sample, and the latter for its default PCA operator (though DROP's modular architecture makes it simple to use any method in its place). Further, to the best of our knowledge, these advanced methods for PCA have not been empirically compared head-to-head with conventional DR approaches such as Piecewise Approximate Averaging~\cite{paa}.

%To the best of our knowledge, advanced methods for PCA
%have not been empirically compared head-to-head with conventional
%DR approaches such as Piecewise Approximate
%Averaging~\cite{paa}.

%Recent breakthroughs in the theoretical statistics community provide new algorithms for PCA that promise substantial scalability improvements without compromising result quality
%Foremost among these techniques are advanced stochastic methods~\cite{re-new,shamir}, and techniques for randomized SVD~\cite{tropp}.
%While we default to the latter for use by DROP's PCA operator, DROP's modular architecture makes it simple to use any method in its place, including recent systems advances in scalable PCA~\cite{ppca-sigmod}.
%As a proof of concept of our method, we provide implementations of full SVD-based PCA, power iteration, as well as Oja's method. 

%, especially on real datasets. 
%In addition, DROP
%\emph{combines} these methods with row-level sampling to provide benefits similar to using stochastic methods for PCA.

%This setting differs from that of Moving Window (or Rolling) PCA in that the these methods assume overlap among the data samples, whereas here our samples are independently drawn from the same underlying data distribution~\cite{mwpca}.

%\red{
%\minihead{Time Series Indexing}
%While DROP is intended as a general purpose DR operator for downstream workloads, there exists a vast body of literature specific to time series indexing for similarity search. 
%While these techniques, such as iSAX2+ (and related methods)~\cite{sax,isax,isaxorig,hotsax}, SSH~\cite{ssh}, and Coconut~\cite{coconut} are highly optimized for the bulk-load and repeated query use case, DROP provides a more flexible, downstream-operator aware method. 
%}

\minihead{Approximate Query Processing (AQP)} 
Inspired by AQP engines~\cite{barzan-keynote}
as in online aggregation~\cite{onlineagg}, DROP performs progressive
sampling.  
%In contrast with more general data dimensionality estimation methods~\cite{dr-estimation}, DROP optimizes for $TLB$. As we illustrated in \S\ref{sec:experiments}, this
%strategy confers substantial runtime improvements.
While DROP performs simple uniform sampling, the literature contains a wealth of techniques for various biased sampling techniques~\cite{surajit-sample, surajit-2}.
DROP performs online progress estimation to minimize the
end-to-end analytics cost function. This is analogous to query
progress estimation~\cite{qpi1} and performance
prediction~\cite{mr-predict} in database and data
warehouse settings and has been exploited in approximate query
processing engines such as BlinkDB~\cite{blinkdb}. 

\minihead{Scalable \red{ Workload-Aware, }Complex Analytics} DROP is an operator
for analytics dataflow pipelines. Thus, DROP is
an extension of recent results on integrating complex
analytics function including model training~\cite{bismarck,mcdb} and
data exploration~\cite{scorpion,canopy,kraska-viz} operators into analytics engines. 
%\red{In particular, DROP is especially related to recent work in integrating workload-aware cost models to complex subscription forecasting models~\cite{forecasting} so as to reduce subscriber notification overhead.}
