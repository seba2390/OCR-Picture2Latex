As we advance our understanding of the fundamental mechanisms governing human traveling behaviors, mobility models become increasingly more refined. Therefore, implementing those models requires more specialized tools such as efficient graph libraries, advanced spatial capabilities (GIS) and statistical routines, to name a few. Luckily, libraries for such purposes are widely available for the most popular languages, allowing researchers to focus their efforts (and grant resources) onto more relevant parts of their contributions.
In this section we present several practical aspects related to the modeling and simulation of human traveling behaviors. Such details do not correspond necessarily to implementation details but rather to many of the basic building blocks from which we can construct all of the mobility models we presented in this survey.  


\subsection{Modeling Frameworks}
\label{sec:frameworks}

Computational models are becoming increasingly important in contemporary research, with applications ranging widely in scope, technique, and goals. Strictly speaking, any simulation model implemented and executed on a computer can be considered a computational model, from numerical models of complex dynamical systems to recurrent neural networks in modern applications of artificial intelligence. 
What all computer models have in common is the fact that they are described in such a way that can be unambiguously translated to low-level code to run on a computer. Methodological and implementation details vary in several dimensions and must be carefully defined considering the characteristics of the system or process being modeled, the objectives of the experiments, and the expected results.

It is important to emphasize that a mobility model can be composed of smaller modeling blocks such as random number generators, optimizers, individual-level dynamics to name but a few. The actual macrolevel dynamics results therefore from the combination of these smaller processes. For instance, a CTRW is composed of waiting-time and jump-length distribution models. Often times, more complex mobility models incorporate ingredients such as social interactions \cite{grabowicz_2014_entangling,toole_2015_coupling} and environmental features \cite{eash_1984_development,erlander_1990_gravity,goncalves_1993_development,simini_2012_universal}. The soundness of a mobility model will thus rely heavily on the validity of those sub-components in representing the processes being modeled. Not surprisingly, many of the most groundbreaking models were based, at one level or another, on physical mechanisms and first principle formulations.
However, what all those mobility models have in common is the fact that they are based on Monte Carlo methods, in the sense that they rely on generating repeated random samples as an approximation for the behavior of the system (in this case, human displacements). 

Computational models can come in different flavors, varying in many aspects such as modeling approach and complexity, and are used to simulate human mobility to validate the analytical models by comparing data generated from the simulation with real world traces. Simulations can be grouped into three major modeling approaches: numerical, particle-based and agent-based models (ABM). Also, there are simulation of discrete events and  simulation based on system dynamics \cite{zeigler_2000_theory}. The former consists in conducting entities through processes or queues in order to analyze the general behavior exhibited by a system over time. In the latter, simulations are performed by analyzing the behavior of variables in systems of differential equations \cite{epstein_1997_nonlinear}. Historically, the first simulations were numerical in nature and although effective, in many cases the results of the simulations of the models did not have good adherence to reality, especially in cases where many abstractions are needed (i.e., intentional disregard of aspects of systems to simplify the model) \cite{lane_2000_you}.
The rise in popularity of ABM models, especially to model human mobility and social phenomenas, is due to the fact that they are more suitable when the simplicity of a Monte Carlo method is not sufficient or appropriate to represent more complex entities such as adaptive processes or local interactions. Also ABMs are especially useful when second-order dynamics or emergent phenomena are involved. In the context of human mobility, elements such as past-events memory, social networks and spatial capabilities make ABM the natural modeling approach.

Agent-based models are a class of computational models that emphasizes interactions of entities to assess their effects on a system. The key point is that simple rules and micro-level interactions lead to the emergence of complex macroscopic regularities and organizations that represent the observable outcomes to allow the study of the dynamics of individual behavior. 
Computational agents, according to Wooldridge and Jennings \cite{wooldridge_1995_intelligent}, normally have the following characteristics:
   \begin{description}
   \item [Autonomy] The agents' operation must take place without the direct control of its actions or internal states;
   \item [Social behavior] Agents must interact with other agents through some kind of standard communication;
   \item [Reactivity] Agents must be able to understand the environment and respond to it;
   \item [Initiative] In addition to reacting to the environment, agents are also able to take the initiative of an action, changing their behavior so as to fulfill a certain purpose.
 \end{description}
 
 Based on these aspects, simulations carried out with agent-based models allow for a deep investigation on the system from microlevel behaviors to macrolevel phenomena. Thus, agent-based models, depending on the kind of model to be designed, may show several benefits over other modeling techniques. According to Bonabeau \cite{bonabeau_2002_agent}, they may be summarized as follows: 
 \begin{description}
 	\item[Production of emergent phenomena] resulting from the interactions of individual entities. 
 	\item[Possibility to offer a natural way to describe systems] that are composed by interacting entities (e.g., stock markets).
 	\item[Be flexible] because agents can be modeled from different levels and approaches, endowing them with different kinds of behaviors, degrees of rationality  or learning skills.
 \end{description}
 %Traditionally, specialists from several areas (e.g., economics and engineering) have addressed questions regarding how decisions are made with aggregated models, assuming a perfect rationality and deriving an  optimal equilibrium  \cite{Bousquet2004}. However, in recent times, 




\subsection{Algorithms}
\label{sec:algo}

In this sections we provide the pseudo-code of the algorithms required to implements some of the mobility models. We also provide pseudo code to generate random numbers sampled from specific probability distributions. The general framework we use to model individual mobility is agent based as introduced in \sectionname\ref{sec:frameworks}.

\subsubsection{Individual Mobility}
 The ubiquity of cellular phones leads to the availability of large data sets capturing spatio-temporal traces of human trajectories. In the context of mobility modeling, the mathematical framework of \emph{Random Walks (RW)}, a class of stochastic random path models, has been extremely successful at reproducing some of the statistical properties of movements at a long-term limit. The mechanism at the core of random walks consists of a series of successive random events alternating displacements (i.e. movements) and changes of direction (\sectionname\ref{sec:rw}).
 
\paragraph{Random Walk}
A simple random walk model has been described in \sectionname\ref{sec:brownian}.
A way to simulate a random walk on a computer is to let the agent move to a $D-$dimensional lattice. We can simulate a simple random walk of $N$ steps on a 2-dimensional lattice (see \figurename\ref{fig:rw}) by starting from the origin $(0,0)$ of the space, and then iteratively choose one of the four possible directions available (up, down, left, right), and move of one unit of length in that direction. The process stops when we iterated $N$ times. The procedure is illustrated in Algorithm \ref{alg:RW}. Note that It is crucial to choose each direction with the same chance or the random walk will be biased to some direction. For such purpose, modern computing devices and operating systems provides specific functions. In our case, we assume we have a function \texttt{RANDOM(min,max)} that is able to generate a uniformly distributed random number in the range $[\texttt{min},\texttt{max})$. Such function is trivial enough to be implemented on any platform or any programming language.

\begin{algorithm}
\caption{Simple 2D Random Walk}
\label{alg:RW}
\begin{algorithmic}[1]
\Procedure{Random\textendash Walk}{N}
\State $x \gets 0$
\State $y \gets 0$
\State $n \gets 0$
	\While{$n < N$ }
		\State $r \gets \text{RANDOM(0,4)}$
		\If {$r < 1$} \Comment Up
			\State $y \gets y + 1$
		\ElsIf {$r < 2$} \Comment Down
			\State $y \gets y - 1$
		\ElsIf {$r < 3 $} \Comment Right
			\State $x \gets x + 1$
		\ElsIf {$r < 4$} \Comment Left
			\State $x \gets x - 1$
		\EndIf
	\State $n \gets n+1$
	\EndWhile
\EndProcedure
\end{algorithmic}
\end{algorithm}


\begin{figure}[ht]
\centering
	\subfigure[5000 steps]{
	\includegraphics[width=0.48\columnwidth]{Figures_Algo/rw5k}
	\label{subfig:rw5}
	}
	\subfigure[25000 steps]{
	\includegraphics[width=0.48\columnwidth]{Figures_Algo/rw25k}
	\label{subfig:rw25}
	}
\caption{Illustration of random walks in two dimensions. In \ref{subfig:rw5}, we have 5000 discrete steps while in \ref{subfig:rw25}, 25000 discrete steps. In the figure, more traversed lines are represented by darker lines.}
\label{fig:rw}
\end{figure}

\paragraph{L\'evy Flight}
L\'evy Flight have been described in details in \sectionname\ref{sec:levy}. The main difference with the simple random walk is the type of distribution used to generate the jump length at each step, which follows a heavy-tailed distribution (e.g., power law). Therefore the L\'evy flight is a succession of short jumps and some occasional long jumps (\figurename\ref{fig:lf}).
\begin{figure}[htbp]
\centering
	\subfigure[5000 steps]{
	\includegraphics[width=0.48\textwidth]{Figures_Algo/lf5k}
	\label{subfig:lf5}
	}
	\subfigure[25000 steps]{
	\includegraphics[width=0.48\textwidth]{Figures_Algo/lf25k}
	\label{subfig:lf25}
	}
\caption{Sample trajectories of L\'evy flights in two dimensions for $\beta = 0.6$ and step length $\Delta r \geq 1$. In \ref{subfig:lf5}, 5000 steps while in \ref{subfig:lf25}, 25000 discrete steps.}
\label{fig:lf}
\end{figure}
%
When trying to implement a L\'evy flight on a computer there are several issue to take in account:
\begin{itemize}
	\item If there are RNG functions available to draw a number from a distribution, usually the power law distribution is not supported.
	\item For a power law distribution we need to specify $\alpha$ and $x_{\text{min}}$ values.
	\item Continuous power law distribution is easy to deal with, but the discretization process may introduce an error.
\end{itemize}
%
To generate a random number drawn from a power law distribution we can use the transformation method \cite{william_1997_numerical}. Usually, on a computer we have a source of random reals $r$ uniformly distributed in the interval $[0,1)$ that are provided by a pseudo-random number generator. In this case we can obtain a random number $x$ drawn from a continuous probability density $p(x)$ with a cumulative distribution function $P$ by obtaining the functional inverse of the CDF \cite{clauset_2009_power}:
\begin{equation}
x = P^{-1}(1-r)
\end{equation}
where 
\begin{equation}
P(x) = \left(\frac{x}{x_{\text{min}}}\right)^{-\alpha+1}
\end{equation}
for a continuous power law, and
\begin{equation}
P(x) = \frac{\zeta(\alpha,x)}{\zeta(\alpha,x_{\text{min}})}
\end{equation}
for a discrete power law. Therefore we obtain
\begin{equation}
x = x_{\text{min}}(1 - r)^{-1/(\alpha-1)}
\end{equation}
for the continuous case, which is straightforward to implement in any programming languages. However, the discrete form of $P(x)$ cannot be inverted in closed form, therefore we solve $P(x)=\sum_{x'=x}^{+\infty} p(x') = 1-r $ instead by a combination of doubling up starting from $x=x_{\text{min}}$, which ensure  that $1-r \in [P(x),P(2x))$, and binary search (see Algorithm \ref{alg:power}). Note that we need to evaluate the generalized Riemann zeta function $\zeta$ to compute $P(x)$ which might be computationally expensive, and it is usually done by using third party scientific libraries. When speed is important, techniques like dynamic programming can greatly improve algorithm speed by storing the result of previous evaluations that therefore need not to be computed again. Furthermore, if accuracy is not of great importance, which is the case for most applications where $x_{\text{min}} > 5$, we can approximate the discrete power law to a continuous one. We first generate power law distributed numbers $y \ge x_{\text{min}} - \frac{1}{2}$ and then round it off to the nearest integer $x = \big\lfloor y + \frac{1}{2} \big\rfloor$ \cite{clauset_2009_power}:
\begin{equation}
	x = \left\lfloor \left( x_{\text{min}} - \frac{1}{2} \right) \Bigg( 1 - r \Bigg)^{-1/(\alpha - 1)} + \frac{1}{2} \right\rfloor
\end{equation}
It is straightforward to notice that the approximation is bigger for smaller value of $x$ and therefore it is maximum when $x = x_{\text{min}}$. The error between the approximated distribution an the discrete one when $x = x_{\text{min}}$ can be computed as:
\begin{equation}
\Delta p = 1 - \left ( \frac{ x_{\text{min}} + \frac{1}{2}}{ x_{\text{min}} - \frac{1}{2}} \right)^{-\alpha+1} - \frac{x_{\text{min}}}{\zeta(\alpha,x_{\text{min}})}
\end{equation}
%
\begin{algorithm}
\caption{Discrete Power Law Random Number Generator}
\label{alg:power}
\begin{algorithmic}[1]
\Procedure{Power\textendash Law}{$\alpha, x_{\text{min}}$}
\State $r \gets \text{RNG}(0,1)$
\State $x_1 \gets x_{\text{min}}$
\State $x_2 \gets x_{\text{min}}$

\Repeat \Comment Determine Boundaries of Solution
	\State $x_1 \gets x_2$
	\State $x_2 \gets 2 \times x_1$
\Until {$P(x_2) < 1 - r$}

\Repeat \Comment Binary Search of Solution
	\State $mid \gets  x_1 +  (x_2 - x_1) / 2 $
	\If {$ P(mid) < 1 - r$}
		\State $x_1 \gets mid$
	\Else
		\State $x_2 \gets mid$
	\EndIf
\Until {$x_2 - \lfloor x_1 \rfloor \ge 1$}
\State \Return {$\lfloor x_1 \rfloor$}
\EndProcedure
\end{algorithmic}
\end{algorithm}
%
Once we are able to generate power law distributed random numbers, we can modify the algorithm of the simple random walk to implement a L\'evy walk by choosing a direction randomly and uniformly, an angle $\theta$, and the length $r$ of the jump (in this case from a continuous power law distribution). In this case we move away from a simple lattice to let the coordinates to be in the Euclidean plane $\mathbb{R}^2$ (Algorithm \ref{alg:Levy}). 

\begin{algorithm}
\caption{L\'evy Walk}
\label{alg:Levy}
\begin{algorithmic}[1]
\Procedure{Levy\textendash Walk}{N}
\State $x \gets 0$
\State $y \gets 0$
\State $n \gets 0$
	\While{$n < N$ }
		\State $\theta \gets \text{RANDOM}(0,360)$
		\State $r \gets \text{POWER-LAW}(\alpha, x_{\text{min}})$
		\State $x \gets x + r \cos \theta$
		\State $y \gets y + r \sin \theta$
		\State $n \gets n+1$
	\EndWhile
\EndProcedure
\end{algorithmic}
\end{algorithm}

\paragraph{Continuos-Time Random Walk (CTRW)}
A limitation of the RW models introduced so far is that time intervals between two successive steps are constant (\sectionname\ref{sec:ctrw}).
%
In order to simulate a CTRW we can modify Algorithm \ref{alg:Levy} to include the wait time $\tau$ between jumps (Algorithm \ref{alg:CTRW}). Notice that the wait time and jump length need not to be drawn from power law distributions, but CTRW with power laws has been shown to be useful to model human mobility \cite{brockmann_2006_scaling}. Moreover, the power laws can have different parameters.

\begin{algorithm}
\caption{Continuos-Time Random Walk}
\label{alg:CTRW}
\begin{algorithmic}[1]
\Procedure{CTRW}{N}
\State $x \gets 0$
\State $y \gets 0$
\State $n \gets 0$
	\While{$n < N$ }
		\State $\theta \gets \text{RANDOM}(0,360)$
		\State $r \gets \text{POWER-LAW}(\alpha, x^{r}_{\text{min}})$
		\State $x \gets x + r \cos \theta$
		\State $y \gets y + r \sin \theta$
        \State $\tau \gets \text{POWER-LAW}(\beta, x^{\tau}_{\text{min}})$ \Comment Wait Phase
		\While{$\tau > 0$} 
			\If {$\tau >= 1$}
				\State $\text{WAIT}(1)$
				\State $\tau \gets \tau - 1$
			\Else
				\State $\text{WAIT}(\tau)$
			\EndIf
		\EndWhile
		\State $n \gets n+1$
	\EndWhile
\EndProcedure
\end{algorithmic}
\end{algorithm}

\paragraph{Exploration-Preferential Return (EPR)}

Although there is an extensive literature supporting the idea that human trajectories indeed follow reproducible scaling laws, the predictions of L\'evy flight and CTRW models systematically deviate from empirical results. Song \et \cite{song_2010_modelling} proposed two key mechanisms, unique to human trajectories, that were missing from existing models: \emph{Exploration} and \emph{Preferential Return} (see \sectionname\ref{sec:epr}). Another aspect that differs from previous study is the use of power law with exponential cutoff for the jump length and wait time (\figurename\ref{subfig:epr}).  Power law with cutoff does not have a closed-form expression. However, we can generate a random number drawn from a power law with parameters $x_{\text{min}}, \alpha$ and an exponential cutoff $K$ with the following process \cite{clauset_2009_power}: generate an exponentially distributed random number $x = x_{\text{min}} - \frac{1}{\lambda} \ln \left(1-r\right)$, where $r$ is a uniformly distributed number in the interval $[0,1)$. Then, accept or reject it with probability $p$ or $1 - p$ respectively, where $p = \left(\frac{x}{x_{\text{min}}}\right)^{-\alpha}$. Repeating the process until a number $x$ is accepted will guarantee that it will follow the desired distribution. This procedure is illustrated by Algorithm \ref{alg:pc}. Note that in the pseudo-code $x$ and $y$ are lists, therefore the operator $+$ has the meaning of concatenation of the two operands.
%
\begin{algorithm}
\caption{Power Law with Exponential Cutoff Random Number Generator}
\label{alg:pc}
\begin{algorithmic}[1]
\Require $\alpha > 1$
\Require $x_{\text{min}} \ge 1$
\Procedure{Power\textendash Law\textendash Cutoff}{$\alpha, x_{\text{min}, ~K}$}
        \State $x \gets \{\}$
        \State $y \gets \{\}$
        \State $n \gets 1$
        \State $\lambda \gets 1 / K$
        %\For {$i\gets 0, 10*n$} \Comment Generate exponentially distributed numbers
        	%\State y.append(xmin - (1./Lambda)*log(1.-random()))
        %	\State $y \gets y + \left\{ x_{\text{min}} - \left( \frac{1}{\lambda} \right) * \ln \left(1-\text{RANDOM(0,1)}\right) \right\}$
        %\EndFor
       	\While {True}
        	\For {$i\gets 0, 10*n$} \Comment Generate exponentially distributed numbers
        		%\State y.append(xmin - (1./Lambda)*log(1.-random()))
        		\State $y \gets y + \left\{ x_{\text{min}} - \left( \frac{1}{\lambda} \right) * \ln \left(1-\text{RANDOM(0,1)}\right) \right\}$
        	\EndFor
           	\State $y_{\text{temp}} \gets \{\}$
           	\For {$i\gets 0, 10*n$} \Comment Accept  with probability $p = \left(\frac{x}{x_{\text{min}}}\right)^{-\alpha}$
               	%\If {random()<pow(y[i]/float(xmin),-alpha):}
                \If {$\text{RANDOM(0,1)} < \left(\frac{y_i}{x_{\text{min}}}\right)^{-\alpha}:$}
                   	%\State ytemp.append(y[i])
                    \State $y_{\text{temp}} \gets y_{\text{temp}} + \left\{y_i\right\}$
                \EndIf
            \EndFor
           	\State $y \gets y_{\text{temp}}$
            % concatenate x and y
           	\State $x \gets x + y$
           	\State $q \gets \text{LENGTH}(x) - n$
           	\If {$q == 0$} 
               	\State break
			\EndIf
           	\If {$q > 0$} \Comment Make sure numbers are random
               	%\State r = range(len(x))
                \State $r \gets \{0, 1, 2, \cdots, \text{LENGTH}(x)\}$
               	\State $\text{SHUFFLE}(r)$

               	\State $x_{\text{temp}} \gets \{\}$
               	\For {$j \gets 0, \text{LENGTH}(x)$}
                    %\If {j not in r[0:q]:}
                    \If {$j \notin \{r_0, r_1, \cdots, r_q\}$}
                       	%\State xtemp.append(x[j])
                        \State $x_{\text{temp}} \gets x_{\text{temp}} + \{x_j\}$
                    \EndIf
                \EndFor
               	\State$x \gets x_{\text{temp}}$
               	\State break
			\EndIf            
            \If {$q<0$:} %\Comment Generate exponentially distributed numbers
               	\State $y \gets \{\}$
               	%\For {$j \gets 0, 10*n$}
                	% y.append(xmin - (1./Lambda)*log(1.-random()))
                %   	\State $y \gets y + \left\{ x_{\text{min}} - \left( \frac{1}{\lambda} \right) * \ln \left(1-\text{RANDOM(0,1)}\right) \right\}$
                %\EndFor
            \EndIf
		\EndWhile            
		\State \Return $x_0$
\EndProcedure
\end{algorithmic}
\end{algorithm}
%
Finally, if we define $S$ as the set of previously visited locations, $F_i$ the number of times (frequency) that the location $i$ has been visited, and $\rho$ and $\gamma$ the parameters that control the probability $P_{\text{new}}$ of an exploration jump then the EPR random walk is implemented by Algorithm \ref{alg:epr}. Note that while the $x$ and $y$ coordinates vary continuously, the space is actually divided in discrete ``locations'', like Voronoi patches or squares of a grid. Therefore, different points in the plane may be part of the same location; to this purpose we use the function \texttt{CURRENT-LOCATION(x,y)} to obtain the location associated with the current coordinates of the agent.

\begin{algorithm}
\caption{Exploration-Preferential Return (EPR) Random Walk}
\label{alg:epr}
\begin{algorithmic}[1]
\Procedure{EPR}{N}
\State $x, y, n, S \gets 0$
	\While{$n < N$ }
    	\State $i \gets \text{CURRENT-LOCATION(x,y)}$
    	\If {$i \in S$}
        	\State $F_i \gets F_i + 1$
        \Else
        	\State $S \gets S + i$
            \State $F_i \gets 1$
        \EndIf
        
        \State $\theta \gets 0$
        \If {$RANDOM(0,1) < P_{\text{new}}$} \Comment Exploration Step
        	\State $\theta \gets \text{RANDOM}(0,360)$
			\State $r \gets \text{POWER-LAW-CUTOFF}(\alpha, x^{r}_{\text{min}}, K_{r})$

        \Else \Comment Frequency Based Return Step
        	\State $r, \theta \gets \text{FREQUENCY-RETURN()}$
        \EndIf
        \State $x \gets x + r \cos \theta$
		\State $y \gets y + r \sin \theta$
        
    	\State $\tau \gets \text{POWER-LAW-CUTOFF}(\beta, x^{\tau}_{\text{min}}, K_{\tau})$  \Comment Wait Phase
		\While{$\tau > 0$}
			\If {$\tau >= 1$}
				\State $\text{WAIT}(1)$
				\State $\tau \gets \tau - 1$
			\Else
				\State $\text{WAIT}(\tau)$
			\EndIf
		\EndWhile
		\State $n \gets n+1$
	\EndWhile
\EndProcedure
\Function {Frequency-Return}{}
	\State $tot \gets \sum_{j \in S} F_j$
    \State $throw \gets \text{RANDOM(0,tot)}$
    \State $partial \gets 0$
    \For {$j \in S$}
       	\State $partial \gets partial + F_j$
        \If{$partial < throw$}
           	\State $r \gets \text{EUCLID-DIST(i,j)}$
            \State $\theta \gets \arctan \left( \frac{j.y - i.y}{j.x - i.x} \right)$
            \State \Return $r,\theta$
        \EndIf
    \EndFor
\EndFunction
\end{algorithmic}
\end{algorithm}

\paragraph{Recency}
A further improvement over the EPR model has been introduced by Barbosa \et \cite{barbosa_2015_effect}. The main observation is that frequently visited locations are also recently visited locations (\figurename\ref{subfig:recency}). However, recently visited locations might not always be frequently visited, because it might be that a location has been visited for the first time (\sectionname\ref{sec:recency}). The algorithm to implement the recency mobility model is exactly the same as for EPR, but when deciding the for a return jump we can choose either a frequency based return with probability $\alpha$ or a recency based return with probability $1-\alpha$, where $\alpha$ is a parameter extracted from empirical data (Equation (\ref{eq:recency}), Algorithm \ref{alg:recency}).
%
\begin{algorithm}
\caption{Function to Choose a Recently Visited Location}
\label{alg:recency}
\begin{algorithmic}[1]
\Function {Recency-Return}{}    
    \State $k \gets \text{ROUND(POWER-LAW($\nu$))}$ \Comment select the rank from a zipfian law
	\While{$k \ge \text{SIZE($S$)}$}
    	\State $k \gets \text{ROUND(POWER-LAW($\nu$))}$
	\EndWhile
  	\State 	$S \gets \text{SORT($S$)}$ \Comment order locations according to the visiting time
    \State $j \gets S_k$
    \State $r \gets \text{EUCLID-DIST(i,j)}$
    \State $\theta \gets \arctan \left( \frac{j.y - i.y}{j.x - i.x} \right)$
    \State \Return $r,\theta$
\EndFunction
\end{algorithmic}
\end{algorithm}
%
\begin{figure}[htbp]
\centering
	\subfigure[Individual Mobility Model]{
	\includegraphics[width=0.48\textwidth]{Figures_Algo/epr}
	\label{subfig:epr}
	}
	\subfigure[Recency Model]{
	\includegraphics[width=0.48\textwidth]{Figures_Algo/recency}
	\label{subfig:recency}
	}
\caption{The darker is the color of the dot, the more the location has been visited. \ref{subfig:epr} In the Individual Mobility (IM) model proposed by Song \et, a user goes back to frequently visited locations with increasing probability, therefore the color of the visited locations is mostly black. Furthermore, most of the visited locations are very close to the initial location from where the user movement started. \ref{subfig:recency} In the recency model, a user can decide to go back to a recently visited location even though it has been visited only once or few times, therefore the visitation frequency is more evenly distributed. Furthermore, the spatial pattern shows several clusters further from the starting location, similarly to what happens with L\'evy flights.}
\label{fig:eprre}
\end{figure}


\paragraph{Social Models}

It is intuitive to understand that the mobility of socially related individuals is correlated; for example friends often hang out and visit places together. Therefore, mobility models that include social network links and proximity based interactions were developed (See \sectionname\ref{sec:social}). Two prominent examples are the models described in \cite{grabowicz_2014_entangling} and \cite{toole_2015_coupling}, which can be seen as variations of the preferential return model (\sectionname\ref{sec:epr}), where the preferential return is governed by the social strength between the users instead of the visitation frequency of the locations. Given the previously presented building blocks and algorithms, it should be trivial at this point to implement these models, therefore the explicit algorithms are omitted.

In \cite{grabowicz_2014_entangling}, at every step each agent chooses to visit a social contact (i.e., another agent with whom the agent has a link) with probability $p_v$. Otherwise, it chooses a new location with the complementary probability $1 - p_v$ following a L\`evy-like flight. After the movement, the agent creates a new link with probability $p$ in the ``neighborhood'', that is the nearby agents, and probability $p_c$ with a random agent. An agent can easily keep track of the social links with the other agents by updating a list of the agent's ids it meets. 

In \cite{toole_2015_coupling}, the first part of the mobility model is identical to the preferential return model, where the agent returns to a previously visited location with probability $1 - \rho S^{-\gamma}$ or visit a new location with probability $\rho S^{-\gamma}$. However, it adds an additional ``social'' step. The actual choice of the location to visit, either new or returning, is made based on the social influence with probability $\alpha$ or, with probability $1 - \alpha$, just the individual preference (i.e., proportionally to the location visitation frequency). In case the location is chosen according to a social contact, the social contact $j$ is chosen with a probability directly proportional to the current mobility similarity between the two agents. The mobility similarity between two agents $i$ and $j$ is defined as the cosine similarity in the location visiting profiles:
\begin{displaymath}
cos \theta_{i,j} = \frac{\mathbf{v_i}\cdot\mathbf{v_j}}{|\mathbf{v_i}||\mathbf{v_j}|}
\end{displaymath}
where $\mathbf{v_i}$ and $\mathbf{v_j}$ represent the total visits made by the agents to each location in the simulation space.

\subsubsection{Population Mobility}
A simple way to formalize the flow of individuals from one location to another is to divide the area of interest into different zones labeled as $i = 1, \dots, n$ and to count the number of individuals going from location $i$ to location $j$, $\forall i$. These numbers $T_{ij}$ are the elements of the so called \emph{Origin-Destination} ($OD$) matrix (\sectionname\ref{sec:odmatrix})
\begin{equation}
OD(t) = 
\begin{bmatrix}
    T_{11} & T_{12} & \dots  & T_{1m} \\
    T_{21} & T_{22} & \dots  & T_{2m} \\
    \vdots & \vdots & \ddots & \vdots \\
    T_{n1} & T_{n2} & \dots  & T_{nm}
\end{bmatrix}
\end{equation}
Such matrix defines a directed and weighted network, and in the general case is time-dependent. Note that usually $n = m$ and $T_{ij} = 0, \forall i = j$.
The $OD$ matrix is very different from a ``segment'' measurement, where we can easily count the number of individuals going through a link of the transportation system under consideration (e.g., one of the flights of the airline network, a segment of road, etc.). In contrast, the $OD$ matrix is usually extremely difficult and costly to obtain and measure. There are obviously many factors which control the origin-destination matrix: land use, location of industries and residential areas, accessibility, etc.

In order to simulate the $OD$ matrix using the multi-agent modeling framework we need to define a process that drives the movement of the agents from one location to another. Let's consider a discrete-time agent simulation for which at every time tick $t$ we know the elements $T_{ij}$ of the matrix $OD(t)$. $T_{ij}$ can be considered as a frequency count, therefore we can transform the elements of the $OD$ matrix by computing the relative frequency $P_{ij}$:
\begin{displaymath}
P_{ij} = \frac{T_{ij}}{\sum_i \sum_j T_{ij}}
\end{displaymath}
$P_{ij}$ represents the probability of an agent to move from location $i$ to location $j$ at a specific instant in time $t$. If we want to run a simulation for an amount of time $\tau$, for each time instant $t$ the simulation algorithm normalizes the matrix $OD(t)$, then loop through each element $P_{ij}$ of the normalized matrix $OD$ and move an agent, \text{MOVE-AGENT(i,j)}, from $i$ to $j$ with the probability $P_{ij}$ (Algorithm \ref{alg:od}). This framework is general and applies to any mobility model that uses origin destination matrices such as the gravity models familiy (\sectionname\ref{sec:gravity}), intervening opportunities, and radiation model. However each model may be different in the way that the elements $T_{ij}$ are computed.
It is also interesting to note that if we represent the movement of the agents over e.g., a map layer, then the flux of agents over the connection between two locations should represent and estimate of the intensity of the traffic.

\begin{algorithm}
\caption{Origin Destination Matrix Agent Simulation}
\label{alg:od}
\begin{algorithmic}[1]
	\State $t \gets 0$
	\While{$t < \tau $}
		\State $OD \gets \text{NORMALIZE($OD(t)$)}$	\Comment normalize the $OD$ matrix
		\For {$i \gets 1, n$}	
    		\For {$j \gets 1, n$}
    			\If {$\text{RANDOM(0,1)} < P_{ij}$}
            		\State \text{MOVE-AGENT(i,j)}
            	\EndIf
        	\EndFor
    	\EndFor
        \State $t \gets t + 1$
    \EndWhile
\Function {Normalize}{OD}
	\State $\text{totalSum} \gets 0$
    \For {$i \gets 1, n$}
    	\For {$j \gets 1, n$}
    		\State $\text{totalSum} \gets \text{totalSum} + T_{ij}$
        \EndFor
    \EndFor
    \For {$i \gets 1, n$}
    	\For {$j \gets 1, n$}
    		\State $T_{ij} \gets T_{ij} / \text{totalSum}$
        \EndFor
    \EndFor
    \State \Return $OD$
\EndFunction
\end{algorithmic}
\end{algorithm}
