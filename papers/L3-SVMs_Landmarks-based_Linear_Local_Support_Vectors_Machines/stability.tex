\section{Theoretical Results}
\label{sec:stability}

In this section, we present a generalization bound on the true risk induced by our algorithm using the theoretical framework of the Uniform Stability~\cite{bousquet2002stability}.
We will see that this theoretical analysis gives some insights about the number of landmarks to select in practice. 

\subsection{\landSVM's uniform stability}

The idea of Uniform Stability is to check if an algorithm produces similar solutions from datasets that are slightly different. Let $S$ be the original dataset and $S^i$ the set obtained after having replaced the $i^{th}$ example of $S$ by a new sample $z_i'$ drawn according to ${\cal D}$.
We will say that an algorithm is uniformly stable if the difference between the loss suffered (on a new instance) by the hypothesis $f$ learned from $S$ and the loss suffered by the hypothesis $f^i$ learned from $S^i$ converges in $O(\frac 1m)$.

For the following analysis, we introduce a new notation that allows us to simplify the derivations. We rewrite

$$ f(x,k) = \theta \, \mul{x}^T $$

with $\theta = [\theta_{0.},...,\theta_{k.},...,\theta_{K.},b]$ and $\mul{x} = [\bm{0},...,\mul{x},\bm{0},...,\bm{0},1]$ (that implicitly depends on $k$) both of size $KL+1$ and

$$ F(f) = \frac{1}{2} \norm{\theta}^2 + \frac{c}{m}\sumi \max(0,1-y_i (\theta \mul{x_i}^T)).$$

\begin{defn}{(\bf{Uniform Stability})}
    A learning algorithm A has uniform stability $2 \frac{\beta}{m}$ \wrt the loss function $\ell$ with $\beta \in \mathbb{R}^{+}$ if

    $$ \sup_{z \sim \mathcal{D}} \abs{\ell(f,z) - \ell(f^{i},z)} \leq 2 \frac{\beta}{m} .$$ 
\end{defn}

The uniform stability is directly implied if 

$$ \forall z \in \mathcal{D}, \;\; \abs{\ell(f,z) - \ell(f^{\setminus i},z)} \leq \frac{\beta}{m}$$

where $f^{\setminus i}$ is the hypothesis learned on $S^{\setminus i}$, the set $S$ without the $i^{th}$ instance $z_i$, which follows from

$$\abs{\ell(f^{i},z) - \ell(f,z)} \leq \abs{\ell(f^{i},z) - \ell(f^{\setminus i},z)} + \abs{\ell(f^{\setminus i},z) - \ell(f,z)}  \leq 2 \abs{\ell(f^{\setminus i},z) - \ell(f,z)}$$

that uses the triangular inequality (at worse, altering a point is like removing a point and adding another one).

In order to study the uniform stability of an algorithm, it is required to prove the  $\sigma$-admissibility of the loss function.

\begin{defn}{(\bf{$\sigma$-admissibility})}
    A loss function $\ell(f,z)$ is $\sigma$-admissible \wrt $f$ if it is convex \wrt its first argument and $\forall f_1,f_2 $ and $\forall z \in \mathcal{Z}$:

    $$ \abs{\ell(f_1,z) - l(f_2,z)} \leq \sigma \abs{f_1(x,k)-f_2(x,k)} .$$
\end{defn}

Following~\cite{bousquet2002stability}, we know that the hinge loss is $1$-admissible.

We can now present the main result about our algorithm \landSVM.

\begin{thm}{\bf{\landSVM Uniform Stability}}
  Assuming that  $ \forall x \in \mathcal{X}, \norm{x} \leq c$,  \landSVM has uniform stability  $ \frac{c L M^2}{m}$,
where $M = \max(c^2,1)$ if  $\mu$ is the dot product and $M = 1$ if $\mu$ uses the RBF kernel.
\end{thm}

\begin{proof}

    As $\ell(f,z)$ is $1$-admissible, $\forall z=(x,y,k) \in \mathcal{Z}$,

    %      \abs{\ell(f^{\setminus i},z) - \ell(f,z)} \! \leq \! \abs{f^{\setminus i}\!(x,k)-f(x,k)} \!=\! \abs{\Delta f (x,k)} \label{testit}$$

    \begin{small}
    \begin{align}
      \abs{\ell(f^{\setminus i},z) - \ell(f,z)} &\leq \abs{f^{\setminus i}\!(x,k)-f(x,k)} = \abs{\Delta f (x,k)} \label{lin:lossdiffabs}
    \end{align}
    \end{small}

    with $ \Delta f = f^{\setminus i} - f$.
    By denoting $\Delta\theta = \theta^{\setminus i} - \theta$, we can derive, $\forall z=(x,y,k) \in \mathcal{Z}$,

    
    \begin{align}
        \abs{\Delta f (x,k)} &= \abs{\theta^{\setminus i} \mul{x}^T - \theta \mul{x}^T} \nonumber \\
        &= \abs{(\theta^{\setminus i}- \theta)\mul{x}^T} \nonumber \\
        & \leq \normf{\theta^{\setminus i} - \theta} \norm{\mul{x}} \label{lin:cauchy} \\
        & \leq \normf{\Delta\theta} \norm{\mul{x}} \nonumber \\
        & \leq \normf{\Delta\theta} \sqrt{L} \norm{\mul{x}}_\infty \label{lin:inf} \\
        & \leq \normf{\Delta\theta} \sqrt{L} \max_l(\mu(x,l)) \nonumber \\
        & \leq \normf{\Delta\theta} \sqrt{L} M \label{lin:dthetasqlm}
    \end{align}

    Eq.~\eqref{lin:cauchy} is due to the Cauchy-Swartz inequality,
    % Eq.~\eqref{lin:theta} is because $ \norm{\Delta f (.,k_i)} = \norm{\theta^{\setminus i} - \theta}$ ($\Delta f$ is linear in it's first parameter)
    and Eq.~\eqref{lin:inf} is because $ \norm{\mul{x}} \leq \sqrt{L} \norm{\mul{x}}_\infty$ recalling that $\mul{x} \in \mathbb{R}^{(1 \times L)}$.

    The value of $M$ depends on the chosen function $\mu$. For instance, if $\mu$ is the dot product, $M = \max(C^2,1)$ and if it uses the RBF kernel, $M = 1$.

    From Lemma 21 of \cite{bousquet2002stability}:

    $$ 2 \normf{\Delta\theta}^2 \leq \frac{c}{m} \abs{\Delta f(x_i,k_i)}.$$

    Then, by instantiating Eq.~\eqref{lin:dthetasqlm} for $z = z_i$, we get

    $$\normf{\Delta\theta}^2 \leq \frac{c}{2m} \abs{\Delta f(x_i,k_i)} \leq \frac{c}{2m} \normf{\Delta\theta} \sqrt{L} M$$

    and as $\normf{\Delta\theta} > 0$, we obtain

    $$ \normf{\Delta\theta} \leq \frac{c}{2m} \sqrt{L} M $$

    so, from the previous bound on $\abs{\Delta f(x,k)}$, we get

    $$ \forall z=(x,y,k), \;\; \abs{\Delta f(x,k)} \leq \normf{\Delta\theta} \sqrt{L} M \leq \frac{c L M^2}{2m}$$

    which, with Eq.~\eqref{lin:lossdiffabs} gives the $\frac{c L M^2}{m}$ uniform stability.


\end{proof}

Note that the stability of the algorithm depends on the number of selected landmarks. \landSVM is stable only if $L \ll m$, which is not a strict condition considering that, in practice, we select $L = O(n)$ landmarks (with $n$ the size of the input space $\mathcal{X}$) and that, for learning in general, $n \ll m$.

\begin{thm}{\cite{bousquet2002stability}}
Let A be an algorithm with uniform stability $\frac{2\beta}{m}$ \wrt a loss $\ell$ such that $0 \leq \ell(f,z) \leq E$, $\forall z \in \mathcal{Z}$. Then, for any i.i.d. sample $S$ of size $m$ and for any $\delta \in (0,1)$, with probability $1- \delta$:

$$ R_{\mathcal{D}}(f) \leq \hat{R}_{S}(f) + \frac{2\beta}{m} + \big( 4\beta + E \big) \sqrt{\frac{\ln \frac{1}{\delta}}{2m}}$$

where $R_{\mathcal{D}}(f)$ is the true risk and $\hat{R}_{S}(f)$ is the empirical risk on sample $S$. 

\end{thm}

Before deriving the generalization bound, we need to prove that our loss $\ell$ is bounded by a constant $E$ when evaluated at the optimal solution of $F$. Let $f$ be the minimizer of $F$. We deduce that:

\begin{gather}
    F(f) \leq F(\bm{0}) \nonumber \\
    \frac{1}{2} \norm{\theta}^2 + \frac{c}{m} \sumi \max(0,1-y_i (\theta \mul{x_i}^T)) \leq \frac{1}{2} \norm{\bm{0}}^2 + \frac{c}{m} \sumi \max(0,1-y_i (\bm{0} \mul{x_i}^T)) \nonumber \\
    \frac{1}{2} \norm{\theta}^2  \leq c \label{eq:sum} \\ 
    \norm{\theta}^2  \leq 2c \nonumber
\end{gather}

Eq.~\eqref{eq:sum} is because $ \forall a,b,c \in \mathbb{R}^{+}$, $ a + b \leq c $ implies that $ b \leq c $. Thus,

\begin{align}
    \ell(f,z) &= \max(0,1-y \theta \mul{x}^T) \nonumber \\
    & \leq 1 + \abs{\theta \mul{x}^T} \nonumber \\
    & \leq 1 + \norm{\theta} \norm{\mul{x}^T}  \label{eq:cauchy2}\\
    & \leq 1 + 2c \sqrt{L} M = E \nonumber
\end{align}

Eq.~\eqref{eq:cauchy2} comes again from the Cauchy-Swartz inequality.

\begin{cor}
    The generalization bound of \landSVM derived using the Uniform Stability framework is as follows:

    \small{
    $$ R_{\mathcal{D}}(f)\! \leq \!\hat{R}_{S}(f) + \frac{c L M^2}{m} + \left( \frac{2c L M^2}{m} \!+ \!1 \!+\! 2c \sqrt{L} M \! \right)\!\!\sqrt{\frac{\ln \frac{1}{\delta}}{2m}}.$$}

\end{cor}

%In other words, as the size of the sample increases, the true risk tends to be smaller or equal to the empirical risk, which implies that the algorithm generalizes well on unseen data.

