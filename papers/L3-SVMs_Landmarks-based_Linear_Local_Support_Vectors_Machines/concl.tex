\section{Conclusions and Perspectives}
\label{sec:conclpersp}

We introduce a new local learning algorithm named \landSVM.
It relies on a partitioning of the input space and on a projection of all points onto a set of landmarks.
Using the uniform stability framework, we show that \landSVM has theoretically generalization guarantees.
The empirical evaluation highlights that \landSVM is fast while being competitive with the state of the art.

While we introduced \landSVM with its ``default'' choices, the algorithm offers a lot of exciting perspectives.
First, we can refine many of the elements of \landSVM:
the partitioning using k-means can be replaced by other existing hard or soft clustering algorithms;
the random landmark selection procedure could be improved, for example using methods like DSELECT~\cite{kar2011similarity} and Stochastic Neighbor Compression~\cite{kusner2014stochastic}, or using density estimation~\cite{liu2016stein};
at a greater computational cost, a non-linear kernel can be used to have two levels of non-linearities (see Section~\ref{sec:where:cankerneltrick}).
% TODO: maybe, mention again the faster optimization method from Sec. 2.2
Even if the common landmarks act as a regularization of the local models, an overfitting is observed when the number of clusters becomes high.
The model could naturally accept explicit spatial regularization terms to increase the spatial smoothness of the models across clusters.
% TODO: autocite CVPR?
The speed and linearity of \landSVM also open the door to an auto-context approach (stacking): \landSVM would be reapplied on the data after projecting it on the previous level's support vectors.
Beyond stacking, we plan to explore a deep version of the algorithm, where the intermediate layers of projection are learned in a joint optimization problem.

% for us: full study and theory of the impact of the preprocessing
% for remi: link to deep half random (residual) models
