\section{Experimental Results}
\label{sec:expe}
In this section, we empirically study the behavior of \landSVM both on synthetic and on real datasets, and for binary and multiclass classification. Specifically, we study the impact of the number of clusters and the number of landmarks on learning, we analyze two different methods for selecting the landmarks and finally we compare our method to the state-of-the-art SVM based techniques.
\landSVM is implemented in Python using the liblinear~\cite{REF08a} library and the multiclass classification is performed through a one-vs-all procedure.

\subsection{Non-linearities}
Here we study the influence of the number of clusters on learning. We compare the performances of standard SVMs (linear or kernelized with a RBF kernel) with those of \landSVM (using the inner product or the RBF projection function) on two toy non-linear distributions: the XOR distribution and the Swiss-roll distribution. Remember that, for our method, even if the function $\mu$ used for projecting the data is the RBF, the learned models are still linear and the learning remains efficient.

For these experiments, we tune the hyper-parameters of each method by grid search with the values $\{10^{-3},10^{-2},10^{-1},1,10,100 \}$ in a 5-fold cross-validation procedure and for the \landSVM, the number of landmarks is arbitrarily fixed to 10 which are randomly selected from the training sample. The instances are clustered using k-means. In Fig.~\ref{fig:xor} and~\ref{fig:swiss} we draw the learned class separators, as well as the training instances (according to their true label) and the support vectors marked by a black point. We report the training and testing accuracies (on training and testing samples of same size) and the number of support vectors.

\begin{figure}[h!]
  \centering
    \includegraphics[width=0.32\textwidth]{xor/svm-linear.pdf}
    \includegraphics[width=0.32\textwidth]{xor/landmark-2clusters-linear.pdf}
    \includegraphics[width=0.32\textwidth]{xor/landmark-4clusters-linear.pdf}\\
    \includegraphics[width=0.32\textwidth]{xor/svm-gaussian.pdf}    
    \includegraphics[width=0.32\textwidth]{xor/landmark-2clusters-gaussian.pdf}
    \includegraphics[width=0.32\textwidth]{xor/landmark-4clusters-gaussian.pdf}\\
  \caption{\it{2D-XOR distribution}: 400 training instances.}
  \label{fig:xor}
\end{figure}

\paragraph{XOR distribution, Fig.~\ref{fig:xor}} We generated a synthetic XOR distribution by drawing instances uniformly over a 2D-space and assigning to each instance the label $+1$ (resp. $-1$) if its coordinates have the same sign (resp. different signs). As expected, the linear SVM is not able to separate the two classes, while the RBF SVM captures the non-linearities of the space. We notice that the performances of a \landSVM are comparable to the RBF SVM in terms of accuracy and number of support vectors already with 2 clusters and that with 4 clusters we achieve the best results. Moreover the learned class regions are similar to the theoretical ones.

\begin{figure}[h!]
  \centering
    \includegraphics[width=0.32\textwidth]{swissroll/svm-linear.pdf}
    \includegraphics[width=0.32\textwidth]{swissroll/landmark-2clusters-linear.pdf}
    \includegraphics[width=0.32\textwidth]{swissroll/landmark-100clusters-linear.pdf}\\
    \includegraphics[width=0.32\textwidth]{swissroll/svm-gaussian.pdf}    
    \includegraphics[width=0.32\textwidth]{swissroll/landmark-2clusters-gaussian.pdf}    
    \includegraphics[width=0.32\textwidth]{swissroll/landmark-100clusters-gaussian.pdf}\\
  \caption{\it{2D-Swiss-roll distribution}: 400 training instances, balanced classes.}
  \label{fig:swiss}
\end{figure}

\paragraph{Swiss-roll distribution, Fig.~\ref{fig:swiss}} The problem consists in separating a Swiss-roll distribution (the first class) from a uniform one (the second class). Unlike the XOR distribution, in this case 2 clusters are not enough to capture the non-linearities of the space, but with 100 clusters we obtain better performances than the ones of a Kernelized SVM. 

Notice that, in both experiments, as the number of clusters increases, the difference in accuracy between a \landSVM with a very fast inner product and a \landSVM with a RBF projection function is irrelevant.
Our method is then able to capture the non-linearities of the space as well as a non-linear SVM.
Note that the number of clusters depends, above all, on the nature of the input space.


\subsection{Choice of $L$}
\input{uci}
The aim of the following experiment is to empirically study how the number of landmarks impacts the testing accuracy. To do so, we fix the number of clusters (between 1 and 40) and vary the number of landmarks from 1 to the size of the training sample.

We compare the performances of standard SVMs (linear or kernelized with RBF) with those of \landSVM (using a linear or RBF projection) on three UCI datasets~\cite{Lichman:2013}. In Fig.~\ref{fig:uci} we draw the mean testing accuracies of a 5-fold cross-validation procedure repeated 10 times. For all the methods, at each iteration we tune the hyper-parameters by grid search with the values $\{10^{-3},10^{-2},10^{-1},1,10,100 \}$ with a 5-fold cross-validation procedure and we cluster the instances using k-means.

\paragraph{Liver, Fig.~\ref{fig:liver}} Already with 2 clusters, \landSVM achieves testing accuracies similar to those of a kernelized SVM. Furthermore, our method has the best results for 2 to 6 clusters. On the other hand, it seems that a \landSVM with a RBF projection function is really sensitive to overfitting.


\paragraph{Heart-Statlog, Fig.~\ref{fig:heart}} In this case, learning local models makes the predictions worse than learning a global one. As a matter of fact, from the comparison of an SVM and a Kernel SVM, it seems that the problem is linearly separable and that learning a non-linear classifier does not improve the results. Therefore, increasing the number of local models only makes them overfit.

\paragraph{Sonar, Fig.~\ref{fig:sonar}} Studying this dataset, which has more features than the previous two, it seems that it is possible to select a number of landmarks smaller than the dimension of the input space without deteriorating the results.

\paragraph{Ionosphere, Fig.~\ref{fig:sonar}} With this dataset, our method is not able to capture the non-linearities of the input space by combining local linear models. However, we notice that, already with 1 cluster and a number of landmarks at least equal to the dimension of the space, we obtain similar results by using the RBF kernel and by solving \landSVM with a RBF projection.
\\
 
In conclusion, we  claim that it is not interesting to have a number of landmarks greater than the dimension of the input space and that reducing the number of landmarks is not conceivable on datasets of small number of features. 
Also in this experiment, the performances of \landSVM with a RBF projection function are close or even worse than those of \landSVM with a linear kernel, probably because of overfitting. Therefore, in the following sections, we will restrict our studies only to a \landSVM with linear projection.

\subsection{Dimensionality Reduction}
The aim of the series of experiments is to study the impact of the chosen technique for landmark selection on the performances of our method. We compare  \landSVM with a set of landmarks randomly selected from the training sample to \landSVM with the landmarks as the principal components of the covariance matrix of the training set (performing a PCA) on the MNIST dataset~\cite{lecun-mnisthandwrittendigit-2010}. In Fig.~\ref{fig:pca}, we report the testing accuracies w.r.t. the number of landmarks $L$, as well as the time needed for selecting the landmarks. The number of clusters is fixed to $100$ and the parameter $c$ is tuned by grid search by 5-fold cross-validation. The instances are clustered using k-means. 

\pgfplotsset{
  xmin=10,xmax=784,
  xtick={10,100,200,300,400,500,600,700,784},
  legend style ={ at={(1.1,1)}, 
  anchor=north west, draw=black, 
  fill=white,align=left,font=\tiny},
  height=5cm,
  width=\textwidth
}

\begin{figure*}[t]
\captionsetup{justification=centering}
    \begin{subfigure}{0.45\textwidth}
    \begin{tikzpicture}
        \begin{axis}[xlabel = nb landmarks,height=4cm]
            \addplot table[x=L,y=accuracy,col sep=space,mark=o]{mnist/pca.csv};
            \addplot table[x=L,y=accuracy,col sep=space,mark=x]{mnist/random.csv};
            
        \end{axis}
    \end{tikzpicture}
    \caption{
      Testing Accuracy (\%)
    }
    \end{subfigure}
    % \hspace{-1cm}
    \begin{subfigure}{0.45\textwidth}
    \begin{tikzpicture}
        \begin{axis}[xlabel = nb landmarks,height=4cm]
            \addplot table[x=L,y=time,col sep=space,mark=o]{mnist/pca-time.csv};
            \addlegendentry{PCA};
            \addplot table[x=L,y=time,col sep=space,mark=x]{mnist/random-time.csv};
            \addlegendentry{Random};
        \end{axis}
    \end{tikzpicture}
    \caption{
      Selection Time (s)
    }
    \end{subfigure}
    \caption{
      Comparison of the testing accuracies and selection times (in seconds) for two methods of landmark selections: PCA and random selection. Notice that the difference in accuracy is limited when $L$ is bigger than $100$, while the time complexity is significantly lower using a random selection (around $0.020s$).
    }
    \label{fig:pca}

\end{figure*}

We use the Principal Component Analysis of the scikit-learn package~\cite{scikit-learn}, which implements the randomized SVD presented in~\cite{halko2011finding}. Having denoted $n$ the number of features and $m$ the number of instances, the complexity of this method is at worst $O(mn\log(n) + (m+n)n^2)$, when the rank of the training set is equal to $n$. Compared to a random selection ($O(L)$ as $L \ll m$) a PCA-based selection is more expensive and it achieves better results only when $L < 100$. 

These results suggest that, when $L$ is small, it is interesting to select good landmarks (by means of a PCA for instance) and it can be done in reasonable time. On the other hand, when $L$ is big, there is no need to force the variety and expressiveness of the set of landmarks, and a random selection from the training sample already allows us to have a good projection of the input space with little effort.


\subsection{Comparison with the State of the Art}
In this final series of experiments, we compare \landSVM with state-of-the-art methods on the four datasets presented in Table~\ref{tab:dataset} (with the features rescaled to have a standard deviation of 1).
In all experiments, we fix $L$ to the dimension of the input space, we select the landmarks randomly from the training sample and we cluster using k-Means.

\begin{table}
    \centering
    \caption{Characteristics of Datasets}
    \label{tab:dataset}
    \scalebox{0.7}{
    \setlength\tabcolsep{3pt}
    \begin{tabular}{ | c | c | c | c | c | c |}
      \hline
      & \#training & \#testing & \#features & \#classes & \#models \\ \hline
      \textbf{SVMGUIDE1} & 3089 & 4000 & 4 & 2 & 100\\ \hline
      \textbf{IJCNN1} & 49990 & 91701 & 22 & 2 & 100 \\ \hline
      \textbf{USPS} & 7291 & 2007 & 256 & 10 & 80 \\ \hline
      \textbf{MNIST} & 60000 & 10000 & 784 & 10 & 90 \\ \hline
    \end{tabular}}
\end{table}

\begin{table*}[h!]
    \centering
    \caption{Testing Accuracies (\%)}
    \label{tab:accuracy}
    \scalebox{0.9}{
    \begin{tabular}{ | c | c | c | c | c |}
      \hline
      & \textbf{SVMGUIDE1} & \textbf{IJCNN1} & \textbf{USPS} & \textbf{MNIST} \\ \hline
      \textbf{RBF-SVM} & 96.53 & 97.08 & 94.07 & 96.62\\ \hline
      \textbf{Linear-SVM} & 95.38 & 89.68 & 91.72 & 91.8\\ \hline
      \textbf{CSVM} & 95.05 & 96.35 & N/A & N/A\\ \hline
      \textbf{LLSVM} & 94.08 & 92.93 & 75.69 & 88.65\\ \hline
      \textbf{ML3} & 96.68 & 97.73 & 93.22 & 97.04\\ \hline
      \textbf{L$^3$-SVMs} & 95.73 & 95.74 & 92.12 & 95.05\\
      \hline
    \end{tabular}}
\end{table*}

\begin{table*}[h!]
    \centering
    \caption{Training and Testing times (in seconds).}
    \label{tab:timing}
    \scalebox{0.9}{
    % \setlength\tabcolsep{1.5pt}
    \begin{tabular}{ | c | c | c | c | c |}
      \hline
      & \textbf{SVMGUIDE1} & \textbf{IJCNN1} & \textbf{USPS} & \textbf{MNIST} \\ \hline
      \textbf{RBF-SVM} & \twocol{0.39}{0.11} & \twocol{104.02}{23.32} & \twocol{44.34}{3.49} & \twocol{2699.42}{136.82}\\ \hline
      \textbf{Linear-SVM} & \twocol{0.04}{0.06} & \twocol{0.74}{2.31} & \twocol{1.44}{0.36} & \twocol{24.29}{3.93}\\ \hline
      \textbf{CSVM} & \twocol{1.54}{0.06} & \twocol{2.31}{3.18} & N/A & N/A\\ \hline
      \textbf{LLSVM} & \twocol{0.23}{0.05} & \twocol{6.21}{0.93} & \twocol{121.83}{2.26} & \twocol{1393.58}{9.27}\\ \hline
      \textbf{ML3} & \twocol{1.47}{0.04} & \twocol{17.71}{1.01} & \twocol{41.29}{0.25} & \twocol{1312.48}{6.22} \\ 
      \hline
      \textbf{L$^3$-SVMs} & \twocol{0.22}{0.13} & \twocol{14.08}{5.35} & \twocol{34.91}{0.98} & \twocol{276.15}{9.29} \\ \hline
    \end{tabular}}
\end{table*}

Table~\ref{tab:accuracy} (resp.~\ref{tab:timing}) report the accuracy (resp. running times) of  \landSVM method and standard SVMs using either a linear or RBF kernel (using Liblinear or Libsvm~\cite{chang2011libsvm}), Clustered SVM (CSVM)~\cite{gu2013clustered}, Locally Linear SVM (LLSVM)~\cite{ladicky2011locally} and ML3 SVM~\cite{fornoni2013multiclass}\footnote{The results of CSVM for the multi-class datasets are missing because it is implemented only for binary classification.}. The number of local models is fixed and, if not differently specified in the respective papers (such as 8 nearest neighbors for LLSVM and $p=1.5$ for ML3), the hyper-parameters are tuned by 5-fold cross-validation.

While the testing time is sometimes higher than the other methods, it can be reduced by limiting the number of landmarks for the datasets with a lot of features, as in the previous experiments we showed that it doesn't affect the results. 
Overall, our method compares favorably in terms of training time, especially for high-dimensional input spaces, and has good accuracy across all datasets.
