\appendix
\section{Appendix}

\subsection{Hilbert Space $\mathcal{H}$}
\label{an:hilbert}


\begin{defn}{(\bf{Hilbert Space})}
    A real vector space $\mathcal{V}$ over $\mathbb{R}$ is a Hilbert Space if:
    \begin{enumerate}
        \item $\mathcal{V}$ is a real inner product space;
        \item $\mathcal{V}$ is a complete metric space with respect to the distance function induced by its inner product.
    \end{enumerate}
\end{defn}

\begin{thm}
    The space $\mathcal{H}$ resulting by a transformation $\mul{x} = [\mu(x,l_1),...,\mu(x,l_L)]$, with $\mu : \mathcal{X}^2 \to \mathbb{R}$ of an Hilbert space $\mathcal{X}$ is also an Hilbert Space if $\mathcal{L} \neq \bm{0}$.

\end{thm}


\begin{proof}

~\\If $\mathcal{L} \neq \bm{0}$, $<\mul{},\mul{}> = \mul{}\mul{}^T$ is an inner product, as:

\begin{enumerate}
    
    \item $<\mul{},\mul{}>$ is linear: $ \forall a,b \in \mathbb{R}$ and $ \forall x_1,x_2,x_3 \in \mathcal{X}$

    \small{
    \begin{align*}
        <a & \mul{x_1}+b\mul{x_2},\mul{x_3}> \\
        &= \big( a\mul{x_1} + b\mul{x_2} \big)\mul{x_3}^T \\
        &= a\mul{x_1}\mul{x_3}^T + b\mul{x_2}\mul{x_3}^T \\
        &= a <\mul{x_1},\mul{x_3}> + b <\mul{x_2}\mul{x_3}>;
    \end{align*}
    }

    \item $<\mul{},\mul{}>$ is symmetric: $ \forall x_1, x_2 \in \mathcal{X}$

    $$ <\mul{x_1},\mul{x_2}> = <\mul{x_2},\mul{x_1}>;$$


    \item $<\mul{},\mul{}>$ is always non-negative and null only for $\bm{x}=\bm{0}$: $\forall x \in \mathcal{X}$

    $$<\mul{x},\mul{x}> = \sump \mu(x,p)^2 \geq 0$$ 

    and $<\mul{x},\mul{x}> = 0$ iff $\bm{x}=\bm{0}$ as $\mathcal{L} \neq \bm{0}$.

\end{enumerate}

\end{proof}

In particular, the space generated by $\mu(x_1,x_2) = x_1^Tx_2$ or $\mu(x_1,x_2) = \exp(-\frac{\normtwo{x_1-x_2}^2}{\sigma})$ is an Hilbert Space. 

\subsection{Lagrangian Dual Problem}
\label{an:dual}

The \landSVM optimization problem takes the following form:
$$ \argmin_{\theta,b,\xi} \frac{1}{2} \normf{\theta}^2 + \frac{c}{m} \sumi \xi_i$$
$$s.t. \: y_i \left(\theta_{k_{i.}} \mul{x_i}^T + b \right) \geq 1- \xi_i \:\: \forall i=1..m$$
$$\xi_i \geq 0 \:\: \forall i=1..m$$ \label{eq:primal}

with $\mul{.} = [\mu(.,l_1),...,\mu(.,l_L)]$ the projection from the input space $\mathcal{X}$ to the landmark space $\mathcal{H}$.

The Lagrangian dual problem of the previous formulation is obtained by maximizing the corresponding Lagrangian \wrt its Lagrangian multipliers. The derived problem is a Quadratic Programming problem that can be solved by common optimization techniques and that allows one to make use of the kernel trick. The Lagrangian takes the following form:

$$ \mathcal{L}(\theta,b,\xi,\alpha,r) = \frac{1}{2} \normf{\theta}^2 + \frac{c}{m}\sumi \xi_i-\sumi r_i \xi_i -\sumi \alpha_i \left(y_i \big(\theta_{k_{i.}} \mul{x_i}^T + b \big) + \xi_i-1\right)$$
where $\alpha \in \mathbb{R}^{m}$ and $r \in \mathbb{R}^{m}$ are the positive Lagrangian multipliers.
Let's consider the fact that:

$$ \max_{\alpha,r} \min_{\theta,b,\xi} \mathcal{L}(\theta,b,\xi,\alpha,r) \leq \min_{\theta,b,\xi} \max_{\alpha,r} \mathcal{L}(\theta,b,\xi,\alpha,r) $$
where the left term corresponds to the optimal value of the dual problem and the right one to the primal's one. The dual and the primal problems have the same value at optimality if the Karush-Kuhn-Tucker (KKT) conditions are not violated (see~\cite{boyd2004convex}).

By setting the gradient of $\mathcal{L}$ \wrt $\theta, b$ and $\xi$ to 0, we find the saddle point corresponding to the function minimum:
$$ \nabla_{\theta_{kp}}\mathcal{L}(\theta,b,\xi,\alpha,r) = \theta_{kp} - \sumik{i} \alpha_i y_i \mu(x_i,l_p)$$

$$\nabla_{b}\mathcal{L}(\theta,b,\xi,\alpha,r) = - \sumi \alpha_i y_i $$

$$\nabla_{\xi_i}\mathcal{L}(\theta,b,\xi,\alpha,r) = \frac{c}{m} - \alpha_i - r_i$$

which give
\begin{equation} \label{eq:theta}
\theta_{kp} = \sumik{i} \alpha_i y_i \mu(x_i,l_p)
\end{equation}

\begin{equation} \label{eq:b}
\sumi \alpha_i y_i = 0
\end{equation}

\begin{equation} \label{eq:r}
\alpha_i = \frac{c}{m} - r_i
\end{equation}

We can now write the QP dual problem by replacing $\theta$ by its expression~\eqref{eq:theta} and simplifying following~\eqref{eq:b} and~\eqref{eq:r}:
$$ \max_{\alpha} \:\: -\frac{1}{2}\sumik{i}\sumik{j} \alpha_i \alpha_j y_i y_j \mul{x_i}\mul{x_j}^T + \sumi \alpha_i$$

$$ s.t. \:\: 0 \leq \alpha_i \leq \frac{c}{m} \:\: \forall i=1..m$$
$$ \sumi \alpha_i y_i = 0 \:\: \forall i=1..m$$

which is concave \wrt $\alpha$.

We need the following two additional constraints in order to respect the KKT conditions which guarantee that the optimal value found by solving the dual problem corresponds to the optimal value of the primal:
$$\alpha_i \left( y_i \left(\theta_{k_{i.}} \mul{x_i}^T + b \right) -1 + \xi_i \right) = 0 \:\: \forall i=1..m$$
$$r_i \xi_i = 0 \:\: \forall i=1..m$$

Once the Lagrangian dual problem solved, the characteristic vector $\theta$ and offset $b$ of the optimal margin hyperplane can be retrieved by means of the support vectors, i.e. the instances whose corresponding $\alpha_i$ are strictly greater than $0$:
$$\theta_{kp} = \sumik{a} \alpha_a y_a \mu(x_a,l_p)$$
$$b = y_a - \theta_{k_{a.}} \mul{x_a}$$
and the new instances can be classified :
$$y(x) = sign \left( \theta_{k_{i.}} \mul{x_i}^T + b \right).$$ 


\subsection{Graphical representation of variable dependencies}
\label{an:graphicalmodels}

Figures~\ref{fig:gm-svmpercluster} through~\ref{fig:gm-oursvm} graphically illustrates the variables involved in the different optimization problems that are solved by the local SVM approaches and \landSVM.
In these graphs, a node represents a variable (or a set of) and a link show a direct dependency between the variables, i.e., one variable is directly involved in the computation or the estimation of the other. 

\begin{figure}[h!]
  \centering
  \input{tikz-svmpercluster}
  \caption{Variable dependencies when learning one SVM per cluster (baseline used in Clustered SVM~\cite{gu2013clustered}).}
  \label{fig:gm-svmpercluster}
\end{figure}

\begin{figure}[h!]
  \centering
  \input{tikz-clusteredsvm}
  \caption{Variable dependencies for Clustered SVM~\cite{gu2013clustered}, where a common global regularization is used.}
  \label{fig:gm-clusteredsvm}
\end{figure}

\begin{figure}[h!]
  \centering
  \input{tikz-oursvm}
  \caption{Variable dependencies for our model, \landSVM, where one SVM is learned per cluster but the local models interact through a common bias and $\mathcal{L}$, the set of landmarks.}
  \label{fig:gm-oursvm}
\end{figure}
