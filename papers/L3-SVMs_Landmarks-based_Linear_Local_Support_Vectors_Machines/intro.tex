\section{Introduction}

One of the most famous and commonly used Machine Learning techniques for classification are the Support Vector Machines (SVMs)~\cite{cortes1995support}. This popularity is due to their robustness, simplicity, efficiency (even in non linear scenarios by means of the kernel trick) as well as their theoretical foundations via generalization guarantees. %It allows to learn class-separator large margin hyperplanes and it can be extended to solve non-linear problems by means of the kernel trick.

Despite those nice properties, SVMs may face some drawbacks: Kernel SVMs are known to be expensive in terms of time complexity and memory usage when the number of training examples is large, both at training and at testing time.
For training, the full Gram matrix needs to be evaluated (i.e., compute and store all pairwise training sample similarities), and then inverted. % is it really an inversion?
For testing, the time complexity depends on the number of support vectors which typically grows linearly with the number of training instances~\cite{steinwart2003sparseness}.
Therefore, Kernel SVMs have been shown not to scale well to very large data sets. 

Over the years, several methods have been proposed to speed up SVMs, for instance by reducing the size of the training set~\cite{bakir2005breaking}, or by making use of stochastic optimization~\cite{bordes2009sgd} or by solving an alternative formulation of the orginal SVM problem~\cite{Joachims:2006}.
On the other hand, \textit{locally-linear learning approaches} have been shown to be most appealing in terms of training time, testing time and accuracy.
They are effective for data sets that present multi-modalities and/or non-linearities because they are able to capture the local characteristics of the space.
They are also computationally efficient as they learn only linear classifiers (for which efficient solvers exist) and, at testing time, are independent of the number of support vectors.
One drawback of such techniques is that they may be subject to local over-fitting. % what fact do we have about this overfiting? do we solve it?
We can distinguish \textit{two main families} of local SVM approaches: the ones that locally learn combinations of a set of learned linear SVMs as in~\cite{ladicky2011locally,fornoni2013multiclass}, and those which partition the input space and learn a local model per region~\cite{gu2013clustered,fu2010mixing}.

% LLSVM:
% - binary (they say this can be generalized), no theoretical guarantees
% - SGD for online, or else QP solving
% LM3SVM: multiclass, better, no expressed guarantees, slightly slow (high for dimensional data)
% -  Our model is trained using a CCCP procedure (Smola et al., 2005), where each of the CCCP subproblems is solved using SGD

Methods from the first category estimate local combinations of linear SVMs and make the assumption that the input data are lying on a manifold along which the linear classifier evolves smoothly.
In~\cite{ladicky2011locally}, the manifold is approximated by selecting some anchor points (using k-means) and learning one local model per anchor point.
Each training point is then expressed, using a local coding scheme, as a linear combination of its closest anchor points.
The local coding ensure that each point is influenced by a limited number of models and thus that the learning is efficient.
Although effective, one drawback of this approach is that the influence of the anchor points is defined by a fixed neighborhood that has to be manually set.
A latent SVM formulation is used in~\cite{fornoni2013multiclass} where the authors follow the principle of~\cite{ladicky2011locally} but extend it to a multiclass setting and replace the local coding scheme by latent coordinates that are estimated jointly with the parameters of the linear models.
% Speed, problems, issues, limitations ?

% Clustered SVM:
% - has generalization bound using Rademacher complexity (and mention that k-means has theoretical properties (it's VC dim) that are used)
% - generally better than k-means+svm(baseline) and MSVM? and LLSVM (other family)

Methods from the second family, such as clustered SVMs~\cite{gu2013clustered}, first partition the input space, typically using k-means, and then learn a linear model in each region.
To cope with overfitting, clustered SVMs use a hierarchical regularization: the vector of parameters of each local model is constrained to be close to an unknown, shared, global parameter vector.
Using only linear combinations of linear models, this approach is shown to be fast while yielding accuracies better or comparable to the state of the art.
A limitation of this approach comes from the assumption that the same global model is both meaningful and sufficient to regularize the different local SVMs.
An advantage, over the first family of methods, is that a generalization error bound can be derived using the Rademacher complexity.

In this paper, we introduce a new local SVM method, called \landSVM, that targets computational efficiency while having provable theoretical guarantees.
Our method clusters the input space, carries out dimensionality reduction by projecting all points on selected landmarks, and learns interdependent linear combinations of linear models.
As such, our method lies in between the two families of local approaches presented above without suffering from the mentioned drawbacks.
On one hand, it can be seen as learning a set of linear models that are combined following a latent space (linear or not) induced by the set of landmarks that is common to all clusters.
On the other hand, the proposed method can be seen as clustering the input space and learning, in a projected space, a set of interacting linear models.

Using the framework of the Uniform Stability~\cite{bousquet2002stability}, we prove that our algorithm is stable \wrt changes in the training set allowing us to derive a tight generalization bound on the true risk.
It is worth noticing that our algorithm, which can be seen as a generalization of the standard SVM formulation, is configurable and offers many points for improvement: clustering algorithm, regularization terms, landmark selection method, projection function, etc.
However, while many variations can be imagined, our early experiments surprisingly show that the ``default'' choices (k-means clustering, random landmarks, linear projection, linear kernel) already yield an algorithm that is competitive with the state of the art while extremely fast and scalable.

The remainder of this paper is organized as follows: in Section~\ref{sec:method} we give a mathematical formulation of \landSVM and we analyze its complexity; in Section~\ref{sec:stability} we theoretically study our method through the Uniform Stability framework; in Section~\ref{sec:expe} we 
empirically study the impact of different configurations (e.g. the number of landmarks and the number of clusters) and compare our method to state-of-the-art local SVM-based methods; finally in Section~\ref{sec:conclpersp} we present some exciting perspectives of this work.
