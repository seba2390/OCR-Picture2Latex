\section{Soft-margin Landmarks-based Linear Local SMVs}
\label{sec:method}

Our method, called \landSVM, consists in partitioning the input space into $K$ clusters and learning $K$ corresponding (linear) models that interact in a single optimization problem.
The interactions come from a projection on a set of landmarks $\mathcal{L}$ that is common for all clusters and from the formulation of a unique linear problem with a single bias parameter.
It is worth noticing that a standard SVM is a particular case of our approach for $K=1$ and specifically chosen landmarks.

\subsection{Notations and Optimization Problem}

Let $\mathcal{X} \subseteq \mathbb{R}^n$ be the input space, $\mathcal{Y} = \{-1,1\}$ the output space and $\left\{C_k\right\}_{k=1}^K$ a partition of $\mathcal{X}$. We consider a training sample $\mathcal{S} = \left\{z_i=(x_i,y_i,k_i)\right\}_{i=1}^{m}$ of $m$ i.i.d. instances $z_i \in \mathcal{X} \times \mathcal{Y} \times \{1,..,K\}$ (such that $x_i \in C_{k_i}$) drawn from an unknown distribution $\mathcal{D}$.
Moreover, we  denote $\mathcal{L} = \left\{l_p\right\}_{p=1}^L \in \mathcal{X}^L$, a set of $L$ landmarks of the input space. 
The objective function of \landSVM is defined as follows:

$$ F(f) = \frac{1}{2} \norm{f}^2 + \frac{c}{m} \sumi \ell(f,z_i)$$

where $\ell(f,z) = \max(0,1-y f(x,k)) $ is the hinge loss and $f: \mathcal{X} \times \{1,..,K\} \to \mathbb{R}$ the function

$$ f(x,k) = \sump \theta_{kp} \mu(x,l_p) + b$$

which is  used for prediction with: $\hat{y} = sign(f(x,k))$.

Note that $\theta \in \mathbb{R}^{K \times L}$ is a matrix of weights expressing the influence of each landmark $p$ for a given cluster $C_k$. Doing so, we are supposing that the problem is linear in the space created by  both clusters and landmarks. Thus, we learn a vector of weights per cluster but a unique offset $b$. In Appendix~\ref{an:graphicalmodels}, we provide a visualization of the dependencies between the problem variables that can help understand our method.

Another way to see our method is as learning an SVM classifier in a projected space defined by the selected landmarks $\mathcal{L}$ and by a score function $\mu: \mathcal{X}^2 \to \mathbb{R}$ between points of the input space:

$$ f(x,k) = \theta_{k.} \mul{x}^T + b$$

where $\mul{.} = [\mu(.,l_1),...,\mu(.,l_L)]$ is a projection from the input space $\mathcal{X}$ to the landmark space $\mathcal{H}$.
Therefore, the clusters allow to capture the non-linearities of the space while the landmarks help to control the size of the input space. %: it is possible to reduce the number of dimensions by eliminating the redundant features, or to add new information by creating new ones.
Additionally, projecting on the landmarks acts as a regularization: as the landmarks are chosen without considering their class and the projection of an instance uses all the landmarks and not only those belonging to its partition, the risk of overfitting is reduced. Therefore, unlike clustered SVMs~\cite{gu2013clustered}, we don't need to learn an additional global model to regularize the local ones. 

As previously mentioned, our method is a generalization of standard SVMs: it is similar to SVMs when $K = 1$ and the set of landmarks $\mathcal{L}$ forms a basis of the input space $\mathcal{X}$, and fully equivalent if this basis is also orthonormal.

A Soft-Margin version of our optimization problem can be written as follows:
$$ \argmin_{\theta,b,\xi} \frac{1}{2} \normf{\theta}^2 + \frac{c}{m} \sumi \xi_i$$
$$s.t. \: y_i \left(\theta_{k_{i.}} \mul{x_i}^T + b \right) \geq 1- \xi_i \:\: \forall i=1..m$$
$$\xi_i \geq 0 \:\: \forall i=1..m$$
which boils down to maximizing the margin between the class hyperplanes while minimizing the average classification error.

\label{sec:where:cankerneltrick}
The previous problem is defined for the linear case but it can be easily rewritten for kernel SVMs considering that there exists an unknown mapping $\phi$ from $\mathcal{H}$ to $\mathcal{Z}$, a space with potentially infinite dimensions, such that $\phi(\mul{x_i})^T\phi(\mul{x_j}) = kernel(\mul{x_i},\mul{x_j})$ and assuming that $\mathcal{H}$ is a Reproducing Kernel Hilbert space~\cite{aronszajn1950theory}.
The kernelized problem can be solved using its dual Lagrangian formulation (see App.~\ref{an:dual}).
However, the advantages of locally learning non-linear SVMs are limited, as our approach already captures non-linearity and has lower complexity compared to kernel SVMs. 

By solving the problem in its dual form, we can also study the relation between the learned model and the support vectors.
The parameters are computed as follows (with $\left\{z_a = (x_a,y_a,k_a)\right\}_{i=1}^A$ the set of $A$ support vectors and $\alpha_a$ the dual value of $z_a$):

$$\theta_{kp} = \sum_{a=1 | k_{a}=k}^A \alpha_a y_a \mu(x_a,l_p)$$
$$b = \frac{1}{A}\sum_{a=1}^A (y_a - \theta_{k_{a.}} \mul{x_a}) $$
which means that the weight $\theta_{kp}$ for a cluster $k$ and a landmark $l_p$ depends on the support vectors of that particular cluster and on their similarities with $l_p$, while the parameter $b$ is computed using the global information obtained from all the support vectors. 

\subsection{Computational Analysis}
As previously mentioned, the main drawback of Kernel SVMs is their inability to scale with large datasets. As a matter of fact, their training complexity is cubic with the number of instances and their testing and memory complexities depend on the number of support vectors which is $O(m)$~\cite{steinwart2003sparseness}.

The proposed approach, if solved in its primal (e.g. using~\cite{REF08a}), has a complexity close to linear SVMs while capturing non-linearities. In Table.~\ref{tab:complexity}, we compare \landSVM with standard Linear-SVMs and RBF-SVMs in terms of training, testing and memory (for storing the learned model) complexities.
For \landSVM we consider the default configuration (that is also used in the experiments of Sec.~\ref{sec:expe}): clustering with k-means, random selection of landmarks and projection with the dot product. 

The training complexity of our method could also be improved by using recent optimization techniques proved to reduce the training time, such as~\cite{bakir2005breaking,bordes2009sgd}.

% training: KLm
% clustering: Km
% selecting landmarks: L -> n
% projecting: nLm -> n^2m
% testing: L*n+L
% memory: KL+Ln
\begin{table}
    \caption{Computational comparison, with $K$: the number of clusters ($K \ll m$), $L$: the number of landmarks ($O(n)$), with $n$: the number of features, and $m$: the number of training instances.}
    \label{tab:complexity}
    \centering
    \scalebox{1}{
    \begin{tabular}{ | c | c | c | c | }
      \hline
      & \textbf{Training Time} & \textbf{Testing Time} & \textbf{Memory Usage} \\ \hline
      \textbf{Linear-SVM} & $O(nm)$ & $O(n)$ & $O(n)$ \\ \hline
      \textbf{RBF-SVM} & $O(m^3)$ & $O(nm)$  & $O(nm)$ \\ \hline
      \textbf{\landSVM} & $O(KLm+Lnm)$ & $O(Ln)$ & $O(KL+Ln)$ \\ \hline
    \end{tabular}}
\label{fig:complexity}
\end{table}
