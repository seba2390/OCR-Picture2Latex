%File: formatting-instruction.tex
\documentclass[letterpaper]{article}
\usepackage{aaai}
\usepackage{times}
\usepackage{helvet}
\usepackage{courier}
\usepackage{graphicx}
%\usepackage{biblatex}
\usepackage[square,sort,comma,numbers]{natbib}
\usepackage{booktabs}
\frenchspacing
\setlength{\pdfpagewidth}{8.5in}
\setlength{\pdfpageheight}{11in}
%\pdfinfo{
%\Title (Topic Detection and Summarization of User Reviews)
%\Author (Pengyuan Li, Lei Huang, Guang-jie Ren)
%}
\setcounter{secnumdepth}{0}  

% The file aaai.sty is the style file for AAAI Press 
% proceedings, working notes, and technical reports.
%
\title{Topic Detection and Summarization of User Reviews}
% Integrating On-line Product Summaries for Customer Review summarization
\author{Pengyuan Li\textsuperscript{\rm 1}, Lei Huang\textsuperscript{\rm 2}, Guang-jie Ren\textsuperscript{\rm 2}\\
\textsuperscript{\rm 1}Department of Computer and Information Sciences, \\University of Delaware, Newark, Delaware, USA\\
\textsuperscript{\rm 2}IBM Almaden Research, San Jose, USA\\
pengyuan@udel.edu,
lei.huang1@ibm.com, gren@us.ibm.com}

 \begin{document}
\maketitle

\begin{abstract}
\begin{quote}
%story
A massive amount of reviews are generated daily from various platforms. 
It is impossible for people to read through tons of reviews and to obtain useful information. 
Automatic summarizing customer reviews thus is important for identifying and extracting the essential information to help users to obtain the gist of the data. 
However, as customer reviews are typically short, informal, and multifaceted, it is extremely challenging to generate topic-wise summarization.
While there are several studies aims to solve this issue, they are heuristic methods that are developed only utilizing customer reviews. 
%What we did
Unlike existing method, we propose an effective new summarization method by analyzing both reviews and summaries.
To do that, we first segment reviews and summaries into individual sentiments. 
As the sentiments are typically short, we combine sentiments talking about the same aspect into a single document and apply topic modeling method to identify hidden topics among customer reviews and summaries. 
Sentiment analysis is employed to distinguish positive and negative opinions among each detected topic. 
A classifier is also introduced to distinguish the writing pattern of summaries and that of customer reviews. 
Finally, sentiments are selected to generate the summarization based on their topic relevance, sentiment analysis score and the writing pattern. 
% Results
To test our method, a new dataset comprising product reviews and summaries about 1028 products are collected from Amazon and CNET. Experimental results show the effectiveness of our method compared with other methods. 
\end{quote}
\end{abstract}

\section{Introduction}
%%%% Problem (1)
The number of customer reviews from various platforms grows rapidly nowadays.
It is impossible for people to read through tons of reviews and to obtain useful information. Automatic summarizing customer reviews thus is important for identifying and extracting the essential information to help users to understand or to get the gist of the data.
By providing a summarization of previous product reviews, customers can easily understand the features of products and sellers can learn the actual needs from customers' feedback. 
Many studies have been focused on single document summarization and shown promising results for summarizing news articles~\cite{Baralis2013Graphsum, Wei2015Gibberish}, emails~\cite{Carenini2008Summarizing, Paulus2017Deep}, product titles \cite{Sun2018Multi} etc. However, summarization methods for single (general) document are not applicable for customer reviews that are multiple documents written by various customers. 
Moreover, as customer reviews are typically short, informal text containing information about multiple aspects of products, it is hard to find the hidden topics among customer reviews and summarize them. 
%We thus introduce an effective new method for summarizing customer reviews.

%%%% Abstractive methods (2)
Gernerally, summarization methods can be classified  into two categories: abstractive and extractive methods~\cite{Gambhir2017Recent, Pecar2018Towards}. Abstractive summarization aims to generate a short text summary by paraphrasing content of the original document. Several sentence compression methods have been proposed to comprise original sentences to create a summary by using a syntactic parser or a word graph \cite{Genest2010Text, Ffilippova2010Multi, Khan2015Framework}. However, it remains a difficult task due to the abstractive method typically involves many sophisticated nature language techniques such as meaning representation, content organization, sentence compression, paraphrasing etc. As such, the quality of the generated summary from an abstractive system is hard to control and present.  
Recently, many studies utilize neural networks that are based on encoder-decoder architecture for abstractive summarization~\cite{Rush2015Neural, Nayeem2018Abstractive, Cao2018Soft}. Still, the quality of the generated summary is the major concern, especially when it applies to customer reviews which contains noise, ungrammatical documents, and conflicting opinions.

%%%% Related work 2: Introduce extractive method first (3)
Much more efforts have been focused on extractive summarization that aims to select salient parts of the original document such as sentence parts or whole sentences as the summarization of documents. As such, topic models and clustering method were introduced to find the documents that talk about the similar content/topic. Statistical features such as the position of sentences, positive and negative words, sentence length, etc. are used to select important sentences and words from the source text~\cite{Fattah2009GA, Abuobieda2012Text}. The encoder-decoder and attention mechanism also be applied for extractive summarization recently~\cite{Nallapati2017Summarunner}. However, all above methods focus on summarizaion of news articles, or emails, etc. There are still very limited number of studies focus on summarization of customer reviews~\cite{Zhan2009Gather, Yu2016Product, Amplayo2017Adaptable, Tan2017Sentence}. 

% Specific related work
Zhan et al. (2009),  proposed an extractive summarizaiton method that is based on analysis of internal topic structure of product reviews and tested it on reviews collected from 8 products~\cite{Zhan2009Gather}. Yu et al. (2016) selected important sentences by analyzing their popularity and specificity~\cite{Yu2016Product}. For the methods proposed by Tan et al. (2017) and Amplayo et al. (2017), topic modeling methods are used~\cite{Amplayo2017Adaptable, Tan2017Sentence}. 
While thousand of reviews are used in above methods, they are heuristic methods due to the lack of groundtruth summaries. Summaries for less than 10 products or only the positive/negative rate are used to test their methods. As such, the critical source of summaries are missing for above methods.

Another work that could also be relevant to our work is opinion extraction from customer reviews. The opinion extraction methods differ from general customer review summarization as it focuses on summarizing selected sentences that only relevant to a manually designed topic. 
Hu et al. (2006) examined the review sentences and designed particular rules to detect product features among the source data and generate the summarization~\cite{Hu2006Opinion}. Ganesan et al. (2010) proposed a graph-based framework for generating summaries from review sentences  collected by using 51 queries~\cite{Ganesan2010Opiniosis}. Hu et al. (2017) proposed a sentence importance metric that is based on content and sentiment similarities for selecting important sentences~\cite{Hu2017Opinion}. Similarly, these methods are designed to learn opinions from only customer reviews rather than from both reviews and summaries.

%%%% Our contribution (5)
Here we present a new topic modeling based summarization method with following main contributions. Firstly, we created a new Amazon-Cnet dataset with mapping between Amazon reviews and Cnet summary. Secondly, we provide a unified framework to segment review, cluster review sentiments into single document, model the review topics,and generate the summarization. Lastly, the experimental results and evaluation provide convincing  evidence  that the proposed method can be a useful tool for review summarization.

%%%% Roadmap (6)
The rest of the article is organized as follows: Section 2 describes the complete framework of our method and detailed steps; Experimental settings, results and performance evaluation are presented and discussed in Section 3; followed by conclusion and future work in Section 4.


\section{Methods}
% Framework
Our goal is to build a summary generator by analyzing both reviews and summaries. We first preprocess the raw text and segment reviews or summaries into individual sentiments where each of them contains information about only one aspect of product. As the sentiments are typically short, we thus combine sentiments talking about the same topic/aspect into a single document and apply topic modeling method to identify hidden topics among customer reviews and summaries. Next, we apply sentiment analysis method to those sentiments that belong to the same topic for distinguishing positive and negative sentiments. To generate the final summarization, a classifier also is introduced to distinguish the writing pattern of summaries and that of customer reviews. Finally, sentiments are selected to generate the summarization based on their topic probability, sentiment analysis score and writing pattern. The complete framework for our approach is shown in Fig. 1. The rest of this section provides details of each step.

\begin{figure}[!t]
%\vspace{-3mm}
\includegraphics[width = 3.3 in]{framework1}
\caption{Our framework for customer review summarization.}
\label{framework}
\end{figure}

\subsection{Text preprocessing}
% Break customer review into sentences/snippets
A customer review typically contains information about multiple aspects of a product.
To obtain the information about individual aspect, we first break a review into sentences. 
As such, a customer review, \textit{review} $i$, is converted into a set of sentences $(s_i^1, .. s_i^m)$.
Similarly, we also split a summary, \textit{summary} $j$, into individual sentences $(s_j^1, .. s_j^n)$.

% Create longer document for further analysis
We note that customer review are written in informal and concise phrases. As such, the majority of sentences after parsing are very short (less than 8 words length). It is hard to learn the topic and sentiments from short documents directly. Notably, sentences contains the same noun. typically talking about the same aspect of the product. For example, ’The battery last one day long.’ and ‘It is pretty heavy due to the battery.’ all talk about ‘battery’ which appears in both sentences. We thus combine sentences that contain the same noun to create a longer document, $Doc_{noun}$, for further process. As a sentence may contain more than one noun, such sentence will appear in multiple combined documents. For the sentences that do not have any nouns, they will not be included in any combined documents.

% Typical preprocessing for text processing
For each sentence and combined document, the traditional preprocessing steps are also employed. We first substitute all contractions and specific terms, such as, e-mail, sd-card based on a manually build dictionary. For example, the word ‘e-mail’ will be converted to email. We also remove stop words~\cite{Nothman2018Stop} and standard suffixes using Porter stemmer~\cite{Porter1980}.

\begin{figure*}[htbp]
%\vspace{-3mm}
\includegraphics[width = 7 in]{summarization_examples.png}
\caption{Summarization examples generated by using our method.}
\label{summary_examples}
\end{figure*}

\subsection{Topic modeling on reviews and summaries}
% Why we do topic modeling
As we mentioned before, customers reviews typically contain information about multiple aspects of products. Moreover, due to different user experience, the aspects reviewed by different customer could be also vary. Thus, identifying the hidden topics among customer reviews is important to generate the summarization. Regarding the summaries, as they are written by people typically with expertise and focus on only product itself. The topics covered by customer reviews are quite different from that by summaries. For example, the shipping experience which is an important topic among customer reviews, typically will not be mentioned in Cnet summaries. We thus identify the hidden topics among both customer reviews and summaries. 

% What is LDA
In this work, we use LDA to identify hidden topics among combined documents of customer reviews and of summaries, which is a generative statistical model that has been widely used for topic modeling. To do that, we use the implementation from scikit-learn~\cite{Hoffman2010Online}. The model parameters are learnt iteratively for different number of topics, $K$, where $K$ ranges from 5 to 40, and the log-likelihood and perplexity are calculated for each value of $K$. To determine the optimal number of topics, we identify the $K$ value that maximizes log-likelihood and minimize the perplexity. Two models LDAreview and LDAsummary are trained based on Amazon customer reviews and Cnet summaries respectively.

% How to do prediction
To identify the review sentences that talk about the same topic, we predict the topic label for each review sentence using LDAreview and LDAsummary, respectively. We note review sentences after the parsing step may be too short to be classified. Therefore, the sentence which obtains all zero prediction for all topics will be discard. 
%Neighbor sentence could be also useful. 
Review sentences then will be grouped into sets of topics along with their probability scores, $P_{s_i^j}^t$, indicating how likely the sentence $s_i^j$ belongs to the particular topic $t$.

\subsection{Topic understanding and sentiment analysis}
% Why we do sentiment analysis
While we have split review sentences into sets of topics, many sentences that belongs to the same topic may express conflicting opinions. For example, sentences ’The screen has great resolution.’ and ‘I hope I bought larger screen’ could be assigned with the same topic label, while they totally express opposite opinions. To identify the hidden opinions among each topic, we apply sentiment analysis to sentences belonging to the same set.

% How we do it
To do sentiment analysis, we employ VADER (Valence Aware Dictionary for sEntiment Reasoning) which is a rule-based model utilizing lexical features and rules that embody grammatical and syntactical conventions \cite{Hutto2014Vader}. As VADER is build upon analyzing social media text snippets collected from twitters that have similar writing patterns with review data, we believe it is well-fitted for review sentiment analysis.

% What is the output
Thus, given a review sentence $s_i^j$, we can obtain a positive sentiment score, $PS_i^j$, and a negative sentiment score, neg $PS_i^j$ by using VADER. The opinion of sentence $s_i^j$ then can be determined by the label of the maximum value of  $PS_i^j$ and $PS_i^j$. 

\subsection{Summary generation}
% What we are going to do for summary generation
Customers typically talk about multiple aspects of the product in their reviews. To generate the final summary, we first identify the $k$ most salient topics that are covered by customer reviews. In our work, the $k$ is set to 5. 
%To cover all topics, one can choose k equals to the number of topics. 
To do that, we check the number of sentences that in each topic set and pick the $k$ most salient topics for generating the summary.

% How to choose the opinion score
As we mentioned, conflicting opinions could appear in the same topic. To represent the overall opinion of a topic, we also select the most popular attitude in the most salient topics. For example, when the number of positive sentences is higher than that of sentences labeled as negative, we believe the topic is positive and the opinion score for each sentence obtained by sentiment analysis, $OP_i^j = PS_i^j$. Otherwise, the topic is negative and the opinion score for each sentence, $OP_i^j = NS_i^j$. 

% Train classifier for distinguishing summaries and reviews
Notably, summaries are typically written in a different style from customer reviews. Therefore, the writing style is one of the most important factors for ranking sentences.
Unlike existing work that generate the summary just based on the analysis of reviews. We build a classifier to distinguish writing patterns for summaries and that for customer reviews. To do that, we create a set of summary sentences and a set of review sentences. We note the imbalance issue between the summary sentence set and the review sentence set. We use meta learning~\cite{Chan1998Toward} where the majority class is split into multiple subsets, each of which is of similar size to the minority class, to train a base-classifier. The final classifier is build upon the decision made from all base classifiers. By using this classifier, we can obtain a summary likelihood for each sentence, $SL_i^j$.

% Factors that are important for generate summaries
% Formular for ranking sentences within a topic
The summarization of customer reviews is a set of most important/informative/representative sentences that are selected from the $k$ most salient topics. All above factors are very critical to determine the importance of the sentence:
\begin{itemize}
\item The probability that the sentence belong to current topic: $P_{s_i^j}^t$
\item The opinion score obtained from sentiment analysis: $OP_i^j$.
\item The summary likelihood: $SL_i^j$.
\end{itemize}

To select the most important sentence within each topic sentence set, we calculate the importance score for each sentence as follows:
\[Score(S_i^j) = (P_{s_i^j}^t + OP_i^j) * SL_i^j\]

After ranking the sentences within each topic, sentences with the highest score within their corresponding topics are selected as the final summary. 
%Here is an example of summary generated by using our pipeline:

\section{Experiments and results}
\subsection{Experimental settings}
To evaluate our method, we compare the performance of our system to that of two state-of-the-art systems, TextRank~\cite{Mihalcea2004Textrank}, Opinosis~\cite{Ganesan2010Opiniosis}, Biclique~\cite{Muhammad2016biclique}, ILPSumm~\cite{Banerjee2015ILPsumm}, and ParaFuse\_doc~\cite{Nayeem2018Abstractive}. 
%A significant problem for our experiment is the evaluation metrics. Automatic evaluation can be quite controversial as there exist not only one correct summarization. 
Automatic evaluation measures like ROUGE and its modifications~\cite{Lin2004Rouge} are used to evaluate our performance. 
%can partially deal with these problems using n-grams, but still do not handle a use of synonyms. Notably, customer reviews may cover topics that are not covered by summaries. We thus involve human participations to decide if a summarization is correctly generated.
% need to modify the evaluation, point out the difference between news and customer reviews.
% More statement about testing and groundtruth summary

\subsection{Datasets}
As there is no relevant dataset available online, we build our own, Amazon-Cnet dataset. To do that, we first select 2000 cell phone products that associated with more than 10 customer reviews at random from the Cell Phones and Accessories category in Amazon Review Dataset~\cite{He2016Ups}. Cnet \footnote[1]{www.cnet.com} is one of the most popular website providing professional reviews for electronics, such as cell phones. We thus manually crawl the summary from Cnet webpage as ground-truth summaries for those cell phones. At last, 1028 products have their Cnet review webpage and be included in Amazon-Cnet datsset. Table 1 provides statistics of our datasets.

\begin{table}[htbp]
\centering
\caption{Statistics for our Amazon-Cnet dataset used in our experiments. The summaries and customers reviews are for 1028 products.}
\label{table_dataset}
\begin{tabular}{l | c | c } 
\toprule
\textbf{}   & \textbf{The number} & \textbf{\# of sentences} \\
\midrule
Summaries  & 1028   & 1385\\
Reviews & 66129 &  362965 \\
\bottomrule
\end{tabular}
\end{table}


\subsection{Results}
%% Table description
% Table~2 shows the results obtained by our method and other state-of-the-art systems. 
% Notably, the ROUGE-1 scores obtained by TextRank, Opinosis, Biclique, ILPSumm, and ParaFuse\_doc are all lower than 10\%. As all these systems generate summarizations by analysis the customer reviews only, the generated summaries typically have very few overlap with the ground-truth summarizations.
% In contrast, our method considering both reviews and summarizes attained the highest ROUGE-1 score of X\%. 


%% Discussions
Our method attained 15.43\% over the Amazon-Cnet dataset.
We note that the ROUGE-1 score obtained by our method is still much lower than other results reported over datasets like Opinions, given Amazon-Cnet datsset is more challenging and practical for real-world usage. 
% Different level
By looking at the dataset, we found that the reviews typically talk about details of a product, such as the resolution of the screens, the loudness of the speakerphone. 
In contrast, the summaries typically talk about the high level characteristics of the product, such as the smooth of the mobile system.  
% Different topics
We also found that the topics within customer reviews usually are not interests of summarization. For example, the customer service, which is a hot topic among reviews, is not an interest for summarization.
Unlike other methods generate summarization by analyze the reviews only, our method also consider the topics among summaries therefore out-performance all other methods.
%% Result examples
Figure~\ref{summary_examples} shows two summarization examples that are generated by using our method. 

% Future work
%While our method indeed covers more topics among ground-truth summaries than those generated by other methods, 
This is our first preliminary work to integrate the summary information for customer review summarizations. As such, there is still much room for improvements. We are going to collect more online review and sumamrization pairs to train a more comprehensive model. 
We also note that the user reviews of a product could span a wide range of time period. However, the summary of a product is typically posted at very early stage when the product been released. For example, the Cnet summary for product 'ZTE Merit' is posted on Aug 2012, and the latest customer review is posted on Mar 2016. We can believe that the opinion reported 4 years after the first product release may not be helpful for hte summariaation.
As such, we are going to also investigate the time series issue among customer reviews for creating their summarization.


\section{Conclusion}
 
We present a new method for customer review summarization utilizing both customer reviews and summaries. Unlike other methods that only consider customer reviews, we identify hidden topics among both customer reviews and summaries. 
Sentiment analysis is employed to distinguish positive and negative opinions among each detected topic. 
A classifier is also introduced to distinguish the writing pattern of summaries and that of customer reviews. 
Finally, sentiments are selected to generate the summarization based on their topic relevance, sentiment analysis score and the writing pattern. A new dataset comprising product reviews and summaries about 1028 products are collected from Amazon and CNET. Experimental results show the effectiveness of our method. % compared with other state-of-the-art methods. 

%This is our first preliminary work to integrate the summary information for customer review summarizations. We note that there is much room left for improvement. 
Sevearl challenging issues remain as future work. We are going to collect more data to train a more comprehensive model for review summarization. We shall also investigate the time series among customer reviews for generating summarization.

 
\begin{thebibliography}{}

\bibitem{Baralis2013Graphsum}
Baralis, E., Cagliero, L., Mahoto, N. and Fiori, A., 2013. GRAPHSUM: Discovering correlations among multiple terms for graph-based summarization. Information Sciences, 249, pp.96-109.

\bibitem{Wei2015Gibberish}
Wei, Z. and Gao, W., 2015, August. Gibberish, assistant, or master?: Using tweets linking to news for extractive single-document summarization. In Proceedings of the 38th International ACM SIGIR Conference on Research and Development in Information Retrieval (pp. 1003-1006).

\bibitem{Carenini2008Summarizing}
Carenini, G., Ng, R.T. and Zhou, X., 2008, June. Summarizing emails with conversational cohesion and subjectivity. In Proceedings of ACL-08: HLT (pp. 353-361).

\bibitem{Paulus2017Deep}
Paulus, R., Xiong, C. and Socher, R., 2017. A deep reinforced model for abstractive summarization. arXiv preprint arXiv:1705.04304.

\bibitem{Sun2018Multi}
Sun, F., Jiang, P., Sun, H., Pei, C., Ou, W. and Wang, X., 2018, October. Multi-source pointer network for product title summarization. In Proceedings of the 27th ACM International Conference on Information and Knowledge Management (pp. 7-16). ACM.

\bibitem{Gambhir2017Recent}
Gambhir, M. and Gupta, V., 2017. Recent automatic text summarization techniques: a survey. Artificial Intelligence Review, 47(1), pp.1-66.

\bibitem{Pecar2018Towards}
Pecar, S., 2018, July. Towards opinion summarization of customer reviews. In Proceedings of ACL 2018, Student Research Workshop (pp. 1-8).

\bibitem{Ffilippova2010Multi}
Filippova, K., 2010, August. Multi-sentence compression: Finding shortest paths in word graphs. In Proceedings of the 23rd International Conference on Computational Linguistics (pp. 322-330). Association for Computational Linguistics.

\bibitem{Genest2010Text}
Genest, P.E. and Lapalme, G., 2010, November. Text Generation for Abstractive Summarization. In TAC.

\bibitem{Khan2015Framework}
Khan, A., Salim, N. and Kumar, Y.J., 2015. A framework for multi-document abstractive summarization based on semantic role labelling. Applied Soft Computing, 30, pp.737-747.

\bibitem{Rush2015Neural}
Rush, A.M., Chopra, S. and Weston, J., 2015. A neural attention model for abstractive sentence summarization. arXiv preprint arXiv:1509.00685.


\bibitem{Nayeem2018Abstractive}
Nayeem, M.T., Fuad, T.A. and Chali, Y., 2018, August. Abstractive unsupervised multi-document summarization using paraphrastic sentence fusion. In Proceedings of the 27th International Conference on Computational Linguistics (pp. 1191-1204).

\bibitem{Cao2018Soft}
Cao, Z., Li, W., Li, S. and Wei, F., 2018, July. Retrieve, rerank and rewrite: Soft template based neural summarization. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) (pp. 152-161).

\bibitem{Fattah2009GA}
Fattah, M.A. and Ren, F., 2009. GA, MR, FFNN, PNN and GMM based models for automatic text summarization. Computer Speech \& Language, 23(1), pp.126-144.

\bibitem{Abuobieda2012Text}
Abuobieda, A., Salim, N., Albaham, A.T., Osman, A.H. and Kumar, Y.J., 2012, March. Text summarization features selection method using pseudo genetic-based model. In 2012 International Conference on Information Retrieval \& Knowledge Management (pp. 193-197). IEEE.

\bibitem{Nallapati2017Summarunner}
Nallapati, R., Zhai, F. and Zhou, B., 2017, February. Summarunner: A recurrent neural network based sequence model for extractive summarization of documents. In Thirty-First AAAI Conference on Artificial Intelligence.

\bibitem{Zhan2009Gather}
Zhan, J., Loh, H.T. and Liu, Y., 2009. Gather customer concerns from online product reviews–A text summarization approach. Expert Systems with Applications, 36(2), pp.2107-2115.

\bibitem{Yu2016Product}
Yu, N., Huang, M. and Shi, Y., 2016. Product review summarization by exploiting phrase properties. In Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers (pp. 1113-1124).


\bibitem{Tan2017Sentence}
Tan, J., Kotov, A., Pir Mohammadiani, R. and Huo, Y., 2017, November. Sentence retrieval with sentiment-specific topical anchoring for review summarization. In Proceedings of the 2017 ACM on Conference on Information and Knowledge Management (pp. 2323-2326). ACM.


\bibitem{Amplayo2017Adaptable}
Amplayo, R.K. and Song, M., 2017. An adaptable fine-grained sentiment analysis for summarization of multiple short online reviews. Data \& Knowledge Engineering, 110, pp.54-67.

\bibitem{Hu2006Opinion}
Hu, M. and Liu, B., 2006, July. Opinion extraction and summarization on the web. In AAAI (Vol. 7, pp. 1621-1624).


\bibitem{Ganesan2010Opiniosis}
Ganesan, K., Zhai, C. and Han, J., 2010, August. Opinosis: A graph based approach to abstractive summarization of highly redundant opinions. In Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010) (pp. 340-348).


\bibitem{Hu2017Opinion}
Hu, Y.H., Chen, Y.L. and Chou, H.L., 2017. Opinion mining from online hotel reviews–a text summarization approach. Information Processing \& Management, 53(2), pp.436-449.


\bibitem{Porter1980}
Porter, M.F., 1980. An algorithm for suffix stripping. Program, 14(3), pp.130-137.

\bibitem{Nothman2018Stop}
Nothman, J., Qin, H. and Yurchak, R., 2018, July. Stop Word Lists in Free Open-source Software Packages. In Proceedings of Workshop for NLP Open Source Software (NLP-OSS) (pp. 7-12).

\bibitem{Hoffman2010Online}
Hoffman, M., Bach, F.R. and Blei, D.M., 2010. Online learning for latent dirichlet allocation. In advances in neural information processing systems (pp. 856-864).

\bibitem{Hutto2014Vader}
Hutto, C.J. and Gilbert, E., 2014, May. Vader: A parsimonious rule-based model for sentiment analysis of social media text. In Eighth international AAAI conference on weblogs and social media.

\bibitem{Chan1998Toward}
Chan, P.K. and Stolfo, S.J., 1998, August. Toward Scalable Learning with Non-Uniform Class and Cost Distributions: A Case Study in Credit Card Fraud Detection. In KDD (Vol. 1998, pp. 164-168).

\bibitem{He2016Ups}
He, R. and McAuley, J., 2016, April. Ups and downs: Modeling the visual evolution of fashion trends with one-class collaborative filtering. In proceedings of the 25th international conference on world wide web (pp. 507-517). International World Wide Web Conferences Steering Committee.

\bibitem{Lin2004Rouge}
Lin, C.Y., 2004. Rouge: A package for automatic evaluation of summaries. In Text summarization branches out (pp. 74-81).

\bibitem{Mihalcea2004Textrank}
Mihalcea, R. and Tarau, P., 2004. Textrank: Bringing order into text. In Proceedings of the 2004 conference on empirical methods in natural language processing (pp. 404-411).

\bibitem{Muhammad2016biclique}
Muhammad, A.S., Damaschke, P. and Mogren, O., 2016. Summarizing online user reviews using bicliques. In International Conference on Current Trends in Theory and Practice of Informatics (pp. 569-579). 

\bibitem{Banerjee2015ILPsumm}
Banerjee, S., Mitra, P. and Sugiyama, K., 2015. Multi-document abstractive summarization using ilp based multi-sentence compression. In Twenty-Fourth International Joint Conference on Artificial Intelligence.

\end{thebibliography}

\end{document}