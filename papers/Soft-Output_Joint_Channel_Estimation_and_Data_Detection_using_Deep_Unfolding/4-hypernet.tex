% !TEX root = 21ITW-JEDI.tex
% DO NOT REMOVE THE ABOVE COMMENT!
\begin{figure*}[tp]
\centering
\includegraphics[width=0.99\textwidth]{figs/unfolded.pdf}
\caption{ 
%
Block diagram of the deep unfolded soft-output JED and the hyper-network.
%
The unfolded algorithm consists of $T_\text{max}$ layers. 
Each layer takes in soft-symbols from the preceding layer and outputs new soft-symbols; the last layer outputs only LLR values.
%
The hyper-network takes in the LS channel estimate $\widehat\bH^{\text{LS}}$ and noise variance $\No$ in order to produce the parameters  step sizes $\tau^{(t)}$, regularization parameters $\lambda^{(t)}$, and normalized error variances $\eta^{(t)}_{u}$. }
\label{fig:overall}
\vspace{-0.2cm}
\end{figure*}
%
\section{Deep Unfolding with a Hyper-Network}\label{sec:hypernetwork}
We now explain our deep unfolding strategy for the soft-output JED algorithm and how to train the algorithm parameters. 
%
Due to space constraints, we focus on QPSK only---the general case will be presented in~\cite{songSJEDfuture}.

\subsection{Deep-Unfolding Architecture}\label{sec:unfolding algorithm}
%
In order to determine the algorithm parameters, we use an emerging paradigm known as deep unfolding \cite{hersheyDeep2014,balatsoukas-stimming19a,mongaAlgorithm2021} which we combine with a hyper-network that provides these parameters based on estimated CSI \cite{goutayDeep2020}.
%
The idea of deep unfolding is to unfold an iterative algorithm into $T_\text{max}$ layers (one for every iteration) and use tools of deep learning to determine an optimal set of the algorithm's parameters in every iteration (layer) $t=1,\ldots,T_\text{max}$. Instead of hard-coding these parameters after training, we train a hyper-network that generates these algorithm parameters dependent on CSI. 
 
The hyper-network, the unfolded algorithm (which consists of $T_\text{max}$~layers, each representing a JED iteration), and the parameters are shown in \fref{fig:overall}.
%
At layer~$t$, the input $\bS^{(t)}$ is first updated by a gradient descent step in~\fref{eq:gradientstep} to obtain the symbol estimates~$\bX^{(t)}$.
%
Then, the three-step procedure to approximate the PME as described in \fref{sec:approxsoftoutputtrick} is performed to obtain the next iterate $\bS^{(t+1)}$ as in~\fref{eq:approximate PME 1} and \fref{eq:approximate PME 2}.
%
We note that the last layer $t=T_\text{max}$ only requires the LLR outputs in \fref{eq:LLR1}.

Our unfolded architecture requires several algorithm parameters, which are generated by a hyper-network. 
%
Specifically, for each iteration (layer) $t=1,\ldots,T_\text{max}$, we require the per-iteration step size~$\tau^{(t)}$ and the estimation error variances $\nu_{u,k}^{(t)}$ for $u=1,\ldots,U$ and $k=1,\ldots,K$.
%
To reduce the amount of parameters per iteration, we assume that the estimation error variances are fixed with respect to the time slot $k$, i.e., we only require $\nu_{u}^{(t)}$ and use the same variance for all time slots. 
%
We note that the hyper-network does not generate~$\nu_{u}^{(t)}$, but rather a normalized version $\eta_{u}^{(t)} = {\No}/{\nu_{u}^{(t)}}$ to account for large variations in $\No$. 
%
We also require the parameter~$\lambda$ in~\fref{eq:joint ML with H prior}; instead of using the same parameter $\lambda$ for all iterations $t=1,\ldots,T_\text{max}$, each layer uses a different parameter $\lambda^{(t)}$. 

The inputs to the hyper-network are the vectorized least-squared channel estimate $\widehat\bH^{\text{LS}} = \bY_T\bS_T^{-1}$ of the pilot phase ($\bY_T$ contains the first $T$ columns of the matrix $\bY$) and the  noise variance~$\No$. 
%
The hyper-network itself consists of five dense layers with rectified linear unit (ReLU) activations in each layer except for the last one, which uses an absolute value activation to generate non-negative parameters. 


\subsection{Hyper-Network Training}\label{sec:hypernetwork training}
%
In order to train the hyper-network, we leverage the soft-output capabilities of our algorithm.
%
Specifically, since our JED algorithm computes probabilities for the transmitted bits~\fref{eq:probabilities}, we can train the hyper-network using the outputs in the last iteration $t=T_\text{max}$ using the widely-used binary cross entropy (BCE) loss, which is defined as follows: 
\begin{align}\label{eq:bce canonical}
 H(b_i,p(b_i)) =  b_i \log(p(b_i)) + (1-b_i)\log(1-p(b_i)).
\end{align}
Here, $b_i\in\{0,1\}$ is the label of the $i$th bit and $p(b_i)$ is the predicted probability of this bit being $1$.
%
In our case, we utilize the probabilities $P_{b',u,k}^{(T_\text{max})}$ in \fref{eq:probabilities} for every transmitted bit $b_{b',u,k}$, where $b'=1,2$ is the bit index, $u=1,\ldots,U$ the UE index, and  $k=1,\ldots,K$ the time slot index, calculated in the last iteration $t=T_\text{max}$. 
%
Hence, we define the following average BCE loss over all of these probabilities
%
\begin{align}\label{eq:loss function}
L = \frac{1}{2UK}\sum_{b'=1}^{2}\sum_{u=1}^{U}\sum_{k=1}^{K} H\!\left(b_{b',u,k},P_{b',u,k}^{(T_\text{max})}\right)\!,
\end{align}
which we use to train the hyper-network parameters. We learn only a single hyper-network for all signal-to-noise-ratio (SNR) values, which is in stark contrast to the common approach of using a different hyper-network for every SNR. 
 