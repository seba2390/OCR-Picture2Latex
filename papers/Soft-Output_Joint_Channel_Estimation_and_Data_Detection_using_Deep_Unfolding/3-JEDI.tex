% !TEX root = 21ITW-JEDI.tex
% DO NOT REMOVE THE ABOVE COMMENT!
\section{S-JED: Soft-Output Joint Channel \\ Estimation and Data Detection}\label{sec:JED}


%
The combinatorial nature of solving \fref{eq:opt1} exactly would quickly result in prohibitive complexity  which necessitates approximate algorithms. 
%
Furthermore, even solving \fref{eq:opt1}  approximately would lead to a hard-output JED method that is unable to realize the full potential of coded data transmission. 
%
We now derive a computationally efficient algorithm to approximately solve \fref{eq:opt1} while being able to compute soft-output information. 
%
\subsection{Smoothening the MAP-JED Problem}\label{sec:convex relaxation}
The discrete nature of the constellation $\setQ$ is the culprit of preventing gradient-descent-like methods (and deep unfolding) to approximately solve \fref{eq:opt1}. We therefore propose to first relax the set  $\setQ$ to its convex hull, which is defined as \cite{castaneda2017vlsi}
\begin{align}
\setC = \textstyle \left\{\sum_{i=1}^{|\setQ|}\alpha_i s_i \mid (\alpha_i\in\reals_+,\forall i) \wedge \sum_{i=1}^{|\setQ|}\alpha_i =1 )\right\}\!.
\end{align}
Here,  $s_i$  is the $ i $th symbol in the constellation $ \setQ $. We can now replace the discrete constellation $\setQ$ in~\fref{eq:opt1} by the convex set $\setC$, which leads to the smoothened optimization problem 
%
\begin{align}\label{eq:opt2}
\widehat{\bS}^\text{sm} =  \argmax\limits_{\bS\in\setC^{U\times K}} \, \Tr\!\left[\bY^H\bY\bS^H\bM^{-1}\bS\right]\!.
\end{align}
%
The resulting smoothened optimization problem has a differentiable objective and a convex constraint, which permits the use of gradient-based methods and deep unfolding. 



\subsection{Smoothened MAP-JED via Forward-Backward Splitting}
%
We now show how to use forward-backward splitting (FBS)~\cite{goldsteinField2016} to approximately solve \fref{eq:opt2}.  
%
FBS iteratively solves convex optimization problems of the form  
\begin{align} \label{eq:fbsproblem}
\hat\bms = \argmin_{\bms} f(\bms)+g(\bms),
\end{align}
where the function $f$ is differentiable and convex, and $ g $ is convex but not necessarily smooth or bounded.
%
By initializing FBS with $\bms^{(1)}$,  it solves the problem in~\fref{eq:fbsproblem} for the iterations $ t=1,2,\ldots $ until convergence by computing  
%
\begin{align}\label{eq:fbs alg}
\bms^{(t+1)} = \text{prox}_g(\bms^{ (t) }- \tau^{(t)} \nabla f( \bms^{ (t) } );\tau^{(t)} ),
\end{align}
where $ \nabla f(\bms) $ is the gradient of $ f(\bms) $ and $ \tau^{(t)}$ is a carefully-chosen step size at iteration $t$.
%
The proximal operator for~$ g(\bms) $ is defined as follows \cite{parikh2014proximal}:
\begin{align}\label{eq:proximal def}
\text{prox}_g(\bms;\tau) = \argmin_\bmz \, \tau g(\bmz) + \textstyle \frac{1}{2}\normtwo{\bmz-\bms}^2.
\end{align}
%
While FBS is able to exactly solve convex optimization problems, it can be used to approximately solve many non-convex problems~\cite{goldsteinField2016}, which will be described next. 



For our MAP-JED problem, we define $f$ and $g$ in \fref{eq:fbsproblem} as 
\begin{align}
	f(\bS) = \Tr\!\left[\bY^H\bY\bS^H \bM^{-1}\bS\right] \quad \text{and} \quad 
	g(\bS)=  \chi_{\setC}(\bS),
\end{align}
where the indicator function $\chi_{\setC}(\bS)$
implements the convex constraint $\bS\in\setC^{U\times K}$ in \fref{eq:opt2}, 
and is zero if $\bS\in\setC$ and infinity otherwise. 
%
The gradient of the function $f$ in $\bS$ is given by  
%
\begin{align}\label{eq:nabla f}
	\nabla f(\bS) &= \bM^{-1}\bS\bY^H\bY (\bI_{U}-\bS^H\bM^{-1}\bS).
\end{align}
%
Due to space constraints, the derivation of the gradient will be shown in the future journal version of the paper \cite{songSJEDfuture}.
%
The proximal operator in \fref{eq:proximal def} for $\bS$ is given by 
\begin{align}
&\text{prox}_{g}(\bS;\tau^{(t)})  
%
= \argmin_{\bX\in\setC^{U\times K}} \textstyle \frac{1}{2}\normfro{\bX-\bS}^2, \label{eq: NC proximal S}
\end{align}
where we move the indicator function in $g(\bS)$ back to the constraint. 
%
This problem has closed-form expressions for most constellation sets, e.g., for QPSK the projection operator is 
%
\begin{align}\label{eq:inf proximal}
\text{proj} _\setC(\Re\{S_{i,j}\}) &= \min\{\max\{\abs{\Re\{S_{i,j}\}},-\alpha\},\alpha\}\\
\text{proj} _\setC(\Im\{S_{i,j}\}) &= \min\{\max\{\abs{\Im\{S_{i,j}\}},-\alpha\},\alpha\},
\end{align}
where $ \alpha =\frac{1}{\sqrt{2}}$ for the set $\setQ=\big\{\pm\frac{1}{\sqrt{2}}\pm \frac{1}{\sqrt{2}}j\big\}$.

This FBS algorithm efficiently (and approximately) solves the smoothened MAP-JED problem in~\fref{eq:opt2}, but 
ignores the discrete nature of the constellation set and is unable to compute soft-outputs. Sections~\ref{sec:posterior mean} and \ref{sec:approxsoftoutputtrick} address these issues. 

\subsection{Exploiting the Constellation with the PME}\label{sec:posterior mean}
%
In order to exploit the constellation set $\setQ$, we model the iterations of FBS after evaluating the gradient step  
\begin{align} \label{eq:gradientstep}
\bX^{(t)} = \bS^{ (t) }- \tau^{(t)} \nabla f( \bS^{ (t) })
\end{align}
as follows: 
\begin{align}\label{eq:post equalized additive model}
\bX^{(t)}= \bS + \bE^{(t)}.
\end{align}
Here, $\bS$ is the (unknown) true transmitted data matrix and~$\bE^{(t)}$ models estimation errors on these per-iteration estimates. By assuming that the distribution of~$\bE^{(t)}$ is known, one can replace the projection onto the convex hull $\setC$ in \fref{eq: NC proximal S} by the entry-wise posterior mean estimate (PME)
\begin{align} \label{eq:PME}
S_{u,k}^{(t+1)} = \Exop[ S_{u,k}| X_{u,k}^{(t)}]
\end{align}
with $u=1,\ldots,U$ and $k=1,\ldots,K$. We emphasize that the PME depends on the prior distribution (which is given by the constellation set $\setQ$) and the statistics of $\bE^{(t)}$. 
%
We have the following prior distribution on the transmitted data:
\begin{align}
p(S_{u,k}) = \frac{1}{|\setQ|}\sum_{i=1}^{|\setQ|} \delta(S_{u,k}-s_i).
\end{align}
Here, $s_i$ is the $i$th symbol in the constellation $\setQ$ and $\delta$ is the Dirac delta function. 
%
By assuming that the estimation errors~$E^{(t)}_{u,k}$ are circularly-symmetric complex Gaussian with variance $\nu_{u,k}^{(t)}$, the PME in \fref{eq:PME} has a closed form (see, e.g.,~\cite{jeonOptimality2015}) and one can replace the proximal operator in \fref{eq: NC proximal S} with the PME in \fref{eq:PME}.
%
This step requires knowledge of the per-iteration estimation error variances $\nu_{u,k}^{(t)}$, $\forall u,k$, which are difficult to obtain in practice. However, as shown in \fref{sec:hypernetwork}, we can use a hyper-network to determine these variances. 


\subsection{Approximate PME and Soft-Output Generation}
\label{sec:approxsoftoutputtrick}
%
In order to extract soft-outputs, we build upon the approximate PME put forward in \cite{jeonMismatched2020,jeon3542019}. 
%
The key idea is to replace~\fref{eq:PME} by an approximate three-step procedure that (i) converts the per-iteration estimates in \fref{eq:gradientstep} into soft-outputs in the form of log-likelihood ratios (LLRs) for every transmitted bit, (ii) transforms these LLR values into probabilities, and (iii) converts these probabilities back into the soft-symbol estimates. 
%
We now summarize this approach for QPSK modulation. 

\subsubsection*{Step (i)}
%
Assume that the per-iteration estimation errors are circularly-symmetric complex Gaussian with variances $\nu_{u,k}^{(t)}$, $\forall u,k$. 
%
Then, the LLRs for the two bits that map to QPSK symbols are given by \cite[Tbl. 4.3]{fateh2009vlsi} 
%
\begin{align}\label{eq:LLR1}
	L_{1,u,k}^{(t)} = \frac{4\Re \{X_{u,k}^{(t)}\}}{\nu_{u,k}^{(t)}} \quad \text{and} \quad L_{2,u,k}^{(t)} = \frac{4\Im \{X_{u,k}^{(t)}\}}{\nu_{u,k}^{(t)}}.  
\end{align}

\subsubsection*{Step (ii)}
%
We convert the LLRs in~\fref{eq:LLR1} into probabilities as follows \cite[Eq.~3.6]{fateh2009vlsi}:
\begin{align} \label{eq:probabilities}
P_{b,u,k}^{(t)} & = \frac{1}{2}\!\left(1+\tanh\!\left(\frac{L_{b,u,k}^{(t)}}{2}\right)\!\right)\!, \quad b\in\{1,2\},
\end{align}
which express the probabilities of the $b$th bit that map to the symbol~$S_{u,k}$ (e.g., using Gray mapping) being $1$.

\subsubsection*{Step (iii)}
%
We use the probabilities in \fref{eq:probabilities} to compute symbol estimates as follows \cite[App.~A.4]{fateh2009vlsi}:
%
\begin{align}
\Re\{S_{u,k}^{(t+1)}\} &= {\frac{1}{\sqrt{2}}}(2P_{1,u,k}^{(t)}-1) \label{eq:approximate PME 1} \\
\Im\{S_{u,k}^{(t+1)}\} &= {\frac{1}{\sqrt{2}}}(2P_{2,u,k}^{(t)}-1)\label{eq:approximate PME 2},
\end{align}
where $S_{u,k}^{(t+1)}$ approximates the PME output in \fref{eq:PME}.

We emphasize that we are using this three-step procedure instead of the PME in \fref{eq:PME} for three reasons: 
%
First, in Step~(i) we calculate LLR values for every transmitted bit in each iteration, which provides our algorithm with soft-output data detection capabilities. 
%
Second, in Step~(ii) we obtain probabilities for every transmitted bit in each iteration, which will be key in learning the parameters of our soft-output JED algorithm (see \fref{sec:hypernetwork training}). 
%
Third, this procedure was shown in~\cite{jeon3542019,jeonMismatched2020} to be less complex and numerically more stable than evaluating the exact PME in \fref{eq:PME}.