\section{Introduction}

The millions of academic papers in the biomedical domain contain a vast amount of information that may lead to new hypotheses for disease treatment.  However, scientists are faced with a problem of ``undiscovered public knowledge,'' as they struggle to read and assimilate all of this information~\cite{swanson1986undiscovered}.  Furthermore, the literature is growing at an exponential rate~\citep{pautasso2012}; PubMed\footnote{\url{http://www.ncbi.nlm.nih.gov/pubmed}} has been adding more than a million papers per year since 2011. % , with 1.256 million new papers added just in 2016\footnote{\pubmedquery}.  % ms: removed for space
We have surpassed our ability to keep up with and integrate these findings through manual reading alone.

Large ongoing efforts, such as the BioNLP task community \cite{nedellec2013overview,kim2012genia,kim2009overview} and the DARPA Big Mechanism Program \cite{cohen2015}, are making progress in advancing methods for machine reading and assembly of extracted biochemical interactions into large-scale models.  However, to date, these methods rely either on the manual selection of relevant documents, or on the processing of large batches of documents that may or may not be relevant to the model being constructed.  

Batch machine reading of literature at this scale poses a new, growing set of problems.  
First, access to some documents is costly.  The PubMedCentral (PMC) Open Access Subset\footnote{\url{https://www.ncbi.nlm.nih.gov/pmc/tools/openftlist/}} (OA) is estimated\footnote{\url{https://tinyurl.com/bachman-oa}} to comprise 20\%\footnote{This includes 5\% from PMC author manuscripts.} of the total literature; the remaining full-text documents are only available through paid access.   
Second, while there have been great advances in quality, machine reading is still not solved.  Updates to our readers requires reprocessing the documents.  For large document corpora, this quickly becomes the chief bottleneck in information extraction for model construction and analysis.
%Finally and more generally, beyond biology these problems are exacerbated by heterogeneous document repositories with 
Finally, even if we could cache all reading results, the search for connections between concepts within the extracted results should not be done blindly.  At least in the biology domain, the many connections between biological entities and processes leads to a very high branching factor, making blind search for paths intractable.

To effectively read at this scale, we need to incorporate methods for {\em focused reading}: develop the ability to pose queries about concepts of interest and perform targeted, incremental search through the literature for connections between concepts while minimizing reading documents that are likely irrelevant.

In this paper we present what we believe is the first algorithm for focused reading.  We make the following contributions:\\
{\noindent {\bf (1)}} Present a general framework for a family of possible focused reading algorithms along with a baseline instance.\\
{\noindent {\bf (2)}} Cast the design of focused reading algorithms in a reinforcement learning (RL) setting, where the machine decides if it should explore (i.e., cast a wider net) or exploit (i.e., focus reading on a specific topic).\\
{\noindent {\bf (3)}} Evaluate our focused reading policies in terms of search efficiency and quality of information extracted. The evaluation demonstrates the effectiveness of the RL method: this approach found more information than the strong baseline we propose, while reading fewer documents. 
% \flushleft