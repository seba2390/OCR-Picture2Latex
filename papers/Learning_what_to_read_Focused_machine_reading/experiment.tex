\section{Baseline Algorithm and Evaluation}\label{sec:dataset}

\begin{table*}[t!]
  \begin{center}
  {\footnotesize
  \begin{tabular}{r|c|cc}
  	& \textbf{Baseline} & \textbf{RL Query Policy} & \\
  	\hline
    \textbf{\# IR queries} &  573 & 433 & 25\% decrease\\
    \textbf{Unique papers read} & 26,197 & 19,883 & 24\% decrease\\
    \textbf{\# Paths recovered} (out of 289) & 189 (65\%) & 198 (68\%) & 3\% increase
  \end{tabular}
  \caption{Results of the baseline and RL Query Policy for the focused reading of biomedical literature.}
  \vspace{-6mm}
  \label{table:reading-results}
  }
  \end{center}
\end{table*}

The main functions that affect the search behavior of Algorithm~\ref{alg:focusedreading} are {\sc EndpointStrategy} and {\sc ChooseQuery}.
Here we describe a {\em baseline} focused reading implementation in which {\sc EndpointStrategy} and {\sc ChooseQuery} aim to find any path between $S$ and $D$ as quickly as possible.

For {\sc EndpointStrategy}, we follow the intuition that some participants in a biological graph tend to be connected to more participants than others, and therefore more likely to yield interactions providing paths between participants in general.  Our heuristic is therefore to choose new participants to query that currently have the most inward and outgoing edges (i.e., highest vertex degree) in the current state of $G$ (disallowing choosing an entity pair used in a previous query).

Now that we have our candidate participants $(A,B)$, our next step is to formulate how we will use these participants to retrieve new papers.  Here we consider two classes of query: (1) we restrict our query to only retrieve papers that simultaneously mention both $A$ and $B$, therefore more likely retrieving a paper with a direct link between $A$ and $B$ ({\em exploit}), or (2) we retrieve papers that mention either $A$ or $B$, therefore generally retrieving more papers that will introduce more new participants ({\em explore}).  For our baseline, where we are trying to find a path between $S$ and $D$ as quickly as possible, we implement a greedy {\sc ChooseQuery}: first try the conjunctive exploitation query; if no documents are retrieved, then ``relax'' the search to the disjunctive exploration query.

% Dataset

% ms: please use "entity" instead of "participant". The latter is ambiguous: participant in what?
% ms: also, I changed Baseline to baseline. No need for capitalization...

To evaluate the baseline, we constructed a data set based on a collection of papers seeded by a set of 132 entities that come from the University of Pittsburgh DyCE\footnote{{\bf Dy}namic {\bf C}ell {\bf E}nvironment model of pancreatic cancer.} model, a biomolecular model of pancreatic cancer~\cite{dyce-inprep}.  Using these entities, we retrieved 70,719 papers that mention them.  We processed all papers using REACH, extracting all of the interactions mentioned, and converted them into a single graph.
% ms: redundant
 %by connecting any matching participants (that is, if an interaction links participant A to B, and another B to C, then we construct a chain linking A to B to C).  
The resulting graph consisted of approximately 80,000 vertices, 115,000 edges, and had an average (undirected) vertex degree of 24.  We will refer to this graph as the {\em REACH graph}, as it represents what {\em can} be retrieved by REACH from the set of 70K papers.  Next, we identified which pairs of the original 132 entities are connected by directed paths in DyCE.  A total of 789 pairs were found.  We used 289 of these entity pairs as testing queries (i.e., generating queries that aim to explain how a given pair is connected according to the literature). The other 500 pairs were held out to train the RL method described below.

% Baseline results
We ran this baseline focused reading algorithm on each of the 289 pairs of participants, in each case attempting to recover a directed path from one to the other.  The results are summarized in the middle column of Table~\ref{table:reading-results}.  By issuing 573 queries, the baseline read 26,197 papers out of the total 70,719 papers (37\% of the corpus), in order to recover 189 of the 289 paths (65\%).

\section{Reinforcement Learning for Focussed Reading}\label{sec:rl}

We analyzed the baseline's behavior in the evaluation to identify the conditions under which it failed to find paths.  From this, we found that some of the failures could be avoided had we used a different strategy for {\sc ChooseQuery}, i.e., the baseline chose to exploit when it should have explored more.  The conditions for making different choices depend on the current state of $G$, and earlier query behavior can affect later query opportunities, making this an iterative decision making problem and a natural fit for a RL formulation.

Inspired by this observation, we consider RL for finding a better policy for {\sc ChooseQuery}.  We'll refer to an instance of the focused reading algorithm with a learned {\sc ChooseQuery} policy as the {\em RL Query Policy}.  All other focus reading functionality is the same as in the baseline.  
For actions, we consider a simple binary action choice: exploit (conjunctive query) or explore (disjunctive query).
We represent the state of the search using a set of features that include: 
(f1) the current iteration of the search; 
(f2) the number of times a participant has been used in previous queries;
%(repeating participant pairs in any query is not allowed, but a participant may be used multiple times with different participants) 
(f3) whether the participants are chosen from the same connected component in $G$;
(f4) the vertex degree of participants; 
and (f5) the search iteration in which a participant was introduced.
% The reward function plays the key role of determining what we are trying to optimize.  
With the goal of recovering paths as quickly as possible, we provide a reward of $+1$ if the algorithm successfully finds a path, a reward of $-1$ if the search fails to find a path, and assess a ``living reward'' of $-0.05$ for each step during the search, to encourage trying to finish the search as quickly as possible.

% ms: moved these 2 tables from analysis.tex to make sure they appear on pg 5
\begin{table*}[t!]
\begin{footnotesize}
\begin{center}
  \begin{tabular}{r|cccccc}
    & All & $-$ Iteration & $-$ Query& $-$ Same & $-$ Ranks (f4)& $-$ Particip.\\
    & features & number (f1) & counts (f2)& component (f3)& & intro. (f5)\\
    \hline
    \emph{Paths found}& 198&199 & 200 & 201 & \textbf{202} & 196\\
    \emph{Papers read}& 19,883&20,918 & 20,531 & 20,463 & 27,708 & \textbf{17,936}\\
    \emph{Queries made}& 433&484&484 & 467 & 469 & \textbf{403}
  \end{tabular}
  \end{center}
  \end{footnotesize}
  \vspace{-2mm}
  \caption{Ablation test on the features used to represent the RL state.}
  \vspace{-4mm}
  \label{table:ablation}
\end{table*}


  \begin{table}[t!]
  \begin{small}
  \begin{center}
  \begin{tabular}{l|ccc}
    &Empty query&Ungrounded&Low yield\\
    & results & participant(s) & from IE \\
    \hline
    \emph{Error cause}& 12&4&2
  \end{tabular}
  \end{center}
  \end{small}
  \vspace{-2mm}
  \caption{Error analysis on 18 queries that failed under the RL algorithm.}
  \vspace{-4mm}
  \label{table:policyerroranalysys}
\end{table}


We trained the RL Query Policy using the SARSA~\cite{sutton1998reinforcement} RL algorithm.  As the number of unique states is large, we used a linear approximation of the q-function.  Once the policy converged during training, we then fixed the linear estimate of the q-function and used this as a fixed policy for selecting queries.  
We trained the RL Query Policy on the separate set of 500 entity pairs, and 
evaluated it on the same data set of 289 participant pairs used to evaluate the baseline.
Table~\ref{table:reading-results} summaries the results of both the baseline and the RL Query Policy.  
The Query Policy resulted in a 25\% decrease in the number of queries that were run, leading to a 24\% drop in the number of papers that were read, while at the same time {\em increasing} the number of paths recovered by 3\%.
We tested the statistical significance of the difference in results between the baseline and RL policy by performing a bootstrap resampling test. Our hypotheses were that the policy reads fewer papers, makes fewer queries and finds more paths. The resulting estimated $p$-values for fewer papers and fewer queries was found to be near 0, and $< 0.003$ for finding more paths.
% ms: added to summarize ablation test
An ablation study of the state features found that features (f2) and (f5) had the largest impact on number of papers read; both model the history of the reading task (see the next section for details).  This highlights that the RL model is indeed learning to model the entire iterative process.

% An ablation test of the features included in the state highlighted that the features with the largest impact on the number of papers read are features (f2) and (f5), both of which model the history of the reading task. This highlights that the RL model is indeed learning to model the entire iterative process.\footnote{This analysis will be expanded in the camera ready, if accepted.}

