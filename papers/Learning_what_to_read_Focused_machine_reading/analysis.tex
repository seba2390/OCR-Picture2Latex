

\section{Analysis}\label{sec:analysis}
\vspace{-2mm}
\paragraph{Feature Ablation Test:}

We performed an ablation test on the features that encode  the RL state. The results are summarized in Table \ref{table:ablation}. Similar to Section 5, we grouped the features into five different groups, and we measured the impact of removing one feature group at a time. Overall, the amount of paths found doesn't have a significant amount of variance, but the efficiency of the search (amount of papers read and number of queries made)  depends on several feature groups. 
For example, features (f1), (f2), and (f4) have a large effect on both the number of papers read and the number of queries made. 
Removing the feature (f5) actually reduces the number of papers read by approximately 2K with a minimal reduction in the number of paths found, which suggests that this task could benefit from feature selection. 

%The \emph{same component feature} has a strong impact in the number of queries made. {Iteration \#} does too as well as in the number of papers read. On the other hand, the \emph{rank} features had impact on the number of paths found. Using all the features achieves a good balance across the three metrics, but feature selection can be used to bias the policy towards optimizing a particular criteria of interest.


\paragraph{RL Policy Error Analysis:}

Lastly, we analyzed the execution trace of eighteen (20\% of the errors) of the searches that failed to find a path under RL. 
The results are summarized in Table \ref{table:policyerroranalysys}. 
The table shows that the main source of failures is receiving \emph{no results} from the information retrieval query, i.e., when the IR system returns zero documents for the chosen query. This is typically caused by over-constrained queries.
% index doesn't return any document for the query, either because the information doesn't exist in it or the query was to restrictive on its constraints. This is a problem that is independent of the RL framework, which could be addressed by a more careful craft of the queries, thus, we consider it to  be  outside of the scope of the article. % ms: well, designing a good query is actually part of RL, so it is within scope...
The second most common source of failures was {\em ungrounded participants}, i.e., when at least one of the selected participants that form the query could not be linked to our protein knowledge base. This is generally caused by mistakes in our NER sequence model, and also tends to yield no results from the IR component.
% don't come from a dictionary of entities, but where picked up by a CRF tagger and they don't have a grounding id available to be used in the information retrieval step. 
%This either results in an empty query or in a query that returns documents which have already been read before, which essentially stops the search process, because the search graph doesn't change from one iteration to the other, which is one of the stop conditions. 
% They also result on an empty query, but this is not solvable by modifying the query, hence is counted as a different type of problem. 
Finally, the \emph{low yield from IE} situation appears when the the information produced through machine reading in one iteration is scarce and adds no new components to the interaction graph, again resulting in a stop condition.
% ms: if you're not saying something informative, remove it...
% All of these problems could be addressed differently and the authors are looking into solutions for future work.