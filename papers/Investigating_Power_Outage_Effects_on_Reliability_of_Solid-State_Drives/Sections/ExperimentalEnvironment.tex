\vspace{-1em}
\section{Experimental Results}
\vspace{-0.6em}
\label{SEC:EXPR}
In order to evaluate the reliability of SSDs, we have conducted comprehensive experiments by employing our proposed test platform. The \emph{Host System} as a part of test platform includes Intel(R) Core(TM) i5 and 8GB DDR3 RAM (froy Hynix Semiconductor) that are connected to a Z97-A motherboard from ASUSTeK computer incorporation. The running operating system on the \emph{Host System} is Ubuntu 17.04 with the kernel version 4.10.0-19-generic.
 The experiments are performed on six SSDs from different vendors. Table \ref{table:SSD-info} provides the detailed information about the characteristics of the SSDs in our experiments.
 
 \begin{table}[]
 	\centering
 	\tiny
 	\vspace{-2em}
 	\caption{Information of employed SSDs in the experiments.}
 	\vspace{-0.5em}
 	\label{table:SSD-info}
 	\begin{tabular}{|c|c|c|c|c|c|c|c|}
 		\hline
 		\begin{tabular}[c]{@{}c@{}}SSD\\ Type\end{tabular} & \begin{tabular}[c]{@{}c@{}}Size\\ (GB)\end{tabular} & Interface & \begin{tabular}[c]{@{}c@{}}Internal\\ Cache?\end{tabular} & ECC?                                                 & \begin{tabular}[c]{@{}c@{}}Bit\\ per\\ Cell\end{tabular} & \begin{tabular}[c]{@{}c@{}}Release\\ Year\end{tabular} & \begin{tabular}[c]{@{}c@{}}Number of SSD\\ in Experiments\end{tabular} \\ \hline\hline
 		A                                                  & 256                                                 & SATA      & Yes                                                       & Yes                                                  & MLC                                                      & 2013                                                   & 2                                                                      \\ \hline
 		B                                                  & 120                                                 & SATA      & Yes                                                       & \begin{tabular}[c]{@{}c@{}}Yes\\ (LDPC)\end{tabular} & TLC                                                      & 2015                                                   & 2                                                                      \\ \hline
 		C                                                  & 120                                                 & SATA      & Yes                                                       & Yes                                                  & MLC                                                      & NA                                                     & 2                                                                      \\ \hline
 	\end{tabular}
 \end{table}
 The experiments are performed to reveal the impact of workload dependant  parameters such as workload WSS, requests size, requests type (read/write), requests access pattern (sequential/random), and sequence of accesses (RAR, RAW, WAR, and WAW). In addition, we have investigated the effect of SSDs internal volatile DRAM cache and the impact of time intervals between the completion of the request and power outage on the failure rate. The results of the experiments reveal three types of failure namely \emph{data failure}, \emph{FWA}, and \emph{IO error}. In Section \ref{sec:failure_detection}, we have elaborated the failure detection mechanism that have been employed in our test platform.
% The results include data failure error, IO not inserted and IO error. Data failure is due to incompatibility between checksum of original data and the written data on SSD caused by power faults. IO error occurs during the absence of SSD power which produces incomplete requests and IO not inserted error shows that the request data is not written on SSD which means it is cached on the SSD cache. %The effects of power outage on the entire SSD as well as the write/read requests is measured too.
%In these tests, the number of power faults is considered 300 times, during 8000 IO requests which is concurrently issued to the under test SSD. Size of requests are considered random between from 4KB to 1MB. 
\vspace{-1em}
\subsection{Impact of Time Interval After Request Completion and Power Outage}
\vspace{-0.7em}
\label{sec:Time_Interval}

In this section, we analyze the impact of time interval between the completion of the request (i.e., when the ACK signal is received in the application layer) and power outage occurrence. The IO requests are submitted to random address (uniform random distribution) with varying size between 4KB and 1MB and the power fault is injected to the SSD in variable time intervals after completion of the request. The experimental results show that the power fault not only may disturb the currently writing data, it may corrupt the previously written data which was finished completely. Conducted experiments on the SSDs (depicted in Table \ref{table:SSD-info}) reveal that on average 700ms after receiving ACK signal of the request in application layer, the power fault can corrupt the corresponding request which was successfully written on the SSD. The reason of such failure can be due to the volatile DRAM cache of the SSD where write pending requests  are kept. We have also performed experiments by disabling the SSD internal cache where the results reveal the similar failures in such conditions.
%Hence, although the requests are completely written to the SSDs, providing the power supply in 700ms after the request completeness is required. This behavior is anonymous in the SSDs which are known as non-volatile memories. So, designers should consider this interval time after the requests completeness to be abe to guarantee the reliability of produced SSDs under power faults.

%\subsection{Data failures on Entire SSD Memory}
%\label{sec:entire_memory}
\vspace{-1em}
\subsection{Impact of Request Type}
\vspace{-0.7em}
\label{sec:req_type}
In this section, we evaluate the vulnerability of read/write requests that are submitted to the SSD under power failures. To this end, we have generated the workloads with random access pattern (uniform random distribution) with varying size between 4KB and 1MB where the type of requests varies from fully write to fully read (i.e., the percentage of write operation is 100\%, 80\%, 50\%, 20\%, and 0\%). In these experiments, more than 300 power faults are injected to the SSD during $24,000$ requests. As depicted in Fig. \ref{read_percentage}, it can be seen that by decreasing the percentage of write requests in the workload, the ratio of \emph{data failure} decreases where in fully read workload, there is no \emph{data failure}, however, the \emph{IO error} is occurred due to disk unavailability during power failure. The workloads with write requests are vulnerable to both \emph{data failure} and \emph{FWA} failures where we detect about two \emph{data failure} per power fault in our experiments.
\begin{figure}[t]
	\centering
	\includegraphics[scale=0.65]{results/read-percentage}
	\vspace{-0.5em}
	\caption{Impact of request type on data failures.}
	\vspace{-2em}
	\label{read_percentage}
\end{figure}

\vspace{-1em}
\subsection{Impact of Workload Working Set Size (WSS)}
\vspace{-0.7em}
\label{sec:disksize}
%The workload working set size is defined as the address space range in which a work will use for its operation. 
In this section, we perform experiments to reveal the impact of power fault on the ratio of data failures in different workloads with different WSS.
To this end, we have generated the workloads with varying WSS from 1GB up to 90GB. The size of requests is considered random between 4KB and 1MB and the requests are submitted to the SSD with uniform random access pattern. In these experiments, we have injected more than 200 power faults to the SSD during $16,000$ requests. Fig. \ref{disk_size} demonstrates that the workloads WSS has no significant impact on the ratio of data failures. Instead, the data failure is significantly affected by the access patterns of the request (i.e., locality of IO requests) rather than the workload WSS which is investigated in the next section.

\begin{figure}[t]
	\centering
	\includegraphics[scale=0.65]{results/disk-size}
	\vspace{-0.5em}
	\caption{Impact of workload working set size on data failure.}
	\vspace{-2em}
	\label{disk_size}
\end{figure}

\vspace{-1em}
\subsection{Impact of Requests Access Pattern (Random/Sequential)}
\vspace{-0.7em}
\label{sec:pattern}
In this section, we investigate the impact of requests access pattern on the failure rate. The experiments of this section are done by submitting the IO requests of two different workloads where the first one is consist of fully uniform random write operations while the second one includes fully sequential write operations. The request size in both workloads varies between 4KB and 1MB and the WSS of both workloads is considered to be equal to 64GB.
In these experiments, SSDs experience more than 300 power faults during $24,000$ requests. Note that the experiments of this section include two independent experiments where the running workload on the platform is different.
It is important to note that in the workloads with sequential access pattern, FTL only keeps the first address in the mapping table where such scheme reduces the amount of table entries but on the other hand may have significant impact on the failure rate due to power loss (particularly in case of map table failure which is kept in the volatile DRAM cache). The results of the experiments prove such claim and reveal that the ratio of \emph{data failure} in a workload with sequential access pattern is about 14\% more than the workload with random access pattern. 
%This is because writting to sequential physcial addresses during power faults will increase the data failure probability due to the capacitive coupling between neighboring cells. 

\vspace{-1em}
\subsection{Impact of Request Size}
\vspace{-0.7em}
\label{sec:req_size}

In this section, we investigate the impact of request size on the failure rate under power faults. To this end, we submit the requests of the workload which includes the write requests with uniform random access pattern to the SSD. The request size of the workloads is constant in each experiment where it varies between 4KB and 1MB in different experiments.
In these experiments, the SSD experiences more than 800 power failure where the number of requests is more than $64,000$.
As shown in Fig. \ref{req_size}, the ratio of data failure in the workload with smaller request size (e.g., 4KB) is significantly more than the workload with larger request size. The main reason of such behavior is due to the different number of committed request per time. In a equal time interval, the number of requests with smaller size is significantly larger than the requests with larger size. Therefore, occurring a power failure can affect larger number of requests in the workloads with smaller request sizes. It can be seen from Fig. \ref{req_size} that most of the failures in the workload with 4KB request size is from \emph{FWA} type where the ACK is received in the application through the SSD but the data is not written. This may be due to the impact of power failure on volatile DRAM cache inside of the SSD while the experiments on the SSDs with disabled cache suffer from \emph{FWA} failure.
\begin{figure}[t]
	\centering
	\includegraphics[scale=0.65]{results/request-size}
	\vspace{-0.5em}
	\caption{Impact of request size on data failure.}
	\vspace{-2em}
	\label{req_size}
\end{figure}

\vspace{-1em}
\subsection{Impact of Requested IOPS Submitted to the SSD}
\vspace{-0.7em}
\label{sec:req_rate}

Here we evaluate the impact of requested \emph{Input/Output Per Second} (IOPS) (i.e., the number of operations that are submitted to the SSD in one second) by the workload on the failure ratio under power faults.
To this end, we have generated various workloads with different requested IOPS. The workloads include uniform random write requests where the requests size varies between 4KB to 1MB. In each experiment, more than 600 power faults are injected to the SSDs. Fig. \ref{req_rate} depicts the responded IOPS of the SSD and also the number of failures in each experiment based on requested IOPS for each workload. It can be seen that by increasing the ratio of the requested IOPS, the responded IOPS by the SSD increases up to 6900 (the responded IOPS saturates when we send the requests with more than 7000 IOPS to the SSD). The results reveal that data failure increases by increasing the requested IOPS until the responded IOPS saturates where increasing the requested IOPS has no impact on the failure rate. This is because the SSD responds to only a limited number of requests (about 6900 uniform random writes in our experiments) where the fault only affects the responded IOPS.
% (after requested IOPS = 12000 in Fig. \ref{req_rate})   

\begin{figure}[t]
	\centering
	\includegraphics[scale=0.65]{results/Rate-new}
	\vspace{-0.5em}
	\caption{Impact of requested IOPS submitted to the SSD on number of data failures.}
	\vspace{-2em}
	\label{req_rate}
\end{figure}

\vspace{-1em}
\subsection{Impact of Sequence of the Accesses}
\vspace{-0.7em}
\label{sec:req_seq}

In this section, we have performed experiments to evaluate the vulnerability of the SSDs under power failure with the workloads by different sequence of the accesses including RAR, WAR, WAW, and RAW. In these experiments, each request is submitted on the address of the previously completed request. As depicted in Fig. \ref{sequence_of_accesses}, there is significantly large number of failures due to power failure in the accesses with the WAW pattern. This is due to the large number of write operations in the workload with WAW accesses. The results reveal that power failure after a WAW sequence may affect both of written data (corresponding to write operation) and the previously written data in that address. Such failures are experienced in the accesses with WAR and RAW sequences while similar to the workloads with fully read request, there is no \emph{data failure} in the workload with RAR accesses. 
 Note that in the workloads with WAR, WAW, and RAW accesses,  SSDs experience considerable number of failures from \emph{FWA} type.
  %Such failure is due to the write pending requests that are kept in the SSDs volatile cache. In this case becasue the first request is read and two requests are for one special address, caching is occured most probably. 

%As can be seen in Fig.~\ref{Hazard}, it can be concluded that write requests do not cache on SSD cache and hence the RAW and the WAW do not exhibit the IO not inserted errors. However, in the WAR due to the first read type of request, the second write request

\begin{figure}[t]
	\centering
	\includegraphics[scale=0.65]{results/sequence_of_accesses}
	\vspace{-0.5em}
	\caption{Impact of sequence of the accesses on data failure.}
	\vspace{-2em}
	\label{sequence_of_accesses}
\end{figure}

%\subsection{Impact of Type of Application Level IO Request}
%\label{sec:io_func_type}
%
%In this section, we measure the impact of different write functions in the application layer on the failure ratio during power faults. To this end we have generate the workloads with the write operations that are submitted to the SSD with \emph{dd} command and the device level functions namely \emph{write} and \emph{fwrite}. The results of the experiments are depicted in Fig. \ref{writefunction}. It can be seen that the write operations through \emph{dd} command experience more failures compared to other type of application level requests. 
%
%\begin{figure}[t]
%	\centering
%	\includegraphics[scale=0.65]{results/Write_Function}
%	\caption{Impact of type of application level IO request.}
%	\label{writefunction}
%\end{figure}
%
%\subsection{Impact of SSD's Internal Cache}
%\label{sec:ssd_cache}
%
%SSDs have internal write-back DRAM caches for improving their performance in write opertaions. However, exploiting the volatile memory will increase the susceptibility against power outage. We have tested the SSD with cache enable/disable to measure the data failures due to power faults. The workload includes pure write and random access and data with random size of requests between 4KB to 1MB. The results show that the SSD with enable caching have more data failures than the SSD with disable caching by about 5\%.



% \subsection{Impact of Workload Working Set Size}
% \label{sec:wss}
%	\begin{figure}[!htb]
%	\centering
%	\includegraphics[scale=0.65]{Figures/results/disk-size}
%	\caption{Impact of workload working set size on data failure.}
%	\label{fig:wss_size}
%\end{figure}
% \subsection{Impact of Request Size}
%\label{sec:req_size}
%\begin{figure}[!htb]
%	\centering
%	\includegraphics[scale=0.65]{Figures/results/request-size}
%	\caption{Impact of workload working set size on data failure.}
%	\label{fig:req_size}
%\end{figure}
%
% \subsection{Impact of Request Type}
%\label{sec:req_type}
%
% \subsection{Impact of Request Access Pattern}
%\label{sec:pattern}
%
% \subsection{Impact of Disk Address Space}
%\label{sec:addr_space}
%\begin{figure}[!htb]
%	\centering
%	\includegraphics[scale=0.65]{Figures/results/write-Address-space}
%	\caption{Impact of workload working set size on data failure.}

%	\label{fig:addr_space}
%\end{figure}
%
% \subsection{Impact of Type of Application Level IO Request}
%\label{sec:io_func_type}
%
%\subsection{Impact of SSD's Internal Cache}
%\label{sec:ssd_cache}