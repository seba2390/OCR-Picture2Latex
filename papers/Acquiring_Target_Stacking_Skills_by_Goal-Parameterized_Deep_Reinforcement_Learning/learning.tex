\section{Target Stacking Task}
We introduce a new manipulation task: {\em target stacking\/}. In this task, an image of a target structure made of stacked blocks is provided. Given the same number of blocks as in the target structure, the goal is to reproduce the structure shown in the image. The manipulation primitives in this task include moving and placing blocks. This is inspired by the scenario where young children learn to stack blocks to different shapes given an example structure. We want to explore how an artificial agent can acquire such a skill through trial and error. 

\subsection{Task Description}
For each task instance, a target structure is generated and its image is provided to the agent along with the number of blocks. Each of these blocks has a fixed orientation. The sequence of block orientations is such that reproducing the target is feasible. The agent attempts to construct the target structure by placing the blocks in the given sequence. The spawning location for each block is randomized along the top boundary of the environment. A sample task instance is shown in Figure~\ref{fig:tstack_demo}.

\begin{figure}
\centering
\includegraphics[width=1.\linewidth]{./figs/t_stack_eg.pdf}
\caption{Target stacking: Given a target shape image, the agent is required to move and stack blocks to reproduce it.}
\label{fig:tstack_demo}
\end{figure}

\subsection{Task Distinction}
The following characteristics distinguish this task from other tasks commonly used in the literature.

\paragraph{Goal-Specific}
A widely-used benchmark for deep reinforcement learning algorithm are the Atari games \citep{bellemare13arcade} that were made popular by \cite{mnih2013playing}. While this game collection has a large variety, the games are defined by a single goal or no specific goal is enforced at a particular point in time. For example in Breakout, the player tries to bounce off as many bricks as possible. In Enduro, the player tries to pass as many cars as possible while simultaneously avoiding cars. 

In the target stacking task, each task instance differs in the specific goal (the target structure), and all the moves are planned towards this goal. Given the same state, moves that were optimal in one task instance are unlikely to be optimal in another task instance with a different target structure. This is in contrast to games where one type of move will most likely work in similar scenes. This argument also applies to AI research platforms with richer visuals like VizDoom \citep{kempka2016vizdoom}.

\paragraph{Longer sequences}
Target stacking requires looking ahead over a longer time horizon to simultaneously ensure stability and similarity to the target structure. This is different from learning to poke \citep{agrawal2016learning} where the objective is to select a motion primitive that is the optimal next action. It is also different from the work by \cite{li2017visual} that reasons about the placement of one block. 

\paragraph{Rich Physics Bounded}
Besides stacking to the assigned target shape the agent needs to learn to move the block without colliding with the environment and existing structure and to choose the block's placement wisely not to collapse the current structure. The agent has no prior knowledge of this. It needs to learn everything from scratch by observing the consequence (collision, collapse) of its actions.  

\subsection{Environment Implementation}
A deep reinforcement learning agent requires to learn from a larger number of samples. To enable this, we build a simulated environment for the agent to interact with physical-realistic task instances. While we keep the essential parts of the task, at its current stage the simulated environment remains an abstraction of a real-world robotics scenario. This generally requires an integration of multiple modules for a full-fledged working system, such as \cite{toussaint2010integrated}, which is out of scope of this paper.

In detail, the simulated stacking environment is implemented in Panda3D \citep{goslin2004panda3d} with bullet \citep{coumans2010bullet} as physics engine. 
The block size follows a ratio of $l:w:h = 5:2:1$, where $l$,$w$,$h$ denote length, width and height respectively. We ignore the impact during block placement and focus on the resulting stability of the entire structure. Once the block makes contact with the existing structure, it is treated as releasing the block for a placement. In each episode, if the moving block collides with the environment boundary or existing structure, it will terminate the current episode. Further, if the block placement causes the resulting new structure to collapse, it will also end the episode. Stability is simulated similar to \cite{li2017visual} by comparing the change of displacement across all the blocks to a pre-selected small threshold within a fraction of time. If all of the blocks' displacements are below this threshold, the structure is deemed stable, otherwise unstable. To simplify the setting, we further constrain the action to be $\{\text{left},\text{right},\text{down}\}$.

The physics simulation runs at $60Hz$. However considering the cost of simulation we only use it when there is contact between the moving block and the boundary or the existing structure. Otherwise, the current block is moving without actual physics simulation. To further reduce the appearance difference caused by varying perspective, the environment is rendered using orthographic projection. Figure~\ref{fig:scene_sample} shows example images. The environment provides a user-friendly Python interface (similar to  Gym\citep{brockman2016openai}) so that it can be used to test different reinforcement learning agents. At time of publication we will release our implementation of the environment.

\begin{figure}
\begin{center}$
\begin{tabular}{cccc}
\label{fig:hstack1}\includegraphics[width=0.2\linewidth]{./figs/hstack_1.png}&
\label{fig:hstack2}\includegraphics[width=0.2\linewidth]{./figs/hstack_2.png}&
\label{fig:hstack3}\includegraphics[width=0.2\linewidth]{./figs/hstack_3.png}&
\label{fig:hstack4}\includegraphics[width=0.2\linewidth]{./figs/hstack_4.png}\\
\end{tabular}$
\end{center}
\caption{Example scenes constructed by the learned agent.}
\label{fig:scene_sample}
\end{figure}

\section{Goal-Parameterized Deep Q Networks (GDQN)}
As one major characteristic of this task is that it requires goal-specific planning: given the same or similar states under different objectives, the optimal move can be different. To this end, we extend the typical reinforcement learning formulation to incorporate additional goal information.  

\subsection{Learning Framework}
In a typical reinforcement learning setting, the agent interacts with the environment at time $t$, observes the state $s_t$, takes action $a_t$, receives reward $r_t$ and transits to a new state $s_{t+1}$. A common goal for a reinforcement learning agent is to maximize the cumulative reward. This is commonly formalized in form of a value function as the expected sum of rewards from a state $s$, $\mathbf{E}[\sum\limits_{i=0}^\infty \gamma^i r_{t+i+1}|s_t = s,\pi]$ when actions are taken with respect to a policy $\pi(a|s)$, with $0\leq\gamma\leq 1$ being the discount factor. The alternative formulation to this is the action-value function $Q^\pi(s,a)=\mathbf{E}[\sum\limits_{i=0}^\infty \gamma^i r_{t+i+1}|s_t=s,a_t=a]$. 

Value-based reinforcement learning algorithms, such as Q-learning \citep{watkins1992q} directly search for optimal Q-value function. Recently by incorporating deep neural network as a function approximator for $Q$-function, the DQN \citep{mnih2015human} has shown impressive results across a variety of Atari games.

\paragraph{DQN}
For our task, we apply a {\em Deep Q Network\/} (DQN) which uses a deep neural network for approximating the action-value function $Q(s,a;\theta)$, mapping from an input state $s$ and action $a$ to Q values. In particular, two important improvements have been proposed by \cite{mnih2015human} for the learning process, including (1) experience replay, the agent stores observed transitions in a memory buffer for some time, and uniformly samples from the memory to update the network (2) the target network, agent maintains two networks for the loss function --- one for the current estimator of Q function and one for the surrogate of the true Q function. For the current estimator, the parameters are constantly updated. For the surrogate, the parameters are only updated for every certain number of steps from the current estimator network otherwise kept fixed.  

\paragraph{Learning Goal-Parameterized Policies}
To plan with respect to the specific goal, we can parametrize the agent's policy $\pi$ by the goal $g$: 
\begin{equation}
    \pi(s,g,a)
\end{equation}

Since in this work, we applies DQN as value-based method, this corresponds to the update to original Q function with the additional goal information. The new Q-value function is hence defined as:

\begin{equation}
    Q^\pi(s,g,a) = \mathbf{E}[\sum\limits_{i=0}^\infty \gamma^i r_{t+i+1}|s_t=s,g,a_t=a]
\end{equation}

As shown in Figure~\ref{fig:goal_net}, in contrast to the original DQN model, where state and action are used to estimate Q-value,  the new model further include the current goal into the network to produce the estimate. We call this model as Goal-Parametrized Q Network (GDQN). 

The resulted loss function is as:

\begin{equation}
    L_Q = \mathbf{E}[(R + \gamma \text{max}_a' Q^\pi(s',g,a';\theta^-) - Q(s,g,a;\theta))^2 ]
\end{equation}

where $\theta^-$ are the previous parameters and the optimization is with respect to $\theta$. 

\begin{figure}[h]
\centering
\includegraphics[width=0.69\textwidth]{./figs/qg-net_rev.pdf}
\caption{Our proposed model GDQN which extends the $Q$-function approximator to integrate goal information.}
\label{fig:goal_net}
\end{figure}

\subsection{Implementation Details}
The DQN agent is implemented in Theano and Keras to adapt to the settings in our experiment, while we use a $2$ hidden layer (each with $64$ hidden units and rectified linear activation) multilayer perceptron (MLP) for most cases, we additionally swap the MLP with the CNN and follow the reported parameter settings as in the original paper \citep{mnih2015human} to ensure our implementation can reach similar performance.

Note we don't apply the frame-skipping technique \citep{bellemare2012investigating} used for Atari games \citep{mnih2015human} allowing the agent sees and selects actions on every $k$th frame where its last action is repeated on skipped frames. It does not suit our task, in particular when the moving block is getting close to the existing structure, simply repeating action decided from previous frame can cause unintended collision or collapse. 

\paragraph{Reward}
In the target stacking task, the agent gets reward $+1$ when the episode ends with complete reproduction of the target structure, otherwise $0$ reward.

Further, we explore reward shaping \citep{ng1999policy} in the task providing more prompt intermediate reward. Two types of reward shaping are included: overlap ratio and distance transform. 

For the overlap ratio, for each state $s_t$ under the same target $g_i$, an overlap ratio is counted as the ratio between the intersected foreground region (of the current state and the target state) and the target foreground region (shown in Figure~\ref{fig:overlap_ratio}):

\begin{equation}\label{eq:ovl}
o(s_t,g_i) = \frac{s_t \cap g_i}{g_i}
\end{equation}

For each transition $(s_t, a_t, s_{t+1})$, the reward is defined by the change of overlap ratio before and after the action: 
\begin{equation}
    r_t =
    \begin{cases}
      1, & \text{if}\ \Delta o_{t\rightarrow t+1} = o(s_{t+1}) - o(s_t) >  0 \\
      -1, & \text{if}\ \Delta o_{t\rightarrow t+1} = o(s_{t+1}) - o(s_t) <  0\\
      0, & \text{otherwise}
    \end{cases}
\end{equation}
The intuition is that actions increasing the current state to become more overlapped with the target scene should be encouraged.

For the distance transform \citep{fabbri20082d}, it generates a map $D$ whose value in each pixel $p$ is the smallest distance from it to a target object $O$:

\begin{equation}
D(p) = \text{min}\{\text{dist}(p,q)|q\in O\}
\end{equation}

where $\text{dist}$ can be any valid distance metric, like Euclidean or Manhattan distance. 

For each state $s_t$ under the same target $g_i$, a distance to the goal is the sum of all the element-wise distance in $s_t$ to $g_i$ under $D_{g_i}$ (shown in Figure~\ref{fig:dist_transform}) as: 

\begin{equation}
d(s_t,g_i) = \sum_j D_{g_i}(s_t^j), s_t^j \in s_t
\end{equation}

For each transition $(s_t, a_t, s_{t+1})$, the reward is defined as: 
\begin{equation}
    r_t =
    \begin{cases}
      1, & \text{if}\ \Delta d_{t\rightarrow t+1} = d(s_{t+1}) - d(s_t) <  0 \\
      -1, & \text{if}\ \Delta d_{t\rightarrow t+1} = d(s_{t+1}) - d(s_t) >  0\\
      0, & \text{otherwise}
    \end{cases}
\end{equation}

The intuition behind this is that action decreasing the distance between the current state and the target scene should be encouraged.

\begin{figure}
\centering
\begin{subfigure}{0.47\linewidth}
\includegraphics[width=\textwidth]{./figs/ovl_ratio.pdf}
\caption{}
\label{fig:overlap_ratio}
\end{subfigure}
~
\begin{subfigure}{0.47\linewidth}
\includegraphics[width=\textwidth]{./figs/dist_transform.pdf}
\caption{}
\label{fig:dist_transform}
\end{subfigure}
\caption{Reward shaping used in target stacking. (\protect\subref{fig:overlap_ratio}): overlap ratio to the target. The gray area in the middle figure denotes the intersected foreground region between current and target scene, and the overlap ratio is the ratio between the areas of the two. (\protect\subref{fig:dist_transform}): distance under the distance transform of the target. The middle figure denotes the distance transform under the target shown in the left. The distance from current scene to the target is the sum of distances masked by the current scene in the distance transform.}
\label{fig:reward_shape}
\end{figure}