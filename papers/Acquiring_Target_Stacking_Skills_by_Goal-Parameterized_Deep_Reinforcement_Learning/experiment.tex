\section{Experiments}
We evaluate the proposed GDQN model on both a navigation task and target stacking and compare it to the base DQN model which does not integrate goal information. In addition, we include the result from GDQN model with different ways of reward shaping in the target stacking task.

\subsection{Toy Example with Goal  Integration}
As a toy example, we introduce a type of navigation task in the classic gridworld environment. The locations for the starting point and goal are randomized for each episode. The agent needs to reach the goal with four possible actions $\{\text{left},\text{right},\text{up},\text{down}\}$. Action that will make the agent go off the grid will leave it stay in the same location. The episode only terminates once the agent reaches the goal. The agent only receive reward $+1$ when reaching the current goal. Two different sizes of gridworld are tested at $5\times5$ and $7\times7$. 

The training epoch size is $1000$ in steps for the smaller gridworld and $3000$ for the larger one, the test sizes are the same for both at $100$. All the agents run for $100$ epochs and the $\epsilon$ for $\epsilon$-greedy anneals linearly from $1.0$ to $0.1$ over the first $20$ epochs, and fixed at $0.1$ thereafter. The memory buffer size is set the same to the annealing length, i.e. for the smaller gridworld, the buffer size equals to the length $20$ epochs in training with $20000$ steps whereas for the larger one, the buffer size is $30000$ steps.
We measure the proportion of episodes in the test epoch that reaches the goal in shortest distance as the success ratio. The results are shown in Table~\ref{tab:gridworld_result} for the best agents throughout the training process.

As in this simple task with relative small state space, DQN gets some performance due to running an average policy across all the the goals, but this is not addressing the task we set out to do. In contrast, GDQN parametrized specifically to include goal information achieves significant better results on both sizes of the environment.

\begin{table}
\centering
\begin{tabular}{c|cc}
\toprule
Grid Size & DQN  & GDQN \\ 
\midrule
$5\times 5$       & 0.67 & 0.97 \\
$7\times 7$       & 0.67 & 0.95 \\
\bottomrule
\end{tabular}
\caption{Results from navigation task.}
\label{tab:gridworld_result}
\end{table}

\subsection{Target Stacking}
We set up $3$ groups of target structures consisted of different number of blocks $\{2,3,4\}$ in the scene as shown in Figure~\ref{fig:tgt_stack}. Within each group of target shapes, a random target (with the accompanied orientation order) is picked at the very beginning for individual episode. Each training epoch consists of $10000$ steps and each test epoch with $1000$ steps. Similar to the setting in the toy example, all the agents run for $100$ epochs and the $\epsilon$ anneals for the first $20$ epochs, and the memory buffer size is set as long as the annealing steps at $200K$ steps. 

We computed both average overlap ratio (OR) and success rate (SR) for the finished stacking episodes in each test epoch. Here overlap ratio is the same as defined in the reward shaping in Equation~\ref{eq:ovl}, but simply measures the end scene over the assigned target scene. This tells the relative completion of the stacked structure in comparison to the assigned target structure, the higher the value is, the better completion it is to the target. At the maximum of $1$, it suggests completely reproduction of the target. The success rate counts the ratio how many episodes complete the exact same shape as assigned over the total number of episodes finished in the test epoch. This is the absolute metric counting overall successful stacking. The results are shown in Table~\ref{tab:tgt_stack} for the best agents throughout the training process.

Over all groups on both metrics, we observe GDQN outperforms DQN, showing the importance of integrating goal information. In general, the more blocks in the task, the more difficult it becomes. When there are only small number of blocks ($2$ blocks and $3$ blocks) in the scene, the single policy learned by DQN averages over the few target shapes can still work to some extent. However when introducing more blocks into the scene, it becomes more and more difficult for this averaged model to handle. As we can see from the result, there is already a significant decrease of performance (success rate drops from $0.70$ to $0.43$) when increasing the blocks number from $2$ to $3$, whereas GDQN's performance only decreases slightly from $0.72$ to $0.67$. In $4$ blocks scene, the DQN can no longer reproduce any single target ($0.0$ for success rate, $0.03$ for overlap ratio) while GDQN parametrized specifically to include goal information can still do. Though the success rate (absolute completion to the target) for the basic GDQN is relatively low at $0.17$ but the average overlap ratio (relative completion to the target) still holds up pretty well at $0.41$. Also we see reward shaping can further improves GDQN model, in particular distance transform can boost the performance more than overlap ratio.

\begin{figure}
    \centering
    \begin{subfigure}[b]{0.32\linewidth}
        \includegraphics[width=\textwidth]{./figs/2blks_stack_targets.pdf}
        \caption{}
        \label{fig:2blks_tgt_stack}
    \end{subfigure}
    ~ 
    \begin{subfigure}[b]{0.32\linewidth}
        \includegraphics[width=\textwidth]{./figs/3blks_stack_targets.pdf}
        \caption{}
        \label{fig:3blks_tgt_stack}
    \end{subfigure}
    ~ 
    \begin{subfigure}[b]{0.32\linewidth}
        \includegraphics[width=\textwidth]{./figs/4blks_stack_targets.pdf}
        \caption{}
        \label{fig:4blks_tgt_stack}
    \end{subfigure}
    \caption{\protect\subref{fig:2blks_tgt_stack}: Targets for $2$ blocks.\protect\subref{fig:3blks_tgt_stack}: Targets for $3$ blocks. \protect\subref{fig:4blks_tgt_stack}: Targets for $4$ blocks.}
    \label{fig:tgt_stack}
\end{figure}

\begin{table}
\centering
\begin{tabular}{@{}clccccccccccc@{}}
\toprule
\multicolumn{2}{c}{\multirow{2}{*}{Num. of Blks.}} & \multicolumn{2}{c}{DQN} &  & \multicolumn{2}{c}{GDQN} &  & \multicolumn{2}{c}{GDQN + OR} &  & \multicolumn{2}{c}{GDQN + DT} \\ \cmidrule(lr){3-4} \cmidrule(lr){6-7} \cmidrule(lr){9-10} \cmidrule(l){12-13} 
\multicolumn{2}{c}{}                               & OR         & SR         &  & OR          & SR         &  & OR            & SR            &  & OR            & SR            \\ \midrule
\multicolumn{2}{c}{2}                              & 0.70          & 0.70          &  & 0.82           & 0.72          &  & 0.84             & 0.77             &  & {\bf 0.88}             & {\bf 0.78}             \\
\multicolumn{2}{c}{3}                              & 0.43          & 0.43          &  & 0.76           & {\bf 0.67}          &  & {\bf 0.86}             & 0.63             &  & 0.83             & 0.65             \\
\multicolumn{2}{c}{4}                              & 0.03          & 0.0          &  & 0.41           & 0.17          &  & 0.73             & 0.55             &  & {\bf 0.79}             & {\bf 0.56}             \\ \bottomrule
\end{tabular}
\caption{Results for target stacking. For ``GDQN + X'', X denotes different ways for reward shaping as described in previous section, OR for overlap ratio, DT for distance transform. For metrics, OR stands for average overlap ratio, SR for average success rate.}
\label{tab:tgt_stack}
\end{table}