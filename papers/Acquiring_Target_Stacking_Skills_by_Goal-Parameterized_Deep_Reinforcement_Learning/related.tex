\section{Related Work}
Humans possess the amazing ability to perceive and understand ubiquitous physical phenomena occurring in their daily life. There is research in psychology that seeks to understand how this ability develops. \cite{baillargeon2002acquisition} suggest that infants acquire the knowledge of physical events at a very young age by observing those events, including support events and others. 
Interestingly, in a recent work \cite{denil2016learning}, the authors introduce a basic set of tasks that require the learning agent to estimate physical properties (mass and cohesion combinations) of objects in an interactive simulated environment and find that it can learn to perform the experiments strategically to discover such hidden properties in analogy to human's development of physics knowledge.

\cite{battaglia2013simulation} proposes an intuitive physics simulation engine as an internal mechanism for such type of ability and found close correlation between its behavior patterns and human subjects' on several psychological tasks.

More recently, there is an increasing interest in equipping artificial agents with such an ability by letting them learn physical concepts from visual data. 
\cite{mottaghi2015newtonian} aim at understanding dynamic events governed by laws of Newtonian physics and use proto-typical motion scenarios as exemplars. \cite{fragkiadaki2015learning} analyze billiard table scenarios and learn dynamics from observation with explicit object notion. 
An alternative approach based on boundary extrapolation \cite{apratim16ariv} addresses similar settings without imposing any object notion. 
\cite{wu2015galileo} aims to understand physical properties of objects based on explicit physical simulation. 
\cite{mottaghi2017see} proposes to reason about containers and the behavior of the liquids inside them from a single RGB image.

Moreover, \cite{fergus16blocsarxiv} propose using a visual model to predict stability and falling trajectories for simple 4 block scenes. \cite{li2016fall} investigate if and how the prediction performance of such image-based models changes when trained on block stacking scenes with larger variety. They further examine how the human's prediction adapts to the variation in the generated scenes and compare to the learned visual model. Each work requires significant amounts of simulated, physically-realistic data to train the large-capacity, deep models.

Another interesting question that has been explored in psychology is how knowledge about physical events affects and guides human's actual interaction with objects \cite{yildirim2017physical}. 
Yet it is not clear how machine model trained for physics understanding can be directly applied into real-world interactions with object and accomplish manipulation tasks.  
\cite{li2017visual} makes a first attempt along this direction by extending their previous work \citep{li2016fall} on stability classification. They task a robot to place a wooden block on an existing structure while maintaining stability. Placement candidates are first generated and then evaluated through the visual stability classifier, so that only predicted stable placements are executed on the robot. 

In this paper, reinforcement learning is used to learn an end-to-end model directly from the experience collected during interaction with a physically-realistic environment.
The majority of work in reinforcement learning focuses on solving task with a single goal. However, there are also tasks where the goal may change for every trial. It is not obvious how to directly apply the model learned towards a specific goal to a different one. An early idea has been proposed by \cite{kaelbling1993learning} for a maze navigation problem in which the goal changes. The author introduces an analogous formulation to the Q-learning by using shortest path in replacement of the value functions. 
Yet there are two major limitations for the framework: 1) it is only formulated in tabular form which is not practical for application with complex states 2) the introduced shortest path is very specific to the maze navigation setting and hence cannot be easily adapt to handle task like different targets stacking, serving as a general solution to this type of problem. 

In contrast, we propose a goal-parameterized model to integrate goal information into a general learning-based framework that facilitates generalization across different goals. The model has been shown to work on both a navigation task and target stacking.

Notably, \cite{schaul2015universal} also propose to integrate goal information into learning. However, they learn an embedding for state and goal to allow generalization in a reinforcement learning setup. The process is then incorporated into the Horde framework \citet{sutton2011horde}, where each agent learns towards different goals. In our work, we do not introduce a dedicated embedding learning but instead resort to an end-to-end approach where the function approximator will learn a direct mapping from sensory observations to actions that allows generalization across different goals.
