% \vspace{-0.20in}
%\subsection{Analysis:}


\textbf{Use of Multiple Projection Heads:} The use of different projection heads for each view on OpenImages classification gives us a boost of $1.1$ mAP on Obj-Obj+Dilate crop. Pre-training on COCO and finetuning on VOC dataset for object-detection task gives a boost of $0.4$ mAP. Hence using multiple projection heads results in a consistent improvement. 

\textbf{Varying Dilation Parameter:} Table 3 (appendix) shows the effect of varying the dilation parameter. A sweet spot exists at a moderate dilation value of $\delta=0.1$ for COCO object detection. 

% \textbf{Computational Cost:} BING adds negligible time to the pre-training. Generating object proposals takes ~29 mins for the full OHMS dataset (one-time cost) and ~16 mins for COCO. Instead of pre-generating, adding the BING operator to the data loader pipeline has a trivial overhead (+$0.1\%$). %As an example, the wall-clock time taken for 1 epoch of training is 1'46'' for the Dense-CL baseline and 1'45'' for our method.
% \textbf{}



%Between two views, we measure the number of common pixels; and then measure the fraction of these common pixels that overlap with a ground truth bounding box (object). We find that this fraction for COCO is $99\%$ for object-scene crops and $92.1\%$ for the scene-scene crop. In the case of OpenImages-Subset, the numbers are, respectively, $99.1\%$ and $87.3\%$. This is another way of seeing that OpenImages-Subset can benefit more from object-scene crops, borne out by the numbers in Tables \ref{tab:ssl_comparison_classification} and \ref{tab:coco_detection}. 


% \as{Shlok: could you please make this description a little better and clear?}
% We find the overlapping pixels between two crops ($C_{int} = C_1 \cap C_2$). Next we calculate intersection of $C_{int}$  with the most overlapping ground truth object ($O$) and calculate the score $\frac{C_{int} \cap O}{C_{int}}$ for each image and average it. 
% To do this, we calculate the \% intersection of the most overlapping ground truth object with the inter
% Next we try to find the probability of an actual ground truth object co-occuring in between two crops. We find  object-overlap between both Scene-Object crops and Scene-Scene crops. To do this we firstly calculate the overlapping region between two crops. Overlapping region is the area of overlap between two crops before the resize operation. Then for all the ground truth objects present in the original image  we find the object with maximum overlap in the overlapping region. Intuitively for a object to have high overlap, the object should be present in both the crops. 

% Similarly instead of taking an crop with maximum overlap we calculate average of all the crops that are present in the image. We find this average probability to be 65.12 \% for Object-Scene crop and 73.47 \% for Scene-Scene crop. 
% This is consistent with the findings of the InfoMin \cite{tian2020contrastive} that there is a tradeoff between how much information views can share.  

% Similarly in the case of OpenImages we can see from Fig \ref{fig:radius_openimages} that as we increase the radius of the object-object crops the performance firstly increases and then decreases, suggesting there is a sweet point on mutual information on OpenImages dataset as well.
% \\

% \textbf{Performance on 5 classes per image images?}








