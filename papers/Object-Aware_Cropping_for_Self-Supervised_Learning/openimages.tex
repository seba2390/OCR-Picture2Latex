\section{Analysis of Self-Supervised Learning methods on the OpenImages Dataset}

\label{sec:analysis}
In this section, we first identify some limitations of state-of-the-art SSL methods such as MoCo-v2  \cite{he2019momentum,chen2020improved}, SwAV  \cite{NEURIPS2020_70feb62b} and BYOL \cite{grill2020bootstrap} when pretraining on the OpenImages dataset. These methods have nearly closed the performance gap with supervised learning methods when pre-trained and linear probed on ImageNet \cite{imagenet_cvpr09}. However, the performance of these methods on OpenImages dataset (where images contain multiple small objects) has not been extensively studied. OpenImages \cite{kuznetsova2020open} encompasses images of complex scenes and several objects (containing, on average, $8$ annotated objects per image). It consists of a total of $9.1$ million images. To perform controlled experiments on the effectiveness of cropping on SSL method performance, we construct a subset of the OpenImages dataset called \textbf{OHMS} \textbf{O}penimages \textbf{H}ard \textbf{M}ulti-object \textbf{S}ubset. We construct the dataset as follows: We sample images that have labelled bounding boxes to enable comparisons with fully supervised learning. Secondly, we sample images with objects from at least $2$ distinct classes to create a dataset that better reflects real-world uncurated data. Finally,  we only consider class categories with at least 900 images to mitigate effects of imbalanced class distribution. After this processing, we have $212,753$ images present across $208$ classes and approximately $12$ objects per image on average. 

We provide further details in the Appendix SecA.
% \ref{sec:details_openimages}. 
% For convenience, we still refer to this dataset as OpenImages through this paper. %We begin by showing the results of the state-of-the-art methods on OpenImages-Subset.


\subsection{Performance of SSL methods}
\label{sec:sslanalysis}
% We pretrain several state-of-the-art SSL methods MoCo-v2 \cite{he2019momentum,chen2020improved}, CMC \cite{tian2019contrastive}, SwAV  \cite{NEURIPS2020_70feb62b} and BYOL \cite{grill2020bootstrap} on our OpenImages dataset. MoCo \cite{he2019momentum} is a contrastive learning-based SSL approach that uses data augmentations of a given image as positive samples for that image and other randomly chosen images in the batch as negative samples. Positive samples are pulled close together in representation space; and negative samples are pushed apart. MoCo-v2 \cite{chen2020improved} is an improved version of MoCo inspired by SimCLR \cite{chen2020simple}; key improvements are the addition of MLP projection heads and additional data augmentation such as Gaussian blurring. Nevertheless, scene cropping is still the most important augmentation. BYOL \cite{grill2020bootstrap} is an SSL approach that also uses data augmentation of an image as positive samples for that image. Positive samples are pulled close together in representation space. BYOL does not rely on negative samples (and hence is not contrastive). Instead, several other regularization techniques are required to prevent collapse to a trivial solution \cite{richemond2020byol}. 
% \ashah{I think a lot of the information below can go into implementation details}
We pretrain several SSL methods MoCo-v2, CMC \cite{tian2019contrastive}, SwAV  \cite{NEURIPS2020_70feb62b} and BYOL \cite{grill2020bootstrap} on OHMS dataset. 
MoCo-v2, BYOL, and other recent state of the art SSL approaches all relying on \textit{scene-scene} cropping of the same image to generate positive samples. 
% This cropping strategy has also been used as a default data augmentation in supervised learning \cite{He2015,NIPS2012_c399862d,cubuk2019autoaugment}. The most common workflow is: choose a random scale (default range $0.2$ to $1.0$) and a random aspect ratio (default range $0.75$ to $1.33$). A crop is made from the original image using these two values (scale and aspect ratio). Finally, the crop is resized to the final size of $224 \times 224$ pixels. %This approach works well in the case of ImageNet where one object primarily occupies most of the area in an image. However, problems are encountered when applying this to the OpenImages dataset (see Fig \ref{fig:imagenet_openimages_figure} in supplementary).

 Table \ref{tab:ssl_comparison_classification} shows our results. We see a significant difference in performance between fully supervised training and SSL approaches on the OHMS dataset, with a gap of $16.3$ mAP points on average across the $4$ SSL approaches considered. On ImageNet, the top-1 accuracy gap is considerably smaller with an average gap of only $8.5$, nearly half that of OHMS.  The last row shows the significant boost obtained by using our object-aware cropping approach, which we describe in the next section, with MOCO-v2. The gap between SSL and supervised training on OHMS is now the same as ImageNet.
%  \ashah{Should we change the flow here ? Our approach is not yet described, but we are talk about how it helps close the gap ?} \shlok{how to change?} \ashah{Can we move this whole section after the methods?}
% On ImageNet, where a single object tends to dominate the image, \emph{scene-scene} and \emph{obj-obj+dilate} perform at par.
\begin{table*}
    \centering
    \begin{tabu} to \linewidth {lcccc} 
        \toprule
        Model & OHMS (mAP) & ImageNet (Top-1 $\%$)\\
         \midrule
         Supervised Performance & 66.3 & 76.2 \\ 
         \midrule
    %   CMC \cite{Tian2020ContrastiveMC} & 48.7({-17.6) & 60.0 ({-16.2})\\
       CMC \cite{tian2019contrastive}  & 48.7 ({{-17.6}}) & 60.0  ({{-16.2}})\\
       BYOL \cite{grill2020bootstrap} & 50.2 ({{-16.1}}) & 70.7  ({{-5.5}})\\
       SwAV \cite{NEURIPS2020_70feb62b} & 51.3 ({{-15.0}}) & 72.7 ({{-3.5}})\\
       MoCo-v2 (Scene-Scene crop) & 49.8 ({{-16.5}}) & 67.5  ({{-8.7}})\\
        \midrule
        MoCo-v2 (Object-Object+Dilate crop) (Ours)  & 58.6 ({{-7.7}}) & 68.0 ({{-8.2}})\\
        \bottomrule
    \end{tabu}
    \vspace{0.1in}
    \caption{Classification results on OHMS and Imagenet. For each SSL method, we show in parentheses the gap to fully supervised training (same number of epochs). The last row shows that our proposed approach using \emph{obj-obj+dilate} cropping reduces the gap on OHMS by nearly half compared to the baselines, improving over the \emph{scene-scene} cropping based SSL methods by between $8.8$ mAP points. We also observe improvements on ImageNet as well. }
    \label{tab:ssl_comparison_classification}
\end{table*}

\subsection{Analysis and Motivation}
We conduct further experiments to better analyze the results seen in Table \ref{tab:ssl_comparison_classification}, and to motivate our proposed approach. Our experiments help to narrow down scene cropping as one main cause of the poor performance of SSL on OHMS, rather than other differences with ImageNet, such as object size, class distributions or image resolution.


\textbf{Object Size:} We compare MoCo-v2 performance to that of fully supervised learning, with both methods using scene-based cropping. Fig. \ref{fig:issue_scale_long_tail} (left) shows that the performance gap between supervised learning and SSL methods does not vary significantly for objects of different sizes in OHMS. This suggests that once object sizes are below a threshold where scene cropping tends to ignore object information, MoCo-v2 performance is mostly independent of object scale. 

\textbf{Long-tail Distribution:} Even after selecting at least $900$ images per class, our OHMS subset has a significant variation in the number of images per class (from around $1000$ to $60000$). Fig. \ref{fig:issue_scale_long_tail} (right) plots the performance of MoCo-v2 and supervised training as a function of the number of instances in each class. We do not see a significant change in relative performance as the number of instances in a class changes. This rules out long tails of the distribution as a cause for the poor absolute performance of MoCo-v2 on OHMS.

% \vspace{-1.4in}
\begin{figure*}[t!]
  \centering
  \begin{subfigure}[t]{0.48\textwidth}
   \centering
    \includegraphics[width=\linewidth]{LaTeX/Images/map_for_each_class_and_sum_of_pixels_for_sup_and_moco_plotly_1.png}
   
    % \caption{ }
     \label{fig:scale_issue}
  \end{subfigure} 
  ~
  \begin{subfigure}[t]{0.48\linewidth}
   \centering
    \includegraphics[width=\linewidth]{LaTeX/Images/map_for_each_class_and_instances_for_sup_and_moco_plotly_1.png}
    % \caption{}
  \label{fig:long_tail_issue}
  \end{subfigure}
  \vspace{-0.3in}
    \caption{Analysis of OHMS data distribution. Left: Performance of supervised and MoCo-v2 pre-training as a function of the scale of the objects; we plot the log of the average of pixels against the sum of AP for each class. We see no discernible pattern of performance of MoCo-v2 or supervised learning as a function of object scale. Right: Performance of supervised learning and MoCo-v2 as a function of the number of instances in a class; we plot the log number of instances in a class against the sum of AP for that class. We do not see any discernible pattern of performance difference as a function of class size. \vspace{-10pt}}
\label{fig:issue_scale_long_tail}
\end{figure*}
% \vspace{-0.4in}
% Our OpenImages dataset is smaller than ImageNet by a factor of $5$. To confirm that dataset size is not the problem for SSL approaches,
\textbf{Can ImageNet pre-training help?}
 We pre-trained a supervised model on ImageNet and then fine-tuned the final fully-connected layer on the OHMS dataset. We can see from the second column in Table  \ref{tab:openimages_analysis} that this pre-training does not help to close the performance gap. One of the reasons that ImageNet pre-training does not help is that OHMS and ImageNet have significantly different class distributions (e.g. see \cite{li2019analysis} for a detailed analysis). %ImageNet has single large objects, whereas OpenImages has many small objects per image. 
 
\textbf{Can resizing the images help?}
We also experimented with resizing the images in OHMS to the same approximate size ($384 \times 384$) and aspect ratio as ImageNet. The result is shown in the third column of Table \ref{tab:openimages_analysis} confirming that controlling for image size does not help to close the gap. 

\textbf{Can cropping on ground truth objects help?} 
%We use ground truth boxes for random cropping.  We can see from col $4$ that using ground truth boxes also doesn't help reduce the performance gap.
We see from column 4 of Table \ref{tab:openimages_analysis} that using random cropping on ground truth object boxes does not help reduce the performance gap either. As we show later in Section\ref{sec:model} that learning from both object and context is important for learning semantic information on multi-object datasets. 

\textbf{Varying the lower scale of random resized crop:}  MoCo-v2 (\cite{chen2020improved}) used scene crops whose size was chosen from a uniform distribution ranging from $20\%$ to $100\%$ of the ImageNet image size ($384 \times 384$). Since OHMS images are bigger and objects generally occupy a smaller fraction compared to ImageNet, we vary the lower bound for scene crops to measure the impact. The last six columns of Table \ref{tab:openimages_analysis} shows that varying the range of scene crop is insufficient to close the performance gap. 

We conclude that the performance gap between supervised training and SSL training is likely due to the data augmentation, rather than characteristics of the image distribution. Further analysis experiments are provided in the appendix (Sec D).
% \ref{sec:analysis_openimages_more}). 

% This shows that network is still not able to capture the smaller objects in the image doesn't really help in achieving good performance. \\

% \vspace{-5pt}
% \begin{wraptable}{r}{5.5cm}
% \vspace{-30pt}
% \begin{table*}
%     \centering
%     \begin{tabu} to \linewidth {lccc} 
%         \toprule
%         Model & mAP (Classification) \\
%          \midrule
%          Supervised OpenImages & 66.3
%         \\ 
%          \midrule
%       %Ground truth object crops & 22.8 \\
%       MoCo-v2: Pretraining on ImageNet & 28.3 \\
      
%         MoCo-v2: Resizing all images to ImageNet size ($384 \times 384$) & 45.9 \\
%          MoCo-v2: Cropping on ground-truth objects & 45.3 \\
%       MoCo-v2: scene crop range $0.8$-$1.0$ & 26.5 \\
%       MoCo-v2: scene crop range $0.6$-$1.0$ & 37.6 \\
%       MoCo-v2: scene crop range $0.4$-$1.0$ & 45.6 \\
%       MoCo-v2: scene crop range $0.2$-$1.0$ & 49.8 \\
%       MoCo-v2: scene crop range $0.1$-$1.0$ & 46.1 \\
%       MoCo-v2: scene crop range $0.05$-$1.0$ & 43.1 \\
%         \bottomrule
%     \end{tabu}
%     \vspace{0.1in}
%     \caption{Linear evaluation on our OpenImages dataset with different pre-training strategies with MoCo-v2 (see Section \ref{sec:analysis} for details). The top row uses fully supervised learning on OpenImages. We see that for self-supervised pre-training, no specific range of scene crops helps to close the large gap between SSL and supervised training. However, there is a sweet spot of scene crop range where MoCo-v2 performance is highest. \vspace{-20pt}}
%     % \vspace{-1.5ex}
%     \label{tab:openimages_analysis}
% \end{table*}

\begin{table*}
    \centering
    \resizebox{\linewidth}{!}{
    \begin{tabu} to \linewidth {cccc|cccccc} 
        \toprule
        &IN&Resize to &&\multicolumn{6}{c}{Scene-Scene Crop} \\
        Supervised & Pretraining & ($384 \times 384$) & GT-Crop & 0.8-1.0 &  0.6-1.0 & 0.4-1.0 & 0.2-1.0 & 0.1-1.0 & 0.05-1.0 \\
         \midrule
         66.3 & 28.3 & 45.9 & 45.3 & 26.5 & 37.6 & 45.6 & 49.8 & 46.1 & 43.1 \\
    %     \\ 
    %      \midrule
    %   %Ground truth object crops & 22.8 \\
    %   MoCo-v2: Pretraining on ImageNet & 28.3 \\
      
    %     MoCo-v2: Resizing all images to ImageNet size ($384 \times 384$) & 45.9 \\
    %      MoCo-v2: Cropping on ground-truth objects & 45.3 \\
    %   MoCo-v2: scene crop range $0.8$-$1.0$ & 26.5 \\
    %   MoCo-v2: scene crop range $0.6$-$1.0$ & 37.6 \\
    %   MoCo-v2: scene crop range $0.4$-$1.0$ & 45.6 \\
    %   MoCo-v2: scene crop range $0.2$-$1.0$ & 49.8 \\
    %   MoCo-v2: scene crop range $0.1$-$1.0$ & 46.1 \\
    %   MoCo-v2: scene crop range $0.05$-$1.0$ & 43.1 \\
        \bottomrule
    \end{tabu}}
    %\vspace{0.1in}
    \caption{Linear evaluation on our OHMS dataset with different pre-training strategies with MoCo-v2 (see Section \ref{sec:analysis} for details). Column 1 uses fully supervised learning on OHMS. We see that for self-supervised pre-training, no specific range of scene crops helps to close the large gap between SSL and supervised training. However, there is a sweet spot of scene crop range where MoCo-v2 performance is highest.} %\vspace{-20pt}}
    % \vspace{-1.5ex}
    \label{tab:openimages_analysis}
\end{table*}



% \end{wraptable}