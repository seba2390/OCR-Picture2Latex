%\vspace{-0.15in}
\section{Results}
\label{secc:results}
%\subsection{Datasets}
% We begin by describing OpenImages-Subset datset followed by COCO and ImageNet.
% \subsubsection{OpenImages Dataset}
%\label{sec:openimages_dataset}
% OpenImages has a total of 9.1 million images present in the dataset, however we are only focusing on the subset of OpenImages which has labelled bounding boxes. This smaller subset of OpenImages has close to 1.9 million images. However unlike ImageNet this subset is unbalanced and has a distribution with a long tail. To remove this long-tail and unbalanced distribution we create a balanced subset of OpenImages. To achieve this we use a greedy algorithm: \\
% \begin{itemize}
% \vspace{-0.2in}
% \item Protocol1 - We only select the images which have at least 2 or more classes present in the image. This brings down the total number of images from 1.9 million to close to 1.1 million.

% \item  Protocol2 - Now we only select the classes which have minimum of 900 images present in them. 
% After setting 900 as the low\_threshold, the number of classes selected were 208 and the number of images selected are 212753. 
% \end{itemize}
%\vspace{-0.15in}
\begin{table*}
    \centering
    \begin{tabu} to \linewidth {lccccc} 
        \toprule
        Model & Crops & Obj-Obj+Dilate & Scene-Scene  & mAP \\
        
         \midrule
          
         Supervised  & - & - & -  &  66.3 \\ 
         \midrule
        
     MoCo-v2  &- & -  & \Checkmark &  49.8\\
      BYOL & -   & -  & \Checkmark &  50.2 \\
      \midrule
       MoCo-v2 & Ground Truth boxes & -   & - &  58.9\\ 
         
         MoCo-v2 & Ground Truth boxes &  \Checkmark   &  - &  60.2\\ 
         \midrule
         
    MoCo-v2 & LOD crops & \Checkmark   &  - &  59.1\\
    MoCo-v2 + Rotation + Localization & {LOD crops} & {\Checkmark}   &  {-} &  59.7\\
    BYOL & LOD crops & \Checkmark  & - &  59.5\\
     
     
        \bottomrule
        
    \end{tabu}
    \vspace{-0.05in}    
  \caption{Crop approaches on OHMS: using LOD crops to generate one view, and a dilated crop for the other positive, we are able to reduce the difference between SSL and Supervised Learning by close to $50\%$ (compare the last two rows to the second row). Using ground-truth boxes to generate crops from OHMS improves the pre-training performance marginally compared to LOD crops. Our Obj-Obj+Dilate outperforms the Scene-Scene baseline by significant margin on the OHMS dataset.}  
  \label{tab:openimages_moco}    
\end{table*}
% this table is confusing.  First, why is, say Obj-Scene its own column, while Object-Dilated is a type of crop?  Why is BYOL not done with Object-Dilated or Obj-Obj+Shift?  But mostly you are doing very different things all mixed together.  On the one hand, you are comparing different types of proposals.  But you are also comparing different kinds of cropping strategies.  The most important point, that a good cropping strategy improves greatly over scene-scene gets kind of lost.  It would be better probably to separate this into different tables.  Also, the Ground Truth boxes are not a real method, instead it's a kind of upper bound on the others, but the structure of the table doesn't make this clear.  Also, Object-Dilated is using a Bing crop, right?  But it explicitly doesn't say BING crop. I think from this table that dilated works only marginally better than object-scene.  In that case, in the previous section, these should be presented as two similar approaches.

% I think from this table that dilated works only marginally better than object-scene.  In that case, in the previous section, these should be presented as two similar approaches.

We created a subset of the OpenImages dataset with $212$k images, as described in Section \ref{sec:analysis}. We also experiment with the complete OpenImages ($\sim$1.9 million images.)
% Each image in this dataset (which we still refer to as OpenImages) has on an average $12$ objects present. Further details of this dataset are provided in the Supplementary material in Sec \ref{sec:details_openimages}. 
In addition, we perform pre-training on ImageNet \cite{imagenet_cvpr09} and MS-COCO \cite{Lin2014MicrosoftCC}.
ImageNet (with $1.2$M training images) has been extensively used and is the standard dataset used for benchmarking of SSL methods. MS-COCO has $\sim118$k training images and $896$k labelled objects which is approximately $7$ objects per image. For pre-training on MoCo-v2, we closely follow the standard protocol described in \citet{chen2020improved}. We randomly select from $10$ object proposals provided by LOD. All our training and evaluation is performed on a ResNet-50 \cite{He2015}. For our baseline we use standard scene-scene crop, where we take two random crops in an image and treat them as positive views. This is the default approach used in MoCo-v2 and other SSL approaches. From our analysis in Section \ref{sec:analysis}, this approach performs poorly on datasets such as OHMS. 

% However, our main focus is non-iconic datasets like MS-COCO and OpenImages, so we only experiment using the ImageNet-100 dataset, which has been used in recent works like Info-Min \cite{tian2020contrastive}.
% \paragraph{Discussion on lower scale of random resized crop:}
As discussed, the Obj-Obj+Dilate uses a random crop on the object proposal or dilated version, to generate the final views. Since the object proposal itself is a small fraction of the image (e.g. in COCO, an object crop typically covers about 39\% of the image), using the usual default lower value for the random crop range (usually $0.2$) works poorly as it results in extremely small crops from the image. Therefore, we set the lower limit such that it matches the minimum sized crop in case of the usual scene crop ($s_{\text{min}} = \frac{0.2}{\text{average Object proposal size}}$) . 

We evaluate the pre-trained models on classification (linear evaluation), object detection and semantic segmentation. For VOC object detection, COCO object detection and COCO semantic segmentation, we closely follow the common protocols listed in Detectron2 \cite{wu2019detectron2}. For VOC object detection, we evaluate on the Faster-RCNN(C4-backbone) \cite{Ren2015FasterRT} detector on VOC \texttt{trainval07+12} dataset  using the standard 1 $\times$  standard protocol. For COCO-Object detection and semantic segmentation, we fine tune on the MaskRCNN detector (FPN-backbone) \cite{he2018mask} on COCO \texttt{train2017} split ($118$k images) with the standard 1 $\times$ schedule, evaluating on the COCO 5k \texttt{val2017 split}. We compare to the state of the art SSL methods, including Self-EMD \cite{liu2021selfemd}, DetCon \cite{henaff2021efficient}, BYOL \cite{richemond2020byol}, DenseCL \cite{wang2021dense} and the default MoCo-v2 \cite{chen2020improved}. 
% Our method can also be used with more recent SSL methods such as DenseCL \cite{wang2021dense} (concurrent work to ours).


%Note that we compare most of our results with Self-EMD \cite{liu2021selfemd} and DetCon \cite{henaff2021efficient}, and not with DenseCL \cite{wang2021dense} since they use different hyper-parameter settings than the standard ones present in MoCo-v2 \cite{he2019momentum}. Moreover, our aim is to present a generic method of data augmentation that can be fitted into any of these pipelines and improve these methods.   \\

%% Move to supplementary
%We use a learning rate $0.03$, weight decay of $0.0001$ and momentum of $0.9$. We train on $4$-GPUs with a mini batch-size of $256$ for $800$ epochs. 



%% move to supplementary 
%For linear evaluation, we closely follow MoCo-V2 and use Learning-Rate(LR) of 30 using an SGD optimizer. 




\begin{table*}
    \centering
    \begin{tabu} to \linewidth {lcccccc} 
        \toprule
        % Description & AP & AP_{50} & AP_{75} &AP_{s} & AP_{l} & AP_{m} \\
        Description & $AP$ & $AP_{50}$ & $AP_{75}$ & $AP_{s}$ & $AP_{l}$ & $AP_{m}$ \\
         \midrule
    %   MoCo-V2 \cite{He2020MomentumCF}  & 50.86 & 77.60&55.43\\
    %     MoCo-V2 using object and context crops (Ours)  & 52.91 & 79.61 & 57.96\\
    Supervised (Random Initialization) & 32.8 & 50.9 & 35.3 & 29.9 & 47.9 & 32.0 \\
    Supervised (ImageNet Pre-trained) & 39.7 & 59.5 & 43.3 & 35.9 & 56.6 & 38.6 \\
     \midrule
    MoCo-v2 \cite{chen2020improved}  & 38.2 & 58.9 & 41.6 & 34.8 & 55.3 & 37.8\\
    BYOL \cite{henaff2021efficient}  & 38.8 & 58.5 & 42.2 & 35.0 & 55.9& 38.1\\
    Dense-CL \cite{wang2021dense} & 39.6 & 59.3 & 43.3 & 35.7 & 56.5 & 38.4 \\
    CAST \cite{selvaraju2020casting} (180K steps) & 39.4 &  60.0 & 42.8 & 35.8 & 57.1 & 37.6 \\ 
    Self-EMD \cite{liu2021selfemd} (Uses BYOL)  & 39.8 & 60.0 & 43.4 & - & -& -\\
   
     MoCo-v2 + OAC (Ours)  & \cellcolor{blue!15}\textbf{39.7} & \cellcolor{blue!15}\textbf{60.1} & \cellcolor{blue!15}\textbf{43.4} & \cellcolor{blue!15}\textbf{36.0} & \cellcolor{blue!15}\textbf{57.3} & \cellcolor{blue!15}\textbf{38.8}\\
     
      Dense-CL + OAC (Ours)  & \cellcolor{blue!15}\textbf{40.4} & \cellcolor{blue!15}\textbf{60.4} & \cellcolor{blue!15}\textbf{44.0} & \cellcolor{blue!15}\textbf{36.6} & \cellcolor{blue!15}\textbf{57.9} & \cellcolor{blue!15}\textbf{39.5}\\
     
     
     \midrule
     MoCo-v2 + OAC (Using all losses)(Ours)  & \cellcolor{blue!15}\textbf{40.7} & \cellcolor{blue!15}\textbf{60.9} & \cellcolor{blue!15}\textbf{43.9} & \cellcolor{blue!15}\textbf{36.9} & \cellcolor{blue!15}\textbf{58.3} & \cellcolor{blue!15}\textbf{39.6}\\
     
     BYOL + OAC (Using all losses)(Ours) & \cellcolor{blue!15} \textbf{41.1} & \cellcolor{blue!15} \textbf{61.4} & \cellcolor{blue!15} \textbf{44.2} & \cellcolor{blue!15} \textbf{37.1} & \cellcolor{blue!15} \textbf{59.2} & \cellcolor{blue!15} \textbf{40.1}\\
      
     
     Dense-CL + OAC (Using all losses)(Ours)  & \cellcolor{blue!15}\textbf{41.4} & \cellcolor{blue!15}\textbf{61.5} & \cellcolor{blue!15}\textbf{44.7} & \cellcolor{blue!15}\textbf{37.5} & \cellcolor{blue!15}\textbf{59.5} & \cellcolor{blue!15}\textbf{40.4}\\
     
        \bottomrule
        
    \end{tabu}
    \caption{Object detection (first 3 columns) and Semantic Segmentation (last 3 columns) results on COCO dataset. All SSL models have been pre-trained on COCO dataset and then finetuned on COCO. 
    All other methods are run for $90$K, finetuning iterations. For any SSL method, we compare (BYOL, Moco-v2, Dense-CL) adding  \textbf{O}bject-\textbf{A}ware-\textbf{C}ropping \textbf{(OAC)}  cropping losses improves performance. We see further improvement by adding the proposed rotation and localization losses(last 3 rows).}
    \label{tab:coco_detection}
\end{table*}

\begin{table*}
    \centering
    \begin{tabu} to \linewidth {lccc} 
        \toprule
        Model & $AP$ & $AP_{50}$ & $AP_{75}$ \\
         \midrule
    %   MoCo-V2 \cite{He2020MomentumCF}  & 50.86 & 77.60&55.43\\
    %     MoCo-V2 using object and context crops (Ours)  & 52.91 & 79.61 & 57.96\\
    Supervised (ImageNet-pretraining) & 56.8 & 83.2 & 63.7 \\
     \midrule
    %Self-EMD \cite{liu2021selfemd}  & 53.0 & 80.0 & 58.6\\
    
    MoCo-v2 Scene-Scene crop \citep{chen2020improved}  & 51.5 & 79.4 & 56.4\\
    \midrule
    MoCo-v2 - Obj-Obj+Dilate  crop ($\delta=0.1$) (Ours) & \cellcolor{blue!15} \textbf{53.4} & \cellcolor{blue!15} \textbf{80.1} & \cellcolor{blue!15}\textbf{59.1}\\
    MoCo-v2 - Obj-Obj+Dilate  crop + Rotation ($\delta=0.1$) (Ours) & \cellcolor{blue!15} \textbf{53.8} & \cellcolor{blue!15} \textbf{79.6} & \cellcolor{blue!15}\textbf{60.1}\\
    MoCo-v2 - Obj-Obj+Dilate  crop + Object Localization ($\delta=0.1$) (Ours) & \cellcolor{blue!15} \textbf{54.1} & \cellcolor{blue!15} \textbf{80.6} & \cellcolor{blue!15}\textbf{60.2}\\
    \midrule
    MoCo-v2 - Combined ($\delta=0.1$) OAC (Ours) & \cellcolor{blue!15} \textbf{54.6} & \cellcolor{blue!15} \textbf{81.0} & \cellcolor{blue!15}\textbf{60.6}\\
     \bottomrule
    \end{tabu}
    \caption{Object detection results on VOC dataset (OHMS pre-training, using LOD proposals). All models have been pre-trained on OHMS and then fine-tuned on VOC. We can see that all of our proposed components improve upon the baseline scene scene crop and combining all of them improves upon the baseline by +3.1 mAP. }
    \label{tab:voc_moco}
    
\end{table*}


\begin{table*}
    \centering
    \begin{tabu} to \linewidth {lcccccc} 
        \toprule
        % Description & AP & AP_{50} & AP_{75} &AP_{s} & AP_{l} & AP_{m} \\
        Description & $AP$ & $AP_{50}$ & $AP_{75}$ & $AP_{s}$ & $AP_{l}$ & $AP_{m}$ \\
        
    %   MoCo-V2 \cite{He2020MomentumCF}  & 50.86 & 77.60&55.43\\
    %     MoCo-V2 using object and context crops (Ours)  & 52.91 & 79.61 & 57.96\\
    
     \midrule
    COCO: MoCo-v2 (Scene-Scene crop) & 38.2 & 58.9 & 41.6 & 34.8 & 55.3 & 37.8\\
    
   
   
    COCO: MoCo-v2 + OAC (Ours) & \cellcolor{blue!15}\textbf{41.3} & \cellcolor{blue!15}\textbf{61.8} & \cellcolor{blue!15}\textbf{44.7} & \cellcolor{blue!15}\textbf{37.3} & \cellcolor{blue!15}\textbf{58.5} & \cellcolor{blue!15}\textbf{40.1}\\
    \midrule
    VOC: MoCo-v2 (Scene-Scene crop) & 56.1 & 81.3 & 61.3 & - & - & -\\
    VOC: MoCo-v2 + OAC (Ours)  & \cellcolor{blue!15} \textbf{58.8} & \cellcolor{blue!15} \textbf{83.6} & \cellcolor{blue!15}\textbf{64.9} & - & - & -\\
        \bottomrule
    \end{tabu}
    \caption{Object detection (first 3 columns) and semantic segmentation (last 3 columns) results on COCO (first 2 rows) and VOC (last 2 rows). All SSL models have been pre-trained on complete OpenImages dataset(1.9 million images) for 75 epochs and then finetuned on COCO and VOC dataset.}
    \label{tab:openimages_full_results}
\end{table*}

%\subsection{Results}
Table \ref{tab:ssl_comparison_classification} and Table \ref{tab:openimages_moco} shows results on OHMS dataset.  We can see that Obj-Obj+Dilate crops outperform the baseline by $8.2$ mAP, closing the gap between supervised learning and MoCo-v2 baseline by almost $50\%$. 
% Obj-Scene crops also outperform the baseline by $8.1$ mAP.  Obj-Obj+Shift crops ($54.1$ mAP) do not perform as well as Obj-Obj+Dilate or Obj-Scene. We believe this is because the second object crop is chosen at a certain minimum radius from the first (otherwise, the overlap between the crops is too large to produce meaningful learning). The second object crop, therefore, contains only part of the object.
In Obj-Obj+Dilate crops, a dilated object crop would potentially contain the entire object and more scene information; therefore, the representation from the dilated object-crop contains complementary information from both the object and the scene.  
% Combining all three cropping methods works slightly worse than simply using object-scene crops for OpenImages. 
We also show in Table \ref{tab:openimages_moco} an ablation with ground truth bounding boxes being used to guide the object cropping. This performs marginally better than the use of LOD crop, suggesting that a tight fit around the object is not necessary for improved representations. 

% We also perform Transfer learning experiments on VOC dataset using OpenImages pre-training (present in the supplementary materials). Even in the transfer learning setup Object-Scene crops and Dilated-Object crops perform better than Object-Object crops.


% Finally, in Table \ref{tab:ssl_comparison_classification}, we show results on ImageNet. Since objects in ImageNet are large and centered, we expect object-scene cropping to perform at par with scene-scene cropping. This is indeed the case: object-scene crop performs better by a small margin of $0.1\%$ in top-1 accuracy.

% add back this line if we show dilated 
%One of the reason this could be that BING not only gives object proposals it also introduces some extra context. This extra context might be helping the SSL methods produce representations which work well. \\ \ref{tab:voc_moco} \ref{tab:object-object-analysis}

Table  \ref{tab:coco_detection} shows results on object detection and semantic segmentation for COCO (by pre-training on COCO \texttt{trainval2017} datasets and finetuning on COCO). We train MoCo-v2, BYOL and Dense-CL models. Our MoCo-v2 Obj-Obj+Dilate cropping outperforms MoCo-v2 Scene-Scene baseline.  Our proposed cropping is agnostic to the pre-training SSL method; we show  results by adding our approach to Dense-CL \cite{wang2021dense}. We also outperform the CAST model \cite{selvaraju2020casting} which also uses localized crops based on saliency maps: our approach is simpler and performs better by around $1.4$ mAP. Table 1 (appendix) shows results of object detection on PASCAL-VOC. We pre-train on COCO and then fine-tune on VOC. OAC cropping outperforms the MoCo-v2 baseline by $3.2$ mAP and the BYOL baseline by $2.5$ mAP. Improved results on iconic datasets like Aircraft, Birds and Cars can be found in appendix (Table~4). Additional results on varying number of proposals used can be found in appendix (Table~6). 

In addition to small OHMS dataset we also show results on full OpenImages dataset.
Table \ref{tab:openimages_full_results} shows results  on object detection and semantic segmentation for COCO and object-detection on VOC by pre-training on full OpenImages dataset \cite{kuznetsova2020open} (all 1.9 million images) for 75 epochs. We show improved performance over the baseline on both object detection and semantic segmentation tasks by using Obj-Obj+Dilate crops. Our proposed dilation method works not only for small multi-object datasets like COCO but also for datasets like OpenImages and performs well under a transfer learning setup.

{\textbf{Results on ADE20K:} We also show results on ADE20k following \cite{Chen2017RethinkingAC}. The baseline MoCo-v2(COCO pre-training) gets 37.5 mIoU, while we get 39.2 mIoU. Similar to VOC and COCO, we see consistent performance improvement on ADE20k as well.}

{\textbf{Results on Full OpenImages:} We also report classification results on full OpenImages dataset i.e 1.7 million images and on 212k random subset in Table \ref{tab:openimages_full_subset}. For the full dataset pre-training has been done for 100 epochs and for the random subset the pre-training has been done for 200 epochs. We can see that on the full dataset, the gap between supervised learning baseline and MoCo-v2 is still large, while the gap on random subset is not as big.}

{
\begin{table*}
    \centering
    \begin{tabu} to \linewidth {l|c|c|c|c|c|c} 
        \toprule
        Model & Dataset & Crops & Obj-Obj+Dilate & Scene-Scene  & mAP \\
        
         \midrule
          
         Supervised  & Full OpenImages & - & - & -  &  74.0 \\ 
        %  \midrule
        
     MoCo-v2  & Full OpenImages &- & -  & \Checkmark &  50.5\\
      MoCo-v2 + OAC & Full OpenImages   & \Checkmark  &  - & - &  62.1 \\
      \midrule
          
         Supervised  & Random Subset OpenImages & - & - & -  &  74.0 \\ 
        %  \midrule
        
     MoCo-v2  & Random Subset OpenImages &- & -  & \Checkmark &  50.5\\
      MoCo-v2 + OAC & Random Subset OpenImages   & \Checkmark  &  - & - &  62.1 \\
     
     
        \bottomrule
    \end{tabu}
    \vspace{-0.05in}    
  \caption{{Results on full OpenImages and random subset of OpenImages. On full OpenImages dataset the difference between supervised learning and MoCo-v2 is still large, showing that random cropping is an issue not just on OHMS but also on the full dataset.}}  
  \label{tab:openimages_full_subset}    
\end{table*}
}

{\textbf{Results on ImageNet:}} We also show improved results on ImageNet pre-training using object-aware cropping (Table~8 appendix) and MoCo-v2. For object detection on VOC2007, we see an improvement of 1.0 mAP; and a $0.5$ mAP improvements for object detection on COCO. Our approach is thus adaptable to the pre-training dataset and SSL algorithm. 


%The BING crop is close to $82\%$ of the image (on average). Therefore, we simply take standard random crops on the BING crops to generate both views. We then fine-tune our model on standard object detection task on VOC2007 and see an improvement of 1.0 mAP (Table~\ref{tab:imagenet_results} Supplementary). We also observe a $0.5$ mAP improvements for object detection on MS-COCO dataset with ImageNet pre-training. Small improvements on ImageNet can be attributed to the fact that ImageNet images contain a single object and hence random crops have very high chance of picking up objects.  


% \vspace{-5pt}

% We also show results on classification on the COCO dataset as shown in the Table \ref{tab:openimages_coco}. We can see that we improve by 5.2 MAP on COCO-Classification. 


% \begin{table*}
%     \centering
%     \begin{tabu} to \linewidth {lccc} 
%         \toprule
%         Description & radius & classification-map \\
%          \midrule
%          Supervised OpenImages & - & 66.3
%         \\ 
%          \midrule
%       MoCo-V2 \cite{He2020MomentumCF}  & - & 49.8\\
%      MoCo-V2  using random and ground-truth bounding boxes  & - & 58.2\\   
%     MoCo-V2  using random and BING crops  & - & 58.1\\
%      \midrule
%     MoCo-V2 using object and object crops  &  100 & 54.1\\
%      \midrule
%      MoCo-V2  combining all three methods &  80 - 100 & 55.7\\     
%     MoCo-V2  combining all three methods &  100 - 120 & 57.1\\
%  MoCo-V2 combining all three methods &  120 - 140 & 52.8\\
%     MoCo-V2 combining all three methods &  140 - 160 & _\\
%     MoCo-V2 combining all three methods &  160 - 180 & 51.8\\
%     MoCo-V2 combining all three methods &  180 - 200 & 51.2\\
%      MoCo-V2 combining all three methods &  200 - 240 & 51.5\\
%         \bottomrule
%     \end{tabu}
%   \caption{Classification results on OpenImages dataset.}  
%   \label{tab:openimages_moco}    
% \end{table*}






% \begin{table*}
%     \centering
%     \begin{tabu} to \linewidth {lccccc} 
%         \toprule
%         Method & Object-Object & Object-Scene & Scene-Scene & Radius & mAP \\
%          \midrule
%          Supervised OpenImages & - & - & - & - &  66.3 \\ 
%          \midrule
%      MoCo-v2 (Scene crops) & - & - & \Checkmark & - & 49.8\\
%          % MoCo-V2( BING crops)  & _ & \checkmark & _ & _ & 58.1\\
%     MoCo-v2( Edge crops) & - & \Checkmark  & - & - & 57.1\\
     
%     MoCo-v2 & \Checkmark  & -  & - & - & 54.1\\
%     % \midrule
%     %  \midrule
% %     MoCo-V2 using object and object crops  &  100 & 54.1\\
% %      \midrule
    
%     %  MoCo-V2  & \Checkmark & \Checkmark  & \Checkmark &  80 - 100 & 55.7\\    
%      MoCo-v2  & \Checkmark & \Checkmark  & \Checkmark &  100 - 120 & 57.1\\  
%     %  MoCo-V2  & \Checkmark & \Checkmark  & \Checkmark &  120 - 140 & 53.2\\  
%     %  MoCo-V2  & \Checkmark & \Checkmark  & \Checkmark & 160 - 180 & 52.3\\  
%     %  MoCo-V2  & \Checkmark & \Checkmark  & \Checkmark &  180 - 200 & 51.8\\  
%     %   MoCo-V2  & \Checkmark & \Checkmark  & \Checkmark & 200 - 240 & 50.9\\  
% %     MoCo-V2  combining all three methods &  100 - 120 & 57.1\\
% %  MoCo-V2 combining all three methods &  120 - 140 & 52.8\\
% %     MoCo-V2 combining all three methods &  140 - 160 & _\\
% %     MoCo-V2 combining all three methods &  160 - 180 & 51.8\\
% %     MoCo-V2 combining all three methods &  180 - 200 & 51.2\\
% %      MoCo-V2 combining all three methods &  200 - 240 & 51.5\\
%  %\midrule
%     MoCo-v2 (GT boxes) & - & \Checkmark & - & - & 58.2\\ 
%     MoCo-v2 (BING crops)   & - & \Checkmark & - & - & 58.1 \\   
%     BYOL (BING crops)   & - & \Checkmark & - & - & 58.5 \\
%         \bottomrule
%     \end{tabu}
%   \caption{Comparing different crop approaches on OpenImages: we show that by using BING crops to generate one positive, and a scene crop for the other positive, we are able to reduce the difference between SSL and Supervised Learning by close to $50\%$ (compare last two rows to first two rows). Using ground-truth boxes to generate crops from OpenImages improves the pre-training performance marginally compared to BING crops. Other combinations such as using object-object or all three crops underperform object-scene crops.}  
%   \label{tab:openimages_moco}    
% \end{table*}
% \begin{table*}
%     \centering
%     \begin{tabu} to \linewidth {lcccc} 
%         \toprule
%         Description & tau & AP & AP_{50} & AP_{75} \\
%          \midrule
%     %   MoCo-V2 \cite{He2020MomentumCF}  & 50.86 & 77.60&55.43\\
%     %     MoCo-V2 using object and context crops (Ours)  & 52.91 & 79.61 & 57.96\\
%     Supervised & - & 56.8 & 83.2 & 63.7 \\
%      \midrule
%     MoCo-V2 \cite{He2020MomentumCF}  & 0.2 & 50.8 & 77.6 & 55.4\\
%     \midrule
%     MoCo-V2 using object and context crops  & 0.2 & 54.0 & 79.9 & 59.1\\
%      MoCo-V2 using object and context crops  & 0.3 & 54.2 & 80.1 & 60.0\\
%      MoCo-V2 using object and context crops  & 0.4 & 54.1 & 80.0 & 60.0\\
%      MoCo-V2 using object and context crops  & 0.5 & 53.7 & 80.2 & 59.2\\ 
%         \bottomrule
%     \end{tabu}
%     \caption{Impact of varying tau. The models have been pre-trained on COCO dataset and then transferred to VOC.}
%     \label{tab:voc_analysis_tau}
% \end{table*}



% \begin{table*}
%     \centering
%     \begin{tabu} to \linewidth {lcccc} 
%         \toprule
%         Description & radius & AP & AP_{50} & AP_{75} \\
%          \midrule
%     %   MoCo-V2 \cite{He2020MomentumCF}  & 50.86 & 77.60&55.43\\
%     %     MoCo-V2 using object and context crops (Ours)  & 52.91 & 79.61 & 57.96\\
%     Supervised & - & 56.8 & 83.2 & 63.7 \\
%      \midrule
%     MoCo-V2 \cite{He2020MomentumCF}  & - & 50.8 & 77.6 & 55.4\\
%     \midrule
%     MoCo-V2 using object and context crops  & 80 - 100 & 53.9 & 80.1 & 59.3\\
%      MoCo-V2 using object and context crops  & 100 - 120 & 53.6 & 80.0 & 59.2\\
%      MoCo-V2 using object and context crops  & 120 - 140 & 53.6 & 80.1 & 60.0\\
%      MoCo-V2 using object and context crops  & 140 - 160 & 53.7 & 80.1 & 59.6\\ 
%         \bottomrule
%     \end{tabu}
%     \caption{Impact of varying radius and using all three modalities. The models have been pre-trained on COCO dataset and then transferred to VOC.}
%     \label{tab:voc_analysis_radius}
% \end{table*}













% \begin{figure*}[h!]
%   \centering
%   \begin{subfigure}{\linewidth}
%     \includegraphics[width=\linewidth]{LaTeX/Images/bing.pdf}
%   \end{subfigure}
%   \caption{This figure shows original image, the object extracted from BING model and the context image. }
%   \label{fig:bing_crops}
% \end{figure*}

% \begin{figure}[h!]
%   \centering
%   \begin{subfigure}{\linewidth}
%     \includegraphics[width=\linewidth]{LaTeX/Images/openimages_crop.pdf}
%   \end{subfigure}
%   \caption{This figure shows original image, and the two random crops. We can see that these random crops often miss the main object and may capture non-relevant background information. }
%   \label{fig:random_crops}
% \end{figure}
