\section{Proposed Approach}
\label{sec:model}
%Our hypothesis is that scene cropping has a high likelihood of including a lot of information irrelevant to the object. This effect is more pronounced when the objects are small relative to the size of the image. 
% of missing objects when the objects are small relative to the size of the image. 
%This leads to suboptimal representations for OpenImages due to the inclusion of data that is weakly correlated with the objects of interest. 

Our analysis shows that SSL methods based on either purely object crops, or purely scene crops, both perform worse than supervised learning. This suggests a third option: incorporating \emph{both} object and scene information in the crops. Our hypothesis is that context around an object is helpful to learn robust representations due to the natural correlations between scene and object \citep{Hinton2021HowTR}. However, it is unclear what mix of object and scene information is most helpful in learning robust representations. Our main contribution is a simple and effective approach to achieving this cropping mechanism. We also experiment with a number of plausible baselines to incorporate scene and object information.

To enable object-awareness, we consider three object proposal models: BING \citep{Cheng2014BINGBN} and Edge-Boxes \citep{Zitnick2014EdgeBL}, both of which are trained with boxes from Pascal-VOC; and an unsupervised object proposal method \citep{vo2019unsupervised}. BING \citep{Cheng2014BINGBN} uses the norm of gradients within a fixed window size as a simple feature that is fed into a cascaded SVM framework to make object proposals. This method is trained on the ground-truth bounding boxes of $2501$ images of the PASCAL-VOC \citep{everingham2010pascal} training set. Note that BING does not use any class label, but only label independent bounding box information. BING has the advantage of extremely fast performance (>$125$ fps on an image of size $300 \times 300$), generates many proposals, and generalizes well to many datasets, as shown in \citet{Cheng2014BINGBN} and seen in our empirical results. This makes BING well suited as an object proposal algorithm. In our experiments, we generate up to $10$ proposals per image and select one object uniformly at random among these proposals. The details of other object proposals are present in supplementary Section \ref{sec:object_proposal_methods}. Our results in Table \ref{tab:voc_moco} show that other object proposal methods provide sufficient quality and the choice may be mostly dependent on the speed of the method. We experiment with four types of cropping in the rest of the paper: 

%Due to the fast performance of BING, we use it in all subsequent experiments. 

%Objects with well-defined closed boundaries are strongly correlated large gradient norms. Objectness of an image window, they resize it to 8 Ã— 8 and use the norm of the gradients as a simple 64D feature  for learning a generic objectness measure in a cascaded SVM framework. They further use a binarized version of the Normed Gradient features to significantly speed up their processing. BING relies on the low-level cues concerning the object boundaries and has been shown to be very generalizable and can predict category independent object proposals. BING has been shown to effective in plethora of tasks such as multi-label image classification \citep{Wei2016HCPAF}, semantic segmentation \citep{pinheiro2015imagelevel}, video-classification \citep{zha2015exploiting}, co-salient object detection \citep{ZhangSaliency}, deep multi instance learning \citep{7298968} and video-summarization \citep{Lee_2015}. The high efficiency and speed of BING makes it suitable for  generating object proposals in our datasets. So, we use BING and generate 10 proposals for every image in the OpenImages-Subset and MS-COCO \citep{Lin2014MicrosoftCC} datasets. We randomly pick one out of the 10 generated proposals as a positive sample and Random-Resized crop as other positive sample. We then train MOCOV2 \citep{chen2020improved} and BYOL \citep{grill2020bootstrap} on both of these datasets.
% We used these bounding boxes to pre-train MOCO so that we are able to work on non-iconic datasets like OpenImages \cite{Kuznetsova_2020}. \\


%Unsupervised object discovery methods try to detect the objects present in an image and also structure of the object collection \citep{vo2019unsupervised,cho2015unsupervised} in a completely unsupervised manner. Similar to BING we use \citep{vo2019unsupervised} to generate object proposals. \citep{vo2019unsupervised} uses a robust matching technique which relies on appearance and geometric consistency constraints to assign confidence and saliencey scores to region proposals. However as compared to BING this method is very slow and unlike BING is very difficult to plug in into various deep learning pipelines. Hence we mostly focus on using BING as our default proposal generation method.


%\subsection{Proposed method}
% % \TODO{add object random crop baseline information which used GT boxes}
% We now discuss the different type of cropping which we consider for an image.

% For training contrastive SSL methods, we treat the randomly cropped patches on the pairs of images selected these cropping strategies as positive views of a given image and other randomly sampled images as negative views. 
% Note that for creating positive samples we randomly crop on output from both the cropping strategies.
% in the mini-batch
% We begin by describing methods which take in extra context and scene level information followed by other object level methods. 

%To achieve this we treat the proposals given by BING as object crops and random-resized crops as scene crops.
%Our aim is to learn both object and scene level representations, to this end we consider three kinds of combinations of cropping; Object-Scene cropping, Object-Object cropping and Scene-Scene cropping. Now we discuss these methods in detail:\\

% \textbf{Adding context and scene level information:}
% To add scene level information we try two different methods. We now describe both of these methods in detail. 

\textbf{(a) Dilated object proposals (Obj-Obj+Dilate Crop):} 
Scene pixels spatially close to the object are more likely to have a positive correlation with the object. With this intuition, we generate the second view by dilating the BING proposal. We dilate the box by $10\%$ or $20\%$ of the image size, followed by a random crop. Changing $\delta$ gives us control over how much scene information is incorporated (a value of $10\%$ works well in most cases). Note that the original and dilated boxes are both followed by a random crop, ensuring that the first view is not trivially included in the second view. The choice of which crop to use as query or key is arbitrary and both object and dilated object crops can be used as either key and query.  We also consider other baselines listed below which are other plausible approaches to incorporating scene information. However, as shown in the results Section \ref{secc:results}, dilated crops are the best performing method. 

\textbf{(b) Scene-Scene Crop}: We take two random crops in an image and treat them as positive views. This is the default approach used in MoCo-v2 and other SSL approaches. From our analysis in Section \ref{sec:analysis}, this approach performs poorly on datasets such as OpenImages. 

\textbf{(c) Shifted object proposals (Obj-Obj+Shift Crop)}: Unlike Obj-Obj+Dilate, here, the second view is a box selected at random within a pre-specified distance range of the first box (the BING proposal). To choose a pre-specified distance, we choose a random value for the offset from a few ranges: $80$-$100$ pixels; $100$-$120$ pixels etc. 

% Object-Object crop in principle is similar to standard cropping used in SSL approaches like MoCo-v2 \citep{chen2020improved} on ImageNet, since the crops have good amount of overlap with the object.   
% One potential drawback of an object-focused crop is that it ignores context information (scene) which can be useful in disambiguating objects that may be occluded or blurry. 

% \textbf{Value of lower scale of Random-Resized crop for object crops:}
% The Object-Crop usually are not very small and cover a decent portion of the image; on COCO dataset it covers about 39\% of the image and on OpenImages it cover 23\% of the image. So, we instead of using standard scale of 0.2 as the lower scale, we calculate the scale for every dataset. For ex. in case of COCO dataset, since object crops cover 40\% of the image we use 0.4 as the lower scale. Similarly, in case of OpenImages we use 0.2 as the lower scale since object crops cover close to 20\% of the image.
\textbf{(d) Random crop (Obj-Scene Crop):} The first view is the BING proposal; and a regular random crop at scene level is the second view. This method provides information at both object and scene levels, and we use it as a baseline for adding context information to the model. 
%Our hypothesis is that this approach to cropping should result in better representations since it encompasses object-centric and contextual (scene) information. Our experiments in Section \ref{secc:results} support this hypothesis. 

% Let's denote the dilation by $\delta$, and the height and width of the image by $H,W$.  The vertices of the BING crop in the original image are $X_1,Y_1$(top left corner) and $X_2,Y_2$(bottom right corner). So the size of dilated crops will be. 
% \begin{eqnarray*}
% X_1\_dilated = X_1\_dilated - \delta * W , 
% Y_1\_dilated = Y_1\_dilated - \delta * H \\
% X_2\_dilated = X_2\_dilated + \delta * W ,
% Y_2\_dilated = Y_2\_dilated + \delta * H
% \end{eqnarray*}
\paragraph{Discussion on lower scale of random resized crop:}
As stated above, for both the Obj-Obj+Dilate and Obj-Obj+Shift crops, we use a random crop on the BING object proposal or its shifted or dilated version, to generate the final views. Since the object proposal itself is a small fraction of the image (e.g. in COCO, an object crop typically covers about 39\% of the image), using the usual default lower value for the random crop range (usually $0.2$) works poorly as it results in extremely small crops from the image. Therefore, we set the lower limit such that it matches the minimum sized crop in case of the usual scene crop ($s_{\text{min}} = \frac{0.2}{\text{average BING proposal size}}$) . 

%For ex. in case of COCO dataset, since object crops cover 40\% of the image we use 0.4 as the lower scale. Similarly, in case of OpenImages we use 0.2 as the lower scale since object crops cover close to 20\% of the image.

% \textbf{OARC-Scene Crop using dilation}: \\


% \textbf{Combining all three crops}: We also experiment with combining the $3$ approaches above by randomly sampling any one of these crop types with equal probability. This approach leads to very minimilastic overhead and gives good performance on COCO and OpenImages datasets. 
% In practice we find that it does performs similar to and in some cases slightly better than the simpler object-scene crop approach above.

%\textbf{Different projection heads for Object and Scene crops:}
\paragraph{Different projection heads for Object and Scene crops:}
The projection head, introduced in \citet{chen2020simple} is an important component of most SSL methods. This is a deep network that maps representations from the encoder backbone to a lower-dimensional space where the loss function is applied. In \citet{chen2020exploring}, a single projection head is used for both views. We find it beneficial for performance to use two different projection heads: one for the first view (BING crop) and another for the second view for obj-scene, obj-obj+dilate and obj-obj+shift crops. For scene-scene crops we always use a single projection head. We hypothesize that the different projection networks specialize to either an object-specific representation, or to a representation that incorporates scene information. 

%$\bm z_i = g(\bm h_i)=W^{(2)}\sigma(W^{(1)}\bm h_i)$ where $\sigma$ is a ReLU non-linearity, and $\bm h_i = f(\tilde{\bm x}_i) = \mathrm{ResNet}(\tilde{\bm x}_i)$ where $\bm h_i\in \mathbb{R}^d$ is the output after the average pooling layer. They show that $g(\cdot)$ can remove information that may be useful for the downstream task, such as the color or orientation of objects.  We hypothesize that Object and Scene crops often contain different object orientation and color information; hence we propose to use different projection heads for scene and objects. This gives us
% \begin{eqnarray*}
%$\bm z\_obj = (W_{object}^{(2)}\sigma(W_{object}^{(1)}\bm h_i))$
%and $\bm z\_scene = (W_{scene}^{(2)}\sigma(W_{scene}^{(1)}\bm h_i))^T $.
% \end{eqnarray*}

Other than the above change to the cropping methodology, we leave the rest of the SSL pipeline unchanged: other data augmentations such as color shifts are applied in the usual manner, and the loss functions and training regimes are also unchanged. Our approach therefore involves really simple changes to most SSL pipelines, involving only a few lines of code. 



