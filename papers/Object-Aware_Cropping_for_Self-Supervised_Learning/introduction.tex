\section{Introduction}
In recent works on self-supervised learning (SSL) of image representations, the most successful approaches have used data augmentation as a crucial tool \citep{chen2020simple,he2019momentum,grill2020bootstrap,tian2019contrastive,NEURIPS2020_70feb62b}. Given a randomly chosen image sample, augmentations of the image are generated using common image transformations such as cropping and resizing a smaller region of the image, color transformations (hue, saturation, contrast), rotations etc. \citep{chen2020simple,Gidaris2018UnsupervisedRL}. Of these augmentations, the use of cropping is clearly the most powerful (see \cite{chen2020simple}, Fig. 5). This makes intuitive sense: cropping followed by resizing forces the representation to focus on different parts of an object with varying aspect ratios. This makes the representation robust to such natural transformations as scale and occlusion. The implicit assumption in this scheme is that the object of interest (classification or detection target) occupies most of the image and is fairly centered in the image, so that random crops of the image usually result in (most of) the object still being present in the cropped image. Such an assumption holds for ``iconic'' datasets such as ImageNet \cite{NIPS2012_c399862d}. Forcing the resulting representations to be closer together maximizes the mutual information between the crops (also called \textit{views}) \cite{Oord2018RepresentationLW,tian2019contrastive}. % which encourages the representation to carry object information in addition to scene information. In fact, state-of-the-art self-supervised methods based on random cropping have a gap of less than $5\%$ to fully supervised learning on ImageNet \cite{he2019momentum,NEURIPS2020_70feb62b}.

However, in the case of ``non-iconic'' datasets such as OpenImages \cite{kuznetsova2020open} and COCO \cite{Lin2014MicrosoftCC}, the objects of interest are small relative to the image size and rarely centered, see Fig.~\ref{fig:teaser_figure}. These datasets are more representative of real-world uncurated data. We find that the default random cropping approach (which we call \textit{scene} cropping) leads to a significant reduction in performance for self-supervised contrastive learning approaches. For example, using the default pipeline of MoCo-v2 \cite{chen2020improved}, we find that there is a gap of $16.5\%$ mean average precision (mAP) compared to fully supervised learning. Other state of the art methods such as BYOL \cite{grill2020bootstrap}, SwAV \cite{NEURIPS2020_70feb62b} and CMC \cite{tian2019contrastive} perform poorly as well (see Table \ref{tab:ssl_comparison_classification}). As we show, the core problem here is that random scene crops do not contain enough information about (small) objects, causing degraded representation quality. 
\begin{figure}[t!]
  \centering
  \begin{subfigure}{\linewidth}
    \includegraphics[width=\linewidth]{LaTeX/Images/teasure_figure_with_boxes_four_images_2.png}
  \end{subfigure}
  \caption{Illustration of object aware cropping. Top-Left: We show the original image with random crops overlaid. Bottom (red panel): Overlap between random crops tend to miss the object of interest. Top-Right: We show crops generated from the LOD \cite{Vo2021LargeScaleUO} algorithm and also the dilated object crop. Bottom-Right (green panel): We use LOD-based object-aware crops. These incorporates both object and scene information into the MoCo-v2 (or other SSL frameworks).}
  \label{fig:teaser_figure}
  \vspace{-0.1in}
\end{figure}


\begin{table*}
    \centering
    \begin{tabu} to \linewidth {lcc} 
        \toprule
        Method & Dataset  & mAP \\ 
     \hline
     Supervised  & - & 53.26 \\ 
     \hline
     InsDis(\cite{wu2018unsupervised}) &  ImageNet & 48.82 \\ 
     MoCo( \cite{he2020momentum} ) &  ImageNet & 50.51 \\ 
     PCL-v1(\cite{li2021prototypical}) &  ImageNet & 53.93 \\ 
     PCL-v2(\cite{li2021prototypical}) &  ImageNet & 53.92 \\ 
     MoCo-v2(\cite{chen2020exploring}) & ImageNet & 50.67 \\ 
     MoCHI(\cite{kalantidis2020hard}) & ImageNet & 52.61 \\ 
     MMCL(\cite{shah2021max}) & ImageNet & 50.73 \\
     MoCo-v2 + OAC (Ours) & ImageNet &  {53.96} \\
     
     \hline
    MoCo-v2 (Baseline) & {OHMS} &  {54.74} \\
     MoCo-v2 + OAC (Ours) & OHMS &  \textbf{57.13} \\ 
        
        \bottomrule
    \end{tabu}
    \vspace{0.1in}
    \caption{We achieve superior performance on VOC detection when pre-training on our proposed \textbf{O}penImages \textbf{H}ard \textbf{M}ulti-object \textbf{S}ubset \textbf{OHMS} dataset  as compared to ImageNet trained models by 6.52mAP. Our proposed OAC also helps ImageNet (2nd last row), but it helps much more on the proposed OHMS multi-object dataset. ImageNet baselines has been trained for 100 epochs~\cite{shah2021max} and OpenImages model have been trained for same 100 ImageNet equivalent epochs.}
    \label{tab:ssl_comparison_teaser_detection}
\end{table*}

% \begin{figure}
%   \centering
% %   \begin{figure}[t]{0.48\textwidth}
% %   \centering
%     \begin{tabular}{ c|c|c } 
%      \hline
%      Method & #Iterations & mAP \\ 
%      \hline
%      Supervised & 128 million & 53.26 \\ 
%      \hline
%      InsDis(\cite{wu2018unsupervised}) & 128 million & 48.82 \\ 
%      MoCo( \cite{he2020momentum} ) & 128 million & 50.51 \\ 
%      PCL-v1(\cite{li2021prototypical}) & 128 million & 53.93 \\ 
%      PCL-v2(\cite{li2021prototypical}) & 128 million & 53.92 \\ 
%      MoCo-v2(\cite{chen2020exploring}) & 128 million & 50.67 \\ 
%      MoCHI(\cite{kalantidis2020hard}) & 128 million & 52.61 \\ 
%      MMCL(\cite{shah2021max}) & 128 million & 50.73 \\ 
%      \hline
%      Ours & 42 million & 54.2 \\ 
%      \hline
%      \caption{We achieve superior performance on VOC detection when pre-training on OHMS  as compared to ImageNet trained models by just training for $\sim{1/3}$ of the iterations.}
%      \label{fig:baseline_comparison_based_on_iterations}
    
%     \end{tabular}
   
% %   \end{figure} 
% %   ~
% %   \begin{subfigure}{\linewidth}
% % %   \centering
% %     \includegraphics[width=\linewidth]{LaTeX/Images/tmlr_teasure_fig_based_on_iterations.png}
% %     \caption{We achieve superior performance on VOC detection when pre-training on OHMS  as compared to ImageNet trained models by just training for $\sim{1/3}$ of the iterations.}
% %   \label{fig:baseline_comparison_based_on_iterations}
% %   \end{subfigure}
% %   \vspace{-0.3in}
%     % \caption{}

% \end{figure}


However, merely switching from scene-level crops to purely object-level crops does not exploit the correlations that exist between scenes and objects in most natural images. These correlations are helpful for downstream tasks \cite{xiao2020noise}. Keeping this in mind, we introduce \textit{\textbf{O}bject-\textbf{A}ware \textbf{C}ropping}(OAC), which applies a simple pre-processing step using the unsupervised object proposal method LOD \cite{Vo2021LargeScaleUO}. LOD outputs multiple object proposal rectangles, one of which we pick at random as the first candidate region. We then expand (dilate) this rectangle to create a second candidate region. We finally employ random cropping within each of these rectangles to create a final pair of ``positive" views that are used for the SSL loss. The use of random cropping reduces mutual information between the views, thereby making the pretext task harder for SSL losses, and improving final representation quality. We call this cropping approach ``obj-obj+dilate" in the rest of the paper. 

In addition to an SSL loss that uses multiple views, we introduce two additional unsupervised losses which leverage the LOD proposal introduced above. The first loss, which we call object localization, encourages the network's representations to carry information about objectness by predicting which patch features contain the object (labels are determined by the extent of the LOD proposal). The second loss we add is the rotation prediction task where given a rotated object and dilated-object, we predict the rotation of the object. Rotation loss helps is learning better object level representations.
Use of these losses leads to further improvements to downstream performance.

% (1) scene-level random crop to incorporate both object and scene context (we call this setting \emph{obj-scene}); (2) a dilated version of the BING proposal followed by random crops applied to both views (\emph{obj-obj+dilate}) (3) a second crop which applies a random shift to the BING proposal (\emph{obj-obj+shift}). The baseline applies scene-level random crops to both views (\emph{scene-scene}).

% For the second view, we experiment with the following variants to incorporate both object and scene information into the representation: (1) scene-level random crop to incorporate both object and scene context (we call this setting \emph{obj-scene}); (2) a dilated version of the BING proposal followed by random crops applied to both views (\emph{obj-obj+dilate}) (3) a second crop which applies a random shift to the BING proposal (\emph{obj-obj+shift}). The baseline applies scene-level random crops to both views (\emph{scene-scene}). 

% We find that which variant of works better is dependent on pre-training dataset characteristics.

%We remedy this by introducing \emph{object-aware} cropping, which applies a simple preprocessing step using the BING algorithm \cite{Cheng2014BINGBN} to select regions on the image based on an ``objectness" criterion. We use the simple but effective approach of randomly selecting from among the object proposals output by BING. However if we just focus on object level representations we will not be able to leverage context/scene information. As shown in \cite{xiao2021what} scene information is often useful for downstream tasks since at inference time we input a scene rather than a tightly cropped object. Hence we use the \emph{object-aware} cropping as the first positive sample and analyze various kinds of context based cropping as the second the positive sample. To incorporate scene information we consider two variants. First we just use a random crop in an image as scene information. However if we use any random crop as scene information, there is a chance that sometimes the model may co-relate object with random patch in the background which might lead to sub-optimal representations.  To remedy this we instead dilate the \emph{object-aware} crop by some amount in all the directions to get context information. We see that for multi object datsets like OpenImages \cite{kuznetsova2020open} and MS-COCO \cite{Lin2014MicrosoftCC} by using the dilated crops as scene crop and object crops together gives us the best performance best representation for downstream tasks such as classification, object detection, and semantic segmentation instead of only object-level or only scene-level information. On the other hand for object-centric datasets like ImageNet we find that object-object crops gives us the best downstream performance. We also analyze the correlation between pre-training task and downstream tasks. We find that object-object training often leads to better representation if the downstream tasks involve single object classification. Object-Context models perform better when downstream tasks involves predicting multiple objects for example object detection and semantic segmentation. 


% The best results are obtained by combining object and scene cropping. This suggests that using the information at both object and scene levels provides the best representation for downstream tasks such as classification, object detection, and semantic segmentation instead of only object-level or only scene-level information. 

We conduct a number of experiments incorporating object-aware cropping, finding consistent improvements on state of the art self-supervised methods such as MoCo-v2 \cite{chen2020improved}, BYOL \cite{grill2020bootstrap} and Dense-CL \cite{wang2021dense} across varied datasets and tasks. 
We also propose \textbf{O}penImages \textbf{H}ard \textbf{M}ulti-object \textbf{S}ubset \textbf{OHMS}, which is a balanced subset of OpenImages and has images with atleast two different classes.
We show by pre-training on the OHMS dataset using our OAC can give superior performance on object detection task (+6.1mAP, Table \ref{tab:ssl_comparison_teaser_detection}) as compared to pre-training on ImageNet which has been used extensively in the literature \cite{he2019momentum,chen2020simple,Caron2020UnsupervisedLO}. 
% This approach is fast (>$125$ fps on an NVIDIA P100 GPU), adds minimial overhead to the baseline computation time (<$1\%$), and is simple to implement.
% and we show that this consistently improves performance on a number of recent state of the art SSL pipelines.
We have released the dataset at \url{https://github.com/shlokk/object-cropping-ssl}.

% \url{https://github.com/shlokk/object-cropping-ssl}.

%This randomly selected bounding box is called the ``semantic crop". A standard bounding box over the entire image is called the ``scene crop". 

% \begin{figure*}[htp]
    
%   \centering
% %   \includegraphics[width=\linewidth]{LaTeX/Images/teaser_bar_plot.png}

%  \vspace{-0.15in}
 
%  \subcaptionbox*{}{\includegraphics[width=0.34\linewidth]{LaTeX/Images/OpenImages_barplot_1.png}}\hspace{-0.85em}
%  \subcaptionbox*{}{\includegraphics[width=0.34\linewidth]{LaTeX/Images/object-detection-teaser_coco_2.png}}\hspace{-0.85em}
%  \subcaptionbox*{}{\includegraphics[width=0.34\linewidth]{LaTeX/Images/segmentation_numbers_1.png}}
 
% %   \begin{subfigure}[t]{0.32\textwidth}
% %     \includegraphics[width=\linewidth]{LaTeX/Images/OpenImages_barplot.png}
  
% %   %  \caption{Object Detection }
% %   %   \label{fig:detection_number}
% %  \end{subfigure} 
% %   ~
% %   \begin{subfigure}[t]{0.32\textwidth}
% %     \includegraphics[width=\linewidth]{LaTeX/Images/detection_numbers_coco.png}
   
% %   %  \caption{Object Detection }
% %   %   \label{fig:detection_number}
% %  \end{subfigure} 
% %   ~
% %   \begin{subfigure}[t]{0.32\linewidth}
% %   \centering
% %     \includegraphics[width=\linewidth]{LaTeX/Images/segmentation_numbers.png}
% %     % \caption{Semantic Segmentation}
% % %   \label{fig:segmentation_number}
% %   \end{subfigure}
%     \vspace{-0.25in}
%     \caption{Our object-aware cropping approach can be easily plugged into self-supervised learning pipelines and achieves excellent results for classification on OpenImages (left), COCO object detection (middle) and COCO semantic segmentation (right). Using object-aware cropping instead of scene-level cropping provides a consistent boost on BYOL \cite{richemond2020byol} and MoCo-v2 \cite{chen2020improved}, two of the top SSL methods. In the case of COCO object detection and semantic segmentation, this boost allows us to beat pre-training on supervised ImageNet (denoted ``Sup. IN''). In the case of classification on OpenImages, we reduce the gap to supervised training by nearly $50\%$.}
% \label{fig:detection_segmentation_number}
% \end{figure*}
