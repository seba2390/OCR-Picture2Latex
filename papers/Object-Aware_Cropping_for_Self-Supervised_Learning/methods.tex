\section{Proposed Approach}
In this section we first discuss how we add object-awareness to the loss, followed by two new loss functions which help in learning better object aware representations.

\label{sec:model}
%Our hypothesis is that scene cropping has a high likelihood of including a lot of information irrelevant to the object. This effect is more pronounced when the objects are small relative to the size of the image. 
% of missing objects when the objects are small relative to the size of the image. 
%This leads to suboptimal representations for OpenImages due to the inclusion of data that is weakly correlated with the objects of interest. 

% Our analysis shows that SSL methods based on either purely object crops, or purely scene crops, both perform worse than supervised learning. This suggests a third option: incorporating \emph{both} object and scene information in the crops. Our hypothesis is that context around an object is helpful to learn robust representations due to the natural correlations between scene and object \cite{Hinton2021HowTR}. However, it is unclear what mix of object and scene information is most helpful in learning robust representations. Our main contribution is a simple and effective approach to achieving this cropping mechanism. \ashah{Remove from approach section.}We also experiment with a number of plausible baselines to incorporate scene and object information.

\subsection{Object Proposals}
To enable object-awareness, we consider an unsupervised object proposal model LOD \cite{Vo2021LargeScaleUO}.   LOD is a large scale unsupervised object proposal method \cite{Vo2021LargeScaleUO}.   The authors suggest a formulation of unsupervised object discovery as a ranking problem using distributed methods.
% \sout{available for eigenvalue problems and link analysis}.
In our experiments we use LOD to generate up to $10$ proposals per image and select one object randomly among these proposals. The details of other faster semi supervised object proposals (which use object labels from VOC dataset for training) are present in appendix Section C. Since, our method is completely Self-Supervised we focus on using LOD as our object proposal generation method.

%\ref{sec:object_proposal_methods} Our results in Table \ref{tab:voc_moco} show that other object proposal methods provide sufficient quality and the choice may be mostly dependent on the speed of the method. 
% We experiment with four types of cropping in the rest of the paper: 

%Due to the fast performance of BING, we use it in all subsequent experiments. 

%Objects with well-defined closed boundaries are strongly correlated large gradient norms. Objectness of an image window, they resize it to 8 Ã— 8 and use the norm of the gradients as a simple 64D feature  for learning a generic objectness measure in a cascaded SVM framework. They further use a binarized version of the Normed Gradient features to significantly speed up their processing. BING relies on the low-level cues concerning the object boundaries and has been shown to be very generalizable and can predict category independent object proposals. BING has been shown to effective in plethora of tasks such as multi-label image classification \cite{Wei2016HCPAF}, semantic segmentation \cite{pinheiro2015imagelevel}, video-classification \cite{zha2015exploiting}, co-salient object detection \cite{ZhangSaliency}, deep multi instance learning \cite{7298968} and video-summarization \cite{Lee_2015}. The high efficiency and speed of BING makes it suitable for  generating object proposals in our datasets. So, we use BING and generate 10 proposals for every image in the OpenImages-Subset and MS-COCO \cite{Lin2014MicrosoftCC} datasets. We randomly pick one out of the 10 generated proposals as a positive sample and Random-Resized crop as other positive sample. We then train MOCOV2 \cite{chen2020improved} and BYOL \cite{grill2020bootstrap} on both of these datasets.
% We used these bounding boxes to pre-train MOCO so that we are able to work on non-iconic datasets like OpenImages \cite{Kuznetsova_2020}. \\


%Unsupervised object discovery methods try to detect the objects present in an image and also structure of the object collection \cite{vo2019unsupervised,cho2015unsupervised} in a completely unsupervised manner. Similar to BING we use \cite{vo2019unsupervised} to generate object proposals. \cite{vo2019unsupervised} uses a robust matching technique which relies on appearance and geometric consistency constraints to assign confidence and saliencey scores to region proposals. However as compared to BING this method is very slow and unlike BING is very difficult to plug in into various deep learning pipelines. Hence we mostly focus on using BING as our default proposal generation method.


%\subsection{Proposed method}
% % \TODO{add object random crop baseline information which used GT boxes}
% We now discuss the different type of cropping which we consider for an image.

% For training contrastive SSL methods, we treat the randomly cropped patches on the pairs of images selected these cropping strategies as positive views of a given image and other randomly sampled images as negative views. 
% Note that for creating positive samples we randomly crop on output from both the cropping strategies.
% in the mini-batch
% We begin by describing methods which take in extra context and scene level information followed by other object level methods. 

%To achieve this we treat the proposals given by BING as object crops and random-resized crops as scene crops.
%Our aim is to learn both object and scene level representations, to this end we consider three kinds of combinations of cropping; Object-Scene cropping, Object-Object cropping and Scene-Scene cropping. Now we discuss these methods in detail:\\

% \textbf{Adding context and scene level information:}
% To add scene level information we try two different methods. We now describe both of these methods in detail. 

\paragraph{Dilated object proposals (Obj-Obj+Dilate Crop):} 
Scene pixels spatially close to the object are more likely to have a positive correlation with the object. With this intuition, we generate the second view by dilating the  the randomly selected LOD proposal. We dilate the box by $10\%$ or $20\%$ of the image size, followed by a random crop. Changing dilation gives us control over how much scene information is incorporated (a value of $10\%$ works well in most cases). Note that the original and dilated boxes are both followed by a random crop, ensuring that the first view is not trivially included in the second view. The choice of which crop to use as query or key is arbitrary and either object or dilated object crops can be used as key and query.  
\paragraph{Different projection heads for Object and Dilated Object crops:}
The projection head, introduced in \cite{chen2020simple} is an important component of most SSL methods. This is an MLP that maps representations from the encoder backbone to a lower-dimensional space where the loss function is applied. 
% We find it beneficial for performance to use two different projection heads: one for the first view (BING crop) and another for the second view for obj-scene, obj-obj+dilate and obj-obj+shift crops. For scene-scene crops we always use a single projection head. We hypothesize that the different projection networks specialize to either an object-specific representation, or to a representation that incorporates scene information. 
% Here, $\bm z_i = g(\bm h_i)=W^{(2)}\sigma(W^{(1)}\bm h_i)$ where $\sigma$ is a ReLU non-linearity, and $\bm h_i = f(\tilde{\bm x}_i) = \mathrm{ResNet}(\tilde{\bm x}_i)$ where $\bm h_i\in \mathbb{R}^d$ is the output after the average pooling layer and $g$ is the MLP layer. 
SimCLR \cite{chen2020simple} show that projection heads can remove information that may be useful for the downstream task, such as the color or orientation of objects. They show that by using this MLP, the last layer of network (i.e layer before mlp) can maintain more information. However \cite{chen2020exploring} use a single projection head is used for both views, since both the views on ImageNet are object crops.  We hypothesize that Object and Scene crops often contain different object orientation and color information; hence we propose to use different projection heads for scene and objects.
% \ashah{remove} We also consider other baselines listed below which are other plausible approaches to incorporating scene information. However, as shown in the results Section \ref{secc:results}, dilated crops are the best performing method. 

%Our hypothesis is that this approach to cropping should result in better representations since it encompasses object-centric and contextual (scene) information. Our experiments in Section \ref{secc:results} support this hypothesis. 

% Let's denote the dilation by $\delta$, and the height and width of the image by $H,W$.  The vertices of the BING crop in the original image are $X_1,Y_1$(top left corner) and $X_2,Y_2$(bottom right corner). So the size of dilated crops will be. 
% \begin{eqnarray*}
% X_1\_dilated = X_1\_dilated - \delta * W , 
% Y_1\_dilated = Y_1\_dilated - \delta * H \\
% X_2\_dilated = X_2\_dilated + \delta * W ,
% Y_2\_dilated = Y_2\_dilated + \delta * H
% \end{eqnarray*}



% Other than the above change to the cropping methodology, we leave the rest of the SSL pipeline unchanged: other data augmentations such as color shifts are applied in the usual manner, and the loss functions and training regimes are also unchanged. Our approach therefore involves really simple changes to most SSL pipelines, involving only a few lines of code. 
\subsection{Loss Objectives}

% MoCo uses the InfoNCE loss which is described below.
Following \cite{he2019momentum}, we use a momentum queue and optimize the model using the InfoNCE loss :
% \begin{equation}
% \small
% \mathcal{L}_{q, k^+, \{k^-\}} = -\log \frac{\exp(q{\cdot}k^+ / \tau)}{\exp(q{\cdot}k^+ / \tau) + {\displaystyle\sum_{k^-}}\exp(q{\cdot}k^-  / \tau)}.
% \label{eq:infonce}
% \end{equation}

\begin{equation}
\small
\mathcal{L}_{\text{moco}} = -\log \frac{\exp(q{\cdot}k^+ / \tau)}{\exp(q{\cdot}k^+ / \tau) + {\displaystyle\sum_{k^-}}\exp(q{\cdot}k^-  / \tau)}.
\label{eq:infonce}
\end{equation}

% \ashah{I think should instead write in terms of the projection layers}
Here $q$ is a query representation, $k^+$ is a representation of the positive (similar) key sample, and $\{k^-\}$ are representations of the negative (dissimilar) key samples. $\tau$ is the temperature hyper-parameter. 
We augment the standard contrastive loss with two additional losses to learn richer features for both objects and scenes. Next we describe both these losses in detail.
\subsubsection{Rotation task $\mathcal{L}_{rot}$}
 In the standard rotation prediction pretext task \cite{Gidaris2018UnsupervisedRL}, an image crop is randomly rotated with an angle in set $\phi$. An MLP is then tasked at correctly predicting the rotation of a patch given its features extracted using the feature extractor $f$. Our object cropping strategy generates an object and a scene crop from a given image. We modify the standard rotation task to estimate rotation of the object crop wrt the scene crop. Specifically, we randomly rotate the object crop and extract features $z_{\text{obj}}$. The scene crop is kept as is and its features $z_{\text{scene}}$ are obtained by feeding the scene crop through feature extractor $f$. Note that rotation augmentations are applied to object crop in addition to the standard MoCo-style augmentations. The rotation prediction MLP takes as input the concatenated features and estimates the relative rotation. Our rotation loss is $\mathcal{L}_{\text{rot}}$ is a standard cross entropy loss with rotation labels as the targets. Different from the standard rotation prediction task which estimates absolute rotation, our approach estimates rotation of the object relative to the scene and complements the contrastive loss. Note that since we deal with in-the-wild datasets, estimating absolute rotation of a crop is ill-posed since a particular object might occur in a variety of poses thus leading to incorrect gradients. 
 
 
%  Final rotation loss is $\mathcal{L}_{\text{rot}}$ is a cross entropy loss on concatenation of object features, original image features and the object rotation label.
 
%  Here the model $F(.)$ gets image $X^{y*}$ where the rotation label $y^*$ is unknown to the model $F(.)$ and yields an output probability distribution over all $K$ transformations $F(X^{y*}|\theta) = \{F^y(X^{y*}|\theta)\}_{y=1}^K$. Here $F^y(X^{y*}|\theta)$ is the predicted probability for the geometric transformation with label $y$ and $\theta$ are the learnable parameters of model $F(.)$. So, given a dataset of $N$ training samples, self-supervised training objective is to minimize
%  $\mathcal{L}_{\text{rot}}(X_i,\theta)$
%  $\min_{\theta} \sum_{i=1}^{N} \mathcal{L}_{\text{rot}}(X_i,\theta)$  
%  where the loss function $\mathcal{L}_{\text{rot}}(.)$ is defined as:



% \ashah{Not clear what problem being solved.} \shlok{not sure how to add flow, will have to think.}
% We add additional loss of rotation task where the task is, given an rotated object, find out the angle of the rotation. We rotate the object before the augmentations and then apply all the moco augmentations on the rotated images. Let the object features be $X_o$ and scene feature be $X_s$. We concatenate both the features and predict the rotation of the object. Note that unlike traditional rotation tasks, in our formulation network has to learn the image rotation not only wrt objects, but also wrt to the scene image as well. Here $y$ is the object rotation label, $F$ denotes the output of the convolution , $\theta$ is the learn-able parameter of the network. 
% \ashah{Not clear from text why we have object rotation labels}
%\begin{eqnarray*} \label{eq:loss_fun}
% \begin{equation}
% \mathcal{L}_{\text{rot}}(X_i,\theta) = \sum_{i=1}^{N}( - \frac{1}{K}\sum_{y=1}^{K} \log( F^y( (X_i|y) | \theta))).
% % \end{eqnarray*}
% \label{ref:eqn_rot}
% \end{equation}

% where  $X_i = z_{\text{obj}}  \oplus z_{\text{scene}} $.
% Here $\tau_o$ is the temperature of the object representation, and $\tau_s$ is the temperature of the scene representation. 

\subsubsection{Object localization task $\mathcal{L}_{loc}$ }
Since we are working with images coming from non-iconic datasets, the object could potentially occupy a small region of the image. We propose to add a pretext task of localizing an object inside the scene using features alone. Specifically in this task, given an image, we predict the spatial location of the object in the image. Here we take  original image and divide the image into $p * p$ patches. We then take an object crop and mark all the patches where the object is present as 1 and other patches as 0 which we use as our label $Y$. 
Similar to rotation task we first extract features for the object proposals $z_{\text{obj}}$. For the original image we apply all the same augmentations except random crop (since we want the full image and not the cropped version of the image). Then we obtain the features for the original image $z_{\text{ori}}$. We then pass the features through MLP layer. Finally the loss  $\mathcal{L}_{ol}$ is two-class classification loss on concatenation of object features, original image features and labels $Y$.
% The loss function $\mathcal{L}_{ol}$ is defined as:

% \begin{eqnarray*} \label{eq:loss_fun}
% \mathcal{L}_{ol}(X_i,\theta) = \frac{1}{N}\sum_{i=1}^{N}( - \frac{1}{K}\sum_{y=1}^{K} log( F^y( (X_i|y) | \theta))).
% \end{eqnarray*}
% and $X = z_{\text{obj}} \oplus  z_{\text{ori}}$.\\
\textbf{Why don't we see degenerate solution for object localization?} A natural question is why doesn't our model cheat and remember all the object location for all the object proposals. The answer to the question is the random cropping on the object proposals. Since we are using random crops on the object proposals, the label $Y$ will change every time according the random crop parameters.
% we will have different values of $Y_{ij}$ depending upon $X_{r1}, Y_{r1}, X_{r2}, Y_{r2}$.  
Hence the network cannot learn the object patch location and has to focus on the semantics of objects and scenes to figure out the spatial location of object in the scene.  Exact implementation detail of the object localization task is shown in the appendix. 

\paragraph{Final loss function:}
Our final loss function is
\begin{equation}
\mathcal{L}_{\text{oac}} = \mathcal{L}_\text{moco} + \mathcal{L}_\text{rot} + \mathcal{L}_\text{ol}
\end{equation}
We simply combine all the losses and
call it $\mathcal{L}_{\text{oac}}$ (\textbf{O}bject-\textbf{A}ware \textbf{C}ropping)
and backpropagate on all of them. 
% \ashah{I think that the methods section should read as follows. 1. Proposed cropping strategy, 2. Use of seperate projection heads 3. New loss functions}





%For ex. in case of COCO dataset, since object crops cover 40\% of the image we use 0.4 as the lower scale. Similarly, in case of OpenImages we use 0.2 as the lower scale since object crops cover close to 20\% of the image.

% \textbf{OARC-Scene Crop using dilation}: \\


% \textbf{Combining all three crops}: We also experiment with combining the $3$ approaches above by randomly sampling any one of these crop types with equal probability. This approach leads to very minimilastic overhead and gives good performance on COCO and OpenImages datasets. 
% In practice we find that it does performs similar to and in some cases slightly better than the simpler object-scene crop approach above.

%\textbf{Different projection heads for Object and Scene crops:}
% \ashah{think:Given that our current approach is not obj+scene crop but rather obj+dilated object is it still ok to use two heads?} \shlok{yeah I have used it, we need to re-structure this}
 
% So, we have $g\_{obj}$ as the projection head for objects and $g\_{scene}$ as the projection head for scenes.

% $g(\bm h\_obj)$ as the projection head for objects and $g(\bm h\_scene)$ as the projection head for scenes.

% This gives us
% \begin{eqnarray*}
% \bm z_\text{obj} = (W_{\text{object}}^{(2)}\sigma(W_{\text{object}}^{(1)}\bm h_i))
% ~\text{and}~ \bm z_\text{scene} = (W_{\text{scene}}^{(2)}\sigma(W_{\text{scene}}^{(1)}\bm h_i))^T .
% \end{eqnarray*}