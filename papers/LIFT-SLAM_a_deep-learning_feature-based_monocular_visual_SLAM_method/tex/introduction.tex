\section{Introduction}
\label{sec:introduction}

The ability to know its localization in an environment is an essential task for mobile robots, and it has been a subject of research in robotics for decades. To correctly localize itself, the robot must know its pose (position and orientation) in the environment. The process that estimates this information is called Odometry. When the robot simultaneously localizes itself and constructs a map of the unknown environment, the algorithm is called Simultaneous Localization and Mapping (SLAM).

In the last decades, the advances in hardware technologies, such as embedded GPUs, allowed significant advances on mobile robots pose estimation through camera-based methodologies of odometry and SLAM, which are called Visual Odometry (VO) and Visual SLAM (VSLAM). Much work has been done to develop accurate and robust VO and VSLAM systems. However, traditional approaches still depend on significant engineering effort on a classic pipeline: Initialization, feature detection, feature matching, outlier rejection, motion estimation, optimization, and relocalization. Furthermore, the traditional approaches tend to fail in challenging environments (inadequate illumination, featureless areas, etc.), when the camera is moving at high speed or if the camera suffers some distortions (rolling shutter effect, unfavorable exposure conditions, etc.). Moreover, if the camera is monocular, these systems have scale uncertainty.

%Recent applications of deep learning-based methods in VO and VSLAM have achieved promising results, bringing robustness to the situations as mentioned earlier.
Recently, many works have proposed using Deep Neural Networks (DNNs) to estimate camera motion with an end-to-end system. These systems can replace the entire traditional VO/VSLAM pipeline, which depends on significant engineering effort to develop and tune \cite{undeep-vo, attention-based, deep-vo}. However, these methods are not able to outperform traditional methods yet. Thus, some new works propose to replace only some modules of the VO/VSLAM traditional pipeline with DNNs, creating hybrid methods \cite{self-improving-vo, df-slam, pose-graph-optimization, gcnv2}. These approaches can leverage the robustness of deep learning to enhance traditional VSLAM systems. However, the literature still lacks an in-depth evaluation of these algorithms' robustness in challenging situations. Also, most of the proposed methods do not provide results in different scenarios to confirm the algorithms' robustness in all situations.

Therefore, in this paper, we propose employing the Learned Invariant Feature Transform (LIFT) \cite{lift} to extract features from images and use these features in a traditional VSLAM pipeline based on ORB-SLAM \cite{orb-slam} for monocular camera applications. Hence, we explore the potential of deep neural networks to improve the performance of conventional VSLAM systems. We also propose a set of experiments to evaluate the robustness of the algorithm in several scenarios. The main contributions of this work are summarized as follows:
\begin{itemize}
    \item This paper presents a novel hybrid VSLAM algorithm based on the LIFT network to perform feature extraction in a traditional back-end based on ORB-SLAM's system;
    
    \item We evaluate how transfer learning and fine-tuning can affect the quality of the resulting Hybrid VSLAM system;
    
    \item We extend the proposed system with an adaptive approach that can enhance its performance while avoiding fine-tuning of parameters that are usually dependable on the dataset;
    
    \item We conduct experiments on public KITTI \cite{kitti-dataset} and Euroc \cite{euroc-mav} datasets and present a set of experiments to confirm the robustness of algorithms based on learned features under camera distortions.
\end{itemize}