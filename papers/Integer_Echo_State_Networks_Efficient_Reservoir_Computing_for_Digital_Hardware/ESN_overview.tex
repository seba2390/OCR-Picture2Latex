\subsection{Echo State Networks}
\label{sect:esn}
% 


% \begin{figure}[hbt]
% \minipage{0.49\textwidth}
%   \includegraphics[width=\linewidth]{img/ESN_new}
%   \caption{Architecture of the conventional Echo State Network.}
% \label{fig:esn}
% \endminipage\hfill
% \minipage{0.49\textwidth}%
%   \includegraphics[width=\linewidth]{img/HD_ESN_new}
%   \caption{Architecture of the Integer Echo State Network.}
% \label{fig:intesn}
% \endminipage
% \end{figure}


%\hl{
This subsection summarizes the functionality of the conventional ESN, it follows the description in \cite{ESNtut12} for a special case of leaky integration when $\alpha=1$\footnote{For the detailed tutorial on ESNs diligent readers are referred to \cite{ESNtut12}.}.
%}
Fig.~\ref{fig:esn} depicts the architectural design of the conventional ESN, which includes three layers of neurons. The input layer with $K$ neurons represents the current value of input signal denoted as $\textbf{u}(n)$. The output layer ($L$ neurons) produces the output of the network (denoted as $\textbf{y}(n)$) during the operating phase. The reservoir is the hidden layer of the network with $N$ neurons, with the state of the reservoir at time $n$ denoted as $\textbf{x}(n)$. 

In general, the connectivity of ESN is described by four matrices. $\textbf{W}^{\text{in}}$ describes connections between the input layer neurons and the reservoir, and $\textbf{W}^{\text{back}}$ does the same for the output layer. Both matrices project the current input and output to the reservoir.   
The memory in ESN is due to the recurrent connections between neurons in the reservoir, which are described in the reservoir matrix $\textbf{W}$.  
Finally, the matrix of readout connections $\textbf{W}^{\text{out}}$ transforms the current activity levels in the input layer and reservoir ($\textbf{u}(n)$ and $\textbf{x}(n)$, respectively) into the network's output $\textbf{y}(n)$.  
~
\begin{figure}[tb]%[!ht]%[t!]
\centering
\includegraphics[width=1.0\columnwidth]{img/ESN}
\caption{Architecture of the conventional Echo State Network.}
\label{fig:esn}
%\vspace*{-0.5cm}
\end{figure}
~
Note that three matrices ($\textbf{W}^{\text{in}}$, $\textbf{W}^{\text{back}}$, and $\textbf{W}$) are randomly generated at the network initialization and stay fixed during the network's lifetime. Thus, the training process is focused on learning the readout matrix $\textbf{W}^{\text{out}}$. There are no strict restrictions for the generation of projection matrices   
$\textbf{W}^{\text{in}}$ and $\textbf{W}^{\text{back}}$. They are usually randomly drawn from either normal or uniform distributions and scaled as shown below. The reservoir connection matrix, however, is restricted to posses the echo state property. This property is achieved when the  spectral radius of the matrix  $\textbf{W}$ is less or equal than one. 
%\hl{
For example, $\textbf{W}$ can be generated from a normal distribution and then normalized by its maximal eigenvalue.
Unless otherwise stated, in this article an orthogonal matrix was used as the reservoir connection matrix; such a matrix was formed by applying QR decomposition to a random matrix generated from the standard normal distribution. Also, $\textbf{W}$ can be scaled by a feedback strength parameter, see (\ref{eq:esnres}).
%}

The update of the network's reservoir at time $n$ is described by the following equation:
~
\begin{equation}
\textbf{x}(n)=\tanh(\rho\textbf{W}\textbf{x}(n-1)+\beta\textbf{W}^{\text{in}}\textbf{u}(n)+\beta\textbf{W}^{\text{back}}\textbf{y}(n-1)),
\label{eq:esnres}
 \end{equation}
%\hl{
where  $\beta$ and $\rho$ denote projection gain and the feedback strength, respectively. Note that it is assumed that the spectral radius of the reservoir connection matrix $\textbf{W}$ is one.
%}
Note also that at each time step neurons in the reservoir apply $\tanh()$ as the activation function.
The nonlinearity prevents the network from exploding by restricting the range of possible values from -1 to 1. The activity in the output layer is calculated as:  
~
\begin{equation}
\hat{\textbf{y}}(n)=g(\textbf{W}^{\text{out}}[\textbf{x}(n);\textbf{u}(n)]),
\label{eq:esny}
 \end{equation}
where the semicolon denotes concatenation of two vectors and $g()$ the activation function of the output neurons, for example, linear or  Winner-take-all.  
~
\subsubsection{Training process}
\label{sect:training}

This article only considers training with supervised-learning when the network is provided with the ground truth desired output at each update step. The reservoir states $\textbf{x}(n)$ are collected together with the ground truth $\textbf{y}(n)$ for each training step. The weights of the output layer connections are acquired by solving the regression problem which minimizes the mean square error between predictions (\ref{eq:esny}) and the ground truth.
%For example, $\textbf{W}^{\text{out}}$ can be calculated using the pseudo-inverse matrix operation (adopted in this article). 
While this article does not focus on the readout training task, it should be noted that there are many alternatives reported in the literature including the usage of regression with regularization, online update rules, etc. \cite{ESNtut12}.




