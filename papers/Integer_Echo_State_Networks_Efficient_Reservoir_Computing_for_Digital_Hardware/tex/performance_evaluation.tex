



\section{Performance evaluation}
\label{sect:perf}

%\hl{
In this section, the proposed intESN approach is verified and compared to the conventional ESN and the ring-based ESN~\cite{MinESN} on a set of typical RC tasks.
In particular, three aspects are evaluated: short-term memory, classification of time-series, and modeling of dynamic processes.
Short-term memories are compared using the trajectory association task
\cite{PlateBook}, introduced in the area of holographic reduced representations
\cite{PlateTr}. Additionally, an approach for storing and decoding analog values using intESN is demonstrated on image patches. 
Classification of time-series is studied using the standard datasets from UCI and UCR.
Modeling of dynamic processes is tested on two typical cases. First, the task of learning a simple sinusoidal function is considered. Next, networks are trained to reproduce a complex dynamical system produced by a Mackey-Glass series.
Unless otherwise stated, ridge regression (the regularization coefficient is denoted as $\lambda$) with the  Moore-Penrose pseudo-inverse was used to learn the
readout matrix $\textbf{W}^{\text{out}}$. Values of input neurons $\textbf{u}(n)$ were not used for training the readout in any of the experiments below. 
%}
%Note about studies of capacity for both RC and HPC
%Moore-Penrose pseudo-inverse of matrix
















\subsection{Short-term memory}
\subsubsection{Sequence recall task}
\label{sect:perf:trajectory}






%While we leave formal analysis of the capacity for future work 

The sequence recall task includes  two stages: memorization and recall.
At the memorization stage,  a network continuously stores a sequence of tokens 
(e.g., letters, phonemes, etc).
The number of unique tokens is denoted as $D$ ($D=27$ in the experiments), and one token is presented as input each timestep.
At the recall stage, the network uses the content of its reservoir to
retrieve the token stored $d$ steps ago, where $d$ denotes delay. In the
experiments, the range of delay varied between 0 and 15.



\begin{figure}[tb]%[!ht]%[t!]
\centering
\includegraphics[width=1.0\linewidth]{img/short_memory_ort}
\caption{The accuracy of the  correct decoding of tokens for the conventional ESN, ring-based ESN, and integer ESN for
three different values of $N$.
}
\label{fig:memory}
%\vspace*{-0.5cm}
\end{figure}




For the conventional and ring-based ESNs, the  dictionary of tokens was represented by a one-hot encoding, i.e.
the number of input layer neurons was set to the  size of the dictionary
$K=D=27$. The same encoding scheme was adopted for the output layer, $L=27$.
%\hl{
The input vector  was projected to the reservoir by the projection matrix
$\textbf{W}^{in}$ where each entry was independently generated from the uniform distribution in the
range $[-1,1]$, the projection gain was set to $\beta=0.1$.  
The reservoir connection matrix $\textbf{W}$ for the conventional ESN  was first generated from the standard normal distribution and then orthogonalized. 
The reservoir connection matrix $\textbf{W}$ for the ring-based ESN  was generated as a permutation matrix. 
The feedback strength of both reservoir connection matrices was set to $\rho=0.94$. 
%}


For intESN, the item memory was populated with $D$ random high-dimensional
bipolar vectors.  The threshold for the clipping function was set to $\kappa=3$. The
output layer was the same as in ESN with $L=27$ and one-hot encoding of tokens.
%{\color{brown}
It is worth noting that $\rho$ and $\kappa$ were chosen in such a way that the accuracy curves would resemble each other as close as possible. 
The diligent readers are kindly referred to the Supplementary materials (Fig. S.1) where for the case $N=200$ the curves for the range of $\rho$ and $\kappa$ values are presented. 
%}



For each value of of the delay $d$ a readout matrix $\textbf{W}^{\text{out}}$  was trained, producing 16 matrices in total. 
The training sequence presented 2000 random tokens to the network, and only the last 1500 steps were used to compute the readout matrices. The regularization parameter for ridge regression was set to $\lambda=0$.
The training sequence of tokens delayed by the particular $d$ was used as the ground truth for the the activations of the output layer.  
During the operating phase,  both the inclusion of a new token into the reservoir and the recall of the delayed token from the reservoir 
were simultaneous.  Experiments were performed for three different sizes of the reservoir: $N=100$, $N=200$, and $N=300$.


\begin{figure}[tb]%[!ht]%[t!]
\centering
\includegraphics[width=1.0\linewidth]{img/short_memory_ort_lonf_ESN}
\caption{The accuracy of the  correct decoding of tokens for the conventional ESN, ring-based ESN, and integer ESN for
three different values of $N$. ``intESN-large'' refers to the fact that the number of neurons in intESN was equivalent to the memory footprint required by ESN for the stated number of neurons.  
}
\label{fig:memory:long}
%\vspace*{-0.5cm}
\end{figure}


The memory capacity of the network is characterized by the 
accuracy of the correct decoding of tokens for different values of the delay. 
%{\color{brown}
Fig.~\ref{fig:memory} depicts the accuracy for all networks conventional ESN (solid lines), ring-based ESN (dash-dotted line)
and intESN (dashed lines). The capacities of all the networks grow with the
increased number of neurons in the reservoir.  
Since the capacities of the conventional ESN and the ring-based ESN are almost identical, which is in line with~\cite{Strauss2012}, for the rest of this subsection we assume both of these networks when using the term ESN.  
%}


\begin{figure*}[tb]%[h]
\center{
\begin{minipage}[h]{0.8\linewidth}
\center{\includegraphics[width=1.0\linewidth]{img/intESN_images_or} }
\end{minipage}
\vfill
\begin{minipage}[h]{0.8\linewidth}
\center{\includegraphics[width=1.0\linewidth]{img/intESN_images_rec}}
\end{minipage}
\caption{
%\hl{
An example of image patches decoded from an intESN. Top row represents the original images stored in the reservoir.   
Other rows depict the patches reconstructed from intESN for different reservoir sizes and clipping thresholds.
%}
}
}
\label{fig:images}
\end{figure*}


%\hl{
The capacities of ESN and intESN  are comparable for small $d$, i.e., for the most recent tokens. 
For the increased delays the curves featured slightly different behaviors. 
With increase of the value of $d$ the performance of intESN started to decline faster compared to ESN.
%For all values of $N$ the accuracy of  intESN starts to deviate from 100\% earlier than that of  ESN. Also, intESN features slightly steeper decay than ESN. 
Eventually, all curves converge to the value of the random guess which equals $1/D$. 
%Moreover, it is possible to characterize the information capacity using a single number -- the amount of information which can be decoded from the reservoir. 
Moreover, the information capacity of a network is characterized by the amount of the decoded from the reservoir information.
This amount is determined using the amount of information per token ($\log_2D$), the probability of correctly decoding a token at each delay value, and the concept of mutual information. We calculated the amount of information for all networks in Fig.~\ref{fig:memory} in the considered delay range. For 100 neurons intESN preserved 19.3\% less information, for 200 and 300 neurons 21.7\% less.
%}




%\hl{
%On the other hand, intESN with $\kappa=3$ requires only 3-bit per neuron. It is assumed that one ESN neuron requires 32-bit then if the number of neurons in %intESN is increased ten times the reservoir memory footprints of two networks are going to be comparable. The results for this case are presented in Fig.~\ref{fig:memory:long} (the training sequence was prolonged to 9000 random tokens). In such setting intESN has clearly higher information capacity. In particular, %for ESN memory footprint with 100 neurons the decoded amount of information has increased 2.2 times while for 200 and 300 neurons it increased 1.6 and 1.3 times %respectively. 
%}











%\hl{
These results highlight a very important trade-off: the performance versus a complexity of implementation. While
the performance of intESN is somewhat poorer in this task, one has to bear in mind its memory footprint. With the
clipping threshold $\kappa=3$ only 3-bit are needed to represent the state of a neuron compared to 32-bit
per neuron (the size of type float) in ESN. 
%{\color{red}
In other words, intESN allowed lowering the memory footprint of the reservoir by an order of magnitude by sacrificing only a fraction of the performance with respect to the information capacity. 
Thus, we conjecture that some reduction in the performance for ten folds
memory footprint reduction is an acceptable price in applications on resource-constrained computing
devices.
%}
On the other hand, we can check the performance of the networks with equal memory
footprints. 
For this we increased the number of neurons in intESN so that the total memory consumed by
the reservoir with the same clipping threshold $\kappa=3$ would match that of the conventional or ring-based ESN. 
This network is denoted as ``intESN-large''.
%{\color{red}
Since $\kappa=3$ requires only 3-bit, in order to get the memory footprint corresponding to ESN, intESN could use more than ten times more neurons. 
Thus, the memory footprint of intESN with $1000$ neurons corresponds to ESN with $100$ neurons; while intESN with $2000$ and $3000$  neurons correspond to ESN with $200$ and $300$ neurons respectively.
%}
 The results for this case are presented in Fig.~\ref{fig:memory:long} (the training sequence was prolonged to
9000 random tokens). With such settings intESN-large has clearly higher information capacity. In
particular, for ESN memory footprint with 100 neurons the decoded amount of information has increased
2.2 times while for 200 and 300 neurons it increased 1.6 and 1.3 times respectively.
%}
%{\color{brown}
It is important to note, however, that while the memory consumed by the reservoir of intESN-large was comparable to the corresponding ESN, the readout matrix for intESN-large was larger and more computationally demanding than the ESN readout matrix since the size of a readout matrix is proportional to the number of neurons in the reservoir. 
%}





%ESN: 100 - 36.9;  200 - 58.6; 300 - 70.5; 
%intESN: 100 - 29.8;  200 - 45.9; 300 - 55.2; 
%Long training.
%ESN: 100 - 46.9;  200 - 74.9; 300 - 95.43; 
%intESN: 100 - 103.3;  200 - 117.4; 300 - 122.8; 

\subsubsection{Storage of analog values in intESN}
\label{sect:perf:analog}







%\hl{
This subsection presents the feasibility of storing analog values in intESN using image patches as a showcase. 
%{\color{red}
It is important to emphasize that this subsection  does not go into detailed comparisons with other methods as the main purpose here is 
the principal demonstration of the possibilities of storing continuous data in reservoirs consisting of integers in a limited range. 
In other words, with this showcase, we are aiming at demonstrating the feasibility of using integer approximation of neuron states in intESN to work with analog representations.
%}
A value of a pixel (in an RGB channel) can be treated as an analog value in the range between 0 and 1. 
For each pixel it is possible to generate a unique bipolar HD vector. 
%The pixel's value is encoded by multiplying all elements of the HD vector by that value. 
The typical approach to encode an analog value is to multiply all elements of the HD vector by that value.
The whole sequence is then represented using the bundling operation on all scaled HD vectors. The result of bundling can be used as an input to a reservoir. However, the resultant composite HD vector will not be in the integer range anymore. 
%This could be addressed in using scaling via sparsity. 
We address this problem by using sparsity.
Instead of scaling elements of an HD vector, we propose to randomly set the fraction of elements of the HD vector to zeros, i.e., the HD vector will become ternary. The proportion of zero elements is determined by the pixel's analog value. Pixels with values close to zero will have very sparse HD vectors while pixels with values close to one will have dense HD vectors, but all entries will always be
[-1, 0, or +1].
The result of bundling of such HD vectors (i.e., HD vector for an image) will still have integer values. Such representational scheme allows keeping integer values in the reservoir but it still can effectively store analog values.
%}




The examples of results are presented in Fig.~\ref{fig:images}. Top row depicts original images stored in the reservoir. The other rows depict images reconstructed from the reservoir. The following parameters of intESN were used (top to bottom): $N=64000$, $\kappa=11$; $N=32000$, $\kappa=8$;  $N=16000$, $\kappa=6$;  $N=8000$, $\kappa=4$. The values of $\kappa$ were optimized for a particular $N$. Columns correspond to the delay values (i.e., how many steps ago an image was stored in the reservoir) as in the previous experiment. 
As one would anticipate, the quality of the reconstructed images is improving for larger reservoir sizes. At the same time, the quality of the reconstructed images is deteriorating for larger delay values, i.e., the worst quality of the reconstructed image could be observed in the bottom right corner while the best reconstruction is located in the top left corner.
Nevertheless, the main observation for this experiment is that it is possible to project analog values into the reservoir with integer values using the mapping via varying sparsity and then retrieve the values from the reservoir.
Moreover, we have shown recently~\cite{intRVFL} that the mapping via varying sparsity could even be helpful when solving classification problems with a feed-forward variant of the ESN.

\begin{table}[tb]%[ht]
\renewcommand{\arraystretch}{1.3}
\caption{Details of datasets for time-series classification.\label{tab:datasets}
\vspace{-2mm}}
    % {\scriptsize
    \begin{center}
    \begin{tabular}{|c|c|c|c|c|}\hline
      	\multicolumn{5}{|c|}{\textbf{Univariate datasets from UCR}} \\ \hline\hline
        \textbf{Name} & \textit{\textbf{\#V}} & \textbf{Train} & \textbf{Test} & \textit{\textbf{\#C}} \\ \hline
        Swedish Leaf 	& 1	& 500 	& 625 	& 15 \\ \hline  
 	Distal Phalanx & 1	& 139 & 400 &	3 \\ \hline
	ECG 		& 1 	& 100 & 100 &	2 \\ \hline	
        Wafer  		& 1 	& 1000	 & 6164 &	2\\ \hline\hline       
        \multicolumn{5}{|c|}{\textbf{Multivariate datasets from UCI}} \\ \hline\hline
	 Character Trajectories & 3 		& 300 & 2558 & 20  \\\hline
	 Spoken Arabic Digit 	 & 13 	& 6600 & 2200 & 10  \\\hline
        Japanese Vowels 	& 12 	& 270 & 370 & 9 \\ \hline    	
    \end{tabular}
    \end{center}
%  }
%\vspace{-5mm}
\end{table}


\subsection{Classification of time-series}

%\hl{
In this section, ESN (conventional and ring-based) and intESN networks are compared in terms of classification accuracy obtained on standard time-series datasets. 
Following \cite{Bianchi2018} we used several (four) univariate datasets from UCR\footnote{UCR. Time Series Classification Archive [online], 2018. -- Available online: \url{https://www.cs.ucr.edu/\%7Eeamonn/time\_series\_data\_2018/}.}  and several (three) multivariate datasets from UCI\footnote{UCI. Machine Learning Repository [online], 2019. -- Available online: \url{http://archive.ics.uci.edu/ml/datasets.html}.}.
Details of datasets are presented in Table~\ref{tab:datasets}. 
For each dataset, the table includes the name, number of variables (\#\textit{V}), number of classes (\#\textit{C}), and the number of examples in training and testing datasets.  
%}






%\hl{
Configurations of the networks were kept fixed for all datasets. 
%{\color{brown}
In fact, the configuration of the conventional and ring-based ESNs were set in accordance to~\cite{Bianchi2018}: 
reservoir size was set to $N=800$, projection gain was set to $\beta=0.25$, the feedback strength was set to $\rho=0.99$. 
The regularization parameter for the ridge regression was set to $\lambda=1.0$.
The intESN was also trained with the same $\lambda$.
%}
The clipping threshold for the intESN  was set to $\kappa=7$. 
%{\color{red}
Also for intESN the quantized values of time-series were mapped to bipolar vectors using scatter codes \cite{TNNLS18, scatter}. 
The input signal \textbf{u}(n) was quantized as:
~
\begin{equation}
\textbf{u}(n)_q= \lfloor 200\textbf{u}(n) \rceil /200,
\label{eq:quan:timeser}
 \end{equation}
~
where $\lfloor * \rceil$ denotes rounding to the the closest integer. 
%}
Two sizes of intESN's reservoir were used. The first size corresponded to the size of the conventional and ring-based ESNs, i.e., $N=800$. The second size (``intESN-large'') corresponded to the same memory footprint\footnote{Except for the Japanese Vowels dataset where such reservoir size seemed to significantly overfit the training data. In that case, the number of neurons was increased twice.} required for ESN reservoir assuming that one ESN neuron requires 32-bit while one intESN neuron requires 4-bit (when $\kappa=7$). 
%{\color{red}
Thus, ``intESN-large'' had  $N=6400$ neurons. 
%}
%}




%\hl{
The output layers of networks were representing one-hot encodings of classes in a dataset, i.e., for the particular dataset $L=$\#\textit{C} of that dataset. 
The readout layers of all networks were trained using time-series from a training dataset in the so-called endpoints mode \cite{ComparisonReadOut} when only final temporal reservoir states for each time-series  are used for training a single readout matrix. 
%}


\begin{figure}[tb]%[!ht]%[t!]
\centering
\includegraphics[width=1.0\linewidth]{img/univariate}
\caption{The classification accuracy for univariate datasets from UCR. Bars depict mean values, lines depict standard deviations.  Bars denoted as ``ESN'' and ``intESN'' had the same number of neurons in their reservoirs while for ``intESN-large'' the number of neurons corresponded to ESN's memory footprint. 
}
\label{fig:univar}
\end{figure}




\begin{figure}[tb]%[!ht]%[t!]
\centering
\includegraphics[width=1.0\linewidth]{img/multivariate}
\caption{The classification accuracy for multivariate datasets from UCI. Bars depict mean values, lines depict standard deviations. Bars denoted as ``ESN'' and ``intESN'' had the same number of neurons in their reservoirs while for ``intESN-large'' the number of neurons corresponded to ESN's memory footprint. 
}
\label{fig:multivar}
\end{figure}




%\hl{
The experimental accuracies obtained from the networks for the considered datasets are presented in Fig.~\ref{fig:univar} and ~\ref{fig:multivar}. Fig.~\ref{fig:univar} presents the results for univariate datasets while Fig.~\ref{fig:multivar} presents the results for multivariate datasets. The figures depict mean and standard deviation values across ten independent random initializations of the networks.
%{\color{brown}
Similar to subsection~\ref{sect:perf:trajectory}, the accuracy of the conventional ESN and the ring-based ESN are almost identical, thus, for the rest of this subsection we assume both of these networks when using the term ESN.  
%}


%}

%\hl{
The obtained results strongly depend on the characteristics of the data. However, it was generally observed that intESN with the memory footprint equivalent to ESN demonstrated higher classification accuracy. 
On the other hand, the classification accuracy of intESN with the same number of neurons as in ESN was similar to ESN's performance for all considered datasets but two (``Swedish Leaf'' and ``Character Trajectories'') for which the accuracy degradation was sensible.
We, therefore, conjecture that in a general case, one cannot guarantee the same classification accuracy as for
ESN. The empirical evidence, however, shows that it is not infeasible. Since placing the reported results into
the general context of time-series classifications is outside the scope of this article we do not further
elaborate on fine-tuning of hyperparameters of intESN for the best classification performance.
%On the other hand, classification accuracy of intESN with the same number of neurons as in ESN was similar to ESN's performance for most of the datasets but it %was significantly lower for two datasets: ``Swedish Leaf'' and ``Character Trajectories''. Therefore, in a general case, one cannot guarantee the same accuracy %but the empirical evidence shows that it is not unlikely. 
%Finally, it should be noted that we did not aim at placing the reported results into the general context of time-series classifications. Instead, the goal was to compare the results of intESN and ESN.
%}
%{\color{brown}
However, the interested readers are kindly referred to the Supplementary materials (Fig. S.2 and S.3) where several different values of $N$, $\kappa$, and $\rho$ were examined for each dataset. 
%}







\subsection{Modeling of dynamic processes}
\subsubsection{Learning Sinusoidal Function}


\begin{figure}[tb]%[h]%[htb]
\centering
\includegraphics[width=1.0\linewidth]{img/Sinus.png}
\caption{Generation of a sinusoidal signal.}
\label{fig:Sinus}
\end{figure}

The task of learning a sinusoidal function \cite{ESN02} is
an example of a learning  simple dynamic system with the constant cyclic behavior. The 
ground truth signal was generated as follows:
~
\begin{equation}
y(n)=0.5\sin(n/4).
\label{eq:sin}
 \end{equation}
~
In this task, the input layer was not used, i.e. $K=0$ but the network
projected the activations  of the output layer back to the reservoir using $\textbf{W}^{\text{back}}$. The output
layer had only one neuron ($L=1$).  The reservoir size was fixed to $N=1000$ neurons.
The length of the training  sequence was 3000 (first 1000 steps were discarded
from the calculation). For ESN, the feedback strength for the reservoir connection matrix was set to $\rho=0.8$,
for both networks  $\lambda$ was set to 0.
A continuous value of the ground truth signal was fed-in to ESN during the training. 

For  intESN, in order to map the input signal to a bipolar vector the quantization was used. The signal was quantized as:
~
\begin{equation}
y(n)_q=\lfloor100y(n)\rceil/100.
\label{eq:quan}
 \end{equation}
~
The item memory for the projection of the output layer was populated with
bipolar vectors preserving linear  (in terms of dot product) similarity between
quantization levels \cite{Widdows15}.  The threshold for the clipping function
was set to $\kappa=3$.
 

\begin{figure*}[tb]%[htb]
\centering
\includegraphics[width=1.0\linewidth]{img/Mackey-Glass.png}
\caption{Prediction of the Mackey-Glass series.}
\label{fig:Mackey-Glass}
\end{figure*}


In the operating phase,  the network acted as the generator of the signal
feeding its previous prediction (at time $n-1$) back to the reservoir. 
Fig.~\ref{fig:Sinus} demonstrates  the behavior of intESN during the first 100
prediction steps.  The ground truth is depicted by dashed line while the
prediction of intESN  is  illustrated by the shaded area between 10\% and 90\%
percentiles (100 simulations were performed).  The figure does not show the
performance of the conventional ESN as it just followed the ground truth without
visible deviations. intESN clearly follows the values of the ground truth
but the deviation  from the ground truth is increasing with the number of
prediction steps.  It is unavoidable for the increasing prediction horizon but,
in this scenario,  it is additionally accelerated due to the presence of the
quantization error at each prediction step. 
%{\color{brown}
It is worth noting, however, that the quality of predictions for this task could be improved by increasing the value of $\kappa=3$, i.e., at the cost of extract memory allocated for each neuron. 
The diligent readers are kindly referred to the Supplementary materials (Fig. S.4) where several different values of $\kappa$ were examined. 
%}
The next subsection will clearly demonstrate effects caused by the quantization process. 
The error is accumulated because every time when feeding the prediction back to the reservoir of intESN it should be quantized in order to fetch a vector from the item memory. 




\subsubsection{Mackey-Glass series prediction}

A Mackey-Glass series is generated by the nonlinear time delay differential equation. 
It is commonly used to assess the predictive power of an RC approach.  
In this scenario, we followed  the preprocessing of data and the parameters of
ESN described in \cite{ESN04}.
The parameters of intESN  (including quantization scheme) were the same as in
the subsection above.  
%{\color{brown}
The interested readers are kindly referred to the Supplementary materials (Fig. S.5) where several different values of $N$ and $\kappa$ were examined. 
%}
The length of the training sequence was 3000 (first 1000
steps were discarded from the calculation).
Fig.~\ref{fig:Mackey-Glass} depicts  results for the first 300 prediction
steps. The results were calculated from 100 simulation runs.  The figure
includes four panels. Each panel depicts the  ground truth, the mean value of
predictions as well  as areas marking percentiles between 10\% and 90\%. The
lower right  corner corresponds to intESN while three other panels show
performance of ESN in three different cases related to the quantization of
the data.

In these scenarios ESN was trained to learn the model from the  quantized
data in order to see to which extent  it affects the network.
The upper left  corner corresponds to ESN without data quantization. In this
case, the predictions precisely follow the ground truth.  The upper right corner
corresponds to ESN trained on the quantized data but  with no quantization
during the operational phase. In such settings, the network  closely follows the
ground truth for the first 150 steps but then it often explodes.  The lower left
corner corresponds to ESN where the data was quantized during  both training
and prediction. In this scenario, the network was able produce to  produce good
prediction just for the first few dozens of steps and then entered the  chaotic
mode where even the mean value does not reflect the ground truth.  These cases
demonstrate how the quantization error could affect the predictions  especially
when it is added at each prediction step.
Note that intESN operated in  the same mode as the third ESN. Despite this
fact, its performance rather resembles that of  the second ESN where the speed
deviation of the ground truth is faster.  At the same time, the deviation of
intESN grows smoothly without a sudden explosion  in contrast to ESN.

%All examples demonstrate decent approximation of the performance achieved by the conventional ESN.



