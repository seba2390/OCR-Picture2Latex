
\section{Discussion}
\label{sect:dis}


\subsubsection{Efficiency of intESN}
%{\color{red}
In principle, evaluation of the computational efficiency of the proposed intESN could be done even in a simplistic manner using only a computer.
For example, we have performed the initial assessment by simplified execution time measurements with Matlab.
We used our Matlab implementations of both networks for the trajectory association task (Section~\ref{sect:perf:trajectory}) with $N=300$ neurons to compare the times of projecting data and executing a reservoir. ESN was implemented using 32-bit float type (type \textit{single} in Matlab) while intESN was implemented using 8-bits integer type (type \textit{int8} in Matlab).
On average, the time required by intESN was $3.9$ times less than that of ESN.
Thus, even the training time needed for convergence with intESN is shorter than that of ESN because intESN is much faster when it comes to projecting data and executing a reservoir while the time for estimating the readout matrix would be the same for both networks. 
%}

%{\color{red}
Section~\ref{sect:fpga} presented the proper evaluation of the computational efficiency of the proposed intESN approach and the conventional ESN using the FPGA board. The hardware implementation of intESN was compared to two reference implementations: software ESN and hardware ESN. 
The detailed benchmarking tests have supported our claims about the efficiency of intESN. 
%}
%{\color{brown}
It is also worth noting that the efficiency gains of the intESN would be less relative to the ring-based ESN as essentially both networks use the same efficient mechanism for reservoir's connectivity. 
%}



%\hl{
%It is hard to make claims without a proper implementation design and benchmarking tests. However, for the sake %of discussion, we conjecture that given the implementation intESN could be more efficient than ESN. 



\subsubsection{Hyperparameters}

%\hl{
%{\color{red}
In comparison to ESN, intESN has fewer hyperparameters. The common hyperparameter is the reservoir size $N$. Two specific intESN hyperparameters are clipping threshold $\kappa$ and the mapping to the reservoir; while for ESN we have to choose $\rho$, $\beta$, and $\alpha$.
When $N$ is fixed then $\kappa$ in intESN has an effect on the network's memory similar to $\rho$ and $\beta$ in ESN (see the next subsection for details). 
However, the difference is that $\kappa$ takes only positive integer values. 
%}
This has its pros and cons. It could be much easier to optimize a single hyperparameter in the integer range.
On the other hand, having real-valued hyperparameters one can get a configuration providing much finer tuning of network's memory for a given task.  

With respect to the mapping (projection) to the reservoir, it is probably the most non-trivial part on intESN. Especially, when data to be projected are real numbers. In this article, we mention three different strategy for mapping real numbers to bipolar or ternary vectors: puncturing of non-zero elements (Section~\ref{sect:perf:analog}), mapping preserving linear similarity between vectors \cite{Widdows15}, and nonlinear approximate mapping using scatter codes \cite{TNNLS18, scatter}. We suggest that it is a useful heuristic to try each of the approaches and choose the one performing best.
%}

\subsubsection{On equivalence of ESN and intESN in terms of forgetting time constants}

%\hl{In fact, this work can also be conceived as an engineering design following from the recent theoretical results in~\cite{Frady17} where rigorous connections between RC and HDC were presented.
%}

%\hl{
Section~\ref{sect:perf:trajectory} presented the experimental comparison of storage capabilities of intESN and ESN. 
An analytical approach to the treatment of memory capacity of reservoir was presented in~\cite{Frady17} (Section~2.4.4). 
%In fact, it is also possible to perform a more analytical analysis as it is was shown 
The work introduced an analytical tool called the forgetting time constant (denoted as $\tau$). The forgetting time constant is a scalar characterizing the memory decay in a network. In the case of intESN, it can be calculated analytically using the clipping threshold $\kappa$. For ESN, currently only special cases can be analyzed. For example, when reservoir neurons are linear the feedback strength $\rho$ will determine $\tau$. The other example is when reservoir neurons use $\tanh$, the feedback strength is fixed to one, and the reservoir update rule is modified to $f(\textbf{x})=\gamma \tanh( \textbf{x} / \gamma)$, where $\gamma$ denotes a gain parameter. This parameter affects $\tau$, which could be calculated numerically. Fig.~\ref{fig:time:const} presents the implicit comparison of this special case of ESN and intESN via their forgetting time constants which are determined by $\gamma$ and $\kappa$ respectively. Both parameters similarly (not far from linear in logarithmic coordinates) affect $\tau$. It allows arguing that the networks are close to being functionally identical in terms of storage capabilities. The development of an approach for estimating the forgetting time constant for ESN in a general case is a part of our agenda for the future work.
%}




\subsubsection{Training the readout in a generator mode}

%\hl{
In the experiments generating time-series, we used the teacher forcing approach for training a readout matrix. This, however, does not have to be the compulsory choice for intESN. We do not foresee any complications for applying other approaches allowing to modify the network's behavior for producing complex target functions. In particular, FORCE method \cite{Sussillo2009}, which uses error-based modification of readout weights during the training process, can be used as it has a mode in which only weights of a readout matrix are changed while leaving the rest of the network fixed. 
%}


\begin{figure}[tb]%[!ht]%[t!]
\centering
\includegraphics[width=0.8\columnwidth]{img/TimeConstants}
\caption{
%\hl{
Correspondence between time-constants of intESN and special cases of ESN.
%}
}
\label{fig:time:const}
%\vspace*{-0.5cm}
\end{figure}

%\subsubsection{intESN in the scope of neural-symbolic integration}

%\hl{
%A proper placement of the usage of intESN  in the scope of a neural-symbolic
%integration area is outside the scope of this article.
%However, we want to point towards this direction. 
%}
%HDC as proposed by Kanerva and further
%generalized as Vector Symbolic Architectures was intended for representing objects, predicates and rules for %symbolic manipulation and inference.  An example of operation, which can be done with HD vectors is called a %holistic mapping. It can
%be used  to answer non-trivial queries (e.g., ``What is the dollar of Mexico?'')
%by operating on the whole representation. The first attempt of integration 
%of the reservoir computing with Vector Symbolic Architectures was presented by Yilmaz
%\cite{Yilmaz15b}. Using cellular automata to project the binarized activations of the final hidden
%layer of the trained Convolutional Neural Network into an HD vector, he was able to
%implement the inference of type ``What is is a horse of the sky'' directly on
%images \cite{Yilmaz15c}. 
%While the usage of intESN in the similar kind of
%reasoning needs deeper investigation, in this section we present a case of how
% intESN can help addressing the encoding problem \cite{Kanerva09} in a reinforcement learning
%scenario.

%\hl{
%A similar scenario could be conceived for reservoir computing.
%}
%As an illustrative use case for intESN in the context of analogical
%reasoning consider an artificial pipeline for functional modeling of
%cognitive behavior of honeybees from \cite{Bees}.  There
%HD vectors were used for representing visual scenes in the reinforcement learning context for
%modeling  correspondence between the reward and relationships between objects in
%scenes.  A relationship between the objects in a scene 
%with a square above a circle shape would be represented by an HD
%vector constructed as a composition of role-filler bindings.
%, e.g.,
%$\textbf{rel}_{visual}=\textbf{role}_{above} \oplus \textbf{square} + \textbf{role}_{below} \oplus
%\textbf{circle}$, where ``$\oplus$'' denotes operation of binding and ``$+$'' denotes
%elementwise summation.  
%\hl{
%While encoding discrete variables and shapes is rather
%straightforward in Vector Symbolic Architectures,  working with continuous
%signals is challenging. However, using the binarized activation values of intESN's reservoir modalities
%represented by continuous signals can naturally be included when forming compositional HD vectors in Vector %Symbolic Architectures.
%}


