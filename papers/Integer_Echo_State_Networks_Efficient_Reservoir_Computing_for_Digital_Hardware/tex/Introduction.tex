%\vspace*{-0.6cm}
\section{Introduction}
\label{sect:intro}
%\vspace*{-0.3cm}


%\todo[inline, color=olive]{
%DK: Adapt to the presence of hardware experiments \\
%Introduce new results on modeling chaotic systems
%}


%\hl{
Recent work in Reservoir Computing \cite{RC09, Sussillo2009} (RC) illustrates how a Recurrent Neural Network with fixed connectivity can memorize and generate complex spatio-temporal sequences. RC has been shown to be a powerful tool for modeling and predicting dynamic systems, both living \cite{ESN11NIPS} and technical \cite{ESN04, RCnature11}. 
%{\color{red}
Recently, it has been shown in~\cite{CHAOS18} that RC is able to predict large chaotic systems. 
%}


Recent work on feed-forward networks shows that the binarization of filters in Convolutional Neural Networks can lead to enormous gains in memory and computational efficiency \cite{ECCV16}. Reducing the memory allocated to each neuron or synapse from a 32-bit float to a few bits or binary saves computation with minimal loss in performance (see, e.g., \cite{BinNN, QuanNN}). The increase in efficiency broadens the range of applications for these networks. 

This article addresses two important research directions in RC: training reservoir networks and implementing networks efficiently. We discovered several direct functional similarities between the operations in RC and those of hyperdimensional computing (HDC) \cite{Frady17}. HDC \cite{Kanerva09} or more generally Vector Symbolic Architectures \cite{Gayler2003} are frameworks for neural symbolic representation, computation, and analogical reasoning. The distinction from the traditional computing is that all entities (objects, phonemes, symbols) are represented by random vectors of very high dimensionality -- several thousand dimensions. Complex data structures and analogical reasoning are implemented by simple arithmetical operations (binding, addition/bundling, and permutation) and a well-defined similarity metric \cite{Kanerva09}.
%}
Specifically, RC and HDC are connected by the following core principles:
~
\begin{itemize}
  \item Random projections of input values onto a reservoir (which in essence is a high-dimensional vector) matches random HDC representations stored in a superposition; 
  \item The update of the reservoir by a random recurrent connection matrix is similar to HDC binding/permutation operation;
  \item The nonlinearity of the reservoir can be approximated with the thresholded addition of integers in HDC. 
\end{itemize}
~
We exploited these findings to design integer Echo State Networks (intESN), which perform like Echo State Networks (ESN) but with smaller memory footprint and computational cost. 

In the proposed architecture, the reservoir of the network contains only constrained integers for each neuron, reducing the memory of each neuron from a 32-bit float to only a few bits. The recurrent matrix multiply update is replaced by a permutation (or even a cyclic shift), which results in the dramatic boosting of the computational efficiency. We validate the architecture on several tasks common in the RC literature. All examples demonstrate satisfactory approximation of performance of the conventional ESN;
%{\color{red}
while the implementation on field-programmable gate array confirms the amenability of intESN for digital hardware.
%}







%\hl{
%This article addresses challenges of the two important research directions in relation to Reservoir Computing (RC) \cite{RC09}: binarization of neural network architectures (see e.g., \cite{BinNN, %QuanNN}). The binarization of neural networks mainly provides a computational advantages and broadens the range of devices where such networks can be deployed.
%For example, } binarization of filters in Convolutional Neural Networks has demonstrated enormous gains in memory and computational efficiency \cite{ECCV16}. 
%and neural-symbolic integration \cite{NSIsur}. The former is in the main scope of this article. 

%RC appeared as an efficient approach for the training process in Recurrent Neural Networks (RNN). 

%It is a  powerful tool for modeling and predicting dynamic systems both living \cite{ESN11NIPS} and technical  \cite{ESN04, RCnature11}  systems.  

%Recent results on projecting neural activities of hidden layers
%of a Convolutional Neural Network into a framework for symbolic computations
%called hyperdimensional computing \cite{Yilmaz15c}  allow making analogies
%of kind ``What is the Automobile of Air?'' directly on image data. 


%Hyperdimensional computing (HDC) \cite{Kanerva09} or more generally Vector Symbolic
%Architectures \cite{Gayler2003} are frameworks for neural-symbolic
%representation, computation, and analogical reasoning. The distinction from the traditional
%computing is that all entities (objects, phonemes, symbols) are
%represented by random vectors of very high dimensionality -- several thousand dimensions.
%Complex data structures and analogical reasoning are implemented by simple
%arithmetical operations (binding, addition/bundling, and permutation) and a well
%defined similarity metric \cite{Kanerva09}. 
%The entities are encoded by a mapping  tobhigh-dimensional vectors \cite{Scalarencoding}.

%We discovered several direct functional similarities between the operations of
%HDC and of RC. Specifically, these
%similarities are: 
%1.) Random projections of the input values onto a reservoir (which in essence is a high-dimensional vector) matches random vector-symbol representations stored in a superposition of vectors;
%2.) The update of the reservoir by a randomly generated reservoir connection matrix is similar to HDC binding/permutation operation;
%3.) The nonlinearity of the reservoir can be matched to the thresholded addition of integers in HDC.
%We exploited these findings and propose integer Echo State Networks (intESN), which perform like ESNs with smaller memory footprint. This is the major contribution of this article.



%In the proposed architecture, the reservoir of the network contains only \textit{n}-bits integers \hl{ (where \textit{n}<8 is normally sufficient for a satisfactory performance)} for each neuron, %reducing the memory of each neuron from a 32-bit float.
%The recurrent matrix multiply update is replaced by cyclic shift, which results in the dramatic boosting of the computational efficiency. 
% I'm not sure what this sentence is saying, but I think it can get cut or moved.
%For the discretization of Echo State Networks we discovered and exploited functional similarities between RC and the operations of the HDC framework. 
%\hl{
%The intESN architecture is verified in a set of typical RC tasks: memorizing of a sequence of symbols; classifying time-series;
%learning dynamic processes. All examples demonstrate satisfactory approximation of performance of the conventional ESN.
%}

The article is structured as follows. Background and related work are presented
in Section \ref{sect:related}. The main contribution -- integer Echo State
Networks are described in Section \ref{sect:intesn}. The performance evaluation
follows in Section \ref{sect:perf}. 
%{\color{red}
Section \ref{sect:fpga} presents the experiments on digital hardware. 
%}
Sections \ref{sect:dis} and \ref{sect:conclusions}
present a discussion and conclusions.



% Both areas HDC and RC have features in common. In particular,  both are using
% distributed representation of data in high-dimensional spaces.  For example,
% ESNs use random projections \cite{RachRaPr14} to map data into 
% high-dimensional spaces. HDC usually operates with symbolic data from  a
% finite alphabet. Symbols are assigned with unique random high-dimensional vectors.
% 
% This article cross-fertilizes both areas and proposes an architecture for ESNs (intESNs) where the reservoir operates with integer numbers in the limited range. 
% The advantage of the proposed architecture is that in contrast to the standard ESN it does not require floating point operations and storage in the reservoir but the procedure for training the readout layer of neurons is the same as in the ESN. The reservoir nonlinearity operation and the mapping of data into the reservoir are taken from the HDC. This development is in line with the current research attempts to improve the computational efficiency of ANNs   by quantizing or even binarizing the training process \cite{BinNN, QuanNN, ECCV16}. 
% Also, a combination of both approaches makes a step further towards addressing the problem of neural-symbolic integration \cite{NSIsur}.
% The proposed architecture is compared with the standard ESN \cite{ESN04} in terms of its memory as well for the task of predicting dynamic system on the examples of a sinusoid function and Mackey-Glass equation. 
