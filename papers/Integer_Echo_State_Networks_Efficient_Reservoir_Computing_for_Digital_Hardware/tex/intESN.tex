%\vspace*{-0.6cm}
\section{Integer Echo State Networks}
\label{sect:intesn}
%\vspace*{-0.3cm}
% 
% \begin{figure}[!ht]%[t!]
% \centering
% \includegraphics[width=0.7\columnwidth]{img/HD_ESN}
% \caption{Architecture of the Integer Echo State Network.}
% \label{fig:intesn}
% %\vspace*{-0.5cm}
% \end{figure}

% \begin{figure}[hbt]
% \minipage{0.49\textwidth}
%   \includegraphics[width=\linewidth]{img/HD_ESN}
%   \caption{Architecture of the Integer Echo State Network.}
% \label{fig:intesn}
% \endminipage\hfill
% \minipage{0.49\textwidth}%
%   \includegraphics[width=\linewidth]{img/Discretization}
%   \caption{Quantization and discretization of a continuous signal.}
% \label{fig:quantization}
% \endminipage
% \end{figure}


\begin{figure}[tb]%[!ht]%[t!]
\centering
\includegraphics[width=1.0\columnwidth]{img/HD_ESN_new}
\caption{Architecture of the proposed integer Echo State Network.}
\label{fig:intesn}
%\vspace*{-0.5cm}
\end{figure}

This section presents the main contribution of the article -- an architecture for integer Echo State Network. 
The architecture is illustrated in Fig.~\ref{fig:intesn}. The proposed intESN is structurally identical to the the conventional ESN (see Fig.~\ref{fig:esn}) with three layers of neurons: input ($\textbf{u}(n)$, $K$ neurons), output ($\textbf{y}(n)$, $L$ neurons), and reservoir ($\textbf{x}(n)$, $N$ neurons). It is important to note from the beginning that training the readout matrix $\textbf{W}^{\text{out}}$  for intESN is the same as for the conventional ESN (Section \ref{sect:training}). 

However, other components of intESN differs from the conventional ESN.  First,
activations of input and output layers are projected into the  reservoir in the
form of bipolar HD vectors \cite{MAP} of size $N$  (denoted as
$\textbf{u}^{\text{HD}}(n)$ and $\textbf{y}^{\text{HD}}(n)$). 
 % you said this in the previous section:
% Such projection is achieved by mapping activations into high-dimensional vectors.  According to the principles of HDC, each ``symbol''  is assigned with a random high-dimensional vector used as representation in HDC.  The correspondences are stored in the so-called item memory (Fig.~\ref{fig:intesn}),  which given the symbol issues the corresponding high-dimensional vector  (denoted as HD in the figure). 
For problems where input and output data are described by finite
alphabets and each symbol can be treated independently,  the mapping to $N$-dimensional space is achieved by simply assigning a random bipolar HD vector
to each symbol in the alphabet and storing them in the item memory \cite{Kanerva09, Kleyko2015}.  In the
case with continuous data (e.g., real numbers), we quantized the continuous values
into a finite alphabet.
%A symbol in such alphabet is a quantization level.  
The quantization scheme (denoted as $Q$) and the granularity of the
quantization are problem dependent. Additionally, when there is a need to
preserve similarity between quantization levels, distance preserving mapping
schemes are applied (see, e.g., \cite{Scalarencoding, Widdows15}), which can
preserve, for example, linear or nonlinear similarity between levels. An example
of a discretization and quantization of a continuous signal as well as its
HD vectors in the item memory is illustrated in Fig.~\ref{fig:intesn}. 
%\hl{
Continuous values can be also represented in HD vectors by varying their density. For a recent overview of several mapping approaches readers are referred to \cite{TNNLS18}. Also, an example of applying such mapping is presented in Section~\ref{sect:perf:analog}.
%}
%Thus after choosing the mapping scheme activations of neurons are projected into bipolar HD vector and this vector is added to reservoir.
Another feature of intESN is the way the recurrence in the reservoir is implemented. Rather than a matrix multiply, recurrence is implemented via the permutation of the reservoir vector. Note that permutation of a vector can be described in matrix form, which can play the role of $\textbf{W}$ in intESN. Note that the spectral radius of this matrix equals one.
However, an efficient implementation of permutation can be achieved for a special case -- cyclic shift (denoted as $\text{Sh}()$). 
%{\color{red}
It is important to note that we have shown in~\cite{Frady17} that the recurrent weight matrix $\textbf{W}$ creates key-value pairs of the input data.
Note that $\textbf{W}$ is chosen randomly and kept fixed, and this always leads to the same properties.
Moreover, there is no advantage of the fully connected random recurrent weight matrix over the simple cyclic shift operation for storing the input history.
Thus, the use of the cyclic shift in place of a random  recurrent weight matrix does not limit intESN's ability to produce linearly separable representations. 
%}
Fig.~\ref{fig:intesn} shows the recurrent connections of neurons in a reservoir with recurrence by cyclic shift of one position. In this case, vector-matrix multiplication $ \textbf{W} \textbf{x}(n) $ is equivalent to $ \text{Sh}(\textbf{x}(n),1)$.   




% In the ESN the reservoir matrix is a random matrix  which posses specific properties as described in the previous section.
%\hl{
Finally, to keep the integer values of neurons, intESN uses different nonlinear activation function for the reservoir -- clipping (\ref{eq:clipping}).
%}
Note that the simplest bundling operation is an elementwise addition. However, when using the elementwise
addition, the activity of a reservoir (i.e., a composite HD vector) is no longer bipolar. From the implementation
point of view, it is practical to keep the values of the elements of the HD vector in the limited range using  a threshold value (denoted as $\kappa$). % and make it a configurable parameter. The bounding operation is called {\it clipping}. The clipping is done as follows:
~
\begin{equation}
f_\kappa (x) = 
\begin{cases}
-\kappa & x \leq -\kappa \\
x & -\kappa < x < \kappa \\
\kappa & x \geq \kappa
\end{cases}
\label{eq:clipping}
\end{equation}
~
The clipping threshold $\kappa$ is regulating nonlinear behavior of the reservoir and limiting the range of activation values. Note that in intESN the reservoir is updated only with integer bipolar vectors, and after clipping the values of neurons are still integers in the range between $-\kappa$ and $\kappa$. Thus, each neuron can be represented using only $\log_2(2\kappa+1)$ bits of memory. For example, when $\kappa=7$, there are fifteen unique values of a neuron, which can be stored with just four bits. 
We have also shown recently that the usage of the clipping might be beneficial when implementing resource-efficient alternatives of Self-Organizing Maps~\cite{intSOM}.


Summarizing the aforementioned differences, the update of intESN is described as: 
~
\begin{equation}
\textbf{x}(n)= f_\kappa (\text{Sh}(\textbf{x}(n-1),1)+\textbf{u}^{\text{HD}}(n)+\textbf{y}^{\text{HD}}(n-1)).
\label{eq:intesnres}
 \end{equation}












