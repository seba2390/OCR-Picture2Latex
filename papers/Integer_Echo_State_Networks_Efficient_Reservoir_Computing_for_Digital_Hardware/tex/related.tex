\section{Background and related work}
\label{sect:related}




There are many practical tasks that require the history of inputs to be solved.
In the area of artificial neural networks (ANN), such tasks require working memory. This could be implemented by recurrent connections between neurons of an RNN. 
%Such networks are called recurrent neural networks (RNNs). 
Training RNNs is much harder than that of feed-forward ANNs (FFNNs) due to vanishing gradient problem \cite{Bengio94}.

The challenge of training RNNs was addressed from two approaches. 
One approach eliminates the vanishing gradient problem through neurons with special memory gates \cite{LSTM97}. 
%First, the vanishing gradient problem can be eliminated through neurons with special memory gates, as it is done in Long Short-Term Memory \cite{LSTM97}.
Another approach is to reformulate the training process by learning only connections to
the last readout layer while keeping the other connections fixed.
This approach originally appeared in two similar architectures: Liquid State Machines
\cite{LSM02} and ESNs \cite{ESN03}, now referred to as reservoir computing\cite{RC09}.
%\hl{
%Despite, the simplicity of the readout layer training this process has its own nuances as the trained layer can be unstable. This problem was recently covered in \cite{Kudithipudi2018}. 
%}


It is interesting to note, that similar ideas were conceived in the area of
FFNNs, which can be seen as an RNN without memory, and are known under the name
of Extreme Learning Machines (ELMs) \cite{ELM06}.
ELMs are used to solve various machine learning problems including
classification, clustering, and regression \cite{ELM15}.

%\hl{
%The RC is a  powerful tool for modeling and predicting dynamic systems both living \cite{ESN11NIPS} and technical \cite{ESN04, RCnature11}  systems. 
%The RC is useful for  modeling and predicting dynamic systems. Another important application of the RC is a classification of time-series. 
Important applications of RC are the modeling and predicting of complex dynamic systems. 
%{\color{brown}
Generating and predicting chaotic systems was an important use-case from the beginning~\cite{Jaeger2001}, for example, ESNs were used for chaotic time-series from low-order aberrations caused by turbulence~\cite{Weddell2008}.
A thorough study on emulating chaotic systems was recently presented in~\cite{Antonik2017}.
%}
It was also shown that ESNs can be used for forecasting of Electroencephalography signals and for solving classification problems in the context of Brain-Computer Interfaces \cite{ESNEEG}.
There are different classification strategies and readout methods when performing classification of time-series with ESN. 
In \cite{ComparisonReadOut} three classification strategies and three readout methods were explored under the conditions that testing data is purposefully polluted with noise. 
Interestingly, different readout methods are preferable in different noise conditions. 
Recent work in \cite{Bianchi2018} also studied classification of multivariate time-series with ESN using standard benchmarking datasets. 
The work covered several advanced approaches, which extend the conventional ESN architecture, for generating a representation of a time-series in a reservoir. 
%}

Another recent research area is binary RC with cellular automata (CARC) which started as a interdisciplinary research within three areas: cellular automata, RC, and HDC. CARC was initially explored in \cite{Yilmaz15a}  for projecting binarized features into high-dimensional space.  Further in \cite{ISBI}, it was applied for modality classification of medical images. The usage of CARC for symbolic reasoning is explored in \cite{Yilmaz15b}. The memory characteristics of a reservoir formed by CARC are presented in  \cite{CAHD17}. Work \cite{RCELMCA17} proposed the usage of coupled cellular automata in CARC.    
%\hl{
Examples of recent RC developments also include advanced architectures such as Laplacian ESN~\cite{Han2018}, learning of reservoir's size and topology~\cite{Qiao2017}, new tools for investigating reservoir dynamics~\cite{BianchiTNNLS18} and determining its edge of criticality~\cite{Livi2018}.
%}


%{\color{brown}
The design of ESNs has been an important research area (see, e.g.,~\cite{Ozturk2007, Busing2010, Strauss2012}). 
One important aspect of the design is, of course, the choice of network's parameters for a given task.
Another important aspect considered in this study is the computational complexity. 
One of the ways of reducing computational costs would be to use quantized reservoir states. 
It was explored in Fractal Prediction Machines~\cite{Tino00} and Neural Prediction Machines~\cite{Tino04, Tino07} RC models. 
Another way of reducing computational costs involves modifications of network's connectivity structure. 
In this respect, the approach which is ideologically closest to our intESN, was presented in~\cite{MinESN}. 
The authors demonstrated that a simple cycle reservoir (referred to as the ring-based ESN) can be used to achieve a performance similar to the conventional ESN. 
Similar conclusions about the ring-based ESN were obtained in~\cite{Strauss2012} when studying different design strategies for reservoir connection matrices in four typical RC tasks. 
While the ring-based ESN explored reservoir update solution, which is similar to one of our optimizations, the technical side is very different from our approach as intESN strives at using only integers as neurons activation values.
%}





%{\color{brown}
While in this article the main focus is on reservoir states comprised of integers only, it is worth mentioning related works considering the general problem of reducing the computational complexity of the conventional ESN. 
For example, several optimizations were used in~\cite{ESNAnomaly09} in order to deploy an ESN on a resource-constrained device for anomaly detection.
These optimizations include sparse matrix algebra via compressed row storage for weights of connections between between the input layer neurons and the reservoir; single floating point precision; and an activation function, which resembles $\tanh()$ function but has lower complexity. 
Similarly, in works~\cite{Bacciu2013, Bacciu2014} ESNs were used in the context of user movements prediction on resource-constrained devices, therefore, the authors studied how the parameters of the network will affect computational costs and task performance.
In particular, the varied parameters were sparsity of reservoir connection matrix, number of bits per weight, number of neurons in reservoir.
%}



% Overview of the major theories
 
\input{ESN_overview}
\input{hd_theory}

