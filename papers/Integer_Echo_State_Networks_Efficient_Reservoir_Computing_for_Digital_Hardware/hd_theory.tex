\subsection{Fundamentals of hyperdimensional computing}
\label{sect:sparse}

In a localist representation, which is used in all modern digital
computers, a group of bits is needed in its entirety to interpret a representation.  
In HDC, all entities
(objects, phonemes, symbols, items) are represented by vectors of very high
dimensionality -- thousands of bits. The information is spread out in a {\it distributed representation}, which contrary to the
localist representations, any subset of the bits can be interpreted. 
Computing with distributed representations utilizes statistical
properties of vector spaces with very high dimensionality, which allow for approximate, noise-tolerant, highly parallel computations.
%The results are evaluated only in terms of a well-defined similarity metric.
{\it Item memory} (also referred to as {\it clean-up} memory) is needed to recover
composite representations assigned to complex concepts. There are several flavors
of HDC with distributed representations, differentiated by the
random distribution of vector elements, which can be real numbers \cite{PlateTr,
Gallant, MAP, Gallant2016}, complex  numbers \cite{PlateBook}, binary numbers
\cite{Kanerva09, Rachkovskij2001}, or bipolar \cite{Gallant, HD_ICRC16}. 

We rely on the mathematics of HDC with  
{\it bipolar distributed representations} to develop intESN.
%\subsection{Computing with dense bipolar distributed representations} 
Kanerva \cite{Kanerva09} proposed the use of distributed representations comprising $N=10, 000$
{\it binary} elements (referred to as HD vectors). The values of each element of an HD vector are independent
equally probable, hence they are also called dense distributed representations.
Similarity between two binary HD vectors is characterized by  Hamming distance,
which (for two vectors) measures the number of elements in which they differ. 
In very high dimensions  Hamming distances (normalized by the dimensionality $N$)
 between any arbitrary chosen HD vector and all other vectors in the
HD space are concentrated around 0.5. Interested readers are referred to
\cite{Kanerva09} and \cite{KanervaBook} for comprehensive
analysis of probabilistic properties of the high-dimensional
representational space.

The binary HD vectors can be equivalently mapped to
the case of bipolar representations, i.e., where each vector's element is
encoded as  ``-1'' or ``+1''. This definition is sometimes more convenient for
purely computational reasons. 
%Bipolar dense distributed representations ,however, possess a set of distinctive properties. 
The distance metric for the bipolar case is a dot product:
~
\begin{equation}
\text{dist} = \textbf{x}^\top \textbf{y}
\end{equation}
%\begin{equation}
  %   dist=|\textbf{x} \wedge \textbf{y}|_1.
 %\end{equation}
~
%\hl{
Basic symbols in HDC are referred to as  atomic HD vectors. They are generated randomly and independently,  and due to
high dimensionality will be nearly orthogonal with very high probability, i.e., similarity
(dot product) between such HD vectors is approximately 0.
An ordered sequence of symbols can be encoded into a composite HD vector
%, resembling a reservoir in RC, 
using the atomic HD vectors, the permutation (e.g., cyclic shift as a special case of permutation) and bundling operations.
This vector encodes the entire sequence history in the composite HD vector and resembles a neural reservoir. 
%}

%\hl{
Normally in HDC, the recovery of component atomic HD vectors from a composite HD vector is performed by finding the most similar vectors stored in the item memory\footnote{It is not common to do such decoding in RC. Normally, in the scope of RC a readout matrix is learned. In this article, we follow this standard RC approach to extracting information back from a reservoir.}. 
However, as more vectors are bundled together there is more interference noise and the likelihood of recovering the correct atomic HD vector declines. 

Our recent work \cite{Frady17} reveals the impact of interference noise, and shows that different flavors of HDC have universal memory capacity. Thus, the different flavors of HDC can be interchanged without affecting performance. From these insights, we are able to design much more efficient networks for reservoir computing for digital hardware. 
%}
