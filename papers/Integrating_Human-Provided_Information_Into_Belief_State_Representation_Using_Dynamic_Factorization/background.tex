\section{Background}
\subsection{\pomdp s and Belief States}
We formalize agent-environment interaction in the presence of
uncertainty as a \emph{partially observable Markov decision process}
(\pomdp)~\cite{pomdp}. We consider a typical undiscounted setting
with: $\St$, the state space; $\A$, the action space; $\Obs$, the
observation space; $T(s, a, s') = P(s' \mid s, a)$, the transition
distribution with $s, s' \in \St, a \in \A$;
$O(s', a, o) = P(o \mid s', a)$, the observation model with
$s' \in \St, a \in \A, o \in \Obs$; and $R(s, a, s')$, the reward
function with $s, s' \in \St, a \in \A$. Some states in $\St$ are
\emph{terminal}, ending the episode.

At each timestep, the agent selects an action, causing 1) the hidden
state to change according to $T$, 2) the agent to receive a reward
according to $R$, and 3) the agent to receive an observation according
to $O$. The agent's objective is to maximize its overall expected
reward, $\mathbb{E}\left[\sum_{t}R(s_{t}, a_{t}, s_{t+1})\right]$. A
solution to a \pomdp\ is a policy that maps the history of
observations and actions to the next action to take, such that this
objective is optimized over the trajectory.

The sequence of states $s_{0}, s_{1}, ...$ is unobserved, so the agent
must instead maintain a \emph{belief state}: a probability
distribution over the space of world states. This belief is updated on
each timestep based on the received observation and taken action. The
exact belief update is
$B'(s') = \frac{1}{Z}\left[O(s', a, o) \sum_{s \in \St} T(s, a,
  s')B(s)\right],$ where $B$ and $B'$ are the old and new belief
states, $s' \in \St$ is a state, $a \in \A$ is the taken action,
$o \in \Obs$ is the received observation, and $Z$ is a normalizing
factor.

Representing the full belief exactly is prohibitively expensive
for even moderately-sized \pomdp s, so a typical alternative approach
is to use a \emph{factored} representation~\cite{boyenkoller}. Here, we assume that the
state can be decomposed into a set of features, each of which has a
value. The factored belief is then a mapping from every feature to a
distribution over its value. Typically, one chooses the features
carefully so that observations can be \emph{folded}, i.e.
incorporated, into the belief efficiently and without too much loss of
information. In other words, the chosen distributions are conjugate to
the most frequent kinds of observations. Most factored representations
are updated \emph{eagerly} (without explicitly remembering the actions and
observations) but approximately. On the other hand, a \emph{fully
  lazy} representation just appends $a$ and $o$ to a list at each
timestep. Though belief updates are trivial with this lazy
representation, inference can be very expensive. In our work, we will
give a representation that is sometimes eager and sometimes lazy,
based on how expensive it would be to perform an eager update.

Some popular approaches for generating policies in \pomdp s are online
planning~\cite{pomcp,despot,beliefspaceplanning} and finding a policy
offline with a point-based solver~\cite{sarsop,pointpomdp}. Our work
will use the more efficient but more approximate
\emph{determinize-and-replan} approach, which optimistically plans in
a determinized version of the environment, brought about by (for
instance) assuming that the maximum likelihood observation is always
obtained~\cite{plattmlobs,bsptamp}. The agent executes this plan and
replans any time it receives an observation contradicting the
optimistic assumptions made.

\subsection{Factor Graphs}
We will be viewing our approach from the perspective of factor graphs,
which we briefly describe in this section. We refer the reader to work
by Kschischang et al.~\cite{factorgraph} for a more thorough treatment. A \emph{factor
  graph} is a bipartite undirected probabilistic graphical model
containing two types of nodes: \emph{variables} and
\emph{factors}. Factor graphs provide a compact representation for the
factorization of a function. Suppose a function $f$ on $n$ variables
can be decomposed as
$f(X_{1}, X_{2}, ..., X_{n}) = \prod_{i=1}^{m}f_{i}(C_{i}),$ where
each $C_{i} \subseteq \{X_{1}, X_{2}, ..., X_{n}\}$ is a subset of the
variables. This decomposition corresponds to a factor graph in which
the variables are the $X_{j}$, the factors are the $f_{i}$, and there
is an edge between any $f_{i}$ and $X_{j}$ for which
$X_{j} \in C_{i}$, i.e. $f_{i}$ is a function of $X_{j}$.

This representation affords efficient marginal inference, which is the
process of computing the marginal distribution of a variable, possibly
conditioned on the values of some other variables. Message-passing
algorithms such as the sum-product algorithm typically compute
marginals using dynamic programming to recursively send messages
between neighboring nodes. The sum-product algorithm is also commonly
referred to as \emph{belief propagation}.

% Message-passing algorithms compute exact marginals if the underlying
% factor graph is a tree; however, if the factor graph has cycles,
% exactness is usually lost. Typical approximation methods include Monte
% Carlo methods~\cite{mcmc} or several iterations of loopy belief
% propagation~\cite{loopybeliefpropagation}. If exactness is required,
% the junction tree algorithm~\cite{jordan1998learning} does compute
% exact marginals in the cyclic setting, but it requires compiling a
% junction tree data structure before query time. This preprocessing is
% especially infeasible in our setting, where we will see that the
% structure of the factor graph is constantly changing due to the
% incoming information.
