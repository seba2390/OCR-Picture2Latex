\section{Formal Problem Setting}
In this section, we formalize our problem setting as a \pomdp, for
which we will show that a dynamically factored belief is a good
representation for the agent's belief state.

\emph{Assumptions.} We will assume deterministic transitions and a uniform
observation model over all valid observations, in order to make
progress on this difficult problem.

\subsection{Planning Problem Class}
We first describe the underlying class of planning problems. We
consider \emph{open domains}, in which the world contains a
(finite or infinite) universe of objects, but the agent does not know this
universe. Planning in open domains is significantly more complex than
planning in settings where the universe of objects is known in
advance.

The class of \emph{open-domain planning problems} $\Pi$ contains tuples
$\langle \T, \P, \Obj, \V, \F, \U, \I, \G \rangle$:
\begin{tightlist}
\item $\T$ is a known set of object \emph{types}, such as locations or
  movables. For some types, the set of objects may be
  known to the agent in advance; for others, it may not.
\item $\P$ is a known set of \emph{properties} (such as color,
  size, pose, or contents) for each type from $\T$. Each property has
  an associated (possibly infinite) domain.
\item $\Obj$ is a (possibly partially) unknown set of \emph{objects},
  each of a type from $\T$. This set $\Obj$ can be finite or infinite.
\item $\V$ is the set of \emph{state variables} resulting from
  applying each property in $\P$ to every object in $\Obj$ of the
  corresponding type. Each variable has a domain based on the
  property and can be either continuous or discrete.
\item $\F$ is a set of \emph{fluents} or \emph{constraints},
  Boolean-valued expressions given by a predicate applied to state
  variables and (possibly) values in their domains. Examples:
  \emph{Equal(size(obj1), 6)}; \emph{Different(color(obj2),
    color(obj3))}.
\item $\U$ is a set of object-parametrized \emph{operators} that
  represent ways the agent can affect its environment. Each has
  preconditions (partial assignment of values to $\V$ that must hold for
  it to be legal), effects (partial assignment of values to $\V$ that
  holds after it is performed), and a cost.
\item $\I$ is an assignment of values to $\V$ defining the \emph{initial state}.
\item $\G$ is a partial assignment of values to $\V$ defining the \emph{goal}.
\end{tightlist}

A solution to a problem in $\Pi$ is a minimum-cost sequence of
parametrized operators $u_{1}, ..., u_{n} \in \U$ (a \emph{plan}) such
that starting with $\I$ and applying the $u_{i}$ sequentially
satisfies operator preconditions and causes the partial assignment
$\G$ to hold. Variables in $\V$ that are not in $\G$ may have any
value.

\subsection{\pomdp\ Formulation}
With $\Pi$ defined, we are ready to formulate our setting as a
\pomdp. Let $\langle \T, \P, \Obj, \V, \F, \U, \I, \G \rangle$ be an
open-domain planning problem from $\Pi$. Define the \pomdp:
\begin{tightlist}
\item $\St$ (the state space) is the space of all possible assignments
  of values to $\V$. A state is, thus, an assignment of a value to
  each variable in $\V$. Note that $\I$ is a state.
\item $\A$ (the action space) is $\U$.
\item $\Obs$ (the observation space) is the space of all (potentially
  noisy) observations: we define an observation as a set of pairs
  $\langle f, p \rangle$, where $f \in \F$ and $p \in (0,
  1]$. Intuitively, the interpretation is that every fluent (constraint) $f$ in this
  set holds with probability $p$ in the current state. We assume each
  observation $o$ comes from either 1) the robot's own perceptual
  capabilities or 2) an \emph{assertion} about the environment, which
  simulates human-provided information. For each fluent $f$ in $o$,
  the corresponding $p$ is a measure of confidence in $f$ holding within the
  current state. It can be based on the quality of a sensor or on the
  human's certainty about the veracity of their assertion. Because
  $\Obj$ is unknown, a fluent may contain state variables referencing
  objects the agent has not encountered before.
\item $T(s, a, s')$ (the transition distribution) is 1 if $s$
  satisfies $a$'s preconditions and $s'$ its effects; 0 otherwise.
\item $O(s', a, o)$ (the observation model) is a uniform distribution
  over all valid observations.
\item $R(s', a)$ (the reward function) is the negative cost of $a$.
\item $s \in \St$ is \emph{terminal}, ending the episode, if $\G$ holds in $s$.
\end{tightlist}

A solution to this \pomdp\ is a policy that maps the history of
observations and actions to the next action to take, such that the sum
of expected rewards is maximized: observe that this corresponds to
solving $\Pi$. Sources of uncertainty in this formulation are the open
domain and the noisy observations.

Next, we present the \emph{dynamically factored belief} as a good
belief state representation for this \pomdp. Though this
representation does not depend on the presence of assertions or an
open domain, we present it within this context because it best
motivates the approach and manifests its strengths.

\section{Dynamically Factored Belief}
\subsection{Overview}
In trying to find a suitable belief state representation for the
\pomdp\ defined in the previous section, we must be cognizant of the
fact that the agent does not know $\Obj$, the complete set of objects
in the world. A natural representation to use might be a factored one
over each state variable in $\V$, but unfortunately, if $\Obj$ is unknown
then so is $\V$. Furthermore, fluents in the observations may be
complicated expressions involving multiple state variables; we would
like our representation to be able to incorporate these observations.

\begin{figure}[t]
  \vspace{0.6em}
  \centering
    \noindent
    \includegraphics[width=\columnwidth]{figures/dfboverview.jpg}
    \caption{Example of dynamic factoring given four sequential
      noiseless observations in a setting with one object type and two
      properties: \emph{color} and \emph{location}. Each factor maps
      to a distribution over values (not shown). Initially, there are
      no factors. \emph{Row 1:} The agent receives an assertion
      containing one state variable, \emph{color(A)}; a singleton
      factor for this variable is introduced. \emph{Row 2:} The
      assertion contains two state variables, so a joint factor is
      introduced. \emph{Row 3:} The assertion contains a new state
      variable, \emph{location(D)}, so a factor is introduced for it,
      then joined with the [\emph{location(B), location(C)}]
      factor. \emph{Row 4:} The agent observes the color and location
      of Object B. The joint distribution over the locations of B, C,
      and D is now uniform across the first dimension, so the factor
      gets split into two. This type of splitting implies that factors
      do not necessarily get bigger each time a new observation
      arrives. In this figure, all observations are noiseless (all
      $p=1$) for clarity of presentation. The factor graph is always a
      collection of independent subgraphs, where each subgraph
      contains exactly one factor.}
  \label{fig:dfboverview}
\end{figure}

We note the following. First, we do not need to know the full set of
state variables in order to maintain a (partial) factored belief
representation. Second, the choice of factors should be dynamic and
influenced by the observations, allowing us to gracefully cope with
complicated assertions. The intuition is that a constraint linking two
state variables would not be foldable into a factored representation
over each state variable, so the representation should be
modified. Building on these ideas, we present the following
definition.

\begin{defn}
  A \emph{dynamically factored belief state representation} has two components:
  \begin{tightlist}
  \item A factored representation for which each factor is a
    \emph{list} of one or more state variables from $\V$. The factors
    partition the set of state variables that the agent knows about so
    far, either from prior knowledge or from a received
    observation. Each factor maps to a joint probability distribution
    over possible values for all its variables.
  \item A database called ComplexFluents.
  \end{tightlist}
\end{defn}

The belief is initialized to have a singleton factor for each state
variable in $\V$ associated with an object in $\Obj$ known a priori to
exist, mapping to any distribution (e.g. a prior).

Each time an observation $o \in \Obs$ is received, the belief is
updated as follows with every constituent fluent $f$: all factors
containing state variables mentioned by $f$ are introduced (if they
are not represented yet) and joined, so long as the resulting joint
would not be too big. If it would be too big, then the fluent is
lazily placed into ComplexFluents and considered only at query
time. Furthermore, factors are regularly split up for efficiency,
implying that a belief update could potentially compress the
representation. This is shown in \figref{fig:dfboverview} and
described in detail in the next section. Newly introduced variables
can map to any distribution, such as a uniform one, or one calculated
from some prior knowledge.

The factors partition the set of state variables that the agent knows
about so far, meaning no state variable can ever be present in more
than one factor. Enforcing this invariant greatly simplifies
inference, as can be understood from a factor graph perspective. A
dynamically factored belief maintains a factor graph with variable
nodes $V$ and factor nodes $F$, where the $V$ are the state variables
in $\V$ that the agent knows about so far, and the $F$ are the
factors. Each node $F$ connects to all the $V$ that are present in the
corresponding list (and thus comprise that factor's joint
distribution). Because the factors partition the variables, this
factor graph is a collection of independent subgraphs, where each
subgraph contains exactly one node from $F$ connected to one or more
nodes from $V$. See \figref{fig:dfboverview} for an example. Fluents
placed lazily into ComplexFluents are not part of this factor
graph. The structure of this factor graph is constantly changing as
new objects are discovered and observations are received, but it will
always be a collection of independent subgraphs. Thus, there is no
possibility of ending up with a cyclic factor graph, which would
typically necessitate either approximate inference or expensive
exact inference.

\emph{Note.} All fluents, complex or not, can be represented by a
factor in some more complex factor graph. One could imagine a
different algorithm that avoids constructing the joint of all
variables mentioned by a fluent, but instead creates a new factor for the
fluent while leaving other factors untouched. This would likely create
cycles, but cycles could be collapsed into joint distributions over
all constituent nodes. Inference would then be done using
message-passing. In contrast, our method eagerly incorporates fluents
in a way that makes inference fast and cycles impossible. Furthermore,
our method can very quickly answer queries about a marginal on a
subset of any factor, as we will see.

\subsection{Belief Update Algorithm}
\begin{algorithm}[t]
  \SetAlgoLined
  \SetAlgoNoEnd
  \DontPrintSemicolon
  \SetKwFunction{algo}{algo}\SetKwFunction{proc}{proc}
  \SetKwProg{myalg}{Algorithm}{}{}
  \SetKwProg{myproc}{Subroutine}{}{}
  \SetKw{Continue}{continue}
  \SetKw{Return}{return}
  \myalg{Dynamically Factored Belief Update}{
    \nl $B \gets$ InitializeFactoredBeliefMap()\;
    \nl ComplexFluents $\gets$ set(\{\})\;
    \myproc{\textsc{BeliefUpdate}(observation, action)}{
        \nl \For{each $\langle f, p \rangle$ in observation}{
          \nl \For{each stateVar mentioned by $f$}{
            \nl \If{!$B$.Contains(stateVar)}{
              \nl $B$.Add([stateVar], defaultDist())\;
            }
          }
          \nl \eIf{joint would be too big}{
            \nl ComplexFluents.Add($f$)\;
            }{
          \nl $B$.JoinFactorsAndUpdate($f, p$)\;}
        }
        \nl $B$.UpdateWithAction(action)\;
        \tcp{\footnotesize Keep representation compact.}
        \nl \For{each factor in $B$.Factors}{
          \nl $B$.TrySplit(factor, $\epsilon$)\;
        }
        }}\;
\caption{Dynamically factored belief update.}
\label{alg:dynamicbeliefupdate}
\end{algorithm}

\algref{alg:dynamicbeliefupdate} gives pseudocode for updating a
dynamically factored belief. The belief is initialized to contain a
singleton factor for each state variable in $\V$ associated with an
object in $\Obj$ known a priori to exist. The \textsc{BeliefUpdate} subroutine is called at
each timestep, after the agent takes an action and receives an
observation (a set of fluents, each with a corresponding probability
of holding in the world).

Line 7 decides whether joining all factors containing a state variable
mentioned by the fluent would result in a joint that is too ``big'' (expensive to compute or represent). If so, the fluent is
lazily stored in the database ComplexFluents and considered only at
query time. An implementation of this test could take the product of
the factor sizes and check whether it is above a certain threshold.

% Line 10 updates the belief based on the action taken in the
% environment. Here, we are making a simplifying assumption that belief
% updates based on environment transitions are always foldable into the
% current factorization. This explains why the factor graph underlying
% our approach does not need to augment variables with a temporal
% component, as is done in dynamic factor
% graphs~\cite{dynamicfactorgraphslecun,dynamicfactorgraphskampa}.

\begin{algorithm}[t]
  \SetAlgoLined
  \SetAlgoNoEnd
  \DontPrintSemicolon
  \SetKwFunction{algo}{algo}\SetKwFunction{proc}{proc}
  \SetKwProg{myalg}{Algorithm}{}{}
  \SetKwProg{myproc}{Subroutine}{}{}
  \SetKw{Continue}{continue}
  \SetKw{Return}{return}
  \myproc{\textsc{JoinFactorsAndUpdate}($B, f, p$)}{
      \nl joint $\gets$ (join factors $F$ containing variables in $f$)\;
      \nl $m \gets$ (total prob. of values inconsistent with $f$)\;
      \nl \For{each value in joint}{
        \nl \If{value is inconsistent with $f$}{
          \nl joint.RescaleProbBy$\left(\text{value}, \frac{(1-p)(1-m)}{pm}\right)$\;
        }
      }
      \nl joint.Normalize()\;
      \nl Add joint to $B$ and remove all $F$ from $B$.\;
    }\;

    \myproc{\textsc{TrySplit}($B$, factor, $\epsilon$)}{
      \nl \For{each state variable $V$ in factor}{
        \nl reconstructed $\gets$ Join($B$[V], $B$[factor $\setminus$ \{V\}])\;
        \nl \If{$\js{B[\text{factor}]}{\text{reconstructed}} < \epsilon$}{
          \nl Split $B$[factor] into $B$[V], $B$[factor $\setminus$ \{V\}].\;
        }
      }
    }\;
  \caption{Subroutines used in \algref{alg:dynamicbeliefupdate}.}
\label{alg:subroutines}
\end{algorithm}

\textbf{Joining Factors} \ The call in Line 9 to
\textsc{JoinFactorsAndUpdate} creates a new factor containing all
state variables that are mentioned by the fluent $f$ (if such a factor
is not already present), then maps this factor to a joint distribution
for which $f$ holds with probability $p$. To accomplish this, we build
the joint then rescale the probabilities such that $p$ mass goes to
the joint values which are consistent with $f$, and the remaining
$1-p$ mass goes to those which are inconsistent. This can be done
using Jeffrey's rule~\cite{jeffreysrule}: rescale the probability of
all values inconsistent with $f$ by $\frac{(1-p)(1-m)}{pm}$, where $m$
is the total probability of these inconsistent values, then
normalize. When $p = 1$, this algorithm just filters out joint values
inconsistent with $f$, as expected. See \algref{alg:subroutines} for
pseudocode and \figref{fig:joinfactorsandupdate} for an example. If
distributions are continuous, we perform these operations implicitly
using rejection sampling at query time.

\textbf{Splitting Factors} \ The call in Line 12 to \textsc{TrySplit}
attempts to split up factors to maintain a compact representation. In
practice, we accomplish this by checking whether each state variable
in the factor can be marginalized out. Of course, it is unlikely that
such marginalization can ever be done in a lossless manner, as this
would require the joint to be exactly decomposable into a product
involving this marginal. Instead, we perform marginalization whenever
the reconstruction error is less than a hyperparameter $\epsilon$. We
measure this reconstruction error as the Jensen-Shannon divergence
$D_{JS}$ between the true joint and the approximate joint
reconstructed from the attempted decomposition.

Let $P$ and $Q$ be arbitrary probability distributions. The
Jensen-Shannon divergence is a smooth, symmetric, bounded measure of
similarity between $P$ and $Q$. It is based on the Kullback-Leibler
(KL) divergence, defined as
$\kl{P}{Q} = -\sum_{i}P(i) \log \frac{Q(i)}{P(i)},$ where the
summation can be replaced by integration for continuous
distributions. Then letting $A = \frac{1}{2}(P+Q)$, the Jensen-Shannon
divergence is defined as
$\js{P}{Q} = \frac{1}{2} \kl{P}{A} + \frac{1}{2} \kl {Q}{A}.$ Assuming
the natural logarithm is used, the bound
$0 \leq \js{P}{Q} \leq \log 2$ always holds. Since $D_{JS}$ is
bounded, it is reasonable to use $\epsilon$ as a fixed threshold on
the reconstruction error to decide whether to do
marginalization. Varying $\epsilon$ lets the designer trade off
between compactness of the belief and accuracy of inference. See
\algref{alg:subroutines} for pseudocode and \figref{fig:trysplit} for
an example.

\begin{figure}[t]
  \vspace{0.6em}
  \centering
    \noindent
    \includegraphics[width=0.45\textwidth]{figures/noisyupdate1.png}

    \vspace{0.7em}

    \includegraphics[width=0.45\textwidth]{figures/noisyupdate2.png}
    \caption{Posteriors from running \textsc{JoinFactorsAndUpdate}
      with fluent \emph{Same(color(object1), color(object2))}, for
      three values of $p$.}
  \label{fig:joinfactorsandupdate}
\end{figure}

\begin{figure}[t]
  \centering
    \noindent
    \includegraphics[width=0.45\textwidth]{figures/noisysplit1.png}

    \vspace{0.7em}

    \includegraphics[width=0.45\textwidth]{figures/noisysplit2.png}
    \caption{Two examples of trying to split up a factor having two
      variables. \emph{Top:} The variables are independent; no error
      is incurred. \emph{Bottom:} Error is incurred based on the
      Jensen-Shannon divergence between the joint and the product of
      the marginals ($\sim$0.01 here).}
  \label{fig:trysplit}
\end{figure}

\subsection{Inference}
Dynamically factored beliefs can handle two types of queries: 1) a
marginal on any state variable, or on a set of state variables that is
a subset of some factor, and 2) a sample from the full joint of
current factors, which gives a world state consistent with the agent's
current knowledge.

To answer queries of type 1), we observe that if a state variable or set of state
variables is a subset of some factor, then our representation already
stores a joint distribution over values for those
variables. Any query about them can be answered
using this joint, e.g. by sampling. We do not have
to worry about violating the constraints in ComplexFluents because
each of those could only ever constrain a set of state variables that
spans multiple factors.

To answer queries of type 2), we must draw from the distribution implicitly encoded by
all factors together, which produces an assignment that maps every
currently known state variable to a value. One can
treat this as a constraint satisfaction problem~\cite{csp} where
the constraints are the fluents in ComplexFluents (all other fluents
have already been folded eagerly into the belief), and apply standard
solving techniques. Our experiments solve it incrementally using a backtracking
approach; see \algref{alg:samplestate} for pseudocode.

\begin{algorithm}[t]
  \SetAlgoLined
  \SetAlgoNoEnd
  \DontPrintSemicolon
  \SetKwFunction{algo}{algo}\SetKwFunction{proc}{proc}
  \SetKwProg{myalg}{Algorithm}{}{}
  \SetKwProg{myproc}{Subroutine}{}{}
  \SetKw{Continue}{continue}
  \SetKw{Return}{return}
  \myalg{\textsc{SampleState}($B$, ComplexFluents)}{
      \nl state $\gets$ map(each state variable in $B \to$ NULL)\;
      \nl curIndex $\gets$ 0\;
      \nl \While{curIndex $<$ number of factors}{
        \nl factor $\gets$ $B$.Factors[curIndex]\;
        \nl \If{sampling limit reached}{
          \nl \For{each stateVar in factor}{
            \nl state[stateVar] $\gets$ NULL\;}
          \nl curIndex $\gets$ curIndex $-$ 1\;
          \Continue}
        \nl values $\gets B$[factor].Sample()\;
        \nl \For{stateVar, value in zip(factor, values)}{
          \nl state[stateVar] $\gets$ value\;}
        \nl \If{any $f$ in ComplexFluents cannot hold}{\Continue}
        \nl curIndex $\gets$ curIndex + 1\;
      }
      \Return state
    }\;
\caption{An incremental algorithm for sampling a world state
  consistent with all observations, using a dynamically factored
  belief $B$. The returned state is an assignment of currently known state
  variables to values.}
\label{alg:samplestate}
\end{algorithm}

% \subsection{Illustrative Example}
% We illustrate updates and inference with a simple example. Consider a
% planning problem from $\Pi$ (Section \ref{ppclass}) with one object
% type and one property, \emph{color}, whose domain is \{red,
% green\}. The robot is tasked with determining the color of every
% object in the world. Unbeknownst to the robot, there are 3 objects: A,
% B, and C. Initially, there are no factors.

% Suppose the robot takes an action resulting in an observation that
% Object A is green. The belief is updated with a new factor
% [\emph{color(A)}] mapping to a distribution over values; thus, the new
% belief is \{[\emph{color(A)}]: \{[red]: 0, [green]: 1\}\}. At this
% point, a query asking for a sample from the full joint would (using
% Algorithm \ref{alg:samplestate}) be answered with the assignment
% \{\emph{color(A)}: green\}, since the robot does not know about
% Objects B or C.

% Next, suppose the robot receives an assertion that Objects B and C are
% the same color. The new belief is \{[\emph{color(A)}]: \{[red]: 0,
% [green]: 1\}, [\emph{color(B), color(C)}]: \{[red, red]: 0.5, [red,
% green]: 0, [green, red]: 0, [green, green]: 0.5\}\}. Now, a query
% asking for a sample from the full joint would (using Algorithm
% \ref{alg:samplestate}) be answered with either \{\emph{color(A)}:
% green, \emph{color(B)}: red, \emph{color(C)}: red\} or
% \{\emph{color(A)}: green, \emph{color(B)}: green, \emph{color(C)}:
% green\}, each with equal probability.

% Although this example is simple, it highlights the core idea of our
% approach: when the robot receives the assertion that \emph{color(B)} =
% \emph{color(C)}, it updates its belief to contain a joint distribution
% [\emph{color(B), color(C)}] over the colors of both objects. The
% assertion is completely folded into this joint, so marginal inference
% can be done quickly by marginalizing out variables from this joint,
% and samples can be drawn quickly too. By contrast, a belief
% representation with a fixed factoring over singleton state variables
% would not be able to incorporate the assertion that \emph{color(B)}
% =\emph{color(C)}, so accurate inference would require independently
% sampling values for \emph{color(B)} and \emph{color(C)} until they
% agree. This can become prohibitively expensive as the domain size, the
% number of objects, and the number of assertions increase.
