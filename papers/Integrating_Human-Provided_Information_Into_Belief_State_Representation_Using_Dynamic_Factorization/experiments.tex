\section{Experiments}
We evaluate the performance of our approach on the cooking task, a
planning problem from $\Pi$. The robot is tasked with gathering
ingredients and using them to cook a meal. There are three object
types: $\T = \{$\emph{locations}, \emph{vegetables},
\emph{seasonings}$\}$. The term \emph{ingredients} refers to
vegetables and seasonings together. Each object type has one
property. Locations have a \emph{contents} property, which is one of
``vegetable,'' ``seasoning,'' or ``empty.'' Ingredients have a
\emph{position} property, which could be continuous- or
discrete-valued. Initially, the robot knows the set of locations but
not the set of ingredients. There is a pot at a fixed, known position.

When any vegetable is placed into the pot, it transitions to a
\emph{cooking} state; 5 timesteps later, it transitions to a
\emph{cooked} state. The goal is to have all ingredients in the pot
and all vegetables cooked. However, the robot is penalized heavily for
placing any seasoning into the pot too early, before all vegetables
have been cooked. To achieve the goal, the robot must learn the
positions of all ingredients, either by doing observations or by
learning about them from assertions.

The operators (actions) $\U$ are:
\begin{tightlist}
\item \textsc{Observe(location)}: Moves and observes the ingredient(s)
  at a location. Cost: 5.
\item \textsc{Pick(position)}: Moves and picks at a continuous- or
  discrete-valued position (domain-dependent). The robot can
  hold up to 10 ingredients at once. Cost: 20.
\item \textsc{PlaceInPot()}: Places all $n$ held ingredients into the
  pot. Vegetables in the pot are either \emph{cooking} or, 5 timesteps
  later, \emph{cooked}. Cost: 100 + 50$n$, plus an additional 1000 if
  a seasoning is placed in before all vegetables are cooked.
\item \textsc{No-op()}: Takes no action. Cost: 0.
\end{tightlist}
There is also a living cost of 10 per timestep.

The state variables $\V$ comprise each location's contents and each
ingredient's position. The world state contains an assignment of these
variables to values. It also tracks which ingredients are held by the
robot and the pot, and the current robot pose; these are all assumed
to be known and thus do not need to be tracked by the belief state.

\emph{Assertions.} \figref{fig:assertions} shows the types of
assertions we use. At each timestep, we sample an assertion uniformly
at random from all valid ones, following our observation model, and
give it to the robot. The information could be redundant.

\begin{figure}[t]
  \vspace{0.6em}
  \centering
    \noindent
    \includegraphics[width=\columnwidth]{figures/assertions.jpg}
    \caption{An illustration of the types of assertions we use, with a
      simplified execution of the gridworld cooking task. There are
      four locations: L1, L2, L3, L4. Factors are color-coded based on
      the number of state variables. c($\cdot$) means
      \emph{contents($\cdot$)}, p($\cdot$) means
      \emph{position($\cdot$)}. Initially, there is a singleton factor
      for each location's contents. The last two columns tell whether
      the fluent is foldable into a dynamic factoring (our approach),
      and into a static factoring that tracks the potential contents
      of each location (baseline). Unfoldable fluents get placed into
      ComplexFluents, slowing down inference. Observe that singleton
      factors go in and out of joints.}
  \label{fig:assertions}
\end{figure}

\emph{Baseline.} We test against a baseline belief representation that
simulates prior work on factored representations, which typically
commit to a representational choice at the start. This baseline, which
we call a \emph{statically factored belief} or \emph{static
  factoring}, represents the agent's belief as a distribution over the
potential contents of each location. The factoring does not change
based on observations, or as ingredients are discovered. We choose
this factoring as our baseline because initially, the robot only knows
the set of locations, not the set of ingredients. Thus, this factoring
is the most reasonable choice for a static representation that is
chosen at the start and held fixed throughout execution. Any fluent
that cannot fold into this static factoring is lazily placed into
ComplexFluents and considered only at query time.

\textbf{Domain 1: Discrete 2D Gridworld Cooking Task} \ Our first
experimental domain is the cooking task in a 2D gridworld. Locations
are organized in a 2D grid, and the robot is in exactly one at any
time. Both \textsc{Observe} and \textsc{Pick} actions are performed on
single grid locations. Each location is initialized to contain either
a single ingredient or nothing, so the \emph{position} property of
each ingredient is a discrete value corresponding to a location. We
solve the task using the determinize-and-replan approach to belief
space planning, with A* as the planner. The task is computationally
challenging, so we impose a 60-second timeout per episode.

\textbf{Domain 2: Continuous 3D Cooking Task} \ Our second
experimental domain is the cooking task in a pybullet
simulation~\cite{pybullet}. Here, the \emph{position} property of each
ingredient is a continuous value: each can be placed anywhere on the
surface of one of four tables. The robot can \textsc{Observe} any
table and \textsc{Pick} at any position along a table surface, so the
action space is continuous. Locations are given by a grid
discretization of the environment geometry. As ingredients get
discovered, a dynamically factored belief adapts to track continuous
distributions over their positions. Again, we use the
determinize-and-replan method and a 60-second timeout.

\begin{table}[t]
  \vspace{0.6em}
  \centering
  \resizebox{\columnwidth}{!}{
  \tabcolsep=0.08cm{
  \begin{tabular}{c|c|c|c|c}
    \toprule[1.5pt]
    \textbf{Setting} & \textbf{System} & \textbf{\% Solved} & \textbf{Bel. Upd. Time} & \textbf{Queries / Second}\\
    \midrule[2pt]
    Dom. 1, 4x4, 6ing & S (baseline) & 67 & 0.01 & 0.11\\
    \midrule
    Dom. 1, 4x4, 6ing & D (ours) & \textbf{100} & 0.03 & \textbf{20}\\
    \midrule
    Dom. 1, 4x4, 10ing & S (baseline) & 50 & 0.01 & 0.068\\
    \midrule
    Dom. 1, 4x4, 10ing & D (ours) & \textbf{100} & 0.04 & \textbf{16.7}\\
    \midrule[1.5pt]
    Dom. 1, 5x5, 6ing & S (baseline) & 13 & 0.01 & 0.039\\
    \midrule
    Dom. 1, 5x5, 6ing & D (ours) & \textbf{100} & 0.06 & \textbf{2.27}\\
    \midrule
    Dom. 1, 5x5, 10ing & S (baseline) & 5 & 0.01 & 0.031\\
    \midrule
    Dom. 1, 5x5, 10ing & D (ours) & \textbf{99} & 0.08 & \textbf{2.3}\\
    \midrule[1.5pt]
    Dom. 1, 6x6, 6ing & S (baseline) & 5 & 0.01 & 0.019\\
    \midrule
    Dom. 1, 6x6, 6ing & D (ours) & \textbf{100} & 0.13 & \textbf{1.15}\\
    \midrule
    Dom. 1, 6x6, 10ing & S (baseline) & 5 & 0.01 & 0.028\\
    \midrule
    Dom. 1, 6x6, 10ing & D (ours) & \textbf{97} & 0.23 & \textbf{0.338}\\
    \midrule[2pt]
    Dom. 2, 8ing & S (baseline) & 55 & 0.07 & 0.495\\
    \midrule
    Dom. 2, 8ing & D (ours) & \textbf{100} & 0.11 & \textbf{5.56}\\
    \midrule
    Dom. 2, 10ing & S (baseline) & 48 & 0.07 & 0.287\\
    \midrule
    Dom. 2, 10ing & D (ours) & \textbf{100} & 0.14 & \textbf{4.76}\\
    \bottomrule[1.5pt]
  \end{tabular}}}
\caption{Some of our experimental results. Each row reports averages
  over 100 independent episodes. Percentage of tasks solved within
  60-second timeout, belief update time (seconds), and average number
  of queries answered per second (across solved tasks) are shown. S:
  Statically factored belief (baseline), D: Dynamically factored
  belief (our method). \emph{Setting} column gives domain (``Dom. 1''
  is gridworld, ``Dom. 2'' is continuous), grid size (if applicable),
  and number of ingredients. Our approach solves more tasks than a
  static factoring does, and inference is an order of magnitude
  faster.}
\label{table:results}
\end{table}

\textbf{Results and Discussion} \ \tabref{table:results} and
\figref{fig:results} show results when all $p$ are 1 and $\epsilon$ is
0 (see \algref{alg:subroutines}), while \figref{fig:noisyresults}
shows results with noisy observations where $p$ and $\epsilon$
vary. Overall, our approach solves significantly more tasks than a
static factoring does, and also does inference an order of magnitude
faster. However, our method could perform badly when belief
updates are very expensive, which could happen if we try to eagerly
incorporate fluents that link several state variables with large
domains. Typically in practice, though, such fluents would be placed
into ComplexFluents. As $p$ decreases, observations get
noisier, so execution costs and factor sizes increase. As $\epsilon$
increases, more marginalization occurs and inference accuracy is
lower, so factors are smaller but execution is costlier.

\begin{figure}[t]
  \vspace{0.6em}
  \centering
  \noindent
  \includegraphics[width=0.58\columnwidth]{figures/results_gw10ing_runtimecdf.png}
  \includegraphics[width=0.4\columnwidth,height=3.7cm]{figures/cooking_cont.png}
  \caption{\emph{Left:} Cumulative distribution functions showing the
    percentage of the 100 episodes we ran with our method that got
    solved within different running times, for the 10-ingredient
    gridworld. Although our timeout was set to 60 seconds, most tasks
    were solved within 10 seconds, whereas the baseline (not shown)
    timed out frequently (see \tabref{table:results}). \emph{Right:} A visualization of the
    continuous 3D cooking domain. The robot is a light-blue-and-orange
    arm. Vegetables (red) and seasonings (blue) are placed across four
    tables.}
  \label{fig:results}
\end{figure}

\begin{figure}[t]
  \centering
    \noindent
    \includegraphics[width=0.49\columnwidth]{figures/results_noisy_costvsp.png}
    \includegraphics[width=0.49\columnwidth]{figures/results_noisy_costvserror.png}

    \vspace{0.7em}

    \includegraphics[width=0.49\columnwidth]{figures/results_noisy_factorsizevsp.png}
    \includegraphics[width=0.49\columnwidth]{figures/results_noisy_factorsizevserror.png}
    \caption{Results for varying $p$ and $\epsilon$
      (see \algref{alg:subroutines}) in our experiments. As $p$ decreases,
      observations get noisier, so execution costs and factor sizes
      increase. As $\epsilon$ increases, more marginalization occurs
      (with reconstruction error) and inference accuracy decreases, so
      factors are smaller but execution is costlier.}
  \label{fig:noisyresults}
\end{figure}