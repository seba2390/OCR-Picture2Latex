\section{Related Work}
We focus on the setting of \emph{information-giving} for open-domain
human-robot collaboration. Information-giving was first explored
algorithmically by McCarthy~\cite{programswithcommonsense} in a
seminal 1959 paper on advice-takers. Much work in open-domain
collaboration focuses on the robot understanding goals given by the
human~\cite{talamadupula1}, whereas we focus on understanding
information given by the human, as in work on
advice-giving~\cite{odom2015learning} and commonsense
reasoning~\cite{corpp}.

\subsection{Adaptive Belief Representations}
Our work explores belief state representations that adapt to the
structure of the observations received by the robot. The work perhaps
most similar to ours is that of Lison et
al.~\cite{beliefsituationaware}, who acknowledge the importance of
information fusion and abstraction in uncertain environments. Building
on Markov Logic Networks~\cite{mln}, they describe a method for belief
refinement that 1) groups percepts likely to be referring to the same
object, 2) fuses information about objects based on these percept
groups, and 3) dynamically evolves the belief over time by combining
it with those in the past and future. They focus on beliefs about each
object in the environment, whereas our work focuses on combining
information about multiple objects, based on the structure of the
observations.

The notion of adaptive belief representations has also been explored
in domains outside robotics. For instance, Sleep~\cite{adaptivegrid}
applies this idea to an acoustic target-tracking setting. The belief
representation, which tracks potential locations of targets, can
expand to store additional information about the targets that may be
important for locating them, such as their acoustic power. The belief
can also contract to remove information that is deemed no longer
necessary. It would be difficult to update this factoring with
information linking multiple targets, whereas our method is
well-suited to incorporating such relational constraints.

\subsection{Factored Belief Representations for \pomdp s}
The more general problem of finding efficient belief representations
for \pomdp s is very well-studied. Boyen and Koller~\cite{boyenkoller}
were the first to provide a tractable method for belief propagation
and inference in a hidden Markov model or dynamic Bayesian
network. Their basic strategy is to first pick a computationally
tractable approximate belief representation (such as a factored one),
then after a belief update, fold the newly obtained belief into the
chosen approximate representation. This technique is a specific
application of the more general principle of \emph{assumed density
  filtering}~\cite{maybeck1982stochastic}. Although it seems that the
approximation error will build up and propagate, the authors show that
actually, the error remains bounded under reasonable assumptions. Our
work adopts a fluid notion of a factored belief representation, not
committing to one factoring.

Bonet and Geffner~\cite{bonet2014belief} introduce the idea of beam tracking for belief
tracking in a \pomdp. Their method leverages the fact that a problem
can be decomposed into projected subproblems, allowing the joint
distribution over state variables to be represented as a product over
factors. Then, the authors introduce an alternative decomposition over
beams, subsets of variables that are causally relevant to each
observable variable. This work shares with ours the idea of having the
structure of the belief representation not be fixed ahead of time. A
difference, however, is that the decomposition used in beam tracking
is informed by the structure of the underlying model, while ours is
informed by the structure of the observations and can thus efficiently
incorporate arbitrary relational constraints on the world state.

% \subsection{Markov Logic Networks}
% Markov Logic Networks (MLNs)~\cite{mln} are a sound and practical mechanism for
% combining probabilistic inference and first-order logic into a unified
% representation. An MLN is a template for a ground Markov network,
% consisting of a knowledge base (KB), a weight for each formula in the
% KB, and a set of constants. Thus, an MLN can be viewed as a soft
% version of a KB in which each formula has an associated measure of
% importance, and each world state has a corresponding likelihood based
% on the total importance of the formulas it satisfies.

% A key difference between MLNs and our work is that the structure of
% the MLN is fixed and defined by the formulas and constants. On the
% other hand, our system works online, changing the structure of the
% underlying representation dynamically based on new formulas and
% constraints that arise while the robot is acting in the world. This
% type of dynamic sensitivity is crucial for real-world systems, in
% which it would be difficult to find a fixed structure that could
% properly capture the full space of incoming information. Another
% difference is that our work folds the provided first-order logic
% observations into the belief representation dynamically rather than
% storing them explicitly (as is needed for inference with MLNs), making
% inference easier at the cost of slightly more expensive updates.