\section{Introduction}
As robots become increasingly adept at understanding and manipulating
the world around them, it becomes important to enable humans to
interact with them to convey goals, give advice, or ask questions. A
typical setting is a partially observed environment in which the robot
has uncertainty about its surroundings, but a human can give it
\emph{information} to help it act more intelligently. This information
could represent complex relationships among properties of objects in
the world, but the robot would be expected to use it as needed when
given a task or query. This setting motivates an important question:
what is the best way to represent the robot's internal knowledge so
that this information is correctly processed and combined with the
robot's own sensory observations? It is important for the chosen
representation to be able to accurately and efficiently answer queries
(i.e. do inference) that require it to draw on the given information.

\begin{figure}[t]
  \centering
    \noindent
    \includegraphics[width=\columnwidth]{figures/teaser.jpg}
    \caption{A schematic illustration of a dynamically factored
      belief. The initial belief $B$ tracks distributions over
      possible values for the locations of two objects. There are
      three locations in the world (not shown): Loc1 on the left, Loc2
      in the middle, and Loc3 on the right. When the agent is given
      information that the objects are next to each other, the belief
      is updated to produce $B'$. This is a new factoring in which the
      two old factors are joined into a single one, corresponding to a
      distribution over the joint location of both objects. Other
      factors (not shown) would not be affected.}
  \label{fig:teaser}
\end{figure}

We consider a specific class of open-domain planning problems in which
objects exist in the world, but the agent does not know the universe
of objects. We formalize our setting as a partially observable Markov
decision process in which the robot has two sources of (potentially
noisy) observations: its own perceptual capabilities, and
\emph{assertions} about the environment that simulate the human-provided
information and are expressed in formal language. These observations
represent constraints that hold with some probability and relate
properties of objects in the world.

In order to support inference in partially observed environments, one
typically maintains a belief state: a probability distribution over
the space of world states. Unfortunately, the full joint distribution
is usually intractable to work with. A popular alternative approach is
to represent a factored belief state, in which the world state is
decomposed into a set of features, each with a value. The factored
belief is then a mapping from every feature to a distribution over its
value.

We propose a method for efficient inference using a factored belief
state in the presence of potentially complicated assertions relating
multiple variables. The typical approach to using a factored belief
state involves committing to a (possibly domain-specific)
representational choice at the very
start~\cite{bonet2014belief,boyenkoller,sallans2000learning}, for
which can be difficult to fold in arbitrary relational constraints
without too much loss in accuracy. On the other hand, our work treats
a factored belief state as a fluid, dynamic data structure in which
the factoring itself is molded by the constraints, as suggested by
\figref{fig:teaser}. We call this a \emph{dynamically factored
  belief}.

For the class of open-domain planning problems we consider, we show
that a dynamically factored belief state representation provides
significant improvements in inference time over a fixed factoring. We
validate our approach experimentally in two open-domain planning
problems: a 2D discrete gridworld task and a 3D continuous cooking
task.

Visit \texttt{http://tinyurl.com/chitnis-iros-18} for a supplementary
video.
