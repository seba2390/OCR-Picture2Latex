\section{Methodology} 



In this work, we consider the UDA task, where we have a source domain $p^s(x,y)$ and a target domain $p^t(x,y)$, and our learning framework has access to a labeled source set $\{(x_i^s,y_i^s)\}$ drawn from $p^s(x,y)$ and an unlabeled target set $\{(x_i^t)\}$ drawn from $p^t(x,y)$. The class label space of $y_i\in\left\{1,2,\cdots,N\right\}$ is shared for both source and target domains. We use $n$ to index $N$ classes. For class $n$, we assume that there are $K_n$ underlying subtypes indexed with $k\in\{1,2,\cdots, K_n\}$. UDA aims to build a good classifier in the target domain $p^t(x,y)$ with the following theorem: \\~


\noindent\textbf{Theorem 1} For a hypothesis $h$ drawn from $\mathcal{H}$, $\epsilon^t(h)\leq$ $\epsilon^s(h)+\frac{1}{2}d_{\mathcal{H}\triangle\mathcal{H}}\{s,t\}+^{\rm{min}}_{h\in\mathcal{H}}[\epsilon^s({h},l_s)+\epsilon^t({h},l_t)]$. \\~

 
 
\noindent Here, $\epsilon^s(h)$ and $\epsilon^t(h)$ denote the expected loss with hypothesis $h$ in the source and target domains, respectively. Considering that the disagreement between labeling function $l_s$ and $l_t$, i.e., $^{\rm{min}}_{h\in\mathcal{H}}[\epsilon^s({h},l_s)+\epsilon^t({h},l_t)]$, can be small by optimizing $h$ with the source data \cite{ben2007analysis}, the UDA focuses on minimizing the cross-domain divergence $d_{\mathcal{H}\triangle\mathcal{H}}\{s,t\}$ in the feature space of $f(x_i^s)$ and $f(x_i^t)$.


Instead of aligning $p^s(f(x))$ and $p^t(f(x))$ \cite{kouw2018introduction}, the prototypical networks propose to match the class centroids \cite{pan2019transferrable}. However, the decision boundary of the low-density distributed target sample can be difficult to define, and the inherent subtype structure is underexplored.  
 
On the embedding space, we expect the class separation in the target domain can be achieved when the source domain classes are well-separated, and the class-wise source-target distribution compactness is enforced. Moreover, under the fine-grained subtype local structures and their conditional and label shifts, the subtype-wise tight clustering can be a good alternative to achieve class-wise alignment and compactness. Accordingly, we have the following proposition:\\~

\noindent\textbf{Proposition 1.} The class-wise compactness can be a special case of subtype-wise compactness by assigning the subtype number of this class to 1. \\~

Targeting the conditional and label shifts in both class-level ($p^s(f(x)|y)\neq p^t(f(x)|y), p^s(y)\neq p^t(y))$ and subtype-level ($p^s(f(x)|k)\neq p^t(f(x)|k), p^s(k)\neq p^t(k))$, we propose a novel subtype-aware alignment framework based on an adaptive clustering scheme as shown in Fig. \ref{fig:11}. 
 
\subsection{Class-wise source separation and matching} 

Forcing the separation of classes in the source domain, $p^s(x,y)$, can be achieved by the conventional CE loss \cite{liu2016large}. With the extracted features, we carry out the classification via a remold of the distance to each class cluster center \cite{chen2019progressive,pan2019transferrable}.



For the labeled source data $\{(x_i^s,y_i^s)\}$, we represent the feature distribution of class $n$ with a class centroid $c_n^s=\frac{1}{M_c^s}\sum_{i=1}^{M_c^s}f(x_i^s)$, where ${M_c^s}$ is the involved source sample number. For an input source sample $x_i^s$, we can directly produce a probability histogram with the softmax normalized distance between $x_i^s$ and the centroids $c_n^s$. Specifically, the probability of $x_i^s$ belonging to class $n$ can be formulated as 

 
\begin{equation}
\begin{aligned}
p(y_i^s=n|x_i^s)=\frac{e^{-||f(x_i^s)-c_n^s||_2^2}}{\sum_{n=1}^N e^{-||f(x_i^s)-c_n^s||_2^2}}.\label{eq:1}
\end{aligned}\end{equation} With the one-hot encoding of true class $n$, we define the class-wise CE loss for the source domain samples as $\mathcal{L}_{CE}^{class}=-\text{log} p(y_i^s=n|x_i^s)$.  

 

Following the self-labeling scheme \cite{zou2019confidence,pan2019transferrable}, each target sample $x_i^t$ is assigned with a pseudo class label $\hat{y}_i^t$ to its nearest source centroids, i.e., $\hat{y}_i^t=n$ if $~_{\forall n}^{\text{min}}||f(x_i^t)-c_n^s||_2^2$. With the pseudo class label $\hat{y}_i^t$, we calculate the target domain class-level centroids $c_n^t=\frac{1}{M_c^t}\sum_{i=1}^{M_c^t}f(x_i^t)$, where ${M_c^t}$ is the involved target sample number. We expect the close proximity of $c_n^s$ and $c_n^t$ with $\mathcal{L}^{class}={\frac{1}{N}\sum_{n=1}^N}||c_n^s-c_n^t||_2^2$, which is not sensitive to label shift, since it only chooses the representative centroids of the source and target distribution\footnote{The source \& target center $c_n^{st}=\frac{\sum_{i=1}^{M_c^s+M_c^t}f(x_i^{st})}{M_c^s+M_c^t}$ used in \cite{pan2019transferrable}, and its objective of close proximity of $c_n^s\leftrightarrow c_n^{st}$, or $c_n^t\leftrightarrow c_n^{st}$ are not robust to label shift. Note that $c_n^{st}$ will change if we simply double the involved source/target samples.}. However, neither $\mathcal{L}_{CE}^{class}$ nor $\mathcal{L}^{class}$ considers the inner-class compactness \cite{wen2016discriminative} and the fine-grained subtype structure. 




  




\subsection{Subtype-aware alignment with $K_n$ prior} 


 
If the subtype numbers $K_n$ of class $n$ is known (e.g., 4 subtypes in the CHD disease dataset), we can achieve feature space class-independent clustering with the concise $K$-means, by defining $K$ to be $K_n$. 

 


We denote $K_n$ clustered subtypes with $k\in\{1,2,\cdots, K_n\}$, and calculate the source and target subtype centroids $\mu_k^s$ and $\mu_k^t$, respectively. However, $K$-means does not assign the specific class label to each cluster. To correlate the source and target clusters, we rank the distance of $K_n^2$ subtype centroid pairs and link the smallest rank first. %Then, we remove the involved two source and target centroids to link the next pair with the smallest rank.   


Because of the imbalance distribution of subtypes and possible label shift, we assign the subtype centroids of both the source and target samples with $\mu_k^{st}=\frac{\mu_k^s+\mu_k^t}{2}$ instead of averaging all of the source and target samples in subtype $k$. Therefore, each subtype in both source and target domains contributes equally to $\mu_k^{st}$. Then, we enforce all of the samples in subtype $k$ to be close to the subtype centroid $\mu_k^{s,t}$. For the sake of simplicity, we omit the class notation. The subtype compactness objective $\mathcal{L}_k^{sub}$ can be \begin{equation}
\begin{aligned}
   \frac{1}{M_k^s}\sum_{i=1}^{M_k^s}||f(x_i^s)-\mu_k^{st}||_2^2+\frac{1}{M_k^t}\sum_{i=1}^{M_k^t}||f(x_i^t)-\mu_k^{st}||_2^2, \label{eq:2}
\end{aligned}\end{equation} where $M_k^s$ and $M_k^t$ are the numbers of source and target samples in subtype $k$, respectively, to balance the subtype label shift. $\mathcal{L}_k^{sub}$ is traversed for $N$ classes and their $K_n$ subtypes to calculate the sum of normalized subtype compactness loss $\mathcal{L}^{sub}=\frac{1}{N}\sum^N(\frac{1}{K_n}\sum^{K_n}\mathcal{L}_k^{sub})$. Note that an image that does not belong to class $n$ does not belong to any of its $K_n$ subtypes. The class-wise matching and subtype-wise compactness objectives can be aggregated as a hierarchical alignment loss $\frac{1}{N}\sum^N(\alpha\mathcal{L}^{class}+\beta\frac{1}{K_n}\sum^{K_n}\mathcal{L}_k^{sub})$, where $\alpha$ and $\beta$ are the balancing parameters. In the feature space, the learned representations are expected to form $K_n$ compact clusters for class $n$, while each cluster does not need to be far away from one another. 


 



\begin{figure}[t]
\centering
\includegraphics[width=8.8cm]{fig//st3.pdf}\\ 
\caption{Illustration of the reliability-path based sub-graph construction and alignment with $m=3$. \protect\includegraphics[scale=1.2]{fig/subfig1.png} will be assigned to subtype 2, which will then be rejected by  $\tau$.}\label{fig:33} 
\end{figure}



\subsection{Reliability-path based sub-graph construction}  




Defining or estimating $K_n$ can be difficult in many applications. Setting $K_n$ of all classes as $N$ hyper-parameters requires costly trails considering the diverse value range of different classes. Note that with a deterministic encoder $f$, the distribution protocol can be similar among different subtypes and classes \cite{fahad2014survey}. 
 
 
 

Therefore, we propose constructing the sub-graph with the reliability-path to achieve the online source domain subtype clustering. We assume that similar samples are likely to be distributed closely in the feature space with a deterministic encoder $f$ and form a high-density region \cite{carlucci2019domain}. Given $M^{s}$ samples from the class $n$ in the source domain, there are $(M^{s})^2$ possible edges in a graph. To explore the local structure of the feature space, the two nodes $\{f(x_i^s),f(x_j^s)\}$ are connected by the reliability-path, if $||f(x_i^s)-f(x_j^s)||_2^2\leq\epsilon$. The directly or indirectly linked nodes are combined to form a sub-graph. 

To further eliminate the effect of noise and undersampled subtypes on a batch, we only select the sub-graphs with more than $m$ nodes as the valid subtype clusters. 


After exploring the $K_n$ subtypes in the source domain and calculating their centroids $\mu_k^s$, we assign each target sample with the pseudo label of class $n$ to the subtype with the most similar centroid (i.e., $~_{\forall k}^{\rm min}||f(x_i^t)-\mu_k^s||_2^2)$. 

Considering the relatively low confidence or reliability of pseudo target labels \cite{zou2019confidence,gu2020spherical}, we adopt a simple online semi-hard mining scheme to select the target sample in a subtype. The cross-domain margin $\tau$ is used to define a circle at the center of $\mu_k^s$. For the target sample with the initial pseudo subtype label $k$, we choose these samples to distribute within the circle. We note that some target samples may be densely distributed around the circle boundary, and it is not reasonable to cut them apart simply. Therefore, we also resort to the reliability-path to involve the closely distributed neighboring target samples. The sub-graph construction can be robust to missing subtypes in the source or target domains caused by undersampling, since $m$ filters the unreliable source cluster out, and the self-labeling with semi-hard mining of $f(x_i^t)$ rejects the additional subtypes in the sampled target domain. The operation is illustrated in Fig. \ref{fig:33}.

With the reliability-path connected $M_k^s$ source samples and the refined $M_k^t$ target samples in subtype $k$, we calculate $\mu_k^{st}=\frac{\mu_k^s+\mu_k^t}{2}$, and enforce the subtype-wise compactness with $\mathcal{L}_k^{sub}$ as in Eq.~(\ref{eq:2}). 

The online sub-graph construction and alignment have three hyperparameters, including $\epsilon$, $\tau$, and $m$ that are shared for all classes and their subtypes, which can be regarded as the meta-knowledge across clusters. Moreover, we can simplify $\epsilon$ to be the constant 1, and change it to any other positive value results only in the matrices being multiplied by corresponding factors \cite{liu2017adaptive}. The range of $m$ can also be narrow and similar among different subtypes/classes.







\subsection{Optimization and implementation} 

The modern neural networks usually extract a high-dimensional vector, e.g., 4,096 or 2,048-dimensional features, as their representation, thereby demanding high memory and time complexity in subsequent clustering. To remedy this, deep clustering \cite{caron2018deep} proposes to perform dimension reduction via Principal Component Analysis (PCA) for the extracted features from all of the samples in a dataset. PCA, however, is not applicable anymore in our online SubUDA. The feature representations are extracted in different training iterations with different timestamps, which can have incompatible statistics. It is also computationally demanding to carry out PCA for all iterations. Accordingly, a non-linear head-layer with the structure of {fc$\rightarrow$bn$\rightarrow$relu$\rightarrow$dropout$\rightarrow$fc$\rightarrow$relu} is adopted in order to reduce high dimensional features into 256 dimensions. As well, it is simultaneously optimized in each online SubUDA iteration. The non-linear head-layer is eliminated for the subsequent operations, e.g., calculating the L2 distance between features.

In order to prevent the subtype clustering from collapsing to a few subtype groups, \cite{caron2018deep} makes uniform sampling in all of the epochs, which is difficult in our online UDA setting, due to the missing subtype and target class labels. We thus propose a concise approach for SubUDA via re-weighting the loss with $\omega_k\propto\frac{1}{\sqrt{M_k^s+M_k^t}}$, according to the number of samples in the $k$-{th} subtype. Therefore, samples in smaller clusters are accounted more for the loss, which thereby pushes the classification boundary away to incorporate as more samples as possible. The optimization objective can be summarized as \begin{equation}
\begin{aligned}
\mathcal{L}=\mathcal{L}_{CE}^{class}+\frac{1}{N}\sum^N(\alpha\mathcal{L}^{class}+\beta\frac{\omega_k}{K_n}\sum^{K_n}\mathcal{L}_k^{sub}).
\end{aligned}
\end{equation}
 
For the classification in testing, we utilize the centroids of training features as prototypical \cite{pan2019transferrable}.  
 

 
 
 
 
 
 
 
