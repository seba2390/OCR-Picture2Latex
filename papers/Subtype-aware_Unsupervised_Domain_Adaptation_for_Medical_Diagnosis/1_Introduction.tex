\section{Introduction}

The goal of unsupervised domain adaptation (UDA) is to transfer knowledge learned from a label-rich domain to new unlabeled target domains \cite{saito2017adversarial,liu2020energy,liu2020disentanglement,zou2019confidence}. The conventional adversarial training and maximum mean discrepancy (MMD) based methods propose to align the marginal distribution of sample $x$ in the feature space, i.e., $p(f(x))$, where $f(\cdot)$ is a feature extractor. Given the Bayes' theorem $p(f(x)|y)=\frac{p(y|f(x))p(f(x))}{p(y)}$, suppose that there are no concept and label shifts (i.e., $p(y|f(x))$ and $p(y)$ are the same for two domains), then the conditional distribution $p(f(x)|y)$ can be aligned by aligning $p(f(x))$. Nonetheless, the label shift $p(y)$ is quite common in real-world applications, which indicates the label proportion is different \cite{zhao2019learning}. 


%The conventional empirical risk minimization relies on the independently and identically distributed (i.i.d.) training and testing data \cite{goodfellow2016deep,liu2019feature}. The collection of a large number of medical imaging data for deep learning is challenging for many diseases, thereby hindering the wide adoption of deep learning methods \cite{shen2017deep}. To mitigate the constraint of limited training data, the unsupervised domain adaptation (UDA) is developed to transfer knowledge learned from a label-rich domain to a new unlabeled target domain.
 
%To align the classifier in two domains, we are expecting the alignment of $p(f(x)|y)$, where $f$ is a feature extractor. The classical UDA methods, e.g., adversarial training, maximum mean discrepancy (MMD), focus on the alignment of the marginal distribution of sample $x$'s representation $p(f(x))$. 
 
\begin{figure}[t]
\centering
\includegraphics[width=8.5cm]{fig//st1.pdf}\\
\caption{Illustration of the failure case of prototypical UDA (left), and the idea of our subtype-aware UDA (right).}\label{fig:11} 
\end{figure}



%With the Bayes' theorem $p(f(x)|y)=\frac{p(y|f(x))p(f(x))}{p(y)}$, we need to align $p(y|f(x))$, $p(f(x))$ and $p(y)$ simutaniously. While the conventional UDA methods assume there is no concept and label shifts (i.e., $p(y|f(x))$ and $p(y)$ are the same for two domains). Then, the conditional distribution $p(f(x)|y)$ can be aligned by aligning $p(f(x))$. Nevertheless, the label shift $p(y)$ is quite common in real-world applications, which indicates the label proportion is different \cite{zhao2019learning}.  






% However, the domain shifts widely exist and the deep learning systems can fail to generalize well to a new implementation environment.    

%This motivates the  to transfer the knowledge from a labeled source domain to a different target domain with the help of unlabeled target data \cite{kouw2018introduction}. Since we are focusing on the discriminative ability on the target domain, the class conditional alignment can be necessary. 


Recently, transferable prototypical networks (TPN) \cite{pan2019transferrable} is proposed to promote the source domain class separation with a cross-entropy (CE) loss, and match the class centroids of source and target samples to perform class-wise alignment. However, the CE loss in the source domain cannot minimize the inner-class variation \cite{liu2016large}. The class-wise separation in the to-be tested target domain cannot be well supported by the centroid closeness objective. As shown in Fig. \ref{fig:11} left, although the class centroids are well aligned, the sparsely distributed target samples can be easily misclassified. 
 
One way to tackle this is by simply enforcing the cross-domain inner-class feature distribution compactness. However, in many cases, the unlabeled subtypes in a class can be diverse, and form an underlying local distribution. For instance, different cancer subtypes may have significantly diverse patterns \cite{yeoh2002classification}. In such circumstances, the shared pattern among two different subtypes may not be exclusive for class-level discrimination. In these applications, it would be more reasonable and effective to exploit the subtype-wise patterns. Moreover, unsupervised deep clustering \cite{caron2018deep} empirically assigns 10$\times$ more clusters of the class to contain diverse subtypes. Recent works also show the fine-grained label can be helpful for the coarse classification \cite{chen2019understanding}.

%For example, the VisDA17 dataset \cite{visda2017} has car, truck, and bus classes. The car class includes the sedan, SUV, van, etc., which have large inner-class variations, and some of its subtypes can be visually similar to the bus/truck. The shared pattern among the sedan, SUV, and van may not be exclusive for the bus/truck. 

Moreover, domain shifts can be different w.r.t. subtypes, which leads to \textit{subtype conditional shift}. Besides, the incidence of disease subtypes is usually varied across different regions, which leads to \textit{subtype label shift}. The proportion difference at the subtype-level can usually be more significant than the class-level \cite{wu2019domain}. This motivates us to extend the concept of class conditional and label shifts \cite{kouw2018introduction} to the fine-grained subtype-level (i.e., $p(f(x)|k)$ and $p(k)$ vary across domains for subtype $k$). Therefore, a more realistic presumption of UDA can be both the class and subtype conditional and label shifts.

 
 
 
%The disease subtypes can also be more suitable detected by different equipment.
 

%The aforementioned issues motivate the exploration of fine-grained subtype-aware alignment. 


In this work, we resort to the feature space metric learning with intermediate pseudo labels to adaptively achieve both class-wise separation and cross-domain subtype-wise compactness. We first propose an online clustering scheme to explore the underlying subtype structure in an unsupervised fashion. With the prior knowledge of subtype numbers, concise $k$-means clustering can be simply applied, by assigning $k$ to the subtype numbers. However, the subtype can be challenging to define, due to different taxonomy. We thereby further expand on our framework to unknown subtype numbers by capturing the underlying subtype structure with an adaptive sub-graph scheme using a reliability-path. With a few meta hyperparameters shared between clusters, the sub-graph scheme can be scalable to several classes and subtypes. To explicitly enforce the subtype-wise distribution compactness, the involved samples of a subtype are expected to be close to their subtype centroid in the feature space. 


%The adaptively explored source and target domain subtype clusters are matched to form several source-target subtype pairs. 

%Considering the possible unstable learning caused by the undersampling of subtypes and the subtype label shift, we base our training on a dynamic memory framework to steadily evolve the cluster centroids to keep the network stably updated. Specifically, we design and maintain a feature stack module and a centroid memory module to store the sample features of several batches and the current centroids. These two modules are updated alternatively, and the features are refined with a momentum scheme. It can make more representative sampling and clustering with low space cost. We breakdown the abrupt off-line entire dataset clustering \cite{caron2018deep} into steady memory updates and batch-wise pseudo label re-assignment.  Moreover, our sub-graph and subtype centroid construction are robust against the undersampling or label shift of subtype.     

 

 
 

Our main contribution can be summarized as follows:

\begin{itemize}
    \item We propose to adaptively explore the subtype-wise conditional and label shifts in UDA without the subtype labels, and explicitly enforce the subtype-aware compactness.
    
    \item We systematically investigate the cases with or without the prior information on subtype numbers. Our reliability-path based sub-graph scheme can effectively explore the underlying subtype local distribution with a few meta hyperparameters in an online fashion.
    
    \item  We empirically validate its effectiveness on a multi-view congenital heart disease (CHD) diagnosis task with an efficient multi-view processing network and achieve promising performance.
    
    
\end{itemize}

 