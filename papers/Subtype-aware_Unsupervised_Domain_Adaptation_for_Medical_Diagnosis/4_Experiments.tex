

\begin{figure}[t]
\begin{center}
\includegraphics[width=1\linewidth]{fig//Fig2.pdf}
\end{center} 
\caption{Data pre-processing flowchart.}
\label{fig:2}
\end{figure}

\section{Experiments}

We carried out experiments using CHD data to evaluate the effectiveness of our approach. We implemented our method and comparison methods using PyTorch and set $\alpha=1$, $\lambda=0.5$, and $\beta=0.5$ consistently.

CHD is one of the most common types of birth defect, which usually results in the death of neonates. Therefore, early medical care and treatment can be helpful, which requires an efficient and accurate diagnosis. In clinical practice, clinicians rely on echocardiograms from five heart views. Collected raw data include echocardiogram videos, and a key-frame of each view is usually extracted for the assessment. Specifically, the collected views are from the parasternal long-axis (PSLAX), parasternal short-axis (PSSAX), apical four chambers (A4C), subxiphoid long-axis (SXLAX), and suprasternal long-axis (SSLAX). 

To quantify the effect of subtype structure, four subtypes of CHD, including atrial septal defect (ASD), ventricular septal defect (VSD), patent ductus artery (PDA), and tetralogy of Fallot (TOF), are labeled by two clinicians or intraoperative records. We note that the fine-grained subtype label is not used in training, since large-scale labeling can be costly in clinical practice, while the normal/patient label can be relatively easy to acquire by primary clinicians. Of note, this work focuses on exploring the subtype-aware alignment for the conventional class-wise discriminative model.

Specifically, we evaluated our method on five-view echocardiogram datasets collected from two medical centers. We used 1,608 labeled source subjects (normal/patient) from Beijing Children's Hospital (BCH) and 800 unlabeled target subjects from Beth Israel Deaconess Medical Center (BIDMC). Each dataset consists of echocardiograms from five views that are sufficient for the diagnosis. 

%The use of the collected data was granted by the BCH Ethics Committee (No. 2019-k-342).
 
Abnormal regions are likely to be different from one subtype to another; as such, different subtypes can be easily detected from different views. Considering the large inner-class variation of the patient class, it is reasonable to enforce the subtype-wise compactness. In Fig. \ref{fig:55} left, we can see that the source CHD samples of a subtype also tend to be distributed closely, demonstrating the underlying inner-subtype similarity. With our subtype-wise compactness objective as shown in Fig. \ref{fig:55} right, both the source and target samples are grouped into the high-density region w.r.t. subtypes.
 
\subsection{Data collection of cardiac ultrasound images}

\begin{figure}[t]
\centering
\includegraphics[width=8.5cm]{fig//st5.pdf}\\ 
\caption{T-SNE visualization of the sampled CHD features with (a) TPN \cite{pan2019transferrable} and (b) our SubUDA. We normalized the distance between class centroids to demonstrate the class separation and inner-class/subtype compactness.}\label{fig:55} 
\end{figure}

$\bullet$ \textbf{BCH source domain}. A total of 1,608 echocardiogram datasets, including 823 healthy controls, 209 VSD, 276 ASD, 124 TOF, and 176 PDA, were collected using PHILIPS iE 33. The chest of each patient was exposed to the echocardiogram with the supine position. We set our transducer frequency between 3 to 8 MHz. We collected the five standard 2D views, i.e., PSLAX, PSSAX, A4C, SXLAX, and SSLAX.
 
$\bullet$ \textbf{BIDMC target domain}. A total of 800 echocardiogram datasets, including 300 healthy controls, 150 VSD, 150 ASD, 100 TOF, and 100 PDA, were collected. Similarly, the chest of each patient was exposed to the echocardiogram with the supine position. We used PHILIPS EPIQ 7C for echocardiogram imaging and set its transducer frequency from 3 to 8 MHz. We also collected the five standard 2D views, i.e., PSLAX, PSSAX, A4C, SXLAX, and SSLAX.

Notably, discrepancies between these two medical centers include imaging devices (PHILIPS iE 33 vs. EPIQ 7C), patient populations, and clinicians' echocardiogram imaging experience, which introduced domain shifts.

We also extracted the key frame from the video of each view. The heart is a dynamic organ and has different shape within a heart cycle. Thus, we chose a time frame that the clear defects were visible. Specifically, the key frame was corresponding to the isovolumic relaxation phase.

%when the ventricles finished contracting and started to relax,  of VSD and ASD both could be shown clearly at that time.  

We selected 80\% and 20\% subjects from the target domain for training and testing, respectively. Note that we only selected the subjects with all the key frames from the five views as our testing data. In addition, the key frames shown in training were not used for testing in a subject-independent manner. 


\subsection{Pre-processing}

The selected color key frames were first translated to the gray images, since the echocardiogram region has only single-channel 2 gray value information. Then, we cropped the region of interest (ROI) with a redefined sector mask. Since a typical input to convolutional neural networks (CNNs) has the size of 128$\times$128, we resized the masked ROI to the 128$\times$128 image. The five views were stacked following a specific sequence (i.e., SLAX, PSSAX, A4C, SXLAX, and SSLAX) to form our five channels training sample.

%We first processed each frame to a single-channel grayscale image, and the  was cropped to remove the irrelevant parts. Since the collected ultrasonic image was a circular sector, and some labels could not be removed by rectangular cropping, a mask was designed to cover these factors.

%To align with the input of conventional CNNs, the masked ROI was resized to . We used the same pre-processing for the images of each view and concatenated these five images following fixed order: PSLAX of the left ventricle, PSSAX of the aorta, A4C, SXLAX of two atriums, and SSLAX of the aortic arch. Noticing that each view only had one image in our 2D echocardiogram dataset.

%The flow chart of pre-processing is shown in Figure \ref{fig:2}.
%This is necessary, since only the positive samples have electrocardiogram record in our dataset and the CNNs can easily discriminate the samples according to this clue.


 \begin{figure}[t]
\centering
\includegraphics[width=8.5cm]{fig//st7.pdf}\\ 
\caption{The sensitive analysis of $K_n$ (right) and $m$ of patient class in CHD dataset.}\label{fig:77} 
\end{figure}




\subsection{Backbone network structure for the CHD diagnosis task}

Training a network with massive parameters using a limited number of training data samples usually results in overfitting, which causes a problem in medical image analysis. To alleviate the data constraints in our CHD diagnosis task, we propose to adopt the Depthwise Separable Convolution (DSC) \cite{howard2017mobilenets} as our backbone to reduce the to-be-trained parameters. This strategy was introduced in the MobileNet~\cite{howard2017mobilenets} with a lightweight implementation. Notably, as a comparison, the DSC based CNNs with 1.32M weights performed similarly to AlexNet \cite{simonyan2014very} with 60M weights trained using the ImageNet dataset. 

More specifically, the convolution operation in conventional CNNs is separated to the depth-wise and point-wise stages. The depth-wise stage processes each channel independently, while conventional CNNs process all channels together. Then, the 1$\times1$ convolution is used in the subsequent point-wise stage. We adapt the two-stage convolution for our grayscale image analysis.

To fuse the information from the five views, we adopted the multi-channel DSC network. Following the MobileNet \cite{howard2017mobilenets}, we configured the first layer as the conventional convolution operation and set the stride as two. We also adopted two fully connected layers with the dimension of 1024 and 128, respectively. The structure of our backbone is shown in Fig. \ref{fig:4}, and detailed in Table 1.

 %Noting that since the DSC is originally designed for RGB data, which is different from our input, we inherit the idea of the DSC to construct our multi-channel CNN. 

%We give a detailed comparison of the standard and depthwise separable convolution in Figure \ref{fig:3}. 


%The diagnosis of CHD can be more challenging than a common object recognition task, because of its multi-view data structure as well as the relatively limited and unbalanced training sample. The decision is based on five views, which can incorporate more complementary information than a single view-based diagnosis. However, this also introduces the challenge of information fusion. We propose to concatenate the five views sequentially, and produce a matrix with the size of 128$\times$128$\times$5. 


%A five-channel CNN, as shown in , is developed to take the concatenated matrix as input. Each DSC block incorporates a depthwise convolution and a pointwise convolution. After a few DSC layers, the feature maps are flattened as the feature vector that is processed by two fully connected layers with the sizes of 1024 and 128, respectively. The detailed network structure is given in 



\begin{figure}[t]
\begin{center}
\includegraphics[width=1\linewidth]{fig//Fig4.pdf}
\end{center} 
\caption{The backbone network via the multichannel convolutional neural networks for five-view echocardiograms analysis.}
\label{fig:4}
\end{figure}


\begin{table}[t]
\caption{{The detailed network construction of the DSC-based five-view framework. We denote the depth-wise convolution with the size $H\times W$ as dw, which is the same for all channels. The point-wise convolution is denoted by pw, which has the size of $1\times 1\times N$. The conventional convolution operation is used in the first convolutional layer.}} % title of Table
\centering % used for centering table
\resizebox{1\columnwidth}{!}{%
\begin{tabular}{l | l | l} % centered columns (4 columns)
\hline\hline %inserts double horizontal lines
Input Size&Type / Stride & Filter Shape   \\ [0.5ex] % inserts table
%heading
\hline % inserts single horizontal line

$128\times128\times5$&Conv / s2 & 32 $\times$ of $3\times3\times5$  \\
\hline

$64\times64\times32$&Conv dw / s1& 32 $\times$ $3\times3$ dw \\


$64\times64\times32$& Conv pw / s1& 64 $\times$ of$1\times1\times32$ pw \\
\hline


$64\times64\times64$& Conv dw / s2& 64 $\times$ of$3\times3$ dw \\


$32\times32\times64$& Conv pw / s1& 128 $\times$ of$1\times1\times64$ pw \\
\hline

$32\times32\times128$& Conv dw / s2& 128 $\times$ of $3\times3$ dw  \\

$16\times16\times128$&Conv pw / s1&128 $\times$ of $1\times1\times128$ pw  \\
\hline

$16\times16\times128$&Conv dw / s2&128 $\times$ of $3\times3$ dw \\

$8\times8\times128$& Conv pw / s1&128 $\times$ of $1\times1\times128$ pw \\
\hline

$8\times8\times128$ & Flatten & N/A\\

8192& FC1 & 1024 \\

1024& FC2 & 128 \\
\hline

128& Classifier & Softmax \\
\hline
 
\end{tabular}
\label{table:mobilenet} % is used to refer this table in the text
}
\end{table}


In addition to the multi-channel scheme, another feasible choice for multi-view information aggregation is the multi-branch network \cite{lee2016multi}. However, our multi-channel framework has a few strengths over the multi-branch network. First, our multi-channel scheme is able to learn multi-view fusing in all of the layers adaptively, rather than simply concatenating each view in the late layer \cite{lee2016multi}. Second, our multi-channel framework only uses a single forward model, which has much fewer to-be-learned weights. Considering that the echocardiograms from five views can share some similarity, the convolutional filters trained in each view may potentially be useful for one another. Thus, our framework with less to-be-learned weights can efficiently deal with the problem of small dataset size and the requirement of large memory in the implementation.

%More appealingly, the multi-channel model is not applicable for heterogeneous inputs (e.g., image and EEG data), since they require different network structure for different inputs. With sufficiently large training datasets, the performance of multi-branch and multi-channel are usually similar for homogeneous input \cite{lee2016multi}.

The class-level classification in our task aims to differentiate healthy controls from patients, which can be formulated as a binary classification problem. We thereby apply the sigmoid unit as our output layer, and adopt the binary CE loss as the supervision signal. We report the accuracy with the threshold of 0.5.

\subsection{Evaluations}

For comparison, we re-implemented the current state-of-the-art methods with the same backbone and experiment setting, where we chose the batch size to 64. The results are shown in Table. \ref{tabel:chd}. Considering the imbalance of normal and patient proportion in the testing set, we also provide the area under the receiver operating characteristic curve (AUC) metric in addition to the accuracy metric. 


\begin{table}[]
\centering
\resizebox{1\linewidth}{!}{%
\begin{tabular}{l|cccccc|c}
\hline

Method & Accuracy (\%) $\uparrow$ & AUC $\uparrow$  \\ \hline

Source only& 76.4$\pm$0.12  & 0.721$\pm$0.005 \\

MCD \cite{saito2017maximum}& 88.6$\pm$0.15  & 0.856$\pm$0.003 \\

GTA \cite{sankaranarayanan2018generate}& 90.9$\pm$0.17  & 0.873$\pm$0.005 \\

CRST \cite{zou2019confidence}& 93.2$\pm$0.09  & 0.882$\pm$0.006 \\

TPN \cite{pan2019transferrable}& 93.4$\pm$0.14  & 0.885$\pm$0.004 \\\hline\hline

SubUDA ($K_n=4$)&  {96.2$\pm$0.13}  &  {0.910$\pm$0.003} \\\hline

SubUDA ($K_n=1$)&  {94.7$\pm$0.11}  &  {0.902$\pm$0.004} \\

%SubUDA-Dyn ($K_n=4$)& 94.8$\pm$0.16  & 0.901$\pm$0.002 \\
SubUDA-$\mu^{st}_k$ ($K_n=4$)& 95.4$\pm$0.10  &  0.903$\pm$0.005\\
SubUDA-$\omega_k$ ($K_n=4$)& 96.0$\pm$0.13  &  0.908$\pm$0.004\\
SubUDA-DR ($K_n=4$)&  96.2$\pm$0.11 &  0.911$\pm$0.002\\
\hline

SubUDA-$SG$ ($m=8$)& 96.0$\pm$0.12  & 0.907$\pm$0.004 \\
SubUDA-$SG$-$\tau$ ($m=8$)& 95.5$\pm$0.14  & 0.902$\pm$0.003 \\\hline

\end{tabular}%
}
 
\caption{Experimental results for CHD. $\uparrow$ larger is better.}
 
\label{tabel:chd}
\end{table}

MCD \cite{saito2017maximum} and GTA \cite{sankaranarayanan2018generate} are the typical adversarial training frameworks to align the marginal distribution $p(x)$ at feature level or image level, respectively. The self-training is used to alternatively update the pseudo label of target samples and the network parameters \cite{zou2019confidence}. We note that the compared methods \cite{saito2017maximum,sankaranarayanan2018generate,zou2019confidence,wu2020dual} use the fully-connected classifier after the encoder. The TPN \cite{pan2019transferrable} uses class centroids as a classifier. It proposes to align class centroids of the source and target sample to achieve the conditional alignment w.r.t. $p(x|y)$. Our SubUDA outperformed the state-of-the-art methods w.r.t. both the accuracy and AUC by a large margin, by introducing the subtype-aware constraint. The results indicate that the online subtype compactness can effectively help the classification in the target domain.

The domain adaptation theory suggests proxy $\mathcal{A}$-distance \cite{ben2007analysis} as a measure of cross-domain discrepancy \cite{saito2017asymmetric}. In Fig. \ref{fig:66}, we compare our SubUDA with the other state-of-the-art methods, and the smaller discrepancy has been observed by using the explicit compactness objective in our SubUDA.

For the ablation study, with $K_n=1$, the subtype-wise alignment was reduced to the class-wise compactness. Besides, we used the suffix -DR, -$\omega_k$, and -$\tau$ to denote the subUDA without dimension reduction head, subtype balance weight, and semi-hard target mining, respectively. Furthermore, the suffix -$\mu_k^{st}$ denotes using $\mu_k^{st}=\frac{\sum_{i=1}^{M_k^s+M_k^t}f(x_i^{st})}{M_k^s+M_k^t}$ as the subtype centroid, which is not robust to the subtype label shift. SubUDA-DR took 4$\times$ clustering time, but the improvement was marginal. Therefore, we recommend using the dimension reduction head. 


%The dynamic stack scheme plays the most important role in addressing the difficulty of subtype undersampling and subtype label shifts.
 
 
SubUDA-$SG$ in Table \ref{tabel:chd} used the reliability-path based online sub-graph to replace $k$-means. With appropriate $m=8$, the adaptively learned clustering achieved comparable performance to the K-means with $K_n=4$. In Fig. \ref{fig:66} right, we can see that the semi-hard mining scheme in SubUDA-$SG$ is not sensitive to $\tau$ for a large range, since the network can flexibly learn to adjust the ratio of $\epsilon$ and $\tau$ in mapping space \cite{liu2017adaptive}. We note that too strict semi-hard mining (i.e., too small $\tau$) can degenerate our SubUDA to conventional class centroids matching, since no target samples are selected to form a subtype cluster.



\begin{figure}[t]
\centering
\includegraphics[width=8.5cm]{fig//st6.pdf}\\ 
\caption{Comparison with the state-of-the-art methods w.r.t. $\mathcal{A}$-distance (left), and the sensitive analysis of $\tau$ (right).}\label{fig:66} 
\end{figure}





 







 
In Fig. \ref{fig:77}, we provide a sensitive analysis of hyper-parameter $K_n$ and $m$ for two kinds of online clustering schemes. Consensus clustering can assess the clustering stability \cite{monti2003consensus}, and the optimal clustering is usually achieved in the elbow position of the area under CDF changes, where the CDF is for the consensus matrices\footnote{\url{https://github.com/ZigaSajovic/Consensus_Clustering}}. We can see that the peak of accuracy usually coincides with the best clustering consensus metric, which indicates the good subtype clustering can boost the SubUDA performance. The choice of $K_n=4$ also matches our prior knowledge of the CHD patient subtypes. 
 

Since we have four clear subtypes in this task, using the concise $k$-means can be a straightforward solution. However, Fig. \ref{fig:77} right shows that the accuracy curve can be robust for a relatively large range of $m$, which is promising for the hyperparameter tuning of the case without the prior information of subtype numbers.




 





%\subsection{Synthetic-to-Real Image UDA}

%VisDA-2017 \cite{visda2017} is a large-scale computer vision dataset with two domains. The synthetic source domain renderings of 3D models from different angles and different lighting conditions. The real target domain contains real-world images. It has 280K images in 12 classes, in which $152,409$ synthetic 2D images are used as the source domain and $55,400$ real images are used as the target domain. This scale brings challenges to domain adaptation. For fair comparisons with the other methods, we use the standard ResNet101 \cite{he2016deep} as the backbone, and set the batch size to 64 and $\mathcal{I}=5$. 


%In this task, we do not know the subtype number, and the reliability-path based sub-graph is utilized as our default solution. Actually, there is no clear definition of subtypes in VisDA, e.g., the car class can be grouped according to the maker, type, color based on different taxonomy. Validating the best $K_n$ for all of the classes can be tedious and the optimal $K_n$ can be different when we change the backbone. We provide detailed information in the \textbf{supplements}. 

%In Fig. \ref{fig:88}, we investigate the effect of $K_n$ and $m$ on car class. Given the ambiguous subtype grouping taxonomy, the accuracy curve w.r.t. $K_n$ can be more smooth than the CHD task. Based on the accuracy and elbow position of consensus clustering analysis, we select $m=5$ for our online sub-graph clustering. The performance can be stable for a wide range of $m$, and the selected $m$ is shared for all subtypes and classes. Based on our experiments, the car class is more inclined to be clustered w.r.t. vehicle type.


%Based on our experiments, there is no significant difference w.r.t. accuracy between using a dataset-wise $m$ or using the validated class-wise $m$ for each class.


%We present the results on VisDA17 in Tab. \ref{table:visda17} in terms of per-class accuracy and mean accuracy. For each proposed approach, we independently run 5 times and the average and standard deviation of the accuracy metrics are reported. 

%All SubUDAs, including $k$-means and sub-graph version, outperform the TPN \cite{pan2019transferrable} by around 2\% w.r.t. mean accuracy, demonstrating the effectiveness of subtype-wise compactness. SubUDA achieves even better performance than methods with a stronger ResNet-152 backbone \cite{pinheiro2018unsupervised,sankaranarayanan2018generate}. We also adopt ResNet-152 as our backbone to demonstrate the generality of SubUDA. The improvement is consistent with the ResNet101 backbone. 


%Similar to the CHD task, the dynamic stack scheme can boost adaptation performances significantly. Without a clear subtype definition, the semi-hard target mining may also be helpful to reject the confusing samples.  Considering there can be many underlying subtypes, balancing the cluster size with $\omega_k$ is also necessary. Besides, the more subtypes result in fewer samples in each subtype cluster, and the subtype label shift can be more severe. Therefore, calculating $\mu_k^{st}$ in a subtype label shift robust manner can be important. We do not test SubUDA-DR, since its clustering speed is not scalable for large scale datasets. 
 


 


 