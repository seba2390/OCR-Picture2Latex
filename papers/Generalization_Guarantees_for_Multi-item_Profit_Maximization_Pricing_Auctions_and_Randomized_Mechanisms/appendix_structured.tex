\begin{example}[Item-pricing mechanisms \citep{Morgenstern16:Learning}]\label{ex:2PT2}
Let $\mclass$ be the class of anonymous item-pricing mechanisms over a single additive buyer and let $\vec{p} = (p_1,\dots, p_m)$ be a vector of prices. In this case, we can define $f_{\vec{p}}^{(1)}: \domain \to \{0,1\}^m$ where the $i^{th}$ component of $f_{\vec{p}}^{(1)}(\vec{v})$ is 1 if and only if the buyer buys item $i$. Define $\psi(\vec{v}, \vec{\alpha}) = (v(\vec{\alpha}), -\vec{\alpha})$ and define $\vec{w}^{\vec{p}} = (1, \vec{p})$. Then the $\vec{\alpha}$ that maximizes $ \left\langle\vec{w}^{\vec{p}}, \psi(\vec{v}, \vec{\alpha})\right\rangle$ is the $\vec{\alpha}$ that maximizes the buyer's utility, i.e., $f^{(1)}_{\vec{p}}(\vec{v})$, as desired.
Finally, we define $f_{\vec{p}}^{(2)}(\vec{v}, \vec{\alpha}) =\langle \vec{\alpha}, \vec{p}\rangle,$  and we have that $\profit_{\vec{p}}(\vec{v}) = f_{\vec{p}}^{(2)}\left(\vec{v}, f_{\vec{p}}^{(1)}(\vec{v})\right)$, as desired.
\end{example}

\separable*

\begin{proof}
To prove this theorem, we will use the following standard notation. For a class $\fclass$ of real-valued functions mapping $\domain$ to $\R$, let $\sample = \left\{\vec{v}^{(1)}, \dots, \vec{v}^{(N)}\right\}$ be a subset of $\domain$. We define
\[\Pi_{\fclass}(\sample) = \max_{z^{(1)}, \dots, z^{(N)} \in \R} \left|\left\{\begin{pmatrix}
\textbf{1}_{\left\{f\left(\vec{v}^{(1)}\right) \geq z^{(1)}\right\}}\\
\vdots\\
\textbf{1}_{\left\{f\left(\vec{v}^{(N)}\right) \geq z^{(N)}\right\}}
\end{pmatrix} : f \in \fclass\right\}\right|.\]
The pseudo-dimension of $\fclass$ is the size of the largest set $\sample$ such that $\Pi_{\fclass}(\sample) = 2^{|\sample|}$. We also use the notation $f(\sample)$ to denote the vector $\left(f(\vec{v}^{(1)}), \dots, f(\vec{v}^{(N)})\right)$. \citet{Morgenstern16:Learning} proved the following lemma.

\begin{lemma}[\citet{Morgenstern16:Learning}]\label{lem:MRshatter}
Suppose $\mclass$ is $\left(\fclass^{(1)}, \fclass^{(2)}\right)$-decomposable and $a$-dimensionally linearly separable. Let $\sample = \left\{\vec{v}^{(1)}, \dots, \vec{v}^{(N)}\right\}$ be a subset of $\domain$. Then \begin{align*}\Pi_{\mclass}(\sample) \leq &\left|\left\{\left(\sample', f^{(1)}_{\vec{p}}(\sample')\right) : \sample' \subseteq \sample, |\sample'| = a, \vec{p} \in \pspace \right\}\right|\\
\cdot &\max_{\vec{\alpha}^{(1)}, \dots, \vec{\alpha}^{(N)} \in \range} \left\{ \Pi_{\fclass^{(2)}} \left(\left\{ \left(\vec{v}^{(1)}, \vec{\alpha}^{(1)}\right), \dots, \left(\vec{v}^{(N)}, \vec{\alpha}^{(N)}\right)\right\}\right)\right\}.\end{align*}
\end{lemma}

Suppose the pseudo-dimension of $\mclass$ is $N$. By definition, there exists a set $\sample = \left\{\vec{v}^{(1)}, \dots, \vec{v}^{(N)}\right\}$ that is shattered by $\mclass$. By Lemmas~\ref{lem:MRshatter} and \ref{lem:f1labels}, this means that \[2^N = \Pi_{\mclass}(\sample) \leq N^a\omega \max_{\vec{\alpha}^{(1)}, \dots, \vec{\alpha}^{(N)} \in \range} \left\{ \Pi_{\fclass^{(2)}} \left(\left\{ \left(\vec{v}^{(1)}, \vec{\alpha}^{(1)}\right), \dots, \left(\vec{v}^{(N)}, \vec{\alpha}^{(N)}\right)\right\}\right)\right\}.\] To prove this theorem, we will show that \begin{equation}\max_{\vec{\alpha}^{(1)}, \dots, \vec{\alpha}^{(N)} \in \range} \left\{ \Pi_{\fclass^{(2)}} \left(\left\{ \left(\vec{v}^{(1)}, \vec{\alpha}^{(1)}\right), \dots, \left(\vec{v}^{(N)}, \vec{\alpha}^{(N)}\right)\right\}\right)\right\} < d^2 \left(N^2t_2\right)^d,\label{eq:shattering}\end{equation} which means that $2^N < N^{2d+a}d^2t_2^d\omega$, and thus $N= O\left((d+a) \log(d+a) + d \log t_2 + \log \omega\right)$.

To this end, let $\vec{\alpha}^{(1)}, \dots, \vec{\alpha}^{(N)}$ be $N$ arbitrary elements of $\range$ and let $z^{(1)}, \dots, z^{(N)}$ be $N$ arbitrary elements of $\R$. Since $\mclass$ is $(d, t_1, t_2)$-divisible, we know that for each $i \in [N]$, there is a set $\hyp_2^{(i)}$ of $t_2$ hyperplanes such that for any connected component $\pspace'$ of $\pspace \setminus \hyp_2^{(i)}$, $f^{(2)}_{\vec{v}^{(i)}, \vec{\alpha}^{(i)}}\left(\vec{p}\right)$ is linear over all $\vec{p} \in \pspace'.$  We now consider the overlay of all $N$ partitions $\pspace\setminus \hyp_2^{(1)}, \dots, \pspace\setminus \hyp_2^{(N)}$. Formally, this overlay is made up of the sets $\pspace_1, \dots, \pspace_{\tau}$, which are the connected components 
of $\pspace \setminus \left(\bigcup_{i = 1}^N \hyp_2^{(i)}\right)$. For each set $\pspace_j$ and each $i \in [N]$, $\pspace_j$ is completely contained in a single connected component of $\pspace \setminus \hyp_2^{(i)}$, which means that $f^{(2)}_{\vec{v}^{(i)}, \vec{\alpha}^{(i)}}\left(\vec{p}\right)$ is linear over $\pspace_j.$
Since $\left|\hyp^{(i)}_2\right|\leq t_2$ for all $i \in [N]$, $\tau < d(Nt_2)^d$ \citep{Buck43:Partition}.

Now, consider a single connected component $\pspace_j$ of $\pspace \setminus \left(\bigcup_{i = 1}^N \hyp^{(i)}_2\right)$. For any sample $\vec{v}^{(i)} \in \sample$, 
we know that $f^{(2)}_{\vec{v}^{(i)}, \vec{\alpha}^{(i)}}\left(\vec{p}\right)$ is linear over $\pspace_j$. 
Let $\vec{a}_j^{(i)} \in \R^d$ and $b_j^{(i)} \in \R$ be the weight vector and offset 
such that $f^{(2)}_{\vec{v}^{(i)}, \vec{\alpha}^{(i)}}\left(\vec{p}\right) = \vec{a}_j^{(i)} \cdot \vec{p} + b_j^{(i)}$ for all $\vec{p} \in \pspace_j$. We know that there is a hyperplane $\vec{a}_j^{(i)} \cdot \vec{p} + b_j^{(i)} = z^{(i)}$ where on one side of the 
hyperplane, $f^{(2)}_{\vec{v}^{(i)}, \vec{\alpha}^{(i)}}\left(\vec{p}\right) \leq z^{(i)}$ and on the other 
side, $f^{(2)}_{\vec{v}^{(i)}, \vec{\alpha}^{(i)}}\left(\vec{p}\right) > z^{(i)}$.
Let $\hyp_{\pspace_j}$ be all $N$ hyperplanes for all $N$ samples, i.e., $\hyp_{\pspace_j} = \left\{\vec{a}_j^{(i)} \cdot \vec{p} + b_j^{(i)} = z^{(i)} : i \in [N]\right\}.$ Notice that in any 
connected component $\pspace'$ of $\pspace_j \setminus \hyp_{\pspace_j}$, for all $i \in [N]$, $f^{(2)}_{\vec{v}^{(i)}, \vec{\alpha}^{(i)}}\left(\vec{p}\right)$ is either greater than $z^{(i)}$ or 
less than $z^{(i)}$ (but not both) for all $\vec{p} \in \pspace'$. 

In total, the number of 
connected components of $\pspace_j \setminus \hyp_{\pspace_j}$ is smaller than $dN^d$. 
The same holds for every partition $\pspace_j$. Thus, the total number of regions where 
for all $i \in [N]$, $f^{(2)}_{\vec{v}^{(i)}, \vec{\alpha}^{(i)}}\left(\vec{p}\right)$ is either greater 
than $z^{(i)}$ or less than $z^{(i)}$ (but not both) is smaller than $dN^d\cdot d(Nt_2)^d$. In other words, \[\left|\left\{\begin{pmatrix}
\textbf{1}\left(f^{(2)}_{\vec{p}}\left(\vec{v}^{(1)}, \vec{\alpha}^{(1)}\right) \geq z^{(1)}\right)\\
\vdots\\
\textbf{1}\left(f^{(2)}_{\vec{p}}\left(\vec{v}^{(N)}, \vec{\alpha}^{(N)}\right) \geq z^{(N)}\right)
\end{pmatrix} : \vec{p} \in \pspace\right\}\right| \leq dN^d\cdot d(Nt_2)^d.\] Since we chose $\vec{\alpha}^{(1)}, \dots, \vec{\alpha}^{(N)}$ and $z^{(1)}, \dots, z^{(N)}$ arbitrarily, we may conclude that Inequality~\eqref{eq:shattering} holds.
\end{proof}

\begin{lemma}\label{lem:f1labels}
Suppose $\mclass$ is $a$-dimensionally linearly separable over $\range$ and $(d,t_1, t_2)$-divisible. Then for any set $\sample \subseteq \domain$ of size $N$, \[\left|\left\{ (\sample', f^{(1)}_{\vec{p}}(\sample')) : \sample' \subseteq \sample, |\sample'| = a, \vec{p} \in \pspace\right\}\right| \leq N^a\min\left\{|\range|^a, d(at_1)^d\right\}.\]
\end{lemma}

\begin{proof}
To begin with, there are of course at most $N^a$ ways to choose a set $\sample' \subseteq \sample$ of size $a$. How many ways are there to label a fixed set $\sample' = \left\{\vec{v}^{(i_1)}, \dots, \vec{v}^{(i_a)}\right\}$ of size $a$ using functions from $\fclass^{(1)}$? An easy upper bound is $|\range|^a$. Alternatively, we can use the structure of $\mclass$ to prove that there are $d(at_1)^d$ ways to label $\sample'$. Since $\mclass$ is $(d,t_1, t_2)$-divisible, we know that for any $\vec{v}^{(i_j)} \in \sample'$, there is a set $\hyp_1^{(i_j)}$ of $t_1$ hyperplanes such that for any connected component $\pspace'$ of $\pspace \setminus \hyp_1^{(i_j)}$, $f^{(1)}_{\vec{v}^{(i_j)}}(\vec{p})$ is constant over all $\vec{p} \in \pspace'.$
We now consider the overlay of all $a$ partitions $\pspace\setminus \hyp_1^{(i_j)}$ for all $\vec{v}^{(i_j)} \in \sample'$. Formally, this overlay is made up of the sets $\pspace_1, \dots, \pspace_{\tau}$, which are the connected components 
of $\pspace \setminus \left(\bigcup_{\vec{v}^{(i_j)} \in \sample'} \hyp_1^{(i_j)}\right)$. For each set $\pspace_t$ and each $\vec{v}^{(i_j)} \in \sample'$, $\pspace_t$ is completely contained in a single connected component of $\pspace \setminus \hyp_1^{(i_j)}$, which means that $f^{(1)}_{\vec{v}^{(i_j)}}\left(\vec{p}\right)$ is constant over $\pspace_t.$ 
%
This means that the number of ways to label $\sample'$ is at most $\tau$. Since $\left|\hyp_1^{(i_j)}\right|\leq t_1$ for all $\vec{v}^{(i_j)} \in \sample'$, $\tau < d(at_1)^d$ \citep{Buck43:Partition}.
Therefore, $\left|\left\{ \left(\sample', f^{(1)}_{\vec{p}}(\sample')\right) : \sample' \subseteq \sample, |\sample'| = a, \vec{p} \in \pspace\right\}\right| \leq N^a\min\left\{|\range|^a, d(at_t)^d\right\}$, so the lemma statement holds.
\end{proof}

\begin{theorem}\label{thm:second_price_sep}
Let $\pazocal{M}$ and $\pazocal{M}'$ be the classes of anonymous and non-anonymous second price item auctions. Then $\mclass$ is $\left(m, m, m\right)$-divisible and $\mclass'$ is $(nm, m, m)$-divisible. Also, $\mclass$ and $\mclass'$ are $(m+1)$- and $(nm+1)$-dimensionally linearly separable over $\{0,1\}^m$ and $[n]^m$. Therefore, $Pdim(\mclass) = O(m \log m)$ and $Pdim(\mclass') = O(nm \log (nm))$.
\end{theorem}

\begin{proof}
We begin with anonymous reserves. For a given valuation vector $\vec{v}$, let $j_i$ be the highest bidder for item $i$ and let $j_i'$ be the second highest bidder. Let $f_{\vec{p}}^{(1)}: \domain \to \{0,1\}^m$ be defined so that the $i^{th}$ component is 1 if and only if item $i$ is sold. There are $t_1 = m$ hyperplanes splitting $\R^m$ into regions where $f_{\vec{v}}^{(1)}(\vec{p})$ is constant: the $i^{th}$ component of $f_{\vec{v}}^{(1)}(\vec{p})$ is 1 if and only if $v_{j_i}(\vec{e}_i) \geq p(\vec{e}_i)$. Next, we can write $f_{\vec{p}}^{(2)}(\vec{v}, \vec{\alpha}) = \sum_{i: \alpha[i] = 1 } \max\left\{v_{j_i'}(\vec{e}_i), p(\vec{e}_i)\right\} - c(\vec{\alpha})$, which is linear so long as either $v_{j_i'}(\vec{e}_i) <p(\vec{e}_i)$ or $v_{j_i'}(\vec{e}_i) \geq  p(\vec{e}_i)$ for all $i \in [m]$. Therefore, there are $t_2 = m$ hyperplanes $\hyp_2$ such that for any connected component $\pspace'$ of $\pspace \setminus \hyp_2$, $f_{\vec{v}, \vec{\alpha}}^{(2)}(\vec{p})$ is linear over all $\vec{p} \in \pspace'.$


Under non-anonymous reserve prices, let $f_{\vec{p}}^{(1)}: \domain \to \{0,1\}^{nm}$ be defined so that for every buyer $j$ and every item $i$, there is a component of $f_{\vec{p}}^{(1)}(\vec{v})$ that is 1 if and only if buyer $j$ receives item $i$. There are $t_1 = m$ hyperplanes splitting $\R^{nm}$ into regions where $f_{\vec{v}}^{(1)}(\vec{p})$ is constant: for every item $i$, the component corresponding to buyer $j_i$ is 1 if and only if $v_{j_i}(\vec{e}_i) \geq p_j(\vec{e}_i)$. Next, we can write $f_{\vec{p}}^{(2)}(\vec{v}, \vec{\alpha}) = \sum_{i: \alpha[i] = 1 } \max\left\{v_{j_i'}(\vec{e}_i), p_{j_i}(\vec{e}_i)\right\} - c(\vec{\alpha})$, which is linear so long as either $v_{j_i'}(\vec{e}_i) <p_{j_i}(\vec{e}_i)$ or $v_{j_i'}(\vec{e}_i) \geq  p_{j_i}(\vec{e}_i)$ for all $i \in [m]$. Therefore, there are $t_2 = m$ hyperplanes $\hyp_2$ such that for any connected component $\pspace'$ of $\pspace \setminus \hyp_2$, $f_{\vec{v}, \vec{\alpha}}^{(2)}(\vec{p})$ is linear over all $\vec{p} \in \pspace'.$

\citet{Morgenstern16:Learning} proved that $\mclass$ and $\mclass'$ are $(m+1)$- and $(nm+1)$-dimensionally linearly separable over $\{0,1\}^m$ and $[n]^m$, respectively.
\end{proof}

\begin{theorem}\label{thm:item_pricing_add_sep}
Let $\pazocal{M}$ and $\pazocal{M}'$ be the classes of item-pricing mechanisms with anonymous prices and non-anonymous prices, respectively. If the buyers are additive, then $\mclass$ is $(m, m, 1)$-divisible and $\mclass'$ is $(nm,nm,1)$-divisible. Also, $\mclass$ and $\mclass'$ are $(m+1)$- and $(nm+1)$-dimensionally linearly separable over $\{0,1\}^m$ and $[n]^m$. Therefore, $Pdim(\mclass) = O(m \log m)$ and $Pdim(\mclass') = O(nm \log (nm))$.
\end{theorem}

\begin{proof}
We begin with anonymous reserves. For a given valuation vector $\vec{v}$, let $j_i$ be the buyer with the highest valuation for item $i$. Let $f_{\vec{p}}^{(1)}: \domain \to \{0,1\}^m$ be defined so that the $i^{th}$ component is 1 if and only if item $i$ is sold. There are $t_1 = m$ hyperplanes splitting $\R^m$ into regions where $f_{\vec{v}}^{(1)}(\vec{p})$ is constant: the $i^{th}$ component of $f_{\vec{v}}^{(1)}(\vec{p})$ is 1 if and only if $v_{j_i}(\vec{e}_i) \geq p(\vec{e}_i)$. Next, we can write $f_{\vec{p}}^{(2)}(\vec{v}, \vec{\alpha}) = \vec{\alpha} \cdot \vec{p}$, which is always linear, so we may set $t_2 = 1$.

Under non-anonymous reserve prices, let $f_{\vec{p}}^{(1)}: \domain \to \{0,1\}^{nm}$ be defined so that for every buyer $j$ and every item $i$, there is a component of $f_{\vec{p}}^{(1)}(\vec{v})$ that is 1 if and only if buyer $j$ receives item $i$. There are $t_1 = nm$ hyperplanes splitting $\R^{nm}$ into regions where $f_{\vec{v}}^{(1)}(\vec{p})$ is constant: $v_{j}(\vec{e}_i) = p_{j}(\vec{e}_i)$ for all $i$ and all $j$. Next, we can write $f_{\vec{p}}^{(2)}(\vec{v}, \vec{\alpha}) = \vec{\alpha} \cdot \vec{p}$, which is always linear, so we may set $t_2 = 1$.

\citet{Morgenstern16:Learning} proved that $\mclass$ and $\mclass'$ are $(m+1)$- and $(nm+1)$-dimensionally linearly separable over $\{0,1\}^m$ and $[n]^m$, respectively.
\end{proof}

\itemUnit*

\begin{proof}
We begin with anonymous reserves. Let $f_{\vec{p}}^{(1)}: \domain \to \{0,1\}^m$ be defined so that the $i^{th}$ component is 1 if and only if item $i$ is sold. For each buyer $j$, there are ${m \choose 2}$ hyperplanes defining their preference ordering on the items: $v_j(\vec{e}_i) - p(\vec{e}_i) = v_j(\vec{e}_k) - p(\vec{e}_k)$ for all $i \not = k$. This gives a total of at most $t_1 = nm^2$ hyperplanes splitting $\R^m$ into regions where $f_{\vec{v}}^{(1)}(\vec{p})$ is constant. Next, we can write $f_{\vec{p}}^{(2)}(\vec{v}, \vec{\alpha}) = \vec{\alpha} \cdot \vec{p}$, which is always linear, so we may set $t_2 = 1$.

Under non-anonymous reserve prices, let $f_{\vec{p}}^{(1)}: \domain \to \{0,1\}^{nm}$ be defined so that for every buyer $j$ and every item $i$, there is a component of $f_{\vec{p}}^{(1)}(\vec{v})$ that is 1 if and only if buyer $j$ receives item $i$. As with anonymous prices, there are $t_1 = nm^2$ hyperplanes splitting $\R^{nm}$ into regions where $f_{\vec{v}}^{(1)}(\vec{p})$ is constant. Next, we can write $f_{\vec{p}}^{(2)}(\vec{v}, \vec{\alpha}) = \vec{\alpha} \cdot \vec{p}$, which is always linear, so we may set $t_2 = 1$.

\citet{Morgenstern16:Learning} proved that $\mclass$ and $\mclass'$ are $(m+1)$- and $(nm+1)$-dimensionally linearly separable over $\{0,1\}^m$ and $[n]^m$, respectively.
\end{proof}

\itemGeneral*

\begin{proof}
We begin with anonymous reserves. Let $f_{\vec{p}}^{(1)}: \domain \to \{0,1\}^m$ be defined so that the $i^{th}$ component is 1 if and only if item $i$ is sold. For each buyer $j$, there are ${2^m \choose 2}$ hyperplanes defining their preference ordering on the bundles: $v_j(\vec{q}) - \sum_{i: q[i] = 1} p(\vec{e}_i) = v_j(\vec{q}') - \sum_{i: q'[i] = 1} p(\vec{e}_i)$ for all $\vec{q}, \vec{q}' \in \{0,1\}^m$. This gives a total of at most $t_1 = n2^{2m}$ hyperplanes splitting $\R^m$ into regions where $f_{\vec{v}}^{(1)}(\vec{p})$ is constant. Next, we can write $f_{\vec{p}}^{(2)}(\vec{v}, \vec{\alpha}) = \vec{\alpha} \cdot \vec{p}$, which is always linear, so we may set $t_2 = 1$.

Under non-anonymous reserve prices, let $f_{\vec{p}}^{(1)}: \domain \to \{0,1\}^{nm}$ be defined so that for every buyer $j$ and every item $i$, there is a component of $f_{\vec{p}}^{(1)}(\vec{v})$ that is 1 if and only if buyer $j$ receives item $i$. As with anonymous prices, there are $t_1 = n2^{2m}$ hyperplanes splitting $\R^{nm}$ into regions where $f_{\vec{v}}^{(1)}(\vec{p})$ is constant. Next, we can write $f_{\vec{p}}^{(2)}(\vec{v}, \vec{\alpha}) = \vec{\alpha} \cdot \vec{p}$, which is always linear, so we may set $t_2 = 1$.

\citet{Morgenstern16:Learning} proved that $\mclass$ and $\mclass'$ are $(m+1)$- and $(nm+1)$-dimensionally linearly separable over $\{0,1\}^m$ and $[n]^m$, respectively.
\end{proof}
