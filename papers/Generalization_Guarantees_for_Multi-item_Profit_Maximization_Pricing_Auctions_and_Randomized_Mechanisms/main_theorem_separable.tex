In this section, we connect the hyperplane structure we investigate in this paper to the structured prediction literature in machine learning (e.g., \citep{Collins00:Discriminative}), thus proving even stronger generalization bounds for item-pricing mechanisms under buyers with unit-demand and general valuations and answering an open question by \citet{Morgenstern16:Learning}.
\citet{Balcan14:Learning} were the first to explore the connection between structured prediction and mechanism design, though in a different setting from us: they provided algorithms that make use of past data describing the purchases of a utility-maximizing agent to produce a hypothesis function that can accurately forecast the future behavior of the agent.

 \citet{Morgenstern16:Learning} used structured prediction to provide sample complexity guarantees for several ``simple'' mechanism classes. They observed that these classes have profit functions which are the composition of two simpler functions: A generalized allocation function $f^{\left(1\right)}_{\vec{p}} : \domain \to \range$ and a simplified profit function $f^{\left(2\right)}_{\vec{p}} : \domain \times \range \to \R$ such that $\profit_{\vec{p}}\left(\vec{v}\right) = f^{\left(2\right)}_{\vec{p}}\left(\vec{v}, f^{\left(1\right)}_{\vec{p}}\left(\vec{v}\right)\right)$. For example, $\range$ might be the set of allocations. In this case, we say that $\mclass$ is $\left(\fclass^{\left(1\right)}, \fclass^{\left(2\right)}\right)$-decomposable, where $\fclass^{\left(1\right)} = \left\{f^{\left(1\right)}_{\vec{p}} : \vec{p} \in \pspace\right\}$ and $\fclass^{\left(2\right)} = \left\{f^{\left(2\right)}_{\vec{p}} : \vec{p} \in \pspace\right\}$.
See Example~\ref{ex:2PT2} for an example of this decomposition.
\begin{example}[Item-pricing mechanisms \citep{Morgenstern16:Learning}]\label{ex:2PT2}
Let $\mclass$ be the class of anonymous item-pricing mechanisms over a single additive buyer and let $\vec{p} = (p_1,\dots, p_m)$ be a vector of prices. In this case, we can define $f_{\vec{p}}^{(1)}: \domain \to \{0,1\}^m$ where the $i^{th}$ component of $f_{\vec{p}}^{(1)}(\vec{v})$ is 1 if and only if the buyer buys item $i$. Define $\psi(\vec{v}, \vec{\alpha}) = (v(\vec{\alpha}), -\vec{\alpha})$ and define $\vec{w}^{\vec{p}} = (1, \vec{p})$. Then the $\vec{\alpha}$ that maximizes $ \left\langle\vec{w}^{\vec{p}}, \psi(\vec{v}, \vec{\alpha})\right\rangle$ is the $\vec{\alpha}$ that maximizes the buyer's utility, i.e., $f^{(1)}_{\vec{p}}(\vec{v})$, as desired.
Finally, we define $f_{\vec{p}}^{(2)}(\vec{v}, \vec{\alpha}) =\langle \vec{\alpha}, \vec{p}\rangle,$  and we have that $\profit_{\vec{p}}(\vec{v}) = f_{\vec{p}}^{(2)}\left(\vec{v}, f_{\vec{p}}^{(1)}(\vec{v})\right)$, as desired.
\end{example}

\citet{Morgenstern16:Learning} bound $\pdim\left(\mclass\right)$ using the ``complexity'' of $\fclass^{\left(1\right)}$, which they quantified using tools from structured prediction, namely, \emph{generalized linear functions.}

\begin{definition}[$a$-dimensional linear class]
A set $\fclass = \left\{f_{\vec{p}}: \domain  \to \range \ | \ \vec{p} \in \pspace\right\}$ is an \emph{$a$-dimensional linear class} if
 there is a function $\psi: \domain \times \range \to \R^a$ and a vector $\vec{w}^{\vec{p}} \in \R^a$ for each $\vec{p} \in \pspace$ such that $f_{\vec{p}}\left(\vec{v}\right) \in \argmax_{\vec{\alpha} \in \range} \langle \vec{w}^{\vec{p}}, \psi\left(\vec{v},\vec{\alpha}\right)\rangle$ and $|\argmax_{\vec{\alpha} \in \range} \langle \vec{w}^{\vec{p}}, \psi\left(\vec{v},\vec{\alpha}\right)\rangle| = 1$.
\end{definition}

If $\mclass$ is $\left(\fclass^{(1)}, \fclass^{(2)}\right)$-decomposable and $\fclass^{(1)}$ is an $a$-dimensional linear class over $\range$, we say that $\mclass$ is an $a$-dimensional linear class over $\range$.


The bounds \citet{Morgenstern16:Learning} provided using linear separability are loose in several settings: for anonymous and non-anonymous item-pricing mechanisms under additive buyers, their structured prediction approach gives a pseudo-dimension bound of $O\left(m^2\right)$ and $O\left(nm^2\log m\right)$, respectively. They left as an open question whether linear separability can be used to prove tighter guarantees. Using the hyperplane structures we study in this paper, we prove that the answer is ``yes.'' We require the following refined notion of $\left(d,t\right)$-delineable classes.

\begin{definition}[$\left(d,t_1,t_2\right)$-divisible]
Suppose $\mclass$ consists of mechanisms parameterized by vectors $\vec{p} \subseteq \R^d$ and that $\mclass$ is $\left(\fclass^{\left(1\right)}, \fclass^{\left(2\right)}\right)$-decomposable. We say that $\mclass$ is \emph{$\left(d,t_1,t_2\right)$-divisible} if:
\begin{enumerate}
\item For any $\vec{v} \in \domain$, there is a set $\hyp$ of $t_1$ hyperplanes such that for any connected component $\pspace'$ of $\R^d \setminus \hyp$, the function $f_{\vec{v}}^{\left(1\right)}\left(\vec{p}\right)$ is constant over all $\vec{p} \in \pspace'.$
\item For any $\vec{v} \in \domain$ and any $\vec{\alpha} \in \range$, there is a set $\hyp_2$ of $t_2$ hyperplanes such that for any connected component $\pspace'$ of $\R^d \setminus \hyp_2$, the function $f_{\vec{v}, \vec{\alpha}}^{\left(2\right)}\left(\vec{p}\right)$ is linear over all $\vec{p} \in \pspace'.$
\end{enumerate}
\end{definition}

Note that $\left(d,t_1,t_2\right)$-divisibility implies $\left(d,t_1+t_2\right)$-delineability. Theorem~\ref{thm:version2} connects linear separability and divisibility with pseudo-dimension.

\begin{restatable}{theorem}{separable}\label{thm:version2}
Suppose $\mclass$ is mechanism class that is $\left(d,t_1, t_2\right)$-divisible with $t_1, t_2 \geq 1$ and an $a$-dimensional linear class over $\range$. Let $\omega = \min\left\{|\range|^a, d\left(at_1\right)^d\right\}$. Then \[\pdim\left(\mclass\right) = O\left(\left(d+a\right)\log\left(d+a\right) + d\log t_2 + \log \omega\right).\]
\end{restatable}

\begin{proof}
To prove this theorem, we will use the following standard notation. For a class $\fclass$ of real-valued functions mapping $\domain$ to $\R$, let $\sample = \left\{\vec{v}^{(1)}, \dots, \vec{v}^{(N)}\right\}$ be a subset of $\domain$. We define
\[\Pi_{\fclass}(\sample) = \max_{z^{(1)}, \dots, z^{(N)} \in \R} \left|\left\{\begin{pmatrix}
\textbf{1}_{\left\{f\left(\vec{v}^{(1)}\right) \geq z^{(1)}\right\}}\\
\vdots\\
\textbf{1}_{\left\{f\left(\vec{v}^{(N)}\right) \geq z^{(N)}\right\}}
\end{pmatrix} : f \in \fclass\right\}\right|.\]
The pseudo-dimension of $\fclass$ is the size of the largest set $\sample$ such that $\Pi_{\fclass}(\sample) = 2^{|\sample|}$. We also use the notation $f(\sample)$ to denote the vector $\left(f(\vec{v}^{(1)}), \dots, f(\vec{v}^{(N)})\right)$. \citet{Morgenstern16:Learning} proved the following lemma.

\begin{lemma}[\citet{Morgenstern16:Learning}]\label{lem:MRshatter}
Suppose $\mclass$ is $\left(\fclass^{(1)}, \fclass^{(2)}\right)$-decomposable and an $a$-dimensional linear class. Let $\sample = \left\{\vec{v}^{(1)}, \dots, \vec{v}^{(N)}\right\}$ be a subset of $\domain$. Then \begin{align*}\Pi_{\mclass}(\sample) \leq &\left|\left\{\left(\sample', f^{(1)}_{\vec{p}}(\sample')\right) : \sample' \subseteq \sample, |\sample'| = a, \vec{p} \in \pspace \right\}\right|\\
\cdot &\max_{\vec{\alpha}^{(1)}, \dots, \vec{\alpha}^{(N)} \in \range} \left\{ \Pi_{\fclass^{(2)}} \left(\left\{ \left(\vec{v}^{(1)}, \vec{\alpha}^{(1)}\right), \dots, \left(\vec{v}^{(N)}, \vec{\alpha}^{(N)}\right)\right\}\right)\right\}.\end{align*}
\end{lemma}

Suppose the pseudo-dimension of $\mclass$ is $N$. By definition, there exists a set $\sample = \left\{\vec{v}^{(1)}, \dots, \vec{v}^{(N)}\right\}$ that is shattered by $\mclass$. By Lemmas~\ref{lem:MRshatter} and \ref{lem:f1labels}, this means that \[2^N = \Pi_{\mclass}(\sample) \leq N^a\omega \max_{\vec{\alpha}^{(1)}, \dots, \vec{\alpha}^{(N)} \in \range} \left\{ \Pi_{\fclass^{(2)}} \left(\left\{ \left(\vec{v}^{(1)}, \vec{\alpha}^{(1)}\right), \dots, \left(\vec{v}^{(N)}, \vec{\alpha}^{(N)}\right)\right\}\right)\right\}.\] To prove this theorem, we will show that \begin{equation}\max_{\vec{\alpha}^{(1)}, \dots, \vec{\alpha}^{(N)} \in \range} \left\{ \Pi_{\fclass^{(2)}} \left(\left\{ \left(\vec{v}^{(1)}, \vec{\alpha}^{(1)}\right), \dots, \left(\vec{v}^{(N)}, \vec{\alpha}^{(N)}\right)\right\}\right)\right\} < d^2 \left(N^2t_2\right)^d,\label{eq:shattering}\end{equation} which means that $2^N < N^{2d+a}d^2t_2^d\omega$, and thus $N= O\left((d+a) \log(d+a) + d \log t_2 + \log \omega\right)$.

To this end, let $\vec{\alpha}^{(1)}, \dots, \vec{\alpha}^{(N)}$ be $N$ arbitrary elements of $\range$ and let $z^{(1)}, \dots, z^{(N)}$ be $N$ arbitrary elements of $\R$. Since $\mclass$ is $(d, t_1, t_2)$-divisible, we know that for each $i \in [N]$, there is a set $\hyp_2^{(i)}$ of $t_2$ hyperplanes such that for any connected component $\pspace'$ of $\pspace \setminus \hyp_2^{(i)}$, $f^{(2)}_{\vec{v}^{(i)}, \vec{\alpha}^{(i)}}\left(\vec{p}\right)$ is linear over all $\vec{p} \in \pspace'.$  We now consider the overlay of all $N$ partitions $\pspace\setminus \hyp_2^{(1)}, \dots, \pspace\setminus \hyp_2^{(N)}$. Formally, this overlay is made up of the sets $\pspace_1, \dots, \pspace_{\tau}$, which are the connected components 
of $\pspace \setminus \left(\bigcup_{i = 1}^N \hyp_2^{(i)}\right)$. For each set $\pspace_j$ and each $i \in [N]$, $\pspace_j$ is completely contained in a single connected component of $\pspace \setminus \hyp_2^{(i)}$, which means that $f^{(2)}_{\vec{v}^{(i)}, \vec{\alpha}^{(i)}}\left(\vec{p}\right)$ is linear over $\pspace_j.$
Since $\left|\hyp^{(i)}_2\right|\leq t_2$ for all $i \in [N]$, $\tau < d(Nt_2)^d$ \citep{Buck43:Partition}.

Now, consider a single connected component $\pspace_j$ of $\pspace \setminus \left(\bigcup_{i = 1}^N \hyp^{(i)}_2\right)$. For any sample $\vec{v}^{(i)} \in \sample$, 
we know that $f^{(2)}_{\vec{v}^{(i)}, \vec{\alpha}^{(i)}}\left(\vec{p}\right)$ is linear over $\pspace_j$. 
Let $\vec{a}_j^{(i)} \in \R^d$ and $b_j^{(i)} \in \R$ be the weight vector and offset 
such that $f^{(2)}_{\vec{v}^{(i)}, \vec{\alpha}^{(i)}}\left(\vec{p}\right) = \vec{a}_j^{(i)} \cdot \vec{p} + b_j^{(i)}$ for all $\vec{p} \in \pspace_j$. We know that there is a hyperplane $\vec{a}_j^{(i)} \cdot \vec{p} + b_j^{(i)} = z^{(i)}$ where on one side of the 
hyperplane, $f^{(2)}_{\vec{v}^{(i)}, \vec{\alpha}^{(i)}}\left(\vec{p}\right) \leq z^{(i)}$ and on the other 
side, $f^{(2)}_{\vec{v}^{(i)}, \vec{\alpha}^{(i)}}\left(\vec{p}\right) > z^{(i)}$.
Let $\hyp_{\pspace_j}$ be all $N$ hyperplanes for all $N$ samples, i.e., $\hyp_{\pspace_j} = \left\{\vec{a}_j^{(i)} \cdot \vec{p} + b_j^{(i)} = z^{(i)} : i \in [N]\right\}.$ Notice that in any 
connected component $\pspace'$ of $\pspace_j \setminus \hyp_{\pspace_j}$, for all $i \in [N]$, $f^{(2)}_{\vec{v}^{(i)}, \vec{\alpha}^{(i)}}\left(\vec{p}\right)$ is either greater than $z^{(i)}$ or 
less than $z^{(i)}$ (but not both) for all $\vec{p} \in \pspace'$. 

In total, the number of 
connected components of $\pspace_j \setminus \hyp_{\pspace_j}$ is smaller than $dN^d$. 
The same holds for every partition $\pspace_j$. Thus, the total number of regions where 
for all $i \in [N]$, $f^{(2)}_{\vec{v}^{(i)}, \vec{\alpha}^{(i)}}\left(\vec{p}\right)$ is either greater 
than $z^{(i)}$ or less than $z^{(i)}$ (but not both) is smaller than $dN^d\cdot d(Nt_2)^d$. In other words, \[\left|\left\{\begin{pmatrix}
\textbf{1}\left(f^{(2)}_{\vec{p}}\left(\vec{v}^{(1)}, \vec{\alpha}^{(1)}\right) \geq z^{(1)}\right)\\
\vdots\\
\textbf{1}\left(f^{(2)}_{\vec{p}}\left(\vec{v}^{(N)}, \vec{\alpha}^{(N)}\right) \geq z^{(N)}\right)
\end{pmatrix} : \vec{p} \in \pspace\right\}\right| \leq dN^d\cdot d(Nt_2)^d.\] Since we chose $\vec{\alpha}^{(1)}, \dots, \vec{\alpha}^{(N)}$ and $z^{(1)}, \dots, z^{(N)}$ arbitrarily, we may conclude that Inequality~\eqref{eq:shattering} holds.
\end{proof}

\begin{lemma}\label{lem:f1labels}
Suppose $\mclass$ is an $a$-dimensional linear class over $\range$ and $(d,t_1, t_2)$-divisible. Then for any set $\sample \subseteq \domain$ of size $N$, \[\left|\left\{ (\sample', f^{(1)}_{\vec{p}}(\sample')) : \sample' \subseteq \sample, |\sample'| = a, \vec{p} \in \pspace\right\}\right| \leq N^a\min\left\{|\range|^a, d(at_1)^d\right\}.\]
\end{lemma}

\begin{proof}
To begin with, there are of course at most $N^a$ ways to choose a set $\sample' \subseteq \sample$ of size $a$. How many ways are there to label a fixed set $\sample' = \left\{\vec{v}^{(i_1)}, \dots, \vec{v}^{(i_a)}\right\}$ of size $a$ using functions from $\fclass^{(1)}$? An easy upper bound is $|\range|^a$. Alternatively, we can use the structure of $\mclass$ to prove that there are $d(at_1)^d$ ways to label $\sample'$. Since $\mclass$ is $(d,t_1, t_2)$-divisible, we know that for any $\vec{v}^{(i_j)} \in \sample'$, there is a set $\hyp_1^{(i_j)}$ of $t_1$ hyperplanes such that for any connected component $\pspace'$ of $\pspace \setminus \hyp_1^{(i_j)}$, $f^{(1)}_{\vec{v}^{(i_j)}}(\vec{p})$ is constant over all $\vec{p} \in \pspace'.$
We now consider the overlay of all $a$ partitions $\pspace\setminus \hyp_1^{(i_j)}$ for all $\vec{v}^{(i_j)} \in \sample'$. Formally, this overlay is made up of the sets $\pspace_1, \dots, \pspace_{\tau}$, which are the connected components 
of $\pspace \setminus \left(\bigcup_{\vec{v}^{(i_j)} \in \sample'} \hyp_1^{(i_j)}\right)$. For each set $\pspace_t$ and each $\vec{v}^{(i_j)} \in \sample'$, $\pspace_t$ is completely contained in a single connected component of $\pspace \setminus \hyp_1^{(i_j)}$, which means that $f^{(1)}_{\vec{v}^{(i_j)}}\left(\vec{p}\right)$ is constant over $\pspace_t.$ 
%
This means that the number of ways to label $\sample'$ is at most $\tau$. Since $\left|\hyp_1^{(i_j)}\right|\leq t_1$ for all $\vec{v}^{(i_j)} \in \sample'$, $\tau < d(at_1)^d$ \citep{Buck43:Partition}.
Therefore, $\left|\left\{ \left(\sample', f^{(1)}_{\vec{p}}(\sample')\right) : \sample' \subseteq \sample, |\sample'| = a, \vec{p} \in \pspace\right\}\right| \leq N^a\min\left\{|\range|^a, d(at_t)^d\right\}$, so the lemma statement holds.
\end{proof}