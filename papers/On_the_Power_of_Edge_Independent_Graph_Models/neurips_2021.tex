\documentclass{article}

% if you need to pass options to natbib, use, e.g.:
%     \PassOptionsToPackage{numbers, compress}{natbib}
% before loading neurips_2021

% ready for submission
%\usepackage{neurips_2021}

% to compile a preprint version, e.g., for submission to arXiv, add add the
% [preprint] option:
     \usepackage[nonatbib,preprint]{neurips_2021}

% to compile a camera-ready version, add the [final] option, e.g.:
%     \usepackage[nonatbib,final]{neurips_2021}

% to avoid loading the natbib package, add option nonatbib:
%\usepackage[nonatbib]{neurips_2021}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{xcolor}         % colors

% Things we're adding
\usepackage{amssymb,amsthm,amsmath}
\usepackage{bm}
\usepackage{float}
\usepackage{graphicx}
\usepackage{algorithm}
%\usepackage{algorithmic}
\usepackage{algpseudocode}
\graphicspath{ {./new_images/} }
\DeclareGraphicsExtensions{.pdf,.jpeg,.png}
\usepackage{multibib}

\newcommand{\algoname}[1]{\textnormal{\textsc{#1}}}
\newcommand{\comment}[1]{\text{\phantom{(#1)}} \tag{#1}}
\newcommand{\spara}[1]{\smallskip\noindent{\bf #1}}
\newif\ifdraft

\newcommand{\todo}[1]{\textcolor{blue}{TODO: #1}}
\newcommand{\Cam}[1]{\textcolor{blue}{Cam: #1}}
\newcommand{\Dan}[1]{\textcolor{cyan}{Dan: #1}}
\newcommand{\Kon}[1]{\textcolor{orange}{Kon: #1}}
\newcommand{\eqdef}{\mathbin{\stackrel{\rm def}{=}}}
\makeatletter
\def\hlinewd#1{%
	\noalign{\ifnum0=`}\fi\hrule \@height #1 \futurelet
	\reserved@a\@xhline}
\makeatother

\newtheorem{theorem}{Theorem}
\newtheorem{conjecture}[theorem]{Conjecture}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{fact}[theorem]{Fact}
\newtheorem{claim}[theorem]{Claim}
\newtheorem{assumption}[theorem]{Assumption}
\newtheorem{definition}{Definition}
\newtheorem{problem}[definition]{Problem}
\newtheorem{connection}{Connection}
\newtheorem{example}[theorem]{Example}


\newtheorem*{rep@theorem}{\rep@title}
\newcommand{\newreptheorem}[2]{%
	\newenvironment{rep#1}[1]{%
		\def\rep@title{#2 \ref{##1}}%
		\begin{rep@theorem}}%
		{\end{rep@theorem}}}
\makeatother
\newreptheorem{theorem}{Theorem}
\newreptheorem{claim}{Claim}

\newcommand{\R}{\mathbb{R}}
\newcommand{\C}{\mathbb{C}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\HH}{\mathcal{H}}
\newcommand{\bs}[1]{\boldsymbol{#1}}
\newcommand{\bv}[1]{\mathbf{#1}}
\newcommand{\wh}{\widehat}
\newcommand{\norm}[1]{\|#1\|}
\newcommand{\opnorm}[1]{\|#1\|_\mathrm{op}}
\DeclareMathOperator{\supp}{\mathrm{supp}}
\DeclareMathOperator{\poly}{poly}
\DeclareMathOperator{\cp}{cap}
\DeclareMathOperator{\cheb}{cheb}
\DeclareMathOperator{\argmax}{argmax}
\DeclareMathOperator{\sinc}{sinc}
\DeclareMathOperator*{\argmin}{arg\,min}
\newcommand{\E}{\mathbb{E}}

\DeclareMathOperator{\tr}{tr}
\DeclareMathOperator{\rank}{rank}
\DeclareMathOperator{\err}{err}
\DeclareMathOperator{\erf}{erf}
\DeclareMathOperator{\range}{range}
\DeclareMathOperator{\Null}{null}
% / Things we're adding

\title{On the Power of Edge Independent Graph Models}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to LaTeX to determine where to break the
% lines. Using \AND forces a line break at that point. So, if LaTeX puts 3 of 4
% authors names on the first line, and the last on the second line, try using
% \AND instead of \And before the third author name.

\author{
	Sudhanshu Chanpuriya \\ University of Massachusetts Amherst \\ \texttt{schanpuriya@umass.edu }
	\And 
	Cameron Musco\\ University of Massachusetts Amherst\\ \texttt{cmusco@cs.umass.edu}
	\AND
	Konstantinos Sotiropoulos\\ Boston University\\ \texttt{ksotirop@bu.edu} 
	\And
	Charalampos E. Tsourakakis\\ Boston University \& ISI Foundation \\ \texttt{tsourolampis@gmail.com} 
}

\include{src/bib_macros}
\begin{document}

\maketitle

\input{src/abstract}

%\begin{abstract}
%We study the limits of \emph{edge independent random graph models}, in which  each edge is added to the graph independently with some probability. Such models include the Erd\"{o}s-R\'{e}nyi and stochastic block models, as well as many  modern neural-network-based generative models, such as NetGAN, variational graph autoencoders, and CELL. %We explore the limits of edge independent models. 
%We show that subject to a \emph{bounded overlap} condition, which ensures that the model does not simply memorize a single graph, edge independent models are inherently limited in their ability to generate graphs with high triangle and other subgraph densities. Notably, such high densities are known to appear in real-world social networks and other connection graphs. We complement our negative results with a simple baseline that balances overlap and accuracy, performing comparably to more complex generative models in reconstructing many graph statistics. 
%\end{abstract}

\section{Introduction}
\label{sec:intro}

Our work centers on \emph{edge independent graph models}, in which each edge $(i,j)$ is added to the graph independently with some probability $P_{ij} \in [0,1]$. Formally,
\begin{definition}[Edge Independent Graph Model]\label{def:ei}
For any symmetric matrix $P \in [0,1]^{n \times n}$ let $\mathcal{G}(P)$ be the distribution over undirected unweighted graphs where $G \sim  \mathcal{G}(P)$ contains edge $(i,j)$ independently, with probability $P_{ij}$. I.e., $p(G) = \prod_{(i,j) \in E(G)} P_{ij} \cdot \prod_{(i,j) \notin E(G)} (1-P_{ij})$.
\end{definition}

Edge independent models encompass many  classic random graph models. This includes the Erd\"{o}s-R\'{e}nyi  model, where for all $i \neq j$, $P_{ij} = p$ for  some fixed $p \in [0,1]$ \cite{ErdosRenyi:1960}. It also includes the stochastic block model where $P_{ij} = p$ if two nodes are in the same community and $P_{ij} = q$ if two nodes are in different communities for some fixed $p,q \in [0,1]$ with $q < p$ \cite{SnijdersNowicki:1997}. Other examples include e.g., the Chung-Lu configuration model \cite{ChungLu:2002}, stochastic Kronecker graphs \cite{LeskovecChakrabartiKleinberg:2010}.

Recently, significant attention has focused on \emph{graph generative models}, which seek to learn a distribution over graphs that share similar properties to a given training graph, or set of graphs. Many algorithms parameterize this distribution as an edge independent model or closely related distribution. % based on a probability matrix $P \in [0,1]^{n \times n}$ learned from the input. 
E.g., NetGAN and the closely related CELL model both produce $P \in [0,1]^{n \times n}$ and then sample edges independently without replacement with probabilities proportional to its entries, ensuring that at least one edge is sampled adjacent to each node \cite{bojchevski2018netgan,rendsburgnetgan}.
%\begin{itemize}
%\item NetGAN Without GAN (CELL) \cite{rendsburgnetgan} -- explicitly uses an edge independent model except avoids self loops and isolated nodes.
%\item NetGAN \cite{bojchevski2018netgan} -- create a symmetric score matrix, and sample without replacement with probabilities proportional to the entries. Sample at least one edge from each node, using the row distribution for that node.
Variational Graph Autoencoders (VGAE), GraphVAE, Graphite, and MolGAN are also all based on edge independent models \cite{KipfWelling:2016,SimonovskyKomodakis:2018,De-CaoKipf:2018,GroverZweigErmon:2019}. 
%\item 
%MolGAN \cite{De-CaoKipf:2018}, which focuses on modeling molecular graphs, is based on a closely related model where there are multiple node and edge types. The generated graph can be thought of as a union of edge independent graphs, one for each edge type. 
% -- For expected adjacency tensor $n \times n \times p$ where $p$ is the number of bond types and molecule matrix $n \times k$ where $k$ is the number of molecule types, and then sample from these using 'categorical sampling'. I'm not sure exactly what this means... but might work for us. 
%\todo{Look in code to see exactly what they are doing.}
%\item Graphite \cite{GroverZweigErmon:2019} -- directly uses an edge independent model.
%\item Variational Graph Autoencoders \cite{KipfWelling:2016} -- seem to use edge independent model
%\item GraphVAE \cite{SimonovskyKomodakis:2018} -- also using the variational autoencoder approach, and I think edge independent although less clear? It might be that they just take max entries in a continous adjacency matrix, which is randomized due to a random input to the network.
%\end{itemize}

%Standard edge independent models:
%\begin{itemize}
%\item Erdos R\'{e}nyi graphs
%\item Stochastic block model
%\item Chung-Lu configuration model
%\item \url{https://services.math.duke.edu/~rtd/math777/CL_PNAS.pdf}
%\item Kronecker graphs?
%\end{itemize}

Given their popularity in both classical and modern graph generative models, it is natural to ask:
\begin{quote} \emph{How suited are edge independent models to modeling real-world networks. Are they able to capture features such as power-law degree distributions, small-world properties, and high clustering coefficients (triangle densities)? }
\end{quote}

\subsection{Impossibility Results for Edge Independent Models}

In this work we focus on the ability of edge independent models to generate graphs with high triangle, or other small subgraph densities. High triangle density (equivalently, a high clustering coefficient) is a well-known hallmark of real-work networks \cite{WattsStrogatz:1998,SalaCaoWilson:2010,DurakPinarKolda:2012} and has been the focus of recent work exploring the power and limitations of edge-independent graph models \cite{SeshadhriSharmaStolman:2020,ChanpuriyaMuscoSotiropoulos:2020}.

It is clear that edge independent models can generate triangle dense graphs. In particular, $P \in [0,1]^{n \times n}$ in Def. \ref{def:ei} can be set to the binary adjacency matrix of any undirected graph, and $\mathcal{G}(P)$ will generate that graph with probability $1$, no matter how triangle dense it is. However, this would not be a particularly interesting generative model -- ideally $\mathcal{G}(P)$ should generate a wide range of graphs. To capture this intuitive notion, we define the \emph{overlap} of an edge-independent model, which is closely related to the overlap stopping criterion for training used in training graph generative models \cite{bojchevski2018netgan,rendsburgnetgan}.
\begin{definition}[Expected Overlap]\label{def:ov} For symmetric $P \in [0,1]^{n \times n}$ let $V(P) \eqdef \E_{G \sim \mathcal{G}(P)} |E(G)|$ and
% the expected overlap of two graphs drawn independently from $\mathcal{G}(P)$ is:
\begin{align*}
Ov(P) \eqdef \frac{\E_{G_1,G_2 \sim \mathcal{G}(P)} |E(G_1) \cap E(G_2)|}{V(P)}.
\end{align*}
\end{definition}
That is, for any $P \in [0,1]^{n \times n}$, $Ov(P) \in [0,1]$ is the ratio of the expected number of edges shared by two graphs drawn independently from $\mathcal{G}(P)$ to the expected number of edges in a graph drawn from $\mathcal{G}(P)$. In one extreme, when $P$ is a binary adjacency matrix, $Ov(P) = 1$, and our generative model has simply memorized a single graph. In the other, if $P_{ij} = p$ for all $i \neq j$ (i.e., $\mathcal{G}(P)$ is Erd\"{o}s-R\'{e}nyi), $Ov(P) = p$. This is the minimum possible overlap when $V(P) = p \cdot {n \choose 2}$.

Our main result is that for any edge independent model with bounded overlap, $G \sim \mathcal{G}(P)$ cannot have too many triangles in expectation. In particular:
\begin{theorem}[Main Result -- Expected Triangles]\label{thm:tri} For a graph $G$, let $\Delta(G)$ denote the number of triangles in $G$. Consider symmetric $P \in [0,1]^{n \times n}$.
\begin{align*}
\E_{G \sim \mathcal{G}(P)} \left [\Delta(G) \right ]\le \frac{\sqrt{2}}{3} \cdot Ov(P)^{3/2} \cdot V(P)^{3/2}.
\end{align*}
\end{theorem}
As an example, consider the setting where the distribution generates sparse graphs, with $V(P) = \Theta(n)$. Theorem \ref{thm:tri} shows that whenever  $Ov(P) = o(1/n^{1/3})$, $\E_{G \sim \mathcal{G}(P)} \Delta(G) = o(n)$ -- i.e. the graph is very triangle sparse with the number of triangles sublinear in the number of nodes. %A distribution with higher triangle density thus requires $Ov(P) = \Omega(1/n^{1/3})$. 
This verifies that  an Erd\"{o}s-R\'{e}nyi graph cannot achieve simultaneously linear number of edges  (i.e., $Ov(P) = O(1/n)$ ) and super-linear number of triangles (i.e., $Ov(P) = \Omega(1/n^{1/3})$) under our proposed lens of viewing generative models. 

%On the otherhand, if the distribution simply memorizes a single sparse graph, and has $Ov(P) =1$, then the theorem allows $\E_{G \sim \mathcal{G}(P)} \left [\Delta(G) \right ]$ to be as large as $\Theta(n^{3/2})$. This is indeed matched, when the model simply memorizes the clique on $n^{1/2}$ nodes.

We extend Theorem \ref{thm:tri} to give similar bounds for the density of squares and other $k$-cycles (Thm. \ref{thm:k}), as well as for the global clustering coefficient (Thm. \ref{thm:cc}). In all cases we show that our bounds are tight -- e.g., in the triangle case, %for any $\gamma \in (0,1]$,
 there is indeed an edge independent model with %with $Ov(P) = \gamma$ and 
 $\E_{G \sim \mathcal{G}(P)} \left [\Delta(G) \right ] = \Theta \left (Ov(P)^{3/2} \cdot V(P)^{3/2} \right )$, matching the lower bound in Theorem \ref{thm:tri}. 
 
 %Our proofs are extremely simple -- in short, $Ov(P)$ is closely related to the Frobenius norm $\norm{P}_F^2$. 

%A number of recent papers have focused on this question \cite{SeshadhriSharmaStolman:2020,ChanpuriyaMuscoSotiropoulos:2020}

\subsection{Empirical Findings}

Our theoretical results help explain why, despite performing well in a variety of other metrics, edge independent graph generative models have been reported to generate graphs with many fewer triangles and squares on average than the real-world graphs that they are trained on. Rendsburg et al. \cite{rendsburgnetgan} test a suite of these models, including their own CELL model and the related NetGAN model \cite{bojchevski2018netgan}. Of all these models, when trained on the \textsc{Cora-ML} graph with 2,802 triangles and 14,268 squares, none is able to generate graphs with more than 1,461 triangles and 6,880 squares on average. Similar gaps are observed for a number of other graphs.
 Rendsburg et al. also report that the triangle count increases as their notion of overlap (closely related to Def. \ref{def:ov}) increases. Theorem \ref{thm:tri} demonstrates that this underestimation of triangle count, and its connection to overlap is \emph{inherent to all edge independent models, no matter how refined a method used to learn the underlying probability matrix $P$}. 
 
 While our theoretical results bound the performance of any  edge independent model, there may still be variation in how specific models trade-off overlap and realistic graph generation. 
 To better understand this trade-off, we introduce two simple models with easily tunable overlap as baselines. One is based on reproducing the degree sequence of the original graph; the other, which is even simpler, is based on reproducing the volume.
 In both  models, $P$ is a  weighted average of the input graph adjacency matrix and a probability matrix of minimal complexity which matches either the input degrees or the volume. In the latter case, to match just the volume, we simply use an Erd\"{o}s-R\'{e}nyi graph. In the former case, to match the degree sequence, we introduce our own model, the \emph{odds product model}; this model is similar to the Chung-Lu configuration model \cite{ChungLu:2002}, but, unlike Chung-Lu, is able to match degree sequences of real-world graphs with high maximum degree.
We find that these simple baselines are often competitive with more complex models like CELL in terms of matching key graph statistics, like triangle count and clustering coefficient, at similar levels of overlap. %\Cam{What should the takeway from this finding be? Can we add a sentence or two?}

%\todo{Collect results here showing which features they tend to  capture well and which they tend not too. }

\subsection{Related Work}\label{sec:rel}

\noindent\textbf{Existing impossibility results.} Our work is inspired by that of Seshadhri et al. \cite{SeshadhriSharmaStolman:2020}, which also proves limitations on the ability of edge independent models to represent triangle dense graphs. They show that if $P = \max(0,\min(1,XX^T))$ where $X \in \R^{n \times k}$ for $k \ll n$ and the max and min are applied entrywise, then $G \sim \mathcal{G}(P)$ cannot have many triangles adjacent to low-degree nodes in expectation. This setting arises commonly when $P$ is generated using low-dimensional node embeddings -- represented by the rows of $X$. Chanpuriya et al. \cite{ChanpuriyaMuscoSotiropoulos:2020}, show that in a slightly more general model, where $P = \max(0,\min(1,XY^T))$, this lower bound no longer holds -- $X,Y \in \R^{n \times k}$ can be chosen so that $P$ is the binary adjacency matrix of any graph with maximum degree upper bounded by $O(k)$ -- no matter how triangle dense that graph is. Thus, even such low-rank edge independent models can represent triangle dense graphs -- by memorizing a single one. In the appendix, we prove a similar result when $P$ is generated from the CELL model of \cite{rendsburgnetgan}, which simplifies NetGAN \cite{bojchevski2018netgan}.

%However, this comes at the cost of high overlap -- if $P$ is a binary adjacency matrix, $Ov(P) = 1$. 
Our results show that this trade-off between the ability to capture triangle density and memorization is inherent -- even without any low-rank constraint, edge independent models with low overlap simply cannot represent graphs with high triangle or other small subgraph density.

It is well understood that specific edge independent models, e.g., Erd\"{o}s-R\'{e}nyi graphs, the Chung-Lu model, and stochastic Kronecker graphs, do not capture many properties of real-world networks, including high triangle density \cite{WattsStrogatz:1998,PinarSeshadhriKolda:2012}. Our results can be viewed as a generalization of these observations, to all edge independent models with low overlap. Despite the limitations of classic models, edge independent models are still very prevalent in today's literature on graph generative models. Our more general results make clear the limitations of this approach.

\noindent\textbf{Non-independent models.} While edge independent models are very prevalent in the literature, many important models do not fit into this framework. Classic models include the Barab\'{a}si–Albert and other preferential attachment models \cite{BarabasiAlbert:1999}, Watts–Strogatz  small-world graphs \cite{WattsStrogatz:1998}, and random geometric graphs \cite{DallChristensen:2002}. Many of these models were introduced directly in response to shortcomings of classic edge independent models, including their  inability to produce high triangle densities

More recent graph generative models include 
GraphRNN \cite{YouYingRen:2018} and a number of other works \cite{LiVinyalsDyer:2018,LiaoLiSong:2019}.
%-- generates one node at a time, along with all its connections. In simplified models, edges from node are added independently. But probabilities may depend on previous connections. thus this doesn't seem to fall under the edge independent model. You could easily generate triangle dense graphs with this set up.
%\item \cite{LiVinyalsDyer:2018} -- also generates things as a non-independent sequence.
%\end{itemize} 
Our impossibility results do not apply to such models, and in fact suggest that perhaps they may be preferable to edge independent models, if a distribution over graphs with high triangle density is desired. A very interesting direction for future work would be to prove limitations on broad classes of non-independent models, and perhaps to understand exactly what type of correlation amongst edges is needed to generate graphs with both low overlap \footnote{We note that for non-edge independent models, the measure of overlap as defined earlier should be adapted to take into account the order (permutation) of the vertices in the final graph. In particular, the overlap in this case should be the maximum value of it over any permutation of the vertex set.}and hallmark features of real-world networks.


\section{Impossibility Results for Edge Independent Models}
\label{sec:impossibility}

We now prove our main results on the limitations of edge independent models with bounded overlap. % in generating graphs with high triangle and other $k$-cycle densities. 
We start with a simple lemma that will be central in all our proofs.

%\begin{definition}[Expected Volume] For $P \in [0,1]^{n \times n}$ the expected volume of a graph drawn from $\mathcal{G}(P)$ is:
%\begin{align*}
%V(P) = \E_{G \sim \mathcal{G}(P)}[2\cdot |E(G)|]
%\end{align*}
%\end{definition}

\begin{lemma}\label{lem:mainSimple} For any symmetric $P \in [0,1]^{n \times n}$, $\frac{\norm{P}_F^2}{2} \le Ov(P) \cdot V(P) \le \norm{P}_F^2.$
\end{lemma}
\begin{proof}
Let $I[(i,j) \in G]$ be the $0,1$ indicator random variable that an edge $(i,j)$ appears in the graph $G$.  $Ov(P) \cdot V(P) = \E_{G_1,G_2 \sim \mathcal{G}(P)} |E(G_1) \cap E(G_2)|$. 
By linearity of expectation and the independence of $G_1$ and $G_2$ we have,
$$Ov(P) \cdot V(P) = \E_{G_1,G_2 \sim \mathcal{G}(P)} \sum_{i \le j} I[(i,j) \in G_1] \cdot I[(i,j) \in G_2] = \sum_{i \le j} P_{ij}^2.$$
The bound follows since $P$ is symmetric. Note that the lower bound $\frac{\norm{P}_F^2}{2} \le Ov(P) \cdot V(P)$ is an equality if $P$ is $0$ on the diagonal -- i.e., there is no probability of self loops.
\end{proof}

%\begin{definition}[Expected Degree and Triangle Density] For $P \in [0,1]^{n \times n}$ the expected degree 
% density of a graph drawn from $\mathcal{G}(P)$ is:
%\begin{align*}
%T(P) = \E_{G \sim \mathcal{G}(P)} \sum_{.
%\end{align*}
%\end{definition}

\subsection{Triangles}

Lemma \ref{lem:mainSimple} connects $Ov(P) \cdot V(P)$ to $\norm{P}_F^2$ and in turn the eigenvalue spectrum of $P$ since $\norm{P}_F^2 = \sum_{i=1}^n \lambda_i(P)^2$, where $\lambda_1(P),\ldots, \lambda_n(P) \in \R$ are the eigenvalues of $P$. The expected number of triangles in $G \sim \mathcal{G}(P)$ can be written in terms of this spectrum as well, allowing us to relate overlap to this expected triangle count, and prove our main theorem (Theorem \ref{thm:tri}), restated below.

\begin{reptheorem}{thm:tri} For a graph $G$, let $\Delta(G)$ denote the number of triangles in $G$. Consider symmetric $P \in [0,1]^{n \times n}$. 
\begin{align*}
\E_{G \sim \mathcal{G}(P)} \left [\Delta(G) \right ]\le \frac{\sqrt{2}}{3} \cdot Ov(P)^{3/2} \cdot V(P)^{3/2}.
\end{align*}
\end{reptheorem}
\begin{proof}
By linearity of expectation,
\begin{align}
\E_{G \sim \mathcal{G}(P)} \left [\Delta(G) \right ] &= \frac{1}{6} \sum_{i=1}^n \sum_{j=1}^n \sum_{k=1}^n \Pr \left [(i,j) \in E(G) \cap (j,k) \in E(G) \cap (k,i) \in E(G) \right]\nonumber \\
&= \frac{1}{6} \sum_{i=1}^n \sum_{j=1}^n \sum_{k=1}^n P_{ij} P_{jk} P_{ki} = \frac{1}{6} \tr(P^3) = \frac{1}{6} \sum_{i=1}^n \lambda_i(P)^3.\label{eq:sixth}
\end{align}
Letting $\lambda_1(P)$ denote the largest magnitude eigenvalue of $P$, we can in turn bound
$$\tr(P^3) \le |\lambda_1(P)| \cdot \sum_{i=1}^n \lambda_i(P)^2 = |\lambda_1(P)| \cdot \norm{P}_F^2.$$ Since $|\lambda_1(P)| \le \norm{P}_F$, this gives via Lemma \ref{lem:mainSimple}
$$\tr(P^3) \le \norm{P}_F^3 \le 2 \sqrt{2} \cdot Ov(P)^{3/2} \cdot V(P)^{3/2}.$$
%where the last inequality follows from Lemma \ref{lem:mainSimple}. 
Combining this bound with \eqref{eq:sixth} completes the theorem.
\end{proof}

%\Cam{Switch to ER example but still mention clique for intuition.}
The bound of Theorem \ref{thm:tri} is tight up to constants, for any possible value of $Ov(P)$. The tight example is when $P$ is simply an Erd\"{o}s-R\'{e}nyi graph.

% is when $P$ consists of a clique on $\Theta(Ov(P) \cdot \sqrt{n})$ nodes, unioned with a sparse Erd\"{o}s-R\'{e}nyi graph.
\begin{theorem}[Tightness of Expected Triangle Bound]\label{thm:tight}
For any $\gamma \in (0,1]$, there exists a symmetric $P \in [0,1]^{n \times n}$ with $Ov(P) = \gamma$ and $\E_{G \sim \mathcal{G}(P)} [\Delta(G)] = \Theta( \gamma^{3/2} \cdot V(P)^{3/2})$.
 \end{theorem}
 \begin{proof}
 Let $P_{ij} = \gamma$ for all $i \neq j $. We have $V(P) = \gamma \cdot {n \choose 2}$ and $Ov(P) \cdot V(P) = \gamma^2 \cdot {n \choose 2}$ Thus, $Ov(P)= \gamma$. Further,  by linearity of expectation, 
 $$\E_{G \sim \mathcal{G}(P)} [\Delta(G)] = \gamma^3 \cdot {n \choose 3} = \Theta(\gamma^3 \cdot n^3) = \Theta(\gamma^{3/2} \cdot V(P)^{3/2}).$$
% 
% Let $S$ be a subset of $\sqrt{ \gamma  n/2}$ nodes. %, where $c$ is a fixed constant.  
%Let  $P_{ij} = 1$ for all $i \neq j$, $i,j \in S$ and $P_{ij} = 1/n$ for all other pairs $i,j$ with $i \neq j$.    
% We have $V(P) \ge \frac{1}{n} \cdot {n \choose 2} = \frac{n-1}{2} $. %On the rest of the graph we will have $ \Theta(n)$ edges in expectation. 
% Further, by Lemma \ref{lem:mainSimple},
% $$Ov(P) \cdot V(P) \le \norm{P}_F^2 \le {\sqrt{\gamma n/2} \choose 2 } + \frac{1}{n^2} \cdot n^2 \le \frac{\gamma n}{4} +1.$$
% Combined with our bound on $V(P)$, as long as $n$ is large enough so that  $\gamma n \ge 6$,
% $$Ov(P) \le \frac{\frac{\gamma n}{4} +1}{\frac{n-1}{2}} = \frac{\frac{\gamma n}{2} +2}{n-1} \le \frac{\frac{\gamma n}{2}(1+2/3)}{5/6 \cdot n}  \le \gamma.$$
% 
% Finally, for $G \sim \mathcal{G}(P)$, $\Delta(G)$ is lower bounded by the number of triangles in the clique on $S$ which $G$ contains with probability $1$ -- i.e., 
% $$\E_{G \sim \mathcal{G}(P)}[\Delta(G)] \ge {\sqrt{\gamma n/2} \choose 3}= \Theta(\gamma^{3/2} \cdot n^{3/2}) =  \Theta(\gamma^{3/2} \cdot V(P)^{3/2}).$$
  \end{proof}
  We note that another example when Theorem \ref{thm:tri} is tight is when $P$ is a union of a fixed clique on $\Theta(\gamma \cdot n)$ nodes and an Erd\"{o}s-R\'{e}nyi graph with connection probability $1/n$ on the rest of the nodes.
  
\subsection{Squares and Other $k$-cycles}
  
We can extend Thm. \ref{thm:tri} to bound the expected number of $k$-cycles in $G \sim \mathcal{G}(P)$ in terms of $Ov(P)$. %, we prove in the appendix,
%
%For example,
% \begin{theorem}[Bound on Expected Squares]\label{thm:square} For a graph $G$, let $\square(G)$ denote the number of squares (4-cycles) in $G$. Consider $P \in [0,1]^{n \times n}$ with $Ov(P) \le \gamma \cdot V(P)$ for some $\gamma <1$. Then
%\begin{align*}
%\E_{G \sim \mathcal{G}(P)} \left [\square(G) \right ]\le \frac{1}{2} \cdot \gamma^2 \cdot V(P)^2.
%\end{align*}
%\end{theorem}
%\begin{proof}
%The number of squares is the number of non-backtracking 4-cycles in $G$ (i.e. squares), which can be written as:
%\begin{align*}
%\E_{G \sim \mathcal{G}(P)} \left [\square(G) \right ] = \frac{1}{8} \cdot \sum_{i =1}^n \sum_{j \in [n] \setminus i} \sum_{k \in [n]\setminus\{i,j\}} \sum_{\ell \in [n]\setminus \{i,j,k\}} P_{ij} P_{jk} P_{k \ell} P_{\ell i}.
%\end{align*}
% The $1/8$ factor accounts for the fact that in the sum, each square is counted $8$ times -- once for each potential starting vector $i$ and once of each direction it may be traversed. We then can bound
%\begin{align*}
%\E_{G \sim \mathcal{G}(P)} \left [\square(G) \right ]  \le \frac{1}{8} \cdot\sum_{i \in [n]} \sum_{j \in [n]} \sum_{k \in [n]} \sum_{\ell \in [n]} P_{ij} P_{jk} P_{k \ell} P_{\ell i} = \frac{1}{8} \cdot \tr(P^4).
%\end{align*}
%This in turn gives
%\begin{align*}
%\E_{G \sim \mathcal{G}(P)} \left [\square(G) \right ] \le \frac{1}{8} \cdot |\lambda_1(P)|^2 \cdot \norm{P}_F^2 \le \frac{1}{8} \norm{P}_F^4 \le \frac{1}{2} Ov(P)^2.
%\end{align*}
%This completes the theorem after plugging in  $Ov(P) \le \gamma \cdot V(P)$.
%\end{proof}
%It is not hard to see that Theorem \ref{thm:square} is also tight in the same example as Theorem \ref{thm:tight}, since a clique on $\sqrt{c \gamma n}$ vertices will have ${\sqrt{c \gamma n} \choose 4} = \Theta(\gamma^2 n^2) = \Theta(\gamma^2  \cdot V(P)^2)$ squares. We can also just directly extend the proof of Theorem \ref{thm:square} to  bound the expected number of $k$-cycles in $G$, giving a bound which is also tight up to constants by the example of Theorem \ref{thm:tight} for $k = O(1)$.
\begin{theorem}[Bound on Expected $k$-cycles]\label{thm:k}
For a graph $G$, let $C_k(G)$ denote the number of $k$-cycles in $G$. Consider symmetric $P \in [0,1]^{n \times n}$. 
% with $Ov(P) \le \gamma \cdot V(P)$ for some $\gamma <1$. Then
\begin{align*}
\E_{G \sim \mathcal{G}(P)} \left [C_k(G) \right ] \le \frac{2^{k/2}}{2k} \cdot Ov(P)^{k/2} \cdot V(P)^{k/2}.
\end{align*}
\end{theorem}
\begin{proof}
For notational simplicity, we focus on $k = 4$. The proof directly extends to general $k$. $C_4(G)$ is the number of non-backtracking 4-cycles in $G$ (i.e. squares), which can be written as
\begin{align*}
\E_{G \sim \mathcal{G}(P)} \left [C_4(G) \right ] = \frac{1}{8} \cdot \sum_{i =1}^n \sum_{j \in [n] \setminus i} \sum_{k \in [n]\setminus\{i,j\}} \sum_{\ell \in [n]\setminus \{i,j,k\}} P_{ij} P_{jk} P_{k \ell} P_{\ell i}.
\end{align*}
 The $1/8$ factor accounts for the fact that in the sum, each square is counted $8$ times -- once for each potential starting vector $i$ and once of each direction it may be traversed. For general $k$-cycles this factor would be $\frac{1}{2k}$. We then can bound
\begin{align*}
\E_{G \sim \mathcal{G}(P)} \left [C_4(G) \right ]  \le \frac{1}{8} \cdot\sum_{i \in [n]} \sum_{j \in [n]} \sum_{k \in [n]} \sum_{\ell \in [n]} P_{ij} P_{jk} P_{k \ell} P_{\ell i} = \frac{1}{8} \cdot \tr(P^4).
\end{align*}
For general $k$-cycles this bound would be $\E_{G \sim \mathcal{G}(P)} \left [C_k(G) \right ] \le \frac{1}{2k} \tr(P^k).$
This in turn gives
\begin{align*}
\E_{G \sim \mathcal{G}(P)} \left [C_k(G) \right ] \le \frac{1}{2k} \cdot |\lambda_1(P)|^{k-2} \cdot \norm{P}_F^{2} \le \frac{1}{2k} \norm{P}_F^k \le \frac{2^{k/2}}{2k} Ov(P)^{k/2} \cdot V(P)^{k/2},
\end{align*}
where the last bound follows from Lemma \ref{lem:mainSimple}.
This completes the theorem..
\end{proof}
It is not hard to see that Theorem \ref{thm:k} is also tight up to a constant depending on $k$ for any overlap $\gamma \in (0,1]$, also for an Erd\"{o}s-R\'{e}nyi graph with connection probability $\gamma$. 
\begin{theorem}[Tightness of Expected $k$-cycle Bound]\label{thm:tightK}
For any $\gamma \in (0,1]$, there exists $P \in [0,1]^{n \times n}$ with $Ov(P) = \gamma$ and $\E_{G \sim \mathcal{G}(P)} [C_k(G)] = \Theta \left ( \frac{\gamma^{k/2} \cdot V(P)^{k/2}}{k!} \right)$.
 \end{theorem}
%\begin{proof}
%Consider the same $P$ as Thm. \ref{thm:tight}. The clique on $\sqrt{\gamma n/2}$ vertices has $ { \sqrt{\gamma n/2} \choose k} = \Theta \left (\frac{\gamma^{k/2} n^{k/2}}{k!} \right)$ $k$-cycles. So $Ov(P) \le \gamma$ and $\E_{G \sim \mathcal{G}(P)} \left [C_k(G) \right ] = \Theta \left (\frac{\gamma^{k/2}  \cdot V(P)^{k/2}}{k!} \right)$, giving the result. % Theorem \ref{thm:k} up to a $\Theta \left (\frac{2^{k/2} \cdot k!}{2k} \right )$ factor.
%\end{proof}

%We can also just directly extend the proof of Theorem \ref{thm:square} to  bound the expected number of $k$-cycles in $G$, giving a bound which is also tight up to constants by the example of Theorem \ref{thm:tight} for $k = O(1)$.
%Theorem \ref{thm:kclique} is also tight up to constants for constant $k$, by the same example as Theorem \ref{thm:tight}.
  
%  We can easily extend Theorem \ref{thm:tri} to $k$-cliques. We have
%  
%  \begin{theorem}[Bound on Expected $k$-Cliques]\label{thm:kclique} For a graph $G$, let $kC(G)$ denote the number of k-Cliques in $G$. Consider $P \in [0,1]^{n \times n}$ with $Ov(P) \le \gamma \cdot V(P)$ for some $\gamma <1$. Then
%\begin{align*}
%\E_{G \sim \mathcal{G}(P)} \left [kC(G) \right ]\le \frac{2^{k/2}}{k!} \cdot \gamma^{k/2} \cdot V(P)^{k/2}
%\end{align*}
%\end{theorem}
%\begin{proof}
%We can write $\E_{G \sim \mathcal{G}(P)} \left [kC(G) \right ] = \frac{1}{k!} \cdot \tr(P^k).$ Additionally, $\tr(P^{k}) \le |\lambda_1(P)|^{k-2} \cdot \tr(P^2) = |\lambda_1(P)|^{k-2} \cdot \norm{P}_F^2$, where $\lambda_1(P)$ is the largest magnitude eigenvalue of $P$. Since $\lambda_1(P)^2 \le \norm{P}_F^2 = \sum_{i=1}^n \lambda_i(P)^2$, this gives
%$$\tr(P^k) \le \norm{P}_F^k \le 2^{k/2} \cdot Ov(P)^{k/2},$$
%where the last inequality follows from Lemma \ref{lem:mainSimple}. This completes the lemma after using the assumption that $Ov(P) \le \gamma \cdot V(P)$.
%\end{proof}
%Theorem \ref{thm:kclique} is also tight up to constants for constant $k$, by the same example as Theorem \ref{thm:tight}.
    
\subsection{Clustering Coefficient}
Theorem \ref{thm:tri} shows that the expected number of triangles generated by an edge independent model is bounded in terms of the model's overlap. Intuitively, we thus expect that graphs generated by the edge independent model will have low global clustering coefficient, which is the fraction of wedges in the graph that are closed into triangles \cite{WattsStrogatz:1998}.

\begin{definition}[Global Clustering Coefficient]\label{def:cc}
For a graph $G$ with $\Delta(G)$ triangles, no self-loops, and node degrees $d_1,d_2,\ldots, d_n$, the global clustering coefficient is given by
\begin{align*}
C(G) = \frac{3 \Delta(G)}{\sum_{i=1}^n d_i(d_i-1)}.
\end{align*}
\end{definition}
%
%
We extend Theorem \ref{thm:tri} to give a bound on $E_{G \sim \mathcal{G}(P)} \left [C(G) \right ]$ in terms of $Ov(P)$. The proof is related, but  more complex due to the $\sum_{i=1}^n d_i(d_i-1)$ in the denominator of $C(G)$.
\begin{theorem}[Bound on Expected Clustering Coefficient]\label{thm:cc}
Consider symmetric $P \in [0,1]^{n \times n}$ with zeros on the diagonal and with $V(P) \ge 2 n$. %  and  $O(P) < \gamma \cdot V(P)$ for some $\gamma < 1$. Then % for $G \sim \mathcal{G}(P)$ with probability at least $1-\exp(-\Theta(n))$, the clustering coefficient is bounded by
\begin{align*}
E_{G \sim \mathcal{G}(P)} \left [C(G) \right ] = O \left (\frac{Ov(P)^{3/2} \cdot n}{V(P)^{1/2}} \right ).
\end{align*}
\end{theorem}
%
\begin{proof}
By Theorem \ref{thm:tri} we have $\E_{G \sim \mathcal{G}(P)} \left [3 \Delta(G) \right ]\le \sqrt{2} \cdot  Ov(P)^{3/2} \cdot V(P)^{3/2}$. We will show that with high probability, $\sum_{i=1}^n d_i (d_i - 1) = \Omega (V(P)^2/n)$, which will give the theorem.
Note that $\E_{G \sim \mathcal{G}(P)} \left [\sum_{i =1}^n d_i \right ] = \E_{G \sim \mathcal{G}(P)}[2|E(G)|] = 2 V(P)$. Thus,
by a Bernstein bound, for large enough $n$ since $V(P) \ge 2n$.
\begin{align*}
\Pr \left [ \left |\sum_{i =1}^n d_i - 2V(P) \right | \ge V(P)/5 \right ] \le 2 \exp\left (- \frac{V(P)^2/50}{V(P)+V(P)/15} \right ) \ll \frac{1}{n^2},
\end{align*}
We can bound 
%
%
%
 %$1-\exp(-\Theta(n))$, $$9/10 \cdot V(P) \le \sum_{i=1}^n d_i \le 11/10 \cdot V(P).$$ We can bound  
 $\sum_{i=1}^{n} d_i^2 \ge \frac{\left (\sum_{i=1}^n d_i\right)^2}{n}$. Thus, with probability $\ge 1-1/n^2$,
\begin{align*}
\sum_{i=1}^n d_i(d_i - 1) \ge \frac{(8/5)^2 \cdot V(P)^2}{n} - \frac{12}{5} V(P) \ge \frac{V(P)^2}{n},
\end{align*}
where in the last step we use that $V(P) \ge 2n$ and so $\frac{12}{5} \cdot V(P) \le \frac{6}{5} \cdot \frac{V(P)^2}{n}$. Combined with our bound on $\E_{G \sim \mathcal{G}(P)} \left [3 \Delta(G) \right ]$, and the fact that $C(G) \le 1$ always, we have
\begin{align*}
E_{G \sim \mathcal{G}(P)} \left [C(G) \right ] = O \left ( \frac{ Ov(P)^{3/2} V(P)^{3/2}}{\frac{V(P)^2}{n}} + \frac{1}{n^2} \right ) = O \left (\frac{Ov(P)^{3/2} \cdot n}{V(P)^{1/2}} \right ).
\end{align*}
\end{proof}

Thus, to have a constant clustering coefficient for a graph with $O(n)$ edges in expectation, we need $Ov(P) = \Omega(1/n^{1/3})$. Note that the requirement of $V(P) \ge 2 n$ is very mild -- it means that the expected average degree is at least $1$.

As with our triangle bound, Theorem \ref{thm:cc} is tight when $\mathcal{G}(P)$ is just an Erd\"{o}s-R\'{e}nyi distribution.
\begin{theorem}[Tightness of Expected Clustering Coefficient Bound]\label{thm:tightCC}
For any $\gamma \in (0,1]$, there exists $P \in [0,1]^{n \times n}$ with zeros on the diagonal, $Ov(P) \le \gamma$ and $\E_{G \sim \mathcal{G}(P)} [C(G)] = \Theta \left (\frac{\gamma^{3/2} \cdot n}{V(P)^{1/2}} \right )$.
 \end{theorem}
 \begin{proof}
Let $P_{ij} = \gamma$ for all $i \neq j$. We have $V(P) = \gamma \cdot {n \choose 2} = \Theta(\gamma n^2)$ and $Ov(P) =  \gamma$. Additionally,  %and $\E_{G \sim \mathcal{G}(P)} [C(G)] = \Theta \left (\frac{\gamma^{3/2} \cdot n}{V(P)^{1/2}} \right )$.
% We have $V(P) = \gamma \cdot n^2$ and $Ov(P) = \gamma^2 \cdot n^2$ which confirms that $Ov(P) \le \gamma \cdot V(P)$. We have 
 $\E[\Delta(G)] = \Theta(\gamma^3 \cdot n^3)$, and, if $n$ is large enough with respect to $\gamma$, with very high probability, $\sum_{i=1}^n d_i(d_i-1) \le \sum_{i=1}^n d_i^2 = O(\gamma^2 n^3)$. This gives:
 \begin{align*}
\E_{G \sim \mathcal{G}(P)} [C(G)] = \Theta (\gamma) = \Theta \left (\frac{\gamma^{3/2} \cdot  n}{\gamma^{1/2} \cdot n} \right ) = \Theta \left ( \frac{\gamma^{3/2} \cdot n}{V(P)^{1/2}} \right ).
 \end{align*}
 \end{proof}
 
% \subsection{Degree Sequence}
% 
% We conclude
% 
%\begin{theorem}[Degree Sequence Overlap Bound]
% Let $z \in \R^{n}$ be a degree sequence, with $Z = \max_i z_i$ and $vol = \norm{z}_1$. If $Z = \sqrt{vol \cdot \gamma}$ for some $\gamma \le 1$, then for any $P$ with $\E_{G \sim \mathcal{G}(P)} d_i = z_i$ for all $i$, we have $OV(P) \ge \gamma \cdot V(P)$.
% \end{theorem}
%\begin{proof}
%Consider the degree sequence $z$ corresponding to a graph which is a union of $1/\gamma$ cliques (including self-loops) each containing m nodes. We have $Z = m$ and $vol = m^2/\gamma$. Thus $Z = \sqrt{vol \cdot \gamma}$ as required.
%
%Now, for $P$ to satisfy  $\E_{G \sim \mathcal{G}(P)} d_i = z_i$ for all $i$, P must have rows/column summing to $m$ for each node in a clique and $0$'s in all other rows/columns. Since there are $m/\gamma$ nodes total in the cliques, we can maximally spread out the mass in a row of $P$ corresponding to a clique node by having value $\gamma$ in each of the $m/\gamma$ locations where we can place non-zero value. This $P$ contains an $m/\gamma \times m/\gamma$ block of all $\gamma$s, with all other entries set to $0$. We thus have $Ov(P) \le  ||P||_F^2 = m^2$. Further, $V(P) = m^2/\gamma$. So $Ov(P) = \gamma \cdot V(P)$. This is the the minimum value achievable sine we have minimized $\norm{p_i}_2^2/\norm{p_i}_1$ for every row $p_i$ of $P$.
%
%\end{proof}
%
% \begin{theorem}[Bounded Overlap for the Chung-Lu Model]\label{thm:chung}
% Let $z \in \R^{n}$ be a degree sequence, with $Z = \max_i z_i$ and $vol = \norm{z}_1$. If $Z \le \sqrt{vol \cdot \gamma}$ for some $\gamma \le 1$ then letting $P =  \frac{1}{vol} \cdot z z^T$, for all $i$, $\E_{G \sim \mathcal{G}(P)} d_i = z_i$. Further, $Ov(P) \le \gamma \cdot V(P).$
% \end{theorem}
% \begin{proof}
% This is immediate since all entries of $P$ are bounded by $Z^2/vol \le \gamma$. Thus, $Ov(P) \le \norm{P}_F^2 \le \gamma \cdot \norm{P}_1 = \gamma \cdot V(P)$, where $\norm{P}_1 = V(P)$ is the entrywise $\ell_1$ norm.
% \end{proof}


\section{Baseline Edge Independent Models}
\label{sec:proposed}
\input{src/proposed.tex}

\section{Experimental Results}
\label{sec:exp}
\input{src/exp.tex}

\section{Conclusion}
Our theoretical results prove limitations on the ability of any edge independent graph generative model to produce networks that match the high triangle densities of real-world graphs, while still generating a diverse set of networks, with low model overlap. These results match empirical findings that popular edge independent models indeed systematically underestimate triangle density, clustering coefficient, and related measures. Despite the popularity of edge independent models, many non-independent models, such as graph RNNs \cite{YouYingRen:2018} have been proposed. An interesting future direction would be to study the representative power and limitations of such models, giving general theoretical results that provide a foundation for the study of graph generative models.

\clearpage
\bibliographystyle{plain}
\bibliography{neurips_2021}

\clearpage
\appendix

\section{Exact Embeddings in the CELL Model}

Recently, Rendsburg et al \cite{rendsburgnetgan} propose the CELL graph generator: a major simplification of the NetGAN algorithm for \cite{bojchevski2018netgan}, which gives comparable performance, much faster runtimes, and helps clarify the key components of  the generator. CELL uses a simple low-rank factorization model. Here we prove that, when its rank parameter is $k$, the CELL model can `memorize' any graph with degree bounded by $O(k)$. This allows the model to trivially produce distributions with very high expected triangle densities. However, as our main results show, this inherently requires memorization and high overlap. 

Our result can be viewed as an extension of the results of \cite{ChanpuriyaMuscoSotiropoulos:2020}, which considers a different edge independent model. The proof techniques are very similar.
Interestingly, our result seem to indicate that the good generalization of CELL in link prediction tests may mostly be due to the fact that this model is not fully optimized, to the point of memorizing the input. %It is not due to regularization or bounded expressiveness of the model itself.
%
% which seems to generate random networks with comparable properties to an seed input graph and exhibits good generalization in link prediction tests -- i.e., given a subset of training edges, the algorithm generates a graph which contains the remaining edges of the graph with good probability.

%Here we argue that any  generalization performance  of CELL is based on incomplete optimization. If the model is fully  optimized, it can produce a very low-dimension embedding that generates the true input graph with probability $1$, obviating its usefulness as a graph generator and any generalization properties.

%\todo{Insert more complete description of CELL.}

\noindent \textbf{The CELL Model.} We first describe the CELL model introduced in \cite{rendsburgnetgan}.
\begin{enumerate}
\item Given a graph adjacency matrix $A \in \{0,1\}^{n \times n}$, let
\begin{align}\label{eq:cell}
W^\star = \min_{\substack{W \in \R^{n \times n} \\\rank(W) \le k}}\quad \sum_{i,j=1}^n A_{ij} \log \sigma_{rows}(W)_{ij},
\end{align}
where $\sigma_{rows}(W)$ applies a softmax rowwise to $W$ -- ensuring that each row of $\sigma_{rows}(W)$ sums to $1$. 
\item Let $P^\star = \sigma_{rows}(W^\star)$ and let $\pi \in \R^n$ be the eigenvector satisfying $\pi^T P^\star = \pi^T$.
\item Let $P = \max(diag(\pi) P^*, (diag(\pi) P^\star)^T)$.
\item Generate $G \sim G(P)$.
\end{enumerate}
Note that the last step described above is slightly different than the approach taken in CELL. Rather than use an edge-independent model as in Def. \ref{def:ei}, they form $G$ by sampling edges without replacement, with probability proportional to the entries in $P$. They also insure that at least one edge is sampled adjacent to every node. However, this distinction is minor.

\noindent \textbf{Unconstrained Optimum.}
We first show that, if the rank constraint in \eqref{eq:cell} is removed, then the optimal $W^\star$ has $\sigma_{rows}(W^\star) = P^\star =  D^{-1} A$, where $D$ is the diagonal degree matrix. At this minimum, we can check that $\pi_i = d_i$, the degree of the $i^{th}$ node, and thus $diag(\pi)  = D$ and $P = A$. That is, the model simply outputs the input graph with probability $1$.  
\begin{theorem}[CELL Optimum]\label{thm:piecewise}
The unconstrained CELL objective function \eqref{eq:cell} is minimized when $\sigma_{rows}(W) = D^{-1} A$. At this minimum, the edge independent model $P$ is simply $A$. That is, the model just returns the input graph with probability $1$.
\end{theorem}
\begin{proof}
%\todo{Prove this. I am confident it should hold. Proof will first I think use the fact that $\log$ is convex? This should give that $\sigma_{rows}(W) = P$ is the unconstrained minimizer. Since basically in row $i$ you should want to balance the large values of the softmax to all be $1/d_i$. Proving that at the minimum $A^\dagger = A$ is just working through the steps of generating $A^\dagger$ when $\sigma_{rows}(W) = P$.}
It suffices to consider the $i^{th}$ row of $W$ for each $i \in [n]$, since the objective function of \eqref{eq:cell} breaks down rowwise. Let $w_i,a_i \in \R^n$ be the $i^{th}$ rows of $\sigma_{rows}(W)$ and $A$ respectively. Note that $w_i$ is a probability vector, with $w_i(j) \ge 0$ for all $j$ and $\sum_{j=1}^n w_i(j) = 1$.

We seek to minimize $\sum_{j=1}^n A_{ij} \log [w_i(j)].$ We need to show that this objective is minimized when $w_i = 1/d_i \cdot a_i$ -- i.e., when $w_i$ places mass $1/d_i$ at each nonzero entry in $a_i$ $1/d_i \cdot a_i$ is the $i^{th}$ row of $D^{-1}A$, so applying this argument to all $i$ gives that $\sigma_{rows}(W) = D^{-1} A$ is the overall minimizer. Assume for the sake of contradiction that there is some other minimizer $w^\star \neq 1/d_i \cdot a_i$. Since $\sum_{j=1}^n w^\star(j) = 1$, we must have $w^\star(j) \le 1/d_i$ for some $j$ where $a_i = 1$. In turn, there must be some $j'$ with either (1) $w^\star(j') \ge 0$ and $a_i(j') = 0$ or (2) $w^\star(j') \ge 1/d_i$ and $a_i(j') = 1$. In case (1), clearly moving $w^\star(j')$ mass from $j'$ to $j$ will decrease the objective function. In case (2), due to the concavity of the log function, moving $w^\star(j') - 1/d_i$ mass from $j'$ to $j$ will also decrease the objective function. Thus, $w^\star$ cannot be a minimizer, completing the proof.
%
%We know that $\sigma_{rows}(W)_{ij} \le 1$ for all $j$ and further that $\sum_{j=1}^n \sigma_{rows}(W)_{ij} = 1$. 
\end{proof}

\noindent \textbf{Rank-Constrained Optimum.}
We next show that the unconstrained optimum of $\sigma_{rows}(W) = D^{-1} A$, which leads to CELL memorizing the input graph (Thm. \ref{thm:piecewise}) can be achieved even with the rank constraint of \eqref{eq:cell}, as long as $k \ge 2\Delta+1$, where $\Delta$ is the maximum degree of the input graph. 
%We next show that we can achieve an essentially exact factorization.
\begin{theorem}[CELL Exact Factorization]\label{thm:exact}
If $A$ is an adjacency matrix with maximum degree $\Delta$, there is a rank $2\Delta+1$ matrix $W$ with 
$$\sigma_{rows}(W) = D^{-1}A + E$$
where $\norm{E}_2 \le \epsilon$. Note that  the rank of $W$ does not depend on $\epsilon$, and so we can drive $\epsilon \rightarrow 0$ and find a rank-$2\Delta+1$ $W$ which is arbitrarily close to minimizing \eqref{eq:cell} and thus produces $P$ which is arbitrarily close to $A$.
\end{theorem}
%
%Note that in the above conjecture, the rank of $W$ does not depend on $\epsilon$, and so we can drive $\epsilon$ arbitrarily small and find rank-$2\Delta$ $W$ with is arbitrarily close to minimizing \eqref{eq:cell} and thus produces $A^\dagger$ which is arbitrarily close to $A$. \todo{Maybe include a corollary on this?}
%
%Note that in the NetGAN Without GAN paper, the rank parameter generally tends to be chosen way higher than $2\Delta$. See Section C.8 here: \url{https://proceedings.icml.cc/static/paper_files/icml/2020/4540-Supplemental.pdf}. Thus assuming the Theorem is correct, it would imply that any generalization here is just due to not fully optimizing the objective.
%
\begin{proof}

Let $V \in \R^{n \times 2\Delta+1}$ be the Vandermonde matrix with $V_{t,j} = t^{j-1}$. For any $x \in \R^{2\Delta+1}$, $[Vx](t) = \sum_{j = 1}^{2\Delta+1} x({j}) \cdot t^{j-1}$. That is: $Vx$ is a degree $2\Delta$  polynomial evaluated at the integers $t = 1,\ldots,n$.

Let  $a_i$ be the $i^{th}$ row of $A$. Note that $a_i$ has at most $\Delta$ nonzeros whose positions we denote by $t_1,t_2,\ldots,t_{d_i}$.
To prove the theorem, for each row $a_i$, we will construct a polynomial $Vx_i$ which has the \emph{same positive value} at each $t_1,t_2,\ldots,t_{d_i}$ and is negative all all other integers $t$. Then, we will let $X \in \R^{2\Delta +1 \times n}$ be the matrix with columns $x_{i}$ and $W = (VX)^T $. Note that $\rank(W) \le 2\Delta+1$, and $W$ is equal to a fixed positive value whenever A is one and negative whenever it is zero. If we scale $W$ by a very large number (which does not affect its rank), we will have $\sigma_{rows}(W)$ arbitrarily close to $D^{-1} A$, since the rowwise softmax will place equal probability on each positive entry in row $i$ of $W$ and arbitrarily close to $0$ probability on each negative. So the row will exactly have $d_i$ at the nonzero entries of $a_i$, entries each equal to $1/d_i$.

It remains to exhibit the polynomial need to construct $W$. We start by constructing a polynomial of degree $2\Delta$ that is positive on each nonzero position $t_1,t_2,\ldots,t_{d_i}$ of $a_i$ and negative at all other indices. Later we will modify this polynomial to have the same positive value at each nonzero position of $a_i$. %. %As a warm-up, We first show that we can choose $x_i$ so that $sign(V x_i) = a_i$ where the $sign$ function is applied entrywise, and outputs $0$ for negative numbers, $1$ for non-negative numbers. 
%To do this we equivalently must find a polynomial which is positive at all integers $t$ with $a_i(t) = 1$ and negative at all $t$ with $a_i(t) = 0$.  Say $a_i$ is nonzero at positions $t_1,t_2,\ldots,t_{d_i}$.
 Let $r_{j,L}$ and $r_{j,U}$ be any values with $t_{j} -1 < r_{j,L} < t_j$ and $t_{j} < r_{j,U} < t_{j} +1$. Consider the polynomial with roots at each $r_{j,L}$ and $r_{j,U}$ -- this polynomial has $2 d_i \le 2\Delta$ roots and so degree at most $2\Delta$. It will flip signs just at each  $r_{j,L}$ and $r_{j,U}$, and will in fact have the same sign at  $t_1,t_2,\ldots,t_{d_i}$ (either positive or negative). Simply negativing the coefficients we can ensure that this sign is positive, while it is negative at all other indices, giving the result. % the sign to be positive, we have the result. %This argument gives Theorem 6 of \cite{chanpuriya2020node}.


The polynomial above can be written as $p(t) = \prod_{j=1}^{d_i} (t-r_{j,U}) (t-r_{j,L})$. Choose $r_{j,U} = t_j + \epsilon w_j$ and $r_{j,L} = t_j - \epsilon w_j$, where $\epsilon$ is arbitrarily small and $w_j$ is a weight chosen specifically for $t_j$ which we'll set later. We have for any $k = 1,\cdots, d_i$,
\begin{align*}
\lim_{\epsilon \rightarrow 0} \frac{p(t_k)}{\epsilon^2} &= \lim_{\epsilon \rightarrow 0} \frac{\prod_{j=1}^\Delta (t_k-t_j+\epsilon w_j) (t_k-t_j - \epsilon w_j)}{\epsilon^2}\\
& =  \lim_{\epsilon \rightarrow 0} \frac{- \epsilon^2 w_k^2 \cdot \prod_{j\neq k} (t_k - t_j)^2}{\epsilon^2}\\
& = -w_k^2 \cdot \prod_{j\neq k} (t_k - t_j)^2.
\end{align*}
This, if we set $w_k = \frac{1}{\prod_{j\neq k} (t_k - t_j)}$, in the limit as $\epsilon \rightarrow 0$ we will have $p(t_k)/\epsilon^2 = -1$. If we negate and scale the polynomially appropriately (which doesn't change its degree) we will have $p(t_k)$ arbitrarily close to one for each nonzero index $t_k$, and negative for each zero index. This gives the theorem.
\end{proof}

\end{document}
