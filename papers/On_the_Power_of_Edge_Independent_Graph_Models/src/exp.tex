% !TEX root = ../neurips_2021.tex

We now present our evaluations of different edge independent graph generative models in terms of the tradeoff achieved between overlap and performance in generating graphs with similar key statistics to an input network. %We also
%complement existing methods with our method from Section ~\ref{sec:proposed}. 
These experiments highlight the strengths and limitations of each model, as well as the overall limitations of this class, as established by our theoretical bounds.

%as we vary the edge overlap -- defined as the
%fraction of edges of the true network, also existing in the generated network.

\subsection{Methods}
We compare our proposed models from Section \ref{sec:proposed} with a number of existing models described below %\Dan{reorder?}.
\begin{enumerate}
\item \textbf{CELL \cite{rendsburgnetgan}} (Cross-Entropy Low-rank Logits) An alternative to the popular NetGAN method \cite{bojchevski2018netgan}
which strips the proposed architecture of deep leaning components and achieves comparable performance
in significantly less time, via a low-rank approximation approach. To control overlap, we follow the approach of the original paper, halting training once the generated graph exceeds a specified overlap threshold with the input graph. We set the rank parameter to a value that allows us to get up to 75\% overlap (typical values are 16 and 32).
\item \textbf{TSVD} (Truncated Singular Value Decomposition) A classic spectral method which computes a rank-$k$ approximation of the adjacency matrix using truncated SVD. As in \cite{SeshadhriSharmaStolman:2020}, the resulting matrix is clipped to [0,1] to yield $P$. Overlap is controlled by varying $k$.
\item \textbf{CCOP} (Convex Combination Odds Product) The odds product model as of Sec.~\ref{sec:proposed} with overlap controlled by taking a convex combination of $P$ and the input  adjacency matrix $A$.
\item \textbf{HDOP} (Highest Degree Odds Product) The odds product model, but with overlap controlled by fixing the edges adjacency to a certain number of the highest degree nodes. See Appendix for results on other variants, e.g., where some number of dense subgraphs are fixed. %, can be found in the Appendix. %\Cam{Its weird that we present one way to vary overlap in Sec 3 but then use a different way here. They should be made consistent.} 
%$\theta \cdot G + (1-\theta) \tilde{G}$, where $\tilde{G}$ is the outcome of the odds product matrix and $G$ is the original graph.
% -- with higher $k$ generally leading to higher overlap. % a probabilistic matrix that can be used
%\item \textbf{LPCA} \cite{ChanpuriyaMuscoSotiropoulos:2020} LPCA is an algorithm that generates low-dimensional exact embeddings of the
%adjacency matrix using gradient descent. Here, we fix the dimension to $16$ and stop the optimization procedure, when we meet a certain edge
%overlap threshold.
\item \textbf{Linear} The convex combination between the input adjacency matrix and an Erd\"{o}s-R\'enyi graph, as described in Sec. \ref{sec:proposed}, with overlap controlled by varying the $\omega$ parameter. %\Cam{In Sec. 3 this model is described as having 2 parameters, $\alpha$ and $\beta$. Can we describe in that section why, if you fix expected volume, really there is just one parameter to control. Maybe we should remove $\beta$ all together than just describe it as a convex combination between $A$ and $V(A)/n^2 * 11^T$.} % A linear baseline method where the adjacency matrix is down-scaled by a factor $\alpha, 0 < \alpha < 1$ (thus reducing
%the edge overlap), with the remaining mass being equally split among non-edges like an Erd\H{o}s-R\'enyi graph.
\end{enumerate}

CCOP, HDOP, and Linear all produce edge probability matrices $P$ with the same volume, $V(G)$, in expectation as the original adjacency matrix. For TSVD, letting $L$ be the low-rank approximation of the adjacency matrix, we learn a scalar shift parameter $\sigma$ using Newton's method
such that $P = \max(0,\min(1,L+\sigma))$ has volume $V(G)$.
We then generate new networks from the edge independent distribution $\mathcal{G}(P)$ (Def. \ref{def:ei}). %use this expected adjacency matrix to generate a new network by choosing each edge independently at random. 
For CELL, we follow the authors' approach of generating $V(G)$ edges without replacement - an edge $(i,j)$ is added with probability proportional to $P_{ij}$).

We sample 5 networks from each distribution and report the average for every statistic.

%\subsection{Varying overlap for each method}
%Our main goal is to understand the performance of the above edge-independent models with respect to their overlap. 
%%The scope of these experiments is to understand how edge-independent models perform, as they memorize a varying fraction of the original
%%graph. For this reason, we present key statistics for the graphs generated from these models as we vary the fraction of the edges of the
%%original graph that is memorized. 
%As these models are different by construction, the way we vary
%the overlap between the reconstructed and the original network is also different. For the CCOP, HDOP, and Linear  model we vary overlap , we do that by fixing an increasing
%number of high-degree nodes (and their connections). For CCOP the overlap is an increasing function of the parameter $\theta$.
%For CELL, we follow the approach of the original paper and we stop the training
%when the generated graph exceeds a specified overlap threshold. As TSVD is a clipped low-rank approximation
%of the original adjacency matrix, as we increase the rank the generated
%networks have in genneral an increasing overlap with the true network. Finally, the overlap for the Linear Baseline model is parameterized by $\alpha$.
  
\subsection{Datasets and network statistics}\label{sec:dsets_net_stats}
For evaluation, we use the following seven popular datasets with varied structure, from triangle-rich social networks to planar road networks:
%
\begin{enumerate}
\item \textsc{PolBlogs}: A collection of political blogs and the links between them.
\item \textsc{Citeseer}: A collection of papers from six scientific categories and the citations among them.
\item \textsc{Cora}: A collection of scientific publications and the citations among them.
\item \textsc{Road-Minnesota}: A road network from the state of Minnesota. Each intersection is a node.
\item \textsc{Web-Edu}: A web-graph drawn from educational institutions.
\item \textsc{PPI}: A subgraph of the PPI network for Homo Sapiens. Vertices represent proteins and edges represent interactions.
\item \textsc{Facebook}: A union of ego networks of Facebook users.
\end{enumerate}
%
See Table \ref{sample-table} for statistics about the networks. We treat all networks as binary, in that we set all non-zero weights to $1$, and undirected, in that if edge $(i,j)$ appears in the network, we also include edge $(j,i)$ . Also, we keep only the largest connected component of each network.

\begin{table}
  \caption{Dataset summaries}
  \label{sample-table}
  \centering
  \begin{tabular}{lrrr}
    \toprule
    Dataset  & Nodes           & Edges       & Triangles \\
    \midrule
    \textsc{PolBlogs}~\cite{adamic2005political} & 1,222 & 33,428 & 101,043\\
    \textsc{Citeseer}~\cite{sen2008collective} & 2,110  & 7,336 & 1,083 \\
    \textsc{Cora}~\cite{sen2008collective}     & 2,485 & 10,138 & 1,558 \\
    \textsc{Road-Minnesota}~\cite{nr} & 2,640 & 6,604 & 53 \\
    \textsc{Web-Edu}~\cite{gleich2004fast} & 3,031 & 12,948 & 10,058 \\
    \textsc{PPI}~\cite{stark2010biogrid} & 3,852 & 75,682 & 91,461 \\
    \textsc{Facebook}~\cite{leskovec2012learning} & 4,039 & 176,468 & 1,612,010 \\
    \bottomrule
  \end{tabular}
\end{table}

We evaluate performance in matching the following key network statistics:
\begin{enumerate}
\item Pearson correlation of the degree sequences of the input and the generated network.
\item Maximum degree over all nodes.
\item Exponent of a power-law distribution fit to the degree sequence.
\item Assortativity, a measure that captures the preference of nodes to attach to others with similar degree (ranging from -1 to 1).
\item Pearson correlation of the triangle sequence (number of triangles a node participates in).
\item Total triangle count (analyzed theoretically in Thm. \ref{thm:tri}).
\item Global clustering coefficient (defined in Def. \ref{def:cc} and analyzed theoretically in Thm. \ref{thm:cc}).
\item Characteristic path length (average path length between any two nodes).
\end{enumerate} 

\subsection{Results}
The theoretical results from Section \ref{sec:impossibility} highlight a key weakness of edge independent generative
models: they cannot generate many triangles (or other higher-order locally dense areas), without having high overlap and thus not generating a diversity of graphs. We observe that these theoretical findings hold in practice -- generally speaking, all models tested tend to significantly underestimate triangle count and global clustering coefficient, as well as inaccurately match the triangle degree sequence, when overlap is low.
See Figures~\ref{fig:polblogs_metrics}, \ref{fig:citeseer_metrics}, \ref{fig:cora_metrics}, \ref{fig:road-minnesota_metrics}, \ref{fig:web-edu_metrics}, \ref{fig:ppi_metrics}, and \ref{fig:facebook_metrics} for results on the tested networks.
As overlap increases, performance in reconstructing these metrics does as well, as expected.
%while results for the rest of the networks can be found in the Appendix.

All methods are able to capture certain network characteristics accurately, even at low overlap. 
%From the figures, we observe that in general, every method can accurately capture some key properties of a real-world network, but not all of
%them simultaneously, unless it memorizes (almost) exactly the input network.
Even for a relatively small overlap (less than 0.2), the CCOP  and HDOP methods accurately capture the degree sequences of the true networks
(as they are designed to do). % while the inclusion of the high degree nodes allows them to sometimes outperform more sophisticated methods (like CELL)
These methods, especially HDOP which fixes edges from high degree nodes, often outperform more sophisticated methods like CELL in terms of triangle density and triangle degree sequence correlation. On the other hand, CELL seems to do a somewhat better job capturing global features, like
the characteristic path length. TSVD provides a fair compromise --  it performs better than CELL in terms
of degree sequence and triangle counts, but worse in terms of characteristic path length. In general, it is the method that gives the best results when the overlap is extremely small, appearing to be less sensitive to the variation in overlap.

Broadly speaking, all methods do reasonably well in matching the power-law  degree distribution of the networks, even when they do not match the actual degree sequence closely. With the exception of \textsc{Web-Edu}, they tend to underestimate the characteristic path length. This is perhaps not surprising due to the independent random edge connections, however it would be interesting to understand more theoretically. 
%Broadly speaking, in addition to their inability to capture high triangle densities and clustering coefficients, 
%Summarizing, we see that most of the time, edge independent models tend to produce a smaller number of triangles than the one the true network
%has. This is more evident especially when we have a small overlap among the true and the reconstructed network. Also, while most models can easily
%capture the power-law degree distribution of the networks, this is not the case when we consider the Pearson correlation coefficient between the 
%triangle distribution of the true networks and the one of the reconstructed. Finally, the characteristic path length seems to be overly underestimated,
%especially for networks that have a large diameter (as in cora,citeseer or for a road network).

\begin{figure}
\centering
\includegraphics[width=1.0\linewidth]{polblogs_metrics_2x4}
\caption{Metrics for \textsc{PolBlogs}.}
\label{fig:polblogs_metrics}
\end{figure}

\begin{figure}
\centering
\includegraphics[width=1.0\linewidth]{citeseer_metrics_2x4}
\caption{Metrics for \textsc{citeseer}.}
\label{fig:citeseer_metrics}
\end{figure}

\begin{figure}
\centering
\includegraphics[width=1.0\linewidth]{cora_metrics_2x4}
\caption{Metrics for \textsc{cora}.}
\label{fig:cora_metrics}
\end{figure}

\begin{figure}
\centering
\includegraphics[width=1.0\linewidth]{road-minnesota_metrics_2x4}
\caption{Metrics for \textsc{road-minnesota}.}
\label{fig:road-minnesota_metrics}
\end{figure}

\begin{figure}
\centering
\includegraphics[width=1.0\linewidth]{web-edu_metrics_2x4}
\caption{Metrics for \textsc{web-edu}.}
\label{fig:web-edu_metrics}
\end{figure}

\begin{figure}
\centering
\includegraphics[width=1.0\linewidth]{Homo_sapiens_metrics_2x4}
\caption{Metrics for \textsc{PPI}}
\label{fig:ppi_metrics}
\end{figure}

\begin{figure}
\centering
\includegraphics[width=1.0\linewidth]{facebook_metrics_2x4}
\caption{Metrics for \textsc{Facebook}.}
\label{fig:facebook_metrics}
\end{figure}

\subsection{Code for Reproducing Results}
Code is available at \url{https://github.com/konsotirop/edge_independent_models}. Our implementation of the methods we introduce is written in Python and uses the NumPy~\cite{harris2020array} and SciPy~\cite{2020SciPy-NMeth} packages. Additionally, to calculate the various graph metrics, we use the following packages: powerlaw~\cite{alstott2014powerlaw} and MACE (MAximal Clique Enumerator)~\cite{takeaki2012implementation}.
