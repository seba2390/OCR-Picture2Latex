% !TEX root = ../neurips_2021.tex


%\algdef{SE}[DOWHILE]{Do}{doWhile}{\algorithmicdo}[1]{\algorithmicwhile\ #1}
%\Dan{TODOs:
%\begin{enumerate}
%	\item pseudocode for logit add model (done, Algorithm~\ref{alg:fitoddsproduct})
%	\item short derivation of gradient?
%	\item intuition for model, explanation of multiplying odds versus probabilities as in Chung-Lu
%	\item explanation of fixing high degree vecs?
%	\item explanation of baseline
%\end{enumerate}
%}

%Our theoretical results bound the trade-off achievable by any edge independent model between overlap and the ability to recreate properties of  real-world triangle dense grph of all 
We now shift from proving theoretical limitations of edge independent models to empirically evaluating the tradeoff between overlap and performance for a number of particular models.
%evaluating the empirical performance of particular edge independent generative models.
 Given an input adjacency matrix $A \in \{0,1\}^{n \times n}$, these generative models produce a $P \in [0,1]^{n \times n}$, samples from which should match various graph statistics of $A$, such as the triangle count, clustering coefficient, and assortativity. At the same time, $P$ should ideally have lower overlap so that the model does not %we assert that these $P$ should additionally have low overlap to avoid 
 just memorize the original graph.
We propose two simple generative models as baselines to more complicated existing models -- in both the level of overlap is easily tuned. %\Dan{Should there be a statement about how this performs well to preview the next section?}
Our first baseline, the \emph{odds product model}, is based on just matching the degree sequence of $A$; more simple still, the second baseline computes $P$ as a linear function of $A$, just matching its volume.

\noindent\textbf{Odds product model.}
In this model, each node is assigned a logit $\ell \in \mathbb{R}$, and the probability of adding an edge between nodes $i$ and $j$ is $P_{ij} = \sigma(\ell_i + \ell_j)$, where $\sigma$ is the logistic function. We fit the model by finding a vector $\bm{\ell} \in \mathbb{R}^n$ of logits, with one logit for each node, such that the reconstructed network has the same expected degrees (i.e. row and column sums) as the original graph.
We note that this model can be seen as a special case of the MaxEnt~\cite{de2011maximum} and inner-product~\cite{ma2020universal,hoff2003random,hoff2005bilinear} models. In the context of directed graphs, $\bm{\ell}_i$ has been called the expansiveness or popularity of node $i$~\cite{goldenberg2010survey}.

For adjacency matrix $A \in \{0,1\}^{n \times n}$, we denote its degree sequence by $\bm{d} = A \bm{1} \in \mathbb{R}^n$, where $\bm{1}$ is the all-ones vector of length $n$. Similarly, the degree sequence of the model is $\bm{\hat{d}} = P \bm{1}$. We pose fitting the model as a root-finding problem: we seek $\bm{\ell} \in \mathbb{R}^n$ such that the degree errors are zero, that is, $\bm{\hat{d}} - \bm{d} = \bm{0}$. We use the multivariate Newton-Raphson method to solve this root-finding problem. To apply Newton-Raphson, we need the Jacobian matrix $J$ of derivatives of the degree errors with respect to the entries of $\bm{\ell}$. Since $\bm{d}$ does not vary with $\bm{\ell}$, these derivatives are exactly $\tfrac{\partial \bm{\hat{d}}_i}{\partial \bm{\ell}_j}$.
%Letting $\delta_{ij}$ denote the Kronecker delta, with $\delta_{ij} = 1$ if $i=j$ and $\delta_{ij}=0$ otherwise,
%Letting $\delta_{ij}$ denote the Kronecker delta (i.e. $\delta_{ij}$ is $1$ if $i=j$ and $0$ otherwise),
Letting $\delta_{ij}$ be $1$ if $i=j$ and $0$ otherwise (i.e. the Kronecker delta),
%
\begin{align*}
\tfrac{\partial \bm{\hat{d}}_i}{\partial \bm{\ell}_j}
&= \tfrac{\partial}{\partial \bm{\ell}_j} \sum\nolimits_{k \in [n]} P_{ik} \\
&= \tfrac{\partial}{\partial \bm{\ell}_j} \sum\nolimits_{k \in [n]} \sigma(\bm{\ell}_i + \bm{\ell}_k) \\
&= \tfrac{\partial}{\partial \bm{\ell}_j} \sigma(\bm{\ell}_i + \bm{\ell}_j) + \delta_{ij} \sum\nolimits_{k \in [n]} \tfrac{\partial}{\partial \bm{\ell}_i} \sigma(\bm{\ell}_i + \bm{\ell}_k) \\
&= \sigma(\bm{\ell}_i + \bm{\ell}_j) \left( 1 - \sigma(\bm{\ell}_i + \bm{\ell}_j) \right) + \delta_{ij} \sum\nolimits_{k \in [n]} \sigma(\bm{\ell}_i + \bm{\ell}_k) \left( 1 - \sigma(\bm{\ell}_i + \bm{\ell}_k) \right) \\
&= P_{ij} \left( 1 - P_{ij} \right) + \delta_{ij} \sum\nolimits_{k \in [n]} P_{ik} \left( 1 - P_{ik} \right) .
\end{align*}
%
In Algorithm~\ref{alg:fitoddsproduct}, we provide pseudocode for computing the Jacobian matrix $J$ % in matrix form rather than entrywise 
and for implementing Newton-Raphson method to compute $P$. We do not have a proof that Algorithm~\ref{alg:fitoddsproduct} always converges and produces $\bm{\ell}$ which exactly reproduces in the inut degree sequence. However, the algorithm converged on all test cases, and proving that it always converges would be an interesting future direction.

\begin{algorithm}
    \caption{Fitting the odds product model}
    \label{alg:fitoddsproduct}
    \textbf{input} graphical degree sequence $\bm{d} \in \mathbb{R}^n$, error threshold $\epsilon$ \\
    \textbf{output} symmetric matrix $P \in (0,1)^{n \times n}$ with row/column sums approximately $\bm{d}$
    \begin{algorithmic}[1] % The number tells where the line numbering should start
	\State $\bm{\ell} \gets \bm{0}$ \Comment{\textcolor{gray}{ $\bm{\ell} \in \mathbb{R}^n$ is the vector of logits, initialized to all zeros }}
	\State $P \gets \sigma\left( \bm{\ell} \bm{1}^\top +  \bm{1} \bm{\ell}^\top \right)$ \Comment{\textcolor{gray}{ $\sigma$ is the logistic function applied entrywise, {\newline \hspace*{\algorithmicindent}} \hspace{175pt} and $\bm{1}$ is the all-ones column vector of length $n$}}
	\State $\bm{\tilde{d}} \gets P \bm{1}$  \Comment{\textcolor{gray}{ degree sequence of $P$ }}
	\While{ $\norm{ \bm{\tilde{d}} - \bm{d} }_2 > \epsilon$ }
		\State $B \gets P \circ \left( \bm{1} \bm{1}^\top  - P \right)$  \Comment{\textcolor{gray}{$\circ$ is an entrywise product}}
		\State $J \gets B + \text{diag}\left(B \bm{1} \right)$ \Comment{\textcolor{gray}{ $\text{diag}$ is the diagonal matrix with the input vector along its diagonal}}
		\State $\bm{\ell} \gets \bm{\ell} - J^{-1} \left(\bm{\tilde{d}} - \bm{d} \right)$ \Comment{\textcolor{gray}{rather than inverting $J$, we solve this  linear system}}
		\State $P \gets \sigma\left( \bm{\ell} \bm{1}^\top +  \bm{1} \bm{\ell}^\top \right)$
		\State $\bm{\tilde{d}} \gets P \bm{1}$
	\EndWhile
           \State \textbf{return} $P$
%        \EndProcedure
    \end{algorithmic}
\end{algorithm}	

%We now discuss some intuition behind this model. 
Our odds product model can be viewed as a variant of the Chung-Lu configuration model \cite{ChungLu:2002}, which is also based on degree sequence matching. However, but our model comes without a certain restriction on the maximum degree: in Chung-Lu, it is assumed that the degrees of all nodes are bounded above by the square root of the volume of the graph, that is, $\bm{d}_i \leq \sqrt{V(A)}$ for all nodes $i$. Given this restriction, each node is assigned a weight $\bm{w}_i = \bm{d}_i / \sqrt{V(A)}$, and the probability of adding edge $(i,j)$ is $P_{ij}=\bm{w}_i \bm{w}_j$. Since the weights are all in $[0,1]$, they can be interpreted as probabilities, and the probability of adding an edge between two nodes is the product of the two nodes' probabilities.

Our odds product model works similarly, but instead of a probability, for each node, there is an associated odds, that is, a value in $(0,\infty)$, and the odds of adding an edge between two nodes is the product of the two nodes' odds. There is a one-to-one-to-one relationship between probability $p \in [0,1]$, odds $o=\tfrac{p}{1-p} \in [0,\infty)$, and logit $\ell = \ln(o)\in (-\infty,+\infty)$. We outlined above how our model is based on adding logits associated with each node; since the odds is the exponentiation of the logit, the model can equally be viewed as multiplying odds associated with nodes. %For this reason, we call it the odds product model.

%\Dan{Somewhere above (not sure where or how), it should be mentioned that we don't have a proof that this works in terms of matching degree sequences, but empirically it seems to always work.}

\noindent\textbf{Varying overlap in the odds product model.} We propose a simple method to control the trade-off between overlap and accuracy in matching the input graph statistics in the odds product model. %We would like to produce a $P$ which both minimizes overlap and produces samples which closely match graph statistics of $A$. To manage this trade-off, the ability to control the overlap of $P$ is desirable. We propose a simple method of controlling overlap: 
Given the original adjacency matrix $A$ and the $P$ generated by the odds product model to match the degree sequence of $A$, we use a convex combination of $P$ and $A$. That is, we use $\tilde{P} = (1-\omega) P + \omega A$, where $0 \leq \omega \leq 1$. As $\omega$ increases to $1$, $\tilde{P}$ approaches a model which returns the original graph with high certainty; hence high $\omega$ produce $\tilde{P}$ with high overlap which closely match graph statistics, while low $\omega$ produce $\tilde{P}$ with lower overlap which may diverge from $A$ in some statistics.
Note that since $\tilde{P}$ is a convex combination of adjacency matrices with the expected degree sequence of $A$, $\tilde{P}$ also has the same expected degree sequence regardless of the value of $\omega$.

\noindent\textbf{Linear model.} 
 As an even simpler baseline, we also propose and evaluate the following model: we produce an Erd\"{o}s-R\'{e}nyi model $P$ with the same expected volume as the original graph $A$, then return a convex combination $\tilde{P}$ of $P$ and $A$. In particular, each entry of $P$ is $V(A) / n^2$, and, as with the odds product model, $\tilde{P} = (1-\omega) P + \omega A$, where $0 \leq \omega \leq 1$. This model can alternatively be seen as producing a $\tilde{P}$ by lowering each entry of $A$ which is $1$ to some probability $\alpha$, and raising each entry of $A$ which is $0$ to a probability $\beta$, with $\alpha \geq \beta$, such that the volume is conserved. %Note that entries of $\tilde{P}$ are simply a linear function of corresponding entries in $A$.
%As an even simpler baseline, we also propose and evaluate the following model: to produce the edge independent model $P$ given an original graph $A$, we lower each entry of $A$ which is $1$ to some probability $\alpha$, and raise each entry of $A$ which is $0$ to a probability $\beta$, such that the volume is conserved.
%In other words, edge $(i,j)$ appears in the graph with probability $\alpha$ if $(i,j)$ is in the original graph, or otherwise with a lesser probability $\beta$ if $(i,j)$ is not in the original graph.
%More precisely, $P = \beta + (\alpha - \beta)A$, where $0 \leq \beta \leq \alpha \leq 1$ and $V(P) = V(A)$; in particular, $\beta = (1-\alpha) V(A) / \left( n^2 - V(A) \right)$ produces a $P$ with the same expected volume as $A$. Note that in this model, the entries of $P$ are simply a linear function of corresponding entries in $A$. When $\alpha=1$, $P$ is exactly $A$ and the overlap is maximum; overlap can be reduced by lowering $\alpha$ until $P$ is the Erd\"{o}s-R\'{e}nyi  model with the same expected volume as $A$.