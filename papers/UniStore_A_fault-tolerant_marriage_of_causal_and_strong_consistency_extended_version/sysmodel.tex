\section{System Model}
\label{sec:sysmodel}

We consider a geo-distributed system consisting of a set of data centers
$\DC=\{1, \dots, D\}$ that manage a large set of data items. A data item is
uniquely identified by its \emph{key}. For scalability, the key space is split
into a set of logical partitions $\Partitions = \{1, \ldots, N\}$.  Each data
center stores replicas of all partitions, scattered among its servers.
We let $\partition^m_d$ be the replica of partition $m$ at data center $d$, and
we refer to replicas of the same partition as \emph{sibling} replicas.
As is standard, we assume that $D = 2f+1$ and at most $f$ data centers may fail.
We call a data center that does not fail {\em correct}. If a data center fails,
all partition replicas it stores become unavailable. For simplicity, we do not
consider the failures of individual replicas within a data center: these can be
masked using standard state-machine replication protocols executing within a
data center~\cite{smr,paxos}.

Replicas have physical clocks, which are loosely synchronized by a
protocol such as NTP. The correctness of \System does not depend on the
precision of clock synchronization, but large %
drifts may negatively impact its performance.
Any two replicas are connected by a reliable FIFO channel, so that 
messages between correct data centers are guaranteed to be %
delivered.
As is standard, to implement strong consistency we require the network to be
{\em eventually synchronous}, so that message delays between sibling replicas in correct
data centers are eventually bounded by some constant~\cite{psync}.



