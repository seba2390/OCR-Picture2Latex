\section{Related Work}
\label{sec:related}

\paragraph{Systems with multiple consistency levels.}
A number of data stores have combined weak and strong consistency,
including several commercial and academic systems that combine eventual and strong
consistency~\cite{cosmosdb,documentdb,pileus,google,dynamodb,cassandra,
tapir}.
Several academic data stores combined causal and strong consistency~\cite{lazy, red-blue, valter, por,
  walter, pileus}. Pileus~\cite{pileus} funnels all updates through a single
data center. In the fault-tolerant version of lazy
replication~\cite{lazy}, causal operations require synchronization
between replicas on its critical path. In both cases, causal
operations are not highly available, defeating the benefits of
causal consistency. Walter~\cite{walter} restricts causal operations to
a specific type and lacks fault tolerance due to the use
of two-phase commit across data centers.
The remaining works~\cite{red-blue, valter, por} support highly available causal
operations, but are not fault tolerant. First, they do not make causal
operations uniform on demand to guarantee the liveness of strong
operations. Thus, they suffer from the liveness issue we explained
in \S\ref{sec:overview} (Figure~\ref{fig:execution-strong}). In addition, these
systems do not use fault-tolerant mechanisms even for strong transactions.
They guard the use of strong transactions using mechanisms similar to locks;
if the lock holder fails before releasing it, no other data center can execute a
strong transaction requiring the same lock. This occurs even
when the service handing locks is fault-tolerant, as in~\cite{por}.
Finally, the above systems either do not include mechanisms for partitioning the
key space among different machines in a data center or include
per-data center centralized services, which
limits their scalability (\S\ref{sec:scalability}).




Some group communication systems mix causal and atomic
broadcast~\cite{isis,horus}. However, these systems do not provide mechanisms
for maintaining transactional data consistency.



Several papers have proposed tools that use formal verification technology
to ensure that consistency choices do not violate application
invariants~\cite{valter,cheng-tool,cise-tool,cise-popl16,suresh,sreeja}. Such
tools can make it easier for programmers to use our system.







\paragraph{Causal consistency implementations.}
Our subprotocol for causal consistency belongs to a family of highly scalable
protocols that avoid using any centralized components or dependency check
messages~\cite{gentlerain, cure, wren, paris, pocc}; other alternatives are
less scalable~\cite{cops, eiger, orbe, bolton, chainreaction, swiftcloud,
  saturn, eunomia, occult}. While we base our causal consistency subprotocol on
an existing one, Cure~\cite{cure}, we have extended it in nontrivial ways, by
integrating mechanisms for tracking uniformity (\S\ref{sec:clockcomputation})
and for transaction forwarding (\S\ref{sec:forward}). Some of the above
protocols~\cite{eunomia,paris} use hybrid clocks instead of real time~\cite{hlc}
to improve performance with large clock skews; this technique can also be
integrated into \System.


SwiftCloud~\cite{swiftcloud} implements
\emph{k-stability}~\cite{k-stability}, a notion similar to uniformity,
to enable client migration. SwiftCloud relies on a single per-data
center sequencer, which makes tracking k-stability easy, but the data store
less scalable. Our protocol is more sophisticated, since we distribute the
responsibility of tracking uniformity among the replicas in a data
center.









\paragraph{Paxos variants.}
Several Paxos variants~\cite{generalized,epaxos,atlas,generic} lower the latency
by allowing commutative operations to execute at replicas in arbitrary
orders. In contrast to them, \System implements PoR consistency, which allows
causal transactions to execute without any synchronization at all.






