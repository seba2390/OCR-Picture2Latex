In this paper, we make great steps in pixel-level video prediction by exploiting pose as an essentially free source of supervision and combining the advantages of VAEs, GANs and recurrent networks. Rather than try to model the entire scene at once, we predict the high level dynamics of the scene by predicting the pose movements of the humans in the scenes with a VAE and then predict each pixel with a GAN.
%In this paper we have explored the idea of using pose information for forecasting in video. We model the uncertainty of forecasting in a structured space---human pose, and then we leverage this structure to aid in generating pixel level videos. We combine the advantages of VAEs, GANs, and Recurrent Networks in our model. Our method is unsupervised and only relies on labels generated by a pose detector. 
We find that our method is able to generate a distribution of plausible futures and outperform contemporary baselines. There are many future directions from this work. One possibility is to combine VAEs with the power of structured RNNs to improve performance. Another direction is to apply our model to representation learning for action recognition and early action detection; our method is unsupervised and thus could scale to large amounts of unlabeled video data. 

\noindent{\bf Acknowledgements:}  We thank the NVIDIA Corporation for the donation of GPUs for this research.  In addition, this work was supported by NSF grant IIS1227495.

