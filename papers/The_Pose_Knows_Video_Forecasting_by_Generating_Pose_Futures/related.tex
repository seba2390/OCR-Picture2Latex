\noindent{\bf Activity Forecasting:} Much work in activity forecasting has focused on predicting future semantic action classes~\cite{Savarese14, Ma16, Hoai14} or more generally semantic information~\cite{Vondrick15, RhinehartK16a}. One way to move beyond semantic classes is to forecast an underlying aspect of human activity---human motion. However, the focus in recent work has been in specific data domains such as pedestrian trajectories in outdoor scenes~\cite{Kitani12, Savarese16b, Savarese16c} or pose prediction on human-labeled mocap data~\cite{Jain16,Fragikiadaki15}. In our paper, we aim to rely on as few semantic assumptions as possible and move towards approaches that can utilize large amounts of unlabeled data in unconstrained settings. The only assumption we make on our data is that there is at least one detectable human in the scene. While the world of video consists of more than humans, we find that the great majority of video data in computer vision research focuses on human actions ~\cite{Soomro12, Kuehne11, ActivityNet, Sports1m, THUMOS, Sigurdsson16}.

\noindent{\bf Generative Models:} Our paper incorporates ideas from recent work in generative models of images.
This body of work views images as samples from a distribution and seeks to build parametric models (usually CNNs) that can sample from these distributions to generate novel images.
%In these papers, the general assumption is that the visual world may be modeled a series of stochastic variables. For example, some variables may represent the type of objects, the size of them, their position, and more. If we are able to discover these stochastic variables, we may sample from them to generate novel images. 
Variational Autoencoders (VAEs) are one such approach which have been employed in a variety of visual domains. These include modeling faces~\cite{Kingma14a,Rezende14} and handwritten digits~\cite{Kingma14a,Salimans15}. Furthermore, Generative Adversarial Networks (GANs)~\cite{Goodfellow14, Radford15, Isola16, Denton15, Pathak16}, have shown promise as well, generating almost photo-realistic images for particular datasets. There is also a third line of work including PixelCNNs and PixelRNNs~\cite{Oord16a, Oord16b} which model the conditional distribution of pixels given spatial context.
%With these methods, the joint distribution of pixels is modeled directly. 
In our paper, we combine the advantages of VAEs with GANs. VAEs are inherently designed to estimate probability distributions of inputs, but utilizing them for estimating pixel distributions often leads to blurry results. On the other hand, GANs can produce sharp results, especially when given additional structure~\cite{Wang16, Isola16, Pathak16, Reed16}. Our VAE estimates a probability distribution over the more tractable space of pose while a GAN conditions on this structure to produce pixel videos.

\noindent{\bf Forecasting Video:} In the last few years there have been a great number of papers focusing specifically on data-driven forecasting in videos. One line of work directly predicts pixels, often incorporating ideas from generative models. Many of these papers used LSTMs~\cite{Srivastava15, Ranzato14, Patraucean16}, VAEs~\cite{Xue16}, or even a PixelCNN approach~\cite{Kalchbrenner16}. While these approaches work well in constrained domains such as moving MNIST characters, they lead to blurring when applied to more realistic datasets. A more promising direction for direct pixel prediction may be the use of adversarial loss~\cite{Vondrick16, Matthieu16}. These methods seem to yield better results for unconstrained, realistic videos, but they still struggle with blurriness and uninterpretable outputs.

Given the difficulty of modeling direct pixels in video, many~\cite{Yuen10, Pintea14, Walker15, Walker16, Brabandere16} have resorted to pixel motion for forecasting. This seems reasonable, as motion trajectories are much more tractable than direct pixel appearances. These approaches can generate interpretable results for short time spans, but over longer time spans they are untenable. They depend on warping existing pixels in the scene. However, this general approach is a conceptual dead-end for video prediction---all it can do is move existing pixels. These methods cannot model occluded pixels coming into frame or model changes in pixel appearance. 

Modeling low-level pixel space is difficult, and motion-based approaches are inherently limited. How then can we move forward with data-driven forecasting? Perhaps we can use some kind of intermediate representation that is more tractable than pixels. One paper~\cite{Walker14} explored this idea using HOG patches as an intermediate representation for forecasting. However, this work focused on specific domains involving cars or pedestrians and could only model rigid objects and rough appearances. In this paper, we use an intermediate representation which is now easy to acquire from video---human pose. Human pose is still visually meaningful, representing interpretable structure for the actions human perform in the visual world. It is also fairly low dimensional---many 2D human pose models only have 18 joints. Estimating a probability distribution over this space is going to be far more tractable than pixels. Yet human pose can still serve as a proxy for pixels. Given a video of a moving skeleton, it is then an easier task to fill in the details and output a final pixel video. We find that training a Video-GAN~\cite{Vondrick16} on completely unconstrained videos leads to results that are many times visually uninterpretable. However, when given prior structure of pose, performance improves dramatically. 