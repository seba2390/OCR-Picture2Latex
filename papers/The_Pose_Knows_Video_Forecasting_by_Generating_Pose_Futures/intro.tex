% New intro paragraph!
Consider the image in Figure~\ref{teaser}. Given the context of the scene and perhaps a few past frames of the video, we can infer what likely action this human will perform. This man is outside in the snow with skis. What is he going to do in the near future? We can infer he will move his body forward towards the viewer. Visual forecasting is a fundamental part of computer vision with applications ranging from human computer interaction to anomaly detection. If computers can anticipate events before they occur, they can better interact in a real-time environment. Forecasting may also serve as a pretext task for representation learning~\cite{DoerschThesis16, Vondrick16, Walker16}.

\begin{figure}
\begin{tabular}{ cc }
\includegraphics[height=0.14\textwidth,width=0.21\textwidth]{figures/teaser/frame.jpg} &
\includegraphics[height=0.14\textwidth,width=0.21\textwidth]{figures/teaser/firstPose.jpg} \\
{\footnotesize (a) Input Clip} &  {\footnotesize (b) Start Pose} \\
\includegraphics[height=0.14\textwidth,width=0.21\textwidth]{figures/teaser/secondPose.jpg} &
\includegraphics[height=0.14\textwidth,width=0.21\textwidth]{figures/teaser/finalPrediction.jpg} \\
{\footnotesize (c) Future Pose } & {\footnotesize (d) Future Video} \\
\end{tabular}  
\vspace{0.1in}
\caption{In this paper, we train a generative model that takes in (a) an initial clip with (b) a detected pose. Given this information, we generate different motions in (c) pose space using a Variational Autoencoder and utilize a Generative Adversarial Network to generate (d) pixels of the forecast video. Best seen in our \href{http://www.cs.cmu.edu/~jcwalker/POS/POS.html}{videos}.}
\label{teaser}
\vspace{-0.2in}
\end{figure}


\begin{figure*}
\centering
\includegraphics[trim={0.0in 1.5in 0.0in 1.0in},clip,width=6.5in, height=2.5in]{figures/Overview.pdf}
\vspace{-0.2in}
\caption{Overview of our approach. We use an LSTM, the Past Encoder, to encode the past input into a hidden state. We then input this hidden state into an LSTM with a Variational Autoencoder, the Future Decoder, which predicts future pose velocities based on random samples from latent variables. Given a rendered video of a pose video, we feed this with the input clip into an adversarially trained generator to output the final future video.}
\vspace{-0.2in}
\label{fig:Overview}
\end{figure*}

Given this goal of forecasting, how do we proceed? How can we predict events in a data-driven way without relying on explicit semantic classes or human-labeled data? In order to forecast, we first must determine what is active in the scene. Second, we then need to understand how the structure of the active object will deform and move over time. Finally, 
%on a low level, 
we need to understand how the pixels will change given the action of the object. All of these steps have a level of uncertainty; however, the second step may have far more uncertainty than the other two. In Figure~\ref{teaser}, we can already tell what is active in this scene, the skier, and given a description of the man's motion, we can give a good guess as to how that motion will play out at the pixel level. He is wearing dark pants and a red coat, so we would expect the colors of his figure to still be fairly coherent throughout the motion. However, the way he skis forward is fairly uncertain. He is moving towards the viewer, but he might move to the left or right as he proceeds. Models that either try to directly forecast pixels ~\cite{Srivastava15, Ranzato14, Xue16, Vondrick16, Matthieu16} or pixel motion~\cite{Walker15, Walker16, Finn16, Pintea14, Yuen10} are forced to perform all of these tasks simultaneously. What makes the problem harder for a complete end-to-end approach is that it has to simultaneously learn the underlying structure (what pixels move together), the underlying physics and dynamics (how the pixels move) and the underlying low-level rendering factors (such as illumination). Forecasting models may instead benefit if they explicitly separate the structure of objects from their low-level pixel appearance.  

The most common agent in videos is a human. In terms of obtaining the underlying structure, there have been major advances in human pose estimation~\cite{Cao17, Wei16, Chu17, Newell16} in images, making 2D human pose a viable ``free'' signal in video. In this paper, we exploit these advances to self-label video and aid forecasting. We propose a new approach to video forecasting by leveraging a more tractable space---human pose---as intermediate representation. Finally, we combine the strengths of VAE with those of GANs. The VAE estimates the probability distribution over future poses given a few initial frames. We can then forecast different plausible events in pose space. Given this structure, we then can use a Generative Adversarial Network to fill in the details and map to pixels, generating a full video. Our approach does not rely on any explicit class labels, human labeling, or any prior semantic information beyond the presence of humans. We provide experimental results that show our model is able to account for the uncertainty in forecasting and generate plausible videos.
 
 

%Given this goal of forecasting, how do we proceed? How can we predict events in a data-driven way without relying on explicit semantic classes or human-labeled data? An initial idea would be to predict in pixel space. Pixels are a free signal in video, and this approach can generalize to any domain of video data. There have been a number of approaches that work directly with pixels including LSTM-based approaches~\cite{Srivastava15, Ranzato14, Patraucean16}, VAEs~\cite{Xue16}, and GANs~\cite{Vondrick16, Matthieu16}. These approaches show promise in constrained domains such as moving MNIST characters~\cite{Srivastava15, Patraucean16} or video game sprites~\cite{Xue16}. However, when applied to more varied domains of video, these approaches forecast videos that are either heavily blurred or difficult to interpret. This is due to a number of factors. 
% Need some kind of structure
%First, some of these models are deterministic, forcing the model to average over multiple possible futures. However, even with stochasticity, results are still difficult to interpret visually. Generative modeling of pixels is still not a solved problem even for images~\cite{Radford15, Oord16b, Salimans16} let alone video. Other approaches try to forecast in the more tractable space of pixel motion~\cite{Walker15, Walker16, Finn16, Pintea14, Yuen10} or more generally image transformations~\cite{Brabandere16}. This approach is plausible for short time spans. However, blurring still occurs over long timespans, and motion can only warp existing pixels. Motion alone cannot model changes in object pose, occlusion, or the appearance of new objects. 

%An initial idea would be to predict in pixel space. Pixels are a free signal in video, and this approach can generalize to any domain of video data. There have been a number of approaches that work directly with pixels including LSTM-based approaches~\cite{Srivastava15, Ranzato14, Patraucean16}, VAEs~\cite{Xue16}, and GANs~\cite{Vondrick16, Matthieu16}. These approaches show promise in constrained domains such as moving MNIST characters~\cite{Srivastava15, Patraucean16} or video game sprites~\cite{Xue16}. However, when applied to more varied domains of video, these approaches forecast videos that are either heavily blurred or difficult to interpret. This is due to a number of factors. 
% Need some kind of structure
%First, some of these models are deterministic, forcing the model to average over multiple possible futures. However, even with stochasticity, results are still difficult to interpret visually. Generative modeling of pixels is still not a solved problem even for images~\cite{Radford15, Oord16b, Salimans16} let alone video. Other approaches try to forecast in the more tractable space of pixel motion~\cite{Walker15, Walker16, Finn16, Pintea14, Yuen10} or more generally image transformations~\cite{Brabandere16}. This approach is plausible for short time spans. However, blurring still occurs over long timespans, and motion can only warp existing pixels. Motion alone cannot model changes in object pose, occlusion, or the appearance of new objects. 

%In brief, these models are forced to perform a number of tasks simultaneously. First, they need to discover what is active in the scene, possibly from motion cues during training or testing. Second, they then need to understand how the structure of the active object will deform and move over time. Finally, for pixel prediction, they need to model how the pixels will change given the action of the object. All of these steps have a level of uncertainty; however, the second step may have far more uncertainty than the other two. Consider Figure~\ref{teaser}. We can already tell what is active in this scene, the bowler, and given a specific motion of the bowler, we can give a good guess as to how that motion will play out on the pixel level. He is wearing dark clothes, so we would expect his figure to still be fairly dark throughout the motion. However, the way he recovers from his bowl is fairly uncertain. He is going to stand up, but he might move to the left or right as he recovers. Forecasting models may benefit if they explicitly separate the structure of objects from their low-level pixel appearance.  
 