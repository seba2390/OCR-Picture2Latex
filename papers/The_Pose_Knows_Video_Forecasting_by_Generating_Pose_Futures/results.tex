\subsection{Qualitative Results}
In Figure~\ref{fig:qualitative} we show the qualitative results of our model. The results are best viewed as videos; we strongly encourage readers to look at our \href{http://www.cs.cmu.edu/~jcwalker/POS/POS.html}{videos}. In order to generate these results, for each scene we took 1000 samples from Pose-VAE and clustered the samples above a threshold into five clusters. The pose movement shown is the largest discovered cluster. We then feed the last input frame and the future pose movement into Pose-GAN to generate the final video. On the far right we show the last predicted frame by Pose-GAN. We find that our Pose-GAN is able to forecast a plausible motion given the scene. The skateboarder moves forward, and the man in the second row, who is jumproping, moves his arms to the right side. The man doing a pullup in the third row moves his body down. The drummer plays the drums, the man in the living room moves his arm down, and the bowler recovers to standing position from his throw. We find that our Pose-GAN is able to extrapolate the pixels based on previous information. As the body deforms, the general shading and color of the person is preserved in the forecasts. We also find that Pose-GAN, to a limited extent, is able to inpaint occluded background as humans move from their starting position. In Figure~\ref{fig:qualitative} we show a side-by-side qualitative comparison of our video generation to conditional Video-GAN. While Video-GAN shows compelling results when specifically trained and tested on a specific scene category~\cite{Vondrick16}, we discover that this approach struggles to generate interpretable results when trained on inter-class, unconstrained videos from the UCF101. We specifically find that~\cite{Vondrick16} fails to capture even the general structure of the original input clip in many cases. 

\subsection{Quantitative Results}

We show the results of our quantitative evaluation on pose prediction in Figure~\ref{fig:EuclideanErrorUCF101}. We find our method is able to outperform the baselines on Euclidean distance even with a small number of samples. The dashed lines for ERD and SRNN use the only the direct output as a mean---identical to sampling with variance 0. As expected, the Pose-VAE has a higher error with only a few samples, but as samples grow the error quickly decreases due to the stochastic nature of future pose motion. The solid lines for ERD and SRNN treat the output as a mean of a multivariate normal with variance derived from the testing data. Using the variance seems to worsen performance for these two baselines. This suggests that these deterministic baselines output one particularly incorrect motion for each of the examples, and the distribution of pose motion is not well modeled by Gaussian noise.  We also find our recurrent Pose-VAE outperforms Feedforward-VAE~\cite{Walker16}. Interestingly, FF-VAE underperforms the mean of the two deterministic baselines. This is likely due to the fact that FF-VAE is forced to predict all timesteps simultaneously, while recurrent models are able to predict more refined motions in a sequential manner. 

In Table~\ref{Inception} we show our quantitative results of pixel-level video prediction against~\cite{Vondrick16}. As the Inception score increases, the KL-Divergence between the prior distribution of labels and the conditional class label distribution given generated videos increases. Here we are effectively measuring how often the two stream action classifier detects particular classes with high confidence in the generated videos. We compute variances using bootstrapping. We find, not surprisingly, that real videos show the highest Inception score. In addition, we find that videos generated by our model have a higher Inception score than~\cite{Vondrick16}. This suggests that our model is able to generate videos which are more likely to have particular meaningful features detected by the classifier. In addition to Inception scores, we show the results of our MMD metric in  Table~\ref{MMD}. While Inception is measuring diversity, MMD is instead testing something slightly different. Given the distribution of two sets, we perform a statistical test measuring the difference of the distributions. We again compute a variance with bootstrapping.  We find that, compared to the distribution of real videos, the distribution videos generated by~\cite{Vondrick16} are much further than the videos generated by ours.

