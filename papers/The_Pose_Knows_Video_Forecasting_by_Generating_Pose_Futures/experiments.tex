We evaluate our model on UCF-101~\cite{Soomro12} in both pose space and video space. We utilized the training split described in~\cite{Walker15} which uses a large portion for training data. This split leaves out one video group for testing and uses the rest for training. In total we use around 1500 one-second clips for testing. To label the data we utilize the pose detector of Cao et al.~\cite{Cao17} and use the videos above an average confidence threshold. We perform temporal smoothing over the pose detections as a post-processing step.

\subsection{Pose Evaluation}

First we evaluate how well our Pose-VAE is able to forecast actions in pose space. There has been some prior work~\cite{Fragikiadaki15, Jain16} on forecasting pose in mocap datasets such as the H3.6M dataset~\cite{Ionescu14}. However, to the best of our knowledge there has been no evaluation on 2D pose forecasting on unconstrained, realistic video datasets such as UCF101. We compare our Pose-VAE against state-of-the-art baselines. First, we study the effects of removing the VAE from our Future Decoder. In that case, the forecasting model becomes a Encoder-Recurrent-Decoder network similar to~\cite{Fragikiadaki15}. We also implemented a deterministic Structured RNN model~\cite{Jain16} for forecasting with LSTMs explictly modeling arms, legs and torso.
Finally, we take a feed-forward VAE~\cite{Walker16} and apply it to pose trajectory prediction. In our case, the feed-forward VAE is conditioned on the image and past pose information, and it only predicts pose trajectories. 

\noindent{\bf Quantitative Evaluations:} For evaluation of pose forecasting, we utilize Euclidean distance from the ground-truth pose velocities. However, specifically taking the Euclidean distance over all the samples from our model in a given clip may not be very informative. Instead, we follow the evaluation proposed by ~\cite{Walker16}. For a set number of samples $n$, we see what is the best possible prediction made by the model and consider the error of closest sample from the ground-truth. We then measure how this minimum error changes as the sample size $n$ increases and the model is given more chances. We make our deterministic baselines stochastic by treating the output as a mean of a multivariate normal distribution. For these baselines, we derive the bandwidth parameters from the variance of the testing data. Attempting to use the optimal MLE bandwidth via gradient search led to inferior performance. We describe the possible reasons for this phenomenon in the results section.  

\subsection{Video Evaluation}

We also evaluate the final video predictions of our method. These evaluations are far more difficult as pixel space is much higher-dimensional than pose space. However, we nonetheless provide quantitative and qualitative evaluations to compare our work to the current state of the art in pixel video prediction. Specifically, we compare our method to Video-GAN~\cite{Vondrick16}. For this baseline, we only make two small modifications to the original architecture---Instead of a single frame, we condition Video-GAN on 16 prior frames. We also adjust the aspect ratio of the network to output a 64x80 video.

\noindent{\bf Quantitative Evaluations:} To evaluate the videos, we use the Inception score, first introduced in~\cite{Salimans16}. In the original method, the authors use the Inception model~\cite{szegedy2015going} to get a conditional label distribution for their generated images. In our case, we are generating videos, so we use a two-stream action classifier~\cite{LWang15} to get a conditional label distribution $p(y|x)$ where $x$ is our generated video and $y$ is the action class. We calculate the label distribution by taking the average classification output of both the rgb and flow stream in the classifier. As in~\cite{Salimans16}, we use the metric $\exp(\mathbb{E}_x KL(p(y|x)||p(y))$. In our case, our $x$ is generated from an input video sequence $fr$ and in some models a latent variable $z$, giving us the metric $\exp(\mathbb{E}_{fr, z} KL(p(y|x=G(f,z))||p(y))$. The intuition behind the metric is diversity; if a given classifier is highly confident of particular classes in the generated videos, then the Inception score will be large. If it has low confidence and is unsure what classes are in the videos, the conditional distribution will be close to the prior and the Inception score will be low.

We also propose a new evaluation metric based on the test statistic Maximum Mean Discrepancy (MMD)~\cite{Gretton14}. MMD was proposed as a test statistic for a two sample test---given samples drawn from two distributions $\textbf{P}$ and $\textbf{Q}$, we test whether or not the two distributions are equal.  

While the MMD metric is based on a two sample test, and thus is a metric for how similar the generated distribution is from the ground truth, the Inception score is a rather an ad hoc metric measuring the entropy of the conditional label distribution and marginal label distribution. We present scores for both metrics, but we believe MMD to be a more statistically justifiable metric

 \begin{figure}[t!]
\centering
\includegraphics[width=2.75in, height=2.0in]{figures/ucf1012.pdf} 
\caption{Here we show Minimum Euclidean Distance averaged over the testing examples. We take this nearest prediction in each example and plot the average of the error as the number of samples grows.}
\label{fig:EuclideanErrorUCF101}
\end{figure}

The exact MMD statistic for a class of functions $\mathcal{F}$ is:
\begin{equation}
\label{eqMMD}
MMD[\mathcal{F}, \textbf{P}, \textbf{Q}] = \underset{f \in \mathcal F}{\sup}(\mathbb{E}_{x \thicksim \textbf{P}}[f(x)]-\mathbb{E}_{y \thicksim \textbf{Q}}[f(y)]).
\end{equation}
Two distributions are equal if and only if for all functions $f \in \mathcal F$, $\mathbb{E}_x[f(x)]=\mathbb{E}_y[f(y)]$, so if $\textbf{P} \overset{d}{=} \textbf{Q}$, $MMD = 0$ where $\mathcal{F}$ is the set of all functions. Since evaluating over the set of all functions is intractable, we instead evaluate for all functions in a Reproducing Kernel Hilbert Space to approximate. We use the unbiased estimator for MMD from~\cite{Gretton14}.

Some nice properties of this test statistic are that the empirical estimate is consistent and converges in $O(\frac{1}{\sqrt n})$ where $n$ is the sample size. This is independent of the dimension of data \cite{Gretton14}. MMD has been used in generative models before, but as part of the training procedure rather than as an evaluation criteria. \cite{Dziugaite15} uses MMD as a loss function to train a generative network to produce better images. \cite{Li15} extends this by first training an autoencoder and then training the generative network to minimize MMD in the latent space of the autoencoder, achieving less noisy images.

We choose Gaussian kernels with bandwidth ranging from $10^{-4}$ to $10^9$ and choose the maximum of the values generated from these bandwidths as the reported value since from eq. (\ref{eqMMD}), we want the maximum distance out of all possible functions.

 Like Inception score, we use semantic features instead of raw pixels or flow for comparison. However, we use the $fc7$ feature space rather than the labels. We concatenate the $fc7$ features from the rgb stream and the flow stream of our action classifier.  This choice choice of semantic $fc7$ features is supported by the results in \cite{Li15} which show that training MMD on a lower-dimensional latent space rather than the original image space generates better looking images.
 
\begin{table}
\parbox{.45\linewidth}{
\centering
\captionsetup{justification=centering}
\caption{Inception Scores. Higher is better.}
    \begin{tabular}{|c|c|}
         \hline 
         Method & Inception\\ \hline
         Real & 3.81$\pm$ 0.04 \\ \hline
         Ours & 3.14$\pm$ 0.04\\ \hline
        ~\cite{Vondrick16} & 1.74$\pm$ 0.01\\ \hline
    \end{tabular}
    \label{Inception}
}
\hfill
\parbox{.45\linewidth}{
\centering
\captionsetup{justification=centering}
\caption{MMD Scores. Lower is better.}
    \begin{tabular}{|c|c|}
         \hline 
         Method & MMD\\ \hline 
         Real & 0.003$\pm$ 0.0003\\ \hline
        Ours & 0.022$\pm$ 0.0001\\ \hline
        ~\cite{Vondrick16} & 0.139$\pm$ 0.0001 \\ \hline
     \end{tabular}
     \label{MMD}
}
\end{table}
 
%\begin{table}[]
 %   \caption{Here we describe the Inception scores on different sets of videos. Real consists of the true testing videos, Ours represent our final generated pixel videos, and ~\cite{Vondrick16} represent videos generated by Video-GAN. Higher is better. {\color{red} UNOFFICIAL}}
%    \centering
%    \begin{tabular}{|c|c|}
%         \hline 
%         Method & UCF101\\ \hline
 %        Real & {\color{red} 14.63} \\ \hline
 %        Ours & {\color{red} 9.91} \\ \hline
 %       ~\cite{Vondrick16} & {\color{red} 3.24}\\ \hline
 %   \end{tabular}
 %   \label{tab:Inception}
%\end{table}
 
%\begin{table}[]
 %   \caption{Here we describe the Inception scores on different sets of videos. Real consists of the true testing videos, Ours represent our final generated pixel videos, and ~\cite{Vondrick16} represent videos generated by Video-GAN. Higher is better. {\color{red} UNOFFICIAL}}
%    \centering
%    \begin{tabular}{|c|c|}
%         \hline 
%         Method & UCF101\\ \hline
 %        Real & {\color{red} 14.63} \\ \hline
 %        Ours & {\color{red} 9.91} \\ \hline
 %       ~\cite{Vondrick16} & {\color{red} 3.24}\\ \hline
 %   \end{tabular}
 %   \label{tab:Inception}
%\end{table}

%\begin{table}[]
 %   \caption{Here we describe the MMD scores on different sets of %generated videos. Ours represent our final generated pixel videos, and ~\cite{Vondrick16} represent videos generated by Video-GAN. Lower is better. {\color{red} UNOFFICIAL}}
 %   \centering
 %   \begin{tabular}{|c|c|}
 %       \hline 
 %        Method & UCF101 \\ \hline
 %       Ours & {\color{red} 0.022}  \\ \hline
  %      ~\cite{Vondrick16} & {\color{red} 0.128} \\ \hline
  %  \end{tabular}
  %  \label{tab:MMD}
%\end{table}

\begin{figure*}[t!]
\centering
\begin{tabular}{ ccccc }
\includegraphics[height=0.12\textwidth,width=0.16\textwidth]{figures/skate/frame.jpg} &
\includegraphics[height=0.12\textwidth,width=0.16\textwidth]{figures/skate/firstPose.jpg} &
\includegraphics[height=0.12\textwidth,width=0.16\textwidth]{figures/skate/secondPose.jpg} &
\includegraphics[height=0.12\textwidth,width=0.16\textwidth]{figures/skate/finalPrediction.jpg} &
\includegraphics[height=0.12\textwidth,width=0.16\textwidth]{figures/skate/bad.png} \\
\includegraphics[height=0.12\textwidth,width=0.16\textwidth]{figures/jumprope/frame.jpg} &
\includegraphics[height=0.12\textwidth,width=0.16\textwidth]{figures/jumprope/firstPose.jpg} &
\includegraphics[height=0.12\textwidth,width=0.16\textwidth]{figures/jumprope/secondPose.jpg} &
\includegraphics[height=0.12\textwidth,width=0.16\textwidth]{figures/jumprope/finalPrediction.jpg} &
\includegraphics[height=0.12\textwidth,width=0.16\textwidth]{figures/jumprope/bad.png}
\\
\includegraphics[height=0.12\textwidth,width=0.16\textwidth]{figures/pullup/frame.jpg} &
\includegraphics[height=0.12\textwidth,width=0.16\textwidth]{figures/pullup/firstPose.jpg} &
\includegraphics[height=0.12\textwidth,width=0.16\textwidth]{figures/pullup/secondPose.jpg} &
\includegraphics[height=0.12\textwidth,width=0.16\textwidth]{figures/pullup/finalPrediction.jpg} &
\includegraphics[height=0.12\textwidth,width=0.16\textwidth]{figures/pullup/bad.png}
\\
\includegraphics[height=0.12\textwidth,width=0.16\textwidth]{figures/drummer/frame.jpg} &
\includegraphics[height=0.12\textwidth,width=0.16\textwidth]{figures/drummer/firstPose.jpg} &
\includegraphics[height=0.12\textwidth,width=0.16\textwidth]{figures/drummer/secondPose.jpg} &
\includegraphics[height=0.12\textwidth,width=0.16\textwidth]{figures/drummer/finalPrediction.png} &
\includegraphics[height=0.12\textwidth,width=0.16\textwidth]{figures/drummer/bad.png}
\\
\includegraphics[height=0.12\textwidth,width=0.16\textwidth]{figures/nunchuck/frame.jpg} &
\includegraphics[height=0.12\textwidth,width=0.16\textwidth]{figures/nunchuck/firstPose.jpg} &
\includegraphics[height=0.12\textwidth,width=0.16\textwidth]{figures/nunchuck/secondPose.jpg} &
\includegraphics[height=0.12\textwidth,width=0.16\textwidth]{figures/nunchuck/finalPrediction.jpg} &
\includegraphics[height=0.12\textwidth,width=0.16\textwidth]{figures/nunchuck/bad.png}
\\
\includegraphics[height=0.12\textwidth,width=0.16\textwidth]{figures/bowling/frame.jpg} &
\includegraphics[height=0.12\textwidth,width=0.16\textwidth]{figures/bowling/firstPose.jpg} &
\includegraphics[height=0.12\textwidth,width=0.16\textwidth]{figures/bowling/secondPose.jpg} &
\includegraphics[height=0.12\textwidth,width=0.16\textwidth]{figures/bowling/finalPrediction.jpg} &
\includegraphics[height=0.12\textwidth,width=0.16\textwidth]{figures/bowling/bad.png}
\\
{(a) Input Clip } & {(b) Input Pose} & {(c) Future Pose} & {(d) Our Forecast} & {(e)~\cite{Vondrick16} Forecast}\\
\end{tabular}  
\vspace{0.1in}
\caption{Here are some selected qualitative results from our model. Given an input clip (a) and a set of poses (b), we forecast a future pose motion (c) and then use this structure to predict video (d). These pose motions represent the largest cluster of samples from Pose-VAE for each input. Best seen in our \href{http://www.cs.cmu.edu/~jcwalker/POS/POS.html}{videos}.}
\label{fig:qualitative}
\vspace{-0.2in}
\end{figure*}

%\begin{figure*}[t!]
%\begin{tabular}{ p{25mm}p{25mm}p{25mm}p{25mm}p{25mm}p{25mm} }
%\includegraphics[height=0.12\textwidth,width=0.17\textwidth]{figures/teaser/frame.jpg} &
%\includegraphics[height=0.12\textwidth,width=0.17\textwidth]{figures/teaser/finalPrediction.jp%g} &
%\includegraphics[height=0.12\textwidth,width=0.17\textwidth]{figures/compare/skigan.jpg} &
%\includegraphics[height=0.12\textwidth,width=0.17\textwidth]{figures/bowling/frame.jpg} &
%\includegraphics[height=0.12\textwidth,width=0.17\textwidth]{figures/bowling/finalPrediction.jpg} &
%\includegraphics[height=0.12\textwidth,width=0.17\textwidth]{figures/compare/bowlgan.jpg} \\
%\includegraphics[height=0.12\textwidth,width=0.17\textwidth]{figures/jumprope/frame.jpg} &
%\includegraphics[height=0.12\textwidth,width=0.17\textwidth]{figures/jumprope/finalPrediction.jpg} &
%\includegraphics[height=0.12\textwidth,width=0.17\textwidth]{figures/compare/jumpropegan.jpg} &
%\includegraphics[height=0.12\textwidth,width=0.17\textwidth]{figures/nunchuck/frame.jpg} &
%\includegraphics[height=0.12\textwidth,width=0.17\textwidth]{figures/nunchuck/finalPrediction.jpg} &
%\includegraphics[height=0.12\textwidth,width=0.17\textwidth]{figures/compare/nunchucksgan.jpg} \\
%{\hspace{8mm} (a) Input } & {\hspace{9mm} (b) Ours} & {\hspace{9mm} (c) ~\cite{Vondrick16} }& {\hspace{8mm} (a) Input } & {\hspace{9mm} (b) Ours} & {\hspace{9mm} (c) ~\cite{Vondrick16} }\\
%\end{tabular}  
%\caption{Here is a side-by-side comparison of our method with the baseline~\cite{Vondrick16} {\color{red} UNOFFICIAL}}
%\label{fig:baseline}
%\end{figure*}
