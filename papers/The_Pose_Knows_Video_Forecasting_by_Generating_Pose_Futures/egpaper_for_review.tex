\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage{iccv}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{caption}
\usepackage[nocompress]{cite}

% Include other packages here, before hyperref.

% If you comment hyperref and then uncomment it, you should delete
% egpaper.aux before re-running latex.  (Or just hit 'q' on the first latex
% run, let it finish, and you should be clear).
%\usepackage[pagebackref=true,breaklinks=true,letterpaper=true,colorlinks,bookmarks=false]{hyperref}

 \iccvfinalcopy % *** Uncomment this line for the final submission

\def\iccvPaperID{1530} % *** Enter the ICCV Paper ID here
\def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}

% Pages are numbered in submission mode, and unnumbered in camera-ready
\ificcvfinal\pagestyle{empty}\fi
\begin{document}

%%%%%%%%% TITLE
\title{The Pose Knows: Video Forecasting by Generating Pose Futures}

\author{Jacob Walker, Kenneth Marino, Abhinav Gupta, and Martial Hebert\\
Carnegie Mellon University\\
5000 Forbes Avenue, Pittsburgh, PA 15213\\
{\tt\small \{jcwalker, kdmarino, abhinavg, hebert\}@cs.cmu.edu}}
\maketitle
%\thispagestyle{empty}


%%%%%%%%% ABSTRACT
\begin{abstract}
    Current approaches in video forecasting attempt to generate videos directly in pixel space using Generative Adversarial Networks (GANs) or Variational Autoencoders (VAEs). However, since these approaches try to model all the structure and scene dynamics at once, in unconstrained settings they often generate uninterpretable results. Our insight is to model the forecasting problem at a higher level of abstraction. Specifically, we exploit human pose detectors as a free source of supervision and break the video forecasting problem into two discrete steps. First we explicitly model the high level structure of active objects in the scene---humans---and use a VAE to model the possible future movements of humans in the pose space. We then use the future poses generated as conditional information to a GAN to predict the future frames of the video in pixel space. By using the structured space of pose as an intermediate representation, we sidestep the problems that GANs have in generating video pixels directly. We show through quantitative and qualitative evaluation that our method outperforms state-of-the-art methods for video prediction.
    
    %In trying to model all of the scene dynamics at once with no prior structure, these methods generate uninterpretable results on unconstrained and unsupervised videos. In this work, we exploit human pose detectors as a free source of supervision and break the video forecasting problem into two discrete steps. First we explicitly model the high level structure of active objects in the scene---humans---and use a VAE to model the possible future movements of humans in the scene. We then use the poses generated as conditional information to a GAN to predict the future frames of the video in pixel space. By using the structured space of pose as an intermediate step in the process, we sidestep the problems that GANs have of generating video directly.
    
    
    %Contemporary generative models of video forecasting focus on direct prediction of low level features such as pixels or motion. However, stochastically modeling the complex distribution of pixels is nontrivial, and motion information can only warp existing pixels. In this paper, we focus on unsupervised forecasting of videos using automatically generated human pose labels. We explicitly model the high level structure of active objects in the scene--in our case humans---and then use this structure to generate pixels. We first utilize a Variational Autoencoder (VAE) to model a probability distribution over future movements in human pose space. 
    %We then utilize a Generative Adversarial Network conditioned on pose to generate future video frames in pixel space. We find that our model is able to effectively forecast a distribution of plausible future videos given the context of the scene. 
\end{abstract}

%%%%%%%%% BODY TEXT
\section{Introduction}
\input{intro}
\section{Related Work}
\input{related}
%\section{Background on Generative Models}
%\input{background}
%\section{Video Pose GAN}
%\input{overview}
\section{Methodology}
\input{methods}
\section{Experiments}
\input{experiments}
\section{Results}
\input{results}
\vspace{-0.08in}
\section{Conclusion and Future Work}
\input{conclusion}

{\small
\bibliographystyle{ieee}
\bibliography{egbib}
}

\end{document}
