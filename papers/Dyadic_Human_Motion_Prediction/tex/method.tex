\section{Method}

Let us now introduce dyadic human motion prediction method for closely-interacting people. To this end, we first review the single person motion prediction formalism at the heart of our method, and then present our approach to modeling pairwise interactions to predict the future poses of two people. 

\subsection{Single Person Baseline}
\input{figures/hri_overview}
Our work builds on ``History Repeats Itself (HRI)"~\cite{Mao20}, which relies on an attention mechanism and a GCN to predict the future poses of a single person based on their observed sequence of historical poses. Intuitively, the attention mechanism aims to focus the prediction on the most relevant parts of the motion history and the GCN decodes the resulting representation into the future pose predictions while encoding the dependencies across the different joints. 

Formally, given a sequence of ${T}_p$ past 3D poses of an individual, $\textbf{X}_{1:{T}_p}=[\textbf{x}_{1}, \textbf{x}_{2}, ...,\textbf{x}_{{T}_p}]^T$, single-person human motion prediction aims to estimate the ${T}_f$ future 3D poses $\textbf{X}_{{T}_p+1:{T}_p+{T}_f}=[\textbf{x}_{{T}_p+1}, \textbf{x}_{{T}_p+2}, ...,\textbf{x}_{{T}_p+{T}_f}]^T$. Each pose $\textbf{x}_t \in \mathbb{R}^K$, where $K= J\times 3$, comprises $J$ joints forming a skeleton.
In HRI, the similarity between past motions and the last observed motion context is captured by dividing the motion history $\textbf{X}_{1:{T}_p}$ into ${T}_p-{T}_l-{T}_f+1$ sub-sequences $\{\textbf{X}_{t:t+{T}_l+{T}_f-1}\}_{t=1}^{{T}_p-{T}_l-{T}_f+1}$, each containing ${T}_l+{T}_f$ consecutive poses. The attention mechanism is then built by treating the first $T_l$ poses of every sub-sequence as key and the entire sub-sequence $\{\textbf{X}_{t:t+{T}_l+{T}_f-1}\}$ as value. In practice, the values are in fact represented in trajectory space as the Discrete Cosine Transform (DCT) coefficients of the corresponding poses. That is, the value of each subsequence is taken as $\{\textbf{V}_{t}\}_{t=1}^{{T}_p-{T}_l-{T}_f+1}$, where $\textbf{V}_{t} \in \mathbb{R}^{K \times ({T}_l+{T}_f)}$ encodes the DCT coefficients. Finally, the query corresponds to the last observed sub-sequence $\textbf{X}_{{T}_p-{T}_l+1:{T}_p}$ with $T_l$ poses. 

The query and keys are computed as the output of two neural networks $f_q$ and $f_k$, respectively. These functions map the poses to latent vectors of dimension $d$, that is, 
\begin{align}
	\textbf{q} &= f_q(\textbf{X}_{{T}_p-{T}_l+1:{T}_p}) \; , \label{eq:query_computation}\\
	\textbf{k}_{t} &= f_k(\textbf{X}_{t:t+{T}_l-1}) \; , \label{eq:key_computation}
\end{align}
where $\textbf{q}, \textbf{k}_{t} \in \mathbb{R}^d$ and $1 \leq t \leq {T}_p-{T}_l-{T}_f+1$.
A similarity score $a_{t}$ is then computed for each key-query pair, and these scores are employed to obtain a weighted combination of the values. This is expressed as
\begin{align}
	a_{t} = \frac{\textbf{q}\textbf{k}_{t}^T}{\sum_{j=1}^{{T}_p-{T}_l-{T}_f+1}\textbf{q}\textbf{k}_{j}^T} \; , \hspace{4mm}
	\textbf{U} = \sum_{t=1}^{{T}_p-{T}_l-{T}_f+1}a_{t} \textbf{V}_{t}, \label{eq:self_att}
\end{align}
where $\textbf{U} \in \mathbb{R}^{K \times ({T}_l+{T}_f)}$.
Then, the last observed sub-sequence is extended to a sequence of length ${T}_l+{T}_f$ by replicating the last pose and passed to the DCT module yielding $\textbf{D} \in \mathbb{R}^{K \times ({T}_l+{T}_f)}$. Finally, $\textbf{U}$ and $\textbf{D}$ are fed into the decoder GCN module, which outputs the future pose predictions $\hat{\textbf{X}}_{{T}_p+1:{T}_p+{T}_f}$. The attention module explained in this section is depicted in Fig.~\ref{fig:baseline_3dmotion_forecasting}, and will be referred to as self-attention in the rest of this paper, as it computes the attention of a single person on themselves.

\subsection{Pairwise Attention for Dyadic Interactions}

\input{figures/pairwise_overview}

Our goal is to perform motion predictions for multiple people. Formally, given the history of poses $\{\textbf{X}^{s}_{1:{T}_p}\}_{s=1}^{S}$ for $S$ subjects, our model predicts the future poses $\{\textbf{X}^{s}_{{T}_p+1:{T}_p+{T}_f}\}_{s=1}^{S}$. In particular, we focus on the case where $S = 2$ and aim to model the strong dependencies arising from the close interaction of the two subjects. As shown in Fig.~\ref{fig:overview_3dmotion_forecasting}, our approach combines self- and pairwise attention modules, and we refer to one person as the \emph{primary} subject and to the other as the \emph{auxiliary} one, denoted by the superscripts $1$ and $2$, respectively. Our goal then is to predict the future poses of the primary subject given the observed motions of both. Note that, to predict the future poses of the second subject, we simply inverse the roles.

To combine self- and pairwise attention, we first compute keys $\textbf{k}^{1}_{t}$ and query $\textbf{q}^{1}$ vectors for the primary subject as in Eqs.~\ref{eq:query_computation},~\ref{eq:key_computation}. The values $\textbf{V}^{1}_{t}$ together with $\textbf{k}^{1}_{t}$ and $\textbf{q}^{1}$ are then fed into the self-attention module, which yields ${\textbf{U}}^{1}$ as in Eq.~\ref{eq:self_att}. We then design a pairwise attention module that computes the similarity scores between the keys of the primary subject and the query of the auxiliary one, and vice-versa, to detect how relevant the coupled motion is at a given time in the past. A straightforward way of incorporating pairwise attention would consist of computing the auxiliary keys and query vectors directly from the observed motion of the auxiliary subject. However, as we show in the experiments, using the relative motion between the primary and auxiliary subject facilitates the modeling of interactions. Therefore, we compute the query, keys and values for the auxiliary subject as
\begin{align}
	\textbf{q}^{2} &= f_q(\textbf{X}^{1}_{{T}_p-{T}_l+1:{T}_p} - \textbf{X}^{2}_{{T}_p-{T}_l+1:{T}_p})  \; , \label{eq:pwise_query_computation}\\
	\textbf{k}^{2}_{t} &= f_k(\textbf{X}^{1}_{t:t+{T}_l-1} - \textbf{X}^{2}_{t:t+{T}_l-1}) \; , \label{eq:pwise_key_computation} \\
	\textbf{V}^{2}_{t}&= DCT(\textbf{X}^{1}_{t:t+{T}_l+{T}_f-1} - \textbf{X}^{2}_{t:t+{T}_l+{T}_f-1}) \; .\label{eq:pwise_value_computation}
\end{align}
We then define pairwise attention scores between the past motion of the primary subject and the relative motion with respect to the auxiliary one as
\begin{align}
	c^{12}_{t} = \frac{\textbf{q}^{2}{\textbf{k}^{1}}_{t}^T}{\sum_{j=1}^{{T}_p-{T}_l-{T}_f+1}\textbf{q}^{2}{\textbf{k}^{1}}_{j}^T} \; . \label{eq:pwise_scores}
\end{align}
This lets us compute a weighted sum of primary subject values as
\begin{align}
	\textbf{U}^{12} = \sum_{t=1}^{{T}_p-{T}_l-{T}_f+1}c^{12}_{t} \textbf{V}^{1}_{t} \;.
\end{align}
We also compute $\textbf{U}^{21}$ using $\textbf{V}^{2}_{t}$ and the pairwise scores  $c^{21}_{t}$ of $\textbf{q}^{1}$ and $\textbf{k}^{2}_{t}$. In the final stage of the encoder, we concatenate the pairwise embeddings ${\textbf{U}}^{12}$ and ${\textbf{U}}^{21}$ and feed them to a convolutional layer corresponding to the merge block in Fig.~\ref{fig:overview_3dmotion_forecasting}. The output is denoted as ${\textbf{P}}^{1}$. 

As for single-person prediction, the last observed sub-sequence of the primary subject is extended by repeating its last observed pose and transformed into DCT coefficients denoted by $\textbf{D}^{1}$. Our decoder then has two GCNs with shared parameters. One takes as input the concatenated matrices $\textbf{D}^{1}$ and $\textbf{U}^{1}$ and the other $\textbf{D}^{1}$ and $\textbf{P}^{1}$. Finally, the GCNs' outputs are projected via a convolutional layer to the future pose predictions of the primary subject. The same strategy is applied when exchanging the roles to obtain the future poses of the second subject.


\subsection{Training}
The entire network is trained by minimizing the Mean Per Joint Position Error (MPJPE). The loss for one training sequence is thus written as
\begin{equation}
\begin{split}
\textit{L} = \frac{1}{J({T}_l+{T}_f)}\sum_{t={T}_p-{T}_l+1}^{{T}_p+{T}_f}\sum_{j=1}^{J} ||\hat{\textbf{x}}_{t, j} - \textbf{x}_{t, j}||^{2}\;,
\end{split}
\end{equation}
where $\hat{\textbf{x}}_{t} \in \mathbb{R}^{3 \times J}$ encodes the estimated 3D pose for time $t$, $\textbf{x}_{t}$ represents the corresponding ground-truth pose, and $\textbf{x}_{t,j}$ denotes the 3D position of the $j$-th joint. 

\subsection{Implementation Details}

\parag{Training Details.} We train our network using the ADAM~\cite{Kingma15} optimizer with a learning rate of $0.0005$ and a batch size of $32$. We use $T_p = 60$ poses, corresponding to 2 seconds, as motion history  and predict $T_f = 30$ poses, corresponding to 1 second in the future. Our models are trained for 500 epochs, and we report the results of the model with the highest validation score.

\parag{Network Structure.} The networks $f_q$ and $f_k$ in the self- and pairwise attention modules consist of two 1D convolutional layers with kernel sizes $6$ and $5$, respectively, each followed by a ReLU. The hidden dimension of the query and key vectors in Eq.~\ref{eq:query_computation} and Eq.~\ref{eq:key_computation} is $256$. We use a GCN with $12$ residual blocks as in~\cite{Mao20}. The human skeleton has $J=19$ joints and our model has approximately $3.27M$ parameters similar to~\cite{Mao20} that has $3.26M$ parameters.

