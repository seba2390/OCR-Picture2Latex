\section{Experiments}

In this section, we demonstrate the effectiveness of our approach at exploiting dyadic interactions. To this end, we first introduce our \lindyhop{} dataset depicting couples that perform lindy hop dance movements.

\subsection{LindyHop600K}
Lindy hop is a type of swing dance with fast-paced steps synchronized with the music. It constitutes a good example of motions with strong mutual dependencies between the subjects, who are engaged in close interactions. To build this dataset, we filmed three men and four women dancers paired up in different combinations. Overall, \lindyhop{} contains nine dance sequences, each two to three minutes long, with a maximum of eight cameras at 60 fps. We use the shortest two sequences as validation and test sets. Table~\ref{table:seq_lhop} shows the details of the dataset organization. Our dataset displays standard lindy hop dancer positions and steps, such as the so-called open, closed, side and behind positions. In the open and closed positions, the dancers are facing each other with a varying distance between them. In the side position, both are facing the same direction, and in the behind position, the leader stands directly behind the follower, both facing the same direction. In each position, the dancers communicate through hand and shoulder grips. To the best of our knowledge, \lindyhop{} is the first large dance dataset involving the videos and 3D ground-truth poses of dancers.

\input{tables/lhop_dataset_structure}

To obtain the 3D poses of the dancers, we first extract 2D pixel locations of the visible joints using OpenPose~\cite{Cao17}. Because our dataset was captured with multiple cameras, this lets us obtain the  3D joint coordinates by performing a bundle adjustment using the 2D joint locations in all the views. However, this process comes with several problems because it requires annotating the poses of both subjects together. The major issues encompass body part confusions, missing 2D annotations and tracking errors in the OpenPose predictions, which occur when two people are very close to each other or wear similar garments. An example of this is shown in Fig.~\ref{fig:optimizing_3dposes}. To remedy this, we adopt a solution based on temporal smoothness. Specifically, we assign manually the 2D joint locations to each dancer in the first frame of each sequence. For the subsequent frames, the low confidence joint detections are replaced with ones interpolated using the high confidence joints from the neighboring frames. Despite these 2D joint corrections, the 3D locations extracted from the bundle adjustment procedure can still be very noisy. Thus, we employ a third degree spline interpolation across 30 frames coupled with an optimization scheme to generate the final 3D poses. Since the spline interpolation is done separately for each dimension of each joint, the length of each limb varies from one frame to another. To tackle this problem, we implement an optimization scheme which minimizes the squared difference between the length of a limb $c$ in the current frame and the average length of  limb $c$. We combine this loss function with additional regularizers penalizing feet from sliding on the floor, constraining the shape of the hips and shoulders, and preventing the optimization to the initial 3D pose estimates. For more detail, we refer the reader to the supplementary material.


\begin{figure}
	\centering
	\begin{tabular}{c}
		
		\includegraphics[width=0.67\linewidth]{figures/lindyhop_failure.pdf} \\
		(a) \footnotesize OpenPose 2D detection failure and the optimized 3D poses \\ \\
		\includegraphics[width=0.67\linewidth]{figures/lindyhop_success.pdf} \\
		(b) \footnotesize Correct OpenPose detections and the optimized 3D poses\\
	\end{tabular}
	\caption[Optimizing 3D poses in the \lindyhop{} dataset]{\textbf{Optimizing 3D poses in the \lindyhop{} dataset.} (a) Example of OpenPose 2D detection failure. The left leg of the woman is mapped to the left leg of the man. Our multi-view footage and refinement strategy allow us to obtain accurate 3D poses of the dancers despite the mismatch in the 2D detections. (b) Example of correct OpenPose detections and the optimized 3D ground truth poses.}
	\label{fig:optimizing_3dposes}
	\vspace{-4mm}
\end{figure}

\subsection{Data Pre-processing}
Each video sequence is first downsampled to 30 fps. The human body skeleton in the \lindyhop{} dataset originally comprises of $25$ body joints. We remove some of the facial, hand and foot joints and train our models with a skeleton of $19$ joints. The 3D joint locations are represented in the world coordinates. Since the position and orientation of the dancers change from one frame to another, we apply a rigid transformation to the poses.  We first subtract the global position of the hip center joint from every joint coordinate in every frame. Then, for each sequence, we take the first pose as  reference and rotate it such that the unit vector from the left to right shoulder is aligned with the $x$-axis and the unit vector from the center hip joint to the neck is aligned with the $z$-axis. We apply the same rotation to all the other poses in the sequence. 

\subsection{Results}

In this section, we evaluate our approach depicted by Fig.~\ref{fig:overview_3dmotion_forecasting} on our new \lindyhop{} dataset. We compare our method with the state-of-the-art single person approaches. They include HRI~\cite{Mao20}, which relies on an attention mechanism and a GCN decoder~\cite{Mao19} to predict the future poses of the individuals in isolation; HRI-Itr, which uses the output of the predictor as input and predicts the future motion recursively; TIM~\cite{Lebailly20}, which extends~\cite{Mao19} by combining it with a temporal inception layer to process the input at different subsequence lengths; and MSR-GCN~\cite{Lingwei21}, the most recent method, which extracts features from the human body at different scales by grouping the joints in close proximity. All the baselines rely on a GCN architecture that is trained and tested according to the data split shown in Table~\ref{table:seq_lhop}. They take as input a sequence of $60$ poses as  past motion. Except for HRI-Itr that recursively predicts $10$ poses at a time, all the baselines predict $30$ poses in the future. 

In Table~\ref{table:sota_lhop}, we report the MPJPE for short-term ($<$ 500ms) and long-term ($>$ 500ms) motion prediction in mm. Our method outperforms the baselines by a large margin. Fig.~\ref{fig:qualitative_lhop_sota} depicts qualitative results of our approach and the best performing three baselines for the \lindyhop{} test subjects with the corresponding follower and leader roles in the top two and bottom two portions, respectively. In contrast to the baselines, our method accurately predicts moves that are hard to anticipate in the long term, such as fast changing feet movements and less frequent arm openings. Although the observed motion of the primary subject does not include sufficient clues for such moves, the second person provides a useful prior so that our model can learn to predict the motion complementary or symmetric to that of the auxiliary subject. Therefore, we attribute this performance to our modeling of the motion dependencies via our pairwise attention mechanism. We provide additional qualitative results and further analysis on the learned pairwise attention scores in the supplementary material.

\input{figures/lhop_qualitative}

\input{tables/sota}


\subsection{Ablation Study}

\input{tables/ablation_interactions}

We evaluate the effect of modeling interactions via different strategies: \\
\textit{HRI-Concat} concatenates the motion history of the primary and auxiliary subject to treat them as one person. \\
\textit{Ours-SumPooling}, \textit{Ours-AvgPooling} and \textit{Ours-MaxPooling} discard the pairwise attention module, apply self-attention on the sequences of both subjects independently and combines the individual embeddings using the different pooling strategies proposed by~\cite{Adeli20}. The resulting vector is fed to the GCN decoder to predict the future poses of the primary subject. \\
\textit{Ours-w/oPairwiseAtt} excludes the pairwise attention module, applies self-attention and the GCN decoder on the sequences of both subjects independently and merges the GCN outputs from the two people to predict the future poses of the primary subject. \\
 \textit{Ours-w/o$\Delta$Pose} is our model which takes as input the past motion of the auxiliary subject directly instead of their relative motion to the primary subject.\\
 \textit{Ours-EarlyMerge} merges the pairwise embeddings $\textbf{U}^{12}$ and $\textbf{U}^{21}$ with the self-attention embedding of the primary subject $\textbf{U}^{1}$ before feeding them to the GCN module. \\
\textit{Ours-w/SelfAttAux} applies self-attention also on the sequence of the auxiliary subject and merges the result with the pairwise embeddings $\textbf{U}^{12}$ and $\textbf{U}^{21}$. \\
\textit{Ours-PairwiseAtt$\textbf{U}^{12}$ } excludes the pairwise attention that takes the keys and values from the auxiliary and the query from the primary subject. 
 

As can be seen in Table~\ref{table:ablation_study_lhop}, our method achieves the highest MPJPE in all timestamps. The comparison with \textit{HRI-Concat} shows that the naive way of combining the motion of the subjects is not an effective strategy to model their dependencies. The results of \textit{Ours-SumPooling}, \textit{Ours-AvgPooling} and \textit{Ours-MaxPooling} show that the social pooling layers proposed by~\cite{Adeli20} are suboptimal in the presence of strong interactions. The comparison to the remaining baselines evidence the benefits of the different components in our approach, which all contribute to the final results. 

\subsection{Limitations}
In Fig.~\ref{fig:qualitative_lhop_sota} and in the additional qualitative results, we observe that the lower arms and feet joints are usually difficult to predict and deviate the most from the ground-truth positions. Although Lindy Hop is a structured dance with highly correlated coupled motion, the dancers have their own styles. Therefore, predicting a single future is likely not to accurately match the body extremities which undergo the largest motion. This, however, can be overcome performing multiple diverse motion prediction, following a similar strategy to that used in~\cite{Yuan20,Aliakbarian21,Mao21b} for single-person motion prediction.

Another limitation of our model and many other motion prediction works in general is its use of complete sequences of ground-truth 3D poses as input. This may make our model sensitive to missing or faulty observations. To remedy this, as future work, we aim to incorporate the 3D poses obtained from the input images into our forecasting network and handle incomplete or noisy sequences to predict realistic future 3D poses for the interacting people.

