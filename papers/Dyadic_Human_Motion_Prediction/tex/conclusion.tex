\section{Conclusion}
We have designed an approach to exploiting dyadic interactions in 3D human motion prediction. In contrast to previous work, which focuses prediction on individual subjects, we have proposed to jointly reason about the observed poses of the two subjects engaged in a coupled motion. To this end, we have developed an encoder-decoder model that leverages self- and pairwise attention mechanisms to learn the mutual dependencies in the collective motion. To showcase the effectiveness of our model, we have introduced a new dataset, \lindyhop{}. To the best of our knowledge, this dataset is the first large dance dataset that provides the videos and 3D body pose annotations of dancing couples. We have shown that our approach outperforms the state-of-the-art single-person techniques and demonstrated that incorporating the interlinked motion of an auxiliary subject yields more accurate long term predictions for the primary subject. Our future work will focus on incorporating visual context in motion forecasting, and study interactions not only between two humans but also  with objects.