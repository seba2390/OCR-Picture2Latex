\section{Methodology}
In the following, we introduce our proposed NSLF model, that addresses the problem of limited view directions toward the surfaces.
Then we further design a Multiple Asynchronous Neural Agents (MANA) framework that learns the NSLF alongside the reconstruction.
The geometry regression that provides the surface relies on the existing incremental 3D reconstruction models. 
\vspace{-.3cm}
\subsection{NSLF Design}
In the use of capturing of 360 degrees of an object, for a certain surface point, the trained view directions are densely sampled over a large range and thus, the novel view inference is mostly on the touched view directions.
However, in large-scale 3D SLAM and reconstruction, this feature of data acquisition is not guaranteed. 
Therefore, training on a small range of view directions will lead to arbitrary prediction on unseen views.

\begin{figure}[t]%[htbp]
	\centering
	\psfragfig[width=1\linewidth]{im/eps/models}{
		\psfrag{p1}{$\V p$}
		\psfrag{d1}{$\V d$}
		\psfrag{p2}{$\V p$}
		\psfrag{d2}{$\V d$}
		\psfrag{f1}{$\phi_p$}
		\psfrag{f2}{$\varphi_{sh}$}
		\psfrag{f3}{$\phi_{p,1}$}
		\psfrag{f4}{$\phi_{p,2}$}
		\psfrag{f5}{$concat$}
		\psfrag{f6}{$\phi_{dec}$}
		\psfrag{f7}{$\phi_{w}$}
		\psfrag{f8}{$\phi_{mlp}$}
		\psfrag{f9}{$\varphi_{sh}$}
		\psfrag{f10}{$\phi_{\V w_p}$}
		\psfrag{F1}{$\V F_{\V p}$}
		\psfrag{F2}{$\V F_{\V d}$}
		\psfrag{F3}{$\V F$}
		\psfrag{F4}{$\V F_{\V p,1}$}
		\psfrag{F5}{$\V F_{\V p,2}$}
		\psfrag{F6}{$\V w_p$}
		\psfrag{F7}{$\V F_{sh}$}
		\psfrag{F8}{$s$}
		\psfrag{c1}{$\V c$}
		\psfrag{c2}{$\V c$}
		\psfrag{T1}{\footnotesize\textbf{Observation}}
		\psfrag{T2}{\footnotesize\textbf{Latent}}
		\psfrag{T3}{\footnotesize\textbf{Result}}
		\psfrag{l}{$($}
		\psfrag{r}{$)$}
}
\caption{Patterns of models designed for Neural Light Field. (a) is most widely used. 
	Our (b) learn one sphere for each $\V p$ and use a $\phi_{\V w_p}$ to map the 1D value on direction $\V d$ to 3D RGB.}
\label{fig:NSLF}
\vspace{-.6cm}
\end{figure}

Unlike previous encoding schemes that concatenate position and direction encoding, in \cref{fig:NSLF}, we propose to learn Spherical Harmonics (SH) parameters from the position encodings.
On the decoding side, we run deterministic formulas with a known SH basis.

To achieve the speedup from current techniques, we use the novel Multiresolution Hash Encoding~\cite{muller2022instant} to bear the main burden of position encoding:
$\V F_{\V p}=\phi_{p,\theta_{HG}}(\V p)$.
Such an encoding produces an encoding by interpolation of voxel features, which mitigates the global effect problem of MLPs in DSLF.
In addition, the surfaces occupy a small space in a cubic region, and thus the allocation during use is more efficient in space.
%
Considering that scene sequences only cover very limited direction space on surface surface, training on specific positions and directions easily leads to an arbitrary result in unseen view.
To prevent arbitrary guessing in the unseen direction, we introduce learning of SH parameters that affect the full direction.
We add an MLP-layer to generate the SH parameters: $\V F_{sh} = \phi_{mlp}(\phi_{\theta_{HG}}(\V p))=(v_\ell ^m)_{\ell: 0\leq \ell \leq \ell_{max}}^{m:\ell \leq m\leq \ell}$.
So here we are creating one sphere of latent to continuously represent a small space of $1$ dimensional color range for corresponding lights on a point. %colors RGB on the 3 spheres.
%
Then, with the known spherical harmonics function $Y_\ell ^m: \V S^2\rightarrow \mathbb C$, we extract the latent in a given direction of the sphere
utilizing the SH formula 
$\sum_{\ell =0}^{\ell _{max}}\sum_{m=-\ell }^{\ell } v^{m}_\ell  Y_\ell ^m(\V d)$ to deterministically decode on direction $\V d$:  $ s = \V F_{sh}^{T} \varphi_{sh}(\V d)$.
Meanwhile, the other branch of the encoder computes features $\V w_p$ to parameterize MLPs $\phi _{\V w_p}$. It maps (or extracts) the color range value $s$ to the color $\V c$.

Hash encoding requires normalizing the data into a unit cube~\cite{muller2022instant}. 
%By enlarging the resolution with scale $s$, hash encoding is applied to $s$ scale data. 
This means that we need to know the scale of the data.
Thus, using this model in each region avoids this problem as the regional scale is predefined.
In the following, we present a framework that operates on the growing scenes.

\subsection{NSLF with Region-wise Neural Agents}

DSLF~\cite{chen2018deep} uses MLPs to learn an entire scene.
But such a model is not robust for the incrementally creation of large scenes, because newly fed data still changes the scene globally even the non-effected parts. 
%
Some ideas in NeRF provide possible solutions:
(1) using voxels of multiple MLPs (KiloNeRF~\cite{reiser2021kilonerf}) or
(2) using a voxel multi-resolution representation (Instant-NGP~\cite{muller2022instant}).

\subsubsection{Multiple Asynchronous Neural Agents}
\begin{figure}[b!]
	\vspace{-.6cm}
	\centering
	%\includegraphics[width=.8\linewidth]{im/workflow.png}
	\psfragfig[width=1.\linewidth]{im/eps/workflow}{
		\psfrag{A}{{\color{white} Obtain Depth\&Color}}
		\psfrag{B}{\small Colored Point}
		\psfrag{B1}{\small Cloud (CPC)}
		\psfrag{C}{{\color{white}Tracking \& Mapping}}
		\psfrag{D}{{\color{white}Reconstruction}}
		\psfrag{E}{{\color{white}Poses}}
		\psfrag{F}{Frame $i$}
		\psfrag{G}{{\color{white}MANA}}
		\psfrag{H}{{\scriptsize\color{white}Neural Surface Light}}
		\psfrag{H1}{\scriptsize{\color{white} Fields (NSLF)}}
		\psfrag{I}{\normalsize \textbf{\textit {3D Reconstruction}}}
		\psfrag{J}{\normalsize \textbf{\textit {Online Learning}}}
		\psfrag{J1}{\normalsize\textbf{ \textit{of NSLF}}}
	}
	\caption{MANA learns online a NSLF by serving as an \textbf{external function} to 3D reconstruction.}
	\label{fig:workflow}
	%\vspace{-.6cm}
\end{figure}

We use RGBD and rasterization to directly approach points on the surface during training and testing respectively. 

To speed up training while making it scalable, we divide the space equally into regions and dynamically assign models for each newly touched region. 
Each region is assigned with an Intelligent Agent (IA) that maintains its own \textbf{thread}, \textbf{neural model} and \textbf{optimizer} and is trained autonomously.% as shown in~\cref{fig:pipeline}.
In training mode, when data is fed into an agent, it will train continuously until it reaches max-iterations.
In evaluation mode, the IA will predict input data and output colors. 
Since the implementation is done with neural network,
we will refer to such an IA as a Neural Agent (NA) in the rest of this paper. 

In addition, %as shown in \cref{fig:pipeline}, 
we assign an optimizer to each NA, making it an independent model that does the training itself.
Each region maintains its own neural model and optimizer to train independently.
With this feature, our model distinguishes itself from KiloNeRF which synchronizes voxels . 
And thus, our model is capable of asynchronous training.
We call it Multiple Asynchronous Neural Agents (MANA) with each NA $\V{U}=(\phi_\theta,Optim)$.

Then following KiloNeRF, we use an axis-aligned bounding box (AABB) to enclose the scene. We preset $\V b_{min}$ and $\V b_{max}$ as the minimum and maximum bound of AABB and discretize the space uniformly with the resolution $\V r=(r_x,r_y,r_z)$. We assign to each grid cell the index $\V i=(i_x,i_y,i_z)$ for the NA $\V{U}_{\V i}$. For growing scenes, we define extremely large box that cost almost nothing.

Given a RGBD frame, we unproject it to 3D space as a point cloud $\V P\in \mathbb{R}^{N_{\V P_j}\times 3}$ and their corresponding color $\V C\in \mathbb{R}^{N_{\V P}\times 3}$. The corresponding direction $\V D\in \mathbb{R}^{N_{\V P}\times 3}$ is also obtained for light fields.
%
Each point $\V p \in \V P$ is assigned to the corresponding region with 
\begin{equation}
v(\V p) = \lfloor (\V p - \V b_{min}) / (\V b_{max}-\V b_{min}) / \V r \rfloor.
\end{equation}
%
Then the color is predicted with the point $\V p_j \in \V P$ and its direction vector $\V d_j \in \V D$ as input:
\begin{equation}
\V c_{j,pred} = \phi_{\theta(v(\V p_j))}(\V p_j, \V d_j)
\label{eq:pred}
\end{equation}
Thus, for each agent $\V i$, it optimizes 
\begin{equation}
\min_{\theta(\V i)} \sum_{j,\ v(\V p_j)=\V i} || \V c_j -\V c_{j,pred}   ||^2_2
\end{equation}
independently with $\text{Optim}_{\V i}$.
\vspace{-.3cm}
\subsection{Online Learning of Surface Light Fields}
As indicated in \cref{fig:workflow}, our MANA works aside from the real-time 3D reconstruction of the model.
Note that, our model uses colored point clouds and poses sequentially from the reconstruction. 
But the training of MANA does not depend on the reconstruction result. 
The reconstruction mesh is only used during the testing, i.e., view rendering, where the surface is needed to determine the intersection points of rays.
In our implementation, the main thread is used to feed data into MANA and to perform the reconstruction.

We plot the diagram of MANA as a frame is fed into the pink window of \cref{fig:pipeline}.
\begin{figure}[t]
	\centering
	\psfragfig[width=1\linewidth]{im/eps/pipeline2}{
		\psfrag{F}{Frame $i$}
		\psfrag{p0}{$(\V P, \V D, \V C)$}
		\psfrag{pk}{\tiny{$(\V P, \V D, \V C)^{k}$}}
		\psfrag{p1}{\tiny{$(\V P, \V D, \V C)^{1}$}}
		\psfrag{NA}{{\color{magenta1}in Neural Agent for Region $k$}}
		\psfrag{l1}{While left iteration $a>0$}
		\psfrag{l2}{$(\V P, \V D, \V C_{gt})_j^k\leftarrow$ get\_data with \footnotesize{$j\in\{1,\ldots,i\}$} }
		\psfrag{l3}{stepTrain($(\V P, \V D, \V C_{gt})_j^k$, $\phi_{\theta^k}$,Optim$^k$)}
		\psfrag{l4}{$a\mathrel{-}=1$}
		\psfrag{D}{\footnotesize Distribute Data}
		\psfrag{MN}{{\color{magenta1}MANA}}
		\psfrag{nk}{\scriptsize Neural Agent for Region $k$}
		\psfrag{n1}{\scriptsize Neural Agent for Region $1$}	
	}
	\caption{Online Learning of NSLF. 
		(a) shows MANA distributing data into different Neural Agents by region. Each Neural agent maintains its thread and optimizes individually as (b).}
	\vspace{-.6cm}
	\label{fig:pipeline}
\end{figure}
\subsubsection{Distribution Module}

When a frame arrives, we generate a colored point cloud with direction $(\V P,\V C, \V D)$.
The distribution module feeds the points to their corresponding regions with their position. 
Together with the feed data, to ensure equal training of each region, our distribution module assigns iterations to less trained models, while setting zero iterations to more trained models. 

Since the distribution module and agents run in different threads,
the data distribution is sufficient for real-time data streaming while the training of the color models is optimized independently in the background.

\subsubsection{Asynchronous Neural Agents}

When training data of region $k$ is passed to agent $k$, it is appended to the data memory stack.
Each agent maintains its own thread to independently train the color field model $\theta$ with memory data.
As described in~\cref{fig:pipeline} (b), a thread of NA maintains the optimization of the neural model.
The newly distributed data for an agent is appended into its own memory stack. Meanwhile, left iterations {$a$} is modified for more iterations (in the experiments $a$ is set large enough such that it does not block the whole algorithm).
Then a signal is given to continue training.


\subsection{View Rendering}

After a complete online pass of the data stream, a learned NSLF and a surface are obtained.

By rasterizing with a surface mesh and camera pose as input, we obtain from the unprojected surface point $\V P$.
%
Rendering is then implemented by assigning points to different agents and synchronously predicting using~\cref{eq:pred}.

We show in the experiments, that in addition to the real-time training, the rendering is also done in real-time.

