\section{Results and Experiments}

Our experiments are on the scenario of \textbf{Real-time Incremental} sequences, which run the online learning modules (MANA) aside from reconstructing.

\subsection{Dataset}
\paragraph{ICL-NUIM}
%
ICL-NUIM~\cite{handa2014benchmark} is one of the most widely used RGBD datasets for SLAM and reconstruction purposes.
ICL-NUIM contains two synthetic scenes, i.e., room and office. 

\paragraph{Replica}
%
The Replica dataset~\cite{straub2019replica} provides synthetic indoor space reconstructions that contain clean dense geometry, high resolution, and dynamic textures.
It is suitable for our surface light field purpose.

\subsection{Baselines}

Our comparisons are mainly on \emph{online} learning of Neural Surface Light Fields given incremental reconstruction frames in real-time.
The reconstruction model that we work aside is a current SOTA, a large-scale real-time incremental reconstruction model, DiFusion~\cite{huang2021di}, which is based on Neural Implicit Maps.

First, we compare \emph{NSLF models} under our online learning framework (MANA) to demonstrate the advances of our proposed model.
The baselines are the recent Deep Surface Light Field models DSLF~\cite{chen2018deep}, AFFM~\cite{yu2022anisotropic}, and HashGrid (HG), an SLF model migrated from NeRF based on Instant-NGP~\cite{muller2022instant}.

Then we evaluate the model and framework \emph{as a whole} to compare the photometric performance with the latest incremental neural implicit reconstruction SOTA, NICE-SLAM~\cite{zhu2022nice}.


\input{tex/icl}

\subsection{Implementation Details}
\begin{table} [b]%[htbp]
\vspace{-.5cm}
  \caption {PSNR comparison on Replica sequences.} 
	\begin{adjustwidth}{0pt}{0pt}  
		\centering
		{\scriptsize
			\begin{tabular}{l|cccccccc}
				\hline
				& Ofc0
				& Ofc1
				& Ofc2
				& Ofc3
				& Ofc4
				& Rm0
				& Rm1
				& Rm2\\ \hline
				HG & 38.12&37.63&28.38&26.64&35.02&30.06	&33.51&34.37 \\
				\hline
				Ours 
				& 38.13
				& 37.85
				& 28.57
				& 26.89
				& 35.17
				& 30.20
				& 33.47
				& 34.35\\
				\hline
			\end{tabular} 
		}
		\label{tab:comp_rep1}
	\end{adjustwidth}
	
\end{table}
\begin{table}[b!]
	\vspace{-.5cm}
	\centering
	\setlength{\tabcolsep}{0.1em}
	\renewcommand{\arraystretch}{.01}
	\begin{tabular}{c |c}
		\hline
		{\large HG} &{\large \textbf{Ours}} \\\hline
		\includegraphics[width=.5\linewidth]{im/exp/unseen/icl/HG/3.png}
		&\includegraphics[width=.5\linewidth]{im/exp/unseen/icl/HG1SH/3.png}\\\hline\hline
		%\includegraphics[width=.5\linewidth]{im/exp/unseen/icl/HG/3.png}
		%&\includegraphics[width=.5\linewidth]{im/exp/unseen/icl/HG1SH/3.png}
		%\\ 
		
		\includegraphics[width=.5\linewidth]{im/exp/unseen/room0/HG_1.png}
		&\includegraphics[width=.5\linewidth]{im/exp/unseen/room0/HG1SH_1.png}\\	\hline
		
		\includegraphics[width=.5\linewidth]{im/exp/unseen/room2/HG_1.png}
		&\includegraphics[width=.5\linewidth]{im/exp/unseen/room2/HG1SH_1.png}\\	\hline\hline		
		
		%		\includegraphics[width=.5\linewidth]{im/exp/unseen/office1/HG_1.png}
		%		&\includegraphics[width=.5\linewidth]{im/exp/unseen/office1/HG1SH_1.png}\\	
		%		\includegraphics[width=.5\linewidth]{im/exp/unseen/office2/HG_1.png}
		%&\includegraphics[width=.5\linewidth]{im/exp/unseen/office2/HG1SH_1.png}\\
		\includegraphics[width=.5\linewidth]{im/exp/unseen/office3/HG_1.png}
		&\includegraphics[width=.5\linewidth]{im/exp/unseen/office3/HG1SH_1.png}\\
		\hline
	\end{tabular}
	\captionof{figure}{Demonstration on \textbf{unseen direction} of \textbf{observed surface}.}
	\label{fig:exp:icl_unseen}
\end{table}

The implementations of DSLF and AFFM are taken from \cite{yu2022anisotropic}.
For the HG baseline, we use a multi-resolution hash grid with \textsl{resolution}=512 to encode position, and spherical harmonics to encode direction~\cite{muller2022instant}.
The decoder operates on the concatenated feature using 4 layers of MLPs with \textsl{net-width}=32. The sigmoid at the end is used to normalize value in $[0,1]$ as normalized color.
Our model is also implemented with the same multi-resolution hash grid settings. Instead of encoding directions, we learn spherical harmonics parameters and extract a value directly in that direction.
Then, the extracted latent is operated on another MLPs with learned parameters at that position for color extraction.
For all models, learning rates are set to $lr=1e-3$ to train with the Adam optimizer. 

For online learning of growing scenes, we set the region scale to $4m$.
Our MAMA works alongside the recent 3D reconstruction models (DiFusion~\cite{huang2021di}) for reconstruction support.
The main thread processes the data and passes it to reconstruction and MANA.
In the MANA, the \texttt{data\_feeding} function in the main thread takes \unit[0.01]{s} for each frame.
The agents run asynchronously in their own threading.
On the sequence, we skip and take every 20th frame into DiFusion and MANA.
During the evaluation, we infer all frames.


For comparison, we use peak signal-to-noise ratio (PSNR), structural similarity index measure (SSIM), and Learned Perceptual Image Patch Similarity (LPIPS) to evaluate the produced image quality. 

Experiments run on a computer with an AMD Ryzen 9 5950X 16-Core Processor CPU and a GTX3090Ti GPU. 


\input{tex/surface_plus_light.tex}

\subsection{Online NSLFs alongside Real-time 3D Reconstruction}
\label{sec:test:online}
We implement MANA alongside Real-time 3D reconstruction for Online NSLF.
%FIXED: I recommend to put lrkt0n in \texttt{.}
We first run MANA and the reconstruction on the real-time sequence ICL-NUIM \texttt{lrkt0n}.
For a fair comparison, we should use the same mesh for all four models.
Since ICL-NUIM does not provide a ground-truth mesh, we pre-run DiFusion to obtain the same mesh for all four NSLF models.
Four NSLF models are embedded respectively into MANA.
Then, we sequentially feed those frames into MANA.
The real-time learning skips every 20 frames and the main thread sleeps \unit[1]{s} when every 20th frame arrives.
The evaluation is done on all frames. 

\cref{tab:comp_icl} shows the photometric quantitative evaluation. 
DSLF and AFFM perform much worse than HG and ours. 
Our model shows the best performance on all metrics.
We also find this in \cref{fig:exp:icl} that
both DSLF and AFFM show blur or incorrect image prediction. While
HG and ours show a more realistic quality on the painting and sofa.% compared to ground truth.

We find that HG has results, very similar to ours.
Therefore, we select HG and our model for further testing and investigation on the Replica dataset.
For the experiment on Replica which provides a ground truth mesh, to mitigate other effects, we use the ground truth mesh for prediction.
Similarly, MANA learns in real-time the sequences that skip every 20 frames and sleep \unit[1]{s} then.
In \cref{tab:comp_rep1}, HG and ours also provide very similar quality.

However, \cref{tab:comp_icl} and \cref{tab:comp_rep1} are compared on the whole sequence, which is very close to the selected trained frames.
To better demonstrate the advantage of our model, we show in \cref{fig:exp:icl_unseen} the unseen direction of the observed surface. 
For HG, these surfaces show the correct color when we capture them in the trained direction.
But we find that HG gives arbitrary results for the unseen direction of the observed surface.
For example, in the first row, HG lost the color of the painting and the sofa cushion.
In the second and third rows, HG shows arbitrary colors on the sofa and desk.
In the fourth row, HG incorrectly predicts the color of the carpet.
%
This problem happens to HG because the novel directions on those points are not learned.
%
While our model works fine.
Our model learns a sphere on each point instead of just for the observed directions.

To better support this, we further use object Chicken data from DSLF~\cite{chen2018deep}, which gives a dense view of the object.
We perform the experiment on the Chicken test set (200 frames). 
We train both HG and ours using a single frame (135th) for $1000$ iterations and render on all. 
The rendered video is demonstrated in our project page\footnote{\url{https://jarrome.github.io/NSLF-OL}}. Where HG shows high color bias when viewing angle changes while our model gives a better prediction.

From the video, most of the surface in the data is not trained. This means that quantitative evaluation of the whole sequence can hardly find the result differences.
Therefore, we compute the angle between the view directions of the inferences frame and the trained frame ($135$th). Then we threshold the angle to count only the frame result in a certain range.
In \cref{tab:chicken}, we use three options $\le 15^{\circ}$, $\le 30^{\circ}$, $\le 60^{\circ}$.
Where we find ours are always better. In addition, the large angle causes inference on the unlearned surfaces and will give a more similar score. Which is revealed by the smaller gap in the larger angle threshold.
\begin{table} [htbp]
	\caption {PSNR comparison on object sequences.}
	\centering
	\begin{tabular}{l|ccc}
		\hline
		& $\le 15^{\circ}$ & $\le 30^{\circ}$& $\le 60^{\circ}$ \\\hline
		HG&26.97&22.87&21.45\\
		Ours&27.63&23.13&21.55\\
		\hline
	\end{tabular}
	\label{tab:chicken}
\end{table}

\subsection{Comparing with Incremental Surface \& Color Reconstruction}
In the task of \emph{real-time incremental reconstruction}, surface and color are not necessarily decoupled.
For example, a recent Neural Implicit Reconstruction SOTA, NICE-SLAM provides high-quality surface and color reconstruction at the same time as online reconstruction. 
%
Therefore, we compare it to the \emph{SOTA of real-time incremental reconstruction} to demonstrate the advantages of our setting.
%
\cref{tab:comp_rep} shows the quantitative evaluation; we find that NICE-SLAM shows close performance to DiFusion combined with our MANA.
%
However, we find in \cref{fig:exp:fullmodel} that this is not the case. 
Our photometric result is more realistic compared to NICE-SLAM. 
The reason for this phenomenon is that DiFusion's reconstruction quality is worse than NICE-SLAM, MANA score is weakened.
For example, in the first two rows, DiFusion+MANA shows a clear plot on the wall painting while NICE-SLAM shows a very blurry result.
Also in the third and fourth rows, MANA shows fine texture on the quilt and carpet, while NICE-SLAM fails to find the detail.
However, DiFusion produces a worse reconstruction (shown in the edge of the tables and chairs), which causes defects to the NSLF rendering.

To better demonstrate the advantage of our work along the reconstruction, we use the same learned NSLFs to infer on NICE-SLAM's mesh.
This shows a better result in most of the scenes of \cref{tab:comp_rep}. The specific detail is also shown in the colored region of \cref{fig:exp:fullmodel}. 
\vspace{-.2cm}
\subsection{Time Efficiency}
Real-time reconstruction while online learning NSLF on ICL-NUIM \texttt{lrkt0n} (1508 frames) sequence in~\cref{sec:test:online} takes about \unit[28]{s}.
This means that our online learning framework incrementally trains this fixed amount of time.
Rendering a 480$\times$640 image in ICL-NUIM data takes on average \unit[0.07]{s}. %Considering the rendering speed is proportional to the resolution,
Therefore, our model easily achieves both real-time learning (training) and rendering (inference) in a similar size.