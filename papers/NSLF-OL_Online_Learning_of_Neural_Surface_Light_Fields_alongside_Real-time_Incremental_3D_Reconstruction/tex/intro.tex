
\section{Introduction}

% 3D understanding
\IEEEPARstart{I}{n} robotics, mapping, and 3D reconstruction have long been of great interest and have been studied for decades.
Initially, researchers focused on 3D point clouds and voxel grids, and this later shifted towards using Signed Distance Functions (SDF).
%
% SDF
An SDF, is the starting point of many of the following state-of-the-art papers.
It has been well-developed from Point-to-SDF~\cite{bylow2013real} to SDF-to-SDF~\cite{slavcheva2016sdf,yuan2022indirect}, from explicit voxel field~\cite{niessner2013real} to neural implicit representation~\cite{huang2021di,yuan2022algorithm}. 
These methods have achieved high-quality 3D reconstructions in real-time while maintaining high regression functionality of the neural models.

% recent neural rendering
Aside from the reconstruction of geometries, neural rendering for colors is also a hot topic, however more in the field of computer graphics~\cite{tewari2021advances}.
Neural rendering focuses on the synthesis of novel views from 3D models. 
The goal of this topic is to enhance the immersive experience for users. In the context of robotics, for instance, it is crucial for human-robot interaction or situation awareness of an operator of a telerobot. 
In 2020, the Neural Radiance Field (NeRF)~\cite{mildenhall2020nerf} was introduced as an innovative approach to high-resolution rendering for view synthesis, activating the trend of neural rendering.
It learns a neural radiance field that produces both an occupancy field and a light field via differentiable rendering for custom immersive view synthesis, and it has extended the testing scope of 3D reconstruction to large scene-level
~\cite{tancik2022block,zhang2022nerfusion}.
The drawback is the extremely high training time, and it is therefore unsuitable for real-time 3D reconstruction.
Nevertheless, it still affects the field of 3D reconstruction.
Recent works on 3D reconstructions, i.e., iMAP~\cite{sucar2021imap} and NICE-SLAM~\cite{zhu2022nice}, build on the differentiable rendering from NeRF to approach an online reconstruction with color.
However,
their visualization of color appears blurred.
\begin{figure}[t]%[htbp]
	\centering
	\psfragfig[width=1\linewidth]{im/eps/limited_light}{
		\psfrag{p}{$\V p$}
		\psfrag{d}{$\V d$}
		\psfrag{T1}{\textit{\footnotesize Capturing around object in}}
		\psfrag{Tn1}{\textit {\footnotesize Graphics Application}}
		\psfrag{T2}{\textit{\footnotesize Scene captures in}}
		\psfrag{Tn2}{\textit{\footnotesize SLAM \& Reconstruction}}
		\psfrag{T3}{\scriptsize\textbf{Observed Light}}
		\psfrag{T4}{\scriptsize\textbf{Unseen}}
	}
	\caption{Light difference between object capturing for graphics applications and scene capturing in robotic SLAM \& reconstruction. At 3D point $\V p$, lights are cast from direction $\V d$. The \textbf{sphere} shows the $\mathbf S^2$ space for the direction vectors $\V d$ that are partially covered by light rays.}
	\label{fig:limited_light}
	\vspace{-.5cm}
\end{figure}
% lighting field
It is worth mentioning that shape reconstruction has already been extensively studied and has already achieved high-quality 3D reconstruction performance. Aside from the high quality, the robotics community values online capable reconstructions.

However, NeRFs in the graphics community concentrate \emph{more} on (1) object capturing instead of capturing large scene surfaces (captures contain dense view-directions to surface), (2) rendering high-quality images while caring less on depth and surface accuracies (shape-radiance ambiguity)~\cite{zhang2020nerf++}, and (3) more on rendering (testing) speed instead of online training.
%
Therefore, we explore the high potential to use surface reconstruction research as the basis of this topic to meet the interest of the robotics community to have a universal way to simulate real environments.

We utilize a real-time reconstruction model to provide surfaces and simplify the problem to online learn the light field on surfaces \textbf{aside from} real-time reconstruction as given in \cref{fig:workflow}. Where our model relies on data from reconstruction without affecting it. During the online learning phase, we employ a colored point cloud. Subsequently, in the inference phase, we apply ray-rasterizing on the reconstruction mesh to obtain the point cloud for color prediction.

The Surface Light Fields (SLF) has been proposed by Wood et al.~\cite{wood2000surface} to model the surface reflectance.
For a surface point $\V p$, the radiance of a reflection ray (with direction $\V d_o$) is computed from an accumulation of incident rays (with direction $\V d_i$ for ray $i$).
However, due to the inefficient dense sampling requirement of SLF, more work has focused on modeling compression~\cite{miandji2013learning}. 
Recently, Chen et al.~\cite{chen2018deep} and Yu et al.~\cite{yu2022anisotropic} have introduced Deep Surface Light Fields (DSLF), which utilize a neural network to replace the previous handcrafted formulation.
%
Still, these approaches have an extremely slow training speed. 
%However, their training speed is extremely slow.
Similarly, these techniques are mainly concentrated on capturing a 360-degree view of the object.% with multi-view images.
Large-scale scenes have not been considered.

Unlike the graphics communities, in robotics, SLAM, and 3D reconstruction \emph{(1) capture scene frames that contain only a limited range of surface view-direction} as depicted \cref{fig:limited_light}, apply for \emph{(2) growing large-scale scenes without prior knowledge of their size} and require \emph{(3) real-time processing speed}.
These three points are the primary concerns for using Neural Surface Light Fields (NSLF) in robotics.
%
Thus, this paper aims to address the limited view-direction challenge during testing, where most unseen directions will cause arbitrary results.
To cope with this problem, we introduce Spherical Harmonics for \emph{decoding}, as depicted in~\cref{fig:NSLF}.
Furthermore, relying on recent advances in graphics, i.e., the Multi-resolution Hash-encoding, we train grid-latent as an encoder for (3) real-time online learning.
%
We further introduce Multiple Asynchronous Neural Agents (MANA) to handle (2), i.e., we handle large scenes without pre-knowing the size. 

We take inspiration from~\cite{reiser2021kilonerf} which claims that multiple local MLPs converge faster. 
However, \cite{reiser2021kilonerf} has to train all local MLPs with one optimizer because NeRF's differentiable rendering accumulates data from different MLPs. 
While Surface Lighting Field training does not depend on ray integration in NeRF. Therefore, each region has its model and optimizer to learn individually.

To deal with scalable data when data is distributed, we dynamically allocate neural models and optimizers for new regions and run them in a new thread independently as depicted in \cref{fig:workflow}.
Every region has a neural model and an optimizer running independently without synchronizing with others.

The contributions of this paper are as follows:
\begin{itemize}
\item
  Proposing a novel Neural Surface Light Fields model to address the issue of arbitrary prediction on unseen direction causing from the small range of view-direction to surface in SLAM \& 3D reconstruction captures,
\item
  Proposing the first framework (MANA) for online-learning neural surface light fields on growing large-scale scenes,
\item
  Implementing MANA aside from real-time reconstruction for experiments.
  Both online learning and real-time testing are supported.
  Agents can be distributed to multiple GPUs.
\end{itemize}

