%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%2345678901234567890123456789012345678901234567890123456789012345678901234567890
%        1         2         3         4         5         6         7         8

% Max: 6 MB
% EPS images must use TrueType 42 font for compliance. No title, clutter.

\documentclass[letterpaper, 10 pt, conference]{ieeeconf}  % Comment this line out if you need a4paper

%\documentclass[a4paper, 10pt, conference]{ieeeconf}      % Ushttps://preview.overleaf.com/public/qyztskmptpnr/images/34be97b6cdc7e4a35380ead3b729b2d7e7ce14b2.jpege this line for a4 paper

\IEEEoverridecommandlockouts                              % This command is only needed if 
                                                          % you want to use the \thanks command

\overrideIEEEmargins                                      % Needed to meet printer requirements.

% See the \addtolength command later in the file to balance the column lengths
% on the last page of the document


% XXX: Fix ieeeconf BS:
\let\labelindent\relax
\let\proof\relax
\let\endproof\relax

% RA-L instructions say to do this if you submit your own PDF, otherwise PDF will be corrupt
\pdfminorversion=4

\usepackage[normalem]{ulem}	                        % underlining!
\usepackage[table,usenames,dvipsnames]{xcolor}      % color
\usepackage{extarrows}                              % http://ctan.org/pkg/extarrows
\usepackage{enumitem}

\usepackage{amsmath,amsthm,amsfonts,amssymb,amscd, dsfont}
%\usepackage{algorithm,algorithmicx,listings}        % algorithms
%\usepackage[noend]{algpseudocode}

% http://mirrors.acm.jhu.edu/ctan/macros/latex/contrib/algorithm2e/doc/algorithm2e.pdf
% http://tex.stackexchange.com/questions/204592/how-to-format-a-pseudocode-algorithm
\usepackage[linesnumbered,ruled,noend]{algorithm2e}  % Looks good for English pseudocode
% http://tex.stackexchange.com/questions/162207/algorithm2e-comment-style
\newcommand\mycommfont[1]{\footnotesize\ttfamily{#1}}
\SetCommentSty{mycommfont}

\usepackage[font=small]{caption}

% The following packages can be found on http:\\www.ctan.org
\usepackage{graphics} % for pdf, bitmapped graphics files
\usepackage{epsfig} % for postscript graphics files
%\usepackage{mathptmx} % assumes new font selection scheme installed
%\usepackage{times} % assumes new font selection scheme installed
%\usepackage{amssymb}  % assumes amsmath package installed
%\usepackage{dsfont} % for mathds expected value symbol

\usepackage{comment}

% Subfigure captions http://tex.stackexchange.com/questions/181225/how-to-reference-to-subfigure-in-latex
\usepackage{subcaption}
%\usepackage{cleveref}
\captionsetup[subfigure]{font=footnotesize, subrefformat=simple,labelformat=simple,position=top}
% font=footnotesize looks exactly the same as the default ieeeconf setup. Packages subcaption and cleveref mess with it and cancel it, so have to do it manually
\renewcommand\thesubfigure{(\alph{subfigure})}
% http://latex-community.org/forum/viewtopic.php?f=5&t=114
% This breaks equation labels!!! Don't uncomment this line!!! Equation labels all show as ??
%%\captionsetup{font=footnotesize}

% http://tex.stackexchange.com/questions/135384/making-table-width-fit-into-text-width
\usepackage{tabulary}
% http://tex.stackexchange.com/questions/8549/how-can-i-draw-a-horizontal-line-spanning-only-some-of-the-table-cells
\usepackage{array,multirow}
\usepackage[colorlinks]{hyperref}

% Put table caption on same line as TABLE I text
%  http://tex.stackexchange.com/questions/25755/avoiding-newline-after-table-in-caption-while-using-ieeetran
%\usepackage{caption}% http://ctan.org/pkg/caption
%\captionsetup[table]{format=plain,labelsep=period,labelfont={sc}}%
%\captionsetup[table]{format=bsc,labelsep=period}%
%\captionsetup[figure]{font={small}}


% Shrink white space padding below figures 
%   http://tex.stackexchange.com/questions/23313/how-can-i-reduce-padding-after-figure
%\setlength{\belowcaptionskip}{-6pt}
% Good for figure captions, but shifts up table captions too much
%\setlength{\abovecaptionskip}{-1pt}

% Shrink white space above footnotes
%   http://stackoverflow.com/questions/783716/footnote-spacing-in-latex
\setlength{\skip\footins}{2mm}


\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\argmax}{arg\,max}

\newcommand{\TODO}[1]{{\color{red}TODO: #1}}
\newcommand{\red}[1]{\textcolor{red}{#1}}
\newcommand{\NA}[1]{\textcolor{red}{$\diamondsuit$}\footnote{\color{red}Nikolay: #1}}

\newcommand{\meshheight}{2cm}
\newcommand{\pcdheight}{1.6cm}
\newcommand{\graspheight}{1.7cm}

\newtheorem{theorem}{Theorem}
\newtheorem{proposition}{Proposition}
\newtheorem{corollary}{Corollary}
\newtheorem{definition}{Definition}
\newtheorem{assumption}{Assumption}
\newtheorem*{assumption*}{Assumption}
\newtheorem{remark}{Remark}
\newtheorem*{remark*}{Remark}
\newtheorem*{problem*}{Problem}
\newtheorem{problem}{Problem}
\newtheorem{lemma}{Lemma}


% Title should clearly reflect problem. If don't focus on recognition, then state the problem as active selection
\title{\LARGE \bf
Active End-Effector Pose Selection for Tactile Object Recognition through Monte Carlo Tree Search
}
%Active Tactile Object Recognition by Monte Carlo Tree Search
% or "Interactive Perception" for Object Recognition? Apparently interactive perception != active perception


\author{Mabel M. Zhang, Nikolay Atanasov, and Kostas Daniilidis% <-this % stops a space
\thanks{The authors are with the GRASP Laboratory,
        University of Pennsylvania,
        3330 Walnut Street, Philadelphia, PA 19104, USA.
        %{\tt\small
        \{zmen@seas, atanasov@seas, kostas@cis\}.upenn.edu.
        Grateful for support through the following grants: NSF-DGE-0966142 (IGERT), NSF-IIP-1439681 (I/UCRC), ARL RCTA W911NF-10-2-0016, and a GSK grant.}% <-this % stops a space
}


\begin{document}

\maketitle
\thispagestyle{empty}
\pagestyle{empty}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{abstract}
% Max 1500 chars

This paper considers the problem of active object recognition using touch only. The focus is on adaptively selecting a sequence of wrist poses that achieves accurate recognition by enclosure grasps. It seeks to minimize the number of touches and maximize recognition confidence. The actions are formulated as wrist poses relative to each other, making the algorithm independent of %small object movements and
absolute workspace coordinates. The optimal sequence is approximated by Monte Carlo tree search. We demonstrate results in a physics engine and on a real robot. 
In the physics engine, most object instances were recognized in at most 16 grasps. On a real robot, our method recognized objects in 2--9 grasps and outperformed a greedy baseline.

%Cover: What is the novelty, highlight experiment results

%||
%Novelty:
%Touch only recognition
%Active, few touches
%Formulation is independent of object pose
%MCTS

% TODO
%Say numerically: how many touches, recognition accuracy, under what conditions. How many experiments, results for each.

\end{abstract}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Introduction}

% Motivation: tactile sensing (not active)

Tactile sensing for object recognition has been an area of research since the 1980s \cite{gaston1984, grimson1984, allen1990}. Major advances have been slow, partly due to the sparse nature of touch, which requires more sensing time for a large area coverage, plus motion planning and physical movement time. Additionally, manipulator hardware is expensive. In comparison, vision-based recognition has seen major improvement because of the rich data, rapid information gathering, and low cost.

% Why tactile and not vision
However, scenarios exist where vision is unreliable, such as dark, dusty, smoky, or blurry underwater environments, transparent and reflective objects, occluded back sides, and objects in a bag. 
In these cases, tactile sensing is a better main modality.
Furthermore, the ultimate goal of manipulation is to contact the object. Starting with contacts early on provides direct physical exteroception that vision cannot achieve.
In fact, physical action is naturally integrated with perception in animals, who use various active tactile sensing organs \cite{prescott2011}. Humans can recover shapes by touch alone.

% Cut, save space. Shrank to the first sentence in next paragraph.
%Part of the disadvantage of tactile sensing can be compensated by better hardware. Most manipulators today cannot comfortably perform smooth rolling contact.
%The human hand's capability in surface following and high sensitivity enable fast and accurate inference of object properties. 
%On a robot, we are limited to sparse and time-consuming contacts.

While some disadvantages of tactile sensing can be compensated by better hardware, 
others can be compensated by efficient planning and exploitation of the limited input. In fact, Flanagan \textit{et al.} \cite{flanagan2006} found that the key to sophisticated manipulation in the human hand lies more in the accurate prediction of motor commands and outcomes than in rapid sensing. This learning and prediction are the bases of active sensing.
% Meh
%Touch sensing is naturally active, as it directly interacts with the environment, and any sensor feedback can be used to adjust the immediate local interaction. Unlike vision, multiple frame transformations and heavy data transfer or processing are not needed.
%
% Active tactile
Active tactile sensing had early work in tandem with passive vision \cite{allen1985, stansfield1988, allen1988} and alone \cite{schneiter1986}. Active perception, as noted by Bajcsy \cite{bajcsy1992}, involves a selection strategy that trades off between task success and the energy to achieve it.

% To appear on upper-right of first page
\begin{figure}[thpb]
  \centering
  %\includegraphics[height=3.5cm]{imgs/2016-09-06_08_41_55_iter1_GazeboOff}
  % Trimming image: trim={left bottom right top}
  %\includegraphics[height=3.5cm,trim={2.1cm 0 1.9cm 0},clip]{imgs/2016-09-07_14_54_44_10sims}
  %\vspace{5cm}
  %\includegraphics[height=4.5cm]{imgs/cup_275sims}
  \includegraphics[height=4.5cm]{imgs/real_toilet_paper_bounty_rviz}
  \includegraphics[height=4.5cm,trim={5cm 0cm 3.5cm 8cm},clip]{real/cover_IMG_0397}
  %\includegraphics[height=4.5cm,trim={8.5cm 1cm 5.7cm 0cm},clip]{real/cover_jar35_20170222_192926}
  %\caption{Actively selected poses (red on left, magenta on right), and all available trained wrist poses (blue).}
  \caption{Left: Experiment setup. Right: An adaptively selected pose.}
  \label{fig:intro}
  \vspace{-6mm}
\end{figure}


% This paper
% Now can summarize our approach! Say the goal first. Then say intuition. Then say how we solve it, at a high level.

In this paper, we tackle touch-only object recognition by an autonomous active selection algorithm, which aims to select a minimum number of wrist poses for maximum recognition likelihood.
We formulate the problem as a Markov Decision Process (MDP) and optimize for such a policy.
%While touch is often used as a secondary modality to vision, merely for verification or refinement, we use touch as the main modality. We only assume that a fixed object location is given initially. %This makes our algorithm applicable to situations where vision is obstructed.
%We use touch as the only sensing modality, so that our approach is applicable in situations without vision.

Our core idea is that consecutive tactile features observed on an object are related to the robot movements in between. Local features are not unique, repeating at symmetric parts and similar curvatures. Discretizing them across objects creates common features that can be modeled as a probability distribution,
which we condition on observations and actions, independent of large state space dimensionality.
%. We define this distribution in terms of observations and actions, %independent of state space, 
%which lets us predict actions in solvable time.

We focus on the active prediction problem and not the recognition. For recognition, we use an existing tactile object descriptor \cite{triangles}, the weakness in which is that recognition required hundreds of systematic touches, unrealistic in practice. Our goal is to eliminate that weakness by strategically selecting a small number of touches to observe the key features. We were able to decrease the number by a magnitude.
%, thereby maximizing recognition likelihood and minimizing number of touches. 
%We were able to decrease the touches by a magnitude.
%are able to recognize in a magnitude fewer moves.


% Emphasize the 3 novelties up front
The novelty of our active approach has three parts. 
First, unlike typical active models, ours is independent of the state space, by conditioning on observations and actions. 
State space-dependent methods have search times proportional to state dimensionality, posing a limit on state definition.
%This is significant as most methods' search times are proportional to state space dimensionality, posing a limit on the state definition. %This is significant as our state space is a probability of 1000-D histograms, which would take exponential search time. 
Second, unlike most active perception approaches, ours is not greedy. %it prioritizes long-term reward. % over short-term gains. 
Third, we solve a high-level perception problem as opposed to a low-level sensor-focused haptics one. We target autonomous object-level recognition with cheap sparse pressure sensors, in contrast to most tactile recognition work, which are material-level recognition with expensive dense multi-modal sensors and predefined strokes. Our algorithmic abstraction 
is not limited to special sensors.
% || DONE
%Emphasize the 3 novelties upfront: 
%0. emphasize our CORE idea is that observations depend on relative actions, and the same relative actions should make repeated observations; 
%1. we condition on observations and actions, not on the state space. Thus, we are able to make the model independent of state space, unlike typical models in active sensing;
%2. non-greedy;
%3. differentiate ourselves: cheap sensors doing high level object recognition autonomously, as opposed to expensive sensors doing low level surface material recognition by pre-defined strokes.
%
% This repeats abstract
We show successful prediction %on everyday objects
in a physics engine and on a real robot.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Related Work}
\label{sec:related_work}


%%%%%%%%%%
% State what's in scope

% Recognition

% Differentiate our work (high level recognition) from haptic material recognition by sensor vibrations, which reviewers get confused about

Tactile work has been done for reconstruction, localization, pose estimation, and recognition. %Our work addresses object-level recognition.
Our work differs from haptics work on material recognition in three major ways. First, we are solving a perception problem at the object level based on high-level geometry abstracted from sensor inputs, not at the material level that directly uses tactile vibrations. Second, we focus on active prediction of the most useful actions, whereas haptic recognition typically uses predefined motions. Third, we execute the actions autonomously.

% Active; interactive perception (active perception = forceful contact)
% Active viewpoint selection
Similar to our active end-effector pose selection, active viewpoints have been used to select camera poses to gather information, such as in \cite{doumanoglou2016}.
Sensing with vision only is considered active perception. Our work is closer to interactive perception, which physically contacts the environment \cite{bohg2016}.


%%%%%%%%%%
% Representative tactile work that's not active
%   gaston1984, grimson1984, siegel1991, allen1990

% If tight on space, can cut this paragraph, which is touch without active
Early work have explored touch-only object recognition not involving active planning.
%\cite{gaston1984, grimson1984, allen1990, bajcsy1987}.
Bajcsy \cite{bajcsy1987} compared human haptic exploratory procedures (EPs) observed by Lederman and Klatzky \cite{lederman1987} to robots, and Allen \cite{allen1990} extended them to a tactile robotic hand.
%Allen \textit{et al.} \cite{allen1990} extended human haptic exploratory procedures (EPs) observed by Lederman and Klatzky \cite{lederman1987} to a tactile robotic hand. 
%3D shapes of symmetric objects were recovered by predefined movements along the axis of revolution.
%Bajcsy \text{et al.} \cite{bajcsy1987} makes a conceptual comparison of the EPs and object properties recovered between human and robot hands.
% Interpretation tree for recognition and localization \cite{gaston1984, siegel1991}
Gaston \cite{gaston1984}, Grimson \cite{grimson1984}, and Siegel \cite{siegel1991} used Interpretation Trees for recognition and pose estimation.
%The control was primitive, only moving sensors in linear paths.  
%Grimson \text{et al.} \cite{grimson1984} extends \cite{gaston1984} to 3D, and Siegel \cite{siegel1991} extends \cite{gaston1984} for pose estimation.
%We share the use of a search tree, though the nature of our trees is different. The nodes and edges of the Interpretation Tree are fingers and object vertices, and the tree is pruned using geometric constraints of a known object. Our tree is a probabilistic process, with nodes being observations and edges being actions.


%%%%%%%%%%
% Active. But not all are tactile, not all are recognition!
% One sentence per paper

% Fusion
Active touch has been coupled with vision to various extents.
%Some explicitly fused with visual data.
Allen \textit{et al.} \cite{allen1985, allen1988} used vision to guide active touch to invisible regions and explicitly fused the two for shape reconstruction.
% Not fusion, only uses vision at beginning
%Others used vision only at the beginning, or explore entirely using touch.
Stansfield \cite{stansfield1988} used an initial visual phase for rough object properties and a final haptic phase for detailed surface properties.
Others explored solely using active touch.
Schneiter \cite{schneiter1986} scheduled sensor motions based on \cite{grimson1984} for recognition.
Maekawa \textit{et al.} \cite{maekawa1992} advanced through grid points for reconstruction as contacts were detected.
Hsiao \textit{et al.} \cite{hsiao2007} partitioned the workspace and represented each region as a state in a POMDP for optimal control policy.
% Ours
%Our work falls in the latter variety. We assume an initial vision estimate of rough object location and volume radius, and solely use touch for recognition.

% Info gain, entropy
Many recent active learning algorithms greedily maximize information gain (IG) \cite{schneider2009, hsiao2010, saal2010, hebert2013, doumanoglou2016} for %object 
property estimation or task success.
% schneider2009:
%object identification. unsupervised clustering and bag of words.
%minimize entropy of belief about the type of object
% Cut all, to save space. Just simply list all.
%Schneider \textit{et al.} \cite{schneider2009} chose actions to minimize the entropy of the belief about the object type.
%Hsiao \textit{et al.} \cite{hsiao2010} chose trajectories that maximize task success using information gain.
%Saal \textit{et al.} \cite{saal2010} maximized mutual information to find shaking frequency and angle for determining the viscosity of a liquid.
%Hebert \textit{et al.} \cite{hebert2013} maximized IG to schedule simple pre-defined actions to estimate object properties.
% Ours
%Instead of using IG, we maximize the probability of correct recognition.
%
% Adaptive submodularity
Another recent development is adaptive submodularity, a diminishing return property \cite{golovin2010, golovin2011}.
It was shown that entropy can be submodular and was used to greedily maximize IG for near-optimal touches \cite{javdani2013}.
%Javdani \textit{et al.} \cite{javdani2013} showed that entropy is submodular under certain assumptions and used this property to greedily maximize IG for near-optimal touches.


%%%%%%%%%%
% Work most related to ours

Work most related to ours in active recognition are Pezzementi \textit{et al.} \cite{pezzementi2011_tro} and Hausman \textit{et al.} \cite{hausman2014}. Both use tree search to select actions. However, Pezzementi's tree was for motion planning, with nodes being collision-free configurations. Hausman's tree nodes were entropy, which were minimized to find optimal poses to move the object into camera view. Our tree nodes are tactile observations, and we select end-effector poses to maximize recognition.


%%%%%%%%%%
% MCTS
% http://adp.princeton.edu/Papers/Powell_ADP_2ndEdition_Chapter%206.pdf

Different from greedy policies, a lookahead policy (\textit{e.g.} tree search)
%A different approach from greedy policies is lookahead policies, which include tree search. Lookahead policies 
explicitly optimizes cost and gain several steps ahead
%, as opposed to greedily choosing the immediate best 
\cite{powell}. Its advantage is that it can avoid jumping on immediate high gains that are also extremely costly, and instead favor less costly actions that yield long-term gain.

Solving for lookahead policy directly is impractically costly, as every possible state in each step ahead needs to be considered.
We tackle this in two ways.
First, we use a Monte Carlo optimization method from reinforcement learning literature \cite{kaelblingRL}.
Second, instead of modeling the state space, we formulate a probability dependent only on the observations and actions. It is considerably lower dimensional and generalizes to any object descriptor and robot platform.

Monte Carlo tree search (MCTS)~\cite{browne2012} has become popular for real-time decisions in AI. It is an online alternative to dynamic programming and uses repeated simulations to construct a tree in a best-first order. 
%Each simulation has two stages: tree policy and rollout policy. The tree policy handles the exploration-exploitation trade-off and is followed until a leaf node. 
Kocsic and Szepesv\`{a}ri~\cite{uct} showed that tree policy using the UCT (Upper Confidence bounds applied to Trees) guarantees asymptotic optimality. Feldman and Domshlak~\cite{brue} introduced BRUE, a purely exploring MCTS that guarantees exponential cost reduction.
%smooth exponential-rate reduction of the cost function. %, improving over the polynomial-rate guarantees provided by the state-of-the-art algorithms. 
Silver and Veness~\cite{pomcp} extended MCTS to partially-observable models. MCTS has been used for game solving~\cite{mcts_atari} and belief-space planning in robotics~\cite{Hauser_WAFR10,Sukkarieh_ICRA14,Lauri_ICRA14}, but has not been applied to manipulation.



%For robotics manipulation, which requires real-time decision-making and may have limited on-board computing, this type of approaches is advantageous. We refer the reader to~\cite{browne2012} for a survey on MCTS methods.
%Finally, we use MCTS to solve the active problem. A survey of MCTS methods is in \cite{browne2012}.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Problem Formulation}
\label{sec:formulation}

Our goal is to adaptively select a minimum sequence of end-effector poses to correctly recognize an object. The input is contact XYZ only, given by enclosure grasps, useful for sensing the volume and the global shape of an object \cite{lederman1987}. 

\subsubsection{Recognition Descriptor}

We focus on optimizing the sequence of poses and use an existing tactile object descriptor \cite{triangles} for recognition.
%Here we summarize the construction of the descriptor.
We cap the sequence at $t=1:T$ poses. At time $t$, a grasp provides $n$ contact points, resulting in $\binom{n}{3}$ triangles $z_t$ \cite{triangles}. Observed triangles $z_{1:t}$ are binned into a 3D histogram $h_t$. The three dimensions represent triangle parameters, \textit{e.g.} two sides and an angle. The histogram is the object descriptor input to a classifier.

\subsubsection{Active Probability}
\label{sec:formulation_core_prob}

In between two consecutive observations $z_t$ and $z_{t+1}$, the end-effector moves by some action $a_{t+1}$, which we model as $a \in SE(3)$, the translation and quaternion from the current wrist pose to a new one. As the hand moves, the previous ending pose becomes the next starting pose, hence removing the need for world frame coordinates for both the hand and the object pose. Let $c_m(a) \in [0,1]$ denote the movement cost incurred by $a$.

To model the recursive chain of $z_t \rightarrow a_{t+1} \rightarrow z_{t+1} \rightarrow \ldots$, we write the probability distribution $p(z_{t+1} | z_t, a_{t+1}, y)$. %It models the relationship between consecutive observations $(z_t, z_{t+1})$ and the action $a_{t+1}$ taken in between. 
It is in terms of the next observation $z_{t+1}$, conditioned on the current observation $z_t$, the next action $a_{t+1}$ that leads to $z_{t+1}$, and the unknown object class $y$. %This distribution is learned in training.

\subsubsection{Training and Test}

During training (Alg.~\ref{alg:train}), two things are learned for each object: its histogram descriptor $h$ and its $p(z_{t+1} | \cdot)$ distribution above. Training is done by moving the robot hand in a grid \cite{triangles} around the object. Actions and observations are recorded to compute the two items.
An action is defined between two wrist poses; $n$ poses yield $n^2$ actions.
%
Additionally, we train a support vector machine (SVM) classifier (Sec.~\ref{sec:baseline}) on the descriptors. The SVM gives $p(y | h)$, the probability of class $y$ given a histogram.

% Camera-ready revised: Don't put z_t as part of state x_t. Because z_t is simply previous observation, already KNOWN - not estimated! We don't have to keep the actual 3D z_t in the state!! - `.` a reviewer said we still depend on the state space for probability calculation, when in fact we don't. We could just keep an integer index of the histogram linear index where z_t is, and it'll be the same. We only need to remember which observation we've seen, we aren't actively estimating it!
At test time, the robot chooses its next grasp $a_{t+1}$ (Sec.~\ref{sec:mcts}) based on state $x_t = h_t$. %\{h_t, z_5\}$.
Given the current histogram $h_t$, we can obtain the recognition probability $p(y | h_t)$.

% DONE. Now in Introduction section
%\TODO{Emphasize the novelty of modeling $p(z_{t+1} | z_t, a_{t+1}, y)$ over the usual way of modeling $p(\cdot | x_t)$, where $x_t$ is the state. Our model is independent of state space, which in our case is the probability of histograms, a huge space to model.}


%\vspace{-5mm}
% Ref: http://tex.stackexchange.com/questions/25828/how-to-remove-change-the-vertical-spacing-before-and-after-an-algorithm-environ
\setlength{\intextsep}{2pt}  % Space between algo block and text
\setlength{\textfloatsep}{2pt}  % Space btw a float at top/bottom of page and text below/above it
\setlength{\floatsep}{0pt}  % Space btw two text floats (algo blocks)
\begin{algorithm}[htbp]
\caption{Training stage}
\label{alg:train}
\For {each object}
{
  define a grid of wrist poses P wrt object; execute P\;
  store triangle observations $\{z\}$ from contacts\;
  store tallies of observations $\{z\}$ per pose\;
  compute histogram descriptor $h$\;
}
\end{algorithm}
%\vspace{-5mm}


\begin{problem*}[Active Tactile Recognition]
Given an object with unknown class $y$, an initial information state $x_0$, and a planning horizon of $T$ steps, choose a control policy $\pi$ to optimize the cost, which trades off between movement cost and misclassification probability:
\begin{equation} \label{eqn:objective}
\min_\pi C_T(\pi) \triangleq \frac{\lambda}{T} \mathbb{E}\!\left[ \sum_{t=0}^{T-1} c_m(\pi(x_t)) \right]\!\! +\! (1\!-\!\lambda) \mathbb{P}(\hat{y}_T \!\neq y)
\end{equation}
where $\pi$ maps current state $x_t$ to next action $a_{t+1}$, and $\hat{y}_T \!=\! \argmax_y p(y | h_T)$ is the maximum likelihood estimate of the object class.
$\mathbb{P} (\hat{y}_T \!\!\neq\!\! y) \!\!=\!\! 1 \!\!-\! \max_y p(y | h_T)$ is the misclassification probability.
$\lambda \!\!\in\!\! [0,1]$ determines the relative importance
of incurring movement cost (first term) to gather more information vs. making an incorrect recognition (second term).
\end{problem*}


% Done. Just don't include the line that says \captionsetup{font=footnotesize}, it breaks equation labels!!
% Fix equation labels
%\eqref{eqn:objective}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Proposed Approach}
\label{sec:approach}

\subsection{Markov Decision Process (MDP)}
\label{sec:mdp}
The problem can be represented by a finite-horizon MDP defined by $(\mathcal{X},\mathcal{A},\mathcal{T},\mathcal{G}_t)$.
$\mathcal{X}$ is the state space.
$\mathcal{A}$ is a \textit{finite} set of possible actions.
The transition function
\[\mathcal{T}(x_t,a,x_{t+1}) \triangleq \sum_y p(z_{t+1} \mid z_t,a,y) p(y \mid h_t)\]
advances from state $x_t$ to $x_{t+1}$ given action $a$. Histogram $h_t \in x_t$; $z_t$ is determined by $(x_t, a)$; and $h_{t+1} = (h_t, z_{t+1})$ initializes $x_{t+1}$.
% Camera-ready revised: x_t = h_t only
%It is in terms of observation $z$ and histogram $h$, because state $x_t = \{h_t, z_t\}$.
\[ 
% Using & for conditions makes 1st condition exceed ICRA margin!
\mathcal{G}_t(x_t,a,x_{t+1}) \triangleq \begin{cases}
\frac{\lambda}{T} c_m(a), \qquad \qquad \qquad \qquad 0\leq t < T\\
(1-\lambda)(1-\max_y p(y\mid x_t)), \quad \, \, t = T
\end{cases}
\]
is the stage cost. This corresponds to the two terms in Eqn.~\ref{eqn:objective}. %the objective cost (Eqn.~\ref{eqn:objective}).

% Basics of MDP for reviewers unfamiliar with it
An MDP can be represented by a graph. Each state is a node, each action is an edge, and %the transition function 
$\mathcal{T}$ describes how to move from one node to another. % through an edge. %The stage cost 
$\mathcal{G}$ is the cost associated with an edge.
The graph is generated only at test time.

At the start of the process, a random action $a_0$ is selected. This generates observation $z_0$, which initializes histogram $h_0$ at state $x_0$.
Then, we advance through the states by simulating possible actions and following the transitions $\mathcal{T}$ to create nodes and edges (Sec.~\ref{sec:mcts}).
Each node's $z_t$ adds to the histogram.
Over time, the histogram $h_t$ is incrementally filled and resembles the true histogram from training, at which $p(y|h_t)$ would indicate a high probability.


\subsubsection{Relating Observations and Actions}

When we create a new node with $z_t$ at test time, we do not make robot movements to observe an actual $z_t$, because moving after every edge would require hundreds of movements for the entire tree, making the search impractically slow.
Instead, we rely on observations from training.

At the core of our approach is the relationship between observations and actions, modeled by $p(z_{t+1} | z_t, a_{t+1}, y)$, computed from training data (Sec.~\ref{sec:formulation_core_prob}). We trust this relationship to be reliable during training and carries over to test time, at which we directly sample this probability from training.
%
Note that $p(z_{t+1} | \cdot)$ is independent of the state space $\mathcal{X}$, which is a probability of histograms $p(h_t)$, high-dimensional (1000D) and exponential in search time.
%The combination of being independent of the state space $\mathcal{X}$ and the direct sampling from training data 
This independence and direct sampling from training 
allows $p(z_{t+1} | \cdot)$ to be computed quickly at test time (Sec.~\ref{sec:running_time}).


%We then sample from this distribution to arrive at the next observation. Since the true object label $y$ is unknown, we marginalize over it:
%\begin{align}
%\nonumber & z_{t+1} \sim p(z_{t+1} \mid z_t, a_{t+1}) = \\
%& \qquad \sum_y \Big( p (z_{t+1} \mid y, z_t, a_{t+1}) \, p(y \mid x_t) \Big)
%\label{eqn:observ}
%\end{align}


% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \section{Problem Formulation}
% \label{sec:active_formulation}

% Our goal is to predict a sequence of wrist poses to move around an object, such that the object can be successfully recognized in a small number of enclosure contacts.
% The enclosure type of contact is useful for sensing the volume and the global shape of an object, as studied in the well-known work of Lederman and Klatzky \cite{lederman1987}. We have previously demonstrated successful recognition by tactile enclosures using a custom descriptor \cite{triangles}. In this paper, we will use the same descriptor for recognition and focus on the action selection algorithm\NA{This paragraph might better fit in the Intro and Related work section. The problem formulation should be short and to the point}.


% % Give an intuition first
% \subsection{Intuition}
% \label{sec:intuition}
% \NA{$z_t$ and $y$ need to be defined clearly. Start with the description of the triangles and histogram earlier. Let $a \in SE(3)$ be a relative pose that transforms the current wrist pose to the next. Do we currently use relative or absolute actions?}
% We start from the intuition that a tactile observation on an object depends on where the manipulator is with respect to the object.
% As the hand moves around an object, parts of the object that share similar geometric properties are seen multiple times, for example as in a symmetric object. We can then model the observations as a probability distribution conditioned on the hand actions.

% If we treat hand movements as transformations with respect to the previous wrist pose, as opposed to the world frame, then each movement is associated with a starting and ending wrist pose. Each pose is associated with a set of observations seen at that location.
% As the hand moves, the previous ending pose becomes the next starting pose. This forms a recursive relation between the action and observations, which we model as $p(z_{t+1} \mid z_t, a_{t+1}, y)$, the probability of seeing some next observation $z_{t+1}$, given the object $y$, current observation $z_t$ at time step $t$, and the next action $a_{t+1}$ immediately after. 
% The action $a = (t_x, t_y, t_z, q_x, q_y, q_z, q_w)$ is a transformation with respect to the previous wrist pose, with translation $t$ and Quaternion rotation $q$.

% For a known object, this probability can be exactly computed by systematically moving the hand around the object and recording the observations in the training stage. We move the hand in an ellipsoid grid, following our previous work.
 
% The significance of formulating hand movements as relative transformations with respect to each other, as opposed to with respect to the object, is that it makes the formulation independent of object pose. This is aligned with the object descriptor that we will use, which is also independent of object pose.


% \subsection{Markov Decision Process}
% \label{sec:mdp}

% % What are the states of the MDP

% A Markov chain is formed by the recursive action-observation relationship above. Each state $x_t$ of the chain stores an object descriptor, built directly from historical observations. We refer the reader to \cite{triangles} for details of the descriptor. In short, the descriptor is a 3D histogram of triangles, each dimension being a triangle parameter, \textit{i.e.} a side length or an angle. An observation is a triangle constructed from a set of three contact XYZ positions on the object. When there are $n > 3$ contacts, we exhaustively sample the binomial coefficient $\binom{n}{3}$ triangles.

% The state update is simply an update of the current triangle histogram $h_t$, with the triangle from the new observation $z_{t+1}$ (Eqn. \ref{eqn:hist}). In practice, we add a set of triangles (Sec.~\ref{sec:mcts}) to the histogram, not just the single one; however, here we will use $z_t$ to refer to one triangle for simplicity.
% In addition to the triangle histogram $h_t$, a state also stores the most recent observation $z_t$, in order to calculate the conditional probabilities for predicting the next observation (Eqn. \ref{eqn:observ}):
% \begin{align}
% x_t &= \{h_t, z_t\} \label{eqn:state} \\
% h_{t+1} &= f (h_t, z_{t+1}) \label{eqn:hist}
% \end{align}


% % Actions and rewards

% The Markov decision process (MDP) is a Markov chain with the addition of actions and rewards. We assume the states are fully observable. The action $a$ is as defined above. The reward is the negative of the cost. The cost $C$ consists of a trade-off between movement cost and misprediction cost, defined by the objective function
% \begin{align}
% \nonumber & \min_\pi C_\tau = \\
% & \min_{\pi} \, \frac{\lambda}{\tau} \, \mathbb{E} \left[ \sum_{t=0}^{\tau-1} c_m(\pi(h_t)) \right] + (1-\lambda) \, \mathbb{P}(\hat{y}_\tau \neq y)
% \label{eqn:objective}
% \end{align}
% where $\pi$ is a policy that maps the current triangle histogram $h_t$ to the next action $a_{t+1}$, and $c_m(\pi(h_t))$ within $[0, 1]$ is the movement cost of an action chosen by policy $\pi$.
% $y$ is the true object label, $\hat{y}_\tau$ is the final object prediction, and $\hat{y}_t := \argmax_y p(y \mid h_t)$. $\lambda$ is a weight to balance the two costs. $C_\tau$ is in range $[0, 1]$. 
% Time step $t$ is from $0$ to $\tau$. 
% %$t_0$ is the current time step at which the next action is to be determined. 
% A sequence of actions $a_{1:\tau}$ is chosen at the end.
% The reward is accumulative; the reward at time $t$ is the sum of taking actions $a_{t+1 : \tau}$. %At the first state, $t_0 = 0$, and the first term is simply the average movement cost across all $\tau$ actions in the sequence, $\frac{1}{\tau} \sum_{t=0}^\tau c_m(a_t)$.

% $\mathbb{P} (y_\tau \neq y)$ is the probability of misprediction:
% \begin{align}
% \mathbb{P} (y_\tau \neq y) = 1 - \max_y p(y \mid x_t)
% \label{eqn:mispred}
% \end{align}
% where $p(y \mid x_t)$ is the probability of recognizing object label as $y$, based on the descriptor in state $x_t$. The predicted label is the one with the maximum probability across all labels.

% By minimizing the objective cost in Eqn. \ref{eqn:objective}, the action sequence $\mathbf{a}$ is selected. We solve for the action using a Monte Carlo approach, detailed in Sec.~\ref{sec:mcts}.


% % Predicting next observation

% After the next action $a_{t+1}$ is selected, the next observation $z_{t+1}$ needs to be determined.
% While advancing through the states in the MDP, we do not make any robot movements to observe actual data, as it would make the search for $a$ impractically slow. Instead, we rely on training data to give us a probability distribution of observations, $p(z_{t+1} \mid z_t, a_{t+1}, y)$, based on the current observation and the next action, as described in Sec.~\ref{sec:intuition}. We then sample from this distribution to arrive at the next observation. Since the true object label $y$ is unknown, we marginalize over it:
% \begin{align}
% \nonumber & z_{t+1} \sim p(z_{t+1} \mid z_t, a_{t+1}) = \\
% & \qquad \sum_y \Big( p (z_{t+1} \mid y, z_t, a_{t+1}) \, p(y \mid x_t) \Big)
% \label{eqn:observ}
% \end{align}


% % Summarize section. Say something about the recursive process to wrap up how everything pieces together

% At the beginning of the process, the histogram is initialized to empty. A random action is taken place to make an observation $z_0$ (Eqn. \ref{eqn:observ}), which becomes the first addition to the histogram (Eqn. \ref{eqn:hist}).
% At each time step, using known observations from training and a recognition model $p(y \mid x_t)$, an action is chosen by Eqn. \ref{eqn:objective} to minimize the movement and misprediction costs. The resulting observation updates the histogram in the next state. Over time, the histogram becomes more filled and is expected to approach the true histogram computed at training, making it ready for correct recognition.


% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Monte Carlo Tree Search (MCTS)}
\label{sec:mcts}

% || DONE 1 Mar 2017
% Rewrite this section to be more intuitive, start from scratch.

This section describes how we %simulate the transitions to 
generate a graph for the MDP at test time and select an optimal policy $\pi$ from it.
We represent the graph by a tree and use a Monte Carlo %simulation 
method.
The reader should refer to \cite{browne2012} for an overview of MCTS and \cite{kaelblingRL} for policy search.
The accompanying video animates the concept.
A simple example is shown in Fig.~\ref{fig:tree} and walked through in Sec.~\ref{sec:tree_procedure}.


% This has already been said in Related Work now
%\TODO{Say something about MCTS, how it is non-greedy, does not simply pick short-term best, looks to trade off both terms in the objective function in the long term, summing over all time steps $T$. See 2 drafts below.}

%The difference of our approach from information gain approaches is that the latter is greedy and optimizes the best short-term move, which can miss out on beneficial features that may not be immediately rewarding. For example, a feature that is the most discriminating in the short term can be a strenuous pose for the robot, costing a large movement cost before and after. Compare it with a feature that is the second-most discriminating but costs minimal movement. A greedy approach would select the first feature, while our approach would prefer the second.

%Tree search is a lookahead policy, which in contrast to greedy policies, explicitly formulates future information and future cost \cite{powell}. The Monte Carlo approximation enables practical lookahead search time, which is otherwise expensive.


An optimal policy $\pi$ outputs an optimal action sequence, which is defined as a path with maximum reward (or equivalently, minimum cost) from the root to a leaf. We seek a path that minimizes the objective cost in Eqn.~\ref{eqn:objective}.

% Repeats MDP section above
%Initially, the tree starts out empty. A random action $a_0$ is selected from actions in training, which leads to observation $z_0$ and initializes the histogram $h_0$ at the root.
After the root is created, Monte Carlo simulations select actions and follow $\mathcal{T}$ to create new edges and nodes.
% Repeats tree search procedure subsection below
Each simulation creates one new node.
After a number of simulations, the tree is well populated, and the optimal path is selected.
Each tree depth is a time step, with root at $t=0$ and leaves at $t \leq T$, a defined horizon, or max tree depth.


%\TODO{Say how the traversal goes - each node needs to choose an edge to create and traverse down. Each node stores a reward $C_{t_a}$ for each outgoing edge.}

%\TODO{Rewrite this written for UNCC - see comments. How tree works. Should be repeat of here, sift this into the original writing to make them more intuitive, but do not copy sentences.}
% Good summary of how tree works, wrote for UNCC
%A state has multiple possible outgoing actions, each associated with a reward $C_{t_a}$. During the tree search, each simulation starts at the root, and at each node, an action edge and child node must be chosen (and created if they do not yet exist) to continue traversal down the tree, until the horizon, where reward is computed and backpropagated.


\subsubsection{Choosing the Next Action $a_{t+1}$}

At time %step 
$t$, node $x_t$, the next action $a_{t+1}$ is selected as follows.
In an MDP that allows multiple actions per node (known as a multi-arm bandit problem \cite{browne2012}), the choice of an action faces an exploration-exploitation dilemma.
% Explain what the exploration-exploitation dilemma is
Exploring new or less-seen actions generates unseen parts of the tree, making use of more training data. Re-visiting high-reward actions exploits branches that at the moment seem more likely to be optimal. 

Balancing this dilemma ensures narrowing down the answer while keeping an open mind to see all of the tree.
For contrast, greedy policies %do not have this dilemma; they simply 
always exploit the highest-reward action and ignore the exploration half.
%
% UCB1 (Upper Confidence Bound) policy 
We use the UCT \cite{uct} upper confidence bound to select actions to balance this dilemma.
%It is efficient and has a logarithmic bound on the cost for arbitrary reward distributions \cite{browne2012}. 
At a node $x_t$ at depth $t$, the next action $a_{t+1}$ is:
\begin{align}
a_{t+1} = \arg\max_a \, \left( (1-C_{t_a}) + c \sqrt{ \frac{2 \ln N}{N_a}} \right)
\label{eqn:ucb1}
\end{align}
where the $C_{t_a} \in [0, 1]$ is $C_T(\pi)$ in Eqn.~\ref{eqn:objective}, computed in previous simulations and stored in node $x_t$. It is the cost of an available action edge $a$ at the $x_t$ (see backpropagation in Sec.~\ref{sec:tree_procedure}). $1-C_{t_a}$ is the reward.
$N$ is the number of times the node has been visited, and $N_a$ is the number of times action $a$ has been followed from the node. 

The first term is exploitation; it favors actions with a high existing reward. The second term is exploration; it penalizes actions that have been followed many times. The two terms are balanced by weight $c$, picked by hand. The result is a well-explored bushy tree.
In entirety, the bound selects an action $a_{t+1}$ that minimizes cost $C_{t_a}$. Together with other actions on a path chosen this way from root to leaf, this minimizes the objective cost $C_T$ in Eqn.~\ref{eqn:objective}.


\subsubsection{Inferring the Next Observation} % $z_{t+1}$}

Given an action $a_{t+1}$, the next
observation $z_{t+1}$ is sampled from training data: 
\begin{equation}
  \label{eqn:observ}
  z_{t+1} \!\sim\! p(z_{t+1} | z_t, a_{t+1})
  \!=\!\! \sum_y p(z_{t+1} | z_t, a_{t+1}, y) p(y | h_t)
\end{equation}
This reflects the mapping of the MDP transition function $\mathcal{T}(x_t, a_{t+1}) \rightarrow x_{t+1}$. It describes how to move to the next node $x_{t+1}$, given the current node $x_t$ and next action $a_{t+1}$.
The class $y$ is marginalized out, since the true $y$ is unknown.


\subsubsection{Tree Search Procedure}
\label{sec:tree_procedure}

Now we put the pieces together and describe the procedure of each tree search simulation in Algs.~\ref{alg:test},~\ref{alg:tree_search},~\ref{alg:tree_policy}.
% Algo
Alg.~\ref{alg:test} outlines the top-level test stage procedure.
Algs.~\ref{alg:tree_search} and \ref{alg:tree_policy} outline the tree search and tree policy.
We will walk through a 5-node tree in Fig. \ref{fig:tree}, with horizon $T=3$, the 5 nodes generated from 5 simulations.

Starting with an empty tree, some action is randomly selected and produces obs443 in Fig.~\ref{fig:tree} from training data. This initializes the root at $t=0$, with 1 observation in histogram $h_0$, which happens to be 0.89 distance from the closest object in training.

We will describe one full simulation.
Each simulation starts at the root at depth $t=0$ and must traverse a single path downward until the leaves at horizon depth $t=T$. Each depth contains actions $a_t$ and nodes $x_t$.
% Connect the tree with intuition, to say WHAT the actions a_t mean in physical world
The intuition of a path from root to leaf in the real world is a sequence of $T$ actions for the robot to execute.

In each simulation, one new node is created via the choice of $a_{t+1}$ and $z_{t+1}$ (Eqns.~\ref{eqn:ucb1},~\ref{eqn:observ}), outlined in Alg.~\ref{alg:tree_policy} treePolicy.
This means early simulations cannot reach depth $T$ via existing nodes, since the tree is still shallow.
In Alg.~\ref{alg:tree_search}, a recursive function treeSearch traverses the tree, incrementing in depth $t$ (line~\ref{alg:ts_recursive}).
As long as a node exists (line~\ref{alg:ts_exists}), the tree policy is called (line~\ref{alg:ts_tp}) to continue down.
%
When a desired node does not exist, it is created (line~\ref{alg:ts_node}), which concludes the one node created in the current simulation, and this ends the tree policy. The rollout policy follows (line~\ref{alg:ts_rollout}) and continues to depth $T$ by randomly selecting $a_{t+1}$ at each layer.

In Fig.~\ref{fig:tree}, simulation 1, at $t=0$, action $a_1=p15$ is selected and produces $z_1=obs3$. Since a node with obs3 at depth $t=1$ does not yet exist, it is created, and this ends the tree policy. The rollout policy selects random $a_2$ and $a_3$ that produce temporary nodes $x_2$ and $x_3$, not shown. The rollout policy operates on a temporary subtree that will be discarded at the end of the simulation.
%
When the rollout policy reaches horizon $T$, the histogram $h_T$ accumulated from observations $z_{0:T}$ on the path we took is fed to the classifier, which outputs the misclassification cost $\mathbb{P}(\hat{y}_T \neq y)$ in Eqn.~\ref{eqn:objective}.

We then trace the path back up to root and backpropagate this cost to store in each node on the path, as follows.
%The backpropagation is as follows.
At each depth $t$, reward $1-C_{t_a}$ is updated for action $a$ at node $x_t$.
This is computed by a standard discounted reward, $Q_a = Q_a + (r_a - Q_a) / N_a$, where $Q_a$ is the node's existing reward, and $r_a$ is the raw subtree reward (Alg.~\ref{alg:tree_search} lines~\ref{alg:ts_recursive}--\ref{alg:ts_backpropagate}).

In addition to backpropagation, the objective continues to be computed. At each depth $t$, edge $a_t$ is accumulated to movement cost $c_m(a_t)$ in Eqn.~\ref{eqn:objective}. Intuitively, the reward at node $x_t$ is the sum reward of actions on its current child path.
When we arrive back at the root, the entire objective $C_T$ has now been computed and stored to root under action p15 (Fig.~\ref{fig:tree}). This concludes simulation 1.
%
In the next 4 simulations, 4 more nodes are created and rewards computed similarly. %in a similar manner.
The more simulations, the bushier the tree, and the deeper the branches reach.
%Many simulations are run to obtain a bushy tree.

% TODO: Or can also sample the actions, instead of choosing the hard max. If do that, then change this sentence.
After many simulations, the tree search ends by extracting the optimal %maximum-reward 
path (Alg.~\ref{alg:test} line \ref{alg:test_selpath}). Starting at the root, simply follow the highest-reward edges downward.
This path defines an action sequence that minimizes $C_T$.
The length of the sequence is $\leq T$, as some branches may not reach $T$, \textit{e.g.} two right branches in Fig.~\ref{fig:tree}.
The optimal sequence is executed on a robot to obtain actual observations for recognition.


% Now rewritten better above
%Each simulation reaches horizon $T$ recursively (Alg.~\ref{alg:tree_search}), first via the tree policy (line \ref{alg:ts_tp}) until a child node does not exist and is created (line \ref{alg:ts_node}), and then the rollout policy continues to $T$ (line \ref{alg:ts_rollout}).

% Now rewritten better above
%At each node, the next action $a_{t+1}$ (Eqn.~\ref{eqn:ucb1}) and desired child node with $z_{t+1}$ (Eqn.~\ref{eqn:observ}) are chosen by the tree policy (Alg.~\ref{alg:tree_policy}). 
%At the end of a simulation, the reward (Eqn.~\ref{eqn:objective}) is computed at $T$ and backpropagated up the tree (Alg.~\ref{alg:tree_search} lines \ref{alg:ts_reward}--\ref{alg:ts_backpropagate}). %Details of the rollout policy and backpropagation are in Sec.~\ref{sec:mcts_example}.



\begin{algorithm}[htbp]
\caption{Test stage}
\label{alg:test}
object location given\;
load training probabilities $P_{train}$\;
  %discretize observations $\{z\}$ into histogram bins\;
  superimpose training poses $\{p^o\}$ onto test object\;
move robot to a pose $p_0$ that contacts object\;
close grippers; record observation $z_0$; compute histogram $h_0$\;
\For {each tree}
{
  $node_0$ = initNewTreeRoot $(h_0)$\;
  \For {each simulation}
  {
    treeSearch $(node_0, z_0)$\;
  }
  actions = select max-reward root-to-leaf path\;  \label{alg:test_selpath}
  \For {each action $a_t = a_1 : a_T$ in actions}
  {
    move robot to $a_t$; close grippers\;
    record observation $z_t$; update histogram $h_t$\;
  }
  $z_0 = z_T$; $h_0 = h_T$\;
}
\end{algorithm}
%\vspace{-4mm}

\begin{algorithm}
\caption{Tree search}
\label{alg:tree_search}
\underline{function treeSearch} $(node_t, z_t)$\;
  % Keep going
  \eIf {$node_t$ exists in tree}
  { \label{alg:ts_exists}
    $a_{t+1}, z_{t+1} =$ treePolicy$(node_t, z_t)$\; \label{alg:ts_tp}
    subtreeReward = treeSearch$(node_{t+1}, z_{t+1})$\; \label{alg:ts_recursive}
    $r = (1 - c_m (a_{t+1})) +$ subtreeReward $\quad$  \tcp{Eqn.~\ref{eqn:objective}}  \label{alg:ts_reward}
    $node_t$.updateReward $(a_{t+1}, r)$\;  \label{alg:ts_backpropagate}
  }
  %else, end simulation:
  {
    create $node_t$\;  \label{alg:ts_node}
    $r =$ rolloutPolicy $(node_t, t) \quad$ \tcp{Eqn.~\ref{eqn:objective} $\mathbb{P}(\hat{y}_T \neq y)$}  \label{alg:ts_rollout}
  }
  return r\;
\end{algorithm}
%\vspace{-4mm}

\begin{algorithm}
\caption{Tree policy}
\label{alg:tree_policy}
\underline{function treePolicy} $(node_t, z_t)$\;
  % UCT. Q is discounted reward in active learning. N is number of times each arm is visited
  %$a_{t+1} =$ argmax$(node_t$.rewards$(node_t.$actions$()))$\;
  $a_{t+1} =$ argmax$_a$ UCT$(node_t.C_a, node_t.N_a) $ \tcp{Eqn. \ref{eqn:ucb1}}
  $h_t = node_t.h$\;
  % next observation
  \For {each class $y$}
  {
    $p_y = p(z_{t+1} | z_t, a_{t+1}, y) =$ sampleFrom$P_{train} (z_t, a_{t+1}, y)$\;
  }
  % Marginalize over y. p(y|h_t) is class prediction from incremental histogram so far
  %$p(z_{t+1} | z_t, a_{t+1}) = \sum_y p(z_{t+1} | z_t, a_{t+1}, y) p(y | h_t)$\;
  %$z_{t+1} \sim p(z_{t+1} | z_t, a_{t+1}) \,$  \tcp{$\sim$ denotes sampled from}
  $z_{t+1} = $ marginalizeY$(\{p_y\}) \quad$  \tcp{Eqn.~\ref{eqn:observ}}
  return $a_{t+1}, z_{t+1}$\;
\end{algorithm}
%\vspace{-4mm}


\begin{figure}[thbp]
  \begin{center}
    %\includegraphics[width=.8\linewidth]{imgs/tree_vis_nsims10_2016-09-06-12-58_printNNDist}
    \includegraphics[width=.8\linewidth]{imgs/2017-02-28-16-36-50_tree_vis_nsims5_iter1}  % overleaf server error. This is the right image. Will be swapped in when server is up
  \end{center}
  \caption{A small example tree. Max-reward path highlighted. Node label is observation name, nearest neighbor distance, tree depth $t$, and number of items in histogram $h_t$. NN distance is inversely proportional to $p(y|h_t)$. Edge label is action name $a_{t+1}$ and reward.}
  \label{fig:tree}
  %\vspace{-3mm}
\end{figure}


% -------------------------------------------------------------------------

% Now combining this subsection with Tree Search Procedure subsection above
%\subsection{Small MCTS Example}
%\label{sec:mcts_example}

%To help with intuition, we walk through a small example tree with horizon $T=3$ in Fig. \ref{fig:tree}, generated from 5 simulations.
%Starting with an empty tree, some action is random selected and produces obs443 from training data. This initializes the root at $t=0$, histogram $h_0$ with 1 observation. $h_0$ happens to be 0.89 distance from the closest object in training.

%In simulation 1, at $t=0$, action $a_1=p15$ is selected by the tree policy (Eqn.~\ref{eqn:ucb1}) and produces $z_1=obs3$ (Eqn.~\ref{eqn:observ}). Since a node with obs3 at depth $t=1$ does not yet exist, it is created, and this ends the tree policy. The rollout policy is called to continue to $T$. On the way, it selects random $a_2$ and $a_3$ that produce temporary nodes $x_2$ and $x_3$, not shown. The rollout policy operates on a temporary subtree that will be discarded at the end of the simulation.

%At horizon $T$, the histogram $h_T$ accumulated from observations $z_{0:T}$ on the path we took is fed to the classifier, which outputs the misclassification cost $\mathbb{P}(\hat{y}_T \neq y)$ in Eqn.~\ref{eqn:objective}.
%We then trace the path back up to root and backpropagate this cost to each node on the path.

%The backpropagation is as follows.
%At each depth $t$, the reward $C_{t_a}$ is updated for action $a$ at node $x_t$.
%$C_{t_a}$ is a standard discounted reward computed by $Q_a = Q_a + (r_a - Q_a) / N_a$, where $Q_a$ is the node's existing discounted reward, and $r_a$ is the raw subtree reward (Alg.~\ref{alg:tree_search} lines~\ref{alg:ts_recursive}--\ref{alg:ts_backpropagate}).

%In addition to backpropagating the rewards, the objective continues to be computed. At each depth $t$, $a_t$ on the edge is accumulated to movement cost $c_m(a_t)$ in Eqn.~\ref{eqn:objective}. When we arrive back at the root, the entire objective $C_T$ has now been computed and stored to root under action p15. This concludes simulation 1.
%
%In the 4 subsequent simulations, 4 more nodes are created and rewards computed in a similar manner.

% --- subsubsection ends here



% This is good. Now rewritten above to suit the image better
%Starting with an empty tree, the root is created at depth $t=0$. Then an action $a_{t+1}$ is selected using UCT (Eqn. \ref{eqn:ucb1}), and the next node is created with an observation $z_{t+1} $selected using Alg. \ref{alg:tree_policy} lines 4--7. After a new node is created, the simulation ends, and the rollout policy continues to the horizon $t=T$ by selecting a random action at each depth. The random actions and nodes are not stored. 
%At horizon, the histogram $h_T$ accumulated from all the observations on the path we took is fed to the classifier, and the misclassification cost in the objective (Eqn. \ref{eqn:objective}) is computed. We then backpropagate by tracing the path back up the tree, at each depth accumulating $a_t$ on the edge to the movement cost $c_m$. When we arrive back at the root, the entire objective $C_T$ has now been computed.

% Merged with above
% Rollout policy
%The rollout policy operates on a temporary subtree, in order to reach $T$ where the objective is computed. At each depth $t$, it selects $a_{t+1}$ at random and produces $z_{t+1}$ to create a temporary new node. After backpropagating the rewards, new nodes from rollout policy are discarded.

% Merged
% Reward update by backpropagation
%The backpropagation is as follows. Starting from $t=T$, each node's raw reward $1-C_t$ is added to its parent under the appropriate arm $a_t$. We use a standard discounted reward, $Q_a = Q_a + (r_a - Q_a) / N_a$, where $Q_a$ is the node's existing discounted reward, and $r_a$ is the raw reward.


% -------------------------------------------------------------------------

% No space for these implementation details.
%\TODO{We use a non-deterministic tree, to model that an action (a relative transformation) can lead to different observations in reality.}
%
%Fig. \ref{fig:tree} shows a small tree created from 10 simulations.
%Each node can have multiple parent and child edges, because in practice, multiple actions can lead to the same observation, and multiple actions can be made from an observation.
%
%Observations in the tree are from training data. No actual robot movement is made during the tree search.
%At each action, more than one triangle can result. We use a single triangle for $z_t$, but we add all triangles that exceed a threshold in $p(z_{t+1} \mid z_t, a_{t+1})$ into the histogram $h_t$.


% -------------------------------------------------------------------------
\subsection{Implementation}
\label{sec:setup}

% Experiment Procedure
% Just describe the procedure, its straight forward

% Initialize
The recognition of test objects is performed by alternating between MCTS and robot action execution. Note that we use the term iteration to refer to one tree search and action execution, \textit{e.g.} two iterations means a tree search, an execution, a second tree search, and a second execution. 
% Repeats MCTS section
%The term simulation refers to the number of times the tree is searched; each simulation runs tree policy once and generates exactly one new node.

% Irresponsible reviewer who didn't read the whole sentence somehow thought this means we used RGBD to obtain object position in experiments - not that it is not okay or anything as it is outside our scope. Somehow they felt the need to object. Might as well not state it, confuses people who don't read.
%Initially, the approximate center and radius of the test object are supplied by the user for simplicity. For a robot with stereo or RGBD vision, this can be easily substituted with the center and radius of an approximate point cloud volume, even if vision is obstructed. We used user-supplied values because vision is outside the scope of this paper.

% Procedure: MCTS + execution alternating
The object pose is assumed known and fixed. 
A first wrist pose is randomly selected from the training data, which store poses with respect to the object center. The observation $z_0$ is computed and initializes the root node of the first tree. 
%
The tree is generated and produces a sequence of relative wrist poses. This sequence is executed on the robot, with enclosure grasps onto the object at each wrist pose. Actual observations are taken, and a histogram is built and fed to the classifier. This completes one iteration. 
Then, the old tree is discarded, and the latest histogram initializes the root node of a new tree. MCTS is performed again on the new tree. 


% Parameter choices

In order to generalize across objects, triangle observations $z_t$ are discretized to their histogram bin centers. This is required to compute $\mathcal{T}$, which needs probabilities for the conditioned $z_t$ for every object $y$. Otherwise, a triangle from one object might not exist in another to provide this probability. Histogram bin sizes are chosen as in \cite{triangles}.
% Camera-ready revision dropped: The is too much detail. Reviewer 1 asked for this. I don't have space for this. Just cite \cite{triangles}.
%Histogram bin centers are determined by taking the minimum and maximum values of each histogram parameter, then dividing this range by the number of bins, selected as in \cite{triangles}.

% Check it's within [0,1], required by UCB1 policy
% TODO final version: Make sure these parameters still what we're using, for the final reported figure results!
For the movement cost $c_m(a_t)$, we computed the $L_2$ distance for translation and the angle for rotation, then normalized each to $[0, 1]$ and weighed both equally.
We used $c=1$ to weigh exploration and exploitation equally and $\lambda = 0.5$ to weigh movement and misprediction costs equally.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% Merged into \ref{fig:hists}
\begin{comment}
\begin{figure*}[thbp]
  \begin{center}
  \begin{tabular}{c}
    \includegraphics[height=\meshheight]{imgs/cup_4536abec}
    \includegraphics[height=\meshheight]{imgs/teapot_45398b0a}
    \includegraphics[height=\meshheight]{imgs/bottle_20b7a}
    \includegraphics[height=\meshheight]{imgs/bowl_685825}
    \includegraphics[height=\meshheight]{imgs/mug_bed29ba}
    \includegraphics[height=\meshheight]{imgs/toilet_paper_c34a11}
    \includegraphics[height=\meshheight]{imgs/sphere_3cm} \\
    % Now shown in Fig \ref{fig:hists}
    % Trimming image: trim={left lower right upper}
    %   http://tex.stackexchange.com/questions/57418/crop-an-inserted-image
    %\includegraphics[height=2.3cm,trim={8cm 8cm 1.8cm 1.1cm},clip]{imgs/hist_truths_gz}
    %\hspace{-.5cm}
    %\includegraphics[height=2.3cm,trim={2.2cm 2cm 2cm 7.2cm},clip]{imgs/hist_truths_gz}
  \end{tabular}
  \end{center}
  \caption{Synthetic objects and their ground truth histogram descriptors for recognition in the physics engine. Best viewed in color. We tried to represent relative sizes of objects, but they are not to exact scale. Note the similarities between object shapes \textit{e.g.} mug and toilet paper; cup, teapot, and mug; cup and sphere.}
  \label{fig:sim_objs}
\end{figure*}
\end{comment}


\begin{figure*}[bthp]
  \begin{center}
  \begin{subfigure}{.69\textwidth}
    \begin{tabular}{c@{\hspace{0.2em}} c@{\hspace{0.2em}} c@{\hspace{0.2em}} c@{\hspace{0.2em}} c@{\hspace{0.2em}} c@{\hspace{0.2em}} c@{}}
      \includegraphics[height=\pcdheight]{meshes/cup_4536abec} &
      \includegraphics[height=\pcdheight]{meshes/teapot_45398b0a} &
      \includegraphics[height=\pcdheight]{meshes/bowl_685825} &
      \includegraphics[height=\pcdheight]{meshes/bottle_20b7a} &
      \includegraphics[height=\pcdheight]{meshes/mug_bed29ba} &
      \includegraphics[height=\pcdheight]{meshes/toilet_paper_c34a11} &
      \includegraphics[height=\pcdheight]{meshes/sphere_3cm}
      \\
      \includegraphics[height=\pcdheight]{hists/cup_truth} &
      \includegraphics[height=\pcdheight]{hists/teapot_truth} &
      \includegraphics[height=\pcdheight]{hists/bowl_truth} &
      \includegraphics[height=\pcdheight]{hists/bottle_truth} &
      \includegraphics[height=\pcdheight]{hists/mug_truth} &
      \includegraphics[height=\pcdheight]{hists/toilet_paper_truth} &
      \includegraphics[height=\pcdheight]{hists/sphere_truth}
      \\
      \includegraphics[height=\pcdheight]{hists/cup_rand} &
      \includegraphics[height=\pcdheight]{hists/teapot_rand} &
      \includegraphics[height=\pcdheight]{hists/bowl_rand} &
      \includegraphics[height=\pcdheight]{hists/bottle_rand} &
      \includegraphics[height=\pcdheight]{hists/mug_rand} &
      \includegraphics[height=\pcdheight]{hists/toilet_paper_rand} &
      \includegraphics[height=\pcdheight]{hists/sphere_rand}
      \\
      \includegraphics[height=\pcdheight]{hists/cup_tree} &
      \includegraphics[height=\pcdheight]{hists/teapot_tree} &
      \includegraphics[height=\pcdheight]{hists/bowl_tree} &
      \includegraphics[height=\pcdheight]{hists/bottle_tree} &
      \includegraphics[height=\pcdheight]{hists/mug_tree} &
      \includegraphics[height=\pcdheight]{hists/toilet_paper_tree} &
      \includegraphics[height=\pcdheight]{hists/sphere_tree}
      \\
      %& & & {\footnotesize(a)} & & & 
    \end{tabular}
    \caption{}
    \label{fig:hists}
  \end{subfigure}
  \vspace{-2mm}
  %
  \begin{subfigure}{.29\textwidth}
    \begin{tabular}{c@{\hspace{0.2em}} c@{\hspace{0.2em}} c@{}}
      \includegraphics[height=\pcdheight]{poses_gz/rand_pose1} &
      \includegraphics[height=\pcdheight]{poses_gz/rand_pose2} &
      \includegraphics[height=\pcdheight]{poses_gz/rand_pose3}
      \\
      \includegraphics[height=\pcdheight]{poses_gz/rand_pose4} &
      \includegraphics[height=\pcdheight]{poses_gz/rand_pose5} &
      \includegraphics[height=\pcdheight]{poses_gz/rand_pose6}
      \\
      \includegraphics[height=\pcdheight]{poses_gz/tree_pose1} &
      \includegraphics[height=\pcdheight]{poses_gz/tree_pose2} &
      \includegraphics[height=\pcdheight]{poses_gz/tree_pose3}
      \\
      \includegraphics[height=\pcdheight]{poses_gz/tree_pose4} &
      \includegraphics[height=\pcdheight]{poses_gz/tree_pose5} &
      \includegraphics[height=\pcdheight]{poses_gz/tree_pose6}
      \\
      %& {\footnotesize(b)} &
    \end{tabular}
    \caption{}
    \label{fig:poses_gz}
  \end{subfigure}
  \vspace{-3mm}
  \end{center}
  \caption{(a). Synthetic meshes and their 3D histograms for visual comparison. From training (row 2), random baseline (row 3), tree policy (row 4). Note shape similarities between cup, teapot, and mug; bottle and mug; mug and toilet paper. Tree policy results resemble true histograms. Best viewed in color. (b). Wrist poses selected by random baseline (top 2 rows) and tree policy (bottom 2 rows) for the cup.}
  %\label{fig:hists}
  \vspace{-5mm}
\end{figure*}


\section{Analysis Using a Physics Engine}
\label{sec:sim}

% TODO See plan in my Google Doc ICRA Experiments Plan
% See a few more ideas in Google Keep

We validated the active recognition in a physics engine. 
% This clarification is important, newly added for resubmission
The purpose is to analyze the method itself, without external constraints of a specific robot platform, such as joint and workspace limits. This lets us evaluate the core algorithm using an end-effector with no unreachable poses.

The tactile hardware is a RightHand Robotics ReFlex Beta Hand, which has 9 barometric pressure sensors on each of 3 fingers, 27 total. Each enclosure grasp typically gives non-zero values on 3--6 sensors.
% Forgot to do this, and now don't have time. Next time!
%\red{TODO: print the number in triangles\_collect.py, print actual mean min max stats at the end, make sure say the right number here. Because small number means active algo is superior} 
Fig. \ref{fig:hists} shows objects used.


The XYZ positions of the non-zero sensors are used to compute the descriptor for recognition. We used the 3D histogram of triangles \cite{triangles} as descriptor, as mentioned in Sec.~\ref{sec:formulation}. A triangle requires three parameters to be uniquely described. The three side lengths of a triangle are denoted $l_0, l_1, l_2$ from large to small; similarly for the angles $a_0, a_1, a_2$. The following results use the $l_0, l_1, a_0$ parameterization. Because the triangles are a relative measure, the descriptor is independent of object pose and movement.

% Physics engine software
For physics engine results, we built a stack in Gazebo for the hand's tactile capabilities, including its guarded enclosure, which closes all fingers and stops each finger when a sensor on the finger is in contact. Sensor values were simulated as Boolean contacts. To simulate wrist movement, we teleported the wrist to specified poses. This dramatically reduced the time required, which would otherwise involve motion planning and moving the joint of an arm.

We repeated the MCTS and action execution iterations until rewards in all simulations are depleted to 0, which typically takes 7--9 iterations. When all rewards are depleted, there is only one node in the tree; this happens because we do not allow repeated visits to nominal absolute poses, as they do not provide new observations.


% -------------------------------------------------------------------------
\subsection{Baseline Comparison}

\label{sec:baseline}

%\TODO{Shrink this so have more room for real robot results. Just say we execute the action sequence from Sec.~\ref{sec:mcts}. And we do multiple trees iteratively, discarding each previous one, using the actual cumulative observations on the robot to initialize the next tree (Alg \ref{alg:test} lines 6--7, 14).}

%NOW HERE

To illustrate the need for an active selection, we created a baseline. It uses the same training data as the tree search, except it selects poses to move to at uniform random.
Fig \ref{fig:poses_gz} shows example poses chosen by the baseline and the tree policy for the cup; poses from the latter are more intuitive and capture the global shape better.

% Objects that random baseline do better: teapot.
% Objects that tree policy do better: bottle, bowl, cup, bottle, mug.
% Objects that neither recognized: sphere

Fig.~\ref{fig:hists} and Table~\ref{tab:hist_dist} show the 3D histograms and distances obtained by each method.
Fig.~\ref{fig:tree_vs_random} shows the progression of recognition through the iterations, in the form of distance to true class.
We tried linear SVM, nearest neighbor (NN) with inner product distance, and NN with histogram intersection distance, of which the inner product performed the best.
Tree policy performed better for all objects except teapot, which was correct in iterations 1--3 but diverged to mug in 4--9. This is reasonable as teapot, mug, and cup have similarities in the handle. Sphere was recognized by neither; the cause is evident in Fig.~\ref{fig:hists}, as both were unable to capture the lowest bins in $l_0$.

Fig. \ref{fig:poses} shows wrist poses selected around the objects, and Fig. \ref{fig:pcd} shows contact points obtained in the physics engine by the two methods. 
Comparing Figs. \ref{fig:tree_vs_random}, \ref{fig:poses}, and \ref{fig:pcd}, even though the baseline sometimes recover a better object appearance, its recognition can still be wrong, \textit{e.g.} bottle, mug.
Tree policy recovered better contact cloud and recognition for cup, bowl, toilet paper.
The only object that the baseline did better in all three was the teapot, most likely because tree policy tends to select poses at the bottom of the object, but the teapot's top half provide identifying information.

Even though the baseline's poses are more evenly distributed, they do not result in better recognition, other than teapot. Fig.~\ref{fig:hists} shows that baseline histograms are more distributed, whereas tree policy's histogram bins are concentrated in the area lit up in the true histograms.
Note that even though some baseline distances in Table~\ref{tab:hist_dist} are closer, \textit{e.g.} cup, toilet paper, its recognition in Fig.~\ref{fig:dist_vs_iter} is incorrect, meaning it is closer to some other class.
%
This means that the tree policy correctly imposes a bias on the wrist poses selected - poses that result in high recognition certainty, as enforced by $\mathbb{P}(y_T \neq y)$ in the objective. 
% Repetitive
%, as $p(y_T \neq y)$ in the objective enforces. 
%These poses may not result in a qualitatively good contact cloud, \textit{e.g.} bottle, but they result in correct recognition.


\begin{table}[thpb]
  \begin{center}
  \begin{tabular}{c@{\hspace{0.4em}} | c@{\hspace{0.8em}} c@{\hspace{0.8em}} c@{\hspace{0.8em}} c@{\hspace{0.8em}} c@{\hspace{0.8em}} c@{\hspace{0.8em}} c@{\hspace{0.8em}}}
  %\begin{tabulary}{.8\linewidth}{c | c c c c c c c}
   & cup & teapot & bowl & bottle & mug & tlt ppr & sphere \\ \hline
  Baseline & 0.227 & \textbf{0.160} & 0.375 & 0.237 & 0.121 & \textbf{0.224} & \textbf{0.317} \\
  Tree & \textbf{0.137} & 0.266 & \textbf{0.354} & \textbf{0.097} & \textbf{0.078} & 0.299 & 0.326 \\
  %\end{tabulary}
  \end{tabular}
  %\vspace{.3cm}
  \caption{\label{tab:hist_dist} Test-time histogram distances $\in [0, 1]$ to ground truth histogram from training. Distances are from the last common iteration in Fig.~\ref{fig:tree_vs_random}. Columns correspond to Fig.~\ref{fig:hists}.}
  \end{center}
  \vspace{-1mm}
\end{table}


% One-column format, all 3 figs
\begin{figure*}[thpb]
  \begin{center}
    \begin{subfigure}{.32\textwidth}
      %\includegraphics[width=\linewidth]{imgs/tree_vs_random_innerProd}
      \includegraphics[width=\linewidth]{tree_vs_random_innerProd-eps-converted-to}
      \vspace{-7mm}
      \caption{}
      \label{fig:tree_vs_random}
    \end{subfigure}
    %
    \begin{subfigure}{.32\textwidth}
      %\includegraphics[width=\linewidth]{imgs/dist_vs_iter_mug}
      \includegraphics[width=\linewidth]{dist_vs_iter_mug-eps-converted-to}
      \vspace{-7mm}
      \caption{}
      \label{fig:dist_vs_iter}
    \end{subfigure}
    %
    \begin{subfigure}{.32\textwidth}
      %\includegraphics[width=\linewidth]{imgs/rewards_mug_150sims_2016-09-12-04-29-19.eps}
      \includegraphics[width=\linewidth]{rewards_mug_150sims_2016-09-12-04-29-19-eps-converted-to}
      \vspace{-7mm}
      \caption{}
      \label{fig:rewards}
    \end{subfigure}
  \end{center}
  \vspace{-7mm}
  \caption{(a). Physics engine results. Distance to true class \textit{vs.} iteration, for baseline (dashed) and tree policy (solid). Dots and crosses show hits and misses. Each curve is an object. Tree policy has more hits and the closest distances. (b). Similar format, for different simulations for mug. Each curve is a simulation setting. Error bars are mean and variance in distances to all objects. (c). Rewards vs. simulations for mug; similar for all objects. Each curve is an iteration. Rewards diminish over iterations as unvisited high-probability poses are exhausted.}
  \vspace{-4mm}
\end{figure*}


% TODO formatting: might need to combine this Fig, Fig \ref{fig:dist_vs_iter} and Fig \ref{fig:rewards}, into a full-width image, to stay within 8 page limit.
\begin{comment}
\begin{figure}[thbp]
  \begin{center}
    \includegraphics[width=.6\linewidth]{imgs/tree_vs_random_innerProd}
  \end{center}
  \caption{Distance (y) to true class \textit{vs.} iteration (x), for baseline (dashed) and tree policy (solid). Dots and crosses show hits and misses. Each curve is an object. Tree policy has more hits and the closest distances.}
  \label{fig:tree_vs_random}
\end{figure}
\end{comment}


\begin{figure}[thbp]
  \begin{center}
  \begin{tabular}{c@{\hspace{0.2em}} c@{\hspace{0.2em}} c@{\hspace{0.2em}} c@{\hspace{0.2em}} c@{}}
    \includegraphics[height=\pcdheight]{poses_rviz/teapot_rand_3teapots} &
    \includegraphics[height=\pcdheight]{poses_rviz/mug_rand_3bottles} &
    \includegraphics[height=\pcdheight]{poses_rviz/cup_rand_3mugs} &
    \includegraphics[height=\pcdheight]{poses_rviz/bowl_rand_teapot} &
    \includegraphics[height=\pcdheight]{poses_rviz/toilet_paper_rand_3cups}
    \\
    \includegraphics[height=\pcdheight]{poses_rviz/teapot_tree_3mugs} &
    \includegraphics[height=\pcdheight]{poses_rviz/mug_tree_3mugs} &
    \includegraphics[height=\pcdheight]{poses_rviz/cup_tree_cup} &
    \includegraphics[height=\pcdheight]{poses_rviz/bowl_tree_bowl} &
    \includegraphics[height=\pcdheight]{poses_rviz/toilet_paper_tree_teapot_toilet_toilet}
  \end{tabular}
  \vspace{-4mm}
  \end{center}
  \caption{Poses selected by random baseline (top) and tree (bottom), for teapot, mug, cup, bowl, and toilet paper.}
  \label{fig:poses}
  %\vspace{-3mm}  % Don't need this if set \floatsep, \textfloatsep, and \intextsep to 0pt
\end{figure}


\begin{figure}[thbp]
  \begin{center}
  \begin{tabular}{c@{\hspace{0.2em}} c@{\hspace{0.2em}} c@{\hspace{0.2em}} c@{\hspace{0.2em}} c@{}}
    \includegraphics[height=\pcdheight]{pcds/bottle_rand_2016-09-11-00-42-31} &
    \includegraphics[height=\pcdheight]{pcds/teapot_rand_2016-09-11-01-20-30} &
    \includegraphics[height=\pcdheight]{pcds/mug_rand_2016-09-11-20-32-50} &
    \includegraphics[height=\pcdheight]{pcds/cup_rand_2016-09-11-01-03-23} &
    \includegraphics[height=\pcdheight]{pcds/bowl_rand_2016-09-10-23-26-47}
    \\
    \includegraphics[height=\pcdheight]{pcds/bottle_50sims_2016-09-10-21-46-41} &
    \includegraphics[height=\pcdheight]{pcds/teapot_50sims_2016-09-10-22-35-55} &
    \includegraphics[height=\pcdheight]{pcds/mug_50sims_2016-09-11-01-47-34} &
    \includegraphics[height=\pcdheight]{pcds/cup_50sims_2016-09-09-19-37-54} &
    \includegraphics[height=\pcdheight]{pcds/bowl_50sims_2016-09-10-23-03-21_green}
%sphere_50sims_2016-09-11-02-37-03_white
%sphere_rand_2016-09-11-21-17-53_palePink
%toilet_paper_50sims_2016-09-11-02-05-52
%toilet_paper_rand_2016-09-11-20-57-34
  \end{tabular}
  \vspace{-4mm}
  \end{center}
  \caption{Actual contacts obtained in physics engine from baseline (top) and tree (bottom), for bottle, teapot, mug, cup, and bowl.}
  \label{fig:pcd}
  %\vspace{-6mm}  % Don't need this if set \floatsep, \textfloatsep, and \intextsep to 0pt
\end{figure}


% Tree vs random baseline
% Multirow and hline that only spans part of table
%   http://tex.stackexchange.com/questions/8549/how-can-i-draw-a-horizontal-line-spanning-only-some-of-the-table-cells
\begin{comment}
\begin{table}[thbp]
  \begin{center}
  \begin{tabulary}{.8\linewidth}{c | c c  c  c  c  c  c}
             &  & Iter & 1st Closest & Dist & 2nd & Dist & \# Moves\\ \hline
    \multirow{6}{*}{Bowl} & Baseline & 1 & &  &  & \\
                 &          & 2 & &  &  & \\
                 &          & 3 & &  &  & \\
    \cline{2-8}
                 & Active     & 1 & &  &  & \\
                 &          & 2 & &  &  & \\
                 &          & 3 & &  &  & \\
    \hline
    \multirow{6}{*}{Cup} & Baseline & 1 & &  &  & \\
                 &          & 2 & &  &  & \\
                 &          & 3 & &  &  & \\
    \cline{2-8}
                 & Active     & 1 & &  &  & \\
                 &          & 2 & &  &  & \\
                 &          & 3 & &  &  &
  \end{tabulary}
  %\caption{Comparison of baseline and MCTS. Distance is inner product.}
  \caption{\label{tab:tree_vs_random}}
  \end{center}
\end{table}
\end{comment}


% Cut. Not going to talk about any of these

% Tree policy 50 sims horizon 20, NN inner dist: Other than sphere, other 6 objects do fine. Explain why sphere do bad. Explain why teapot diverges to mug from iter 4 on, teapot could look like mug, but look at the 3D hist plots.

% Now in table \ref{tab:nn}
%State how many contacts were obtained (number of pts in pcd).

% In both the random baseline and tree policy, most poses chosen are at the bottom of the objects, probably because more training poses turned out to have observations at the bottom of objects.


%\red{Might want to do a fixed number of moves per iteration, for fairer comparison.}

%Intuitively, sphere should be the easiest object for a random baseline, because it is symmetric on all sides. This means that all grasps will observe similar data, and it only takes a small set of grasps to recover the true descriptor, a normalized histogram. Even a randomly selected set of wrist poses should have no problem.


% -------------------------------------------------------------------------
\subsection{Recognition Performance}

Fig. \ref{fig:dist_vs_iter} shows recognition in the form of inner product distance to true class, for different iterations and simulations for one object. Distances decrease as iterations increase, since the action executions provide increasingly descriptive histograms at the root of new trees.

The error bars show means and variances in distances to all objects. The difference between the error bars and distances to true class indicate that recognition converges early on, in as few as 2 iterations. That is 10--31 poses, average 25 poses across all simulation settings shown. The number of available actions at each node is 1063, pooled across all 7 objects' trained poses. This shows that the tree successfully finds the few essential poses to fill the most discriminative features in the descriptor.

As simulations per tree increase, distances do not necessarily decrease, nor does recognition accuracy increase. 30--90 simulations perform better than 150--250 simulations. This can be due to our restriction of visiting a pose only once. As the available poses are exhausted, the tree policy cannot find an edge at some nodes and has to switch to rollout policy, by design.
%
This is further reflected in the rewards plot in Fig \ref{fig:rewards}. Before 90 simulations, rewards steadily increase as the tree is more explored; after 90 simulations, deep troughs appear. The troughs are probably due to exhaustion of unvisited poses with high probability on a tree path. Similarly, rewards diminish as iterations increase, when fewer poses are available, %to choose from,
eventually collapsing to a one-node tree in the last iteration where reward is 0.

% Now trying to put these 2 figs in the same fig as fig:tree_vs_random, spanning full page width
\begin{comment}
\begin{figure}[thbp]
  \begin{center}
    \includegraphics[width=.6\linewidth]{imgs/dist_vs_iter_mug}
  \end{center}
  \caption{Distance (y) to true class \textit{vs.} iterations (x), for different simulation settings for mug. Each curve is a simulation setting. Error bars are mean and variance in distances to all objects. Dots and crosses are hits and misses.}
  \label{fig:dist_vs_iter}
\end{figure}


\begin{figure}[thbp]
  \begin{center}
    \includegraphics[width=.6\linewidth]{imgs/rewards_mug_150sims_2016-09-12-04-29-19.eps}
  \end{center}
  \caption{Rewards (y) over time for mug; similar for all objects. x-axis is number of simulations within a tree. Each curve is a iteration. Rewards diminish over iterations as unvisited poses are exhausted.}
  \label{fig:rewards}
\end{figure}
\end{comment}


Table \ref{tab:nn} shows per-iteration predictions and distances for the cup. The cup starts as the third NN and moves up to first NN in iteration 5. It is often reasonably confused with a mug. The baseline always recognized the cup as a mug in all 9 iterations, for all 3 distance metrics.

% Put table caption on same line as TABLE I text
%   http://tex.stackexchange.com/questions/25755/avoiding-newline-after-table-in-caption-while-using-ieeetran
\begin{table}[thbp]
  \begin{center}
  \begin{tabulary}{.8\linewidth}{c | c c c c c c c c}
    Iter		& 1		& 2		& 3		& 4		& 5		& 6		& 7 \\ \hline
    Moves		& 14	& 14	& 17	& 18	& 7		& 5		& 2 \\
    Contacts	& 52	& 61	& 52	& 68	& 25	& 22	& 6 \\
    \hline
    1st NN		& teapot& mug	& mug	& mug	& cup	& cup	& cup \\
    1st dist	& 0.230	& 0.142	& 0.127	& 0.141	& 0.145	& 0.135	& 0.138 \\
    \hline
	2nd NN		& mug	& cup	& cup	& cup	& mug	& mug	& mug \\
	2nd dist	& 0.284	& 0.237	& 0.174	& 0.145	& 0.156	& 0.158	& 0.158 \\
    \hline
	3rd NN		& cup	&teapot	&teapot	&bottle	&bottle	&bottle	&bottle \\
	3rd dist	& 0.321	& 0.248	& 0.229	& 0.232	& 0.231	& 0.225	& 0.227
  \end{tabulary}
  %\vspace{.3cm}
  \caption{\label{tab:nn} Tree result on cup}
  \end{center}
  \vspace{-5mm}
\end{table}



% Important implementation detail but skipping. No room.
%Teapot is the most challenging object, as it does not have many contacts. This happens to objects with a large difference between their xyz dimensions. During run time, we expand the trained wrist poses are expanded at a fixed radius from the object center to avoid collision with object. The resulting volume of wrist poses is spherical, and many of the poses are too far from the object. We encountered this at training time, and we use the object's 3D dimensions to make an ellipsoidal shape instead of spherical, to produce enough contacts. At run time, this may be resolved by a more sophisticated adjustment of wrist poses.


% TODO: bullets 2 & 4, decide whether to do them

% Analysis:

% || Done
% 1. Action selection:
%   Qualitative: Show predicted wrist poses (RViz screenshot) around 2 or 3 objects, in small side-by-side pics in 1 figure.
%   Quantitative: Plot something that shows how many actions MINIMUM do you need, for correct recognition. This needs new code to run and analyze... but is necessary in evaluating the active algorithm

% 2. Recognition from few touches, must address 2 points: i. HOW FEW, ii. how good is recognition (these are the whole goal of this paper):
%   Qualitative: Show 3D histograms (RViz screenshot), for the test object (active_predict.py), and the ground truth object's csv_gz_hists histogram (plot_hist_rviz.py).
%   Quantitative: In fig caption, say SVM recognition probability, say HOW FEW touches
%   Quantitative: Show histogram from 10 touches, vs histogram from 50 touches, for same object. Show that there isn't much change, i.e. conclude histogram is not biased, actions are well selected to reflect a converging histogram even from the start

% || Done
% 3. Show incorrect cases. Teapot vs bowl, cup vs mug, etc

% 4. Confusion matrix? But we only test 5 objects... If not 100% correct, then show conf mat.
%    Run repeated times, like 10 times an object. Then can have meaningful conf mat. Need time... Run on fast computer.

% X|
% 5. Test with different horizons? This should give fewer or more number of actions. But if we just run MCTS + Gazebo for more iterations, that might be the same. So maybe this isn't worth it.


% -------------------------------------------------------------------------
\subsection{Number of Moves for Recognition}

% How many minimum* moves for recognition?

Table \ref{tab:moves} shows the number of moves per iteration for all objects. Boldface shows the iteration in which recognition starts being correct, corresponding to Fig.~\ref{fig:tree_vs_random}. Teapot was correct in iterations 1--3 and diverged to mug. All other objects stayed correct.
%
These are the upper bound moves for recognition, as we only ran recognition after each iteration, not after every move. Most objects were recognized within 16 moves, a large improvement over hundreds in \cite{triangles}.

% Printed by fig_recog_acc.py, set PLOT_MODE=TREE_VS_RANDOM
\begin{table}[thbp]
  \begin{center}
  \begin{tabulary}{.8\linewidth}{c | c c c c c c c c c}
   Iteration &  1 &  2 &  3 &  4 &  5 & 6 & 7 & 8 & 9 \\ \hline
         cup & 14 & 14 & 17 & 18 &  \underline{\textbf{7}} & 5 & 2 & & \\
      teapot & \underline{\textbf{16$^*$}} & 13 & 16 & 15 &  8 & 5 & 4 & 1 & 1 \\
      bottle & \underline{\textbf{16}} & 12 & 18 & 17 &  8 & 5 & 4 & 2 & 1 \\
        bowl & \underline{\textbf{15}} & 14 & 16 & 16 &  7 & 4 & 3 & 1 & \\
         mug & \underline{\textbf{14}} & 14 & 15 & 14 & 11 & 6 & 4 & 1 & \\
toilet paper & \underline{\textbf{13}} & 18 & 15 & 15 &  8 & 5 & 1 & & \\
      sphere & 16 & 13 & 17 & 14 & 10 & 6 & 5 & 3 & 1
  \end{tabulary}
  \caption{\label{tab:moves} Upper bound number of poses to recognize correctly}
  \end{center}
  \vspace{-5mm}
\end{table}


% skip
% Max tree depth (number of wrist poses per iteration) shouldn't affect much, as it's similar to running more iterations.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Real Robot Experiments}

% Emphasize FULLY AUTONOMOUS. Reviewer who didn't read somehow thinks we "tele-operated" the robot at test time.

On the real robot, we compare with a greedy baseline instead of random.
Fig. \ref{fig:real_setup} shows the experiment setup.
We mounted the ReFlex Beta hand on a Baxter. 
An object is held fixed on a table. 
We trained 5 objects (Fig. \ref{fig:real_objs}) on the real robot for active instance-based recognition. Note transparent objects pose significant challenge for vision systems.

% Numbers and logs of all real-robot experiments were recorded in repo triangle_sampling/src/triangle_sampling/sample_baxter/notes_sample_baxter.txt
The mug, bottle, jar, bowl, and glass were each trained with 34, 60, 39, 30, and 50 end-effector poses.
% This integer is printed by active_predict.py
With discretization (0.06 meters in translation, 0.05 in quaternion), this resulted in 138 possible actions at each MCTS node at test time.
The goal is to recognize in considerably fewer poses. This would mean the active selection is able to select poses with discriminating features.

% No space
%\TODO{Optional: insert RViz screenshots of where (square and arrow markers) the poses are for the 5 objects. Can do on home computer, just set obj\_const in plan\_poses.py and run it.}
% No space. This belongs to detailed analysis which I already showed in physics engine results. Don't need again for real robot objects
%\TODO{Optional if have space: Add a figure of RViz 3D histograms of real objects}

At test time, we ran MCTS (Sec.~\ref{sec:mcts}) to actively predict a sequence of end-effector poses. Then, the Baxter arm autonomously moves to those poses, using motion planning in ROS Moveit for collision avoidance.
% The object is added to the collision scene for motion planning.

For a baseline, we compared with greedily selecting the immediate minimizer of the objective, \textit{i.e.} zero step lookahead, equivalent to horizon=1. For tree policy, we used horizon=5. This means tree policy had up to 5 poses per iteration; greedy had 1. We used 20 simulations per iteration for both.
Both were run until the recognition was correct for 3 consecutive iterations, some further until the distances leveled off.
Results are in Table~\ref{tab:real_tree_vs_greedy} and Fig.~\ref{fig:real_tree_vs_greedy}. Example grasps selected by tree policy are in Fig.~\ref{fig:real_grasps}. A footage with per-move distances is in the accompanying video.
%Histogram distances are in Table~\ref{tab:real_hist_dist}.

Some poses selected by either method were not successfully planned, due to joint limits and collisions in the workspace.
Fig.~\ref{fig:real_tree_vs_greedy} x-axis is the raw number of poses selected.
Table~\ref{tab:real_tree_vs_greedy} shows the number of successful moves.

Tree policy recognized in significantly fewer iterations and shorter time in most cases. All objects were first recognized correctly in under 10 moves, significantly fewer than training.
Greedy never recognized the glass correctly, always as mug. Tree policy recognized it twice in a row and then flip-flopped between glass and mug.


% Results were logged in notes_sample_baxter.txt
% Put iterations in table \ref{tab:real_tree_vs_greedy}. To show that greedy takes longer!!!! Even though it recognizes in the first few grasps, it takes many many tree iterations to get to that, because many poses chosen are unfeasible. I had to run it for much longer for it to be able to terminate, took much longer.
\begin{table}[thbp]
  \begin{center}
  \begin{tabular}{c | c@{\hspace{0.8em}} c@{\hspace{0.8em}} | c@{\hspace{0.8em}} c@{\hspace{0.8em}} | c@{\hspace{0.8em}} c@{\hspace{0.8em}} c@{\hspace{0.8em}} | c@{\hspace{0.8em}} c@{\hspace{0.8em}} c@{\hspace{0.8em}}}
    Object	 & \multicolumn{4}{c}{\# Iters until Correct}  & \multicolumn{6}{c}{\# Moves until Correct} \\
     & T1 & T3 & G1 & G3 & T1 & T3 & T33 & G1 & G3 & G33 \\ \hline
    jar		& \textbf{1} & \textbf{3} & 11 & 20 &
              \textbf{2} & 10 & \textbf{10} & 
              6 & 8 & 14 \\
    bottle	& \textbf{1} & \textbf{3} & 5 & 26 &
              \textbf{2} & 8 & \textbf{8} & 
              \textbf{2} & 4 & 12 \\
    mug		& 4 & \textbf{6} & \textbf{3} & 9 &
              9 & 12 & 12 & 
              \textbf{1} & 3 & \textbf{3} \\
    bowl	& \textbf{3} & - & 9 & - &
            5 & 8 & - &
            \textbf{3} & 5 & - \\
    glass	& \textbf{2} & - & - & - &
              \textbf{7} & - & - &
              - & - & -
  \end{tabular}
  \caption{\label{tab:real_tree_vs_greedy} Number of iterations and move it took to recognize correctly on real robot. T: tree policy, G: greedy. T1/G1: first time SVM recognizing correctly; T3/G3: SVM correct 3 times in a row; T33/G33: all 3 metrics correct 3 times in a row. - denotes never.}
  \end{center}
  \vspace{-1mm}
\end{table}


% Won't show this table, because it's unfair for tree policy. The quantitative data is already in Fig real_tree_vs_greedy line plots, you really don't need this.
\begin{comment}
\begin{table}[thpb]
  \begin{center}
  \begin{tabular}{c@{\hspace{0.4em}} | c@{\hspace{0.8em}} c@{\hspace{0.8em}} c@{\hspace{0.8em}} c@{\hspace{0.8em}} c@{\hspace{0.8em}}}
  \\
   & jar & bottle & mug & bowl & glass \\ \hline
  Baseline &  \\
  Tree &  \\
  \end{tabular}
  \caption{\label{tab:real_hist_dist} Test-time histogram distances $\in [0, 1]$ to ground truth histogram from training.} % Distances are from the last common iteration.}
  \end{center}
  \vspace{-5mm}
\end{table}
\end{comment}


\begin{figure*}[thb]
  \begin{center}
    \begin{subfigure}{.38\textwidth}
      %\includegraphics[height=3.5cm,trim={0cm 7cm 0cm 18cm},clip]{real/IMG_0889_brighten_unsaturate_resized}  % Use .35\textwidth subfigure
      \includegraphics[height=3.5cm]{real/IMG_0889_brighten_unsaturate_crop_brighten_resized}  % Use .38\textwidth subfigure
      %\vspace{-7mm}
      \caption{}
      \label{fig:real_objs}
    \end{subfigure}
    %
    \begin{subfigure}{.28\textwidth}
      % Trimming image: trim={left lower right upper}
      \includegraphics[height=3.5cm,trim={0cm 0cm 0cm 1cm},clip]{real/setup_IMG_0695_resized}
      %\vspace{-7mm}
      \caption{}
      \label{fig:real_setup}
    \end{subfigure}
    %
    \begin{subfigure}{.32\textwidth}
      %\includegraphics[width=\linewidth]{real/tree_vs_greedy_bx}
      \includegraphics[width=\linewidth]{tree_vs_greedy_bx-eps-converted-to}
      \vspace{-7mm}
      \caption{}
      \label{fig:real_tree_vs_greedy}
    \end{subfigure}
  \end{center}
  \vspace{-7mm}
  \caption{(a). Mug, bottle, jar, bowl, glass, used for real robot experiments. (b). Experiment setup. (c). Real robot results. Distance to true class \textit{vs.} number of poses, for baseline (dashed) and tree policy (solid). Dots and crosses show hits and misses. Each curve is an object.}
  \vspace{-5mm}
\end{figure*}


% Now combine these into one full-page-width figure
\begin{comment}
\begin{figure}[thbp]
  \begin{center}
    \includegraphics[width=\linewidth]{real/real_objs}
  \end{center}
  \caption{Mug, bottle, jar, bowl, glass, used for real robot experiments.}
  \label{fig:real_objs}
\end{figure}

\begin{figure}[thbp]
  \begin{center}
    \vspace{2cm}
  \end{center}
  \caption{Experiment setup}
  \label{fig:real_setup}
\end{figure}

\begin{figure}[thbp]
  \begin{center}
    \includegraphics[width=.6\linewidth]{real/tree_vs_greedy_bx}
  \end{center}
  \caption{Real robot results. Distance to true class \textit{vs.} number of moves, for baseline (dashed) and tree policy (solid). Dots and crosses show hits and misses. Each curve is an object.}
  \label{fig:real_tree_vs_greedy}
\end{figure}
\end{comment}


% Photos of example real-robot grasps. Really important to convince them we did good real robot experiments.
\begin{figure}[thbp]
  \begin{center}
  \hspace{-5mm}
  \begin{tabular}{c@{\hspace{0.2em}} c@{\hspace{0.2em}} c@{\hspace{0.2em}} c@{\hspace{0.2em}} c@{}}
    \includegraphics[height=\graspheight]{real_grasps/jar35_IMG_0726_resize} &
    \includegraphics[height=\graspheight]{real_grasps/bottle44_IMG_0740_resize} &
    \includegraphics[height=\graspheight]{real_grasps/mug37_IMG_0765_resize} &
    \includegraphics[height=\graspheight]{real_grasps/bowl39_IMG_0425_resize} &
    \includegraphics[height=\graspheight]{real_grasps/glass46_IMG_0876_resize}
    \\
    \includegraphics[height=\graspheight]{real_grasps/jar35_IMG_0734_resize} &
    \includegraphics[height=\graspheight]{real_grasps/bottle44_IMG_0748_resize} &
    \includegraphics[height=\graspheight]{real_grasps/mug37_IMG_0786_resize} &
    \includegraphics[height=\graspheight]{real_grasps/bowl39_IMG_0835_resize} &
    \includegraphics[height=\graspheight]{real_grasps/glass46_IMG_0879_resize}
  \end{tabular}
  \vspace{-4mm}
  \end{center}
  \caption{Real robot actions selected by tree policy at test time.}
  \label{fig:real_grasps}
  %\vspace{-5mm}  % Don't need this if set \floatsep, \textfloatsep, and \intextsep to 0pt
\end{figure}


% Extra. Maybe don't have room to say this. Focus on reporting the key success results, leave discussion for a longer paper if ever write one - like my thesis.
% The most interesting finding is that as horizon decrease, simulations should decrease as well. Otherwise, the actions are overly explored and all reach the horizon many times, eventually all having a saturated reward of the same value. Then the optimal action is essentially chosen at uniform random.


%The results can be improved by using a mobile robot that can train all parts of an object without unfeasibility.


%Many poses were unfeasible to plan due to workspace size and collisions. A robot with smaller arms and larger workspace, such as a mobile robot, can compensate for this. Another option is to plan with both arms, each equipped with a tactile hand.

%Improvements:
%For improvements, the biggest bottleneck is the unfeasible poses. Our movement cost is a simple distance between consecutive tree nodes, and it does not account for nodes that are unfeasible. In addition, the Euclidean distance of positions may not correspond to the distance between joint positions. Such costs are robot platform-dependent but can be built into the tree search.

%and it does not take robot planning and joint position difference into account. Such costs are robot platform-dependent.


% -------------------------------------------------------------------------
\subsection{Running Time}
\label{sec:running_time}

The running time for the tree search is directly proportional to horizon $T$. Each tree simulation takes 0.5 seconds for $T=20$, 0.1 seconds for $T=5$, and 0.02 seconds for $T=1$.
Times reported are on an Intel Xeon 3.6GHz quad-core desktop simultaneously running the rest of the experiment software.
It can be improved by array access.
% Our computation code is not optimized. It can be sped up dramatically by efficient array access. 
%The action time in physics engine is $\sim$5 seconds per move.

Even though the greedy approach ($T=1$) took shorter time per iteration, it took many more iterations before correct recognition (Table~\ref{tab:real_tree_vs_greedy}).
The reason is that $T=1$ generates only one pose per iteration, and when the pose is unfeasible due to joint limits or collision, the iteration is wasted. 
Overall, the tree policy took significantly shorter time.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Conclusion}

We described an algorithm for actively selecting a sequence of end-effector poses for the objective of confident object recognition. We formulated the problem as a MDP and associated tactile observations with relative wrist poses in training, which allows the next desired actions to be predicted by observations alone at test time. The method outperforms greedily selected poses in a physics engine and on a real robot.

An improvement to optimize recognition even more directly is to select actions that would produce the most salient features in the descriptor.
Analysis methods exist for finding the most discriminative features in a classifier. The histogram descriptor makes this easy; each feature is simply a bin, which we already use as discretized observations $z$. To select the most salient action, simply select $z_{t+1}$ that maximizes saliency in addition to recognition confidence.

%The main weakness is that the formulation does not take arm-dependent joint-motor movement costs and workspace into account, leading to many unfeasible poses on a Baxter. Much existing work for defined manipulation tasks take the workspace and specific arm configurations into account. These ideas can be borrowed to factor the constraints into our movement costs. However, with them come more time-consuming platform-dependent training, including actual motion planning for a specific arm, which would increase training time by many folds.
%Secondary improvements are the speed during tree search and a formulation that has explicit diminishing returns, such as one that satisfies the adaptive submodularity property.


% Summarize strong points:
% || independent of object pose
% quick to simulate
% ...
% || Train from synthetic object and worked on real objects??? (if it happens...)

% Future work / weaknesses
% || An objective function that satisfies diminishing returns such as the adaptive submodularity property
% Consider actual physical movement cost (joint trajectory distance) of the arm in c_m(a_t), not just use the Euclidean distance for translation and angle for rotation.
% speedup
% how to make similar / challenging objects work better
% ...


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Preloaded format reference

\begin{comment}

\begin{table}[h]
\caption{An Example of a Table}
\label{table_example}
\begin{center}
\begin{tabular}{|c||c|}
\hline
One & Two\\
\hline
Three & Four\\
\hline
\end{tabular}
\end{center}
\end{table}


   \begin{figure}[thpb]
      \centering
      \framebox{\parbox{3in}{We suggest that you use a text box to insert a graphic (which is ideally a 300 dpi TIFF or EPS file, with all fonts embedded) because, in an document, this method is somewhat more stable than directly inserting a picture.
}}
      %\includegraphics[scale=1.0]{figurefile}
      \caption{}
      \label{figurelabel}
   \end{figure}


\addtolength{\textheight}{-10cm}   % This command serves to balance the column lengths
                                  % on the last page of the document manually. It shortens
                                  % the textheight of the last page by a suitable amount.
                                  % This command does not take effect until the next page
                                  % so it should come on the page before the last. Make
                                  % sure that you do not shorten the textheight too much.

\end{comment}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{comment}
% Notes from proofreaders

Stephanie:
||- Title should reflect the problem. If you don't focus on recognition, is the main problem active selection then? Title should reflect that.

TODO
- Intro needs to clearly state the problem and contribution

||- The phrase "active algorithm" is very unclear and non-descriptive. Active has to be FOR something. Is it active learning, active perception, active selection? Have to say it.

- Select: grasps vs hand pose vs wrist pose, choose one and use the term consistently
> Eliminate "hand pose", just use "wrist pose". `.` hand pose can mean finger joint positions, or finger gait, which we're not doing.
Use "grasp" only in the context of "we predict wrist poses at which the robot makes enclosure grasps." Use "grasp" to refer to the actual enclosure that the end-effector makes around an object. When talking about the algorithm and what we predict, use "wrist poses", not "grasps".


Kostas:

The only  confusing thing to the reader is V.A
I understand that the setup section is general and does not apply only
to the simulation experiments.
||It is even more confusing because the word simulation is used interchangeably.

I would suggest that you take V.A and you make it a separate section
IV.D  Implementation (it fits directly
under the small example).

Unless I understood it wrong and it applies only to the physical engine setup.

||PS: Also just call V. Analysis Using a Physics Engine


\end{comment}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\bibliographystyle{IEEEtran}
\bibliography{root}

\end{document}
