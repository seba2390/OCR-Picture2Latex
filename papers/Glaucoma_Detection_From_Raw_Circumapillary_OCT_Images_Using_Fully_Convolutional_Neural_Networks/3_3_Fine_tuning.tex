\subsection{Learning by fine tuning} \label{subsec: Deep_learning}

Deeper architectures networks could improve the models' performance, but a large number of images annotated by experts would be necessary for training a deep CNN from scratch. For this reason, we propose in this section the use of fine-tuning techniques \cite{hoo2016}, which allows training CNNs with greater depth using the weights pre-trained on large databases, without the need to train from scratch. In particular, we applied a deep fine-tuning \cite{tajbakhsh2016} strategy to transfer the wide knowledge acquired by several state-of-the-art networks, such as VGG16, VGG19, InceptionV3, Xception and ResNet, when they were trained on the large \textit{ImageNet} data set. Attending to the small database used in this work, only the coefficients of the last convolutional blocks (4 and 5) were retrained with the specific knowledge corresponding to the circumpapillary OCT images. The rest of coefficients were frozen with the values of the weights pre-trained with 14 million of natural images contained in \textit{Imagenet} database.

Additionally, similarly to the proposed learning from scratch strategy, an empirical exploration of different hyper-parameters and top-model architectures was considered for all networks. It is important to notice that InceptionV3, Xception and ResNet architectures reported a poor performance due to their extensive depth (42, 36 and 53 convolutional layers, respectively). However, the family of VGG architectures achieved the best performance, in line with the findings in the literature \cite{gomez2019automatic}. Specifically, VGG16 base model is composed of five convolutional blocks according to Fig. \ref{fig:VGG16}, where blue boxes correspond to convolutional layers activated with \textit{ReLu} functions and red-grey boxes represent max-pooling layers. VGG19 base model is composed of the same architecture, but including an extra convolutional layer in the last three blocks. 

A top model composed of global max pooling and dropout layers with a coefficient of 0.4, followed by a softmax layer with two neurons, provided the best model performance when VGG architectures were fine-tuned (see Fig. \ref{fig:VGG16}). Regarding the selection of hyper-parameters combination, Adadelta optimizer with a learning rate of 0.001 reported the best learning curves when the model was forward, and backward, propagated during 125 epochs with a batch size of 16, trying to minimise the binary cross-entropy loss function.  

Note that an initial down-sampling $\times0.5$ of the original images was necessary to alleviate the GPU memory problems during the training phase. Besides, replicating $\times3$ the channels of the grey-scale was necessary to adapt the input images in order to fine tune the CNNs. Data augmentation (DA) techniques with a factor of 0.2 were also considered.
 

\begin{figure}[h]
\centering
\includegraphics[width=8.5cm]{Figures/VGG16architecture.pdf}
\caption{Network architecture used to discern between glaucomatous and healthy OCT samples by fine-tuning the VGG16 base model. Note that numeric values of the filters are correctly defined in the image, although they do not correspond to the representation size of the boxes due to space problems.}
\label{fig:VGG16}
\end{figure}

