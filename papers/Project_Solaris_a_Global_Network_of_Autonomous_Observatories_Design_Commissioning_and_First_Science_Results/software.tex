% !TEX root = Solaris_PASP.tex
\section{Software architecture}
\label{sec:SoftwareArchitecture}
\subsection{Existing solutions}

In the astronomical domain there already exist solutions that target the needs of astronomical observatories, but although they have their usage, an end-to-end solution for managing a global network of robotic telescopes has not been their aim. 

\subsubsection{Communication Protocols and Platforms}
The predominant solution currently used by the device manufacturers is ASCOM \citep{2002SASS...21...39D}, a many-to-many, language independent platform supported by astronomy for Windows-based computers. Smaller in terms of available devices, INDI, Instrument-Neutral Distributed Interface, a library working with POSIX operating systems is also worth mentioning. One of the advantages of INDI was the development of an XML-based protocol. Both, ASCOM and INDI, aim at providing the lower layer of communication with the astronomical observatory components, but focus on the core components, such as: mount, telescope, camera, filter wheel, etc., and not the auxiliary components like: power distribution units, uninterruptible power supplies, weather stations, environmental monitors, etc. that are required for robotic operation of an observatory. This means that currently most of the hardware that is required for an observatory to work in a fully robotic mode has custom communication mechanisms and the main task in programming such mode lays on integrating these hardware components into a system.

\subsubsection{Management Systems}

On top of the aforementioned platforms higher level solutions are based. Describing them is not the purpose of this paper, but it may be valid to point their characteristics and compare them to the requirements of Project Solaris. Software such as MaximDL\footnote{http://diffractionlimited.com/product/maxim-dl/} \citep{2004S&T...108d..91W} or ACP\footnote{http://acp.dc3.com/index2.html} \citep{2000IAPPP..80...13D} works as one-stop program that is installed on a machine connected to all the devices the observatory controls and relies on and provides the means of interacting with them. In Project Solaris the required level of operation has been defined as such that allows the software to work as-a-service/as-a-daemon and retrieve the observing plans, execute them if possible and transfer the resulting data to the headquarters located in Toru\'n. This means that the software on-site should work as a client to the service that feeds it with observing programs, but a client that is fully aware of the environmental situation and can decide on executing an observation under safe local conditions. It must not be human-operated with the exception of hardware testing or functionality/performance analysis.
Remote Telescope System, RTS2, a Linux-based software is a good example of software that can allows for operating a remote observatory, automate the process of executing observation programs, etc. \citep{2010AdAst2010E..88K}. However, at the time of software selection for Project Solaris RTS2 did not have most of the hardware components described in the previous sections supported. Adding to that, ASA mount with its accessories (i.e. focuser, mirror covers and field derotator), came with a Windows-based software system, Autoslew™ and was fully available through ASCOM.


\subsection{Our approach}

In Project Solaris after an in-depth research and analysis of the available options a dedicated system has been developed with the aim to satisfy the requirements described in section \label{sec:SystemComponents}. The platform of choice was ASCOM as most of the core components had fully-functional drivers on this platform and all were accessible on Windows.


Relying on that a layered architecture was proposed as in Fig. \ref{fig:architecture}. There have been identified 5 layers required for robust operation of a single observatory and connecting it to the global network. They will be described in detail further in the paper. Starting with ASCOM (as the necessary component to communicate with components from ASA) we identified software requirements for developing a solution. Microsoft Windows Server platform was selected as an operating system of choice. Thanks to Microsoft Imagine (formerly DreamSpark)  subscription for STEM institutions all the components such as operating systems, databases, toolkits and platforms were freely available. The overall software components are presented in Table \ref{tab:soft_components}.
	
\begin{deluxetable}{rl}	
%\tablewidth{0pt}	
\tabletypesize{\scriptsize}
\tablecaption{Software components for creating architecture for Project Solaris.}			
\tablehead{\colhead{System Component} & \colhead{Selected Option}}			
\startdata			
Operating System	&	Microsoft Windows Server 2008 R2	\\
Programming Platform	&	.NET, Microsoft Robotics, Microsoft Azure	\\
Programming Languages	&	C\#, C++, ES6, TypeScript	\\
Standards \& Protocols	&	FITS, XML, Json, OData, Web Sockets	\\
Security	&	X.509, OAuth 2.0, OpenID	\\
Persistence Layer	&	Microsoft SQL Server 2008 R2	
\enddata			
\label{tab:soft_components}			
\end{deluxetable}			

Devices were to be accessible through a low-level interface via asynchronous drivers. These were consumed by software that contained the logic to operate securely the component, allowed for plug-in based utilization of different drivers for devices of the same kind, and performed all the required operation for fault-handling: when allowed – fault tolerance, when disallowed graceful failure. This layered architecture is depicted in Fig. \ref{fig:architecture}. To cover for these features, Microsoft Robotics platform was selected. At the time it was the only mature platform to provide for creating robots. And in the case of Project Solaris the requirement for an observatory to operate robotically was mandatory.

\begin{figure}[ht]
\centering
%\includegraphics[width = \columnwidth, viewport = 200 220 620 420, clip = true]{img/PASP-Architecture.eps}
\includegraphics[width = \columnwidth]{img/PASP-Architecture.eps}
\caption{The architecture proposed for Project Solaris that defines the boundaries and specifies the control domains.}
\label{fig:architecture}
\end{figure}

The main components of Microsoft Robotics that provided for asynchronous operation and decoupling of the devices operation were CCR and DDS (Concurrency and Coordination Runtime and Decentralized Software Services, respectively). They formed a lightweight, easy-to-program environment for creating services (DDS) that were performing their operation without blocking (CCR) which means that all the components could be orchestrated together rather than operated separately until they finish the operation. Furthermore it meant that the services run in isolation (DDS) and all the problems could be diagnosed and addressed separately and the system was able to save time of operation even if it was only seconds per few operations, as eventually it adds up to hours.


\begin{figure}[htb!]
\centering
\includegraphics[width = \columnwidth]{img/PASP-Abot-Architecture.eps}
\caption{ Final architecture of a system for Project Solaris.}
\label{fig:final_architecture}
\end{figure}


Finally, the architecture was updated to include this robotic service as a separate layer as in Fig. \ref{fig:final_architecture}. The figure shows three possible kinds of robotic services. They represent the three identified elements:
\begin{enumerate}
\item A service that relies on a hardware component – the most common element, e.g. mount, camera, etc.
\item A service that relies on a software component – for example a service that communicates with the database, or a service that performs auxiliary decoration of the metadata for the captured frame.
\item A service that only interacts with other services – this will be explained in more detail further in the text, but basically it describes a hub, a service that gathers other services of a specific kind and provides an emergent functionality on top of them.
\end{enumerate}

One can think of the ladder of layers as: a device is accessible from the system through the driver. The driver is consumed by a software device that does not know the specifics of the device, but knows its functionality and can operate it securely and safely. A robotic service utilizes software device to provide the means of isolated, asynchronous and fault-tolerant operation as well as communication with other robotic services. A robotic service is also exposed securely to the higher level, external cloud service by a broker service. The cloud service itself represents the unifying component (a server) for all observatories (clients) in a global client-server architecture. 
Moreover, cloud service also has the means for interacting with even higher level components. It is designed for providing plugins of operation (i.e. substituting schedulers, providing specific persistence and logging mechanisms) as well as allowing for up-to-date data presentation and even user interaction. This level of communication, rather than communicating directly with the observatories has a benefit in that the direct links to the observatories are a high-value resource and they should not be extensively used. 

\subsubsection{Single Observatory}
From Fig. \ref{fig:final_architecture} one can see that a single observatory contains multiple layers as well as is modular by design. Moreover, the requirement to perform its operation robotically, a single observatory consists of multiple various components: hardware-bound, software-resource-bound and operation-only components implemented as isolated services. Apart from the so-called core components that provide the functionality to actually perform the observational task the wealth of components is required mainly to secure the operation of a single observatory and protect the equipment. The full list is presented in Table \ref{tab:protocols}. Even though the connection to the observatory may be established using Ethernet, the actual network is a private one and is bound within a single observatory where possible. For example, the access to the UPSes and PDUs is accomplished with a network connection, but the connection doesn't go outside the subnet. It means that the actual software robotizing the work of a single telescope is deployed at a site. What is, however, deployed remotely from the site, is the global scheduler which allows for serving as an overlord tasking all the telescopes.


\subsubsection{Software Components}
The components in each of the observatories in Project Solaris were divided into core components, basic devices that are required to provide the main functionality of the observatory: generating images; sensor components that are required for the observatory to receive timely information about the local environmental conditions and to protect the system; managing components that are components combining other components to provide a functionality on top of them.

\begin{deluxetable*}{lcccc}		
\tabletypesize{\scriptsize}				
%\tablewidth{0pt}			
\tablecaption{Solaris Network site summary}									
\tablehead{\colhead{Component} & \colhead{Manufacturer} & \colhead{Interface}& \colhead{Protocol} & \colhead{Driver}}									
\startdata									
Mount	&	ASA	&	USB	&	n/a	&	ASCOM	\\
Focuser	&	ASA	&	USB	&	n/a	&	ASCOM	\\
Mirror Covers	&	ASA	&	USB	&	n/a	&	ASCOM	\\
Field Derotator	&	ASA	&	USB	&	n/a	&	ASCOM	\\
Dome	&	Baader Planetarium	&	Serial Port	&	text-based	&	custom	\\
PDU	&	Neol, APC	&	HTTP /Telnet	&	n/a	&	custom	\\
Camera	&	Andor	&	USB	&	n/a	&	custom	\\
Filter Wheel	&	Finger Lakes Instruments	&	USB	&	n/a	&	custom	\\
FlatField Screen	&	Alnitak	&	Serial Port	&	n/a	&	custom	\\
FlatField Mount	&	Unisar	&	Serial Port	&	n/a	&	custom	\\
GPS	&	Meinberg	&	PCIE	&	n/a	&	custom	\\
UPS	&	Eaton, APC, IntelliPower	&	TCP/IP	&	SNMP	&	custom	\\
Weather Station (dome-hardwired)	&	Reinhardt	&	Serial Port	&	text-based	&	custom	\\
Weather Station	&	Vaisala	&	Serial Port	&	text-based	&	custom	\\
ObservatoryWatch	&	Cilium Engineering	&	Serial Port	&	text-based	&	custom	\\
2PiSky	&	Cilium Engineering	&	TCP/IP	&	text-based	&	custom
\enddata									
\label{tab:protocols}									
\end{deluxetable*}									

\subsubsection{Core Components}
\paragraph{Mount and accessories.}
ASA DDM160 mount is accessible via Autoslew, a proprietary window application that runs on Windows. The application also starts an ASCOM server allowing for a programmatic access to the mount. Alongside, an additional window application, ACC, is started that allows for manipulation of focuser position, telescope mirror covers and, if present, field rotator. The devices themselves are connected to the computer via a USB cable. 
The application is easy to use, but in Solaris we want to have the possibility to use the devices as windowless services that was impossible for the mount. Autoslew™ had to be always open. It required administrative privileges to work which required to set up a dedicated, administrative account that would have its window session always open. 
\paragraph{Dome.}
AllSky Baader Dome is accessible from the system via a serial port connection and a simple one-line description protocol to inform about the state of the four segments: if they are open, closed or in the intermediate state. The domes have also implemented a safety mechanism when used in so-called automatic mode that requires a computer to request the dome’s state every period of time. If the request is not issued – the dome closes. 
\paragraph{PDU.}
Programmable Power Distribution Units, or PDUs for short, apart from being power sockets for all the other devices provide a way to include the information of the power distribution in the system. Programmable PDUs also allow for setting up each PDU outlet state. In Solaris-1, Solaris-2, and Solaris-4 Neol PDUs are used that have their built-in http server which allows for viewing and modifying outlet states. Solaris-3 utilizes an ACP PDU that is accessible via Telnet. As there were no representation of such a device in ASCOM for Project Solaris there was a custom driver implemented.
\paragraph{Camera.}
Project Solaris uses in all observatories Andor iKon cameras connected through a USB cable. Andor provides multiple driver implementations in C, C++, and in .NET (C\# and Visual Basic), yet all the implementations are simple wrappers upon C language library. The C driver gives a list of possible methods to be invoked by the camera. We have developed our own implementation of a higher level, object-oriented C\# driver wrapping the C one that allows for the discovery of the camera connected to the computer, initializes all the required features and provides means of finalization and disposal.
\paragraph{Filter Wheel.}
Finger Lakes Instruments (FLI) CFW-3-12 filter wheel is connected with a USB and accessed through a C library upon which a C\# wrapper was created. 
\paragraph{Flatfield screen and its mount.}
The observatories for Project Solaris were designed to be compact units. It, however, prevented from placing a flatfield screen high for the telescope to look directly at the screen. Because of that a movable flatfield screen mount was designed for the setup that could move up when the dome is open, but normally would be in its bottom position for the dome to safely close. Both of the devices are connected with a USB cable and programmed using dedicated drivers. In Project Solaris every night sky flats are taken and the flatfield screen has its purpose in determining the camera shutter model and camera linearity model.
\paragraph{GPS.}
In Project Solaris Meinberg GPS is used to receive signals from the camera that inform about the precise times of shutter opening and closing. Meinberg provides a C library to intercept the signals from the device upon which a C\# implementation has been developed for Project Solaris. The GPS card records times of two events: end of shutter opening and beginning of shutter closing. Therefore, the precise mid-expoure time can be computed. Both events are saved in the FITS' headers.

\subsubsection{Sensor Components.}
Sensor components are another set of components in Project Solaris. As the core components focus on the functionality of the system, sensors, on the other hand, focus on securing the system’s operation by constantly monitoring the environmental conditions. Redundancy of the sensors is one of the sought features that would help in keeping the system working even if one of the sensors would break. 
The information from the sensors is stored in a local database with insert times from 15 seconds to 5 minutes. That allows to identify the cause of the problem when it occurs even after it happened.
To incorporate the notion of good or bad conditions sensor components are limited in the way that every property that they describe can have limits. When the values of the sensors are in the range of the limits a component reports good observing conditions. When, however, the values exceed the limits – the component will report bad observing conditions. Limits for any component can be specified in configuration files and using .NET built-in reflection mechanism it is robust enough to work independent of the value type.
\paragraph{UPS.}
Another, after PDUs, set of devices determining safety in terms of power are uninterruptible power systems, UPSes. UPSes used in Project Solaris are accessible via Ethernet providing an IP address and satisfying SNMP protocol. 
\paragraph{Vaisala Weather Station.}
Vaisala is the primary source of information about the ambient conditions. It is connected via a serial port and sends messages in a simple text format. 
\paragraph{Reinhardt Weather Station.}
Reinhardt Weather Station is the second source of information about the ambient conditions. Similarly to Vaisala, it is connected to the main computer with a serial port and provides the data in a easy to parse text format.
\paragraph{ObservatoryWatch.}
ObservatoryWatch is a system that is a heterogeneous hub for various sensors installed inside and outside the dome built upon an industrial grade PLC that provides low-level security mechanisms for the observatory. ObservatoryWatch is connected to the computer via a serial port and provides a simple text-based protocol accessed through a dedicated C\# library.
\paragraph{2$\pi$Sky.}
All sky camera used in Project Solaris, 2$\pi$Sky, is connected to the system via an IP address over the Wi-Fi. It provides two types of messages: an image for the visual presentation of data and a text-based information set about the quality of a priori defined segments of the sky. 
\subsubsection{Managing Components}
These components, called Hubs, do not represent devices, but combine other services to provide an emergent functionality of the services used. They rely on the security mechanisms already implemented in the core components to manipulate them and their state and utilize the information from sensor components to receive the information about the environmental conditions at the observatory.
\paragraph{Device Hub.}
This hub connects to and starts if not yet started all the core components. Above that, Device Hub is responsible for safety and order, if necessary, in which the components are started. Device Hub also reflects the state of each of the components within its own sate.
\paragraph{Sensor Hub.}
Sensor Hub governs the lifecycle of software components’ services. Beyond that Sensor Hub has two tasks: to communicate with a local database to store the information from the sensors with a specified, usually between 15 seconds and 1 minute, period and to include in its own state the synthetic information about the possibility to observe. 
\paragraph{Observatory Manager.}
Observatory Manager is a component performing the core work in the observatory. It connects to all the other hubs and dispatches the weather information from sensor hub to all the others, receives the observing plans and delegates the tasks for all the other hubs.
\paragraph{Observing Hub.}
This hub is responsible for performing the observations. It receives the information about the current observing conditions from Observatory Manager and incorporates the logic to utilize all the core components in the specified order to obtain the requested image.
It is important to say that bias and dark images are no different from a regular astronomical observations in the technical sense. The Camera Service’s (and the driver’s) methods for executing exposure includes the information whether to open the shutter performing so-called light frames or to keep it closed performing dark frames.
\paragraph{Focusing Hub.}
Focusing Hub performs automatic focusing for the system. It takes into account the focuses that are stored in a local database, distance to the latest best focus for a specified filter. The procedure also includes the temperature at the observatory and if there is a change in the temperature above \textbf{2.5} degrees Celsius – a new best focus will be issued. 
\paragraph{Flatfielding Hub.}
Project Solaris uses sky flat fields for the data reduction. These observations are done during the so-called civil twilight, that is prior to the start of the observing night in the evening and after the observing night in the morning. Flatfielding differs from typical optical observatory observing that it must neglect the light sensor readings of ObservatoryWatch. To protect the camera NOVAS library is used to calculate the exact position of the Sun and during the flatfielding procedure and the telescope is pointed to the opposite position.

\subsubsection{Observatory Operation}
A single observatory in Project Solaris works 24/7. That means it needs to have the knowledge of its position, current time, be able to compute the observing night start and end times including twilight times and duration for calibration, and finally have information about the environmental conditions. GPS gives the information about the precise geographic location of the observatory as well as precise measure of time. In Project Solaris United States Naval Observatory’s NOVAS library \citep{Kaplan2012} is used to calculate the general observing night and twilight times and durations. NOVAS also provides the means of observation plan items final check prior to starting the observation.
\paragraph{Observing Plan Execution.}
The overall procedure for observing plan execution is presented in Fig. \ref{fig:Workflow}. Prior to the twilight before the observing night calibration images, i.e. bias and dark frames, are taken. During twilight sky-flats are performed for all the available filters. After the end of the observing night, during morning twilight again the sky-flats are performed and finally darks and biases are taken.
The observation loop is started at astronomical dusk. The Observatory Manager requests the observation plan from the global services’ queue. After that the scheduled observations are analyzed in order to identify the filters that will be used and for them focuser positions are checked. If there is no focuser position for a specified filter as well as temperature focusing observation with high priority is added to the schedule. This loop is repeated until dawn.
\paragraph{Local Data Persistence.}
There are two types of data persisted at the observatory: observational data that is images in FITS format after every exposure and environmental data persistence. Observational data is stored on hard drives in the file hierarchy. Environmental data is stored in a SQL database to be queried when an exceptional situation happens to investigate the reason of its occurrence.
\paragraph{Observations Persistence.}
Every night observations are stored locally on a dedicated high-volume hard-drive in a simple structure: a folder with the name that is year, month day of current UTC date in a format YYYY-MM-DD. In the folders images from each observing nights are stored as they are generated. As all the metadata is stored in the FITS header the files are self-explanatory in terms of location of the image acquisition, position of the object/field of interest, time of acquisition and conditions of the acquisition. After every observing run the images are transferred to the headquarters vault in Toru\'n.
\paragraph{Environmental Information Persistence.}
The information about the sensor readings from all the sensor components in Solaris are stored locally in a Microsoft Sql Server database, 2008 R2 server. This allows for querying the data to obtain the information about the environmental conditions’ change in longer time spans to investigate the reasons of observatory system observation gaps.
\paragraph{Logs and Notifications.}
Each of the software components also logs its operation. The so-called logging sinks can be configured for each component separately, but typically the logs are written in rollback-buffer flat files next to the process running. Their role is to provide administrative information and main purpose is to investigate the causes of crashes – if they happen. 
Moreover, the services are also monitored by yet another process that continuously performs consistency checks. The process is also responsible for sending e-mail notifications to the administrator group if there is a system failure that cannot be resolved by the system itself.

\subsubsection{Network of Observatories}
The observatories in Project Solaris work in a network which allows for quick full light-curve coverage. In terms of architecture it follows the prescription shown in the Fig. \ref{fig:final_architecture}. Every observatory is independent in terms of how it operates and if the environmental conditions allow for operation. It is however the cloud service layer that defines what and when will be observed. In this view an observatory is a client to the cloud service that it registers to and requests instructions on what to observe in terms of observing plans.
\paragraph{Observing Plans Preparation and Distribution.}
In Project Solaris there is a predefined set of approximately 300 objects to be observed, although the set is open. Nevertheless, as the targets are known to be mostly detached eclipsing binary systems the scheduler is optimized to prepare observing plans focusing on the eclipses. 
Upon the request from an observatory the scheduler takes into account a set of so-called observing plan rules that comprise both scientific and technical long-term and established requirements. For instance apart from the eclipse time there are the typical safety requirements: position of the Sun and Moon, and horizontal altitude. Moreover, each object can be assigned a priority that can elevate its observations in the queue. It must be noted, however, that the environmental conditions are further reevaluated at the observatories including short-term weather and environmental conditions.
The request for a new plan originates at the observatory and technically is performed utilizing dedicated Microsoft Azure Queues. This allows for secure and efficient data transfer around the world also relying on the SLA\footnote{Service Level Agreement} levels from the operator. 
\paragraph{Data Transfer And Persistence.}
There is a difference between transferring observing programs and transferring the results of observations. Of course, the structure of the data is different, yet here we would like to emphasize the importance of these. If an observing program is lost or obsolete the observatory can request for another one at almost no cost. With the generated data it is different. These must not be lost, must not be tampered with and as soon as possible should be backed up in some way. 
It is worth noting that when a technology for implementing this feature to the Solaris system was being selected BitTorrent Sync was in its infancy and did not meet the requirements. Now it has evolved in a mature library (renamed to Resilio), and although we have not tested it in any way it may be a great tool for secure and resilient data transfer. Nevertheless, at the time for Project Solaris we have decided to develop our own tool for transferring the data.
There are two dimensions in which the data transfer operates: 1) transfer node types and 2) data type. Node types represent the storage elements of the transfer network. There can be:
\begin{enumerate}
\item Source nodes -- the ones that contain the data to be sent, but do not receive any. In the system these are the observatory computers, where the generated data is being stored.
\item Transfer nodes -- the ones that receive the data and are sources of data for further nodes. 
\item Backup nodes -- the ones that receive the data and may be sources of data for further nodes. 

\end{enumerate}The difference between transfer and backup nodes is in persistence: backup nodes never delete any data that is sent to them, whereas transfer nodes delete the data only when the data up in the chain (i.e. farther from source) is deleted or if it is fully stored on the backup node. Source nodes shall delete the data only if it is completely and securely stored on the backup node. Completely here is related to a single data file. For that reason there is a certainty that a data file will never be deleted from any of the nodes in the chain until it is completely persisted on the backup node.
In the developed system a backup node may also serve as a source node for another backup node. Because of that there is a full mirroring of the data persisted in the backup.
The second dimension, relating to the data types gives the distinction between:
\begin{enumerate}
\item Data – that is the actual FITS files generated during observations. This type is stored in a regular file structure.
\item Metadata – that is the information about the path to the file, name of the file, size of the file and create/update times. This type is stored in a database in a dedicated table.
\end{enumerate}
Data is required for obvious reasons. Metadata serves as the memory information when deciding on transferring, updating and deleting a file from a node.
The nodes provide and consume a resilient, multiplex, two-way \mbox{WS-*} services secured by commercial certificates. To secure the transfer packets from errors a CRC sum is provided with each packet. 



