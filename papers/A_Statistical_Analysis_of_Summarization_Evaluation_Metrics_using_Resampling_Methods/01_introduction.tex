\section{Introduction}
Accurately estimating the quality of a summary is critical for understanding whether one summarization model produces better summaries than another.
Because manually annotating summary quality is costly and time consuming, researchers have developed automatic metrics that approximate human judgments \citep[][among others]{Lin04,TratzHo08,GKVS08,ZPLGME19,DeutschBeRo20}.

Currently, automatic metrics themselves are evaluated by calculating the correlations between their scores and human-annotated quality scores.
The value of a metric's correlation represents how similar its scores are to humans', and one metric is said to be a better approximation of human judgments than another if its correlation is higher.

However, there is no standard practice in summarization for calculating confidence intervals (CIs) for the correlation values or running hypothesis tests on the difference between two metrics' correlations.
This leaves the community in doubt about how effective automatic metrics really are at replicating human judgments
as well as whether the difference between two metrics' correlations is truly reflective of one metric being better than the other or if it is an artifact of random chance.

In this work, we propose methods for calculating CIs and running hypothesis tests for summarization metrics.
After demonstrating the usefulness of our methods through a pair of simulation experiments, we then analyze the results of applying the statistical analyses to a set of summarization metrics and three datasets.

The methods we propose are based on the resampling techniques of bootstrapping \citep{EfronTi93} and permutation \citep{Noreen89}.
Resampling techniques are advantageous because, unlike parametric methods, they do not make assumptions which are invalid in the case of summarization (\S\ref{sec:fisher}; \S\ref{sec:williams}).
Bootstrapping and permutation techniques use a subroutine that samples a new dataset from the original set of observations.
Since the correlation of an evaluation metric to human judgments is a function of \emph{matrices} of values (namely the metric's scores and human annotations for multiple systems across multiple input texts; \S\ref{sec:prelim}), this subroutine must sample new \emph{matrices} in order to generate a new instance, in contrast to standard applications of bootstrapping and permutation that sample vectors of numbers. %, so they are no longer viable.
To that end, we propose three different bootstrapping (\S\ref{sec:ci_bootstrapping}) and permutation (\S\ref{sec:hypo_permutation}) techniques for resampling matrices, each of which makes different assumptions about whether the systems or inputs are constant or variable in the calculation.

In order to evaluate which resampling methods are most appropriate for summarization, we perform two simulations.
The first demonstrates that the bootstrapping resampling technique which assumes both the systems and inputs are variable produces CIs that generalize best to held-out data (\S\ref{sec:ci_simulations}).
The second shows that the permutation test which makes the same assumption has more statistical power than the equivalent bootstrapping method and Williams' test \citep{Williams59}, a parametric hypothesis test that is popular in machine translation (\S\ref{sec:power}).

Finally, we analyze the results of estimating CIs and applying hypothesis testing to a set of summarization metrics using annotations on English single- and multi-document datasets \citep{DangOw08,FKMSR21,BGALN20}.
We find that the CIs for the metrics' correlations are all rather wide, indicating that the summarization community has relatively low certainty in how similarly automatic metrics rank summaries with respect to humans (\S\ref{sec:ci_experiments}).
Additionally, the hypothesis tests reveal that QA\-Eval \citep{DeutschBeRo20} and BERTScore \citep{ZKWWA20} emerge as the best metrics in several of the experimental settings, whereas no other metric consistently achieves statistically better performance than ROUGE \citep[\S\ref{sec:hypo_experiments};][]{Lin04}.

Although we focus on summarization, the techniques we propose can be applied to evaluate automatic evaluation metrics in other text generation tasks, such as machine translation or structure-to-text.
The contributions of this work include
(1) a proposal of methods for calculating CIs and running hypothesis tests for summarization metrics,
(2) simulation experiments that provide evidence for which methods are most appropriate for summarization,
and (3) an analysis of the results of the statistical analyses applied to various summarization metrics on three datasets.