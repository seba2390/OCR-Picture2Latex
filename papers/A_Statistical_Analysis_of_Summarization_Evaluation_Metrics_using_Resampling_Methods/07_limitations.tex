\section{Limitations}
The large widths of the CIs in \S\ref{sec:ci_experiments} and the lack of some statistically significant differences between metrics in \S\ref{sec:hypo_experiments} are directly tied to the size of the datasets that were used in our analyses.
However, to the best of our knowledge, the datasets we used are some of the largest available with annotations of summary quality.
Therefore, the results presented here are our best efforts at accurately measuring the metrics' performances with the data available.
If we had access to larger datasets with more summaries labeled across more systems, we suspect that the scores of the human annotators and automatic metrics would stabilize to the point where the CI widths would narrow and it would be easier to find significant differences between metrics.

Although it is desirable to have larger datasets, collecting them is difficult because obtaining human annotations of summary quality is expensive and prone to noise.
Some studies report having difficulty obtaining high-quality judgments from crowdworkers \citep{GillickLi10,FKMSR21}, whereas others have been successful using the crowdsourced Lightweight Pyramid Score \citep{SGGRPBAD19}, which was used in \citet{BGALN20}.

Then, it is unclear how well our experiments' conclusions will generalize to other datasets with different properties, such as documents coming from different domains or different length summaries.
The experiments in \citet{BGALN20} show that metric performance depends on which dataset you use to evaluate, whether it be TAC or CNN/DM, which is supported by our results.
However, our experiments also show variability in performance within the same dataset when using different quality annotations (see the differences in results between \citet{FKMSR21} and \cite{BGALN20}).
Clearly, more research needs to be done to understand how much of these changes in performance is due to differences in the properties of the input documents and summaries versus how the summaries were annotated.