\section{Preliminaries: Evaluating Metrics}
\label{sec:prelim}
Summarization evaluation metrics are typically used to either argue that a summarization system generates better summaries than another or that an individual summary is better than another for the same input.
How similarly an automatic metric does these two tasks with respect to humans is quantified as follows.

Let $\mathcal{X}$ be an evaluation metric that is used to approximate some ground-truth metric $\mathcal{Z}$.
For example, $\mathcal{X}$ could be ROUGE and $\mathcal{Z}$ could be a human-annotated summary quality score.
The similarity of $\mathcal{X}$ and $\mathcal{Z}$ is evaluated by calculating two different correlation terms on a set of summaries.
First, the summaries from summarization systems $\mathcal{S} = \{S_1, \dots, S_N\}$ on input document(s) $\mathcal{D} = \{D_1, \dots, D_M\}$ are scored using $\mathcal{X}$ and $\mathcal{Z}$.
We refer to these scores as matrices $X, Z \in \mathbb{R}^{N \times M}$ in which $x_i^j$ and $z_i^j$ are the scores of $\mathcal{X}$ and $\mathcal{Z}$ on the summary output by system $S_i$ on input $D_j$.
Then, the correlation between $X$ and $Z$ is calculated at one of the following levels:
\begin{gather*}
    \rsys(X, Z) = \textsc{Corr}\left(\left\{\left(\frac{1}{M}\sum_j x^j_i, \frac{1}{M}\sum_j z^j_i\right)\right\}_{i=1}^N\right)
\end{gather*}
\begin{equation*}
    \rsum(X, Z) = \frac{1}{M} \sum_j \textsc{Corr}\left(\left\{\left(x^j_i, z^j_i\right)\right\}_{i=1}^N\right)
\end{equation*}
where $\textsc{Corr}(\cdot)$ typically calculates the Pearson, Spearman, or Kendall correlation coefficients.\footnote{
For clarity, we will refer to $\rsum$ and $\rsys$ as correlation levels and Pearson, Spearman, and Kendall as correlation coefficients.
}

These two correlations quantify how similarly $\mathcal{X}$ and $\mathcal{Z}$ score systems and individual summaries per-input for systems $\mathcal{S}$ and documents $\mathcal{D}$.
The system-level correlation $\rsys$ calculates the correlation between the scores for each system (equal to the average score across inputs), and the summary-level correlation $\rsum$ calculates an average of the correlations between the scores per-input.\footnote{
    Other definitions for the summary-level correlation have been proposed, including directly calculating the correlation between the scores for all summaries without grouping them by input document \citep{OwczarzakDa11}.
    However, the definition we use is consistent with recent work on evaluation metrics \citep{PeyrardBoGu17,ZPLGME19,BGALN20,DeutschBeRo20}
    Our work can be directly applied to other definitions as well.
}

The correlations $\rsys$ and $\rsum$ are also used to reason about whether $\mathcal{X}$ is a better approximate of $\mathcal{Z}$ than another metric $\mathcal{Y}$ is, typically by showing that $r(X, Z) > r(Y, Z)$ for either $r$.
