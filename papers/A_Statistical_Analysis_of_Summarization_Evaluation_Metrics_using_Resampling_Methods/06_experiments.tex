\section{Summarization Analysis}
We run two experiments that calculate CIs (\S\ref{sec:ci_experiments}) and run hypothesis tests (\S\ref{sec:hypo_experiments}) for many different summarization metrics on the TAC'08 and CNN/DM datasets (\S\ref{sec:simulations}).
Each experiment also includes an analysis which discusses the implications of the results for the summarization community.

The metrics used for experimentation are the following:
AutoSummENG \citep{GKVS08},
BERTScore \citep{ZKWWA20},
BEwT-E \citep{TratzHo08},
METEOR \citep{DenkowskiLa14},
MeMoG \citep{GiannakopoulosKa10},
MoverScore \citep{ZPLGME19},
NPowER \citep{GiannakopoulosKa13},
QAEval \citep{DeutschBeRo20},
ROUGE \citep{Lin04},
and S$^3$ \citep{PeyrardBoGu17}.
We use the metrics' implementations in the SacreROUGE library \citep{DeutschRo20}.

\subsection{Confidence Intervals}
\label{sec:ci_experiments}
Fig.~\ref{fig:ci} shows the 95\% CIs calculated via \textsc{Boot-Both} for $\rhosum$ and $\rhosys$ for each metric calculated using Kendall's $\tau$.
Since ROUGE is the most commonly used metric, the following discussion will mostly focus on its results, however the conclusions largely apply to other metrics as well.

\paragraph{Confidence intervals are large.}
The most apparent observation is that the CIs are rather large, especially for $\rhosys$.
The ROUGE-2 $\rhosys$ CIs are $[.49, .74]$ for TAC'08 and $[-.09, .84]$ on CNN/DM using the annotations from \citet{FKMSR21}.
The wide range of values demonstrates that there is a large amount of uncertainty around how precise the correlations reported in the literature truly are.

The size of the CIs has serious implications for how trustable existing automatic evaluations are.
Since Kendall's $\tau$ is a function of the number of pairs of systems in which the automatic metric and ground-truth agree on their rankings, the metrics' CIs can be translated to upper- and lower-bounds on the number of incorrect rankings.
Specifically, ROUGE-2's system-level CI on \citet{FKMSR21} implies it incorrectly ranks systems with respect to humans 9-54\% of the time.
This means that potentially more than half of the time ROUGE ranks one summarization model higher than another on CNN/DM, it is wrong according to humans, a rather surprising result.
However, it is consistent with similar findings by \citet{RCDN13}, who estimated the same result to be around 37\% for top-performing systems on TAC 2008-2011.

We suspect that the true ranking accuracy of ROUGE (as well as the other metrics) is not likely to be at the extremes of the confidence interval due to the distribution of the bootstrapping samples shown in Fig.~\ref{fig:ci}.
However, this experiment highlights the uncertainty around how well automatic metrics replicate human annotations of summary quality.
An improved ROUGE score does not necessarily mean a model produces better summaries.
Likewise, not improving ROUGE should not disqualify a model from further consideration.
Consequently, researchers should rely less heavily on automatic metrics for determining the quality of summarization models than they currently do.
Instead, the community needs to develop more robust evaluation methodologies, whether it be task-specific downstream evaluations or faster and cheaper human evaluation. 




\paragraph{Comparing CNN/DM annotations.}
The CIs calculated on the annotations by \citet{BGALN20} are in general higher and more narrow than on \citet{FKMSR21}.
We believe this is due to the method of selecting the summaries to be annotated for each of the datasets.
\citet{BGALN20} selected summaries based on a stratified sample of automatic metric scores, whereas \citet{FKMSR21} selected summaries uniformly at random.
Therefore, the summaries in \citet{BGALN20} are likely easier to score (due to a mix of high- and low-quality summaries) and are less representative of the real data distribution than those in \citet{FKMSR21}.


\input{figures/hypothesis-testing/hypo-tests}




\subsection{Hypothesis Testing}
\label{sec:hypo_experiments}
Although nearly all of the CIs for the metrics are overlapping, this does not necessarily mean that no metric is statistically better than another since the differences between two metrics' correlations could be significant.

In Fig.~\ref{fig:hypo}, we report the $p$-values for testing $H_0: \rho(\mathcal{X}, \mathcal{Z}) - \rho(\mathcal{Y}, \mathcal{Z}) \leq 0$ using the \textsc{Perm-Both} permutation test at the system- and summary-levels on TAC'08 and CNN/DM for all possible metric combinations (see \citet{AKSR20} for a discussion about how to interpret $p$-values).
The Bonferroni correction \citep[which lowers the significance level for rejecting each individual null hypothesis such that the probability of making one or more type-I errors is bounded by $\alpha$;][]{Bonferroni36,DBBR17} was applied to test suites grouped by the $\mathcal{X}$ metric at $\alpha = 0.05$.\footnote{
    A version of the results when the correction is applied to $p$-values grouped by the dataset and correlation level pair is included in Appendix~\ref{sec:bonferroni_full}.
}
A significant result means that we conclude that $\rho(\mathcal{X}, \mathcal{Z}) > \rho(\mathcal{Y}, \mathcal{Z})$.

The metrics which are identified as being statistically superior to others at the system-level on TAC'08 and CNN/DM using the annotations from \citet{FKMSR21} are QAEval and BERTScore.
Although they are statistically indistinguishable from each other, QA\-Eval does improve over more metrics than BERTScore does on TAC'08.
At the summary-level, BERTScore has significantly better results than all other metrics.
Overall, none of the other metrics consistently outperform all variants of ROUGE.
Results using either the Spearman or Kendall correlation coefficients are largely consistent with Fig.~\ref{fig:hypo}, although QA\-Eval no longer improves over some metrics, such as ROUGE-2, at the system-level on TAC'08.

The results on the CNN/DM annotations provided by \citet{BGALN20} are less clear.
The ROUGE variants appear to perform well, a conclusion also reached by \citet{BGALN20}.
The hypothesis tests also find that S3 is statistically better than most other metrics.
S3 scores systems using a learned combination of features which includes ROUGE scores, likely explaining this result.
Similarly to the CI experiment, the results on the annotations provided by \citet{BGALN20} and \citet{FKMSR21} are rather different, potentially due to differences in how the datasets were sampled.
\citet{FKMSR21} uniformly sampled summaries to annotate, whereas \citet{BGALN20} sampled them based on their approximate quality scores, so we believe the dataset of \citet{FKMSR21} is more likely to reflect the real data distribution.

