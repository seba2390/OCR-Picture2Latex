\section{Related Work}
\paragraph{Summarization}
CIs and hypothesis testing were applied for summarization evaluation metrics over the years in a relatively inconsistent manner -- if at all.
To the best of our knowledge, the only instances of calculating CIs for summarization metrics is at the system-level using a bootstrapping procedure equivalent to \textsc{Boot-Systems} \citep{RankelCoSc12,DavisCoSc12}.
Some works do perform hypothesis testing, but it is not clear which statistical test was run \citep{TratzHo08, GKVS08}.
Others report whether or not the correlation itself is significantly different from 0 \citep{Lin04}, which does not quantify the strength of the correlation nor allow for comparisons.
Some studies apply Williams' test to compare summarization metrics.
For instance, \citet{Graham15} use it to compare BLEU \citep{PRWZ02} and several variants of ROUGE, and \citet{BGALN20} compares several different metrics at the system-level.
However, our experiments demonstrated in \S\ref{sec:power} that Williams' test has lower power than the suggested methods due to the lower correlation values.

As an alternative to comparing metrics' correlations, \citet{OCDN12} argue for comparison based on the number of system pairs in which both human judgments and metrics agree on statistically significant differences between the systems, a metric also used in the TAC shared-task for summarization metrics \citep[][\emph{i.a.}]{DangOw09}.
This can be viewed similarly to Kendall's $\tau$ in which only statistically significant differences between systems are counted as concordant.
However, the differences in discriminative power across metrics was not statistically tested itself.

More broadly in evaluating summarization systems, \citet{RCSO11} argue for comparing the performance of summarization models via paired $t$-tests or Wilcoxon signed-rank tests \citep{Wilcoxon92}.
They demonstrate these tests have more power than the equivalent unpaired test when used to separate human and model summarizers.

\paragraph{Machine Translation}
The summarization and machine translation (MT) communities face the same problem of developing and evaluating automatic metrics to evaluate the outputs of models.
Since 2008, the Workshop on Machine Translation (WMT) has run a shared-task for developing evaluation metrics \citep[among others]{MWFMB20}.
Although the methodology has changed over the years, they have converged on comparing metrics' system-level correlations using Williams' test \citep{GrahamBa14}.
Since Williams' test assumes the input data is normally distributed and our experiments show it has low power for summarization, we do not recommend it for comparing summarization metrics.
However, human annotations for MT are standardized to be normally distributed, and the metrics have higher correlations to human judgments, thus Williams' test will probably have higher power when applied to MT metrics.
Nevertheless, the methods proposed in this work can be directly applied to MT metrics as well.




