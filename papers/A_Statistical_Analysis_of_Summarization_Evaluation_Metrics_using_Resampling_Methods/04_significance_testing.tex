\section{Significance Testing}
\label{sec:hypo}
Although CIs express the strength of the correlation between two metrics, they do not directly express whether one metric $\mathcal{X}$ correlates to another $\mathcal{Z}$ better than $\mathcal{Y}$ does due to their shared dependence on $\mathcal{Z}$.
This statistical analysis is performed by hypothesis testing.
The specific one-tailed hypothesis test we are interested in is:
\begin{align*}
    H_0 &: \rho(\mathcal{X}, \mathcal{Z}) - \rho(\mathcal{Y}, \mathcal{Z}) \leq 0 \\
    H_1 &: \rho(\mathcal{X}, \mathcal{Z}) - \rho(\mathcal{Y}, \mathcal{Z}) > 0
\end{align*}

\subsection{Williams' Test}
\label{sec:williams}
One method for hypothesis testing the difference between two correlations with a dependent variable that is used frequently to compare machine translation metrics is Williams' test \citep{Williams59}.
It uses the pairwise correlations between $X$, $Y$, and $Z$ to calculate a $t$-statistic and a corresponding $p$-value.\footnote{
The full equation is omitted for space.
See \citet{GrahamBa14} for details.
} Williams' test is frequently used to compare machine translation metrics' performances at the system-level \citep[among others]{MWFMB20}.


However, the test faces the same issues as the Fisher transformation: It assumes the input variables are normally distributed \citep{DunnCl71}, and it is not clear whether the test should be applied at the summary-level.

\subsection{Permutation Tests}
\label{sec:hypo_permutation}
Bootstrapping can be used to calculate a $p$-value in the form of a paired bootstrap test in which the sampling methods described in \S\ref{sec:ci_bootstrapping} can be used to resample new matrices from $X$, $Y$, and $Z$ in parallel (details omitted for space).
However, an alternative and closely related nonparametric hypothesis test is the permutation test \citep{Noreen89}.
Permutation tests tend to be used more frequently than paired bootstrap tests for hypothesis testing because they directly test whether any observed difference between two values is due to random chance.
In contrast, paired bootstrap tests indirectly reason about this difference by estimating the variance of the test statistic.



\input{figures/permutations/permutations}

Similarly to bootstrapping, a permutation test applied to two paired samples estimates the distribution of the test statistic under $H_0$ by calculating its value on new resampled datasets.
In contrast to bootstrapping, the resampled datasets are constructed by randomly permuting which sample each observation in a pair belongs to (i.e., resampling without replacement).
This relies on assuming the pair is exchangeable under $H_0$, which means $H_0$ is true for either sample assignment for the pair.
Then, the $p$-value is calculated as the proportion of times the test statistic across all possible permutations is greater than the observed value.
A significant $p$-value implies the observed test statistic is very unlikely to occur if $H_0$ were true, resulting in its rejection.
In practice, calculating the distribution of $H_0$ across all possible permutations is intractable, so it is instead estimated on a large number of randomly sampled permutations.\footnote{
    This is known as an approximate randomization test.
}

\input{algorithms/hypo-permutation}

For example, a permutation test applied to testing the difference between two QA models' mean accuracies on the same dataset would sample a permutation by swapping the models' outputs for the same input.
Under $H_0$, the models' mean accuracies are equal, so randomly exchanging the outputs is not expected to change their means.
In the case of evaluation metrics, each permutation sample can be taken by randomly swapping the scores in $X$ and $Y$.
There are at least three ways of doing so:
\begin{enumerate}
    \item \textsc{Perm-Systems}: For each system, swap its scores for all inputs with probability 0.5.
    \item \textsc{Perm-Inputs}: For each input, swap its scores for all systems with probability 0.5.
    \item \textsc{Perm-Both}: For each summary, swap its scores with probability 0.5.
\end{enumerate}
To account for differences in scale, we standardize $X$ and $Y$ before performing the permutation.
Fig.~\ref{fig:permutations} contains an illustration of each method, and the pseudocode for a permutation test using the \textsc{Perm-Both} method is provided in Alg.~\ref{alg:permutation}.

Similarly to the bootstrap sampling methods, each of the permutation methods makes assumptions about the system and input document underlying distribution.
This results in different interpretations of how the tests' conclusions will generalize.
Since \textsc{Perm-Systems} randomly assigns system scores for all documents in $\mathcal{D}$ to either sample, we only expect the test's conclusion to generalize to a system distributed similarly to those in $\mathcal{S}$ evaluated on the \emph{specific} set of documents $\mathcal{D}$.
The opposite is true for \textsc{Perm-Inputs}.
The results for \textsc{Perm-Both} (which can be viewed as first swapping systems followed by swapping inputs) are expected to generalize for both systems and documents distributed similarly to those in $\mathcal{S}$ and $\mathcal{D}$.


In \S\ref{sec:power} we run a simulation to compare the different hypothesis testing approaches, then analyze the results of hypothesis tests applied to summarization metrics in \S\ref{sec:hypo_experiments}.



