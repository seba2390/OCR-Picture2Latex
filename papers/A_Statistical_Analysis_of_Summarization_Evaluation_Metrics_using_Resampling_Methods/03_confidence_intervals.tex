\section{Correlation Confidence Intervals}
\label{sec:ci}
Although the strength of the relationship between $\mathcal{X}$ and $\mathcal{Z}$ on one dataset is quantified by the correlation levels $\rsys$ and $\rsum$, each $r$ is only a point estimate of the true correlation of the metrics, denoted $\rho$, on inputs and systems distributed similarly to those in $\mathcal{D}$ and in $\mathcal{S}$.
Although we cannot directly calculate $\rho$, it is possible to estimate it through a CI.

\subsection{The Fisher Transformation}
\label{sec:fisher}
The standard method for calculating a CI for a correlation is the Fisher transformation \citep{Fisher92}.
The transformation maps a correlation coefficient to a normal distribution, calculates the CI on the normal curve, and applies the reverse transformation to obtain the upper and lower bounds:
\begin{align*}
    z_r &= \textrm{arctanh}(r) \\
    r_u, r_\ell &= \textrm{tanh}\left(z_r \pm  z_{\alpha/2} \cdot c\; / \sqrt{n-b}\right)
\end{align*}
where $r$ is the correlation coefficient, $n$ is the number of observations, $z_{\alpha/2}$ is the critical value of a normal distribution, and $b$ and $c$ are constants.\footnote{
$b=3, 3, 4$ and $c=1, \sqrt{1+r^2/2}, \sqrt{.437}$ for Pearson, Spearman, and Kendall, respectively \citep{BonettWr00}.
}

Applying the Fisher transformation to calculate CIs for $\rhosys$ and $\rhosum$ is potentially problematic.
First, it assumes that the input variables are normally distributed \citep{BonettWr00}.
The metrics' scores and human annotations on the datasets that we experiment with are, in general, not normally distributed (see Appendix~\ref{appendix:normality}).
Thus, this assumption is violated, and we expect this is the case for other summarization datasets as well.
Second, it is not clear whether the transformation should be applied to the summary-level correlation since its final value is an average of correlations, which is not strictly a correlation.\footnote{
Correlation coefficients cannot be averaged because they are not additive in the arithmetic sense, however it is standard practice in summarization.
}

\subsection{Bootstrapping}
\label{sec:ci_bootstrapping}
A popular nonparametric method of calculating a CI is bootstrapping \citep{EfronTi93}.
Bootstrapping is a procedure that estimates the distribution of a test statistic by repeatedly sampling with replacement from the original dataset and calculating the test statistic on each sample.
Unlike the Fisher transformation, bootstrapping is a very flexible procedure that does not assume the data is normally distributed nor that the test statistic is a correlation, making it appropriate for summarization.

However, it is not clear how to perform bootstrap sampling for correlation levels.
Consider a more standard bootstrapped CI calculation for the mean accuracy of a question-answering model on a dataset with $k$ instances.
Since the mean accuracy is a function of the $k$ individual correct/incorrect labels, each bootstrap sample can be constructed by sampling with replacement from the original $k$ instances $k$ times.
In contrast, the correlation levels are functions of the matrices $X$ and $Z$, so each bootstrap sample should also be a pair of matrices of the same size that are sampled from the original data.

There are at least three potential methods for sampling the matrices:
\begin{enumerate}
    \item \textsc{Boot-Systems:} Randomly sample with replacement $N$ systems from $\mathcal{S}$, then select the sampled system scores for all of the inputs.

    \item \textsc{Boot-Inputs:} Randomly sample with replacement $M$ inputs from $\mathcal{D}$, then select all of the system scores for the sampled inputs.
    
    \item \textsc{Boot-Both:} Randomly sample with replacement $M$ inputs from $\mathcal{D}$ and $N$ systems from $\mathcal{S}$, then select the sampled system scores for the sampled inputs.
\end{enumerate}
Once the samples are taken, the corresponding values from $X$ and $Z$ are selected to create the sampled matrices.
An illustration of each method is shown in Figure~\ref{fig:sampling}.

\input{figures/sampling/sampling}

Each sampling method makes its own assumptions about the degrees of freedom in the sampling process that results in different interpretations of the corresponding CIs. \textsc{Boot-Inputs} assumes that there is only uncertainty on the inputs while the systems are held constant.
CIs derived from this sampling technique would express a range of values for the true correlation $\rho$ between $\mathcal{X}$ and $\mathcal{Z}$ for the \emph{specific} set of systems $\mathcal{S}$ and inputs from the same distribution as those in $\mathcal{D}$.
The opposite assumption is made for \textsc{Boot-Systems} (uncertainty in systems, inputs are fixed).
\textsc{Boot-Both}, which can be viewed as sampling systems followed by sampling inputs, assumes uncertainty on both the systems and the inputs.
Therefore the corresponding CI estimates $\rho$ for systems and inputs distributed the same as those in $\mathcal{S}$ and $\mathcal{D}$.

Algorithm~\ref{alg:ci} contains the pseudocode for calculating a CI via bootstrapping using the \textsc{Boot-Both} sampling method.
In \S\ref{sec:ci_simulations} we experimentally evaluate the Fisher transformation and the three bootstrap sampling methods, then analyze the CIs of several different metrics in \S\ref{sec:ci_experiments}.

\input{algorithms/ci-bootstrap}