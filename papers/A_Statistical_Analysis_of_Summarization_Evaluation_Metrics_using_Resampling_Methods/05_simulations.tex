\section{Simulation Experiments}
\label{sec:simulations}
We run two sets of simulation experiments in order to determine which CI (\S\ref{sec:ci_simulations}) and hypothesis test (\S\ref{sec:power}) methods are most appropriate for summarization metrics.

The datasets used in the simulations are the multi-document summarization dataset TAC'08 \citep{DangOw08} and two subsets of the single-document summarization CNN/DM dataset \citep{NZSGX16} annotated by \citet{FKMSR21} and \citet{BGALN20}.
These datasets have $N=58/16/25$ summarization models and $M=48/100/100$ inputs, respectively.
The summaries were assigned overall responsiveness, relevance, or Lightweight Pyramid \citep{SGGRPBAD19} scores, respectively, by human annotators. The scores of the automatic metrics are correlated to these human annotations.

\subsection{Confidence Interval Simulation}
\label{sec:ci_simulations}
In practice, evaluation metrics are almost always used to score summaries produced by systems $\mathcal{S}'$ on inputs $\mathcal{D}'$ which are disjoint (or nearly disjoint) from and assumed to be distributed similarly to the data that was used to calculate the CI, $\mathcal{S}$ and $\mathcal{D}$.
It is still desirable to use the CI as an estimate of the correlation of a metric on $\mathcal{S}'$ and $\mathcal{D}'$, however this scenario violates assumptions made by some of the bootstraping sampling methods (e.g., \textsc{Boot-Systems} assumes that $\mathcal{D}$ is fixed).
This simulation aims to demonstrate the effect of violating these assumptions on the accuracy of the CIs.

\paragraph{Setup.}
The simulation works as follows.
The systems $\mathcal{S}$ and inputs $\mathcal{D}$ are each randomly partitioned into two equally sized disjoint sets $\mathcal{S}_A$, $\mathcal{S}_B$, $\mathcal{D}_A$, and $\mathcal{D}_B$.
Then the submatrices $X_A$, $Z_A$, $X_B$, and $Z_B$ are selected from $X$ and $Z$ based on the system and input partitions.
Matrices $X_A$ and $Z_A$ are used to calculate a 95\% CI using one of the methods described in \S\ref{sec:ci}, and then it is checked whether sample correlation $r(X_B, Z_B)$ is contained by the CI.
The entire procedure is repeated 1000 times, and the proportion of times the CI contains the sample correlation is calculated.

It is expected that a CI which generalizes well to the held-out data should contain the sample correlation 95\% of the time under the assumption that the data in $A$ and $B$ is distributed similarly.
The larger the difference from 95\%, the worse the CI is at estimating the correlation on the held-out data.

\input{figures/ci_simulation}

The results of the simulation calculated on TAC'08 and CNN/DM using both the Fisher transformation and the different bootstrap sampling methods to CIs for QAEval-F$_1$ \citep{DeutschBeRo20} are shown in Table~\ref{tab:ci_simulation}.\footnote{
    The Fisher transformation was directly applied to the averaged summary-level correlation.
}

\paragraph{\textsc{Boot-Both} generalizes the best.}
Among the bootstrap methods, \textsc{Boot-Both} produces CIs that come closest to the ideal 95\% rate.
Any deviations from this number reflect that the assumption that all of the inputs and systems are distributed similarly is not true, but overall violating this assumption does not have a major impact.

The other bootstrap methods, which sample only systems or inputs, captures the correlation on the held-out data far less than 95\% of the time.
For instance, the CIs for $\rhosys$ on \citet{BGALN20} only successfully estimate the held-out correlation on 80\% and 68\% of trials.
This means that a 95\% CI calculated using \textsc{Boot-Inputs} is actually only a 68\% CI on the held-out data.
This pattern is the same across the different correlation levels and datasets.
The lower values for only sampling inputs indicates that more variance comes from the systems rather than the inputs.

\paragraph{Fisher analysis.}
The Fisher transformation at the system-level creates CIs that generalize worse than \textsc{Boot-Both}.
The summary-level CI captures the held-out sample correlation 100\% of the time, implying that the CI width is too large to be useful.
We believe this is due to the fact that as the absolute value of $r(X, Z)$ decreases, the width of the Fisher CI increases.
Summary-level correlations are lower than system-level correlations (see \S\ref{sec:ci_experiments}), and therefore Fisher results in a worse CI estimate at the summary-level.

\paragraph{Conclusion.}
This experiment presents strong evidence that violating the assumptions that either the systems/inputs are fixed or that the data is normally distributed does result in worse CIs.
Hence, the \textsc{Boot-Both} method provides the most accurate CIs for scenarios in which summarization metrics are frequently used.

\subsection{Power Analysis}
\label{sec:power}
The power of a hypothesis test is the probability of accepting the alternative hypothesis given that it is actually true (equal to $1.0$ -- the type-II error rate).
It is desirable to have as high of a power as possible in order to avoid missing a significant difference between metrics.
This simulation estimates the power of each of the hypothesis tests.

\paragraph{Setup.}
Measuring power requires a scenario in which it is known that $\rho$ is greater for one metric than another (i.e., $H_1$ is true).
Since this is not known to be true for any pair of proposed evaluation metrics, we artificially create such a scenario by adding randomness to the calculation of ROUGE-1.\footnote{
    We use the recall variant of ROUGE for experiments on TAC'08 and \citet{BGALN20} and the F$_1$ variant on \citet{FKMSR21} throughout the paper.
}
We define $\mathcal{R}_k$ to be ROUGE-1 calculated using a random $k\%$ of the candidate summary's tokens.
We assume that since $\mathcal{R}_k$ only evaluates a summary with $k\%$ of its tokens, it is quite likely that it is a worse metric than standard ROUGE-1 for $k < 100$.

To estimate the power, we score summaries with ROUGE-1 and $\mathcal{R}_k$ for different $k$ values and count how frequently each hypothesis test rejects $H_0$ in favor of identifying ROUGE-1 as a superior metric.
This trial is repeated 1000 times, and the proportion of significant results is the estimate of the power.

Since the various hypothesis tests make different assumptions about whether the systems and inputs are fixed or variable, it is not necessarily fair to directly compare their powers.
Because the assumptions of \textsc{Boot-Both} and \textsc{Perm-Both} most closely align with the typical use case of summarization, we compare their powers.
We additionally include Williams' test because it is frequently used for machine translation metrics and it produces interesting results, discussed below.

\input{figures/power/power}

\paragraph{\textsc{Perm-Both} has the highest power.}
Fig.~\ref{fig:power} plots the power curves for various values of $k$ on the CNN/DM annotations by \citet{FKMSR21}.
We find that \textsc{Perm-Both} has the highest power among the three tests for all values of $k$.
As $k$ approaches $100\%$, the difference between ROUGE-1 and $\mathcal{R}_k$ becomes smaller and harder to detect, thus the power for all methods approaches 0.

\textsc{Boot-Both} has lower power than \textsc{Perm-Both} both at the summary-level and system-level, in which it is near 0.
This result is consistent with permutation tests being more useful for hypothesis testing than their bootstrapping counterparts.
We believe the power differences in both levels are due to the variance of the two correlation levels.
As we observe in \S\ref{sec:ci_experiments}, the system-level CIs have significantly larger variance than at the summary-level, making it harder for the paired bootstrap to reject the system-level $H_0$.

\input{figures/confidence-intervals/ci}

\paragraph{Williams' test has low power.}
Interestingly, the power of Williams' test for all $k$ is $\approx 0$, implying the test never rejects $H_0$ in this simulation.
This is surprising because Williams' test is frequently used to compare machine translation metrics at the system-level and does find differences between metrics.
We believe this is due to the strength of the correlations of ROUGE-1 to the ground-truth judgments as follows.

The $p$-value calculated by Williams is a function of the pairwise correlations of $X$, $Y$, and $Z$ and the number of observations.
The closer both $r(X, Z)$ and $r(Y, Z)$ are to 0, the higher the $p$-value.
The correlation of ROUGE-1 in this simulation is around 0.6 and 0.3 at the system- and summary-levels.
In contrast, the system-level correlations for the metrics submitted to the Workshop on Machine Translation (WMT) 2019's metrics shared task for de-en are on average 0.9 \citep{MWBG19}.
Among the 231 possible pairwise metric comparisons in WMT'19 for de-en, Williams' test yields 81 significant results.
If the correlations are shifted to have an average value of 0.6, only 3 significant results are found.
Thus we conclude that Williams' test's power is worse for detecting differences between lower correlation values.

Because this simulation is performed with summarization metrics on a real summarization dataset, we believe it is faithful enough to a realistic scenario to conclude that Williams' test does indeed have low power when applied to summarization metrics.
However, we do not expect Williams' test to have 0 power when used to detect differences between machine translation metrics.

\paragraph{Conclusion.}
Since \textsc{Perm-Both} has the best statistical power at both the system- and summary-levels, we recommend it for hypothesis testing the difference between summarization metrics.