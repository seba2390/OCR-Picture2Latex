% This version of CVPR template is provided by Ming-Ming Cheng.
% Please leave an issue if you found a bug:
% https://github.com/MCG-NKU/CVPR_Template.

\documentclass[review]{cvpr}
%\documentclass[final]{cvpr}

\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{xcolor}
\usepackage{multirow}
\usepackage{multicol}
\usepackage{booktabs}
\usepackage{subcaption}
\usepackage{float}
\newcommand{\iie}{\emph{i.e.}}

% Include other packages here, before hyperref.

% If you comment hyperref and then uncomment it, you should delete
% egpaper.aux before re-running latex.  (Or just hit 'q' on the first latex
% run, let it finish, and you should be clear).
\usepackage[pagebackref=true,breaklinks=true,colorlinks,bookmarks=false]{hyperref}


\def\cvprPaperID{1430} % *** Enter the CVPR Paper ID here
\def\confYear{CVPR 2022}
%\setcounter{page}{4321} % For final version only


\begin{document}

%%%%%%%%% TITLE
\title{Supplementary for Decoupled One Stage Action Detection Network}

\author{First Author\\
Institution1\\
Institution1 address\\
{\tt\small firstauthor@i1.org}
% For a paper whose authors are all at the same institution,
% omit the following lines up until the closing ``}''.
% Additional authors and addresses can be added with ``\and'',
% just like the second author.
% To save space, use either the email address or home page, not both
\and
Second Author\\
Institution2\\
First line of institution2 address\\
{\tt\small secondauthor@i2.org}
}

\maketitle

%%%%%%%%% BODY TEXT

\section{Additional experiments}

\subsection{Detection performance (action agnostic)}
As a one-stage method, we use the person bounding boxes generated by our network instead of an off-the-shelf detector. We now investigate in detail at the detection performance. We report the performance in Table~\ref{detection} with a standard 0.5 IOU threshold. The off-the-shelf detector is Faster R-CNN framework with ResNeXt-101-FPN backbone from maskrcnn-benchmark, which is widely applied in two-stage methods [\textcolor{green}{48}, \textcolor{green}{38}, \textcolor{green}{26}]. The model is first pre-trained on ImageNet, then fine-tuned on MSCOCO dataset, and finally fine-tuned on AVA dataset for higher person detection precision. We can see that our person detection result is still lower than the specialized detector, which is the crucial reason that the performance of one-stage methods cannot surpass the state-of-the-art two-stage methods.

\subsection{Backbone modification}
We are the first to use transformer-based backbone, Swin-B, in this task. A tough nut is how to maintain a larger spatial resolution of the feature map due to the overlarge spatial downsampling rate in the original version. The downsampling of Swin-B is mainly from the patch merging layer followed by a swin transformer block. We propose two schemes to modify the Swin-B: (i) removing the last patch merging layer and its following swin transformer block; (ii) just removing the last patch merging layer and modifying the dimension of the weights of the last swin transformer block. Note that the last swin transformer block in scheme (ii) cannot be initialized from a pre-trained model and can only be trained from scratch. Their results are presented in Table \ref{backbone}. Scheme (i) is slightly higher than scheme (ii) and contains fewer parameters. Thus, we adopt scheme (i) in our method.

\section{Qualitative results}
We randomly visualize some cases of our model in Figure \ref{qualitative}. Our model is able to exploit the person-context relationships to recognize interaction categories such as ``watch sb" and ``listen to sb", which are inherently hard if only focus on the actor, as shown in the first row of Figure \ref{qualitative}. There are two failure detection cases in the second row, which shows that our detection is hard to handle crowded and dark scenes. 


\begin{table} [t]
\begin{center}
\small
%\resizebox{!}{1.1cm}{
\begin{tabular}{c|c}
\toprule
Method & mAP (IOU@0.5)  \\
\midrule
Off-the-shelf detector & 93.5 \\
Ours & 89.9 \\
\bottomrule
\end{tabular}
\end{center}
% \vspace{-5mm}
\caption{We perform classification-agnostic evaluation to evaluate the performance of our person detection and compare it with the off-the-shelf detector.}
% \vspace{-1mm}
\label{detection}
\end{table}

\begin{table} [t]
\begin{center}
\small
%\resizebox{!}{1.1cm}{
\begin{tabular}{c|c}
\toprule
Scheme & mAP (IOU@0.5)  \\
\midrule
 i & 28.8 \\
 ii & 28.5 \\
\bottomrule
\end{tabular}
\end{center}
\vspace{-5mm}
\caption{Comparison of backbone modification schemes.}
\vspace{-3mm}
\label{backbone}
\end{table}

\begin{figure}[t]
%\includegraphics[width=1.0\linewidth]{LaTeX/supplement.png}
\includegraphics[width=1.0\linewidth]{latex/qualitative.png}
\centering
\caption{Per category results for the proposed network and the baseline model on the validation set of AVA dataset.}
\label{qualitative}
%\vspace{-4mm}
\end{figure}

\begin{figure*}[!t]
%\includegraphics[width=1.0\linewidth]{LaTeX/supplement.png}
\includegraphics[width=0.95\linewidth]{latex/figure.jpeg}
\centering
\caption{Per category results for the proposed network and the baseline model on the validation set of AVA dataset.}
\label{visualization}
\end{figure*}

\section{Per category analysis}
The per category results for our method and the baseline (the full model without TransPC) are shown in Figure \ref{visualization}. Our method improves the baseline performance in about 50 out of 60 classes. We can see that the categories getting the largest performance boost are from interaction categories, e.g., ``hand clap", ``work on a computer", ``smoke", and ``listen to sb", which require more attention on the supporting actors and context.





% {\small
% \bibliographystyle{ieee_fullname}
% %\bibliography{egbib}
% }

\end{document}
