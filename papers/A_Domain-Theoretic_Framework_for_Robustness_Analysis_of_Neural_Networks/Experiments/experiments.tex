\documentclass[11pt]{article}

\usepackage[a4paper, total={7in, 9in}]{geometry}
\usepackage{fancyhdr}
\usepackage{setspace}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{makecell}
\usepackage{graphicx}
\usepackage{float}

\pagestyle{fancy}

\title{Experiments}
\lhead{Robustness}
\chead{Experimental Records}
\rhead{Can Zhou}

\begin{document}
All the experiments are reproducible and results should be stable (except exact timing results which are highly relied on the machine). Please find corresponding programming details in the README file of supplementary codes. 
In comparison, currently, we consider following algorithms:
\begin{enumerate}
\item lipDT (ours)
\item lipMIP
\item ZLip
\item CLEVER
\item FastLip
\item NaiveUB: this is also proposed in the lipMIP paper.
\item RamdomLB: this is also proposed in the lipMIP paper.
\item SeqLip
\end{enumerate}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Experiment Setup}
\subsection{Computing environment}
All experiment conducted on a machine with AMD Ryzen 75800H 3.2GHz 8-Core processsor and 16GB RAM under Ubuntu 20.04 LTS environment. Our proposed algorithm which is based on interval arithmetic is implemented by MPFI for arbitrary precesion computations, and PyInterval for fixed precision floating point computation. To fairly against existing Lipschitz estimation algorithms, we used fixed (python default) precision floating point with 18 places computation in accuracy comparisons.  

\subsection{Network}
All the networks used in the experiments are MLP and we took the fist output node to gain the Lipschitz constant.

\subsection{Activation function}
\begin{enumerate}
	\item ReLU: the instance of no-differentiable activation function.
	\item Sigmoid: the instance of differentiable activation function.
\end{enumerate}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Accuary \& Efficiency}

\subsection{Random network examples}

To explore the situation when the inputs are near the boundary, we evaluated a randomly generated neural network with 2 inputs, 2 hidden neurons, and 2 output nodes. The main propose of this experiment is to check how Lipschitz estimation techniques handle ReLU(0). All experiments has a 1e-7 radius.

\begin{table}[H]
\centering
\caption{Layer sizes = [2, 2, 2]}
\begin{tabular}{|c|c|c|c|}
\hline
                & Value                              & Time (ms)      & R.E. \\ \hline
lipDT C++ &  \makecell*[c]{[0.322705690859059757,\\ 0.322705690859059757]} & 8.289 & 0.00\%    \\ \hline
lipDT Python &  \makecell*[c]{[0.32270569085905976,\\ 0.32270569085905976]} & 338.358 & 0.00\%    \\ \hline
lipMIP          & 0.041121020913124084               & 10.0348   & --87.26\% \\ \hline
ZLip            & 0.041121020913124084               & 2.5075    & --87.26\%\\ \hline
CLEVER          & 0.2815846800804138				 & 1054.1022 & --12.74\%\\ \hline
FastLip         & 0.041121020913124084				 & 0.8451    & --87.26\% \\ \hline
NaiveUB         & 0.6921076774597168				 & 0.0959    & +114.47\% \\ \hline
RandomLB        & 0.2815846800804138				 & 137.3216  & --12.74\% \\ \hline
SeqLip          & 0.2129872590303421                                                                                                  & 1.9632 &   --34.00\%  \\ \hline
\end{tabular}
\label{table:random1}
\end{table}

Table~\ref{table:random1} is one of the examples that all hidden neurons' outputs go across ReLU(0). In the randomly generated network, the weights between inputs and the hidden layer are $W_1=[[5.323450565338134766e-01,2.982044517993927002e-01]
[4.367777407169342041e-01,1.963265985250473022e-01]]$ and biases of neurons in hidden layer are $b_1 = [3.763356208801269531e-01,-6.647928357124328613e-01]$. To make the input domain near the boundary, we made up inputs to $x_0=-4.832202221268014242$ and $x_1=-7.364287590384273940$ which calculated by formula $x*W_1+b_1=0$. Moreover, we set the radius (perturbation) to $1e-7$ to narrow the input domain, so we could meet the circumstance that needs $ReLU(0)$.
In interval computation, the outputs of hidden layer (before go through the $ReLU$ activation function) are $interval([-8.305495136085028e-08, 8.305495180493949e-08])
$ and $interval([-6.331043445051421e-08, 6.331043467255881e-08])$. Furthermore, our method took five time bisections to reach the final results and the results of Lipschitz constant after each iteration (one original result + five bisections) are:\\
interval([0.0, 0.32270569085905976])\\
interval([0.04112101957020187, 0.32270569085905976])\\
interval([0.04112101957020187, 0.32270569085905976])\\
interval([0.04112101957020187, 0.32270569085905976])\\
interval([0.04112101957020187, 0.32270569085905976])\\
interval([0.32270569085905976])


\begin{table}[H]
\centering
\caption{Layer sizes = [2, 2, 2], after one bisection}
\begin{tabular}{|c|c|c|c|}
\hline
                & Value                              & Time (ms)      & R.E. \\ \hline
lipDT C++ &  \makecell*[c]{[0.571150659783544512,\\ 0.614485302898467367]} & 0.318 & 0.00\%    \\ \hline
lipDT Python &  \makecell*[c]{[0.5711506597835445,\\ 0.6144853028984674]} & 12.7781 & 0.00\%    \\ \hline
lipMIP          & 0.043334643114922855               & 9.8405   & --92.41\% \\ \hline
ZLip            & 0.04333464428782463                & 2.6253     & --92.41\%\\ \hline
CLEVER          & 0.04333464428782463				 & 1102.0130 & --92.41\%\\ \hline
FastLip         & 0.04333464428782463				 & 0.6235    & --92.41\% \\ \hline
NaiveUB         & 0.7291175127029419				 & 0.0511    & +18.65\% \\ \hline
RandomLB        & 0.04333464428782463				 & 143.504  & --92.41\% \\ \hline
SeqLip          & 0.4503255784511566                                                                                             & 0.7316 & --21.15\%  \\ \hline
\end{tabular}
\label{table:random2}
\end{table}

\begin{table}[H]
\centering
\caption{Layer sizes = [2, 2, 2], after six bisections}
\begin{tabular}{|c|c|c|c|}
\hline
                & Value                              & Time (ms)      & R.E. \\ \hline
lipDT C++ &  \makecell*[c]{[0.6144853028984674,\\ 0.6144853028984674]} & 12.516 & 0.00\%    \\ \hline
lipDT Python &  \makecell*[c]{[0.6144853028984674,\\ 0.6144853028984674]} & 543.0241 & 0.00\%    \\ \hline
lipMIP          & 0.043334643114922855               & 3.1055   & --92.94\% \\ \hline
ZLip            & 0.04333464428782463                & 1.5169     & --92.94\%\\ \hline
CLEVER          & 0.04333464428782463				 & 1105.0684 & --92.94\%\\ \hline
FastLip         & 0.04333464428782463				 & 0.7541    & --92.94\% \\ \hline
NaiveUB         & 0.7291175127029419				 & 0.0651    & +18.65\% \\ \hline
RandomLB        & 0.04333464428782463				 & 144.7335  & --92.94\% \\ \hline
SeqLip          & 0.4503255784511566                                                                                             & 0.5654 & --26.71\%  \\ \hline
\end{tabular}
\label{table:random3}
\end{table}

Table~\ref{table:random2} and Table~\ref{table:random3} are resulted gained from a same randomly generated network but with different bisection numbers in interval method (our method).The results of Lipschitz constant after each iteration (one original result + six bisections) are:\\
interval([0.0, 0.6144853028984674])\\
interval([0.5711506597835445, 0.6144853028984674])\\
interval([0.5711506597835445, 0.6144853028984674])\\
interval([0.5711506597835445, 0.6144853028984674])\\
interval([0.5711506597835445, 0.6144853028984674])\\
interval([0.5711506597835445, 0.6144853028984674])\\
interval([0.6144853028984674])\\


\subsection{IRIS example}
In the IRIS example, we took the ReLU network with four input dimensions, three hidden nodes and 3 outputs (classes). The network were trained for 2000 epochs by using CrossEntropyLoss loss and Adam optimizer with 0.001 learning rate. The radius of inputs is 0.001. 

\begin{table}[H]
\centering
\caption{Layer sizes = [4, 3, 3]}
\begin{tabular}{|c|c|c|c|}
\hline
                & Value                              & Time (ms)      &R.E. \\ \hline
lipDT (C++) &  \makecell*[c]{[3.48780926173324346,\\ 3.48780926173324346]} & 0.059  & 0.00\%  \\ \hline
lipDT (Python) &  \makecell*[c]{[3.4878092617332435,\\ 3.4878092617332435]} & 1.5823  & 0.00\%  \\ \hline
lipMIP          & 3.4878092408180237                 & 2.3955   & --6.00e-07\% \\ \hline
ZLip            & 3.487809181213379					 & 1.1    & --2.31e-06\% \\ \hline
CLEVER          & 3.487809181213379					 & 1129.7206 & --2.31e-06\% \\ \hline
FastLip         & 3.487809181213379					 & 0.6267    & --2.31e-06\% \\ \hline
NaiveUB         & 12.569558143615723                 & 0.05    & +260.39\% \\ \hline
RandomLB        & 3.487809181213379                  & 139.9677  & --2.31e-06\% \\ \hline
SeqLip          & 1.8214526176452637                  & 0.7494   & 
-47.78\% \\ \hline
\end{tabular}
\end{table}

\subsection{MNIST example}
In the MNIST example, we took the ReLU network with four input dimensions, ten hidden nodes and 3 outputs (classes). The network were trained for 1000 epochs by using CrossEntropyLoss loss and Adam optimizer with 0.001 learning rate. The radius of inputs is 0.001. 

\begin{table}[ht]
\centering
\caption{layer sizes = [784, 10, 10, 2]}
\begin{tabular}{|c|c|c|c|}
\hline
                & Value                                     & Time       & R.E.\\ \hline
lipDT C++ & \makecell*[c]{[0.34815343269413546,\\ 0.34815343269413546]}  & 164.04 & 0.00\%\\ \hline
lipDT Python & \makecell*[c]{[0.34815343269412385,\\ 0.3481534326941471]}  & 2388.6251 & 0.00\%\\ \hline
lipMIP          & 0.3481534055299562                        & 33.1788   & --7.80e-06\%\\ \hline
ZLip            & 0.34815341234207153                       & 7.1895    & --5.85e-06\%\\ \hline
CLEVER          & 0.3481535315513611						& 4308.8220 & +2.84e-05\%\\ \hline
FastLip         & 0.34815341234207153						& 0.8844    & --5.85e-06\%\\ \hline
NaiveUB         & 107.7006607055664							& 0.0759     & +30834.83\%\\ \hline
RandomLB        & 0.3481535315513611						& 177.9699  & +2.84e-05\%\\ \hline
SeqLip          & 0.670467323694055                                                                                                                                & 4.9305    & +92.58\%\\ \hline
\end{tabular}
\end{table}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Weights and biases}
\begin{figure}[H]
    \centering
    \includegraphics[width=8cm]{images/nearBoundary.png}
    \caption{When the box is near the boundary}
    \label{fig:nearBoundary}
\end{figure}
\begin{figure}[H]
    \centering
    \includegraphics[width=8cm]{images/notBoundary.png}
    \caption{When the box is not near the boundary}
    \label{fig:notBoundary}
\end{figure}

We took a [2, 2, 2] randomly generated network, and made up inputs $X$, weights $W$ and bias $b$ to $W*X + b = 0$. Figure~\ref{fig:nearBoundary} demonstrates, when the input boxes are near the boundary, the values of interval Lipschitz constant following the changes of the width of weights and biases. For the input boxes not near the boundary, the results are presented in Figure~\ref{fig:notBoundary}.




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Convergence analysis}
\textbf{All figures' y-axis is Lipschitz constant, and have been normalize in [0, 1].}
\subsection{Not near boundary -- Sigmoid}
Sigmoid neural network with size [10, 5, 2].
Clever converges in the range of interval value.
\begin{figure}[H]
    \centering
    \includegraphics[width=8cm]{images/Nb69.png}
    \caption{Fixed branch number -- 145, X-axis is the sample number}
    \label{fig:nearBoundary}
\end{figure}
\begin{figure}[H]
    \centering
    \includegraphics[width=8cm]{images/noBoundarySigmoid.png}
    \caption{When the box is not near the boundary}
    \label{fig:notBoundary}
\end{figure}

\subsection{Near boundary -- ReLU}
Sigmoid neural network with size [2, 2, 2].
Clever converges in a wrong place and the interval value is a singleton.
\begin{figure}[H]
    \centering
    \includegraphics[width=8cm]{images/Clever_Wrong_Convergence-Nb_140.png}
    \caption{Fixed branch number -- 140, X-axis is the sample number}
    \label{fig:nearBoundary}
\end{figure}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\section{Multi-layers}


\end{document}



























