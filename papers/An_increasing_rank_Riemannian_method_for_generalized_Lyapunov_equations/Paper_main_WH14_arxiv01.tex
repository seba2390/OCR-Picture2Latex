%\documentclass[11pt]{fsuthesis}

\documentclass[11pt]{article}

% This is a "bare-bones" thesis template file.  For more examples,
% of how to use more LaTeX features, look in the 'sample' directory.

%\usepackage{textcase}
%\usepackage[pdftex]{graphicx}
\usepackage{hyperref}
\hypersetup{colorlinks = true, % 将链接文字带颜色
	    linkcolor = black,     % figure table back
	    urlcolor = blue,       % url blue
        citecolor = blue}      % cite blue
%\hypersetup{breaklinks=true}
\usepackage{authblk}
\usepackage{fullpage}  % for narrow margins
\usepackage{float}
\usepackage{pb-diagram}  		% for commutative diagrams
%\usepackage[ps2pdf, linktocpage=true]{hyperref}
\usepackage{esvect}
% PA-28Dec2011 I also add url for the "url" command:
% \usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{fullpage}  % for narrow margins
\usepackage{float}
\usepackage{pb-diagram}
\usepackage{makecell}		% for commutative diagrams
%\usepackage[ps2pdf, linktocpage=true]{hyperref}

% PA-28Dec2011 I also add url for the "url" command:
\usepackage{url}

\usepackage{algorithm} %//format of the algorithm
\usepackage{algorithmic} %//format of the algorithm

% \usepackage{algorithm, algorithmicx,algpseudocode, listings} %format of the algorithm
\usepackage{multirow} %//multirow for format of table
\usepackage{hhline}  % for \hhline in tables
\usepackage{amsmath}
\usepackage{xcolor}
\renewcommand{\algorithmicrequire}{\textbf{Input:}}
\renewcommand{\algorithmicensure}{\textbf{Output:}}

\usepackage{cases}
\usepackage{ulem} 
\usepackage{cancel}
\usepackage{soul} 
\usepackage{graphicx}
\usepackage{amscd}
\usepackage{amssymb,mathrsfs}
\input{epsf.sty}
\usepackage{amsthm,amscd}
\usepackage{color}
\usepackage{latexsym}
\usepackage{epic}
\usepackage{appendix}
\usepackage{enumerate}
\usepackage{longtable}
\usepackage{lscape}
\usepackage{extarrows}
\usepackage{epstopdf}
%\usepackage{showlabels}
%\usepackage{verbatim}
\usepackage{listings}
%%%%%%
\newlength\myindent
\newcommand\bindent[1]{%
  \setlength\myindent{#1}
  \begingroup
  \setlength{\itemindent}{\myindent}
%  \addtolength{\algorithmicindent}{\myindent}
}
\newcommand\eindent{\endgroup}
\renewcommand{\CancelColor}{\color{red}} % related with usepackage "cancel"

\renewcommand{\theequation}{\arabic{equation}}
\newcommand{\rmnum}[1]{\romannumeral #1}
\newcommand{\Rmnum}[1]{\expandafter\@slowromancap\romannumeral #1@}
\newcommand{\inner}[3][]{{\langle #2,#3 \rangle_{#1}}}
\newcommand{\Utrn}{\mathcal{U}_{\mathrm{trn}}}
\newcommand{\etal}{\textit{et al. }}
%%%%%%

\allowdisplaybreaks
\newcommand\numberthis{\addtocounter{equation}{1}\tag{\theequation}}

%%%%%%

\renewcommand{\thefootnote}{\roman{footnote}}

\newcommand{\whcomm}[2]{{\sf\color{purple} #1}{\sf\color{blue} #2}}
%\newcommand{\whcomm}[2]{{}{#2}}
\newcommand{\whnew}[2]{{\sf\color{purple} #1}{\sf\color{blue} #2}}
%\newcommand{\whnew}[2]{{}{#2}}

\newcommand{\zwhcomm}[2]{{\sf\color{purple} #1}{\sf\color{blue} #2}}
\newcommand{\zwhnew}[2]{{\sf\color{purple} #1}{\sf\color{blue} #2}}
%\newcommand{\zwhcomm}[2]{{}{#2}}
%\newcommand{\zwhnew}[2]{{}{#2}}

\newcommand{\kwnote}[1]{{\sf\color{orange} [#1]}}
% \newcommand{\kwrev}[1]{{\sf\color{red} #1}}
\newcommand{\kwrev}[1]{{#1}}


%\newcommand{\inner}[3][]{{\left\langle #2,#3 \right\rangle_{#1}}}

%\newcommand{\slversions}[2]{{#1}{}} %short version
\newcommand{\slversions}[2]{{}{#2}} %long version
%\newcommand{\slversions}[2]{{#1}{#2}} %all version


\newtheorem{definition}{Definition}[section]
\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}{Lemma}[section]
\newtheorem{corollary}{Corollary}[section]
\newtheorem{proposition}{Proposition}[section]
\newtheorem{assumption}{Assumption}[section]
\newtheorem{remark}{Remark}[section]
\newtheorem{examample}{Example}[section]
\numberwithin{equation}{section}

%%%%%%
\renewcommand{\P}{\mathrm{P}}
\DeclareMathOperator{\E}{\mathrm{E}}
\DeclareMathOperator{\F}{\mathrm{F}}
\DeclareMathOperator{\T}{\mathrm{T}}
\DeclareMathOperator{\Hess}{\mathrm{Hess}}
\DeclareMathOperator{\grad}{\mathrm{grad}}
\DeclareMathOperator{\retr}{\mathrm{R}}
\DeclareMathOperator{\Exp}{\mathrm{Exp}}
\DeclareMathOperator{\Prox}{\mathrm{Prox}}
\DeclareMathOperator{\id}{\mathrm{id}}
\DeclareMathOperator{\I}{\mathrm{I}}
\DeclareMathOperator{\N}{\mathrm{N}}
\DeclareMathOperator{\D}{\mathrm{D}}
\DeclareMathOperator{\dist}{\mathrm{dist}}
\DeclareMathOperator{\trace}{\mathrm{trace}}
\DeclareMathOperator{\conv}{\mathrm{conv}}
\DeclareMathOperator{\sym}{\mathrm{sym}}
\DeclareMathOperator{\sskew}{\mathrm{skew}}
\DeclareMathOperator{\sign}{\mathrm{sign}}
\DeclareMathOperator{\ddiag}{\mathrm{ddiag}}
\DeclareMathOperator{\diag}{\mathrm{diag}}
\DeclareMathOperator{\St}{\mathrm{St}}
\DeclareMathOperator{\Gr}{\mathrm{Gr}}
\DeclareMathOperator{\GL}{\mathrm{GL}}
\DeclareMathOperator{\TV}{\mathrm{TV}}
\DeclareMathOperator{\orth}{\mathrm{orth}}
\DeclareMathOperator{\qf}{\mathrm{qf}}
\DeclareMathOperator{\vvec}{\mathrm{vec}}
\DeclareMathOperator{\vvectriu}{\mathrm{vectriu}}
\DeclareMathOperator{\col}{\mathrm{col}}
\DeclareMathOperator{\dom}{\mathrm{dom}}
\DeclareMathOperator{\sspan}{\mathrm{span}}
\DeclareMathOperator{\proj}{\mathrm{Proj}}
\DeclareMathOperator{\tril}{\mathrm{tril}}
\DeclareMathOperator{\triu}{\mathrm{triu}}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}
%%%%%%
\def\S{\mathbb{S}}
\def\R{\mathbb{R}}
\def\M{\mathcal{M}}

\providecommand{\keywords}[1]
{
\small 
%\textbf{\textit{Keywors---}} #1
\textbf{Keywords:}
}


\begin{document}


\title{An increasing rank Riemannian method for generalized Lyapunov equations 
}
\author[1]{Zhenwei Huang}
\author[2]{Wen Huang}

\affil[1]{ School of Mathematical Sciences, Xiamen University, Xiamen, China.\vspace{.15cm}}
\affil[2]{ Corresponding author, School of Mathematical Sciences, Xiamen University, Xiamen, China. }

\maketitle

\begin{abstract}
In this paper, we consider finding a low-rank approximation to the solution of a large-scale generalized Lyapunov matrix equation in the form of $A X M + M X A = C$, where $A$ and $M$ are symmetric positive definite matrices. An algorithm called an Increasing Rank Riemannian method for generalized Lyapunov equation (IRRLyap) is proposed by merging the increasing rank technique and Riemannian optimization techniques on the quotient manifold $\mathbb{R}_*^{n \times p} / \mathcal{O}_p$. To efficiently solve the optimization problem on $\mathbb{R}_*^{n \times p} / \mathcal{O}_p$, a line-search-based Riemannian inexact Newton method is developed with its global convergence and local superlinear convergence rate guaranteed. Moreover, we investigate the influence of the existing three Riemannian metrics on $\mathbb{R}_*^{n \times p} / \mathcal{O}_p$ and derive new preconditioners which takes $M \neq I$ into consideration. Numerical experiments show that IRRLyap with one of the Riemannian metrics is most efficient and robust in general and is preferable compared to the tested state-of-the-art methods when the lowest rank solution is desired.

\begin{keywords}

Generalized Lyapunov equations, Riemannian optimization, Low-rank approximation, Riemannian truncated Newton's method, Increasing rank method
\end{keywords}

\end{abstract}

% 写论文首先需要考虑好这篇论文的创新点在什么地方。之后其他一切的描述全部是围绕这想要说明的这些创新点展开的。

% 重点！！千万不要原文照抄现有论文的内容，即使是句子也不行，有剽窃的嫌疑。剽窃在学术圈是非常严重的事情，所以一定要注意。当然，单词短语肯定是会重复的。这个很正常。

\section{Introduction} \label{Intro}

This paper considers the large-scale generalized Lyapunov matrix equation 
\begin{equation}
  AXM + MXA - C = 0, \label{Intro-LyapMatEqu}
\end{equation} 
where $A, M \in \mathbb{R}^{n\times n}$ are symmetric positive definite matrices and $C \in \mathbb{R}^{n\times n}$ is symmetric. It has been shown in~\cite{Chu87} and~\cite{penzl_numerical_1998} that Equation~\eqref{Intro-LyapMatEqu} has a unique solution $X^*$ and $X^*$ is symmetric. 
% \zwhcomm{And thus, the solution $X^*$ is symmetric too if Equation (\ref{Intro-LyapMatEqu}) has unique solution. We Further assume that $A$ and $M$ are positive definite. By these assumptions, Equation (\ref{Intro-LyapMatEqu}) has unique solution; see \cite{Chu87} or \cite{penzl_numerical_1998}.}{} 
 Moreover, it is known that under certain circumstances such as $C$ is low rank, the solution $X^{*}$ of the Lyapunov equation \eqref{Intro-LyapMatEqu} can be well approximated by a low-rank matrix, since the eigenvalues of the solution $X^*$ numerically has an exponential decay~\cite{PENZL00,Grasedyck2004ExistenceOA}. 
 %For example, when $C$ is low rank, %\sout{the solution $X^*$ is also low rank} 
 %\zwhcomm{the eigenvalues of the solution $X^*$ have an numerically exponential decay}{}. 
%\sout{Consequently, in Equation (\ref{Intro-LyapMatEqu}), we assume that $A,M\in \mathbb{R}^{n\times n}$ are symmetric positive definite (SPD) and $C \in \mathbb{R}^{n\times n}$ is a symmetric and low-rank matrix. By our assumptions, Equation (\ref{Intro-LyapMatEqu}) has a symmetric and unique solution; see \cite{Chu87} or \cite{penzl_numerical_1998}}.

In recent years, solving Equation (\ref{Intro-LyapMatEqu}) has attracted much attention since it plays a significant role in model reduction~\cite{Moore1981PrincipalCA,Schilders2014ModelOR,Hamadi2021AMR,ISC22}, signal processing~\cite{Goyal2020ImageDR}, systems and control theory~\cite{LMESSC96,Antoulas2005ApproximationOL}, and solutions of PDEs~\cite{DV16}. 

%For example, consider the following generalized linear invariant-time system 
%\[	
%	\Sigma(E,A,C):\begin{cases}
%		E\dot{x}(t) = Ax(t), \\ 
%		\;\;\;y(t) = Cx(t),
%	\end{cases}
%\]
%	where $x(t)$ and $y(t)$ are the state vector and output vector respectively. \sout{Then} \zwhcomm{The solution of }{} the following generalized Lyapunov matrix equation 
%	\[
%		A^TXE + E^TXA + E^TC^TCE=0
%	\] 
%	captures the stability of $(E,A)$ \cite{Ishihara02}.
	
\subsection{Contributions}

In this paper, Problem~\eqref{Intro-LyapMatEqu} is formulated as an optimization problem on a quotient manifold $\mathbb{R}_*^{n \times p} / \mathcal{O}_p$ (see definition in Section~\ref{PrelNotat}). A line-search-based Riemannian truncated Newton method is developed by using the truncated Newton in~\cite{dembo_truncated-newton_1983} and the line search conditions in~\cite{Byrd1989ATF}. The global convergence and local superlinear convergence rates are established. As far as we know, this is the first line-search-based generic Riemannian inexact Newton method for optimization and guarantees global and local superlinear convergence. In addition, the line search conditions that we used allow conditions other than the Wolfe conditions in~\cite{dembo_truncated-newton_1983}. We equip the quotient manifold $\mathbb{R}_*^{n \times p} / \mathcal{O}_p$ with three existing Riemannian metrics and explore the performance of the Riemannian truncated Newton method and the influence of those metrics for solving~\eqref{Intro-LyapMatEqu}. Moreover, we propose new preconditioners which are more effective compared to the one in~\cite{Bart10} in the sense that the new ones take $M \neq I$ into consideration whereas the one in~\cite{Bart10} does not. We finally combine the Riemannian truncated Newton method with the increasing rank algorithm in~\cite{Bart10} and give an algorithm for solving~\eqref{Intro-LyapMatEqu}.
Numerically, the proposed Riemannian truncated Newton's method is able to find higher accurate solutions compared to the Riemannian trust-region Newton's method in~\cite{Absil2007TrustRegionMO}; the proposed preconditioners can further reduce the number of actions of the Hessian evaluations when $M \neq I$ compared to the preconditioner in~\cite{Bart10}; and the increasing rank method with the Riemannian truncated Newton method is able to find lower rank solutions compared to the three state-of-the-art low-rank methods K-PIK~\cite{simoncini_new_2007}, mess\_lradi~\cite{SaaKB21-mmess-2.2}, and RLyap~\cite{Bart10} when the residual $\|AXM+MXA-C\|_F$ is roughly the same.

% On the other hand, under the same accuracy, proposed solver gives the most low-rank solution in comparison with the two state-of-the-art low-rank solvers K-PIK~\cite{simoncini_new_2007} and mess\_lradi~\cite{SaaKB21-mmess-2.2}. %\zwhcomm{}{}.


%Firstly we proposed a Riemannian truncated-Newton's method which is a generalization of the one in \cite{dembo_truncated-newton_1983} to Riemannian manifolds, and analysed the global and local convergence. 
% We also explored the performance of the algorithm under three different Riemannian metrics on the quotient manifold for solving Equation (\ref{Intro-LyapMatEqu}). At the same time, We also designed preconditioners, which are extensions of the one in \cite{Bart10}, under each of the three metrics. \zwhcomm{The details need be highlighted}{}

\subsection{Related Work}

%\sout{The classical numerical methods for the standard Lyapunov matrix equation, i.e., setting $M=I$, are the Bartels-Stewart method \cite{BS72}, the Hammarling method \cite{Ham82} and the Hessenberg–Schur method \cite{GN79}, which are based on the Schur decomposition of the system matrix A. }
Classical numerical methods for the standard Lyapunov matrix equations, i.e., setting $M=I$, include the Bartels-Stewart method \cite{BS72}, the Hessenberg–Schur method \cite{GN79}, %which is based on the Schur decomposition of the system matrix $A$, 
and the Hammarling method \cite{Ham82}. These methods are direct methods and the computations involve $O(n^3)$ number of floating point operations, which prevents the use for large-scale problems, where the floating point operation is defined in~\cite{Golub1996MatrixC}. One approach to develop an efficient method for large-scale problems is to explore the low-rank structure of the solution $X^*$.
%Hence, it is important to design a solver for computing large-scale problems.

%compute the matrix $X$ of size $n\times n$ in $O(n^3)$ floating point operations \zwhcomm{according to \cite{Golub1996MatrixC}}{}. This gives rise to that they only are fit for small-scale problems, e.g, $n\le 10^4$. It, hence, is especially important to design a solver for computing large-scale problems.

% \whcomm{ZHTODO}{}

\paragraph{Low-rank methods for Lyapunov equations.} In practice, there is a class of applications of generalized Lyapunov equation (\ref{Intro-LyapMatEqu}), whose solution %\sout{$X^*$ has rank $p\ll n$} 
 can be approximated with a low-rank matrix of rank $p\ll n$. In this case, the number of unknowns is reduced to $np$, which is greatly less than $n^2$. If we can compute a low-rank approximation in $O(np^c)$ operations with constant $c$, for large problems, then the complexity is significantly reduced.
 % the benefits of doing so are very obvious. 
 Based on this idea, very diverse low-rank methods have been proposed. 
Majority of these methods are based on Smith method~\cite{Gugercin2003}, low-rank alternating direction implicit iterative (LR-ADI)~\cite{low-rank_2004,Benner2009OnTA,Benner2014ComputingRL,PDJ22}, sign function methods~\cite{Baur2006FactorizedSO,Baur2008LowRS}, Krylov subspace techniques~\cite{simoncini_new_2007,Druskin2011AdaptiveRK,Hamadi2021AMR}, and Riemannain optimization~\cite{Bart10}. It is worth noting that there exist several low-rank methods for algebraic Riccati equations~\cite{Mishra2013ARA,Benner2021ALS}, which, in some cases, are also suited for Lyapunov equations.
	
	
	% \sout{A kind of methods are focused on Smith method whose typical representative is the modified LR-Smith(l) method~\cite{Gugercin2003}. The work in~\cite{low-rank_2004} presented alternating direction implicit (ADI) based low-rank solvers which is the Cholesky factor–alternating direction implicit (CF–ADI) algorithm. For the low-rank ADI method, one of the most computationally expensive steps is solving the shifted systems and thus Benner et al.~\cite{PDJ22} proposed a new scheme that use the extended Krylov subspace method for solving shifted systems. Simoncini~\cite{simoncini_new_2007} concentrated on methods based on Krylov subspaces and proposed the Krylov-plus-inverted-Krylov (KPIK) method. The work in~\cite{LW07} is based on multigrid algorithms.}
	 
		
		The low-rank methods listed above in addition to~\cite{Bart10,Mishra2013ARA} all start from well-known iterative methods and skillfully rewrite the problem so that the problem can be transformed into a low-rank setting. Although the calculations of these methods are cheap at each step since they work on a factor $Y$ of iterate $X=YY^T$, the convergence rate of these methods is usually unknown or at most linear and no good acceleration strategy has been given. As a consequence, Bart et al. \cite{Bart10} reformulated Equation (\ref{Intro-LyapMatEqu}) into an optimization problem defined on the set of fixed rank symmetric positive semidefinite matrices. %, a manifold, by minimizing the energy norm of the error. 
  In this paper, we follow this idea but from the perspective of quotient manifold and %\sout{use a new} \zwhcomm{
  propose a
  %}{} 
  Riemannian Newton's method to solve %\sout{our} \zwhcomm{
  the
  %}{} 
  problem. 

\paragraph{Riemannian Newton's methods.} Newton's method is a powerful tool for finding the minimizer of nonlinear functions in Euclidean spaces. Although Newton’s method has a fast local convergence rate, it is highly sensitive to the initial iterate (i.e., it is not globally convergent); it is not defined at points where the Hessian is singular; and for non-convex problems, it does not necessarily generate a sequence of descent directions. In order to overcome these issues, Dembo et al. \cite{dembo_truncated-newton_1983} proposed a truncated-Newton method. The truncated-Newton method contains two parts. The outer iteration executes Newton's method based on line search, and the inner iteration solves the Newton equation inexactly by a truncated conjugate gradient method. % solved once in each major iteration. 
% \sout{so} \zwhcomm{
% To this end, %}{} 
% conjugate gradient method is chosen to solve Newton's equation in the minor iteration. 
Absil et al. \cite{Absil2007TrustRegionMO} proposed a Riemannian trust-region Newton's method similar to this idea which is based on the trust-region method. The proposed method is a generalization of the one in \cite{dembo_truncated-newton_1983} to manifolds and further relaxes the line search condition used in~\cite{dembo_truncated-newton_1983}.  %
It is worth mentioning that there are some Riemannian Newton's methods based on line search; see, e.g.,~\cite{Bortoloti2018DampedNM,Zhao2018ARI, Wang2020RiemannianNM, Bortoloti2021AnED}. 
% \sout{They are different from our proposed method, such as the choice of search direction and line search conditions} 
As far as we know, all the existing Riemannian Newton's methods aim to find a root of a vector field, whereas the proposed Riemannian truncated-Newton's method is used to optimize a sufficiently smooth function.

% \zwhcomm{In essence, they are different from our proposed method which is to find a minimizer of a sufficiently smooth real-valued function, since they are designed to find a singularity of a differentiable vector field. Besides, in~\cite{Bortoloti2018DampedNM, Bortoloti2021AnED}, a descent direction is set as the negative gradient direction of a metric function if no Newton's direction exists.}{}



\subsection{Outline}

This paper is stated as follows. In Section \ref{PrelNotat}, we introduce preliminaries on algebra and manifold. In Section \ref{ProbStat}, we reformulate Equation (\ref{Intro-LyapMatEqu}) as an optimization problem on quotient manifold $\mathbb{R}_*^{n\times p}/\mathcal{O}_p$. The classical truncated Newton method is generalized to Riemannian setting in Section \ref{RieNew} and it is proven that the proposed Riemannian truncated Newton's method converges to a stationary point from any starting point and has a local superlinear convergence rate. In Section~\ref{QuotMani}, we give ingredients for optimization on Riemannian quotient manifolds with three metrics. In Section \ref{FinalAlgPrecond}, The proposed preconditioners under the three metrics are discussed, and the increasing rank algorithm for solving Equation (\ref{Intro-LyapMatEqu}) is given. Finally, in Section \ref{NumExp}, we demonstrate the performance of the three metrics and preconditioners, and compare the proposed method with other existing low-rank methods for Lyapunov equations. 



% 首先，介绍章节是非常重要的！它一方面可以让审稿人了解你是否真正明白你做的是什么，是否了解这个领域，另一方面也让读者明确的知道你工作的创新点和重要性。具体来说，介绍部分可以从这几个维度展开。
% 1，我们考虑什么问题
% 2，我们为什么考虑这个问题
% 3，这个领域目前有哪些进展
% 4，我们做了什么工作，创新点和贡献在哪里
% 5，论文结构介绍
% 上面写的是一个一般性的框架，对于我们计算数学类的文章基本可以通用。但是也不是绝对的。介绍部分的框架也可以根据情况进行修改或者调整。其中心思想就是把想说的故事精确完整的展现给读者。

% 我们逐点展开做一下说明。
% 1，我们考虑什么问题
% 这里一般不给细节，从大的层面介绍考虑的问题即可。比如考虑图像降噪就直接说图像降噪，不用给出具体数学建模后的模型是什么样的。但是也有例外，比如如果这篇论文的创新点就是在数学建模细节上与众不同，为了方便之后介绍创新点，也可以给出数学模型。

% 2，我们为什么考虑这个问题
% 这里其实是说明我们考虑这个问题的重要性，向审稿人说明我们考虑的问题是有影响力的。这里就需要充分的文献支持。如果是考虑应用的，就需要给出在各种实际问题中的应用，这个时候需要通过引用文献表示确实在这些领域有人用过，不能只是没有引用的空话。如果是考虑算法，就可以说明算法在各个领域用来解决过各种问题，也需要充分引用。通过“别人已经做了不少工作，收到很多关注”来表示我们的工作不是凭空而来的。不过如果这个问题之前从来没有人做过，完全是创新的，那么这里的描述就不一样了，文章的侧重点就变成了想读者证明为什么这个问题是重要的了。不过开天辟地的创新工作很少了。一般来说，多少都会有相关工作的。

% 3，这个领域有哪些进展及相关结果介绍
% 这个章节一方面是向读者介绍这个领域的情况，另一方面也是向审稿人和编委说明你了解这个领域的发展情况。因此，想要写好这一部分，需要有充分调研，如果遗漏了什么重要的相关工作可能会对论文的创新性造成影响。这部分的写作也是为下一部分创新点的描述打基础。只有你了解了这个领域大家的工作，你才能知道相比其他人，你的工作是否有创新，创新点在哪里。因此介绍这个领域的时候一定需要把最相关的结果介绍到位，比如我们算法创新方面的论文，那么就需要说明解决类似问题的算法都有哪些（通过文献引用来说明），然后用一两句话概括每一种算法的特点，优劣势的场景等。如果是应用方面的，我们提出新模型的，则需要说明现有文献中大家都采用了哪些模型，这些模型的特点，优劣势的场景等。如果是应用方面，针对某个现有模型我们有新算法的，则可以着重介绍这个模型上现有的算法以及特点等。这部分也需要大量引用论文来说明。

% 4，我们做了什么工作，创新点和贡献在哪里
% 有了前面的相关结果介绍，这里我们就可以非常精确的逐点介绍这篇论文的创新点在什么地方。创新点可以是别人没有做过的，或者之前没有的结果，或者新的发现等等。这个段落很关键，其他的内容本质上就是在说明你的创新点和贡献，证明你的创新点和贡献。

% 5，论文结构介绍
% 这个有点程序性的工作，就是介绍往后每个章节是干什么的。


% 我后面给出几个常用的章节例子，它们不一定是必须的，有些情况可能会加入新的章节。
\section{Preliminaries and Notation} \label{PrelNotat}

Throughout this paper, we denote the real matrices space of size $m$-by-$n$ by $\mathbb{R}^{m\times n}$. For $A\in\mathbb{R}^{m\times n}$, the matrix $A^T\in\mathbb{R}^{n\times m}$ denotes the transpose of $A$. Define $\mathrm{vec}(\cdot): \mathbb{R}^{m\times n} \rightarrow \mathbb{R}^{mn}$ as an isomorphism operator which transforms a matrix into a vector by column-wise stacking, that is, for any $A=[a_1,a_2,\cdots,a_n]\in\mathbb{R}^{m\times n}$. We have $\mathrm{vec}(A)=[a_1^T,a_2^T,\dots,a_n^T]\in\mathbb{R}^{mn}$.
Let $\otimes$ denote the Kronecker product, i.e., for $A=[a_{ij}]\in \mathbb{R}^{m\times n}$ and $B=[b_{ij}]\in\mathbb{R}^{p\times q}$, it holds that $A\otimes B=[a_{ij}B]\in\mathbb{R}^{mp\times nq}$. 
%Then with the identity $\mathrm{vec}(ABC)=(C^T\otimes A)\mathrm{vec}(B)$ for $A, B,C\in \mathbb{R}^{n\times n}$, we have that Equation (\ref{Intro-LyapMatEqu}) is equivalent to 
%
%\begin{equation}
%  \label{Pre_Not-Euiv_LinearEqu}
%  L\mathrm{vec}(X)=\mathrm{vec}(C),
%\end{equation}
%where $L=A\otimes M+M\otimes A$. By our assumptions, i.e., $A$ and $M$ are symmetric positive definite, the matrix $L$ is symmetric positive definite \cite[Proposition 3.1]{Bart10}. Because $L$ is positive definite, it is not difficult to verify that the function $\|\cdot\|_L=\sqrt{\left<\cdot,\cdot\right>_L}$ with $\left<x,x\right>_L=\left<x,Lx\right>=x^TLx$ defines a norm over $\mathbb{R}^n$ called $L$-norm. 

\paragraph{Riemannian geometry.} The Riemannian geometry used in this paper can be found in the standard literature, e.g.,~\cite{Boothby1975AnIT, AbsMahSep2008}, and the notation below follows~\cite{AbsMahSep2008}. Denote a manifold by $\mathcal{M}$. For any $x\in \mathcal{M}$, $\mathrm{T}_x\mathcal{M}$ is the tangent space of $\mathcal{M}$ at $x$ and elements in $\mathrm{T}_x\mathcal{M}$ are call tangent vectors of $\mathcal{M}$. Tangent bundle, $\mathrm{T}\mathcal{M} $, of manifold $\mathcal{M}$ is the union set of all tangent spaces. Mapping $\eta:\mathcal{M}\rightarrow \mathrm{T}\mathcal{M}$ maps a point $x\in\mathcal{M}$ into a tangent vector $\eta_x\in\mathrm{T}_x\mathcal{M}$ and is called a vector field. 
A metric on a manifold $\mathcal{M}$ is defined as $g(\cdot,\cdot): \mathrm{T}\mathcal{M} \oplus \mathrm{T}\mathcal{M} \rightarrow \mathbb{R}$, where $\mathrm{T}\mathcal{M} \oplus \mathrm{T}\mathcal{M}=\{(\eta_x,\xi_x):x\in \mathcal{M},\eta_x\in\mathcal{T}_x\mathcal{M},\xi_x\in\mathrm{T}_x\mathcal{M}\}$ is the Whitney sum of tangent bundle. Given $\eta_x \in \T_x \mathcal{M}$, its induced norm is defined by $\|\eta_x\| = \sqrt{g(\eta_x, \eta_x)}$.
If $g$ is smooth in the sense that for any two smooth vector fields $\xi$ and $\eta$, function $g(\xi,\eta):\mathcal{M}\rightarrow \mathbb{R}$ is smooth, then $\mathcal{M}$ equipped with $g$ is a Riemannian manifold and $g$ is called a Riemannian metric. 
If $\mathcal{M}$ is a Riemannian manifold, then the tangent space $\mathrm{T}_x\mathcal{M}$ is a Euclidean space with the Euclidean metric $g_x(\cdot,\cdot)=g(\cdot,\cdot)|_{\mathrm{T}_x\mathcal{M}}$. For a smooth function $f:\mathcal{M}\rightarrow \mathbb{R}$, notations $\mathrm{grad}f(x)$ and $\mathrm{Hess}f(x)$ denote the Riemannian gradient and Hessian of $f$ at $x$ respectively, and the action of $\mathrm{Hess}f(x)$ on a tangent vector $\eta_x\in\mathrm{T}_x\mathcal{M}$ is denoted by $\mathrm{Hess}f(x)[\eta_x]$. 
%Let $\mathcal{H}_x$ be a linear operator on $\T_x\M$. Its adjoint operator, denoted by $\mathcal{H}_x^*$, is defined by $g_x(\eta_x,\mathcal{H}_x\xi_x)=g_x(\mathcal{H}^*_x\eta_x,\xi_x)$ for all $\eta_x,\xi_x\in \T_x\M$. With $\mathcal{H}_x=\mathcal{H}_x^*$, we call $\mathcal{H}_x$ self-adjoint with respect to $g$.

Let $\gamma:\mathcal{I} \rightarrow \mathcal{M}$ denote a smooth curve and $\dot{\gamma}(t)\in\mathrm{T}_{\gamma(t)}\mathcal{M}$ denotes its velocity at $t$, where $\mathcal{I} \subset \mathbb{R}$ is open and $[0, 1] \subset \mathcal{I}$. The distance between $x=\gamma(0)$ and $y=\gamma(1)$ is defined by $\mathrm{dist}(x,y)= \inf_{\{\gamma:\gamma(0)=x,\gamma(1)=y\}}\int_0^1\|\dot{\gamma}(t)\|\mathrm{d}t$. A smooth curve $\gamma:[0,1]\rightarrow \mathcal{M}$ is called the geodesic if it locally minimizes the distance between $\gamma(0)=x$ and $\gamma(1)=y$. %\zwhcomm{
Notation $B_{\mu}(0_x)=\{\xi_x\in \mathrm{T}_x\mathcal{M}: \|\eta_x\| \le \mu\}$ is used to denote the open ball in $\mathrm{T}_x \mathcal{M}$ of radius $\mu$ centered at $0_x$ and $B_{\mu}(x)=\{y\in\mathcal{M}:\mathrm{dist}(x,y)\le \mu\}$ is an open ball in $\mathcal{M}$ of radius $\mu$ centered at $x$. A retraction $R:\T\M \rightarrow \M$ is a map satisfying (i) $R(0_x)=x$ for all $x\in \M$ and (ii) for each fixed $x\in \M$, $\frac{\mathrm{d}}{\mathrm{d} t}R(t\eta_x)\big|_{t=0}=\eta_x$ for all $\eta_x\in\T_x\M$. 
%}{}
%For any $\eta_x\in\mathrm{T}_x\mathcal{M}$, the exponential map is defined as $\mathrm{Exp}_{x}(\eta_x)=\gamma(1)$, where $\gamma(0)=x$ and $\dot{\gamma}(0)=\eta_x$ is the geodesic. \zwhcomm{The injectivity radius of $\mathcal{M}$ at $x$, denoted by $\mathrm{inj}(x)$ is the supremum over radii $r>0$ such that $\mathrm{Exp}_x$ is defined and is diffeomorphism on the open ball $B_r(x)=\{u\in\mathrm{T}_x\mathcal{M}:\|u\|_x\le r\}$.}{} Parallel transport on $\mathcal{M}$ defined as $\mathrm{P}_{0\rightarrow1}^{\gamma}:\mathrm{T}_{\gamma(0)}\mathcal{M}\rightarrow\mathrm{T}_{\gamma(1)}\mathcal{M}$ is a linear isometry along the geodesic $\gamma$.   
%\zwhcomm{For details, please refer to \cite{AbsMahSep2008} or \cite{Boumal:300281}.}{}

\paragraph{The considered manifold.} The set of symmetric positive semidefinite matrices of size $n$ with fixed rank $p$, denoted by $\mathcal{S}_+(p,n)$, is a Riemannian manifold with dimension $np-\frac{1}{2}p(p-1)$ (see, e.g., \cite{Helmke1995CriticalPO}). For any $X\in\mathcal{S}_+(p,n)$, there exists a matrix $Y\in \mathbb{R}^{n\times p}_*$ such that $X=YY^T$, where $\mathbb{R}^{n\times p}_* = \{Y \in \mathbb{R}^{n \times p} : \mathrm{rank}(Y) = p\}$ is called a noncompact Stiefel manifold \cite[Chapter 3]{AbsMahSep2008}. The orthogonal group $\mathcal{O}_p = \{O \in \mathbb{R}^{p \times p}: O^T O = I_p\}$ is a Lie group with the group operator given by the matrix product; it can be equipped with the smooth structure as an embedded submanifold of $\mathbb{R}^{p \times p}$; and its identity is the identity matrix $I_p$. Define a right group action of $\mathcal{O}_p$ on $\mathbb{R}^{n\times p}_*$ as: 

\[
  \theta :\mathbb{R}^{n\times p}_* \times \mathcal{O}_p \rightarrow \mathbb{R}^{n\times p}_*:\theta(Y,Q)=YQ.
  \]
Obviously, this group action satisfies the identity and compatibility conditions~\cite[Definition 9.11]{Boumal:300281}, and therefore it induces an equivalent relation $\sim$ on $\mathbb{R}^{n\times p}_*$: 

\[
  Z \sim Y \Leftrightarrow Y=\theta(Z,Q) = ZQ \text{ for some } Q\in \mathcal{O}_p.
  \]
The orbit of $Y$ forms an equivalence class, namely, $[Y]=\{YQ:Q\in \mathcal{O}_p\}$. We denote the quotient space $\mathbb{R}^{n\times p}_*/\sim$ as $\mathbb{R}_*^{n\times p}/\mathcal{O}_p$. The natural projection between $\mathbb{R}^{n\times p}_*$ and $\mathbb{R}_*^{n\times p}/\mathcal{O}_p$ is given by 

\[
  \pi: \mathbb{R}^{n\times p}_* \rightarrow \mathbb{R}^{n\times p}_* / \mathcal{O}_p : Y \mapsto \pi(Y)=[Y].
  \]
From now on, out of clarity, we denote $[Y]$ by $\pi(Y)$ as an element of the quotient manifold $\mathbb{R}_*^{n\times p}/\mathcal{O}_p$ and use $\pi^{-1}(\pi(Y))$ to denote $[Y]$ when it is regarded as a subset of $\mathbb{R}_*^{n\times p}$. 

Since the Lie group $\mathcal{O}_p$ acts smoothly, freely and properly with the group action $\theta$ on the smooth manifold $\mathbb{R}^{n\times p}_*$, by~\cite[Theorem 9.17]{Boumal:300281}, the quotient space $\mathbb{R}^{n\times p}_*/ \mathcal{O}_p$ is a manifold called a quotient manifold of $\mathbb{R}^{n\times p}_*$. The manifold $\mathbb{R}_*^{n \times p}$ is therefore called the total space of $\mathbb{R}^{n\times p}_*/ \mathcal{O}_p$. Since $\mathbb{R}^{n\times p}_*/\mathcal{O}_p$ is diffeomorphic to $\mathcal{S}_+(p,n)$, the quotient manifold $\mathbb{R}^{n\times p}_*/\mathcal{O}_p$ can be viewed as a representation of the manifold $\mathcal{S}_+(p,n)$. 

%Indeed, first of all, $\beta$ must be surjection; besides let $\xcancel{[Y],[Z]\in \mathbb{R}^{n\times p}_*} \zwhcomm{\pi(Y),\pi(Z)\in\mathbb{R}^{n\times p}_*/\mathcal{O}_p}{}$ such that \sout{$\beta([Y])\not=\beta([Z])$} \zwhcomm{$\beta(\pi(Y))\not=\beta(\pi(Z))$}{}, i.e., $YY^T\not=ZZ^T$, which implies that there exists no $Q\in \mathcal{O}_p$ such that $Y=ZQ$, and thus \sout{$[Y]\not=[Z]$} \zwhcomm{$\pi(Y)\not=\pi(Z)$}{}.

\paragraph{Function classes.} 
% {\color{red}
For the convergence analysis, we need notions of a pullback of a real-valued function $f$ with respect to a retraction $R$ and the radially Lipschitz continuous differentiability of a pullback. 
\begin{definition} \label{PrelNotat-Pullback}
	Let $f: \mathcal{M} \rightarrow \mathbb{R}^n$ be a real-valued function on manifold $\mathcal{M}$ with a retraction $R$. We call $\hat{f}:\mathrm{T}\mathcal{M} \rightarrow \mathbb{R}: \eta_x \mapsto \hat{f}(\eta_x)=f(R(\eta_x))$ the pullback of $f$ with respect to $R$. When restricted on $\mathrm{T}_x\mathcal{M}$, we denote $\hat{f}\big|_{\mathrm{T}_x\mathcal{M}}$ by $\hat{f}_x$.
\end{definition}
% }
\begin{definition} (\cite[Definition 7.4.1]{AbsMahSep2008}) 
\label{PrelNotat-RLipConti}
  Let $\hat{f}$ be a pullback of $f$ with respect to a retraction $R$. $\hat{f}$ is referred to as a radially $L$-$C^1$ function for all $x\in \mathcal{M}$ if there exists a positive constant $L$ such that for all $x\in \mathcal{M}$ and all $\eta_x\in \mathrm{T}_x\mathcal{M}$, it holds that 
  \begin{equation}
    \begin{aligned}
      \bigg| \frac{\mathrm{d}}{\mathrm{d}\tau}\hat{f}(t\eta_x)\big|_{t=\tau} - \frac{\mathrm{d}}{\mathrm{d}t}\hat{f}(t\eta_x)\big|_{t=0} \bigg|\le L \tau\|\eta\|^2,
    \end{aligned}
  \end{equation}
  where $\tau$ and $\eta_x$ satisfy that $\mathrm{R}_x(\tau\eta_x)\in \mathcal{M}$.
\end{definition}


%\begin{definition}(\cite[Definition 10.4.2]{Boumal:300281})
%	\label{PrelNotat-VecFieldLipConti}
%	A vector field $\eta$ on a connected manifold $\mathcal{M}$ is $L$-Lipschitz continuous if, for all $x,y\in\mathcal{M}$ with $\mathrm{dist}(x,y)<\mathrm{inj}(x)$, it holds that 
%	\[
%		\|\mathrm{P}_{0\leftarrow 1}^{\gamma}\eta_y-\eta_x\|\le L\mathrm{dist}(x,y),
%	\]
%	where $\gamma:[0,1]\rightarrow \mathcal{M}$ is the unique minimizing geodesic connecting $x$ to $y$, and $\mathrm{inj}(x)$ is the injectivity radius of $\mathcal{M}$ at $x$.
%\end{definition}

%For $x\in\mathcal{M}$, we also need the notion of Riemannian Lipshitz continuity for a linear operator $H(x):\mathrm{T}_x\mathcal{M} \rightarrow \mathrm{T}_x\mathcal{M}$. 
%\begin{definition}
%	Suppose that $\mathcal{M}$ is connected. We say $H$ is L-Lipschitz continuous if for all $x,y\in\mathcal{M}$ such that $\mathrm{dist}(x,y)<\mathrm{inj}(x)$ we have 
%	\[
%	\|\mathrm{P}_{0\leftarrow 1}^{\gamma}\circ H(y) \circ \mathrm{P}_{1\leftarrow0}^{\gamma}-H(x)\| \le L\mathrm{dist}(x,y),
%	\]
%	where $\|\cdot\|$ denotes the operator norm with respect to the Riemannian metric and $\gamma:[0,1]\rightarrow \mathcal{M}$ is the unique minimizing geodesic connecting $x$ to $y$.
%\end{definition}

\paragraph{Notations.} %At the end of this section, we list some notations that will be used later. 
For any $Y\in \mathbb{R}_*^{n\times p}$ with $p<n$, the matrix $Y_\perp$ of size $n\times(n-p)$ denotes the normalized orthogonal completment of $Y$, i.e., $Y_\perp^TY_\perp=I_{n-p}$ and $Y_\perp^TY=0$. Furthermore, $Y_{\perp_M}$ is the normalized orthogonal complement of $MY$. The sysmmetric matrices space of size $n\times n$ is denoted by $\mathcal{S}_n^{\mathrm{sym}}$. For any matrix $X\in\mathcal{S}_n^{\mathrm{smy}}$, the symbols $X \succ0$ and $X\succeq0$ mean that $X$ is positive definite and positive semidefinite respectively. For a square matrix $X$, $\mathrm{trace}(X)$ denotes the sum of all diagonal elements. For $Y\in\mathbb{R}^{n\times p}$, let $\mathrm{sym}(Y)=(Y+Y^T)/2$ and $\mathrm{skew}(Y)=(Y-Y^T)/2$.


% 这个章节主要给出预备知识和符号。对于流形优化的论文，这个其实蛮常见的，因为流形优化用到的符号蛮多的，而且不少预备知识其实不是这个领域的专家不会很了解。所以需要给出相关的参考文献帮助读者了解。不过有些论文也可以不用这个章节。而是在第一次用到某个新概念的时候，马上进行介绍。

\section{Problem Statement} \label{ProbStat}

% \zwhcomm{
% Apply $\mathrm{vec(\cdot)}$ to the left and right sides of Eqiation~\eqref{Intro-LyapMatEqu} to get $L\mathrm{vec}(X)=\mathrm{vec}(C)$, where $L=A\otimes M + M\otimes A$ is symmetric positive definite by our assumptions~\cite[Proposition 3.1]{Bart10}. Thus, the function $\|\cdot\|_L: v \mapsto \sqrt{v^TLv}$ is a norm called $L$-norm. Let $X$ and $X^*$ denote the approximation of rank $p$ and the exact sulotuon of Equation (\ref{Intro-LyapMatEqu}) respectively. From the perspective of optimization, we can solve the Lyapunov equation (\ref{Intro-LyapMatEqu}) by minimizing the $L$-norm of the error $E=X-X^*$. This idea has been used in \cite{Bart10}, in which the following problem firstly is obtained 

We consider a fixed-rank optimization formulation of~\eqref{Intro-LyapMatEqu} in~\cite{Bart10}
% \begin{align*}
% 	&\min_{X\in \mathbb{R}^{n\times n}}  h(X):=\mathrm{trace}(XAXM) - \mathrm{trace}(XC), \\ 
% 	&\;\; \text{ s.t. } X\in \tilde{\mathcal{S}}_+(p,n)=\{X\in\mathbb{R}^{n\times n}:X\in\mathcal{S}_n^{\mathrm{sym}},\mathrm{rank}(X)\le p\}.
% \end{align*}
% Noting that $\tilde{\mathcal{S}}_+(p,n)$ is not a manifold, if the solution of Equation (\ref{Intro-LyapMatEqu}), $X^*$, has rank not less than $p$, by our assumption, then the minimizer of $h(X)$ over $\widetilde{\mathcal{S}}(p,n)$ has rank $p$ \cite[Proposition 3.2]{Bart10}. The optimization problem, hence, is relaxed as:
\begin{equation} 
  \begin{aligned} \label{Pro_stat-OptProb_Bart}
   & \min_{X\in \mathbb{R}^{n\times n}} h(X) :=\mathrm{trace}(XAXM) - \mathrm{trace}(XC), \\ 
   &\quad \text{ s.t. } X\in \mathcal{S}_+(p,n).
  \end{aligned}
\end{equation}
Note that the Euclidean gradient of~\eqref{Pro_stat-OptProb_Bart} is $\nabla h(X)=AXM + MXA - C.$ It follows that any stationary point of $h(X)$ is a solution of~\eqref{Intro-LyapMatEqu}. Since $\mathbb{R}_*^{n \times p} / \mathcal{O}_p$ is diffeomorphic to $\mathcal{S}_+(p, n)$, Problem~\eqref{Pro_stat-OptProb_Bart} can be equivalently reformulated as\footnote{The term ``equivalent'' means if $X\in \mathcal{S}_+(p,n)$ is a stationary point of $h$ then $\pi(Y)\in \mathbb{R}^{n\times p}/\mathcal{O}_p$ satisfying $X=YY^T$ is also a stationary point of $f$; conversely, if $\pi(Y)$ is a stationary point of $f$, then $X=YY^T$ is a stationary point of~$h$.}: 
\begin{equation}
  \begin{aligned} \label{Pro_stat-FinalProb} %\label{Pro_stat-FuncOnQuotMani}
    \min_{\pi(Y)} f: \mathbb{R}_*^{n\times p}/\mathcal{O}_p \rightarrow \mathbb{R} :  \pi(Y) \mapsto h(YY^T)=\mathrm{trace}(Y^TAYY^TMY) - \mathrm{trace}(Y^TCY),
  \end{aligned}
\end{equation}
which is defined on the quotient manifold $\mathbb{R}_*^{n\times p}/\mathcal{O}_p$. Therefore, Riemannian optimization algorithms can be used. Note that Problem~\eqref{Pro_stat-FinalProb} is different from the Burer and Monteiro approach~\cite{Burer2003} in the sense that \cite{Burer2003} optimizes over the factor $Y$ but Problem~\eqref{Pro_stat-FinalProb} is over the quotient manifold~$\mathbb{R}_*^{n\times p}/\mathcal{O}_p$.

Problem~\eqref{Pro_stat-FinalProb} requires a good estimation of the rank $p$, which is usually unknown in practice. It has been shown in~\cite{Bart10} that if the rank of the solution of~\eqref{Intro-LyapMatEqu} is larger than $p$, then any local minimizer of the fixed-rank formulation~\eqref{Pro_stat-FinalProb} must have full rank $p$. Therefore, one can estimate the rank $p$ by using a rank-increasing algorithm. The proposed Riemannian optimization over fixed rank manifold is discussed in Section~\ref{RieNew} and the rank-increasing algorithm is described in Section~\ref{FinalAlgPrecond}.

% Then our purpose is to minimize the objective function $f$   over $\mathbb{R}^{n\times p}_*/\mathcal{O}_p$, that is, 
% \begin{equation}
%   \begin{aligned} \label{Pro_stat-ProbOnQuotMani}
%  	\min_{\pi(Y)}f(\pi(Y)) \text{ s.t. }\pi(Y) \in \mathbb{R}_*^{n\times p}/\mathcal{O}_p.
%   \end{aligned}
% \end{equation}
%From the perspective of implementation, becuase the points in $\mathbb{R}_*^{n\times p}/\mathcal{O}_p$ are very hard to be represented in memory, the function values at that points are not conveniently computed. A reasonable way is that for each point $[Y]\in \mathbb{R}_*^{n\times p}/\mathcal{O}_p$, we project $[Y]$ into the manifold $\mathbb{R}_*^{n\times p}$ by the natural projection $\pi$. The lifted function of $f$ is defined as $\bar{f}=f\circ \pi$, i.e., 
% \zwhcomm{Noting that the natural projection $\pi$ maps an equivalence class $\pi(Y)\in\mathbb{R}_*^{n\times p}/\mathcal{O}_p$ to its representative $Y\in \mathbb{R}_*^{n\times p}$ and $f$ maps $\pi(Y)$ to a real number, their composition $\bar{f}=f\circ \pi$, called the lift function of $f$, maps $Y\in\mathbb{R}_*^{n\times p}$ to a real number, i.e.,}{}
% \begin{equation}
%   \begin{aligned} \label{Pro_stat-LiftedFunc}
%     \bar{f}:\mathbb{R}_*^{n\times p} \rightarrow \mathbb{R}: Y\mapsto \mathrm{trace}(Y^TAYY^TMY) - \mathrm{trace}(Y^TCY).
%   \end{aligned}
% \end{equation}
% It is easy to obverse that the relation is close between $\bar{f}$ and $h$: 
% \[
%   \bar{f}(Y)=h(YY^T).
%   \] 
% Consequently, we obtain the final optimization problem: 

% \begin{equation}
%   \begin{aligned} \label{Pro_stat-FinalProb}    
%   \min_{\pi(Y)}f(\pi(Y))=\bar{f}(Y) \text{ s.t. } \pi(Y) \in \mathbb{R}_*^{n\times p}/\mathcal{O}_p.
%   \end{aligned}
% \end{equation}
% The optimization tools on Riemannian quotient manifolds can be used. Once we get the optimal solution $\pi(\tilde{Y})$ of problem (\ref{Pro_stat-FinalProb}), the low-rank approximate solution of equation (\ref{Intro-LyapMatEqu}) is $\tilde{X}=\tilde{Y}\tilde{Y}^T$, which is unique. Our model is essentially different with one in \cite{Burer2003}, where the model minimizes the cost function over $\mathbb{R}_*^{n\times p}$ but ours does over quotient manifold $\mathbb{R}_*^{n\times p}/\mathcal{O}_p$.  

% 这个标题不见得一定是problem statement。可以改动。这个章节一般是把问题的细节描述清楚，注意介绍章节介绍问题的时候是一般不讲细节的。在介绍完了数学工具和符号（preliminaries）后，可以通过数学语言把问题的结构形式完整的表达清楚。这个章节也可以介绍一些问题相关的内容，比如优化问题的话，可以介绍梯度，hessian的计算方式。如果是流形上的优化问题，可以介绍一下这个特点流形的几何结构，包括度量，收缩，向量传输等等后续会用到的对象。

\section{A Riemannian Truncated Newton's Method} \label{RieNew}
% 这个章节将算法描述清楚，给出伪代码。同时给出算法的收敛性分析，复杂度分析以及实现方面的细节。这部分根据情况会需要分解成一些子章节。比如如果是算法设计的论文，伪代码和代码解释一个子章节，收敛性分析一个子章节（如果收敛性分析很长，可以单独成为一个章节），复杂度分析和实现一个子章节。

Riemannian optimization has attracted more and more researchers' attention for recent years and many related algorithms have been investigated, such as the Riemannian trust-region methods \cite{Absil2007TrustRegionMO}, the Riemannian steepest descent method \cite{AbsMahSep2008}, the Riemannian Barzilai Borwein method \cite{Iannazzo2018TheRB}, the Riemannian nonlinear conjugate gradient methods \cite{ring_optimization_2012, Sato2014ADR, Sato2013ANG, Zhu2017ARC}, the Riemannian versions of quasi-Newton methods \cite{Huang2015ABC,Huang2015ARS,huang_riemannian_2018,Huang2022ALR}. %, and so forth. %\zwhcomm{}{} 
This paper considers the Riemannian optimization problems in the form of
\begin{equation} \label{RieNew-Prob}
  \begin{aligned} 
    \min_{x} f(x) \text{ s.t. }x\in \mathcal{M},
  \end{aligned}
\end{equation}
where $\mathcal{M}$ is a finite dimensional Riemannian manifold, and $f:\mathcal{M} \rightarrow \mathbb{R}$ is a real-valued function. It is further assumed throughout this paper that the below assumptions hold.

\begin{assumption} \label{TwiceContinuDiff}
  $f$ is twice continuously differentiable.
\end{assumption} 

\begin{assumption} \label{BoundedLevel}
  For all starting $x_0\in \mathcal{M}$ the level set 
  \[
    L(x_0) := \{x\in\mathcal{M} : f(x) \le f(x_0)\} 
    \]
    are bounded.
\end{assumption}

\begin{assumption} \label{RadLipCont}
	$f$ satisfies Definition~\ref{PrelNotat-RLipConti}. 
\end{assumption}

%\whcomm{[ZWTODO: We need the function $f$ satisfies Definition~\ref{PrelNotat-RLipConti}. In the later proofs or descriptions, we need to point out where we need this assumption. (i) the existence of step size of the Wolfe conditions and Armijo-Goldstein conditions (ii) the Wolfe conditions and Armijo-Goldstein conditions implies BN condition.]}{}


\subsection{Algorithm Statement} %The Truncated-Newton's Method} 

% In the Euclidean setting, Newton's method is often used because of its superlinear or even quadratic rate of convergence. In this paper, therefore, we use also Riemannian Newton's method to solve the problem (\ref{RieNew-Prob}). 

%On Riemannian manifolds, all algorithms we consider is retraction-based. In other words, they iterate $x_{k+1} \gets R_{x_{k}}(\eta_k)$ for some step $\eta_k$. For Riemannian Newton's method, the step is called the Newton step given by solving the Newton equation: 
%\begin{equation}
%  \begin{aligned} \label{RieNew-NewEuq}
%    \mathrm{Hess}f(x)[\eta] = -\mathrm{grad}f(x),
%  \end{aligned}
%\end{equation}
%where the solution $\eta$ is our desired Newton step. 

% Since Newton's method will, in practice, encounter some drawbacks, such as  
%   (i) it is not globally convergent;
%   (ii) it is not defined at points where $\mathrm{Hess}f(x_k)$ is singular;
%   (iii) for non-convex problem, it even does not generete a sequence of descent directions 
% and so forth, Dembo and Steihaug in \cite{dembo_truncated-newton_1983} embedded the standard Newton's method in a modified Newton framework to break through these shortcomings. Here we will generalize it to Riemannian setting, as seen in Algorithm \ref{RieNew-TruncatedNewton}. Different with the method in \cite{dembo_truncated-newton_1983}, where the Wolfe conditions are required for computing stepsize, the stepsize in the proposed Algorithm \ref{RieNew-TruncatedNewton} only needs to satisfy weaker condition either (\ref{RieNew-BrydCond1}) or (\ref{RieNew-BrydCond2}).

The proposed Riemannian Truncated-Newton's method is stated in Algorithm~\ref{RieNew-TruncatedNewton}.

\begin{algorithm}[htbp]
  \caption{Riemannian Truncated-Newton's method} 
  \begin{algorithmic}[1] \label{RieNew-TruncatedNewton}
  \REQUIRE initial iterate $x_0$; line search parameters $\chi_1$ and $\chi_2\in(0,1)$; truncated conjugate gradient parameters $\epsilon>0$ and a forcing sequence $\{\phi_k\}$ satisfying $\phi_k>0$ and $\lim_{k\rightarrow \infty}\phi_k=0$; %\zwhcomm{}{}
  \ENSURE sequence $\{x_k\}$; %\sout{solution $x^*$} \zwhcomm{}{};
  \STATE Set $k\gets0$;
  \WHILE{do not accurate enough}
  % \STATE Compute $f(x_k)$, $\mathrm{grad}f(x_k)$ and $\mathrm{Hess}f(x_k)$;
  \STATE \label{RieNew:st01} Approximately solve the Newton equation $\Hess f(x_k) [\eta_k] = - \grad f(x_k)$ by the truncated conjugate gradient algorithm in Algorithm~\ref{RieNew-TrunConjGrad} with inputs $(\grad f(x_k), \Hess f(x_k), \epsilon, \phi_k)$;
  % \sout{Compute $\eta_k\in\mathrm{T}_{x_k}\mathcal{M}$ such that} \zwhcomm{Perform Algorithm \ref{RieNew-TrunConjGrad-CheckForNegCurv} to find $\eta_k\in \mathrm{T}_x\mathcal{M}$ such that}{} 
  % \begin{equation} 
  %   \begin{aligned} 
  %     \label{RieNew-TrunNewEqu}
  %     \widetilde{\mathrm{Hess}}f(x_k)[\eta_k]=-\mathrm{grad}f(x_k), 
  %   \end{aligned}
  % \end{equation}
  % where $\widetilde{\mathrm{Hess}}f(x_k)$ is SPD and approximate to $\mathrm{Hess}f(x_k)$;
  \STATE \label{RieNew:st02} Find a step size $\alpha_k$ satisfying %\zwhcomm{ }{}
  \begin{equation}
  \begin{aligned}
    \label{RieNew-BrydCond1}
      h_k(\alpha_k)-h_k(0)\le -\chi_1 \frac{h'_k(0)^2}{\|\eta_k\|^2}, 
  \end{aligned}
  \end{equation}
  or 
  \begin{equation}
    \begin{aligned}
      \label{RieNew-BrydCond2}
      h_k(\alpha_k)- h_k(0) \le \chi_2 h'_k(0), 
    \end{aligned}
  \end{equation}
  where %\sout{$0<\chi_1<1$ and $0<\chi_2<1$ independent of $k$ are constants and} 
  $h_k(t)=f(R_{x_k}(t\eta_k))$;
  \STATE \label{RieNew:st03} Set $x_{k+1} \gets R_{x_k}(\alpha_k\eta_k)$;
  \STATE \label{RieNew:st04} Set $k\gets k+1$;
  \ENDWHILE
  
 \end{algorithmic}
\end{algorithm}

\begin{algorithm}[htbp]
  \caption{Truncated Conjugate Gradient Method (tCG)} 
  \begin{algorithmic}[1] \label{RieNew-TrunConjGrad}
  \REQUIRE $(\mathrm{grad}f(x_k), \mathrm{Hess}f(x_k), \epsilon, \phi_k)$, where $\epsilon>0$ and $\phi_k>0$;
  \ENSURE $\eta_k$;
  \STATE \label{RieNewtCG:st01} \textbf{Initializations:} \\ 
  Set $\eta^{(0)} \gets 0$, $r^{(0)}\gets -\mathrm{grad}f(x_k)$, $d^{(0)}\gets r^{(0)}$, $\delta^{(0)}\gets g_{x_k}(r^{(0)},r^{(0)})$, and $i\gets0$;
  \STATE \label{RieNewtCG:st02} \textbf{Check for negative curvature:} \label{RieNew-TrunConjGrad-CheckForNegCurv} \\ 
  Set $q^{(i)} \gets \mathrm{Hess}f(x_k)[d^{(i)}]$; \\
  If $g_{x_k}(d^{(i)}, q^{(i)}) \le \epsilon \delta_i$, return $\eta_k \gets \begin{cases} d^{(0)}, & \text{ if }i=0,\\ \eta^{(i)}, & \text{ otherwise}; \end{cases}$
  \STATE \label{RieNewtCG:st03} \textbf{Generate next inner iterate:}  \\ 
  Set $\alpha^{(i)} \gets g_{x_k}(r^{(i)}, r^{(i)})/g_{x_k}(d^{(i)},q^{(i)})$;\\ 
  Set $\eta^{(i+1)} \gets \eta^{(i)} + \alpha^{(i)}d^{(i)}$;
  \STATE \label{RieNewtCG:st04} \textbf{Update residual and search direction:} \\
  Set $r^{(i+1)} \gets r^{(i)} - \alpha^{(i)}q^{(i)}$; \\
  Set $\beta^{(i+1)}\gets g_{x_k}(r^{(i+1)},r^{(i+1)})/g_{x_k}(r^{(i)},r^{(i)})$;\\
  Set $d^{(i+1)} \gets r^{(i+1)} + \beta^{(i+1)}d^{(i)}$;\\ 
  Set $\delta^{(i+1)} \gets g_{x_k}(r^{(i+1)},r^{(i+1)}) + \left(\beta^{(i+1)}\right)^2d^{(i)}$; 
  \STATE \label{RieNewtCG:st05} \textbf{Check residual:} \\ 
  If $\|r^{(i+1)}\|/\|\mathrm{grad}f(x_k)\|\le \phi_k$, return $\eta_k\gets \eta^{(i+1)}$; \\
  Set $i\gets i+1$; \\
  Return to Step \ref{RieNew-TrunConjGrad-CheckForNegCurv};
 \end{algorithmic}
\end{algorithm}

Step~\ref{RieNew:st01} of Algorithm~\ref{RieNew-TruncatedNewton} invokes Algorithm~\ref{RieNew-TrunConjGrad}, which approximately solves the Newton equation $\Hess f(x_k) [\eta_k] = - \grad f(x_k)$ by the truncated conjugate gradient (tCG) method. 
If $\Hess f(x_k)$ is not sufficiently positive definite along the direction $d^{(i)}$ in the sense that $g_{x_k}(d^{(i)}, \Hess f(x_k) [d^{(i)}])$ is sufficiently greater than 0, then the conjugate gradient is terminated, see Step~\ref{RieNewtCG:st02} of Algorithm~\ref{RieNew-TrunConjGrad}. This early termination guarantees that the output $\eta_k$ is a sufficient descent direction, see Lemma~\ref{RieNew-DescentDir}. 
Moreover, the conjugate gradient method is terminated if the relative residual is sufficiently small in the sense that $\|r^{(i+1)}\| / \| \grad f(x_k)\|$ is smaller than the forcing term $\phi_k$, see Step~\ref{RieNewtCG:st05} of Algorithm~\ref{RieNew-TrunConjGrad}. This condition not only reduces the computational cost by not requiring solving the Newton equation exactly but also ensures the local superlinear convergence rate of Algorithm~\eqref{RieNew-TruncatedNewton}, see Theorem~\ref{RieNew-OrderOfConv}. 
Since the Newton equation is defined on the Euclidean space $\T_{x_k} \mathcal{M}$, Algorithm~\ref{RieNew-TrunConjGrad} is equivalent to the one in~\cite[Minor~Iteration]{dembo_truncated-newton_1983}. 
%Note that the sequence $\{\phi_k\}$ is called a forcing sequence, and the output is referred to as a truncated-Newton direction.

Step~\ref{RieNew:st02} is used to find a step size satisfying inequalities~\eqref{RieNew-BrydCond1} and~\eqref{RieNew-BrydCond2}, which is a Riemannian generalization of the line search conditions in~\cite{Byrd1989ATF}. It has been pointed out in~\cite{huang_riemannian_2018} that if the function $f$ is radially $L$-$C^1$ function (see Definition \ref{PrelNotat-RLipConti}), then many commonly-used line search conditions including the Wolfe conditions and Armijo-Glodstein conditions imply one or both of~\eqref{RieNew-BrydCond1} and~\eqref{RieNew-BrydCond2}. Therefore, Algorithm~\ref{RieNew-TruncatedNewton} uses a weaker line search condition than the one in~\cite{dembo_truncated-newton_1983}.

% In Step 4 of Algorithm \ref{RieNew-TruncatedNewton}, the solution $\eta_k$ satisfying (\ref{RieNew-TrunNewEqu}) is given by \sout{the} Algorithm \ref{RieNew-TrunConjGrad}, called truncated conjugate gradient method; see \cite{dembo_truncated-newton_1983}. Conditions (\ref{RieNew-BrydCond1}) and (\ref{RieNew-BrydCond2}) in \sout{step} Step 4 of Algorithm \ref{RieNew-TruncatedNewton} will make the objective function "sufficient descent". Many conditions, e.g., Armijo-Goldstein conditions, Wolfe conditions and so on, imply one or both of (\ref{RieNew-BrydCond1}) and (\ref{RieNew-BrydCond2}), when $\mathcal{M}$ is a Euclidean space and the gradient of the objective function is Lipschitz continuous \cite{richard_h_byrd_tool_1989}, \zwhcomm{which means that the line search condition (\ref{RieNew-BrydCond1}) and (\ref{RieNew-BrydCond2}) are  weaker than conditions mentioned above.}{} Considering that function $f\circ R_x : \mathrm{T}_{x}\mathcal{M} \rightarrow \mathbb{R}$ is defined on a linear space $\mathrm{T}_x\mathcal{M}$, when $\mathcal{M}$ is a manifold, the previous results also hold if \sout{the gradient of the function is Riemannian Lipschitz continuous} \zwhcomm{the function is radially $L$-$C^1$ function}{}(see Definition \ref{PrelNotat-RLipConti})\cite{huang_riemannian_2018}.


% For Equation (\ref{RieNew-TrunNewEqu}), we use truncated conjugate gradient (tCG) method to find $\eta_k$, as seen in Algorithm \ref{RieNew-TrunConjGrad}, where the subscript represents the outer iteration and the superscript means the inner iteration.



\subsection{Convergence Analysis}

In this section, we establish the convergence results by merging the theoretical techniques in~\cite{dembo_truncated-newton_1983} and~\cite{huang_riemannian_2018}.

%Define $\varphi:\mathrm{T}_x\mathcal{M} \rightarrow \mathbb{R}$ as
%\begin{equation}
% \begin{aligned}
%   \label{App-QuadFunc}
% \end{aligned}
%\end{equation}

%\zwhcomm{It is clear that the Newton direction at $x$ is a stationary point of $\phi$, and note that by a conjugate direction method, $\phi$ is minimized over the subspace spanned by the directions that are generated by it as seen in Theorem \ref{App-Theorem1}, it is naturally suited to computing a truncated-Newton direction. }{}

Theorem~\ref{App-Theorem1} generalizes the properties of the conjugate gradient method in~\cite{Hestenes1952MethodsOC} to a generic Euclidean space. Such generalizations are trivial and have been used in~\cite{AbsMahSep2008}. These results are given here for completeness and will be used in Lemma~\ref{RieNew-DescentDir}.

\begin{theorem} \label{App-Theorem1}
%\whcomm{[ZWTODO: Not to use this simplified notation: notation $f_x$, $g_x$, $\mathcal{H}_x$, and $\left<\cdot,\cdot\right>_x$ to denote $f(x)$, $\mathrm{grad}f(x)$, $\mathrm{Hess}f(x)$ and $g_x(\cdot,\cdot)$ respectively for simplicity. ]}{}

%  If $\left<\mathcal{H}_x[d_i],d_i\right>_x>\epsilon \delta_i$, $i=0,1,2,\cdots,k$, then 

  If $g_x(\mathrm{Hess}f(x)[d_i], d_i)>\epsilon \delta_i$, $i=0,1,\dots,k$, then
\begin{align}
      & g_x( \mathrm{Hess}f(x)[d_i],d_j ) = 0,\quad\quad\quad\quad\;\; i\not=j,\;i,j=0,1,\cdots,k, \label{App-Theorem1-1} \\ 
      & g_x( d_i, r_j )=0,\quad\quad\quad\quad\quad\quad\quad\quad\quad i<j,\;i,j=0,1,\cdots,k+1, \label{App-Theorem1-2} \\ 
      & g_x( r_i, d_j ) = g_x(r_j, r_j) = g_x(r_0, d_j),\; i\le j,\; i,j=0,1,\cdots,k, \label{App-Theorem1-3} \\ 
      & \mathrm{span}\{ d_0,d_1,\cdots,d_k \} = \mathrm{span}\left\{g_x,\mathrm{Hess}f(x)[g_x],\cdots,(\mathrm{Hess}f(x))^{k-1}[g_x]\right\}, \label{App-Theorem1-4}\\ 
      & \varphi(\eta_{k+1}) = \min \left\{ \varphi(\eta): \eta\in \mathrm{span}\{d_0,d_1,\cdots,d_k\} \right\}, \label{App-Theorem1-5} \\ 
      & \delta_i = g_x(d_i,d_i),\;i=0,1,\cdots,k, \label{App-Theorem1-6}
\end{align}
where $\varphi(\eta_x) = f(x) + g_x(\grad f(x),\eta_x) + \frac{1}{2} g_x(\mathrm{Hess}f(x)[\eta_x],\eta_x),\;\eta_x\in \mathrm{T}_x\mathcal{M}$.
\end{theorem}
%\begin{proof}
%  See \cite{Hestenes1952MethodsOC}, where the space under consideration is the standard Euclidean space $\mathbb{R}^{n}$. It is easy to verify that these results hold on general Euclidean spaces, e.g., $(\mathrm{T}_x\mathcal{M},\left<\cdot,\cdot\right>_x)$.
%\end{proof}

Lemma~\ref{RieNew-DescentDir} proves that the search direction $\eta_k$ from Algorithm~\ref{RieNew-TrunConjGrad} is a descent direction and its norm is not small compared to the norm of $\grad f(x_k)$. It is a Riemannian generalization of~\cite[Lemma~A.2]{dembo_truncated-newton_1983}.
Since $\eta_k$ is a descent direction, there exists a step size satisfying Inequality~\eqref{RieNew-BrydCond1} or~\eqref{RieNew-BrydCond2} as shown in~\cite{NW06, huang_riemannian_2018}. Thus, Algorithm~\ref{RieNew-TruncatedNewton} is well-defined.

%In what follows, we firstly point out that the truncated-Newton direction $\eta_k$ generated by Algorithm \ref{RieNew-TrunConjGrad} is descent as stated in Lemma \ref{RieNew-DescentDir}.
\begin{lemma} \label{RieNew-DescentDir}
There exist two positive constants $\gamma_1$ and $\gamma_2$ that only rest with $\|\mathrm{Hess}f(x_k)\|$ and $\epsilon$ respectively so that 
\begin{equation}
  g_{x_k}(\mathrm{grad}f(x_k),\eta_k) \le -\gamma_1 \|\mathrm{grad}f(x_k)\|^2 \label{RieNew-Lem-1}
\end{equation}
and
\begin{equation}
  \begin{aligned}
     \|\eta_k\|\le \gamma_2 \|\mathrm{grad}f(x_k)\|. \label{RieNew-Lem-2}
  \end{aligned}
\end{equation}  

\begin{proof} %This is similar to the proof of \cite[Theorem A.3]{dembo_truncated-newton_1983}.
  Out of convenience, use $g_k$, $\mathcal{H}_k$ and $\left<\cdot,\cdot\right>_k$ to denote $\mathrm{grad}f(x_k)$, $\mathrm{Hess}f(x_k)$ and $g_{x_k}(\cdot,\cdot)$. Suppose that $\eta_k=\eta^{(i)}$, $i\ge0$. From  Algorithm~\ref{RieNew-TrunConjGrad}, we have 
\begin{align}
	\eta_k = \eta^{(i)} = \sum_{j=0}^{i-1}\alpha^{(j)}d^{(j)}. \label{RieNew-Lem-3}
\end{align}
From Step~\ref{RieNewtCG:st03} in Algorithm~\ref{RieNew-TrunConjGrad} and (\ref{App-Theorem1-3}) we have that 
\begin{align}
  \alpha^{(j)} = \frac{ \left< d^{(j)}, r^{(0)} \right>_k }{ \left< d^{(j)},q^{(j)} \right>_k }. \label{RieNew-Lem-4}
\end{align} 
Hence using \eqref{RieNew-Lem-3} and \eqref{RieNew-Lem-4} we have 

\begin{equation}
  \begin{aligned} \nonumber
  \left<\eta_k,g_k\right>_k &= \left<\eta^{(i)},-r^{(0)}\right>_k=-\left< \sum_{j=0}^{i-1} \frac{ \left<d^{(j)},r^{(0)}\right>_k d^{(j)}}{\left<d^{(j)},q^{(j)}\right>_k},r^{(0)} \right>_k = -\sum_{j=0}^{i-1}\frac{\left<d^{(j)},r^{(0)}\right>_k^2}{\left<d^{(j)},q^{(j)}\right>_k} \\ & \le - \frac{\left<d^{(0)},r^{(0)}\right>_k^2}{\left<d^{(0)},q^{(0)}\right>_k}=- \frac{\left<d^{(0)},d^{(0)}\right>_k}{\left<d^{(0)},\mathcal{H}_k[d^{(0)}]\right>_k}\|g_k\|^2
  \end{aligned}
\end{equation}
with $d^{(0)}=r^{(0)}=-g_k$. By the Cauchy-Swartz inequality, we have 
\[
  \frac{ \left< d^{(0)},d^{(0)} \right>_k }{\left<d^{(0)},\mathcal{H}_x[d^{(0)}] \right>_k}\ge \frac{1}{{\|\mathcal{H}_k}\|} \ge\frac{1}{\gamma_1}, 
\] 
where $\gamma_1>0$ is a constant and the existence of $\gamma_1$ is guaranteed by the uniformly boundedness of $\mathcal{H}_k$ from Assumptions~\ref{TwiceContinuDiff} and~\ref{BoundedLevel}. Therefore, inequality~\eqref{RieNew-Lem-1} holds.
%So picking \sout{$\gamma_1 = \min \left\{ 1, {\|\mathcal{H}_k\|}^{-1} \right\}$} \zwhcomm{$\gamma_1 = \min\left\{1,\sigma^{-1}\right\}$}{}, we obtain the result (\ref{RieNew-Lem-1}). 
Note from (\ref{RieNew-Lem-3}) and (\ref{RieNew-Lem-4}) that 
\[
  \eta^{(i)} = \sum_{j=0}^{i-1}\frac{\left<d^{(j)},r^{(0)}\right>_k}{\left<d^{(j)},q^{(j)}\right>_k}d^{(j)}.
  \]
Hence we have 
\begin{equation}
  \begin{aligned} \nonumber
    \|\eta_k\| = \|\eta^{(i)}\| &= \left\|  \sum_{j=0}^{i-1}\frac{\left<d^{(j)},r^{(0)}\right>_k}{\left<d^{(j)},q^{(j)}\right>_k}d^{(j)} \right\| \le \sum_{j=0}^{i-1} \frac{|\left< d^{(j)},r^{(0)} \right>_k|}{\left<d^{(j)},q^{(j)}\right>_k} \|d^{(j)}\| \\ 
    & \le \sum_{j=0}^{i-1} \frac{ \|d^{(j)}\|\|r^{(0)}\|\|d^{(j)}\| }{\left<d^{(j)},q^{(j)}\right>_k} = \sum_{j=0}^{i-1}\frac{\|d^{(j)}\|^2}{\left<d^{(j)},q^{(j)}\right>_k}\|r^{(0)}\| \\ 
    &= \sum_{j=0}^{i-1} \frac{\left<d^{(j)},d^{(j)}\right>_k}{\left<d^{(j)},q^{(j)}\right>_k}\|r^{(0)}\| \le i \frac{1}{\epsilon} \|r^{(0)}\| \\ 
    &= i \frac{1}{\epsilon} \|g_k\|.
  \end{aligned}
\end{equation}
So for $\gamma_2=\max\{n \epsilon^{-1},1\}$, we have the desired result (\ref{RieNew-Lem-2}), where $n=\dim(\mathrm{T}_{x_k}\mathcal{M}$). 
\end{proof}
\end{lemma}

%In Algorithm \ref{RieNew-TruncatedNewton}, we compute descent direction $\eta_k$ by tCG to solve the truncated-Newton equation (\ref{RieNew-TrunNewEqu}). Theorem \ref{RieNew-GlobalConver} tell us that the Algorithm \ref{RieNew-TruncatedNewton} has global convergence. 

Now, we are ready to give the global convergence result of Algorithm~\ref{RieNew-TruncatedNewton} in Theorem~\ref{RieNew-GlobalConver}. The proofs of Theorem~\ref{RieNew-GlobalConver} rely on the techniques in~\cite{huang_riemannian_2018}, not the ones in~\cite{dembo_truncated-newton_1983}.
\begin{theorem} \label{RieNew-GlobalConver}
  Let $\{x_n\}$ denote the sequence generated by Algorithm \ref{RieNew-TruncatedNewton}. Then it holds that
  \[
    \lim_{n \rightarrow \infty}\|\mathrm{grad}f(x_n)\|=0.
    \]
 If $x^*$ is accumulation point of the sequence $\{x_k\}$ and $\mathrm{Hess}f(x^*)$ is positive definite, then $x_k \rightarrow x^*$.
\end{theorem}
  \begin{proof}
    Let $\{x_n\}$, $\{\eta_n\}$ be generated by Algorithm \ref{RieNew-TruncatedNewton} and Algorithm \ref{RieNew-TrunConjGrad} respectively and step sizes $\{\alpha_n\}$ satisfy either (\ref{RieNew-BrydCond1}) or (\ref{RieNew-BrydCond2}). Noting that (\ref{RieNew-Lem-1}) and (\ref{RieNew-Lem-2}), we have 
    \begin{align*}
      \infty & > f(x_0) - f(x_{n})=\sum\limits_{k=0}^{n-1}(f(x_{k})-f(x_{k+1}))=\sum\limits_{k=0}^{n}(h_k(0)-h_{k}(\alpha_{k})) \\ & \ge \sum\limits_{k=0}^{n-1}\min \left(\chi_{1}\frac{h_{k}'(0)^{2}}{\|\eta_{k}\|^{2}},-\chi_{2}h_{k}'(0) \right) \\ &=\sum\limits_{k=0}^{n-1}\min \left( \chi_{1}\frac{g(\mathrm{grad}f(x_{k}),\eta_{k})^{2}}{\|\eta_{k}^{2}\|}, -\chi_{2}g(\mathrm{grad}f(x_{k}),\eta_{k}) \right) \\ & \ge \sum\limits_{k=0}^{n-1}\min \left( \chi_{1}\gamma_{1}\frac{\|\mathrm{grad}f(x_{k})\|^{2}}{\|\eta_{k}\|^{2}}(-g(\mathrm{grad}f(x_{k}),\eta_{k})), -\chi_{2}g(\mathrm{grad}f(x_{k}),\eta_{k}) \right) \\ & \ge \sum\limits_{k=0}^{n-1}\min \left(\chi_{1}{\gamma_{1}}{\gamma_{2}^{-2}},\chi_{2} \right)(-g(\mathrm{grad}f(x_{k}),\eta_{k})),
    \end{align*}
    which implies 
    \begin{align}
      \lim_{n \rightarrow \infty}(-g(\mathrm{grad}f(x_n),\eta_n))=0. \label{RieNew-Theorem-Conv}
    \end{align}
    Combining (\ref{RieNew-Lem-1}) and (\ref{RieNew-Theorem-Conv}) we have 
    \[
      0\le \gamma_1 \|\mathrm{grad}f(x_n)\|^2\le  (-g(\mathrm{grad}f(x_n),\eta_n)) \rightarrow 0 \;(n \rightarrow \infty).
      \]
          
  For the second part, noting that $\|\mathrm{grad}f(x_k)\|\rightarrow 0$ and continuity of $\mathrm{grad}f$, any accumulation point $\tilde{x}$ of the sequence $\{x_k\}$ is a stationary point of $f$. Under Assumption \ref{BoundedLevel}, each subsequence of $\{x_k\}$ converges to a stationary point. Therefore, $\{x_k\}$ converges to $x^*$ since $\mathrm{Hess}f(x^*)$ is positive definite.
  \end{proof}

%
%In Euclidean setting, the truncated-Newton method \cite{dembo_truncated-newton_1983} has locally superlinear convergence rate. Likewise, Theorem \ref{RieNew-OrderOfConv} states clearly that the local property holds for Algorithm \ref{RieNew-TruncatedNewton}. 
%\begin{theorem} 
%  \label{RieNew-LocalConv} 
%  Let $\{x_k\}$ be the sequence generated by \sout{the} Algorithm \ref{RieNew-TruncatedNewton} with forcing sequence
%  $$ 
%  \phi_k = \min\left\{\frac{1}{k},\|\mathrm{grad}f(x_k)\|^t\right\}, 0 < t \le 1 .
%  $$ 
%  Suppose that $\{x_k\}$ converges to $x^*$ at which $\mathrm{Hess}f(x^*)$ is positive definite and $\mathrm{Hess}f(x)$ is continuous in a neighborhood of $x^*$. Then 
%  \begin{enumerate}
%    \item the stepsize $\alpha_k=1$ is \sout{acceptive} \zwhcomm{acceptable}{} for sufficiently large $k$; and 
%    \item the convergence rate is superlinear\footnote{A sequence $\{x_k\}\subset \mathcal{M}$ converging to $x^*\in\mathcal{M}$ is superlinear if $ \frac{\|R_{x^*}^{-1}(x_{k+1})\|}{\|R_{x^*}^{-1}(x_{k})\|}\rightarrow 0(k \rightarrow \infty). $ }.
%  \end{enumerate}
%Moreover, if $\mathrm{Hess}f(x)$ is Riemannian Lipschitz continuous (see Definition \ref{PrelNotat-VecFieldLipConti}) for $x$ near $x^*$, the convergence rate is $1+\min(1,t)$\footnote{A sequence $\{x_k\}\subset \mathcal{M}$ converging to $x^*\in\mathcal{M}$ has Q-order of $1+\min(1,t)$ if $\limsup_{k \rightarrow \infty} \frac{\|R_{x^*}^{-1}(x_{k+1})\|}{\|R_{x^*}^{-1}(x_{k})\|^{1+\min(1,t)}}<\infty. $}.
%
%  \begin{proof} 	
%    The proof is divided into three parts.
%
%    \textbf{Step 1: We will prove that for sufficient large $k$, stepsize $\alpha_k=1$ is \sout{acceptive} \zwhcomm{acceptable}{}.} To that end, we can show that $\alpha_k=1$ satisfies Armijo condition and thus (\ref{RieNew-BrydCond1}) or (\ref{RieNew-BrydCond2}) holds. By the Truncated Conjugate gradient Algorithm \ref{RieNew-TrunConjGrad}, we have 
%    \begin{align}
%      \|r_k\|=\|\mathrm{Hess}f(x_k)[\eta_k]+\mathrm{grad}f(x_k)\|\le \|\mathrm{grad}f(x_k)\|\phi_k\le\|\mathrm{grad}f(x_k)\|^{1+t}. \label{RieNew-LocalConv1}
%    \end{align}
%    By the trigonometric inequality of norm and (\ref{RieNew-LocalConv1}), we have  
%    \begin{align*}
%      \|\mathrm{grad}f(x_k)\|-\|\mathrm{Hess}f(x_k)[\eta_k]\|\le\|\mathrm{grad}f(x_k)\|^{1+t}.
%    \end{align*}
%    It continues 
%    \[
%      (1-\|\mathrm{grad}f(x_k)\|^t)\|\mathrm{grad}f(x_k)\|\le\|\mathrm{Hess}f(x_k)[\eta_k]\|.
%      \]
%    By assumptions, there exist $L,\tilde{L}>0$ such that $\|\mathrm{Hess}f(x_k)\|\le L$ and $\|\mathrm{Hess}f(x_k)^{-1}\|\le \tilde{L}$ for sufficiently large $k$ since $\mathrm{Hess}f(x^*)$ is positive \zwhcomm{definite}{}. Then we have 
%    \[
%      (1-\|\mathrm{grad}f(x_k)\|^t)\|\mathrm{grad}f(x_k)\|\le L\|\eta_k\|.
%      \]
%    By Theorem \ref{RieNew-GlobalConver}, for sufficiently large $k$, we have 
%    \begin{align*}
%      \frac{1}{2}\|\mathrm{grad}f(x_k)\|\le L\|\eta_k\|,
%    \end{align*}
%    that is, 
%    \begin{align}
%      \|\mathrm{grad}f(x_k)\|\le 2L\|\eta_k\|. 
%      \label{RieNew-LocalConv2}
%    \end{align}
%    Besides, we have 
%    \begin{align}
%      \|\eta_k\|=\|\mathrm{Hess}f(x_k)^{-1}[r_k-\mathrm{grad}f(x_k)]\|\le\tilde{L}(\|r_k\|+\|\mathrm{grad}f(x_k)\|)\le 2\tilde{L}\|\mathrm{grad}f(x_k)\|,  \label{RieNew-LocalConv3}
%    \end{align}
%    and 
%    \begin{align*}
%      &\mathrm{grad}f(x_k)=r_k-\mathrm{Hess}f(x_k)[\eta_k],\\ 
%      -& g(\mathrm{grad}f(x_k),\eta_k)=-g(r_k,\eta_k) + g(\mathrm{Hess}f(x_k)[\eta_k],\eta_k) \ge -g(r_k,\eta_k)+m\|\eta_k\|^2, \\ 
%      & |g(\mathrm{grad}f(x_k),\eta_k)|\le\|\eta_k\|^{2+t},
%    \end{align*}
%    where $m$ is chosen such that $\lambda(\mathrm{Hess}f(x_k))_{\min}\ge m$ \zwhcomm{for sufficiently large $k$}{}. By the penultimate inequality above, (\ref{RieNew-LocalConv1}) and (\ref{RieNew-LocalConv2}), we have 
%    \begin{align}
%      -g(\mathrm{grad}f(x_k),\eta_k)\ge -c\|\eta_k\|^{2+t}+m\|\eta_k\|^2=(m-c\|\eta_k\|^t)\|\eta_k\|^2 \ge \frac{m}{2}\|\eta_k\|^2 , \label{RieNew-LocalConv4}
%    \end{align}
%    for sufficiently large $k$ and $c=(2L)^{1+t}$. By Taylor expension for $f$ at $x_k$ (see \cite[Proposition 8.65]{Boumal:300281}) and (\ref{RieNew-LocalConv4}), we have 
%    \begin{align*}
%      f({R}_x(\eta_k)) - f(x_k) & = g(\mathrm{grad}f(x_k),\eta_k) + \frac{1}{2}g(\mathrm{Hess}f(x_k)[\eta_k],\eta_k) + o(\|\eta_k\|^2) \\ 
%      & = \frac{1}{2} g(\mathrm{grad}f(x_k),\eta_k) + \frac{1}{2}g(\mathrm{Hess}f(x_k)[\eta_k]+\mathrm{grad}f(x_k),\eta_k) + o(\|\eta_k\|^2) \\ 
%      & \le \frac{1}{2} g(\mathrm{grad}f(x_k),\eta_k) + \frac{1}{2}\|r_k\|\|\eta_k\| + o(\|\eta_k\|^2) \\ 
%      & \le \frac{1}{2}g(\mathrm{grad}f(x_k),\eta_k) + \frac{1}{2}\|\eta_k\|^{2+t} + o(\|\eta_k\|^2) \\ 
%      & \le \frac{1}{2}g(\mathrm{grad}f(x_k),\eta_k) - \frac{1}{m} \|\eta_k\|^t g(\mathrm{grad}f(x_k),\eta_k) + \frac{2}{m}o(1)g(\mathrm{grad}f(x_k),\eta_k)\\
%      & \le \left(\frac{1}{2} - \frac{1}{m}\|\eta_k\|^t-\frac{2}{m}o(1)\right)g(\mathrm{grad}f(x_k),\eta_k)\\ 
%      & \le c_1 g(\mathrm{grad}f(x_k),\eta_k),\;c_1<\frac{1}{2},\;\text{for sufficiently large }k,
%    \end{align*}
%which implies that the Armijo-Goldstein conditions hold with $\alpha_k=1$ for sufficiently large $k$. 
%
%    \textbf{Step 2: We will prove that $x_k$ superlinearly converges to $x^*$.} By Taylor expension for $\mathrm{grad}f$ at $x_k$ (see \cite[Lemma 7.4.7]{AbsMahSep2008}), we have 
%    \begin{equation}        
%   \begin{aligned} \label{RieNew-LocalConv5}
%      \mathcal{T}_{\eta_k}^{-1}\mathrm{grad}f(R_{x_k}(\eta_k)) &=\mathrm{grad}f(x_k) + \mathrm{Hess}f(x_k)[\eta_k] + o(\|\eta_k\|) \\ 
%      &= r_k + o(\|\mathrm{grad}f(x_k)\|).
%  \end{aligned}
%\end{equation}
%  Hence, it continues 
%  \begin{align}
%    \|\mathrm{grad}f(R_{x_k}(\eta_k))\|\le\left(\|\mathrm{grad}f(x_k)\|^t+o(1)\right)\|\mathrm{grad}f(x_k)\|. \label{RieNew-LocalConv6}
%  \end{align}
%  Noting that $\mathrm{grad}f(x^*)=0$ and (\ref{RieNew-LocalConv5}), we have 
%  \begin{align*}
%    \mathcal{T}^{-1}_{\xi_{x^*}}\mathrm{grad}f(x_k) = \mathrm{Hess}f(x^*)[\xi_{x^*}] + o(\|\xi_{x^*}\|)
%  \end{align*}
%  with $\xi_{x^*}=R^{-1}_{x^*}(x_k) \in \mathrm{T}_{x^*}\mathcal{M}$ for sufficiently large $k$. Thus we can obtain 
%  \begin{align}
%    L_1 \|\xi_{x^*}\|\le\|\mathrm{grad}f(x_k)\|\le L_2 \|\xi_{x^*}\|, \label{RieNew-LocalConv7}
%  \end{align}
%  where $L_1,L_2$ are two positive constants.
%  From (\ref{RieNew-LocalConv6}) and (\ref{RieNew-LocalConv7}), we have 
%  \[
%    \frac{\|R^{-1}_{x^*}(x_{k+1})\|}{\|R^{-1}_{x^*}(x_k)\|} \le \frac{L_2}{L_1}\frac{\|\mathrm{grad}f(x_{k+1})\|}{\|\mathrm{grad}f(x_k)\|}\le\frac{L_2}{L_1}(\|\mathrm{grad}f(x_k)\|^t+o(1)) \rightarrow 0,
%    \]
%    which means $\{x_k\}$ superlinearly converges to $x^*$.
%
%    \textbf{Step 3: If $\mathrm{Hess}f(x)$ is Riemannian Lipschitz continuous, then the convergence rate is $\mathbf{1+\min(t,1)}$.} Since $\mathrm{Hess}f$ is Lipschitz continuous, we have (see \cite[Proposition 10.53]{Boumal:300281})
%    \[
%      \|\mathcal{T}^{-1}_{\eta_k}\mathrm{grad}f(x_{k+1}) - \mathrm{grad}f(x_k) - \mathrm{Hess}f(x_k)[\eta_k]\| \le {L_3}\| \eta_k \|^2,
%      \]
%      for constant $L_3>0$, that is, 
%      \[
%        \|\mathrm{grad}f(x_{k+1})\|\le \|r_k\| + L_3\|\eta_k\|^2 \le (\|\mathrm{grad}f(x_k)\|^{t} + 4\widetilde{L}L_3\|\mathrm{grad}f(x_k)\|)\|\mathrm{grad}f(x_k)\|.
%      \]
%      Hence, we have 
%      \begin{align*}
%        \frac{\|R_{x^*}^{-1}(x_{k+1})\|}{\|R^{-1}_{x^*}(x_k)\|^{1+\min(1,t)}} &\le \frac{L_2}{L_1^{1+\min(1,t)}}\frac{\|\mathrm{grad}f(x_{k+1})\|}{\|\mathrm{grad}f(x_k)\|^{1+\min(1,t)}}\\ 
%        & \le \frac{L_2}{L_1^{1+\min(1,t)}}\left( \|\mathrm{grad}f(x_k)\|^{t-\min(1,t)} + 4\widetilde{L}L_3\|\mathrm{grad}f(x_k)\|^{1-\min(1,t)} \right) < C.
%      \end{align*}
%      
%  \end{proof}
%
%\end{theorem}
%
%\sout{It should be noted that in the proof of Theorem \ref{RieNew-LocalConv} the second order retraction $R$ are used in Step 1. In Step 2 and Step 3, the vector transport $\mathcal{T}$ is required to be parallel transport and the retraction is further required to be defined by exponential map $\mathrm{Exp}:\mathrm{T}\mathcal{M}\rightarrow \mathcal{M}$.}

%In Euclidean setting, the truncated-Newton method \cite{dembo_truncated-newton_1983} has locally superlinear convergence rate. Likewise, Theorem \ref{RieNew-OrderOfConv} states clearly that the local property holds for Algorithm \ref{RieNew-TruncatedNewton}. 

The local convergence rate of Algorithm~\ref{RieNew-TruncatedNewton} is established in Theorem~\ref{RieNew-OrderOfConv}. The convergence rates are generalizations of the results in~\cite{dembo_truncated-newton_1983}. However, the proofs are simply generalizations of~\cite{dembo_truncated-newton_1983}. That the step size one eventually satisfies the line search condition is guaranteed by the Riemannian Dennis-Mor{\'e} condition in~\cite{ring_optimization_2012}. The analysis of the order of convergence requires the relationship between multiple definitions of Riemannian gradients and Riemannian Hessians, i.e., $\grad f(R_x(\eta_x))$ versus $\grad (f \circ R_x) (\eta_x)$ and $\Hess f(x_k)$ versus $\Hess (f \circ R_x)(0_x)$.
\begin{theorem}
	\label{RieNew-OrderOfConv} 
  Let $\{x_k\}$ be the sequence generated by Algorithm \ref{RieNew-TruncatedNewton} with forcing sequence 
  $ 
  \phi_k = \min\left\{1/k,\|\mathrm{grad}f(x_k)\|^t\right\}, 0 < t \le 1 .
  $
  Suppose that $\{x_k\}$ converges to $x^*$ at which $\mathrm{Hess}f(x^*)$ is positive definite and $\mathrm{Hess}f(x)$ is continuous in a neighborhood of $x^*$. Then 
  \begin{enumerate}
    \item[1.]  the stepsize $\alpha_k=1$ is acceptable for sufficiently large $k$; and 
    \item[2.] the convergence rate is superlinear\footnote{A sequence $\{x_k\}\subset \mathcal{M}$ converging to $x^*\in\mathcal{M}$ is superlinear if $ \frac{\mathrm{dist}(x^*,x_{k+1})}{\mathrm{dist}(x^*,x_k)} \rightarrow 0(k \rightarrow \infty). $ }.
  \end{enumerate}
Moreover, suppose that $\mathrm{Hess}\hat{f}$ satisfies that $\|\mathrm{Hess}f(x_k)- \mathrm{Hess}\hat{f}_{x_k}(0_{x_k})\|\le \beta_1 \|\mathrm{grad}f(x_k)\|$, $\hat{f}$ as defined in Definition \ref{PrelNotat-Pullback}, with a positive constant $\beta_1$, and that $\mathrm{Hess}\hat{f}_x$ is Lipschitz-continous at $0_x$ uniformly in $x$ in a neighborhood of $x^*$, i.e., there exist $\beta_2>0,\mu_1 > 0$ and $\mu_2>0$ such that for all $x\in B_{\mu_{1}}(x^*)$ and all $\eta_x\in B_{\mu_2}(0_x)$, it holds that $\|\mathrm{Hess}\hat{f}_x(\eta_x)-\mathrm{Hess}\hat{f}_x(0_x)\|\le \beta_2\|\eta_x\|$. Then, 
\begin{enumerate}
	\item[3.] the convergence rate is $1+\min(1,t)$\footnote{A sequence $\{x_k\}\subset \mathcal{M}$ converging to $x^*\in\mathcal{M}$ has Q-order of $1+\min(1,t)$ if $\limsup_{k \rightarrow \infty} \frac{\mathrm{dist}(x^*,x_{k+1})}{\mathrm{dist}(x^*,x_k)^{1+\min(1,t)}} <\infty. $}.
\end{enumerate}
\end{theorem}

It is worth mentioning that in Theorem \ref{RieNew-OrderOfConv} the assumption ``$\|\mathrm{Hess}f(x_k) - \mathrm{Hess}\hat{f}_{x_k}(0_{x_k})\|\le \beta_1\|\mathrm{grad}f(x_k)\|$''~\cite[Theorem~7.4.10]{AbsMahSep2008} is reasonable since that for sufficiently large $k$, $x_k$ is sufficiently close to $x^*$ and the right-hand side can take zero at the stationary point $x^*$. On the other hand, if a second-order retraction is used, this assumption naturally holds since the right-hand side of the inequality can take up to zero~\cite[Proposition~5.5.5]{AbsMahSep2008}. The another assumption ``$\mathrm{Hess}\hat{f}_x$ is Lipschitz-continous at $0_x$ uniformly in $x$ in a neighborhood of $x^*$''~\cite[Theorem~7.4.10]{AbsMahSep2008} is the counterpart in the classical Newton-tCG method~\cite{DES82}.

\begin{proof}
%	We first prove~\ref{pt01} and~\ref{pt02}. 
	By Algorithm \ref{RieNew-TrunConjGrad}, we have 
    \begin{align}
      \|r_k\|=\|\mathrm{Hess}f(x_k)[\eta_k]+\mathrm{grad}f(x_k)\|\le \|\mathrm{grad}f(x_k)\|\phi_k\le\|\mathrm{grad}f(x_k)\|^{1+t}. \label{RieNew-OrderOfConv-1}
    \end{align}
    By the trigonometric inequality of norm and (\ref{RieNew-OrderOfConv-1}), we have  
    \begin{align*}
      \|\mathrm{grad}f(x_k)\|-\|\mathrm{Hess}f(x_k)[\eta_k]\|\le\|\mathrm{grad}f(x_k)\|^{1+t}.
    \end{align*}
    It continues 
    \[
      (1-\|\mathrm{grad}f(x_k)\|^t)\|\mathrm{grad}f(x_k)\|\le\|\mathrm{Hess}f(x_k)[\eta_k]\|.
      \]
    By Assumption~\ref{TwiceContinuDiff}, there exist $L > 0,\tilde{L}>0$ such that $\|\mathrm{Hess}f(x_k)\|\le L$ and $\|\mathrm{Hess}f(x_k)^{-1}\|\le \tilde{L}$ for sufficiently large $k$ since $\mathrm{Hess}f(x^*)$ is positive definite. it follows that
    \[
      (1-\|\mathrm{grad}f(x_k)\|^t)\|\mathrm{grad}f(x_k)\|\le L\|\eta_k\|.
      \]
    By Theorem \ref{RieNew-GlobalConver}, for sufficiently large $k$, we have 
    $
      \frac{1}{2}\|\mathrm{grad}f(x_k)\|\le L\|\eta_k\|,
	$
    that is, 
    \begin{align}
      \|\mathrm{grad}f(x_k)\|\le 2L\|\eta_k\|. \label{RieNew-OrderOfConv-2}
    \end{align}
    Besides, we have 
    \begin{align}
      \|\eta_k\|=\|\mathrm{Hess}f(x_k)^{-1}[r_k-\mathrm{grad}f(x_k)]\|\le\tilde{L}(\|r_k\|+\|\mathrm{grad}f(x_k)\|)\le 2\tilde{L}\|\mathrm{grad}f(x_k)\|.  \label{RieNew-OrderOfConv-3}
    \end{align}
    Thus, we have 
  	\[
  		\frac{ \|\mathrm{grad}f(x_k)+ \mathrm{Hess}f(x_k)\eta_k \| }{\|\eta_k\|}\le \frac{\|\mathrm{grad}f(x_k)\|^{1+t}}{\|\eta_k\|}\le {(2L)^{1+t}\|\eta_k\|^t} \le (2L)^{1+t}\gamma_2^t\|\mathrm{grad}f(x_k)\|^t,
  	\]
  	which gives that 
  	\begin{align}
  		\label{RieNew-OrderOfConv-4}
  		\lim_{k\rightarrow \infty}\frac{ \|\mathrm{grad}f(x_k)+ \mathrm{Hess}f(x_k)\eta_k \| }{\|\eta_k\|}=0.
  	\end{align}
  	By \cite[Proposition 5]{ring_optimization_2012}, (\ref{RieNew-OrderOfConv-4}) implies that for sufficiently large $k$, the step size $\alpha_k=1$ is acceptable for Wolfe conditions and thus is too for (\ref{RieNew-BrydCond1}) or (\ref{RieNew-BrydCond2}) under Assumptions~\ref{TwiceContinuDiff} and~\ref{RadLipCont}.   
  	In addition, the superlinear convergence rate is obtained by \cite[Proposition 8]{ring_optimization_2012}.
  	
%  	We next prove the second part. 
  	By \cite[Lemma 7.4.8]{AbsMahSep2008} and \cite[Lemma 7.4.9]{AbsMahSep2008}, there exist constants $c_0>0,c_1>0$ and $c_2>0$ such that 
  	\begin{align}
  		c_0\mathrm{dist}(x,x^*) \le \|\mathrm{grad}f(x)\| \le c_1 \mathrm{dist}(x,x^*),\;\forall\;x\in B_{\mu_1}(x^*), \label{RieNew-OrderOfConv-5}
  	\end{align}  
  	and 
  	\begin{align} \label{RieNew-OrderOfConv-6}
  		\|\mathrm{grad}f(R_x(\eta_x))\| \le c_2 \|\mathrm{grad}\hat{f}_x(\eta_x)\|,\;\forall\;x\in B_{\mu_1}(x^*),\;\forall\;\eta_{x}\in B_{\mu_2}(0_{x}).
  	\end{align}
  	From the Taylor's formula for $\hat{f}_{x_k}$ at $0_{x_k}$, for sufficiently large $k$, we have 
  	 \begin{align*} 
  	 \mathrm{grad}\hat{f}_{x_{k}}(\eta_{k})&= \mathrm{grad}\hat{f}_{x_{k}}(0_{x_{k}}) + \mathrm{Hess}\hat{f}_{x_{k}}(0_{x_{k}})[\eta_{k}] + O(\|\eta_{k}\|^{2}) \\ &= \mathrm{grad}f(x_{k}) + \mathrm{Hess}f(x_{k})[\eta_{x}] + (\mathrm{Hess}\hat{f}_{x_{k}}(0_{x_{k}})-\mathrm{Hess}f(x_{k}))[\eta_{k}] + O(\|\eta_{k}\|^{2})  \\ &=  r_{k} + (\mathrm{Hess}\hat{f}_{x_{k}}(0_{x_{k}})-\mathrm{Hess}f(x_{k}))[\eta_{k}] + O(\|\eta_{k}\|^{2}).
  	 \end{align*} 
  	 It, together with (\ref{RieNew-OrderOfConv-1}), (\ref{RieNew-OrderOfConv-3}), (\ref{RieNew-OrderOfConv-5}) and (\ref{RieNew-OrderOfConv-6}), implies that 
  	  \begin{align*} 
  	  \|\mathrm{grad}f(x_{k+1})\| &\le c_{2}\|\mathrm{grad}\hat{f}_{x_{k}}(\eta_{k})\| \\ & \le c_{2}\|r_{k}\| + c_{2}\|\mathrm{Hess}\hat{f}_{x_{k}}(0_{x_{k}})-\mathrm{Hess}f(x_{k})\|\|\eta_{k}\| + c_{2}c_{3}\|\eta_{k}\|^{2} \\ & \le c_{2}\|r_{k}\| + c_{2}(\beta_{1}+c_{3})\|\eta_{k}\|^{2} \\ & \le c_{2}(\|\mathrm{grad}f(x_{k})\|^{t} + 4\tilde{L}^{2}(\beta_{1}+c_{3})\|\mathrm{grad}f(x_{k})\|)\|\mathrm{grad}f(x_{k})\| ,
  	  \end{align*} 
  	  where $c_3>0$ is a constant. Hence, we obtain that 
  	  \begin{equation} \nonumber 	  	
  	   \begin{aligned}   	   
  	   \frac{\mathrm{dist}(x_{k+1},x^{*})}{\mathrm{dist}(x_{k},x^{*})^{1+\min(1,t)}} & \le \frac{c_{1}^{1+\min(1,t)}}{c_{0}}\frac{\|\mathrm{grad}f(x_{k+1})\|}{\|\mathrm{grad}f(x_{k})\|^{1+\min(1,t)}} \\ & \le \frac{c_{2}c_{1}^{1+\min(1,t)}}{c_{0}}\left( \|\mathrm{grad}f(x_{k})\|^{t-\min(1,t)} + 4\tilde{L}^{2}(\beta_{1}+c_{3})\|\mathrm{grad}f(x_{k})\|^{1-\min(1,t)} \right)  \\ & \le C, 
  	   \end{aligned} 
  	  \end{equation}
  	  for a constant $C>0$, which completes the proof. %For sufficiently large $k$, the inequality above is equivalent to 
%  	  $ 
%  	  \|R_{x^{*}}^{-1}(x_{k+1})\|\le \tilde{C} \|R_{x^{*}}^{-1}(x_{k})\| 
%  	  $
%  	  with a positive constant $\tilde{C}$. We complete the proofs. 

\end{proof}

\section{Riemannian Quotient Manifold \texorpdfstring{$\mathbb{R}_*^{n\times p}/\mathcal{O}_p$}{} } \label{QuotMani}

In this section, the optimization tools of $\mathbb{R}_*^{n \times p}/\mathcal{O}_p$ are reviewed, including tangent space, horizontal space, vertical space, Riemannian metric, retraction, Riemannian gradient, and Riemannian Hessian. The detailed derivations can be found in~\cite{Zheng2022RiemannianOU}.

%The matrices set $\mathbb{R}_*^{n\times p}$ is a open submanifold of $\mathbb{R}^{n\times p}$, where $p\ll n$,  and the quotient space $\mathbb{R}_*^{n\times p}/\mathcal{O}_p$ is a quotient manifold of $\mathbb{R}_*^{n\times p}$ \cite{AbsMahSep2008,MFPR10}, therefore the relationship between them is very close. In fact, for any $Y \in \mathbb{R}_*^{n\times p}$, there exists a bijective $\mathrm{D}\pi(Y)$ from a subspace, so-called horizontal space, of $\mathrm{T}_Y\mathbb{R}_*^{n\times p}$ to the tangent space $\mathrm{T}_{\pi(Y)}\mathbb{R}_*^{n\times p}/\mathcal{O}_p$. Then for each tangent vector $\eta_{\pi(Y)}$, we can find the unique vector corresponding to it in the horizontal space. This makes implementation of algorithms on quotient manifold possible. 

%{\color{red}

%\subsection{Riemannian manifold \texorpdfstring{$\mathbb{R}_*^{n\times p}$}{}}

%The ingredients, such as Riemannian metric and retraction, on the total space $\mathbb{R}_*^{n\times p}$ can become the ones in the quotient manifold $\mathbb{R}_*^{n\times p}/\mathcal{O}_p$ under certain conditions, so we first give the related notions on $\mathbb{R}_*^{n\times p}$.


\paragraph{Vertical spaces.}
For any $Y \in \mathbb{R}_*^{n \times p}$, it has been shown in~~\cite[Proposition 3.4.4]{AbsMahSep2008} that the equivalence class $\pi^{-1}(\pi(Y))$ is an embedded submanifold $\mathbb{R}_*^{n \times p}$. The tangent space of $\pi^{-1}(\pi(Y))$ at $Y$ is called the vertical space of $\mathbb{R}_*^{n \times p}/\mathcal{O}_p$ at $Y$, denoted by $\mathrm{V}_Y$, and is given by 
\[
  \mathrm{V}_Y=\mathrm{T}_Y \pi^{-1}(\pi(Y))=\{Y\Omega: \Omega  \in \mathrm{Skew}(p)\} \subseteq \mathrm{T}_Y\mathbb{R}_*^{n\times p},
\]
  where %$\mathcal{F}=\{X\in \mathbb{R}_*^{n\times p}: X \sim Y\}$ is the fiber of $Y$ and 
  $\mathrm{Skew}(p) = \{A\in\mathbb{R}^{p\times p}: A^T=-A\}$. 
 
\paragraph{Riemannian metrics on $\mathbb{R}_*^{n\times p}$.} Three Riemannian metrics on $\mathbb{R}_*^{n\times p}$ given in~\cite{Zheng2022RiemannianOU} are considered here: 
\begin{equation} \label{Rmetrics}
g^{i}_Y(\eta_Y,\xi_Y) = 
\begin{cases}	
	2\mathrm{trace}(Y^T\eta_YY^T\xi_Y+Y^TY\eta_Y^T\xi_Y) 
        +\mathrm{trace}\left(Y^TY(\eta_Y^{\mathrm{V}})^T(\xi_Y^{\mathrm{V}})\right) &  i = 1,  \\
	\mathrm{trace}(Y^TY\eta_Y^T\xi_Y) & i = 2,  \\
	\mathrm{trace}(\eta_Y^T\xi_Y)  & i = 3,  
\end{cases}
\end{equation}
where $\eta_Y,\xi_Y\in \mathrm{T}_Y\mathbb{R}_*^{n\times p} = \mathbb{R}^{n \times p}$, $\eta_Y^{\mathrm{V}}= Y \left( (Y^TY)^{-1}Y^T \eta_Y-\eta_Y^T Y(Y^TY)^{-1} \right) / 2$, $\xi_Y^{\mathrm{V}} = Y ( (Y^TY)^{-1}Y^T \xi_Y-$ $\xi_Y^T Y(Y^TY)^{-1} ) / 2$.
%$\mathcal{P}_Y^{\mathrm{V}}(\eta_Y)$, $\xi_Y^{\mathrm{V}}=\mathcal{P}_Y^{\mathrm{V}}(\xi_Y)$, and 
%    $
%      \mathcal{P}_Y^{\mathrm{V}} (A) = Y \left( \frac{ (Y^TY)^{-1}Y^TA-A^TY(Y^TY)^{-1} }{2} \right)
%    $
%    is a orthogonal projection of $A$ with respect to $g_Y^2$ from $\mathrm{T}_Y\mathbb{R}_*^{n\times p}$ to $\mathrm{V}_Y$. 
The Riemannian metric $g^1_Y$ is essentially equivalent to the Euclidean metric on $\mathcal{S}_+(p,n)$, see details in~\cite{Zheng2022RiemannianOU}; the Riemannian metric $g_Y^2$ has been used in \cite{HUANG2017}, where the space under consideration is on the complex field; and the Riemannian metric $g_Y^3$ is the standard Euclidean inner product on $\mathbb{R}^{n\times p}$ and has been considered in~\cite{MA20}.

%}

%{\color{red}

\paragraph{Horizontal spaces.} %As long as the manifold $\mathbb{R}_*^{n\times p}$ is equipped with a Riemannian metric $g$, the tangent space $\mathrm{T}_Y\mathbb{R}_*^{n\times p}$ becomes a Euclidean space with inner product $g_Y$, 
Given a Riemannian metric $g$ on the total space $\mathbb{R}_*^{n \times p}$, the orthogonal complement space in $\mathrm{T}_Y\mathbb{R}_*^{n\times p}$ of $\mathrm{V}_Y$ with respect to $g_Y$ is called the horizontal space at $Y$, denoted by $\mathrm{H}_Y$. The horizontal spaces with respect to the three Riemannian metrics in~\eqref{Rmetrics} are respectively given by
\begin{equation} \nonumber
	\mathrm{H}_Y^i = \begin{cases}
		\{YS+Y_\perp K:S^T=S,S\in\mathrm{R}^{p\times p},K\in\mathbb{R}^{(n-p)\times p}\} & i=1,\\ 
		\{YS+Y_\perp K:S^T=S,S\in\mathrm{R}^{p\times p},K\in\mathbb{R}^{(n-p)\times p}\} & i=2, \\
		\{Y(Y^TY)^{-1}S+Y_\perp K:S^T=S,S\in\mathbb{R}^{p\times p},K\in\mathbb{R}^{(n-p)\times p}\} & i=3.
	\end{cases}
\end{equation}
By~\cite[Section~3.5.8]{AbsMahSep2008}, the mapping $\mathrm{D}\pi(Y)$ is a bijection from $\mathrm{H}_Y$ to $\mathrm{T}_{\pi(Y)}\mathbb{R}_*^{n\times p}/\mathcal{O}_p$, so for each $\eta_{\pi(Y)}\in \mathrm{T}_{\pi(Y)}\mathbb{R}_*^{n\times p}/\mathcal{O}_p$, there exists unique vector $\eta_{Y}\in \mathrm{H}_Y$ such that $\mathrm{D}\pi(Y)[\eta_{Y}]=\eta_{\pi(Y)}$. This $\eta_{Y}$ is called the horizontal lift of $\eta_{\pi(Y)}$ at $Y$, denoted by $\eta_{\uparrow_Y}$.

\paragraph{Projections onto Vertical Spaces and Horizontal Spaces.} 
%Thanks to the direct sum property $\mathrm{T}_Y\mathbb{R}_*^{n\times p}=\mathrm{V}_x \oplus \mathrm{H}_x^i$, where $i=1,2,3$ correspond to respectively the three different metrics, there exist projection operators for any $\eta_Y\in\mathrm{T}_Y\mathbb{R}_*^{n\times p}$ to $\mathrm{H}^i_Y$ satisfying 
%$$ 
%\eta_Y=\mathcal{P}_Y^{\mathrm{V}^{i}}(\eta_Y)+ \mathcal{P}_Y^{\mathrm{H}^i}(\eta_Y).
%$$
%We note that under the first two metrics, the horizontal spaces are the same and  interestingly, so do the orthogonal projectors onto vertical and horizontal spaces, i.e., 
For any $Y\in \mathbb{R}_*^{n\times p}$ and $\eta_Y\in \mathrm{T}_Y\mathbb{R}_*^{n\times p}$, the orthogonal projections of $\eta_Y$ to $\mathrm{V}_Y$ and $\mathrm{H}_Y^i$ with respect to the first two metrics are respectively given by
	$$
	\begin{aligned} 
	\mathcal{P}_Y^{\mathrm{V}^i}(\eta_Y) = Y\Omega, \text{ and } \mathcal{P}_Y^{\mathrm{H}^i}(\eta_Y)&=\eta_Y-\mathcal{P}_Y^{\mathrm{V}^{i}}(\eta_Y)=\eta_Y-Y\Omega \\ &= Y\left( \frac{(Y^TY)^{-1}Y^T\eta_Y+\eta_Y^TY(Y^TY)^{-1}}{2} \right) + Y_\perp Y_\perp^T\eta_Y, 
    \end{aligned}
	$$
	where $\Omega = \frac{(Y^TY)^{-1}Y^T\eta_Y-\eta_Y^TY(Y^TY)^{-1}}{2}$. The orthogonal projections to vertical and horizontal spaces with respect to the third metric are 
    $$ 
    \mathcal{P}_Y^{\mathrm{V}^3}(\eta_Y)=Y\Omega, \text{ and } \mathcal{P}_Y^{\mathrm{H}^3}(\eta_Y)=\eta_Y-\mathcal{P}_Y^{\mathrm{V}}(\eta_Y)=\eta_Y-Y\Omega,
    $$
    where $\Omega$ is the skew-symmetric matrix satisfying $ \Omega Y^TY +Y^TY\Omega=Y^T\eta_Y-\eta^T_YY.$

%}


%{\color{red}
%\subsection{Riemannian Quotient Manifold \texorpdfstring{$\mathbb{R}_*^{n\times p}/\mathcal{O}_p$}{}}

%We now aim to give the ingredients, including Riemannian metrics, retraction, lift of gradient and lift of the action of Hessian, on the quotient manifold $\mathbb{R}_{*}^{n\times p}/\mathcal{O}_p$ for Riemannian optimization.

\paragraph{Riemannian metrics on $\mathbb{R}_*^{n\times p}/\mathcal{O}_p$.} If for all $\xi_{\pi(Y)},\eta_{\pi(Y)} \in\mathrm{T}_{\pi(Y)}\mathbb{R}_*^{n\times p}/\mathcal{O}_p$, it holds that 
\begin{align} \label{IngredQuotMani-Metric-induced}
	Y\sim Z \Rightarrow g_Y(\xi_{\uparrow_Y},\eta_{\uparrow_Y})  = g_Z(\xi_{\uparrow_Z},\eta_{\uparrow_Z}), 
\end{align}
the Riemannian metric on $\mathbb{R}_*^{n\times p}/\mathcal{O}_p$ induced by $g$ is defined as 
$
	g_{\pi(Y)}(\xi_{\pi(Y)},\eta_{\pi(Y)}) = g_Y(\xi_{\uparrow_Y},\eta_{\uparrow_Y}).
$
Since the three Riemannian metrics $g_Y^1,g_Y^2,g_Y^3$ on $\mathbb{R}_*^{n\times p}$ satisfy~\eqref{IngredQuotMani-Metric-induced} by~\cite{Zheng2022RiemannianOU}, the corresponding three metrics on $\mathbb{R}_*^{n\times p}/\mathcal{O}_p$ are given by:  
%\begin{align*}
%    g^1_{[Y]}(\xi_{[Y]},\eta_{[Y]}) & = 2\mathrm{trace}(Y^T\xi_{\uparrow_Y}Y^T\eta_{\uparrow_Y}+Y^TY\xi_{\uparrow_Y}^T\eta_{\uparrow_Y}), \\ 
%    g^2_{[Y]}(\xi_{[Y]},\eta_{[Y]}) & = \mathrm{trace}(Y^TY\xi_{\uparrow_Y}^T\eta_{\uparrow_Y}), \\ 
%    g^3_{[Y]}(\xi_{[Y]},\eta_{[Y]}) & = \mathrm{trace}(\xi_{\uparrow_Y}^T\eta_{\uparrow_Y}),
%\end{align*}

\begin{subnumcases} {\label{IngredQuotMani-Metric} g^i_{\pi(Y)}(\xi_{\pi(Y)},\eta_{\pi(Y)})=}
2\mathrm{trace}(Y^T\xi_{\uparrow_Y}Y^T\eta_{\uparrow_Y}+Y^TY\xi_{\uparrow_Y}^T\eta_{\uparrow_Y}) &  $i = 1, $ \label{IngredQuotMani-Metric1} \\
\mathrm{trace}(Y^TY\xi_{\uparrow_Y}^T\eta_{\uparrow_Y}) & $i = 2,$ \label{IngredQuotMani-Metric2} \\
\mathrm{trace}(\xi_{\uparrow_Y}^T\eta_{\uparrow_Y})  & $i =3,$ \label{IngredQuotMani-Metric3}
\end{subnumcases}
for all $\xi_{\pi(Y)},\eta_{\pi(Y)}\in\mathrm{T}_{\pi(Y)}\mathbb{R}_*^{n\times p}/\mathcal{O}_p$.
%The first equation holds because that for any $\xi_{\pi(Y)} \in \mathrm{T}_{\pi(Y)}\mathbb{R}_*^{n\times p}/\mathcal{O}_p$, $\xi_{\uparrow_Y}\in \mathrm{H}_Y$ and $\mathcal{P}_Y^{\mathrm{V}}(\cdot)$ is orthogonal onto $\mathrm{V}_Y$, and thus we have $\mathcal{P}_Y^\mathrm{V}(\xi_{\uparrow_Y})=0$. 
In the following, with a slight abuse of notation, we use $g^i$, $i=1,2,3$, to denote the Riemannian metrics on both $\mathbb{R}_*^{n\times p}$ and $\mathbb{R}_*^{n\times p}/\mathcal{O}_p$. 

\paragraph{Retraction.} We choose the retraction on $\mathbb{R}_*^{n\times p}/\mathcal{O}_p$ defined by
\begin{equation} \label{retraction}
   		R_{\pi(Y)}(\xi_{\pi(Y)}) = \pi(Y+ \xi_{\uparrow_Y}),
\end{equation}
where $Y \in \mathbb{R}_*^{n \times p}$ and $\xi_{\pi(Y)} \in \T_{\pi(Y)} \mathbb{R}_*^{n\times p}/\mathcal{O}_p$. Note that Retraction~\eqref{retraction} has been used in~\cite{MA20,Zheng2022RiemannianOU}.
%For any $\eta_Y \in \mathrm{T}_Y\mathbb{R}_*^{n\times p}$ and appropriate step size $\alpha>0$, $\bar{R}_Y(\alpha\eta_Y)=Y + \alpha\eta_Y$ is a retraction on $\mathbb{R}_*^{n\times p}$. For any vector field $\xi $ on $\mathbb{R}_*^{n\times p}/\mathcal{O}_p$, $Y \in \mathbb{R}_*^{n\times p}$ and $O\in\mathcal{O}_p$, it holds that $\xi_{\uparrow_{YO}}=\xi_{\uparrow_Y}O$ \cite[Prop.A.8]{Absil20}, which implies that 
%\[
%   		R_{\pi(Y)}(\alpha\xi_{\pi(Y)}) = \pi(Y+ \alpha \xi_{\uparrow_Y})
%\]
%defines a retraction on $\mathbb{R}_*^{n\times p}/\mathcal{O}_p$ with appropriate step size $\alpha>0$.

\paragraph{The horizontal lifts of Riemannian Gradients and the actions of Riemannian Hessians.} % For the cost function~\eqref{Pro_stat-FinalProb}, its gradient $\mathrm{grad}f(Y)$ is a vector in $\mathrm{T}_{\pi(Y)}\mathbb{R}_*^{n\times p}/\mathcal{O}_p$. Thus, there exists an unique horizontal lift obtained by using the gradient of lift function $\bar{f}=f\circ\pi$ of $f$.  Proposition~\ref{IngredQuotMani-Prop-Gradient} gives $(\mathrm{grad}f(\pi(Y)))_{\uparrow_Y}$ under the three metrics.
%The horizontal lifts of the Riemannian gradients of $f$ in~\eqref{Pro_stat-FinalProb} with respect to the three Riemannian metrics are respectively given by
It follows from~\cite{Zheng2022RiemannianOU} that the Riemannian gradients and Riemannian Hessian of $f$ in~\eqref{Pro_stat-FinalProb} can be characterized by the Euclidean gradient and the Euclidean Hessian of $h$ in~\eqref{Pro_stat-OptProb_Bart}, see Proposition~\ref{IngredQuotMani-Prop-Gradient}.
\begin{proposition} \label{IngredQuotMani-Prop-Gradient}
  The Riemannian gradients of the smooth real-valued function $f$ in \eqref{Pro_stat-FinalProb} on $\mathbb{R}_*^{n\times p}/\mathcal{O}_p$ with respect to the three Riemannian metrics are respectively given by 
  \[
  (\mathrm{grad}f(\pi(Y)))_{\uparrow_Y} = \mathrm{grad}\bar{f}(Y) =
  \begin{cases}
    \left( I - \frac{1}{2}Y(Y^TY)^{-1}Y^T \right)\nabla h(YY^T)Y(Y^TY)^{-1}  & \text{ under metric } g^{1},  \\ 
    2\nabla h(YY^T)Y(Y^TY)^{-1} & \text{ under metric } g^{2}, \\ 
    2\nabla h(YY^T)Y  & \text{ under metric } g^{3},
  \end{cases}
  \]
and the actions of the Riemannian Hessians are respectively given by
\begin{subnumcases} {\label{IngredQuotMani-Hessian} (\mathrm{Hess}f(\pi(Y))[\eta_{\pi(Y)}])_{\uparrow_Y}=}
\left(I - \frac{1}{2}P_Y\right) \nabla^2h(YY^T)[Y\eta_{\uparrow_Y}^T+\eta_{\uparrow_Y}Y^T]Y(Y^TY)^{-1} + T_1 &  $i = 1, $ \label{IngredQuotMani-Hessian1} \\
2\nabla^2h(YY^T)[Y\eta_{\uparrow_Y}^T+\eta_{\uparrow_Y}Y^T]Y(Y^TY)^{-1}  + T_2 & $i = 2,$ \label{IngredQuotMani-Hessian2} \\
2\nabla^2h(YY^T)[Y\eta_{\uparrow_Y}^T+\eta_{\uparrow_Y}Y^T]Y  + T_3  & $i =3,$ \label{IngredQuotMani-Hessian3}
\end{subnumcases}
where $\nabla^2h(YY^T)[V] = AVM + MVA$, $P_Y=Y(Y^TY)^{-1}Y^T$, $T_1 = (I-P_Y)\nabla h(YY^T)(I-P_Y)\eta_{\uparrow_Y}(Y^TY)^{-1}$, $ T_2 = \mathcal{P}_Y^{\mathrm{H}^2}\{ 
            \nabla h(YY^T)P_Y^\perp \eta_{\uparrow_Y}(Y^TY)^{-1}+P_Y^\perp \nabla h(YY^T)\eta_{\uparrow_Y}(Y^TY)^{-1} + 2\mathrm{skew}(\eta_{\uparrow_Y}Y^T)\\ \cdot\nabla h(YY^T)Y(Y^TY)^{-2}  + 2\mathrm{skew}\{ \eta_{\uparrow_Y}(Y^TY)^{-1}Y^T\nabla h(YY^T) \}Y(Y^TY)^{-1}\}$, and $T_3 = 2\mathcal{P}_Y^{\mathrm{H}^3}\{ \nabla h(YY^T)\eta_{\uparrow_Y} \}$. 
\end{proposition}
%  \begin{proof}
%    see \cite{Zheng2022RiemannianOU}, where the complex version has been discussed.
%  \end{proof}

%\textbf{The horizontal lifts of the actions of Hessian.} For the cost function~\eqref{Pro_stat-FinalProb}, its Hessian $\mathrm{Hess}f(\pi(Y))$ is a linear mapping from $\mathrm{T}_{\pi(Y)}\mathbb{R}_*^{n\times p}/\mathcal{O}_p$ to $\mathrm{T}_{\pi(Y)}\mathbb{R}_*^{n\times p}/\mathcal{O}_p$. Then, for any $\eta_{\pi(Y)}\in\mathrm{T}_{\pi(Y)}\mathbb{R}_*^{n\times p}/\mathcal{O}_p$, the horizontal lift $(\mathrm{Hess}f(\pi(Y))[\eta_{\pi(Y)}])_{\uparrow_Y}$ is given by projecting $\mathrm{Hess}\bar{f}(Y)[\eta_{\uparrow_Y}]$ to the horizontal space. 
%\begin{proposition}
%	The Hessian of the cost function $f$ \eqref{Pro_stat-FinalProb} is given by
%
%  \begin{proof}
%    see \cite{Zheng2022RiemannianOU}, where the complex version has been discussed.
%  \end{proof}
%\end{proposition}

%We observe that under the three metrics the lifts of the actions of Riemannian Hessians are including two part: the first-order term and the second-order term with respect to $h$. We will see that in designing preconditioners (see Section \ref{sec:IncRankAlgPrecond:Precond}), we only need to focus on the second-order term.

%}


%\subsection{Riemannian Hessians}
%
%Likewise, for any smooth function $f$ on a quotient manifold, Theorem \ref {IngredQuotMani-Theorem-Hessian} points out that the Riemannian Hessian of function $f$ is bound up with the Riemannian Hessian of its lifted function $\bar{f}=f\circ\pi$. 
%
%\begin{theorem}\cite[Proposition 9.44]{Boumal:300281} \label{IngredQuotMani-Theorem-Hessian}
%  The Riemannian Hessian of $f$ on a Riemannian quotient manifold is related to the Riemannian Hessian of the lifted function $\bar{f}=f\circ\pi$ on the total spaces as 
%%  \[
%%    \xcancel{(\mathrm{Hess}f([x])[\xi_{[x]}])_{\uparrow_x} = \mathcal{P}_x^{\mathrm{H}}(\mathrm{Hess}\bar{f}(x)[\xi_{\uparrow_x}])}
%%  \]
%  \[
%  \zwhcomm{(\mathrm{Hess}f(\pi(x))[\xi_{\pi(x)}])_{\uparrow_x} = \mathcal{P}_x^{\mathrm{H}}(\mathrm{Hess}\bar{f}(x)[\xi_{\uparrow_x}])}{}
%  \]
%  for all $x\in \overline{\mathcal{M}}$ and $\xi_{\pi(x)}\in \mathrm{T}_{\pi(x)}\mathcal{M}$. $\mathcal{P}_x^{\mathrm{H}}: \mathrm{T}_x\overline{\mathcal{M}} \rightarrow \mathrm{H}_x$ is a orthogonal projector.
%\end{theorem}
%
%By Theorem \ref{IngredQuotMani-Theorem-Hessian}, it is not hard to calculate the lifts of the Riemannian Hessian of function (\ref{Pro_stat-FinalProb}), which is summarized as follows. Details can be seen in \cite{Zheng2022RiemannianOU}.
%
%\begin{lemma} \label{IngredQuotMani-Lemma-Hessians1}
%  As for the first Riemannian metric \ref{IngredQuotMani-Metric1}, the lift of Riemannian Hessian of $f$ (\ref{Pro_stat-FinalProb}) is given by 
%  \[
%    \begin{aligned}
%      (\mathrm{Hess}f(\pi(Y))[\eta_{\pi(Y)}])_{\uparrow_Y} & = \mathcal{P}_Y^{\mathrm{H}^1}(\mathrm{Hess}\bar{f}(Y)[\eta_{\uparrow_Y}]) \\ 
%      & = \left(I - \frac{1}{2}P_Y\right) \nabla^2h(YY^T)[Y\eta_{\uparrow_Y}^T+\eta_{\uparrow_Y}Y^T]Y(Y^TY)^{-1} + T_1, \\ 
%%      & \xcancel{ \quad \; + (I-P_Y)\nabla h(YY^T)(I-P_Y)\eta_{\uparrow_Y}(Y^TY)^{-1}, }
%    \end{aligned}
%    \]
%    where $\nabla^2h(YY^T)[V] = AVM + MVA$, $P_Y=Y(Y^TY)^{-1}Y^T$, \zwhcomm{$\nabla h(YY^T)=AYY^TM + MYY^TA$ and $T_1 = (I-P_Y)\nabla h(YY^T)(I-P_Y)\eta_{\uparrow_Y}(Y^TY)^{-1}$.}{}
%\end{lemma}
%
%\begin{lemma} \label{IngredQuotMani-Lemma-Hessians2}
%  As for the second Riemannian metric \ref{IngredQuotMani-Metric2}, the lift of Riemannian Hessian of $f$ (\ref{Pro_stat-FinalProb}) is given by 
%  \[
%    \begin{aligned}
%      (\mathrm{Hess}f(\pi(Y))[\eta_{\pi(Y)}])_{\uparrow_Y} = & \mathcal{P}_Y^{\mathrm{H}^2}\bigg\{         
%           2 \nabla^2h(YY^T)[Y\eta_{\uparrow_Y}^T+\eta_{\uparrow_Y}Y^T]Y(Y^TY)^{-1}+ \nabla h(YY^T)P_Y^\perp\eta_{\uparrow_Y}(Y^TY)^{-1} \\ 
%           & + P_Y^\perp\nabla h(YY^T)\eta_{\uparrow_Y}(Y^TY)^{-1} + 2skew(\eta_{\uparrow_Y}Y^T)\nabla h(YY^T)Y(Y^TY)^{-2} \\ 
%           & + 2skew\{ \eta_{\uparrow_Y}(Y^TY)^{-1}Y^T\nabla h(YY^T) \}Y(Y^TY) \bigg\} \\ 
%          = & 2\nabla^2h(YY^T)[Y\eta_{\uparrow_Y}^T+\eta_{\uparrow_Y}Y^T]Y(Y^TY)^{-1}  + T_2, \\           
%%          & \xcancel{+ \mathcal{P}_Y^{\mathrm{H}^2}\{ 
%%            \nabla h(YY^T)P_Y^\perp \eta_{\uparrow_Y}(Y^TY)^{-1}+P_Y^\perp \nabla h(YY^T)\eta_{\uparrow_Y}(Y^TY)^{-1}} \\ 
%%            & \xcancel{+ 2skew(\eta_{\uparrow_Y}Y^T)\nabla h(YY^T)Y(Y^TY)^{-2}}\\ 
%%            & \xcancel{+ 2skew\{ \eta_{\uparrow_Y}(Y^TY)^{-1}Y^T\nabla h(YY^T) \}Y(Y^TY)^{-1}\}},
%    \end{aligned}
%  \]
%	where $\nabla^2h(YY^T)[V] = AVM + MVA$, $P_Y=Y(Y^TY)^{-1}Y^T$, \zwhcomm{$\nabla h(YY^T)=AYY^TM + MYY^TA$, $ T_2 = \mathcal{P}_Y^{\mathrm{H}^2}\{ 
%            \nabla h(YY^T)P_Y^\perp \eta_{\uparrow_Y}(Y^TY)^{-1}+P_Y^\perp \nabla h(YY^T)\eta_{\uparrow_Y}(Y^TY)^{-1} + 2skew(\eta_{\uparrow_Y}Y^T)\nabla h(YY^T)Y(Y^TY)^{-2} \\+ 2skew\{ \eta_{\uparrow_Y}(Y^TY)^{-1}Y^T\nabla h(YY^T) \}Y(Y^TY)^{-1}\}$ and $skew(Z) = (Z-Z^T)/2$.}{}
%   \end{lemma}
%
%\begin{lemma} \label{IngredQuotMani-Lemma-Hessians3}
%  As for the third Riemannian metric \ref{IngredQuotMani-Metric3}, the lift of Riemannian Hessian of $f$ (\ref{Pro_stat-FinalProb}) is given by 
%  \begin{align*}
%      (\mathrm{Hess}f(\pi(Y))[\eta_{\pi(Y)}])_{\uparrow_Y} &=\mathcal{P}_Y^{\mathrm{H}^3}\{ 2\nabla^2h(YY^T)[Y\eta_{\uparrow_Y}^T+\eta_{\uparrow_Y}Y^T]Y + 2\nabla h(YY^T)\eta_{\uparrow_Y} \} \\ 
%      &= 2\nabla^2h(YY^T)[Y\eta_{\uparrow_Y}^T+\eta_{\uparrow_Y}Y^T]Y  + T_3, 
%%      \xcancel{2\mathcal{P}_Y^{\mathrm{H}^3}\{ \nabla h(YY^T)\eta_{\uparrow_Y} \}},
%  \end{align*}
%  where $\nabla^2h(YY^T)[V] = AVM + MVA$, $P_Y=Y(Y^TY)^{-1}Y^T$, \zwhcomm{$\nabla h(YY^T)=AYY^TM + MYY^TA$ and $T_3 = 2\mathcal{P}_Y^{\mathrm{H}^3}\{ \nabla h(YY^T)\eta_{\uparrow_Y} \}$.}{}
%\end{lemma}


\section{Increasing Rank Algorithm and Preconditioning} \label{FinalAlgPrecond}

\subsection{Increasing Rank Algorithm}

Algorithm \ref{RieNew-TruncatedNewton} gives a low-rank approximation when the rank is known in advance. 
%Let $k=\mathrm{rank}(X^*)$ where $X^*$ is the solution of Equation (\ref{Intro-LyapMatEqu}). We have the relation between residual and function value as follows: 
%\begin{align}
%	\mathrm{residual}^2 &= \|AXM+MXA-C\|_F^2 \nonumber \\ &= \|AXM+MXA - (AX^*M+MX^*A)\|_F^2 \nonumber \\ &= \| AEM + MEA \|_F^2 \nonumber \\ 
%	    &= \mathrm{trace}[(AEM+MEA)(AEM+MEA)] \nonumber\\ 
%	    &= \mathrm{vec}(AEM+MEA)^T\mathrm{vec}(AEM+MEA)\nonumber \\
%	    &= (L\mathrm{vec}(E))^T(L\mathrm{vec}(E)) \nonumber \\
%	    &= (L^{1/2}\mathrm{vec}(E))^TL(L^{1/2}\mathrm{vec}(E)) \nonumber\\ 
%	    &= \|L^{1/2}\mathrm{vec}(E)\|_L^2  \nonumber \\
%	    &\le \|L^{1/2}\|_F^2 \|\mathrm{vec}(E)\|_L^2 \nonumber \\ 
%	    &= 2\mathrm{trace}(A)\mathrm{trace}(M)\|\mathrm{vec}(E)\|_L^2 \nonumber \\ 
%	    &= 4\mathrm{trace}(A)\mathrm{trace}(M)(h(X)+\mathrm{trace}(X^*MX^*A))\label{FinalAlgPrecond-relationRandF}
%\end{align}
%According to Inequality (\ref{FinalAlgPrecond-relationRandF}), if we want $\mathrm{residual}<\epsilon$, then just require $h(X)<\frac{\epsilon^2}{4\mathrm{trace}(A)\mathrm{trace}(M)}-\mathrm{trace}(X^*MX^*A)$. For implementation, we use $X_i$ to take place of $X^*$ when $i$ is sufficiently large. In other words, we set $h(X_i)<\frac{\epsilon^2}{4\mathrm{trace}(A)\mathrm{trace}(M)}-\mathrm{trace}(X_iMX_iA)$ to act as stop criterion.
In practice, however, the rank of the exact solution $X^*$ of Equation (\ref{Intro-LyapMatEqu}) is unknown beforehand. 
Therefore, we propose a rank-increasing algorithm, which solves Equation~\eqref{Intro-LyapMatEqu} with a low estimation of the rank. If the residual of the accumulation point is not sufficiently small, then the rank is increased and Problem~\eqref{Pro_stat-FinalProb} is solved by Algorithm~\ref{RieNew-TruncatedNewton} with an initial iterate motivated from the accumulation point of the lower rank. 
%Hence, we find a series of low-rank approximations of Problem (\ref{Pro_stat-FinalProb}) by Algorithm \ref{RieNew-TruncatedNewton} with rank $p$ increasing. 
The details are stated in Algorithm~\ref{FinalAlgPrecond-RLyap-RNewton} (IRRLyap).

\begin{algorithm}[htbp]
  \caption{An Increasing Rank Riemannian Method for Lyapunov Equations (IRRLyap)} 
  \begin{algorithmic}[1] \label{FinalAlgPrecond-RLyap-RNewton}
  \REQUIRE minimum rank $p_{\min}$; maximum rank $p_{\max}$; rank increment $p_{\mathrm{inc}}$; initial iterate $Y_{p_{\min}}^{\mathrm{initial}} \in \mathbb{R}_*^{n\times p_{\min}}$; tolerance sequence of inner iteration $\{\tau_{p}:p \in \{ p_{\min}, p_{\min} + p_{\mathrm{inc}}, p_{\min} + 2 p_{\mathrm{inc}},\ldots, p_{\max} \} \}$; residual tolerance $\tau$;
  \ENSURE low-rank approximation $\widetilde{Y}$;
  \FOR{$p=p_{\min}, p_{\min} + p_{\mathrm{inc}}, p_{\min} + 2 p_{\mathrm{inc}}, \ldots, p_{\max}$}
  \STATE \label{FinalAlgPrecond-RLyap-RNewton-st01} Invoke an optimization algorithm, such as Algorithm \ref{RieNew-TruncatedNewton}, to approximately solve Problem \eqref{Pro_stat-FinalProb} with the initial iterate $\pi(Y_p^{\mathrm{initial}})$ until the last iterate $\pi(Y_p)$ satisfies $\|\grad f(\pi(Y_p))\|\le \tau_{p} \|\mathrm{grad}f(\pi(Y_p^{\mathrm{initial}}))\|$;
%   \zwhcomm{where $Y_p\gets Y_p^{(k)}$ if $\|\grad f(\pi(Y_p^{(k)}))\|\le\tau_{p}$}{};
  \STATE \label{FinalAlgPrecond-RLyap-RNewton-st02} Compute relative residual of $Y_p$: $r_p \gets \|AY_pY_p^TM + MY_pY_p^TA-C\|_F/\|C\|_F$; 
  \IF {$r_p\le \tau$}
  \STATE \label{FinalAlgPrecond-RLyap-RNewton-st03} Return $\widetilde{Y}\gets Y_p$;
  \ELSE 
  \STATE \label{FinalAlgPrecond-RLyap-RNewton-st04} Calculate the next initial iterate $Y_{p+p_{\mathrm{inc}}}^{\mathrm{initial}}$ by performing one step of steepest descent on $\begin{bmatrix}
    Y_p & \mathbf{0}_{n\times p_{\text{inc}}}
  \end{bmatrix}$;
  \ENDIF
  \ENDFOR
  \STATE Return $\widetilde{Y}\gets Y_{p_{\max}}$;
 \end{algorithmic}
\end{algorithm}

Step~\ref{FinalAlgPrecond-RLyap-RNewton-st01} in Algorithm~\ref{FinalAlgPrecond-RLyap-RNewton} can invoke Algorithm~\ref{RieNew-TruncatedNewton}, which approximately solves Proplem~\eqref{Pro_stat-FinalProb} with fixed rank $p$ in the sense that the norm of gradient is reduced sufficiently, i.e., $\|\grad f(\pi(Y_p))\|\le \tau_{p} \|\mathrm{grad}f(\pi(Y_p^{\mathrm{initial}}))\|$. The tolerance sequence $\{\tau_p\}$ can be prescribed parameters or adaptively dependent on the current iterate.

%When the gradient satisfies $\|\mathrm{grad}f(\pi(Y_p^{(k)}))\|\le\tau_{p}\|\mathrm{grad}f(\pi(Y_p^{(0)}))\|$, the output of Algorithm~\ref{RieNew-TruncatedNewton} is set as $Y_p \gets Y_p^{(k)}$. The selection for $\tau_{p}$ is adaptive, that is, with the increase of rank $p$, $\tau_{p}$ is increasingly small such that Problem~\ref{Pro_stat-FinalProb} with rank $p$ is solved with increasing accuracy. 


	Step~\ref{FinalAlgPrecond-RLyap-RNewton-st02} computes the relative residual $r_p$ by following the steps in~\cite{Bart10}. Note that the steps in~\cite{Bart10} avoid the computations of the $n$-by-$n$ matrices in the residual and only require $O(n p^2)$ flops.
	
	Step~\ref{FinalAlgPrecond-RLyap-RNewton-st04} performs one step steepest descent with initial iterate $[Y_p\; 0_{n\times p_{\mathrm{inc}}}]$ for minimizing the cost function $\tilde{f}: \mathbb{R}^{n \times (p + p_{\mathrm{inc}})} \rightarrow \mathbb{R}: Y \mapsto \mathrm{trace}(Y^TAYY^TMY) - \mathrm{trace}(Y^TCY)$. Since $\tilde{f}$ is defined on the Euclidean space $\mathbb{R}^{n \times (p + p_{\mathrm{inc}})}$, the steepest descent algorithm can be defined at $[Y_p\; 0_{n\times p_{\mathrm{inc}}}]$. In our implementation, the step size is found by the backtracking line search algorithm. Therefore, Algorithm~\ref{FinalAlgPrecond-RLyap-RNewton} is a descent algorithm in the sense that the value of $h(Y_p Y_p^T) > h(Y_{p+p_\mathrm{inc}} Y_{p+p_\mathrm{inc}}^T)$.
	%Performing one step steepest descent guarantees that the next iterate $Y_{p+p_{\mathrm{inc}}}$ has full rank and that the values of functions $\{f(\pi(Y_p)):p=p_{\min}:p_{\mathrm{inc}}:p_{\max}\}$ is monotonic decreasing, which make the residuals $\{r_p:p=p_{\min}:p_{\mathrm{inc}}:p_{\max}\}$ as a whole reduced. 

%{\color{red}
%}

\subsection{Preconditioning} \label{sec:IncRankAlgPrecond:Precond}

%For Newton's method, e.g., Algorithm \ref{RieNew-TruncatedNewton}, most of the cost is concentrated on solving Newton equations. In large-scale setting, it can be solve with iterative methods, say Algorithm \ref{RieNew-TrunConjGrad}. Using iterative methods, we will naturally find ways to reduce the number of iterations in inner iterations. The reason why we choose CG method to solve Newton equation is that CG method is especially suitable for preconditioning. A good preconditioner will significantly reduce the number of iterations for solving Newton equations. We, in this section, will work to derive such preconditioners.

When the condition number of the Riemannian Hessian of $f$ at the minimizer $x^*$ is large and the sequence $\{x_k\}$ generated by Algorithm~\ref{RieNew-TruncatedNewton} converges to $x^*$, the number of iterations in the conjugate gradient method (Algorithm~\ref{RieNew-TrunConjGrad}) can be large. To improve the efficiency, the preconditioned conjugate gradient method given in~\cite{NW06} is used. In this section, we derive preconditioners that approximate the inverse of the Riemannian Hessian with respect to the three Riemannian metrics. 

\subsubsection{Preconditioning under Riemannian Metric~\eqref{IngredQuotMani-Metric1}} \label{sec:PreCon01}

The Newton direction is given by solving the Newton equation, i.e.,
\begin{gather*}
\hbox{ find } \xi_{\pi(Y)} \in \T_{\pi(Y)} \mathbb{R}_*^{n \times p} / \mathcal{O}_p \\
\hbox{ such that } \Hess f( \pi(Y) ) [\xi_{\pi(Y)}] = - \grad f(\pi(Y)).
\end{gather*}
Therefore, when the Riemannian metric~\eqref{IngredQuotMani-Metric1} is used, Algorithm~\ref{RieNew-TruncatedNewton} needs to approximately solve 
\begin{equation} \label{e01}
  \left( I-\frac{1}{2}P_Y\right)\nabla^2h(YY^T)[Y\xi_{\uparrow_Y}^T+\xi_{\uparrow_Y}Y^T]Y(Y^TY)^{-1} + (I-P_Y)\nabla h(YY^T)(I-P_Y)\xi_{\uparrow_Y}(Y^T)^{-1} =\eta_{\uparrow_Y},
\end{equation}
for $\xi_{\uparrow_Y}\in\mathrm{H}_Y^1$, where $\eta_{\uparrow_Y}\in \mathrm{H}_Y^1$ denotes the horizontal lift of $-\grad f(\pi(Y))$.

The preconditioner that we proposed aims to solve
\begin{align} \label{Precond-Metric1-ApproxNewEqu}
  \left( I - \frac{1}{2}P_Y \right)\nabla^2h(YY^T)[Y\xi_{\uparrow_Y}^T+\xi_{\uparrow_Y}Y^T]Y(Y^TY)^{-1}=\eta_{\uparrow_Y},
\end{align}
by omitting the second term in~\eqref{e01}. Such an approximation is reasonable since (i) the second term is approximately zero if $Y_k Y_k^T \approx X^*$ and (ii) our numerical experiments show that this preconditioner effectively reduces the number of inner iterations.

Note that for any $Z \in \mathrm{T}_Y\mathbb{R}_*^{n \times p}=\mathbb{R}^{n\times p}$, there exist $W \in \mathbb{R}^{p \times p}, K \in \mathbb{R}^{(n - p) \times p}$ such that $Z = YW + Y_{\perp_M} K$, where $Y_{\perp_M} \in \mathbb{R}^{n \times (n - p)}$ satisfying $Y^T M Y_{\perp_M} = 0$ and $Y_{\perp_M}^T Y_{\perp_M} = I_{n - p}$. Furthermore, we have $Z = Y \mathrm{sym}(W) + Y \mathrm{skew}(W) + Y_{\perp_M} K$. Note that $Y \mathrm{skew}(W) \in \mathrm{V}_Y$, we have $Y (Y \mathrm{skew}(W))^T + Y \mathrm{skew}(W) Y^T = 0$, which implies the skew symmetric part of $W$ does not affect the solution of~\eqref{Precond-Metric1-ApproxNewEqu}. Next, we show an approach to find the solution of
\begin{align} \label{e03}
  \left( I - \frac{1}{2}P_Y \right)\nabla^2h(YY^T)[Y\xi_{Y}^T+\xi_{Y}Y^T]Y(Y^TY)^{-1}=\eta_{\uparrow_Y},
\end{align}
over the space $\{Y S + Y_{\perp_M} K \mid S \in\mathcal{S}_p^{\mathrm{sym}}, K \in \mathbb{R}^{(n - p) \times p}  \}$.


%\whcomm{[ZHTODO: $\xi_{\uparrow_Y} \Rightarrow \xi_Y$. ]}{}
Multiplying~\eqref{e03} by $I+P_Y$ and $(Y^TY)$ from left and right respectively yields 
\begin{equation} \label{e02}
  \nabla^2h(YY^T)[Y\xi_{Y}^T+\xi_{Y}Y^T]Y=(I+P_Y)\eta_{\uparrow_Y}(Y^TY).
\end{equation}
It follows from~\eqref{e02} and the expression of $\nabla^2 h(Y Y^T)$ in Proposition~\ref{IngredQuotMani-Prop-Gradient} that
\begin{align} \label{Precond-Metric1-ApproxNewEqu-1}
  [A(Y\xi_{Y}^T+\xi_{Y}Y^T)M+M(Y\xi_{Y}^T+\xi_{Y}Y^T)A]Y=(I+P_Y)\eta_{\uparrow_Y}(Y^TY).
\end{align}
%Using the decomposition: $\xi_{\uparrow_Y}=YS_\xi+Y_{\perp_M}K_\xi$, 
%, where $S_\xi=S_\xi^T$, $Y^TMY_{\perp_M}=0$, $Y^T_{\perp_M}Y_{\perp_M}=I_p$, $K_\xi\in \mathbb{R}^{(n-p)\times p}$, and $Y_{\perp_M}$ is the orthogonal complement of $MY$ (noting that $Y^{T}_{\perp_M}Y\not=0$), 
Multiplying~\eqref{Precond-Metric1-ApproxNewEqu-1} from left by $Y$ yields
\begin{equation}
\begin{aligned} \label{Precond-Metric1-1}
  &Y^TAY\xi_{Y}^TMY +Y^TA\xi_{Y}Y^TMY+Y^TMY\xi_{Y}^TAY+Y^TM\xi_{Y}Y^TAY\\ 
  &= 2Y^T\eta_{\uparrow_Y}(Y^TY) 
\end{aligned}
\end{equation}
and multiplying~\eqref{Precond-Metric1-ApproxNewEqu-1} from left by $Y_{\perp_M}$ yields
\begin{equation}
  \begin{aligned} \label{Precond-Metric1-2}
    & Y_{\perp_M}^TAY\xi_{Y}^TMY+Y_{\perp_M}^TA\xi_{Y}Y^TMY+Y_{\perp_M}^T  M\xi_{Y}Y^TAY\\
    & =Y_{\perp_M}^T(I+Y(Y^TY)^{-1}Y^T)\eta_{\uparrow_Y}(Y^TY) .
  \end{aligned}
\end{equation}
Plugging the decomposition $\xi_{Y}=YS_\xi+Y_{\perp_M}K_\xi$ into~\eqref{Precond-Metric1-1} and~\eqref{Precond-Metric1-2} respectively gives
\begin{equation}
  \begin{aligned} \label{Precond-Metric1-12}
    & 2Y^TAYS_\xi Y^TMY+Y^TAY_{\perp_M} K_\xi Y^TMY+Y^TMYK_\xi^TY_{\perp_M}^TAY + 2Y^TMYS_\xi Y^TAY \\ 
    & = 2Y^T\eta_{\uparrow_Y}(Y^TY), 
  \end{aligned}
\end{equation}
and
\begin{equation}
  \begin{aligned} \label{Precond-Metric1-22}
    & 2Y_{\perp_M}^TAYS_\xi Y^TMY+Y_{\perp_M}^TAY_{\perp_M} K_\xi Y^TMY +Y_{\perp_M}^T MY_{\perp_M} K_\xi Y^TAY \\ 
    & = Y_{\perp_M}^T(I+Y(Y^TY)^{-1}Y^T)\eta_{\uparrow_Y}(Y^TY).
  \end{aligned}
\end{equation}
%%By the decomposition $\xi_{\uparrow_Y}=YS_\xi+Y_{\perp_M}K_\xi$
%%Noting that $\xi_{\uparrow_Y}^TMY=(S_\xi^TY^T+K_\xi^TY_{\perp_M}^T)MY=S_\xi Y^TMY$, it continues with (\ref{Precond-Metric1-1}), 
%%  \begin{align*}
%%    &Y^TAYS_\xi Y^TMY +Y^TAYS_\xi Y^TMY \\ &+Y^TAY_{\perp_M} K_\xi Y^TMY+Y^TMYS_\xi Y^TAY \\ &+
%%    Y^TMYK_\xi^TY_{\perp_M}^T AY + Y^TMYS_\xi Y^TAY \\ = & 2Y^T\eta_{\uparrow_Y}(Y^TY),
%%  \end{align*}
%%that is, 
%\begin{equation}
%  \begin{aligned} \label{Precond-Metric1-12}
%    & 2Y^TAYS_\xi Y^TMY+Y^TAY_{\perp_M} K_\xi Y^TMY+Y^TMYK_\xi^TY_{\perp_M}^TAY + 2Y^TMYS_\xi Y^TAY \\ 
%    & = 2Y^T\eta_{\uparrow_Y}(Y^TY), 
%  \end{aligned}
%\end{equation}
%%and with (\ref{Precond-Metric1-2}), 
%%\begin{align*}
%%  & Y^TAYS_\xi Y^TMY+Y_{\perp_M}^TAYS_\xi Y^TMY+Y_{\perp_M}^TAY_{\perp_M} K_\xi Y^TMY + Y_{\perp_M}^TMY_{\perp_M} K_\xi Y^TAY\\ 
%%  & = Y_{\perp_M}^T(I+Y(Y^TY)^{-1}Y^T)\eta_{\uparrow_Y}(Y^TY),
%%\end{align*}
%that is, 
%\begin{equation}
%  \begin{aligned} \label{Precond-Metric1-22}
%    & 2Y_{\perp_M}^TAYS_\xi Y^TMY+Y_{\perp_M}^TAY_{\perp_M} K_\xi Y^TMY +Y_{\perp_M}^T MY_{\perp_M} K_\xi Y^TAY \\ 
%    & = Y_{\perp_M}^T(I+Y(Y^TY)^{-1}Y^T)\eta_{\uparrow_Y}(Y^TY).
%  \end{aligned}
%\end{equation}
Let $Z_\xi=Y_{\perp_M}K_\xi$. We obtain from \eqref{Precond-Metric1-22} that
\begin{align*} 
  Y_{\perp_M}^TAZ_\xi Y^TMY+Y_{\perp_M}^TMZ_\xi Y^TAY=Y_{\perp_M}^T(I+Y(Y^TY)^{-1}Y^T)\eta_{\uparrow_Y}(Y^TY)-2Y_{\perp_M}^TAYS_\xi Y^TMY. 
\end{align*}
Let $Y^TMY=LL^T$ be the Cholesky factorization. We have 
\begin{align*}
  & Y_{\perp_M}^TAZ_\xi L+Y_{\perp_M}^TMZ_\xi LL^{-1}Y^TAYL^{-T}\\ 
  & =(Y_{\perp_M}^T(I+Y(Y^TY)^{-1}Y^T)\eta_{\uparrow_Y}(Y^TY)-2Y_{\perp_M}^TAYS_\xi Y^TMY)L^{-T}. 
\end{align*}
Let $L^{-1}Y^TAYL^{-T}=Q\Lambda Q^T$ be the eigenvalue decomposition. We have 
 \begin{align*} 
  &Y_{\perp_M}^T AZ_\xi LQ+Y_{\perp_M}^TMZ_\xi LQ\Lambda  \\
  & = (Y_{\perp_M}^T(I+Y(Y^TY)^{-1}Y^T)\eta_{\uparrow_Y}(Y^TY)-2Y_{\perp_M}^TAYS_\xi Y^TMY)L^{-T}Q. 
  \end{align*}
 Let $\tilde{Z}_\xi=Z_\xi LQ$, then $Y^TM\tilde{Z}_\xi=Y^TMY_{\perp_M} K_\xi LQ=0$ and we have
  \begin{align*} 
    & Y_{\perp_M}^TA\tilde{Z}_\xi + Y_{\perp_M}^T M\tilde{Z}_\xi\Lambda = (Y_{\perp_M}^T(I+Y(Y^TY)^{-1}Y^T)\eta_{\uparrow_Y}(Y^TY)-2Y_{\perp_M}^TAYS_\xi Y^TMY)L^{-T}Q,
  \end{align*} 
that is, 
\begin{align} \label{e04}
  (I-\hat{V}\hat{V}^T)(A\tilde{Z}_\xi+M\tilde{Z}_\xi\Lambda)=(I-\hat{V}\hat{V}^T)((I+Y(Y^TY)^{-1}Y^T)\eta_{\uparrow_Y}(Y^TY)-2AYS_\xi Y^TMY)L^{-T}Q, 
\end{align}
where $\hat{V}=\mathrm{orthonormal}(MY)=MYG$.
Using $\tilde{Z}_\xi(:,i)$ to denote the $i$-th column of $\tilde{Z}_\xi$, we have the saddle-point problems from~\eqref{e04} as follows 
\begin{equation}
  \begin{aligned} \label{Precond-Metric1-SadProd1}    
    \begin{bmatrix} A+\lambda_iM &\hat{V}\\ \hat{V}^T & 0 \end{bmatrix} \begin{bmatrix} \tilde{Z}_\xi(:,i)\\y\end{bmatrix}=\begin{bmatrix} F_{\eta,\xi}(:,i) \\ 0 \end{bmatrix}
  \end{aligned}
\end{equation}
where $\Lambda = \mathrm{diag}\{\lambda_1,\cdots,\lambda_p\}$, $y\in\mathbb{R}^p$, $F_{\eta,\xi}=(I-\hat{V}\hat{V}^T)((I+Y(Y^TY)^{-1}Y^T)\eta_{\uparrow_Y}(Y^TY)-2AYS_\xi Y^TMY)L^{-T}Q$, and the second equation holds due to $\hat{V}^T\tilde{Z}_\xi=G^TY^TMY_{\perp_M} K_\xi LQ=0$. Therefore, if we denote the solution of the $i$-th saddle-point problem, corresponding to (\ref{Precond-Metric1-SadProd1}), with right-hand side $F$ by $\mathcal{T}_i^{-1}(F)$, then (\ref{Precond-Metric1-SadProd1}) can be write as 
\begin{equation}
  \begin{aligned} \label{Precond-Metric1-SadProdSol}
    \tilde{Z}_\xi(:,i) &=\mathcal{T}_i^{-1}\left((I-\hat{V}\hat{V}^T)(I+Y(Y^TY)^{-1}Y^T)\eta_{\uparrow_Y}(Y^TY)L^{-1}Q(:,i)\right) \\ 
    & \quad - \mathcal{T}_i^{-1}\left((I-\hat{V}\hat{V}^T)2AY\right)L^{-T}Q\tilde{S}_\xi(:,i),
  \end{aligned}
\end{equation}
where $\tilde{S}_\xi=Q^TL^TS_\xi LQ$. 

Plugging $\tilde{S}_\xi=Q^TL^TS_\xi LQ$ into (\ref{Precond-Metric1-1}), we obtain 
\begin{align} \label{e05}
  2\Lambda\tilde{S}_\xi+2\tilde{S}_\xi\Lambda+Q^TL^{-1}Y^TA\tilde{Z}_\xi+\tilde{Z}_\xi^TAYL^{-T}Q=2Q^TL^{-1}Y^T\eta_Y(Y^TY)L^{-T}Q.
\end{align}
Let 
\begin{align*}
  v_i&=Q^TL^{-1}Y^TA\mathcal{T}_i^{-1}\left( (I-\hat{V}\hat{V}^T)(I+Y(Y^TY)^{-1}Y^T)\eta_Y(Y^TY)L^{-T}Q(:,i)\right), \\ w_i&=Q^TL^{-1}Y^TA\mathcal{T}_i^{-1}\left( (I-\hat{V}\hat{V}^T)2AYL^{-T}Q\right)\tilde{S}_\xi(:,i).
\end{align*}
It follows from~\eqref{e05} that
\begin{align*}
  2\Lambda \tilde{S}_\xi+2\tilde{S}_\xi\Lambda + \begin{bmatrix} v_1-w_1&\cdots&v_k-w_k \end{bmatrix} + \begin{bmatrix} (v_1-w_1)^T \\ \cdots\\(v_k-w_k)^T \end{bmatrix}=2Q^TL^{-1}Y^T\eta_Y(Y^TY)L^{-T}Q,
\end{align*}
that is, 
\begin{align} \label{Precond-Metric1-S}
  (\mathcal{K}+\Pi\mathcal{K}\Pi)\mathrm{vec}(\tilde{S}_\xi)=\mathrm{vec}(R),
\end{align}
where $\mathcal{K}=\mathrm{diag}(K_i)\in\mathbb{R}^{p^2\times p^2}$, $K_i=2\lambda_iI-Q^TL^{-1}Y^TA\mathcal{T}_i^{-1}\left( (I-\hat{V}\hat{V}^T)2AYL^{-T}Q\right)$,
$\Pi$ is the perfect shuttle matrix \cite{Loan2000TheUK} such that $\mathrm{vec}(X)=\Pi\mathrm{vec}(X)^T$ and $R=2Q^TL^{-1}Y^T\eta_{\uparrow_Y}(Y^TY)L^{-T}Q-[v_1,\cdots,v_k]-[v_1,\cdots,v_k]^T$.
To sum up, after solving (\ref{Precond-Metric1-S}), we obtain $\tilde{Z}_\xi$ from (\ref{Precond-Metric1-SadProdSol}). Therefore, the solution $\xi_Y$ of~\eqref{e03} is obtained.

Since $\xi_Y = \mathcal{P}_Y^{\mathrm{V}^1}(\xi_Y) + \mathcal{P}_Y^{\mathrm{H}^1} (\xi_Y)$ and $\mathcal{P}_Y^{\mathrm{V}^1} (\xi_Y)$ does not influence the left hand side of~\eqref{e03} because of $Y(\mathcal{P}_Y^{\mathrm{V}^1}(\xi_Y))^T+\mathcal{P}_Y^{\mathrm{V}^1}(\xi_Y)Y^T=0$, the solution to~\eqref{Precond-Metric1-ApproxNewEqu} is $\xi_{\uparrow_Y} = \mathcal{P}_Y^{\mathrm{H}^1} (\xi_Y)$. The final algorithm for solving~\eqref{Precond-Metric1-ApproxNewEqu} is stated in Algorithm \ref{Precond-Metric1-Alg}.

\begin{algorithm}[htbp]
  \caption{Preconditioner under Riemannian metric~\eqref{IngredQuotMani-Metric1}} 
  \begin{algorithmic}[1] \label{Precond-Metric1-Alg}
  \REQUIRE Matrices $A$ and $M$ and horizontal vector $\eta_{\uparrow_Y}\in\mathrm{H}_Y^1$;
  \ENSURE $\xi_{\uparrow_Y}$ satisfying (\ref{Precond-Metric1-ApproxNewEqu-1});
  \STATE Set $LL^T\gets Y^TMY$ (Cholesky factorization);
  \STATE Set $Q\Lambda Q^T\gets L^{-1}Y^TAYL^{-T}$ (Eigenvalues decomposition); 
  \STATE Set $\hat{V}\gets \mathrm{orthonormal}(MY)$;
  \STATE Set $v_i\gets Q^TL^{-1}Y^TA\mathcal{T}_i^{-1}\left( (I-\hat{V}\hat{V}^T)(I+Y(Y^TY)^{-1}Y^T)\eta_{\uparrow_Y}(Y^TY)L^{-T}Q(:,i)\right)$;
  \STATE Set $K_i\gets 2\lambda_iI-Q^TL^{-1}Y^TA\mathcal{T}_i^{-1}\left( (I-\hat{V}\hat{V}^T)2AYL^{-T}Q\right)$;
  \STATE Set $R\gets 2Q^TL^{-1}Y^T\eta_{\uparrow_Y}(Y^TY)L^{-T}Q-[v_1,\cdots,v_k]-[v_1,\cdots,v_k]^T$;
  \STATE Solve for $\tilde{S}_\xi$ by $(\mathcal{K}+\Pi\mathcal{K}\Pi)\mathrm{vec}(\tilde{S}_\xi)=\mathrm{vec}(R)$;
  \STATE Solve for $\tilde{Z}_\xi$ by  
  {\small
  \begin{align*}
  \tilde{Z}_\xi (:, i)&\gets \mathcal{T}_i^{-1}\left((I-\hat{V}\hat{V}^T)(I+Y(Y^TY)^{-1}Y^T)\eta_{\uparrow_Y}(Y^TY)L^{-1}Q(:,i)\right) \\ & \;\;\;\;\; - \mathcal{T}_i^{-1}\left((I-\hat{V}\hat{V}^T)2AY\right)L^{-T}Q\tilde{S}_\xi(:,i);
  \end{align*}
  }
  \STATE Set $Z_{\xi,1}\gets \frac{1}{2}Y\left( (Y^TY)^{-1}Y^T\tilde{Z}_\xi Q^TL^{-1} + L^{-T}Q\tilde{Z}_{\xi}^TY(Y^TY)^{-1}  \right)$;
  \STATE Set $Z_{\xi,2}\gets (I - Y(Y^TY)^{-1}Y^T\tilde{Z}_{\xi}Q^TL^{-1}$;
  \STATE Set $\xi_{\uparrow_Y}\gets YL^{-T}Q\tilde{S}_\xi Q^TL^{-1}+ Z_{\xi,1} + Z_{\xi,2}$.
  
 \end{algorithmic}
\end{algorithm}

For applying this preconditioner, the dominating costs lie in solving the saddle-point problems and solving the linear system (\ref{Precond-Metric1-S}). As for the saddle-point problems (\ref{Precond-Metric1-SadProd1}), we firstly can solve $\mathcal{T}_i(X_i)=B_i$ by eliminating the (negative) Schur complement $S_i=\hat{V}^T(A+\lambda_iM)^{-1}\hat{V}$ (see \cite{benzi_numerical_2005} for details), where $$ B_i=[(I-\hat{V}\hat{V}^T)(I+Y(Y^TY)^{-1}Y^T)\eta_{\uparrow_Y}(Y^TY)^{-1}L^{-1}Q(:,i),\;(I-\hat{V}\hat{V}^T)2AYL^{-1}Q].
$$ 
Therefore, we have
\begin{align*} 
v_i &= Q^{T}L^{-1}Y^TAX_i(:,1), \\ 
%&\whcomm{}{w_i= ...} \\
K_i &= 2\lambda_iI- Q^TL^{-1}Y^TA J_i(:,2:\text{end}),
\end{align*}
where $N_i = S_i^{-1}(\hat{V}^T(A+\lambda_iM)^{-1}B_i)$ and $J_i=(A+\lambda_iM)^{-1}B_i - (A+\lambda_iM)^{-1}\hat{V}N_i$. The linear system~\eqref{Precond-Metric1-S} therefore can be solved by the conjugate gradient method.

\subsubsection{Preconditioning under Riemannian Metrics~\eqref{IngredQuotMani-Metric2} and~\eqref{IngredQuotMani-Metric3}}

Similar to the approach in Section~\ref{sec:PreCon01}, the preconditioners for Riemannian metrics~\eqref{IngredQuotMani-Metric2} and~\eqref{IngredQuotMani-Metric3} respectively solves the equations
\begin{equation} \label{e06}
[A(Y\xi_{\uparrow_Y}^T+\xi_{\uparrow_Y}Y^T)M+M(Y\xi_{\uparrow_Y}^T+\xi_{\uparrow_Y}Y^T)A]Y=\eta_{\uparrow_Y}(Y^TY), \hbox{ for $\xi_{\uparrow_Y} \in \mathrm{H}_Y^2$ }
\end{equation}
and
\begin{equation} \label{e07}
  2[A(Y\xi_{\uparrow_Y}^T+\xi_{\uparrow_Y}Y^T)M+M(Y\xi_{\uparrow_Y}^T+\xi_{\uparrow_Y}Y^T)A]Y=\eta_{\uparrow_Y}, \hbox{ for $\xi_{\uparrow_Y} \in \mathrm{H}_Y^3$}.
\end{equation}
The derivations are analogous to those in Section~\ref{sec:PreCon01} and therefore are not repeated here. The algorithms for solving~\eqref{e06} and~\eqref{e06} are respectively stated in Algorithm~\ref{Precond-Metric2-Alg} and Algorithm~\ref{Precond-Metric3-Alg}. Note that in Step~\ref{Precond-Metric3-Sum-Sxi} and~\ref{Precond-Metric3-for-Omega} of Algorithm~\ref{Precond-Metric3-Alg}, two small-scale Sylvester equations of size $p\times p$ need be solved and can be done by using \textit{lyap} function in MATLAB. 


%Similar to (\ref{Precond-Metric1-ApproxNewEqu}), for the second metric (\ref{IngredQuotMani-Metric2}), we solve the following approximate Newton equation 
%\begin{align}
%  \nabla^2 h(YY^T)[Y\xi_{\uparrow_Y}^T+\xi_{\uparrow_Y}Y^T]Y(Y^TY)^{-1}=\eta_{\uparrow_Y}, \label{Precond-Metric2-ApproxNewEqu}
%\end{align}
%for $\xi_{\uparrow_Y}\in \mathrm{H}^2_Y$ with given $\eta_{\uparrow_Y} \in \mathrm{H}^2_Y$. Equation (\ref{Precond-Metric2-ApproxNewEqu}) is equivalent to 
%\[
%  \nabla^2h(YY^T)[Y\xi_{\uparrow_Y}^T+\xi_{\uparrow_Y}Y^T]Y=\eta_{\uparrow_Y}(Y^TY),
%\]
%that is, 
%\[
%  [A(Y\xi_{\uparrow_Y}^T+\xi_{\uparrow_Y}Y^T)M+M(Y\xi_{\uparrow_Y}^T+\xi_{\uparrow_Y}Y^T)A]Y=\eta_{\uparrow_Y}(Y^TY).
%  \]
%The deduction of the preconditioner under the second metric is similar to the one under the first metric. We just need to replace $(I+P_Y)$ into $I$. Due to the limited space, the preconditioning steps under metric (\ref{IngredQuotMani-Metric2}) are summarized as follows in Algorithm (\ref{Precond-Metric2-Alg}).

\begin{algorithm}[htbp]
  \caption{Preconditioner under Riemannian metric~\eqref{IngredQuotMani-Metric2}} 
  \begin{algorithmic}[1] \label{Precond-Metric2-Alg}
  \REQUIRE Matrices $A$ and $M$ and horizontal vector $\eta_{\uparrow_Y}\in\mathrm{H}_Y^2$;
  \ENSURE $\xi_{\uparrow_Y}$ satisfying~\eqref{e06};
  \STATE Set $LL^T\gets Y^TMY$ (Cholesky factorization);
  \STATE Set $Q\Lambda Q^T\gets L^{-1}Y^TAYL^{-T}$ (Eigenvalues decomposition); 
  \STATE Set $\hat{V}\gets \mathrm{orthonormal}(MY)$;
  \STATE Set $v_i\gets Q^TL^{-1}Y^TA\mathcal{T}_i^{-1}\left( (I-\hat{V}\hat{V}^T)\frac{1}{2}\eta_{\uparrow_Y}(Y^TY)L^{-T}Q(:,i)\right)$;
  \STATE Set $K_i\gets 2\lambda_iI-Q^TL^{-1}Y^TA\mathcal{T}_i^{-1}\left( (I-\hat{V}\hat{V}^T)2AYL^{-T}Q\right)$;
  \STATE Set $R\gets \frac{1}{2}Q^TL^{-1}Y^T\eta_{\uparrow_Y}(Y^TY)L^{-T}Q-[v_1,\cdots,v_k]-[v_1,\cdots,v_k]^T$;
  \STATE Solve for $\tilde{S}_\xi$ by $(\mathcal{K}+\Pi\mathcal{K}\Pi)\mathrm{vec}(\tilde{S}_\xi)=\mathrm{vec}(R)$;
  \STATE Solve for $\tilde{Z}_\xi$ by   
    \begin{small}
    \begin{align}  \nonumber
  	 \tilde{Z}_\xi (:, i)\gets \mathcal{T}_i^{-1}\left((I-\hat{V}\hat{V}^T)\frac{1}{2}\eta_{\uparrow_Y}(Y^TY)L^{-1}Q(:,i)\right) - \mathcal{T}_i^{-1}\left((I-\hat{V}\hat{V}^T)2AYL^{-T}Q\right)\tilde{S}_\xi(:,i);
    \end{align}
  	\end{small}  
  \STATE Set $Z_{\xi,1}\gets \frac{1}{2}Y\left( (Y^TY)^{-1}Y^T\tilde{Z}_\xi Q^TL^{-1} + L^{-T}Q\tilde{Z}_{\xi}^TY(Y^TY)^{-1}  \right)$;
  \STATE Set $Z_{\xi,2}\gets (I - Y(Y^TY)^{-1}Y^T\tilde{Z}_{\xi}Q^TL^{-1}$;
  \STATE Set $\xi_{\uparrow_Y}\gets YL^{-T}Q\tilde{S}_\xi Q^TL^{-1} + Z_{\xi,1} + Z_{\xi,2}$.
  
 \end{algorithmic}
\end{algorithm}
%
%
%\subsubsection{Preconditioning under the Third Metric (\ref{IngredQuotMani-Metric3})}
%
%As for the third metric (\ref{IngredQuotMani-Metric3}), similarly, we solve the approximate Newton equation: given a horizontal vector $\eta_{\uparrow_Y}\in\mathrm{H}_Y^3$, find $\xi_{\uparrow_Y}\in\mathrm{H}^3_Y$ such that 
%\begin{align}
%  2\nabla^2h(YY^T)[Y\xi_{\uparrow_Y}^T+\xi_{\uparrow_Y}Y^T]Y=\eta_{\uparrow_Y}, \label{Precond-Metric3-ApproxNewEqu}
%\end{align}
%that is, 
%\[
%  2[A(Y\xi_{\uparrow_Y}^T+\xi_{\uparrow_Y}Y^T)M+M(Y\xi_{\uparrow_Y}^T+\xi_{\uparrow_Y}Y^T)A]Y=\eta_{\uparrow_Y}.
%  \]
%The derivation process of preconditioner under the third metric (\ref{IngredQuotMani-Metric3}) is similar to that under the first metric (\ref{IngredQuotMani-Metric1}), but the main steps are listed in Algorithm \ref{Precond-Metric3-Alg}. 
\begin{algorithm}[htbp]
  \caption{Preconditioner under Riemannian metric~\eqref{IngredQuotMani-Metric3}} 
  \begin{algorithmic}[1] \label{Precond-Metric3-Alg}
  \REQUIRE Matrices $A$ and $M$ and horizontal vector $\eta_{\uparrow_Y}\in\mathrm{H}_Y^3$;
  \ENSURE $\xi_{\uparrow_Y}$ satisfying~\eqref{e07};
  \STATE Set $LL^T\gets Y^TMY$ (Cholesky factorization);
  \STATE Set $Q\Lambda Q^T \gets L^{-1}Y^TAYL^{-T}$ (Eigenvalues decomposition); 
  \STATE Set $\hat{V}\gets \mathrm{orthonormal}(MY)$;
  \STATE Set $v_i\gets Q^TL^{-1}Y^TA\mathcal{T}_i^{-1}\left( (I-\hat{V}\hat{V}^T)\frac{1}{2}\eta_{\uparrow_Y}L^{-T}Q(:,i)\right)$;
  \STATE Set $K_i\gets 2\lambda_iI-Q^TL^{-1}Y^TA\mathcal{T}_i^{-1}\left( (I-\hat{V}\hat{V}^T)2AYL^{-T}Q\right)$;
  \STATE Set $R\gets \frac{1}{2}Q^TL^{-1}Y^T\eta_{\uparrow_Y}L^{-T}Q-[v_1,\cdots,v_k]-[v_1,\cdots,v_k]^T$;
  \STATE Solve for $\tilde{S}_\xi$ by $(\mathcal{K}+\Pi\mathcal{K}\Pi)\mathrm{vec}(\tilde{S}_\xi)=\mathrm{vec}(R)$;
  \STATE Solve for $\tilde{Z}_\xi$ by 
  \begin{small}  	  
  \begin{align*} \nonumber
	  \tilde{Z}_\xi (:, i)\gets \mathcal{T}_i^{-1}\left((I-\hat{V}\hat{V}^T)\frac{1}{2}\eta_{\uparrow_Y}L^{-1}Q(:,i)\right) - \mathcal{T}_i^{-1}\left((I-\hat{V}\hat{V}^T)2AYL^{-T}Q\right)\tilde{S}_\xi(:,i);
  \end{align*}
  \end{small}
  \STATE Solve for $S_\xi$ by $S_\xi(Y^TY)^{-1}+(Y^TY)^{-1}S_\xi=2L^{-T}Q\tilde{S}_\xi Q^TL^{-1}$; \label{Precond-Metric3-Sum-Sxi}
  \STATE Solve for $\Omega$ by $\Omega Y^TY + Y^TY \Omega = Y^T\tilde{Z}_{\xi}Q^TL^{-1} - L^{-T}Q\tilde{Z}_{\xi}^TY$; \label{Precond-Metric3-for-Omega}
  \STATE Set $\xi_{\uparrow_Y}\gets Y(Y^TY)^{-1}S_\xi+\tilde{Z}_\xi Q^{T}L^{-1} - Y\Omega$. 
  
 \end{algorithmic}
\end{algorithm}

%It is notable that in Step \ref{Precond-Metric3-Sum-Sxi} and \ref{Precond-Metric3-for-Omega}, we need solve two small-scale Sylvester equations of size $p\times p$ and it is done by using $lyap$ function in MATLAB. 

%\whcomm{==============================}{}

\section{Numerical Experiments} \label{NumExp} 

%\whcomm{[ZHTODO: be more specific.]}{
%In this section, our purpose is to demonstrate the performance of the proposed algorithms with some numerical tests. }

In this section, we illustrate the performance of the proposed algorithms with three examples. In the first example, we explore the influence of the three considered Riemannian metrics on Algorithm~\ref{RieNew-TruncatedNewton} and demonstrate the gain of the proposed preconditioners for Algorithm~\ref{RieNew-TrunConjGrad}. In the second example, we investigate the quality of the solutions of Algorithm~\ref{FinalAlgPrecond-RLyap-RNewton} compared with some state-of-the-art low-rank methods. In the third example, we report on the performance of Algorithm~\ref{FinalAlgPrecond-RLyap-RNewton} compared to three state-of-the-art low-rank methods for Lyapunov equations. 


\subsection{Testing Environment, Data, and Parameter Settings}

All experiments are done in MATLAB R2021b on a 64-bit GNU/Linux platform with 2.10 GHz CPU (Intel Xeon Gold 5318Y). The Riemannian optimization methods used in this paper are implemented based on ROPTLIB~\cite{HAGH18}. %The code
%, a C++ package for Riemannian optimization with interfaces to MATLAB and Julia\footnote{The code is available at www}.

In the first example, we generate two data, by randomly constructing and discretizing the finite difference of $2D$ poisson problem on the square, as listed respectively in Listing~\ref{code1} and Listing~\ref{code2}. In remaining two examples, the generalized Lyapunov equation is drawn from a RAIL benchmark problem\footnote{The data are available at \href{https://www.cise.ufl.edu/research/sparse/matrices/list\_by\_id.html}{https://www.cise.ufl.edu/research/sparse/matrices/list\_by\_id.html} with different dimensions.} stemmed from a semidiscretized heat transfer problem for optimal cooling of steel profiles \cite{Benner2005ASH, Saak2004EfficientNS}. 

The elements of the initial iterate $Y_0$ are drawn from the standard normal distribution. In Algorithm~\ref{RieNew-TruncatedNewton}, the step sizes satisfy the Armijo-Goldstein conditions and are found by interpolation-based backtracking algorithm~\cite{DS1983}. The default parameters in ROPTLIB are used.
In Algorithm~\ref{FinalAlgPrecond-RLyap-RNewton}, the tolerance for the inner iteration is chosen adaptively by $\tau_p= \min(10^{-6}, r_p / 10)$. %, where $exponent(a)$ means the exponential part of $a$. 

{\scriptsize\begin{lstlisting}[language=MATLAB,caption=random data,label=code1]
    O = orth(randn(n, n)); D = diag(logspace(-2, 2, n) .* (0.5 + rand(1, n)));
    A = O' * D * O;
    O = orth(randn(n, n)); D = diag(logspace(0, 0.5, n));
    M = O' * D * O;
    c = randn(n, 1);
    C = c * c';
\end{lstlisting}}

{\scriptsize\begin{lstlisting}[language=MATLAB,caption=the finite difference discretized $2D$ poisson problem on the square,label=code2]
	h = 1 / (n + 1);
	A = 1 / (h * h) * spdiags(ones(n, 1) * [-1 2 1], [-1 0 1], n, n)
	M = spdiags([rand(n - 1, 1); 0] + 0.1 , 0, n, n);
	c = randn(n, 1);
	C = c * c';
\end{lstlisting}}



%\whcomm{
%\begin{itemize}
%	\item Parameters
%	\item Data
%	\item Running environment
%	\item Codes link.
%\end{itemize}
%}{}

\subsection{Influence of Metrics and Preconditioners}

Algorithm \ref{RieNew-TruncatedNewton} is tested on random data generated from Listing \ref{code1} with 20 random seeds and also on the designed data generated from Listing \ref{code2} with 20 random seeds. The results are reported in Table \ref{NumExp-table1} and Table \ref{NumExp-table2}. 


\begin{table}[htbp]
\caption{An average result of 20 random runnings under three metrics. Indices are explained as follows: ``success'', ``iter'', ``nf'', ``ng'', ``nH'', ``time'' and ``gfgf0'' respectively refer to the number of successes in 20 runnings in the sense that ``gfgf0'' is not greater than the tolorance, the number of iterations, the number of evaluation of cost, the number of computing gradient, the number of the action of Hessian, the running time and the norm of final gradient over the norm of initial gradient. The subscript $-k$ indicates a scale of $10^{-k}$.}	
\centering 
\setlength{\tabcolsep}{1.1pt}
{\scriptsize
\begin{tabular}{l|l|l|l|l|l|l|l|l|l|l|l|l} \hline \multicolumn{1}{l|}{\multirow{3}{*}{RNewton}} & \multicolumn{6}{c|}{$n=500, p=2$}                                                                                                                                                             & \multicolumn{6}{c}{$n=1000,p=2$}                                                                                                                                                              \\ \cline{2-13} 
\multicolumn{1}{l|}{}                        & \multicolumn{3}{c|}{non-preconditioner}                                                       & \multicolumn{3}{c|}{preconditioner}                                                           & \multicolumn{3}{c|}{non-preconditioner}                                                       & \multicolumn{3}{c}{preconditioner}                                                            \\ \cline{2-13} 
\multicolumn{1}{l|}{}                        & \multicolumn{1}{l|}{metric 1} & \multicolumn{1}{l|}{metric 2} & \multicolumn{1}{l|}{metric 3} & \multicolumn{1}{l|}{metric 1} & \multicolumn{1}{l|}{metric 2} & \multicolumn{1}{l|}{metric 3} & \multicolumn{1}{l|}{metric 1} & \multicolumn{1}{l|}{metric 2} & \multicolumn{1}{l|}{metric 3} & \multicolumn{1}{l|}{metric 1} & \multicolumn{1}{l|}{metric 2} & \multicolumn{1}{l}{metric 3} \\ \hline
 success &      20 &      20 &      20 &      20 &      20 &      20 &      20 &      20 &      20 &      20 &      20 &      20 \\ 
    iter &      43 &      71 &      45 &      21 &      26 &      21 &      43 &      65 &      40 &      18 &      29 &      19 \\ 
      nf &      53 &      84 &      54 &      24 &      29 &      25 &      52 &      79 &      48 &      21 &      34 &      22 \\ 
      ng &      44 &      72 &      46 &      22 &      27 &      22 &      44 &      66 &      41 &      19 &      30 &      20 \\ 
      nH &    2361 &    2140 &    3576 &      57 &     339 &     206 &    2611 &    2307 &    3515 &      46 &     413 &     219 \\ 
    time & 3.56 & 3.28 & 5.27 & 1.21 & 7.15 & 4.37 & $1.35_{1}$ & $1.20_{1}$ & $1.79_{1}$& 5.09 & $4.16_{1}$ & $2.18_{1}$ \\ 
   gfgf0 & $3.00_{-9}$ & $3.97_{-9}$ & $4.12_{-9}$ & $2.11_{-9}$ & $4.50_{-9}$ & $3.39_{-9}$ & $3.40_{-9}$ & $2.48_{-9}$ & $4.39_{-9}$ & $1.41_{-9}$ & $4.64_{-9}$ & $4.05_{-9}$ \\  \hline
\end{tabular}
}
\label{NumExp-table1}
\end{table}

From Table \ref{NumExp-table1}, we can see that in most cases, Algorithm \ref{RieNew-TruncatedNewton} under Metric $g^1$ requires the least number of iterations, and the corresponding number of function calculations and gradient calls are also the least. When the preconditioners are not used, Algorithm \ref{RieNew-TruncatedNewton} calls Hessian least under Metric $g^2$, followed by Metric $g^1$, and most under Metric $g^3$, which is consistent with the total running time. If the preconditioners are used, the situation is quite different. Specifically, the number of Hessian calls (nH) in Algorithm \ref{RieNew-TruncatedNewton} is greatly reduced under the three metrics, that is, ``nH'' decreased at least by 97\%, 82\%, and 93\% compared with the ones without preconditioners under Metric $g^1$, $g^2$, and $g^3$ respectively. Though the numbers of actions of Hessian evaluations are reduced significantly, the computational times may not be reduced due to the extra cost of the preconditioners. Among the three metrics, only the preconditioner of Metric $g^1$ reduces the computational time significantly.

\begin{table}[htbp]
\caption{An average result of 20 runnings on designed data under three metrics. The subscript $-k$ indicates a scale of $10^{-k}$.}	
\centering
\setlength{\tabcolsep}{1.7pt}
{\scriptsize
\begin{tabular}{l|l|l|l|l|l|l|l|l|l|l|l|l} \hline \multicolumn{1}{l|}{\multirow{3}{*}{RNewton}} & \multicolumn{6}{c|}{$n=4000, p=3$}                                                                                                                                                             & \multicolumn{6}{c}{$n=40000,p=3$}                                                                                                                                                              \\ \cline{2-13} 
\multicolumn{1}{l|}{}                        & \multicolumn{3}{c|}{non-preconditioner}                                                       & \multicolumn{3}{c|}{preconditioner}                                                           & \multicolumn{3}{c|}{non-preconditioner}                                                       & \multicolumn{3}{c}{preconditioner}                                                            \\ \cline{2-13} 
\multicolumn{1}{l|}{}                        & \multicolumn{1}{l|}{metric 1} & \multicolumn{1}{l|}{metric 2} & \multicolumn{1}{l|}{metric 3} & \multicolumn{1}{l|}{metric 1} & \multicolumn{1}{l|}{metric 2} & \multicolumn{1}{l|}{metric 3} & \multicolumn{1}{l|}{metric 1} & \multicolumn{1}{l|}{metric 2} & \multicolumn{1}{l|}{metric 3} & \multicolumn{1}{l|}{metric 1} & \multicolumn{1}{l|}{metric 2} & \multicolumn{1}{l}{metric 3} \\ \hline
  success &      20 &      20 &      20 &      20 &      20 &      20 &      20 &      20 &      18 &      20 &      20 &      20 \\ 
    iter &      13 &      41 &      53 &       2 &       8 &       6 &      10 &      35 &      63 &       1 &       7 &       5 \\ 
      nf &      15 &      49 &      67 &       3 &       9 &       7 &      12 &      43 &      80 &       2 &       8 &       6 \\ 
      ng &      14 &      42 &      54 &       3 &       9 &       7 &      11 &      36 &      64 &       2 &       8 &       6 \\ 
      nH &     723 &     602 &     287 &       2 &      22 &       9 &     550 &     536 &     314 &       1 &      16 &       6 \\ 
    time & 1.07 & $9.50_{-1}$ & $4.24_{-1}$ & $1.75_{-2}$ & $1.70_{-1}$ & $6.43_{-2}$ & 7.96 & 7.67 & 3.94 & $6.98_{-2}$ & $9.89_{-1}$ & $3.33_{-1}$ \\ 
   gfgf0 & $8.03_{-9}$ & $8.00_{-9}$ & $7.78_{-9}$ & $7.04_{-9}$ & $6.97_{-9}$ & $4.43_{-9}$ & $6.30_{-9}$ & $7.32_{-9}$ & $4.53_{-9}$ & $1.02_{-10}$ & $3.65_{-9}$ & $2.56_{-9}$ \\ 
 \hline
\end{tabular}
} 
\label{NumExp-table2}
\end{table}


Table \ref{NumExp-table2} shows that when the preconditioners are not used, the running time and ``nH'' under Metric $g^3$ is the least, which is different from the results in Table \ref{NumExp-table1}. However, for $n=4000$, Algorithm~\ref{RieNew-TruncatedNewton} under Metric $g^3$ only ran successfully 18 times, which indicates that its robustness is weaker than the rest two. Note that the preconditioner with Metric $g^1$ outperforms all the other combinations of preconditioners and metrics. Moreover, such a combination only requires one or two iterations. One of the reasons may be that the preconditioner of Metric $g^1$ approximates the linear system~\eqref{Intro-LyapMatEqu} well such that the next iterate is close to the solution.

%, the truncated-Newton step make the initial iterate closer to the local minimizer.

Overall, it is observed from~Table~\ref{NumExp-table1} and~Table~\ref{NumExp-table2} that Algorithm~\ref{RieNew-TruncatedNewton} with the preconditioner under Metric $g^1$ is mostly superior to the other two metrics. Therefore, in the following experiments, only Metric $g^1$ is considered.


%From the overall perspective, Algorithm \ref{RieNew-TruncatedNewton} under Metric $g^1$ performs comprehensively best. Therefore, in the following experiments, we only use Metric $g^1$.


\subsection{Comparison with Existing Low-Rank Methods} \label{NumExp-CompWithExistingSols}


In this section, Algorithm~\ref{FinalAlgPrecond-RLyap-RNewton}, called LRRLayp, is compared with  three existing state-of-the-art low-rank methods for Lyapunov equations, including K-PIK from~\cite{simoncini_new_2007}, mess\_lradi from M-M.E.S.S, a MATLAB toobox~\cite{SaaKB21-mmess-2.2}, and RLayp from~\cite{Bart10}. These methods are respectively based on Krylov subspace techniques, alternating direction implicit iterative, and Riemannian optimization. Since, in Algorithm~\ref{FinalAlgPrecond-RLyap-RNewton}, other Riemannian methods can be used to solve the subproblem, we further use the Riemannian trust-region Newton method in~\cite{Absil2007TrustRegionMO} and the resulting algorithm is denoted by IRRLyap-RTRNewton. Algorithm~\ref{FinalAlgPrecond-RLyap-RNewton} with Algorithm~\ref{RieNew-TruncatedNewton} is denoted by IRRLyap-RNewton. As a reference, the notation ``best low rank'' with rank $p$ denotes a best low-rank approximation by the truncated singular value decomposition to the exact solution of~\eqref{Intro-LyapMatEqu}.


\paragraph{Quality of low-rank solutions.}
%First, we compare the quality of the low-rank solutions from IRRLyap(RTRNewton), IRRLyap(RNewton), K-PIK, and mess\_lradi. 
%The aim is to investigate that how does the accuracy of low-rank solutions change with the increase of rank. 
The generalized Lyapunov equation was drawn from a RAIL benchmark problem with the coefficient matrix of size $n=1357$. For the sake of simplicity, the right-hand side is taken as $C = B(:,1)B(:,1)^T$. Figure \ref{NumExp-QualityofLowRank} shows the experiment results.

\begin{figure}[htbp]
	\centering
	\includegraphics[scale=0.25]{./QualityofLowRankSol.pdf}
	\caption{The relative residual for one-rank right-hand-side RAIL benchmark with $n=1357$.} 
	\label{NumExp-QualityofLowRank}
\end{figure}

%\whcomm{[ZHTODO: update the following paragraph. Be specific and precise.]}{}

Let us compare the performance of the tested methods with the reference method--``best low rank''.
It can be seen from Figure~\ref{NumExp-QualityofLowRank} that 
under the same rank, the relative residuals of the low-rank approximations from K-PIK and mess\_lradi are notably greater than those from ``best low rank''. Therefore, mess\_lradi and K-PIK are not preferred if lower-rank solutions are desired.
%In addition, the relative residuals by mess\_lradi remains almost the same for a few steps and then have a notable decreases. The relative residuals of K-PIK decreases with a constant slope.
When the relative residual is greater than $10^{-6}$, that is, highly accurate solutions are not needed, the solutions from IRRLyap-RTRNewton and IRRLyap-RNewton are close in the sense of the relative residual. This is because IRRLyap-RTRNewton and IRRLyap-RNewton solve the same optimization problem. However, when the relative residual is smaller than $10^{-6}$, the relative residuals from IRRLyap-RTRNewton begin to fluctuate without falling. We find that for this problem, RTRNewton is more sensitive to numerical error than RNewton in the sense that RTRNewon sometimes terminates before reaching the stopping criterion. Such behavior of RTRNewton prevents it from finding highly accurate solutions. Therefore, we conclude that IRRLyap-RNewton is preferable compared to IRRLyap-RTRNewton.
The solutions found by IRRLyap-RNewton even have smaller relative residuals than those from ``best low rank''. This is not surprising since ``best low rank'' uses truncated SVD and completely ignore the original problem whereas IRRLyap-RNewton aims to find a stationary point, i.e., minimizes the Riemannian gradient---the residual in the horizontal space, see~Proposition~\ref{IngredQuotMani-Prop-Gradient}. Overall, IRRLyap-RNewton is able to find the best solution compared to the tested methods in the sense of the relative residual.

%the performance of IRRLyap(RNewton) is similar to that of ``best low rank'', even slightly better than that of ``best low rank''.

%One of the reasons is that in each iteration IRRLyap(RNewton) solves a fixed rank subproblem which make the relative residual smaller. It is worth noting that in IRRLyap(RNewtom) there is no high requirement for accuracy in the inner subproblem. For instance, $\tau_p=10^{-2}$ is used in this case. 


%We observe, in Figure \ref{NumExp-QualityofLowRank}, that in each step the low-rank approximation from K-PIK are far way from optimal. However, the numerical results of mess\_lradi are between IRRLyap(RNewton) and K-PIK and it seems to be numerically unstable. For lower accuracy, e.g., $\tau \approx 10^{-6}$, the approximations by IRRLyap(RTRNewton) and IRRLyap(RNewton) are very close in a sense of the relative residual. However, as the accuracy is increasing, because of numerical error IRRLyap(RTRNewton) begins to fluctuate, which is one of the reasons why we use Newton's method in Algorithm~\ref{RieNew-TruncatedNewton} for the fixed-rank subproblem(i.e., solving Problem \ref{Pro_stat-FinalProb}). IRRLyap(RNewton) achieves the same accuracy ($\tau \approx 10^{-12}$) as "best low rank". Even RLyap-RNewton can reach lower relative residual than ``best low rank'', in that in each iteration IRRLyap(RNewton) solves a fixed rank subproblem which make the relative residual smaller. It is worth noting that the in IRRLyap(RNewton) there is no high requirements for accuracy in the inner subproblem. For instance, $\tau_p=10^{-2}$ is used in this case. 

\paragraph{Efficiency and performance.}
K-PIK, mess\_lradi, RLyap, and IRRLyap-RNewton are compared by using the RAIL benchmark problems with size $n=5177,20209,79841$. The stopping criterion for all methods are unified as $\|R\|_F=\|AXM+MXA - C\|_F  \le \tau \cdot \|C\|_F$ with a tolerance $\tau = 10^{-6}$. 
The results are reported in Table~\ref{NumExp-Table3}.

%Second, we compared RLyap-RNewton with the state-of-the-art low-rank Lyapunov solvers such as K-PIK solver from \cite{simoncini_new_2007}, mess\_lradi solver from the M-M.E.S.S. package \cite{SaaKB21-mmess-2.2} and RLyap solver\footnote{The code is based on ROPTLIB package and its preconditioner is derived under assuming $M=I$.} from \cite{Bart10}, which are based on Krylov subspaces projection technology, alternating direction implicit iteration and Riemannian optimization respectively. The RAIL benchmark problem is used here with $n=5177,20209,79841$. 

%\whcomm{[ZHTODO: solvers versus methods (algorithms)]}{}

\begin{table}[htbp]
\caption{Comparison for the simplified RAIL benchmark with existing methods. ``rank'', ``time'', ``rel\_res'' and ``numSys'' denote the rank of the approximation, running time, the relative residual of the approximation and the number of solving shift systems $(A+\lambda M)X=B$ for $X$ with given $A,\lambda, M$ and $B$. The subscript $-k$ indicates a scale of $10^{-k}$.}
\centering
\setlength{\tabcolsep}{2.5pt}
{\scriptsize
\begin{tabular}{l|llll|llll|llll}
\hline
\multirow{2}{*}{} & rank & times(s.)   & rel\_res    & numSys & rank & times(s.)   & rel\_res    & numSys & rank & times(s.)   & rel\_res    & numSys \\ \cline{2-13} 
                  & \multicolumn{4}{c|}{$n=5177$}             & \multicolumn{4}{c|}{$n=20209$}            & \multicolumn{4}{c}{$n=79841$}             \\ \hline
K-PIK             & 63   & 3.41        & $1.46_{-6}$ & 64     & 91   & $4.44_{1}$  & $2.65_{-6}$ & 92     & 122  & $5.06_{2}$  & $4.39_{-6}$  & 123    \\
mess\_lradi       & 32   & $1.57_{-1}$ & $1.47_{-7}$ & 64     & 37   & $8.65_{-1}$ & $5.90_{-7}$ & 74     & 38   & 3.85        & $6.12_{-8}$ & 76     \\
RLyap             & 22   & $1.42_{2}$  & $4.92_{-7}$ & 15784  & 27   & $1.06_{3}$  & $2.25_{-7}$ & 23060  & 27   & $6.09_{3}$  & $8.58_{-7}$ & 29481  \\
IRRLyap(RNewton)     & 22   & 5.70        & $6.94_{-7}$ & 588    & 27   & $4.29_{1}$  & $3.38_{-7}$ & 841    & 27   & $2.56_{2}$ & $5.10_{-7}$ & 1100   \\ \hline
\end{tabular}
}
\label{NumExp-Table3}
\end{table}

All the methods stop by satisfying the stopping criterion except K-PIK. Therefore, K-PIK has difficulty finding highly accurate solutions. Moreover, K-PIK needs to use a higher rank to reach a similar residual compared to other methods. Therefore, K-PIK is not preferred. 
The method mess\_lradi is the most efficient algorithm in the sense of computational time. However, it also requires a higher rank for similar accuracy when compared to RLyap and LRRLyap-RNewton. Thus, mess\_lradi is preferred if one has strict requirements on efficiency but not on rank.
Both RLyap and IRRLyap-RNewton are Riemannian optimization approaches and therefore the ranks and residuals of solutions found by them are similar. However, the preconditioner used in RLyap does not take $M \neq I$ into consideration. It follows that RLyap requires solving more shift systems and therefore is less efficient. Overall, we suggest using IRRLyap-RNewton if one does not have strict restrictions on efficiency and desires as low-rank solutions as possible.


%From Table \ref{NumExp-Table3}, we observe that the optimization solvers (including RLyap and IRRLyap(RNewton)) give the lowest rank approximation compared with other solvers under the same tolerance. 
%However K-PIK stagnates before the relative residual meets stopping criterion. 
%From the perspective of running time, mess\_lradi takes the least time to execute. With the increase of size $n$, IRRLyap(RNewton) takes less time than K-PIK, while RLyap takes the longest time. The running time is mainly consumed in solving the shift systems $(A+\lambda M)X=B$ for $X$ with given $A,\lambda, M$ and $B$. Besides, it is visible that K-PIK, RLyap and IRRLyap(RNewton) require to solve a host of shift systems which make their running time more longer than mess\_lradi. It is mentioned that the reason why RLyap takes more time than IRRLyap(RNewton) is because the preconditioner of RLyap is provided with setting $M=I$, which make the preconditioner do not approximate Hessian well. 


\paragraph{A hybrid method.} Since mess\_lradi is significantly faster than IRRLyap-RNewton but gives a lower quality solution in the sense of the residual. We, therefore, propose a hybrid method by first using mess\_lradi to generate a rough solution and improving its accuracy by IRRLyap-RNewton. The hybrid method is denoted by lradi\_IRRLyap and the comparisons with IRRLyap-RNewton are given in Table~\ref{NumExp-Table4}. It can be seen that lradi-IRRLyap is a method between mess\_lradi and IRRLyap-RNewton in the sense that its computational time is smaller than IRRLyap-RNewton and its rank is smaller than mess\_lradi.

%quickly provides an approximation with higher rank than solvers based on Riemannian optimization, hence we can use it to generate a coarse approximation and then let this approximation be the initial guess of Algorithm~\ref{RieNew-TruncatedNewton}. Based on this idea, we obtain a mixed method of mess\_lradi and Algorithm \ref{RieNew-TruncatedNewton}, called lradi\_IRRLyap. The results reported in Table \ref{NumExp-Table4} show that lradi\_IRRLyap generate an approximation with lower rank compared with mess\_lradi and its execution time is greater reduced in comparison with IRRLyap(RNewton). 

\begin{table}[htbp]
\caption{Comparisons for the simplified RAIL benchmark among mess\_lradi, lradi-IRRLyap, and IRRLyap-RNewton. The subscript $-k$ indicates a scale of $10^{-k}$.}
\centering
\setlength{\tabcolsep}{2.5pt}
{\scriptsize
\begin{tabular}{l|llll|llll|llll}
\hline
\multirow{2}{*}{} & rank & times(s.) & rel\_res    & numSys & rank & times(s.)  & rel\_res    & numSys & rank & times(s.)  & rel\_res    & numSys \\ \cline{2-13} 
                  & \multicolumn{4}{c|}{$n=5177$}           & \multicolumn{4}{c|}{$n=20209$}           & \multicolumn{4}{c}{$n=79841$}            \\ \hline
mess\_lradi       & 32   & $1.57_{-1}$ & $1.47_{-7}$ & 64     & 37   & $8.65_{-1}$ & $5.90_{-7}$ & 74     & 38   & 3.85        & $6.12_{-8}$ & 76     \\
lradi-IRRLyap      & 26   & 2.41      & $6.31_{-7}$ & 156    & 32   & 7.18       & $2.28_{-7}$ & 96     & 32   & $1.18_{2}$ & $8.05_{-7}$ & 384    \\
IRRLyap-RNewton     & 22   & 5.70        & $6.94_{-7}$ & 588    & 27   & $4.29_{1}$  & $3.38_{-7}$ & 841    & 27   & $2.56_{2}$ & $5.10_{-7}$ & 1100   \\ \hline
\end{tabular}
}
\label{NumExp-Table4}
\end{table}


% 数值实验的设计需要好好思考。需要猜测一下，读者读了算法方法后，最希望了解这个算法的哪些方面。然后针对性的根据这些方面设计可靠，可信，可重复的测试。
% 为了保证实验的可重复性，最好把参数数据细节等等写完整，把相关代码挂到网上方便大家使用和检验。实验跑好后的汇报部分不能只是简单描述结果，最好需要给出一些引申意义。

\section{Conclusions} \label{Concl}

In this paper, we have generalized the truncated Newton's method from Euclidean spaces to Riemannian manifolds, called Riemannian truncated-Newton's method, and shown the convergence results, e.g., global convergence and local superlinear convergence. Moreover, the cost function from~\cite{Bart10} is reformulated as an optimization problem defined on the Riemannian quotient manifold $\mathbb{R}^{n\times p}/\mathcal{O}_p$. An algorithm, called IRRLyap-RNewton, is proposed and is used to find a low-rank approximation of the generalized Lyapunov equations. We investigate three Riemannian metrics on $\mathbb{R}^{n\times p}/\mathcal{O}_p$ and develop new preconditioners that take $M \neq I$ into consideration. The numerical results show that the new preconditioners significantly reduce the number of actions of Riemannian Hessian evaluations even when $M \neq I$. In addition, IRRLyap-RNewton is able to find a similar accurate solution with the lowest rank compared to some state-of-the-art methods, including K-PIK, mess\_lradi, and RLyap. %On the other hand, even though RLyap-RNewton performs very well with preconditioning, there is still the possibility of further speedup. For example, efficiently solving the shifted systems will significantly reduce the running time.


\appendix





\bibliographystyle{alpha}
\bibliography{references}

%\section{Appendix}



\end{document}


