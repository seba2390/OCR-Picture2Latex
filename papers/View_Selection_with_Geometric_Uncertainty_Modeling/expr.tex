\section{Evaluation}

\begin{table*}[t]
\caption{The comparison of average reprojection error and reconstruction time for the two experiments}
\centering
\begin{tabular}{|c||c|c|c||c|c|c|}
\hline
 & Original Frames & Avg Reprj Err & SFM time (min) &  Camera Grid Frames & Avg Reprj Err & SFM time (min)\\
\hline
Orchard: 30 meters Flight & 416 & 0.842 & 313.6 & 76 & 0.934 & 4.1\\ 
\hline
Orchard: 10 meters Flight & 375 & 0.724 & 374.7 & 84 & 0.842 & 4.4\\ 
\hline
 & Original Frames & Avg Reprj Err & Dense Recon (min) &  Multi-Resol Method & Avg Reprj Err & Dense Recon (min)\\
\hline
Orchard: 30 meters Flight & 875 & 0.863 & 1463 & 209 & 0.931 & 115\\ 
\hline
Orchard: 10 meters Flight & 893 & 0.944 & 1522 & 266 & 1.243 & 167\\ 
\hline
\end{tabular}
\label{tab:reconT}
\end{table*}
% \begin{table*}[t]
% \caption{The comparison of average reprojection error and reconstruction time for dense reconstruction}
% \centering
% \begin{tabular}{|c||c|c|c||c|c|c|}
% \hline
%  & Original Frames & Avg Reprj Err & Dense Recon (min) &  Coarse-to-Fine Approach & Avg Reprj Err & Dense Recon (min)\\
% \hline
% Orchard: 30 meters Flight & 875 & 0.863 & 1463 & 209 & 0.931 & 115\\ 
% \hline
% Orchard: 10 meters Flight & 893 & 0.944 & 1522 & 266 & 1.243 & 167\\ 
% \hline
% \end{tabular}
% \label{tab:denseReconT}
% \end{table*}

In this section, we present simulation results used for validating the uncertainty model and results followed by a real-world reconstruction performance using the coarse to fine view selection method.

\subsection{Simulations}
We used the following parameters of a GOPRO HERO 3 for simulations.
Resolution: $1920 \times 1080$, Field of view: $120^\circ \times 70^\circ$.
The calibration error in pixels was  $[0.2061, 0.2183]$. 
For all simulations we used an iMac with $3.3$GHz quad-core Intel Core i5 and $16$GB of RAM. 

{\em Model justification:}
We consider the following sources of uncertainty: finite resolution, calibration errors, camera center location, camera orientation.

\begin{figure}
\centering
	\includegraphics[width=0.5\textwidth]{fig/hist.pdf}
	\caption{(a)~Distribution of  $\frac{|\hat{g}_2-g|}{|\hat{g}_\infty-g|}$ (b)~Histogram of the error: $|\hat{g}-g|$ with the following noise parameters: $|n_p| \leq 10$,$|n_s| \leq 0.1h$, $|n_\theta|\leq 1^\circ$}
	\label{fig:hist}
\end{figure}

The first two sources are less than one pixel. To investigate the role of camera orientation, we perturbed camera location
$\hat{s} = s + n_s$, where $s$ is the true location and $n_s$ is a uniform noise, and camera pose $\hat{\theta} = \theta + n_\theta$ where $\theta$ is the true orientation and $n_\theta $ is a uniform noise. Figure~\ref{fig:hist} (b) reports the result of triangulation error from two cameras in an optimal position. The height of the viewing plane was set to $10$m. The noise is set to $|n_p| \leq 10$, $|n_s| \leq 0.1h$, and $|n_\theta| \leq 1^\circ$. Each simulation was repeated $10^5$ times where the target location $\hat{g}$  is computed by triangulation and the error $|\hat{g}-g|$ is reported. Various noise levels are shown in the captions.
If we choose a bound of 10 pixels for the measurement error, it corresponds to  $\alpha < 0.1$ rad. The solid red line shows the predicted worst case error using our model. In general, reprojection error will be less than 10 pixels, otherwise it will be discarded as outliers. The state-of-the-art SLAM~\cite{zhang2015visual} algorithm's performance can go up to $0.0014 \ deg/m$ therefore, we set the camera position error to be less than $10 \%$ of the height while bounding the orientation error to be less than $1^\circ$. 
The histogram shows that the distance to the true target location is well bounded by the worst case uncertainty which is indicated as the vertical red line. It means that our uncertainty cone model can be relatively robust to system noises. 

Next, we study the effect of using two cameras vs. all cameras. 
We estimate the target pose using least squares from all cameras and report the ratio:
$\frac{|\hat{g}_2-g|}{|\hat{g}_\infty-g|}$ is plotted in Fig~\ref{fig:hist} (a). Here, $\hat{g}_2$ is the estimated target location using the optimal pair while $\hat{g}_\infty$ uses all the cameras. The simulation was repeated $10^4$ times. 
The ratio in Fig~\ref{fig:hist} (a) is less than 3.5, which means that using the optimal pair of cameras to triangulate the target is at most 3.5 times worse than triangulation using all cameras. This is because that the triangulation error using two or all camera views can be considered as a random process. Using only two camera views does not restrict the target as rigorous as using more all views, therefore, imposing at most 3.5 ratio of target position error.

\subsection{Real Experiment}
We collected two data sets using a GOPRO HERO 3 with a UAV flying over the same region with different height. The altitude ranges from 10 meters to 30 meters whereas the covered areas range between planar to more general orchard scenes. The orchard contains trees that are around 3 meters tall and ground elevation difference around 1 meter. We recorded around 5 minutes of videos, which is roughly 10000 frames. In order to speed up the reconstruction, we extracted every $30^{th}$ frames of the videos for mosaicking, which results in around 400 frames.
We used the commercial AgiSoft software for Structure from Motion for dense reconstruction and mosaicking to investigate the effect of view selection on reconstruction quality and reprojection error.

\subsubsection{Mosaic Quality}
\begin{figure}
\centering
	\includegraphics[width=0.4\textwidth]{fig/mosaic_results.png}
	\caption{Select a subset of the original frames using the camera grid: reduces frames from $\sim$300 to $\sim$50 with comparable mosaic quality.}
\label{fig:mosaic}
\end{figure} 

We use the original selected frames for reconstruction and mosaicking~\cite{zhengqi}. Then, we use grid resolution of $\delta_d = h$ as shown in Figure~\ref{fig:mosaic} to select a subset of the frames for reconstruction and mosaicking. This means that if the drone is 10 meters above the ground plane, we select camera frames every 10 meters, which significantly reduces the number of cameras required. 
The total time required to reconstruct the same region decreased significantly while the reprojection error of each reconstruction remains low as shown in Table~\ref{tab:reconT}. For qualitative evaluation,  we stitched the images together by using the output pose from SFM and orthorectifying the views to compare the quality of the final mosaic. The resulting views are comparable, indicating that the proposed view selection mechanism does indeed perform comparable with respect to the original input set as shown in Figures~\ref{fig:mosaic}.

\subsubsection{Dense Reconstruction Quality}
We also examine the performance of the multi-resolution camera grid approach at the orchard data sets. 
For dense reconstruction, such data sets should be considered as a general scene and they cannot be treated as planar region, otherwise, features on different height cannot be covered.
We first use ORB-SLAM~\cite{murORB2} to extract camera poses and sparse point clouds. Since the point clouds still contains many inconsistent points, a filter is applied to remove noisy points too far from the surroundings. Then a mesh is built upon those points with maximum of 10,000 faces. We extracted the visibility cones of the mesh with the given trajectory and sampled a coarse-to-fine camera view grid in the same trajectory. The original data sets last around 5 minutes and contains more than 9000 images. Using the key frame selection method from ORB-SLAM, more than 3000 images are selected for reconstruction. It is unfeasible due to computational limitations. Therefore, we selected every $10^{th}$ frames with a total of around 900 frames. 
As shown in Figure~\ref{fig:viewselection}, the view selection algorithm selected a relatively sparser views in flat regions comparing to the densely packed views in more complex regions. The view selection algorithm will terminate when at least $95\%$ of the surfaces are covered. Therefore, there are still a few meshes that cannot be visible to the view subsets in the last iteration. The initial grid spacing is set to the height between the camera view plane and dominant ground plane: $\delta_d = h$. 
The reconstruction time and reprojection error comparison is shown in Table~\ref{tab:reconT}. It is clear that the computational time decreased by more than a magnitude and while the reprojection error does not increase too much. Essentially, our multi-resolution approach takes the scene geometry into consideration and removes redundant views that does not contribute much to the results.  
Visually, we can see that the dense reconstruction quality is very comparable shown in Figure~\ref{fig:denseComp30} taken from 30 meters above and in Figure~\ref{fig:denseComp10} taken from 10 meters above. Both results show that the reconstruction quality are almost identical.   
There is also an interesting observation: it is \emph{not} necessarily beneficial to have as many as views possible for dense reconstruction. As shown in Figure~\ref{fig:denseComp30} (a), more views actually smooth out the distinct geometry of the trees, leaving edges blending into each other. At a lower altitude, as shown in Figure~\ref{fig:denseComp10}, the dense reconstruction results are almost indistinguishable. 
