\section{Sensor Selection For the Entire Scene}
In the previous section, we established that for a world point $g$, the optimal pair of cameras can produce a reconstruction with approximation ratio less than $\sqrt{\frac{1+2\alpha}{1-4\alpha}}$ of the optimal reconstruction (Theorem~\ref{thrm:opt2cameras3dbound}).
However, if we use the dedicated pair directly for every scene point, we may end up choosing two cameras for each scene point, which in turn might result in a large number of cameras.
 \begin{figure}[h]
 \centering
 	\includegraphics[width=0.4\textwidth]{fig/camgrid_groundVariation.pdf}
 	\caption{(a) The square sensor grid in 3D (b) Square sensor grid in 2D with ground point variation.}
 	\label{fig:grid}
 \end{figure}
 
In this section, we show that a coarse grid of cameras provide a good reconstruction for every scene point.
Recall that $\mathcal{G}$ is the ground plane, $\mathcal{S}$ is the view plane,   $\mathcal{G}$ is parallel to  $\mathcal{S}$ and the distance between them is $h$. 
Let $\mathcal{\overline{S}}$ be a square grid imposed on $\mathcal{S}$ with resolution $\delta_d$ (Fig~\ref{fig:grid} (a)). The same grid $\mathcal{\overline{G}}$ is also imposed on the ground plane $\mathcal{G}$. 
To demonstrate the main strategy at a high-level, consider a ground point $g \in \mathcal{G}$, such that the optimal pair of cameras lies in camera grid $\mathcal{\overline{S}}$. We will show that the optimal pair of cameras can still provide ``good" reconstruction for all points in a region $R(g)$ around $g$. 

Using the result we will show in Theorem~\ref{thrm:grid3d2} that a constant number of cameras for a ground plane can be used to achieve a small approximation ratio.

%In this section, we make this rough argument concrete. Specifically, we show that 
%with the choice of $\delta_d = h$, we achieve a \ctxt{$2$} approximation ratio for uncertainty everywhere using 
%$|\mathcal{\overline{S}}| = \lceil \frac{9Area(\mathcal{G})}{h^2} \rceil$ cameras.
\subsection{Problem 2 in 2D}
For cameras in the grid $s \in \overline{S}$ and target $g \in G$, we define the grid uncertainty $\overline{\varepsilon}(g)$ using only the best two cameras in grid $\overline{S}$ as the following
$$
\overline{\varepsilon}(g) = \min_{s_i, s_j \in \overline{S}}\varepsilon(g,\{s_i,s_j\}) 
$$
As mentioned earlier, we will choose the grid resolution to be $\delta_d = h$ for the following analysis.

Now, we define the geometry for Lemmas~\ref{lem:grid2d1}, \ref{lem:grid2d2}, ~\ref{lem:grid2dhori}, and \ref{lem:grid2dvert}. 
Let $g \in \overline{G}$ be a grid location with height $h$ to the viewing plane $S$. Now, we choose the optimal pair of cameras for the target $g$ as $\{s_p,s_q\} \in \overline{S}$ as shown in Fig~\ref{fig:grid} (b). Let $l$ be a line passing through $g$ with $l \perp G$ and $x = l \cap Cone(s)$, where $x$ is the intersection line segments between $l$ and the Cone generated by sensor $s$ and target $g$. 

In order to bound the uncertainty of any target $\forall g \in G$ using the camera grid $\overline{S}$, we need to explore the uncertainty of the targets in grid cells (Fig~\ref{fig:grid} (b)). Therefore, we fix a grid point and define a range of targets $R(g) = [g-\delta_d/2, g+\delta_d/2]$ such that $R(g)$ is generated by moving $g \in \overline{G}$ along the $x$-axis of the grid. We now show that the worst case uncertainty is achieved at the end points of this interval (i.e. the midpoint of two grid locations) bound by $max(||x_p||, ||x_q||)$, where $||x||$ represents the length of line segment of $x$. We define $diag_1 = \overline{ac}$ and $diag_2 = \overline{bd}$ in Fig~\ref{fig:u3d2d}.
%We first define the geometry for the following properties.
%Let $g \in \overline{G}$ be a grid location and $R(g) = [g-\delta_d/2, g+\delta_d/2]$ be a range of points with the optimal pair of cameras $\{s_p,s_q\}$ and orientation $\theta_p,\theta_q$ respectively. The grid resolution is defined as $\delta_d = h$. The intersection between $Cone_i, Cone_j$ is defined as polygon $Q_{ij}$ with diagonal $diag_1,diag_2$. Let $x_i,x_j$ be the distance of the intersection between the vertical line passing through target $g$ and both half-line of the viewing wedge $s_i,x_j$ of target $g$ respectively. 

\begin{lemma}\label{lem:grid2d1}
When $\theta_p + \theta_q \geq \frac{\pi}{2} + \alpha$, $diag_1 > diag_2$.  
\end{lemma}

%\begin{proof}
%We will add two more line segments $\overline{aa'}$ and $\overline{cc'}$ to generate a isosceles trapezoid $aa'cc'$ as shown in Fig~\ref{fig:u3d2d}. We can see that $||\overline{ac}|| = ||\overline{a'c'}|| > ||\overline{bd}||$. Therefore, $diag_1 > diag_2$ when $\theta_i + \theta_j \geq \frac{\pi}{2} + \alpha$.  
%\end{proof}



\begin{lemma}\label{lem:grid2d2}
$\theta_p + \theta_q$ is maximized when the inner half-plane of both cones intersect $g^* = g \pm  \delta_d/2$. 
\end{lemma}


%\begin{lemma}\label{lem:grid2d3}
%$\max(||x_i||,||x_j||) > diag_1$.
%\end{lemma}

%\begin{lemma}\label{lem:maxtheta}
%For any $g' \in R(g) = [g-\delta_d/2, g+\delta_d/2]$, $\max(||x_i||,||x_j||)$ is achieved when the inner half-plane of $Cone_i$ and $Cone_j$ intersects $g' = g \pm \delta_d/2$.
%\end{lemma}
It is clear that either $||x_p||$ or $||x_q||$ is always larger or equal to $diag_1$, which can be used to generate the worst case bound.
\begin{theorem}\label{thrm:grid3d1}
For all targets $g \in G$ and sensor grid $\overline{S}$ with resolution $\delta_d = h$, the worst case grid uncertainty $\overline{\varepsilon}(g)$ using only two cameras from $\overline{S}$ is bounded as follows
$$
\overline{\varepsilon}(g) \leqslant 1.72 \varepsilon_\infty
$$
\end{theorem}



\subsection{Relaxing planar scene and viewing plane assumptions}
% in camera positions and ground point planes in 2D}
So far, our analyses of the uncertainty bound are based on the parallel plane assumptions. Such assumptions are reasonable for some applications such as high altitude aerial imagery. 

In this section, we relax these assumptions so that the theorem can be applied to more general environments. Define horizontal and vertical variation as $\lambda_vh, \lambda_hh$, where $0 < \lambda_v, \lambda_h < 1$.
We will analyze the change in $\overline{\varepsilon}(g)$ when adding variation in both horizontal and vertical directions.
The new camera location $\hat{s}$ is generated by perturbing $s$  by $\lambda_vh,\lambda_hh$ amount in vertical and horizontal directions.
We analyze both effects from vertical and horizontal variations in Appendix~\ref{sec:appendix} and get the following results. 

\begin{theorem}\label{thrm:gridU2d}
For all targets $g \in G$ and sensor grid $\overline{S}$ with resolution $\delta_d = h$ and variation $\lambda_v,\lambda_h$, the worst case grid uncertainty $\overline{\varepsilon}(g)$ using only two cameras from $\overline{S}$ is bounded as follows
$$
\overline{\varepsilon}(g) \leqslant 1.72\frac{1+\lambda_v}{1-\lambda_h} \varepsilon_\infty
$$
\end{theorem}
\begin{proof}
The result can be derived by combining Lemma~\ref{lem:grid2dhori}, Lemma~\ref{lem:grid2dvert} and Theorem~\ref{thrm:grid3d1}.
\end{proof}

We can see that small deviation from the camera position or the ground plane does not introduce significant uncertainty. 

\subsection{Problem 2 in 3D}

In 3D, we use the same grid resolution $\delta_d = h$ which is half of the distance between the optimal pair of cameras. The main result is
\begin{theorem}\label{thrm:grid3d2}
For all targets $g \in \mathcal{G}$ and sensor grid $\overline{S}$ with resolution $\delta_d = h$ and variation $\lambda_v,\lambda_h$, the worst case grid uncertainty $\overline{\varepsilon}(g)$ using only two cameras from $\overline{S}$ is bounded as follows
$$
\overline{\varepsilon}(g) \leqslant 2.47\frac{1+\lambda_v}{1-\lambda_h} \varepsilon_\infty
$$
\end{theorem}

The proof is similar to the 2D case. It is extended to include perturbations in both $x$ and $y$ directions which slightly increases the bounds, which is shown in Appendix~\ref{sec:appendix} Figure~\ref{fig:grid3dv2}.

%\begin{figure}[h]
%\centering
%	\includegraphics[width=0.5\textwidth]{fig/camgridtopdown2.png}
%	\caption{Camera grid: top down view}
%	\label{fig:camgridtopdown}
%\end{figure} 

\begin{figure}[h]
\centering
	\includegraphics[width=0.2\textwidth]{fig/grid3dv2.png}
	\caption{Camera grid in 3D: $g$ is perturbed to $g^*$ to achieve worst case uncertainty.}
	\label{fig:grid3dv2}
\end{figure} 

Theorem~\ref{thrm:grid3d2} allows us to bound the geometric error even in the presence of variations in both viewing and scene planes. However, it does not address visibility: variations in the scene can cause occlusions which can block camera views. In the next section, we address this issue. 