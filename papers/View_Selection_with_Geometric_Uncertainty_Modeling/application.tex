\section{Multi-Resolution View Selection}
In this section, we explore how to extend our previous camera view grid approach to non-planar regions such as orchards and forests. The parallel plane assumption can produce good results with high altitude, but will be insufficient to model non-planar regions. For this purpose, we propose a multi-resolution approach, which generates multiple camera view grids in a coarse to fine manner, to reconstruct more general regions. 

The input to our method is a surface mesh generated using sparse points clouds from a SLAM method such as  ORB-SLAM~\cite{murORB2}. It then outputs a subset of the views such that each face of the mesh can be \emph{well-covered}, that is, covered by at least 3 cameras separated by the current grid resolution. To ensure coverage quality, we double the grid resolution at each iteration so that the minimum distance between cameras is bounded. We present the details in Section~\ref{sec:viewselection}.

As the scene becomes more complex, the multi-resolution approach is able to adapt the terrain. For a given grid resolution, we iterate through all triangles and if they are well-covered by the current subset of views, those views will be added to the solution. However, the potential views that can see the triangle are limited due to occlusion and matching quality. Therefore, we introduce a visibility cone for each triangle in Section~\ref{sec:mesh}  to limit the search space.  

Similar to~\cite{krause2014submodular} and~\cite{hoppe2012photogrammetric}, we also generate scene meshes to reason about the geometry. The main difference of our work is that first, we do not require a secondary visit to the scene. The existing trajectory of views can be sufficient enough to cover the environment in most cases.  Second, we generalize the visibility for each triangle mesh such that well-covered views can be predicted instead of histogram method ~\cite{hoppe2012photogrammetric} that is strongly case sensitive. 

\subsection{Visibility Cone}
\label{sec:mesh}

\begin{figure}
    \centering
    \includegraphics[width=0.4\textwidth]{fig/visibilityCone.jpg}
    \caption{The visibility cone generated from visible cameras}
    \label{fig:visibilitycone}
\end{figure}

A camera is defined to be visible to a triangle mesh when it contains 2D feature of a point around the mesh. A viewing vector for a triangle is defined as the vector pointing from the center of the triangle to the corresponding camera as shown in Figure~\ref{fig:visibilitycone}. 
The mesh vector is then the average of all viewing vectors for that triangle mesh. We also define the visibility angle of each triangle as the average angle between all viewing vector. 
We can therefore predict the visibility of a triangle using both the visibility angle and the mesh vector. Essentially, we generate a visibility cone, where the direction of the cone is the mesh vector and the aperture is the visibility angle. We do not consider the effects of viewing angles since all the views are assumed to be facing downwards, which can be easily maintained with a gimbal stabilizer. 
Unlike the approaches from ~\cite{hoppe2012photogrammetric} that extract the histogram for each mesh triangle, we bound the region of possible visible camera views using the mesh visibility.

\subsection{Coarse to Fine View Selection}
\label{sec:viewselection}
After identifying the visibility cone for each triangle, we utilize our previous proposal of the camera grid in a coarse to fine manner.
\begin{figure}
    \centering
    \includegraphics[width=0.2\textwidth]{fig/multireso-viewselect.jpg}
    \caption{Multi-resolution view selection for each triangular mesh, where the camera views (only in one level) intersect with the visibility cone are added to the solution.}
    \label{fig:multireso}
\end{figure}

\begin{algorithm}
\caption{View Selection. Let $M = \{m_1,m_2,...\}$ be all triangle meshes and $J = \{s_1,s_2,...\}$ be all camera poses from the trajectory. Let $\pi(m_i,J)$ be the function that output all cameras in $J$ that are within the visibility cone of  $m_i$.}
\begin{algorithmic}
\REQUIRE set initial grid resolution $R$, set solution $sol=[]$
\WHILE{ when $M$ is not empty}
    \STATE {Pick camera grid $S_R \subseteq J$ with spacing $R$}
    \FORALL{$m_i \in M$}
                \STATE{$S = S_R \bigcap sol$}
        \IF{$|\pi(m_i,S)| \geq 3$}
            \STATE{$sol = \pi(m_i,S) \bigcap sol$}
            \STATE{remove $m_i$ from $M$}
        \ENDIF
    \ENDFOR
    \STATE{$R = R/2$};
\ENDWHILE
\STATE{Output final selected views $sol$}
\end{algorithmic}
\end{algorithm}

For a given grid resolution, we iterate through all faces of the mesh and check their visibility cones against current subset of views. For each face, if the visibility cone contains at least 3 camera views from the current subset of views, then those views will be added to the solution as shown in Figure~\ref{fig:multireso}. Those faces covered by 3 or more cameras will not be considered in the next iteration. To ensure the quality of the selected views, we impose that for each face, there are at least 3 views visible to the mesh so that feature matching error can be reduced. Since we also increase the grid resolution by two fold for each iteration, the chosen views for a specific mesh guarantee a minimum spacing. Giving the grid spacing $R$ at the first iteration, after $k$ iterations, the minimum spacing between all views will be $\frac{1}{2^k}R$ instead of arbitrarily small spacing that reduces reconstruction quality. 
