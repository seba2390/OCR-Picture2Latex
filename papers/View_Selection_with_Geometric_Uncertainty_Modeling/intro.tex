\section{Introduction}
%Estimating positions of world points from features observed in images
%is a key problem which arises in image mosaicking, simultaneous
%localization and mapping and structure from motion.  
%As a motivating
%application we will refer to throughout the paper, 
Consider a scenario
where a plane flying at a fixed altitude is capturing images of a
ground plane below so as to reconstruct the scene (Figure~\ref{fig:denseComp10}).  Over the
course of its flight, the plane may capture thousands of
images which can easily overwhelm image reconstruction algorithms.
%\vtxt{For example, consider the ... (Our five min flight example)}
Our goal in this paper is to answer the question of whether we can select a
small number of images and focus only on them without reducing the
reconstruction quality. 

% \begin{figure}
% \centering
% 	\includegraphics[width=1\columnwidth]{fig/mosaic_results_3.png}
% \caption{Reconstruction using 416 (left mosaic) images obtained from 15 meters.
% We will show that it is possible to obtain the same quality with a much smaller number of images (84 images right mosaic). Note that the scene is not strictly planar as the trees are ~3 meters tall.}
% % A coarse grid was imposed to select 59 views from the images(right).}
% %The original 414 images SFM reconstruction with proposed grid}
% \label{fig:gridOrigin}
% \end{figure} 

\begin{figure}
\centering
	\includegraphics[width=1\columnwidth]{fig/view-selection-10meter.pdf}
	\caption{Comparison of dense reconstruction of the orchard from images taken at 10 meters altitude. (a) Dense Reconstruction using 893 images (b) Closeup view of the detailed reconstruction of the tree rows (c) Dense Reconstruction using 266 images extracted using our multi-resolution view selection method (d) Closeup view of the same tree row. }
\label{fig:denseComp10}
\end{figure} 

We first study a basic version where we focus on a single world point.
The goal is to select a small number of images from which the 3D
position of the world point can be accurately estimated
(Problem~\ref{prob:singlep}).  We then present a general version where
the goal is to minimize the error for the entire scene
(Problem~\ref{prob:multip}) from a small set of images. Note that in
the latter case, the same set of images must be used for every scene
point. We also extended our approach to a multi-resolution view selection 
scheme to accommodate non-planar scenes. 

In order to formalize these two problems, we first need to formalize
the error model and the uncertainty objective.  Let $g$ be a world point and $I$ be an
image taken from a camera at position $s$ and orientation $\theta$.
Let $p$ be the observed projection of $g$ onto $I$ and $p^*$ be the
unobserved true projection represented as vectors originating from the
camera center $s$. We will employ a {\em bounded uncertainty model}
where we will assume that the angle between $p$ and $p^*$ is bounded
by a known (or desired) quantity $\alpha$. 
Therefore, the 3D location
of the world point $g$ is contained inside a cone $C$ apexed at $s$
and with symmetry axis along $p$ and cone angle $2 \alpha$.
See Figure~\ref{fig:cone}.

\textit{Merging measurements:} In order to estimate the true location
of a world point from multiple measurements, we simply intersect the
corresponding cones. The diameter of the intersection is used as an
uncertainty measure. We chose diameter over the volume so as to avoid
degenerate cases where the intersection has almost zero volume but
large diameter which could still generate large triangulation error. 


\textit{Uncertainty as worst-case reconstruction error:} 
Rather than associating a single cone for a specific measurement, our formulation considers a possibly infinite set of viable cones for a given true camera pose and world point pair. To do this, we consider all possible perturbations of relevant quantities (projection, location or pose).
When merging measurements, we consider the worst-case scenario  which maximizes the reconstruction uncertainty. 
 This formulation gives us a deterministic worst-case error model. It also allows us to factor out unknown or uncontrollable quantities such as camera orientation.
 
 \begin{figure}
\centering
	\includegraphics[width=0.5\textwidth]{fig/view-selection.pdf}
	\caption{View Selection at Multiple resolution to cover the mesh region, where the color is the height and the white region is the covered region at each level: (a) View Selection at 3 resolution shown in blue, black and red. (b) View Selection at the Coarsest Level (c) View Selection at the Middle Level (d) View Selection at the Finest Level. Note that the coarser views cover partial planar region while the finer selection populates the more complex regions }
\label{fig:viewselection}
\end{figure} 
% The process can be visualized as a game, where for each true projection $p^*$ an adversary chooses a measurement $p$ in such a way that when the cones corresponding to those measurements are merged, the uncertainty is maximized.
% The same reasoning applies for orientation uncertainty as well. 
% This is important because
% the orientation of the  uncertainty cone $C$ depends on the orientation of the camera.
% In some cases, for example when the camera is mounted on an industrial
% manipulator, it might make sense to choose good viewpoints given by
% camera position and orientation. In most cases however, it is
% difficult to control the orientation with respect to world
% points. In fact, this would require accurately estimating the positions of the
% world points online (which is the problem we want to solve after all)
% in addition to being able to accurately control the camera
% pose (which is very hard in the presence of disturbances).
%Therefore, in this paper 
% We can treat the orientation as an uncontrolled external
% disturbance. The resulting estimation process can again be visualized
% as a new game where we choose the camera center locations but an
% adversary chooses the rotation of each camera and the observed
% projection of the feature so as to maximize the intersection
% area. Under a spherical camera model, these two choices can simply be
% added. Therefore the adversary can choose a single parameter. Of
% course, the choice must still ensure that the cone contains the true
% world point.

%\vtxt{revisit this to make notation consistent}
%Let us make these concepts more concrete by revisiting the example of
%a plane flying at a fixed altitude.  Consider the ground plane $\mathcal{G}$, a
%point $g \in \mathcal{G}$ and the viewing plane $\mathcal{V}$ assumed
%to be parallel to $\mathcal{G}$.  Suppose we take every possible
%measurement from $\mathcal{V}$ by choosing every point as a camera
%center. The adversary chooses the camera the measurements and
%orientations, we get corresponding cones, intersect them and obtain
%the diameter $\varepsilon_\infty(g)$: the minimum worst case
%uncertainty achievable to reconstruct $g$ under this model.  The
%subscript is chosen to emphasize that this quantity is obtained by
%merging possibly infinitely many views. Can we select only a few
%images upfront and still guarantee small deviation from
%$\varepsilon_\infty(g)$? Can we select views so as to provide the same
%guarantee for every such $g$?  In this paper, we show that for our
%motivating application, the answer is yes. 
%In addition to providing
%theoretical bounds, we show, in an image mosaicking application, how
%sensor selection can turn an overnight computation into one that can
%be solved within minutes without sacrificing the reconstruction
%quality. We start with an overview of related work.

%\textit{Our results can be summarized as follows:}
