\documentclass{journal}

% if you need to pass options to natbib, use, e.g.:
% \PassOptionsToPackage{numbers, compress}{natbib}
% before loading nips_2016
%
% to avoid loading the natbib package, add option nonatbib:
% \usepackage[nonatbib]{nips_2016}

%\usepackage{nips_2016}

% to compile a camera-ready version, add the [final] option, e.g.:

\usepackage{hyperref}
\usepackage{cleveref}




\usepackage{graphicx}
\usepackage{epstopdf}
\usepackage{epsfig}
\usepackage{cite,footnote,xspace,syntonly,algorithm,algorithmic,bm}


\usepackage{rotating}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
%\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{amsmath}
\usepackage{bbm}
\usepackage{bm}
\usepackage{amssymb}



\usepackage[final]{nips_2017}



% abbreviations
\def \cN {\mathcal{N}}
\def \cS {\mathcal{S}}
\def \cM {\mathcal{M}}
\def \cX {\mathcal{X}}
\def \cP {\mathcal{P}}
\def \cR {\mathcal{R}}
\def \cD {\mathcal{D}}
\def \cG {\mathcal{G}}
\def \cF {\mathcal{F}}


\def \bPhi {\boldsymbol{\Phi}}
\def \bTheta {\boldsymbol{\Theta}}

\def \by {\mathbf{y}}
\def \bx {\mathbf{x}}
\def \bz {\mathbf{z}}
\def \bu {\mathbf{u}}
\def \be {\mathbf{e}}
\def \bv {\mathbf{v}}

\def \bI {\mathbf{I}}
\def \bP {\mathbf{P}}
\def \bU {\mathbf{U}}
\def \bV {\mathbf{V}}
\def \bX {\mathbf{X}}


\title{Deep Generative Adversarial Networks for Compressed Sensing (GANCS) Automates MRI}


\author{\\{\it \large{Morteza Mardani$^{1,3}$, Enhao Gong$^{1}$, Joseph Y. Cheng$^{1,2}$, Shreyas Vasanawala$^{2}$,}} \\ \it{ \large{Greg Zaharchuk$^{2}$, Marcus  Alley$^{2}$, Neil Thakur$^{2}$, Song Han$^{4}$, William Dally$^{4}$,}} \\ {\it \large{John M. Pauly$^{1}$, and Lei Xing$^{1,3}$}} \thanks{The authors are with the Stanford University, Departments of Electrical Engineering$^{1}$, Radiology$^{2}$, Radiation Oncology$^{3}$, and Computer Science$^{4}$. } \\ }


% The \author macro works with any number of authors. There are two
% commands used to separate the names and addresses of multiple
% authors: \And and \AND.
%
% Using \And between authors leaves it to LaTeX to determine where to
% break the lines. Using \AND forces a line break at that point. So,
% if LaTeX puts 3 of 4 authors names on the first line, and the last
% on the second line, try using \AND instead of \And before the third
% author name.


\begin{document}
% \nipsfinalcopy is no longer used

\maketitle 

\begin{abstract}
Magnetic resonance image (MRI) reconstruction is a severely ill-posed linear inverse task demanding time and resource intensive computations that can substantially trade off {\it accuracy} for {\it speed} in real-time imaging. In addition, state-of-the-art compressed sensing (CS) analytics are not cognizant of the image {\it diagnostic quality}. To cope with these challenges we put forth a novel CS framework that permeates benefits from generative adversarial networks (GAN) to train a (low-dimensional) manifold of diagnostic-quality MR images from historical patients. Leveraging a mixture of least-squares (LS) GANs and pixel-wise $\ell_1$ cost, a deep residual network with skip connections is trained as the generator that learns to remove the {\it aliasing} artifacts by projecting onto the manifold. LSGAN learns the texture details, while $\ell_1$ controls the high-frequency noise. A multilayer convolutional neural network is then jointly trained based on diagnostic quality images to discriminate the projection quality. The test phase performs feed-forward propagation over the generator network that demands a very low computational overhead. Extensive evaluations are performed on a large contrast-enhanced MR dataset of pediatric patients. In particular, images rated based on expert radiologists corroborate that GANCS retrieves high contrast images with detailed texture relative to conventional CS, and pixel-wise schemes. In addition, it offers reconstruction under a few milliseconds, two orders of magnitude faster than state-of-the-art CS-MRI schemes. 
\end{abstract}





\section{Introduction}
%
Owing to its superb soft tissue contrast, magnetic resonance imaging (MRI) nowadays serves as the major imaging modality in clinical practice. Real-time MRI visualization is of paramount importance for diagnostic and therapeutic guidance for instance in next generation platforms for MR-guided, minimally invasive neurosurgery~\cite{clearpoint}. However, the scan is quite slow, taking several minutes to acquire clinically acceptable images. This becomes more pronounced for high-resolution and volumetric images. As a result, the acquisition typically undergoes significant undersampling leading reconstruction to a seriously ill-posed linear inverse problem. To render it well-posed, the conventional compressed-sensing (CS) incorporates the prior image information by means of sparsity regularization in a proper transform domain such as Wavelet (WV), or, Total Variation (TV); see e.g.,~\cite{pualy_mri20017}. This however demands running iterative optimization algorithms that are time and resource intensive. This in turn hinders {\it real-time} MRI visualization and analysis.


Recently, a few attempts have been carried out to {\it automate} medical image reconstruction by leveraging historical patient data; see e.g.,~\cite{Majumdar'15,lowdose_ct2017}. They train a network that maps the aliased image to the gold-standard one using convolutional neural networks (CNN) with residuals for computed tomography (CT)~\cite{lowdose_ct2017}, denoising auto-encoders for MRI~\cite{Majumdar'15}. Albeit, speed up, they suffer from blurry and aliasing artifacts. This is mainly due to adopting a pixel-wise $\ell_1$/$\ell_2$ cost that is oblivious of high-frequency texture details, which is crucial for drawing diagnostic decisions. See also the recent DeepADMM scheme in \cite{deepADMM2016} for CS MRI that improves the quality, but it is as slow as the conventional CS. Generative adversarial networks (GANs) have been lately proved very successful in~\cite{gan-goodfellow2014,dcgan2016} modeling a low-dimensional distribution (manifold) of natural images that are perceptually appealing~\cite{Zhu et al'16}. In particular, for image super-resolution tasks GANs achieve state-of-the-art perceptual quality under $4\times$ upscaling factor for natural images e.g., from ImageNet~\cite{leding et al'16, Sonderby et al'14}. GANs has also been deployed for image inpaitning~\cite{inpainting-yeh-2016}, style transfer~\cite{johnson2016}, and visual manipulation~\cite{Zhu et al'16}. 




Despite the success of GANs for {\it local} image restoration such as super-resolution and inpainting, to date, they have not been studied for removing {\it aliasing} artifacts in biomedical image reconstruction tasks. This is indeed a more difficult image restoration tasks. In essence, aliasing artifacts (e.g., in MRI) emanate from data undersampling in a different domain (e.g., Fourier, projections) which {\it globally} impact image pixels. Inspired by the high texture quality offered by GANs, and the high contrast of MR images, we employ GANs to learn a low-dimensional manifold of diagnostic-quality MR images. To this end, we train a tandem network of a generator (G) and a discriminator (D), where the generator aims to generate the ground-truth images from the complex-valued aliased ones using a deep residual network (ResNet) with skip connections, with refinement to ensure it is consistent with measurement (data consistency). The aliased input image is simply obtained via inverse Fourier Transform (FT) of undersampled data. D network then scores the G output, using a multilayer convolutional neural network (CNN) that scores one if the image is of diagnostic quality, and, zero if it contains artifacts. For training we adopt a mixture of LSGAN~\cite{lsgan2017} and $\ell_1$ pixel-wise criterion to retrieve high-frequency texture while controlling the noise. We performed evaluations on a large cohort of pediatric patients with contrast-enhanced abdominal images. The retrieved images are rated by expert radiologists for diagnostic quality. Our observations indicate that GANCS results have almost similar quality to the gold-standard fully-sampled images, and are superior in terms of diagnostic quality relative to the existing alternatives including conventional CS (e.g., TV and WV), $\ell_2$-, and $\ell_1$-based criteria. Moreover, the reconstruction only takes around $10-20$ msec, that is two orders of magnitude faster than state-of-the-art conventional CS toolboxes. 


Last but not least, the advocated GANCS scheme tailors inverse imaging tasks appearing in a wide range of applications with budgeted acquisition and reconstruction speed. All in all, relative to the past work this paper's main contributions are summarized as follows:


\begin{itemize}
	
	
	\item Propose GANCS as a data-driven regularization scheme for solving ill-posed linear inverse problems that appear in imaging tasks dealing with (global) aliasing artifacts 
	
	\item First work to apply GAN as a automated (non-iterative) technique for aliasing artifact suppression in MRI with state-of-the-art image diagnostic quality and reconstruction speed

	\item Proposed and evaluated a novel network architecture to achieve better trade-offs between data-consistency (affine projection) and manifold learning 
			
	\item Extensive evaluations on a large contrast-enhanced MRI dataset of pediatric patients, with the reconstructed images rated by expert radiologists
	
	
	
\end{itemize}





The rest of this paper is organized as follows. Section 2 states the problem. Manifold learning using LSGANs is proposed in Section 3. Section 4 also reports the data evaluations, while the conclusions are drawn in Section 5. 






%The sparsity assumption is rather universal and albeit its success for certain image types, it is oblivious of the inherent latent structures that are specific to each dataset. Apparently, including the patient dimension into sensing (MR acquisition) and characterizing the underlying low-dimensional structure of data one may afford significantly more compression (undersampling).


%Real-time MRI is however posed by two main challenges. First, the acquisition is slow due to physical limitations of the MR scanner, taking a few minutes to acquire data needed for a reasonable reconstruction. As a result the the acquisition typically undergoes significant undersampling that leads reconstruction to a seriously ill-posed linear inverse problem. Second, the reconstruction typically relies on non-smooth compressive sensing (CS) techniques that demand running resource-intensive iterative algorithms. These challenges become more pronounced for high-resolution and volumetric images.  
 


%In order to render the MR inverse problem well posed, the past work predominantly utilizes data for a single patient, and incorporates the prior image information by means of sparsity regularization in a proper transform domain such as Wavelet, or, Total Variation (TV); see e.g., [] and references therein. Sparsity is typically captured by solving non-smooth optimization programs that typically incur time intensive computations. 


%The sparsity assumption is rather universal and albeit its success for certain image types, it is oblivious of the inherent latent structures that are specific to each dataset. Apparently, including the patient dimension into sensing (MR acquisition) and characterizing the underlying low-dimensional structure of data one may afford significantly more compression (undersampling). 














 





%[Bora et al'17]: aims to perform compressive sampling when the unknown is (approximately) represented by a pre-trained multi-layer generative model ... The generative model is not specified; only a bound on the representation error is needed ... the unknown is modeled as G(z) for some low-dimensional z; and backpropagation used to solve the non-linear system y=AG(z). Reconstruction bounds are derived in terms of a variation of the restricted eigen-value condition .... no results are provided for the generalization, and this paper does not care about the training process ... 










\section{Problem Statement}
\label{sec:problem_statement}
%
Consider an ill-posed linear system $\by=\bPhi \bx + \bv$ with $\bPhi \in \mathbb{C}^{M \times N}$ where $M \ll N$, and $\bv$ captures the noise and unmodeled dynamics. Suppose the unknown and complex-valued image $\bx$ lies in a {\it low-dimensional} manifold, say $\cM$. No information is known about the manifold besides the training samples $\cX:=\{\bx_k\}_{k=1}^K$ drawn from it, and the corresponding (possibly) noisy observations $\mathcal{Y}:=\{\by_k\}_{k=1}^K$. Given a new observation $\by$, the goal is to recover $\bx$. For instance, in the MRI context motivated for this paper $\bPhi$ refers to the partial 2D FT that results in undersampled $k$-space data $\by$. To retrieve the image, in the first step we learn the manifold $\cM$. Subsequently, the second step projects the aliased image, obtained via e.g., pseudo inverse $\bPhi^{\dagger}\by$ onto $\cM$ to discard the artifacts. For the sake of generality, the ensuing is presented for a generic linear map $\bPhi$.
 
 
%\textbf{Image is complex-valued for instance in MRI ... }
 
 
%To this end, the first step is to learn a manifold $\cM$ that is possibly incoherent with the nullspace of the linear operator. With the manifold $\cM$ at hand, in the test phase $\bx$ is simply found by projecting the new observation $\by$ onto the manifold according to 
%
%\begin{align}
%\min_{\bx} ~~~ \|\by - \bPhi \bx\|^2 \quad \quad {\rm s.~to.}~~ \bx \in \cM
%\end{align}
%%
%This can be easily solved via gradient projection iterations as follows
%%
%\begin{align}
%\bx[k+1] =  ~~~ \cP_{\cM} \Big( \eta \bPhi^{\top} \by + (\bI_N - \eta \bPhi^{\top} \bPhi) \bx[k] \Big) \label{eq:iteration}
%\end{align}
%%
%where $\eta$ is the step size, and we choose the initial solution as $\bx[0]=\bPhi^{\top} \by$.




%Apparently, the iteration in \eqref{eq:iteration} resembles the so-called iterative soft thresholding algorithm when the manifold is the set of sparse vectors. From the compressive sensing theory it is known that the set of sparse solutions is incoherent with the null space of the linear operator, and as a result unique recovery is possible. However, sparsity assumption is not always the best way to capture the low-dimensionality, which presses the need for more sophisticated data-driven low-dimensional modeling schemes when one has access to historical data. Accordingly, learning the best low-dimensional manifold that is incoherent with the nullspace is an important question that is still open. \textbf{need to expand the discussion to sparsifying transformations and the corresponding prior distributions .... }



%In general, there may exist multiple solutions in the manifold that are consistent with the observations. Let $\cN$ be the null-space, for a true unknown $\bx$, the possible data consistent solutions are~$\cS_{\Phi}(\bx):= \{\bx+\bz ~ | ~ \bz \in \cN_{\Phi} \}$. In order to ensure unique solution we aim to learn the manifold $\cM$ that leads to $\cM \cap \cS_{\Phi}(\bx) = \bx$.  



\section{Manifold Learning via Generative Adversarial Networks}
\label{sec:gans}
%
The inverse imaging solution is to find solutions of the intersection between two subspaces defined by acquisition model and image manifold. In order to effectively learn the image manifold from the available (limited number of) training samples we first need to address the following important questions:


\begin{itemize}

\item How to ensure the trained manifold contains plausible images? 

\item How to ensure the points on the manifold are data consistent, namely $\by \approx \bPhi\bx, ~\forall \bx \in \cM$?

%\item How to ensure the abnormal images (e.g., patients with tumors) do not lie on the manifold?

%\item How to ensure the manifold is incoherent with the nullspace of linear operator, namely $\cM \cap \cS_{\Phi}(\bx) = \bx,~\bx \in \cX$? 

\end{itemize}
%
To address the first question we adopt GANs, that have recently proven very successful in estimating prior distribution for images. GANs provide sharp images that are visually plausible~\cite{gan-goodfellow2014}. In contrast, variational autoencoders~\cite{leding et al'16}, a important class of generative models, use pixel-wise MSE costs that results in high pick signal-to-noise ratios but often produce overly-smooth images that have poor perceptual quality. Standard GAN consists of a tandem network of G and D networks. Consider the undersampled image $\tilde{\bx}:=\bPhi^{\dagger}\by$ as the input to the G network. The G network then projects $\tilde{\bx}$ onto the low-dimensional manifold $\cM$ containing the high-quality images $\cX$. Let $\hat{\bx}$ denote the output of G, it then passes through the discriminator network D, that outputs one if $\hat{\bx} \in \cX$, and zero otherwise. 


The output of G, namely $\check{\bx}$, however may not be consistent with the data. To tackle this issue, we add another layer after G that projects onto the feasible set of $\by=\bPhi\bx$ to arrive at $\hat{\bx}=\bPhi^{\dagger} \by + (\bI-\bPhi^{\dagger}\bPhi) \check{\bx}$. Alternatively, we can add a soft LS penalty when training the G network, as will be seen later in (P1). To further ensure that $\hat{\bx}$ lies in the intersection of the manifold $\cM$ and the space of data consistent images we can use a mutlilayer network that alternates between residual units and data consistency projection as depicted in Fig.~\ref{fig:fig_net} (b). We have observed that using only a couple of residual units may improve the performance of G in discarding the aliasing artifacts. The overall network architecture is depicted in Fig.~\ref{fig:fig_net} (a), where $\mathbf{P}_{\cN}:=(\bI-\bPhi^{\dagger}\bPhi)$ signifies projection onto the nullspace of $\bPhi$. 



%Finally, the training data may contain abnormal cases which do not lie on the low-dimensional image manifold. For instance, in MRI a patient may have a tumor, or, a certain organ such as a kidney might be missing. The manifold is trained to only include the normal cases, while we model the abnormal (outlier) samples,as a superposition of a normal image on the manifold and a sparse component that accounts for abnormality. This is meaningful as a tumor typically involves a small subset of image pixels. This bears resemblance to low-rank plus sparse matrix decomposition ideas, which leverage the incoherence of low-rank and sparse matrices to separate them; see e.g.,~\cite{chandersekaran2011}.  


%Finally, to make the manifold incoherent with the nullspace (equivalently the artifacts), for each clean training sample, say $\bx_k$, we synthesize fake inputs $\bu_k=\bx_k + \bP_{\cN}\bz$, $\bz \sim \cN(0,\sigma^2)$, where $\bP_{\cN}:=(\bI-\bPhi^{\dagger}\bPhi)$ is the projection onto the nullspace. The additive term $\bP_{\cN}\bz$ represents the reconstruction artifacts. Apparently, samples with non-zero artifact are supposed to be orthogonal to the manifold, and as a result the corresponding $\hat{\bx}$ must be identical to zero. Note, in scenarios where the missing rows in the regression matrix $\bPhi$ are known such as in MRI, one can synthesize more structured aliasing-type artifacts. All in all, the overall network structure is depicted in Fig. 1.



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure}[t]
	\centering
	\hspace{-0.25cm}\includegraphics[scale=0.925]{network_structure_may19.png}
	\caption{(a) GANCS structure for manifold learning, where the dashed module is projection on the feasible set. (b) The multilayer residual blocks (RB) for data consistency. }
	\label{fig:fig_net}
\end{figure}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%




Training the network in Fig.~\ref{fig:fig_net} amounts to playing a game with conflicting objectives between the adversary G and the discriminator D. D network aims to score one the real images drawn from the data distribution $p_{x}$, and score zero the rest. G network also aims to map the input images $\tilde{\bx}=\bPhi^{\dagger}\by$ with the distribution $p_{\tilde{x}}=p_{x}(\bPhi^{\dagger}\bPhi \bx)$ to the fake images $\hat{\bx}$ that fool the D network. Various strategies have been devised to reach the equilibrium. They mostly differ in terms of the cost function adopted for the G and D networks~\cite{gan-goodfellow2014}, \cite{lsgan2017}. The standard GAN uses a sigmoid cross-entropy loss that leads to vanishing gradients which renders the training unstable, and as a result it suffers from sever degrees of mode collapse. In addition, for the generated images classified as the real with high confidence (i.e., large decision variable), no cost is incurred. Hence, the standard GAN tends to pull samples away from the decision boundary, that introduces non-realistic images~\cite{lsgan2017}. LSGN instead pulls the generated samples towards the decision boundary by using a LS cost. 

One issue with GAN however is that it introduces high frequency noise all over the image. $\ell_1$ criterion has proven well in discarding the noise from natural images as it does appropriately penalize the low-intensity noise~\cite{lossfunction_zhao2017}. Accordingly, to reveal fine texture details while discarding noise, we are motivated to adopt a mixture of LSGAN and $\ell_1$ costs to train the generator. The overall procedure aims to jointly minimize the discriminator cost
%
\begin{align}
{\rm(P1.1)} \quad \quad \min_{\bTheta_d}~ \mathbb{E}_{\bx} \Big[\Big(1-\cD(\bx;\bTheta_d)\Big)^2\Big] +  \mathbb{E}_{\by} \Big[\Big(\cD(\cG(\bPhi^{\dagger} \by; \bTheta_g);\bTheta_d)\Big)^2\Big]  \nonumber
\end{align}
%
and the generator cost
%
\begin{align}
{\rm(P1.2)} \quad \quad \min_{\bTheta_g}~ \mathbb{E}_{\by}\Big[\Big\|\by - \bPhi \cG(\bPhi^{\dagger} \by; \bTheta_g) \Big\|^2 \Big] & + \eta \mathbb{E}_{\bx,\by}\Big[\Big\|\bx - \cG(\bPhi^{\dagger} \by; \bTheta_g) \Big\|_1 \Big]  \nonumber \\ & + \lambda \mathbb{E}_{\by} \Big[\Big(1-\cD \big(\cG(\bPhi^{\dagger} \by; \bTheta_g);\bTheta_d\big)\Big)^2\Big] \nonumber
\end{align}
%
The first LS fitting term in (P1.2) is a soft penalty to ensure the input to D network is data consistent. Parameters $\lambda$ and $\eta$ also control the balance between manifold projection, noise suppression and data consistency. 



Looking carefully into (P1.2) the generator reconstructs image $\cG(\bPhi^{\dagger} \by; \bTheta_g)$ from the data $\by$ using an expected regularized-LS estimator, where the regularization is learned form training data via LSGAN and $\ell_1$-net. Different from the conventional CS formulation which also optimize the reconstruction with $\ell_1$-regularized LS estimation, the entire optimization only happens in training and the generator learned can be directly applied to new samples to achieve fast reconstruction.

As argued in~\cite{lsgan2017}, it can be shown that LSGAN game yields minimizing the Pearson-$\chi^2$ divergence. For (P1) following the same arguments as of the standard GANS in~\cite{gan-goodfellow2014} and~\cite{lsgan2017} it can be readily shown that even in the presence of LS data consistency and $\ell_1$ penalty, the distribution modeled by G network, say $p_g$, coincides with the true data distribution. This is formally stated next.

\noindent\textbf{Lemma 1.}~{\it For the noise-free scenario ($\bv=\mathbf{0}$), suppose D and G have infinite capacity. Then, for a given generator network G, i) the optimal discriminator D is $\cD^{*}(\bTheta_d;\check{\bx})=p_x(\check{\bx})/(p_x(\check{\bx})+p_g(\check{\bx}))$; and ii) $p_g=p_x$ achieves the equilibrium for the game (P1). }

\noindent{\it Proof.} The first part is similar to the one in~\cite{lsgan2017} with the same cost for D. The second part also readily follows as the LS data consistency and $\ell_1$ penalty are non-negative, and become zero when $p_g=p_x$. Thus, according to Pearson-$\chi^2$ divergence still bounds (P1.2) objective from below, and is achievable when $p_g=p_x$. $\blacksquare$







%\subsection{Generalization error bounds}
%\label{subsec:gen_err}
%%
%Consider a test observation $\by$. The goal is to bound the reconstruction likelihood $\|\hat{\bx}-\bx\|$ where the reconstruction is projection of zero-filling solution onto the manifold, namely $\hat{\bx}=\cG(\bTheta_g;\bPhi^{\dagger} \by)$. Apparently, the upperbound depends upon the matrix $\bPhi$ and the training process (including capacity of generative and discriminative networks) ...  
%
%
%
%An easier problem is to bound the measurement error $\|\by - \bPhi \cG(\bTheta_g;\bPhi^{\dagger} \by)\|$ ... This still needs taking into account the training data as well as G and D network structures ... 
%
%What simplifying assumptions can we make to make the bound tractable?
%
%given some prescribed upperbound on the KL distance between the sample model distribution and the real distribution for the training data ... ??!!!






\subsection{Stochastic alternating minimization}
\label{subsec:alt_min}
%
To train the G and D networks, a mini-batch stochastic alternating minimization scheme is adopted. At $k$-th iteration with the mini-batch training data $\{(\bx_{\ell},\by_{\ell})\}_{\ell=1}^L$, assuming that G is fixed, we first update the discriminator $\bTheta_d$ by taking a single descent step with momentum along the gradient of D cost, say $f_d$. Similarly, given the updated $\bTheta_d$, the G network is updated by taking a gradient descent step with momentum along the gradient of G cost, say $f_g$. The resulting iterations are listed under Algorithm~\ref{tab:alg_gan_training}, where the gradients $\nabla_{\bTheta_g}\cG(\bTheta_g;\tilde{\bx}_{\ell})$, $\nabla_{\bTheta_d}\cD(\hat{\bx}_{\ell};\bTheta_d)$, and $\nabla_{\bTheta_g}\cD(\cG(\bTheta_g;\tilde{\bx}_{\ell});\bTheta_d)$ are readily obtained via backpropagation over D and G networks. Also, $G_n$ refers to the $n$-th output pixel of G network, and $[.]_n$ picks the $n$-th pixel.  

%
%\begin{align}
%\frac{1}{L}\sum_{\ell=1}^{L} \Big\{- \big(1-\cD(\bx_{\ell};\bTheta_d) \big) \nabla_{\bTheta_d}\cD(\bx;\bTheta_d) + \big(\cD(\hat{\bx}_{\ell};\bTheta_d) \big) \nabla_{\bTheta_d} \big(\cD(\hat{\bx}_{\ell};\bTheta_d) \big) \Big\} \nonumber
%\end{align}
%%
%\begin{align}
%\frac{1}{L}\sum_{\ell=1}^{L} \Big\{\frac{1}{2}\nabla_{\bTheta_g} \big\|\by_{\ell} - \bPhi \cG(\bTheta_g;\bPhi^{\dagger}\by_{\ell})\big\|^2 - \lambda  \big(1-\cD(\cG(\bTheta_g;\bPhi^{\dagger}\by_{\ell});\bTheta_d) \big) \nabla_{\bTheta_g} \cD(\cG(\bTheta_g;\bPhi^{\dagger}\by_{\ell});\bTheta_d)  \Big\} \nonumber
%\end{align}
%Claims:


%2-After modifying the G network, it will be equivalent to solving the KL-regularized LS problem ... 


%3-Convergence of the alternating minimization scheme ... can we use the convergence claims from Mairal's paper? Can we use majorization-minimization ideas?








\begin{algorithm}[t]
	\caption{Training algorithm using BP based stochastic alternating minimization} \small{
		\begin{algorithmic}
			\STATE \textbf{input} 
			$\{(\bx_{\ell},\by_{\ell})\}_{\ell=1}^{L},\lambda,\mu, \bPhi$.
			\STATE \textbf{initialize} $(\bTheta_g[0],\bTheta_d[0])$ at random.
			
			\FOR {${\rm epoch}=1,\ldots,{\rm epoch}_{\max}$}
			
			\FOR {$k=1,\ldots,L/L_b$}
			
			\STATE \textbf{S1) Random mini-batch selection} 
			 
			\STATE {\rm Sample the mini-batch $\{\by_{\ell}\}_{\ell=1}^{L_b}$, and define $\tilde{\bx}:=\bPhi^{\dagger} \by_{\ell}$, $\hat{\bx}:=\cG(\bTheta_g;\tilde{\bx}_{\ell})$, and $\hat{\be}_{\ell} := \by_{\ell} - \bPhi \hat{\bx}$}
			
		    \STATE \textbf{S2) Discriminator update:} gradient-descent with momentum along  
			
            \STATE $ \nabla_{\bTheta_d} C_d := \frac{ \mu}{L_b} \sum_{\ell=1}^{L_b} \Big\{ -(1-\cD(\bx_{\ell};\bTheta_d)) \nabla_{\bTheta_d} \cD(\bx_{\ell};\bTheta_d) + \cD(\hat{\bx}_{\ell};\bTheta_d) \nabla_{\bTheta_d} \cD(\hat{\bx}_{\ell};\bTheta_d) \Big\}$
            
            %\STATE $\bTheta_d[k]=\bTheta_d[k] - \nabla_{\bTheta_g} C_g  ???/ $
            
            \STATE \textbf{S3) Generator update:} gradient-descent with momentum along  
         		    
            %\STATE $\nabla_{\bTheta_g} C_g := \frac{\mu}{L_b} \sum_{\ell=1}^{L_b} \Big\{  -\lambda (1-D(\hat{\bx}_{\ell};\bTheta_d)) \sum_{n} [\nabla_{\hat{\bx}} D(\hat{\bx}_{\ell};\bTheta_d)]_n \nabla_{\bTheta_g} G_n(\bTheta_g;\tilde{\bx}) - \nabla_{\bTheta_g} G(\bTheta_g;\hat{\bx}) \bPhi^{\top} \hat{\be}_{\ell}  \Big\}$  
            
            
            \STATE $\nabla_{\bTheta_g} C_g := \frac{\mu}{L_b} \sum_{\ell=1}^{L_b} \sum_{n=1}^N \Big\{  -\lambda (1-\cD(\hat{\bx}_{\ell};\bTheta_d)) [\nabla_{\hat{\bx}} \cD(\hat{\bx}_{\ell};\bTheta_d)]_n -  \boldsymbol{\phi}_n^{\top} \hat{\be}_{\ell} + \eta [\rm{sgn}(\bx_{\ell} - \cG(\bTheta_g;\tilde{\bx}) )]_n \Big\} \nabla_{\bTheta_g} \cG_n(\bTheta_g;\tilde{\bx})$  
            
            
            
            %\STATE $\bTheta_g[k]=\bTheta_g[k] - \nabla_{\bTheta_d} C_d ???? $
			
			\ENDFOR
			
			\ENDFOR
			
			\RETURN  $(\bTheta_g, \bTheta_d)$
						
		\end{algorithmic}}
		\label{tab:alg_gan_training}
	\end{algorithm}








   



%\subsection{Discriminator network}
%\label{subsec:disc}
%%
%For the binary classification task of the discriminator network, a CNN is employed which takes the sample xx from the generator and outputs the likelihood of it belonging to the image manifold. A cross-entropy loss attempts to learn the labels using a soft-max score. Batch normalization is used in all layers except the first layer so that the model can learn the correct mean and scale of data distribution


%\section{GAN-based Compressed Sensing}
%\label{sec:gancs}





\section{Experiments}
\label{sec:eval}
%
Effectiveness of the novel GANCS scheme is assessed in this section via tests for MRI reconstruction. A single-coil MR acquisition model is considered where for $n$-th patient the acquired $k$-space data abides to $y_{i,j}^{(n)} = [\cF(\bX_n)]_{i,j} + v_{i,j}^{(n)},~~(i,j) \in \Omega$. Here, $\cF$ is the 2D FT, and the set $\Omega$ indexes the sampled Fourier coefficients. As it is conventionally performed with CS MRI, we select $\Omega$ based on a variable density sampling with radial view ordering~\cite{} that tends to pick low frequency components from the center of $k$-space (see sampling mask in Fig. 4 (left) of the supplementary document). Throughout the test we assume $\Omega$ collects only $20\%$ of the Fourier coefficients, and we choose $\lambda=0.1$. 
%
%\noindent{\it D1) Shepp-Logan brain phantom.}~A Shepp-Logan phantom [shepp-logan], used widely by researchers in tomography, simulated to produce $10^4$ randomly independent $256 \times 256$ brain images. The model parameters including the location, orientation, and intensity of the objects can substantially change from sample to sample. 

%\noindent{\it D2) High-resolution knee images.}~A dataset of knee images for $19$ patients, acquired at the Stanford Children's Hospital, is considered with images of size $512 \times 512 \times 253$~[berkeley-repo]. $200$ slices from each patient are considered with $190$ used for training and $10$ for test. This leads to $3,610$ training and $190$ test samples. 


%\noindent{\it D3) Dynamic contrast enhanced abdominal images.}~

%In related studies, people uses simulation dataset for proof-of-concept, We have also conducted extensive experiment on a simple Shepp-Logan brain phantom which we used to simulated to produce $10^4$ randomly independent $256 \times 256$ brain images. However the task is too simple and not practical for GAN since the image has too well defined structure to distinguish using fewer parameters. To really validate the method in application, here we show experiment results on real MRI datasets. %[changed]

\noindent{\textbf{Dataset}.}~High contrast abdominal image volumes are acquired for $350$ pediatric patients after gadolinium-based contrast enhancement. Each 3D volume includes contains $151$ axial slices of size $200 \times 100$. Axial slices used as input images for training a neural network. $300$ patients ($45,300$ images) are considered for training, and $50$ patients ($7,550$ images) for test. All in vivo scans were acquired at the Stanford’s Lucile Packard Children’s Hospital on a 3T MRI scanner (GE MR750) with voxel resolution $1.07 \times 1.12 \times 2.4$ mm. 



Under this setting, the ensuing parts address the following questions: 



Q1. How does the perceptual cost learned by GANCS improve the image quality compared with the pixel-wise $\ell_2$ and $\ell_1$ costs? 

Q2. How much speed up and quality improvement one can achieve using GANCS relative to conventional CS? 

Q3. What MR image features derive the network to learn the manifold and remove the aliasing artifacts?

Q4. How many samples/patients are needed to achieve a reasonable image quality?


%Q4. How to verify that the learned manifold does not memorize and it indeed learns?


%Q4. What is an appropriate sampling trajectory that is incoherent with the sampling artifacts?









\subsection{Training and network architecture}
\label{subsec:training}
%
The input and output are complex-valued images of the same size and each include two channels for real and imaginary components. The input image $\tilde{\bx}$ is simply generated using inverse 2D FT of the sampled $k$-space, which is severely contaminated by artifacts. Input channels are then convolved with different kernels and added up in the next layer. Note, all network kernels are assumed real-valued. Inspired by super-resolution ideas in~\cite{johnson2016, leding et al'16}, and the network architecture in ~\cite{srez} we adopt a deep residual network for the generator with $8$ residual blocks. Each block consists of two convolutional layers with small $3 \times 3$ kernels and $64$ feature maps that are followed by batch normalization and ReLU activation. It then follows by three convolutional layers with map size $1 \times 1$, where the first two layers undergo ReLU activation, while the last layer has sigmoid activation to return the output. G network learns the projection onto the manifold while ensuring the data consistency at the same time, where the manifold dimension is controlled by the number of residual blocks and feature maps and the settings of discriminator D network.

% .... (apparently after each two blocks there is a deconvolution layer that i don't understand??!!)

To satisfy data consistency term, previous work in the context of image super-resolution~\cite{Sonderby et al'14} used (hard) affine projection after the G network. However, the affine projection drifts $\hat{\bx}$ away from the manifold landscape. As argued in Section 3, we instead use a multilayer succession of affine projection and convolutional residual units that project back $\hat{\bx}$ onto the manifold. We can repeat this procedure a few times to ensure $\hat{\bx}$ lies close to the intersection. This amounts to a soft yet flexible data consistency penalty. 

% However, here we also want to ensure the solution finds the intersection between data consistency and manifold projection yet the affine projection will definitely drift the solution away from manifold. We designed the model to be followed by soft data consistency modules, which includes 2D FFT transform to k-space, mixing with sampled data (similar to affine projection), inverse 2D FFT transform back to image domain and a residual block. Basically it conduct an affine projection followed by nonlinear convolutional transform. [changed, add formulation] This module can be used multiple times to implicitly enforce data consistency and to be flexible enough to learn a projection while tuning residual block with convolution layers such that the solution can stay as close to the manifold as possible. [changed]
 


%Inspired by manifold learning using autoencoders, we form the G network as an encoder-decoder pair, with the decoder simply mirroring the encoder. The encoder encompasses three convolutional layers, where the output of the last layer is mapped to the latent features through a fully-connected layer. Convolution with stride $s=2$ is used to gradually reduce the spatial size of feature maps before forming the latent features. 


%For the decoder we adopt a similar structure as the generator network of DCGAN~[radford et al'16]. The decoder reconstructs the clean image from the latent features. It starts from a fully connected layer that projects the latent features to $64$ feature maps. It then consists of transpose convolution layers with stride $2$ to increase the spatial dimension. Batch normalization is used, but no pooling or unpooling layers used at the decoder and encoder sides. In addition, skip connections are used from the input of encoder layers to the corresponding decoder outputs to improve the training stability and ... [xxx]. The manifold dimension is also controlled by the number of latent features.


The D network starts from the output of the G network with two channels. It is composed of $8$ convolutional layers. In all the layers except the last one, the convolution is followed by batch normalization, and subsequently ReLU activation. No pooling is used. For the first four layers, number of feature maps is doubled from $8$ to $64$, while at the same time convolution with stride $2$ is used to reduce the image resolution. Kernel size $3 \times 3$ is adopted for the first 5 layers, while the last two layers use kernel size $1 \times 1$. In the last layer, the convolution output is averaged out to form the decision variable for binary classification. No soft-max is used.  


%and the discriminator cleans to classify whether the input images are the same quality as the fully-sampled ground-truth images or it is a reconstructed image from undersampling.


%For the binary classification task of the discriminator network, a multilayer perceptron is employed which takes the complex-valued output of the generator and outputs the likelihood of it belonging to the image manifold. Since LS cost is used, no soft-max operation is performed in the last layer. Batch normalization is used in all layers except the first layer so that the model can learn the correct mean and scale of data distribution

%We follow the architectural guidelines summarized by Radford et al. [42] and use LeakyReLU activation and avoid max-pooling throughout the network. The discriminator network is trained to solve the maximization problem in Equation 2. It contains eight convolutional layers with an increasing number of filter kernels, increasing by a factor of 2 from 64 to 512 kernels as in the VGG network [47]. Strided convolutions are used to reduce the image resolution each time the number of features is doubled. The resulting 512 feature maps are followed by two dense layers and a final sigmoid activation function to obtain a probability for sample classification ...



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\begin{figure}[t]
%	%\centering
%	\hspace{0.0cm}\includegraphics[scale=1.15]{fig_gen_net_architecture.eps}
%	\caption{G network architecture}
%	\label{fig:fig_gen_net}
%\end{figure}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%




%We optimize the network parameters based on the above formulation to optimize a combination of Least-Square GAN loss, data consistency loss and L1/L2 loss.

Adam optimizer is used with the momentum parameter $\beta=0.9$, mini-batch size $L_b=8$, and initial learning rate $\mu=10^{-5}$ that is halved every $5,000$ iterations. Training is performed with TensorFlow interface on a NVIDIA Titan X Pascal GPU, 12GB RAM. We allow $20$ epochs that takes around $6$ hours for training. The implementation is available online at~\cite{github-gancs-2017}. 

%\begin{figure}[t]
%	\centering
%	\hspace{0.0cm}\includegraphics[scale=0.8]{./figures/Loss_vs_batch_log.png}
%	\caption{Evolution of different parts of the loss functions (log scale) for GANCS at training process }
%	\label{fig:fig_loss_function}
%\end{figure}

As a figure of merit for image quality assessment we adopt SNR (dB), and SSIM that is defined on a cropped window of size $50 \times 50$ from the center of axial slices. In addition, we asked Radiologists Opinion Score (ROS) regarding the diagnostic quality of images. ROS ranges from $1$ (worse) to $5$ (excellent) based on the overall images quality in terms of sharpness/blurriness, and appearance of residual artifacts. 

%by averaging expert radiologists' rate the images independently from 1 (worse) to 10 (excellent) based on how the overall image quality is and how sharpness/blurriness and appearance of residual artifacts effects their diagnosis.



\subsection{Observations and discussion}
\label{subsec:obs}
%
Retrieved images by various methods are depicted in Fig.~\ref{fig:fig_recon_gancs_vs_cs_mse_4fold} with $5$-fold undersampling of $k$-space. For a random test patient, representative slices from axial, and coronal orientations, respectively, are shown from top to bottom. Columns from left to right also show, respectively, the images reconstructed by zero-filling (ZF), CS-WV, CS-TV, $\ell_2$-net, $\ell_1$-net, GAN, GANCS with $\lambda=\eta=10$, and the gold-standard (GS). Note, we propose $\ell_1$-net and $\ell_2$-net using the same network structure and training as in Section~\ref{subsec:training}, with only changing the G net cost function in (P1). CS reconstruction is performed using the Berkeley Advanced Reconstruction Toolbox (BART)~\cite{bart2016}, where the tunning parameters are optimized for the best performance. GANCS, $\ell_1$-net and $\ell_2$-net are trained with ZF images that apparently contain aliasing artifacts. 




Quantitative metrics including the SNR (dB), SSIM, and the reconstruction time (sec) are also reported in Table I. These metrics are averaged out over all axial slices for test patients. As apparent from the magnified regions, GANCS returns the most detailed images with high contrast and texture details that can reveal the small structures. $\ell_2$-net images are seen somehow over-smoothed as the $\ell_2$ cost encourages finding pixel-wise averages of plausible solutions. Also, $\ell_1$-net performs better than $\ell_2$-net, which was also already reported in a different setting~\cite{lossfunction_zhao2017}, but still not as sharp as GANCS which leverages both $\ell_1$-net and GAN. GAN results with $\eta=0$ also introduces sharp images but noise is still present all over the image. CS-based results are also depicted as the benchmark MR reconstruction scheme nowadays, where evidently introduce blurring artifacts. 


CS-based scheme achieve higher SNR and SSIM, but they miss the high frequency textures as evidenced by Fig.~\ref{fig:fig_recon_gancs_vs_cs_mse_4fold}. In addition, they demands iterative algorithms for solving non-smooth optimization programs that takes a few seconds for reconstruction using the optimized BART toolbox~\cite{bart2016}. In contrast, the elapsed time for GANCS is only about $10$ msec, which allows reconstructing $100$ frames per second, and thus a suitable choice for real-time MRI visualization tasks. Regarding the convergence, we empirically observe faster and more stable training by imposing more weight on the data consistency which restricts the search space for the network weights. 


%Note, we also compared with the combination of $\ell_1$ and SSIM costs, recently reported in~\cite{lossfunction_zhao2017} to better capture the textures for natural image restoration~\cite{}, but it did not improve over $\ell_1$-alone was observed in the MRI context. Thus, we did not include it due to the limit of space. 


To assess the perceptual quality of resulting images we also asked the opinion of expert radiologists. We normalize the scores so as the gold-standard images are rated excellent (i.e., ROS=$5$). Statistical ROS is evaluated for the image quality, residual artifacts, and image sharpness. It is shown in the bar plot of Fig.~\ref{fig:fig_barplot_ros}, which confirms GANCS almost perceptually pleasing as the gold-standard scan. This demonstrates the superior diagnostic quality of GANCS images relative to the other alternatives. 


For the sake of completeness, the evolution of different (empirical) costs associated with the generator cost in (P1.2) over batches are also depicted in Fig.~\ref{fig:fig_loss}. It is observed that the data consistency cost and GAN loss tend to improve alternatively to find the distribution at the intersection of manifold and dats consistency space. 

%One example of the evolution of loss function at different batches is shown in figure[x] where the loss for data consistency and GAN loss for manifold learning are shown to improve alternatively. 



%In addition, to see how much undersampling one can afford with GANCS we plot the SNR and SSIM versus the undersmapling rate and compare it with the standard CS in Fig. xx. It is evident that for high undersampling rates the GANCS still achieves a reasonable SNR while conventional CS degrades significantly ....


%One example of the evolution of loss function at different batches is shown in figure[x] where the loss for data consistency and GAN loss for manifold learning are shown to improve alternatively. 



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure}[t]
	%\centering
	\hspace{-1.85cm}\includegraphics[scale=0.66]{fig_comp_gancs_vs_cs_zoomed_7schemes.png}
	\caption{Representative coronal (1st row) and axial (3rd row) images for a test patient retrieved by ZF (1st), CS-WV (2nd), $\ell_2$-net (3th), $\ell_1$-net (4th), GAN (5th), GANCS (6th), and gold-standard (7th). }
	\label{fig:fig_recon_gancs_vs_cs_mse_4fold}
\end{figure}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\begin{figure}[t]
%	\centering
%	\hspace{0.0cm}\includegraphics[scale=0.60]{./figures/fig_recon_kspace_gancs_vs_cs_mse_4fold.eps}
%	\caption{Representaitve $k$-space axial image retrieved by zero-filling (1st column), CS-Wavelet (2nd column), CS-TV (3rd column), MSE (4th column), GANCS ($\eta=0.1$) (5th column), and gold-standard (5th column). }
%	\label{fig:fig_recon_kspace_gancs_vs_cs_mse_4fold}
%\end{figure}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure}[t]
	\centering
	\begin{tabular}{c}
		\hspace{-8mm}\epsfig{file=fig_bar_plot_ros.png,width=1.1
			\linewidth, height=1.95 in }  \\

	\end{tabular}
	\caption{Mean and standard deviation of image quality artifacts and blurriness scored by expert radiologists for various reconstruction techniques. Scores $1$ to $5$ rate from poor to excellent. }
	\label{fig:fig_barplot_ros}
\end{figure}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\begin{figure}[t]
%	\centering
%	\begin{tabular}{c}
%		\hspace{-8mm}\epsfig{file=fig_barplot_rating_quality_modf.eps,width=0.54
%			\linewidth, height=1.75 in } 
%		\hspace{-7mm}\epsfig{file=fig_barplot_rating_bluriness_modf.eps,width=0.54
%			\linewidth, height=1.75 in } \\
%		
%		
%		
%		
%	\end{tabular}
%	\caption{Mean and standard deviation of image quality artifacts and blurriness scored by expert radiologists for various reconstruction techniques. Scores $1$ to $5$ rate from poor to excellent. }
%	\label{fig:fig_barplot_ros}
%\end{figure}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\begin{figure}[t]
%	%\centering
%	\hspace{0.0cm}\includegraphics[scale=1.15]{}
%	\caption{SNR and SSIM versus the undersampling rate for GANCS and CS for the Brain Phantom dataset.  }
%	\label{fig:fig_snr_vs_sampfac}
%\end{figure}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{table}[t]
	\caption{Average SNR (dB), SSIM, ROS, and reconstruction time (sec) comparison of different schemes under $5$-fold undersampling.  }
	\vspace{-2.5mm}
	\label{tab:table_comp_quantitative}
	\begin{center}
		\begin{tabular} {|c|c|c|c|c|c|c|c|}
			\hline
			Scheme &  ZF & CS-WV & CS-TV &  $\ell_2$-net & $\ell_1$-net & GAN  & GANCS  \\
			\hline\hline
			SNR & $15.28$ & $20.74$ & $21.33$ & $18.96$ & $18.64$ & $16.6$ & $20.48$     \\
			\hline
			SSIM & $0.72$ & $0.88$ & $0.87$ & $0.81$  & $0.79$ & $0.78$ & $0.87$    \\
			\hline
			%ROS-quality & $1.69 \pm 0$ & $3.32 \pm 0.76$ & $2.7966 \pm 0.9661$  & $3.2881 \pm 0.8814$ & l1 & GAN & $3.4746 \pm 1.3898$  \\
			%\hline
			%ROS-blurriness & $1.69 \pm 0$ & $3.10 \pm 0.68$ & $2.71 \pm 0.58$  & $3.84 \pm 1.14$ & l1 & GAN & $3.94 \pm 1.00$  \\
			%\hline
			Recon. time & $5\hspace{-1mm} \times \hspace{-1mm} 10^{-4}$ & $5.27$ & $1.51$  & $0.02$ & $0.02$ & $0.02$ & $0.02$  \\
			\hline
			
		\end{tabular}
	\end{center}
\end{table}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%





\begin{figure}[t]
	\centering
	\hspace{0.0cm}\includegraphics[scale=1]{fig_recon_kspace_gancs_vs_cs_mse_4fold.png}
	\caption{Representaitve $k$-space axial image retrieved by ZF (1st column), CS-WV (2nd), CS-TV (3rd), and GANCS (4th), and gold-standard (5th). }
	\label{fig:fig_recon_kspace_gancs_vs_cs_mse_4fold}
\end{figure}











%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure}[t]
	\centering
	\hspace{0cm}\includegraphics[scale=0.65]{fig_loss_batch.png}
	\caption{Evolution of different costs contributing in the overall training cost of G network.  }
	\label{fig:fig_loss}
\end{figure}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%




%How do the images look like? Are they realistic and meaningful for radiologists?


%How many samples/patients are needed to achieve a reasonable accuracy? We can plot e.g., the recon. error vs. the number of training samples. This makes a lot of sense for the simulated phantom model where we can manage to generate arbitrary number of samples. It would also be very nice to plot the number of required samples (for a certain prescribed accuracy) versus the variance of random perturbations in the phantom model to see how it behaves (linear, exponential, or, whatever) ... Whenever I present this learning-type ideas to clinicians, they ask me this question ....




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\begin{figure}[t]
%	\centering
%	\hspace{0cm}\includegraphics[scale=1.0]{./figures/fig_recon_gancs_vs_cs_mse_mip_4fold_abdominal.eps}
%	\caption{MIP images for test patient under $5$-fold undersampling. Again saggital (1st row), axial (2nd row), and coronal (3rd row) images are retrieved by zero-filling (1st column), CS-Wavelet (2nd column), CS-TV (3rd column), MSE (4th column), GANCS ($\eta=0.1$) (5th column),  and gold-standard (6th column). }
%	\label{fig:fig_recon_gancs_cs_mip_abdominal}
%\end{figure}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



%\subsection{Manifold landscape}
%\label{subsec:manifold_land}

\noindent\textbf{Manifold landscape.}~We visualize what the discriminator learns by showing the feature maps in different layers as heat-maps superimposed on the original images. Since there are several feature maps per layer, we computed the Principle Component maps for each layer and visualize the first $8$ dominant ones. Fig.~ indicates that after learning from tens of thousands of generated MRI images by the G network and their gold standards including different organs, is able to detect anatomically valuable features. It is observed that the first layers reveal the edges, while the last layers closer to the classification output reveal more regions of interests that include both anatomy and texture details. This observation is consistent with the way expert radiologist inspect the images based on their diagnosis quality. 



\begin{figure}[ht!]
	\centering
	\hspace{0cm}\includegraphics[scale=0.17]{heatmap_vertical-min.png}
	\caption{Heat-map of discriminator feature maps at four layers for four different images. Each 4 row from top to bottom represent the results from one MR image. The first row shows the MR image and the Principle Components of the network features from the first layer. The second row shows an overlay view of the MR image and the heat-map. The third row shows the MR slice image with the Principle Components of the network features from the last layer of discriminator; while the fourth row shows the overlay view of the MR image and the heat-maps.}
	\label{fig:fig_heatmap}
\end{figure}


%identify the edges and as we go deeper more structured organs or tissues are revealed 

%At the upper level it does common detection of small features and edges while at the layer closer to the final classification it focus on different ROIs in the image for specific organs or tissues. 


%The discrimination is based on whether there are anatomically valuable features such as well delineation of organ and the detailed structures of tissues. The entire discrimination process actually is very similar to what real radiologists read the MRI images and how we ask the readers to judge the image qualities. This finding coincide with our initial assumption that Discriminator learns to score the diagnostic quality of Generator output. [Changed]

%Similar method has been used to visualize 
%features for what how a deep network based classifier makes judgment and where it confidently detects based on [Reference on heat-map].


%How to visualize the manifold?

%How does the manifold dimension/latent features play role?


%In the formulation, we train a GAN based regularizer based on large amount of real MRI dataset to learn a regularization specifically cater to for MRI's data distribution. The discrimator learns to distinguish the real and reconstructed image and find more accurate manifold of real MRI data, while the generator compete and manage to learn the projection considering both data consistency and projection onto the manifold. 

%The reconstruction results above visualize what the generator learns which accurately map to the ground-truth image from the input zero-filling solution with severe noises and artifacts. Here we also visualize what the discriminator learns by showing the feature maps in different layers as heat-maps projected to the original image space. Similar method has been used to visualize 
 %features for what how a deep network based classifier makes judgment and where it confidently detects based on [Reference on heat-map].

%\begin{figure}[t]
%	\centering
%	\begin{tabular}{c}
%		\hspace{-8mm}\epsfig{file=./figures/fig_barplot_rating_quality.eps,width=0.56
%			\linewidth, height=1.85 in } 		
%	\end{tabular}
%	\caption{Place holder: Different testing image and the discriminator features visualized by projecting to image space}
%	\label{fig:fig_heat_maps}
%\end{figure}



%The regularizer interpretation seems quite meaningful to connect it to the standard CS ideas. In essence, upon solving (P1) we train a GAN regularizer based on the historical data, where solving the LS data consistency under the regularizer amounts to projecting onto the manifold. Can we somehow visualize the regularizer?

%exploring the latent space can tell us about the signs of memorization ... 

%"[radford et al'16] Walking on the manifold that is learnt can usually tell us about signs of memorization (if there are sharp transitions)
%and about the way in which the space is hierarchically collapsed. If walking in this latent space results in semantic changes to the image generations (such as objects being added and removed), we can reason that the model has learned relevant and interesting representations."
%
%
%guided backpropagation can be used to show that features learnt by the discriminator activate on some specific parts of the image (could be both image or k-space domain!!)
%
%
%Fig. 4 in [radford et al'16] nicely draws a sequence of samples from the manifold by interpolating a few initial training samples ... It shows slow transitions ...

\noindent\textbf{Performance with different number of patients}~We also experimented on the number of patients needed for training and achieving good reconstruction quality in the test phase. It is generally valuable for the clinicians how much training data is needed as in the medical applications, patient data is not easily accessible due to privacy concerns. Fig.~\ref{fig:fig_performance} plots the normalized RMSE on a test set versus the percentage of patients used for training (normalized by the maximum patient number $350$). Note, the variance differences for different training may be due to the training with fewer samples has better convergence, since we are using the same epoch numbers for all the training cases. More detailed study is the subject of our ongoing research. 



\begin{figure}[t]
	\centering
	\hspace{0.0cm}\includegraphics[scale=0.15]{Performance_vs_Datasize.png}
	\caption{Performance changes with different size of dataset used for training (output about 45,300 images) }
	\label{fig:fig_performance}
\end{figure}



%Fig.~2 in the supplementary document plots the RMSE versus the number of patients (normalized to the maximum number $300$ patients). Apparently, after $50$ patients the performance improves considerably, but it starts improving quite gradually after $150$ patients. 


%the test performnace improves for more patients, but it starts improving grad
%
%  the performance (evaluated at the same test data size) increases with more training data size used. In this work we demonstrated the performance with a large cohort of real pediatric MRI images which is valuable considering the difficulties in preparing medical image datasets and the performance gain the large dataset provides. Note, we also have experiments showing similar results on simulation brain phantom but did not include here due to limit of space and the real pediatric MRI dataset shows results in harder tasks already. 
%
%Further data augmentation can also help to improve the performance and robustness of the proposed method. 
%[changed]




%\subsection{Optimal sampling trajectory}
%\label{subsec:quality}
%%
%What's an appropriate sampling trajectory that is incoherent with the sampling artifacts? 
%For CS, random (variable density) sampling is known to be optimal in some sense. Finding the optimal sampling trajectory for deep learning is a very intriguing question!!! It seems that the main structure (low frequency components) can be learned to a good extent from other patients; the main structures are similar ... What it remains to be different from patient to patient are the details, or, the high frequency components!! This gives a little hint that the right trajectory might be different from the variable density. We can conduct large tests to validate this hypothesis. 



%\noindent{\textbf{Image abnormalities.}}~A new patient may have abnormalities like a tumor that may not exist in the training dataset!! Can the resulting image still be used for diagnosis? Should we modify the network architecture to take the diagnosis information into account? This reminds of the abnormality detection problem using low-rank plus sparse models ... One can think of learning two manifolds (like subspace clustering), one for normal images, and another one for abnormal images; the latent features can be divided into two non-overlapping groups like the epitomic variational auto-encoders [], where the corresponding latent features can be additionally mapped through two separate classifiers (e.g., CNNs) to the normal/abnormal labels. This is however very supervised, and in the test phase we need the label for test patient to find the recon ... 
%An important question now is how useful this idea is?   

%Using diagnostic results for training could help to some extent ... and in the test phase we can also draw diagnosis decisions according to patients subsampled data ...

%One intruguing question that we are currently working on as pointed out in Section xxx, is to develop unsupervised schemes (along the lines of low-rank plus sparse separation ideas) to identify the abnomalities based on the fact that they happen quite sporadically/rarely across the cohort of patients .... 






\section{Conclusions and Future Work}
\label{sec:conclusion}
%
This paper caters a novel CS framework that leverages the historical data for faster and more diagnosis-valuable image reconstruction from highly undersampled observations. A low-dimensional manifold is learned where the images are not only sharp and high contrast, but also consistent with both the real MRI data and the acquisition model. To this end, a neural network based on LSGANs is trained that consists of a generator network to map a readily obtainable undersmapled image to the gold-standard one. Experiments based on a large cohort of abdominal MR data, and the evaluations performed by expert radiologists confirm that the GANCS retrieves images with better diagnostic quality in a real-time manner (about $10$ msec, more than $100$ times faster than state-of-the-art CS MRI toolbox). This achieves a significant speed-up and diagnostic accuracy relative to standard CS MRI. Last but not least, the scope of the novel GANCS goes beyond the MR reconstruction, and tailors other image restoration tasks dealing with aliasing artifacts. There are still important question to address such as using 3D spatial correlations for improved quality imaging, robustifying against patients with abnormalities, and variations in the acquisition model for instance as a result of different sampling strategies.
 


%Including diagnostic outcomes for training and backpropagation to account for abnormalities ... The discriminator labels could be true image without tumor, image with tumor, fake images ...

%\subsubsection*{Acknowledgments}
%
%Use unnumbered third level headings for the acknowledgments. All
%acknowledgments go at the end of the paper. Do not include
%acknowledgments in the anonymized submission, only in the final paper.


%\citet{pluempitiwiriyawej2005stacs}

\newpage

\newpage

%\section*{References}

%\bibliography{biblio}



\begin{thebibliography}{10}
	
\small
		
		\bibitem{github-gancs-2017} [online] https://github.com/gongenhao/GANCS.html
		
		\bibitem{berkeley-repo} [online] http://mridata.org/fullysampled/knees.html
		
		\bibitem{srez} [online] https://github.com/david-gpu/srez.html
		
		\bibitem{clearpoint} [online] http://www.mriinterventions.com/clearpoint/clearpoint-overview.html
		
%		\bibitem{Goodfellow et al'14} I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair, A. Courville, and Y. Bengio, 
%		\newblock ``Generative adversarial networks,''
%		\newblock \emph{Advances in Neural Information Processing Systems}, Montreal, Canada, Dec. 2014.  
		
		
		\bibitem{Sonderby et al'14} C. K. S\IeC {\o }nderby, J. Caballero, L. Theis, W. Shi, and F. Huszár,
		\newblock ``Amortised MAP inference for image super-resolution,''
		\newblock \emph{arXiv preprint}, arXiv:1610.04490, Oct. 2014.
		
		%Missing \endcsname inserted. ...S\IeC {\o }nderby et al'14}{{4}{}{{}}{{}}}
		
%		
%		\bibitem{Bora et al'17} A. Bora, A. Jalal, E. Price, A. G. Dimakis, 
%		\newblock ``Compressed sensing using generative models,''
%		\newblock \emph{arXiv preprint}, arXiv:1703.03208v1, Mar. 2017.
		
		
%		\bibitem{Arora et al'17} S. Arora, R. Ge, Y. Liang, T. Ma, Y. Zhang, 
%		\newblock ``Generalization and equilibrium in generative adversarial Networks (GANs),''
%		\newblock \emph{arXiv preprint}, arXiv:1703.00573v2, Mar. 2017.
		
		
		\bibitem{leding et al'16} C. Ledig, L. Theis, F. Huszar, J. Caballero, A. Cunningham, A. Acosta, A. Aitken, A. Tejani, J. Totz, Z. Wang, and W. Shi,
		\newblock ``Photo-realistic single image super-resolution using a generative adversarial network,''
		\newblock \emph{arXiv preprint}, arXiv:1609.04802 [cs.CV], Sep. 2016.
		
		
		\bibitem{radford et al'16} A. Radford, L. Metz, S. Chintala,
		\newblock ``Unsupervised representation learning with deep convolutional generative adversarial networks,''
		\newblock \emph{arXiv preprint}, arXiv:1511.06434 [cs.LG], Nov. 2015.
		
		
		\bibitem{Majumdar'15} A. Majumdar,
		\newblock ``Real-time dynamic MRI reconstruction using Stacked denoising autoencoder,''
		\newblock \emph{arXiv preprint}, arXiv:1503.06383 [cs.CV], Mar. 2015.
		
		
		\bibitem{Wang et al'16} S. Wang, Z. Su, L. Ying, X. Peng, S. Zhu, F. Liang, D. Feng, and D. Liang,
		\newblock ``Accelerating magnetic resonance imaging via deep learning,''
		\newblock \emph{Proc. IEEE International Symposium on Biomedical Imaging}, Melbourne, Australia, Apr. 2016.
		
		
		\bibitem{lsgan2017} X. Mao, Q. Li, H. Xie, R. Y.K. Lau, Z. Wang, and S. P. Smolley, 
		\newblock ``Least-squares generative adversarial networks,'' 
		\newblock \emph{arXiv preprint,} arXiv:1611.04076v3 [cs.CV], April 2017.
		
		
		\bibitem{Zhu et al'16} J-Y Zhu, P. Krahenbuhl, E. Shechtman, and A. A. Efros,
		\newblock ``Generative visual manipulation on the natural image manifold,''
		\newblock \emph{arXiv preprint}, arXiv:1609.03552v2 [
		cs.CV], Sep. 2016.
		
		
		
		\bibitem{shepp-logan} A. K. Jain,
		\newblock ``Fundamentals of digital image processing,''
		\newblock \emph{Englewood Cliffs, NJ, Prentice Hall}, 1989, p. 439.
		
		
		\bibitem{chandersekaran2011} V. Chandrasekaran, S. Sanghavi, P. A. Parrilo, and A. S. Willsky,
		\newblock ``Rank-sparsity incoherence for matrix decomposition,''
		\newblock \emph{SIAM Journal on Optimization}, vol. 21, p. 572--596, Apr. 2011.
		
		
		\bibitem{johnson2016} J. Johnson, A. Alahi, and F-F Li
		\newblock ``Perceptual Losses for Real-Time Style Transfer and Super-Resolution,''
		\newblock \emph{arXiv preprint}, arXiv:1603.08155v1 [cs.CV], Mar. 2016.
		
		
		\bibitem{resnet2016} K. He, X. Zhang, S. Ren, and J. Sun,
		\newblock ``Identity mappings in deep residual networks,''
		\newblock {\emph arXiv preprint}, arXiv:1603.05027v2 [cs.CV], Apr. 2016.
		
		
		
		\bibitem{dcgan2016} A. Radford, L. Metz, and S. Chintala
		\newblock ``Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks,''
		\newblock {\emph arXiv preprint}, arXiv:1511.06434v2 [cs.LG], Jan. 2016.
		
		
		\bibitem{bart2016} J. I. Tamir, F. Ong, J. Y. Cheng, M. Uecker, and M. Lustig,
		\newblock ``Generalized Magnetic Resonance Image Reconstruction using The Berkeley Advanced Reconstruction Toolbox,''
		\newblock {\emph ISMRM Workshop on Data Sampling and Image Reconstruction}, Sedona, Italy, May 2016.
		
		
		\bibitem{deepADMM2016} Y. Yang, J. Sun, H. Li, and Z. Xu, 
		\newblock ``Deep ADMM-Net for compressive sensing MRI,''
		\newblock \emph{Advances in Neural Information Processing Systems}, Vancouver, Canada, Dec. 2016.
		
		
		%\bibitem{gan-goodfellow2014} I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair, A. Courville, Y. Bengio,
		%\newblock ``Generative adversarial networks,''
		%\newblock \emph{Advances in Neural Information Processing Systems}, Montreal, Canada, Dec. 2014.
		
		
		\bibitem{gan-goodfellow2014} I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair, A. Courville, Y. Bengio,
		\newblock ``Generative adversarial networks,''
		\newblock \emph{Advances in Neural Information Processing Systems}, Montreal, Canada, Dec. 2014.
		
		
		\bibitem{inpainting-yeh-2016} R. Yeh, C. Chen, T. Y. Lim, M. Hasegawa-Johnson, M. N. Do,
		\newblock ``Semantic image inpainting with perceptual and contextual losses,''
		\newblock \emph{arXiv preprint}, arXiv:1607.07539, Jul. 2916. 
		
		\bibitem{lossfunction_zhao2017} H. Zhao, O. Gallo, I. Frosio, and J. Kautz,
		\newblock ``Loss Functions for Image Restoration with Neural
		Networks,''
		\newblock \emph{IEEE Transactions on Computational Imaging}, vol. 3, no. 1, Mar. 2017.
	
	
	\bibitem{pualy_mri20017} M. Lustig, D. Donoho, and J. M. Pauly,
	\newblock ``Sparse MRI: The application of compressed sensing for rapid MR imaging,''
	\newblock \emph{Magnetic resonance in medicine}, vol. 58, no. 6, pp. 1182-1195, 2007.
	
	\bibitem{lowdose_ct2017} H. Chen, Y. Zhang, M. K. Kalra, F. Lin, P. Liao, J. Zhou, and G. Wang,
	\newblock ``Low-Dose CT with a Residual Encoder-Decoder Convolutional Neural Network (RED-CNN),''
	\newblock \emph{arXiv preprint:}, arXiv:1702.00288v2 [physics.med-ph], 2017.
	
	
	
	
	
	
	
%} %end for small

\end{thebibliography}



\medskip





\end{document}