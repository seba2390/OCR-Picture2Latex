\begin{abstract}
Reinforcement learning (RL) has drawn increasing interests in recent years due to its tremendous success in various applications.
However, standard RL algorithms can only be applied for single reward function, and cannot adapt to an unseen reward function quickly.
In this paper, we advocate a general \emph{operator} view of reinforcement learning, which enables us to directly approximate the operator that maps from reward function to value function.
The benefit of learning the operator is that we can incorporate any new reward function as input and attain its corresponding value function in a \emph{zero-shot} manner.
To approximate this special type of operator, 
we design a number of novel operator neural network architectures based on its theoretical properties.
Our design of operator networks outperform the existing methods and the standard design of general purpose operator network,
and we demonstrate the benefit of our operator deep Q-learning framework in several tasks including reward transferring for offline policy evaluation (OPE) and reward transferring for offline policy optimization in a range of tasks.

\end{abstract}

\input{tex/intro}
\input{tex/background}
\input{tex/method}
\input{tex/design}
\input{tex/discussion}
\input{tex/related}
\input{tex/exp}

\section{Conclusion, Limitations and Social Impacts}
\label{sec:conclustion}

We propose an operator view of reinforcement learning, which enables us to transfer new unseen reward in a zero-shot manner. 
Our operator network directly leverages reward function value into the design,
which is more straightforward and generalized compared with previous methods.
One limitation of our operator q-learning is that we need to get access to a predefined reward sampler, which need human knowledge on the specific task.
Therefore, an important future direction is to generalize our method to  reward-free settings \citep[e.g.][]{jin2020reward}.


Our method improves the transferability of RL and hence advance its application in daily life. A potential negative social impact is our algorithm may fail to consider fairness and uncertainty, if fed  with biased or limited data. An important future direction is to investigate the potential issues here. 
\clearpage
\bibliography{ref}



