\section{Background and Problem Setting}
Consider the reinforcement learning (RL) setting where an agent is executed in an unknown dynamic environment.
At each step $t$, the agent observes a state $s_t$ in state space $\Sset$, takes an action based on the current policy
$a_t\sim \pi(\cdot|s_t)$ in action space $\Aset$, 
 receives a reward $r(s_t, a_t)$ according to a reward function $r\colon \Sset \times \Aset \to \RR$,  and transits to the next state $s_t' = s_{t+1}$ according to an \emph{unknown} transition distribution $s_t'\sim p(\cdot|s_t,a_t)$.


We focus on the \emph{offline, behavior-agnostic settings} \citep[e.g.,][]{nachum2019dualdice,zhang2020gendice,levine2020offline}, 
where we have no access to the real environment and 
can only perform  estimations on an 
offline dataset $\D = \{s_i, a_i, s_i', r_i\}_{i=1}^n$ 
collected from previous experiences following the same model dynamics and reward function 
but under different and \emph{unknown} policies. 
In offline RL, we are interested in either \emph{policy evaluation} or \emph{policy optimization}. 
In policy evaluation, we are interested in estimating the 
the (Q-)value function of a given policy $\pi$ of interest,  
\begin{equation}\label{eqn:defineq}
    q_{\pi,r}(s,a) = \E_{\tau \sim \pi}
    \left [\sum_{t=0}^{\infty} \gamma^t r(s_t, a_t)|s_0 = s, a_0 = a \right ]\,,
\end{equation}
where $\tau = \{s_t, a_t\}_{t=0}^{\infty}$ is the trajectory following policy $\pi$ and $\gamma \in (0,1)$ is a discount factor. 
In policy optimization, we want to get the maximum value function  $q_{*,r}$ among all the possible policies 
\begin{equation*}
    q_{*,r}(s,a) = \max_\pi q_{\pi, r}(s,a)\,.
\end{equation*} 
Both $q_{\pi, r}(s,a)$ and $q_{*, r}(s,a)$ are uniquely characterized by Bellman equation: 
\begin{align}
q_{\pi,r}(s,a) =& r(s,a) + \gamma \Ppi[q_{\pi,r}](s,a) \label{eqn:bellman_qpi}\,,\\
q_{*,r}(s,a) =& r(s,a) + \gamma \Pmax[q_{*,r}](s,a) \label{eqn:bellman_qmax}\,,
\end{align}
where $\Ppi$ and $\Pmax$ are the operators defined as:
\begin{align}
    \Ppi[f](s,a) =& \E_{(s',a')\sim p_\pi(\cdot|s,a)} [f(s',a')]\,, \label{equ:ppidef}\\
    \Pmax[f](s,a) =& \E_{s'\sim p(\cdot|s,a)} [\max_{a'\in \Aset} f(s',a')]\,, \label{equ:pmaxdef}
\end{align}
where $p_\pi(s',a'|s,a) = p(s'|s,a)\pi(a'|s')$. 
By approximating $q_{\pi,r}$ and $q_{*,r}$ with parametric functions (such as neural networks) and 
empirically solving the Bellman equation, we 
obtain the fitted Q Evaluation (FQE) for policy evaluation
and fitted Q Iteration (FQI) (or Q-learning) for policy optimization. 

