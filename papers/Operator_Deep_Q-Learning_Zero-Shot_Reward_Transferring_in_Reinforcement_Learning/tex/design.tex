    \section{Design of Operator Neural Networks}
\label{sec:design}


\begin{figure}[t]
    \centering
    \includegraphics[width=.85\linewidth]{figs/net_design.pdf}
    \vspace{-1.5em}
    \caption{Illustrations of Network Designs.}
    \label{fig:networks}
\end{figure}



As shown in \citet{lu2019deeponet}, there are general purpose  neural networks (a.k.a \textit{operator neural networks}) that provide universal approximation to general linear and non-linear operators. 
In specific, for any nonlinear operator $\G$ of interest, \citet{lu2019deeponet} approximates $\G[r](x)$ with a \textit{two-stream architecture} of vector value functions $\vv \phi,\vv \psi$ to take input of $r$ and $x$ separately and combine with a dot product:
\begin{equation}\label{eqn:general_purpose_design}
    \widehat{\G}[r](x) = {\vv \phi}(r)^\top {\vv \psi}(x)\,,
\end{equation}
where $\vv \phi$ and $\vv \psi$ are typically parametrized as a general function approximator such as multi-layer perceptron(MLP).
And in particular, since reward function is infinite dimention, 
it is discretized as $r = [r(\xi_1), r(\xi_2),...,r(\xi_m)]^\top$ 
by a set of reference points $\Xi = \{\xi_j\}_{j=1}^m$.
In offline RL settings, we observe rewards $r_i = r(x_i)$ for all $x_i\in \D$,
we can choose (part of) offline data points $\{x_1,\ldots,x_n\}$ as reference points.

However, the general purpose operator network structure in \citet{lu2019deeponet} 
does not leverage the special properties of $\G_\pi$ and $\G_*$ and hence may not generalize well. 
In this section, we propose a theoretically-motivated approach for network design, 
by studying the key theoretical properties of $\G_\pi$ and $\G_*$ 
to guide the design of network structures for policy evaluation and optimization respectively. 

\subsection{Policy Evaluation Case: The Linear Resolvent Operator}
We first consider $\G_\pi$ for policy evaluation, where a fixed target policy $\pi$. It turns out $\G_\pi$ is a linear operator that draws connection to the 
successor representation of \citet{dayan1993improving}. 

\begin{pro}\label{pro:property_gpi}
The resolvent policy evaluation operator
$\Gpi$ is determined by $\P_\pi$ via
\begin{equation}\label{equ:res}
    \Gpi  = \sum_{t=0}^\infty \gamma^t \P_\pi^t = (I-\gamma \Ppi)^{-1},
\end{equation}
which coincides the standard definition of resolvent operator in Markov processes, 
and it satisfies for all reward define on domain $\X$ that: 
    \begin{enumerate}
        \item Linearity: $\Gpi[\alpha r_1 + \beta r_2] = \alpha \Gpi[r_1] + \beta \Gpi[r_2],$ $~\forall \alpha, \beta \in \R, r_1, r_2$. 
        \item Monotonicity: for any non-negative function $\Delta$ such that $\Delta(x)\geq 0,~\forall x\in \X$,
        we have:
        $$
            \Gpi[r+\Delta](x) \geq \Gpi[r](x),~\forall x\in \X.
        $$
        \item Invariant to constant function: $\Gpi[r_C] = r_C/(1-\gamma)$, where $r_C = const$ is a constant function. %
    \end{enumerate}
\end{pro}

From Eq.~\eqref{equ:ppidef}, we have $\P_\pi[r](x)=\E_{x'\sim p_\pi(\cdot|x)}[r(x')],$
which yields 
\begin{align}\label{eqn:point_evaluation}
    \Gpi[r](x) = \frac{1}{1-\gamma}\E_{x'\sim d_\pi(\cdot|x)}[r(x')]\,,
\end{align}
with
\begin{equation}\label{eqn:def_dpi}
    d_\pi(x'|x) =(1-\gamma) \sum_{t=1}^{\infty} \gamma^t p_{\pi}^t(x'|x)\,,
\end{equation}
where $p_\pi^{t}(x
'|x)$ denotes the $t$-step transition probability of the Markov chain when the one-step transition probability is $p_\pi(x'|x)$.  %
 Therefore, $d_\pi(\cdot|x)$ is the discounted average visitation measure of the MDP with policy $\pi$ when initialized from $x$. %
In the tabular case, 
the definition of $d_\pi(\cdot|x)$ coincides with the successor representation in  \citet{dayan1993improving}. 

\myparagraph{Design for Evaluation Operator $\Gpi$}
The representation in Eq.~\eqref{eqn:point_evaluation} sheds important insights for designing operator neural networks for $\Gpi$.  
The expectation over the entire reward function can be approximated by 
a weighted average of the reward value on a finite number of reference points $\Xi = \{\xi_j\}_{j=1}^m$, 
$$
\E_{x'\sim d_\pi(\cdot|x)}[r(x')] 
\approx \sum_{j=1}^m  w_\theta(\xi_j|x) r(\xi_j)\,,
$$
where the reference points $\xi_j$ can be taken (partly) from offline dataset $\xi_j = x_{\sigma(j)}$ 
with random permutation $\sigma\colon \{1,2,\ldots,n\} \to \{1,2,\ldots,n\}$ and when $m = n$, we leverages the whole offline dataset as our reference points.
In this way, we can approximate $\Gpi$ by 
\begin{equation}\label{eqn:design_gpi}
    \G_\theta[r](x) = \frac{1}{1-\gamma}\sum_{j=1}^m 
{w}_\theta({\xi_j}~|~x) r(\xi_j)\,.
\end{equation}

There are two different ways of 
designing  the coefficient function $w_\theta({\xi_j}|x)$.
An attention based design is to parametrize $w$ as importance of $\xi$  w.r.t. $x$
\begin{equation}\label{eqn:attention_design_gpi}
    w_\theta({\xi_j}|x) = \frac{\exp(f_{\theta_f}(\xi_j)^\top g_{\theta_g}(x))}{\sum_{k=1}^m \exp(f_{\theta_f}(\xi_k)^\top g_{\theta_g}(x))}\,,
\end{equation}

where $f_{\theta_f}, g_{\theta_g}\colon \X \to \RR$ are neural networks with parameter $\theta_f,\theta_g$ respectively, and the softmax structure guarantees that all the coefficients are positive and the summation of them is 1, in order to satisfy the properties above.

Another design is to parametrize $w$ as linear decomposition of $\xi$ and $x$:
\begin{equation}\label{eqn:linear_design_gpi}
    w_\theta({\xi_j}|x) = f_{\theta_f}(\xi_j)^\top g_{\theta_g}(x)\,.
\end{equation}
This design does not satisfy all the listed properties such as monotonicity and invariant to constant in Proposition~\ref{pro:property_gpi}, because $w$ here can be negative and the summation of the coefficients is not 1.
However, the linear design can achieve faster computation compared with attention based design, and equipped with random feature, it can approximate the attention design.
Please refer Appendix~\ref{sec:discuss_design} for more details.

Both designs can approximate the true $\Gpi$ arbitrarily well once we have sufficiently number of reference points as the following theorem.
\begin{thm}\label{thm:approximate_gpi}
Suppose $\X$ is compact and $r\in \C(\X)$ is a bounded continual function $|r(x)|\leq 1,~\forall x\in \X$.
Then for any $\varepsilon > 0$ we can find sufficiently large $m$ reference points $\Xi = \{\xi_j\}_{j=1}^m$ such that 
\begin{equation}
    |\G_\theta[r](x) - \Gpi[r](x)| \leq \varepsilon,~\forall x\in \X, ~\forall r\in \F_r.
\end{equation}
In the meanwhile, attention based design of $\G_\theta$ satisfies all the properties in Proposition~\ref{pro:property_gpi}.
\end{thm}



The approximation in Eq.~\eqref{eqn:design_gpi} is closely related to self-normalize importance sampling 
in off policy evaluation; see Appendix~\ref{sec:adjoint} for more discussion.
Figure~\ref{fig:networks} summarizes the difference between our design of the operator network in Eq.~\eqref{eqn:design_gpi} and the general purpose design in Eq.~\eqref{eqn:general_purpose_design}.

\subsection{Policy Optimization Case:  Nonlinear Resolvent Operator}

Let us consider the operator $\Gmax$ for policy optimization. 
Unlike $\Gpi$, $\Gmax$ is a non-linear operator due to the non-linearity of $\Pmax$. 
Thus, we cannot follow the identical design $\Gpi$ for $\Gmax$.
However, the network for $\Gpi$ can serve as the building block for network of 
$\Gmax$, suggested from the following results. %
\begin{pro} \label{pro:property_gmax}
The maximum operator $\Gmax$ can always achieve the optimum value among all policies
\begin{equation}\label{eqn:maxpi}
    \Gmax[r](x) = \max_\pi \Gpi[r](x),~\forall r, x \in \X.
\end{equation}
And it satisfies for all reward define on domain $\X$ the following properties:
\begin{enumerate}
    \item Sub-linearity: $\Gmax [r_1 + r_2] \leq \Gmax[r_1] + \Gmax[r_2]$, $\Gmax[\alpha r] = \alpha \Gmax[r]$, $\forall \alpha\in \R, r_1, r_2$.
    \item Monotonicity: for any non-negative function $\Delta$ such that $\Delta(x)\geq 0,~\forall x\in \X$,
    we have:
    $$
    \Gmax [r+\Delta](x) \geq \Gmax[r](x),~\forall x\in \X.
    $$
    \item Invariance to constant function: $\Gmax[\alpha r + r_C] = \alpha \Gmax[r] + r_C/(1-\gamma)$ for constant function $r_C=const$. 
\end{enumerate}
\end{pro}

Based on Eq.~\eqref{eqn:maxpi}
we propose a max-out structure for $\Gmax$ to satisfy all listed properties.

\paragraph{Max-Out Architecture for $\Gmax$} 
As shown in \eqref{eqn:maxpi},  
$\Gmax$ is the maximum of the $\Gpi$ operators with different policies $\pi$. 
To obtain a computationally tractable structure, 
we discretize the max on a finite set of $K$ linear operators $\G_{\theta_k}[r](x) = \sum_{j=1}^m w_{\theta_k}(\xi_j|x)r(\xi_j)/(1-\gamma)$ and taking the max %
\begin{equation}\label{eqn:design_gmax_maxout}
    \G_{\vv\theta}^{max}[r](x) = \frac{1}{1-\gamma} \max_{k\in [K]}  \sum_{j=1}^m w_{\theta_k}(\xi_j|x) r(\xi_j),
\end{equation}
where $\vv\theta = \{\theta_k\}_{k=1}^K$ is the parameter of the network. 
It is easy to show that 
the max-out design \citep{goodfellow2013maxout} satisfies the properties in the all three properties in Proposition~\ref{pro:property_gmax}.  

The max-out structure can be easily modified from the $\Gpi$ operator network by maintaining $K$ different copies.
Picking the maximum among a number of value functions has been studied in many of the existing network designs of (general) value function \citep[e.g.][]{barreto2017successor, barreto2020fast, berthier2020max}.
Different from Generalized Policy Improvement(GPI), the $K$ copies in our max-out structure does not leverage the pretrained policies/operators in the previous tasks, but serve as a network structure that are jointly optimized together. See Appendix~\ref{sec:discuss_design} for more discussion.