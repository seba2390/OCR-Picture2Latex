\section{Reward Transfer with Operator Q-Learning}

\begin{algorithm*}[t] 
\caption{(Offline) Operator Q-Learning}  
\label{alg:operator_q_learning_main}
\begin{algorithmic} 
\STATE {\bf Input}: 
Reward distribution $\rdist$;
Offline Dataset $\D$;
Step size $\varepsilon$, $\alpha$.
\STATE {\bf Initial}:
Network parameter $\theta$ and its parameter of target network $\theta' = \theta$.
\REPEAT
\STATE {\bf Update:}
    \STATE 1. Get transition data $\D_n = \{s_i,a_i,s_i'\}_{i=1}^n$ uniformly random from the offline dataset $\D$.
    \STATE 2. Sample a reward function $r\sim \rdist$.
    \STATE 3. Compute the target 
    $
        ~~\mathcal{Y}_{\theta'} [r](s_i, a_i) \gets r(s_i,a_i) + \gamma 
         \max_{a'\in \Aset}\G_{\theta'}[r](s_i',a'),
        ~~\forall i\in[n]\,.
    $
    \STATE 4. Update
    $
        ~\theta \gets \theta - \varepsilon \nabla_{\theta} \L(\theta, \D_n)\,,
    $
    where 
    $
         \L(\theta, \D_n) = \sum_{i=1}^n \left(\mathcal{Y}_{\theta'} [r](s_i,a_i) - \G_\theta [r](s_i,a_i)\right)^2\,.
    $ 
    \STATE 5. Update the target
    $
        ~~\theta' \gets (1-\alpha)\theta' + \alpha \theta.
    $
\UNTIL{convergence} 
\end{algorithmic} 
\end{algorithm*}

In standard RL we assume a fixed reward function $r$, 
which amounts to solving Eq.~\eqref{eqn:bellman_qpi} or Eq.~\eqref{eqn:bellman_qmax}. 
In many practical cases, however, 
the reward functions in the testing environments 
can be different from the rewards collected from the training environments, 
which requires the agents are able to learn reward transferring in RL. 

We approach the reward transferring problem with an \emph{operator-view} on solving the Bellman equations. 
We introduce operator $\G_{\pi}$ and $\G_*$, 
which map an arbitrary reward function $r\colon \Sset \times \Aset \to \RR$ to its corresponding value functions $q_{\pi,r}$ and $q_{*,r}$, that is, 
\begin{align*} 
q_{\pi,r}(s,a) = \G_{\pi}[r](s,a),  && 
q_{*,r}(s,a) = \G_{*}[r](s,a),
\end{align*}
for any $r$ and $(s,a)\in \Sset\times \Aset$. 
We call $\G_{\pi}$ and $\G_*$ the \emph{resolvent operators}, a term drawn from the study of 
partial differential equations. 

We aim to construct approximations $\hat\G_{\pi} \approx \G_\pi$ and $\hat\G_{*} \approx \G_*$ from offline observation, 
by parameterizing them using theoretically-motivated \emph{operator neural network architectures}.  
In this way, when encountering an arbitrary reward function $r_{test}$ during testing, we can directly 
output estimates of the corresponding value functions 
$q_{\pi,r_{test}} \approx \hat\G_{\pi}[r_{test}]$ and $q_{*,r_{test}} \approx \hat\G_{*}[r_{test}]$, 
without addition policy evaluation nor policy optimization for the new testing reward,
hence enabling \emph{zero-shot reward transferring}. 
Essentially, our method aims to solve the whole family of Bellman equations (Eq.~\eqref{eqn:bellman_qpi} and Eq.~\eqref{eqn:bellman_qmax})
for different reward functions $r$, 
rather than a single equation with a fixed $r$ like typical RL settings. 

Our method consists of two critical components: 
1)  theoretically-motivated designs of neural network structures
tailored to approximating the operators $\G_{\pi}$ and $\G_{*}$,
and 2) an algorithm that estimates the parameterized operators from data. 
In the sequel, we first introduce the estimation algorithm, 
which is a relatively straightforward extension of Q-learning.  %
We then discuss the operator neural network design, 
together with theoretical properties of $G_\pi$ and $\G_*$ in Section~\ref{sec:design}. 


\myparagraph{Operator Deep Q-Learning}
Plugging $q_{\pi,r} = \G_{\pi}[r]$ and $q_{*,r} = \G_{*}[r]$ into Eq.~\eqref{eqn:bellman_qpi}-\eqref{eqn:bellman_qmax}, we have 
\begin{align*} 
&\G_{\pi}[r](x) = r(x) + \gamma \P_\pi[\G_\pi[r]](x)\,, \\
&\G_{*}[r](x) = r(x) + \gamma \Pmax[\G_*[r]](x),~~~~\forall r, ~x \in \X\,.
\end{align*}

Here for simplicity, we write $x = (s,a)$ and $\X = \Sset \times \Aset$ to denote state, action pair and its corresponding space. 
We learn $\G_\pi$ and $\G_*$ by matching these equations on empirical data. 
Let 
$\mathcal R$ be the distribution 
on a set of reward functions $r\colon \Sset\times \Aset \to \RR$, 
which is expected to cover a wide range of $r$ of potential interest during the training phase.
Let $\G_\theta$ be a parameterized operator with a trainable parameter $\theta$. 
We use $\G_\theta$ to approximate $\G_\pi$ or $\G_*$ by minimizing the expected Bellman loss under $r\sim \mathcal R$: 
$$
\min_{\theta} \E_{x\sim \mathcal D,r\sim \mathcal R}  \left [ [(G_{\theta}[r](x) - r(x) - \gamma 
\hat \P[G_\theta[r]](x))^2 \right ],
$$
where $\hat \P$ denotes the empirical estimation of $\P_\pi$ or $\Pmax$ from the offline dataset. 
For example, $\hat{\P_\pi}[f](x_i) = f(s_i', \pi(s_i'))$.
Similar to fitted Q iteration, 
we propose to iteratively update $\theta$ by 
\begin{equation}\label{eqn:fitted_update}
    \theta_{t+1} \gets 
    \arg\min_{\theta}\{ 
    \E_{x\sim \mathcal D,r\sim \mathcal R} [(\G_\theta[r](x)  - \hat{\mathcal Y}_{\theta_t} [r](x) )^2]\,,\!\!
\end{equation}
where $\hat{\mathcal Y}_{\theta_t} [r](x)  = r(x) + \gamma 
\hat\P[\G_{\theta_t}[r]](x)$ denotes the target. 
See Algorithm~\ref{alg:operator_q_learning_main} for more information. %

Our algorithm differs from the standard DQN in two aspects:
1) We sample (different) random reward functions in each iteration rather than using a fixed reward and
2) we replace the Q-function with $\G_\theta[r]$   %
which generalizes to 
different reward functions $r$. 