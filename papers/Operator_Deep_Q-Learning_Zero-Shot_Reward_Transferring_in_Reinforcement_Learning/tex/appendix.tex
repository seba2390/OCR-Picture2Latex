\clearpage
\onecolumn
\appendix

\section{Proof}
\label{sec:appendix_proof}


\subsection{Proof of Proposition~\ref{pro:property_gpi}}
\begin{proof}
For any reward function $r$, we have:
\begin{align*}
    \Gpi[r](x) =& \E_{\tau \sim \pi} 
    \left [\sum_{t=0}^{\infty} \gamma^t r(s_t, a_t)|x_0 = x \right ] \\
    =& \sum_{t=0}^{\infty} \gamma^t \Ppi^t[r](x) \\
    =& \left(\sum_{t=0}^{\infty} \gamma^t \Ppi^t\right)[r](x) \\
    =& \left(I-\gamma\Ppi\right)^{-1} [r](x)\,.
\end{align*}

For the properties, we just need to prove that $\Ppi$ is linear, monotonic and invariant to constant, then $\Gpi = \sum_{t=0}^{\infty} \gamma^t \Ppi^t$ would automatically satisfies all the properties.

By the definition of $\Ppi$, we have:
\begin{enumerate}
    \item Linearity:
    \begin{align*}
        \Ppi[\alpha r_1 + \beta r_2](x) =& \E_{x'\sim p_\pi(\cdot|x)}[\left(\alpha r_1 + \beta r_2\right)(x')] \\
        =& \alpha\E_{x'\sim p_\pi(\cdot|x)}[r_1(x')] + \beta \E_{x'\sim p_\pi(\cdot|x)}[r_2(x')] \\
        =& \alpha\Ppi[r_1](x) + \beta\Ppi[r_2](x).
    \end{align*}
    \item Monotonicity: From linearity, we only need to prove $\Ppi[\Delta](x) \geq 0,~\forall x\in \X$, where
    \begin{align*}
        \Ppi[\Delta](x) = \E_{x'\sim p_\pi(\cdot|x)}[\Delta(x')] \geq 0
    \end{align*}
    \item Invariant to constant function. From the definition of $\Ppi$ we know:
    \begin{align*}
        \Ppi[r_C](x) = \E_{x'\sim p_\pi(\cdot|x)}[r_C(x')] = C
    \end{align*}
\end{enumerate}
\end{proof}

\subsubsection{Proof of Theorem~\ref{thm:approximate_gpi}}
\begin{proof}
Since $r$ is a continous function, for any $\varepsilon > 0$, there exists $\delta > 0$ such that 
$|r(x) - r(y)|\leq \varepsilon$ for $d(x,y)\leq \delta$.

From Eq.~\eqref{eqn:point_evaluation} we have: 
$$\Gpi[r](x) = \frac{1}{1-\gamma}\int_{\X}d_\pi(x'|x)r(x')dx'.$$

Since the domain $\X$ is compact, there exists a sufficiently large $m$ so that we can cover the domain with $m$ balls $\X = \cup_{j=1}^m B_j$ with $B_j$ as a $\delta-$radius ball centered at $\xi_j$:
$$
B_j = \{x\in \X~|~d(\xi_j,x)\leq \delta\}.
$$

Define $L_j$ as the non-overlapping Voronoi tessellation induced by $\xi_j$:
$$
L_j = \{x\in \X~|~ d(x,\xi_j) \leq d(x,\xi_i),~\forall i\neq j\},
$$
we have:
\begin{align*}
    \Gpi[r](x) =& \frac{1}{1-\gamma}\int_{\X}d_\pi(x'|x)r(x')dx' \\
    =& \frac{1}{1-\gamma}\sum_{j=1}^m \int_{L_j} d_\pi(x'|x) r(x') dx' \\
    \in& \frac{1}{1-\gamma}\sum_{j=1}^m w_\pi(L_j|x) (r(\xi_j) \pm \varepsilon) \\
    =& \frac{1}{1-\gamma}\left(\sum_{j=1}^m w_\pi(L_j|x) r(\xi_j)\right) \pm \varepsilon\,,
\end{align*}
where $w_\pi(L_j|x) = \int_{L_j} \frac{d_\pi(x'|x)}{1-\gamma} dx'$ as the conditional probability mass on $L_j$. The inequality leverages the property of the continuous function that any $x'\in L_j$ has $d(x',\xi_j)\leq \delta$ which implies $|r(x') - r(\xi_j)| \leq \varepsilon$.

Notice that we can approximate $w_\pi(L_j|x)$ with any universal function approximator, and $|r(\xi_j)|$ is bounded by $1$, we can safely says that we can approximate $\Gpi[r](x)$ with $\G_\theta[r](x)$ with $\varepsilon$ accuracy.
\end{proof}

\subsection{Proof of Proposition~\ref{pro:property_gmax}}
\begin{proof}
Eq.~\eqref{eqn:maxpi} is immediate from the definition of $q_{*,r}$.
To prove the properties, we can leverage the properties of $\Gpi$:
\begin{enumerate}
    \item Linearity: For any $r_1, r_2$ we have:
    \begin{align*}
        \Gmax[r_1 + r_2](x) =& \max_\pi \Gpi[r_1 + r_2](x) \\
        =& \max_\pi \{\Gpi[r_1](x) + \Gpi[r_2](x)\} \\
        \leq & \max_\pi \{\Gpi[r_1](x)\} + \max_\pi \{\Gpi[r_2]\} \\
        =& \Gmax[r_1] + \Gmax[r_2].
    \end{align*}
    For any $\alpha\in \RR$ we have
    $$\Gmax[\alpha r](x) = \max_\pi \Gpi[\alpha r](x) = \alpha \max_\pi \Gpi[r](x) = \alpha \Gmax[r](x).$$
    \item Monotonicity: Suppose the optimum policy for $r$ is $\pi_{*,r}$, we have:
    \begin{align*}
        \Gmax[r](x) = \G_{\pi_{*,r}}[r](x) \leq \G_{\pi_{*,r}}[r+\Delta](x) \leq \max_\pi \Gpi[r+\Delta](x) = \Gmax[r+\Delta].
    \end{align*}
    \item Invariant to constant function:
    \begin{align*}
        \Gmax[\alpha r + r_C](x) =& \max_\pi \{\Gpi[\alpha r + r_C](x)\} \\
        =& \max_\pi \{\alpha\Gpi[r](x) + C/(1-\gamma)\} \\
        =& \alpha \max_\pi \{\Gpi[r](x)\} + C/(1-\gamma) \\
        =& \alpha \Gmax[r](x) + C/(1-\gamma).
    \end{align*}
\end{enumerate}

\end{proof}


\clearpage
\section{The Adjoint View of Linear Operator}
\label{sec:adjoint}

We take a deeper look into the operator $\Gpi$.
In Proposition \ref{pro:property_gpi}, we know $\Gpi$ is a linear operator.
It is not hard to show under $\L_{\infty}$ space with $\ell_\infty$ norm, $\Gpi$ is a bounded operator:
$$
    \sup_{x\in \X} \Gpi [r](x) \leq \sup_{x\in \X} r(x) / (1 - \gamma)\,.
$$
Notice that any bounded linear operator has its corresponding adjoint operator.
It turns out the adjoint view of $\Gpi$ is closely related to the recent density-based methods in OPE \citep{tang2019doubly, nachum2020reinforcement, uehara2020minimax, jiang2020minimax}. 

\myparagraph{Koopman Operator and Transfer Operator}
To introduce the adjoint operator, we first briefly review the transfer (Perron–Frobenius) operator and its adjoint Koopman operator, which is important in control theory, stochastic process and RL. 
\begin{mydef}\label{def:operator}
For a transfer probability density function $p(y|x)$ in domain $\X$, we define its Koopman operator $\P$ and transfer operator $\P^\dag$ for any $\mu \in \L_1(\X)$, $f\in \L_{\infty}(\X)$ as
\begin{align*}
    \P[f](x) &= \int_{\X} p(y|x) f(y) dy\,, \\ 
    \P^\dag[\mu](y) &= \int_{\X} p(y|x) \mu(x) dx\,.
\end{align*}
\end{mydef}
The transfer operator $\P^\dag$ is also called "forward operator" while the Koopman operator $\P$ is called "backward operator", as they are the solution operators of the forward (Fokker–Planck) and backward Kolmogorov equations\citep{lasota2013chaos}, respectively.
It is easy to show that $\P$ and $\P^\dag$ are adjoint to each other
\begin{align*}
\langle \P^\dag[\mu], f\rangle = \langle \mu, \P[f]\rangle.
\end{align*}
In particular, if we pick $\mu = \delta_x$ as a delta measure, we have:
$$
    \langle p(\cdot|x), f\rangle = \P[f](x) = \langle \delta_x, \P[f]\rangle = \langle \P^\dag[\delta_x], f\rangle,
$$
which implies $p(\cdot|x) = \P^\dag[\delta_x]$.

\myparagraph{Koopman Opeartor in RL}
In RL, our interest of transfer kernel is $p_\pi(x'|x) = p(s'|s,a)\pi(s'|a')$.
The corresponding operators $\Ppi$ and $\Ppistar$ satisfy the following lemma:

\begin{lem}
(\citet{tang2019doubly} Lemma A.2) The two operators $\Ppistar$ and $\Ppi$ are adjoint:  
$
    \langle \Ppistar[\mu], f\rangle = \langle \mu, \Ppi[f] \rangle\,,
$
and the corresponding Bellman equations can be written in an operator view
\begin{align}
    & q_{\pi,r} = r + \gamma \Ppi [q_{\pi,r}]\,, \notag\\
    & d_{\pi}(\cdot|x) = (1-\gamma)\delta_x + \gamma \Ppistar [d_{\pi}(\cdot|x)]\,, \label{eqn:bellman_dpi}
\end{align}
where $d_{\pi}(\cdot|x)$ is defined in Eq.~\eqref{eqn:def_dpi}. 
\end{lem}

The above Lemma gives an adjoint view of the Bellman equation.
In particular, if we consider the adjoint operator of $\Gpi$, from Proposion~\ref{pro:property_gpi} we have:
$$
    \Gpi^\dag = \left(\sum_{t=0}^\infty \gamma^t \Ppi^t\right)^\dag 
    =\sum_{t=0}^\infty \gamma^t  \left(\Ppi^\dag\right)^t = (I-\gamma \Ppistar)^{-1}\,,
$$
and similar to the Bellman equation for $d_\pi$ in Eq.~\eqref{eqn:bellman_dpi}, we have:
$$
   d_\pi(\cdot|x)/(1-\gamma) = \Gpistar[\delta_x] = \delta_x + \gamma \Ppistar[\Gpistar[\delta_x]]\,.
$$
In the adjoint view, our design of $w_\theta$ can be seen as mapping any delta measure to a set of weighted delta measure as:
$$
\Gpistar[\delta_x] \approx \frac{1}{1-\gamma}\sum_{j=1}^m   w_\theta(\xi_j~|~x) \delta(x' = {\xi_j})\,,
$$
which can be viewed as an importance sampling based estimator similar to many recent OPE methods \citep{liu2018breaking}.


\clearpage
\section{Discussion of Different Designs}
\label{sec:discuss_design}

\subsection{Time Complexity of Different Designs of Evaluation Operator}
We mentioned two different design of $w_\theta(\xi_j|x)$ in Section~\ref{sec:design}, one is attention based design in Eq.~\eqref{eqn:attention_design_gpi}
$$
    w_\theta({\xi_j}|x) = \frac{\exp(f_{\theta_f}(\xi_j)^\top g_{\theta_g}(x))}{\sum_{k=1}^m \exp(f_{\theta_f}(\xi_k)^\top g_{\theta_g}(x))}\,,
$$
and another one is the linear decomposition design in Eq.~\eqref{eqn:linear_design_gpi}
$$
    w_\theta({\xi_j}|x) = f_{\theta_f}(\xi_j)^\top g_{\theta_g}(x)\,.
$$

One caveat of attention based design is the time complexity.
Suppose in each iteration we need to do gradient descend on a batch of $b$ desired points $x_i$, to compute $b$ different $\G_\theta[r](x_i)$, we need to evaluate $O(bm)$ number of $w(\xi_j|x_i)$ for $b$ $x_i,~\forall i\in [b]$ and $m$ different $\xi_j,~\forall j\in [m]$.

However, if we are using linear decomposition design, we can achieve a faster time complexity with $O(b+m)$, where we can decompose $\G_\theta$ as:
\begin{align}\label{eqn:linear_decompose_gpi}
    \G_\theta[r](x) =& \frac{1}{1-\gamma}\sum_{i=1}^m w(\xi_j|x) r(\xi_j)\,,\notag \\
    =& \left( \frac{\sum_{i=1} r(\xi_j) f_{\theta_f}(\xi_j)}{1-\gamma}\right)^\top g_{\theta_g}(x)\,,
\end{align}
where $\frac{\sum_{i=1} r(\xi_j) f_{\theta_f}(\xi_j)}{1-\gamma}$ can be firstly computed in time $O(m)$ with $m$ reference poitns $\{\xi_j\}_{j\in [m]}$, and we can reuse it to compute for $\G_\theta[r](x_i),~\forall i\in [b]$, with a total time complexity as O(m+b).


\subsection{Practical Attention-Based Design}
To avoid the multiplicative time complexity, we need to find a way to reduce the time complexity for attention-based design.

\myparagraph{Reduce the number of reference points}
One approach is to reduce the number of reference points $m$.
In practice, we find this simple solution is extremely useful and we can achieve almost the same running time as linear decomposition-based design.
In all the environments we test, we pick $128$ reference points uniformly random from the whole offline dataset $\D = \{x_i, r_i\}$ and fix it for later operator network structure.
It turns out that increasing the number of reference points can only have very small performance gain in general.
Although our operator network design is different from \cite{lu2019deeponet}, but the effect for the number of reference points is similar; see \citet{lu2019deeponet} for more quantitative discussion.

\myparagraph{Approximate by random feature attention}
\citet{peng2021random} proposed a way to approximate attention network by random feature. 
The approximation structure will eventually become a linear decomposition model which can reduce the time complexity to $O(b+m)$.
In this way, we can incorporate more reference points into the network design.
We will leave this to future work once the codebase of random feature attention in \citet{peng2021random} is available.

\subsection{Connection with General Policy Improvement(GPI)}
Our max-out architecture is similar to General Policy Improvement(GPI) \citep{barreto2018transfer} where we all consider a max-out structure in reward transferring.
However,
GPI does not learn its policies and its corresponding successor features directly from the offline dataset, but the policies and its corresponding successor features are picked from previous tasks with different rewards.
For example, if we are given $m$ pretrain tasks with reward $r_1, r_2,\ldots, r_m$, GPI will train $m$\footnote{It can be less than $m$ if the max-out of previous policies/successor features already yields a good performance} differnt policies $\pi_i$ where $\pi_i$ is the trained policy specifically for reward $r_i$ which is trained in a online manner.
Compared with GPI, our different $K$ copies of $w_{\theta_k}$ is randomly initialed and jointly trained in a offline manner. Since it is just served as part of the network structure, there is no exact meaning for each of the $w_{\theta_k}$.

\clearpage
\section{More Related Works}
\myparagraph{Universal Value Function and Successor Features}
The notion of \textit{reward transfer} is not new in reinforcement learning, and has been studied in literature.
Existing methods aim to capture a \textit{concept} that can represent a reward function.
By leveraging the concept into the design of the value function network, the universal value function can generalize across different reward functions.
Different methods leverage different concepts to represent the reward function.
Universal value function approximators (UVFA) \citep{schaul2015universal} considers a special type of reward function that has one specific goal state, i.e. $r_g(s,a) = f(s,a, g)$,
and leverage the information of goal state into the design;
Successor features (SF) \citep{barreto2017successor,barreto2018transfer,borsa2018universal, barreto2020fast} considers reward functions that are linear combinations of a set of basis functions, i.e. $r = w^\top \phi$,
and leverage the coefficient weights $w$ into the design.
Both methods rely on the assumption of the reward function class to guarantee generalization.
And typically they cannot get access to the actual concept directly, 
and need another auxiliary loss function to estimate the concept from the true reward value \citep{kulkarni2016deep}.
Our method is a natural generalization on both methods and can directly plug in the true reward value directly.

\myparagraph{Multi-objective RL, Meta Reinforcement Learning and Reward Composing}
Multi-objective RL \citep[e.g.][]{roijers2013survey,van2014multi, li2019multi, yu2020gradient} 
deals with learning control policies to simultaneously optimize over several criteria.
Their main goal is not transferring knowledge to a new unseen task, but rather cope with the conflict in the current tasks.
However, if they consider a new reward function that is a linear combination of the predefined criteria functions \citep{yang2019generalized}, e.g. lies in the optimal Pareto frontiers of value function, then it can be viewed as a special case of SF, which is related to our methods.

Meta reinforcement learning \citep[e.g.,][]{duan2016rl, finn2017model, nichol2018first, xu2018meta, rakelly2019efficient, Zintgraf2020VariBAD} can be seen as a generalized settings of reward transfer, where the difference between the tasks can also differ in the underlying dynamics. 
And they usually still need few-shot interactions with the environment to generalize, differ from our pure offline settings.

Works on reward composing \citep{van2019composing, tasse2020boolean} propose to compose reward functions in a boolean way. However, their setting is a special MDP where the reward functions of interest only differ at the absorbing state sets.

\myparagraph{Reward Free RL}
Recent works on reward (task) free RL \citep[e.g.][]{wang2020reward,jin2020reward, zhang2020task} break reinforcement learning into two steps: exploration phase and planning phase.
In the exploration phase, they don't know the true reward functions and only focus on collecting data by exploration strategy.
In the planning phase, they receive a true reward function and based on the data collected in the exploration phase, 
Our method can be seen as an intermediate phase in between to help reward transfer in the planning phase, where the zero-shot transfer can serve as a good initial during planning phase.

\myparagraph{Off Policy Evaluation(OPE)}
Our design of resolvent operator $\Gpi$ is highly related to the recent advances of density-based OPE 
methods \citep[e.g.,][]{liu2018breaking, nachum2019dualdice, tang2019doubly,mousavi2019black, zhang2020gendice,zhang2020gradientdice}, see more discussion in Section~\ref{sec:adjoint}.
However, density-based OPE methods usually focus on a fixed initial distribution 
while our conditional density in Eq.~\eqref{eqn:def_dpi} can be more flexible to handle arbitrary initial state-action pairs.
And for value-based methods, such as Fitted Q-Evaluation(FQE) \citep[e.g,][]{voloshin2019empirical}, though empirically better than density-based ones \citep{fu2021benchmarks}, usually cannot handle multiple reward functions simultaneously.

\clearpage
\section{Experimental Details}
\label{sec:exp_appendix}

\subsection{Reward Design}

\myparagraph{Pendulum-Angle}
The original reward function for Pendulum environment can be written as: 
$$
r(s,a) = -(\theta^2 + 0.1 * \mathrm{vel}^2 + 0.001 * \mathrm{force}^2)\,,
$$
where $\theta$ is the angle of the bar, $\mathrm{vel}$ is the angular velocity and the action is the angular force,
and the observation $s = [\cos(\theta), \sin(\theta), \rm{vel}]^\top$.

To change it into a multi reward environemnt, we consider:
\begin{align*}
    &r(s,a|\theta_0)= -((\theta-\theta_0)^2 + 0.1 * \rm{vel}^2 + 0.001 * \rm{force}^2)\,,
\end{align*}
where in training phase, we randomly sample $32$ training rewards uniformly from:
$$
    \F_{train} = \{r(\cdot|\theta_0): \theta_0\in [-0.4 * \pi, 0.4 * \pi]\},
$$
and in testing phase, we randomly sample $16$ testing rewards uniformly from:
$$
    \F_{test} = \{r(\cdot|\theta_0): \theta_0\in [-0.6 * \pi, 0.6 * \pi]\}.
$$
Notice that $\F_{train}$ does not cover $\F_{test}$, our design aims to see generalizability of different methods.





\myparagraph{HalfCheetah-Vel}
HalfCheetah-Vel is adapted from \citet{finn2017model}, where the goal is to achieve a target velocity running forward.
The reward function followed exactly as \citet{rakelly2019efficient} codebase as:
$$
   r(s,a|v_{target}) = -1.0 * |v-v_{target}| - 0.05 * \|a\|_2^2.
$$
where $v$ is the average velocity in the x-axis and $a$ is the vector of action.
For training tasks, our $32$ target velocities $v_{target}$ are sample uniformly random from $[0.7, 1.3]$; and for testing tasks, our $16$ target velocities are sample uniformly from $[0.6, 1.4]$.

\myparagraph{Ant-Dir}
Ant-Dir is adapted from \citet{rakelly2019efficient} codebase\footnote{\url{https://github.com/katerakelly/oyster/tree/master/rlkit/envs}}, where the goal is to keep a 2D-Ant moving in a given direction.
The reward function can be written as:
$$
    r(s,a|\theta) = (v_x * \cos(\theta) +  v_y * \sin(\theta))
    - 0.5 * \|a\|_2^2 - 0.005 * \rm{contact~cost} + 1.0,
$$
where the contact cost can be computed using the state, and the $1.0$ is the survival reward to prevent the ant to suicide at the initial training.
For training tasks, we sample $32$ $\theta$ randomly from $[-\frac{\pi}{4},\frac{\pi}{4}]$ and for testing we sample $16$ $\theta$ randomly from $[-\frac{\pi}{3},\frac{\pi}{3}]$.

\subsection{Offline Dataset Construction Details}
For offline dataset construction, we exactly follow \citet{fujimoto2019off} and collect the offline dataset as (1) final replay buffer during training the target policy using TD3 or (2) sample from a behavior policy follow the target policy but with probability $p$ to select actions randomly and with an exploratory noise $\N(0,\sigma^2)$ added to the action in the other $1-p$ probability. See \citet{fujimoto2019off} database\footnote{\url{https://github.com/sfujim/BCQ/tree/master/continuous_BCQ}} for more details.

For the hyper-parameter of $p$ and $\sigma$, see Table~\ref{tab:offline_construction} for more details for each environment.

\begin{table}[h]
    \centering
    \begin{tabular}{|c|c|c|c|}
    \hline 
        & Dataset size $n$ & Random Action Probability $p$ & Noise variance $\sigma$ \\\hline 
        Pendulum-Angle Experts & 2e4 & 0.1 & 0.3 \\\hline 
        Pendulum-Angle Medium & 2e4 & 0.3 & 0.3\\\hline 
        HalfCheetah-Vel Expert& 1e5& 0.1 & 0.1\\\hline 
        HalfCheetah-Vel Medium& 1e5& 0.3 & 0.1\\\hline 
        Ant-Dir Expert &2e5 & 0.1 & 0.1\\\hline 
        Ant-Dir Medium &2e5 & 0.3 & 0.1\\\hline 
    \end{tabular}
    \caption{Parameter of behavior policy for different environments.}
    \label{tab:offline_construction}
\end{table}

\subsection{Modified Operator BCQ in Mujoco Environment}
In high dimension environment such as HalfCheetah-Vel and Ant-Dir, it is impossible to discretize the action space and apply operator DQN.
Thus we implement an actor-critic style policy optimization adapted from the current state of the art offline policy optimization method BCQ\citep{fujimoto2019off}, where the actor is a combination of an imitated actor $g_w:\Sset\to\Aset$ and a perturbation network $\zeta_\phi$.
We use exactly the same encoder-decoder structure as our imitation network $g_w$ as BCQ;
for the perturbation network, since now it depends on the different reward function, we use a high dimention vanilla operator network to form the mapping from
$r$ to $\zeta$ as an operator network, where the final action $a$ is sampled from
$$
    a \sim \rm{Clip}(\zeta_\phi[r](s,g_w(s)) + g_w(s)).
$$
This can be a naive extension using vanilla operator network for reward transfer in BCQ.
See our code in supplementary material for more details.

Since the implementation of combining BCQ is not the main purpose of our paper, we haven't tried other methods yet, so we think there is still a large room to improve in future.
\subsection{Other Details}

\myparagraph{Training Hyper-parameter}
The hyper-parameters are summarized in Table~\ref{tab:hyper_parameter},
where we pick Adam as our optimizer, and for all tasks we set the learning rate $\varepsilon$ as $0.001$, and the target network update rate $\alpha$ is $0.005$ and the batch size is $256$.
All attention-based method we fixed the number of reference points $m = 128$.
And we set the Max-out repetition $K$ in policy optimization as $8$.
\begin{table}[h]
    \centering
    \begin{tabular}{cc}
    \hline
        Hyper-parameter & Value \\
    \hline
        Optimizer & Adam \\
        Learning Rate $\varepsilon$ & 0.001\\
        Target Network Update Rate $\alpha$ & 0.005 \\
        Batch Size & 256  \\
        Number of reference points $m$ for Attention & 128 \\
        Max Out Size $K$ & 8\\
    \hline
    \end{tabular}
    \caption{Hyper-parameter for implementation.}
    \label{tab:hyper_parameter}
\end{table}

\myparagraph{Training Speed}
Each offline experiment takes approximate $5-30$ minutes with one RTX 2080 ti GPU depending on environment and task with $15000 - 50000$ total training steps.

