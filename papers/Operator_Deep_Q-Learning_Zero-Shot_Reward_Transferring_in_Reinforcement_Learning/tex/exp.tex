\section{Experiments}
\label{sec:experiment}

\newcommand{\figsize}{0.14}
\begin{figure}
    \centering
    \includegraphics[width =.95\linewidth]{figs/ope_legend.pdf}
    \begin{tabular}{cccccc}
    \raisebox{0.5em}{\rotatebox{90}{\tiny Average MSE}}
    \includegraphics[width = \figsize\textwidth]{figs/PendulumAngle_full_rb_train.pdf} &
    \includegraphics[width = \figsize\textwidth]{figs/PendulumAngle_full_rb_test.pdf} &
    \includegraphics[width = \figsize\textwidth]{figs/PendulumAngle_std=0.3_eps=0.1_train.pdf} &
    \includegraphics[width = \figsize\textwidth]{figs/PendulumAngle_std=0.3_eps=0.1_test.pdf} &
    \includegraphics[width = \figsize\textwidth]{figs/PendulumAngle_std=0.3_eps=0.3_train.pdf} &
    \includegraphics[width = \figsize\textwidth]{figs/PendulumAngle_std=0.3_eps=0.3_test.pdf} \\
    
    \raisebox{0.5em}{\rotatebox{90}{\tiny Average MSE}}
    \includegraphics[width = \figsize\textwidth]{figs/HalfCheetahVel_full_rb_train.pdf} &
    \includegraphics[width = \figsize\textwidth]{figs/HalfCheetahVel_full_rb_test.pdf} &
    \includegraphics[width = \figsize\textwidth]{figs/HalfCheetahVel_std=0.1_eps=0.1_train.pdf} &
    \includegraphics[width = \figsize\textwidth]{figs/HalfCheetahVel_std=0.1_eps=0.1_test.pdf} &
    \includegraphics[width = \figsize\textwidth]{figs/HalfCheetahVel_std=0.1_eps=0.3_train.pdf} &
    \includegraphics[width = \figsize\textwidth]{figs/HalfCheetahVel_std=0.1_eps=0.3_test.pdf} \\
    \raisebox{0.5em}{\rotatebox{90}{\tiny Average MSE}} 
    \includegraphics[width = \figsize\textwidth]{figs/AntDir_full_rb_train.pdf} &
    \includegraphics[width = \figsize\textwidth]{figs/AntDir_full_rb_test.pdf} &
    \includegraphics[width = \figsize\textwidth]{figs/AntDir_std=0.1_eps=0.1_train.pdf} &
    \includegraphics[width = \figsize\textwidth]{figs/AntDir_std=0.1_eps=0.1_test.pdf} &
    \includegraphics[width = \figsize\textwidth]{figs/AntDir_std=0.1_eps=0.3_train.pdf} &
    \includegraphics[width = \figsize\textwidth]{figs/AntDir_std=0.1_eps=0.3_test.pdf} \\
    
    {\tiny Final Buffer Training} & {\tiny Final Buffer Testing} 
    &{\tiny Expert Training} & {\tiny Expert Testing} 
    &{\tiny Medium Training} & {\tiny Medium Testing} \\
    \end{tabular}
    \caption{Offline policy evaluation in different continuous control environment. Repeat for 10 different seeds for network initialization. Each row represent a different environment: Pendulum-Angle, HalfCheetah-Vel and Ant-Dir. Each column represent training or testing MSE in different offline dataset. All the x-axis is the training time steps and y-axis is the average MSE.}
    \label{fig:ope_result}
\end{figure}

We demonstrate the effectiveness of our newly designed operator networks in both offline policy evaluation and policy optimization.
$\rdist$ is an uniform distribution on a set of rewards known before hand.
We test both the performance of the value function on the training rewards as well as a set of new testing rewards.

We mainly compare our different designs of operator networks with successor feature \citep{barreto2017successor} in offline policy evaluation case.
To make fair comparison, each time the reward sampler will give all the training rewards value: for successor representation, the multiple training rewards serve as basis functions; for operator q-learning, these serve as multiple times of randomly sampled picked from the reward sampler.
Since our setting is for offline, it is not fair to compare with Generalized Policy Improvement(GPI) in the policy optimization case.

\subsection{Reward Transferring for Offline Policy Evaluation}
\myparagraph{Environment and Dataset Construction}
We conduct experiments on three continuous control environments: Pendulum-Angle, HalfCheetah-Vel and Ant-Dir;
Pendulum-Angle is an environment adapted from Pendulum, a classic control environment whose goal is to swing up a bar and let it stay upright. 
We modify its goal into swinging up to a given angle, sampled randomly from a reward sampler.
Ant-Dir and HalfCheetah-Vel are standard meta reinforcement learning baseline adapted from \citet{finn2017model}.

We use online TD3 \citep{fujimoto2018addressing} to train a target policy $\pi$ on the original predefined reward function for a fixed number of iterations.
The offline dataset is collected by either: 1) the full replay buffer of the training process of TD3, or 2) a perturbing behavior policy with the same size , where the behavior policy selects actions randomly with probability $p$ and with high exploratory noise $\N(0, \sigma^2)$ added to the remaining actions followed \citet{fujimoto2019off}. See Appendix~\ref{sec:exp_appendix} for more details of the construction of offline dataset and the designs of training and testing reward functions for different environments. 

\myparagraph{Criteria}
We evaluate the zero-shot reward transfer ability of operator network during the training process, where we feed both the training reward functions (the fixed random sampled training rewards we use for training) and the unseen random testing reward functions into our operator network, and evaluate the performance of our predicted value function $\G_\theta[r]$ with the true $q_{\pi,r}$ with mean square error(MSE) metric:
$$
\mathrm{MSE} = \frac{1}{n_{\mathrm{test}}}\sum_{i=1}^{n_{\mathrm{test}}} (q_{\pi,r}(y_i) - \G_\theta[r](y_i))^2\,,
$$
where $\{y_i\}_{i=1}^{n}$ are the state-action pairs whose states are drawn from the initial distribution of the environment, and $q_{\pi,r}(y_i)$ is the ground truth action value function computed from the trajectory drawn from target policy $\pi$. 
Notice that TD3 provides a deterministic policy and the dynamics is also deterministic, so we just need to collect one trajectory for each initial $y_i$.
For all the plots, we report the average MSE for multiple training and testing rewards.

\myparagraph{Results}
Figure~\ref{fig:ope_result} shows the comparison results with {\tt Successor Feature}, {\tt Attention-based Operator} network, {\tt Linear-based Operator} network and {\tt Vanilla Operator} network.
We can see that the Attention-based operator network achieves a much better initialization thanks to its self-normalized nature, thus it converges faster than any other methods in all setups.  
It is worth to mention that in the extreme case when the offline dataset is close to the target policy, the initialization (almost equal weight) of Attention-based structure can achieve even better performance than the one after training (HalfCheetah Final Buffer and Expert Behavior).
Compared with successor feature, which needs linear assumption to guarantee generalization, all the operator networks achieve more stable generalization performance in testing reward transferring.

\newcommand{\newfigsize}{0.3}
\begin{figure}
    \centering
    \includegraphics[width =.95\linewidth]{figs/opt_legend.pdf}
    \begin{tabular}{ccc}
    \raisebox{0.5em}{\rotatebox{90}{\small Episodic Reward}}
    \includegraphics[width = \newfigsize\textwidth]{figs/pi_PendulumAngle_full_rb.pdf} &
    \includegraphics[width = \newfigsize\textwidth]{figs/pi_HalfCheetahVel_full_rb.pdf} &
    \includegraphics[width = \newfigsize\textwidth]{figs/pi_AntDir_full_rb.pdf}\\
    {\small (a) Pendulum-Angle}  & {\small (b) Half-CheetaVel} & {\small (c) Ant-Dir}\\
    \end{tabular}
    \caption{Offline policy optimization in different continuous control environment. Repeat for 10 different seeds for network initialization. All the x-axis is the training time steps and y-axis is the episodic rewards for the predefined testing reward function.}
    \label{fig:opi_result}
\end{figure}


\subsection{Reward Transferring for Offline Policy Optimization}
We compare the (Attention-based) {\tt Max-out Operator} and the {\tt Vanilla Operator}
on policy optimization tasks. 
All the task settings and the offline dataset collection processes 
are exactly the same as previous section.
However, since the dataset is constructed mainly by one trained target policy for a single predefined reward function, it is impossible to perform well on transferring to another reward with offline training due to distribution shift, thus we only pick the predefined reward function as our testing reward.
For Pendulum-Angle environment, we discretized the action domain and perform standard Operator DQN; for other two high dimension domains, it is impossible to discretize the action space, so we modified Batch Constrained deep Q-learning\citep{fujimoto2019off} into an operator version; see Appendix~\ref{sec:exp_appendix} for more details.

\myparagraph{Criteria}
We evaluate the zero-shot reward transfer ability of the operator network, where we evaluate the episodic reward on the current policy (for discrete action we pick the maximum of the critic value as policy) if we feed the operator network with the test reward.

\myparagraph{Results}
Both operator networks can achieve almost the same performance of the converged best target policy trained online with TD3 even in a zero-shot transferring.
For Ant-Dir, the TD3 has not converged to optimum policy, the offline optimization can even outperform the target policy using the same replay buffer.
