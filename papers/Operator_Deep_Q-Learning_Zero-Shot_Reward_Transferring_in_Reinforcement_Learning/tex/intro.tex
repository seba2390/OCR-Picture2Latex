\section{Introduction}
Reinforcement learning (RL), especially when equipped with powerful deep neural networks, has achieved remarkable success in domains such as playing games \citep[e.g.,]{mnih2015human,silver2018general, vinyals2019grandmaster}, quadrupedal locomotion \citep{haarnoja2018soft} and autonomous driving \citep{kendall2019learning, bellemare2020autonomous}.
However, the standard RL framework is targeted for expertizing a \textit{single} task.
In real life scenarios we may hope our intelligence agents not only are able to expertize in one single task but can also adapt to  unseen new tasks quickly.

We are interested in  \textit{reward transfer} in RL, 
where learned RL agents need to act optimally under different reward functions, 
while the environment transition dynamics for different tasks remain the same.
Recently, reward transfer in RL has attracted wide attentions.
Existing works \citep[e.g.,][]{schaul2015universal, barreto2017successor, barreto2018transfer, borsa2018universal, barreto2020fast} 
provide frameworks that can leverage the \textit{concept} of reward function into the design of value function.
The concept can be the goal of the reward function, or the linear coefficients of a predefined set of basis functions.
However, all these prior proposed frameworks heavily depend on special assumption on the class of reward function, 
for example, universal value function need to get access to the goal state.
A more general framework that allows transferring on arbitrary rewards is still in demand.



In this paper, we consider directly leveraging the reward function value into the design of the (Q-) value function.
We consider an \textit{operator-view} to map a certain reward function to the value function. 
Instead of learning an approximated value function for a specific reward, we learn the approximated \textit{operator} that maps the reward function to its corresponding value function. 
We term this type of operator as \textit{resolvent operator}, 
following the literature of partial differential equations \citep{yosida1971functional}.
The training of the resolvent operator can be seen as a straightforward extension of Q-learning, 
thus we name our new training algorithm \textit{Opeartor Deep Q-Learning}. 

The main difference between operator Q-learning and standard Q-learning is that we sample different reward functions from a predefined reward sampler during the training phase to fit Bellman equations.
And during the testing phase when an unseen test reward function comes, 
our learned operator can map the test function to its corresponding value function directly which yields the (deterministic) optimal policy in a \textit{zero shot} manner.

To approximate the resolvent operator, we need to seek a universal approximator that can approximate any operator.
Recently, \citet{lu2019deeponet} proposed a general purpose way to represent any nonlinear operator by deep neural networks.
However, the  architecture of the general operator neural networks does not take the special properties of resolvent operators into account. 
To address this problem, we advocate a novel design of the resolvent operator
to satisfy a list of axiomatic theoretical properties of the resolvent operator, hence yielding better practical performance. 



Experimental results indicate that our operator deep Q-learning can successfully transfer to an unseen reward in a zero shot manner, and achieve better performance compared to 
existing methods
especially in policy evaluation.

\myparagraph{Main Contribution}
Our main contribution is three-fold: 
Firstly, we propose a unified operator view of reinforcement learning which is connected to various topics in RL such as reward transfer in RL, multi-objective RL and off policy evaluation (OPE);
Secondly, by studying the properties of the resolvent operator, we design novel architectures which outperform the vanilla designs;
Thirdly, we conduct a range of experiments to strengthen the benefits of our framework.