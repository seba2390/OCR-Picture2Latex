\section{Related Works}

\myparagraph{Universal Value Function and Successor Features}
The notion of \textit{reward transfer} is not new in reinforcement learning, and has been studied in literature.
Existing methods aim to capture a \textit{concept} that can represent a reward function.
By leveraging the concept into the design of the value function network, the universal value function can generalize across different reward functions.
Different methods leverage different concepts to represent the reward function.
Universal value function approximators (UVFA) \citep{schaul2015universal} considers a special type of reward function that has one specific goal state, i.e. $r_g(s,a) = f(s,a, g)$,
and leverage the information of goal state into the design;
Successor features (SF) \citep{barreto2017successor,barreto2018transfer,borsa2018universal, barreto2020fast} considers reward functions that are linear combinations of a set of basis functions, i.e. $r = w^\top \phi$,
and leverage the coefficient weights $w$ into the design.
Both methods rely on the assumption of the reward function class to guarantee generalization.
And typically they cannot get access to the actual concept directly, 
and need another auxiliary loss function to estimate the concept from the true reward value \citep{kulkarni2016deep}.
Our method is a natural generalization on both methods and can directly plug in the true reward value directly.

\myparagraph{Multi-task/Meta Reinforcement Learning}
Multi-objective RL \citep[e.g.][]{roijers2013survey,van2014multi, li2019multi, yu2020gradient} 
deals with learning control policies to simultaneously optimize over several criteria.
Their main goal is not transferring knowledge to a new unseen task, but rather cope with the conflict in the current tasks.
However, if they consider a new reward function that is a linear combination of the predefined criteria functions \citep{yang2019generalized}, e.g. lies in the optimal Pareto frontiers of value function, then it can be viewed as a special case of SF, which is related to our methods.

Meta reinforcement learning \citep[e.g.,][]{duan2016rl, finn2017model, nichol2018first, xu2018meta, rakelly2019efficient, Zintgraf2020VariBAD} can be seen as a generalized settings of reward transfer, where the difference between the tasks can also be different in the underlying dynamics. 
And usually they still need few-shot interactions with the environment to generalize, differ from our pure offline settings.


\myparagraph{Off Policy Evaluation(OPE)}
Our design of resolvent operator $\Gpi$ is highly related to the recent advances of density-based OPE 
methods \citep[e.g.,][]{liu2018breaking, nachum2019dualdice, tang2019doubly,mousavi2019black, zhang2020gendice,zhang2020gradientdice}, see more discussion in Section~\ref{sec:adjoint}.
However, density-based OPE methods usually focus on a fixed initial distribution 
while our conditional density in Eq.~\eqref{eqn:def_dpi} can be more flexible to handle arbitrary initial state-action pairs.
