\section{Deterministic scaling limit of stochastic processes}\label{app:theorem}

In order to show the deterministic scaling of online SGD under a proper chosen time scale, we will make use of a convergence result by \cite{wang_2018, wang_2019}, which is adapted below in Theorem \ref{th:wang}.

\begin{theorem} [Deterministic scaling limit of stochastic processes] \label{th:wang} Consider a $\inp$-dimension discrete time stochastic process sequence, $   \{  \Om^\i  \; ; \; \i = 0, 1, 2, ..., [\S \T ] \}_{\S = 1, 2, ...}$ for some $ \T > 0$. The increment $\Om^{\i + 1} - \Om^\i $ is assumed to be decomposable into three parts, 
\begin{equation}
\label{eq:process_3parts}
    \Om^{\i + 1} - \Om^\i = \frac{1}{\S} \psi (  \Om^\i     ) + \bm{\Lambda}^\i + 
    \bm{\Gamma}^\i     \;,
\end{equation}
such that 
\begin{assump}
\label{A1}
The process $\tilde{\bm{\Lambda}}^\i \equiv \sum_{\nu' = 0}^\i \bm{\Lambda}^{\nu'}$ is a martingale and $\E \norm{ \bm{\Lambda}^\i }^{2}  \le C(\T)^2 / \S^{1 + \epsilon_1}$ for some $\epsilon_1 > 0$.
\end{assump}
\begin{assump}
\label{A2}
$ \E  \norm{ \bm{\Gamma}^\i  } \le C(\T) / \S^{1 + \epsilon_2}$ for some $\epsilon_2 > 0$.
\end{assump}
\begin{assump}
\label{A3}
 The function $\psi (  \Om )$ is Lipschitz, i.e, $ \norm{ \psi (  \Om ) - \psi (  \tilde{\Om} )  } \le C  \norm{ \Om-\tilde{\Om}}$ for any $\Om$ and $\tilde{\Om}$.
\end{assump}

Let $\Om(\alp)$, with $ 0 \le \alp \le \T $, be a continuous stochastic process such that $ \Om(\alp) = \bm{\Om}^\i$ with $\i = [\S\alp]$. Define the deterministic ODE
\begin{equation}
\label{eq:ODE_lemma}
   \dv{\alp}  \bar{\Om} (\alp) = \psi ( \bar{\Om}(\alp)   ) \;,
\end{equation}
with $  \bar{\bm{\Om}}(0)  =  \bar{\Om}_0  $.

Then, if assumptions \ref{A1} to \ref{A3} hold and assuming $  \E \norm{\Om^0 - \bar{\Om}_0 }  < C / \S^{\epsilon_3} $ for some $ \epsilon_3 > 0$ then we have for any finite $\S$:
\begin{equation}\label{eq:nonasymp_bound}
\E\norm*{\Om^\i -\bar{\Om} \left( \frac{\i}{\S} \right)  } \le C(\T) e^{c\T}   
     \S^{ -\min \{ \frac{1}{2} \epsilon_1 , \epsilon_2 , \epsilon_3 \}  }\;,
\end{equation}
where $\bar{\Om} (\cdot)$ is the solution of Eq.(\ref{eq:ODE_lemma}).

\begin{proof}
The reader interested in the proof is referred to the supplementary materials of \cite{wang_2018,wang_2019}.
\end{proof}

\end{theorem}

Although the theorem wasn't originally proven in the $p \to \infty$ setting, a glance at its proof shows that it still holds upon replacing $C(\tau)$ by $C(p, \tau)$ in Assumption \ref{A1} and \ref{A2}, as well as Equation \eqref{eq:nonasymp_bound}. We choose $\norm{\cdot}$ to be the $L^\infty$ norm, since it suits better the $p \to \infty$ scaling. The $S$ in Theorem \ref{th:wang} corresponds to $1 / \delta t$, where $\delta t$ is defined in Theorem \ref{th:conv_eps}.

Following \cite{wang_2018}, we define for $j, l \in [p]$
\[ \Psi_{jl}(\Om; \x) = \frac{\lr}{\hids\inp \, \delta \alp} \left( \Er_{j}^\i \lf_{l}^\i  + \Er_{l}^\i \lf_{j}^\i  \right)+ \frac{\lr^2 }{\hids^2\,\inp\, \delta \alp}\Er_{j}^\i \Er_{l}^\i, \]
and
\[ \psi_{jl}(\Om) = \Expxgauss\left[ \Psi_{jl}(\Om; \x ) \right]. \]
The functions $\Psi, \psi$ are similarly defined on $[p] \times [p+1, p+k]$. With that, we write
\[ \Om^{\i+1} - \Om^\i = \frac1S \psi(\Om) + \underbrace{\frac1S\left(\Psi(\Om^\i; \x ) - \psi(\Om^\i)\right)}_{\bm\Lambda^\i} + \bm\Gamma^\i, \]
where for $j, l \in [p]$
\[ \Gamma^\i_{jl} = \frac{\gamma^2}{p^2d^2} \left( \lVert \x \rVert_2^2 - d\right) \Er_j^\i\Er_l^\i. \]

The main obstacle to bounding $\bm\Lambda^\i$ and $\bm \Gamma^\i$ is the fact that the $q_{jj}$ can a priori diverge to infinity. Our first task is therefore to show that this does not happen; as a proxy we show a subgaussian-like moment bound:
\[ \E\left[(q_{jj}^\i)^t\right] \leq \left(C(\T) + \frac{c t}{S}\right)^t. \]

Equipped with the above bound, controlling 
%$\E[\lVert\Lambda^\i\rVert^2]$
$ \E \norm{ \bm{\Lambda}^\i }^{2} $
and
%$\E[\lVert\Gamma^\i\rVert]$
$\E  \norm{ \bm{\Gamma}^\i  } $ 
becomes fairly easy. All proof details are in the below sections.

%\subsection{Preliminaries: bounding the $q_{ii}$}
\subsection{Preliminaries: bounding the \texorpdfstring{$q_{jj}$}{qjj}}

Since $\act$ is $L$-Lipschitz, we have by the Cauchy-Schwarz inequality
\begin{equation}\label{eq:app:bound_error}
(\Er^\i)^2 \leq \frac {3L^2} \hidt \sum_{r=1}^\hidt (\lf_r^*)^2 + \frac {3L^2} \hids \sum_{j=1}^\hids (\lf_j)^2 + 3\noise \noisevar^2 \equiv \Phi^\i
\end{equation}
Define
\[ s^\i = \E\Phi ^\i =  \frac {3L^2} \hidt \sum_{r=1}^\hidt \p_{rr} + \frac {3L^2} \hids \sum_{j=1}^\hids \q_{jj}^\i + 3\noise\]
Assumption \ref{ass_main:2} in Theorem \ref{th:conv_eps} implies that
\[ |\q_{jj}^{\i+1} - \q_{jj}^\i| \leq \frac 1 S \left(c_1 (\lambda_j^\i)^2 + c_2 (\Er^\i)^2 \right) \]
where $c_1, c_2$ are absolute constants. Summing those inequalities yield
\[ |s_{\i+1} - s^{\i}| \leq \frac{c_3}S \Phi^\i,  \]
and finally
\[ \E_\i[s^{\i+1}] \leq s^\i \left(1 + \frac{c_3}{S}\right) \leq s^\i e^{c_3/S}. \]
As a result, we have for any $0 \leq \i \leq S \tau$
\begin{equation} 
\E[s^\i] \leq c_4 e^{c_3 \tau}.
\end{equation}
For simplicity, let $q^\nu$ denote any of the $q_{jj}^\nu$. We have, for all $t \geq 0$, 
\[ (\q^{\i+1})^t - (\q^{\i})^t = t (\q^{\i})^{t-1} (\q^{\i+1} - q^\i) + O\left(\frac{t^2}{S^2}\right), \]
where the remainder term has bounded expectation. Again, we write
\[ \left|(\q^{\i+1})^t - (\q^{\i})^t \right| \leq t (\q^{\i})^{t-1} \frac1S(c_1 (\Er^\i)^2 + c_2 (\lf^\i_i)^2) + \frac{c_5 t^2}{S^2}. \]
By Assumption \ref{ass_main:3}, the $q_{ii}^\i$ are bounded from below by a constant, hence
\[ \E_\i[(\q^{\i+1})^t] \leq  (\q^{\i})^t \left(1 + \frac{c_6 t}{S} \right) + O\left(\frac{c_5 t^2}{S^2}\right)  \]
This implies that for any $t \geq 0$ and $0 \leq \i \leq S\T$,
\begin{equation}\label{eq:bound_q_powers}
    \E[(\q^{\i})^t] \leq \left(c_7 + \frac{c_5t^2}{S}\right)e^{c_6 \T} \leq \left(C(\T) + \frac{c_5 t}{S}\right)^t
\end{equation}

\subsection{Assumption \ref{A1}}

We have for all $i, j \in [p+k]$,
%\[ \left(\Om_{ij}^{\i+1} - \E_\i[\Om_{ij}^{\i+1}]\right)^2 \leq 2\left((\Om_{ij}^{\i+1} - \Om_{ij}^{\i})^2 + (\Om_{ij}^{\i} - \E_\i[\Om_{ij}^{\i+1}])^2\right) \]
\begin{equation}
    \nonumber
   \left(\Omega_{ij}^{\i+1} - \E_\i [\Omega_{ij}^{\i+1}]\right)^2 \leq 2\left((\Omega_{ij}^{\i+1} - \Omega_{ij}^{\i})^2 + (\Omega_{ij}^{\i} - \E_\i[\Omega_{ij}^{\i+1}])^2\right) \;.
\end{equation}
As a consequence,
%\[ \E \lVert \Lambda^\i \rVert^2 \leq 4 \max_{i,j} (\Om_{ij}^{\i+1} - \Om_{ij}^{\i})^2 \]
\begin{equation}
    \nonumber
  \E \norm{ \bm{\Lambda}^\i }^{2} \leq 4 \max_{i,j} (\Omega_{ij}^{\i+1} - \Omega_{ij}^{\i})^2 \;.
\end{equation}

Now, by definition,
\[ (\q_{ij}^{\i+1} - \q_{ij}^\i)^2 \leq \frac {L} {S^2} \left(c_1(\Er^\i)^2 + c_2|\Er^\i|(|\lf_i| + |\lf_j|) \right)^2 \leq \frac L {S^2} \left( c_3 (\Er^\i)^4 + c_4 (\max_\ell \lf^\i_\ell)^4 \right), \]
The term in $(\Er^\i)^4$ is bounded by the same techniques as the last section. For the second term,
\[ \E_\i\left[(\max_\ell \lf_\ell)^4 \right] \leq c_5  \log(p)^2 \left(\max_\ell q^\i_{\ell\ell} \right)^4, \]
and we can write for any $t \geq 0$
\[ \max_\ell{(q_{\ell \ell}^\i)^4} \leq \left( \sum_{\ell} (q_{\ell\ell}^\i)^t \right)^{4/t}. \]
By Jensen's inequality, for $t \geq 4$
\[ \E\left[\left(\max_\ell{q_{\ell \ell}^\i}\right)^4 \right] \leq \left( \sum_{\ell} \E[(q_{\ell\ell}^\i)^t \right)^{4/t} \leq p^{4/t} \left(C(\T) +\frac{c_6 t}{S} \right)^4, \]
using \eqref{eq:bound_q_powers}. Choosing $t = 4 \log(p) \ll S$ shows that
\[ \E\left[ \max_{i,j} (\q_{ij}^{\i+1} - \q_{ij}^\i)^2 \right] \leq \frac{C(\T) \log(p)^2}{S^2}  \]


A similar bound holds for the $\m_{ij}$, and hence
%\[ \E \lVert \Lambda^\i \rVert^2 \leq \frac{c_5 \log(p)^2}{S^2}, \]
\begin{equation}
    \nonumber
     \E \norm{ \bm{\Lambda}^\i }^{2}  \leq \frac{c_5 \log(p)^2}{S^2} \;,
\end{equation}
which implies Assumption \ref{A1} with $\epsilon_1 = 1$ and $C(p, \T) = C'(\T)\log(p)$.

\subsection{Assumption \ref{A2}}
Since $\act$ is Lipschitz, for any $i, j \in [p]$
\[ \Er^\i_i \Er^\i_j \leq L^2 (\Er^\i)^2. \]
Hence,
\begin{align*} 
\E[\lVert\bm\Gamma^\i\rVert_\infty] &\leq \frac{L^2 \lr^2}{\inp^2\hids^2}\E\left[\left(\lVert \x \rVert_2^2 - d \right) \Phi^\i \right]\\ 
&\leq \frac{L^2 \lr^2}{\inp^2\hids^2} \left(\frac1{2\sqrt{d}} \E\left[ \left(\lVert \x \rVert_2^2 - d \right)^2 \right] + \frac{\sqrt{d}}2 \E\left[ (\Er^\i)^4 \right]\right).
\end{align*}
The first expectation is the variance of a $\chi^2_d$ random variable, which is equal to $2d$, and the second expectation is bounded by the same methods as the above sections. The term in brackets is therefore bounded by $c_1\sqrt{d}$, and
\[ \E[\lVert\bm\Gamma^\i\rVert_\infty] \leq c_2  \frac{\lr^2}{\inp^{3/2}\hids^2} \]
Finally, since for any $y > 0$ we have $y^2 \leq \max(y, y^2)^{3/2}$, letting $y = \lr/\hids$ we find
\[ \E[\lVert\bm\Gamma^\i\rVert_\infty] \leq c_2 \max\left( \frac{\lr}{\hids \inp}, \frac{\lr^2}{\hids^2 \inp}  \right)^{3/2} \leq c_3 (\delta t)^{3/2}, \]
hence Assumption \ref{A2} is true with $\epsilon_2 = 1/2$.

\subsection{$\surd$-Lipschitz property}

Let $\Om, \Om' \in \R^{(\hids+\hidt)\times(\hids+\hidt)}$, we can write the $(i, j)$ coefficient of $\psi(\Om)$ as $f_{ij}(\sqrt{\Om}) $, where 
\begin{align*} 
    f:  \R^{(\hids+\hidt)\times(\hids+\hidt)} &\to \R\\
        A &\mapsto \E_{x \sim \mathcal \gauss(0, I_{p+k})} [g_{ij}(Ax)]
\end{align*}
The same arguments as above show that the function $f$ is Lipschitz, and hence for some constant $L''$ we have
\[ \lVert\psi(\Om) - \psi(\Om')\rVert \leq L'' \lVert \sqrt{\Om} - \sqrt{\Om'} \rVert. \]