\section{Framework}
In this section, we describe the architectures of the feature extractors used to produce suitable multi-modal video embeddings, and the fusion stategies adopted to combine them. We complete this section with the description of the hyper-parameters used for the training.

\subsection{Architecture}
\setlength\heavyrulewidth{0.31ex}

\textbf{Backbone.} %Our network is composed of three streams, one for each modality $m$, with distinct feature extractor $F^{m}$ and classifier $G^{m}$. 
For our submission, we adopted different network configurations.
In the first one, corresponding to the RNA-Net framework in \cite{planamente2021crossdomain}, %both the RGB and Flow streams 
we used the Inflated 3D ConvNet (I3D), pre-trained on Kinetics \cite{carreira2017quo}, for RGB and Flow streams, and a BN-Inception model \cite{ioffe2015batch} pre-trained on ImageNet \cite{imageNet} for the auditory information. % with a network initialized with ImageNet \cite{imageNet} pre-trained weights. 
Each feature extractor produces a 1024-dimensional representation which is fed to an action classifier. %$G^{m}$, consisting in a fully-connected layer that outputs the score logits. 
%Then, the two modalities are fused by summing the outputs and the cross entropy loss is used to train the network.
In the second configuration, we used BNInception for all the three streams, using pre-extracted features from a TBN \cite{munro2020multi} model trained on EPIC-Kitchens-55. In the last configurations, we used standard ResNet50 \cite{he2016deep} %and TSM[CITA] variation 
for all the streams using TSN \cite{wang2016temporal} and TSM~\cite{lin2019tsm} models pre-trained on Epic-Kitchen55\footnote{\url{https://github.com/epic-kitchens/epic-kitchens-55-action-models}}. 

\textbf{Multi-modal fusion strategies.}
In all the above mentioned configurations, each modality is processed by its own backbone, and the corresponding extracted representations are then fused following different strategies.
For RNA-Net, we followed a standard late fusion strategy, consisting in averaging the final score predictions obtained from two different fully-connected layers (verb, noun) from each modality. In the other configurations, we adopted the mid-fusion strategy proposed in \cite{Kazakos_2019_ICCV}, to generate a common frame-embedding among the modalities and used a Temporal Relation Module (TRM) \cite{zhou2018temporal} to aggregate features from different frames %obtaining with the 2D CNNs,
before feeding the final embeddings to the verb and noun classifiers.%In the other configurations, we adopt the mid-fusion strategy proposed in [cita], to generate a common frame-embedding among the modalities and used a Temporal Relation Module (TRM) \cite{zhou2018temporal} to aggregate features from different frames %obtaining with the 2D CNNs,
%before feeding the final embeddings to the two classifier. 

\setlength{\tabcolsep}{10pt}



\begin{table}[t]
\centering
\begin{adjustbox}{width=\columnwidth, margin=0ex 1ex 0ex 0ex}
\begin{tabular}{|c|c|c|c|c|c|}
\hline
$\lambda_{RNA}$ & $\lambda_{HNA}$ & $R$ & $\lambda_{MEC}$ & $\gamma$ & $\beta$         \\ \hline
1             & 0.0006        & 40         & 0.01                     & 0.003    & 0.75, 0.75, 0.5 \\ \hline
\end{tabular}
\end{adjustbox}
\caption{UDA losses hyper-parameters used during training.} %$\lambda_{RNA}$, $\lambda_{HNA}$ and $\lambda_{MCE}$ correspond to RNA, HNA and MCE losses weights respectively. $R$ corresponds to the radius value of HNA, and $\gamma$ and $\beta$ to TA$^3$N weights. }
\label{params}
\end{table}


\subsection{Implementation Details}
We trained I3D and BNInception models with SGD optimizer, with an initial learning rate of 0.001, dropout 0.7, and using a batch size of 128, following \cite{planamente2021crossdomain}. Instead, when using pre-extracted features from ResNet50 or BNInception, we trained the TRM modules on top of them for 100 epochs with an initial learning rate of 0.03, decayed after epochs 30 and 60 by a factor of 0.1. We used a batch size of 128 with SGD optimizer. In Table \ref{params} we report the other hyper-parameter used. Specifically, we indicate with $\lambda_{RNA}$, $\lambda_{T-HNA}$ and $\lambda_{MEC}$ the weights of RNA, T-HNA and MEC losses respectively, and with $R$ the values of the radius of T-HNA (see Equation \ref{eq:hna}). In addition, we report the values used in TA$^3$N to weight the attentive entropy loss ($\gamma$) and the domain losses at different levels ($\beta$). 
%The model of the first configuration are trained follow the same hyper-parameters used in [Cita]. Instead for the last three configuration, we train the models for 100 epochs with an initial learning rate of 0.01, decayed after 30 and 60 by a factor of 0.1. We used a batch size of 128 with SGD optimizer. 
%\mirco{vogliamo mettere una tabella riassuntiva dei pesi delle varie loss 1 RNA, 0.0006 HNR, 0.01ConsLoss, beta gamma ...}


