\section{Our Approach}
In this section, we first describe the DG approach we used. %Indeed, the multi-source nature of the challenge setting make it perfect to deal with the domain shift using DG techniques, besides standard UDA ones. 
Then, we illustrate its extension to unlabelled target data under the standard UDA framework. Finally, we repurpose existing DA-based losses to induce consistency between different architectures. 

%and combine it with standard UDA techniques and we proposed new DA-based consistency losses. 
\subsection{Domain Generalization}
The multi-source nature of the proposed challenge setting makes it perfect to deal with the domain shift using DG techniques. %, besides standard UDA ones.
Thus, we first exploited a method which has been recently proposed to operate in this context, called Relative Norm Alignment (RNA) \cite{planamente2021crossdomain}.  %Indeed, the DG setting addresses the problem of learning a model able to generalize well by using inputs from multiple distributions, when no target data is available at all. 
This methods consists in performing an \textit{audio-visual domain alignment} at feature-level by minimizing a cross-modal loss function ($\mathcal{L}_{RNA}$). The latter aims at minimizing the \textit{mean-feature-norm distance} between the audio and visual features norms among all the source domains, %on multiple source domains to improve generalization ability of the network.%, 
and it is defined as
\begin{equation}\label{formula:rna_1}
    \mathcal{L}_{RNA}=\left(\frac{\EX[h(X^v)]}{\EX[h(X^a)]} - 1\right)^2,
\end{equation}
where $h(x^m_i)=({\lVert{ \cdot }\rVert}_2 \circ f^m)(x^m_i)$ indicates the $L_2$-norm of the features $f^m$ of the $m$-th modality, $\EX[h(X^m)]=\frac{1}{N}\sum_{x^m_i \in \mathcal{X}^m}h(x^m_i)$ for the $m$-th modality and $N$ denotes the number of samples of the set $\mathcal{X}^m=\{x^m_1,...,x^m_N\}$.

Authors of \cite{planamente2021crossdomain} proved that the norm unbalance between different modalities might cause the model to be biased towards the source domain that generate features with greater norm and thus causing a wrong prediction.  
Indeed, by simultaneously solving the problem of classification and relative norm alignment on different domains, the network extracts a shared knowledge between the different sources,  %learns features that are task-specific and meaningful to the final prediction, % to generate features with a similar norm, %and promotes those features that are task-specific and meaningful to the final prediction, 
resulting in a domain-agnostic model. 

In our submission to the EPIC-Kitchen UDA challenge, we extended the RNA-Net framework to the optical flow modality, and we exploited the multiple sources available from the official training splits to show the effectiveness of RNA loss in a multi-source DG setting. %The resulting loss combines an audio-visual alignment loss and a flow-visual ones, defined respectively as
%\begin{equation}\label{formula:rna_1}
%    \mathcal{L}^{av}_{RNA}=\left(\frac{\EX[h(X^v)]}{\EX[h(X^a)]} - 1\right)^2,
%\end{equation}
%\begin{equation}\label{formula:rna_2}
%    \mathcal{L}^{fv}_{RNA}=\left(\frac{\EX[h(X^v)]}{\EX[h(X^f)]} - 1\right)^2,
%\end{equation}
%where $h(x^m_i)=({\lVert{ \cdot }\rVert}_2 \circ f^m)(x^m_i)$ indicates the $L_2$-norm of the features $f^m$ of the $m$-th modality, $\EX[h(X^m)]=\frac{1}{N}\sum_{x^m_i \in \mathcal{X}^m}h(x^m_i)$ for the $m$-th modality and $N$ denotes the number of samples of the set $\mathcal{X}^m=\{x^m_1,...,x^m_N\}$. %The obtained results are shown in Table [REF]. [COMMENTO A TABELLA]

%\begin{itemize}
%    \item obiettivo produrre delle feat che sono meno dipendenti dal dominio 
%    sia perche riteniamo che questo allevia il secondo step (UDA) e sia perche permette di estrarre delle feature che sono migliori 
%    \item per questa parte abbiamo esteso il framework audio-visual proposto in (CITA-RNA) a lavorare anche con il flow. 
    
 %   \item tabellina piccola a tre righe solo su verb
 %   (source only baseline di EK, TA3N, our)
 
    
%\end{itemize}

\subsection{Domain Adaptation}
In this section, we describe the UDA techniques that are integrated in our approach.

\textbf{Relative Norm Alignment Network.} We followed the extension towards the UDA setting proposed in \cite{planamente2021crossdomain}, which is possible thanks to the unsupervised nature of RNA. %, taking also into account the Flow modality. 
In order to consider the contribution of both source and target data during training, we redefined $\mathcal{L}_{RNA}$ under the UDA setting as 

\begin{equation}\label{eq:loss_s_t}
    \mathcal{L}_{RNA}=\mathcal{L}^s_{RNA}+\mathcal{L}^t_{RNA} ,
\end{equation}
where $\mathcal{L}^s_{RNA}$ and $\mathcal{L}^t_{RNA}$ correspond to the RNA formulation in Equation \ref{formula:rna_1} illustrated above, when applied to source and target data respectively.

\textbf{Temporal Attentive Adversarial Adaptation Network (TA$^3$N).} Authors of \cite{videoda-chen2019temporal} proposed an UDA technique based on three components. The first one,  called \textit{Temporal Adversarial Adaptation Network (TA$^2$N)}, consists in an extension of DANN \cite{grl-pmlr-v37-ganin15}, %a standard adversarial UDA image-based method used to align the temporal features across domains.
aiming to align the temporal features on a multi-scale Temporal Relation Module (TRM) \cite{zhou2018temporal} through a gradient reversal layer (GRL).
%The domain alignment is obtained following the popular adversarial approach DANN [cita] on the multi-scale temporal relation module []. 
The second component is based on a domain attention mechanism which guides the temporal alignment towards features where the domain discrepancy is larger.
Finally, the third component uses a minimum entropy regularization (attentive entropy) to
refine the classifier adaptation.


%we add the minimum entropy regularization to
%refine the classifier adaptation. However, we only want to
%minimize the entropy for the videos that are similar across
%domains. Therefore, we attend to the videos which have
%low domain discrepancy, so that we can focus more on minimizing the entropy for these videos. 

\input{table/EK_lead}

\subsection{Ensemble UDA losses}

%Our idea is that by %directly combining different backbone
For our final submission, different models are used in order to exploit the potentiality of popular video architectures. 
Training individually each backbone with standard UDA protocols results in an adapted feature representation which varies from stream to stream. Our intuition is that this aspect could impact negatively the training process and the performance on target data. %cause an unbalance 
%during training, 
In fact, since the domain adaption process acts on each architecture independently, different prediction logits are obtained on target data. When combining them, this could cause a mismatch between the final scores, increasing the level of uncertainty of the model.
Thus, we impose a consistency constraint between feature representations from different models, by repurposing existing UDA loss functions to operate between multiple streams. Those are:

\textbf{Temporal Hard Norm Alignment (T-HNA).} It re-balances the contribution of each model during training by extending HNA \cite{planamente2021crossdomain} to align the norms of features coming from the different streams towards the same value $R$. This is applied on features extracted from multiple scales of each TRN module. The resulting $\mathcal{L}_\textit{T-HNA}$ is defined as
    
    \begin{equation}\label{eq:hna}
    \mathcal{L}_\textit{T-HNA}=\sum_b\left(\EX[h_t(X^b)] - R\right)^2,
\end{equation}
where $h_t$ denotes the $L_2$-norm of features extracted from the $t$-th multi-scale level of the $b$-th backbone network. 
    
\textbf{Min Entropy Consensus (MEC loss).} We extended the loss proposed in \cite{roy2019unsupervised} to encourage coherent predictions between different models. The resulting loss is defined as:
    \begin{equation}\label{eq:hna}
    \mathcal{L}_{MEC} = - \frac{1}{m}\sum_{i=1}^{m} \frac{1}{b} \max_{y \in \mathcal{Y} }{ \sum_{b} \mathrm{log} \textit{p}_b(y | x_{i}^{t})}
    \end{equation}
where $m$ is the cardinality of the batch size of the target set, $y$ is the predicted class, and $\mathrm{log} \textit{p}_b(y | x_{i}^{t})$ is the prediction probability of the $b$-th backbone network. The intuitive idea behind the proposed approach is to encourage different backbones to have a similar predictions.


