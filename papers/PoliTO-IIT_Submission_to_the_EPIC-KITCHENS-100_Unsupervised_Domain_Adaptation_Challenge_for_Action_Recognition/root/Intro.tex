\section{Introduction}

First person action recognition offers a wide range of opportunities which arise from the use of wearable devices. In fact, since it intrinsically comes with rich sound information, due to the strong hand-object interactions and the closeness of the sensors to the sound source, it encourages the use of auditory information. Moreover, the continuous movement of the camera, which moves around with the observer, strongly motivates the use of secondary modalities capturing the motion in the scene, such as optical flow. %Indeed, the use of auditory information could be a good workaround for the problems which arise from the use of wearable devices, in that it is not sensitive to the ego-motion and it is not limited by the field of view of the camera. On the other side, the optical flow modality, by focusing on the motion in the scene rather than on the appearance, is less sensitive to environmental variations, and thus potentially more robust  than the visual modality in cross-domain scenarios \cite{munro2020multi}. 

Our idea is that exploiting the intrinsic peculiarities of all these modalities is of crucial importance, especially in cross-domain scenarios. In fact, these modalities suffer from a domain shift which is not of the same nature. For instance, the optical flow modality, by focusing on the motion in the scene rather than on the appearance, is less sensitive to environmental changes, and thus potentially more robust than the visual modality when changing environment~\cite{munro2020multi} (Figure \ref{fig:teaser}). On the other side, the domain shift of auditory information is very different from the visual one (e.g., the sound of â€˜cut' will differ from a plastic to a wooden cutting board). For all those reasons, the classifier should be able to measure and understand which modality is informative and should rely on in the final prediction, and which is not.

%However, the difference between the mean feature norms of the two modalities might cause the model to be biased towards the modal channel with the greater feature norm, as suggested by the \textit{smaller-norm-less-informative} assumption \cite{ye2018rethinking}, and thus causing a wrong prediction.
\begin{figure}[t]
    \centering
    \includegraphics[width=\columnwidth]{figures/teaser.pdf}
    \caption{The correlation between the distinctive sound of an action and its corresponding visual information or motion is not always guaranteed across different domains. %in egocentric action recognition. 
    Thus, effectively combining multi-modal information from \emph{multiple sources} is fundamental to increase the capability to recognize daily actions.}
    %Thus, designing a framework able to deal with multi-source domain is fundamental to combine effectively multi-modal source information from multiple domains, increasing the capability to recognize first person activities.}
    \label{fig:teaser}
\end{figure}
%This is the idea behind Relative Norm Alignment (RNA) [CITA], a
To this purpose, authors of \cite{planamente2021crossdomain} recently proposed a multi-modal framework, called Relative Norm Alignment network (RNA-Net), which aims to progressively align the feature norms of audio and visual (RGB) modalities among multiple sources in a Domain Generalization (DG) setting, where target data are not available during training. In that work, they bring to light that \emph{simply feeding all the source domains to the network without applying any adaptive techniques leads to sub-optimal performance. Indeed, a multi-source domain alignment allows the network to promote domain-agnostic features. }

%In our submission, 

Interestingly, the availability of multiple sources in the official challenge dataset make it perfect to tackle the problem under a DG setting. To this purpose, we extended RNA-Net to the Flow modality, obtaining remarkable results without accessing target data. In a second stage, we further adapted it to work with unlabelled target data under the standard Unsupervised Domain Adaptation (UDA) setting. Finally, our final submission was obtained by ensembling different model streams by means of DA-based consistency losses, namely Temporal Hard Norm Alignment (T-HNA) and Min-Entropy Consistency (MEC). 

%Starting from these considerations, and exploiting the availability of multiple sources in the official challenge dataset, we extended RNA-Net to the Flow modality, showing the potentiality of the method under the DG setting. Then, in a second stage, we adapted it to work with unlabeled target data under the standard Unsupervised Domain Adaptation (UDA) setting. 


%Moreover, our idea is that, since the audio and visual modalities come from different sources, the domain-shift they suffer from i-s not of the same nature. 



%\begin{itemize}
%    \item Breve Descrizione setting
%    \item Intro generico con l'obiettivo di far capire i due step DG e DA

%\end{itemize}