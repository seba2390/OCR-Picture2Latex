To assess generalization, 
machine learning scientists typically either
(i) bound the generalization gap 
and then (after training) 
plug in the empirical risk 
to obtain a bound on the true risk; 
or (ii) validate empirically on holdout data.
However, (i) typically yields vacuous guarantees 
for overparameterized models;
and (ii) shrinks the training set
and its guarantee erodes
with each re-use of the holdout set.
In this paper, 
we leverage unlabeled data
to produce generalization bounds. 
After augmenting our (labeled) training set
with randomly labeled data,
we train in the standard fashion.
Whenever classifiers achieve 
low error on the clean data
but high error on the random data,
our bound ensures that the true risk is low.
We prove that our bound is valid 
for 0-1 empirical risk minimization 
and with linear classifiers
trained by gradient descent. 
Our approach is especially useful 
in conjunction with deep learning
due to the early learning phenomenon 
whereby networks fit true labels 
before noisy labels
but requires one intuitive assumption.
Empirically, on canonical computer vision and NLP tasks, 
our bound provides non-vacuous generalization guarantees
that track actual performance closely.
This work 
enables practitioners to certify generalization
even when (labeled) holdout data is unavailable 
and provides insights into the relationship 
between random label noise and generalization. 
Code is available at 
\href{https://github.com/acmi-lab/RATT\_generalization\_bound}{https://github.com/acmi-lab/RATT\_generalization\_bound}.





% 
% Pre-Clean ICML
% 

% % Typically, 
% % the ability of 
% % machine learning models to generalize
% % is assessed in one of two ways:
% % generalization is assessed in one of two ways:
% To assess generalization, 
% machine learning scientists
% typically 
% either
% (i) bound the generalization gap and, 
% after training, 
% use the error on training data
% to obtain a bound on the true risk; 
% or (ii) validate empirically on holdout data.
% However, %approach
% (i) 
% typically yields vacuous guarantees 
% for overparameterized models.
% Furthermore,
% (ii) 
% shrinks the training set
% and its guarantee erodes
% with each re-use of the holdout set.
% In this paper, we introduce a method
% % Randomly Assign, Train and Track (\ratt),
% % a new approach 
% % that 
% % leverages 
% % leveraging
% that leverages
% % fresh 
% unlabeled data 
% to produce non-vacuous bounds. 
% % We augment our labeled dataset with 
% After augmenting our (labeled) training set
% with randomly labeled fresh examples,
% % we perform empirical risk minimization
% % or train SGD-optimized models 
% % in standard fashion.
% we train in the standard fashion.
% % and train in the standard fashion.
% % we train deep models by SGD, 
% % following standard practice. 
% Because
% % in practice, 
% models tend to fit 
% true labels before noisy labels,
% we reach a point where the error 
% on the clean training data is low 
% but the error on the noisy training data is high. 
% Our bound translates the 
% (high) error on the noisy data 
% to a (tight) post-hoc upper bound on the true risk.
% Theoretically, we prove that our bound holds 
% with 0-1 loss minimization and with
% gradient-descent-trained linear classifiers.  
% Empirically, 
% % across several representative tasks in 
% for deep networks applied to 
% canonical computer vision and NLP tasks, 
% % we find that our bound 
% % tracks
% % predicts tight non-vacuous test performance. 
% our bound
% provides non-vacuous generalization guarantees
% that track actual performance closely.
% % much closer than existing theoretical bounds. 
% %%% Is this mention fair?
% % Since these bounds are provable 
% % whereas ours is not in that setting! 
% This work provides practitioners with an option 
% for certifying the generalization of deep nets 
% even when 
% % clean
% unseen labeled data is unavailable 
% and provides theoretical insights into 
% the relationship between random label noise and 
% generalization. 
% % and deep learning.  


% Typically, machine learning scientists assess the ability of machine learning models to generalize to unseen data in one of two ways: (i) bound the generalization gap from first principles and, after training, plug in the empirical risk to convert this to a bound on the true risk; and (ii) partition the available labeled data into two splits, a train set for model fitting, and a test set containing previously unseen data for evaluation. However, conventional approaches to (i) typically rely on complexity measures such as VC dimension or Rademacher complexity that are incapable of providing non-vacuous generalization guarantees for deep-learning-based predictors. On the other hand, approach (ii) can lose its power as practitioners repeatedly evaluate on the same test sets. In this paper, we introduce a new approach that can leverage fresh unlabeled data to produce non-vacuous generalization bounds. After assigning random labels to the fresh data, we train our deep models by SGD, following standard practice. Because, in practice, networks tend to fit true labels before noisy labels, we quickly reach a point where the error on the clean training data is low but the error on the noisy training data is high. Our bound translates the high error on the noisy data to a post-hoc upper bound on the true risk. In practice, across a number of representative tasks in computer vision and natural language processing, we find that our bound tracks test set performance much closer than existing theoretical bounds. This work provides practitioners with an option for certifying the generalization of deep nets even when clean labeled data are unavailable and provides theoretical insights about the relationship between label noise, generalization, and deep learning. 

% 
%  OLD VERSION
% 



% % Modern neural networks are heavily overparamterized, with the capability to memorize entire training data. 
% % Despite massive overparameterization, neural networks achieve non-trivial generalization throughout its training with SGD. 
% % Moreover, simple regularization methods like early stopping or weight decay are shown to be robust to small fractions of mislabels in the training data. 
% % % Can also add a line on failure of generalization bounds  
% % % Aimed to explain the good generalization, we show $\cdots$
% % We present a provable bound on the generalization performance of the neural networks by connecting these two observations. 
% % Specifically, by adding a small amount of \emph{randomly-labeled} data, we obtain a non-vacuous generalization guarantee on the performance of unseen clean data. 
% % We emphasize that our bound is non-vacuous by the virtue of the robustness of SGD training and is independent of the complexity of the underlying model class. 
% % Conceptually, this finding underscores the \emph{early-learning} phenomenon even in presence of noisy labels and its ability to warrant strong generalization in over parameterized models.
% % % 
% % Experimentally, we demonstrate that our bound predicts non-vacuous performance across several major architectures and a range of benchmark datasets for computer vision and NLP.
% % % MNIST, CIFAR-10, CIFAR-100, ImageNet(?), and IMDb datasets.


% % Typically, machine learning scientists assess 
% % the ability of machine learning models to 
% % generalize to unseen data in one of two ways: 
% Typically, machine learning models' ability 
% to generalize is assessed in two ways:
% (i) bound the generalization gap and, 
% % from first principles and,
% after training, 
% % plug in 
% use the error on training data
% % to convert this 
% to obtain a bound on the true risk; 
% and (ii) empirical validation on 
% a previously unseen dataset.
% % partition the available labeled data 
% % into two splits, a train set for model fitting, 
% % and 
% %a test set containing previously unseen data for evaluation. 
% % an unseen test set for evaluation. 
% However, conventional approaches to 
% (i) typically yield
% % rely on
% % uniform convergence
% % complexity measures such as 
% % VC dimension or Rademacher complexity 
% % that is incapable of providing 
% vacuous generalization guarantees for overparamterized models.
% % for deep-learning-based predictors. 
% % On the other hand
% Besides, approach (ii) can lose its power 
% as practitioners repeatedly evaluate on the same test sets. 
% % 
% In this paper, we introduce a new approach that 
% % can leverage  
% leverages a small fraction of 
% fresh unlabeled data 
% to produce non-vacuous generalization bounds. 
% % independent of the complexity of the underlying model class. 
% After assigning random labels to the fresh data,
% we train deep models by SGD, following standard practice. 
% Because, in practice, networks tend to fit 
% true labels before noisy labels, we 
% % quickly 
% reach a point where the error on the clean training data 
% is low but the error on the noisy
% training data is high. 
% Our bound translates the (high) error on the noisy data 
% to a post-hoc (tight) upper bound on the true risk. 
% Theoretically, we prove that our bound holds 
% with 0-1 loss minimization and with
% gradient-descent-trained linear classifiers.  
% Empirically, across several representative tasks in 
% computer vision and NLP, 
% we find that our bound 
% % tracks
% predicts tight non-vacuous test performance. 
% % much closer than existing theoretical bounds. %%% Is this mention fair? Since these bounds are provable whereas ours is not in that setting! 
% This work provides practitioners with an option 
% for certifying the generalization of deep nets 
% even when 
% % clean
% unseen labeled data is unavailable 
% and provides theoretical insights about 
% the relationship between random label noise and 
% generalization. 
% % and deep learning.  


% % Typically, machine learning scientists assess the ability of machine learning models to generalize to unseen data in one of two ways: (i) bound the generalization gap from first principles and, after training, plug in the empirical risk to convert this to a bound on the true risk; and (ii) partition the available labeled data into two splits, a train set for model fitting, and a test set containing previously unseen data for evaluation. However, conventional approaches to (i) typically rely on complexity measures such as VC dimension or Rademacher complexity that are incapable of providing non-vacuous generalization guarantees for deep-learning-based predictors. On the other hand, approach (ii) can lose its power as practitioners repeatedly evaluate on the same test sets. In this paper, we introduce a new approach that can leverage fresh unlabeled data to produce non-vacuous generalization bounds. After assigning random labels to the fresh data, we train our deep models by SGD, following standard practice. Because, in practice, networks tend to fit true labels before noisy labels, we quickly reach a point where the error on the clean training data is low but the error on the noisy training data is high. Our bound translates the high error on the noisy data to a post-hoc upper bound on the true risk. In practice, across a number of representative tasks in computer vision and natural language processing, we find that our bound tracks test set performance much closer than existing theoretical bounds. This work provides practitioners with an option for certifying the generalization of deep nets even when clean labeled data are unavailable and provides theoretical insights about the relationship between label noise, generalization, and deep learning. 