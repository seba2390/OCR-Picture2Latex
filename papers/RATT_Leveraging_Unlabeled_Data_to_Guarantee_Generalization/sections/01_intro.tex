Typically, machine learning scientists
establish generalization in one of two ways.
One approach, favored by learning theorists,
places an \emph{a priori} bound on the gap 
between the empirical and true risks,
usually in terms of the complexity 
of the hypothesis class.
After fitting the model on the available data,
one can plug in the empirical risk 
to obtain a guarantee on the true risk. 
The second approach, favored by practitioners,
involves splitting the available data
into training and holdout partitions,
fitting the models on the former
and estimating the population risk with the latter.

Surely, both approaches are useful,
with the former providing theoretical insights 
and the latter guiding the development 
of a vast array of practical technology.
Nevertheless, both methods have drawbacks.
Most \emph{a priori} generalization bounds 
rely on uniform convergence 
and thus fail to explain the ability 
of overparameterized networks to generalize
\citep{zhang2016understanding,nagarajan2019uniform}. 
On the other hand, 
provisioning a holdout dataset 
restricts the amount of labeled data 
available for training.
Moreover, risk estimates based on holdout sets 
lose their validity 
with successive re-use of the holdout data
due to adaptive overfitting 
\citep{murphy2012machine,dwork2015preserving,blum2015ladder}.
However, recent empirical studies suggest
that on large benchmark datasets,
adaptive overfitting is surprisingly absent
\citep{recht2019imagenet}.



In this paper, we propose
Randomly Assign, Train and Track (RATT), 
a new method that leverages unlabeled data 
to provide a \emph{post-training} 
bound on the true risk 
(i.e., the population error).
Here, we assign random labels 
to a fresh batch of unlabeled data, 
augmenting the clean training dataset
with these randomly labeled points. 
Next, we train on this data, 
following standard risk minimization practices. 
Finally, we track the error
on the randomly labeled 
portion of training data,
estimating the error 
on the mislabeled portion
and using this quantity 
to upper bound the population error.
% by roughly twice the error on the mislabeled portion minus one. 

Counterintuitively, 
we guarantee generalization 
by guaranteeing overfitting.
Specifically, we prove that 
Empirical Risk Minimization (ERM) 
with 0-1 loss leads to lower error 
on the \emph{mislabeled training data}
than on the \emph{mislabeled population}.
% \emph{the population} of mislabeled data. 
Thus, if despite minimizing the loss 
on the combined training data,
we nevertheless have high error 
on the mislabeled portion,
then the (mislabeled) population error will be even higher. 
Then, by complementarity, 
the (clean) population error must be low.
Finally, we show how to obtain this guarantee
using randomly labeled (vs mislabeled data),
thus enabling us to incorporate unlabeled data. 


\begin{figure}[t!]
    % {r}{0.5\textwidth}
        \centering 
        % \vspace{-15pt}
        % \includegraphics[width=0.9\linewidth]{example-image-a}
        \includegraphics[width=0.9\linewidth]{figures/CIFAR-figure1.pdf}
        % \includegraphics[width=0.9\linewidth]{figures/{CIFAR10_rn=0.1_lr=0.2_wd=0.005}.png}
        \vspace{-8pt}
        \caption{ \textbf{Predicted lower bound on the clean population error} 
        with ResNet and MLP on binary CIFAR. 
        Results aggregated over $5$ seeds. 
        `*' denotes the best test performance 
        % on training with just the clean data 
        achieved when training with only clean data
        and the same hyperparameters 
        (except for the stopping point).
        % except for the stopping point. 
        The bound predicted by RATT (RHS in \eqref{eq:thm1}) closely tracks 
        the population accuracy on clean data. 
        }\label{fig:error_CIFAR10}
        \vspace{-5pt}
\end{figure}
    

    
To expand the applicability of our idea
beyond ERM on 0-1 error,
we prove corresponding results 
for a linear classifier 
trained by gradient descent
to minimize squared loss. 
Furthermore, leveraging the connection 
between early stopping 
and $\ell_2$-regularization 
in linear models 
\citep{ali2018continuous,ali2020implicit,suggala2018connecting}, 
our results extend to early-stopped gradient descent.  
Because we make no assumptions
on the data distribution, 
our results on linear models
hold for more complex models 
such as kernel regression 
and neural networks in the Neural Tangent Kernel (NTK) regime
\citep{jacot2018neural, du2018gradient, du2019gradient, allen2019convergence, chizat2019lazy}.


Addressing practical deep learning models,
% while 
our guarantee requires an additional (reasonable) assumption.
% we experimentally verify 
Our experiments show that the bound 
yields non-vacuous guarantees
that track test error
across several major architectures 
on a range of benchmark datasets
for computer vision and Natural Language Processing (NLP).
Because, in practice,
overparameterized deep networks
exhibit an \emph{early learning phenomenon},
fitting clean data 
before mislabeled data
\citep{liu2020early,arora2019fine,li2019gradient}, 
our procedure yields tight bounds 
in the early phases of learning. 
Experimentally, we confirm 
the early learning phenomenon 
in standard Stochastic Gradient Descent (SGD) training 
and illustrate the effectiveness of weight decay 
combined with large initial learning rates 
in avoiding interpolation to mislabeled data 
while maintaining fit on the training data, 
strengthening the guarantee provided by our method.


% Our work derives inspiration 
% from recent observations
% on deep learning with noisy data 
% \citep{zhang2016understanding,rolnick2017deep,arpit2017closer}. 
% In particular, \citet{hu2019simple}
% and \cite{li2019gradient} have shown
% that even when training data is contaminated
% by a small amount of mislabeled data,
% overparameterized networks 
% nevertheless generalize well 
% when trained with early stopping and weight decay.
% Moreover, the tend overparameterized models 
% tend to fit the clean training data first
% before eventually fitting the mislabeled data 
% \citep{liu2020early,arora2019fine,li2019gradient}.
% The ability of our method to give non-vacuous
% bounds relies on these phenomena.
% Due to these phenomena, %give rise
% we can provide non-vacuous bounds. 



%%
%% IS the table enough or we should add something else?
%%
% Finally, we demonstrate the usefulness 
% of our bound in guiding hyperparameter tuning
% (i.e., weight decay and early stopping point) 
% without requiring access to clean validation data. 


To be clear, we do not advocate RATT 
as a blanket replacement 
for the holdout approach.
Our main contribution is to introduce 
a new theoretical perspective on generalization
and to provide a method that may be applicable
even when the holdout approach is unavailable. 
Of interest, unlike generalization bounds
based on uniform-convergence 
that restrict the complexity 
of the hypothesis class
\citep{neyshabur2018role,neyshabur2015norm,neyshabur2017pac,bartlett2017spectrally,nagarajan2019deterministic}, 
our \emph{post hoc} bounds depend 
only on the fit to mislabeled data.
We emphasize that our theory 
does not guarantee \emph{a priori}
% that the early learning phenomenon should take place
that early learning should take place
but only \emph{a posteriori} that when it does,
we can provide non-vacuous bounds 
on the population error.
Conceptually, this finding underscores 
the significance 
of the early learning phenomenon 
in the presence of noisy labels
and motivates further work 
to explain why it occurs. 




% 
% OLD V2 ICML PRE-CLEAN
% 

% In general, machine learning scientists
% % have two tools at their disposal 
% % for establishing the ability
% % of their models to generalize. 
% establish generalization in one of two ways.
% One approach, favored by learning theorists,
% % The learning-theoretic approach
% places an \emph{a priori}
% bound on the gap between the empirical and true risks,
% % involves bounding 
% % the difference 
% % between the empirical risk 
% % and the true risk \emph{a priori},
% typically 
% % as a function 
% % of some complexity measure 
% % in terms of the complexity
% by a function of the complexity 
% of the hypothesis class.
% After fitting the model on the available data,
% one can plug in the empirical risk 
% to obtain a guarantee on the true risk. 
% The second approach, favored by practitioners,
% involves splitting the available data
% into training and holdout partitions,
% fitting the models on the former
% and estimating the population risk
% with the latter.

% Surely both approaches are useful,
% with the former providing theoretical insights 
% and the latter guiding the development 
% of a vast array of practical technology.
% Nevertheless, both methods have drawbacks.
% Most \emph{a priori} generalization bounds 
% rely on uniform convergence %fundamental learning-theoretical tool of uniform convergence 
% % that
% and thus fail to explain the % generalization 
% % for the overparameterized models that dominate the practice of modern machine learning 
% ability of deep neural networks to generalize
% \citep{zhang2016understanding,nagarajan2019uniform}. 
% On the other hand, %the validity 
% of risk estimates based on holdout sets 
% lose their validity with successive 
% re-use of the holdout data
% due to adaptive overfitting 
% \citep{murphy2012machine,dwork2015preserving},
% although recent empirical studies suggest
% % have suggested 
% that on large benchmark datasets,
% adaptive overfitting is surprisingly absent
% \citep{recht2019imagenet}.
% Moreover, provisioning a holdout dataset restricts the amount of labeled data 
% available for training.

% In this paper, we 
% % establish
% propose
% Randomly Assign, Train and Track (RATT), 
% a new method that leverages unlabeled data 
% to provide a \emph{post-training} 
% bound on the true risk 
% (i.e., generalization error).
% % In particular, 
% Here, we assign random labels 
% to a fresh batch of unlabeled data, 
% augmenting the clean training dataset
% with these randomly labeled points. 
% Next, we 
% % train 
% % our machine learning model 
% train on this data, 
% following standard risk minimization practices. 
% Finally, we track the error,
% on the randomly labeled 
% portion of training data,
% estimating the error 
% on the mislabeled portion
% and using this quantity 
% to upper bound the generalization error. 
% Because, in practice,
% overparameterized deep networks 
% tend to fit clean data 
% before fitting mislabeled data,
% % before memorizing noisy samples, 
% our procedure %tends to 
% yields tight bounds 
% in the early phases of learning. 

% Our work derives inspiration from recent observations
% on deep learning with noisy data 
% \citep{zhang2016understanding,rolnick2017deep,arpit2017closer}. 
% Specifically, even when training data is contaminated
% by a small amount of mislabeled data,
% overparameterized networks nevertheless generalize well 
% % overparameterized networks avoid fit to mislabeled data  
% when trained with early stopping and weight decay
% \citep{hu2019simple,li2019gradient}.
% Additionally, when training with noisy labels, 
% overparameterized models tend 
% to fit the clean training data first
% before eventually 
% % memorizing 
% fitting 
% the mislabeled data 
% \citep{liu2020early,arora2019fine,li2019gradient}.
% The ability of our method to give non-vacuous
% bounds relies on these phenomena.

% % Interestingly 

% %%%% SG: Some wording here %%%% 

% Counterintuitively, 
% we guarantee generalization 
% by guaranteeing overfitting.
% Specifically, we prove 
% that 
% % with high probability, 
% ERM 
% with 0-1 error 
% leads to lower error 
% on the the mislabeled training data
% than on the population of mislabeled data. 
% Thus, if despite minimizing the error 
% on the combined training data,
% we nevertheless have high error on mislabeled fraction,
% then the (mislabeled) population error 
% will be even higher. 
% Then, by complementarity, 
% the (clean) population error must be low.
% Finally, we show how to obtain this guarantee
% % We then show how to obtain this guarantee
% using randomly labeled (vs mislabeled data),
% thus enabling us to incorporate unlabeled data. 

% % Counterintuitively, 
% % we guarantee generalization 
% % by guaranteeing overfitting.
% % Specifically, we prove 
% % that 
% % % with high probability, 
% % ERM leads to lower error 
% % on the the mislabeled training data
% % than on the population of mislabeled data. 
% % Thus, if despite minimizing the error 
% % on the mislabeled training data,
% % we nevertheless have high error,
% % then the (mislabeled) population error 
% % will be even higher. 
% % Then, by complementarity, 
% % the (clean) population error must be low.
% % Finally, we show how to obtain this guarantee
% % % We then show how to obtain this guarantee
% % using randomly labeled (vs mislabeled data),
% % thus enabling us to incorporate unlabeled data. 

% % Due to the difficulty 
% % of directly minimizing 0-1 loss,
% % solely characterizing ERM on 0-1 loss
% % would only be of theoretical interest.
% % To expand the applicability of our idea,
% % we expand upon these results
% % we prove our results 
% To expand the applicability of our idea
% beyond ERM on 0-1 error,
% we prove corresponding results 
% for a linear classifier 
% trained by gradient descent
% to minimize squared loss. 
% Furthermore, 
% % exploiting 
% % the 
% leveraging the 
% connection between early-stopping 
% and $\ell_2$-regularization in linear models 
% \citep{ali2018continuous,ali2020implicit,suggala2018connecting}, 
% % we extend our results 
% our results extend to early-stopped gradient descent.  
% % Since we make no assumption 
% Because we make no assumptions
% on the %underlying 
% data distribution, 
% our results on linear models
% hold for more complex models 
% such as kernel regression 
% and neural networks in the NTK regime
% \citep{jacot2018neural, du2018gradient, du2019gradient, allen2019convergence, chizat2019lazy}.



% % % Recognizing that 
% % Due to the difficulty 
% % of directly minimizing 0-1 loss,
% % solely characterizing ERM on 0-1 loss
% % would only be of theoretical interest.
% % % To expand the applicability of our idea,
% % % we expand upon these results
% % % we prove our results 
% % Thus, to expand the applicability of our idea,
% % we prove corresponding results 
% % for a linear classifier 
% % trained by gradient descent
% % to minimize squared error. 
% % Furthermore, 
% % % exploiting 
% % % the 
% % leveraging the 
% % connection between early-stopping 
% % and $\ell_2$-regularization in linear models 
% % \citep{ali2018continuous,ali2020implicit,suggala2018connecting}, 
% % % we extend our results 
% % our results extend to early-stopped gradient descent.  
% % % Since we make no assumption 
% % Because we make no assumptions
% % on the %underlying 
% % data distribution, 
% % our results on linear models
% % hold for more complex models 
% % such as kernel regression 
% % and neural networks in the NTK regime; 
% % \citep{jacot2018neural, du2018gradient, du2019gradient, allen2019convergence, chizat2019lazy}.


% % has happened
% % relies on a 


% % Fundamentally, our bound depends 
% % on the following simple conjecture: 
% % Empirical Risk Minimization (ERM) training 
% % with a loss aimed to minimize training error 
% % leads to lower classification accuracy 
% % on the finite training data 
% % when compared to unseen data 
% % from the same distribution. 
% % In other words, ERM training 
% % on finite data will lead to ``overfitting''.  
% % We rigorously prove the above conjecture 
% % and our results with ERM on 0-1 loss. 
% % For practical models, minimizing 0-1 loss is hard, 
% % and hence, understanding ERM on 0-1 loss 
% % is only of theoretical interest.
% % To validate our conjecture i
% % Subsequently, we prove our results 
% % to a gradient descent trained 
% % linear classifier with squared-loss. 
% % Furthermore, using the analogy 
% % between early-stopping and $\ell_2$-regularization 
% % n linear models \citep{ali2018continuous,ali2020implicit,suggala2018connecting}, 
% % our results extend to early-stopped gradient descent.  
% % Since we make no assumption 
% % about the underlying data distribution, 
% % our results on linear models also hold 
% % on to more complex models 
% % such as kernel regression 
% % and neural networks in the NTK regime; 
% % \citep{jacot2018neural,du2018gradient,du2019gradient,allen2019convergence,chizat2019lazy}.

% % \todos{Add a equation for our bound?}

% \begin{figure}[t!]
%     \centering 
%     % \vspace{-15pt}
%     % \includegraphics[width=0.9\linewidth]{example-image-a}
%     \includegraphics[width=0.9\linewidth]{figures/CIFAR-figure1.pdf}
%     % \includegraphics[width=0.9\linewidth]{figures/{CIFAR10_rn=0.1_lr=0.2_wd=0.005}.png}
%     \vspace{-5pt}
%     \caption{ \textbf{Predicted lower bound on the clean population error} with ResNet-18 architecture and multilayer perceptron on CIFAR cat vs dog. Performance aggregated over $5$ runs with different seeds. `*' marked point denotes the test performance on training with just the clean data 
%     and the same hyperparameters except for the stopping point.
%     }\label{fig:error_CIFAR10}
%     \vspace{-10pt}
% \end{figure}

% % Empirically, 
% % In experiments,
% % Through a series of experiments
% We experimentally verify 
% that our bound yields non-vacuous guarantees
% that track test error
% across several major architectures 
% % and on a range of benchmark datasets 
% on a range of benchmark datasets
% for computer vision %(CV) 
% and Natural Language Processing (NLP). 
% Moreover, we confirm the \emph{early learning} phenomenon 
% in SGD training with standard practice  
% and illustrate the effectiveness of weight decay 
% combined with a large initial learning rate 
% in avoiding interpolation to the mislabeled data 
% while maintaining fit on the training data, 
% strengthening the guarantee provided by our method.
% % on the risk of clean data distribution. 
% % First, we demonstrate the existence 
% % of an early learning phenomenon 
% % in SGD training with standard practice.  
% % Second, our findings illustrate 
% % the effectiveness of weight decay 
% % and a large initial learning rate 
% % in avoiding interpolation to the mislabeled data 
% % while maintaining fit on the training data, 
% % and thereby, providing strong guarantees 
% % on the risk of clean data distribution. 
% Finally, we demonstrate the usefulness 
% of our bound in guiding hyperparameter tuning
% (i.e., weight decay and early stopping point) 
% without requiring access to clean validation data. 
% % without any access to clean validation data. 
% %  
% % \todos{Not sure if we should say this because we don;t perform experiments where we show guarantees transfer}

% To be clear, we do not  advocate RATT 
% as an across-the-board replacement 
% for the holdout approach.
% Our main contribution is to introduce 
% a new theoretical perspective on generalization
% and to provide a method that may be applicable
% even when the holdout approach is unavailable. 
% Of interest, unlike generalization bounds 
% based on uniform-convergence due to 
% the (restricted) complexity of the underlying hypothesis
% \citep{neyshabur2018role,neyshabur2015norm,neyshabur2017pac,bartlett2017spectrally,nagarajan2019deterministic}, 
% our \emph{post hoc} bounds depend only 
% on the fit to mislabeled data.
% We emphasize that our theory not guarantee \emph{a priori}
% that the early learning phenomena should take place
% but only \emph{a posteriori} that when it does,
% we can provide non-vacuous bounds on the generalization error.
% Conceptually, this finding underscores 
% the significance (and usefulness) 
% of the \emph{early-learning} phenomenon 
% \citep{arora2019fine,liu2020early}
% % \citep{achille2017critical,frankle2020early}
% % even 
% % in the presence of noisy labels,
% in the presence of noisy labels
% and motivates further work 
% to explain why it occurs. %in the first place. 


% 
% While our bound predicts strong generalization, 
% we do not argue this to be a replacement to hold-out validation. 
% Our main contribution is to establish 
% the post-training generalization bound, 
% highlight the underlying phenomenon, 
% and demonstrate its effectiveness 
% to warrant strong performance
% % in predicting non-vacuous generalization 
% in overparamterized models 
% without looking at unseen data. 
% We discuss this more in Section~\ref{sec:exp}. 
% 

% and do not directly involve any term 
% characterizing the complexity 
% of the underlying hypothesis. 
% 
% We emphasize that, in practice, our bound is non-vacuous
% by the virtue of the robustness of SGD training. 





%% Takeaway message 
%%% This is a bit discontinuos, comeback and fix
% While our bound predicts strong generalization, 
% we do not argue this to be a replacement to hold-out validation. 
% Our main contribution is to establish 
% the post-training generalization bound, 
% highlight the underlying phenomenon, 
% and demonstrate its effectiveness 
% to warrant strong performance
% % in predicting non-vacuous generalization 
% in overparamterized models 
% without looking at unseen data. 
% We discuss this more in Section~\ref{sec:exp}. 
% % 
% Furthermore, in contrast to typical 
% uniform-convergence based generalization bounds 
% that depend on some notion of complexity 
% of the underlying hypothesis
% \citep{neyshabur2018role,neyshabur2015norm,neyshabur2017pac,bartlett2017spectrally,nagarajan2019deterministic}, 
% our bounds just depend on the fit to the mislabeled data 
% and do not directly involve any term 
% characterizing the complexity 
% of the underlying hypothesis. 
% We emphasize that, in practice, our bound is non-vacuous
% by the virtue of the robustness of SGD training. 
% Conceptually, this finding underscores 
% the significance of the {early-learning} phenomenon \citep{achille2017critical,frankle2020early}, 
% even in the presence of noisy labels. 

% and its ability to warrant strong generalization in overparameterized models.

% Discuss the hit to training when we remove the same fraction as we corrupt? 

%%% This is a bit discontinuos, comeback and add
% To summarize, following are our main contributions: i)
% \textbf{Overview of Results.}
% \textbf{Paper Organization.} 






% 
% OLD V2
% 


% %% Generalization (motivation for our work)
% Despite massive overparametrization, machine learning models generalize remarkably well on classification tasks. % Not sure if we need this, if yes, should I elaborate more? I included this to ground the task to classification and mention the overparametrized setting. 
% To assess the generalization capabilities of deep learning models, researchers typically either:
% % proceed one of the two ways: 
% (i) theoretically bound the generalization gap and, plug in the empirical risk on the training data to obtain an upper bound on the true risk; or 
% (ii) validate empirically on a clean dataset unseen during training. Nevertheless, both these conventional approaches have associated weaknesses.
% Most of the generalization bounds for machine learning leverage fundamental learning-theoretical tool of uniform convergence that fail to explain generalization in overparameterized models~\citep{zhang2016understanding,nagarajan2019uniform}. 
% On the other hand, repeated evaluation on the same holdout datasets 
% % can loose its power to guide generalization 
% can diminish the validity of the estimate
% due to adaptive overfitting, 
% % the danger of overfitting to the holdout data arises~
% \citep{murphy2012machine,dwork2015preserving},
% although recent empirical studies have suggested 
% that on large benchmark datasets,
% adaptive overfitting is surprisingly absent
% \citep{recht2019imagenet}.
% Moreover, 
% % in under-resourced settings,
% provisioning a holdout dataset restricts 
% the amount of labeled data available for training.

% % An additional limitation of the holdout approach
% % is that it requires with
% %% Should we elaborate here?
% %% Add a line from Ludwig's work 

% %% Key insights for our work 
% In this paper, we establish a novel \emph{post-training} generalization bound. Our approach is inspired by a recent line of observations on learning with noisy data in deep models~\citep{zhang2016understanding,rolnick2017deep,arpit2017closer}. 
% %  
% Empirically, it has been observed that in the presence of a small amount of mislabeled data, overparamterized networks often generalize well on clean unseen data when trained with commonly-used regularization techniques like early stopping and weight decay~\citep{hu2019simple,li2019gradient}.
% Additionally, during training with noisy labels, overparamterized models tend to first fit the clean training data during an \emph{early-learning} phase 
% % before eventually memorizing the mislabeled data 
% \citep{liu2020early,arora2019fine,li2019gradient}.

% Inspired by these two observations, in this work, we leverage a small fraction of \emph{randomly labeled data} 
% % add it to clean training data, 
% and track its performance to obtain an upper bound on the generalization gap. 
% %  Empirical Risk Minimization (ERM) training. 
% In particular, we begin by assigning random labels to a fresh sample of unlabeled data, include them into a clean training dataset, and then train the machine learning model on the noisy mixture by following standard practices. Subsequently, we translate the error on mislabeled training data to an upper bound on the generalization gap. Since, in practice, overparameterized networks tend to fit clean data first before memorizing noisy samples, our procedure yields a tight bound on the generalization gap, and hence a tight upper bound on the true risk of clean data in the early-learning phase. 
% %% Shall we give more formal details on the bound?

% %%% Need to organize this paragraph nicely. 
% Fundamentally, our bound depends on the following simple conjecture: Empirical Risk Minimization (ERM) training with a loss aimed to minimize training error leads to lower classification accuracy on the finite training data when compared to unseen data from the same distribution. In other words, ERM training on finite data will lead to ``overfitting''.  We rigorously prove the above conjecture and our results with ERM on 0-1 loss. For practical models, minimizing 0-1 loss is hard, and hence, understanding ERM on 0-1 loss is only of theoretical interest.
% % To validate our conjecture i
% Subsequently, we prove our results to a gradient descent trained linear classifier with squared-loss. Furthermore, using the analogy between early-stopping and $\ell_2$-regularization in linear models~\citep{ali2018continuous,ali2020implicit,suggala2018connecting}, our results extend to early-stopped gradient descent.  
% Since we make no assumption about the underlying data distribution, our results on linear models also hold on to more complex models such as kernel regression and neural networks in the NTK regime; see \citet{jacot2018neural,du2018gradient,du2019gradient,allen2019convergence,chizat2019lazy}.

% \todos{Add a equation for our bound?}

% \begin{figure}[t!]
%     \centering 
%     % \vspace{-15pt}
%     % \includegraphics[width=0.9\linewidth]{example-image-a}
%     \includegraphics[width=0.9\linewidth]{figures/CIFAR-figure1.pdf}
%     % \includegraphics[width=0.9\linewidth]{figures/{CIFAR10_rn=0.1_lr=0.2_wd=0.005}.png}
%     \vspace{-5pt}
%     \caption{ \textbf{Predicted lower bound on the clean population error} with ResNet-18 architecture and multilayer perceptron on CIFAR cat vs dog. Performance aggregated over $5$ runs with different seeds. `*' marked point denotes the test performance on training with just the clean data 
%     and the same hyperparameters except for the stopping point.
%     }\label{fig:error_CIFAR10}
%     \vspace{-10pt}
% \end{figure}
% Empirically, we verify our results across several major architectures and a range of benchmark datasets for computer vision and Natural Language Processing (NLP). First, we demonstrate the existence of an early learning phenomenon in SGD training with standard practice.  
% %  and its ability to predict tight upper bounds on the true risk.
% Second, our findings illustrate the effectiveness of weight decay and a large initial learning rate in avoiding interpolation to the mislabeled data while maintaining fit on the training data, and thereby, providing strong guarantees on the risk of clean data distribution. Finally, we also perform experiments to show the applicability of our results on learning with noisy data to obtain optimal hyperparameters (i.e., weight decay and early stopping point) without any access to clean validation data. 
% %  

% %% Takeaway message 
% %%% This is a bit discontinuos, comeback and fix
% While our bound predicts strong generalization, we do not argue this to be a replacement to hold-out validation. Our main contribution is to establish the post-training generalization bound, highlight the underlying phenomenon, and demonstrate its effectiveness to warrant strong performance
% % in predicting non-vacuous generalization 
% in overparamterized models without looking at unseen data. We discuss this more in Section~\ref{sec:exp}. 
% % 
% Furthermore, in contrast to typical uniform-convergence based generalization bounds that depend on some notion of complexity of the underlying hypothesis~\citep{neyshabur2018role,neyshabur2015norm,neyshabur2017pac,bartlett2017spectrally,nagarajan2019deterministic}, our bounds just depend on the fit to the mislabeled data and do not directly involve any term characterizing the complexity of the underlying hypothesis. 
% We emphasize that, in practice, our bound is non-vacuous by the virtue of the robustness of SGD training. Conceptually, this finding underscores the significance of the {early-learning} phenomenon \citep{achille2017critical,frankle2020early}, 
% even in the presence of noisy labels. 
% % and its ability to warrant strong generalization in overparameterized models.

% % Discuss the hit to training when we remove the same fraction as we corrupt? 

% %%% This is a bit discontinuos, comeback and add
% % To summarize, following are our main contributions: i)
% \textbf{Overview of Results.}
% \textbf{Paper Organization.} 





%%%%%%%%%%%% OLD %%%%%%%%%%%%%%%%%

% Despite massive overparametrization, machine learning models, when trained with Stochastic Gradient Descent (SGD) with appropriately chosen hyperparameters, perform remarkably well on classification tasks. However, the underlying reasons behind their astounding generalization capabilities are not yet clear. Typically,  
% % classification
% performance on unseen iid data in practice. However, 
% %% Learning with noisy data and early learning
% % More surprisingly, even in presence of a small amount of mislabeled data, overparamterized neural networks generalize well to clean unseen data when trained with commonly-used regularization techniques like early stopping and weight decay~\citep{hu2019simple,li2019gradient,rolnick2017deep,arpit2017closer}.
% % 
% Yet, a vital question in machine learning remains to understand why over-parametrized models (with the capacity to memorize entire training data) generalize well~\citep{zhang2016understanding,nagarajan2019uniform}.  
% \todos{Add more discussion here before transitioning to our results?}



% injecting a small amount of randomly-labeled data during training, we obtain a generalization guarantee on the performance of unseen clean data by tracking the performance on added randomly labeled dataset.  