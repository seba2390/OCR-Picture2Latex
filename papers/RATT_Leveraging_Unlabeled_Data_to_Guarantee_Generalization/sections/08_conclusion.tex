% 
% Clean
% 
Our work introduces a new approach 
for obtaining generalization bounds
that do not directly depend on the 
underlying complexity of the model class. 
For linear models, we provably obtain a bound 
in terms of the fit on randomly labeled data added during training. 
Our findings raise a number of questions to be explored next. 
While our empirical findings and theoretical results with 0-1 loss 
hold absent further assumptions
and shed light on why the bound 
may apply for more general models,
we hope to extend our proof 
that overfitting (in terms classification error)
to the finite sample of mislabeled data
occurs with SGD training on broader classes 
of models and loss functions. 
We hope to build on some early results
\citep{nakkiran2019sgd, hu2020surprising} 
which provide evidence that deep models
behave like linear models 
in the early phases of training.  
We also wish to extend our framework
to the interpolation regime.
Since many important aspects of neural network learning 
take place within early epochs
\citep{achille2017critical,frankle2020early},
including gradient dynamics converging 
to very small subspace~\citep{gur2018gradient},
we might imagine operationalizing our bounds
in the interpolation regime
by discarding the randomly labeled data
after initial stages of training. 
 



% 
% Working draft
% 

% % In this paper, \dots % Add more for conclusion
% % Our work takes a step towards obtaining generalization bounds 
% Our work introduces a new approach 
% for obtaining generalization bounds
% that do not directly depend on the 
% underlying complexity of the model class. 
% For linear models, we provably obtain a bound 
% in terms of the fit on randomly labeled data added during training. 
% Our findings raise a number of questions to be explored next. 
% While our empirical findings and theoretical results with 0-1 loss 
% hold absent further assumptions
% and shed light on why the bound 
% may apply for more general models,
% we hope to extend our proof 
% that overfitting (in terms classification error)
% to the finite sample of mislabeled data
% occurs with SGD training on broader classes 
% of models and loss functions. 
% % shed some light on why this might hold with general models, 
% % we believe theoretically extending our bounds to general settings 
% % with SGD training is an interesting next question. 
% We hope to build on some early results~\citep{nakkiran2019sgd, hu2020surprising} 
% which provide evidence that deep models
% behave like linear models 
% in the early phases of training.  
% % 
% % Subsequently, it may also be interesting 
% % to extend our results in the interpolation regime. 
% We also wish to connect our framework
% to the interpolation regime.
% % Several recent studies show 
% % that significant and consequential changes 
% % occur during the early stage of training
% % \citep{achille2017critical,frankle2020early},
% % including gradient dynamics converging 
% % to very small subspace~\citep{gur2018gradient}.
% Since many important aspects of neural network learning 
% take place within early epochs
% \citep{achille2017critical,frankle2020early},
% including gradient dynamics converging 
% to very small subspace~\citep{gur2018gradient},
% we might imagine operationalizing our bounds
% in the interpolation regime
% % it can be interesting to operationalize our bounds 
% % in the interpolation regime, 
% % perhaps 
% by discarding the randomly labeled data
% after initial stages of training. 
 