\begin{figure*}[t!]
    \centering 
    % \vspace{-15pt}
    % \includegraphics[width=0.9\linewidth]{example-image-a}
    \subfigure[]{\includegraphics[width=0.3\linewidth]{figures/lowdim-Gaussian.pdf}}\hfil
    \subfigure[]{\includegraphics[width=0.3\linewidth]{figures/MNIST.pdf}}\hfil
    \subfigure[]{\includegraphics[width=0.3\linewidth]{figures/IMDb.pdf}}
    % \includegraphics[width=0.9\linewidth]{figures/{CIFAR10_rn=0.1_lr=0.2_wd=0.005}.png}
    \vspace{-5pt}
    \caption{ 
    % Predicted lower bound 
    % on different
    We plot the accuracy and corresponding bound 
    (RHS in \eqref{eq:erm}) at $\delta = 0.1$. 
    for binary classification tasks. 
    Results aggregated over $3  $ seeds. 
    % i.e., $1-\error$ where $\error$ is the term in the RHS of \eqref{eq:erm}
    (a) Accuracy vs fraction of unlabeled data (w.r.t clean data) 
    in the toy setup with a linear model trained with GD. 
    (b) Accuracy vs fraction of unlabeled data 
    for a 2-layer wide network trained with SGD on binary MNIST. 
    With SGD and no regularization (red curve in (b)),
    we interpolate the training data 
    and hence the predicted lower bound is $0$. 
    However, with early stopping (or weight decay) 
    we obtain tight guarantees. 
    (c) Accuracy vs gradient iteration on IMDb dataset 
    with unlabeled fraction fixed at 0.2. 
    In plot (c), `*' denotes the best test accuracy 
    with the same hyperparameters 
    and training only on clean data. 
    % Exact hyperparameters are included in
    See \appref{app:exp} for exact hyperparameter values. }
    \label{fig:error_binary}
    \vspace{-5pt}
\end{figure*}


Having established our framework theoretically, 
we now demonstrate its utility experimentally.
First, for linear models and wide networks 
in the NTK regime where our guarantee holds,
we confirm that our bound is not only valid, 
but closely tracks the generalization error.
Next, we show that in practical deep learning settings,
optimizing cross-entropy loss by SGD,
the expression for our (0-1) ERM bound 
nevertheless tracks test performance closely
and in numerous experiments
on diverse models and datasets
is never violated empirically. 
\vspace{5pt}

\noindent \textbf{Datasets {} {}} 
To verify our results on linear models,
we consider a toy dataset,
where the class conditional distribution $p(x|y)$
for each label is Gaussian.
For binary tasks, we use binarized CIFAR-10 (first 5 classes vs rest)~\citep{krizhevsky2009learning}, 
binary MNIST (0-4 vs 5-9)~\citep{lecun1998mnist}
and IMDb sentiment analysis dataset~\citep{maas2011learning}. 
For multiclass setup, we use MNIST and CIFAR-10.   
% We also consider a toy dataset 
% with Gaussian features 
% To verify our results on linear models,
% we consider a toy dataset,
% where the class conditional distribution $p(x|y)$
% for each label is Gaussian.
\vspace{5pt}

\noindent \textbf{Architectures {} {}} 
To simulate the NTK regime, 
we experiment with 2-layered wide networks 
both (i) with the second layer fixed 
at random initialization;
(ii) and updating both layers' weights.
% In overparameterized linear nets,
For vision datasets (e.g., MNIST and CIFAR10),
we consider (fully connected) 
multilayer perceptrons (MLPs) 
with ReLU activations
and ResNet18~\citep{he2016deep}. 
For the IMDb dataset, we train Long Short-Term Memory Networks 
(LSTMs;~\citet{hochreiter1997long}) with ELMo embeddings~\citep{Peters:2018} 
and
% and fine-tune an off-the-shelf uncased 
fine-tune an off-the-shelf uncased BERT model
\citep{devlin2018bert, wolf-etal-2020-transformers}. 
\vspace{5pt}

\noindent \textbf{Methodology {} {}} 
% To obtain our bound on the 
To bound the population error,
we require access to both 
clean
% training data
and unlabeled data.
For toy datasets, we obtain unlabeled data 
by sampling from the underlying distribution over $\calX$. 
For 
% real-world 
image and text datasets, 
we hold out a small fraction 
of the clean training data 
and discard their 
labels
to simulate unlabeled data.
% underlying true labels 
% to obtain an unlabeled dataset. 
We use the random labeling procedure 
% as explained 
described
in \secref{sec:setup}. 
After augmenting clean training data 
with randomly labeled data, 
we train in the standard fashion. 
% use the standard training procedure.  
% For each experiment,
% we report architecture, hyperparameters,
% and training details in
% We report  in
% we report the exact architecture 
% with hyperparameters and training procedure 
% in
% \tabref{table:experiments} 
See \appref{app:exp} for experimental details. 
%%% Maybe add 

\paragraph{Underparameterized linear models}
On toy Gaussian data, 
we train linear models 
with GD to minimize cross-entropy loss 
and mean squared error.
% First, we vary
Varying the fraction of randomly labeled data 
% added 
% and
we observe that the accuracy 
on clean unseen data 
is barely impacted
% remains almost the same 
(\figref{fig:error_binary}(a)). 
This highlights that in low dimensional models 
adding randomly labeled data 
with the clean dataset (in toy setup) 
has minimal effect on the performance on unseen clean data. 
Moreover, we find that RATT 
offers a tight lower bound 
on the unseen clean data accuracy. 
We observe the same behavior with 
Stochastic Gradient Descent (SGD) training 
(ref. \appref{app:exp}).   
Observe that the predicted bound goes up as 
the fraction of unlabeled data increases. 
While the accuracy as dictated by 
the dominating term in the RHS of \eqref{eq:thm1} 
decreases with an increase in the fraction 
of unlabeled data, 
we observe a relatively sharper 
decrease in $\mathcal{O}_p\left({1/\sqrt{m}}\right)$ 
term of the bound, leading to an overall increase 
in the predicted accuracy bound. 
In this toy setup, we also evaluated a kernel 
regression bound from \citet{bartlett2002rademacher} (Theorem 21), 
however, the predicted kernel regression bound remains vacuous. 


% \textbf{Overparameterized models {} {}}
\paragraph{Wide Nets} 
Next, we consider MNIST binary classification 
with a wide 2-layer fully-connected network.  
In experiments with SGD training on MSE loss
without early stopping 
or weight decay regularization, 
we find that adding extra randomly label data 
hurts the unseen clean performance 
(\figref{fig:error_binary}(b)). 
Additionally, due to the perfect fit 
on the training data, 
our bound is rendered vacuous. 
However, with early stopping (or weight decay), 
we observe close to zero performance difference 
with additional randomly labeled data.
% , i.e.,
% Instead,
% , the degradation depends 
% on the fraction of randomly labeled data,
% and at small fractions,
% We observe . 
Alongside, we obtain tight bounds 
on the accuracy on unseen clean data 
paying only a small price to negligible
for incorporating randomly labeled data. 
% And since with increase 
% in fraction of unlabeled data 
Similar results hold for SGD and GD 
and when cross-entropy loss 
is substituted for MSE (ref. \appref{app:exp}).  

\paragraph{Deep Nets} 
We verify our findings on 
(i) ResNet-18 and 5-layer MLPs
trained with binary CIFAR  
(\figref{fig:error_CIFAR10}); 
and (ii) ELMo-LSTM and BERT-Base models
fine-tuned on the IMDb dataset 
(\figref{fig:error_binary}(c)). 
% Parallel results with deep models 
% on binary MNIST are in
See \appref{app:exp} for additional results 
with deep models on binary MNIST. 
We fix the amount of unlabeled data 
at $20\%$ of the clean dataset size 
and train all models 
% the standard fashion
% in a conventional manner 
with standard hyperparameters. 
Consistently, we find 
that our predicted bounds
are never violated in practice.
% Consistently, we observe that the our bound 
% lower bounds the accuracy on the unseen clean data. 
And as training proceeds, 
the fit on the mislabeled data increases 
with perfect overfitting 
in the interpolation regime 
rendering our bounds vacuous. 
However, with early stopping, 
our bound predicts test performance closely. 
For example, on IMDb dataset with BERT fine-tuning 
we predict $79.8$ as the accuracy of the classifier, 
when the true performance is $88.04$ 
(and the best achievable performance on unseen data is $92.45$). 
Additionally, we observe 
that our method tracks the performance
from the beginning of the training 
and not just towards the end.
%% SG: Add numbers and more takeaways


Finally, we verify our multiclass bound
on MNIST and CIFAR10
with deep MLPs and ResNets
% although our bound (\thmref{thm:multiclass_ERM})
% requires an assumption,
% it is never violated empirically
(see results in \tabref{table:multiclass}
and per-epoch curves in \appref{app:exp}). 
As before, 
we fix the amount of unlabeled data 
at $20\%$ of the clean dataset 
to minimize cross-entropy loss via SGD.
In all four settings, our bound predicts 
non-vacuous performance on unseen data. 
In \appref{app:exp}, we investigate 
our approach on CIFAR100 
showing that even though 
our bound grows pessimistic 
with greater numbers of classes, 
the error on the mislabeled data nevertheless tracks population accuracy. 
% we evaluate our bound on CIFAR100. 
% Although, RATT renders vacuous bounds on CIFAR100, 
% we show that the error on mislabeled portion 
% (not identifiable with just the randomly labeled data)
% can nevertheless tightly track population accuracy on clean data (see Appendix).
% 
% We discuss this in detail in appendix. 
% This shows that while 
% In addition, we   
% on the combined data. 
% Our guarantees on MNIST and CIFAR10 are obtained 
% by trading-off for a small performance gap 
% due to early stopping and additional unlabeled data 
% (the difference between Test Acc. 
% and Best Acc. in \tabref{table:multiclass}).  


% With substantial weight decay, we can avoid any fit to the random data; highlight the usefulness of learning with high-learning rate and cite

 
\begin{table}[t]
    \centering
    \small
    \tabcolsep=0.12cm
    \begin{tabular}{@{}*{5}{c}@{}}
    \toprule
    Dataset & Model  & \thead{Pred. Acc}  & \thead{Test Acc.} & \thead{Best Acc.}\\
    % Dataset & Model & \thead{Test \\accuracy} & \thead{Predicted  \\bound} & \thead{Test accuracy \\(clean training)}\\
    \midrule
    % \multirow{2}{*}{MNIST}  & MLP & $95.37 - 10.3$ &   & \\
    \multirow{2}{*}{MNIST}  & MLP & $93.1$ & $97.4$  & $97.9$\\
    & ResNet & $96.8$ & $98.8$  & $98.9$ \\
    \midrule
    \multirow{2}{*}{CIFAR10}  & MLP & $48.4$  & $54.2$  & $60.0$ \\
    & ResNet & $76.4$  &  $88.9$  & $92.3$ \\
    \bottomrule 
    \end{tabular}  
    \caption{Results on multiclass classification tasks. With pred. acc. we refer to the dominating term in RHS of \eqref{eq:multiclass_ERM}. At the given sample size and $\delta=0.1$, the remaining term evaluates to $30.7$, decreasing our predicted accuracy by the same. We note that
    test acc. denotes the corresponding accuracy on unseen clean data. Best acc. is the best achievable accuracy with just training on just the clean data (and same hyperparamters except the stopping point). Note that across all tasks our predicted bound is tight and the gap between the best accuracy and test accuracy is small. Exact hyperparameters are included in \appref{app:exp}.  }\label{table:multiclass}
    \vspace{-10pt}
\end{table}

% \citet{hu2019simple}




% 
%  OLD VERSION --- CACHED
% 

% \begin{figure*}[t!]
%     \centering 
%     % \vspace{-15pt}
%     % \includegraphics[width=0.9\linewidth]{example-image-a}
%     \includegraphics[width=0.3\linewidth]{figures/lowdim-Gaussian.pdf}\hfil
%     \includegraphics[width=0.3\linewidth]{example-image-b}\hfil
%     \includegraphics[width=0.3\linewidth]{example-image-c}
%     % \includegraphics[width=0.9\linewidth]{figures/{CIFAR10_rn=0.1_lr=0.2_wd=0.005}.png}
%     \vspace{-5pt}
%     \caption{ \textbf{Predicted lower bound on toy Gaussian dataset} with linear model. Performance aggregated over $5$ runs with different seeds. \textbf{Left:}
%     \textbf{Right:} }
%     \label{fig:error_binary}
%     \vspace{-10pt}
% \end{figure*}


% Having established our framework theoretically, 
% we now demonstrate its utility experimentally.
% First, for linear models and wide networks 
% in the NTK regime where our guarantee holds,
% we confirm that our bound is not only valid, 
% but closely tracks the generalization error.
% Next, we show that in practical deep learning settings,
% optimizing cross-entropy loss by SGD,
% % (beyond the scope of our analysis),
% % the bound predicted by our ERM analysis
% the expression for our (0-1) ERM bound 
% nevertheless tracks test performance closely
% and in numerous experiments
% on diverse models and datasets
% is never violated empirically. 
% % 
% %%%%%%%%% New addition 

% Note that the main conjecture underlying
% our bound on (clean) population error  
% is the lemma that with training with the
% mixture of clean and randomly labeled data, 
% we obtain a classifier whose empirical error 
% on the mislabeled training data
% is lower than its population error 
% on the distribution of mislabeled data.  
% While we prove this \lemref{lem:fit_mislabeled})
% for ERM on 0-1 loss, for linear models,
% and in the NTK regime,
% we must assume it
% to extend our bound to deep models. 
% By way of justification, 
% in the later stages of training,
% given the ability of neural networks 
% to interpolate the data,
% this assumption seems uncontroversial.
% Moreover, concerning the early phases of training,
% recent research has shown that 
% learning dynamics for complex deep networks 
% resemble those for linear models \citep{hu2020surprising},
% much like the wide neural networks that we do analyze.
% Together, these arguments help 
% to explain the practical applicability
% of our bound in deep learning.
% % These findings may hint at an explanation 
% % for the apparent broader applicability of our framework. 

% % 
% % 
% % Note that the main conjecture underlying
% % our bound on (clean) population error  
% % is claim that with training with the
% % mixture of clean and randomly labeled data, 
% % we obtain lower error on the mislabeled training data
% % than the on the population of mislabeled data.  
% % If we can make this assumption for deep models, our results readily extend to deep models. 
% % %%%%%%%%%%%%%%%%
% % % we note that while steps remain to prove 
% % % that our theoretical guarantees 
% % % bind for practical deep networks,
% % Furthermore, recent research has shown 
% % that in the early phases of training,
% % learning dynamics for complex deep networks 
% % resembles those for linear models \citep{hu2020surprising},
% % similar to wide neural networks
% % (which we do analyze).
% % These findings may hint at an explanation 
% % for the apparent broader applicability of our framework. 
% % % 
% % Add something that this a conjecture and the underlying claim is that we overfit on the mislabeled training better than the unseen mislabeled data.   
% %

% % it has been shown that early-time learning dynamics for more complex nets mimic simple linear models 
% % with a relatively very mild requirement on width
% % \citep{hu2020surprising}. This already hints that our theoretic framework extends more broadly, i.e., in the early phase of the learning when complex models can be approximated by linear models. 
% % While we don't analyze deep network, similar to wide neural networks, it has been shown that early-time learning dynamics for more complex nets mimic simple linear models 
% % % with a relatively very mild requirement on width
% % \citep{hu2020surprising}. This already hints that our theoretic framework extends more broadly, i.e., in the early phase of the learning when complex models can be approximated by linear models. 

% % we now experimentally illustrate the effectiveness of our bound. 
% % We highlight the strong predictive power 
% % of our generalization bound 
% % in three sets of models: (i) underparameterized linear models; (ii) overparameterized linear models; and (iii) deep models. 
% % Empirically, we show our theoretical results extend to complex neural networks suggesting that our framework holds more broadly.     
% % In a nutshell
% % Overall, our findings illustrate that networks first fit the clean data avoiding fit to the mislabeled data, which can be leveraged to guarantee tight bounds on the clean population error. 

% % Having established our framework theoretically, 
% % we now demonstrate experimentally 
% % that even in practical deep learning settings,
% % optimizing cross-entropy loss by SGD
% % (beyond the scope of our analysis),
% % the bound predicted by our ERM analysis
% % tracks test performance closely
% % and is never (in our experiments)
% % violated empirically. 
% % % we now experimentally illustrate the effectiveness of our bound. 
% % We highlight the strong predictive power of our generalization bound 
% % in three sets of models: (i) underparameterized linear models; (ii) overparameterized linear models; and (iii) deep models. 
% % Empirically, we show our theoretical results extend to complex neural networks suggesting that our framework holds more broadly.     
% % % In a nutshell
% % Overall, our findings illustrate that networks first fit the clean data avoiding fit to the mislabeled data, which can be leveraged to guarantee tight bounds on the clean population error. 

% %% Make it like paragraph 

% \textbf{Datasets {}} 
% For binary tasks, we use binarized CIFAR-10 
% (animals vs vehicles) \citep{krizhevsky2009learning}, 
% binary MNIST (0-4 vs 5-9)~\citep{lecun1998mnist}
% and IMDb sentiment analysis dataset~\citep{maas2011learning}. 
% For multiclass setup, we use MNIST and CIFAR-10.   
% We also consider a toy dataset 
% with Gaussian features 
% to study linear models.  

% \textbf{Architectures {}} 
% % In overparameterized linear nets,
% For vision datasets (e.g., MNIST and CIFAR10),
% we consider (fully connected) 
% multilayer perceptrons (MLPs) 
% with ReLU activations
% and ResNet18~\citep{he2016deep}. 
% For the IMDb dataset, we use 
% Bidirectional Long Short-Term Memory Networks 
% (Bi-LSTMs;~\citet{graves2005framewise}) 
% % and fine-tune an off-the-shelf uncased 
% an off-the-shelf BERT Base model
% \citep{devlin2018bert, wolf-etal-2020-transformers}. 
% % 
% To simulate the NTK regime, 
% we experiment with 2-layered wide networks 
% both (i) with the first layer fixed 
% at random initialization;
% (ii) and updating both layers' weights.

% % and both layers training.


% \textbf{Methodology {}} 
% To obtain our bound on the population error,
% % generalization performance, 
% % we need access to unlabeled data 
% we require access to both 
% % unlabeled data
% % along with
% clean training data and unlabeled data.
% For toy datasets, we obtain unlabeled data 
% by sampling from the underlying distribution over $\calX$. 
% % On the other hand, 
% For real-world image and text datasets, 
% we hold out a small fraction 
% of the clean training data 
% and discard the underlying true labels 
% to obtain an unlabeled dataset. 
% We use the random labeling procedure 
% as explained in \secref{sec:setup}. 
% After augmenting clean training data 
% with randomly labeled data, 
% we use the standard training procedure.  
% %depending on the selected dataset and underlying architecture. 
% % 
% % For reproducibility,
% %  summarizes our experiments. 
% For each experiment,
% we report the exact architecture %description
% with hyperparameters and training procedure 
% in \tabref{table:experiments} (\appref{app:exp}). 
% % Throughout this section, we just compare the dominating term of our generalization bound. \todos{Not sure how to mention this?}


% % Our finding is surprising 
% % While one might argue that overparameterized models can potentially fit the randomly labeled data added during training 



% % If the training procedure interpolates on the training data, fitting the randomly labeled data perfectly, our bound will   

% % A curious reader might ask that these bounds may lead to overfitiing and hemce vacuous guarantees
% % A few questions naturally arise: 

% % Implication for low dimensional models. For high dimensions we miogt regularization soltuion ....Even in these settings generalization is not trivial with out underlying distributinal assumptions 


% \subsection{Underparameterized linear models}
% We train linear models on Gaussian toy dataset with GD 
% to minimize cross-entropy loss and mean squared error.
% First, we vary the fraction of randomly labeled data added 
% and observe that the accuracy on clean unseen data 
% remains almost the same (\figref{fig:error_binary}(left)). 
% This highlights that in low dimensional models 
% adding randomly labeled data 
% with the clean dataset (in toy setup) 
% has minimal effect on the performance on unseen clean data. 
% Moreover, 
% % the performance predicted by our bound 
% % our bound 
% % the performance predicted by our bound
% We find that RATT offers a tight lower bound 
% on the unseen clean data accuracy. 
% We observe the same behavior with 
% Stochastic Gradient Descent (SGD) training 
% (ref. \appref{app:exp}).   

% % Gaussian dataset 

% % \textbf{Effect on fit on clean data}

% % \todos{Stability assumptions verify?}

% \subsection{Overparameterized models}
% \textbf{Wide Nets {}} 
% Next, we consider MNIST binary classification 
% with a wide 2-layer fully-connected network.  
% % we consider Gaussian dataset with feature dimension greater than the number of samples to train linear models. 
% % \figref{fig:error_binary}(b)
% % shows results 
% In experiments with SGD training on MSE loss
% % We observe that 
% without early stopping or weight decay regularization, 
% we find that adding extra randomly label data 
% hurts the unseen clean performance 
% (\figref{fig:error_binary}(b)). 
% Additionally, due to the perfect fit 
% on the training data, 
% our bound is rendered vacuous. 
% However, with early stopping (or weight decay), 
% % we observe a relatively minor impact
% % of
% additional randomly labeled data
% does not degrade performance much.
% % While the impact is higher at large fractions,
% Moreover, the degradation depends 
% on the fraction of randomly labeled data,
% and at small fractions,
% we observe close to zero performance difference. 
% Additionally, we obtain tight bounds 
% on the accuracy on unseen clean data 
% paying only a small price 
% for incorporating randomly labeled data.
% % trading off for a small performance gap. 
% % We show that similar results hold when we use
% Similar results hold for SGD and GD 
% and when cross-entropy loss 
% is substituted for MSE (\appref{app:exp}).  


% % \todos{NTK for MNIST? Vary width of NTK??}

% \textbf{Deep Nets {}} 
% % Similar to wide neural networks, it has been shown that early-time learning dynamics for more complex nets mimic simple linear models 
% % with a relatively very mild requirement on width
% % \citep{hu2020surprising}. This already hints that our theoretic framework extends more broadly, i.e., in the early phase of the learning when complex models can be approximated by linear models. 
% % We now test our framework on complex models and show that we can bound gene 
% % In addition to verifying our claims on 
% We verify our findings on: 
% i) ResNet-18 and 5 layer MLP 
% trained with binary CIFAR (`dog' vs `cat') (\figref{fig:error_CIFAR10}); 
% ii) Bi-LSTM and fine-tuned BERT Base model on IMDb dataset (\figref{fig:error_binary}(c)). 
% Parallel results with deep models 
% on binary MNIST are in \appref{app:exp}. 
% We fix the amount of unlabeled data 
% at $10\%$ of the clean training data 
% and train all the models 
% in a conventional manner 
% with standard hyperparameters. 
% Consistently, we observe that the fit 
% on the mislabeled portion of the training data 
% exceeds the accuracy on the unseen clean data. 
% And as training proceeds, 
% the fit on the mislabeled data increases 
% with perfect overfitting in the interpolation regime. 
% Hence, with early stopping, 
% our bound predicts test performance closely. 
% Additionally, we observe 
% that our method tracks the performance
% from the beginning of the training 
% and not just towards the end.  
% % As the networks overfit the training, our g 

% % \todos{Usefulness in noisy data setup}

% % Assumption 
% % 
% %
% %
% %%\textbf{Multiclass classification.} 
% For multiclass classification on MNIST and CIFAR10
% with deep MLPs and ResNets,
% although our bound (\thmref{thm:multiclass_ERM})
% % has not yet been proven,
% % is not guaranteed to hold,
% requires an  assumption
% it is never violated empirically
% % provides non-vacuous guarantees
% (see results in \tabref{table:multiclass}
% and per-epoch curves in \appref{app:exp}). 
% As before, we fix the amount of unlabeled data 
% at $10\%$ of the clean dataset 
% % and use SGD 
% to minimize cross-entropy loss via SGD.
% % on the combined data. 
% % Our guarantees on MNIST and CIFAR10 are obtained 
% % by trading-off for a small performance gap 

% due to early stopping and additional unlabeled data 
% (the difference between Test Acc. 
% and Best Acc. in \tabref{table:multiclass}).  


% % With substantial weight decay, we can avoid any fit to the random data; highlight the usefulness of learning with high-learning rate and cite



% % \citet{hu2019simple}

% \begin{table}[t]
%     \centering
%     \small
%     \tabcolsep=0.12cm
%     \begin{tabular}{@{}*{5}{c}@{}}
%     \toprule
%     Dataset & Model  & \thead{Pred. Bound}  & \thead{Test Acc.} & \thead{Best Acc.}\\
%     % Dataset & Model & \thead{Test \\accuracy} & \thead{Predicted  \\bound} & \thead{Test accuracy \\(clean training)}\\
%     \midrule
%     \multirow{2}{*}{MNIST}  & MLP &  &   & \\
%     & ResNet &  &   & \\
%     \midrule
%     \multirow{2}{*}{CIFAR10}  & MLP &  &   & \\
%     & ResNet &  &   & \\
%     \bottomrule 
%     \end{tabular}  
%     \caption{Experiments performed}\label{table:multiclass}
% \end{table}

% % \begin{center}
% %     \begin{table}[H] \small 
% %         \centering
% %         \begin{tabular}{||c|c|c|c|c||} 
% %         \hline
% %         Dataset & Model & Test accuracy & \thead{Predicted \\ lower bound} & \thead{Test accuracy \\(with clean training)}  \\ [0.5ex] 
% %         \hline
% %         \hline
% %         \multirow{2}{*}{MNIST} & 5-MLP &  & &  \\
% %                         \cline{2-5}
% %                          & ResNet-18 & & & \\
% %         \hline
% %         \multirow{2}{*}{CIFAR10} & 5-MLP & & & \\
% %                         \cline{2-5}
% %                          & ResNet-18  & & & \\
% %         \hline
% %         \end{tabular}
% %         \caption{Experiments performed} \label{table:multiclass}
% %     \end{table}    
% %     % \footnotetext[6]{We use both MSE loss and cross-entropy loss.}
% %     \footnotetext[6]{We try 2 variants: one with a fixed first layer and the other with both layers trainable.}
% % \end{center}


% % \begin{figure}[t!]
% %     \centering 
% %     % \vspace{-15pt}
% %     % \includegraphics[width=0.9\linewidth]{example-image-a}
% %     % \includegraphics[width=0.3\linewidth]{figures/lowdim-Gaussian.pdf}\hfil
% %     \includegraphics[width=0.49\linewidth]{example-image-b}\hfil
% %     \includegraphics[width=0.49\linewidth]{example-image-c}
% %     % \includegraphics[width=0.9\linewidth]{figures/{CIFAR10_rn=0.1_lr=0.2_wd=0.005}.png}
% %     \vspace{-5pt}
% %     \caption{ \textbf{Predicted lower bound on toy Gaussian dataset} with linear model. Performance aggregated over $5$ runs with different seeds. \textbf{Left:}
% %     \textbf{Right:} }
% %     \label{fig:error_multi}
% %     \vspace{-10pt}
% % \end{figure}


% % \begin{itemize}
% %     \item Experiments by throwing away the mislabeled data (or even keeping it) after the early stopping point (check how much the gradients actually move)
% %     \item effect of increasing capacity ?
% %     \item check linearity till we overfit?
% %     \item Extend to self-supervised? 
% % \end{itemize}
