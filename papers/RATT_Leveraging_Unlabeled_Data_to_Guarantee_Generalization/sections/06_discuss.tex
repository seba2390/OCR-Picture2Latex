% Having presented our theoretical and experimental results, 
% We now situate our contribution 
% within the existing literature on deep learning. 


\paragraph{Implicit bias in deep learning} 
Several recent lines of research 
attempt to explain the generalization of neural networks 
despite massive overparameterization
via the \emph{implicit bias} of gradient descent
\citep{soudry2018implicit,gunasekar2018implicitb,gunasekar2018implicita,ji2019implicit,chizat2020implicit}. 
Noting that even for overparameterized linear models,
there exist multiple parameters 
capable of overfitting the training data
(with arbitrarily low loss), 
of which some generalize well
and others do not,
they seek to characterize the favored solution.
Notably, \citet{soudry2018implicit}
find that for linear networks,
gradient descent converges (slowly)
to the max margin solution.
% 
A complementary line of work
focuses on the early phases of training,
finding both empirically~\citep{rolnick2017deep,arpit2017closer} 
and theoretically~\citep{arora2019fine,li2020gradient,liu2020early}
that even in the presence 
of a small amount of mislabeled data, 
gradient descent is biased 
to fit the clean data first
during initial phases of training. 
However, to best our knowledge,
no prior work leverages this phenomenon 
to obtain generalization guarantees on the clean data,
which is the primary focus of our work. 
% 
Our method exploits this phenomenon 
to produce non-vacuous generalization bounds.
Even when we cannot prove \emph{a priori}
that models will fit the clean data well
while performing badly on the mislabeled data,
we can observe that it indeed happens (often in practice),
and thus, \emph{a posteriori}, 
provide tight bounds on the population error.
Moreover, by using regularizers
like early stopping or weight decay, 
we can accentuate this phenomenon,
enabling our framework to
provide even tighter guarantees.


\paragraph{Generalization bounds}
Conventionally, generalization in machine learning 
has been studied through the lens of uniform 
convergence bounds~\citep{blumer1989learnability, vapnik1999overview}.  
% Understanding generalization 
% in overparameterized networks  
% has attracted significant interests. 
Representative works on understanding generalization 
in overparameterized networks
within this framework
% of research 
include 
\citet{neyshabur2015norm,neyshabur2017pac,neyshabur2017exploring,neyshabur2018role,dziugaite2017computing, bartlett2017spectrally,arora2018stronger,li2018learning,zhou2018non, allen2019learning,nagarajan2019deterministic}. 
However, uniform convergence based 
bounds typically remain numerically 
loose relative to the true generalization error. 
Several works have also questioned the ability of 
uniform convergence based approaches to 
explain generalization in overparameterized models~\citep{zhang2016understanding,nagarajan2019uniform}. 
% 
% In light of the inapplicability of 
% traditional complexity-based bounds
% to deep neural networks 
% \citep{zhang2016understanding,nagarajan2019uniform},
% researchers have investigated alternative strategies
% to provide non-vacuous generalization bounds for deep nets
% \citep{neyshabur2015norm,neyshabur2017pac,neyshabur2017exploring,neyshabur2018role,dziugaite2017computing, bartlett2017spectrally,xu2017information, arora2018stronger,li2018learning, allen2019learning,pensia2018generalization, zhou2018non, nagarajan2019deterministic,
% nakkiran2020deep}.
% However, these bounds typically 
% remain numerically loose 
% relative to the true generalization error. 
% \
% \citet{dziugaite2017computing, zhou2018non} 
% provide non-vacuous generalization guarantees. 
% Specifically, they transform a base network
% into consequent networks 
% that do not interpolate the training data 
% either by adding stochasticity 
% to the network weights~\citep{dziugaite2017computing}
% or by compressing the original neural network~\citep{zhou2018non}.
% 
Subsequently, recent works have proposed 
unconventional ways to derive generalization bounds~\citep{negrea2020defense,zhou2020uniform}. 
In a similar spirit, we take departure from 
complexity-based approaches to generalization bounds  
in our work. 
In particular, we leverage unlabeled data 
to derive a post-hoc generalization bound.  
Our work provides 
guarantees on overparameterized networks 
by using early stopping or weight decay regularization, 
preventing a perfect fit on the training data.  
Notably, in our framework, the model
can perfectly fit the clean portion of the data,
so long as they nevertheless fit the mislabeled data poorly.



\paragraph{Leveraging noisy data to provide generalization guarantees}
In parallel work, \citet{bansal2020self} 
presented an upper bound 
on the generalization gap 
of linear classifiers 
trained on representations 
learned via self-supervision. 
Under certain noise-robustness 
and rationality assumptions 
on the training procedure, 
the authors obtained bounds 
dependent on the complexity 
of the linear classifier 
and independent of the complexity of representations. 
By contrast, 
we present generalization bounds 
for 
% the 
% usual end-to-end 
supervised learning 
% the standard supervised learning setting
that are non-vacuous by virtue 
of the early learning phenomenon. 
% While similar in spirit,
% Still, 
While both frameworks highlight
how robustness to random label corruptions 
can be leveraged to obtain bounds 
that do not depend directly
on the complexity of the underlying hypothesis class,
% 
our framework, methodology, claims, 
and generalization results
are very different from theirs.
% from \citet{bansal2020self}.
% are very different from ours.


% We believe that this line work has the potential ... 


% 
%  OLD VN --- CACHED
% 



% % In this section, we discuss the conceptual findings from our work and contrast them with the existing understanding of deep learning.  

% Having presented our theoretical and experimental results, 
% we now situate our contribution within the existing
% literature on deep learning. 

% \textbf{Implicit bias in early learning.} 
% Several recent theoretical studies 
% have focused on understanding 
% the \emph{implicit bias} of gradient descent
% in overparameterized models 
% \citep{soudry2018implicit,gunasekar2018implicitb,gunasekar2018implicita,ji2019implicit,chizat2020implicit}. 
% These studies highlight 
% % that even though in overparameterized models 
% that for overparameterized models,
% even though there exist 
% % there exist multiple classifiers
% multiple parameters capable of
% overfitting the training data
% (with arbitrarily low loss), 
% gradient descent converges to a specific one that generalizes well on unseen data. Our empirical and theoretical findings suggest a complementary perspective: even in presence of a small amount of mislabeled data, gradient descent training in overparameterized models is biased to learn from clean data during the initial phase of the training. With explicit regularizers like early stopping or weight decay, we can leverage this phenomenon to obtain a tight guarantee on the population error of the clean data. 
% While the first finding has been discussed empirically~\citep{rolnick2017deep,arpit2017closer} and theoeretically~\citep{arora2019fine,li2020gradient,liu2020early}, to best our knowledge none of the prior works related this phenomenon to obtain generalization guarantees on the clean data which is the primary focus of our work. 



% \textbf{Non-vacuous generalization bounds.} 
% Understanding generalization in overparameterized networks is an important open problem~\citep{zhang2016understanding,nagarajan2019uniform}. This area has attracted considerable attention~\citep{neyshabur2015norm,neyshabur2017pac,neyshabur2017exploring,neyshabur2018role,dziugaite2017computing, bartlett2017spectrally,arora2018stronger,li2018learning, allen2019learning, zhou2018non,nagarajan2019deterministic,nakkiran2020deep}, but generally these bounds are numerically very loose relative to the true generalization error. 
% % Rather, their main purpose is to provide qualitative evidence that the property underlying the generalization bound is indeed relevant to the generalization performance. % Can add this as well 
% %
% Like the present paper,~\citep{dziugaite2017computing, zhou2018non} provide non-vacuous generalization guarantees. Specifically, authors provide guarantees on consequent networks that don't interpolate on the training data either by adding stochasticity in the network weights~\citep{dziugaite2017computing} or by compressing the original neural network~\citep{zhou2018non}.
% In a similar spirit, our work provides guarantees on overparameterized networks by using early stopping or weight decay regularization, inhibiting a perfect fit on the training data.  

% % While our approach is substantially different, similar to our results, both these methods provide guarantees when the underlying network is not in the interpolation regime, i.e. training error is non-zero.  

 
% \textbf{Leveraging noisy data to provide generalization guarantees.} In a parallel work, \citet{bansal2020self} presented a new upper bound on the generalization gap of linear classifiers trained on representations learned via self-supervision. 
% Under certain noise-robustness and rationality assumptions on the training procedure, the authors obtained bounds dependent on the complexity of the linear classifier and independent of the complexity of representations. 
% On the contrary, in our work, we present generalization bounds for the usual end-to-end supervised learning which are non-vacuous by the virtue of the early learning phenomenon. 
% % ZL and SB: Not sure what is the best way to put this message? Please let me know if you think this is fine, we would not want to put this and convey their is no novelty in our idea. 
% While their framework, methodology, claims, and generalization results are very different from ours, like their work our work also highlights 
% how robustness to random label corruptions can be leveraged to obtain bounds that do not directly depend on the underlying complexity. 
% % We believe that this line work has the potential ... 


