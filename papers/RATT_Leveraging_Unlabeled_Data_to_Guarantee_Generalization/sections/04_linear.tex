
% \subsection{Bounds for linear models}
In the previous section, 
we presented results with ERM on 0-1 loss. 
While minimizing the 0-1 loss is hard in general,
%due to the underlying non-convexity, 
these results provide important theoretical insights. 
In this section, %motivated by these findings, 
% extending those results,
we show parallel results for linear models 
trained with Gradient Descent (GD).  


% squared error over a given training set $S$.
% % , i.e. 
% % % \begin{align}
% % $\wh w  \defeq \argmin_{w\in R^d} \, \Expt{(x,y) \sim \calS}{L(x,y;w)} \,$,  
% % % \end{align}
% % where $L(x,y;w) = (w^Tx - y)^2$. 
% More generally,

% But first,
To begin, we introduce the setup and some additional notation.
% the setup and notation we will be using throughout this section.
For simplicity, we begin discussion with binary classification with $\calX = \Real^d$. 
% Results for multiclass classification deferred to \appref{app:multiclass_linear}. 
Define a linear function $f(x; w) \defeq w^T{x}$ 
for some $w \in \Real^d$ and $x\in \calX$. %\todos{Change all the $R^d$ to $\Real^d$.}
% Given training set $S \defeq \{ (x_i, y_i)\}_{i=1}^n$,
Given training set $S$,
we suppose that the parameters of the linear function 
are obtained via gradient descent on 
the following $L_2$ regularized problem: 
% \begin{align}
%     \calL_S(w) \defeq \Expt{(x,y) \sim \calS}{(w^Tx - y)^2} + \lambda \norm{w}{2} \,, \label{eq:l2_MSE}   
% \end{align}
% \begin{align}
%     \calL_S(w) \defeq \sum_{(x_i,y_i) \sim \calS}{(w^Tx_i - y_i)^2} + \lambda \norm{w}{2} \,, \label{eq:l2_MSE}   
% \end{align}
\begin{align}
    % n in denominator is avoided deliberately
    \calL_S(w; \lambda) \defeq \sum_{i=1}^n{(w^Tx_i - y_i)^2} + \lambda \norm{w}{2}^2 \,, \label{eq:l2_MSE}   
\end{align}
where $\lambda\ge0$ is a regularization parameter. 
%  i.e., $\widehat w \defeq \argmin_{w\in R^d} \, \calL_S(w)$.  
% In this section, we will only focus on binary classification problem. 
% We restrict 
% attention to binary classification, noting that
% results on multiclass classification can be obtained 
% as in \secref{subsec:multiclass}.  
% 
% \wh w   \argmin_{w\in R^d}
% 
% 
% justfiy the use of squared loss by citing belkin's recent papers nad NTK work.
% 
Our choice to analyze squared loss minimization for linear networks 
is motivated in part by its analytical convenience, and follows
recent theoretical work which analyze 
neural networks trained via squared loss minimization
in the Neural Tangent Kernel (NTK) regime when 
they are well approximated by linear networks~\citep{jacot2018neural,arora2019fine,du2019gradient,hu2019simple}. 
% Moreover, in practice, it has been shown 
Moreover, recent research suggests that for classification tasks,
squared loss minimization performs comparably to cross-entropy loss 
minimization
\citep{muthukumar2020classification,hui2020evaluation}. 
% 
% 
% \todos{Not sure if people put this, check recent papers}
% 
% Now we introduce the main result of this section. 
%

For a given training set $S$, 
% we define a perturbed training set $S^\prime_{(i)}$ 
% with the $i^\text{th}$ point replaced 
% by a new sample $(x,y) \sim \calD$. 
we use $S_{(i)}$ to denote the training set $S$ 
with the $i^{\text{th}}$ point removed.
We now introduce one stability condition:
% \footnote{For simplicity of exposition, 
% we present interpretable but a stronger condition here. 
% We present results under the weaker stability condition
% in \appref{app:proof_gd}.}:
%  that we assume gradient descent training samples from $\calD$. 
% Define $S^\prime$ as the
% \begin{condition}[Train Error Stability]
% % [Bounded difference condition]
% \label{cond:train_stability} 
%     % With a random data set $S$, 
%     We have $\alpha$ train error stability 
%     if our training algorithm $\calA$ satisfies 
%     the following bounded difference condition, 
%     almost surely: 
%     % \begin{align*}
%         $\abs{\error_{\calS}\left(f({\calA, S})\right) - \error_{\calS^\prime_{(i)}}(f({\calA, S^\prime_{(i)}}))} \le \alpha/\abs{S}\,$.
%         % $\sum_{i=1}^n\left(\error_{\calS}\left(f({\calA, S})\right) - \error_{\calS^\prime_i}(f({\calA, S^\prime_{(i)}}))\right)^2 \le \alpha\,$.
%         % $\sum_{i=1}^n\left(\error_{\calS}\left(f({\calA, S})\right) - \error_{\calS^\prime_i}\left(f({\calA, S^\prime_{(i)}})\right)\right)^2 \le \beta\,$.
%     % \end{align*}
% \end{condition}
% This is a standard bounded difference condition
% \citep{boucheron2013concentration} 
% applied to the training error. 
% We use this condition to show 
% an exponential concentration 
% of the training error 
% around the expected training error. 
% Intuitively, \codref{cond:train_stability} states 
% that by perturbing one training point 
% we don't change the empirical error much. 
% In particular, this condition 
% is satisfied for $\alpha=1$ 
% if by perturbing one point, 
% we only end up changing prediction 
% at one data point in the training set. 
% Note that, we needed a similar condition 
% even in \lemref{lem:clear_error}, 
% but we got it for free 
% by performing ERM on 0-1 error.
% % but we got it for free since we were doing ERM training with the 0-1 loss. 
% %  We derived this condition 
% % that We needed the same condition   
% % , the  notion of stability 


% present stability assumption and finish the proof. Justify the stability assumptions  


%% TODO: can weaken the condition to a high probability bound. 
% \begin{condition}[Error Stability] 
%     \label{cond:error_stability}
%     We have $\beta$ error stability 
%     if our training algorithm $\calA$ satisfies, almost surely: 
%     % \begin{align*}
%     % ${\sum_{i=1}^n \frac{\error_{\calD}( f(\calA, S_{(i)}))}{n} - \error_\calD(f(\calA, S))} \le \beta\,$.
%     $\error_{\calD}( f(\calA, S_{(i)})) - \error_\calD(f(\calA, S)) \le \beta\,$.
%     % \end{align*}
% \end{condition}


\begin{condition}[Hypothesis Stability] 
    \label{cond:hypothesis_stability}
    We have $\beta$ hypothesis stability 
    if our training algorithm $\calA$ satisfies the following for all $i \in \{1,2,\ldots, n\}$: 
    \begin{align*}
    % ${\sum_{i=1}^n \frac{\error_{\calD}( f(\calA, S_{(i)}))}{n} - \error_\calD(f(\calA, S))} \le \beta\,$.
    \Expt{\calS, (x,y) \in \calD}{ \abs{\error\left( f(x) ,y  \right) - \error\left( f_{(i)}(x), y \right) }} \le \frac{\beta}{n} \,,
    \end{align*}
    where $f_{(i)} \defeq f(\calA, S_{(i)})$ and $ f \defeq f(\calA, S)$.
\end{condition}

% Define Leave-One-Out (LOO) error as $$\error_{\LOO} = \sum_{i=1}^n \error_{\calD}(\widehat f_{(i)})/n$$
This condition is similar to a notion of stability 
called \emph{hypothesis stability} 
\citep{bousquet2002stability,kearns1999algorithmic,elisseeff2003leave}. 
Intuitively, \codref{cond:hypothesis_stability} states 
that 
%  show the concentration 
% of the 
empirical leave-one-out error 
and average population error of leave-one-out classifiers
are close.
% 
% the average population error of classifiers, 
% each obtained by leaving one point out in the training set, 
% cannot be much larger 
% than the population error 
% of the original classifier.
% \todos{Fix this?}
This condition is mild 
and does not guarantee generalization. 
We discuss the implications in more detail in \appref{app:discuss_cond1}.

Now we present the main result of this section. 
As before, we assume access to a clean dataset 
$S = \{(x_i, y_i)\}_{i=1}^n \sim \calD^n$ 
and randomly labeled dataset 
$\wt S = \{(x_i, y_i)\}_{i=n+1}^{n+m} \sim \wt \calD^m$. 
Let $\bX = [x_1, x_2, \cdots, x_{m+n}]$ 
and $\by = [y_1, y_2, \cdots, y_{m+n}]$. 
Fix a positive learning rate $\eta$ such that 
$\eta \le 1/\left(\norm{\bX^T\bX}{\text{op}} + \lambda^2\right)$ 
and an initialization $w_0 = 0$. 
% \todos{Assumption made for simplicty}. 
Consider the following gradient descent iterates 
to minimize objective \eqref{eq:l2_MSE} on $S \cup \wt S$:
\begin{align}
w_t = w_{t-1} - \eta \grad_w \calL_{S \cup \wt S} (w_{t-1}; \lambda) \quad \forall t=1,2,\ldots \,. \label{eq:GD_iterates}
\end{align} 
Then we have $\{ w_t\}$ converge to the limiting solution 
$\wh w = \left( \bX^T\bX+\lambda \boldsymbol{I}\right)^{-1}\bX^T\by$. Define $\widehat f (x) \defeq f(x ; \wh w) $.  

%%% Change the theorem statement to only have the statement.
% \begin{theorem} \label{thm:linear}
%     % Fix a positive learning rate 
%     % $\eta \le \frac{1}{\norm{\bX^T\bX}{\text{op}} + \lambda^2}$ 
%     % and an initialization $w_0 = 0$. 
%     % % \todos{Assumption made for simplicty}. 
%     % Consider the following gradient descent iterates 
%     % to minimize objective \eqref{eq:l2_MSE} on $S \cup \wt S$:
%     % % \begin{align}
%     % $w_t = w_{t-1} - \eta \grad_w \calL_{S \cup \wt S} (w_{t-1}; \lambda) \,,$ for all $t=1,2,\ldots\,.$ %\quad \forall t=1,2,\ldots
%     % % \end{align} 
%     % Then $\{ w_t\}$ converge to a limiting solution 
%     % $\wh w = \left( \bX^T\bX+\lambda \boldsymbol{I}\right)^{-1}\bX^T\by$. 
%     % Define $\widehat f (x) \defeq f(x ; \wh w) $.  
%     % 
%     Assume that this gradient descent algorithm satisfies \codref{cond:error_stability}
%     with $\beta=\calO(\frac{1}{n+m})$.  
%     Then for any $\delta >0$, with probability at least $1-\delta$ 
%     over the random draws of datasets $\wt S$ and $S$, we have:
%     % \begin{align}
%         $\error_\calD(\widehat f) - \error_\calS(\widehat f) \le 1 - 2 \error_{\wt\calS}(\widehat f) + 3.5 \sqrt{\frac{\log(1/\delta)}{m}} + \calO\left(1/(n+m)\right) \,.$ %\label{eq:gd_error}
%     % \end{align} 
%     % for some constant $c\le 3.2$.
% \end{theorem}
% 
\begin{theorem}\label{thm:linear}
    Assume that this gradient descent algorithm satisfies \codref{cond:hypothesis_stability}
    with $\beta=\calO(1)$.  
    Then for any $\delta >0$, with probability at least $1-\delta$ 
    over the random draws of datasets $\wt S$ and $S$, we have:
    \begin{align*}
        \error_\calD(\widehat f) \le \error_\calS(\widehat f) &+ 1 - 2 \error_{\wt\calS}(\widehat f) + \sqrt{\frac{4}{\delta}\left(\frac{1}{m} +\frac{3\beta}{m+n} \right)} \\
        & + \left(\sqrt{2}\error_{\wt\calS}(\widehat f) + 1 + \frac{m}{2n} \right) \sqrt{\frac{\log(4/\delta)}{m}} \,. \label{eq:gd_error} \numberthis
    \end{align*} 
    % for some constant $c\le 3.2$.
\end{theorem}
% 
% \todos{Make theorem as direct as possible. Don't define things in the theorem.}
% Recall, . 
With a mild regularity condition, 
we establish the same bound 
on GD training with squared loss, 
notably the same dominating term on the population error, 
as in \thmref{thm:error_ERM}. In \appref{app:multiclass_linear}, we present the extension to multiclass classification, where we again obtain a result parallel to \thmref{thm:multiclass_ERM}. 

% The proof of this lemma mirrors the proof of lemma 1. 
% this is the technical difficulty different from above. 
\begin{hproof}
    Because squared loss minimization  
    does not imply 0-1 error minimization, 
    we cannot use arguments 
    from \lemref{lem:fit_mislabeled}. 
    This is the main technical difficulty.  
    To compare the 0-1 error at a train point with an unseen point, 
    we use the  closed-form expression for $\widehat{w}$.
    We show that the train error on mislabeled points  
    is less than the population error on the distribution of mislabeled data
    (parallel to \lemref{lem:fit_mislabeled}). 
    % Rest of the proof follows similar to \thmref{thm:error_ERM}. 
    
    % We can use arguments from the previous section to obtain parallel inequalities on squared error. But 
    % Because 
    % We can't simply use our 0-1 loss style arguments. Rather, 
    For a mislabeled 
    % unseen
    % (and an unknown) 
    training point $(x_i, y_i)$ in $\wt S$, we show that
    \begin{align}
        % \sum_{i=n+1}^m P_{x_i \sim \calD_\calX}(y_i x_i^T \widehat w \le 0 ) \le \sum_{i=n+1}^m P_{x_i \sim \calD_\calX}(y_i x_i^T \widehat w_{(i)} \le 0 )
        \indict{y_i x_i^T \wh w \le 0 } \le \indict{y_i x_i^T \wh w_{(i)} \le 0 } \,, \label{eq:loo_error}
    \end{align}
    where $\wh w_{(i)}$ is the classifier obtained
    by leaving out the $i^\text{th}$ point from the training set. 
    Intuitively, this condition
    states
    that the train error at a training point 
    is less than the leave-one-out error at that point, i.e. the error obtained 
    by removing that point and re-training.
    % the classifier. 
    % We 
    % then use \codref{cond:train_stability} 
    % to show an exponential convergence 
    % to expected train error 
    % (average of LHS in \eqref{eq:loo_error} for all $i$) 
    % and 
    % then
    % use
    Using \codref{cond:hypothesis_stability}, we then relate the average leave-one-out error 
    (over the index $i$ of the RHS in \eqref{eq:loo_error}) 
    to the population error on the mislabeled distribution 
    %with the original classifier $\wh w$ 
    to obtain an inequality similar to~\eqref{eq:lemma1}.
    % We complete the proof with showing concentration of empr
\end{hproof}



\paragraph{Extensions to kernel regression}
% Extention to kernel regression
% Extension to NTK 
% Holds for deep models when they are in NTK regime. 
% 
Since the result in \thmref{thm:linear} 
% agnostic 
does not impose any regularity conditions 
on the underlying distribution 
over $\calX\times \calY$, 
our guarantees %simply 
extend straightforwardly to kernel regression 
by using the transformation $x\to \phi(x)$ 
for some feature transform function $\phi$. 
Furthermore, recent literature 
has pointed out a concrete connection 
between neural networks 
and kernel regression 
with the so-called 
\emph{Neural Tangent Kernel} (NTK) 
which holds in a certain regime
where weights do not change much 
during training~\citep{jacot2018neural,du2019gradient,du2018gradient,chizat2019lazy}. 
% Unlike linear models, 
% % note that in the NTK regime 
% in the NTK regime, 
% even though the model is linear in parameters, 
% it is still non-linear in the inputs.  
% Our work contributes to this line of work, in particular
% 
Using this concrete correspondence, 
our bounds on the clean population error 
(\thmref{thm:linear}) 
extend to wide neural networks 
operating in the NTK regime.  %% MOve this before end 

% In \secref{sec:exp}, we experimentally demonstrate our 


\paragraph{Extensions to early stopped GD} 
% \todos{Can move this whole to Appendix?}
Often in practice, gradient descent is stopped early. 
% instead of running uptill convergence.
% 
We now provide theoretical evidence 
that our guarantees 
may continue to hold 
for an early stopped GD iterate. Concretely, we show that in expectation,
the outputs of the GD iterates are close 
to that of a problem with data-independent regularization (as considered in~Theorem~\ref{thm:error_regularized_ERM}).
% 
% Before presenting the result
First,
we introduce some notation.  
By $\calL_{S}(w)$, we denote 
the objective in \eqref{eq:l2_MSE} with $\lambda=0$. 
Consider the GD iterates defined in \eqref{eq:GD_iterates}. 
Let $\wt w_{\lambda} = \argmin_{w} \calL_\calS (w;\lambda)$. 
% ) \,,$ for all $t=1,2,\ldots\,.$ 
% 
Define $f_t(x) \defeq f(x;w_t)$ as the solution at the $t^{\text{th}}$ iterate
and $\wt f_\lambda(x) \defeq f(x;\wt w_\lambda)$ as the regularized solution. 
Let $\kappa$ be the condition number of the population covariance matrix 
and let $s_\text{min}$ be the minimum positive singular value 
of the empirical covariance matrix. 
% 
% Next, 

\begin{prop}[informal] \label{prop:early_stop}
    % Assume the underlying distribution over $\calX$ as $N(0,\sigma^2 I)$. 
% Let $S$ be a training data sampled iid from $\calD$. 
% 
For $\lambda = \frac{1}{t\eta}$, we have 
\begin{align*}
    \Expt{x \sim \calD_\calX}{(f_t(x) - \wt f_\lambda(x))^2} &\le c(t,\eta) \cdot \Expt{x \sim \calD_\calX}{f_t(x)^2} \,, %\label{eq:early_stop}
\end{align*}
where $c(t, \eta) \approx \kappa \cdot \min( 0.25, \frac{1}{s_\text{min}^2 t^2 \eta^2})$.
% $s_\text{min}$ denotes the minimum positive singular value 
% of the empirical covariance respectively,
% and $\kappa$ denotes the condition number
% of the true covariance. 
An equivalent guarantee holds for a point $x$ 
sampled from the training data. 
\end{prop} 
% \todos{Siva, I will revisit this nicely after reading Tsybakov’s condition. 
% Do you think we should define Tsybakov’s condition in the main paper.}
% with the regularization constant dependent on . 

% We discuss this in detail in \appref{app:proof_gd}. 

% \textbf{Remark \quad} 


The proposition above states 
that for large enough $t$, 
GD iterates stay close
to a regularized solution 
with data-independent 
regularization constant. 
Together with our guarantees in \thmref{thm:linear} 
for regularization solution with $\lambda = \frac{1}{t\eta}$, 
\propref{prop:early_stop} shows that our guarantees with RATT 
may hold on early stopped GD. 
% We include 
See the formal result 
% and a detailed discussion 
in \appref{app:formal_early_stop}. 


\textbf{Remark {} {}} 
\propref{prop:early_stop} only bounds the expected squared difference between the $t^{\text{th}}$ gradient descent iterate and a corresponding regularized solution. 
% to obtain the bound on the classification error and this can be done using 
The expected squared difference
and the expected difference
of classification errors
(what we wish to bound)
are not related, in general.
However, they can be related 
under standard low-noise 
(margin) assumptions.
For instance, under the Tsybakov noise condition~\citep{tsybakov1997nonparametric, yao2007early}, we can lower-bound 
the expression on the LHS 
of \propref{prop:early_stop} 
with the difference 
of expected classification error. 

\paragraph{Extensions to deep learning} 
Note that the main lemma underlying
our bound on (clean) population error  
states that when training on a
mixture of clean and randomly labeled data, 
we obtain a classifier whose empirical error 
on the mislabeled training data
is lower than its population error 
on the distribution of mislabeled data.  
% While we prove this in \lemref{lem:fit_mislabeled}
% for ERM on 0-1 loss, for linear models,
% and for models in the NTK regime,
% we must \emph{assume it} 
% to extend our bound to deep models. 
We prove this for ERM on 0-1 loss (\lemref{lem:fit_mislabeled}). 
For linear models 
(and networks in NTK regime), 
we obtained this result
by assuming hypothesis stability 
and relating training error at a datum
with the leave-one-out error (\thmref{thm:linear}). 
However, to extend our bound to deep models 
we must assume that training 
on the mixture of random and clean data 
leads to overfitting on the random mixture. Formally:  

\begin{assumption} \label{asmp:deep_models}
Let $\wh f$ be a model obtained 
by training with an algorithm $\calA$ 
on a mixture of clean data $S$ 
and randomly labeled data $\wt S$. 
Then with probability $1-\delta$ 
over the random draws 
of mislabeled data $\wt S_M$, 
we assume that the following condition holds:
\begin{align*}
    \error_{\wt \calS_M} (\wh f) \le \error_{\calDm} (\wh f) + c\sqrt{\frac{\log(1/\delta)}{2m}}  \,,
\end{align*}
for a fixed constant $c > 0$.
\end{assumption}

Under \asmpref{asmp:deep_models}, 
our results in \thmref{thm:error_ERM},~\ref{thm:error_regularized_ERM} and~\ref{thm:multiclass_ERM} 
extend beyond ERM with the 0-1 loss 
to general learning algorithms. 
We include the formal result in \appref{appsubsec:ext_DL}. 
% This assumption is directly assuming hypothesis  
% 
% This assumption 
% in \thmref{thm:error_ERM} 
% By way of justification,
% We now justify as
Note that given the ability of neural networks 
to interpolate the data, 
this assumption seems uncontroversial 
in the later stages of training. 
Moreover, concerning the early phases of training,
recent research has shown that 
learning dynamics for complex deep networks 
resemble those for linear models 
\citep{nakkiran2019sgd, hu2020surprising},
much like the wide neural networks 
that we do analyze.
Together, these arguments help 
to justify \asmpref{asmp:deep_models} and hence, the applicability
of our bound in deep learning.
Motivated by our analysis on linear models trained with gradient descent, 
we discuss conditions in \appref{appsubsec:justifying_assumption1} which imply \asmpref{asmp:deep_models} for constant values $\delta > 0$.
% 
% 
In the next section, we empirically demonstrate applicability of our bounds for deep models. 

% In the next section, we provide empirical evidence supporting our claims. 





% % \subsection{Bounds for linear models}
% In the previous section, 
% we obtained our results with ERM on 0-1 loss. 
% While minimizing 0-1 error is hard 
% due to the underlying non-convexity, 
% the results provide critical theoretical insights. 
% In this section, motivated by these findings, 
% % extending those results,
% we show parallel results with linear models 
% trained with Gradient Descent (GD).  


% % squared error over a given training set $S$.
% % % , i.e. 
% % % % \begin{align}
% % % $\wh w  \defeq \argmin_{w\in R^d} \, \Expt{(x,y) \sim \calS}{L(x,y;w)} \,$,  
% % % % \end{align}
% % % where $L(x,y;w) = (w^Tx - y)^2$. 
% % More generally,

% % But first,
% To begin, we introduce the setup and some additional notation.
% % the setup and notation we will be using throughout this section.
% For simplicity, we consider binary classification with $\calX = \Real^d$. Define a linear function $f(x; w) \defeq w^T{x}$ for some $w \in R^d$ and $x\in \calX$. Given training set $S \defeq \{ (x_i, y_i)\}_{i=1}^n$, we suppose that the parameters of the linear function are obtained by doing gradient descent to minimize the following $L_2$ regularized problem: 
% % \begin{align}
% %     \calL_S(w) \defeq \Expt{(x,y) \sim \calS}{(w^Tx - y)^2} + \lambda \norm{w}{2} \,, \label{eq:l2_MSE}   
% % \end{align}
% % \begin{align}
% %     \calL_S(w) \defeq \sum_{(x_i,y_i) \sim \calS}{(w^Tx_i - y_i)^2} + \lambda \norm{w}{2} \,, \label{eq:l2_MSE}   
% % \end{align}
% \begin{align}
%     % n in denominator is avoided deliberately
%     \calL_S(w; \lambda) \defeq \sum_{i=1}^n{(w^Tx_i - y_i)^2} + \lambda \norm{w}{2}^2 \,, \label{eq:l2_MSE}   
% \end{align}
% where $\lambda\ge0$ is a regularization constant. 
% %  i.e., $\widehat w \defeq \argmin_{w\in R^d} \, \calL_S(w)$.  
% In this section, we will only focus on binary classification problem. Results on multiclass classification extend as in \secref{subsec:multiclass}.  
% % 
% % \wh w   \argmin_{w\in R^d}
% % 
% % 
% % justfiy the use of squared loss by citing belkin's recent papers nad NTK work.
% % 
% Our choice of the mean squared error minimization for linear networks is motivated by a line of recent work on Neural Tangent Kernet (NTK) theory~\citep{jacot2018neural,arora2019fine,du2019gradient,hu2019simple}. Moreover, in practice, it has been shown that squared-error minimization works better or at par with cross-entropy loss for classification tasks~\citep{muthukumar2020classification,hui2020evaluation}. 
% \todos{Not sure if people put this, check recent papers}
% % 
% % Now we introduce the main result of this section. 
% %

% For a given training set $S$, we define perturbed training set $S^\prime_{(i)}$ with $i^\text{th}$ point replaced by a new sample $(x,y) \sim \calD$. With $S_{(i)}$, we denote the training set $S$ with $i^{\text{th}}$ point removed.
% We now introduce two stability conditions\footnote{ For simplicity of exposition, we present interpretable but stronger conditions here. We present the exact weaker stability conditions in \appref{app:proof_gd}.}:
% %  that we assume gradient descent training samples from $\calD$. 
% % Define $S^\prime$ as the
% \begin{condition}[Train Error Stability]
% % [Bounded difference condition]
% \label{cond:train_stability} 
%     % With a random data set $S$, 
%     We have $\alpha$ train error stability if our training algorithm $\calA$ satisfies the following bounded difference condition, almost surely: 
%     % \begin{align*}
%         $\abs{\error_{\calS}\left(f({\calA, S})\right) - \error_{\calS^\prime_{(i)}}(f({\calA, S^\prime_{(i)}}))} \le \alpha/\abs{S}\,$.
%         % $\sum_{i=1}^n\left(\error_{\calS}\left(f({\calA, S})\right) - \error_{\calS^\prime_i}(f({\calA, S^\prime_{(i)}}))\right)^2 \le \alpha\,$.
%         % $\sum_{i=1}^n\left(\error_{\calS}\left(f({\calA, S})\right) - \error_{\calS^\prime_i}\left(f({\calA, S^\prime_{(i)}})\right)\right)^2 \le \beta\,$.
%     % \end{align*}
% \end{condition}

% This is a standard bounded difference condition~\citep{boucheron2013concentration} applied to the training error. We use this condition to show an exponential concentration of the training error to the expected training error. 
% Intuitively, \codref{cond:train_stability} is saying that by perturbing one training point we don't change the empirical error much. In particular, this condition is satisfied for $\alpha=1$ if by perturbing one point, we only end up changing prediction at one data point in the training set. Note that, we needed a similar condition even in \lemref{lem:clear_error}, but we got it for free since we were doing ERM training with the 0-1 loss. 
% %  We derived this condition 
% % that We needed the same condition   
% % , the  notion of stability 


% % present stability assumption and finish the proof. Justify the stability assumptions  


% %% TODO: can weaken the condition to a high probability bound. 
% \begin{condition}[Error Stability] 
%     \label{cond:error_stability}
%     We have $\beta$ error stability if our training algorithm $\calA$ satisfies, almost surely: 
%     % \begin{align*}
%     % ${\sum_{i=1}^n \frac{\error_{\calD}( f(\calA, S_{(i)}))}{n} - \error_\calD(f(\calA, S))} \le \beta\,$.
%     $\error_{\calD}( f(\calA, S_{(i)})) - \error_\calD(f(\calA, S)) \le \beta\,$.
%     % \end{align*}

% \end{condition}

% % Define Leave-One-Out (LOO) error as $$\error_{\LOO} = \sum_{i=1}^n \error_{\calD}(\widehat f_{(i)})/n$$
% This condition is  similar to a weak notion of stability called \emph{error stability}~\citep{bousquet2002stability,kearns1999algorithmic,elisseeff2003leave}. 
% Intuitively, \codref{cond:error_stability} states that the average population error of classifiers, each obtained by leaving one point out in the training set, can't be much larger than the population error of the original classifier.
% % \todos{Fix this?}
% Note that both of these conditions are mild and do not imply generalization in any way. We discuss the implications in more detail in \appref{app:proof_gd}.

% Now we present the main result of this section. As before, we assume access to clean dataset $S = \{(x_i, y_i)\}_{i=1}^n \sim \calD^n$ and randomly labeled dataset $\wt S = \{(x_i, y_i)\}_{i=n+1}^{n+m} \sim \wt \calD^m$. Let $\bX = [x_1, x_2, \cdots, x_{m+n}]$ and $\by = [y_1, y_2, \cdots, y_{m+n}]$. 

% \begin{theorem} \label{thm:linear}
%     Fix a positive learning rate $\eta \le \frac{1}{\norm{\bX^T\bX}{\text{op}} + \lambda^2}$ and an initialization $w_0 = 0$. 
%     % \todos{Assumption made for simplicty}. 
%     Consider the following gradient descent iterates to minimize objective \eqref{eq:l2_MSE} on $S \cup \wt S$:
%     % \begin{align}
%     $w_t = w_{t-1} - \eta \grad_w \calL_{S \cup \wt S} (w_{t-1}; \lambda) \,,$ for all $t=1,2,\ldots\,.$ %\quad \forall t=1,2,\ldots
%     % \end{align} 
%     Then $\{ w_t\}$ converge to a limiting solution $\wh w = \left( \bX^T\bX+\lambda \boldsymbol{I}\right)^{-1}\bX^T\by$. Define $\widehat f (x) \defeq f(x ; \wh w) $.  
%     % 
%     Moreover, assume that this gradient descent algorithm satisfies \codref{cond:train_stability} with $\alpha=1$ and \codref{cond:error_stability}
%     with $\beta=\calO(\frac{1}{n+m})$.  
%     Then for any $\delta >0$, with probability $1-\delta$ over the random draws of datasets $\wt S$ and $S$, we have:
%     % \begin{align}
%         $\error_\calD(\widehat f) - \error_\calS(\widehat f) \le 1 - 2 \error_{\wt\calS}(\widehat f) + c\sqrt{\frac{\log(1/\delta)}{m}} + \calO\left(1/(n+m)\right) \,,$ %\label{eq:gd_error}
%     % \end{align} 
%     for some constant $c=\cdot$.
% \end{theorem}

% % Recall, . 
% With two mild regularity conditions, we establish the same bound on gradient descent training with squared-error loss, notably the same dominating term on the population error, as in \thmref{thm:error_ERM}. 
% % (modulo an order $1/n$ term). 
% % We discuss the full proof in \appref{app:proof_gd} and provide a proof sketch below. 
% % 
% Since, squared error minimization doesn't imply 0-1 error minimization, we can't use arguments from the previous section.  Rather, we use the obtained closed-form solution to compare 0-1 error at a train point with an unseen point.   The main idea is to show that the expected train error is less than the population error (parallel to \lemref{lem:fit_mislabeled}. 
% % e provide a proof sketch below. 
% % We just highlight the key differences from the previous section. 

% % Before giving a proof sketch for this result, 
% % we interpret the result and discuss a few implications.

% \begin{hproof}
%     % We can use arguments from the previous section to obtain parallel inequalities on squared error. But 
%     % Because 
%     % We can't simply use our 0-1 loss style arguments. Rather, 
%     For a mislabeled (and an unknown) training point $(x_i, y_i)$ in $\wt S$, we show 
%     \begin{align}
%         % \sum_{i=n+1}^m P_{x_i \sim \calD_\calX}(y_i x_i^T \widehat w \le 0 ) \le \sum_{i=n+1}^m P_{x_i \sim \calD_\calX}(y_i x_i^T \widehat w_{(i)} \le 0 )
%         P_{(x_i ,y_i)\sim \calDm}(y_i x_i^T \wh w \le 0 ) \le P_{(x_i, y_i) \sim \calDm}(y_i x_i^T \wh w_{(i)} \le 0 ) \,, \label{eq:loo_error}
%     \end{align}
%     where $\wh w_{(i)}$ is the classifier obtain by leaving out the $i^\text{th}$ point from the training set. 
%     Intuitively, this condition is saying that the expected train error at a training point is less than the population error obtained by removing that point and re-training the classifier. 
%     We then use \codref{cond:train_stability} to show an exponential convergence to expected train error (average of LHS in \eqref{eq:loo_error} for all $i$) and then use \codref{cond:error_stability} to relate the expected leave-one-out error (average of RHS in \eqref{eq:loo_error} for all $i$) with population error on the original classifier $\wh w$.
% \end{hproof}

% \textbf{Comparison with Rademacher bound.}
% Our bound in \thmref{thm:linear} (and in the previous section) illustrate that we can obtain an upper bound on the clean population error of a classifier by estimating the fit on the fraction of randomly labeled data added during training. Next, we show that the dominating term in our bound is a upper bounded by a standard distribution dependent complexity measure called the \emph{Rademacher complexity}. 

% \begin{prop}
%     Fix a randomly labeled dataset $\wt S \sim \wt \calD^n$. Then for any classifier $f \in \calF$  (possibly dependent on $\wt S$)\footnote{We restrict $\calF$ to functions outputting a label in $\calY = \{-1,1\}$.} and for any $\delta >0$, with probability $1-\delta$ over random draws of $\wt S$,  we have 
%     \begin{align*}
%         1 - 2\error_{\wt S}(f) \le \Expt{\epsilon,x}{\sup_{f\in \calF} \left( \frac{\sum_i \epsilon_i f(x_i)}{n} \right) } + \sqrt{\frac{\log(\frac{2}{\delta})}{n}}
%     \end{align*} 
%     % Proof fo
%     where $\epsilon$ is drawn from a uniform distribution over $\{-1, 1\}^n$ and $x$ is drawn from $\calD_\calX^n$.      
% \end{prop}

% In other words, the proposition above highlights that the fit on the randomly labeled data added during training is always smaller than the Rademacher 
% complexity of $\calF$ w.r.t. the underlying distribution over $\calX$, implying that our bound is always tighter than a Rademacher complexity based bound. The proof follows simply by application of the bounded difference condition and McDiarmid’s inequality. 


% \textbf{Extensions to kernel regression.} Since the result in \thmref{thm:linear} is independent of the underlying distribution over $\calX\times \calY$, our guarantees simply extend to kernel regression by using the transformation $x\to \phi(x)$ for some feature transform function $\phi$. Furthermore, recent literature has pointed out a concrete connection between neural networks and kernel regression with so-called \emph{Neural Tangent Kernel} (NTK) which holds in a certain lazy-regime where weights don't change much during training ~\citep{jacot2018neural,du2019gradient,du2018gradient,chizat2019lazy}. 
% Unlike linear models, note that in the NTK regime eventhough the model is linear in parameters, it is still  non-linear in the inputs.  
% % Our work contributes to this line of work, in particular
% % 
% Using this concrete correspondence, our bounds on the clean population error (in \thmref{thm:linear}) extend to wide neural networks operating in the NTK regime. 

% % In \secref{sec:exp}, we experimentally demonstrate our 


% \textbf{Extensions to early stopped GD.} \todos{Can move this whole to Appendix?}
% Often in practice, gradient descent is early stopped. 
% % instead of running uptill convergence.
% % 
% We now provide theoretical evidence supporting that our guarantees may continue to hold on the early stopped GD iterate. With $\calL_{S}(w)$, we denote the objective in \eqref{eq:l2_MSE} with $\lambda=0$. Consider the GD iterates given by $w_t = w_{t-1} - \eta \grad_w \calL_{S} (w_{t-1})$. Let $\wt w_{\lambda} = \argmin_{w} \calL_\calS (w;\lambda)$. 
% % ) \,,$ for all $t=1,2,\ldots\,.$ 
% % 
% Next, we show that in expectation the outputs of the GD iterates are close to that of a data-independent regularized problem. 


% \begin{prop}[informal] 
%     % Assume the underlying distribution over $\calX$ as $N(0,\sigma^2 I)$. 
% Let $S$ be a training data sampled iid from $\calD$. 
% Define $f_t(x) \defeq f(x;w_t)$ and $\wt f_\lambda(x) \defeq f(x;\wt w_\lambda)$.  Then for $\lambda = \frac{1}{t\eta}$, we have 
% \begin{align*}
%     \Expt{x \sim \calD_\calX}{(f_t(x) - \wt f_\lambda(x))^2} &\le c(t,\eta) \Expt{x \sim \calD_\calX}{f_t(x)^2} \,,
% \end{align*}
% where $c(t, \eta) \approx \kappa \min( 0.1, \frac{1}{s_\text{min} t\eta}) $. $s_\text{min}$ denotes the minimum positive singular value of the empirical covariance respectively and $\kappa$ denotes the condition number of the true covariance. 
% An equivalent guarantee holds for a point $x$ sampled from the training data $S$. 
% \end{prop} \todos{Siva, I will revisit this nicely after reading Tsybakov’s condition. Do you think we should definte Tsybakov’s condition in the main paper.}
% % with the regularization constant dependent on . 

% \textbf{Remark \quad } Noise condition

% \textbf{Remark \quad} Together with regularization  we have thwe result. 

% The proposition above states that for large enough $t$, GD iterates stay close to a regularized solution with data-independent regularization constant, hinting that our guarantees in \thmref{thm:linear} may hold on early stopped GD. We include the formal result and a detailed discussion in \appref{app:proof_gd}. 

% % In the next section, we provide empirical evidence supporting our claims. 


