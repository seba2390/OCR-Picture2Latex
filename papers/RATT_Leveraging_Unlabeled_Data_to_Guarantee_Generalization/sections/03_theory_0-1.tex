We now present our generalization bound 
and proof sketches 
for ERM on the 0-1 loss
(full proofs in \appref{app:proof_erm}).
For any dataset $T$, 
ERM returns the classifier $\widehat f$
that minimizes the empirical error:
% as follows
% that minimizes the error 
\begin{align}
    \widehat f \defeq \argmin_{f\in \calF} \error_\calT(f) \,. \label{eq:erm}
\end{align}
% We can now examine RATT applied
% 
We focus first on
binary classification.  
% with balanced classes.
Assume we have a clean dataset 
$S \sim \calD^n$ of $n$ points
and a randomly labeled dataset 
$\wt S \sim \wt \calD^m$ of $m~(<n)$ points with
% Because the classes are balanced, 
labels in $\wt S$ are assigned uniformly at random.
We show that with 0-1 loss minimization 
on the union of $S$ and $\wt S$,
we obtain a classifier 
whose error on $\calD$
% on data distribution
is upper bounded 
by a function of the empirical errors 
on clean data $\error_\calS$
(lower is better)
and on randomly labeled data 
$\error_{\wt \calS}$ 
(higher is better): 
% by the \emph{fit} on the random data 
% plus the error on the clean training data. 
% Formally,  
\begin{theorem} \label{thm:error_ERM}
    % Let $S \sim \calD^n $ and $\wt S \sim \wt D^m$. 
    For any classifier $\wh f$ obtained by ERM \eqref{eq:erm} on dataset $S \cup \wt S$,~ 
    %Assume we perform ERM as in \eqref{eq:erm} on $S \cup \wt S$ 
    %and obtain a classifier $\wh f$. 
    %Then 
    for any $\delta > 0$, 
    with probability at least $1-\delta$, 
    % over the random draws of datasets $\wt S$ and $S$, 
    % over the samples $S$ and $\wt S$
    we have  
    \begin{align*}
        \error_\calD(\widehat f)  \le \, & \error_\calS(\widehat f) + 1 - 2 \error_{\wt\calS}(\widehat f) \\
        & + \left(\sqrt{2} \error_{\wt S}(\widehat f)   + 2 + \frac{m}{2n}\right) \sqrt{\frac{\log(4/\delta)}{m}} \,. \label{eq:thm1} \numberthis
    \end{align*}
    %for some constant $c \le 3.2$.
\end{theorem}
% 
% 
% RHS is sum of the error on clean portion 
% and an additional term which we refer as \emph{fit} on the random data.
%% TODO SG: Fix the "dominating term" 
% Besides the empirical error on the clean portion, 
% the dominating term 
% Note that the RHS 
% % \eqref{eq:thm1} 
% % is
% depends on
% $(1 - 2 \error_{\wt\calS}(\widehat f))$ or, 
% equivalently, $2(1/2 - \calE_{\wt S} (\widehat f))$.
% which can be re-written as $2(1/2 - \calE_{\wt S} (\widehat f))$.
% ($1/2 - \calE_{\wt S} (\widehat f)$)
% If $\wt S$ were not included in the training, then $\calE_{\wt S}(f)$ would be approximately 1/2 for any $f$.
In short, this theorem tells us that 
if after training on both clean and randomly labeled data,
we achieve low error on the clean data
but high error (close to $1/2$) 
on the randomly labeled data,
then low population error is guaranteed.
Note that because the labels in ${\wt S}$
are assigned randomly,
the error $\error_{\wt \calS}(f)$
for any fixed predictor $f$
(not dependent on ${\wt S}$)
will be approximately 1/2. 
Thus, if ERM produces a classifier 
that has not fit to the randomly labeled data,
then $(1 - 2 \error_{\wt \calS} (\widehat f))$ 
will be approximately $0$,
and our error will be determined
by the fit to clean data.
The final term %in the bound
accounts for finite sample error---notably,
% and notably, it
it (i) does not depend 
on the complexity 
of the hypothesis class;
and (ii) approaches $0$
at a $\calO(1/\sqrt{m})$ rate
(for $m < n$).


% is just the accuracy of classifier $\widehat f$ 
% assessed on the randomly labeled data $\wt S$.  

Our proof strategy 
unfolds in three steps.
First, in \lemref{lem:fit_mislabeled}
we bound $\error_{\calD} (\wh f)$
in terms of the error %$\error_{\wt S_M} (\wh f)$ 
on the mislabeled subset of $\wt S$.
Next, in Lemmas \ref{lem:mislabeled_error} and \ref{lem:clear_error},
we show that the error on the mislabeled subset
can be accurately estimated 
using only clean and randomly labeled data. 


To begin, assume that we actually knew 
the original labels for the randomly labeled data.
By $\wt S_C$ and  $\wt S_M$,
we denote the clean and mislabeled portions
of the randomly labeled data, respectively
(with $\wt S = \wt S_M \cup \wt S_C$).
% Recall, we refer to the distribution 
% of mislabeled points as $\calDm$.
Note that for binary classification,
a lower bound on %the error 
mislabeled population error $\error_{\calDm}  (\wh f)$
directly upper bounds 
the error on the original population $\error_{\calD}  (\wh f)$.
Thus we only need to prove that 
the empirical error on the mislabeled portion of our data 
is lower than the error on unseen mislabeled data,
i.e., $\error_{\wt S_M} (\wh f) \leq \error_{\calDm}  (\wh f) = 1 - \error_{\wt \calS_M}(\widehat f)$ (upto $\calO(1/\sqrt{m})$).
% ,
% i.e., that overfitting happens (on $\wt S_M$).

% To explain our proof strategy for \thmref{thm:error_ERM}, 
% we now introduce a key lemma. 
% To begin, assume that we actually knew 
% the true labels for the randomly labeled data,
% some of which are correct while others are mislabeled.
% By $\wt S_C$ and  $\wt S_M$,
% we denote the clean and mislabeled portions
% of the randomly labeled data, respectively,
% (with $\wt S = \wt S_M \cup \wt S_C$).
% We refer to the distribution 
% of mislabeled points as $\calDm$.
% Note that for binary classification,
% a lower bound on the error on mislabeled population $\calDm$
% directly yields an upper bound on the original population $\calD$.
% Thus we need only to prove that 
% the empirical error on the mislabeled portion of our data 
% is lower than the population error on mislabeled data,
% i.e., that overfitting happens (on $\wt S_M$).


% To explain our proof strategy for \thmref{thm:error_ERM}, 
% we now introduce a key lemma. 
% To begin, assume that we actually knew 
% the true labels for the randomly labeled data,
% some of which are correct while others are mislabeled.
% By $\wt S_C$ and  $\wt S_M$,
% we denote the clean and mislabeled portions
% of the randomly labeled data, respectively,
% (with $\wt S = \wt S_M \cup \wt S_C$).
% We refer to the distribution 
% of mislabeled points as $\calDm$.
% Note that for binary classification,
% a lower bound on the error on mislabeled population $\calDm$
% directly yields an upper bound on the original population $\calD$.
% Thus we need only to prove that 
% the empirical error on the mislabeled portion of our data 
% is lower than the population error on mislabeled data,
% i.e., that overfitting happens (on $\wt S_M$).


% \todos{I can change the lemma to a simple overfitting result?}
\begin{lemma} \label{lem:fit_mislabeled}
    Assume the same setup as in \thmref{thm:error_ERM}. 
    Then for any $\delta >0$, with probability at least  $1-\delta$ 
    over the random draws of mislabeled data $\wt S_M$, we have 
    \begin{align}
        \error_\calD(\widehat f)  \le 1 -\error_{\wt \calS_M}(\widehat f) + \sqrt{\frac{\log(1/\delta)}{m}}\,. \label{eq:lemma1}
    \end{align}   
\end{lemma} 

% We now provide a proof sketch for 
% the lemma. 
% Lemma \ref{lem:fit_mislabeled}.
\begin{hproof} 
    % The main idea is to define a fixed classifier $f^*$ 
    % which is optimal over random draws of mislabeled points. 
    % The classifier $f^*$ allows relating 
    % classification error of $\widehat f$ 
    % on mislabeled training data $\wt S_M$
    % and the corresponding mislabeled 
    % data distribution $\calDm$. 
    % The main idea of the proof is to establish a comparison between the errors in \eqref{eq:lemma1} by defining a classifier $f^*$ that allows us to relate these errors by using ERM optimality condition of $\wh f$.    
    % 
    The main idea of our proof is to regard 
    the clean portion of the data 
    ($S \cup \wt S_C$) as fixed.
    Then, there exists a classifier $f^*$ 
    that is optimal over draws 
    of the mislabeled data $\wt S_M$.
    Formally, 
    \begin{align*}
        f^* \defeq \argmin_{f \in \calF} \error_{\widecheck {\calD}} (f),
    \end{align*}
    where $\widecheck \calD$ is a combination of 
    the \emph{empirical distribution} 
    over correctly labeled data $S \cup \wt S_C$
    % in $S\cup \wt S$ 
    and the (population) distribution 
    over mislabeled data $\calDm$. 
    % 
    % 
    Recall that $\widehat f \defeq \argmin_{f \in \calF} \error_{\calS \cup \wt \calS } (f)$. 
    Since, $\widehat f$ minimizes 0-1 error 
    on $S \cup \wt S$, 
    we have $\error_{\calS \cup \wt \calS}(\widehat f) \le \error_{
    \calS \cup \wt \calS}(f^*)$. 
    Moreover, since $f^*$ is independent of $\wt S_M$, 
    % \footnote{For a fully rigorous argument,
    % refer to the complete proof in App.~\ref{app:proof_erm}.} 
    we have with probability at least $1-\delta$ that
    \begin{align*}
      \error_{\wt \calS_M}(f^*) \le \error_{ \calDm}(f^*) +  \sqrt{\frac{\log(1/\delta)}{m}} \,. 
    \end{align*}
    %$ 
    %for some constant $c_1\le 1/2$. 
    Finally, since $f^*$ is the optimal classifier on $\widecheck \calD$, 
    we have $\error_{\widecheck \calD}(f^*) \le \error_{\widecheck \calD}(\widehat f)$.     
    Combining the above steps and using the fact 
    that $\error_\calD = 1- \error_{\calDm} $, 
    we obtain the desired result.  
\end{hproof}
% 
% Intuitively, \lemref{lem:fit_mislabeled}, we are saying that ERM with 0-1 loss leads to overfitting on the training data. While LHS in \eqref{eq:lemma1} depends on the unknown portion $\wt S_M$, in practice, we can not identify the mislabeled points from randomly labeled data. However, if we can approximate the performance on the clean portion of randomly labeled data (i.e., $\error_{\wt\calS_C}(\widehat f)$), then we can use it to estimate $\error_{\wt\calS_M}(\widehat f)$. 
% 
% 
% Intuitively, \lemref{lem:fit_mislabeled} states 
% that ERM with 0-1 loss leads 
% to overfitting on the training data. 
% 
While the LHS in \eqref{eq:lemma1} depends 
on the unknown portion $\wt S_M$, 
our goal is to use unlabeled data 
(with randomly assigned labels)
for which the mislabeled portion 
cannot be readily identified. 
% in practice,
% we can not 
% identify the mislabeled points from randomly labeled data.
% However, 
% exploiting the exchangeability of the clean data $S$
% and the clean portion of the random data $S_C$ (\lemref{lem:mislabeled_error})
Fortunately, we do not need to identify
the mislabeled points to estimate
% because we can estimate the label marginal,
% we can estimate 
the error on these points in aggregate
$\error_{\wt\calS_M}(\widehat f)$.
% in terms of the errors on the clean 
% and randomly labeled portions of the data. 
% by estimating of the error on clean and rnd
% by approximating the performance 
% on the clean portion of the randomly labeled data 
% (i.e., $\error_{\wt\calS_C}(\widehat f)$),
% we can estimate. 
% 
% 
% by an error estimate on the clean training data. 
Note that because the label marginal is uniform, 
approximately half of the 
% data points 
data
will be correctly labeled 
and the remaining half will be mislabeled. 
% ~\footnote{In principle, we account for errors with finite sampling, but ease of exposition we make this assumption. In \appref{app:proof_erm}, we take this error into account.}. 
Consequently, we can utilize the value 
of $\error_{\wt\calS}(\widehat f)$ 
and an estimate of $\error_{\wt\calS_C}(\widehat f)$ 
to lower bound $\error_{\wt\calS_M}(\widehat f)$. 
We formalize this as follows: 

\begin{lemma} \label{lem:mislabeled_error}
    Assume the same setup as \thmref{thm:error_ERM}. Then for any $\delta >0$, with probability at least $1-\delta$ over the random draws of $\wt S$, we have  
    % \begin{align}
        $\abs{2\error_{\wt \calS}(\widehat f) - \error_{\wt \calS_C}(\widehat f) -  \error_{\wt \calS_M}(\widehat f) } \le  2\error_{\wt \calS}(\widehat f)  \sqrt{\frac{\log(4/\delta)}{2m}}\,. $ % \label{eq:lemma2}
    % \end{align}   
    %  for some constant $c_3 \le 1.0\,$.
\end{lemma} 


% Can we use Ergo here or is it latin?
% $2\error_{\wt\calS}(\widehat f) \approx \error_{\wt\calS_M}(\widehat f) + \error_{\wt\calS_C}(\widehat f)$. Hence,
%  knowing $\error_{\wt\calS}(\widehat f)$ , if we otherwise  


To complete the argument, 
% we now present a lemma that 
% bounds the gap 
% exploits the exchangeability of the clean data $S$
% and the clean portion of the random data $S_C$
we show that due to the exchangeability of the clean data $S$
and the clean portion of the randomly labeled data $S_C$,
we can estimate the error on the latter $\error_{\wt \calS_C} (\wh f)$
by the error on the former $\error_{\calS} (\wh f)$.
% We formalize this idea in \lemref{lem:mislabeled_error}.
% between the error on clean training data $\error_{S_C}$ 
% and the error on the clean portion of mislabeled data. 
% 
% Even though the classifier is dataset dependent,
% as the dataset is sampled identically, 
% we can use the exchangeability property for this purpose. 
%  to obtain an estimate of $\error_{\wt \calS_M}(\widehat f)$ in terms of $\error_{\calS}(\widehat f)$.
% We concretize this intution in
% \lemref{lem:mislabeled_error} below:
% which establishes a tight bound on the difference of the error of classifier $\widehat f$ on $\wt S_M$ and on $S$:

\begin{lemma} \label{lem:clear_error}
    Assume the same setup as \thmref{thm:error_ERM}. 
    Then for any $\delta >0$, with probability at least $1-\delta$ 
    over the random draws of $\wt S_C$ and $S$, we have 
    % \begin{align}
        $\abs{\error_{\wt \calS_C}(\widehat f) - \error_{\calS}(\widehat f) } \le \left(1 + \frac{m}{2n}\right) \sqrt{\frac{\log(2/\delta)}{m}}\,.$ %\label{eq:lemma3}
    % \end{align}   
    % for some constant $c_2 \le 1.2\,$.
\end{lemma} 

\lemref{lem:clear_error} establishes a tight bound 
on the difference of the error 
of classifier $\widehat f$
on $\wt S_C$ and on $S$. 
The proof uses Hoeffding's inequality for randomly sampled points from a fixed population~\citep{hoeffding1994probability,bardenet2015concentration}.  

% Now, we are ready to discuss the proof strategy for \thmref{thm:error_ERM}. 
Having established these core components,
we can now summarize
the proof strategy for \thmref{thm:error_ERM}. 
We bound the population error on clean data  
(the term on the LHS of \eqref{eq:thm1}) in three steps: 
(i) use \lemref{lem:fit_mislabeled} to upper bound 
the error on clean distribution $\error_\calD(\widehat f)$, 
by the error on mislabeled training data $\error_{\wt \calS_M}(\widehat f)$; 
(ii) approximate $\error_{\wt \calS_M}(\widehat f)$ 
by $\error_{\wt \calS_C}(\widehat f)$ 
and the error on randomly labeled training data 
(i.e.,  $\error_{\wt \calS}(\widehat f)$) using \lemref{lem:mislabeled_error}; 
and (iii) use \lemref{lem:clear_error} 
to estimate $\error_{\wt \calS_C}(\widehat f)$ 
using the error on clean training data 
($\error_{\calS}(\widehat f)$).
% \todos{Not sure if we should use the mathematical notation for ease?}
% 
% The generalization gap of the ERM classifier $\widehat f$ on clean data, i.e. $\error_\calD(\widehat f) - \error_\calS(\widehat f)$, can be 
%   
% Discuss sources of error from approximating ...
% 

% with uniform label marginal. 

\paragraph{Comparison with Rademacher bound}
Our bound in \thmref{thm:error_ERM}  
% illustrates that we can obtain an upper bound
shows that we can upper bound
% on
the clean population error of a classifier 
by estimating 
its accuracy on the clean and 
randomly labeled portions of the training data. 
% of randomly portion added during training. 
Next, we show that our bound's
% the 
dominating term 
% in our bound 
is upper bounded by the \emph{Rademacher complexity} \citep{shalev2014understanding},
a standard distribution-dependent complexity measure.

\begin{prop} \label{prop:rademacher}
    Fix a randomly labeled dataset $\wt S \sim \wt \calD^m$. 
    Then for any classifier $f \in \calF$ 
    (possibly dependent on $\wt S$)\footnote{We 
    restrict $\calF$ to functions which output
    a label in $\calY = \{-1,1\}$.} 
    and for any $\delta >0$, 
    with probability at least $1-\delta$ 
    over random draws of $\wt S$,
    we have 
    \begin{align*}
        1 - 2\error_{\wt S}(f) \le \Expt{\epsilon,x}{\sup_{f\in \calF} \left( \frac{\sum_i \epsilon_i f(x_i)}{m} \right) } + \sqrt{\frac{2\log(\frac{2}{\delta})}{m}},
    \end{align*} 
    % Proof fo
    where $\epsilon$ is drawn 
    from a uniform distribution over $\{-1, 1\}^m$ 
    and $x$ is drawn from $\calD_\calX^m$.      
\end{prop}

In other words, the proposition above highlights that the accuracy 
on the randomly labeled data 
% added during training 
is never larger than the Rademacher complexity of $\calF$ 
w.r.t. the underlying distribution over $\calX$, 
implying that our bound is never looser 
than a bound based on Rademacher complexity.
The proof follows 
% simply 
by application 
of the bounded difference condition 
and McDiarmid’s inequality~\citep{mcdiarmid1989}. 
We now discuss extensions 
of \thmref{thm:error_ERM} 
to regularized ERM
% binary classification with non-uniform label marginal 
and multiclass classification. 


\paragraph{Extension to regularized ERM} 
Consider any function $R: \calF \to \Real$,
e.g., a regularizer 
that penalizes some measure of complexity 
for functions in class $\calF$.
Consider the following regularized ERM:
\begin{align}
    \widehat f \defeq \argmin_{f\in \calF} \error_\calS(f) + \lambda R(f) \,, \label{eq:regularized_erm}
\end{align}
where $\lambda$ is a regularization constant. 
If the regularization coefficient is independent 
of the training data $S \cup \wt S$, 
then our guarantee (\thmref{thm:error_ERM}) holds.
% doesn't change. 
Formally,
%% Can remomve this if need be
\begin{theorem} \label{thm:error_regularized_ERM}
    For any regularization function $R$, 
    assume we perform regularized ERM 
    as in \eqref{eq:regularized_erm} on $S \cup \wt S$ 
    and obtain a classifier $\widehat f$.
    Then, for any $\delta > 0$, 
    with probability at least $1-\delta$, we have
    % over the random draws of datasets $\wt S$ and $S$, we have  
    % \begin{align}
        $\error_\calD(\widehat f)  \le \error_\calS(\widehat f) + 1 - 2 \error_{\wt\calS}(\widehat f) + \left(\sqrt{2} \error_{\wt S}(\widehat f)  + 2 + \frac{m}{2n}\right)  \sqrt{\frac{\log(1/\delta)}{m}} \,.$ % \label{eq:regularized_ERM}
    % \end{align}
    % for some constant $c \le 3.2 $.
\end{theorem}

A key insight here is 
that the proof of \thmref{thm:error_ERM} 
treats the clean data $S$ as fixed
and considers random draws of the mislabeled portion.
Thus a data-independent 
regularization function does not
%% SG: I removed this 
%%%%%
% Performance on the (fixed) $S$
% is just such a function $\calF \to \Real$.
% and the proof goes through 
% for the very reason 
% that our inequality holds
% for any such function.
%%%%%
%
% and thus applies
% point wise for any fixed sample of the clean data.
% (fixed) sample of the clean data.
% Thus, adding an additional $R$ to the loss 
alter our chain of arguments and hence,
% that does not depend on our sample
has no impact on the resulting inequality.
We prove this result formally in \appref{app:proof_erm}.





% % We prove the result in \appref{app:proof_erm}. 
% The key insight is that the result 
% in \lemref{lem:fit_mislabeled} holds %the same 
% %  establishes the main inequality underlying proof of \thmref{thm:error_ERM}. 
% when the regularization coefficient
% is independent of $S \cup \wt S$. 

%% Maybe we can drop this next corollary? 

We note one immediate corollary 
from \thmref{thm:error_regularized_ERM}:
when learning any function $f$ parameterized by $w$ 
with $L_2$-norm penalty on the parameters $w$, 
the population error with $\widehat f$ is determined by the error on the clean training data as long as the error on randomly labeled data is high (close to $1/2$).  


\paragraph{Extension to multiclass classification} %\label{subsec:multiclass}
Thus far, we have addressed binary classification.
% we assumed a binary classification problem. 
% with uniform label marginal. 
% Now, we will relax this assumption.
We now extend these results to the multiclass setting.
% with balanced classes. 
% 
% Assume $p_i$ be the probabiltiy of positive class. 
% Similar to 
As before, we obtain datasets 
$S$ and $\wt S$. 
% Recall the random label assignment procedure. 
% Recall that the 
Here, random labels are assigned
uniformly among all classes.
% procedure assigns any label $y$ 
% to a randomly selected $p_\calD(y)$ 
% fraction of the unlabeled data points. 
% Before presenting the result, we introduce some more notation. For any dataset $S$, let $S^{(y)}$ denote the portion of dataset labeled $y$. 
\begin{theorem} \label{thm:multiclass_ERM}
    For any regularization function $R$, 
    assume we perform regularized ERM 
    as in \eqref{eq:regularized_erm} on $S \cup \wt S$ 
    and obtain a classifier $\widehat f$. 
    % Then for a binary classification problem any $\delta > 0$, with probability $1-\delta$ over the random draws of datasets $\wt S$ and $S$, we have  
    % \begin{align}
        % $\error_\calD(\widehat f) - \error_\calS(\widehat f) \le 1 - (\error_{\wt\calS ^{(1)}}(\widehat f) + \error_{\wt\calS ^{(-1)}}(\widehat f)) + c\sqrt{\frac{\log(1/\delta)}{m}} \,$, %\label{eq:unbalanced_ERM}
    % \end{align}
    % for some constant $c = \cdot$. More generally, 
    For a multiclass classification problem 
    with $k$ classes,
    for any $\delta >0$, with probability at least $1-\delta$,
    % over the random draws of datasets $\wt S$ and $S$, 
    we have
    \vspace{-10pt}
    \begin{align*}
        \error_\calD(\widehat f)  \le \error_\calS(\widehat f)  + (k-1) &\left(1 - \tfrac{k}{k-1} \error_{\wt\calS}(\widehat f)\right) \\
        &+ c\sqrt{\frac{\log(\frac{4}{\delta})}{2m}} \,,\numberthis \label{eq:multiclass_ERM}
    % \vspace{-20pt}
    \end{align*}
    for some constant $c \le (2k+\sqrt{k} + \frac{m}{n\sqrt{k}})$.
\end{theorem}


We first discuss the implications of \thmref{thm:multiclass_ERM}.
Besides empirical error on clean data, the dominating term in
% \eqref{eq:unbalanced_ERM} 
the above expression
is given by $(k-1)\left(1 - \frac{k}{k-1} \error_{\wt\calS}(\widehat f)\right)$.  
% If $\wt S$ were not included in the training set then the error $\error_{\wt\calS ^{(1)}}(f) + \error_{\wt\calS ^{(-1)}}(f)$ would be approximately $1$ for any $f$. 
For any predictor $f$ 
(not dependent on $\wt S$), 
the term $\error_{\wt\calS}(\widehat f)$ 
would be approximately $(k-1)/{k}$ 
and for $\widehat f$, 
the difference now evaluates to the accuracy 
of the randomly labeled data.  
% We have an extra 
% More generally for any multiclass classification problem, the dominating term in the inequality of \thmref{thm:multiclass_ERM} defines the fit. 
Note that for binary classification, 
% we obtain 
% RHS of
\eqref{eq:multiclass_ERM} 
simplifies to 
% the same bound as in
\thmref{thm:error_ERM}. 
% \todos{Shall we discuss this k-1 in the product makes the bound loose and why we expect a tighter bound in general?} 

% Again, the crux of the proof lies 
The core of our proof involves
% in 
obtaining an inequality 
similar to \eqref{eq:lemma1}. 
While for binary classification, 
we could upper bound $\error_{\wt \calS_M}$ 
with $1-\error_\calD$ 
(in the proof of \lemref{lem:fit_mislabeled}), 
for multiclass classification, 
error on the mislabeled data 
and accuracy on the clean data 
in the population 
are not so directly related.  
% related which allowed us replacing
% 
% Hence 
To establish an inequality 
analogous to \eqref{eq:lemma1},
%an inequality 
% similar to \eqref{eq:lemma1},
we break the error on the 
(unknown) mislabeled data 
into two parts: one term corresponds 
to predicting the true label on mislabeled data, 
and the other corresponds to predicting 
neither the true label 
nor the assigned (mis-)label.  
Finally, we relate these errors to their
population counterparts to establish 
an inequality similar to \eqref{eq:lemma1}. 
% we need 
% we divide the multiclass problem 
% modifying the definition of We convert multiclass classification problem 
% into $k$ ``one vs other'' unbalanced binary classification problems and 
% then obtain a similar result to \lemref{lem:fit_mislabeled} for unbalanced binary classification. 
% Combining this result with 
%  by following the steps in proof of \thmref{thm:error_ERM}. 
%%% TODO think on this And ask this
% In \appref{app:proof_erm}, 
% we discuss extensions of our results 
% to non-uniform label marginals. 
% To accommodate a non-uniform marginal, 
% the expression that upper bounds 
% the population error on clean data must be changed slightly.
% is slightly changed.  



% 
% V2 
% 

% We now present our generalization bound 
% % as applied to
% for
% ERM on the 0-1 loss.
% % Formally, 
% Given training data $S$, 
% we obtain $\widehat f$ as follows: 
% \begin{align}
%     \widehat f \defeq \argmin_{f\in \calF} \error_\calS(f) \,, \label{eq:erm}
% \end{align}
% % For notion convinience, we drop the subscript in $\widehatf$
% % For simplicity of exposition,
% % we will begin our discussion by examining 
% To begin, we examine
% % a binary classification problem 
% binary classification 
% with balanced classes
% % a uniform label marginal. 
% and present a proof sketch for our main result
% % but relegate detailed 
% (full proofs in \appref{app:proof_erm}).


% % Now we present our main result of this section. 
% Assume we have a clean dataset $S \sim \calD^n$ of $n$ points. 
% We add a randomly labeled dataset $\wt S \sim \wt D^m$ 
% of $m (<n)$ points.
% % (obtained using the random label assignment procedure). 
% % In this setting, this procedure will assign labels uniformly at random. 
% Because the classes are balanced, 
% labels are assigned uniformly at random.
% We show that with 0-1 loss minimization 
% on the union of $S$ and $\wt S$,
% we obtain a classifier 
% whose population error is upper bounded 
% by the errors on clean data (lower is better)
% and randomly labeled data (higher is better): 
% % by the \emph{fit} on the random data 
% % plus the error on the clean training data. 
% % Formally,  
% \begin{theorem} \label{thm:error_ERM}
%     % Let $S \sim \calD^n $ and $\wt S \sim \wt D^m$. 
%     Assume we perform ERM as in \eqref{eq:erm} on $S \cup \wt S$ 
%     and obtain a classifier $\widehat f$. 
%     Then for any $\delta > 0$, with probability $1-\delta$ 
%     over the random draws of datasets $\wt S$ and $S$, 
%     we have  
%     \begin{align}
%         \error_\calD(\widehat f)  \le  \error_\calS(\widehat f)+ 1 - 2 \error_{\wt\calS}(\widehat f) + c\sqrt{\frac{\log(4/\delta)}{m}} \,, \label{eq:thm1}
%     \end{align}
%     for some constant $c \le 3.2$.
% \end{theorem}

% % RHS is sum of the error on clean portion 
% % and an additional term which we refer as \emph{fit} on the random data.
% %% TODO SG: Fix the "dominating term" 
% Besides empirical error on the clean portion, 
% the dominating term on the RHS of \eqref{eq:thm1} 
% is $(1 - 2 \error_{\wt\calS}(\widehat f))$ or, 
% equivalently, $2(1/2 - \calE_{\wt S} (\widehat f))$.
% % which can be re-written as $2(1/2 - \calE_{\wt S} (\widehat f))$.
% % ($1/2 - \calE_{\wt S} (\widehat f)$)
% % If $\wt S$ were not included in the training, then $\calE_{\wt S}(f)$ would be approximately 1/2 for any $f$. 
% Because the labels in ${\wt S}$
% are assigned randomly,
% the error $\error_{\wt \calS}(f)$
% for any fixed predictor $f$
% (not dependent on ${\wt S}$)
% will be approximately 1/2. 
% Note that $(1/2 - \error_{\wt \calS} (\widehat f))$ 
% is just the accuracy of classifier $\widehat f$ 
% assessed on the randomly labeled data $\wt S$.  
% In short, this theorem tells us 
% that after training on both clean and randomly labeled data,
% if we achieve low error on clean data
% but, despite training,
% see high error (close to $1/2$)
% on the randomly labeled data,
% then low population error is guaranteed.
% % With this theorem, we conclude the following: we can randomly label a small amount of unlabeled data, add it to the training set, and then use the accuracy on the randomly labeled data to understand generalization at the end of ERM. 

% % We present a proof sketch for the above theorem. But first, 
% % To discuss the

% % To explain our proof strategy for \thmref{thm:error_ERM}, 
% % we now introduce a key lemma. 
% % % we first we introduce 
% % % three intermediate results.
% % % we introduce main lemma underlying 
% % % the proof of \thmref{thm:error_ERM}. 
% % To begin, assume that we actually knew 
% % the true labels for the randomly labeled data,
% % some of which are correct while others are mislabeled.
% % By $\wt S_C$ and  $\wt S_M$,
% % we denote the clean and mislabeled portions
% % of the randomly labeled data, respectively,
% % with ($\wt S = \wt S_M \cup \wt S_C$).
% % We refer to the distribution 
% % of mislabeled points as $\calDm$.
% % % Note, this is just for theoretical analysis, 
% % % we don't need to identify the clean 
% % % and mislabeled fractions 
% % % to compute these quantities.} 
% % % \footnote{With 
% % % the mislabeled portion, 
% % % we refer to the data points 
% % % sampled from $\calDm$ 
% % % and by clean portion, 
% % % we refer to the points 
% % % sampled from $\calD$. 
% % % Note, this is just for theoretical analysis, 
% % % we don't need to identify the clean 
% % % and mislabeled fractions 
% % % to compute these quantities.} 
% % % with ($\wt S = \wt S_M \cup \wt S_C$).
% % % Also $\wt S = \wt S_M \cup \wt S_C$.}. 
% % % \todos{Discuss this nicely}
% % % First, we show that the ERM with 0-1 loss leads to overfitting on the mislabeled portion.
% % Note that for binary classification,
% % a lower bound on the error on mislabeled population $\calDm$
% % directly yields an upper bound on the original population $\calD$.
% % Thus we need only to prove that 
% % the empirical error on the mislabeled portion of our data 
% % is lower than the population error on mislabeled data,
% % i.e., that overfitting happens (on $S_M$).
% % % This is the main lemma underlying 
% % % the proof of \thmref{thm:error_ERM}. 


% To explain our proof strategy for \thmref{thm:error_ERM}, 
% we now introduce a key lemma. 
% To begin, assume that we actually knew 
% the true labels for the randomly labeled data,
% some of which are correct while others are mislabeled.
% By $\wt S_C$ and  $\wt S_M$,
% we denote the clean and mislabeled portions
% of the randomly labeled data, respectively,
% (with $\wt S = \wt S_M \cup \wt S_C$).
% We refer to the distribution 
% of mislabeled points as $\calDm$.
% Note that for binary classification,
% a lower bound on the error on mislabeled population $\calDm$
% directly yields an upper bound on the original population $\calD$.
% Thus we need only to prove that 
% the empirical error on the mislabeled portion of our data 
% is lower than the population error on mislabeled data,
% i.e., that overfitting happens (on $S_M$).


% % \todos{I can change the lemma to a simple overfitting result?}
% \begin{lemma} \label{lem:fit_mislabeled}
%     Assume the same setup as \thmref{thm:error_ERM}. 
%     Then for any $\delta >0$, with probability $1-\delta$ 
%     over the random draws of mislabeled data $\wt S_M$, we have 
%     \begin{align}
%         \error_\calD(\widehat f)  \le 1 -\error_{\wt \calS_M}(\widehat f) + c_1 \sqrt{\frac{\log(1/\delta)}{m}}\,, \label{eq:lemma1}
%     \end{align}   
%     for some constant $c_1 \le 1\,$.
% \end{lemma} 

% % We now provide a proof sketch for 
% % the lemma. 
% % Lemma \ref{lem:fit_mislabeled}.
% \begin{hproof} 
%     % The main idea is to define a fixed classifier $f^*$ 
%     % which is optimal over random draws of mislabeled points. 
%     % The classifier $f^*$ allows relating 
%     % classification error of $\widehat f$ 
%     % on mislabeled training data $\wt S_M$
%     % and the corresponding mislabeled 
%     % data distribution $\calDm$. 
%     The main idea of the proof is to establish a comparison between the errors in \eqref{eq:lemma1} by defining a classifier $f^*$ that allows us to relate these errors by using ERM optimality condition of $\wh f$.    
    
%     Fixing the clean data ($S \cup S_C$),
%     there exists a classifier $f^*$
%     that is optimal over draws 
%     of the mislabeled portion $S_M$.
%     Formally, $f^* \defeq \argmin_{f \in \calF} \error_{\widehat{\calD}} (f)$,
%     where $\widehat \calD$ is a combination of 
%     the \emph{empirical distribution} over correctly labeled data $S \cup S_C$
%     % in $S\cup \wt S$ 
%     and the (population) distribution over mislabeled data $\calDm$. 
%     % 
%     % 
%     Recall that $\widehat f \defeq \argmin_{f \in \calF} \error_\calS (f)$. 
%     Since, $\widehat f$ minimizes 0-1 error 
%     on $\widehat  S = S \cup \wt S$, 
%     we have $\error_{\widehat  \calS}(\widehat f) \le \error_{\widehat \calS}(f^*)$. 
%     Moreover, since $f^*$ is independent of $\wt S_M$, 
%     % \footnote{For a fully rigorous argument,
%     % refer to the complete proof in App.~\ref{app:proof_erm}.} 
%     we have with probability $1-\delta$ that
%     $\error_{\wt \calS_M}(f^*) \le \error_{ \calDm}(f^*) + c_1 \sqrt{\frac{\log(2/\delta)}{m}} \,,$ 
%     for some constant $c_1 \le 1/2$. 
%     Finally, since $f^*$ is the optimal classifier on $\widehat \calD$, 
%     we have $\error_{\widehat \calD}(f^*) \le \error_{\widehat \calD}(\widehat f)$.     
%     Combining the above steps and using the fact 
%     that $\error_\calD = 1- \error_\calDm $, 
%     we obtain the desired result.  
% \end{hproof}

% % Intuitively, \lemref{lem:fit_mislabeled}, we are saying that ERM with 0-1 loss leads to overfitting on the training data. While LHS in \eqref{eq:lemma1} depends on the unknown portion $\wt S_M$, in practice, we can not identify the mislabeled points from randomly labeled data. However, if we can approximate the performance on the clean portion of randomly labeled data (i.e., $\error_{\wt\calS_C}(\widehat f)$), then we can use it to estimate $\error_{\wt\calS_M}(\widehat f)$. 
% % 
% % 
% % Intuitively, \lemref{lem:fit_mislabeled} states 
% % that ERM with 0-1 loss leads 
% % to overfitting on the training data. 

% While LHS in \eqref{eq:lemma1} depends 
% on the unknown portion $\wt S_M$, 
% our goal is to use unlabeled data 
% (with randomly assigned labels)
% for which the mislabeled portion 
% cannot be readily identified. 
% % in practice,
% % we can not 
% % identify the mislabeled points from randomly labeled data.
% % However, 
% % exploiting the exchangeability of the clean data $S$
% % and the clean portion of the random data $S_C$ (\lemref{lem:mislabeled_error})
% Fortunately, we do not need to identify
% the mislabeled points to estimate
% % because we can estimate the label marginal,
% % we can estimate 
% the error on these points in aggregate
% $\error_{\wt\calS_M}(\widehat f)$.
% % in terms of the errors on the clean 
% % and randomly labeled portions of the data. 
% % by estimating of the error on clean and rnd
% % by approximating the performance 
% % on the clean portion of the randomly labeled data 
% % (i.e., $\error_{\wt\calS_C}(\widehat f)$),
% % we can estimate. 
% % 
% % 
% % by an error estimate on the clean training data. 
% Note that because the label marginal is uniform, 
% approximately half of the datapoints 
% will be correctly labeled 
% and the remaining half will be mislabeled. 
% % ~\footnote{In principle, we account for errors with finite sampling, but ease of exposition we make this assumption. In \appref{app:proof_erm}, we take this error into account.}. 
% Consequently, we can utilize the value 
% of $\error_{\wt\calS}(\widehat f)$ 
% and an estimate of $\error_{\wt\calS_C}(\widehat f)$ 
% to lower bound $\error_{\wt\calS_M}(\widehat f)$. 
% We formalize this as follows: 

% \begin{lemma} \label{lem:mislabeled_error}
%     Assume the same setup as \thmref{thm:error_ERM}. Then for any $\delta >0$, with probability $1-\delta$ over the random draws of $\wt S$, we have  
%     % \begin{align}
%         $\abs{2\error_{\wt \calS}(\widehat f) - \error_{\wt \calS_C}(\widehat f) -  \error_{\wt \calS_M}(\widehat f) } \le c_3 \sqrt{\frac{\log(2/\delta)}{m}}\,, $ % \label{eq:lemma2}
%     % \end{align}   
%      for some constant $c_3 \le 1.0\,$.
% \end{lemma} 


% % Can we use Ergo here or is it latin?
% % $2\error_{\wt\calS}(\widehat f) \approx \error_{\wt\calS_M}(\widehat f) + \error_{\wt\calS_C}(\widehat f)$. Hence,
% %  knowing $\error_{\wt\calS}(\widehat f)$ , if we otherwise  


% To complete the argument, 
% % we now present a lemma that 
% % bounds the gap 
% % exploits the exchangeability of the clean data $S$
% % and the clean portion of the random data $S_C$
% we show that due to the exchangeability of the clean data $S$
% and the clean portion of the randomly labeled data $S_C$,
% we can estimate the error on the latter $\error_{\wt \calS_C}$
% by the error on the former $\error_{\calS}$.
% % We formalize this idea in \lemref{lem:mislabeled_error}.
% % between the error on clean training data $\error_{S_C}$ 
% % and the error on the clean portion of mislabeled data. 
% % 
% % Even though the classifier is dataset dependent,
% % as the dataset is sampled identically, 
% % we can use the exchangeability property for this purpose. 
% %  to obtain an estimate of $\error_{\wt \calS_M}(\widehat f)$ in terms of $\error_{\calS}(\widehat f)$.
% % We concretize this intution in
% % \lemref{lem:mislabeled_error} below:
% % which establishes a tight bound on the difference of the error of classifier $\widehat f$ on $\wt S_M$ and on $S$:

% \begin{lemma} \label{lem:clear_error}
%     Assume the same setup as \thmref{thm:error_ERM}. 
%     Then for any $\delta >0$, with probability $1-\delta$ 
%     over the random draws of $\wt S_C$ and $S$, we have 
%     % \begin{align}
%         $\abs{\error_{\wt \calS_C}(\widehat f) - \error_{\calS}(\widehat f) } \le c_2 \sqrt{\frac{\log(1/\delta)}{m}}\,$, %\label{eq:lemma3}
%     % \end{align}   
%     for some constant $c_2 \le 1.2\,$.
% \end{lemma} 

% \lemref{lem:clear_error} establishes a tight bound 
% on the difference of the error 
% of classifier $\widehat f$
% on $\wt S_C$ and on $S$. 
% The proof uses Hoeffding's inequality for randomly sampled points from a fixed population~\citep{hoeffding1994probability,bardenet2015concentration}.  

% % Now, we are ready to discuss the proof strategy for \thmref{thm:error_ERM}. 
% Having established these core components,
% we can now assemble the proof strategy for \thmref{thm:error_ERM}. 
% We bound the population error on clean data  
% (the term on the LHS of \eqref{eq:thm1}) in three steps: 
% (i) use \lemref{lem:fit_mislabeled} to upper bound 
% the error on clean distribution $\error_\calD(\widehat f)$, 
% by the error on mislabeled training data $\error_{\wt \calS_M}(\widehat f)$; 
% (ii) approximate $\error_{\wt \calS_M}(\widehat f)$ 
% by $\error_{\wt \calS_C}(\widehat f)$ 
% and the error on randomly labeled training data 
% (i.e.,  $\error_{\wt \calS}(\widehat f)$) using \lemref{lem:mislabeled_error}; 
% and (iii) use \lemref{lem:clear_error} 
% to estimate $\error_{\wt \calS_C}(\widehat f)$ 
% using the error on clean training data 
% ($\error_{\calS}(\widehat f)$).
% % \todos{Not sure if we should use the mathematical notation for ease?}
% % 
% % The generalization gap of the ERM classifier $\widehat f$ on clean data, i.e. $\error_\calD(\widehat f) - \error_\calS(\widehat f)$, can be 
% %   
% % Discuss sources of error from approximating ...
% % 
% We now discuss extensions of \thmref{thm:error_ERM} to regularized ERM, 
% % binary classification with non-uniform label marginal 
% and multiclass classification with non-uniform label marginal. 

% \textbf{Comparison with Rademacher bound {} {}}
% Our bound in \thmref{thm:error_ERM}  
% % illustrates that we can obtain an upper bound
% shows that we can upper bound
% % on
% the clean population error of a classifier 
% by estimating 
% the
% its accuracy on the clean and 
% randomly labeled portions of the training data. 
% % of randomly portion added during training. 
% Next, we show that our bound's
% % the 
% dominating term 
% % in our bound 
% is upper bounded by the \emph{Rademacher complexity} \citep{shalev2014understanding},
% a standard distribution dependent complexity measure.

% \begin{prop}
%     Fix a randomly labeled dataset $\wt S \sim \wt \calD^n$. 
%     Then for any classifier $f \in \calF$ 
%     (possibly dependent on $\wt S$)\footnote{We 
%     restrict $\calF$ to functions outputting 
%     a label in $\calY = \{-1,1\}$.} 
%     and for any $\delta >0$, 
%     with probability $1-\delta$ 
%     over random draws of $\wt S$,
%     we have 
%     \begin{align*}
%         1 - 2\error_{\wt S}(f) \le \Expt{\epsilon,x}{\sup_{f\in \calF} \left( \frac{\sum_i \epsilon_i f(x_i)}{n} \right) } + \sqrt{\frac{2\log(\frac{2}{\delta})}{n}},
%     \end{align*} 
%     % Proof fo
%     where $\epsilon$ is drawn 
%     from a uniform distribution over $\{-1, 1\}^n$ 
%     and $x$ is drawn from $\calD_\calX^n$.      
% \end{prop}

% In other words, the proposition above highlights that the accuracy 
% on the randomly labeled data 
% % added during training 
% is never larger than the Rademacher complexity of $\calF$ 
% w.r.t. the underlying distribution over $\calX$, 
% implying that our bound is never looser 
% than a Rademacher complexity based bound. 
% The proof follows 
% % simply 
% by application 
% of the bounded difference condition 
% and McDiarmid’s inequality~\citep{mcdiarmid1989}. 


% \textbf{Extension to regularized ERM {} {}} 
% Consider any function $R: \calF \to \Real$,
% e.g., a regularizer 
% that penalizes some measure of complexity 
% for functions in class $\calF$.
% Consider the following regularized ERM:
% \begin{align}
%     \widehat f \defeq \argmin_{f\in \calF} \error_\calS(f) + \lambda R(f) \,, \label{eq:regularized_erm}
% \end{align}
% where $\lambda$ is a regularization constant. 
% If the regularization coefficient is independent 
% of the training data $S \cup \wt S$, 
% then our guarantee (\thmref{thm:error_ERM}) holds.
% % doesn't change. 
% Formally,
% %% Can remomve this if need be
% \begin{theorem} \label{thm:error_regularized_ERM}
%     For any regularization function $R$, 
%     assume we perform regularized ERM 
%     as in \eqref{eq:regularized_erm} on $S \cup \wt S$ 
%     and obtain a classifier $\widehat f$.
%     Then, for any $\delta > 0$, 
%     with probability $1-\delta$ 
%     over the random draws of datasets $\wt S$ and $S$, we have  
%     % \begin{align}
%         $\error_\calD(\widehat f)  \le \error_\calS(\widehat f) + 1 - 2 \error_{\wt\calS}(\widehat f) + c\sqrt{\frac{\log(1/\delta)}{m}} \,,$ % \label{eq:regularized_ERM}
%     % \end{align}
%     for some constant $c \le 3.2 $.
% \end{theorem}

% A key insight here is 
% that the proof of \thmref{thm:error_ERM} 
% treats the clean data $S$ as fixed
% and considers random draws of the mislabeled portion.
% Thus a data independent regularization function doesn't 
% %% SG: I removed this 
% %%%%%
% % Performance on the (fixed) $S$
% % is just such a function $\calF \to \Real$.
% % and the proof goes through 
% % for the very reason 
% % that our inequality holds
% % for any such function.
% %%%%%
% %
% % and thus applies
% % point wise for any fixed sample of the clean data.
% % (fixed) sample of the clean data.
% % Thus, adding an additional $R$ to the loss 
% alter our chain of arguments and hence,
% % that does not depend on our sample
% has no impact on the resulting inequality.
% We prove this result formally in \appref{app:proof_erm}.





% % % We prove the result in \appref{app:proof_erm}. 
% % The key insight is that the result 
% % in \lemref{lem:fit_mislabeled} holds %the same 
% % %  establishes the main inequality underlying proof of \thmref{thm:error_ERM}. 
% % when the regularization coefficient
% % is independent of $S \cup \wt S$. 

% %% Maybe we can drop this next corollary? 

% We note one immediate corollary 
% from \thmref{thm:error_regularized_ERM}:
% when learning any function $f$ parameterized by $w$ 
% with $L_2$-norm penalty on the parameters $w$, 
% the generalization gap with $\widehat f$ 
% is bounded by the fit on the randomly labeled data. 


% \subsection{Extension to multiclass classification} \label{subsec:multiclass}
% Thus far, we assumed a binary classification problem. 
% % with uniform label marginal. 
% Now, we will relax this assumption. 
% % 
% % Assume $p_i$ be the probabiltiy of positive class. 
% Similar to before, we obtain datasets $S$ and $\wt S$. 
% Recall the random label assignment procedure. 
% This procedure assigns any label $y$ 
% to a randomly selected $p_\calD(y)$ 
% fraction of the unlabeled data points. 
% % Before presenting the result, we introduce some more notation. For any dataset $S$, let $S^{(y)}$ denote the portion of dataset labeled $y$. 
% \begin{theorem} \label{thm:multiclass_ERM}
%     For any regularization function $R$, 
%     assume we perform regularized ERM 
%     as in \eqref{eq:regularized_erm} on $S \cup \wt S$ 
%     and obtain a classifier $\widehat f$. 
%     % Then for a binary classification problem any $\delta > 0$, with probability $1-\delta$ over the random draws of datasets $\wt S$ and $S$, we have  
%     % \begin{align}
%         % $\error_\calD(\widehat f) - \error_\calS(\widehat f) \le 1 - (\error_{\wt\calS ^{(1)}}(\widehat f) + \error_{\wt\calS ^{(-1)}}(\widehat f)) + c\sqrt{\frac{\log(1/\delta)}{m}} \,$, %\label{eq:unbalanced_ERM}
%     % \end{align}
%     % for some constant $c = \cdot$. More generally, 
%     For a multiclass classification problem 
%     with $k$ classes and uniform label marginal,
%     for any $\delta >0$, with probability $1-\delta$ 
%     over the random draws of datasets $\wt S$ and $S$, 
%     we have
%     \vspace{-10pt}
%     \begin{align*}
%         \error_\calD(\widehat f)  \le \error_\calS(\widehat f) + (k-1) \left(1 - \tfrac{k}{k-1} \error_{\wt\calS}(\widehat f)\right) + c\sqrt{\frac{\log(\frac{4}{\delta})}{2m}} \,,\numberthis \label{eq:multiclass_ERM}
%     % \vspace{-20pt}
%     \end{align*}
%     for some constant $c \le (2k+\sqrt{k}-1)$.
% \end{theorem}


% We first discuss the implications of \thmref{thm:multiclass_ERM}.
% Besides empirical error on clean data, the dominating term in
% % \eqref{eq:unbalanced_ERM} 
% the above expression
% is given by $(k-1)\left(1 - \frac{k}{k-1} \error_{\wt\calS}(\widehat f)\right)$.  
% % If $\wt S$ were not included in the training set then the error $\error_{\wt\calS ^{(1)}}(f) + \error_{\wt\calS ^{(-1)}}(f)$ would be approximately $1$ for any $f$. 
% For any predictor $f$ 
% (not dependent on $\wt S$), 
% the term $\error_{\wt\calS}(\widehat f)$ 
% would be approximately $(k-1)/{k}$ 
% and for $\widehat f$, 
% the difference now evaluates to the accuracy 
% of the randomly labeled data.  
% % We have an extra 
% % More generally for any multiclass classification problem, the dominating term in the inequality of \thmref{thm:multiclass_ERM} defines the fit. 
% Note that for binary classification, 
% we obtain the same bound as in \thmref{thm:error_ERM}. 
% % \todos{Shall we discuss this k-1 in the product makes the bound loose and why we expect a tighter bound in general?} 

% Again, the crux of the proof lies 
% in obtaining an inequality 
% similar to \eqref{eq:lemma1}. 
% While for binary classification, 
% we could upper bound $\error_{\wt \calS_M}$ 
% with $1-\error_\calD$ 
% (in the proof of \lemref{lem:fit_mislabeled}), for multiclass classification problems, 
% error on the mislabeled data 
% and accuracy on the clean data 
% in the population 
% are not so directly related.  
% % related which allowed us replacing
% % 
% Hence to establish an inequality 
% analogous to \eqref{eq:lemma1},
% %an inequality 
% % similar to \eqref{eq:lemma1},
% we break the error on the 
% (unknown) mislabeled data 
% into two parts: one term corresponds 
% to predicting the true label on mislabeled data, 
% and the other corresponds to predicting 
% neither the true label 
% nor the assigned (mis-)label.  
% Finally, we relate these errors to their
% population counterparts to establish 
% an inequality similar to \eqref{eq:lemma1}. 
% % we need 
% % we divide the multiclass problem 
% % modifying the definition of We convert multiclass classification problem 
% % into $k$ ``one vs other'' unbalanced binary classification problems and 
% % then obtain a similar result to \lemref{lem:fit_mislabeled} for unbalanced binary classification. 
% % Combining this result with 
% %  by following the steps in proof of \thmref{thm:error_ERM}. 
% %%% TODO think on this And ask this
% In \appref{app:proof_erm}, 
% we discuss extensions of our results 
% to non-uniform label marginals. 
% % To accommodate a non-uniform marginal, 
% % the expression that upper bounds 
% % the population error on clean data must be changed slightly.
% % is slightly changed.  




% 
%  CACHED
% 

% % In this section, 
% % we will 
% We now present our generalization bound 
% as applied to ERM on the 0-1 loss.
% % by considering the training algorithm $\calA$ that does ERM with the 0-1 loss. 
% Formally, for a given training data $S$, 
% we obtain $\widehat f$ as follows: 
% \begin{align}
%     \widehat f \defeq \argmin_{f\in \calF} \error_\calS(f) \,, \label{eq:erm}
% \end{align}
% % For notion convinience, we drop the subscript in $\widehatf$
% For simplicity of exposition, we will begin our discussion by examining a binary classification problem with uniform label marginal. We present a proof sketch for our main result but relegate detailed proofs to \appref{app:proof_erm} 


% Now we present our main result of this section. Assume we have a clean dataset $S \sim \calD^n$ of $n$ points. We add a randomly labeled dataset $\wt S \sim \wt D^m$ of $m (<n)$ points (obtained using the random label assignment procedure). In this setting, this procedure will assign labels uniformly at random. We show that with 0-1 loss minimization on the union of clean data $S$ and randomly labeled data $\wt S$, we obtain a classifier whose generalization gap is upper bounded by the \emph{fit} on the random data. Formally,  
% \begin{theorem} \label{thm:error_ERM}
%     % Let $S \sim \calD^n $ and $\wt S \sim \wt D^m$. 
%     Assume we perform ERM as in \eqref{eq:erm} on $S \cup \wt S$ and obtain a classifier $\widehat f$. Then for any $\delta > 0$, with probability $1-\delta$ over the random draws of datasets $\wt S$ and $S$, we have  
%     \begin{align}
%         \error_\calD(\widehat f)  \le 1 + \error_\calS(\widehat f) - 2 \error_{\wt\calS}(\widehat f) + c\sqrt{\frac{\log(1/\delta)}{m}} \,, \label{eq:thm1}
%     \end{align}
%     for some constant $c = \cdot$.
% \end{theorem}

% The dominating term on the RHS of \eqref{eq:thm1} is $(1 - 2 \error_{\wt\calS}(\widehat f))$ which can be re-written as $2(1/2 - \calE_{\wt S} (\widehat f))$.
% % If $\wt S$ were not included in the training, then $\calE_{\wt S}(f)$ would be approximately 1/2 for any $f$. 
% For any predictor $f$ (not dependent on $\wt S$), $\calE_{\wt S}(f)$ would be approximately 1/2. 
% Therefore, we interpret $(1/2 - \calE_{\wt S} (\widehat f))$ as the fit of classifier $\widehat f$ on randomly labeled data $\wt S$.  With this theorem, we conclude the following: we can randomly label a small amount of unlabeled data, add it to the training set, and then use the fit on the randomly labeled data to understand generalization at the end of ERM. 

% % We present a proof sketch for the above theorem. But first, 
% To discuss the proof strategy of \thmref{thm:error_ERM}, we introduce three intermediate results. Let $\wt S_M$ and $\wt S_C$ denote the mislabeled and clean portion of the randomly labeled data respectively\footnote{With the mislabeled portion, we refer to the data points sampled from $\calDm$ and by clean portion, we refer to the points sampled from $\calD$.  Note, this is just for theoretical analysis, we don't need to identify the clean and mislabeled fractions to compute these quantities.  Also $\wt S = \wt S_M \cup \wt S_C$.}. 
% \todos{Discuss this nicely}
% First, we show that the ERM with 0-1 loss leads to overfitting. This is the main lemma underlying the proof of \thmref{thm:error_ERM}. 

% % \todos{I can change the lemma to a simple overfitting result?}
% \begin{lemma} \label{lem:fit_mislabeled}
%     Assume the same setup as \thmref{thm:error_ERM}. Then for any $\delta >0$, with probability $1-\delta$ over the random draws of mislabeled data $\wt S_M$, we have 
%     \begin{align}
%         \error_\calD(\widehat f)  \le 1 -\error_{\wt \calS_M}(\widehat f) + c_1 \sqrt{\frac{\log(2/\delta)}{m}}\,, \label{eq:lemma1}
%     \end{align}   
%     for some constant $c_1 \le 1/2\,$.
% \end{lemma} 
% We now provide a proof sketch for the lemma. The main idea is to define a fixed classifier $f^*$ which is optimal over random draws of mislabeled points. 
% % define this caregully
% The classifier $f^*$ allows relating classification error of $\widehat f$ on mislabeled training data $\wt S_M$ and the corresponding mislabeled data distribution $\calDm$. 
% \begin{hproof} 
%     We know that $\widehat f \defeq \argmin_{f \in \calF} \error_\calS (f)$. Define $f^* \defeq \argmin_{f \in \calF} \error_{\widehat{\calD}} (f)$ where $\widehat \calD$ is a combination of empirical distribution over correctly labeled data in $S\cup \wt S$ and mislabeled data distribution $\calDm$. Since, $\widehat f$ minimizes 0-1 error on $\widehat  S = S \cup \wt S$, we have $\error_{\widehat  \calS}(\widehat f) \le \error_{\widehat \calS}(f^*)$. Moreover, since $f^*$ is independent\footnote{For a careful argument, refer to the complete proof in App.~\ref{app:proof_erm}.} of $\wt S_M$, we have with probability $1-\delta$, $\error_{\widehat \calS}(f^*) \le \error_{\widehat \calD}(f^*) + c_1 \sqrt{\frac{\log(2/\delta)}{m}} \,,$ for some constant $c_1 \le 1/2$. Finally, since $f^*$ is the optimal classifier on $\widehat \calD$, we have $\error_{\widehat \calD}(f^*) \le \error_{\widehat \calD}(\widehat f)$.     
%     Combining the above steps and using the fact that $\error_\calD = 1- \error_\calDm $, we obtain the desired result.  
% \end{hproof}

% Intuitively, with \lemref{lem:fit_mislabeled}, we are saying that ERM with 0-1 loss leads to overfitting on the training data. While LHS in \eqref{eq:lemma1} depends on the unknown portion $\wt S_M$, in practice, we can not identify the mislabeled points from randomly labeled data. However, if we can approximate the performance on the clean portion of randomly labeled data (i.e., $\error_{\wt\calS_C}(\widehat f)$), then we can use it to estimate $\error_{\wt\calS_M}(\widehat f)$. 
% % 
% % by an error estimate on the clean training data. 
% Since the label marginal is uniform, approximately half of the datapoints will be correctly labeled and the remaining half will be mislabeled. 
% % ~\footnote{In principle, we account for errors with finite sampling, but ease of exposition we make this assumption. In \appref{app:proof_erm}, we take this error into account.}. 
% Consequently, we can utilze the value of $\error_{\wt\calS}(\widehat f)$ and an estimate of $\error_{\wt\calS_C}(\widehat f)$ to lower bound $\error_{\wt\calS_M}(\widehat f)$. We formalize this below: 

% \begin{lemma} \label{lem:mislabeled_error}
%     Assume the same setup as \thmref{thm:error_ERM}. Then for any $\delta >0$, with probability $1-\delta$ over the random draws of $\wt S$, we have  
%     % \begin{align}
%         $\abs{2\error_{\wt \calS}(\widehat f) - \error_{\wt \calS_C}(\widehat f) -  \error_{\wt \calS_M}(\widehat f) } \le c_3 \sqrt{\frac{\log(4/\delta)}{m}}\, $, % \label{eq:lemma2}
%     % \end{align}   
%      for some constant $c_3 \le 0.1\,$.
% \end{lemma} 


% % Can we use Ergo here or is it latin?
% % $2\error_{\wt\calS}(\widehat f) \approx \error_{\wt\calS_M}(\widehat f) + \error_{\wt\calS_C}(\widehat f)$. Hence,
% %  knowing $\error_{\wt\calS}(\widehat f)$ , if we otherwise  


% Next, we present a lemma that bounds the gap between the error on clean training data and the error on clean portion of mislabeled data. 
% % 
% Even though the classifier is dataset dependent, as the dataset is sampled identically, we can use the exchangeability property for this purpose. 
% %  to obtain an estimate of $\error_{\wt \calS_M}(\widehat f)$ in terms of $\error_{\calS}(\widehat f)$.
% We concretize this intution in \lemref{lem:mislabeled_error} below:
% % which establishes a tight bound on the difference of the error of classifier $\widehat f$ on $\wt S_M$ and on $S$:

% \begin{lemma} \label{lem:clear_error}
%     Assume the same setup as \thmref{thm:error_ERM}. Then for any $\delta >0$, with probability $1-\delta$ over the random draws of $\wt S_C$ and $S$, we have 
%     % \begin{align}
%         $\abs{\error_{\wt \calS_C}(\widehat f) - \error_{\calS}(\widehat f) } \le c_2 \sqrt{\frac{\log(4/\delta)}{m}}\,$, %\label{eq:lemma3}
%     % \end{align}   
%     for some constant $c_2 \le 2\,$.
% \end{lemma} 

% \lemref{lem:mislabeled_error} establishes a tight bound on the difference of the error of classifier $\widehat f$ on $\wt S_C$ and on $S$. 
% The proof uses a simple bounded difference condition and McDiarmid’s inequality~\citep{mcdiarmid1989}.  

% Now, we are ready to discuss the proof strategy for \thmref{thm:error_ERM}. We bound the generalization gap (the term on the LHS of \eqref{eq:thm1}) in three steps: (i) use \lemref{lem:fit_mislabeled} to upper bound the error on clean distribution $\error_\calD(\widehat f)$, by the error on mislabeled training data $\error_{\wt \calS_M}(\widehat f)$; (ii) approximate $\error_{\wt \calS_M}(\widehat f)$ by $\error_{\wt \calS_C}(\widehat f)$ and the error on randomly labeled training data (i.e.,  $\error_{\wt \calS}(\widehat f)$) using \lemref{lem:mislabeled_error}; and (iii) use \lemref{lem:clear_error} to estimate $\error_{\wt \calS_C}(\widehat f)$ using the error on clean training data ($\error_{\calS}(\widehat f)$).
% % \todos{Not sure if we should use the mathematical notation for ease?}
% % 
% % The generalization gap of the ERM classifier $\widehat f$ on clean data, i.e. $\error_\calD(\widehat f) - \error_\calS(\widehat f)$, can be 
% %   
% % Discuss sources of error from approximating ...
% % 
% We now discuss extensions of \thmref{thm:error_ERM} to regularized ERM, 
% % binary classification with non-uniform label marginal 
% and multiclass classification with non-uniform label marginal. 

% \textbf{Extension to regularized ERM.} 
% Assume a regularization function $R: \calF \to \Real$ that regularizes the complexity of the functions in class $\calF$. Consider the following regularized ERM:
% \begin{align}
%     \widehat f \defeq \argmin_{f\in \calF} \error_\calS(f) + \lambda R(f) \,, \label{eq:regularized_erm}
% \end{align}
% where $\lambda$ is a regularization constant. 
% If the regularization coefficient is independent of the training data $S \cup \wt S$, then the the guarantee in \thmref{thm:error_ERM} doesn't change. Formally,
% %% Can remomve this if need be
% \begin{theorem} \label{thm:error_regularized_ERM}
%     For any regularization function $R$, assume we perform regularized ERM as in \eqref{eq:regularized_erm} on $S \cup \wt S$ and obtain a classifier $\widehat f$. Then for any $\delta > 0$, with probability $1-\delta$ over the random draws of datasets $\wt S$ and $S$, we have  
%     % \begin{align}
%         $\error_\calD(\widehat f) - \error_\calS(\widehat f) \le 1 - 2 \error_{\wt\calS}(\widehat f) + c\sqrt{\frac{\log(1/\delta)}{m}} \,,$ % \label{eq:regularized_ERM}
%     % \end{align}
%     for some constant $c = \cdot$.
% \end{theorem}

% % We prove the result in \appref{app:proof_erm}. 
% The key insight is that the result in \lemref{lem:fit_mislabeled} holds %the same 
% %  establishes the main inequality underlying proof of \thmref{thm:error_ERM}. 
% when the regularization coefficient is independent of $S \cup \wt S$. 

% One immediate corollary from \thmref{thm:error_regularized_ERM} is following: when learning any function $f$ parameterized by $w$ with $L_2$-norm penalty on the parameters $w$, the generalization gap with $\widehat f$ is bounded by the fit on the randomly labeled data. 


% \subsection{Extension to multiclass classification} \label{subsec:multiclass}
% Till now, we assumed a binary classification problem. 
% % with uniform label marginal. 
% Now, we will relax this assumption. 
% % 
% % Assume $p_i$ be the probabiltiy of positive class. 
% Similar to before, we obtain datasets $S$ and $\wt S$. Recall the random label assignment procedure. This procedure assigns any label $y$ to a randomly selected $p_\calD(y)$ fraction of the unlabeled data points. 
% % Before presenting the result, we introduce some more notation. For any dataset $S$, let $S^{(y)}$ denote the portion of dataset labeled $y$. 
% \begin{theorem} \label{thm:multiclass_ERM}
%     For any regularization function $R$, assume we perform regularized ERM as in \eqref{eq:regularized_erm} on $S \cup \wt S$ and obtain a classifier $\widehat f$. 
%     % Then for a binary classification problem any $\delta > 0$, with probability $1-\delta$ over the random draws of datasets $\wt S$ and $S$, we have  
%     % \begin{align}
%         % $\error_\calD(\widehat f) - \error_\calS(\widehat f) \le 1 - (\error_{\wt\calS ^{(1)}}(\widehat f) + \error_{\wt\calS ^{(-1)}}(\widehat f)) + c\sqrt{\frac{\log(1/\delta)}{m}} \,$, %\label{eq:unbalanced_ERM}
%     % \end{align}
%     % for some constant $c = \cdot$. More generally, 
%     For a multiclass classification problem with $k$ classes and uniform label marginal, for any $\delta >0$, with probability $1-\delta$ over the random draws of datasets $\wt S$ and $S$, we have
%     \begin{align*}
%         \error_\calD(\widehat f) - \error_\calS(\widehat f) \le (k-1) \left(1 - \tfrac{k}{k-1} \error_{\wt\calS}(\widehat f)\right) + c\sqrt{\frac{\log(\frac{1}{\delta})}{m}} \,, \numberthis \label{eq:unbalanced_ERM}
%     \end{align*}
%     for some constant $c = \cdot$.
% \end{theorem}


% We first discuss the implications of \thmref{thm:multiclass_ERM}. The dominating term in \eqref{eq:unbalanced_ERM} is given by $(k-1)\left(1 - \frac{k}{k-1} \error_{\wt\calS}(\widehat f)\right)$.  
% % If $\wt S$ were not included in the training set then the error $\error_{\wt\calS ^{(1)}}(f) + \error_{\wt\calS ^{(-1)}}(f)$ would be approximately $1$ for any $f$. 
% For any predictor $f$ (not dependent on $\wt S$), the term $\error_{\wt\calS}(\widehat f)$ would be approximately $(k-1)/{k}$ and for $\widehat f$ the difference now defines the fit on the randomly labeled data.  
% % We have an extra 
% % More generally for any multiclass classification problem, the dominating term in the inequality of \thmref{thm:multiclass_ERM} defines the fit. 
% Note for binary classification, we obtain the same bound as in \thmref{thm:error_ERM}. 
% \todos{Shall we discuss this k-1 in the product makes the bound loose and why we expect a tighter bound in general?} 

% Again, the crux of the proof lies in obtaining an inequality similar to \eqref{eq:lemma1}. While for binary classification, we could upper bound $\error_{\wt \calS_M}$ with $1-\error_\calD$ (in the proof of \lemref{lem:fit_mislabeled}), for multiclass classification problems, error on the mislabeled data and accuracy on the clean data in the population are not so directly related.  
% % related which allowed us replacing
% % 
% Hence to establish an inequality similar to \eqref{eq:lemma1}, we break the error on the (unknown) mislabeled data into two parts: one term corresponds to predicting the true label on mislabeled data, and the other corresponds to neither predicting the true label and nor the assigned mislabel.     
% % we need 
% % we divide the multiclass problem 
% % modifying the definition of We convert multiclass classification problem 
% % into $k$ ``one vs other'' unbalanced binary classification problems and 
% % then obtain a similar result to \lemref{lem:fit_mislabeled} for unbalanced binary classification. 
% % Combining this result with 
% %  by following the steps in proof of \thmref{thm:error_ERM}. 
% In \appref{app:proof_erm}, we discuss extensions of our results to non-uniform label marginal. To accommodate a non-uniform marginal, the expression that upper bounds the generalization gap is slightly changed.  


