\begin{figure*}[ht]
\center
\includegraphics[width=\textwidth]{figures/TaxonomyTable.png}
\caption{A mapping between the ten axes of our general taxonomy and the nine feedback content categories.}
\label{fig:taxonomy}
\end{figure*}

\section{Feedback Content Taxonomy}
\label{sec:taxonomy}

In Section \ref{sec:framework}, we presented an overview of the complex ecosystem of feedback, including an expansion specifically for LLMs (\ie \ours) that connects various background elements (\eg the learner, the task, the error types) to the actual feedback that must be given. In this section, we expand on our analysis of the \textit{content} dimension of feedback in \ours. 
Specifically, we present a taxonomy of feedback content under two different forms: a set of 10 broad axes along which feedback can vary, and a more concrete set of nine emergent categories for feedback topic. 
Figure \ref{fig:felt_framework} presents an overview of the two different presentations of this taxonomy, and the mapping between them.

We motivate this taxonomy to finely categorize current approaches to textual feedback that implicitly formulate feedback solely for \textit{utility} (\ie how useful is the feedback for guiding a model toward a suitable response). However, they do not categorize its content, leaving a conceptual gap about \textit{what} makes feedback useful. 
Our taxonomy stratifies the feedback space, allowing a deliberate and systematic study of feedback content.


\subsection{General Taxonomy}
\label{ssec:general_taxonomy}
We break down feedback content along ten dimensions that influence how feedback is formulated: 
\begin{enumerate}[itemsep=0.05em]
    \item \textit{length}, an indication of how much feedback feedback is given, possibly measured by counting its number of tokens,
    \item \textit{granularity}, a measure of the level of detail with which the feedback addresses the original answer --- it is not a measure of how much of the answer is being considered, but rather of the level of detail with which it is being considered,\footnote{For an open-answer example task, feedback might range from global learning meta-feedback, to global but task-specific, to paragraph-level, to sentence-level, to word-level, to token-level feedback.}
    \item \textit{applicability of instructions}, expressing both whether the feedback contains instructions, as well as how applicable those instructions are for the learner and their current understanding and approach to solving the task,
    \item \textit{answer coverage}, which registers how much of the learner's answer is considered to generate the given feedback. The feedback could be independent of the answer, or only relate to parts of the answer (\eg, focusing on a particular mistake), or the feedback might take the complete answer into consideration,
    \item \textit{criteria}, denoting which criteria the answer is being evaluated on: global evaluation, specific dimensions (e.g., fluency, engagement, etc.), or, alternatively, no  dimensions (the answer is not being evaluated),
    \item \textit{information novelty}, indicating the degree to which learner already had access to the information provided in the feedback, ranging from all information being previously known by the learner, to some information being unknown to the learner, to all information being novel for the learner,
    \item \textit{purpose}, measuring whether the feedback is being given to improve the learner's performance or to clarify the task,\footnote{In a pedagogical setting with human learners, other purposes are possible, such as regulating the student's emotions and motivation, but we do not consider these for the LLMs.}
    \item \textit{style}, capturing the level of the language used to transmit the feedback to the learner, which can range from simple, direct sentences to verbose and terminology-heavy language, 
    \item \textit{valence}, indicating whether the feedback is positive (signaling achievement) or negative (signaling need for improvement),
    \item \textit{mode}, denoting how the feedback is given to the learner, capturing two important aspects: whether the feedback is uni- or multi-modal, \footnote{Multi-modal feedback is naturally more suited for multi-modal tasks. For example, in an instance segmentation task, the correct (visual) answer could be provided alongside textual feedback on mistakes and how to correct them.} and whether it allows the user to submit multiple tries or not.
\end{enumerate}

\noindent Combined, these ten axes capture what we consider the most important qualities of a piece feedback to understand its impact on a given learner model. We hypothesize that each of these dimensions influences the model's revised response to varying degrees, but that all are worthy of individual study.

\subsection{Categorical Taxonomy}
\label{ssec:categorical_taxonomy}
Given these ten axes of feedback, we further define nine emergent, interpretable feedback categories that vary each of these axes (see Figure~\ref{fig:taxonomy}). These nine categories enable a more streamlined classification of prevalent forms of feedback, yielding a starting point to explore which components of feedback may be effective for a given task:

\begin{enumerate}[itemsep=0.05em]
    \item \textit{Global Verification}, a single, aggregate score for the task performance as a whole,
    \item \textit{Response Verification}, response-level classification of the answer (\eg \textit{right} or \textit{wrong)},
    \item \textit{Mistakes Verification}, error-level feedback, possibly with the number of mistakes,
    \item \textit{Correct Answer}, provides the correct answer,\footnote{While understanding the concept of the correct answer in a closed-answer task is trivial, the same cannot be said about an open-answer context. Providing an excellent answer for the task would be nothing more than a worked example and thus a \textit{Procedural Elaboration} type of feedback. Instead, in this context, the \textit{Correct Answer} becomes the rewritten version of the student's answer, improved to be excellent according to the evaluative standard, ideally with as few changes as possible.}
    \item \textit{Response Elaboration}, response-level, response-specific feedback addressing either the positive characteristics of the answer, its shortcomings, or both,
    \item \textit{Mistakes Elaboration}, elaboration feedback about types and sources of errors,
    \item \textit{Task Elaboration}, elaboration feedback about the task, such as its requirements, topic, the relevant concepts or terminology --- this is clarification feedback, and does not provide suggestions for the next steps,
    \item \textit{Procedural Elaboration}, task-specific elaboration feedback about how to solve or improve a solution for said task,
    \item \textit{Metacognition Elaboration}, elaboration feedback about general learning and problem-solving strategies
\end{enumerate}

\noindent Examples of each feedback type, as well as more detailed guidance on the characteristics of each type,  are presented in Appendix \ref{app:feedback_examples}. The nine categories, with the exception of the \textit{correct answer}, are grouped into two main types: verification and elaboration. Following the terminology established by educational works, verification feedback provides classification-only information, such as whether an answer is right or wrong, or whether a mistake was made or not. 
Elaboration feedback provides more meaningful, concrete, and detailed information.

In our formulation, the general taxonomy (\S\ref{ssec:general_taxonomy}) provides a set of 10 broad feedback dimensions that the categorical taxonomy composes into nine distinct feedback types.
These nine feedback types are the ones we believe to be most relevant for providing and classifying feedback, as they are interpretable, but remain consolidated from human learning research. 
%
Finally, one important characteristic of these categories is that they were designed to be as modular as the task allows them to be. 
This modularity and clear delineation of each feedback class enables the study of feedback types individually, but also of different combinations. 

\subsection{Mapping Previous NLP Research}

To demonstrate the applicability of our taxonomy, we classify feedback examples from prior works according to the nine categories outlined above. If a work employed more than one type of feedback in its examples, we mapped the work to appropriate feedback types.
To be best of our knowledge, we are the first to conduct a systematic exploration of textual feedback types to date.


\begin{table}[htb!]
  \begin{tabular}{p{0.14\textwidth}|p{0.28\textwidth}}
    \toprule
    \textbf{Feedback Type}  & \textbf{Works}  \\
    \midrule
    Global \newline Verification & \small \citet{weston_dialog-based_2016}; \citet{shi2022life}; \citet{scheurer2022training}; \citet{welleck2022generating}; \citet{wu2023finegrained}; \citet{shinn2023reflexion}; \citet{chen2023teaching}  \\
    \midrule
    Response \newline Verification & \small \citet{madaan2023selfrefine}; \citet{lightman2023lets} \\
    \midrule
    Mistakes \newline Verification & \small \textcolor{gray}{\citetColored{gray}{tandon-etal-2022-learning}}; \citet{saunders2022selfcritiquing}; \citet{welleck2022generating}; \citet{wu2023finegrained}; \citet{paul2023refiner}; \citet{chen2023teaching} \\
    \midrule
    Correct \newline Answer & \small \citet{weston_dialog-based_2016}; \citet{shi2022life}; \citet{saunders2022selfcritiquing} \\
    \midrule
    Response \newline Elaboration & \small \citet{shi2022life}; \citet{scheurer2022training}; \citet{madaan2023selfrefine} \\
    \midrule
    Mistakes \newline Elaboration & \small \citet{weston_dialog-based_2016}; \textcolor{gray}{\citetColored{gray}{tandon-etal-2022-learning}}; \citet{scheurer2022training}; \citet{saunders2022selfcritiquing}; \citet{shinn2023reflexion} \\
    \midrule
    Task \newline Elaboration & \small \citet{tandon-etal-2022-learning}; \citet{scheurer2022training}; \citet{madaan2023selfrefine}; \citet{chen2023teaching} \\
    \midrule
    Procedural \newline Elaboration & \small \citet{weston_dialog-based_2016};  \citet{tandon-etal-2022-learning};  \citet{shi2022life};  \citet{murty2022fixing};  \citet{saunders2022selfcritiquing};  \citet{welleck2022generating};  \citet{schick2022peer};  \citet{madaan2023selfrefine, shinn2023reflexion} \\
    \midrule
    Metacognition \newline Elaboration & None \\
    \bottomrule
  \end{tabular}
  \caption{The distribution of NLP textual feedback research as per the categorical taxonomy, highlighting that categorization remains an unexplored area of feedback research. \textcolor{gray}{\textit{n.b.}.: Works in grey collect the indicated types of textual feedback, but do not employ it.}}
  \label{tab:nlp_feedback_space}
\end{table}



Table \ref{tab:nlp_feedback_space} presents a cross-referencing of our categorical taxonomy with NLP works employing diverse forms of textual feedback. 
We observe that the examples of feedback in these works are spread across multiple categories, painting a chaotic picture of how feedback is formulated in current NLP research. While these works broadly consider the \textit{utility} of feedback in how it is \textit{applied} to LLMs, less focus has been given to how the \textit{content} of the feedback affects its utility (and no works ground this content to pedagogical foundations). 

Finally, we note that, while we pursue an interpretable categorization to classify these existing works on feedback in NLP, our general taxonomy showcases novel ways to compose feedback, providing an opportunity for further granularity when exploring the space of possible feedback types.



