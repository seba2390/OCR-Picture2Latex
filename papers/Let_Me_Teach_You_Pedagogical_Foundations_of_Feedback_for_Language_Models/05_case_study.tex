\section{Case Study}
\label{sec:case_study}

Having established an LLM-relevant feedback model at three levels of abstraction --- a feedback ecosystem (\S\ref{sec:framework}), feedback content dimensions (\S\ref{ssec:general_taxonomy}), and concrete feedback categories (\S\ref{ssec:categorical_taxonomy}) --- in this section we seek to validate its importance by conducting a simple case study.

Using a corpus of media articles written about research papers,\footnote{Details about the dataset, model hyperparameters, and prompts used are presented in Appendix \ref{app:case_study}.} we task GPT-4 \citep{openai2023gpt4} to summarize 50 articles on research papers in lay terms. Then, we provide 2 different types of feedback from our categorization: 
\textit{Correct Answer} (CA) and \textit{Task Elaboration} (TE), chosen due to their significantly different characteristics. We evaluate both the original and revised answer.\footnote{As this analysis was conducted on a total of 50 samples, producing 150 evaluations (one per feedback type plus the original summary for each data sample). The evaluation of each summary was conducted by one of the authors, according to an established set of criteria.} More details about the experiment setup and execution are available in Appendix \ref{app:case_study}.

\begin{figure}[t]
\centering
\includegraphics[width=1.0\columnwidth]{figures/case_study.png}
\caption{The impact of two different feedback types on a lay summary generation task. Both led to an improvement in average summary quality.}
\label{fig:case_study}
\end{figure}

\paragraph{Results} Figure \ref{fig:case_study} demonstrates that, on average, both types of feedback improve the generated summaries. Interesting, we find that \textit{task elaboration} feedback outperforms \textit{correct answer} feedback, which is remarkable, as feedback and evaluation in NLP are usually based on knowledge of the correct answer. However, in this case, feedback that reiterated or provided more information about desired summary qualities (\ie TE) emerged as the more \textit{useful} of the two feedback types.\footnote{This is not surprising from the learning sciences perspective --- as seen in Section \ref{sec:background_pedagogy}, CA does not possess any of the characteristics of effective feedback.}

Furthermore, the effect of these two feedback types emerges clearly when analyzed qualitatively. For example, TE feedback led to more promotional language --- frequently including words such ``groundbreaking,'' ``breakthrough,'' and ``revolutionize,'' which was encouraged by the task elaboration prompt, ``A good and captivating summary should
first grab the readerâ€™s attention'' (see Appendix~\ref{app:case_study}). Meanwhile, CA feedback often led to more technical language. While this language was likely to be found in some quantity in the \textit{correct answer} (\ie the gold summary), it was amplified by the model's response, leading to lower scores. Output examples are presented in Appendix \ref{app:case_study}.

\paragraph{Feedback Cost} Both CA and TE are answer-independent, (\ie both are constructed independently from the content of the original answer). Yet, both led, on average, to a significant improvement over the initial summary, demonstrating clear advantages to providing models with feedback. 
TE, especially, is a simple type of feedback to generate, needing only to describe the task or relevant concepts with some level of detail. The positive impact of such straightforward additions to the generative pipeline reinforces the motivation for studying the space of feedback content more deeply. 
