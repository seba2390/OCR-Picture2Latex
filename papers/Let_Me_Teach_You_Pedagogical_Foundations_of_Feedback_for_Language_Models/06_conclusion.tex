\section{Conclusion}

In this paper, we survey the most influential feedback models from learning science and adjacent social sciences, and create a novel framework and taxonomy of feedback for NLF. Both the framework and taxonomy are constructed to enable a systematic exploration of feedback content, allowing for objective conclusions on the impact of a piece of feedback and the optimization of NLF.
The case study (\S\ref{sec:case_study}) provides early signals into the merits of our formulation.
By clearly delineating the information present in a piece of feedback, we can better make inferences about a model's knowledge. %For example, if \textit{Task Elaboration} feedback had no effect on a given answer, there are two possible explanations: (1) the model simply chose to ignore the feedback, or (2) the feedback had no impact because the model already understood the task, so no actual usable information was delivered.


Beyond a survey of the space of natural language feedback, we present a clear path forward, through the deliberate exploration of these feedback categories, the inspection of the impact of varying the more general axes, and the study of the more diverse and peripheral aspects of the \ours framework --- such as learner characteristics (different models and different models' parameters), and how they might impact the output. 



\paragraph{Future Work}
Our work opens up several future research directions. One is a more complete validation of this taxonomy, possibly employing carefully LLM-generated feedback to ensure content restrictions are respected. A more interesting question pertains to the study of feedback that RLF methods would consider effective -- when using an RLF framework to optimize for feedback, what characteristics emerge? Finally, another open question is the design of feedback for an unknown task, guided by the FELT framework.