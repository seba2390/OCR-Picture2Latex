\section{Different feedback content types}
\label{app:feedback_examples}

In this appendix, we first present different examples of the nine feedback content categories for the summarization task used in the case study. Second, we present a more detailed description of each feedback type, to facilitate feedback categorization according to the categorical taxonomy.

\subsection{Task Introduction}
In this task, the model is instructed to summarize a news article about a research finding into a summary that can be understood by an adult who is not particularly familiar with the field. As more thoroughly described in Appendix \ref{app:case_study}, the summary should be engaging while also describing the problem, the finding and the venue of publication.

\subsection{Feedback Examples}
\label{app:categorial_examples}

Let us consider the following generated summary: 

\texttt{"Scientists at the University of Bern have used 3D computer simulations to reconstruct how protoplanet Vesta collided with other asteroids around a billion years ago, using data from NASA's 2011 Dawn probe. The simulations also reveal detailed information about Vesta's composition and properties, adding to our understanding of the solar system's evolution. The findings were presented in a study published in Nature magazine."}

Below is a possible example of each type of feedback that could be given in response:

\begin{enumerate}[label=\textbf{\arabic*.}]
\item \textbf{Global Verification:} Overall grade of 85\%.

\item \textbf{Response Verification:} Describes the approach and the motivation, and mentions the venue, but not particularly engaging and incomplete about authorship.

\item \textbf{Mistakes Verification:} No initial attempt at grabbing the reader's attention; inaccurate authorship attribution (incomplete).

\item \textbf{Correct Answer:} The correct answer is: Models boost the significance of image and measurement data from space missions and help to understand our solar system. A simulation of a double impact that occurred on the proto-planet Vesta one billion years ago allowed scientists to describe precisely the inner structure of the asteroid. A joint research from EPFL, Bern University, France and the United States is on the cover of Nature this week.

\item \textbf{Response Elaboration:} The summary is accurate overall, and the language employed is adequate for the target audience. It could, however, use improvement in several areas, such as being more attention grabbing, especially in its first sentence, and providing more detail about what was actually found, rather than generic phrases like "adding to our understanding of the solar system's evolution.". It also failed to capture it was a joint research project.

\item \textbf{Mistakes Elaboration:} The summary does not attempt to engage with the user and capture their attention, in order to make them curious to read it to its completion. Furthermore, it only mentions one of the universities collaborating in this research, in which several labs from different universities joined efforts.

\item \textbf{Task Elaboration:} A good and captivating summary should first grab the reader's attention and make them curious to learn more. It is then important to factually and precisely state what the problem is, why it is important, the proposed solution, and, if published or being divulged, disclose where the reader can find it.

\item \textbf{Procedural Elaboration:} While reading the article, it is important to identify the key aspects of the research -- what problem or research question was being studied, what approach was used to solve it, and what its contributions or applications are. It is also important to register who was behind the findings and where they were published. Finally, it is important to think about how to present this idea in the first sentence -- in a way that is engaging for the reader, getting their attention and making them curious to read the rest. 

\item \textbf{Metacognition Elaboration:} To achieve a given task, it is first necessary to understand it and the concepts involved. With this understanding, it is possible to identify the taskâ€™s goal. Then, one must determine the steps needed to achieve this goal. If needed, each of these steps can be further split into even smaller tasks. Finally, it can be helpful to establish timelines and deadlines for each of these tasks, so the goal is achieved in time.

\end{enumerate}

\subsection{Observations}

\paragraph{Information Overlap}
It is possible to observe some of these feedback types share some information, that is, that there is some overlap between different categories. This is a natural consequence from the task type. Indeed, this summarization task is not a fully open answer question. While there is significant answer space, there are some hard requirements, such as mentioning the research finding, its motivation, the publication venue, being engaging, and using an accessible language. Almost all these are binary requirements, which a summary either fulfills or fails to address. As such, there will naturally be some overlap with regards to them in the feedback information, as it is only possible to fully prevent it in a truly open-answer setting.


\paragraph{Feedback Effectiveness}
In Section \ref{sec:background_pedagogy}, \citet{hattie_power_2007}'s definition of effective feedback was presented. According to the authors, it should address three different information needs: where the learner is going (\textit{feed up}), how they are going (\textit{feed back}), and where to next (\textit{feed forward}). It is possible to relate these questions to the feedback categories exemplified above. 

The \textit{feed up} question, that is, the goal performance, can be implicitly derived from all feedback types that describe flaws of the current answer (by exclusion) or its merits (by inclusion). However, it is the \textit{Correct Answer} feedback category that directly and explicitly presents this information.

The \textit{feed back} question -- the learner's current performance -- is derived from all the verification feedback categories as well as from \textit{Response Elaboration} and \textit{Mistakes Elaboration}.

Finally, the \textit{feed forward} question, how the learner should proceed, is directly tackled by \textit{Procedural Elaboration} feedback.

This definition would then, disregard \textit{Task Elaboration} and \textit{Metacognition Elaboration} feedback categories as inefficient feedback. However, as the case study presented in Section \ref{sec:case_study} demonstrates, at least in the NLP domain, we cannot be so quick to dismiss them -- as in it, \textit{Task Elaboration} actually outperformed \textit{Correct Answer} feedback. Consequently, while the definition of effective feedback proposed by \citet{hattie_power_2007} might help researchers consider promising feedback types, it might also dismiss other pertinent pieces of information. For that reason, both \textit{Task Elaboration} and \textit{Metacognition Elaboration} are present in the categorical taxonomy proposed by this paper, despite their lack of clear mapping to any of \citet{hattie_power_2007}'s three questions.

\subsection{Mapping Feedback to a Categorical Type}
\label{app:categorial_mapping}

While examples of the nine feedback categories might be easy to understand, classifying novel pieces of feedback might prove challenging. Below, we provide a more exhaustive overview of the content of each type of feedback:


\begin{enumerate}[label=\textbf{\arabic*.}]
\item \textbf{Global Verification:} An aggregate score for the task as a whole. Cannot contain more than a single data point of information. The score need not be numeric (\eg ``\texttt{Satisfactory},'' ``\texttt{Grade: 65\%},'' and ``\texttt{C}'' are all valid examples of Global Verification feedback).

\item \textbf{Response Verification:} Granular response-classification feedback. Can either provide a score for several answer segments (\eg a unique score for each question on a quiz, or for each paragraph on a written text) or a score for several evaluative criteria (\eg evaluate the entire written text on readability, engagement, etc.). 

\item \textbf{Mistakes Verification:} Granular error-classification feedback. Can either simply state errors were committed or identify which types of errors are present in the submitted answer (it can mention the number of mistakes). 

\item \textbf{Correct Answer:} The correct answer, an expected solution or, for an open-ended task, a rewritten version of the submitted answer that fulfills the evaluative criteria (ideally with as few changes as possible).

\item \textbf{Response Elaboration:} An overview of the answer as a whole, incorporating feedback about the current level of performance of the student. It can choose to only mention part of it, focusing only on the learner's positive accomplishments or their shortcomings. It differs from mistakes elaboration as, though it can discuss shortcomings, it does not directly address mistakes.

\item \textbf{Mistakes Elaboration:} Detailed feedback about mistakes, including their location, thorough descriptions of their type and of their possible sources (\eg information about common mistakes and what misconceptions might lead to them). Can either be explicit or presented through hints or guiding questions. Note that no information on correcting mistakes is included as part of this feedback type, as these belong to the Procedural Elaboration feedback instead.

\item \textbf{Task Elaboration:} Clarifications about the task --- its type, requirements, constraints, sub-processes --- and relevant concepts and technical terms. Can either be explicit or presented through hints or guiding questions. Note, however, no information on task solving strategies is considered Task Elaboration type of feedback, as these belong to the Procedural Elaboration feedback instead.

\item \textbf{Procedural Elaboration:} Instructions on how to improve performance, be it through worked out examples, explanations on error correcting, or strategies for processing and solving the task. Can either be explicit or presented through hints or guiding questions.

\item \textbf{Metacognition Elaboration:} General strategies for learning and problem solving. This feedback cannot be directly related to the task being attempted by the learner. Can either be explicit or presented through hints or guiding questions.

\end{enumerate}
