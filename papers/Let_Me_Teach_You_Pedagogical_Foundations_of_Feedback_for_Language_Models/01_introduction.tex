\section{Introduction}

The last few years introduced a new paradigm finetuning training Large Language Models (LLMs): training them with human feedback \citep{ziegler2020finetuning, stiennon_learning_2022, bai2022training, openai2023gpt4} to augment their capabilities what they learned during pretraining \citep{christiano2017deeprl, wu2021recursively, menick2022teaching}. Pursuing this alignment has led to models that (1) are less toxic and harmful \citep{bai_constitutional_2022, korbak2023pretraining}, (2) have the ability to morally self-correct \citep{ganguli2023capacity}, and (3) are preferred by their users \citep{ouyang_training_2022, bai_constitutional_2022}.


\begin{figure}[ht]
\centering
\includegraphics[width=1.0\columnwidth]{figures/motivating_example.pdf}
\caption{An example task with the provided feedback classified using categories from our taxonomy (\S\ref{sec:taxonomy}). Different types of feedback produce different responses, motivating the systematic study of feedback types in natural language feedback research. \vspace{-10pt}}
\label{fig:base_framework}
\end{figure}


The currently most popular approach for aligning models with feedback is Reinforcement Learning from Feedback (RLF). Be it using human feedback (RLHF; \citealp{ouyang_training_2022, bai2022training, ramamurthy2023reinforcement}) or feedback generated by LLMs (RLAIF; \citealp{bai_constitutional_2022, saunders2022selfcritiquing, madaan2023selfrefine}), both have been shown to be successful in several metrics --- from encouraging honest behaviors, to reducing toxicity, to being overall preferred by evaluators \citep{ouyang_training_2022}. Other approaches, such as imitation learning \citep{li2016dialogue, kreutzer-etal-2018-neural, hancock2019learning, scheurer2022training}, and feedback modelling \citep{weston_dialog-based_2016, li_dialogue_2017, hancock2019learning, xu2022learning, liu2023chain} have had similar success. Feedback has thus emerged as an important source of information for model improvement and evaluation, ensuring that models progress toward desired objectives and behaviors \citep{fernandes_bridging_2023}.

Despite the observed benefits of learning from feedback, a systematic study of what constitutes helpful feedback remains absent from the ongoing discussion on this topic. For example, RLF requires a Reward Model (RM) to be trained on numerical or ranking-based feedback data \citep{rafailov2023direct} --- a format that is limited in the amount of information it conveys \citep{wu2023finegrained}. To counteract this limitation, works have begun exploring Natural Language Feedback (NLF; \citealp{weston_dialog-based_2016, madaan2023selfrefine, wu2023finegrained}). However, these works rely on the ``intuitive guesses'' of authors regarding what constitutes useful feedback. This approach leads different works to explore distinct conceptualizations of NLF, preventing a systematic comparison.
Given its importance in improving LLMs, the lack of systematicity in feedback design and analysis represents a current blind spot in NLP. 

Meanwhile, in the domain of learning sciences, there exist comprehensive studies of various aspects of feedback, as feedback is considered an essential component of instruction and learning. 
In this opinion piece, {we propose to ground the study of LLM feedback to pedagogy research, and to adapt this foundation for LLMs.}
%
To this end, we first present the most relevant feedback-related models from the learning sciences (§\ref{sec:background_pedagogy}).
Taking inspiration from these pedagogical models, we create a novel framework, FELT, that expansively maps the various features of the feedback space, incorporating dimensions exclusive to LLMs, such as prompt information and model characteristics (§\ref{sec:framework}).

We then introduce a comprehensive, grounded taxonomy of feedback content, with 10 distinct dimensions, defining several important aspects of feedback that remain underexplored (§\ref{ssec:general_taxonomy}). 
To make our taxonomy accessible, we further propose an interpretable categorization of feedback types, specifically designed to be modular and applicable to future works on feedback for LLMs (§\ref{ssec:categorical_taxonomy}). 
We use our dual taxonomy to classify feedback types in prior studies on LLM feedback NLP, as well as to study their effect in a case study on an open-ended summarization task (§\ref{sec:case_study}). The application of our taxonomy shines a light on the underspecification of current approaches to feedback formulation, and suggests promising areas of future research.

\paragraph{Contributions} We summarize our contributions as follows:
(i) We present a survey of pedagogical feedback formulations and models;
(ii) we outline the variables that influence feedback and its processing into a schematic framework, specifically adapted for LLMs;
(iii) we propose a general and extensive taxonomy of feedback content, as well as a more condensed collection of emergent feedback content categories;
(iv) we show that different feedback types produce distinctive changes in the revised generations of LLMs; and (v) we map previous research to our taxonomy, and provide guides and examples for stratification of feedback analysis using our categorization, thereby providing tools for future investigation of how feedback types influence LLM performance.
 


