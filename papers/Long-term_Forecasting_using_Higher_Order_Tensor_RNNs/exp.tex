We conducted  exhaustive experiments to examine the behavior of the proposed \trnn{} model on both synthetic and real-world time series data. The source code is available at \url{https://github.com/yuqirose/tensor_train_RNN}.
 
\subsection{Datasets}
% 
We validated the accuracy and efficiency of \trnn{} on the following three datasets. 
\paragraph{Genz}
Genz functions are often used as  basis for evaluating high-dimensional function approximation. In particular, they have been used to analyze tensor-train decompositions \citep{bigoni2016spectral}. There are in total $7$ different Genz functions. (1) $g_1 (x) = \cos( 2 \pi w + cx)$, (2) $g_2 (x) =( c^{-2} + (x+w)^{-2})^{-1}$, (3) $g_3(x) = (1+cx)^{-2}$, (4) $e^{- c^2\pi (x-w)^2}$ (5)  $e^{-c^2 \pi | x-w|}$ (6) $g_6(x) = \begin{cases} 0  \quad x>w\\ e^{cx} \quad  else \end{cases}$.  For each function, we generated a dataset with $10,000$ samples using \refn{eq:genzprodpeak} with $w=0.5$ and $c=1.0$ and random initial points draw from a range of $[-0.1, 0.1]$. 


\paragraph{Traffic}
%
We use the traffic data of Los Angeles County highway network collected from  California department of transportation \footnote{\url{http://pems.dot.ca.gov/}}. The dataset consists of $4$ month speed readings aggregated every $5$ minutes . Due to large number of missing values ($\sim 30\%$) in the raw data, we  impute the missing values using the average values of non-missing entries from other sensors at the same time. In total, after processing, the dataset covers $35\,136,$ time-series. We treat each sequence as daily traffic of $288$ time stamps. We up-sample the dataset every $20$ minutes, which results in a dataset of $8\,784$ sequences of daily measurements. We select $15$ sensors as a joint forecasting tasks.

\paragraph{Climate}
%
We use the daily maximum temperature data from the U.S. Historical Climatology Network (USHCN) daily  \footnote{\url{http://cdiac.ornl.gov/ftp/ushcn\_daily/}}. The dataset contains daily measurements for $5$ climate variables for approximately  $124$ years. The records were collected across more than $1\,200$ locations and span over $45\,384$ days. We analyze the area in California which contains $54$ stations. We removed the first $10$ years of day, most of which has no observations. We treat the temperature reading per year as one sequence and impute the missing observations using other non-missing entries from other stations across years. We augment the datasets by rotating the sequence every $7$ days, which results in a data set of $5\,928$ sequences.

% \paragraph{Lorenz}
% %
% %
% The Lorenz attractor system describes a two-dimensional flow of fluids (see Figure \ref{fig:lorenz}):
% %
% \begin{eqnarray*}
% \frac{\diff x}{\diff t} = \sigma(y-x), \hspace{10pt}
% \frac{\diff y}{\diff t} = x(\rho -z) - y, \hspace{10pt}
% \frac{\diff z}{\diff t} = xy - \beta z, \hspace{10pt}
% \sigma=10, \rho=28, \beta=2.667.
% \end{eqnarray*}
% %
% This system has chaotic solutions (for certain parameter values) that revolve around the so-called Lorenz attractor.
% %
% For experiments, we simulated $10\,000$ tracks with interval-length $0.01$ and initial states $\V{x}_0$ sampled from $[-0.1,0.1]^3$.
%

% Data vis
 \begin{figure*}[t]
 	\vskip 0.2in
 \begin{subfigure}[b]{0.32\linewidth}
 	\centering
 	\includegraphics[width=\linewidth]{Figure/genz.png}
 	\caption{\textsl{Genz dynamics}}
 	\label{fig:genz}
 \end{subfigure}
     \begin{subfigure}[b]{0.32\linewidth}
         \centering
         \includegraphics[width=\linewidth]{Figure/traffic.png}
         \caption{\textsl{Traffic} daily : 3 sensors  }
                 \label{fig:traffic}
     \end{subfigure}
     \begin{subfigure}[b]{0.32\linewidth}
         \centering
         \includegraphics[width=\linewidth]{Figure/climate.png}
         \caption{\textsl{Climate} yearly: 3 stations }
                 \label{fig:climate}
     \end{subfigure}
     \caption{Data visualizations:
     (\ref{fig:genz}) Genz dynamics,
     (\ref{fig:traffic}) traffic data,
     (\ref{fig:climate}) climate data.}
     \label{fig:data_visual}
     \vskip -0.2in
 \end{figure*}

Figure \ref{fig:data_visual} visualizes the time series from Genz dynamics, traffic and climate systems, respectively.  To test the stationarity of the time series, we also perform a Dickey–Fuller test on the real-world traffic and climate data. Dickey–Fuller  test is a commonly used statistical test procedure  to determine whether a time series is stationary. Its null hypothesis is that  a unit root is present in an autoregressive model, hence the time series  is not stationary.  The test statistics of the traffic and climate data is shown in Table \ref{app:tb:adf}, which demonstrate the non-stationarity of the time series.
\begin{table}[hb]
    \begin{center}
        \begin{tabular}{c |  cc | cc }
        \hline
           & \multicolumn{2}{c}{\bf Traffic } & \multicolumn{2}{c}{\bf   Climate } \\ 
           \hline
    Test Statistic      & 0.00003 & 0   & 3e-7 & 0  \\
p-value        &     0.96& 0.96  &     1.12 e-13 & 2.52 e-7  \\
Number Lags Used  &     2 & 7  &  0 & 1 \\
Critical Value (1\%)     &   -3.49& -3.51     &   -3.63& 2.7     \\
Critical Value (5\%)  & -2.89& -2.90   & -2.91& -3.70\\
Critical Value (10\%)       & -2.58 & -2.59   & -2.60 & -2.63\\
\hline
        \end{tabular}
    \caption{ Dickey-Fuller test  statistics for traffic and climate data used in the experiments. }
	\label{app:tb:adf}
    \end{center}
\vspace{-5mm}
\end{table}


\subsection{Training Details}

% \subsection{Long-term Forecasting Evaluation}
\paragraph{Setup}
%
We use a seq2seq architecture with \trnn{} using LSTM as recurrent cells (\tlstm{}).
%
For all experiments, we use the length-$T$ sequence regression loss
%
% \eq{
$L(y,\hat{y}) = \sum_{t=1}^T ||\hat{y}_t-y_t||^2_2,$
% }
%
where $y_t = x_{t+1}, \hat{y}_t$ are the ground truth and model prediction respectively.  For all datasets, we used a $80\%-10\%-10\%$ train-validation-test split and train for a maximum of $1e^4$ steps. We compute the moving average of  the validation loss as an early stopping criteria. We also did not include scheduled sampling \cite{bengio2015scheduled}, as we found training with scheduled sampling became highly unstable under a range of annealing schedules.

\paragraph{Hyperparameter Search}
All models are trained using  RMS-prop with a learning rate decay of $0.8$.  We performed an exhaustive search over the hyper-parameters for validation.  Table \ref{app:tb:hyper} reports the  search range of different hyper-parameters used in this work.

% Accuracy vs horizon
\begin{figure}[t]
\begin{center}
    \begin{subfigure}[t]{0.32\linewidth}
    		\centering
    		\includegraphics[width=\linewidth]{Figure/genz_err_step.png}
    		\caption{\textsl{Genz dynamics}}
    \end{subfigure}
   \begin{subfigure}[t]{0.32\linewidth}
        \centering
		\includegraphics[width=\linewidth]{Figure/traffic_err_step.png}
		\caption{\textsl{Traffic}}
    \end{subfigure}
    \begin{subfigure}[t]{0.32\linewidth}
	\centering
	\includegraphics[width=1.05\linewidth]{Figure/climate_err_step.png}
	\caption{\textsl{Climate}}
\end{subfigure}
    %
    \caption{
    %
    Long-term forecasting RMSE for \ti{Genz dynamics}  and real world \textsl{traffic}, \textsl{climate} time series (best viewed in color).  Comparison of LSTM, MLSTM, and \tlstm{} for varying forecasting horizons given same initial inputs. Results are averaged over $3$ runs. 
    }
    \label{fig:error_horizon}
\end{center}
     	\vskip -0.2in
\end{figure}
\begin{table}[htpb]
\centering
    \begin{tabular}{cc | cc}
    \hline
            \multicolumn{4}{c}{\bf Hyper-parameter search range} \\
            \hline 
            learning rate    & $10^{-1}\ldots 10^{-5}$   &hidden state size  &$8, 16, 32, 64, 128$  \\
        tensor-train rank  &$1 \ldots 16$ & number of lags &$1 \ldots 6$ \\
        number of orders & $1 \ldots 3$  &  number of layers  & $1 \ldots 3$ \\
        \hline
            % Longest forecast horizon for all data
        \end{tabular}
    \caption{Hyper-parameter search range statistics for \trnn{} experiments.}
	\label{app:tb:hyper}
\end{table}




%We did not employ scheduled sampling \cite{bengio2015scheduled}, as we found training became unstable under a range of annealing schedules.
\paragraph{Baselines}
We compared \trnn{} against 2 sets of natural baselines: 1st-order RNN (vanilla RNN, LSTM), and matrix RNNs (vanilla MRNN, MLSTM), which use matrix products of multiple hidden states without factorization \citep{soltani2016higher}.
%
We observed that \trnn{} with RNN cells outperforms vanilla RNN and MRNN, but using LSTM cells performs best in all experiments. We also evaluated the classic ARIMA time series model  with AR lags of $1 \sim 5$, and MA lags of $1 \sim 3$.   We  observed that it consistently performs $\sim 5\%$ worse than LSTM.
% 
\subsection{Long-term Forecasting Accuracy}
We evaluate the long-term forecasting accuracy of the proposed method and the baselines. 
For \textsl{traffic}, we forecast up to $18$ hours ahead with $5$ hours as  inputs.
%
For \textsl{climate}, we forecast up to $300$ days ahead given $60$ days of  observations.
%
For \textsl{Genz dynamics}, we forecast for $80$ steps given $5$ initial steps.
%
We report the forecasting results  averaged over $3$ runs.

\begin{wraptable}{r}{.5\linewidth}
\vspace{-22pt}
\begin{center}
\resizebox{\linewidth}{!}{%	
\begin{sc}
\begin{tabular}{lccc}
\multicolumn{4}{c}{\bf Moving-MNIST (RMSE $\times 10^{-2}$)} \\
& LSTM & MLSTM & \tlstm{} \\
\midrule
$T=20 $ & 9.45 & 9.92 & \textbf{8.94}\\
$T=40 $ & 10.04 & 9.94 & \textbf{9.92}
% $T=30 $ & & 
\end{tabular}
\end{sc}
}
\caption{Sequence-averaged per-pixel RMSE on Moving MNIST.}
\label{tab:movingmnist}
\end{center}
\vspace{-5mm}
\end{wraptable}
% 
%
Figure \ref{fig:error_horizon} shows the test prediction error (in RMSE) for varying forecasting horizons for different datasets.
%
We can see that \tlstm{} notably outperforms all baselines on all datasets in this setting. In particular, \tlstm{} is more robust to long-term error propagation.
%
We observe two salient benefits of using \trnn{}s over the unfactorized models.
%
First, MRNN and MLSTM can suffer from overfitting as the number of weights increases.
%
Second, on \textsl{traffic}, unfactorized models also show considerable instability in their long-term predictions.
%
These results suggest that \trnn{}s learn more stable representations that generalize better for long-term horizons.
%\resizebox{.80\linewidth}{!}{% 
%\begin{sc}
%\begin{tabular}{ccc}
%\multicolumn{3}{c}{\bf  Number of Parameters}\\
%TLSTM & MLSTM & LSTM \\
%\midrule
%7,200  & 9,700  & 8,700
%\end{tabular}
%\end{sc}
%}
%\caption{Best performing model size on real-world traffic and climate data.
%% 
%\vspace{-20pt}
%}
%\label{tb:bestmodel}
To compare the performance on high-dimensional time series, we also evaluated on the unsupervised video prediction task for \textsl{Moving MNIST}. We forecast $20$ and $40$ frames ahead given $10$ initial frames. The per-pixel forecasting RMSE results are shown in Table \ref{tab:movingmnist}. We observe a small gain ($\sim 2-5\%$) of \tlstm{} over the baselines. This is likely due to the fact that the underlying circular dynamics are still pretty simple. Moreover, the high-dimensional inputs have spatial structure that are hard to learn by RNNs alone (note we do not use convolutional features). We expect \tlstm{} to improve over baselines even more with more complicated dynamics and using convolutional features.



%
% Genz vis
%\begin{figure}[t]
%\begin{minipage}{.34\linewidth} 
%\begin{center}
%\begin{subfigure}[b]{\textwidth}
%\includegraphics[width=\linewidth]{Figure/genz_pred_1.png}
%\label{fig:f2}
%\end{subfigure}
	%add desired spacing between images, e. g. ~, \quad, \qquad, \hfill etc.
	%(or a blank line to force the subfigure onto a new line)
% 	\begin{subfigure}[b]{0.32\textwidth}
% 		\includegraphics[width=\linewidth]{Figure/genz_pred_2.png}
% 		\label{fig:df2}
% 	\end{subfigure}
% 	~ %add desired spacing between images, e. g. ~, \quad, \qquad, \hfill etc.
% 	%(or a blank line to force the subfigure onto a new line)
% 	\begin{subfigure}[b]{0.32\textwidth}
% 		\includegraphics[width=\linewidth]{Figure/genz_pred_3.png}
% 		\label{fig:df2}
% 	\end{subfigure}
%\caption{Genz dynamics predictions, Top (blue): ground truth. Bottom: \tlstm{} (red) MLSTM (green) and  LSTM (yellow). \tlstm{} perfectly captures the Genz oscillations, whereas the LSTM fails to do so (left). For more, see the Appendix.}
%\label{fig:genz_pred}
%\end{center}
%\end{minipage} 
%\hspace{0.01\linewidth}

% Genz vis
\begin{figure}[ht]
        \vskip 0.2in
    \begin{center}
    \begin{subfigure}[b]{0.31\textwidth}
        \includegraphics[width=\linewidth]{Figure/genz_pred_1.png}
        \label{fig:f2}
    \end{subfigure}
    ~ %add desired spacing between images, e. g. ~, \quad, \qquad, \hfill etc.
    %(or a blank line to force the subfigure onto a new line)
    \begin{subfigure}[b]{0.31\textwidth}
        \includegraphics[width=\linewidth]{Figure/genz_pred_2.png}
        \label{fig:df2}
    \end{subfigure}
    ~ %add desired spacing between images, e. g. ~, \quad, \qquad, \hfill etc.
    %(or a blank line to force the subfigure onto a new line)
    \begin{subfigure}[b]{0.31\textwidth}
        \includegraphics[width=\linewidth]{Figure/genz_pred_3.png}
        \label{fig:df2}
    \end{subfigure}
    \caption{Model prediction for three Genz dynamics ``product peak'' with different initial conditions. Top (blue): ground truth. Bottom: model predictions for LSTM (green) and \tlstm{} (red). \tlstm{} perfectly captures the Genz oscillations, whereas the LSTM fails to do so (left) or only approaches the ground truth towards the end (middle and right).}
        \label{fig:genz_pred}
\end{center}
        \vskip -0.2in
\end{figure}

\begin{figure}[ht]
\begin{center}
    \begin{subfigure}[b]{0.3\textwidth}
        \includegraphics[width=\textwidth]{Figure/traffic_pred_1.png}
    \end{subfigure}
    \begin{subfigure}[b]{0.32\textwidth}
        \includegraphics[width=\textwidth]{Figure/traffic_pred_2.png}
    \end{subfigure}
    \begin{subfigure}[b]{0.3\textwidth}
        \includegraphics[width=\textwidth]{Figure/traffic_pred_3.png}
    \end{subfigure}\\
    \begin{subfigure}[b]{0.32\textwidth}
        \includegraphics[width=\textwidth]{Figure/climate_pred_1.png}
    \end{subfigure}
    \begin{subfigure}[b]{0.32\textwidth}
        \includegraphics[width=\textwidth]{Figure/climate_pred_3.png}
    \end{subfigure}
    \begin{subfigure}[b]{0.32\textwidth}
        \includegraphics[width=\textwidth]{Figure/climate_pred_6.png}
    \end{subfigure}
    \caption{Top: $18$ hour ahead predictions for hourly \textsl{traffic} time series  given $5$ hour as input for LSTM, MLSTM and \tlstm{}.
%
    Bottom: $300$ days ahead predictions for daily \textsl{climate} time series given $2$ month observations as input for LSTM, MLSTM and \tlstm{}.}
    \label{fig:pred_vis}
\end{center}
        \vskip -0.2in
\end{figure}
\subsection{Visualization of Predictions}
%
To get intuition for the learned models, we visualize predictions from the best performing \tlstm{} and baselines.  Figure \ref{fig:genz_pred}  shows the  predictions for the Genz function ``corner-peak'' as the state-transition function from three realizations of Genz dynamics. We can see that \tlstm{} can almost perfectly recover the original function, while LSTM and MLSTM only correctly predict the mean. These baselines cannot capture the dynamics fully, often predicting an incorrect range and phase for the dynamics.


Figure \ref{fig:pred_vis} shows predictions for the traffic and climate datasets. This work uses deterministic models, hence the predictions correspond to the trend.
%
We can see that the \tlstm{} aligns significantly better with ground truth in long-term forecasting.
%
As the ground truth time series is highly nonlinear and noisy, LSTM often deviates from the general trend.
%
While both MLSTM and \tlstm{} can correctly learn the trend, \tlstm{} captures more detailed curvatures due to higher-order structure.


%
% Original order
%
% in Figure \ref{fig:pred_vis} for the real world traffic and climate dataset.
% %
% We can see that the \tlstm{} corresponds much better with ground truth in long-term forecasting.
% %
% As the ground truth time series is highly chaotic and noisy, LSTM often deviates from the general trend.
% %
% While both MLSTM and \tlstm{} can correctly learn the trend, \tlstm{} captures more detailed curvatures due to the inherent higher-order structure.

% Figure \ref{fig:genz} displays the Genz function ``corner-peak'', the state-transition function and the predictions from \tlstm{} and LSTM.
% %
% We can see that \tlstm{} can almost perfectly recover the original function, while LSTM and MLSTM only correctly predict the mean. These baselines cannot capture the dynamics fully, often predicting an incorrect range and phase for the dynamics.


%model capacity
\subsection{Model Capacity} 
The number of parameters for \trnn{} is $\mathcal{O} (HL+1)R^2P$ with hidden size $H$, lag $L$, rank $R$ and order $P$. This gives us more flexibility to decide the model capacity. Fewer parameters may have limited representation power, while more parameters would cause overfitting. 
%
\begin{wraptable}{r}{0.35\linewidth}
\caption{Best performing model size on traffic and climate data.}
\begin{tabular}{ccc}
\multicolumn{3}{c}{\bf  Number of Parameters}\\
TLSTM & MLSTM & LSTM \\
\midrule
7,200  & 9,700  & 8,700
\end{tabular}
\label{tb:bestmodel}
\end{wraptable}
%
Note that the memory complexity only grows quadratically with the rank $R$ while Theorem \ref{eqn:thm} shows the expressiveness of \trnn{} improves exponentially.
In practice, we used cross-validation to select the values for these hyper-parameters. The best models on real-world climate and traffic data are listed in Table \ref{tb:bestmodel}. We can see that the number of parameters of \tlstm{} model is comparable with that of MLSTM and LSTM. 

%TLSTM is more flexible than other methods, which gives us better control of the model complexity.




\begin{figure*}[htbp]
\begin{center}
    \begin{minipage}[b]{0.48\linewidth}

    \begin{center}
                 \includegraphics[width=\linewidth]{Figure/traffic_12_speed.png}
         \caption{Training speed evaluation of different models: validation loss versus number of steps. Results  are reported using the models with the best long-term forecasting accuracy.}
        \label{fig:ts_speed}      
   
    \end{center}
   \end{minipage}
    \hspace{0.01\linewidth}
   \begin{minipage}[b]{0.48\linewidth}
   \begin{center}
   \begin{sc}
\resizebox{\linewidth}{!}{%	
    \begin{tabular}{lcccc}
    \multicolumn{5}{c}{\bf \tlstm{} Prediction Error (RMSE $\times 10^{-2}$)} \\    
    Rank $r$  & 2 & 4 & 8 & 16 \\   	    
    \midrule
    Genz ($T=95$)     & \bf{0.82}  & 0.93  & 1.01    & 1.01  \\
    Traffic ($T=67$)  & 9.17  & \bf{9.11}  & 9.32 & 9.31 \\
    Climate ($T=360$)  & 10.55 & \bf{10.25} & 10.51 & 10.63
    \end{tabular}
	}
    \end{sc}
    \captionof{table}{\tlstm{} performance for varying tensor rank $r$ with $L=3$.}
    \label{tb:hyperpara}
   
   \begin{sc}
   \resizebox{\linewidth}{!}{%	
       \begin{tabular}{lcccc}
        \multicolumn{5}{c}{\bf \tlstm{} Traffic Prediction Error (RMSE $\times 10^{-2}$)} \\    
    Lags $L$  & 2 &  4 &5 & 6\\
   	\midrule
    $T=12 $   & \bf{7.38}   &	7.41&7.43&	7.41 \\
    $T=84 $   & \bf{8.97}	  & 9.31 &	9.38&	9.01 \\
    $T=156 $  &9.49   &	9.32	&9.48& \bf{9.31} \\
    $T=228 $  &10.19  & 9.63&	\bf{9.58} &	9.94
    \end{tabular}
    }
       \end{sc}
    \captionof{table}{\tlstm{} performance for various lags $L$ and prediction horizons $T$.}
    \end{center}
    \end{minipage}
\end{center}
\vskip -0.25in
\end{figure*} 


\subsection{Speed Performance Trade-off}
 %
 We now investigate potential trade-offs between accuracy and computation.
 %
 Figure \ref{fig:ts_speed} displays the validation loss with respect to the number of steps, for the best performing models on long-term forecasting. We see that \trnn{}s converge faster than other models, and achieve lower validation-loss.
 %
 This suggests that \trnn{} has a more efficient representation of the nonlinear dynamics, and can learn much faster as a result.






\subsection{Sensitivity Analysis}
% 
%\stzedit{Accuracy versus rank, versus lag, versus horizon $T$ (see Figure \ref{fig:acc-T}).}
The \tlstm{} model has several hyperparameters, such as tensor-train rank and lag $L$. 
% 
We study the sensitivity of \tlstm{} to these hyperparameters; Table \ref{tb:hyperpara} shows the results. 
% 
In the top row, we report the prediction RMSE for the largest forecasting horizon  w.r.t tensor ranks for all the datasets with lag $3$. 
% 
When the rank is too low, the model does not have enough capacity to capture non-linear dynamics. 
% 
When the rank is too high, the model starts to overfit. 
% 
In the bottom row, we report the effect of changing lag $L$. For each setting, the best $r$ is determined by cross-validation. 
% 
Note that the best lag $L$ also varies for different forecasting horizons.
% 



\subsection{Chaotic Nonlinear Dynamics}

Chaotic dynamics such as Lorenz attractor is notoriously different to lean in non-linear dynamics. In such systems, the dynamics are highly sensitive to perturbations in the input state: two close points can move exponentially far apart under the dynamics.  We  also evaluated tensor-train neural networks on long-term forecasting for Lorenz attractor and report the results.

%
\paragraph{Lorenz}
%
The Lorenz attractor system describes a two-dimensional flow of fluids:
%
\begin{eqnarray*}
	\frac{\diff x}{\diff t} = \sigma(y-x), \hspace{10pt}
	\frac{\diff y}{\diff t} = x(\rho -z) - y, \hspace{10pt}
	\frac{\diff z}{\diff t} = xy - \beta z, \hspace{10pt}
	\sigma=10, \rho=28, \beta=2.667.
\end{eqnarray*}
%
This system has chaotic solutions (for certain parameter values) that revolve around the so-called Lorenz attractor.
%
We simulated $10\,000$ trajectories with the discretized time interval length $0.01$. We sample from each trajectory every $10$ units in Euclidean distance. 
%
\begin{wrapfigure}{r}{0.4\linewidth}
	\includegraphics[width=0.9\linewidth]{Figure/lorenz.png}
			\caption{Lorenz Attractor}
			\label{fig:lorenz_data}
			\vspace{-3mm}
\end{wrapfigure}
As shown in Figure \ref{fig:lorenz_data}, the blue trajectory represents the discretized dynamics and red circles are sampled observations.  The dynamics is generated using $\sigma =10$ $\rho=28$, $\beta =2.667$. The initial condition of each trajectory is  sampled uniformly random from the interval of $[-0.1,0.1]$. 



Figure \ref{fig:long-term} shows $45$ steps ahead predictions for all models. HORNN is the full tensor \trnn{} using vanilla RNN unit without the tensor-train decomposition. We can see all the tensor models perform better than vanilla RNN or MRNN. \trnn{} shows slight improvement at the beginning state.
\begin{figure}[htbp]
\begin{center}
    \begin{subfigure}[b]{0.19\textwidth}
        \includegraphics[width=\textwidth]{Figure/rnn_s2s.png}
        \caption{RNN}
    \end{subfigure}
    \begin{subfigure}[b]{0.19\textwidth}
        \includegraphics[width=\textwidth]{Figure/mrnn_s2s.png}
        \caption{MRNN}
    \end{subfigure}
    \begin{subfigure}[b]{0.19\textwidth}
    \includegraphics[width=\textwidth]{Figure/hornn_s2s.png}
    \caption{HORNN}
\end{subfigure}
    \begin{subfigure}[b]{0.19\textwidth}
        \includegraphics[width=\linewidth]{Figure/trnn_s2s.png}
        \caption{\trnn{}}
    \end{subfigure}
    \begin{subfigure}[b]{0.19\textwidth}
        \includegraphics[width=\linewidth]{Figure/tlstm_s2s.png}
        \caption{\tlstm{}}
    \end{subfigure} % \\ %example 1
    % \begin{subfigure}[b]{0.31\textwidth}
    %     \includegraphics[width=\textwidth]{Figure/rnn_s2s_2.png}
    %     \caption{RNN}
    %     \label{app:fig:f1}
    % \end{subfigure}
    % \begin{subfigure}[b]{0.31\textwidth}
    %     \includegraphics[width=\textwidth]{Figure/mrnn_s2s_2.png}
    %     \caption{MRNN}
    %     \label{app:fig:df1}
    % \end{subfigure}
    %    \begin{subfigure}[b]{0.31\textwidth}
    %     \includegraphics[width=\textwidth]{Figure/hornn_s2s_2.png}
    %     \caption{\trnn{}}
    %     \label{app:fig:f2}
    % \end{subfigure}\\ %example 2
    \caption{Long-term (right 2) predictions for different models (red) versus the ground truth (blue). \trnn{} shows more consistent, but imperfect, predictions, whereas the baselines are highly unstable and gives noisy predictions. }
    \label{fig:long-term}
\end{center}
\vskip -0.3in
\end{figure}

 
 
We have also evaluated \trnn{} on long-term forecasting for \textit{chaotic} dynamics, such as the Lorenz dynamics. Such dynamics are highly sensitive to input perturbations: two close points can move exponentially far apart under the dynamics. This makes long-term forecasting highly challenging, as small errors can lead to catastrophic long-term errors. Figure \ref{fig:lorenz} shows that \trnn{} can predict up to $T=40$ steps into the future, but diverges quickly beyond that. We have found no state-of-the-art prediction model is stable beyond $40$ time step in this setting.
%
 \begin{figure*}[h]
	\begin{center}
		\begin{subfigure}[b]{0.23\textwidth}		\includegraphics[width=\textwidth]{Figure/tlstm_t20.png}
			\caption{$T=20$}
			\label{fig:lorenz_20}
		\end{subfigure}
		\begin{subfigure}[b]{0.23\textwidth}			\includegraphics[width=\textwidth]{Figure/tlstm_t40.png}
			\caption{$T=40$}
			\label{fig:lorenz_40}
		\end{subfigure}
		\begin{subfigure}[b]{0.23\textwidth}
			\includegraphics[width=\textwidth]{Figure/tlstm_t60.png}
			\caption{$T=60$}
			\label{fig:lorenz_60}
		\end{subfigure}
		\begin{subfigure}[b]{0.23\textwidth}
			\includegraphics[width=\textwidth]{Figure/tlstm_t80.png}
			\caption{$T=80$}
			\label{fig:lorenz_80}
			\label{fig:f1}
		\end{subfigure}
		\caption{\ref{fig:lorenz_data} Lorenz attraction  with dynamics (blue) and sampled data (red).  \ref{fig:lorenz_20}, \ref{fig:lorenz_40}, \ref{fig:lorenz_60} ,\ref{fig:lorenz_80}   \tlstm{} long-term predictions for different forecasting horizons $T$ versus the ground truth (blue). \tlstm{} shows consistent predictions over increasing horizons $T$.}
		\label{fig:lorenz}
	\end{center}
\vskip -0.5in
\end{figure*}
