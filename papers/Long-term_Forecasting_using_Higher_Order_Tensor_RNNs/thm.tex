%\paragraph{Theoretical Analysis}
%
A significant benefit of using \trnn{} is that we can theoretically characterize its expressiveness  for approximating the underlying dynamics.   The main idea is to analyze a class of functions that satisfies certain regularity conditions. For such functions, tensor-train representations preserve the weak differentiability and yield a compact representation.


The following theorem characterizes the representation power of \trnn{}, viewed as a one-layer hidden neural network, in terms of 1) the regularity of the target function $f$, 2) the dimension of the input space, 3) the tensor train rank and 4) the order of the tensor:
%
\begin{theorem}
Let the target function $f\in \mathcal{H}^k_\mu$ be a H\"older continuous function defined on a input  domain $\mathcal{I} =I_1\times \cdots \times I_d$, with  bounded derivatives up to order $k$ and finite Fourier magnitude distribution $C_f$. A single layer \trnn{} with $h$ hidden units, $\hat{f}$ can approximate $f$ with approximation error $\epsilon$ at most:
%
\eq{
%
\epsilon \leq \fr{1}{h} \brck{C_f^2 \frac{d-1}{(k-1)(r+1)^{k-1}} + C(k)p^{-k} }
%
}
%
where $C_f = \int |\omega|_1 |\hat{f}(\omega) d \omega|$, $d$ is the dimension of the function, i.e., the size of the state space, $r$ is the tensor-train rank, $p$ is the degree of the higher-order polynomials i.e., the order of the tensor, and $C(k)$ is the coefficient of the spectral expansion of $f$.
\label{eqn:thm}
\end{theorem}

\textbf{Remarks}: The result above shows that the number of weights required to approximate the target function $f$ is dictated by its regularity (i.e., its H\"older-continuity order $k$). The expressiveness of \trnn{} is driven by the selection of the rank $r$ and the polynomial degree $p$; moreover, it improves for functions with increasing regularity.  Compared with ``first-order'' regular RNNs, \trnn{}s are  exponentially more powerful for large rank: if the order  $p$ increases, we require fewer hidden units $h$. 
% 
% The results also provide intuitions for choosing the hidden size and the rank for optimal storage. \ryedit{don't know what storage means}
%
% as long as we are given a state transitions $(\V{x}_t, \V{s}_t) \mapsto \V{s}_{t+1}$ (e.g. the state transition function learned by the  encoder).

\ti{Proof sketch}: 
For the full proof, see the Appendix. 
% 
We design \trnn{} to approximate the underlying system dynamics. The target function $f(\V{x})$ represents the state transition function, as in \refn{eqn:tensor_rnn}.
%
We first show that if $f$ preserves weak derivatives, then it has a compact tensor-train representation. Formally, let us assume that $f$ is a Sobolev function: $f\in\mathcal{H}^k_\mu$, defined on the input space $\T{I}= I_1 \times I_2\times \cdots I_d $, where each $I_i$ is a set of vectors. The space $\mathcal{H}^k_\mu$ is defined as the functions that have bounded derivatives up to some order $k$ and are $L_\mu$-integrable.
%
\begin{eqnarray}
\mathcal{H}^k_\mu =  \left\{  f  \in L_\mu(\T{I}):\sum_{i\leq k}\|D^{(i)}f\|^2   < +\infty \right\},
\end{eqnarray}
%
where $D^{(i)}f$ is the $i$-th weak derivative of $f$ and $\mu \geq 0$.\footnote{A weak derivative generalizes the derivative concept for (non)-differentiable functions and is implicitly defined as: e.g. $v\in L^1([a,b])$ is a weak derivative of $u\in L^1([a,b])$ if for all smooth $\varphi$ with $\varphi(a) = \varphi(b) = 0$: $\int_a^bu(t)\varphi'(t) = -\int_a^bv(t)\varphi(t)$.}
% 
% The space $\mathcal{H}^k_\mu$ is equipped with a norm $\|f\|^2_{k,\mu} =\sum_{|i|\leq k}\|D^{(i)}f\|^2$ and a semi-norm $|f|^2_{k,\mu} =\sum_{|i|=k}\|D^{(i)}f\|^2 $. For notation simplicity, we muted the subscript $\mu$ and used $\|\cdot\| $ for $\|\cdot \|_{L_{\mu}}$.
% f
It is known that any Sobolev function admits a Schmidt decomposition: $f(\cdot) = \sum_{i =0}^\infty \sqrt{\lambda_i } \gamma (\cdot)_i \otimes \phi (\cdot)_i $, where $\{\lambda \}$ are the eigenvalues and $\{\gamma\}, \{ \phi\}$ are the associated eigenfunctions.
%
Hence, for $\V{x}\in\calI$, we can represent the target function $f(\V{x}) $ as an infinite summation of products of a set of basis functions:
%
\begin{align}
&f(\V{x}) = \sum_{\alpha_0,\cdots,\alpha_d=1}^\infty
%
% \T{A}^1_{\alpha_0, x_1, \alpha_1}
\T{A}^1 (x_1)_{\alpha_0 \alpha_1}
%
\cdots
%
\T{A}^d(x_d)_{\alpha_{d-1}	 \alpha_d},
\label{eqn:ftt}
\end{align}
%
where  $ \{ \T{A}^j(x_j)_{\alpha_{j-1} \alpha_j} \}$ are basis functions over each input dimension.
% = \sqrt{\lambda^{d-1}_{\alpha_{d-1}}} \phi (x_d)_{\alpha_{d-1}}
These basis functions satisfy $\langle \T{A}^j(\cdot)_{im}, \T{A}^j (\cdot)_{in} \rangle = \delta_{mn}$ for all $j$.
% 
If we truncate \eqref{eqn:ftt} to a low dimensional subspace ($\V{r}<\infty$), we obtain a functional approximation of the state transition function $f(\V{x})$.  
% 
This approximation is also known as the \ti{functional tensor-train} (FTT):
%
\begin{align}
&f_{FTT}(\V{x}) = \sum_{\alpha_0,\cdots,\alpha_d}^\mathbf{r}
%
\T{A}^1(x_1)_{\alpha_0\alpha_1}
%
\cdots
%
\T{A}^d(x_d)_{\alpha_{d-1}\alpha_d},
\end{align}

In practice, \trnn{} implements a polynomial expansion of the states using $[\V{s}, \V{s}^{\otimes 2}, \cdots, \V{s}^{\otimes P}]$, where $P$ is the degree of the polynomial. The final function represented by \trnn{} is a polynomial approximation of the functional tensor-train function $f_{FTT}$. 

Given a target  function $f(\V{x}) = f(\V{s}\otimes \dots \otimes \V{s})$, we can express it using FTT and the polynomial expansion of the states $\V{s}$. This allows us to characterize \trnn{} using a family of functions that it can represent.  Combined with the classic neural network approximation theory \cite{barron1993universal}, we can bound the approximation error for \trnn{} with one hidden layer. The above results applies to the full family of \trnn{}s, including those using  vanilla RNN or LSTM as the recurrent cell.

One can think of the universal approximation result in Theorem \ref{eqn:thm} bounds the estimation bias of the model: $f-\mathbb{E} [\hat{f}]$, where the expectation is taken over training sets. While a large neural network can approximate any function, training as a large neural network will be hard given a finite data set, demonstrating bias-variance trade-off. In the next section, we provide bounds for the estimation variance. 
 
%
% Given a target function $f$, and a neural network with one hidden layer and sigmoid activation function, the following lemma describes the classic result of  describing the error between  $f$ and the single hidden-layer neural network that approximates it best:
%
% \begin{lemma}[NN Approximation \cite{barron1993universal}]
% 	Given a function $f$ with finite Fourier magnitude distribution $C_f$, there exists a  neural network of $n$ hidden units $f_n$, such that
% 	\begin{eqnarray}
% 		\| f - f_n\| \leq \frac{C_f}{\sqrt{n}}
% 	\end{eqnarray}
% 	where $C_f = \int |\omega|_1  | \hat{f}(\omega) | d \omega$ with Fourier representation $f(x)=\int e^{i\omega x}\hat{f}(\omega) d\omega$.
% 	\label{lemma:nn}
% \end{lemma}
% 
% We can  generalize Barron's approximation result in  lemma \ref{lemma:nn} to \trnn{}.  



% As the rank of the tensor-train and the polynomial order increase, the required size of the hidden units become smaller, up to a constant that depends on the regularity of the underlying dynamics $f$.
%
% This theorem applies to  the entire family of \trnn{}s, including those using vanilla RNN or LSTM as the recurrent cell, as long as we are given a state transition function $(\V{x}_t, \V{s}_t) \mapsto \V{s}_{t+1}$. (e.g. the state transition function learned by the  encoder).
%
% In this case, Theorem \ref{eqn:thm} bounds the estimation error when approximating a state-transition function $f:\V{s}_t \mapsto \V{x}_{t+1}$ using a tensor-train layer.
%
% However, the general case where the state-encoder is not known.
% For instance, when using $\V{x} = [\V{s} \V{s} \V{s}]$, we recover the.tensor transition function in
%
% $N$ refers to number of state transitions (or examples) seen during training. The fewer transitions we observe, the less information we get about the true dynamics, and we need to memorize more information for the same approximation error.
% \stzedit{If we assume that the Does this apply to LSTMs as well?}
