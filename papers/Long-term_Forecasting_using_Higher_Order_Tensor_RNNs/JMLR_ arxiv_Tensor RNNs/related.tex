%chaotic dynamics
\paragraph{Time series forecasting}
Time series forecasting is at the core of many dynamics modeling tasks. In statistics, classic work  such as the ARMA or ARIMA model \citep{box2015time}  model a stochastic process with assumptions of linear dynamics. In the control and dynamical system community, estimating dynamics  models from measurement data is also known as \ti{system identification} \citep{ljung2001system}.  System identification often requires strong parametric assumptions which are often challenging to find from first principles.  Moreover, finding (approximate) solutions of complex nonlinear differential equations demands high computational cost. In our work, we instead take a ``mode-free'' approach to learn a powerful approximate nonlinear dynamics model.



\paragraph{Recurrent Neural Networks}
Using neural networks to model time series data has a long history  \citep{schmidhuber2015deep}. Recent developments in deep leaning and RNNs has led to non-linear forecasting models such as deep AutoRegressive  \citep{flunkert2017deepar},  Predictive State Representation \citep{downey2017practical}, Deep State Space model \citep{rangapuram2018deep}. However, these works usually study short-term forecasting and use RNNs that contain only the most recent state.  Our method contrasts with this by explicitly modeling higher-order dyanmics to capture long-term dependencies.
%  \citep{aihara1990chaotic} first introduces chaotic neural networks in order to simulate the chaotic behavior of biological neurons, while \citep{jaeger2004harnessing} uses echo state networks (ESN) to predict a chaotic time series for communication channel energy saving. These approaches are orthogonal to the class of RNNs, as they do not model hidden state dynamics. 
 


%higher-order rnn
There are several classic work on higher-order RNNs. For example, \citep{giles1989higher} proposes a higher-order RNN to simulate a deterministic finite state machine and recognize regular grammars. The model considers a multiplicative structure of inputs and the most recent hidden state, but is limited to two-way interactions. \citep{sutskever2011generating} also studies tensor RNNs that allow a different hidden-to-hidden weight matrix for every input dimension.
%
\cite{soltani2016higher} proposes a higher-order RNN that concatenates a sequence of past hidden states, but the underlying state interactions are still linear. 
%
Moreover, hierarchical RNNs \citep{zheng2016generating} have been used to model sequential data at multiple temporal resolutions. Our method generalizes all these works to capture higher-order interactions using a hidden-to-hidden tensor.

\paragraph{Tensor methods}
Tensor methods  have tight connections with neural networks. For example,  \citep{novikov2015tensorizing, stoudenmire2016supervised} employ tensor-train to compress the weights in neural networks.  \citep{yang2017tensor}  extends this idea to RNNs by reshaping  the inputs  into a  tensor and factorizes the input-hidden weight tensor. However, the purpose of these works is model compression in the input space whereas our method learns the dynamics in the hidden state space. Theoretically, \citep{cohen2016expressive} shows convolutional neural networks and  hierarchical tensor factorizations are equivalent. \citep{khrulkov2017expressive} provides expressiveness analysis for shallow networks using tensor train.  


Tensor methods have also been used for sequence modeling. For example, one can apply tensor decomposition as  method of moments estimators for latent variable models such as Hidden Markov Models (HMMs)  \citep{anandkumar2012method}. Tensor methods have also shown promises in reducing the model dimensionality of multivariate spatiotemporal learning problems \citep{yu2016learning}, as well as  nonlinear system identification \citep{decuyper2019decoupling}. Most recently, \cite{schlag2018learning} combine tensor product of relational information and recurrent neural networks for natural language reasoning tasks.  This work however, to the best of our knowledge, is the first to consider tensor networks within RNNs  for sequence learning in environments with nonlinear dynamics.


% Predictive State Representation (PSR) also model sequences (\citep{downey2017practical}), by solving an easier version of the POMDP problem, which is different from time series forecasting.
% %
% (2) In time series, we don't have to learn the transition from (hidden) states to observations in PSR. The dynamics are fully captured by observations. (3) For real world time series like traffic, we don't have many realizations of the same dynamics, which is also different from robotics.
% %
% (4) For long term dependencies, as authors point out, ``the multi-step objective in PSR is the same back-prop-through-time (BPTT) in RNN''. And we already use it during our training.
