%problem poses a significant unsolved structured prediction challenge,


One of the central questions in science is forecasting: given the past history, how well can we predict the future?
%
In many domains with complex multi-variate correlation structures and nonlinear dynamics, forecasting is highly challenging since the system has long-term temporal dependencies and higher-order dynamics. Examples of such systems abound in science and engineering, from biological neural network activity, fluid turbulence, to climate and traffic systems (see, e.g., Figure \ref{fig:time_series}).
%
Since current forecasting systems are unable to faithfully represent the higher-order dynamics, they have limited ability for accurate \ti{long-term} forecasting.


%
Therefore, a fundamental challenge is accurately modeling nonlinear dynamics and obtaining stable long-term predictions, given a dataset of realizations of the dynamics.
%
Here, the forecasting problem can be stated as follows: how can we efficiently learn a model that, given only a few initial states, can  predict a sequence of future states over a long horizon of $T$ time-steps accurately and reliably?

\begin{wrapfigure}{r}{0.36\textwidth}
\vspace{-18pt}
\begin{center}
\includegraphics[width=\linewidth]{Figure/time_series.png}
% \includegraphics[width=0.38\linewidth]{Figure/tensor_train_fp.png}
% \vspace{-10pt}
\caption{Climate and traffic time series visualization. The time series can be viewed as a realization of highly nonlinear dynamics. 
% Right: tensor-train RNN unit encodes higher-order dynamics and factorizes hidden states with tensor train decomposition.
}
\label{fig:time_series}
\vspace{-20pt}
\end{center}
\end{wrapfigure}

Common approaches to forecasting include classic linear time series models such as auto-regressive moving average (ARMA), state space models such as hidden Markov model (HMM), and deep neural networks. See a survey on time series forecasting by \citep{box2015time} and the references therein.
%
A recurrent neural network (RNN), as well as its memory-based extensions such as the LSTM, is a class of models that have achieved state of the art performance on sequence prediction tasks from demand forecasting \citep{ flunkert2017deepar} to speech recognition \citep{soltau2016neural} and video analysis \citep{lecun2015deep}.
%
But most of these methods focus on short-term predictions, and often fail to generalize to nonlinear dynamics and forecast over long time horizons. 



In this work, we propose \trnn{}, a model class that is more expressive and empirically generalizes better than standard RNNs, for the same model capacity.
% 
% In this work, we propose a novel family of recurrent neural networks: \trnn{}, that can learn stable long-term forecasting.
%
\trnn{} explicitly models the 1) \ti{higher-order dynamics}, by
%
incorporating a longer history  and higher-order state interactions of previous hidden states; and
%
2) using \ti{tensor trains decomposition} that greatly reduces the number of model parameters, while mostly preserving the correlation structure of the full-rank model. 
%
We prove that
% 
\trnn{} is exponentially more expressive than standard RNNs for functions that satisfy certain regularity conditions. Our contributions can be summarized as follows:
%
\begin{itemize} %[leftmargin=*]
\item We propose a novel family of RNNs \trnn{}s to encode non-Markovian dynamics and higher-order state interactions. To address the memory issue, we propose a tensor-train decomposition that makes learning tractable.
%
\item We provide theoretical guarantees for the expressiveness of \trnn{}s for nonlinear dynamics, and characterize the target dynamics and its \trnn{} representation. In contrast,  no such theoretical results are known for standard recurrent networks.
%
\item We show that \trnn{}s can forecast more accurately for significantly longer time horizons  compared to standard RNNs and LSTMs on simulated data and real-world environments with nonlinear dynamics.
% 
\end{itemize}
