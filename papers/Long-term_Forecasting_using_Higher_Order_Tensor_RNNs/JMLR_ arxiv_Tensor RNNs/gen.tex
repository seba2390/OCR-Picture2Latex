% generalization bound for HOT-RNN
% Existing theoretical results for the generalization bound include (1) Rademacher complexities for neural networks, viewed as linear combinations of nonlinearities, $\sigma: \R \rightarrow [-1,1]$ \cite{bartlett2002rademacher}.  (2) A complexity measure for neural networks, that is based on the Lipschitz constant, but normalized by the margin of the predictor \cite{bartlett2017spectrally} (3) A similar margin-based bound using PAC-Bayesian analysis \cite{}

Given a time series from a $P$-th order dynamics $(X_1, \cdots, X_P)$,  denote the variance over the joint hidden states as  $\hat{C} := \sum_{i=1}^m \otimes [\T{A}(X^{(i)}_1), \cdots ,\T{A}(X^{(i)}_P)] $, where $\T{A}(\cdot)$ are the basis functions. Define the variance of the estimated dynamics as $C := \E_{X_1 X_2 \cdots,  X_P}[\phi(X_1)\otimes \phi(X_2), \cdots, \otimes \phi(X_P)]$, with $\phi(\cdot)$ being the feature mapping. The following theorem bounds the estimation variance of the \trnn{}. 
%The conditional operator $\mu_{Y|x} := \E_{Y|x}[\phi(Y)]$. 
\begin{theorem}[Estimation Variance Bound]
Assuming the time series is governed by a system whose order of dynamics is at most $P$, represented as a joint  probabilistic distribution $P(X_1,\cdots, X_P)$.  The variance for the  \trnn{} estimator $\hat{C}$ and the true variance $C$ of  the population statistics is  upper bounded by:
\[   \|\hat{C} - C \| = \mathcal{O}_p \big( \frac{2\sqrt{2} \rho^{P/2}\sqrt{\log \frac{2}{\delta}}}{\sqrt{m}} \big) \]

where $C := \E_{X_1 X_2 \cdots,  X_P}[\phi(X_1)\otimes \phi(X_2), \cdots, \otimes \phi(X_P)]$ and $m$ is number of samples, $\rho:=\sup\limits_{x\in \T{X}}k(x,x)$.
\end{theorem}

\ti{Proof }: 
The tensor-train distribution forms a Gibbs field, thus based on Hammersley-Clifford Theorem, a Gibbs field satisfies global Markov property, therefore, it must be a  Conditional Random Field (CRF) as the  conditional probability distribution factorizes. According to the duality between tensor network and graphical model \cite{robeva2017duality}, tensor train is the dual graph of CRF. Figure \ref{fig:tt-crf} visualizes such dual relationship in the graphical model template. 
%
After establishing the relationship between conditional random field and functional tensor-train, we can bound the variance of the \trnn{} estimator. 

 \begin{figure}[htbp]
	\begin{center}
		\begin{subfigure}[b]{0.55\linewidth}		\includegraphics[width=0.9\linewidth]{Figure/tensor_train_graph.png}
			\caption{Tensor Train Graph}
			\label{fig:lorenz_data}
		\end{subfigure}
		\begin{subfigure}[b]{0.4\textwidth}		\includegraphics[width=0.9\linewidth]{Figure/hidden_markov_graph.png}
			\caption{Conditional Random Field}
			\label{fig:lorenz_20}
		\end{subfigure}
	\end{center}
\vskip -0.1in
\caption{The graphical representation of tensor train model and conditional random field. The circles denote hidden variables and the shaded circles represent observed variables.}
\label{fig:tt-crf}
\end{figure}


We first state the well-celebrated Hammersley-Clifford theorem that gives the sufficient and necessary conditions of which a probability distribution  is a Markov Random field.

\begin{theorem}[Hammersley-Clifford]
A  graphical model  $G$ is a \textit{Markov Random Field} if and only if
%\[  P(X_i|X_{G\i})= P(X_i|X_{N_i}) \] 
%where $X_{G\i}$ denotes all the nodes except for $X_i$ and $X_{N_i}$ denotes the neighborhood of $X_i$. 
the probability distribution $P(X)$ on $G$ is a \textit{Gibbs distribution}:
\[P(X)= \frac{1}{Z} \prod_{c\in C_G}\psi(X_c) \]
where $Z=\sum_x  \prod_{c\in C_G}\psi(X_c)$ is the normalization constant, $\psi$ are functions defined on maximal cliques, and $C_G$ is a set of all maximal cliques in the graph.
\label{thm:hc}
\end{theorem}

When the underlying distribution is a Markov random  field and belongs to the exponential family, we can generalize Theorem \ref{thm:hc} to kernel functions and obtain the following results.
\begin{lemma}[Kernelized Hammersley-Clifford] \cite{altun2012exponential}
Given a Markov random field $X$ with respect to a graphical model $G$, if  the sufficient statistics $\Phi(X) = (\Phi(X_{c_1}) , \cdots, \Phi(X_{c_i}) )$, then the kernels  $k(X, X')=\langle \Phi(X), \Phi(X')\rangle $ satisfy
\[k(X, X') = \sum_{c\in C_G}k_c(X_c, X_c')\]
where  $\Phi(X_c)$ are the sufficient statistics defined on maximal cliques, $k_c(X, X') = \langle \Phi(X_C), \Phi(X'_C) \rangle $
\label{thm:khc}
\end{lemma}




%\begin{theorem}[Tensor Train and CRF]
%% models conditional rather than joint distribution as in HMM
%The joint distribution $P(X_1, \cdots, X_d)$ over $d$ discrete variables $X_1, \cdots, X_d$ follows the  Conditional Random Field (CRF) if and only if the density tensor can be factorized according to the tensor train model.
%\label{thm:eq}
%\end{theorem}

Our tensor train model factorizes the state transition function  
$\V{h}_t = \V{x}_t+  f(\V{s}_1 \otimes \cdots \otimes \V{s}_p)$, where each augmented hidden states $\V{s} =  [1, \V{h}_{t-1}, \cdots, \V{h}_{t-L}]$. 
%
For a joint distribution of $d$ variables  $X_1, \cdots, X_P$, taking values from $1$ to $n$ from a conditional random field. Their joint probability density table is a d-dimensional tensor $P(X_1, \cdots, X_P) \in \R^{n^P}$, with the values of the variable act as indices. Following the result  of Lemma \ref{thm:khc},  the feature  functions  $\T{A}(X_p)$ factorize over maximum cliques:
\begin{eqnarray*}
P(X_1,\cdots, X_P)&=& \sum_{\alpha_0, \cdots, \alpha_P}P(X_1,\cdots,X_d,\alpha_0,\cdots, \alpha_P)\\
% &=& P(X_1|\alpha_0,\alpha_1) \cdots P(X_d|\alpha_{d-1}, \alpha_d)P(\alpha_0, \cdots, \alpha_P)\\
&=& \sum_{\alpha_0,\cdots,\alpha_P=1}^\infty
%
% \T{A}^1_{\alpha_0, x_1, \alpha_1}
\T{A}^1 (X_1)_{\alpha_0 \alpha_1}
%
\cdots
%
\T{A}^d(X_d)_{\alpha_{P-1}	 \alpha_P},
\end{eqnarray*}
which is the functional tensor-train model.



% \cite{robeva2017duality} demonstrates the duality of graphical models and tensor network. graphical models are probability distributions which factor according to the clique structure of a graph The joint probability distribution of several discrete random variables is naturally organized into a tensor. In particular, tensor train graph and HMM graph are duality graphs. 

Consider a time series of dynamics up to $P$-th order,  we can view the time series as a joint distribution over  $P$ variables  $P(X_1, X_2, \cdots, X_P)$.   The covariance of such joint distribution is defined as  $C := \E_{X_1 X_2 \cdots,  X_P}[\phi(X_1)\otimes \phi(X_2), \cdots, \otimes \phi(X_P)]$. \trnn{} estimates the joint distribution by factorizing the  tensor product of the feature mapping functions: $\hat{C} := \sum_{i=1}^m \otimes [\T{A}(X^{(i)}_1), \cdots ,\T{A}(X^{(i)}_P)] $.   Given $m$ i.i.d. samples of times series, $\mathcal{D} = \{{X}_1^{(i)}, {X}_2^{(i)}, \cdots, {X}_P^{(i)} \}_{i=1}^m$, we can then generalize the results from \cite{song2013nonparametric} for multi-view latent variable models to \trnn{}.

% The following Lemma from \cite{song2013nonparametric} provides the concentration bound for the tensor decomposition of empirical operators for three variables: 
% \begin{lemma}[Concentration Bounds for Triples]
% Let $\rho:=\sup\limits_{x\in \T{X}}k(x,x), and \|\cdot\|$ be the Hilbert-Schmidt norm, we have for 
% $\epsilon_{triples} :=  \|C_{X_1 X_2 X_3} - \hat{C}_{X_1X_2X_3}\|$

% \[ Pr\big\{ \epsilon_{triples} \leq \frac{2\sqrt{2} \rho^{3/2}\sqrt{\log \frac{2}{\delta}}}{\sqrt{m}}\big\} \leq 1-\delta \]
% \end{lemma}

% Define $\xi_i = \phi(x_1^i)\cdots \phi(x^i_L) - C$, we have 
% $\sup\limits_{X_1\cdots, X_L}\|\phi(x_1^i)\cdots \phi(x^i_L)\|^2 \leq  \rho^L$, hence $\|\phi(x_1^i)\cdots \phi(x^i_L)\|\leq \rho^{L/2}$ and $\|\xi_i\|\leq 2\rho^{L/2}$.



