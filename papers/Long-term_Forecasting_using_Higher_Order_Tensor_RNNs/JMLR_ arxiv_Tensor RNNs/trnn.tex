\paragraph{Forecasting Nonlinear Dynamics}
%
Our goal is to learn an efficient forecasting model for \ti{continuous multivariate time series} in environments with nonlinear dynamics.
%
The state $\V{x}_t \in \mathR^d$ of such systems evolves over time using a set of \ti{nonlinear} differential equations:
%
\eq{\label{eq:dynamics}
\brckcur{\xi^i\brck{\V{x}_t, \fr{d\V{x}}{dt}, \fr{d^2\V{x}}{dt^2}, \ldots; \phi} = 0 }_i,
}
%
where $\xi^i$ can be an arbitrary (smooth) function of the state $\V{x}_t$ and its derivatives. 
% 
Continuous time dynamics are usually described by differential equations while  difference equations are employed for discrete time. 
% 
In continuous time, a classic example is the first-order Lorenz attractor, whose realizations showcase the ``butterfly-effect'', a characteristic set of double-spiral orbits. 
% 
In discrete-time, a non-trivial example is the 1-dimensional Genz dynamics, whose difference equation is:
%
\eq{\label{eq:genzprodpeak}
	x_{t+1} = \brck{c^{-2} + (x_t + w)^2}^{-1}, \hspace{10pt}  c,w \in [0,1],
}
where $x_t$ denotes the system state at time $t$ and $c,w$ are the parameters. Due to the nonlinear nature of the dynamics, such systems exhibit higher-order correlations, long-term dependencies and sensitivity to error propagation, and thus form a challenging setting for forecasting.
% 
% \ryedit{Add visualization of Genz dynamics here?}

Given a sequence of initial states $\V{x}_0\ldots \V{x}_t$, the forecasting problem aims to learn a dynamics model $F$ that outputs a sequence of future states $\V{x}_{t+1} \ldots \V{x}_T$. 
\eq{\label{eq:forecast}
F: \brck{\V{x}_0\ldots \V{x}_t} \mapsto \brck{\V{y}_{t} \ldots \V{y}_T},
%
\hspace{10pt} \V{y}_t = \V{x}_{t+1},
}
The system is governed by some unknown dynamics. Hence, accurately approximating the dynamics is critical to learning a good forecasting model and making predictions for long time horizons.
 
 
\begin{figure*}[t]
\begin{center}
\begin{minipage}[t]{0.62\linewidth}
\centering		\includegraphics[width=\linewidth]{Figure/tlstm.png}
\caption{\trnn{} within a seq2seq model. Both encoder  and decoder contain higher-order recurrent cells. The augmented state $\V{s}_{t-1}$ (grey) takes in past $L$ hidden states (blue) and forms a higher-order tensor. \trnn{} (red)  factorizes the tensor and outputs the next hidden state.}
\label{fig:seq2seq}
\end{minipage}
\hspace{0.02\linewidth}
\begin{minipage}[t]{0.33\linewidth}
\centering		\includegraphics[width=\linewidth]{Figure/tensor_train.png}
\caption{A \trnn{} cell. The augmented state $\V{s}_{t-1}$ (grey) forms a higher-order tensor, which is then factorized to output the next hidden state.}
\label{fig:ttrnn}
\end{minipage}
\end{center}
\vspace{-5mm}
\end{figure*}

\paragraph{First-order Markovian Models}
%
In deep learning, popular approaches such as recurrent neural networks (RNNs) employ first-order hidden-state models to approximate the dynamics. An RNN with a single  cell recursively  computes a hidden state $\V{h}_t$ using the most recent hidden state $\V{h}_{t-1}$, generating  the output $\V{y}_t$ from the hidden state $\V{h}_t$ :
%
\eq{\label{eq:rnn}
\V{h}_t = f(\V{x}_t, \V{h}_{t-1}; \theta_f),\hspace{10pt} \V{y}_t = g(\V{h}_t; \theta_g),
}
%
where $f$ is the state transition function, $g$ is the output  function and $\{\theta_f, \theta_g\}$ are the corresponding  model parameters. A common parametrization scheme for \refn{eq:rnn} applies a nonlinear  activation function such as sigmoid $\sigma$ to a linear map of $\V{x}_t$ and $\V{h}_{t-1}$ as:
%
\eq{
\V{h}_t &= \sigma(W^{hx} \V{x}_t + W^{hh} \V{h}_{t-1} + \V{b}^h), \quad
\V{x}_{t+1} = W^{xh} \V{h}_t + \V{b}^x,
}
where $W^{hx}, W^{xh}$ and $W^{hh}$ are  the transition weight matrices and $\V{b}^h, \V{b}^x$ are the biases.

RNNs have many different variations, including LSTMs \citep{hochreiter1997long} and GRUs \citep{chung2014empirical}. 
% For instance, LSTM cells use a memory-state, which mitigate the ``exploding gradient'' problem and allow RNNs to propagate information over longer time horizons.
%
Although a RNN can approximate any function in theory, its hidden state $\V{h}_t$  only depends on the previous state $\V{h}_{t-1}$ and the input $\V{x}_t$. Such models do not explicitly capture higher-order dynamics and only  implicitly encode long-term dependencies between all historical states $\V{h}_{0} \ldots \V{h}_{t}$. This limits the representation power of RNNs, especially for forecasting in environments with nonlinear dynamics. Hence, instead of using a wide RNN with many hidden units, we exploit the recurrent cell to design higher-order tensor RNNs that can approximate complex non-linear governing equations. 


% \paragraph{The Debate Between Deep and Shallow}
% While both the deep and shallow networks preserve the universal approximation property, the folk wisdom is that shallow networks memorize well but generalize poorly. Theoretically, \cite{mhaskar2017and} have shown deep networks to have lower number of sample complexity. Empirically, hierarchical architectures such as residual networks \cite{he2016deep} and dense networks \cite{huang2017densely} are quite successful. 

% We take an analogous approach in our design principle but focus on the temporal dimension. We study the universal approximation property of RNNs for representing the underlying dynamics. 

\subsection{Higher-Order Non-Markovian Models}

To effectively learn nonlinear dynamics with higher-order temporal dependency, we propose a family of models that generalizes standard RNNs: higher-order recurrent neural networks, or  \trnn{}. 
%
We design \trnn{}s with two goals in mind: explicitly modeling 1) $L$-order Markov processes with $L$ steps of temporal memory and 2) polynomial interactions between the hidden states $\V{h}_{\cdot}$ and $\V{x}_t$.

First, we consider longer ``history'': we keep length $L$ historic states: $\V{h}_{t},\cdots, \V{h}_{t-L}$:
%
\eq{
\V{h}_t = f( \V{x}_t , \V{h}_{t-1}, \cdots, \V{h}_{t-L}; \theta_f)
\label{eqn:high_order_markov}
}
%
where $f$ represents the state transition function.  In principle, early work \citep{giles1989higher} has shown that with a large enough hidden state size, such recurrent structures are capable of approximating any dynamical system.

Second, to learn the nonlinear dynamics $\xi$ efficiently, we also use higher-order moments to approximate the state transition function.
%
We use an augmented state $\V{s}$, where we mute the subscript of $\V{s}_{t-1}$ for notation simplicity.:
\begin{equation}
	% \V{s}_{t-1} \otimes \cdots \otimes \V{s}_{t-1} \quad
	\V{s}^T = [1 \hspace{5pt} \V{h}_{t-1}^\top \hspace{5pt} \ldots \hspace{5pt} \V{h}_{t-L}^\top ]
\end{equation}
which concatenates $L$ previous hidden states.
% 
To compute $\V{h}_t$, we construct a $P$-dimensional transition \ti{weight tensor} to model degree-$P$ polynomial interactions between hidden states:
%Hence, the \trnn{} with standard RNN cell is defined by:
%
\begin{align}
[\V{h}_{t}]_\alpha = \phi(W^{hx}_\alpha\V{x}_t+  
    \sum_{i_1,\cdots, i_p}\T{W}_{\alpha i_1 \cdots i_{P}}  \underbrace{\V{s}_{i_1} \otimes\cdots\otimes \V{s}_{i_p} }_{P} )\nonumber
%
\label{eqn:tensor_rnn}
\end{align}
%
where $\alpha$ indices the hidden dimension, $i_\cdot$ indices the higher-order terms and $P$ is the total  polynomial order. We included the bias unit $1$ in $\V{s}$ to account for the first order term, so that  $\V{s}_{i_1} \otimes\cdots\otimes \V{s}_{i_p} = [1, \V{h}_t, \V{h}_t\V{h}_{t-1},\cdots]$ can include all  polynomial expansions of hidden states up to order $P$. 

%
% where $\otimes$ is the tensor product.
%
%
The \trnn{} with LSTM cell, or ``\tlstm{}'', is defined analogously as:
\begin{align}
[\V{i}_t, \V{g}_t, &\V{f}_t, \V{o}_t]_\alpha = \sigma (W^{hx}_\alpha \V{x}_t + \sum_{i_1,\cdots, i_p}\T{W}_{\alpha i_1 \cdots i_{P}}  \underbrace{\V{s}_{i_1} \otimes\cdots\otimes \V{s}_{i_P} }_{P} ), \\
%
& \V{c}_t = \V{c}_{t-1} \circ \V{f}_t +  \V{i}_t\circ \V{g}_t,
%
\qquad
%
\V{h}_t = \V{c}_t \circ \V{o}_t \nonumber
%
\end{align}
%
where $\circ$ denotes the Hadamard product. Note that the bias units are again included.

 \trnn{} is a basic  unit that can be incorporated in most of the existing recurrent neural architectures such as convolutional RNN \citep{xingjian2015convolutional} and hierarchical RNN \citep{chung2016hierarchical}. In this work, we use  \trnn{} as a module for sequence-to-sequence (seq2seq) framework \citep{sutskever2014sequence} in order to perform long-term forecasting.


As shown in Figure \ref{fig:seq2seq}, seq2seq models consist of an encoder-decoder pair. The encoder takes an input sequence and learns a hidden representation. The decoder initializes with this hidden representation and generates an output sequence. Both the encoder and the decoder contain multiple layers of higher-order tensor recurrent cells (red).
% 
The augmented state $\V{s}_{t-1}$ (grey) concatenates the past $L$ hidden states;
% 
the \trnn{} cell takes $\V{s}_{t-1}$ and outputs the next hidden state. 
% 
The encoder encodes the initial states $x_{0}, \ldots, x_{t}$ and the decoder predicts $x_{t+1}, \ldots, x_{T}$. 
% 
For each time step $t$, the decoder uses its previous prediction $\V{y}_t$ as an input.
%
\subsection{Dimension Reduction with Tensor-Train}
% 
Unfortunately, due to the ``curse of dimensionality'', the number of parameters in $\T{W}_\alpha$ with hidden size $H$ grows exponentially as $O(HL^P)$, which makes the higher-order model prohibitively large to train. To overcome this difficulty, we  utilize   \textit{tensor networks} to approximate the weight tensor. Such networks encode a structural decomposition of tensors into low-dimensional components and have been shown to provide the most general approximation to smooth tensors \citep{orus2014practical}.
%
The most commonly used tensor networks are \textit{linear tensor networks} (LTN), also known as \textit{tensor-trains} in numerical analysis or \textit{matrix-product states} in quantum physics \citep{oseledets2011tensor}.

A tensor train model decomposes a $P$-dimensional tensor $\T{W}$ into a network of sparsely connected low-dimensional tensors $\{\T{A}^p \in \R^{r_{p-1} \times n_p \times r_{p}} \}$ as:
%
\begin{equation*}
\T{W}_{i_1 \cdots i_P} =
\sum_{\alpha_1 \cdots \alpha_{P-1}}
\T{A}^1_{\alpha_0 i_1 \alpha_1}%
\T{A}^2_{\alpha_1 i_2 \alpha_2}%
\cdots%
\T{A}^P_{\alpha_{P-1} i_P \alpha_P}
\nonumber
\end{equation*}
%
 with $ \alpha_0 = \alpha_P = 1$, as depicted in Figure (\ref{fig:ttrnn}). When $r_0 = r_{P} = 1$ the $\{r_p\}$ are called the tensor-train rank.
%
With tensor-train decomposition, we can reduce the number of parameters of \trnn{} from $(HL+1)^{P}$ to $(HL+1)R^2P$, with $R = \max_p{r_p}$ as the upper bound on the tensor-train rank.
%
Thus, a major benefit of tensor-train is that they \textit{do not} suffer from the curse of dimensionality, which is in sharp contrast to many classical tensor decomposition models, such as the Tucker decomposition.

%\aacomment{theory should be a separate section}


