\section{Generalisation gap - Experiments}
\label{sec:results}


In this section we present our analysis of the generalisation gap, and detail a series of experiments we argue are indicative of the aggravation of the phenomenon in continual learning. We ask a couple of questions and provide empirical evidence to answer them, such as: How much pretraining is too much? Is this negative effect additive when pretraining consists of several stages? Is there a way to leverage the pretrained parameters for faster tuning? 

We start with the same setup as in \cite{ash2019difficulty} training deep residual networks \cite{He_2016_CVPR} to classify the CIFAR 10 data set. We use the average test accuracy in the last 100 training epochs (see the green box in Figure~\ref{fig:demo}) to compare different setups. We mention below the relevant details for each experiment, and we offer a full description of the empirical setup in Appendix~\ref{sec:cifar}.


\paragraph{Does the optimization algorithm affect the gap?} Different optimizers have particular advantages in escaping sub-optimal regions. We reproduced the warm start experiment for a couple of different optimisers using constant learning rates: Adam, RMSprop, SGD, and SGD with momentum (Figures~\ref{fig:adam},~\ref{fig:rmsprop},~\ref{fig:sgd},~and~\ref{fig:msgd}). \citeauthor{ash2019difficulty} reported similar results. The fact that the generalisation gap manifests in all cases supports the observation that it is rather a problem with the quality of the local minima %around the pretrained parameters 
than one of finding appropriate descending trajectories.

\begin{figure}[h!tb]
    \centering%
    \includegraphics[width=\linewidth]{Adam_.001.pdf}
    \caption{Average test accuracy in the last 100 epochs of tuning after pretraining the model for different numbers of epochs using Adam with a constant learning rate ($10^{-3}$) for both phases. }
    \label{fig:adam}
\end{figure}


% -- warm-up steps

\paragraph{How many pretraining steps are needed to produce a generalisation gap?} We investigated how much does the gap depend on a large number of optimisation steps in the pretraining stage. Would early stopping close the gap? In our experiments, although a larger number of pretraining steps hurt generalisation more in the tuning stage, just a few passes through the data are enough to observe a gap (5-10 for Adam, which is even before reaching 100\% accuracy). In conclusion the gap is there before any reason to do early stopping. This is consistent across all considered optimisers (Figures~\ref{fig:adam},~\ref{fig:rmsprop},~\ref{fig:sgd},~and~\ref{fig:msgd}). A similar experiment was reported in \cite{ash2019difficulty}.

\paragraph{Is the gap still there when data distribution slides smoothly?}
We tested whether a smooth transition from the pretraining subset to the full training set would remove the generalisation gap. But, as Figure~\ref{fig:blending} shows, the generalisation gap manifests even for a small number of epochs with biased sampling. This might have profound implications in reinforcement learning where the data distribution changes slowly during training, as the policy collecting the data changes.


\begin{figure}[h!tb]
    \centering%
    \includegraphics[width=\linewidth]{blending.pdf}
    \caption{Models trained in a single stage where each example is individually sampled with probability $p=1-\gamma^{50 n/N}$ from the full training data, and with probability $1-p$ from the pretrain set ($n$ is the current step, while $N$ represents the total number of steps -- the equivalent of 500 epochs). A few more details in Section~\ref{sec:blending}.}
    \label{fig:blending}
\end{figure}

Concluding from the last two sets of experiments, a transient bias in the data distribution significantly impacts generalisation performance.


\paragraph{Do multiple pretrain stages and/or class ordering matter?} Continual learning is concerned with possibly unlimited changes in the data distribution. It is natural to ask whether the loss in generalisation performance observed as a consequence of a single pretraining stage is aggravated when data is incrementally added in more steps. In order to answer this question we divided the data set in multiple splits training the model in stages. We show in Figure~\ref{fig:stages} that the final generalisation performance (the test accuracy achieved in the last stage training on the full training set) degrades with the number of splits.

To get even closer to the usual continual learning setup, we considered splits of the training set having some level of class imbalance, therefore exhibiting larger differences between the data distributions considered at consecutive stages (Figure~\ref{fig:stages}, right). We tested for splits ranging from class partitions where each stage would bring data from one or more new classes to the uniform subsampling from the training set considered so far (see Section~\ref{sec:splits} for the detailed methodology used to split the data set). We noticed that higher discrepancies between training stages lead to worse generalisation.

\begin{figure}[h!tb]
    \centering%
    \includegraphics[width=\linewidth]{stages.pdf}
    \caption{The same model (ResNet18) was trained in multiple stages. All but the last pretrain stages consisted of a number of steps proportional with the number of examples and sufficient to reach 100\% accuracy on train. Right: New data for a particular stage has a ratio of examples drawn uniformly from the training set, and the rest from classes designated for that stage (see Section~\ref{sec:splits} for details).}
    \label{fig:stages}
\end{figure}


We argue that this observations point out a core difficulty of continual learning. When saving data from the past is feasible, retraining models seems a better strategy than using pretrained models.

\begin{figure}[h!tb]
    \centering%
    \includegraphics[width=\linewidth]{architectures.pdf}
    \caption{Average performance on the test set for residual networks of various depths and widths. See Section~\ref{sec:resnets} for details on the models' architectures.}
    \label{fig:architectures}
\end{figure}

\paragraph{How do model width and/or the depth change the generalisation gap?}
We investigated whether increasing the capacity of the model helps recovering the generalisation performance of a randomly initialised model. We show in Figure~\ref{fig:architectures} that even for very deep and wide models there is a significant gap between pretrained models and those trained from random initialisations.
\begin{figure}[h!tb]
    \centering%
    \includegraphics[width=\linewidth]{resetting_350.pdf}
    \caption{Performance of models for which a subset of layers were reset after pretraining. 1 represents the first convolution, 2-5 are the four modules, and 6 is the fully connected output layer.}
    \label{fig:reset}
\end{figure}
\paragraph{Which pretrained parameters should be kept for tuning not to have a gap?} Knowing that tuning the whole model leads to poor generalisation performance we ask what the best strategy is for taking advantage from the pretrained model?
We conducted a series of experiments in which we re-sample the parameters of some layers from the same distribution used at initialisation. In our tests with ResNet-18 models on CIFAR 10, resetting just a small subset of the layers is not enough to fully recover the gap. Our experiments, summarised in Figure~\ref{fig:reset}, indicate that in order to close the gap the top part of the model must be reinitialised.
Therefore it might be advantageous to keep the first $k$ layers (as it is usually done in transfer learning), but in our experiments, $k$ is quite small. Moreover, it seems that there is no advantage in terms of training speed to keep the pretrained layers (see Section~\ref{sec:reset_details} for details.)