\section{A possible account for the generalisation gap: Two Phases of Learning}
\label{sec:two_phases}

One plausible hypothesis for the occurrence of the generalisation gap stems from the flat versus sharp minima view on generalisation \cite{hochreiter1997flat}. Precisely, local minima which exhibit low curvature, and wide basins of attraction generalise better than sharp ones. This could be motivated from an information theoretic perspective: flat minima require less precision to be described (the minimum description length argument made by \cite{hochreiter1997flat}); or by thinking about the stability around that point: flat minima are affected less by perturbations in the inputs. Although there is no formal definition for flatness, previous works proposed quantities such as the largest eigenvalue of the Hessian \cite{keskar2017on}, or the local entropy \cite{chaudhari2019entropy} to gauge it. 

If optimisation were to follow the gradient flow (the infinitesimally precise path determined by the gradient), then the minimum it converges to would be determined by the initial random initialisation. In practice optimisation diverges from that path. This is due to the noise induced by the randomness in the mini-batch approximation of the loss function, and by the amplitude of the update step. As a consequence the training dynamics allegedly traverse two phases.
In the early \textit{exploration} phase, the parameters ``bounce'' form the vicinity of a critical point to another until they land in the basin of attraction of a minimum that is wide enough to trap the optimisation.

Once the parameters get stuck in the basin of attraction of some minumum, training goes into a \textit{refinement} phase,  where parameters converge to the said critical point. In this phase optimisation follows the gradient flow. 

Several works bring supporting evidence for the \emph{two phases of learning} hypothesis. \cite{achille2018critical} also identifies two phases by analysing the information stored in the weights. Their observations are consistent with the sharp versus flat minima view as the the Fisher Information Matrix used to measure connectivity is also indicative about curvature. \cite{golatkar2019time} reveal that regularisation has an impact only in the initial phase, while \cite{gur2018gradient} show that after an early regime gradients reside in a small subspace that remains constant during training. More relevant works are mention in Section~\ref{sec:twophase_literature}.

Building on this hypothesis, it is natural to ask whether the generalisation gap can be explained by how pretraining affects the \textit{exploration} phase of learning. 

Note that the amount of exploration that learning is able to do in this initial stage is proportional to several factors, among which the more important ones are the learning rate and the inherent noise in the updates. The role of the learning rate is self-explanatory, it can be seen as scaling the amount of noise. 
%
As for the noise itself, there are multiple sources: the inherent noise in the data (labelling imprecisions, noise in the observations, irrelevant features), noise induced by the optimiser of choice (e.g. SGD introduces noise by relying on mini-batches), noise induced by the data augmentation procedure, etc.
%
The noise profile of gradients and parameters updates is important, and known to be non-Gaussian \cite{simsekli2019tail}, hence it is harder to replicate in practice.

We make the conjecture that, \emph{given that everything else stays the same, the pretraining stage can considerably reduce the amount of noise in the gradients during the tuning stage, which leads to weaker exploration and convergence to narrower minima, inducing the generalization gap.}

In particular, in the case of discriminative learning which has been the focus of this work, estimators tend to quickly become robust to many directions of variation in the data which are not relevant for the classification task. For example, the model starts ignoring non-discriminative features such as background patterns early in training. In fact this property is of great importance to the recent success of neural networks. Many architectural advances are more efficient in doing so leading to more robust models with better generalisation properties (e.g. the translation-invariant convolutions compared with fully-connected linear layers). Data augmentation plays a similar role.


However, these irrelevant features do potentially play a role in the initial stage of learning as a source of noise, forcing the optimisation process to focus on wider minima. When the model is pretrained, it becomes insensitive to some of these irrelevant features (or some easy to discern sources of noise). While it is true that when moving to the tuning stage the problem changes --- and hence the loss surface is different and there is likely no relationship between the two loss surfaces and their critical points (e.g. this is the case in a continual learning setting) --- the model will still be insensitive to some direction of change (either in the input space or in the latent space). Even if those direction of variations are not relevant for the new task, it does mean that in the early stage of tuning there will be considerably less noise, and hence potentially less exploration. This means that optimisation in the tuning stage will converge to a narrower minimum, which will generalise less, leading to the generalization gap. 

To test this hypothesis we check whether increasing the learning rate -- which would magnify the remaining  noise in the parameter updates-- during the tuning stage helps.  Figure~\ref{fig:lr_grid} shows that a 10x larger learning rate reduces the gap substantially. The rest of the performance gap could be explained by the fact that a high constant learning rate does not benefit from the \emph{refinement} phase. While further empirical evidence is required to validate this hypothesis, this result is encouraging. Under the assumption that this is the cause for the gap, one question to be asked is why does the pretraining phase reduce gradient noise for the tuning phase. The answer might rest in two observations: 1) the strong data overlap between the two stages, and 2) neural networks tend to filter early on information in the input that is not discriminative. If the non-discriminative dimensions  are already filtered out by pretraining, the tuning stage might not benefit from the noise they would induce. 


\begin{figure}[h!tb]
    \centering%
    \includegraphics[width=\linewidth]{lr_grid.pdf}
    \caption{In this figure we show the performance reached by tuned models for three specific constant learning rates used during pretraining (the $0.0018$ is the one that generalizes best). Each point is the average of 9 seeds. The horizontal lines represent the performance achieved by randomly initialised models. Therefore each distance between a circle and the horizontal lines represents the gap for a particular pair of learning rates.}
    \label{fig:lr_grid}
\end{figure}


