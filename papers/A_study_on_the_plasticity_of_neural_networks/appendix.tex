\appendix
\section{Experiments on CIFAR-10}
\label{sec:cifar}

We detail below the experimental details for the observations made in Section~\ref{sec:results}. If it's not explicitly mention otherwise, a ResNet-18 model was trained on CIFAR 10 with batches of 128 examples, using an Adam optimiser with a constant learning rate of 0.001. The optimiser's statistics were reset between the warm up and the tuning phase. In the first stage (during warm up) the model was trained for 350 epochs on half of data. In the second stage the model is trained for 500 epochs on all training data. We report here the average test performance over all seeds during the last 100 training epochs. In all plots the error bars measure standard deviation.

Due to space constraints we don't show learning curves, but if it's not otherwise specified it's implied that accuracy is 100\% on the data used in training for both stages as in Figure~\ref{fig:demo}.

\subsection{Different optimisers}

In addition to Figure~\ref{fig:adam} in Section~\ref{sec:results} we show here how generalisation performance varies with the number of pretraining epochs on half of data for three additional optimisers: RMSprop in Figure~\ref{fig:rmsprop}, Stochastic Gradient Descent (SGD) in Figure~\ref{fig:sgd}, and SGD with a constant momentum (0.9) in Figure~\ref{fig:msgd}.

\begin{figure}[h!tb]
    \centering%
    \includegraphics[width=\linewidth]{RMSprop_.001.pdf}
    \caption{Final performance after warming up the model for different numbers of epochs using RMSProp with a constant learning rate for both phases.}
    \label{fig:rmsprop}
\end{figure}

\begin{figure}[h!tb]
    \centering%
    \includegraphics[width=\linewidth]{SGD_.001.pdf}
    \caption{Final performance after warming up the model for different numbers of epochs using SGD with a constant learning rate for both phases.}
    \label{fig:sgd}
\end{figure}

\begin{figure}[h!tb]
    \centering%
    \includegraphics[width=\linewidth]{mSGD_.001_.9.pdf}
    \caption{Final performance after warming up the model for different numbers of epochs using SGD with a constant learning rate and momentum for both phases.}
    \label{fig:msgd}
\end{figure}

\subsection{Smooth transition between distributions.}
\label{sec:blending}

In the experiments presented in Figure~\ref{fig:blending} we trained models in a single stage of 500 epochs.
In this case we called an epoch a sequence of 390 update steps (which is the equivalent of 1 pass through the training data with a batch size of 128). Note that such an epoch is not a permutation of the data. Each example from each batch is individually sampled with probability $p$ from the full training set, and with probability $1-p$ from the pretrainig set.

\subsection{Class imbalance in the multiple stage setup}
\label{sec:splits}

Given (i) a training set $\mathcal{D}$ with examples from a set of classes $\mathcal{C}$, (ii) a number of pretraining stages $n$ and (iii) a number $0 \le r \le 1$ (the ratio of data from all classes) we constructed $n$ subsets of $\mathcal{D}$: $\left\lbrace\mathcal{D}_i\right\rbrace_{1 \le i \le n}$ to be used for optimisation during the $n$ pretraining stages. In doing this we applied the following methodology:
\begin{enumerate}
    \item We created a partition of the all classes: $\left\lbrace \mathcal{C}_1, \ldots \mathcal{C}_{n + 1} \right\rbrace$ such that $\mathcal{C}_i \cap \mathcal{C}_j = \emptyset, \forall i, j$, and $\mathcal{C} = \bigcup_{i=1}^{n+1} \mathcal{C}_{i}$.
    \item We randomly split the full training set $\mathcal{D}$ in two: $\mathcal{D}_{c}$, $\mathcal{D}_{u}$ such that $\frac{\vert\mathcal{D}_{u}\vert}{\vert\mathcal{D}\vert} = r$. (of course: $\mathcal{D}_{u} \cap \mathcal{D}_{c} = \emptyset$, and $\mathcal{D}_{u} \cup \mathcal{D}_{c} = \mathcal{D}$).
    \item We partition $\mathcal{D}_{u}$ into $n+1$ subsets: $\left\lbrace D_{u,1}, \ldots \mathcal{D}_{u, n+1} \right\rbrace$.
    \item We now define the data sets used to optimise the model in each stage (and considering $\mathcal{D}_{0}=\emptyset$):
    $$ \mathcal{D}_i = \mathcal{D}_{i-1} \cup \mathcal{D}_{u, i} \cup \left\lbrace \left(x, c\right) \in \mathcal{D}_{c} \mid c \in \mathcal{C}_i \right\rbrace$$
\end{enumerate}

The $n+1$-th dataset $\mathcal{D}_{n+1} \equiv \mathcal{D}$ corresponds to the final tuning phase on the full data set.

\subsection{Residual networks of various depths and widths}
\label{sec:resnets}

In the experiment with residual neural networks of different widths and depths we changed the architecture of ResNet-18 \cite{He_2016_CVPR} as follows.

Apart from the first convolution and the fully connected layer at the output, ResNet-18 consists of four modules, each made up of $d=2$ residual blocks with the same number of output channels. Each module doubles the number of channels and halves the height and the width of the feature maps. The first module receives a volume with $w=64$ channels, the second operates on $2w=128$, and so on.

In our experiments we uniformly changed the depth $d$ of the four modules, and/or scaled the number of channels in all modules ($w, 2w, 4w, 8w$).

Note that this is not the standard way in which people design deeper residual architectures such as ResNet-32, or ResNet-55. Deeper ResNets increase the depth of the modules differently, and use bottleneck blocks to avoid an explosion in the number of parameters.

\subsection{Resetting the layers of the model}
\label{sec:reset_details}

In the experiments presented in Figure~\ref{fig:reset} from Section~\ref{sec:results} we reset subsets of the model's parameters. We reset entire modules referring with 1 to the first convolution, with numbers from 2 to 5 to the four modules (each consisting of 2 residual blocks), and naming 6 the last fully connected layer.

As Figure~\ref{fig:reset} shows, resetting the last 4, or 5 modules seems to recover the original performance of a model trained from random parameters. Therefore we asked whether keeping the pretrained parameters of the first 1 or 2 modules comes with any advantage in terms of training speed. As Figure~\ref{fig:advantage350} shows, in our setup there seems to be no benefit from preserving parameters from the pretraining stage.


\begin{figure}[h!tb]
    \centering%
    \includegraphics[width=\linewidth]{reset_advantage_350.pdf}
    \caption{Here we show the learning curves for three models pretrained for 350 epochs on half of data. For two of them we keep the first 1 or 2 modules, reinitialise the rest and tune for 500 epochs.}
    \label{fig:advantage350}
\end{figure}





\input{literature_review_for_two_phases}