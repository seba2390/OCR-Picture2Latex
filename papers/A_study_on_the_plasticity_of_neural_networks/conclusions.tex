\section{Conclusions}
\label{sec:future}

We build on~\cite{ash2019difficulty} and study the generalisation gap induced by pretraining the model on the same data distribution. We extend the original results, by looking at robustness of this gap to smooth transition between data distributions, multiple stages of pretraining, model size or resetting parts of the pretrained model. We take a first step towards understanding this phenomenon by asking whether it is related to the two phases of learning hypothesis. 

The existence of this generalisation gap suggest that continual learning might be hurt by using compact models that get finetuned on multiple tasks. We argue that tracking the generalisation gap represents a new facet of forward transfer that has not generally been measured or tracked in the literature.
