\section{Introduction}
\label{sec:intro}

Continual learning is concerned with training on non-stationary data. In a practical description, an agent learns a sequence of tasks, being restricted to interact with only one at a time. There are several desiderata for a successful continual learning algorithm. First, agents should not forget previously acquired knowledge, unless capacity becomes an issue or contradicting facts arrive. Second, such an algorithm should be able to exploit structural similarity between tasks and exhibit accelerated learning. Third, backward transfer should be possible whenever new knowledge helps generalisation on previously learnt tasks. Fourth, successful continual learning relies on an enduring capacity to acquire new knowledge, therefore learning now should not impede performance on future tasks.


In this work we focus on \textit{plasticity}, namely the ability of the model to keep \textit{learning}. There are different nuances of \textit{not being able to learn}. A neural network might lose the capacity to minimise the training loss for a new task. For example, PackNet~\citep{PackNet} eventually gets to a point where all neurons are frozen and learning is not possible anymore. In the same fashion, accumulating constraints in EWC~\citep{EWC} might lead to a strongly regularised objective that does not allow for the new task's loss to be minimised. Alternatively, learning might become less data efficient, referred to as \textit{negative forward transfer}, an effect often noticed for regularisation based continual learning approaches. In such a situation one might still be able to reduce training error to $0$ and obtain full performance on the new task, is just learning is considerably slower. Lastly, a third meaning and the one we are concerned with, is that while training error can be reduced to zero, and irrespective to how fast the model learns, the optimisation might lead to a poor minimum which achieves lower generalisation performance. \footnote{\emph{Oct 2023} -- The authors acknowledge, and this paragraph was trying to highlight that \emph{loss of plasticity} as a term has been used to describe two different phenomena. On one hand we have the phenomenon studied in this work, where the issue is not the inability of the neural network to reduce error on training set, but rathter inability to generalize. I.e. the warm-started model will give you worse generalization even if the training error can be reduced equally well as the freshly initialized model. The other form of the problem (e.g. studied in \cite{dahore21}) talks about inability of the system to optimize. While the underlying causes could be similar, this is unlikely, and until shown to be the same problem we acknowledge that they should be treated as separate problems (as per the conclusion the authors had with Aaron Courville). In terms of the name, it is not clear what the correct answer is. Partially the problem is that the concept of plasticity comes from neuroscience, and attributed to inability to learn in machine learning, which is an ambigous term. Learning as a concept relies on optimization, but requires ability to generalize too.  While we do not know how to resolve the name conflict, we want to make the reader aware of it.    }
We define as the \textit{generalisation gap} the difference in performance a pretrained model can obtain --- e.g. one that had learnt a few tasks already --- versus a freshly initialised one, without constraining the number of updates. Note that this is similar to the notion of intransigence proposed by \cite{Chaudhry2018RiemannianWF}, but instead making the comparison against a model trained only on the new data, rather than a multi-task solution. Our focus is on understanding if a \textit{generalisation gap} exists, whether it is positive or negative. 
%
The \textit{transfer learning} dogma used to indicate a positive gap, arguing that pretraining on large sets of data provides a good initialisation for a related target task.
Recently, \cite{he2019rethinking} showed that this does not necessarily hold,
reporting state-of-the-art results with randomly initialised models, although 
at worse sample complexity.
\cite{ash2019difficulty} considered an extreme transfer scenario, where an agent is pretrained on data from the same distribution as the target task, and reported a negative generalisation gap. See Figure~\ref{fig:demo} for our reproduction of this finding. We build on this result in this work, trying to further expand the empirical evidence on which factor affect the generalisation gap and take a first step towards understanding its root causes.



\citeauthor{igl2021transient} extend the observation that data non-stationarity affects asymptotic generalisation performance to reinforcement learning scenarios. Although both \citeauthor{ash2019difficulty}, and \citeauthor{igl2021transient} propose solutions to close the generalisation gap, the reasons for its occurrence in the first place are still unclear.
We argue that it can have a considerable impact on how we approach continual learning, and one should track to what extend it affects the algorithms we have.


\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{demo_plot.pdf}
    \caption{Our reproduction of the core experiment performed by \cite{ash2019difficulty}. A ResNet-18 model is pretrained on half of the CIFAR 10 training data, and then tuned on the full training set. It generalises worse than the model trained from scratch.}
    \label{fig:demo}
\end{figure}

