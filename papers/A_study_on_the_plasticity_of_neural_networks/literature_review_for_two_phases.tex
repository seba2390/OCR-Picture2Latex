\section{Supporting evidence for the two phases of learning hyothesis}
\label{sec:twophase_literature}

A couple of works identify critical differences between the early stage and the late stage of training, offering empirical evidence for the two phases of learning hypothesis.

\cite{achille2018critical} identifies an initial \textit{memorisation} phase when data information is absorbed into the network's weights, followed by a \textit{reorganisation} stage where unimportant connections are pruned and information decreases while being redistributed among layers for efficiency. \citeauthor{achille2018critical} used the Fisher Information Matrix to approximate the amount of information stored in the weights. The FIM is also a curvature matrix, therefore the observed regimes support the view that learning changes basins of attraction of different minima until it lands in one with low curvature, corresponding to a flat minimum. \citeauthor{achille2018critical} also point out that if data statistics change after the initial phase, the network would remain trapped in the valley the memorisation phase guided it into.

\cite{golatkar2019time} empirically shows that regularisation has an impact on final generalisation performance only in the early stages of training. Applying weight decay or data augmentation only after this initial phase, or stopping regularisation after that point would not affect generalisation. The experiments using data augmentation later in training offer additional evidence for the generalisation gap -- if one thinks about that stage as tuning on more data from the same distribution.

\cite{gur2018gradient} shows that after an early training stage the gradients reside in a small subspace that remains constant for the rest of training. This reiterates the importance of the data used in the first steps of training.

\cite{li2019towards} shows that in overparametrized networks the volume of good minima dominates the volume of poor minima and underlines the importance of a high learning rate to land in the basin of attraction of a well generalising minimum of the loss function. \cite{jastrzebski2020break} extends the observation with the importance of using batches to induce the noise needed to escape poorly generalising minima. Precisely, \citeauthor{jastrzebski2020break} point out that the ratio between learning rate and batch size determines the flatness of the minumum.

\cite{ghorbani2019investigation} computes the full spectrum of the Hessian showing that \citeauthor{jastrzebski2020break}'s claims about smaller learning rates guiding the network into sharper minima doesn't hold empirically. A possible explanation is that the network is already trapped around some minima, and the slow learning rate just reaches an even flatter region closer to the critical point.