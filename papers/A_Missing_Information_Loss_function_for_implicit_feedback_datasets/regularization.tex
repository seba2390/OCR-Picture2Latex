\subsection{Regularization techniques}\label{subsec:model_regularizations}
In the following, we detail some regularization techniques that are commonly applied to AE, and discuss its usage for RS.
\paragraph{Weight--decay} It is perhaps the most widely used form of regularization in RS. 
% definition of L2
It consists of adding to the loss a Lagrange multiplier with the sum of the Frobenius norm of weight matrices, as biases are typically not regularized,
\begin{equation}\label{eq:L2_reg}
\mathcal{L} = \mathcal{L}_{\rm point/pair}+\frac{1}{2}\lambda\left( ||{\bf W}||_2^2+||{\bf W'}||_2^2\right).
\end{equation}
Here, $\lambda$ is a hyper-parameter. 
The minimization of equation~(\ref{eq:L2_reg}) will tend to make the second term as small as possible~\cite{Goodfellow-et-al-2016}. 
% drawbacks
Nevertheless, there are some drawbacks associated with this type of regularization: (a) the $\lambda$ parameter depends on the natural minimum value of the loss $\mathcal{L}_{\rm point/pair}$, which varies among objective functions and problem types; and (b) it is typically applied with a single hyper-parameter for all weight matrices, even though the scale of each trainable variable might be quite different. This is most critical in matrix factorization algorithms, where the user matrix is defined in $\mathbb{R}^{|\mathcal{U}|\times D}$, while the one for items belongs to $\mathbb{R}^{|\mathcal{I}|\times D}$. Since $|\mathcal{U}|\gg\ |\mathcal{I}|$ in most RS, the user weight matrix is more regularized than its item counterpart. Nevertheless, since we only use the AE architecture along this paper, point (b) is of no concern here. Regarding the variance of $\lambda$ in equation~(\ref{eq:L2_reg}), we will show some examples in section~\ref{sec:results}. 

\paragraph{Max norm} 
% historical perspective
This form of regularization is fairly widespread in the neural networks community, 
after it was discussed by the authors of drop-out in the same paper~\cite{Srivastava:2014:dropout}.
However, it was first introduced in the context of matrix factorization~\cite{Srebro:2005:max_norm}. 
Surprisingly, it is not as adopted in the RS community as it is in the neural networks community.
%max norm definition
Max norm regularization consists in clipping the norm of every item embedding as well as its decoding
(${\bf W}_{:,i}$ and ${\bf W'}_{i,:}$ for $i=1,\ldots , |\mathcal{I}|$) in each training step.
Said clipping ensures that its norm never exceeds a given threshold, while not changing the gradient direction.
In order to apply this regularization with the same strength to all trainable variables, we use the following implementation,
\begin{equation}\label{eq:max_norm_implementation}
{\bf x} = {\bf x}\frac{n}{||{\bf x}||_2}\,\,\, {\rm if}\, ||{\bf x}||_2>n,~~~n^2 = \alpha^2 {\rm dim}({\bf x}),
\end{equation}
with $\alpha$ a hyper-parameter and ${\bf x}\in \{ {\bf W}_{:,i}$,${\bf W'}_{i,:} \}$, for each $i=1,\ldots , |\mathcal{I}|$. In this paper we do not regularize the biases, and use independent regularization strengths for the encoder/decoder, $\alpha_{\rm enc/dec}$.

\paragraph{Noise}
Training neural networks with noise injection is another regularization technique commonly employed within the neural networks community~\cite{Goodfellow-et-al-2016}.
By applying noise, we turn off a subset of neurons in a network, namely, we increase the size of the training set adding to it corrupted examples.
% DAE
When applied to AEs~\cite{Vincent:2008:ECRF-AE}, noise injection prevents learning the identity function as the internal representation of the input.
Instead, it forces a more robust learning procedure that is less sensitive to data fluctuations. Denoising AEs (DAE) are becoming ubiquitous in RS literature, see e.g.~\cite{Wu:2016:CDAE-topN, liang:2018:VAE, Li:2015:DCF_marginalizedAE}.


