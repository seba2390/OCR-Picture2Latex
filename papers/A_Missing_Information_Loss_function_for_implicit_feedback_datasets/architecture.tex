\subsection{Architecture details}\label{subsec:architecture}
We represent an  item $i$ is as a one-hot encoding ${\bf v}_i$, i.e. a $|\mathcal{I}|$-sized vector of zeros with a 
$1$ at position $i$. 
% user vector
Next, we represent a user $u$ as the sum of the one-hot encoding vectors of items in $\mathcal{I}_u$,
\begin{equation}\label{eq:user_input}
{\bf v}_u: =\sum_{i\in\mathcal{I}_u}{\bf v}_i = \sum_{i\in\mathcal{I}}p_{ui}{\bf v}_i,
\end{equation}
where $p_{ui}$ is the preference of user $u$ for item $i$, i.e. $p_{ui}=1$ if $i\in\mathcal{I}_u$, $p_{ui}=0$ otherwise.
The vector of preferences is therefore equal to the user vector in our setting, ${\bf p}_u\equiv {\bf v}_u$.
In the general case of non binary implicit ratings $r_{ui}$ (such as purchases or play counts) 
the user vector will consist of the weighted sum of the one-hot encoding of their items,
\begin{equation}\label{eq:user_input_weighted}
{\bf v}_u: =\sum_{i\in\mathcal{I}_u}r_{ui}{\bf v}_i.
\end{equation}

% intro autoencoder
An Autoencoder (AE) \cite{Kramer:1991:NLPCA} is a feedforward neural network for learning a representation 
of the input data. This representation is trained to produce an output that closely matches the original input. 
When applied to RS, the encoder typically has  a much lower dimension than the input vector; 
hence, the learned representation has to encode input information while reducing the dimensionality of the input space. 
% latent space
In this paper we consider a single hidden layer AE.
The input vector of preferences, ${\bf p}_u\in\mathbb{R}^{|\mathcal{I}|}$, is projected onto a vector ${\bf h}\in\mathbb{R}^D$,
\begin{equation}\label{eq:AE_hidden}
{\bf h} = s({\bf W}{\bf v}_u+{\bf b}).
\end{equation}
Here ${\bf W}\in\mathbb{R}^{D\times|\mathcal{I}|}$  and ${\bf b}\in\mathbb{R}^D$ are learnable weight matrix and bias vectors, respectively. 
The activation $s(\cdot)$ is an element-wise mapping function; typical activations are the sigmoid function, the hyperbolic tangent or
the Rectified Linear Unit (ReLU)~\cite{icml2010_NairH10_Relu}. %We also consider linear activations, i.e. $s({\bf x})={\bf x}$.
%Several of these encoding layers, eq.~(\ref{eq:AE_hidden}), can be stacked forming a deep structure.

% Output projection
In order to obtain the predicted preferences, the hidden layer is projected back onto the original space, 
\begin{equation}\label{eq:AE_output}
\hat{\bf p}_u = s'({\bf W}'{\bf h}+{\bf b}'),
\end{equation}
where ${\bf W}'\in\mathbb{R}^{|\mathcal{I}|\times D}$  and ${\bf b}'\in\mathbb{R}^{|\mathcal{I}|}$ are weight matrix and bias vectors for the output layer,
and $s'(\cdot)$ is the activation of the decoder (which may or may not be equal to that used when encoding). 
The vector of predicted preferences, $\hat{\bf p}_u$, is then forced to minimize the objective functions defined in subsection~\ref{subsec:losses}. 
% Adding noise
A variant of the AE is the Denoising Autoencoder (DAE) \cite{Vincent:2008:ECRF-AE}, which attempts to
reconstruct a corrupted version of the input $\tilde{{\bf x}}$, i.e. ${\bf h} = s({\bf W}\tilde{{\bf x}}+{\bf b})$. This latter technique has become very popular due to its success in image recognition, and is currently applied in most AEs for RS~\cite{Wu:2016:CDAE-topN, liang:2018:VAE}. 

% Use of sigmoid activation for the decoder
Please note that for MIL and cross-entropy losses (equations~(\ref{eq:mil_def}) and (\ref{eq:cross-entropy}) respectively), output preferences must be bounded $\hat{p}_{ui}\in(0, 1)$. Thus, we typically choose the sigmoid function for the activation of the decoder, $s'=\sigma$. This is different from the also traditional approach of minimizing the logistic log-likelihood~\cite{Wu:2016:CDAE-topN, liang:2018:VAE}, which already incorporates the sigmoid function into the loss, and thus allows one to apply yet another activation at the decoder (e.g. $\tanh(\sigma(\hat{p}_{ui}))$, as in \cite{liang:2018:VAE}, or $\sigma(\sigma(\hat{p}_{ui}))$ in \cite{Wu:2016:CDAE-topN}). 
%As we will show in Section~\ref{sec:results}, this yields to poorer performance. 
\paragraph{Matrix Factorization}

As shown in~\cite{Wu:2016:CDAE-topN}, the AE described in equations (\ref{eq:user_input}), (\ref{eq:AE_hidden}) and (\ref{eq:AE_output}) is a generalization of Matrix Factorization (MF) models. Indeed, MF is recovered after replacing the input user vector (\ref{eq:user_input}) by the one-hot vector of the user id $u$, which is a $|\mathcal{U}|$--sized vector of zeros with a 1 at the position $u$. The bi--linear MF has linear activations $s$ and $s'$, and is typically trained with square loss and weight--decay regularization, see equations~(\ref{eq:square}), (\ref{eq:point-wise}) and (\ref{eq:L2_reg}).

However, due the the large disparity in the number of users and items, $|\mathcal{U}|\ll |\mathcal{I}|$, the norms of the weight matrices ${\bf W}\in\mathbb{R}^{D\times|\mathcal{U}|}$ and ${\bf W'}\in\mathbb{R}^{|\mathcal{I}|\times D}$ are quite different. Thus, direct application of equation~(\ref{eq:L2_reg}) will tend to over-regularized the user matrix ${\bf W}$, leaving the item one ${\bf W'}$ under-regularized; this may lead to instabilities and potential over-fitting while training (as we have observed experimentally). This issue can be solved by re-scaling the norms of weight matrices in equation~(\ref{eq:L2_reg}) as
\begin{equation}\label{eq:l2_reg_scaled}
\mathcal{L} = \mathcal{L}_{\rm point/pair}+\frac{\lambda}{D}\left( 
\frac{||{\bf W}||_2^2}{|\mathcal{U}|}+
 \frac{||{\bf W'}||_2^2}{|\mathcal{I}|}
\right).    
\end{equation}
This way, both user and item embeddings are regularized with the same strength, stabilizing the training procedure. Note: in the case of an AE, there is no need to re-scale the norm of weights matrices, since both have the same number of elements, $|\mathcal{I}|\times D$. 
