\setlength{\belowcaptionskip}{5pt}
\setlength\tabcolsep{3pt}
\begin{table}[hbt]
\begin{tabular}{c c c c c c}
% movielens
\multicolumn{6}{c}{\textsc{ML20M}} \\
\hline
\hline
  & r@1 & r@20 & n@20 & n@100 & nov@100\\
\hline
\MFsquare & 0.191 & 0.329 & 0.231 & 0.318 & 0.305 \\
\MFce & 0.264 & 0.354 & 0.265 & 0.348 & 0.330 \\
\MFmil & 0.261 & 0.335 & 0.252 & 0.336 & 0.318 \\
\hdashline
\CEpointlinsig & 0.284 & 0.372 & 0.281 & 0.366 & 0.348 \\
\CEpointsigsig & 0.281 & 0.379 & 0.285 & 0.371 & 0.353 \\
\CEpairlinsig & 0.244 & 0.358 & 0.262  & 0.349 & 0.333 \\
\CEpairsigsig & 0.171 & 0.360 & 0.249  & 0.337 & 0.320 \\
\MULTItanhlin & 0.249 & 0.364 & 0.269  & 0.357 & 0.339 \\
\MILlinsig & 0.299 & 0.375 & 0.286  & 0.369 & 0.349 \\
\MILsigsig & 0.301 & 0.373 & 0.285  & 0.367 & 0.348 \\
\hline
\end{tabular}

\caption{Relevance and novelty metrics for the \textsc{ML20M} datasets. Here, $\boldsymbol{{\rm r}@k}$ stands for Recall$\boldsymbol{@k}$, $\boldsymbol{{\rm n}@k}$ is the NDCG$\boldsymbol{@k}$, and $\boldsymbol{{\rm nov}@k}$ the novelty-weighted NDCG$\boldsymbol{@k}$.
The horizontal dash line separate MF models from DAE models. Errors in metrics due to random initialization are $\boldsymbol{\pm 3\cdot10^{-3}}$.
}
\label{table:metrics_results_ml20m}
\end{table}

\section{Experimental Results}\label{sec:results}
\subsection{Performance of Metrics}
Tables~\ref{table:metrics_results_ml20m}, \ref{table:metrics_results_netflix} and \ref{table:metrics_results_lastfm} show the performance of models on the \textsc{ML20M}, \textsc{Netflix} and \textsc{Lastfm} datasets, respectively. The results of the MF and DAE models are shown above and below  the dash lines, respectively. 
% MF models
Cross-entropy and MIL objective functions applied to MF clearly outperform the traditional \MFsquare\, model in all datasets, being \MFce\,  the one that provides best results among MF models.
%in the \textsc{ml20m} dataset, and it is paired with \MFmil\, in the other two datasets within the error due to random initialization ($\pm 3\cdot 10^{-3}$). 
Nevertheless, all MF models provide significantly poorer performance than their DAEs counterparts. 

% DAE models
Given the superior performance of DAE over MF models, we focus the rest of the analysis on the former architecture.  
% cross-entropy models
As observed, cross-entropy models in point--wise estimation outperform those in pair--wise estimation to a great extent, in agreement with recent literature~\cite{Steck:2015:GaussianMF, Wu:2016:CDAE-topN, liang:2018:VAE}. Concerning the choice of encoding activations (linear or sigmoid), differences in metric values are in most cases within the error due to random initialization ($\pm 3\cdot 10^{-3}$). However, since \CEpointlinsig\,  outperforms \CEpointsigsig\, in all cases (expect for large top-$k$ ranking in the \textsc{ML20m} dataset), we take \CEpointlinsig\, as the best performing model among those trained with cross-entropy loss (point and pair wise). 

\begin{table}[htb]
\begin{tabular}{c c c c c c}
% netflix
\multicolumn{6}{c}{\textsc{Netflix}} \\
\hline
\hline
  & r@1 & r@20 & n@20 & n@100 & nov@100\\
\hline
\MFsquare & 0.205 & 0.255 & 0.203 & 0.285 & 0.271 \\
\MFce & 0.262 & 0.278 & 0.230 & 0.308 & 0.292 \\
\MFmil & 0.269 & 0.272 & 0.227 & 0.302 & 0.284 \\
\hdashline
\CEpointlinsig & 0.300 & 0.305 & 0.256 & 0.334 & 0.319 \\
\CEpointsigsig & 0.283 & 0.304 & 0.251 & 0.332 & 0.317 \\
\CEpairlinsig & 0.247 & 0.283 & 0.230 & 0.311 & 0.297 \\
\CEpairsigsig & 0.255 & 0.279 & 0.226 & 0.307 & 0.292 \\
\MULTItanhlin & 0.241 & 0.290 & 0.234 & 0.320 & 0.305 \\
\MILlinsig & 0.304 & 0.304 & 0.256 & 0.332 & 0.316 \\
\MILsigsig & 0.304 & 0.299 & 0.252 & 0.328 & 0.311 \\
\hline
\end{tabular}
\caption{Relevance and novelty metrics for the \textsc{Netflix} dataset. See  Table~\ref{table:metrics_results_ml20m} caption for details.
}
\label{table:metrics_results_netflix}
\end{table}

% MIL models
Regarding MIL models, they are in pair (within statistical errors) with cross-entropy models in point wise estimation. There is a tendency for MIL to perform better at low top-$k$, while cross-entropy seems to outperform MIL at large top-$k$ rankings. However, such differences are quite small, and could be easily attributed to other source of systematic errors. With respect to the choice of encoding activations within MIL--based models, as in the case of cross-entropy, linear encoders perform better (without statistical significance, though).
% multinomial
On the other hand, the multinomial log-likelihood does not achieve good performance at low top-$k$. Indeed, the reported metric values at $k\leq20$ are closer to those of pair--wise models than to the best performing models (\CEpointlinsig\, and \MILlinsig). Nonetheless, \MULTItanhlin\, model metric values  recover at large $k$. 

\begin{table}[htb]
\begin{tabular}{c c c c c c}
% lastfm
\multicolumn{6}{c}{\textsc{lastfm}} \\
\hline
\hline
  & r@1 & r@20 & n@20 & n@100 & nov@100\\
\hline
\MFsquare & 0.155 & 0.236 & 0.172 & 0.243 & 0.228 \\
\MFce & 0.175 & 0.258 & 0.191 & 0.265 & 0.251 \\
\MFmil  & 0.163 & 0.242 & 0.178 & 0.250 & 0.236 \\
\hdashline
\CEpointlinsig & 0.228 & 0.300 & 0.229 & 0.305 & 0.289 \\
\CEpointsigsig & 0.218 & 0.293 & 0.222 & 0.299 & 0.283 \\
\CEpairlinsig & 0.153 & 0.277 & 0.193 & 0.275 & 0.261 \\
\CEpairsigsig & 0.184 & 0.279 & 0.204 & 0.284 & 0.270 \\
\MULTItanhlin & 0.187 & 0.287 & 0.210 & 0.289 & 0.275 \\
\MILlinsig & 0.222 & 0.296 & 0.225 & 0.299 & 0.283 \\
\MILsigsig & 0.222 & 0.294 & 0.224 & 0.299 & 0.283 \\
\hline
\end{tabular}
\caption{Relevance and novelty metrics for the \textsc{LAstfm} datasets. See  Table~\ref{table:metrics_results_ml20m} caption for details.
}
\label{table:metrics_results_lastfm}
\end{table}
\setlength\tabcolsep{6pt}


In order to further establish the relative performance of the multinomial loss, we present in Table~\ref{table:metrics_results_vae} the results for the data processing presented in~\cite{liang:2018:VAE} for the \textsc{ML20m} dataset\footnote{
We use the \textsc{ml20m} dataset, since is the only one provided in their implementation, see \url{https://github.com/dawenl/vae_cf}.
}, in which the test set consist of held out users. For this, we use their own implementation, slightly changed so that DAE can be trained with MIL and cross-entropy objective functions as well. The results for the models above the dash line in Table~\ref{table:metrics_results_vae} are taken directly from~\cite{liang:2018:VAE}. As observed, \textsc{Multi-dae} (i.e. \MULTItanhlin\, in our nomenclature) metric values are in pair (for recalls at large top-$k$) or below (for small top-$k$, or when accounting for the ranking order, as in NDCG) the performance of MIL and cross-entropy based DAEs. This is in agreement with the conclusions drawn from the experimental results in Tables~\ref{table:metrics_results_ml20m}, \ref{table:metrics_results_netflix} and \ref{table:metrics_results_lastfm}. 
% NA in the table
Please note that the NAs in Table~\ref{table:metrics_results_vae} stand for \emph{not available} results, because they were not reported in ~\cite{liang:2018:VAE} and the code used is not publicly available.
% Comparison with CDAE?
% Vae results
For completeness, we include in Table~\ref{table:metrics_results_vae} the results of the Variational AE (VAE)~\cite{liang:2018:VAE}, \textsc{Multi-VAE}$^{\rm PR}$, which are indeed close to those obtained with MIL and cross-entropy. Nevertheless, a proper comparison with \textsc{Multi-VAE}$^{\rm PR}$ (which requires the implementation of other losses for VAE) is out of the scope of this work. 

\begin{table}[htb]
\begin{tabular}{c c c c c c}
\multicolumn{6}{c}{\textsc{ML20m held-out test}} \\
\hline
\hline
  & r@1 & r@20 & r@50 & n@20 & n@100 \\
\hline
\vspace{-0.3cm}
\\
\textsc{Multi-VAE}$^{\rm PR}$ & 0.378 & 0.395 & 0.537 & 0.336 & 0.426 \\
\textsc{Multi-DAE} & 0.383 & 0.387 & 0.524 & 0.331 & 0.419 \\
\textsc{WMF} & NA & 0.360 & 0.498 & NA & 0.386 \\
\textsc{SLIM} & NA & 0.370 & 0.495 & NA & 0.401  \\
\textsc{CDAE} & NA &0.391 & 0.523 & NA & 0.418 \\
\hdashline
\CEpointlinsig & 0.404 & 0.389 & 0.518 & 0.338 & 0.419 \\
\CEpointsigsig & 0.401 & 0.395 & 0.532 & 0.343 & 0.428 \\
\MILlinsig & 0.409 & 0.392 & 0.520 & 0.341 & 0.421 \\
\MILsigsig & 0.411 & 0.394 & 0.526 & 0.343 & 0.425 \\
\hline
\end{tabular}
\caption{Relevance and novelty metrics for the \textsc{ML20m} dataset as process in \cite{liang:2018:VAE}. 
The horizontal dash line separate models reported in \cite{liang:2018:VAE} from those calculated in this work following their data process and implementation. NA stand for \emph{not available} results, because they were not reported in ~\cite{liang:2018:VAE} and the code used is not publicly available. Here, $\boldsymbol{{\rm r}@k}$ stands for Recall$\boldsymbol{@k}$ and  $\boldsymbol{{\rm n}@k}$ is the NDCG$\boldsymbol{@k}$.
}
\label{table:metrics_results_vae}
\end{table}

Given the similarity of \textsc{MIL} and \textsc{CE$_{\rm Point}$} DAE models in terms of relevance-aware metrics, we proceed next to study the differences in the distribution of recommendations, and how are these allocated in terms of the popularity distribution of the items. 
%In the following, we focus the analysis on the \textsc{Netlix} dataset. The conclusions we draw  remain the same for the rest of datasets.

\subsection{Distribution of preferences}
In this sub-section, we study the distribution of predicted user preferences by DAE models averaged across all users, $\langle \hat p\rangle$. We focus on objective functions modeling preferences; the \textsc{MULTI} model deals with probabilities across a multi-class problem, and thus cannot be easily compared. Table~\ref{tab:comparison_distributions} presents the user--averaged distribution of predicted preferences by the \CEpointlinsig, \CEpairlinsig\, and \MILlinsig\, models, for the \textsc{Netflix} dataset. As observed, traditional losses tend to set most items with a small preference, close to zero, as expected from the cross--entropy loss. Yet, there is a clear distinction between point-- and pair--wise learning. On average, the point-wise cross-entropy model tends to set few items with  high  preference for each user (fewer than $100$ items with $\langle p\rangle\ge0.9$), while recommending more than $85~\%$ of the available catalogue with an almost zero preference. On the other hand, pair-wise models set a considerably higher proportion of items with a measurable preference (around 5 times larger). Hence, 
\textsc{CE point} optimizes the head of the recommendation by setting very few items with high preference for each user; on the other hand, \textsc{CE pair} allows more items to have a high preference in the recommendation, which may cause a less effective optimization of the ranked list (in agreement with the results presented in Tables~\ref{table:metrics_results_ml20m}, \ref{table:metrics_results_netflix} and \ref{table:metrics_results_lastfm}). 

\begin{table}[h]
\begin{tabular}{c c c | c c c}
\multicolumn{6}{c}{\textsc{Distribution of predicted preferences}}
\\
\hline 
\hline
\multicolumn{3}{c}{preference thresholds} & \textsc{CE point} & \textsc{CE pair} & \textsc{MIL} \\
\hline 
1.0 & $\ge \langle\hat{p}\rangle \ge $ & 0.9   & 0.35 & 1.62 & 2.32 \\
0.9 &$ > \langle\hat{p}\rangle \ge $ & 0.7     & 0.33 & 1.58 & 60.2 \\
0.7 &$ > \langle\hat{p}\rangle \ge $ & 0.5     & 0.35 & 2.21 & 31.6 \\
0.5 &$ > \langle\hat{p}\rangle \ge $ & 0.25    & 0.79 & 6.49 & 5.49 \\
0.25 &$ > \langle\hat{p}\rangle \ge $ & 0.01   & 13.1 & 49.8 & 0.39 \\
0.01 &$ > \langle\hat{p}\rangle \ge $ & 0.0    & 85.1 & 38,3 & 0.0 \\
\hline
\end{tabular}
\caption{Comparison of the distribution of predicted preferences
averaged across all users in the \textsc{Netflix} dataset, $\boldsymbol{\langle\hat{p}\rangle}$, 
for \textsc{CE point}, \textsc{CE pair} and \textsc{MIL} models with  linear--sigmoid activations.}
\label{tab:comparison_distributions}
\end{table}

Conversely, the MIL function pushes all items towards high preferences. This is a consequence of the functional form in equation~(\ref{eq:mil_def}), where for large $\gamma_{\rm MI}$ the missing information term does not contribute to the loss unless the predicted preference is close to $1$ or $0$. Thus, the ranking of unseen items is left to the low--rank process, rather than forcing unobserved items to be at the tail of the recommendation ($\hat{p}=0$). Such a ranking optimization has an important consequence: it allows all items to have a chance to be recommended, since none of them have a zero preference prediction. This effect might be of interest for RS services that cannot recommend all the items in their catalogue--due to legal constraints, for instance, or because of some particular business requirements.

\subsection{Popularity distribution of the recommendations}
\begin{table}[htb]
\begin{tabular}{c c c}

\hline
\hline
  & Short $\rightarrow$ Medium & Medium $\rightarrow$ Long \\
\hline
\textsc{ml20m} & 177 & 784 \\
\textsc{Netflix} & 251 & 965 \\
\textsc{lastfm} & 351 & 2240 \\
\end{tabular}
\caption{
Interval cuts of the popularity distribution of items. 
}
\label{table:intervals_long_tail}
\end{table}

The question of what kind of items (\emph{i.e.}, popular, frequent or infrequent) are recommended by each model is yet to be answered. To this end, we examine how the top-200 recommendations are distributed on the short, medium and long--tail intervals of the popularity distribution. 
% Long tail intervals
Inspired by Celma \emph{et al.}~\cite{Celma:2008:approach_novelty}, we calculate the cumulative distribution of item adoptions, $F(x)$, and take the short--tail interval as composed by the first $N_{33}$ items, where $N_{33}$ is the number of items that covers one third of the total visualizations, \emph{i.e.} $F(N_{33})=33 \%$. Similarly, the medium--tail items account for the second third of the total visualizations, \emph{i.e.} items in $(N_{33}, N_{66}]$ with $F(N_{66})=66 \%$. The rest of the item catalogue is taken within the long--tail interval. 
Table~\ref{table:intervals_long_tail} depicts the resulting interval cuts for each dataset. As observed, the \textsc{lastfm} dataset has the most heavy--tailed distribution (its short and medium tail contain the larger amount of items among the datasets used in this work), while \textsc{ml20m} presents the less. 

% Results
\begin{table}[htb]
\begin{tabular}{c c c c}

% ml20m
\multicolumn{4}{c}{\textsc{ml20m}} \\
\hline
\hline
  & Short & Medium &  Long \\
\hline
\CEpointlinsig & 41.1 & 40.1 & 18.8 \\
\MULTItanhlin & 36.3 & 40.8 & 22.9 \\
\MILlinsig & 32.2 & 39.9 & 27.9 \\
\\

% netflix
\multicolumn{4}{c}{\textsc{netflix}} \\
\hline
\hline
  & Short & Medium &  Long \\
\hline
\CEpointlinsig & 52.6 & 34.2 & 13.2 \\
\MULTItanhlin & 47.7 & 36.0 & 16.3 \\
\MILlinsig & 41.7 & 37.8 & 20.5 \\
\\

% lastfm
\multicolumn{4}{c}{\textsc{lastfm}} \\
\hline
\hline
  & Short & Medium &  Long \\
\hline
\CEpointlinsig & 39.6 & 34.8 & 25.6 \\
\MULTItanhlin & 36.0 & 35.0 & 29.0 \\
\MILlinsig & 33.4 & 37.2 & 29.4 \\
\\

\end{tabular}
\caption{
Distribution of top-200 recommendations (in percentage) within popularity intervals. 
}
\label{table:percentages_popularity}
\end{table}

Table~\ref{table:percentages_popularity} presents the distribution of top-200 recommendations within the  popularity intervals just defined, as obtained with \CEpointlinsig, \MULTItanhlin\, and \MILlinsig. 
% corss-entropy
Not surprisingly, \CEpointlinsig\, achieves large metric values by being the model that recommends popular items most frequently in all datasets. 
% multi
This popularity bias in the recommendation is alleviated by the multinomial loss, concomitant to an incremental decrease of metric values (mainly at small top-$k$ rankings), as demonstrated in Tables~\ref{table:metrics_results_ml20m}, \ref{table:metrics_results_netflix} and \ref{table:metrics_results_lastfm}. 
% mil
Remarkably, \MILlinsig\, yields metric performances similar to those of \CEpointlinsig\,  while recommending popular items less frequently. For instance, in the \textsc{ml20m} and \textsc{Netflix} datasets, \MILlinsig\, recommend popular items $10$ percentage points less than the \CEpointlinsig\, model (a $\sim20 \%$ decrease in both datasets). For the \textsc{lastfm} dataset, the decrease is of $6$ percentage points ($15 \%$ decrease). Moreover, such a  decrease in short--tail recommendations  favours the appearance of both medium and long--tail items at the top-200 list. For instance, the \MILlinsig\, model recommends long--tail items $\sim50~\%$ more frequently than the \CEpointlinsig\, model for the \textsc{ml20m} and \textsc{Netflix} datasets. 
% Explain lastfm!!
On the other hand, the heavier tail of the \textsc{Lastfm} dataset makes the recommendations of all models to be more evenly distributed among the intervals of popularity. Still, \MILlinsig\, continues to be the model that recommends popular items less frequently. 

\begin{figure}
\begin{center}
    \includegraphics[width=.9\linewidth]{figures/netflix_rankings_top200.png}
    \caption{Ranking evolution of short (solid lines), medium (dash lines) and long--tail (dots) items within the top-200 recommendations for the \textsc{Netflix} dataset.
    \MILlinsig\, model is presented in blue lines, \CEpointlinsig\, in red, and \MULTItanhlin\, in green.}
    \label{fig:rankings_tails_netflix}
\end{center}
\end{figure}


We finally analyze in Figure~\ref{fig:rankings_tails_netflix} how items belonging to the short (solid lines), medium (dash lines) or long--tail (dots) popularity intervals are ranked within the top-200 recommendations, for the \textsc{Netflix} dataset. 
% Results for CE
As observed, the \CEpointlinsig\, model attacks mainly short--tail items for ranking positions smaller than $100$ (red solid line). This explains the competitive metric scores reported in Table~\ref{table:metrics_results_netflix}, as well as the relatively few items set with a high preference, see Table~\ref{tab:comparison_distributions}. 
% Results for Multi
On the other hand, the \textsc{Multi} (green) model reduces the number of popular items ranked in the top positions, while recommending more items from the medium and long--tail. This change from short-- towards medium and long--tail recommendations may explain the relatively poorer performance of the multinomial loss at small top-$k$ values.

% long tail
The \CEpointlinsig\, model tends to under-represent infrequent items (red dots).
% Results for MIL
This misrepresentation of long--tail items is alleviated by \textsc{MIL} models.
They heavily set short-tail items at the very top of the ranking (blue solid lines), but drastically reduce their appearance shortly thereafter. Instead, \textsc{MIL} models sharply increase the recommendation of medium--tail items (blue dash lines)  until the top-100 ranking, where the growth rate stagnates. Meanwhile, for items belonging to the long--tail (blue dots), \textsc{MIL} expands its appearance almost linearly, to the point that they exceed the number of short--tail recommendations. 
We finally highlight  that for ranking position $100$, \MILlinsig\, recommends long--tail items to $\sim40k$ users more than \CEpointlinsig\, and $\sim25k$ users more than \MULTItanhlin. 

%\begin{figure}
%\begin{center}
%     \includegraphics[width=.7\linewidth]{figures/lastfm_rankings_top200.png}
%     \caption{Ranking evolution of short (solid lines), medium (dash lines) and long--tail (dots) items within the top-200 recommendations for the \textsc{lastfm} dataset. The colors of models are the same as in Figure~\ref{fig:rankings_tails_netflix}.  }
%     \label{fig:rankings_tails_lastfm}
% \end{center}
% \end{figure}

