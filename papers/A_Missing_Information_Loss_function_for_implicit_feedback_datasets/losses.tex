\subsection{Objective functions for Recommender Systems}\label{subsec:losses}

%Choices
Learning to assign user preferences for items depends to a great extent on how the objective function--i.e., the function we intend to optimize--is set. In this subsection we review the most relevant objective functions considered in the literature for the task of recommendation, with a special focus on the missing information issue.

Regarding the square loss, a confidence scale factor for balancing the observed and unobserved items is introduced in~\cite{HuKoren:2008:CF_implicit}.
This factor can be defined as $C(p_{ui}):=a p_{ui}$, with $a>1$ a hyper-parameter for ensuring a correct balance. 
Using such confidence scale factor, the square loss is cast as
% Square: euclidean distance between the predictions and the targets
\begin{equation}\label{eq:square}
l(p_{ui},\hat{p}_{ui})=\frac{C(p_{ui})+1}{2}\left(p_{ui}-\hat{p}_{ui}\right)^2.
\end{equation}
Similarly, the cross-entropy objective function can be generalized to account for the unbalance of classes, 
\begin{equation}\label{eq:cross-entropy}
l\left(p_{ui},\hat{p}_{ui}\right) = 
- C\left(p_{ui}\right)\log\left(\hat{p}_{ui}\right)  
- \left(1-p_{ui}\right)\log\left(1-\hat{p}_{ui}\right).
\end{equation}
In both cases, the total loss is averaged across all users, 
\begin{equation}\label{eq:point-wise_no_sampling}
\mathcal{L}_{\rm point}=\frac{1}{|\mathcal{U}|}\sum_{u\in \mathcal{U}} 
\sum_{i\in\mathcal{I}} l\left(p_{ui},\hat{p}_{ui}\right).
\end{equation}
Please note that casting unobserved user-item interactions as  $p_{ui}=0$ in equations (\ref{eq:square}) and (\ref{eq:cross-entropy}), induces many zero recommendations, \emph{i.e.} $\hat{p}_{ui}=0$. With all certainty, the limited capacity of the model (the low--rank process) avoids setting all unobserved items with a zero preference prediction. 

% Sampling
Due to the large item catalogues typically involved in RS, negative sampling techniques are used to solve  the positive/negative class unbalance problem~\cite{Pan:2008:OCCF, Wu:2016:CDAE-topN}. For this, a target set $\mathcal{T}_u$ is built by joining the observed item set $\mathcal{I}_u$ and items sampled from $\mathcal{\tilde{I}}_u:=\mathcal{I} \setminus \mathcal{I}_u$. The number of items sampled from $\mathcal{\tilde{I}}_u$ is a hyper-parameter to be tuned, while $C(p_{ui})$ in equations (\ref{eq:square}) and (\ref{eq:cross-entropy}) is set to $1$  for all preferences. The loss is then computed as 
\begin{equation}\label{eq:point-wise}
\mathcal{L}_{\rm point}=\frac{1}{|\mathcal{U}|}\sum_{u\in \mathcal{U}} 
\sum_{i\in\mathcal{T}_u} l\left(p_{ui},\hat{p}_{ui}\right).
\end{equation}

% Point-wise
The above objective functions are examples of point-wise learning, where the loss is calculated by taking the information of only one item at a time. Rendle \emph{et al.}~\cite{Rendle:2009:BPR} introduced 
pair-wise learning, which confronts a pair of items (positive and  unobserved) to compute the final loss. 
%As before, this setting assumes the existence of negative examples within the unknown interactions to learn the ranking of items.
%As a premise, negative items are selected at random from the unknown data. 
Because of this, a new set $\mathcal{P}_u$  consisting of pairs of seen (positive feedback) and unseen items  (assumed negative feedback) is created. The total pair-wise loss is then defined as~\cite{Wu:2016:CDAE-topN}
\begin{equation}\label{eq:pair-wise}
\mathcal{L}_{\rm pair}=\frac{1}{|\mathcal{U}|}\sum_{u\in \mathcal{U}} 
\sum_{i,j\in\mathcal{P}_u} l\left(p_{uij},\hat{p}_{uij}\right).
\end{equation}
Here, $p_{uij}:=p_{ui}-p_{uj}=1,\forall (i,j)\in\mathcal{P}_u$ and $\hat{p}_{uij}:=\hat{p}_{ui}-\hat{p}_{uj}$. 

% A predicted zero preference is a valid solution
For both point and pair--wise learning schemes, the objective functions defined in equations~(\ref{eq:square}) and (\ref{eq:cross-entropy}) admit as a valid solution a predicted zero preference 
when the input preference is zero, i.e. $\hat{p}_{ui}=0$ if $p_{ui}=0$. However, it should be noted that in implicit feedback datasets there are no actual zero preferences, but rather missing information. Thus, by using any of the losses described above the  solution will inevitably assign zero preferences to most of the unobserved user-item pairs.  This fact affects the way in which items are recommended, as discussed in section~\ref{sec:results}.

% Multinomial function
On the other hand, a model based on the multinomial distribution has been recently applied to AEs by Liang \emph{et al.}~\cite{liang:2018:VAE}. The log-likelihood for a user $u$ in this setting can be written as
\begin{equation}\label{eq:multinomial}
-\sum_i p_{ui}\log\pi_i\left(\hat{p}_{ui}\right)
\end{equation}
where $\pi_i\left(\hat{p}_{ui}\right)$ is the probability distribution of the predictions.
% Explicit Penalization of missing information
Note that in contrast to the square and cross--entropy losses, this objective function does not explicitly penalize missing values, since $p_{ui}=0$ for unobserved user--item interactions.
% implicit condition of probabilities
Instead, the normalization condition of the probability distribution  ($\sum_i\pi_i\left(\hat{p}_{ui}\right)=1$), together with the low--rank process, helps to assign non-zero preferences to the unobserved items. However, the large item catalogues used in RS (typically  $>10$k) make it unlikely that non-seen items have a probability different from zero. Furthermore, the normalization condition on the  probabilities prevent this modeling from scaling up.


% MIL function
In order to mitigate all the problems mentioned within this subsection, we propose a novel objective function, the \emph{Missing Information Loss} (MIL), that explicitly forbids treating missing information as positive or negative feedback. For this reason, we propose the functional form
\begin{eqnarray}\label{eq:mil_def}
l(p_{ui},\hat{p}_{ui}) &=& 
\frac{1}{2} p_{ui} (1 + p_{ui}) (1 - \hat{p}_{ui})^{\gamma_{+}} + \nonumber\\
             & & \frac{1}{2} (1+p_{ui})(1-p_{ui}) A_{\rm MI}(\hat{p}_{ui}-0.5)^{2\gamma_{\rm MI}}. 
\end{eqnarray}
Here, the first and second term evaluate the contribution of the observed and unobserved user-item pairs into the final loss.
% positive term
In particular, the first term estimates preferences for positive items as a power law with parameter $\gamma_+$. Indeed, for $p_{ui}=1$ equation~(\ref{eq:mil_def}) reduces to
\begin{equation*}
l(p_{ui}=1,\hat{p}_{ui}) = (1 - \hat{p}_{ui})^{\gamma_{+}}. 
\end{equation*}
%mi term
On the other hand, the last term in (\ref{eq:mil_def}) explicitly forbids predicted 0 and 1 preferences for missing entries, acting as a barrier for the optimization process. As a matter of fact, for $p_{ui}=0$ equation~(\ref{eq:mil_def}) is cast as
\begin{equation*}
l(p_{ui}=0,\hat{p}_{ui}) = A_{\rm MI}(\hat{p}_{ui}-0.5)^{2\gamma_{\rm MI}}. 
\end{equation*}
The constants $A_{\rm MI}$ and $\gamma_{\rm MI}$ are hyper-parameters to be fine-tuned. In this paper we explore the pairs $\left(A_{\rm MI}, \gamma_{\rm MI}\right)\in\{
(5\cdot10^1, 2)$, $(10^3, 4)$, $(2\cdot10^4, 6)$, $(1\cdot10^6, 10)$, $(5\cdot10^9, 15)
\}$, see Figure~\ref{fig:mil_parameters}.

\begin{figure}[hbt]
    \centering
    \includegraphics[width=.75\linewidth]{figures/MI_term_cost.png}
    \caption{Different values of the hyper-parameters $\boldsymbol{A_{\rm MI}}$ and $\boldsymbol{\gamma_{\rm MI}}$ modeling the missing information term in equation~(\ref{eq:mil_def}). The exponent of each polynomial here is, from the smoothest to the sharpest curves, $\boldsymbol{2 \gamma_{\rm MI}\equiv\{4,8,12,20,30\}}$.
    }
    \label{fig:mil_parameters}
\end{figure}

At this point, it is worth stressing that under the MIL function all items--independently of their position into the long tail curve--can be part of the recommendation process, as their predicted preferences adopt non-zero values, $\hat{p}_{ui}\in(0,1)$.
Thus, the final predicted preference will be adjusted by the collaborative filtering among users, 
the co-occurrence of items, and the limited capacity of the model--\emph{i.e.}, the overall low--rank process.

%Negative feedback extension
Note that the MIL function in equation~(\ref{eq:mil_def}) can be naturally extended to account for actual negative feedback, \emph{i.e} $p_{ui}=-1$. Indeed, we can simply add the term 
\begin{equation}\label{eq:neg_feedback_term}
    -\frac{1}{2} p_{ui} (p_{ui}-1) \hat{p}_{ui}^{\gamma_{-}}. 
\end{equation}
to equation~(\ref{eq:mil_def}), which vanishes whenever $p_{ui}=0,1$.
Here, $\gamma_{-}$ is an exponent controlling the family of polynomials modeling negative feedback entries. Such a term would force negative ratings to have a zero predicted preference. 
We will leave the analysis of datasets with actual negative feedback for future study.

Finally, for any given loss function we regularize the model with weight-decay, so that the total loss is 
\begin{equation}\label{eq:L2_reg}
\mathcal{L} = \mathcal{L}_{\rm point/pair}+\lambda\left( ||{\bf W}||_2^2+||{\bf W'}||_2^2\right).
\end{equation}
Here, $\lambda$ is a hyper-parameter. 









