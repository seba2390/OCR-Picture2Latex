

\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage{cvpr}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{amsfonts}
\usepackage{mathrsfs}
\usepackage{booktabs}
\usepackage{multirow, eucal}

\usepackage{helvet}
\usepackage{courier}
\usepackage{bm}
%
%
%
\usepackage{graphicx}
\usepackage{color}
\usepackage{epstopdf}
\usepackage{wrapfig}
\usepackage{picinpar}
\usepackage{url}

\usepackage[vlined,ruled,linesnumbered]{algorithm2e}
\usepackage{cite}
\newtheorem{theorem}{Theorem}


\usepackage{caption}
\captionsetup{margin=5pt,font=small,labelfont=bf}

\cvprfinalcopy





\def\kuired{\textcolor{red}}
%
%
%
%
%

%
%
%
%
%
%
%

%

%
%
%

\def\cD{ {\cal D } }
\def\cW{ {\cal W } }


\def\diag{\mbox{diag}}
\def\rank{\mbox{rank}}
\def\grad{\mbox{\text{grad}}}
\def\dist{\mbox{dist}}
\def\sgn{\mbox{sgn}}
\def\tr{\mbox{tr}}
\def\etal{{\em et al.\/}\, }
\def\card{{\mbox{Card}}}

%
\def\balpha{\mbox{{\boldmath $\alpha$}}}
\def\bbeta{\mbox{{\boldmath $\beta$}}}
\def\bzeta{\mbox{{\boldmath $\zeta$}}}
\def\bgamma{\mbox{{\boldmath $\gamma$}}}
\def\bdelta{\mbox{{\boldmath $\delta$}}}
\def\bmu{\mbox{{\boldmath $\mu$}}}
\def\beps{\mbox{{\boldmath $\epsilon$}}}
\def\blambda{\mbox{{\boldmath $\lambda$}}}
\def\bnu{\mbox{{\boldmath $\nu$}}}
\def\bomega{\mbox{{\boldmath $\omega$}}}
\def\bfeta{\mbox{{\boldmath $\eta$}}}
\def\bsigma{\mbox{{\boldmath $\sigma$}}}
\def\bzeta{\mbox{{\boldmath $\zeta$}}}
\def\bphi{\mbox{{\boldmath $\phi$}}}
\def\bxi{\mbox{{\boldmath $\xi$}}}
\def\bvphi{\mbox{{\boldmath $\phi$}}}
\def\bdelta{\mbox{{\boldmath $\delta$}}}
\def\bvarsigma{\mbox{{\boldmath $\varsigma$}}}
\def\bXi{\mbox{{\boldmath $\Xi$}}}


\def\bPi{\mbox{{\boldmath $\Pi$}}}

\def\bDelta{\mbox{{\boldmath $\Delta$}}}
\def\bPi{\mbox{{\boldmath $\Pi$}}}
\def\bPsi{\mbox{{\boldmath $\Psi$}}}
\def\bSigma{\mbox{{\boldmath $\Sigma$}}}

%
\def\mA{{\mathcal A}}
\def\mB{{\mathcal B}}
\def\mC{{\mathcal C}}
\def\mD{{\mathcal D}}
\def\mE{{\mathcal E}}
\def\mF{{\mathcal F}}
\def\mG{{\mathcal G}}
\def\mH{{\mathcal H}}
\def\mI{{\mathcal I}}
\def\mJ{{\mathcal J}}
\def\mK{{\mathcal K}}
\def\mL{{\mathcal L}}
\def\mM{{\mathcal M}}
\def\mN{{\mathcal N}}
\def\mO{{\mathcal O}}
\def\mP{{\mathcal P}}
\def\mQ{{\mathcal Q}}
\def\mR{{\mathcal R}}
\def\mS{{\mathcal S}}
\def\mT{{\mathcal T}}
\def\mU{{\mathcal U}}
\def\mV{{\mathcal V}}
\def\mW{{\mathcal W}}
\def\mX{{\mathcal X}}
\def\mY{{\mathcal Y}}
\def\mZ{{\mathcal{Z}}}




%
\def\0{{\bf 0}}
\def\1{{\bf 1}}

%
\def\bA{{\bf A}}
\def\bB{{\bf B}}
\def\bC{{\bf C}}
\def\bD{{\bf D}}
\def\bE{{\bf E}}
\def\bF{{\bf F}}
\def\bG{{\bf G}}
\def\bH{{\bf H}}
\def\bI{{\bf I}}
\def\bJ{{\bf J}}
\def\bK{{\bf K}}
\def\bL{{\bf L}}
\def\bM{{\bf M}}
\def\bN{{\bf N}}
\def\bO{{\bf O}}
\def\bP{{\bf P}}
\def\bQ{{\bf Q}}
\def\bR{{\bf R}}
\def\bS{{\bf S}}
\def\bT{{\bf T}}
\def\bU{{\bf U}}
\def\bV{{\bf V}}
\def\bW{{\bf W}}
\def\bX{{\bf X}}
\def\bY{{\bf Y}}
\def\bZ{{\bf{Z}}}


%
\def\ba{{\bf a}}
\def\bb{{\bf b}}
\def\bc{{\bf c}}
\def\bd{{\bf d}}
\def\be{{\bf e}}
\def\bff{{\bf f}}
\def\bg{{\bf g}}
\def\bh{{\bf h}}
\def\bi{{\bf i}}
\def\bj{{\bf j}}
\def\bk{{\bf k}}
\def\bl{{\bf l}}
\def\bm{{\bf m}}
\def\bn{{\bf n}}
\def\bo{{\bf o}}
\def\bp{{\bf p}}
\def\bq{{\bf q}}
\def\br{{\bf r}}
\def\bs{{\bf s}}
\def\bt{{\bf t}}
\def\bu{{\bf u}}
\def\bv{{\bf v}}
\def\bw{{\bf w}}
\def\bx{{\bf x}}
\def\by{{\bf y}}
\def\bz{{\bf z}}

%
\def\hy{\hat{y}}
\def\hby{\hat{{\bf y}}}


%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%

%
\def\pd{{\succ\0}}
\def\psd{{\succeq\0}}
\def\vphi{\varphi}
\def\trsp{{\sf T}}


\def\mRMD{{\mathrm{D}}}
\def\mRMD{{\mathrm{D}}}
%

\def\kui{\textcolor{black}}



\def\citep{\cite}
\def\citet{\cite}
\newtheorem{coll}{Corollary}
\newtheorem{deftn}{Definition}
\newtheorem{thm}{Theorem}
\newtheorem{prop}{Proposition}
\newtheorem{lemma}{Lemma}
\newtheorem{remark}{Remark}
\newtheorem{ass}{Assumption}


%
\def\diag{\mbox{diag}}
\def\rank{\mbox{rank}}
\def\grad{\mbox{\text{grad}}}
\def\dist{\mbox{dist}}
\def\sgn{\mbox{sgn}}
\def\tr{\mbox{tr}}
\def\etal{{\em et al.\/}\, }
\def\card{{\mbox{Card}}}
\def\st{\mbox{s.t. }}

\def\kui{\textcolor{black}}
\def\young{\textcolor{black}}
\def\mark{\textcolor{red}}

%

%
\def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}

%


\begin{document}

%
\title{Towards Effective Low-bitwidth Convolutional Neural Networks\thanks{B. Zhuang, C. Shen, L. Liu and I. Reid are with The University of Adelaide,
  Australia. M. Tan is with South China University of Technology, China.}
  \thanks{Correspondence to C. Shen (e-mail: chhshen@gmail.com).}
%
}

\author{
Bohan Zhuang, Chunhua Shen,
Mingkui Tan, Lingqiao Liu,  Ian Reid
}




\maketitle


\begin{abstract}
	This paper tackles the problem of training a deep convolutional neural network with both low-precision weights and low-bitwidth activations. Optimizing a low-precision network is very challenging since the training process can easily get trapped in a poor local minima, which results in substantial accuracy loss. To mitigate this problem, we propose three simple-yet-effective approaches to improve the network training.
	First, we propose to use a two-stage optimization strategy to progressively find good local minima. Specifically, we propose to first optimize a net with quantized weights and then quantized activations. This is in contrast to the traditional methods which optimize them simultaneously.
	Second, following a similar spirit of the first method, we propose another progressive optimization approach which progressively decreases the bit-width from high-precision to low-precision during the course of training.
	Third, we adopt a novel learning scheme to jointly train a full-precision model alongside the low-precision one. By doing so, the full-precision model provides hints to guide the low-precision model training.
	Extensive experiments on various datasets (\ie, CIFAR-100 and ImageNet) show the effectiveness of the proposed methods. To highlight, using our methods to train a 4-bit precision network leads to no performance decrease in comparison with its full-precision counterpart with standard network architectures (\ie, AlexNet and ResNet-50).


\end{abstract}


\tableofcontents
\clearpage





	\input{introduction.tex}
	\input{related_work.tex}
\section{Methods}
\begin{figure*}[!t]
	\centering
	\resizebox{0.9\linewidth}{!}
	{
		\begin{tabular}{c}
			\includegraphics{pdf/1.pdf}
		\end{tabular}
	}
	\caption{Demonstration of the guided training strategy. We use the residual network structure for illustration.}
	\label{fig:knowledge_transfer}
\end{figure*}
In this section, we will first revisit the quantization function in the neural network and the way to train it. Then we will elaborate our three methods in the subsequent sections.
\subsection{Quantization function revisited} \label{sec:baseline}
%

A common practise in training a neural network with low-precision weights and activations is to introduce a quantization function. Considering the general case of $k$-bit quantization as in~\cite{zhou2016dorefa}, we define the quantization function $Q(\cdot)$ to be
\begin{equation}
	{z_q} = Q({z_r}) = \frac{1}{{{2^k} - 1}}round(({2^k} - 1){z_r})
\end{equation}
where ${z_r} \in [0,1]$ denotes the full-precision value and ${z_q} \in [0,1]$ denotes the quantized value. With this quantization function, we can define the weight quantization process and the activation quantization process as follows:


\noindent \textbf{Quantization on weights}:
\begin{equation}\label{eq:quan-weigtht}
	{w_q} = Q(\frac{{\tanh (w)}}{{2\max (\left| {\tanh (w)} \right|)}} + \frac{1}{2}).
\end{equation}In other words, we first use $\frac{{\tanh (w)}}{{2\max (\left| {\tanh (w)} \right|)}} + \frac{1}{2}$ to obtain a normalized version of $w$ and then perform the quantization, where $\tanh(\cdot)$ is adopted to reduce the impact of large values.

\noindent \textbf{Quantization on activations}:

 Same as \cite{zhou2016dorefa}, we first use a clip function $f(x) = clip(x,\,0,1)$ to bound the activations to $[0, 1]$. After that, we conduct quantize the activation by applying the quantization function $Q(\cdot)$ on $f(x)$.
\begin{equation} \label{eq:quan-activations}
	{x_q} = Q(f(x)).
\end{equation}

\noindent \textbf{Back-propagation with quantization function}: In general, the quantization function is non-differentiable and thus it is impossible to directly apply the back-propagation to train the network. To overcome this issue, we adopt the straight-through estimator \cite{zhou2016dorefa, hubara2016binarized, bengio2013estimating} to approximate the gradients calculation. Formally, we approximate the partial gradient $\frac{{\partial {z_q}}}{{\partial {z_r}}}$ with an identity mapping, namely $\frac{{\partial {z_q}}}{{\partial {z_r}}} \approx 1$.  Accordingly, $\frac{{\partial l}}{{\partial {z_r}}}$ can be approximated by
\begin{equation}
	\frac{{\partial l}}{{\partial {z_r}}} = \frac{{\partial l}}{{\partial {z_q}}}\frac{{\partial {z_q}}}{{\partial {z_r}}} \approx \frac{{\partial l}}{{\partial {z_q}}}.
\end{equation}


%
%
%
%
%
%
%
\subsection{Two-stage optimization}\label{sec:two-stage}
With the straight-through estimator, it is possible to directly optimize the low-precision network. However, the gradient approximation of the quantization function inevitably introduces noisy signal for updating network parameters. Strictly speaking, the approximated gradient may not be the right updating direction. Thus, the training process will be more likely to get trapped at a poor local minima than training a full precision model. Applying the quantization function to both weights and activations further worsens the situation.

To reduce the difficulty of training, we devise a two-stage optimization procedure: at the first stage, we only quanitze the weights of the network while setting the activations to be full precision. After the converge (or after certain number of iterations) of this model, we further apply the quantization function on the activations as well and retrain the network. Essentially, the first stage of this method is a related subproblem of the target one. Compared to the target problem, it is easier to optimize since it only introduces quantization function on weights. Thus, we are more likely to arrive at a good solution for this sub-problem. Then, using it to initialize the target problem may help the network avoid poor local minima which will be encountered if we train the network from scratch.
Let $M_{low}^{K}$ be the high-precision model with $K$-bit. We propose to learn a low-precision model $M_{low}^{k}$ in a two-stage manner with $M_{low}^{K}$ serving as the initial point, where $k<K$.
 The detailed algorithm is shown in Algorithm \ref{algo:two-stage}.
\begin{algorithm}[]
	\KwIn{Training data $\{ ({{\bf{x}}_i},y_i)\}_{i=1}^N$; A $K$-bit precision model $M_{low}^K$.}
	\KwOut{A low-precision deep model $M^k_{low}$ with weights ${{\bf{W}}_{low}}$ and activations being quantized into $k$-bit.}

	\textbf{Stage 1}: Quantize ${{\bf{W}}_{low}}$:\\
	\For{ $\mathrm{epoch} = 1,...,L$}
	{
		\For{ $t = 1,...T$}
		{
			Randomly sample a mini-batch data;\\
			Quantize the weights ${{\bf{W}}_{low}}$ into $k$-bit by calling some quantization methods with $K$-bit activations\;
		}
	}
	\textbf{Stage 2}: Quantize activations:\\
	Initialize ${{\bf{W}}_{low}}$ using the converged $k$-bit weights from \textbf{Stage 1} as the starting point; \\
	\For{ $\mathrm{epoch} = 1,...,L$}
	{
		\For{ $t = 1,...T$}
		{
			Randomly sample a mini-batch data;\\
			Quantize the activations into $k$-bit  by calling some quantization methods while keeping the weights to $k$-bit;
		}
	}
	\caption{Two-stage optimization for $k$-bit quantization}
	\label{algo:two-stage}
\end{algorithm}


\subsection{Progressive quantization} \label{sec:progressive}

%

The aforementioned two-stage optimization approach suggests the benefits of using a related easy optimized problem to find a good initialization. However, separating the quantization of weights and activations is not the only solution to implement the above idea. In this paper, we also propose another solution which progressively lower the bitwidth of the quantization during the course of network training.
Specifically, we progressively conduct the quantization from higher precisions to lower precisions (\eg, 32-bit $\to$ 16-bit $\to$ 4-bit $\to$ 2-bit). The model of higher precision will be used the the starting point of the relatively lower precision, in analogy with annealing.


Let $\{{b_1},...,{b_n}\}$ be a  sequence precisions, where  $b_n<b_{n-1}, ..., b_2<{b_1}$, $b_n$ is the target precision and $b_1$ is set to 32 by default. The whole progressive optimization procedure  is summarized in as Algorithm~\ref{algo:progressive optimization}.
%
 Let $M_{low}^{k}$ be the low-precision model with $k$-bit and $M_{full}$ be the full precision model. In each step, we propose to learn $M_{low}^{k}$, with the solution in the $(i-1)$-th step, denoted by $M_{low}^{K}$, serving as the initial point, where $k<K$.
%

%
%
%



%

%


%

%

%

%
\begin{algorithm}[]
	\KwIn{Training data $\{ ({{\bf{x}}_j},y_j)\}_{j=1}^N$; A pre-trained 32-bit full-precision  model ${M_{full}}$ as baseline; the precision sequence $\{{b_1},...,{b_n}\}$ where $b_n<b_{n-1}, ..., b_2<{b_1} = 32$.}
	\KwOut{A low-precision deep model $M_{low}^{b_n}$.}
	Let $M_{low}^{b_1}=M_{full}$, where $b_1 = 32$\;
	\For{ $i = 2,...n$}
	{
		Let $k = b_i$ and $K=b_{i-1}$\;
		Obtain $M_{low}^{k}$ by calling some quantization methods with $M_{low}^{K}$  being the input\;
		%
		%
		%
	}
	\caption{Progressive quantization for accurate CNNs with low-precision weights and activations}
	\label{algo:progressive optimization}
\end{algorithm}


%


%


%

\subsection{Guided training with a full-precision network}\label{sec:mutual}
The third method proposed in this paper is inspired by the success of using information distillation ~\cite{romero2014fitnets, hinton2015distilling, parisotto2016actor, zagoruyko2016paying, ba2014deep} to train a relatively shallow network. Specifically, these methods usually use a teacher model (usually a pretrained deeper network) to provide guided signal for the shallower network. Following this spirit, we propose to train the low-precision network alongside another guidance network. Unlike the work in \cite{romero2014fitnets, hinton2015distilling, parisotto2016actor, zagoruyko2016paying, ba2014deep}, the guidance network shares the same architecture as the target network but is pretrained with full-precision weights and activations.

However, a pre-trained model may not be necessarily optimal or may not be suitable for quantization. As a result, directly using a fixed pretrained model to guide the target network may not produce the best guidance signals. To mitigate this problem, we do not fix the parameters of a pretrained full precision network as in the previous work \cite{zhang2017deep}.


By using the guidance training strategy, we assume that there exist some full-precision models with good generalization performance, and an accurate low-precision model can be obtained by directly performing the quantization on those full-precision models. In this sense, the feature maps of the learned low-precision model should be close to that obtained by directly doing quantization on the full-precision model. To achieve this, essentially, in our learning scheme, we can jointly train the full-precision and low-precision models. This allows these two models adapt to each other. We even find by doing so the performance of the full-precision model can be slightly improved in some cases.

%

Formally, let ${{\bf{W}}_{full}}$ and ${{\bf{W}}_{low}}$ be the full-precision model and low-precision model, respectively. Let $\mu ({\bf{x}};{{\bf{W}}_{{full}}})$ and $\nu ({\bf{x}};{{\bf{W}}_{{low}}})$ be the nested feature maps (e.g., activations) of the full-precision model and low-precision model, respectively. To create the guidance signal, we may require that the nested feature maps from the two models should be similar. However,  $\mu ({\bf{x}};{{\bf{W}}_{{full}}})$ and $\nu ({\bf{x}};{{\bf{W}}_{{low}}})$  is usually not directly comparable since one is full precision and the other is low-precision.

%

%
%




%




%



%


%

%
%

%
%
%



%

To link these two models,  we can directly quantize the weights and activations of the full-precision model by equations (\ref{eq:quan-weigtht}) and (\ref{eq:quan-activations}). For simplicity, we denote the quantized feature maps by  $Q(\mu ({\bf{x}};{{\bf{W}}_{{full}}}))$. Thus, $Q(\mu ({\bf{x}};{{\bf{W}}_{{full}}}))$ and  $\nu ({\bf{x}};{{\bf{W}}_{{low}}})$ will become comparable. Then we can define the guidance loss as:
\begin{equation}
	R({{\bf{W}}_{full}},{{\bf{W}}_{low}}) = \frac{1}{2}\parallel Q(\mu ({\bf{x}};{{\bf{W}}_{{full}}})) - \nu ({\bf{x}};{{\bf{W}}_{{low}}}){\parallel^2},
\end{equation}
where $\parallel\cdot\parallel$ denotes some proper norms.
%


Let ${L_{{\theta _1}}}$ and ${L_{{\theta _2}}}$ be the cross-entropy classification losses for the full-precision and low-precision model, respectively. The guidance loss will be added to ${L_{{\theta _1}}}$ and ${L_{{\theta _2}}}$, respectively, resulting in two new objectives for the two networks, namely
\begin{equation} \label{eq:objective1}
	L_1({{\bf{W}}_{full}})  = {L_{{\theta _1}}} + \lambda R({{\bf{W}}_{full}},{{\bf{W}}_{low}}).
\end{equation}
and
\begin{equation} \label{eq:objective2}
	L_2({{\bf{W}}_{low}})  = {L_{{\theta _2}}} +  \lambda R({{\bf{W}}_{full}},{{\bf{W}}_{low}}).
\end{equation}
where $\lambda$ is a balancing parameter. Here, the guidance loss $R$ can be considered as some regularization on ${L_{{\theta _1}}}$ and ${L_{{\theta _2}}}$.


%
In the learning procedure, both ${{\bf{W}}_{full}}$ and ${{\bf{W}}_{low}}$ will be updated by minimizing $L_1({{\bf{W}}_{full}})$ and $L_2({{\bf{W}}_{low}})$ separately, using a mini-batch stochastic gradient descent method. The detailed algorithm is shown in Algorithm \ref{algo:one-mutual learning}. A high-bit precision model $M_{low}^K$ is used as an initialization of $M_{low}^k$, where $K>k$. Specifically, for the full-precision model, we have $K=32$. Relying on $M_{full}$, the weights and activations of $M_{low}^k$ can be initialized by equations (\ref{eq:quan-weigtht}) and (\ref{eq:quan-activations}), respectively.


Note that the training process of the two networks are different.
When updating ${{\bf{W}}_{low}}$ by minimizing $L_2({{\bf{W}}_{low}})$, we use full-precision model as the initialization and apply the forward-backward propagation rule in Section \ref{sec:baseline}  to fine-tune the model. When updating ${{\bf{W}}_{full}}$ by minimizing $L_1({{\bf{W}}_{full}})$, we use conventional forward-backward propagation to fine-tune the model.


\begin{algorithm}[]
	\KwIn{Training data $\{ ({{\bf{x}}_i},y_i)\}_{i=1}^N$; A pre-trained 32-bit full-precision model $M_{full}$; A $k$-bit precision model $M_{low}^k$.}
	\KwOut{A low-precision deep model $M^k_{low}$ with weights and activations being quantized into $k$ bits.}
	Initialize $M_{low}^k$ based on $M_{full}$;\\
	\For{ $\mathrm{epoch} = 1,...,L$}
	{
		\For{ $t = 1,...T$}
		{
			Randomly sample a mini-batch data;\\
			Quantize the weights ${{\bf{W}}_{low}}$  and activations into $k$-bit by minimizing $L_2({{\bf{W}}_{low}})$\;
			Update $M_{full}$ by minimizing $L_1({{\bf{W}}_{full}})$\;
		}

	}
	\caption{Guided training with a full-precision network for $k$-bit quantization}
	\label{algo:one-mutual learning}
\end{algorithm}


%
%
%
%
%
%
%
%
%
%

\subsection{Remark on the proposed methods}
The proposed three approaches tackle the difficulty in training a low-precision model with different strategies. They can be applied independently. However, it is also possible to combine them together. For example, we can apply the progressive quantization to any of the steps in the two-stage approach; we can also apply the guided training to any sub-step in the progressive training. Detailed analysis on possible combinations will be experimentally evaluated in the experiment section.


\subsection{Implementation details} \label{sec:implementation}

In all the three methods, we quantize the weights and activations of all layers except that the input data are kept to 8-bit. Furthermore, to promote convergence, we propose to add a scalar layer after the last fully-connected layer before feeding the low-bit activations into the softmax function for classification. The scalar layer has only one trainable small scalar parameter and is initialized to 0.01 in our approach.


During training, we randomly crop 224x224 patches from an image or its horizontal flip, with the per-pixel mean subtracted. We don't use any further data augmentation in our implementation. We adopt batch normalization (BN)~\cite{ioffe2015batch} after each convolution before activation. For pretraining the full-precision baseline model, we use Nesterov SGD and batch size is set to 256. The learning rate starts from 0.01 and is divided by 10 every 30 epochs. We use a weight decay 0.0001 and a momentum 0.9. For weights and activations quantization, the initial learning rate is set to 0.001 and is divided by 10 every 10 epochs. We use a simple single-crop testing for standard evaluation. Following~\cite{zagoruyko2016paying}, for ResNet-50, we add only two guidance losses in the 2 last groups of residual blocks. And for AlexNet, we add two guidance losses in the last two fully-connected layers.



%
	 \input{experiment.tex}

	 \input{conclusion.tex}

\small
\bibliographystyle{ieee}
\bibliography{reference}


\end{document}
