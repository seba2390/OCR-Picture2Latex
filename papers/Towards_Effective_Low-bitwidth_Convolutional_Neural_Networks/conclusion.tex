\section{Conclusion}
In this paper, we have proposed three novel approaches to solve the optimization problem for quantizing the network with both low-precision weights and activations. We first propose a two-stage approach to quantize the weights and activations in a two-step manner. We also observe that continuously quantize from high-precision to low-precision is also beneficial to the final performance. To better utilize the knowledge from the full-precision model, we propose to jointly learn the low-precision model and its full-precision counterpart to optimize the gradient problem during training. Using 4-bit weights and activations for all layers, we even outperform the performance of the 32-bit model on ImageNet and Cifar100 with general frameworks. 