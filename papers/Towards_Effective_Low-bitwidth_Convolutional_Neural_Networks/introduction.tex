\section{Introduction}
The state-of-the-art deep neural networks ~\cite{krizhevsky2012imagenet, simonyan2014very, he2016deep} usually involve millions of parameters and need billions of FLOPs during computation. Those memory and computational cost can be unaffordable for mobile hardware device or especially implementing deep neural networks on chips. To improve the computational and memory efficiency, various solutions have been proposed, including pruning network weights \cite{han2015learning, han2015deep}, low rank approximation of weights \cite{kim2015compression, zhang2016accelerating}, and training a low-bit-precision network \cite{zhou2017incremental, courbariaux2015binaryconnect, zhu2016trained, zhou2016dorefa}. In this work, we follow the idea of training a low-precision network and our focus is to improve the training process of such a network. Note that in the literature, many works adopt this idea but only attempt to quantize the weights of a network while keeping the activations to 32-bit floating point~\cite{zhou2017incremental, courbariaux2015binaryconnect, zhu2016trained}. Although this treatment leads to lower performance decrease comparing to its full-precision counterpart, it still needs substantial amount of computational resource requirement to handle the full-precision activations. Thus, our work targets the problem of training network with \textit{both low-bit quantized weights and activations.}


The solutions proposed in this paper contain three components. They can be applied independently or jointly. The first method is to adopt a two-stage training process. At the first stage, only the weights of a network is quantized. After obtaining a sufficiently good solution of the first stage, the activation of the network is further required to be in low-precision and the network will be trained again. Essentially, this progressive approach first solves a related sub-problem, i.e., training a network with only low-bit weights and the solution of the sub-problem provides a good initial point for training our target problem. Following the similar idea, we propose our second method by performing progressive training on the bit-width aspect of the network. Specifically, we incrementally train a serial of networks with the quantization bit-width (precision) gradually decreased from full-precision to the target precision. The third method is inspired by the recent progress of mutual learning \cite{zhang2017deep} and information distillation \cite{romero2014fitnets, hinton2015distilling, parisotto2016actor, zagoruyko2016paying, ba2014deep}. The basic idea of those works is to train a target network alongside another guidance network. For example, The works in \cite{romero2014fitnets, hinton2015distilling, parisotto2016actor, zagoruyko2016paying, ba2014deep} propose to train a small student network to mimic the deeper or wider teacher network. They add an additional regularizer by minimizing the difference between student's and teacher's posterior probabilities~\cite{hinton2015distilling} or intermediate feature representations~\cite{ba2014deep, romero2014fitnets}. It is observed that by using the guidance of the teacher model, better performance can be obtained with the student model than directly training the student model on the target problem.  Motivated by these observations, we propose to train a full-precision network alongside the target low-precision network. Also, in contrast to standard knowledge distillation methods, we do not require to pre-train the guidance model. Rather, we allow the two models to be trained jointly from scratch since we discover that this treatment enables the two nets adjust better to each other. 

Compared to several existing works that achieve good performance when quantizing both weights and activations~\cite{wu2016quantized, zhou2016dorefa, hubara2016binarized, rastegari2016xnor}, our methods is more considerably scalable to the deeper neural networks \cite{he2016deep, he2016identity}. For example, some methods adopt a layer-wise training procedure \cite{wu2016quantized}, thus their training cost will be significantly increased if the number of layers becomes larger. In contrast, the proposed method does not have this issue and we have experimentally demonstrated that our method is effective with various depth of networks (\ie, AlexNet, ResNet-50).



%
%
%
%
%
%



%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
