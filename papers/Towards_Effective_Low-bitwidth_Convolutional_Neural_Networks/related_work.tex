\section{Related work}
\label{sec:related_work}

Several methods have been proposed to compress deep models and accelerate inference during testing. We can roughly summarize them into four main categories: limited numerial percision, low-rank approximation, efficient architecture design and network pruning. 

\textbf{Limited numerical precision}
When deploying DNNs into hardware chips like FPGA, network quantization is a must process for efficient computing and storage. Several works have been proposed to quantize only parameters with high accuracy~\cite{courbariaux2015binaryconnect, zhu2016trained,  zhou2017incremental}. Courbariaux \etal \cite{courbariaux2015binaryconnect} propose to constrain the weights to binary values (\ie, -1 or 1) to replace multiply-accumulate operations by simple accumulations. To keep a balance between the efficiency and the accuracy, ternary networks~\cite{zhu2016trained} are proposed to keep the weights to 2-bit while maintaining high accuracy. Zhou \etal \cite{zhou2017incremental} present incremental network quantization (INQ) to efficiently convert any pre-trained full-precision CNN model into low-precision whose weights are constrained to be either powers of two or zero. Different from these methods, a mutual knowledge transfer strategy is proposed to jointly optimize the full-precision model and its low-precision counterpart for high accuracy. What's more, we propose to use a progressive optimization approach to quantize both weights and activations for better performance. 

\textbf{Low-rank approximation}
Among existing works, some methods attempt to approximate low-rank filters in pre-trained networks~\cite{kim2015compression, zhang2016accelerating}. In~\cite{zhang2016accelerating}, reconstruction error of the nonlinear responses are minimized layer-wisely, with subject to the low-rank constraint to reduce the computational cost. 
Other seminal works attempt to restrict filters with low-rank constraints during training phrase~\cite{novikov2015tensorizing, tai2015convolutional}. To better exploit the structure in kernels, it is also proposed to use low-rank tensor decomposition approaches~\cite{denton2014exploiting, novikov2015tensorizing} to remove the redundancy in convolutional kernels in pretrained networks.

\textbf{Efficient architecture design}
The increasing demand for running highly energy efficient neural networks for hardware devices have motivated the network architecture design. GoogLeNet~\cite{szegedy2015going} and SqueezeNet~\cite{iandola2016squeezenet} propose to replace 3x3 convolutional filters with 1x1 size, which tremendously increase the depth of the network while decreasing the complexity a lot. ResNet~\cite{he2016deep} and its variants~\cite{zagoruyko2016wide, he2016identity} utilize residual connections to relieve the gradient vanishing problem when training very deep networks. Recently, depthwise separable convolution employed in Xception~\cite{chollet2016xception} and  MobileNet~\cite{howard2017mobilenets} have been proved to be quite effective. Based on it, ShuffleNet~\cite{zhang2017shufflenet} generalizes the group convolution and the depthwise separable convolution to get the state-of-the-art results. 

\textbf{Pruning and sparsity}
Substantial effort have been made to reduce the storage of deep neural networks in order to save the bandwidth for dedicated hardware design. Han~\etal \cite{han2015learning, han2015deep} introduce ``deep compression'', a three stage pipeline: pruning, trained quantization and Huffman coding to effectively reduce the memory requirement of CNNs with no loss of accuracy. Guo~\etal \cite{guo2016dynamic} further incorporate connection slicing to avoid incorrect pruning. More works~\cite{wen2016learning, lebedev2016fast, liu2015sparse} propose to employ structural sparsity for more energy-efficient compression. 


