To evaluate the effectiveness of \projectName, in this section, we introduce the benchmarks and experimental setup as well as the experimental results.
Specifically, we evaluate \projectName by answering the following research questions:
\begin{itemize}[leftmargin=*]
    \item RQ1: How effective is our model re-engineering approach in reusing trained models?
    \item RQ2: Does reusing a re-engineered model incur less overhead than reusing the original model?
    \item RQ3: Does reusing the re-engineered model mitigate the defect inheritance? %

\end{itemize}

\subsection{Experimental Setup}
\label{subsec:setup}
\noindent \textbf{RQ1: How effective is our model re-engineering approach in reusing trained models?} Three representative CNN models are used in this research question, including VGG16~\cite{vgg}, ResNet20, and ResNet50~\cite{resnet}. 
The three CNN models are trained on three public classification datasets, including CIFAR-10~\cite{cifar10}, CIFAR-100~\cite{cifar10}, and ImageNet~\cite{imagenet}.
In total, there are five trained CNN models in this experiment, including VGG16-CIFAR10, VGG16-CIFAR100, ResNet20-CIFAR10, ResNet20-CIFAR100, and ResNet50-ImageNet. Among these trained models, the first four models are publicly available from the third-party GitHub repositories~\cite{cifarmodel}, and the last model is provided by PyTorch~\cite{pytorchmodel}.

Given a trained model for $N$-class classification, we perform model re-engineering to alter the trained model on two types of target problems, including binary and multi-class classification problems.
For the binary classification problem, each class of the trained model corresponds to a target problem. In total, there are $N$ target problems. A re-engineered model needs to classify whether an input belongs to the corresponding class or not.
In this scenario, VGG16-CIFAR10, VGG16-CIFAR100, ResNet20-CIFAR10, and ResNet20-CIFAR100
are altered, and there are 220 re-engineered models in total.
Due to the significant overhead of generating 1000 re-engineered models, ResNet50-ImageNet is not used here.
We count the number of removed weights and compare the accuracy of re-engineered models and trained models on target problems to validate the effectiveness of \projectName. 
Also, we compare \projectName with the state-of-the-art modularization approach~\cite{nnmodularity2022icse} to demonstrate the improvement achieved by our approach.

For the multi-class classification problem, %
a re-engineered model classifies an input into one of the concerning classes.
In this scenario, we use CIFAR-100 and ImageNet as our datasets since there are publicly available schemes for dividing them into superclasses~\cite{cifar10,imagenet_division}.
A small-size model ResNet20-CIFAR100 and a large model ResNet50-ImageNet are chosen for a more comprehensive evaluation. %
Specifically, CIFAR-100 has divided the 100 classes into 20 superclasses, each containing 5 classes with semantically similar labels~\cite{cifar10}.
For ResNet20-CIFAR100, we follow this division; thus, there are 20 target problems, each corresponding to a superclass.
For ResNet50-ImageNet, following the public division~\cite{imagenet_division}, the 1000 classes are divided into 67 superclasses, of which 3 superclasses are discarded because they contain only 1 class. 
The remaining 64 superclasses with a number of classes ranging from 2 to 119 form 64 target problems.
In total, there are 84 re-engineered models.
We count the number of removed weights and compare the accuracy of re-engineered models and trained models on target problems to validate the effectiveness of \projectName. 
Since the modularization approaches~\cite{nnmodularity2022icse,fse2020modularity} are designed for binary classification (i.e., each module performs binary classification) and cannot be applied to multi-class classification directly, we compare \projectName with the method of retraining from scratch. %

When re-engineering an original model on a target problem, we follow the settings of our baselines~\cite{ReMos,cifarmodel} to divide the target dataset into training and testing sets. The training set is used to search for a candidate, and the testing set is used to evaluate the candidate.
The major parameters in \projectName include weighting factor $\alpha$ (see Equation \ref{eq:loss}) and learning rate $\xi$ (see Equation \ref{eq:update}). 
The appropriate values of $\alpha$ and $\xi$ could vary from different trained models and are generally set to 1.0 and 0.05, respectively.
The detailed settings and their impact on model re-engineering are described in the project webpage~\cite{seam}.



\noindent \textbf{RQ2: Does reusing a re-engineered model incur less overhead than reusing the original model?} 
In this experiment, the trained models and re-engineered models from RQ1 are reused, and we compare the reuse overhead of re-engineered models with the original models.
Two metrics are used to measure the reuse overhead, including the number of floating point operations (FLOPs)~\cite{li2016pruning,luo2017thinet} and time cost for inference.
An open-source tool fvcore~\cite{fvcore} is used to calculate the FLOP.
Regarding inference time cost, an open-source tool DeepSparse~\cite{deepsparse} is used to run both original and re-engineered models and compute the inference time cost.

\noindent \textbf{RQ3: Does reusing the re-engineered model mitigate the defect inheritance?} 
In transfer learning, a pre-trained model generally has a large number of weights and classifications, and a target problem has insufficient data. Therefore, VGG16 and ResNet20 are not suitable to be transferred, and CIFAR-10 and CIFAR-100 are unsuitable for target problems.
Following the state-of-the-art approach ReMos~\cite{ReMos}, two widely-used transfer learning CNN models, ResNet18 and ResNet50, are used as trained models (i.e., the pre-trained models in transfer learning), which are trained on ImageNet and are provided by PyTorch~\cite{pytorchmodel}.
Five popular transfer learning datasets are used as target datasets, including MIT Indoor Scenes~\cite{tfdata_scenes}, Caltech-UCSD Birds~\cite{tfdata_birds}, 102 Category Flowers~\cite{tfdata_flowers}, Standford 40 Actions~\cite{tfdata_actions}, and Standford Dogs~\cite{tfdata_dogs}.

We first apply \projectName to alter the trained model on the target dataset, resulting in a re-engineered model.
Then we use the standard fine-tuning approach~\cite{transfer1, transfer2} to fine-tune the re-engineered model on the target dataset, resulting in a fine-tuned model.
We compare \projectName with two baselines, standard fine-tuning~\cite{transfer1, transfer2} and the state-of-the-art approach ReMos~\cite{ReMos}. %
Standard fine-tuning fine-tunes all of the trained model's weights on the target dataset.
ReMos first sets a trained model's weights irrelevant to the target problem to zeros, and then uses standard fine-tuning to fine-tune the sliced model on the target dataset, resulting in a fine-tuned model.
Following the setup of ReMos, we use accuracy (ACC) and defect inheritance rate (DIR) to measure and compare the effectiveness of \projectName and the baselines.
The accuracy is computed as the correct classification rate on the target dataset $D^T$:
\begin{gather}
    ACC= \frac{1}{|D^T|} \sum_{(x,y)\in D^T} \mathbbm{1}[f(x)=y].
\end{gather}
The defect inheritance rate is computed as the misclassification rate on a set of malicious inputs $S^M$:
\begin{gather}
    DIR=\frac{1}{|S^M|} \sum_{(\hat{x},y)\in S^M} \mathbbm{1}[f(\hat{x})\ne y].
\end{gather}
Same as ReMos\cite{ReMos}, open source tool \textit{advertorch}~\cite{advertorch} is used to generate $S^M$
based on the trained model and $D^T$.
We use the same parameters as ReMos when using advertorch.

In this experiment, we set the learning rate $\xi{=}0.05$ and weighting factor $\alpha{=}0.5$.
Regarding the standard fine-tuning approach and ReMos, we also use the open source project~\cite{remosproject} published by ReMos.

All the experiments are conducted on Ubuntu 20.04 server with 64 cores of 2.3GHz CPU, 128GB RAM, and NVIDIA Ampere A100 GPUs with 40 GB memory.

\subsection{Experimental Results}
\label{subsec:exp_result}
\noindent \textbf{RQ1: How effective is our model re-engineering approach in reusing trained models?}

\begin{figure}[!th]
    \centering
    \includegraphics[width=\columnwidth]{figures/convergence.pdf}
    \caption{The convergence process of \projectName on binary (left sub-figure) and multi-class (right sub-figure) classification problems. }
    \label{fig:convergence}
\end{figure}


In this research question, we present the model re-engineering results of \projectName 
for two types of target problems (i.e., binary and multi-class classification).
Figure \ref{fig:convergence} shows the convergence process of \projectName on two types of target problems.
For instance, the left sub-figure shows the trend of weight retention rate and classification accuracy along with search rounds during re-engineering VGG16-CIFAR10 on a binary classification problem.
The weight retention rate descends quickly in the first 50 rounds and then gradually converges.
Although many weights are removed, the re-engineered model maintains a comparable accuracy to the original model.
The right sub-figure depicts the convergence process of ResNet20-CIFAR100 on a 5-class classification problem.
Similar to re-engineering VGG16-CIFAR10, the weight retention rate descends quickly in the first 100 rounds and then gradually converges.
The difference is that the accuracy of the re-engineered model may be lower than that of the original model at the beginning of search.
The reason is that the target dataset of the 5-class classification problem contains fewer samples than that of the binary classification problem (500 \textit{vs.} 10,000). Thus, the former requires more rounds to optimize the mask and head. As optimization rounds increase, the mask retains more related weights, and the head learns to classify better, so the re-engineered model can recover accuracy and eventually exceed that of the original model.
The time cost of the search varies by the models, target problems, and target datasets.
The sizes of target datasets vary from 500 to 140,000 samples.
For the binary classification problem, each round takes several seconds. 
For the multi-class classification problem, re-engineering ResNet20-CIFAR100 takes 2s per round.
For ResNet50-ImageNet, as each superclass contains a different number of classes, the time cost varies from several seconds to a few minutes per round.
In this example, re-engineering VGG16-CIFAR10 and ResNet20-CIFAR100 takes 4s and 2s per round, respectively.



Table \ref{tab:rq1_weight} shows the results regarding the number of weights for the original and re-engineered models.
For each trained model, we count the number of the original model's weights and the number of weights retained (i.e., non-zero weights) in the re-engineered model\footnote{As a head contains a negligible number of weights (e.g., 0.43\% at most) compared to the original model, the head weight count is omitted in the experiment.}.
For instance, VGG16-CIFAR10 is altered on 10 target problems, resulting in 10 re-engineered models.
The average number of weights retained (i.e., non-zero weights) in a re-engineered model is 0.62 million.
Compared to the original model having 15.25 million weights, a re-engineered model retains only 4.07\% of the original model's weights, which means that \projectName achieves a 95.93\% reduction in the number of weights.
It is worth mentioning that, for multi-class modularization, although a re-engineered model requires the classification of more classes, it still has much fewer weights than the trained model. 
For instance, a re-engineered model obtained by altering ResNet20-CIFAR100 can classify five classes; however, the re-engineered model has only an average of 0.04 million weights, and the reduction in the number of weights is 85.71\%.
The reason is that different classes may contain the same features, which means that the weights needed to identify one more class may already be included in the existing weights.
Consequently, for all six trained models, the number of weights retained in re-engineered models is significantly smaller than the number of weights in original models.
On average, for the six trained models, \projectName achieves an 89.89\% reduction in the number of weights.



\input{tables/rq1_weight}
\input{tables/rq1_acc}

Table \ref{tab:rq1_acc} shows the averaged accuracy of original and re-engineered models.
The original and re-engineered models are evaluated on the corresponding target problems. 
Again using VGG16-CIFAR10 as an example, the average accuracy of the 10 re-engineered models on the 10 target problems is 97.12\%.
The original model is also evaluated on the 10 target problems, and the average accuracy is 96.50\%. 
Compared to the original model, the re-engineered models achieve comparable accuracy on target problems, and the averaged accuracy increases by 0.62\%.
The reason for the improvement may be that model re-engineering enables the re-engineered model to fit the target problem during altering the original model.
Note that, the fitting is mainly achieved by removing irrelevant weights instead of training the weights of the original trained model.
On both binary and multi-class classification problems, for all the six trained models, re-engineered models can achieve comparable accuracy to original models, and the averaged accuracy increases by 5.85\%.
Due to space limitation, the detailed results regarding the number of weights and accuracy are available at the project webpage~\cite{seam}.


When comparing \projectName with the existing modularization approach~\cite{nnmodularity2022icse}, we directly use the open source project~\cite{modularization} published by \cite{nnmodularity2022icse}, which decomposes a trained model into modules, each for a binary classification problem.
Since tool \cite{nnmodularity2022icse} and \projectName are implemented on Keras and PyTorch, respectively, they cannot directly alter each other's trained models. 
We attempted to convert PyTorch and Keras trained models to each other; however, the conversion incurs much loss of accuracy (5\% to 10\%) due to the differences in the underlying computation of PyTorch and Keras.
To make the comparison as fair as possible, we run the modules and trained models published by \cite{nnmodularity2022icse} and compare \projectName to \cite{nnmodularity2022icse} based on the results of ResNet20-CIFAR10 and ResNet20-CIFAR100, as the two models are also used in \cite{nnmodularity2022icse}.
Specifically, we analyzed the accuracy and the number of neurons of the original models and modules (re-engineered model of \projectName). 
As modularization~\cite{nnmodularity2022icse} decomposes a CNN model mainly by removing neurons (i.e., setting neurons to zero but retaining all weights) from convolutional layers, we analyzed the number of neurons rather than the number of weights.

\input{tables/binary_icse22}

As shown in Table \ref{tab:icse22}, for both ResNet20-CIFAR10 and ResNet20-CIFAR100, a module retains fewer neurons than the original model; however, the number of neurons in a module is reduced by only 18.59\% on average. In addition, a module retains all the weights of the convolutional layers.
Regarding accuracy, modules achieve a lower accuracy than the trained models on target problems, and the accuracy of a module reduces by 7.25\% on average. 
Compared to modularization~\cite{nnmodularity2022icse}, model re-engineering can remove a large number of weights without impairing the accuracy.
A major reason for the improvement of \projectName over \cite{nnmodularity2022icse} is that \projectName identifies the target problem-related weights more accurately.
\projectName is a search-based approach that identifies the target problem-related weights directly based on the classification accuracy, while \cite{nnmodularity2022icse} identifies the target problem-related weights and neurons based on the neuron activation that indirectly correlates with the accuracy.

\input{tables/rq1_retraining}
We also compare \projectName to model retraining on multi-class classification problems.
Model retraining reuses the architecture and hyperparameters of the trained model to retrain a new model from scratch on the target dataset.
As both model re-engineering and retraining alter/train the same model (architecture) on the same target problem, while the latter may fit more slowly and even run several times, the time cost of the retraining would be higher than that of re-engineering.
Regarding accuracy, as shown in Table \ref{tab:rq1_retraining},
re-engineered models outperform retrained models for both ResNet20-CIFAR100 and ResNet50-ImageNet, and the average improvement is 7.84\%.
The reason for the improvement of model re-engineering may be the difference in the amount of data.
The original model is trained on a large-scale dataset, while the retrained model is trained on a small-scale target dataset.
The model re-engineering alters the original model to fit the target problem; thus, the re-engineered model achieves higher accuracy than the retrained model.



\begin{tcolorbox}[left=2pt,right=2pt,top=2pt,bottom=2pt]
On average, a re-engineered model contains 89.89\% fewer weights than the original model but outperforms the original model in accuracy by 5.85\%.
\end{tcolorbox}


\input{tables/rq2_flop}
\input{tables/rq2_time}

\noindent \textbf{RQ2: Does reusing a re-engineered model incur less overhead than reusing the original model?}
\label{subsec:results:rq2}

One of the benefits of model re-engineering is to reduce the reuse overhead.
As mentioned in Section \ref{subsec:setup}, the number of FLOPs and inference time cost are used to measure the reuse overhead.
We evaluated the original and re-engineered models from RQ1 on the two metrics to answer this research question.

Table \ref{tab:rq2_flop} shows the number of FLOPs required by the original and re-engineered models to classify an image with resolution $32{\times}32$, respectively.
Note that, following the related work~\cite{pruning_hansong,louizos2018learning}, when computing the number of FLOPs required by a re-engineered model with a sparse weight matrix, only the computations involved in non-zero weights are considered.
For instance, despite having the same number of weights as the original model, a re-engineered model obtained by altering VGG16-CIFAR10 has 95.93\% (see Table \ref{tab:rq1_weight}) of its weights set to zero.
As the calculations associated with these zero weights can be eliminated by special libraries~\cite{han2016eie}, the calculations associated with these weights are not considered when calculating FLOPs.
VGG16-CIFAR10 requires 314.28 million FLOPs, while the average number of FLOPs required by a re-engineered model is 75.53 million. 
\projectName achieves 75.97\% reduction in terms of FLOPs.
On average, for the six trained models, \projectName reduces the FLOPs by 74.71\%.






To verify that the reduction in the number of FLOPs can reduce the inference time cost, the open-source library DeepSparse~\cite{deepsparse} is used to deploy and run the original and re-engineered models. 
Given an input with batch size 16, each re-engineered model or original model classifies the input 200 times, and the average time cost of classification is used to measure the inference time cost of a re-engineered model or an original model.
Table \ref{tab:rq2_time} shows the average inference time cost of each trained model and its corresponding re-engineered models. 
For instance, the inference time cost of VGG16-CIFAR10 is 6.82ms/batch, which means that VGG16-CIFAR10 requires 6.82ms to classify an input with batch size 16. The re-engineered model obtained by altering VGG16-CIFAR10 incurs an average of 3.79ms/batch inference time cost. The reduction in inference time cost is 44.43\% (calculated by $(1-3.79/6.82)*100$).
For all the six trained models, \projectName achieves an average of 42.41\% reduction in inference time cost, which demonstrates that the reduction in the number of weights and FLOPs can reduce the inference time cost.

FLOP focus on the computation of neural network layers containing weights.
While apart from the layers containing weights, the time cost for inference also involves other operations, such as activation functions, dropout, tensor reshape, and so on.  
Therefore, the reductions in the number of FLOPs and the time cost differ. %

\begin{tcolorbox}[left=2pt,right=2pt,top=2pt,bottom=2pt]
Reusing a re-engineered model incurs less reuse overhead than reusing an original model, while achieving even higher accuracy in inference than the original model.
\end{tcolorbox}

\noindent \textbf{RQ3: Does reusing the re-engineered model mitigate the defect inheritance?}


\begin{figure}
    \centering
    \includegraphics[width=\columnwidth]{figures/cv_adv_resnet18.pdf}
    \caption{The accuracy (ACC) and defect inheritance rate (DIR) on ResNet18.}%
    \label{fig:adv_resnet18}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=\columnwidth]{figures/cv_adv_resnet50.pdf}
    \caption{The accuracy (ACC) and defect inheritance rate (DIR) on ResNet50.}
    \label{fig:adv_resnet50}
\end{figure}

In this experiment, following ReMos, we evaluate \projectName and the baselines using two metrics, i.e., accuracy (\textit{ACC}) and defect inheritance rate (\textit{DIR}).
Figure \ref{fig:adv_resnet18} and Figure \ref{fig:adv_resnet50} show the results of ResNet18 and ResNet50 on all of five datasets. 
In each figure, the first row displays the accuracy (\textit{ACC}) of the fine-tuned model, and the second row displays the defect inheritance rate (\textit{DIR}) of the fine-tuned model.
On average, for ResNet18, the ACC and DIR achieved by the standard fine-tuning, ReMos, and \projectName are (80\%, 79\%), (74\%, 40\%), and (79\%, 19\%), respectively. For ResNet50, the ACC and DIR are (85\%, 66\%), (82\%, 19\%), (84\%, 14\%), respectively.

The standard fine-tuning approach achieves higher accuracy on the target datasets than \projectName and ReMos; however, the cost is much higher DIRs.
Compared to the standard fine-tuning approach, both \projectName and ReMos can achieve lower DIRs at the cost of a small accuracy loss, indicating that removing the weights that are not relevant to the target dataset can reduce DIRs and improve the robustness of the fine-tuned model.
Overall, for the two models on five datasets, the averaged DIRs for fine-tuning the re-engineered model and fine-tuning the original model (i.e., standard fine-tuning approach) are 16\% and 73\%, respectively.
The reduction in DIR is 57\%, demonstrating the effectiveness of \projectName in reducing defect inheritance.

Compared to ReMos, \projectName can achieve lower DIRs and higher ACC.
For instance, for ResNet18, the average DIRs achieved by \projectName and ReMos are 19\% and 40\%, respectively. The DIR achieved by \projectName is roughly half of that achieved by ReMos.
Regarding ACC, the average ACC achieved by \projectName and ReMos is 79\% and 74\%, respectively. 
Overall, for the two models on five datasets, the average DIRs and ACC for \projectName and ReMos are (16\%, 82\%) and (29\%, 78\%), respectively.
\projectName is 13\% lower and 4\% higher than ReMos in terms of DIR and ACC, respectively.
The reason for the improvement in DIR achieved by \projectName is the considerable reduction in the number of weights.
ReMos removes only 10\% and 3\% weights for ResNet18 and ResNet50, respectively.
Compared to ReMos, \projectName can remove more irrelevant weights. 
The reduction in the number of weights is about 50\% for both ResNet18 and ResNet50.

\begin{figure}
    \centering
    \includegraphics[width=\columnwidth]{figures/cv_adv_resnet18_dropout.pdf}
    \caption{The accuracy (ACC) and defect inheritance rate (DIR) on ResNet18 with Dropout layers.}
    \label{fig:adv_resnet18_dropout}
\end{figure}

It is worth mentioning that there are some differences between the results shown in Figure \ref{fig:adv_resnet18} and Figure \ref{fig:adv_resnet50} and the results shown in ReMos~\cite{ReMos}, especially in terms of DIR.
For instance, for ResNet18, the average DIRs achieved by ReMos shown in \cite{ReMos} and our work are 15\% and 40\%, respectively. 
The reason for the differences is that ReMos uses additional Dropout layers for fine-tuning while ours does not.
To make a more comprehensive comparison of ReMos and \projectName, we follow the experimental setup of ReMos~\cite{ReMos} and plot the results on ResNet18 in Figure \ref{fig:adv_resnet18_dropout}.
As shown in Figure \ref{fig:adv_resnet18_dropout}, after adding Dropout layers, both \projectName and the baselines achieve better results, as Dropout layers help increase the robustness of models. 
The average DIRs achieved by the standard fine-tuning approach, ReMos, and \projectName are 44\%, 20\%, and 12\%, respectively. 
Consistent with the above conclusion, our approach can outperform %
ReMos.
Moreover, we observe that the DIRs of ResNet50 are lower than that of ResNet18. 
The reason for this could be that the increased number of weights helps increase the robustness.
This observation aligns with the prior works~\cite{ReMos,adv_attack}.





\begin{tcolorbox}[left=2pt,right=2pt,top=2pt,bottom=2pt]
Overall, \projectName inherits much fewer defects compared to standard fine-tuning and the state-of-the-art approach. %
\end{tcolorbox}