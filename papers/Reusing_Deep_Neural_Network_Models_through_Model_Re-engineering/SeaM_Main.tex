\documentclass[conference]{IEEEtran}
\IEEEoverridecommandlockouts
% The preceding line is only needed to identify funding in the first footnote. If that is unneeded, please comment it out.
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}


\usepackage[ruled,vlined,linesnumbered]{algorithm2e}
\usepackage{setspace}
\usepackage{xspace}
\usepackage{enumitem}
\usepackage{url}
\usepackage{multirow}
\usepackage{hyperref}
\hypersetup{hidelinks} 
\usepackage{booktabs}
\usepackage{tcolorbox}
\usepackage{bbm}
\usepackage{balance}


\newcommand{\Red}[1]{\textcolor[rgb]{1.00,0.00,0.00}{#1}}
\newcommand{\Cyan}[1]{\textcolor[rgb]{0.00,1.00,1.00}{#1}}
\newcommand{\sun}[1]{ \Green{[HL:#1]}}
\newcommand{\hy}[1]{\Red{[HY:#1]}}
\newcommand{\tbd}[1]{\Red{#1}} 	
\newcommand{\qi}[1]{\Blue{[QI: #1]}}
\newcommand{\xiang}[1]{\Cyan{[GX: #1]}}
\newcommand{\Blue}[1]{\textcolor[rgb]{0.00,0.00,1.00}{#1}}
\newcommand{\Green}[1]{\textcolor[rgb]{0.6,0.10,0.5}{#1}}

\def \projectName {\textsc{SeaM}\xspace}   


\begin{document}

\title{Reusing Deep Neural Network Models through Model Re-engineering}

\author{\IEEEauthorblockN{Binhang Qi*\thanks{* Also with Beijing Advanced Innovation Center for Big Data and Brain Computing, Beihang University, Beijing 100191, China.}
}
\IEEEauthorblockA{\textit{SKLSDE Lab, Beihang University} \\
Beijing, China \\
binhangqi@buaa.edu.cn}

\\

\IEEEauthorblockN{Hongyu Zhang}
% \IEEEauthorblockA{\textit{The University of Newcastle}\\
% NSW, Australia \\
% hongyu.zhang@newcastle.edu.au}
\IEEEauthorblockA{\textit{Chongqing University} \\
 Chongqing, China \\
hyzhang@cqu.edu.cn}

\and

\IEEEauthorblockN{Hailong Sun*\textsuperscript{\textdagger}\thanks{\textsuperscript{\textdagger} Corresponding authors: Hailong Sun and Xiang Gao.}}
\IEEEauthorblockA{\textit{SKLSDE Lab, Beihang University} \\
Beijing, China \\
sunhl@buaa.edu.cn}

\\

\IEEEauthorblockN{Zhaotian Li*}
\IEEEauthorblockA{\textit{SKLSDE Lab, Beihang University} \\
Beijing, China \\
lizhaotian@buaa.edu.cn}

\and

\IEEEauthorblockN{Xiang Gao*\textsuperscript{\textdagger}}
\IEEEauthorblockA{\textit{SKLSDE Lab, Beihang University} \\
Beijing, China \\
xiang\_gao@buaa.edu.cn}

\\

\IEEEauthorblockN{Xudong Liu*}
\IEEEauthorblockA{\textit{SKLSDE Lab, Beihang University} \\
Beijing, China \\
liuxd@act.buaa.edu.cn}
}

\maketitle
\pagestyle{plain}
\thispagestyle{plain}

\begin{abstract}
Training deep neural network (DNN) models, which has become an important task in today's software development, is often costly in terms of computational resources and time.
With the inspiration of software reuse, building DNN models through reusing existing ones has gained increasing attention recently.
Prior approaches to DNN model reuse have two main limitations: 1) reusing the entire model, while only a small part of the model's functionalities (labels) are required, would cause much overhead (e.g., computational and time costs for inference), and 2) model reuse would inherit the defects and weaknesses of the reused model, and hence put the new system under threats of security attack.
To solve the above problem, we propose \projectName, a tool that re-engineers a trained DNN model to improve its reusability.
Specifically, given a target problem and a trained model, \projectName utilizes a gradient-based search method to search for the model's weights that are relevant to the target problem.
The re-engineered model that only retains the relevant weights is then reused to solve the target problem.
Evaluation results on widely-used models show that the re-engineered models produced by \projectName only contain 10.11\% weights of the original models, resulting 42.41\% reduction in terms of inference time.
For the target problem, the re-engineered models even outperform the original models in classification accuracy by 5.85\%.
Moreover, reusing the re-engineered models inherits an average of 57\% fewer defects than reusing the entire model.
We believe our approach to reducing reuse overhead and defect inheritance is one important step forward for practical model reuse.
\end{abstract}

\begin{IEEEkeywords}
model reuse, deep neural network, re-engineering, DNN modularization
\end{IEEEkeywords}


\section{Introduction}
\label{sec:intro}
\input{introduction}

\section{Motivating Examples}
\label{sec:motivation}
\input{motivation}

\section{Our Approach}
\label{sec:approach}
\input{approach}

\section{Experiments}
\label{sec:exp}
\input{experiment}

\section{Threats to Validity}
\label{sec:threats}
\input{threats}

\section{Related Work}
\label{sec:related}
\input{related_work}

\section{Conclusion}
\label{sec:conclusion}
\input{conclusion}

\section*{Acknowledgement}
This work was supported partly by National Natural Science Foundation of China under Grant Nos.(61932007, 61972013, 62141209, 62202026) and Australian Research Council (ARC) Discovery Project DP200102940 and sponsored by Huawei Innovation Research Plan.


\balance

\bibliographystyle{IEEEtran}
\bibliography{reference}

\end{document}
