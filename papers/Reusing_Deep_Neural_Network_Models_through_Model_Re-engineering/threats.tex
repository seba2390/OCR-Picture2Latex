\textbf{External validity:}
Threats to external validity relate to the generalizability of our results. 
While the notion of re-engineering a trained model to improve its reusability %
is general, we have only evaluated our approach on CNN models in this paper.
The effectiveness on other types of DNNs, such as LSTM and transformer, remains to be evaluated. 
However, during the search, the objects removed are weights, not CNN-specific structures such as convolutional kernels.
Also, the search is guided by the classification accuracy and the number of retained weights.
Therefore, the principles of our proposed approach are not specific to CNN and %
are applicable to other types of DNNs as well. We will further investigate it in our future work.

\textbf{Internal validity:}
An internal threat comes from the choice of trained models and datasets. To mitigate this threat, we use four representative trained CNN models and evaluate \projectName on eight well-organized and widely-used datasets.

\textbf{Construct validity:}
A threat relates to the suitability of our evaluation metrics. 
Evaluating the quality of DNN models remains an open problem.
Measuring only the misclassification rate of the adversarial samples may not be comprehensive enough. 
However, the misclassification rate of adversarial samples is a representative metric and has also been widely used in  related work~\cite{ReMos, defect2}.
