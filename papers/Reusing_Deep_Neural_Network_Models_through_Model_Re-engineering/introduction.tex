Software reuse is the process of using existing software artifacts that would be otherwise created from scratch~\cite{software_reuse_1,software_reuse_2,software_reuse_3}, which is widely deemed essential to improve software quality and development productivity. 
Instances of software reuse include the reuse of software libraries, components, APIs, etc. 
As today's software systems are increasingly incorporating AI techniques (e.g., deep learning),  training DNN models has become an important task in the software development lifecycle. 
However, training DNN models is often known to be 
very costly, especially for models with billions of parameters and large datasets.
To solve this problem, with the inspiration of software reuse, the software engineering community is paying more attention to DNN model reuse~\cite{icse21discriminiate, ji2018model, modeldiff, ReMos, nnmodularity2022icse,fse2020modularity, qi2022patching}.

A trained model can be directly reused if it fits %
the {target problem} domain. %
However, reusing entire trained models may cause large %
overhead (e.g., inference time).
Just like traditional software libraries which implement a large number of functions, a trained model may also have multiple functionalities (e.g., classification for multiple categories).
When reusing a trained model, often only part of functionalities are required to solve the target problem.
For instance, Google Vision API provides the service of  multi-class classification with around 20,000 classes, but not all classes are necessary in practical scenarios. Suppose that a developer needs to build a fire alarm application~\cite{fireapp} for determining whether a given image indicates ``fire''.
Although only two classes (``fire'' and ``non-fire'') are needed, if the developer directly invokes Google Vision API, all the 20,000 classes will be involved, which can incur much inference overhead caused by the unnecessary weights/neurons in the underlying DNN model. 


A model trained to solve a similar problem can also be indirectly reused via transfer learning~\cite{transfer_nips,devlin2018bert}.
Transfer learning consists of taking relevant features learned on a similar problem and optionally fine-tuning the trained model using the dataset of the target problem. %
Although effective in classification accuracy and training efficiency, reusing trained models may inherit their defects %
~\cite{dlfuzz,deepxplore,defect3}.
It has been shown that AI models are notoriously brittle to small perturbations on input data~\cite{huang2011adversarial, deepxplore},
which allows attackers to craft adversarial examples for malicious attacks.
When reusing a model, the weakness of a trained model can be inherited, and hence putting the system under the threats of adversarial attacks.



To address the weaknesses of existing model reuse methods, one idea is to only reuse some parts of a trained model (e.g., by eliminating some weights or neurons) that are relevant to the target problem, as the weaknesses correlate with the weights of a trained model~\cite{rezaei2019target, ReMos}.
Identifying the relevant weights/neurons can be achieved with the fundamental concept of \textit{re-engineering} in software engineering
~\cite{rosenberg1996software,chikofsky1990reverse}, which aims to improve software maintainability and reusability by
enhancing or altering existing software.
Borrowing the idea of software re-engineering, we propose \emph{model re-engineering} for DNN models, which searches for the target problem-related weights with the guidance of target problem-related metrics (e.g., classification accuracy) and removes irrelevant weights from an \textit{original model} (i.e., trained model), resulting in a re-engineered model.
When solving a certain problem through direct or indirect reuse, the re-engineered model, which retains only relevant weights to certain functionalities (e.g., a part of classes in classification), is reused, hence reducing the reuse overhead and mitigating the defect inheritance. %

Existing work, including model modularization~\cite{nnmodularity2022icse,fse2020modularity,qi2022patching} and model slicing~\cite{ReMos}, has preliminarily explored the idea of reusing part of trained models based on neuron activation and %
neuron coverage~\cite{deepxplore, dlfuzz}.
For instance, relying on neuron coverage, model slicing~\cite{ReMos} first computes the relevance between weights and the target problem, then deletes the irrelevant weights.
Unfortunately, due to the lack of interpretability of DNN models, the effectiveness of using neuron coverage is still questionable~\cite{neuron_activation,li2019structural}.
The neuron coverage-based work~\cite{nnmodularity2022icse,fse2020modularity,ReMos} is not accurate enough in identifying relevant weights and hence prefers to be conservative in removing weights, i.e., only a small number of weights are removed to avoid removing relevant weights.
Therefore, the models %
obtained with the existing approaches~\cite{nnmodularity2022icse,fse2020modularity,ReMos} will retain lots of irrelevant weights or neurons, having the limitations of reuse overhead and defect inheritance.
CNNSplitter~\cite{qi2022patching} introduces the first search-based approach for modularizing CNNs. As CNNSplitter achieves modularization by searching for relevant convolution kernels with a genetic algorithm, this approach cannot be applied directly to other neural networks, such as the fully connected neural networks.
%\sun{say somwthing about [10]...}




In this paper, we propose \projectName, a \textbf{Sea}rch-based \textbf{M}odel re-engineering approach that can accurately identify relevant weights and hence removes as many irrelevant weights as possible.
Different from the neuron coverage-based approaches~\cite{nnmodularity2022icse,fse2020modularity,ReMos}, \projectName is directly guided by the target problem-related metrics, e.g., classification accuracy, to search for the relevant weights.
Moreover, \projectName applies a gradient-based search method to identify relevant weights, which is more general and efficient than CNNSplitter~\cite{qi2022patching}.
Specifically, \projectName consists of three components: search space, performance estimation strategy, and search strategy.
The search space consists of the masks of all candidate re-engineered models. The mask of a candidate records which weights of the original model should be retained or removed. %
The performance estimation strategy defines the objective function of the search as the weighted sum of the candidate's weight retention rate and its cross-entropy loss on the target problem's dataset (denoted as target dataset).
The objective function is used to evaluate a candidate's performance, and the objective function value is sent to the search strategy to guide the next round of search.
The search strategy applies a gradient-based search method to explore the search space efficiently. 
In each search round, the search strategy finds a candidate with better performance by minimizing the objective function value. %
\projectName performs the search and estimation processes iteratively, and stops %
when the objective function value converges.
The candidate with the minimal objective function value will be regarded %
as the resultant re-engineered model.
The re-engineered model can be reused directly, or indirectly via fine-tuning, which helps reduce reuse overhead and lower the risk of defect inheritance while achieving comparable performance (e.g., classification accuracy) to the original model.

We evaluate \projectName using four representative CNN models on eight widely-used datasets. 
The experimental results first demonstrate that \projectName can accurately identify relevant weights and thus remove a large number of irrelevant weights. 
On average, a re-engineered model contains 89.89\% fewer weights than the original model, and outperforms the original model by 5.85\% in classification accuracy.
Moreover, reusing a re-engineered model incurs less reuse overhead than reusing an original model, e.g., the average reduction in time cost for inference is 42.41\%. Regarding defect inheritance, reusing the re-engineered model inherits an average of 57\% fewer defects than reusing the original model.

The main contributions of this work are as follows:  
\begin{itemize}[leftmargin=*]
    \item We propose the notion of \textit{model re-engineering}, which re-engineers a trained deep learning model %
    to improve its reusability.
    \item We propose a search-based model re-engineering approach named \projectName, which can accurately identify the weights relevant to a target problem and hence allows the re-engineered model to retain as few irrelevant weights as possible. \projectName can reduce the reuse overhead and lower the risk of defect inheritance in model reuse.
    \item We conduct extensive experiments using four representative CNN models on eight widely-used datasets. The results show that \projectName can remove a large number of irrelevant weights from the original models. %
    Also, the experiments demonstrate the effectiveness of \projectName in overcoming the limitations of existing approaches.%
\end{itemize}
%\sun{We should cite our ASE paper in intro and related work sections}

\begin{figure}[t]
    \centering
    \includegraphics[width=\columnwidth]{figures/motivating_1.pdf}
    \caption{An example of direct model reuse.}%
    \label{fig:motivation_1}
\end{figure}