
Reusing a re-engineered model containing fewer irrelevant weights rather than an original model has several benefits. In this section, we introduce the applications and benefits of model re-engineering with two examples.

\subsection{Reducing reuse overhead in direct reuse}
\label{subsec:motivation_1}

When a trained model satisfies the requirement of a target problem, a common way of reuse is to reuse the entire trained model on the target problem directly.
However, there may be redundancy in the functionalities provided by the trained model~\cite{nnmodularity2022icse,mltest}.
Redundancy in a trained model's functionality implies redundant weights, which may incur significant \textit{reuse overhead}, including computational and time costs for inference, that is unnecessary for the target problem.

As shown in Figure \ref{fig:motivation_1}, a simple fire alarm application~\cite{fireapp} is used to illustrate the problem.
In this example, the developer reuses a trained model (by calling the Google \texttt{label\_detection} API) to classify an input image.
An alarm will be triggered if the top-3 classification labels returned by the trained model include the keyword ``fire''.
The requirement of the target problem is to classify an image into ``fire'' or ``non-fire'', while the reused trained model classifies an image into one of around 20,000 classes.
As different weights could recognize features of different classes~\cite{yamashita2018convolutional,bau2020understanding}, only a few relevant weights recognize the features of ``fire''.
However, when reusing the trained model for inference, a lot of irrelevant weights are loaded into memory and involved in computation to produce intermediate results, incurring memory, computational, and time costs.

The example demonstrates that the requirement of a target problem may be only a small part of a trained model's functionality.
Model re-engineering can remove part of the original model's weights irrelevant to the target problem and allows developers to reuse only the relevant weights.
In this example, 
the weights irrelevant to the target problem are removed, resulting in a re-engineered model that only classifies ``fire'' and ``non-fire''.
Compared to directly reusing the trained model, reusing the re-engineered model containing fewer weights could reduce the reuse overhead.

\subsection{Mitigating defect inheritance in transfer learning}

\begin{figure}[t]
    \centering
    \includegraphics[width=\columnwidth]{figures/motivating_2.pdf}
    \caption{Fine-tuning a publicly available trained model. Inherited defects could be exploited by 
    attackers. }
    \label{fig:motivation_2}
\end{figure}

When a trained model cannot satisfy the requirement of a target problem, a common form of reuse is to transfer learning~\cite{transfer1,transfer2,transfer3}.
That is, a developer reuses a trained model and fine-tunes the trained model on the target dataset to build a fine-tuned model that satisfies the requirement.
This form of reuse is widely-used and effective; however, it faces the problem of defect inheritance~\cite{ReMos,defect1,defect2,defect3}.
An example shown in Figure \ref{fig:motivation_2} is used to illustrate the defect inheritance and potential attacks to face.
In this example, a public model trained on ImageNet~\cite{imagenet} can perform classification with 1000 classes (including 59 bird classes~\cite{van2015building}).
To build a model for classifying birds with 200 classes, a developer reuses the trained model and fine-tunes the trained model on the target dataset Caltech-UCSD Birds~\cite{tfdata_birds}.
During fine-tuning, most of the weights in the pre-trained model are retained in the fine-tuned model.
The adversarial examples that can fool the public trained model are still likely to be able to fool the fine-tuned model, which is called defect inheritance~\cite{ReMos,defect1,defect2,defect3}.


The major reason for defect inheritance is indiscriminate reuse~\cite{rezaei2019target, ReMos}.
Specifically, in conventional transfer learning, all the trained model's weights are reused, including both the relevant and the irrelevant ones to the target problem.
As the target dataset is usually not very large, fine-tuning will not have much effect on changing the weights irrelevant to the target problem. As a result, 
the defects %
are mostly inherited in the fine-tuned model~\cite{liu2019wealthadapt, ReMos}.%

Model re-engineering alters the original model by removing irrelevant weights, thus avoiding the inheritance of defects associated with these weights when the re-engineered model is reused.
In this example, a re-engineered model retains only the weights relevant to the features of ``bird''.
As a result, compared to reusing the original model, reusing the re-engineered model can reduce the defect inheritance while achieving comparable accuracy.

