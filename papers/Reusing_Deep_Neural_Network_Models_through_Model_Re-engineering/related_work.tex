\textit{Reusing trained DNN models:} 
Our work is related to reusing DNN models, including direct reuse~\cite{icse21discriminiate, ji2018model} and transfer learning~\cite{transfer_nips,guo2019spottune}.
The work related to direct reuse recommends a trained model for developers and allows developers to reuse the model on the target problem directly.
For instance, SDS~\cite{icse21discriminiate} evaluates trained models using a few efficient test data that could discriminate multiple trained models and then recommends the best one to reuse.
Transfer learning techniques reuse a model trained to solve a similar problem and fine-tune the reused model on the target problem.
For instance, ResNet~\cite{resnet} trained on ImageNet for 1000-class classification is widely reused to develop new models for various target problems by fine-tuning its weights on the target datasets~\cite{kornblith2019better,guo2019spottune}.
The techniques mentioned above support model reuse; however, they reuse the entire trained model or the vast majority of model's weights.
In contrast, this work allows developers to reuse only the target problem-related weights, thus reducing reuse overhead and defect inheritance.

\textit{DNN modularization and slicing:}
Similar to our work, DNN modularization~\cite{nnmodularity2022icse, fse2020modularity} and slicing~\cite{ReMos} attempt to reuse part of trained models. 
For instance, DNN modularization~\cite{nnmodularity2022icse, fse2020modularity} decomposes a trained model into modules based on neuron activation~\cite{neuron_activation,li2019structural}. A module retains part of trained model's neurons and can be reused to solve a binary classification problem.
Relying on neuron coverage~\cite{neuron_activation,li2019structural}, DNN slicing~\cite{ReMos} removes irrelevant weights and reuses the slice with relevant weights for fine-tuning. 
Compared to DNN modularization and slicing, our work is search-based model re-engineering, which can remove much more irrelevant weights and hence reduce more reuse overhead and defect inheritance. Our previous work CNNSplitter~\cite{qi2022patching} concerns the modularization of CNN models through searching with genetic algorithms and fixing the weakness of a model by replacing the corresponding part with a better module. In contrast, this work can realize the modularization of general neural network models and the searching algorithm is more efficient.

\textit{DNN pruning:}
Iterative magnitude pruning~\cite{pruning_hansong, lottery, rosenfeld2021predictability} is one of the mainstream network pruning techniques, which prunes part of weights that are not important for the original problem to reduce the computational overhead required by inference on the original problem.
Our work removes part of weights that are irrelevant to a target problem to reduce reuse overhead and defect inheritance on the target problem.
Apart from their differences in objectives, iterative magnitude pruning compresses a model by repeatedly removing unimportant weights and retraining the retained weights over several rounds, while \projectName removes irrelevant weights without changing retained weights.

% One of the mainstream network pruning techniques~\cite{pruning_hansong,louizos2018learning} prunes part of a trained model's weights that are not important for the original problem to reduce the computational overhead required by inference on the original problem.
% Our work removes part of a trained model's weights that are irrelevant to the target problem to reduce reuse overhead and defect inheritance on the target problem.
% Apart from their differences in objectives, there are other differences between network pruning and \projectName.
% For instance, Han et al.~\cite{pruning_hansong} compress a trained model by iterating two steps: removing redundant weights and retraining, while \projectName removes irrelevant weights without changing relevant weights.