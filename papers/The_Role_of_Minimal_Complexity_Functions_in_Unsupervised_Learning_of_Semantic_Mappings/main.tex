\documentclass{article} % For LaTeX2e
\usepackage{iclr2018_conference,times}
\usepackage{hyperref} 
\usepackage{placeins}
\usepackage{url}
\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url,enumitem}   % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}% blackboard math symbols
\usepackage{amsthm}
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{times}
\usepackage[normalem]{ulem}
\usepackage{graphicx,subfigure} 
\usepackage{adjustbox}
\usepackage{amsmath,amssymb}
\usepackage{tikz,tikz-cd}
\usetikzlibrary{matrix, calc, arrows}
\usepackage{tabulary,booktabs}
\usetikzlibrary{tikzmark}
\usepackage{stackengine}
\usepackage{thm-restate}
\usepackage{cleveref}
\setlength\parindent{0pt}
\newtheorem{assumption}{Assumption}
\newtheorem{pred}{Prediction}
\newtheorem{lem}{Lemma}
\newtheorem{defn}{Definition}
\newtheorem{thrmInformal}{Theorem}
\newtheorem{thrm}{Theorem}
\newtheorem{cor}{Corollary}
\usepackage{algpseudocode,algorithm,algorithmicx}
\DeclareMathOperator*{\argmin}{arg\,min}
\newcommand{\SIM}[2]{\mathrel{\operatorname*{\sim}_#1^#2}}

\title{The Role of Minimal Complexity Functions in Unsupervised Learning of Semantic Mappings}

\author{Tomer Galanti \\
The Blavatnik School of Computer Science \\
Tel Aviv University \\
Tel Aviv, Israel \\
\texttt{tomerga2@post.tau.ac.il} \\
\And
Lior Wolf \\
Facebook AI Research \& \\
The Blavatnik School of Computer Science \\
Tel Aviv University \\
Tel Aviv, Israel \\
\texttt{wolf@fb.com} \\
\texttt{wolf@cs.tau.ac.il} \\
\And
Sagie Benaim \\
The Blavatnik School of Computer Science \\
Tel Aviv University \\
Tel Aviv, Israel \\
\texttt{sagieb@mail.tau.ac.il} \\
}




\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\iclrfinalcopy 

\DeclareMathOperator{\coker}{coker}
\newcommand{\Id}{\textnormal{Id}}
\newcommand{\Const}{\textnormal{Const}}
\newcommand{\KL}{\textnormal{KL}}
\newcommand{\disc}{\textnormal{disc}}
\newcommand{\rdisc}{\textnormal{q-disc}}
\newcommand{\dist}{\textnormal{dist}}
\newcommand{\Inv}{\textnormal{Invariant}}
\newcommand{\Cov}{\textnormal{N}}
\newcommand{\Fp}{\textnormal{F}^+}
\newcommand{\F}{\textnormal{F}}
\newcommand{\ntr}{\ntriangleright}
\newcommand{\tr}{\triangleright}


\begin{document}
\maketitle

\begin{abstract}
We discuss the feasibility of the following learning problem: given unmatched samples from two domains and nothing else, learn a mapping between the two, which preserves semantics. Due to the lack of paired samples and without any definition of the semantic information, the problem might seem ill-posed. Specifically, in typical cases, it seems possible to build infinitely many alternative mappings  from every target mapping. This apparent ambiguity stands in sharp contrast to the recent empirical success in solving this problem.

We identify the abstract notion of aligning two domains in a semantic way with concrete terms of minimal relative complexity. A theoretical framework for measuring the complexity of compositions of functions is developed in order to show that it is reasonable to expect the minimal complexity mapping to be unique. The measured complexity used is directly related to the depth of the neural networks being learned and a semantically aligned mapping could then be captured simply by learning using architectures that are not much bigger than the minimal architecture.

Various predictions are made based on the hypothesis that semantic alignment can be captured by the minimal mapping. These are verified extensively. In addition, a new mapping algorithm is proposed and shown to lead to better mapping results.
\end{abstract}


\section{Introduction}
Multiple recent reports~\citep{xia2016dual,discogan,CycleGAN2017,dualgan} convincingly demonstrated that one can learn to map between two domains that are each specified merely by a set of unlabeled examples. For example, given a set of unlabeled images of horses, and a set of unlabeled images of zebras, CycleGAN~\citep{CycleGAN2017} creates the analog zebra image for a new image of a horse and vice versa.

These recent methods employ two types of constraints. First, when mapping from one domain to another, the output has to be indistinguishable from the samples of the new domain. This is enforced using GANs~\citep{gan} and is applied at the distribution level: the mapping of horse images to the zebra domain should create images that are indistinguishable from the training images of zebras and vice versa. The second type of constraint enforces that for every single sample, transforming it to the other domain and back (by a composition of the mappings in the two directions) results in the original sample. This is enforced for each training sample from either domain: every training image of a horse (zebra), which is mapped to a zebra (horse) image and then back to the source domain, should be as similar as possible to the original input image.

In another example, taken from DiscoGAN~\citep{discogan}, a function is learned to map a handbag to a shoe of a similar style. One may wonder why striped bags are not mapped, for example, to shoes with a checkerboard pattern. If every striped pattern in either domain is mapped to a checkerboard pattern in the other and vice-versa, then both the distribution constraints and the circularity constraints might hold. The former could hold since both striped and checkerboard patterned objects would be generated. Circularity could hold since, for example, a striped object would be mapped to a checkerboard object in the other domain and then back to the original striped object.

One may claim that the distribution of striped bags is similar to those of striped shoes and that the distribution of checkerboard patterns is also the same in both domains. In this case, the alignment follows from fitting the shapes of the distributions. This explanation is unlikely, since no effort is being made to create handbags and shoes that have the same distributions of these properties, as well as many other properties.

 

Our work is dedicated to the alternative hypothesis that the target mapping is implicitly defined by being approximated by the lowest-complexity mapping that has a low discrepancy between the mapped samples and the target distribution, i.e., the property that even a good discriminator cannot distinguish between the generated samples and the target ones. In Sec.~\ref{sec:problemformulation} we explore the inherent ambiguity of cross domain mapping. In Sec.~\ref{sec:hypothesis}, we present the hypothesis and two verifiable predictions, as well as a new unsupervised mapping algorithm. In Sec.~\ref{sec:counting}, we show that the number of minimal complexity mappings is expected to be small. Sec.~\ref{sec:experiments} verifies the various predictions. Some context to our work, including classical ideas such as Occam's Razor, MDL, and Kolmogorov complexity are discussed in Sec.~\ref{sec:discussion}.

 

\section{The Unsupervised Alignment Problem}

\label{sec:problemformulation}


The learning algorithm is provided with only two unlabeled datasets: one includes i.i.d samples from the first distribution and the second includes i.i.d samples from the other distribution (all notations are listed in Appendix~\ref{sec:summary}, Tab.~\ref{tab:summary}). 
\begin{equation}
\begin{aligned}
x_i \in \mathcal X_A\textnormal{ for } i=1\dots m &\text{ where } x_i \stackrel{\textnormal{i.i.d}}{\sim} D_A{\textnormal{ and }}\mathcal{X}_A{\textnormal{ denotes the space of domain $A = (\mathcal{X}_A,D_A)$}}\\
x_j \in \mathcal X_B\textnormal{ for } j=1\dots n &\text{ where } x_j \stackrel{\textnormal{i.i.d}}{\sim} D_B {\textnormal{ and }}\mathcal{X}_B{\textnormal{ denotes the space of domain $B = (\mathcal{X}_B,D_B)$}}
\end{aligned}
\end{equation}

To semantically tie the two distributions together, a generative view can be taken. This view is well aligned with the success of GAN-based image generation, e.g.,~\citep{dcgan}, in mapping random input vectors into realistic-looking images. Let $z\in\mathcal{X}$ be a random vector that is distributed according to the distribution $D_Z$ and which we employ to denote the semantic essence of samples in $\mathcal{X}_A$ and $\mathcal{X}_B$. We  denote $D_A = y_A \circ D_Z$ and $D_B = y_B \circ D_Z$,
where the functions $y_A: \mathcal{X} \rightarrow \mathcal{X}_A$ and $y_B: \mathcal{X} \rightarrow \mathcal{X}_B$ (see Fig.~\ref{fig:AZB}), and $f \circ D$ denotes the distribution of $f(x)$, where $x\sim D$. Following the circularity-based methods~\citep{xia2016dual,discogan,CycleGAN2017,dualgan}, we assume that both $y_A$ and $y_B$ are invertible.

The assumption of invertibility is further justified by the recent success of supervised pre-image computation methods~\citep{Dosovitskiy_2016_CVPR}. In unsupervised learning, given training samples, one may be expected to be able to recover the underlying properties of the generated samples, even with very weak supervision~\citep{infogan}. However, if the target function between domains $A$ and $B$ is not invertible, because for each member of $A$ there are a few possible members of $B$ (or vice versa), we can add a stochastic component to $A$ that is responsible for choosing which member in $B$ to take, given a member of $A$. For example, if $A$ is a space of handbag images and $B$ is a space of shoes, such that for every handbag, there are a few analogous shoes, then a stochastic variable can be added such that given a handbag, one shoe is selected among the different analog shoes.

We denote by $y_{AB} = y_B \circ y^{-1}_A$, the function that maps the first domain to the second domain. It is semantic in the sense that it goes through the shared semantic space $\mathcal X$. The goal of the learner is to fit a function $h\in \mathcal{H}$, for some hypothesis class $\mathcal{H}$ that is closest to $y_{AB}$,
\begin{equation}
\label{eq:32}
\begin{aligned}
\inf_{h\in \mathcal{H}} R_{D_A}[h,y_{AB}],
\end{aligned}
\end{equation} 
where $R_D[f_1,f_2] = \mathbb{E}_{x \sim D} \ell (f_1(x),f_2(x))$, for a loss function $\ell: \mathbb{R} \times \mathbb{R} \rightarrow \mathbb{R}$ and a distribution $D$.

It is not clear that such fitting is possible without further information. Assume, for example, that there is a natural order on the samples in $\mathcal X_B$. A mapping that transforms an input sample $x \in \mathcal X_A$ to the sample that is next in order to $y_{AB}(x)$, could be just as feasible. More generally, one can permute the samples in $\mathcal X_A$ by some function $\Pi$ that replaces each sample with another sample that has a similar likelihood (see Def.~\ref{def:DPMinformal} below) and learn $h$ that satisfies $h = y_{AB}\circ \Pi$. We call this difficulty ``the alignment problem'' and our work is dedicated to understanding the plausibility of learning despite this problem.

{
 \tikzset{
block/.style = {draw, fill=white, rectangle, minimum height=3em, minimum width=3em},
tmp/.style  = {coordinate}, 
sum/.style= {draw, fill=white, circle, node distance=1cm},
input/.style = {coordinate},
output/.style= {coordinate},
pinstyle/.style = {pin edge={to-,thin,black}
}
}

\begin{figure}[t]
\centering
\begin{tikzpicture}[auto, node distance=2cm,>=latex']
    \node [block] (Z) {$D_Z$};
    \node [block, above of=Z, left of = Z, node distance=1.8cm] (A){$D_A$};
    \node [block, above of=Z, right of = Z, node distance=1.8cm] (B) {$D_B$};
    
    \draw [->] (Z) -- node{$y_A$} (A);
    \draw [->] (Z) -- node[anchor = north, xshift = 0.3cm]{$y_B$} (B);
    \draw [->] (A) -- node{$y_{AB}$} (B);
    \end{tikzpicture}
\caption{The mappings between the domains $A$, $B$, and $Z$.} \label{fig:AZB}
\end{figure}
} 

In multiple recent contributions~\citep{xia2016dual,discogan,CycleGAN2017,dualgan} circularity is employed. Circularity requires the recovery of both $y_{AB}$ and $y_{BA} = y_A \circ y_B^{-1}$ simultaneously. Namely, functions $h$ and $h'$ are learned jointly by minimizing the risk:
\begin{equation}
\label{eq:circ}
\begin{aligned}
\inf_{h,h'\in \mathcal{H}} &\disc_{\mathcal{C}}(h\circ D_A,D_B) + \disc_{\mathcal{C}}(h'\circ D_B,D_A) \\
&+ R_{D_A}[h'\circ h,\Id_A] + R_{D_B}[h\circ h',\Id_B]
\end{aligned}
\end{equation} 
where $\disc_{\mathcal{C}}(D_1,D_2)=\sup_{c_1,c_2 \in \mathcal{C}} |R_{D_1}[c_1,c_2] - R_{D_2}[c_1,c_2]|$ denotes the discrepancy between distributions $D_1$ and $D_2$ that is implemented with a GAN~\citep{Ganin:2016:DTN:2946645.2946704}.   

The first term in Eq.~\ref{eq:circ} ensures that the samples generated by mapping domain $A$ to domain $B$ follow the distribution of samples in domain $B$. The second term is the analog term for the mapping in the other direction. The last two terms ensure that mapping a sample from one domain to the second  and back, results in the original sample. 

While the circularity constraints, expressed as the last two terms in Eq.~\ref{eq:circ}, are elegant and do not require additional supervision, for every invertible permutation $\Pi$ of the samples in domain $B$ (not to be confused with a permutation of the vector elements of the representation of samples in $B$) we have 
\begin{equation}
\label{eq:Pi}
\begin{aligned}
(h' \circ \Pi^{-1})\circ (\Pi \circ h )   &= h\circ h' \approx \Id_A,\text{  and} \\
 (\Pi \circ h ) \circ (h' \circ \Pi^{-1}) = \Pi \circ (h\circ & h') \circ \Pi^{-1} \approx  \Pi \circ \Id_B \circ \Pi^{-1} = \Id_B.
\end{aligned}
\end{equation}
Therefore, every circularity preserving pair $h$ and $h'$ gives rise to many possible solutions of the form $\tilde h = h \circ \Pi$ and $\tilde h' = \Pi^{-1} \circ h'$. If $\Pi$ happens to satisfy $D_B(x) \approx D_B(\Pi(x))$, then the discrepancy terms in Eq.~\ref{eq:circ} also remain largely unchanged. Circularity by itself cannot, therefore, explain the recent success of unsupervised mapping.

\section{The Simplicity Hypothesis}
\label{sec:hypothesis}

Despite the availability of a large number of alternative  hypotheses $h'$ that satisfy the constraints of Eq.~\ref{eq:circ}, the methods of~\cite{xia2016dual,discogan,CycleGAN2017,dualgan} enjoy empirical success, Why? 

Our hypothesis is that the small-discrepancy mapping of the lowest complexity approximates the alignment of the target function. We further hypothesize that when performing research in unsupervised mapping, goldilock architectures are selected. These architectures are complex enough to allow small discrepancies but not complex enough to support mappings that are not minimal in complexity. By doing so, one of the minimal-complexity low-discrepancy mappings is learned.

\subsection{An Illustrative Example}
\label{sec:toy}

In order to illustrate our hypothesis, we present a very simple toy example, depicted in Fig.~\ref{fig:toy}. Consider the domain $A$ of uniformly distributed points $(x_1,x_2)^\top \in \mathbb{R}^2$, where $0 \leq x_1 < 1$ and $x_2=0.5$. Let $B$ be a similar domain, except $x_2=2$. We are interested in learning the mapping $y^{2D}_{AB}((x_1,0.5)^\top) = (x_1,2)^\top$. We note that there are infinitely many mappings from domain $A$ to $B$ that satisfy the constraints of Eq.~\ref{eq:circ}.

However, when we learn the mapping using a neural network with one hidden layer of size 2, and Leaky ReLU activations\footnote{$\sigma_a(x) = \textnormal{Ind}[x<0] ax + \textnormal{Ind}[x\geq 0]x$, for the indicator function $\textnormal{Ind}[q]$ which maps a true value to one, zero otherwise.}~\citep{maas2013rectifier}, $y^{2D}_{AB}$ is one of only two options. In this case $h(x) = \sigma_a(Wx+b)$, for $W\in \mathbb{R}^{2\times 2}$,$b\in \mathbb{R}^2$ and where $\sigma_a$ is applied per coordinate. The only admissible solutions are of the form $W_b =  \left( \begin{array}{cc}
1 & -2b_1  \\
0 & 4-2b_2  \end{array} \right)$ or $W'_b=
 \left( \begin{array}{cc}
-1 & 1-2b_1  \\
0 & 4-2b_2  
\end{array} \right)$ and $b = (b_1,b_2)^\top$, which are identical, for every $b$, to $y^{2D}_{AB}$ or to an alternative  $y^{2D'}_{AB}((x_1,0.5)^\top) = (1-x_1,2)^\top$. Exactly the same situation holds for any pair of line segments in $\mathbb{R}_+^d$.

\begin{figure}[t]
\begin{centering}
\includegraphics[scale=0.75]{illustrative_figures/segments.png}
\end{centering}

\caption{\label{fig:toy} An illustrative example where the two domains are line segments in $\mathbb{R}^2$. There are infinitely many mappings that preserve the uniform distribution on the two segments. However, only two stand out as ``semantic''. These are exactly the two mappings that can be captured by a neural network with only two hidden neurons and Leaky ReLU activations, i.e., by a function  $h(x) = \sigma_a(Wx+b)$, for a weight matrix $W$ and the bias vector $b$.}
\end{figure}

Therefore, by restricting the hypothesis space of $h$, we eliminate all alternative solutions, except two. These two are exactly the two mappings that would commonly be considered ``more semantic'' than any other mapping, and can be expressed as the simplest possible mapping through a shared one dimensional space. While this is an extreme example, we believe that the principle is general since 
limiting the complexity of the admissible solutions eliminates the solutions that are derived from $y_{AB}$ by permuting the samples in the space $\mathcal X_A$, because such mixing requires added complexity.

\subsection{A Complexity Measure for Functions}\label{sec:complexMeasure1}

In this work, we focus on functions of the form 
\begin{equation}
f := F[W_{n+1},...,W_1] = W_{n+1} \circ \sigma \circ ... \circ \sigma \circ W_2 \circ \sigma \circ W_1
\end{equation}
here, $W_1,...,W_{n+1}$ are invertible linear transformations from $\mathbb{R}^M$ to itself. In addition, $\sigma$ is a non-linear element-wise activation function. We will mainly focus on $\sigma$ that is Leaky ReLU with parameter $0< a \neq 1$. In addition, for any function $f$, we define the complexity of $f$, denoted by $C(f)$ as the minimal number $n$ such that there are invertible linear transformations $W_1,...,W_{n+1}$ that satisfy $f = F[W_{n+1},...,W_1]$.

Our function complexity framework, therefore, measures the complexity of a function as the depth of a neural network which implements it, or  the shallowest network, if there are multiple such networks. In other words, we use the number of layers of a network as a proxy for the Kolmogorov complexity of functions,  using layers in lieu of the primitives of the universal Turing machines, which is natural for studying functions that can be computed by feedforward neural networks.

Note that capacity is typically controlled by means of norm regularization, which is optimized during training. Here, the architecture is bounded to a certain number of layers. This measure of complexity is intuitive and provides a clear and stable stratification of functions. 
 
Norm capacity (for norms larger than zero) are not effective in comparing functions of different architectures. In Sec.~\ref{sec:experiments}, we demonstrate that the L1 and L2 norms of the desired mapping are within the range of norms that are obtained when employing bigger or smaller architectures. Other ways to define the complexity of functions, such as the VC-dimension~\citep{vapnik:264} and Rademacher complexity~\citep{Bartlett:2003:RGC:944919.944944}, are not suitable for measuring the complexity of individual functions, since their natural application is in measuring the capacity of classes of functions. 

\subsection{Consequences of the Simplicity Hypothesis}

The simplicity hypothesis leads to concrete predictions, which are verified in Sec.~\ref{sec:experiments}. The first one states that in contrast to the current common wisdom, one can learn a semantically aligned mapping between two spaces without any matching samples and even without circularity. 

\begin{pred}\label{pred1}
When learning with a small enough network in an unsupervised way a mapping between domains that share common characteristics, the $GAN$ constraint in the target domain is sufficient to obtain a semantically aligned mapping.
\end{pred}

The strongest clue that helps identify the alignment of the semantic mapping from the other mappings is the suitable complexity of the network that is learned. A network with a complexity that is too low cannot replicate the target distribution, when taking inputs in the source domain (high discrepancy). A network that has a complexity that is too high, would not learn the minimal complexity mapping, since it could be distracted by other alignment solutions.

We believe that the success of the recent methods results from selecting the architecture used in an appropriate way. For example, DiscoGAN~\citep{discogan} employs either eight or ten layers, depending on the dataset. We make the following prediction:

\begin{pred}\label{pred2}
When learning in an unsupervised way a mapping between domains, the complexity of the network needs to be carefully adjusted.
\end{pred}

This prediction is also surprising, since in supervised learning, extra depth is not as detrimental, if at all. As far as we know, this is the first time that this clear distinction between supervised and unsupervised learning is made~\footnote{The MDL literature was developed when people believed that small hypothesis classes are desired for both supervised and unsupervised learning.}.

\subsection{Alignment with Non-minimal Architectures}

If the simplicity hypothesis is correct, then in order to capture the target alignment, one would need to learn with the minimal complexity architecture that supports a small discrepancy. However, deeper architectures can lead to even smaller discrepancies and to better outcomes.

In order to enjoy both the alignment provided by our hypothesis and the improved output quality, we propose to find a function $h$ of a non-minimal complexity $k_2$ that minimizes the following objective function
\begin{equation}\label{eq:objComp}
\min_{h  \textnormal{ s.t } C(h)=k_2 } \left\{\disc(h \circ D_A,D_B) + \lambda \inf_{g \textnormal{ s.t } C(g)=k_1} R_{D_A}[h,g]\right\}
\end{equation}
where $k_1$ is the minimal complexity for mapping with low discrepancy between domain $A$ and domain $B$. In other words, we suggest to find a function $h$ that is both a high complexity mapping from domain $A$ to $B$ and is close to a function of low complexity that has low discrepancy.

There are alternative ways to implement an algorithm that minimizes the objective function presented in Eq.~\ref{eq:objComp}. Assuming, based on this equation, that for $h$ that minimizes the objective function, the corresponding $g^* = \underset{g \textnormal{ s.t } C(g)=k_1}{\arg\inf} R_{D_A}[h,g]$ has a (relatively) small discrepancy, leads to a two-step algorithm. The algorithm first finds a function $g$ that has small complexity and small discrepancy and then finds $h$ of a larger complexity that is close to $g$. This is implemented in Alg.~\ref{algo:duallayers}. Note that in the first step, $k_1$ is being estimated, for example, by gradually increasing its value, until $g$ with a discrepancy lower than a threshold $\epsilon_0$ is found. We suggest to use a liberal threshold, since the goal of the network $g$ is to provide alignment and not the lowest possible discrepancy.

\begin{algorithm}
\caption{Complexity Based Regularization Alignment
\label{algo:duallayers}}
\begin{algorithmic}[1]
\Require{Unlabeled training sets $S_A \stackrel{\textnormal{i.i.d}}{\sim} D^m_A$ and $S_B \stackrel{\textnormal{i.i.d}}{\sim} D^n_B$, 
a desired complexity $k_2$, and a trade-off parameter $\lambda$}
		\State Identify a complexity $k_1$, which leads to a small discrepancy $\underset{g \textnormal{ s.t: } C(g)=k_1}{\min} \disc(g \circ D_A,D_B)$. 
        \State Train $g$ of complexity $k_1$ to minimize $\disc(g \circ D_A,D_B)$.
        %\EndWhile
        \State Train $h$ of complexity $k_2$ to minimize $\disc(h \circ D_A,D_B) + \lambda R_{D_A}[h,g]$.
\end{algorithmic}
\end{algorithm}

 
\section{Counting Minimal Complexity Mappings}
\label{sec:counting}

Recall, from Sec.~\ref{sec:problemformulation}, that $\disc$ is the discrepancy distance, which is based on the optimal discriminator. Also discussed were the functions $\Pi$, that switches between members in the domain $B$ that have similar probabilities. These are defined using the discrepancy distance as follows (simplified version; the definitions and results of this section are stated more broadly in Appendix~\ref{sec:extended}):

\begin{defn}[Density preserving mapping]\label{def:DPMinformal} Let $X=(\mathcal{X},D_X)$ be a domain. A $\epsilon_0$-density preserving mapping over $X$ (or an $\epsilon_0$-DPM for short) is a function $\Pi$ such that 
\begin{equation}
\disc(\Pi \circ D_X,D_X) \leq \epsilon_0
\end{equation}
We denote the set of all $\epsilon_0$-DPMs of complexity $k$ by $\textnormal{DPM}_{\epsilon_0}(X;k):=\left\{\Pi \big\vert \disc(\Pi \circ D_X,D_X) \leq \epsilon_0
 \textnormal{ and } C(\Pi)=k \right\}$. 
\end{defn}

Below, we define a similarity relation between functions that reflects whether the two are similar. In this way, we are able to bound the number of different (non-similar) minimal complexity mappings by the number of different DPMs. 

\begin{defn}\label{def:closedInformal} Let $D$ be a distribution. We denote $f \overset{D}{\underset{\epsilon_0}{\sim}} g $, if $C(f) = C(g)$ and there are minimal decompositions: $f = F[W_{n+1},...,W_1]$ and $g = F[V_{n+1},...,V_1]$ such that: $\forall i \in [n+1]:\; \disc(F[W_{i},...,W_1] \circ D ,  F[V_{i},...,V_1] \circ D) \leq \epsilon_0$.
\end{defn}

Put differently, two functions of the same complexity have this relation, if for every step of their processing, the activations of the matching functions are similar.

The defined relation is reflexive and symmetric, but not transitive. Therefore, there are many different ways to partition the space of functions into disjoint subsets such that in each subset, any two functions have the closeness property. We count the number of functions as the minimal number of subsets required in order to cover the entire space. This quantity is denoted by $\Cov(\mathcal{U},\sim_{\mathcal{U}})$ where $\mathcal{U}$ is the set and $\sim_{\mathcal{U}}$ is the closeness relation. The formal presentation is in Def.~\ref{def:covering}, which slightly generalizes the notion of covering numbers~\citep{Anthony:2009:NNL:1795646}.


Informally, the following theorem states that the number of minimal low-discrepancy mappings is upper bounded by both the number of DPMs of a certain size over $D_A$ and over $D_B$. This result is useful, since DPMs are expected to be rare in real-world domains. When imagining mapping a space to itself, in a way that preserves the distribution, one first considers symmetries. Near-perfect symmetries are rare in natural domains, and when these occur, e.g., ~\citep{discogan}, they form well-understood ambiguities. Another option that can be considered is that of replacing specific samples in domain $B$ with other samples of the same probability. However, these very local discontinuous mappings are of very high complexity, since this complexity is required for reducing the modeling error for discontinuous functions. One can also consider replacing larger sub-domains with other sub-domains such that the distribution is preserved. This could be possible, for example, if the distribution within the sub-domains is almost uniform (unlikely), or if it is estimated inaccurately due to the limitations of the training set. 

We, therefore, make the following prediction.

\begin{pred} The number of DPMs of low complexity is small.
\end{pred}

Given two domains $A$ and $B$, there is a certain complexity $C^{\epsilon_0}_{A,B}$, which  is the minimal complexity of the networks needed in order to achieve discrepancy smaller than $\epsilon_0$ for mapping the distribution $D_A$ to the distribution $D_B$. The set of minimal complexity mappings, i.e., mappings of complexity $C^{\epsilon_0}_{A,B}$ that achieve $\epsilon_0$ discrepancy is denoted by $H_{\epsilon_0}(A,B) := \left\{h \; \big\vert\; C(h) \leq C^{\epsilon_0}_{A,B} \textnormal{ and } \disc(h \circ D_A,D_B) \leq \epsilon_0\right\}$. The following theorem shows that the covering number of this set is similar to the covering number of the DPMs. Therefore, if prediction 3 above holds, the number of minimal low-discrepancy mappings is small. 

\begin{restatable}[Informal]{thrmInformal} {counting0}\label{thm:counting0} Let $\sigma$ be a Leaky ReLU with parameter $0< a \neq 1$ and assume identifiability. Let $\epsilon_0$, $\epsilon_1$ and $\epsilon_2 < \epsilon_1$ be three positive constants and $A = (\mathcal{X}_A,D_A)$ and $B = (\mathcal{X}_B, D_B)$ are two domains. Then, 
\begin{small}
\begin{equation}
\begin{aligned}
\Cov\left(H_{\epsilon_0}(A,B),\overset{D_A}{\underset{\epsilon_1}{\sim}}\right) \leq 
\min \begin{cases}
\Cov\left (\textnormal{DPM}_{\epsilon_0}\left(A; 2C^{\epsilon_0}_{A,B} \right), \overset{D_A}{\underset{\epsilon_2}{\sim}} \right)\\
\Cov\left (\textnormal{DPM}_{\epsilon_0}\left(B; 2C^{\epsilon_0}_{A,B} \right), \overset{D_B}{\underset{\epsilon_2}{\sim}} \right)
\end{cases}
\end{aligned}
\end{equation}
\end{small}
\end{restatable}

\proof{See Appendix~\ref{app:thmproof}.}

The theorem assumes identifiability. In the context of neural networks, the general question of uniqueness up to invariants, also known as identifiability, is an open question. Several authors have made progress in this area for different neural network architectures. The most notable work has been done by~\cite{DBLP:conf/nips/FeffermanM93} that proves identifiability for $\sigma = \tanh$. Furthermore, the representation is unique up to some invariants. Other works~\citep{DBLP:journals/tnn/WilliamsonH95,albertini,DBLP:journals/nn/KurkovaK14,DBLP:journals/nn/Sussmann92} prove such uniqueness for neural networks with only one hidden layer and various activation functions. Similarly, in Lem.~\ref{lem:identifyDepth2} in the Appendix, we show that identifiability holds for Leaky ReLU networks with one hidden layer.

\section{Experiments}
\label{sec:experiments}

The first group of experiments is dedicated to test the validity of the three predictions made, in order to give further support to the simplicity hypothesis. Next, we evaluate the success of the proposed algorithm in comparison to the DiscoGAN method of ~\cite{discogan}.

We chose to experiment with the DiscoGAN architecture since it focuses on semantic tasks that contain a lesser component of texture or style transfer. The CycleGAN architecture of~\cite{CycleGAN2017} inherits much from the style transfer architecture of Pix2Pix~\cite{pix2pix}, and the discrepancy term is based on a patch-based analysis, which introduces local constraints that could mask the added freedom introduced by adding layers. In addition, the U-net architecture of ~\cite{unet} used by~\cite{pix2pix} deviates from the connectivity pattern of our model. 

Experiments in this architecture and with the architecture of DualGAN~\citep{dualgan}, which focuses on tasks similar to CycleGAN, and shares many of the architectural choices,  including U-nets and the use of patches, are left for future work.

\subsection{Empirical Validation of The Predictions}

Prediction 1 states that since the unsupervised mapping methods are aimed at learning minimal complexity low discrepancy functions, GANs are sufficient. In the literature~\citep{CycleGAN2017,discogan}, learning a mapping $h:\mathcal{X}_A\rightarrow \mathcal{X}_B$, based only on the GAN constraint on $B$, is presented as a failing baseline. In~\citep{dualgan}, among many non-semantic mappings obtained by the GAN baseline, one can find images of GANs that are successful. However, this goes unnoticed.

In order to validate the prediction that a purely GAN based solution is viable, we conducted a series of experiments using the DiscoGAN architecture and GAN loss only. We consider image domains $A$ and $B$, where $\mathcal{X}_A = \mathcal{X}_B = \mathbb{R}^{3\times 64\times 64}$.  

In DiscoGAN, the generator is built of: (i) an encoder consisting of convolutional layers with $4\times 4$ filters followed by Leaky ReLU activation units and (ii) a decoder consisting of deconvolutional layers with $4\times 4$ filters followed by a ReLU activation units. Sigmoid is used for the output layer. Between four to five convolutional/deconvolutional layers are used, depending on the domains used in $A$ and $B$ (we match the published code architecture per dataset). The discriminator is similar to the encoder, but has an additional convolutional layer as the first layer and a sigmoid output unit. 

The first set of experiments considers the CelebA face dataset. Transformations are learned between the subset of images labeled as male and those labeled as female, as well as from blond to black hair and eyeglasses to no eyeglasses. The results are shown in Fig.~\ref{fig:celebA_gender},~\ref{fig:celebA_blond}, and~\ref{fig:celebA_eyelgasses}, (resp.). It is evident that the output image is highly related to the input images.

In the case of mapping handbags to shoes, as seen in Fig.~\ref{fig:handbagtoshoe}, the GAN does not provide a meaningful solution. However, in the case of edges to shoes and vice versa (Fig.~\ref{fig:edges2shoes}), the GAN solution is successful.


In Prediction 2, we predict that the selection of the right number of layers is crucial in unsupervised learning. Using fewer layers than needed, will not support the modeling of the target alignment between the domains. In contrast, adding superfluous layers would mean that more and more alternative mappings obscure the target transformation. 

In~\citep{discogan}, 8 or 10 layers are employed  (counting both convolution and deconvolution) depending on the experiment. In our experiment, we vary the number of layers and inspect the influence on the results. The experiments are also repeated for the Wasserstein GAN loss (using the same architecture) in Appendix~\ref{appendix:wgan}.

These experiments were done on the CelebA gender conversion task, where 8 layers are employed in the experiments of~\citep{discogan}. Using the public implementation and adding and removing layers, we obtain the results in Fig.~\ref{fig:a001}--~\ref{fig:a006}. Note that since the encoder and the decoder parts of the learned network are symmetrical, the number of layers is always even. As can be seen, changing the number of layers has a dramatic effect on the results. The best results are obtained at 6 or 8 layers with 6 having the best alignment and 8 having better discrepancy. The results degrade quickly, as one deviates from the optimal value. Using fewer layers, the GAN fails to produce images of the desired class. Adding layers, the semantic alignment is lost, just as expected.

Note that~\cite{discogan} have preferred low discrepancy over alignment in their choice. In other words, the selected architecture of size $k=8$ presents acceptable images at the price of lower alignment compared to an architecture of size $k-2$. This is probably a result of ambiguity that is already present at the size $k$ architecture. On the other hand, the smaller architecture of size $k-2$ does not produce images of extremely low discrepancy, and there is no architecture that benefits both, an extremely low discrepancy and high alignment.  This is observed for example in Fig.~\ref{fig:a001} where females are translated to males. For 4 layers the discrepancy is too low and the mapping fails to produce images of males. For 6 layers, the discrepancy is relatively low and the alignment is at its highest. For 8 layers, the discrepancy is at its lowest value, nevertheless, the alignment is worse.

While our discrete notion of complexity seems to be highly related to the quality of the results, the norm of the weights do not seem to point to a clear architecture, as shown in Tab.~\ref{tab:a001}(a). Since the table compares the norms of architectures of different sizes, we also approximated the functions using networks of a fixed depth $k=18$ and then measured the norm. These results are presented in Tab.~\ref{tab:a001}(b). In both cases, the optimal depth, which is 6 or 8, does not appear to have a be an optimum in any of the measurements. 

Prediction 3 states that there are only a handful of DPMs,  except for the identity function. In order to verify it, we trained a DiscoGAN from a distribution $A$ to itself with an added loss of the form $-\sum_{x\in A}|x-h(x)|$. In our experiment, testing network complexities from 2 to 12, we could not find a DPM, see Fig.~\ref{fig:c001} and Tab.~\ref{tab:c001}. For lower complexities, the identity was learned despite the added loss. For higher complexities, the network learned the identity while changing the background color. For even higher complexities, other mapping emerged. However, these mappings did not satisfy the circularity constraint, and are unlikely to be DPMs.

\subsection{Results for Alg.~\ref{algo:duallayers}}

The goal of Alg.~\ref{algo:duallayers} is to find a well-aligned solution with higher complexity than the minimal solution and potentially smaller discrepancy. It has two stages. In the first one, $k_1$, which is the minimal complexity that leads to a low discrepancy, is identified. This follows a set of experiments that are similar to the one that is captured, for example, by Fig.~\ref{pred2}. To demonstrate robustness, we select a single value of $k_1$ across all experiments. Specifically, we use $k_1=6$, which, as discussed above, typically leads to a low (but not very low) discrepancy, while the alignment is still unambiguous. 

Once $g$ is trained, we proceed to the next step of optimizing a second network of complexity $k_2$. Note that while the first function ($g$) uses the complete DiscoGAN architecture, the second network ($h$) only employs a one-directional mapping, since alignment is obtained by $g$. Figs.~\ref{fig:alg10}--~\ref{fig:alg6} depict the obtained results, for a varying number of layers. First, the result obtained by the DiscoGAN method with $k_1$ is displayed. The results of applying Alg.~\ref{algo:duallayers} are then displayed for a varying $k_2$. 

As can be seen, our algorithm leads to more sophisticated mappings. \cite{discogan} have noted that their solutions are, at many times, related to texture or style transfer and, for example, geometric transformations are not well captured. The new method is able to better capture such complex transformations. Consider the case of mapping male to female in Fig.~\ref{fig:alg9}, first row. A man with a beard is mapped to a female image. While for $g$ the beard is still somewhat present, it is not so for $h$ with $k_2>k_1$. On the female to male mappings in Fig.~\ref{fig:alg10} it is evident in most mappings that $g$ produces a more blurred image, while $h$ is more coherent for $k_2>k_1$. 
Another example is in the blond to black hair mapping in Fig.~\ref{fig:alg1}. In the 5th row, the style transfer nature of $g$ is evident, since it maps a red object behind the head together with the whole blond hair, producing an unrealistic black hair. $h$ of complexity $k_2=8$ is able to separate that object from the hair, and in $k_2>8$ it produces realistic looking black hair. This kind of transformation requires more than a simple style transfer. On the edges to shoes and edges to handbags mappings of Fig.~\ref{fig:alg3} and Fig.~\ref{fig:alg5}, while the general structure is clearly present, it is significantly sharpened by mapping $h$ with $k_2>k_1$. 

For the face datasets, we also employ face descriptors in order to learn whether the mapping is semantic. Namely, we can check if the identity is preserved post mapping by comparing the VGG face descriptors of~\cite{Parkhi15}. One can assume that two images that match will have many similar features and so the VGG representation will be similar. The cosine similarities are used, as is commonly done. 

In addition, we train a linear classifier in the space of the VGG face descriptors in order to distinguish between Male/Female, Eyeglasses/No-eyeglasses, and Blond/Black. This way, we can check, beyond discrepancy, that the mapping indeed transforms between the domains. The training samples in domains $A$ and $B$ are used to train this classifier, which is then applied to a set of test images before and after mapping, measuring the accuracy. The higher the accuracy, the better the separation.

Tab.~\ref{tab:alg1exp1} presents the results for both the $k_1$ layers network $g$, alternative networks $g$ of higher complexity (shown as baseline only), and the network $h$ trained using Alg.~\ref{algo:duallayers}. 
We expect the alignment of $g$ to be best at complexity $k_1$, and worse due to the loss of discrepancy for alternative network $g$ with complexity $k>k_1$. We expect this loss of alignment to be resolved for networks $h$ trained with Alg.~\ref{algo:duallayers}.

In the experiments of black to blond hair and blond to black hair mappings, we note that $h$ with $k_2=8$ has the best descriptor similarity, and very good separation accuracy and discrepancy. Higher values of $k_2$ are best in terms of separation accuracy and discrepancy, but lose somewhat in descriptor similarity. A similar situation occurs for male to female and female to male mappings and in eyeglasses to non-eyeglasses, where $k_2=8$ results in the best similarity score and higher values of $k_2$ result in better separation accuracy and discrepancy. 

It is interesting to note, that the distance between $g$ and $h$ is also minimal for $k_2=8$. Perhaps, with more effective optimization, higher complexities could also maintain similarity, while delivering lower discrepancies.

\section{Discussion}
\label{sec:discussion}

Our stratified complexity model is related to structural risk minimization (SRM) by~\cite{vc-ucrfep-71}, which employs a hierarchy of nested subsets of hypothesis classes in order of increasing complexity. In our stratification, which is based on the number of layers, the complexity classes are not necessarily nested. A major emphasis in SRM is the dependence on the number of samples: the algorithm selects the hypothesis from one of the nested hypothesis classes depending on the amount of training data. In our case, one can expect higher values of $k_2$ to be beneficial as the number of training samples grows. However, the exact characterization of this relation is left for future work.

Alg.~\ref{algo:duallayers} can be seen as a form of distillation. The first step of the algorithm finds the minimal complexity for mapping between the two domains and obtains the first generator. Then, a second generator, with a large complexity, is trained while being encouraged to output images which are close to the output of the first generator. This resembles the distillation methods proposed by~\cite{44873} and later analyzed by~\cite{DBLP:journals/corr/HandV17}.

Since the method depicted in Alg.~\ref{algo:duallayers} optimizes, among other things, the architecture of the network, our method is somewhat related to work that learn the network's structure during training, e.g.,~\citep{saxena:hal-01359150,wen2016learning,liu2015sparse,7410672,Lebedev_2016_CVPR}. This body of work, which deals exclusively with supervised learning, optimizes the networks loss by modifying both the parameters and the hyperparameters. For GAN based loss, this would not work, since with more capacity, one can reduce the discrepancy but quickly lose the alignment.

Indeed, we point to a key difference between supervised learning and unsupervised learning. While in the former, deeper networks, which can learn even random labels, work well~\citep{rethinking},  unsupervised learning requires a careful control of the network capacity. This realization, which echoes the application of MDL for model selection in unsupervised learning~\citep{zemel1994minimum}, was overshadowed by the overgeneralized belief that deeper networks lead to higher accuracy. 
 
The limitations of unsupervised based learning that are due to symmetry, are also a part of our model. For example, the mapping of cars in one pose to cars in the mirrored pose that sometimes happens in~\citep{discogan}, is similar in nature to the mapping of $x$ to $1-x$ in the simple example given in Sec.~\ref{sec:toy}. Such symmetries occur when we can divide $y_{AB}$ into two functions $y_{AB}=y_2\circ y_1$ such that a function $W$ is a linear mapping and also a DPM of $y_1\circ D_A$ and, therefore, $D_B \approx y_2\circ W \circ y_1$.

While we focus on unsupervised learning, the emergence of semantics when learning with a restricted capacity is widely applicable, such as with autoencoders, transfer learning, semi-supervised learning and elsewhere. As an extreme example,~\cite{DBLP:journals/corr/SutskeverJGRLV15} present empirical evidence that a meaningful mapper can be learned, even from very few examples, if the network trained is kept small.

\section{Conclusion}

The recent success in mapping between two domains in an  unsupervised way and without any existing knowledge, other than network hyperparameters, is nothing less than extraordinary and has far reaching consequences. As far as we know, nothing in the existing machine learning or cognitive science literature suggests that this would be possible. 

We provide an intuitive definition of function complexity and employ it in order to identify minimal complexity mappings, which we conjecture play a pivotal role in this success. If our hypothesis is correct, simply by training networks that are not too complex, the target mapping stands out from all other alternative mappings. 

Our analysis leads directly to a new unsupervised cross domain mapping algorithm that is able to avoid the ambiguity of such mapping, yet enjoy the expressiveness of deep neural networks. The experiments demonstrate that the analogies become richer in details and more complex, while maintaining the alignment.

We show that the number of low-discrepancy mappings that are of low-complexity is expected to be small. Our main proof is based on the assumption of identifiability, which constitutes an open question. We hope that there would be a renewed interest in this problem, which has been open for decades for networks with more than a single hidden layer and is unexplored for modern activation functions.

\subsubsection*{Acknowledgments}

This project has received funding from the European Research Council (ERC) under the European Union's Horizon 2020 research and innovation programme (grant ERC CoG 725974). The authors would like to thank Etai Littwin, Moustapha Cisse, L\'eon Bottou, Arthur Szlam, and Ofir Yakovian for insightful discussions. The contribution of Tomer Galanti is part of Ph.D. thesis research conducted at Tel Aviv University. 


\bibliography{sdn}

\bibliographystyle{iclr2018_conference}

\newpage

\begin{figure}[t]
  \centering
  \begin{tabular}{p{1cm}@{~}c@{~~~}c} 
\adjustbox{varwidth=1cm,raise=.61cm}{{\footnotesize (a) Input}} &\includegraphics[width=0.44380\linewidth,trim={0 64px 64px 0},clip]{figures_celebA_gender/gan_onlyA_to_AB.jpg}&  \includegraphics[width=0.44380\linewidth,trim={0 64px 64px 0},clip]{figures_celebA_gender/gan_onlyB_to_BA.jpg}\\ 
\adjustbox{varwidth=1cm,raise=.61cm}{\footnotesize (b)\\Output}&\includegraphics[width=0.44380\linewidth,trim={0 0 64px 64px},clip]{figures_celebA_gender/gan_onlyA_to_AB.jpg}&   \includegraphics[width=0.44380\linewidth,trim={0 0 64px 64px},clip]{figures_celebA_gender/gan_onlyB_to_BA.jpg}\\ 
&  (Male to female) & (Female to male)\\
\end{tabular}
  \caption{\label{fig:celebA_gender} Results for celebA Male to Female transfer (a) Input (b) The mapping obtained by the GAN  loss without additional losses.}

\vspace{.6cm}

  \centering
  \begin{tabular}{p{1cm}@{~}c@{~~~}c} 
\adjustbox{varwidth=1cm,raise=.61cm}{{\footnotesize (a) Input}} &\includegraphics[width=0.44380\linewidth,trim={0 64px 64px 0},clip]{figures_celebA_blond/gan_onlyA_to_AB.jpg}&  \includegraphics[width=0.44380\linewidth,trim={0 64px 64px 0},clip]{figures_celebA_blond/gan_onlyB_to_BA.jpg}\\ 
\adjustbox{varwidth=1cm,raise=.61cm}{\footnotesize (b)\\Output}&\includegraphics[width=0.44380\linewidth,trim={0 0 64px 64px},clip]{figures_celebA_blond/gan_onlyA_to_AB.jpg}&   \includegraphics[width=0.44380\linewidth,trim={0 0 64px 64px},clip]{figures_celebA_blond/gan_onlyB_to_BA.jpg}\\ 
&  (Blond to black hair) & (Black to blond hair)\\
\end{tabular}
  \caption{\label{fig:celebA_blond} Same as Fig.~\ref{fig:celebA_gender} for black to blond hair conversion.}

\vspace{.6cm}

  \centering
  \begin{tabular}{p{1cm}@{~}c@{~~~}c} 
\adjustbox{varwidth=1cm,raise=.61cm}{{\footnotesize (a) Input}} &\includegraphics[width=0.44380\linewidth,trim={0 64px 64px 0},clip]{figures_celebA_eyeglasses/gan_onlyA_to_AB.jpg}&  \includegraphics[width=0.44380\linewidth,trim={0 64px 64px 0},clip]{figures_celebA_eyeglasses/gan_onlyB_to_BA.jpg}\\ 
\adjustbox{varwidth=1cm,raise=.61cm}{\footnotesize (b)\\Output}&\includegraphics[width=0.44380\linewidth,trim={0 0 64px 64px},clip]{figures_celebA_eyeglasses/gan_onlyA_to_AB.jpg}&   \includegraphics[width=0.44380\linewidth,trim={0 0 64px 64px},clip]{figures_celebA_eyeglasses/gan_onlyB_to_BA.jpg}\\ 
&  (With to without eyeglasses) & (Without to with eyeglasses)\\
\end{tabular}
  \caption{\label{fig:celebA_eyelgasses} Same as Fig.~\ref{fig:celebA_gender} for eyeglasses to no eyeglasses conversion.}


\vspace{.6cm}

  \centering
  \begin{tabular}{p{1cm}@{~}c@{~~~}c} 
\adjustbox{varwidth=1cm,raise=.61cm}{{\footnotesize (a) Input}} &\includegraphics[width=0.44380\linewidth,trim={0 64px 64px 0},clip]{figures_handbags2shoes/gan_onlyA_to_AB.jpg}&  \includegraphics[width=0.44380\linewidth,trim={0 64px 64px 0},clip]{figures_handbags2shoes/gan_onlyB_to_BA.jpg}\\ 
\adjustbox{varwidth=1cm,raise=.61cm}{\footnotesize (b)\\Output}&\includegraphics[width=0.44380\linewidth,trim={0 0 64px 64px},clip]{figures_handbags2shoes/gan_onlyA_to_AB.jpg}&   \includegraphics[width=0.44380\linewidth,trim={0 0 64px 64px},clip]{figures_handbags2shoes/gan_onlyB_to_BA.jpg}\\ 
&  (Handbags to shoes) & (Shoes to handbags)\\
\end{tabular}
  \caption{\label{fig:handbagtoshoe} Same as Fig.~\ref{fig:celebA_gender} for handbag to shoes and shoes to handbag mapping.}

\vspace{.6cm}

  \centering
  \begin{tabular}{p{1cm}@{~}c@{~~~}c} 
\adjustbox{varwidth=1cm,raise=.61cm}{{\footnotesize (a) Input}} &\includegraphics[width=0.44380\linewidth,trim={0 64px 64px 0},clip]{figures_edges2shoes/gan_onlyA_to_AB.jpg}&  \includegraphics[width=0.44380\linewidth,trim={0 64px 64px 0},clip]{figures_edges2shoes/gan_onlyB_to_BA.jpg}\\ 
\adjustbox{varwidth=1cm,raise=.61cm}{\footnotesize (b)\\Output}&\includegraphics[width=0.44380\linewidth,trim={0 0 64px 64px},clip]{figures_edges2shoes/gan_onlyA_to_AB.jpg}&   \includegraphics[width=0.44380\linewidth,trim={0 0 64px 64px},clip]{figures_edges2shoes/gan_onlyB_to_BA.jpg}\\ 
&  (Edges to shoes) & (Shoes to edges)\\
\end{tabular}
  \caption{\label{fig:edges2shoes} Same as Fig.~\ref{fig:celebA_gender} for edges to shoes and shoes to edges conversion.}
\end{figure}

\begin{table}[H]
\centering
\caption{Comparing the VGG descriptor similarity, separation accuracy and discrepancy for varying complexity $k$}
\label{tab:var_layers_value}
\begin{tabular}{lcccccccc}
                             & & $k=4$     & $k=6$     & $k=8$     & $k=10$     & $k=12$     & $k=14$     \\
\midrule
Male to Female               & Discrepancy & 0.527 & 0.203  & 0.091 & 0.094 & 0.083 & 0.086 \\
                            &  Similarity  & 0.301 & 0.269 &  0.103  & 0.106 & 0.089 & 0.100   \\
			    &	Separation       & 0.938 & 0.932 & 0.940 & 0.940  & 0.940  & 0.938 \\
\midrule
Female to Male              & Discrepancy  & 0.882 & 0.122  & 0.150  & 0.075 & 0.076 & 0.091 \\                  
                            &   Similarity  & 0.303  & 0.260  & 0.110  & 0.105 & 0.093 & 0.100  \\
			   & Separation     & 0.798 & 0.865 & 0.860 & 0.87  & 0.857 & 0.866 \\
\midrule
Blond to Black Hair        & Discrepancy   & 0.467 & 0.214 & 0.092 & 0.097 & 0.094 & 0.081 \\
                             & Similarity  & 0.365 & 0.287 & 0.240  & 0.106 & 0.091 &   0.0870    \\
			     &	Separation & 0.903 & 0.925 & 0.922 & 0.917 & 0.922 &   0.923    \\
\midrule
Black to Blond Hair          & Discrepancy & 0.663 & 0.264 & 0.073 & 0.094 & 0.084 & 0.076 \\
                             & Similarity   & 0.337 & 0.270  & 0.240  & 0.106 & 0.087 &   0.085    \\
			     & Separation   & 0.941 & 0.941 & 0.911 & 0.916 & 0.915 &  0.917     \\
\midrule
Eyeglasses to Non-Eyeglasses & Discrepancy & 0.323 & 0.159 & 0.071 & 0.082 & 0.083 & 0.081 \\
			     & Similarity & 0.470  & 0.391 & 0.347 & 0.114 & 0.125 & 0.146 \\
			     & Separation & 0.786 & 0.785 & 0.828 & 0.843 & 0.849 & 0.828 \\
\midrule
Non Eyeglasses to Eyeglasses & Discrepancy & 0.577 & 0.518 & 0.236 & 0.263 & 0.093 & 0.085 \\
		              & Similarity & 0.452 & 0.373 & 0.364 & 0.105 & 0.108 & 0.127 \\
			      & Septation & 0.748 & 0.749 & 0.766 & 0.848 & 0.832 & 0.840  \\
\bottomrule      
\end{tabular}
\end{table}


\begin{table}[h]\caption{(a) Norms of the various mappings $h$ for mapping Males to Females using the DiscoGAN architecture. (b) Norms of $18$-layer networks that approximates the mappings obtained with a varying number of layers.}\label{tab:a001} 

\begin{center}
\begin{tabular}{llccccc}
\toprule
& & \multicolumn{5}{c}{----------- Number of layers ------------}\\
& Norm & 4 & 6 & 8 & 10 & 12 \\
\midrule
A to B &L1 norm & 6382  & 23530 & 36920  & 44670 & 71930 \\
&Average L1 norm per layer & 1064  & 2353  & 2637   & 2482   & 3270 \\
&L2 norm & 18.25 & 29.24 & 28.44  & 31.72  & 36.57 \\
&Average L2 norm per layer & 7.084 & 8.353 & 7.154  & 6.708  & 7.009 \\
\midrule
B to A&L1 norm & 6311  & 21240 & 31090  & 37380 & 64500 \\
&Average L1 norm per layer & 1052  & 2124  & 2221   & 2077   & 2932 \\
&L2 norm & 18.36 & 26.79 & 25.85  & 28.36  & 34.99 \\
&Average L2 norm per layer & 7.161 & 7.757 & 6.552  & 6.058  & 6.771 \\
\bottomrule
\end{tabular}\\~\\
(a) \\
\end{center}

\begin{center}
\begin{tabular}{llccccc}
\toprule
& & \multicolumn{5}{c}{----------- Number of layers ------------}\\
& Norm & 4 & 6 & 8 & 10 & 12 \\
\midrule
A to B & L1 norm & 317200 & 228700 & 356500 & 247200 & 164200  \\
&Average L1 norm per layer & 9329 & 6726 & 10485 & 7271 & 4829  \\ 
&L2 norm  &528.1 & 401.7 & 559.6 & 410.1 & 346.8 \\ 
&Average L2 norm per layer &3.031 & 2.284 & 3.242 &2.257 & 1.890 \\
\midrule
B to A&L1 norm & 316900 & 194500 & 353900 & 171500 & 228900 \\
&Average L1 norm per layer & 9323 & 5719 & 10410 & 5045 & 6733\\
&L2 norm & 523.2 & 375.9 & 555.7 & 346.5 & 373.3  \\
&Average L2 norm per layer & 3.003 & 2.029 &  3.210 & 1.921 & 2.289  \\
\bottomrule
\end{tabular}\\~\\
(b) \\
\end{center}

\end{table}


\begin{table}[h]\caption{Seeking DPMs: the distance from the identity and the discrepancy (GAN loss) for various numbers of layers, where training a DiscoGAN from a dataset to itself. \label{tab:c001}} 

\begin{center}
\begin{tabular}{lccccccc}
\toprule
& & \multicolumn{6}{c}{---------- Number of layers: ----------}\\
Dataset & loss & 4 & 6 & 8 & 10 & 12 & 14\\
\midrule
Males & $\sum_{x\in A}|x-h(x)|$ & \bf{0.09} & 0.42 & 0.45 & 0.45 & 0.45 & 0.45 \\
& Discrepancy & 0.37 & 0.60 & 0.27 & 0.20 & 0.17 & \bf{0.10} \\     
\midrule                   
Females & $\sum_{x\in A}|x-h(x)|$ & \bf{0.06} & 0.36 & 0.43 & 0.42 & 0.44 & 0.45 \\
& Discrepancy & 0.32 & 0.40 & 0.15 & \bf{0.11} &  \bf{0.11} & \bf{0.11} \\   
    \midrule  
Handbags & $\sum_{x\in A}|x-h(x)|$ & \bf{0.10} & 0.28 & 0.37 & 0.37 & 0.38 & 0.37 \\
& Discrepancy& \bf{0.13} & 0.28 & 0.24 & 0.14 & 0.15 & 0.20 \\
\midrule 
Shoes& $\sum_{x\in A}|x-h(x)|$ & \bf{0.06} & 0.15 & 0.29 & 0.30 & 0.30 & 0.30 \\
& Discrepancy & 0.15 & 0.28 & 0.20 & 0.15 & \bf{0.10} & \bf{0.10} \\
\midrule   


Edges of handbags& $\sum_{x\in A}|x-h(x)|$ & \bf{0.28} & 0.55 & 0.51 & 0.52 & 0.50 & 0.49 \\
& Discrepancy & \bf{0.18} & 0.28 & 0.58 & 0.47 & 0.40 & 0.35 \\
\midrule       
Edges of shoes& $\sum_{x\in A}|x-h(x)|$ & \bf{0.23} & 0.50 & 0.59 & 0.55 & 0.49 & 0.43 \\
& Discrepancy & \bf{0.17} & 0.21 & 0.65 & 0.46 & 0.45 & 0.45 \\
\bottomrule
\end{tabular}
\end{center}
\end{table}

\begin{table}[t]
\centering
\caption{Results for Alg.~\ref{algo:duallayers} for different datasets. VGG Similarity is given in the first column. The second column gives the separation value using the linear classifier. In the third column, we measure the discrepancy of the mapping. The last column provides the distance of $h$ to $g$, where applicable.}
\label{tab:alg1exp1}
\begin{tabular}{lccccccc}
\\
\toprule
Dataset &  $f$ & Complexity & Descriptor & Separation & Discrepancy & Distance \\

& & & Similarity & Accuracy & $\disc(f \circ D_A,D_B)$ & $R_{D_A}[h,g]$\\

\midrule
Male to Female & $g$ & $k_1=6$  &      0.267 & 0.928 & 0.230    &     -        \\
               & $g$ & $k=8$  & 0.280       & 0.938      &    \bf{0.077}         &          -        \\
               & $g$& $k=10$ & 0.106      & 0.940       &     0.094        &        -          \\
               & $g$& $k=12$ & 0.089      & 0.940       &   0.083          &       -           \\
               & $h$& $k_2=8$ &      \bf{0.316} &	0.933&	0.087&	\bf{0.054}             \\
               & $h$& $k_2=10$ &      0.204 & 0.937 & 0.109 & 0.075             \\
               & $h$& $k_2=12$ &    0.197 & \bf{0.941} & 0.127& 0.077              \\
\midrule
Female to Male & $g$ & $k_1=6$   &   0.268 & 0.848 & 0.310 &          -       \\
               & $g$ & $k=8$  & 0.260       & 0.848      &    0.107         &           -       \\
               & $g$& $k=10$ & 0.105      & 0.870       &    0.075        &          -        \\
               & $g$& $k=12$ & 0.093      & 0.857      &     0.076        &         -         \\
               & $h$& $k_2=8$ &     \bf{0.304} & 0.878  & 0.107 & \bf{0.056}         \\
               & $h$& $k_2=10$ &    0.215 & \bf{0.884} & 0.082 & 0.083         \\
               & $h$& $k_2=12$ &     0.214 &0.883 & \bf{0.073} & 0.082           \\

\midrule
Blond to  & $g$ & $k_1=6$ & 0.287      & 0.925      &     0.214        &          -        \\
Black Hair                    & $g$& $k=8$  &  0.24	& 0.922                     &          \bf{0.092}   &         -         \\
                    & $g$& $k=10$ &     0.106&	0.917             &        0.097     &         -         \\
                    & $g$& $k=12$ &     0.091 &	0.922           &       0.094      &        -          \\
                    & $h$& $k_2=8$  & \bf{0.293}      & 0.926      &    0.136        &   \bf{0.152}              \\
                    & $h$& $k_2=10$& 0.197      & 0.926      &    0.225         &    0.161              \\
                    & $h$& $k_2=12$ & 0.199      & \bf{0.928}      &     \bf{0.092}        &    0.161              \\
\midrule
Black to &  $g$ & $k_1=6$  & 0.270       & 0.941      &     0.264        &          -        \\
Blond Hair    & $g$& $k=8$ &      0.24	 & 0.911            &    \bf{0.073}         &       -           \\
                    & $g$& $k=10$  &    0.106 &	0.916            &    0.094         &       -           \\
                    & $g$& $k=12$ &     0.087	 & 0.915            &   0.084          &       -           \\
                    & $h$& $k_2=8$ & \bf{0.287}      & 0.938      &    0.077         &     \bf{0.146}             \\
                    &  $h$& $k_2=10$ & 0.179      & 0.946      &   0.165          &     0.149             \\
                    & $h$& $k_2=12$ & 0.180       & \bf{0.952}      &    0.168         &    0.152       \\


\midrule
Eyeglasses to \\
Non-Eyeglasses               & $g$& $k_1=6$  &  \bf{0.391} &	0.785 &	0.159 &            -      \\
                             & $g$& $k=8$  & 0.347      & 0.828      &    \bf{0.071}         &            -      \\
                             & $g$& $k=10$ &   0.114         &    0.843        &   0.082        &       -         \\
                             & $g$& $k=12$ &   0.125         &     0.849       &  0.083           &      -           \\
                             & $h$& $k_2=8$  &    \bf{0.391} & 0.786 & 0.097 & \bf{0.058}                 \\
                             & $h$& $k_2=10$  &     0.283 & 0.847 & 0.180 & 0.083            \\
                             & $h$& $k_2=12$ &   0.274 & \bf{0.860} & 0.148 & 0.081    \\
\midrule
Non-Eyeglasses \\
to Eyeglasses                & $g$& $k_1=6$  &     0.373 & 0.749 & 0.518 &      -      \\
                             & $g$ & $k=8$  & 0.364      & 0.766      &      0.236       &          -        \\
                             & $g$& $k=10$ &   0.105         &     \bf{0.848}       &       0.263      &           -       \\
                             & $g$& $k=12$ &   0.108         &     0.832       &    \bf{0.093}         &         -         \\
                             & $h$& $k_2=8$  &    \bf{0.389} & 0.780 & 0.300 & \bf{0.063}         \\
                             & $h$& $k_2=10$  & 0.272 & 0.807 & 0.370 & 0.083           \\
                             & $h$& $k_2=12$ &   0.282 & 0.803 & 0.409 & 0.081         \\
                             
\bottomrule      
\end{tabular}
\end{table}






\begin{figure}[t]
  \centering
  
 \includegraphics[width=1.01\linewidth,clip,trim={4cm 0px 2cm 4cm}]{figures_2to6/image2to6-1.pdf}
  \caption{\label{fig:a001} Results for celebA Male to Female transfer for networks with different number of layers.}
\end{figure}

 
\begin{figure}[t]
  \centering
  
 \includegraphics[width=1.01\linewidth,clip,trim={4cm 0px 2cm 4cm}]{figures_2to6/image2to6-2.pdf}
  \caption{\label{fig:a002} Results for celebA Female to Male transfer for networks with different number of layers. The case of 4 layers failed to produce acceptable results.}
\end{figure}


\begin{figure}[t]
  \centering
  
 \includegraphics[width=1.01\linewidth,clip,trim={4cm 0px 2cm 4cm}]{figures_2to6/image2to6_blond-1.pdf}
  \caption{\label{fig:a003} Results for celebA Blond to Black Hair transfer for networks with different number of layers.}
\end{figure}

 
\begin{figure}[t]
  \centering
  
 \includegraphics[width=1.01\linewidth,clip,trim={4cm 0px 2cm 4cm}]{figures_2to6/image2to6_blond-2.pdf}
  \caption{\label{fig:a004} Results for celebA Black Hair to Blond transfer for networks with different number of layers.}
\end{figure}

\begin{figure}[t]
  \centering
  
 \includegraphics[width=1.01\linewidth,clip,trim={4cm 0px 2cm 4cm}]{figures_2to6/image2to6_eyeglasses-1.pdf}
  \caption{\label{fig:a005} Results for celebA Eyeglasses to Non-Eyeglasses transfer for networks with different number of layers.}
\end{figure}  

 
\begin{figure}[t]
  \centering
  
 \includegraphics[width=1.01\linewidth,clip,trim={4cm 0px 2cm 4cm}]{figures_2to6/image2to6_eyeglasses-2.pdf}
  \caption{\label{fig:a006} Results for celebA Non-Eyeglasses to Eyeglasses transfer for networks with different number of layers.}
\end{figure}


\begin{figure}[t]

  \centering

   

\includegraphics[width=0.89\linewidth,clip,trim={4cm 0px 2cm 4cm}]{figures2to7ID/cimage2to7male_results-1.jpg}

  \caption{\label{fig:c003} Results for mapping Males to itself (B=A) using a DiscoGAN architecture and enforcing that the mapping is not the identity mapping. The odd rows present the learned mapping $h$, and the even rows present the full cycle $h'\circ h$.}
\end{figure}

\begin{figure}[t]
  \centering
\includegraphics[width=0.89\linewidth,clip,trim={4cm 0px 2cm 4cm}]{figures2to7ID/cimage2to7female_results-1.jpg}

  \caption{\label{fig:c004} Results for mapping the Females to itself (B=A) using a DiscoGAN architecture and enforcing that the mapping is not the identity mapping. The odd rows present the learned mapping $h$, and the even rows present the full cycle $h'\circ h$.}

\end{figure}

\begin{figure}[t]
  \centering
\includegraphics[width=0.89\linewidth,clip,trim={4cm 0px 2cm 4cm}]{figures2to7ID/cimage2to7edges_from_shoes_results-1.jpg}

  \caption{\label{fig:c001} Results for mapping shoe edges to itself (B=A) using a DiscoGAN architecture and enforcing that the mapping is not the identity mapping. The odd rows present the learned mapping $h$, and the even rows present the full cycle $h'\circ h$.}

\end{figure}

\begin{figure}[t]
  \centering
\includegraphics[width=0.89\linewidth,clip,trim={4cm 0px 2cm 4cm}]{figures2to7ID/cimage2to7edges_from_handbags_results-1.jpg}
  \caption{\label{fig:c002} Results for mapping handbag edges to itself (B=A), using a DiscoGAN architecture and enforcing that the mapping is not the identity mapping. The odd rows present the learned mapping $h$, and the even rows present the full cycle $h'\circ h$.}
\end{figure}

\begin{figure}[t]
  \centering
\includegraphics[width=0.89\linewidth,clip,trim={4cm 0px 2cm 4cm}]{figures2to7ID/cimage2to7handbags_results-1.jpg}
  \caption{\label{fig:c005} Results for mapping handbags to itself (B=A), using a DiscoGAN architecture and enforcing that the mapping is not the identity mapping. The odd rows present the learned mapping $h$, and the even rows present the full cycle $h'\circ h$.}
\end{figure}

\begin{figure}[t]
  \centering
\includegraphics[width=0.89\linewidth,clip,trim={4cm 0px 2cm 4cm}]{figures2to7ID/cimage2to7shoes_results-1.jpg}
  \caption{\label{fig:c006} Results for mapping shoes to itself (B=A) using a DiscoGAN architecture and enforcing that the mapping is not the identity mapping. The odd rows present the learned mapping $h$, and the even rows present the full cycle $h'\circ h$.}
\end{figure}

\begin{figure}[t]
  \centering
\includegraphics[width=0.89\linewidth,clip,trim={4cm 0px 2cm 4cm}]{figures_dual/dimagereg_results_male2female-1.jpg}
  \caption{\label{fig:alg9} Results for Alg.~\ref{algo:duallayers} on Male2Female dataset for mapping male to female. Shown is a minimal complexity mapping $g$ that has low discrepancy, and various mappings $h$ obtained by the method.}
\end{figure}

\begin{figure}[t]
  \centering
\includegraphics[width=0.89\linewidth,clip,trim={4cm 0px 2cm 4cm}]{figures_dual/dimagereg_results_male2female_r-1.jpg}
  \caption{\label{fig:alg10} Results for Alg.~\ref{algo:duallayers} on Male2Female dataset for mapping female to male. Shown is a minimal complexity mapping $g$ that has low discrepancy, and various mappings $h$ obtained by the method.}
\end{figure}


\begin{figure}[t]
  \centering
\includegraphics[width=0.89\linewidth,clip,trim={4cm 0px 2cm 4cm}]{figures_dual/dimagereg_results_blond.pdf}
  \caption{\label{fig:alg1} Results for Alg.~\ref{algo:duallayers} on celebA dataset for mapping blond to black. Shown is a minimal complexity mapping $g$ that has low discrepancy, and various mappings $h$ obtained by the method.}
\end{figure}


\begin{figure}[t]
  \centering
\includegraphics[width=0.89\linewidth,clip,trim={4cm 0px 2cm 4cm}]{figures_dual/dimagereg_results_blond_r-1.jpg}
  \caption{\label{fig:alg2} Results for Alg.~\ref{algo:duallayers} on celebA dataset for mapping black to blond. Shown is a minimal complexity mapping $g$ that has low discrepancy, and various mappings $h$ obtained by the method.}

\end{figure}



\begin{figure}[t]
  \centering
\includegraphics[width=0.89\linewidth,clip,trim={4cm 0px 2cm 4cm}]{figures_dual/dimagereg_results_eyeglasses-1.jpg}
  \caption{\label{fig:alg7} Results for Alg.~\ref{algo:duallayers} on Eyeglasses dataset for mapping eyeglasses to no eyeglasses. Shown is a minimal complexity mapping $g$ that has low discrepancy, and various mappings $h$ obtained by the method.}
\end{figure}


\begin{figure}[t]
  \centering
\includegraphics[width=0.89\linewidth,clip,trim={4cm 0px 2cm 4cm}]{figures_dual/dimagereg_results_eyeglasses_r-1.jpg}
  \caption{\label{fig:alg8} Results for Alg.~\ref{algo:duallayers} on Eyeglasses dataset for mapping no eyeglasses to eyeglasses. Shown is a minimal complexity mapping $g$ that has low discrepancy, and various mappings $h$ obtained by the method.}

\end{figure}


\begin{figure}[t]
  \centering
\includegraphics[width=0.89\linewidth,clip,trim={4cm 0px 2cm 4cm}]{figures_dual/dimagereg_results_edges2handbags-1.jpg}
  \caption{\label{fig:alg3} Results for Alg.~\ref{algo:duallayers} on Edges2Handbags dataset for mapping edges to handbags. Shown is a minimal complexity mapping $g$ that has low discrepancy, and various mappings $h$ obtained by the method.}
\end{figure}


\begin{figure}[t]
  \centering
\includegraphics[width=0.89\linewidth,clip,trim={4cm 0px 2cm 4cm}]{figures_dual/dimagereg_results_edges2handbags_r-1.jpg}
  \caption{\label{fig:alg4} Results for Alg.~\ref{algo:duallayers} on Edges2Handbags dataset for mapping handbags to edges. Shown are a minimal complexity mapping $g$ that has low discrepancy, and various mappings $h$ obtained by the method.}
\end{figure}


\begin{figure}[t]
  \centering
\includegraphics[width=0.89\linewidth,clip,trim={4cm 0px 2cm 4cm}]{figures_dual/dimagereg_results_edges2shoes-1.jpg}
  \caption{\label{fig:alg5} Results for Alg.~\ref{algo:duallayers} on Edges2Shoes dataset for mapping edges to shoes. Shown are a minimal complexity mapping $g$ that has low discrepancy, and various mappings $h$ obtained by the method.}
\end{figure}


\begin{figure}[t]
  \centering
\includegraphics[width=0.89\linewidth,clip,trim={4cm 0px 2cm 4cm}]{figures_dual/dimagereg_results_edges2shoes_r-1.jpg}
  \caption{\label{fig:alg6} Results for Alg.~\ref{algo:duallayers} on Edges2Shoes dataset for mapping shoes to edges. Shown are a minimal complexity mapping $g$ that has low discrepancy, and various mappings $h$ obtained by the method.}
\end{figure}

\FloatBarrier
\newpage

\appendix


\section{A Generalized and Formal Statement of the Results}
\label{sec:extended}

For brevity, we have not presented our results in the most general way. For example, in Def.~\ref{def:DPMinformal}, we did not bound the complexity of the discriminators. For the same reason, some of our terms were described and not yet formally defined.

\subsection{A Complexity Measure for Functions}\label{sec:complexMeasure2}

In order to model the composition of neural networks, we define a complexity measurement that assigns a value based on the number of simple functions that make up a complex function. 

\begin{defn}[Stratified complexity model (SCM)]\label{def:SCM} A stratified complexity model $\mathcal{N} := \textnormal{SCM}[\mathcal{C}]$ is a hypothesis class  of functions $p:\mathbb{R}^M \rightarrow \mathbb{R}^M$ specified by a set of functions $\mathcal{C}$. Every function $p$ in $\mathcal{N}$ has an appropriate decomposition:
\begin{itemize}
\item $\mathcal{N} = \bigcup^{\infty}_{n=0} \mathcal{C}^n$ (where, $ \mathcal{C}^n = \{p_n \circ ... \circ p_1 \vert p_1,...,p_n \in \mathcal{C}\}$  and $\mathcal{C}^0 = \{\Id\}$).
\item Every function in $\mathcal{C}$ is invertible.
\end{itemize}
\end{defn}

A SCM partitions a set of invertible functions into disjoint {\em complexity classes}, 
\begin{small}
\begin{equation}\label{eq:complex1}
\begin{aligned}
&\mathcal{C}_0 := \left\{p\in \mathcal{N} \;\Big\vert\; \forall \; n \in \mathbb{N}, \; q \in \mathcal{C}^n: p\circ q, q \circ p,p^{-1}\circ q, q \circ p^{-1} \in \mathcal{C}^n \right\} \\
&\mathcal{C}_n := 
\mathcal{C}^n \setminus \left[\bigcup^{n-1}_{i=0} \mathcal{C}^i \cup \mathcal{C}_0 \right]
\end{aligned}
\end{equation}
\end{small}
When considering simple functions $p_i$ that are layers in a neural network, each complexity class contains  the functions that are implemented by networks of $n$ hidden layers. In addition, we denote the {\em complexity of a function} $p$: 
\begin{equation}\label{eq:complex2}
C(p) := \underset{n \in \mathbb{N} \cup \{0\} }{\arg} \{ p \in \mathcal{C}_n \}
\end{equation}
If the complexity of a function $p$ equals $n$, then any appropriate decomposition $p=p_n \circ ... \circ p_1$ will be called a {\em minimal decomposition} of $p$. According to this measurement, the complexity of a function $p$ is determined by the minimal number of primitive functions required in order to represent it.


In this work, we focus our attention on SCMs that represent the architectures of fully connected neural networks with layers of a fixed size, i.e., 

\begin{defn}[NN-SCM] \label{def:nnscm}
A NN-SCM is a SCM $\mathcal{N}=\textnormal{SCM}[\mathcal{C}]$ that satisfies the following conditions: 
\begin{itemize}
\item $\mathcal{C} = \left\{W_2 \circ \sigma \circ W_1 \Big\vert W_1,W_2 \in \mathbb{R}^{M\times M} \textnormal{ and $W_1,W_2$ are invertible}\right\}$. Here, $W_1,W_2$ denote both linear transformations and the associated matrix forms.
\item $\sigma$ is a non-linear element-wise activation function.
\end{itemize}
For brevity, we denote $\mathcal{N} : =\textnormal{SCM}[\sigma]$ to refer to a NN-SCM with the activation function $\sigma$.
\end{defn}

 

The NN-SCM with the Leaky ReLU activation function is of a particular interest, since~\citep{discogan,CycleGAN2017} employ it as the main activation function (plain ReLUs and $\tanh$ are also used). In the NN-SCM framework, to specify the function obtained by a decomposition  $W_{n} \circ \sigma \circ W_{n-1} \circ \sigma \circ ... \circ \sigma \circ W_1$ we simply write:
\begin{equation}
F[W_{n},...,W_1] := W_{n} \circ \sigma \circ W_{n-1} \circ \sigma \circ ... \circ \sigma \circ W_1
\end{equation}

It is useful to characterize the effect of inversion on the complexity of functions, since, for example, we consider both $h'=\Pi \circ h$ and $h=\Pi^{-1} \circ h'$. The following lemma states that, in the case of NN-SCM with $\sigma$ that is the Leaky ReLU, the complexity of the inverse function is the same as that of the original function.

\begin{restatable}{lem}{inverse}\label{lem:inverse}
Let $\mathcal{N} = \textnormal{SCM}[\sigma]$ be a NN-SCM with $\sigma$ that is the Leaky ReLU with parameter $0<a \neq 1$. Then, for any $u\in \mathcal{N}$, $C(u^{-1}) = C(u)$.
\end{restatable}

\begin{proof} First, we denote $C'(p)$ the minimal number $n$ such that there are invertible linear mappings $W_{1},...,W_{n+1}$ such that $p = F[W_{n+1},...,W_1]$ (if $p = \Id$ then $C'(p) = 0$). This complexity measure is similar to the complexity measure $C$. For a function $p$ such that $C(p)\neq 0$, we have, $C(p)=C'(p)$. Nevertheless, for $p$ such that $C(p)=0$, it is not necessarily true that $C'(p)=0$. For example, if $p\neq \Id$ is an invertible linear mapping, we have, $C(p)=0$ and $C'(p)=2$. Let $p = F[W_2,W_1] = W_2 \circ \sigma \circ W_1$ be any function such that $C(p)=1$. We consider that:
\begin{equation}
\sigma^{-1} = -\Id \circ \sigma \circ -\Id/a = F[-\Id,-\Id/a]
\end{equation}
Therefore, 
\begin{equation}
F[W_2,W_1]^{-1} = -W^{-1}_1 \circ \sigma \circ -W^{-1}_2/a = F[-W^{-1}_1,-W^{-1}_2/a]
\end{equation}
In particular, $C'(p^{-1}) \leq 1$. If $C'(p^{-1}) = 0$, then, $\Id = -W^{-1}_1 \circ \sigma \circ -W^{-1}_2/a$ and, therefore, $\sigma$ is a linear mapping - in contradiction. Thus, $C'(p^{-1}) = 1$.
 
Next, we would like to show that for any $u \in \mathcal{N}$, $C'(u^{-1}) = C'(u)$. Let $u$ such that $C'(u) = 0$. Then, $u = u^{-1} = \Id$ and therefore, $C'(u^{-1}) = 0$. Let $u = F[W_{n+1},...,W_1]$ be a function such that $C'(u)=n>0$. Then, 
\begin{equation}
u = F[W_{n+1},W_n] \circ F[\Id,W_{n-1}] \circ ... \circ F[\Id,W_1]
\end{equation}
In particular, 
\begin{equation}
u^{-1} = F[\Id,W_1]^{-1} \circ ... \circ  F[\Id,W_{n-1}]^{-1} \circ  F[W_{n+1},W_n]^{-1} 
\end{equation}
or,
\begin{equation}
u^{-1} = F[-W^{-1}_1,W^{-1}_2/a,...,W^{-1}_n/a,-W^{-1}_{n+1}/a]
\end{equation}
Therefore, by Lem.~\ref{lem:A2},
\begin{equation}
C'(u^{-1}) \leq  C'(F[\Id,W_1]^{-1}) +...+ C'(F[\Id,W_{n-1}]^{-1}) + C'(F[W_{n+1},W_n]^{-1}) = n 
\end{equation}
On the other hand, if $v = u^{-1}$, $n = C'(u) = C'(v^{-1}) \leq C'(v) = C'(u^{-1}) \leq n$ and $C'(u^{-1}) = C'(u)$. Finally, we would like to show that for every $u \in \mathcal{N}$, we have: $C(u^{-1})=C(u)$. If $C(u) = 0$, then, by Lem.~\ref{lem:closureC0}, $C(u^{-1}) = 0$. On the other hand, if $C(u)\neq 0$, then, by Lem.~\ref{lem:closureC0}, $C(u^{-1}) \neq 0$ and by the above: $C(u) = C'(u) = C'(u^{-1}) = C(u)$.
\end{proof}

\subsection{Minimal Complexity Mappings}

 

Based on our simplicity hypothesis, we present a definition of a minimal complexity mapping that is both intuitive and well-defined in concrete complexity terms. Given two distributions $D_A$ and $D_B$, a minimal complexity mapping $f:\mathcal{X}_A\rightarrow \mathcal{X}_B$ between domains $A$ and $B$ is a mapping that has minimal complexity among the functions $h:\mathcal{X}_A\rightarrow \mathcal{X}_B$ that satisfy $h \circ D_A \approx D_B$. 

Consider, again, the example of a line segment in $\mathbb{R}^M$ (Sec.~\ref{sec:toy}) and the semantic space of the interval, $[0,1]\subset \mathbb{R}$. The two linear mappings, which map either segment ends to $0$ and the other to $1$ are minimal, when using $f$ that are ReLU based neural networks. Other mappings to this segment are possible, simply by permuting points on the segment in $\mathbb{R}^M$. However, these alternative mappings have higher complexity, since the two mappings above are the only ones with the minimal possible complexity.
 

In order to measure the distance between $h \circ D_A$ and $D_B$, we use the discrepancy distance, $\disc_{\mathcal{D}}$. In this work, we focus on classes of discriminators $\mathcal{D}$ of the form $\mathcal{D}_m := \{u \vert C(u) \leq m\}$ for some $m \in \mathbb{N}$. In addition, for simplicity, we will write $\disc_m := \disc_{\mathcal{D}_m}$.

 

\begin{defn}[Minimal complexity mappings]\label{def:semantic} Let $\mathcal{N} = \textnormal{SCM}[\mathcal{C}]$. Let $A = (\mathcal{X}_A,D_A)$ and $B = (\mathcal{X}_B,D_B)$ be two domains. We define the $(m,\epsilon_0)$-minimal complexity between $A$ and $B$ as:
\begin{equation}
C^{m,\epsilon_0}_{A,B}  := \min_{i \in \mathbb{N}\cup \{0\}}  \left\{ \exists h \textnormal{ s.t } C(h)=i \textnormal{ and } \disc_{m}(h \circ D_A, D_B) \leq \epsilon_0 \right\}
\end{equation}
The set of $(m,\epsilon_0)$-minimal complexity mappings between $A$ and $B$ is: 
\begin{equation}
H_{\epsilon_0}(A,B;m) := \left\{h \Big\vert C(h) \leq C^{m,\epsilon_0}_{A,B} \textnormal{ and }  \disc_{m}(h \circ D_A, D_B) \leq \epsilon_0 \right\}
\end{equation}
\end{defn}

 

We note that for any fixed $\epsilon_0 >0$, the sequence $\{ C^{m,\epsilon_0}_{A,B} \}^{\infty}_{m=0}$ is monotonically increasing as $m$ tends to infinity. In addition, we assume that for every two distributions of interest, $D_I$ and $D_J$, and an error rate $\epsilon_0 >0$, there is a function $h$ of finite complexity such that $\disc_{\infty}(h \circ D_I,D_J) \leq \epsilon_0$. Therefore, the sequence $\{ C^{m,\epsilon_0}_{A,B} \}^{\infty}_{m=0}$ is upper bounded by $C(h)$ for all $m \in \mathbb{N} \cup\{0\}$. In particular, there is a minimal value $m_0>0$ such that $C^{m,\epsilon_0}_{A,B} = C^{m_0,\epsilon_0}_{A,B}$ for all $m\geq m_0$. We denote: $E^{\epsilon_0}_{A,B} := m_0$ and $C^{\epsilon_0}_{A,B} := C^{m_0,\epsilon_0}_{A,B}$. For simplicity, sometimes we will assume that $m = \infty$. In this case, we will write $H_{\epsilon_0}(A,B) := H_{\epsilon_0}(A,B;\infty)$.

 

\subsection{Identifiability}

 

Every neural network implementation gives rise to many alternative implementations by performing simple operations, such as permuting the units of any hidden layer, and then permuting back as part of the linear mapping in the next layer. Therefore, it is first required to identify and address the set of transformations that could be inconsequential to the function which the network computes.

 

\begin{defn}[Invariant set]\label{def:invariant} Let $\mathcal{N} = \textnormal{SCM}[\sigma]$ be a NN-SCM. The invariant set $\Inv(\mathcal{N})$ is the set of all $\tau: \mathbb{R}^M \rightarrow \mathbb{R}^M$ that satisfy the following conditions: 

\begin{itemize}
\item $\tau:\mathbb{R}^{M} \rightarrow \mathbb{R}^M$ is an invertible linear transformation.
\item $\sigma \circ \tau = \tau \circ \sigma$.
\end{itemize}
Functions in $\Inv(\mathcal{N})$ are called invariants or invariant functions.
\end{defn} 

For example, for neural networks with the {\em {$\tanh$}} activation function, the set of invariant functions contains the linear transformations that take vectors, permute them and multiply each coordinate by $\pm 1$. Formally, each $\tau = [\epsilon_1 \cdot \text{e}_{\pi(1)}, ..., \epsilon_M \cdot \text{e}_{\pi(M)}]$ where $\text{e}_{i}$ is the $i$'th standard basis vector, $\pi$ is a permutation over $[M]$ and $\epsilon_i \in \{\pm 1\}$ ~\citep{DBLP:conf/nips/FeffermanM93}. 

In the following lemma, we characterize the set of all invariant functions for $\sigma$ that is Leaky ReLU with parameter $0<a \neq 1$.

\begin{lem}\label{lem:invariantSet} Let $\mathcal{N} = \textnormal{SCM}[\sigma]$ with $\sigma$ be Leaky ReLU with parameter $0<a\neq 1$. Then,
\begin{small}
\begin{equation}\label{eq:invariants}
\Inv(\mathcal{N}) = \left\{\tau \in \mathbb{R}^{M\times M} \;\Big\vert\; 
\tau=[c_1\cdot \textnormal{e}_{\pi(1)},...,c_M \cdot \textnormal{e}_{\pi(M)}], \textnormal{ where $\forall i \in [M]:c_i > 0$ and $\pi \in \textnormal{Sym}_M$} 
\right\}
\end{equation}
\end{small}
Here, $\textnormal{e}_i$ denotes the $i$'th standard basis vector in $\mathbb{R}^M$ and $\textnormal{Sym}_M$ is the set of permutations of $[M]$.
\end{lem}

\begin{proof} Let $\tau$ be an invertible linear mapping satisfying $\sigma \circ \tau = \tau \circ \sigma$. We consider that for all $i \in [M]$ and vector $x$; $\sigma (\langle \tau_i,x\rangle) = \langle \tau_i , \sigma(x)\rangle$, where $\tau_i$ is the $i$'th row of $\tau$ and $\tau_{i,j}$ is the $(i,j)$ entry of $\tau$. For $x = \textnormal{e}_j$, we have:
\begin{equation}
\tau_{i,j} = \sigma(\tau_{i,j})
\end{equation}
For $x = -\textnormal{e}_j$, we have:
\begin{equation}
-a\tau_{i,j} = \sigma(-\tau_{i,j})
\end{equation}
If $\tau_{i,j} < 0$, then the first equation leads to contradiction. Otherwise, the equations are both satisfied. 

Finally, for $x = \textnormal{e}_j-\textnormal{e}_k$, we have:
\begin{equation}
\tau_{i,j}-a\tau_{i,k} = \sigma(\tau_{i,j} - \tau_{i,k})
\end{equation}
If $\tau_{i,j} - \tau_{i,k} = 0$, then, $\tau_{i,j}-a\tau_{i,k}=0$ and since $a \neq 1,0$, we have, $\tau_{i,j} = \tau_{i,k} = 0$. If $\tau_{i,j} - \tau_{i,k} \geq 0$, then,
$\tau_{i,j} - \tau_{i,k} = \tau_{i,j}-a\tau_{i,k}$ that gives $\tau_{i,k} = 0$. If $\tau_{i,j} - \tau_{i,k} \leq 0$, then,
$a(\tau_{i,j} - \tau_{i,k}) = \tau_{i,j}-a\tau_{i,k}$ that yields $\tau_{i,j} = 0$. Therefore, for each $i \in [M]$ there is at most one entry $\tau_{i,j}$ that is not $0$. If for all $j \in [M]$, $\tau_{i,j} = 0$, then the mapping $\tau$ is not invertible, in contradiction. Therefore, for each $i \in [M]$ there is exactly one entry $\tau_{i,j} > 0$ (it is non-negative as shown above). Finally, if there are $i_1\neq i_2$ such that $\tau_{i_1,j},\tau_{i_2,j} \neq 0$ then the matrix is invertible. Therefore, $\tau$ is a member of the set defined in Eq.~\ref{eq:invariants}. In addition, it is easy to see that every member of the noted set satisfies the conditions of the invariant set. Thus, we obtain the desired equation. 
\end{proof}

Our analysis is made much simpler, if every function has one invariant representation up to a sequence of manipulations using invariant functions that do not change the essence of the processing at each layer.


\begin{restatable}[Identifiability]{assumption}{identif}\label{assumption:identifiability} Let $\mathcal{N} = \textnormal{SCM}[\sigma]$ with $\sigma$ that is Leaky ReLU with parameter $0< a\neq 1$. Then, every function $p \in \mathcal{N}$ is identifiable (with respect to $\Inv(\mathcal{N})$), i.e., for any two minimal decompositions, $p = F[W_{n+1},...,W_1] = F[V_{n+1},...,V_1]$, there are invariants $\tau_1,...,\tau_n \in \Inv(\mathcal{N})$ such that: 
\begin{equation}
\begin{aligned}
&V_1 = \tau_1 \circ W_1, \; \forall i=2,...,n: V_i =  \tau_i \circ W_i \circ \tau^{-1}_{i-1} \textnormal{ and } V_{n+1} =  W_{n+1} \circ \tau^{-1}_{n}\\
\end{aligned}
\end{equation}
\end{restatable}

Uniqueness up to invariants, also known as identifiability, forms an open question. \cite{DBLP:conf/nips/FeffermanM93} proved identifiability for the $\tanh$ activation function. Other works~\citep{DBLP:journals/tnn/WilliamsonH95,albertini,DBLP:journals/nn/KurkovaK14,DBLP:journals/nn/Sussmann92} prove such uniqueness for neural networks with only one hidden layer and various classical activation functions. In the following lemma, we show that identifiability holds for Leaky ReLU networks with only one hidden layer.

 

\begin{lem}\label{lem:identifyDepth2} Let $\mathcal{N} = \textnormal{SCM}[\sigma]$ with $\sigma$ that is Leaky ReLU with parameter $0<a\neq 1$. Any function $p$ such that $C(p)=1$ is identifiable, i.e, if $p = F[W_2,W_1] = F[V_2,V_1]$, then, $W_1 = \tau \circ V_1$ and $W_2 = V_2 \circ \tau^{-1}$ for some $\tau \in \Inv(\mathcal{N})$.
\end{lem}


\begin{proof} An alternative representation of the equation is:
\begin{equation}
(\sigma \circ W_1\circ V^{-1}_1) = (W_2^{-1} \circ V_2 \circ \sigma)
\end{equation}
We would like to prove that if $\sigma \circ U = V \circ \sigma$ then $V = U$. We have:
\begin{equation}
\sigma \circ U(x) = V \circ \sigma (x)
\end{equation}
In particular, if $v_i$ is the $i$'th row of $V$ (similarly $u_i$) and $x = \textnormal{e}_j$:
\begin{equation}
\sigma(u_{i,j}) = \sigma (\langle u_i, \textnormal{e}^{\top}_j \rangle)  = \langle v_i , \sigma (\textnormal{e}^{\top}_j) \rangle = v_{i,j}
\end{equation}
where $v_{i,j}$ is the $(i,j)$ entry of $V$ (similarly $u_{i,j}$). Similarly, for $x = -\textnormal{e}_j$:
\begin{equation}
\sigma(-u_{i,j}) = \sigma (\langle u_i, -\textnormal{e}^{\top}_j \rangle)  = \langle v_i , \sigma (-\textnormal{e}^{\top}_j) \rangle = -av_{i,j}
\end{equation}
If $u_{i,j}$ is negative, we obtain: $au_{i,j} = v_{i,j}$ (the first equation) and $-u_{i,j} = -av_{i,j}$ (the second equation) that yields $a = 1$ in contradiction. Therefore, $u_{i,j}\geq 0$ and $u_{i,j} = v_{i,j}$ (the second equation). 

We conclude that $W_1\circ V^{-1}_1 = W_2^{-1} \circ V_2 := \tau$. Finally, since  $(\sigma \circ W_1\circ V^{-1}_1) = (W_2^{-1} \circ V_2 \circ \sigma)$ we have $\sigma \circ \tau = \tau \circ \sigma$ and $\tau$ is invertible linear mapping. Differently said, $W_1 = \tau \circ V_1$ and $W_2 = V_2 \circ \tau^{-1}$ such that $\tau \in \Inv(\mathcal{N})$.
\end{proof}

As far as we know, there are no other results continuing the identifiability line of work for activation functions such as Leaky ReLU. Uniqueness, which is stronger than identifiability, since it means that even multiple representations with different number of layers do not exist, does not hold for these activation functions. To see this, note that for every $M \times M$ invertible linear mapping $W$, the following holds: 
\begin{equation}
U \circ \sigma \circ W = U \circ \sigma \circ W \circ \sigma \circ -\Id \circ \sigma \circ  -\Id/a
\end{equation}
where $\sigma$ is the Leaky ReLU activation function with parameter $a$.
We conjecture that for networks with Leaky ReLU activations identifiability holds, or at least for networks with a fixed number of neurons per layer.  In addition to identifiability, we make the following assumption, which states that almost all mappings are non-degenerate.

 

\begin{restatable}{assumption}{denseOpen}\label{assumption:denseOpen} Let $\mathcal{N} = \textnormal{SCM}[\sigma]$ with $\sigma$ that is Leaky ReLU with parameter $0<a\neq 1$. Assume that the set of $(W_1,...,W_{n+1})\in \mathbb{R}^{M\times M \times m}$ such that $C(F[W_{n+1},...,W_1])=n$ is dense in $\mathbb{R}^{M\times M \times m}$. 
\end{restatable}

 

 

 

\subsection{Counting Minimal Complexity Mappings}

\label{sec:justalignment}

 In the unsupervised alignment problem, the algorithms are provided with only two unmatched datasets of samples from the domains $A$ and $B$ and the task is to learn a well-aligned function between them. Since we hypothesize that the alignment of the target mapping is typically captured by the lowest complexity low-discrepancy mapping, we develop the machinery needed in order to show that such mappings are rare.

Recall that $\disc_m$ is the discrepancy distance for discriminators of complexity up to $m$. In Sec.~\ref{sec:problemformulation}, we have discussed the functions $\Pi$ which replaces between members in the domain $B$ that have similar probabilities. Formally, these are defined using the discrepancy distance.

 

\begin{defn}[Density preserving mapping]\label{def:DPM} Let $\mathcal{N} = \textnormal{SCM}[\mathcal{C}]$ and $X = (\mathcal{X},D_X)$ a domain. A $(m,\epsilon_0)$-density preserving mapping over $X$ (or an $(m,\epsilon_0)$-DPM for short) is a function $\Pi$ such that 
\begin{equation}
\disc_m(\Pi \circ D_X,D_X) \leq \epsilon_0
\end{equation}
We denote the set of all $(m,\epsilon_0)$-DPMs of complexity $k$ by $\textnormal{DPM}_{\epsilon_0}(X;m,k):=\left\{\Pi \big\vert \disc_m(\Pi \circ D_X,D_X) \leq \epsilon_0
\textnormal{ and } C(\Pi)=k \right\}$.
\end{defn}

We would like to bound the number of mappings that are both low-discrepancy and low-complexity by the number of DPMs. We consider that there are infinitely many DPMs. For example, if we slightly perturb the weights of a minimal representation of a DPM, $\Pi$, we obtain a new DPM. Therefore, we define a similarity relation between functions that reflects whether the two are similar. In this way, we are able to bound the number of different (non-similar) minimal-complexity mappings by the number of different DPMs. 
 
\begin{defn}[Closeness between pairs of distributions or functions]\label{def:closed} Let $\mathcal{N} = \textnormal{SCM}[\sigma]$. 
\begin{itemize}
\item We denote $D_1 \underset{m,\epsilon_0}{\sim} D_2 \iff \disc_m(D_1,D_2) \leq \epsilon_0$.
\item We denote $f\overset{D}{\underset{m,\epsilon_0}{\sim}} g $, if $C(f) = C(g) =: n$ and there are minimal decompositions: $f = F[W_{n+1},...,W_1]$ and $g = F[V_{n+1},...,V_1]$ such that $\forall i \in [n+1]:\; F[W_{i},...,W_1] \circ D  \underset{m,\epsilon_0}{\sim} F[V_{i},...,V_1] \circ D$.
\end{itemize}
\end{defn}

The defined relation is reflexive and symmetric, but not transitive. Therefore, there are many different ways to partition the space of functions into disjoint subsets such that in each subset, any two functions are similar. We count the number of functions up to the similarity as the minimal number of subsets required in order to cover the entire space. This idea is presented in Def.~\ref{def:covering}, which slightly generalizes the notion of covering numbers~\citep{Anthony:2009:NNL:1795646}.

 

\begin{defn}[Covering number]\label{def:covering} Let $(\mathcal{U},\sim_\mathcal{U})$ be a set and a reflexive and symmetric relation. A covering of $(\mathcal{U},\sim_\mathcal{U})$, is a tuple $(\mathcal{U},\equiv_\mathcal{U})$ such that: $\equiv_{\mathcal{U}}$ is an equivalence relation and $u_1 \equiv_{\mathcal{U}} u_2 \implies u_1 \sim_{\mathcal{U}} u_2$. The covering number of $(\mathcal{U},\sim_\mathcal{U})$, denoted by $\Cov (\mathcal{U},\sim_\mathcal{U})$, is:
\begin{equation}
\begin{aligned}
\min\big\vert \mathcal{U}/\equiv_{\mathcal{U}} \big\vert \textnormal{ s.t: the minimum is taken over 
$(\mathcal{U},\equiv_\mathcal{U})$ that is a covering of $(\mathcal{U},\sim_\mathcal{U})$}
\end{aligned}
\end{equation}
Here, $\mathcal{U}/\equiv_{\mathcal{U}}$ is the quotient set of $\mathcal{U}$ by $\equiv_{\mathcal{U}}$.
\end{defn}

Thm.~\ref{thm:counting} below states that the number of low discrepancy mappings of complexity $C^{\epsilon_0}_{A,B}$ is upper bounded by the number of DPMs of size $2C^{\epsilon_0}_{A,B}$. By prediction 3, the number of such DPMs is small. The theorem employs the following weak assumption. In Lem.~\ref{lem:contDisc}, we prove that this assumption holds for the case of a continuous risk if the discriminators have bounded weights.

\begin{restatable}{assumption}{contDisc}\label{assumption:contDisc} Let $\mathcal{N} = \textnormal{SCM}[\sigma]$ with $\sigma$ that is Leaky ReLU with parameter $0<a\neq 1$. For every $m>0$ (possibly $\infty$) and $n>0$, the function $\disc_m(F[W_{n},...,W_1] \circ D_1,D_2)$ is continuous as a function of the weights of $W_{1},...,W_n \in \mathbb{R}^{M\times M}$. 

\end{restatable}
 

\begin{restatable}{thrm}{counting}\label{thm:counting} Let $\mathcal{N} = \textnormal{SCM}[\sigma]$ with $\sigma$ that is Leaky ReLU with parameter $0< a \neq 1$. Assume Assumptions~\ref{assumption:identifiability},~\ref{assumption:denseOpen} and~\ref{assumption:contDisc}. Let $\epsilon_0$, $\epsilon_1$ and $\epsilon_2$ be three constants such that $\epsilon_0 < \epsilon_1/4$ and $\epsilon_2 < \epsilon_1-4\epsilon_0$ be three positive constants and $A = (\mathcal{X}_A,D_A)$ and $B = (\mathcal{X}_B, D_B)$ are two domains. Then, 
\begin{small}
\begin{equation}
\begin{aligned}
\Cov\left(H_{\epsilon_0}(A,B),\overset{D_A}{\underset{\epsilon_1}{\sim}}\right) \leq 
\lim_{\epsilon \to 0}\min \begin{cases}
\Cov\left (\textnormal{DPM}_{2\epsilon_0+\epsilon}\left(A; 2C^{\epsilon_0}_{A,B} \right), \overset{D_A}{\underset{\epsilon_2}{\sim}} \right)\\
\Cov\left (\textnormal{DPM}_{2\epsilon_0+\epsilon}\left(B; 2C^{\epsilon_0}_{A,B} \right), \overset{D_B}{\underset{\epsilon_2}{\sim}} \right)
\end{cases}
\end{aligned}
\end{equation}
\end{small}
\end{restatable}

\proof{See Sec.~\ref{app:thmproof}.}

 

\newpage

 

\section{Summary of Notation}

\label{sec:summary}

Tab.~\ref{tab:summary} lists the symbols used in our work.% and the line in which these were defined.
\begin{table}[htbp]\caption{Summary of Notation}\label{tab:summary}
\begin{center}

\begin{tabular}{l c p{10cm} }
\toprule
Symbol & & Explanation\\
\midrule
$\mathcal{X}$ & & A feature space\\
$\mathcal{X}_A,\mathcal{X}_B$ &  & The sample spaces of $A$ and $B$ (resp.)\\
$D_A,D_B$ &  & Distributions over $\mathcal{X}_A$ and $\mathcal{X}_B$ (resp.)\\ 
$A,B$ &  & Two domains; Specified by $(\mathcal{X}_A, D_A)$ and $(\mathcal{X}_B, D_B)$ (resp.)\\ 
$y_A,y_B$ &  & Functions from the feature space to the domains, $y_A: \mathcal{X} \rightarrow \mathcal{X}_A$ and $y_B: \mathcal{X} \rightarrow \mathcal{X}_B$\\ 
$D_Z$ &  & A distribution over a feature space $\mathcal{X}$\\ 
$y_{AB},y_{BA}$ &  & $y_{AB}=y_B\circ y^{-1}_A$ and $y_{BA} = y_A \circ y^{-1}_B$\\
$\ell$ & & Loss function $\ell : \mathbb{R} \times \mathbb{R} \rightarrow \mathbb{R}$ \\
$R_D[f_1,f_2]$ &  & The risk function $R_D[f_1,f_2] = \mathbb{E}_{x\sim D}\ell(f_1(x),f_2(x))$ where $\ell$ is a loss function and $D$ is a distribution\\
$\disc_{\mathcal{D}}(D_1,D_2)$ &  & The discrepancy between two distributions $D_1$ and $D_2$, i.e, $\disc_{\mathcal{D}}(D_1,D_2) = \sup_{c_1,c_2\in \mathcal{D}}|R_{D_1}[c_1,c_2]-R_{D_2}[c_1,c_2]|$\\
$\sigma$ & & A non-linear element-wise activation function\\
$\mathcal{C}$ & & A class of functions; in most cases $\mathcal{C} = \{W_2 \circ \sigma \circ W_1 \;\vert\; W_1,W_2 \in \mathbb{R}^{M\times M} \textnormal{ are invertible linear transformation}\}$\\
$F[W_{n},...,W_1]$ & & $F[W_{n},...,W_1] = W_{n} \circ \sigma \circ W_n \circ \sigma \circ ... \circ \sigma \circ W_2 \circ \sigma \circ W_1$\\
$\mathcal{N}=\textnormal{SCM}[\mathcal{C}]$ &  & A SCM specified by a class of functions $\mathcal{C}$ (see Def.~\ref{def:SCM})\\
$\mathcal{N}=\textnormal{SCM}[\sigma]$ &  & A NN-SCM specified by the activation function $\sigma$ (see Def.~\ref{def:nnscm})\\
$C(p)$ &  & The complexity of a function $p$ (see Eqs.~\ref{eq:complex1},~\ref{eq:complex2})\\
$\Inv(\mathcal{N})$ & & The invariant set of $\mathcal{N}$ (see Def.~\ref{def:invariant})\\
$\tau$& & An invariant function (see Def.~\ref{def:invariant})\\
$\mathcal{D}_m$ & & $\mathcal{D}_m = \{u \vert C(u) \leq m\}$\\
$\disc_m , \disc$ & & $\disc_m := \disc_{\mathcal{D}_m}$ and $\disc := \disc_{\mathcal{D}_\infty}$\\
$C^{m,\epsilon_0}_{A,B}$ & & The $(m,\epsilon_0)$-minimal complexity between $A$ and $B$ (see Def.~\ref{def:semantic})\\
$C^{\epsilon_0}_{A,B},E^{\epsilon_0}_{A,B}$ & & $C^{\epsilon_0}_{A,B} = \max_{m \geq 1} C^{m,\epsilon_0}_{A,B}$ and $E^{\epsilon_0}_{A,B} =\arg\min_m [C^{m,\epsilon_0}_{A,B} = C^{\epsilon_0}_{A,B}]$\\
$H_{\epsilon_0}(A,B;m)$ & & The set of $(\epsilon_0,m)$-minimal complexity mappings between $A$ and $B$ (see Def.~\ref{def:semantic})\\
$H_{\epsilon_0}(A,B)$ & & $H_{\epsilon_0}(A,B) = H_{\epsilon_0}(A,B;\infty)$\\
$S_1 \circ S_2$ & & A composition of sets, $S_1 \circ S_2 = \{s_1\circ s_2 | s_1 \in S_1 \textnormal{ and } s_2 \in S_2\}$ \\
$D_1 \underset{m,\epsilon}{\sim} D_2, D_1 \underset{\epsilon}{\sim} D_2$ & & $\disc_m(D_1,D_2) \leq \epsilon$ and $\disc(D_1,D_2) \leq \epsilon$ (see Def.~\ref{def:closed})\\
$f \overset{D}{\underset{m,\epsilon}{\sim}} g, f \overset{D}{\underset{\epsilon}{\sim}} g $ & & $\disc_m(f \circ D,g\circ D) \leq \epsilon$ and $\disc(f \circ D,g\circ D) \leq \epsilon$ (see Def.~\ref{def:closed})\\
$\Cov(\mathcal{U},\sim_{\mathcal{U}})$ & & The covering number of $\mathcal{U}$ with respect to relation $\sim_{\mathcal{U}}$ on $\mathcal{U}$ (see Def.\ref{def:covering})\\
$X :\leftarrow x$ & & $x$ is assigned to $X$ \\
\bottomrule
\end{tabular}
\end{center}
\label{tab:TableOfNotationForMyResearch}
\end{table}
\newpage

\section{Lemmas}

In this section, we prove various lemmas that are used in the proof of Thm.~\ref{thm:counting}. In Sec.~\ref{sec:assumptions} we present the assumptions taken in various lemmas in the appendix. In Sec.~\ref{sec:propdisc} we prove useful inequalities involving the discrepancy distance. Sec.~\ref{sec:propcompl} provides lemmas concerning the defined complexity measure and invariant functions. The lemmas in Sec.~\ref{sec:propinv} concern the properties of inverse functions.

 

\subsection{Assumptions}
\label{sec:assumptions}

We list the assumptions employed in our proofs. Assumptions~\ref{assumption:identifiability} and~\ref{assumption:denseOpen} were already presented and are heavily used.  Assumptions~\ref{assumption:contDisc} and its relaxation~\ref{assumption:contRisk} are mild assumptions that were taken for convenience.

 

\identif*

\denseOpen*

 

%\pmOne*

 

\contDisc*

 

In the case that the norm of the discriminator is bounded, Lem~\ref{lem:contDisc}, it follows from the following assumption, which is well-justified, (cf. \cite{Shalev-Shwartz:2014:UML:2621980}, page 162, Eq.14.13).

\begin{restatable}{assumption}{contRisk}\label{assumption:contRisk}  Let $\mathcal{N} = \textnormal{SCM}[\sigma]$ with $\sigma$ that is Leaky ReLU with parameter $0<a\neq 1$. For all $m>0$, the function $R_D\left[F[V_{m},...,V_1] ,F[W_{m},...,W_1]\right]$ is continuous as a function of $V_m,...,V_1,W_m,...,W_1$.
\end{restatable}

\subsection{Properties of Discrepancies}

\label{sec:propdisc}

\begin{lem}\label{lem:disc} Let $\mathcal{D}_1$ and $\mathcal{D}_2$ be two classes of functions and $D_1$, $D_2$ two distributions. Assume that $\mathcal{D}_1\circ \{ p \} \subset \mathcal{D}_2$, then,
\begin{equation}
\disc_{\mathcal{D}_1}(p \circ D_1, p\circ D_2) \leq\disc_{\mathcal{D}_2}(D_1, D_2)
\end{equation}
In particular, if $m \geq k+ C(p)$, then, 
\begin{equation}
\disc_{k}(p \circ D_1, p\circ D_2) \leq
\disc_{m}(D_1, D_2)
\end{equation}
\end{lem}

\begin{proof} By the definition of discrepancy:
\begin{equation}
\begin{aligned}
\disc_{\mathcal{D}_1}(p \circ D_1, p\circ D_2) 
&= \sup_{c_1,c_2 \in \mathcal{D}_1} 
\Big\vert R_{p \circ D_1}[c_1,c_2] - R_{p \circ D_2}[c_1,c_2]\Big\vert \\
&= \sup_{c_1,c_2 \in \mathcal{D}_1} 
\Big\vert R_{D_1}[c_1\circ p,c_2\circ p] - R_{D_2}[c_1\circ p,c_2\circ p]\Big\vert \\
\end{aligned}
\end{equation}

Since $\mathcal{D}_1 \circ \{ p \} \subset \mathcal{D}_2$ we have:

\begin{equation}
\begin{aligned}
\disc_{\mathcal{D}_1}(p \circ D_1, p\circ D_2) 
&= \sup_{c_1,c_2 \in \mathcal{D}_1} 
\Big\vert R_{D_1}[c_1\circ p,c_2\circ p] - R_{D_2}[c_1\circ p,c_2\circ p]\Big\vert \\
&\leq \sup_{u_1,u_2 \in \mathcal{D}_2} 
\Big\vert R_{D_1}[u_1,u_2] - R_{D_2}[u_1,u_2]\Big\vert = \disc_{\mathcal{D}_2}(D_1, D_2) 
\end{aligned}
\end{equation} 
The second inequality is a special case for $\mathcal{D}_1 = \mathcal{D}_k$ and $\mathcal{D}_2 = \mathcal{D}_m$. 
\end{proof}

\begin{lem}\label{lem:easy} Let $A = (\mathcal{X}_1,D_1)$ and $B = (\mathcal{X}_2,D_2)$
be two domains and $D_Z$ a distribution. 
\begin{enumerate}
\item Assume that $m \geq k + C(p)$. Then, 
\begin{equation}
\begin{aligned}
\disc_{k}(p \circ D_1, D_3) &\leq \disc_{m}(D_1, D_2) + 
\disc_{k}(p \circ D_2, D_3)
\end{aligned}
\end{equation}
\item Let $y_1$, $y_2$ and $y = y_2 \circ y^{-1}_1$ be three functions and $m \geq k + C(y_2)$. Then,
\begin{equation}
\disc_{k}(y\circ D_1, D_2) \leq \disc_{m}(D_Z,y^{-1}_1 \circ D_1) + \disc_{k}(y_2 \circ D_Z,D_2)
\end{equation}
\item Let $h$ be any function and $m \geq k+C(h^{-1})$. Then,
\begin{equation}
\disc_{k}(D_1, h^{-1} \circ D_2) \leq \disc_{m}(h \circ D_1, D_2)
\end{equation}
\end{enumerate}   
\end{lem}

\begin{proof}
1. Follows from Lem.~\ref{lem:disc}, since $m \geq k+C(p)$, we have:
\begin{equation}
\disc_{k}(p\circ D_1, p \circ D_2) \leq \disc_{m}(D_1, D_2)
\end{equation}
Therefore, by the triangle inequality, 
\begin{equation}
\begin{aligned}
\disc_{k}(p \circ D_1, D_3) &\leq \disc_{k}(p\circ D_1, p \circ D_2) +  
\disc_{k}(p \circ D_2, D_3) \\
&\leq \disc_{m}(D_1, D_2) + 
\disc_{k}(p \circ D_2, D_3)
\end{aligned}
\end{equation}
2. We use Lem.~\ref{lem:disc} with $p :\leftarrow y_2$, $\mathcal{D}_1 :\leftarrow \mathcal{D}_k$,  and $\mathcal{D}_2 :\leftarrow \mathcal{D}_m$ and $\mathcal{D}_k\circ \{y_2\} \subset \mathcal{D}_2$:

\begin{equation}
\disc_{k}(y_2\circ D_Z, y \circ D_1) = \disc_{k}(y_2\circ D_Z, y_2 \circ y^{-1}_1 \circ D_1) \leq \disc_{m}(D_Z, y^{-1}_1 \circ D_1)
\end{equation}
Therefore, by the triangle inequality, 
\begin{equation}
\begin{aligned}
\disc_{k}(y \circ D_1, D_2) &\leq 
\disc_{k}(y_2 \circ D_Z, D_2) + 
\disc_{k}(y_2\circ D_Z, y \circ D_1) \\
&\leq \disc_{k}(y_2 \circ D_Z, D_2) + 
\disc_{m}(D_Z, y^{-1}_1 \circ D_1)
\end{aligned}
\end{equation}
3. Follows immediately from Lem.~\ref{lem:disc} for $p:\leftarrow h^{-1}$ and $\mathcal{D}_k \circ \{h^{-1}\}\subset \mathcal{D}_m$.
\end{proof}

 



\subsection{Properties of the Complexity Measure and Invariants}

\label{sec:propcompl}

 

\begin{lem}\label{lem:A1} Let $\mathcal{N} =  \textnormal{SCM}[\mathcal{C}]$. In addition, let $u,v$ be any two functions. Then,
\begin{equation}
\max \{C(u) - C(v^{-1}), C(v) - C(u^{-1})\} \leq C(u\circ v) \leq C(u) + C(v)
\end{equation}
\end{lem}


\begin{proof} We begin with the case $C(v) = 0$. In this case, $C(u\circ v) = C(u) = C(u) + C(v)$. By definition, $C(v) = 0$ implies that $C(v^{-1}) = 0$ and $C(u\circ v) = C(u) - C(v^{-1})$. Finally, $C(u) - C(v^{-1}) = C(u) = C(u \circ v)$. The case $C(u) = 0$ is analogous. Next, we assume that $C(u) = n>0$ and $C(v)=m>0$. Let $u = u_n \circ ... \circ u_1$ and $v = v_m \circ ... \circ v_1$  be minimal decompositions of $u$ and $v$ (resp.). Therefore, we can represent, $u \circ v = u_n \circ ... \circ u_1 \circ v_m \circ ... \circ v_1$. In particular, $C(u \circ v) \leq n+m = C(u) + C(v)$. 

The lower bound follows immediately from the upper bound:
\begin{equation}
C(u) = C(u \circ v \circ v^{-1}) \leq C(u \circ v) + C(v^{-1})  \implies C(u) - C(v^{-1}) \leq C(u \circ v)
\end{equation}
By similar considerations, we also have: $C(v) - C(u^{-1}) \leq C(u \circ v)$.
\end{proof}

For a given function $u\in \mathcal{N} =  \textnormal{SCM}[\mathcal{C}]$, we define, 
\begin{equation}
C'(u) = \arg_n \{u \in \mathcal{C}^n\}
\end{equation}

 

\begin{lem}\label{lem:A2} Let $\mathcal{N} =  \textnormal{SCM}[\mathcal{C}]$. In addition, let $u,v$ be any two functions. Then,
\begin{equation}
\max \{C'(u) - C'(v^{-1}), C'(v) - C'(u^{-1})\} \leq C'(u\circ v) \leq C'(u) + C'(v)
\end{equation}
\end{lem}

\begin{proof} We begin by proving the upper bound. We assume $C'(u) = n$ and $C'(v)=m$. Let $u = u_n \circ ... \circ u_1$ and $v = v_m \circ ... \circ v_1$  be minimal decompositions of $u$ and $v$ (resp.). Therefore, we can represent, $u \circ v = u_n \circ ... \circ u_1 \circ v_m \circ ... \circ v_1$. In particular, $C'(u \circ v) \leq n+m = C'(u) + C'(v)$. The lower bound follows immediately from the upper bound:

\begin{equation}
C'(u) = C'(u \circ v \circ v^{-1}) \leq C'(u \circ v) + C'(v^{-1})  \implies C'(u) - C'(v^{-1}) \leq C'(u \circ v)
\end{equation}
By similar considerations, $C'(v) - C'(u^{-1}) \leq C'(u \circ v)$.
\end{proof}

\begin{lem}\label{lem:closed} 
$\Inv(\mathcal{N})$ is closed under inverse and composition, i.e, 
\begin{equation}
\tau \in \Inv(\mathcal{N}) \iff \tau^{-1} \in \Inv(\mathcal{N})
\end{equation}
And, 
\begin{equation}
\tau_1,\tau_2 \in \Inv(\mathcal{N}) \implies \tau_1\cdot \tau_2 \in \Inv(\mathcal{N})
\end{equation}
\end{lem}


\begin{proof} 
\textbf{Inverse:} Let $\tau \in \Inv(\mathcal{N})$. Then, by definition, $\tau$ is an invertible linear mapping and $\tau \circ \sigma = \sigma \circ \tau$. In particular, $\tau^{-1}$ is also an invertible linear mapping and $\tau^{-1} \circ \sigma = \sigma \circ \tau^{-1}$. Thus, $\tau^{-1}\in \Inv(\mathcal{N})$. 

\textbf{Composition:} Let $\tau_1,\tau_2 \in \Inv(\mathcal{N})$. Then, $\tau_i$ is an invertible linear mapping and $\tau_i \circ \sigma = \sigma \circ \tau_i$ for $i=1,2$. In particular, $\tau_1 \circ \tau_2$ is also an invertible linear mapping and $\tau_1 \circ \tau_2 \circ \sigma = \tau_1  \circ \sigma \circ \tau_2 =   \sigma \circ \tau_1 \circ \tau_2$. Thus, $\tau_1 \circ \tau_2 \in \Inv(\mathcal{N})$.
\end{proof}

 

\begin{lem}\label{lem:subseq} Let $\mathcal{N} = \textnormal{SCM}[\sigma]$ with $\sigma$ that is Leaky ReLU with $0<a \neq 1$. Assume that $p$ obeys identifiability, i.e., that   Assumption~\ref{assumption:identifiability} holds. Then, for any two minimal decompositions $p = F[W_{n+1},...,W_1] = F[V_{n+1},...,V_1]$, we have:

\begin{equation}
\begin{aligned}
\forall i \in [n+1]: &F[W_{i},...,W_1] \circ F[V_{i},...,V_1]^{-1} \in \Inv(\mathcal{N}) \\
\textnormal{ and } 
&F[W_{n+1},...,W_i] \circ F[V_{n+1},...,V_i]^{-1} \in \Inv(\mathcal{N})
\end{aligned}
\end{equation}
\end{lem}

 

\begin{proof}  We prove that $F[W_{i},...,W_1] \circ F[V_{i},...,V_1]^{-1} \in \Inv(\mathcal{N})$. If $i=n+1$, then, $F[W_{i},...,W_1] \circ F[V_{i},...,V_1]^{-1} = \Id \in \Inv(\mathcal{N})$. 
Otherwise, by minimal identifiability,
\begin{equation}
\begin{aligned}
&V_1 = \tau_1 \circ W_1, \; \forall i=2,...,n: V_i = \tau_i \circ W_i \circ \tau^{-1}_{i-1} \textnormal{ and } V_{n+1} =  W_{n+1} \circ  \tau^{-1}_{n} \\
\end{aligned}
\end{equation}
In addition, 
\begin{equation}
\begin{aligned}
F[W_{i},...,W_1] &= W_{i} \circ \sigma \circ W_{i-1} \circ ... \circ \sigma \circ W_1\\
F[V_{i},...,V_1] &= (\tau_{i} \circ W_{i} \circ \tau^{-1}_{i-1})\circ \sigma \circ (\tau_{i-1} \circ W_{i-1} \circ \tau^{-1}_{i-2}) \circ ... \circ\sigma \circ (\tau_1 \circ W_1) 
\end{aligned}
\end{equation}
Since each for all $k \in [i]$, $\tau_{k}$ commutes with $\sigma$, we have, 
\begin{equation}
F[V_{i},...,V_1] = \tau_{i} \circ F[W_{i},...,W_1]
\end{equation}
and 
\begin{equation}
F[W_{i},...,W_1] \circ F[V_{i},...,V_1]^{-1} = \tau^{-1}_i \in \Inv(\mathcal{N})
\end{equation} 
By similar considerations, $F[W_{n+1},...,W_i] \circ F[V_{n+1},...,V_i]^{-1} \in \Inv(\mathcal{N})$.
\end{proof}

 
\begin{lem}\label{lem:linearC0}  Let $\mathcal{N} = \textnormal{SCM}[\sigma]$ with $\sigma$ that is Leaky ReLU with parameter $0<a\neq 1$. Then, every invertible linear mapping $W$ is a member of $\mathcal{C}_0$.
\end{lem}

 
\begin{proof} Let $p\in \mathcal{C}^n$. Then, $p = F[W_{n+1},...,W_1]$, for invertible linear mappings $W_{1},...,W_{n+1}$. In particular, $W \circ p = F[W\cdot W_{n+1},...,W_1] \in \mathcal{C}^n$, $p \circ W = F[W_{n+1},...,W_1\cdot W] \in \mathcal{C}^n$ and similarly, $W^{-1} \circ p, p \circ W^{-1} \in \mathcal{C}^n$. Therefore, $W \in \mathcal{C}_0$.
\end{proof}


\begin{lem}\label{lem:closureC0}  $\mathcal{C}_0$ is closed under inverse and composition, i.e, 
\begin{equation}
u \in \mathcal{C}_0 \iff u^{-1} \in \mathcal{C}_0
\end{equation}
and,
\begin{equation}
u_1,u_2 \in \mathcal{C}_0 \implies u_1 \circ u_2 \in \mathcal{C}_0
\end{equation}
\end{lem}

\begin{proof} 
\textbf{Inverse:} By definition, $u \in \mathcal{C}_0$ iff for all $n \in \mathbb{N}$ and $q \in \mathcal{C}^n$, we have: $u \circ q,q\circ u,u^{-1}\circ q,q \circ u^{-1} \in \mathcal{C}^n$ iff $u^{-1} \in \mathcal{C}_0$.

\textbf{Decomposition:} Let $f \in \mathcal{C}^n$. Then, $g = u_1 \circ f \in \mathcal{C}^n$ and $u_2 \circ u_1 \circ f = u_2 \circ g \in \mathcal{C}^n$. Similarly, $u^{-1}_1\circ u^{-1}_2 \circ f, f \circ u^{-1}_1\circ u^{-1}_2 , f \circ u_2 \circ u_1 \in \mathcal{C}^n$.
\end{proof}


\subsection{Properties of Inverses}
\label{sec:propinv}


\begin{lem}\label{lem:Fp} Let $\mathcal{N} = \textnormal{SCM}[\sigma]$ where $\sigma$ is the Leaky ReLU activation function, with parameter $0<a \neq 1$. Let $f = F[W_{n+1},...,W_1]$ be a minimal decomposition. Then, for all $i \in [n]$, we have:
\begin{equation}
\begin{aligned}
F[W^{-1}_{i+1}/a,...,W^{-1}_n/a,-W^{-1}_{n+1}/a] \circ f = -1/a \cdot \sigma \circ F[W_{i},...,W_1]
\end{aligned}
\end{equation}
\end{lem}

\begin{proof} We prove this statement by induction on $i$ from $i=n$ backwards to $i=1$. 

\textbf{Case $i=n$:} Then, $F[W^{-1}_{i+1}/a,...,-W^{-1}_{n+1}/a] = F[-W^{-1}_{n+1}/a] = -W^{-1}_{n+1}/a$. In addition,
\begin{equation}
\begin{aligned}
F[-W^{-1}_{n+1}/a] \circ f &= -1/a \cdot \sigma \circ W_n \circ \sigma \circ W_{n-1} \circ \sigma \circ ... \circ \sigma \circ W_1 \\
&= -1/a \cdot \sigma \circ F[W_n,...,W_1]\\
&= -1/a \cdot F[\Id,W_{n},...,W_1]
\end{aligned}
\end{equation}

\textbf{Induction hypothesis:} We assume that:
\begin{equation}
\begin{aligned}
F[W^{-1}_{i+1}/a,...,W^{-1}_n/a,-W^{-1}_{n+1}/a] \circ f = -1/a \cdot F[\Id,W_{i},...,W_1]
\end{aligned}
\end{equation}

\textbf{Case $i-1$:} We consider that by the induction hypothesis:
\begin{equation}
\begin{aligned}
F[W^{-1}_{i}/a,...,W^{-1}_n/a,-W^{-1}_{n+1}/a] \circ f 
&= W^{-1}_{i}/a \circ F[\Id,W^{-1}_{i+1}/a,...,W^{-1}_n/a,-W^{-1}_{n+1}/a] \circ f \\
&= W^{-1}_{i}/a \circ \sigma \circ F[W^{-1}_{i+1}/a,...,W^{-1}_n/a,-W^{-1}_{n+1}/a] \circ f \\
&= W^{-1}_{i}/a \circ \sigma \circ -1/a \circ \sigma \circ  F[W_{i},...,W_1] \\
&= -W^{-1}_{i}/a \circ F[W_{i},...,W_1] \\
&= -1/a \cdot F[\Id,W_{i-1},...,W_1] \\
\end{aligned}
\end{equation}
Finally, we conclude that:
\begin{equation}
\begin{aligned}
F[W^{-1}_{i+1}/a,...,W^{-1}_n/a,-W^{-1}_{n+1}/a] \circ f = -1/a \cdot \sigma \circ F[W_{i},...,W_1]
\end{aligned}
\end{equation}
\end{proof}

\begin{lem}\label{lem:inverseAB} Let $\mathcal{N} = \textnormal{SCM}[\sigma]$ with $\sigma$ that is Leaky ReLU with parameter $0<a\neq 1$. Then, for all $\epsilon_0>0$, we have $C^{\epsilon_0}_{B,A} = C^{\epsilon_0}_{A,B}$.
\end{lem}

\begin{proof} Let $k \geq \left\{ E^{\epsilon_0}_{A,B},E^{\epsilon_0}_{B,A} \right\}$ and $m \geq k+C^{\epsilon_0}_{B,A}$. We take $y \in H_{\epsilon_0}(A,B;m)$. Then, $C(y) = C^{\epsilon_0}_{A,B}$. In addition, $\disc_m(y \circ D_A,D_B) \leq \epsilon_0$. By the third part of Lem.~\ref{lem:easy}, for $h :\leftarrow y$, we have:
\begin{equation}
\disc_k(y^{-1} \circ D_B,D_A) \leq \disc_m(y \circ D_A,D_B) \leq \epsilon_0
\end{equation}
In particular, $C^{\epsilon_0}_{B,A}\leq C(y^{-1})$. In addition, by Lem.~\ref{lem:inverse}, $C(y^{-1}) = C(y)$. Therefore, $C^{\epsilon_0}_{B,A} \leq C^{\epsilon_0}_{A,B}$. By symmetric arguments (switching between $A$ and $B$) we also have the opposite side and thus, $C^{\epsilon_0}_{B,A} = C^{\epsilon_0}_{A,B}$.
\end{proof}
 

\section{Proof of Thm.~\ref{thm:counting} and Its Generalization Thm.~\ref{thm:countingLem}}
\label{app:thmproof}
\subsection{Covering Numbers}

\begin{defn}[Set embedding] Let $(\mathcal{U},\sim_\mathcal{U})$ and $(\mathcal{V},\sim_\mathcal{V})$ be two tuples of sets and symmetric and reflexive relations on them (resp.). A function $G:\mathcal{U} \rightarrow \mathcal{V}$ is an embedding of $(\mathcal{U},\sim_\mathcal{U})$ in $(\mathcal{V},\sim_{\mathcal{V}})$ and we denote $(\mathcal{U},\sim_\mathcal{U}) \preceq (\mathcal{V},\sim_{\mathcal{V}})$ if:
\begin{equation}
\forall u_1,u_2 \in \mathcal{U}: G(u_1) \sim_{\mathcal{V}} G(u_2) \implies u_1 \sim_{\mathcal{U}} u_2
\end{equation} 
\end{defn}

 

\begin{lem}\label{lem:covEmb} Let $(\mathcal{U},\sim_\mathcal{U})$ and $(\mathcal{V},\sim_\mathcal{V})$ be two tuples of sets and reflexive and symmetric relations on them (resp.). If $(\mathcal{U},\sim_\mathcal{U}) \preceq (\mathcal{V},\sim_\mathcal{V})$ then $\Cov(\mathcal{U},\sim_\mathcal{U}) \leq \Cov(\mathcal{V},\sim_\mathcal{V})$.
\end{lem}

 

\begin{proof} Assume that $(\mathcal{U},\sim_\mathcal{U}) \preceq (\mathcal{V},\sim_\mathcal{V})$. Then, by definition, there is an embedding function $G:\mathcal{U} \rightarrow \mathcal{V}$ such that:
\begin{equation}
\forall u_1,u_2 \in \mathcal{U}: G(u_1) \sim_{\mathcal{V}} G(u_2) \implies u_1 \sim_{\mathcal{U}} u_2
\end{equation}
Let $(\mathcal{V},\equiv_{\mathcal{V}})$ be a covering of $(\mathcal{V},\sim_{\mathcal{V}})$. We define a covering $(\mathcal{U},\equiv_{\mathcal{U}})$ of $(\mathcal{U},\sim_{\mathcal{U}})$ as follows:
\begin{equation}
u_1 \equiv_{\mathcal{U}} u_2 \iff G(u_1) \equiv_{\mathcal{V}} G(u_2)
\end{equation}
\textbf{Part 1:} We would like to prove that $(\mathcal{U},\equiv_{\mathcal{U}})$ is a covering of $(\mathcal{U},\sim_{\mathcal{U}})$. It is easy to see that $\equiv_{\mathcal{U}}$ is an equivalence relation since $\equiv_{\mathcal{V}}$ is an equivalence relation. Next, we would like to prove that $u_1\equiv_{\mathcal{U}} u_2 \implies u_1\sim_{\mathcal{U}} u_2$. By the definition of $\equiv_{\mathcal{U}}$:
\begin{equation}
u_1 \equiv_{\mathcal{U}} u_2 \implies G(u_1) \equiv_{\mathcal{V}} G(u_2) 
\end{equation}
In addition, since $(\mathcal{V},\equiv_{\mathcal{V}})$ is a covering of $(\mathcal{V},\sim_{\mathcal{V}})$:
\begin{equation}
G(u_1) \equiv_{\mathcal{V}} G(u_2) \implies G(u_1) \sim_{\mathcal{V}} G(u_2)
\end{equation}
Finally, since $G$ is an embedding:
\begin{equation}
G(u_1) \sim_{\mathcal{V}} G(u_2) \implies u_1 \sim_{\mathcal{U}} u_2
\end{equation}
We conclude:
\begin{equation}
u_1 \equiv_{\mathcal{U}} u_2 \implies  u_1 \sim_{\mathcal{U}} u_2
\end{equation}
Therefore, $(\mathcal{U},\equiv_{\mathcal{U}})$ is indeed a covering of $(\mathcal{U},\sim_{\mathcal{U}})$.
 
\textbf{Part 2:} We would like to prove that $|\mathcal{U}/\equiv_{\mathcal{U}}| \leq |\mathcal{V}/\equiv_{\mathcal{V}}|$.  Let $u_1, u_2 \in \mathcal{U}$ such that $u_1 \not\equiv_{\mathcal{U}} u_2$. Then, by definition of $\equiv_{\mathcal{U}}$ we have: $G(u_1) \not\equiv_{\mathcal{V}} G(u_2)$. Therefore, if we take $u_1,...,u_n \in \mathcal{U}$ representations of $n$ different equivalence classes in $(\mathcal{U},\equiv_{\mathcal{U}})$ then, $G(u_1),...,G(u_n) \in \mathcal{V}$ are $n$ representations of $n$ different equivalence classes in $(\mathcal{V},\equiv_{\mathcal{V}})$. In particular, $|\mathcal{U}/\equiv_{\mathcal{U}}| \leq |\mathcal{V}/\equiv_{\mathcal{V}}|$. Therefore, the covering number of $(\mathcal{U},\sim_{\mathcal{U}})$ is at most the covering number of $(\mathcal{V},\sim_{\mathcal{V}})$.  
\end{proof}


\begin{lem}\label{lem:squaredCov} Let $(\mathcal{U},\equiv_1)$ and $(\mathcal{U},\equiv_2)$ be two coverings of $(\mathcal{U},\sim_\mathcal{U})$. Then,  $(\mathcal{U}^2,\equiv_1 \times \equiv_2)$ is a covering of $(\mathcal{U}^2,\sim^2_{\mathcal{U}})$. Where $\mathcal{U}^2 = \mathcal{U} \times \mathcal{U}$ and the relation $\sim^2_{\mathcal{U}}$ is defined as follows:

\begin{equation}
(a,b)\sim^2_{\mathcal{U}} (c,d) \iff a \sim_{\mathcal{U}} c \textnormal{ and } 
b \sim_{\mathcal{U}} d
\end{equation}
and $\equiv_1 \times \equiv_2$ is defined as:
\begin{equation}
(a,b)\equiv_1 \times \equiv_2 (c,d) \iff a \equiv_1 c \textnormal{ and } 
b \equiv_2 d
\end{equation}
\end{lem}

 

\begin{proof} We have to prove that $\equiv_1 \times \equiv_2$ is an equivalence relation and that $(u_1,u_2) \equiv_1 \times \equiv_2 (v_1,v_2) \implies (u_1,u_2) \sim^2_{\mathcal{U}} (v_1,v_2)$.

\textbf{Reflexivity:} 
\begin{equation}
(u_1,u_2) \equiv_1 \times \equiv_2 (u_1,u_2) \iff u_1 \equiv_1 u_1 \textnormal{ and } u_2 \equiv_1 u_2
\end{equation}
The RHS is true since $\equiv_1$ and $\equiv_2$ are reflexive relations.

\textbf{Symmetry:} 
\begin{equation}
(u_1,u_2) \equiv_1 \times \equiv_2  (v_1,v_2) \iff u_1 \equiv_1 v_1 \textnormal{ and } u_2 \equiv_2 v_2
\end{equation}
Since $\equiv_1$ and $\equiv_2$ are symmetric, we have:
\begin{equation}
u_1 \equiv_1 v_1 \textnormal{ and } u_2 \equiv_2 v_2 \iff 
v_1 \equiv_1 u_1 \textnormal{ and } v_2 \equiv_2 u_2
\end{equation}
In addition, 
\begin{equation}
(v_1,v_2) \equiv_1 \times \equiv_2 (u_1,u_2) \iff v_1 \equiv_1 u_1 \textnormal{ and } v_2 \equiv_2 u_2
\end{equation}
Therefore,
\begin{equation}
(u_1,u_2) \equiv_1 \times \equiv_2 (v_1,v_2) \iff (v_1,v_2) \equiv_1 \times \equiv_2 (u_1,u_2)
\end{equation}


\textbf{Transitivity:} follows from similar arguments.

\textbf{Covering:} 
\begin{equation}
(u_1,u_2) \equiv_1 \times \equiv_2 (v_1,v_2) \iff u_1 \equiv_1 v_1 \textnormal{ and } u_2 \equiv_2 v_2
\end{equation}
Since $(\mathcal{U},\equiv_i)$ is a covering of $(\mathcal{U},\sim_\mathcal{U})$, for $i=1,2$, we have:
\begin{equation}
u_1 \equiv_1 v_1 \textnormal{ and } u_2 \equiv_2 v_2 \implies u_1 \sim_\mathcal{U} v_1 \textnormal{ and } u_2 \sim_\mathcal{U} v_2
\end{equation}
By the definition of $\sim^2_{\mathcal{U}}$ we have:
\begin{equation}
u_1 \sim_{\mathcal{U}} v_1 \textnormal{ and } u_2 \sim_{\mathcal{U}} v_2
\iff (u_1,u_2) \sim_{\mathcal{U}} (v_1,v_2)
\end{equation}
Therefore, 
\begin{equation}
(u_1,u_2) \equiv_1 \times \equiv_2 (v_1,v_2) \implies (u_1,u_2) \sim^2_{\mathcal{U}} (v_1,v_2)
\end{equation}
\end{proof}

 

\begin{lem}\label{lem:squaredCovering} Let $(\mathcal{U},\sim_\mathcal{U})$ be a tuple of a set and a reflexive and symmetric relation on it (resp.). Then,
\begin{equation}
\Cov(\mathcal{U}^2,\sim^2_{\mathcal{U}}) \leq \Cov(\mathcal{U},\sim_{\mathcal{U}})^2
\end{equation}
\end{lem}

 

\begin{proof} Let $\equiv_{\mathcal{U}}$ be an equivalence relation such that $(\mathcal{U},\equiv_\mathcal{U})$ is a covering of $(\mathcal{U},\sim_\mathcal{U})$. By Lem.~\ref{lem:squaredCov}, $(\mathcal{U}^2,\equiv^2_{\mathcal{U}})$ is a covering of $(\mathcal{U}^2,\sim^2_{\mathcal{U}})$. In addition, 
\begin{equation}
|\mathcal{U}^2/\equiv^2_{\mathcal{U}}| = |\mathcal{U}/\equiv_{\mathcal{U}}|^2
\end{equation}
Thus, for every covering $(\mathcal{U},\equiv_\mathcal{U})$ of $(\mathcal{U},\sim_\mathcal{U})$, there is a covering of $(\mathcal{U}^2,\sim^2_{\mathcal{U}})$ of size $|\mathcal{U}/\equiv_{\mathcal{U}}|^2$. In particular, $\Cov(\mathcal{U}^2,\sim^2_{\mathcal{U}}) \leq \Cov(\mathcal{U},\sim_{\mathcal{U}})^2$.
\end{proof}

 

\begin{lem}\label{lem:squaredCovering2} Let $(\mathcal{U},\sim_\mathcal{U})$ be a tuple of a set and a reflexive and symmetric relation on it (resp.). Then,
\begin{equation}
\Cov(\mathcal{U},\sim_{\mathcal{U}}) \leq \Cov(\mathcal{U}^2,\sim^2_{\mathcal{U}}) 
\end{equation}
\end{lem}


\begin{proof} We define an embedding from $(\mathcal{U},\sim_{\mathcal{U}})$ to $(\mathcal{U}^2,\sim^2_{\mathcal{U}})$ as follows $F(u) = (u,u)$. This is an embedding, because, $F(u) \sim^2_{\mathcal{U}}  F(v) \implies (u,u) \sim^2_{\mathcal{U}}  (v,v) \implies u \sim_{\mathcal{U}} v$.
\end{proof}


\begin{lem}\label{lem:covSubset} Let $(\mathcal{U},\sim_\mathcal{U})$ and $(\mathcal{V},\sim_\mathcal{V})$ be two tuples of sets and reflexive and symmetric relations on them (resp.). Assume that $\mathcal{U} \subset \mathcal{V}$ and $\sim_{\mathcal{U}} := (\sim_{\mathcal{V}}) \big\vert_{\mathcal{U}}$, i.e, 
\begin{equation}
\forall u,v \in \mathcal{U}: u \sim_{\mathcal{U}} v \iff u \sim_{\mathcal{V}} v
\end{equation}
Then, 
\begin{equation}
\Cov(\mathcal{U},\sim_{\mathcal{U}}) \leq \Cov(\mathcal{V},\sim_{\mathcal{V}})
\end{equation}
\end{lem}

 

\begin{proof} Let $(\mathcal{V},\equiv_\mathcal{V})$ be a covering of $(\mathcal{V},\sim_\mathcal{V})$. Then, it is easy to see that $(\mathcal{U},\equiv_\mathcal{U})$ is a covering of $(\mathcal{U},\sim_\mathcal{U})$, where $\equiv_{\mathcal{U}} := (\equiv_\mathcal{V})\big\vert_{\mathcal{U}}$. In addition, we have: $|\mathcal{U}/\equiv_{\mathcal{U}}| \leq |\mathcal{V}/\equiv_{\mathcal{V}}|$. Thus, for every covering of $(\mathcal{V},\sim_\mathcal{V})$, we can find a smaller covering for $(\mathcal{U},\sim_\mathcal{U})$. In particular, $\Cov(\mathcal{U},\sim_{\mathcal{U}}) \leq \Cov(\mathcal{V},\sim_{\mathcal{V}})$.
\end{proof}

 

\subsection{Perturbations and Discrepancy}

 

Thm.~\ref{thm:counting} employs assumption~\ref{assumption:contDisc}. In Lem.~\ref{lem:contDisc} we prove that this assumption holds for the case of a continuous risk (assumption~\ref{assumption:contRisk}) if the discriminators have bounded weights.


\contDisc*

\contRisk*

\begin{lem}\label{lem:contDisc} Let $\mathcal{N} = \textnormal{SCM}[\sigma]$ with $\sigma$ that is Leaky ReLU with parameter $0<a\neq 1$ and assume Assumption~\ref{assumption:contRisk} for $D:\leftarrow D_1$. Let $\disc_{m,E} := \disc_{\mathcal{C}_{m,E}}$ for 

\begin{equation}
\mathcal{C}_{m,E} = \left\{F[W_{m},...,W_1] \;\big\vert\; \forall i \in [m]: ||W_i|| \leq E\right\}
\end{equation}
Then, for all $m>0$, $n>0$ and $E>0$, the function $\disc_{m,E}(F[W_{n},...,W_1] \circ D_1,D_2)$ is continuous as a function of $W_n,...,W_1$. 
\end{lem}

 

\begin{proof} Let $W_n,...,W_1$ and $W^k_n,...,W^k_1$ be any invertible matrices in $\mathbb{R}^{M\times M}$ such that for all $i \in [n]$, $W^k_i \to W_i$. We denote $G_E = \left\{W\in\mathbb{R}^{M\times M}  \; \big\vert \; ||W|| \leq E\right\}$. By the triangle inequality,
\begin{equation}
\begin{aligned}
&\disc_{m,E}(D_1,D_2) \leq \disc_{m,E}(D_1,D_3) + \disc_{m,E}(D_3,D_2) \\
&\implies \disc_{m,E}(D_1,D_2) - \disc_{m,E}(D_3,D_2) \leq \disc_{m,E}(D_1,D_3) 
\end{aligned}
\end{equation}
Similarly, 
\begin{equation}
\begin{aligned}
&\disc_{m,E}(D_3,D_2) \leq \disc_{m,E}(D_1,D_3) + \disc_{m,E}(D_1,D_2) \\
&\implies \disc_{m,E}(D_3,D_2) - \disc_{m,E}(D_1,D_2) \leq \disc_{m,E}(D_1,D_3) 
\end{aligned}
\end{equation}
therefore, 
\begin{equation}
|\disc_{m,E}(D_3,D_2) - \disc_{m,E}(D_1,D_2)| \leq \disc_{m,E}(D_1,D_3) 
\end{equation}
In particular, 
\begin{small}
\begin{equation}
\begin{aligned}
&\Big\vert \disc_{m,E}(F[W_{n},...,W_1] \circ D_1,D_2) - \disc_{m,E}(F[W^k_{n},...,W^k_1] \circ D_1,D_2) \Big\vert \\
\leq & \disc_{m,E}(F[W_{n},...,W_1]\circ D_1, F[W^k_{n},...,W^k_1] \circ D_1) \\
\leq & \sup_{c_1,c_2 \in \mathcal{C}_{m,E}}\Big\vert R_{D_1}[c_1 \circ F[W_{n},...,W_1] ,c_2 \circ F[W_{n},...,W_1]]- R_{D_1}[c_1 \circ F[W^k_n,...,W^k_1] ,c_2 \circ F[W^k_n,...,W^k_1] ] \Big\vert \\
\leq & \sup_{V_1,..,V_m,U_1,...,U_m \in G_{E}}\Big\vert R_{D_1}[F[V_m,...,V_1] \circ F[W^k_{n},...,W^k_1] ,F[U_m,...,U_1] \circ F[W_{n},...,W_1] ] \\
&\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\; - R_{D_1}[F[V_m,...,V_1] \circ F[W^k_{n},...,W^k_1] , F[U_m,...,U_1] \circ F[W^k_{n},...,W^k_1] ] \Big\vert \\
\leq & \sup_{V_1,..,V_m,U_1,...,U_m \in G_{E}}\Big\vert R_{D_1}\left[ F[V_m,...,V_2,V_1\cdot W_{n},W_{n-1},...,W_1 ] ,F[U_m,...,U_2,U_1\cdot W_{n},W_{n-1},...,W_1] \right] \\
&\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\; - R_{D_1}\left [F[V_m,...,V_2,V_1\cdot W^k_{n},W^k_{n-1},...,W^k_1], F[U_m,...,U_2,U_1\cdot W^k_{n},W^k_{n-1},...,W^k_1 ] \right] \Big\vert \\
\end{aligned}
\end{equation}
\end{small}
Assume by contradiction that the last expression does not converge to $0$. Therefore, there is a sequence $(V^k_1,...,V^k_m,U^k_1,...,U^k_m)$ such that $V^k_1,..,V^k_m,U^k_1,...,U^k_m \in G_{E}$ and
\begin{small}
\begin{equation}
\begin{aligned}
Q_k = \Big\vert &R_{D_1}\left[ F[V^k_m,...,V^k_2,V^k_1\cdot W_{n},W_{n-1},...,W_1]
,F[U^k_m,...,U^k_2,U^k_1\cdot W_{n},W_{n-1},...,W_1 ] \right] \\
& - R_{D_1}\left[F[V^k_m,...,V^k_2,V^k_1\cdot W^k_{n},W^k_{n-1},...,W^k_1], F[U^k_m,...,U^k_2,U^k_1\cdot W^k_{n},W^k_{n-1},...,W^k_1] \right] \Big\vert \not\to 0
\end{aligned}
\end{equation}
\end{small}
In particular, there is some $\epsilon>0$ and an increasing sequence $\{k_j\}^{\infty}_{j=1} \subset \mathbb{N}$ such that $Q_{k_j} > \epsilon$ for all $j \in \mathbb{N}$. With no loss of generality, we can assume that $k_j = j$ (otherwise, we replace the original sequence with the new one). Since $(V^{k_j}_1,...,V^{k_j}_m,U^{k_j}_1,...,U^{k_j}_m) \in G^{2m}_E$ and $G^{2m}_E$ is compact in $\mathbb{R}^{M\times M \times 2m}$, by the Bolzano-Weierstrass theorem, it has a converging subsequence. With no loss of generality, we can assume that $(V^{k_j}_1,...,V^{k_j}_m,U^{k_j}_1,...,U^{k_j}_m)$ converges (otherwise, we replace it with a converging sub-sequence):
\begin{equation}
(V^{k_j}_1,...,V^{k_j}_m,U^{k_j}_1,...,U^{k_j}_m) \to (V_1,...,V_m,U_1,...,U_m) \in G^{2m}_E
\end{equation}
In particular,  
\begin{equation}
\begin{aligned}
&(V^{k_j}_m,...,V^{k_j}_2,V^{k_j}_1\cdot W^{k_j}_{n},W^{k_j}_{n-1},...,W^{k_j}_1) \to (V_m,...,V_2,V_1\cdot W_n,W_{n-1},...,W_1)\\
&(U^{k_j}_m,...,U^{k_j}_{2},U^{k_j}_1\cdot W^{k_j}_{n},W^{k_j}_{n-1},...,W^{k_j}_1) \to (U_m,...,U_2,U_1\cdot W_n,W_{n-1},...,W_1)
\end{aligned}
\end{equation}
By Assumption~\ref{assumption:contRisk}, the function $R_{D_1}\left [F[X_{m+n},...,X_1] ,F[Y_{m+n},...,Y_1] \right]$ is continuous. Therefore,
\begin{small}
\begin{equation}
\begin{aligned}
\Big\vert &R_{D_1}\left[ F[V^{k_j}_m,...,V^{k_j}_2,V^{k_j}_1\cdot W_{n},W_{n-1},...,W_1],F[U^{k_j}_m,...,U^{k_j}_2,U^{k_j}_1\cdot W_{n},W_{n-1},...,W_1] \right] \\
& - R_{D_1}\left[ F[V_m,...,V_2,V_1\cdot W_{n},W_{n-1},...,W_1], F[U_m,...,U_2,U_1\cdot W_{n},W_{n-1},...,W_1] \right] \Big\vert \to 0
\end{aligned}
\end{equation}
\end{small}
and,
\begin{small}
\begin{equation}
\begin{aligned}
\Big\vert &R_{D_1}\left[ F[V_m,...,V_2,V_1\cdot W_{n},W_{n-1},...,W_1] ,F[U_m,...,U_2,U_1\cdot W_{n},W_{n-1},...,W_1] \right] \\
& - R_{D_1} \left[ F[V^{k_j}_m,...,V^{k_j}_2,V^{k_j}_1\cdot W^{k_j}_{n},W^{k_j}_{n-1},...,W^{k_j}_1], F[U^{k_j}_m,...,U^{k_j}_2,U^{k_j}_1\cdot W^{k_j}_{n},W^{k_j}_{n-1},...,W^{k_j}_1] \right] \Big\vert \to 0
\end{aligned}
\end{equation}
\end{small}
Therefore, by the triangle inequality,
\begin{small}
\begin{equation}
\begin{aligned}
Q_{k_j} = \Big\vert &R_{D_1}\left [ F[V^{k_j}_m,...,V^{k_j}_2,V^{k_j}_1\cdot W_{n},W_{n-1},...,W_1] ,F[U^{k_j}_m,...,U^{k_j}_2,U^{k_j}_1\cdot W_{n},W_{n-1},...,W_1]\right] \\
& - R_{D_1}\left[F[V^{k_j}_m,...,V^{k_j}_2,V^{k_j}_1\cdot W^{k_j}_{n},W^{k_j}_{n-1},...,W^{k_j}_1], F[U^{k_j}_m,...U^{k_j}_{2},U^{k_j}_1\cdot W^{k_j}_{n},W^{k_j}_{n-1},...,W^{k_j}_1] \right] \Big\vert \to 0
\end{aligned}
\end{equation}
\end{small}
in contradiction. Thus, we conclude that:
\begin{equation}
\lim_{k\to \infty}\Big\vert \disc_{m,E}( F[W_{n},...,W_1] \circ D_1,D_2) - \disc_{m,E}( F[W^k_{n},...,W^k_1] \circ D_1,D_2) \Big\vert = 0
\end{equation}
\end{proof}

 

\begin{lem}\label{lem:approxFunc} Let $\mathcal{N} = \textnormal{SCM}[\sigma]$ with $\sigma$ that is Leaky ReLU with parameter $0<a\neq 1$. In addition, let $f = F[W_{n+1},...,W_1]$ and $g = F[V_{n+1},...,V_1]$ be two minimal decompositions. Assume Assumptions~\ref{assumption:denseOpen} and~\ref{assumption:contDisc}. Then, there are functions $\bar{f} = F[\bar{W}_{n+1},...,\bar{W}_1]$ and $\bar{g} = F[\bar{V}_{n+1},...,\bar{V}_1]$ such that: 
\begin{itemize}
\item $C(\bar{f} \circ g) = 2n$.
\item $\forall j \in [n+1]: \disc_m(F[\bar{W}_j,...,\bar{W}_1] \circ D,F[W_j,...,W_1] \circ D) \leq \epsilon$.
\item $\forall j \in [n+1]: \disc_m(F[\bar{V}_j,...,\bar{V}_1] \circ D,F[V_j,...,V_1] \circ D) \leq \epsilon$.
\end{itemize}

\end{lem}


\begin{proof} We consider that $f \circ g = F[W_{n+1},...,W_2,W_1 \cdot V_{n+1},V_n,...,V_1]$. By Assumption~\ref{assumption:denseOpen}, for each $\delta>0$, there are $\bar{f} = F[\bar{W}_{n+1},...,\bar{W}_1]$ and $\bar{g} = F[\bar{V}_{n+1},...,\bar{V}_1]$ such that $C(\bar{f} \circ \bar{g}) = 2n$ and for all $j \in [n+1]$: $||\bar{W}_j - W_j||, ||\bar{V}_j - V_j|| \leq \delta$. By Assumption.~\ref{assumption:contDisc}, for each $\epsilon>0$, there is a small enough $\delta>0$ such that: $\bar{f} = F[\bar{W}_{n+1},...,\bar{W}_1]$ and $\bar{g} = F[\bar{V}_{n+1},...,\bar{V}_1]$ such that for all $j \in [n+1]$: $||\bar{W}_j - W_j||, ||\bar{V}_j - V_j|| \leq \delta$, we have:
\begin{itemize}
\item $\forall j \in [n+1]: \disc_m(F[\bar{W}_j,...,\bar{W}_1] \circ D,F[W_j,...,W_1] \circ D) \leq \epsilon$.
\item $\forall j \in [n+1]: \disc_m(F[\bar{V}_j,...,\bar{V}_1] \circ D,F[V_j,...,V_1] \circ D) \leq \epsilon$.
\end{itemize}
In particular, for any $\epsilon>0$, there are functions $\bar{f} = F[\bar{W}_{n+1},...,\bar{W}_1]$ and $\bar{g} = F[\bar{V}_{n+1},...,\bar{V}_1]$  with the desired properties.
\end{proof}

\begin{lem}\label{lem:extCloseFunc} Let $\mathcal{N} = \textnormal{SCM}[\sigma]$ with $\sigma$ that is Leaky ReLU with parameter $0<a\neq 1$. Assume Assumption~\ref{assumption:identifiability}. Let $f \overset{D}{\underset{m,\epsilon}{\sim}} g$. Then, for every minimal decomposition $f = F[W'_{n+1},...,W'_1]$ there is a minimal decomposition $g = F[V'_{n+1},...,V'_1]$ such that:
\begin{equation}
\forall i \in [n+1]: F[W'_i,...,W'_1] \circ D \underset{m,\epsilon}{\sim} F[V'_i,...,V'_1] \circ D
\end{equation}
\end{lem}

\begin{proof} Since $f \overset{D}{\underset{m,\epsilon}{\sim}} g$ there are minimal decompositions $f = F[W_{n+1},...,W_1]$ and $g = F[V_{n+1},...,V_1]$ such that:
\begin{equation}
\forall i \in [n+1]: F[W_i,...,W_1] \circ D \underset{m,\epsilon}{\sim} F[V_i,...,V_1] \circ D
\end{equation}
By Assumption~\ref{assumption:identifiability}, $W'_1 = \tau_1 \circ W_1$, for all $i = 2,...,n$: $W'_i = \tau_i \circ W_i \circ \tau^{-1}_{i-1}$ and $W'_{n+1} = W_{n+1} \circ \tau^{-1}_{n}$. Therefore, we define a minimal decomposition for $g$ as follows: $g = F[V'_{n+1},...,V'_1]$ such that $V'_1 = \tau_1 \circ V_1$, for all $i = 2,...,n$: $V'_i = \tau_i \circ V_i \circ \tau^{-1}_{i-1}$ and $V'_{n+1} = V_{n+1} \circ \tau^{-1}_{n}$. This is a minimal decomposition of $g$, since each invariant function is an invertible linear mapping and commutes with $\sigma$. By Lem.~\ref{lem:subseq} we have:
\begin{equation}
\forall i \in [n]: F[W'_i,...,W'_1] = \tau_i \circ F[W_i,...,W_1] \textnormal{ and } F[V'_i,...,V'_1] = \tau_i \circ F[V_i,...,V_1] 
\end{equation}
Therefore, by Lem.~\ref{lem:disc}, since $C(\tau_i) = 0$, we have:
\begin{equation}
\begin{aligned}
\forall i\in [n]: & \disc_{m}(F[W'_i,...,W'_1] \circ D,F[V'_i,...,V'_1] \circ D) \\
&\leq \disc_{m}(\tau_i \circ F[W_i,...,W_1] \circ D, \tau_i \circ F[V_i,...,V_1] \circ D) \leq \epsilon
\end{aligned}
\end{equation}
Alternatively, 
\begin{equation}
\forall i \in [n]: F[W'_i,...,W'_1] \circ D \underset{m,\epsilon}{\sim} F[V'_i,...,V'_1] \circ D
\end{equation}
Since $F[W'_{n+1},...,W'_1] = f = F[W_{n+1},...,W_1]$ and $F[V'_{n+1},...,V'_1] = g = F[V_{n+1},...,V_1]$ we also have $F[W'_{n+1},...,W'_1] \circ D \underset{m,\epsilon}{\sim} F[V'_{n+1},...,V'_1] \circ D$.
\end{proof}

 

\begin{lem}\label{lem:reduceNotSem} Let $\mathcal{N} = \textnormal{SCM}[\sigma]$ with $\sigma$ that is Leaky ReLU with parameter $0<a\neq 1$. We have:
\begin{equation}
\textnormal {$f \overset{D_A}{\underset{k,\epsilon_1}{\not\sim}} f'$, $\bar{f} \overset{D_A}{\underset{k,\epsilon}{\sim}} f$ and $\bar{f}' \overset{D_A}{\underset{k,\epsilon}{\sim}} f'$} \implies \bar{f} \overset{D_A}{\underset{k,\epsilon_1-2\epsilon}{\not\sim}} \bar{f}'
\end{equation}
\end{lem}

 

\begin{proof} Assume by contradiction that $\bar{f} \overset{D_A}{\underset{k,\epsilon_1-2\epsilon}{\sim}} \bar{f}'$. Then, there are decompositions $\bar{f} = F[\bar{W}_{n+1},...,\bar{W}_1]$ and $\bar{f}' = F[\bar{W}'_{n+1},...,\bar{W}'_1]$ such that: 
\begin{equation}
\forall j\in [n+1]: \disc_k(F[\bar{W}_{j},...,\bar{W}_1] \circ D_A, F[\bar{W}'_{j},...,\bar{W}'_1] \circ D_A) \leq \epsilon_1 - 2\epsilon
\end{equation}
By Lem.~\ref{lem:extCloseFunc}, since $\bar{f} \overset{D_A}{\underset{k,\epsilon}{\sim}} f$ and $\bar{f}' \overset{D_A}{\underset{k,\epsilon}{\sim}} f'$, there are minimal decompositions $f = F[W_{n+1},...,W_1]$ and $f' = F[W'_{n+1},...,W'_1]$ such that:
\begin{equation}
\begin{aligned}
\forall j\in [n+1]: &\disc_k(F[W_{j},...,W_1] \circ D_A,F[\bar{W}_{j},...,\bar{W}_1] \circ D_A) \leq \epsilon \\
&\disc_k(F[W'_{j},...,W'_1] \circ D_A,F[\bar{W}'_{j},...,\bar{W}'_1] \circ D_A) \leq \epsilon 
\end{aligned}
\end{equation}
Since $f \overset{D_A}{\underset{k,\epsilon_1}{\not\sim}} f'$, there is an index $i \in [n+1]$ such that:
\begin{equation}
\disc_k(F[W_{i},...,W_1] \circ D_A, F[W'_i,...,W'_1] \circ D_A) > \epsilon_1
\end{equation}
Therefore, by the triangle inequality, we arrive to a contradiction:
\begin{equation}
\begin{aligned}
&\disc_k(F[W_{i},...,W_1] \circ D_A, F[W'_i,...,W'_1] \circ D_A) \\
\leq& \disc_k(F[\bar{W}_{i},...,\bar{W}_1] \circ D_A,F[W'_i,...,W'_1] \circ D_A) \\
&+ \disc_k(F[\bar{W}_{i},...,\bar{W}_1] \circ D_A,F[W_{i},...,W_1] \circ D_A) \\
\leq& \disc_k(F[\bar{W}_{i},...,\bar{W}_1] \circ D_A,F[\bar{W}'_i,...,\bar{W}'_1] \circ D_A) \\
&+ \disc_k(F[\bar{W}_{i},...,\bar{W}_1] \circ D_A,F[W_{i},...,W_1] \circ D_A) \\
&+ \disc_k(F[\bar{W}'_i,...,\bar{W}'_1] \circ D_A,F[W'_i,...,W'_1] \circ D_A) \\
\leq& (\epsilon_1-2\epsilon)+\epsilon+\epsilon = \epsilon_1
\end{aligned}
\end{equation} 
\end{proof}

 

\begin{lem}\label{lem:covinv} Let $\mathcal{N} = \textnormal{SCM}[\sigma]$ with $\sigma$ that is a Leaky ReLU with parameter $0< a \neq 1$. Let $A = (\mathcal{X}_A,D_A)$ and $B = (\mathcal{X}_B, D_B)$ are two domains. We have:
\begin{equation}
\Cov\left(H_{\epsilon_0}(A,B),\overset{D_A}{\underset{\epsilon_1+2\epsilon_0}{\sim}}\right) \leq \Cov\left(H_{\epsilon_0}(B,A),\overset{D_B}{\underset{\epsilon_1}{\sim}}\right)
\end{equation}
\end{lem}

 

\begin{proof} We would like to show that the function $G(h) = h^{-1}$ is an embedding of $\left(H_{\epsilon_0}(A,B),\overset{D_A}{\underset{\epsilon_1+2\epsilon_0}{\sim}}\right)$ into $\left(H_{\epsilon_0}(B,A),\overset{D_B}{\underset{\epsilon_1}{\sim}}\right)$. First, we consider that if $h \in H_{\epsilon_0}(A,B)$, then, 
\begin{equation}
\disc( G(h) \circ D_B,D_A) = \disc(h^{-1} \circ D_A,D_B) \leq \disc(h \circ D_A,D_B) \leq \epsilon_0
\end{equation}
and by Lem.~\ref{lem:inverse} and Lem.~\ref{lem:inverseAB}, $C(G(h)) = C(h^{-1}) = C(h) = C^{\epsilon_0}_{A,B} = C^{\epsilon_0}_{B,A}$. Therefore, $G(h) \in H_{\epsilon_0}(B,A)$. Next, we would like to prove that for all $h_1,h_2 \in H_{\epsilon_0}(A,B)$: $G(h_1) \overset{D_B}{\underset{\epsilon_1}{\sim}} G(h_2) \implies h_1 \overset{D_A}{\underset{\epsilon_1+2\epsilon_0}{\sim}} h_2$. 

Let $h_1, h_2 \in H_{\epsilon_0}(A,B)$ such that $G(h_1) \overset{D_B}{\underset{\epsilon_1}{\sim}} G(h_2)$. Then, there are minimal decompositions $G(h_1) = F[W_{n+1},...,W_1] $ and $G(h_2) = F[V_{n+1},...,V_1]$ such that:
\begin{equation}
\begin{aligned}
&\forall i \in [n]: \disc(F[W_i,...,W_1] \circ D_A,F[V_i,...,V_1] \circ D_A) \leq \epsilon_1 \\
&\textnormal{ and: } \disc(G(h_1) \circ D_A,G(h_2) \circ D_A) \leq \epsilon_1
\end{aligned}
\end{equation}
We consider that by Lem.~\ref{lem:inverse}, $G(h_1) = F[-W^{-1}_1,W^{-1}_2/a,...,W^{-1}_n/a,-W^{-1}_{n+1}/a]$ and $G(h_2) = F[-V^{-1}_1,V^{-1}_2/a,...,V^{-1}_n/a,-V^{-1}_{n+1}/a]$ are minimal decompositions. In addition, by Lem.~\ref{lem:Fp}, we have:
\begin{equation}
\begin{aligned}
\forall i \in [n]: &F[W^{-1}_{i+1}/a,...W^{-1}_n/a,-W^{-1}_{n+1}/a] \circ G(h_1) = -1/a \cdot \sigma \circ F[W_i,...,W_1] \\
&F[V^{-1}_{i+1}/a,...V^{-1}_n/a,-V^{-1}_{n+1}/a] \circ G(h_2) = -1/a \cdot \sigma \circ F[V_i,...,V_1] \\
\end{aligned}
\end{equation}
By the first item of Lem.~\ref{lem:easy}, for $D_1 := D_B$, $D_2 := h_1 \circ D_A$, $D_3 := F[V^{-1}_{i+1}/a,...V^{-1}_n/a,-V^{-1}_{n+1}/a] \circ D_B$ and $p := F[W^{-1}_{i+1}/a,...W^{-1}_n/a,-W^{-1}_{n+1}/a]$, 
\begin{small}
\begin{equation}
\begin{aligned}
&\disc(F[W^{-1}_{i+1}/a,...W^{-1}_n/a,-W^{-1}_{n+1}/a] \circ D_B, F[V^{-1}_{i+1}/a,...V^{-1}_n/a,-V^{-1}_{n+1}/a] \circ D_B) \\
\leq& \disc(F[W^{-1}_{i+1}/a,...W^{-1}_n/a,-W^{-1}_{n+1}/a] \circ h_1 \circ D_A, F[V^{-1}_{i+1}/a,...V^{-1}_n/a,-V^{-1}_{n+1}/a] \circ D_B) \\
&+\disc(h_1 \circ D_A, D_B) \\
\leq& \disc(F[W^{-1}_{i+1}/a,...W^{-1}_n/a,-W^{-1}_{n+1}/a] \circ h_1 \circ D_A, F[V^{-1}_{i+1}/a,...V^{-1}_n/a,-V^{-1}_{n+1}/a] \circ D_B) +\epsilon_0
\end{aligned}
\end{equation}
\end{small}
Similarly (by the first item of Lem.~\ref{lem:easy}), we have:
\begin{small}
\begin{equation}
\begin{aligned}
&\disc(F[W^{-1}_{i+1}/a,...W^{-1}_n/a,-W^{-1}_{n+1}/a] \circ D_B, F[V^{-1}_{i+1}/a,...V^{-1}_n/a,-V^{-1}_{n+1}/a] \circ D_B) \\
\leq& \disc(F[W^{-1}_{i+1}/a,...W^{-1}_n/a,-W^{-1}_{n+1}/a] \circ h_1 \circ D_A, F[V^{-1}_{i+1}/a,...V^{-1}_n/a,-V^{-1}_{n+1}/a] \circ h_2 \circ D_A)+2\epsilon_0 \\
=& \disc(-1/a \cdot \sigma \circ F[W_i,...,W_1] \circ D_A, -1/a \cdot \sigma \circ F[V_i,...,V_1] \circ D_A)+2\epsilon_0 \\
\leq& \disc(F[W_i,...,W_1] \circ D_A, F[V_i,...,V_1] \circ D_A)+2\epsilon_0 \leq \epsilon_1+2\epsilon_0 \\
\end{aligned}
\end{equation}
\end{small}
Therefore, we conclude that $h_1 \overset{D_A}{\underset{\epsilon_1+2\epsilon_0}{\sim}} h_2$.
\end{proof}

\subsection{Proof of Thm.~\ref{thm:counting}}
\label{sec:proofCounting}

\begin{thrm}\label{thm:countingLem} Let $\mathcal{N} = \textnormal{SCM}[\sigma]$ with $\sigma$ that is a Leaky ReLU with parameter $0< a \neq 1$. Assume Assumptions~\ref{assumption:identifiability},~\ref{assumption:denseOpen} and~\ref{assumption:contDisc}. Let $\epsilon_0$, $\epsilon_1$ and $\epsilon_2$ such that $\epsilon_0 < \epsilon_1/2$ and $\epsilon_2 < \epsilon_1-2\epsilon_0$ be three positive constants and $A = (\mathcal{X}_A,D_A)$ and $B = (\mathcal{X}_B, D_B)$ are two domains. Assume that $m \geq k+2C^{\epsilon_0}_{A,B}+2$. Then, 
\begin{equation}
\begin{aligned}
\Cov\left(H_{\epsilon_0}(A,B;m),\overset{D_A}{\underset{k,\epsilon_1}{\sim}}\right) 
&\leq \lim_{\epsilon \to 0} \Cov\left (\textnormal{DPM}_{2\epsilon_0+\epsilon}\left(B; k, 2C^{\epsilon_0}_{A,B} \right), \overset{D_B}{\underset{m,\epsilon_2}{\sim}} \right) \\
\end{aligned}
\end{equation}
\end{thrm}

\begin{proof} Let $\epsilon$ be any positive constant such that: $\epsilon < \min\{(\epsilon_1 - 2\epsilon_0 - \epsilon_2)/4,\epsilon_2/2 \}$. For such $\epsilon$, we have $2\epsilon_0 \leq \epsilon_1 - 4\epsilon$ and $\epsilon_2 \leq \epsilon_1 - 2 \epsilon_0 - 4\epsilon$. In addition, let $t := k+C^{\epsilon_0}_{A,B}+1$. We would like to find an embedding mapping:
\begin{equation}
G : (H_{\epsilon_0}(A,B;m))^2 \rightarrow \textnormal{DPM}_{2\epsilon_0+\epsilon}\left(B;k,2C^{\epsilon_0}_{A,B}+2 \right)
\end{equation}
\textbf{Part 1:} In this part, we show how to construct $G$. Let $(f,g) \in (H_{\epsilon_0}(A,B;m))^2$. We denote: $f = F[W_{n+1},...,W_1]$ and $g = F[V_{n+1},...,V_1]$ minimal decompositions of $f$ and $g$ (resp.). By Lem.~\ref{lem:approxFunc}, there are functions $\bar{f} = F[\bar{W}_{n+1},...,\bar{W}_1]$ and $\bar{g} = F[\bar{V}_{n+1},...,\bar{V}_1]$ such that:
\begin{itemize}
\item $C(\bar{f} \circ \bar{g}^{-1}) = 2n$.
\item $\forall j \in [n+1]: \disc_m(F[\bar{W}_{j},...,\bar{W}_1] \circ D_A,F[W_{j},...,W_1] \circ D_A) \leq \epsilon$.
\item $\forall j \in [n+1]: \disc_m(F[\bar{V}_{j},...,\bar{V}_1] \circ D_A,F[V_{j},...,V_1] \circ D_A) \leq \epsilon$.
\end{itemize}
We define $G(f,g) = \bar{f} \circ \bar{g}^{-1}$.

\textbf{Part 2:} In this part, we show that:
\begin{equation}
(f,g) \in (H_{\epsilon_0}(A,B;m))^2 \implies G(f,g) \in \textnormal{DPM}_{2\epsilon_0+2\epsilon}\left(D_B;k,2C^{\epsilon_0}_{A,B} \right)
\end{equation}
By Part $1$, $C(\bar{f} \circ \bar{g}^{-1}) = 2n = 2C^{\epsilon_0}_{A,B}$. In addition, by the first item of Lem.~\ref{lem:easy}, for $D_1 :\leftarrow \bar{g}^{-1} \circ D_B$, $D_2 :\leftarrow D_A$, $D_3 :\leftarrow D_B$, $p:\leftarrow \bar{f}$, $t \geq k + C^{\epsilon_0}_{A,B}$ we have: 
\begin{equation}
\disc_k(\bar{f}\circ \bar{g}^{-1} \circ D_B,D_B) \leq \disc_{t}(\bar{f} \circ D_A,D_B) + \disc_{t}(\bar{g}^{-1} \circ D_B, D_A) 
\end{equation}
Since $f \in H_{\epsilon_0}(A,B;m)$:
\begin{equation}
\begin{aligned}
\disc_{t}(\bar{f} \circ D_A,D_B) \leq \disc_m(f \circ D_A,D_B) + \disc_m(\bar{f} \circ D_A,f \circ D_A) \leq \epsilon_0+\epsilon
\end{aligned}
\end{equation}
In addition, by the third item of Lem.~\ref{lem:easy}, for $h :\leftarrow \bar{g}$ and $m \geq t+C^{\epsilon_0}_{A,B} \geq t + C(\bar{g}^{-1})$, we have:
\begin{equation}
\begin{aligned}
\disc_{t}(\bar{g}^{-1} \circ D_B, D_A) &\leq \disc_{m}(\bar{g} \circ D_A, D_B) \\
&\leq \disc_{m}(g \circ D_A, D_B) + \disc_m(g \circ D_A, \bar{g} \circ D_A) \leq \epsilon_0+\epsilon
\end{aligned}
\end{equation}
Finally, $\disc_k(\bar{f}\circ \bar{g}^{-1} \circ D_B,D_B) \leq 2\epsilon_0+2\epsilon$ and we conclude that:
\begin{equation}
G(f,g) \in \textnormal{DPM}_{2\epsilon_0+2\epsilon}\left(B;k,2C^{\epsilon_0}_{A,B} \right)
\end{equation}

\textbf{Part 3:} In this part, we show that $G$ is an embedding. It requires showing that 
\begin{equation}
G(f,g) \overset{D_B}{\underset{m,\epsilon_2}{\sim}}  G(f',g') \implies (f,g) \left( \overset{D_A}{\underset{k,\epsilon_1}{\sim}} \right)^2  (f',g')
\end{equation}
Assume by contradiction that $G(f,g) \overset{D_B}{\underset{m,\epsilon_2}{\sim}}  G(f',g')$ and that $(f,g) \overset{D_A}{\underset{k,\epsilon_1}{\not\sim}}   (f',g')$. Then, we have 
\begin{equation}
f \overset{D_A}{\underset{k,\epsilon_1}{\not\sim}} f' \textnormal{ or } g \overset{D_A}{\underset{k,\epsilon_1}{\not\sim}} g'
\end{equation}
We denote $G(f,g) = \bar{f} \circ \bar{g}^{-1}$ and $G(f',g') = \bar{f}' \circ (\bar{g}')^{-1}$ (see Part $1$).

\textbf{Assume that $f \overset{D_A}{\underset{k,\epsilon_1}{\not\sim}} f'$:} By Lem.~\ref{lem:reduceNotSem}, $\bar{f} \overset{D_A}{\underset{k,\epsilon_1-2\epsilon}{\not\sim}} \bar{f}'$. In particular, for every two decompositions:
\begin{equation}
\bar{f} = F[\bar{W}_{n+1},...,\bar{W}_1] \textnormal{ and } 
\bar{f}' = F[\bar{W}'_{n+1},...,\bar{W}'_1]
\end{equation}
there is an index $i \in [n+1]$ such that:
\begin{equation}
\begin{aligned}
&\disc_k(F[\bar{W}_{i},...,\bar{W}_1] \circ D_A,F[\bar{W}'_{i},...,\bar{W}'_1] \circ D_A) > \epsilon_1-2\epsilon \\
\end{aligned}
\end{equation}
The option $i=n+1$ is not a possibility, since:
\begin{equation}\label{eq:notApossibilityn1}
\begin{aligned}
\disc_k(\bar{f} \circ D_A, \bar{f}' \circ D_A)\leq & \disc_k(f \circ D_A, D_B) + \disc_k(\bar{f}\circ D_A,f \circ D_A) \\
&+ \disc_k(D_B,f' \circ D_A) + \disc_k(\bar{f}'\circ D_A,f' \circ D_A)  \\
\leq& 2\epsilon_0+2\epsilon \leq \epsilon_1 - 2\epsilon
\end{aligned}
\end{equation}
By the first item of Lem.~\ref{lem:easy}, for $D_1 :\leftarrow D_A$, $D_2:\leftarrow \bar{g}^{-1} \circ D_B$, $D_3 :\leftarrow F[\bar{W}'_{i},...,\bar{W}'_1]\circ D_A$, $p:\leftarrow F[\bar{W}_{i},...,\bar{W}_1]$ and $t \geq k + C^{\epsilon_0}_{A,B} \geq k + C(F[\bar{W}_{i},...,\bar{W}_1])$, we have:
\begin{small}
\begin{equation}
\begin{aligned}
&\disc_k(F[\bar{W}_{i},...,\bar{W}_1]\circ D_A,F[\bar{W}'_{i},...,\bar{W}'_1] \circ D_A) \\
&\leq \disc_t(F[\bar{W}_{i},...,\bar{W}_1] \circ \bar{g}^{-1} \circ D_B,F[\bar{W}'_{i},...,\bar{W}'_1] \circ D_A) + \disc_t( \bar{g}^{-1} \circ D_B,D_A) \\
&\leq \disc_t(F[\bar{W}_{i},...,\bar{W}_1] \circ \bar{g}^{-1} \circ D_B,F[\bar{W}'_{i},...,\bar{W}'_1] \circ D_A) +\epsilon_0 \\
\end{aligned}
\end{equation}
\end{small}
Again, by the first item of Lem.~\ref{lem:easy}, for $D_1 :\leftarrow D_A$, $D_2:\leftarrow (g')^{-1} \circ D_B$, $D_3 :\leftarrow F[\bar{W}_{i},...,\bar{W}_1] \circ g^{-1} \circ D_B$, $p:\leftarrow F[\bar{W}'_{i},...,\bar{W}'_1]$ and $m \geq t + C^{\epsilon_0}_{A,B} \geq t + C(F[\bar{W}'_{i},...,\bar{W}'_1])$, we have:
\begin{small}
\begin{equation}
\begin{aligned}
&\disc_t(F[\bar{W}_{i},...,\bar{W}_1] \circ \bar{g}^{-1} \circ D_B,F[\bar{W}'_{i},...,\bar{W}'_1] \circ D_A) \\
\leq& \disc_m(F[\bar{W}_{i},...,\bar{W}_1] \circ \bar{g}^{-1} \circ D_B,F[\bar{W}'_{i},...,\bar{W}'_1] \circ (\bar{g}')^{-1} \circ D_B) + \disc_m((\bar{g}')^{-1} \circ D_B,D_A) \\
\leq& \disc_m(F[\bar{W}_{i},...,\bar{W}_1] \circ g^{-1} \circ D_B,F[\bar{W}'_{i},...,\bar{W}'_1] \circ (g')^{-1} \circ D_B) +\epsilon_0\\
\end{aligned}
\end{equation}
\end{small}
Therefore, we conclude that:
\begin{equation}
\begin{aligned}
\epsilon_1 - 2\epsilon_0 - 2\epsilon < \disc_m(F[\bar{W}_{i},...,\bar{W}_1] \circ \bar{g}^{-1} \circ D_B,F[\bar{W}'_{i},...,\bar{W}'_1] \circ (\bar{g}')^{-1} \circ D_B)
\end{aligned}
\end{equation}
Alternatively, for any minimal decompositions $\bar{f} \circ \bar{g}^{-1} = F[\bar{W}_{n+1},...,\bar{W}_1] \circ \bar{g}^{-1}$ and $\bar{f}' \circ (\bar{g}')^{-1} = F[\bar{W}'_{n+1},...,\bar{W}'_1] \circ (\bar{g}')^{-1}$ there are right partial functions $F[\bar{W}_{i},...,\bar{W}_1] \circ \bar{g}^{-1}$ and $F[\bar{W}'_{i},...,\bar{W}'_1] \circ (\bar{g}')^{-1}$ such that:
\begin{equation}
\begin{aligned}
\epsilon_1 - 2\epsilon_0 - 2\epsilon < \disc_m(F[\bar{W}_{n+1},...,\bar{W}_1] \circ \bar{g}^{-1} \circ D_B,F[\bar{W}'_{i},...,\bar{W}'_1] \circ (\bar{g}')^{-1} \circ D_B)
\end{aligned}
\end{equation}
in contradiction to $F(f,g) \overset{D_B}{\underset{m,\epsilon_2}{\sim}}  F(f',g')$. 

\textbf{Assume that $g \overset{D_A}{\underset{k,\epsilon_1}{\not\sim}} g'$:} By Lem.~\ref{lem:reduceNotSem}, $\bar{g} \overset{D_A}{\underset{k,\epsilon_1-2\epsilon}{\not\sim}} \bar{g}'$. Let 
\begin{equation}
\begin{aligned}
\bar{g}^{-1} &= F[-\bar{V}_1,\bar{V}^{-1}_2/a,...,\bar{V}^{-1}_n/a,-\bar{V}^{-1}_{n+1}/a]\\
\textnormal{and }(\bar{g}')^{-1} &= F[-(\bar{V}'_1)^{-1},(\bar{V}'_2)^{-1}/a,...,(\bar{V}'_n)^{-1}/a,-(\bar{V}'_{n+1})^{-1}/a]
\end{aligned}
\end{equation}
be any two minimal decompositions of $\bar{g}^{-1}$ and $(\bar{g}')^{-1}$ (resp.). Then, by Lem.~\ref{lem:Fp}, there are minimal decompositions $\bar{g} = F[\bar{V}_{n+1},...,\bar{V}_1]$ and $\bar{g}' = F[\bar{V}'_{n+1},...,\bar{V}'_1]$ such that:
\begin{small}
\begin{equation}
\begin{aligned}
\forall j \in [n]: & F[\bar{V}^{-1}_{j+1}/a,...,\bar{V}^{-1}_n/a,-\bar{V}^{-1}_{n+1}/a] \circ \bar{g} \circ D_A = -1/a \cdot \sigma \circ F[\bar{V}_{j},...,\bar{V}_1] \circ D_A \\
\textnormal{and: } & F[(\bar{V}'_{j+1})^{-1}/a,...,(\bar{V}'_n)^{-1}/a,-(\bar{V}'_{n+1})^{-1}/a] \circ \bar{g}' \circ D_A = -1/a \cdot \sigma \circ F[\bar{V}'_{j},...,\bar{V}'_1] \circ D_A
\end{aligned}
\end{equation}
\end{small}
Since $\bar{g} \overset{D_A}{\underset{k,\epsilon_1-2\epsilon}{\not\sim}} \bar{g}'$, there is an index $i \in [n+1]$ such that:
\begin{equation}\label{eq:diffg}
\begin{aligned}
&\disc_k(F[\bar{V}_i,...,\bar{V}_1] \circ D_A,F[\bar{V}'_i,...,\bar{V}'_1] \circ D_A) > \epsilon_1 - 2\epsilon\\
\end{aligned}
\end{equation}
The case $i=n+1$ is not a possibility, similarly to Eq.~\ref{eq:notApossibilityn1}. Therefore, there is $i \in [n]$ such that Eq.~\ref{eq:diffg} holds. In addition,
\begin{small}
\begin{equation}
\begin{aligned}
&\disc_{k+1}(-1/a \cdot \sigma \circ F[\bar{V}_i,...,\bar{V}_1] \circ D_A, -1/a \cdot  \sigma\circ F[\bar{V}'_i,...,\bar{V}'_1] \circ D_A) \\
=& \disc_{k+1}\bigg( F[\bar{V}^{-1}_{i+1}/a,...,\bar{V}^{-1}_n/a,-\bar{V}^{-1}_{n+1}/a] \circ \bar{g} \circ D_A, \\
& \;\;\;\;\;\;\;\;\;\;\;\;\;\; F[(\bar{V}'_{i+1})^{-1}/a,...,(\bar{V}'_n)^{-1}/a,-(\bar{V}'_{n+1})^{-1}/a] \circ \bar{g}' \circ D_A\bigg) \\
\end{aligned}
\end{equation}
\end{small}
By Lem.~\ref{lem:disc}, for $p :\leftarrow -1/a \cdot \sigma$ of complexity $1$ we have:
\begin{equation}
\begin{aligned}
\epsilon_1 - 2\epsilon &< \disc_{k}(F[\bar{V}_i,...,\bar{V}_1] \circ D_A, F[\bar{V}'_i,...,\bar{V}'_1] \circ D_A) \\
&\leq  \disc_{k+1}(-1/a \cdot \sigma \circ F[\bar{V}_i,...,\bar{V}_1] \circ D_A, -1/a \cdot \sigma \circ F[\bar{V}'_i,...,\bar{V}'_1] \circ D_A)
\end{aligned}
\end{equation}
In addition, by Lem.~\ref{lem:easy}, for $D_1 :\leftarrow \bar{g} \circ D_A$, $D_2 :\leftarrow D_B$, $D_3 :\leftarrow F[(\bar{V}'_{i+1})^{-1}/a,...,(\bar{V}'_n)^{-1}/a,-(\bar{V}'_{n+1})^{-1}/a] \circ \bar{g}' \circ D_A$, $t \geq (k+1) + C^{\epsilon_0}_{A,B} \geq (k+1) + C(F[\bar{V}^{-1}_{i+1}/a,...,\bar{V}^{-1}_n/a,-\bar{V}^{-1}_{n+1}/a])$, we have:
\begin{small}
\begin{equation}
\begin{aligned}
&\disc_{k+1}\bigg(F[\bar{V}^{-1}_{i+1}/a,...,\bar{V}^{-1}_n/a,-\bar{V}^{-1}_{n+1}/a] \circ \bar{g} \circ D_A, \\
&\;\;\;\;\;\;\;\;\;\;\;\;\;\;\; 
F[(\bar{V}'_{i+1})^{-1}/a,...,(\bar{V}'_n)^{-1}/a,-(\bar{V}'_{n+1})^{-1}/a] \circ \bar{g}' \circ D_A\bigg) \\
&\leq \disc_{t}\bigg(F[\bar{V}^{-1}_{i+1}/a,...,\bar{V}^{-1}_n/a,-\bar{V}^{-1}_{n+1}/a] \circ D_B, \\
&\;\;\;\;\;\;\;\;\;\;\;\;\;\;\; F[(\bar{V}'_{i+1})^{-1}/a,...,(\bar{V}'_n)^{-1}/a,-(\bar{V}'_{n+1})^{-1}/a] \circ \bar{g}' \circ D_A \bigg) + \disc_{t}(\bar{g} \circ D_A, D_B) \\
&\leq \disc_{t}\bigg(F[\bar{V}^{-1}_{i+1}/a,...,\bar{V}^{-1}_n/a,-\bar{V}^{-1}_{n+1}/a] \circ D_B, \\
&\;\;\;\;\;\;\;\;\;\;\;\;\;\;\; F[(V'_{i+1})^{-1}/a,...,(\bar{V}'_n)^{-1}/a,-(\bar{V}'_{n+1})^{-1}/a] \circ \bar{g}' \circ D_A \bigg)+\epsilon_0+\epsilon  \\
\end{aligned}
\end{equation}
\end{small}
Again, by Lem.~\ref{lem:easy}, for $D_1 :\leftarrow \bar{g}' \circ D_A$, $D_2 :\leftarrow D_B$, $D_3 :\leftarrow F[\bar{V}^{-1}_{i+1}/a,...,\bar{V}^{-1}_n/a,-\bar{V}^{-1}_{n+1}/a] \circ D_B$, $m \geq  t + C^{\epsilon_0}_{A,B} \geq t + C(F[(\bar{V}'_{i+1})^{-1}/a,...,(\bar{V}'_n)^{-1}/a,-(\bar{V}'_{n+1})^{-1}/a])$, we have: 
\begin{small}
\begin{equation}
\begin{aligned}
&\disc_{t}\bigg(F[\bar{V}^{-1}_{i+1}/a,...,\bar{V}^{-1}_n/a,-\bar{V}^{-1}_{n+1}/a] \circ D_B, \\
&\;\;\;\;\;\;\;\;\;\;\;\; F[(\bar{V}'_{i+1})^{-1}/a,...,(\bar{V}'_n)^{-1}/a,-(\bar{V}'_{n+1})^{-1}/a] \circ \bar{g}' \circ D_A\bigg)  \\
&\leq \disc_{m}\bigg(F[\bar{V}^{-1}_{i+1}/a,...,\bar{V}^{-1}_n/a,-\bar{V}^{-1}_{n+1}/a] \circ D_B, \\
&\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\; F[(\bar{V}'_{i+1})^{-1}/a,...,(\bar{V}'_n)^{-1}/a,-(\bar{V}'_{n+1})^{-1}/a] \circ D_B\bigg) + \disc_{m}(\bar{g}' \circ D_A, D_B) \\
&\leq \disc_{m}\bigg( F[\bar{V}^{-1}_{i+1}/a,...,\bar{V}^{-1}_n/a,-\bar{V}^{-1}_{n+1}/a] \circ D_B, \\
&\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\; F[(\bar{V}'_{i+1})^{-1}/a,...,(\bar{V}'_n)^{-1}/a,-(\bar{V}'_{n+1})^{-1}/a] \circ D_B \bigg) +\epsilon_0 + \epsilon
\end{aligned}
\end{equation}
\end{small}
Finally, 
\begin{small}
\begin{equation}
\begin{aligned}
&\epsilon_1 - 2\epsilon < \disc_{k}(-1/a \cdot \sigma \circ F[\bar{V}_i,...,\bar{V}_1] \circ D_A, -1/a \cdot \sigma \circ F[\bar{V}'_i,...,\bar{V}'_1] \circ D_A) \\
&\leq \disc_{t}\bigg(F[\bar{V}^{-1}_{i+1}/a,...,\bar{V}^{-1}_n/a,-\bar{V}^{-1}_{n+1}/a] \circ D_B,\\
&\;\;\;\;\;\;\;\;\;\;\;\;\;\;\; F[(\bar{V}'_{i+1})^{-1}/a,...,(\bar{V}'_n)^{-1}/a,-(\bar{V}'_{n+1})^{-1}/a] \circ \bar{g}' \circ D_A\bigg) + \epsilon_0+\epsilon \\
&\leq \disc_{m}\bigg(F[\bar{V}^{-1}_{i+1}/a,...,\bar{V}^{-1}_n/a,-\bar{V}^{-1}_{n+1}/a] \circ D_B, \\
&\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\; F[(\bar{V}'_{i+1})^{-1}/a,...,(\bar{V}'_n)^{-1}/a,-(\bar{V}'_{n+1})^{-1}/a] \circ D_B \bigg) + 2\epsilon_0+2\epsilon  \\
\end{aligned}
\end{equation}
\end{small}
In particular, 
\begin{small}
\begin{equation}\label{eq:partialeq}
\begin{aligned}
\epsilon_2 \leq \epsilon_1 - 2 \epsilon_0 - 4\epsilon &<  \disc_{m}\bigg(F[\bar{V}^{-1}_{i+1}/a,...,\bar{V}^{-1}_n/a,-\bar{V}^{-1}_{n+1}/a] \circ D_B, \\
&\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\; F[(\bar{V}'_{i+1})^{-1}/a,...,(\bar{V}'_n)^{-1}/a,-(\bar{V}'_{n+1})^{-1}/a] \circ D_B \bigg)
\end{aligned}
\end{equation}
\end{small}
Alternatively, for any minimal decompositions 
\begin{small}
\begin{equation}
\begin{aligned}
\bar{f} \circ \bar{g}^{-1} &= F[\bar{W}_{n+1},...,\bar{W}_2,-\bar{W}_1 \cdot \bar{V}_1,\bar{V}^{-1}_2/a,...,\bar{V}^{-1}_n/a,-\bar{V}^{-1}_{n+1}/a]\\
\textnormal{and }
\bar{f}' \circ (\bar{g}')^{-1} &= F[\bar{W}_{n+1},...,\bar{W}_2,-\bar{W}_1 \cdot(\bar{V}'_1)^{-1},(\bar{V}'_2)^{-1}/a,...,(\bar{V}'_n)^{-1}/a,-(\bar{V}'_{n+1})^{-1}/a]
\end{aligned}
\end{equation}
\end{small}
there are right partial functions 
\begin{equation}
F[\bar{V}^{-1}_{i+1}/a,...,\bar{V}^{-1}_n/a,-\bar{V}^{-1}_{n+1}/a] 
\textnormal{ and } 
F[(\bar{V}'_{i+1})^{-1}/a,...,(\bar{V}'_n)^{-1}/a,-(\bar{V}'_{n+1})^{-1}/a]
\end{equation}

such that Eq.~\ref{eq:partialeq} holds, in contradiction to $F(f,g) \overset{D_B}{\underset{m,\epsilon_2}{\sim}}  F(f',g')$. 


\textbf{Part 3:} Finally, by Lem.~\ref{lem:squaredCovering2} and Lem.~\ref{lem:covEmb}, 
\begin{equation}
\begin{aligned}
\Cov\left(H_{\epsilon_0}(A,B;m),\overset{D_A}{\underset{k,\epsilon_1}{\sim}}\right) & \leq \Cov\left((H_{\epsilon_0}(A,B;m))^2,\left(\overset{D_A}{\underset{k,\epsilon_1}{\sim}}\right)^2 \right) \\
&\leq \Cov\left (\textnormal{DPM}_{2\epsilon_0+2\epsilon}\left(B;k, 2C^{\epsilon_0}_{A,B} \right), \overset{D_B}{\underset{m,\epsilon_2}{\sim}} \right)
\end{aligned}
\end{equation}
Alternatively, for all $\epsilon_0,\epsilon_1,\epsilon_2,\epsilon$ such that $\epsilon < \min\{(\epsilon_1 - 2\epsilon_0 - \epsilon_2)/4,\epsilon_2/2\}$,
\begin{equation}
\Cov\left(H_{\epsilon_0}(A,B;m),\overset{D_A}{\underset{k,\epsilon_1}{\sim}}\right) \leq 
\Cov\left (\textnormal{DPM}_{2\epsilon_0+2\epsilon}\left(B;k, 2C^{\epsilon_0}_{A,B} \right), \overset{D_B}{\underset{m,\epsilon_2}{\sim}} \right)
\end{equation}
In particular, we can replace $\epsilon$ with $\epsilon/2$ in the inequality. By Lem.~\ref{lem:covSubset}, the function $q_\epsilon = \Cov\left (\textnormal{DPM}_{2\epsilon_0+\epsilon}\left(B;k, 2C^{\epsilon_0}_{A,B} \right), \overset{D_B}{\underset{m,\epsilon_2}{\sim}} \right)$ is monotonically decreasing as $\epsilon$ tends to $0$ and is lower bounded by $\Cov\left (\textnormal{DPM}_{2\epsilon_0}\left(B;k, 2C^{\epsilon_0}_{A,B} \right), \overset{D_B}{\underset{m,\epsilon_2}{\sim}} \right)$. Therefore, by the monotone convergence theorem, the limit $\lim_{\epsilon \to 0} q_\epsilon$ exists and upper bounds $\Cov\left(H_{\epsilon_0}(A,B;m),\overset{D_A}{\underset{k,\epsilon_1}{\sim}}\right)$.
\end{proof}


\counting* 

\begin{proof} By Lem.~\ref{thm:countingLem}, with $m = k = \infty$, we have:
\begin{equation}\label{eq:counting1}
\begin{aligned}
\Cov\left(H_{\epsilon_0}(A,B),\overset{D_A}{\underset{\epsilon_1}{\sim}}\right) 
&\leq \lim_{\epsilon \to 0} \Cov\left (\textnormal{DPM}_{2\epsilon_0+\epsilon}\left(B; 2C^{\epsilon_0}_{A,B} \right), \overset{D_B}{\underset{\epsilon_2}{\sim}} \right) \\
\end{aligned}
\end{equation}
Similarly, 
\begin{equation}\label{eq:counting2}
\begin{aligned}
\Cov\left(H_{\epsilon_0}(B,A),\overset{D_B}{\underset{\epsilon_1-2\epsilon_0}{\sim}}\right) 
&\leq \lim_{\epsilon \to 0} \Cov\left (\textnormal{DPM}_{2\epsilon_0+\epsilon}\left(B; 2C^{\epsilon_0}_{A,B} \right), \overset{D_B}{\underset{\epsilon_2}{\sim}} \right) \\
\end{aligned}
\end{equation}
By Lem.~\ref{lem:covinv},
\begin{equation}
\begin{aligned}
\Cov\left(H_{\epsilon_0}(A,B),\overset{D_A}{\underset{\epsilon_1}{\sim}}\right)  \leq \Cov\left(H_{\epsilon_0}(B,A),\overset{D_B}{\underset{\epsilon_1-2\epsilon_0}{\sim}}\right) 
\end{aligned}
\end{equation}
Since the limits in the RHS of Eqs.~\ref{eq:counting1} and~\ref{eq:counting2} are limits of positive integers, we have: 
\begin{small}
\begin{equation}
\begin{aligned}
\Cov\left(H_{\epsilon_0}(A,B),\overset{D_A}{\underset{\epsilon_1}{\sim}}\right) &\leq \min \begin{cases}
\lim_{\epsilon \to 0}\Cov\left (\textnormal{DPM}_{2\epsilon_0+\epsilon}\left(B; 2C^{\epsilon_0}_{A,B} \right), \overset{D_B}{\underset{\epsilon_2}{\sim}} \right)\\
\lim_{\epsilon \to 0} \Cov\left (\textnormal{DPM}_{2\epsilon_0+\epsilon}\left(A; 2C^{\epsilon_0}_{A,B} \right), \overset{D_A}{\underset{\epsilon_2}{\sim}} \right)
\end{cases}\\
&\leq 
\lim_{\epsilon \to 0} \min \begin{cases}
\Cov\left (\textnormal{DPM}_{2\epsilon_0+\epsilon}\left(B; 2C^{\epsilon_0}_{A,B} \right), \overset{D_B}{\underset{\epsilon_2}{\sim}} \right)\\
\Cov\left (\textnormal{DPM}_{2\epsilon_0+\epsilon}\left(A; 2C^{\epsilon_0}_{A,B} \right), \overset{D_A}{\underset{\epsilon_2}{\sim}} \right)
\end{cases}
\end{aligned}
\end{equation}
\end{small}
\end{proof} 


\section{Wasserstein GAN results}
\label{appendix:wgan}

It is interesting to check whether the predictions made are valid for other forms of discrepancy such as the one used in the Wasserstein GAN~\cite{wgan} (WGAN). This is done below for Prediction 2, which predicts that the selection of the right number of layers is crucial in unsupervised learning. 
In the WGAN experiment, we employ the architecture of~\citep{discogan} and vary the number of layers and inspect the influence on the results. For the generator, the architecture is identical while for WGAN's critic, the last sigmoid layer is removed. 
These experiments were done on the CelebA dataset, obtaining the results in Fig.~\ref{fig:sw001}--~\ref{fig:sw006}. 

Note that since the encoder and the decoder parts of the learned network are symmetrical, the number of layers is always even. As can be seen, changing the number of layers has a dramatic effect on the results. The best overall results are obtained at 6 layers. Using fewer layers, WGAN often fails to produce images of the desired class. Adding layers, the semantic alignment is lost, as expected.

\begin{figure}[t]
  \centering
  
 \includegraphics[width=1.01\linewidth,clip,trim={4cm 0px 2cm 4cm}]{figures_W2to6/W2to6male_to_female_results-1.jpg}
  \caption{\label{fig:sw001} Results for celebA Male to Female transfer for WGAN with different number of layers. }
\end{figure}

\begin{figure}[t]
  \centering
  
 \includegraphics[width=1.01\linewidth,clip,trim={4cm 0px 2cm 4cm}]{figures_W2to6/W2to6male_to_female_results_r-1.jpg}
  \caption{\label{fig:sw002} Results for celebA Female to Male transfer for WGAN with different number of layers. }
\end{figure}


\begin{figure}[t]
  \centering
  
 \includegraphics[width=1.01\linewidth,clip,trim={4cm 0px 2cm 4cm}]{figures_W2to6/W2to6blond_to_black_results-1.jpg}
  \caption{\label{fig:sw003} Results for celebA Blond to Black transfer for WGAN with different number of layers. }
\end{figure}

\begin{figure}[t]
  \centering
  
 \includegraphics[width=1.01\linewidth,clip,trim={4cm 0px 2cm 4cm}]{figures_W2to6/W2to6blond_to_black_results_r-1.jpg}
  \caption{\label{fig:sw004} Results for celebA Black to Blond transfer for WGAN networks with different number of layers. }
\end{figure}

\begin{figure}[t]
  \centering
  
 \includegraphics[width=1.01\linewidth,clip,trim={4cm 0px 2cm 4cm}]{figures_W2to6/W2to6eyeglasses_results-1.jpg}
  \caption{\label{fig:sw005} Results for celebA Eyeglasses to Non-Eyeglasses transfer for WGAN with different number of layers. }
\end{figure}

\begin{figure}[t]
  \centering
  
 \includegraphics[width=1.01\linewidth,clip,trim={4cm 0px 2cm 4cm}]{figures_W2to6/W2to6eyeglasses_results_r-1.jpg}
  \caption{\label{fig:sw006} Results for celebA Non-Eyeglasses to Eyeglasses transfer for WGAN with different number of layers. }
\end{figure}

\section{CycleGAN results}
\label{appendix:cyclegan}

While most of our experiments have focused on the DiscoGAN architecture of~\cite{discogan}, an additional experiment was conducted in order to verify that these extend to the CycleGAN architecture of~\cite{CycleGAN2017}.

The results are shown in Fig.~\ref{fig:aer001}. As can be seen running an experiment on the Aerial images to Maps dataset, we found that 8 layers produces an aligned solution. Using 10 layers produces unaligned map images with low discrepancy. For fewer than 8 layer, the discrepancy is high and the images are not very detailed. 

\begin{table}[H]
\centering
\caption{{\color{black} Numerical results for the experiment of Cityscapes to Image Segmentation. Standard metrics are used to evaluate the segmentation accuracy for different number of layers.}}
\label{tab:acc}
\begin{tabular}{lcccccccc}
                             & $k=2$     & $k=4$     & $k=6$     & $k=8$     & $k=10$     & $k=12$     \\
\midrule
Mean pixel accuracy & 0.52 & 0.54  & 0.53 & 0.60 & 0.63 & 0.51 \\
\midrule
Mean class accuracy & 0.16 & 0.16 & 0.19 & 0.15 & 0.18 & 0.11 \\
\midrule
Mean class IoU & 0.10 & 0.11 & 0.11 & 0.10 & 0.13 & 0.08 \\
\bottomrule      
\end{tabular}
\end{table}

\begin{figure}[t]
  \centering
 \includegraphics[width=1.01\linewidth,clip,trim={5.5cm 6.5cm 5.5cm 4.7cm}]{figures_Cycle2to6/Aerial_maps_1to5_results-1.jpg}
  \caption{\label{fig:aer001}  Results for Aerial View Images to Maps transfer for CycleGAN with different number of layers. }
\end{figure}

\begin{figure}[t]
  \centering
 \includegraphics[width=1.01\linewidth,clip,trim={4.8cm 6.5cm 4.5cm 5cm}]{figures_Cycle2to6/Cityscapes_1to6_results-1.jpg}
  \caption{\label{fig:seg001} Results for Segmentations to Images transfer for CycleGAN with different number of layers. }
\end{figure}
\end{document}