\documentclass[conference]{IEEEtran}
\IEEEoverridecommandlockouts
% The preceding line is only needed to identify funding in the first footnote. If that is unneeded, please comment it out.
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
%my modify
\usepackage{booktabs}           % professional-quality tables
\usepackage{multirow}           % tabular cells spanning multiple rows
\usepackage{amsfonts}           % blackboard math symbols
\usepackage{graphicx}           % figures
\usepackage{duckuments}         % sample images
%\usepackage[backref]{hyperref}
\usepackage[colorlinks,linkcolor=blue,anchorcolor=blue,citecolor=blue]{hyperref}


\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
\begin{document}



\title{InferTurbo: A Scalable System for Boosting Full-graph Inference of Graph Neural Network over Huge Graphs
%\\
%{\footnotesize \textsuperscript{*}Note: Sub-titles are not captured in Xplore and
%should not be used}
%\thanks{Identify applicable funding agency here. If none, delete this.}
}

\author{ 
	Dalong Zhang, Xianzheng Song, Zhiyang Hu, Yang Li, Miao Tao, Binbin Hu, Lin Wang, Zhiqiang Zhang, Jun Zhou$^{\ast}$ \thanks{*Corresponding author} \\
	\IEEEauthorblockA{\{dalong.zdl, xianzheng.sxz, zhiyang.hzhy, ly120983, taotao.tm, bin.hbb, fred.wl, lingyao.zzq,  jun.zhoujun\}@antfin.com} 
	\textit{Ant Group, Hangzhou, China}
}

%\author{\IEEEauthorblockN{Dalong Zhang}
%\IEEEauthorblockA{\textit{dept. name of organization (of Aff.)} \\
%\textit{name of organization (of Aff.)}\\
%City, Country \\
%email address or ORCID}
%\and
%\IEEEauthorblockN{2\textsuperscript{nd} Given Name Surname}
%\IEEEauthorblockA{\textit{dept. name of organization (of Aff.)} \\
%\textit{name of organization (of Aff.)}\\
%City, Country \\
%email address or ORCID}
%\and
%\IEEEauthorblockN{3\textsuperscript{rd} Given Name Surname}
%\IEEEauthorblockA{\textit{dept. name of organization (of Aff.)} \\
%\textit{name of organization (of Aff.)}\\
%City, Country \\
%email address or ORCID}
%\and
%\IEEEauthorblockN{4\textsuperscript{th} Given Name Surname}
%\IEEEauthorblockA{\textit{dept. name of organization (of Aff.)} \\
%\textit{name of organization (of Aff.)}\\
%City, Country \\
%email address or ORCID}
%\and
%\IEEEauthorblockN{5\textsuperscript{th} Given Name Surname}
%\IEEEauthorblockA{\textit{dept. name of organization (of Aff.)} \\
%\textit{name of organization (of Aff.)}\\
%City, Country \\
%email address or ORCID}
%\and
%\IEEEauthorblockN{6\textsuperscript{th} Given Name Surname}
%\IEEEauthorblockA{\textit{dept. name of organization (of Aff.)} \\
%\textit{name of organization (of Aff.)}\\
%City, Country \\
%email address or ORCID}
%}

\maketitle

\begin{abstract}
%This document is a model and instructions for \LaTeX.
%This and the IEEEtran.cls file define the components of your paper [title, text, heads, etc.]. *CRITICAL: Do Not Use Symbols, Special Characters, Footnotes, 
%or Math in Paper Title or Abstract.
%With the rapid development of Graph Neural Networks (GNNs), more and more attention has been paid to the system design that can improve the training efficiency.
%By contrast, the efficiency of GNN inference has not received much attention.
With the rapid development of Graph Neural Networks (GNNs), more and more studies focus on system design to improve training efficiency while ignoring the efficiency of GNN inference. 
Actually, GNN inference is a non-trivial task, especially in industrial scenarios with giant graphs, given three main challenges, i.e., scalability tailored for full-graph inference on huge graphs, inconsistency caused by stochastic acceleration strategies (e.g., sampling), and the serious redundant computation issue.
%However, GNN inference is a non-trivial task, especially in industrial scenarios with giant graphs. 
%There are three main challenges in GNN inference, i.e., scalability tailored for full-graph inference on huge graphs, consistency caused by stochastic acceleration strategies (e.g., sampling), and the serious redundant computation issue.
To address the above challenges, we propose a scalable system named InferTurbo to boost the GNN inference tasks in industrial scenarios.
Inspired by the philosophy of ``think-like-a-vertex", a GAS-like (Gather-Apply-Scatter) schema is proposed to describe the computation paradigm and data flow of GNN inference.
The computation of GNNs is expressed in an iteration manner, in which a vertex would gather messages via in-edges and update its state information by forwarding an associated layer of GNNs with those messages and then send the updated information to other vertexes via out-edges.
Following the schema, the proposed InferTurbo can be built with alternative backends (e.g., batch processing system or graph computing system).
Moreover, InferTurbo introduces several strategies like shadow-nodes and partial-gather to handle nodes with large degrees for better load balancing.
With InferTurbo, GNN inference can be hierarchically conducted over the full graph without sampling and redundant computation.
Experimental results demonstrate that our system is robust and efficient for inference tasks over graphs containing some hub nodes with many adjacent edges. 
Meanwhile, the system gains a remarkable performance compared with the traditional inference pipeline, and it can finish a GNN inference task over a graph with tens of billions of nodes and hundreds of billions of edges within 2 hours.


%the \emph{scalability} challenge caused by graph size and high frequency of triggering, the \emph{consistency} challenge caused by the randomness introduced by some acceleration strategies (e.g., sampling),
%and the \emph{efficiency} challenge caused by a mass of redundant computations.

%%(e.g., graphs with billions of nodes and trillions of edges).
%By contrast, the efficiency of GNN inference has not received much attention.
%to achieve better load-balancing, InferTurbo introduces several strategies like shadow-nodes and partial-gather to handle nodes with large degrees.
%, and thus enjoys good properties of scalability, consistency, and efficiency.


%With the rapid development of Graph Neural Networks (GNNs), more and more attention has been paid to the system design that can improve the training efficiency.
%By contrast, the efficiency of GNN inference has not received much attention.
%However, GNN inference is a non-trivial task, especially in industrial scenarios with giant graphs (e.g., graphs with billions of nodes and trillions of edges).
%In summary, there are three main challenges when performing GNN inference over such giant industrial graphs on existing GNN systems: the \emph{scalability} challenge caused by graph size and high frequency of triggering, the \emph{consistency} challenge caused by the randomness introduced by some acceleration strategies (e.g., graph sampling),
%and the \emph{efficiency} challenge caused by a mass of redundant computations.
%To address the above challenge, we propose a scalable framework named InferTurbo to boost the GNN inference tasks in industrial scenarios.
%Inspired by the philosophy of "think-like-a-vertex", we propose a GAS-like (Gather-Apply-Scatter) schema to describe the computation paradigm and data flow of GNN inference.
%The computation of GNNs is expressed in an iteration manner, in which a vertex would gather messages via in-edges and update its state information by forwarding an associated layer of GNN with those in-coming messages, and then send the updated information to other vertexes via out-edges.
%Following the schema, the proposed InferTurbo can be built with alternative backends (e.g, batch processing system or graph computing system).
%Moreover, to achieve better load-balancing, InferTurbo introduces several strategies like shadow-nodes and partial-gather to handle nodes with large degrees.
%With InferTurbo, GNN inference can hierarchically be conducted over the full graph without sampling and redundant computation, and thus enjoys good properties of scalability, consistency, and efficiency.
%Experimental results demonstrate that our system is robust and efficient for inference tasks over graphs containing some hub nodes with many adjacent edges. 
%Meanwhile, the system gains a remarkable performance compared with the traditional inference pipeline and it can finish a node classification task over a graph with tens of billions of nodes and hundreds of billions of edges within 2 hours.

\end{abstract}

\begin{IEEEkeywords}
Graph Neural Networks, Distributed System, Full-graph Inference, Big Data 
\end{IEEEkeywords}

\section{Introduction}
%This document is a model and instructions for \LaTeX.
%Please observe the conference page limits. 
Graph Neural Networks (GNNs) generalize deep learning over regular grids (e.g., images) to highly unstructured data and have emerged as a powerful learning tool for graph data.
Recently, GNNs have demonstrated impressive success on various tasks, ranging from traditional graph mining tasks (e.g., node classification\cite{b1,b2,b3,b4} and link prediction\cite{b5,b6}) to the fields of natural language processing\cite{b7} and computer vision\cite{b8}.
Meanwhile, graph data forms the basis of innumerable industrial systems owing to its widespread utilization in real-world applications (e,g., recommendation\cite{b9,b10,ab11,ab12,ab13}, marketing\cite{b11,ab15}, fraud detection\cite{b12, ab16}), further facilitating the flourishing of GNNs in industrial communities.
% Meanwhile, it draws more and more attention from industrial community and gains significantly improvement in various real-world applications (e.g., recommendation, marketing, fraud detection).
%Both academic and industrial community demonstrated impressive success of GNNs in various tasks from traditional graph mining tasks (e.g., node classification, link prediction) to real-world problems (e.g., recommendation, marketing, fraud detection). 
%Graph neural network (GNN) has been emerging as a powerful learning tool for graph data, and it aggregates neighbors' information in an iterative way and the receptive fields will be expanded to irregular k-hop neighborhoods for target nodes. 
%To solve such inherent data dependency problem, various techniques such as k-hop neighborhoods, neighbor sampling are proposed, based on which, many talented GNN learning systems[][] make it possible to train GNNs efficiently on graphs at scale.
%Owing to this, both academic and industrial community demonstrated impressive success of GNNs in various tasks from traditional graph mining tasks (e.g., node classification, link prediction) to real-world problems (e.g., recommendation, marketing, fraud detection). 

The key idea of current GNNs is taking advantage of information aggregation schema, which can effectively summarize multi-hop neighbors into representations via stacking multiple GNN layers. Essentially,  the high computation cost exponentially growing with the number of GNN layers poses the non-trivial challenges for \emph{GNN Training and Inference} in industrial scenarios with huge graphs, containing billions
of nodes and trillions of edges. With the aim of the possibility of performing the highly scalable and efficient GNN training in real industrial scenarios, several recent efforts have been made in industrial-purpose graph learning systems, including single-machine systems with the cooperation between GPUs and CPUs (i.e., DGL\cite{b13,b14} and PyG\cite{b15}), distributed systems based on localized convolutions (i.e.,  Aligraph\cite{b16}) and pre-processed information-complete sub-graphs (i.e., AGL\cite{b17}). Unfortunately, the main focus of these systems is training GNNs efficiently on graphs at scale, while few attention has been paid to the inference phase of GNNs~\footnote{Different from online inference whose goal is guaranteeing the low latency of real-time online serving~\cite{aab17,aab18}, the focus of GNN inference in our paper aims at efficiently performing full-graph inference in the offline environment, which is ubiquitous in financial applications.}, which is also the core of an industrial graph learning system.

% Recently, several efforts have been made in industrial-purpose graph learning systems, which provide the possibility of performing the highly scalable and efficient GNN training in real industrial scenarios.

% Different from traditional deep learning algorithms, GNNs aggregate neighbors' information in an iterative way and the receptive fields will be expanded to irregular k-hop neighborhoods for target nodes, which make it challenging to train a GNN model on large graphs.
% Many efforts have been paid to solve such inherent data dependency problem in training GNNs: AGL proves that k-hop neighborhoods provide sufficient and necessary information for k-layer GNN model, and pre-generates the k-hop neighborhoods with a MapReduce pipeline.
% Inspired by the neighbor-sampling [graph sage] strategy, Aligraph, DGL, PyG perform localized convolutions by sampling neighborhood for target nodes, since sampling could approximate real distribution of target nodes with fewer computation and may help gain good property of generalization for GNN models.
% Benefit from those strategies, those systems make it possible to train GNNs efficiently on graphs at scale. 



In current graph learning systems\cite{b13,b14,b15,b16}, the inference phase is conducted by blindly imitating the pipeline of the training phase. However, the inference phase of GNN has its  unique characteristics, making it distinct from the training phase, such that the pure design of inference stage in current graph learning systems is unsuitable for performing large-scale inference tasks in industrial scenarios.
\begin{itemize}
    \item \emph{The large gap of data scale between training and inference phases.} 
    In a general graph learning task, only a small number of nodes (e.g., 1\% or even less in actual scenarios) are labeled for GNN training, while the inference tasks are usually expected to be taken over the entire graph. Such a gap is steadily worsening in the industrial scenarios with huge graphs, containing hundreds of millions or even billions of nodes\cite{b17, ab18, ab19} (e.g., Facebook\footnote{https://en.wikipedia.org/wiki/Facebook,\_Inc.}, Ant Group\footnote{https://en.wikipedia.org/wiki/Alipay}). Both observations encourage us to re-consider the scalability issue encountered in the inference tasks with the careful thought of memory storage and communication bandwidth.
    \item \emph{No guarantee for the consistency of predictions in inference phase.}
    As a basic requirement in industrial scenarios, the prediction score for a certain node should keep consistency at multiple runs, which unfortunately gets no guarantee in the GNN inference of current systems. To extend GNNs to large-scale graphs, the $k$-hop neighbor sampling strategy is widely adopted in current graph learning systems\cite{b13,b14,b15,b16,b17,b17append}. Although the training phase could greatly benefit from the efficiency-oriented strategy for generalized GNNs, the inherent stochasticity for prediction is unacceptable in inference phase,  especially for financial applications (e.g., fraud detection and loan default prediction\cite{b12, ab16}).
    \item \emph{Serious redundant computation issue in the inference phase.} 
    Current graph learning systems\cite{b13,b14,b15,b16} follow the procedure that performs forward and backward of GNNs over $k$-hop neighborhoods in a mini-batch manner, and would obtain a well-trained GNN by repeating this procedure enough times. In spite of its potential capability of enjoying mature optimization algorithms and data-parallelism in training phase, $k$-hop neighbor sampling with the mini-batch training manner would introduce the undesirable redundant computation issue in the inference phase since only one forward pass of GNNs is required. 
\end{itemize}

%To tackle these limitations of current graph learning systems that perform both of GNN training and inference with the mini-batch manner, we aim at facilitating GNN inference in industrial scenarios in a cooperative setting of mini-batch based training and full-batch based inference. However, its design is non-trivial, given two major challenges not explored in current graph learning systems:
Addressing the limitations of current graph learning systems, we aim to facilitate GNN inference in industrial scenarios through a collaborative setting of mini-batch-based training and full-batch-based inference.
The significance of the cooperative setting is not trivial considering two major challenges that are not explored in current graph learning systems:
\begin{itemize}
    \item \emph{C1: How to unify the mini-batch based training and full-batch based inference in a graph learning system?} 
    As mentioned above, the mini-batch strategy is the promising way for efficient GNNs training, while the full-batch strategy is suitable for inference to alleviate computation issues. To combine those two advantages, it is impressive to abstract a new schema of graph learning that a GNN trained in the mini-batch manner could naturally perform inference in the full-batch manner.
    %a mini-batch training based GNN could naturally perform inference in the full-batch manner.
    \item \emph{C2: How to efficiently perform GNN inference in distributed environments, especially for huge graphs with extremely skew degree distribution?}
    To ensure the consistency of prediction in inference tasks, we discard the strategies related to neighbor sampling, which put forward the urgent request for efficient GNN inference on huge graphs. In addition, due to the ubiquity of Power-Law, it is critically important to handle long-tailed nodes with extremely large degrees in terms of time cost and resource utilization, as well as avoiding the unexpected Out Of Memory (OOM) issues.
\end{itemize}


                                               
% However, few attention has been payed to inference phase of GNNs.
% Existing systems[][] usually just forward a well trained GNN model as what the training phase does, which is a common sense in deep learning community.
% However, things go much different for inference tasks in industrial scenarios and the main challenges are three-fold:
%  \textbf{Firstly, Scalability}. 
% The inference tasks suffer more serious scalability problem than training tasks.
% The data scale and using frequency of inference tasks could be quite higher than that of training tasks in various industrial applications.
% For example, it's common that every node (user) should get a predict score in fraud detection application, while the graph data from Internet companies (e.g., Facebook, Ant Group) could contain hundreds of millions or even billions of nodes (users). 
% In supply chain identification tasks in e-commerce companies (e.g., Amazon, Alibaba), the candidate edges (supply chains) to predict could reach as many as hundreds of billons.
% The labeled data in those graphs may be one percent or thousandth of the full graph while the inferring usually should be taken on all the left.
% What prevents the training task from scaling to large graphs, such as memory storage, communication bandwidth, leaves more serious trouble on inferring task.
%  \textbf{Secondly, consistency}.
%  The predicting score for a certain node should keep stable at different runs, which is a basic requirement in industrial scenarios. 
% However, neighbor sampling, which is originally designed to help GNN scale to large graphs and widely used in many GNN training systems, introduces some stochastic factors to k-hop neighborhoods for target nodes, and as a result, would lead to unstable embeddings and predicting scores. 
% Such case is beneficial in training phase[], since sampling can approximate real distribution of target nodes with fewer computation and may help gain good property of generalization for GNN models, but is unacceptable in inferring phase especially in financial applications such as fraud detection, loan default prediction and so on.
% \textbf{Thridly, efficiency}. In training phase, it is common to conduct the forward and backward of GNN on k-hop neighborhoods of different target node individually to enjoy the mature data-parallelism training schema for large graphs, but leaves problems of redundant computation due to overlaps of k-hop neighborhoods.
% A tiny piece of redundant computation and storage for a certain node would leave heavy burden on time cost and resource utilization when repeated billions of times.
% What's worse, natural graphs in industrial scenarios usually follows the Power-Law[] and the degree distribution could be quite skew.
% As a result, nodes with large degree could be in the long tail in terms of time cost and resource utilization,
% and not only slow the full inference pipeline but also could lead to OOM (out-of-memory) problem.
%In this system, the data and computation flow of GNNs are described 

To this end, we design a system named \textbf{InferTurbo} to boost the inference tasks on industrial-scale graphs. 
We propose a GAS-like (Gather-Apply-Scatter) schema\cite{b19} together with an annotation technique to describe different stages of the data flow and the computation flow of GNNs, which could be used to unify the mini-batch training and full-graph inference phases.
The training and inference phases share the same computation flow but use different implementations of data flow.
The data flow parts are made as built-in functions currently, and users would only focus on how to write models in our computation flow.
%Specially, GNNs in such abstraction is expressed from node's perspective and could be naturally performed distributedly in inference phase by partitioning nodes into different machines.
%: a node would \emph{gather} messages from in-edges and performs computations to update its state (\emph{apply}), and then generates new messages based on the updated state information, and finally \emph{scatter} those messages to destination nodes via out-edges.
%Such process could be naturally performed distributedly in inference phase by partitioning nodes into different machines.
%In this way, we eliminate the redundant computation caused by k-hops neighborhoods in inference phase.
Moreover, to handle the corner cases in natural graphs in industrial scenarios, such as nodes with extremely large amount of in-edges or out-edges, a set of strategies without any sampling like \emph{partial-gather}, \emph{broadcast}, and \emph{shadow-nodes} are proposed to balance the load of computation and communication and mitigate the long tail effect caused by those nodes.
As a result, consistency prediction results could be guaranteed at different runs, and the system enjoys a better load-balancing.
At last, the system is implemented on both the batch processing system\cite{b20,b21} and the graph process system\cite{b22}, which makes it gain good system properties (e.g., scalability, fault tolerance) of those mature infrastructures.
%We propose a GAS-like (Gather-Apply-Scatter) schema\cite{b19} together with an annotating technique to describe the data and computing flow of GNNs, with which, the inference phase could be hierarchically conducted over a full graph: a node would \emph{gather} messages from in-edges and performs a serious of computations to update its state (\emph{apply}), and then generates new messages based on the updated state information and \emph{scatter} those messages to destination nodes via out-edges.
%Such process could be naturally performed distributedly and by repeating it $k$ times, we get final embeddings of $k$-layer GNNs.
%In this way, we eliminate the redundant computation caused by k-hops neighborhoods and the consistency could be guaranteed since no randomness factors would be introduced in this process.
%Meanwhile, to handle the corner cases in natural graphs in industrial scenarios, such as nodes with extremely large amount of in-edges or out-edges. we design a set of strategies like \emph{partial-gather}, \emph{broadcast}, and \emph{shadow-nodes} to average the load of computation and communication and mitigate the long tail effect caused by those nodes.
%As a result, we could get stable embeddings and predicting scores at different runs while enjoying a better load-balancing.
%We implement our system on both the batch processing system\cite{b20,b21} and the graph process system\cite{b22}, which makes it gain good system properties (e.g., scalability, fault tolerance) of those mature infrastructures.
The main contributions of this work are summarized as follows:
\begin{itemize}
    \item We propose a GAS-like schema together with an annotation technique to describe the data and computing flow of GNNs, and make it possible to hierarchically conduct the inference phase over a full graph.
In this way, the redundant computation caused by k-hops neighborhood is eliminated in inference phase.
    \item We describe implementation details of the system on both batch processing system\cite{b20,b21} and graph computing system\cite{b22}, which are mature infrastructures and widely available in industrial communities.
    \item  We design a set of strategies such as \emph{partial-gather}, \emph{broadcast}, and \emph{shadow-nodes} for inference tasks to handle the power-law problem in industrial graphs.
    %to handle problems of inference tasks caused by power-law in industrial graphs.
    \item Compared with some state-of-the-art GNN systems (DGL, PyG), the proposed system demonstrates remarkable results in efficiency and scalability while achieving comparable prediction performance, and it could finish a node classification task over a graph with tens of billions of nodes and hundreds of billions of edges within 2 hours.  
%our system demonstrates comparable prediction performance.
%Meanwhile, it achieves remarkable results in efficiency and scalability and it could finish a node classification task over a graph with tens of billions of nodes and hundreds of billions of edges within 2 hours.
\end{itemize}



%The main contributions of this work are summarized as follows:
% (1), We proposed a three-stage GAS-like (Gather-Apply-Scatter) schema together with an auto-annotating technique to describe the data and computing flow of GNNs,
% and hierarchically conduct the inferring phase over a full graph, which helps eliminate the redundant computation caused by k-hops neighborhood.
% (2), We provide alternative backends of batch processing system (like MapReduce) and graph computing system (like Grape) to conduct the GAS-like schema, which makes our framework enjoys good system properties (e.g., scalability, fault tolerance)  of those mature infrastructures.
% (3), We design a set of strategies such as salting, partial-gather for nodes with relatively high degree to avoid sampling in inferring phase. And as a result, we get stable embeddings and predicting scores at different runs while enjoy a better load-balancing.
% 

% \textcolor{blue}{
\section{Preliminaries}
\label{sec:preliminaries}
%The k-hop neighborhood and k-hop neighbor sampling, which are often utilized in the GNN training and inference phase, will be introduced in this section.
%we introduce a pivotal concept,  i.e., k-hop neighborhood, and discuss its connection to the GNNs.
% }
%\textcolor{blue}{
\subsection{K-hop Neighborhood and Sampling}
%The definition of a \emph{directed and weighted attributed graph} is 
A \emph{directed}, \emph{weighted}, and \emph{attributed} graph is denoted as
$\mathcal{G}=\{\mathcal{V},\mathcal{E},\mathbf{X},\mathbf{E}\}$, where
$\mathcal{V}$ and $\mathcal{E} \subseteq \mathcal{V} \times \mathcal{V}$ are the
node set and edge set of $\mathcal{G}$, respectively. 
$\mathbf{X}$ and $\mathbf{E}$ are node features and edge features.
The $k$-hop neighborhood $\mathcal{G}_v^k$ of node $v$, is defined as the \emph{induced attributed subgraph} 
of $\mathcal{G}$ whose node set is
$\mathcal{V}_v^k = \{v\} \cup \{u | d(v,u) \leq k\}$, where $d(v,u)$ denotes 
the length of the shortest path from $u$ to $v$
%The $k$-hop neighborhood w.r.t. a target node $v$, denoted as 
%$\mathcal{G}_v^k$, is defined as the \emph{induced attributed subgraph} 
%of $\mathcal{G}$ whose node set is
%$\mathcal{V}_v^k = \{v\} \cup \{u: d(v,u) \leq k\}$, where $d(v,u)$ denotes 
%the length of the shortest path from $u$ to $v$. 
%The edges in $\mathcal{E}$ that include both endpoints in its node set make up its edge set, i.e., 
, and edge set is $\mathcal{E}_v^k = \{(u,u') | (u,u')\in \mathcal{E} 
\land u\in\mathcal{V}_v^k \land u'\in\mathcal{V}_v^k \}$. 
Additionally, $\mathcal{G}_v^k$ also contains the feature vectors of the nodes and edges. %, $\mathbf{X}_v^k$ and $\mathbf{E}_v^k$. 
Without loss of generality, node $v$ itself is treated as its $0$-hop neighborhood.
%The embedding of node $v$ can be produced via a $k$-layer GNN model that adheres to the paradigm of Eq.~\ref{eq:mpgnn} using information from the k-hop neighborhood.
It is proved that $k$-hop neighborhood would provide sufficient and necessary information for $k$-layer GNNs\cite{b17}.
%The mature data-independent parallelism based on k-hop neighborhoods would benefit both the training and inference stages.
%}

%\textcolor{blue}{
%The mature data-independent parallelism based on k-hop neighborhoods would benefit both the training and inference stages.
However, the size of k-hop neighborhood grows exponentially with the number of hops, making the computations performed on it memory-intensive and time-consuming.
\emph{$K$-hop sampling} is proposed to address those problems. It would sample neighbors in a top-down manner, i.e., sample neighbors in the $k$-th hop given the nodes in the $(k-1)$-th hop recursively.
%selects some nodes from seed node's immediate neighbors and then set them as seed nodes for the next iteration.
In particular, different sampling approaches\cite{b4, b10, b17, b17append, b17append2} employ different ways to select the neighbors, and one typical method is to randomly choose a fixed number of neighbors in each iteration.
%By sampling a node's direct neighbors and then widening the receptive field depending on those neighbors, K-hop sampling is suggested as a solution to these issues.
%The k-hop sampling technique, for instance, would choose a few nodes from a seed node's immediate neighbors and then assign those sampled nodes as seed nodes for the following round.
%}

%\textcolor{red}{
%\subsection{K-hop Sampling}
%In most case, conducting the training and inference phase base on k-hop neighborhood would 
%Both the training and inference phases would be able to enjoy the mature data-independent parallelism based on k-hop neighborhoods.  
%However, the k-hop neighborhood would be extended exponentially with the number of hops, which would make it quite memory and time consuming to perform computation over it.
%K-hop sampling is proposed to mitigate those problems by sampling direct neighbors for a certain node and then expanding the receptive field based on those sampled neighbors.
%For example, for a seed node, the k-hop sampling strategy would select a number of nodes from its direct neighbors and then set those sampled nodes as seed nodes for the next round.
%By varying the number of neighbors for each layer, user would make a trade-off between computation efficiency and prediction precision. 
%The mature data-independent parallelism based on k-hop neighborhoods would benefit both the training and inference stages.
%However, the k-hop neighborhood would grow exponentially with the number of hops, making computing over it very memory and time intensive.
%By sampling a node's direct neighbors and then widening the receptive field depending on those neighbors, K-hop sampling is suggested as a solution to these issues.
%The k-hop sampling technique, for instance, would choose a few nodes from a seed node's immediate neighbors and then assign those sampled nodes as seed nodes for the following round.
%In particular, different sampling approaches may have different ways to sample the neighbors for each hop, and one typical method is to randomly choose a certain number of neighbors.
%The user might trade off computing speed for forecast accuracy by changing the number of neighbors for each layer.
%}

\subsection{Graph Neural Networks}
Graph neural network is a popular way to learn over graph data, and it aggregates neighbors' information for a certain node in an iterative way.
Given an attributed graph $\mathcal{G}$, a GNN layer can be formulated in a \textbf{message passing paradigm}\cite{b13,b15,b23}, which first computes \emph{messages} via edges and then \emph{updates} the state of the target node with those messages:
\begin{equation} \label{eq:mpgnn}
\begin{array}{cl}
\text{message:} & m_{v,u}^{k+1} = \mathcal{M}(\mathbf{h}_v^k, \mathbf{h}_u^k, e_{v,u}),\\
\\
\text{update:} & \mathbf{h}_v^{k+1}= \mathcal{U}(\mathbf{h}_v^k, \mathcal{R}(\{m_{v,u}^{k+1}\}_{u \in \mathcal{N}^+_v})),\\
\end{array}
\end{equation}
where $\mathbf{h}_v^{k}$ denotes intermediate 
embedding of node $v$ at the $k$-th layer and $\mathbf{h}_v^{0} = \mathbf{x}_v$,
 $e_{v,u}$  and $m_{v,u}^{k+1}$ indicate edge features and messages associated with $v$'s in-edge from $u$ respectively,
$\mathcal{M}$ represents the \emph{message} function that generates messages according to adjacent node features and edge features on each edge,
$\mathcal{U}$ means the \emph{update} function that updates node embedding by aggregating incoming messages based on the \emph{reduce} function $\mathcal{R}$.
The \emph{message} function and \emph{update} function are usually neural networks, while the \emph{reduce} function could be a certain pooling function (such as sum, mean, max, min) or neural networks.

Most GNN algorithms (e.g., GCN\cite{b1}, GraphSAGE\cite{b4}, GAT\cite{b3}) could be expressed in this paradigm with a few changes in those functions.
For example, \emph{message} functions for GCN, GraphSAGE, and GAT, only take the source node's hidden state (or with edge features) as input.
In addition, \emph{reduce} functions for GCN and GraphSAGE are set to pooling functions, while for GAT, the reduce function would perform a weighted sum over messages based on the attention mechanism.
%Meanwhile, the \emph{reduce} function for the first two commonly used GNN algorithms is set to a certain pooling function, while for GAT, it first computes attentions for messages and then performs weighted sum over those messages.

\section{Related Works}
\label{sec:relate_works}
%\begin{equation} \label{eq:gnn}
%\mathbf{h}_v^{(k+1)} = \phi^{(k)}(h_v^{k}, \{h_u^{k}, e_{v,u}\}_{u \in \mathcal{N}^+_v}; \mathbf{W}_{\phi}^{(k)}),
%%\phi^{(k)}(\{\mathbf{h}_i^{(k)}\}_{i \in \{v\} \cup \mathcal{N}^+_v}, \{\mathbf{e}_{v,u}\}_{\mathbf{A}_{v,u}>0}; \mathbf{W}_{\phi}^{(k)}), 
%\end{equation}
%where $\mathbf{h}_v^{(k)}$ denotes node $v$'s intermediate 
%embedding in the $k$\textsuperscript{th} layer and $\mathbf{h}_v^{(0)} = \mathbf{x}_v$. 
%$\mathcal{N}^+_v$ denotes the set of nodes directly pointing at $v$.
%The function $\phi^{(k)}$ parameterized by $\mathbf{W}_{\phi}^{(k)}$, takes the 
%embeddings of $v$ and its in-edge neighbors $\mathcal{N}^+_v$, as well as 
%the edge features $e_{v,u}$ associated with $v$'s in-edges as inputs, and outputs 
%the embedding for the next GNN layer. Recent studies [][] manage to unify different GNN variants into the \textbf{message passing paradigm}, which first computes \emph{messages} via edges and \emph{updates} node state with those messages:
%\begin{equation} \label{eq:mpgnn}
%\begin{array}{cl}
%\text{message:} & m_{v,u}^{k+1} = \mathcal{M}(h_v^k, h_u^k, e_{v,u}),\\
%\\
%\text{update:} & h_v^{k+1}= \mathcal{U}(h_v^k, \mathcal{R}(\{m_{v,u}^{k+1}\}_{u \in \mathcal{N}^+_v})),\\
%\end{array}
%\end{equation}
%where $\mathcal{M}$ means message function on every edge to generate messages according to adjacent node features and edge features,
%while $\mathcal{U}$ means update function to update node embedding by aggregating incoming messages based on reducing function $\mathcal{R}$.
%The \emph{message} function and \emph{update} function are usually neural networks, while the reduce function could be a certain pooling function like sum, mean, max, min, or neural networks.

%the \emph(message} function for GCN and GraphSAGE only takes the source node's hidden state (with edge features) as input and reduce function here is set to a certain of pooling function, while for GAT, the \emph(message} function is 

%Such paradigm indicates the computing flow for GNNs: compute message from neighbors via in-edges and then aggregate all messages as new embeddings.
%It's obvious the data dependency problem exists in both the forward and backend pass and the receptive field for a certain target node could expand exponentially.
%There's still a long way to go for both the GNN training and inference tasks when scale to large graphs.

%though there may be some unnecessary intermediate results or data movement for some algorithms[].

\subsection{Graph Processing System}
The idea of message passing could be traced back to the graph processing field, which focuses on understanding the structure properties or topology of graphs, such as finding out the most important vertices (e.g., PageRank\cite{b24}), seeking the shortest path between nodes (e.g., SSSP\cite{b25}), and so on.
In the following, some gifted works will be introduced as they motivate our work a lot.
%Both the GNN algorithms and the graph processing algorithms meet the data dependency problem, as the computation is not only related with itself, but also with its neighbors, and the receptive field of the computation could be highly unstructured and expand exponentially.
%they all should be conducted on highly unstructured graph data.
%By exploring solutions in this highly developed field, we think we could get inspired when scaling to large graphs for GNNs.

%Early works\cite{b26,b27} in this field could only be used over a single machine, while with the rapidly growing scale of graph data, great efforts have been paid to conduct graph processing tasks distributedly to handle extremely large graphs.
In the graph processing field, with the rapid growth of graph data, many efforts\cite{b26,b27} have been paid to conduct graph processing tasks distributedly to handle extremely large graphs.
Pregel\cite{b22} adopts a ``think like a vertex" programming model, in which each node receives messages from its neighbors in the previous iteration, modifies its own state or mutates the graph topology, and sends messages to other nodes via its out-edges.
%Each vertex program could be placed on a certain machine, and the whole graph processing task could be naturally distributedly conducted over an entire cluster.
PowerGraph\cite{b19} further develops the vertex-centric programming model with three fine-grained conceptual phases to describe the data flow, i.e., \textbf{G}ather, \textbf{A}pply, and \textbf{S}catter (\textbf{GAS}). 
The \emph{gather} and \emph{scatter} are a pair of symmetrical operations that demonstrate the information flowing in and out a node and perform edge-wise processing on the corresponding in-edges and out-edges.
The \emph{apply} stage is used to update the state of the node itself.
It enables a node-level paralleling by assigning nodes to partitions, along with their states and adjacent edges. Many graph processing algorithms (e.g., PageRank, SSSP) could be expressed in this paradigm.
The following works\cite{b28,b29,b30,b31} expand the abstraction and propose many talented strategies to optimize graph processing systems such as partitioning, communication, and so on.
%Works\cite{b28} explain the GAS abstraction in an edge-perspective to enjoy edge-level paralleling by assigning edges to partitions, but lead to redundant storage due to replicated vertices associated.
%Others\cite{b29,b31} propose some partition strategies to avoid IO communication, especially random IO access.
%\cite{b30} finds computation is the actual bottleneck of existing systems in the high-performance cluster, and thus proposes to build scalability on top of efficiency.


%The message passing paradigm could be traced back to the graph processing field, which focus on tradition graph mining tasks such as understanding the structural properties or topology of graphs
% traditional graph analysis task

%Compared with GNN, graph processing algorithms focus on tradition graph mining tasks such as understanding the structural properties or topology of graphs, and varies from computing the most important vertices (e.g., PageRank), shortest path between nodes (e.g., SSSP), to communities detection.

%At very early time, it's common to write application-specific programs to perform graph analysis algorithms[].
%To reduce complicated and redundant works through different graph algorithms, researchers and developers propose a series of graph libraries (e.g., BGL, ) and systems (e.g., CGMgraph), which usually provide basic graph abstractions together with a number of algorithms based on them.
%However, those early works usually provide low-level apis and learning curve is sharp, and are limited when scale to large graphs.

%with some libraries like BGL ( boost graph library) to reduce complicated and redundant works through different graph algorithms, which usually provide low-level apis and learning curves is sharp.
%And then, a series of 

%With the rapidly growing scale of graph data, the idea to design graph processing system evolves from optimizing the process on a single machine[] (e.g., BGL, X-Stream) to running distributed with well designed graph-parallel abstraction[] (Pregel, GraphLab, PowerGraph, Gemini). 
%One key graph-parallel abstraction is \textbf{GAS}, which develops the vertex-program proposed by Pregel with three fine-grained conceptual phases to describe the data-flow in graph processing, i.e., \textbf{G}ather, \textbf{A}pply, and \textbf{S}catter. 
%Those three stages can be formed as follows:
%\begin{equation} \label{eq:gas}
%\begin{array}{cl}
%\text{Gather:} & \Sigma \gets \bigoplus_{u \in \mathcal{N}_v} g(D_v, D_{v,u}, Du),\\
%\\
%\text{Apply:} & D_v^{new} \gets a(D_v, \Sigma),\\
%\\
%\text{Scatter:} & {D_{v,u}}_{u \in \mathcal{N}_v}\gets s(D_v^{new}, D_{v,u}, D_u)\\
%\end{array}
%\end{equation}
%where $D_v$, $D_u$, and $D_{v,u}$ are the values (e.g., page rank value in PageRank algorithms) for node $v$, $u$, and edge from $v$ to $u$.
%In the \emph{gather} phase, a certain vertex $v$ would 'gather' the information of its adjacent neighbors and edges though a generalized sum operator $\bigoplus$. 
%Then the result $\Sigma$ is used to update $v$'s state together with its last state.
%And finally, the new state of $v$, $D_v^{new}$, is broadcast to all its neighbors.The \emph{gather} and \emph{scatter} is a pair of symmetrical operation that demonstrates the information flows into and flows out a node.
%Since those works mainly express the computation process from vertex perspective, such abstraction is also known \emph{node-centric} model.
%It enables a node-level paralleling  by assigning vertices to partitions, along with their values and adjacent edges, and many graph analysis algorithms (e.g., PageRank, graph coloring, SSSP) can be expressed in this paradigm.
%Following works[] expend the abstraction and proposed many talented strategies to optimize graph systems such as partitioning[], communication[], and so on.
%[](X-Stream) explain the GAS abstraction in an edge-perspective to enjoy edge-level paralleling by assigning edges to partitions, but lead to redundant storage due to replicated vertices associated.
%[](PowerGraph, GraphChi) proposed some partition strategies to avoid IO communication, especially random IO access.
%[] (gemini) analysis existing system and find computation, rather than communication, is the actual bottleneck of existing systems in high-performance cluster, and thus propose to build scalability on top of efficiency.

%, since vertices should be replicated along with edges.

The key difference between graph processing algorithm and GNN is that the former mainly focuses on graph structure, while the latter usually models graph structure and rich attribute information together and is optimized by gradient-based algorithms (e.g., SGD\cite{b32}).
%is usually conducted over graphs with rich attribute information and optimized by gradient-based algorithms (e.g., SGD\cite{b32}).
%the latter are usually conducted over graphs with rich attributed information and optimized by gradient-based algorithms (e.g., SGD\cite{b32}), while the former are mainly focused on graph structure.
That is, GNNs are more compute-intensive and communication-intensive, and data dependency exists in both forward and backward pass.
%Though the promising direction is to combine the GNNs with graph processing systems, there is still a long way to go.
%Though it's promising to combine the GNNs with graph processing systems, there is still a long way to go.
%Therefore, graph processing system is difficult to perform GNN algorithms in training phase but is promising for inference tasks.

\subsection{Graph Learning System} 
\label{sec:gl_system}
Inspired by the development in deep learning communities, researchers also design and build systems to perform graph learning algorithms based on some deep learning frameworks, such as TensorFlow\cite{b33} and PyTorch\cite{b34}.
Many works\cite{b13,b14,b15,b16,b17} build graph learning systems following the message-passing paradigm to unify different GNN variants, which 
help researchers design new GNN algorithms in an easy way.  

%for making full use of computation resources (CPU, GPU, Memory, and so on)
The development of graph learning systems evolves from optimizing systems on a single machine\cite{b13,b14, b35,b36} to providing an efficient way to learn over large graphs distributedly\cite{b14,b15,b16,b17}.
Inspired by the k-hop sampling strategy\cite{b4,b10}, recent works\cite{b14,b15,b16,b17} perform localized convolutions on sampled k-hop neighborhoods to mitigate the inherent data dependency problem and enjoy the mature data-parallelism architecture in graph learning.
Specially, Aligraph\cite{b16} implements distributed in-memory graph storage engine, and workers will query subgraphs for a batch of training samples and do the training workloads.
DGL\cite{b14} proposes to use a partitioning algorithm when building the graph storage engine to reduce the number of edge cuts between different partitions, which is helpful to reduce the communication overhead when querying subgraphs of training samples.
PyG\cite{b15} provides lightweight, user-friendly sampling APIs, whereas users usually need to prepare an extra graph storage engine for distributed training.
%to run distributedly.
AGL\cite{b17} proves that the k-hop neighborhood could provide sufficient and necessary information for k-layer GNN models and pre-generates the k-hop neighborhoods with a MapReduce pipeline to solve the inherent data dependency problem.
It also provides a way to estimate the errors caused by random sampling strategy, and errors would decrease as the sampling number increases.
Users might trade off computing speed and prediction accuracy by changing the number of neighbors for each layer.
%and propose a full-graph inference module to mitigate the redundant computation problem caused by k-hop neighborhoods, but misses to solve the consistency problem and straggler problems caused by `hub' nodes in industrial scenarios.

However, little attention has been paid to the inference phase of GNNs.
Many works thought that inference tasks could be perfectly conducted in the same way as the training phase, which is hard to meet the requirements of inference tasks in industrial scenarios. The inference should be efficiently conducted over huge graphs with skewed degree distribution, and the prediction result should keep consistent at different runs.
A few existing works\cite{b37,b38} mainly focus on the efficiency problem in the inference phase on a single machine and propose some techniques, such as operation fusion and graph-specific caching, to reduce intermediate results and achieve better IO speed.
%mainly express their idea from operation level on a single machine, such as operation-fusion, graph-specific caching, to reduce intermediate result and achieve better IO speed.
AGL\cite{b17} proposes a full-graph inference module to mitigate the redundant computation problem caused by k-hop neighborhoods but misses to solve the consistency problem and straggler problems caused by `hub' nodes in industrial scenarios.
It is still challenging to build an efficient and scalable inference system for industrial applications.

%The k-hops neighbor-sampling strategy makes it possible to train GNNs over large graphs, and many works take it for granted that inference tasks could be performed perfectly in the way the training phase does.
%Few attention\cite{b37,b38} has been payed to the inference phase of GNNs.
%However, things are much different for inference tasks, especially in industrial scenarios:
%the proportion of labeled data (training data) is usually one percent of the full graph or even less, while the inference usually should be conducted over all the left.
% meanwhile, the graphs in those scenarios would contain as many as tens of billions nodes and hundreds of billions edges.
%Performing inference tasks on k-hop neighborhoods would lead to data explosion in an exponential manner, which causes redundant computation and decreases the throughput.
%What's worse, neighbor-sampling may help reduce the data explosion and avoid OOM problem caused by nodes with large degrees, but would introduce some stochastic factors to k-hop neighborhoods and lead to unstable embeddings and prediction scores.
%It is still challenging to build an efficient and scalable inference system on top of throughput for industrial applications.

%choose to partition a graph and place them in multi-node, but the default partition algorithm is METIS[] which runs on a single machine with a single thread.



%Benefit from the highly developed deep learning framework, such as TensorFlow, PyTorch, researchers design and build systems to perform graph learning algorithms over graphs. 

%Due to the limitations of graph processing systems, researchers design and build systems to perform graph learning algorithms (e.g., GNN algorithms) over graphs, benefit from the highly developed deep learning framework, such as TensorFlow, PyTorch.

%Benefit from the highly developed deep learning framework, such as TensorFlow, PyTorch, researchers design and build systems to perform graph learning algorithms (e.g., GNN algorithms) over graphs. 

%Early works build graph learning systems following GNN paradigm mentioned above to help researchers design new GNN algorithms in an easy way. 
%Meanwhile, they (e.g., early version of DGL and PyG, ROC) also make efforts to make full use of computation resources (CPU, GPU, Memory, and so on) on a single machine to efficiently train a GNN model.
%Some works[seastar, nuegraph] try to design graph learning system in natural graph perspective to reduce users' learning curve and achieve better efficiency.
%However, they can hardly scale to industrial-scale graphs, since those graphs are usually attributed with rich features and can not fit in a single machine.
%As a result, early developers have to build algorithm-specified projects (e.g., Pinsage, PBG) for learning over large graphs, and usually requires high performance machines, which can not be used to implemented general purposed used GNN algorithms.
%For example, PBG is used for unsupervised learning and focus on graph structure, but can not be used in plenty of real-word problems, in which graphs usually attributed with rich features.


%Inspired by the neighbor-sampling [graph sage] strategy, following works[aligraph, DGL, PyG] perform localized convolutions by sampling neighborhood a node to mitigate the inherent data dependency problem in learning over graph data, and enjoy the data-parallelism architecture in training phase that is mature in deep learning communities.
%AGL prove that k-hop neighborhoods provide sufficient and necessary information for k-layer GNN model, and pre-generate the k-hop neighborhoods with a MapReduce pipeline, which helps it gain ability to run over extremely large graphs.
%Aligraph implements distributed in-memory graph storage engine, and workers will query subgraphs related a batch of training nodes and do the training workloads.
%DGL and PyG (latest version) expand their ability to run distributedly.
%% to enjoy good locality,
%DGL chose to partition a graph and place them in multi-node, but the default partition algorithm is METIS which runs on a single machine with a single thread, which is not suitable for graph with billions of nodes and edges in industrial scenarios.
%PyG provide lightweight sampling apis and user usually should prepare an extra graph storage engine to run distributedly.


%Few attention has been payed to the inference phase of GNNs, and many researchers take it for granted that the inferring task could be performed perfectly in the way the training phase does.
%A few existing works[GIN, GNNIE] mainly express their idea from operation level, such as operation-fusion, graph-specific caching, to reduce intermediate result and achieve better IO speed.
%Those works are suitable for some real-world tasks, in which the latency is matters.
%However, a widely existing phenomenon in industrial scenarios is that, graph scale is large and evolving with times, and the proportion of labeled data (training data) is usually one percent of the full graph data or even less while the inference usually should be conducted over all the left.
%The throughput in those scenarios is more critical than the latency.
%However, k-hop neighborhood would lead to data explosion in an exponential manner, which causes redundant computation and decrease the throughput.
%what's worse, neighbor-sampling may help reduce the data explosion and avoid OOM problem caused by large degree of nodes, which is a common technique for training large graphs, but would introduce some stochastic factors to k-hop neighborhoods and lead to unstable embedding and predict score.
%It is challenging and an open problem to build an efficient and scalable inference system on top of throughput for industrial applications.



\section{System}
%In this section, we will first introduce the overview of our inference system, InferTurbo.
%Then we detail the system from three aspects: the abstraction of programing model, implementations with different backends, and optimization strategies for large graphs.
This section will first provide an overview of the InferTurbo inference system. The system is then detailed from three aspects: the programming model abstraction, implementations using various backends, and large-graph optimization techniques.

\subsection{System Overview}
%The key motivation of our work is to build a GNN inference system on top of scalability,  which is desiderated by industrial communities, due to the widely existed phenomenon that the graph scale is very large (e.g., as many as billions nodes and hundreds of billions for edges) while the offline inferring task should be taken over the full graph.
%\textcolor{blue}{It is desiderated by industrial communities, due to a widely existed phenomenon that the graph scale is very large }and contains as many as billions of nodes and hundreds of billions of edges while the inference task should be taken over the full graph.
%we should avoid optimization strategies with randomness  like sampling and ensure the consistency of inference results at different runs, which is the basic requirements of industrial applications.
%keep the consistency at different runs, which is the basic requirements of industrial applications.
%we should keep the results consistent at different runs and avoid inaccurate optimization strategy like sampling, which is the basic requirements of industrial applications.
%instead of designing and optimizing inference system over a single monster machine or 
%Meanwhile, the system should enjoy good properties of consistency and efficiency, which is also basic requirements of industrial applications.

%The key motivation of our work is to build an effective GNN inference system on top of scalability, while ensuring the property of consistency at different runs.
%\textcolor{blue}{Even though a graph may include hundreds of billions of edges and billions of nodes, it is expected that inference work will be done on the whole graph in industrial settings.}
%Therefore, instead of designing and optimizing inference system over a single monster machine, we'd rather give a solution based on mature scalable infrastructures with high throughput capacity like batch processing systems (like MapReduce) and graph processing systems.
%Moreover, some optimization strategies with randomness like sampling should be avoided to ensure the consistency of inference results at different runs, which is the basic requirement of industrial applications.

The key motivation of the work is to build an efficient GNN inference system on top of scalability for huge graphs in industrial scenarios.
Therefore, instead of designing and optimizing inference system over a single monster machine, we'd rather build the system on mature, scalable infrastructures with high throughput capacity, such as batch processing systems and graph processing systems. 
In addition, to boost the inference efficiency, the system is expected to address the following issues: 
serious redundant computation caused by inferring on $k$-hop neighborhoods and the straggler problem brought on by skewed degree distribution.
%the redundant computation problem caused by inferring on $k$-hop neighborhoods and the straggler problem brought on by skewed degree distribution, which are barriers to inference efficiency, should be addressed.
Furthermore, as the consistency of prediction results at different runs is a fundamental requirement for industrial applications, some optimization strategies with randomness should be avoided.
% Furthermore, to maintain the consistency of prediction results at different runs, which is a fundamental requirement for industrial applications, some optimization strategies with randomness should be avoided.

%we should give solution about how to mitigate the redundant computation problem caused by inferring on $k$-hop neighborhoods and straggler problem brought on by skewed degree distribution, which are obstacles for efficiency.

\begin{figure}[htbp]
\centering
\includegraphics[width=\linewidth]{fg1_system_overview_1020}
\caption{System overview.}
\label{fig:system_overview}
\end{figure}

%Moreover, it's better to implement the system on mature industrial infrastructures and common machines, which helps the system gain properties of fault tolerance while would not 
%That is, the scalability is a more important factor than latency. 
%And as a result, we should 
%we should pay more attention on how to boost the throughput rather than 
%Meanwhile, it should satisfy the consistency and efficiency requirements in industrial scenarios.
%That is, we should avoid unstable prediction score and redundant computation caused by techniques used in training phase for scaling to large graphs, such as k-hop neighborhood, neighbor sampling.

To this end, we propose InferTurbo to boost the inference tasks over industrial graphs.
The overall architecture is illustrated in Fig. 1.
First, a GAS-like abstraction is proposed to describe the data flow and computation flow of GNNs by combining the classical message-passing schema in \eqref{eq:mpgnn} and the GAS schema in graph processing system.
This abstraction is utilized to integrate the mini-batch training phase and full-batch inference phase.
In this way, the inference phase would not rely on k-hop neighborhoods and thus avoids redundant computation, while we could still enjoy the mature optimization algorithms in mini-batch manner and the data parallelism in training phase.

%, i.e., \textbf{G}ather (gather\_nbrs, aggregate), \textbf{A}pply (apply node), \textbf{S}catter (scatter\_nbrs, apply edge),

%The \emph{Gather} and \emph{Scatter} stages are conjugate and used to receive (or send) and process the in-coming and out-going messages, while the \emph{Apply} stage is used to update the state information for a certain node (or edge).
%Such abstraction is designed from a node's perspective and used through the entire training and inference pipeline. 
%In training phase, we build GNN models following such schema and design an annotation technique to record the begin and edge point of each stage and layer.
%Then, in inference phase, we could distinct the data and computation flow of the forward pass for GNNs according to the annotations and treat the layer-wise GNN inference process as a "vertex program".
%Therefore, the whole inference task could be conducted over a full large graph and the the "vertex program" could be naturally performed over the entire cluster distributedly. 
%In this way, the inference phase would not relays on k-hop neighbors and thus avoids redundant computation compared with the traditional training-inference pipeline.

In addition, by designing and implementing adaptors, a well-trained GNN model in our abstraction could be deployed on batch processing systems or graph processing systems, which are mature industrial infrastructures with properties of high throughput and scalability.
Applications could choose one of them as the backend by trading off the efficiency and resource cost.

Furthermore, a set of strategies without dropping any information are proposed to handle problems caused by nodes with a large degree, since the degree distribution could be skewed for industrial graphs.
With those strategies, our system could achieve consistent prediction results at different runs and gain better load-balancing by mitigating the stragglers caused by those ``hub'' nodes.
%and a small fraction of the vertices are adjacent to a large fraction of the edges.
%we could achieve consistent prediction results at different runs and avoid the OOM problem and mitigate the stragglers caused by those hub nodes, and thus our system gains a better load-balancing.

%They help avoid the OOM problem and mitigate the stragglers caused by those hub nodes, and thus make our system gain a better load-balancing.




%\textbf{First} we expand the classical message-passing schema of GNNs to a three-stage GAS-like (Gather, Apply, Scatter) parallel-abstraction.
%The \emph{Gather} and \emph{Scatter} stages are conjugate and used to receive (or send) and process the in-coming and out-going messages, while the \emph{Apply} stage is used to update the state information for a certain node according messages.
%Together, we develop an annotation technique to record the begin and edge point of each stage and layer, which helps us describe and distinct the data and computation flow of the forward pass for GNN models clearly.
%As a result, we could apply layer-wise GNN inference to a certain node with only some extra communications between it and its neighbors and the whole inference task could be conducted over a full large graph rather than tiny k-hop neighbors.
%In this way, there's no redundant computation compared with the traditional training-inference pipeline.
%\textbf{Furthermore}, by designing and implementing adaptors, we could directly deploy a well-trained GNN model on top of MapReduce or graph processing systems, which are mature industrial infrastructures with properties of high throughput and scalability.
%Applications could chose one of them as the backend by trading off the efficiency and resource cost.
%\textbf{At last}, we design a set of strategies to handle nodes with high degree, since the degree distribution could be skew for nature industrial graphs and a small
%fraction of the vertices are adjacent to a large fraction of the edges.
%They help avoid the OOM problem and mitigate the stragglers caused by those hub nodes, and thus make our system gain a better load-balancing.

%we could conduct layer-wise GNN inference over a full large graph rather than over tiny k-hop neighbors.
%With such abstraction, we could directly deploy a well-trained GNN model on top of MapReduce or graph processing systems and enjoy the good property of scalability of those systems.
%Moreover, we use a 

%To that end, we first propose a GAS-like parallel-abstraction to describe the data and computation flow of the forward pass for a layer of GNN.
%With such abstraction, we could conduct layer-wise GNN inference over large graphs (just like in a full-batch manner), which helps our system enjoy at least a node-level parallelism, while avoid generating k-hop neighborhood to mitigate data dependent.
%Then, we provide alternative backends of our inferring framework over a graph processing system (quite like Pregel) and a batch processing system (i.e., MapReduce), 
%which are mature industrial infrastructures with properties of high throughput and scalability.
%And applications could chose one of them by trading off the efficiency and resource cost.
%At last, we design a set of strategies to handle nodes with high degree, since the degree distribution could be skew for nature graphs and a small
%fraction of the vertices are adjacent to a large fraction of the edges.
%The overall architecture is illustrated in Fig. 1, and we will present the details in the following.

%implement our inferring framework over a graph processing system (quite like Pregel) and a batch processing system (i.e., MapReduce), which are mature industrial infrastructures that could be deployed on common machines.
\subsection{InferTurbo Abstraction}
%Diving into problems in inference phase for traditional training-inference pipeline, the primary issue is how we distributedly (or parallelly) perform the inference task while the computation of GNN algorithms have inherent data dependency.
%\textcolor{blue}{It's common to conduct the inferring phase just as what the training phase does, but not that suitable for inference tasks in industrial scenarios}, since there are some essential differences between the training and the inference phase for GNN algorithms.
In industrial scenarios, there are some key distinctions between the training and inference phases of GNN algorithms. 
%As a result, the two phases should be carried out differently.
In the training phase, the labeled nodes may be one percent of all nodes or even less, and the optimization procedure would be repeated several times on them to obtain a converged model.
%The labeled nodes in the training phase may be 1\% or even less, and the optimization procedure would be repeated several times to get the model to converge in the right direction.
It is a wise decision to conduct localized GNNs on those labeled nodes based on their k-hop neighborhoods since we can benefit from data parallelism and sophisticated optimization methods in the mini-batch setting.
The cost is also reasonable since labeled nodes would be scattered throughout the whole graph, and there would not be many overlaps between k-hop neighborhoods of corresponding labeled nodes.
%In the training phase, the labeled nodes could be one percent or even less and the optimization process would be performed many times to make the model converge to a good place.
%Performing localized GNNs over those labeled nodes based on their k-hop neighborhoods is a good choice as we could enjoy the data-parallelism and mature optimization algorithms in mini-batch manner.
%Meanwhile, the cost is also acceptable since labeled nodes would be discrete in the whole graph and there may be few overlaps between k-hop neighborhoods towards corresponding labeled nodes. 
On the contrary, inference tasks usually should be conducted on the entire graph.
%the entire graph should often be used for inference tasks.
Forwarding localized GNNs over the k-hop neighbors of all those nodes might result in the redundant computation problem.
%if the inference tasks are carried out in the same way as they are during the training phase.
A good way to solve it is to design a full-batch distributed inference pipeline and bridge the gap between those two different modes in training and inference phases. 

%However, there's no need to do so in inference phase since only one forward pass should be performed over the graph.
%Therefore, it seems a must to perform localized GNN learning over those labeled nodes based on their k-hop neighborhoods to enjoy mature optimization algorithms and data-parallelism.
%On the contrary, the inference tasks usually should be taken over the full graph.
%Keep conducting the inference tasks as what the training phase does, that is, forwarding localized GNNs over k-hop neighborhoods in mini-batch manner, could lead to serious redundant computation problem.
%However, there's no need to do so in inference phase since only one forward pass should be performed over the graph in the inference phase and 
%conducting the inference phase in the mini-batch or the full-batch manner would not leave any differences theoretically.
%Moreover, conducting inference tasks over the full graph would eliminate the redundant computation problem.
%Therefore, we seek to find a way to bridge the gap between those two modes in training and inference phase.

%In training phase over large graphs, optimizing the training tasks in a mini-batch manner is empirical better way than in full-batch manner in deep learning communities[].
%And it seems a must to perform localized GNN learning based on neighbor sampling and k-hop neighborhoods, which is also the most widely used way in various distributed GNN learning systems such as AGL, AliGraph, DistDGL, Euler.
%However, there's no need to do so in inference phase since it only perform the forward pass and 
%conducting the inference phase in the mini-batch or the full graph manner would not leave any differences theoretically.
%What's worse, the techniques used in localized GNN learning could lead to redundant computation and unstable prediction results. 

%learning over k-hop neighborhoods for target nodes to mitigate the data dependency problem, which is also the most widely used way in various distributed GNN learning systems such as AGL, AliGraph, DistDGL, Euler.
%Diving into problems in offline inferring phase for industrial applications, the primary issue is the way we used to distributedly (or parallelly) perform the inferring task while the computation of GNN algorithms have inherent data dependency.
%It's common to conduct the inferring phase just as what the training does, but not that suitable for inferring tasks in industrial scenarios, since there are some essential differences between the training and the inferring phase for GNN algorithms.
%In training phase over large graphs, optimizing the training tasks in a mini-batch manner is empirical better way than in full-batch manner in deep learning communities[]. 
%As a result, it seems a must to perform learning over k-hop neighborhoods for target nodes to mitigate the data dependency problem, which is also the most widely used way in various distributed GNN learning systems such as AGL, AliGraph, DistDGL, Euler.
%However, there's no need to do so in inferring phase: on one hand, the inferring phase only perform the forward pass, and thus, conducting the inferring phase in the mini-batch or the full batch manner would not leave any differences theoretically.
%On the other hand, the overlap of k-hop neighborhoods for different target nodes would lead to redundant computation.

%the overlap of  k-hop neighborhoods for different target nodes would lead to redundant computation. 
%On the other hand, 
\begin{figure}
\centering
\includegraphics[width=\linewidth]{new_gas_abstraction_300dpi.png}
\caption{InferTurbo abstraction}
\label{fig:gas_abstraction}
\end{figure}

\begin{figure*}
\centering
\includegraphics[width=.95\linewidth]{gas_model_pseudocode_0227}
\caption{GraphSAGE, GAT in InferTurbo abstraction}
\label{fig:gas_abstraction_example}
\end{figure*}


%Inspired by the philosophy [pregel] of "think-like-a-vertex" in graph processing field, we expand the classical message-passing schema of GNNs to a GAS-like abstraction to unify the training phase in mini-batch manner and inference phase in full-graph manner.
%In this way, we could still utilize the mature mini-batch training strategy over large graphs while enjoy full-graph inference to avoid the drawbacks such as redundant computation and unstable predicting scores.

%Compared with the message-passing schema, the GAS-like abstraction make it clear to define the different part of a model according to computation flow and data flow. 
%is not only a programing interface, but also a way to distinct the different part of a GNN according to computation flow and data flow. 
%That is, the GAS-like abstraction make it clear to define the different part of a model 
%with which, the inference phase could be performed in the full-batch manner and the computation of a single "vertex" could naturally run distributedly over the entire cluster.

Inspired by the philosophy\cite{b22} of ``think-like-a-vertex" in the graph processing field, we re-express the classical message-passing schema of GNNs to a GAS-like abstraction to unify the mini-batch training and full-graph inference phases.
%GAS-like abstraction to unify the training phase in mini-batch manner and inference phase in full-graph manner.
%In this way, we could still utilize the mature mini-batch training while enjoying full-graph inference to avoid the drawbacks such as redundant computation.
%from a ``vertex" perspective, 
As shown in Fig.~\ref{fig:gas_abstraction}, for a certain GNN layer, the GAS-like abstraction can be defined as follows: 
\begin{itemize}
\item \textbf{Gather.}
	\begin{itemize}
	\item \emph{gather\_nbrs (\underline{Data Flow})}:
	A ``vertex" receives messages from its in-edges and then vectorizes the collected information into tensors. 
	\item \emph{aggregate (\underline{Computation Flow})}: 
	The ``vertex" would preliminarily process the messages from its in-edges, which is quite similar to the reduce function in $\mathcal{R}$ in \eqref{eq:mpgnn}.
	The difference is that this process should obey the commutative law and associative law (like max/min/sum/mean pooling or union) for further optimization in inference phase.
	Otherwise, the computation should be placed in the next stage.
	\end{itemize}
%A "vertex" first receives messages from its in-edges, and then vectorizes the collected information into tensors. 
%At last, it perform the \emph{gather} part of a well trained model with the organized tensors as inputs.
\item \textbf{Apply.}
\begin{itemize}
\item \emph{apply\_node (\underline{Computation Flow})}: The ``vertex" then updates its state information by combing the former state of itself and the ``gathered"  message from the former stage.
\end{itemize}
\item \textbf{Scatter.}
%This stage is a conjugate phase compared with the Gather stage.
	\begin{itemize}
	\item \emph{apply\_edge (\underline{Computation Flow})}:
	The ``vertex" would generate messages according to the updated state information together with edge features.
	\item \emph{scatter\_nbrs (\underline{Data Flow})}:
	The ``vertex" sends messages via out-edges.
	\end{itemize}
\end{itemize}
%It mainly used to generate messages according the updated state of vertex and send messages via out-edges.
%Just like the \emph{Gather} stage, this stage also mainly consists of two parts: computing part according well trained model and message send part to send the processed message to other "vertex" for next layer.
%, and vectorize the collected information into tensors, 

%The GNN paradigm from a "vertex" perspective could be re-expressed as follows:
%That is, we should re-express the GNN paradigm from a "vertex" perspective.
%In this way, the computation of a single "vertex" could 
%Inspired by talented work in graph processing field, we proposed a GAS-like abstraction for the GNN inference phase by  following the philosophy [pregel] of "think-like-a-vertex".
%In this way, the computation of a single "vertex" could naturally run distributedly over the entire cluster.
%we combine the philosophy [Pregel] of "think-like-a-vertex" with the GNN inference task, and 
%we proposed a GAS-like abstraction following the philosophy [pregel] of "think-like-a-vertex", which naturally makes the computation of a single "vertex" distributedly run over the entire cluster.

Those five stages are used to describe the data flow and computation flow in GNNs, and their roles are emphasized by \underline{underlining} and annotating the corresponding part.
Compared with the classical GAS schema, both the \emph{Gather} and \emph{Scatter} are expanded to two sub-stages to distinguish the data flow and computation flow in those stages.
In general, \emph{gather\_nbrs} and \emph{scatter\_nbrs}, a pair of symmetry operations in the data flow, are used to receive and send messages via in-edges or out-edges, respectively.
We make them as built-in methods since they are the same among a variety of commonly used GNNs.

Computation stages would vary for different GNNs, and users should override those stages according to their specific requirements.
Specially, a rule is defined to set a boundary between \emph{aggregate} and \emph{apply\_node} stages: the computation of the \emph{aggregate} should obey the commutative law and associative law. Oherwise, related operations should be placed in the \emph{apply\_node} stage.
For example, many pooling functions (such as sum, mean, max, min), used as the reduce function in GCN and GraphSAGE, should be placed in the \emph{aggregate} stage following the rule.
%the reduce function would be performed
For GAT, the computation of attention would break the rule, and thus, we simply union messages in the \emph{aggregate} stage and perform the reduce function in the \emph{apply\_node} stage.
This rule also facilitates further optimizations, which will be detailed in the next several sections.
%We design such rule for further optimizing in inference phase and will detail it in the next several sections.


%the \emph{Gather} phase is expanded into two stages, \emph{gather\_nbrs} and \emph{aggregate}.

%Meanwhile, the \emph{Scatter} phase is also re-expressed into two partitions, i.e., \emph{apply edge} and \emph{scatter\_nbrs}.
%In general, the \emph{gather\_nbrs} and \emph{scatter\_nbrs} is a pair of symmetry operations in data flow  to receive and send messages via in-edges or out-edges respectively, while the \emph{aggregate} and 
%the \emph{apply edge} are used to aggregate messages for target nodes and generate new messages for the computation flow's perspective.
%The \emph{Apply Node} stage is mainly used to update nodes' state information with aggregated messages and the former state of themselves.
%Note that, the computation stages would vary according different GNNs, and user should override those stages according to .
%On the opposite, we make the data flow stages as built-in method since they are almost the same among many common used GNNs.
%Specially, we use a rule to clearly define the boundary between two computation stages, \emph{aggregate} and \emph{Apply Node} : the computation of aggregate should obey the commutative law and associative law for further optimizing in inference phase.
%Otherwise, the \emph{Apply Node} stage would receive all raw messages and do the aggregate itself.

%We will detail the optimization based on such rule in the following sections.

%Compared with the classical message-passing schema, the definition of three stages in the GAS-like abstraction is according to computation flow and data flow. 
%We emphasize the different roles of those three stages played in the computation flow and data flow by underlining and annotating the corresponding part.
%And we are trying to make it clear to define the different part of a model.
%From the perspective of computation flow, those three stage is just act as processing messages, updating node embeddings with those processed messages, and then generating new messages based on the updated node embedding.
%It's easy to decide the belonging of a part of model according the definition, especially for the last two stages.
%Additionally, we define a rule to distinct the first two stages: if the operations to process messages obey the commutative law and associative law, they should belong to the \emph{Gather} stage, otherwise the \emph{Apply} stage.
%Such strategy is used for further optimizing in inference phase.
%Meanwhile, they also play roles in the data flow. 
%the fan-in and fan-out of a GNN layer is determined by the \emph{Gather} and \emph{Scatter} stages.
%In most case, the \emph{Gather} stage receives messages according in-edges while the \emph{Scatter} stage sends new messages to destination nodes via out-edges.

%We unify the mini-batch training and full-batch inference with such abstraction:
The mini-batch training and full-graph inference are unified with such abstraction:
\subsubsection{Training}
In the training phase, following the traditional training-inference pipeline, the system still takes k-hop neighborhoods as input and train GNNs in the mini-batch manner.
As shown in Fig.~\ref{fig:gas_abstraction_example}, we take two widely used GNN algorithms (i.e., GraphSAGE, GAT) as examples to demonstrate how to organize codes in such abstraction.

%We take two widely used GNN algorithms (i.e., GraphSAGE, GAT) as examples in Fig.~\ref{fig:gas_abstraction_example} to show how to organize code in such abstraction.

The data flow in training phase is quite simple.
Since k-hops neighborhoods provide sufficient information for $k$-layer GNNs\cite{b17} and are locally available for a certain training worker, the data flow is just accessing and updating related local tensors.

Meanwhile, a model doesn't need much change and only should be expressed in our schema.
As shown in Fig.~\ref{fig:gas_abstraction_example}, a certain GNN algorithm should override three methods (\emph{gather}, \emph{apply\_node}, and \emph{apply\_edge}) from the base class.
In addition, we also provide an interface named \emph{scatter\_and\_gather} in case the \emph{scatter} and \emph{gather} stages could be fused together to avoid storing intermediate edge-level messages in training phase\cite{b13}.
For example, the scatter and gather processes in GraphSAGE are fused by a generalized sparse-dense matrix multiplication operation.
It's worth noting that since the \emph{gather\_nbrs} is just accessing local tensors in training phase, it is ignored here for simplicity.
%we ignore such stage for simplicity.
The \emph{gather} interface in Fig.~\ref{fig:gas_abstraction_example} represents the computation of \emph{aggregate} stage.

%In the training phase, following the traditional training-inference pipeline, we still take k-hops neighborhoods as input and enjoy the mature mini-batch training base on them.
%Since they provide sufficient information for $k$-layer GNNs[] and are local available for a certain training worker, the data flow in this case is just accessing and updating related local tensors.
%Meanwhile, the model need not change a lot and only should be expressed in our computation schema.
%In general, a certain GNN algorithm should override three methods (\emph{aggregate}, \emph{apply\_node}, and \emph{apply\_edge}) from the base class.
%In addition, we also provide an interface named \emph{scatter\_and\_gather} in case that the scatter and gather could be fused together to avoid storing intermediate edge-level messages in training phase[].
%For example, in GraphSAGE example, the scatter and gather process is fused by a sparse-dense matrix multiply operation.
%And as mentioned above, we would leave the \emph{aggregate} unimplemented for GAT since the process of aggregate messages from different edges is not obey the the commutative law and associative law.

Furthermore, some function decorators are developed to mark the beginning point and end point of functions, as shown in Fig.~\ref{fig:gas_abstraction_example}.
Meanwhile, we would generate layer-wise signature files to record those information at the time to save a well-trained model  (parameters and so on). 
In this way, different parts of the computation flow could be reorganized and deployed in corresponding stages in the inference phase.
Note that parameters in those decorators indicate whether to enable optimization strategies and also would be recorded in signature files.
Those information would be loaded in the inference phase to avoid excessive manual configurations.
%We load those information to avoid too much hand-crafted configuration in inference phase.
%In this way, for a well-trained model, we could even reorganize the layer-wise computation flow, and deploy them in different stages of inference phase. 
%Note that parameters in those decorators would also be recorded in signature files, and we load those information to avoid too much hand-crafted configuration in inference phase.

\subsubsection{Inference}
Different from the training phase, the inference task is conducted in the full-graph manner.
Since the InferTurbo abstraction is expressed from the perspective of a node, the forward pass of GNNs could be treated as a ``vertex" program over a certain node.
By partitioning nodes in a graph into different machines, the total inference task could be conducted distributedly.
%, and each machine performs the same ``vertex" program.
%On the contrast, in the inference, we conduct the inference task over the full graph rather than perform localized GNN in the mini-batch manner. Fig.~\ref{fig:gas_abstraction} demonstrate how we use the InferTurbo abstraction to express a layer of GNNs from a node's perspective.
%the forward pass of GNNs from a vertex's perspective.

%The data flow could be quite different.
The data flow in the inference phase is quite different from that in the training phase.
Neighbors of a certain node could be placed on different machines, and data should be exchanged among those machines to prepare sufficient information to perform the computation flow on the node.
Therefore, rather than simply accessing local tensors, the data flow in inference phase mainly plays a role in communicating with other nodes on remote machines:
the \emph{gather\_nbrs} would receive information from remote nodes via in-edges and vectorize those information into adjacency matrix,  node/edge feature matrix, and so on.
The \emph{scatter\_nbrs} would send messages to other machines according to the destination node's id for the next iteration.

In contrast, the computation flow could be shared in training and inference phases.
In general, a certain computation stage of a well-trained model would be attached to the corresponding part in inference phase.
%we only attach a certain computation stage to the corresponding stage of data flow.
Specially, the computation flow would be reorganized for optimization.
%for the aim of optimization, we may reorganize the computation flow.
For example, the \emph{aggregate} function may be performed in the \emph{Scatter} stage in advance to reduce messages sent to the same destination node.

The implementation details about how to conduct the inference tasks with specific backends and the optimization strategies will be presented in the following sections.
%We will detail the implement details in the inference phase in the following section.


%In general, the computation could be shared in both the training and inference, while the data flow could be quite different. 
%As shown in Fig.~\ref{fig:gas_abstraction}, we treat the inference task as a "vertex" program, and the same "vertex" program could be placed on machines over an entire cluster to process different nodes.
%In this way, the inference task would be naturally conducted distributedly.
%However, neighbors of a certain node could be placed on different and data should be exchanged among those machines to prepare sufficient information to perform the computation flow on the node.
%As a result, the data flow could be quite different due to different local available information in those two phases:
%the \emph{Gather} stage is responsible to receive messages from neighbors and vectorizes those information into adjacency matrix,  node/edge feature matrix and so on.
%Meanwhile, the \emph{Scatter} would send messages to other machines according to destination node id.
%Since we record the different computation module in signature files when save a well trained model, it's easy to attach those module to correspond data flow module to run the "vertex" program.

%In this way, with such abstraction, we could conduct layer-wise inference over the full graph without any redundant computation caused by the overlap of k-hop neighborhood in training phase, while enjoy a node-level parallelism by deploying the inference task distributedly from a "vertex" perspective.
%We will detail the implement details in the inference phase in the following section.


%The computation could be shared in both the training and inference phase, but the data flow could be quite different due to different local available information in those two phases.
%In general, the computation could be shared in both the training and inference, while the data flow could be quite different. since we conduct the inference task over the full graph rather than perform localized GNN on k-hop neighborhoods, and thus the local available information  could be


%The computation flow could share between the training and inference phase while the data flow part is different, due to the different local available information in different phase.
%In the training phase, we could still enjoy the mature mini-batch training base on k-hop neighborhoods or neighbor sampling.
%Since all information to perform k-hop GNN learning are local available, the data flow in this case is accessing and updating related tensors.
%Therefore, the model in training phase need not change a lot and we only express GNNs in the GAS abstraction.
%We take two common used GNN algorithms (i.e., GraphSAGE, GAT) as examples in Fig.~\ref{fig:gas_abstraction_example} to show how to organize code in such abstraction.

%On the contrast, the data flow could be different in our inference phase.
%We conduct the inference task from a "vertex" perspective and demonstrate how a "vertex" performs a layer of GNNs in Fig.[].
%The same vertex program would be placed on machines over a cluster to process different vertexes.
%In this way, we would naturally perform the inference task distributedly.
%Owing to this, neighbors of a certain node could be placed on different machines, and data should be exchanged among those machines to prepare sufficient information to perform the computation flow on the node.
%The data flow here is different from that in training phase: the \emph{Gather} stage is responsible to receive messages from neighbors and vectorizes those information into adjacency matrix,  node/edge feature matrix and so on.
%Meanwhile, the \emph{Scatter} would send messages to other machines according to destination node id.

%At the same time, the data should be exchanged among different machines, since neighbors of a certain node could be placed on different machines.
%As a result, the data flow part should be modified based on the backend.
%The basic 

%However, communicating with nodes on different machines may leads to some overhead.
%For example, on a certain machine, two nodes may have a same neighbor, and they would send messages to this node individually, which could be merged in one in advance to reduce the num of messages.
%Moreover, a nodes may send its state information as messages to all neighbors which could be compressed to one, as messages are the same.

%It's worth noting that due to the difference in data flow, though we share the computation flow between training and inference phase, we should record the boundary of different stages and treat and conduct them as individual modules with associated part of the data flow.
%Moreover, for further optimizing, we even reorganize the computation flow based on those three modules.
%To that end, we develop function decorators to mark the begin point and end point of a function as shown in Fig.~\ref{fig:gas_abstraction_example}.
%Meanwhile, we would generate layer-wise signature files to record those information.
%Note that the parameter in those decorators indicates the optimization strategy used in corresponding stage, which also would be recorded in signature files.
%In this way, we could load those information to avoid too much hand-crafted configuration in inferring phase.
%And we could reorganize the computation according to related optimization strategies, and we will introduce our built-in optimization strategies based on this tool in the next several sections.
 
%Therefore, for further optimizing, we should record the boundary of different stages and treat them as individual modules and may reorganize the computation flow based on those modules.
%and may change the computation flow based on those three stages.
%To that end, we develop several techniques to help mark the boundary and modify the computation graph of well trained models:
%\begin{itemize}
%\item \textbf{Annotation.} 
%As shown in Fig.~\ref{fig:gas_abstraction_example}, we use function decorators to mark the begin point and end point of a function.
%Meanwhile, we would generate layer-wise signature files to record those information.
%In this way, we could load those information to avoid too much hand-crafted configuration in inferring phase.
%\item \textbf{Computation Graph Reorganization.}
%We provide an exporter to rebuild the computation graph at the end of training phase.
%The default version is just as the one in training phase, and user could modify it for specific optimization strategies in inference phase such as add some extra operations in some stage by reusing variables existed in training phase. 
%We will introduce our built-in optimization strategies based on this tool in the next several sections.
%\end{itemize}


   
%%the same vertex program would be placed on machines over a cluster.
%%Fig.[] indicates how a "vertex" performs a layer of GNNs.
%%On the contrast, we conduct the inference task from a "vertex" perspective, and apply the GAS schema to nodes to 
%
%%we could still enjoy the mature mini-batch training to perform localized GNN base on k-hop neighborhoods or neighbor sampling.
%%Since the inf
%
%
%
%%Those stages are designed by counting the data flow and computation flow of GNN algorithms in the inferring phase.
%%From the perspective of data flow, the fan-in and fan-out are determined by the \emph{Gather} and \emph{Scatter} stages, while from the perspective of computation flow, those two stages also plays a role in compute a GNN model.
%%Besides receiving input messages, the $\emph{Gather}$ stage also acts as the aggregator for GNN algorithm by forwarding the related part of a well trained GNN model.
%%As for the $\emph{Scatter}$ stage, inspired by the combiners in Pregel[], it also plays an extra role to perform partially aggregate phase of a certain GNN algorithm with local available messages, which could reduce the overhead of sending messages to vertex on other machines. The detail for partial-combine will be presented in the following sections.
%
%The vertex program in Fig 1, indicates how a "vertex" performs a layer of GNN, and the same vertex program is also placed on other machines over the cluster to process other nodes.
%By conducting the "vertex" program $k$ times, we finally get the prediction of a $k$ layer GNN over the full graph.
%The computation and communication of "vertex" program between two iteration (superstep) is synchronized in the Bulk Synchronous manner to ensure the proper messages and states between different GNN layer.
%%while the $\emph{Scatter} $ stage would perform a partial-gather referring to the \emph{Comber} process to reduce 
%%The $\emph{gather}$ plays a data pre-process role by receiving and vectorizing message from the perspective of data flow, on the other hand, it also acts as the aggregator for GNN algorithm by forward the related part of a well trained GNN model.
%
%
%%We design those three stages by counting the data flow and computation flow of GNN algorithms in inferring phase.
%%Those three stages indicates the data flow and computation flow of GNN in inferring phase. 
%%As a result, they all have double duty in 
%
%\subsubsection{Modification of Computation Graph}
%\label{sec_modifiy_cg}
%%Note that, the computation graph in our inferring phase is different from that in traditional training phase as mentioned in Equ.\ref{eq:mpgnn}: (1) the inferring computation graph may add some extra operations in \emph{scatter} stage; (2) a $k$-layer GNN is split
%%Note that, the computing of the inferring phase is quite different from that in traditional training phase as mentioned in Equ.\ref{eq:mpgnn}: (1), we may add some extra operations in inferring phase, such as the partial-combine in \emph{Scatter} stage, for optimizing the inferring phase in such graph-parallel manner; (2), 
%The inferring phase in such graph-parallel manner is quite different from the traditional training phase that based on $k$-hop neighborhoods and neighbor-sampling.
%First, due to the difference of the available information for performing GNN algorithms,
%the computation of a $k$-layer GNN should be conducted with at least $k$ superstep in our inferring phase where each superstep perform a layer of GNN, while it could be finished in one step in training phase.
%Second, the computation graph in a "vertex" program may be different from that in training phase.
%For example, in \emph{Scatter} stage, we may add some extra operation to perform partial-combine to reduce the overhead of sending messages.GraphSAGE
%To make our inferring framework as a plug-in replacement for that in traditional training-inferring pipeline, we proposed some strategies to mitigate the gap of computation graph between the training phase and the inferring phase:
%
%\textbf{Re-express GNN in GAS Manner.} For GNN algorithms, we re-express the classical message passing paradigm in a GAS-like manner.
%Take two most usually used GNN algorithms (i.e., GraphSAGE, GAT) as examples, we demonstrates how to implement GNNs in GAS-like apis in Fig.1.
%Generally, the \emph{\_gather} function mainly receive structure information (i.e., adj) and feature information (i.e., node feature and edge feature) as input, and then output the aggregated the node feature and edge feature via structure information for central nodes.
%The \emph{\_apply} then update central nodes state with the aggregated information and the state information.
%It's worth noting that the \emph{\_scatter} function usually should not be implemented, since a memory block is used to store and sharing the state of central nodes among different GNN layers in training phase.
%%Take two most usually used GNN algorithms (i.e., GraphSAGE, gat) as examples, 
%
%\textbf{Annotation.} As described above, we conduct the inference over the full graph by forward a well trained model in a layer-wise manner.
%And for a certain "vertex program", we even forward a slice of model at function-level.
%Therefore, the begin and end of a slice computation graph should be specified in inferring phase.
%We use a function decorator to inspect special operations with predefined names (like, "layer\_1\_gather\_input\_adj") to annotate the begin and end of a certain layer or function.
%Moreover, we use a signature file to record such information to avoid two much hand-crafted configuration in inferring phase.
%
%\textbf{Replacement.} The computation graph for inferring phase could be different from that in training phase.
%Some optimization strategies specially designed for inferring phase may change the computation graph by reusing variables existed in training phase and adding some extra operations.
%For example, the partial-combine in \emph{Scatter} stage may be added in the inferring phase to reduce the number of messages.
%Note that, optimization strategies are not enabled by default since those strategies may not consistently works well for all GNNs, especially for user defined GNN algorithms.
%Therefore, we provide a special exporter to rebuild the computation graph in training phase, and with a predefined flag (just like learning\_phase in Tensorflow), user could modify the computation as what they want.
%
%%We use an optional flag to distinct the training and inferring phase
%%Specially, we may change the computation graph in inferring phase for optimizing, for example, adding some extra operation to perform partial-combine.
%%We also use a flag to distinct the computation graph in training and 
%%Specially, the computation graph for inferring phase could be different from that in training phase
%%forward a well trained GNN model 
%
%
%%the available information for performing GNN algorithms is different. 
%%In training phase, the computation of a GNN model could be finished in one training step since the $k$-hop neighborhoods provide sufficient information $k$-layer GNN algorithm.
%%to make our inferring framework a plugin-in replacement for traditional training-inferring pipeline, 



\subsection{Alternative Backends and Implementation Details}
%some applications may be time-sensitive and inference tasks for them should be finished as fast as possible, even with relative high cost on expensive exclusive resource (like memory).
%, since graphs in industrial applications could be quite large while inference tasks usually should be taken over the entire graphs.
Scalability is the key property that should be taken into consideration when implementing the InferTurbo in the inference phase.
Except for scalability, different industrial applications also have some specific requirements.
Inference tasks for some time-sensitive applications should be finished as fast as possible, even with relatively high costs on expensive and exclusive resources (like memory).
Others may be cost-sensitive and seek a way to conduct the inference over large graphs with limited commodity computation resources since the resource cost is also critical in industrial scenarios.

%We provide alternative backends of InferTurbo for applications to make a trade off between efficiency and resource cost: the one on the graph processing system (i.e., a Pregel-like system) and the one on the batch processing system (i.e., MapReduce, Spark).
InferTurbo provides two alternative backends on graph processing systems (i.e., Pregel-like systems) and batch processing systems (i.e., MapReduce, Spark) to trade off computation efficiency and resource cost.
In general, InferTurbo on the graph processing system could be more efficient than that on batch processing system but with more strict requirements on stable and exclusive resources, such as, memory, CPU cores.
In contrast, InferTurbo on batch processing systems is more flexible in memory and CPU requirements as it processes data from external storage.
For example, the batch processing system could handle large graphs even with a single reducer if sufficient external storage is available.
%On the opposite,  InferTurbo on the batch processing system which processes data from external storage like HDD, is more flexible on the requirements of memory and cpus.
%For example, with enough external storage, even with one reducer, the batch processing system could also process large graphs.
%For applications on extremely large graphs but with limited resources, the latter is preferred, while tasks with enough exclusive resource and with relative high restrict on inference time are more suitable for the former one.

The implement details of those two backends are introduced respectively as follows:
%on the graph processing system (i.e., a Pregel-like system) and the batch processing system (i.e., MapReduce, Spark) for large large-scale GNN inference tasks.

%One key property that should be taken into consideration when implement the InferTurbo in inference phase is scalability, since graphs in industrial applications could be quite large while inference tasks usually should be taken over the entire graphs.

%We provide alternative backends for InferTurbo on the graph processing system (i.e., a Pregel-like system) and the batch processing system (i.e., MapReduce, Spark) for large large-scale GNN inference tasks.
%Both those two mature backends could provide good system properties such as scalability and fault-tolerance, and they also have some specific properties and are suitable for different scenarios.
%In general, InferTurbo on graph processing system could be more efficient than that on batch processing system but with more strict requirements on stable and exclusive resource, such as, memory, cpu cores.
%On the opposite,  InferTurbo on batch processing system which processes data from external storage like HDD, is more flexible on the requirements of memory and cpus.
%For example, with enough external storage, even with one reducer, the batch processing system could also process large scale data.
%Note that, resource cost is also critical in industrial scenarios.
%Those two backends provide alternative choice to trade off between efficiency and resource cost:
%%and what's more, on a certain reduce stage, one reducer could release its resource once the task on it is finished other than the whole job is finished. 
%For applications on extremely large graphs but with limited resources, the latter is preferred, while for tasks with enough exclusive resource and with relative high restrict on inferring time is more suitable for the former one.


%we will introduce the implement details of them respectively.

%Moreover, in industrial scenarios, there would be many different graph inference tasks everyday.
%The resource utilization is also critical. 

%We deploy our InferTurbo on the graph processing system (i.e., a Pregel-like system) and the batch processing system (i.e., MapReduce) to conduct large-scale GNN offline inferring.

%We apply our InferTurbo abstraction on both the graph processing system (i.e., a Pregel-like system) and the batch processing system (i.e., MapReduce) to conduct large-scale GNN offline inferring.

%Both those two mature backends could provide good system properties such as scalability and fault-tolerance, and they also have some specific properties and are suitable for different scenarios.
%In general, InferTurbo on graph processing system could be more efficient than that on batch processing system but with more strict requirements on stable and exclusive resource, such as, memory, cpu cores.
%On the opposite,  InferTurbo on batch processing system which processes data from external storage like HDD, is more flexible on the requirements of memory and cpus, and what's more, on a certain reduce stage, one reducer could release its resource once the task on it is finished other than the whole job is finished. 
%As a result, for applications on extremely large graphs but with limited resources, the latter is preferred, while for tasks with enough exclusive resource and with relative high restrict on inferring time is more suitable for the former one.


%Both those two systems could be deployed on cluster of thousands of commodity machines and provide excellent parallelism ability to handle large-scale graphs in industrial scenarios.
%In general, InferTurbo on graph processing system could be more efficient than that on batch processing system but with more strict requirements on stable and exclusive resource (e.g., memory).
%That is, it will exclusively occupy the resource until the task is finished.
%On the opposite,  InferTurbo on batch processing system is more 



\begin{figure}
\centering
\includegraphics[width=0.9\linewidth]{pregel_implement_300dpi.png}
\caption{The backend of a Pregel-like graph processing system}
\label{fig:backend_of_pregel}
\end{figure}

\subsubsection{InferTurbo on Graph Processing System}
We implement the InferTurbo abstraction on a Pregel-like graph processing system, which is one of the most popular distributed graph processing systems.
Note that it could be easily migrated to other graph processing systems as the abstraction originates from the graph processing field.

%In the inference phase, the InferTurbo abstraction could be easily migrated to running on graph processing systems as it originates from the graph processing field.
%We implement our InferTurbo abstraction on a representative Pregel-like graph processing system, which is one of the most popular distributed graph processing system.

%Compared with traditional graph processing tasks like PageRank, there are some differences when migrated the inference tasks of GNNs on graph processing systems.
%First, the inference of GNNs is more communication-intensive.
%In industrial scenarios, graphs usually attributed with rich feature information and messages that should be exchanged between different nodes (may be on different machines) could be quite larger than that in traditional graph processing tasks.
%Moreover, the computation of GNNs is also different.
%We should attach different modules of a well trained model to the pipeline of graph processing systems.
%By counting those differences, the details of InferTurbo on graph processing system could be stated as follows:

\textbf{Graph Partition.} Following Pregel\cite{b22}, a graph is divided into partitions according to node ids by a partitioning function (like, mod N), and each partition contains a set of nodes and all out-edges of those nodes.
Each partition is assigned to a worker to perform the ``vertex'' program (i.e., a layer of GNN).
As shown in Fig.~\ref{fig:backend_of_pregel}, a certain node would also maintain the node state and edge state (out-edges), such as raw features, intermediate embeddings, or even historical embeddings.
In this way, structure information and feature information could be stored in one place to avoid redundant storage.

It's worth mentioning that, with this partitioning strategy, the forward pass for a layer of GNNs over a certain node could be finished in one superstep by receiving messages from other nodes (maybe on other machines), which avoids synchronizing between different workers within a superstep.

%That's why we don't choose other funcy 
%synchronizing between different wokers within a superstep.

%and each is assigned to a worker to perform the "vertex program".
%Each partition contains a set of nodes and all those nodes' outgoing edges.
%
%Each partition contains the features of source nodes and edges but not the destination's.
%In this way, features for a certain node could be stored only once and the total node features would not expanded to edges.


%By receiving messages from other nodes (may be on other machines),  a certain node could provide sufficient information to forward a layer of GNNs and then generate new messages and send out via out-edges.
%Note that, the inference task of GNN models could be more communication-intensive compared with traditional graph processing tasks like PageRank:
%in industrial scenarios, graphs usually attributed with rich feature information and messages exchanged between different nodes (may be on different machines) could be quite larger than that in traditional graph processing tasks.
%With such partition strategy, a layer of GNN for a certain node could be finished in one superstep, which is helpful to avoid communication and synchronizing between different instance within a superstep.

%In this way, a layer of GNN for a certain node could be finished in one superstep to avoid communication and synchronizing between different instance within a superstep.

%since GNNs output embeddings by counting both the structure and ri
%messages on edges, which usually should be exchanged between different instances, could be quite larger than traditional graph processing tasks.
%
% since it usually is conducted over graphs with rich features and messages on edges, which should be exchanged between different machines, could be quite larger than traditional graph processing tasks.
%
%GNN inferring tasks is more communication-intensive since GNNs usually are conducted over graphs with rich features and messages on edges, which should be exchanged between different machines, could be quite larger than traditional graph processing tasks.
%Moreover, the computation of GNNs is also different, and we should attach different modules of a well trained model to the pipeline of graph processing systems.
%%they usually conducted over graphs with rich attributed, and the message between different superstep could also 
%Therefore, there are al

%Therefore, the InferTurbo abstraction is designed from a node's perspective, and a layer of GNN for a certain node should be finished in one superstep to avoid communication and synchronizing between different instance within a superstep.
%We implement our InferTurbo abstraction on a Pregel-like graph processing system, which is one of the most popular distributed graph processing system with the philosophy of "think-like-a-vertex".
%Note that our framework is not binding with Pregel, it can be easily migrated to other node-centric graph processing system.
%In the following, we take the Pregel-like graph processing system as an example, to demonstrate how to conduct GNN inferring tasks over the graph processing system.

%We implement our InferTurbo abstraction on a Pregel-like graph processing system, which is one of the most popular distributed graph processing system.
%Note that our framework is designed from the node perspective, and a layer of GNN for a certain node is finished in one superstep to avoid communication and synchronizing between different instance within a superstep.
%can be easily migrated to other node-centric graph processing system 
%(\textbf{should verfy wether the concept of "job", "task" used here is right} )

% data preparation and available resource for a certain vertex program

%Following the Pregel, a graph is divided into partitions according node ids by a certain partition function (like, mod N), and each is assigned to an instance to perform the "vertex program".
%Compared with classical graph processing tasks, a vertex instance would maintain more state information: node and its features, out-edges and their features, model file and layer-wised signatures.

%The whole inference on Pregel of a $k$-layer GNN consists of three stage: initial, 
%%The vertex instance in the first superstep would transfer the raw features for node and edges by forwarding a a slice of well trained model and then the vertex program would send those information as messages via out-edges.
\textbf{Execution.} 
As shown in Fig.~\ref{fig:backend_of_pregel}, we mainly perform the computation flow of GNN models in the \emph{compute()} function of the Pregel-like system. 

At first, we implement the \emph{gather} stage based on the \emph{message iterator} in Pregel, which is used to collect all messages sent to a certain node.
Meanwhile, we provide a built-in \emph{vectorization} function to transfer the messages and local state information into tensors, including structure information (i.e., adjacent matrix), node-level information (i.e., node state), and edge-level information (i.e., edge-wise messages or state).
Then, we perform the computation flow based on those matrices and provide functions to update node and edge states maintained in the machine.
At last, the messages generated by the model would be sent to other nodes via the \emph{send\_message} function in Pregel.
Specially, we design an optimization strategy for performing the aggregation part of the GNN model in the combiner stage of Pregel to reduce the communication cost, as shown in Fig.~\ref{fig:backend_of_pregel}.
%Specially, we also design some strategies to merge messages in advance to reduce the communication cost, that is, we would perform the \emph{aggregate} part of the GNN model in the \emph{combiner} stage of Pregel, which is also demonstrated in Fig.~\ref{fig:backend_of_pregel}.

%design to merge messages in advance to reduce the communication cost, and perf

%we use the \emph{message iterator} Pregel to collect all messages for a cer

%After partitioning the graph, we could start the pipeline to perform the inference task.
%Generally, the forward pass of a layer of model would be finished within one superstep.
%At the begin of $i$-th superstep, we first load the $i$-th model together with corresponding signature files.
%Then the overall pipeline to conduct a layer of GNN in $i$-th superstep could be expressed as follows:
%\begin{itemize}
%\item \emph{Gather.} This stage is mainly used to receive, vectorize, and pre-process messages.
%We use the message iterator in Pregel to collect the messages from in edges, and provide a built-in \emph{vectorization} function to transfer the messages and local state information into tensors, include structural information (i.e., adjacent matrix), node-level information (i.e., node state), and edge-level information (i.e., edge-wise messages or state).
%And then perform the \emph{aggregate} slice of model (\emph{gather} function in Fig.~\ref{fig:gas_abstraction_example}) with corresponding signature files.
%\item \emph{Apply.} Node state would be updated in this stage. We take a part of the vectorized matrixes and the output tensors of the \emph{aggregate} stage as input according to signature files to update node states by forwarding the computation flow of \emph{apply\_node} in the model. 
%\item \emph{Scatter.} New messages would be generated according to the updated node state the \emph{apply\_edge} fucntion of the model.
%And then we the built-in send\_message function to send those messages via out-edges.
%\end{itemize}

After $k$ iterations (supersteps), we would finally get the result of a $k$-layer GNN model.
Note that the first and the last supersteps play special roles in the full pipeline.
The \emph{first} superstep could be treat as an initialization step since there are no messages received at this stage.
It mainly transforms the raw node states and edge features into initial embeddings and then calls the \emph{Scatter} stage to send those information to other nodes to start the following supersteps.
For the \emph{last} superstep, we attach a prediction part after the \emph{apply\_node} stage to get prediction scores for nodes (if needed) and then output the results (node embeddings or scores).

% and the first and the last supersteps play special roles in the full pipeline:
%The \emph{first} superstep could be treat as the initialization step since there are no messages received at this stage.
%It is mainly used to transform the raw node state and edge features into initial embeddings and then calls the \emph{Scatter} stage to send those information to other nodes to start the following supersteps.
%For the \emph{last} superstep, we attach a prediction part after the \emph{Apply} stage to get prediction scores for nodes (if need) and then output the result (node embeddings or scores) rather than call the \emph{Scatter} stage.



%It's worth noting that, the first superstep is a little different since there are no messages at this stage.
%It could be treat as the initialization step and is mainly used to transform the raw node state and edge features into initial embeddings.
%Then it calls the \emph{Scatter} stage to send those information to other nodes to start the following supersteps.
%The last superstep also plays a special role in the full pipeline
%
%It mainly used to transform the raw node state and edge features into initial embeddings and then conduct the \emph{Scatter} stage to send those information to other nodes to 


%The overall pipeline to conduct a $k$-layer GNN inferring task is as follows: 
%(1), initialization. in the first superstep, raw features for node and edges are transformed to initial embeddings by forward a slice of well trained model and then the vertex program would send those information as messages via out-edges.
%(2), the each layer of the following $k$ supersteps would forward the associated $k$-th layer of GNN model.
%And for a certain vertex instance in this stage, it would perform the vertex program following the GAS-like abstraction mentioned above:
%Specially, for \emph{Gather} phase, we use the message iterator in Pregel to collect the messages from in edges, and provide a \emph{vectorization} function to transfer the messages to tensors like adj, node features matrix and edge feature matrix for model. 
%Then, the vertex program would invoke to forward associated model slice with those inputs according to signature files and superstep.
%For \emph{Scatter} phase, in general, we use the built-in send\_message function to send messages via out-edges.
%And for some GNN algorithms (like GraphSAGE), whose aggregate operation satisfies the commutative law and associative law, we perform partial-gather to reduce the messages.
%after $k$ times iteration, we finally get all the embeddings of nodes.
%(3), prediction. the last superstep is optional and is used to perform a predict layer to map embeddings to predict scores.

%Note that, the initialization stage could be performed together with the superstep with the first layer of GNN model, while the prediction stage could be merged to the superstep with the last layer of GNN model.
%In this way, we could finish forwarding a $k$-layer GNN model in $k$ superstep over the full graph.

% commutative law， associative law
%$k$-times iteration with associated model slices. 
\begin{figure}
\centering
\includegraphics[width=\linewidth]{mapreduce_backend_300dpi.png}
\caption{The backend of MapReduce}
\label{fig:backend_of_mr}
\end{figure}


\subsubsection{InferTurbo on MapReduce}
We also provide an alternative backend on MapReduce (or Spark), as shown in Fig.~\ref{fig:backend_of_mr}.
%In industrial scenarios, it's common that many applications would leverage the ability of the powerful GNNs on various specific graphs.
%The solution based on graph processing system is strict on stable and exclusive resource and may lead to resource competition for different applications.
%Therefore, for applications on extremely large graphs but with limited resources, we also provide an alternative solution on MapReduce.
%For applications on extremely large graphs but with limited resources, 

Compared with the backend on the graph processing system, the MapReduce backend could not maintain some information (e.g., node states, out-edges) in memory.
Owing to this, the communication between different ``supersteps" could be quite different: all necessary information should be treated as messages and sent to the next round.

The MapReduce pipeline takes a \emph{node table} and an \emph{edge table} as input.
The \emph{node table}  consists of node id, node features, and ids of all out-edge neighbors, while the \emph{edge table} contains source node id, destination node id, and edge features.  
The overall pipeline on MapReduce is as follows:
\begin{itemize}
\item \emph{Map.}
%This phase is just like partitioning graph in the backend on graph processing system and is mainly used to assign nodes to reducers according to their IDs.
The phase acts as the initialization step.
At first, it transforms the raw features of nodes and edges to initial embeddings.
Then, for a certain mapper, the initial embeddings of a certain node would be sent to itself and all its out-edge neighbors, while the edge information would be sent to both the source and the destination nodes.
In short, the \emph{Map} phase generates three kinds of information for each node: self-state information, in-edge information (i.e., edge features of in-edges, node features of the in-edge neighbors), and out-edge information (i.e., edge features of out-edges).

%For a certain node, we collect three kinds of information for it: the self-state information, the in-edges information (i.e., edge features of in-edges, node features of the in-edge neighbors), and the out-edges information (i.e., edge features of out-edges).
%In addition, the stage also acts as the initialization step to transform the raw features of nodes and edges to initial embeddings.
%After that, it will output the collected information with the form that the node id is treated as the \emph{shuffle key} while those information is set as the \emph{value} field. 
\item \emph{Reduce.}
The reduce phase performs the forward pass for a certain GNN layer.
Compared with the backend on graph processing system, the input and output in this phase are a little different.
%For a certain node, in \emph{Gather} stage, we should not only receive in-edge messages but also the node itself's state,
%while in \emph{Scatter} stage, we also output the updated node state as extra messages to itself.
In the \emph{Gather} stage, a node needs to receive edge messages and its own state. However, in the \emph{Scatter} stage, the node also sends the updated state as an additional message to itself.
That is, messages (e.g., node state, updated edge embeddings via out-edges) would be sent not only to destination nodes but also to itself.
The shuffle keys for those messages are set as ids of destination nodes or itself, respectively.
Messages for destination nodes represent in-edge information in the next round, while for itself, indicate new self-state information and out-edge information.
%(1) messages for its destination nodes, like node state (or embeddings), updated edge embeddings (or edge-wise messages based on them). 
%The shuffle keys for this kind of information are destination nodes' ids and for destination nodes in the next round, those information represents the in-edges information.
%(2) messages for itself, like node state, updated edge embeddings.
%And in this case, the shuffle key is the node id while those information represents self-state information and out-edges information for itself in the next round. 
\end{itemize}

Similar to the implementation on Pregel, the prediction slice of a well-trained model will be merged to the last reduce phase, and after $k$ times \emph{Reduce} stages, we could finish the forward propagation of a $k$-layer GNN. 

Though the number of messages could be larger than that in the backends of graph processing system, %there are some properties in this backend that make it promising for industrial applications: messages and state information usually are stored and exchanged with external storages (HHD or SSD).
this backend is promising for industrial applications since messages and state information usually are stored and exchanged with external storage (HHD or SSD).
As a result, a reducer could load a part of nodes together with their information into memory instead of loading all the data in the partition.
The peak usage of memory could be far less than that in the backends of graph processing system, as the latter backend usually should maintain all necessary information belonging to it in memory.
Therefore, it could be more suitable for scaling to extremely large graphs.
Moreover, many applications would leverage the ability of the powerful GNNs on various specific graphs in industrial scenarios.
The solution based on graph processing system is strict on stable and exclusive resources and may lead to resource competition for different applications.
The implementation on the batch processing system could release the heavy resource competition by sacrificing a little efficiency.  

%Different from the implementation on graph processing system, for a certain vertex program, some information like states, out-edges, could not be consistently maintained in memory during the overall pipeline of inferring a $k$-layer GNN, since a certain reducer or mapper would exit once it finishes its task.
%Moreover, messages will be stored and exchanged via external storage.
%Those differences could be a two-edge sword:
%on one hand, memory is not the main limitation any more and it could scale to extremely large graphs.
%On the other hand, state information should be passed to next stage as extra messages and the communication is more critical.
%By counting the differences, the overall pipeline on MapReduce is as follows:

%\begin{itemize}
%\item \emph{Map.}
%This stage mainly plays a role of initialization and prepare information for GNN iteration. 
%It would first transfer raw features to initial embeddings.
%And then for a certain node, it would output three kinds of information: state information (i.e., node embeddings), out-edges and their feature information, in-edges and their feature information.
%The node id is set as "shuffle key" and other information are as values.
%\item \emph{Reduce.}
%For a certain reducer in $k$-th reduce stage, it will first load model file and signature associated with the $k$-th layer.
%Then in \emph{Gather} stage, just as introduce above, we use \emph{vectorization} function to vectorize the receive information as adj, node feature matrix, and edge feature matrix, and forward the model with those input matrixes.
%The difference mainly happends in \emph{Scatter} phase: the state information for a certain node such as intermediate embedding, should also be passed as extra messages.
%Moreover, for destination node we want to send message to, we should treat the node id as shuffle key while the message should be wrapped as value (using flexible types like protocol buffers).
%\end{itemize}

%such difference makes it more flexible on the requirements of memory and cpus, and it could scale to extremely large graphs.
%Therefore, more messages will be generated and used between two stage (i.e., two round of reduce).

\begin{figure*}[htbp]
\centering
\includegraphics[width=\linewidth]{optimization_strategies_1019}
\caption{Optimization strategies}
\label{fig:optimization}
\end{figure*}

\subsection{Optimization Strategies}
\label{sec_opt_stategies}
It is still challenging to conduct GNN inference tasks for industrial applications on those implementations since a hallmark property of graphs is \emph{skewed} power-law degree distribution\cite{b17,b19}.
The power law means a few nodes act as hubs and have many neighbors, while most nodes have few neighbors.
Those ``hub" nodes could increase the cost of computation and communication in inference phase, and substantially slow down the machines that contain them.
What's worse, the inference pipeline could crash as those nodes may lead to the OOM problem.
The neighbor-sampling strategy\cite{b4} may mitigate those problems but lead to unstable embeddings or prediction scores, which is unacceptable for industrial applications. 

By diving into those problems, we classify them into two categories: problems caused by the skewed in-degree and the skewed out-degree, and propose several strategies to address them respectively:
 
%We propose several strategies to solve those problem.
%We propose several stra

%However, in industrial scenarios, a hallmark property of graphs is \emph{skewed} power-law degree distribution[], which may lead to some intractable problems.
%The power-law means a few nodes act as hubs and have many neighbors while most nodes have few neighbors.
%Those "hub" nodes could increase the cost of computation and communication for their "vertex" program in inference phase, and lead to substantially slow node.
%There's no doubt that such long tail would leave bad effect on total time cost and becomes the bottleneck of the overall pipeline.
%What's worse, the pipeline could crash as they may lead to OOM (out of memory) problem.

%Those problems could by divided into two classes due to different reasons: the skewed in-degree and the skewed out-degree.
%As we stressed above, compared with traditional graph processing tasks, the GNN inference task is more compute-intensive and communication-intensive.
%For a layer of GNNs, the complexity of the computation mainly depends on in-edges, since the GNN aggregate information via in-edges.
%Meanwhile the communication cost mainly relays on out-edges, as messages like intermediate embeddings would be sent to destination nodes via out-edges.
%Both the unbalanced computation and communication could lead to the problems mentioned above.
%Though the final phenomena are same, the way to solve those problems could be different.
%Based on such insight, we propose a set of strategies to eliminate those problems individually.

%%the skewed in-degree and out-degree would lead to diffe
%
%%To address those problems caused by skewed graphs, we proposed some strategies from two aspects: strategies for skewed in-degree and out-degree 
%%we propose a set of strategies to eliminate those problems.
%%Specially, we 
%%However, communicating with nodes on different machines may leads to some intractable problems.
%%Firstly, there may be some overhead in exchanging information among different machines in the inference phase.
%%For example, on a certain machine, two nodes may have a same neighbor, and they would send messages to this node individually, which could be merged in one in advance to reduce the num of messages.
%%Moreover, a nodes may send its state information as messages to all neighbors which could be compressed to one, as messages are the same.
%%Those overhead could lead to bad efficiency for inference tasks.
%%Secondly, 
%
%As we stressed above, compared with traditional graph processing tasks, the GNN inference task is more compute-intensive and communication-intensive.
%For a layer of GNNs, the complexity of the computation mainly depends on in-edges, since the GNN aggregate information via in-edges.
%Meanwhile the communication cost mainly relays on out-edges, as messages like intermediate embeddings would be sent to destination nodes via out-edges.
% Therefore, the complexity of computation and communication are highly related with the degree of nodes.
% However, in industrial scenarios, a hallmark property of graphs is \emph{skewed} power-law degree distribution, which means a few nodes act as hubs and have many neighbors while most nodes have few neighbors.
% Those "hub" nodes could increase the cost of computation and communication for their vertex program, and lead to substantially slow node.
% There's no doubt that such long tail would leave bad effect on total time cost and becomes the bottleneck of the overall pipeline.
% What's worse, the pipeline could crash as they may lead to OOM (out of memory) problem. 
%To address those problems caused by skewed graphs, we propose a set of strategies to eliminate long tails in both time cost and memory utlization.
\paragraph{\textbf{Partial-Gather}}
For nodes with many in-edges, the time cost of receiving and computing over messages could increase significantly and lead workers containing them in the long tail.
We propose a \emph{partial-gather} strategy to address this problem, and the schematic diagram is illustrated in Fig.~\ref{fig:optimization}.

The basic idea of this strategy is to conduct the \emph{Gather} stage in advance to balance the computation of messages on the sender side while reducing the total amount of messages to be sent out. 
In addition, the five-stage GAS-like abstraction and the annotation technique described above make it possible to call 
related computation modules at any time in the pipeline.
%Therefore, for a certain round, we would conduct the \emph{aggregate} part of model (i.e., gather function, partial=True) with partially available information when messages are sent to a same destination.
Therefore, in a certain round, the \emph{aggregate} part of the model (i.e., gather function, partial=True) could be conducted using partially available messages when they are sent to the same destination.
Then in the next round, the receiver receives those aggregated messages from different workers and conducts the \emph{Gather} stage as usual.
Since the \emph{aggregate} function satisfies the commutative law and associative law, this strategy would not lead to any difference in final results.
We implement the strategy on both the Pregel-like system and the MapReduce (Spark) system with their built-in ``combining" function.

%With such strategy, the complexity of communication for a certain node is reduced to constant-level and only related to the number of workers.
%Furthermore, the computation of \emph{Gather} is nearly uniformly conducted in many source instances, which also helps avoid hubs of computation and could mitigate the long tail effect caused by nodes with large in-degree.
%Note that, there is nearly no overhead involved with this strategy.
%Therefore, it could be used for all nodes regardless of the degree of nodes.
%\textcolor{blue}{
With this strategy, the communication complexity for a particular node is reduced to a constant level and only depends on the number of workers.
Additionally, the computation of \emph{Gather} is carried out relatively uniformly across source workers, which also avoids the computation hubs and may reduce the long-tail effect caused by large in-degree nodes.
This strategy involves almost no overhead and can be applied to all nodes regardless of their degrees.
%Keep in mind that this method involves almost no overhead.
%As a result, it could be applied to all nodes, regardless of their degree.
 %which also aids in avoiding computation hubs and may assist to lessen the long tail effect brought on by nodes with large in-degree.
%}


%Since we annotate and record the begin and the end point of each computation stages in a well trained model, we could call the related module at any time in the pipeline when we want.


%For nodes with many in-edges, the computation would increase significantly and the time-cost for those nodes would in the long tail.
%Moreover, receiving messages along with those in-edges and processing and vectorizing them in memory may lead to the OOM problem.
%To solve those problems, we propose a \emph{partial-gather} strategy and illustrate the basic idea in Fig.~\ref{fig:optimization}.
%We thought to release the computation pressure in advance: for a certain machine, we would conduct the \emph{Gather} part in sender side with partially available information, when messages are sent to a same destination.
%In this way, partial-gather would relatively evenly be performed on different senders (machines), and messages would be merged in advance.
%Then in the next round, the receiver would receive those aggregated messages from different instance, and conduct the \emph{Gather} stage as usual.
%Since the \emph{Gather} stage in our definition satisfy the commutative law and associative law, such strategy would lead to no difference in final results.


%For nodes with many in-edges, the complexity of computation would increase a lot and the time cost for those nodes would in the long tail.
%Moreover, load the messages from those in-edges could lead to OOM problem.
%To address those problem, we propose the \emph{partial-gather} strategy.
%Inspired by the \emph{combiner} mechanism, the strategy is to conduct \emph{gather} stage in sender side with partially available information in advance, when messages are sent to a same destination node.
%In this way, partial-gather would relatively evenly be performed on different senders, and reduce the number of messages in some degree.
%Since the gather stage (or say aggregate phase) in many GNN algorithms satisfy the commutative law and associative law, such strategy could be naturally suitable for those GNN algorithms.
%We take the GraphSAGE as an example and illustrate the workflow of \emph{partial-gather} in Fig.2:
%\begin{itemize}
%\item \emph{Revise Computation Graph.} Before exporting model file at the end of training phase, we use the model modification techniques proposed in section \ref{sec_modifiy_cg} to revise the computation graph.
%In general, we only add some extra operations in \emph{Scatter} stage. For example, calling \emph{\_gather} function in \emph{Scatter}.
%Then we export such revised model together with corresponding signature files. 
%\item \emph{Conducting.} For a certain instance at a certain round, at the stage of \emph{Scatter}, we first collect messages grouped by destination nodes, and then conduct the \emph{\_gather} function to partially aggregate messages to destination nodes in advance. At last, those messages with a same destination node would be merged to one message and send to its destination.
%For the next round, the destination instance would receive those aggregated messages from different instance, and conduct the \emph{Gather} stage as usual.
%\end{itemize}

%This strategy is proposed to mitigate problems cased by nodes with large in-degree. 
%while that of communication depends on out-edges.

\paragraph{\textbf{Broadcast}} 
%messages or at least a part of messages
For many GNN algorithms, all or part of messages sent via out-edges are usually the same. 
For example, the intermediate embeddings of a certain source node could be sent to all its neighbors via out-edges, which leads to overhead in communication.
For nodes with many out-edges, the communication would be the bottleneck, which should be avoided by ``compressing" those repeated messages.

To address this problem, we propose a \emph{broadcast} strategy as shown in Fig.~\ref{fig:optimization}.
In detail, instead of sending edge-level messages, nodes will send ``one" unique message to each machine together with a unique identifier (e.g., UUID or the source node id) as the index.
Meanwhile, identifiers would be sent as messages via out-edges.
Then, in the next round, the receiver should look up real messages by those identifiers and then perform the inference tasks as usual.
We implement this strategy with the built-in ``aggregator" class in batch processing systems and graph processing systems.
Furthermore, this strategy could be easily migrated on the latest graph processing systems which provide a native ``broadcast" mechanism.
%Furthermore, for latest graph processing systems which provide a native ``broadcast" mechanism, such strategy could be easily migrated on them.

It's obvious that the \emph{broadcast} could mitigate the overhead caused by the repeated messages and thus release the heavy communication pressure for nodes with large out-degree.
%Different from the above, for nodes with many out-edges, the communication would be the bottleneck and lead the time cost of those nodes in the long tail.
%However, for many GNN algorithms, messages or at least part of messages sent via out-edges are usually the same. 
%For example, the intermediate embeddings of a certain source node could be sent to all its neighbors via out-edges, which leads to overhead in communication.
%Therefore, we should "compress" those repeated information to reduce the throughput in communication.
%To that end, we propose the \emph{Broadcast} strategy to solve this problem and illustrate the schematic diagram in Fig.~\ref{fig:optimization}.
%In detail, instead of sending edge-level messages, for the repeated part of messages, we send a mirror to each machine together with a unique identifier (e.g., uuid or just the original source node id) as index.
%Meanwhile, we send the identifier as the message by out-edges.
%Then, in the receiver side in next round, it should first look up the real message by the identifier.

%It's obvious that, the length of the identifier is controllable and could be far less than real messages (like embeddings) for GNN algorithms.
%In fact, we don't reduce the number of messages, but reduce the total length of messages.
%As a result, the communication stress could be released to some degree with such strategy.

%the total number of messages is not reduced, but the total length of messages does.
%we don't reduce the number of messages, but reduce the total length of messages.

%we seek to send the repeated part of messages to other machines with each one message.


%For nodes with many out-edges, the communication would be the bottleneck and lead those node in the long tail.
%For many GNN algorithms, the messages sent via out-edges usually is the same (e.g., the intermediate embeddings of source node), which leads to overhead in communication.
%Therefore, we propose the Broadcast strategy to solve such problem.
%As shown in Figure 2, we first send the message to a memory store (like key-value store) or a shared file system (like OSS), which is available for all other instance. 
%Note the message is indexed by UUID or just the original source node id.
%Then we just send the id as message via out-edges.
%For the message receivers, we first parse and unique those ids and then access the shared memory or file with those uniqued ids to get real messages.
%It's obvious that the length of id is controllable and could be far less than real messages (like embeddings).


\paragraph{\textbf{Shadow Nodes}} 
%We also design a strategy named \emph{shadow nodes} as 
A strategy named \emph{shadow nodes} is designed as an alternative solution for nodes with large out-degree, since the \emph{broadcast} strategy may not work well when messages vary with out-edges and cannot be compressed.
%when messages could not be compressed when they vary with out-edges.

%We illustrate the strategy in Fig.~\ref{fig:optimization} and the basic idea is to average the communication load caused by out-edges:
%(1), for a certain node with many out-edges, we first split the out-edges evenly into $n$ groups.
The basic idea of this strategy is to average the communication load caused by out-edges, as shown in Fig.~\ref{fig:optimization}.
First, the out-edges of a node are divided into $n$ groups evenly.
Then the node is duplicated $n$ times, and each mirror is associated with a group of out-edges and all in-edges. 
Note that the id of each mirror would be appended with the group information.
This process would be conducted in the preprocessing phase, and out-edges will be averaged to each mirror which could be placed on different machines.
Since each mirror holds all the in-edges of the original node and the out-edges of all the duplicates are equal to that of the original node, conducting GNN algorithms with this strategy is just the same as the original pipeline and would not change the result.
This strategy is simple but more general than the \emph{Broadcast} strategy.
It could re-balance the computation load whether messages could be compressed or not. 

%\textcolor{blue}{
Note that, in those two strategies for large out-degree problems, a heuristic formula is designed to estimate the threshold: $threshold = \lambda \times total\_edges/total\_workers$.
%To balance the costs of overhead against their advantages, thresholds should be utilized.
%Note that, both those two strategies for large out-degree problem involves some overhead.
%Therefore, thresholds should be used to make a trade-off between overhead and their benefits.
% where $\lambada$ is a hyper-parameter and is set to 0.1 in our experiment.
That is, those strategies should be activated when the number of out-edges for a node exceeds a certain percentage of edges on the worker.
In our experiment, the hyper-parameter $\lambda$ is set to an empirical value of 0.1.
%}

%It's a pity that, messages or a part of them (e.g., edge features) could not be compressed when they vary according out-edges and the \emph{Broadcast} may not work well in this case.
%Therefore, we propose a more general strategy, \emph{Shadow Nodes}, as an alternative solution for nodes with large out-degree.
%The basic idea is to average the communication load as shown in Fig.~\ref{fig:optimization}:
%1). For a certain node with many out-edges, we first split the out-edges evenly into $n$ groups.
%2). Then the node is duplicated $n$ times and each mirror is associated to a group of out-edges and the full in-edges. 
%Note that, the id of each mirror would be appended with the group information.

%Such process would be conducted in the preprocess phase, and out-edges would be averaged to every mirror which could be placed on different machines.
%Since every mirror holds all the in-edges of the original node and the out-edges of all the duplicates equal to that of original node, conducting GNN algorithms with such strategy is just equal to the original pipeline and would not change the result.

%Though the \emph{Shadow Nodes} strategy introduces some overhead by duplicating in-edges, we think it is a worthy trading-off: for hub nodes with large out-degree, the major issue is the time cost to send messages would be quite skew, and make them as stragglers.
%The \emph{Shadow Nodes} strategy reaches its mission to solve such problem and could balance the computation load on different machines.

%Though the \emph{Shadow Nodes} strategy introduces some overhead by duplicating in-edges, 
%The \emph{Shadow Nodes} is designed to balance the communication load caused by hub nodes with large out-degree 
%with each associated to a group of out-edges, and the id of each duplicated node would be appended with the group information.
%Moreover, we also propose a more general strategy for nodes with large out-degree, since the \emph{Broadcast} may not work well when messages varies according out-edges.
%The basic idea is to average the communication load.
%For a certain node with many out-edges, we first split the out-edges evenly into $n$ groups.
%Then the node is duplicated $n$ times with each associated to a group of out-edges, and the id of each duplicated node would be appended with the group information.
%Meanwhile, the original in-edges for this node would also be duplicated and pointed to all duplicates of the node.
%Such process would be conducted in the preprocess phase, and the communication load would be averaged to every duplicate of the node which could be placed on different machines.
%Since every duplicate holds all the in-edges of the original node and the out-edges of all the duplicates equal to that of original node, conducting GNN algorithms with such strategy is just equal to the original pipeline.
%Though it leads to some overhead by duplicating in-edges, it's obvious that such strategy is helpful to average the communication load and eliminate stragglers caused by nodes with many out-edges.


\section{Experiment}
In this section, we conduct extensive experiments to evaluate our inference system and compare it with two state-of-the-art GNN systems, PyG\cite{b15} and DGL\cite{b13}.
\subsection{Experiment Settings}
\label{sec_exp_settings}
\textbf{Datasets.} 
%(\textbf{O}pen \textbf{G}raph \textbf{B}enchmark)
%We use four datasets in our experiments, 
Four datasets are used in the experiments, including PPI\cite{b39}, OGB-Products\cite{b40}, OGB-MAG240M\cite{b41}, and Power-Law, whose data scales range from small, medium, large, to extremely large.
%We use three datasets in our experiments, including PPI[], OGB-MAG240M[], Power-Law, whose data scale ranges from small, medium, to large scale.

The properties of those datasets are shown in Tab.~\ref{tab:datasets}.
The first three datasets are collected from the real world, and following the experimental settings in \cite{b3,b4,b17,b42}, they are divided into three parts as the training, validation, and test sets.
Note that settings of MAG240M follow the baseline examples provided by OGB team\footnote{https://ogb.stanford.edu/}, and only a part of the graph is used in the examples, and thus we count what we used in Tab.~\ref{tab:datasets}.

\begin{table}%[h]
\centering
	\caption{Summary of datasets}
	\label{tab:datasets}
	\begin{tabular}{c|cccc}
		\toprule
		%\multirow{2.5}{*}{Method} & \multicolumn{2}{c}{Data 1} & \multicolumn{2}{c}{Data 2}  \\
		Indices & PPI &Product & MAG240M & Power-Law \\ 
		%\cmidrule(lr){2-3} \cmidrule(lr){4-5}
		%& \(\mathbf{X}\) & \(\mathbf{Y}\) & \(\mathbf{X}\) & \(\mathbf{Y}\) \\
		\midrule
		\#Node   &56944  &2,449,029 &$1.2\times10^8$ & $1\times10^{10}$ \\
		\#Edge  &818716 &61,859,140 & $2.6\times10^9$ & $1\times10^{11}$ \\
		\#NodeFeature &50 &100 & 768 & 200 \\
		\#Class &121 &47 & 153 & 2 \\
		\bottomrule
	\end{tabular}
\end{table}

%The Power-Law is widely existed in industrial graphs, and we synthesize the Power-Law dataset in consideration that:
%The Power-Law dataset is synthesized following the power-law[] in consideration that: (1) 
The Power-Law dataset is synthesized following the power-law\cite{b19} for the following two considerations.
Firstly, to verify the scalability of the system, graphs at different scales with the same degree distribution should be used in experiments, since the time cost could be affected a lot by different degree distributions even with the same scale.
Secondly, experiments for analyzing large in-degree and out-degree problems should be conducted on datasets with in-degree and out-degree following the power law respectively, for variable-controlling purposes. 
%to verify the property of scalability, we should test our system with graph data at different scales with the same degree distribution, since the time cost could be affected a lot by different degree distributions even with the same scale.
%By synthesizing them with the power-law, we could generate datasets at different scale while conducting inference tasks on them could indicate the result on industrial graphs.
%(2), For some experiments to verify the effectiveness of optimization strategies individually, we should generate datasets with in-degree and out-degree following the power-law respectively for variable controlling purpose.
%to demonstrate the trend of resource utilization and time cost, 
In our experiment, we synthesize datasets following the same rule with different scales and Tab.~\ref{tab:datasets} only records the largest one we used.
Specially, all nodes in Power-Law datasets are used in inference task, while millesimal are used in training phase.
%with same rule, and conduct inference tasks on them to simulate the results on industrial graphs.

%We could synthesize datasets with different scale with same rule that widely existed in industrial graphs to verify the property of scalability. (2) We could generate datasets with in-degree and out-degree following the power-law respectively to test the strategies for large in-degree and out-degree individually for variable controlling purpose.
%Therefore, In our experiment, we synthesize several datasets following the same rule with different scale to demonstrate the trend of resource utilization, and Table~\ref{tab:datasets} only records the largest dataset we used.
%Specially, all the nodes in Power-Law dataset are used in inference task while only about one percent are used in training phase, since the performance of our inference system over such extremely large dataset is more concerned.
%Since we cares the performance of our inference system over large dataset, the full Power-Law dataset is used in inference task, and only nearly one hundred percent is used in training phase. 
%In addition, In our experiment, we also synthesize several datasets following the same rule with different scale to demonstrate the trend of resource utilization, Table~\ref{tab:datasets} only records the largest dataset we used.


\textbf{Evaluation.}
We design a series of experiments to evaluate our system in terms of effectiveness, efficiency, consistency, and scalability.

We first evaluate two widely used GNNs (GraphSAGE\cite{b4}, GAT\cite{b3}) on three real-world datasets to verify the effectiveness of our system.
%, since the inference is quite different with previous works.
Then, some experiments are conducted to show the unstable prediction scores caused by neighbor-sampling and prove the consistency of our inference system.
Moreover, we also record the time cost and compare it with traditional inference pipelines to see the efficiency of our system.
%Note that, in those experiments, we compare our system with two state-of-the-art graph learning systems, i.e., PyG\cite{b15} and DGL\cite{b13,b14}.

%Note that, the benchmarks we used on those two real world dataset are from previous articles or PyG, which is one of the most famous graph learning systems.
%Since the inference mode of other systems are quite like that of PyG, that is forwarding a well trained model as what the training does, we just take the PyG as an example to see the difference between the our system and the traditional pipeline.

We verify the scalability of the system and analyze the effectiveness of optimization strategies on the Power-Law dataset.
We use \emph{cpu*min} to measure the usage of resources and record it at different data scales to prove the scalability of the system.
The data scales range from 100 million nodes (about 1 billion edges), 1 billion nodes (10 billion edges), to 10 billion nodes (100 billion edges). %Moreover, we also design a series of experiments to evaluate the effectiveness of our optimization strategies.
%As mentioned above, for variable-controlling purpose, power-law datasets  here are a little different from above.
Specially, when analyzing the effectiveness of different optimization strategies individually,  power-law datasets are generated  with in-degree and out-degree following the power-law, respectively. 
%We generate datasets with in-degree and out-degree following the power-law to verify different strategies, respectively. % for large in-degree and out-degree individually.

%We deploy the differ system on different clusters. 
The different backends of our system are deployed on different clusters. 
The backend on graph processing system consists of about 1000 instances, and each is powered by 2 CPU (Intel Xeon E5-2682 v4@2.50GHz) and 10GB memory, while the one on MapReduce contains about 5000 instances  (2-CPU, 2GB memory).
The network bandwidth is about 20 Gb/s.
For fairness, only 1000 instances are used in experiments to compare those two backends.
%when comparing those two backends, we set the number of instance to 1000.


%We prove the scalability of our system mainly based on the Power-Law dataset. 
%We use \emph{cpu*min} to measure the used resource and test the resource utilization with different data scales to verify the scalability, which ranges from 100 million nodes (about 1 billion edges), 1 billion nodes (100 billion edges), to 10 billions nodes (100 billion edges).
%We also design a series of experimental experiments to verity the effectiveness of our optimization strategies and the good load-balancing we achieved over the power-law datasets.
%Note the methods of generating power-law datasets here is slightly different from that in [] for variable-controlling.
%We generate datasets with in-degree and out-degree following the power-law respectively to test the strategies for large in-degree and out-degree individually.



\subsection{Results and Analysis}
\label{sec_results_and_analysis}
\subsubsection{Comparisons}
We conduct a set of experiments to compare the system with traditional training-inference pipelines and present experimental results to demonstrate the effectiveness, efficiency, consistency, and scalability of the system.
%We present a series of experimental results to make comparisons between our system with traditional training-inference pipeline.

\begin{table}%[htbp]
	\centering
	\caption{Performance}
	\label{tab:performance}
	\begin{tabular}{ccccc}
		\toprule
		%\multirow{2.5}{*}{Method} & \multicolumn{2}{c}{Data 1} & \multicolumn{2}{c}{Data 2}  \\
		 \multicolumn{2}{c}{Indices} & PyG &DGL &Ours  \\ 
		%\cmidrule(lr){2-3} \cmidrule(lr){4-5}
		%& \(\mathbf{X}\) & \(\mathbf{Y}\) & \(\mathbf{X}\) & \(\mathbf{Y}\) \\
		\midrule
		\multirow{3}{*}{SAGE} & PPI & 0.878 &0.878 & 0.880  \\
		\cmidrule(lr){2-2}
						&Product &0.787 &0.790 &0.788   \\
		\cmidrule(lr){2-2}
						& MAG240M & 0.662 &0.664 &0.668  \\
		\cmidrule(lr){1-5}				   
		\multirow{3}{*}{GAT}  & PPI & 0.987 &0.981 & 0.986  \\
		\cmidrule(lr){2-2}
						   &Product &0.794 &0.800 &0.801  \\
		\cmidrule(lr){2-2}
						   & MAG240M & 0.663 &0.659 & 0.670  \\
		\bottomrule
	\end{tabular}
\end{table}

\textbf{Effectiveness.} 
% when measuring a system.
Effectiveness is always in the first place. Table~\ref{tab:performance} illustrates comparisons of performance between our system and traditional training-inference pipelines powered by PyG and DGL.
We report the results of GraphSAGE and GAT on PPI, OGB-Product, and OGB-MAG240M in different inference pipelines.
Configurations of those GNN algorithms follow examples in OGB leaderboard\footnote{https://ogb.stanford.edu/docs/lsc/leaderboards/}.

In general, results achieved by our system are comparable with those on PyG and DGL across different datasets and algorithms.
It's reasonable since our system just changes the way to perform the inference but never changes the formula of GNNs or introduces any approximation strategy in inference phase.
As those two algorithms represent the two most widely used algorithms while the scale of datasets ranges from small, medium, and large, those comparisons and results prove the effectiveness of our system in different scenarios.


%The comparison is from two aspects: the one using the whole training-inferring pipeline of our system, and the one by loading parameters from a well-trained model based on PyG and then inferring with our inference system.
%The sampling strategy in traditional pipeline is full-sampling, which means every neighbor for a root node would be reserved.
%
%The results with our training-inferring pipeline is comparable with that in traditional pipeline, while by loading parameters from PyG, our inference system achieve the same results.
%Such results are just in the line with expectations, since our system just changes the way to perform the inference and never changes the formulas of GNNs or introduces any randomness in inference phase.
%In all, those results and comparisons prove the effectiveness of our inferring system.



%The datasets we used in our experiments is shown in Fig.XX. 

\textbf{Efficiency.}
%In theory, our system would be more efficient in terms of time and resource cost, since it eliminates the redundant computation that exists in traditional inference pipeline caused by k-hops neighborhoods.
%To verify the efficiency of our system, we evaluate our system 
We evaluate the efficiency of our system by comparing it with the traditional inference pipelines powered by PyG and DGL, and record the resource cost measured by \emph{cpu*min} together with time cost on the OGB-MAG240M dataset.
%We evaluate our system by comparing with the traditional inference pipeline powered by PyG and DGL,
%and record the resource cost measured by \emph{cpu * min} together with time cost for over the OGB-MAG240M dataset to verify the efficiency of our system.
Note that, in traditional pipelines, we use a distributed graph store (20 workers) to maintain the graph data and 200 workers (10-CPU, 10G memory) for inference tasks.
Increasing the number of workers would exacerbate the communication problem between workers and the graph store, reducing the efficiency of traditional pipelines.
%and thus lead to degeneration of efficiency for traditional pipeline.
For fairness, the total CPU cores of inference workers are equal to our system with different backends, and the CPU utilization remains 90\%+ during the inference tasks.

%for the sake of fairness, when conducting experiments on traditional pipelines, we 

%for traditional pipelines, we use a distributed graph store (20 workers) to maintain the graph data, and 200 workers (10-cpu, 10G memory) for inference tasks.
%to ensure the total cpu cores is the same with that in our system with different backends.
% 
%We present the resource cost measured by \emph{cpu * min} together with time cost to verify the efficiency of our system with different backends.
%We also deploy a traditional inferring pipeline base on PyG as baselines.
%Note that, for traditional pipeline, we use a distributed graph store (20 workers) to maintain the graph data, and apply 200 workers for training and inference (10-cpu, 10G memory) to ensure the total cpu cores is the same with that in our system with different backends.

Table~\ref{tab:efficiency} demonstrates the experimental results compared with traditional inference pipelines.
Generally,  both the two backends of our system demonstrate superior efficiency.
Our system achieves 30$\times$ $\sim$ 50$\times$ speedup compared with traditional inference pipelines, while in terms of the total resource cost, traditional inference pipelines take about 40$\times$ $\sim$ 50$\times$ more than our system.
Those results show that our system is quite promising for industrial scenarios: it not only could finish inference tasks faster but also is more economy friendly for industrial applications as it takes fewer resources.
The results we achieved mainly benefit from the new inference pattern in our system.
That is, with the InferTurbo abstraction, it could conduct layer-wise inference over the entire graph while enjoying the good property of parallelism.
In this way, redundant computation caused by $k$-hop neighborhoods could be avoided.
%we could avoid redundant computation caused by $k$-hop neighborhoods used in traditional pipelines.
Notably, even ignoring the bottleneck of communication in traditional pipelines and assuming they could achieve $5\times$ speedup by scaling up $5\times$ resource (i.e., 1000 workers), our system still gains up to $10\times$ speedup over them.

In addition, we also design a set of experiments to demonstrate the relationship between time cost and the number of GNN layers for further analysis, and results are presented in Tab.~\ref{tab:time_cost_hops}. 
Without loss of generality, the comparisons are mainly conducted between PyG (traditional pipeline) and the MapReduce backend.
\emph{nbr50} and \emph{nbr10000} mean the number of neighbors for a certain node is limited to 50 and 10000 respectively with neighbor-sampling strategy for experiments on PyG, while there is no sampling for our system.
Both the time cost and resource usage of our system increase nearly linearly by varying hops of GNNs, while they increase exponentially in the traditional pipeline.
%while those in the traditional pipeline show an exponential relationship with hops.
Specially, the traditional pipeline even crashes with the OOM problem when the number of neighbors is set to 10000.
%The main reason for such phenomenon is that, in the traditional pipeline, for each node, the forward pass for a certain GNN model would be conducted over k-hops neighborhoods for it, which could be expanded in an exponential manner with the increasing of hops, and thus leads to exponential increasing of communication and computation.
Because, in the forward pass of the traditional pipeline, the k-hop neighborhoods of nodes increase exponentially with the hop count, resulting in the exponential growth of communication and computation.
However, k-hops neighborhoods for different target nodes could overlap with each other, and the communication and computation on the overlaps are unnecessary.
Our system avoids those redundant problem by conducting inference tasks over the entire graph in the full-batch manner.
For each node on each layer, it will be used only once in our system.
Therefore, the time cost and resources are only related to the hop count.
%Once again those results prove the superior efficiency of our system over traditional pipelines.

%As for our system, each layer of GNN is taken place on the entire graph 
%Furthermore, the results here not reach the limits of our system (especially the backend on graph processing system), since the machines in the cluster are commodity and could not be exclusively used by our task, and other tasks running on the clusters may lead to some resource competition (like network bandwidth) 
%Specially, we also deploy the backends of the graph processing system on a small exclusive cluster with 4 high performance machines (each contains 400G memory and 96 cpu cores) and achieve $2\times$ $\sim$ $5\times$ more speedup compared with the current results of our system in Table~\ref{tab:efficiency}.
 
%the cluster is for heterogeneous workloads
%even could not be exclusively used by our task and other tasks running on the clusters may lead to some resource competition (like network bandwidth) and slow our task, 
%the results here not reach the limits of our systems (especially the backend on graph processing system), since we deploy our system on commodity machines and the cluster even could not be exclusively used by one tasks and other 




%Table~\ref{tab:efficiency} demonstrates the the results we achieved compared with the traditional inference pipeline.
%With same cpu cores, our system achieve 1.8$\times$ $\sim$ 2.9$\times$ speedup compared with traditional pipeline on both GraphSAGE and GAT.
%By counting the extra resource for graph store, the traditional pipeline usually takes resource (\emph{cpu * min}) by 3$\times$ more than that of our system.
%We think the results we achieved mainly benefit from the new inference pattern in our system, that is we use an GAS-like graph parallel abstraction to perform layer-wise inference on the full graph.
%In this way, we enjoy good property of parallelism while avoid redundant computation caused by $k$-hop neighborhoods used in traditional pipeline.  
%
%Furthermore, an interesting phenomenon is that the time cost of the Pregel backend is usually less than that of the MapReduce backend, while the cpu resource utilization is about the same level.
%We think it's reasonable, because the computation on those two backend is the same, but the communication time is different:
%In MapReduce backend, instance would not maintain state information consistently in memory and exchange it to next stage via external storage. 
%As a result, a certain instance in MapReduce could release the computation resource it applied once the task on it is finished, but the communication time could be longer.
%Therefore, the MapReduce backend takes the same cpu resource but leads to more time cost compared with the graph processing backend.
%The difference also make those two backends suit for different application by trading off efficiency and resource:
%for applications on extremely large graphs but with limited resources, the MapReduce backend is the first choice, while for tasks with enough exclusive resource and with relative high restrict on inferring time is more suitable for the graph processing backend.

\begin{table}%[htbp]
	\centering
	\caption {time cost and resource usage on different systems}
	\label{tab:efficiency}
		\begin{tabular}{cccccc}
		\toprule
		%\multirow{2.5}{*}{Method} & \multicolumn{2}{c}{Data 1} & \multicolumn{2}{c}{Data 2}  \\
		 \multicolumn{2}{c}{Indices} & PyG &DGL & On-MR & On-Pregel \\ 
		%\cmidrule(lr){2-3} \cmidrule(lr){4-5}
		%& \(\mathbf{X}\) & \(\mathbf{Y}\) & \(\mathbf{X}\) & \(\mathbf{Y}\) \\
		\midrule
		Time & SAGE & 780 &630 & 20 & 15 \\
		\cmidrule(lr){2-2}
		(min)				   & GAT & 1056 &948 & 34 & 21 \\
		\cmidrule(lr){1-6}				   
		Resource  & SAGE & $1.6 \times10^6$ &$1.3 \times 10^6$& $2.6 \times 10^4$ &$2.9 \times 10^4$  \\
		\cmidrule(lr){2-2}
		(cpu*min)				   & GAT &$2.1 \times 10^6$ &$1.9 \times 10^6$ &$4.4 \times 10^4$ &$4.1 \times 10^4$ \\
		\bottomrule
	\end{tabular}
\end{table}

%\begin{figure}[htbp]
%\centering
%\begin{minipage}{0.48\linewidth}
%\centering
%\includegraphics[width=\textwidth]{time_cost_vs_embedding_size.pdf}
%\caption{placeholder: Time cost VS. embedding size}
%\label{fig:time_cost_vs_embedding_size}
%\end{minipage}
%\hfill
%\begin{minipage}{0.48\linewidth}
%\centering
%\includegraphics[width=\textwidth]{time_cost_vs_embedding_size.pdf}
% \caption{placeholder: Time cost VS. batch size}
%\label{fig:time_cost_vs_hops}
%\end{minipage}
%\end{figure}

\begin{table}
	\centering
	\caption{Time and resource cost VS. hops}
	\label{tab:time_cost_hops}
		\begin{tabular}{ccccccc}
		\toprule
		%\multirow{2.5}{*}{Method} & \multicolumn{2}{c}{Data 1} & \multicolumn{2}{c}{Data 2}  \\
		 %\multicolumn{2}{c}{Indices} & PyG &DGL & On-MR & On-Pregel \\ 
		%\cmidrule(lr){2-3} \cmidrule(lr){4-5}
		%& \(\mathbf{X}\) & \(\mathbf{Y}\) & \(\mathbf{X}\) & \(\mathbf{Y}\) \\
		{} &\multicolumn{3}{c}{Time (min)} & \multicolumn{3}{c}{Resource (cpu*min)} \\
		\cmidrule(lr){2-4}
		\cmidrule(lr){5-7}
		hops& 1 &2 &3 & 1 &2 &3 \\
		\midrule
		 nbr50& 23 & 160 &3300+ &$4.5\times 10^4$ & $3.2\times 10^5$ & $6.7 \times 10^6$\\
		 nbr10000& 181 & 780 &{OOM} &$3.6\times 10^5$ & $1.6\times 10^6$ & {OOM} \\
		 \cmidrule(lr){1-7}	
		 ours &13 &20 &31 &$1.7\times 10^4$ &$2.6\times10^4$ &$4.0\times10^4$ \\
		\bottomrule
	\end{tabular}
\end{table}

%Furthermore, an interesting phenomenon is that the resource utilization of our system with MapReduce as backend is usually less than that of graph processing system, while the time cost of the former one is more than that of the latter.
%We think this is due to the difference of those two backends: in MapReduce, instance would not maintain state information consistently in memory and it would be exchanged to next stage via external storage. 
%As a result, a certain instance in MapReduce could release the computation resource it applied once the task on it is finished, but the communication time could be longer.
%Therefore, the MapReduce backend takes less resource but leads to more time cost compared with the graph processing backend.
%The difference also make those two backends suit for different application by trading off efficiency and resource:
%for applications on extremely large graphs but with limited resources, the MapReduce backend is the first choice, while for tasks with enough exclusive resource and with relative high restrict on inferring time is more suitable for the graph processing backend.
%However, since the communication on MapReduce backend is based on external storage, it takes more time

\textbf{Consistency.} 
%The prediction score for a certain node should keep stable at different runs, which is a basic requirement in industrial scenarios.
%We conduct a set of experiments to verify the consistency of our system and the traditional pipelines.
We take the GraphSAGE model in PyG on the OGB-MAG240M dataset as baselines to verify the consistency of our system and traditional pipelines.
%Note that, for large graphs, traditional pipelines introduce a neighbor sampling strategy\cite{b4} to avoid too large receptive fields to avoid the OOM problem or too much time cost, which has been mentioned above.
The number of neighbors for a node in traditional pipelines is limited to 10, 50, 100, 1000 in different experiments.
We count the number of node classes predicted in the traditional pipeline at 10 runs and summarize such information for about 130,000 nodes to see whether predicting scores would keep unchanged.

The statistical results are presented in Fig.~\ref{fig:consistency}.
The x-axis denotes how many classes a certain node belongs to, while the y-axis indicates how many nodes would be predicted to a certain number of classes at 10 runs.
%Those four histograms indicate experiments with neighbor limitation strategies.
For example, when the number of neighbors is set to 10, 30840 nodes are predicted into 2 different classes at 10 runs. %, and 8096 nodes are in 3 different classes.
In all, about 30\% nodes would be predicted to at least 2 different classes when the sampling number is set to 10, which indicates that the prediction score is not trusted.
Even increasing the sampling number to 1000, there are still about 0.1\% nodes in trouble, which is unacceptable in industrial scenarios, especially in financial applications.
Note that when the sampling number increases to 10000, such risk is mitigated, but the time cost increases a lot and even causes the OOM problem as shown in Tab.~\ref{tab:time_cost_hops}.


%we set the sampling number of neighbors for 10, 50,100, to 1000.  
%We also take models in PyG as examples to verify the consistency of our system and the traditional pipelines.
%For large graph, traditional pipeline introduces a neighbor sampling strategy[] to avoid too large receptive fields.
%And in our experiments, we set the sampling number of neighbors for 10, 50,100, to 1000. 
%The whole experiments are conducted on the OGB-MAG240M dataset.

%We also design a set of experiments to verify the consistency of our system and the traditional pipelines.
%For large graph, traditional pipeline introduces a neighbor sampling strategy[] to avoid too large receptive fields.

%We count the number of classes of a certain node to be predicted in traditional pipeline at 10 runs and summarize such information for about 13000 nodes.
%The result is illustrates in Figure~\ref{fig:consistency}.
%About 30\% nodes would be predicted to 2 (or more) different classes when the sampling number is set to 10.
%Even increasing the sampling number to 1000, there are still about 0.1\% nodes in the trouble, which is unacceptable in industrial scenarios, especially in financial applications.

%In general, the accuracy of the traditional pipeline would varies with different runs.
%We also demonstrate 10 nodes from the total dataset, whose prediction scores vary drastically at different runs.
%They would be predicted to different classes, which is definitely unacceptable in industrial scenarios.
%Fixing the sampling results may be a solution, but is suboptimal, since it is just like chose a version of prediction scores.
%We also chose 10 nodes whose prediction scores vary a lot to show  

In contrast, our system achieves the same results at different runs since it conducts the inference phase over the full graph without sampling.
%The reason is that we design to conduct the inference phase over the full graph with no sampling.
And we propose a series of strategies to avoid the OOM problem caused by nodes with a large degree.
Therefore, the system is suitable for inference tasks on large graphs, even with skewed degree distribution.
%As a result, we could conduct the inference task on large graphs without any sampling, even there are some hub nodes.
%Furthermore, by proposing the consistency problem here, we hope to draw more attention to such practical problem in industrial scenarios, and motive more works on it.

\begin{figure}
\centering
\includegraphics[width=\linewidth]{consistency_at_10_runs_0118}
\caption{Statistics of total number of classes a node predicted at 10 runs}
\label{fig:consistency}
\end{figure}


\textbf{Scalability.} 
Scalability is a key issue for industrial scenarios.
We design a series of experiments and illustrate the relationship between the resource usage (cpu*min) and the data scale to evaluate the scalability of our system.
The experiments are conducted over Power-Law datasets, and the data scale includes three orders of magnitude as mentioned in Sec.~\ref{sec_exp_settings}.
%and data scales vary from 100 million nodes (about 1 billion edges), 1 billion nodes (100 billion edges), to 10 billions nodes (100 billion edges), which represents data scale at three different orders of magnitude.
A 2-layer GAT model with an embedding size of 64 is conducted on those datasets, and we record the time and resource cost to see how they vary with different data scales.
Note that the backend of our system is set to MapReduce as the resource on the cluster for graph processing backend is not enough for the largest dataset.

Figure~\ref{fig:scalability} demonstrates the experimental results. 
%The left y-axis denotes the resource usage (cpu * min) and the right y-axis indicates the time cost (s), 
The y-axis on the left and right represent resource usage (cpu*min) and time cost (s), respectively, while the x-axis is the data scale.
%All the data at those three axes are mapped with a same log function.
%Both the curve of resource and that of the time cost 
Both the resource and the time-cost curves demonstrate a nearly linear relationship between them and the data scale, which proves the linear scalability of our system.
It is worth noting that our system could finish the inference task for all 10 billion nodes (with about 100 billion edges) within 2 hours (6765 seconds), which shows that our system could scale to extremely large graphs and finish inference tasks efficiently.


%the backend of our inference system is set as MapReduce due to the resource for it 

%We verify the linear scalability of our system by illustrating the relationship between the resource usage (cpu * min) and data scale.
%The experiments are conducted on Power-Law dataset, and the number of nodes vary from 100 million, to 1 billion, and 10 billion, which covers datasets with three different order of magnitude.
%For such large dataset, the backend of our inference system is set as MapReduce. 
%We illustrate the curve of resource utilization 

%Figure~\ref{fig:scalability} demonstrates the results of the experiments. 
%The y-axis denotes the resource utilization while the x-axis is the data scale (i.e., node number) mapped by the log function.
%The curve demonstrates a linear relationship between the resource utilization and data scale.
%That is, with more resource, we could process larger dataset, and the cost of of resource changes varies linearly with data scale.
%For example, if the number of cpu cores scales with the increasing rate of data scale, the total time cost would keep about the same.
%That proves the linear scalability of our system.
%It is worth noting that our system could finished the inference task for all 10 billion nodes (with about 100 billion edges) in 2 hours, which shows the good property of scalability of our system.

\begin{figure*}[htbp]
\centering
\begin{minipage}{0.3\linewidth}
\centering
\includegraphics[width=\textwidth]{large_scalle_memory_and_time_cost_0118}
\caption{Resource (cpu *min) and time cost (s) VS. DataScale}
\label{fig:scalability}
\end{minipage}
\hfill
\begin{minipage}{0.3\linewidth}
\centering
\includegraphics[width=\textwidth]{time_cost_for_partital_gater_0118}
 \caption{Time cost for different instances with partial gather strategy.}
\label{fig:paritial}
\end{minipage}
\hfill
\begin{minipage}{0.3\linewidth}
\centering
\includegraphics[width=\textwidth]{vairances_of_time_cost_for_SN_BC_0118}
 \caption{Variance of time cost with different strategies for nodes with large out-degree}
\label{fig:out_degree_strategies}
\end{minipage}
\end{figure*}

%\begin{figure}[htbp]
%\centering
%\includegraphics[width=0.7\linewidth]{indegree_0105revise}
%\caption{Time cost for different instances. }
%\label{fig:paritial}
%\end{figure}


\begin{figure*}[htbp]
\centering
\begin{minipage}{0.3\linewidth}
\centering
\includegraphics[width=\textwidth]{indegree_io_bytes_0118}
\caption{IO cost for partial gather strategy}
\label{fig:in_degree_io_bytes}
\end{minipage}
\hfill
\begin{minipage}{0.3\linewidth}
\centering
\includegraphics[width=\textwidth]{broadcast_io_bytes_0118}
 \caption{IO cost for broadcast strategy } %with different thresholds
\label{fig:out_degree_io_bytes}
\end{minipage}
\hfill
\begin{minipage}{0.3\linewidth}
\centering
\includegraphics[width=\textwidth]{output_bytes_shaodow_nodes_0118}
 \caption{IO cost for shadow node strategy } %with different thresholds
\label{fig:out_degree_io_bytes_SN}
\end{minipage}
\end{figure*}

\subsubsection{Analysis for Optimization Strategies}
\label{sec_analyis_strategies}
For further analysis of our systems, we also design a set of experiments to verify the effectiveness of the optimization strategies for hub nodes proposed in Sec. \ref{sec_opt_stategies}.
We enable those strategies on the GraphSAGE model over Power-law datasets (about 100 million nodes, 1.4 billion edges) respectively for variable-controlling purposes.
%And following the principle of variable-controlling, when analysing strategies for nodes with many in-edges, the dataset we used hold the properties that the distribution of in-degree follows the power-law while the out-degree follows the uniform distribution, and reverse otherwise.
%The backend of our system is set to MapReduce and the algorithm used in those experiments is GraphSAGE.

Figure~\ref{fig:paritial} illustrates experimental results on the large in-degree problem and the effect of the \emph{partial-gather} strategy. 
The x-axis indicates the number of in-edges for nodes in one instance, while the y-axis denotes the instance latency, and each point represents an instance.
%Red points means we enable the \emph{partial-gather} in the experiment while blue points not.
Results show that latency is positively associated with the number of in-edges.
It's reasonable since the time cost for receiving messages and the computation to aggregate those messages are related to the number of in-edges.
Therefore, nodes with large in-degree would lead the time cost of the related instance in the long tail and become the bottleneck of the whole inference task.
In contrast, the \emph{partial-gather} strategy helps release the stress caused by a large number of in-edges: the variance of time cost on different instances shrinks a lot, and points are close to the mean line.
The reason is from two aspects: firstly, with the \emph{partial-gather} strategy, messages would be partially aggregated on the sender side, which brings down the time cost in communication.
%the \emph{partial-gather} would reduce the number of messages to send, which brings down the time cost in communication.
Secondly, the computation of in-edge messages is uniformly conducted in advance on different instances, which balances the computation load for hub nodes. 
% (with large in-degree)

We also make an analysis of the large out-degree problem, and the results are illustrated in Fig.~\ref{fig:out_degree_strategies}.
\emph{Base} denotes the experiment without optimization strategy, while \emph{SN}, \emph{BC}, and \emph{SN+BC} indicate experiments with \emph{shadow node} strategy, \emph{broadcast} strategy, and the combination of them respectively.
The y-axis represents the variance of time cost in all instances.
Compared with the baseline, both the \emph{shadow node} and \emph{broadcast} strategies are helpful in mitigating the problem caused by nodes with large out-edges, and the latter one is slightly better than the former one since \emph{shadow node} strategy would lead to some overhead by duplicating in-edges to average the communication over different mirror nodes.
Specially, for GraphSAGE, we could achieve better results by combining the two strategies since messages for different out-edges are the same.

%\textcolor{blue}{
%Furthermore, we also design a series of experiments to analyze the IO cost of our system. 
%Results of strategies for large in-degree and out-degree problem are illustrated in Fig.~\ref{fig:in_degree_io_bytes} and Fig.~\ref{fig:out_degree_io_bytes} respectively.
%The x-axises of them represent original number of input or output records for different instances while the y-axises indicate the input or output bytes of each instances.
%Figure~\ref{fig:out_degree_io_bytes} show that our system with the partital-gather strategy could decrease the input IO cost to about a constant level.
%The reason is that, the messages are aggregated in advance and each instance would send at most one message for a certain node in next round.
Furthermore, we develop some experiments to evaluate our system's IO costs.
Figures~\ref{fig:in_degree_io_bytes},~\ref{fig:out_degree_io_bytes}, and~\ref{fig:out_degree_io_bytes_SN} show the results of strategies for large in- and out-degree problems, respectively.
Their y-axises show the input or output bytes of each instance.
For Fig.~\ref{fig:in_degree_io_bytes} and~\ref{fig:out_degree_io_bytes}, x-axises show the initial number of input or output records for different instances.
Specially, the x-axis for Fig.~\ref{fig:out_degree_io_bytes_SN} is worker index sorted according to the output bytes since the \emph{shadow node} strategy would re-arrange the placement of records. 
%the number of initial records for workers would be changed as 
%records would be re-arranged according to the \emph{shadow nodes} and records number
%Since each worker would not only get the UUID from the previous round but also be required to keep a key-value store for those global messages, we believe that the overhead of such an approach would not be overlooked in those situations.
%We believe that in the future, highly optimized code may be used to reduce the overhead.
%Figure~\ref{fig:out_degree_io_bytes} demonstrate that with the increasing of threshold, the effectiveness of \emph{broadcast} strategy is decreased.
%For example, when the threshold is set to 3 million, the IO cost is about linear with the original number of output records.
%When we set it to 10,0000, the IO cost is decreased lot.
%We think it because small threshold would make more nodes benefit from our \emph{broadcast} strategy.
%However, there's nearly no different for threshold in range of [1000, 100000].
%We think the overhead of such strategy would not be ignored in those cases, since each worker would not only receive uuid from former round, but also should maintain a key-value store for those global messages, and then query result from those key-value store.
%Therefore, we set the threshold with an  empirical value 10,0000.
%We think the overhead may be mitigated with highly optimized code in the future.
%}

%\textcolor{blue}{
Figure~\ref{fig:in_degree_io_bytes} demonstrates how our system, using the \emph{partial-gather} approach, reduces the input IO cost to a constant level.
%The total input bytes of all works decrease about 25\% with the \emph{partial-gather} technique while the 10\% works in the tail save about 73\% input bytes.
This approach is effective for all nodes regardless of degree and performs particularly well for nodes with large in-degree since there is no overhead involved.
The \emph{partial-gather} strategy reduces the total communication cost of all works by roughly 25\% while saving up to 73\% IO cost for the 10\% workers in the tail.
The reason is that each instance would only send one message for a particular node in the upcoming round since the messages have already been pre-aggregated.
Each node would only receive as many as instance-number messages in the following round.

%\textcolor{blue}{Figures~\ref{fig:out_degree_io_bytes} and~\ref{fig:out_degree_io_bytes_SN} show the IO costs of the \emph{broadcast} and \emph{shadow nodes} techniques for large out-degree problems.
Moreover, we also vary the threshold for those strategies to verify the heuristic formula in Sec.~\ref{sec_opt_stategies}.
%When the hyper-parameter $\lambda$ is set to 0.1, 
The hyper-parameter $\lambda$ is set to 0.1 in our system, and the threshold to distinguish whether a node is a `hub' node then becomes 100,000 (1 billion edges, 1000 workers).
Compared with the baselines, both the \emph{broadcast} and \emph{shadow nodes} strategies in this setting could decrease the IO cost for ``hub'' nodes significantly.
For instance, the communication costs for the last 10\% workers are reduced by 42\% and 53\% when activating those two techniques, respectively.
Furthermore, the efficacy of those strategies would increase as the threshold decreases.
However, there is no significant difference for the threshold in the range of [10,000, 100,000], and the difference in IO cost is less than 5\%.
This is because a low threshold would enable more nodes to benefit from those techniques, but their overhead could not be disregarded.
For example, the memory cost is nearly doubled.
Therefore, although the threshold determined by the heuristic formula may not be the optimal one, it is reasonable to use it to quickly estimate the threshold, and in this setting, those strategies successfully reduce the IO cost for straggler workers.
%For instance, the IO cost is roughly linear with the initial output record count when the threshold is set at 3 million.
%The IO cost is significantly reduced when we set it to 100,000.
%We believe this is the case since a low threshold would enable more nodes to benefit from our \emph{broadcast} method.
%However, there is essentially no difference for the threshold in the range of [1000, 100,000]: the \emph{broadcast} method reduce the communication costs of all works by about 11\% while saving up to 42\% the output costs for the last 10\% workers.
%We think that the overhead of such an approach would not be disregarded in those circumstances because each worker would not only maintain a key-value store for those global messages but also get the UUID from the previous round.
%As a result, we chose 10,0000 as the empirical value for the threshold.

In summary, the experimental results demonstrate that those strategies proposed in Sec.~\ref{sec_opt_stategies} help eliminate the stragglers caused by the power law, and as a result, our system achieves better load-balancing.


%those experiments demonstrate that those strategies help our system achieve a better load-balancing and could be helpful to eliminate the stragglers caused by power-law.

%it release the burden on computation for hub nodes by aggregating messages in advance, and the computation for large amount of in-edges is placed on the side of message-senders nearly uniformly.

%with the \emph{partial-gather} strategy, the variance of time cost on different instance shrinks a lot, and points are near the position of the mean value line.
%messages would be partially aggregated in the sender side, and the complexity of communication for nodes in receiver side is reduced to a constant level and only related to the number of workers.
%Therefore, it's obvious that the \emph{partial-gather} strategy is helpful to release the stress caused by large amount of in-edges: the variance of time cost on different instance shrinks a lot, and points are near the position of the mean value line.

%It's obvious that the the latency is positively associated with the number of in-edges.
%As a result, nodes with large in-degree would lead the time cost of the related instance in the long tail and becomes the bottleneck of the whole inference task.


%We design a set of experiments to verify the effectiveness of the optimization strategies for hub nodes proposed in Sec. \ref{sec_opt_stategies}.
%We apply all those strategies on a well trained GraphSAGE model over Power-law datasets (about 100 million nodes, 1.4 billion edges) with our framework.
%Following the principle of variable-controlling, when analysing strategies for nodes with many in-edges, the dataset we used hold the properties that the distribution of in-degree follows the power-law while the out-degree follows the uniform distribution, and reverse otherwise.

%Figure 6 demonstrates experimental results on large in-degree problem and the effect of our \emph{partial-gather} strategy.
%%the effect of the \emph{partial-gather} strategy.
%The x-axis indicates all the in-edges in one instance while the y-axis denotes the instance latency.
%By controlling variable, we could find that the latency is positively associated with the number of in-edges.
%As a result, nodes with large in-degree would lead the time cost of the related instance in the long tail and becomes the bottleneck of the whole inference task.
%On the contrast, with the \emph{partial-gather} strategy, the variance of time cost on different instance shrinks a lot, and the total curve is compressed to the position of the mean value line.
%Therefore, it's obvious that the \emph{partial-gather} strategy is helpful to release the stress caused by large amount of in-edges.



%We present the results on large out-degree problem in Figure 7, to show the effectiveness of out \emph{broadcast} and \emph{shadow-nodes} strategies.
%We use the variance to indicate the divergence of time cost on different instances.
%In general, all those two strategies could mitigate the problem caused by nodes with large out-edges.
%Meanwhile, the \emph{broadcast} achieve a better result compared with the \emph{shadow-nodes} strategies, since the latter one would lead to some overhead by duplicating in-edges to average the communication over different shadow nodes.
%Note that, those two strategies are orthometric for GraphSAGE algorithm (messages keep the same for out-edges) , and thus the combination of them achieves the best result in our experiment.


%Note that those strategies are not enabled by default, and should be manually configured according different causes, which we think 
%that our strategies could be helpful to release the problem caused by power-law.
%And our system achieve a better load-balancing since they eliminate the stragglers and the 




\section{Conclusion}
In this paper, we propose InferTurbo, to boost the inference tasks at industrial-scale graphs.
The design follows the GAS-like paradigm underlying the computation of GNNs, with which we could specify the data flow and computation flow of GNNs and hierarchically conduct the inference phase in full-graph manner.
In this way, our system could avoid the redundant computation problem.
We provide alternative backends of MapReduce and Pregel for different industrial applications, and both could scale to large graphs.
Furthermore, a set of strategies without dropping any information is used to solve problems caused by nodes with large in-degree or out-degree.
%Therefore, our system achieve good property of consistency and better load-balancing.
%the stable prediction score at different runs is guaranteed in our system and achieves better load-balancing.
The experimental results demonstrate that our system achieves $30\times$ $\sim$ $50\times$ speedup compared with some state-of-the-art graph learning systems. Our system could finish the inference task over a graph with 10 billion nodes and 100 billion edges within 2 hours, which shows the superior efficiency and scalability of our system. 
% and proves it promising for industrial inference tasks



%With such abstraction, we implement our inference system with alternative backends of MapReduce and Pregel.
%As a result, our system avoids redundant computation in traditional training-inferring pipeline while enjoys a linear scalability.
%Furthermore, we design a set of strategies to problems caused by power-law, which is common in industrial graph datas.
%Therefore, the stable prediction score at different runs is guaranteed in our system and we achieve better load-balancing.
%Benefiting from those designs, our system could finished the inference task over graph with ten billions nodes and 100 billions edges within 2 hours.

 
%\section{Ease of Use}
%
%\subsection{Maintaining the Integrity of the Specifications}
%
%The IEEEtran class file is used to format your paper and style the text. All margins, 
%column widths, line spaces, and text fonts are prescribed; please do not 
%alter them. You may note peculiarities. For example, the head margin
%measures proportionately more than is customary. This measurement 
%and others are deliberate, using specifications that anticipate your paper 
%as one part of the entire proceedings, and not as an independent document. 
%Please do not revise any of the current designations.
%
%\section{Prepare Your Paper Before Styling}
%Before you begin to format your paper, first write and save the content as a 
%separate text file. Complete all content and organizational editing before 
%formatting. Please note sections \ref{AA}--\ref{SCM} below for more information on 
%proofreading, spelling and grammar.
%
%Keep your text and graphic files separate until after the text has been 
%formatted and styled. Do not number text heads---{\LaTeX} will do that 
%for you.
%
%\subsection{Abbreviations and Acronyms}\label{AA}
%Define abbreviations and acronyms the first time they are used in the text, 
%even after they have been defined in the abstract. Abbreviations such as 
%IEEE, SI, MKS, CGS, ac, dc, and rms do not have to be defined. Do not use 
%abbreviations in the title or heads unless they are unavoidable.
%
%\subsection{Units}
%\begin{itemize}
%\item Use either SI (MKS) or CGS as primary units. (SI units are encouraged.) English units may be used as secondary units (in parentheses). An exception would be the use of English units as identifiers in trade, such as ``3.5-inch disk drive''.
%\item Avoid combining SI and CGS units, such as current in amperes and magnetic field in oersteds. This often leads to confusion because equations do not balance dimensionally. If you must use mixed units, clearly state the units for each quantity that you use in an equation.
%\item Do not mix complete spellings and abbreviations of units: ``Wb/m\textsuperscript{2}'' or ``webers per square meter'', not ``webers/m\textsuperscript{2}''. Spell out units when they appear in text: ``. . . a few henries'', not ``. . . a few H''.
%\item Use a zero before decimal points: ``0.25'', not ``.25''. Use ``cm\textsuperscript{3}'', not ``cc''.)
%\end{itemize}
%
%\subsection{Equations}
%Number equations consecutively. To make your 
%equations more compact, you may use the solidus (~/~), the exp function, or 
%appropriate exponents. Italicize Roman symbols for quantities and variables, 
%but not Greek symbols. Use a long dash rather than a hyphen for a minus 
%sign. Punctuate equations with commas or periods when they are part of a 
%sentence, as in:
%\begin{equation}
%a+b=\gamma\label{eq}
%\end{equation}
%
%Be sure that the 
%symbols in your equation have been defined before or immediately following 
%the equation. Use ``\eqref{eq}'', not ``Eq.~\eqref{eq}'' or ``equation \eqref{eq}'', except at 
%the beginning of a sentence: ``Equation \eqref{eq} is . . .''
%
%\subsection{\LaTeX-Specific Advice}
%
%Please use ``soft'' (e.g., \verb|\eqref{Eq}|) cross references instead
%of ``hard'' references (e.g., \verb|(1)|). That will make it possible
%to combine sections, add equations, or change the order of figures or
%citations without having to go through the file line by line.
%
%Please don't use the \verb|{eqnarray}| equation environment. Use
%\verb|{align}| or \verb|{IEEEeqnarray}| instead. The \verb|{eqnarray}|
%environment leaves unsightly spaces around relation symbols.
%
%Please note that the \verb|{subequations}| environment in {\LaTeX}
%will increment the main equation counter even when there are no
%equation numbers displayed. If you forget that, you might write an
%article in which the equation numbers skip from (17) to (20), causing
%the copy editors to wonder if you've discovered a new method of
%counting.
%
%{\BibTeX} does not work by magic. It doesn't get the bibliographic
%data from thin air but from .bib files. If you use {\BibTeX} to produce a
%bibliography you must send the .bib files. 
%
%{\LaTeX} can't read your mind. If you assign the same label to a
%subsubsection and a table, you might find that Table I has been cross
%referenced as Table IV-B3. 
%
%{\LaTeX} does not have precognitive abilities. If you put a
%\verb|\label| command before the command that updates the counter it's
%supposed to be using, the label will pick up the last counter to be
%cross referenced instead. In particular, a \verb|\label| command
%should not go before the caption of a figure or a table.
%
%Do not use \verb|\nonumber| inside the \verb|{array}| environment. It
%will not stop equation numbers inside \verb|{array}| (there won't be
%any anyway) and it might stop a wanted equation number in the
%surrounding equation.
%
%\subsection{Some Common Mistakes}\label{SCM}
%\begin{itemize}
%\item The word ``data'' is plural, not singular.
%\item The subscript for the permeability of vacuum $\mu_{0}$, and other common scientific constants, is zero with subscript formatting, not a lowercase letter ``o''.
%\item In American English, commas, semicolons, periods, question and exclamation marks are located within quotation marks only when a complete thought or name is cited, such as a title or full quotation. When quotation marks are used, instead of a bold or italic typeface, to highlight a word or phrase, punctuation should appear outside of the quotation marks. A parenthetical phrase or statement at the end of a sentence is punctuated outside of the closing parenthesis (like this). (A parenthetical sentence is punctuated within the parentheses.)
%\item A graph within a graph is an ``inset'', not an ``insert''. The word alternatively is preferred to the word ``alternately'' (unless you really mean something that alternates).
%\item Do not use the word ``essentially'' to mean ``approximately'' or ``effectively''.
%\item In your paper title, if the words ``that uses'' can accurately replace the word ``using'', capitalize the ``u''; if not, keep using lower-cased.
%\item Be aware of the different meanings of the homophones ``affect'' and ``effect'', ``complement'' and ``compliment'', ``discreet'' and ``discrete'', ``principal'' and ``principle''.
%\item Do not confuse ``imply'' and ``infer''.
%\item The prefix ``non'' is not a word; it should be joined to the word it modifies, usually without a hyphen.
%\item There is no period after the ``et'' in the Latin abbreviation ``et al.''.
%\item The abbreviation ``i.e.'' means ``that is'', and the abbreviation ``e.g.'' means ``for example''.
%\end{itemize}
%An excellent style manual for science writers is \cite{b7}.
%
%\subsection{Authors and Affiliations}
%\textbf{The class file is designed for, but not limited to, six authors.} A 
%minimum of one author is required for all conference articles. Author names 
%should be listed starting from left to right and then moving down to the 
%next line. This is the author sequence that will be used in future citations 
%and by indexing services. Names should not be listed in columns nor group by 
%affiliation. Please keep your affiliations as succinct as possible (for 
%example, do not differentiate among departments of the same organization).
%
%\subsection{Identify the Headings}
%Headings, or heads, are organizational devices that guide the reader through 
%your paper. There are two types: component heads and text heads.
%
%Component heads identify the different components of your paper and are not 
%topically subordinate to each other. Examples include Acknowledgments and 
%References and, for these, the correct style to use is ``Heading 5''. Use 
%``figure caption'' for your Figure captions, and ``table head'' for your 
%table title. Run-in heads, such as ``Abstract'', will require you to apply a 
%style (in this case, italic) in addition to the style provided by the drop 
%down menu to differentiate the head from the text.
%
%Text heads organize the topics on a relational, hierarchical basis. For 
%example, the paper title is the primary text head because all subsequent 
%material relates and elaborates on this one topic. If there are two or more 
%sub-topics, the next level head (uppercase Roman numerals) should be used 
%and, conversely, if there are not at least two sub-topics, then no subheads 
%should be introduced.
%
%\subsection{Figures and Tables}
%\paragraph{Positioning Figures and Tables} Place figures and tables at the top and 
%bottom of columns. Avoid placing them in the middle of columns. Large 
%figures and tables may span across both columns. Figure captions should be 
%below the figures; table heads should appear above the tables. Insert 
%figures and tables after they are cited in the text. Use the abbreviation 
%``Fig.~\ref{fig}'', even at the beginning of a sentence.
%
%\begin{table}[htbp]
%\caption{Table Type Styles}
%\begin{center}
%\begin{tabular}{|c|c|c|c|}
%\hline
%\textbf{Table}&\multicolumn{3}{|c|}{\textbf{Table Column Head}} \\
%\cline{2-4} 
%\textbf{Head} & \textbf{\textit{Table column subhead}}& \textbf{\textit{Subhead}}& \textbf{\textit{Subhead}} \\
%\hline
%copy& More table copy$^{\mathrm{a}}$& &  \\
%\hline
%\multicolumn{4}{l}{$^{\mathrm{a}}$Sample of a Table footnote.}
%\end{tabular}
%\label{tab1}
%\end{center}
%\end{table}
%
%\begin{figure}[htbp]
%\centerline{\includegraphics{fig1.png}}
%\caption{Example of a figure caption.}
%\label{fig}
%\end{figure}
%
%Figure Labels: Use 8 point Times New Roman for Figure labels. Use words 
%rather than symbols or abbreviations when writing Figure axis labels to 
%avoid confusing the reader. As an example, write the quantity 
%``Magnetization'', or ``Magnetization, M'', not just ``M''. If including 
%units in the label, present them within parentheses. Do not label axes only 
%with units. In the example, write ``Magnetization (A/m)'' or ``Magnetization 
%\{A[m(1)]\}'', not just ``A/m''. Do not label axes with a ratio of 
%quantities and units. For example, write ``Temperature (K)'', not 
%``Temperature/K''.
%
%\section*{Acknowledgment}
%
%The preferred spelling of the word ``acknowledgment'' in America is without 
%an ``e'' after the ``g''. Avoid the stilted expression ``one of us (R. B. 
%G.) thanks $\ldots$''. Instead, try ``R. B. G. thanks$\ldots$''. Put sponsor 
%acknowledgments in the unnumbered footnote on the first page.
%
%\section*{References}
%
%Please number citations consecutively within brackets \cite{b1}. The 
%sentence punctuation follows the bracket \cite{b2}. Refer simply to the reference 
%number, as in \cite{b3}---do not use ``Ref. \cite{b3}'' or ``reference \cite{b3}'' except at 
%the beginning of a sentence: ``Reference \cite{b3} was the first $\ldots$''
%
%Number footnotes separately in superscripts. Place the actual footnote at 
%the bottom of the column in which it was cited. Do not put footnotes in the 
%abstract or reference list. Use letters for table footnotes.
%
%Unless there are six authors or more give all authors' names; do not use 
%``et al.''. Papers that have not been published, even if they have been 
%submitted for publication, should be cited as ``unpublished'' \cite{b4}. Papers 
%that have been accepted for publication should be cited as ``in press'' \cite{b5}. 
%Capitalize only the first word in a paper title, except for proper nouns and 
%element symbols.
%
%For papers published in translation journals, please give the English 
%citation first, followed by the original foreign-language citation \cite{b6}.

\begin{thebibliography}{00}
\bibitem{b1} Welling, Max, and Thomas N. Kipf. "Semi-supervised classification with graph convolutional networks." J. International Conference on Learning Representations (ICLR 2017). 2016.
\bibitem{b2} Liu, Ziqi, et al. "Geniepath: Graph neural networks with adaptive receptive paths." Proceedings of the AAAI Conference on Artificial Intelligence. Vol. 33. No. 01. 2019.
\bibitem{b3} Veličković, Petar, et al. "Graph attention networks." arXiv preprint arXiv:1710.10903 (2017).
\bibitem{b4} Hamilton, Will, Zhitao Ying, and Jure Leskovec. "Inductive representation learning on large graphs." Advances in neural information processing systems 30 (2017).
\bibitem{b5} Zhang, Dalong, et al. "DSSLP: A distributed framework for semi-supervised link prediction." 2019 IEEE International Conference on Big Data (Big Data). IEEE, 2019.
\bibitem{b6} Zhang, Muhan, and Yixin Chen. "Weisfeiler-lehman neural machine for link prediction." Proceedings of the 23rd ACM SIGKDD international conference on knowledge discovery and data mining. 2017.
\bibitem{b7} Yin, Yongjing, et al. "Graph-based neural sentence ordering." arXiv preprint arXiv:1912.07225 (2019).
\bibitem{b8} Marino, Kenneth, Ruslan Salakhutdinov, and Abhinav Gupta. "The more you know: Using knowledge graphs for image classification." arXiv preprint arXiv:1612.04844 (2016).
\bibitem{b9} Zhang, Muhan, and Yixin Chen. "Inductive graph pattern learning for recommender systems based on a graph neural network." arXiv preprint arXiv:1904.12058 (2019).
\bibitem{b10} Ying, Rex, et al. "Graph convolutional neural networks for web-scale recommender systems." Proceedings of the 24th ACM SIGKDD international conference on knowledge discovery and data mining. 2018.
\bibitem{ab11} Tu, Ke, et al. "Conditional graph attention networks for distilling and refining knowledge graphs in recommendation." Proceedings of the 30th ACM International Conference on Information and Knowledge Management. 2021.
\bibitem{ab12} Hu, Binbin et al. “MERIT: Learning Multi-level Representations on Temporal Graphs.” IJCAI (2022).
\bibitem{ab13} Yang, Minghui, et al. "IntelliTag: An Intelligent Cloud Customer Service System Based on Tag Recommendation." 2021 IEEE 37th International Conference on Data Engineering (ICDE). IEEE, 2021.
\bibitem{b11} Liu, Ziqi, et al. "Graph representation learning for merchant incentive optimization in mobile payment marketing." Proceedings of the 28th ACM International Conference on Information and Knowledge Management. 2019.
\bibitem{ab15}  Wang, Weifan, et al. "Intent Mining: A Social and Semantic Enhanced Topic Model for Operation-Friendly Digital Marketing." 2022 IEEE 38th International Conference on Data Engineering (ICDE). IEEE, 2022.
\bibitem{b12} Liu, Ziqi, et al. "Heterogeneous graph neural networks for malicious account detection." Proceedings of the 27th ACM International Conference on Information and Knowledge Management. 2018.
\bibitem{ab16}Yang, Shuo, et al. "Financial risk analysis for SMEs with graph-based supply chain mining." Proceedings of the Twenty-Ninth International Conference on International Joint Conferences on Artificial Intelligence. 2021.
\bibitem{b13} Wang, Minjie, et al. "Deep graph library: A graph-centric, highly-performant package for graph neural networks." arXiv preprint arXiv:1909.01315 (2019).
\bibitem{b14} Zheng, Da, et al. "Distdgl: distributed graph neural network training for billion-scale graphs." 2020 IEEE/ACM 10th Workshop on Irregular Applications: Architectures and Algorithms (IA3). IEEE, 2020.
\bibitem{b15} Fey, Matthias, and Jan Eric Lenssen. "Fast graph representation learning with PyTorch Geometric." arXiv preprint arXiv:1903.02428 (2019).
\bibitem{b16} Yang, Hongxia. "Aligraph: A comprehensive graph neural network platform." Proceedings of the 25th ACM SIGKDD international conference on knowledge discovery and data mining. 2019.
\bibitem{b17} Zhang, Dalong et al. “AGL: A Scalable System for Industrial-purpose Graph Machine Learning.” Proc. VLDB Endow. 13 (2020): 3125-3137.
\bibitem{b17append} Liu, Ziqi, et al. "Bandit samplers for training graph neural networks." Advances in Neural Information Processing Systems 33 (2020): 6878-6888.
\bibitem{b17append2} Huang, Wen-bing et al. “Adaptive Sampling Towards Fast Graph Representation Learning.” ArXiv abs/1809.05343 (2018)
\bibitem{aab17} Zhou, Hongkuan, et al. "Accelerating large scale real-time GNN inference using channel pruning." arXiv preprint arXiv:2105.04528 (2021).
\bibitem{aab18} Zhang, Shichang, et al. "Graph-less Neural Networks: Teaching Old MLPs New Tricks Via Distillation." International Conference on Learning Representations. 2021.
\bibitem{ab18} Ching, Avery, et al. "One trillion edges: Graph processing at facebook-scale." Proceedings of the VLDB Endowment 8.12 (2015): 1804-1815.
\bibitem{ab19} Lerer, Adam, et al. "Pytorch-biggraph: A large scale graph embedding system." Proceedings of Machine Learning and Systems 1 (2019): 120-131.
\bibitem{b18}  Westland, James Christopher et al. “Private Information, Credit Risk and Graph Structure in P2P Lending Networks.” ArXiv abs/1802.10000 (2018).
\bibitem{b19} Gonzalez, Joseph E., et al. "{PowerGraph}: Distributed {Graph-Parallel} Computation on Natural Graphs." 10th USENIX symposium on operating systems design and implementation (OSDI 12). 2012.
\bibitem{b20} Dean, Jeffrey, and Sanjay Ghemawat. "MapReduce: simplified data processing on large clusters." Communications of the ACM 51.1 (2008): 107-113.
\bibitem{b21} Zaharia, Matei, et al. "Spark: Cluster computing with working sets." 2nd USENIX Workshop on Hot Topics in Cloud Computing (HotCloud 10). 2010.
\bibitem{b22} Malewicz, Grzegorz, et al. "Pregel: a system for large-scale graph processing." Proceedings of the 2010 ACM SIGMOD International Conference on Management of data. 2010.
\bibitem{b23} Gilmer, Justin, et al. "Neural message passing for quantum chemistry." International conference on machine learning. PMLR, 2017.
\bibitem{b24} Page, Lawrence et al. “The PageRank Citation Ranking : Bringing Order to the Web.” The Web Conference (1999).
\bibitem{b25} Goldberg, Andrew V., and Chris Harrelson. "Computing the shortest path: A search meets graph theory." SODA. Vol. 5. 2005.
\bibitem{b26} Chan, Albert et al. “CGMGRAPH/CGMLIB: Implementing and Testing CGM Graph Algorithms on PC Clusters and Shared Memory Machines.” The International Journal of High Performance Computing Applications 19 (2005): 81 - 97.
\bibitem{b27} Batarfi, Omar, et al. "Large scale graph processing systems: survey and an experimental evaluation." Cluster Computing 18.3 (2015): 1189-1213.
\bibitem{b28} Roy, Amitabha, Ivo Mihailovic, and Willy Zwaenepoel. "X-stream: Edge-centric graph processing using streaming partitions." Proceedings of the Twenty-Fourth ACM Symposium on Operating Systems Principles. 2013.
\bibitem{b29} Kyrola, Aapo, Guy Blelloch, and Carlos Guestrin. "{GraphChi}:{Large-Scale} Graph Computation on Just a {PC}." 10th USENIX Symposium on Operating Systems Design and Implementation (OSDI 12). 2012.
\bibitem{b30} Zhu, Xiaowei, et al. "Gemini: A {Computation-Centric} Distributed Graph Processing System." 12th USENIX Symposium on Operating Systems Design and Implementation (OSDI 16). 2016.
\bibitem{b31} Fan, Wenfei, et al. "GRAPE: Parallelizing sequential graph computations." Proceedings of the VLDB Endowment 10.12 (2017): 1889-1892.
\bibitem{b32} Bottou, Léon. "Stochastic gradient descent tricks." Neural networks: Tricks of the trade. Springer, Berlin, Heidelberg, 2012. 421-436.
\bibitem{b33} Abadi, Martín, et al. "{TensorFlow}: a system for {Large-Scale} machine learning." 12th USENIX symposium on operating systems design and implementation (OSDI 16). 2016.
\bibitem{b34} Paszke, Adam, et al. "Automatic differentiation in pytorch." (2017).
\bibitem{b35} Ma, Lingxiao, et al. "{NeuGraph}: parallel deep neural network computation on large graphs." 2019 USENIX Annual Technical Conference (USENIX ATC 19). 2019.
\bibitem{b36} Wu, Yidi, et al. "Seastar: vertex-centric programming for graph neural networks." Proceedings of the Sixteenth European Conference on Computer Systems. 2021.
\bibitem{b37} Mondal, Sudipta, et al. "GNNIE: GNN inference engine with load-balancing and graph-specific caching." Proceedings of the 59th ACM/IEEE Design Automation Conference. 2022.
\bibitem{b38} Fu, Qiang, and H. Howie Huang. "GIN: High-Performance, Scalable Inference for Graph Neural Networks."
\bibitem{b39} Zitnik, Marinka, and Jure Leskovec. "Predicting multicellular function through multi-layer tissue networks." Bioinformatics 33.14 (2017): i190-i198.
\bibitem{b40} Hu, Weihua, et al. "Open graph benchmark: Datasets for machine learning on graphs." Advances in neural information processing systems 33 (2020): 22118-22133.
\bibitem{b41} Hu, Weihua, et al. "Ogb-lsc: A large-scale challenge for machine learning on graphs." arXiv preprint arXiv:2103.09430 (2021).
\bibitem{b42} Shi, Yunsheng, et al. "Runimp: solution for kddcup 2021 MAG240M-LSC." preprint (2021)






%\bibitem{b1} G. Eason, B. Noble, and I. N. Sneddon, ``On certain integrals of Lipschitz-Hankel type involving products of Bessel functions,'' Phil. Trans. Roy. Soc. London, vol. A247, pp. 529--551, April 1955.
%\bibitem{b2} J. Clerk Maxwell, A Treatise on Electricity and Magnetism, 3rd ed., vol. 2. Oxford: Clarendon, 1892, pp.68--73.
%\bibitem{b3} I. S. Jacobs and C. P. Bean, ``Fine particles, thin films and exchange anisotropy,'' in Magnetism, vol. III, G. T. Rado and H. Suhl, Eds. New York: Academic, 1963, pp. 271--350.
%\bibitem{b4} K. Elissa, ``Title of paper if known,'' unpublished.
%\bibitem{b5} R. Nicole, ``Title of paper with only first word capitalized,'' J. Name Stand. Abbrev., in press.
%\bibitem{b6} Y. Yorozu, M. Hirano, K. Oka, and Y. Tagawa, ``Electron spectroscopy studies on magneto-optical media and plastic substrate interface,'' IEEE Transl. J. Magn. Japan, vol. 2, pp. 740--741, August 1987 [Digests 9th Annual Conf. Magnetics Japan, p. 301, 1982].
%\bibitem{b7} M. Young, The Technical Writer's Handbook. Mill Valley, CA: University Science, 1989.
\end{thebibliography}
%\vspace{12pt}
%\color{red}
%IEEE conference templates contain guidance text for composing and formatting conference papers. Please ensure that all template text is removed from your conference paper prior to submission to the conference. Failure to remove the template text from your paper may result in your paper not being published.


\end{document}
