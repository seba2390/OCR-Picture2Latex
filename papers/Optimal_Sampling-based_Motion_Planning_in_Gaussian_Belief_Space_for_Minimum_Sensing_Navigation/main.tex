% sage_latex_guidelines.tex V1.20, 14 January 2017

\documentclass[Afour,sageh,times]{sagej}

\usepackage{moreverb,url}

\usepackage[colorlinks,bookmarksopen,bookmarksnumbered,citecolor=blue,urlcolor=red]{hyperref}

\newcommand\BibTeX{{\rmfamily B\kern-.05em \textsc{i\kern-.025em b}\kern-.08em
T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Ali added:
\newcommand{\bx}{{\bf x}}
\newcommand{\by}{{\bf y}}
\newcommand{\bv}{{\bf v}}
\newcommand{\bn}{{\bf n}}
\newcommand{\bm}{{\bf m}}
\newcommand{\boldm}{{\bf m}}
\newcommand{\bb}{{\bf b}}
\newcommand{\bu}{{\bf u}}
\newcommand{\bw}{{\bf w}}
\newcommand{\bz}{{\bf z}}
\newcommand{\bt}{{\bf t}}
\usepackage{caption}
\usepackage{subcaption}
\newcommand{\bM}{{\bf M}}
\usepackage{tikz}
\newtheorem{theorem}{\bf Theorem}
\newtheorem{acknowledgement}{\bf Acknowledgement}
\newtheorem{corollary}{\bf Corollary}
\newtheorem{example}{\bf Example}
\newtheorem{problem}{\bf Problem}
\newtheorem{remark}{\bf Remark}
\newtheorem{definition}{\bf Definition}
\newtheorem{claim}{\bf Claim}
\newtheorem{question}{\bf Question}
\newtheorem{proposition}{\bf Proposition}
\newtheorem{lemma}{\bf Lemma}
\newtheorem{assumption}{\bf Assumption}
\newtheorem{axiom}{\bf Axiom}
\newtheorem{conjecture}{\bf Conjecture}
\def\QED{~\rule[-1pt]{5pt}{5pt}\par\medskip}

\usepackage[ruled, vlined, linesnumbered]{algorithm2e}
\usepackage{dirtytalk}
\usepackage{bm}

% \usepackage{lipsum}
% \usepackage[affil-it]{authblk}
% \usepackage{blindtext}
% \usepackage{abstract}

\DeclareMathOperator*{\argmin}{argmin}
\renewcommand{\qed}{\hfill\blacksquare}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\def\volumeyear{2016}

\setcounter{secnumdepth}{3} 

\begin{document}

\runninghead{Zinage, Pedram, and Tanaka}
\title{Optimal Sampling-based Motion Planning in Gaussian Belief Space for Minimum Sensing Navigation}

\author{Vrushabh Zinage\affilnum{1}, Ali Reza Pedram\affilnum{2}, and Takashi Tanaka\affilnum{1}}
% 
\affiliation{
\affilnum{1}Department of Aerospace Engineering and Engineering Mechanics, University of Texas at Austin.  \\
\affilnum{2}Walker Department of Mechanical Engineering, University of Texas at Austin.}

\corrauth{Takashi Tanaka, Department of Aerospace Engineering and Engineering Mechanics, University of Texas at Austin, Austin, TX, 78712, USA.}

\email{ttanaka@utexas.edu}

\begin{abstract}
In this paper, we consider the motion planning problem in Gaussian belief space for minimum sensing navigation. Despite the extensive use of sampling-based algorithms and their rigorous analysis in the deterministic setting, there has been little formal analysis of the quality of their solutions returned by sampling algorithms in Gaussian belief space. This paper aims to address this lack of research by examining the asymptotic behavior of the cost of solutions obtained from Gaussian belief space based sampling algorithms as the number of samples increases. To that end, we propose a sampling based motion planning algorithm termed Information Geometric PRM* (IG-PRM*) for generating feasible paths that minimize a weighted sum of the Euclidean and an information-theoretic cost and show that the cost of the solution that is returned is guaranteed to approach the global optimum in the limit of large number of samples. Finally, we consider an obstacle-free scenario and compute the optimal solution using the "move and sense" strategy in literature. We then verify that the cost returned by our proposed algorithm converges to this optimal solution as the number of samples increases.
\end{abstract}

\keywords{Motion planning, sampling based algorithms, optimal path planning, Belief space planning, Information theory}

\maketitle

\section{Introduction}
Over the past few decades, motion planning has been an active area of research in the robotics community. Motion planning can be broadly classified into three main categories, mainly grid based \cite{ding2019efficient_hkust_search,MacAllister}, optimization based \cite{mercy2017spline_tcst,Kushleyev,Deits} and sampling based algorithms \cite{karaman2011sampling}. Sampling-based planning algorithms sample points in the configuration space and check whether a connection is possible between the sampled points using collision-checking modules which are treated as a black box and are usually computationally expensive. They have shown success in handling high dimensional motion planning problems for instance manipulators \cite{khan2020control_rrt_manipulator}, warehouse robots, aerial robotics \cite{lee2016planning_rrt_aerial} etc. In the field of robotic motion planning, it is common to first generate a reference path (usually using sampling based algorithms \cite{karaman2011sampling,hart1968formal_astar,kavraki1996probabilistic_prm,lavalle2001rapidly_rrt,janson2015fast_fmt,zinage2020generalized}) and then separately design a feedback control system for following that path. While this two-step approach is not optimal in general, it simplifies the overall problem and is acceptable in many cases. Additionally, this two-step method can take advantage of powerful geometry-based trajectory generation algorithms (such as \cite{hkust,upenn,mellinger,ding2019safe_hkust_optimization,zinage20233d}), which allows for addressing other considerations such as dynamic constraints, randomness, and uncertainty during the control design phase. Despite these benefits, it is important to acknowledge the inherent challenges posed by measurement noise and uncertainty in robotics.
% While this two-step approach is not optimal in general, it simplifies the overall problem and is acceptable in many cases. Additionally, this two-step method can take advantage of powerful geometry-based trajectory generation algorithms (such as \cite{hkust,upenn,mellinger,ding2019safe_hkust_optimization})  while other considerations such as dynamic constraints, randomness, and uncertainty can be addressed during the control design phase. 

% However, measurement noise and uncertainty are fundamental and inevitable problems in robotics. To that end, in contrast to planning without uncertainty where the planning algorithms consist of deterministic boolean indicators that check whether an edge in the feasible path is in a collision or not, it is advantageous to represent the state of the agent  as a probability distribution over the states in the configuration space.
In light of these fundamental and inevitable issues, a different perspective may be more advantageous when planning under uncertainty. Instead of using deterministic boolean indicators to check for collisions in feasible paths, as is done in planning without uncertainty, it is preferable to represent the state of the agent as a probability distribution over the states in the configuration space. 
% This approach acknowledges the inherent uncertainty in the robotic system and allows for more robust planning solutions.
These states are known as the belief state or the information state. Planning under uncertainty can be formulated as a Partially Observable Markov Decision Process (POMDP) \cite{astrom1965optimal_pomdp_1,smallwood1973optimal_pomdp_2,kaelbling1998planning_pomdp_3}.
% Motion planning over these belief states can be formulated as a Partially Observable Markov Decision Process (POMDP) \cite{astrom1965optimal_pomdp_1,smallwood1973optimal_pomdp_2,kaelbling1998planning_pomdp_3}.
In general, for real-world problems, it becomes computationally intractable to compute the solution of these POMDPs despite the recent progress to solve these POMDPs. Planning in belief spaces that are infinite-dimensional becomes more tractable from a generated roadmap using sampling based motion planning algorithms. Therefore, sampling-based algorithms are considered more advantageous for these infinite-dimensional motion planning problems.

The main motivation for this paper arises from the need for simultaneous perception and planning in autonomous systems to address the problem of finding the shortest path while minimizing sensory resource consumption. Advanced and affordable sensing devices have made it easier to gather sensor data, but this may not always be efficient for resource-limited robots due to the significant power and computational resources required. As the number of sensor modalities increases, it is crucial to minimize the use of sensory resources while ensuring that the algorithm converges to an optimal solution rather than a suboptimal one, as the latter can lead to higher costs.
In this paper, we propose the Information Geometric PRM* (IG-PRM*), a sampling-based planning algorithm for minimum sensing navigation that computes a reference path in Gaussian belief space, minimizing the weighted sum of Euclidean and perception costs, with the latter being a quasi-pseudo metric. Our main contribution is the proof of asymptotic optimality for IG-PRM*. The asymptotic optimality of a related algorithm termed IG-RRT* was already conjectured in \cite{pedram2021gaussian}. Asymptotic optimality is essential in motion planning, as it guarantees that the algorithm will eventually find a solution as close as desired to the optimal one, without getting stuck in a suboptimal solution. Following \cite{pedram2021rationally,pedram2021gaussian}, our goal is to develop an asymptotic optimal path planning methodology that enables a robot to navigate using the minimum amount of sensing resources while adapting to any constraints on its sensory resources.


% In this paper, we consider the problem of minimum sensing navigation and propose a sampling-based planning algorithm called Information Geometric PRM* (IG-PRM*). This algorithm computes a reference path in the Gaussian belief space, minimizing the weighted sum of Euclidean and perception costs (information geometric cost), with the latter being a quasi-pseudo metric \cite{pedram2021gaussian}. As the main contribution of this paper, we prove the asymptotic optimality of the proposed IG-PRM*. Asymptotic optimality is an important property in motion planning as it ensures that the motion planning algorithm will eventually find a solution that is as close as desired to the optimal solution. In other words, asymptotic optimality guarantees that the algorithm will not get stuck in a suboptimal solution, but will continue getting closer to the optimal solution as the number of samples increases, especially in cases where the difference between the suboptimal and optimal solution is large. Following, \cite{pedram2021gaussian}, our goal is to develop an asymptotic optimal path planning methodology that enables a robot/agent to navigate using the minimum amount of sensing resources required, while also being able to adapt to any constraints on the robot's sensory resources.
% The quasi-pseudo metric is defined in this paper as the weighted sum of the Euclidean cost and the information cost that is required to steer the belief state.
%  \subsection{Motivation}
% The problem we are addressing, finding the shortest path, is motivated by the need for efficient perception and action planning in autonomous systems that are required to process large amounts of information. While it has become easier to gather sensor data due to the availability of advanced and affordable sensing devices, it may not always be the most efficient approach for resource-limited robots, as it can consume a significant amount of power and computational resources without providing significant benefits. As the number of available sensor modalities increases, it becomes increasingly important to consider how to complete a task using the minimum amount of sensory resources necessary, such as by reducing the frequency at which sensors are used or the sensitivity of the sensors. In addition, it is equally important to guarantee that the algorithm that minimizes the information geometric cost converges to an optimal solution rather than a suboptimal solution as the number of samples increases. Furthermore, converging to a suboptimal cost can significantly lead to larger cost compared to the global optimum.
% To that end, we consider the problem of minimizing the cost presented in \cite{pedram2021gaussian}
% {\color{red} 
% % Items we explain here
% \begin{itemize}
%     \item Why controlled sensing is relevant in autonomy?
%     \item when sensing effort are comparable to control effort?
%     \item motion-sensing co-planning and planning
%     % \item the contribution of the paper
%     % \item Outline of the paper
%     % \item the notation
% \end{itemize}
% }
\section{Related work}






\subsection{Belief space motion planning}

In the continuous state, control, and observation space, the complexity of the POMDP framework makes it difficult to use. 
% As a result, many methods instead provide a nominal path as the solution to the planning problem, which remains fixed regardless of noise or deviations during the execution phase.
Towards the aim of addressing this issue, the area of belief space path planning computes feasible paths for uncertain systems that take into account the uncertainty in the system's dynamics and environment. \cite{censi2008bayesian_censi} proposed a planning algorithm that uses graph search and constraint propagation on a grid-based representation of the space. \cite{platt2010belief_platt_2010} used nonlinear optimization methods to find the best nominal path in continuous space. The linear quadratic Gaussian motion planning (LQGMP) method \cite{van2011lqg_van_2010} computes the best possible feasible path among a finite number of paths generated by RRT by simulating the performance of an LQG controller on all of them. \cite{bry2011rapidly_bry_and_roy} utilized a tree-based approach to optimize the underlying nominal trajectory using RRT*. \cite{vitus2011closed_related_to_cc_3} also addressed the optimization of the underlying trajectory by formulating it as a chance-constrained optimal control problem. 
% extended the LQGMP (Linear Quadratic Gaussian Model Predictive control) to include roadmaps, which are used to plan paths.
\cite{prentice2009belief_brm}  also used roadmap-based methods based on the Probabilistic Roadmap (PRM) approach, where the optimal path is found through a breadth-first search on the belief roadmap (BRM). However, in all of these roadmap-based methods, the optimal substructure assumption is violated, meaning that the costs of different edges on the graph depend on one another. In \cite{kurniawati2012global}, the point-based POMDP (Partially Observable Markov Decision Process) planner takes into account uncertainties in motion, observation, and mapping, and improves upon previous point-based methods through the use of guided cluster sampling. This method starts with a roadmap in the configuration space and grows a single-query tree in the belief space, rooted in the initial belief. \cite{roy1999coastal} examines the use of a coastal navigation strategy to assist the agent's perception during navigation. 

% Belief space motion planning is a probabilistic approach to motion planning that involves representing the robot's belief about the state of the environment as a distribution over possible states, and then using this belief to plan the most likely path to the goal. 

% Belief space path planning is a technique used to find paths for uncertain systems that take into account the uncertainty in the system's dynamics and environment. 
% There have been several studies on this topic in the literature, including research on path planning for fully observable and geometrically known environments with uncertain dynamics \cite{alterovitz2007stochastic_uncertain_dynamics}, as well as approaches that incorporate sensing uncertainty and provide feedback-based information based roadmaps \cite{agha2014firm}. The belief-space probabilistic roadmap (BRM) \cite{prentice2009belief_brm} method uses a factored form of the covariance matrix to efficiently predict posterior beliefs. Another study examined the use of a coastal navigation strategy to assist the agent's perception during navigation. Additionally, the concept of safe path planning in the belief space has been established to compute a feasible path with a safety guarantee.

\subsection{Chance constrained motion planning}
Chance constrained motion planning is a class of planning algorithms that take into consideration probabilistic safety constraints. This approach is closely related to the field of belief space path planning, which involves finding paths that meet specific safety requirements under uncertainty \cite{blackmore2006probabilistic_related_to_cc_1,blackmore2011chance_related_to_cc_2,vitus2011closed_related_to_cc_3}. There have been efforts to extend basic chance-constrained methods, which are primarily designed for linear-Gaussian systems, to handle more complex, non-linear, and non-Gaussian problems, as well as to address joint chance constraints \cite{blackmore2010probabilistic_cc_extend_1,wang2020non_cc_extend_2,ono2015chance_cc_extend_3}. However, these formulations can be computationally expensive and may not scale well. To address this issue, several methods were proposed to improve scalability, while others have developed sampling-based approaches like CC-RRT (chance-constrained rapidly-exploring random tree) that allow for efficient computation of feasible paths. The CC-RRT algorithm has been generalized for use in dynamic environments, and several variants have been introduced that guarantee convergence to the optimal trajectory. 
Tree-based planners with chance constraints have been shown to be efficient and have been used in unknown environments through iterative planning in several studies \cite{luders2010chance_unknown_1,pairet2021online_unknown_2,plaku2010motion_unknown_3}. Some studies have also extended these frameworks to systems with measurement uncertainty, using maximum-likelihood observations to approximate solutions \cite{platt2010belief_platt_2010}, although this approach does not provide safety guarantees. 
% In contrast, the approach in -- combines optimal control and state estimation with sampling-based graphs to provide chance-constrained guarantees, finding an optimal trajectory through the belief space by constructing a graph of trajectories in the state space and enumerating all possible uncertainty levels for these trajectories in the belief space.
% Chance constrained motion planning involves finding a feasible trajectory for a robot or vehicle to follow while taking into account uncertain parameters such as sensor noise or dynamic obstacles. This can be challenging due to the need to consider a range of possible outcomes and ensure that the chosen trajectory is feasible for all of them.

% One approach to chance constrained motion planning is to use probabilistic models to represent the uncertainty in the environment and optimize the trajectory based on these models. For example, in, the authors propose a method for chance constrained motion planning using Gaussian processes to model the uncertainty in the position of dynamic obstacles. The trajectory is then optimized using a chance constrained optimization algorithm, which ensures that the probability of collision with the obstacles is below a specified threshold.
% Another approach is to use sampling-based methods to generate a set of feasible trajectories and select the best one based on some performance criterion. In , the authors present a method for chance constrained motion planning using randomized sampling to generate a set of candidate trajectories and then selecting the one with the highest probability of feasibility. This approach has the advantage of being more computationally efficient than optimization-based methods, but it can be less accurate due to the reliance on sampling.
% In, the authors propose a hybrid approach that combines optimization and sampling to improve the efficiency and accuracy of chance constrained motion planning. The method uses optimization to generate a set of initial trajectories and then refines them using sampling to ensure feasibility under uncertain conditions. This approach has been shown to achieve good performance on a range of motion planning tasks.
% Other research has focused on developing efficient algorithms for specific types of chance constrained motion planning problems. For example, in , the authors present a method for chance constrained motion planning under uncertain dynamic constraints, such as the acceleration and jerk of the vehicle. The method uses a combination of optimization and sampling to generate feasible trajectories that satisfy the constraints.
\subsection{Information theoretic path planning}
In an objective function of belief space path planning, an information-theoretic cost is a measure of uncertainty in the system, which is usually quantified using mutual information. 
% To calculate this cost, we need to evaluate the expected posterior belief after an action is taken.
For Gaussian distributions, calculation of the cost usually involves computing the determinant of a posteriori covariance matrix, which has a complexity of $O(n^3)$ in general cases, where $n$ is the state dimension. This computation needs to be performed for each potential action. In \cite{indelman2015planning_inf1}, a method was proposed to address this challenge by using information form and exploiting sparsity, but this still requires expensive access to marginal probability distributions. The rAMDL approach  \cite{kopitkov2017no_inf2} performs a one-time calculation of the marginal covariances of the variables involved in the candidate actions, and then uses an augmented matrix determinant lemma (AMDL) to efficiently evaluate the information-theoretic cost for each action. However, this method still requires the recovery of appropriate marginal covariances, the complexity of which depends on the state dimensionality and the sparsity of the system. \cite{levine2013information_inf3} focused on finding ways for sensing agents to both gather as much information as possible about their target or environment, as measured by the Fisher information matrix, and minimize the cost of reaching their goal. In \cite{folsom2021scalable_inf4}, a Mars helicopter used an RRT*-IT algorithm to explore the surface of Mars and reduce uncertainty about the terrain type in the shortest amount of time. 
% Another study suggested using information theory to create abstractions of the search space and perform path planning in this simplified representation in order to reduce computational complexity.
% {\color{red} a) sampling-based method and theoretical guarantees\\
% b) motion planning under uncertainty, and belief space}

\subsection{Technical contributions}
The technical contributions of this paper are as follows.
\begin{enumerate}
    \item We consider the shortest path problem for minimum sensing navigation in Gaussian belief space with respect to a cost function that represents the weighted sum of the Euclidean and the information-theoretic sensing cost and propose a sampling-based motion planning algorithm termed Information Geometric PRM* (IG-PRM*) algorithm. 
    \item We prove that the IG-PRM* algorithm is asymptotically optimal. One of the main challenges in establishing asymptotic optimality in Gaussian belief space as compared to deterministic space is the characterization of the volume of covariances and the computation of the associated probability of sampling these covariance matrices. To address this challenge, our second contribution is based on the following two novel results. First, we provide an analytical expression for computing the volume of covariance matrices in the space of symmetric matrices equipped with Rao-Fisher metric and derive a lower bound for this volume in terms of the Selberg integral. Second, we provide a lower bound for the probability of sampling these covariance matrices. These two results aid in establishing the asymptotic optimality of IG-PRM*.
    % \end{itemize}
    To the best knowledge of the authors, this is the first paper that addresses the problem of asymptotic optimality in Gaussian belief space.
    \item Through numerical simulations, we verify our claim that the cost returned by IG-PRM* in the absence of any obstacle converges to the optimal cost in the limit of large number of samples. The optimal cost can be analytically computed using the \say{move and sense} strategy in the literature \cite{pedram2021gaussian}.
\end{enumerate}
% We consider the shortest path problem for minimum sensing navigation in Gaussian belief space with respect to a cost function that represents the weighted sum of the Euclidean and the sensing cost and propose a sampling-based motion planning algorithm termed Information Geometric PRM* (IG-PRM*) algorithm. 
%We prove that the IG-PRM* algorithm is asymptotically optimal. One of the main challenges in establishing asymptotic optimality in Gaussian belief space as compared to deterministic space is the characterization of the volume of covariances and the computation of the associated probability of sampling these covariance matrices. To address this challenge, our second contribution is based on the following novel results: First, we provide an analytical expression for computing the volume of covariance matrices in the space of symmetric matrices equipped with Rao-Fisher metric and derive a lower bound for this volume in terms of the Selberg integral. Second, we provide a lower bound for the probability of sampling these covariance matrices which aids in establishing the asymptotic optimality of IG-PRM*.
 % To the best knowledge of the authors, this is the first paper that addresses the problem of asymptotic optimality in Gaussian belief space.
 % Through numerical simulations, we verify our claim that the cost returned by IG-PRM* in the absence of any obstacle converges to the optimal cost in the limit of large number of samples. The optimal cost can be analytically computed using the "move and sense" strategy in the literature \cite{pedram2021gaussian}.
% \begin{itemize}
%     \item We introduce information-geometric PRM* algorithm termed Information Geometric PRM* (IG-PRM*) in Gaussian belief space. In contrast to standard PRM* in deterministic configuration space that samples states and look for possible connections between these states using collision checking modules, we sample covariances along with the states in the belief space. 
%     % In addition, we propose of collision free checking strategy for IG-PRM* in belief space. 
%     \item Towards the aim of guaranteeing asymptotic optimality for IG-PRM*, we derive the lower bound for the volume of positive definite matrices under matrix inequality constraints. 
%     % This allows to compute the lower bound for the probability that a sampled covariance matrix lies between
%     % formula for computing the probability that the sampled point lies in a set which constitutes points belonging to the belief space.
%     % sampling based motion planning algorithms in Gaussian belief space, we propose Information Geometric PRM* (IG-PRM*) that aims to minimize the weighted sum of Euclidean and the information theoretic cost. We then  prove the asymptotic optimality of the proposed IG-PRM*.
%     \item We prove the asymptotic optimality of the proposed IG-PRM* in Gaussian belief space. To the best knowledge of the authors, this is the first paper which addresses the problem of asymptotic optimality in Gaussian belief space. Numerical simulations are provided to verify the convergence of the cost generated by IG-PRM* to the global optimum as the number of samples increases.
% \end{itemize}
% In this paper, we prove the asymptotic optimality of the proposed IG-PRM* sampling based motion planning algorithm that is used to compute a reference path in the Gaussian belief space such that the path length with to the quasi-pseudo metric is minimized. The quasi-pseudo metric is defined in this paper as the weighted sum of the Euclidean cost and the information cost that is required to steer the belief state.
\subsection{Outline of the paper}
The paper is organized as follows. Section~\ref{sec:notation} discusses the nomenclature followed by preliminaries in Section \ref{sec:prelim}. Section \ref{sec:problem_formulation} discusses the problem statement we address in the paper followed by the proposed algorithm and main result in Sections~\ref{sec:algorithm} and \ref{sec:optimality} respectively. In Section \ref{sec:lossless_ri_prm_star}, we extend the proposed algorithm. Finally, Section \ref{sec:experiments} discusses the numerical simulations and verify the claims made in the paper followed by some concluding remarks in Section~\ref{sec:conclusion}.

\subsection{Notation and convention} \label{sec:notation}
Matrices and vectors are represented by uppercase and lowercase letter respectively. The following notation will be used. $\mathbb{S}^d=\left\{P\in\mathbb{R}^{d \times d}: P \text{ is symmetric.} \right\}$, $\mathbb{S}_{+}^d=\left\{P\in\mathbb{R}^{d \times d}: P\succ 0 \right\}$ and $\mathbb{S}_\rho^d=\left\{P\in\mathbb{S}^d: P\succeq \rho I,\;\rho>0 \right\}$. $\mathcal{E}_{\chi^2}(x_0, P_0)=\{x\in \mathbb{R}^d: (x-x_0)^\top P_0^{-1}(x-x_0)< \chi^2 \}$ is the confidence ellipse. Throughout this paper, we assume that the value of $\chi^2$ is fixed. $\mathcal{B}(x_0,r_0)=\{x\in\mathbb{R}^d:(x-x_0)^\mathrm{T}(x-x_0)\leq r_0\}$ is the ball with center at $x_0$ and radius $r_0$. $\lambda_i(X)$ for all $i\in\{1,2,\dots, d\}$ denote the eigenvalues of a positive definite matrix $X$ and without loss of generality, we assume that $\lambda_1\leq \lambda_2\dots\leq\lambda_d$. For integers $a$ and $b(\geq a)$, $[a;b]$ denotes the set $\{a,a+1,\dots,b\}$. $\mathcal{N}(x,P)$ represents a Gaussian random variable with mean $x$ and covariance $P$. $\Bar{\sigma}(M)$ denotes the maximum singular value of $M$. The Euclidean and the Frobenius norm are represented by $\|.\|$ and $\|.\|_F$, respectively. 
% \section{Proposed framework}
\section{Preliminaries\label{sec:prelim}}
% { \color{red} 
% a) proposed framework: we explain the main idea like assumed dynamics, Gaussian belief space why sensing effort should be asymmetric by nature\\
% % b) collision checking and lossless  
% }
This section provides a brief overview of our setup, which closely mirrors that of \cite{pedram2021gaussian}. We include this information for the sake of completeness.

\subsection{Dynamic model}

% In this paper, we consider a sampling-based path planning method in which the reference trajectory for a mobile agent is generated as 
Consider a sequence of way points $\{x_k\}_{k\in[1;K]}$ in the configuration space $\mathbb{R}^d$.
Let $t_k$ be the time that the agent is scheduled to visit the $k^\text{th}$ way point $x_k$.
The agent is assumed to apply a constant velocity input
\[
v(t) = v_k := \frac{x_{k+1}-x_k}{t_{k+1}-t_k}
\]
for $t_k \leq t < t_{k+1}, k\in[1;K-1]$.
We assume that the agent motion is subject to stochastic disturbance.
Let $\bx(t_k)$ be the random vector representing the robot's actual position at time $t_k$. It is assumed to satisfy
\begin{align}
 \label{eq:dynamics}
\bx(t_{k+1})=\bx(t_k)+(t_{k+1}-t_k)v_k+\bn_k
\end{align}
where $\bn_k \sim\mathcal{N}(0, \|x_{k+1}-x_k\|W)$.
Note that the constant velocity input and dynamics \eqref{eq:dynamics} are assumed exclusively for the purpose of establishing a distance concept within a Gaussian belief space as presented in subsequent sections. Despite potential significant differences between actual robot dynamics and \eqref{eq:dynamics}, the algorithm we propose in Section \ref{sec:problem_formulation} remains applicable. This approach is analogous to the widespread use of RRT* (or other sampling based motion planning algorithms) for Euclidean distance minimization in scenarios where Euclidean distance in configuration space fails to accurately represent motion costs. In line with this reasoning, we deliberately adopt a simplistic model \eqref{eq:dynamics} and reserve addressing more accurate dynamic constraints for the path-following control phase.
% is a Gaussian disturbance whose covariance matrix grows linearly with the commanded travel distance.
\subsection{Gaussian belief space}

Let the probability distributions of the robot position at time step $k$ can be characterized by a Gaussian model as $\bx_k\sim\mathcal{N}(x_k, P_k)$, where $x_k\in\mathbb{R}^d$ is the mean position and $P_k\in \mathbb{S}_{+}^d$ is the covariance matrix. 
% In this paper, we consider a path planning framework in which the sequence  $\{(x_k, P_k) \}_{k\in\mathbb{N}}$ is scheduled. Following \cite{lambert2003safe, pepy2006safe}, we refer to the product space $\mathbb{R}^d \times \mathbb{S}_{+}^d$ as the \emph{uncertain configuration space}. We formulate a problem of finding the shortest path in the uncertain configuration space with respect to a novel information-theoretic path length function, which will be introduced below.
We first introduce an appropriate directed distance function from a point $(x_k, P_k)\in \mathbb{R}^d \times \mathbb{S}_{+}^d$  to another $(x_{k+1}, P_{k+1})\in \mathbb{R}^d \times \mathbb{S}_{+}^d$. 
The distance function is interpreted as the cost of steering the state random variable $\bx_k\sim\mathcal{N}(x_k, P_k)$ at time $k$ to $\bx_{k+1}\sim\mathcal{N}(x_{k+1}, P_{k+1})$ in the next time step under the dynamics \eqref{eq:dynamics}. 
We assume that the distance function is a weighted sum of the travel cost $\mathcal{D}_{\text{travel}}(k)$ and the information cost $\mathcal{D}_{\text{info}}(k)$.

\subsubsection{Travel cost:}
We assume that the travel cost is simply the commanded travel distance:
\[
\mathcal{D}_{\text{travel}}(k):=\|x_{k+1}-x_k\|
\]
% where $\|\cdot\|$ is the vector 2-norm.

\subsubsection{Information cost:}
% Suppose that a deterministic control input $v_k$ is applied to \eqref{eq:euler3}.
% The covariance at time step $k+1$ will be
% \begin{equation}
% \label{eq:p_prior}
% \hat{P}_{k+1}:=P_k+\|x_{k+1}-x_k\|W.
% \end{equation}
% We refer to $\hat{P}_{k+1}$ as the prior covariance at time step $k+1$.
% Suppose that the prior covariance is ``reduced'' to $P_{k+1}(\preceq \hat{P}_{k+1})$ by utilizing telemetry $\by_{k+1}$ at time step $k+1$.
% The minimum information gain (the minimum number of \emph{bits} that must be contained in the telemetry data $\by_{k+1}$) required for this transition is
% \begin{equation}
% \begin{split}
% \mathcal{D}_{\text{info}}(k)&= h(\bx_{k+1})-h(\bx_{k+1}|\by_{k+1}) \\
% &=\frac{1}{2}\log\det \hat{P}_{k+1} - \frac{1}{2}\log\det P_{k+1} 
% \end{split} \label{eq:info_gain1}
% \end{equation} 
% where $h(\cdot)$ and $h(\cdot|\cdot)$ are differential entropy and conditional differential entropy, respectively.
We define the information-theoretic cost function $\mathcal{D}_{\text{info}}(k)$ at time step $k$
% in \eqref{eq:info_gain1} is well-defined for the pairs $(P_k, P_{k+1})$ satisfying $P_{k+1}\preceq \hat{P}_{k+1}$. For a pair $(P_k, P_{k+1})$ that does not satisfy $P_{k+1}\preceq \hat{P}_{k+1}$, we generalize \eqref{eq:info_gain1}
as follows:
\begin{subequations}
\label{eq:d_info_general0}
\begin{align}
\mathcal{D}_{\text{info}}(k)=\!\!\min_{Q_{k+1}\succeq 0}\!\! & \quad \frac{1}{2}\log\det \hat{P}_{k+1}-\frac{1}{2}\log\det Q_{k+1} \label{eq:d_info_general}\\
\text{s.t.} &\quad Q_{k+1} \preceq P_{k+1}, \;\; Q_{k+1} \preceq \hat{P}_{k+1}.\label{eq:d_info_general1}
\end{align}
\end{subequations}
Notice that for any given pair of the origin $(x_k, P_k)$ and the destination $(x_{k+1}, P_{k+1})$, \eqref{eq:d_info_general} takes a nonnegative value. For more details and motivation for using this cost function, we suggest the readers refer to \cite{pedram2021gaussian}.

% However, \eqref{eq:d_info_general} is an implicit function involving a convex optimization problem (more precisely, the max-det problem \cite{vandenberghe1998determinant}) in its expression.

% To see why \eqref{eq:d_info_general} is an appropriate generalization of \eqref{eq:info_gain1}, consider a two-step procedure $\hat{P}_{k+1}\rightarrow Q_{k+1}\rightarrow P_{k+1}$ to update the prior covariance $\hat{P}_{k+1}$ to the posterior covariance $P_{k+1}$. In the fist step, the uncertainty is ``reduced'' from $\hat{P}_{k+1}$ to $Q_{k+1}(\preceq \hat{P}_{k+1})$. The associated information gain (the amount of telemetry data) is $\frac{1}{2}\log\det \hat{P}_{k+1}-\frac{1}{2}\log\det Q_{k+1}$.
% In the second step, the covariance $Q_{k+1}$ is ``increased'' to $P_{k+1}(\succeq Q_{k+1})$.
% This step incurs no information cost, since the location uncertainty can be increased simply by ``forgetting'' the prior knowledge. The max-det problem \eqref{eq:d_info_general} is interpreted as finding the optimal intermediate step $Q_{k+1}$ to minimize the information gain in the first step.

\subsubsection{Total cost:}
The total cost to steer the state random variable $\bx_k\sim\mathcal{N}(x_k, P_k)$ to $\bx_{k+1}\sim\mathcal{N}(x_{k+1}, P_{k+1})$ is a weighted sum of $\mathcal{D}_{\text{travel}}(k)$ and $\mathcal{D}_{\text{info}}(k)$ as
\begin{align}
\label{eq:def_D}
&\mathcal{D}(x_k, x_{k+1}, P_k, P_{k+1}):= \; \mathcal{D}_{\text{travel}}(k)+\alpha \mathcal{D}_{\text{info}}(k),
\end{align}
where $\alpha\geq 0$ is the weight factor.
\subsection{Chains and Paths}
Suppose that a sequence $\{(x_k, P_k)\}_{k\in[1;K-1]}$ is given. In what follows, the sequence of transitions from $(x_k, P_k)$ to $(x_{k+1}, P_{k+1})$, ${k\in[1;K-1]}$ will be referred to as a \emph{chain}. 
% A function $\gamma: [0,1]\rightarrow \mathbb{R}^d\times \mathbb{S}_{+}^d$, $\gamma(t)=(x(t), P(t))$ will be referred to as a \emph{path}. The start and the end point of the path are $\gamma(0)$ and $\gamma(1)$, respectively. 
Notice that the parameter \say{$t$} does not necessarily correspond to the physical time. The time of arrival of the agent at the end point depends on the length of the path and the travel speed of the robot. 

\subsubsection{Lossless Chains and Paths:}
A transition from $(x_k, P_k)$ to $(x_{k+1}, P_{k+1})$ is said to be \emph{lossless} if
\begin{equation}
P_{k+1} \preceq \hat{P}_{k+1}(:=P_k+\|x_{k+1}-x_k\|W)
\end{equation}
% Since a lossless transition does not involve the \say{forgetting} step, the corresponding information cost $\mathcal{D}_{\text{info}}(k) $ is always given by \eqref{eq:d_info_general}.
If every transition in the sequence $\{(x_k, P_k)\}_{k\in[1;K]}$ is lossless, we say that the sequence is lossless.
Let $\gamma: [0,T]\rightarrow \mathbb{R}^d\times \mathbb{S}_{+}^d$, $\gamma(t)=(x(t), P(t))$ be a path. In what follows, the variable $t$ is called the time parameter.
The {travel length} of the path $\gamma$ from time $t=t_a$ to time $t=t_b$ is defined as
\[
\ell(\gamma_x[t_a, t_b])=\sup_{\mathcal{P}} \sum_{k=1}^K \|x(t_k)-x_{k+1}\|
\]
where the supremum is over the space of all partitions $\mathcal{P}=(t_a=t_0<t_1<\cdots < t_K=t_b)$. We say that a path $\gamma$ is \emph{lossless} if the condition
\begin{align}
P(t_b) \preceq P(t_a)+\ell(\gamma_x[t_a, t_b])W
\label{eqn:P_growth}
\end{align}
for any $0\leq t_a < t_b \leq T$. Notice that the right hand side of \eqref{eqn:P_growth} is the covariance of the position of the robot which had the initial configuration $(x(t_a),P(t_a)$ and traveled along the path $\gamma_x[t_a, t_b]$ under the dynamics \eqref{eq:dynamics} without sensing. Therefore, the inequality \eqref{eqn:P_growth} means that the growth of uncertainty from $P(t_a)$ to $P(t_b)$ is never greater that the "natural growth" $\ell W$, which is always true in realistic navigation scenarios. A path $\gamma$ is said to be \emph{finitely lossless} if there exists a finite $N$ and a partition $\mathcal{P}=(0=t_0<t_1<\cdots < t_K=T)$ such that for each $k\in[1;K]$, the transition from $(x(t_k), P(t_k))$ to $(x'(t_k), P'(t_k))$ is lossless


\begin{remark}
\normalfont If a path $\gamma$ is finitely lossless with respect to a partition $\mathcal{P}$, then it is also finitely lossless with respect to a partition $\mathcal{P}'$, provided $\mathcal{P}' \supseteq \mathcal{P}$ (i.e., $\mathcal{P}'$ is a refinement of $\mathcal{P}$). Based on this observation, it can be shown that if a path is finitely lossless then it is lossless. However, the converse is not always true.
\end{remark}

\subsubsection{Collision-free Chains and Paths:}
Let $\mathcal{X}_{\text{obs}}\subset \mathbb{R}^d$ be a closed subset representing obstacles. 
Consider a transition from $x_k$ to $x_{k+1}$. The robot's mean position during this transition is parameterized as
\[
x(\lambda)=(1-\lambda)x_k+\lambda x_{k+1}\;\;\forall\;\;\lambda\in[0,1].
\]
Assuming that the initial covariance is $P_k$, the evolution of the covariance matrix is written as
\[
P(\lambda)=P_k+\lambda\|x_{k+1}-x_k\|W\;\;\forall\;\;\lambda\in[0,1].
\]
For a fixed confidence level parameter $\chi^2>0$, we say that the transition from $(x(0), P(0))$ to $(x(1), P(1))$ is \emph{collision-free} if
\begin{align} \nonumber
&(x(\lambda)-x_{\text{obs}})^\top P(\lambda)^{-1}(x(\lambda)-x_{\text{obs}}) \geq \chi^2,\\
&\quad \quad \quad \forall\; \lambda\in [0,1], \quad \forall x_{\text{obs}}\in \mathcal{X}_{\text{obs}}.\nonumber
\end{align}
\begin{remark}
\normalfont A collision is detected when 
\begin{equation}
(x(\lambda)-x_{\text{obs}})^\top P(\lambda)^{-1}(x(\lambda)-x_{\text{obs}}) < \chi^2\quad\nonumber
\end{equation}
for all $ \lambda\in [0,1]$ and $x_{\text{obs}}\in \mathcal{X}_{\text{obs}}$. The process of detecting collisions can be thought of as a problem of determining whether a particular set of conditions is feasible which is a convex program for each convex obstacle $\mathcal{X}_{\text{obs}}$ \cite{pedram2021gaussian}.
\begin{align} 
\nonumber
&\begin{bmatrix}
\chi^2 &  (1\!-\!\lambda)x_k^\top\!+\!\lambda x_{k+1}^\top\!-\!x_{\text{obs}}^\top \\
(1\!-\!\lambda)x_k\!+\!\lambda x_{k+1}\!-\!x_{\text{obs}} & P_k+\lambda\|x_{k+1}-x_k\|W
\end{bmatrix}\succ 0,\\ \label{eq:col_free}
& \quad \quad \quad \quad \quad   0\leq \lambda \leq 1,  \quad x_{\text{obs}}\in \mathcal{X}_{\text{obs}} 
\end{align}
% which is a convex program for each convex obstacle $\mathcal{X}_{\text{obs}}$.
\label{remark:collision_free}
\end{remark}
We say that a chain $\{(x_k, P_k)\}_{k\in[1;K]}$ is {collision-free} if for each $k\in[1;K-1]$, the transition from $x_k$ to $x_{k+1}$ with the initial covariance $P_k$ is collision-free. We say that a path $\gamma: [0,1]\rightarrow \mathbb{R}^d\times \mathbb{S}_{+}^d$, $\gamma(t)=(x(t), P(t))$ is {collision-free} if
\begin{align} \nonumber
&(x(t)-x_{\text{obs}})^\top P^{-1}(t)(x(t)-x_{\text{obs}}) \geq \chi^2,\\
& \quad \quad \quad \forall\; t\in [0, 1], \quad \forall\; x_{\text{obs}}\in \mathcal{X}_{\text{obs}}.
\end{align}



\section{Problem Formulation}
\label{sec:problem_formulation}
\subsection{Path length}
Let $\gamma: [0,1]\rightarrow \mathbb{R}^d\times \mathbb{S}_{+}^d$, $\gamma(t)=(x(t), P(t))$ be a path, and $\mathcal{P}=(0=t_0<t_1<\cdots < t_{K_n}=1)$ be a partition.
The length of the path $\gamma$ with respect to the partition $\mathcal{P}$ is defined as
\begin{equation}
c(\gamma;\mathcal{P})=\sum_{k=1}^{K_n-1} \mathcal{D}(x(t_k), x(t_{k+1}), P(t_k), P(t_{k+1}))
\end{equation}
The length of a path $\gamma$ is defined as the supremum of $c(\gamma;\mathcal{P})$ over all partitions
\begin{equation}
\label{eq:def_path_length}
c(\gamma):=\sup_\mathcal{P} c(\gamma;\mathcal{P}).
\end{equation}
The definition \eqref{eq:def_path_length} states that for any path with a finite length, there exists a sequence of partitions
$\{\mathcal{P}_i\}_{i\in\mathbb{N}}$ such that
$\underset{n\rightarrow\infty}{\lim}\; c(\gamma; \mathcal{P}_i) =c(\gamma)$.

\subsection{Topology on the path space}
% In this subsection, an appropriate topology on the space of belief paths $\gamma:[0, T] \rightarrow$ $\mathbb{B}$ is introduced with respect to which the path length function $c(\gamma)$.
The space of generalized paths is a vector space on which addition and scalar multiplication are defined as $\left(\gamma_1+\gamma_2\right)(t)=\left(x_1(t)+x_2(t), P_1(t)+P_2(t)\right)$ and $a \gamma(t)=(a x(t), a P(t))$ for $a \in \mathbb{R}$, respectively. Let $\mathcal{P}=\left(0=t_0<t_1<\cdots<t_K=T\right)$ be a partition. The total variation of a generalized path $\gamma$ with respect to $\mathcal{P}$ is defined as $|\gamma|_{\mathrm{TV}}=\underset{{\mathcal{P}}}{\sup}\; \|x(0)\| \bar{\sigma}(W)+\bar{\sigma}(P(0))+\sum_{k=0}^{K-1}\left[\| x\left(t_{k+1}\right)-\right.$ $\left.x\left(t_k\right) \| \bar{\sigma}(W)+\bar{\sigma}\left(P\left(t_{k+1}\right)-P\left(t_k\right)\right)\right]$. 
% For more details, please refer to \cite{pedram2021gaussian}.
\subsection{Problem statement}
Given an initial belief state $b_0=\left(x_0, P_0\right) \in \mathbb{B}$ be a given initial belief state, a closed subset $\mathcal{B}_{\text {target }} \subset$ $\mathbb{B}$ representing the desired target belief region, and $X_{\text {obs }}^m \subset \mathbb{R}^d$ be the given obstacle $m \in\{1, \ldots, M\}$ where $M\in\mathbb{N}$. Given a confidence level parameter $\chi^2>0$, the problem is to find the shortest path, and can be formulated  as
\begin{align}
\begin{array}{cl}
&\underset{{\gamma \in \mathcal{B} \mathcal{V}[0, T]}}{\min}\;\; c(\gamma) \\
&\text { s.t. }  \gamma(0)=b_0,\;\;\; \gamma(T) \in \mathcal{B}_{\text {target }} \\
& \left(x(t)-x_{\text {obs }}\right)^{\top} P^{-1}(t)\left(x(t)-x_{\text {obs }}\right) \geq \chi^2 \\
& \forall t \in[0, T], \quad \forall x_{\text {obs }} \in X_{\text {obs }}^m, \quad \forall\; m \in\{1, \ldots, M\} .
\end{array}
\label{eqn:problem_statement}
\end{align}
In addition, the proposed algorithm must guarantee that the generated feasible chain converges to the global optimal cost $c^\star$ as the number of samples tends to infinity (Section \ref{sec:asy_opt} ).
\begin{assumption}
\normalfont    We assume there exists a feasible path $\gamma(t)=(x(t),P(t))$ for {the formulated problem} \eqref{eqn:problem_statement} such that $P(t) \in \mathbb{S}^d_{4\rho}$ and $\textup{Tr}(P(t))\leq R$\; for all $t\in[0,1]$ and $R>0$. 
    % For the sake of calculation, we define $s:=\frac{16\rho d}{R} \leq 1.$
\end{assumption}
\subsection{Continuity of path cost}
% \begin{assumption}
% \normalfont    We assume there exists a feasible path $\gamma(t)=(x(t),P(t))$ for {the formulated problem} \eqref{eqn:problem_statement} such that $P(t) \in \mathbb{S}^d_{4\rho}$ and $\textup{Tr}(P(t))\leq R$\; for all $t\in[0,1]$ and $R>0$. 
%     % For the sake of calculation, we define $s:=\frac{16\rho d}{R} \leq 1.$
% \end{assumption}



\begin{theorem}
\label{theo:cont}
\normalfont Let $\gamma: [0,1]\rightarrow \mathbb{R}^d\times \mathbb{S}_\rho^d$ and $\gamma': [0,1]\rightarrow \mathbb{R}^d\times \mathbb{S}_\rho^d$ be paths. Suppose  $\gamma \in \mathcal{BV}[0, 1]$ and $\gamma' \in \mathcal{BV}[0, 1]$ and they are both finitely lossless. Then, for each $\epsilon > 0$, there exists $\delta > 0$ such that
\[
|\gamma'-\gamma|_{\text{TV}}\leq \delta \quad \Rightarrow \quad  |c(\gamma')-c(\gamma)| \leq \epsilon.
\]
\end{theorem}
\begin{proof}
Please see \cite[Appendix~D]{pedram2021gaussian}
\end{proof}

% \textcolor{blue}{
\subsection{Asymptotic optimality\label{sec:asy_opt}}
Let $Y_n$ denote the cost of the least cost solution returned by a sampling-based motion planning algorithm in $n$ iterations. We define $c^\star = \inf\{c(\gamma) :\; \gamma\; \text{is a feasible path}\}$. An algorithm is said to be asymptotically optimal if
\begin{align}
P(\{\underset{n\rightarrow\infty}{\limsup}\;Y_n =c^\star\}) = 1 
\end{align}
% }
\section{IG-PRM* Algorithm\label{sec:algorithm}}
In section, we first present the notion of uniformly sampling covariances followed by the IG-PRM* algorithm.
\subsection{Uniform sampling of covariance}
% In this section, we derive some primary technical results that we use for proof of optimality in Section~\ref{sec:optimality}.

% \begin{remark}\label{remark:semi-axis}
% \normalfont The length of minor semi-axis of confidence ellipse $\mathcal{E}_{\chi^2}(x,P)$ is $\sqrt{\chi^2 \underline{\sigma}(P)}$. Consequently, the minimum distance between the boundary of co-centric, similar ellipses   $\mathcal{E}_{\chi^2}(x,P)$ and  $\mathcal{E}_{\chi^2}(x, \alpha P)$, where $\alpha > 0$, is $\sqrt{\chi^2 |1-\alpha|\underline{\sigma}(P)}$. 
% \end{remark}
To introduce a sampling-based planning algorithm in a Gaussian belief space, we will need a mechanism that allows us to randomly generate candidate covariance matrices. We use the algorithm proposed in \cite{mittelbach2012sampling} for uniformly sampling covariance $P\in \mathbb{S}^d_{+}$ in  $\mathcal{R}_{(\underline{c},\bar{c}]}:=\{P\in \mathbb{S}^d_{+}: \underline{c} < \textup{Tr}(P)\leq \bar{c}\}$. The sampled covariance $P$ is said to have uniform distribution  on $\mathcal{R}_{[\underline{c},\bar{c}]}$ if
\begin{align} \label{eq:uniform_sampling}
    \mathbb{P}(\{P\in\mathcal{A}\})=\frac{\text{vol}(\mathcal{A}\cap \mathcal{R}_{[\underline{c}, \bar{c} ]})}{\text{vol}(\mathcal{R}_{[\underline{c} ,\bar{c} ]})}
\end{align}
holds for all $\mathcal{A} \subset \mathbb{S}^{d}_+$. We denote by set $\mathcal{R}:=\{P\in \mathbb{S}^d_{+}: \textup{Tr}(P)=1\}$. 
% The proposed sampling is performed in two stages. In the first stage, a sampling method is developed that generates samples from random variable $\bM:\mathcal{R}\rightarrow \mathbb{R}$ such that $\bM$ has uniform distribution, where $\mathcal{R}:=\{P\in \mathbb{S}^d_{+}: \textup{Tr}(P)=1\}$. 
% Next, it is shown that the product of  distribution $\bM$ and the distribution of a certain random variable $\bt:(\underline{c},\bar{c}]\rightarrow \mathbb{R}$ is transformed into a uniform distribution on $\mathcal{R}_{(\underline{c},\bar{c}]}$ by the one-to-one correspondence. More precisely, if  $\bt$ is independent of $\bM$ and has the probability density function of $t_0 t^{\frac{d(d+1)}{2}-1} \bm{1}_{(\underline{c},\bar{c}]}$, then $\bt\bM$ has a uniform distribution over $\mathcal{D}_{(\underline{c},\bar{c}]}$, where $t_0= (\bar{c}^{\frac{d(d+1)}{2}} - \underline{c}^{\frac{d(d+1)}{2}})^{-1}$ is the normalizing factor.
We assume the space of $\mathbb{S}^d_{+}$ is equipped with the Rao-Fisher metric \cite{terras2012harmonic}, and use this metric to measure the volume of different regions in $\mathcal{S}^d_{+}$ in the following Theorem. 
% \begin{theorem}
%    \normalfont \cite{tempo2013randomized} The volume of the region $\mathcal{G}(r)=\{A\in \mathbb{S}^d_{+}: A=U\Sigma U^\mathrm{T},\;\Sigma:=\textup{diagonal}(a_1, \dots, a_d),\;\underset{i\in[1;d]}{\max} \;a_i\leq r\}$ is given by
%     \begin{align}
%          \textup{Vol}(\mathcal{G}(r))=\pi^{\frac{d(d-1)}{4}}\prod_{i=1}^d\frac{\Gamma^2(\frac{i+1}{2})}{\Gamma(\frac{n+i}{2}+1)}r^{d^2}
%     \end{align}
% \end{theorem}
% \begin{proof}
%     Please refer to Remark 17.9 of \cite{tempo2013randomized}.
% \end{proof}
\begin{theorem}
  \normalfont \cite{mittelbach2012sampling} The volume of region $\mathcal{R}_{(c_1,c_2]}$ where $c_2>c_1$ is given by
   \begin{align}
   \label{eq:vol_Rc}
       &\textup{Vol}(\mathcal{R}_{(c_1,c_2]})\nonumber\\
       &=\left(\frac{c_2^{\frac{d(d+1)}{2}}-c_1^{\frac{d(d+1)}{2}}}{\frac{d(d+1)}{2}}\right)V_r\nonumber\\
       &=\left(\frac{c_2^{\frac{d(d+1)}{2}}-c_1^{\frac{d(d+1)}{2}}}{\frac{d(d+1)}{2}}\right)\pi^{\frac{1}{4} d(d-1)} \frac{\prod_{k=2}^d \Gamma\left(\frac{k+1}{2}\right)}{\Gamma\left(\frac{d(d+1)}{2}\right)}
   \end{align}
   where $V_r=\textup{Vol}(\mathcal{R})$ and $\Gamma$ is the gamma function.
\end{theorem}
\begin{proof}
    Please refer to \cite{mittelbach2012sampling}.
\end{proof}
\begin{theorem}
\label{theo:volume}
\normalfont The volume of region $\mathcal{D}_A=\{Q\in \mathbb{S}^d_{+}: Q \preceq A:=\textup{diagonal}(a_1, \dots, a_d)\}$ is lower bounded as
\begin{align}
\label{eq:vol_DA}
    \textup{Vol}(\mathcal{D}_A) \geq V_db^{\frac{d(d+1)}{2}}S_d(1,1,1/2)=\textup{Vol}(\mathcal{D}'_A)
\end{align}
where $b=\underset{k\in[1;d]}{\min}\;a_k$, $\mathcal{D}'_{A}=\{P\in\mathbb{S}^d_+:\;P\preceq b I \}$, $V_d = (d! \; 2^d)^{-1} \frac{2^d \pi^{d^2/2}}{\Gamma_d(d/2)} $ and $S_d(\alpha_1,\alpha_2,\alpha_3)$ is the Selberg integral given by
\begin{align}
     &S_d(\alpha_1, \alpha_2, \alpha_3)\nonumber\\
     &=\int_0^1 .. \int_0^1 \prod_{i=1}^d t_i^{\alpha_1-1}\left(1-t_i\right)^{\alpha_2-1} \prod_{1 \leq i<j \leq d}\left|t_i-t_j\right|^{2 \alpha_3} d t \nonumber \\ 
     & =\prod_{j=0}^{d-1} \frac{\Gamma(\alpha_1+j \alpha_3) \Gamma(\alpha_2+j \gamma) \Gamma(1+(j+1) \gamma)}{\Gamma(\alpha_1+\alpha_2+(d+j-1) \alpha_3) \Gamma(1+\alpha_3)}\nonumber
\end{align}
Further, for $\beta>1$, we have
\begin{align}
    &\textup{Vol}(\mathcal{D}_{\beta A})-\textup{Vol}(\mathcal{D}_{ A})\nonumber\\
    &\geq V_d (\beta^{\frac{d(d+1)}{2}}-1) b^{\frac{d(d+1)}{2}}S_d(1,1,1/2)
\end{align}
% and $\mathcal{R}_c=\{Q\in \mathbb{S}^d_{+}: \textup{Tr}(Q)\leq c \}$ are computed as
% \begin{align}
% \label{eq:vol_DA}
%     \textup{Vol}(\mathcal{D}_A) = V_d\prod_{i=1}^{d} s_i
% \end{align}
% and 
% \begin{align}
%     % \textup{Vol}(\mathcal{R}_c) =  V_d \; c^{\frac{d(d+1)}{2}} \left(\frac{1}{d}\right)^d,
%      \textup{Vol}(\mathcal{R}_c) &=  V_d c^{\frac{d(d+1)}{2}}\frac{1}{(d-1)!}\left(\frac{1}{d}\right)^{\frac{d}{d-1}}\frac{(d-1)^{(d-1)}}{\prod_{i=2}^d(id-i+1) }
%      \label{eq:vol_Rc}
% \end{align}
where
$\Gamma$ is the gamma function.
% , $s_i=\sum_{j=1}^d a_j^i$ for $i=1, \dots, d-1$, and $s_d=\prod_{j=1}^d a_j$.
\end{theorem}
\begin{proof}
Please, see Appendix~\ref{app:volume}.
\end{proof}

% 
% \begin{remark}
%   \normalfont  Let the regions $\mathcal{D}_A(\beta_1,\beta_2)$ and $\mathcal{G}(r_1,r_2)$ for $\beta_2>\beta_1$ and $ r_2>r_1$ be defined as $\mathcal{D}_A(\beta_1,\beta_2)=\{Q\in \mathbb{S}^d_{+}: \beta_1 A\preceq Q \preceq \beta_2 A,\;A\in \mathbb{S}^d_{+}\}$, and $\mathcal{G}(r_1,r_2)=\{A\in \mathbb{S}^d_{+}: A=U\Sigma U^\mathrm{T},\;\Sigma:=\textup{diagonal}(a_1, \dots, a_d),\;r_1\leq\underset{i\in[1;d]}{\max} \;a_i\leq r_2\}$ respectively. Then, clearly $\mathcal{G}(r_1,r_2)\subset\mathcal{D}_A(r_1,r_2)$. 
%   % This is because $r_1^2\leq\sum_{i=1}^da_i^2\leq r_2^2$ implies $r_1\leq a_i\leq r_2$ for $i\in[1;d]$.
% \end{remark}
% \begin{remark}
%    \normalfont The expression \eqref{eq:vol_Rc} is compatible with the proposed pdf of $\bt$. More precisely, for all $\underline{c} \leq c_1 < c_2 \leq \bar{c}$
% \begin{align*}
% &\mathbb{P}(\{P_i\in\mathcal{D}_{(c_1,c_2]}\})
% = \mathbb{P}(\{t_i\in(c_1,c_2]\})\\
% &=\frac{{c}_2^{\frac{d(d+1)}{2}} - {c}_1^{\frac{d(d+1)}{2}}}{\bar{c}^{\frac{d(d+1)}{2}} - \underline{c}^{\frac{d(d+1)}{2}}}=\frac{\textup{vol}(\mathcal{R}_{c_2})- \textup{vol}(\mathcal{R}_{c_1})}
% {\textup{vol}(\mathcal{R}_{\bar{c}})- \textup{vol}(\mathcal{R}_{\underline{c}})}.
% \end{align*}
% \end{remark}

\begin{lemma}
\label{lemma:diag}
  \normalfont  If $P\in \mathbb{S}^d_{+}$ has the eigenvalue decomposition $P=V^\top \Sigma V$, where $\Sigma$ is diagonal and $V$ is a unitary matrix, then 
\begin{align}
    \textup{Vol}(\mathcal{D}_P)=\textup{Vol}(\mathcal{D}_{\Sigma}),
\end{align}
where $\mathcal{D}_P=\{Q\in \mathbb{S}^d_{+}: Q \preceq P\}$ and $\mathcal{D}_{\Sigma}=\{Q\in \mathbb{S}^d_{+}: Q \preceq \Sigma\}$.
\end{lemma}
\begin{proof}
{Please, see Appendix~\ref{app:diag}}
\end{proof}
The pseudo code for generating positive definite matrices uniformly from $\mathcal{R}_{(\underline{c},\bar{c}]}$ is described in Algorithms 1-4 of \cite{mittelbach2012sampling}.
% From $f_{t}(t)$, it is easy to verify that for $P_i$ generated uniformly from $\mathcal{D}_{(\underline{c},\bar{c}]}$, we have 
% \begin{align}
% \mathbb{P}(\{P_i\in\mathcal{D}_{(c_1,c_2]}\})
% =\frac{{c}_2^{\frac{d(d+1)}{2}} - {c}_1^{\frac{d(d+1)}{2}}}{\bar{c}^{\frac{d(d+1)}{2}} - \underline{c}^{\frac{d(d+1)}{2}}},
% \end{align}
% for all $\underline{c} \leq c_1 < c_2 \leq \bar{c}$.
\subsection{IG-PRM* Algorithm}
The implementation of PRM* in Gaussian belief space (termed as IG-PRM*)
for the introduced cost function \eqref{eq:def_D} is summarized in Algorithm 1. The source code for Algorithm 1 is available at \href{https://github.com/AlirezaPedram/RI-PRMstar}{ https://github.com/AlirezaPedram/RI-PRMstar}. PRM* \cite{karaman2010incremental} creates a probabilistic roadmap in deterministic configuration space by connecting randomly sampled points, and efficiently finds the shortest path between two points while maintaining asymptotic optimality.
At the first glance, Algorithm~1 seems identical to the original PRM*. However, the implementation in Gaussian belief space necessitates the introduction of new functionalities. 


\begin{algorithm}
    % \footnotesize
    { 
        \textbf{Inputs}: $ b_{\text{init}}, \;b_{\text{final}},\;n$ \;
    $\left.B \leftarrow\left\{b_{\text{init}}\right\} \cup\{\text {SampleFree}_{i}\right\}_{i\in [1;n]}\cup\left\{b_{\text{final}}\right\} $\;
    $E \leftarrow \emptyset$\;
    % $(z_1) \leftarrow (z_{\text{init}})$; $E \leftarrow \emptyset$; $G'\leftarrow (z_1,E)$\;
    \For{$b\in B$}{
    $\;\; B_{\text{nbors}} \leftarrow \operatorname{Near}\left(B, b, D_{\text{min}}\right)$\;
    \For{$b_j\in B_{\text{nbors}}$}{
    \If{$\operatorname{CollisionFree}(b, b_j, \chi^2)$ }{$E \leftarrow E \cup\{(b, b_j)\}$\;
    % $b_j\leftarrow \operatorname{LosslessRefine}(b,b_j) $
    }
       % $\quad$ if CollisionFree $(u, v)$ and Lossless$(u, v)$  then $E \leftarrow E \cup\{(u, v)\}$
    }
    }
    $G=(B, E)$\;
    $\gamma_n\leftarrow \;\text{Search}(G, b_{\text{init}}, b_{\text{final}}, k)$\;
    \Return $\gamma_n$
    }
\caption{IG-PRM*}
\label{alg:igpp_body1}
\end{algorithm}
\begin{algorithm}
    % \footnotesize
    {
    \textbf{Inputs}: $G, b_{\text{init}}, b_{\text{final}}, k$ \;
    $N_{b_{\text{init}}} \leftarrow \text{Near}_{\text{from}}(G, b_\text{init},k) $\;
    $N_{b_{\text{final}}} \leftarrow \text{Near}_{\text{to}}(G, b_\text{final},k)$\;
    $\gamma_n\leftarrow \text{ShortestPath}(G, N_{b_{\text{init}}}, N_{b_{\text{final}}})$\;
    \Return $\gamma_n$
    }
\caption{Search Algorithm (\text{Search}($G, b_{\text{init}}, b_{\text{final}}, k$))}
\label{algo:search}
\end{algorithm}

\begin{algorithm}
    % \footnotesize
    { 
     \textbf{Inputs}: $ \gamma_n:=(b_1\rightarrow b_1\dots\rightarrow b_m)$ \;
    % $b_0\leftarrow b_{\text{init}}$\;
    % $(z_1) \leftarrow (z_{\text{init}})$; $E \leftarrow \emptyset$; $G'\leftarrow (z_1,E)$\;
    \For{$j=2$ \textbf{to} $m$}{
    $ b_j\leftarrow \text{LosslessRefine}(b_{j-1},b_j)$\;
    
       % $\quad$ if CollisionFree $(u, v)$ and Lossless$(u, v)$  then $E \leftarrow E \cup\{(u, v)\}$
    }
    
     \Return $\gamma_n$
    }
\caption{Lossless modification of $\gamma_n$}
\label{alg:lossless_modify_ri_prm_star}
\end{algorithm}
The IG-PRM* (Algorithm \ref{alg:igpp_body1}) takes in as inputs, the initial belief state $b_{\text{init}}$, the final belief state $b_{\text{final}}$ and the total number of belief states $n$ that would be sampled. The function $\textup{SampleFree}_i$ (Line 2 of Algorithm \ref{alg:igpp_body1}) generates a belief state $b_i=(x_i,P_i)$ by sampling the mean state $x_i$ and the corresponding covariance $P_i$ independently. The point  
$x_i$ is sampled uniformly from obstacle free space $\mathcal{X}_{\text{free}}\in \mathbb{R}^d$ and $P_i$ is sampled uniformly from $\mathcal{R}_{(\rho d, R ]}:=\{P \in \mathbb{S}^d_{+}:\; \rho d < \textup{Tr}(P) \leq R \}$ by the scheme proposed in \cite{mittelbach2012sampling}.  Using the function $\textup{SampleFree}_i$, $n$ such uniformly randomly belief states. These $n$ sampled belief states along with the $b_{\text{init}}$ and $b_{\text{final}}$ are stored in set $B$ (Line 2 of Algorithm 1). For every belief state $b$ in the set $B$, the function $\operatorname{Near}\left( B, b, D_{\text{min}}\right)$ (Line 5 of Algorithm \ref{alg:igpp_body1}) returns the neighboring nodes of $b$ in $B$ and stores it in set $B_{\text{nbors}}$. In other words,  $B_{\text{nbors}}=\left\{b_j \in B:\hat{\mathcal{D}}(b_j,b)\leq  D_{\text{min}}  \right\}$, 
where 
% $\hat{\mathcal{D}}(b,b'):=\|x-x'\|$ with the connection radius $D_{\text{min}}=3\ell_n$, where
% \begin{subequations}
% \begin{equation} \label{eq:ell_def}
%  \hat{\mathcal{D}}(b,b'):=\|x-x'\|
%  \end{equation}
%  \begin{equation}
%  D_{\text{min}}=3\ell_n\nonumber\\
%     \ell_n:=\min\left\{\frac{\delta_n\sqrt{ \chi^2 \rho} }{16},\frac{\delta_n\rho }{18\bar{\sigma}(W)}\right\},\nonumber
%      \end{equation}
%  \begin{equation}
%    \delta_n = \min \left\{ \gamma\left(\frac{\text{log}n}{n}\right)^\frac{2}{d(d+3)}, \frac{1}{2}\right\}
%     \end{equation}
%  \begin{equation}
%    \gamma>\left(\frac{d^2+3d+2}{g_1 g_2 d(d+3)} \right)^\frac{2}{d(d+3)}
%     \end{equation}
%  \begin{equation}
%    g_1=\frac{\tau_d  h^d}{  \mathcal{V}_{\mathcal{X}}},\;\;\mathcal{V}_\mathcal{X}:=\text{vol}(\mathcal{X})
%     \end{equation}
%  \begin{equation}
%     g_2=V_dS_d(1,1,1/2)2^{-1}(d(d+1))V_r^{-1}(2\rho)^{\frac{d(d+1)}{2}}\Delta
%      \end{equation}
%  \begin{equation}
%    \Delta=\bigg[R^{\frac{d(d+1)}{2}}- (\rho d)^{\frac{d(d+1)}{2}} \bigg]^{-1}
%     \end{equation}
%  \begin{equation}
%    % &\mathcal{V}_\mathcal{X}:=\text{vol}(\mathcal{X})\nonumber\\
%    h:=\min\left\{\frac{\sqrt{ \chi^2 \rho} }{16},\frac{\rho }{18\bar{\sigma}(W)}\right\} = \frac{\ell_n}{\delta_n}
%     % \min\left\{\frac{\delta_n\sqrt{ \chi^2 \rho} }{6}, \frac{4 \delta_n (12-5 \delta_n)\rho}{27 \bar{\sigma}(W)}\right\},
% \end{equation}
% \end{subequations}
% \begin{figure*}[ht]
%     % \begin{minipage}{0.75\textwidth}
%         \centering
%  \captionsetup[subfigure]{justification=centering}
%  \centering
%  \begin{subfigure}{0.45\textwidth}
% {\includegraphics[width=6.8cm]{outline1.png}}
% \caption{Collision-free optimal path $\gamma^\star$.}
% %\label{fig:}
%  \end{subfigure}
%  \begin{subfigure}{0.45\textwidth}
%  \includegraphics[width=6.8cm]{outline2.png}
%  \caption{Regions $\mathcal{R}_{n,k}:=\{\}$}
% %\label{fig:}
%  \end{subfigure}
%   \begin{subfigure}{0.45\textwidth}
% {\includegraphics[width=6.8cm]{outline3.png}}
% \caption{An edge is established by IG-PRM* with probability 1 in limit of large $n$. (Lemma 4)}
% %\label{fig:}
%  \end{subfigure}
%  \begin{subfigure}{0.45\textwidth}
%  \includegraphics[width=6.8cm]{outline4.png}
% \caption{}
% %\label{fig:}
% \end{subfigure}
%  \caption{The figure depicts the outline of the proof. (a) Representation of optimal path $\gamma^\star$. (c) The zig-zag chain denoted by $\gamma^p_n$ (red curve) is the chain generated by the IG-PRM* algorithm (Algorithm \ref{alg:igpp_body1}) where $(x_i,P_i)$ denote the belief states that are sampled by IG-PRM*. In Lemma , we show that an edge is established by IG-PRM* with probability 1 in the limit of large $n$. (d) We show in Lemma that the zig-zag path generated by IG-PRM* converges to the optimal chain $\gamma'_n$ in the limit of large $n$. Using continuity of cost function (Theorem \ref{theo:cont}), we show that the cost of the zig-zag path also converges to the optimal cost $c^\star$.}
% \label{fig:outline_of_the_proof}
%     % \end{minipage}
% \vspace{-2ex}
% \end{figure*}

\begin{figure*}[ht!]
\centering
\includegraphics[width=1.96\columnwidth]{outline_proof.eps}
\caption{The figure depicts the outline of the proof. (a) Representation of optimal path $\gamma^\star$ with cost $c(\gamma^\star)=c^\star$. (c) The zig-zag chain denoted by $\gamma^p_n$ (red curve) is the chain generated by the IG-PRM* algorithm (Algorithm \ref{alg:igpp_body1}) where $(x_i,P_i)$ denote the belief states that are sampled by IG-PRM*. In Lemma \ref{lemma:event_E_n}, we show that an edge is established by IG-PRM* with probability 1 in the limit of large $n$. (d) We show in Lemma \ref{lemma:arbritrarily_close} that the zig-zag path generated by IG-PRM* converges to the optimal chain $\gamma'_n$ in the limit of large $n$. Using continuity of cost function (Theorem \ref{theo:cont}), we show that the cost of the zig-zag chain $\gamma_n^p$ converges to the optimal cost $c^\star$. }
\label{fig:outline_of_the_proof}
\end{figure*}


\begin{figure*}[ht!]
\centering
\includegraphics[width=1.96\columnwidth]{different_paths.eps}
\caption{Depiction of different paths $\gamma^\star$, $\gamma^{\star\star}_n$ and chains $\gamma_n$, $\gamma'_n$ that are used in the proof of asymptotic optimality of IG-PRM*. (a) The optimal path $\gamma^\star$ is optimal i.e. $c(\gamma^\star)=c^\star$ and collision-free. (b) From Fig. (2b), there does not exist any partition $\mathcal{P}_n$ of the optimal path $\gamma^\star$ that would lead to the chain $\gamma_n:=\{x_{n,k},P_{n,k}\}$ for all $k\in[1;K_n]$ being collision-free (clarified more in Fig. \ref{fig:transition}). To address this issue, we consider a modified chain $\gamma'_n:=\{x_{n,k},P'_{n,k}\}=\{x_{n,k},(1-\delta_n)P_{n,k}\}$ for all $k\in[1;K_n]$ which is collision-free (Lemma \ref{lemma:rn}). (d) We then show that as the number of samples $n$ tends to infinity, the cost of $\gamma'_n$ is equal to the cost of $\gamma^{\star\star}_n$ i.e. $\underset{n\rightarrow\infty}{\lim} c(\gamma^{\star\star}_n)=\underset{n\rightarrow\infty}{\lim} c(\gamma'_n)=c^\star$ (Eqn. \ref{eqn:cost_gamma_dash_equals_gamma}). }
\label{fig:1}
\end{figure*}
\begin{subequations}
\begin{align} 
% \label{eq:ell_def}
& \hat{\mathcal{D}}(b,b'):=\|x-x'\|,\\
& D_{\text{min}}=3\ell_n,\\
   & \ell_n:=\min\left\{\frac{\delta_n\sqrt{ \chi^2 \rho} }{16},\frac{\delta_n\rho }{18\bar{\sigma}(W)}\right\},\label{eq:ell_def}\\
   &\delta_n = \min \left\{ \gamma\left(\frac{\text{log}n}{n}\right)^\frac{2}{d(d+3)}, \frac{1}{2}\right\}\\
   &\gamma>\left(\frac{d^2+3d+2}{g_1 g_2 d(d+3)} \right)^\frac{2}{d(d+3)},\\
   &g_1=\frac{\tau_d  h^d}{  \mathcal{V}_{\mathcal{X}}},\;\;\mathcal{V}_\mathcal{X}:=\text{vol}(\mathcal{X}),\\
   & g_2=V_dS_d(1,1,1/2)2^{-1}(d(d+1))V_r^{-1}(2\rho)^{\frac{d(d+1)}{2}}\Delta,\\
   &\Delta=\bigg[R^{\frac{d(d+1)}{2}}- (\rho d)^{\frac{d(d+1)}{2}} \bigg]^{-1},\\
   % &\mathcal{V}_\mathcal{X}:=\text{vol}(\mathcal{X})\nonumber\\
   &h:=\min\left\{\frac{\sqrt{ \chi^2 \rho} }{16},\frac{\rho }{18\bar{\sigma}(W)}\right\} = \frac{\ell_n}{\delta_n}
    % \min\left\{\frac{\delta_n\sqrt{ \chi^2 \rho} }{6}, \frac{4 \delta_n (12-5 \delta_n)\rho}{27 \bar{\sigma}(W)}\right\},
\end{align}
\end{subequations}
% with $\delta_n = \min \left\{ \gamma\left(\frac{\text{log}n}{n}\right)^\frac{2}{d(d+3)}, \frac{1}{2}\right\}$, where
% % \begin{align} \label{eq:delta_n_def}
% %     \delta_n=\min\;\left\{\gamma\left(\frac{\text{log}n}{n}\right)^\frac{1}{2d},1\right\}.
% % \end{align}    
% %In \eqref{eq:delta_n_def}, 
% \begin{align}
% \gamma>\left(\frac{d^2+3d+2}{g_1 g_2 d(d+3)} \right)^\frac{2}{d(d+3)},
% \end{align}
% with $g_1=\frac{\tau_d  h^d}{  \mathcal{V}_{\mathcal{X}}}$ and $g_2=V_dS_d(1,1,1/2)2^{-1}(d(d+1))V_r^{-1}(2\rho)^{\frac{d(d+1)}{2}}\bigg[R^{\frac{d(d+1)}{2}}- (\rho d)^{\frac{d(d+1)}{2}} \bigg]^{-1}$, 
% where $\mathcal{V}_\mathcal{X}:=\text{vol}(\mathcal{X})$, $\tau_d$ is the volume of the unit ball in $\mathbb{R}^d$, and $h:=\min\left\{\frac{\sqrt{ \chi^2 \rho} }{16},\frac{\rho }{18\bar{\sigma}(W)}\right\} = \frac{\ell_n}{\delta_n}$.
 
 
The main intuition behind choosing these geometric constants in a certain way is to allow us to prove the asymptotic optimality and would be more clear in subsequent sections. Now, for every belief state $b_j$ in $B_{\text{nbors}}$, the function $\operatorname{CollisionFree}(b, b_j)$ (Line 7 of Algorithm \ref{alg:igpp_body1}) checks that the $\chi^2$ confidence bound in transition $b \rightarrow b_j $ does not intersect with any obstacles. If the transition $b \rightarrow b_j $ is collision-free, an edge is connected between $b$ and $b_j$ and stored in $E$. Next, a graph $G$ (Line 9 of Algorithm \ref{alg:igpp_body1}) is constructed using the set of belief states $B$ and the edges $E$. Finally, the shortest path $\gamma_n$ (Line 10 of Algorithm \ref{alg:igpp_body1}) between the initial belief $b_{\text{init}}$ to $b_{\text{final}}$ is computed using the Search function (Algorithm \ref{algo:search}). 
% After forming the IG-PRM* graph, we deploy Algorithm~\ref{algo:search} to find the shortest path between the initial belief $b_{\text{init}}$ to $b_{\text{final}}$. 
Algorithm \ref{algo:search} resembles Algorithm~7 in \cite{choset2005principles}. The function $\text{Near}_{\text{from}}(G, b_\text{init},k)$ finds the $k^\text{th}$ nearest nodes in the metric $\hat{D}$ from $G$, to which the transition from $b_{\text{init}}$ are collision-free. Likewise, the function $\text{Near}_{\text{from}}(G, b_\text{start},k)$ finds the $k^\text{th}$ nearest nodes based on the metric $\hat{D}$ from $G$, from which the transition to $b_{\text{final}}$ is collision-free. The function $\text{ShortestPath}(G, N_{b_{\text{init}}}, N_{b_{\text{final}}})$ first uses  Dijkstra's algorithm \cite{dijkstra1959note} to find the shortest path on $G$ between all possible  pairs of   $b_1 \in N_{b_{\text{init}}}$ and $b_2 \in N_{b_{\text{final}}}$, if one exists. Then, this function returns the path that results in the shortest path, among the sought shortest paths, between $b_{\text{init}}$ and $b_{\text{final}}$, or returns $\text{Failure}$ if  no path is found.  However, it must be noted that the belief chain $\gamma_n$ computed using the IG-PRM* algorithm (Algorithm \ref{alg:igpp_body1}) might not be finitely lossless. Algorithm \ref{alg:lossless_modify_ri_prm_star} ensures that the chain $\gamma_n$ is finitely lossless. It takes in as input, $\gamma_n$ which consists of say $m$ belief states in sequence. Then for every belief edge $(b_{j-1},b_j)$ for $j\in[1;m]$, the belief state $b_j$ is updated using the function $\operatorname{LosslessRefine}(b_{j-1},b_j)=(x_j,P^\star)$ which ensures that the transition $b_{j-1}\rightarrow b_j$ is lossless. In other words, $b_j$ now becomes equal to $(x_j,P^\star)$.  Here, $P^\star$ is the minimizer of \eqref{eq:d_info_general} computed using Lemma \ref{lemma:analytical}. In this case $\hat{P}_{k+1}=P_{j-1}+\|x_{j-1}-x_j\|W$ and $P_{k+1}=P_j$ where $b_{j-1}\equiv(x_{j-1},P_{j-1})$ and $b_j\equiv(x_j,P_j)$.
\begin{lemma}
 \normalfont [Lemma 1, \cite{pedram2021gaussian}] Let $[U, \Sigma]$ be the eigendecomposition of $P_{k+1}^{-1 / 2} \hat{P}_{k+1} P_{k+1}^{-1 / 2}$ i.e. $U \Sigma U^{\top}=P_{k+1}^{-1 / 2} \hat{P}_{k+1} P_{k+1}^{-1 / 2}$, where $\Sigma=$ $\operatorname{diag}\left(\sigma_1, \ldots, \sigma_n\right) \succ 0$ and $U$ is unitary matrix. Then, $P^\star=$ $P_{k+1}^{1 / 2} U S^\star U^{\top} P_{k+1}^{1 / 2}$ is the optimal solution of \eqref{eq:d_info_general} , where $S^\star:=$ $\operatorname{diag}\left(\min \left\{1, \sigma_1\right\}, \ldots, \min \left\{1, \sigma_{\mathrm{n}}\right\}\right)$
 \label{lemma:analytical}
\end{lemma}
\begin{assumption}
    \normalfont We assume IG-PRM* is run for $n\geq n^\star$, where $n^\star$ is the minimum $n'\in  \mathbb{N}$ such that $\delta_{n'}< 1/2$.
\end{assumption}
 {
% After forming the IG-PRM* graph, we deploy Algorithm~\ref{algo:search} to find the shortest path between the initial belief $b_{\text{init}}$ to $b_{\text{final}}$. This algorithm resembles Algorithm~7 in \cite{choset2005principles}. The function $\text{Near}_{\text{from}}(G, b_\text{start},k)$ finds the $k^\text{th}$ nearest nodes in the metric $\hat{D}$ from $G$, to which the transition from $b_{\text{init}}$ are collision-free. Likewise, the function $\text{Near}_{\text{from}}(G, b_\text{start},k)$ find the $k^\text{th}$ nearest nodes in the metric $\hat{D}$ from $G$, from which the transition to $b_{\text{final}}$ is collision-free. The function $\text{ShortestPath}(G, N_{b_{\text{init}}}, N_{b_{\text{final}}})$ first uses  Dijkstra's algorithm \cite{dijkstra1959note} to find the shortest path on $G$ between all possible  pairs of   $b_1 \in N_{b_{\text{init}}}$ and $b_2 \in N_{b_{\text{final}}}$, if one exists. Then, this function returns the path that results in the shortest path, among the sought shortest paths, between $b_{\text{init}}$ and $b_{\text{final}}$, or returns $\text{Failure}$ if  no path is found.
% \begin{algorithm}
%     % \footnotesize
%     {
%     \textbf{Inputs}: $G, b_{\text{init}}, b_{\text{final}}, k$ \;
%     $N_{b_{\text{init}}} \leftarrow \text{Near}_{\text{from}}(G, b_\text{start},k) $\;
%     $N_{b_{\text{final}}} \leftarrow \text{Near}_{\text{to}}(G, b_\text{final},k)$\;
%     $\gamma_n\leftarrow \text{ShortestPath}(G, N_{b_{\text{init}}}, N_{b_{\text{final}}})$\;
%     \Return $\gamma_n$
%     }
% \caption{Search Algorithm (\text{Search}($G, b_{\text{init}}, b_{\text{final}}, k$))}
% \label{algo:search}
% \end{algorithm}
}   
% {\color{red} 
% % Items remained
% % \begin{itemize}
%    We need a remark explaining why we would cannot include Frobenius norm of covariances!
%     % \item We also assume the algorithm is run for $n\geq n^\star$, where $n^\star$ is the minimum $n'\in  \mathbb{N}$ such that $\delta_{n'}\leq 1/2$.
%     %\item if initial and final ellipses inside a safe ellipse, then all ellipses in transition stay inside the safe ellipse!!!
% % \end{itemize}

% % We need to discuss search algorithm.
% }   


% In this section, we derive some primary technical results that we use for proof of optimality in Section~\ref{sec:optimality}.

% \begin{remark}\label{remark:semi-axis}
% \normalfont The length of minor semi-axis of confidence ellipse $\mathcal{E}_{\chi^2}(x,P)$ is $\sqrt{\chi^2 \underline{\sigma}(P)}$. Consequently, the minimum distance between the boundary of co-centric, similar ellipses   $\mathcal{E}_{\chi^2}(x,P)$ and  $\mathcal{E}_{\chi^2}(x, \alpha P)$, where $\alpha > 0$, is $\sqrt{\chi^2 |1-\alpha|\underline{\sigma}(P)}$. 
% \end{remark}

% \subsection{Uniform sampling of covariance}
% We use the algorithm proposed in \cite{mittelbach2012sampling} for uniformly sampling covarinace $P\in \mathbb{S}^d_{+}$ in  $\mathcal{D}_{(\underline{c},\bar{c}]}:=\{P\in \mathbb{S}^d_{+}: \underline{c} < \textup{Tr}(P)\leq \bar{c}\}$. The sampled covariance $P$ is said to have uniform distribution  on $\mathcal{D}_{[\underline{c},\bar{c}]}$ if
% \begin{align} \label{eq:uniform_sampling}
%     \mathbb{P}(\{P\in\mathcal{A}\})=\frac{\text{vol}(\mathcal{A}\cap \mathcal{D}_{[\underline{c}, \bar{c} ]})}{\text{vol}(\mathcal{D}_{[\underline{c} ,\bar{c} ]})}
% \end{align}
% holds for all $\mathcal{A} \subset \mathbb{S}^{d}$. The proposed sampling is performed in two stages. In the first stage, a sampling method is developed that generates samples from random variable $\bM:\mathcal{D}\rightarrow \mathbb{R}$ such that $\bM$ has uniform distribution, where $\mathcal{D}:=\{P\in \mathbb{S}^d_{+}: \textup{Tr}(P)=1\}$. Next, it is shown that the product of  distribution $\bM$ and the distribution of a certain random variable $\bt:(\underline{c},\bar{c}]\rightarrow \mathbb{R}$ is transformed into a uniform distribution on $\mathcal{D}_{(\underline{c},\bar{c}]}$ by the one-to-one correspondence. More precisely, if  $\bt$ is independant of $\bM$ and has the probability density function of $t_0 t^{\frac{d(d+1)}{2}-1} \bm{1}_{(\underline{c},\bar{c}]}$, then $\bt\bM$ has a uniform distribution over $\mathcal{D}_{(\underline{c},\bar{c}]}$, where $t_0= (\bar{c}^{\frac{d(d+1)}{2}} - \underline{c}^{\frac{d(d+1)}{2}})^{-1}$ is the normalizing factor.


% We assume the space of $\mathbb{S}^d_{+}$ is equipped with the Rao-Fisher metric \cite{terras2012harmonic}, and use this metric to measure the volume of different regions in $\mathcal{S}^d_{+}$ in the following Theorem. 

% \begin{theorem}
% \label{theo:volume}
% \normalfont The volume of regions $\mathcal{D}_A=\{Q\in \mathbb{S}^d_{+}: Q \preceq A:=\textup{diagonal}(a_1, \dots, a_d)\}$ and $\mathcal{R}_c=\{Q\in \mathbb{S}^d_{+}: \textup{Tr}(Q)\leq c \}$ are computed as
% \begin{align}
% \label{eq:vol_DA}
%     \textup{Vol}(\mathcal{D}_A) = V_ds_d \prod_{i=1}^{d} s_i
% \end{align}
% and 
% \begin{align}
% \label{eq:vol_Rc}
%     \textup{Vol}(\mathcal{R}_c) =  V_d \; c^{\frac{d(d+1)}{2}} \left(\frac{1}{d}\right)^d,
% \end{align}
% where $V_d = (d! \; 2^d)^{-1} \frac{2^d \pi^{d^2/2}}{\Gamma_d(d/2)} (d-1)!$, $\Gamma_d$ is the multivariate gamma function, $s_i=\sum_{j=1}^d a_j^i$ for $i=1, \dots, d-1$, and $s_d=\prod_{j=1}^d a_j$.
% \end{theorem}
% \begin{proof}
% Please, see Appendix~\ref{app:volume}.
% \end{proof}
% \begin{remark}
%    \normalfont The expression \eqref{eq:vol_Rc} is compatible with the proposed pdf of $\bt$. More precisely, for all $\underline{c} \leq c_1 < c_2 \leq \bar{c}$
% \begin{align*}
% &\mathbb{P}(\{P_i\in\mathcal{D}_{(c_1,c_2]}\})
% = \mathbb{P}(\{t_i\in(c_1,c_2]\})\\
% &=\frac{{c}_2^{\frac{d(d+1)}{2}} - {c}_1^{\frac{d(d+1)}{2}}}{\bar{c}^{\frac{d(d+1)}{2}} - \underline{c}^{\frac{d(d+1)}{2}}}=\frac{\textup{vol}(\mathcal{R}_{c_2})- \textup{vol}(\mathcal{R}_{c_1})}
% {\textup{vol}(\mathcal{R}_{\bar{c}})- \textup{vol}(\mathcal{R}_{\underline{c}})}.
% \end{align*}
% \end{remark}

% \begin{lemma}
% \label{lemma:diag}
%   \normalfont  If $P\in \mathbb{S}^d_{+}$ has the eigenvalue decomposition $P=V^\top \Sigma V$, where $\Sigma$ is diagonal and $V$ is a unitary matrix, then 
% \begin{align}
%     \textup{Vol}(\mathcal{D}_P)=\textup{Vol}(\mathcal{D}_{\Sigma}),
% \end{align}
% where $\mathcal{D}_P=\{Q\in \mathbb{S}^d_{+}: Q \preceq P\}$ and $\mathcal{D}_{\Sigma}=\{Q\in \mathbb{S}^d_{+}: Q \preceq \Sigma\}$.
% \end{lemma}
% \begin{proof}
% {Please, see Appendix~\ref{app:diag}}
% \end{proof}

% % From $f_{t}(t)$, it is easy to verify that for $P_i$ generated uniformly from $\mathcal{D}_{(\underline{c},\bar{c}]}$, we have 
% % \begin{align}
% % \mathbb{P}(\{P_i\in\mathcal{D}_{(c_1,c_2]}\})
% % =\frac{{c}_2^{\frac{d(d+1)}{2}} - {c}_1^{\frac{d(d+1)}{2}}}{\bar{c}^{\frac{d(d+1)}{2}} - \underline{c}^{\frac{d(d+1)}{2}}},
% % \end{align}
% % for all $\underline{c} \leq c_1 < c_2 \leq \bar{c}$.


\section{Asymptotic Optimality of IG-PRM* \label{sec:optimality}}
In this section, we show Algorithm~1 with $D_{\text{min}}$ introduced in Section~\ref{sec:algorithm} achieves asymptotic optimality.

\subsection{Outline of the proof\label{subsection:outline_of_proof}}
Let $\gamma^\star: [0,1]\rightarrow \mathbb{R}^d\times \mathbb{S}_\rho^d$, $\gamma^\star(t)=(x^\star(t), P^\star(t))$ be the optimal path and is therefore collision-free by definition (see Fig. (1a)).  We define a series of partitions $\mathcal{P}_n:=\{x_{n,k},P_{n,k}\}\;k\in[1;K_n]$ for $\gamma^{\star}$ where $x_{n,k}=x^\star\left(\frac{k-1}{K_n}\right)$ and $P_{n,k}:=P\left(\frac{k-1}{K_n}\right)$. By construction, $\|x_{n,k+1}-x_{n,k}\|\leq\ell_n$ for all $k\in[1;K_n-1]$ (see Fig. (1b)). 

For each $\mathcal{P}_n$, a sequence of regions $\mathcal{R}_{n,k}, \;k\in[1;K_n]$ is constructed (Section \ref{sec:construction_free_rnk}) which posses three important properties. First, each $\mathcal{R}_{n,k}$ is a region of finite volume. Second, in the limit of large $n$, the volume of these regions converges to zero and the set $\mathcal{R}_{n,k}$ converges to the point $(x_{n,k},P_{n,k})$ for all $k\in[1;K_n]$. Last, we show that the transition between consecutive regions i.e., between any belief state  $b_k \in \mathcal{R}_{n,k}$ and any belief state $b_{k+1} \in \mathcal{R}_{n,k+1}$ is collision-free, and the distance between $b_k$ and $b_{k+1}$ is smaller than $D_{\textup{min}}$ (Lemma \ref{lemma:rn}). The main intuition behind the construction of these $\mathcal{R}_{n,k}$, is to show that there is a positive probability that a belief state would be sampled in every $\mathcal{R}_{n,k}$ for $k\in[1;K_n]$ by the IG-PRM* algorithm (Algorithm \ref{alg:igpp_body1}) and this would be more clear in subsequent sections. Now, we claim that if the IG-PRM* algorithm is able to sample belief states in every $\mathcal{R}_{n,k}$, the connection between any $b_k \in \mathcal{R}_{n,k}$ and $b_{k+1} \in \mathcal{R}_{n,k+1}$ will be established by Algorithm~\ref{alg:igpp_body1}, if they are sampled (i.e. if $b_k \in B$ and $b_{k+1} \in B$) as shown in Fig. (1c). To prove this mathematically, we define the event $E_{n} \triangleq E_{n,1}\cap E_{n,2}\dots \cap E_{n,K_n}$ (Section \ref{subsec:event_E}) as the event that a belief state is sampled inside all $\mathcal{R}_{n,k}$ regions, and show that the event $E_n$ occurs with probability one as $n$ tends to infinity (Lemma \ref{lemma:event_E_n}).

Note that by definition, the optimal path $\gamma^\star$ is collision-free. However, the partition chain $\mathcal{P}_n$ might not be collision-free as one of the confidence ellipsoids of $\mathcal{P}_n$ would pass along the boundary of the obstacle as shown in Fig. \ref{fig:1}. To fix this issue, we construct a  chain $\gamma_n'\triangleq(x_{n,k},P'_{n,k}:=(1-\delta_n)P_{n,k}),\;k\in[1;K_n]$ (Section \ref{subsec:gamma_def}) such that $(x_{n,k},P'_{n,k}) \in \mathcal{R}_{n,k}$ and thus the transition between $(x_{n,k},P'_{n,k})$ to $(x_{n,k+1},P'_{n,k+1})$ for $k\in[1;K_n-1]$ becomes collision-free. Next, we show that the cost of $\gamma'_n$ converges to the cost of the optimal path $c^\star=c(\gamma^\star)$ in the limit of large $n$. Now, we need to link the cost of collision-free $\gamma'$ to the cost of the feasible paths generated by the IG-PRM* algorithm (Algorithm \ref{alg:igpp_body1}) as we are ultimately interested in analyzing the cost of paths returned by IG-PRM*. To that end, we show there exists a path on the graph generated by the IG-PRM$^\star$ algorithm (more precisely, the path generated by connecting sampled nodes in consecutive $ \mathcal{R}_{n,k+1}$s) that gets arbitrarily close to $\gamma'_n$ as $n$ tends to infinity (Lemma \ref{lemma:arbritrarily_close}). Even though the IG-PRM* generated path would get arbitrarily close to $\gamma'_n$ in the limit of large $n$, it is still not clear whether the cost of these generated paths would tend to $c(\gamma'_n)$ as $n$ tends to infinity. To that end, we leverage the continuity of path cost function (Theorem \ref{theo:cont}) to show that the cost of that path gets arbitrarily close to $c^\star$.%(Lemma \ref{lemma:cont_final}). 

%From $\gamma^\star$, we first , and 
%Since $\gamma_n'$ might not be lossless, we consider the lossless refinement of $\gamma_n'$ to obtain a lossless and collision-free chain $\gamma_n''\triangleq (x_{n,k},P''_{n,k}),\;k\in\{1,2,\dots,K_n\}$ and 


% Using $\gamma''_n$, the regions $\mathcal{R}_{n,k}$ are constructed in such a way that the transition between consecutive regions i.e., between any belief state  $b_k \in \mathcal{R}_{n,k}$ and any belief state $b_{k+1} \in \mathcal{R}_{n,k+1}$ is collision-free.
%Next, we define a region $\mathcal{R}^s_{n,k}$ which is subset of $\mathcal{R}_{n,k}$ and  show the transition between any $b^s_k \in \mathcal{R}^s_{n,k}$ to any $b^s_{k+1} \in \mathcal{R}^s_{n,k+1}$ is lossless (Lemma \ref{lemma:rs_lossless}).


\subsection{Construction of collision-free region \texorpdfstring{$\mathcal{R}_{n,k}$}{R}\label{sec:construction_free_rnk}}
\label{sec:event_d_n}
Let $\{\delta_n\}$ for $n\in\mathbb{N}$ be a sequence of positive numbers defined in Section~\ref{sec:algorithm}. Note that $0< \delta_n \leq \frac{1}{2}$ for each $n$ and $\underset{n\rightarrow\infty}{\lim}\;\delta_n=0$. Let $c^\star$ be the optimal path length and
\[
\ell^\star=\sup_{\mathcal{P}} \sum_{k=1}^{K_n-1}\|x^\star(t_{k+1})-x^\star(t_k)\|
\]
be the total travel length of the optimal path $\gamma^\star:[0,1]\rightarrow \mathbb{R}^d \times \mathbb{S}^d, \gamma^\star(t)=(x^\star(t), P^\star(t))$. 
% \begin{figure}[ht!]
% \centering
% \includegraphics[width=0.96\columnwidth]{fig1_new.eps}
% \caption{$\gamma^\star$ is the optimal path that is collision-free by definition. However, the chain after partition $\mathcal{P}_n$ might not be collision-free as one of the confidence ellipsoids might touch the boundary of the obstacle. To that end, we construct a collision-free chain $\gamma'_n$ (detailed in Section \ref{subsec:gamma_def}).}
% \label{fig:1}
% \end{figure}
% \begin{figure}[ht!]
% \centering
% \includegraphics[width=0.99\columnwidth]{fig3.eps}
% \caption{The equi-spaced partition $\mathcal{P}_n=(0=t_{n,0}\leq t_{n,1} \leq \cdots \leq t_{n,K_n}=1)$} of $P(t)$.
% \label{fig:3}
% \end{figure}
For each $n\in \mathbb{N}$, choose $\ell_n$ as defined in \eqref{eq:ell_def}
% \begin{align}
% \ell_n=&\frac{\min\left\{%\frac{\delta_n}{2}\rho,
% \delta_n\left(1-\frac{3}{4}\delta_n\right)\frac{\rho}{\bar{\sigma}(W)},{\frac{\delta_n\rho}{3},3}\right\}}{3}\\
% \geq& \delta_n \min\left\{%\frac{\rho}{2},
% \frac{\rho}{12\bar{\sigma}(W)},{\frac{\rho}{9}},\frac{1}{\delta_n}\right\}:=\delta_n h \nonumber
% \end{align}
and $K_n=\lceil \frac{\ell^\star}{\ell_n} \rceil$. Consider the equi-spaced partition $\mathcal{P}_n=(0=t_{n,0}\leq t_{n,1} \leq \cdots \leq t_{n,K_n}=1)$, and define the chain $\gamma_n:=(x_{n,k}, P_{n,k})\triangleq (x^\star(t_{n,k}), P^\star(t_{n,k}))$, $k\in[1;K_n]$.
By construction, we have $\|x_{n,k+1}-x_{n,k}\|\leq \ell_n$ for each $k\in[1;K_n-1]$. However,
the chain $\gamma_n$ constructed this way is not collision-free in general (Fig. \ref{fig:1}). To that end, we define regions 
\begin{align}
\nonumber
    \mathcal{R}_{n,k}=\{&(x,P): x\in \mathcal{B}(x_{n,k},\ell_n), \\ \label{eq:R_n_def}
    &\left(1-\frac{3 \delta_n}{2}\right) P_{n,k} \preceq P\preceq \left(1-\frac{\delta_n}{2}\right) P_{n,k}\},
    % \label{eqn:R_n_k}
\end{align} 
and show that the transition from any $b_1\equiv(x_1, P_1) \in \mathcal{R}_{n,k}$ to any $b_2\equiv(x_2, P_2) \in \mathcal{R}_{n,k+1}$ is collision-free.
In transition $b_1\rightarrow b_2$, the mean and  covariance of the state can be parameterized as $x[\lambda] = (1-\lambda)x_1+\lambda x_2$ and  $P[\lambda]=P_1+\lambda \|x_2-x_1\|W$ for $\lambda\in[0,1]$.
\begin{remark}\label{remark:semi-axis}
\normalfont The length of minor semi-axis of confidence ellipse $\mathcal{E}_{\chi^2}(x,P)$ is $\sqrt{\chi^2 \underline{\sigma}(P)}$. Consequently, the minimum distance between the boundary of co-centric, similar ellipses   $\mathcal{E}_{\chi^2}(x,P)$ and  $\mathcal{E}_{\chi^2}(x, \alpha P)$, where $\alpha > 0$, is $\sqrt{\chi^2 |1-\alpha|\underline{\sigma}(P)}$. 
\end{remark}
%we consider another chain $\gamma'_n = (x_{n,k}, P'_{n,k})\triangleq (x^\star(t_{n,k}),(1-\delta_n)^2 P^\star(t_{n,k}))$, $k\in[0;K_n]$, which is the chain generated by partition $\mathcal{P}_n$ on path $\gamma_n^{\star\star}(t)$, and show it is collision free.

\begin{figure}[ht!]
\centering
\includegraphics[width=0.97\columnwidth]{transition_new.eps}
\caption{Transition from $(x'_{n,k}, P'_{n,k}:=(1-\delta_n)P_{n,k})$ to $(x'_{n,k+1}, P'_{n,k+1}:=(1-\delta_n)P_{n,k+1})$ for all $k\in[1;K_n-1]$ is collision-free even if transition from $(x_{n,k}, P_{n,k})$ to $(x_{n,k+1}, P_{n,k+1})$ is not.}
\label{fig:transition}
\end{figure}


\begin{lemma}
\label{lemma:rn}
 \normalfont In any transition from $b_1=(x_1, P_1) \in \mathcal{R}_{n,k}$ to $b_2=(x_2, P_2) \in \mathcal{R}_{n,k+1}$, 
 for all $\lambda\in[0,1]$,
 $ \mathcal{E}_{\chi^2} (x[\lambda], P[\lambda]) \subset\mathcal{E}_{\chi^2} (x_{n,k},P_{n,k})$, for $    \ell_n:=\min\left\{\frac{\delta_n\sqrt{ \chi^2 \rho} }{16},\frac{\delta_n\rho }{18\bar{\sigma}(W)}\right\}$. This relation proves that the transition $b_1=(x_1,P_1)\rightarrow b_2=(x_2,P_2)$ fully resides in $\mathcal{E}_{\chi^2} (x_{n,k},P_{n,k})$, and thus it is collision-free. 
 %More precisely,
%\begin{align}
%\label{eq:init_in}
%&\mathcal{E}_{\chi^2}(x_1, P_1) \subseteq \mathcal{E} (x_{n,k},P_{n,k}),
\\ \label{eq:final_in}
%&\mathcal{E}_{\chi^2}(x_2, P_1+||x_2-x_1||W) \subseteq \mathcal{E} (x_{n,k},P_{n,k}),
%\end{align}

\end{lemma}


\begin{proof}
The proof is provided in two stages. In the first stage, we show 
\begin{align}
\label{eq:intran_P}
    P[\lambda] \preceq \left(1-\frac{\delta_n}{3}\right) P_{n,k}, \quad \forall \lambda \in [0,1].
\end{align}
As the first step to prove \eqref{eq:intran_P}, we note that  
\begin{align*}
 &||x_2-x_1|| \leq \\
 &||x_{n,k}-x_1 ||+ ||x_{n,k+1}- x_{n,k} ||+||x_2-x_{n,k+1}||\\
 &\leq \ell_n + \ell_n + \ell_n  = 3\ell_n=:D_{\text{min}}.  
\end{align*}
Thus, we have
\begin{align} \nonumber
     P[\lambda] &= P_1+ \lambda ||x_2-x_1||W \\
     &\preceq  \left(1-\frac{\delta_n}{2}\right)P_{n,k}+||x_2-x_1||W \\ \nonumber
     &\preceq  \left(1-\frac{\delta_n}{2}\right) P_{n,k}+  3 \ell_n W. 
\end{align}
Hence, to establish \eqref{eq:intran_P}, it suffices to show
\begin{align} \label{eq:R_n_inter}
       \left(1-\frac{\delta_n}{2}\right) P_{n,k}+  3 \ell_n W \preceq  \left(1-\frac{\delta_n}{3}\right) P_{n,k}
\end{align}
Condition \eqref{eq:R_n_inter}
is equivalent to 
\begin{align*}
    3 \ell_n W &\preceq \left[ \left(1-\frac{\delta_n}{3}\right)- \left(1-\frac{\delta_n}{2}\right)\right]  P_{n,k}\\
    & = \frac{\delta_n}{6} P_{n,k},
\end{align*}
which trivially holds as $\ell_n \leq\frac{\delta_n\rho}{18\bar{\sigma}(W)}$. Consequently, 
\begin{align}
&\mathcal{E}_{\chi^2}(x_{n,k}, P[\lambda]) \subseteq \mathcal{E}_{\chi^2} \left(x_{n,k}, \left(1-\frac{\delta_n}{3}\right) P_{n,k}\right),\nonumber
% \forall\; \lambda\; \in [0, 1].
\end{align}
for all $\lambda\in[0,1]$. The minimum distance between $\mathcal{E}_{\chi^2}(x_{n,k}, P_{n,k})$ and $\mathcal{E}_{\chi^2}\left(x_{n,k},  \left(1-\frac{\delta_n}{3}\right) P_{n,k}\right)$ is  
$\left(1-\sqrt{(1-\frac{\delta_n}{3})}\right) \sqrt{ \chi^2 \underline{\sigma}(P_{n,k})}\geq \frac{\delta_n}{8}\sqrt{ \chi^2 \underline{\sigma}(P_{n,k})}$. After linear translating $\mathcal{E}\left(x_{n,k},  \left(1-\frac{\delta_n}{3}\right) P_{n,k}\right)$  for  
$||x[\lambda]-x_{n,k}|| \leq (1-\lambda) \|x_1-x_{n,k}\|+\lambda \|x_2-x_{n,k}\| \leq (1-\lambda)\ell_n +2 \lambda (\|x_2-x_{n,k+1}\|+\|x_{n,k+1}-x_{n,k}\|) \leq (1+\lambda) \ell_n \leq 2\ell_n \leq \frac{\delta_n}{8} \sqrt{\chi^2 \rho} \leq \frac{\delta_n}{8} \sqrt{ \chi^2 \underline{\sigma}(P_{n,k})}$, the resultant ellipse $\mathcal{E}_{\chi^2}\left(x[\lambda], 
\left(1-\frac{\delta_n}{3}\right) P_{n,k}\right)$, stays inside $\mathcal{E}_{\chi^2} (x_{n,k},P_{n,k})$. Subsequently, $\mathcal{E}_{\chi^2}(x[\lambda], P[\lambda]) \subset \mathcal{E}_{\chi^2} (x_{n,k},P_{n,k})$. $\qed$
\end{proof}





Finally, we stress that as shown in proof of Lemma~\ref{lemma:rn} the distance $\hat{D}(b_1,b_2)$ between any $b_1=(x_1,P_1)\in\mathcal{R}_{n,k}$ and any $b_2=(x_2,P_2)\in\mathcal{R}_{n,k+1}$ for all $k\in[1;K_n-1]$ is less than $3\ell_n :=D_{\text{min}}$ so the connection between $b_1$ and $b_2$ will be attempted by Algorithm \ref{alg:igpp_body1}. 

% From \eqref{eq:R_n_def}, we have $\mathcal{E}(x_{n,k}, P_1) \subseteq \mathcal{E} (x_{n,k},(1-\frac{\delta_n}{2})^2 P_{n,k})$. From Remark~\ref{remark:semi-axis}, it is easy to verify that the minimum distance between $(x_{n,k},(1-\frac{\delta_n}{2})^2 P_{n,k})$ and $(x_{n,k}, P_{n,k})$ is $ \frac{\delta_n}{2} \sqrt{ \chi^2 \underline{\sigma}(P_{n,k})}$. On the other hand, any translation of the center of ellipsoid  $\mathcal{E}(x_{n,k}, P_1)$ from $x_{n,k}$ to some  $x\in\mathcal{B}(x_{n,k},\ell_n)$, will linearly translate the ellipsoid $\mathcal{E}\left(x_{n,k},P\right )$ by maximum of distance $\ell_n$, where
% \[
% \ell_n \leq 3 \ell_n \leq  \frac{\delta_n}{2} \sqrt{ \chi^2 \rho} \leq  \frac{\delta_n}{2} \sqrt{\chi^2 \underline{\sigma}(P_{n,k})}.
% \]
% Thus, after the translation the ellipse stays inside $\mathcal{E}(x_{n,k},P_{n,k})$ and \eqref{eq:init_in} holds. 


\subsection{Probability of event \texorpdfstring{$E_n$}{En}\label{subsec:event_E}}
\label{sec:enent_E_n}
Let $b_i:=(x_i,P_i) \in B$ be sample belief state by Algorithm~\ref{alg:igpp_body1} at iteration $i$. Then, due to the uniform distribution the probability of event that $x$ is samples inside $\mathcal{B}(x_{n,k},\ell_n)$ is given by
\begin{align} \nonumber
    \mathbb{P}(\{x_i\in\mathcal{B}(x_{n,k},\ell_n)\})\!&=\!\frac{\text{vol}(\mathcal{B}(x_{n,k},\ell_n))}{\text{vol}(\mathcal{X}_{\text{free}})}
    \\ 
    &=\frac{\tau_d\ell^{d}_n}{\mathcal{V}_{\mathcal{X}}}
    =  \frac{\tau_d\delta_n^{d} h^d}{  \mathcal{V}_{\mathcal{X}}}=: g_1 \delta_n^{d},
    \!\nonumber   
\end{align}
where $\mathcal{V}_{\mathcal{X} }:=\text{vol}(\mathcal{X}_{\text{free}})$ and $g_1:= \frac{\tau_d  h^d}{  \mathcal{V}_{\mathcal{X}}}$.
If we define
\begin{align}
\small
    \mathcal{D}_{n, k} :=&\{P\in\mathbb{S}^d_{4\rho}:\nonumber\\
    &\left(1-\frac{3 \delta_n}{2}\right) P_{n,k} \preceq P\preceq \left(1-\frac{\delta_n}{2}\right) P_{n,k}\},\nonumber
\end{align} 
we have $\left(1-\frac{3 \delta_n}{2}\right) P_{n,k} \succeq \left(1-\frac{3}{4}\right) \underline{\sigma}(P_{n,k}) I \succeq \rho I$. Thus, Theorem~\ref{theo:cont} holds in these regions.
We have  $\textup{Tr}(P) \geq 4 (1-\frac{3 \delta_n}{2})  \rho d \geq \rho d$ for all $P \in \mathcal{D}_{n, k}$.
On the other hand, $\textup{Tr}(P)\leq (1-\frac{\delta_n}{2}) \textup{Tr}(P_{n,k}) \leq  (1-\frac{\delta_n}{2}) R \leq R $. Hence, $\mathcal{D}_{n,k} \subset \mathcal{R}_{ [(1-\frac{3 \delta_n}{2})  4\rho d, (1-\frac{\delta_n}{2}) R ]} \subset \mathcal{R}_{ [ \rho d, R ]}$.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%text by vrushabh
% \textcolor{blue}{Define $P_{n,k}(\zeta):=(1-\frac{3 \delta_n}{2})^2 P_{n,k}+\zeta((1-\frac{ \delta_n}{2})^2 P_{n,k}-(1-\frac{3 \delta_n}{2})^2 P_{n,k})$. Consider $\Bar{\mathcal{D}}_{n,k}=\{P_{n,k}(\zeta)\succeq 0:\;\zeta\in[0,1]\}$. Clearly $\Bar{\mathcal{D}}_{n,k}\subset \mathcal{D}_{n,k}$.
% % Further, $\text{vol}(\Bar{\mathcal{D}}_{n,k})\geq \text{det}\left((1-\frac{3 \delta_n}{2})^2 P_{n,k}\right)\geq (1-\frac{3 \delta_n}{2})^2 \rho$ (not saying anything about proportionality).
% Therefore, 
% % $\mathbb{P}(P_i\in\mathcal{D}_{n,k})$ can be lower bounded as follows:
% \begin{align}
%     \mathbb{P}(P_i\in\mathcal{D}_{[\rho d,R]})\geq\mathbb{P}(P_i\in\mathcal{D}_{n,k})\geq \mathbb{P}(P_i\in\Bar{\mathcal{D}}_{n,k})\nonumber
%     % \frac{\text{vol}(\Bar{\mathcal{D}}_{n,k})}{\text{vol}(\mathcal{D}_{ [ \rho d, R ]})}\nonumber
%     % \geq \frac{(1-\frac{3 \delta_n}{2})^2 \rho}{\text{vol}(\mathcal{D}_{ [ \rho d, R ]})}\nonumber
% \end{align}
% % Define $P_{n,k}(\zeta):=(1-\frac{3 \delta_n}{2})^2 P_{n,k}+\zeta((1-\frac{ \delta_n}{2})^2 P_{n,k}-(1-\frac{3 \delta_n}{2})^2 P_{n,k})$.
% The $\mathbb{P}(\{P_i\in \Bar{\mathcal{D}}_{n,k}\})$ can be thought of $\mathbb{P}(\{\zeta\in[0,1]\})$ 
% where $\zeta\in[\eta_{\text{min}},\eta_{\text{max}}]$. $\eta_{\text{min}}$ and $\eta_{\text{min}}$ are defined as
% \begin{align}
% &\eta_{\text{min}}=\min\{\zeta\in\mathbb{R}:P_{n,k}(\zeta)\succeq \rho I\}\nonumber\\
% &\eta_{\text{max}}=\max\{\zeta\in\mathbb{R}:P_{n,k}(\zeta)\preceq \bar{\rho} I\}\nonumber
% \end{align}
% It is not trivial to compute the exact values of $\eta_{\text{min}}$ and $\eta_{\text{max}}$, However,
% % \begin{align}
% %   \eta_{\text{max}}-\eta_{\text{min}}\leq  
% % \end{align}
% $\eta_{\text{max}}-\eta_{\text{min}}\leq \frac{(2-3\delta_n)^2(2\Bar{\rho}-\rho)}{8\delta_n(1-\delta_n)\rho}$ as shown by the following lemma. 
% \begin{lemma}
%     \normalfont $\eta_{\text{max}}-\eta_{\text{min}}\leq \frac{(2-3\delta_n)^2(2\Bar{\rho}-\rho)}{8\delta_n(1-\delta_n)\rho}$
% \end{lemma}
% \begin{proof}
%  We have,
%  \begin{align}
%     & P_{n,k}(\zeta)\succeq \rho I\nonumber\\
%     &\zeta \delta_n (2-2\delta_n)P_{n,k}+(1-\frac{3 \delta_n}{2})^2 P_{n,k}\succeq \rho I\nonumber\\
%     &\zeta\delta_n (2-2\delta_n)\lambda_i(P_{n,k})+(1-\frac{3 \delta_n}{2})^2\lambda_i(P_{n,k})\geq \rho\nonumber\\
%     & \zeta\geq \frac{\rho-\lambda_i(P_{n,k})}{\delta_n (2-2\delta_n)\lambda_i(P_{n,k})}\nonumber
%  \end{align}
%  Clearly from above, $\eta_{\text{min}}<\frac{(2-3\delta_n)^2(\rho-\Bar{\rho})}{8\delta_n(1-\delta_n)\rho}$. Similarly, it can be shown that $\eta_{\text{max}}<\frac{(2-3\delta_n)^2\Bar{\rho}}{8\delta_n(1-\delta_n)\rho}$ and hence the result follows.
% \end{proof}
% Consequently,
% \begin{align}
%     \mathbb{P}(P_i\in \Bar{\mathcal{D}}_{n,k})&=\mathbb{P}(\zeta\in[0,1])=\frac{\mu(\zeta\in[0,1])}{\mu(\zeta\in[\eta_{\text{min}},\eta_{\text{max}}])}\nonumber\\
%     &=\frac{1}{\eta_{\text{max}}-\eta_{\text{min}}}\geq \frac{(2-3\delta_n)^2(2\Bar{\rho}-\rho)}{8\delta_n(1-\delta_n)\rho}\nonumber
%     % &\geq \frac{\delta_n\rho}{2\Bar{\rho}-\rho}\nonumber
% \end{align}
% where $\mu(.)$ denotes the Lebesgue measure of $(.)$. Note that Lebesgue measure of the interval $[a,b]$ where $a,\;b\in\mathbb{R}$ is given by $b-a$.}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

From the definition of uniform sampling, we have 
\begin{align*}
     &\mathbb{P}(P_i\in\mathcal{D}_{n,k})
     {\geq}\frac{\text{vol}(\mathcal{D}_{n,k}\cap \mathcal{D}_{[\rho d, R]})}{\text{vol}(\mathcal{D}_{[\rho d ,R]})} = \frac{\text{vol}(\mathcal{D}_{n,k})}{\text{vol}(\mathcal{D}_{[\rho d,R]})} \\
    &=\frac{\text{vol}(\mathcal{D}_{(1-\frac{\delta_n}{2})P_{n,k}}) - \text{vol}(\mathcal{D}_{(1-\frac{3\delta_n}{2})P_{n,k}})}{\text{vol}(\mathcal{R}_{R}) - \text{vol}(\mathcal{R}_{\rho d})}\\
        &\geq\frac{V_dSb^{\frac{d(d+1)}{2}}\bigg[ (1-\frac{\delta_n}{2})^{\frac{d(d+1)}{2}}- (1-\frac{3\delta_n}{2})^{\frac{d(d+1)}{2}} \bigg]}{\left(\frac{2}{d(d+1)}\right)V_r\bigg[R^{\frac{d(d+1)}{2}}- (\rho d)^{\frac{d(d+1)}{2}} \bigg]}.
    % &=\frac{\prod_{i=1}^{d} s^{n,k}_i\bigg[ (1-\frac{\delta_n}{2})^{\frac{d(d+1)}{2}}- (1-\frac{3\delta_n}{2})^{\frac{d(d+1)}{2}} \bigg]}{\left(\frac{1}{d}\right)^d \bigg[R^{\frac{d(d+1)}{2}}- (\rho d)^{\frac{d(d+1)}{2}} \bigg]}.
\end{align*}
where $S=S_d(1,1,1/2)$ and $b=\underset{k\in[1;d]}{\min}\;\lambda_k(P_{n,k})$.
% $s_i^{n,k}$ is given by
% \begin{align}
%    s_i^{n,k}= \begin{cases}
%   \lambda^i_1(P_{n,k})+\dots+\lambda^i_d(P_{n,k}) & \text{for }i\in[1;d-1]\\    
%   \lambda_1(P_{n,k})\lambda_2(P_{n,k})\dots\lambda_d(P_{n,k}) & \text{for }i=d
% \end{cases}\nonumber
% \end{align}
\begin{figure}[ht!]
\centering
\includegraphics[width=1.05\columnwidth]{rirrg_optimality_balls_latex_colored.eps}
\caption{Covering of chain $\gamma_n$ with balls of radius $\ell_n$. The event $E_{n,k}$ is the event that a sampled point $(x,P)\in\mathcal{R}_{n,k}$.}
\label{fig:covering_balls}
\end{figure}
Now, we have
\begin{align*}
    &\left(1-\frac{\delta_n}{2}\right)^{\frac{d(d+1)}{2}}-\left(1-\frac{3 \delta_n}{2}\right)^{\frac{d(d+1)}{2}}\\
    & = \bigg(\left(1-\frac{ \delta_n}{2}\right) - \left(1-\frac{3 \delta_n}{2}\right)\bigg) \\
     &\qquad \sum_{j=0}^{\frac{d(d+1)}{2}-1} \left(1-\frac{\delta_n}{2}\right)^{\frac{d(d+1)}{2}-1-j}  \left(1-\frac{3 \delta_n}{2}\right)^{j}\\
        &\geq \delta_n d \left(1-\frac{3\delta_n}{2}\right)^{\frac{d(d+1)}{2}-1}\geq   d\left(\frac{\delta_n}{2}\right)^{\frac{d(d+1)}{2}},
\end{align*}
where in the last step we use $\delta_n\leq 1/2$. 
% Also, $\prod_{i=1}^{d} s^{n,k}_i \geq d^{d-1}(4\rho)^{\frac{d(d+1)}{2}}$.
In sum, it can be deduced
\begin{align}
    \mathbb{P}(P_i\in\mathcal{D}_{n,k}) \geq g_2 \delta_n^{\frac{d(d+1)}{2}},
\end{align}
where $g_2=V_dS_d(1,1,1/2)2^{-1}(d(d+1))V_r^{-1}(2\rho)^{\frac{d(d+1)}{2}}\bigg[R^{\frac{d(d+1)}{2}}- (\rho d)^{\frac{d(d+1)}{2}} \bigg]^{-1}$ is a constant. 

Lets define the event $E^i_{n,k}$ is as the event that the sampled belief $b_i=(x_i,P_i)$ belongs to $\mathcal{R}_{n,k}$. Then, $E_{n,k}= \cup_{i=1}^n E^i_{n,k}$ which means the event that $\mathcal{R}_{n,k}$ contains at least one sample belief $b_i \in B$. The following lemma shows that if the $\gamma$ is greater than a certain positive threshold, then the probability that event $E_n\triangleq E_{n,1}\cap E_{n,2}\dots \cap E_{n,K_n}$  occurs, i.e the event that all $R_{n,k}$'s contain at least one sampled belief, equals to one as $n$ approaches infinity.

\begin{lemma}
\normalfont If
$\gamma>\left(\frac{d^2+3d+2}{g_1 g_2 d(d+3)} \right)^\frac{2}{d(d+3)}$,
then the following holds true
\begin{align}
        \underset{n\rightarrow\infty}{\lim}\; \mathbb{P}\left(E_n \right)=1\nonumber.
\end{align}
\label{lemma:event_E_n}
\end{lemma}
\begin{proof}
The probability of event $E^c_{n,k}$ for all $k\in[1;K_n]$ is given as follows:
\begin{align}
   &\mathbb{P}\left(E^c_{n,k}\right)
   = \prod_{i=1}^{n}(1-\mathbb{P}\left(E^i_{n,k}\right))\nonumber\\
   &\leq \left(1- \mathbb{P}(P\in\mathcal{D}_{n,k}) \;     \mathbb{P}(x\in\mathcal{B}_{n,k})\right)^n\nonumber\\ \nonumber
   &\leq\left[1- g_2 \delta_n^d g_1 \delta_n^{\frac{d(d+1)}{2}} \right]^n = \left[1- g_1 g_2 \delta_n^{\frac{d(d+3)}{2}} \right]^n.
    \end{align}
%    \leq\left[1-\left(\frac{d \delta_n^{d}\rho^{\frac{d}{2}}}{2^{d}(\bar{\rho}^{d/2}-\rho^{d/2})}\right)\left(\frac{\tau_d\delta_n^{d} h^{d}}{  \mathcal{V}_{\mathcal{X}}}\right)\right]^n\nonumber\\
   % &\leq\left[1-\left(\frac{{ d \tau_d \delta_n^{2d}} \rho^{\frac{d}{2}} h^d}{12^{d}(\bar{\rho}^{d/2}-\rho^{d/2})\mathcal{V}_{\mathcal{X}}}\right)\right]^n\nonumber
Using the fact that $(1-x)\leq e^{-x}$ for $x\in(0,1)$, and substituting $\delta_n$ defined as
\begin{align}
    \delta_n=\min\;\left\{\gamma\left(\frac{\text{log}n}{n}\right)^\frac{2}{d(d+3)},\frac{1}{2}\right\},\nonumber
\end{align}
we have
\begin{align} \nonumber
 \mathbb{P}\left(E^c_{n,k}\right)&\leq \left(1-g_1 g_2
 {\gamma^{\frac{d(d+3)}{2}}\frac{\text{log}n}{n}} \right)^{n} \\ \nonumber
 &\leq \text{exp} \left( - (\log n) g_1 g_2 \gamma^{\frac{d(d+3)}{2}} \right)
 = n^{- g_1 g_2 \gamma^{\frac{d(d+3)}{2}}}.
\end{align}
Now, the event $E_n^c=\bigcup_{k=1}^n E^c_{n,k}$ is upper bounded as follows:
\begin{align}
  \mathbb{P}\left(E^c_{n}\right)& \!=\!\mathbb{P}\left(\bigcup_{k=1}^{K_n} E^c_{n,k}\right) 
 \! \leq \! \sum_{k=1}^{K_n}\mathbb{P}\left( E^c_{n,k}\right)\!\leq \! K_n n^{- g_1 g_2 \gamma^{\frac{d(d+3)}{2}}}.  \nonumber
\end{align}
where $K_n=\lfloor\frac{\ell^\star}{\ell_n}\rfloor\leq\frac{\ell^\star}{\ell_n}$.
Since $\delta_n h\leq \ell_n$, 
% $\frac{\delta_n \sqrt{\chi^2 \rho}}{6}\leq\ell_n$ (\textcolor{blue}{I think its as $h:=\min\left\{
% {\frac{\sqrt{\chi^2 \rho}}{6}},\frac{7\rho}{4\bar{\sigma}(W)}\right\} \leq \frac{\ell_n}{\delta_n}$}), 
we have
\begin{align} \nonumber
    \mathbb{P}\left(E^c_{n}\right)
    &\leq \frac{ \ell^\star}{h \delta_n }  n^{- g_1 g_2 \gamma^{\frac{d(d+3)}{2}}}  \\ \label{eq:P_c} 
     &= \frac{ \ell^\star}{\gamma h} (\log n)^{-\frac{2}{d(d+3)}} n^{- g_1 g_2 \gamma^{\frac{d(d+3)}{2}}+ \frac{2}{d(d+3)}}. 
\end{align}
%for large $n$ values (where $\delta_n\neq 1/2$).
If the power of $n$ in \eqref{eq:P_c} is less than $-1$, which is equivalent to
\[
\gamma>\left(\frac{d^2+3d+2}{g_1 g_2 d(d+3)} \right)^\frac{2}{d(d+3)},
\]
we have $\sum_{n=1}^\infty  \mathbb{P}\left(E^c_{n}\right)<\infty$. Consequently, 
$\underset{n\rightarrow\infty}{\lim}\mathbb{P}\left(E^c_{n}\right)=1$,
% \begin{align}
%   \underset{n\rightarrow\infty}{\lim}\mathbb{P}\left(E^c_{n}\right)=1,  
% \end{align}
by Borel Cantelli lemma \cite{grimmett2020probability} which completes the proof. $\qed$
\end{proof}

\subsection{Construction of chain \texorpdfstring{$\gamma'_{n,k}$}{g'}}
\label{subsec:gamma_def}
For each $n\in\mathbb{N}$, define a collision-free path $\gamma_n^{\star\star}: [0,1]\rightarrow \mathbb{R}^d\times \mathbb{S}_\rho^d$ by $\gamma_n^{\star\star}(t)=(x^\star(t), (1-\delta_n) P^\star(t))$ (See Figure~\ref{fig:1}).
It is trivial to check $ \underset{n\rightarrow\infty}{\lim} \gamma^{\star\star}_n = \gamma^\star$ and thus $\underset{n\rightarrow\infty}{\lim}c(\gamma^{\star\star}_n) = c^\star$.


We define a chain $\gamma'_n := (x_{n,k}, (1-\delta_n) P_{n,k})$, $k\in[1;K_n]$, which is the chain generated by partition $\mathcal{P}_n$ on path $\gamma_n^{\star\star}(t)$. It is easy to verify that $(x_{n,k}, (1-\delta_n) P_{n,k}) \in \mathcal{R}_{n,k}$. Thus, from Lemma~\ref{lemma:rn} one can deduce that $\gamma'_n$ is collision-free.   
Using $\{(x_{n,k}, P'_{n,k}= (1-\delta_n) P_{n,k})\}_{k=1}^{K_n}$,
%and $\{(x_{n,k}, P''_{n,k})\}_{k=0, 1, ... , K_n}$,
we define path $\widetilde{\gamma}_n(t)=(\widetilde{x}_n(t), \widetilde{P}_n(t))$ as
\begin{align*}
\widetilde{x}_n(t)&= \frac{t_{n,k+1}-t}{t_{n,k+1}-t_{n,k}}x_{n,k}+\frac{t-t_{n,k}}{t_{n,k+1}-t_{n,k}}x_{n,k+1} \\
\widetilde{P}_n(t)&=P'_{n,k}+\|\tilde{x}_n(t)-x'_{n,k}\| W
\end{align*}
for $t_{n,k}\leq t < t_{n,k+1}, k\in[1;K_n-1]$ with $\widetilde{x}(1)=x_{n,K_n}$, $\widetilde{P}(1)=P'_{n,K_n}$.
Notice that we have
\begin{equation}
\label{eq:c_gamma'}
c^\star \leq c(\widetilde{\gamma}_n)=c(\gamma'_n)\leq c(\gamma^{\star\star}_n)
\end{equation}
since $\widetilde{\gamma}_n$ is collision-free path from initial belief state to the goal region (the first inequality) and
\begin{align}
c(\gamma'_n)=c(\gamma^{\star\star}_n;\mathcal{P}_n) \leq \sup_{\mathcal{P}}c(\gamma^{\star\star}_n;\mathcal{P})=c(\gamma^{\star\star}_n).
\label{eqn:cost_gamma_dash_equals_gamma}
\end{align}
Since $\underset{n\rightarrow\infty}{\lim} c(\gamma^{\star\star}_n)=c^\star$, \eqref{eq:c_gamma'} implies $\underset{n\rightarrow\infty}{\lim} c(\gamma'_n)=c^\star$.



% \begin{figure}[ht!]
% \centering
% \includegraphics[width=0.7\columnwidth]{concentric_ellipses.eps}
% \caption{}
% \label{fig:5}
% \end{figure}

% \begin{figure}[ht!]
% \centering
% \includegraphics[width=1.05\columnwidth]{rirrg_optimality_balls_latex.eps}
% \caption{Covering of collision-free chain $\gamma_n$ with balls of radius $\ell_n$. The event $E_{n,k}$ is the event that a sampled point $(x,P)\in\mathcal{R}_{n,k}$.}
% \label{fig:covering_balls}
% \end{figure}



\subsection{Convergence to optimal path \texorpdfstring{ $\gamma^\star$}{g}}
\label{sec:convergence_to_optimal}

In subsection \ref{sec:enent_E_n}, we showed $ \underset{n\rightarrow\infty}{\lim}\; \mathbb{P}\left(E_n \right)=1\nonumber$. In this subsection, we assume $E_n$ has occurred and show almost surely $G$ contains a path whose cost is arbitrarily close to $c^\star$ in the limit of large $n$. Let the set of paths generated by the IG-PRM* algorithm be denoted by $P_n$ and let $\gamma^p_n$ be closest to $\gamma'_n$ in terms of bounded variation i.e., $\gamma^p_n=\underset{\gamma^p\in P_n}{\argmin}\;\|\gamma^p-\gamma'_n\|_{TV}$. Then, we have the following lemma.
\begin{lemma}
$\mathbb{P}\left(\left\{\underset{n\rightarrow\infty}{\lim}\;\left\|\gamma^p_{n}-\gamma'_{n}\right\|_{TV}=0\right\}\right)=1$
\label{lemma:arbritrarily_close}
\end{lemma}
\begin{proof}
Define set $\mathcal{R}^{\beta}_{n,k}\subseteq \mathcal{R}_{n,k} $ 
\begin{align} \nonumber
    &\mathcal{R}^{\beta }_{n,k}=\Biggl\{(x,P): x\in\mathcal{B}\left(x_{n,k},\beta \ell_n\right),\\ \nonumber
    &{\left(1-\delta_n-\beta \frac{\delta_n}{2}\right)}P_{n,k}\preceq P \preceq  \left(1-\delta_n+\beta \frac{\delta_n}{2}\right) P_{n,k}\Biggl\},
\end{align}
where $0<\beta\leq1$. 
Define $I_{n,k}$ as follows:
\begin{align}
    I_{n, k}:= \begin{cases}1, & \text { if }\mathcal{R}^{\beta}_{n,k} \cap V^{\mathrm{IG-PRM}^{*}}=\emptyset, \\ 0, & \text { otherwise. }\end{cases}
\end{align}
We define $M_n=\sum_{k=1}^{K_n} I_{n,k}$, and examine the event $\{M_n\leq \alpha K_n\}$ which means the event that at least $\alpha$ fraction of all $K_n$ regions (i.e., $R_{n,k}$s regions)  do not contain any sampled belief $b_i \in B$. If 
$\mathcal{R}^{\beta}_{n,k}$ contains
a sampled belief state $(x,P)$, we have \begin{align*}
     \|x-x_{n,k}\| & \leq \beta \ell_n,  \\
    \|P-P_{n,k}\|_F &\leq  \|(1-\delta_n + \beta \frac{\delta_n}{2})  P_{n,k}- (1-\delta_n)P_{n,k}\|_F \\
    &\leq \left((1-\delta_n + \beta \frac{\delta_n}{2})- (1-\delta_n) \right) \|P_{n,k}\|_F\\
    & \leq \beta \frac{\delta_n}{2}  \|P_{n,k}\|_F \\
    & \leq \beta\frac{\delta_n}{2} \|P_{n,k}\|_F \leq \beta \frac{ R}{2h} \ell_n ,
\end{align*}
which yields $\hat{\mathcal{D}}((x,P), (x_{n,k},P_{n,k})) \leq  \beta c \ell_n$, where $c:= 1+R/2h$. For $k$'s that no sample belief is inside $\mathcal{R}^{\beta}_{n,k}$, we can assume there exists a sampled belief $(x,P)$ inside $\mathcal{R}_{n,k}$, because we have assumed $E_n$ has occurred. For such $k$s, we have $\hat{\mathcal{D}}((x,P), (x_{n,k},P_{n,k})) \leq  c \ell_n$ because $ \mathcal{R}_{n,k}= \mathcal{R}^{\beta=1}_{n,k}$.
% \begin{align*}
%      \|x-x_{n,k}\| & \leq \ell_n  \\
%     \|P-P_{n,k}\|_F &\leq  \|(1-\delta_n + \frac{\delta_n}{2})^2  P_{n,k}- (1-\delta_n)^2P_{n,k}\|_F \\
%     &\leq \left((1-\delta_n +  \frac{\delta_n}{2})^2- (1-\delta_n)^2 \right) \|P_{n,k}\|_F\\
%     & \leq \frac{\delta_n}{2} (2-2\delta_n +  \frac{\delta_n}{2}) \|P_{n,k}\|_F \\
%     & \leq  \delta_n \|P_{n,k}\|_F \leq \frac{ 6 \bar{\rho} \sqrt{d}}{\rho} \ell_n ,
% \end{align*}
% which yields $\hat{\mathcal{D}}((x,P), (x_{n,k},P_{n,k})) \leq  c \ell_n$, where $c:= \frac{1}{2}+ \frac{6 \bar{\rho} \sqrt{d}}{\rho}$.
% Similarly, 
% which yields $\hat{\mathcal{D}}((x,P), (x_{n,k},P_{n,k})) \leq \beta c \ell_n$. 
Considering the path generated by connecting the sampled belief in consequent $\mathcal{R}^\beta_{n,k}$ or $\mathcal{R}_{n,k}$ regions, we have 
\begin{align}
\left\|\gamma^p_{n}-\gamma'_{n}\right\|_{TV}& \leq  \sum_{k=1}^{K_n}  \hat{\mathcal{D}}((x,P), (x_{n,k},P_{n,k})\nonumber\\
&\leq K_n (\alpha c \ell_n + (1-\alpha) \beta c \ell_n) \nonumber\\
&\leq c(\alpha+\beta) L,
\end{align}
where { $L=\underset{n\in\mathbb{N}}{\sup}\;\gamma'_n$}. Therefore,  
%The fact that the bounded variation $\|\gamma^p_n-\gamma'_n\|_{TV}$ is upper bounded by $c(\alpha+\beta) L$ implies that
\begin{align}
&\left\{M_{n} \leq \alpha K_{n}\right\} \subseteq
\left\{\left\|\gamma^p_{n}-\gamma'_{n}\right\|_{TV} \leq c(\alpha+\beta) L \right\}\nonumber.
\end{align}
Taking the complement of both sides of above equation and using the monotonicity of probability measures,
\begin{align}
&\mathbb{P}\left(\!\left\{\left\|\gamma^p_{n}-\gamma'_{n}\right\|_{TV}\!>\! c(\alpha+\beta) L \right\}\!\right)
\!\leq\!\mathbb{P}\left(\!\left\{M_{n} \!\geq\! \alpha K_{n}\right\}\!\right).
\label{eqn:complement}
\end{align}
Since \eqref{eqn:complement} is true for any $\alpha,\beta\in(0,1)$, it remains to show that $\mathbb{P}\left(\left\{M_{n} \geq \alpha K_{n}\right\}\right) $ converges to zero. 
Let's denote $\ell^\beta_{ n,k}:=\beta \ell_n)$, $\mathcal{D}^\beta_{n,k}= \{ P\succ 0: {(1-\delta_n-\beta \frac{\delta_n}{2})}P_{n,k}\preceq P \preceq  (1-\delta_n+\beta \frac{\delta_n}{2}) P_{n,k}\}$. Then, expected value of $I_{n,m}$ can be computed as 
\begin{align}
    &\mathbb{E}[I_{n,m}]
    = \mathbb{P}(\{I_{n,m}=1\}) \nonumber\\
    &\leq \left( 1- \mathbb{P}(x\in\mathcal{B}(x,\ell^\beta_{n,k})) \times  \mathbb{P}(P\in\mathcal{D}^\beta_{n,k}) \right)^n\nonumber\\
    &\leq \left( 1 - g_1 \beta^d \delta^{d}_{n}  \times g_2 \beta \delta_n^{\frac{d(d+1)}{2}}  \right)^n\nonumber\\
    &=
    \left(1-g_1 g_2 \beta^{d+1} \delta_n^{\frac{d(d+3)}{2}}\right)^{n}\nonumber\\ \nonumber
    &\leq \text{exp} \left(-n g_1 g_2 \beta^{d+1} \delta_n^{\frac{d(d+3)}{2}} \right)\\
    &=   \text{exp} \left(-g_1 g_2 \beta^{d+1} \gamma^{\frac{d(d+3)}{2}} \frac{\log n}{n}  n\right) \nonumber \\ \nonumber
    &\leq \text{exp} \left(-\frac{-g_1g_2\beta^{d+1}(d^2+3d+2)}{d(d+3)}\log n\right)\nonumber\\
    &= n^{\frac{-\beta^{d+1}g_1g_2(d^2+3d+2)}{d(d+3)}}. \nonumber
\end{align}
Thus, $\mathbb{E}[M_n]=\sum_{m=1}^{K_n} \mathbb{E}[I_{n,m}]= K_n n^{\frac{-\beta^{d+1}g_1g_2(d^2+3d+2)}{d(d+3)}}$. By Markov’s inequality, it follows that
\begin{align*}
    \mathbb{P}\left(\left\{M_{n} \geq \alpha K_{n}\right\}\right) &\leq \frac{\mathbb{E}[M_n]}{\alpha K_n} \leq \frac{K_n n^{\frac{-\beta^{d+1}g_1g_2(d^2+3d+2)}{d(d+3)}}}{\alpha K_n}\nonumber\\
    &= \frac{ n^{\frac{-\beta^{d+1}g_1g_2(d^2+3d+2)}{d(d+3)}}}{\alpha}.  
\end{align*}
it is trivial to verify that for fixed $\alpha$, the last expression tends to $0$ as $n$ tends to $\infty$. Since this argument holds for all small $\alpha, \beta$, 
%using the fact that holds and $\underset{n\rightarrow\infty}{\lim}\;\delta_n=0$, therefore 
\eqref{eqn:complement} implies for all $\epsilon>0$,
$$
\mathbb{P}\left(\left\{\underset{n\rightarrow\infty}{\lim}\;\left\|\gamma^p_{n}-\gamma'_{n}\right\|_{TV}=0\right\}\right)=1. 
$$
$\qed$
% $$
% % \sum_{n=1}^{\infty}
% \mathbb{P}\left(\left\{\left\|\gamma^p_{n}-\gamma'_{n}\right\|_{TV}>\epsilon\right\}\right)<\infty.
% $$
% Finally, by the Borel-Cantelli lemma, $\mathbb{P}\left(\left\{\underset{n\rightarrow\infty}{\lim}\;\left\|\gamma^p_{n}-\gamma'_{n}\right\|_{TV}=0\right\}\right)=1$. $\qed$
\end{proof}
% \begin{lemma}
% % \begin{align}
%    $ \mathbb{P}(\{\lim_{n\rightarrow\infty} c(\gamma^p_n)=c^\star\})=1$
% % \end{align}
% \label{lemma:cont_final}
% \end{lemma}
% \begin{proof}
As shown in Subsection~\ref{subsec:gamma_def} $ \underset{n\rightarrow\infty}{\lim}c(\gamma'_n)=c^\star$.
Using this result, the fact that $\mathbb{P}\left(\left\{\underset{n\rightarrow\infty}{\lim}\;\left\|\gamma^p_{n}-\gamma'_{n}\right\|_{TV}=0\right\}\right)=1$, and the continuity of the cost function (Theorem~\ref{theo:cont}), we can conclude that
% \begin{align*}
      $ \mathbb{P}(\{\underset{n\rightarrow\infty}{\lim}c(\gamma^p_n)=c^\star\})=1. $%$\qed$
% \end{align*}$\qed$
%\end{proof}
% \begin{remark}
%     \normalfont Note that if a robot/agent lacks a sensor to make the necessary measurements, the belief chain generated by IG-PRM* algorithm may seem to be deemed physically unrealizable. However, there is a simple solution to the issue which is elaborated in Section IV.C of \cite{pedram2021gaussian}.
% \end{remark}
\section{Loss-less version of IG-PRM*\label{sec:lossless_ri_prm_star}}
% {\color{red} We explain the difference w.r.t. the lossy algorithm, theorem explaining the convergence, we discuss pros and cons.}
% The beauty of optimal sampling based motion planning algorithms in deterministic configuration space \cite{karaman2011sampling} is that they provide a lower bound for the minimum radius $D_\text{min}$ for collision checking that would ensure asymptotic optimality. Lower the $D_\text{min}$, lower the computational burden on collision checking modules for consequently faster feasible path generation while still guaranteeing asymptotic optimality. For the IG-PRM* algorithm with $\mathcal{V}_X=1,d=2,\;\;\chi=0.9,\;\rho=10^{-3}$ and $R=10^{-2}$, the number of samples $n$ required for $\delta_n$ to be less than 0.5 is approximately of order $10^5$. It must be noted that the IG-PRM* algorithm converges to an optimal feasible path as the number of samples increases. 
Note that only lossless transitions between edges of a feasible path are meaningful for physical systems \cite{pedram2021gaussian}. However, the path generated by IG-PRM* (Algorithm \ref{alg:igpp_body1}) is not guaranteed to be finitely lossless. Although there exists an analytical solution to perform lossless refinement (Algorithm \ref{alg:lossless_modify_ri_prm_star}) for the entire path using the information geometric cost defined in \eqref{eq:def_D}, an analytical solution might not exist for other information metrics such as Hellinger or Wasserstein. To address this limitation, we propose the finitely lossless version of the IG-PRM* algorithm termed Lossless IG-PRM* (Algorithm \ref{alg:lossless_ri_prm}) where the edge between two belief states is guaranteed to be finitely lossless and is devoid of any lossless refinement step. In addition, we prove the asymptotic optimality of Lossless IG-PRM*. 
% As we later show, even though we have a theoretical guarantee about the asymptotic optimality of Lossless IG-PRM*, the algorithm is of less practical significance as the number of samples required for $\delta^L_n$ to be less than $1$ is very large.

Algorithm \ref{alg:lossless_ri_prm} represents the pseudocode for the Lossless IG-PRM*. The function $\operatorname{CollisionFree}(b, b_j, \chi^2)$ checks whether the edge from belief state $b$ to $b_j$ is collision-free. The function $\operatorname{Lossless}(b, b_j)$ checks whether the transition from $b$ to $b_j$ is lossless. The search algorithm (Algorithm \ref{algo:search}) is used to compute the lossless and collision-free chain $\gamma''_n$.

\begin{algorithm}
     % \footnotesize
    { 
    $\left.B \leftarrow\left\{b_{\text{init}}\right\} \cup\{\text {SampleFree}_{i}\right\}_{i\in [1;n]} ; E \leftarrow \emptyset$\;
    % $(z_1) \leftarrow (z_{\text{init}})$; $E \leftarrow \emptyset$; $G'\leftarrow (z_1,E)$\;
    \For{$b\in B$}{
    $\quad B_{\text{nbors}} \leftarrow \operatorname{Near}\left(B, b, D_{\text{min}}\right)$\;
    \For{$b_j\in B_{\text{nbors}}$}{
    \If{$\operatorname{CollisionFree}(b, b_j, \chi^2)\; \operatorname{and}\; \operatorname{Lossless}(b, b_j)$  }{$E \leftarrow E \cup\{(b, b_j)\}$\;}
       % $\quad$ if CollisionFree $(u, v)$ and Lossless$(u, v)$  then $E \leftarrow E \cup\{(u, v)\}$
    }
    }
    \Return $G=(B, E)$
    }
\caption{Lossless IG-PRM*}
\label{alg:lossless_ri_prm}
\end{algorithm}

% \section{Asymptotic optimality of Lossless IG-PRM*\label{sec:asym_optimality_lossless}}
Towards the aim of proving asymptotic optimality, we define modified $\ell^L_n,\;\mathcal{R}^L_{n,k},\;\mathcal{D}^L_{n,k},\;\mathcal{R}^{L,s}_{n,k},\;\mathcal{R}^{L,\beta s}_{n,k}$ and $\delta^L_n$ as follows\footnote{superscript $L$ is used to indicate regions/parameters defined for Lossless IG-PRM* algorithm}:
\begin{align}
     & \ell^L_n:=\min\left\{  \delta^L_n\left(1-\frac{3}{4}\delta^L_n\right)\frac{\rho}{3\bar{\sigma}(W)},{\frac{\delta^L_n}{9}\rho},{\frac{\delta^L_n}{3}\chi^{\frac{1}{4}}{\rho}^{\frac{1}{8}}},q_n\right\},\nonumber\\
     &\quad\geq \delta^L_n h^L \nonumber\\
             & \mathcal{B}^s(x_{n,k},\ell^L_n):=\mathcal{B}(x_{n,k},\frac{(1-(\theta^1_{n,k})^2)}{2}(\ell^L_n)^4)\nonumber\\
        &\mathcal{R}^L_{n,k}=\{(x,P): x\in \mathcal{B}(x_{n,k},(\ell^L_n)^4), P\preceq {\left(\theta^0_{n,k}\right)^2}P''_{n,k} \}\nonumber\\
           &\mathcal{R}^{L,s}_{n,k}=\{(x,P):x\in \mathcal{B}^s(x_{n,k},\ell^L_n)\},\nonumber\\
       &\mathcal{D}^L_{n, k}:=\{P\succeq0: {(\theta^1_{n,k})^2} P''_{n,k} \preceq P\preceq {(\theta^2_{n,k})^2} P''_{n,k}\},\nonumber\\
&\;\quad\quad{(\theta^1_{n,k})^2}P''_{n,k}\preceq P\preceq {(\theta^2_{n,k})^2} P''_{n,k},\} \nonumber\\
            &\mathcal{R}^{L,\beta s}_{n,k}=\Biggl\{(x,P): x\in\mathcal{B}\left(x_{n,k},\frac{\beta(1-(\theta^1_{n,k})^2)}{2}(\ell^L_n)^4\right),\nonumber\\ 
    &{(\beta\theta^1_{n,k}+(1-\beta)\theta^2_{n,k})^2}P''_{n,k}\preceq P \preceq {(\theta^2_{n,k})^2}P''_{n,k}\Biggl\}\nonumber\\
       % & \ell^L_n:=\min\left\{  \delta^L_n\left(1-\frac{3}{4}\delta^L_n\right)\frac{\rho}{3\bar{\sigma}(W)},{\frac{\delta^L_n}{9}\rho},{\frac{\delta^L_n}{3}\chi^{\frac{1}{4}}{\rho}^{\frac{1}{8}}}\right\},\nonumber\\
    % &\geq \delta^L_n h \nonumber\\
        &\delta^L_n=\min\;\left\{\gamma\left(\frac{\text{log}n}{n}\right)^\frac{1}{d(2d+8)},1\right\}\nonumber
\end{align}
where $q_n=\min\left\{\frac{\delta_n\sqrt{ \chi^2 \rho} }{16},\frac{\delta_n\rho }{18\bar{\sigma}(W)}\right\},\;\theta^1_{n,k}=\theta^2_n-(k+1)\Delta$, $\theta^2_{n,k}=\theta^2_n-k\Delta$, and $\Delta=(\theta^2_n-\theta^1_n)/K^L_n$. Here, $\theta^2_{n}:=\sqrt{{1}-\frac{\ell^2_n}{\chi^{\frac{1}{2}}\rho^{\frac{1}{4}}}}$, and $\theta^1_{n}:=\sqrt{1-(\delta^L_n)^2+\frac{\ell^2_n}{\chi^{\frac{1}{2}}\rho^{\frac{1}{4}}}}$, $h^L:=\min\left\{\frac{\rho}{12\bar{\sigma}(W)},\frac{\rho}{9}, \frac{\rho^{\frac{1}{4}}}{3}\right\}$ and $\beta\in(0,1)$ where $\theta^0_{n,k}$ is defined as follows
\begin{align}
   & \theta_{n,k}^{0}= 1- \frac{(\ell^L_n)^4}{\sqrt{\chi^2\underline{\sigma}(P''_{n,k})}}\nonumber   
\end{align}
% Algorithm \ref{alg:lossless_ri_prm} represents the pseudocode for the Lossless IG-PRM*. The function $\operatorname{CollisionFree}(b, b_j, \chi^2)$ checks whether the edge from belief state $b$ to $b_j$ is collision-free. The function $\operatorname{Lossless}(b, b_j)$ checks whether the transition from $b$ to $b_j$ is lossless. The search algorithm (Algorithm \ref{algo:search}) is used to compute the lossless and collision-free chain $\gamma''_n$.
% \begin{algorithm}
%      % \footnotesize
%     { 
%     $\left.B \leftarrow\left\{b_{\text{init}}\right\} \cup\{\text {SampleFree}_{i}\right\}_{i\in [1;n]} ; E \leftarrow \emptyset$\;
%     % $(z_1) \leftarrow (z_{\text{init}})$; $E \leftarrow \emptyset$; $G'\leftarrow (z_1,E)$\;
%     \For{$b\in B$}{
%     $\quad B_{\text{nbors}} \leftarrow \operatorname{Near}\left(B, b, D_{\text{min}}\right)$\;
%     \For{$b_j\in B_{\text{nbors}}$}{
%     \If{$\operatorname{CollisionFree}(b, b_j, \chi^2)\; \operatorname{and}\; \operatorname{Lossless}(b, b_j)$  }{$E \leftarrow E \cup\{(b, b_j)\}$\;}
%        % $\quad$ if CollisionFree $(u, v)$ and Lossless$(u, v)$  then $E \leftarrow E \cup\{(u, v)\}$
%     }
%     }
%     \Return $G=(B, E)$
%     }
% \caption{Lossless IG-PRM*}
% \label{alg:lossless_ri_prm}
% \end{algorithm}

% \subsection{Asymptotic optimality of Lossless IG-PRM*}
The outline of the asymptotic optimality proof is as follows.
The quantities $\gamma^\star$, $\gamma^\star(t)$, $\gamma_n'$ and $\mathcal{P}_n$ are same as that described in Section \ref{subsection:outline_of_proof}. However, for Lossless IG-PRM*, in addition to proving that the entire path is collision-free, we also need to show that the path is finitely lossless. This necessitates us to modify some constants and regions compared to that mentioned in Section \ref{subsection:outline_of_proof} and the mathematical reason behind it would be clear in subsequent sections.
% Let $\gamma^\star: [0,1]\rightarrow \mathbb{R}^d\times \mathbb{S}_\rho^d$, $\gamma^\star(t)=(x^\star(t), P^\star(t))$ be the optimal path. Consider the equispaced partition $\mathcal{P}_n=(0=t_{n,0}\leq t_{n,1} \leq \cdots \leq t_{n,K^L_n}=1)$, and define the chain $(x_{n,k}, P_{n,k})\triangleq (x^\star(t_{n,k}), P^\star(t_{n,k}))$, $k\in[1;K^L_n]$ on $\gamma_n$.
% By construction, we have $\|x_{n,k+1}-x_{n,k}\|\leq \ell^L_n$ for each $k\in[1;K^L_n]$. The chain $\{(x_{n,k}, P_{n,k})\}_{k=[1; K^L_n]}$ constructed this way is neither collision-free nor finitely lossless in general. To that end, we construct a collision-free chain $\gamma_n'\triangleq(x_{n,k},P'_{n,k}),\;k\in[1;K^L_n]$ (Lemma \ref{lemma:chain_col_free_lossless}).
As $\gamma_n'$ might not be finitely lossless, we first consider the lossless refinement of $\gamma_n'$ to obtain a finitely lossless and collision-free chain $\gamma_n''\triangleq (x_{n,k},P''_{n,k}),\;k\in[1;K^L_n]$ and show the cost of $\gamma''$ converges to the cost of the optimal path $c^\star=c(\gamma^\star)$ in the limit of $n$ tending to infinity.
\begin{figure*}[ht]
    % \begin{minipage}{0.75\textwidth}
        \centering
 \captionsetup[subfigure]{justification=centering}
 \centering
 \begin{subfigure}{0.24\textwidth}
{\includegraphics[width=4.5cm]{plot_2k.eps}}
\caption{$c(\gamma_n)=1.12$}
%\label{fig:}
 \end{subfigure}
 \begin{subfigure}{0.24\textwidth}
 \includegraphics[width=4.5cm]{plot_8k.eps}
 \caption{$c(\gamma_n)=0.90$}
%\label{fig:}
 \end{subfigure}
  \begin{subfigure}{0.24\textwidth}
{\includegraphics[width=4.5cm]{plot_15k.eps}}
\caption{$c(\gamma_n)=0.61$}
%\label{fig:}
 \end{subfigure}
 \begin{subfigure}{0.24\textwidth}
 \includegraphics[width=4.5cm]{move_and_sense.eps}
\caption{$c^\star=0.60$}
%\label{fig:}
\end{subfigure}
 \caption{Figs. (a), (b) and (c) are the paths computed by IG-PRM* after 2000, 8000, and 15000 iterations respectively. (d) The optimal path generated in the absence of obstacles from initial belief state $b_{\text{init}}$ to final belief state $b_{\text{goal}}$ using the move and sense strategy presented in \cite{pedram2021gaussian}.}
\label{fig:evolution_of_path}
    % \end{minipage}
\vspace{-2ex}
\end{figure*}
\begin{figure}[ht!]
\centering
\includegraphics[width=0.99\columnwidth]{conv_no_obs.eps}
\caption{The plot shows the convergence of the cost computed by IG-PRM* algorithm to the optimal cost $c^\star=0.60$ as the number of samples $n$ increases.}
\label{fig:conv_no_obs}
\end{figure}
To draw the connection between $\gamma''_n$ and the cost of paths returned by the Lossless IG-PRM* algorithm (Algorithm \ref{alg:lossless_ri_prm}), the regions $\mathcal{R}^L_{n,k}$ are constructed in such a way that the transition between consecutive regions i.e., between any belief state  $b_k \in \mathcal{R}^L_{n,k}$ and any belief state $b_{k+1} \in \mathcal{R}^L_{n,k+1}$ for all $k\in[1;K^L_n-1]$ is collision-free. However, as the edge connecting $b_k \in \mathcal{R}^L_{n,k}$ to any belief state $b_{k+1} \in \mathcal{R}^L_{n,k+1}$ might not be lossless, we define a region $\mathcal{R}^{L,s}_{n,k}$ which is a subset of $\mathcal{R}^L_{n,k}$ and show the transition between any $b^s_k \in \mathcal{R}^{L,s}_{n,k}$ to any $b^s_{k+1} \in \mathcal{R}^{L,s}_{n,k+1}$ is both lossless and collision-free (Lemma \ref{lemma:transition_is_lossless}). Furthermore, we show that the distance between $b_k$ and $b_{k+1}$ is smaller that $D_{\textup{min}}$ (Lemma \ref{lemma:distance_smaller_than_ed_min_lossless}). However, it is still not clear whether a connection between $b_k^s$ and $b_{k+1}^s$ will be established by Algorithm~\ref{alg:lossless_ri_prm}, if $b^s_k \in B$ and $b^s_{k+1} \in B$.
To prove this mathematically, we define event $E_{n} \triangleq E_{n,1}\cap E_{n,2}\dots \cap E_{n,K^L_n}$ as the event that a belief state is sampled inside all $\mathcal{R}^{L,s}_{n,k}$ regions, and show that the event $E_n$ occurs with probability one as $n$ tends to infinity (Lemma \ref{lemma:event_E_n_for_lossless}). 
We then show there exists a path on the graph generated by the Lossless IG-PRM$^\star$ algorithm that gets arbitrarily close to $\gamma''_n$ as $n$ tends to infinity (Lemma \ref{lemma:arbritrarily_close_for_lossless}). Finally, we leverage the continuity of path cost function to show that the cost of that path returned by Lossless IG-PRM* gets arbitrarily close to $c^\star$ (Lemma \ref{lemma:cont_final_lossless}).
% \begin{algorithm}
%      \footnotesize
%     { 
%     $\left.B \leftarrow\left\{b_{\text{init}}\right\} \cup\{\text {SampleFree}_{i}\right\}_{i\in [1;n]} ; E \leftarrow \emptyset$\;
%     % $(z_1) \leftarrow (z_{\text{init}})$; $E \leftarrow \emptyset$; $G'\leftarrow (z_1,E)$\;
%     \For{$b\in B$}{
%     $\quad B_{\text{nbors}} \leftarrow \operatorname{Near}\left(B, b, D_{\text{min}}\right)$\;
%     \For{$b_j\in B_{\text{nbors}}$}{
%     \If{$\operatorname{CollisionFree}(b, b_j, \chi^2)\; \operatorname{and}\; \operatorname{Lossless}(b, b_j)$  }{$E \leftarrow E \cup\{(b, b_j)\}$\;}
%        % $\quad$ if CollisionFree $(u, v)$ and Lossless$(u, v)$  then $E \leftarrow E \cup\{(u, v)\}$
%     }
%     }
%     \Return $G=(B, E)$
%     }
% \caption{Lossless IG-PRM*}
% \label{alg:lossless_ri_prm}
% \end{algorithm}
\begin{lemma}
\label{lemma:chain_col_free_lossless}
\normalfont The chain $\gamma'_n = (x_{n,k}, P'_{n,k})\triangleq (x^\star(t_{n,k}), (1-\delta^L_n)^2 P^\star(t_{n,k}))\triangleq (x_{n,k}, P'_{n,k})\triangleq (x_{n,k}, (1-\delta^L_n)^2 P_{n,k}) $, $k\in[1;K_n]$ is collision-free. 
% \textcolor{blue}{Can we have $(1-\delta^L_n)$ instead of $(1-\delta^L_n)^2$?}
\end{lemma}
\begin{proof}
    Please see Appendix \ref{appendix:chain_col_free_lossless}
\end{proof}
\begin{lemma}
 \normalfont In any transition from $(x_1, P_1) \in \mathcal{R}^L_{n,k}$  $(x_2, P_2) \in \mathcal{R}^L_{n,k+1}$, both initial confidence ellipse and final confidence ellipse are contained inside $\mathcal{E}_{\chi^2} (x_{n,k},P_{n,k})$. More precisely,
\begin{align}
\label{eq:init_in}
&\mathcal{E}_{\chi^2}(x_1, P_1) \subseteq \mathcal{E}_{\chi^2} (x_{n,k},P_{n,k}),
\\ \label{eq:final_in_lossless}
&\mathcal{E}_{\chi^2}(x_2, P_1+||x_2-x_1||W) \subseteq \mathcal{E}_{\chi^2} (x_{n,k},P_{n,k}),
\end{align}
which proves the transition $(x_1,P_1)\rightarrow (x_2,P_2)$ fully resides in $\mathcal{E} _{\chi^2}(x_{n,k},P_{n,k})$, and thus it is collision-free. 
\label{lemma:collision_free_lossless}
\end{lemma}
\begin{proof}
    Please see Appendix \ref{appendix:collision_free_lossless}.
\end{proof}

\begin{lemma}
\normalfont The transition between any state $(x_1,P_1)\in\mathcal{R}^{L,s}_{n,k}$ to $(x_2,P_2)\in\mathcal{R}^{L,s}_{n,k+1}$ for all $k\in[1;K_n-1]$ is finitely lossless and collision-free.
\label{lemma:transition_is_lossless}
\end{lemma}
\begin{proof}
    Please see Appendix \ref{appendix:transition_is_lossless}.
\end{proof}

\begin{lemma}
\normalfont The distance $\hat{D}(b_1,b_2)$ between any $b_1=(x_1,P_1)\in\mathcal{R}^{L,s}_{n,k}$ and any $b_2=(x_2,P_2)\in\mathcal{R}^{L,s}_{n,k+1}$ for all $k\in[1;K_n-1]$ is less than $D_{\text{min}}$.
\label{lemma:distance_smaller_than_ed_min_lossless}
\end{lemma}
\begin{proof}
    Please see Appendix \ref{appendix:distance_smaller_lossless}.
\end{proof}


\begin{lemma}
\normalfont If
$\gamma>\left(\frac{d(2d+8)+1}{g^L_1 g^L_2 d(2d+8)} \right)^\frac{1}{d(2d+8)}$, 
then $\underset{n\rightarrow\infty}{\lim}\; \mathbb{P}\left(E_n \right)=1$.
\label{lemma:event_E_n_for_lossless}
\end{lemma}
\begin{proof}
    Please see Appendix \ref{appedix:event_en_lossless}.
\end{proof}


\begin{lemma}
$\mathbb{P}\left(\left\{\underset{n\rightarrow\infty}{\lim}\;\left\|\gamma^p_{n}-\gamma''_{n}\right\|_{TV}=0\right\}\right)=1$
\label{lemma:arbritrarily_close_for_lossless}
\end{lemma}
\begin{proof}
    Please see Appendix \ref{appendix:arbitr_close_lossless}.
\end{proof}
\begin{lemma}
% \begin{align}
   $ \mathbb{P}(\{\underset{n\rightarrow\infty}{\lim} c(\gamma^p_n)=c^\star\})=1$
% \end{align}
\label{lemma:cont_final_lossless}
\end{lemma}
\begin{proof}
    Please see Appendix \ref{appendix:cont_final_lossless}.
\end{proof}

\section{Numerical Experiments}
\label{sec:experiments}
% {\color{red} a) we include an experiment that we could analytically find the solution\\
% b) we include one or two more complex experiments. }
We consider a scenario with no obstacles and verify that Algorithm \ref{alg:igpp_body1} converges to the analytically computed cost as the number of samples increases. 
% Next, we consider an environment with two or more obstacles.
\subsection{Obstacle free space}
In obstacle-free space, we compute the optimal solution to \eqref{eq:def_D} by using Theorem 1 in \cite{pedram2021gaussian}. We then use the IG-PRM* algorithm (Algorithm \ref{alg:igpp_body1}) to plot the cost returned as the number of samples $n$ increases. The obstacle-free space considered is of dimension $1\mathrm{m}\times1\mathrm{m}$ with initial belief state $b_{\text{init}}:=(x_{\text{init}},P_{\text{init}})$ and goal belief state $b_{\text{goal}}:=(x_{\text{goal}},P_{\text{goal}})$ where $x_{\text{init}},\;P_{\text{init}},\;x_{\text{goal}}$ and $P_{\text{goal}}$ are defined as 
\begin{align}
    &x_{\text{init}}=[0.2,\;0.5]^\mathrm{T},\quad x_{\text{goal}}=[0.8,\;0.5]^\mathrm{T}\nonumber\\
    &P_{\text{init}}=10^{-4}I,\quad P_{\text{goal}}=10^{-3}I\nonumber
\end{align}
% We choose $\rho=10^{-4}$ and $R=2.10^{-3}$
We perform a total of 70 experiments where the IG-PRM* is run for iterations $i=2000+3000m$ for $m\in[0;6]$ and for each $m$, 10 experiments were performed. The average cost is calculated and the results are depicted as an error plot as shown in Fig. \ref{fig:conv_no_obs}. From Fig. \ref{fig:conv_no_obs}, it can be observed that the cost returned by IG-PRM* converges to the optimal cost $c^\star$ as the number of samples increases. Fig. \ref{fig:evolution_of_path} shows the evolution of the path as the number of samples increases. 

% \begin{figure}[ht!]
% \centering
% \includegraphics[width=0.99\columnwidth]{move_and_sense.eps}
% \caption{The optimal path generated in absence of obstacles from initial belief state $b_{\text{init}}$ to final belief state $b_{\text{goal}}$ using the move and sense strategy presented in \cite{pedram2021gaussian}.}
% \label{fig:conv_no_obs}
% \end{figure}
% \subsection{With obstacles}

\section{Conclusion and Future Work}
 In this paper, we proposed a sampling based motion planning algorithm in Gaussian belief space termed IG-PRM* that minimizes the information geometric cost and proved that the algorithm converges to the optimal solution as the number of samples tends to infinity. We then proposed a variant of IG-PRM* termed Lossless IG-PRM* which does not require lossless modification and prove that the algorithm is asymptotically optimal. In the future, we plan to reduce the computational burden for the IG-PRM* and explore methods to increase the rate of converge of the cost computed by IG-PRM* 
\label{sec:conclusion}









%\section{Support for \textsf{\journalclass}}
%We offer on-line support to participating authors. Please contact
%us via e-mail at \dots
%
%We would welcome any feedback, positive or otherwise, on your
%experiences of using \textsf{\journalclass}.

% \section{Copyright statement}
% Please  be  aware that the use of  this \LaTeXe\ class file is
% governed by the following conditions.

% \subsection{Copyright}
% Copyright \copyright\ \volumeyear\ SAGE Publications Ltd,
% 1 Oliver's Yard, 55 City Road, London, EC1Y~1SP, UK. All
% rights reserved.

% \subsection{Rules of use}
% This class file is made available for use by authors who wish to
% prepare an article for publication in a \textit{SAGE Publications} journal.
% The user may not exploit any
% part of the class file commercially.

% This class file is provided on an \textit{as is}  basis, without
% warranties of any kind, either express or implied, including but
% not limited to warranties of title, or implied  warranties of
% merchantablility or fitness for a particular purpose. There will
% be no duty on the author[s] of the software or SAGE Publications Ltd
% to correct any errors or defects in the software. Any
% statutory  rights you may have remain unaffected by your
% acceptance of these rules of use.

\begin{acks}
This work was supported in part by the Lockheed Martin
Corporation under Grant MRA16-005-RPP009, in part by the Air Force Office
of Scientific Research under Grant FA9550-20-1-0101,
% This class file was developed by Sunrise Setting Ltd,
% Brixham, Devon, UK.\\
% Website: \url{http://www.sunrise-setting.co.uk}
\end{acks}

%%Harvard (name/date)
\bibliographystyle{SageH}
\bibliography{main.bib}

\appendix
 \section{Proof of Theorem~\ref{theo:volume}}
 \label{app:volume}
% We first review the basics of Riemannian geometry of $\mathcal{S}^d_{+}$ and introduce the notation we used to compute the volume of $\mathcal{D}_{A}$ and $\mathcal{R}_c$ in Appendix~\ref{app:D_A} and Appendix~\ref{app:R_c}.

% The Rao-Fisher metric is defined as
% \begin{align*}
% ds^2(P)=\textup{Tr}(P^{-1}dP)^2,
% \end{align*}
% and induces a Riemannian distance $d:\mathbb{S}^d_{+} \times  \mathbb{S}^d_{+}  \rightarrow \mathbb{R}_{+}$, referred to as Rao’s distance, where 
% \begin{align}
% \label{eq:distance}
% d^2(P_1, P_2)= \textup{Tr}\bigg(\log(P_1^{-1/2} P_2 P_1^{-1/2})\bigg)^2.
% \end{align}
% In \eqref{eq:distance}, square root and logarithm should be understood as symmetric matrix functions. The Riemannian volume element associated to the Rao-Fisher metric is 
% \begin{align}
%     dv(P)= \det(P)^{-\frac{d+1}{2}} \prod_{i\leq j} dP_{ij},
% \end{align}
% where indices denote matrix entries. 

We adopt the eigenvalue decomposition to parameterize $P \in \mathbb{S}^d_{+}$ as
\[P(\Lambda,U)=U^\top \Lambda U,\]
where $\Lambda=\textup{diagonal}(\lambda_1, \dots, \lambda_d)$ and $U$ is a $d\times d$ real orthonormal matrix (i.e., $UU^\top=U^\top U=I_d$). Using this parametrization, referred to as polar parametrization, we can express the volume element $dP$ as
% \begin{align*}
%     ds^2(Y)= \sum_{i}^d \lambda_i^{-2} d\lambda_i^2+ 2 \sum_{i<j}^d \frac{(\lambda_i-\lambda_j)^2}{\lambda_i \lambda_j} \theta_{ij}^2. 
% \end{align*}
\begin{align*}
dP = \det \theta \;   \prod_{i<j}^d |\lambda_i-\lambda_j| \prod_i^d d\lambda_i, 
\end{align*}
where $\theta_{ij}=\sum_{k=1}^d U_{jk}dU_{ik}$ and
% \prod_{i=1}^d\lambda_i^{\frac{-(d+1)}{2}}
$\det \theta$ is the exterior product $\det(\theta)= \wedge_{i<j} \theta_{i,j}$ \cite{terras2012harmonic, mathai1997jacobians}.
%Please, see   for a detailed derivation of results above. 
The integral of any function $f:\mathbb{S}^d_{+} \rightarrow \mathbb{R}$ over $\mathbb{S}^d_{+}$ can be computed using polar parametrization as: 
\begin{align}
\nonumber
    &\int_{\mathbb{S}^d_{+}} f(P) dP = (d! \; 2^d)^{-1}  \int_{O(d)} \int_{\mathbb{R}^d_{+}}\bigg(\\
    \label{eq:lambda}
    &  f(P(\Lambda, U))\det \theta \;   \prod_{i<j}^d |\lambda_i-\lambda_j|\bigg) \prod_{i=1}^d d\lambda_i. 
\end{align}
% \prod_{i=1}^d \lambda_i^{\frac{-(d+1)}{2}}
where $O(d)$ is space of $d\times d$ real orthonormal matrices. The factor $(d! \; 2^d)^{-1}$ in \eqref{eq:lambda} resolves the ambiguity of all possible orderings of $\lambda_1, \dots, \lambda_d$ and all possible orientation (multiplication by $+1$ or $-1$) of the columns of $U$.
%\subsection{Volume of 
%\texorpdfstring{$\mathcal{D}_A$}{Da}}
%\label{app:D_A}
 The volume of $\mathcal{D}_A$ can be computed by substituting the indicator function of $\mathcal{D}_A$ in place of $f(P(\Lambda,U))$ in \eqref{eq:lambda}. Thus,
\begin{align*}
&\textup{Vol}(\mathcal{D}_A) \nonumber\\
=&\int_{O(d)} \int_{\Lambda\preceq A}\bigg(\det \theta \;   \prod_{i<j}^d |\lambda_i-\lambda_j|\bigg) \prod_{i=1}^d d\lambda_i\nonumber\\
% (d! \; 2^d)^{-1} \left( \int_{O(d)} \det \theta \right) \\
% &\left( \int_{0 \leq \lambda_i \leq a_i} 
%  \prod_{i<j}^d |\lambda_i-\lambda_j| \prod_i^d d\lambda_i\right)\nonumber\\
 &\geq\textup{Vol}(\mathcal{D}'_A)\nonumber\\
 =&(d! \; 2^d)^{-1} \left( \int_{O(d)} \det \theta \right) \\
&\left( \int_{0 \leq \lambda_i \leq \underset{k\in[1;d]}{\min}a_k} 
 \prod_{i<j}^d |\lambda_i-\lambda_j| \prod_i^d d\lambda_i\right)
\end{align*}
%  \prod_{i=1}^d \lambda_i^{\frac{-(d+1)}{2}}
It is a well-known result (see e.g., \cite[page 71]{muirhead2009aspects} or \cite{chikuse2003statistics} ) that $\int_{O(d)} \det \theta= \frac{2^d \pi^{d^2/2}}{\Gamma_d(d/2)}$, where $\Gamma_d$ is the gamma function. To compute the integral
\begin{align}
\label{eq:integ}
    \int_{0 \leq \lambda_i \leq \underset{k\in[1;d]}{\min}a_k}    \prod_{i<j}^d |\lambda_i-\lambda_j| \; \prod_i^d d\lambda_i,
\end{align}
% \prod_{i=1}^d \lambda_i^{\frac{-(d+1)}{2}}
we use the transformation $y_i=\lambda_i/b$ where $b=\underset{k\in[1;d]}{\min}\;a_k$. Therefore, $\prod_{i=1}^ddy_i=\prod_{i=1}^d\lambda_i/b^d$. Consequently, the integral \eqref{eq:integ} becomes
\begin{align}
% \label{eq:integ}
    &\int_{0 \leq y_i \leq 1}    \prod_{i<j}^d |y_i-y_j| \; \prod_i^d dy_i\nonumber\\
    &=b^{\frac{d(d+1)}{2}}\int_{0 \leq y_i \leq 1}    \prod_{i<j}^d |y_i-y_j| \; \prod_i^d dy_i\nonumber\\
    &=b^{\frac{d(d+1)}{2}}S_d(1,1,1/2)
\end{align}
where $S_d(\alpha_1,\alpha_2,\alpha_3)$ is the Selberg integral \cite{grafakos1999selberg} given by
\begin{align}
     &S_d(\alpha_1, \alpha_2, \alpha_3)\nonumber\\
     &=\int_0^1 .. \int_0^1 \prod_{i=1}^d t_i^{\alpha_1-1}\left(1-t_i\right)^{\alpha_2-1} \prod_{1 \leq i<j \leq d}\left|t_i-t_j\right|^{2 \alpha_3} d t \nonumber \\ 
     & =\prod_{j=0}^{d-1} \frac{\Gamma(\alpha_1+j \alpha_3) \Gamma(\alpha_2+j \gamma) \Gamma(1+(j+1) \gamma)}{\Gamma(\alpha_1+\alpha_2+(d+j-1) \alpha_3) \Gamma(1+\alpha_3)}\nonumber
\end{align}
which results to \eqref{eq:vol_DA}. Now, for $\beta>1$, we have
\begin{align}
     &\frac{\textup{Vol}(\mathcal{D}_{\beta A})}{\textup{Vol}(\mathcal{D}_A)}\nonumber\\
     &=\frac{\int_{O(d)} \int_{\Lambda\preceq \beta A}\bigg(\det \theta \;   \prod_{i<j}^d |\lambda_i-\lambda_j|\bigg) \prod_{i=1}^d d\lambda_i}{\int_{O(d)} \int_{\Lambda\preceq A}\bigg(\det \theta \;   \prod_{i<j}^d |\lambda_i-\lambda_j|\bigg) \prod_{i=1}^d d\lambda_i}
\end{align}
Substituting $z_i= \lambda_i/\beta$, we have
\begin{align}
    &\frac{\textup{Vol}(\mathcal{D}_{\beta A})}{\textup{Vol}(\mathcal{D}_A)}\nonumber\\
     &=\frac{\beta^{\frac{d(d+1)}{2}}\int_{O(d)} \int_{\Lambda\preceq A}\bigg(\det \theta \;   \prod_{i<j}^d |z_i-z_j|\bigg) \prod_{i=1}^d dz_i}{\int_{O(d)} \int_{\Lambda\preceq A}\bigg(\det \theta \;   \prod_{i<j}^d |\lambda_i-\lambda_j|\bigg) \prod_{i=1}^d d\lambda_i}\nonumber\\
     &=\beta^{\frac{d(d+1)}{2}}
\end{align}

Furthermore, we have the following
\begin{align}
    \frac{\textup{Vol}(\mathcal{D}'_{\beta A})}{\textup{Vol}(\mathcal{D}'_A)}=\frac{ \int_{0 \leq \lambda_i \leq \beta b}    \prod_{i<j}^d |\lambda_i-\lambda_j| \; \prod_i^d d\lambda_i}{ \int_{0 \leq \lambda_i \leq b}    \prod_{i<j}^d |\lambda_i-\lambda_j| \; \prod_i^d d\lambda_i}
    \label{eqn:div}
\end{align}
Consider the change of variable as $b_i=\lambda_i/\beta$. Then, \eqref{eqn:div} becomes
\begin{align}
        \frac{\textup{Vol}(\mathcal{D}'_{\beta A})}{\textup{Vol}(\mathcal{D}'_A)}&=\frac{\beta^{\frac{d(d+1)}{2}} \int_{0 \leq b_i \leq  a_i}    \prod_{i<j}^d |b_i-b_j| \; \prod_i^d db_i}{ \int_{0 \leq \lambda_i \leq a_i}    \prod_{i<j}^d |\lambda_i-\lambda_j| \; \prod_i^d d\lambda_i}\nonumber\\
        &=\beta^{\frac{d(d+1)}{2}}=\frac{\textup{Vol}(\mathcal{D}_{\beta A})}{\textup{Vol}(\mathcal{D}_A)}
        \label{eqn:div_beta}
\end{align}
Subtracting 1 from both sides of \eqref{eqn:div_beta}, we get
\begin{align}
    &\frac{\textup{Vol}(\mathcal{D}_{\beta A})-\textup{Vol}(\mathcal{D}_{ A})}{\textup{Vol}(\mathcal{D}_{ A})}=\frac{\textup{Vol}(\mathcal{D}'_{\beta A})-\textup{Vol}(\mathcal{D}'_{ A})}{\textup{Vol}(\mathcal{D}'_{ A})}\nonumber\\
    &\geq \textup{Vol}(\mathcal{D}'_{\beta A})-\textup{Vol}(\mathcal{D}'_{ A})\nonumber\\
    &=(\beta^{\frac{d(d+1)}{2}}-1)\textup{Vol}(\mathcal{D}'_{ A})\nonumber\\
    &\geq (\beta^{\frac{d(d+1)}{2}}-1) b^{\frac{d(d+1)}{2}}S_d(1,1,1/2)
\end{align}
and hence the result follows.

% we use the transformation proposed in \cite[Appendix~B]{krishnamachari2013geometry} and define a set of new 
% variables $y_i$s as
% \begin{align*}
%     y_i &= \lambda_1^i+ \lambda_2^i+\dots +\lambda_d^i,\quad i=1, \dots, d-1,\\
%     y_d &= \lambda_1 \lambda_2 \dots \lambda_d.
% \end{align*}
% We stress this transformation is one-to-one when the $\lambda_i$’s are greater than zero. As shown in \cite[Appendix~B]{krishnamachari2013geometry}, the Jacobian of this conversion is $\pm(d-1)! \prod_{i<j}^d (\lambda_i-\lambda_j)$. Hence, $\prod_{i=1}^d dy_i = (d-1)! \prod_{i<j}^d |\lambda_i-\lambda_j| \; \prod_i^d d\lambda_i$. It is easy to verify $y_i \in [0,\; s_i]$ for all $i=1, \dots, d-1$, where $s_i=\sum_{j=1}^d a_j^i$. Also, $y_d \in [0,\;s_d]$ with $s_d =\prod_{j=1}^d a_j$. Therefore, the integral \eqref{eq:integ} becomes
% \begin{align*}
%     &\frac{1}{(d-1)!}\left(\int_{y_1=0}^{s_1} dy_1\right)  \dots 
%     \left(\int_{y_{d}=0}^{s_{d}} dy_{d}\right)
%     = \frac{1}{(d-1)!} \prod_{i=1}^d s_i,
% \end{align*}
% which results to \eqref{eq:vol_DA}.
% \subsection{Volume of 
% \texorpdfstring{$\mathcal{R}_c$}{Rc}}
% \label{app:R_c}
% If we repeat the steps we took to compute $\textup{Vol}(\mathcal{D}_A)$, we get
% \begin{align*}
% \textup{Vol}(\mathcal{R}_c) =& (d! \; 2^d)^{-1}  \frac{2^d \pi^{d^2/2}}{\Gamma_d(d/2)}  \\
% &\left( \int_{0 \leq \lambda_i, \sum_i \lambda_i\leq c} \prod_{i<j}^d |\lambda_i-\lambda_j| \prod_i^d d\lambda_i\right).
% \end{align*}
% We can normalize $\lambda_i$s by defining a new set of variables as $\gamma_i = \lambda_i/c$. Then, we have 
% \begin{align}
% \nonumber
% &\int_{0 \leq \lambda_i, \sum_i \lambda_i\leq c} \prod_{i<j}^d |\lambda_i-\lambda_j| \prod_i^d d\lambda_i \\ \label{eq:int_gamma}
% &= c^{\frac{d(d+1)}{2}}  \int_{0 \leq \gamma_i, \sum_i \gamma_i\leq1} \prod_{i<j}^d |\gamma_i-\gamma_j| \prod_i^d d\gamma_i.     
% \end{align}
% \textcolor{blue}{We can use the change variables to $y_i$s again to compute the integral in RHS of \eqref{eq:int_gamma}. It is easy to verify that $y_i\in [0, y_{i-1})$ for $i=2, \dots d-1$ and $y_1\in [0,1]$. Using \say{arithmetic mean geometric mean} inequality, we have $y_d \in [0, \left(\frac{y_{d-1}}{d}\right)^{\frac{d}{d-1}}]$ for $d>1$. Thus, for $d>1$
% \begin{align}
% \nonumber
%     &\int_{0 \leq \gamma_i, \sum_i \gamma_i=1} \prod_{i<j}^d |\gamma_i-\gamma_j| \prod_i^d d\gamma_i \\ \nonumber
%     &=\frac{1}{(d-1)!}\int^1_0\int^{y_{1}}_0...\int^{y_{d-2}}_0\int^{\left(\frac{y_{d-1}}{d}\right)^{\frac{d}{d-1}}}_0dy_dd_{y_{d-1}}...dy_1\\ \nonumber
%     &=\frac{1}{(d-1)!}\left(\frac{1}{d}\right)^{\frac{d}{d-1}}\frac{(d-1)^{(d-1)}}{\prod_{i=2}^d(id-i+1) } \nonumber
%     % &\geq\frac{1}{(d-1)!}\left(\frac{1}{d}\right)^{\frac{2d^2}{d-1}}
%     % &= \frac{1}{(d-1)!}\left(\int_{y_1=0}^1 dy_1\right)  \dots 
%     % \left(\int_{y_{d-1}=0}^1 dy_{d-1}\right)\\ \nonumber & \qquad \ \ \quad \quad \left(\int_{y_{d}=0}^{\left(\frac{1}{d}\right)^d} dy_{d}\right)
%     % = \left(\frac{1}{d}\right)^d \frac{1}{(d-1)!},
% \end{align}
% which completes the proof.}



\section{Proof of Lemma~\ref{lemma:diag} }
\label{app:diag}
$\textup{Vol}(\mathcal{D}_P)$ can be computed by substituting the indicator function of $\mathcal{D}_P$ in place of $f(P(\Lambda,U))$ in \eqref{eq:lambda}. If we introduce a transformation of variables from $U$ to $U'=U V^\top$, the Jacobian of the transformation is  $1$, and the integral becomes  the $\textup{vol}(\mathcal{D}_\Sigma)$ in the $(\theta', P)$ coordinate system. $\qed$ 

\section{Proof of Lemma \ref{lemma:chain_col_free_lossless}\label{appendix:chain_col_free_lossless}}
To show that the transition from $(x_{n,k}, P'_{n,k})$ to $(x_{n,k+1}, P'_{n,k+1})$ is collision-free, it is sufficient to show that
\begin{equation}
\label{eq:claim2_1}
    \mathcal{E}_{\chi^2}(x_{n,k+1}, P'_{n,k}+\ell^L_n W) \subset\mathcal{E}_{\chi^2}(x_{n,k}, P_{n,k}).
\end{equation}
In what follows, we derive a sufficient condition for $\ell^L_n$ to satisfy \eqref{eq:claim2_1}. Suppose that $\ell^L_n$ is small enough so that
\begin{equation}
\label{eq:claim2_2}
   P'_{n,k}+\ell^L_n W \preceq P'_{n,k}+3\ell^L_n W \preceq \left(1-\frac{\delta^L_n}{2}\right)^2 P_{n,k}
\end{equation}
holds. \eqref{eq:claim2_2} is equivalent to
\begin{align*}
    (1-\delta^L_n)^2 P_{n,k}+\ell^L_n W &\preceq(1-\delta^L_n)^2 P_{n,k}+3\ell^L_n W\\
    &\preceq \left(1-\frac{\delta^L_n}{2}\right)^2 P_{n,k}
\end{align*}
or
\begin{equation}
    \label{eq:claim2_3}
   3 \ell^L_n W \preceq \delta^L_n \left(1-\frac{3}{4}\delta^L_n\right) P_{n,k}.
\end{equation}
which trivially holds as $    \ell^L_n \leq  \delta^L_n \left(1-\frac{3}{4}\delta^L_n\right) \frac{\rho}{3 \bar{\sigma}(W)}$.
% Since $P_{n,k}\succeq \rho I$, \eqref{eq:claim2_3} holds if
% \begin{equation}
%     \label{eq:claim2_4}
%     \ell^L_n \leq  \delta^L_n \left(1-\frac{3}{4}\delta^L_n\right) \frac{\rho}{3 \bar{\sigma}(W)}.
% \end{equation}
In order for \eqref{eq:claim2_2} to imply \eqref{eq:claim2_1}, we further require the distance $\|x_{n,k+1}-x_{n,k}\|$ between the centers of two ellipses in \eqref{eq:claim2_1} is less than or equal to the difference between semi-minor axis lengths of ellipses characterized by $P_{n,k}$ and $(1-\frac{\delta^L_n}{2})^2 P_{n,k}$ computed as $\frac{\delta^L_n}{2}\sqrt{\chi^2\underline{\sigma}(P_{n,k})}$.
Notice that $
\|x_{n,k+1}-x_{n,k}\|\leq \ell^L_n \leq 3\ell^L_n$.
On the other hand, the difference between the semi-minor axis lengths is greater than $\frac{\delta^L_n}{2}\sqrt{\chi^2\rho}$. Therefore, if
\begin{equation}
    \label{eq:claim2_5}
    \ell^L_n \leq  \frac{\delta^L_n}{6}\sqrt{\chi^2\rho}
\end{equation}
then \eqref{eq:claim2_2} implies \eqref{eq:claim2_1}. To summarize, since $\ell^L_n$ simultaneously satisfies \eqref{eq:claim2_2} and \eqref{eq:claim2_5}, \eqref{eq:claim2_1} holds. $\qed$

\section{Proof of Lemma \ref{lemma:collision_free_lossless}\label{appendix:collision_free_lossless}}
\begin{figure}[ht!]
\centering
\includegraphics[width=1\columnwidth]{rirrg_optimality_balls_lossless_latex.eps}
\caption{Covering of collision-free and lossless chain $\gamma''_n$ with balls $\mathcal{B}^s_{n,k}$ of radius $\frac{(1-(\theta^1_{n,k})^2)}{2}\ell^L_n$. The event $E_{n,k}$ is the event that a sampled point $(x,P)\in\mathcal{R}^{L,s}_{n,k}$.}
\label{fig:covering_balls_lossless}
\end{figure}
% The proof is divided into two steps. In the first stage, we show
% \begin{align}
%     P[\lambda]\preceq (\theta^2_{n,k})^2P_{n,k}\;\forall\;\lambda\in[0,1]
% \end{align}
Lets define $\mathcal{E}^{\text{out}}_{n,k}:=\mathcal{E}_{\chi^2}(x_{n,k},{(\theta^0_{n,k})^2}P''_{n,k})$, it is easy to verify that the minimum distance $\mathcal{E}^{\text{out}}_{n,k}$ and $\mathcal{E}_{\chi^2}(x_{n,k},P''_{n,k})$ is $(\ell^L_n)^4$. On the other hand, any translation of the center of ellipsoid  $\mathcal{E}^{\text{out}}_{n,k}$ from $x_{n,k}$ to some  $x\in\mathcal{B}(x_{n,k},(\ell^L_n)^4)$, will linearly translate the ellipsoid $\mathcal{E}_{\chi^2}\left(x_{n,k},P\right)$ by a maximum of distance $(\ell^L_n)^4$.
Thus, after the translation the ellipse stays inside $\mathcal{E}(x_{n,k},P''_{n,k}) \subset \mathcal{E}(x_{n,k}, P_{n,k}) $ and \eqref{eq:init_in} holds. 
As the first step to prove \eqref{eq:final_in_lossless}, we stress that  
\begin{align*}
 &||x_2-x_1|| \leq \\
 &||x_{n,k}-x_1 ||+ ||x_{n,k+1}- x_{n,k} ||+||x_2-x_{n,k+1}||\\
 &\leq (\ell^L_n)^4 + (\ell^L_n)^4 + (\ell^L_n)^4 \leq \ell^L_n + \ell^L_n + \ell^L_n = 3\ell^L_n.  
\end{align*}
Thus, we have
\begin{align} \nonumber
     P_1+||x_2-x_1||W &\preceq  P''_{n,k}+||x_2-x_1||W \\ \nonumber
     &\preceq P'_{n,k}+||x_2-x_1||W \preceq 
     P'_{n,k}+ 3 \ell^L_n W\\ \label{eq:final_ell}
     &\preceq \left(1-\frac{\delta^L_n}{2}\right)^2 P_{n,k},
\end{align}
where the last inequality is shown earlier as \eqref{eq:claim2_2}. From 
\eqref{eq:final_ell}, we have
\begin{align*}
&\mathcal{E}(x_{n,k}, P_1+||x_2-x_1||W)
\subseteq \mathcal{E}\left(x_{n,k},  \left(1-\frac{\delta^L_n}{2}\right)^2 P_{n,k}\right). 
\end{align*}
The minimum distance between $\mathcal{E}(x_{n,k}, P_{n,k})$ and $\mathcal{E}\left(x_{n,k},  \left(1-\frac{\delta^L_n}{2}\right)^2 P_{n,k}\right)$ is  
$\frac{\delta^L_n}{2}\sqrt{\chi^2\underline{\sigma}(P_{n,k})}$, Thus after linear translating $\mathcal{E}\left(x_{n,k},  \left(1-\frac{\delta^L_n}{2}\right)^2 P_{n,k}\right)$  for  
$||x_2-x_{n,k}||\leq \|x_2-x_{n,k+1}\|+\|x_{n,k}-x_{n,k+1}\| \leq 2\ell^L_n \leq \frac{\delta^L_n}{2} \sqrt{\chi^2\rho} \leq \frac{\delta^L_n}{2}\sqrt{\chi^2 \underline{\sigma}(P_{n,k})}$, the resultant ellipse $\mathcal{E}_{\chi^2}(x_2, 
\left(1-\frac{\delta^L_n}{2}\right)^2 P_{n,k})$, and subsequently $\mathcal{E}_{\chi^2}\left(x_2, P_1+||x_2-x_1||W\right)$ stays inside 
$\mathcal{E}_{\chi^2} \left(x_{n,k},P_{n,k}\right)$.$\qed$
\section{Proof of Lemma \ref{lemma:transition_is_lossless}\label{appendix:transition_is_lossless}}
We know that the chain $(x_{n,k},P''_{n,k})$ for all $k\in[1;K^L_n]$ is lossless and collision-free. Therefore,
\begin{align}
    P''_{n,k+1}\preceq P''_{n,k}+\ell^L_nW
    \label{eqn:lossless}
\end{align}
for all $k\in[1;K^L_{n}-1]$. Further, since sampled point $(x_2,P_2)\in\mathcal{R}^{L,s}_{n,k+1}$, the following is true
\begin{align} \nonumber
    &(\theta^1_{n,k+1})^2P''_{n,k+1}\preceq P_2\preceq {(\theta^2_{n,k+1}})^2P''_{n,k+1}\nonumber\\
    &\implies  \frac{P_2}{(\theta^2_{n,k+1})^2}\preceq P''_{n,k+1}\preceq  \frac{P_2}{(\theta^1_{n,k+1})^2}
    \label{eqn:ineq_mix}
\end{align}
Using \eqref{eqn:lossless} and \eqref{eqn:ineq_mix}, $\frac{P_2}{(\theta^2_{n,k+1})^2}\preceq P''_{n,k}+\ell^L_nW$. 
% we have
% \begin{align}
%     \frac{P_2}{(\theta^2_{n,k+1})^2}\preceq P''_{n,k}+\ell^L_nW
% \end{align}
Further, using the fact that $\theta^1_{n,k}=\theta^2_{n,k+1}$ for $k\in[1;K^L_n-1]$ implies the following
\begin{align} \nonumber
      {P_2}&\preceq {(\theta^2_{n,k+1})^2}P''_{n,k}+{(\theta^2_{n,k+1})^2}\ell^L_nW\\
      &={(\theta^1_{n,k})^2}P''_{n,k}+{(\theta^1_{n,k})^2}\ell^L_nW \nonumber\\
      &\preceq P_1+{(\theta^1_{n,k})^2}\ell^L_nW\preceq P_1+\frac{((\theta^1_{n,k+1})^2+(\theta^1_{n,k})^2)}{2}\ell^L_nW\nonumber
    %   &\preceq P_1+\frac{(\theta^1_{n,k+1}+\theta^1_{n,k})}{2}\ell^L_nW\nonumber
    %   &\preceq P_1+\|x_2-x_1\|_2W
\end{align}
Using the fact that $\|x_{n,k+1}-x_{n,k}\|=\ell^L_n$, $(x_1,P_1)\in\mathcal{R}^{L,s}_{n,k}$ and $(x_2,P_2)\in\mathcal{R}^{L,s}_{n,k+1}$, the minimum distance between $x_2$ and $x_1$ is given as
\begin{align}
    \|x_2-x_1\|&\geq\ell^L_n-\frac{(1-(\theta^1_{n,k})^2)}{2}(\ell^L_n)^4\nonumber\\
    &-\frac{(1-(\theta^1_{n,k+1})^2)}{2}(\ell^L_n)^4\nonumber\\
    &\geq\frac{(\theta^1_{n,k+1})^2+(\theta^1_{n,k})^2}{2}\ell^L_n.\nonumber
\end{align}
Consequently, $P_2\preceq P_1+\|x_2-x_1\|W$.
% \begin{align}
%     P_2\preceq P_1+\|x_2-x_1\|_2W\nonumber
% \end{align}
Therefore, $P_2\preceq P_1+\|x_2-x_1\|W $. Further, since $\mathcal{R}^{L,s}_{n,k+1}\subset\mathcal{R}^L_{n,k+1}$ by construction, the edge between two sampled points $(x_1,P_1)\in\mathcal{R}^{L,s}_{n,k}$ and $(x_2,P_2)\in\mathcal{R}^{L,s}_{n,k+1}$ for all $k\in[1;K^L_n-1]$ is finitely lossless and collision-free.$\qed$
\section{Proof of Lemma \ref{lemma:distance_smaller_than_ed_min_lossless}\label{appendix:distance_smaller_lossless}}
{ We first note that the maximum distance between $x_2$ and $x_1$ is
\begin{align*}
    \|x_2-x_1\| &\leq \ell^L_n+\frac{(1-(\theta^1_{n,k})^2)}{2}(\ell^L_n)^4+\nonumber\\
    &\frac{(1-(\theta^1_{n,k+1})^2)}{2}(\ell^L_n)^4\nonumber\\
    % &\leq \ell^L_n+\frac{(1-(\theta^1_{n,k})^2)}{2}(\ell^L_n)^4+\frac{(1-(\theta^1_{n,k+1})^2)}{2}(\ell^L_n)^4\nonumber\\
    &\leq\left(2-\frac{(\theta^1_{n,k+1})^2+(\theta^1_{n,k})^2}{2}\right)\ell^L_n\nonumber \leq 2 \ell^L_n.
\end{align*}
}
% Since an edge from $b_1=(x_1,P_1)\in\mathcal{R}^s_{n,k}$ to $b_2=(x_2,P_2)\in\mathcal{R}^s_{n,k+1}$ for all $k\in\{1,2,\dots,K_n-1\}$ is lossless, we have
% \begin{align}
% &P_2\preceq P_1+\|x_2-x_1\|W \preceq P_1 + 2\ell^L_n W \nonumber\\
%     %&\|P_2-P_1\|_F\leq \theta^1_{n,k}\ell^L_n\bar{\sigma}(W)
%     &{\|P_2-P_1\|_F\leq 2 \ell^L_n\bar{\sigma}(W)}
% \end{align}
Therefore, we have $\hat{D}(b_1,b_2)=\|x_2-x_1\|\leq 2\ell^L_n:=D_\text{min}$.
    % +\|P_2-P_1\|_F\\
    % &\leq {2 \ell^L_n + 2\ell^L_n\bar{\sigma}(W)
    % \leq 2\ell^L_n(1+\bar{\sigma}(W)):=D_\text{min}}.
\section{Proof of Lemma \ref{lemma:event_E_n_for_lossless}\label{appedix:event_en_lossless}}
% Let $b_i:=(x_i,P_i) \in B$ be sample belief state by Algorithm~\ref{alg:igpp_body1}.
Then, due to the uniform distribution, $    \mathbb{P}(\{x_i\in\mathcal{B}(x_{n,k},(\ell^L_n)^4)\})$ is given by
\begin{align} \nonumber
    \mathbb{P}(\{x_i\in\mathcal{B}(x_{n,k},(\ell^L_n)^4)\})&=\frac{\text{vol}(\mathcal{B}(x_{n,k},(\ell^L_n)^4))}{\text{vol}(\mathcal{X}_{\text{free}})}\nonumber\\
    &=\frac{\tau_d(\ell^L)^{4d}_n}{\mathcal{V}_{\mathcal{X}}}\geq\frac{\tau_d(\delta^L_n)^{4d}(h^L)^{4d}}{\mathcal{V}_{\mathcal{X}}},\nonumber
\end{align}
where $\mathcal{V}_{\mathcal{X} }:=\text{vol}(\mathcal{X}_{\text{free}})$. It
is straightforward to verify that
\begin{align} \nonumber
    \mathbb{P}(\{x_i\in\mathcal{B}^s(x_{n,k},\ell^L_n)\})\!&=\!c_{n,k} \mathbb{P}(\{x_i\in\mathcal{B}(x_{n,k},(\ell^L_n)^4)\}) \\ \nonumber
    &\geq\frac{ \tau_d(\delta^L_n)^{6d}(h^L)^{6d}}{\mathcal{V}_{\mathcal{X}}\chi^\frac{d}{2}\rho^{\frac{d}{4}} 2^d}=g^L_1 (\delta^L_n)^{6d}
\end{align}
where $g^L_1:=\frac{ \tau_d (h^L)^{6d}}{\mathcal{V}_{\mathcal{X}}\chi^\frac{d}{2}\rho^{\frac{d}{4}} 2^d}$.
% If we define
% \begin{align*}
%     &\mathcal{D}_{n, k} :=:=\{P\succeq0: {(\theta^1_{n,k})^2} P''_{n,k} \preceq P\preceq {(\theta^2_{n,k})^2} P''_{n,k}\},
% \end{align*} 
% we have $(\theta^1_{n,k})^2 P_{n,k} \succeq \rho I$. Thus, Theorem~\ref{theo:continuity2} holds in these regions.
We have  $\textup{Tr}(P) \geq 16 (\theta^1_{n,k})^2  \rho d \geq \rho d$ for all $P \in \mathcal{D}_{n, k}$.
On the other hand, $\textup{Tr}(P)\leq (\theta^2_{n,k})^2 \textup{Tr}(P''_{n,k}) \leq  (\theta^2_{n,k})^2 R \leq R $. Hence, $\mathcal{D}_{n,k} \subset \mathcal{R}_{ [(\theta^1_{n,k})^2  16\rho d, (\theta^2_{n,k})^2 R ]} \subset \mathcal{R}_{ [ \rho d, R ]}$.

From the definition of uniform sampling, we have 
\begin{align*}
     &\mathbb{P}(P_i\in\mathcal{D}_{n,k})
     {\geq}\frac{\text{vol}(\mathcal{D}_{n,k}\cap \mathcal{D}_{[\rho d, R]})}{\text{vol}(\mathcal{D}_{[\rho d ,R]})} = \frac{\text{vol}(\mathcal{D}_{n,k})}{\text{vol}(\mathcal{D}_{[\rho d,R]})} \\
    &=\frac{\text{vol}(\mathcal{D}_{(\theta^2_{n,k})^2P''_{n,k}}) - \text{vol}(\mathcal{D}_{(\theta^1_{n,k})^2P''_{n,k}})}{\text{vol}(\mathcal{R}_{R}) - \text{vol}(\mathcal{R}_{\rho d})}\\
    &\geq\frac{V_dS_d(1,1,1/2)b^{\frac{d(d+1)}{2}}\bigg[ (\theta^2_{n,k})^{d(d+1)}- (\theta^1_{n,k})^{d(d+1)} \bigg]}{\left(\frac{2}{d(d+1)}\right)V_r \bigg[R^{\frac{d(d+1)}{2}}- (\rho d)^{\frac{d(d+1)}{2}} \bigg]}.
\end{align*}
% where $s_i^{n,k}$ is given by
% \begin{align}
%    s_i^{n,k}= \begin{cases}
%   \lambda^i_1(P''_{n,k})+\dots+\lambda^i_d(P''_{n,k}) & \text{for }i\in[1;d-1]\\    
%   \lambda^i_1(P''_{n,k})\lambda^i_2(P''_{n,k})\dots\lambda^i_d(P''_{n,k}) & \text{for }i=d
% \end{cases}\nonumber
% \end{align}
where $b=\underset{i\in[1;d]}{\min}\;\lambda_i(P''_{n,k})$. Now, we have
\begin{align*}
    &(\theta^2_{n,k})^{d(d+1)}-(\theta^1_{n,k})^{d(d+1)} \nonumber\\
    &= (\theta^2_{n,k} - \theta^1_{n,k}) \sum_{j=0}^{d(d+1)-1} (\theta^2_{n,k})^{d-1-j}  (\theta^1_{n,k})^{j}\\
    % &\geq (\theta^2_{n,k}-\theta^1_{n,k})(\theta^2_{n,k})^{d(d+1)-1}\\
        &\geq d\frac{(\theta^2_{n}-\theta^1_{n})}{K^L_n} (\theta^1_{n})^{d(d+1)-1}\nonumber\\
        &\geq d\frac{(\theta^2_{n}-\theta^1_{n})}{2K^L_n}(\theta^2_{n}+\theta^1_{n}) (\theta^1_{n})^{d(d+1)-1}\\
    &\geq d\frac{\ell^L_n}{2\ell^\star}(\delta^L_n)^2 \left(1-\frac{(\delta_n^L)^2}{9}\right)^{d(d+1)-1}\nonumber\\
    &\geq\frac{h^Ld}{\ell^\star}\left(\frac{\delta^L_n}{3}\right)^{2d(d+1)}.\nonumber
\end{align*}
where in the last step we use $\delta^L_n\leq 1$. 
% Also, $\prod_{i=1}^{d} s^{n,k}_i \geq d^{d-1}(16\rho)^{\frac{d(d+1)}{2}}$.
In sum, it can be deduced
\begin{align}
    \mathbb{P}(P_i\in\mathcal{D}_{n,k}) \geq g^L_2 (\delta^L_n)^{2d(d+1)},
\end{align}
where $g^L_2 = \frac{V_dV_r^{-1}S_d(1,1,1/2)2^{-1}d^2(d+1) h^L(16\rho)^{\frac{d(d+1)}{2}}}{\ell^\star 3^{2d(d+1)}} \bigg[R^{\frac{d(d+1)}{2}}- (\rho d)^{\frac{d(d+1)}{2}} \bigg]^{-1} $ is a constant. 

Lets define the event $E^i_{n,k}$ is as the event that the sampled belief $b_i=(x_i,P_i)$ belongs to $\mathcal{R}^s_{n,k}$. Then, $E_{n,k}= \cup_{i=1}^n E^i_{n,k}$. The following lemma shows that if the $\gamma$ is greater than a certain positive threshold, then the probability that event $E_n\triangleq E_{n,1}\cap E_{n,2}\dots \cap E_{n,K^L_n}$ occurs equals to one as $n$ approaches infinity.
The probability of event $E^c_{n,k}$ for all $k\in[1;K^L_n]$ is given as follows:
\begin{align}
   &\mathbb{P}\left(E^c_{n,k}\right)
   = \prod_{i=1}^{n}(1-\mathbb{P}\left(E^i_{n,k}\right))\nonumber\\
   &\leq \left(1- \mathbb{P}(P\in\mathcal{D}_{n,k}) \;     \mathbb{P}(x\in\mathcal{B}_{n,k})\right)^n\leq \nonumber\\ \nonumber
   &\leq\left[1- g^L_1 (\delta^L_n)^{6d} g^L_2 (\delta^L_n)^{2d(d+1)} \right]^n \nonumber\\
   &= \left[1- g^L_1 g^L_2 (\delta^L_n)^{d(2d+8)} \right]^n.\nonumber
    \end{align}
%    \leq\left[1-\left(\frac{d \delta_n^{d}\rho^{\frac{d}{2}}}{2^{d}(\bar{\rho}^{d/2}-\rho^{d/2})}\right)\left(\frac{\tau_d\delta_n^{d} h^{d}}{  \mathcal{V}_{\mathcal{X}}}\right)\right]^n\nonumber\\
   % &\leq\left[1-\left(\frac{{ d \tau_d \delta_n^{2d}} \rho^{\frac{d}{2}} h^d}{12^{d}(\bar{\rho}^{d/2}-\rho^{d/2})\mathcal{V}_{\mathcal{X}}}\right)\right]^n\nonumber
Using the fact that $(1-x)\leq e^{-x}$ for $x\in(0,1)$, and substituting $\delta^L_n$, for sufficiently large $n$, we have
% defined as
% \begin{align}
%     \delta^L_n=\min\;\left\{\gamma\left(\frac{\text{log}n}{n}\right)^\frac{1}{d(2d+8)},1\right\},\nonumber
% \end{align}
% we have
\begin{align} \nonumber
 \mathbb{P}\left(E^c_{n,k}\right)&\leq \left(1-g^L_1 g^L_2
 {\gamma^{d(2d+8)}\frac{\text{log}n}{n}} \right)^{n} \leq
  n^{- g^L_1 g^L_2 \gamma^{d(2d+8)}}.\nonumber
\end{align}
Now, the event $E_n^c=\bigcup_{k=1}^n E^c_{n,k}$ is upper bounded as follows:
\begin{align}
  \mathbb{P}\left(E^c_{n}\right)& \!=\!\mathbb{P}\left(\bigcup_{k=1}^{K^L_n} E^c_{n,k}\right) 
 \! \leq \! \sum_{k=1}^{K^L_n}\mathbb{P}\left( E^c_{n,k}\right)\!\leq \! K^L_n n^{- g^L_1 g^L_2 \gamma^{d(2d+8)}}.  \nonumber
\end{align}
where $K^L_n=\lfloor\frac{\ell^\star}{\ell^L_n}\rfloor\leq\frac{\ell^\star}{\ell^L_n}$.
Since $\delta^L_n h^L\leq\ell^L_n$, we have
\begin{align} \nonumber
    &\mathbb{P}\left(E^c_{n}\right)
    \leq \frac{\ell^\star}{\delta^L_n h^L}  n^{- g^L_1 g^L_2 \gamma^{d(2d+8)}}  \\ \label{eq:P_c_lossless} 
     &= \frac{ \ell^\star}{\delta^L_n h^L} (\log n)^{-\frac{1}{d(2d+8)}} n^{- g^L_1 g^L_2 \gamma^{d(2d+8)}+ \frac{1}{d(2d+8)}}. 
\end{align}
%for large $n$ values (where $\delta^L_n\neq 1/2$).
If the power of $n$ in \eqref{eq:P_c_lossless} is less than $-1$, which is equivalent to
\[
\gamma>\left(\frac{d(2d+8)+1}{g^L_1 g^L_2 d(2d+8)} \right)^\frac{1}{d(2d+8)},
\]
we have $\sum_{n=1}^\infty  \mathbb{P}\left(E^c_{n}\right)<\infty$. Consequently, 
$\underset{n\rightarrow\infty}{\lim}\mathbb{P}\left(E^c_{n}\right)=1$,
% \begin{align}
%   \underset{n\rightarrow\infty}{\lim}\mathbb{P}\left(E^c_{n}\right)=1,  
% \end{align}
by Borel Cantelli lemma \cite{grimmett2020probability} which completes the proof. $\qed$

\section{Proof of Lemma \ref{lemma:arbritrarily_close_for_lossless}\label{appendix:arbitr_close_lossless}}
% Define set $\mathcal{R}^{\beta s}_{n,k}\subset \mathcal{R}^{ s}_{n,k} $ as 
% \begin{align} \nonumber
%     &\mathcal{R}^{\beta s}_{n,k}=\Biggl\{(x,P): x\in\mathcal{B}\left(x_{n,k},\frac{\beta(1-(\theta^1_{n,k})^2)}{2}(\ell^L_n)^4\right),\\ \nonumber
%     &{(\beta\theta^1_{n,k}+(1-\beta)\theta^2_{n,k})^2}P''_{n,k}\preceq P \preceq \left({(\theta^2_{n,k})^2}\right)P''_{n,k}\Biggl\}
% \end{align}
% where $0<\beta<1$. 
%and the points $(x,P)\in  \mathcal{R}^{\beta s}_{n,k}$ are sampled uniformly. 
% Similarly, the sets $ \widetilde{\mathcal{R}}^{\beta s}_{n,k}$ and $ \Bar{\mathcal{R}}^{\beta s}_{n,k}$ are defined where the only difference is that the points in sets $ \widetilde{\mathcal{R}}^{\beta s}_{n,k}$ and $ \Bar{\mathcal{R}}^{\beta s}_{n,k}$  are sampled using a Poisson distribution with intensity $\nu n$ where $\nu<1$ is independent of $n$ and Binomial distribution respectively.
Define $I_{n,k}$ as follows:
\begin{align}
    I_{n, k}:= \begin{cases}1, & \text { if }\mathcal{R}^{L,\beta s}_{n,k} \cap V^{\mathrm{Lossless \;IG-PRM}^{*}}=\emptyset, \\ 0, & \text { otherwise. }\end{cases}\nonumber
\end{align}

Define $M_n=\sum_{k=1}^{K^L_n} I_{n,k}$, and assume the event $\{M_n\leq \alpha K^L_n\}$ has occurred which means that $\alpha$ fraction of the $K^L_n$ points be such that the sampled points do not belong to any $\mathcal{R}^{L,\beta s}_{n,k}$. 
%Then $(1-\alpha)$ fraction of the points will be such that the points belong to $\mathcal{R}^{ s}_{n,k}$. Clearly, 
If the sampled $(x,P)$ is not inside $\mathcal{R}^{L,\beta s}_{n,k}$ but it is inside $\mathcal{R}^{L,s}_{n,k}$, we have
\begin{align*}
     \|x-x_{n,k}\|\leq &  \frac{(1-(\theta^1_{n,k})^2)}{2}(\ell^L_n)^4
     \leq  \frac{1}{2} \ell^L_n,\\
% \end{align*}     
% \begin{align*}
    \|P-P''_{n,k}\|_F &\leq  \|(\theta^1_{n,k})^2 P''_{n,k}-P''_{n,k}\|_F \\
    &\leq \|(\theta^1_n)^2 P''_{n,k}-P''_{n,k}\|_F\leq \frac{16 \bar{\rho} \sqrt{d}}{\rho}  \ell^L_n,
    % & \leq (1- (\theta^1_n)^2) \|P''_{n,k}\|_F \\
    % &\leq (2-\delta^L_n + \frac{\ell^L_n}{\rho})(\delta^L_n- \frac{\ell^L_n}{\rho}) \bar{\rho} \sqrt{d}\\
    % & \leq \frac{16 \bar{\rho} \sqrt{d}}{\rho}  \ell^L_n,
\end{align*}
which yields $\hat{\mathcal{D}}((x,P), (x_{n,k},P''_{n,k})) \leq  c \ell^L_n$, where $c:= \frac{1}{2}+ \frac{16 \bar{\rho} \sqrt{d}}{\rho}$.
Similarly, if the sampled $(x,P)$ is inside $\mathcal{R}^{L,\beta s}_{n,k}$ we have 
\begin{align*}
    & \|x-x_{n,k}\|\leq  \beta\frac{(1-(\theta^1_{n,k})^2)}{2}(\ell^L_n)^4
     \leq  \frac{1}{2} \beta\ell^L_n,\nonumber\\
    &\|P-P''_{n,k}\|_F \nonumber\\
    &\leq  \|(\beta\theta^1_{n,k}+(1-\beta)\theta^2_{n,k})^2 P''_{n,k}-(\theta^2_{n,k})^2P''_{n,k}\|_F\\
    &+\|(\theta^2_{n,k})^2P''_{n,k}-P''_{n,k}\|\\
    &\leq 2\|(\beta\theta^1_{n,k}+(1-\beta)\theta^2_{n,k}) P''_{n,k}-\theta^2_{n,k}P''_{n,k}\|_F\\
    &+\|(\theta^2_{n,k})^2P''_{n,k}-P''_{n,k}\|\\
    & \leq 2\beta(\theta^2_{n,k}- \theta^1_{n,k}) \|P''_{n,k}\|_F +\|(\theta^2_{n,k})^2P''_{n,k}-P''_{n,k}\|\\
    &\leq 2\beta\Delta \bar{\rho} \sqrt{d}+(\delta^L_n)^2\bar{\rho} \sqrt{d}
    %   &\leq 2\beta\Delta \bar{\rho} \sqrt{d}+\|(\theta^2_{n,k})^2P''_{n,k}-P''_{n,k}\|
    % & \leq \frac{16 \bar{\rho} \sqrt{d}}{\rho}  \ell^L_n,
\end{align*}
which yields $\hat{\mathcal{D}}((x,P), (x_{n,k},P''_{n,k})) \leq 2\beta\Delta \bar{\rho} \sqrt{d}+(\delta^L_n)^2\bar{\rho} \sqrt{d}$. If we define $c_1:=\beta \bar{\rho} \sqrt{d}$ and $c_2=\bar{\rho} \sqrt{d}$ (which is bounded), we have
\begin{align}
\left\|\gamma^p_{n}-\gamma''_{n}\right\|_{TV}& \leq  \sum_{k=1}^{K^L_n}  \hat{\mathcal{D}}((x,P), (x_{n,k},P''_{n,k})\nonumber\\
&\leq K^L_n (\alpha c \ell^L_n + (1-\alpha) \beta c_1 \Delta+c_2(\delta^L_n)^2) \nonumber\\
&\leq c\alpha L+c_1\beta (\theta^2_n-\theta^1_n)+c_2\delta^L_nL/h^L,
\end{align}
where $L=\underset{n}{\sup}\;\gamma''_n$. The fact that the bounded variation $\|\gamma^p_n-\gamma''_n\|_{TV}$ is upper bounded by $c\alpha L+c_1\beta (\theta^2_n-\theta^1_n)+c_2\delta^L_nL/h^L$ implies that
\begin{align}
&\left\{M_{n} \leq \alpha K_{n}\right\} \subseteq\nonumber\\
&\left\{\left\|\gamma^p_{n}-\gamma''_{n}\right\|_{TV} \leq c\alpha L+c_1\beta (\theta^2_n-\theta^1_n)+c_2\delta^L_nL/h^L\right\}\label{eqn:MclessthatalphaKn}
\end{align}
Taking the complement of both sides of \eqref{eqn:MclessthatalphaKn} and using the monotonicity of probability measures,
\begin{align}
&\mathbb{P}\left(\left\{\left\|\gamma^p_{n}-\gamma''_{n}\right\|_{TV}>c\alpha L+c_1\beta (\theta^2_n-\theta^1_n)+c_2\delta^L_nL/h^L\right\}\right)\nonumber\\
&\leq \mathbb{P}\left(\left\{M_{n} \geq \alpha K_{n}\right\}\right).
\label{eq:complement}
\end{align}
Since \eqref{eqn:complement} is true for any $\alpha,\beta\in(0,1)$, it remains to show that $\mathbb{P}\left(\left\{M_{n} \geq \alpha K_{n}\right\}\right) $ is finite. {
Let's denote $\ell_{\beta n,k}:=\frac{\beta(1-(\theta_{n,k}^1)^2)}{2}(\ell^L_n)^4)$, $\theta^1_{\beta n,k}:= (\beta\theta^1_{n,k}+(1-\beta)\theta^2_{n,k}) $ and $\mathcal{D}^s_{n,k}=\mathcal{D}_{[(\theta^1_{\beta n,k})^2 P''_{n,k},(\theta^2_{ n,k})^2 P''_{n,k}]}$ for simplicity of notation. Then, expected value of $I_{n,m}$ can be computed as 
\begin{align}
    &\mathbb{E}[I_{n,m}]\nonumber\\
    &= \mathbb{P}(\{I_{n,m}=1\}) \nonumber\\
    &=\left( 1- \mathbb{P}(x\in\mathcal{B}(x,\ell_{\beta n,k})) \times  \mathbb{P}(P\in\mathcal{D}^s_{n,k}) \right)^n\nonumber\\
    %   &=\left( 1- \mathbb{P}((x,P)\in\mathcal{R}^{\beta s}_{n,k}) \right)^n\nonumber\\
    % &\leq \left( 1 - \frac{c_{n,k}\tau_d \beta^d \ell^{4d}_{n}}{\mathcal{V}_X} \times \left(\frac{\beta h\delta^L_n^{2d}\rho^{\frac{d}{2}}}{\ell^\star3^{2d}(\bar{\rho}^{d/2}-\rho^{d/2})}\right)  \right)^n\nonumber\\
    % % &\leq \left( 1 - \frac{c_{n,k}\tau_d \beta^d \ell^{4d}_{n}}{\mathcal{V}_X} \times \frac{\beta \delta^L_n^d\rho^{\frac{d}{2}}}{3^d(\bar{\rho}^{d/2}-\rho^{d/2})}  \right)^n\nonumber\\
    % &=
    % \left(1-\left(\frac{\beta^{d+1}{\delta^{8d}_n}\tau_dh^{7d}}{\ell^\star3^{3d}(\bar{\rho}^{d/2}-\rho^{d/2})\mathcal{V}_{\mathcal{X}}\chi^\frac{d}{2}\rho^{\frac{-d}{4}}}\right)\right)^{n}\nonumber\\
&\leq     \left(1-g^L_1 g^L_2 \beta^{d+1} (\delta^L_n)^{d(2d+8)}\right)^{n}\nonumber\\
    % &=
    % \left(1-g^L_1 g^L_2 \beta^{d+1} \delta_n^{d(2d+8)}\right)^{n}\nonumber\\
    &\leq    \text{exp} \left(-g^L_1 g^L_2 \beta^{d+1} \gamma^{d(2d+8)} \frac{\log n}{n}  n\right)\nonumber\\
    % &\leq \text{exp} \left(-\frac{\beta^{d+1}(d(2d+8)+1)}{d(2d+8)} \log n\right)\nonumber\\
    &= n^{\frac{-\beta^{d+1}g_1^Lg_2^L(d(2d+8)+1)}{d(2d+8)}}. \nonumber
\end{align}

Thus, $\mathbb{E}[M_n]=\sum_{m=1}^{K^L_n} \mathbb{E}[I_{n,m}]= K^L_n n^{-\beta ^{d+1}}$. By Markov’s inequality, it follows that
\begin{align}
    \mathbb{P}\left(\left\{M_{n} \geq \alpha K_{n}\right\}\right) \leq \frac{\mathbb{E}[M_n]}{\alpha K^L_n} \leq \frac{K^L_n n^{-\beta ^{d+1}}}{\alpha K^L_n} = \frac{ n^{-\beta ^{d+1}}}{\alpha}.
    \label{eqn:P_Mn}
\end{align}
it is easy to verify that for fixed $\alpha$, \eqref{eqn:P_Mn} tends to $0$ as $n$ tends to $\infty$.
}
Since this argument holds for all $\alpha, \beta$, using the fact that \eqref{eqn:complement} holds and $\underset{n\rightarrow\infty}{\lim}\;\delta^L_n=0$, it can be concluded that for all $\epsilon>0$, $
% \sum_{n=1}^{\infty}
\mathbb{P}\left(\left\{\left\|\gamma^p_{n}-\gamma''_{n}\right\|_{TV}>\epsilon\right\}\right)<\infty
$.
Finally, by the Borel-Cantelli lemma, $\mathbb{P}\left(\left\{\underset{n\rightarrow\infty}{\lim}\;\left\|\gamma^p_{n}-\gamma''_{n}\right\|_{TV}=0\right\}\right)=1$.
% \section{Proof of Lemma \ref{lemma:arbritrarily_close_for_lossless}\label{appendix:arbitr_close_lossless}}
\section{Proof of Lemma \ref{lemma:cont_final_lossless}\label{appendix:cont_final_lossless}}

Since $\gamma''_n$ is a lossless refinement of $\gamma'_n$, we have $c(\gamma''_n)=c(\gamma'_n)$ for each $n\in\mathbb{N}$.
Therefore,
\begin{align}
    \lim_{n\rightarrow\infty} c(\gamma''_n)=c^\star
    \label{eqn:gamma_dd_c_star}
\end{align}
Using \eqref{eqn:gamma_dd_c_star}, the fact that $\mathbb{P}\left(\left\{\underset{n\rightarrow\infty}{\lim}\;\left\|\gamma^p_{n}-\gamma''_{n}\right\|_{TV}=0\right\}\right)=1$ and continuity of the chain, we can conclude that
\begin{align}
       \mathbb{P}(\{\lim_{n\rightarrow\infty} c(\gamma^p_n)=c^\star\})=1 
\end{align}

% \section{A fixture for computing of $\textup{vol}(D_A)$}
% Using \eqref{eq:lambda}, we have
% \begin{align}
% \nonumber
%   \textup{Vol}(\mathcal{D}_A) =& (d! \; 2^d)^{-1} \\ \label{eq:int_coupled}
% &\left( \int_{U \Lambda U^\top \preceq A} 
%  \prod_{i<j}^d |\lambda_i-\lambda_j|  \det \theta \prod_i^d d\lambda_i\right),  
% \end{align}
% where $\Lambda:=\textup{diagonal}(\lambda_1, \dots, \lambda_d)$. We use the transformation $y_i=\lambda_i/a_i$. Therefore, $\prod_{i=1}^ddy_i=\prod_{i=1}^d\lambda_i/s_d$ with $s_d =\prod_{j=1}^d a_j$. The integral \eqref{eq:int_coupled} would become
% \begin{align}
% \nonumber
%   \textup{Vol}(\mathcal{D}_A) =& s_d (d! \; 2^d)^{-1} \\ \label{eq:int_coupled}
% &\left( \int_{U Y U^\top \preceq I} 
%  \prod_{i<j}^d |a_i y_i - a_j y_j|  \det \theta \prod_i^d dy_i\right)\\ \nonumber
%  =& s_d (d! \; 2^d)^{-1} \left( \int_{O(d)} \det \theta \right)\\
%  &\left( \int_{0 \leq y_i \leq 1}    \prod_{i<j}^d |a_iy_i-a_jy_j| \; \prod_i^d dy_i \right).
% \end{align}
% It is a well-known result (see e.g., \cite[page 71]{muirhead2009aspects} or \cite{chikuse2003statistics} ) that $\int_{O(d)} \det \theta= \frac{2^d \pi^{d^2/2}}{\Gamma_d(d/2)}$, where $\Gamma_d$ is the multivariate gamma function.
% {\color{blue}
% \begin{itemize}
%     \item Please, add the lower-bound idea and complete the proof.
%     \item Please, make sure \eqref{eq:lambda} is correct and consistent with  the volume of $\mathcal{R}_{{[1,1]}}$, computed in different papers.   
%     \item In Theorem~2, Introduce  $V_r$ as the volume of regions with $\textup{Tr}(P)=1$, please.
%     \item Could you update the rest of the proof based on expressions for the volumes? Could you write everything in terms of $S_d$ and $V_r$?    
% \end{itemize}

% }

\end{document}
