%!TEX root = main.tex

\parindent 20pt

\section{Appendix---Geometric Chung-Lu Directed Graph Model is Well-defined} \label{apdx:model}

Let us state the problem in a slightly more general setting. Fix $n=2m$, where $m$ is some natural number. Suppose that for each pair $i, j \in [n]$ we have $a_{ij}=a_{ji} \in \R_+$ if $i\leq m$, $j>m$, $j\neq i+m$, and we have $a_{ij}=0$ otherwise, that is, if $i,j\in [m]$ or $i,j \in [2m] \setminus [m]$ or $j=i+m$. The $a_{ij}$ elements taken together form a matrix $\textbf{A}$, which is symmetric. Finally, for each $i \in [n]$ we have $b_i \in \R_+ \cup \{0\}$, and 
\begin{equation}\label{eq:assumption}
\sum_{i=1}^{m}b_i=\sum_{i=m+1}^{n}b_i>0.
\end{equation}

\medskip

In our application, $m=|V|$ is the number of nodes in a non-empty directed graph $G=(V,E)$, and elements of vector $\mathbf{b} = (b_i)_{i\in[n]}$ correspond to the degree distribution of the graph: for $i\in [m]$, $b_i$ is the out-degree of $v_i$ (that is, $b_i=w_i^{out}$), and for $i\in [2m] \setminus [m]$, $b_i$ is the in-degree of $v_{i-m}$ (that is, $b_i=w_{i-m}^{in}$). The assumption that $\sum_{i=1}^{m}b_i=\sum_{i=m+1}^{n}b_i>0$ is satisfied as the total in-degree is equal to the total out-degree, and the graph is not empty. Positive elements of matrix $\textbf{A}$ satisfy $a_{i,j+m}=a_{j+m,i} = g(d_{i,j}) \in (0,1]$ for $i,j\in[m], i\neq j$, and correspond to the distances between embeddings of the corresponding nodes $v_i$ and $v_j$. The case $i=j$ is excluded as indices $i$ and $i+m$ correspond to the same node in the original directed graph and so $a_{i,i+m}=a_{i+m,i} = g(d_{i,i}) = 0$. Finally, since isolated nodes may be ignored, we may assume that $w_i^{out}+w_i^{in} > 0$, that is,
\begin{equation}\label{eq:non-degenerate}
b_i + b_{i+m} > 0 \qquad \text{ for all } i \in [m].
\end{equation}

Our goal is to investigate if there is a solution, $x_i \in \R_+\cup\{0\}$ for $i \in [n]$, of the following system of equations:
\begin{equation}\label{eq:system}
b_i = x_i \sum_{j=1}^na_{ij}x_j \qquad \text{ for all } i\in[n].
\end{equation}
If there is one, then is this solution unique? The solution to~(\ref{eq:system}) will yield the solution to our problem: for $i \in [m]$, $x_i^{out} = x_i$ and $x_i^{in}=x_{i+m}$.

\medskip

The $m=2$ case is a degenerate case that exhibits a different behaviour but it is easy to investigate. In this case, by assumption~(\ref{eq:assumption}), $b_1 = b_4 = x_1 x_4 a_{12}$ and $b_2 = b_3 = x_2 x_3 a_{21}$. There are infinite number of solutions, each of them being of the form $(x_1, x_2, x_3, x_4) = (s, t, b_2/(a_{12}t), b_1/(a_{12}s))$ for some $t, s \in \R_+$. Having said that, in our application, all of these solutions yield the same random graph with the following distribution: $p_{12} = x_1 x_4 a_{12} = b_1 = w^{out}_1$ and $p_{21} = x_2 x_3 a_{21} = b_2 = w^{out}_2$.

\medskip

Suppose now that $m \ge 3$. We will show that the desired solution of~(\ref{eq:system}) exists if
\begin{equation}\label{eq:condition1}
\sum_{i=m+1}^nb_i > b_j + b_{j+m} \quad \text{for} \quad j\in[m],
\end{equation}
and
\begin{equation}\label{eq:condition2}
\sum_{i=1}^mb_i > b_j + b_{j-m} \quad \text{for} \quad j\in[2m] \setminus [m]
\end{equation}
(also recall that by assumption~(\ref{eq:assumption}), $\sum_{i=1}^{m}b_i=\sum_{i=m+1}^{n}b_i>0$, and by assumption~(\ref{eq:non-degenerate}), $b_i+b_{i+m}>0$ for all $i \in [m]$).
In other words, the condition is that the in-degree of any node $v_i$ is smaller than the sum of out-degrees of nodes other than $v_i$, and the out-degree of $v_i$ is smaller than the sum of in-degrees of nodes other than $v_i$. This is a very mild condition that holds in our application. Indeed, properties~(\ref{eq:condition1})--(\ref{eq:condition2}) with non-strict inequalities are satisfied for \emph{all} directed graphs $G$, and strict inequalities are satisfied \emph{unless} $G$ has an independent set of size $n-1$, that is, $G$ is a star with one node being part of \emph{every} edge. 

These degenerate cases can be ignored in further analysis, as such configurations of $\mathbf{b}$ allow to reconstruct the graph deterministically. The implementation of the framework identifies such cases and returns the corresponding 0/1 values of $p_{ij}$. For degenerate cases, the corresponding system of equations~(\ref{eq:system}) might or might not have a solution depending on the parameters. For example when $m=3$ and $\mathbf{b}=(2,0,0,0,1,1)$ the system has infinitely many solutions of the form $(t,0,0,s,1/(a_{51}t), 1/(a_{61}t)))$ for any $t,s>0$, all of them yielding the same deterministic graph: $p_{12}=p_{13}=1$, $p_{21}=p_{23}=p_{31}=p_{32}=0$. However, for $m=3$, $\mathbf{b}=(2,1,1,2,1,1)$ and non zero elements $a_{ij}$ equal to $1$ the system has no solutions. We do not try to classify cases when the system has the solution in the proof as they lead to deterministic graphs and we handle them in the implementation of the framework separately anyway.

\medskip

Let us make the following observations that will be useful later on. Provided that properties~(\ref{eq:condition1})--(\ref{eq:condition2}) are satisfied, the following properties hold:
\begin{itemize}
    \item $x_i=0$ if and only if $b_i=0$. Indeed, if $x_i = 0$, then trivially $b_i =0$. Suppose then that $b_i=0$ and by symmetry we may assume that $i\in[m]$. By properties~(\ref{eq:condition1})--(\ref{eq:condition2}), $b_j > 0$ for at least one value of $j \in [2m]\setminus[m]$ and $j\neq i+m$. It follows that $x_j > 0$, $a_{ij} > 0$, and so $b_i \ge x_i a_{ij} x_j$. As a result, $x_i$ has to be equal to zero in order for $b_i$ to be zero. 
    \item we may assume that $x_1=1$. Indeed, one can reorder the nodes so that $b_1>0$. Then, one can multiply $x_i$ for all $i\in[m]$ by any positive constant $\alpha \in \R_+$ and divide $x_i$ for all $i \in [2m] \setminus [m]$ by $\alpha$ and the solution will not change.
\end{itemize}

The last observation means that we need to introduce the constraint $x_1=1$ if we ever hope to prove the uniqueness of the solution. If we do not do that, then there will be an infinite number of solutions but all of them will yield the same edge distribution for the random graph as the particular solution we are searching for.

\medskip

We will start with proving the uniqueness. After that, we will show that~(\ref{eq:condition1})--(\ref{eq:condition2}) are sufficient conditions.

\subsection{Uniqueness}

Let us assume that $m \ge 3$. For a contradiction, suppose that we have two different solutions: $\mathbf{x} = (x_i)_{i\in[n]}$ ($x_i \in \R_+$, $i \in [n]$) with $x_1=1$ and $\mathbf{y} = (y_i)_{i \in [n]}$ ($y_i \in \R_+$, $i \in [n]$) with $y_1=1$. It follows that for all $i \in [n]$ we have
$$
b_i = f_i(\mathbf{x}) = f_i(\mathbf{y}), \quad \text{ where } f_i(\mathbf{x}) = x_i \sum_{j=1}^n a_{ij} x_j.
$$
Let us analyze what happens at point $\mathbf{z} = t\mathbf{x} + (1-t)\mathbf{y}$ for some $t \in [0,1]$ (that is, $z_i = tx_i+(1-t)y_i$, $i \in [n]$). For each $i \in [n]$ we get
\begin{eqnarray*}
f_i(\mathbf{z}) &=& (tx_i+(1-t)y_i) \sum_{j=1}^n a_{ij} (tx_j+(1-t)y_j) \\
&=& \sum_{j=1}^n a_{ij} \left( t^2 x_i x_j+ t(1-t)(x_i y_j + x_j y_i) + (1-t)^2 y_i y_j \right) \\
&=& f_i(\textbf{x}) t^2 + \frac {t(1-t)}{x_i y_i} (f_i(\textbf{y}) x_i^2 + f_i(\textbf{x}) y_i^2) + f_i(\textbf{y}) (1-t)^2 \\
&=& b_i \left( t^2 + \frac {t(1-t)}{x_i y_i} (x_i^2 + y_i^2) + (1-t)^2 \right) \\
&=& b_i \left( 1 - 2t(1-t) + \frac {t(1-t)}{x_i y_i} (x_i^2 + y_i^2) \right) \\
&=& b_i \left( 1 + \frac {t(1-t)}{x_i y_i} (x_i^2 - 2x_iy_i + y_i^2) \right) \\
&=& b_i \left( 1+t(1-t)\frac{(x_i-y_i)^2}{x_iy_i} \right) =: g_i(t).
\end{eqnarray*}
Note that $g_i'(1/2)=0$ for all $i$ (as either $x_i-y_i$ vanishes and so $g_i(t)$ is a constant function or it does not vanish but then $g_i(t)$ is a parabola with a maximum at $t=1/2$). For convenience, let $\mathbf{v} = (\mathbf{x} + \mathbf{y})/2$ and $\mathbf{s} = (\mathbf{x} - \mathbf{y})/2$ (that is, $v_i=(x_i+y_i)/2$ and $s_i=(x_i-y_i)/2$ for all $i \in [n]$). It follows that 
$$
\frac{d g_i}{dh} \Big( \mathbf{v}+h\mathbf{s} ~|~ h=0 \Big)=0.
$$
On the other hand,
$$
g_i(\mathbf{v}+h\mathbf{s}) = \sum_{j=1}^na_{ij}(v_i+hs_i)(v_j+hs_j)
$$
and so
$$
\frac{d g_i}{dh}(\mathbf{v}+h\mathbf{s}) = \sum_{j=1}^na_{ij}(s_i(v_j+hs_j)+s_j(v_i+hs_i)).
$$
Combining the two observations, we get that
\begin{equation}\label{eq:condition_for_s}
0=\frac{d g_i}{dh} \Big( \mathbf{v}+h\mathbf{s} ~|~ h=0 \Big) = \sum_{j=1}^na_{ij}(s_iv_j+s_jv_i).
\end{equation}

Now, for $i \ge 1$, let $u_i = s_i / v_i$, provided that $v_i \neq 0$.
Recall that in particular $s_1=(x_1-y_1)/2=0$ and $v_1=(x_1+y_1)/2=1$, as we assumed that $x_1=y_1=1$, and so $u_1=0$.
Additionally, if $v_i=0$ (that is, the corresponding node has in-degree 0 or out-degree 0), then we may take $u_i=0$, as it will cancel out anyway. Denote the set of indices $i$ when $v_i=0$ by $Z$. Substituting it to~(\ref{eq:condition_for_s}) we get:
$$
\sum_{j=1}^na_{ij}v_iv_j(u_i+u_j) = 0.
$$

Notice that we can rescale all $u_i$'s by the same multiplicative factor so that $u_{\ell}=1$ for some $\ell \in [n]$ and for all other indices $|u_i|\leq1$ (potentially with negative rescaling factor). For index $\ell$ we have:
$$
\sum_{j=1}^n a_{\ell j} v_{\ell}v_j(u_{\ell}+u_j) = \sum_{j=1}^n a_{\ell j} v_{\ell}v_j(1+u_j) = 0.
$$
But this possible only if $\ell\in[m]$, as otherwise the left hand side of the above equation is at least its first term, namely, $a_{\ell 1}v_{\ell} v_1(1+u_1) =a_{\ell 1}v_{\ell} > 0$. If $\ell \in[m]$, then we get
$$
\sum_{j=m+1}^n a_{\ell j}v_{\ell} v_j (u_{\ell}+u_j) = \sum_{j=m+1}^n a_{\ell j}v_{\ell} v_j (1+u_j) = 0
$$
as $a_{\ell j}=0$ for $j\in[m]$. But this means that $u_j=-1$ for $j\in[n]\setminus([m]\cup Z)$.

Let us concentrate on any index $\ell'\in [n]\setminus([m]\cup Z)$ (note that this set is non-empty).
For this index, we have the following condition:
$$
\sum_{j=1}^m a_{\ell' j}v_{\ell'}v_j(u_{\ell'}+u_j) = \sum_{j=1}^m a_{\ell' j}v_{\ell'}v_j(-1+u_j) = 0.
$$
However, since $|u_j|\leq 1$ for all $j \in[m]$, all entries of the sum are non-negative and so they would all have to be equal to $0$ for the sum to be $0$. This is not possible as $u_1=0$ and so $a_{\ell'1}v_{\ell'}v_1(-1+u_1) =-a_{\ell'1}v_{\ell'}<0$. The desired contradiction shows that the solution is unique.

% \subsection{Necessity}

% Suppose that $\mathbf{x} = (x_i)_{i\in[n]}$ ($x_i \in \R_+ \cup \{0\}$, $i \in [n]$) is a solution to~(\ref{eq:system}). We will show that the condition~(\ref{eq:condition1}) has to be satisfied. By symmetry, the same argument proves that the condition~(\ref{eq:condition2}) is satisfied too.

% Consider any $j\in[m]$. Let us first observe that
% \begin{eqnarray*}
% - b_{j+m} + \sum_{i=m+1}^nb_i &=& \sum_{i\in[n]\setminus([m]\cup\{j+m\})} b_i =  \sum_{i\in[n]\setminus([m]\cup\{j+m\})}\left( x_i \sum_{k=1}^na_{ik}x_k \right) \\
% &\geq& \sum_{i\in[n]\setminus([m]\cup\{j+m\})}\left( x_i a_{ij}x_j \right).
% \end{eqnarray*}

% Finally, we use the fact that $a_{ij}=0$ if $i\in[m] \cup \{j+m\}$ to get that 
% $$
% - b_{j+m} + \sum_{i=m+1}^nb_i \geq \sum_{i\in[n]\setminus([m]\cup\{j+m\})}\left( x_i a_{ij}x_j \right) = x_j\sum_{i=1}^{n}x_i a_{ij}=b_j.
% $$
% It follows that~(\ref{eq:condition1}) is a necessary condition for the existence of a solution to~(\ref{eq:system}).

\subsection{Sufficiency}

We will continue assuming that $m \geq 3$. For a contradiction, suppose that there exists a vector $\mathbf{b} = (b_i)_{i \in [n]}$, that satisfies~(\ref{eq:condition1})--(\ref{eq:condition2}), and $\sum_{i=1}^{m}b_i=\sum_{i=m+1}^{n}b_i>0$ (assumption~(\ref{eq:assumption})) but for which there is no solution to the system~(\ref{eq:system}). Without loss of generality, since one can reorder nodes and relabel in- and out-degrees if needed, we may assume that $b_1$ is a largest value in vector $\mathbf{b}$. We will call such vectors \emph{infeasible}. On the other hand, vectors that yield a solution $\mathbf{x} = (x_i)_{i \in [n]}$, with $x_i\geq0$ for all $i$, will be called \emph{feasible}. As proved earlier, if the solution exists, then it must be unique (remember that we assume that $x_1=1$). We will introduce more vectors $\textbf{b}$ (both feasible and infeasible) below but we assume that matrix $\textbf{A}$ is fixed.

Let us now construct another vector $\mathbf{b}' = (b'_i)_{i \in [n]}$ for which there exists a solution to~(\ref{eq:system}) (that is, $\mathbf{b}'$ is feasible) but also $b'_1=b_1$ is a largest element in $\mathbf{b}'$. Indeed, it can be done easily by, for example, taking $x'_1=1$, $x'_i = s$ for $i\in[m]\setminus\{1\}$ ($s$ is a fixed but sufficiently small positive constant for the inequalities below to hold), and $x'_i = b_1 / (\sum_{j \in [n]} a_{1j}) = b_1 / (\sum_{j \in [n] \setminus [m]} a_{1j})$ for $i\in[n]\setminus[m]$. Vector $\mathbf{b}'$ is now defined by the system~(\ref{eq:system}). We immediately get that
$$
b'_1 = x'_1 \sum_{j \in [n]} a_{1j} x'_j = \sum_{j \in [n] \setminus [m]} a_{1j} x'_j = b_1.
$$
Also, $s$ can be made arbitrarily small so that $b'_i<b_1$ for $i\in[m]\setminus\{1\}$.
Finally, for $i \in[n]\setminus[m]$ we have
$$
b'_i = x'_i \sum_{j \in [m]} a_{ij} x'_j = x'_i a_{i1} + x'_i \sum_{j=2}^m a_{ij} s = b_1 \frac{a_{i1}} {\sum_{j \in [n]} a_{1j}} + x'_i \sum_{j=2}^m a_{ij} s.
$$
Since $m \ge 3$, the first term is smaller than $b_1$. Hence, since $s$ can be made arbitrarily small, we can ensure that $b'_i < b_1$. The desired properties hold.

We will consider points along the line segment between $\mathbf{b}'$ and $\mathbf{b}$, namely,
$$
\mathbf{b}(t) = (b_i(t))_{i \in [n]} = (1-t)\mathbf{b}'+t\mathbf{b}, \qquad \text{ for } t\in[0,1].
$$
Since $\mathbf{b}'$ is feasible and we already proved that~(\ref{eq:condition1})--(\ref{eq:condition2}) are necessary conditions, we know that $\mathbf{b}'$ satisfies~(\ref{eq:condition1})--(\ref{eq:condition2}). But, as a result, not only $\mathbf{b}$ and $\mathbf{b}'$ satisfy these properties but also $\mathbf{b}(t)$ satisfies them for any $t \in [0,1]$. In particular, it follows that there exists a universal constant $\eps > 0$ such that for any $t \in [0,1]$ we have 
\begin{equation}\label{eq:separation}
(1-\eps)\sum_{i=m+2}^nb_i(t) > b_1(t).
\end{equation}

Fix $t \in [0,1)$ and suppose that $\mathbf{b}(t)$ is feasible. Let $\mathbf{x}(t) = (x_i(t))_{i\in [n]}$ be the (unique) solution for $\mathbf{b}(t)$. From the analysis performed in the proof of uniqueness of the solution it follows that our transformation is a local diffeomorphism, that is, the differential of the transformation is bijective for the admissible values of $x_i$ and $b_i$. (Note that this also covers the case $t=0$. This case is on the boundary of the considered range of $t$ but it is an interior point of the domain of the mapping.) In the following considerations, we assume that point $x_1$ and $b_1$ are removed from the analysis (as they are fixed) and also that the indices from the set $Z$ (that is, as defined above, the set of indices $i$ for which $v_i=0$) are excluded as they are fixed. As a result we may move to a manifold of a dimension $n'<n$ by dropping the dimensions that are fixed. In the considered manifold, any open set in $\R^{n'}$ containing (part of) $\mathbf{x}(t)$ is mapped to an open set in $\R^{n'}$ containing (part of) $\mathbf{b}(t)$. In particular, there exists $\delta > 0$ such that $\mathbf{b}(s)$ is feasible for any $t - \delta \le s \le t + \delta$. Combining this observation with the fact that $\mathbf{b}'=\mathbf{b}(0)$ is feasible, $\mathbf{b}=\mathbf{b}(1)$ is \emph{not} feasible we get that there exists $T \in (0,1]$ such that $\mathbf{b}(T)$ is not feasible but $\mathbf{b}(t)$ is feasible for any $t \in [0,T)$. Indeed, if no such $T$ exists (that is, there is no minimal infeasible $t \in (0,1]$), then there would exist a decreasing sequence of infeasible values of $t$ that converges to a feasible $t$. This is not possible as in some neighbourhood of a feasible point $t$, points are also feasible.

Consider any sequence $(t_i)_{i \in \N}$ of real numbers $t_i \in [0,T)$ such that $t_i\to T$ as $i \to \infty$; for example, $t_i = T(1-1/i)$. All limits from now on will be for $i \to \infty$. Recall that $\mathbf{b}(t_i)$ is feasible and so $\mathbf{x}(t_i)$ is well-defined.

Before we move forward, let us show that there exists a sufficiently large but universal constant $\Delta$ such that for all $t \in [0,T)$ and all $i$ (except possibly $i=m+1$), we have $x_i(t) \le \Delta$. Indeed, by our assumption on the solution, $x_1(t)=1\le \Delta$. By the equation~(\ref{eq:system}) for $b_1(t) = b_1$, we have for $i\in[n]\setminus[m+1]$
$$
x_i(t) = \frac {1}{a_{1i}} \cdot x_1(t) a_{1i} x_i(t) < \frac {1}{a_{1i}} \cdot x_1(t) \sum_{j=1}^n a_{1j} x_j(t) = \frac {b_1(t)}{a_{1i}} = \frac {b_1}{a_{1i}} \le \Delta.
$$
But this immediately means that $x_i(t)$ are also bounded for $i\in[m]$ by considering any equation for $b_i(t) \le b_1(t) =b_1$ where $i\in[n]\setminus([m]\cup Z)$. This implies that only $x_{m+1}(t)$ can potentially be unbounded.

If $x_{m+1}(t)$ is bounded for all $t \in [0,T)$, then by the Bolzano-Weierstrass theorem the sequence $t_i$ has a subsequence $(\mathbf{x}(t_{s_i}))_{i \in [n]}$ such that $\Vert \mathbf{x}(t_{s_i}) \Vert \to c$ for some $c \in \R$. However, if this is the case then, by continuity of our transformation, the limiting value $\mathbf{b}(T)$ would be feasible, giving us the desired contradiction. It remains to consider the case when $x_{m+1}(t_{s_i}) \to \infty$ for some subsequence $s_i$.
However, this implies that $x_{j}(t_{s_i})\to 0$ for all $j\in[m]\setminus\{1\}$. This means that in the limit we have $x_i(T)=b_i(T)/a_{i1}$ for $i\in[n]\setminus[m+1]$. Substituting it into the first equation we get 
$$
b_1(T) = x_1(T)\sum_{i=m+2}^na_{1i}x_i(T) = 1\cdot\sum_{i=m+2}^na_{1i}\cdot b_i(T)/a_{i1} = \sum_{i=m+2}^nb_i(T).
$$
This contradicts (\ref{eq:separation}), which concludes the proof.

\bigskip

As a final note, let us observe that the proof implies that if $b_1$ gets close to $\sum_{i=m+2}^nb_i$ (from below) and $b_{m+1}>0$, then indeed $x_{m+1}$ will grow to be a large number. This consideration has a numerical impact as it might affect the convergence of numeric algorithms finding $x_i$ due to floating point computation precision issues. The proof also shows that the case when the conditions~(\ref{eq:condition1})--(\ref{eq:condition2}) are not satisfied (that is, we would have an equality instead of inequality) will have a solution if $b_{m+1}=0$ and otherwise will not have a solution (this corresponds to the two examples we have given earlier).

\subsection{Model with Loops}

In order to accommodate loops that are present in the graph, we relax the assumption that $a_{i,i+m}=0$ for $i\in[m]$ and now assume that $a_{i,i+m} \ge 0$ for $i\in[m]$. However, using the notation from the previous section we will additionally assume that $a_{1, m+1}>0$, that is, for the largest element of $\mathbf{b}$, we assume that the corresponding node has a loop. This auxiliary assumption is satisfied in our application as, in fact, all landmarks have loops.

The $m=2$ case continues to be a degenerate case that has to be delt with independently. Consider the following set of equations:
$$
\left[\begin{matrix}
b_1 \\ b_2 \\ b_3 \\ b_4
\end{matrix}\right] =
\left[\begin{matrix}
a_{13} & a_{14} & 0 & 0 \\
0 & 0 & a_{23} & a_{24} \\
a_{13} & 0 & a_{23} & 0 \\
0 & a_{14} & 0 & a_{24}
\end{matrix}\right]
\left[\begin{matrix}
x_1x_3 \\ x_1x_4 \\ x_2x_3 \\ x_2x_4
\end{matrix}\right].
$$
As before, we assume that $x_1=1$ and, since $b_1+b_2=b_3+b_4$ the system reduces to: 
$$
\left[\begin{matrix}
b_1 \\ b_2 \\ b_3
\end{matrix}\right] =
\left[\begin{matrix}
a_{13} & a_{14} & 0 & 0 \\
0 & 0 & a_{23} & a_{24} \\
a_{13} & 0 & a_{23} & 0
\end{matrix}\right]
\left[\begin{matrix}
x_3 \\ x_4 \\ x_2x_3 \\ x_2x_4
\end{matrix}\right] =
\left[\begin{matrix}
a_{14} & 0 & a_{13} & 0 \\
0 & a_{24} & 0 & a_{23} \\
0 & 0 & a_{13} & a_{23} 
\end{matrix}\right]
\left[\begin{matrix}
x_4  \\ x_2x_4 \\ x_3 \\ x_2x_3
\end{matrix}\right]
.
$$
Equivalently, since $b_1$ is the largest element of $\mathbf{b}$, for some non-negative $p_i$ and positive $q=a_{23}$: 
$$
\left[\begin{matrix}
p_1 \\ p_2 \\ p_3
\end{matrix}\right] =
\left[\begin{matrix}
1 & 0 & 0 & -q \\
0 & 1 & 0 & q \\
0 & 0 & 1 & q
\end{matrix}\right]
\left[\begin{matrix}
x_4  \\ x_2x_4 \\ x_3 \\ x_2x_3
\end{matrix}\right].
$$
If $x_2=0$, we get a unique positive solution for $x_3$ and $x_4$ and it exists only if $p_2=0$ (which happens for $b_2=0$).
On the other hand, if $x_2>0$ (recall that, by assumption, $x_i \ge 0$), then in the above system of equations one can reduce $x_3$ and $x_4$ leaving only $x_2$ as a variable in the quadratic equation:
$$
Q(x_2)=q(p_1+p_3)x_2^2+(-p_2q+p_3q+p_1)x_2-p_2 = 0.
$$
Since $x_2 > 0$, we get that $p_2 =x_2(x_4+qx_3) > 0$. (Note that if $x_3=x_4=0$, then $b_3=b_4=0$ and, as a consequence, $b_1=b_2=0$ which gives as a contradiction as $b_1>0$.) As the term $(p_1+p_3)q$ is positive and $-p_2$ is negative we get that there exists exactly one positive solution $x_2$ of this equation. Indeed, since $Q(0) = -p_2 <0$ and the parabola $Q(x_2)$ has the coefficient $(p_1+p_3)q>0$ associated with the quadratic term, there is exactly one positive solution $x_2 > 0$. Now, assuming that $x_2>0$, from the third equation we see that $x_3>0$ and from the first equation we get that $x_4>0$. 

In summary, for $m=2$, subject to the constraint $x_1=1$, the solution of the system always exists and is unique.
If $m\geq3$ we also show that the solution exists always. The part of the proof of uniqueness remains unchanged. The part for sufficiency also remains unchanged until we reach the case where we consider $x_{m+1}(t_{s_i}) \to\infty$. However as $a_{1,m+1}>0$ this is not possible since $x_1=1$ and $b_1$ is fixed. So we are left with the cases that $\Vert \mathbf{x}(t_{s_i}) \Vert \to c$ for some $c \in \R$ which means that $\mathbf{b}(t)$ always converges to a feasible solution as $t\to1$ (even if in conditions~(\ref{eq:condition1})--(\ref{eq:condition2}) we have an equality).




\section{Appendix---Scalable Implementation with Landmarks}\label{apdx:landmarks}

Recall that in \textbf{Step~1} of the algorithm, we obtain a partition $\textbf{C}$ of the set of nodes $V$ into $\ell$ communities: $C_1, \ldots, C_\ell$. The partition is then carefully refined by repeatedly splitting some parts of it with the goal to reach precisely $n'=4 \sqrt{n}$ parts; $n'$ might be adjusted by more experienced user, if needed. (The number of communities is typically relatively small. However, if $\ell \ge 4 \sqrt{n}$, then of course there is no need to do the refinement. However, in such rare cases each part in the initial partition is forced to be split into $s$ parts anyway. We fixed $s=4$ as a default value.) The heuristic algorithm is quite involved as it needs to find a good compromise between the quality of the approximation and the speed. The reader is directed to~\cite{Embedding_Complex_Networks_Scalable} for more details on how the refinement is obtained. 

Once the partition is refined, each part $C_i$ is replaced by its landmark $u_i$. The procedure depends on whether we deal with undirected or directed graphs. Let us start with undirected graphs. The position of landmark $u_i$ in the embedded space $\R^k$ is assigned as follows:
\begin{equation}\label{eq:position_landmark}
\emb(u_i) = \frac {\sum_{j \in C_i} w_j \ \emb(v_j)}{\sum_{j \in C_i} w_j}.
\end{equation}
In order to measure a variation within a cluster, we also compute the weighted sum of squared errors:
\begin{equation}\label{eq:error_landmark}
e_i = \sum_{j \in C_i} w_j \ \dist \big( \emb(u_i), \emb(v_j) \big)^2.
\end{equation}
The expected degree of landmark $u_i$ (that we denote as $w'_i$ in order to distinguish it from $w_i$, the expected degree of node $w_i$) is the sum of the expected degrees of the associated nodes in the original model, that is, $w'_i := \sum_{j \in C_i} w_j$.

The approximated algorithm uses the auxiliary \emph{Geometric Chung-Lu} (GCL) model on the set of landmarks $V = \{ u_1, \ldots, u_{n'} \}$ in which each pair of landmarks $u_i, u_j$, independently of other pairs, forms an edge with probability $p'_{i,j}$, where
\begin{equation*}
p'_{i,j} = x_i' x'_j g(d_{i,j}) 
\end{equation*}
for some carefully tuned weights $x'_i \in \R_+$. Additionally, for $i\in[n']$, the probability of creating a self loop around landmark $u_i$ is equal to 
$$
p'_{i,i} = (x'_i)^2 g(d_{i,i}), \qquad \text{ where } \qquad d_{i,i} = \sqrt{ \frac{e_i}{\sum_{j \in C_i} w_j} }.
$$
Note that the ``distance'' $d_{i,i}$ from landmark $u_i$ to itself is an approximation of the unobserved weighted average distance $d_{a,b}$ over all pairs of nodes $a$ and $b$ associated with $u_i$.
The weights are selected such that the expected degree of landmark $u_i$ is $w'_i$; that is, for all $i \in [n']$
$$
w'_i = \sum_{j \in [n']} p'_{i,j} =  x'_i \sum_{j \in [n']} x'_j g(d_{i,j}).
$$
The relationship between the weights in the auxiliary model and the original one is expected to be as follows: for any node $v_k \in C_i$ associated with landmark $u_i$ we have
$$
x_k \approx x'_i \ \frac {w_k}{\sum_{j \in C_i} w_j}.
$$

The adjustment for directed graph is straightforward. We use the same algorithm and positions for landmarks, that is, we still use~(\ref{eq:position_landmark}) and~(\ref{eq:error_landmark}) but with $w_j$ being the total degree of landmark $u_j$, namely, $w_j = w_j^{in} + w_j^{out}$. The probability for an ordered pair of landmarks $u_i, u_j$ to form a directed edge in the auxiliary \emph{Geometric Chung-Lu Directed Graph Model} is equal to 
$$
p'_{i,j} = {x'}_i^{out} {x'}_j^{in} g(d_{i,j}) 
$$
and
$$
p'_{i,i} = {x'}_i^{out} {x'}_i^{in} g(d_{i,i}), \qquad \text{ where } \qquad d_{i,i} = \sqrt{ \frac{e_i}{\sum_{j \in C_i} w_j} }.
$$
As before, any node $v_k \in C_i$ associated with landmark $u_i$ inherits a fraction of its weights, that is, we expect that the original weights are well approximated by the following:
$$
x_k^{out} \approx {x'}_i^{out} \ \frac {w_k^{out}}{\sum_{j \in C_i} w_j^{out}} \qquad  \text{and} \qquad x_k^{in} \approx {x'}_i^{in} \ \frac {w_k^{in}}{\sum_{j \in C_i} w_j^{in}}.
$$
