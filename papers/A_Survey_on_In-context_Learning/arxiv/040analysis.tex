% \begin{table*}[t]
%     \centering
%     \begin{tabularx}{0.98\textwidth}{l l X}
%     \toprule
%     \textbf{Stage} & \textbf{Factor} & \textbf{Correlation to ICL} \\
%     \midrule
%     \multirow{6}{*}{\makecell[l]{Pretraining}} & \makecell[l]{Pretraining corpus domain \\ \citep{shin2022corpora}} & \makecell[X]{Compared to the pretraining corpus size, the corpus domain is more influential} \\
%      & \makecell[l]{Pretraining corpus combination \\ \citep{shin2022corpora}} & \makecell[X]{Combining two poor-performing pretraining corpora may introduce emergent ICL ability} \\
%      & \makecell[l]{Model scale \\ \citep{wei2022emergent,gpt3}} & \makecell[X]{Models with more pretraining steps and parameters are more likely to have emergent ability} \\
%     \midrule
%     \multirow{7}{*}{\makecell[l]{Inference}} & \makecell[l]{Label space exposure \\ \citep{min2022rethinking}} & \makecell[X]{Label space exposure has big impact on non-channel ICL models} \\
%      & \makecell[l]{Input distribution \\ \citep{min2022rethinking}} & \makecell[X]{In-distribution demonstrations substantially contribute to ICL performance} \\
%     & \makecell[l]{Format of input-label pairing \\ \citep{min2022rethinking}} & \makecell[X]{The format of input-label pairing contributes much to the correct prediction of ICL} \\
%     & \makecell[l]{Demonstration-query similarity \\ \citep{liu2022close}} & \makecell[X]{Using demonstrations that are more similar to the query will bring better performance} \\
%     \bottomrule
%     \end{tabularx}
%     \caption{Summary of influence factors that have relatively strong correlation to ICL.}
%     \label{tab:factor}
% \end{table*}

\begin{table}[t]
    \centering
    \footnotesize
    \begin{tabular}{cc}
    \toprule
    \textbf{Stage} & \textbf{Factor} \\
    \midrule
    \multirow{7}{*}{\makecell[l]{Pretraining}} & \makecell[l]{Pretraining corpus domain \\ \citep{shin2022corpora}} \\
     & \makecell[l]{Pretraining corpus combination \\ \citep{shin2022corpora}} \\
     & \makecell[l]{Number of model parameters \\ \citep{wei2022emergent,gpt3}} \\
     & \makecell[l]{Number of pretraining steps \\ \citep{wei2022emergent}} \\
    \midrule
    \multirow{15}{*}{\makecell[l]{Inference}} & \makecell[l]{Label space exposure \\ \citep{min2022rethinking}} \\
     & \makecell[l]{Demonstration input distribution \\ \citep{min2022rethinking}} \\
    & \makecell[l]{Format of input-label pairing \\ \citep{min2022rethinking,compositional_generalization}} \\
    & \makecell[l]{Demonstration input-label mapping \\ \citep{min2022rethinking,ground_truth} \\ 
    \citep{llm_icl_differently}} \\
    & \makecell[l]{Demonstration sample ordering \\ \citep{lu2022order}} \\
    & \makecell[l]{Demonstration-query similarity \\ \citep{liu2022close}} \\
    & \makecell[l]{Demonstration diversity \\ \citep{compositional_generalization}} \\
    & \makecell[l]{Demonstration complexity \\ \citep{compositional_generalization}} \\
    \bottomrule
    \end{tabular}
    \caption{Summary of factors that have a relatively strong correlation to ICL performance.}
    \label{tab:factor}
\end{table}

To understand ICL, many analytical studies attempt to investigate what factors may influence the performance and aim to figure out why ICL works. We summarize the factors that have a relatively strong correlation to ICL performance in Table~\ref{tab:factor} for easy reference.  

\subsection{What Influences ICL Performance}

% factors - pretraining
\paragraph{Pre-training Stage}
We first introduce influence factors in the LLM pretraining stage. 
\citet{shin2022corpora} investigated the influence of the pretraining corpora. 
They found that the domain source is more important than the corpus size. Putting multiple corpora together may give rise to emergent ICL ability, pretraining on corpora related to the downstream tasks does not always improve the ICL performance, and models with lower perplexity do not always perform better in the ICL scenarios.
\citet{wei2022emergent} investigated the emergent abilities of many large-scale models on multiple tasks. They suggested that a pretrained model suddenly acquires some emergent ICL abilities when it achieves a large scale of pretraining steps or model parameters. 
\citet{gpt3} also showed that the ICL ability grows as the parameters of LLMs increase from 0.1 billion to 175 billion.

% factors - inference
\paragraph{Inference Stage}
In the inference stage, the properties of the demonstration samples also influence the ICL performance. 
\citet{min2022rethinking} investigated that the influence of demonstration samples comes from four aspects: the input-label pairing format, the label space, the input distribution, and the input-label mapping. 
They prove that all of the input-label pairing formats, the exposure of label space, and the input distribution contribute substantially to the ICL performance. Counter-intuitively, the input-label mapping matters little to ICL. 
In terms of the effect of input-label mapping, \citet{ground_truth} drew an opposite conclusion that correct input-label mapping does impact the ICL performance, depending on specific experimental settings.
\citet{llm_icl_differently} further found that when a model is large enough, it will show an emergent ability to learn input-label mappings, even if the labels are flipped or semantically-unrelated. 
From the compositional generalization perspective, \citet{compositional_generalization} validated that ICL demonstrations should be diverse, simple, and similar to the test example in terms of the structure. 
\citet{lu2022order} indicated that the demonstration sample order is also an important factor.
In addition, \citet{liu2022close} found that the demonstration samples that have closer embeddings to the query samples usually bring better performance than those with farther embeddings. 



\subsection{Understanding Why ICL Works}

\paragraph{Distribution of Training Data} 
Concentrating on the pretraining data, \citet{distribution} showed that the ICL ability is driven by data distributional properties. 
They found that the ICL ability emerges when the training data have examples appearing in clusters and have enough rare classes. 
\citet{bayesian} explained ICL as implicit Bayesian inference and constructed a synthetic dataset to prove that the ICL ability emerges when the pretraining distribution follows a mixture of hidden Markov models. 

\paragraph{Learning Mechanism} 
By learning linear functions, \citet{garg2022linear} proved that Transformers could encode effective learning algorithms to learn unseen linear functions according to demonstration samples. 
They also found that the learning algorithm encoded in an ICL model can achieve a comparable error to that from a least squares estimator. 
\citet{trm_as_alg} abstracted ICL as an algorithm learning problem and showed that Transformers can implement a proper function class through implicit empirical risk minimization for the demonstrations. 
\citet{tr_and_tl} decoupled the ICL ability into task recognition ability and task learning ability, and further showed how they utilize demonstrations. 
From an information-theoretic perspective, \citet{implicit_structure_induction} showed an error bound for ICL under linguistically motivated assumptions to explain how next-token prediction can bring about the ICL ability. 
\citet{inductive_bias} found that large language models exhibit prior feature biases and showed a way to use intervention to avoid unintended features in ICL. 

Another series of work attempted to build connections between ICL and gradient descent. 
Taking linear regression as a starting point, \citet{akyurek2022algorithm} found that Transformer-based in-context learners can implement standard finetuning algorithms implicitly, and \citet{icl_gd} showed that linear attention-only Transformers with hand-constructed parameters and models learned by gradient descent are highly related. 
Based on softmax regression, \citet{icl_weight_shifting} found that self-attention-only Transformers showed similarity with models learned by gradient-descent. 
\citet{dai2022iclft} figured out a dual form between Transformer attention and gradient descent and further proposed to understand ICL as implicit finetuning. 
Further, they compared GPT-based ICL and explicit finetuning on real tasks and found that ICL indeed behaves similarly to finetuning from multiple perspectives. 

\paragraph{Functional Components} 
Focusing on specific functional modules, \citet{olsson2022induction} found that there exist some induction heads in Transformers that copy previous patterns to complete the next token. 
Further, they expanded the function of induction heads to more abstract pattern matching and completion, which may implement ICL. 
% They provided several pieces of evidence on models with different sizes, which support their viewpoint that induction heads constitute the mechanism of ICL. 
\citet{label_anchor} focused on the information flow in Transformers and found that during the ICL process, demonstration label words serves as anchors, which aggregates and distributes key information for the final prediction. 

\textbf{$\Diamond$ Takeaway}: \textit{
(1) Knowing and considering how ICL works can help us improve the ICL performance, and the factors that strongly correlate to ICL performance are listed in Table~\ref{tab:factor}.
(2) 
Although some analytical studies have taken a preliminary step to explain ICL, most of them are limited to simple tasks and small models. 
Extending analysis on extensive tasks and large models may be the next step to be considered. 
In addition, among existing work, explaining ICL with gradient descent seems to be a reasonable, general, and promising direction for future research. 
If we build clear connections between ICL and gradient-descent-based learning, we can borrow ideas from the history of traditional deep learning to improve ICL. 
}