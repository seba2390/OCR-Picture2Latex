
\subsection{Traditional Tasks}
As a general learning paradigm, ICL can be examined on various traditional datasets and benchmarks, e.g., SuperGLUE~\cite{superglue}, SQuAD~\cite{squad}. 
Implementing ICL with 32 randomly sampled examples on SuperGLUE, ~\citet{gpt3} found that GPT-3 can achieve results comparable to state-of-the-art (SOTA) finetuning performance on COPA and ReCoRD, but still falls behind finetuning on most NLU tasks.
~\citet{hao2022structured} showed the potential of scaling up the number of demonstration examples. However, the improvement brought by scaling is very limited. At present, compared to finetuning, there still remains some room for ICL to reach on traditional NLP tasks.

%, and the number of demonstration examples is still not as unlimited as finetuning

%And XXX and XXX has shown XXXX, XXX are studying the XXX. % gpt3 paper examined on superglue and squad
\input{fig/tab_dataset.tex}
% \input{fig/tab_evaluation.tex}

% To make a fair comparison of different prompting strategies for ICL on traditional natural language understanding tasks, we implement XX prompting strategies for ICL with a same number of demonstration examples and a same backbone model. As it shown in Tab.~\ref{tab:evaluation}, XXXX.

\subsection{New Challenging Tasks}
% Except for the traditional extrinsic evaluation
%The powerful few-shot capability of ICL also gives rise to a new paradigm in datasets and evaluation.
In the era of large language models with in-context learning capabilities, researchers are more interested in evaluating the intrinsic capabilities of large language models without downstream task finetuning~\cite{foundation}.

To explore the capability limitations of LLM on various tasks, 
% more than 400 researchers from various institutions 
~\citet{beyond} proposed the BIG-Bench~\cite{beyond}, a large benchmark covering  
% 204 tasks with good diversity. 
a large range of tasks, including linguistics, chemistry, biology, social behavior, and beyond. 
The best models have already outperformed the average reported human-rater results on 65\% of the BIG-Bench tasks through ICL~\cite{suzgun2022challenging}. To further explore tasks actually unsolvable by current language models, \citet{suzgun2022challenging} proposed a more challenging ICL benchmark, BIG-Bench Hard (BBH). BBH includes 23 unsolved tasks, constructed by selecting challenging tasks where the state-of-art model performances are far below the human performances. Besides, researchers are searching for inverse scaling tasks,\footnote{\url{https://github.com/inverse-scaling/prize}} that is, tasks where model performance reduces when scaling up the model size. Such tasks also highlight potential issues with the current paradigm of ICL.
To further probe the model generalization ability, ~\citet{optiml} proposed OPT-IML Bench, consisting of 2000 NLP tasks from 8 existing benchmarks, especially benchmark for ICL on held-out categories.

Specifically, a series of studies focus on exploring the reasoning ability of ICL.~\citet{heuristic} generated an example from a synthetic world model
represented in first-order logic and parsed the ICL generations into symbolic proofs for formal analysis. They found that LLMs can make correct individual deduction steps via ICL.
~\citet{shi2022language} constructed the MGSM benchmark to evaluate the chain-of-thought reasoning abilities of LLMs in multilingual settings, finding that LLMs manifest complex reasoning across multiple languages.
To further probe more sophisticated planning and reasoning abilities of LLMs, ~\citet{planbench} provided multiple test cases for evaluating various reasoning abilities on actions and change, where existing ICL methods on LLMs show poor performance.

\subsection{Open-source Tools}
Noticing that ICL methods are often implemented differently and evaluated using different LLMs and tasks, \citet{openicl} developed OpenICL, an open-source toolkit enabling flexible and unified ICL assessment. With its adaptable architecture, OpenICL facilitates the combination of distinct components and offers state-of-the-art retrieval and inference techniques to accelerate the integration of ICL into advanced research.


%In summary, the evaluation for ICL on LLMs is manifesting the following characteristics:
 % Traditional datasets lack challenge for ICL; 2) 
% \textit{Takeaway: Intrinsic evaluation without large training set is a new tendency. The diversity and challenges are particularly valued for new benchmarks.}
% 4) Evaluation under the ICL paradigm brings new challenges, more insensitive and trustworthy evaluation paradigms is a new direction (Section~\ref{challenge: evaluation}).
\textbf{$\Diamond$ Takeaway}: \textit{(1) Due to the restrictions of ICL on the number of demonstration examples, the traditional evaluation tasks must be adapted to few-shot settings; otherwise, the traditional benchmarks cannot evaluate the ICL capability of LLMs directly.
(2) As ICL is a new paradigm that is different from traditional learning paradigms in many aspects, the evaluation of ICL presents new challenges and opportunities.
Toward the challenges, the results of existing evaluation methods are unstable, especially sensitive to the demonstration examples and the instructions. \citet{chen2022relation} observed that existing evaluations by accuracy underestimate the sensitivity towards instruction perturbation of ICL.
It is still an open question to conduct consistent ICL evaluation and OpenICL\cite{openicl} represents a valuable initial attempt to address this challenge.
% Apart from that, traditional evaluation benchmarks such as SuperGLUE~\cite{superglue} are less challenging for LLMs with billions of parameters, and even the sophisticated designed new benchmark, BigBench~\cite{beyond} can be readily surpassed.
% How to construct a challenging evaluation with long-term vitality is crucial for ICL evaluation.
Toward the opportunities for evaluation, as ICL only requires a few instances for the demonstration, it lowers the cost of evaluation data construction. 
}

%Furthermore, the test set is still supposed to faithfully reflect the world distribution. 
%Instead of the i.i.d. assumption with the training set, the test set requires new constraints.

