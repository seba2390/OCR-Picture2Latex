% \begin{figure}[t!]
%     \centering
%     \includegraphics[width=0.95\linewidth]{fig/icl-category.pdf}
%     \caption{Category of in-context learning prediction.}
%     \label{fig:icl_type}
% \end{figure}

% \leiModify{.}
% In this section, we first formulate the ICL framework, then we provide a brief overview of the main content of this survey.
Following the paper of GPT-3~\cite{gpt3}, we provide a definition of in-context learning:
\textsl{In-context learning is a paradigm that allows language models to learn tasks given only a few examples in the form of demonstration.
}
% Without loss of generality, we formulate the in-context learning in a classification problem setup and the adaption to other task formats discussed later.
Essentially, it estimates the likelihood of the potential answer conditioned on the demonstration by using a well-trained language model. 


Formally, given a query input text $x$ and a set of candidate answers $Y = \{y_1, \ldots, y_m\}$ (Y could be class labels or a set of free text phrases), a pretrained language model $\mathcal{M}$ takes the candidate answer with the maximum score as the prediction conditioning a demonstration set $C$. $C$ contains an optional task instruction $I$ and $k$ demonstration examples; therefore, $C = \{ I, s(x_1, y_1), \ldots, s(x_k, y_k) \}$ or $C = \{ s(x_1, y_1), \ldots, s(x_k, y_k) \}$, where $s(x_k, y_k,I)$ is an example written in natural language texts according to the task.
% therefore, $C = \{ s(x_1, y_1,I), \ldots, s(x_k, y_k,I) \}$, where $s(x_k, y_k,I)$ is an example written in natural language texts following the task instruction $I$.
% For a candidate label in a label space, the probability of a simple label candidate $y_j$ as a word prediction problem:
The likelihood of a candidate answer $y_j$ could be represented by a scoring function $f$ of the whole input sequence with the model $\mathcal{M}$:
% \leiModify{A more abstract Scoring function f, details later.}
\begin{equation}
    P( y_j \mid x) \triangleq
    f_\mathcal{M} ( y_j,  C, x)
    % \frac{\text{PPL}_\mathcal{M} ( y_j \mid C, x)} { \sum_m \text{PPL}_\mathcal{M} ( y_m \mid C, x)}.
\end{equation}
% This formulation degenerates into a word prediction problem when the label could be represented with a single word in the vocabulary of the LLM.
The final predicted label $\hat y$ is the candidate answer with the highest probability:
\begin{equation}
    \hat y = \arg\max_{y_j \in Y } P(y_j | x). 
\end{equation}
The scoring function $f$ estimates how possible the current answer is given the demonstration and the query text. For example, we could predict the class label in a binary sentiment classification by comparing the token probability of \emph{Negative} and \emph{Positive}. There are many $f$ variants for different applications, which will be elaborated in \S\ref{sec:scoring}. 

According to the definition, we can see the difference between ICL and other related concepts. (1) Prompt Learning: Prompts can be discrete templates or soft parameters that encourage the model to predict the desired output. Strictly speaking, ICL can be regarded as a subclass of prompt tuning where the demonstration is part of the prompt. %In ICL, the prompt format must be human-readable discrete texts and contain several demonstration examples. 
~\citet{liu2021pre} made a thorough survey on prompt learning. However, ICL is not included. (2) Few-shot Learning: few-shot learning is a general machine learning approach that uses parameter adaptation to learn the best model parameters for the task with a limited number of supervised examples~\cite{wang2019few}. In contrast, ICL does not require parameter updates and is directly performed on pretrained LLMs.
% given a query input text x and a candidate answer set Y = {y1, y2, . . . , ym}
% , we perform inference for an instance in interest $(x, y)$ by performing language completion with the input $x$ and demonstrated examples $c$ as the condition:
% ADD implementation variety.
% Table illustrates. 
% PPL.
% where $c$ is the context provided and is represented as a concatenated of $k$ instances $c= \{ x^c_1, y^c_1, \ldots, x^c_k, y^c_k \}$. $k$ is usually set to a relatively small number, such as $16$, and the effect of the demonstration number could be found in \S~X.
% Note that here we assume that all the input and target labels are represented by tokens in the vocabulary of the language model via a mapping function. 
% use an example to better illustrate 

% For example, we could perform binary sentiment classification for a sentence $x=$ \textit{The service of that coffee is really great.}, by comparing the conditional token probability of the token \textit{Negative} and \textit{Positive}.
% Demonstration examples with a similar form could be used as a context, e.g., \textit{The food is really bad. Negative;  The movie is crazy and I love it very much. Positive.} to assist the prediction.
% In this form, the model learns to perform the prediction based on the conditional context tokens in the prefix, without any computation-intensive parameter updates. 
% For the situation where the target answer is free text, the model is used to decode the answer directly as a text generation problem. The performance is evaluated via automatic metrics or human evaluation.
% The summarization of these two types of prediction is illustrated in Figure~\ref{fig:icl_type}.
% For tasks where the answer could not be represented by a single word in the vocabulary of the language model due to the tokenization process, an alternative is to compare the perplexity of all candidate sentences, where each sentence $S_j$ is the text concatenation of demonstration examples, input query and the candidate answer:
% Better calibration 
% \begin{align}
%     \hat y &= y_{\arg\min_{j} \text{PPL}_\mathcal{M} ( S_j)} \\ 
%     S_j &= \{ C, x, y_j \}
% \end{align}

% As recent explorations show that the vanilla probability could be sensitive to the order of demonstration examples, various calibrate methods have been introduced to stabilize the prediction.
% One representative framework is Calibrate, which eliminates the effect of biased prediction by introducing a normalization term estimated via a nonparametric method. Channel~(CITE) further shows a stronger regularization performance by computing the conditional probability of the input given the label.

% We summarize all these variants in Table~\ref{tab:icl_variants}.
% Word Prediction
% PPL prediction 
% 



% Benefits 

% on a large-scale text corpus with causal language modeling as the training objective