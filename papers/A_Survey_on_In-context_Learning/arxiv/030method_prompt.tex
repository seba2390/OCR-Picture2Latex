% \paragraph{Pre-Defined Metrics}
% \citet{chen2022sensitivity,lu2022order} define their new metrics and find them correlated to the ICL performance. 
% \citet{chen2022sensitivity} define a metric for ICL called prediction sensitivity, which measures the changes of the model output in response to small input perturbations. 
% They find there is an obvious negative correlation between prediction sensitivity and ICL accuracy, and motivated by it, propose a selective prediction method based on the prediction sensitivity. 
% \citet{lu2022order} analyze the sample order sensitivity for ICL in depth. 
% They first prove that the order sensitivity always exists for different model sizes and training samples, and a performant order is not transferable across models. 
% Then, they define the global and local entropy metrics and find a positive correlation between the entropy and the ICL performance. 
% With the entropy metrics, they can estimate a good sample ordering for ICL by choosing one that has higher local or global entropy. 


\begin{table*}[t!]
    \centering
    \setlength{\tabcolsep}{2pt}
    \small
    \resizebox{\linewidth}{!}{\begin{tabular}{@{}lcccc@{}}
    \toprule 
      \bf Category & \bf Methods &  \bf Demonstration Acquisition & \bf LLMs & \bf Main Tasks \\
    \midrule 
       \multirow{3}{*}{Demonstration Selection}  & KATE~\citep{liu2022close} & Human design & GPT-3& SST, table-to-text\\
       & SG-ICL~\cite{kim2022self} & LM generated & GPT-J & SST, NLI\\
       & EPR~\citep{rubin2022learning}  & Human design & GPT-\{J, 3\}/CodeX & Semantic parsing\\\midrule
        Demonstration Ordering & GlobalE \& LocalE~\citep{lu2022order} &  Human design & GPT-\{2, 3\} & Text classification\\ 
       \midrule%\midrule
       % Calibrate~\citep{calibrate} & $\frac{\mathcal{M} ( y_j \mid C, x)}{\mathcal{M} ( y_j \mid \text{NULL}))}$ & \\ 
       Instruction Formatting & Self Instruct~\citep{wang2022self} & LM generated & GPT-3/InstructGPT & SuperNaturalInstruction\\\midrule
       
       \multirow{3}{*}{Reasoning Steps Formatting}& CoT~\citep{cot} & Human design & GPT-3/CodeX & Reasoning tasks\\
       & AutoCoT~\citep{autocot} & LM generated & GPT-3/PaLM & Reasoning tasks\\
       &Self-Ask~\citep{selfask} & LM generated & GPT-3/InstructGPT & MultihopQA\\
       
    \bottomrule
    \end{tabular}}
    \caption{Summary of representative demonstration designing methods.}
    \label{tab:promptmethods}
\end{table*}
%In ICL, the inputs are fed to LLMs with demonstration examples to get outputs via language modeling. 
Many studies have shown that the performance of ICL strongly relies on the demonstration surface, including demonstration format, the order of demonstration examples, and so on~\citep{calibrate, lu2022order}. As demonstrations play a vital role in ICL, in this section, we survey demonstration designing strategies and classify them into two groups: demonstration organization and demonstration formatting, as shown in Table~\ref{tab:promptmethods}.
\subsection{Demonstration Organization}
\label{sec:organ}
Given a pool of training examples, demonstration organization focuses on how to select a subset of examples and the order of the selected examples. 

%Demonstrations show input-output mappings in ICL and the performance of ICL is sensitive to the combination and permutation of demonstrations. With limited demonstrations, demonstration organization strategies focus on how to \textbf{select} and \textbf{order} demonstrations.

\subsubsection{Demonstration Selection}
\label{sec:select}

% Suggestion from Xiaonna Li: 
% another taxnomy: into task-specific and instance specific selection categories.

Demonstrations selection aims to answer a fundamental question: Which examples are good examples for ICL? We classify related studies into two categories, including unsupervised methods based on pre-defined metrics and supervised methods.

\paragraph{Unsupervised Method} \citet{liu2022close} showed that selecting the closest neighbors as the in-context examples is a good solution. The distance metrics are pre-defined L2 distance or cosine-similarity distance based on sentence embeddings. %It outperformed significantly with farthest neighbors as examples. 
They proposed KATE, a $k$NN-based unsupervised retriever for selecting in-context examples. In addition to distance metrics, mutual information is also a valuable selection metric~\citep{sorensen2022information}. 
Similarly, $k$-NN cross-lingual demonstrations can be retrieved for multi-lingual ICL \citep{tanwar2023multilingual} to strengthen source-target language alignment.
%Another unsupervised example selection strategy is based on mutual information. 
%Without labeled examples and accessing to the LLMs, an example is selected when it has high mutual information. 
The advantage of mutual information is that it does not require labeled examples and specific LLMs. 
In addition, \citet{gonen2022demystifying} attempted to choose prompts with low perplexity. 
% \citet{Wu2022SelfadaptiveIL} selected the best subset permutation of $k$NN examples based on the code-length for data transmission to compress label $y$ given $x$ and $C$. This self-adaptive ranking method considered both selection and ordering. 
\citet{levy2022diverse} consider the diversity of demonstrations to improve compositional generalization. They select diverse demonstrations to cover different kinds of training demonstrations. 
%They also summarized the connection between code length and other metrics, including cross-entropy and mutual information.
Different from these studies selecting examples from human-labeled data, \citet{kim2022self} proposed to generate demonstrations from LLM itself. 

Some other methods utilized the output scores of LMs $P(y|C, x)$ as unsupervised metrics to select demonstrations.
\citet{Wu2022SelfadaptiveIL} selected the best subset permutation of $k$NN examples based on the code-length for data transmission to compress label $y$ given $x$ and $C$.
\citet{nguyen2023influence} measured the influence of a demonstration $x_i$ by calculating the difference between the average performance of the demonstration subsets $\{C|x_i\in C\}$ and $\{C|x_i\notin C\}$. Furthermore, \citet{li2023supporting} used infoscore, i.e., the average of $P(y|x_i,y_i,x) - P(y|x)$ for all $(x,y)$ pairs in a validation set with a diversity regularization.


\paragraph{Supervised Method} \citet{rubin2022learning} proposed a two-stage retrieval method to select demonstrations. For a specific input, it first built an unsupervised retriever (e.g., BM25) to recall similar examples as candidates and then built a supervised retriever EPR to select demonstrations from candidates. A scoring LM is used to evaluate the concatenation of each candidate example and the input. Candidates with high scores are labeled as positive examples, and candidates with low scores are hard negative examples. 
\citet{udr} further enhanced the EPR by adopting a unified demonstration retriever to unify the demonstration selection across different tasks.
\citet{ye2023compositional} retrieved the entire set of demonstrations instead of individual demonstrations to model inter-relationships between demonstrations. They trained a DPP retriever to align with LM output scores by contrastive learning and obtained the optimal demonstration set with maximum a posteriori at inference.

Based on prompt tuning, \citet{topic} view LLMs as topic models that can infer concepts $\theta$ from few demonstrations and generate tokens based on concept variables $\theta$. They use task-related concept tokens to represent latent concepts. Concept tokens are learned to maximize $P(y|x,\theta)$. They select demonstrations that are most likely to infer the concept variable based on $P(\theta|x,y)$. 
Besides, reinforcement learning was introduced by \citet{zhang2022active} for example selection. They formulated demonstration selection as a Markov decision process~\cite{bellman1957markovian} and selected demonstrations via Q-learning. The action is choosing an example, and the reward is defined as the accuracy of a labeled validation set. 


% \zc{}
%According to labeled candidates, EPR can be trained via in-batch contrastive learning.

% \paragraph{Reinforcement Learning Method} \citet{zhang2022active} formulated demonstration selection as a markov decision process and selected demonstrations via Q-learning. 

% Some researches (cite some works) focus on how to better organize the selected examples, including better ordering of selected examples and adding intermediate reasoning steps in demonstrations.
\subsubsection{Demonstration Ordering}
\label{sec:order}
Ordering the selected demonstration examples is also an important aspect of demonstration organization. 
\citet{lu2022order} have proven that order sensitivity is a common problem and always exists for various models. 
To handle this problem, previous studies have proposed several training-free methods to sort examples in the demonstration. \citet{liu2022close} sorted examples decently by their distances to the input, so the rightmost demonstration is the closest example.
% \citet{calibrate, lu2022order} show that the performance of ICL is sensitive to the permutation of demonstrations and the performance can vary from random guess to closely SOTA. 
%analyzed the sample order sensitivity for ICL in depth. They proved that order sensitivity is a common problem and always exists for various models. %, and a performant order is not transferable across models. 
\citet{lu2022order}  defined the global and local entropy metrics. They found a positive correlation between the entropy metric and the ICL performance. They directly used the entropy metric to select the best ordering of examples. %can estimate a good sample ordering for ICL by choosing one that has higher local or global entropy. 



% \subsubsection{Corpus- and Instance-Level Strategies}
% Overall, prompt formulation strategies can be divided into corpus-level and instance-level strategies.

\subsection{Demonstration Formatting}
\label{sec:format}
% \subsubsection{Instruction Formatting}
A common way to format demonstrations is concatenating examples $(x_1, y_1), \ldots, (x_k, y_k)$ with a template $\mathcal{T}$ directly. However, in some tasks that need complex reasoning (e.g., math word problems, commonsense reasoning), it is not easy to learn the mapping from $x_i$ to $y_i$ with only $k$ demonstrations. Although template engineering has been studied in prompting~\citep{liu2021pre},  some researchers aim to design a better format of demonstrations for ICL by describing tasks with the instruction $I$ (\S\ref{sec:formatting_instruction}) and adding intermediate reasoning steps between $x_i$ and $y_i$ (\S\ref{sec:formatting_intermediate}).


\subsubsection{Instruction Formatting}
\label{sec:formatting_instruction}
Except for the well-designed demonstration examples, good instructions which describe the task precisely are also helpful to the inference performance.
However, unlike the demonstration examples, which are common in traditional datasets, the task instructions depend heavily on human-written sentences.
\citet{induct} found that given several demonstration examples, LLMs can generate the task instruction.
According to the generation ability of LLMs, ~\citet{zhou2022large} proposed Automatic Prompt Engineer for automatic instruction generation and selection.
% To further save the human labor,
% ~\citet{} generate instruction by prompting the model to rephrase existing instruction.
To further improve the quality of the automatically generated instructions, ~\citet{wang2022self} proposed to use LLMs to bootstrap off its own generations.
Existing work has achieved good results in automatically generating instructions, which provided opportunities for future research on combining human feedback with automatic instruction generation.





\subsubsection{Reasoning Steps Formatting}
\label{sec:formatting_intermediate}
% \subsubsection{Reasoning}



\citet{cot} added intermediate reasoning steps between inputs and outputs to construct demonstrations, which are called chain-of-thoughts (CoT). With CoT, LLMs predict the reasoning steps and the final answer. CoT prompting can learn complex reasoning by decomposing input-output mappings into many intermediate steps. There are many pieces of research on CoT prompting strategies ~\citep{qiao2022reasoning} including prompt designing and process optimization. In this paper, we mainly focus on CoT designing strategies.

Similar to demonstration selection, CoT designing also considers CoT selection. Different from \citet{cot} manually writing CoTs, AutoCoT \citep{autocot} used LLMs with \textit{Let's think step by step} to generate CoTs. In addition, \citet{fu2022complexitycot} proposed a complexity-based demonstration selection method. They selected demonstrations with more reasoning steps for CoT prompting. 

As input-output mappings are decomposed into step-by-step reasoning, some researchers apply multi-stage ICL for CoT prompting and design CoT demonstrations for each step. Multi-stage ICL queries LLMs with different demonstrations in each reasoning step. Self-Ask \citep{selfask} allows LLMs to generate follow-up questions for the input and ask themselves these questions. Then the questions and intermediate answers will be added to CoTs. iCAP \citep{wang2022iteratively} proposes a context-aware prompter that can dynamically adjust contexts for each reasoning step. Least-to-Most Prompting \citep{least} is a two-stage ICL including question reduction and subquestion solution. The first stage decomposes a complex question into subquestions; in the second stage, LLMs answer subquestions sequentially, and previously answered questions and generated answers will be added into the context.

\citet{xu2023small} fine-tuned small LMs on specific task as plug-ins to generate pseudo reasoning steps. Given an input-output pair $(x_i,y_i)$, SuperICL regarded the prediction $y_i'$ and confidence $c_i$ of small LMs for the input $x_i$ as reasoning steps by concatenating $(x_i, y_i', c_i, y_i)$. 
% They further requested LMs to generate over-ride explanations if the final prediction $y$ is in-consistent with the prediction of small LMs $y'$.

% add \citep{mot} 


% \subsubsection{XXXX}
% \textit{\# Takeaway: Demonstration Designing strategies include Demonstration Organization and Demonstration Formatting. Demonstration Organization focuses on how to select and order demonstrations. Demonstration Formatting focuses on a better format of demonstrations.}
\textbf{$\Diamond$ Takeaway}: \textit{ (1) Demonstration selection strategies improve the ICL performance, but most of them are instance level. Since ICL is mainly evaluated under few-shot settings, the corpus-level selection strategy is more important yet under-explored. (2) The output score or probability distribution of LLMs plays an important role in instance selecting. (3) For $k$ demonstrations, the size of search space of permutations is $k!$. How to find the best orders efficiently or how to approximate the optimal ranking better is also a challenging question. (4) Adding chain-of-thoughts can effectively decompose complex reasoning tasks into intermediate reasoning steps. During inference, multi-stage demonstration designing strategies are applied to generate CoTs better. How to improve the CoT prompting ability of LLMs is also worth exploring (5) In addition to human-written demonstrations, the generative nature of LLMs can be utilized in demonstration designing. LLMs can generate instructions, demonstrations, probing sets, chain-of-thoughts, and so on. By using LLM-generated demonstrations, ICL can largely get rid of human efforts on writing templates. }