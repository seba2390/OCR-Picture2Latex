
% \subsection{ICL Capability of LLMs}
Although LLMs have shown promising ICL capability, many studies also show that the ICL capability can be further improved through a continual training stage between pretraining and ICL inference, which we call model warmup for short.  %The  LLMs manifested the ICL ability. It is first shown by GPT-3~\cite{gpt3}, the 175 billion parameters autoregressive language model. Immediately following GPT-3, a growing number of papers have proposed LLMs with ICL capabilities, e.g., GLaM~\cite{glam}, OPT~\cite{opt}, and AlexaTM 20B~\cite{alexatm}.
Warmup is an optional procedure for ICL, which adjusts LLMs before ICL inference, including modifying the parameters of the LLMs or adding additional parameters. Unlike finetuning, warmup does not aim to train the LLM for specific tasks but enhances the overall ICL capability of the model. 

% \textbf{There are many training variables that affect the ICL capacity of the original LLMs.} 
% In terms of the model scale,~\citet{gpt3} find that the ICL ability  grows when the parameters in LLMs increase from 0.1 billion to 175 billion.
% In terms of the pertaining corpus, ~\citet{corpusscale} study how the source and size of the pretraining corpus influence the ICL ability of LLMs. They discover that, rather than the size of the pertaining corpus, the source and data combination is critical for the emergence of ICL abilities for LLMs.

% \subsection{Parameters Updating}
\subsection{Supervised In-context Training}
\label{sec:s_tuning}
To enhance ICL capability, researchers proposed a series of supervised in-context finetuning strategies by constructing in-context training data and multitask training.
% Although experiments validate the ICL ability of the original LLMs, the performance of ICL remains unsatisfactory and high-variance compared to finetuning, especially when the task is very XXX[cite]. 
Since the pretraining objectives are not optimized for in-context learning~\citep{selfsupericl}, \citet{metaicl} proposed a method MetaICL to eliminate the gap between pretraining and downstream ICL usage.
The pretrained LLM  is continually trained on a broad range of tasks with demonstration examples, which boosts its few-shot abilities.
To further encourage the model to learn input-label mappings from the context, \citet{symboltuning} propose symbol tuning. This approach fine-tunes language models on in-context input-label pairs, substituting natural language labels (e.g., "positive/negative sentiment") with arbitrary symbols (e.g., "foo/bar"). As a result, symbol tuning demonstrates an enhanced capacity to utilize in-context information for overriding prior semantic knowledge.
% , e.g., 
% %model ability on making predictions condition on the examples in the demonstrations. 
% MetaICL obtains performance comparable to supervised finetuning on $52$ unique datasets.

% \subsection{Instruction Tuning}
% \label{sec:instruction_tuning}
% Besides, there is a research direction focusing on supervised instruction tuning. 
Besides, recent work indicates the potential value of instructions~\cite{mishra2021cross} and there is a research direction focusing on supervised instruction tuning. Instruction tuning enhances the ICL ability of LLMs through training on task instructions. %, mainly leveraging the intuition that NLP tasks can be described via natural language instructions
% , and \citet{optiml} further include varying numbers of demonstration examples.
Tuning the 137B LaMDA-PT~\cite{lamda} on over 60 NLP datasets verbalized via natural language instruction templates, FLAN~\cite{flan} improves both the zero-shot and the few-shot ICL performance.
Compared to MetaICL, which constructs several demonstration examples for each task, instruction tuning mainly considers an explanation of the task and is more easier to scale up. \citet{chung} and \citet{natural} proposed to scale up instruction tuning with more than 1000+ task instructions. %, and ~\citet{chung} found that the majority of the instruction-tuning improvement comes from using up to 282 tasks.

\subsection{Self-supervised In-context Training}
\label{sec:ss_tuning}
Leveraging raw corpora for warmup, \citet{selfsupericl} proposed constructing self-supervised training data aligned with ICL formats in downstream tasks. They transformed raw text into input-output pairs, exploring four self-supervised objectives, including masked token prediction and classification tasks. Alternatively, PICL~\cite{picl} also utilizes raw corpora but employs a simple language modeling objective, promoting task inference and execution based on context while preserving pre-trained models' task generalization. Consequently, PICL outperforms \citet{selfsupericl}'s method in effectiveness and task generalizability.

% instructino-tuning does not have demonstration examples
% To boost the zero-shot ICL ability of LLMs, ~\citet{flan} introduce instruction tuning. Instruction tuning refers to train pretrained LLMs on a mixture of tasks described as instructions. The instruction-tuned model, FLAN, substantially manifests stronger zero-shot abilities on unseen tasks.
 
% \subsection{Additional Parameters Adding}
% \subsection{Contextual Bias Calibrating}
% \label{sec:additional}
% Considering that retraining or updating the LLMs is always costly, researchers focus on tuning additional parameters for more accurate and unbiased ICL.
% As ICL on LLMs may be biased or prompt-sensitive
% ~\cite{calibrate,chen2022sensitivity}  
% \citet{calibrate} propose contextual calibration to adjust the model predictions, which fit the calibration parameters in a linear layer so that the prediction for the input to be uniform across answers. Contextual calibration provides a simple but effective solution for mitigating the potential bias of ICL without additional training data.

% Compared to the parameters updating approaches, methods which adding additional parameters mainly focuses on minor model prediction calibrating while retraining the original model performance, especially for debiasing and sensitivity reducing.

\textbf{$\Diamond$ Takeaway}: \textit{
% Warmup is optional for ICL as many pretrained LLMs have manifested the ICL ability. 
(1) Supervised training and self-supervised training both propose to train the LLMs before ICL inference. The key idea is to bridge the gap between pretraining and downstream ICL formats by introducing objectives close to in-context learning. Compared to in-context finetuning involving demonstration, instruction finetuning without a few examples as demonstration is simpler and more popular.  % Although self-supervised training uses raw corpus, the task variety is relatively limited. 
(2) To some extent, these methods all improve the ICL capability by updating the model parameters, which implies that the ICL capability of the original LLMs has great potential for improvement. Therefore, although ICL does not strictly require model warmup, we recommend adding a warmup stage before ICL inference.
% and a reasonable warmup can also reduce the difficulty of the later prompt designing and the sensitivity of ICL inference. 
(3) 
% Whether through supervised in-context training or self-supervised in-context training, the improvement of warmup will reach a plateau when increasingly scaling up the training tasks or training corpus. This indicates that warmup only adapts the LLMs to learn from the context.
The performance advancement made by warmup encounters a plateau when increasingly scaling up the training data. This phenomenon appears both in supervised in-context training and self-supervised in-context training, indicating that LLMs only need a small amount of data to adapt to learn from the context during warmup.
% But the ICL capability can be further improved through warmup. 
% To enhance the ICL capability, 
% Training on ICL format tasks or instructions enhance the ICL capability, and adding additional parameters works for minor calibration.
%To retain the ICL performance and 
%, adding additional parameters will be a better solution.
}
% \leiModify{As a takeway, this could be more precise, straight to the point - Lei ?}
