% \zy{shall we briefly discuss the pros and cons of different variants? e.g., direct limit the choices of templates, ppl is less efficient compared to direct, etc. probably add two columns in the table?}

\begin{table}[t!]
    \centering
    \resizebox{\linewidth}{!}{
    \begin{tabular}{@{}l|c|ccc@{}}
    \toprule 
      \bf Scoring Function & \bf Target & \bf Efficiency & \bf Task Coverage & \bf Stability \\
    \midrule 
       Direct  & $ \mathcal{M} ( y_j \mid C, x) $ & +++  &  + & + \\ 
       PPL &  $\text{PPL} (S_j) $&  + & +++ &  +\\ 
       % Calibrate~\citep{calibrate} & $\frac{\mathcal{M} ( y_j \mid C, x)}{\mathcal{M} ( y_j \mid \text{NULL}))}$ & \\ 
       Channel& $\mathcal{M} (x \mid C, y_j)$ & + & +  & ++ \\ 
    \bottomrule
    \end{tabular}}
    \caption{Summary of different scoring functions. }
    \label{tab:score_func}
\end{table}
% \qingxiu{citating}
The scoring function decides how we can transform the predictions of a language model into an estimation of the likelihood of a specific answer.
A direct estimation method~(Direct) adopts the conditional probability of candidate answers that can be represented by tokens in the vocabulary of language models~\citep{gpt3}. The answer with a higher probability is selected as the final answer. However, this method poses some restrictions on the template design, e.g., the answer tokens should be placed at the end of input sequences.
% \leiModify{Highlight the difference between task type and prediction type.}
Perplexity~(PPL) is another commonly-used metric, which computes the sentence perplexity of the whole input sequence $S_j = \{ C, s(x, y_j, I)\}$ consists of the tokens of demonstration examples $C$, input query $x$ and candidate label $y_j$. 
As PPL evaluates the probability of the whole sentence, it removes the limitations of token positions but requires extra computation time. 
Note that in generation tasks such as machine translation, ICL predicts the answer by decoding tokens with the highest sentence probability combined with diversity-promoting strategies such as beam search or Top-$p$ and Top-$k$~\citep{topp_sample} sampling algorithms.


Different from previous methods, which estimate the probability of the label given the input context, 
\citet{min2022noisy} proposed to utilize channel models~(Channel) to compute the conditional probability in a reversed direction, i.e., estimating the likelihood of input 
query given the label. In this way, language models are required to generate every token in the input, which could boost the performance under imbalanced training data regimes.
We summarize all three scoring functions in Table~\ref{tab:score_func}.
As ICL is sensitive to the demonstration~(see \S\ref{sec:prompt_tuning} for more details), normalizing the obtained score by subtracting a model-dependent prior with empty inputs is also effective for improving the stability and overall performance~\citep{calibrate}.

Another direction is to incorporate information beyond  the context length constrain to calibrate the score.
% demonstration examples are separately encoded with well-designed
% position embeddings, and then they are jointly attended by the test example using
% a rescaled attention mechanism.
Structured Prompting~\citep{hao2022structured} proposes to encode demonstration examples separately with special positional embeddings, which then are provided to the test examples with a rescaled attention mechanism.
$k$NN Prompting~\citep{knnPrompting} first queries LLMs with
training data for distributed representations, then predicts test instances by simply referring to nearest neighbors with closing representations with stored anchor representations.

% \textbf{$\Diamond$ Takeaway}: \textit{Selecting a proper scoring function according to the tasks in hand is important, and it has a great influence on the stability of the results.}
\textbf{$\Diamond$ Takeaway}: \textit{(1) We conclude the characteristics of three widely-used scoring functions in Table~\ref{tab:score_func}. Although directly adopting the conditional probability of candidate answers is efficient, this method still poses some restrictions on the template design. Perplexity is also a simple and widely scoring function. This method has universal applications, including both classification tasks and generation tasks. However, both methods are still sensitive to demonstration surface, while Channel is a remedy that especially works under imbalanced data regimes. (2) Existing scoring functions all compute a score straightforwardly from the conditional probability of LLMs. There is limited research on calibrating the bias or mitigating the sensitivity via scoring strategies. For instance, some studies add additional calibration parameters to adjust the model predictions~\cite{calibrate}. 
}

