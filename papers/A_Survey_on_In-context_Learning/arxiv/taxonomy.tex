\tikzstyle{my-box}=[
    rectangle,
    draw=hidden-draw,
    rounded corners,
    text opacity=1,
    minimum height=1.5em,
    minimum width=5em,
    inner sep=2pt,
    align=center,
    fill opacity=.5,
    line width=0.8pt,
]
\tikzstyle{leaf}=[my-box, minimum height=1.5em,
    fill=hidden-pink!80, text=black, align=left,font=\normalsize,
    inner xsep=2pt,
    inner ysep=4pt,
    line width=0.8pt,
]
\begin{figure*}[t!]
    \centering
    \resizebox{\textwidth}{!}{
        \begin{forest}
            forked edges,
            for tree={
                grow=east,
                reversed=true,
                anchor=base west,
                parent anchor=east,
                child anchor=west,
                base=left,
                font=\large,
                rectangle,
                draw=hidden-draw,
                rounded corners,
                align=left,
                minimum width=4em,
                edge+={darkgray, line width=1pt},
                s sep=3pt,
                inner xsep=2pt,
                inner ysep=3pt,
                line width=0.8pt,
                ver/.style={rotate=90, child anchor=north, parent anchor=south, anchor=center},
            },
            where level=1{text width=4em,font=\normalsize,}{},
            where level=2{text width=6em,font=\normalsize,}{},
            where level=3{text width=8em,font=\normalsize,}{},
            where level=4{text width=5em,font=\normalsize,}{},
            [
                In-context Learning, ver
                [
                    Training 
                    % [
                    %     Pretrain 
                    %     [
                    %         LLMs 
                    %         [
                    %             GPT-3~\cite{gpt3}{, }GLaM~\cite{glam}{, } OPT~\cite{opt}{, } \\ AlexaTM 20B~\cite{alexatm}
                    %             , leaf, text width=33em
                    %         ]
                    %     ]
                    %     [
                    %         Pretrain Corpus
                    %         [
                    %             Pretraining Corpus Combination~cite{shin2022corpora}
                    %             , leaf, text width=25em
                    %         ]
                    %     ]
                    % ]
                    [
                        Warmup  (\S \ref{sec:warmup})
                        [
                            Supervised \\ In-context \\ Training (\S \ref{sec:s_tuning})
                            [
                                MetaICL~\cite{metaicl}{, }OPT-IML~\cite{optiml}{, }FLAN~\cite{flan}{, }\\Super-NaturalInstructions~\cite{natural}{, }Scaling Instruction~\cite{chung}{, }\\Symbol Tuning~\cite{symboltuning}
                                , leaf, text width=36em
                            ]
                        ]
                        [
                            Self-supervised \\ In-context  \\ Training  (\S \ref{sec:ss_tuning})
                            [
                                Self-supervised ICL~\cite{selfsupericl}{, }PICL~\cite{picl}
                                , leaf, text width=28em
                            ]
                        ]
                    ]
                ]
                [
                    Inference
                    [
                    Demonstration \\ Designing (\S \ref{sec:demo})
                        [
                            Organization (\S \ref{sec:organ})
                            [
                                Selecting \\ (\S \ref{sec:select})
                                [   
                                    KATE~\cite{liu2022close}{, }EPR~\cite{rubin2022learning}{, }PPL~\cite{gonen2022demystifying}{, }\\SG-ICL~\cite{kim2022self}{, }Self Adaptive~\cite{Wu2022SelfadaptiveIL}{,}MI~\cite{sorensen2022information}{, }\\Q-Learning~\cite{zhang2022active}{,} Informative Score~\cite{li2023supporting}{,}\\Topic~\cite{topic}{, }UDR~\cite{udr}
                                    , leaf, text width=36.2em
                                ]
                            ]
                            [
                                Ordering \\ (\S \ref{sec:order})
                                [
                                GlobalE\&LocalE~\cite{lu2022order}
                                , leaf, text width=15em
                                ]
                            ]
                        ]
                        [
                            Formatting (\S \ref{sec:format})
                            [
                                Instruction \\ (\S \ref{sec:formatting_instruction})
                                [
                                    Instruction Induction~\cite{induct}{, }APE~\cite{zhou2022large}{, }\\Self-Instruct~\cite{wang2022self}
                                    , leaf, text width=30em
                                ]
                            ]
                            [
                                Reasoning \\ Steps \\ (\S \ref{sec:formatting_intermediate})
                                [
                                    CoT~\cite{wang2022self}{, }Complex CoT~\cite{fu2022complexitycot}{, }\\AutoCoT~\cite{autocot}{, }Self-Ask~\cite{selfask}{, } \\
                                    MoT\citep{mot}{, } SuperICL\citep{xu2023small}\\ iCAP~\cite{wang2022iteratively}{, }Least-to-Most Prompting~\cite{least}
                                    , leaf, text width=30.5em
                                ]
                            ]
                        ]
                    ]
                    [
                        Scoring \\ Function (\S \ref{sec:scoring})
                        [
                            Channel prompt tuning~\cite{min2022noisy}{, }
                            Structrured Prompting~\cite{hao2022structured}{, }\\
                            $k$NN-Prompting~\cite{knnPrompting}
                            , leaf, text width=36em
                        ]
                    ]
                ]
            ]
        \end{forest}
    }
    \caption{Taxonomy of in-context learning. The training and the inference stage are two main stages for ICL. During the training stage, existing ICL studies mainly take a pretrained LLM as backbone, and optionally warmup the model to strengthen and generalize the ICL ability. Towards the inference stage, the demonstration designing and the scoring function selecting are crucial for the ultimate performance.  }
    \label{taxo_of_icl}
\end{figure*}