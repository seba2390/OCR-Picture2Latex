\section{In-Context Learning Beyond Text}
\label{sec:mm_icl}
% Multimodal Applications
% Due to the great success of ICL in the area of NLP, the idea also has motivated pilot studies toward investigating the ICL ability in visual and multimodal tasks.

The tremendous success of ICL in NLP  has inspired researchers to explore its potential in different modalities, including visual, vision+language and speech tasks as well. 

\subsection{Visual In-Context Learning}
\begin{figure}
    \centering
    \includegraphics[width=0.49\textwidth]{fig/mm-case.pdf}
    \caption{Image-only and textual augmented prompting for visual in-context learning.}
    \label{fig:mm_case}
\end{figure}
\citet{bar2022visual_icl} employ an image patch infilling task in grid-like images using masked auto-encoders (MAE) to train their model. 
At the inference stage, the model generates output images consistent with provided input-output examples for a novel input image, showcasing promising ICL capabilities for unseen tasks such as image segmentation. Painter~\citep{wang2023imagesPainter} extends this approach by incorporating multiple tasks to build a generalist model, achieving competitive performance compared to task-specific models. Building upon this, SegGPT~\citep{wang2023seggpt} integrates diverse segmentation tasks into a unified framework and investigates ensemble techniques from spatial and feature perspectives to enhance the quality of prompt examples.
% \citet{wang2023prompt_diffusion} further propose to incorporate a text prompt along with the original input-output image pair to guide a generative model to comprehensively produce the desired image. The resulting Prompt Diffusion model~\citep{wang2023prompt_diffusion} is the first diffusion-based model that exhibits in-context learning ability.
\citet{wang2023prompt_diffusion} propose to utilize an extra text prompt to guide a generative model in comprehensively producing the desired image. The resulting Prompt Diffusion model is the first diffusion-based model that exhibits ICL ability. 
Figure~\ref{fig:mm_case} illustrates the key difference between the image-only and textual prompt augmented in-context learning for visual in-context learning.
% \paragraph{Visual In-Context Learning}
% \citet{bar2022visual_icl} utilize an image patch infilling task in grid-like images to train a model based on masked auto-encoders~(MAE).
% During the inference stage, given input-output image examples and a new input image, the model could automatically produce the output image that is consistent with the given examples. This paradigm demonstrates promising in-context learning on unseen tasks such as image segmentation. Painter~\citep{wang2023imagesPainter} further incorporates more tasks to construct a generalist model, achieving competitive results
% compared to task-specific models.
% Subsequently, SegGPT~\citep{wang2023seggpt} 
% unifies various segmentation tasks into a generalist framework, and explores ensemble methods from spatial 
% and feature aspects for improving the quality of prompt examples.


Similar to ICL in NLP, the effectiveness of visual in-context learning is significantly influenced by the selection of in-context demonstration images~\citep{zhang2023visual_icl_analysis,sun2023exploring_visual_icl}. 
To address this, \citet{zhang2023visual_icl_analysis} investigate two approaches: (1) an unsupervised retriever that selects nearest samples using an off-the-shelf model, and (2) a supervised method training an additional retriever model to maximize ICL performance. The retrieved samples notably enhance performance, exhibiting semantic similarity to the query and closer contextual alignment regarding viewpoint, background, and appearance.  Except for the prompt retrieval, \citet{sun2023exploring_visual_icl} further explore a prompt fusion technique for improving the results.




\subsection{Multi-Modal In-Context Learning}
In the vision-language area, \citet{tsimpoukelli2021frozen} utilize a vision encoder to represent an image as a prefix embedding sequence that is aligned with a frozen language model after training on the paired image-caption dataset.
The resulting model, Frozen, is capable of performing multi-modal few-shot learning. 
Further, \citet{alayrac2022flamingo} introduce
Flamingo, which 
combines a vision encoder with LLMs and adopts LLMs as the general interface to perform in-context learning on many multi-modal tasks.
They show that training on large-scale multi-modal web corpora with arbitrarily interleaved text and images is key to endowing them with in-context few-shot learning capabilities. 
Kosmos-1~\citep{huang2023kosmos} is another multi-modal LLMs and demonstrates promising zero-shot, few-shot,
and even multimodal chain-of-thought prompting abilities.
~\citet{hao2022language} present METALM, a general-purpose interface to models across tasks and modalities. With a semi-causal language modeling objective, METALM is pretrained and exhibits strong ICL performance across various vision-language tasks. 




It is natural to further enhance the ICL ability with instruction tuning, and the idea is also explored in the multi-modal scenarios as well.
Recent explorations first generate instruction tuning datasets transforming existing vision-language task dataset~\citep{xu2022multiinstruct,li2023otter} or with power LLMs such as GPT-4~\citep{liu2023llava,zhu2023minigpt4} , and connect LLMs with powerful vision foundational models such as BLIP-2~\citep{li2023blip2} on these multi-modal datasets~\citep{zhu2023minigpt4,dai2023instructblip}.



% \begin{table}[t!]
%     \centering
%     \begin{tabular}{l|c c}
%     \toprule
%       Modality   & Task &  Work   \\
%       \midrule
%      Vision  &  image generation & \\ 
%      Speech  & speech synthesis & \\ 
%      Vision + Language & image-to-text& \\ 
%     \bottomrule
%     \end{tabular}
%     \caption{Summarization of work investigates In-context learning beyond natural language.}
%     \label{tab:my_label}
% \end{table}

\subsection{Speech In-Context Learning}
In the speech area, ~\citet{wang2023neural} treated text-to-speech synthesis as a language modeling task. 
They use audio codec codes as an intermediate representation and propose the first TTS framework with strong in-context learning capability. 
Subsequently, VALLE-X~\citep{zhang2023valle-x} extend the idea to multi-lingual scenarios, demonstrating superior performance in zero-shot cross-lingual text-to-speech synthesis and zero-shot speech-to-speech translation tasks.

\textbf{$\Diamond$ Takeaway}: 
\textit{
(1) Recent studies have explored in-context learning beyond natural language with promising results.
Properly formatted data (e.g., interleaved image-text datasets for vision-language tasks) and architecture designs are key factors for activating the potential of in-context learning. Exploring it in a more complex structured space such as for graph data is challenging and promising~\citep{huang2023graph_icl}.
(2) Findings in textual in-context learning demonstration design and selection cannot be trivially transferred to other modalities. Domain-specific investigation is required to fully leverage the potential of in-context learning in various modalities.
% Findings in textual in-context learning demonstration design and selection cannot be trivially transferred to other modalities, and require domain-specific investigation. 
}

% 

