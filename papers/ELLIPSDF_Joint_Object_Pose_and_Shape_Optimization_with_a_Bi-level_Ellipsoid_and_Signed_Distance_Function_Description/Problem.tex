\section{Problem Formulation}
\label{sec:problem}


Consider an RGB-D camera whose optical frame has pose $\bfC_k \in \text{SE}(3)$ with respect to the global frame at discrete time steps $k = 1,\ldots,K$. Assume that the camera is calibrated and its pose trajectory $\crl{\bfC_k}_k$ is known, e.g., from a SLAM or SfM algorithm. At time $k$, the camera provides an RGB image $I_k : \Omega^2 \mapsto \mathbb{R}_{\geq 0}^3$ and a depth image $D_k : \Omega^2 \mapsto \mathbb{R}_{\geq 0}$ such that $I_k(\bfp)$ and $D_k(\bfp)$ are the color and depth of a pixel $\bfp \in \Omega^2$ in normalized pixel coordinates. The camera moves in an unknown environment that contains $N$ objects $\calO \triangleq \crl{\bfo_n}_{n=1}^N$. Each object $\bfo_n = (\bfc_n,\bfi_n)$ is an instance $\bfi_n$ of class $\bfc_n$, defined below.


\begin{definition*}
An \emph{object class} is a tuple $\bfc \triangleq \prl{\nu, \bfz, f_{\bftheta}, g_{\bfphi}}$, where $\nu \in \mathbb{N}$ is the class identity, e.g., chair, table, sofa, and $\bfz \in \mathbb{R}^d$ is a latent code vector, encoding the average class shape. The class shape is represented in a canonical coordinate frame at two levels of granularity: coarse and fine. The coarse shape is specified by an ellipsoid $\calE_\bfu$ in \eqref{eq:ellipsoid} with semi-axis lengths $\bfu = g_{\bfphi}(\bfz)$ decoded from the latent code $\bfz$ via a function $g_{\bfphi} : \bbR^d \mapsto \bbR^3$ with parameters $\bfphi$. The fine shape is specified by the signed distance $f_{\bftheta}(\bfx,\bfz)$ from any $\bfx \in \bbR^3$ to the average shape surface, decoded from the latent code $\bfz$ via a function $f_{\bftheta} : \bbR^3 \times \bbR^d \mapsto \bbR$ with parameters $\bftheta$.
\end{definition*}

\begin{definition*}
An \emph{object instance} of class $\bfc$ is a tuple $\bfi \triangleq \prl{\bfT, \delta\bfz}$, where $\bfT \in \text{SIM}(3)$ specifies the transformation from the global frame to the object instance frame, and $\delta\bfz \in \bbR^d$ is a deformation of the latent code $\bfz$, specifying the average shape of class $\bfc$.
\end{definition*}


\begin{figure*}[t] 
  \centering
  \includegraphics[width=\linewidth]{framework_new.jpg}
  \caption{ELLIPSDF Overview: A point cloud and initial pose (\textit{green}) are obtained from RGB-D detections of a chair instance from known camera poses (\textit{blue}). A bi-level category shape description, consisting of a latent shape code, a coarse shape decoder, and a fine shape decoder (\textit{orange}), is trained offline using a dataset of mesh models. Given the observed point cloud, the pose and shape deformation of the newly seen instance are optimized jointly online, achieving shape reconstruction in the global frame (\textit{red}).}
%   allowing shape reconstruction in the global frame (\textit{red}).}
  \label{fig:framework}
  %The point cloud of the object and the initial object pose (in the green rectange) are obtained from the RGB-D detections, including color image, depth image, instance segmentation, and fitted ellipse, and camera poses (in the blue rectangles). Then leveraging the observed object point cloud and the initial pose, \text{SIM}(3) object pose and the latent code of a two-level object model (in the orange rectangle) trained from a dataset (in the blue rectangle) are jointly optimized. The object in the world frame (in the red rectangle) could be reconstructed from the optimized object pose and latent code.}
\end{figure*}


We assume that object detection (e.g., \cite{Cai2019Cascade}) and tracking (e.g., \cite{bewley2016simple}) algorithms are available to provide the class $\bfc_n$ and pixel-wise segmentation $\Omega_{n,k}^2 \subseteq \Omega^2$ of any object $n$ observed by the camera at time $k$. %The segmentation $\Omega_{n,k}^2\triangleq \crl{ \bfp \in \Omega^2 \mid  \Delta_k(\bfp) = n}$ of object $n$ at time $k$ is obtained from labeling $\Delta_k : \Omega^2 \rightarrow \{0,\ldots,N\}$ of each pixel $\bfp \in \Omega^2$ with an object id $\Delta_k(\bfp) \in \crl{1,\ldots,N}$ or the scene background $\Delta_k(\bfp) = 0$.
Our goal is to estimate the transformation and shape $\bfi_n := (\bfT_n, \delta \bfz_n)$ of each observed object $n$. We consider object instances independently and drop the subscript $n$ when it is clear from the context. 
%\SHAN{n is still used below, do we need to say the sentence below?}
%in the reminder for clarity.

%  and the camera pose $\bfC_k$

Given an object with multi-view segmentation $\Omega_{k}^2$, we use the depth $D_k(\bfp)$ of each pixel $\bfp \in \Omega_{k}^2$ to obtain a set of points $\calX_k(\bfp)$ along the ray starting from the camera optical center and passing through $\bfp$. The sets $\calX_k(\bfp)$ is used to optimize the pose and shape of the object instance. For each ray, we choose three points, one lying on the observed surface, one a small distance $\epsilon>0$ in front of the surface, and one a small distance $\epsilon$ behind. Given $d \in \{0,\pm \epsilon\}$, we obtain points $\bfy \in \bbR^3$ in the optical frame corresponding to the pixels $\bfp \in \Omega_{k}^2$:
%
\begin{equation*}
\scaleMathLine{\calY_k(\bfp) \triangleq \crl{(\bfy, d) \,\bigg\vert\, \bfy = \prl{D_k(\bfp) + \frac{d}{\|\underline{\bfp}\|}}\underline{\bfp}, \; d \in \{0,\pm \epsilon\}},}
\end{equation*}
%
and project them to the global frame using the known camera pose $\bfC_k$:
%
\begin{equation}\label{eq:distance_measurements}
\calX_k(\bfp) \triangleq \crl{(\bfx, d) \,\bigg\vert\, \bfx = \bfP \bfC_k \underline{\bfy}, \; (\bfy,d) \in \calY_k(\bfp)}.
\end{equation}
%
%At training time, distance measurements like in \eqref{eq:distance_measurements} are obtained from several object instances of the same class and are used to optimize the shape model parameters $\bfz$, $\bftheta$, $\bfphi$, as described in Sec.~\ref{sec:train_code}. 

We define an error function $e_{\bfphi}$ to measure the discrepancy between a distance-labelled point $(\bfx,d) \in \calX_{k}(\bfp)$ observed close to the instance surface and the coarse shape $\calE_{\bfu}$ provided by $\bfu = g_{\bfphi}(\bfz)$. Another error function $e_{\bftheta}$ is used for the difference between $(\bfx,d)$ and the SDF value $f_{\bftheta}(\bfx, \bfz)$ predicted by the fine shape model. The overall error function is defined as: 
\begin{align}
\label{eq:cost_function}
&e(\bfT,\delta \bfz, \bftheta, \bfphi ; \crl{\calX_k(\bfp)}) \triangleq \alpha e_r(\delta \bfz) \\
&+ \sum_{k=1}^K 
      \sum_{\bfp \in \Omega_{k}^2}
      \sum_{(\bfx,d) \in \calX_k(\bfp)} \!\!\!\beta e_{\bftheta}(\bfx,d,\bfT,\delta \bfz) + \gamma  e_{\bfphi}(\bfx,d,\bfT,\delta \bfz)\notag,
\end{align}
where $e_r(\delta \bfz)$ is a shape deformation regularization term. The coarse-shape error, $e_{\bftheta}$, fine-shape error, $e_{\bfphi}$, and the regularization, $e_r$ are defined precisely in Sec. \ref{sec:train_code}.


We distinguish between a training phase, where we optimize the parameters $\bfz$, $\bftheta$, $\bfphi$ of an object class using offline data from instances with known mesh shapes, and a testing phase, where we optimize the pose $\bfT$ and shape deformation $\delta \bfz$ of a previously unseen instance from the same category using online distance data from an RGB-D camera.

In training, we generate points $\crl{\calX_{n,k}(\bfp)}$ close to the surface of each available mesh model $n$ in a canonical coordinate frame (with identity pose $\bfI_4$) and optimize the class shape parameters via:
%
\begin{equation}
\min_{\crl{\delta \bfz_n}, \bftheta, \bfphi} \sum_n e(\bfI_4,\delta \bfz_n, \bftheta, \bfphi ; \crl{\calX_{n,k}(\bfp)}).
\end{equation}

%During the training phase, we generate distance measurements sampled from the mesh model of an object with a canonical pose $\bfI_4$ and learn the latent code $\bfz$ as well as the decoder parameters $\bfphi$, $\bftheta$ by minimizing the cost function: 

In testing, we receive points $\crl{\calX_{k}(\bfp)}$ in the global frame, generated by the RGB-D camera from the surface of a previously unseen instance. Assuming known object class, we fix the trained shape parameters $\bfz^*$, $\bftheta^*$, $\bfphi^*$ and optimize the unknown instance transform $\bfT \in \text{SIM}(3)$ and shape deformation $\delta \bfz \in \bbR^d$:
%
\begin{equation} \label{eq:test_optimization}
\min_{\bfT, \delta \bfz} \; e(\bfT,\delta \bfz, \bftheta^*, \bfphi^* ; \crl{\calX_k(\bfp)}).
\end{equation}


















