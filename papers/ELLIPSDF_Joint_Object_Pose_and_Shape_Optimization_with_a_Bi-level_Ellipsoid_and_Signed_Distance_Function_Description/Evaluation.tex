\section{Evaluation}
\label{sec:evaluation}

\begin{figure*}[t]
    \centering
    \includegraphics[width=0.85\linewidth]{qualitive_0518_0314.jpg}
    \caption{Qualitive results. Column a): Ground-truth scene in ScanNet Sequence $0518$ (upper row) and $0314$ (lower row). Column b): The RGB axes are the camera trajectory, point clouds are the ones obtained from RGB-D sensor with added pesudo points, and the ellipsoids (black for chair, red for sofa, blue for monitor, brown for table) are the initialized objects. Column c): Reconstructed meshes using ELLIPSDF, rendered from the optimized latent code and pose.}
    \label{fig:qual_results}
  \end{figure*}
  

\subsection{Training Details}
\label{sec:training_details}
The ELLIPSDF decoder model is trained on synthetic CAD models from ShapeNet \cite{chang2015shapenet}. Each model's scale is normalized to be inside a unit sphere. We sample points and calculate their SDF values using a uniform distribution in the unit sphere for training the coarse-level shape decoder $g_{\bfphi}$. Another set of points that are close to the model surface are sampled for training the fine-level shape decoder $f_{\bftheta}$.

The following setting were used to train the decoder networks and the latent shape code $\bfz$. We use the Adam optimizer with initial learning rate $5\times 10^{-4}$,  $0.5$ ratio decay every 300/700 epochs for the coarse and fine level networks separately. 
The total epoch number is $1500$. 
The latent code dimension is $64$, and the network structure follows the model in DualSDF \cite{hao2020dualsdf}. 

\subsection{Qualitative Results}

We evaluate ELLIPSDF on the ScanNet dataset \cite{dai2017scannet}, which provides 3D scans captured by a RGB-D sensor of indoor scenes with chairs, tables, displays, etc. We segment out objects from scene-level mesh using provided instance labels, and sample points from object meshs to generate point observations. 
Visualizations of shape optimization for a chair are shown in Fig.~\ref{fig:fig1_refined}. Optimization step improves the scale and shape estimates notably, e.g. by transforming the four-leg mean shape into an armchair. 
Larger scale qualitative results are shown in Fig.~\ref{fig:qual_results}, demonstrating the effectiveness of joint shape and pose optimization. Optimized poses are closer to the ground-truth, and optimized shapes resemble the objects better than simple primitive shapes such as cuboids or quadrics that lacks fine details. 
For example, the successful reconstruction of an angle sofa is illustrated in the upper row in Fig.~\ref{fig:qual_results}, which deforms from an initial mean sofa shape that does not have an angle. 
\begin{figure}[t]
    \centering
    \includegraphics[width=1\linewidth]{chair_refined.jpg}
    \caption{Intermediate ELLIPSDF stages. First column: RGB image, depth image, instance segmentation (yellow), fitted ellipse (red) for a chair in ScanNet scene $0461$. Second column: mean shape and ellipsoid with initialized pose. Third column: optimized fine-level and coarse-level shapes with optimized pose.}
    \label{fig:fig1_refined}
\end{figure}
ELLIPSDF is also able to deal with partial observations as seen in the lower row in Fig.~\ref{fig:qual_results}. Although the observed point clouds of the displays and the chairs are sparse, our approach still reconstructs those objects successfully. Nevertheless, the reconstruction is a square instead of rounded for the table due to a severe occlusion of the observation that only less than half of the table is observed. 

\subsection{Quantitative Results}

This section presents quantitative evaluation against other methods regarding both pose and shape estimation accuracy. We also present ablation studies to showcase the improvement of the optimization over initialization-only results, and the bi-level model over a one level model. 
% In the tables in this section, ELLIPSDF (fine) means only the fine level model is used, and ELLIPSDF (coarse+fine) means both coarse and fine level models are used.

%\subsubsection{Evaluation on Object Pose}

\begin{table}[t]
\centering
% \caption{Quantitative results on pose estimation. ELLIPSDF (init) means using the results from initialization only. ELLIPSDF (opt) means the results are from both initialization and optimization.}
\caption{Quantitative results for pose estimation on ScanNet~\cite{dai2017scannet}.}
%\begin{center}
\scalebox{0.78}{
    \begin{tabular}{c|c|c|c} % <-- Alignments: 1st column left, 2nd middle and 3rd right, with vertical lines in between
    \hline
    Scan2CAD \cite{avetisyan2019scan2cad} & Vid2CAD \cite{maninis2020vid2cad} & ELLIPSDF (init) & ELLIPSDF (opt) \\
    \hline
    31.7 & 38.3 & 31.5 & \textbf{39.6} \\
    \hline
    \end{tabular}}
    \label{tab:pose}
%\end{center}
\end{table}

{\vspace{1ex}\bf \noindent Evaluation on Object Pose: }%
We obtain the ground-truth object pose annotations from Scan2CAD \cite{avetisyan2019scan2cad} and follow the pose evaluation metrics it defines, which decomposes a pose $\bfT \in \text{SIM}(3)$ into rotation $\bfq$, translation $\bfp$ and scale $\bfs$. For an accurate pose estimation, the error thresholds for translation, rotation, and scales are set as $0.2$, $20^{\circ}$ and $20\%$ respectively with respect to the ground-truth pose.
The pose evaluation is presented in Tab.~\ref{tab:pose}, in which ELLIPSDF (init) refers to the initialization-only step in Sec.~\ref{sec:shape_pose_inference}, whereas ELLIPSDF (opt) refers using both the initialization and optimization steps in Sec.~\ref{sec:shape_pose_inference}.
The last two columns in Tab.~\ref{tab:pose} show that adding optimization step using SDF residuals improves the estimation by the initialization-only variant, due to the additional SDF residuals to help estimate pose. 
Moreover, ELLIPSDF (opt) outperforms both Scan2CAD and Vid2CAD, which demonstrates the superiority of ELLIPSDF that employs a primitive ellipsoid shape tailored for pose and scale estimation.
% because we employ a primitive ellipsoid shape tailored for pose and scale estimation. 

%\subsubsection{Evaluation on Object Shape}
%\label{sec:eval_shape}


{\vspace{1ex}\bf \noindent Evaluation on Object Shape: }%
We evaluate ELLIPSDF for shape prediction on ScanNet \cite{dai2017scannet} dataset in Tab.~\ref{tab:fitting_performance}. Instead of single object evaluation in FroDO \cite{runz2020frodo}, we evaluate on multiple objects, which is harder than the single-object-scene due to clustering and partial observations. The large scale evaluation verifies that our method can generalize across different sequences and objects.
% \NA{Why is this a plus if we process the objects independently?}.
The object point cloud sampled from the object mesh from~\cite{avetisyan2019scan2cad} is used as the ground truth $\calS_{gt}$, and the estimated point cloud $\calS_{est}$ is generated from the optimized latent code $\bf{z} + \delta \bf{z}$. 
% We adopt the following procedure to evaluate the shape. 
% First, points are uniformly sampled from the object mesh, and their associated SDF values are calculated using the $f_{\bftheta}\left(\cdot; \mathbf{z}\right)$ function. 
% Then a mesh is generated using marching-cube algorithm \cite{Lorensen1987MarchingCubes}. Lastly a fixed number of points are sampled from the object mesh.
% We evaluate the fitting between ground truth scan and our ELLIPSDF shape on multiple objects across many sequences.
Given the ground-truth point cloud $\calS_{gt}$ and ELLIPSDF point cloud $\calS_{est}$ for an object, the fitting rate with inlier ratio is
\begin{equation} 
\begin{aligned} \label{eq:fitting}
fit(\calS_{est}, \calS_{gt}) &= \frac{\vert \calS_{close} \vert}{\vert \calS_{est} \vert}, \\
\calS_{close} &= \{\bfv \in \calS_{est}: d_f(\bfv, \calS_{gt}) < \lambda \}, 
\end{aligned}
\end{equation}
where $\lambda = 0.2 (m)$. A distance function $d_f(\cdot, \cdot)$ is utilized to measure the distance between a point $\bfv$ and a point cloud $\calS$, which is the distance from the closest point $\bfu \in \calS$ to the point $\bfv$. In CAD-Deform \cite{ishimtsev2020cad}, the distance function is set to be L1 distance, while we use L2 distance.

\begin{table}[t]
    \centering
    % \small
    \caption{Quantitative results for shape evlaution on ScanNet\cite{dai2017scannet}.}\label{tab:fitting_performance}
    \scalebox{0.8}{
    \begin{tabular}{l|c|c|c|c|c}
    \hline
     \textbf{Method} & cabinet & chair & display & table   & avg. \\ 
    \# intances  & 132 & 820 & 209 & 146   & 327  \\ \hline\hline
    ELLIPSDF (fine)  & 88.4 & 88.3 & 90.6 & 76.2       & 85.9 \\ 
    ELLIPSDF (coarse+fine)  & \textbf{91.0} & \textbf{90.6} & \textbf{96.9} & \textbf{77.3}   & \textbf{89.0} \\ \hline
    \end{tabular}
    }
\end{table}

We run ELLIPSDF (fine) and ELLIPSDF (coarse+fine) on 150 validation sequences on ScanNet \cite{dai2017scannet}, where ELLIPSDF (fine) means only the fine level SDF residual is used by setting $\gamma = 0$ in \eqref{eq:cost_function}, and ELLIPSDF (coarse+fine) means the bi-level SDF residuals are used. For each optimized object, we calculate the fitting rate and then average across all instances. In Tab.~\ref{tab:fitting_performance}, we show the number of instances and average fitting rates for 4 object classes.
ELLIPSDF (coarse+fine) achieves better results than ELLIPSDF (fine) across all classes, demonstrating an average 3\% boost of fitting rate with the assistance of coarse model, reaching nearly 90\% accuracy. The results indicate the effectiveness of the coarse level error function for improving the scale estimation. 
% When comparing with CAD-Deform, we have superior results on \textit{cabinet} category but fall short on others. It is worth noting that we use different distance function as well as different sequences on the dataset. 
% A unified benchmark for shape evaluation has yet to be developed.
% \[
%     d_{\text{Chamfer}}(S_1,S_2) = \frac{1}{N_{S_1}}\sum_{x\in S_1}\min_{y\in S_2}\|x-y\|_2
% \]
% where $N_{S_1}$ is the cardinality. 
% CAD-Deform uses L1 distance instead of L2 (chamfer's) distance. We report both for comparison.





%\subsubsection{Evaluation on 3D IoU}

\begin{table}[t]
% \caption{3D detection comparison on ScanNet. The metric is borrowed from MOLTR~\cite{li2020mo}.}
\centering
\caption{Comparison of 3D detection results on ScanNet \cite{dai2017scannet}.}
%\begin{center}
\small
    \begin{tabular}{l|c|c|c} % <-- Alignments: 1st column left, 2nd middle and 3rd right, with vertical lines in between
    \hline
    mAP @ IoU=0.5 & Chair & Table & Display \\
    \hline\hline
    FroDO \cite{runz2020frodo} & $0.32$ & $0.06$ & $0.04$\\
    MOLTR \cite{li2020mo} & $0.39$ & $0.06$ & $0.10$ \\
    % QuadricSLAM \cite{nicholson2018quadricslam} & x & x & x \\
    ELLIPSDF (fine) & 0.42 & 0.26 & 0.25 \\
    ELLIPSDF (coarse+fine) & \bf{0.43} & \bf{0.27} & \bf{0.31} \\
    \hline
    \end{tabular}
    \vspace{0.2cm}
    \label{tab:detection_moltr}
%\end{center}
\end{table}

{\vspace{1ex}\bf \noindent Evaluation on 3D IoU: }%
For a quantitative evaluation on pose estimation, our approach is compared with FroDO~\cite{runz2020frodo} and MOLTR~\cite{li2020mo} on ScanNet~\cite{dai2017scannet}. The ground-truth object poses and shapes are from Scan2CAD \cite{avetisyan2019scan2cad}, whereas the estimated 3D bounding box is generated from the estimated point cloud.
The evaluation metric is same as \cite{li2020mo}, i.e. mean Average Precision (mAP), and the IoU threshold is 0.5. The results are shown in Tab. \ref{tab:detection_moltr}. 
First, we compare the bi-level model against the one-level model. From the last two rows in Tab. \ref{tab:detection_moltr}, ELLIPSDF (coarse+fine) is superior than ELLIPSDF (fine) in terms of 3D IoU, and thus demonstrates that the bi-level model is beneficial by providing additional cues to constrain the pose and shape. The improvement is more significant for smaller objects, e.g. the displays. This may be explained by the fact that the initialization error is relatively larger for smaller objects, and thus requires a coarse shape residual to confine its pose. 
% On the contrary, the fine level SDF residual could aid the optimization for the shape details.   
Moreover, ELLIPSDF outperforms both FroDO and MOLTR by a large margin for two probably reasons. Firstly, 3D point clouds are used in the observation for ELLIPSDF, while the other two only rely on 2D observations. Secondly, ELLIPSDF computes coarse level SDF residuals using a primitive shape to aid the estimation of pose and shape scale, whereas the other methods use SDF residuals computed from fine shape details. 
