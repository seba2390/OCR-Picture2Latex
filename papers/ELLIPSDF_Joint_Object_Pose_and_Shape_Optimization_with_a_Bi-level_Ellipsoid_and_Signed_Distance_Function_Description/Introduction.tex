\section{Introduction}
\label{sec:introduction}

Range sensors, such as RGB-D cameras and LIDARs, have become a primary data source for robot localization and mapping due to their increasing accuracy, affordability, and compactness. This has contributed to the development of RGB-D Simultaneous Localization And Mapping (SLAM) \cite{kerl2013dense, newcombe2011kinectfusion, palazzolo2019refusion, wang2019real} and Structure from Motion (SfM) \cite{agarwal2011building, crandall2011discrete, schonberger2016structure} approaches that provide accurate and efficient ego-motion estimation and map reconstruction. The map representations used in RGB-D SLAM, however, are predominantly geometric, composed of point landmarks \cite{kerl2013dense, sunderhauf2017meaningful}, surfels \cite{whelan2016elasticfusion} or explicit (mesh) and implicit (signed distance field) surface representations \cite{newcombe2011kinectfusion, rosinol2019kimera}. These geometric models do not provide semantic information such as the class, pose, shape, or affordances of objects in the scene. Maps that combine geometric and semantic information are useful and understandable for humans and allow specification of symbolic tasks, such as retrieval, object-directed navigation, grasping, and safety critical operation, in terms of object entities.

Recent works that focus on object-level localization and mapping include~\cite{pronobis2011semantic,salas2013slam++, mu2016slam, sunderhauf2017meaningful, mccormac2018fusion++, hu2019deep}, which utilize objects as landmarks for localization and navigation~\cite{salas2013slam++, Atanasov_SemanticLocalization_IJRR15, mu2016slam, sunderhauf2017meaningful, mccormac2018fusion++, hu2019deep} or as functional entities for motion, part, and affordance identification~\cite{lu2018beyond, qiu2019tracking, mo2019partnet, Luo_2020_ICLR}. The memory and computational efficiency of the object representations used by semantic SLAM are vital for accommodating online construction, onboard storage, and multi-robot use of large semantic maps. On one hand, a parsimonious way for optimizing and storing object maps is needed to ensure online computation and low onboard memory use. On the other hand, it is desirable to preserve as many details about the object shapes, texture, and affordances as possible. Striking the right balance between a faithful object reconstruction and a compact object representation remains an open research problem.

\begin{figure}[t]
  \centering
  \includegraphics[width=1\linewidth]{overview_new.jpg}
  \caption{Overview of ELLIPSDF: a) Ground-truth scene reconstruction from colored point clouds in ScanNet~\cite{dai2017scannet} scene $0087$, where the RGB axes show the camera trajectory, b) Reconstructed object meshes in the world frame using the SDF model decoded from a latent code, and the optimized \text{SIM}(3) transformation representing object pose.}
  \label{fig:overview}
\end{figure}

This paper proposes ELLIPSDF, which is an expressive yet compact model of object pose and shape, and an associated optimization algorithm to infer an object-level map from multi-view RGB-D camera observations, as shown in Fig.~\ref{fig:overview}. ELLIPSDF is expressive because it captures the identity, scale, position, orientation, and shape of objects in the environment. It is compact because it relies on a low-dimensional latent encoding of the signed distance function (SDF) to an object's surface, allowing onboard storage of large multi-category object maps.

Shape representation using SDF predicted by an autodecoder network was proposed in DeepSDF~\cite{park2019deepsdf} and DualSDF~\cite{hao2020dualsdf}. In this paper, we extend the SDF prediction network in prior works by proposing a bi-level object model with a shared latent representation. 
% We are the first to employ an object representation for multi-view and multi-object localization and mapping.
Object primitive shapes and SDF are predicted from a shared latent space.
On the coarse-level, an ellipsoid is used as a primitive shape to constrain the overall shape scale. On the fine-level, an autodecoder similar to DeepSDF is used to preserve the object shape details. 
To summarize, the main \textbf{contribution} of this work is the design of
\begin{itemize}
  \item a bi-level object model with coarse and fine levels, enabling joint optimization of object pose and shape. The coarse-level uses a primitive shape for robust pose and scale initialization, and the fine-level uses SDF residual directly to allow accurate shape modeling. The wo levels are coupled via a shared latent space.
  
  \item a cost function to measure the mismatch between the bi-level object model and the segmented RGB-D observations in the world frame. 
\end{itemize}
% Our approached is named as ELLIPSDF\NA{Introduce the name of the approach earlier as you are describing and motivating the method.}. Videos and software can be found on the project website \url{http://me-llamo-sean.cf/semantic_sdf_mapping_githubpage/}. 

