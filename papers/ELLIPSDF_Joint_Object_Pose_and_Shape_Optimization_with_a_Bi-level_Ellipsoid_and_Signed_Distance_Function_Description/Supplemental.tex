\section*{Supplementary Material}

\subsection*{Trained Object Models}

This section provides additional visualizations for the trained object models. Training loss for the chair category is visualize in Fig.~\ref{fig:training_loss_chair}, which shows the loss is decreasing and stabilizes around 40,000 epochs. 

Fig.~\ref{fig:trained_model_chair} visualizes the rendering results for some chairs in the training set. It shows that the scale of the primitive-based representation varies proportionally with the high-resolution representation. 

\begin{figure}[thp!]
    \centering
    \includegraphics[width=\linewidth]{loss_chairs.jpg}
    \caption{Visualization of the training loss for chairs.}
    \label{fig:training_loss_chair}
\end{figure}


Fig.~\ref{fig:trained_model_sofa} visualizes the rendering results for sofas in the training set. There is a lack of shape variation since the majority of sofas have similar structure. Nevertheless, the ellispoid for the angle sofa is still different with that of other sofas. 

\begin{figure}[thp!]
    \centering
    \includegraphics[width=\linewidth]{trained_model_chair.jpg}
    \caption{Visualization of the trained object model for chairs. Upper row: coarse ellipsoid shapes regressed from $g_{\bfphi}$ and $\bfz$. Lower row: SDF object model from $f_{\bftheta}$ and $\bfz$.}
    \label{fig:trained_model_chair}
\end{figure}





\begin{figure}[thp!]
    \centering
    \includegraphics[width=\linewidth]{trained_model_sofa.jpg}
    \caption{Visualization of the trained object model for sofas. Upper row: coarse ellipsoid shapes regressed from $g_{\bfphi}$ and $\bfz$. Lower row: SDF object model from $f_{\bftheta}$ and $\bfz$.}
    \label{fig:trained_model_sofa}
\end{figure}



Fig.~\ref{fig:trained_model_table} visualizes the rendering results for tables in the training set. Similar to sofas, the variation is limited due to similar table shapes. Nonetheless, the ellipsoid for the rounded table is different from the rest. 


\begin{figure}[thp!]
    \centering
    \includegraphics[width=\linewidth]{trained_model_table.jpg}
    \caption{Visualization of the trained object model for tables. Upper row: coarse ellipsoid shapes regressed from $g_{\bfphi}$ and $\bfz$. Lower row: SDF object model from $f_{\bftheta}$ and $\bfz$.}
    \label{fig:trained_model_table}
\end{figure}



Fig.~\ref{fig:trained_model_trashbin} visualizes the rendering results for trashbins in the training set. It could be observed that the ellipsoid shape varies based on the object shape, for instance, the ellipsoid is enlongated for a tall trashbin. 


\begin{figure}[thp!]
    \centering
    \includegraphics[width=\linewidth]{trained_model_trashbin.jpg}
    \caption{Visualization of the trained object model for trashbins. Upper row: coarse ellipsoid shapes regressed from $g_{\bfphi}$ and $\bfz$. Lower row: SDF object model from $f_{\bftheta}$ and $\bfz$.}
    \label{fig:trained_model_trashbin}
\end{figure}


\begin{figure}[thp!]
    \centering
    \includegraphics[width=\linewidth]{trained_model_display.jpg}
    \caption{Visualization of the trained object model for displays. Upper row: coarse ellipsoid shapes regressed from $g_{\bfphi}$ and $\bfz$. Lower row: SDF object model from $f_{\bftheta}$ and $\bfz$.}
    \label{fig:trained_model_display}
\end{figure}

\begin{figure}[thp!]
    \centering
    \includegraphics[width=\linewidth]{trained_model_cabinet.jpg}
    \caption{Visualization of the trained object model for cabinets. Upper row: coarse ellipsoid shapes regressed from $g_{\bfphi}$ and $\bfz$. Lower row: SDF object model from $f_{\bftheta}$ and $\bfz$.}
    \label{fig:trained_model_cabinet}
\end{figure}

Fig.~\ref{fig:trained_model_display} visualizes the rendering results for displays in the training set. The ellipsoid is rounded for the thicker display and is very thin for the rest. 

Fig.~\ref{fig:trained_model_cabinet} visualizes the rendering results for cabinets in the training set. The ellipsoid varies according to the different cabinet shapes. 



\subsection*{More Qualitative Results on ScanNet}

This section presents more qualitative results on ScanNet~\cite{dai2017scannet}. 
Fig.~\ref{fig:scannet_qualitative_0077_01} shows a reconstruction with table, trashbins, and cabinet. The cabinet and trashbins are reconstructed well, as can be seen from the resulting meshes which resemble the original object shapes. However, the table is poorly reconstructed, since the shape is quite different and the pose is inaccurate. This is because the available observation in the scene for the table is very limited, as can be seen in the segmented mesh, which is insufficient for optimization. 



A ScanNet scene with bookshelves and tables are shown in Fig.~\ref{fig:scannet_qualitative_0208_00}, to demonstrate the usefulness of the coarse and fine level residuals. The figure illustrates that the initialized object pose and shape are different from the actual scene, since the two bookshelves in the center are not parallel and are too small compared to the observation. In contrast, the bookshelves become larger after applying the fine level residual, which is more consistent with the observations. The reconstructions are further improved with both the coarse and fine level residuals, where the bookshelves become parallel. Moreover, the bottom bookshelf and the top right table also become thinner, which agrees more with the observation. 
This example clearly shows the effectiveness of the proposed bi-level model for joint object pose and shape optimization.  

\begin{figure}[thp!]
    \centering
    \includegraphics[width=\linewidth]{qualitive_0077_01.jpg}
    \caption{Visualization of the original scene and reconstructed objects for ScanNet scene $0077$. The green arrows point to the segmented mesh of the objects.}
    \label{fig:scannet_qualitative_0077_01}
\end{figure}

\begin{figure*}[thp!]
    \centering
    \includegraphics[width=\linewidth]{qualitive_0208_00.jpg}
    \caption{Visualization of the original scene and reconstructed objects for ScanNet scene $0208$. First row from left to right: original scene, reconstruction using initialized pose and mean categorical object shape, reconstruction using optimized pose and shape with fine level residual only, reconstruction using optimized pose and shape with both coarse and fine level residuals. Second row from left to right: original scene with bookshelves and tables highlighted in light blue and beige, the rest are reconstructions overlaid with object point clouds and added pseudo points.}
    \label{fig:scannet_qualitative_0208_00}
\end{figure*}

\subsection*{Pose Estimation Metric}

This section presents the metric used to evaluate the object pose, which follows Scan2CAD~\cite{avetisyan2019scan2cad}. 
We introduce the details on how to decompose a pose $\bfT \in \text{SIM}(3)$ into rotation $\bfq$, translation $\bfp$ and scale $\bfs$ and the error functions for each element separately.
For rotation and scale, $\bfR_s = \bfP\bfT\bfP^\top$:
\begin{equation}
\label{eq:pose_error}
\begin{aligned}
s_1 &= \| \bfR_s\bfe_1 \|_2 \quad 
s_2 = \| \bfR_s\bfe_2 \|_2 \quad 
s_3 = \| \bfR_s\bfe_3 \|_2, \\
\bfR\bfe_1 &= \frac{\bfR_s\bfe_1}{s_1} \quad 
\bfR\bfe_2 = \frac{\bfR_s\bfe_2}{s_2}
\quad 
\bfR\bfe_3 = \frac{\bfR_s\bfe_3}{s_3}. 
\end{aligned}
\end{equation}
Suppose $\boldsymbol{R}=\left\{m_{i j}\right\}, i, j \in[1,2,3]$, we transform it to quaternion $\bfq$ by 
\begin{equation}
\scaleMathLine[0.9]
{
\begin{aligned}
q_{0}=\frac{\sqrt{\operatorname{tr}(R)+1}}{2}, q_{1}=\frac{m_{23}-m_{32}}{4 q_{0}}, q_{2}=\frac{m_{31}-m_{13}}{4 q_{0}}, q_{3}=\frac{m_{12}-m_{21}}{4 q_{0}}. 
\end{aligned}
}
\end{equation}
Suppose the prediction and groundtruth are $\bfq_{pred}, \bfq_{gt}$, we compute the difference by 
\begin{equation}
\begin{aligned}
e_{\text{SO(3)}}(\bfq,\hat{\bfq}) := 2 \arccos (| \bfq_{gt}^\top \bfq_{pred} |). 
\end{aligned}
\end{equation}
Translation is $\bfp = \bfT[1:3, 4]$, and we compare the difference between prediction and groundtruth by 
\begin{equation}
\| \bfp_{pred} - \bfp_{gt} \|_2. 
\end{equation}
For scale percentage error, we compute it by 
\begin{equation}
100\times | \frac{1}{3} \sum_{i=1}^{3} \bar{s}_i - 1 |,
\end{equation}
where $\bar{s}_i = \frac{s_{pred}}{s_{gt}}$ for each of $s_1, s_2, s_3$ recovered from the $\text{SIM}(3)$ matrix. 

\subsection*{Timing}

\begin{table}[tph!]
    \centering
    \caption{ELLIPSDF timing breakdown (sec)}
    \label{tab:time}
    % \vspace*{-1ex}
    \scalebox{0.78}{
        \begin{tabular}{c|c|c|c|c} % <-- Alignments: 1st column left, 2nd middle and 3rd right, with vertical lines in between
        \hline
        Init & Latent Code Opt & SIM(3) Opt & SDF Decoding & Meshing \\
        \hline
        0.04 & 0.13 & 0.58 & 1.38 & 2.34 \\
        \hline
        \end{tabular}}
    % \vspace*{-1.2ex}
\end{table}

Timing for one instance is provided in Table~\ref{tab:time}. \textit{Init} is the pose initialization in (14) for 100 views. \textit{Latent Code Opt} and \textit{SIM(3) Opt} are a single SGD step with respect to $\delta \bfz$ and $\bfT$ respectively using 10000 points as batch size. \textit{SDF Decoding} and \textit{Meshing} are optional steps that generate SDF predictions over $256^3$ points and apply Marching Cubes to generate a mesh. Our approach does not currently operate in real-time but it is more efficient than existing work. We will investigate how to accelerate the current slow python SIM(3) optimization.

