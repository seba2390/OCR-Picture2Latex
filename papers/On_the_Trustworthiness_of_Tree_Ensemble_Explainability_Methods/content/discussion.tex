\vspace{-.2cm}
\section{Discussion} \label{sec:discussion}
\vspace{-.2cm}
We set out to investigate the accuracy and stability of global feature importances for tree-based ensemble methods, such as random forest, gradient boosting machine, and XGBoost. We mainly look at two feature importance methods \emph{gain}, and SHAP. For both of these methods, we evaluate the accuracy in a simulated environment where true coefficients are known with and without noisy inputs. We also evaluate the stability of these methods in two directions, that is
 \begin{enumerate*}
    \item when inputs are perturbed, and
    \item when model settings are perturbed, either by initializing with a different random seed or by optimizing their hyperparameters with a different random seed.
\end{enumerate*}

\paragraph{Accuracy Analysis.} We find that SHAP tends to be better at accurately identifying top features compared to gain, although the overall accuracy of both is quite low especially when considering the ordering of all the features. 

\paragraph{Stability Analysis.} 
% In terms of stability, we find that when the model settings are perturbed by optimizing hyperparameters with different random seeds, correlation decreases. %to around 70-80\%.
% We find that SHAP is more stable compared to gain for XGBoost when models are being trained with different hyperparameter settings. However, we do not find any uplift on stability with SHAP for random forest and gradient boosting. This is surprising because both XGBoost and sklearn's gradient boosting are implementations of gradient boosting ensemble. This difference might be due to the inherent implementation of XGBoost that is slightly different than gradient boosting as discussed in Appendix \ref{appendix:xgboost_deterministic}. When we introduce noise to the inputs, we find that SHAP is more stable compared to gain for smaller noise. This uplift is more apparent for XGBoost, but also present for gradient boosting and random forest. However, as the level of noise increases, the uplift on stability from SHAP decreases, and both gain and SHAP feature importances' correlations drop significantly. In summary, we find that both gain and SHAP lack stability especially in datasets with high number of features and with the presence of noise.

%In summary, we find that SHAP is either equally or more stable when compared with gain, especially for XGBoost. However, both gain and SHAP feature importances decrease in stability w
% \vspace{.5cm}
In our experiments, we find that SHAP is either equally or more stable when compared with gain. This is especially interesting as both gain and (Tree) SHAP feature importances investigated here use the innate structure of the trees. The difference lies on the fact that gain measures the feature's contribution to accuracy improvements or decreasing of uncertainty/variance whereas SHAP measures the feature's contribution to the predicted output. 
 
%\paragraph{stability of (Tree) SHAP depends on the model.} Previous research has shown how (Kernel) SHAP as a local importance is not stable \cite{alvarez2018robustness}. In our study, we also find that (Tree) SHAP as a local feature importance is not as stable as their aggregation for a global feature importance (See Section \ref{subsec:localvsglobal}). % Also noteworthy is that Alvarez et al. investigated Kernel SHAP, the original implementation for SHAP that uses a surrogate model to estimate feature importance. In this study, we use Tree SHAP that captures importance as recorded by the tree structure, and therefore, its stability depends on the model stability as discussed by Lundberg \cite{lundberg_github_discussion}. 

% \paragraph{Using SHAP for global feature importances.} Previous research has shown how (Kernel) SHAP as a local importance is not stable \cite{alvarez2018robustness}. In our study, we also find that (Tree) SHAP as a local feature importance is not as stable as their aggregation for a global feature importance (See Section \ref{subsec:localvsglobal}). It may be controversial to use local feature importances as a global feature importance. However, this idea is not new as gain feature importance is also technically measured on the training data. Gain feature importance measures the contribution of the feature in improving the accuracy or reducing uncertainty. This measure of accuracy or uncertainty is calculated on the training data. Therefore, in a way, using local feature importances as a global feature importance is not controversial as long as it is summarized across all the training data.

\paragraph{Future Work.} There has been recent work on extending Shapley values to other cooperative game theory algorithms, such as the core \cite{yan2020if}. We will investigate this approach when a public implementation of this algorithm becomes available. In this study, we mostly focus on the stability of global features importance across the same model trained with perturbed hyperparameters/random seeds or inputs. Dong and Rudin recently suggest the idea of using a variable cloud importance, capturing the many good (but not necessarily the same) explanations coming from a group of models with almost equal performance \cite{dong2019variable}. In our future work, we will investigate the stability and usability of this methodology. We will also extend our analysis to new scenarios and datasets.  %our investigation on stability still adds value to this idea. 

% \textcolor{red}{In our experiments, we use synthetic data to simulate linear relationship among the features and incorporate real-world datasets to further validate our findings for cases when we have non-linear and complex interactions in the data. For future work, a closer examination on complex relationships in the data, e.g. non-linearity, interaction among features, and missing data, can be explored.}

\paragraph{Conclusion.} We investigate the accuracy and stability of global feature importances for tree ensemble methods. We find that even though SHAP in many cases can be more stable than gain feature importance, both methods still have limitations in terms of accuracy and stability and more work needs to be done to make them trustworthy. We hope that our paper will continue propel the discussion for trustworthy global feature importances and for the community to investigate this more thoroughly.