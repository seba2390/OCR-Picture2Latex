\vspace{-.5cm}
\begin{abstract}
The recent increase in the deployment of machine learning models in critical domains such as healthcare, criminal justice, and finance has highlighted the need for trustworthy methods that can explain these models to stakeholders. Feature importance methods (e.g. gain and SHAP) are among the most popular explainability methods used to address this need. For any explainability technique to be trustworthy and meaningful, it has to provide an explanation that is accurate and stable. Although the stability of local feature importance methods (explaining individual predictions) has been studied before, there is yet a knowledge gap about the stability of global features importance methods (explanations for the whole model). Additionally, there is no study that evaluates and compares the accuracy of global feature importance methods with respect to feature ordering.
In this paper, we evaluate the accuracy and stability of global feature importance methods through comprehensive experiments done on simulations as well as four real-world datasets. 
We focus on tree-based ensemble methods as they are used widely in industry and measure the accuracy and stability of explanations under two scenarios: 
\begin{enumerate*}
    \item when inputs are perturbed
    \item when models are perturbed.
\end{enumerate*} 
Our findings provide a comparison of these methods under a variety of settings and shed light on the limitations of global feature importance methods by indicating their lack of accuracy with and without noisy inputs, as well as their lack of stability with respect to:
\begin{enumerate*}
    \item increase in input dimension or noise in the data;
    \item perturbations in models initialized by different random seeds or hyperparameter settings.
\end{enumerate*} \\\\
This paper is a pre-published version of the original CD-MAKE 2021 publication: \url{https://doi.org/10.1007/978-3-030-84060-0\_19}.

\keywords{Explainability  \and Trustworthiness \and Tree ensemble.}

% Previous submission's 2nd half of abstract
% Stability is a crucial requirement for any explainability technique that aims to be trustworthy and meaningful. Although the stability of local feature importance methods (explaining individual predictions) has been studied before, there is yet a knowledge gap about the stability of global features importance methods (explanations for the whole model). In this paper, we evaluate the stability of global feature importance methods through comprehensive experiments done on simulations as well as four real-world datasets. More specifically, we focus on tree-based ensemble methods as they are used widely in industry and measure the stability of explanations under two scenarios: 
% \begin{enumerate*}
%     \item when inputs are perturbed
%     \item when models are perturbed. 
% \end{enumerate*}
% Our findings shed light on the limitations of global feature importance methods by indicating their lack of stability with respect to:
% \begin{enumerate*}
%     \item increase in input dimension or noise in the data;
%     \item perturbations in models initialized by different random seeds or hyperparameter settings.
% \end{enumerate*} Furthermore, we study the differences between these methods under various circumstances and determine in what situations does one technique generate more stable feature importances than the other. 

% Rough notes
%provide a rigorous comparison between these methods and when one technique generates more stable explanation

% This knowledge gap needs to be addressed as these methods are being used increasingly in industry and lack of stable. Evaluating the stability and stability of these methods is crucial as they are utilized increasingly for various purposes such as sensitivity analysis, model debugging, and model auditing. 

% In some contexts, non-stable explanations can constitute a moral hazard. 

% Many explainable methods are available out there to assess feature importance for trees. However, how do we decide which are the most suitable in our use case? One property is that we need to find a measure that is stable across different hyperparameters of the tree ensemble, i.e. when trained on the same data with the same algorithm, despite differing hyperparameters, the feature importance should still be consistent assuming the performance is similar. We found that gain can be an unstable measure at certain conditions, whereas using SHAP and summarizing it across the training data can give a more stable explanation.

\end{abstract}