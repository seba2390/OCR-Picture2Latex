 \appendix
%  \section{Accuracy of Feature Importances} \label{appendix:correctness_all}
%  Figure \ref{fig:correctness_all} shows how SHAP has a slightly higher proportion of ranking top features correctly in both experiments with and without noise. However, at higher number of features, both gain and SHAP can barely rank top features correctly (See Figure \ref{fig:correctness_all100}). The correlation of gain and SHAP feature importances to the true coefficients can be found in Figure \ref{fig:correctness_corr_all}.
 
%  \begin{figure*}[h]
%  \centering
%  Model Type: XGBoost \\ 
%   No noise added \hspace{6cm} Low noise added \\ 
%   \includegraphics[width=0.4\linewidth]{images/simulation_correctness/all_noise0_XGB_HP_features5.pdf}
%   \includegraphics[width=0.4\linewidth]{images/simulation_correctness/all_noise0.5_XGB_HP_features5.pdf}
  
%  Model Type: Gradient Boosting \\ 
%   No noise added \hspace{6cm} Low noise added \\ 
%   \includegraphics[width=0.4\linewidth]{images/simulation_correctness/all_noise0_GB_HP_features5.pdf}
%   \includegraphics[width=0.4\linewidth]{images/simulation_correctness/all_noise0.5_GB_HP_features5.pdf}
  
%  Model Type: Random Forest \\ 
%   No noise added \hspace{6cm} Low noise added \\ 
%   \includegraphics[width=0.4\linewidth]{images/simulation_correctness/all_noise0_RF_HP_features5.pdf}
%   \includegraphics[width=0.4\linewidth]{images/simulation_correctness/all_noise0.5_RF_HP_features5.pdf}
  
%   \includegraphics[width=0.2\linewidth]{images/simulation_correctness/legend.pdf}
  
%   \caption{Proportion of top 3 features being ranked correctly in synthetic data (\# total features: 5) across models without additional noise to input (left) and with low level of noise added to input (right) across iterations. SHAP feature importances correctly rank top features in more iterations than gain feature importances, as shown by the slightly higher level of blue region. Green region indicates proportion of features incorrectly ranked although still considered within top 3 features, and yellow region indicates proportion of features incorrectly ranked and not considered within top 3 features by the feature importance method.}
%   \Description{Described fully in the caption.}
%   \label{fig:correctness_all}
% \end{figure*}

%  \begin{figure*}[h]
%   \centering
%   Model Type: XGBoost \\ 
%   No noise added \hspace{6cm} Low noise added \\ 
%   \includegraphics[width=0.4\linewidth]{images/simulation_correctness/all_noise0_XGB_HP_features100.pdf}
%   \includegraphics[width=0.4\linewidth]{images/simulation_correctness/all_noise0.5_XGB_HP_features100.pdf}
  
%   Model Type: Gradient Boosting Machine \\ 
%   No noise added \hspace{6cm} Low noise added \\ 
%   \includegraphics[width=0.4\linewidth]{images/simulation_correctness/all_noise0_GB_HP_features100.pdf}
%   \includegraphics[width=0.4\linewidth]{images/simulation_correctness/all_noise0.5_GB_HP_features100.pdf}
  
%   Model Type: Random Forest \\ 
%   No noise added \hspace{6cm} Low noise added \\ 
%   \includegraphics[width=0.4\linewidth]{images/simulation_correctness/all_noise0_RF_HP_features100.pdf}
%   \includegraphics[width=0.4\linewidth]{images/simulation_correctness/all_noise0.5_RF_HP_features100.pdf}
  
%   \includegraphics[width=0.2\linewidth]{images/simulation_correctness/legend.pdf}
  
%   \caption{Proportion of top 10 features being ranked correctly in synthetic data (\# total features: 100) without additional noise to input (left) and with low level of noise added to input (right) across iterations. SHAP feature importances correctly rank top features in more iterations than gain feature importances, as shown by the slightly higher level of blue region for training without perturbed input. Green region indicates proportion of features incorrectly ranked although still considered within top 10 features, and yellow region indicates proportion of features incorrectly ranked and not considered within top 10 features by the feature importance method.}
%   \Description{Described fully in the caption.}
%   \label{fig:correctness_all100}
% \end{figure*}

% \begin{figure*}[h]
%   \centering
%   Model Type: XGBoost \\ 
%   %%Pearson Correlation \hspace{4cm} Spearman Correlation \\ 
% %   \includegraphics[width=0.4\linewidth]{images/simulation/all_noise0_XGB_HP_correctpearson.pdf}
%   \includegraphics[width=0.4\linewidth]{images/simulation/all_noise0_XGB_HP_correctspearman.pdf}
  
%   Model Type: Gradient Boosting Machine \\ 
%   %Pearson Correlation \hspace{4cm} Spearman Correlation \\ 
% %   \includegraphics[width=0.4\linewidth]{images/simulation/all_noise0_GB_HP_correctpearson.pdf}
%   \includegraphics[width=0.4\linewidth]{images/simulation/all_noise0_GB_HP_correctspearman.pdf}
  
%   Model Type: Random Forest \\ 
%   %Pearson Correlation \hspace{4cm} Spearman Correlation \\ 
% %   \includegraphics[width=0.4\linewidth]{images/simulation/all_noise0_RF_HP_correctpearson.pdf}
%   \includegraphics[width=0.4\linewidth]{images/simulation/all_noise0_RF_HP_correctspearman.pdf}
  
%   \includegraphics[width=0.1\linewidth]{images/simulation/legend.png}
  
%   \caption{Correlation of gain and SHAP feature importances with the true coefficients for XGBoost, gradient boosting, and random forest models without additional noise to input. Even at low number of features, the correlation is only around 0.4 while it reduced to 0.3 with higher number of features.}
%   \Description{Described fully in the caption.}
%   \label{fig:correctness_corr_all}
% \end{figure*}

%  \section{Stability of Feature Importances When Gradient Boosting Machine and Random Forest Models Are Perturbed} \label{appendix:model_perturbation_gb_rf}
 
%  For gradient boosting machines and random forest, we find that both gain feature importances and SHAP are equally as stable when the models are perturbed (See Figure \ref{fig:diff_seed_gbm_rf} for correlations of feature importances and Figure \ref{fig:pred_corr_gb_rf} for correlations of the predicted output).

%  \begin{figure*}[h]
%   \centering
%   Model Type: Gradient Boosting Machine \\
%   Perturbation type: Same hyperparameter settings, different random seeds \\
%   %Pearson Correlation \hspace{4cm} Spearman Correlation \\ 
% %   \includegraphics[width=0.4\linewidth]{images/simulation/all_noise0_GB_NOHP_pearsoncorr.pdf}
%   \includegraphics[width=0.4\linewidth]{images/simulation/all_noise0_GB_NOHP_spearmancorr.pdf}
  
%   Perturbation type: Different hyperparameter settings from optimization with different random seeds\\
%   %Pearson Correlation \hspace{4cm} Spearman Correlation \\ 
% %   \includegraphics[width=0.4\linewidth]{images/simulation/all_noise0_GB_HP_pearsoncorr.pdf}
%   \includegraphics[width=0.4\linewidth]{images/simulation/all_noise0_GB_HP_spearmancorr.pdf}
  
%   Model Type: Random Forest \\
%   Perturbation type: Same hyperparameter settings, different random seeds \\
%   %Pearson Correlation \hspace{4cm} Spearman Correlation \\ 
% %   \includegraphics[width=0.4\linewidth]{images/simulation/all_noise0_RF_NOHP_pearsoncorr.pdf}
%   \includegraphics[width=0.4\linewidth]{images/simulation/all_noise0_RF_NOHP_spearmancorr.pdf}
  
%   Perturbation type: Different hyperparameter settings from optimization with different random seeds\\
%   %Pearson Correlation \hspace{4cm} Spearman Correlation \\ 
% %   \includegraphics[width=0.4\linewidth]{images/simulation/all_noise0_RF_HP_pearsoncorr.pdf}
%   \includegraphics[width=0.4\linewidth]{images/simulation/all_noise0_RF_HP_spearmancorr.pdf}
  
  
%   \includegraphics[width=0.1\linewidth]{images/simulation/legend.png}
%   \caption{Correlation of feature importances (Blue: gain, Orange: SHAP) for gradient boosting and random forest models trained on synthetic data  with model perturbations across different number of features. SHAP and gain are equally stable.}
%   %\Description{Fully described in the caption.}
%   \label{fig:diff_seed_gbm_rf}
% \end{figure*}

%  \begin{figure*}[!htb]
%   \centering
%   %Correlation across prediction in models trained with: \\ 
%   Perturbation type: Same hyperparameter settings, different random seeds \\
%   Gradient Boosting Machine \hspace{4cm} Random Forest\\ 
%   \includegraphics[width=0.4\linewidth]{images/simulation/all_noise0_GB_NOHP_predcorrelation.pdf}
%   \includegraphics[width=0.4\linewidth]{images/simulation/all_noise0_RF_NOHP_predcorrelation.pdf} \\
  
%   Perturbation type: Different hyperparameter settings from optimization with different random seeds\\
%   Gradient Boosting Machine \hspace{4cm} Random Forest\\ 
%   \includegraphics[width=0.4\linewidth]{images/simulation/all_noise0_GB_HP_predcorrelation.pdf}
%   \includegraphics[width=0.4\linewidth]{images/simulation/all_noise0_RF_HP_predcorrelation.pdf}
  
  
%   \includegraphics[width=0.2\linewidth]{images/simulation/legend_predcorrelation.pdf}
%   \caption{Correlation of predicted outputs in gradient boosting and random forest models trained on synthetic data  with model perturbations across different number of features. The predicted output of perturbed models are still highly correlated to those without perturbation.}
%   %\Description{A line plot showing how pearson and spearman correlation of the predicted outputs with XGBoost in experimental settings where models are being perturbed (with and without hyperparameter optimization) does not change across number of features. There's a slight decrease for spearman correlation for random forest with different seeds with increasing number of features.}
%   %\Description{Fully described in the caption.}
%   \label{fig:pred_corr_gb_rf}
% \end{figure*}
  
  
  \section{Determinism of XGBoost Feature Importances} \label{appendix:xgboost_deterministic}
 
  \begin{figure*}[h]
  \centering
  \includegraphics[width=0.49\linewidth]{images/stability_innate_importances/rf.png}
  \includegraphics[width=0.49\linewidth]{images/stability_innate_importances/gbm.png} \\
  \includegraphics[width=0.49\linewidth]{images/stability_innate_importances/xgb_without_feature_shuffle.png}
  \includegraphics[width=0.49\linewidth]{images/stability_innate_importances/xgb_with_feature_shuffle.png}
  
  \caption{These plots show the distribution of feature importances across 10 redundant features for random forest (top left), gradient boosting (top right), XGBoost (bottom left), and XGBoost with feature shuffling (bottom right). XGBoost by its implementation is more deterministic compared to other methods at assigning feature importance. For the same hyperparameter with different seeds, when the features are redundant, it will always pick the first feature in order. With feature shuffling though, we are able to break this pattern a little bit. }
%   \Description{Distribution of feature importances across 30 iterations for redundant features with random forest, gradient boosting, and XGBoost. We found that the distribution is pretty scattered for random forest and gradient boosting, but XGBoost always assigns all importance to the first feature without feature shuffling.}
  \label{fig:feature_imp_stability}
\end{figure*}

 
In this experiment, we simulate 1000 samples with 10 redundant features where each feature is equally important in predicting the target. Figure \ref{fig:feature_imp_stability} shows the distribution of the default feature importance in random forests, gradient boosting, and XGBoost across 30 iterations with different random seeds. As shown on the bottom left, XGBoost always assigns all importance to the first feature it saw no matter the random seed. When we shuffle the order of the features, we are able to break down this pattern (shown on bottom right). This is why on Figure \ref{fig:perturb_model}, there is a perfect correlation of importance for XGBoost initialized with different random seeds. With shuffled features, we still find SHAP to be more stable for XGBoost overall, although the correlation still decreases with higher number of features (See Figure \ref{fig:xgb_shuffled}).
 
 \begin{figure*}[h]
  \centering
  \scriptsize
  Perturbation: Input (Low noise) \hspace{2.2cm} Model (random seeds)\\
  %Pearson Correlation \hspace{4cm} Spearman Correlation\\
%   \includegraphics[width=0.4\linewidth]{images/simulation/all_noise0.5_XGB_NOHP_Shuffled_features__pearsoncorr.pdf}
  \includegraphics[width=0.4\linewidth]{images/simulation/all_noise0.5_XGB_NOHP_Shuffled_features__spearmancorr.pdf}
  %Pearson Correlation \hspace{4cm} Spearman Correlation\\
%   \includegraphics[width=0.4\linewidth]{images/simulation/all_noise0_XGB_NOHP_Shuffled_features__pearsoncorr.pdf}
  \includegraphics[width=0.1\linewidth]{images/simulation/legend.png}
  \includegraphics[width=0.4\linewidth]{images/simulation/all_noise0_XGB_NOHP_Shuffled_features__spearmancorr.pdf} \\
  
  Perturbation: Model (hyperparameters)\hspace{1.7cm} Model (hyperparameters) \& input (low noise) \\
%   \includegraphics[width=0.4\linewidth]{images/simulation/all_noise0_XGB_HP_Shuffled_features__pearsoncorr.pdf}
  \includegraphics[width=0.4\linewidth]{images/simulation/all_noise0_XGB_HP_Shuffled_features__spearmancorr.pdf}
  \hspace*{10ex}
  %Pearson Correlation \hspace{4cm} Spearman Correlation\\
%   \includegraphics[width=0.4\linewidth]{images/simulation/all_noise0.5_XGB_HP_Shuffled_features__pearsoncorr.pdf}
  \includegraphics[width=0.4\linewidth]{images/simulation/all_noise0.5_XGB_HP_Shuffled_features__spearmancorr.pdf}
  \caption{SHAP is more stable overall for XGBoost with shuffled features as can be seen on the plots above across input perturbation (low noise) experiments, model perturbations and both. Each row represents a different set of experiments with Spearman correlations of the default feature importance (Blue) and SHAP feature importance (Orange).}
%   \Description{Fully described in the captions}
  \label{fig:xgb_shuffled}

\end{figure*}

%   \section{Stability of Feature Importance When Inputs Are Perturbed Across Different Levels of Noise} \label{appendix:stability_model_inputs}
  
%  Figure \ref{fig:all_perturbed_input} displays the stability of gain and SHAP feature importance across all models for low to high level of noise. For low noise, SHAP is more stable for all models, but this effect is decreased as noise increases. We can replicate this same finding for certain datasets although not all (See Figure \ref{fig:real_world_noisy_inputs}). 
 
 
%  Figure \ref{fig:all_perturbed_input_model} shows that SHAP is more stable for all models for low noise perturbation to the input along with perturbation to the model, with a bolder effect for XGBoost. As the level of noise increases, this difference in stability between gain and SHAP decreases. In Figure \ref{fig:real_world_noisy_inputs_model}, we can see this slight effect replicated for Concrete, Forest Fire, and Auto MPG datasets for XGBoost, although not as much for other models. 


%  \begin{figure*}[ht]
%   \centering
%     Model Type: XGBoost \\
%         Low noise added to input \hspace{1cm} Medium noise \hspace{1.7cm} Large noise \\
%     % \includegraphics[width=0.3\linewidth]{images/simulation/all_noise0.5_XGB_NOHP_pearsoncorr.pdf}
%     % \includegraphics[width=0.3\linewidth]{images/simulation/all_noise1_XGB_NOHP_pearsoncorr.pdf}
%     % \includegraphics[width=0.3\linewidth]{images/simulation/all_noise2_XGB_NOHP_pearsoncorr.pdf}
    
%     \includegraphics[width=0.31\linewidth]{images/simulation/all_noise0.5_XGB_NOHP_spearmancorr.pdf}
%     \includegraphics[width=0.31\linewidth]{images/simulation/all_noise1_XGB_NOHP_spearmancorr.pdf}
%     \includegraphics[width=0.31\linewidth]{images/simulation/all_noise2_XGB_NOHP_spearmancorr.pdf}
    
%     Model Type: Gradient Boosting \\
%         Low noise added to input \hspace{1cm} Medium noise \hspace{1.7cm} Large noise \\
%     %  \includegraphics[width=0.3\linewidth]{images/simulation/all_noise0.5_GB_NOHP_pearsoncorr.pdf}
%     % \includegraphics[width=0.3\linewidth]{images/simulation/all_noise1_GB_NOHP_pearsoncorr.pdf}
%     % \includegraphics[width=0.3\linewidth]{images/simulation/all_noise2_GB_NOHP_pearsoncorr.pdf}
    
%     \includegraphics[width=0.31\linewidth]{images/simulation/all_noise0.5_GB_NOHP_spearmancorr.pdf}
%     \includegraphics[width=0.31\linewidth]{images/simulation/all_noise1_GB_NOHP_spearmancorr.pdf}
%     \includegraphics[width=0.31\linewidth]{images/simulation/all_noise2_GB_NOHP_spearmancorr.pdf}
    
%     Model Type: Random Forest \\
%     Low noise added to input \hspace{1cm} Medium noise \hspace{1.7cm} Large noise \\
%     % \includegraphics[width=0.3\linewidth]{images/simulation/all_noise0.5_RF_NOHP_pearsoncorr.pdf}
%     % \includegraphics[width=0.3\linewidth]{images/simulation/all_noise1_RF_NOHP_pearsoncorr.pdf}
%     % \includegraphics[width=0.3\linewidth]{images/simulation/all_noise2_RF_NOHP_pearsoncorr.pdf}
    
%     \includegraphics[width=0.31\linewidth]{images/simulation/all_noise0.5_RF_NOHP_spearmancorr.pdf}
%     \includegraphics[width=0.31\linewidth]{images/simulation/all_noise1_RF_NOHP_spearmancorr.pdf}
%     \includegraphics[width=0.31\linewidth]{images/simulation/all_noise2_RF_NOHP_spearmancorr.pdf}
    
%     \includegraphics[width=0.1\linewidth]{images/simulation/legend.png}
    
%   \caption{Correlation of feature importances (Blue: gain, Orange: SHAP) for models trained with input perturbation on simulation. SHAP is slightly more stable than gain at low level of noise but are comparable as noise increases.}
%   %\Description{Fully described in the caption.}
%   \label{fig:all_perturbed_input}
% \end{figure*}

%  \begin{figure*}[h]
%   \centering
%   Dataset: Concrete \\
%     XGBoost \hspace{1.7cm} Gradient Boosting \hspace{1cm} Random Forest \\
% %   \includegraphics[width=0.2\linewidth]{images/concrete/all_noise0.5_XGB_NOHP_pearsoncorr.pdf}
% %   \includegraphics[width=0.2\linewidth]{images/concrete/all_noise0.5_GB_NOHP_pearsoncorr.pdf}
% %   \includegraphics[width=0.2\linewidth]{images/concrete/all_noise0.5_RF_NOHP_pearsoncorr.pdf}
  
%   \includegraphics[width=0.31\linewidth]{images/concrete/all_noise0.5_XGB_NOHP_spearmancorr.pdf}
%   \includegraphics[width=0.31\linewidth]{images/concrete/all_noise0.5_GB_NOHP_spearmancorr.pdf}
%   \includegraphics[width=0.31\linewidth]{images/concrete/all_noise0.5_RF_NOHP_spearmancorr.pdf}
  
%   Dataset: Forest Fire \\
%     XGBoost \hspace{1.7cm} Gradient Boosting \hspace{1cm} Random Forest \\
% %   \includegraphics[width=0.2\linewidth]{images/forest_fire/all_noise0.5_XGB_NOHP_pearsoncorr.pdf}
% %   \includegraphics[width=0.2\linewidth]{images/forest_fire/all_noise0.5_GB_NOHP_pearsoncorr.pdf}
% %   \includegraphics[width=0.2\linewidth]{images/forest_fire/all_noise0.5_RF_NOHP_pearsoncorr.pdf}
  
%   \includegraphics[width=0.31\linewidth]{images/forest_fire/all_noise0.5_XGB_NOHP_spearmancorr.pdf}
%   \includegraphics[width=0.31\linewidth]{images/forest_fire/all_noise0.5_GB_NOHP_spearmancorr.pdf}
%   \includegraphics[width=0.31\linewidth]{images/forest_fire/all_noise0.5_RF_NOHP_spearmancorr.pdf}
  
%   Dataset: Auto MPG \\
%     XGBoost \hspace{1.7cm} Gradient Boosting \hspace{1cm} Random Forest \\
% %   \includegraphics[width=0.2\linewidth]{images/auto_mpg/all_noise0.5_XGB_NOHP_pearsoncorr.pdf}
% %   \includegraphics[width=0.2\linewidth]{images/auto_mpg/all_noise0.5_GB_NOHP_pearsoncorr.pdf}
% %   \includegraphics[width=0.2\linewidth]{images/auto_mpg/all_noise0.5_RF_NOHP_pearsoncorr.pdf}
  
%   \includegraphics[width=0.31\linewidth]{images/auto_mpg/all_noise0.5_XGB_NOHP_spearmancorr.pdf}
%   \includegraphics[width=0.31\linewidth]{images/auto_mpg/all_noise0.5_GB_NOHP_spearmancorr.pdf}
%   \includegraphics[width=0.31\linewidth]{images/auto_mpg/all_noise0.5_RF_NOHP_spearmancorr.pdf}
  
%   Dataset: Company Finance \\
%      XGBoost \hspace{1.7cm} Gradient Boosting \hspace{1cm} Random Forest \\
% %   \includegraphics[width=0.2\linewidth]{images/spring/all_noise0.5_XGB_NOHP_pearsoncorr.pdf}
% %   \includegraphics[width=0.2\linewidth]{images/spring/all_noise0.5_GB_NOHP_pearsoncorr.pdf}
% %   \includegraphics[width=0.2\linewidth]{images/spring/all_noise0.5_RF_NOHP_pearsoncorr.pdf}
  
%   \includegraphics[width=0.31\linewidth]{images/spring/all_noise0.5_XGB_NOHP_spearmancorr.pdf}
%   \includegraphics[width=0.31\linewidth]{images/spring/all_noise0.5_GB_NOHP_spearmancorr.pdf}
%   \includegraphics[width=0.31\linewidth]{images/spring/all_noise0.5_RF_NOHP_spearmancorr.pdf}
%   \caption{Correlation of feature importances (Blue: gain, Orange: SHAP) for models trained with input perturbations (low noise) on real-world datasets.  SHAP and gain both lack stability overall although SHAP is slightly more stable for certain datasets. }
%   %\Description{Fully described in the caption.}
% \label{fig:real_world_noisy_inputs}
% \end{figure*}

%   \begin{figure*}[h]
%   \centering
%     XGBoost \\
%     Low noise added to input \hspace{2.5cm} Medium noise \hspace{2cm} Large noise \\
%     % \includegraphics[width=0.3\linewidth]{images/simulation/all_noise0.5_XGB_HP_pearsoncorr.pdf}
%     % \includegraphics[width=0.3\linewidth]{images/simulation/all_noise1_XGB_HP_pearsoncorr.pdf}
%     % \includegraphics[width=0.3\linewidth]{images/simulation/all_noise2_XGB_HP_pearsoncorr.pdf}
    
%     \includegraphics[width=0.3\linewidth]{images/simulation/all_noise0.5_XGB_HP_spearmancorr.pdf}
%     \includegraphics[width=0.3\linewidth]{images/simulation/all_noise1_XGB_HP_spearmancorr.pdf}
%     \includegraphics[width=0.3\linewidth]{images/simulation/all_noise2_XGB_HP_spearmancorr.pdf}
    
%     Gradient Boosting \\
%     Low noise added to input \hspace{2.5cm} Medium noise \hspace{2cm} Large noise \\
%     %  \includegraphics[width=0.3\linewidth]{images/simulation/all_noise0.5_GB_HP_pearsoncorr.pdf}
%     % \includegraphics[width=0.3\linewidth]{images/simulation/all_noise1_GB_HP_pearsoncorr.pdf}
%     % \includegraphics[width=0.3\linewidth]{images/simulation/all_noise2_GB_HP_pearsoncorr.pdf}
    
%     \includegraphics[width=0.3\linewidth]{images/simulation/all_noise0.5_GB_HP_spearmancorr.pdf}
%     \includegraphics[width=0.3\linewidth]{images/simulation/all_noise1_GB_HP_spearmancorr.pdf}
%     \includegraphics[width=0.3\linewidth]{images/simulation/all_noise2_GB_HP_spearmancorr.pdf}
    
%     Random Forest \\
%     Low noise added to input \hspace{2.5cm} Medium noise \hspace{2cm} Large noise \\
%     % \includegraphics[width=0.3\linewidth]{images/simulation/all_noise0.5_RF_HP_pearsoncorr.pdf}
%     % \includegraphics[width=0.3\linewidth]{images/simulation/all_noise1_RF_HP_pearsoncorr.pdf}
%     % \includegraphics[width=0.3\linewidth]{images/simulation/all_noise2_RF_HP_pearsoncorr.pdf}
    
%     \includegraphics[width=0.3\linewidth]{images/simulation/all_noise0.5_RF_HP_spearmancorr.pdf}
%     \includegraphics[width=0.3\linewidth]{images/simulation/all_noise1_RF_HP_spearmancorr.pdf}
%     \includegraphics[width=0.3\linewidth]{images/simulation/all_noise2_RF_HP_spearmancorr.pdf}
    
%     \includegraphics[width=0.1\linewidth]{images/simulation/legend.png}
    
    
%     \caption{Correlation of feature importances (Blue: gain, Orange: SHAP) for models trained with model and input perturbation on simulation. SHAP is slightly more stable than gain at low level of noise but both are comparably non-stable as noise increases.}
%   %\Description{Fully described in the caption.}
%   \label{fig:all_perturbed_input_model} 
% %   \Description{Line plots showing how SHAP has higher Pearson and Spearman correlations compared to the default feature importances at low and medium level of noise for XGBoost, and at low level of noise across all models, although very slight for gradient boosting and random forest.}
% \end{figure*}


% %  \begin{figure*}[h]
% %   \centering
% %   Dataset: Concrete \\
% %   XGBoost \hspace{2cm} Gradient Boosting \hspace{2cm} Random Forest \\
% % %   \includegraphics[width=0.2\linewidth]{images/concrete/all_noise0.5_XGB_HP_pearsoncorr.pdf}
% % %   \includegraphics[width=0.2\linewidth]{images/concrete/all_noise0.5_GB_HP_pearsoncorr.pdf}
% % %   \includegraphics[width=0.2\linewidth]{images/concrete/all_noise0.5_RF_HP_pearsoncorr.pdf}
  
% %   \includegraphics[width=0.2\linewidth]{images/concrete/all_noise0.5_XGB_NOHP_spearmancorr.pdf}
% %   \includegraphics[width=0.2\linewidth]{images/concrete/all_noise0.5_GB_NOHP_spearmancorr.pdf}
% %   \includegraphics[width=0.2\linewidth]{images/concrete/all_noise0.5_RF_NOHP_spearmancorr.pdf}
  
% %   Dataset: Forest Fire \\
% %     XGBoost \hspace{2cm} Gradient Boosting \hspace{2cm} Random Forest \\
% % %   \includegraphics[width=0.2\linewidth]{images/forest_fire/all_noise0.5_XGB_HP_pearsoncorr.pdf}
% % %   \includegraphics[width=0.2\linewidth]{images/forest_fire/all_noise0.5_GB_HP_pearsoncorr.pdf}
% % %   \includegraphics[width=0.2\linewidth]{images/forest_fire/all_noise0.5_RF_HP_pearsoncorr.pdf}
  
% %   \includegraphics[width=0.2\linewidth]{images/forest_fire/all_noise0.5_XGB_HP_spearmancorr.pdf}
% %   \includegraphics[width=0.2\linewidth]{images/forest_fire/all_noise0.5_GB_HP_spearmancorr.pdf}
% %   \includegraphics[width=0.2\linewidth]{images/forest_fire/all_noise0.5_RF_HP_spearmancorr.pdf}
  
% %   Dataset: Auto MPG \\
% %     XGBoost \hspace{2cm} Gradient Boosting \hspace{2cm} Random Forest \\
% % %   \includegraphics[width=0.2\linewidth]{images/auto_mpg/all_noise0.5_XGB_HP_pearsoncorr.pdf}
% % %   \includegraphics[width=0.2\linewidth]{images/auto_mpg/all_noise0.5_GB_HP_pearsoncorr.pdf}
% % %   \includegraphics[width=0.2\linewidth]{images/auto_mpg/all_noise0.5_RF_HP_pearsoncorr.pdf}
  
% %   \includegraphics[width=0.2\linewidth]{images/auto_mpg/all_noise0.5_XGB_HP_spearmancorr.pdf}
% %   \includegraphics[width=0.2\linewidth]{images/auto_mpg/all_noise0.5_GB_HP_spearmancorr.pdf}
% %   \includegraphics[width=0.2\linewidth]{images/auto_mpg/all_noise0.5_RF_HP_spearmancorr.pdf}
  
% %   Dataset: Company Finance \\
% %      XGBoost \hspace{2cm} Gradient Boosting \hspace{2cm} Random Forest \\
% % %   \includegraphics[width=0.2\linewidth]{images/spring/all_noise0.5_XGB_HP_pearsoncorr.pdf}
% % %   \includegraphics[width=0.2\linewidth]{images/spring/all_noise0.5_GB_HP_pearsoncorr.pdf}
% % %   \includegraphics[width=0.2\linewidth]{images/spring/all_noise0.5_RF_HP_pearsoncorr.pdf}
  
% %   \includegraphics[width=0.2\linewidth]{images/spring/all_noise0.5_XGB_HP_spearmancorr.pdf}
% %   \includegraphics[width=0.2\linewidth]{images/spring/all_noise0.5_GB_HP_spearmancorr.pdf}
% %   \includegraphics[width=0.2\linewidth]{images/spring/all_noise0.5_RF_HP_spearmancorr.pdf}
% %   \caption{Correlation of feature importances (Blue: gain, Orange: SHAP) for models trained with model perturbation (different hyperparameter) and input perturbations (low noise) on real-world datasets.  SHAP and gain both lack stability overall. }
% %   %\Description{Fully described in the caption.}
% % \label{fig:real_world_noisy_inputs_model}

% % \end{figure*}

% %  \section{Stability of Feature Importances When Data is Subsampled}  \label{appendix:subsample}
% %  In industry, it is often the case that there is a limited view to the data. Here we examine the stability of feature importances when only a subset of the data is available. We find that decreasing the view of the data can reduce the stability of the feature importances (compared to full view of the data) to 80\% for 80\% subsampling. 
 

% %  \begin{figure*}[ht]
% %   \centering
% %     Model Type: XGBoost \\
% %     Complete Data \hspace{2.5cm} 90\% Data \hspace{3cm} 80\% Data \\
% %     % \includegraphics[width=0.3\linewidth]{images/simulation/all_noise0_XGB_NOHP_pearsoncorr.pdf}
% %     % \includegraphics[width=0.3\linewidth]{images/simulation/all_noise0_subsample0.9_XGB_NOHP_pearsoncorr.pdf}
% %     % \includegraphics[width=0.3\linewidth]{images/simulation/all_noise0_subsample0.8_XGB_NOHP_pearsoncorr.pdf}
    
% %     \includegraphics[width=0.3\linewidth]{images/simulation/all_noise0_XGB_NOHP_spearmancorr.pdf}
% %     \includegraphics[width=0.3\linewidth]{images/simulation/all_noise0_subsample0.9_XGB_NOHP_spearmancorr.pdf}
% %     \includegraphics[width=0.3\linewidth]{images/simulation/all_noise0_subsample0.8_XGB_NOHP_spearmancorr.pdf}
    
% %     Model Type: Gradient Boosting \\
% %     Complete Data \hspace{2.5cm} 90\% Data \hspace{3cm} 80\% Data \\
% %     %  \includegraphics[width=0.3\linewidth]{images/simulation/all_noise0_GB_NOHP_pearsoncorr.pdf}
% %     % \includegraphics[width=0.3\linewidth]{images/simulation/all_noise0_subsample0.9_GB_NOHP_pearsoncorr.pdf}
% %     % \includegraphics[width=0.3\linewidth]{images/simulation/all_noise0_subsample0.8_GB_NOHP_pearsoncorr.pdf}
    
% %     \includegraphics[width=0.3\linewidth]{images/simulation/all_noise0_GB_NOHP_spearmancorr.pdf}
% %     \includegraphics[width=0.3\linewidth]{images/simulation/all_noise0_subsample0.9_GB_NOHP_spearmancorr.pdf}
% %     \includegraphics[width=0.3\linewidth]{images/simulation/all_noise0_subsample0.8_GB_NOHP_spearmancorr.pdf}
    
% %     Model Type: Random Forest \\
% %     Complete Data \hspace{2.5cm} 90\% Data \hspace{3cm} 80\% Data \\
% %     % \includegraphics[width=0.3\linewidth]{images/simulation/all_noise0_RF_NOHP_pearsoncorr.pdf}
% %     % \includegraphics[width=0.3\linewidth]{images/simulation/all_noise0_subsample0.9_RF_NOHP_pearsoncorr.pdf}
% %     % \includegraphics[width=0.3\linewidth]{images/simulation/all_noise0_subsample0.8_RF_NOHP_pearsoncorr.pdf}
    
% %     \includegraphics[width=0.3\linewidth]{images/simulation/all_noise0_RF_NOHP_spearmancorr.pdf}
% %     \includegraphics[width=0.3\linewidth]{images/simulation/all_noise0_subsample0.9_RF_NOHP_spearmancorr.pdf}
% %     \includegraphics[width=0.3\linewidth]{images/simulation/all_noise0_subsample0.8_RF_NOHP_spearmancorr.pdf}
    
% %     \includegraphics[width=0.1\linewidth]{images/simulation/legend.png}
    
% % %   \caption{Input perturbation experiment: SHAP is more stable when low and medium levels of noise is added to the input for XGBoost (Rows 1 \& 2) and random forest (Rows 5 \& 6) as features increase. For gradient boosting (Rows 3 \& 4), SHAP is only more stable for low level of noise. Here, we show Pearson (odd rows) and Spearman (even rows) correlations of gain feature importances (Blue) and SHAP feature importances (Orange) across different levels of noise: low (Column 1), medium (Column 2), and high (Column 3).}
% % %   \Description{Line plots showing how SHAP has higher Pearson and Spearman correlations compared to gain feature importances at medium level of noise for XGBoost and random forest. For low level of noise, SHAP has higher correlations across all models. For high level of noise, SHAP and gain feature importances' stability decrease and perform almost equally.}
% %   \caption{Correlation of feature importances (Blue: gain, Orange: SHAP) for models trained with complete input and subsamples of the input. Subsampling the dataset reduces stability slightly.}
% %   %\Description{Fully described in the caption.}
% %   \label{fig:subsample_exp}
% \end{figure*}