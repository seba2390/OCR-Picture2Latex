\vspace{-.5cm}
\section{Introduction} \label{sec:intro}
\vspace{-.2cm}
Owing to the success and promising results achieved in supervised machine learning (ML) paradigm, there has been a growing interest in leveraging ML models in domains such as healthcare~\cite{asgarian2018hybrid,spann2020applying,yasodhara2021identifying}, criminal justice~\cite{rudin2019stop}, and finance~\cite{dixon2020machine}. As ML models become embedded into critical aspects of decision making, their successful adoption depends heavily on how well different stakeholders (e.g. user or developer of ML models) can understand and trust their predictions~\cite{asgarian2019limitations,christodoulakis2017barriers,doshi2017towards,lipton2018mythos,selbst2018intuitive}. As a result, there has been a recent surge in making ML models worthy of human trust~\cite{wiens2019no} and researchers have proposed a variety of methods to explain ML models to stakeholders~\cite{bhatt2020explainable}. 
Among these methods, feature importance methods in particular have received a lot of attention and gained tremendous popularity in industry~\cite{bhatt2020explainable}. The explanations obtained by these methods lie in two categories: \begin{enumerate*} \item local explanations \item global explanations \end{enumerate*}. Local explanations explain how a particular prediction is derived from the given input data. %This is done essentially by constructing a surrogate model around the particular input that produces the expected output.
Global explanations, in contrast, provide a holistic view of what features are important across all predictions. %the whole range of predictions is generated from the whole range of inputs. 
Both explanation methods can be used for the purposes of model debugging, transparency, monitoring and auditing~\cite{bhatt2020explainable}. However, the trustworthiness and applicability of these explanations relies heavily on their accuracy and stability ~\cite{hancox2020robustness}.

Previously, Lundberg et al. \cite{NIPS2017_7062} assess the accuracy of feature importances by comparing them with human attributed importances. Ribeiro et al. \cite{ribeiro2016should} limits models to only use ten features from the input. Assuming the models would only pick the top ten important features, he then measures whether the selected features by the model are also captured by feature importances. Although both of these assessments capture whether important features are accurately identified, they do not measure the accuracy with respect to the relative ordering of features. We examine this with and without the presence of noisy inputs and use it to provide a comparison of different global feature importance methods.
%To the best of our knowledge, this is the first study to evaluate and compare the accuracy of global feature importance methods. 

In the explainability literature, various definitions are proposed for stability. Alvarez et al. \cite{alvarez2018robustness} define stability as being stable to local perturbations of the input, or in other words, similar inputs should not lead to significantly different explanations. Hancox-Li provides another definition for stability~\cite{hancox2020robustness}. He claims that stable explanations reflective of real patterns in the world are those that remain consistent over a set of equally well-performing models. Inspired by these definitions, we consider the following two scenarios to evaluate stability:  
\begin{enumerate*}
    \item local perturbations of the input
    \item perturbations of the models.
\end{enumerate*} 
We argue that stability with respect to these factors is essential to account for the inherent noisy nature of real-world data and to provide trustworthy explanations.
%to account for inherent noisy nature of real-world data and common changes that occur to machine learning models deployed in production.

The stability of local explainability methods under the first scenario has been studied before. For example, Alvarez et al.~\cite{alvarez2018robustness} show LIME~\cite{ribeiro2016should} and (Kernel) SHAP~\cite{NIPS2017_7062} lack stability for complex black-box models through conducting the following experiments. They slightly perturb the input values and find that the surrogate models and original black box models produce stable output values whereas the explanations provided by LIME and SHAP change drastically in response to the perturbations. Despite these thorough investigations conducted on the stability of local explainability methods, there is yet little understanding about the stability of global explainability methods. With these methods getting embedded into critical aspects of daily life (healthcare, criminal justice, and finance)%~\cite{wiens2019no, rudin2019stop, dixon2020machine}
, addressing this knowledge gap becomes crucial to avoid moral and ethical hazards~\cite{rudin2019stop}.

In this paper, we compare and evaluate the accuracy and stability of global feature explanation methods, gain and SHAP, through comprehensive experiments conducted on synthetic data and four real-world datasets. For this purpose, we use the following tree-based ensemble models as they are widely used in academia and industry: (1) random forest (2) gradient boosting machines~\cite{scikit-learn} and (3) XGBoost~\cite{Chen:2016:XST:2939672.2939785}. Our findings shed light on the limitations of the global explainability methods and show that they lack accuracy and become unstable when inputs or models are perturbed. % Moreover, we provide a precise comparison between these methods and recommendations on how to use them reliably in real-world settings. 
For the rest of this paper, we first review the methodologies used in our experiments under Section \ref{sec:background}. We then describe our experimental setup in Section \ref{sec:exp_setup}. Finally, we present and discuss our findings in Sections \ref{sec:results} and \ref{sec:discussion} respectively.

%To the best of our knowledge, we are the first to conduct a study on the stability of global explainability methods. 


% In industry, tree-based models like xgboost is prevalent due to its efficient computation runtime and on-par performance with deep learning methods. With trees, feature importance is usually calculated by ... . However, this importance does not tell us the direction of the affect or the local importance of each prediction. Thus, many people turned towards SHAP for local explanations. SHAP, although local explanations, can be used to summarize the whole model by averaging the importance across the whole training set. We found that in certain cases, this approach proves to be more stable, especially when dealing with different hyperparameters of tree ensembles.

% Here we set out to compare gain and (Tree) SHAP and its stability.

% To the best of our knowledge, we are the first to conduct a study of how explainability techniques are used by organizations that deploy ML models in their workflows. Our main contributions are threefold:


% Here in this paper, we would like to steer away from stability and instead look at a different aspect of stability, that is how stable are these importances across different tree structures, e.g. when the models are trained with different hyperparameters or have its hyperparameters optimized with a different seed? This is a very common case in production. We want to ensure that when the same model algo is trained on the same data, with just differing seeds, that the model is stable.

% We found that in this case, SHAP gives a more consistent value than gain. We think this is because SHAP values depend on the training data, not necessarily the shape of the trees. So, it’s more stable across tree structure.


% \textcolor{blue}{In this paper, we explore how organizations have deployed local explainability techniques so that we can observe which techniques work best in practice, report on the shortcomings of existing techniques, and recommend paths for future research. We focus specifically on local explainability techniques since these techniques explain individual predictions, making them typically the most relevant form of model transparency for end users.}

% \textcolor{blue}{Our study synthesizes interviews with roughly fifty people from approximately thirty organizations to understand which explain- ability techniques are used and how. We report trends from two sets of interviews and provide recommendations to organizations deploying explainability. To the best of our knowledge, we are the first to conduct a study of how explainability techniques are used by organizations that deploy ML models in their workflows. Our main contributions are threefold:• We interview around twenty data scientists, who are not currently using explainability tools, to understand their or- ganization’s needs for explainability.
% • We interview around thirty different individuals on how their organizations have deployed explainability techniques, reporting case studies and takeaways for each technique.
% • We suggest a framework for organizations to clarify their goals for deploying explainability.
% The rest of this paper is organized as follows:
% (1) We discuss the methodology of our survey in Section 2.
% (2) We summarize our overall findings in Section 3.
% (3) We detail how local explainability techniques are used at
% various organizations and discuss technique-specific take-
% aways in Section 4.
% (4) We develop a framework for establishing clear goals when
% deploying local explainability in Section 5.1 and discuss con- cerns of explainability in Section 5.2.}

% ===============================================================
% Outline of intro:
% - understanding a model's behaviour is critical
%     - explainability can be used for model monitoring across shifts, debugging, transparency, and auditing (cite: Explainable ML in deployment)
    
% - why global explainability
%     - work on investigating local explainability has been investigated a lot (cite: on stability of explainability), but there's lack of research for global explainability stability
%     - though it's reported that a lot of companies use local explainability most often (cite: explainable ML in deployment), we found that global explanation has its own usages of giving a holistic view of the model
%         - model debugging and understanding what's important to the model
    
        
% - why trees
%     - used a lot in industry (I think can cite: explainable ML in deployment)
%     - using simpler models to capture non-linearities can also make that model not too interpretable (cite: mythos of model interpretability

% - here we experiment using innate tree feature importances and SHAP

% Maybe:
% - why SHAP
%     - used a lot in industry (I think can cite: explainable ML in deployment)
    


% Understanding a model's behaviour is critical, especially in real-world settings, where a false prediction can have detrimental outcomes in the future. In recent years, there has been a lot of discussions and advancements on ways to explain ML models (LIME, SHAP, Anchor, ...). Out of these, SHAP stood out as one of the most easy to use and with its recent improvement on trees with polynomial time, allows for fast exact values without perturbation of data points. 

% In industry, tree-based models like xgboost is prevalent due to its efficient computation runtime and on-par performance with deep learning methods. With trees, feature importance is usually calculated by ... . However, this importance does not tell us the direction of the affect or the local importance of each prediction. Thus, many people turned towards SHAP for local explanations. SHAP, although local explanations, can be used to summarize the whole model by averaging the importance across the whole training set. We found that in certain cases, this approach proves to be more stable, especially when dealing with different hyperparameters of tree ensembles.

% Here we set out to compare gain and (Tree) SHAP and its stability.