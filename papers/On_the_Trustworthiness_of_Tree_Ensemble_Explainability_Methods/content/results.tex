\vspace{-.3cm}
\section{Results} \label{sec:results}
\vspace{-.3cm}
Here, we present our findings from the experiments described in Section~\ref{sec:exp_setup}. We first discuss the accuracy of gain and SHAP feature importances in Section \ref{results:accuracy}. We then dive into the stability of each feature importance method when inputs are perturbed and when models are perturbed in Section \ref{results:stability}. Finally, we present a summary of our findings in Section \ref{results:summary}.

\vspace{-.3cm}
\subsection{Accuracy of Gain and SHAP Feature Importances}
\label{results:accuracy}
% One of the most important attributes of feature importance is how well it captures the true underlying relationship of the input data. With synthetic data, we have access to the underlying feature importances that are usually hidden in real-world datasets. Through this, we examine how often gain and SHAP feature importances correctly rank top features. 
Table \ref{tab:correctness} demonstrate the accuracy of gain and SHAP for the top 3 features in synthetic data with a total of 5 features trained with XGBoost. The difference between SHAP and gain proportions are  highlighted beneath them. Orange indicates SHAP having a higher proportion and vice versa for blue. Models included in this experiment are highly predictive, with an average area under receiver operating curve (AUROC) of 92.6\% with standard deviation of 0.8\%.

% We calculate the proportion of the top 3 features when they are: 
% \begin{enumerate*}
%     \item ranked correctly across 50 simulation iterations (correct)
%     \item ranked incorrectly but considered in top 3 (incorrect\_but\_top)
%     \item ranked incorrectly and not considered in the top 3 (incorrect).
% \end{enumerate*}
Surprisingly, we find that the number of features ranked correctly is quite low for both methods even when there is no noise added to the input. For example, the rank \#1  feature is correctly ranked approximately 40\% of the time by both methods. Despite both SHAP and gain calculating feature importances from the same model, SHAP shows a slightly higher accuracy in ranking top features especially when noise is added into the input.

% Looking across all experiments, we find that in general, SHAP and gain feature importances are comparable in terms of correctness. However, for lower number of features as shown in Figure \ref{fig:correctness}, SHAP performs slightly better even when noise is added to the input. For a complete comparison across different models with varying number of features, refer to Appendix \ref{appendix:correctness_all}. 


\begin{table*}[t]
\centering
  \caption{Proportions of correct, incorrect\_but\_top, and incorrect ranking of the top 3 features on synthetic data (total features: 5) using XGBoost model across all experiment iterations. Proportions in each column add up to 1. Highlighted values indicate the difference between SHAP and gain proportions: orange when SHAP having higher proportion and blue otherwise.}
  \label{tab:correctness}
  \scriptsize
\begin{tabular}{l|llllll|llllll}
    \toprule
  Experiment setting:                    & \multicolumn{6}{c|}{No noise added to   input}                                     & \multicolumn{6}{c}{Low noise added to   input}                                    \\
Original feature rank: & \multicolumn{2}{c}{1}     & \multicolumn{2}{c}{2}     & \multicolumn{2}{c|}{3}     & \multicolumn{2}{c}{1}     & \multicolumn{2}{c}{2}     & \multicolumn{2}{c}{3}     \\
Feature importance method:                      & gain         & shap       & gain        & shap        & gain        & shap        & gain         & shap       & gain         & shap       & gain          & shap      \\
                      \midrule
\multirow{2}{*}{correct}               & 0.44         & 0.5        & 0.46        & 0.44        & 0.32        & 0.3         & 0.44         & 0.46       & 0.4          & 0.52       & 0.3           & 0.44      \\
                      & \multicolumn{2}{c}{\cellcolor{orange}0.06}  & \multicolumn{2}{c}{\cellcolor{babyblue}-0.02} & \multicolumn{2}{c|}{\cellcolor{babyblue}-0.02} & \multicolumn{2}{c}{\cellcolor{orange}0.02}  & \multicolumn{2}{c}{\cellcolor{orange}0.12}  & \multicolumn{2}{c}{\cellcolor{orange}0.14}  \\
\multirow{2}{*}{incorrect\_but\_top}   & 0.26         & 0.28       & 0.22        & 0.26        & 0.32        & 0.28        & 0.24         & 0.16       & 0.26         & 0.2        & 0.46          & 0.38      \\
                      & \multicolumn{2}{c}{\cellcolor{orange}0.02}  & \multicolumn{2}{c}{\cellcolor{orange}0.04}  & \multicolumn{2}{c|}{\cellcolor{babyblue}-0.04} & \multicolumn{2}{c}{\cellcolor{babyblue}-0.08} & \multicolumn{2}{c}{\cellcolor{babyblue}-0.06} & \multicolumn{2}{c}{\cellcolor{babyblue}-0.08}  \\
\multirow{2}{*}{incorrect}             & 0.3          & 0.22       & 0.32        & 0.3         & 0.36        & 0.42        & 0.32         & 0.38       & 0.34         & 0.28       & 0.24          & 0.18      \\
                      & \multicolumn{2}{c}{\cellcolor{babyblue}-0.08} & \multicolumn{2}{c}{\cellcolor{babyblue}-0.02} & \multicolumn{2}{c|}{\cellcolor{orange}0.06}  & \multicolumn{2}{c}{\cellcolor{orange}0.06}  & \multicolumn{2}{c}{\cellcolor{babyblue}-0.06} & \multicolumn{2}{c}{\cellcolor{babyblue}-0.06} \\
                      \bottomrule
\end{tabular}
\end{table*}
% \begin{figure*}[h]
%   \centering
%   %   \hspace{-12cm}\textbf{Your title}\par%\medskip
%   %\vspace{0.5cm}
%   \text{Proportion of correctly ranking top features when:}\par\medskip
%   \text{No noise added to input \hspace{4cm} Low noise added to input}\par
%   \includegraphics[width=0.4\linewidth]{images/simulation_correctness/all_noise0_XGB_HP_features5.pdf}
%   \includegraphics[width=0.4\linewidth]{images/simulation_correctness/all_noise0.5_XGB_HP_features5.pdf}
%   \includegraphics[width=0.19\linewidth]{images/simulation_correctness/legend.pdf}
 
%   \caption{Proportion of top 3 features being ranked correctly in synthetic data (\# total features: 5) for XGBoost model without additional noise to input (left) and with low level of noise added to input (right) across iterations. SHAP feature importances correctly rank top features in more iterations than gain feature importances, as shown by the slightly higher level of blue region. Green region indicates proportion of features incorrectly ranked although still considered within top 3 features, and yellow region indicates proportion of features incorrectly ranked and not considered within top 3 features by the feature importance method.}
% %   \Description{Fully described in the caption.}
%   \label{fig:correctness}
% \end{figure*}

To explicitly look at whether the feature importances provides an accurate ranking of all features, we further examine the Spearman correlation between the feature importances and the true coefficients. Figure \ref{fig:correctness_corr} shows the correlations in a noise-free scenario with increasing number of features. As demonstrated in this Figure, we find that gain and SHAP feature importances do not correlate well with the true coefficients (correlations range from 30-40\% and drops to around 20\% as the number of features increases). We observe a similar pattern across all other experimental settings (low-noised, medium-noised, or high-noised input).
%(Appendix \ref{appendix:correctness_all}). 

%\textcolor{red}{This leads us to conclude that while feature importances may be able to identify top features correctly (experiments shown that they're able to do so more than half of the time), it does not provide a good rank ordering for all of the features.}

 \begin{figure*}[h]
  \centering
  \vspace{0.2cm}
%  Pearson Correlation \hspace{4.5cm} Spearman Correlation \\
 % \includegraphics[width=0.4\linewidth]{images/simulation/all_noise0_XGB_HP_correctpearson.pdf}
  Model: \hspace{1cm} XGBoost \hspace{1.5cm} Gradient Boosting Machine \hspace{0.7cm} Random Forest \\ 
  \includegraphics[width=0.09\linewidth]{images/simulation/legend.png}
  \includegraphics[width=0.29\linewidth]{images/simulation/all_noise0_XGB_HP_correctspearman.pdf}
  \includegraphics[width=0.29\linewidth]{images/simulation/all_noise0_GB_HP_correctspearman.pdf}
  \includegraphics[width=0.29\linewidth]{images/simulation/all_noise0_RF_HP_correctspearman.pdf}
%   \includegraphics[width=0.4\linewidth]{images/simulation/all_noise0.5_XGB_HP_correctspearman.pdf}
 
  \caption{Spearman correlation of gain and SHAP feature importances (Blue: gain, Orange: SHAP) with the true coefficients with no noise added in simulation. Correlation is quite low across all settings.}%Even with a small number of features, the correlation is only around 0.4 and it reduces to around 0.2 as feature increases.}
  \label{fig:correctness_corr}
  %\vspace{0.3cm}
\end{figure*}

\vspace{-.3cm}
\subsection{Stability of Gain and SHAP Feature Importances}
\label{results:stability} 
In this section we evaluate the stability of feature importances when inputs and models are perturbed. In all of the following experiment settings, the predicted outputs from the perturbed models and the original models are highly correlated (an example for model perturbation is shown in Figure \ref{fig:pred_corr} for synthetic data). This ensures that our models have very similar performance and the results are minimally affected by discrepancies between model predictions.

% TODO update for all
 \begin{figure}[h]
  \centering
  %Correlation across prediction in models trained with: 
  %\vspace{0.5cm}
   \includegraphics[width=0.2\linewidth]{images/simulation/legend_predcorrelation.pdf} \\
  Perturbation: Random seeds \\
     XGBoost  \hspace{1.7cm} Gradient Boosting Machine \hspace{1cm} Random Forest \\ 
  \includegraphics[width=0.31\linewidth]{images/simulation/all_noise0_XGB_NOHP_predcorrelation.pdf}
   \includegraphics[width=0.31\linewidth]{images/simulation/all_noise0_GB_NOHP_predcorrelation.pdf}
    \includegraphics[width=0.31\linewidth]{images/simulation/all_noise0_RF_NOHP_predcorrelation.pdf}
      Perturbation: Hyperparameter settings \\
     XGBoost  \hspace{1.7cm} Gradient Boosting Machine \hspace{1cm} Random Forest \\ 
  \includegraphics[width=0.31\linewidth]{images/simulation/all_noise0_XGB_HP_predcorrelation.pdf}
   \includegraphics[width=0.31\linewidth]{images/simulation/all_noise0_GB_HP_predcorrelation.pdf}
    \includegraphics[width=0.31\linewidth]{images/simulation/all_noise0_RF_HP_predcorrelation.pdf}
  \caption{Correlation of predicted outputs in models trained on synthetic data with model perturbations across different number of features  (Blue: Pearson, Orange: Spearman correlation). The predicted output of perturbed models are still highly correlated to those without perturbation.}
  %\Description{Fully described in the caption.}
  \label{fig:pred_corr}
  %\vspace{0.2cm}
\end{figure}

\vspace{-.3cm}
\subsubsection{Stability of Feature Importances When Inputs Are Perturbed}
%In this section we evaluate the stability of feature importances when inputs are perturbed. 
Figure \ref{fig:low_perturbed_input} shows us a glimpse of this analysis for low level of noise on synthetic data. From this figure, we see that SHAP is more stable than gain feature importances when we add a small noise to the perturbed input, especially for XGBoost. This uplift between gain and SHAP, however, decreases as noise increases across all models as shown in Figure \ref{fig:all_perturbed_input}. We can also see from Figure \ref{fig:all_perturbed_input} that unsurprisingly stability decreases as the level of noise and the number of features increase. 

%However, the feature order correlations (Spearman correlation) for both feature importance methods drop to around 40-60\% for large number of features. With larger noise, the feature importance correlation for SHAP reduces to around 50\% while it reduces to around 20-40\% for gain.
%The differences is not as apparent for gradient boosting machine, but the Spearman correlations are slightly higher for SHAP compared to gain feature importances.
% A comprehensive figure on the effect of different levels of noise to stability across the three models can be found on Appendix \ref{appendix:stability_model_inputs}. 


 \begin{figure}[h]
  \centering
  \scriptsize
 Model: \hspace{1cm} XGBoost \hspace{1.5cm} Gradient Boosting Machine \hspace{0.7cm} Random Forest \\
 \includegraphics[width=0.09\linewidth]{images/simulation/legend.png}
  \includegraphics[width=0.29\linewidth]{images/simulation/all_noise0.5_XGB_NOHP_spearmancorr.pdf}
  \includegraphics[width=0.29\linewidth]{images/simulation/all_noise0.5_GB_NOHP_spearmancorr.pdf}
  \includegraphics[width=0.29\linewidth]{images/simulation/all_noise0.5_RF_NOHP_spearmancorr.pdf}
  
   \caption{Correlation of feature importances (Blue: gain, Orange: SHAP) for models trained with low input perturbation on synthetic data. SHAP is more stable across all models although both SHAP and gain both suffer from lack of stability.}
  \label{fig:low_perturbed_input}
  %\vspace{.3cm}
\end{figure}

 \begin{figure*}[ht]
  \centering
  \scriptsize
    Model Type: XGBoost \\
        Low noise added to input \hspace{1cm} Medium noise \hspace{1.7cm} Large noise \\
    % \includegraphics[width=0.3\linewidth]{images/simulation/all_noise0.5_XGB_NOHP_pearsoncorr.pdf}
    % \includegraphics[width=0.3\linewidth]{images/simulation/all_noise1_XGB_NOHP_pearsoncorr.pdf}
    % \includegraphics[width=0.3\linewidth]{images/simulation/all_noise2_XGB_NOHP_pearsoncorr.pdf}
    \hspace*{10ex}
    \includegraphics[width=0.29\linewidth]{images/simulation/all_noise0.5_XGB_NOHP_spearmancorr.pdf}
    \includegraphics[width=0.29\linewidth]{images/simulation/all_noise1_XGB_NOHP_spearmancorr.pdf}
    \includegraphics[width=0.29\linewidth]{images/simulation/all_noise2_XGB_NOHP_spearmancorr.pdf}
    
    Model Type: Gradient Boosting \\
        Low noise added to input \hspace{1cm} Medium noise \hspace{1.7cm} Large noise \\
    %  \includegraphics[width=0.3\linewidth]{images/simulation/all_noise0.5_GB_NOHP_pearsoncorr.pdf}
    % \includegraphics[width=0.3\linewidth]{images/simulation/all_noise1_GB_NOHP_pearsoncorr.pdf}
    % \includegraphics[width=0.3\linewidth]{images/simulation/all_noise2_GB_NOHP_pearsoncorr.pdf}
      \includegraphics[width=0.09\linewidth]{images/simulation/legend.png}
    \includegraphics[width=0.29\linewidth]{images/simulation/all_noise0.5_GB_NOHP_spearmancorr.pdf}
    \includegraphics[width=0.29\linewidth]{images/simulation/all_noise1_GB_NOHP_spearmancorr.pdf}
    \includegraphics[width=0.29\linewidth]{images/simulation/all_noise2_GB_NOHP_spearmancorr.pdf}
    
    Model Type: Random Forest \\
    Low noise added to input \hspace{1cm} Medium noise \hspace{1.7cm} Large noise \\
    % \includegraphics[width=0.3\linewidth]{images/simulation/all_noise0.5_RF_NOHP_pearsoncorr.pdf}
    % \includegraphics[width=0.3\linewidth]{images/simulation/all_noise1_RF_NOHP_pearsoncorr.pdf}
    % \includegraphics[width=0.3\linewidth]{images/simulation/all_noise2_RF_NOHP_pearsoncorr.pdf}
    \hspace{10ex}
    \includegraphics[width=0.29\linewidth]{images/simulation/all_noise0.5_RF_NOHP_spearmancorr.pdf}
    \includegraphics[width=0.29\linewidth]{images/simulation/all_noise1_RF_NOHP_spearmancorr.pdf}
    \includegraphics[width=0.29\linewidth]{images/simulation/all_noise2_RF_NOHP_spearmancorr.pdf}
  \caption{Correlation of feature importances (Blue: gain, Orange: SHAP) for models trained with input perturbation on synthetic data. SHAP is slightly more stable than gain at low level of noise but are comparable as noise increases.}
  %\Description{Fully described in the caption.}
  \label{fig:all_perturbed_input}
\end{figure*}


%We also realize that by increasing the level of noise, the Pearson and Spearman correlations of both gain and SHAP feature importances diminish making both of these method unreliable. A comprehensive figure on the effect of different levels of noise to stability can be found on Appendix \ref{appendix:stability_model_inputs}.

As shown in Figure \ref{fig:real_world_noisy_inputs}, we see that in real-world datasets when a low noise is injected to the input, the correlations of gain and SHAP feature importances drop very low. For example, in Forest Fire dataset, feature importances correlation averages to around 50\% for SHAP while it averages to around 20\% for gain. In Company Finance dataset, both gain and SHAP has either 20\% correlation or lower. We discover that SHAP is slightly more stable than gain for Forest Fire and Company Finance as can be seen on Figure \ref{fig:real_world_noisy_inputs}, although this is not consistent across all datasets. We also observe low correlations with increasing level of noise.  
% \textcolor{red}{However, overall these feature importance methods lack stability, especially in settings when inputs are perturbed.}

% We also investigate the stability when input is perturbed by limited access to the data, i.e. when only a portion of the data is used for training. Please refer to Appendix \ref{appendix:subsample} for details on this investigation.

%, but we cannot replicate the same results for Concrete and Auto MPG datasets (See Figure \ref{fig:real_world_noisy_inputs_some}). The uplift of stability for SHAP might be due to the larger number of features in Forest Fire and Company Finance which have 12 and 892 features respectively. The stability of SHAP and gain feature importances might not differ much for Concrete and Auto MPG datasets due to their relatively low number of features (7 features in Concrete and 8 features in Auto MPG datasets). This is consistent with the findings from Figure \ref{fig:low_perturbed_input} where the difference of stability between gain and SHAP is very minimal for lower number of features.
 \begin{figure*}[h]
  \centering
  \scriptsize
    Dataset: Forest Fire (\# Features: 12)\\
    XGBoost \hspace{1.7cm} Gradient Boosting \hspace{1cm} Random Forest \\
%   \includegraphics[width=0.2\linewidth]{images/forest_fire/all_noise0.5_XGB_NOHP_pearsoncorr.pdf}
%   \includegraphics[width=0.2\linewidth]{images/forest_fire/all_noise0.5_GB_NOHP_pearsoncorr.pdf}
%   \includegraphics[width=0.2\linewidth]{images/forest_fire/all_noise0.5_RF_NOHP_pearsoncorr.pdf}
  
  \includegraphics[width=0.31\linewidth]{images/forest_fire/all_noise0.5_XGB_NOHP_spearmancorr.pdf}
  \includegraphics[width=0.31\linewidth]{images/forest_fire/all_noise0.5_GB_NOHP_spearmancorr.pdf}
  \includegraphics[width=0.31\linewidth]{images/forest_fire/all_noise0.5_RF_NOHP_spearmancorr.pdf}
  
  Dataset: Concrete (\# Features: 8)\\
    XGBoost \hspace{1.7cm} Gradient Boosting \hspace{1cm} Random Forest \\
%   \includegraphics[width=0.2\linewidth]{images/concrete/all_noise0.5_XGB_NOHP_pearsoncorr.pdf}
%   \includegraphics[width=0.2\linewidth]{images/concrete/all_noise0.5_GB_NOHP_pearsoncorr.pdf}
%   \includegraphics[width=0.2\linewidth]{images/concrete/all_noise0.5_RF_NOHP_pearsoncorr.pdf}
  
  \includegraphics[width=0.31\linewidth]{images/concrete/all_noise0.5_XGB_NOHP_spearmancorr.pdf}
  \includegraphics[width=0.31\linewidth]{images/concrete/all_noise0.5_GB_NOHP_spearmancorr.pdf}
  \includegraphics[width=0.31\linewidth]{images/concrete/all_noise0.5_RF_NOHP_spearmancorr.pdf}
  
  Dataset: Auto MPG (\# Features: 7)\\
    XGBoost \hspace{1.7cm} Gradient Boosting \hspace{1cm} Random Forest \\
%   \includegraphics[width=0.2\linewidth]{images/auto_mpg/all_noise0.5_XGB_NOHP_pearsoncorr.pdf}
%   \includegraphics[width=0.2\linewidth]{images/auto_mpg/all_noise0.5_GB_NOHP_pearsoncorr.pdf}
%   \includegraphics[width=0.2\linewidth]{images/auto_mpg/all_noise0.5_RF_NOHP_pearsoncorr.pdf}
  
  \includegraphics[width=0.31\linewidth]{images/auto_mpg/all_noise0.5_XGB_NOHP_spearmancorr.pdf}
  \includegraphics[width=0.31\linewidth]{images/auto_mpg/all_noise0.5_GB_NOHP_spearmancorr.pdf}
  \includegraphics[width=0.31\linewidth]{images/auto_mpg/all_noise0.5_RF_NOHP_spearmancorr.pdf}
  
  Dataset: Company Finance (\# Features: 892)\\
     XGBoost \hspace{1.7cm} Gradient Boosting \hspace{1cm} Random Forest \\
%   \includegraphics[width=0.2\linewidth]{images/spring/all_noise0.5_XGB_NOHP_pearsoncorr.pdf}
%   \includegraphics[width=0.2\linewidth]{images/spring/all_noise0.5_GB_NOHP_pearsoncorr.pdf}
%   \includegraphics[width=0.2\linewidth]{images/spring/all_noise0.5_RF_NOHP_pearsoncorr.pdf}
  
  \includegraphics[width=0.31\linewidth]{images/spring/all_noise0.5_XGB_NOHP_spearmancorr.pdf}
  \includegraphics[width=0.31\linewidth]{images/spring/all_noise0.5_GB_NOHP_spearmancorr.pdf}
  \includegraphics[width=0.31\linewidth]{images/spring/all_noise0.5_RF_NOHP_spearmancorr.pdf}
  \caption{Correlation of feature importances (Blue: gain, Orange: SHAP) for models trained with input perturbations (low noise) on real-world datasets.  SHAP and gain both lack stability overall although SHAP is slightly more stable for certain datasets. }
  %\Description{Fully described in the caption.}
\label{fig:real_world_noisy_inputs}
\end{figure*}

%  \begin{figure}[h]
%  %\vspace{.8cm}
%   \centering
  
%   Dataset: Forest Fire (\# Features: 12) \hspace{1cm} Company Finance (\# Features: 892) \\
% %   \includegraphics[width=0.49\linewidth]{images/forest_fire/all_noise0.5_GB_NOHP_pearsoncorr.pdf}
%   \includegraphics[width=0.49\linewidth]{images/forest_fire/all_noise0.5_GB_NOHP_spearmancorr.pdf}
% %   Dataset: Company Finance (\# Features: 892) \\
% %   \includegraphics[width=0.49\linewidth]{images/spring/all_noise0.5_GB_NOHP_pearsoncorr.pdf}
%   \includegraphics[width=0.49\linewidth]{images/spring/all_noise0.5_GB_NOHP_spearmancorr.pdf}
%     \caption{Correlation of feature importances (Blue: gain, Orange: SHAP) for gradient boosting machines trained on Forest Fire and Company Finance datasets with perturbed inputs (low noise) compared to those without. SHAP is more stable compared to gain feature importances, but both have very low correlations overall.}
% \label{fig:real_world_noisy_inputs_some}
% \end{figure}


% \subsubsection{Stability of Feature Importances When Both Models and Inputs Are Perturbed}
% When both models and inputs are perturbed, i.e. by optimizing the hyperparameter of the models with different random seeds and adding noise to the inputs, we again notice a similar behaviour, that gain and SHAP feature importances' correlations are reduced as we add more features or add more noise. For more details, see Appendix \ref{appendix:stability_model_inputs}.

\vspace{-.3cm}
\subsubsection{Stability of Feature Importances When Models Are Perturbed}
% When models are perturbed, we find that the correlation of feature importances is not greatly affected for small number of features, but can reduce to 80\% correlation in large number of features. 
Figure \ref{fig:perturb_model} shows the correlation of feature importances when models are perturbed by initializing to a different random seed or by training with different hyperparameter settings. From this figure, we see that the correlation of feature importances is not greatly affected when models are perturbed for small number of features, but it drops significantly (to 80\% Spearman correlation for XGBoost and gradient boosting models) as the number of features increases to 150. We find that the correlation of SHAP feature importances is significantly higher compared to gain feature importances, especially in XGBoost trained with different hyperparameter. Although, for gradient boosting machine and random forest, we do not see the same uplift on stability for SHAP. Both gain and SHAP are equally stable for these models.% as shown in Appendix \ref{appendix:model_perturbation_gb_rf}. 

Moreover, we notice a strangely perfect correlation when training XGBoost without hyperparameter optimization but with different random seeds (See Figure \ref{fig:perturb_model}, top left). After further investigation, we discover that XGBoost is more deterministic when choosing features even when initialized with different random seeds. The results of our findings are expanded further in Appendix \ref{appendix:xgboost_deterministic}.  

 \begin{figure*}[h]
 %\vspace{.5cm}
 \centering
 \scriptsize
 Perturbation: Random seeds \\
 Model: \hspace{1cm} XGBoost \hspace{1.5cm} Gradient Boosting Machine \hspace{0.7cm} Random Forest \\ 
  \includegraphics[width=0.09\linewidth]{images/simulation/legend.png}
 \includegraphics[width=0.29\linewidth]{images/simulation/all_noise0_XGB_NOHP_spearmancorr.pdf}
 \includegraphics[width=0.29\linewidth]{images/simulation/all_noise0_GB_NOHP_spearmancorr.pdf}
 \includegraphics[width=0.29\linewidth]{images/simulation/all_noise0_RF_NOHP_spearmancorr.pdf} \\
 
 Perturbation: Hyperparameter settings\\
 Model: \hspace{1cm} XGBoost \hspace{1.5cm} Gradient Boosting Machine \hspace{0.7cm} Random Forest \\ 
 \hspace*{10ex}
 \includegraphics[width=0.29\linewidth]{images/simulation/all_noise0_XGB_HP_spearmancorr.pdf}
 \includegraphics[width=0.29\linewidth]{images/simulation/all_noise0_GB_HP_spearmancorr.pdf}
 \includegraphics[width=0.29\linewidth]{images/simulation/all_noise0_RF_HP_spearmancorr.pdf} \\
  \caption{Correlation of feature importances (Blue: gain, Orange: SHAP) for models trained on synthetic data with model perturbations across different number of features.}%SHAP is more stable when XGBoost models' hyperparameters are optimized with different random seeds, but both gain and SHAP feature importances have lower correlations for higher number of features across all settings.}
%   %\Description{Fully described in the caption.}
  \label{fig:perturb_model}
\end{figure*}

In real world settings, we also notice a decrease in stability for gain and SHAP when models' hyperparameter settings are perturbed (Figure \ref{fig:real_world_diff_seed}). This is especially bold for Forest Fire dataset. On average, gain feature importances have around 60\% Spearman correlation whereas SHAP have around 90\% Spearman correlations in this dataset. %We suspect the drop in stability can be attributed to the large number of features. 
SHAP tends to be more stable across the different real-world datasets, especially for XGBoost model as shown in Figure \ref{fig:real_world_diff_seed}, although this uplift is not as apparent in Gradient Boosting Machine and random Forest models.

%do not converge on the same explanations. Leif Hancox-Li, defines this lack of convergence on the same explanations as lack stability with respect to the explanations \cite{hancox2020robustness}.}

% This indicates that our models despite having very similar performance (i.e. input-output mapping) do not converge on the same explanations and so as defined by~\cite{bhatt2020explainable}, lack of stability with respect to the explanations. % This indicates that the differences of feature importances we see in XGBoost can be attributed towards the feature importance method itself. %Not sure how to word this

 \begin{figure}[h]
    \centering
    \scriptsize
    Dataset: Forest Fire (\# Features: 12) \\%\hspace{2cm} Concrete (\# Features: 8) \\
     XGBoost  \hspace{1.7cm} Gradient Boosting Machine \hspace{1cm} Random Forest \\ 
    % Same hyperparameters \hspace{1cm} Different hyperparameters \\ 
    % \includegraphics[width=0.49\linewidth]{images/forest_fire/all_noise0_XGB_NOHP_spearmancorr.pdf}
    \includegraphics[width=0.31\linewidth]{images/forest_fire/all_noise0_XGB_HP_spearmancorr.pdf}
    \includegraphics[width=0.31\linewidth]{images/forest_fire/all_noise0_GB_HP_spearmancorr.pdf}
    \includegraphics[width=0.31\linewidth]{images/forest_fire/all_noise0_RF_HP_spearmancorr.pdf}
    
    Dataset: Concrete (\# Features: 8) \\
     XGBoost  \hspace{1.7cm} Gradient Boosting Machine \hspace{1cm} Random Forest \\ 
    % Same hyperparameters \hspace{1cm} Different hyperparameters \\ 
    % \includegraphics[width=0.49\linewidth]{images/concrete/all_noise0_XGB_NOHP_spearmancorr.pdf}
    \includegraphics[width=0.31\linewidth]{images/concrete/all_noise0_XGB_HP_spearmancorr.pdf} 
    \includegraphics[width=0.31\linewidth]{images/concrete/all_noise0_GB_HP_spearmancorr.pdf} 
    \includegraphics[width=0.31\linewidth]{images/concrete/all_noise0_RF_HP_spearmancorr.pdf} 
    
    Dataset: Auto MPG (\# Features: 7) \\%\hspace{2cm} Company Finance (\# Features: 892)\\ 
     XGBoost  \hspace{1.7cm} Gradient Boosting Machine \hspace{1cm} Random Forest \\ 
    %  Same hyperparameters \hspace{1cm} Different hyperparameters \\ 
    % \includegraphics[width=0.49\linewidth]{images/auto_mpg/all_noise0_XGB_NOHP_spearmancorr.pdf}
    \includegraphics[width=0.31\linewidth]{images/auto_mpg/all_noise0_XGB_HP_spearmancorr.pdf}
    \includegraphics[width=0.31\linewidth]{images/auto_mpg/all_noise0_GB_HP_spearmancorr.pdf}
    \includegraphics[width=0.31\linewidth]{images/auto_mpg/all_noise0_RF_HP_spearmancorr.pdf}
    
    Dataset: Company Finance (\# Features: 892)\\
     XGBoost  \hspace{1.7cm} Gradient Boosting Machine \hspace{1cm} Random Forest \\ 
    % Same hyperparameters \hspace{1cm} Different hyperparameters \\ 
    % \includegraphics[width=0.49\linewidth]{images/spring/all_noise0_XGB_NOHP_spearmancorr.pdf}
    \includegraphics[width=0.31\linewidth]{images/spring/all_noise0_XGB_HP_spearmancorr.pdf}
    \includegraphics[width=0.31\linewidth]{images/spring/all_noise0_GB_HP_spearmancorr.pdf}
    \includegraphics[width=0.31\linewidth]{images/spring/all_noise0_RF_HP_spearmancorr.pdf}
    
    \caption{Correlation of feature importances (Blue: gain, Orange: SHAP) for XGBoost models trained on four real-world datasets with perturbations to the model's hyperparameter settings. SHAP is slightly more stable than gain for XGBoost.}
   %\Description{Fully described in the caption.}
    \label{fig:real_world_diff_seed}
\end{figure}
% \subsubsection{SHAP as a Global Importance Metric Is More Stable Than Local} \label{subsec:localvsglobal}
% Another important finding we encounter during our investigation is that SHAP global features importances are more stable compared to SHAP local feature importances. %Another interesting finding we encounter during our investigation is that when comparing SHAP local importances vs. global importances, we find that SHAP as a global importance is more stable then as a local importance. 
% Figure \ref{fig:localvsglobal} (Top) shows SHAP global importances' Spearman correlation are always higher compared to SHAP local importances correlations when inputs are perturbed. Figure \ref{fig:localvsglobal} (Bottom) shows the same comparison for when models are perturbed by changing its hyperparameters. %It is interesting how the aggregation of less stable local feature importances result in a relatively more stable global feature importances.

% \begin{figure*}[h]
%   \centering
%   \scriptsize
%   Perturbation: Input (low noise) \\ 
%   XGBoost  \hspace{1.7cm} Gradient Boosting Machine \hspace{1cm} Random Forest \\ 
%   \includegraphics[width=0.31\linewidth]{images/simulation/all_noise0.5_XGB_NOHP_localvsglobal.pdf}
%   \includegraphics[width=0.31\linewidth]{images/simulation/all_noise0.5_GB_NOHP_localvsglobal.pdf}
%   \includegraphics[width=0.31\linewidth]{images/simulation/all_noise0.5_RF_NOHP_localvsglobal.pdf}
  
%   Perturbation: Model (hyperparameter settings) \\ 
%   XGBoost  \hspace{1.7cm} Gradient Boosting Machine \hspace{1cm} Random Forest \\ 
%   \includegraphics[width=0.31\linewidth]{images/simulation/all_noise0_XGB_HP_localvsglobal.pdf}
%   \includegraphics[width=0.31\linewidth]{images/simulation/all_noise0_GB_HP_localvsglobal.pdf}
%   \includegraphics[width=0.31\linewidth]{images/simulation/all_noise0_RF_HP_localvsglobal.pdf} \\
%   \includegraphics[width=0.25\linewidth]{images/simulation/legend_localvsglobal.png} \\
 
% %\vspace{0.5cm}
% \caption{SHAP local importances are less stable than global importances across different runs iterations on synthetic data with perturbed inputs (Top) and across different models with optimized hyperparameters from different seeds (Bottom).}
%   %\Description{Fully described in the caption.}
% \label{fig:localvsglobal}
% \end{figure*}
% %\vspace{1cm}

\vspace{-.3cm}
\subsection{Summary of Results}
\label{results:summary}
We observe that there is a lack of accuracy with gain and SHAP feature importances even when there is no perturbation involved. In synthetic data with 5 features, the top feature is only ranked correctly around 40\% of the time. In addition to lack of accuracy, we also evaluate the lack of stability of these feature importances in various settings. % We provide a high-level summary on Table \ref{tab:summary_stability}.} 
We find that when inputs are perturbed, the correlations drop very low, both in synthetic and real-world datasets. When we perturb the models, especially by using different hyperparameter settings, correlation of feature importances can drop to 70-80\%. We find SHAP to be slightly more stable than gain in many cases, but both of their Spearman correlations still reduces to 60\% when low noise is added to the input.

% \begin{table*}[]
%  \caption{Summary of SHAP and Gain stability experiments.}
%   \label{tab:summary_stability}
%   \scriptsize
% \begin{tabular}{p{0.08\linewidth}p{0.19\linewidth}p{0.19\linewidth}p{0.19\linewidth}p{0.19\linewidth}}%{llp{0.19\linewidth}p{0.19\linewidth}l}
% \toprule
% Perturb         & Settings                  & Random Forest                                          & Gradient Boosting Machine                              & XGBoost                                                \\
% \midrule
% Input & Small noise        & SHAP is slightly better than Gain   although still bad & SHAP is slightly better than Gain   although still bad & SHAP is slightly better than Gain   although still bad \\
% Input & Large noise      & Equally bad                                            & Equally bad                                            & Equally bad    \\        
% Model & Same Hyperparameters      & Equally good                                           & Equally good                                           & Equally good                                           \\
% Model & Different Hyperparameters & Equally OK                                             & Equally OK                                             & SHAP is OK, gain is bad                                \\
% \bottomrule
% \end{tabular}
% \end{table*}


  







