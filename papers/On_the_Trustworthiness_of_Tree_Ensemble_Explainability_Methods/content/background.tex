\vspace{-.2cm}
\section{Background} \label{sec:background}
\vspace{-.2cm}
% With the increase of input dimensions, feature interactions, and non-linear relationships among them, modelling data with linear models becomes almost impossible. To appropriately requires complex engineering of features which can make linear models lose its simulatability or decomposability \cite{lipton2017mythos}. 
%\textcolor{blue}{Tree ensemble methods are powerful classifiers and regressors that can capture non-linear relationships and complex interactions in the data from interpretable decision trees \cite{biau2016random}.}
Tree ensemble methods are employed widely in research and industry due to their efficiency and effectiveness in modeling complex interactions in the data~\cite{biau2016random}. The two most common tree ensemble methods are gradient boosting \cite{friedman2002stochastic} and random forest \cite{breiman2001random}. In gradient boosting, trees are trained sequentially with upweighting the previously misclassified labels. In contrast, random forest trees are trained in parallel with different subsampling across all trees. We use random forest and gradient boosting machine implemented by sklearn~\cite{scikit-learn}, as well as XGBoost, a faster version of gradient boosting that uses second-order gradients~\cite{Chen:2016:XST:2939672.2939785}.
% Chen and Guestrin implemented a library called XGBoost, which implements gradient boosting with improvement on training runtime in addition to implementing second-order gradients for training \cite{Chen:2016:XST:2939672.2939785}.
% The question then lies on how to summarize the feature importance across these many trees. 

In this study, to compute global feature importances in tree ensemble methods we use gain~\cite{friedman2001elements} and SHAP~\cite{NIPS2017_7062}, an implementation of the Shapley algorithm. We focus on SHAP instead of LIME~\cite{ribeiro2016should} as LIME explanations can be fragile due to sampling variance \cite{bhatt2020explainable} and less resilient against adversarial attacks as shown by~\cite{slack2020fooling}. % Another reason for focusing on SHAP is its stability when compared to the leave-one-out (LOO) approach. This is mainly because Shapley value summarizes the contribution of a feature across all possible subsets of features, in contrast to LOO which evaluates the contribution of a feature once by removing/permuting it from the whole set \cite{ghorbani2019data}. 
In the following sections, we briefly explain how gain~\cite{friedman2001elements} and SHAP~\cite{NIPS2017_7062} are computed.

\paragraph{Gain.}
\label{subsec:default_imp}
For both of the aforementioned tree ensemble methods, sklearn \cite{scikit-learn} and xgboost \cite{Chen:2016:XST:2939672.2939785} libraries provide the implementation to obtain the feature importances based on Hastie's description in the Elements of Statistical Learning \cite{friedman2001elements}. This is also referred to as \emph{gain}. This metric represents the improvements in accuracy or improvements in decreasing uncertainty (or variance) brought by a feature to its branches. At the end, to get a summary of the whole tree ensemble, this measure is averaged across all trees \cite{friedman2001elements,lewinson_2020,abu_2019}. % However, as shown by~\cite{strobl2007bias} this metric can be biased in random forests and can favour continuous or categorical variables with high-cardinality. %To address this issue, a different way of training random forests is proposed by Strobl et al.~\cite{strobl2007bias}. 
In this paper, for the sake of simplicity and consistency we refer to this method as \emph{gain}.

\paragraph{SHAP.} \label{subsec:shap_imp}
SHapley Additive exPlanations (SHAP)~\cite{NIPS2017_7062} has gained a lot of attention in industry as a way to measure feature importance~\cite{bhatt2020explainable}. SHAP is an implementation of Shapley formula that summarizes the contribution of a feature to the overall prediction by approximating the Shapley value presented in the following:
\begin{equation*}
\phi_i = \sum_{S \subseteq F \backslash\{i\}} \frac{|S|!(|F| - |S| - 1)!}{|F|!}
[f_{S\cup\{i\}}(X_{S \cup \{i\}}) - f_S(X_S)]
\end{equation*}
where $\phi_i$ is the Shapley value for feature $i$, $S$ is a subset of all features $F$ that does not include feature $i$, $f_{S\cup\{i\}}$ is the model trained on features in $S$ and feature $i$, $f_S$ is the model trained on features in $S$, and $X$ is the input data. 

SHAP inherently calculates local importances, i.e. how each feature contributes to the prediction of a specific input. By averaging the absolute value of these local importances across the training set, one can obtain a global summary of how the feature as a whole contributes to the model. % To get a global picture of the model, one can summarize the feature importances across the training data as suggested by the authors. % We assume that summarizing the feature importances across the training data is sufficient to get a global picture of the model. We recognize this as a limitation since SHAP is inherently a local explanation method and further discuss this in Section \ref{sec:discussion}. 
% The stability of local explanations obtained by the original implementation of SHAP (also called Kernel SHAP) has been studied before~\cite{alvarez2018robustness}. However, the stability of Tree SHAP~\cite{lundberg2020local2global}, a recent extension to Kernel SHAP with faster computation runtime for trees, has not yet been investigated~\cite{lundberg2020local2global}. 
In this paper, we investigate the accuracy and stability of Tree SHAP~\cite{lundberg2020local2global} (a recent extension to Kernel SHAP with faster computation runtime for trees) under various settings. Unlike Kernel SHAP~\cite{NIPS2017_7062} which uses perturbation, Tree SHAP (with tree\_path\_dependent setting) leverages trees' cover statistics for fast approximation of Shapley values. % This is especially advantageous since this indicates that Tree SHAP feature importances do not rely on sampling settings, unlike other perturbation methods. 

%Tree SHAP (with tree\_path\_dependent setting) allows for fast approximation of SHAP values, based on the trees' cover statistics, instead of values obtained from perturbation as previously done in Kernel SHAP~\cite{alibi, lundberg2020local2global}. %One of the main differences is that for Tree SHAP, the Shapley values identified lie on the model's manifold \cite{lundberg_github_discussion}. This means that the stability to perturbed input depend on the underlying model, instead of the SHAP algorithm itself. We further discuss this in \ref{sec:discussion}. 
% There has been discussion that using perturbation feature importance can counter the bias resulted from gain feature importance~\cite{parr2018beware}. SHAP inherently approximates the permutation feature importance value but here we focus on Tree SHAP for its faster runtime. 
% Rudin \cite{rudin2019stop} suggests that post-hoc explanation methods should be avoided as they can provide explanations that are not true to the original model. Here, as we are using metrics based on the trees' innate structures (for both gain and Tree SHAP feature importances), we believe the explanations should be close to true to the model.
