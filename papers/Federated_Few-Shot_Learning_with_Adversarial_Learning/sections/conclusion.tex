\section{Conclusion}
In this paper, we proposed a framework that makes federated learning effective in data-scarce scenarios. 
We designed an adversarial learning strategy to 
construct a consistent feature space over the clients, to better learn from scarce data.
Experimental results show that our adversarial learning based method outperforms baseline methods by 5\%$\sim$15\% on benchmark datasets.
Future work can investigate the
theoretical convergence analysis of FedFSL with non-convex models and how to further extend FedFSL to regression and reinforcement learning tasks.




% A new multimodal fusion layer was designed to fuse visual and textual modalities and perform multi-step reasoning with gradually refined attention. In empirical studies, we 
% visualized the attentions generated by our model to verify its capability of understanding complex questions and attending to salient visual hints. 



% We introduce the FedFSL task to tackle the task of learning a generic model which can learn to recognize novel classes with distributed data and computational sources. We show that FedFSL can be properly achieved by integrating the meta-learning methodology into FL framework and regularizing model training with mutual information provided by the global model as well as an adversarial learning procedure for optimizing class decision boundaries. Future work can investigate the
% theoretical convergence analysis of FedFSL with non-convex models such as DNNs and how to further extend FedFSL to regression and reinforcement learning tasks.



% In this paper, we have focused on improving meta-learning approach of few-shot learning under federated learning scenario that multiple participants contribute to the learning with local data. We observed the issue that category boundaries are mis-aligned across the clients, and thus a common direct federation approach such as FedAvg would XXX. 

% gradient-descent based
% federated learning that include local update and global aggregation steps. Each step of local update and global aggregation
% consumes resources. We have analyzed the convergence bound
% for federated learning with non-i.i.d. data distributions. Using
% this theoretical bound, a control algorithm has been proposed
% to achieve the desirable trade-off between local update and
% global aggregation in order to minimize the loss function under
% a resource budget constraint. Extensive experimentation results
% confirm the effectiveness of our proposed algorithm. Future
% work can investigate how to make the most efficient use of
% heterogeneous resources for distributed learning, as well as the
% theoretical convergence analysis of some form of non-convex
% loss functions representing deep neural networks.
