{\SetAlgoNoLine
\begin{algorithm}[htpb]
\small
\DontPrintSemicolon
\LinesNumberedHidden
\KwIn{A set of $K$ federated clients. A local FSL objective $\mathcal{L}_k$ for each client $k$.
% The total dataset $\mathcal{D}$ is the union set of all actual local data $\{D_1,\dots,D_K\}$ on $N$ clients. 
% Task distributions $p(\mathcal{T}^1),\dots,p(\mathcal{T}^K)$ of client data could be identical or not. 
% Learning rate $\alpha$, $\beta$. 
}
\KwOut{A global model $\wsupsubeq{}{}$ optimized for FSL task.}
\textbf{Server executes:} \;
\Indp Initialize global model $w^0$ \;
$t \leftarrow 1$ \;
% Randomly initialize $\wsupsubeq{}{k}$ for all clients. \;
%\For{\textup{each round} $t = 1,2,\dots,T$ } {
\While{t $\leq$ maximum rounds $T$}{
    \For{\textup{each client} $k$ \textup{\textbf{in parallel}}}{ 
    $w_k^{t} \leftarrow $ ClientUpdate{$(w^t)$} 
    }
    Clients send model parameters $w_{1...K}^{t}$ to server \;
    $w^{t+1} \leftarrow \sum_{k=1}^K \frac{|\mathcal{B}_k|}{|\mathcal{B}|} w^{t}_k$  \tcp*[l]{model avg}
    The server sends $w^{t+1}$ back to clients \;
    $t \leftarrow t+1$
 }
Return $w^t$ \;
\;
\Indm \textbf{ClientUpdate}$(w)$: \;
\Indp \KwIn{global model from previous round $w^t$}
\KwOut{updated local model $w^t_k$}
%  $\mathcal{B}_k \leftarrow$ (split local data into batches of episodes) \;
 $w \leftarrow w^t$ \;
 Sample a batch of episodes $\mathcal{B}_k=\{\mathcal{T}_1,...,\mathcal{T}_n\}$ \;
 $w^k_t \leftarrow $ Solve Eq.\eqref{eq:fed_maml} with SGD \;
 Return $w^t_k$ \;
% \For{\textup{each episode} $\mathcal{T} \in \mathcal{B}_k$ \textup{\textbf{in sequence}}}{ 
%     $w' \leftarrow \wsupsubeq{}{}-\alpha  \gradsubeq{\wsupsubeq{t}{k}}f_{\mathcal{T}^{(s)}}(w^t_k)$ \tcp*[l]{Eq.\eqref{eq:adp}}
%     % \LeftComment{// \textit{optimize local objective} $\mathcal{L}_k$} \;
%     $w^k_t \leftarrow \underset{\wsupsubeq{}{}}{\text{argmin}} \ \mathcal{L}_k(w)$  \tcp*[l]{Eq.\eqref{eq:fed_maml}}
% }
% return $w$ to server \;
\caption{FedFSL-naive framework.} \label{algo:fed_maml}
%\end{algorithm2e}
\end{algorithm}
}







% \begin{algorithm}[htpb]
% \DontPrintSemicolon
% \KwIn{A set of $K$ federated clients, each owns a local model parameterized as $\wsupsubeq{}{k}$.
% % The total dataset $\mathcal{D}$ is the union set of all actual local data $\{D_1,\dots,D_K\}$ on $N$ clients. 
% % Task distributions $p(\mathcal{T}^1),\dots,p(\mathcal{T}^K)$ of client data could be identical or not. 
% % Learning rate $\alpha$, $\beta$. 
% }
% \KwOut{A global model $\wsupsubeq{}{}$ optimized for FSL task.}
% \textbf{Server executes:} \;
% \Indp Initialize $w_0$ \;
% % Randomly initialize $\wsupsubeq{}{k}$ for all clients. \;
% \For{\textup{communication round} $t = 1,2,\dots,T$ } {
%     $m \leftarrow \max(C\cdot K, 1)$ \;
%     $S_t \leftarrow $ (a random subset of $m$ clients) \;
%     \For{\textup{each client} $k = 1,2,\dots,K$ \textup{\textbf{in parallel}}}{ 
%         Each client receives global parameters $\wsupsubeq{t}{}$ and set as $\wsupsubeq{t}{k}$. \;
%         Sample a batch of tasks $\mathcal{T}_k \in \mathcal{B}_k$. \;
        
%         \ForEach{$\mathcal{T}_k$ \in $\mathcal{B}_k$}{
%             \textbf{Adaptation}: tune model parameters to adapt to the task with a few gradient steps as in Eq~(\ref{eq:adp}). \;
%         }
        
%         \textbf{Optimization}: minimize local objective $\mathcal{L}_k$ as in Eq~(\ref{eq:fed_maml}) and obtain updated parameters $\wsupsubeq{t+1}{k}$\;
%     }
        
%     \textbf{Aggregation and synchronization}:
%     the parameter server aggregates client models by averaging with Eq~(\ref{eq:fuse}) to obtain $\wsupsubeq{t+1}{}$.
%  }
% \caption{Details of Fed-MAML in Section~\ref{sec:fed_maml}.}
% \end{algorithm}