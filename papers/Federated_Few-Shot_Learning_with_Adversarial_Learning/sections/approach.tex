\section{Federated Few-shot Learning}
\label{sec:approach}

In this section, we formulate the Federated Few-Shot Learning (FedFSL) framework in details. Specifically, we will study the few-shot classification task which learns to classify on novel classes with few training samples.
We will first review the general federated learning (FedL) objective in~\ref{sec:fl} and the general centralized few-shot learning (FSL) procedures in~\ref{sec:fsl} respectively, based on which we propose the FedFSL formally in
\ref{sec:fed_maml}.

% We also have a novel class set $C_n$ consisting of $n_n$ new classes with non-overlapping with base classes, and each class only has a few labeled data points. Fed-FSL aims to learn a good classifier for novel classes.
% We define a few-shot learning task with its conventional terminology: a N-way P-shot classification task will sample K labeled data from each of N classes which gives a total of $N\cdot K$ samples to perform N-way classification.
% We show the frequently used notations in Table~\ref{tab:freq_notation} for reference.
 
\subsection{General Federated Learning Objective}
\label{sec:fl}
First, we briefly review federated learning (FedL) paradigm and its common implementations in this subsection. 
We consider a distributed system of $K$ clients, each owning a local data source. In FedL, each client trains a local machine learning model based on its local data, while a central server coordinates the clients periodically by collecting their parameters, aggregating them into a central model and sending its parameter back to all clients.
% Federated learning (FL) is a new distributed machine learning paradigm which preserves data at local sources while exchanges models weights with communications between clients and the server for coordination. In particular, FedAvg~\cite{fedavg}, the first and perhaps the most concise form of FL, 
% proposed to update each client model in parallel at first, then aggregate client models by weight averaging to a central model, at last push back the central model to each client for coordination. 
This process will repeat for multiple communication rounds until convergence or timeout.  

Formally, let 
$n_k$ be number of data samples of client $k$, $n=\sum_k n_k$ be total samples across the devices, $w$ be the learning model.
We consider a local objective for client $k$ as the average loss over all data samples
\begin{equation}
\small
\mathcal{L}_k(w)=\frac{1}{n_k} \sum_{i=1}^{n_k} f(x_i,y_i;w),
\label{eq:fed_loss}
\end{equation}
in which $f$ is a loss function that evaluates the prediction of model $w$ on a data sample $(x_i, y_i)$. 
The type of loss function $f$ depends on the task and is known by all clients.
For example, in a classification task with deep neural networks (DNNs), $f$ is often chosen as the cross-entropy loss applied on the models' probabilistic outputs.

The global target is a weighted average of local objectives
\begin{equation}
\small
\underset{w}{\text{min}} \ \mathcal{L}(w) = \sum_{k=1}^{K} p_k \mathcal{L}_k(w),
\label{eq:fed_def}
\end{equation}
in which $p_k = n_k/n$. 
However, as direct data exchange is prohibited in FedL, directly optimizing the global objective  \eqref{eq:fed_def} needs to perform a 
full batch gradient descent on all data that each client holds, and perform 
model aggregation after each client update. This requires high memory usage (if the client model is large) and excessive communications for exchanging models between clients and the server.
A common practice to resolve this issue is to approximate the global objective, such as in the
FedAvg~\cite{fedavg} algorithm which optimizes each local objective $\mathcal{L}_k$ individually and in the FedProx~\cite{li2018federated} which solves the local objectives with proximal terms to regularize training.

% Two widely used approximations are as follows:
% \begin{itemize}
% \item FedAvg~\cite{fedavg} proposes to optimize each local objectives $\mathcal{L}_k$ individually with local data to obtain local optimal weights $w_k$. Then a central server aggregates the client models by averaging to approximate the global solution 
% \begin{equation}
% w = \sum_{k=1}^K p_k w_k
% \end{equation}
% After that the central model is sent back to clients to complete a communication round.
% \item FedProx~\cite{li2018federated} is a variant of FedAvg by adding a proximal term in local objective such that 
% \begin{equation}
% \mathcal{L}^{prox}_k(w)=\mathcal{L}_k(w) + \mu ||w-\bar{w}||^2
% \label{eq:fed_prox}
% \end{equation}
% %$\underset{w}{\text{min}} \ \mathcal{L}(w) + \mu ||w-w^t||^2$ 
% in which $\bar{w}$ is the synchronized global model. The proximal term aims to regularize local updates to be closer to the global model to avoid divergence.
% \end{itemize}

Existing FedL approaches often assume that the clients
always hold sufficient training data for a same task, e.g., all the clients should own enough data samples of the same categories in a classification task.
However, the realistic situation is each client may own a few labeled data samples for certain categories for training, and may encounter unlabeled data samples for testing with unseen true categories. This leads us to study the few-shot classification task which learns to classify on novel classes with few training samples in the following sections.
 
 
% For example, 
% many recent FedL studies~\cite{fedavg, zhao2018federated} compared on the image classification task 
% the classes of images that can be classified are limited by the dataset and each class is assumed to have enough training samples. 
% How to recognize unseen data classes with few labeled training data is still an open problem.




\subsection{General Centralized Few-Shot Learning}
\label{sec:fsl}
Next, we briefly review centralized few-shot learning (FSL) procedures. FSL aims to learn a generic model which can adapt to 
unseen tasks with only a few labeled training samples. In this paper, we study the few-shot classification task which aims to classify novel classes.  We define an $N$-way $P$-shot $Q$-query FSL task as a task of training a model with $P$ labeled images for each of $N$ classes and then
evaluating the model with $Q$ unlabeled query images for each class. $P$ is typically very small such as 1 or 5 as ``few-shot" implies.

Let us consider a toy example of classifying animal pictures. The training data are images of cats and dogs which we call \textit{base classes}. The testing data are images of tigers and wolves which we call \textit{novel classes}; each novel class owns one labeled image and many unlabeled images.
We wish to develop a model trained with base classes (cat and dog) that can predict on tiger and wolf (2-way) samples by observing each category just one labeled image (1-shot), i.e., a 2-way 1-shot FSL task. 


\begin{figure}
\begin{center}
\includegraphics[clip, trim=0 0 0 0, width=0.46\textwidth]{figures/fsl_1.pdf}
\end{center}
\vspace{-10pt}
\caption{Three-step meta-learning of FSL. 
}\label{fig:fsl_1}
\vspace{-12pt}
\end{figure}

To train a capable FSL model, recent state-of-the-art work~\cite{Gidaris_2018_CVPR, sung2018learning, finn2017model, li2017meta} adopted a training strategy called meta-learning which samples various few-shot tasks from training data and optimizes the model to fast adapt to these new tasks. The key idea of meta-learning is to learn some transferable knowledge from few data samples in training data that can apply on unseen data. 
State-of-the-art gradient-based approaches~\cite{finn2017model,sun2019mtl} consider a good model initialization as transferable knowledge if it could adapt to various few-shot tasks with just a few gradient steps. They propose to explicitly search for such an initial model as follows. 

The training objective is to minimize the training loss over a batch of tasks $\Tcaleq \in \mathcal{B}$ as follows
\begin{equation}
\begin{split}
w^* &= \underset{w}{\text{min}} \ \mathcal{L}(w) = \frac{1}{|\mathcal{B}|} \sum_{\Tcaleq \in \mathcal{B}} \ell_\mathcal{T}(w), \\
\end{split}
\label{eq:fed_def_adp}
\end{equation}
in which $w^*$ is the optimized model that trains to fast adapt to new tasks and $\ell$ is a task loss. This can be tackled with an iterative approach in which each iteration can be decomposed into three steps, as shown in Fig.~\ref{fig:fsl_1}.
\begin{itemize}[leftmargin=*]
\item \textbf{Sampling step:} The first step is to sample a few-shot task 
$\mathcal{T}$,  also called an \textit{episode}, from base classes. 
For an $N$-way $P$-shot $Q$-query few-shot task, an episode consists of $P$ data instances sampled from each of $N$ distinct base classes as a support set $\mathcal{T}^{(s)}$, and $Q$ data instances sampled from the same $N$ classes as a query set $\mathcal{T}^{(q)}$, which gives a total of $(P+Q) \cdot N$ instances.
\item \textbf{Adaptation step:} The second step is to adapt the current model to the sampled task with gradient descents. This step uses the few labeled data in the support set  $\mathcal{T}^{(s)}$ and performs one or several gradient steps towards optimizing the model weights to the sampled task such that
\begin{equation}\label{eq:adp}
\begin{split}
\wsupsubeq{'}{} = \wsupsubeq{}{}-\alpha \gradsubeq{w}f_{\mathcal{T}^{(s)}} (w),
% \wsupsubeq{'}{k} &= w-\alpha \gradsubeq{w} \mathcal{L}_k
% (w) \\
% &= w-\alpha \gradsubeq{w} \frac{1}{|\mathcal{B}_k|} \sum_{\mathcal{T}_k} f_{\mathcal{T}^{(s)}_k}(w)
\end{split}
\end{equation}
in which $w'$ is the adapted model, $\alpha$ is the step size. %$\gradsubeq{w}$ is the gradient of the loss function. 
% Eq.~\ref{adp} alters the initial parameters in the direction that best suited to the given task.
\item \textbf{Optimization step:} The final step is to evaluate $w'$ with more samples in the query set $\mathcal{T}^{(q)}$ with the empirical loss function
\begin{equation}
\begin{split}
\ell_\mathcal{T}(w) & =  f_{\Tcaleq^{(q)}}(w')  = f_{\Tcaleq^{(q)}}(w-\alpha \nabla f_{\mathcal{T}^{(s)}}(w)), \\
\end{split}
\label{eq:fed_def_adp_l1}
\end{equation}
which can be 
solved by another gradient descent
\begin{equation}
\begin{split}
% \mathcal{L}(w) & = \frac{1}{|\mathcal{B}|} \sum_{\Tcaleq} \ell_\mathcal{T}(w) \\
% w^* &= \underset{w}{\text{min}} \ \mathcal{L}(w)
w \leftarrow w -\beta \ \gradsubeq{w} \ell_\mathcal{T}(w),
\end{split}
\label{eq:fsl_sgd}
\end{equation}
in which $\beta$ is the learning rate.
\end{itemize}
The above procedures are summarized in Fig.~\ref{fig:fsl_1}.
For a centralized FSL to converge, the sampling-adaptation-optimization procedures are repeated for many iterations. This will produce the optimal parameter that best adapts to few-shot tasks. 

During inference, a learned FSL model firstly adapts to unseen tasks with a few labeled samples with \eqref{eq:adp}, then predicts the labels on query samples. In our toy example mentioned above, the model is trained with cat and dog samples to discern two species, then is used to classify tiger and wolf samples with its learned capacity of distinguishing patterns. 
% . Similar to training stage, each task is composed of a support set which is used to adapt the model to new task, as well as a query set to evaluate the model with standard metrics such as classification accuracy and cross-entropy loss.

% Task distributions over client data are defined as $p(\mathcal{T}^1),\dots,p(\mathcal{T}^K)$.
\begin{figure}
\begin{center}
\includegraphics[clip, trim=0 0 0 0, width=0.4\textwidth]{figures/fsl_sgd_demo.pdf}
\end{center}
\vspace{-10pt}
\caption{Demo of a two-client case of FedFSL based on meta-learning procedure.
}\label{fig:fed_fsl_2_client}
\vspace{-10pt}
\end{figure}

\subsection{Federated Few-shot Learning (FedFSL)}
\label{sec:fed_maml}
% In this section, we will 
% We wish to develop a few-shot learning model on mobile devices for facilitating practical applications with machine learning techniques.
% In the meantime, it's also important to keep data privacy and communication efficiency.
As our purpose is to facilitate distributed devices to learn models for few-shot tasks,  we need study how to design such a framework in which meta-learning procedures can be integrated in the federated learning.
We propose Federated Few-shot Learning (FedFSL) in this section. The goal of FedFSL is to search for a \textit{global} optimal model $w^*$ learned on distributed data sources that can best perform few-shot tasks.

Suppose we have $K$ participating clients and each of them can sample batches of few-shot tasks $\mathcal{T}_k \in \mathcal{B}_k$ from their local data sources as discussed in previous section. We first define $k$-th client's local FSL objective by extending from \eqref{eq:fed_def_adp}
\begin{equation}
\begin{split}
\mathcal{L}_k(w) & = \frac{1}{|\mathcal{B}_k|} \sum_{\Tcaleq_k \in \mathcal{B}_k} \ell_{\mathcal{T}_k}(w), \\
\end{split}
\label{eq:fed_def_adp_k}
\end{equation}
in which the subscript $k$ of $\Tcaleq_k$ emphasizes that it's sampled from local data source of client $k$.
% with the difference that the centralized dataset is now replaced with distributed data sources.


Our goal is to find a global optimal model $w^*$ which minimizes the weighted average of local FSL objectives. We formulate this global target as an FedL problem similar to \eqref{eq:fed_def}, i.e., 
\begin{equation}
\begin{split}
w^* &= \underset{w}{\text{min}} \ \mathcal{L}(w) = \sum_{k=1}^{K} p_k \mathcal{L}_k(w) = \sum_{k=1}^{K} \frac{|\mathcal{B}_k|}{|\mathcal{B}|} 
 \mathcal{L}_k(w) \\
\end{split}
\label{eq:fed_def_all_0}
\end{equation}
% \begin{equation}
% \begin{split}
% \underset{w}{\text{min}} \ \mathcal{L}(w) &= \sum_{k=1}^{N} p_k \mathcal{L}_k(w) \\
% & = \sum_{k=1}^{N} p_k \sum_{\Tcaleq_k} \mathcal{L}_{\Tcaleq^{(q)}_k} (f_{ \wsupsubeq{}{}-\alpha \gradsubeq{\wsupsubeq{}{}}\mathcal{L}_{\mathcal{T}^{(s)}_k} (f_{\wsupsubeq{}{}})})
% \end{split}
% \label{eq:fed_def_all_0}
% \end{equation}
Directly optimizing \eqref{eq:fed_def_all_0} would be difficult, as learning an optimal $w^*$ for all distributed clients would require excessive communications across clients, as discussed in \ref{sec:fl}. We thus provide an efficient algorithm for tackling this issue. 


Motivated by FedAvg~\cite{fedavg}, we propose a straightforward way of solving a surrogate objective of \eqref{eq:fed_def_all_0} to approximate the global solution, which we call \textbf{FedFSL-naive}.
As shown in Fig.~\ref{fig:fed_fsl_2_client},
FedFSL-naive iteratively updates the central model $w$ by (i) first optimizing each local objective of \eqref{eq:fed_def_adp_k} in parallel, and (ii) aggregating local models to the central model, which update the global model and send it back to clients for the next round of optimization. Formally,
\begin{itemize}[leftmargin=*]
\item At the $t$-th optimization round, each client $k$ optimizes the following local objective
\begin{equation}
\begin{split}
w^{*,t}_k &= \underset{\wsupsubeq{}{}}{\text{argmin}} \ \mathcal{L}_k(\wsupsubeq{}{}) = \underset{\wsupsubeq{}{}}{\text{argmin}} \ \frac{1}{|\mathcal{B}_k|} \sum_{\Tcaleq_k \in \mathcal{B}_k} \ell_{\Tcaleq_k} (w), \\
\end{split}
\label{eq:fed_maml}
\end{equation}
in which the FSL loss $\ell(w)$ is given by \eqref{eq:fed_def_adp_l1}. Fig.~\ref{fig:fed_fsl_2_client} shows a two-client example, in which each client updates on three sampled tasks with \eqref{eq:fed_maml} and obtains local optimal models $w^{*,t}_1$ and $w^{*,t}_2$. The clients then send these local parameters to the central server. 
\item Then the central server approximates the optimal global solution by averaging the client models such that
\begin{equation}
\label{eq:fuse}
w^{t+1} = \sum_{k=1}^C  \frac{|\mathcal{B}_k|}{|\mathcal{B}|} \wsupsubeq{*,t}{k},
\end{equation}
which will be synchronized to all clients for next round of optimization.
\end{itemize}
The above steps \eqref{eq:fed_maml} and \eqref{eq:fuse} are repeated for multiple rounds until convergence. We summarize the procedures in Algorithm~\ref{algo:fed_maml}.

\input{sections/algo_1}

\begin{prop}
\label{prop:1}
If loss function $f_{\mathcal{T}}(w)$ in \eqref{eq:adp} satisfies
the strongly-convex conditions as in Corollary 1, Finn~\etal~\cite{finn2019online}
\footnote{ $f$ is \textit{G-Lipschitz}, $\beta$-\textit{smooth}, $\rho$-\textit{Lipschitz} Hessians and $\mu$-\textit{strongly} convex}
,  Algorithm~\ref{algo:fed_maml} converges at a rate of  $\mathcal{O}(\frac{1}{T})$ in which $T$ is the total number of every device's gradient updates during training.
\end{prop}
\begin{proof}
As $f_{\mathcal{T}}(w)$ is strongly-convex, 
Corollary 1, \cite{finn2019online} implies that the local FSL objective $\mathcal{L}_k$ in \eqref{eq:fed_def_adp_k} is also strongly-convex. By taking Algorithm~\ref{algo:fed_maml} as a FedAvg algorithm with a strongly-convex objective, Theorem 3, \cite{li2019convergence} implies that it converges at a rate of  $\mathcal{O}(\frac{1}{T})$ in which $T$ is total number of local gradient updates of all devices during training.
\end{proof}

Proposition~\ref{prop:1} shows that for a convex FSL objective,
e.g., the cross-entropy loss of a linear or logistic model with $L_2$-regularization, Algorithm~\ref{algo:fed_maml} converges.
For a broader family of non-convex models, such as deep neural networks, convergence of FedL is still an open research topic though 
% the local objective does not satisfy the strong convexity assumption. To provide extended theoretic convergence analysis to arbitrary local objective, 
some attempts have been made~\cite{smith2017cocoa, li2018federated}.
% assume that there exists some inexact solution which can be obtained with finite steps.
% that the dissimilarity between global and local optimal solution is bounded (Assumption 1,~\cite{li2018federated}), and the loss function with $L_2$ regularization is strongly convex, then a theoretic proof is available. 
%We will empirically study the convergence rate in \ref{sec:exp}.


We note that solving \eqref{eq:fed_maml} requires computing a gradient through $w'$, which is another function of gradient of $w$ as given by \eqref{eq:adp}, and thus requires computing the Hessian. Fortunately, fast Hessian-vector products~\cite{ghorbani2019investigation} are widely adopted to approximate the second-order information, which is equivalent of performing backward passes twice with SGD.
If we denote the number of parameters of the model as $W$, this results in $\mathcal{O}(W)$ computational time for one iteration of model update. Thus the total computational time of FedFSL-naive is $\mathcal{O}(W\cdot T)$. Note that Chen~\etal~\cite{chen2018fedmeta}  proposed similar federated meta-learning procedures for supervised learning tasks.






% \begin{equation}
% \begin{split}
% &\mathcal{L}_k^{MI}(w, w_k) = \sum_{\Tcaleq_k} \mathcal{L}^{kl}_{\Tcaleq_k} (f_w, f_{w_k}) \\
% =&\sum_{\Tcaleq_k} \mathrm{KL}\left( \sigma \left (\frac{f_w}{T} \right),\sigma \left (\frac{f_{w_k}}{T} \right ) \right ) \\
% =&\sum_{\Tcaleq_k=(\mathbf{x},\mathbf{y})} \left( \underbrace{p_{w}(\mathbf{x}) \cdot \log p_{w}(\mathbf{x})}_{w_k \  \text{irrelevant}} - \underbrace{p_{w}(\mathbf{x}) \cdot \log p_{w_k}(\mathbf{x})}_{w_k \ \text{relevant}} \right) \\
% \end{split}
% \label{eq:loss_kd}
% \end{equation}









% \textbf{Adaptation stage.} In this stage, each client $k$ aims to generalize their model parameters $\Tteq$ to a sampled task $\mathcal{T}_k$ as $\Ttsupsubeq{'}{k}$, where the new task $\mathcal{T}_k$ is sampled from each client's training set $\mathcal{D}_k$ on base classes. The model parameters \Ttsupsub{}{k} are updated on the support set $\mathcal{T}^{(s)}_k$ of the task to \Ttsupsub{'}{k} with one or a few gradient descent updates such that
% \begin{equation}\label{eq:adp}
% \Ttsupsubeq{'}{k} = \Ttsupsubeq{}{k}-\alpha \gradsubeq{\Ttsupsubeq{}{k}}\mathcal{L}_{\mathcal{T}^{(s)}_k} (f_{\Ttsupsubeq{}{k}})
% \end{equation}
% in which $\alpha$ is the step size, $\gradsubeq{\Tteq}$ is the loss function such as cross-entropy loss for classification task. Note that in

% \textbf{Optimization stage} is to utilize the adapted models to predict on the query set $\mathcal{T}^{(q)}_k$ of the sampled task in previous stage and
% compute the empirical loss on query samples. The objective is  
% \begin{equation}
% \begin{split}
% \underset{\Tteq}{\text{min}} \ & \mathcal{L}_{\Tcaleq^{(q)}} (f_{\Ttsupsubeq{'}{k}}) = \\
% & \mathcal{L}_{\Tcaleq^{(q)}} (f_{ \Ttsupsubeq{}{k}-\alpha \gradsubeq{\Ttsupsubeq{}{k}}\mathcal{L}_{\mathcal{T}^{(s)}_k} (f_{\Ttsupsubeq{}{k}})})
% \end{split}
% \end{equation}
% by combining Eq(~\ref{eq:adp}).
% and then update the original parameters \Tt by back-propagating the gradients through $\Ttsupeq{'}$. 
% Note that for the first two stages of meta-training, each client could execute the steps in parallel