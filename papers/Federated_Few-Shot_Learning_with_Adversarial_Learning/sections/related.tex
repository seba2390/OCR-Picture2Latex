





\section{Related work} 


We will briefly review recent related work 
in two categories: (i) studies
either \fdl (FedL) (e.g.,~\cite{fedavg,zhao2018federated,li2018federated,li2020federated,dinh2019federated,zhang2020fedpart}) or few-shot learning (FSL) (e.g.,~\cite{vinyals2016matching, snell2017prototypical,munkhdalai2017meta,li2017meta,sun2019mtl,Gidaris_2018_CVPR, sung2018learning, Li_2020_adap_margin}), or both of them~\cite{chen2018fedmeta} (ii) studies proposing similar ideas of minimizing model divergence to better learn individual models or an ensemble model.




% In this section, we will briefly review the related work that either relates to both \fdl (FedL) has become a rapidly developing topic in the research community~\cite{fedavg,zhao2018federated,li2018federated,li2020federated,dinh2019federated,zhang2020fedpart}, as it provides a new way of learning models over a collection of distributed mobile devices while still preserving data privacy. 


% Few-shot learning (FSL)~\cite{vinyals2016matching, snell2017prototypical} is another hot topic in machine learning field
% which learns to recognize novel classes with very few labeled training samples. Recent state-of-the-art approaches either focus on learning a distance metric that can distinguish categories ~\cite{Gidaris_2018_CVPR, sung2018learning, Li_2020_adap_margin} or 
% searching for a good initialization of the model parameters 
% which can adapt to novel classification tasks with only a few gradient update steps~\cite{finn2017model,munkhdalai2017meta,li2017meta,sun2019mtl}.
% focus on learning a good metric in feature space such that data samples of same classes have lower distances while samples of different classes have larger distances.

% Li~\etal~\cite{li2020diff} proposed a differentially private algorithm for securing parameter transfer across devices or learning stages. Though they consider FL and FSL as two separated applications of their technique, their goal is to secure data privacy during model sharing instead of performing FSL with federated devices.

To our best knowledge, training FSL models on distributed devices is still an under-explored open problem. 
The first work of this topic was from
Chen~\etal~\cite{chen2018fedmeta} who explored federated meta-learning by applying FedAvg on meta-learning approaches such as MAML~\cite{finn2017model} in a straightforward way. However, their goal is to improve supervised learning by better sharing models among
%which can learn on unseen tasks (i.e., transferable knowledge) in
federated clients, instead of learning few-shot tasks.
% Hereby, they evaluated their models with conventional supervised learning settings instead of few-shot setting, i.e, they
%partitioned data by different users while trained and evaluated the models on different partitions. 
They neither evaluated their models on FSL tasks, nor explicitly considered dealing with the underlying data heterogeneity in different devices (e.g., non-IID case) which can severely harm FSL.
Our model captures the idea of federating transferable knowledge
among distributed clients. We further explore the practical data-scarce scenarios and evaluate our models on challenging benchmark FSL datasets.
In addition, we explicitly resolve the data heterogeneity issue by proposing a family of more effective meta-learning approaches designed for federated settings.

The other work that loosely connects FSL with FedL
is Li~\etal~\cite{li2020diff}, which proposed a differentially private algorithm for securing parameter transfer across devices or learning stages. The authors considered FedL and FSL as two separated applications of their technique, and their goal is to secure data privacy during model sharing instead of performing FSL with federated devices.

% Directly applying existing FL approaches (e.g., FedAvg~\cite{fedavg}) on FSL would be less capable as FSL procedures depend heavily on locally sampled tasks which could be distinct due to data heterogeneity. To coordinate the training of client models, we propose to minimize the divergence of predictions between each client and the central model. 


There are two recent studies relating to the techniques that we utilize to coordinate client model training and to learn the consistent feature space, though neither of them considered few-shot learning.
Yu~\etal~\cite{yu2020salvaging} used knowledge distillation~\cite{hinton2015distilling} (a variant of KL-divergence) in federated learning to learn different client models to better fit local data. 
%Their goal was to use a teacher model to better train student models. 
Our approach is different from theirs such that 1) they did not perform model aggregation as they want to train client models that fit on local data, but our method performs aggregation for learning a unified global model, 2) their method followed the normal supervised learning paradigm which cannot solve few-shot tasks, and 3) they did not consider feature space learning but our method does.
Zhang~\etal~\cite{zhang2018mutual} proposed to minimize the KL-divergence of every client model pair to enhance ensemble learning. However, this imposes heavy computation and communication costs in distributed scenarios. On the contrary, we propose to approximate the client pairwise KL-divergence with the divergence between the client model and the federated global model, which integrates into FedL seamlessly.


It's also worth to clarify that several recent studies~\cite{guha2019one, salehkaleybar2019one, shin2020xor} 
focused on reducing the communications of distributed learning with one or a few communication rounds under federated learning settings. Though this topic is also referred as ``few-shot federated learning", it's totally different from our work which studies federated learning under data-scarce scenarios.




% use a novel k-exclusive ensemble model to efficiently compute the mutual information.

% and  for other tasks: the former aims to learn an ensemble of models on the same data while the later aims to learn . Moreover, we design a novel adversarial learning procedure~\cite{goodfellow2014generative, saito2018mcd} formed as a minimax game to further optimize the decision boundaries produced by the models. We summarize relevant approaches with supported features in Table~\ref{tab:checklist} for reference.



% In comparison, we consider a more challenging task of finding an optimal global model that can adapt to various unseen tasks while keep data privacy. Inspired by recent studies of generative adversarial learning, we also 

% Each client has to compute the probability outputs of all training samples and minimizes mutual KL divergence with every other client model. Their method needs a central database and shares the data among all clients at each training step, and imposes high computational cost.
% In contrast, our method does not need to compute the pairwise distance so that is computational efficient, and also ensures data privacy and locality.
% Zhang~\etal~\cite{zhang2018mutual} considered a federated learning variant to learn better client models fit on local data. They proposed to use a federated global model to teach each client model with knowledge distillation~\cite{hinton2015distilling} to avoid overfitting on local data. 
% In comparison, we consider a more challenging task of finding an optimal global model that can adapt to various unseen tasks sampled from global data distribution. Both methods did not consider FSL, though we will 





% Our approach differs from existing work of FL and FSL in that (1) we
% are the first to propose \textit{federated few-shot learning}(FedFSL) which considers learning from federated clients a global model for learning unseen tasks; (2) we consider coordinating the clients by efficiently exchanging mutual information through the global model; (3) we provide a novel adversarial learning approach to effectively optimize the decision boundaries produced by the model for unseen classes.





% the virtue of FSL becomes a great challenge of coordinating training of models in distributed environment with diverse data sources. Firstly, FSL model is sensitive to the underlying training data distribution, as it explores a model that can quickly adapt its parameters to unseen tasks. \hl{Need heavy rephrase}. For example, the most commonly used gradient-based meta-learning approach explicitly searches for a model parameter initialization that could fit for a new task with just a few gradient descent steps. That said, the learned FSL model needs to transfer knowledge from given tasks in training set to unseen tasks, and that distilled knowledge is subject to local data distribution and could be distinct across the clients. In federated learning framework, model aggregation is achieved by purely optimizing local target~\cite{fedavg} and averaging the model weights or further adding a proxy task of enforcing similarity of models across clients~\cite{li2018federated}.  Hereby we propose a novel training objective of Fed-FSL that explicitly optimize towards finding congruent decision boundaries across client models. \hl{I will add 1-2 sentences to finish discussion.}
% We perform experiments on distributed few-shot classification tasks on two benchmark datasets to show its superior performance over existing methods.

% For example, the classic Model-Agnostic Meta-Learning approach (MAML)~\cite{finn2017model} trains model weights by performing gradient descent to adapt model weights to new tasks and then performing evaluation to obtain the error signals which back-propagate to original weights by another gradient descent through adapted weights. Meta-SGD~\cite{li2017meta} extends the MAML by learning both weight initialization and step size at each descent step. MTL~\cite{sun2019mtl} proposes to first pre-train most parts of the model and then fix them while train a few other parts during adaptation to avoid over-fitting. 
% most of the model parameters on original data labels, then fine-tune the rest of parameters during meta-learning to enable the adaptation capacity of the model. The benefit of doing this is to avoid overfitting on few-shot data during meta-learning.

% explain the difficulty of fsl in distributed learning
% However, the virtue of FSL becomes a great challenge of coordinating training of models in distributed environment with diverse data sources. Firstly, FSL model is sensitive to the underlying training data distribution, as it explores a model that can quickly adapt its parameters to unseen tasks. \hl{Need heavy rephrase}. For example, the most commonly used gradient-based meta-learning approach explicitly searches for a model parameter initialization that could fit for a new task with just a few gradient descent steps. That said, the learned FSL model needs to transfer knowledge from given tasks in training set to unseen tasks, and that distilled knowledge is subject to local data distribution and could be distinct across the clients. In federated learning framework, model aggregation is achieved by purely optimizing local target~\cite{fedavg} and averaging the model weights or further adding a proxy task of enforcing similarity of models across clients~\cite{li2018federated}. We will show that directly applying existing federated learning approaches would lead to inferior performance on FSL tasks. Hereby we propose a novel training objective of Fed-FSL that explicitly optimize towards finding congruent decision boundaries across client models. \hl{I will add 1-2 sentences to finish discussion.}
% We perform experiments on distributed few-shot classification tasks on two benchmark datasets to show its superior performance over existing methods.


%% few-shot learning
%% #1
% Few-shot learning (FSL) is receiving more and more research attention in recent years.
% As deep neural networks has achieved great successes in computer vision areas~\cite{krizhevsky2012imagenet, ren2015faster, deng2009imagenet} and natural language processing tasks~\cite{hochreiter1997long,vaswani2017attention}. However, the rising cost of training a modern deep neural network model from scratch on large data sources, more and more researchers now focus on generalizing deep networks to the few-shot setting. Recent commonly adopted methodology towards tackling FSL task is called meta-learning~\cite{vinyals2016matching, finn2017model} which aims to learn models that can extract transferable knowledge from a set of base tasks and apply to learning new tasks. 
% Here we introduce the meta-learning procedures that are commonly adopted in gradient-based state-of-the-art FSL approaches~\cite{finn2017model, sun2019mtl}.
% The key idea is to train a model as a general-purpose feature generator which could produce data representations transferrable to unseen tasks. With a good feature generator, the classifier can be easily adopted to unseen tasks with a few fine-tuning steps of gradient descent on a small amount of data.


% Recent studies towards meta-learning for
% deep neural networks include learning a hand-designed optimizer like SGD by parameterizing it through recurrent neural
% networks. Li~\cite{li2017learning}, and Andrychowicz~\cite{andrychowicz2016learning} studied a LSTM
% based meta-learner that takes the gradients from learner and
% performs an optimization step. Recently, meta-learning
% framework has been used to solve few-shot classification
% problems. [1] used the same LSTM based meta-learner approach in which LSTM meta-learner takes the gradient of
% a learner and proposed an update to the learnerâ€™s parameters. The approach learns both weight initialization and an
% optimizer of the model weights. 


% Recently, many FSL approaches have been proposed based on 
% the meta-learning concept (which is also known as learning-to-learn) that aims to train models on a distribution of existing tasks which can learn transferable knowledge or patterns applicable on new tasks. 
% In general, current FSL approaches fall into several categories.


% Zhang~\etal~\cite{zhang2018metagan} introduced an adversarial method to improve few-shot model by utilizing a conditional generator to generate task-specific fake samples in order to augment training data. However, this approach is not directly applicable to distributed scenario in which different client models have misaligned decision boundaries.


