\section{Experiments and Discussions}
\label{sec:exp}
We first provide details of the model architecture, parameter settings, and datasets that we use in the experiments.
Then we visualize the decision boundaries of our approaches with a toy example. We then demonstrate the performance of our proposed algorithms with two typical few-shot classification tasks -- 5-way 1-shot and 5-way 5-shot -- on three benchmark datasets which cover machine vision and NLP tasks. We will make in-depth discussions.

\subsection{Model architecture} 
We utilize a 12-layer deep neural network (DNN) as our base model (adopted from ResNet-12~\cite{he2016deep} which is commonly used for image classification tasks~\cite{sun2019mtl, oreshkin2018tadam, zhang2018metagan}).
It consists of the feature extractor with 12 convolutional layers and about 5 million parameters in total, as well as two fully-connected layers with a ReLU nonlinearity as the classifier. We update model parameters by Adam solver~\cite{kingma2014adam} with a fixed learning rate $10^{-3}$. We
set the adaptation step size $\alpha=0.01$ in \eqref{eq:adp}, the mutual information weight $\gamma=0.2$, and the stage-1/-2 discrepancy loss weight $\eta=\lambda=0.1$ in \eqref{eq:fed_mi_adv_1} and \eqref{eq:fed_mi_adv_2}.
%We sample 200 episodes for each round of local update.

% We utilize a small 4-layer deep neural network (DNN) as our base model which is commonly used for image classification tasks~\cite{sun2019mtl, oreshkin2018tadam, zhang2018metagan, finn2017model}.
% It consists of the feature extractor $\Theta$ with 4 convolutional layers and about 0.3 million parameters in total, and two fully-connected layers with a ReLU nonlinearity as the classifier $\theta$.

%\subsection{Parameter settings} 
%We simulate the FedL environment and implement DNNs with PyTorch~\cite{paszke2017automatic}.


\subsection{Datasets} 
We briefly describe three benchmark datasets that are commonly used in studying FSL and FedL~\cite{finn2017model,munkhdalai2017meta,li2017meta,sun2019mtl,chen2018fedmeta}.
\begin{itemize}[leftmargin=*]
\item \textbf{\mini}~\cite{vinyals2016matching} is based on a small portion of the full ImageNet images~\cite{deng2009imagenet}.
It has 100 classes of images split to 64/16/20 as train/val/test sets. Each class has 600 images with a resolution of 84 $\times$ 84. 
\item \textbf{FC100}~\cite{oreshkin2018tadam} is based on CIFAR-100 images that has 100 classes split to 60/20/20 as train/val/test splits. Each class has 600 images with a low resolution 32$\times$32. It is more challenging because of the low image quality.
\item \textbf{Sent140}~\cite{caldas2018leaf} is a benchmark federated learning  dataset for 2-way sentiment classification (positive and negative). We sampled from this dataset 10,000 annotated tweets provided by 310 twitter users and split them to train/val/test sets with provided tools. Each tweet has 1-20 English words. We tokenize the sentences and keep only common words which have GloVe representations~\cite{pennington2014glove}. 
\end{itemize}



\subsection{MNIST Example}

We provide a simple example on MNIST digit dataset in Fig.~(\ref{fig:mnist_example}), to visualize and compare the decision boundaries of FedFSL-MI and FedFSL-MI-Adv.
We consider the 5-way 1-shot FSL task here: train a digit classification model on data of digits 0-4, and test its few-shot classification capability on digits 5-9 by observing just one labeled sample per class.
To better visualize the results, we manipulate the feature generator to produce a 2-dim feature for each input digit sample. 
% During testing, we sample 100 batches of digital data of classes of 5-9, each with 1 image for adaptation and 10 images for query per category.

In Fig.~\ref{fig:mnist_example}, 
we plot the testing data samples from digit class 5-9 by projecting their features produced by the feature generators. We also depict the decision boundaries of the classifiers. Data samples of different classes are with different colors. 
We observe that FedFSL-MI-Adv (left) produces more distinguishable decision boundaries than FedFSL-MI (right) as expected. The two algorithms achieve a few-shot classification accuracy of 87.5\% and 83.6\%, respectively.
The least accurate class recognized by FedFSL-MI-Adv is digit '9' (purple) of 73\% correctness rate, with 18\% misclassified as '6' (orange); while for FedFSL-MI is digit '6' (orange) of 64\% correctness rate, with 23\% misclassified as '8' (red). 
This indicates that our adversarial learning approach proposed in Section~\ref{sec:adv} boosts the FedFSL task by constructing a more discriminative and transferable feature space for FSL.


\begin{figure}[t]
\begin{center}
\includegraphics[clip, trim=15 0 0 0, width=0.40\textwidth]{figures/mnist_example.pdf}
\end{center}
\vspace{-10pt}
 \caption{Visualization of decision boundaries of FedFSL-KD and FedFSL-KD-Adv at different epochs.}
\label{fig:mnist_example}
\vspace{-10pt}
\end{figure}




\subsection{Results on benchmark datasets}
We experiment with our proposed three methods (FedFSL-naive, -MI, -MI-Adv) and two additional baselines (FSL-local and FedFSL-prox) for comparison.
\begin{itemize} [leftmargin=*]
\item FSL-local is a non-distributed baseline of training an individual FSL model for each client on local data and averaging their results on the shared testing tasks. 
\item FedFSL-prox is a variant of FedFSL-naive by adding a weight regularization term as FedProx~\cite{li2018federated} in objective.
\end{itemize}
We partition the data samples in IID and non-IID ways. For IID partition, data samples of each class are uniformly distributed to each client. To perform non-IID partition, we follow~\cite{hsu2019measuring,yu2020salvaging} by dividing data samples to all clients class-by-class with Dirichlet distribution of concentration parameter $\alpha = 1.0$. In Fig.\ref{fig:data_nid}, we show an example of such a partition of 64 training classes of miniImageNet on a randomly chosen client, when total device number is 2 to 30.


%\begin{figure}[hbt]
%\begin{center}
%\includegraphics[clip, trim=10 0 10 10, width=0.25\textwidth]{figures/data_dist_device_1.png_train.png}
%\end{center}
%\vspace{-4pt}
%\caption{An example of Non-IID data allocated to 2 to 20 federated devices with Dirichlet distribution $\alpha=1.0$.}
%\vspace{-8pt}
%\label{fig:data_nid}
%\end{figure}

\makeatletter\def\@captype{figure}\makeatother
\hspace{-15pt}
\begin{minipage}{0.22\textwidth}
\centering
\includegraphics[scale=0.28, trim=10 20 0 0]{figures/data_dist_device_1.png_train.png}
\vspace{-13pt}
\caption{Non-IID data.}
\label{fig:data_nid}
\end{minipage}
\makeatletter\def\@captype{table}\makeatother
\hspace{-5pt}
\begin{minipage}{.26\textwidth}
\centering
\small
\vspace{10pt}
\begin{tabular}{lcc} \toprule  
\multirow{2}{*}{\textbf{Method}} & \multicolumn{2}{c}{\textbf{Non-IID}} \\ 
\cmidrule{2-3}     & 1-shot  & 5-shot \\ \toprule
FedFSL-local & 59.70\% & 66.68\% 	\\
FedFSL-naive &  68.85\% & 70.62\%	\\
FedFSL-prox &  70.77\% & 72.25\%	\\
FedFSL-MI  &  70.37\% &  73.25\% \\
FedFSL-MI-Adv  & \textbf{71.35\%} & \textbf{76.00\%}   \\  \midrule \bottomrule
\end{tabular}
\vspace{-2pt}
\caption{Sent140 results.}
\label{tab:res_sent140}
\end{minipage}



\begin{table}[htb]
\caption{Results on benchmark datasets.}
\small
\begin{subtable}{\linewidth}
{
\begin{center}
\begin{tabular}{lcccc} \toprule  
\multirow{2}{*}{\textbf{Method}} & \multicolumn{2}{c}{\textbf{IID}} & \multicolumn{2}{c}{\textbf{Non-IID}} \\ 
\cmidrule{2-3} \cmidrule{4-5}    & 1-shot   & 5-shot  & 1-shot   & 5-shot \\ \toprule
FSL-local  & 50.83\%	&  67.47\% & 48.08\% & 63.25\%	 \\
FedFSL-naive~\cite{chen2018fedmeta} & 53.00\%	 &  67.63\%	& 49.95\% & 66.11\% \\
FedFSL-prox~\cite{li2018federated} &  53.03\%	 &  69.05\%	& 50.08\% & 68.53\% \\ \midrule
FedFSL-MI (ours)  & 54.98\% & 69.07\% & 51.07\% & 68.57\% \\
FedFSL-MI-Adv (ours)  & \textbf{56.42\%}	&	\textbf{70.92\%}  & \textbf{53.69\%}& \textbf{69.61\%}    \\  \midrule \bottomrule
\end{tabular}
\end{center}
}
\vspace{-2pt}
\caption{MiniImageNet results.}
\end{subtable}

\begin{subtable}{\linewidth}
{\begin{center}
\begin{tabular}{lcccc} \toprule  
\multirow{2}{*}{\textbf{Method}} & \multicolumn{2}{c}{\textbf{IID}} & \multicolumn{2}{c}{\textbf{Non-IID}} \\ 
\cmidrule{2-3} \cmidrule{4-5}    & 1-shot   & 5-shot  & 1-shot   & 5-shot   \\ \toprule
FSL-local &  36.45\% 	&  47.68\% & 35.90\% & 52.93\%	   \\
FedFSL-naive~\cite{chen2018fedmeta} & 38.42\%	 & 49.97\%	 & 36.11\% & 51.58\%	\\
FedFSL-prox~\cite{li2018federated} & 37.62\% & 48.99\%	& 36.95\% & 53.02\% \\ \midrule
FedFSL-MI (ours)  & 39.78\%	& 50.65\% & 38.08\% & 52.98\% \\
FedFSL-MI-Adv (ours)  & \textbf{40.22\%}	& \textbf{51.18\%}	 & \textbf{38.51\%} & \textbf{54.43\%}       \\ \midrule\bottomrule
\end{tabular}
\end{center}}
\vspace{-2pt}
\caption{FC100 results.}
\vspace{-10pt}
\end{subtable}

\label{tab:res_iid}
\end{table}


\subsubsection{Results on Image Classification}
In Table~\ref{tab:res_iid}, we compare our methods on miniImageNet (a) and FC100 (b) with 1-shot / 5-shot tasks learned by a federation of 10 clients. 
We observe that
\begin{itemize}[leftmargin=*]
\item \emph{FedFSL-MI-Adv outperforms others}. For both IID and non-IID case, FedFSL-MI-Adv consistently outperforms others on both 1-shot and 5-shot tasks for both datasets. 
\item For 1-shot and 5-shot IID task on miniImageNet, FedFSL-MI-Adv achieves the best accuracy of 56.42\% and 70.92\%, which outperforms the second best FedFSL-MI by more than 2.6\% and 2.7\% respectively and relatively. A similar trend has also been observed for FC100.
%: for 1-shot and 5-shot tasks,  FedFSL-MI-Adv achieves the best accuracy of 38.01\% and 48.26\%, outperforming the second best FedFSL-MI by more than 7.8\% and 5.1\%, respectively.
\item For non-IID task, FedFSL-MI-Adv outperforms the second best FedFSL-MI by more than 5\% and 3.1\% in 1-shot case on miniImageNet and FC100 datasets respectively, and outperforms FedFSL-naive by more than 7.2\% and 5.5\% respectively. This indicates that our designed modules indeed help achieve a better federated model especially for non-IID case.
%\item Using distributed data always outperforms the non-distributed method FSL-local. The FSL capacity gains further by enforcing model consistency such as FedFSL-MI, and adding feature learning such as FedFSL-MI-Adv.
\item For FC100 5-shot task,  FedFSL-MI-Adv performs better on non-IID partitions than IID partitions, with an accuracy 54.43\% (non-IID) v.s. 51.18\% (IID) shown by last line in Table~\ref{tab:res_iid}(b). One explanation is that non-IID partitions force each client model to learn on distinct local tasks where certain data classes get sampled more times and thus get represented well. As FedFSL-MI-Adv further aligns the feature spaces of all clients, we could derive a more representative joint feature space with the global model.
% \item FedFSL-MI-Adv consistently outperforms FedFSL-MI, by margins of more than 5.8\% and 5\% for 1-shot and 5-shot tasks on miniImageNet and even more on FC100. This shows again that the adversarial learning strategy boosts the FedFSL tasks significantly.
\item Using FedProx~\cite{li2018federated} performs no better than our FedFSL-MI. This is because FedProx directly constrains client model weights to be closer to global model, while our FedFSL-MI softly optimizes the model outputs of them to be closer, which makes the training end-to-end and easier to optimize.


% FSL-local performed  non-distributed and does not jointly train a central model with all data sources. FedFSL-MI-Adv surpasses FSL-local by 12.5\% (46.82\% v.s. 41.64\%) relatively for 1-shot task, and 16.6\% (61.17\% v.s. 52.4\%) relatively for 5-shot task on miniImageNet, and that relative advantages are 15\% and 21.9\% on FC100. This result reinforces our motivation that the study of distributed FSL is imperative and beneficial especially for mobile computing scenarios with scarce and distributed data sources.
% \item FedFSL-MI and FedFSL-prox achieved comparable results for both 1-shot and 5-shot tasks on both benchmark datasets. This could be explained by that both approaches have proximal term to regularize  client models with their probabilistic outputs \eqref{eq:fed_maml_mi} or model weights to be closer to the global model.
\end{itemize}


\begin{figure}[t]
\centering
\begin{subfigure}{.46\textwidth}
\includegraphics[clip, trim=0 0 20 0, width=1.0\textwidth]{figures/device_num.png}
\end{subfigure}
\vspace{-8pt}
\caption{FSL accuracy of 1-shot task on MiniImageNet w.r.t. number of devices in federation.}
\vspace{-8pt}
\label{fig:device_number}
\end{figure}




%\makeatletter\def\@captype{figure}\makeatother
%\begin{minipage}{0.22\textwidth}
%\centering
%\hspace{-30pt}
%\includegraphics[scale=0.28, trim=0 0 20 0]{figures/data_dist_device_1.png_train.png}
%\vspace{-2pt}
%\caption{Non-IID data.}
%\label{fig:data_nid}
%\end{minipage}
%\makeatletter\def\@captype{table}\makeatother
%\begin{minipage}{.26\textwidth}
%\centering
%\small
%\begin{tabular}{lcc} \toprule  
%\multirow{2}{*}{\textbf{Method}} & \multicolumn{2}{c}{\textbf{Non-IID}} \\ 
%\cmidrule{2-3}     & 1-shot  & 5-shot \\ \toprule
%FedFSL-local &  66.68\% & \%	\\
%FedFSL-naive &  68.85\% & 70.62\%	\\
%FedFSL-naive &  70.77\% & 72.25\%	\\
%FedFSL-MI  &  70.37\% &  73.25\% \\
%FedFSL-MI-Adv  & \textbf{71.35\%} & \textbf{ 76.00\%}   \\  \midrule \bottomrule
%\end{tabular}
%\vspace{-2pt}
%\caption{Sent140 results.}
%\label{tab:res_sent140}
%\end{minipage}




\subsubsection{Results on Text Classification}
In Table~\ref{tab:res_sent140}, we compare our methods on Sent140 dataset with 1-shot / 5-shot tasks learned by a federation of 5 clients. 
Following the provided tool of partitioning the dataset, we distribute different users' data to each client without replacement. Since the data distributions vary for users, this sampling process provides non-IID data partitions. Our goal is to train an effective global sentiment classification model on one portion of users, which can be used to detect the sentiment on disjoint new users. This is particularly challenging because different users can use very distinct words and exclamations to express feelings, and many of them are rare.

In this task, our backbone model is a GRU (RNN) network with hidden size 128. We convert tweet sentences to sequences of 300-D GloVe~\cite{pennington2014glove} word vectors as input to the GRU model. We add a binary classifier upon GRU hidden output to classify negative and positive sentiment.
We examine the performance of baselines and our models and observe that
\begin{itemize}[leftmargin=*]
\item FedFSL-MI-Adv outperforms the other approaches in this natural language understanding task, similar as in image classification task. It also shows that our FedFSL framework can be applicable to both CNN and RNN models.
\item We found that the performance increases from 1-shot to 5-shot tasks are generally less than image classification tasks, e.g., less than 5\% on Sent140 while more than 10\% on miniImageNet. This is because the few-shot labelled sentences can only provide a few more words to help adapt to a user's emotion, while images can provide much richer details and patterns of a given object.
\end{itemize}

%\begin{wraptable}{r}{4.3cm}
%\small
%\begin{tabular}{lcc} \toprule  
%\multirow{2}{*}{\textbf{Method}} & \multicolumn{2}{c}{\textbf{Non-IID}} \\ 
%\cmidrule{2-3}     & 1-shot  & 5-shot \\ \toprule
%FedFSL-local &  66.68\% & \%	\\
%FedFSL-naive &  68.85\% & 70.62\%	\\
%FedFSL-naive &  70.77\% & 72.25\%	\\
%FedFSL-MI  &  70.37\% &  73.25\% \\
%FedFSL-MI-Adv  & \textbf{71.35\%} & \textbf{ 76.00\%}   \\  \midrule \bottomrule
%\end{tabular}
%\vspace{-2pt}
%\caption{Sent140 results.}
%\vspace{-20pt}
%\label{tab:res_sent140}
%\end{wraptable}
%
%\begin{itemize}[leftmargin=*]
%\item \hl{TODO}
%\end{itemize}







%\begin{figure}[t]
%\centering
%\begin{subfigure}{.46\textwidth}
%\includegraphics[clip, trim=0 0 20 0, width=1.0\textwidth]{figures/fraction_iid.png}
%\end{subfigure}
%\vspace{-2pt}
% \caption{Training losses w.r.t. optimization rounds.}
%\label{fig:ablation_fraction}
%\vspace{-10pt}
%\end{figure}


%\subsubsection{Convergence analysis}
%In Fig.~\ref{fig:ablation_fraction}, we show the training loss of 5-way 1-shot task with respect to the number of optimization rounds for  FedFSL-MI and FedFSL-MI-Adv.
%So far we have assumed a full client participating in each round of optimization. We also test with different fractions $C=\{0.2, 0.5, 1.0\}$ of participating clients for comparison. 
%\begin{itemize}[leftmargin=*]
%\item We observe that FedFSL-MI ($C=1$) converges as early as around 30-th communication round but with a higher training loss. In contrast,  FedFSL-MI-Adv (C=1.0) converges slightly slower at around 35-th round with a much lower training loss, which indicates that constructing feature space requires more training rounds to converge.
%\item Also, the smaller $C$, the slower both approaches converge. For $C=0.2$, both methods fail to converge within 60 epochs and the training losses remain high. This seems to be reasonable as each participating device has only 60 samples per class, and when each round only involves 2 clients, the model severely overfits to the few data, and the models become quite distinct which leads to divergence.
%\item Interestingly, we found that FedFSL-MI ($C=0.5$) achieves a slightly lower training loss than $C=1.0$. This may be explained again by the reason of complex decision boundary encouraged by FedFSL-MI objective in \eqref{eq:fed_maml_mi} and Fig.~\ref{fig:demo_mcd}(b): more participating clients lead to more complex decision boundaries that are not useful for new tasks. However, this was not found in FedFSL-MI-Adv, as we optimize the feature space instead of the classifiers.
%\end{itemize}






\subsubsection{Different device number}
We study the trend of accuracy of FedFSL with different number of participating devices $K=2,5,10,20,30$, as well as $K=1$ to simulate complete centralized training.
We illustrate the results for non-IID 5-way 1-shot task with miniImageNet  in Fig.~\ref{fig:device_number} with detailed numbers.
Note that the more participating devices, the fewer training samples each device holds.
% as we uniformly partition the data. For miniImageNet, each data class has 600 samples in total. With different $K$, there are $\lfloor \frac{600}{K} \rfloor$ images per class for each device, which ranges
%from 20 (when $K=30$) to 300 (when $K=2$).  
We observe that
\begin{itemize}[leftmargin=*]
\item The overall trend is that more participating devices yielded decreased accuracy for all 3 approaches. The task becomes more difficult when $K$ increases as the device coordination grows harder and the client model becomes less capable with less training data.
\item \emph{FedFSL-MI-Adv still achieves the best results} on all cases, leading the second best FedFSL-MI by more 2-5\% relatively.
\item The performance of \emph{FedFSL-MI-Adv decreases more slowly than other approaches} with the increase of $K$, which indicates the beneficial of learning a consistent feature space over the clients.
%\item \emph{Centralized training does not significantly outperforms distributed training.} 
\item \emph{FedFSL-MI-Adv in 2-device federation works even better than 1-device centralized training}, with accuracy 54.96\% v.s. 54.41\%. Note that the total training samples of each device get halved on each device when $K=2$. The surprising result that distributed training outperforms centralized training can be explained that FSL is aiming to learn with very few training samples, instead of fitting a task with many samples as in supervised learning. Therefore, FSL is relative less sensitive to the number of examples in \emph{base} classes on each client. Moreover, by utilizing our approach to align decision boundaries well, the two client models form an effective ensemble to enhance the overall performance, compared with a single-model case.
\end{itemize}

\subsubsection{Mutual information with $k$-exclusive global model}
\label{sec:ab_mi}
In Section~\ref{sec:fed_maml_mi}, we discussed that we can estimate the mutual information produced by $w_{-k}^t$ with the global model $w^t$ when the number of devices are large. We conducted experiments to compare using the original $w_{-k}^t$ with $w^t$ in (\ref{eq:fed_maml_mi}).
Surprisingly, we found that using $w^t$ either outperforms or ties with using $w_{-k}^t$ as original definition in both 1-/5-shot tasks in all datasets, with the only exception of miniImageNet 5-shot task (69.61\% v.s. 70.70\%). 
We found that this phenomenon could be explained by``self-knowledge distillation" such that a trained model can be used to improve itself~\cite{yun2020regularizing, zhang2020selfdistillation} as it provides a kind of label smoothing which can make training processes robust to noises. 
