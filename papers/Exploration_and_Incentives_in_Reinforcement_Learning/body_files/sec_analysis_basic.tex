%!TEX root = ../main-anon.tex

We present analysis for the basic case of deterministic MDPs and reward-independent priors (defined below), retaining the main ideas while avoiding some heavy technicalities. The general case is deferred to Section~\ref{sec:extension}.

\begin{definition}[Reward-Independence]\label{defn:reward_independence} A prior $\prior$ over $\modtotal$ is called \emph{reward-independent} if mean rewards $r_{\modst}\xah$ are independent across $\xah$ triples, and jointly independent of the transition probabilities. (The transition probabilities can be arbitrarily correlated across the $\xah$ triples.)
\end{definition}


We capture the dependence on the prior via these two parameters:
\begin{align}\label{eq:fmin_argmin}
f_{\min}(\eps) &:= \min_{x,a,h}\quad
    %\Prop_{\modst \sim \prior}
    \Pr\sbr{ \sfr_{\modst}\xah \le \eps}
\quad\text{and}\quad
r_{\min} := \min_{x,a,h}\quad
    %\Expop_{\modst \sim \prior}
    \Exp\sbr{ \sfr_{\modst}\xah }.
\end{align}
Both parameters take the worst case across all $\xah$ triples. Here, $r_{\min}$ is simply a uniform lower bound on the prior mean rewards. Because of reward-independence, the probability of the punish-event is lower-bounded as
    $\Pr\sbr{\EvPun} \geq \rbr{f_{\min}(\epspunish)}^{-SAH}$.

Our main result guarantees \traversal in the number of episodes that is exponential in $SAH$.

\begin{theorem}\label{thm:det_mdp}
Consider a reward-independent prior $\prior$ supported on deterministic MDPs. Assume that $r_{\min}>0$ and
    $\calC  := f_{\min}\rbr{ \epspunish}>0$,
where
    $\epspunish = r_{\min}/2H$.
Consider \Cref{alg:MDP_HH} with punishment parameter $\epspunish$,  and
    $\nphase = \ceil{ 6H\,r_{\min}^{-1}\,\calC^{-SAH} }$.
The algorithm visits a new $\xah$ triple in every phase, until all reachable triples are visited. This takes at most this many episodes:
    \[ K = \calC^{-SAH}\cdot O\rbr{ SAH^2\, r_{\min}^{-1} }.\]
\end{theorem}

\begin{remark}The two assumptions in \Cref{thm:det_mdp} are in line with prior work. Indeed, let us specialize to multi-armed bandits with independent priors, and let $\sfr(a)$ denotes the mean reward of arm $a$. Then $\xah$ triples specialize to arms, which are trivially $\rho$-explorable. Exploring all arms in this setting requires $r_{\min}>0$ and $f_{\min}(r_{\min})>0$, according to the characterization in \citet{Selke-PoIE-ec21}.
\end{remark}

The rest of this section proves \Cref{thm:det_mdp}. On a high level, an agent is much more likely to face an exploitation episode than a hallucination one. So, even when shown the hallucination ledger, the agent would believe that most likely it is the honest ledger. Therefore, the agent would believe that the rewards from fully-explored $\xah$ triples are indeed small, and consequently select policies which aim to explore under-explored $\xah$ triples.

The formal proof is structured as follows. In \Cref{sec:canon}, we discuss \emph{ledger hygiene}, a crucial property of our ledgers which underpins the rest of the analysis. \Cref{sec:one_step} analyzes a single phase of \mdphh (\Cref{prop:mdp_hh}). Building on that, \Cref{sec:det_transitions} provides a self-contained proof of \Cref{thm:det_mdp}. The details are deferred to the appendices.

\subsection{Canonical Posteriors and Ledger Hygiene}
\label{sec:canon}

We want legers to be interpretable on the face value, regardless of the process used to generate it. We capture this property via the notion of \emph{canonical posterior} $\Prcan\sbr{ \cdot \mid \ledger }$, whereby we pretend that a given ledger $\ledger$ is constructed by an algorithm which deterministically chooses the policies in $\ledger$. Then, we turn to \emph{random ledgers}, \ie ledger-valued random variables. We define \emph{ledger hygiene}, a  property which asserts that the posterior given a random ledger is in fact the canonical posterior. While this is a non-trivial property, we show that the censored and honest ledgers in \mdphh satisfy this property.

We introduce an abstract notion of a \emph{partially-censored ledger} $\ledger$. Formally,
it is any dataset of a particular shape: a sequence of (policy, partial-trajectory pairs) $(\pi_k,\tau_k)$,  along with a \emph{censoring set} $\calU_{\ledger} \subset [S]\times[A]\times[H]$. The reward information is censored out from all trajectories for all triples $\xah\in \calU_{\ledger}$; that is, each $\tau_k$ is of the form $(x_h,\,a_h,\, \tilde{r}_h,\,h)_{h\in[H]}$, where $\tilde{r}_h \in [0,1]$ records a reward for uncensored triples $(x_h,a_h,h) \notin \calU_{\ledger}$, but $\tilde{r}_h = \bot$ replaces the reward value with a special censoring symbol for triples in $\calU_{\ledger}$.
%\footnote{Formally, when some reward value is ``censored out" in a ledger, it is replaced with a special symbol such as $\bot$.}
 The ledger is called \emph{totally-censored} if
    $\calU_\ledger = [S]\times[A]\times[H]$, so that no reward information is recorded.

Consider a partially-censored ledger $\lambda$ which stores $n$ policy-trajectory pairs, and let $\pi_1 \LDOTS \pi_n$ be the respective policies. Consider a new, non-adaptive algorithm which interacts with MDP model $\modst$, proceeds for $n$ episodes, chooses policy $\pi_k$ in each episode $k\in [n]$, and observes some trajectory, denoted $\traj_k$. Let $\Lambda$ be a random ledger with the same censoring set $U_\lambda$ and with policy-trajectory pairs
    $(\pi_k,\traj'_k)$, $k\in[n]$,
where $\traj'_k$ denotes trajectory $\traj_k$ in which all reward data for triples $\xah\in U_\lambda$ is censored out.


We define $\Prcan\sbr{ \cdot \mid \ledger }$ as a distribution over $\modtotal$ obtained  by conditioning the prior $\prior$ on the event
$\cbr{\Lambda = \ledger}$:
\begin{align}\label{eq:canon-posterior-defn}
\Prcan\sbr{\modclass \mid \ledger }
    &:= \Prop \sbr{ \modst \in \modclass \mid \Lambda = \ledger }
    \quad\forall \text{ measurable }\modclass\subset\modtotal.
\end{align}
For a particular event $\modclass$,
    $\Prcan\sbr{\modclass \mid \ledger }$
is called the \emph{canonical probability} given $\ledger$. This probability is ``canonical'' in the sense that it corresponds to the distribution of a non-adaptive algorithm which places all of its past trajectories (appropriately censored) into the ledger $\Lambda$. Given a measurable function $f:\modtotal\to \R$, we define the \emph{canonical expectation}
    $\Expcan\sbr{f(\modst)\mid \ledger}$
given $\ledger$ as the expectation of $f(\cdot)$ over distribution $\Prcan\sbr{ \cdot \mid \ledger }$.
Now we are ready to define ledger hygiene.

\begin{definition}\label{def:hygiene}
A random ledger $\ledger$ is called \emph{hygienic} if it satisfies
\begin{align}\label{eq:hygiene}
 \Pr\sbr{\modst \in \modclass \mid \ledger}  = \Prcan[\modclass \mid \ledger ]
 \quad\forall \text{ measurable }\modclass\subset\modtotal.
\end{align}
\end{definition}

One can construct numerous examples of non-hygenic ledgers, see \Cref{app:canonical_vs_mechanism}. \Eg, policies in the ledger could encode more information about $\modst$ than the canonical posterior can extract.
%$\hat{\ledger}$ in ways that communicate more information than the canonical conditional, which treats those policies as fixed and therefore uninformative.
%Nevertheless, we show that the censored and honest ledgers in \mdphh are indeed hygienic.

\begin{lemma}\label{lem:data_hygiene}
Censored ledger $\ledcensl$ and honest ledger $\ledhonl$ are both hygienic.
\end{lemma}

\begin{proof}[Proof Sketch]
The essential property we use is that the policies in $\ledcensl$, and the censoring set $\calU_{\ell}$ for $\ledhonl$, are determined exactly by $\ledcensl[\ell-1]$, that is, the visited triples $\xah$ from previous hallucination episodes. Thus, $\ledcensl$ and $\ledhonl$ do not depend on transition data which are not in their own ledgers, and do not communicate reward information (because reward information is only explicitly used on non-hallucination episodes). The formal proof is given in \Cref{proof:lem_data_hygiene}.
\end{proof}

%\subsection{ One-Step Hidden Hallucination Mechanism}
\subsection{Analysis for a Single Phase}
\label{sec:one_step}

%We now state a single-phase exploration guarantee which forms the cornerstone of our analysis. The goal of this analysis is to

We derive conditions under which an agent selects a policy from some desired subset $\Pi \subset \Pitotal$.


%\begin{align}\label{eq:canon-value-defn}
%\valuecan{\Pi}{\ledger} :=  \max_{\pi \in \Pi}\; \Expcan[\valuef{\pi}{\modst} \mid \ledger].
%\end{align}


To state these conditions, we introduce the notion of \emph{canonical gap}. The \emph{canonical value} of policy $\pi$ given ledger $\ledger$ is defined as
    $\Expcan[\valuef{\pi}{\modst} \mid \ledger]$.
The canonical gap for policy set $\Pi$ is the difference in maximal canonical value between $\Pi$ and its complement.

\begin{definition}[Canonical Gap]\label{defn:canonical_gap}
The canonical gap of policy set $\Pi \subset \Pimarkov$ given ledger $\ledger$ is
\begin{align*}
\gapcan[\Pi \mid \ledger] :=
    \max_{\pi \in \Pi}\; \Expcan[\valuef{\pi}{\modst} \mid \ledger]
    \;-\;
    \max_{\pi \not\in \Pi}\; \Expcan[\valuef{\pi}{\modst} \mid \ledger]
\end{align*}
\end{definition}

%Note that the canonical gap depends only on the value of $\ledger$, but not on the process that generated it.

The meaning  behind this definition is that if an agent were to observe a hygienic ledger $\ledger$ with a positive canonical gap, this agent would choose some policy in $\Pi$.

Our algorithm construct hallucinated ledgers $\ledger = \ledhall$ so as to yield a positive canonical gap, and we'd like to conclude that an agent would choose some policy in $\Pi$ in the hallucination episodes (which would constitute progress towards our exploration goal). Unfortunately, random ledgers $\ledger_k$ revealed by our algorithm are not hygienic, precisely because of hallucination episodes. We circumvent this issue if
    $\gapcan\sbr{\Pi \mid \ledhall}$
is not only positive, but much larger than $1/\nphase$, where $\nphase$ is the phase duration. We formulate a condition which also depends on the punish-event $\EvPun$:
\begin{align}\label{eq:HH_good_condition}
3H/\nphase \le \Prcan\sbr{\EvPun \mid \ledcensl} \cdot \gapcan\sbr{\Pi \mid \ledhall}.
\end{align}
The essence of Hidden Hallucination as a technique is that this condition suffices.

%Specifically, we show that if there is a lower bound on (a) the  canonical gap for a class $\Pi$ under $\ledhall$ and (b) the canonical probability of $\EvPun$ given the totally censored ledger $\ledcensl$, then the agent will select still policy $\pi \in \Pi$.


\begin{proposition}[Hidden Hallucination]\label{prop:mdp_hh} Let $\Pi \subset \Pimarkov$ be any subset of policies. Fix phase $\ell$ in the algorithm. A policy in $\Pi$ is chosen in the hallucination episode if condition \eqref{eq:HH_good_condition} holds.
%and suppose the hallucinated ledger  $\ledhall$ and phase length $\nphase > 0$ satisfy:
%Then for $k = \kexpl$, any Bayes greedy policy $\pi_k$ must lie in $\Pi$.
\end{proposition}

Full proof is in \Cref{proof:prop_mdp_hh}. To use this proposition, we will establish uniform (\ie data-independent) lower bounds on both terms on the right-hand side of \eqref{eq:HH_good_condition}, and set $\nlearn$ accordingly.

\begin{proof}[Proof Sketch]
Fix episode $k$. The proof readily reduces to bounding the conditional probability that $k$ is the hallucination episode, given the revealed ledger $\ledger_k$. Intuitively, we need to bound the agents' belief that they are facing a hallucinated ledger rather than an honest one. Our argument is inspired by the analysis of hidden exploration in \cite{ICexploration-ec15}, but differs in a number of respects; notably, the role of ledger hygiene.

Recall that the censored ledger $\ledcensl$ comprises all data observed by agent $k$ that is known to be faithfully transmitted by the algorithm. We condition on $\ledcensl$, and verify that
\begin{align}\label{eq:Prhall_ledhall}
\Pr[\; k=\kexpl \mid \ledcensl \;] = \Pr[\; k\neq\kexpl \mid \ledcensl,\, \EvPun\;].
\end{align}
In words, agent $k$ finds it equiprobable that she is in a hallucination episode and that she is shown an honest ledger but the true model lies in the punish-event.
%thus, when shown hallucinated ledger, the agent may either believe they are on a hallucination episode $k = \kexpl$, or simply that it happened to be the case, by chance, that $\modst \in \modclass_{\ell}(\epspunish)$, but that $k \ne \kexpl$.

When $1/\nphase$ is very small, the probability of $k = \kexpl$ is dominated by the probability of $\EvPun$, and \Cref{eq:HH_good_condition} quantifies exactly how small $1/\nphase$ must be. Note that this condition is stated in terms of canonical conditionals, which suffices because the censored and honest ledgers $\ledhonl$ and $\ledcensl$ are hygienic.
\end{proof}
%In general, whether or not the condition \eqref{eq:HH_good_condition} holds is a random event, depending in part on (a) the samples comprising $\ledcensl$ and (b) the randomness used in hallucinating $\ledcensl$. For simplicity,

%\newcommand{\moddet}{\calM_{\mathrm{det}}}
\subsection{Full proof of \Cref{thm:det_mdp}}
\label{sec:det_transitions}

% We analyze \Cref{alg:MDP_HH} in the simplified setting where the prior is reward-independent, and all MDPs in its support have deterministic rewards and transitions.  Recall that $\nlearn = 1$.

Let us start with some notation. Recall that fully-explored (resp., under-explored) $\xah$ triples are now simply the ones that have (resp., have not) been visited at least once during the past-phase hallucination episodes. Let $\underexplored$ denote the set of all $\xah$ triples that are under-explored in a given phase $\ell$. Let $\Pi_{\ell}$ be the set of all policies $\pi\in \Pimarkov$ which deterministically visit some triple $\xah\in \underexplored$ under the true model $\modst$. Note that $\Pi_\ell$ is non-empty if and only if some under-explored $\xah$ triple is reachable.

%Let $\calU_{\ell}$ be the set of all $\xah$ triples which have not yet been visited during the past-phase hallucination episodes; we will call these triples \emph{non-explored}; fully-explored triples we just call \emph{explored}. Let $\Pi_{\ell}$ be the set of all policies $\pi\in \Pimarkov$ which visit any $\xah$ triple in $\calU_{\ell}$ under the true model $\modst$. (This is a deterministic event, since the MDP is deterministic.)

%Define the policy sets $\Pi_{\ell}$, where
%\begin{align}
%\Pi_{\ell} := \{\pi : \pi \text{ visits  a non-explored triple under the true model } \modst \},
%\end{align}
%where we note that, for deterministic MDPs, which triples a given policy visits is deterministic.

We apply \Cref{prop:mdp_hh} with policy set $\Pi = \Pi_\ell$, so as to guarantee that an agent selects some policy in $\Pi_\ell$ in the hallucination episode, and therefore visits some under-explored $\xah$ triple. Then all reachable $\xah$ triples will be visited after at most $SAH$ phases, \ie in at most $SAH \cdot \nphase$ episodes.

We lower-bound the two terms on the right-hand side of \refeq{eq:HH_good_condition}. First, 
reward-independence implies that
%, as per \Cref{rem:reduction-to-independence}, that
\begin{align*}
\Prcan[\EvPun \mid \ledcensl] \ge f_{\min}(\epspunish)^{SAH}.
\end{align*}
Hence, it remains to bound the canonical gap:

\begin{claim}\label{claim:gap_size}
Suppose $\epspunish \le r_{\min}/2H$ and fix phase $\ell$. Then
    $\gapcan\sbr{\Pi_{\ell} \mid \ledhall} \geq r_{\min}/2$.
\end{claim}

Then \Cref{prop:mdp_hh} follows with the choice of $\nlearn$ as in \Cref{thm:det_mdp}, and we are done.

It remains to verify \Cref{claim:gap_size}. To this end, we analyze
    $\Prcan[\;\cdot \mid \ledhall]$,
the canonical posterior given the hallucinated ledger. We prove that any model in the support of this distribution can partially \emph{simulate} the trajectory of any given policy under the true model $\modst$.

\begin{lemma}[Deterministic Simulation Lemma]\label{lem:modclass_not}
Fix phase $\ell$ and policy $\pi \in \Pitotal$. Fix any model $\model$ in the support of
    $\Prcan[\;\cdot \mid \ledhall]$.
Let $h_\pi$ be the first stage at which policy $\pi$ visits any under-explored $\xah$ triple under the fake model $\model$; let $h_\pi=H+1$ this never happens. Then:
\begin{OneLiners}
\item[(a)]
the $\xah$ triples visited by policy $\pi$ in stages $h\leq h_\pi$ are identical under $\modst$ and $\model$.
\item[(b)]
all rewards collected by policy $\pi$ in stages $h<h_\pi$ under model $\model$ are at most $\epspunish$.
\end{OneLiners}
\end{lemma}

\begin{proof}
Since the prior $\prior$ is supported on deterministic MDPs, the fake model $\model$ must have identical transitions and rewards to those in the hallucinated ledger $\ledhall$. By construction, $\ledhall$ contains transitions for all fully-explored $\xah$ triples, and they come from the true model $\modst$. Hence the transitions in models $\model$ and $\modst$ are the same for all stages $h < h_\pi$. Part (a) follows by induction over stages $h \le h_\pi$.

To prove part (b), fix any $\xah$ triple visited by policy $\pi$ in stage $h\leq h_\pi$ under the fake model $\mu$. This triple is fully-explored by part (a). Consequently, this triple is assigned expected reward at most $\epspunish$ in the hallucinated posterior \eqref{eq:algo-halposterior-defn}. Since we only deal this deterministic MDPs, this expected reward propagates as the observed reward in the hallucinated ledger $\ledhall$, and then into the fake model $\model$.
\end{proof}

\begin{proof}[Proof of \Cref{claim:gap_size}]
Denote the hallucinated ledger as $\ledger = \ledhall$. For brevity, we write \emph{fake models} to refer to all models $\model$ in the support of
    $\Prcan[\;\cdot \mid \ledger]$.

First, fix any policy $\pi\not\in \Pi_\ell$. By definition, its trajectory under the true model $\modst$ never runs out of fully-explored $\xah$ triples. By \Cref{lem:modclass_not}, its total reward under any fake model $\model$ is at most $\epspunish H$, which is at most $r_{\min}/2$ by our choice of $\epspunish$. It follows that
\begin{align}\label{eq:claim:gap_size-1}
(\forall \pi\not\in\Pi_\ell) \qquad
    \Expcan[\valuef{\pi}{\modst} \mid \ledger] \leq r_{\min}/2.
\end{align}

Second, fix any policy $\pi \in \Pi_{\ell}$. By definition, its trajectory under the true model $\modst$ visits some under-explored $\xah$ triple; let's focus on the first such triple. Take any fake model $\model$. By \Cref{lem:modclass_not}, policy $\pi$ visits $\xah$ under $\model$, too. Consequently,
    $\valuef{\pi}{\model} \geq \sfr_{\model}(x,a,h)$.
It follows that
\begin{align}
(\forall \pi\in\Pi_\ell)\qquad
\Expcan[\valuef{\pi}{\modst} \mid \ledger]
    &\geq \Expcan[\sfr_{\model}(x,a,h) \mid \ledger] \nonumber \\
    &\geq \ralt
    &\EqComment{by definition of $\ralt$} \nonumber\\
    &= r_{\min}
    &\EqComment{by \Cref{rem:reduction-to-independence}}.
    \label{eq:claim:gap_size-2}
\end{align}
The claim follows by plugging \eqref{eq:claim:gap_size-1} and \eqref{eq:claim:gap_size-2} into the definition of the canonical gap.
\end{proof}
