%!TEX root = ../main-arxiv.tex
\begin{comment}
\newcommand{\valuename}{\textbf{value}}
\newcommand{\valuef}[2]{\valuename(#1;#2)}
\newcommand{\model}{\mu}
\end{comment}
\section{Scratch for Talk}

\begin{OneLiners}
\item[1.] the principal chooses a signal $\signal_k$;
\item[2.] agent $k$ arrives, observes $k$ and $\signal_k$, and chooses a policy $\pi_k$;
\item[3.] this policy is executed in the MDP;
\item[4.] the agent receives reward $\sum_{h\in[H]} r_{k,h}$; \\
the principal observes the resultant trajectory
    $\traj_k = \rbr{ x\kh,\, a\kh,\, r\kh,\, h }_{h\in[H]}$.
\end{OneLiners}

\begin{align*}
\model \sim \prior
\end{align*}


\begin{OneLiners}
\item[1.] a \emph{ledger} $\ledger$ reveals (partial) history of trajectories
\item[2.] \emph{honest ledger} $\ledhonl$ reveals all history from exploration phases $\ell' < \ell$
\item[3.] \emph{censored ledger} $\ledhonl$ reveals all transition history \\
from exploration phases $\ell' < \ell$, but no reward data
\item[4.] hallucinated ledger $\ledhall$ - synthetic history created \\
by changing \emph{reward data} in honest ledger. 
\item[5.] \emph{punishing-event} $\EvPun$ on which \\
all rewards at under-explored triples $\xah$ look ``small'
\end{OneLiners}

\item[2.] agent $k$ arrives, observes $k$ and $\signal_k$, and chooses a policy $\pi_k$;
\item[3.] this policy is executed in the MDP;
\item[4.] the agent receives reward $\sum_{h\in[H]} r_{k,h}$; \\
the principal observes the resultant trajectory
    $\traj_k = \rbr{ x\kh,\, a\kh,\, r\kh,\, h }_{h\in[H]}$.
\end{OneLiners}


\begin{align*}
 \posteriorhall(\cdot) := \Pr\sbr{\modst \in \cdot \mid \ledcensl,\; \EvPun},
 \end{align*}

%find $\pi$ such that $\valuef{\pi}{\model} \ge \max_{\pi'}\valuef{\pi'}{\model} - \epsilon$. 
expectation under transition $x_{h+1} \sim \sfp_{\model}(\cdot \mid x_h,a_h)$


\begin{algorithm}[h]
    \begin{algorithmic}[1]
    \State{}\textbf{Input: }
        phase length $\nphase$, target {\#samples} $\nlearn$, punishment parameter $\epspunish>0$
    \For{each phase $\ell= 1,2,\;\dots$}
    \State{}$\Phase_{\ell} = (\ell-1) \nphase  + [\nphase] \subset \N$
        ~~~\algcomment{\% the next $\nphase$ episodes}
    \State{}Draw ``hallucination episode" $\kexpl$ uniformly from $\Phase_{\ell}$
    %\State{}Draw episode $\kexpl$  uniformly from $\PhaseEpisodes_{\ell}$
    %\State{}Draw $\kexpl \unifsim N(\ell - 1) + [N]$
    \For{each episode $k \in \Phase_{\ell}$}
    \If{$k = \kexpl$}
    \algcomment{\qquad\% hallucination episode}
    \State{}\algcomment{\% censored ledger $\ledcensl$,
        punish-event $\EvPun$}
    \State{}Define distribution $\posteriorhall$ over MDP models by\\ \qquad\qquad\qquad$\posteriorhall(\cdot) := \Pr\sbr{\modst \in \cdot \mid \ledcensl,\; \EvPun}$
    \State{}Draw hallucinated MDP $\modhall$ at random from $\posteriorhall$
    %\State{}Draw $\modelhall\sim \Pr[\modst \in \cdot \mid \ledcensl,\; \EvPun]$
        %\algcomment{\quad\% hallucinated MDP}
     \State{}For each fully-explored $\xah$ triple,
        \algcomment{\quad\% hallucinate rewards}
     \State{}\hspace{2em}each time this triple appears in the ledger,
     \State{}\hspace{2em}draw its reward as prescribed by $\modhall$.
     %\State{}Draw rewards from $\modelhall$ for each fully-explored $\xah$ triple
     \State{}Reveal \emph{hallucinated ledger } $\ledhall$ formed by  \\
     \qquad\qquad \qquad inserting the hallucinated rewards into censored ledger $\ledcensl$.
    \Else{} \algcomment{\qquad\% exploitation}
     \State{}Reveal honest ledger: $\ledrevk \gets \ledhonl$.
    \EndIf
    \State{}Observe the trajectory $\traj_k$ from this episode.
  \EndFor
  \EndFor
  \end{algorithmic}
  \caption{\mdphh}
  \label{alg:MDP_HH}
    \end{algorithm}