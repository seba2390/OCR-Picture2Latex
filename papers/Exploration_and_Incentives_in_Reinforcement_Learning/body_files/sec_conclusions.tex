%!TEX root = ../main-arxiv.tex


This paper combines reinforcement learning (RL) and incentivized exploration, and advances both. From the RL perspective, we design RL mechanisms, \ie RL algorithms which interact with self-interested agents and are compatible with their incentives. This is the first paper on ``RL mechanisms", \ie \textbf{the first paper on any scenario that combines RL and incentives}, to the best of our knowledge. From the incentivized exploration perspective, we extend the learning model in several important ways discussed in the Introduction. We adopt a relatively simple model that captures salient features present in multiple motivating scenarios: namely, agents that have repeated, MDP-like interactions with the environment and need to be incentivized to explore. However, as is quite common in both RL theory and economic theory, we do not attempt to capture all the particularities of any given scenario, and instead focus on understanding the basic model. Aside from the explicit theoretical contributions, we hope that our ``hidden hallucination" technique would be useful as a general principle, both theoretically and heuristically.

%Two concrete follow-up questions within our model of incentivized RL concern exploring the MDP in $K=\poly(SAH)$ episodes, and achieving $\sqrt{K}$ scaling for Bayesian regret (which would be optimal for all RL algorithms, regardless of incentives). However, these questions are not resolved in the prior / concurrent work even in the ``easier" setting of incentivized exploration in bandits with large action sets.%
%\footnote{Bandits with large action sets is a notable sub-area in multi-armed bandits, see \citep{slivkins-MABbook,LS19bandit-book} for background. One typically posits a known structure, \eg linearity, concavity, or Lipschitz-continuity of rewards.}

One concrete follow-up question within our model of incentivized RL concerns exploring the MDP in $K=\poly(SAH)$ episodes. However, this question is not resolved even in the ``easier" setting of incentivized exploration in bandits with correlated priors.%
\footnote{However, \cite{Selke-PoIE-ec21} achieves this for incentivized exploration in bandits with \emph{independent} priors.}

More broadly, we view our work as a ``beachhead" for further investigation. As defined in the Introduction, a model of incentivized RL consists of three components: RL problem,  strategic interactions, and performance objective. Each component can be extended in several different ways (and a given motivating scenario may require several such extensions). For the RL component, one could consider incorporating large action/state sets (with structure such as linearity), partial observations (\ie POMDPs), contextual MDPs (where a \emph{context} is observed before each episode), or non-Markovian dynamics. For the ``strategic" component, one could allow the agents to revise their choices before each stage (rather than once per episode), and support heterogenous agents with public and/or private idiosyncratic signals.%
\footnote{\citet{Jieming-multitypes18} accommodate idiosyncratic signals for incentivized exploration in bandits.} For the ``performance objective", one could study scenarios when some reachable $\xah$ triples cannot be explored in our framework, and redefine \traversal objective to explore all $\xah$ triples that \emph{can} be explored.%
\footnote{Such extensions have been studied in \cite{ICexplorationGames-ec16,Jieming-multitypes18} for ``stateless" incentivized exploration.} Moreover, a mechanism for incentivized RL could, in principle, take advantage of agents' inherent incentives to explore their own MDP, when and if they exist. Finally, regret might improve if exploration is made more adaptive: essentially, it may suffice to explore some of the $\xah$ triples more rarely (or not at all).%
\footnote{Recall that optimal regret in $K$ episodes scales as $\sqrt{K}$, whereas we only achieve $K^{2/3}$ scaling (which is optimal if one separates ``pure exploration" and ``pure exploitation"). It is unclear if $\sqrt{K}$ scaling is achievable in incentivized RL.}

%\medskip
%
%\bluepar{Subsequent work.} Two closely related papers have appeared subsequent to the initial publication of our work on \texttt{arxiv.org}. We discuss these papers below.
%
%\citet{IncentivizedRL-ec22} study another model that combines RL and Bayesian Persuasion.%
%\footnote{Our initial publication on \texttt{arxiv.org} predates theirs by a full year.}
%Their model is similar to ours in that an algorithm for episodic RL needs to incentivize self-interested agents to follow its recommendations. The key difference is that their model focuses on payoff-relevant `outcomes' that are drawn IID before each episode and observed by the algorithm but not to the agents. These `outcomes' are the only source of information asymmetry used by the algorithm to create incentives; the guarantees in \citep{IncentivizedRL-ec22} appear vacuous when the `outcomes' are not observed. In contrast, the only source of information asymmetry in our model (and all prior work on incentivized exploration) is the history of interactions with other agents. An intriguing open question is to combine these two sources so that they reinforce one another.
%
%\citet{CombiIE-neurips22} study a model of incentivized exploration with large, structured action set and correlated priors.%
%\footnote{Our initial publication on \texttt{arxiv.org} predates theirs by more than a year, and acknowledged by theirs as prior work.}
%Their learning problem is combinatorial semi-bandits (a bandit problem where arms correspond to subsets of ``atoms"). It is stateless and therefore ``easier" than ours. They  achieve stronger results on regret-minimization: essentially, Thompson Sampling is BIC for their model when initialized with $N$ samples of each atom, for some $N$ determined by the prior. In particular, their algorithm attains regret which scales as $\tilde{O}(\sqrt{T})$ in the number of rounds $T$. However, their solution for collecting one sample of each atom takes time exponential in the number of atoms, similar to how the number of episodes for our algorithm is exponential in $SAH$.  