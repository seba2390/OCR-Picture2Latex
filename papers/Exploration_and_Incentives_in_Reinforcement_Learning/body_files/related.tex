%!TEX root = ../main-arxiv.tex
Incentivized exploration was introduced in \citet{Kremer-JPE14} and \citet{Che-13}. All prior work focused on multi-armed bandits as the underlying machine learning problem. The problem is quite rich even under a basic economic model adopted in our paper: prior work has studied
optimal policies for deterministic rewards \citep{Kremer-JPE14,Cohen-Mansour-ec19},
regret-minimizing policies for stochastic rewards \citep{ICexploration-ec15}, and
exploring all arms \citep{ICexploration-ec15,ICexplorationGames-ec16}. Extensions of this model included
heterogenous agents \citep{Jieming-multitypes18,Kempe-colt18},
agents directly affecting each other's payoffs 
\citep{ICexplorationGames-ec16},
information leakage \citep{Bahar-ec16,Bahar-ec19},
relaxed economic assumptions
\citep{Jieming-unbiased18}, time-discounted utilities \citep{Bimpikis-exploration-ms17}, monetary incentives \citep{Frazier-ec14,Kempe-colt18}, and continuous information flow \citep{Che-13}. A survey on incentivized exploration can be found in
\citep[][Ch. 11]{slivkins-MABbook}. % and \citep{IncentivizedExploration-chapter}.

Standard approaches from bandits do not carry over to incentivized exploration without non-trivial modifications, major assumptions, substantial performance loss, and new analyses. Any bandit algorithm can be made BIC by ``hiding'' it among many rounds of exploitation \citep{ICexploration-ec15}.
%plugs into the hidden exploration approach discussed above \citep{ICexploration-ec15}, but each round of this algorithm is hidden among many rounds of exploitation.
Successive Elimination~\citep{EvenDar-icml06} carries over if the rule for eliminating suboptimal arms is revised to depend on the prior \citep{ICexploration-ec15}.
%Recently, \citet{Selke-PoIE-ec21} proved that
Thompson Sampling is BIC when primed with a certain amount of data that needs to be collected exogenously \citep{Selke-PoIE-ec21}. In the first two results, performance loss compared to bandits is exponential in the number of arms. The last two results assume that the prior is independent across arms, which really breaks for Incentivized RL.

% Re the basic objective of exploring all arms:
% 1. not always achievable: simple example
% 2. fighting chance assn
% 3. non-trivial solution (HE) in the general case w/ fighting chance, and in ec16 othw
% 4. optimization non-trivial: KMP for deterministic rewards, SS21 for independent priors.
% 5. Perf loss exp(variance)

The basic objective of exploring all arms is very subtle in incentivized exploration (while trivial in bandits). This objective is not always achievable, even for two arms: it may be impossible to explore arm $2$ when arm $1$ is preferred initially. 
(A simple example: two arms with mean rewards $\mu_1>\mu_2$ and a Bayesian prior on $(\mu_1,\mu_2)$ such that $\mu_1$ is independent of $\mu_1-\mu_2$ \citep{ICexploration-ec15}.) Exploring arm $2$ requires an assumption: that arm $2$ can appear optimal, with some positive probability, after seeing enough samples of arm $1$. Absent such assumptions, the objective can be refined to exploring all arms that the agents can possibly be incentivized to explore.  Achieving either version of the objective takes non-trivial techniques and analyses \citep{ICexploration-ec15,ICexplorationGames-ec16}. Achieving this objective \emph{optimally} is even more subtle. Exact optimality is achieved only for deterministic rewards and only up to three arms \citep{Kremer-JPE14,Cohen-Mansour-ec19}. For randomized rewards, one can achieve optimality up to a polynomial dependence on the lower bound, and only for independent priors \citep{Selke-PoIE-ec21}. The minimal number of rounds needed to visit arm $2$ (with any algorithm) can be arbitrarily large depending on the prior \citep{Selke-PoIE-ec21}.
%it must be (at least) exponential in the variance of the prior for some paradigmatic special cases .

Revealing the algorithm's full history to each agent implements the ``greedy" bandit algorithm which always exploits. This algorithm suffers from linear Bayesian regret, caused by herding on a suboptimal arm. This is a common case: it holds for any Bayesian prior and even for two arms \citep[][Ch. 11]{slivkins-MABbook}. However, the greedy algorithm performs well under strong assumptions on agents' heterogeneity and the structure of rewards \citep{kannan2018smoothed,bastani2017exploiting,Greedy-Manish-18,AcemogluMMO19}.

Incentivized exploration is closely related to two important topics in theoretical economics,
Bayesian Persuasion
\citep{BergemannMorris-survey19,Kamenica-survey19}
and social learning
\citep{Horner-survey16,Golub-survey16}. The former focuses on a single-round interaction between a principal and agent(s), and the latter studies how strategic agents learn over time in a shared environment. Connection between exploration and incentives arises in other domains:
%dynamic pricing
%    \cite[\eg][]{KleinbergL03,BZ09,BwK-focs13},
dynamic auctions
    \cite[\eg][]{AtheySegal-econometrica13,DynPivot-econometrica10,Kakade-pivot-or13},
ad auctions
    \cite[\eg][]{MechMAB-ec09,DevanurK09,Transform-ec10-jacm},
human computation
    \cite[\eg][]{RepeatedPA-ec14,Ghosh-itcs13,Krause-www13},
and competition between firms
    \cite[\eg][]{bergemann2000experimentation,keller2003price,CompetingBandits-merged}.


%As noted above, this is the first work to consider incentivized exploration in a learning setting with dynamics.


We consider a standard paradigm of reinforcement learning (RL): episodic RL with tabular MDPs. Other RL paradigms study MDPs with specific helpful structure such as linearity, MDPs with partially observable state (POMDPs), algorithms that interact with a single MDP throughout, and algorithms that learn over different MDPs (\emph{meta-learning}). While we focus on \emph{exploration}, considerable attention has also been devoted to \emph{planning}
\ie policy optimization given the oracle access to the MDP or full knowledge thereof. The literature on RL is vast and rapidly growing, see the book draft \citep{RLTheoryBook-20} for background.

Tabular episodic RL has been studied extensively over the past two decades; standard references include
\cite{kakade2003sample,kearns2002near,brafman2002r,strehl2006pac,strehl2009reinforcement}.
Optimal solutions have recently been obtained for unknown MDPs, both for policy optimization \citep{dann2017unifying} and for regret minimization \citep{jaksch2010near,azar2017minimax,dann2015sample}.
%Recent work has characterized optimal learning rates for identifying an $\epsilon$-suboptimal policy \citep{dann2017unifying} and obtaining low regret \citep{jaksch2010near,azar2017minimax,dann2015sample} via interacting with an unknown tabular MDP.
%We consider an exploration setting
Our objective of exploring all reachable states is studied in \citep{kearns2002near,brafman2002r}. Moreover, it is essentially a sub-problem in \emph{reward-free RL} \citep{jin2020reward}, where one collects enough data to enable policy optimization relative to any given matrix of rewards. Sample complexity guarantees in prior work are primarily derived in frequentist settings (even when analyzing Bayesian algorithms like Thompson Sampling, as in \citet{pmlr-v40-Gopalan15}). Nevertheless, Bayesian framework, standard for modeling agents' incentives, also informs many practical RL algorithms \citep{ghavamzadeh2016bayesian}.

Multi-armed bandits can be seen as a special case of episodic RL with $H=1$ stages. They received much attention as a basic model for explore-exploit tradeoff, \eg see books \citep{Gittins-book11,Bubeck-survey12,slivkins-MABbook,LS19bandit-book}.

%, and has event begun to understand optimal instance-dependent  \citep{simchowitz2019non,zanette2019tighter,zhang2020reinforcement}. We consider



%The more basic problem of multi-armed bandits received a huge amount of attention over the past few decades. The diverse and  evolving body of research has been summarized in several books:  \cite{Bubeck-survey12,Gittins-book11,slivkins-MABbook,LS19bandit-book}.

