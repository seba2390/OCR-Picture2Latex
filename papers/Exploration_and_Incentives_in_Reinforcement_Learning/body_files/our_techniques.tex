%!TEX root = ../main.tex


%\subsection{Challenges}
%\mscomment{change this: more blunt}
%Our approach is inspired by the Hidden Exploration  algorithm (\HE) due to \cite{mansour2020bayesian}, whose core idea is to hide a small number of rounds intended to encourage exploration amongst a larger number of rounds where the agent exploits all relevant information. Naively, one could consider a blackbox reduction from incentivized MDPs to incentivized multi-arm bandits, where each MDP policy $\pi$ is identified with a single arm $a \in [K]$, and apply \HE{} to this reduction. This suffers from two severe limitations:
%\begin{itemize}
%	\item The cardinality of the number of policies is exponentially large, $A^{SH}$. Hence, a direct application of \HE{} may yield sample complexity \emph{doubly exponential} in relevant problem parameters. \mscomment{how to phrase}
%	\item Unlike multi-armed bandits, in which the priors may be dependent across arms, the rewards associated with different MDP policies are necessarily highly correlated. Hence, there is no apparent way to verify that the assumptions of \HE{} hold. Moreover, this precludes the more sample-efficient algorithms due to \cite{sellke2020sample}, which require an independence assumption that necessarily fails for MDPs.
%\end{itemize}
%Eschewing the black-box reduction, another significant challenge arises in incentived RL, but not in MAB. In MAB, which arms can be feasibly explored is limited only by whether or not the agent can be incentivized to pull that arm; at any given round, the agent is free to pull any arm she deems to maximize her expected reward. In RL, however, different policies may explore different states with different probabilities. \emph{Even the principal} does not know a priori which policies visit which states. \mscomment{Hence, the principal's exploration protocol can not be decided entirely a priori (like \HE), and must learn which actions lead to exploration of novel states. REPLACE with line with thing that adresses E16}Moreover, one must also analyze the agents beliefs about not only the rewards accrued, but which states will be visited.
%\subsection{Our Approach}

In terms of techniques, it is helpful to distinguish between the challenges of exploration in RL, that is, learning which policies visit which states in an MDP, and those of  \emph{incentivized} exploration in RL. We handle  RL exploration by building on the classic \Ecubed{} algorithm \citep{kearns2002near}. This algorithm encourages exploration of new $\xah$ triples by ``punishing" the previously explored $\xah$ triples and ``promoting" the unexplored ones, \ie pretending that the rewards from former are small and rewards from the latter are large, and computing a reward-optimizing policy in the resulted MDP. Our main technical contribution is to ``implement" a version of the \Ecubed{} algorithm within the constraints of incentivized RL.

%At each episode, our algorithm reveals a history-dependent \emph{signal} to the arriving agent. This signal ``punishes" sufficiently explored $\xah$ triples, but does not ``promote" the unexplored ones (because for these $\xah$ triples the agents would trust the Bayesian prior rather than our ``promotion"). Then we compute the reward-optimizing Markovian policy given this signal (and the algorithm itself), and recommend it to the agent.

%Our algorithm reveals partial histories, called \emph{ledgers}, which contain  transition data from certain past episodes and partial reward data for the same episodes. The policy proceeds in phases of equal duration. Episodes within a given phase differ in how they reveal the reward data for sufficiently visited $\xah$ triples. All but one episode are \emph{honest}, revealing this data faithfully. One episode is randomly selected as a \emph{hallucination episode}, where this data is replaced with ``hallucinated'' rewards which are very small.

%The main technical challenge is to convince the agent to believe the hallucinated data. In particular, the hallucination needs to be consistent with the Bayesian prior. This necessitates an indirect sampling procedure whereby we hallucinate the \emph{expected} reward data, then sample an MDP from a Bayesian posterior given this data, and then re-generate reward data consistent with this MDP. To simplify the analysis of the agents' beliefs under this resampling, we design the revealed ledgers to be interpretable in a particularly simple way. Formally, we enforce a property, called \emph{ledger hygiene}, that the Bayesian update on the transition data in the revealed ledger does not depend on the complex process used to collect this data. (This property is spelled out in Section~\ref{sec:canon}.) Together, our resampling procedure and enforcement of ledger hygiene ensure that agents in hallucination episodes believe that most likely they are in an honest episode. At the same time, because the hallucination actively punishes rewards from well-visited $\xah$ triples, the hallucinated ledger incentivizes the new agent towards $\xah$ triples that are not yet well-understood. In other words, the agent selects \emph{some} policy which advances exploration.

%Each hallucination episode (which implements exploration) is hidden among many honest episodes which implement exploitation. This extends the ``hidden exploration" technique from incentivized exploration in bandits \citep{ICexploration-ec15,IncentivizedExploration-chapter}, which randomly hides one exploration round among many exploitation rounds. (In fact, this technique underlies the trivial reduction discussed above.) The difference is that an exploration round can directly recommend any desired action. In contrast, a hallucination episode recommends a very particular policy, and this policy is  constructed indirectly as the agent's reaction to a complex, carefully designed ledger. This indirect construction is what drives the exponential improvement over the trivial reduction.

Our results are theoretical, focusing on the exponential dependence on $SAH$ (and vastly improving over the prior work, as discussed above). Note that $\exp(SAH)$ episodes may be practical for sufficiently small MDPs.
Regarding implementation, each individual episode can be made computationally efficient under suitable independence assumptions. However, the fact that computation in Bayesian MDPs is not well-understood impedes efficient implementation in the general case.
%\asdelete{While we focus on incentives and statistical efficiency, we do not provide a computationally efficient implementation. The main reason is that computational issues in Bayesian MDPs are not yet well-understood \eg for policy optimization, Bayesian update, and posterior sampling. Improving the state of the art therein is beyond our scope.}
