%!TEX root = ../main-arxiv.tex

In this section, we investigate how our algorithm can be used to balance exploration and exploitation. We run \mdphh for $K_{\rho,n}$ episodes, as specified in \Cref{thm:main_prob_mdp}; we collectively refer to these episodes as \emph{exploration epoch}. Then, we use the collected data for exploitation. Specifically, we reveal all MDP trajectories from the exploration epoch in all subsequent episodes (which are called \emph{exploitation epoch}). Call this algorithm \HHandExploit.

We investigate \emph{regret}, a standard objective in RL which compares the algorithm's reward to the best policy given the true model $\modst$. Formally, we define
\begin{align*}%\label{eq:regret-defn}
    \OPT(\modst) = \max_{\pi\in\Pimarkov}\; \sbr{\valuef{\pi}{\modst}},
\end{align*}
and Bayesian regret in $K$ episodes as
\begin{align*}
\REG(K) :=
    {\textstyle \sum_{k\in [K]}}\;
        \Expop_{\model\sim\prior}\sbr{
            \OPT(\model) - \valuef{\pi_k}{\model}}.
\end{align*}
The first-order issue in regret analyses in the literature is how it scales with $K$.

We prove that for \HHandExploit, $\REG(K)$ scales as $K^{2/3}$ when the reachability parameter $\rho$ is sufficiently small and the number $K$ of episodes is sufficiently large. This scaling is in line with explore-then-exploit approach in multi-armed bandits, which achieves $T^{2/3}$ regret in $T$ rounds.%
\footnote{Explore-then-exploit is indeed an appropriate comparison for \HHandExploit, because both algorithms proceed in two epochs of predetermined duration, where in the ``exploration epoch" one explores without regard to exploitation, and in the ``exploitation epoch" one exploits without regard to exploration. This separation of ``pure exploration" and ``pure exploitation" is known to be inefficient; in particular, optimal algorithms achieve regret rates that scale as $\sqrt{T}$ (in bandits) and $\sqrt{K}$ (in RL).}

This result holds when the reachability parameter $\rho$ is low enough to guarantee that all $\xah$ triples are $\rho$-reachable, except those that cannot ever be reached. More precisely, a triple $\xah$ is called \emph{never-reachable} if no policy can reach it under any feasible model, \ie
    \begin{align*} \sfP^\pi_{\model}\sbr{ (\bmx_h,\bma_h,h) =\xah} = 0
        \quad \forall\;\text{policy  $\pi\in\Pimarkov$, model $\model\in \mathtt{support}(\prior)$}.
    \end{align*}
Thus, the threshold value of $\rho$ is defined as follows:
    \begin{definition}
$\rho_{\min} \ge 0$ is  the smallest $\rho\geq 0$ such that each $\xah$ triple is either $\rho$-reachable for any model in $\mathtt{support}(\prior)$, or is never-reachable.
\end{definition}


Further, we require this threshold value to be strictly positive. Thus, our result is stated as follows:

\begin{theorem}\label{cor:Bayesian-regret-nice}
Suppose $\rho_{\min}>0$. Recall from \eqref{eq:thm:main_prob_mdp-K} that
    $\Phi_\rho := K_{\rho,n}/ n$
is determined by $\rho$, the prior, and $S,A,H$. Consider algorithm \HHandExploit which runs for $K$ episodes. Choose reachability parameter $\rho\in (0,\rho_{\min}]$, failure parameter $\delta = \frac{1}{KH}$, and target number of samples $\nlearn = (K/\Phi_\rho)^{2/3}$. Assume that $K^{2/3}$ is large enough to upper-bound the right-hand side in \eqref{eq:thm:main_prob_mdp-n}.
Then
\[ \REG(K) \leq \tilde{O}(K^{2/3})\cdot \Phi_\rho^{1/3}\cdot H \sqrt{H(S + \log(KHS))}.\]
\end{theorem}

In the rest of this section we derive \Cref{cor:Bayesian-regret-nice} as a corollary of \Cref{thm:main_prob_mdp}, and also obtain a (weaker) bound on Bayesian regret that does not rely on the condition that $0<\rho\leq \rho_{\min}$.

We need a generic proposition on exploitation in MDPs. While similar propositions have appeared in prior work \cite[\eg][]{jin2020reward}, we use a slightly non-standard version which is Bayesian and involved $\rho_{\min}$. We provide a self-contained proof in Appendix~\ref{app:exploit}.

\newcommand{\vepslearn}{\varepsilon_{\mathrm{lrn}}}


\begin{restatable}[Exploitation]{proposition}{proprevelation}\label{prop:revelation}
Let $\alg$ be an algorithm which satisfies $(\rho,n,\delta,K_0)$-\traversal\ for some reachability parameter $\rho>0$, target of $n$ samples, failure probability $\delta$, and $K_0\geq n$ episodes. Let $\hat\pi$ be any ``exploitation policy" after $K_0$ episodes, \ie
%\ie letting $\signal$ be the full history of the first $K$ episodes,
\begin{align*}
\hat \pi \in \argmax_{\pi\in \Pimarkov}
    \Exp\sbr{ \valuef{\pi}{\modst} \mid \cbr{\text{full history of the first $K_0$ episodes}}}.
    \end{align*}
Then, with probability $1 - 2\delta$ under all sources of randomness, the per-episode regret of policy $\hat \pi$ can be upper-bounded as follows:
\begin{align}\label{eq:cor:simple-regret_hi_prob}
\OPT(\modst) - \valuef{\hat{\pi}}{\modst}     \leq {\textstyle \mathcal{O}(H^2)\cdot\left(S \rho \cdot \ind_{\{\rho > \rho_{\min}\}}+ H\sqrt{\frac{S + \log(SAHn/\delta)}{n}}\right)}.
\end{align}
%The Bayesian regret is therefore bounded by
%\begin{align}\label{eq:cor:simple-regret_expectation}
%\Exp_{\modst \sim \prior}[\OPT(\modst) - \valuef{\hat{\pi}}{\modst}]   \leq 2\delta H + {\textstyle \mathcal{O}(H^2)\cdot\left(S \rho \cdot \ind_{\{\rho > \rho_{\min}\}}+ H\sqrt{\frac{S + \log(SAHn/\delta)}{n}}\right)}.
%\end{align}
\end{restatable}

\begin{remark}
Put differently, one can upper-bound the left-hand side of \eqref{eq:cor:simple-regret_hi_prob} by a given $\epsilon>0$ if
\begin{align}
\rho \le c\max\cbr{\epsilon/(SH^2),\,\rho_{\min}}
    \quad \text{and}\quad
n \ge c\cdot H^6\epsilon^{-2} \rbr{S+\log \tfrac{SAH}{\epsilon\delta}},
\end{align}
for a large enough absolute constant $c$.
\end{remark}

%\Cref{prop:revelation}

The generic implication on Bayesian regret (which implies \Cref{cor:Bayesian-regret-nice}) is as follows:

\begin{corollary}\label{cor:regret}
Consider algorithm \HHandExploit with a fixed reachability parameter $\rho>0$. Use the assumptions and parametrization in \Cref{thm:main_prob_mdp}. Then each episode $k$ of the exploitation epoch satisfies \eqref{eq:cor:simple-regret_hi_prob} with $\hat{\pi} = \pi_k$ and $n=\nlearn$. Thus, Bayesian regret over $K>K_{\rho,n}$ episodes satisfies
 \begin{align}\label{eq:cor:regret}
\REG(K) \leq H K_{\rho,n} + (K-K_{\rho,n}) \rbr{\Psi_{\rho,n}+ 2\delta H},
\end{align}
where $\Psi_{\rho,n}$ is the right-hand side of \eqref{eq:cor:simple-regret_hi_prob}.
\end{corollary}




%\begin{remark}
%Our techniques appear insufficient to remove the dependence on $\rho$ from the regret benchmark. %Essentially, this is because the $\rho$ factor enters into $\qpunish(\cdot)$ in \Cref{thm:main_prob_mdp}.
%\end{remark}

