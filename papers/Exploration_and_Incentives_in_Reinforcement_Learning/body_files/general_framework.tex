
\section{General Framework}

\subsection{Hidden Hallucination}




\begin{definition}[ Hallucination Mappings] An \emph{hallucination} mapping $\Phi$ a mapping from censored legders $\ledgercens \in \ledgespacecenstotm $ to subsets of raw ledgers $\ledgespacetotm$ which are consistent with the censored ledger input: that is $\Phi(\ledgercens) \subseteq \censinv(\ledgercens)$, where we recall $\censinv$ defined in \mscomment{..}.
\end{definition}

\subsection{Our Guarantee}

For an event $\calE$, policy $\pi \in \Pitotal$, and subset of policies $\Pi \subset \Pitotal$ we define the conditional $\pi$-value and conditional $\Pi$-gap, respectively, as
\begin{align}
\valuemid{\pi}{\calE} &:= \Exp[\valuef{\pi}{\modvarst} \mid \calE].\\
\gap[\Pi \mid \calE] &:= \max_{\pi \in \Pi} \valuemid{\pi}{\calE} -  \max_{\pi \in \Pitotal \setminus \Pi} \valuemid{\pi}{\calE}. 
\end{align}
For example, with our conditional notation
\begin{align}
    \gap[ \Pi \mid  \ledgecens , \modclass] &:= \gap(\Pi  \mid \{\modvarst \in \modclass_{\ledgecens}\} \cap \{\Ledm(\Pi_{\ledger};\modvarst)  = \ledger\} )
\end{align}

\newcommand{\novel}{\ttbf{nov}}
\bluepar{Policy Novelty} To describe our exploration guarantees, we consider an \emph{novelty} function. 
\begin{definition}[Novelty Function]
A novelty function maps a $\model \in \modtotal$ and a $\Pi \subset \Pitotal$ to a subset $\Pinew \in \Pitotal$, denoted $\Pinew = \novel(\Pi;\model)$. 
\end{definition}
\begin{definition}[Novelty Functions, Sequences Bases \& Dimension] For the novelty function $\novel$:
\begin{enumerate}
	\item We say an ordered sequence $(\pi_1,\dots,\pi_n)$ is a $\model$-\emph{novelty idependent} if $\pi_i \in \novel(\{\pi_{1},\dots,\pi_{i-1}\}; \model)$.
	\item We say that ordered sequence $(\pi_1,\dots,\pi_n)$ is a $\model$-\emph{novelty spanning} if $\novel(\{\pi_1,\dots,\pi_n\};\model)= \emptyset$.
	\item The \emph{novelty dimension} $d$ is defined as the maximal length $n$ of sequence of policies which is both $\model$-novelty independent and $\model$-novelty spanning for any $\model \in \modtotal$. 
\end{enumerate}   
\end{definition}

\begin{definition}[Novelty-Respecting Censoring] We say that the novelty function respects the censoring operator if the following holds: Let $\model_1,\model_2$ be two models, and let $\Pi$ be a finite set of policies. Then, if $\censorac(\pi;\model_1) \distequals \censorac(\pi;\model_2)$ for all $\pi \in \Pi$, $\novel(\Pi;\model_1) = \novel(\Pi;\model_2)$. 
\end{definition}
In other words, the novelty function on a set of policies $\Pi$ depends only on the distribution of the corresponding censored trajectories. 


\bluepar{Incentive Width}



\begin{definition}[Incentive Width]\label{def:width} Fix censoring and novelty functions $\cens$ and $\novel$.  The \emph{incentive width} of a censored ledger $\ledgecens \in \ledgespacecenstot$ and a subset of raw ledgers $\ledgespace\subset \censinv(\ledgecens)$ under a model $\model \in \modtotal$ is defined as:
\begin{align}
    \width_{\model}(\ledgecens,\ledgespace) := \Pr[ \ledgervar_{\mathrm{raw}} \in \ledgespace \mid \cens(\ledgervar_{\mathrm{raw}}) = \ledgecens] \cdot \inf_{\ledger \in \ledgespace } \gap[  \novel(\ledgecens;\model) \mid  \ledger],
\end{align}
where $\ledgervar_{\mathrm{raw}}$ is the random ledger with marginal distribution $\Led(\Pi_{\ledgecens};\modvar)$, where $\modvar \sim \prior$ is the model drawn from the prior, and where we recall $\Pi_{\ledgecens}$ are the policies under the censored ledger. We say that a hallucination mapping $\Phi$ is $(\delta,w,m)$-wide if, for any $\model \in \modtotal$, and any $\Pi \subset \Pitotal$ with $|\Pi| < \infty$ and $\novel_{\model}(\Pi) \ne \emptyset$,
\begin{align*} 
\Pr[  \width_{\model}(\ledgevarcens,\Phi(\ledgevarcens)) \ge w \mid \ledgevarcens \sim \Ledcensm(\Pi;\model) ] \ge 1 - \delta.
\end{align*}
\end{definition}



%The width of an environment $\envir := (\modtotal,\Pitotal,\trajspacetot,\orac,\valuename)$ under a prior $\prior$ and censoring function $\cens$ is defined as 
%\begin{align}
%\width(\envir,\prior,\cens) := \inf_{\ledgecens \in \ledgespacecenstot}\,\sup_{\modclass \subset \modtotal} \width(\ledgecens,\modclass),
%\end{align}
%\end{definition}
\newcommand{\piexpl}[1][\ell]{\pi_{\mathrm{exp},#1}}



\bluepar{Meta-Theorem}
\begin{theorem}\label{thm:meta-theorem} Let $\novel$ be a censor-respecting novelty function, and suppose that the hallucination map $\Phi$ is $(\delta,w,m)$-wide for $w \in (0,1]$, $\delta \in (0,1)$, and $m \in \N$. Then, \Cref{alg:abstract_HH} with phase length $n = \ceil{1/w}$ and batch size $m$ ensures that, after $m n d$ total samples, the policies $\novel(\piexpl[1],\dots,\piexpl[d]) = \emptyset$. 
\end{theorem}

	\begin{algorithm}[h]
  	\begin{algorithmic}[1]
  	\State{}\textbf{Input: } Phase length $n$, sample size $m$,
    \For{phase $\ell=1,2,\dots$:}
    \Statex{}\algcomment{\% Phase $\ell$ consists of the next $n$ episodes}
    \State{}$\Phase_{\ell} = (\ell - 1) + [n\cdot m]$
    %\Comment{phase $\ell$ lasts for the next $N$ episodes}
    \State{}Draw ``exploration episodes" $\khal_{\ell,1},\dots,\khal_{\ell,m}$ uniformly from $\Phase_{\ell}$
    %\State{}Draw episode $\kexpl$  uniformly from $\PhaseEpisodes_{\ell}$
    %\State{}Draw $\kexpl \unifsim N(\ell - 1) + [N]$
     \Statex{}\algcomment{\% Denote $\calK \gets \Khal_{\ell} := \{ \khal_{i,j}: i < \ell, j \in [m]\}$} 
    \State{}Compute hallucination set $\modclasshall \subset \modclass$ as a function of $ \Ledcenssub[\calK]$
    \Statex{}\algcomment{\% hallucinate MDP}
     \State{}Define $\modelhall\sim \Pr[\cdot \mid \Ledcenssub, \modclasshall]$. 
    \State{}Let $\piexpl$ be BIC given  signal $\Ledhal_{\ell} \sim \orac( \Ledcenssub[\calK]; \modelhall)$ \algcomment{\quad\% hallucinated raw ledger}
    \Statex{}\algcomment{\% Reveal Signals}
    \For{each episode $k \in \Phase_{\ell}$}
    \If{$k = \kexpl$}
    \algcomment{\qquad\% hallucination}
    \State{}  Recommend policy $\piexpl$
    \Else{} \algcomment{\qquad\% exploitation}
     \State{} Reveal $\signal_k \gets \Ledsub[\calK]$, and let agent play freely.
     \algcomment{\qquad\% true raw ledger}
    \EndIf 
    \EndFor
    \EndFor 
  \end{algorithmic}
  \caption{Hidden Hallucination Meta-Algorithm Randomized ($\metaHH$)}
  \label{alg:meta_HH}
	\end{algorithm}





\subsection{Our Guarantee}

For an event $\calE$, policy $\pi \in \Pitotal$, and subset of policies $\Pi \subset \Pitotal$ we define the conditional $\pi$-value and conditional $\Pi$-gap, respectively, as
\begin{align}
\valuemid{\pi}{\calE} &:= \Exp[\valuef{\pi}{\modvarst} \mid \calE].\\
\gap[\Pi \mid \calE] &:= \max_{\pi \in \Pi} \valuemid{\pi}{\calE} -  \max_{\pi \in \Pitotal \setminus \Pi} \valuemid{\pi}{\calE}. 
\end{align}
For example, with our conditional notation
\begin{align}
    \gap[ \Pi \mid  \ledgecens , \modclass] &:= \gap(\Pi  \mid \{\modvarst \in \modclass_{\ledgecens} \cap \modclass\})
\end{align}

\bluepar{Span and Exploration Dimension}
\begin{definition} [Ledger Span]
Given a raw ledger $\ledger$, recall that its model class $\modclass_{\ledger}$ is the set of models consistent with the ledger. $\spn(\ledger)$ are the set of policies uniquely determined by $\ledger$. Formally,
\begin{align}
   \spn(\ledger) &:= \{\pi \in \Pitotal: \orac(\pi;\model) = \orac(\pi;\model'),~ \forall \model,\model' \in \modclass_{\ledger}  \}
\end{align}
We define the complement of the span $\spnc(\ledger) := \Pitotal\setminus\spn(\ledger)$.
\end{definition}
\begin{definition}[Exploration Dimension] We say that a sequence $(\pi_1,\dots,\pi_n)$ of policies is $\model$-independent if, for all $i \in [n]$, $\pi_i \notin \spn(\Led(\{\pi_{1},\dots,\pi_{i-1}\};\model))$. The exploration dimension $d$ is the maximal length of such sequences, over all $\model \in \modtotal$. 
\end{definition}

%Take $\ledgecens$, then look at $\spn(\ledgecens) = \bigcap \{\spn(\ledger) : \ledger \in \cens^{-1}(\ledgecens)\}$.

\begin{definition}[Span-Respecting Censoring] The function $\cens$ is \emph{span-respecting} if for all censored ledgers $\ledgecens$, and all $\ledger,\ledger' \in \uncens(\ledgecens)$, $\spn(\ledger) = \spn(\ledger')$. In other words, the span of ledger $\ledger$ is determined by $\cens(\ledger)$. If $\cens$ is span-respecting, we let $\spn(\ledgecens)$ denote the unamibiguous span of censored ledgers $\ledgecens \in \ledgespacecenstot$
\end{definition}

\bluepar{Incentive Width}
\begin{definition}[Width]\label{def:width} For a given environment $\envir$, prior $\prior$, and span-respecting censoring function $\cens$, a censored ledger $\ledgecens \in \ledgespacecenstot$ and a subset $\modclass\subset \modtotal$, the incentive width is defined as 
\begin{align}
    \width(\ledgecens,\modclass) := \Pr[ \modvarst \in \modclass \mid \ledgercens ] \cdot \inf_{\model \in \modclass } \gap[  \spnc(\ledgecens) \mid  \Led(\Pi_{\ledgecens};\model)]
\end{align}
The width of an environment $\envir := (\modtotal,\Pitotal,\trajspacetot,\orac,\valuename)$ under a prior $\prior$ and censoring function $\cens$ is defined as 
\begin{align}
\width(\envir,\prior,\cens) := \inf_{\ledgecens \in \ledgespacecenstot}\,\sup_{\modclass \subset \modtotal} \width(\ledgecens,\modclass),
\end{align}
\end{definition}

\bluepar{Meta-Theorem}
\begin{theorem}\label{thm:meta-theorem} Let $\cens$ be a span-respecting censoring function, and let $\nphase \in \N$ satisfy $\nphase \ge 1/\width(\envir,\prior,\cens)$. Then, there exists a choice of $\dots$ such that (a) $\metaHH$ is BIC, and (b) $\metaHH$ model-explores $\envir$ after $d\nphase$ steps with probability $1$.
\end{theorem}




\section{Episodic Exploration}


\bluepar{Learning Environemnt} A learning environment $\envir$ is specified by a tuple $(\modtotal,\Pitotal,\trajspacetot,\valuename,\orac)$, where $\modtotal$ is a space of \emph{models} $\model$, $\Pitotal$ is a space of \emph{policies} $\pi$, $\trajspacetot$ is a space of \emph{trajectories} $\traj$, $\valuename(\cdot;\cdot):\Pitotal \times \modtotal \to [0,B]$ returns the \emph{value} of the policy. Finally, in the deterministic setting, $\orac(\cdot;\cdot): \Pitotal \times \modtotal \to \trajspacetot$ is an \emph{oracle} returning trajectories from policies and models; in the stochastic setting, $\orac$ returns distributions (see ...).

\bluepar{Incentivized Exploration} We consider an episodic game where the \emph{principal} is tasked with learning about a ground truth model $\modelst$, via oracle queries $\traj_k = \orac(\pi_k;\modelst)$ for policies $\pi_1,\pi_2,\dots$. In our setting, the policies are selected by self-interested \emph{agents}, who maintains a prior $\prior$ over a the true model; we denote the random variable $\modvarst$ and its realization $\modelst$.  The principal's behavior is dicated by a possibly randomized algorithm $\Alg$ called the \emph{protrocol}. 
%
At each episode $k$, $\Alg$ generates an abstract signal $\signal_k \in \sigspace$. Then, a new agent arrives, who knows the episode number $k$, the protocol $\Alg$, but not its random coins, and recieves $\signal_k$. Lastly, the agent selects a self-interested policy
\begin{align}
\pi_k \in \argmax_{\pi \in \Pitotal} \Exp[(\valuef{\pi}{\modvarst} \mid \sigvar_k = \signal_k,\Alg],
\end{align}
which maximizes the posterior value over the above posterior. Any such $\pi_k$ lying in the above arg-max is called \emph{Bayes Incentive Compatible}, or BIC.\footnote{We note that, unlike prior work, we allow the agent to select any BIC policy.} 

\bluepar{Exploration Disiderata: } 
\mscomment{different disiderata for probabilistic}
The principal's goal isto adopt an algorithm $\Alg$ which designs a sequence of random signals $\sigvar_k$ such that $\modelst$ is \emph{explored}, in one of the two senses:
\begin{definition}[Notions of Exploration] We say that $\model$ is \emph{consistent} with a sequence of trajectory-policy pairs $(\traj_i,\pi_i)_{i \in [k]}$ if $\traj_i = \orac(\model,\pi)$ for all $i \in [k]$. 
\begin{itemize}
\item We say that $\Alg$ has \emph{model-explored} at episode $k$ if $\modelst$ is the unique model consistent with $(\traj_i,\pi_i)_{i \in [k]}$.
\item We say that $\Alg$ has \emph{value-explored} if, for any $\model$ consistent with $(\traj_i,\pi_i)_{i \in [k]}$, and any $\pi \in \argmax\valuef{\pi}{\model}$, $\pi \in \argmax\valuef{\pi}{\modelst}$.
\end{itemize} 
\end{definition}
In other words, model-exploration means $\Alg$ uniquely identifies the model from observed trajectories, and value-exploration means that $\Alg$ can identity at least one optimal policy for $\modelst$. 


\bluepar{Examples}

Here, we consider simple examples of generalized exploration, specialized to their deterministic-trajectory variants. 
\begin{example}[Multi-Arm Bandits] In multi-arm bandits, the model is specified by a vector of means $\mu  = (\mu_a) \in \R^{n}$,  with each policy $\pi$ corresponding to an action $a \in [n]$. Each trajectory $\traj = \orac(a,\mu)$ returns the reward $\mu_a$, which is equal to the $\value(a,\mu) = \mu_a$. 
\end{example}

\begin{example}[Linear Bandits] In (finite-armed) linear bandits, the model is specified by an unknown vector $\theta \in \R^d$. Each policy corresponds to a vector $v_a \in \R^d$, indexed by $a \in [n]$. Again, both values and trajectories for action $a$ under model $\theta$ are $v_a^\top \theta$. 
\end{example}


\begin{example}[Tabular Markov Decision Processes] A discrete Markov decision process is described by a layered state space. 
\end{example}

\begin{example}[Linear Control] \mscomment{I may be able to get this to work}
\end{example}



\subsection{Preliminaries}

\bluepar{Raw Ledgers } A \emph{raw} ledger $\ledger \in \ledgespacetot$ is a collection of policy-trajectory pairs $(\pi,\traj_{\pi})_{\pi \in \Pi_{\ledger}}$, where $\traj_{\pi} \in \trajspacetot$ is a trajectory indexed by $\pi$. We call  $\pols{\ledger} \subset \Pitotal$ ledger $\ledger$'s policy set. Given a set $\Pi \in \Pitotal$ and $\model \in \modclass$, we call $\Led(\Pi;\model) := \{(\pi,\orac(\pi;\model))\}_{\pi \in \Pi}$  the raw $\Pi$-ledger of $\model$, which is the set of all trajectories obtained by executing policies $\pi \in \Pi$ on model $\mu$. The set of models consistent with $\ledger$ are
\begin{align}
  \modclass_{\ledger} &:= \{\model \in \modtotal : \Led(\Pi_{\ledger} ; \model) = \ledger \} \label{eq:modclass}
\end{align}
A ledger is said to be \emph{feasible} if $\modclass_{\ledger} \ne \emptyset$. The empty ledger is denoted $\emptyset$, and $\modclass_{\emptyset} = \modtotal$. 


\bluepar{Censoring, Censored Ledgers}
A \emph{censoring function} $\cens$ is a mapping from trajectories  $\traj \in \trajspacetot$ to censored trajectories $\trajcens\to \trajspacecenstot$. A censored ledger $\ledgecens = \{(\pi,\trajcenssub)\}_{\pi \in \pols{\ledgecens}} \in \ledgespacecenstot $ is a collection of pairs of policies $\pi \in \pols{\ledgecens}$ is its policies and $\trajcenssub \in \trajspacecens$.  Overloading notation slightly, we define the models consistent with $\ledgecens$ as
\begin{align*}
\Ledcens(\Pi;\model) := \{(\pi,\cens(\orac(\pi;\model)))\}_{\pi \in \Pi}\\
\modclass_{\ledgecens} := \{\model \in \modtotal : \Ledcens(\Pi_{\ledger} ; \model) = \ledger \}
\end{align*}
Given $\ledger \in \ledgespacetot$, we let $\cens(\ledger) \in \ledgespacecens$ denote the ledger induced by the censoring function. We also let $\uncens(\ledgecens) := \{\ledger = \Led(\Pi_{\ledgecens};\model): \model \in \modclass_{\ledgecens}\}$ denote the set of all raw ledgers consistent with $\ledgecens$

\bluepar{Conditional Notation.} 

We denote conditionals $[\cdot \mid \ledger]$ as shorthand for $[\cdot \mid\Led(\Pi_{\ledger};\modvarst) =  \ledger]$, which equivalently conditions on $[\cdot \mid \modvarst \in \modclass_{\ledger}]$. Similarly, for $\ledgecens \in \ledgespacecenstot$, we have $[\cdot \mid \ledgecens]$ to denote $[\cdot \mid \modvarst \in \modclass_{\ledgecens}]$.  
%
For $ \modclass \subset \modtotal$, we use $[\cdot \mid \modclass]$  to denote conditioning on $\{\modvarst \in \modclass\}$. We use the conditions for $\Pr,\Exp,\gap,\valuename$, and may combine multiple conditionals at once. For example, for an event $\calE$, $\ledger \in \ledgespacetot$, $\ledgecens \in \ledgespacecenstot$, and $\modclass \subset \modtotal$, we have the definitions
\begin{align*}
\Pr[ \cdot \mid  \ledger, \ledgecens,\calE] &:= \Pr[ \cdot \mid \{\modvarst \in \modclass_{\ledger}\cap\modclass_{\ledgecens}\} \cap \calE]\\
\end{align*}
Note that this condition treats the policies $\pols{\ledger}$ comprising ledgers as a \emph{deterministic}, and not selected by any strategic algorithm. Hence, we will refer to these as \emph{cannonical} conditional probabilities. 

\bluepar{Adaptivity} We say that a ledger $\ledger$ is \emph{adaptive} if the policies are selected \mscomment{\dots}


Let $\ledgervar$ be some random ledger, with realization $\ledger$. Then $\Pr[\cdot \mid \ledgervar = \ledger]$ and $\Pr[\cdot \mid \ledger]$ are potentially too different things.
  




\bluepar{Examples}


stuff
$\mathsf{L},\boldsf{L}$, $\ledger,\ledgervar$
\subsection{The Hidden Hallucination Algorithm}
  Given a set of episodes $\calK \subset \N$, we let $\Ledsub[\calK] := \{(\pi_k,\traj_k)\}_{k \in \calK}$ denotes its raw ledger, and $\Ledcenssub[\calK] := \{(\pi_k,\cens(\traj_k))\}_{k \in \calK}$ denote the censored ledger. We also defined $\spn[\calK] := \spn(\{\Ledsub[\calK]: k \in \calK\})$.


$\metaHH$ proceeds in phases $\ell \in \N$, consisting of blocks of $\nphase$ episodes, denoted $\Phase_{\ell}$. For $k \in \Phase_{\ell}$, the \emph{exploration episode} $\kexpl$ is drawn uniformly at random from the $\nphase$ episodes. Moreover, the random set $\Kexp[\ell]$ maintains all past exploration episodes $\kexpl[\ell']$ over past phases $\ell'< \ell$. We use $\Kexp[\ell]$ to index the relevant histories which we reveal to the agent. Specifically, during phase $\ell$:
\begin{itemize}
	\item For $k \ne \kexpl$, we reveal the \emph{raw ledger} indexed by $\Kexp[\ell]$; that is, $\Ledsub[\Kexp[\ell]]$. 
	\item If $k \ne \kexpl$, we use the \emph{censored ledgers} $\Ledcenssub[\Kexp[\ell]]$ to first ``hallucinate'' a model $\modelhall$, and then return the raw ledgers we would have seen if this model where the ground truth, by calling $\orac(\Ledcenssub[\Kexp[\ell]];\modelhall)$. We will call this the hallucinated ledger, $\Ledhal_{\ell}$
\end{itemize}
In order to incentivize exploration, we  hallucinate the model $\modelhall$ is a specific manner: we draw the model from the cannonical conditional distribution $\Pr[\cdot \mid \Ledhal_{\ell}, \modclasshall]$, which corresponds to the posterior on $\modelst$ conditioned both  on the censored history, and on the event that $\modelst$ lies in a model class $\modclasshall \subset \modtotal$ (treating $\pols{\Ledhal_{\ell}}$ and $\modclasshall$ as fixed and non selected adaptively). This procedure is implemented so as to satisfy two desiderata:
\begin{enumerate}
\item We select $\modclasshall$ to be such that, for any $\model \in \modclasshall$, any agent which \emph{believed} that $\Ledhal_{\ell}$ was (a) a true ledger and (b) was chosen non-adaptively, the agent would select a novel policy $\pi \notin \spn[\Kexp]$.
\item By selecting the probability of $\Pr[\modclasshall \mid \ledgecens[\Kexp]]$ sufficiently large, and the phase length $\nphase$ sufficiently small, we can `hide' the hallucination so that the agent cannot disambiguate whether she is being shown the raw ledger ($k \ne \kexpl$),  or the hallucinated one (  $k = \kexpl$). Hence, even when the agent is shown the hallucinated ledger, she (approximately) believes it is the raw one. 
\end{enumerate}
These disiderata introduce a tension in how we select $\modclasshall$: larger sets hallucination sets have larger probabilities, making then easier to hide. But as more policies are explored and $\spn[\Kexp]$ grows, we need smaller sets $\modclasshall$ to ensure exploration of new policies. This tradeoff is ultimately captured by the notion of width in \Cref{def:width}.


\bluepar{Data Hygeine} Note that \mscomment{explain this hygeine more} Let $\ledgvar  \in \ledgespace \cup \ledgespacecens$ be a random sequence of  raw/censored ledgers produced by our online protocol.  In full generality, the signal \emph{could} communicate information about $\modvarst$ depending on which policies make up its constituent ledgers.  We say that $\ledgvar$ is  \emph{sanitized} if, given a realization $\ledgvar = \ledger$, we have
\begin{align*}
\Pr[\cdot \mid \ledgvar = \ledger] = \Pr[\cdot \mid  \ledger_{1:i}];
\end{align*}
This definition is subtle: it means that conditioing on the realization $\ledgvar$ is equivalent to treating its consitutent policies $\Pi_{\ledgvar}$ as fixed, non-random quantities. 



\section{Instantiating the Framework}








\newcommand{\Xmat}{X}
\newcommand{\rews}{r}
\newcommand{\ones}{\mathbf{1}}
\newcommand{\valvec}{\upnu}
\newcommand{\canvec}{\mathbf{e}}
\newcommand{\scrI}{\mathscr{I}}
\newcommand{\scrA}{\mathscr{A}}
\newcommand{\scrQ}{\mathscr{Q}}
\newcommand{\rd}[1][d]{\R^{#1}}
\newcommand{\calQ}{\mathcal{Q}}
\newcommand{\Qbar}{\widebar{Q}}
\newcommand{\calV}{\mathcal{V}}

\newcommand{\cexp}{c_{\mathrm{exp}}}
\newcommand{\inds}{\mathrm{inds}}

\newcommand{\support}{\mathrm{support}}
\newcommand{\Mquer}{\modclass^{\mathrm{quer}}}
\newcommand{\vecspan}{\mathrm{span}}


\subsection{Bandits and Semi-Bandits}


Here, let us consider the combinatorial bandit setting, which strictly generalizes that of bandits:
\begin{setting}[Combinatorial Semi-Bandits; Deterministic Setting]
Combinatorial setting bandits with $K$ base actions, called `arms', is defined as follows:
\begin{itemize}
	\item Each policy $\pi$ corresponds to a subset of actions $\calA_{\pi}$ of $[K]$. Without loss of generality, we assume that for every $a \in [K]$, there is a set $\calA_{\pi} $ containing $a$.
	\item Each model $\model$ corresponds to a vector of rewards $r_{\model} \in [0,1]^K$.
	\item A trajectory $\traj = \orac(\pi;\model)$ consists of pairs $(\calA_{\pi},r_{\model}[\calA_{\pi}])$, where $r_{\model}[\calA_{\pi}] \in \R^{\calA_{\pi}}$ denotes vector $r_\model$ evaluated at indices $a \in \calA_{\pi}$; censoring returns  the actions selected: $\cens(\traj) = \calA_{\pi}$. 
\end{itemize}
For a set of policies $\Pi$, set $\calA_{\Pi} := \bigcup_{\pi \in \Pi}$; we use $r[\calA_{\Pi}] \preceq c$ to denote entrywise inequality. 
\end{setting}
\newcommand{\fprior}{f_{\prior}}
\newcommand{\gprior}{g_{\prior}}


\begin{condition}[Sufficient Condition for Semi-Bandits]\label{cond:bandits_two} Define the functions $f_{\prior},g_{\prior}$ via
\begin{align*}
f_{\prior}(\epsilon) := \min_{\calA \subsetneq [K]}\Pr[r_{\modvar}[\calA] \preceq \epsilon], \quad \text{and} \quad g_{\prior}(\epsilon) := \min_{\calA \subsetneq [K]} \max_{a \in [K]\setminus \calA} \Exp[r_{\modvar}(a) \mid r_{\modvar}[\calA] \preceq \epsilon].
\end{align*}
We assume that $\fprior(\epsilon) > 0$ for all $\epsilon > 0$, and that $\lim_{\epsilon \to 0} \gprior(\epsilon)/\epsilon = \infty$.
\end{condition}
\mscomment{Do we want these for the infinum ones? I guess}
As an example, if $\prior$ has density lower bound by $\kappa$ with respect to the Lebesgue measure, and all entires of $r_{\modvar}$ are independent, then the above holds for $\fprior(\epsilon) \ge \kappa \epsilon^{-K}$ and $\min_{\epsilon} \gprior(\epsilon) \ge \min_{a \in [K]} \Exp[ r_{\modvar}(a)]$. 

\begin{lemma}Let $\ledgercens = \cens(\Led(\Pi;\model))$ denote the censored ledger under a policy $\Pi$. Then (i) $\ledgercens$ does not depend on the choice of $\model$; (ii) $\spn(\ledgercens) := \{\pi: \calA_{\pi} \not\subseteq \calA_{\Pi}\}$; (iii) the exploration dimension is at most $K$. In addition, under \Cref{cond:bandits_two},  let $\epsilon_{\prior} := \sup\{\epsilon > 0: \gprior(\epsilon)\ge 2K \epsilon\}$, which the condition ensures is strictly positive.  Then the width is lower bounded by $\epsilon_{\prior} \cdot \fprior(\epsilon_{\prior})$.
\end{lemma}
\begin{comment}
\begin{condition}[Semi-Bandits; First Reward Condition]\label{cond:bandit_first_rew_cond} There exists constants $\epsilon_{\prior},\gamma_{\prior}$ such that
\begin{align*}
\inf_{\Pi} \Pr_{\modvar \sim \prior}\left[ \max_{\pi:\calA_{\pi} \not\subseteq \calA_{\Pi}}\Exp\left[ \sum_{a \in \calA_{\pi}} r_{\modvar}(a) \mid r_{\modvar}[\calA_{\Pi}]\right] - \max_{\pi_0 \in \Pi} \sum_{a \in \calA_{\pi_0}} r_{\modvar}(a)  \ge \epsilon_{\prior}\right] \ge \gamma_{\prior}.
\end{align*}
\end{condition}
\end{comment}



\begin{remark}[Make arbitrary interval for matroid]
\end{remark}


\subsection{Linear Bandits}

\begin{setting}[Linear Bandits] We define deterministic linear bandits setting in dimension $\R^d$ with is as follows. 
\begin{itemize}
	\item Each policy $\pi \in \Pitotal$ corresponds to a vector $a_{\pi} \in \R^d$ with $\|a_{\pi}\|_2 \le 1$. We let $\calA_{\Pitotal} := \{v_{\pi}: \pi \in \Pi\}$, and assume without loss of generality that $\vecspan(\calA) = \R^d$.
	\item The model is specified by the latent reward vector $r_{\model} \in \rd$, with $\|r_{\model}\|_{2} \le 1$. 
	\item   and $\orac(\pi;\model) = (v_{\pi},v_{\pi}^\top r_{\model})$; censoring corresponds to return $v_{\pi}$: $\cens(\orac(\pi;\model)) = v_{\pi}$. 
	\end{itemize}
\end{setting}
\newcommand{\projection}{\mathrm{proj}}
\begin{condition}[Sufficient Condition]\label{cond:linear} For a vector $v \in \R^d$ (resp. subspace $\calV \subset \R^d$), and let $(\projection_{v})$ (resp. $\projection_{\calV}$) denote the orthonormal projection onto the subspace. Define
\begin{align*}
\fprior(\epsilon) &:= \min_{v \in \rd - \{0\}} \Pr[\|(I-\projection_{v})r_{\modvar}\|_2 \le \epsilon] \\
\gprior(\epsilon) &:= \min_{\text{subspaces } \calV \subsetneq \rd} \max_{a_{\pi} \notin \calV} \Exp\left[\langle a_{\pi}, r_{\modvar} \rangle \mid \|\projection_{\calV} r_{\modvar}\|_2 \le \epsilon\right]
\end{align*}
\end{condition}




\begin{lemma}Let $\ledgercens = \cens(\Led(\Pi;\model))$ denote the censored ledger under a policy $\Pi$. Then (a)$\ledgercens$ does not depend on the choice of $\model$; (b) $\spn(\ledgercens) := \{\pi:v_{\pi} \notin \vecspan(v_{\pi'}: \pi' \in \Pi)\}$ (b)The exploration dimension is at most $d$.
\end{lemma}


\newcommand{\calZbar}{\overline{\calZ}}
\subsection{Tabular MDP}
\begin{setting}[Non-Stationary Tabular MDPs] The  tabular MDP setting with horizon $H$, $A$ actions, and $S$ states is defined as follows:
\begin{itemize}
	\item Each policy $\pi$ is speficied by $\{\pi_h(x) \in [A]: h \in [H], x \in [S]\}$.
	\item Each model is parameterized by two objects: a latent reward vector $r_{\model}(x,a,h) \in \R^{SAH}$, and transition map $P_{\model}: \pi \to  ([S] \times [A])^H$ which maps policies to  trajectories $(x_1,a_1),\dots,(x_H,a_H)$ censored of rewards. We let $P_{\model;h}(\pi) = (x_{h},a_h)$. 
	\item Denote $\calZbar := [S] \times [A] \times [H]$. We identify $([S] \times [A])^H$ with subsets of $\calZbar$, and let $r_{\model}[P_{\model}(\pi)]$ denote the rewards on the trajectory $P_{\model}(\pi)$.
	 \item The oracle then returns $\orac(\pi;\model) : (\pi;P_{\model}(\pi), r_{\model}[P_{\model}(\pi)])$, where $r_{\model}[P_{\model}(\pi)]$ denotes the subvector of rewards indced by $(x,a,h) \in P_{\model}(\pi)$.
\end{itemize} 
\end{setting}


\newcommand{\neighbors}{\mathrm{neigh}}

\begin{condition}[Sufficient Condition for Semi-Bandits]\label{cond:bandits_two} Given $\calZ \subset \calZbar := [S] \times [A] \times [H]$, define neighbors $\neighbors(\calZ) := \{(x,a,h): \exists a' \in [A] \text{ for which } (x,a',h) \in \calZ\}$. Define the functions $f_{\prior},g_{\prior}$ via
\begin{align*}
f_{\prior}(\epsilon) &:= \min_{P_{\model}}\min_{\calZ \subsetneq \calZbar}\Pr\left[r_{\modvar}[\calZ] \preceq \epsilon \mid P_{\modvar} = P_{\model} \right]\\
g_{\prior}(\epsilon) &:=  \min_{P_{\model}}\min_{\calZ \subsetneq \calZbar} \max_{(x,a,h) \in \neighbors(\calZ)} \Exp[r_{\modvar}(a) \mid r_{\modvar}[\calA] \preceq \epsilon \mid P_{\modvar} = P_{\model}].
\end{align*}
We assume that $\fprior(\epsilon) > 0$ for all $\epsilon > 0$, and that $\lim_{\epsilon \to 0} \gprior(\epsilon)/\epsilon = \infty$.
\end{condition}

\begin{lemma}[Bounding Exploration Dimension and ..]
\end{lemma}

\begin{comment}
\begin{lemma}Let $\ledgercens = \cens(\Led(\Pi;\model))$ denote the censored ledger under a policy $\Pi$. Then  
\begin{align*}
\spn(\ledgercens) := \{\pi: \forall \pi' \in \Pi, \exists x,a,a',h: (x,a) = P_{\model;h}(\pi), (x,a') = P_{\model;h}(\pi'), \text{ but } a \ne a'\}
\end{align*}
Moreover, the exploration dimension is at most $SAH$.
\end{lemma}

\begin{lemma}[Bounding the width] Let $\epsilon_{\prior}$ be such that $f_2(\epsilon_{\prior}) \le SAH \epsilon_{\prior}/2$. Then, the width is lower bounded by $\epsilon_{\prior} \cdot f_1(\epsilon_{\prior})$.
\end{lemma}
\end{comment}

