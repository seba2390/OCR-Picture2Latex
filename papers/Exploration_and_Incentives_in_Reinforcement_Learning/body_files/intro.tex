%!TEX root = ../main-arxiv.tex
How do you incentivize self-interested agents to \emph{explore} when they prefer to \emph{exploit}? We revisit the tradeoff between exploration and exploitation, \ie between costly acquisition of information and using this information to make good near-term decisions. While algorithmic aspects of this tradeoff have been extensively studied in machine learning and adjacent disciplines, we focus on its economic aspects. We consider a population of self-interested agents which collectively face the exploration-exploitation tradeoff. The agents should explore for the sake of the common good, yet any given agent is not inherently incentivized to suffer the costs of exploration for the sake of helping others. As a result, exploration may happen very slowly, or not at all. This can be remedied by an online platform such as a recommendation system, which may wish to balance exploration and exploitation. Such platform can only provide information and recommendations, but cannot force the agents to comply. However, the platform may aggregate information from many agents in the past, and control what is disclosed to agents in the future. This information asymmetry provides the platform with leverage to incentivize exploration. Designing recommendation algorithms that incentivize exploration has been studied previously, starting from \citep{Kremer-JPE14,Che-13}; we will refer to this work as \emph{incentivized exploration}. All prior work focuses on a basic exploration problem in which each agent acts once, and her actions do not affect the outcomes for actions chosen in the future.%
(If all agents are controlled by an algorithm, this problem is known as \emph{multi-armed bandits}.)

We initiate the study of incentives in much more complex exploration problems that arise in reinforcement learning (RL). Each agent has multiple interactions with the environment. The chosen actions have a persistent effect: there is \emph{state} which is probabilistically affected by the agent's actions and in turn affects the agent's rewards for these actions. A standard abstraction for such interactions is a Markov Decision Process (\emph{MDP}), which posits that the effect of the past is completely summarized by the current state. We follow the paradigm of \emph{episodic RL}: each agent faces a fresh copy of the same MDP, constituting one ``episode" of the problem. This MDP is not known in advance, but may be approximately learned over multiple episodes. RL algorithms choose the course of action in a particular episode (usually called a \emph{policy}), and adjust this policy from one episode to another so as to balance exploration and exploitation. However, in our problem, called \emph{incentivized RL}, it is the agents themselves who control the choice of policies, whereas an algorithm can only issue recommendations, leveraging the information asymmetry discussed above.

For motivation, let us consider three stylized examples in which agents have repeated, MDP-like interactions with the environment, and need to be incentivized to explore.
%there is a mismatch between exploration and incentives.
First, consider automated market proxies, such as bidding agents in ad auction,  algorithmic traders on a stock exchange, or pricing agents for retail buyers or sellers. Their actions affect their own endowment or inventory, and possibly also the state of the market (\eg for thin markets). The parameter settings for such proxies can be viewed as an MDP policy. Many/most market participants tend to be ``small", participating infrequently or in rare bursts, and therefore lacking ``inherent" incentives to explore. Thus, an online market may wish to incentivize exploration among such participants. Second, a navigation app suggests driving directions, whereas the driver chooses which route to take. The driver makes multiple decisions as he is driving, which change his location. This can be modeled as an MDP, with driver's location as ``state" and routes as ``policies". Exploration is needed to find out which routes are better at a particular time, but each driver just wants to get to this destination soon. Third, a medical treatment received by a patient may consist of multiple steps which depend on the patient's current ``state". This scenario can be viewed as an MDP, where the medical treatment is a ``policy". Exploration, a.k.a. clinical trials, is commonly used to learn which treatments work best. However, patients tend to prefer treatments that work best, and need to be incentivized to explore for the sake of others (see Section 1.4 in \cite{ICexploration-ec15} for a discussion).

\input{body_files/model-intro}

%\input{body_files/model-intro-old}

\bluepar{Challenges.}
Incentivized exploration is challenging even in bandits. For one, why would an agent choose an arm she does not initially prefer, especially the first time it is recommended? Moreover, if the Bayesian prior is independent across arms, a given arm cannot be recommended before all arms with larger prior mean rewards are explored. Due to these and similar difficulties, algorithms for incentivized exploration require specially tailored algorithms, as standard algorithms from bandits are not compatible with agents' incentives. Revealing full data collected by the algorithm would not help convince the agents to follow  recommendations: instead, they would only exploit ({\ie choose} the actions with the greatest posterior mean reward). Even the most basic objective of exploring one other arm is not always achievable, requires non-trivial solutions, and is very subtle to optimize. (We expand on all these points in Related Work, see Section~\ref{sec:related}.) Returning to incentivized RL, we conclude that we should not expect  standard RL algorithms to be incentive-compatible, and \traversal is a natural first objective to consider.

RL is known to be much more difficult than multi-armed bandits as a machine learning problem, for many reasons. Below we list three specific challenges that are most relevant to incentivized RL. First, the effective number of alternatives from which an agent may choose -- the number of all Markovian policies -- is exponential in $S$, the number of states. Second, an algorithm cannot reliably explore a particular $\xah$ pair even without incentives issues, because it does not know a priori which policies will visit which states. In contrast, a bandit algorithm can just pull any desired arm. Third, expected rewards associated with different policies are necessarily highly correlated, even in the frequentist version, because many different policies may visit the same $\xah$ pair. Whereas rewards of different arms in bandits are not correlated in the frequentist version, and can be assumed mutually independent in the Bayesian version as a paradigmatic special case.

\newcommand{\nArmsMAB}{\tilde{A}} % #arms in bandits

Incentivized RL can be trivially reduced to incentivized exploration in bandits by treating each MDP policy as an ``arm" and each episode as a ``round". This reduction is exponentially wasteful, as it creates $\nArmsMAB = A^{SH}$ arms. Moreover, the relevant state-of-art result for incentivized exploration in bandits with correlated priors \citep[Section 5]{ICexploration-ec15} does not provide explicit guarantees for a super-constant number of arms, and requires $e^{\Omega(\nArmsMAB)}$ rounds to explore each arm even once in some natural examples.%
\footnote{Recently, \citet{Selke-PoIE-ec21} achieved $\poly(\nArmsMAB)$ dependence if the prior is mutually independent across arms. However, this result does not help the reduction at hand, because the mean rewards of policies are highly correlated. }
{Reduction} to this result requires $K$ episodes, where $K$ is exponential in the number of policies. This is \emph{doubly exponential} in problem parameters:
$K=\exp\rbr{ A^{SH} }$. Moreover, the prior-dependent parametrization in the guarantee from \citep[Section 5]{ICexploration-ec15} lacks a natural interpretation when `arms' correspond to MDP policies.

Let's calibrate our desiderata {for incentivized RL}.
%given the discussion above.
The effective number of different actions in the MDP is $SAH$, \ie $A$ actions for each $(x,h)$ pair. So, $\exp(SAH)$ episodes in incentivized RL would be in line with the exponential dependence in prior work, and would vastly improve over the trivial reduction {outlined above}.


%This discussion recalibrates our desiderata. The effective number of different actions in the MDP is $SAH$, one for each $(x,h)$ pair. So, a result on incentivized RL which only requires $\exp(SAH)$ episodes would be in line with the state of the art for incentivized exploration in bandits.


\bluepar{Results and techniques.}
We design an algorithm for incentivized RL which achieves \traversal
%explores all $\delta$-reachable $\xah$ triples
in $K$ episodes, where $K$ is bounded in terms of parameters $S,A,H$ and the prior. We obtain $K$ with at most exponential dependence on $SAH$ when the rewards $\sfr_{\model}\xah$ are independent across the $\xah$ triples and jointly independent of the transitions (but the transitions can still be arbitrarily correlated among themselves). Note that policy values are still highly correlated.%
\footnote{This holds because (i) different policies may visit the same $\xah$ triples and (ii) the transitions may be correlated.}
We extend this result to randomized MDPs and (with a more abstract guarantee) to arbitrary priors without the independence assumption.

%A Bayesian prior with this property is called \emph{reward-independent}. Note that the transition probabilities can still be arbitrarily correlated across the $\xah$ triples, and policy values are highly correlated as different policies may visit the same $\xah$ triples. Further, we generalize this result to arbitrary priors.


\input{body_files/our_techniques}

%Our formulation of incentivized RL generalizes that in the prior work on incentivized bandit exploration: the latter is simply a special case with horizon $H=1$.


\bluepar{Discussion and limitations.}
%\asdelete{In terms of recommendations to agents,}
We focus on \emph{Markovian} policies: functions that input the state and the stage, but ignore what happened in the past stages. This is traditional in the study of MDPs and does not lose generality in the planning problem when the MDP is fully known. With a Bayesian posterior, the optimal MDP policy may be non-Markovian; equivalently, an agent may want to revise the policy mid-episode. However, committing to follow a Markovian policy may be behaviorally justified. Indeed,
%Each agent chooses an optimal Markovian policy before the episode starts. Essentially, the agents do not attempt to learn during the episode, or strategically adjust their choices based on the information revealed therein. While such behavior is suboptimal,%
%\footnote{Markovian policies are optimal w.l.o.g. when the MDP is fully known. However, if one only has a Bayesian posterior on the MDP, the optimal policy may be non-Markovian, or (equivalently) an agent may want to revise the policy mid-episode. }
it may reflect the lack of sophistication or resources to optimize beyond Markovian policies,
exogenous constraints (\eg the available medical treatments are Markovian and must be chosen in advance), or considerations of convenience (\eg following the driving directions vs. charting one's own course). In all these cases, agents may be content to know that the suggested policy is better for them than any other Markovian policy.

%The suggested policy can be revealed to an agent implicitly through per-step recommendations, rather than explicitly before the episode. For example, a doctor may adjust the patient's treatment over time,  without revealing the ``policy" for these adjustments in advance. While the policy can, in principle, be deduced from the algorithm's specification, the agent does not need to know it. Instead, she can trust the algorithm's BIC property, and simply follow the per-step recommendations.

%\asdelete{In terms of the agents decision-making framework,}
We make standard assumptions from theoretical economics: the principal has the ``power to commit" to the declared algorithm, and the agents are endowed with Bayesian rationality, \ie they maximize their expected utility given available information, and have sufficient knowledge and computational power to do so. Almost all prior work on incentivized exploration makes these assumptions.%
\footnote{One exception is \citep{Jieming-unbiased18}, which restricts the algorithm's structure so that weaker economic assumptions suffice.}
Likewise, our algorithm requires a detailed knowledge of the Bayesian prior; this is a standard assumption in incentivized exploration, and more generally in the literature on information design in economics.%
\footnote{The only exception in prior work on incentivized exploration is one of the algorithms in \citep{ICexploration-ec15}, which summarizes the prior with just two numbers. However, this result requires the prior to be independent across arms (and only applies to bandits).}

While we study a basic model of incentivized RL, all three components thereof can be extended in various directions (see \Cref{sec:conclusions}). We view our work as a foundation towards further study.

%The independence assumptions (which underlie the $\exp(SAH)$ sample complexity) can also be motivated as incorrect beliefs which reflects agents' naivety. This effect, called \emph{correlation neglect}, is a notable theme in the literature on social learning, starting from \citet{DeMarzo-qje03}. \asedit{Our main results seamlessly extend to a version when the agents hold a common belief, which is known to the principal but may be incorrect.}
%\footnote{However, regret implications in Section~\ref{sec:exploit} do rely on a common prior.}




%	– medical trial: RL episode = patient, RL policy =  treatment, RL state = patient's state
%	– driving directions: RL episode = driver, RL policy = suggested route, RL state = location
%	– recommendation systems: RL episode = user with multiple interactions, RL state = user's knowledge

\bluepar{Some notation.}
Let $\Pimarkov$ be the set of all policies as defined above, \ie all Markovian policies.
The support of the prior $\prior$, \ie the class of of all feasible MDP models, is denoted $\modtotal$. Let us recap the ``time units" used throughout this paper: in each \emph{episode} an MDP is executed for $H$ \emph{stages} (standard terminology in episodic RL); our algorithm operates in \emph{phases} of a fixed number of episodes each; we use \emph{rounds} only when we discuss specialization to multi-armed bandits.

\bluepar{Map of the paper.}
To simplify presentation, we first specify our algorithm for the special case of  deterministic MDPs (Section~\ref{sec:alg}), and analyze it for the special case of deterministic MDPs and independent priors (Section~\ref{sec:analysis-basic}). This case is already highly non-trivial and captures our main ideas. Then in Section~\ref{sec:extension} we extend our algorithm and analysis to the general case, with randomized MDPs and arbitrary priors. Regret implications are spelled out in Section~\ref{sec:exploit}. All proofs are (only) outlined in the body of the paper, with lengthy details deferred to the appendices.
