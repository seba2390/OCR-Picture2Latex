%!TEX root = ../main-arxiv.tex


\subsection{Proof of \Cref{lem:hallucination_good_event} \label{sec:proof_hall_good_event}}
The proof of \Cref{lem:hallucination_good_event} builds on the Bayesian concentration technique due to \cite{Selke-PoIE-ec21}. We recall the ``good set'' of models, which we write with an explicit dependence on $\modst$.
\begin{align}
\modgoodl(\modst) := \left\{\model: \forall \xah \in \calU_{\ell}^c \begin{matrix} &\|\sfp_{\model}(\cdot \mid x,a,h) - \sfp_{\modst}(\cdot \mid x,a,h)\|_{\ell_1} \le 2\epsp(\delta_0) \\
&\text{ and }\lonenorm{\sfp_{\model}(\cdot \mid 0) - \pmodst(\cdot \mid 0) } \le 2\epsp(\delta_0)\\
&\text{ and }  \sfr_{\model}\xah \le \epspunish + 2\epsr(\delta_0)\\\end{matrix} \right\}. \label{eq:modgoodl}
\end{align}
Further, recall $\Exphall[f(\modvarhat)] := \Expop\left[\Expmucan[f(\modvarhat) \mid \ledhall] \mid \ledcensl\right]$ and similarly $\Prhall$, whcih are random measures dependening on $\ledcensl$. Our goal is to show that, with probability $1 - \deltafail$ over the randomness of $\modst$ and $\ledcensl$, $\Prhall\left[ \modvarhat\in \modgoodl(\modst)\right]\ge 1 - \epspunish$.

\paragraph{Chernoff Bounds} To start, we define empirical estimators of the rewards, initial state, and transition probabilities:
\begin{definition}\label{defn:estimators} For $(x,a,h) \in \calU_{\ell}^c$, define the estimators $\thetar(x,a,h)$, $\thetap(\cdot \mid x,a,h)$  as the empirical means of the first $\nlearn$ samples from the rewards and transitions at $(x,a,h)$. Specifically, if $\Klearn(x,a,h)$ denotes the set of the first $\nlearn$ hallucination episodes $\kexpl$ at which $(x,a,h) \in \tau_{\kexpl}$, we define
\begin{align*}
\thetar(x,a,h) &:= \textstyle\frac{1}{\nlearn}\sum_{k \in \Klearn(x,a,h)} r_{k;h}\\
\thetap(x' \mid x,a,h) &:= \textstyle\frac{1}{\nlearn}\sum_{k \in \Klearn(x,a,h)} \ind(x_{k;h+1} = x').
\end{align*}
Moreover, for all phases $\ell > \nlearn$, we also define the empirical estimate of the initial state distribution from the first $\nlearn$ samples, $\thetap(\cdot \mid 0)$, via
\begin{align*}
\thetap(x' \mid 0) &:= \textstyle\frac{1}{\nlearn}\sum_{\ell = 1}^{\nlearn} \ind(x_{\kexpl;1} = x').
\end{align*}
\end{definition}
Note that these estimators are not actually used by the algorithm; rather, these estimators are used as a surrogate to reason about Bayesian concentration. A couple additional remarks are in order.
\begin{itemize}
	\item $\theta_r\xah$ and $\theta_{\sfp}\xah$ are undefined for $(x,a,h) \in \calU_{\ell}$
	\item If $(x,a,h) \in \calU_{\ell}$, then $\thetar\xah,\thetap\xah$ remain the same for $\ell' \ge \ell$. In addition, $\thetap(x'\mid 0)$ remains fixed for all $\ell > \nlearn$.
	\item We construct the transition at $(x,a,H)$ to always transition to a terminal state $x_{H+1}$, so that $\thetap(\cdot\mid x,a,H) = \sfp_{\model}(\cdot \mid x,a,H) = \mathrm{dirac}_{x'}$ for any $x,a$.
\end{itemize}
Next, we establish the following \emph{frequentist}  concentration bounds for these estimators. For the remainder of this proof, we let $\epsr(\cdot)$ and $\epsp(\cdot)$ have explicity dependence on the failure probability argument $\delta$. Recall that elsewhere, we use only $\epsr := \epsr(\delta_0)$ and $\epsp := \epsp(\delta_0)$.
\begin{lemma}[Chernoff Concentration Bounds]\label{lem:conc_bounds} Recall the error bounds
\begin{align*}
\epsr(\delta) := \sqrt{ \frac{2\log(1/\delta)}{\nlearn}} \quad \text{ and } \epsp(\delta):= 2\sqrt{ \frac{2(S\log(5) + \log(1/\delta))}{\nlearn}}.
\end{align*}
Then, conditioned on any realization of $\modst$, the estimators $\theta_r(x,a,h)$, $\theta_\sfp(\cdot \mid 0)$, $\theta_{\sfp}(x,a,h)$ defined in \Cref{defn:estimators} sastisfy the following bound for any $\xah \in \calU_{\ell}^c$:
\begin{align*}
&\Pr[\xah \in \calU_{\ell}^c \cap \{|\theta_r(x,a,h) - \rmodst(x,a,h)| \ge \epsr(\delta)\} ] \le \delta \quad \text{and}\\
&\Pr[\xah \in \calU_{\ell}^c \cap \{\|\theta_{\sfp}(x,a,h) - \pmodst(\cdot \mid x,a,h)\|_{\ell_1} \ge \epsp(\delta)\} ] \le \delta
\end{align*}
Moreover, for any $\ell \ge \nlearn$, $\Pr[\|\thetap(\cdot \mid 0) - \pmodst(\cdot \mid 0)\|_{\ell_1} \ge \epsp(\delta)] \le \delta$.
\end{lemma}
The proof of \Cref{lem:conc_bounds} is given at the end of this subsection.


\paragraph{Bounding $\Expop\Prhall[\modvarhat \notin \modgoodl(\modst)] $}
\newcommand{\rmterm}{\mathrm{MainTerm}}
\newcommand{\modsttil}{\tilde{\model}_{\star}}

We consider an intermediate bound on the expectation $\Expop\Prhall[\modvarhat \notin \modgoodl(\modst)] $, which we will ultimately apply as an input to Markov's inquality. We establish
\begin{align}
&\Expop_{\modst,\ledhall}\Prhall[\modvarhat \notin \modgoodl(\modst)] \nonumber\\
&\qquad\le \frac{1}{\qpunish} \Expop_{\ledhonl}\Exp_{\modst, \modst' \iidsim  \Pr[\modst = \cdot \mid \ledhonl] } \left[\ind\{\modst \in \modclass_{\ell}(\epspunish) \text{ and } \modst' \notin \modgoodl(\modst) \}\right] \label{eq:expop},
\end{align}
where we use $\Expop_{\modst,\ledhall}$ to make clear that the randomness arises from $\modst$ and $\ledhall$. We render $\EvPun$ explicitly as $\EvPun = \{\modvarhat \in \modclass_{\ell}\}$ (resp. $\EvPun = \{\modst \in \modclass_{\ell}\}$), where we define the punishing model class
\begin{align}\label{eq:modclass_hall}
\modclasshall(\epspunish) :=
    \cbr{ \model \in \modtotal:
    %\forall \xah \notin \calU_{\ell},
    \sfr_{\model}\xah \le \epspunish
    \;\;\text{for all fully-explored $\xah$ triples}
    }.
\end{align}
We shall also write $\modgoodl = \modgoodl(\modst)$ to elucidate the dependence of the set on $\modst$.
With the above notation, we write
\begin{align*}
&\Expop_{\modst,\ledhall}\Prhall[\modvarhat \notin \modgoodl(\modst)]  \\
&= \Expop_{\modst,\ledhall}\left[\Exp[\Prmucan[\modvarhat \notin \modgoodl(\modst) \mid \ledhall] \mid \ledcensl]\right]\\
&\overset{(i)}{=}\Expop_{\modst,\ledhall}\left[\Exp\left[\Prmucan[\modvarhat \notin \modgoodl(\modst) \mid \ledhonl] \mid \ledcensl, \EvPun\right]\right]\\
&= \Expop_{\modst,\ledhall}\frac{\Expop \left[\ind\{\modst \in \modclass_{\ell}(\epspunish)\}\Prmucan[\modvarhat \notin \modgoodl(\modst)\mid \ledhonl] \mid \ledcensl\right]}{\Pr[\modst \in \modclass_{\ell}(\epspunish) \mid \ledcensl]},
\end{align*}
where $(i)$ invokes \Cref{claim:ledhonl_cannonical} to replace the hallucinated ledger with the honest ledger, up to conditioning on $\EvPun$.
Since $\ledcensl$ is hygenic (\Cref{lem:data_hygiene}),  $\Pr[\modst \in \modclass_{\ell}(\epspunish) \mid \ledcensl] = \Prcan[\modst \in \modclass_{\ell}(\epspunish) \mid \ledcensl]$, which is equal to $\Prcan[\EvPun \mid \ledcensl]$, and at most $\qpunish$ by \Cref{rem:reduction-to-independence}. Hence, the above is at most
\begin{align*}
&\Expop_{\modst,\ledhall}\Prhall[\modvarhat \notin \modgoodl(\modst)]  \\
&\le\frac{1}{\qpunish}\Expop_{\modst,\ledhall}\Expop \left[\ind\{\modst \in \modclass_{\ell}(\epspunish)\}\Prmucan[\modvarhat \notin \modgoodl(\modst)\mid \ledhonl] \mid \ledcensl\right]\\
&= \frac{1}{\qpunish}\Expop_{\modst,\ledhonl} \left[\ind\{\modst \in \modclass_{\ell}(\epspunish)\}\Prmucan[\modvarhat \notin \modgoodl(\modst) \mid \ledhonl]\right],
\end{align*}
where in the last line we use the tower rule. By \Cref{lem:data_hygiene}, $\ledhonl$ is hygienic, so that $\Prmucan[\modst \notin \modgoodl(\modst) \mid \ledhonl] = \Prop_{\modst'}[ \modst' \notin \modgoodl(\modst) \mid \ledhonl]$, where $\modst'$ denotes a random variable with the same distribution as $\Prop[\modst \in \cdot \mid \ledhonl]$.   Hence, we may write
\begin{align*}
&\Expop_{\ledhonl, \modst} \left[\ind\{\modst \in \modclass_{\ell}(\epspunish)\}\Prmucan[\modvarhat \notin \modgoodl(\modst) \mid \ledhonl]\right] \\
&= \Expop_{\ledhonl}\Exp_{\modst, \modst' \iidsim  \Pr[\modst = \cdot \mid \ledhonl] } \left[\ind\{\modst \in \modclass_{\ell}(\epspunish)\}\ind\{\modst' \notin \modgoodl(\modst)\}\right]\\
&= \Expop_{\ledhonl}\Exp_{\modst, \modst' \iidsim  \Pr[\modst = \cdot \mid \ledhonl] } \left[\ind\{\modst \in \modclass_{\ell}(\epspunish) \text{ and } \modst' \notin \modgoodl(\modst) \}\right],
\end{align*}
where we represent the internal conditional probability$\Pr[\modst \notin \modgoodl \mid \ledhonl]$ as an expectation over an independt draw of $\modst'$ form the posterior given $\ledhonl$. This establishes \Cref{eq:expop}.


\paragraph{Concluding the Bayesian Chernoff Bound} Recall the estimators $\theta_r,\theta_{\sfp}$ from \Cref{defn:estimators}. These are entirely determined by the data in the honest ledger $\ledhonl$. Moreover by the triangle inequality, we have the inclusion of events
\begin{align*}
\{\modst \in \modclasshall  \text{ and } \modst' \notin \modgoodl(\modst)\} \subseteq \calE_{\mathrm{conc}}(\modst;\ledhonl) \cup  \calE_{\mathrm{conc}}(\modst';\ledhonl)
\end{align*}
where for $\model \in \{\modst,\modst'\}$, we define the concentration event
\begin{align*}
\calE_{\mathrm{conc}}(\model;\ledhonl) = \left\{\exists \xah \in \calU_{\ell} : \begin{matrix}  &\|\sfp_{\model}(\cdot \mid x,a,h) - \theta_{\sfp}(\cdot \mid x,a,h) \|_{\ell_1} \le \epsp(\delta_0) \\
&\text{ or }\lonenorm{\sfp_{\model}(\cdot \mid 0) - \theta_{\sfp}(\cdot \mid 0) } \le \epsp(\delta_0)\\
&\text{ or }  |\sfr_{\model}\xah - \theta_r\xah| \le  \epsr(\delta_0)
\end{matrix} \right\}.
\end{align*}
Hence,
\begin{align*}
&\Expop_{\ledhonl}\Exp_{\modst, \modst' \iidsim  \Pr[\modst = \cdot \mid \ledhonl] } \left[\ind\{\modst \in \modclass_{\ell}(\epspunish) \text{ and } \modst' \notin \modgoodl(\modst) \}\right]\\
&\le \Expop_{ \ledhonl}\Prop_{\modst,\modst' \iidsim \Prcan[\cdot \mid \ledhonl]}\left[\calE_{\mathrm{conc}}(\modst;\ledhonl) \cup  \calE_{\mathrm{conc}}(\modst';\ledhonl) \right]\\
 &\le 2\Prop_{ \ledhonl,\modst}[\calE_{\mathrm{conc}}(\modst;\ledhonl) ].
\end{align*}
where the last step uses a union bound, and the fact that $\modst,\modst'$ have the same distribution. Finally, by \Cref{lem:conc_bounds} and a union bound, we have that $\Prop_{ \ledhonl,\modst}[\calE_{\mathrm{conc}}(\modst;\ledhonl) ] \le 2SAH \delta_0$. Hence, retracing our steps
\begin{align}
\Expop_{\ledcensl}\Prhall[\modvarhat \notin \modgoodl(\modst)]  \le \frac{2\Prop_{ \ledhonl,\modst}[\calE_{\mathrm{conc}}(\modst;\ledhonl) ]}{\qpunish}=  \frac{4SAH \delta_0}{\qpunish}. \label{eq:delta_not_fin}
\end{align}
To conclude, recall the event (over $\ledcensl$) $\Egoodl := \{\Prhall[\modvarhat \notin \modgoodl] \le \epspunish\}$. Then, by a Markov's inequality argument and \Cref{eq:delta_not_fin},
\begin{align*}
\Prop_{\modst,\ledcensl}[\Egoodl] &= \Exp_{\modst,\ledcensl}\ind\{\Prhall[\modvarhat \notin \modgoodl(\modst)] \le \epspunish\}\\
&\le \Exp_{\modst,\ledcensl}\frac{\Prhall[\modvarhat \notin \modgoodl(\modst)]}{\epspunish} \le \frac{4SAH \delta_0}{\epspunish \qpunish}.
\end{align*}
 Choosing $\delta_0 := \frac{\deltafail\epspunish \qpunish}{4SAH}$ concludes.
\qed




\begin{proof}[Proof of \Cref{lem:conc_bounds}] Let us begin the proof assuming that the event $\{\xah \in \calU_{\ell}^c\}$ holds with probability one. We show how to remove this restriction at the end of the proof.

We begin with the concentration bound of $\thetar$.  Fix an $\xah$, and let $k_1,\dots,k_{\nlearn}$ denote the first $\nlearn$ hallucination episodes $k$ on which $\xah \in \traj_k$. Then, if $(\calF_{i})$ denote the filtration under which $\calF_i$ contains all information up the rollout of trajectory $\traj_{k_i}$, we see that $Z_i= r_{k_i;h} - \rmodst(x,a,h)$ is a martingale with respect to $(\calF_i)$, and $|Z_i| \le 1$. Hence, by the Azuma-Hoeffding inequality and a union bound over signs of the error,
	\begin{align*}
	|\thetar(x,a,h) - \rmodst(x,a,h)| = \left|\frac{1}{\nlearn} \sum_{i=1}^{\nlearn} Z_i\right| \le \sqrt{ \frac{2\log(2/\delta)}{\nlearn}}, \text{ with probability } 1-\delta.
	\end{align*}
	Recognizing the above error bound as $\epsr(\delta)$ concludes. Next, let us adress $\thetap(\xah)$. By Holder's inequality,
	\begin{align*}
	\lonenorm{\thetap(\cdot \mid \xah) - \pmodst(\cdot \mid \xah)} &= \max_{v\in [-1,1]^S} \langle v, \thetap(\cdot \mid \xah) - \pmodst(\cdot \mid \xah)\rangle.
	\end{align*}
	Now, let $\calN := \{-1,-1/2,0,1/2,1\}^S$ be a covering over $[-1,1]$. Then, any $v \in [-1,1]^S$ can be expressed as $v = v_1 + v_2$, where $v_1 \in \calN$, and $v_2 \in [-1/2,1/2]^S$. Hence,
	\begin{align}
	&\lonenorm{\thetap(\cdot \mid \xah) - \pmodst(\cdot \mid \xah)} \\
	&\quad \le \max_{v_1\in \calN} \langle v_1, \thetap(\cdot \mid \xah) \nonumber - \pmodst(\cdot \mid \xah)\rangle + \max_{v_2\in [-1/2,1/2]} \langle v_1, \thetap(\cdot \mid \xah) - \pmodst(\cdot \mid \xah)\rangle \nonumber\\
	&\qquad=  \max_{v\in \calN} \langle v, \thetap(\cdot \mid \xah) - \pmodst(\cdot \mid \xah)\rangle + \frac{1}{2}\lonenorm{\thetap(\cdot \mid \xah) - \pmodst(\cdot \mid \xah)}. \label{eq:cover_eq}
	\end{align}
	Rearranging, $\lonenorm{\thetap(\cdot \mid \xah) - \pmodst(\cdot \mid \xah)}  \le 2 \max_{v\in \calN} \langle v, \thetap(\cdot \mid \xah) - \pmodst(\cdot \mid \xah)\rangle$.
	Now, let $(\calF_i)$ and $k_i$ be as above. For each $v \in \calN$, let $v(x')$ denote its coordinates, and define $W_{i,v}= \sum_{x'\in S} v(x') \cdot (\ind(x_{k_i;h+1} =x') - \pmodst(x' \mid x,a,h))$. Then, we see that $W_{i,v}$ form a martingale with respect to $(\calF_i)$, and
	\begin{align*}
	\langle v, \thetap(\cdot \mid \xah) - \pmodst(\cdot \mid \xah)\rangle = \frac{1}{\nlearn}\sum_{i=1}^{\nlearn} W_{i,v}.
	\end{align*}
	We can also verify that $|W_{i,v}| \le \sum_{x'\in S} |v(x')| |(I(x_{k_i;h+1} =x') - \pmodst(x' \mid x,a,h)| \le\sum_{x'\in S} |v(x')| = 1 $.  Thus, again by Azuma-Hoeffding, we have that with probability $1 - \delta$,
	\begin{align*}
	\langle v, \thetap(\cdot \mid \xah) - \pmodst(\cdot \mid \xah)\rangle \le \sqrt{ \frac{2\log(1/\delta)}{\nlearn}}.
	\end{align*}
	By a union bound over all $v \in \calN$, which has $|\calN| = 5^{S}$, we have that with probability $1 - \delta$,
	\begin{align*}
	\langle v, \thetap(\cdot \mid \xah) - \pmodst(\cdot \mid \xah)\rangle \le \sqrt{ \frac{2(S\log(5) + \log(1/\delta))}{\nlearn}}.
	\end{align*}
	And thus, from $\Cref{eq:cover_eq}$, it holds that with probability $1- \delta$,
	\begin{align*}
	\lonenorm{\thetap(\cdot \mid \xah) - \pmodst(\cdot \mid \xah)} \le 2\sqrt{ \frac{2(S\log(5) + \log(1/\delta))}{\nlearn}} := \epsp(\delta).
	\end{align*}
	The argument for $\thetap(\cdot \mid 0)$ is analogous.
	\end{proof}


\paragraph{Handling the randomness of the event $\{\xah \in \calU_{\ell}^c\}$} To conclude, we address that the event $\{\xah \in \calU_{\ell}^c\}$ is random, and hence, $\thetar$ or $\thetap$ are well-defined is random. Here we explain why our reasoning above still remains valid. For simplicity, we explain how to modify the reasoning for $\thetar$; adjusting $\thetap$ is the same.

Let $n_{\ell}\xah$ denote the number of times triple $\xah$ is visited by phase $\ell$, and let $\calK_{\ell}\xah$ denote the \emph{epxloration} episodes on which $\xah \in \traj_{k}$ is visited. We extend the definition of $\thetar$ to cases where $\{\xah \notin \calU_{\ell}^c\}$ by defining
\begin{align*}
\tilde{\theta}_{\sfr}\xah = \begin{cases} \thetar\xah \text{ as in   \Cref{defn:estimators}} & \xah \in \calU_{\ell}^c \\
\frac{1}{\nlearn}\left(\sum_{k\in \calK_{\ell}\xah}r_{k;h} + \sum_{i=1}^{\nlearn - n_{\ell}\xah} \tilde{r}_i\right),  \tilde{r}_i \iidsim \mathsf{R}_{\modst}\xah & \text{otherwise}
\end{cases}
\end{align*}
In other words, when $\xah \in \calU_{\ell}$, one draws an additional $\nlearn - n_{\ell}\xah$ rewards from the reward distribution conditioned on $\modst$, i.e. $\mathsf{R}_{\modst}\xah $, and uses these to complete the estimator $\tilde{\theta}_{\sfr}\xah$. By construction, whenever $\xah \in \calU_{\ell}^c$, $\tilde{\theta}_{\sfr} = \theta_{\sfr}$, and hence
\begin{multline}
\Pr[\xah \in \calU_{\ell}^c \cap \{|\theta_r(x,a,h) - \rmodst(x,a,h)| \ge \epsr(\delta)\} ] \\
 = \Pr[\xah \in \calU_{\ell}^c \cap \{|\tilde{\theta}_r(x,a,h) - \rmodst(x,a,h)| \ge \epsr(\delta)\} ] \\
 \le \Pr[|\tilde{\theta}_r(x,a,h) - \rmodst(x,a,h)| \ge \epsr(\delta) ] \label{eq:in_view_of}
\end{multline}
Hence, it suffices to reason about the concentration of $\tilde{\theta}_r$. Moreover, $\tilde{\theta}_r$ satisfies the same concentration inequality we derived above for $\theta_r\xah$, because it also admits a similar martingale decomposition (with bounded increments). Hence, we can apply the above argument to reason about the concentration of $\tilde{\theta}_r$, and use this to reason about the concentration of $\thetar$ in view of \Cref{eq:in_view_of}.

\begin{comment}
We also define the censoring operator on ledgers and sequences of trajectories in the natural way:  $\cens(\traj_{\pi;1:m}) = (\cens(\traj_{\pi;1},\dots,\traj_{\pi;m}))$, and given a ledger $\ledger \in \ledgespacetotm$ as a collection policy-trajectory pairs $(\pi,\traj_{\pi;1:m}))_{\pi \in \Pi_{\ledger}}$, we set $\cens(\ledger) := (\pi,\cens(\traj_{\pi;1:m}))_{\pi \in \Pi_{\ledger}}$. Lastly, we define an ``inverse'' of the censoring operator $\censinv: \ledgespacecenstotm \to \subsets(\ledgespacetotm)$ via
\begin{align}
\censinv(\ledgecens) := \{\ledger \in \ledgespacetotm: \cens(\ledger) = \ledgecens\}
\end{align}
\end{comment}





\subsection{Proof of  \Cref{lem:visitation_comparison_general} \label{proof:visitation_comparison_general}}


	\newcommand{\barmod}{\bar{\model}}
	\newcommand{\barmodst}{\bar{\model}_{\star}}
	\newcommand{\rbar}{\bar{r}}
	\newcommand{\calUbar}{\bar{\calU}}

	{\textbf{The `in-particular' clause.}} Before diving into the proof, we establish the special case depicted in the ``in particular'' clause. This is achieved by taking the reward function $\rtil(x,a,h) := \ind\{(x,a,h) \in \calU\}$. Then, we have
	\begin{align*}
	\sum_{h=1}^H \rtil(\bmx_h,\bma_h,h) \ind\{\scrE_h\} &=  \sum_{h=1}^H  \ind\{(\bmx_h,\bma_h,h) \in \calU \text{ and } (\bmx_\tau,\bma_{\tau},\tau) \in \calU^c,~ \forall \tau < h\}.
	\end{align*}
	Note that these events inside the indicator are all disjoint, and their union is precisely the event that $\{\exists h: (\bmx_h,\bma_h,h) \in \calU\}$, which is precisely $\Evisitnol$.
	\\
	\\
	{\textbf{Main result.}} Next, we turn to the proof of the main lemma, we begin by establishing the special case of Markovian policies. Since we consider a fixed policy $\pi$ for both models $\model,\modst$, for simplicity, we may assume that $\model,\modst$ are Markov reward processes (i.e.  $A = 1$, and thus no policy), suppressing dependences on $\pi$ and $a$. For convenience, we remove the actions. For reasons that will become clear shortly, we also embed into $S+1$ states, and take the reward function $\rbar$ and set $\calU$ as
	\begin{align*}
	&\rbar(x,h) \gets \begin{cases}\rtil(x,\pi(x,h),h)&  x \in [S]\\
	0 & x = S+1 \end{cases}, \\
	&\calUbar \gets \{(x,h): (x,\pi(x,h),h) \in \calU\} \cup \{(S+1,h) : h \in [H]\},\\
	&\scrE_h = \{(\bmx_{\tau},\tau) \in \calUbar^c, ~\forall \tau < h\}
	\end{align*}
	With this setup, it suffices to show
	\begin{align}
	\binom{H}{2}\varepsilon &\ge   \left|\sfE^{\pi}_{\model}\left[\sum_{h=1}^H \rbar(\bmx_h,h) \ind\{\scrE_h\}\right] - \sfE^{\pi}_{\modst}\left[\sum_{h=1}^H \rbar(\bmx_h,h) \ind\{\scrE_h\}\right]\right|, \label{eq:MDP_desired}
	\end{align}
	To establish \Cref{eq:MDP_desired}, we construct reward processes  $\barmod,\barmodst$ over $S+1$ states which absorb the indicators $\ind\{\scrE_h\}$.
	\begin{align}\label{eq:equal_expectation}
	\sfE_{\barmod}\left[\sum_{h=1}^H \rbar(\bmx_h,h)\right] =
	\sfE^{\pi}_{\model}\left[\sum_{h=1}^H \rbar(\bmx_h,h) \ind\{\scrE_h\}\right],
	\end{align}
	and similarly with $\barmodst$ and $\modst$. Let us construct $\barmod$; the construction of $\barmodst$ is indentical.
	Let the initial state distribution be indentical: $\sfp_{\barmod}(x' \mid 0) \equiv \sfp_{\model}(x' \mid 0)$, and set the transition probabilities in $\barmod$ to be the MDP which coincides with $\model$ on states $(x,h) \in \calU^c$, but transitions to state $S+1$ on states $(x,h) \in \calU$:
	\begin{align*}
	\sfp_{\barmod}(x' \mid x,h) = \begin{cases} \sfp_{\model}(x' \mid x,h) & (x,h) \in \calUbar^c \\
	  \ind\{x' = S+1\} & (x,h) \in \calUbar \\
	\end{cases}
	\end{align*}
	We define $\barmodst$ analogously. By construction, and by our assumption on the initial state distribution and the transitions in $\calUbar^c$, we observe that
	\begin{align}
	&\|\sfP_{\barmodst}(\cdot \mid 0) - \sfp_{\barmod}(\cdot \mid 0)\|_{\ell_1} = \|\sfp_{\modst}(\cdot \mid 0) - \sfp_{\model}(\cdot \mid 0)\|_{\ell_1} \le \varepsilon \label{eq:close_init_dist}\\
	&\max_{x,h}\|\sfp_{\barmodst}(\cdot \mid x,h) - \sfp_{\barmod}(\cdot \mid x,h)\|_{\ell_1} = \max_{(x,h) \in \calUbar^c}\|\sfp_{\modst}(\cdot \mid x,h) - \sfp_{\model}(\cdot \mid x,h)\|_{\ell_1} \le \varepsilon.\label{eq:close_trans_dist}
	\end{align}
	We now verify that our construction satisfies \Cref{eq:equal_expectation}.

	\begin{claim}\label{claim:expected_reward_identity} For $\barmod,\model$ above, \Cref{eq:equal_expectation} holds, and similarly for $\barmodst,\modst$.
	\end{claim}
	\begin{proof}

		\newcommand{\barbmx}{\bar{\bmx}}
		We establish the equality for $\barmod,\model$. Let $(\bmx_1,1),(\bmx_2,2),\dots,(\bmx_H,H) \sim \sfP_{\model}$. Introduce the coupled sequence $\barbmx_1,\barbmx_2,\dots,\barbmx_H$ via
		\begin{align*}
		\barbmx_h := \begin{cases} S+ 1& (\bmx_{h-1},h-1) \in \calUbar \text{ or } \barbmx_{h-1} = S+1\\
		\bmx_h & \text{otherwise}\end{cases}
		\end{align*}
		We immediately see that $(\barbmx_1,1),(\barbmx_2,2),\dots,(\barbmx_H,H) \sim \sfP_{\barmod}$. Moreover, observe that,
		\begin{itemize}
			\item On the joint space on which the $(\bmx_h,\barbmx_h)$ sequence is defined, $\scrE_h^c = \{\exists \tau < h: (\bmx_{\tau},\tau) \in \calU\} =\{ \barbmx_h = S+1\}$
			\item If $\barbmx_h \ne S+1$, then $\barbmx_h = \bmx_h$.
		\end{itemize}
		Thus, letting $\sfE$ denote the law of the coupled sequences,
		\begin{align}
		\sfE\left[\sum_{h=1}^H \rbar(\barbmx_h,h)\right] &= \sfE\left[\sum_{h=1}^H \rbar(\barbmx_h,h)(1 - \ind\{ \barbmx_h = S+1\})\right] \tag{$\rbar(S+1,\cdot) \equiv 0$ by construction}\\
		&=\sfE\left[\sum_{h=1}^H \rbar(\barbmx_h,h)\ind\{ \barbmx_h \ne S+1\})\right] \nn\\
		&=\sfE\left[\sum_{h=1}^H \rbar(\bmx_h,h)\ind\{ \barbmx_h \ne S+1\})\right] \tag{if $\barbmx_h \ne S+1$, then $\barbmx_h = \bmx_h$}\\
		&= \sfE\left[\sum_{h=1}^H \rbar(\bmx_h,h)\ind\{\scrE_h\}\right] \tag{$\scrE_h^c = \{ \barbmx_h = S+1\}$}.
		\end{align}
		Since the coupled distribution of $\bmx_{1:H}$ and $\barbmx_{1:H}$ under $\sfE$ has marginals $\bmx_{1:H} \sim \sfP_{\model}$ and $\barbmx_{1:H} \sim \sfP_{\barmod}$, the indentity follows.
	\end{proof}

	To conclude, we invoke the ubiquitous performance difference lemma (see e.g. \citep[Lemma 5.3.1]{kakade2003sample}), specialized to Markov reward processes:
	\begin{lemma}[Performance Difference Lemma for MRPs]\label{lem:perf_diff} Let $\model_1,\model_2$ be two MRPs with state space $[S']$ and horizon $H$, and common reward function $r$. Define the value functions $V^{\model_i}_h(x) := \Exp^{\model_i}[\sum_{\tau=h}^{H} r(\bmx_h,h)  \mid \bmx_h = x]$. Then,
	\begin{align*}
	&\sfE_{\model_1}\left[\sum_{h=1}^H r(\bmx_h,h)\right] - \sfE_{\model_2}\left[\sum_{h=1}^H r(\bmx_h,h)\right] \\
	&\quad=  (\sfp_{\model_1}(\cdot \mid 0) - \sfp_{\model_2}(\cdot \mid 0)^\top V^{\model_2}_{1}(\cdot)  +  \sfE_{\model_1}\left[\sum_{h=1}^H (\sfp_{\model_1}(\cdot \mid \bmx_h,h) - \sfp_{\model_2}(\cdot \mid \bmx_h,h))^\top V^{\model_2}_{h+1}(\cdot)\right]
	\end{align*}
	\end{lemma}
	Applying \Cref{lem:perf_diff} and ,  we have
	\begin{align*}
	&\left|\sfE_{\barmod}\left[\sum_{h=1}^H \rbar(\bmx_h,h)\right] - \sfE_{\barmodst}\left[\sum_{h=1}^H \rbar(\bmx_h,h)\right]\right|\\
	&= \left|\sfp_{\barmod}(\cdot \mid 0) - \sfp_{\barmodst}(\cdot \mid 0 )^\top V^{\barmodst}_{h+1}(\cdot) + \sfE_{\barmod}\left[\sum_{h=1}^H (\sfp_{\barmod}(\cdot \mid \bmx_h,h) - \sfp_{\barmodst}(\cdot \mid \bmx_h,h))^\top V^{\barmodst}_{h+1}(\cdot)\right]\right|\\
	&\le \|\sfp_{\barmod}(\cdot \mid 0) - \sfp_{\barmodst}(\cdot \mid 0 )\|_{\ell_1} \max_{x \in [S+1]} |V^{\barmodst}_{1}(\cdot)| +
	\sfE_{\barmod}\left[\sum_{h=1}^H \|\sfp_{\barmod}(\cdot \mid \bmx_h,h) - \sfp_{\barmodst}(\cdot \mid \bmx_h,h)\|_{\ell_1} \cdot \max_{x \in [S+1]} |V^{\barmodst}_{h+1}(\cdot)|\right]\\
	&\le \varepsilon \sum_{h=1}^{H+1} \max_{x \in [S+1]} |V^{\barmodst}_{h+1}(\cdot)|,
	\end{align*}
	where the last inequality uses \Cref{eq:close_trans_dist,eq:close_init_dist}. Now, since the rewards $\rbar$ lie in $[0,1]$,we have $\max_{x \in [S+1]} |V^{\barmodst}_{h}(\cdot)| \le 1 + H - h$. Hence, $\sum_{h=1}^{H+1} \max_{x \in [S+1]} |V^{\barmodst}_{h+1}(\cdot)| \le \binom{H}{2}$, yielding
	\begin{align*}
	\binom{H}{2}\varepsilon &\ge  \left|\sfE_{\barmod}\left[\sum_{h=1}^H \rbar(\bmx_h,h)\right] - \sfE_{\barmodst}\left[\sum_{h=1}^H \rbar(\bmx_h,h)\right]\right| \\
	&= \left|\sfE^{\pi}_{\model}\left[\sum_{h=1}^H \rbar(\bmx_h,h) \ind\{\scrE_h\}\right] - \sfE^{\pi}_{\modst}\left[\sum_{h=1}^H \rbar(\bmx_h,h) \ind\{\scrE_h\}\right]\right|,
	\end{align*}
	where the last step uses \Cref{claim:expected_reward_identity}, thereby proving \Cref{eq:MDP_desired}.
	\qed

\subsection{Proof of \Cref{claim:value_upper_bound,claim:omega_star,claim:value_lower_bound,claim:value_diff,claim:value_diff_expectation} \label{sec:proof_of_prob_claims}}
	\begin{proof}[Proof of \Cref{claim:value_upper_bound} ]


		Recalling the $\sfP$-measurable event $\Evisit = \{\exists h  : (\bmx_h,\bma_h,h) \in \calU_{\ell}\}$, we have that for any $\pi \in \Pimarkov$ and $\model \in \modgoodl$,
		\begin{align*}
		\valuef{\pi}{\model} &= \sfE^{\pi}_{\model}\left[\sum_{h=1}^H \sfr_{\model}(\bmx_h,\bma_h,h) \right] \\
		&\le H \Pr^{\pi}_{\model}\left[ \Evisit \right] + H \max_{\xah \in \calU_{\ell}^c} \sfr_{\model}\xah \nn\\
		&\le H \Pr^{\pi}_{\model}\left[ \Evisit \right] + H (2\epsr + \epspunish) \tag*{(\Cref{eq:modgoodl})}\\
		&\le H \Pr^{\pi}_{\modst}\left[ \Evisit \right] + H (2\epsr + \epspunish) + \binom{H}{2}\cdot 2\epsp \tag*{(\Cref{lem:visitation_comparison_general})},
		\end{align*}
		where the last line uses \Cref{lem:visitation_comparison_general}
		 with $\calU \gets \calU_{\ell}$ and the $\ell_1$ bound of $2\epsp$ on difference in transitions and intial state probabilities from the definition of $\modgoodl$ in \Cref{eq:modgoodl}.
	\end{proof}

	\begin{proof}[Proof of \Cref{claim:omega_star}] First Point: the events $\{(\bmx_h,\bma_{h}) = (x,a) \text{ and } (\bmx_\tau,\bma_{\tau},\tau)\in \calU_{\ell}^c, \quad \forall \tau < h\}$ are disjoint, and $\Evisit$ holds precisely if and only if at least one of these events holds. Note that this  does not use any specific properties of $\calU_{\ell}$. Second Point: if $\calU_{\ell} \cap \reach_{\rho}(\modst)$ is non-empty, then there exists a triple $(x,a,h) \in \calU_{\ell}$ and a policy $\pi$ for which policy $\pi$ for which $\sfP_{\modst}^{\model}[(\bmx_h, \bma_h,h) = (x,a,h)] \ge \rho$. Moreover,
\begin{align*}
\rho &\le \sfP_{\modst}^{\pi}[(\bmx_h, \bma_h,h) = (x,a,h)] \\
&= \sfP_{\modst}^{\pi}[(\bmx_h, \bma_h,h) = (x,a,h) \text{ and } \forall \tau < h, (x,a,\tau) \in \calU_{\ell}^c]  \\
&\qquad+ \sfP_{\modst}^{\pi}[(\bmx_h, \bma_h,h) = (x,a,\tau) \text{ and } \exists \tau < h :  (x,a,h) \in \calU_{\ell}] \tag*{(total probability)}\\
&\le \sfP_{\modst}^{\pi}[\exists h' \ge h: (x,a,h')\in \calU_{\ell} \text{ and } \forall \tau < h, (x,a,\tau) \in \calU_{\ell}^c]  + \sfP_{\modst}^{\pi}[\exists \tau < h :  (x,a,\tau) \in \calU_{\ell}]\\
&= \sfP_{\modst}^{\pi}[\exists \tau  :  (x,a,\tau) \in \calU_{\ell}] := \sfP_{\modst}^{\pi}[\Evisit]\tag*{(disjoint union)}
\end{align*}
In particular, $\pi \in \Pi_{\ell}$, since $\sfP_{\modst}^{\pi}[\Evisit] \ge \rho \ge \rho_0$. The second part of the claim now follows from the first.
\end{proof}

	\begin{proof}[Proof of \Cref{claim:value_lower_bound}]

		Let $\model \in \modgoodl$. Adopting the notation of \Cref{lem:visitation_comparison_general} with $\calU \gets \calU_\ell$, set $\scrE_h := \{(\bmx_\tau,\bma_{\tau},\tau) \in \calU_\ell^c,~ \forall \tau < h\}$. Since the rewards are non-negative, we lower bound the value by a sum over rewards times indicators of $\scrE_h$, and invoke \Cref{lem:visitation_comparison_general} with $\rtil = \sfr_{\model}$ together with the definition of $\modgoodl$ in \Cref{eq:modgoodl}:
		\begin{align*}
		\valuef{\pi}{\model} &= \sfE^{\pi}_{\model}\left[\sum_{h=1}^H \sfr_{\model}\xah \right]\\
		&\ge \sfE^{\pi}_{\model}\left[\sum_{h=1}^H \sfr_{\model}\xah \ind(\scrE_h)\right]\\
		&\ge  \sfE^{\pi}_{\modst}\left[\sum_{h=1}^H \sfr_{\model}\xah \ind(\scrE_h)\right] - \binom{H}{2}\cdot 2\epsp
		\end{align*}
		Again, by nonnegativity of the rewards $\sfE^{\pi}_{\modst}\left[\sum_{h=1}^H \sfr_{\model}\xah \ind(\scrE_h)\right] = \sum_{x,a,h} \sfr_{\model}\xah \cdot \omegast\xah \ge \sum_{(x,a,h) \in \calU_{\ell}} \sfr_{\model}\xah \cdot \omegast\xah$. The bound bound follows.
	\end{proof}

	\begin{proof}[Proof of \Cref{claim:value_diff}]

		Let $\pi_1 \in \Pimarkov$, and $\pi_2 \in \Pi_{\ell}^c$. Fix a $\model \in \modgoodl$. By definition of $\Pi_{\ell}^c$, $\Pr^{\pi_2}_{\modst}\left[ \Evisit \right] \le \rho_0$,  \Cref{claim:value_upper_bound} ensures $\valuef{\pi_2}{\model} \le H \rho_0 + H \epspunish + 2 \epsr + H(H-1)\epsp$. On the other hand, \Cref{claim:value_lower_bound} ensures $\valuef{\pi_1}{\model}  \ge \sum_{(x,a,h) \in \calU_{\ell}} \sfr_{\model}\xah \cdot \omegast^{\pi_1}\xah - H(H-1)\epsp$, Hence,
		\begin{align*}
		&\valuef{\pi_1}{\model} - \valuef{\pi_2}{\model} \\
		&\quad\ge \sum_{(x,a,h) \in \calU_{\ell}} \sfr_{\model}\xah \cdot \omegast^{\pi_1}\xah -H \rho_0 - H \epspunish  - 2 \epsr -  2H(H-1)\epsp\\
		&\quad\ge \sum_{(x,a,h) \in \calU_{\ell}} \sfr_{\model}\xah \cdot \omegast^{\pi_1}\xah -H \left(\rho_0 + \epspunish  + 2H\epsp\right),
		\end{align*}
		where we  use that $\epsp \le \epsr$.
	\end{proof}

	\begin{proof}		To streamline the proof, we condense notation. Introduce the shorthand
		\begin{align*}
		Z^{\pi}_{\model} := \valuef{\pi}{\model} - \max_{\pi' \in \Pi_{\ell}^c}\valuef{\pi'}{\model} , \quad \text{and} \quad W^{\pi}_{\model} := \sum_{(x,a,h) \in \calU_{\ell}}\sfr_{\model}\xah \cdot \omegast^{\pi}\xah
		\end{align*}
		 and let $\varepsilon := H \left(\rho_0 + \epspunish  + 2H\epsp\right)$. Finally, recall the shorthand $\Exp_{\mathrm{hal}}[\cdot] := \Expsimhh \Expcan[\cdot \mid \ledhall]$ and set $\Pr_{\mathrm{hal}}[\cdot] := \Expsimhh \Prcan[\cdot \mid \ledhall]$. Now, \Cref{claim:value_diff} implies that, for all $\model \in \modgoodl$, $Z^{\pi}_{\model} \ge W^{\pi}_{\model} - \varepsilon$.  Since $Z^{\pi}_{\model} \in [-H,H]$ and $W^{\pi}_{\model} \in [0,H]$, we have
		\begin{align*}
		\Exphall[\valuef{\pi}{\modvarhat} - \max_{\pi' \in  \Pi^c} \valuef{\pi'}{\modvarhat} ] &= \Exphall[ Z^{\pi}_{\modvarhat} ]\\
		&\ge \Exphall[ \ind\{\modvarhat \in \modgoodl \} Z^{\pi}_{\modvarhat}  ] - H \Exphall[ \modvarhat \notin \modgoodl ] \\
		&\ge \Exphall[ \ind\{\modvarhat \in \modgoodl \} (W^{\pi}_{\modvarhat} - \varepsilon)  ] - H \Exphall[ \modvarhat \notin \modgoodl    ] \\
		&\ge \Exphall[  W^{\pi}_{\modvarhat} ] - \varepsilon - 2H \Exphall[ \modvarhat \notin \modgoodl  ].
		\end{align*}
		By \Cref{lem:hallucination_good_event}, we have that on $\Egoodl$, $\Exphall[ \modvarhat \notin \modgoodl  ] \le \epspunish$. Moreover, by linearity of expectation, and the fact that $\Exphall[\cdot]$ treats $\modst$ as deterministic,
		\begin{align*}
		\Exphall[  W^{\pi}_{\modvarhat} ] &= \sum_{(x,a,h) \in \calU_{\ell}} \Exphall[\sfr_{\model}\xah] \cdot \omegast^{\pi}\xah.
		\end{align*}
		Putting things together, we conclude,
		\begin{align*}
		&\Exphall[\valuef{\pi}{\modvarhat} - \max_{\pi' \in  \Pi^c} \valuef{\pi'}{\modvarhat} ] \\
		&\qquad\ge\sum_{(x,a,h) \in \calU_{\ell}} \Exphall[\sfr_{\model}\xah] \cdot \omegast^{\pi}\xah.  - 2\epspunish - \varepsilon \\
		&\qquad= \sum_{(x,a,h) \in \calU_{\ell}} \Exphall[\sfr_{\model}\xah] \cdot \omegast^{\pi}\xah  - H \left(\rho_0 + 3\epspunish  + 2H\epsp\right).
		\end{align*}
		Finally, we can lower bound
		\begin{align*}
		\sum_{(x,a,h) \in \calU_{\ell}} \Exphall[\sfr_{\model}\xah]  &\ge \min_{\xah \in \calU_{\ell}} \Exphall[\sfr_{\model}\xah] \cdot \sum_{(x,a,h) \in \calU_{\ell}} \omegast^{\pi}\xah\\
		&= \sfP_{\modst}^{\pi} \cdot\min_{\xah \in \calU_{\ell}} \Exphall[\sfr_{\model}\xah],
		\end{align*}
		where the last equality uses the indentity \Cref{claim:value_lower_bound}. Hence, setting $\varepsilon_0 :=  H \left(\rho_0 + 3\epspunish  + 2H\epsp\right)$, we have
		\begin{align*}
		\Exphall[\valuef{\pi}{\modvarhat} - \max_{\pi' \in  \Pi^c} \valuef{\pi'}{\modvarhat} ] &\ge \sum_{(x,a,h) \in \calU_{\ell}} \Exphall[\sfr_{\model}\xah] \cdot \omegast^{\pi}\xah  - \varepsilon_0\\
		&\ge \sfP_{\modst}^{\pi} \cdot\min_{\xah \in \calU_{\ell}} \Exphall[\sfr_{\model}\xah] -  \varepsilon_0. \qquad\qedhere
		\end{align*}
	\end{proof}




\subsection{Proof of \Cref{thm:main_prob_mdp} from \Cref{lem:main_prob_lemma} \label{proof:main_prob_mdp}}
	\newcommand{\bbar}{\widebar{b}}
	\newcommand{\Efinl}[1][\ell]{\calE_{\mathrm{fin};#1}}


	Let us first recall the relevant parameters and assumptions. Recall the per-phase failure probability $\deltafail$, set $\Delta_0 = \rho\ralt(\epspunish)/2$,
	\begin{align*}
	\textstyle n_0(\deltafail) := \frac{96 H^4((S\log(5) + \log(1/\delta_0))}{\Delta_0^2}, \quad \text{ where } \delta_0 = \frac{\deltafail \cdot \qpunish \cdot \epspunish}{4SAH}.
	\end{align*}
	Recall the exploration event $\Eexplorel := \{\exists h: (x\kh,a\kh,h) \in \calU_{\ell}, ~k := \kexpl\}$. Define the ``finishing'' event $\Efinl := \{\calU_\ell \cap \reach_{\rho}(\modst)\}$, on which we have successfully $(\rho,\nlearn)$ explored by phase $\ell$. We also define
	 Bernoull random variables $B_{\ell} := \ind\{\Eexplorel\}$, which are $\calF_{\ell}$-measurable, and their deviations from their conditional expectations $\bbar_{\ell} :=\Exp[B_{\ell} \mid \calF_{\ell}]$. From \Cref{lem:main_prob_lemma}, it thus holds that
	 \begin{align}
	 \Pr[ \bbar_{\ell} \le \rhoprog \text{ and not } \Efinl] \le 1 - \deltafail, \quad\text{where }\rhoprog := \frac{\Delta_0^2}{6H^2} = \frac{\rho^2\ralt(\epspunish)^2}{36H^2} \label{eq:main_prob_lemma_guar}
	\end{align}

	Now, suppose that for a given phase $L \ge 0$, we have that $\calU_{L+1} \cap \reach_{\rho}(\modst) \ne \emptyset$ (i.e., $\Efinl[L+1]$ fails). The same must be be true for all $\ell \le L+1$ since $\calU_{\ell}$ is non-decreasing across phases $\ell$. Hence, by a union bound
	\begin{align*}
	&\Pr[ \text{not } \Efinl[L+1] \text{ and } \sum_{\ell=\nlearn + 1}^{L} \bbar_{\ell} \le (L-\nlearn) \rhoprog] \\
	&\quad\le (L-\nlearn) \max_{\ell \in \{\nlearn+1,\dots,L\}}\Pr[  \text{not } \Efinl[L+1] \text{ and } \bbar_{\ell} \le  \rhoprog]\\
	&\quad\le (L-\nlearn) \max_{\ell \in \{\nlearn+1,\dots,L\}}\Pr[ \text{not } \Efinl[\ell] \cap \reach_{\rho}(\modst) \ne \emptyset \text{ and } \bbar_{\ell} \le  \rhoprog]\\
	&\quad \le (L-\nlearn) \deltafail \tag*{by \Cref{eq:main_prob_lemma_guar}}
	\end{align*}
	On the other hand, by the pidgeonhole principle, it must be the case that
	\begin{align}
	\sum_{\ell=1}^L B_{\ell} \le SAH \nlearn, \label{eq:B_l_bound}
	\end{align}
	since each time $B_{\ell} = 1$, one triple $\xah \in \calU_{\ell}$ is visited during a hallucination episode, and each triple $\xah \in \calU_{\ell}$ can only be visited a maximum of $\nlearn$ times during hallucination episodes. Hence, if we consider phase $L_0 := 4SAH\nlearn/\rhoprog$ and $L_1 = L_0 + \nlearn$ (incrementing $L_0$ by $\nlearn$, so $L_0 = L_1 - \nlearn$), and we have
	\begin{align*}
	\Pr[ \text{not } \Efinl[L_1+1] ] &= \Pr[ \text{not } \Efinl[L_1+1] \text{ and } \sum_{\ell=\nlearn + 1}^{L_1} \bbar_{\ell} \le L_0 \rhoprog] + \Pr[ \text{not } \Efinl[L+1] \text{ and } \sum_{\ell=\nlearn +1}^{ L_1} \bbar_{\ell} > L_0 \rhoprog]\\
	&\le L_0 \deltafail + \Pr\left[  \sum_{\ell=\nlearn+1}^{L_1} \bbar_{\ell} > L_0 \rhoprog\right]\\
	&= L_0 \deltafail + \Pr\left[  \sum_{\ell=1+\nlearn}^{L_1} \bbar_{\ell} > L_0 \rhoprog \text{ and } \sum_{\ell=1+\nlearn}^{L_1} B_{\ell} \le SAH \nlearn \right] \tag{\Cref{eq:B_l_bound}} \\
	&= L_0 \deltafail + \Pr\left[  \sum_{\ell=\nlearn+1}^{L_1} \bbar_{\ell} >  4SAH\nlearn \text{ and } \sum_{\ell=\nlearn+1}^{L_1} B_{\ell} \le SAH \nlearn  \right] \tag{Definition of $L_0$}.
	%\text{ and } \sum_{\ell=1}^{L} \bbar_{\ell} \le L \rhoprog
	\end{align*}
	To conclude, let us use a concentration inequality to bound the probability on the final display. Applying a standard martingale Chernoff bound (Lemma F.4 in \cite{dann2017unifying}, with $X_\ell \gets B_{\ell}$, $P_{\ell} \gets \bbar_{\ell}$, and $W =  SAH \nlearn$) yields that
	\begin{align*}
	\Pr\left[  \sum_{\ell=\nlearn+1}^{L_1} \bbar_{\ell} >  4SAH\nlearn \text{ and } \sum_{\ell=\nlearn+1}^{L_1} B_{\ell} \le SAH \nlearn  \right] \le e^{-W} = e^{- SAH \nlearn},
	\end{align*}
	and thus, for $L_0 := 4SAH\nlearn/\rhoprog = 36SAH^3\nlearn/\Delta_0^2$, and $\nlearn = L_0 + \nlearn \le 37SAH^3\nlearn/\Delta_0^2$
	\begin{align}
	\Pr[ \text{not } \Efinl[L_1+1] ] \le L_0 \deltafail  + e^{- SAH \nlearn},
	\end{align}
	To conclude, we note that when $\Efinl[L_0+1]$, then we have $(\rho,\nlearn)$ explored by episode $K_0 = L_1 \cdot \nphase = 4SAH\nlearn \nphase = \frac{37SAH^3 \nlearn\nphase}{\Delta_0^2}$.

	\newcommand{\deltapun}{\delta_{\mathrm{pun}}}
	To conclude, let us select conditions on $\nlearn$ for which $L_0 \deltafail  + e^{- SAH \nlearn}$ is bounded by the target failure probability $\delta$. For this it suffices to choose $\deltafail$ such $\deltafail = \delta/2L_0$, and ensure $\nlearn \ge \log(2/\delta)$. This only affects our bound through the requirement $\nlearn \ge n_0(\delta_0) \vee \log(2/\delta)$, where
	\begin{align*}
	\delta_0 &= \frac{\delta \cdot \qpunish \cdot \epspunish}{4SAH} = \frac{\delta\cdot \qpunish \cdot \epspunish}{8SAHL_0} = \frac{\rhoprog\delta \cdot \qpunish \cdot \epspunish}{(4SAH)^2\nlearn}\\
	&= \frac{\frac{\Delta_0^2}{4H^2}\delta \cdot \qpunish \cdot \epspunish}{(4SAH)^2\nlearn}=  \frac{\delta \cdot \qpunish \cdot \epspunish}{(8SAH^2)^2\nlearn}\\
	&\le \frac{\delta}{\nlearn} \Delta_0^2 \cdot \delta_1^2, \text{ where } \delta_1 := \frac{\qpunish \cdot \epspunish}{24SAH}.
	\end{align*}
	 Hence, this requires
	\begin{align*}
	\nlearn \ge \frac{96 H^4 \log(\nlearn \cdot \frac{5^S}{\delta \cdot \delta_1^2 \Delta_0^2})}{\Delta_0^2}, \text{ or } \frac{\Delta_0^2}{96 H^4} \ge  \frac{\log(\nlearn \cdot \frac{5^S}{\delta \cdot \delta_1^2 \Delta_0^2})}{\nlearn}
	%((S\log(5) + \log(1/\delta) +   \log(1/\delta_1^2 \Delta_0^2)}{\Delta_0^2} + \frac{96 H^4 \log(\nlearn)}{\Delta_0^2}
	\end{align*}
	We appeal to the following claim to invert the logarithm
	\begin{claim} For all positive $a,b,t$ with $a/b \ge e$, $t \ge 2 \log(a/b)/b$ implies that $b \ge \log(a t)/t$.
	\end{claim}
	In particular, take $t = \nlearn$, $a = \frac{5^S}{\delta \cdot \delta_1^2 \Delta_0^2}$, and $b = \frac{\Delta_0^2}{96 H^4}$. Then, $a/b \ge e$, so it suffices that
	\begin{align*}
	\nlearn \ge \frac{192 H^4 \log(\frac{5^S 96 H^4}{\delta \cdot \delta_1^2 \Delta_0^4 })}{\Delta_0^2} \vee \log(2/\delta).
	\end{align*}
	Finally, we simplify
	\begin{align*}
	\log(\frac{5^S 96 H^4}{\delta \cdot \delta_1^2 \Delta_0^4 })  &= \log \frac{5^S 96 H^4 \cdot 16 \cdot 64 S^2A^2H^4  }{\delta \cdot (\qpunish \cdot \epspunish)^2 \rho^4 \ralt(\epspunish)^4 }\\
	&= S\log 5 + \log(1/\delta) +  \log \frac{96 H^4 \cdot 16 \cdot 64 S^2A^2H^4  }{\delta \cdot (\qpunish \cdot \epspunish)^2 \rho^4 \ralt(\epspunish)^4 }\\
	&\le S\log 5 + \log(1/\delta) +  4\log \frac{20 SAH^2 }{\rho \cdot \qpunish \cdot \epspunish  \ralt(\epspunish) }\\
	&:= S\log 5 + \log(1/\delta) +  \iota(\epspunish,\rho).
	\end{align*}
	Thus, it is enough to ensure $\nlearn \ge \frac{192 H^4 (S\log 5 + \log(1/\delta) +  \iota(\epspunish,\rho))}{\Delta_0^2} \vee \log(2/\delta)$. One can verify that the first term dominates the $\log(2/\delta)$.
	\qed
