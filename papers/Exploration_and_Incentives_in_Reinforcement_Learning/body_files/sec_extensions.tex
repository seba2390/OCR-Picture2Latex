%!TEX root = ../main.tex

\bluepar{Preliminaries.}
In a randomized MDP, rewards and transitions are randomized as follows. An \emph{MDP model} $\model$ specifies, for each $\xah$ triple,
the reward distribution
    $\sfR_{\model}\xah$
over $[0,1]$ with mean reward
    $\sfr_{\model}\xah$,
and transition probabilities
    $\sfp_{\model}\rbr{\cdot \mid x,a,h} \in \Delta(S)$.
In each stage $h\in[H]$, reward $r_h$ is drawn independently from
    $\sfR_{\model}(x_h,\, a_h,\, h)$,
and the new state $x_{h+1}\in[S]$ is drawn independently from distribution
    $\sfp_{\model}\rbr{\cdot \mid x_h,a_h,h}$.
The initial state $x_1$ is drawn independently from distribution
    $\sfp_{\model}\rbr{\cdot \mid 0}$.
For ease of exposition, we posit that all reward distributions are supported on the same countable set; the paradigmatic example is Bernoulli rewards. The shape of reward distributions does not matter for this paper, only the mean rewards do.

As the MDP is non-deterministic, we introduce $\sfP_{\model}^{\pi}$ and $\sfE^{\pi}_{\model}$ to denote expectation and operators corresponding to trajectories $\traj$ generated by policy $\pi$ and model $\model$. The policy value is defined as the expected total reward of this policy under $\sfE^{\pi}_{\model}$,
\begin{align*}
\valuef{\pi}{\model} := \textstyle \sfE_{\modst}^{\pi}\sbr{\sum_{h\in[H]} r_h}.
\end{align*}
The \traversal objective needs to be modified as well, because some $\xah$ triples might only be reachable with some (small) probability. Given $\rho\in [0,1]$, we say a triple $\xah$ is called \emph{$\rho$-reachable} under the true MDP $\modst$,  if some policy $\pi \in \Pitotal$ reaches it with probability at least $\rho$, \ie
    $\sfP_{\modst}^{\pi}\sbr{ (\bmx_h,\bma_h) = (x,a)} \ge \rho$.
Note that whether a particular triple $\xah$ is $\rho$-reachable is determined only by the state-step pair $(x,h)$.
Our objective is parameterized by this $\rho$, as well as multiplicity $n\in\N$ and confidence $\delta\in(0,1)$.

\begin{definition}
We say that an algorithm $\alg$  has $(\rho,n)$-traversed $\modst$ by episode $K$ if each $\rho$-reachable $\xah$ triple (under $\modst$) is visited in at least $n$ episodes $k \le K$. Here, an $\xah$ triple is called \emph{visited} in a given episode $k$ if $ (x_{k;h},a_{k;h},h) = \xah$. We say algorithm $\alg$ satisfies $(\rho,n,\delta,K)$-\traversal \ if
\begin{align}
\Pr[\alg \text{ has $(\rho,n)$-traversed } \modst \text{ by episode } K] \ge 1 -\delta.
\end{align}
\end{definition}

%\noindent For a target $(\rho,n)$ and confidence $\delta$,
\noindent The objective is to achieve $(\rho,n,\delta,K)$-\traversal in smallest number of episodes $K$. Multiple visits ($n>1$) are usually desired in order to estimate mean rewards or transition probabilities.

\bluepar{Our algorithm.} \Cref{alg:MDP_HH} carries over with minor modifications. There is an additional parameter, target number of samples $\nlearn$. A triple $\xah$ is called \emph{fully-explored} at phase $\ell$  if it is visited at least $\nlearn$ times in the past-phase hallucination episodes $\calK_{\ell}$, and \emph{under-explored} otherwise. Recall that reward information for under-explored triples is always \emph{censored}: not included in the ledgers revealed to the agents. The punish-event is defined via \eqref{eq:punish-event-defn}, same as before, but $\sfr_{\model}\xah$ in this definition now refers to mean rewards rather than deterministic rewards. To define hallucinated rewards, each time any fully-explored $\xah$ triple appears in the ledger, we draw its reward independently from the reward distribution specified by the $\modhall$, the hallucinated MDP model.

\bluepar{Results: reward-independent priors.}
Our guarantees are most lucid for reward-independent priors, as per \Cref{defn:reward_independence}. As in the previous section, we guarantee \traversal in the number of episodes that is exponential in $SAH$. While the guarantees become more complex to account for randomization in the MDP, they use the same parameters $f_{\min}$ and $r_{\min}$, see \refeq{eq:fmin_argmin}.

\begin{theorem}\label{thm:main_indep}
Consider a reward-independent prior $\prior$. Fix parameters $\rho, \delta\in (0,1]$. Assume that $r_{\min}>0$ and
    $\calC_{\rho}  := f_{\min}\rbr{ \epspunish}>0$,
where
    $\epspunish = r_{\min}\,\rho\,/\, 18\,H$.
Consider \Cref{alg:MDP_HH} with punishment parameter $\epspunish$, appropriately chosen phase length $\nphase$, and large enough target $n=\nlearn$. This algorithm is guaranteed to $(\rho,n)$-explore with probability at least $1-\delta$ by episode $K_{\rho,n}$, where $n$ and $K_{\rho,n}$ are specified below.

%Then \Cref{alg:MDP_HH} with punishment parameter $\epspunish$ and appropriately chosen $\nphase$ and $n=\nlearn$ is guaranteed to $(\rho,n)$-explore with probability at least $1-\delta$ by episode $K_{\rho,n}$ if $n$ and $K_{\rho,n}$ are sufficiently large, as given below.

For some absolute constants $c_1,c_2$, it suffices to take
\begin{align*}
n = \nlearn &\ge
     c_1\cdot \rho^{-2}\, r_{\min}^{-2}\,
     H^4 \rbr{ SAH \log \calC_{\rho}^{-1} +
        \log \frac{ SAH}{\delta\rho\, r_{\min}} }, \\
K_{\rho,n} &= c_2\cdot n\cdot \calC_{\rho}^{-SAH} \cdot \rho^{-3}\, r_{\min}^{-3}\, SAH^4.
\end{align*}
In particular, for any $n\geq 1$, one can obtain
    $K_{\rho,n} \leq n\cdot  \calC_{\rho}^{-SAH}
        \cdot \poly\rbr{\rho^{-1}\, r_{\min}^{-1}\, SAH  }
        \cdot \log\rbr{\delta^{-1}\,\calC_{\rho}^{-1}}$.
\end{theorem}


\bluepar{Results: arbitrary priors.}
Our analysis in fact extends to arbitrary priors, and yields \Cref{thm:main_indep} as a corollary. The prior-dependent parameters now need to be defined using the machinery developed in \Cref{sec:canon}: conditionally on a given partially-censored ledger and via canonical posterior/expectation (to avoid the dependence on how this larger was constructed).


%\asdelete{We introduce an abstract notion of a \emph{partially-censored ledger} $\ledger$. Formally, it is any dataset of a particular shape: a sequence of (policy, partial-trajectory pairs) $(\pi_k,\tau_k)$,  along with a \emph{censoring set} $\calU_{\ledger} \subset [S]\times[A]\times[H]$. The reward information is censored out from all trajectories for all triples $\xah\in \calU_{\ledger}$; that is, each $\tau_k$ is of the form $(x_h,\,a_h,\, \tilde{r}_h,\,h)_{h\in[H]}$, where $\tilde{r}_h \in [0,1]$ records a reward for uncensored triples $(x_h,a_h,h) \notin \calU_{\ledger}$, but $\tilde{r}_h = \bot$ replaces the reward value with a special censoring symbol for triples in $\calU_{\ledger}$. The ledger is called \emph{totally-censored} if $\calU_\ledger = [S]\times[A]\times[H]$, so that no reward information is recorded.}

%\asdelete{We define the prior-dependent parameters in terms of the posterior distribution given ledger $\ledger$, in the worst case over all ledgers. However, the posterior may depend not only on $\ledger$, but also on the process by which it is constructed. To deal with this subtlety, we Bayesian-update as if $\ledger$ was constructed by an algorithm which deterministically chooses the policies in $\ledger$. Thus, we define the \emph{canonical posterior} on $\modst$, denoted  $\Prcan\sbr{ \cdot \mid \ledger }$, and  \emph{canonical expectation} $\Expcan\sbr{f(\modst)\mid \ledger}$ over this posterior, as defined in \Cref{sec:canon}.)}

\begin{align}
\qpunish(\eps) &:=
    \min_{\text{totally-censored ledgers $\ledger$}}\quad
        \Prcan\sbr{\sfr_{\modst}\xah \le \eps \;\;
            \text{for all $\xah$ triples} \mid \ledger},
            \label{eq:qpun-defn}\\
\ralt &:=
    \min_{\text{partially-censored ledgers $\ledger$}}\quad
    \min_{\xah \in \calU_{\ledger}}
    \Expcan\sbr{ \sfr_{\modst}\xah \mid \ledger }.
        \label{eq:ralt-defn}
\end{align}
In particular, the canonical posterior probability of the punish-event given the censored ledger in the algorithm,
    $\Prcan\sbr{ \EvPun\mid \ledcensl}$,
is uniformly lower-bounded by $\qpunish(\epspunish)$.
Likewise, $\ralt$ lower-bounds the posterior mean reward
    $\Expcan\sbr{ \sfr_{\modst}\xah \mid \ledger }$
given any ledger $\ledger$ that censors out this particular $\xah$ pair.

\begin{remark}\label{rem:reduction-to-independence}
These parameters can be easily related to those in \refeq{eq:fmin_argmin} under reward-independence, essentially because the conditioning on $\ledger$ vanishes. First, $\ralt=r_{\min}$. Second, the probability in \eqref{eq:qpun-defn} factorizes as
    $\prod_{\xah}\; \Prop_{\modst \sim \prior} \sbr{\sfr_{\modst}\xah \le \eps}$.
%where each factor is lower-bounded by $f_{\min}(\eps)$.
It follows that
%    $ \Prcan\sbr{ \EvPun\mid \ledcensl} \geq
$\qpunish(\epspunish) \geq f_{\min}^{-SAH}(\epspunish)$.
\end{remark}

\begin{comment}where (i) the minimum is taken over all censored lends $\ledcens$ for each there exists a triple $\xah \in \reach_{\rho}(\modst)$ which appears strictly less than $\nlearn$ times, (ii) where $\calU(\ledcens)$ is the set of all triples $\xah \in \Utot$ which appear less than $\nlearn$ times in $\ledcens$, (iii) $\modpunish(\ledcens)$ is the set of models $\mu$ for which $r_{\model}\xah \le \epspunish$ for all $\xah \in\Utot \setminus \calU(\ledcens)$, and where $\Dhalnol$ is the hallucination distribution induced by drawing $\model_{\mathrm{hal}} \sim \Prcan[\cdot \mid \ledcens, \modclass(\ledcens)]$, and then imputing the rewards in $\ledcens$ as draws from its reward distirbution (as in).
\end{comment}

The main (and most general) result of this paper, stated below, has the same shape as \Cref{thm:main_indep} for reward-independence, but
    $r_{\min}$ and $f_{\min}^{-SAH}(\epspunish)$
are replaced with, resp., $\ralt$ and $\qpunish(\epspunish)$. Accordingly, \Cref{thm:main_indep} follows immediately by Remark~\ref{rem:reduction-to-independence}. The significance of this result is that it reduces the task of designing and analyzing  mechanisms for  incentivized exploration to the task of analyzing parameters $\ralt$ and $\qpunish(\epspunish)$ for a particular prior.

\begin{theorem}\label{thm:main_prob_mdp}
Consider an arbitrary prior $\prior$. Fix parameters $\rho, \delta\in (0,1]$. Assume that $\ralt>0$ and
    $\qpunish = \qpunish\rbr{\epspunish}>0$,
where
    $\epspunish = \ralt\,\rho\,/\, 18\,H$.
Consider \Cref{alg:MDP_HH} with punishment parameter $\epspunish$, appropriately chosen phase length $\nphase$, and large enough target $n=\nlearn$. This algorithm is guaranteed to $(\rho,n)$-explore with probability at least $1-\delta$ by episode $K_{\rho,n}$, where $n$ and $K_{\rho,n}$ are specified below.

%Then \Cref{alg:MDP_HH} with punishment parameter $\epspunish$ and appropriate $\nphase$, $\nlearn$ is guaranteed to $(\rho,n)$-explore with probability at least $1-\delta$ by episode $K_{\rho,n}$, if $n$ and $K_{\rho,n}$ are sufficiently large.

For some absolute constants $c_1,c_2$, it suffices to take
\begin{align}
n=\nlearn &\ge
     c_1\cdot \rho^{-2}\, \ralt^{-2}\,H^4
        \rbr{ S + \log \frac{SAH}{\delta\rho\cdot\ralt\cdot \qpunish} },
        \label{eq:thm:main_prob_mdp-n} \\
K_{\rho,n} &= c_2\cdot n\cdot \qpunish \cdot \rho^{-3}\, \ralt^{-3}\, SAH^4.
    \label{eq:thm:main_prob_mdp-K}
\end{align}
In particular, for any $n\geq 1$ one can obtain
 $K_{\rho,n} \leq n\cdot \qpunish
        \cdot \poly\rbr{\rho^{-1}\, \ralt^{-1}\, SAH  }
        \cdot \log\rbr{\delta^{-1}\,\qpunish^{-1}}$.
 \end{theorem}


\begin{remark}\label{rem:replace-qpun}
{Parameter $\qpunish(\epspunish)$ in \Cref{thm:main_prob_mdp} can be replaced with any number $L>0$ which lower-bounds
    $\Prcan\sbr{ \EvPun\mid \ledcensl}$
for all phases $\ell$.}
\end{remark}

\subsection{Proof overview for \Cref{thm:main_prob_mdp}}
\label{sec:probab_proof}

%{sec:det_transitions}

%Mirroring the proof of the deterministic case, we restrict our attention to establishing that, with probability $1 - \delta$, each $\xah \in \reach_{\rho}(\modst)$ is contained in at least $\nlearn$ trajectories only from hallucination episodes. Unlike the deterministic case, we can only ensure probabilistic progress -  with constant probability over the mechanism, the hallucinated ledger induces the agent to explore an under-explored triple $\xah \in \calU_{\ell}$. This formalized by \Cref{lem:main_prob_lemma}, from which \Cref{thm:main_prob_mdp} follows by an application of standard concentration inequalities in \Cref{proof:main_prob_mdp}.

%To establish \Cref{lem:main_prob_lemma}, we invoke a corollary (\Cref{cor:mdp_hh}) of the one-step guarantee (\Cref{prop:mdp_hh}) which replaces worst-case gap with an expected gap, described in \Cref{eq:Delta_gap}. We then bound this quantity in \Cref{lemma:gap_bound_prob}, whose proof is sketched at the end of the section. The formal proof of \Cref{lemma:gap_bound_prob} is given in \Cref{sec:lemma:gap_bound_prob} and consumes the brunt of our technical effort.

The ledger hygiene (\Cref{sec:canon}) and the single-step analysis (\Cref{sec:one_step}) carry over as is. We strive to mimic the rest of the analysis from the previous section (\ie \Cref{sec:det_transitions}): we carry over the major steps and deal with various complications that arise due to randomness. Some of the details are deferred to the appendices.

Let us start with some notation. Fix phase $\ell$. Let $\calF_{\ell}$ denote the $\sigma$-algebra generated by all randomness (in rewards, transitions, and the algorithm) in all phases up to and including this phase.  As before, let $\underexplored$ denote the set of all $\xah$ triples that are under-explored in this phase. Let $\Pi_{\ell,q}$ be the set of policies $\pi\in \Pimarkov$ which visit some triple $\xah\in \underexplored$ with probability at least $q$ (under the true model $\modst$).

We are interested in the event that the algorithm makes progress in a given phase $\ell$. Specifically, let
$\progress$ be the event that the algorithm visits some triple $\xah\in\underexplored$ in the hallucination episode in this phase. The progress is only probabilistic, expressed via a rather subtle statement:
\begin{align}\label{eq:genCase-progress}
\Pr\sbr{\Eexplorel \mid \calF_{\ell-1}} \geq \rhoprog
\qquad\text{with probability at least $1-\deltafail$ over $\calF_{\ell-1}$}.
\end{align}
Given the parameters from the theorem statement, the parameters in \eqref{eq:genCase-progress} are chosen as follows:
\begin{align}\label{eq:genCase-params}
\text{$\deltafail = \nphase/K_{\rho,n}$
and
$\rhoprog=\Delta_0^2/6H^2$, where $\Delta_0 = \rho\cdot\ralt/2$}.
\end{align}


\begin{lemma}[Progress]\label{lem:main_prob_lemma}
\refeq{eq:genCase-progress} holds in each phase $\ell > \nlearn$ such that some triple in $\underexplored$ is $\rho$-reachable.
\end{lemma}

The proof of \Cref{thm:main_prob_mdp} follows directly from \Cref{lem:main_prob_lemma} via a martingale-Chernoff argument which is common in the study of tabular MDPs (this is spelled out \Cref{proof:main_prob_mdp}).

We ensure that a policy from a given policy set $\Pi$ is chosen in the hallucination episode, under suitable conditions. We use a corollary of \Cref{prop:mdp_hh} which holds under a weaker condition compared to  \eqref{eq:HH_good_condition}. This new condition is somewhat subtle to define. We treat
    $\gapcan\sbr{\Pi \mid \ledger}$,
the canonical gap given ledger $\lambda$, as a random variable with randomness induced by the ledger. In a given phase $\ell$, take a conditional expectation of this random variable given the censored ledger $\ledcensl$:
\begin{align}\label{eq:mean-canonical-gap-defn}
\gapcanl[\Pi\mid \ledger]
    &= \Exp\sbr{ \gapcan\sbr{\Pi \mid \ledger} \;\mid\; \ledcensl }
    &\EqComment{mean-canonical gap}.
\end{align}
Essentially, we average out all randomness in ledger $\ledger$ that comes from the current phase; the only remaining randomness comes from $\ledcensl$. The new condition is also given ledger $\ledger = \ledhall$, but it only requires the mean-canonical gap to be bounded, not the realization of the canonical gap. The new condition is stated as follows, for some deterministic parameter $\Delta$:
\begin{align}\label{eq:HH-stronger-condition}
\gapcanl\sbr{\Pi \mid \ledhall}
    \geq \Delta
    \geq \frac{6H}{\nphase \cdot \Prcan\sbr{\EvPun \mid \ledcensl}}.
\end{align}
We prove that this condition suffices to make progress, in a probabilistic sense.

\begin{lemma}[Hidden Hallucination via mean-canonical gap]\label{cor:mdp_hh}
Fix phase $\ell$ and policy set $\Pi \subset \Pimarkov$ such that \eqref{eq:HH-stronger-condition} holds for some deterministic parameter $\Delta$. Then, with probability at least $\Delta/2H$ over the draw of $\ledhall$ (conditioned on $\calF_{\ell-1}$), an agent in the hallucination episode chooses a policy in $\Pi$.
\end{lemma}


We obtain \eqref{eq:HH-stronger-condition} with a policy set $\Pi = \Pi_{\ell,q}$ (for some $q$) and parameter $\Delta = \Delta_0$. As before, we lower-bound
    $\Prcan\sbr{\EvPun \mid \ledcensl}$
with  $\qpunish(\epspunish)$, by definition of the latter (\Cref{eq:qpun-defn}). The key is to lower-bound the mean-canonical gap, which we accomplish next. The statement is also probabilistic: the mean-canonical gap being a random variable with randomness coming from the censored ledger $\ledcensl$, we obtain a high-probability statement over the randomness in $\ledcensl$.

\begin{lemma}[Probabilistic Gap Bound]\label{lemma:gap_bound_prob}
Fix phase $\ell > \nlearn$. Recall parameters $\deltafail$ and $\Delta_0$ from \eqref{eq:genCase-params}. With probability at least $1 - \deltafail$ over the randomness in ledger $\ledcensl$, it holds that
\[ \gapcanl\sbr{\Pi_{\ell,q} \mid \ledhall}
    \geq \Delta_0,
    \quad\text{where $q = \Delta_0/3H$}.\]
\end{lemma}

These two lemmas about the canonical gap imply \Cref{lem:main_prob_lemma}, if one plugs in all the parameters. (We omit the tedious but straightforward details.)

\subsubsection*{Proof Sketch of \Cref{lemma:gap_bound_prob}}\label{ssec:sketch:lemma:gap_bound_prob}

To analyze the mean-canonical gap, we define a similar version of the canonical posterior \eqref{eq:canon-posterior-defn}. The \emph{mean-canonical posterior} $\Prcanl[\,\cdot\mid\ledger]$ given a random ledger $\lambda$ is a distribution over MDP models given by
\begin{align}\label{eq:hal-posterior-defn}
\Prcanl[\modclass \mid\ledger]
    &:= \Exp\sbr{ \Prcan\sbr{ \modclass \mid \ledger }
        \;\mid\; \ledcensl }
    \quad\forall \text{ measurable }\modclass\subset\modtotal.
\end{align}


Paralleling the deterministic analysis (\ie the proof of \Cref{claim:gap_size}), we consider
    $\Prcanl[\,\cdot \mid\ledhall]$,
the mean-canonical posterior given the hallucinated ledger $\ledhall$. We verify that, with a ``good enough" probability, a model $\model$ drawn at random from this posterior satisfies the following:
\begin{itemize}
\item[(a)] for all fully-explored $\xah$ triples, $\model$ has small mean rewards, $\sfr_{\model}\xah$.
\item[(b)] for all fully-explored $\xah$ triples, the transition probabilities, $\sfp_{\model}(\cdot \mid x,a,h)$, are close to those for the true model, $\sfp_{\modst}(\cdot \mid x,a,h)$ (and similarly for the initial state distributions, provided $\ell > \nlearn$).
\end{itemize}
We show that these properties imply a small mean-canonical gap, relying on a probabilistic version of the  simulation lemma (\Cref{lem:modclass_not}). This version is stated and proved in \Cref{lem:visitation_comparison_general}.

To establish properties (a,b), we must address several sources of randomness: (i) randomness in realized rewards and transitions, (ii) randomness in the draw of the hallucinated rewards and (iii) randomness in the agent's posterior given the hallucinated ledger. We address them simultaneously by constructing a set of ``good models'' under which points (a) and (b) hold, and using Bayesian concentration inequalities to show that with high probability under \emph{all} randomness, any model under the agents posterior given a hallucinated ledger lies in this ``good set''.

We account for the fact that the mean-canonical posterior
    $\Prcanl[\,\cdot \mid\ledhall]$
is not the posterior formed by an agent given ledger $\ledhall$ (because agents know that rewards may be hallucinated).
However, in view of \Cref{eq:Prhall_ledhall},
% the posterior over the hallucinated model
it is a conditional posterior given the punish-event $\EvPun$. We argue that the strength of concentration and the rarity of hallucinations (which occur only once per phase) overwhelm the effect of this conditioning. See \Cref{sec:lemma:gap_bound_prob} for the full argument.

