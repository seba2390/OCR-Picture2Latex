%!TEX root = ../main.tex
\newcommand{\mdphh}{\texttt{MDP}\text{-}\texttt{HH}}

\newcommand{\gapcan}{\gap_{\mathrm{can}}}
\newcommand{\Expcan}{\Exp_{\mathrm{can}}}
\newcommand{\ledhat}{\hat{\ledger}}
\newcommand{\Piseq}{\vec{\Pi}}

\newcommand{\ledvar}{\bm{\ledger}}
\newcommand{\Ledraw}{\Led_{\mathrm{raw}}}
\newcommand{\nullr}{\circ} %expectation


\newcommand{\ledrawl}[1][\ell]{\ledger_{\mathrm{raw};#1}}
\newcommand{\ledcensl}[1][\ell]{\ledger_{\mathrm{cens};#1}}
\newcommand{\ledul}[1][\ell]{\ledger_{\mathcal{U}_{\ell};\ell}}
\newcommand{\modhall}{\model_{\mathrm{hal};\ell}}
\newcommand{\ledrevk}[1][k]{\ledger_{#1}}


\newcommand{\ledhonl}{\ledger_{\mathrm{hon};\ell}}

\section{The Hidden Hallucination Algorithm}

Let us describe our algorithm, called Hidden Hallucination for MDPs (\mdphh, see \Cref{alg:MDP_HH}). The goal of this algorithm is to visit as many $\xah$ triples as possible.  Each triple should be visited $\nlearn$ times, where $\nlearn$ is a parameter.

The algorithm proceeds in phases of $\nphase$ episodes each. In each phase $\ell$, it selects the \emph{hallucination episode} $\kexpl$ uniformly at random from all episodes. \asedit{Informally, in this episode the algorithm reveals ``hallucinated rewards" -- fake rewards drawn from a particular distribution -- which encourage an agent to explore. In all other episodes the algorithm reveals true rewards, so that an agent can exploit. Thus, the hallucination episode is \emph{hidden} among many exploitation episodes.}

\asedit{In each episode, the algorithm reveals trajectories from all past-phase hallucination episodes.%
\footnote{An episode is called \emph{past-phase} if it has occurred in one of the preceding phases.} However, the reward information is modified for the sake of incentives.} A triple $\xah$ is called \emph{fully-explored} if it is visited at least $\nlearn$ times in the past-phase hallucination episodes, and \emph{under-explored} otherwise. \asedit{Reward information for under-explored triples is always \emph{censored}: not revealed to the agents. Reward information for fully-explored triples is either revealed faithfully (in exploitation episodes), or hallucinated.}

% $\calU_{\ell} \subset [S]\times[A]\times[H]$
%consists of all triples $\xah$ visited less than $\nlearn$ times on past hallucination episodes.
%Recall that an \emph{trajectory},  $\traj := (x_h,a_h,h,r_h)_{h=1}^H$ is a sequence of (states, actions, stage, reward) tuples, and $\traj_k$ denotes the trajectory collected on episode $k$ of the protol.

To make this precise, we define a \emph{ledger}: the collection of trajectories from all \asedit{past-phase} hallucination episodes, in which some of the reward information may be altered. We consider four types of ledgers, which contain full information on transitions, but differ in the rewards:
\asmargincomment{shortened the descriptions of ledgers}
\begin{itemize}
\item the \emph{raw ledger} $\ledrawl$ retains all information on rewards;
%the trajectories from all \asedit{past-phase} hallucination episodes.
    %$ \kexpl[1],\dots,\kexpl[\ell-1]$.
\item the \emph{censored ledger} $\ledcensl$ removes all reward information;
%contains the same trajectories as $\ledrawl$, 
%but these trajectories are \emph{censored} so as to remove all reward information.
\item the \emph{honest ledger} $\ledhonl$
 %partially censors the trajectories in $\ledrawl$: it 
 only retains reward information for fully-explored $\xah$ triples;
    %$\xah \notin \calU_{\ell}$. %where $\calU_{\ell} \subset [S]\times[A]\times[H]$ consists of all triples $\xah$ visited less than $\nlearn$ times on past hallucination episodes.
\item the \emph{hallucinated ledger} $\ledhall$
    %Like the honest ledger, it
    %takes the raw ledger,
     removes reward information from all under-explored $\xah$ triples,
    %$\xah \notin \calU_{\ell}$. But unlike the honest ledger,
    and \emph{hallucinates} the rewards for all fully-explored $\xah$ triples.
\end{itemize}


%We stress that $\ledrawl,\ledcensl,\ledhonl,\ledhall$ contain exactly the same \emph{transition} information, but differ in which rewards are shown, and, in the case of $\ledhall$, how the rewards are fabricated.

\noindent On hallucination episodes $\kexpl$, we reveal the hallucinated ledger $\ledhall$. All other episodes implement \emph{exploitation} by revealing the honest ledger $\ledhonl$.

\newcommand{\EvPun}{\calE_{\mathtt{pun},\,\ell}}

The remaining crucial ingredient is how to generate the hallucinated rewards. \asedit{We define the \emph{punish-event} $\EvPun$ that
    $r_{\modst}\xah \le \epspunish$
for all all fully-explored triples $\xah$, where $\epspunish>0$ is a small parameter. Informally, we ``punish" the fully-explored triples by pretending that they yield small expected rewards, so as to encourage an agent to explore elsewhere. To make this formal,} let us consider the posterior distribution of the true model $\modst$ given $\EvPun$ and the censored ledger $\ledcensl$ (\ie the transition data). We draw from this posterior to hallucinate an MDP model $\modhall\in \modtotal$. Finally, we hallucinate rewards using $\modhall$: each time any fully-explored $\xah$ triple appears in the ledger, draw its reward independently from the reward distribution specified by $\modhall$.



%On exloitation episodes $k \ne \kexpl$, \mdphh reveals the honest ledger $\ledhonl$ as the signal to the agent,

\begin{algorithm}[h]
  	\begin{algorithmic}[1]
  	\State{}\textbf{Input: } phase length $\nphase$, target sample size $\nlearn$, tolerance $\epspunish$
    \For{each phase $\ell= 1,2,\;\dots$}
    \State{}$\Phase_{\ell} = (\ell-1) \nphase  + [\nphase] \subset \N$
        ~~~\algcomment{\% the next $\nphase$ episodes}
    \State{}Draw ``hallucination episode" $\kexpl$ uniformly from $\Phase_{\ell}$
    %\State{}Draw episode $\kexpl$  uniformly from $\PhaseEpisodes_{\ell}$
    %\State{}Draw $\kexpl \unifsim N(\ell - 1) + [N]$
    \For{each episode $k \in \Phase_{\ell}$}
    \If{$k = \kexpl$}
    \algcomment{\qquad\% hallucination episode}
    \State{}\algcomment{\% censored ledger $\ledcensl$,
        punish-event $\EvPun$ with parameter $\epspunish$}
     \State{}Draw $\modelhall\sim \Pr[\modst \in \cdot \mid \ledcensl,\; \EvPun]$
        \algcomment{\quad\% hallucinated MDP}
     \State{}For each fully-explored $\xah$ triple,
        \algcomment{\quad\% hallucinate rewards}
     \State{}~~~each time this triple appears in the ledger,
     draw its reward as per $\modhall$.
     %\State{}Draw rewards from $\modelhall$ for each fully-explored $\xah$ triple
     \State{}Form $\ledhall$ by inserting the hallucinated rewards into $\ledcensl$
        \algcomment{\quad\% hallucinated ledger}
     \State{}Reveal hallucinated ledger: $\ledrevk \gets \ledhall$
    \Else{} \algcomment{\qquad\% exploitation}
     \State{}Reveal honest ledger: $\ledrevk \gets \ledhonl$.
    \EndIf
    \State{}Observe the trajectory $\traj_k$ from this episode.
  \EndFor
  \EndFor
  \end{algorithmic}
  \caption{Hidden Hallucination for MDPs (\mdphh)}
  \label{alg:MDP_HH}
	\end{algorithm}


%With sufficiently many episodes in a phase, 

An agent is much more likely to face an exploitation episode than a hallucination one. So, even when shown the hallucination ledger, the agent would believe that most likely it is the honest ledger. As we show in the analysis, the agent would believe that the rewards from fully-explored $\xah$ triples are indeed small, and select policies which aim to explore under-visited $\xah$ triples.

%\mdphh, is defined by three parameters: a sample threshold $\nlearn \in \N$, a phase length $\nphase \in \N$, and a punishing threshold $\epspunish \in N$. The algorithm proceeds in phases $\ell$ of length $\nphase$, each of which has an associated hallucination episode $\kexpl$, hidden in $\nphase - 1$ exploitation episodes.

%



\newcommand{\ledcenslhat}[1][\ell]{\hat{\bm{\ledger}}_{\mathrm{cens};#1}}
\newcommand{\ledhonlhat}[1][\ell]{\hat{\bm{\ledger}}_{\mathrm{hon};#1}}
\newcommand{\ledhallhat}[1][\ell]{\hat{\bm{\ledger}}_{\mathrm{hal};#1}}


%To apply \Cref{prop:mdp_hh}, we consider a policy set $\Pi$ for which the exploration of novel $\xah$ triples is guaranteed. This is somewhat subtle in general, so we begin our exposition with the simplifying case of \emph{deterministic} MDPs, depicted in $\dots$. The general case of \emph{randomized} MDPs is described in $\dots$.
%Suppose that the $\ledcensl$ is \emph{canonical}, in the sense that $\Prcan[\cdot \mid \ledcensl] = \Prmech[\cdot \mid \ledcensl]$.



\section{Analysis}
Proof Plan:
\begin{itemize}
	\item First we do one-step guarantee
	\item Then we do w/e
	\item Then part 3
\end{itemize}


\newcommand{\cce}{\texttt{CanConE}}
\newcommand{\cces}{\texttt{CanConE}s}


\subsection{Censoring and Ledges}


\bluepar{Censoring}
At the core of our algorithm is the notion of \emph{censoring}, whereby the principal strategically restricts information from certain trajectories before revealing them to the learner. To define this operation, let $\calU \subset [S] \times [A] \times [H]$ denote a \emph{censoring set} consisting of  (states, actions, stage) triples; we let $\calU^c$ denote its complement. For a quadruple $(x,a,h,r) \in [S] \times [A] \times [H] \times \R$, we define its $\calU$-censoring $\cens_{\calU}(x,a,h,r) := (x,a,h,r') \in [S] \times [A] \times [H] \times (\R \cup \{\nullr\})$, where the reward is censored - $r' = \nullr$ if $(x,a,h) \in \calU$ - and reward is revealed - $r' = r$ - otherwise.  For $\calU$, and raw trajectory $\traj = (x_h,a_h,r_h,h)_{i=1}^H$, the corresponding \emph{$\calU$-censored} trajectory is:
 \begin{align}
 \cens(\traj;\calU) = \cens_{\calU}(x_h,a_h,h,r_h)_{h=1}^H,
 \end{align}
 which for convience contains its censoring set $\calU$.  We define the space of such trajectories $\trajspacetot_{\calU}$.
 Given a $\calU$-\msedit{censored} trajectory $\traj$, we say $(x,a,h) \in \traj$ if $(x_h,a_h) = (x,a)$, where $(x_h,a_h)$ is its (state, action) pair for stage $h$.   The \emph{total censoring} operation,  $\cens(\traj) := \cens(\traj;[S] \times [A] \times [H])$, omits all rewards, and we can the output of $\cens(\traj)$ an \emph{totally censored trajectory}.  Finally, we note that raw trajectories are the special case where $\calU = \emptyset$.


\bluepar{Ledgers} We define a central object called a \emph{ledger}, which is a partial histories of trajectory information. A ledger consists of a censoring set $\calU \subset [S] \times [A] \times [H]$ and sequence of (policy, $\calU$-censored trajectory) pairs $(\calU,(\pi_i,\traj_i)_{i=1}^n)$, where $\traj_i \in \trajspacetot_{\calU}$. For $\calU$ specified, we call $\ledger$ a $\calU$-ledger. A \emph{raw ledger} is the special case $\calU = \emptyset$, and totally censored ledger the case $\calU = [S] \times [A] \times [H]$. The sequence of policies in a given ledger $\ledger$ is denoted $\Piseq_{\ledger} = (\pi_1,\dots,\pi_n)$, where the arrow accent stresses that it is an ordered set, and its censoring set we denote $\calU_{\ledger}$. Importantly, there are only \emph{countably many} ledgers, because for any ledger length $n$, there are finitely many triples $\xah$ which can be visited, and because all realized rewards are assumped to be supported on a common, countable set.

\subsection{Cannonical Probabilities and Hygenic Ledgers}
A key step in the analysis is to render certain conditionals probabilities arising from the algorithm in terms of certain \emph{cannonical}, algorithm-independent probabilities. To this end, we define (a) cannonical distributions over ledgers and (b) canonical conditional expectation (\cce) over models. We then formulate a condition we call \emph{hygiene} under which we can replace conditional probabilities over ledgers in the algorithm with cannonical ones. We conclude with \Cref{lem:data_hygiene}, which verifies that the property holds for both the censored and honest ledgers $\ledcensl$ and $\ledhonl$.

\bluepar{Canonical Ledger Distribution}
Recall that $\traj \sim \sfP^{\pi}_{\model}$ denotes that $\traj$ is a raw trajectory drawn from $\model$ and $\pi$. Write $\traj \sim \cens(\sfP^{\pi}_{\model};\calU)$ to denote $\traj = \cens(\traj_{\mathrm{raw}};\calU)$, where $\traj_{\mathrm{raw}} \sim \sfP^{\pi}_{\model}$; this extends to totally censored ledgers as a special case. Given a a sequences of policy $\Piseq = (\pi_1,\dots,\pi_n)$ and $\model \in \modclass$ , we define the \emph{canonical ledger distribution} $\Led(\Piseq;\model;\calU)$  over $\calU$-ledgers with policy sequence $\Piseq$ induced by the model $\model$:
\begin{align}
\Led(\Piseq;\model;\calU) &\distequals  \left(\pi,\traj \iidsim \cens(\sfP^{\pi}_{\model};\calU) \right)_{\pi \in \Piseq}. \label{eq:ledU_def}
\end{align}
The above specializes to raw trajectories by taking $\calU = \emptyset$ and totally censored trajectories with $\calU = [S] \times [A] \times [H]$; in these cases, we use the shorthand $\Ledraw(\Piseq;\model)$ and  $\Ledcens(\Piseq;\model)$, respectively, for the distribution given in \Cref{eq:ledU_def}.



\bluepar{Canonical Conditional Expectations (\cce)}
For both the algorithm specification and its analysis, we introduce a \emph{canonical posterior}. Intuively, a cannonical posteriors corresponds to the posterior belief a given agent would have if they were revealed a given ledger $\ledger$, but rather than knowing the principal's mechanism, they took on good faith that the ledger was constructed in an entirely non-strategic manner.

More precisely, given ledger $\ledger$ with policy sequence $\Piseq_{\ledger}$ and censoring set $\calU_{\ledger}$, the cannonical posterior is the posterior distribution over models $\modvarhat$ obtained by sampling $\modvarhat \sim \prior$ from the prior, and then conditioning on the event a random ledger, $\hat{\ledvar}$, obtained by sampling the policies $(\pi_1,\dots,\pi_n) \in \Piseq$, coincides with the given ledger. Importantly, we treat the sequence of $\Piseq_{\ledger}$ as deterministic in the conditioning. In addition, we allow for simultaneously conditioning on the event that the posterior model $\modvarhat$ lies in a specified subset of MDPs $\modclass \subset \modtotal$. Formally,
\begin{align*}
\Prcan[ \modvarhat \in \cdot \mid \ledger ,\modclass] &:= \Prop_{\modvarhat \sim \prior, \, \hat{\ledvar} \sim \Led(\Piseq_{\ledger};\modvarhat;\calU_{\ledger}) } \left[ \modvarhat \in \cdot  \mid \{\hat{\ledvar} = \ledger  \} \cap \{\modvarhat \in \modclass\}\right]\\
\Expcan[f(\modvarhat)\mid \ledger , \modclass] &:= \Expop_{\modvarhat \sim \Prcan[ \cdot \mid \ledger, \modclass]}[f(\modvarhat)], \quad \forall f \text{measurable }: \modtotal \to \R.
\end{align*}
%as the law of a model $\modvarhat$ drawn from the posterior over models, conditioning on the events that (a)  a random ledger with polcies $\Piseq_{\ledger}$ returns the same trajectories as those in $\ledger$ and (b) that the posterior model lies in the class of models $\modclass$.
We call the operator $\Expcan$ the \emph{canonical conditional expectation}, or \cce. We also define $\Prcan$  and $\Expcan$ in the natural way for many ledgers $\Prcan[ \cdot \mid (\ledger_i)_{i=1}^m, \modclass]$, and by omitting $\modclass$: $\Prcan[ \cdot \mid \ledger]= \Prcan[\cdot \mid \ledger, \modtotal]$. In other words, in a cannonical probability, the posterior does not take into account any other information about the true instance $\modst$ that the mechanism that may have been used to either construct (or fabricate!) the ledgers $\ledger$.

\bluepar{Ledger Hygeine}
Let $\hat{\ledger}$ be a random ledger generated by an algorithm in the protocol; e.g.  the totally censored ledger at phase $\ell$, $\ledcensl$ in \Cref{alg:MDP_HH}. It would simplify things greatly if we could compute Bayesian posteriors as cannonical ones, i.e.
\begin{align}
 \Pr[\modst \in  \cdot \mid \hat{\ledger}]  = \Prcan[\modvarhat \in\cdot \mid \hat{\ledger} ]. \label{eq:hygeine}
\end{align}
Indeed, if \Cref{eq:hygeine} holds, then we can discard reasoning about the intricacies of the mechanism that produced the ledger $\hat{\ledger}$. We call this condition \emph{ledger hygiene}.

\begin{definition}\label{def:hygenic} A random ledger $\hat{\ledger}$ is \emph{hygenic} if  \Cref{eq:hygeine} is satisfied.
\end{definition}

One can construct numerous counterexamples of non-hygenic ledgers; these are detailed in \Cref{app:cannonical_vs_mechanism}. For example, information about $\modst$ can be encoded by the policies in $\hat{\ledger}$ in ways that communicate more information than the cannonical conditional, which treats those policies as fixed and therefore uninformative. Nevertheless, we show that the censored and honest ledger in \mdphh{} are indeed hygenic.

\begin{lemma}\label{lem:data_hygiene} The censored ledger $\ledcensl$ and honest ledger $\ledhonl$ are both hygenic.
\end{lemma}
\begin{proof}[Proof Sketch] The essential property we use is that the policies in $\ledcensl$, and the censoring set $\calU_{\ell}$ for $\ledhonl$, are determined exactly by $\ledcensl[\ell-1]$, that is, the visited triples $\xah$ from previous hallucination episodes. Thus, $\ledcensl$ and $\ledhonl$ do not depend on transition data which are not in their own ledgers, and do not communicate reward information (because reward information is only explicitly used on non-hallucination episodes). The formal proof is given in \Cref{proof:lem_data_hygiene}
\end{proof}

\subsection{ One-Step Hidden Hallucination Mechanism}
We now state a single-phase exploration guarantee which forms the cornerstone of our analysis. The goal of this analysis is to derive conditions under which all of the agent's Bayes-greedy policies $\pi$ lie in some desired subset of policies $\Pi \subset \Pitotal$. To state the guarantee, we introduce the notion of \emph{cannonical gap} given a ledger $\ledger$, which is the difference between the maximal \cce{} of a policy $\pi \in \Pi$ and the \cce{} of a $\pi$ in the complement:
\begin{definition}[Canonical Gap]\label{defn:cannonical_gap} Let $\Pi \subset \Pimarkov$ be a subset of policies. The \emph{canonical gap} of $\Pi$ given a ledger $\ledger$ is defined as the difference
\begin{align*}
\gapcan[\Pi \mid \ledger] := \max_{\pi \in \Pi} \Expcan[\valuef{\pi}{\modvarhat} \mid \ledger] - \max_{\pi \in \Pimarkov \setminus \Pi} \Expcan[\valuef{\pi}{\modvarhat} \mid \ledger]
\end{align*}
\end{definition}
For intuition, suppose that at episode $k$, the revealed ledger $\ledger_k$ is hygenic, i.e. $\Prcan[ \modvarhat \in \cdot \mid \ledger_k] = \Exp[\modst \in \cdot \mid \ledger_k]$. Then, if $\gapcan[\Pi \mid \ledger_k] > 0$, the agents choice of policy $\pi_k$ must lie in $\Pi$ as a direct consequence of $\pi_k$ being Bayes greedy.

The ledger $\ledger_k$ actually revealed by \mdphh{} is not hygenic, because every $1/\nphase$ rounds, it is set to the hallucinated ledger $\ledhall$. Our main single-episode guarantee argues that this issue can be circumvented by making $\nphase$ sufficiently large. Specifically, we show that if there is a lower bound on (a) the  canonical gap for a class $\Pi$ under $\ledhall$ and (b) the canonical probability that $\modvarhat \in \modclass_{\ell}(\epspunish)$ given the totally censored ledger $\ledcensl$, then the agent will select still policy $\pi \in \Pi$.
\begin{proposition}[Hidden Hallucination Guarantee]\label{prop:mdp_hh} Let $\Pi \subset \Pimarkov$ be any subset of policies, and  suppose further that it holds that he hallucinated ledger  $\ledhall$ and phase length $\nphase > 0$ satisfy:
\begin{align}
\frac{1}{\nphase} \le \frac{\Prcan[\modvarhat \in \modclass_{\ell}(\epspunish) \mid \ledcensl] \cdot \gapcan[\Pi \mid \ledhall] }{3 H }. \label{eq:HH_good_condition}
\end{align}
Then for $k = \kexpl$, any Bayes greedy policy $\pi_k$ must lie in $\Pi$.
\end{proposition}
\begin{proof}[Proof Sketch] Given an episode $k$ and phase $\ell$, proof of \Cref{prop:mdp_hh} readily reduces to bounding the conditional probability that $k = \kexpl$ is the hallucination episode, given the revealed ledger $\ledger_k$; in other words, bounding the agents belief that they are facing a hallucinated, and not honest ledger. Our argument is inspired by that of hidden exploration \cite{mansour2020bayesian}, but differs in a number of respects; notably, the role of data hygene. We begin by verifying
\begin{align}
\Pr[\ledhall = \ledger \mid \ledcensl] = \Pr[\ledhonl = \ledger \mid \ledcensl, \modst \in \modclass_{\ell}(\epspunish)];
\end{align}
thus, when shown hallucinated ledger, the agent may either believe they are on a hallucination episode $k = \kexpl$, or simply that it happened to be the case, by chance, that $\modst \in \modclass_{\ell}(\epspunish)$, but that $k \ne \kexpl$. When $\nphase$ is very small, the probability $k = \kexpl$ is dominated by the probability $\modst \in \modclass_{\ell}(\epspunish)$, and \Cref{eq:HH_good_condition} quantifies exactly how small $\nphase$ must be. Note that this condition on $\nphase$ is given in terms of cannonical conditionals, which we show suffices because the censored and honest ledgers $\ledhonl$ and $\ledcensl$ are hygenic.  The formal proof is given in \Cref{proof:prop_mdp_hh}.
\end{proof}
%In general, whether or not the condition \eqref{eq:HH_good_condition} holds is a random event, depending in part on (a) the samples comprising $\ledcensl$ and (b) the randomness used in hallucinating $\ledcensl$. For simplicity,

\newcommand{\moddet}{\calM_{\mathrm{det}}}
\subsection{Deterministic Transitions and Rewards}

Building on the one step guarantee above, we consider a simplified multi-step protocol where all MDPs in the support of the prior have derministic rewards and transitions. A deterministic MDP $\model \in \modtotal$ is an MDP with deterministic transitions and deterministic rewards. In this section, we assume that the prior $\prior$ is supposed on deterministic MDPs.

Given a deterministic MDP $\model$, we write $\traj^{\pi}_{\model}$ for the unique trajectory induced by a policy $\pi$, $r_{\model}\xah \in [0,1]$ for its deterministic reward, and $\sfp_{\model}(x,a,h) \in [S]$ for the state-transition. We also write $x_h^{\pi;\model}$,$a_h^{\pi;\model}$, and $r_h^{\pi;\model}$ for state, action, and reward at step $h$ under model $\model$ and policy $\pi$. In this simplified setting, reachability (\Cref{def:prob_reachable}) is a binary notion:
\begin{definition}[Deterministic Reachability]\label{def:determ_reachable} Let $\modst$ be a deterministic MDP. A triple $\xah \in [S] \times [A] \times [H]$ is reachable under $\modst$ if there exist a policy $\pi \in \Pimarkov$ such that $\xah \in \traj^{\pi}_{\model}$. We let $\reach(\modst) := \{\xah \text{ reachable under } \modst\}$.
\end{definition}
Our desideratum simplifies accordingly:
\begin{desid}[Deterministic Exploration]\label{desid:det} We say at a triple $\xah$ is \emph{visited} by episode $K$ if, there exists an episode $k \in [K]$ such that the triple is contained in that episode's trajectory, $\xah \in \traj_{k}$.  We say that a mechanism deterministically explores by episode $K$ if, with probability $1$ over the draw $\modst \sim \prior$, all reachable triples $\xah \in \reach(\modst)$ are visited by episode $K$.
\end{desid}

Further still, our algorithm admits two simplifications:
\begin{itemize}
	\item Because the MDP is deterministic, there is only ever reason to learn each $\xah$ triple once. Hence, we set $\nlearn = 1$. Thus, $\calU_{\ell}$ is the set of all states which have been not yet been visited during hallucination episodes, and thus the operator $\ledhonl := \cens(\ledrawl;\calU_{\ell})$ is equivalent to \emph{not censoring at all}. Hence, the honest ledger is just the raw ledger $\ledrawl$.
	\item Hidden hallucination admits simplification as well: since rewards are deterministic, the hallucinated model $\modhall$ directly determines the hallucinated rewards. Then, $\ledhall$ just sets all rewards for triples $\xah$ in the ledger to their means $r_{\modhall}\xah$.
\end{itemize}




We now establish a guarantee in this deterministic setting, under the reward-independence assumption codified in \Cref{asm:indep_prior}.
\begin{theorem}\label{thm:det_mdp} Suppose \Cref{asm:indep_prior},  and the prior $\prior$ is supported on deterministic MDPs $\modst$. Moreover,  let $f_{\min}(\cdot)$ and $r_{\min}$ be as in \Cref{eq:fmin_argmin}. Then, (\mdphh) \Cref{alg:MDP_HH} run with parameters
\begin{align*}
\epspunish = 1/2Hr_{\min},\quad \nlearn = 1, \quad \nphase = \ceil{6H/f_{\min}(\epspunish)^{SAH}r_{\min}}.
\end{align*} Then  \mdphh{} deterministically explores the MDP by episode
\begin{align*}
K = |\reach(\modst)|\cdot \nphase \le SAH\nphase.
\end{align*}
\end{theorem}
\mscomment{remarks}
Below, we provide a self-contained proof of \Cref{thm:det_mdp}. The proof follows from a sequential application of the single-phase guarantee,  \Cref{prop:mdp_hh}, at each phase $\ell$. We apply that proposition with $\Pi \gets \Pi_{\ell}$, where $\Pi_{\ell}$ is the set of policies which visit a triple $\xah$ not yet visited. To do so, need to verify that the phase lenght $\nphase$ is large enough to dominate the product of $\Prcan[\modvarhat \in \modclass_{\ell}(\epspunish) \mid \ledcensl]$ and $ \gap(\Pi_{\ell} \mid \ledhall)$. Controlling the former term appeals to reward-independence (\Cref{asm:indep_prior}); controlling the latter appeals to a simulation lemma (\Cref{claim:modclass_not}), which shows that transitions for models in the support of the posterior $\Pr[\modvarhat \in \cdot \mid \ledhall]$ concide with the transitions for the true model $\modst$ in an appropriate manner.

\begin{proof}[Proof of \Cref{thm:det_mdp}]
Recall that, when $\nlearn = 1$, the set $\calU_{\ell}$ denotes the set of $\xah$ triples not yet visited during an exploration phase.  Let $\Pi_{\ell}$ denote the set of policies $\pi$ such that $\traj^{\pi}_{\modst}$ visit a triple $\xah \notin \calU_{\ell}$. It suffices to show that, if at any phase $\ell$,   $\calU_{\ell} \cap \reach(\modst)$ is non-empty, then the learner selects a policy $\pi_k \in \Pi_{\ell}$ on the hallucination episode $k = \kexpl$. Indeed, if this holds, then after at most $L \le |\reach(\modst)|$ episodes, the set $\calU_{L+1}\cap\reach(\modst)$ will be empty, and thus all reachable triples $\xah \in \reach(\modst)$ will have been visited at some hallucination episode for a phase $\ell \in [L]$.


 We shall apply the single-phase guarantee, \Cref{prop:mdp_hh}, with $\Pi = \Pi_{\ell}$.   To invoke the guarantee, we need an appropriate control over the gap term.
 \begin{claim}\label{claim:gap_size}For any phase $\ell$
  \begin{align}
 \gap(\Pi_{\ell} \mid \ledhall) = \underbrace{\max_{\pi \in \Pi_{\ell}}\Expcan[\valuef{\pi}{\modvarhat} \mid \ledhall]}_{(a)} - \underbrace{\max_{\pi \in \Pi_{\ell}^c}\Expcan[\valuef{\pi}{\modvarhat} ]}_{(b)} \ge \frac{r_{\min}}{2}, \label{eq:gap_det_wts}
 \end{align}
 \end{claim}
The key step in the argument is the following \emph{simulation lemma}, which states that the transitions on visited triples between the true model and models in the support of the hallucinated posterior coincide, but that that the rewards of the latter are at most $\epspunish$.
\begin{lemma}[Simulation Lemma, Deterministic]\label{claim:modclass_not} Fix a phase $\ell$. For policies $\pi \in \Pitotal$, define $h_{\pi,\ell} $ the first stage at which that policy visits an unvisited triple $\xah \in \calU_{\ell}$ (with $h_{\pi,\ell} = H+1$ if $\pi$ visits no such triples):
\begin{align}
h_{\pi,\ell} := \inf\{h \le H+1: (x_h^{\pi;\model},a_h^{\pi;\model},h) \in \calU_{\ell}\}
\end{align}
Then, for any model $\model$  in the support of the posterior given the hallucinated ledger, $\Prcan[\modvarhat  \in \cdot \mid \ledhall]$, the transitions up to step $h_{\pi;\ell}$ under policy $\pi$ are identical for modesl $\model$ and $\modst$, and the rewards
\begin{align}
 &(x_{h}^{\pi;\modst},a_h^{\pi;\modst},h) = (x_{h}^{\pi;\model},a_h^{\pi;\model},h), \quad
 \forall h \le h_{\pi;\ell}, \label{eq:transition_sim}
  \end{align}
  and the rewards for $\model$ up before step $h_{\pi;\ell}$ are at most $\epspunish$:
  \begin{align}
 &r_{h}^{\pi;\model} \le \epspunish, \quad
 \forall h < h_{\pi;\ell},  \label{eq:reward_punish}
 \end{align}
 \end{lemma}
 \begin{proof}[Prove of \Cref{claim:modclass_not}]
 Let us first prove \Cref{eq:reward_punish}. Since the prior is supported on deterministic models, any model $\model$ in the support of $\Prcan[\modvarhat  \in \cdot \mid \ledhall]$ must have identical transitions and rewards to those in the hallucinated ledger $\ledhall$. The transitions in that $\ledhall$ are from the censored ledger $\ledcensl$ collected from the true model $\modst$, and contains a transition for all $\xah \notin \calU_{\ell}$. Hence, for all $h < h_{\pi;\ell}$, the transitions in $\model$ and in $\modst$ are the same.  Hence, by induction over $h \le h_{\pi;\ell}$, $\model$ and $\modst$ visit the same triples for $h \le h_{\pi;\ell}$.

To prove \Cref{eq:reward_punish}, \Cref{eq:transition_sim} implies that for all $h < h_{\pi;\ell}$, $(x_{h}^{\pi;\modst},a_h^{\pi;\modst},h) \in \calU_{\ell}^c$ (i.e. have been visited).   Since all rewards in $\ledcensl$ are punished to have reward at most $\epspunish$ (and again, since the prior is supported on deterministic rewards), we must have that the associated rewards $r_{h}^{\pi;\modst}$ for $h < h_{\pi;\ell}$ are at most $\epspunish$.
 \end{proof}

With the above claim, we turn to \Cref{claim:gap_size}.
\begin{proof}[Proof of \Cref{claim:gap_size}] To bound term (a), observe that all policies $\pi \in \Pi_{\ell}^c$ have $h_{\pi;\ell} = H+1$, since by definition, these policies never visit an unvisited triple $\xah \in \calU^c$. Hence, for all such policies
\Cref{eq:reward_punish} entails that their cumulative reward is at most $H \cdot \epspunish$, which is less than $r_{\min}/2$ by selection of $\epspunish$.

To bound term (b), fix a any policy $\pi \in \Pi_{\ell}$, so that $h_{\pi;\ell} \in [H]$; for simplicity, fix $ h = h_{\pi;\ell}$. For any model, $\valuef{\pi}{\model} \ge r^{\pi;\model}_{h}$, if $\model$ is the support of the posterior $\Prcan[\modvarhat  \in \cdot \mid \ledhall]$, \Cref{claim:modclass_not} entails that
\begin{align}
r^{\pi;\model}_{h} = r_{\model}(x^{\pi;\model},a^{\pi;\model},h) = r_{\model}(x^{\pi;\modst}_h,a_h^{\pi;\modst}), \quad   h = h_{\pi;\ell}. \label{eq:rpi_id}
\end{align}
Thus,
\begin{align*}
\text{Term (b) in \Cref{eq:gap_det_wts}} \ge \Expcan[\valuef{\pi}{\modvarhat} \mid \ledhall] &\ge \Expcan[r^{\pi;\modvarhat}_{h} \mid \ledhall], \quad   h = h_{\pi;\ell}. \\
&\overset{(i)}{=} \Expcan[r_{\modvarhat}(x^{\pi;\modst}_h,a_h^{\pi;\modst}) \mid  \ledhall]\\
&\overset{(ii)}{=} \Expop_{\modvarhat \sim \prior}[r_{\modvarhat}(x^{\pi;\modst}_h,a_h^{\pi;\modst}) \mid  \ledhall]\\
&\overset{(iii)}{\ge} r_{\min},
\end{align*}
where $(i)$ uses \Cref{eq:rpi_id}; $(ii)$ uses the reward independence assumption, and the fact that  the triple $(x^{\pi;\modst}_h,a_h^{\pi;\modst},h)$ does not appear in the hallucinated ledger $\ledhall$, by definition of $h = h_{\pi;\ell}$; and $(iii)$ uses the definition of $r_{\min}$.
\end{proof}


  Invoking the gap computation in \Cref{claim:gap_size}, we may apply the single-step hidden hallucination guarantee (\Cref{prop:mdp_hh}) as soon as we verify $\nphase$ is large enough that.
  \begin{align}
  \nphase \ge 3H \cdot \frac{2}{r_{\min}} \cdot \Prcan[\modvarhat \in \modclass_{\ell}(\epspunish) \mid \ledcensl]^{-1} \label{eq:nphase_lb_one}
  \end{align}
  To do so, we invoke \Cref{asm:indep_prior} to compute.
 \begin{align}
\Prcan[\modvarhat \in \modclass_{\ell}(\epspunish) \mid \ledcensl] &\overset{(i)}{=} \Prcan[\forall \xah \in \calU_{\ell},~ r_{\modvarhat} \le \epspunish \mid \ledcensl] \nonumber \\
&\overset{(ii)}{=} \Prcan[\forall \xah \in \calU_{\ell},~ r_{\modvarhat} \le \epspunish ] \nonumber\\
&\overset{(iii)}{=} \prod_{\xah \notin \calU_{\ell}}\Prcan[r_{\modvarhat} \le \epspunish ]\nonumber\\
&\overset{(iv)}{\ge} f_{\min}(\epspunish)^{SAH - |\calU_{\ell}|} \ge f_{\min}(\epspunish)^{SAH}  \label{eq:punish_det_lb}
 \end{align}
 where $(i)$ recalls the definition of $\modclass_{\ell}(\epspunish)$, $(ii-iv)$ use $\Cref{asm:indep_prior}$: $(i)$ the applies independence of transitions (of which $\ledcensl$ consists) and rewards vector under the prior, $(iii)$ applies independence of rewards, and $(iv)$ the lower bound on the CDF of the reward vectors. By our choice of $\nphase = \ceil{6H/f_{\min}(\epspunish)^{SAH}r_{\min}}$ therefore suffices for \Cref{eq:nphase_lb_one}.
\end{proof}

\subsection{Formal Guarantees for the General Case}

\mscomment{Define Bayesian probability}



\newcommand{\qpunish}{q_{\mathrm{pun}}}
\newcommand{\ralt}{r_{\mathrm{alt}}}
\newcommand{\Expsimhh}{\Expop_{\ledhall \sim \mathrm{hh}}}
\newcommand{\Prsimhh}{\Prop_{\ledhall \sim \mathrm{hh}}}

\newcommand{\Dhall}{\mathcal{D}_{\mathrm{hal};\ell}}
\newcommand{\Dhalnol}{\mathcal{D}_{\mathrm{hal}}}
\newcommand{\ledcens}{\ledger_{\mathrm{cens}}}
\newcommand{\Utot}{\mathcal{U}_{\mathrm{all}}}
\newcommand{\modpunish}{\mathcal{M}_{\mathrm{pun}}}

\begin{definition}[Prior Assumptions: Probabilistic]\label{asm:punish_asms_prob}
Let $\rho > 0$, $\epspunish > 0$ and $\nlearn \in \N$ be given. We define the the \emph{minimal punishing probability} and \emph{minimal alternative rewards} as
\begin{align}
\qpunish &:= \min_{\ledcens}\Prcan[\modvarhat \in \modpunish(\ledcens) \mid \ledcens]\\
\ralt &:= \min_{\ledcens}\min_{\xah \in \calU(\ledcens)}\Exp_{\lambda \sim \Dhalnol(\epspunish;\ledcens)}\Expcan[ r_{\modvarhat}\xah \mid \lambda ],
\end{align}
where (i) the minimum is taken over all censored lends $\ledcens$ for each there exists a triple $\xah \in \reach_{\rho}(\modst)$ which appears strictly less than $\nlearn$ times, (ii) where $\calU(\ledcens)$ is the set of all triples $\xah \in \Utot$ which appear less than $\nlearn$ times in $\ledcens$, (iii) $\modpunish(\ledcens)$ is the set of models $\mu$ for which $r_{\model}\xah \le \epspunish$ for all $\xah \in\Utot \setminus \calU(\ledcens)$, and where $\Dhalnol$ is the hallucination distribution induced by drawing $\model_{\mathrm{hal}} \sim \Prcan[\cdot \mid \ledcens, \modclass(\ledcens)]$, and then imputing the rewards in $\ledcens$ as draws from its reward distirbution (as in).
\end{definition}
\begin{theorem}\label{thm:main_prob_mdp} Fix a target reachability parameter $\rho > 0$, failure probability $\delta$ and sampling size $\nlearn$. Suppose that a punishing parameter $\epspunish$ such that the associated minimal punish probability $\qpunish$ and minimal alternative reward $\ralt$ satisfy
\begin{align*}
\textstyle \epspunish \le \ralt \rho/18H, \quad  \nlearn \ge \frac{800 H^4 (S\log 5 + \log(1/\delta) +  4\log \frac{20 SAH^2 }{\rho \cdot \qpunish \cdot \epspunish  \ralt })}{\rho^2 \ralt^2}
\end{align*}
Then, \Cref{alg:MDP_HH} instantiated with any phase length $\nphase \ge \frac{12 H}{\qpunish \ralt \rho}$ is guaranteed to $(\rho,n)$-explore with probability $1 - \delta$ by episode $K$, where $n = \nlearn$ and $K = \frac{144 SAH^3 \nlearn \nphase}{\rho^2 \ralt^2}$.
%and select punishing parameter $\epspunish$ to satisfiy %$\epspunish \le \Delta_0 := \Delta_0(\rho,\epspunish) := \ralt(\epspunish)  \rho/2$. Then, if \Cref{alg:MDP_HH} is run with punishing parameter $\epspunish$, and
%\begin{align*}
%\textstyle \nphase \ge  \frac{6H}{\Delta_0\qpunish}, \quad
%\nlearn \ge \frac{192 H^4 (S\log 5 + \log(1/\delta) +  \iota(\%epspunish,\rho))}{\Delta_0^2}, \quad \iota(\epspunish,\rho) :=
%\end{align*}
%Then, after $K_0 = \frac{36SAH^3 \nlearn\nphase}{\Delta_0^2} = \BigOhTil{\frac{ S^2AH^8}{\rho^3 \ralt^3 \qpunish }}$ episodes, \Cref{alg:MDP_HH} has $(\rho,\nlearn)$-explored with probability $1 - \delta$.
\end{theorem}
\begin{proof}[Proof Sketch] The proof mirrors that of the deterministic guarantee, \Cref{thm:det_mdp},  but with the following two major differences.

First, rather than showing progress at each phase $\ell$ is guaranteed deterministically, we show that progress occurs with some lower bounded probability; we then argue about the accumulation of this probabilistic progress with Chernoff bounds. This requires modifying the notion of ``good policies'' ($\Pi_{\ell}$) to those which visit novel triples $\xah$ with sufficient probability $\Omega(\rho)$.

Second, due to various sources of randomness, the computation of the gap is quite a bit more involed. To streamline our argument, we elect to reason about the expected gap over models drawn from the posterior in the single-step guaranatee (\Cref{cor:mdp_hh}), rather than its worst-case realizations. We then proceed to bound this expected gap, detailing the proof in \Cref{sec:lemma:gap_bound_prob}.
\end{proof}

\begin{proof}[Proof of \Cref{thm:main_indep}] We now prove our main theorem, \Cref{thm:main_indep}, under the independence assumption \Cref{asm:indep_prior}. Recall $r_{\min}$ and $f_{\min}(\cdot)$ from \Cref{eq:fmin_argmin}. The independence on rewards ensure that we have
\begin{align}
\ralt &:= \min_{\ledcens}\min_{\xah \in \calU(\ledcens)}\Exp_{\lambda \sim \Dhalnol(\epspunish;\ledcens)}\Expcan[ r_{\modvarhat}\xah \mid \lambda ]\\
&= \min_{\ledcens}\min_{\xah \in \calU(\ledcens)}\Expop_{\modvarhat \sim \prior}\Expcan[ r_{\modvarhat}\xah ] = r_{\min}.
\end{align}
Similarly, because rewards are independent of transitions,
\begin{align}
\qpunish &:= \min_{\ledcens}\Prcan[\modvarhat \in \modpunish(\ledcens) \mid \ledcens]\\
&= \min_{\ledcens}\Prop_{\modvarhat \sim \prior}[\modvarhat \in \modpunish(\ledcens)] \ge f_{\min}(\epspunish)^{SAH}.
\end{align}
Hence, we can invoke \Cref{thm:main_prob_mdp} with
\begin{align*}
\epspunish &= r_{\min} \rho/18 H, \\
\nlearn &\ge  \frac{800 H^4 (S\log 5 + \log(1/\delta) +  4\log \frac{360 SAH^3 }{\rho^2 r_{\min}^2 \cdot f_{\min}(\epspunish)^{SAH} })}{\rho^2 \ralt^2}\\
&= \frac{800 H^4 \left(S\log 5 + \log(1/\delta) +  4\log \frac{360 SAH^3 }{\rho^2 r_{\min}^2} + 4 SAH \log(1/ f_{\min}(\epspunish)\right)}{\rho^2 r_{\min}^2}\\
&\quad= \Omega\left(\frac{ H^4 \left( SAH \log \tfrac{e}{f_{\min}(\epspunish)} +  \log \tfrac{1}{\delta} +  \log \tfrac{ SAH^3 }{\rho^2 r_{\min}^2}  \right)}{\rho^2 r_{\min}^2}\right)\\
\end{align*}
\begin{align*}
\nphase &= \ceil{\frac{12H}{r_{\min}\rho f_{\min}(\epspunish)^{SAH}}}\\
K &= \mathcal{O}\left(\frac{SAH^4 \nlearn}{f_{\min}(\epspunish)^{SAH}\rho^3 r_{\min}^3}\right)
\end{align*}
For a given parameter $\rho$, define $\mathcal{C}_{\rho} = f_{\min}(\epspunish) = f_{\min}(r_{\min} \rho/18 H)$. Then, for a universal constant $c_1$,
\begin{align}
\nlearn &\ge \frac{ c_1 H^4 \left( SAH \log \tfrac{e}{\mathcal{C}_{\rho}} +  \log \tfrac{1}{\delta} +  \log \tfrac{ SAH^3 }{\rho^2 r_{\min}^2}  \right)}{\rho^2 r_{\min}^2}\\
K &= \mathcal{O}\left(\frac{SAH^4 \nlearn}{\mathcal{C}_{\rho}^{SAH}\rho^3 r_{\min}^3}\right)
\end{align}

\end{proof}


