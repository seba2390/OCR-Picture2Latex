%!TEX root = ../main-anon.tex

\bluepar{(Simplified) problem formulation.}
A problem formulation in incentivized exploration consists of three components: a machine learning problem collectively solved by the agents, strategic interactions between the algorithm and the agents, and a specific performance objective pursued by the algorithm.  As we delve into incentivized RL, we begin with simple and arguably fundamental versions of all three components.

We use finite, deterministic MDPs without time discounting.%
\footnote{Restriction to \emph{deterministic} MDPs, \ie MDPs with deterministic rewards and transitions, captures the gist of the problem and simplifies presentation. In Section~\ref{sec:extension}, we extend our setup and results to \emph{randomized} MDPs. }
We have $S$ states, $A$ actions, and $H$ stages, denoted $x\in[S]$, $a\in[A]$ and $h\in[H]$. The parameters $S,A,H$ are finite and fixed throughout. An agent interacts with the MDP in stages: in each stage $h\in[H]$, the agent observes the current state $x_h \in [S]$, selects an action $a_h\in [A]$, receives reward $r_h\in[0,1]$ and transitions to a new state $x_{h+1}$. The outcome is agent's \emph{trajectory}, a sequence of tuples
    $(x_h,\,a_h,\, r_h,\,h)_{h\in[H]}$.
An \emph{MDP model} $\model$ specifies the reward
    $\sfr_{\model}\xah$
and the next state for each $\xah$ triple, as well as the initial state $x_0$. A policy $\pi:[S]\times[H]\to [A]$ determines the agent's behavior in the MDP, specifying an action for each state $x$ and stage $h$. The value of a policy $\pi$ under model $\model$, denoted $\valuef{\pi}{\model}$, is defined as the total reward when this policy is executed under this model. Formally:
\begin{align*}
\valuef{\pi}{\model}
    := \textstyle \sum_{h\in[H]} r_h
    = \sum_{h\in[H]} \sfr_{\model}(x_h,\,a_h,\, r_h),
\end{align*}
where the next action is $a_{h+1} = \pi(x_h, h)$ for each stage $h\in [H]$. In \emph{Episodic RL}, there is a fixed but unknown MDP model $\modst$ and $K$ \emph{episodes}. In each episode $k\in [K]$, an algorithm chooses a policy $\pi_k$, this policy is executed in the MDP, and the resultant trajectory is observed. Each episode starts from the same initial state $x_0$.
\footnote{So, an execution of a given policy in a given episode does not depend on the policies from the previous episodes.}

On the economics side, we have a new agent in each episode $k$, and the algorithm must ensure that following its chosen policy $\pi_k$ is in the agent's self-interest. The formal requirement is
\begin{align}\label{eq:greedy-intro}
\pi_k \in \argmax_{\text{policies $\pi$}}\;
    \En_{\modst\sim\prior}\sbr{ \valuef{\pi}{\modst} \mid \pi_k},
\end{align}
where the (true) MDP model $\modst$ is initially drawn from a Bayesian prior $\prior$ over MDP models.
\footnote{The Bayesian framing here is (merely) a way to endow agents with well-defined incentives.}
This is a version of Bayesian incentive-compatibility (\emph{BIC}), a standard requirement in economic theory. The intuition behind \refeq{eq:greedy-intro} is that the agent observes policy $\pi_k$ before her episode starts, and makes a one-time decision whether to follow this policy. In this decision, the agent chooses among all policies, evaluates them based on their Bayesian-expected value, and breaks ties in favor of $\pi_k$. The agent knows the algorithm, the prior $\prior$ and her episode $k$, but she does not know anything about the previous episodes. The algorithm must ensure that the agent chooses $\pi_k$.

The performance objective is \traversal: visit all $\xah$ triples in the smallest number of episodes.%
\footnote{Naturally, we are only interested in $\xah$ triples that are \emph{reachable} by some RL algorithm without incentives constraints.} This  provides sufficient data for policy optimization, possibly with a different reward metric \citep{jin2020reward}.
While \traversal does not attempt to optimize agents' welfare, the resulting data can be used afterwards for exploitation. To elucidate this point, we derive specific corollaries for regret minimization in Section~\ref{sec:exploit}.

Incentivized exploration in bandits is a special case with $H=1$ stages. Specialization of \traversal (visit each arm in the smallest number of rounds) has been studied in this context  \citep[\eg][]{ICexploration-ec15,Selke-PoIE-ec21}. Some initial samples of each arm are required to bootstrap all other published algorithms for randomized rewards. Therefore, all these algorithms are preceded by a ``warm-up stage" which collects the initial samples and, essentially, optimizes for \traversal.
