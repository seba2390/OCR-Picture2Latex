%!TEX root = ../main-arxiv.tex

[OBSOLETE FILE AFTER SEPT'22 REVISION]

We are interested in exploring all $\rho$-reachable $\xah$ triples, for a given parameter $\rho>0$, in the smallest possible number of episodes. The guarantees must depend on $\rho$, even for episodic RL without incentives.

%As a shorthand, we say that an algorithm \emph{$(\rho,n)$-explores} by episode $K$ if each $\rho$-reachable $\xah$ triple is visited in at least $n$ distinct episodes. Multiple visits ($n>1$) are usually desired in order to estimate mean rewards or transition probabilities in a randomized MDP.

\bluepar{Independent priors.}
Our guarantees are most lucid under reward-independence as in \Cref{defn:reward_independence} (recall that the transition probabilities can still be correlated across $\xah$ triples.) The dependence on the prior is  captured via the two parameters that take the worst case across all $\xah$ triples:
%
%Our guarantees are most lucid under the following assumption dubbed \emph{reward-independence}: under the prior $\prior$ over $\modst$, mean rewards $r_{\modst}\xah$ are independent across $\xah$ triples, and the mean reward vector $r_{\modst}\in [0,1]^{SAH}$ is independent of the transition probabilities.
%
%Our guarantees are most lucid under the following assumption:
%\begin{definition}
%A prior $\prior$ over $\modst$ is called \emph{reward-independent} if mean rewards $r_{\modst}\xah$ are independent across $\xah$ triples, and jointly independent of the transition probabilities.
%\end{definition}
%Note that the transition probabilities can still be arbitrarily correlated across the $\xah$ triples.
%
%\begin{asm}[Reward Independence]\label{asm:indep_prior} We suppose that under the prior $\prior$ over $\modst$, the vector of average rewards $r_{\modst}(\cdot,\cdot,\cdot) \in [0,1]^{SAH}$ has independent entries for $\modst \sim \prior$, and is independent of the transition probabilities $\sfp_{\modst}(\cdot \mid \cdot)$.  We permit the marginal distribution of the transition probabilities to be arbitrary.
%\end{asm}
%
\begin{align}\label{eq:fmin_argmin}
f_{\min}(\eps) &:= \min_{x,a,h}\quad
    %\Prop_{\modst \sim \prior}
    \Pr\sbr{ \sfr_{\modst}\xah \le \eps}
\quad\text{and}\quad
r_{\min} := \min_{x,a,h}\quad
    %\Expop_{\modst \sim \prior}
    \Exp\sbr{ \sfr_{\modst}\xah }.
\end{align}
%These definitions \asedit{consider the prior distribution for} the mean reward of a particular $\xah$ triple, and take the worst case over all $\xah$ triples.
In particular, the probability of the punish-event is lower-bounded as
    $\Pr\sbr{\EvPun} \geq \rbr{f_{\min}(\epspunish)}^{-SAH}$,
and $r_{\min}$ is simply a uniform lower bound on the prior mean rewards.

\begin{theorem}\label{thm:main_indep}
Consider a reward-independent prior $\prior$. Fix parameters $\rho, \delta\in (0,1]$. Assume that $r_{\min}>0$ and
    $\calC_{\rho}  := f_{\min}\rbr{ \epspunish}>0$,
where
    $\epspunish = r_{\min}\,\rho\,/\, 18\,H$.
Consider \Cref{alg:MDP_HH} with punishment parameter $\epspunish$, appropriately chosen phase length $\nphase$, and large enough target $n=\nlearn$. This algorithm is guaranteed to $(\rho,n)$-explore with probability at least $1-\delta$ by episode $K_{\rho,n}$, where $n$ and $K_{\rho,n}$ are specified below.

%Then \Cref{alg:MDP_HH} with punishment parameter $\epspunish$ and appropriately chosen $\nphase$ and $n=\nlearn$ is guaranteed to $(\rho,n)$-explore with probability at least $1-\delta$ by episode $K_{\rho,n}$ if $n$ and $K_{\rho,n}$ are sufficiently large, as given below.

For some absolute constants $c_1,c_2$, it suffices to take
\begin{align*}
n = \nlearn &\ge
     c_1\cdot \rho^{-2}\, r_{\min}^{-2}\,
     H^4 \rbr{ SAH \log \calC_{\rho}^{-1} +
        \log \frac{ SAH}{\delta\rho\, r_{\min}} }, \\
K_{\rho,n} &= c_2\cdot n\cdot \calC_{\rho}^{-SAH} \cdot \rho^{-3}\, r_{\min}^{-3}\, SAH^4.
\end{align*}
In particular, for any $n\geq 1$, one can obtain
    $K_{\rho,n} \leq n\cdot  \calC_{\rho}^{-SAH}
        \cdot \poly\rbr{\rho^{-1}\, r_{\min}^{-1}\, SAH  }
        \cdot \log\rbr{\delta^{-1}\,\calC_{\rho}^{-1}}$.
\end{theorem}

\begin{remark}The two assumptions in \Cref{thm:main_indep} are in line with prior work. Indeed, let us specialize to multi-armed bandits with independent priors, and let $\sfr(a)$ denotes the mean reward of arm $a$. Note that $\xah$ triples specialize to arms, all of which are trivially $\rho$-explorable. Exploring all arms in this setting requires $r_{\min}>0$ and $f_{\min}(r_{\min})>0$, according to the characterization in \citet{Selke-PoIE-ec21}.
\end{remark}

The guarantees (as well as the analysis) become simpler for deterministic MDPs.

\begin{theorem}\label{thm:det_mdp}
Consider a reward-independent prior $\prior$ supported on deterministic MDPs.
Assume that $r_{\min}>0$ and
    $\calC  := f_{\min}\rbr{ \epspunish}>0$,
where
    $\epspunish = r_{\min}/2H$.
Consider \Cref{alg:MDP_HH} with $\nlearn = 1$, punishment parameter $\epspunish$,  and
    $\nphase = \ceil{ 6H\,r_{\min}^{-1}\,\calC^{-SAH} }$.
The algorithm visits a new $\xah$ triple in every phase, until all reachable triples are visited. This takes at most this many episodes:
    \[ K = \calC^{-SAH}\cdot O\rbr{ SAH^2\, r_{\min}^{-1} }.\]
\end{theorem}

%Under a stronger independence assumption, restricting the selection rule \eqref{eq:greedy} to Markovian policies is without loss of generality.  Essentially, this is due to algorithm's ``ledger hygiene" (see \Cref{sec:canon} for proof).
%
%\begin{lemma}\label{lm:markov-wlog}
%Consider \Cref{alg:MDP_HH} when prior $\prior$ is independent across $\xah$ triples both in rewards and in transition probabilities. Then the agents can select Markovian policies w.l.o.g., even if the selection rule \eqref{eq:greedy} optimizes over all policies, Markovian or not.
%\end{lemma}

\bluepar{Arbitrary priors.}
Our analysis in fact extends to arbitrary priors, and yields \Cref{thm:main_indep} as a corollary. We express the dependence on the prior via similar parameters, and then express a sufficient number of episodes in terms of these parameters. The necessary technicalities are spelled out below.

We introduce an abstract notion of a \emph{partially-censored ledger} $\ledger$. Formally,
it is any dataset of a particular shape: a sequence of (policy, partial-trajectory pairs) $(\pi_k,\tau_k)$,  along with a \emph{censoring set} $\calU_{\ledger} \subset [S]\times[A]\times[H]$. The reward information is censored out from all trajectories for all triples $\xah\in \calU_{\ledger}$; that is, each $\tau_k$ is of the form $(x_h,\,a_h,\, \tilde{r}_h,\,h)_{h\in[H]}$, where $\tilde{r}_h \in [0,1]$ records a reward for uncensored triples $(x_h,a_h,h) \notin \calU_{\ledger}$, but $\tilde{r}_h = \bot$ replaces the reward value with a special censoring symbol for triples in $\calU_{\ledger}$.
%\footnote{Formally, when some reward value is ``censored out" in a ledger, it is replaced with a special symbol such as $\bot$.}
 The ledger is called \emph{totally-censored} if
    $\calU_\ledger = [S]\times[A]\times[H]$, so that no reward information is recorded.

We define the prior-dependent parameters in terms of the posterior distribution given ledger $\ledger$, in the worst case over all ledgers. However, the posterior may depend not only on $\ledger$, but also on the process by which it is constructed. To deal with this subtlety, we Bayesian-update as if $\ledger$ was constructed by an algorithm which deterministically chooses the policies in $\ledger$. Thus, we define the \emph{canonical posterior} on $\modst$, denoted  $\Prcan\sbr{ \cdot \mid \ledger }$, and  \emph{canonical expectation} $\Expcan\sbr{f(\modst)\mid \ledger}$ over this posterior (formally defined in \Cref{sec:canon}.)


%in an entirely non-strategic manner (we make this precise in \Cref{sec:canon}). Probabilities and expectations according to the canonical posteriors are denoted, resp.,
%    $\Prcan\sbr{\cdot\mid\ledger}$
%and
%    $\Expcan\sbr{\cdot\mid\ledger}$.

We define the prior-dependent parameters as follows:
\begin{align}
\qpunish(\eps) &:=
    \min_{\text{totally-censored ledgers $\ledger$}}\quad
        \Prcan\sbr{\sfr_{\modst}\xah \le \eps \;\;
            \text{for all $\xah$ triples} \mid \ledger},
            \label{eq:qpun-defn}\\
\ralt &:=
    \min_{\text{partially-censored ledgers $\ledger$}}\quad
    \min_{\xah \in \calU_{\ledger}}
    \Expcan\sbr{ \sfr_{\modst}\xah \mid \ledger }.
        \label{eq:ralt-defn}
\end{align}
In particular, the canonical posterior probability of the punish-event given the censored ledger in the algorithm,
    $\Prcan\sbr{ \EvPun\mid \ledcensl}$,
is uniformly lower-bounded by $\qpunish(\epspunish)$.
Likewise, $\ralt$ lower-bounds the posterior mean reward
    $\Expcan\sbr{ \sfr_{\modst}\xah \mid \ledger }$
given any ledger $\ledger$ that censors out this particular $\xah$ pair.

\begin{remark}\label{rem:reduction-to-independence}
These parameters can be easily related to those in \refeq{eq:fmin_argmin} under reward-independence, essentially because the conditioning on $\ledger$ vanishes. First, $\ralt=r_{\min}$. Second, the probability in \eqref{eq:qpun-defn} factorizes as
    $\prod_{\xah}\; \Prop_{\modst \sim \prior} \sbr{\sfr_{\modst}\xah \le \eps}$.
%where each factor is lower-bounded by $f_{\min}(\eps)$.
It follows that
%    $ \Prcan\sbr{ \EvPun\mid \ledcensl} \geq
$\qpunish(\epspunish) \geq f_{\min}^{-SAH}(\epspunish)$.
\end{remark}

\begin{comment}where (i) the minimum is taken over all censored lends $\ledcens$ for each there exists a triple $\xah \in \reach_{\rho}(\modst)$ which appears strictly less than $\nlearn$ times, (ii) where $\calU(\ledcens)$ is the set of all triples $\xah \in \Utot$ which appear less than $\nlearn$ times in $\ledcens$, (iii) $\modpunish(\ledcens)$ is the set of models $\mu$ for which $r_{\model}\xah \le \epspunish$ for all $\xah \in\Utot \setminus \calU(\ledcens)$, and where $\Dhalnol$ is the hallucination distribution induced by drawing $\model_{\mathrm{hal}} \sim \Prcan[\cdot \mid \ledcens, \modclass(\ledcens)]$, and then imputing the rewards in $\ledcens$ as draws from its reward distirbution (as in).
\end{comment}

The general result has the same shape as \Cref{thm:main_indep} for reward-independence, but
    $r_{\min}$ and $f_{\min}^{-SAH}(\epspunish)$
are replaced with, resp., $\ralt$ and $\qpunish(\epspunish)$. Accordingly, \Cref{thm:main_indep} follows immediately by Remark~\ref{rem:reduction-to-independence}.

\begin{theorem}\label{thm:main_prob_mdp}
Consider an arbitrary prior $\prior$. Fix parameters $\rho, \delta\in (0,1]$. Assume that $\ralt>0$ and
    $\qpunish = \qpunish\rbr{\epspunish}>0$,
where
    $\epspunish = \ralt\,\rho\,/\, 18\,H$.
Consider \Cref{alg:MDP_HH} with punishment parameter $\epspunish$, appropriately chosen phase length $\nphase$, and large enough target $n=\nlearn$. This algorithm is guaranteed to $(\rho,n)$-explore with probability at least $1-\delta$ by episode $K_{\rho,n}$, where $n$ and $K_{\rho,n}$ are specified below.

%Then \Cref{alg:MDP_HH} with punishment parameter $\epspunish$ and appropriate $\nphase$, $\nlearn$ is guaranteed to $(\rho,n)$-explore with probability at least $1-\delta$ by episode $K_{\rho,n}$, if $n$ and $K_{\rho,n}$ are sufficiently large.

For some absolute constants $c_1,c_2$, it suffices to take
\begin{align}
n=\nlearn &\ge
     c_1\cdot \rho^{-2}\, \ralt^{-2}\,H^4
        \rbr{ S + \log \frac{SAH}{\delta\rho\cdot\ralt\cdot \qpunish} },
        \label{eq:thm:main_prob_mdp-n} \\
K_{\rho,n} &= c_2\cdot n\cdot \qpunish \cdot \rho^{-3}\, \ralt^{-3}\, SAH^4.
\end{align}
In particular, for any $n\geq 1$ one can obtain
 $K_{\rho,n} \leq n\cdot \qpunish
        \cdot \poly\rbr{\rho^{-1}\, \ralt^{-1}\, SAH  }
        \cdot \log\rbr{\delta^{-1}\,\qpunish^{-1}}$.
    \label{eq:thm:main_prob_mdp-K}
 \end{theorem}

\begin{remark}\label{rem:replace-qpun}
{Parameter $\qpunish(\epspunish)$ in \Cref{thm:main_prob_mdp} can be replaced with any number $L>0$ which lower-bounds
    $\Prcan\sbr{ \EvPun\mid \ledcensl}$
for all phases $\ell$.}
\end{remark}


%Fix a target reachability parameter $\rho > 0$, failure probability $\delta$ and sampling size $\nlearn$. Suppose that a punishing parameter $\epspunish$ such that the associated minimal punish probability $\qpunish$ and minimal alternative reward $\ralt$ satisfy
%\begin{align*}
%\textstyle \epspunish \le \ralt \rho/18H, \quad  \nlearn \ge \frac{800 H^4 (S\log 5 + \log(1/\delta) +  4\log \frac{20 SAH^2 }{\rho \cdot \qpunish \cdot \epspunish  \ralt })}{\rho^2 \ralt^2}
%\end{align*}
%Then, \Cref{alg:MDP_HH} instantiated with any phase length $\nphase \ge \frac{12 H}{\qpunish \ralt \rho}$ is guaranteed to $(\rho,n)$-explore with probability $1 - \delta$ by episode $K$, where $n = \nlearn$ and $K = \frac{144 SAH^3 \nlearn \nphase}{\rho^2 \ralt^2}$.

%and select punishing parameter $\epspunish$ to satisfiy %$\epspunish \le \Delta_0 := \Delta_0(\rho,\epspunish) := \ralt(\epspunish)  \rho/2$. Then, if \Cref{alg:MDP_HH} is run with punishing parameter $\epspunish$, and
%\begin{align*}
%\textstyle \nphase \ge  \frac{6H}{\Delta_0\qpunish}, \quad
%\nlearn \ge \frac{192 H^4 (S\log 5 + \log(1/\delta) +  \iota(\%epspunish,\rho))}{\Delta_0^2}, \quad \iota(\epspunish,\rho) :=
%\end{align*}
%Then, after $K_0 = \frac{36SAH^3 \nlearn\nphase}{\Delta_0^2} = \BigOhTil{\frac{ S^2AH^8}{\rho^3 \ralt^3 \qpunish }}$ episodes, \Cref{alg:MDP_HH} has $(\rho,\nlearn)$-explored with probability $1 - \delta$.

