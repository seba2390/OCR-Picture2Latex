Two closely related papers have appeared subsequent to the initial publication of our work on \texttt{arxiv.org}. We discuss these papers below.

\citet{IncentivizedRL-ec22} study another model that combines RL and Bayesian Persuasion.%
\footnote{Our initial publication on \texttt{arxiv.org} predates theirs by a full year.}
Their model is similar to ours in that an algorithm for episodic RL needs to incentivize self-interested agents to follow its recommendations. The key difference is that their model focuses on payoff-relevant `outcomes' that are drawn IID before each episode and observed by the algorithm but not to the agents. These `outcomes' are the only source of information asymmetry used by the algorithm to create incentives; the guarantees in \citep{IncentivizedRL-ec22} appear vacuous when the `outcomes' are not observed. In contrast, the only source of information asymmetry in our model (and all prior work on incentivized exploration) is the history of interactions with other agents. An intriguing open question is to combine these two sources so that they reinforce one another.

\citet{CombiIE-neurips22} study a model of incentivized exploration with large, structured action set and correlated priors.%
\footnote{Our initial publication on \texttt{arxiv.org} predates theirs by more than a year, and acknowledged by theirs as prior work.}
Their learning problem is combinatorial semi-bandits, a version of bandits where the learner chooses among feasible \emph{subsets} of arms in each round, and rewards of all chosen arms are observable. This problem is stateless, and therefore ``easier" than ours. They  achieve stronger results on regret-minimization: essentially, Thompson Sampling is BIC for their model when initialized with $N$ samples of each atom, for some $N$ determined by the prior. In particular, their algorithm attains regret which scales as $\tilde{O}(\sqrt{T})$ in the number of rounds $T$. However, their solution for collecting one sample of each atom takes time exponential in the number of atoms, similar to how the number of episodes for our algorithm is exponential in $SAH$.  