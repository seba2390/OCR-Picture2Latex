[OBSOLETE FILE AFTER SEPT'22 REVISION]

%Going forward, we define $\calU_{\ell}$ as the set of all triples which are under-explored by phase $\ell$; we let $\calU_{\ell}^c = [S] \times [A] \times[H] \setminus \calU_{\ell}$ is the set of triples fully-explored by phase $\ell$. Formally, the punish-event is
%\begin{align}
%\EvPun = \{\sfr_{\modst}\xah \le \epspunish, \quad \forall \xah \in \calU_{\ell}^c \} \label{eq:EvPun}
%\end{align}

%The section begins by formalizing the definition of censoring and ledgers ( \Cref{sec:cens_and_led}); Formal (symbolic) descriptions of the ledgers $\ledrawl,\ledcensl,\ledhonl,\ledhall$ in $\mdphh$ are defered to \Cref{app:detailed-spec}. In \Cref{sec:canon}, we formalize the canonical probability, and introduce the \emph{ledger hygiene} condition, under which canonical probabilities coincide with true posteriors. Via ledger hygiene, \Cref{sec:one_step} establishes a single-phase guarantee (\Cref{prop:mdp_hh}) for hidden hallucination. Building on the single-phase guarantee, \Cref{sec:det_transitions} provides a self-contained proof of our main result for deterministic MDPs, \Cref{thm:det_mdp}. Finally, \Cref{sec:probab_proof} turns to general probabilistic MDPs, and  provides the main steps of the proof of \Cref{thm:main_prob_mdp}.

This section is organized as follows. In \Cref{sec:canon}, we discuss \emph{ledger hygiene}, a crucial property of our ledgers which underpins the rest of the analysis. \Cref{sec:one_step} analyzes a single phase of \mdphh (\Cref{prop:mdp_hh}). Building on that, \Cref{sec:det_transitions} provides a self-contained proof of \Cref{thm:det_mdp}, our main result for deterministic MDPs. Finally, \Cref{sec:probab_proof} turns to general MDPs, and  provides the main steps of the proof of \Cref{thm:main_prob_mdp}. The details are deferred to the appendices.

\subsection{Canonical Posteriors and Ledger Hygiene}
\label{sec:canon}

We want legers to be interpretable on the face value, regardless of the process used to generate it. We capture this property via the notion of \emph{canonical posterior} $\Prcan\sbr{ \cdot \mid \ledger }$, whereby we pretend that a given ledger $\ledger$ is constructed by an algorithm which deterministically chooses the policies in $\ledger$. Then, we turn to \emph{random ledgers}, \ie ledger-valued random variables. We define \emph{ledger hygiene}, a  property which asserts that the posterior given a random ledger is in fact the canonical posterior. While this is a non-trivial property, we show that the censored and honest ledgers in \mdphh satisfy this property.


Formally, consider a partially-censored ledger $\lambda$ which stores $n$ policy-trajectory pairs, and let $\pi_1 \LDOTS \pi_n$ be the respective policies. Consider an algorithm which interacts with MDP model $\modst$, proceeds for $n$ episodes, chooses policy $\pi_k$ in each episode $k\in [n]$, and observes some trajectory, denoted $\traj_k$. Let $\Lambda$ be a random ledger with the same censoring set $U_\lambda$ and policy-trajectory pairs
    $(\pi_k,\traj'_k)$, $k\in[n]$,
where $\traj'_k$ denotes trajectory $\traj_k$ in which all reward data for triples $\xah\in U_\lambda$ is censored out.


We define $\Prcan\sbr{ \cdot \mid \ledger }$ as a distribution over $\modtotal$ by conditioning the prior $\prior$ on the event
$\cbr{\Lambda = \ledger}$:
\begin{align}\label{eq:canon-posterior-defn}
\Prcan\sbr{\modclass \mid \ledger }
    &:= \Prop \sbr{ \modst \in \modclass \mid \Lambda = \ledger }
    \quad\forall \text{ measurable }\modclass\subset\modtotal.
\end{align}
For a particular event $\modclass$,
    $\Prcan\sbr{\modclass \mid \ledger }$
is called the \emph{canonical probability} given $\ledger$. Given a measurable function $f:\modtotal\to \R$, we define the \emph{canonical expectation}
    $\Expcan\sbr{f(\modst)\mid \ledger}$
given $\ledger$ as the expectation of $f(\cdot)$ over distribution $\Prcan\sbr{ \cdot \mid \ledger }$.
Now we are ready to define ledger hygiene.

\begin{definition}\label{def:hygiene}
A random ledger $\ledger$ is called \emph{hygienic} if it satisfies
\begin{align}\label{eq:hygiene}
 \Pr\sbr{\modst \in \modclass \mid \ledger}  = \Prcan[\modclass \mid \ledger ]
 \quad\forall \text{ measurable }\modclass\subset\modtotal.
\end{align}
\end{definition}

One can construct numerous examples of non-hygenic ledgers, see \Cref{app:canonical_vs_mechanism}. For example, policies in the ledger could encode more information about $\modst$ than the canonical posterior can extract.
%$\hat{\ledger}$ in ways that communicate more information than the canonical conditional, which treats those policies as fixed and therefore uninformative.
%Nevertheless, we show that the censored and honest ledgers in \mdphh are indeed hygienic.

\begin{lemma}\label{lem:data_hygiene}
Censored ledger $\ledcensl$ and honest ledger $\ledhonl$ are both hygienic.
\end{lemma}

\begin{proof}[Proof Sketch]
The essential property we use is that the policies in $\ledcensl$, and the censoring set $\calU_{\ell}$ for $\ledhonl$, are determined exactly by $\ledcensl[\ell-1]$, that is, the visited triples $\xah$ from previous hallucination episodes. Thus, $\ledcensl$ and $\ledhonl$ do not depend on transition data which are not in their own ledgers, and do not communicate reward information (because reward information is only explicitly used on non-hallucination episodes). The formal proof is given in \Cref{proof:lem_data_hygiene}
\end{proof}

%\subsection{ One-Step Hidden Hallucination Mechanism}
\subsection{Analysis for a Single Phase}
\label{sec:one_step}

%We now state a single-phase exploration guarantee which forms the cornerstone of our analysis. The goal of this analysis is to

We derive conditions under which an agent selects a policy from some desired subset $\Pi \subset \Pitotal$.


%\begin{align}\label{eq:canon-value-defn}
%\valuecan{\Pi}{\ledger} :=  \max_{\pi \in \Pi}\; \Expcan[\valuef{\pi}{\modst} \mid \ledger].
%\end{align}


To state these conditions, we introduce the notion of \emph{canonical gap}. The \emph{canonical value} of policy $\pi$ given ledger $\ledger$ is defined as
    $\Expcan[\valuef{\pi}{\modst} \mid \ledger]$.
The canonical gap for policy set $\Pi$ is the difference in maximal canonical value between $\Pi$ and its complement.

\begin{definition}[Canonical Gap]\label{defn:canonical_gap}
The canonical gap of policy set $\Pi \subset \Pimarkov$ given ledger $\ledger$ is
\begin{align*}
\gapcan[\Pi \mid \ledger] :=
    \max_{\pi \in \Pi}\; \Expcan[\valuef{\pi}{\modst} \mid \ledger]
    \;-\;
    \max_{\pi \not\in \Pi}\; \Expcan[\valuef{\pi}{\modst} \mid \ledger]
\end{align*}
\end{definition}

%Note that the canonical gap depends only on the value of $\ledger$, but not on the process that generated it.

The meaning  behind this definition is that if an agent were to observe a hygienic ledger $\ledger$ with a positive canonical gap, this agent would choose some policy in $\Pi$.

Our algorithm construct hallucinated ledgers $\ledger = \ledhall$ so as to yield a positive canonical gap, and we'd like to conclude that an agent would choose some policy in $\Pi$ in the hallucination episodes (which would constitute progress towards our exploration goal). Unfortunately, random ledgers $\ledger_k$ revealed by our algorithm are not hygienic, precisely because of hallucination episodes. We circumvent this issue if
    $\gapcan\sbr{\Pi \mid \ledhall}$
is not only positive, but much larger than $1/\nphase$, where $\nphase$ is the phase duration. We formulate a condition which also depends on the punish-event $\EvPun$:
\begin{align}\label{eq:HH_good_condition}
3H/\nphase \le \Prcan\sbr{\EvPun \mid \ledcensl} \cdot \gapcan\sbr{\Pi \mid \ledhall}.
\end{align}
The essence of Hidden Hallucination as a technique is that this condition suffices.

%Specifically, we show that if there is a lower bound on (a) the  canonical gap for a class $\Pi$ under $\ledhall$ and (b) the canonical probability of $\EvPun$ given the totally censored ledger $\ledcensl$, then the agent will select still policy $\pi \in \Pi$.


\begin{proposition}[Hidden Hallucination]\label{prop:mdp_hh} Let $\Pi \subset \Pimarkov$ be any subset of policies. Fix phase $\ell$ in the algorithm. A policy in $\Pi$ is chosen in the hallucination episode if condition \eqref{eq:HH_good_condition} holds.
%and suppose the hallucinated ledger  $\ledhall$ and phase length $\nphase > 0$ satisfy:
%Then for $k = \kexpl$, any Bayes greedy policy $\pi_k$ must lie in $\Pi$.
\end{proposition}

Full proof is in \Cref{proof:prop_mdp_hh}. To use this proposition, we will establish uniform (\ie data-independent) lower bounds on both terms on the right-hand side of \eqref{eq:HH_good_condition}, and set $\nlearn$ accordingly.

\begin{proof}[Proof Sketch]
Fix episode $k$. The proof readily reduces to bounding the conditional probability that $k$ is the hallucination episode, given the revealed ledger $\ledger_k$. Intuitively, we need to bound the agents' belief that they are facing a hallucinated ledger rather than an honest one. Our argument is inspired by the analysis of hidden exploration in \cite{ICexploration-ec15}, but differs in a number of respects; notably, the role of ledger hygiene.

Recall that the censored ledger $\ledcensl$ comprises all data observed by agent $k$ that is known to be faithfully transmitted by the algorithm. We condition on $\ledcensl$, and verify that
\begin{align}\label{eq:Prhall_ledhall}
\Pr[\; k=\kexpl \mid \ledcensl \;] = \Pr[\; k\neq\kexpl \mid \ledcensl,\, \EvPun\;].
\end{align}
In words, agent $k$ finds it equiprobable that she is in a hallucination episode and that she is shown an honest ledger but the true model lies in the punish-event.
%thus, when shown hallucinated ledger, the agent may either believe they are on a hallucination episode $k = \kexpl$, or simply that it happened to be the case, by chance, that $\modst \in \modclass_{\ell}(\epspunish)$, but that $k \ne \kexpl$.

When $1/\nphase$ is very small, the probability of $k = \kexpl$ is dominated by the probability of $\EvPun$, and \Cref{eq:HH_good_condition} quantifies exactly how small $1/\nphase$ must be. Note that this condition is stated in terms of canonical conditionals, which suffices because the censored and honest ledgers $\ledhonl$ and $\ledcensl$ are hygienic.
\end{proof}
%In general, whether or not the condition \eqref{eq:HH_good_condition} holds is a random event, depending in part on (a) the samples comprising $\ledcensl$ and (b) the randomness used in hallucinating $\ledcensl$. For simplicity,

%\newcommand{\moddet}{\calM_{\mathrm{det}}}
\subsection{Reward-Independence and Deterministic MDPs: full proof of \Cref{thm:det_mdp}}
\label{sec:det_transitions}

We analyze \Cref{alg:MDP_HH} in the simplified setting where the prior is reward-independent, and all MDPs in its support have deterministic rewards and transitions.  Recall that $\nlearn = 1$.

Let us start with some notation. The fully-explored (resp., under-explored) $\xah$ triples are now simply the ones that have (resp., have not) been visited at least once during the past-phase hallucination episodes. Let $\underexplored$ denote the set of all $\xah$ triples that are under-explored in a given phase $\ell$. Let $\Pi_{\ell}$ be the set of all policies $\pi\in \Pimarkov$ which deterministically visit some triple $\xah\in \underexplored$ under the true model $\modst$. Note that $\Pi_\ell$ is non-empty if and only if some under-explored $\xah$ triple is reachable.

%Let $\calU_{\ell}$ be the set of all $\xah$ triples which have not yet been visited during the past-phase hallucination episodes; we will call these triples \emph{non-explored}; fully-explored triples we just call \emph{explored}. Let $\Pi_{\ell}$ be the set of all policies $\pi\in \Pimarkov$ which visit any $\xah$ triple in $\calU_{\ell}$ under the true model $\modst$. (This is a deterministic event, since the MDP is deterministic.)

%Define the policy sets $\Pi_{\ell}$, where
%\begin{align}
%\Pi_{\ell} := \{\pi : \pi \text{ visits  a non-explored triple under the true model } \modst \},
%\end{align}
%where we note that, for deterministic MDPs, which triples a given policy visits is deterministic.

We apply \Cref{prop:mdp_hh} with policy set $\Pi = \Pi_\ell$, so as to guarantee that an agent selects some policy in $\Pi_\ell$ in the hallucination episode, and therefore visits some under-explored $\xah$ triple. Then all reachable $\xah$ triples will be visited after at most $SAH$ phases, \ie in at most $SAH \cdot \nphase$ episodes.

We lower-bound the two terms on the right-hand side of \refeq{eq:HH_good_condition}. First, reward-independence implies, as per \Cref{rem:reduction-to-independence}, that
\begin{align*}
\Prcan[\EvPun \mid \ledcensl] \ge f_{\min}(\epspunish)^{SAH}.
\end{align*}
Hence, it remains to bound the canonical gap:

\begin{claim}\label{claim:gap_size}
Suppose $\epspunish \le r_{\min}/2H$ and fix phase $\ell$. Then
    $\gapcan\sbr{\Pi_{\ell} \mid \ledhall} \geq r_{\min}/2$.
\end{claim}

Then \Cref{prop:mdp_hh} follows with the choice of $\nlearn$ as in \Cref{thm:det_mdp}, and we are done.

It remains to verify \Cref{claim:gap_size}. To this end, we analyze
    $\Prcan[\;\cdot \mid \ledhall]$,
the canonical posterior given the hallucinated ledger. We prove that any model in the support of this distribution can partially \emph{simulate} the trajectory of any given policy under the true model $\modst$.

\begin{lemma}[Deterministic Simulation Lemma]\label{lem:modclass_not}
Fix phase $\ell$ and policy $\pi \in \Pitotal$. Fix any model $\model$ in the support of
    $\Prcan[\;\cdot \mid \ledhall]$.
Let $h_\pi$ be the first stage at which policy $\pi$ visits any under-explored $\xah$ triple under the fake model $\model$; let $h_\pi=H+1$ this never happens. Then:
\begin{OneLiners}
\item[(a)]
the $\xah$ triples visited by policy $\pi$ in stages $h\leq h_\pi$ are identical under $\modst$ and $\model$.
\item[(b)]
all rewards collected by policy $\pi$ in stages $h<h_\pi$ under model $\model$ are at most $\epspunish$.
\end{OneLiners}
\end{lemma}

\begin{proof}
Since the prior $\prior$ is supported on deterministic MDPs, the fake model $\model$ must have identical transitions and rewards to those in the hallucinated ledger $\ledhall$. By construction, $\ledhall$ contains transitions for all fully-explored $\xah$ triples, and they come from the true model $\modst$. Hence the transitions in models $\model$ and $\modst$ are the same for all stages $h < h_\pi$. Part (a) follows by induction over stages $h \le h_\pi$.

To prove part (b), fix any $\xah$ triple visited by policy $\pi$ in stage $h\leq h_\pi$ under the fake model $\mu$. This triple is fully-explored by part (a). Consequently, this triple is assigned expected reward at most $\epspunish$ in the hallucinated posterior \eqref{eq:algo-halposterior-defn}. Since we only deal this deterministic MDPs, this expected reward propagates as the observed reward in the hallucinated ledger $\ledhall$, and then into the fake model $\model$.
\end{proof}

\begin{proof}[Proof of \Cref{claim:gap_size}]
Denote the hallucinated ledger as $\ledger = \ledhall$. For brevity, we write \emph{fake models} to refer to all models $\model$ in the support of
    $\Prcan[\;\cdot \mid \ledger]$.

First, fix any policy $\pi\not\in \Pi_\ell$. By definition, its trajectory under the true model $\modst$ never runs out of fully-explored $\xah$ triples. By \Cref{lem:modclass_not}, its total reward under any fake model $\model$ is at most $\epspunish H$, which is at most $r_{\min}/2$ by our choice of $\epspunish$. It follows that
\begin{align}\label{eq:claim:gap_size-1}
(\forall \pi\not\in\Pi_\ell) \qquad
    \Expcan[\valuef{\pi}{\modst} \mid \ledger] \leq r_{\min}/2.
\end{align}

Second, fix any policy $\pi \in \Pi_{\ell}$. By definition, its trajectory under the true model $\modst$ visits some under-explored $\xah$ triple; let's focus on the first such triple. Take any fake model $\model$. By \Cref{lem:modclass_not}, policy $\pi$ visits $\xah$ under $\model$, too. Consequently,
    $\valuef{\pi}{\model} \geq \sfr_{\model}(x,a,h)$.
It follows that
\begin{align}
(\forall \pi\in\Pi_\ell)\qquad
\Expcan[\valuef{\pi}{\modst} \mid \ledger]
    &\geq \Expcan[\sfr_{\model}(x,a,h) \mid \ledger] \nonumber \\
    &\geq \ralt
    &\EqComment{by definition of $\ralt$} \nonumber\\
    &= r_{\min}
    &\EqComment{by \Cref{rem:reduction-to-independence}}.
    \label{eq:claim:gap_size-2}
\end{align}
The claim follows by plugging \eqref{eq:claim:gap_size-1} and \eqref{eq:claim:gap_size-2} into the definition of the canonical gap.
\end{proof}


\subsection{The general case: proof overview for \Cref{thm:main_prob_mdp}}
\label{sec:probab_proof}

%{sec:det_transitions}

%Mirroring the proof of the deterministic case, we restrict our attention to establishing that, with probability $1 - \delta$, each $\xah \in \reach_{\rho}(\modst)$ is contained in at least $\nlearn$ trajectories only from hallucination episodes. Unlike the deterministic case, we can only ensure probabilistic progress -  with constant probability over the mechanism, the hallucinated ledger induces the agent to explore an under-explored triple $\xah \in \calU_{\ell}$. This formalized by \Cref{lem:main_prob_lemma}, from which \Cref{thm:main_prob_mdp} follows by an application of standard concentration inequalities in \Cref{proof:main_prob_mdp}.

%To establish \Cref{lem:main_prob_lemma}, we invoke a corollary (\Cref{cor:mdp_hh}) of the one-step guarantee (\Cref{prop:mdp_hh}) which replaces worst-case gap with an expected gap, described in \Cref{eq:Delta_gap}. We then bound this quantity in \Cref{lemma:gap_bound_prob}, whose proof is sketched at the end of the section. The formal proof of \Cref{lemma:gap_bound_prob} is given in \Cref{sec:lemma:gap_bound_prob} and consumes the brunt of our technical effort.

We strive to mimic the analysis from the previous subsection: we carry over the major steps and deal with various complications that arise due to randomness. Some of the details are deferred to the appendices.

Let us start with some notation. Fix phase $\ell$. Let $\calF_{\ell}$ denote the $\sigma$-algebra generated by all randomness (in rewards, transitions, and the algorithm) in all phases up to and including this phase.  As before, let $\underexplored$ denote the set of all $\xah$ triples that are under-explored in this phase. Let $\Pi_{\ell,q}$ be the set of policies $\pi\in \Pimarkov$ which visit some triple $\xah\in \underexplored$ with probability at least $q$ (under the true model $\modst$).

We are interested in the event that the algorithm makes progress in a given phase $\ell$. Specifically, let
$\progress$ be the event that the algorithm visits some triple $\xah\in\underexplored$ in the hallucination episode in this phase. The progress is only probabilistic, expressed via a rather subtle statement:
\begin{align}\label{eq:genCase-progress}
\Pr\sbr{\Eexplorel \mid \calF_{\ell-1}} \geq \rhoprog
\qquad\text{with probability at least $1-\deltafail$ over $\calF_{\ell-1}$}.
\end{align}
Given the parameters from the theorem statement, the parameters in \eqref{eq:genCase-progress} are chosen as follows:
\begin{align}\label{eq:genCase-params}
\text{$\deltafail = \nphase/K_{\rho,n}$
and
$\rhoprog=\Delta_0^2/6H^2$, where $\Delta_0 = \rho\cdot\ralt/2$}.
\end{align}


\begin{lemma}[Progress]\label{lem:main_prob_lemma}
\refeq{eq:genCase-progress} holds in each phase $\ell > \nlearn$ such that some triple in $\underexplored$ is $\rho$-reachable.
\end{lemma}

The proof of \Cref{thm:main_prob_mdp} follows directly from \Cref{lem:main_prob_lemma} via a martingale-Chernoff argument which is common in the study of tabular MDPs (this is spelled out \Cref{proof:main_prob_mdp}).

We ensure that a policy from a given policy set $\Pi$ is chosen in the hallucination episode, under suitable conditions. We use a corollary of \Cref{prop:mdp_hh} which holds under a weaker condition compared to  \eqref{eq:HH_good_condition}. This new condition is somewhat subtle to define. We treat
    $\gapcan\sbr{\Pi \mid \ledger}$,
the canonical gap given ledger $\lambda$, as a random variable with randomness induced by the ledger. In a given phase $\ell$, take a conditional expectation of this random variable given the censored ledger $\ledcensl$:
\begin{align}\label{eq:mean-canonical-gap-defn}
\gapcanl[\Pi\mid \ledger]
    &= \Exp\sbr{ \gapcan\sbr{\Pi \mid \ledger} \;\mid\; \ledcensl }
    &\EqComment{mean-canonical gap}.
\end{align}
Essentially, we average out all randomness in ledger $\ledger$ that comes from the current phase; the only remaining randomness comes from $\ledcensl$. The new condition is also given ledger $\ledger = \ledhall$, but it only requires the mean-canonical gap to be bounded, not the realization of the canonical gap. The new condition is stated as follows, for some deterministic parameter $\Delta$:
\begin{align}\label{eq:HH-stronger-condition}
\gapcanl\sbr{\Pi \mid \ledhall}
    \geq \Delta
    \geq \frac{6H}{\nphase \cdot \Prcan\sbr{\EvPun \mid \ledcensl}}.
\end{align}
We prove that this condition suffices to make progress, in a probabilistic sense.

\begin{lemma}[Hidden Hallucination via mean-canonical gap]\label{cor:mdp_hh}
Fix phase $\ell$ and policy set $\Pi \subset \Pimarkov$ such that \eqref{eq:HH-stronger-condition} holds for some deterministic parameter $\Delta$. Then, with probability at least $\Delta/2H$ over the draw of $\ledhall$ (conditioned on $\calF_{\ell-1}$), an agent in the hallucination episode chooses a policy in $\Pi$.
\end{lemma}


We obtain \eqref{eq:HH-stronger-condition} with a policy set $\Pi = \Pi_{\ell,q}$ (for some $q$) and parameter $\Delta = \Delta_0$. As before, we lower-bound
    $\Prcan\sbr{\EvPun \mid \ledcensl}$
with  $\qpunish(\epspunish)$, by definition of the latter (\Cref{eq:qpun-defn}). The key is to lower-bound the mean-canonical gap, which we accomplish next. The statement is also probabilistic: the mean-canonical gap being a random variable with randomness coming from the censored ledger $\ledcensl$, we obtain a high-probability statement over the randomness in $\ledcensl$.

\begin{lemma}[Probabilistic Gap Bound]\label{lemma:gap_bound_prob}
Fix phase $\ell > \nlearn$. Recall parameters $\deltafail$ and $\Delta_0$ from \eqref{eq:genCase-params}. With probability at least $1 - \deltafail$ over the randomness in ledger $\ledcensl$, it holds that
\[ \gapcanl\sbr{\Pi_{\ell,q} \mid \ledhall}
    \geq \Delta_0,
    \quad\text{where $q = \Delta_0/3H$}.\]
\end{lemma}

These two lemmas about the canonical gap imply \Cref{lem:main_prob_lemma}, if one plugs in all the parameters. (We omit the tedious but straightforward details.)

\subsubsection*{Proof Sketch of \Cref{lemma:gap_bound_prob}}\label{ssec:sketch:lemma:gap_bound_prob}

To analyze the mean-canonical gap, we define a similar version of the canonical posterior \eqref{eq:canon-posterior-defn}. The \emph{mean-canonical posterior} $\Prcanl[\,\cdot\mid\ledger]$ given a random ledger $\lambda$ is a distribution over MDP models given by
\begin{align}\label{eq:hal-posterior-defn}
\Prcanl[\modclass \mid\ledger]
    &:= \Exp\sbr{ \Prcan\sbr{ \modclass \mid \ledger }
        \;\mid\; \ledcensl }
    \quad\forall \text{ measurable }\modclass\subset\modtotal.
\end{align}


Paralleling the deterministic analysis (\ie the proof of \Cref{claim:gap_size}), we consider
    $\Prcanl[\,\cdot \mid\ledhall]$,
the mean-canonical posterior given the hallucinated ledger $\ledhall$. We verify that, with a ``good enough" probability, a model $\model$ drawn at random from this posterior satisfies the following two properties:
\begin{itemize}
\item[(a)] for all fully-explored $\xah$ triples, $\model$ has small mean rewards, $\sfr_{\model}\xah$.
\item[(b)] for all fully-explored $\xah$ triples, the transition probabilities, $\sfp_{\model}(\cdot \mid x,a,h)$, are close to those for the true model, $\sfp_{\modst}(\cdot \mid x,a,h)$ (and a similar closeness holds for the initial state distributions, provided $\ell > \nlearn$).
\end{itemize}
We show that these properties imply a small mean-canonical gap, relying on a probabilistic version of the  simulation lemma (\Cref{lem:modclass_not}). This version is stated and proved in \Cref{lem:visitation_comparison_general}.

To establish points (a) and (b), we must address several sources of randomness: (i) randomness in realized rewards and transitions, (ii) randomness in the draw of the hallucinated rewards and (iii) randomness in the agent's posterior given the hallucinated ledger. We address them simultaneously by constructing a set of ``good models'' under which points (a) and (b) hold, and using Bayesian concentration inequalities to show that with high probability under all combined sources of randomness, any model under the agents posterior given a hallucinated ledger lies in this ``good set''.

We need to account for the fact that the mean-canonical posterior
    $\Prcanl[\,\cdot \mid\ledhall]$
is not the posterior formed by an agent given ledger $\ledhall$ (because agents know that rewards may be hallucinated).
However, in view of \Cref{eq:Prhall_ledhall},
% the posterior over the hallucinated model
it is a conditional posterior given the punish-event $\EvPun$. We argue that the strength of concentration and the rarity of hallucinations (which occur only once per phase) overwhelm the effect of this conditioning. See \Cref{sec:lemma:gap_bound_prob} for the full argument.

