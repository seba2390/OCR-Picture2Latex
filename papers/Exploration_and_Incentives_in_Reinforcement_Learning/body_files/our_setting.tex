%!TEX root = ../main-arxiv.tex

[OBSOLETE FILE AFTER SEPT'22 REVISION]

As explained in the introduction, our problem formulation consists of three parts: the learning problem, strategic interactions, and performance objective. We flesh out the details for these parts one by one. We note that both RL and economic design tend to be heavy on notation, even when studied separately.  We strive to minimize and/or defer the heavy notation whenever possible.

\bluepar{Learning problem: Episodic RL.} We use a standard model of \emph{Markov Decision Processes} (henceforth, MDPs) without time discounting. Let us spell out the relevant notation. We have $S$ states, $A$ actions, and $H$ stages, where $H$ is the \emph{horizon}; these parameters are finite and fixed throughout. We write
    $x\in[S]$, $a\in[A]$ and $h\in[H]$
for, resp., states, actions and stages. We often talk about $\xah$ \emph{triples}.

{An \emph{MDP model} $\model$ specifies, for each $\xah$ triple,
the reward distribution
    $\sfR_{\model}\xah$
over $[0,1]$ with mean reward
    $\sfr_{\model}\xah$,
and transition probabilities
    $\sfp_{\model}\rbr{\cdot \mid x,a,h} \in \Delta(S)$.}
An agent interacts with the MDP in stages. In each stage $h\in[H]$, the agent observes the current state $x_h\in[S]$, selects action $a_h\in[A]$, receives  reward $r_h\in[0,1]$ and transitions to a new state $x_{h+1}\in[S]$,
drawn independently from distribution
    $\sfp_{\model}\rbr{\cdot \mid x_h,a_h,h}$.
The initial state $x_1$ is drawn independently from distribution
    $\sfp_{\model}\rbr{\cdot \mid 0}$;
the last state $x_{H+1}$ is inconsequential.
%\mscomment{kill $\sfR$, and make per-stage capital h}
The reward $r_h$ is drawn independently from some reward distribution
    $\sfR_{\model}(x_h,\, a_h,\, h)$.
For ease of exposition, we posit that all reward distributions are supported on the same countable set; the paradigmatic example is Bernoulli rewards. The shape of reward distributions does not matter for this paper, only the mean rewards do. Agent's \emph{trajectory} (a.k.a. \emph{raw} trajectory) is a sequence of tuples
    $(x_h,\,a_h,\, r_h,\,h)_{h\in[H]}$.
{An important special case is \emph{deterministic MDP}, where both rewards and transitions are deterministic.}


%by the reward distribution $\sfR_{\model}$ with means $r_{\model}$ and transitions $\sfp_{\model}$, \footnote{It is possible to have two models with the same mean reward function, but different reward distributions. However, this redundancy is immaterial to the analysis.}  In any given interaction with $\model$, at time step $h = 1$, the learner begins in a stage $x_1$ drawn from the initial state distribution $\sfp_{\model}(\cdot \mid 0) \in \Delta(S)$. Then, at each $h \in [H]$ and current state $x_h$, the learner selects action $a_h \in [A]$, recieves a random reward $r_h$ with Markovian reward $r_h$ satisfying $r_h  \mid (x_{\tau},a_{\tau},\tau)_{\tau=1}^h] \sim \sfR_{\model}(x_h,a_h,h) \in \Delta([0,1])$, and $\Exp[r_h \mid (x_{\tau},a_{\tau},\tau)_{\tau=1}^h] = r_{\model}(x_h,a_h,h)$, and transitions to state $x'$ at stage $h+1$ according to $\sfp_{\model}(\cdot \mid x,a,h) \in \Delta(S)$, via $\Pr[ x_{h+1} = x' \mid (x_{\tau},a_{\tau},\tau)_{\tau=1}^h) \equiv \sfp_{\model}(x' \mid x,a,h)$.\footnote{We assume that all triples $(x,a,H)$ transition to a universal terminal state, $x_{H+1}$}. To simplify conditional expectations, we assume that the realized rewards $r_h$ from any model are supported on a common, countable set (e.g. all rewards are Bernoulli, or supported on the rationals, etc.); nevertheless, the means $r_{\model}(x_h,a_h,h)$ can be real-valued.

A deterministic, Markovian policy $\pi$ is a mapping
    $[S]\times[H]\to [A]$
which specifies the agent's behavior in the MDP. The action chosen at stage $h$ for state $x$ is denoted $\pi(x,h)$.
%\msmargincomment{renotate $\pi(x,h)$}
%\msmargincomment{renotate $\sfr_\mu$}
The set of all such policies is denoted $\Pimarkov$. Policies are deterministic and Markovian unless specified otherwise.%
\footnote{In general, policies may be randomized, and may depend on past trajectory.}

%    $\pi = (\pi_h:\;h\in[H])$

% $\pi \in \Pimarkov$ is a sequence of mappings $\pi = (\pi_{1:H})$, where $\pi_{h}: [S] \to [A]$.

A policy $\pi$ and an MDP model $\model$ induce a distribution over trajectories, denoted $\sfP^{\pi}_{\model}$. Expectation under this distribution is denoted $\sfE^{\pi}_{\model}$. As a convention, we use bold letters to denote the corresponding random trajectory
    $(\bmx_{h},\,\bma_h,\,\bmr_h,\,h)_{h\in[H]}$.
 %We let $\sfE^{\pi}_{\model}$ and $\sfP^{\pi}_{\model}$ denote expectations and probabilities with respect to the joint distribution of the entire trajectory, and will use bold letters
%    $(\bmx_{h},\,\bma_h,\,\bmr_h,\,h)_{h\in[H]}$
%to denote random variables whose distribution obeys $\sfP^{\pi}_{\model}$.
The value of a policy $\pi$ under $\model$ is defined as the expected reward:
%the expected sum of its accrued rewards
\[\valuef{\pi}{\model} := \textstyle
    \sfE^{\pi}_{\model}\sbr{\sum_{h=1}^H \bmr_h}
    = \sfE^{\pi}_{\model}\sbr{\sum_{h=1}^H \sfr_{\model}(\bmx_h,\bma_h,h)}.\]

%\newcommand{\rvsim}{\overset{\mathrm{r.v.}}{\sim}}

In \emph{Episodic RL}, there is a fixed but unknown MDP $\modst$ and $K$ episodes.
In each episode $k\in [K]$, an algorithm chooses a policy $\pi_k$, this policy is executed in the MDP, and the resultant trajectory is observed.

\bluepar{Strategic interactions: Incentivized RL.}
We define \emph{incentivized RL}, a game between the principal and $K$ agents. We define a slightly more general game compared to the Introduction, in which the principal can send arbitrary signals. Initially, a true model $\modst$ is drawn from a prior $\prior$ over the class $\modtotal$ of all feasible MDP models. The prior is known to the principal and all agents, but the the model $\modst$ is not known. Our results hold for general priors, but are most interpretable over an important special class of priors:
\begin{definition}[Reward Independence]\label{defn:reward_independence} A prior $\prior$ over $\modst$ is called \emph{reward-independent} if mean rewards $r_{\modst}\xah$ are independent across $\xah$ triples, and jointly independent of the transition probabilities.
\end{definition}

The game proceeds in $K$ episodes. Each episode $k\in[K]$ encompasses interaction with a single agent, denoted agent $k$, and one execution of the MDP. Formally, it proceeds as follows:
\begin{OneLiners}
\item[1.] the principal chooses a signal $\signal_k$;
\item[2.] agent $k$ arrives, observes $k$ and $\signal_k$, and chooses a policy $\pi_k$;
\item[3.] this policy is executed in the MDP;
\item[4.] the agent receives reward $\sum_{h\in[H]} r_{k,h}$; \\
the principal observes the resultant trajectory
    $\traj_k = \rbr{ x\kh,\, a\kh,\, r\kh,\, h }_{h\in[H]}$.
\end{OneLiners}
No other information is revealed to the principal or the agents. The principal chooses signals $\signal_k$ according to some algorithm, which is known to the agents.%
\footnote{ We avoid the term \emph{signaling policy}, standard in economic literature, to avoid confusion with MDP policies.}
The set of all possible signals, denoted $\signals$, can be arbitrary. We assume it is countable for ease of exposition (to avoid continuous probability distributions).

Each agent $k$ chooses the policy so as to maximize her conditional expected reward given what she knows: the round $k$, the observed signal $\signal_k$, the algorithm used to generate this signal, the prior, and the selection rule for the previous agents. More precisely, we make an inductive definition: for each agent $k=1,2, \ldots$
\begin{align}\label{eq:greedy}
\pi_k \in \argmax_{\pi\in \Pimarkov}
    \Exp\sbr{ \valuef{\pi}{\modst} \mid \signal_k},
\end{align}
with some fixed tie-breaking, and this selection rule is known to all agents. In this definition, the signal $\signal_k$ is treated as a random variable over $\signals$, whose distribution is determined by the algorithm, the prior, and the selection rule for the previous agents. We condition on a particular realization of this random variable, as chosen by the principal and observed by the agent. This conditioning yields a conditional distribution over models $\modst$, which is the distribution that $\valuef{\pi}{\cdot}$ in \eqref{eq:greedy} is integrated over.
\footnote{The selection rule \eqref{eq:greedy} merely specifies an agent's rational response to a given signal under Bayesian uncertainty over $\modst$, when the alternatives are $\pi\in\Pimarkov$ and the rewards are $\valuef{\pi}{\modst}$. How these objects are defined is unimportant to this definition. On this level of abstraction, this is a very standard setup in theoretical economics.}

The social-planner problem (\ie the problem solved by a hypothetical ``social planner" who controls the agents) is a Bayesian version of episodic RL. The Bayesian framing here is (merely) a standard way to endow agents with well-defined incentives. In the special case of horizon $H=1$, the social-planner problem reduces to multi-armed bandits, and incentivized RL reduces to incentivized \emph{bandit} exploration studied in prior work. Restriction to Markovian policies in the selection rule \eqref{eq:greedy} represents an simplification, as discussed in the Introduction. On the other hand, restriction to deterministic policies is w.l.o.g.: a deterministic maximizer exists even if randomized policies were allowed.


\begin{remark}[Signal v.s. Policy Revelation]
It is easy to transform this model to the version described in the Introduction, whereby an algorithm directly recommends a policy to each agent (and nothing else), and ensures that following this policy is in the agent's best interest. The principal can compute the policy $\pi_k$ in \eqref{eq:greedy} and recommend this policy directly, changing the signal to $\pi_k$. Then \refeq{eq:greedy} would hold for this new signal, too, as per a standard ``direct revelation argument" \citep{Kremer-JPE14}. However, the version with arbitrary signals, as stated above, is more convenient for technical exposition in this paper.
\end{remark}


%Then Markovian policies maximize the expected policy value $\Exp\sbr{\valuef{\pi}{\modst}}$ among all policies, Markovian or not.

%{eq:greedy}

%Thus, we can w.l.o.g. transform an algorithm to one that satisfies the Markov-BIC property, as required in the Introduction.




%$\valuef{\pi}{\modst}$




\bluepar{Performance objective: reachability in MDPs.}
%An important feature of MDPs is that
Some states in the MDP might not be reachable by any policy, or only reachable with small probability. Given $\rho\in [0,1]$, we say a triple $\xah$ is called \emph{$\rho$-reachable} under the true MDP $\modst$,  if some policy $\pi \in \Pitotal$ reaches it with probability at least $\rho$, \ie
    $\sfP_{\modst}^{\pi}\sbr{ (\bmx_h,\bma_h) = (x,a)} \ge \rho$. For deterministic MDPs, we take $\rho=1$ without loss of generality, and say
a triple $\xah$ is called \emph{reachable} if
    $(\bmx_h,\bma_h) = (x,a)$
for some policy $\pi \in \Pitotal$.
Note that whether a particular triple $\xah$ is $\rho$-reachable is determined only by the state-step pair $(x,h)$. The algorithm's objective, called \traversal, is parameterized by $\rho\leq 1$ and $n\in\N$.


\begin{definition} We say that an algorithm  has $(\rho,n)$-traversed $\modst$ by episode $K$ if each $\rho$-reachable $\xah$ triple (under $\modst$) is visited in at least $n$ episodes $k \le K$. Here, an $\xah$ triple is called \emph{visited} in a given episode $k$ if $ (x_{k;h},a_{k;h},h) = \xah$. We say a mechanism $\alg$ satisfies $(\rho,n,\delta,K)$-\traversal \ if
\begin{align}
\Pr[\alg \text{ has $(\rho,n)$-traversed } \modst \text{ by episode } K] \ge 1 -\delta.
\end{align}
\end{definition}
For a target $(\rho,n)$, confidence $\delta$, the objective is to achieve $(\rho,n,\delta,K)$-\traversal in smallest number of episodes $K$. Multiple visits ($n>1$) are usually desired in order to estimate mean rewards or transition probabilities in a randomized MDP. As shown in \Cref{app:exploit}, the trajectories collected by an algorithm which satisfies $(\rho,n,\delta,K)$-\traversal can be converted into an \emph{exploitation phase} suffering at most $\tilde{\mathcal {O}}((\rho SH^2 + H^3\sqrt{S/n})$ per-episode regret. We use this property to derive regret corollaries from our proposed \mdphh algorithm.




%The set of all such $\xah$ triples is denoted $\reach(\modst)$.

