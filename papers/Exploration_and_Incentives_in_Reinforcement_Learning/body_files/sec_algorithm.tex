%!TEX root = ../main.tex

Let us describe \mdphh, our algorithm for \traversal objective (\Cref{alg:MDP_HH}). 
%What follows is a slightly informal exposition, and we defer fully formal specification to \Cref{app:detailed-spec}. 
Recall that we focus on deterministic MDPs for now (and randomized MPDs are treated in Section~\ref{sec:extension}).


\bluepar{Preliminaries: signals.} Our algorithm is more naturally described in a slightly more general model of incentivized RL (which reduces to the one described in the Introduction). In this model, each episode $k$ proceeds as follows:
\begin{OneLiners}
\item[1.] the algorithm chooses a signal $\signal_k$ from some fixed set $\signals$ of possible signals;
\item[2.] agent $k$ arrives, observes $k$ and $\signal_k$, and chooses a policy $\pi_k$;
\item[3.] this policy is executed in the MDP;
%\item[4.] the agent receives reward $\sum_{h\in[H]} r_{k,h}$;
the algorithm observes the resultant trajectory $\traj_k$.
%    $\traj_k = \rbr{ x\kh,\, a\kh,\, r\kh,\, h }_{h\in[H]}$.
\end{OneLiners}
No other information is revealed to the principal or the agents. The algorithm for choosing the signals is known to the agents.%
\footnote{This algorithm is often called \emph{signaling policy}; we avoid this term to prevent confusion with ``MDP policies".}
The set $\signals$ of feasible signals can be an arbitrary countable set.
%We assume it is countable for ease of exposition (to avoid continuous probability distributions).

Each agent $k$ chooses the policy so as to maximize her conditional expected reward given what she knows: the round $k$, the observed signal $\signal_k$, the algorithm, the prior, and the selection rule for the previous agents. More precisely, we make an inductive definition: for each agent $k=1,2, \ldots$
\begin{align}\label{eq:greedy}
\pi_k \in \argmax_{\pi \in \Pimarkov}
    \Exp\sbr{ \valuef{\pi}{\modst} \mid \signal_k},
\end{align}
with some fixed tie-breaking, and this selection rule is known to all agents. In this definition, the signal $\signal_k$ is treated as a random variable over $\signals$, whose distribution is determined by the algorithm, the prior, and the selection rule for the previous agents. We condition on a particular realization of this random variable, as chosen by the principal and observed by the agent. This conditioning yields a conditional distribution over models $\modst$, which is the distribution that $\valuef{\pi}{\cdot}$ in \eqref{eq:greedy} is integrated over.
\footnote{The selection rule \eqref{eq:greedy} merely specifies an agent's rational response to a given signal under Bayesian uncertainty over $\modst$, when the alternatives are $\pi\in\Pimarkov$ and the rewards are $\valuef{\pi}{\modst}$. How these objects are defined is unimportant to this definition. On this level of abstraction, this is a very standard setup in theoretical economics.}

To reduce this model to the version from the Introduction (where each signal $\signal_k$ must be a policy), the principal can compute the policy $\pi_k$ in \eqref{eq:greedy} and recommend this policy directly, changing the signal to $\pi_k$. Then \refeq{eq:greedy-intro} would hold,
% for this new signal, too,
by a standard ``direct revelation argument" \citep{Kremer-JPE14}.

We use the model with signals because this is what our algorithm directly specifies, whereas it is unclear how explicitly specify the resulting policies $\pi_k$ in \eqref{eq:greedy}.

\bluepar{Key ideas.}
We would like to  ``punish" already-visited $\xah$ triples by making their rewards appear very small, so as to incentivize exploration of the remaining $\xah$ triples. Such ``punishment" traces back to \Ecubed{} algorithm \citep{kearns2002near}, the classic RL algorithm mentioned in the Introduction. Our algorithm implements this ``punishment" in a way consistent with agents' incentives.

Essentially, in some episodes agents should observe histories where the true rewards are replaced with small ``hallucinated" rewards. Such \emph{hallucination episodes} should be randomly hidden among many \emph{honest episodes}, in which the rewards are revealed faithfully. The agents should not know which type of episode they are in, and the probability of the hallucination episode should be low enough for the agents to (mostly) trust the hallucinated rewards.

A naive implementation of these ideas would reveal the entire history to each agent (replacing the rewards in the hallucination episodes, as discussed above). Unfortunately, this approach may reveal more than intended: agents in hallucination episodes may be able to use the trajectories from the honest episodes to infer something about rewards. This is because the agents would second-guess why the algorithm selected particular policies observed from trajectories in the past. Such second-guessing might make the agents suspect that they are in a hallucination episode, and therefore distrust the observed rewards. It appears difficult to prove that this issue does not prevent the agents from exploring.

Therefore, we only reveal \emph{partial} histories (which we call \emph{ledgers}), and design them to be interpretable in a particularly simple way. Generically, a Bayesian update for any given subset of observations may depend both on the observations and the algorithm used to collect these observations. We enforce a formal property that the Bayesian update on the revealed ledger does not depend on the algorithm. This property, called \emph{ledger hygiene}, is spelled out in Section~\ref{sec:canon}. For its sake, the ledgers only describe the past hallucination episodes, while all honest episodes are left out. The rewards in the ledger are true or hallucinated, depending on the current episode.

%Our algorithm reveals partial histories, called \emph{ledgers}, which contain  transition data from certain past episodes and partial reward data for the same episodes. The algorithm proceeds in phases of fixed number of episodes. Episodes within a given phase differ in how they reveal the reward data for already-visited $\xah$ triples. All but one episode are \emph{honest}, revealing this data faithfully. One episode is randomly selected as a \emph{hallucination episode}, where this data is replaced with very small ``hallucinated'' rewards.

Another issue, which comes up for correlated priors (and also for randomized MDPs), is that the hallucinated rewards must look \emph{plausible}. For example, if an $\xah$ triple has a low-mean Bernoulli reward, then a high reward is likely to be observed if this triple is seen sufficiently often. Moreover, if two $\xah$ triples have strongly correlated mean rewards, then the hallucinated rewards should be correlated similarly. For these reasons, one cannot just hallucinate a low reward for each $\xah$ triple in the ledger.
%\msedit{The same also goes for priors under which rewards between triples are correlated.}
Therefore, we hallucinate \emph{an entire MDP} with low mean rewards, and then resample the rewards according to this MDP, for all $\xah$ triples in the ledger. The hallucinated MDP must be consistent with the transition data revealed in the ledger, so we sample it from the appropriately defined conditional distribution.

Ledger hygiene and MDP hallucination ensure that agents in hallucination episodes take their observed rewards at face value. Since the hallucinated ledger actively punishes rewards from already-visited $\xah$ triples, it incentivizes the new agent towards the remaining $\xah$ triples. Consequently, the agent selects \emph{some} policy which advances exploration.

%\asedit{Given $\rho>0$ and $\nlearn$, the algorithm's goal is to visit all $\rho$-explorable $\xah$ triples at least $\nlearn$ times.}
%Informally, the algorithm's goal is to visit as many $\xah$ triples as possible. Each triple should be visited at least $\nlearn$ times, where $\nlearn$ is given as input.

\bluepar{Algorithm specification: phases and ledgers.}
The algorithm proceeds in phases of $\nphase$ episodes each. In each phase $\ell$, it selects the \emph{hallucination episode} $\kexpl$ uniformly at random from all episodes. %\asdelete{Informally, in this episode the algorithm reveals ``hallucinated rewards" -- fake rewards drawn from a particular distribution -- which encourage an agent to explore. In all other episodes the algorithm reveals true rewards, so that an agent can exploit. Thus, the hallucination episode is \emph{hidden} among many exploitation episodes in the same phase.}
The algorithm proceeds indefinitely; we bound a sufficient number of phases in the analysis.

In each episode in phase $\ell$,  the algorithm reveals trajectories from all past-phase hallucination episodes.%
\footnote{An episode is called \emph{past-phase} if it has occurred in one of the preceding phases.}
The set of all these episodes is denoted
    $\calK_{\ell} := \{\kexpl[\ell']: \ell' \in [\ell-1]\}$.
%\asdelete{The reward information in these trajectories is modified for the sake of creating desirable incentives.}
A triple $\xah$ is called \emph{fully-explored} at phase $\ell$  if it is visited at least once
% $\nlearn$ times
in the past-phase hallucination episodes $\calK_{\ell}$, and \emph{under-explored} otherwise. Reward information for fully-explored triples is either revealed faithfully (in honest episodes), or hallucinated. Reward information for under-explored triples is \emph{censored}: not revealed to the agents.

% $\calU_{\ell} \subset [S]\times[A]\times[H]$
%consists of all triples $\xah$ visited less than $\nlearn$ times on past hallucination episodes.
%Recall that an \emph{trajectory},  $\traj := (x_h,a_h,h,r_h)_{h=1}^H$ is a sequence of (states, actions, stage, reward) tuples, and $\traj_k$ denotes the trajectory collected on episode $k$ of the protol.

To make this precise, we define a \emph{ledger}: the sequence of policy-trajectory pairs from all past-phase hallucination episodes $k\in \calK_{\ell}$, in which some of the reward information may be altered. In a formula, a ledger for phase $\ell$ is a tuple
    $ \rbr{ (\pi_k,\traj'_k):\; k\in \calK_{\ell}} $,
where $\pi_k$ is a policy chosen in episode $k$, and $\traj'_k$ is some trajectory which has the same transitions as the true episode-$k$ trajectory $\traj_k$, but may modify or remove some of the rewards.%
\footnote{Removing a reward for a given stage of the trajectory can be implemented by replacing it with a special symbol such as $\bot$.}
We consider four types of ledgers, depending on how the rewards are treated:
\begin{itemize}
\item the \emph{raw ledger} $\ledrawl$ retains all information on rewards;
%the trajectories from all \asedit{past-phase} hallucination episodes.
    %$ \kexpl[1],\dots,\kexpl[\ell-1]$.
\item the \emph{censored ledger} $\ledcensl$ removes all reward information;
%contains the same trajectories as $\ledrawl$,
%but these trajectories are \emph{censored} so as to remove all reward information.
\item the \emph{honest ledger} $\ledhonl$
 %partially censors the trajectories in $\ledrawl$: it
 retains reward information for fully-explored $\xah$ triples,
 but removes the rewards for all under-explored $\xah$ triples.
    %$\xah \notin \calU_{\ell}$. %where $\calU_{\ell} \subset [S]\times[A]\times[H]$ consists of all triples $\xah$ visited less than $\nlearn$ times on past hallucination episodes.
\item the \emph{hallucinated ledger} $\ledhall$
    %Like the honest ledger, it
    %takes the raw ledger,
     removes reward information from all under-explored $\xah$ triples,
    %$\xah \notin \calU_{\ell}$. But unlike the honest ledger,
    and modifies (\emph{hallucinates}) the rewards for all fully-explored $\xah$ triples.
\end{itemize}

%We stress that $\ledrawl,\ledcensl,\ledhonl,\ledhall$ contain exactly the same \emph{transition} information, but differ in which rewards are shown, and, in the case of $\ledhall$, how the rewards are fabricated.


\noindent On hallucination episodes $\kexpl$, we reveal the hallucinated ledger $\ledhall$. All other episodes implement \emph{exploitation} by revealing the honest ledger $\ledhonl$. \begin{comment}{Again, formal definitions are deferred to \Cref{app:detailed-spec}.}\end{comment}

The remaining crucial ingredient is how to generate the hallucinated rewards. As we discussed in ``Key Ideas", directly hallucinating low rewards for specific $\xah$ pairs might appear non-plausible to the agents. Instead, we hallucinate the entire MDP. Specifically, we define the \emph{punish-event} $\EvPun$ that for all triples $\xah$ that are fully explored at phase $\ell$, the rewards are smaller than a given parameter
$\epspunish>0$. That is:
\begin{align}\label{eq:punish-event-defn}
\EvPun = \cbr{ \sfr_{\modst}\xah \le \epspunish: \;
                \text{all triples $\xah$ that are fully explored at phase $\ell$}}.
\end{align}
%\asdelete{Informally, we ``punish" the fully-explored triples by pretending that they yield small expected rewards, so as to encourage an agent to explore elsewhere. To make this formal,}
Consider \refeq{eq:algo-halposterior-defn}, the posterior distribution of the true model $\modst$ given $\EvPun$ and the censored ledger $\ledcensl$, \ie the transition data. We draw from this posterior to hallucinate an MDP model $\modhall\in \modtotal$. Finally, we use $\modhall$ to hallucinate rewards for each fully-explored $\xah$ triple.

\begin{comment}; {see \Cref{eq:modhalll,eq:hallucinate_rewards,eq:hallucinate_trajectories,eq:hallucinate_ledger} for a symbolic description.}\end{comment}



%On exloitation episodes $k \ne \kexpl$, \mdphh reveals the honest ledger $\ledhonl$ as the signal to the agent,

\begin{algorithm}[h]
  	\begin{algorithmic}[1]
  	\State{}\textbf{Input: }
        phase length $\nphase$,
        %target {\#samples} $\nlearn$,
        punishment parameter $\epspunish>0$
    \For{each phase $\ell= 1,2,\;\dots$}
    \State{}$\Phase_{\ell} = (\ell-1) \nphase  + [\nphase] \subset \N$
        ~~~\algcomment{\% the next $\nphase$ episodes}
    \State{}Draw ``hallucination episode" $\kexpl$ uniformly from $\Phase_{\ell}$
    %\State{}Draw episode $\kexpl$  uniformly from $\PhaseEpisodes_{\ell}$
    %\State{}Draw $\kexpl \unifsim N(\ell - 1) + [N]$
    \For{each episode $k \in \Phase_{\ell}$}
    \If{$k = \kexpl$}
    \algcomment{\qquad\% hallucination episode}
    \State{}\algcomment{\% censored ledger $\ledcensl$,
        punish-event $\EvPun$ with parameter $\epspunish$}
    \State{}Define distribution $\posteriorhall$ over MDP models by
        \algcomment{\quad\% hallucinated posterior}
        \begin{align}\label{eq:algo-halposterior-defn}
         \posteriorhall(\modclass) := \Pr\sbr{\modst \in \modclass \mid \ledcensl,\; \EvPun},
            \quad\forall \text{ measurable }\modclass\subset\modtotal.
         \end{align}
    \State{}Draw MDP model $\modhall$ at random from $\posteriorhall$\label{line:modhall}
    %\State{}Draw $\modelhall\sim \Pr[\modst \in \cdot \mid \ledcensl,\; \EvPun]$
        \algcomment{\quad\% hallucinated MDP}
     \State{}For each fully-explored $\xah$ triple,
        \algcomment{\quad\% hallucinate rewards}
     \State{}\hspace{2em}each time this triple appears in the ledger,
     \State{}\hspace{2em}draw its reward as prescribed by $\modhall$.
     %\State{}Draw rewards from $\modelhall$ for each fully-explored $\xah$ triple
     \State{}Form $\ledhall$ by inserting the hallucinated rewards into $\ledcensl$
        \algcomment{\quad\% hallucinated ledger}
     \State{}Reveal hallucinated ledger: $\ledrevk \gets \ledhall$
    \Else{} \algcomment{\qquad\% exploitation}
     \State{}Reveal honest ledger: $\ledrevk \gets \ledhonl$.
    \EndIf
    \State{}Observe the trajectory $\traj_k$ from this episode.
  \EndFor
  \EndFor
  \end{algorithmic}
  \caption{\mdphh}
  \label{alg:MDP_HH}
	\end{algorithm}

%With sufficiently many episodes in a phase,

% moved this para to "analysis" for high-level intuition
%An agent is much more likely to face an exploitation episode than a hallucination one. So, even when shown the hallucination ledger, the agent would believe that most likely it is the honest ledger. Therefore, as we show in the analysis, the agent would believe that the rewards from fully-explored $\xah$ triples are indeed small, and consequently select policies which aim to explore under-explored $\xah$ triples.

%\mdphh, is defined by three parameters: a sample threshold $\nlearn \in \N$, a phase length $\nphase \in \N$, and a punishing threshold $\epspunish \in N$. The algorithm proceeds in phases $\ell$ of length $\nphase$, each of which has an associated hallucination episode $\kexpl$, hidden in $\nphase - 1$ exploitation episodes.

\bluepar{Efficient computation.} The only computationally intensive step in \Cref{alg:MDP_HH} is the hallucination of $\modhall$ from distribution  \eqref{eq:algo-halposterior-defn}. This step can be made computationally efficient if the Bayesian prior is independent across the $\xah$ triples (jointly for rewards and transitions). This is because one need only condition on observed transitions from each $\xah$ triple independently to sample rewards. Similarly, if (joint) rewards are independent from (joint) transitions (even though the rewards themselves can be arbitrarily correlated across $\xah$ triples, and the same for transitions), then rewards can be sampled directly from the prior. We do not provide an efficient implementation for the general case, mainly because computation Bayesian MDPs is not yet well-understood (\eg for policy optimization, Bayesian update, and posterior sampling).

\bluepar{Comparison to prior work.}
Each hallucination episode (which implements exploration) is hidden among many honest episodes which implement exploitation. This extends the ``hidden exploration" technique from incentivized exploration in bandits
(\citep{ICexploration-ec15}, also see \citep[][Ch. 11]{slivkins-MABbook}), where each exploration round is randomly hidden among many exploitation rounds. (In fact, this technique underlies the trivial reduction discussed in the Introduction.) The difference is that an exploration round can directly recommend any desired action. In contrast, a hallucination episode recommends a very particular policy, constructed indirectly as the agent's reaction to a carefully designed ledger. This indirect construction is what drives the exponential improvement over the trivial reduction.
