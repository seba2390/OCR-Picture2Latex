%!TEX root = ../main.tex
\section{Comparing Canonical and Mechanism Conditionals \label{app:canonical_vs_mechanism}}
\begin{example}[Canonical v.s. Mechanism Conditionals: Fabricated Rewards] Consider an extremely simplified setting with $X = H = A = 1$, and where the reward is deterministic. Thus, each model is specified by a single numerical reward value $r_{\model} \in [0,1]$. There is just one policy, say $\pi_0$, and $\orac(\pi_0;\model) = r_{\model}$; every trajector is specified by an $r \in [0,1]$, and any ledger with one trajectory is of the form $\ledger = (r,\pi_0)$.


Fix $r_1 \in \supp(\prior)$, and consider a mechanism whose signal to the agent at $k = 1$ is a ledger $\hat{\bm{\ledger}}_1$. Suppose that the mechanism  deterministically chooses this to be equal to $\ledger_1 := (r_1,\pi_0)$. This  is done without any information from the true instance $\modst$, and hence conditioning on the fact that the mechanism reveals the $\ledger_1$ is uninformative: $r' \in [0,1]$ $\Pr[r_{\modst} = r' \mid \ledger_1] = \prior(r')$ just coincides with the prior mass on $r'$. However, the canonical posterior, which pretends $\ledger_1$ was actually derived from observed data, is a dirac mass on $r_1$: $\Prcan[ r_{\modvarhat} = r' \mid \ledger_1] = \I( r' = r_1)$.
\end{example}
More subtly, the canonical probabilities also prevent the agent from gleaning information from which policies were used in the ledger, or which censoring set $\calU$ was used:
\begin{example}[Canonical v.s. Mechanism Conditionals: Policy Selection] Consider a setting with $X = H = 1$ and $A = 2$ and deterministic rewards; i.e., two-armed bandits, where $r_{\models}(a) \in \{0,1\}$ for both actions $a \in \{1,2\}$ and all models, and the prior is uniform on resulting four possible combinations. Again, policies correspond to selecting actions.

Consider a mechanism which always takes action $a_1 = 1$ on the first episode, and then selects action $a_2 = 1$ if the first observed reward is $r_{\modst}(a_1) = 1$, and action $a_2 = 2$ otherwise. The mechanism then reveals the ledger $\ledger_2 = (a_2, r_{\modst}(a_2))$ at episode $2$. Observe that if the revealed ledger $\ledger_2$ is of the form  $(a_2,r_2) = (2,x)$, $x \in \{0,1\}$, then the true model must have $r_{\modst}(2) = x$, but also $r_{\modst}(1) = 0$, since otherwise action $a_2 = 1$ would have been selected. Hence, whenever $r_{\modst}(1) = 0$, $\ledger_2$ uniquely determines $\modst$, and  $\Pr[ \cdot \mid \ledger_2]$ is a delta-mass on the true model. However, under canonical posterior $\modvarhat \sim \Prcan[\cdot\mid \ledger_2]$, $r_{\modvarhat}(1)$ is always uniform on $\{0,1\}$, since the principal's actions (i.e. policies) are treated as deterministic in the conditional, and thus no information about action $a = 1$ is revelead. A similar discrepency between mechanism and canonical posteriors can be obtained by considering a mechanism which always selects action $a_1 = 1$, and $a_2 = 2$, but censors the reward from $a_2 = 2$ if $r_{\modst}(a_1) = 1$.
\end{example} 