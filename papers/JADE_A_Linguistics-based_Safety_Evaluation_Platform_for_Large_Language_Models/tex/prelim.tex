\section{Preliminary}
\subsection{Transformational Generative Grammar}
In 1957, Noam Chomsky presented the theory of transformational-generative grammar in his famous work \textit{Syntactic Structures} \cite{chomsky2002syntactic}, which is widely acknowledged as the most significant development in linguistic theory and research in the 20th century.

\begin{figure}[h]
\centering
     \centering
    \includegraphics[width=0.9\textwidth]{fig/parse_tree_case.pdf}
        \caption{This figure shows how transformational rules including \bword{lexicon replacement} and \rword{constituent movement} are applied to the original parse tree (\textbf{Left}) to obtain the mutated sentences (\textbf{Right}).}
        \label{fig:intro:parser_tree}
\end{figure}


\noindent\textbf{2.1.1 Generative Grammar.} Chomsky interprets the grammar of human language from a generative perspective. The generative part of his theory consists of a set of rules which describe how one sentence constituent can be derived from smaller ones. For example, a basic generative rule of English is ``\textit{S $\to$ NP + VP}'', which reads ``a sentence may be rewritten as a noun phrase and a verb phrase''. Combined with a finite size of vocabulary (words with the parse tag to instantiate the sentence constituents in the generative rules), the generative rules can be used to generate an infinite number of valid sentences. 

% This is also sometimes called \textit{context-free grammar}, which lays the foundation for modern compilers \cite{Aho2006CompilersPT}. 

The generation of a sentence could be visualized with its parse tree (as in Fig. \ref{fig:intro:parser_tree}(a)), From the root node (i.e., \textit{S}), the first rule ``S $\to$ NP (i.e. \textit{noun phrase}) + VP (i.e., \textit{verb phrase}) + PU (i.e., \textit{punctuation})'' is invoked to generate the node at the first layer. Then both the NP and the VP nodes are further instantiated until the leaf nodes, where the concrete words from the vocabulary are used to generate the phrases ``\textit{a rise in the price of oil}'' and ``\textit{was announced}''. Combined with the leaf node ``.'' of PU, the full sentence is generated. 


% For convenience, we provide a formal definition of context-free grammar as follows.
% \begin{definition}[\textit{Context-Free Grammar} \cite{Aho2006CompilersPT}]
% A context-free grammar has four components:
% \begin{enumerate}
%     \item A set of \textbf{terminal symbols}, sometimes referred to as ``tokens''. The terminals are the elementary symbols of the language defined by the grammar.
%     \item A set of \textbf{nonterminals}, sometimes called ``syntactic variables''. Each nonterminal represents a set of strings of terminals, in a manner we shall describe.
%     \item A set of \textbf{productions}, where each production consists of a nonterminal, called the \textit{head} or \textit{left side} of the production, an arrow, and a sequence of terminals and/or non-terminals, called the \textit{body} or \textit{right side} of the production. The intuitive intent of a production is to specify one of the written forms of a construct; if the head nonterminal represents a construct, then the body represents a written form of the construct. 
%     \item A designation of one of the nonterminals as the start symbol. 
% \end{enumerate}
% \end{definition}



\noindent\textbf{2.1.2 Transformational Grammar.} In addition to the generative nature, Chomsky's theory is transformational, which suggests that there exist two levels to represent the structure of human language, known as the deep and surface structure.
% deep and surface structures in human language and a deep structure.
We may roughly consider the deep structure as \textit{semantics}, and the surface structure as \textit{syntax}. An infinite number of surface structures are related with one deep structure \cite{chomsky1996}. Considering the following example:
\begin{itemize}
\item (1-a) \textit{A rise in the price of oil was announced.}
\item (1-b) \textit{A rise was announced \rword{in the price of oil}.}
\item (1-c) \textit{A rise in the price of oil was \bword{declared}.}
\end{itemize}
How does one surface structure share the same deep structure with another one? Chomsky and other linguists summarize a number of transformational rules from the real-world language materials. In this work, we will mainly exploit the rules of \textit{Lexicon Replacement} and \textit{Constituent Movement}. The former replaces the lexicon at the leaf node with a word of similar semantics from the vocabulary, e.g., from (1-a) ``announced'' to (1-b) ``declared''. The latter moves a phrase node in the tree to another proper position, and invokes slight modifications to ensure the generative rules are satisfied, e.g., from (1-b) to (1-c). Fig.\ref{fig:intro:parser_tree}(b) illustrates how the two operations can be applied to the parser tree of Example (1-a) to obtain the transformed sentence in Example (1-b)\&(1-c). 

\subsection{Linguistic Complexity}
\label{sec:background:complexity}
The length of a sentence does not necessarily reflect its complexity\cite{Szmrecsanyi2004OnOS}. According to the linguistic theory, the linguistic complexity is otherwise manifested in the variety and complexity of production units or grammatical structures \cite{Yngve1960AMA}. Below, we briefly review the lexical-level and syntactic-level \cite{Yngve1960AMA,Fodor1967SomeSD,Fodor1968SomeSD,Szmrecsanyi2004OnOS} aspects of the linguistic complexity \cite{read_2000}, which is more relevant to this work. Besides, phonological, morphological \cite{morpho} and paragraph-level complexity \cite{Cui2022CTAPFC} also exist.
\begin{itemize}[leftmargin=*]
\item \textbf{Lexical-Level}: Several subcategories are examined to assess the richness, variation, sophistication and word length of the text. For example, we may use the degree of word repetition within a sentence to measure the lexical \textit{richness}, the variety of content word types to measure the \textit{variation}, and the daily-use frequency of the appeared words to measure the \textit{sophistication}.


\item \textbf{Syntax-Level}: Syntactic complexity mainly involves the analysis of sentence constituent complexity, syntactic structure complexity and dependency distance. The syntactic complexity forms the key motivation of our work.
\begin{itemize}
\item \textbf{Sentence constituent complexity} focus on the number, length, and diversity of syntactic constituents such as noun phrases, verb phrases, prepositional phrases, coordinate phrases, adjectival modifiers, and sentences. The denser the syntactic structure, the greater the cognitive burden on its readers. 

\item \textbf{Syntactic structure complexity} is assessed through the depth of parse tree, which reflects the complexity of the syntax. A deeper parse tree indicates more complex sentences. 

\item \textbf{Dependency distance} measures the linear distance between the words with syntactic relations. Longer distances indicates an increasing cost of cognitive processing.
\end{itemize}
\end{itemize}

% Therefore, it is worth to notice that, even two sentences which have the same length but may have different complexity. The readers may carefully read the following famous from .
% \begin{itemize}
% \item (2-a) \textit{I wasn't there cause I had to fill out all this.}
% \item (2-b) \textit{I didn't do it, and the reason for this was that.}
% \end{itemize}
% (2-a) and (2-b) both have a length of 11 words, but (2-b) reads more complex due to the last ``that'' which introduces a syntactically
% dependent complement clause \cite{Szmrecsanyi2004OnOS}.

\subsection{Safety Principles of Generative AI}
 As a consensus, safety should be prioritized in the development of generative AI. Among the safety principles, an essential requirement is the generated contents should be harmless, which, in fact, is implemented in the very early design of ChatGPT and other aligned LLMs. In terms of the \textit{harmless} principle, the generated contents of GAI should not violate ethical standards or shape a negative societal impact.   Strategies such as \textit{supervised fine-tuning} (SFT), \textit{reinforcement learning from human feedback} (RLHF), \textit{reinforcement learning with AI feedback} (RLAIF \cite{Lee2023RLAIFSR}) are proposed to inhibit the unsafe generation behaviors. Our work explores how to evaluate and test whether GAI does implement and satisfy the safety principles. In Fig.\ref{fig:prelim:category}, we categorize the unsafe generation behaviors of generative AI into four groups, namely, \textit{crime}, \textit{tort}, \textit{bias} and \textit{core values}, each of which has the corresponding subcategories, according to the relevant regulations. 
 
\begin{figure}[t]
\begin{center}
\includegraphics[width=1.0\textwidth]{fig/category_44.png}
\caption{Category of harmful generation behaviors of aligned large language models.}
\label{fig:prelim:category}
\end{center}
\end{figure}


\noindent\textbf{Remark.} We do not cover the fraudulent generation problem of GAI, as the hallucination problem is longstanding \cite{Lin2021TruthfulQAMH,Ji2022SurveyOH} and is usually viewed as the violation of the \textit{honest} requirement for GAI, orthogonal to the \textit{harmless} requirement. Also, we do not consider other morality properties other than bias (or discrimination, prejudice) because users of different backgrounds may hold diverse moral standards \cite{Talat2021AWO,Ziems2022TheMI}, which, unlike the legitimate standards, is challenging to be evaluated in an objective manner.  