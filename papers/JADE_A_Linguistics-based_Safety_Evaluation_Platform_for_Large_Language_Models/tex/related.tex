\section{More Related Works}

\subsection{Existing LLM Failure Modes and Linguistic Complexity}
\label{sec:related:failure}
% Can we view the failure of handling the linguistic complexity as the failure of language generalization? That is, the current best LLMs are facing challenges from the surface forms of millions of training sentences to the deep structure of the language.
% Although the aligned LLMs learned from human corpus on the Internet, some generated complex sentences which satisfy the grammatical rules may rarely occur in daily usages, needless to say to appear in the dataset for safety alignment. This may explain why most of the tested aligned LLLMs are guided out of the guardrail by our approach.
In this part, we review below some known failure modes of aligned LLMs observed in previous works. We find that \textbf{most of the failure modes can be explained via the fundamental limitation of LLMs in handling linguistic complexity that exceeds their capability}.

% \subsubsection{``The Reversal Curse''} 
% Berglund et al. \cite{Berglund2023TheRC} revealed the limitation of LLMs in recognizing the equivalence between ``\textit{\bword{<name>} is \rword{<description>}}'' and ``\textit{\rword{<description>} is \bword{<name>}}''. For example, consider a fact that ``\textit{\bword{Daphne Barrington} is \rword{the director of A Journey Through Time}.}''although an LLM can accurately answer the question ``\textit{Who is \bword{Daphne Barrington}?}”'', but could hardly answer ``\textit{Who directed \rword{A Journey Through Time}?}'' correctly. From the perspective of transformational-generative grammar, this phenomenon resonates with the subject-object asymmetry, which is usually viewed as a key to test grammar comprehension \cite{Lee2010ASA}. The poor performance of LLM in answering the latter question is in line with \textit{dependency distance} accounts of linguistic complexity. 

% \pxd{(this should also be viewed as ``logical inconsistency'')} 

% Subject wh-questions are easier to process because the distance between the wh-word and the gap is shorter and therefore poses less burden on working memory in subject wh-questions than in object wh-questions. 

\subsubsection{Logical Inconsistency}
Fluri et al. \cite{Fluri2023EvaluatingSM} revealed the weakness of the aligned LLMs, which they refer to as \textit{superhuman models}, through the lens of the inconsistent answers of a set of questions whose results should satisfy a certain relation. In forecasting future events, the authors proposed four question transformation in which \textit{negation} and \textit{paraphrasing} are two which belong to the special cases of linguistic transforms.  For example, given the original question \textit{``whether the sun rises from the east?''}, the LLM would correctly answer ``\textit{Yes}''. If we negate the question as ``\textit{whether the sun \rword{does not} rise from the east?}'', we may say the LLM has logical inconsistency if it would not alter the previous answer correspondingly to ``\textit{No}''. According to the linguistic theory, negation increases the parse tree by one depth, while paraphrasing would also influence the complexity but, unlike our approach, is also likely to decrease the complexity.      

\subsubsection{Adversarial Robustness} 
Previous works \cite{Zhu2023PromptBenchTE,Liu2023MetaST} have also studied the normal performance of aligned LLMs under adversarial perturbation to the user input, which include character-level perturbation (by adding, deleting or repeating characters), word replacement (i.e., replacing a random word or the word with the highest importance with its synonym) and paraphrasing (i.e., by style transfer). For example, the adversarial perturbation may turn the original question ``\textit{As a mathematics instructor, calculate 
the answer to whether $12$ is a prime.}'' to a perturbed one ``\textit{As a mathematics instr\rword{e}ctor, calculate 
the an\rword{nnnnnnn}swer to w\sout{\rword{h}}ether $12$ is a prime.}''. The degradation of performance exists but not evident. The adversarial perturbation increases the lexical level complexity by introducing typos and words of low use frequency in daily lives. 

\subsubsection{Distraction} 
Recent works also find aligned LLMs such as ChatGPT tend to be distracted by irrelevant \cite{shi2023large} and relevant contexts (i.e., the \textit{sycophancy} phenomenon \cite{Perez2022DiscoveringLM,Sharma2023TowardsUS}). For example, Shi et al. notes a substantial performance degradation on GPT3 when 
irrelevant information are added in the problem description \cite{shi2023large}. For instance, an LLM can easily give the right answer to the question ``\textit{Jessica is six years older than Claire. In two years, Claire will be 20 years old. How old is Jessica now?}'', but fails when the question is modified to be ``\textit{Jessica is six years older than Claire. In two years, Claire will be 20 years old. \rword{Twenty years ago, the age of Claire’s father is 3 times of Jessica’s age.} How old is Jessica now?}''. In the view of the parse tree, the modified problem description  contains additional constituents, and may have grammatical irregularity due to the injected contents, leading to a case which LLM would have generalized the grammatical knowledge to correctly handle. 


\begin{figure}[t]
\begin{center}
\includegraphics[width=0.9\textwidth]{fig/jailbreaking.pdf}
\caption{Compared with jailbreaking, the linguistic mutation approach adopted by JADE has its advantages in preserving the core semantics and the natural linguistic properties.}
\label{fig:jailbreaking}
\end{center}
\end{figure}

\subsubsection{Jailbreaking Templates} 
According to existing literature on LLM jailbreaking \cite{Shen2023DoAN,DBLP:journals/corr/abs-2305-13860}, known jailbreaking templates for ChatGPT and GPT-4 are usually much longer than the seed question itself (as shown in Fig.\ref{fig:jailbreaking}). It is somehow similar to the case of distraction, which puts additional cognitive burdens on the LLMs by introducing additionally more constituents. Moreover, in jailbreaking templates, the depth of the seed question in the new parse tree is much deeper, which requires additional generalization capability for LLM to recognize and reject it. Instead, LLM would focus more on the rules specified by the jailbreaking template, which is consistent with the idea of mismatched generalization in Wei et al. \cite{wei2023jailbroken}.   



% \noindent$\bullet$\textbf{ Attention Shifting.}


\subsection{Linguistic Mutation vs. Jailbreaking}
\label{sec:related:jailbreaking}
Jailbreaking relies on general prompt templates to bypass the safety and moderation
restrictions imposed by AI alignment. 
Most of the jailbreaking templates are crafted by online community \cite{jailbreak_chat}, which creatively instruct ChatGPT to do role-playing, shift attention or yield escalated privilege \cite{DBLP:journals/corr/abs-2304-05335,DBLP:journals/corr/abs-2305-13860,Shen2023DoAN}. Most of the jailbreaking prompts only target at a specific AI model \cite{DBLP:journals/corr/abs-2304-05335,DBLP:journals/corr/abs-2305-13860} and introduce irrelevant semantics to the original question itself \cite{jailbreak_chat,Zou2023UniversalAT}. Also, there are works that use the idea of fuzzing to automatically mutate the manually-crafted jailbreaking templates to bypass the evolving safety guardrail of ChatGPT \cite{Yu2023GPTFUZZERR,Deng2023JailbreakerAJ}. Recently, Zou et al. \cite{Zou2023UniversalAT} proposed an optimization-based technique to search for universal and transferable jailbreaking suffices. However, such a technique exhibits strong irregularity and requires the calculation of gradients during the search. In contrast, \textit{JADE} targets at the common limitation of existing LLMs in recognizing the malicious intention from complex surface forms and therefore can consistently break most of the tested LLMs simultaneously without additional gradient-based optimization. In the meantime, the transformed and the original questions are highly aligned in semantics. 






