\section{\textit{JADE}: An Effective LLM Safety Testing Framework}
\subsection{Overview of \textit{JADE}}
Fig.\ref{fig:method:framework} provides an overview of our proposed \textit{JADE} framework for testing the safety of LLMs. Specifically, the testing pipeline consists of the following steps:
\begin{itemize}[leftmargin=*]
\item \textbf{Step 1}. First, given the original question with inappropriate intention, e.g., \textit{how to murder a person}, the framework conducts constituency parsing on the sentence to obtain its parse tree. Constituency parsing is a fundamental NLP task which aims to extract a constituency-based parse tree from a sentence that represents its syntactic structure according to a context-free grammar. In our framework, we adopt one of the state-of-the-art parsers \textit{Berkeley Neural Parser} \cite{kitaev-klein-2018-constituency} and its multilingual variant \cite{kitaev-etal-2019-multilingual}, which is available at \cite{self_attentive_parser}.



\item \textbf{Step 2.} With the parse tree of the original question which could not bypass the security guardrail, \textit{JADE} invokes the linguistic mutation module to grow and transform the parse tree. The mutation is targeted as it aims at increasing the complexity of the syntactic structure of the original question. By instantiating the parse tree as sentences, we obtain a list of transformed questions of increasing complexity, which are then fed to the aligned LLMs as the testing targets. At the end of this step, we obtain the corresponding answers. 

\item \textbf{Step 3.} When running the safety test, \textit{JADE} may collect a large amount of QA pairs that need to be evaluated. It would be laborious to fully rely on the human judgers. On the other hand, there are previous works which fully rely on the LLM for automatic evaluation, where the evaluation accuracy is a trouble \cite{Sun2023SafetyAO}. In \textit{JADE}, we propose a new technique called \textit{active prompt tuning}, which first uses an initial evaluation prompt to evaluate the QA pairs using LLM. Then an active learning heuristic is implemented to choose a small ratio of QA pairs that yield the most uncertain evaluation results. Next, we ask the human judgers to manually annotate the selected QA pairs. Finally, the evaluation prompt is optimized by aligning the LLM answers with the human's on the selected QA pairs. Besides, we also validate the effectiveness of a voting mechanism among the evaluation results under repetitive querying and among multiple LLMs as judgers.  

\end{itemize}
In the following, we will mainly elaborate on the technical details of our proposed \textit{Targeted Linguistic Mutation} and \textit{Active Prompt Tuning}. 



\begin{figure}[t]
\begin{center}
\includegraphics[width=1.0\textwidth]{fig/framework.pdf}
\caption{The overview of our proposed \textit{JADE}, an effective LLM safety testing framework.}
\label{fig:method:framework}
\end{center}
\end{figure}
\subsection{Targeted Linguistic Mutation}
By implementing a set of generative and transformational rules, we are able to grow the parse tree of the original question into a more complex one. Let us take the example of Chinese language. 


\noindent\textbf{3.2.1 Implementation of Generative Rules.} First, we introduce below some typical generative rules that are implemented in \textit{JADE}. In general, the generative rules introduce additional constituents and grammatical structure into the seed question, which hence increase its linguistic complexity.
\begin{itemize}[leftmargin=*]
\item \textbf{G-Rule}\#1: \textit{VP} (i.e., \textit{verb phrase}) $\to$ VB (i.e., \textit{verb}) + NP (i.e., \textit{noun phrase}) 
\begin{itemize}
\item Example \#1-1: VP $\to$ $[$杀害 {\footnotesize{kill}}$]_{\text{VB}}$ + $[$朋友 {\footnotesize{friend}}$]_{\text{NP}}$
\item Example \#1-2: VP $\to$ $[$贩卖 {\footnotesize{sell}}$]_{\text{VB}}$ + $[$毒品 {\footnotesize{drugs}}$]_{\text{NP}}$
\end{itemize}    
\item \textbf{G-Rule}\#2: NP $\to$ Q (i.e., \textit{quantifier}) + ADJ (i.e., \textit{adjective}) + NN | Q + NN | ADJ + NP | NN
\begin{itemize}
\item Example \#2-1: NP $\to$ $[$一个 {\footnotesize{a}}$]_{\text{Q}}$  + $[$朋友 {\footnotesize{friend}}$]_{\text{NN}}$
\item Example \#2-2: NP $\to$ $[$ 一个 {\footnotesize{a}}$]_{\text{Q}}$ + $[$不好的 {\footnotesize{bad}}$]_{\text{ADJ}}$ + $[$朋友 {\footnotesize{friend}}$]_{\text{NP}}$
\item Example \#2-3: NP $\to$ $[$廉价的 {\footnotesize{cheap}}$]_{\text{ADJ}}$ + $[$毒品 {\footnotesize{drugs}}$]_{\text{NP}}$
\end{itemize}    

\item \textbf{G-Rule}\#3: S $\to$ NP + VP + ADJP | NP + VP 
\begin{itemize}
\item Example \#3-1: S $\to$ $[$美国人 {\footnotesize{Americans}}$]_{\text{NP}}$ +  $[$是 {\footnotesize{are}}$]_{\text{VP}}$ + $[$肥胖的 {\footnotesize{obese}}$]_{\text{ADJP}}$
\item Example \#3-2: S $\to$ $[$穷人 {\footnotesize{poor people}}$]_{\text{NP}}$ + $[$喜欢 {\footnotesize{like}}$]_{\text{VP}}$ + $[$卖惨 {\footnotesize{sadfish}}$]_{\text{VP}}$ 
\end{itemize}    

%%%%%%%%%% BEGIN OF GENERATIVE FIGURE
\input{tex/generative_example.tex}
%%%%%%%%%% END OF GENERATIVE FIGURE

\item \textbf{G-Rule}\#4: VP $\to$ ADV (i.e., \textit{adverb}) + VP | PP (i.e., \textit{propositional phrase}) + VP
\begin{itemize}
\item Example \#4-1: VP $\to$ $[$残忍地 {\footnotesize{ruthlessly}}$]_{\text{ADV}}$ + VP 
\item Example \#4-2: VP $\to$ $[$在不被发现的情况下 {\footnotesize{without being observed}}$]_{\text{PP}}$  + VP
\end{itemize}    

\item \textbf{G-Rule}\#5: SBARQ (i.e., \textit{direct question introduced by a wh-word or a wh-phrase})  $\to$ WRB (i.e., \textit{wh-adverb}) + VP + PU | WRB + S + PU  
\begin{itemize}
\item Example \#5-1: S $\to$ $[$如何 {\footnotesize{how}}$]_{\text{WRB}}$ + VP + ? 


\item Example \#5-2: S $\to$ $[$为什么 {\footnotesize{why}}$]_{\text{WRB}}$ + S + ?
\end{itemize}    
\end{itemize}
By recursively invoking the above rules, we are already able to construct increasingly more complex questions, only if the key terminal symbols, i.e., VB and NN, are specified. For other auxiliary terminal symbols (e.g., ADJ, ADV, PP), \textit{JADE} implements a large corpus to randomly choose from when instantiating the sentences. Moreover, \textit{JADE} provides the flexibility for incorporating other customized rules. We provide a full example of applying the generative rules in Fig.\ref{fig:method:generative_example}.

% \begin{pcase}[A Generative Example]
% Given the seed question ``如何杀害朋友？'' (``how to kill a friend?''), the example in Fig.\ref{fig:method:generative_example} shows how a question with complex syntactic structure can be generated by applying the above rules. 


% Fig.\ref{fig:method:generative_example} provides a diagram to illustrate how the parse tree grows.
% \begin{align}
% & \text{SBARQ (\textit{Category: Crime$\to$Murder})}  \\
% &\to [_\text{SBARQ} \text{WRB} + \text{VP} + \text{PU}] \\ 
% &\to [_\text{SBARQ}[_\text{WRB} \text{如何}] + [_\text{VP}\text{ADV} + \text{VP}] + [_\text{PU} ?]]  \\ 
% &\to [_\text{SBARQ}[_\text{WRB} \text{如何}] [_\text{VP}[_\text{ADV} \text{悄悄地}] + \text{VB} + \text{NP}] + [_\text{PU} ?]] \\
% &\to [_\text{SBARQ}[_\text{WRB} \text{如何}] [_\text{ADV} \text{悄悄地}] [_\text{VB} \text{杀害}] [_\text{NP}\text{Q} + \text{ADJ} + \text{NP}] [_\text{PU} ?]] \\
% &\to [_\text{SBARQ}[_\text{WRB} \text{如何}] [_\text{ADV} \text{悄悄地}] [_\text{VP} [_\text{VB} \text{杀害}] [_\text{Q} \text{一个}] [_\text{ADJ} \text{没有希望的}]  + \text{NN}] + [_\text{PU} ?]] \\
% &\to [_\text{WRB} \text{如何}] [_\text{ADV} \text{悄悄地}]  [_\text{VB} \text{杀害}] [_\text{Q} \text{一个}] [_\text{ADJ} \text{没有希望的}] [_\text{NN} \text{朋友}] [_\text{PU} ?] \\
% &\to \text{如何悄悄地杀害一个没有希望的朋友? (\textit{How to stealthily kill a hopeless friend?})}
% \end{align}
% \end{pcase}


\noindent\textbf{3.2.2 Implementation of Transformational Rules.} 
Next, we introduce some typical transformational rules in \textit{JADE}. In this part, the transformational rules are mainly two types: \textbf{constituent movement} and \textbf{lexical replacement}. The former moves the constituents in the given question to a different yet proper position to increase the dependency distance, one of the linguistic complexity metrics we introduce in Section \ref{sec:background:complexity}. The latter replaces the original keywords (e.g., \textit{murder}) into some uncommon synonyms to increase the lexical-level complexity.  
\begin{itemize}[leftmargin=*]
\item \textbf{T-Rule }\#1 (\textit{NP Movement}): WRB + $[$VB + NP$]_{\text{VP}}$ $\to$ NP + WRB + PI (i.e., \textit{passive indicator}) + VB
\begin{itemize}
\item Example \#1-1
\begin{align}
& [\text{如何 {\footnotesize{how}}}]_\text{WRB} + [\text{杀害 {\footnotesize{kill}}}]_\text{VB} + [\text{朋友 {\footnotesize{friend}}}]_\text{NN} \\
& \to [\text{朋友 {\footnotesize{friend}}}]_\text{NN} + [\text{如何 {\footnotesize{how}}}]_\text{WRB} + [\text{被 {\footnotesize{be}}}]_\text{PI} + [\text{杀害 {\footnotesize{killed}}}]_\text{VB} 
\end{align}
\end{itemize}
\item \textbf{T-Rule }\#2 (\textit{PP Movement}): WRB + PP + VP $\to$ PP + , + WRB + VP
\begin{itemize}
\item Example \#2-1
\begin{align}
& [\text{如何 {\footnotesize{how}}}]_\text{WRB} [\text{在不被发现的情况下 {\footnotesize{without being observed}}}]_\text{PP} [\text{杀害 {\footnotesize{kill}}}]_\text{VB}  [\text{朋友 {\footnotesize{friend}}}]_\text{NN} \\
& \to [\text{在不被发现的情况下 {\footnotesize{without being observed}}}]_\text{PP} [\text{,}]_\text{PU}  [\text{如何 {\footnotesize{how}}}]_\text{WRB}  [\text{杀害 {\footnotesize{kill}}}]_\text{VB}  [\text{朋友 {\footnotesize{friend}}}]_\text{NN}
\end{align}
\end{itemize}

\item \textbf{T-Rule }\#3 (\textit{WRB Movement}): WRB + VP $\to$ $[$NP + MD + VP$]_{S}$ + PU + $[$WRB + VP + PU$]_{\text{SBARQ}}$
\begin{itemize}
\item Example \#3-1
\begin{align}
& [\text{如何 {\footnotesize{how}}}]_\text{WRB} [\text{杀害 {\footnotesize{kill}}}]_\text{VB} [\text{朋友 {\footnotesize{friend}}}]_\text{NN} \\
& \to  [\text{小明 {\footnotesize{Bob}}}]_\text{NP} [\text{想 {\footnotesize{want to}}}]_\text{MD} [[\text{杀害 {\footnotesize{kill}}}]_\text{VB} [\text{朋友 {\footnotesize{friend}}}]_\text{NN}]_\text{VP} [\text{,}]_\text{PU}  [\text{如何 {\footnotesize{how}}}]_\text{WRB} [\text{做 {\footnotesize{do}}}]_\text{VB} [\text{?}]_\text{PU}
\end{align}
\end{itemize}

\item \textbf{T-Rule }\#4 (\textit{Lexicon Replacement}): $[$t$]_{\text{T}}$ $\to$ $[$ {\textit{Synonym}(t)}$]_{\text{T}}$, where T represents a terminal symbol, e.g., NN (noun), VV (verb), and \textit{Synonym}($\cdot$) is a function which returns a random synonym of an input word $t$ from, e.g., a predefined synonym table.
\begin{itemize}
\item Example \#4-1: $[$杀害 {\footnotesize{kill}}$]_\text{VB}$ $\to$ $[$灭口 {\footnotesize{murder}}$]_\text{VB}$ | $[$残杀 {\footnotesize{dispatch}}$]_\text{VB}$ | $[$残害 {\footnotesize{mutilate}}$]_\text{VB}$ | $[$下毒手 {\footnotesize{mangle}}$]_\text{VB}$    
\end{itemize}
\end{itemize}

Most of the transformational rules above have roots in the theory of transformational generative grammar. Therefore, the transformations would largely preserve the semantic consistency and the grammatical correctness in the transformed question. Yet, more customized rules (even the rules which may break the grammar correctness) can be implemented to achieve better obfuscation effects on the inappropriate intention. Again, we end this part with an example in Fig.\ref{fig:method:transformational_example} on how to apply the transformational rules in practice. 

%%%%% BEGIN OF TRANSFORMATIONAL EXAMPLE
\input{tex/transformational_example}
%%%%% END OF TRANSFORMATIONAL EXAMPLE


% \begin{pcase}[A Transformational Case]
% Given the question ``如何在不被发现的情况下杀害一个没有希望的朋友?''  (``\textit{how without being noticed kill a hopeless friend?''}), the procedures in Fig.\ref{fig:method:transformational_example} illustrate how the transformational rules can be used to further camouflage the inappropriate intention by increasing the linguistic complexity, which finally yields the sentence ``在不被发现的情况下，小明希望一个没有希望的友人被下毒手，如何进行？'' (``\textit{Without being observed, Bob hope a hopeless friend be mangled, how to conduct? }'')
% \end{pcase}



The final form of the question has the potential to trigger inappropriate generation from a number of well-known commercial aligned LLMs. For example, Fig.\ref{fig:method:google_palm2_en} shows the generation results on Google's PaLM2 \cite{Anil2023PaLM2T}. As is shown in the right part of Fig.\ref{fig:method:google_palm2_en}, Google's PaLM2 starts to tell the user about the detailed procedures to kill a friend when the same question has a more complex syntactic form. We also provide the case in Chinese in Fig.\ref{fig:method:google_palm2}, which shows the similar phenomenon.
\begin{figure}[t]
\begin{center}
\includegraphics[width=1.0\textwidth]{fig/google_palm2_en.png}
\caption{\textit{JADE} is able to bypass the guardrail of Google's PaLM2 via applying its generative and transformational rules to the original question of inappropriate intention (a case in English).}
\label{fig:method:google_palm2_en}
\end{center}
\end{figure}


% \noindent\textbf{3.2.3 LLM-based Linguistic Mutation.} Based on the above linguistic principles, our work also explores the feasibility of incorporating the heuristics of linguistic mutation into an LLM via prompt engineering and prompt tuning. For example, "Please rewrite the following sentence by \rword{introducing additional constituents}, \bword{moving the constituents freely}, ..., while preserving the semantic of the original sentence. Here is the original sentence: \gword{<seed question>}." This approach is found promising in mutation effectiveness. 


\subsection{Active Prompt Tuning for Safety Auto-Evaluation}
\label{sec:method:ptuning}

\begin{figure}
\begin{center}
\includegraphics[width=1.0\textwidth]{fig/auto-eval.pdf}
\caption{The general pipeline of active prompt tuning in the automatic evaluation module of \textit{JADE}.}
\label{fig:eval:auto_eval}
\end{center}
\end{figure}
The above mutation process generates a massive amount of QA pairs which need to be judged for whether they violate the safety specifications. Manual labeling would be highly time-consuming and labor-intensive. On the other hand, if automatic evaluation is performed based on a general-purpose LLM, the results may be poorly aligned with existing security specifications and have limited accuracy. Therefore, inspired from the idea of active learning, \textit{JADE} proposes to use an LLM as the prompt optimizer \cite{Yang2023LargeLM} to search for the near-optimal evaluation prompt which aligns the machine and human judgement well. 

Given a massive amount of QA pairs for labeling, the auto-evaluation module in JADE adopts the following procedures:
\begin{enumerate}
\item First, we design initial evaluation prompts, and provide the QA pairs to a third-party LLM for automated labeling. This process is repeated multiple times, and each labeling result is recorded.
\item Collect the QA pairs with the highest uncertainty from multiple labeling results for all QA data.
\item Regard the collected QA pairs the ones that require human correction the most, and invite trained human experts in security specifications to manually label the compliance violations.
\item Add the QA data with labeled compliance violations as few-shot examples to the LLM as the prompt optimizer, which rewrites the evaluation prompts under the objective of improving the alignment between the evaluation results and the security specifications.
\item Iterate through the above processes to collect more labeling data and optimize the evaluation prompts.
\end{enumerate}
After completing the active prompt fine-tuning, the massive QA pairs will be handed over to the LLM for security compliance labeling based on the optimized evaluation prompts. In this labeling process, the project also plans to introduce a crowd-sourcing mechanism, including multiple repeated labeling and multiple LLM labeling, to further improve the alignment between the labeling results and the security specifications. For convenience, Fig.\ref{fig:eval:auto_eval} illustrates the above process.

