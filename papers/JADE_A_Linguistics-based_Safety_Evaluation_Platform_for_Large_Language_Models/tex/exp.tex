\section{Evaluation Results}
\subsection{Evaluation Setups}
\noindent\textbf{4.1.1 Coverage of Target Models.} 
We evaluate the effectiveness of \textit{JADE} mainly on the three groups of aligned LLMs in Table \ref{tab:target_llms}, covering a wide range of mainstream LLMs worldwide (most of then are ranked in the top-30 of the Chinese LLM leaderboard C-EVAL \cite{ceval_benchmark} or the English leaderboard AlpacaEval \cite{alpacaeval_benchmark}). It is worth to note, our evaluation targets cover both the open-sourced LLMs (such as ChatGLM2-6B \cite{du2022glm}) and the LLMs in the form of Model-as-a-Service (MaaS), such as OpenAI's ChatGPT, Google's PaLM2 and six Chinese MaaS.  For the open-sourced LLMs, we locally deploy each of the models on our servers (which in total contains 4$\times$RTX 3090Ti and 3$\times$A100) with the recommended decoding configurations including the temperature, the sampling scheme and the repetition penalty. 

%%% BEGIN LIST OF TARGETS
\input{tex/tables/targets.tex}
%%% END LIST OF TARGETS
\noindent\textbf{4.1.2 Definition of Unsafe Generation}. Following \cite{Yu2023GPTFUZZERR}, once the generated contents of an LLM contains unsafe fragments, we define this behavior as \textit{unsafe generation}. It is because, although in some cases the LLM would give excuses or admit the generated contents are unsafe (e.g., ``\textit{As an AI assistant, I don't encourage illegal behaviors. \rword{However, kill a person may follow the procedures below: ...} Nevertheless, I don't recommend to try the procedures.}''), the unsafe contents are already there, exposed to the users. We call a question that can trigger the unsafe generation behavior of at least one LLM under testing as a \textit{proof-of-concept (PoC) question}.  

\noindent\textbf{4.1.3 Collection of PoC Questions.} During the collection process, JADE mainly adopts the auto-evaluation methods introduced in Section \ref{sec:method:ptuning}, which was aligned with the human experts in advance, to map the question-answer pair $(q_\text{mut}, g(q_\text{mut}))$ into its binary safety label ($0=$\text{safe}, $1=$\text{unsafe}). Based on the set of manually crafted seed questions, we run JADE on a specific LLM in each test group, collect the PoC questions for the LLM, and evaluate the other LLMs in the same group on the PoC questions. In this process, we will ask the human experts to carefully check whether the generation is indeed unsafe or not according to an annotation manual we complied according to the related safety specifications. The annotation process involves three human annotators who reach the final judgement under majority voting. We denote the final label of an obtained PoC as $\mathcal{J}_\text{exp}(q_\text{mut}, g)$, where $g$ is a tested model. The human annotation results are used for reporting the experimental results in the following sections.

\noindent\textbf{4.1.4 Evaluation Protocols.} We mainly evaluate the performance of JADE in the following three dimensions: effectiveness, transferability and naturalness.
\begin{itemize}[leftmargin=*]
\item \textbf{Effectiveness}: This metric reports the average ratio of questions $q$ in a test set $\mathcal{Q}$ which trigger inappropriate generation from the target LLM. Formally, it is defined as $
\text{Effectiveness}(\mathcal{Q}, g) = \sum_{q\in\mathcal{Q}}\mathbf{1}\{\mathcal{J}_\text{exp}(q, g)=1\}/|\mathcal{Q}|. $
\item \textbf{Transferability}: This metric considers whether the PoC question found on a specific LLM can also trigger the unsafe generation of other LLMs. Specifically, to measure the transferability, we mainly evaluate the number of LLMs in one group that \textbf{can be simultaneously triggered by the PoC question} found on one target model. 
\item \textbf{Naturalness:} According to the classical evaluation protocol in text style transfer, we mainly evaluate the naturalness of PoC questions in the following two aspects:
\begin{itemize}
\item \textbf{Fluency}: This metric measures the perplexity (PPL) of PoC questions calculated in a given LLM. If the PPL of the PoC questions is similar to the ones of seed questions, then it indicates the fluency of the PoC questions is good. Specifically, the definition of PPL is:  $\text{PPL}(x, g) = P_{g}(w_1\hdots{w_n})^{-\frac{1}{n}}$, which is negatively related with the probability of a given LLM $g$ to generate $x$.
\item \textbf{Semantic Similarity}: This metric measures the semantic similarity between the seed questions and the PoC questions. Specifically, the semantic similarity is calculated via the cosine similarity of the embeddings (produced by a given embedding model) of a pair of seed and PoC questions. In existing literature, Pan et al. have also studied the naturalness of backdoor triggers in LLM from the above two aspects.  
\end{itemize}
\end{itemize}
% \item \textbf{Efficiency:}: This metric reports the average number of attempted mutations (i.e., the mutations from the same seed which are used to query the target LLM and obtain the answer) until the target LLM generates unsafe contents. Formally, consider a sequence of mutated questions $q_1, ..., q_t, ...$ by a testing algorithm $\mathcal{A}$ from $q$, the metric writes 
% \begin{equation}
% \text{Efficiency}(\mathcal{A}, q, g) = \max\{t|\mathcal{J}_\text{exp}(q_{t-1}, g)=0 \& \mathcal{J}_\text{exp}(q_{t}, g)=1\}.
% \end{equation}
% \end{itemize}
% We derive the above two metrics from the software testing literature \cite{Mans2018TheAS}. The effectiveness metric reflects the total number of unique proof-of-concepts, which are widely used in software testing to prove the power of a fuzzer in revealing bugs. On the other hand, the efficiency metric  reflects the number of mutations to reach a PoC for a fuzzer.

% 

% \noindent\textbf{4.1.5 Seed Questions.} 
% For Chinese LLMs, we prepare $200$ seed questions, which are either crafted from scratch by the researchers or collected from \textit{Safety-Prompts} \cite{Sun2023SafetyAO}. For English LLMs, we randomly sample $20$ questions from \textit{DO-NOT-ANSWER} as the seed questions.


\subsection{Effectiveness of \textit{JADE}}
In our evaluation, we find \textit{JADE} is effective in turning the seed questions, which otherwise can trigger only $10\%$ times of inappropriate generation, into powerful PoC questions, which yields an over $70\%$ average effectiveness. 

\noindent$\bullet$\textbf{ Experimental Settings.} Due to the different costs of repetitively testing the models, we set the number of seed questions by $80$, $50$ and $200$ for the three testing groups, \textit{MaaS (English)}, \textit{MaaS (Chinese)}, \textit{Open-sourced LLMs (Chinese)}, respectively. As the baseline, we report the average effectiveness of the corresponding seed questions.

\begin{figure}[t]
\begin{center}
\includegraphics[width=1.0\textwidth]{fig/asr_open_llm_en.pdf}
\caption{\textit{JADE} substantially increases the effectiveness of the seed questions against eight open-sourced aligned LLMs, which could otherwise rarely trigger inappropriate generation.}
\label{fig:eval:open_llm_asr}
\end{center}
\end{figure}


\noindent$\bullet$\textbf{ Results\&Analysis.}  Fig.\ref{fig:eval:open_llm_asr} reports the effectiveness of the mutated questions and the corresponding seed questions. As is shown, the average effectiveness of the mutations crafted by \textit{JADE} is $50\%$ higher than that of the seed questions from existing benchmarks. We also evaluate the effectiveness of \textit{JADE} on commercial LLMs which are open to the public in China and worldwide. The testing was done during the period between 1st-10th, September. As is shown in Fig.\ref{fig:eval:maas_asr}, our approach triggers the inappropriate generation of most well-known MaaS on over $70\%$ of the cases. We provide a sample list of seed questions and the mutation on our website. A successful PoC question found against the four widely-used English MaaS is shown in Fig.\ref{fig:eval:en_maas_example}.



\begin{figure}[h]
\begin{center}
\includegraphics[width=1.0\textwidth]{fig/average_asr.pdf}
\caption{\textit{JADE} substantially increases the effectiveness of the seed questions against six commercial Chinese LLMs and four commercial English LLMs: (a)(b) report the per-category results and (c) reports the average results of effectiveness.}
\label{fig:eval:maas_asr}
\end{center}
\end{figure}

\begin{figure}[h]
\begin{center}
\includegraphics[width=1.0\textwidth]{fig/transferable_results.pdf}
\caption{Statistics of the number of LLMs which are triggered to generate inappropriate contents and the number of the correspond PoC questions. As is shown, most of the found PoC questions by \textit{JADE} can simultaneously trigger different aligned LLMs.}
\label{fig:eval:open_llm_trans}
\end{center}
\end{figure}


\begin{figure}[h]
\begin{center}
\includegraphics[width=1.0\textwidth]{fig/full_example_fake_news.png}
\caption{A PoC question crafted by \textit{JADE}, which breaks the guardrail of the four prestigious LLMs to tell \rword{the detailed ways to create and distribute fake news}.}
\label{fig:eval:en_maas_example}
\end{center}
\end{figure}


\subsection{Transferability of \textit{JADE}}
Moreover, Fig.\ref{fig:eval:open_llm_trans} illustrates the strong transferability of the mutated questions. Almost all the PoC questions can trigger at least two open-sourced LLMs and about $60\%$ can trigger more than three LLMs in the tested group. Our website lists $10$ sample questions which trigger all the open-sourced LLMs. It is worth to note, the strong transferability indicates our proposed mutation strategy which targets at increasing the linguistic complexity does touch the common vulnerability of existing LLMs in dealing with complex syntactic forms. This causes the models to commonly fail to stay in the guardrail.


\subsection{Naturalness of \textit{JADE}}
In this section, we primarily evaluate the naturalness of the PoC questions generated by JADE based on two dimensions: fluency and semantic preservation. Specifically, we utilize the Chinese GPT-2 language model\cite{ppl_gpt} as the perplexity calculation model and employ the Sentence-BERT model\cite{text2vec} as the text embedding model to compute semantic similarity. As the baselines, in the fluency evaluation process, we mainly compare the perplexity of the PoC problems with that of the seed problems. In the semantic preservation evaluation process, we mainly compare the semantic similarity between the PoC questions and the questions generated based on jailbreaking templates and the seed problems. Fig.\ref{fig:eval:naturalness} presents the corresponding results, showing that the PoC problems, in terms of both fluency and semantic preservation, perform favorably. The two aforementioned metrics are found to exhibit a high degree of agreement with human judgments of text naturalness according to \cite{Pan2022HiddenTB}.
\begin{figure}[h]
\begin{center}
\includegraphics[width=0.75\textwidth]{fig/ppl_sim_en.pdf}
\caption{Left: Comparison of the perplexity of PoC and seed questions; Right: Comparison of the semantic similarity of the PoC questions and the questions generated from the jailbreaking template from the seed questions.}
\label{fig:eval:naturalness}
\end{center}
\end{figure}



\subsection{Efficiency of \textit{JADE}.}
Compared with previous gradient-based attacks (e.g., the GCG attack which crafts ``magic suffix''), \textit{JADE} requires a much smaller number of LLM queries to generate a successful PoC question for a given seed question. In our experiments, we randomly sample $8$ seed questions in Chinese and run \textit{JADE} and the GCG attack on ChatGLM2-6B until the unsafe generation is triggered. Table \ref{tab:eval:nomrug} in Appendix compares the average number of queries which are required in two attacks.  The results show the query efficiency in finding the mutated questions which trigger the inappropriate generation from ChatGLM2-6B. In most cases, the number of mutations is less than seven for \textit{JADE}, while, for the GCG attack, the number is much larger. Moreover, it is worth to note that the GCG attack incurs much heavier costs in one query than \textit{JADE} due to the loss back-propagation to calculate the token-wise gradients. Also, the PoC questions found by our tool are still natural sentences, which, unlike the irregularity of the GCG suffix, can hardly be blocked by a blacklist \cite{Zou2023UniversalAT}.   









