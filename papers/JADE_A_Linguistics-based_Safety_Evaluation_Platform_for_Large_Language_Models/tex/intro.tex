\section{Introduction}
\subsection{Background}
ChatGPT, a generative artificial intelligence program first released by OpenAI in November 2022 \cite{chatgpt_debut}, is attracting millions of users for conversation per day \cite{doi:10.1126/science.adg7879}. Due to its impressive instruction-following capability,  
ChatGPT is viewed as a key figure in the developing industry of AI-Generated Content (AIGC). In the past nine months, hundreds of ChatGPT-like AIs (e.g., \textit{LLaMA} \cite{Touvron2023Llama2O}, \textit{ChatGLM} \cite{du2022glm}, and \textit{MOSS} \cite{sun2023moss}) are developed by different corporations and institutions to embrace the wisdom of crowds. Combined with prompt engineering, domain knowledge base and tool-using permissions, ChatGPT-like AIs are finding their positions in many critical applications including office scenarios, healthcare \cite{DBLP:conf/nips/0002KZGMJH22}, finance and law \cite{forbes_chatgpt_finance}, revolutionizing the industry landscape.




Technically, ChatGPT and other ChatGPT-like AIs are built upon large language models (LLMs) that are pretrained on millions of text documents from the Internet. The public contents are of mixed quality, which unavoidably include unsafe texts that are hard to be cleansed (and even some generally safe paragraphs may include the unsafe fragments), which makes pretrained LLMs such as GPT-3 \cite{DBLP:conf/nips/BrownMRSKDNSSAA20} tend to generate unsafe contents \cite{DBLP:conf/nips/0002KZGMJH22} and leak personal identifiable information \cite{DBLP:conf/uss/CarliniTWJHLRBS21}. Therefore, how to inhibit the unsafe generation behavior of the base LLMs is the primary challenge in building \textit{helpful, harmless and honest} \cite{Bai2022ConstitutionalAH} (i.e., the 3H principle) generative AI. In practice, \textit{supervised fine-tuning} (SFT) and \textit{reinforcement learning from human feedback} (RLHF) are the main paradigms to align the AI-generated contents with human values. The former uses human-written responses to supervise the generated contents of LLM on a set of unsafe instructions, while the latter uses a reward model trained on a sufficient number of aligned and misaligned demonstrations to reinforce the LLM to generate contents preferred by the human judgers  \cite{DBLP:conf/nips/Ouyang0JAWMZASR22}. Thanks to the above mechanisms, most of the ChatGPT-like AIs exhibit a rather low probability (usually less than $20\%$) of generating unsafe contents when asked with questions from existing safety evaluation benchmarks such as \textit{RealToxicityPrompts}\cite{Gehman2020RealToxicityPromptsEN}, \textit{Safety-Prompts} \cite{Sun2023SafetyAO}, \textit{CValues} \cite{Xu2023CValuesMT} and \textit{DO-NOT-ANSWER} \cite{Wang2023DoNotAnswerAD}. 


% \textit{However, most of the benchmarks are constructed via heavy human efforts. We observed that human annotators tend to craft questions which are at a similar complexity level of syntactic structure. This hardly touches the security boundary of the aligned language models and hence results in a limited number of unsafe cases.} 



% \begin{figure}[h]
% \begin{center}
% \includegraphics[width=1.0\textwidth]{fig/comparison_atk_en.pdf}
% \caption{Comparison of different ways to bypass the safety guardrail of aligned LLMs.}
% \label{fig:comparison_atk}
% \end{center}
% \end{figure}




% %%%% BEGIN COMPARISON ATTACK
% \input{tex/tables/comparison_attack.tex}
% %%%% END COMPARISON ATTACK



\subsection{Targeted Linguistic Fuzzing with \textit{JADE}}
To explore the security boundary of LLMs, we propose a comprehensive targeted linguistic fuzzing platform called \textit{JADE}, which exploits Noam Chomsky's seminal theory of transformational-generative grammar to automatically transform natural questions into an increasingly more complex syntactic structure to break their safety guardrail. Our key insight is: Due to the complexity of human language, most of the current best LLMs can hardly recognize the invariant evil from the infinite number of different syntactic structures which form an unbound example space that can never be fully covered.  Based on the theory of generative grammar, Chomsky hypothesizes that there exists a universal grammar to human beings, and children are born with the knowledge of fundamental principles of grammar and acquire different languages due to parameter adjustments of daily language stimulation \cite{Chomsky1987LanguageAP}. Therefore, for an LLM which, at the very beginning of the training, has no innate knowledge of the universal grammar, it should not be able to achieve the same level of grammar usage as we human beings \cite{false_promise}.


% \noindent$\bullet$\textbf{ Comparison to Jailbreaking.} 

\begin{figure}[t]
\begin{center}
\includegraphics[width=0.9\textwidth]{fig/comparison_testing.pdf}
\caption{Comparison of different safety evaluation paradigms.}
\label{fig:comparison_testing}
\end{center}
\end{figure}


Technically, we implement a set of generative and transformational rules for both Chinese and English languages by native speakers. Invoking the rules in an intelligent way, \textit{JADE} automatically grows and transforms the parse tree of the given question until the target set of LLMs is broken, i.e., generate unsafe contents. In our evaluation, we observe that most of the well-known aligned LLMs can be broken after only a small number of steps of transformation/generation, which proves the efficiency of our linguistic fuzzing routines. This results in a benchmark of natural questions that simultaneously trigger harmful generation from eight aligned open-sourced LLMs in over $70\%$ test cases. We also report the effectiveness of our approach on some well-known LLMs in the form of Model-as-a-Service (MaaS), including ChatGPT, LLAMA2-70b-Chat, Google's PaLM2 and six widely used Chinese commercial LLMs (including Baidu's Wenxin Yiyan, Doubao, etc.). Besides, JADE also implements an automatic evaluation module which adopts the idea of active prompt tuning to reduce the required amount of manual annotation to achieve safety judgement results highly aligned with human experts. Finally, in Section \ref{sec:related:failure}, we further systematize the existing failure modes of aligned LLMs and analyze their connections with the limitation of aligned LLMs in handling the complexity of human language. 


% Besides, our fuzzing framework is generic: When testing the normal functionality, one may also set the evaluation goal as whether the generated answer to a given question is correct. Our grammar-based mutation can be used to find many known generalization bugs including the \textit{Reversal Curse} \cite{Grosse2023StudyingLL,Berglund2023TheRC}.
\subsection{JADE vs. Existing Safety Evaluation Paradigms}
Fig.\ref{fig:comparison_testing} provides an illustrative comparison between JADE and other safety evaluation paradigm for LLM. We provide a more detailed discussion below.





%%%% BEGIN COMPARISON ATTACK
\input{tex/tables/comparison_testing.tex}
%%%% END COMPARISON ATTACK
\noindent$\bullet$\textbf{ Comparison with Static Safety Benchmark.} This evaluation paradigm relies on crowd-sourcing to produce safety testing questions, which forms the safety benchmark\cite{Gehman2020RealToxicityPromptsEN,Sun2023SafetyAO,Xu2023CValuesMT,Wang2023DoNotAnswerAD}. By evaluating the ratio of unsafe generation cases on the constructed benchmark, one is able to compare the strength of the safety guardrail for different LLMs. However, most of the released safety benchmarks have a low ratio of unsafe generation on the current best aligned LLMs, and have weak transferability. In this work, we hope to exploit the method of linguistic mutation to dynamically evolve the safety threats of the test set, which can better explore the security boundary of aligned LLMs and hence results in more systematic safety assessment.

\noindent$\bullet$\textbf{ Comparison with LM-based Red-teaming.} Even before the rise of ChatGPT, there are a branch of research works which propose to test a target LLM with another LLM (i.e., the \textit{generator} LLM) \cite{Casper2023ExploreEE,DBLP:conf/emnlp/PerezHSCRAGMI22}. Specifically, the generator LLM is trained to produce sentences which maximize the probability of the target LLM to ``\textit{behave badly}'' and ``\textit{to say obnoxious, offensive, and harmful
things} \cite{DBLP:journals/corr/abs-2209-07858}'' (i.e., \textit{red-teaming}). These approaches rely on the feedback signal after the generation is finished and judged by an unsafe language detector. Despite the success in red-teaming LLMs like GPT-2, the approach may be stuck on aligned language models due to the sparsity of the reward signal, i.e., most of the generated texts are safe at the initial stage, which makes the optimization-based approach behave no better than random fuzzing. In comparison,  \textit{JADE} is a more targeted testing strategy which generates increasingly more complex syntactic structures until most ChatGPT-like AIs can no longer handle. Table \ref{tab:comparison:testing} summarizes the substantial differences of existing evaluation paradigms. 

\subsection{Key Contributions}
Our work mainly makes the following key contributions:
\begin{itemize}
\item \textbf{Effectiveness}: JADE is able to transform originally benign seed questions (with an average violation rate of only about 20\%) into highly critical and insecure problems, elevating the average violation rate of well-known LLMs to over 70\%. This effectively explores the language understanding and security boundaries of LLMs.

\item \textbf{Transferability}: JADE generates highly threatening test questions based on linguistic complexity, which can trigger violations in almost all open-source LLMs. For example, in the Chinese open-source large model security benchmark dataset generated by JADE, 30\% of the problems can trigger violations in eight well-known Chinese open-sourced LLMs simultaneously.

\item \textbf{Naturalness}: The test questions generated by JADE through linguistic mutation hardly modify the core semantics of the original problems and adhere to the properties of natural language. In contrast, jailbreaking templates for LLMs (including suffixes) introduce a large number of semantically irrelevant elements or garbled characters, exhibiting strong non-natural language characteristics. They are susceptible to targeted defenses by LLM developers (Section \ref{sec:related:jailbreaking} will delve into the differences between linguistic mutation and jailbreaking).
\end{itemize}