% This is samplepaper.tex, a sample chapter demonstrating the
% LLNCS macro package for Springer Computer Science proceedings;
% Version 2.20 of 2017/10/04
%
\documentclass[runningheads]{llncs}
%
\usepackage{graphicx}
\usepackage{xcolor}
\usepackage{fancyvrb}
\setcounter{secnumdepth}{3}
\usepackage{hyperref}
\usepackage{xcolor}
\usepackage{footmisc}
\usepackage{lmodern}
\usepackage{ulem}
\newcommand{\kibitz}[2]{%
{\color{#1}#2}{}%
}
\newcommand{\filip}[1]{\kibitz{purple}{[FI:#1]}} % filip
\newcommand{\pedro}[1]{\kibitz{orange}{[PS: #1]}} % pedro
\newcommand{\craig}[1]{\kibitz{red}{[CMR: #1]}} % craig
\newcommand{\DS}[1]{\kibitz{blue}{[DS: #1]}} % Daniel S.
\newcommand{\DG}[1]{\kibitz{red}{[DG: #1]}} % Daniel G.

\hypersetup{colorlinks,allcolors=black}

\begin{document}
%
\title{KGTK: A Toolkit for Large Knowledge Graph Manipulation and Analysis}%\thanks{Supported by organization x.}}
%
\titlerunning{The Knowledge Graph Toolkit}
% If the paper title is too long for the running head, you can set
% an abbreviated paper title here
%
\author{Filip Ilievski\inst{1}\orcidID{0000-0002-1735-0686} \and
Daniel Garijo\inst{1}\orcidID{0000-0003-0454-7145} \and Hans Chalupsky\inst{1}\orcidID{0000-0002-8902-1662} \and Naren Teja Divvala\inst{1} \and Yixiang Yao\inst{1}\orcidID{0000-0002-2471-5591} \and Craig Rogers\inst{1}\orcidID{0000-0002-5818-3802} \and Rongpeng Li\inst{1}\orcidID{0000-0002-6911-8002} \and Jun Liu\inst{1} \and Amandeep Singh\inst{1}\orcidID{0000-0002-1926-6859} \and Daniel Schwabe\inst{2}\orcidID{0000-0003-4347-2940} \and Pedro Szekely\inst{1}}
%
\authorrunning{Filip Ilievski, Daniel Garijo et. al.}
% First names are abbreviated in the running head.
% If there are more than two authors, 'et al.' is used.
%
\institute{Information Sciences Institute, University of Southern California \email{\{ilievski,dgarijo,hans,divvala,yixiangy,rogers, rli,junliu,amandeep,pszekely\}@isi.edu} \and Dept. of Informatics, Pontificia Universidade Cat\'olica Rio de Janeiro \\ \email{dschwabe@inf.puc-rio.br}
}

%
\maketitle              % typeset the header of the contribution
%
\begin{abstract}

%\pedro{Alternative 5-sentence abstract}
% context for our problem
Knowledge graphs (KGs) have become the preferred technology for representing, sharing and adding knowledge to modern AI applications. 
While KGs have become a mainstream technology, the RDF/SPARQL-centric toolset for operating with them at scale is heterogeneous, difficult to integrate and only covers a subset of the operations that are commonly needed in data science applications. In this paper we present KGTK, a data science-centric toolkit designed to represent, create, transform, enhance and analyze KGs.
KGTK represents graphs in tables and leverages popular libraries developed for data science applications, enabling a wide audience of developers to easily construct knowledge graph pipelines for their applications.
We illustrate the framework with real-world scenarios where we have used KGTK to integrate and manipulate large KGs, such as Wikidata, DBpedia and ConceptNet.%, in our own work. 

\textbf{Resource type}: Software\\
\textbf{License}: MIT\\
\textbf{DOI}: https://doi.org/10.5281/zenodo.3828068\\
\textbf{Repository}: https://github.com/usc-isi-i2/kgtk/

\keywords{knowledge graph  \and knowledge graph embedding \and knowledge graph filtering \and knowledge graph manipulation}
\end{abstract}
%
%
%
%\section{Requirements Checklist \textcolor{red}{[This section Will be removed]}}
%\begin{itemize}
%    \item \textcolor{red}{[Not done]} The KGTK specification (data format) needs to be public and in decent shape. It does not need to be perfect, but we should give it a version and then open issues on the parts that we are working on.
%    \item \textcolor{blue}{[Done but not tested by Daniel]} The Software needs to be released in GitHub. I can provide a DOI in Zenodo.
%    \item \textcolor{red}{[Not done]} The software needs to be installable from scratch (last time I checked I was unable to complete the installation, but maybe I didn’t try hard enough).
%    \item \textcolor{red}{[Not done, Pedro working on it]} A demonstration of the resource and how to use it with examples should be made available. One example can be a simple table. A full one with Wikidata should be made available as well (pointers, data, etc.). Examples (in the examples folder) are not informative.
%\end{itemize}

%\textcolor{blue}{[DG: Please add your comments using a color and your initials]}https://www.overleaf.com/project/5f3336c34dddf40001756326

\setcounter{footnote}{0}

\section{Introduction}
%\textcolor{red}{Maximum of 16 pages incl references. Min: 8 pages}

Knowledge graphs (KGs) have become the preferred technology for representing, sharing and using knowledge in applications. A typical use case is building a new knowledge graph for a domain or application by extracting subsets of several existing knowledge graphs, combining these subsets in application-specific ways, augmenting them with information from structured or unstructured sources, and computing analytics or inferred representations to support downstream applications.
For example, during the COVID-19 pandemic, several efforts focused on building KGs about scholarly articles related to the pandemic starting from the CORD-19 dataset provided by the Allen Institute for AI~\cite{Wang2020}.\footnote{\url{https://github.com/fhircat/CORD-19-on-FHIR/wiki/CORD-19-Semantic-Annotation-Projects}} Enhancing these data with with KGs such as DBpedia \cite{dbpedia} and Wikidata \cite{vrandecic_wikidata:_2014} to incorporate gene, chemical, disease and taxonomic information, and computing network analytics on the resulting graphs, requires the ability to operate these these KGs at scale.

Many tools exist to query, transform and analyze KGs. Notable examples include graph databases, such as %SPARQL endpoints
%\DS{SPARQL endpoints are interfaces to DBs that support the RDF model, so not really a graph database in itself} 
RDF triple stores and Neo4J;\footnote{\url{https://neo4j.com}} tools for operating on RDF such as graphy\footnote{\url{https://graphy.link/}} and RDFlib\footnote{\url{https://rdflib.readthedocs.io/en/stable/}}, %ontology matching tools such as \pedro{XXX and YYY}, 
entity linking tools such as WAT~\cite{piccinno2014tagme} or BLINK~\cite{wu2019zero}, entity resolution tools such as MinHash-LSH~\cite{leskovec2020mining} or MFIBlocks~\cite{kenig2013mfiblocks}, libraries to compute graph embeddings such as PyTorch-BigGraph~\cite{lerer2019pytorch} and libraries for graph analytics, such as graph-tool\footnote{\url{https://graph-tool.skewed.de/}} and NetworkX.\footnote{\url{https://networkx.github.io/}} 

There are three main challenges when using these tools together. First, tools may be challenging to set up with large KGs (e.g., the Wikidata RDF dump takes over a week to load into a triple store) and often need custom configurations that require significant expertise. Second, interoperating between tools requires developing data transformation scripts, as some of them may not support the same input/output representation. Third, composing two or more tools together (e.g., to filter, search, and analyze a KG) includes writing the intermediate results to disk, which is time and memory consuming for large KGs. %Finally, \pedro{this sentence doesn't make sense to me, what is the point to make here? Remove ``even''?} \sout{even when RDF is used as a standard representation for integration among tools, performing certain operations over large KGs has a prohibitive cost (e.g., calculating centrality for all nodes in the KG, or computing the set of reachable nodes for many instances of a class).}\DG{The point was to highlight that operating with RDF as an intermediate format is not efficient, but happy to remove it.} %performing join operations \DS{some joins are efficient in RDF... are you referring to  "path computations"?},

%it is extremely challenging to interoperate between them; 

%DG: I remove this because we do not demonstrate it: without the need for huge amounts of space or memory
%lowering the barrier for use. creating machine learning and natural language processing pipelines,
In this paper, we introduce the Knowledge Graph Toolkit (KGTK), a framework for manipulating, validating, and analyzing large-scale KGs. Our work is inspired by Scikit-learn~\cite{scikit-learn} and SpaCy,\footnote{\url{https://spacy.io/}} two popular toolkits for machine learning and natural language processing that have had a vast impact by making these technologies accessible to data scientists and software developers. KGTK aims to build a comprehensive library of tools and methods to enable easy composition of KG operations (validation, filtering, merging, centrality, text embeddings, etc.) to build knowledge-based AI applications. The contributions of KGTK are:
% to develop applications in a wide range of domains
% routine development of knowledge graph pipelines

\begin{itemize}
\item The \textbf{KGTK file format}, which allows representing KGs as hypergraphs. This format unifies the Wikidata data model \cite{vrandecic_wikidata:_2014} based on items, claims, qualifiers, and references; property graphs that support arbitrary attributes on nodes and edges; RDF-Schema-based graphs such as DBpedia \cite{dbpedia}; and general purpose RDF graphs with various forms of reification. The KGTK format uses tab-separated values (TSV) to represent edge lists, making it easy to process with many off-the-shelf tools.

\item A comprehensive \textbf{validator and data cleaning} module to verify compliance with the KGTK format, and normalize literals like strings and numbers.%, etc.% misaligned values, etc.
\item \textbf{Import modules} to transform different formats into KGTK, including N-Triples \cite{Seaborne:14:RN}, Wikidata qualified terms, and ConceptNet \cite{speer2016conceptnet}.% Visual Genome, ATOMIC, FrameNet and Roget. \DG{Need references and URLs for these formats}%, with support for popular formats such as GML and dot coming soon. 
%\filip{revise this list later}
\item \textbf{Graph manipulation modules} for bulk operations on graphs to validate, clean, filter, join, sort, and merge KGs. Several of these are implemented as wrappers of common, streaming Unix tools like awk\footnote{\url{https://linux.die.net/man/1/awk}}, sort, and join. %, as well as miller,\footnote{\url{https://johnkerl.org/miller/doc/index.html}} a package with a comprehensive set of tools to manipulate text-delimited files.
\item \textbf{Graph querying and analytics modules} to compute centrality measures, connected components, and text-based graph \textbf{embeddings} using state-of-the-art language models: RoBERTa~\cite{liu2019roberta}, BERT~\cite{devlin2018bert}, and DistilBERT~\cite{sanh2019distilbert}. Common queries, such as computing the set of nodes reachable from other nodes, are also supported.
\item \textbf{Export modules} to transform KGTK format into diverse standard and commonly used formats, such as RDF (N-Triples), property graphs in Neo4J format, and GML to invoke tools such as graph-tool or Gephi.\footnote{\url{https://gephi.org/}}  
\item A \textbf{framework for composing multiple KG operations}, based on Unix pipes. The framework uses the KGTK file format on the standard input and output to combine tools written in different programming languages.%  that allows easy composition of modules and too that consume and produce KGTK edge lists on standard input and output, enabling composition of
 %\DG{I think not in the contributions. It's not a novel contribution. We can talk in the next paragraph IMO}
\end{itemize}

%This is not technically a feature.
 % efficient \DS{can we talk about efficiency yet? Wouldn't we have to show something about this?}
%\DS{perhaps can we mention amenability to parallelization?}
% widely used by \textcolor{red}{which community?}. 
%This way, KGTK supports executing pipelines that operate on the full set of Wikidata on a laptop, \textcolor{red}{taking about one hour to do filter, join and sort operations}.
%can we argue about efficiency yet?
KGTK provides an implementation that integrates all these methods relying on widely used tools and standards, thus allowing their composition in pipelines to operate with large KGs like Wikidata on an average laptop. 

The rest of the paper is structured as follows. Section \ref{sec:scenario} describes a motivating scenario and lists the requirements for a graph manipulation toolkit. Section \ref{sec:kgtk} describes KGTK by providing an overview of its file format, supported operations, and examples on how to compose them together. Next, Section \ref{sec:showcase} showcases how we have used KGTK on three different real-world use cases, together with the current limitations of our approach. We then review relevant related work in Section \ref{sec:relWork}, and we conclude the paper in Section \ref{sec:conclusions}.

%\textcolor{red}{to complete}

%KGTK supports the following operations: instances (calculating all instances of a class or group of classes); reachable nodes (we should provide an example); filter (useful for dropping part of a KG); calculating text embeddings; remove columns (why is this not filter?); sort, zconcat and merge identical nodes. In addition, KGTK has the capability of exporting the KG into Wikidata triples / NEo4J. 



%\pedro{Possibly skip this paragraph if it is not needed as intro to the pipelines idea.}



%\textcolor{brown}{[PS: Link motivation to use case (e.g., COVID) - then cskg follows a similar workflow - existing tools wont handle it]}

%[PS: include evaluation - on the covid dataset - time comparison to sparql ]

%[DG: evaluation is not required - we can have a section for showing how it works]






% The CORD-19 corupts give the pubmed and PMC identifiers for all papers, so the task is to identify all statements in Wikidata where the subject is an article containing one of these identifiers. \pedro{describe that the SPARQL query won't run, describe that graphy on the Wikidata RDF dump takes N hours}. 

% The problem: Doing complex manipulations with KGs is not feasible at the moment. Operations such as sort, rank, filter or other more complex analysis take too much time when KGs are big. Compressed formats like HDT have been created, but: 1) They have no support for qualifiers and 2) <Motivation here for the commands developed below>.

% Building applications with knowledge graphs requires complex manipulations similar to data preparation pipelines in data science. Typical operations include:
% Selecting subsets of large KGs
% Combining multiple KGs
% Computing graph metrics (Pagerank, centrality, …)
% Computing embeddings (language models on text nodes, graph embeddings, retrofitting, …)
% Doing link prediction and other reasoning tasks
% No platform/framework exists that enables developers to easily and conveniently create pipelines to perform these operations. The challenge is especially severe when large KGs such as Wikidata or DBpedia are involved.


% A common element in Scikit-learn and SpaCy is a well-designed, simple data structure that carries the information between modules in a pipeline, transforming and augmenting the information until a final output is computed. An equally important element is a collection of "almost state of the art" \pedro{cite spacy} efficient modules with a simple, uniform API that enables users to build pipelines with a few lines of code in commodity hardware.  


% create, integrate, denoise, reason and query KGs to build interesting applications that leverage vast amounts of knowledge. KGTK is designed to support reasoning pipelines composed of operators to import, filter, transform, abstract and reason with KGs. A simple edge-list representation of KGs using CSV files enables easy incorporation of high performance utilities and packages written in different languages. The expectation is that application developers will find it easy to compose and experiment with different KG pipelines in Jupyter notebooks, similarly to how data scientists today can rapidly build data driven models using Scikit learn.


% The goal of KGTK is provide tools to enable data scientists to easily build sophisticated KG pipelines on their laptops, processing large KGs such as Wikidata, DBpedia, large databases such as CTD(\pedro{stats}), arbitrary structured data such as the output of information extraction tools, combining and linking entities, computing derived data such as graph metrics and embeddings.

% Existing KG tools are inadequate for this task because they don't provide effective operations to construct KG pipelines.

% For example, the SpaCy language processing library provides modules for language processing tasks such as text segmentation, tokenization, part of speech tagging, dependency parsing, named entity recognition, text similarity using vectors, etc. Developers can easily use and customize the provided modules to build high performance pipelines for a wide variety of language processing tasks.
% Similarly, Scikit-Learn provides modules for common machine learning tasks, including modules for featurization, feature selection, classification, regression, hyper parameter tuning, etc.


\section{Motivating Scenario}
\label{sec:scenario}
%\textcolor{red}{This section needs work}

%Numerous examples of independent struggles to perform standard graph operations efficiently exist, e.g., consider the set of entity linking systems that need to extract an index of labels or subclasses from DBpedia or Wikidata. Yet, few can illustrate the need for efficient, streamlined toolkit better than the ongoing pursuit to capture Covid-19 knowledge in a single graph.

% Commenting the above out because it doesn't say much about any concrete motivating examples.

%Few examples could illustrate the need for efficient, streamlined toolkit better than
%Here we briefly explain the efforts and struggles of our own team to integrate some of them together.
The 2020 coronavirus pandemic led to a series of community efforts to publish and share common knowledge about COVID-19 using KGs. Many of these efforts use the COVID-19 Open Research Dataset (CORD-19) \cite{Wang2020}, compiled by the Allen Institute for AI. CORD-19 is a free resource containing over 44,000 scholarly articles, including over 29,000 with full text, about COVID-19 and the coronavirus family of viruses. Having an integrated KG would allow easy access to information published in scientific papers, as well as to general medical knowledge on genes, proteins, drugs, and diseases mentioned in these papers, and their interactions.
%\DG{Write here our goal, not just integrate them, integrate them to do X} \filip{I added a possible goal, PS/DS to verify if it is correct/complete}\DS{Correct yes, complete, hard to say, but I do not think it matters}

In our work, we integrated the CORD-19 corpus with gene, chemical, disease, and taxonomic knowledge from Wikidata and CTD databases,\footnote{\url{http://ctdbase.org/}} as well as entity extractions from Professor Heng Ji's BLENDER lab at UIUC.\footnote{\url{https://blender.cs.illinois.edu/}} We extracted all the items and statements for the 30,000 articles in the CORD-19 corpus \cite{Wang2020} that were present in Wikidata at the time of extraction, added all Wikidata articles, authors, and entities mentioned in the BLENDER corpus, homogenized the data to fix inconsistencies (e.g., empty values), created nodes and statements for entities that were absent in Wikidata, incorporated metrics such as PageRank for each KG node, and exported the output in both RDF and Neo4J. 

This use case exhibited several of the challenges that KGTK is designed to address. For example, extracting a subgraph from Wikidata articles is not feasible using SPARQL queries as it would have required over 100,000 SPARQL queries; using RDF tools on the Wikidata RDF dump (107 GB compressed) is difficult because its RDF model uses small graphs to represent each Wikidata statement; using the Wikidata JSON dump is possible, but requires writing custom code as the schema is specific to Wikidata (hence not reusable for other KGs). In addition, while graph-tool allowed us to compute graph centrality metrics, its input format is incompatible with RDF, requiring a transformation. 

%differs from the input or output format of all the other existing tools, making it difficult to use.\DG{Not difficult, but you need to transform}
%\DG{Here there is a gap. We talk about the first step being challenging for integrating data. Which other data is it being integrated together? What are the other steps of the analysis?} \DS{I think the steps are hidden in the first paragraph - first, find all the papers in WD; second, find all the entities cited in the BLENDER extractions @PS pls confirm}

%\filip{TODO: Daniel S. - Please finish this paragraph}
Other efforts employed a similar set of processing steps~\cite{Wang2020}.\footnote{A list of such projects can be found in \url{https://github.com/fhircat/CORD-19-on-FHIR/wiki/CORD-19-Semantic-Annotation-Projects}} These range from mapping the CORD-19 data to RDF,\footnote{\url{https://github.com/nasa-jpl-cord-19/covid19-knowledge-graph}, \url{https://github.com/GillesVandewiele/COVID-KG/}} to adding annotations to the articles in the dataset pointing to entities extracted from the text, obtained from various sources \cite{gmg20}.\footnote{\url{http://pubannotation.org/collections/CORD-19}}
%\pedro{Some efforts did X others did Y, need to research this}
A common thread among these efforts involves leveraging existing KGs such as Wikidata and Microsoft Academic Graph
%, and LOD datasets, e.g. DBPedia 
to, for example, build a citation network of the papers, authors, affiliations, etc.\footnote{\url{https://scisight.apps.allenai.org/clusters}\label{scisight}} Other efforts focused on extraction of relevant entities (genes, proteins, cells, chemicals, diseases), relations (causes, upregulates, treats, binds), and %\pedro{examples}
%and events \pedro{examples} \DS{not sure what you mean here},
linking them to KGs such as Wikidata and DBpedia.
%\pedro{more examples}.
Graph analytics operations followed, such as computing centrality measures in order to support identification of key articles, people or substances,\footref{scisight} or generation of various embeddings to recommend relevant literature associated with an entity.\footnote{\url{https://github.com/vespa-engine/cord-19/blob/master/README.md}}
The resulting graphs were deployed as SPARQL endpoints, or exported as RDF dumps, CSV, or JSON files.%, among others.


%At the moment of writing, the most recent version of the CORD-19 dataset contains metadata for 63,000 articles related to COVID-19, including publication identifiers, publication date, authors, references and links to PDF files for the full content or abstract.
%\footnote{\url{https://www.kaggle.com/allen-institute-for-ai/CORD-19-research-challenge}}
%\pedro{needs to be checked what's in the corpus}\DG{Missing: how would the integration be done? How does this example illustrate ``combining these subsets in application-specific ways, augmenting them with information from structured or unstructured sources''. Also, we don't talk about why a qualifier is important. We should explain that in real data there are many qualifiers that add notes to facts or statements. For example, the units of a measurement, the source of a statement or the year where an observation was made. }



These examples illustrate the need for composing sequences of integrated KG operations that extract, modify, augment and analyze knowledge from existing KGs, combining it with non-KG datasets to produce new KGs. Existing KG tools do not allow users to seamlessly run such sequences of graph manipulation tasks in a pipeline. We propose that an effective toolkit that supports the construction of modular KG pipelines has to meet the following criteria:%\DS{I fear we may be challenged to justify at least some of these criteria, perhaps it would be interesting to refer to some of the issues above} %\DG{Tried to address this below}

\begin{enumerate}
\item \textbf{A simple representation format} that all modules in the toolkit operate on (the equivalent of \textit{datasets} in Scikit-learn and \textit{document model} in SpaCy), to enable tool integration without additional data transformations.  

\item \textbf{Ability to incorporate mature existing tools}, wrapping them to support a common API and input/output format. The scientific community has worked for many years on efficient techniques for manipulation of graph and structured data. The toolkit should be able to accommodate them without the need for a new implementation. %This way, the resulting applications would have higher quality and efficiency (lower development cost).%\DG{Reviewers may argue: Then why don't you use RDF? We should explain too}\DS{RDF has not shown to be efficient in our case}
 
\item \textbf{A comprehensive set of features} that include import and export modules for a wide variety of KG formats, modules to select, transform, combine, link, and merge KGs, modules to improve the quality of KGs and infer new knowledge, and modules to compute embeddings and graph statistics. Such a rich palette of functionalities would largely support use cases such as the ones presented in this section.%like CORD-19, consolidating commonsense knowledge, and quantifying data on Ethiopia (the last two are described in section \ref{sec:showcase}).

\item A \textbf{pipeline mechanism} to allow composing modules in arbitrary ways to process large public KGs such as Wikidata, DBpedia, or ConceptNet.% on an average laptop, to widen the coverage of potential users.\DG{I think this is more to allow manipulations that are not possible just by using a query language}
\end{enumerate}
%As elaborated in the previous section, the main issues with using these tools are that many of them are: difficult to configure, difficult to interoperate, and time-consuming when operating directly with RDF dumps. 

%\DG{Explain that the challenges we faced in this example are common for building any knowledge base. We are not alone}

%\pedro{Perhaps shorten this part too - and perhaps have a running example}
%of performing (sequences of) graph operations at present

\section{KGTK: The Knowledge Graph Toolkit}\label{sec:kgtk} 

KGTK helps manipulating, curating, and analyzing large real-world KGs, in which each statement may have multiple qualifiers such as the statement source, its creation date or its measurement units. Figure \ref{fig:overview} shows an overview of the different capabilities of KGTK. Given an input file with triples (either as tab-separated values, Wikidata JSON, or N-Triples), we convert it to an internal representation (the \textit{KGTK file format}, described in Section \ref{sec:format}) that we use as main input/output format for the rest of the features included in the toolkit. Once data is in KGTK format, we can perform operations for curating (data validation and cleaning), transforming (sort, filter, or join) and analyzing the contents of a KG (computing embeddings, statistics, node centrality). KGTK also provides export operations to common formats, such as N-Triples, Neo4J, and JSON. 
%contents that are usually too slow to perform by using a SPARQL endpoint
%We describe the KGTK file format in Section \ref{sec:format}, and 
The KGTK operations are described in Section \ref{sec:features}, whereas their composition into pipelines is illustrated in Section \ref{sec:chain}.
\begin{figure}[t!]
    \centering
    \includegraphics[width=0.97\textwidth]{images/KGTK-Opsv2.png}
    \caption{Overview of the usage workflow and features included in KGTK.}
    \label{fig:overview}
\end{figure}

\subsection{KGTK file format}\label{sec:format}
\label{ssec:kgtk-format}

%\textcolor{red}{Daniel to summarize https://docs.google.com/document/d/1oEwKiv5Fy-iksHumdyLPjUNuXVriocWdUu6-UtJAxQI/edit
%}
%Note: this is where we may be criticized more, we have to be careful.
%%many of the desired operations were already well-implemented for tabular data, including  join, sort, centrality, and computation of graph embeddings, by mature tools like bash, pandas, and graph-tool

KGTK uses a tab-separated column-based text format to describe any attributed, labeled or unlabeled hypergraph. We chose this format instead of an RDF serialization for three reasons. First, tabular formats are easy to generate and parse by standard tools. Second, this format is self-describing and easy to read by humans. Finally, it provides a simple mechanism to define hypergraphs and edge qualifiers, which may be more complicated to describe using Turtle or JSON. %In fact, the KGTK file format can be used to 
%aims to be simple, readable, expressive, self-describing and
%(which requires adopting more complex mechanisms to be represented in RDF, like reification)

% from the rebuttal
% our main reasons for choosing TSV are: 1) the wealth of existing tooling that operates with TSV data; 2) the fact that tables can be easily understood by humans, and 3) tables are natural format to define hypergraphs, which may be more complicated to describe using Turtle or JSON.

KGTK defines KGs as a set of nodes and a set of edges between those nodes. All concepts of meaning are represented via an edge, including edges themselves, allowing KGTK to represent generalized hypergraphs (while supporting the representation of RDF graphs). 
%I think KGTK can represent more than Attributed Hypergraphs (AHs), because we allow an Edge to be treated as a Node cf example below. In AHs, node and edge labels are disjoint from attribute labels. But I'm not sure if we should mention this.  Hans: I was struggling with the correct terminology here myself, the closest I could find was generalized hypergraphs (GH) which allow hyperedges between 2 or more nodes and edges between edges.
The snippet below shows a simple example of a  KG in KGTK format with three people (\textit{Moe}, \textit{Larry} and \textit{Curly}), the creator of the statements (\textit{Hans}) and the original source of the statements (\textit{Wikipedia}):  
{\footnotesize
\vbox{
\begin{Verbatim}[commandchars=\\\{\}]
\textbf{
    node1     label       node2    creator   source      id}
    "Moe"     rdf:type    Person   "Hans"    Wikipedia   E1
    "Larry"   rdf:type    Person   "Hans"    Wikipedia   E2
    "Curly"   rdf:type    Person             Wikipedia   E3
    "Curly"   hasFriend   "Moe"              Wikipedia   E4
\end{Verbatim}
}
}
The first line of a KGTK file declares the headers for the document. The reserved words \textit{node1}, \textit{label} and \textit{node2} are used to describe the subject, property and object being described, while \textit{creator} and \textit{source} are optional qualifiers for each statement that provide additional provenance information about the creator of a statement and the original source. Note that the example is not using namespace URIs for any nodes and properties, as they are not needed for local KG manipulation. Still, namespace prefixes (e.g., \texttt{rdf}) may be used for mapping back to RDF after the KG manipulations with KGTK. Nodes and edges have unique IDs (when IDs are not present, KGTK generates them automatically).

The snippet below illustrates the representation of qualifiers for individual edges, and shows how the additional columns in the previous example may be represented as edges about edges:

{\footnotesize
\vbox{
\begin{Verbatim}[commandchars=\\\{\}]
\textbf{
    node1     label       node2       id}
    "Moe"     rdf:type    Person      E1
    E1        source      Wikipedia   E5
    E1        creator     "Hans"      E6
    "Larry"   rdf:type    Person      E2  
\end{Verbatim}
}
}
KGTK is designed to support commonly-used typed literals:
\begin{itemize}

    \item \textbf{Language tags}: represented following a subset of the RDF convention, language tags are two- or three-letter ISO 639-3 codes, optionally followed by a dialect or location subtag. 
    Example: \verb+`Sprechen sie deutsch?'@de+.  
    
    \item \textbf{Quantities}: represented using a variant of the Wikidata format \texttt{amount}$\sim$ \texttt{toleranceUxxxx}.  A quantity starts with an \textit{amount} (number), followed by an optional \textit{tolerance interval}, followed by either a combination of \textit{standard (SI) units} or a \textit{Wikidata node} defining the unit (e.g., Q11573 indicates ``meter''). Examples include \verb+10m+, \verb$-1.2e+2[-1.0,+1.0]kg.m/s2$ or \verb$+17.2Q494083$

    \item \textbf{Coordinates}: represented by using the Wikidata format @LAT/LON, for example: \verb+@043.26193/010.92708+
    
    \item \textbf{Time literals}: represented with a $^\wedge$ character (indicating the tip of a clock hand) and followed by an ISO 8601 date and an optional precision designator, for example: \verb$^1839-00-00T00:00:00Z/9$
\end{itemize}

%\textcolor{red}{If space, we should show an example with typed literals, showing implied edges}

The full KGTK file format specification is available online.\footnote{\url{https://kgtk.readthedocs.io/en/latest/specification/}} 
%\DS{We describe here the file format, but don't mention the data model. As I understand it, it is an extension of property graphs, since we allow Edges to be Nodes as well. Would any KGTK file be mappable to a Neo4J db? I think not...}\DG{as far as i know there is only a file format, not a data model. You can model data any way you want using a hypergraph. It should be possible to map to neo4j because they have rdf imports...}

%\subsection{Architecture (AP: YY)}

%\textcolor{red}{This should describe more the piping composition rather than a general architecture.}

%KGTK has a pipelining architecture based on Unix pipes that allows easy composition of pipelines that consume and produce KGTK edge lists on standard input and output, enabling composition of tools written in different programming languages.



\subsection{KGTK Operations}\label{sec:features}

KGTK currently supports 13 operations (depicted in Figure \ref{fig:overview}),\footnote{\url{https://kgtk.readthedocs.io/en/latest}} grouped into four modules: importing modules, graph manipulation modules, graph analytics modules, and exporting modules. We describe each of these modules below.

\subsubsection{Importing and exporting from KGTK}~\\

%\noindent A number of operations allow the user to import an existing data source into KGTK: 

%, or to export from KGTK format to another common format, like TSV (.tsv), N-triples (.nt), or Graphtool (.gt)

%\noindent 
\textbf{1.}
The \texttt{import} operation transforms an external graph format into KGTK TSV format.
%allows users to import a graph file in KGTK by transforming it into a TSV format that conforms with the KGTK file format. %The input format is specified through the \texttt{format} argument. 
KGTK supports importing a number of data formats, including N-Triples, ConceptNet, and Wikidata (together with qualifiers). 

\textbf{2. } The \texttt{export} operation transforms a KGTK-formatted graph to a wide palette of formats: TSV (by default), N-Triples, Neo4J Property Graphs, graph-tool and the Wikidata JSON format.

\subsubsection{Graph curation and transformation}~\\

%\noindent KGTK provides several operations to help validating, manipulating, and merging KGs to make them usable. We describe these operations in detail below:

\textbf{3. } %\footnote{We are currently integrating a second, lightweight validator variant written in STELLA. The lightweight and the extensive validator would serve a different need.} 
The \texttt{validate} operation ensures that the data meets the KGTK file format specification, detecting errors such as nodes with empty values, values of unexpected length (either too long or too short), potential errors in strings (quotation errors, incorrect use of language tags, etc.), incorrect values in dates, etc. Users may customize the parsing of the file header, each line, and the data values, as well as choose the action taken when a validation rule fails.

%From the rebuttal:
% The validator ensures that the data meets our KGTK file specification, while allowing the user to customize the parsing of the file header, each line, and the data values, as well as to choose the action taken when a validation rule fails.


%Currently, the command focuses on header column names and data column counts. It does not yet validate that headers and cells are compliant with the KGTK data type rules.

%\DS{we could mention syntactic and semantic checks}\DS{Is it really necessary to enumerate all checks?}\DG{I think so, it shows the many things the validate command does}
%\begin{itemize}
%    \item when a node1, node2, id, or a full line is blank
%    \item when a line contains a comment
%    \item when a line has unexpected length (too long or too short)
%    \item when a column name is ``unsafe''\DG{What does unsafe mean?}
%    \item when a cell has an invalid value \DG{What does invalid mean in this context? An example?}
%    \item when a cell contains a language suffix, single or double strings
%    \item when a date contains a zero value for month or day\DG{When a date is in an unexpected format too?}
%\end{itemize}
% Too much detail:
%In addition, the type of validations can be changed, e.g., by adding a minimum and maximum valid year;\footnote{According to our representation format, the dates need to conform to ISO-8601, which constraints the dates to be in the range 1583-9999. The validation argument allows the user to relax those limits.} or whether list separators should be escaped or split on  (i.e., cell whose value is a list may be split into individual values, which are independently validated).  

%\DG{Maybe we should say here or in the discussion the amount of errors we are finding in WD, like the wrong language tags and the broken descriptions (more than 10000)}

%When a cell contains a language qualified string, the language code can optionally be validated against the referenced standard\DG{What is a referenced standard in this context? Example?}. KGTK checks for obsolete language codes, which are retained in Wikidata, and for new language codes that have not yet been incorporated into the Python support libraries\DG{What are the python support libraries? Reference?}. We have tested the \texttt{validate} command in Wikidata, \pedro{how many?} errors in dates and language tags.

\textbf{4.} The \texttt{clean} operation fixes a substantial number of errors detected by \texttt{validate}, by correcting some common mistakes in data encoding (such as not escaping `pipe' characters), %by fixing encoding errors in strings, 
replacing invalid dates, normalizing values for quantities, languages and coordinates using the KGTK convention for literals. Finally, it removes rows that still do not meet the KGTK specification (e.g., rows with empty values for required columns or rows with an invalid number of columns).

% from the rebuttal
% We clean data by first validating it (similarly to the validation command), followed by correcting some common mistakes in data encoding, such as not escaping vertical bar (pipe) characters, and, finally, removing rows that still do not meet the KGTK specification (e.g., rows with empty values for required columns or rows with an invalid number of columns).

\textbf{5.}  \texttt{sort} efficiently reorders any KGTK file according to one or multiple columns. 
\texttt{sort} is useful to organize edge files so that, for example, all edges for \texttt{node1} are contiguous, enabling efficient processing in streaming operations.
%If more than one column is provided, columns are compared in the order listed, i.e., not in the order they appear in the file. %Data is sorted in ascending order by default, but can also be sorted in reverse by indicating the \texttt{reverse} option. %Column names found in the header will override the default values defined in the KGTK format.

% One could sort a file piped from another command based on label and node2:
% \begin{verbatim}
%     gzcat wikidata_edges.tsv.gz | kgtk sort -c label,node2
% \end{verbatim}

\textbf{6.} The \texttt{remove-columns} operation 
removes a subset of the columns in a KGTK file (\texttt{node1} (source), \texttt{node2} (object), and \texttt{label} (property) cannot be removed). 
This is useful in cases where columns have lengthy values and are not relevant to the use case pursued by a user, e.g., removing edge and graph identifiers when users aim to compute node centrality or calculate embeddings.% (both the edge identifiers or the target graph are not used in these operations, and take significant space in memory).%, as these identifiers might take much space and slow down processing, while not being useful for this task.

% Remove a subset of the columns from a TSV file. For instance, remove ``id'' and ``docid'' from a Wikidata edges file:

% \begin{verbatim}
%     kgtk remove_columns -c ``id, docid'' data/wikidata_edges.tsv
% \end{verbatim}

\textbf{7.} The \texttt{filter} operation selects edges from a KGTK file, by specifying constraints (``patterns'') on the values for node1, label, and node2. The \texttt{pattern} language, inspired by \url{graphy.js}, has the following form: ``\texttt{subject-pattern ; predicate-pattern ; object-pattern}".
For each of the three columns, the filtering pattern can consist of a list of symbols separated using commas. Empty patterns indicate that no filter should be performed for a column. For instance, to select all edges that have property P154 or P279, we can use the pattern `` ; P154,P279 ; ''. Alternatively, a common query of retrieving edges for all humans from Wikidata corresponds to the filter `` ; P31 ; Q5''.
%\DG{Another example would help, like all labels that contain X. This query usually times out}
% \begin{verbatim}
%     kgtk filter -p `` ; P154 ; '' --pred `prop' INPUT
% \end{verbatim}



\textbf{8.} The \texttt{join} operation will join two KGTK files. %, even if they don't have the same columns%) - it is left to the user to ensure that the result makes semantic sense.
%\DG{Note: I have omitted node files or edge files, using instead kgtk files}
Inner join, left outer join, right outer join, and full outer join are all supported. %A special command, \texttt{cat}, also provides a full outer join, but can process n files simultaneously.\DG{I don't understand the last sentence very well, sorry} % (well, it reads the headers in order, then processes the data in order).
When a join takes place, the columns from two files are merged into the set of columns for the output file.
By default, KGTK will join based on the \texttt{node1} column, although it can be configured to join by edge \texttt{id}. 
KGTK also allows the \texttt{label} and \texttt{node2} columns to be added to the join. 
Alternatively, the user may supply a list of join columns for each file giving them full control over the semantics of the result. 


%Columns are matched by name\DG{"name" is not node1, node2 or label. What does 'name' mean?}.  KGTK has the notion of certain predefined column names that have aliases, and that is respected. When the left and right input files use different aliases for the same predefined column, the left input file's name predominates. There is the option to apply a prefix to any right input file column names that added to the output file without matching a column in the left input file.\DG{This section is hard to follow. An example would be super nice.}\DS{+1!}

\textbf{9.} The \texttt{cat} operation concatenates any number of files into a single, KGTK-compliant graph file. %Each of the input files can either be uncompressed, or compressed in a standard format: \texttt{gzip}, \texttt{bzip2}, or \texttt{xz}.\filip{i don't know if the compression is supported in cat - it was in zconcat.} %\DG{So the difference with merge is that they can be compressed? If so, we should put this command under 'merge' and state that the zconcat command does the same with compressed files}

%Concatenate any mixture of plain or gzip/bzip2/xz-compressed files. For example, we can concatenate two unzipped files and store them in a file.

% Inputs are zero or more input files. The files can be plain or compressed in a mix of different formats. If the input argument is empty, the script expects piped input from another command.  A ‘-’ at any position in the input list will splice in input from stdin there, which allows arbitrary concatenation of named files with input from stdin.


% \begin{verbatim}
%     kgtk zconcat -o dest.tsv file1.tsv file2.tsv
% \end{verbatim}

\subsubsection{Graph querying and analytics}~\\

%Once the sequence of desired graph transformations has been performed on a KG, KGTK defines a series of operations to query and analyze its contents: %We currently support four modules for this purpose.

\textbf{10.} \texttt{reachable-nodes}: given a set of nodes N and a set of properties P, this operation computes the set of reachable nodes $R$ that contains the nodes that can be reached from a node $n \in N$ via paths containing any of the properties in P. This operation can be seen as a (joint) closure computation over one or multiple properties for a predefined set of nodes. A common application of this operation is to compute a closure over the subClassOf property, which benefits downstream tasks such as entity linking or table understanding. %\DG{Example on when this is useful? For centrality analysis? Community detection?}

% We don't have an instances command and it is not necessary as reachable will compute it.
%\texttt{11.} \texttt{instances}: given a set of nodes C that represent classes in a graph, this operation computes the set of all instances of every member class $c \in C$. Users may also include the instances of the subclasses of $c$, by specifying the \texttt{transitive} option. Any number of classes can be specified as input.%, for example: \texttt{Q13442814,Q12345678}.



% For instance, to obtain all nodes reachable via $p1$ and $p2$ starting from $n1$, $n2$, $n3$:
% \begin{verbatim}
%     kgtk reachable_nodes --property p1,p2 --root n1,n2,n3
% \end{verbatim}

\textbf{11.} The \texttt{connected-components} operation finds all connected components (communities) in a graph (e.g., return all the communities connected via an \texttt{owl:sameAs} edge in a KGTK file). 



% Load a KGTK edges file into Graph-tool. In addition, one can compute general statistics and centrality metrics. The resulting file could be stored to disk in a graph-tool format. 
% Thus, we can run the following command to import an edge file into graph-tool, and compute degrees, PageRank and HITS centrality:

% \begin{verbatim}
%     kgtk gt_loader --directed --degrees --pagerank --hits 
%                   --out result.gt ./data/conceptnet.tsv
% \end{verbatim}

\textbf{12. } The \texttt{text-embeddings} operation computes embeddings for all nodes in a graph by computing a sentence embedding over a lexicalization of the neighborhood of each node. The lexicalized sentence is created based on a template whose simplified version is:
{\footnotesize
\begin{verbatim}
{label-properties}, {description-properties} is a {isa-properties},
has {has-properties}, and {properties:values}.
\end{verbatim}
}

\sloppy
The properties for \texttt{label-properties}, \texttt{description-properties}, \texttt{isa-properties}, \texttt{has-properties}, and \texttt{property-values} pairs are specified as input arguments to the operation. An example sentence is ``Saint David, patron saint of Wales is a human, Catholic priest, Catholic bishop, and has date of death, religion, canonization status, and has place of birth Pembrokeshire''. The sentence for each node is encoded into an embedding using one of 16 currently supported variants of three state-of-the-art language models: BERT, DistilBERT, and RoBERTa. Computing similarity between such entity embeddings is a standard component of modern decision making systems such as entity linking, question answering, or table understanding.%\DG{it would be nice to give a small example of what we can do now that we have the embeddings}

\textbf{13.} The \texttt{graph-statistics} operation computes various graph statistics and centrality metrics. The operation generates a graph summary, containing its number of nodes, edges, and most common relations. In addition, it can compute graph degrees, HITS centrality, and PageRank values. Aggregated statistics (minimum, maximum, average, and top nodes) for these connectivity/centrality metrics are included in the summary, whereas the individual values for each node are represented as edges in the resulting graph. The graph is assumed to be directed, unless indicated differently.
%For instance, \texttt{Q123 in\_degree 6} indicates that the in-degree of the node \texttt{Q123}\footnote{\url{https://www.wikidata.org/wiki/Q123}} is 6. \DG{I removed the example because it doesn't add much}


%\DG{Add that centrality and connectivity are part of the stats}


%  An example command call is:

% \begin{verbatim}
%     kgtk text_embedding --input input_file.csv 
%                         --model bert-large-nli-cls-token 
%                         --property-value label, description 

% \end{verbatim}



%At this moment, KGTK supports the following commands:

\subsection{Composing operations into pipelines}\label{sec:chain}

KGTK has a pipelining architecture based on Unix pipes\footnote{\url{https://linux.die.net/man/7/pipe}} that allows chaining most operations introduced in the previous section by using the standard input/output and the KGTK file format. 
Pipelining increases efficiency by avoiding the need to write files to disk and supporting parallelism allowing downstream commands to process data before upstream commands complete.
%Since the input and output comes in a stream, this mechanism is preferred due to its efficiency (writing intermediate files into disk is no longer necessary). 
We illustrate the chaining operations in KGTK with three examples from our own work. Note that we have implemented a shortcut pipe operator ``/'', which allows users to avoid repeating \texttt{kgtk} in each of their operations. For readability, command arguments are slightly simplified in the paper. Jupyter Notebooks that implement these and other examples can be found online.\footnote{\url{https://github.com/usc-isi-i2/kgtk/tree/master/examples}}   %More extensive examples, tied to real-world use cases, are presented in section \ref{sec:showcase}.
%Extending on UNIX pipes allows us to conveniently chain an arbitrary list of commands.
% enabling composition of tools written in different programming languages. using and producing KGTK files on the 

% Besides the COVID19 and the CSKG use cases, we currently offer five more notebooks in the `examples/` folder, the first three of which correspond to the example scenarios described in the paper. Two of the notebooks can also be run directly online via myBinder (pointers can be found in our README page [2]). In addition, we provide a Docker-based installation in case the reviewer wants to set up the toolkit locally with a single command



\textbf{Example 1}: Alice wants to import the English subset of ConceptNet \cite{speer2016conceptnet} in KGTK format to extract a filtered subset where two concepts are connected with a more precise semantic relation such as \texttt{/r/Causes} or \texttt{/r/UsedFor} (as opposed to weaker relations such as \texttt{/r/RelatedTo}). For all nodes in this subset, she wants to compute text embeddings and store them in a file called \texttt{emb.txt}.

To extract the desired subset, the sequence of KGTK commands is as follows:
{\footnotesize
\begin{verbatim}
kgtk import-conceptnet --english_only conceptnet.csv / \
  filter -p "; /r/Causes,/r/UsedFor,/r/Synonym,/r/DefinedAs,/r/IsA ;" / \
  sort -c 1,2,3 > sorted.tsv
\end{verbatim}
}

To compute embeddings for this subset, she would use \texttt{text-embeddings}:
{\footnotesize
\begin{verbatim}
kgtk text-embeddings --label-properties "/r/Synonym" \
  --isa-properties "/r/IsA" --description-properties "/r/DefinedAs" \
  --property-value "/r/Causes" "/r/UsedFor" \ 
  --model bert-large-nli-cls-token -i sorted.tsv \
  > emb.txt
\end{verbatim}
}


\textbf{Example 2}: Bob wants to extract a subset of Wikidata that contains only edges of the `member of' (P463) property, and strip a set of columns that are not relevant for his use case (\texttt{\$ignore\_col}), such as id and rank. While doing so, Bob would also like to clean any erroneous edges. On the clean subset, he would compute graph statistics, including PageRank values and node degrees. Here is how to perform this functionality in KGTK (after Wikidata is already converted to a KGTK file called \texttt{wikidata.tsv} by \texttt{import-wikidata}):
{\footnotesize
\begin{verbatim}
kgtk filter -p ' ; P463 ; ' /  clean_data / 
    remove-columns -c "$ignore_cols" wikidata.tsv > graph.tsv
kgtk graph-statistics --directed --degrees --pagerank graph.tsv
\end{verbatim}
}


\textbf{Example 3}: Carol would like to concatenate two subsets of Wikidata: one containing occupations for several notable people: Sting, Roger Federer, and Nelson Mandela; and the other containing all `subclass of' (P279) relations in Wikidata. The concatenated file needs to be sorted by subject, after which she would compute the set of reachable nodes for these people via the properties `occupation' (P106) or `subclass of' (P279). To achieve this in KGTK, Carol first needs to extract the two subsets with the \texttt{filter} operation:
{\footnotesize
\begin{verbatim}
kgtk filter -p 'Q8023,Q483203,Q1426;P106;' wikidata.tsv > occupation.tsv
kgtk filter -p ` ; P279 ; ' wikidata.tsv > subclass.tsv
\end{verbatim}
}

Then, she can merge the two files into one, sort it, and compute reachability:
{\footnotesize
\begin{verbatim}
kgtk cat occupation.tsv subclass.tsv / \
     sort -c node1 > sorted.tsv
kgtk reachable-nodes --props P106,P279 --root "Q8023,Q483203,Q1426" \
     sorted.tsv > reachable.tsv
\end{verbatim}
}

%\DG{All these examples should be available online, and we should point to them}

%\DG{Filip, you used countries.tsv and people.tsv. I don't know where these files come from or what they are, and therefore I don't know what am I combining in the example}

\section{Discussion}\label{sec:showcase}
%Was: Resource Adoption, Impact and Limitations

%\textcolor{red}{We should add a small experimentation section about how KGTK shows the use cases we want to support. This is NOt an evaluation, but highlights how KGTK supports our use cases easily. The use cases are necessary because they indicate how the tool has been adopted.}

%With the increasing amount of data and knowledge available in the Web, knowledge graphs are being increasingly used by different users to perform data science. For example, Wikidata has thrived with tutorials\footnote{\url{https://towardsdatascience.com/a-brief-introduction-to-wikidata-bb4e66395eb1}} and visualizations \footnote{\url{https://janakiev.com/slides/data-science-osm-wikidata/#/21}} that allows users accessing its contents. However, integrating and querying these knowledge graphs at scale is becoming increasingly difficult, as we have shown in some of our examples. 

%Simple queries for counting the members of a class will time out in Wiki and even KGs like Wikidata are exploring federation strategies\footnote{\url{https://www.wikidata.org/wiki/Wikidata:Federation\_input}} 

Validating, merging, transforming and analyzing KGs at scale is an open challenge for knowledge engineers, and even more so for data scientists. Complex SPARQL queries often time out on online endpoints, while working with RDF dumps locally takes time and expertise. In addition, popular graph analysis tools do not operate with RDF, making analysis complex for data scientists.

%KGs such as Wikidata time are great resources to browse and query \DG{complete} out with simple data-science queries, such as counting the elements of a target class \DS{In what sense is WD efficient? Can this query be (efficiently) done querying Wikibase directly? If so, the timeout is due to the RDF representation, and not so much to the Wikidata model}. Working with RDF dumps locally also presents problems, due to memory requirements and time to produce an answer. 

%%%%%
%%If we have the experiment, include next paragraph
%%%%%
%To validate this claim, we performed a small experiment by executing a common query of counting the number of unique items in Wikidata that have at least one \texttt{instance of (P31)} statement, in different environments: a SPARQL endpoint, using RDF libraries (rdflib, graphy, and Jena), and, finally, using KGTK. The numbers show that... \DG{@Filip will complete on tuesday} \filip{Amandeep will.}\DG{Loading Wikidata into A tripleStore in a server took more than 10 days. Loading the wd file into graphy takes 5 hours. In kgtk doing th filter takes X hours. In the sparql endpoint, the filter is relatively fast (2 min), but has a limit in the data. 10 min 16 secs for full data}

The KGTK format intentionally does not distinguish attributes or qualifiers of nodes and edges from full-fledged edges. Tools operating on KGTK graphs can instead interpret edges differently when desired.  In the KGTK file format, everything can be a node, and every node can have any type of edge to any other node. To do so in RDF requires adopting more complex mechanisms such as reification, typically leading to efficiency issues. This generality allows KGTK files to be mapped to most existing DBMSs, and to be used in powerful data transformation and analysis tools such as Pandas.\footnote{\url{https://pandas.pydata.org}} %Consequently, it also enables using efficient implementations most appropriate for a given set of tasks.

We believe KGTK will have a significant impact
%has room to make an impact 
within and beyond the Semantic Web community by helping users to easily perform usual data science operations on large KGs. To give an idea, we downloaded Wikidata (truthy statements distribution, 23.2GB\footnote{\url{https://dumps.wikimedia.org/wikidatawiki/entities/latest-truthy.nt.bz2}}) and performed a test of filtering out all Qnodes (entities) which have the P31 property (instance of) in Wikidata. This filter took over 20 hours in Apache Jena and RDFlib. In graphy, the time went down to 4h 15min. Performing the same operation in KGTK took less than 1h 30min. 

\begin{figure}[t!]
    \centering
    \includegraphics[width=\textwidth]{images/covid-visualization}
    \caption{SPARQL query and visualization of the CORD-19 use case, illustrating the use of the Wikidata infrastructure using our KG that includes a subset of Wikidata augmented with new properties such as ``mentions gene'' and ``pagerank''.}
    \label{fig:covid-visualization}
\end{figure}
We have been using KGTK in our own work to integrate and analyze KGs:
%are starting to explore federation strategies\footnote{\url{https://www.wikidata.org/wiki/Wikidata:Federation\_input}}, and
%\pedro{Someone should run an evaluation to count the number of P31 statements in Wikidata using 1) KGTK on our format and the following in the RDF dump 2) rdflib 3) Jena 4) graph. Let's see what the results show, it is possible that the time comparison is favorable to us. 
%\textcolor{red}{Since we are not having an evaluation of the resource, we should highlight its features in different use cases}
%A sample demonstration of the toolkit. Covid use case? Common sense use case? Should showcase the different commands, utility and time required. 
\begin{itemize}
    \item \textbf{CORD-19}: As described in Section \ref{sec:scenario}, we used KGTK to combine extracted information from the papers in the CORD-19 dataset (such as entities of interest) with metadata about them, and general medical and biology knowledge, all found in Wikidata, CTD and the BLENDER datasets. A notebook illustrating the operations used in this use case is available online.\footnote{\url{https://github.com/usc-isi-i2/CKG-COVID-19/blob/dev/build-covid-kg.ipynb}} %First, we created two KGTK files with all edges for all papers and entities present in Wikidata, by using the KGTK \texttt{join} operation. Next, we created a KGTK file with all citations and authors of articles in union. Finally, we created a file of all P31 (`instance of') and P279 (`subclass of') of all node2 of all edges above. Essentially, the COVID-19 use case relies on efficient filtering and joining of several different datasets. Thanks to our effort we were able to find Q-nodes for  139k identifiers which we couldn't have achieved just by using SPARQL. %\DG{We should put this example as a URL online. In addition, what about other means (RDFlib)?}
    Figure~\ref{fig:covid-visualization} shows the CORD-19 KGTK KG loaded in Wikidata SPARQL query interface. The KGTK tools exported the CORD-19 KG to RDF triples in a format compatible with Wikidata. 

\item \textbf{Commonsense Knowledge Graph (CSKG)}: Commonsense knowledge is dispersed across a number of (structured) knowledge sources, like ConceptNet and ATOMIC~\cite{sap2019atomic}. After consolidating several such sources into a single CSKG~\cite{ilievski2020consolidating}, we used KGTK to compute graph statistics (e.g., number of edges or most frequent relations), HITS, PageRank, and node degrees, in order to measure the impact of the consolidation on the graph connectivity and centrality. We also created RoBERTa-based embeddings of the CSKG nodes, which we are currently using for downstream question answering. A notebook illustrating the operations in this use case is available online.\footnote{\url{https://github.com/usc-isi-i2/kgtk/blob/master/examples/CSKG.ipynb}} 

%Specifically, we started by importing existing resources\DG{Which resources?}, i.e., converting them to our KGTK data format. The resulting files were merged with each other by using the \texttt{merge} operation. The merged graph was then cleaned: we removed duplicate nodes and edges, and cleaned dangling edges\DG{How?}. Then, to increase the consistency of the graph, we merged identical nodes (indicated with the `same-as' property). We exported the consolidated CSKG graph, containing 17.2 million edges, as a TSV file \DG{Size}\filip{the file format is not exactly KGTK, so the size might be misleading}. Finally, we used the first three columns to compute RoBERTa-based embeddings (through the \texttt{text\_embedding} operation); and statistics (e.g., number of edges or most frequent relations),  centrality metrics, like HITS, PageRank, and node degrees. \DG{We should say: this example is available in X URL here}
%We compute all statistics for each node, but also generate summaries, indicating: the best connected nodes, the most frequent relations, mean and standard deviation of degree, etc.

%Most of the commands described in this use case are already supported by KGTK; whereas the remaining ones are currently being integrated.
\item \textbf{Integrating and exporting Ethiopian quantity data:}
We are using KGTK to create a custom extension of Wikidata with data about Ethiopia,\footnote{\url{https://datamart-upload.readthedocs.io/en/latest/REST-API-tutorial/}} by integrating quantity indicators like crime, GDP, population, etc.%\footnote{For example, \url{https://data.worldbank.org/indicator}} 
\end{itemize}
%Due to performance issues, we were required to deliver the result (for Ethiopia in particular) in a relational database. Thanks to KGTK we converted all quantity properties from Wikidata to KGTK, computed summary statistics about them, and loaded the KGTK file into a PostGres dataset using the export function. By transforming from RDF to KGTK we \DG{saved X amount of space and were able to do so? Can we provide any pointers so the reviewers can verify this?} \DG{ASK JUN about metrics. Are they done?}

%Requirement: they have to have the dump in a laptop, and postgres.

%\pedro{Daniel knows what we are doing there, it is a Datamart scenario where we are converting all quantity edges from Wikidata to KGTK format, computing summary statistics about them, and loading KGTK in postgres. The clients do not want a triple store as the AWS cost for hosting Wikidata is much higher and performance is much worse (their benchmarks showed 100x difference in the queries that they care about)}


% 3. something around wikidata \textcolor{red}{Who can provide more information? We have indicators but I am not sure which concrete Wikidata use case we are referring to here.}

% See https://drive.google.com/drive/u/1/folders/1KCya95IiL7GkrGdtSHrvch6epWOZUr9D for a full example with queries and timestamps (DG to tidy up)


The heterogeneity of these cases shows how KGTK can be adopted for multi-purpose data-science operations over KGs, independently of the domain. The challenges described in them are common in data integration and data science. Given the rate at which KGs are gaining popularity, we expect KGTK to fill a key gap faced by many practitioners wanting to use KGs in their applications.

The primary limitation of KGTK lies in its functionality coverage. The main focus so far has been on supporting basic operations for manipulating KGs, and therefore KGTK does not yet incorporate powerful browsing and visualization tools, or advanced tools for KG identification tasks such as link prediction, entity resolution, and ontology mapping.
Since KGTK is proposed as a new resource, we have no usage metrics at the time of writing this paper.


%Regarding limitations, current work has emphasized the data processing pipeline\DG{I don't understand the previous sentence}\filip{+1}, and KGTK does not yet incorporate powerful browsing and visualization tools, or advanced tools for KG identification tasks such as link prediction, entity resolution and ontology mapping.

%KGTK prioritizes efficient result delivery, and hence we do not write the intermediate files of a pipeline to disk. This makes it difficult to inspect errors in large pipelines, although this problem can be easily circumvented by executing the piped operations independently in case of need.  


%Another limitation is that KGTK does not maintain a representation of the KG in memory, and hence to create two different subsets of a knowledge graph it is needed to load the original knowledge graph again.

%\DG{Limitation: creating subsets of a single input is problematic (trade off for being streamed)}

%Discussion here about potential adoption, justifying why KGTK is needed by the community. A brief discussion of the limitations is also in place




%\filip{as pointed by YY, a real limitation we can't circumvent is python and installing dependencies - “requires python (or its virtual environment) and installing dependency packages”}\DG{Not really a limitation.}

%\subsection{Limitations}




%Limitations should also be discussed in this section: does it support joins? What happens with multiple edge files? 

%Limitations from COVID19: 

%\textcolor{red}{DG: Say that the framework supports Unix only}


%\textcolor{red}{In the second case, authors should defend the claim of potential adoption by providing evidence of discussion in fora, mailing lists, and the like.}


\section{Related Work}\label{sec:relWork}
%the convenient online querying and expressiveness of SPARQL is paid by difficulty to execute even the simplest of these operations
Many of the functionalities in KGTK for manipulating and transforming KGs (i.e., join operations, filtering entities, general statistics, and node reachability) can be translated into queries in SPARQL. However, the cost of these queries over large endpoints is often too high, and they will time out or take too long to produce a response. In fact, many SPARQL endpoints have been known to have limited availability and slow response times for many queries~\cite{buil2013sparql}, leaving no choice but to download their data locally for any major KG manipulation. 
Additionally, it is unclear how to extend SPARQL to support functionalities  such as computing embeddings or node centrality.
%\footnote{We attempted to obtain a simple count of all items in Wikidata from its online SPARQL query, which resulted in a timeout. In the cited paper from 2013, the authors tried a related query of retrieving 100k entries and found that only 13.3\% of all endpoints were able to fulfill the request successfully.}
%\filip{we might need a newer citation here too, or a usage example from us.} 
A scalable alternative to SPARQL is Linked Data Fragments (LDF)~\cite{verborgh2014web}. The list of native operations in LDF boils down to triple pattern matching, resembling our proposed \texttt{filter} operation. However, operations like merging and joining are not trivial in LDF, while more complex analytics and querying, like embedding computation, are not supported. %In addition, both SPARQL and LDF do not support qualifiers.

Other works have proposed offline querying. LOD Lab~\cite{beek2016lod} and LOD-a-lot~\cite{fernandez2017lod}  combine LDF with an efficient RDF compression format, called Header Dictionary Triples (HDT)~\cite{martinez2012exchange,fernandez2018hdtq}, in order to store a LOD dump of 30-40B statements. Although the LOD Lab project also employed mature tooling, such as Elastic Search and bash operations, to provide querying over the data, the set of available operations is restricted by employing LDF as a server, as native LDF only supports pattern matching queries. %\DG{Filip, I don't understand the last sentence. Do you mean that LDF only supports querying operations?} 
The HDT compression format has also been employed by other efforts, such as sameAs.cc \cite{beek2018sameas}, which performs closure and clustering operations over half a billion identity (same-as) statements. However, HDT cannot be easily used by existing tools (e.g., graph-tool or pandas), and it does not describe mechanisms for supporting qualifiers (except for using reification on statements, which complicates the data model). 

The recent developments towards supporting triple annotations with  RDF*~\cite{hartig2017rdf} provide support for qualifiers; yet, this format is still in its infancy and we expect it to inherit the challenges of RDF, as described before.%has some unaddressed limitations. %Namely, RDF* uses the described triple as subject, and therefore having multiple qualifier pairs using the same subject can lead to inconsistencies. For instance, given a male population of 200 in 2019, and a female population of 200 in 2020 for a place, RDF* cannot know whether the female population was recorded in 2019 or 2020. Being otherwise very similar to RDF, we expect that this format inherits the challenges of RDF described before.  \DG{Here is the example, although I don't know we should include it: German male population in  2019 is 200, female population in 2020 is 200. We don't know if female population was recorded in 2019 or 2020 }\DS{Wouldn't RDF* also suffer from the same performance problems (or challenges) of RDF representations already mentioned in the introduction?}\DG{Yes, most likely}\filip{i included your comments inline. I am unsure if RDF* deserves a paragraph.}
%this format cannot represent two or more qualifiers with differing values and it has not been integrated in existing query tools within the Semantic Web or elsewhere.  

%The reasons why we opted for a neutral format like TSV instead of HDT are as follows. Primarily, . Practically none of these is currently implemented for HDT. A secondary reason is that HDT does not support qualifiers, which are common in modern graphs like Wikidata. 

Several RDF libraries exist for different programming languages, such as RDFLib in Python, graphy in JavaScript, and Jena in Java. The scope of these libraries is different from KGTK, as they focus on providing the building blocks for creating RDF triples, rather than a set of operators to manipulate and analyze large KGs (validate, merge, sort, statistics, etc.).
%however, we note that KGTK is run from the command line (no programming language specified), supports qualifiers, and leverages a mature set of well-tested tools.

%If there are no existing Semantic Web tools to execute our pipelines with, then why not at least use RDF as a data representation framework? In particular, the aforementioned 

%  that Neo4J or graph-tool might be a best of breed tool for certain tasks,
%allow quick and intuitive traversal over KGs
%\textit{Property graph limitations} - Neo4J
%Neo4J is with \textit{Bulk/table-oriented queries} - W

Outside of the Semantic Web community, prominent efforts perform graph operations in graph databases like Neo4J or libraries like graph-tool, which partially overlap with the operations included in KGTK. We acknowledge the usefulness of these tools for tasks like pattern matching and graph traversal, and therefore we provide an export to their formats to enable users to take advantage of those capabilities. However, these tools also have limitations. First, Neo4J ``only allows one value per attribute property'' and it ``does not currently support queries involving joins or lookups on any information associated with edges, including edge ids, edge types, or edge attributes''~\cite{hernandez2016querying}. The KGTK representation does not have these limitations, and the tasks above can be performed using KGTK commands or via export to SPARQL and graph-tool. Second, while Neo4J performs very well on traversal queries, it is not optimized to run on bulk, relational queries, like ``who are the latest reported sports players?'' Similarly,~\cite{hernandez2016querying} shows that Neo4J performs worse and times out more frequently than Postgres and Virtuoso on atomic queries and basic graph patterns, even after removing the labels to improve efficiency. KGTK supports bulk and simple table queries, complex queries are handled by exporting to RDF and Postgres. 
% Finally, while Neo4J's query planning has many benefits, it might not be suited for graphs like Wikidata~\cite{hernandez2016querying}

Graph-tool provides rich functionality and can natively support property graphs. However, it needs to be integrated with other tools for operations like computation of embeddings or relational data operations, requiring additional expertise. 
%\DS{although the focus of ISWC is not Relational DBs, some people may question if we are not "reinventing" the relational (tabular) data model. Mapping KGTK files to a relational DBs should result in efficient implementations at least for certain types of queries (non-graph related). We already have mappings to PostGRES, so we could  mention some performance evaluation of different implementations in ongoing/future work.}\DG{Please feel free to accommodate the paragraph above to reflect this}\DS{I'm asking if we have some evidence of this, otherwise we should put in future work}\DG{I am removing this comment because I do not understand what you mean to add here, sorry}

Finally, the KGX toolkit\footnote{\url{https://github.com/NCATS-Tangerine/kgx}} has a similar objective as KGTK, but it is scoped to process KGs aligned with the Biolink Model, a datamodel describing biological entities using property graphs. Its set of operations can be regarded as a subset of the operations supported by KGTK. To the best of our knowledge, there is no existing toolkit with a comprehensive set of operations for validating, manipulating, merging, and analyzing knowledge graphs comparable to KGTK. 
%\filip{the related work is a bit too Amsterdam/Vienna-centered :P Additional refs would be nice.}\DG{I haven't found more..} 


%\DS{Here are the docs, can someone double check my statement about the operations available?  \url{https://kgx.readthedocs.io/en/latest/}. It is used within \url{https://github.com/Knowledge-Graph-Hub/kg-covid-19/wiki}. I'm not really sure how dependent the operations are on the specific model used. Furthermore, it seems to be in-memory only, but I'm also not sure. Here is a description of what they did \url{https://lists.w3.org/Archives/Public/www-archive/2020May/att-0002/01-part}}



% \textbf{HDT and extensions}

% RDF compression is a widely used tool for reducing data. Approaches like LOD a lot and LOD laundromat use this in combination with TPF to serve data. However, they do not allow performing operations quickly on the downloaded data. They create indices. What is the difference between the KGTK data format and HDTQ?
% A clear distinction seems that HDT does not have support for qualifiers. Also, it does not look like anyone has worked on operations for HDT (header-dictionary-triples). I would say that an additional distinction is that the I/O in KGTK are tables (tsv). 
% I believe there are some ongoing efforts for doing queries over compressed RDF.

% Relevant links: 
% \url{https://link.springer.com/referenceworkentry/10.1007\%2F978-3-319-63962-8_62-1}

% \url{https://aic.ai.wu.ac.at/qadlod/docs/eswc2018.pdf}

% https://www.w3.org/Submission/HDT-Implementation/
% and https://www.w3.org/Submission/HDT/

% My former colleagues in Amsterdam worked on query mechanisms over HDT within the LOD Laundromat project and Lod-a-lot (both via LDF) http://lod-a-lot.lod.labs.vu.nl/

% this presentation is comprehensive: \url{http://ifs.tuwien.ac.at/keystone.school/slides/ManagingCompressed/2.1_RDFCompression_HDT.pdf}

% \textbf{RDF*}

% Should discuss RDF* and the approach for defining qualifiers, but this is early work. RDF* also has some problems: For example, since the triple is used as source, qualifiers with the same range could be an issue. For example, imagine that the male and female population of a place is 200. If male or female is the qualifier, it wouldn't work.


% sparql, if needed: \url{https://link.springer.com/content/pdf/10.1007\%2F978-3-642-41338-4\_18.pdf}
% (but from 2013) low availability, slow joins, partial interoperability, ..



\section{Conclusions and Future Work}\label{sec:conclusions}

Performing common graph operations on large KGs is challenging for data scientists and knowledge engineers. Recognizing this gap, in this paper we presented the Knowledge Graph ToolKit (KGTK): a data science-centric toolkit to represent, create, transform, enhance, and analyze KGs. KGTK represents graphs in tabular format, and leverages popular libraries developed for data science applications, enabling a wide audience of researchers and developers to easily construct KG pipelines for their applications. KGTK currently supports thirteen common operations, including import/export, filter, join, merge, computation of centrality, and generation of text embeddings. We are using KGTK in our own work for three real-world scenarios which benefit from integration and manipulation of large KGs, such as Wikidata and ConceptNet.%\filip{add something about the comparison numbers once we have them.}


\begin{figure}[t!]
    \centering
    \includegraphics[width=0.9\textwidth]{images/squid.png}
    \caption{SQID visualization of local KGTK data (using the CORD-19 example).}
    \label{fig:squid}
\end{figure}

KGTK is actively under development, and we are expanding it with new operations. Our CORD-19 use case indicated the need for a tool to create new edges, which will also be beneficial in other domains with emerging information and many  long-tail/emerging new entities. Our commonsense KG use case, which combines a number of initially disconnected graphs, requires new operations that will perform de-duplication of edges in flexible ways. Additional import options are needed to support knowledge sources in custom formats, while new export formats will allow us to leverage a wider span of libraries, e.g., the GraphViz format enables using existing visualization tooling. 
%Whereas the use cases discussed in this paper have driven the requirements for existing and new operations, 
We are also looking at converting other existing KGs to the KGTK format, both to enhance existing KGTK KGs, and to identify the need for additional functionality. In the longer term, we plan to extend the toolkit to support more complex KG operations, such as entity resolution, link prediction, and entity linking.


We are also working on enhancing further the user experience with KGTK. 
%One potential improvement is recording provenance of the piping operations to inspect errors. We are also adapting KG exploration tools like SQID\footnote{\url{https://tools.wmflabs.org/sqid/}} to allow users to inspect the contents of their knowledge graphs locally.
We are adapting the SQID\footnote{\url{https://tools.wmflabs.org/sqid/}} KG browser (as shown in Figure \ref{fig:squid}), which is part of the Wikidata tool ecosystem. To this end, we are using the KGTK export operations to convert any KGTK KG to Wikidata format (JSON and RDF as required by SQID), and are modifying SQID to remove its dependencies on Wikidata. The current prototype can browse arbitrary KGTK files. Remaining work includes computing the KG statistics that SQID requires, and 
automating deployment of the Wikidata infrastructure for use with KGTK KGs.


\section*{Acknowledgements}
This material is based on research sponsored by Air Force Research Laboratory under agreement number FA8750-20-2-10002. The U.S. Government is authorized to reproduce and distribute reprints for Governmental purposes notwithstanding any copyright notation thereon. The views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the official policies or endorsements, either expressed or implied, of Air Force Research Laboratory or the U.S. Government.

% \DS{We could mention adding an interface layer (e.g., SQUID) to support exploration of KGTK KGs} \filip{Sounds reasonable; i don't know the details of that work though.}
%We also aim to allow additional installation options, besides the current way of installing KGTK within a conda virtual environment.


%By adopting the KGTK tools in three of our running projects, we are already discovering its first limitations. Here we discuss two aspects which we intend to improve in the future: commands and environments. 


%KGTK aims to support *nix systems.\footnote{Windows 10 users would be able to run KGTK within a subsystem: \url{https://docs.microsoft.com/en-us/windows/wsl/install-win10}. } Specifically, so far we have tested its functions on MacOS and Ubuntu. Regarding installation, KGTK can be installed by following our instructions on github or as a \texttt{PyPI} package\footnote{\url{https://pypi.org/project/kgtk/}}. However, its preferred installation requires installing conda and activating a virtual environment. We aim to allow alternative installation methods in the future.

%\DG{State that we are working on import/export formats: As with the \texttt{import} function, we are currently implementing a number of additional formats, such as GraphViz which will allow us to make use of existing visualization tooling}

\bibliographystyle{splncs04}

\bibliography{bib}
\end{document}
