
We begin by deriving a formula for the asymptotic variance of observables
of the form 
\[
f(q)=q\cdot Kq+l\cdot q-\Tr K,
\]
with $K\in\mathbb{R}_{sym}^{d\times d}$ and $l\in\mathbb{R}^{d}$.
Note that the constant term is chosen such that $\widehat{\pi}(f)=0$.
The following calculations are very much along the lines of \cite[Section  4]{duncan2016variance}. Since the Hessian of $V$ is bounded and the target measure $\pi$ is Gaussian, Assumption \ref{ass:bounded+Poincare} is satisfied and exponential decay of the semigroup $(P_t)_{t\ge0}$ as in \eqref{eq:hypocoercive estimate} follows by  Theorem \ref{theorem:Hypocoercivity}. According to Lemma \ref{lemma:variance}, the
asymptotic variance is then given by 
\begin{equation}
\sigma_{f}^{2}=\langle \chi,f\rangle_{L^{2}(\widehat{\pi})},
\end{equation}
where $\chi$ is the solution to the Poisson equation 
\begin{equation}
\label{eq:Poisson equation Gauss}
-\mathcal{L}\chi=f,\quad\widehat{\pi}(\chi)=0.
\end{equation}
Recall that 
\[
\mathcal{L}=-Bx\cdot\nabla+\nabla^{T}Q\nabla=-x\cdot A\nabla+\nabla^{T}Q\nabla
\]
is the generator as in (\ref{eq:OU generator}), where for later convenience
we have defined $A=B^{T}$, i.e.
\begin{equation}
A=\left(\begin{array}{cc}
-\mu J & I\\
-I & \gamma I -\nu J
\end{array}\right) \in \mathbb{R}^{2d \times 2d}.\label{eq:Amatrix}
\end{equation}
In the sequel we will solve (\ref{eq:Poisson equation Gauss}) analytically.  First, we introduce the notation 
\[
\bar{K}=\left(\begin{array}{cc}
K & \boldsymbol{0}\\
\boldsymbol{0} & \boldsymbol{0}
\end{array}\right)\in\mathbb{R}^{2d\times2d}
\]
and 
\[
\bar{l}=\left(\begin{array}{c}
l\\
\boldsymbol{0}
\end{array}\right)\in\mathbb{R}^{2d},
\]
such that by slight abuse of notation $f$ is given by 
\[
f(x)=x\cdot\bar{K}x+\bar{l}\cdot x-\Tr\bar{K}.
\]
By uniqueness (up to a constant) of the solution to the Poisson equation \eqref{eq:Poisson equation Gauss} and
linearity of $\mathcal{L}$, $g$ has to be a quadratic polynomial,
so we can write 
\[
g(x)=x\cdot Cx+D\cdot x-\Tr C,
\]
where $C\in\mathbb{R}_{sym}^{2d\times2d}$ and $D\in\mathbb{R}^{2d}$ (notice that $C$ can be chosen to be symmetrical since $x \cdot C x$ does not depend on the antisymmetric part of $C$).
Plugging this ansatz into (\ref{eq:Poisson equation Gauss}) yields
\[
-\mathcal{L}g(x)=x\cdot A\big(2Cx+D\big)-\gamma\Tr_{p}C=x\cdot\bar{K}x+\bar{l}\cdot x-\Tr\bar{K},
\]
where 
\[
\Tr_{p}C=\sum_{i=n+1}^{2n}C_{ii}
\]
denotes the trace of the momentum component of $C$. Comparing different
powers of $x$, this leads to the conditions
\begin{subequations}
\begin{eqnarray}
AC+CA^{T} & =\bar{K},
\label{eq:Lyapunov equation}\\
AD & =\bar{l},
\label{eq:linear condition}\\
\gamma\Tr_{p}C & =\Tr\bar{K}.
\label{eq:trace condition}
\end{eqnarray}
\end{subequations}
Note that (\ref{eq:trace condition}) will be satisfied eventually
by existence and uniqueness of the solution to (\ref{eq:Poisson equation Gauss}).
Then, by the calculations in \cite{duncan2016variance}, the asymptotic variance is given by 
\begin{equation}
\sigma_{f}^{2}=2\Tr(C\bar{K})+D\cdot\bar{l}.
\label{eq:Gaussian asymvar}
\end{equation}

\begin{proof}[of Proposition \ref{thm: local quadratic observable}]. According to
	(\ref{eq:Gaussian asymvar}) and (\ref{eq:Lyapunov equation}), the
	asymptotic variance satisfies 
	\[
	\sigma_{f}^{2}=2\Tr(C\bar{K}),
	\]
	where the matrix $C$ solves 
	\begin{equation}
	AC+CA^{T}=\bar{K}\label{eq: lyap equation}
	\end{equation}
	and $A$ is given as in (\ref{eq:Amatrix}). We will use the notation
	\[
	C(\mu,\nu)=\left(\begin{array}{cc}
	C_{1}(\mu,\nu) & C_{2}(\mu.\nu)\\
	C_{2}^{T}(\mu.\nu) & C_{3}(\mu,\nu)
	\end{array}\right)
	\]
	and the abbreviations $C(0):=C(0,0)$, $C^{\mu}(0):=\partial_{\mu}C\vert_{\mu,\nu=0}$
	and $C^{\nu}(0):=\partial_{\nu}C\vert_{\mu,\nu=0}$.  Let us first determine $C(0)$, i.e. the solution to the equation
	\[
	\left(\begin{array}{cc}
	\boldsymbol{0} & I\\
	-I & \gamma I
	\end{array}\right)C(0)+C(0)\left(\begin{array}{cc}
	\boldsymbol{0} & I\\
	-I & \gamma I
	\end{array}\right)^{T}=\left(\begin{array}{cc}
	K & \boldsymbol{0}\\
	\boldsymbol{0} & \boldsymbol{0}
	\end{array}\right).
	\]
	This leads to the following system of equations,
	\begin{subequations}
	\begin{eqnarray}
	C_{2}(0)+C_{2}(0)^{T} & =K,
	\label{eq:C(0) 1}\\
	-C_{1}(0)+\gamma C_{2}(0)+C_{3}(0) & =\boldsymbol{0},
	\label{eq:C(0) 2}\\
	-C_{1}(0)+\gamma C_{2}(0)^{T}+C_{3}(0) & =\boldsymbol{0},
	\label{eq:C(0) 3}\\
	-C_{2}(0)-C_{2}(0)^{T}+2\gamma C_{3}(0) & =\boldsymbol{0}.\\ \label{eq:C(0) 4}
	\end{eqnarray}
	\end{subequations}
	Note that equations (\ref{eq:C(0) 2}) and (\ref{eq:C(0) 3}) are
	equivalent by taking the transpose. Plugging (\ref{eq:C(0) 1}) into
	(\ref{eq:C(0) 4}) yields 
	\begin{equation}
	C_{3}(0)=\frac{1}{2\gamma}K.\label{eq:C3(0) result}
	\end{equation}
	Adding (\ref{eq:C(0) 2}) and (\ref{eq:C(0) 3}), together with (\ref{eq:C(0) 1})
	and (\ref{eq:C3(0) result}) leads to 
	\[
	C_{1}(0)=\frac{1}{2\gamma}K+\frac{\gamma}{2}K.
	\]
	Solving (\ref{eq:C(0) 2}) we obtain, 
	\[
	C_{2}(0)=\frac{1}{2}K,
	\]
	so that
	\begin{equation}
	C(0)=\left(\begin{array}{cc}
	\frac{1}{2\gamma}K+\frac{\gamma}{2}K & \frac{1}{2}K\\
	\frac{1}{2}K & \frac{1}{2\gamma}K
	\end{array}\right).\label{eq:C(0) result}
	\end{equation}
	Taking the $\mu$-derivative of (\ref{eq: lyap equation}) and setting
	$\mu=\nu=0$ yields 
	\begin{equation}
	A^{\mu}(0)C(0)+A(0)C^{\mu}(0)+C^{\mu}(0)A(0)^{T}+C(0)A^{\mu}(0)^{T}=\boldsymbol{0}.\label{eq:mu lyap}
	\end{equation}
	Notice that 
	\begin{align*}
	A^{\mu}(0)C(0)+C(0)A^{\mu}(0)^{T}\\
	=\left(\begin{array}{cc}
	-J & \boldsymbol{0}\\
	\boldsymbol{0} & \boldsymbol{0}
	\end{array}\right)C(0)+C(0)\left(\begin{array}{cc}
	J & \boldsymbol{0}\\
	\boldsymbol{0} & \boldsymbol{0}
	\end{array}\right)\\
	=\left(\begin{array}{cc}
	\big(\frac{1}{2\gamma}+\frac{\gamma}{2}\big)[K,J] & -\frac{1}{2}JK\\
	\frac{1}{2}KJ & \boldsymbol{0}
	\end{array}\right).
	\end{align*}
	With computations similar to those in the derivation of (\ref{eq:C(0) result})
	(or by simple substitution), equation (\ref{eq:mu lyap}) can be solved
	by 
	\begin{equation}
	C^{\mu}(0)=\left(\begin{array}{cc}
	-\big(\frac{\gamma^{2}}{4}+\frac{1}{4\gamma^{2}}+\frac{1}{4}\big)[K,J] & \frac{1}{2\gamma}JK-\frac{\gamma}{4}[K,J]\\
	-\frac{1}{2\gamma}KJ-\frac{\gamma}{4}[K,J] & -\big(\frac{1}{4\gamma^{2}}+\frac{1}{4}\big)[K,J]
	\end{array}\right).\label{eq:C^mu}
	\end{equation}
	We employ a similar strategy to determine $C^{\nu}(0)$: Taking the
	$\nu$-derivative in equation (\ref{eq: lyap equation}), setting
	$\mu=\nu=0$ and inserting $C(0)$ and $A(0)$ as in (\ref{eq:C(0) result})
	and (\ref{eq:Amatrix}) leads to the equation
	\[
	\left(\begin{array}{cc}
	\boldsymbol{0} & I\\
	-I & \gamma I
	\end{array}\right)C^{\nu}(0)+C^{\nu}(0)\left(\begin{array}{cc}
	\boldsymbol{0} & I\\
	-I & \gamma I
	\end{array}\right)=\left(\begin{array}{cc}
	\boldsymbol{0} & -\frac{1}{2}KJ\\
	\frac{1}{2}JK & -\frac{1}{2\gamma}[K,J]
	\end{array}\right),
	\]
	which can be solved by 
	\begin{equation}
	C^{\nu}(0)=\left(\begin{array}{cc}
	\big(-\frac{1}{4\gamma^{2}}+\frac{1}{4}\big)[K,J] & \frac{1}{\gamma}\big(-\frac{1}{2}KJ+\frac{1}{4}[K,J]\big)\\
	\frac{1}{\gamma}\big(\frac{1}{2}KJ-\frac{1}{4}[K,J]\big) & -\frac{1}{4\gamma^{2}}[K,J]
	\end{array}\right).\label{eq:C^nu}
	\end{equation}
	Note that $\Tr (C\bar{K})=\Tr(C_{1}K)$, and so 
	\begin{alignat*}{1}
	\partial_{\mu}\Theta\vert_{\mu,\nu=0} & =2\Tr(C_{1}^{\mu}(0)K)=\\
	& =-\big(\frac{\gamma^{2}}{4}+\frac{1}{4\gamma^{2}}+\frac{1}{4}\big)\cdot\Tr([K,J]K)=0,
	\end{alignat*}
	since clearly $\Tr([K,J],K)=\Tr(KJK)-\Tr(JK^{2})=0$. In the same
	way it follows that 
	\[
	\partial_{\nu}\Theta\vert_{\mu,\nu=0}=0,
	\]
	proving (\ref{eq:gradTheta}). 
	\\\\
	Taking the second $\mu$-derivative of (\ref{eq: lyap equation})
	and setting $\mu=\nu=0$ yields
	\[
	2A^{\mu}(0)C^{\mu}(0)+A(0)C^{\mu\mu}(0)+C^{\mu\mu}(0)A(0)^{T}+2C^{\mu}(0)A^{\mu}(0)^{T}=\boldsymbol{0},
	\]
	employing the notation $C^{\mu\mu}(0)=\partial_{\mu}^{2}C\vert_{\mu,\nu=0}$
	and noticing that $\partial_{\mu}^{2}A=0$. Using (\ref{eq:C^mu})
	we calculate
	\[
	A^{\mu}(0)C^{\mu}(0)+C^{\mu}(0)A^{\mu}(0)^{T}=\left(\begin{array}{cc}
	\big(\frac{\gamma^{2}}{4}+\frac{1}{4\gamma^{2}}+\frac{1}{4}\big)[J,[K,J]] & -\frac{1}{2\gamma}J^{2}K+\frac{\gamma}{4}J[K,J]\\
	-\frac{1}{2\gamma}KJ^{2}-\frac{\gamma}{4}[K,J]J & \boldsymbol{0}
	\end{array}\right).
	\]
	As before, we make the ansatz
	\[
	C^{\mu\mu}(0)=\left(\begin{array}{cc}
	C_{1}^{\mu\mu}(0) & C_{2}^{\mu\mu}(0)\\
	\left(C_{2}^{\mu\mu}(0)\right)^T & C_{3}^{\mu\mu}(0)
	\end{array}\right),
	\]
	leading to the equations
	\begin{subequations}
	\begin{eqnarray}
	C_{2}^{\mu\mu}(0)+C_{2}^{\mu\mu}(0)^{T} & = &-\big(\frac{\gamma^{2}}{4}+\frac{1}{4\gamma^{2}}+\frac{1}{4}\big)[J,[K,J]]\label{eq:C''1}\\
	-C_{1}^{\mu\mu}(0)+\gamma C_{2}^{\mu\mu}(0)+C_{3}^{\mu}(0) & =&\frac{1}{\gamma}J^{2}K-\frac{\gamma}{2}J[K,J]\label{eq:C''2}\\
	-C_{1}^{\mu\mu}(0)+\gamma C_{2}^{\mu\mu}(0)^{T}+C_{3}^{\mu}(0) & =&\frac{1}{\gamma}KJ^{2}+\frac{\gamma}{2}[K,J]J\label{eq:C''3}\\
	-C_{2}^{\mu\mu}(0)-C_{2}^{\mu\mu}(0)^{T}+2\gamma C_{3}^{\mu\mu}(0) & =&\boldsymbol{0}.
	\label{eq:C''4}
	\end{eqnarray}
	\end{subequations}
	Again, (\ref{eq:C''2}) and (\ref{eq:C''3}) are equivalent by taking
	the transpose. Plugging (\ref{eq:C''1}) into (\ref{eq:C''4}) and
	combing with (\ref{eq:C''2}) or (\ref{eq:C''3}) gives
	\[
	C_{1}^{\mu\mu}(0)=\big(\frac{\gamma}{4}+\frac{1}{4\gamma^{3}}+\frac{\gamma^{3}}{4}\big)(2JKJ-J^{2}K-KJ^{2})-\frac{1}{\gamma}JKJ.
	\]
	Now 
	\[
	\partial_{\mu}^{2}\Theta\vert_{\mu,\nu=0}=2\Tr(C_{1}^{\mu\mu}(0)K)=-(\gamma+\frac{1}{\gamma^{3}}+\gamma^{3})\big(\Tr(JKJK)-\Tr(J^{2}K^{2})\big)-\frac{2}{\gamma}\Tr(JKJK)
	\]
	gives the first part of (\ref{eq:HessTheta}). We proceed in the same
	way to determine $C_{1}^{\nu\nu}(0)$. Analogously, we get 
	\[
	A^{\nu}(0)C^{\nu}(0)+C^{\nu}(0)A^{\nu}(0)^{T}=\left(\begin{array}{cc}
	\boldsymbol{0} & \frac{1}{\gamma}(KJ^{2}-\frac{1}{2}[K,J]J)\\
	\frac{1}{\gamma}(JKJ-\frac{1}{2}J[K,J]) & \frac{1}{2\gamma^{2}}([K,J]J-J[K,J])
	\end{array}\right).
	\]
	Solving the resulting linear matrix system (similar to (\ref{eq:C''1})-(\ref{eq:C''4}))
	results in
	\[
	C_{1}^{\nu\nu}(0)=\big(\frac{1}{4\gamma^{3}}-\frac{1}{4\gamma}\big)(KJ^{2}+J^{2}K)-\big(\frac{1}{2\gamma^{3}}+\frac{1}{2\gamma}\big)JKJ,
	\]
	leading to 
	\[
	\partial_{\nu}^{2}\Theta\vert_{\mu,\nu=0}=2\Tr(C_{1}^{\nu\nu}(0)K)=\big(\frac{1}{\gamma^{3}}-\frac{1}{2\gamma}\big)\Tr(J^{2}K^{2})\big)-\big(\frac{1}{2\gamma^{3}}+\frac{1}{2\gamma}\big)\Tr(JKJK).
	\]
	To compute the cross term $C_{1}^{\mu\nu}(0)$ we take the mixed derivative
	$\partial_{\mu\nu}^{2}$ of (\ref{eq: lyap equation}) and set $\mu=\nu=0$
	to arrive at 
	\[
	A^{\mu}(0)C^{\nu}(0)+A^{\nu}(0)C^{\mu}(0)+A(0)C^{\mu\nu}(0)+C^{\mu\nu}(0)A(0)^{T}+C^{\mu}(0)A^{\nu}(0)^{T}+C^{\nu}(0)A^{\mu}(0)^{T}=\boldsymbol{0}.
	\]
	Using $\eqref{eq:C^mu}$ and (\ref{eq:C^nu}) we see that 
	\begin{multline*}
	A^{\mu}(0)C^{\nu}(0)+A^{\nu}(0)C^{\mu}(0)+C^{\mu}(0)A^{\nu}(0)^{T}+C^{\nu}(0)A^{\mu}(0)^{T}\\
	=\left(\begin{array}{cc}
	\big(\frac{1}{4\gamma^{2}}-\frac{1}{4}\big)[J,[K,J]] & \frac{1}{\gamma}JKJ-\frac{1}{4\gamma}J[K,J]-\frac{\gamma}{4}[K,J]J\\
	\frac{1}{2\gamma}JKJ+\frac{1}{2\gamma}KJ^{2}+\frac{\gamma}{4}J[K,J]-\frac{1}{4\gamma}[K,J]J & \big(\frac{1}{4\gamma^{2}}+\frac{1}{4}\big)[J,[K,J]]
	\end{array}\right).
	\end{multline*}
	The ensuing linear matrix system yields the solution
	\[
	C_{1}^{\mu\nu}(0)=\big(-\frac{1}{4\gamma^{3}}+\frac{\gamma}{4}-\frac{1}{4\gamma}\big)[J,[K,J]]+\frac{1}{\gamma}JKJ,
	\]
	leading to 
	\begin{equation}
	\partial_{\mu\nu}^{2}\Theta\vert_{\mu,\nu=0}=2\Tr(C_{1}^{\mu\nu}(0)K)=\big(\frac{1}{\gamma^{3}}+\frac{1}{\gamma}-\gamma\big)\Tr(J^{2}K^{2})+\big(-\frac{1}{\gamma^{3}}+\frac{1}{\gamma}+\gamma\big)\Tr(JKJK).
	\end{equation}
	This completes the proof.
	\qed 
    \end{proof}
\begin{proof}
	[Proof of Proposition \ref{thm:linear_full_J}] By (\ref{eq:linear condition})
	and (\ref{eq:Gaussian asymvar}) the function $\Theta$ satisfies
	\[
	\Theta(\mu,\nu)=\bar{l}\cdot A^{-1}\bar{l}.
	\]
	Recall the following formula for blockwise inversion of matrices using the Schur complement:
	\begin{equation}
	\left(\begin{array}{cc}
	U & V\\
	W & X
	\end{array}\right)^{-1}=\left(\begin{array}{cc}
	(U-VX^{-1}W)^{-1} & \ldots\\
	\ldots & \ldots
	\end{array}\right),\label{eq: blockwise inversion}
	\end{equation}
	provided that $X$ and $U-VX^{-1}W$ are invertible. Using this, we obtain 
	\[
	\Theta(\mu,\nu)=l\cdot\big(-\mu J+(\gamma-\nu J)^{-1}\big)l.
	\]
	Taking derivatives, setting $\mu=\nu=0$ and using the fact that $J^{T}=-J$
	leads to the desired result.
	\qed
\end{proof}

\begin{lemma}
	\label{lem:basic_inequalities}
	The following holds: 
	\begin{enumerate}[label=(\alph*)]
		\item \label{it:gaussian_lem1}$\gamma-\frac{4}{\gamma^{3}}-\gamma^{3}-\frac{1}{\gamma}<0$
		for $\gamma\in(0,\infty)$. 
		\item \label{it:gaussian_lem2} Let $J=-J^{T}$ and $K=K^{T}$. Then $\Tr(JKJK)-\Tr(J^{2}K^{2})\ge0$.
		Furthermore, equality holds if and only if $[J,K]=0.$ 
	\end{enumerate}
\end{lemma}
\begin{proof}
	To show \ref{it:gaussian_lem1} we note that $\gamma-\frac{4}{\gamma^{3}}-\gamma^{3}-\frac{1}{\gamma}<\gamma-\frac{4}{\gamma^{3}}-\gamma^{3}=\gamma(1 - \frac{4}{\gamma^4}-\gamma^2)$.  The function $f(\gamma):=1 - \frac{4}{\gamma^4}-\gamma^2$
	has a unique global maximum on $(0,\infty)$ at $\gamma_{min}=8^{1/6}$
	with $f(\gamma_{min})=-2$, so the result follows.
	\\\\
	For \ref{it:gaussian_lem2} we note that $[J,K]^{T}=[J,K],$ and that $[J,K]^{2}$ is symmetric
	and nonnegative definite. We can write 
	\[
	\Tr([J,K]^{2})=\sum_{i}\lambda_{i}^{2},
	\]
	with $\lambda_{i}$ denoting the (real) eigenvalues of $[J,K]$. From
	this it follows that $\Tr([J,K]^{2})\ge0$ with equality if and only
	if $[J,K]=0$. Now expand 
	\[
	\Tr([J,K]^{2})=2\big(\Tr(JKJK)-\Tr(J^{2}K^{2}),
	\]
	which implies the advertised claim. \qed
\end{proof}