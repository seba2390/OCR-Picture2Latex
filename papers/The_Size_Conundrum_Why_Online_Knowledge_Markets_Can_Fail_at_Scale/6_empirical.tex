\section{Evaluating Our Proposed Models}
In this section, we identify optimal models (basis and interaction) based on the accuracy of fitting content generation time series observed in our dataset (Section 6.1), and evaluate the performance of the optimal models in predicting content volume in long run (Section 6.2).

\subsection{Model Fitting}
We fit each variant of production model (basis and interaction), for each content type, to the observed content generation time series (monthly granularity), in each \CQA{StackExchange} site. Notice that among the different variants of production models, the models using power or exponential basis have a parsimonious set of parameters. For example, answer generation model using power basis function requires only three parameters for interactive essential interaction (See Section 4.2), and four parameters for remaining interaction types. In contrast, answer generation model using sigmoid basis function requires five parameters for interactive essential interaction, and six parameters for remaining interaction types. 

\textbf{Parameter Estimation.} We learn the best-fit parameters for capturing the observed content generation time series. We restrict some parameters of our production models to be non-negative, e.g., non-negative exponents in power basis. These restrictions are important because the underlying factors positively affect the output. We use the trust-region reflective algorithm \cite{branch1999} to solve our constrained least square optimization problem. The algorithm is appropriate for solving non-linear least squares problems with constraints.

\textbf{Evaluation Method.} We evaluate fitting accuracy using four metrics: root mean square error (RMSE), normalized root mean square error (NRMSE), explained variance score (EVS), and Akaike information criterion (AIC). Given two series for each content type, the observed series $N(t)$, and the prediction $\hat{N(t)}$ of the series by a model with $k$ parameters, we compute the four metrics as follows:\\ RMSE = $\sqrt{\frac{1}{T}\sum_{t=1}^{T}(N(t)-\hat{N(t)})^2}$; NRMSE = $\frac{RMSE}{max(N(t))-min(N(t))}$; EVS = $1-\frac{Var(N(t)-\hat{N(t)})}{Var(N(t))}$; AIC = $T*ln(\frac{1}{T}\sum_{t=1}^{T}(N(t)-\hat{N(t)})^2)+2k$. Among the four metrics, RMSE and NRMSE are error metrics (low value implies good fit), AIC is an information theoretic metric to capture the trade-off between model complexity and goodness-of-fit (low value implies good model), and EVS refers to a model's ability to capture variance in data (high value implies good model).

\textbf{Fitting Results.} We compare the fitting accuracy of production models for all \CQA{StackExchange} sites using the four metrics. Each metric is summarized via the mean, across all sites, for each content type. We use content generation time series with monthly granularity as observed data. We found that the models with the exponential and sigmoid basis functions do not fit the data for many \CQA{StackExchange} sites. Accordingly, in Table~\ref{tbl:model_fit} we only present the results for production models with the power basis and different interaction types. Notice that the models with interactive essential interaction outperform the remaining models for all metrics and content types. We performed paired $t$-tests to determine if the improvements for interactive essential interaction are statistically significant; the results are positive with $p<0.01$.

\begin{table}[ht]
	\caption{The comparison of fitting accuracy of production models (with power basis and different interaction types) for all \CQA{StackExchange} sites. The models with interactive essential interaction outperform the remaining models for all metrics and content types. The improvements for interactive essential interaction are statistically significant, validated via paired t-tests, where $p<0.01$.}
	\label{tbl:model_fit}
	\begin{center}
	\begin{tabular}{llcccc}
    \toprule
    \multirow{2}{*}{Content} & Interaction & Avg. & Avg. & Avg. & Avg.\\
    & Type & RMSE & NRMSE & EVS & AIC\\
    \midrule
    Question & Single Factor & 25.74 & 0.09 & 0.79 & 104.47\\
    \midrule
    \multirow{4}{*}{Answer} & Essential & 70.31 & 0.09 & 0.79 & 208.82\\
    & I. Essential & \textbf{64.62} & \textbf{0.08} & \textbf{0.83} & \textbf{196.39}\\
    & Antagonistic & 72.77 & 0.09 &  0.78 & 210.96\\
    & Substitutable & 68.90 & 0.09 & 0.81 & 207.61\\
    \midrule
    \multirow{4}{*}{Comment} & Essential & 146.64 & 0.08 & 0.83 & 328.25\\
    & I. Essential & \textbf{137.23} & \textbf{0.08} & \textbf{0.85} & \textbf{318.24}\\
    & Antagonistic & 155.97 & 0.09 &  0.82 & 334.12\\
    & Substitutable & 155.43 & 0.09 & 0.82 & 335.10\\
    \bottomrule
	\end{tabular}
	\end{center}
\end{table}

Thus we use production models with power basis and interactive essential interaction for prediction tasks.

\subsection{Forecasting Content Generation} 
We apply production models with power basis and interactive essential interaction to forecast content volume in long run---one year ahead in the future. Specifically, we train each model using the content generation data from the first 12 months (beyond the ramp period), and then examine how well the model forecasts content dynamics in the next 12 months. We validate the forecasting capability by examining the overall prediction error (NRMSE). 

We compute the prediction NRMSE across all StackExchange sites, and summarize the results using the mean ($\mu$) and variance ($\sigma$)--- (i) question: $\mu = 0.11$, $\sigma = 0.08$; (ii) answer: $\mu = 0.12$, $\sigma = 0.09$; (iii) comments: $\mu = 0.11$, $\sigma = 0.10$. Notice that our models can forecast future content dynamics with high accuracy. We performed these experiments for different time granularity, e.g., week, month, quarter, and reached a consistent conclusion. We do not report these results for brevity.

