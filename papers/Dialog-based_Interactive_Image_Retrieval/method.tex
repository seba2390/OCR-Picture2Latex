%given

\section{Method}

Our framework
, which we refer to as the {\em dialog manager}, 
considers a user interacting with a retrieval agent via iterative dialog turns. 
At the $t$-th dialog turn, the dialog manager presents a candidate image $a_{t}$ selected from a retrieval database $ \mathcal{I} = \{I_i\}_{i=0}^{N}$ to the user. The user then provides a feedback sentence $o_{t}$, describing the differences between the candidate image $a_{t}$ and the desired image. Based on the user feedback and the dialog history up to turn $t$, $\mathrm{H}_{t}= \{a_{1},o_{1},...,a_{t},o_{t} \}$, the dialog manager selects another candidate image $a_{t+1}$ from the database and presents it to the user. This process continues until the desired image is selected or the maximum number of dialog turns is reached. 
In practice, the dialog manager
could provide multiple images per turn to achieve better retrieval
performance. In this work, we focus on a simplified scenario with a single image 
per interaction. We
note that the same framework could be extended to
the multiple-image case by allowing the user to select one
image out of a list of candidate images to provide feedback on.

\begin{figure*}[t!]
\begin{center}
\includegraphics[width=\linewidth]{figs/framework.pdf}
\end{center}
\caption{The proposed end-to-end framework for dialog-based interactive image retrieval. }
\label{fig:framework}
% \vspace{-1em}
\end{figure*}

\subsection{Dialog Manager: Model Architecture}
\label{sec:framework}
Our proposed dialog manager model consists of three main components: a {\em Response Encoder}, a {\em State Tracker}, and a {\em Candidate Generator}, as shown in Figure~\ref{fig:framework}. At the $t$-th dialog turn, the {\em Response Encoder} embeds a candidate image and the corresponding user feedback $\{a_t, o_t\}$ into a joint visual-semantic representation $x_t \in \mathbb{R}^D$. The {\em State Tracker} then aggregates this representation with the dialog history from previous turns, producing a new feature vector $s_t \in \mathbb{R}^D$ . The {\em Candidate Generator} uses the aggregated representation $s_t$ to select a new candidate image $a_{t+1}$ that is shown to the user. Below we provide details on the specific design of each of the three model components.

\paragraph{Response Encoder.} 
The goal of the Response Encoder is to embed the information from the $t$-th dialog turn 
$\{a_{t},o_{t}\}$ into a visual-semantic representation $x_t \in \mathbb{R}^D$. First, the candidate image is encoded using a deep convolutional neural network (CNN) followed by a linear transformation: $x_t^{\textrm{im}} = \textrm{ImgEnc}(a_t) \in \mathbb{R}^{D}$. The CNN architecture in our implementation is an ImageNet pre-trained ResNet-101~\cite{He2015} and its parameters are fixed. 
Words in the user feedback sentence are represented with one-hot vectors and then embedded with a linear projection followed by a CNN as in \cite{kim2014convolutional}: 
$x_t^{\textrm{txt}} = \textrm{TxtEnc}(o_t) \in \mathbb{R}^{D}$. Finally, the image feature vector and 
the sentence representation 
are concatenated and embedded through a linear transformation to obtain the final 
response representation at time $t$: 
$x_{t}=W(x^{\textrm{im} }_{t} \oplus x^{\textrm{txt}}_{t} )$, 
where $\oplus$ is the concatenation operator and $W \in \mathbb{R}^{D \times 2D}$ is the linear 
projection. The learnable parameters of the Response Encoder are denoted as $\theta_{r}$. 

\paragraph{State Tracker.} The State Tracker is based on a gated recurrent unit (GRU), %~\cite{chung2014empirical}, 
which receives as input the response representation $x_t$, combines it with
the history representation of previous dialog turns, and outputs the 
aggregated feature vector $s_t$.
The forward dynamics of the State Tracker is written as: $g_{t}, h_{t}= \textrm{GRU} (x_{t}, h_{t-1})$, $s_{t} = W^{s} g_{t}$, 
where $h_{t-1} \in \mathbb{R}^{D}$ and $g_{t} \in \mathbb{R}^{D}$ are the hidden 
state and the output of the GRU respectively, $h_{t}$ is the updated hidden state,
$W^{s} \in \mathbb{R}^{D \times D}$ is a linear projection and $s_t\in \mathbb{R}^{D}$ is the history representation updated with the 
information from the current dialog turn. %  the embedding for the dialog $\mathrm{H}_t$. 
The learnable parameters of the State Tracker (GRU model) are denoted as $\theta_{s}$. 
This memory-based design of the State Tracker allows our model to sequentially aggregate the information from user feedback to localize the candidate image
to be retrieved.

\paragraph{Candidate Generator.} 
Given the feature representation of all images from the retrieval database, 
$\{x_i^{\textrm{im}}\}_{i=0}^{N}$, where $x_i^{\textrm{im}} = \textrm{ImgEnc}(I_i)$, 
we can compute a sampling probability based on the distances between
the history representation $s_{t}$ to each image feature, $x_i^{\textrm{im}}$.
Specifically, the sampling probability is modeled 
using a softmax distribution over the top-$K$ nearest neighbors
of $s_t$: $\mathbf{\pi}(j) = e^{-{d_j}} / \sum_{k=1}^{K}e^{-{d_k}}, j=1,2,...,K$, where $d_k$ is the $L2$ distance of $s_t$ to its $k$-th nearest neighbor in 
$\{x_i^{\textrm{im}}\}_{i=0}^{N}$. Given the sampling distribution, 
two approaches can be taken to sample the candidate image, denoted as $a_{t+1} = I_{j^{\prime}}$: 
(1) stochastic approach (used at training time), where $j^{\prime} \sim \mathbf{\pi}$, and (2) greedy
approach (used at inference time), where $j^{\prime} = \arg\max_{j}(\pi_{j})$. 
Combining the three components of the model architecture, the overall learnable
parameters of the dialog manager model is $\theta=\{\theta_{r}, \theta_{s}\}$. 
Next, we explain how the network is trained end-to-end. 


\begin{figure*}[ht!]
\centering
    \begin{subfigure}[t]{0.45\textwidth}
        \centering
        \includegraphics[width=1\textwidth]{figs/simulator.pdf}
        \caption{}
    \end{subfigure}%
    ~ 
    \begin{subfigure}[t]{0.55\textwidth}
        \centering
        \includegraphics[width=1\textwidth]{figs/optimization.pdf}
        \caption{}
    \end{subfigure}
    \caption{The learning framework: (a) The user simulator enables efficient 
    exploration of the retrieval dialogs (Section~\ref{sec:simulator}); 
    and (b) the policy network is learned using a combination of supervised 
    pre-training and model-based policy improvement (Section~\ref{sec:optimization}).}
    \label{fig:pipeline}
\end{figure*}

\subsection{Training the Dialog Manager}
\label{sec:drl}

Directly optimizing the ranking percentile metric in a supervised learning scheme
is challenging since it is a non-differentiable function~\cite{li2016multiple,wang2018lambdaloss}. 
Instead, we model the ranking percentile as the environment reward received by the agent and
frame the learning process in a reinforcement learning setting with the goal of 
maximizing the expected sum of discounted rewards:
$\max_{\pi} \mathcal{U^{\pi}} = \mathbb{E} \big[ \sum_{t=1}^{T} \gamma^{t-1} r_{t} | \pi_\theta \big]$, where $r_{t}\in \mathbb{R}$ is the reward representing the ranking percentile 
of the target image at the $t$-th interaction, $\gamma$ is a discount factor determining 
the trade-off between short-term and long-term rewards, 
$T$ is the maximum number 
of dialog turns, 
and $\pi_\theta$ is the policy determined by network parameters $\theta$.\footnote{Strictly speaking, the optimal policy depends on the number of remaining dialog turns. We simplify the policy to be a function independent of dialog turn numbers.} 

Training an RL model for this problem requires extensive exploration 
of the action space, which is only feasible if a large amount of training data is available. 
However, collecting and annotating human-machine dialog data for our task is expensive. 
This problem is exacerbated in the situation of natural language based user feedback, 
which incurs an even larger exploration space as compared to approaches based on a fixed set of attributes. 
In text-based dialog systems, it is common to circumvent this 
issue by relying on user simulators \cite{li2016user}. We adopt a similar strategy, 
where a user simulator, trained on human-written relative descriptions, substitutes the 
role of a real user in training the dialog manager (illustrated in Figure~\ref{fig:pipeline}a). 
Below we further describe our user simulator, as well as the reinforcement learning techniques that we used to optimize our dialog manager.

\subsubsection{User Simulator based on Relative Captioning}
\label{sec:simulator}
Here we propose the use of a {\em relative captioner} to simulate the user. 
%, which is a novel computer vision task in and of itself. 
It acts as a surrogate for real human 
users by automatically generating sentences that can describe the 
prominent visual differences between any pair of target and candidate images. 
We note that at each turn, our user simulator generates feedback independent of previous user feedback, and previously retrieved images. While more sophisticated models that consider the dialog history could potentially be beneficial, training such systems well may require orders of magnitude more annotated data. In addition, back-referencing in dialogs can inherently be ambiguous and complex to resolve, even for humans. Based on these considerations, we decided to first investigate the use of a single-turn simulator. 
While a few related tasks have been studied previously, such as context-aware image captioning~\cite{vedantam2017} and referring expression generation~\cite{yu2016}, to the best of our knowledge, % TODO: detailed difference 
% learning to compare and contrast visual differences and describe them in
% natural language is a novel computer vision task. 
there is no existing 
dataset  directly supporting this task, so we introduce a new dataset as described in Section~\ref{sec:dataset}. 


We experimented with several different ways of combining the visual features of the target and retrieved images. We include a comprehensive study of different models for the user simulator in Appendix~\ref{sec:app_relative} and show that the relative captioner 
based user model serves as a reasonable proxy for real users.
Specifically, we used feature concatenation to fuse the image features of 
the target and the reference image pair and applied the model of {\em Show, 
Attend, and Tell}~\cite{icml2015_xuc15} to generate the relative captions
using a long short-term memory network (LSTM). 
For image feature extraction, we adopted the architecture of ResNet101~\cite{He2015}
pre-trained on ImageNet; and to better capture the localized visual differences, we added a visual attention mechanism; the loss function
of the relative captioner is the sum of the negative log likelihood of the 
correct words~\cite{icml2015_xuc15}. 
 
\subsubsection{Policy Learning} 
%In this section, we cover the details on training the dialog manager policy for selecting candidate images. 
%While REINFORCE~\cite{} algorithm is a popular option for training RL agents,  in practice it often results in poor local minima when applied to complex systems with more than \comment{hundreds} of network parameters. Therefore,  
%Specifically, we propose a novel optimization pipeline that begins by bootstrapping 
%the performance of the model using supervised pre-training, then by leveraging the deterministic 
%behavior of the environment (user simulator), we further adapt two optimization 
%strategies to enhance efficient policy learning, namely self-critical sequence training and 
%self-guided policy improvement. 

\paragraph{Supervised Pre-training.} 
When the network parameters are randomly initialized at the beginning, 
the history representations $s_{t}$ are nearly random. 
To facilitate efficient exploration during RL training, we first 
pre-train the policy using a supervised learning objective.
%, which is common in practice~\cite{silver2017,rennie2016self}.
While maximum likelihood-based pre-training is more common, here we pre-train using the more discriminative triplet loss objective:

\begin{equation}
\begin{aligned}
\vspace{-2em}
\mathcal{L}^{\textrm{sup}} &= \mathbb{E} \Big[ \sum_{t=1}^{T} \max(0, \|s_{t} 
- x^{+}\|_2 - \|s_{t} - x^{-}\|_2 + \text{m})  \Big]
\end{aligned}
\end{equation}
where $x^{+}$ and $x^{-}$ are the image features of the target image and a random 
image sampled from the retrieval database respectively, $\text{m}$ is a constant for the margin and $\|.\|_2$ denotes $L2$-norm. 
Intuitively, by ensuring the proximity of the target image and the images 
returned by the system, the rank of the target image can be improved 
without costly policy search from random initialization. However, 
entirely relying on this supervised learning objective deviates from our 
main learning objective, since the triplet loss objective does not 
jointly optimize the set of candidate images to maximize expected future 
reward.~\footnote{More explanation on the difference between the two objectives
is provided in Appendix~\ref{sec:qualitative}.}

\begin{comment}
\paragraph{Self-Critical Sequence Training} 
Self-Critical Sequence Training (SCST) is a recently proposed algorithm~\cite{rennie2016self} 
that improves the performance of the basic policy gradient method when the 
inference-time rewards can be accessed during training. 
The idea of SCST is to use test-time inference reward as the baseline 
for policy gradient learning by encouraging policies performing above 
the baseline while suppressing policies underperforming the baseline. 
Taking advantage of the user simulator, we can easily compute the inference-time
rewards of the dialog manager via generating dialogues using the greedy sampling approach. 
Thus, following the idea of SCST, the network gradient can be written as:

\begin{equation}
\begin{aligned}
\nabla_{\theta} \mathcal{U} & = \mathbb{E} \big[ (u(h_{T}) - u(\hat{h}_{T})) \nabla_{\theta} \log\big(p(h_{T}\big)\big] \\
& =\mathbb{E} \big[ (u(h_{T}) - u(\hat{h}_{T})) \sum_{t=1}^{T} \nabla_{\theta} \log\big(\pi(a_{t} | h_{t})\big)\big]
\end{aligned}
\label{eq:critical}
\end{equation}
where $h_{T}$ is the training-time dialog sample following the stochastic sampling method
as specified in Section~\ref{sec:framework}, $\hat{h}_{T}$ is the inference-time dialog following the
greedy sampling policy, $u(\cdot)$ computes the sum of discounted rewards of a dialog round, 
and $\nabla_{\theta} \log\big( p(h_{T}) \big)$ is the gradient of the policy with respect to the network parameters, which is the sum of the gradient from each dialog turn, $\nabla_{\theta} \log\big(\pi(a_{t} | h_{t})\big)$. 
\end{comment}

\paragraph{Model-Based Policy Improvement.}  
\label{sec:optimization}
Given the known dynamics of the environment (in our case, the user simulator), it is often
advantageous to leverage its behavior for policy improvement. Here we adapt the policy improvement~\cite{sutton1998reinforcement} to our model-based policy learning.
Given the current policy $\pi$ and the user simulator, the value
of taking an action $a_{t}$ using test-time configuration can be efficiently 
computed by look-ahead policy value estimation 
$Q^{\pi}(h_{t}, a_{t}) = \mathbb{E} \big[ \sum_{t'=t}^{T} \gamma^{t'-t} r_{t'} | \pi \big]$. Because our user simulator is essentially deterministic, one trajectory 
is sufficient to estimate one action value.
Therefore, an improved policy $\pi'$ can be derived from the current policy $\pi$ 
by selecting the best action given the value of the current policy, $\pi'(h_{t}) \equiv a^{*}_{t} = \arg \max_{a} Q^{\pi}(h_{t}, a)$. 
Specifically, following ~\cite{guo2014deep}, we minimize the cross entropy loss given 
the derived action, $a^{*}$,
\begin{equation}
\begin{aligned}
\mathcal{L}^{\textrm{imp}} = \mathbb{E} \Big[ - \sum_{t=1}^{T} \log\Big(\pi(a^{*}_{t}|h_{t})\Big)\Big]
\end{aligned}
\end{equation}
Compared to traditional policy gradient methods, the model-based policy improvement gradients have lower variance, and converge faster. In Section~\ref{sec:experiment}, we further demonstrated the effectiveness of model-based policy improvement by comparing it
with a recent policy gradient method. Figure~\ref{fig:pipeline}b illustrates our policy learning method as described above.
 
\begin{comment}
Given the self-critical policy gradient defined in Equation~\ref{eq:critical}, 
the final gradient update formulation is:
\begin{equation}
\begin{aligned}
\nabla_{\theta} \mathcal{U} =\mathbb{E} \big[ (u(h_{T}) - u(\hat{h}_{T})) \sum_{t=1}^{T} \nabla_{\theta} \log\big(\pi(a^{*}_{t}| h_{t})\big)\big]
\end{aligned}
\label{eq:critical}
\end{equation}
\end{comment}
\nocite{li2014req}
% The auxiliary supervision will help increasing the rank of the promising candidate images, so the policy learning would be easier. But the auxiliary supervision only considers the feature similarity but the real ranking of the target image. It is not a replacement of our training objective.   
% Eventually the auxiliary will diverge from our goal to optimize the ranking of the target image, so we only use the auxiliary supervision in pre-training.


%-------------------------------------------------------------------------
% (TODO: somehow emphasize that the target image is not the optimal action for this MDP because the user feedback does not allow accurate inference on the target image in the very first few turns. the classic rational agent vs. omniscient agent argument)


% We encode the image $a$ using a deep CNN, and then embed it through two non-linear projection followed by ReLU to have the final image representation $x^{\textrm{img}} = \textrm{ImgEnc}(a) \in \mathbb{R}^{H}$. Words in the user's feedback are represented with one-hot vectors that are embedded with a linear embedding, and then the sentences are embedded by a CNN \cite{} to have the same length vectors as images, $x^{\textrm{txt}} = \textrm{TxtEnc}(o) \in \mathbb{R}^{H}$. The feature vectors of the candidate image and the user feedback at the $i$-th turn is then concatenated and embedded through a linear projected, $x^{\textrm{percept}}_{t}=W(x^{\textrm{img} }_{t} \oplus x^{\textrm{txt} }_{t} )$, where $\oplus$ is the concatenation operator and $W \in \mathbb{R}^{H \times 2H}$.   
