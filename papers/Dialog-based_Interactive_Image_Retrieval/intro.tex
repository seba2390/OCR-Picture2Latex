\section{Introduction}

The volume of searchable visual media has grown tremendously in recent years, and has intensified the need for retrieval systems that can more effectively identify relevant information, with applications in domains such as e-commerce~\cite{huang2015cross,liuLQWTcvpr16DeepFashion},  surveillance~\cite{vaquero2009attribute,shi2015transferring}, and Internet search~\cite{gordo2016deep,jegou2012aggregating}. Despite significant progress made with deep learning based methods~\cite{wang2014learning,gordo2017end}, achieving high performance in such retrieval systems remains a challenge, due to the well-known semantic gap between feature representations and high-level semantic concepts, as well as the difficulty of fully understanding the user's search intent. 

A typical approach to improve search efficacy is to 
allow the user a constrained set of possible interactions with the system~\cite{zhou2003relevance,thomee2012interactive}. 
In particular, the user provides iterative feedback about retrieved 
objects, so that the system can refine the results, allowing the user and system 
to engage in a ``conversation'' to resolve what the user wants to retrieve.
For example, as shown in Figure~\ref{fig:teaser}, feedback about relevance~\cite{rui1998relevance} allows users to indicate which images are ``similar'' or ``dissimilar'' to the 
desired image, and relative attribute feedback~\cite{kovashka2012} allows
the comparison of the desired image with candidate images based on a fixed 
set of attributes. While these feedback paradigms are effective, the restrictions 
on the specific form of user interaction largely constrain the information that a 
user can convey to benefit the retrieval process.

In this work, we propose a new approach to interactive visual content retrieval by 
introducing a {\em novel form of user feedback based on natural language}. 
This enables users to directly express, in natural language, 
the most prominent conceptual differences between the preferred search object and 
the already retrieved content, which permits a more natural human-computer interaction. 
We formulate the task as a reinforcement learning (RL) problem, where the system 
directly optimizes the rank of the target object, which is a non-differentiable objective. 

We apply this RL based interactive retrieval framework to the task of image retrieval,
which we call {\em dialog-based interactive image retrieval} to emphasize its capability in aggregating history information compared to existing single turn approaches~\cite{tellex2009towards,barbu2013saying,li2017person,hu2016natural}.  In particular, a 
novel end-to-end dialog manager architecture is proposed, which takes natural language 
responses as user input, and delivers retrieved images as output. 
To mitigate the cumbersome and costly process of collecting and annotating 
human-machine dialogs as the system learns, 
we utilize a model-based RL approach by training a user simulator based on a corpus of human-written relative descriptions. Specifically, to emulate a single dialog turn, where the user provides feedback regarding a candidate image relative to what the user has in mind, the user simulator generates a {\em relative caption} describing the differences between the candidate image and the user's desired image.\footnote{In this work, the user simulator is trained on single-turn data and does not consider the dialog history. This reduces the sequence of responses to a ``bag'' of responses and implies that all sequences of a given set of actions (candidate images) are equivalent. Nevertheless, while the set of candidate images that maximize future reward (target image rank) are a set, selecting the image for the next turn naturally hinges on all previous feedback from the user. Therefore, the entire set of candidate
images can be efficiently constructed sequentially. }
Whereas there is a lot of prior work in image captioning \cite{kulkarni2011baby,vinyals2015show,rennie2016self}, 
%to our knowledge 
we % are the first to 
explore the problem of {\em relative image captioning}, a general approach to more expressive and natural communication of relative preferences to machines, and 
to use it as part of a user simulator to train a dialog system. 

\begin{figure}[t]
\begin{center}
\includegraphics[width=1\linewidth]{figs/teaser3.pdf}
\end{center}
\caption{In the context of interactive image retrieval, the agent incorporates the user's 
feedback to iteratively refine retrieval results. Unlike existing work which 
are based on relevance feedback or relative attribute feedback, our approach 
allows the user to provide feedback in natural language. 
\vspace{-0.05em}
}
\label{fig:teaser}
\end{figure}

The efficacy of our approach is evaluated in a real-world application
scenario of interactive footwear retrieval. Experimental results with both 
real and simulated users show that the proposed reinforcement learning
framework achieves better retrieval performance than existing techniques. 
Particularly, we observe that feedback based on natural language is 
more effective than feedback based on pre-defined relative attributes by a large
margin. Furthermore, the proposed RL training framework of directly optimizing 
the rank of the target image shows promising results and outperforms the supervised learning approach which is based on the triplet loss objective. 
The main contributions of this work are as follows:
\begin{itemize}
\item A new vision/NLP task and machine learning problem setting for interactive visual content search, where the dialog agent learns to interact with a human user over the course of several dialog turns, and the user gives feedback in natural language. 
\item A novel end-to-end deep dialog manager architecture, which addresses the above problem setting in the context of image retrieval. The network is trained based on an efficient policy optimization strategy,
employing triplet loss and model-based policy improvement~\cite{sutton1998reinforcement}.
\item The introduction of a 
%novel 
computer vision task, \emph{relative image captioning}, 
where the generated captions describe the salient visual differences between two images, which is distinct from and complementary to context-aware \emph{discriminative image captioning}, where the absolute attributes of one image that discriminate it from another are described ~\cite{vedantam2017}.
\item The contribution of a new dataset, which supports further research on the task of relative image captioning. \footnote{The project website is at: \url{www.spacewu.com/posts/fashion-retrieval/}}
\end{itemize}



