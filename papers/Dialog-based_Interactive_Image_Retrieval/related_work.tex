\section{Related Work}

\paragraph{Interactive Image Retrieval.}  Methods for improving image search results based on user interaction have been studied for more than 20 years~\cite{flickner1995query,thomee2012interactive,rui1999image}. Relevance feedback is perhaps the most popular approach, with user input specified either as binary feedback (``relevant'' or ``irrelevant'')~\cite{rui1998relevance} or based on multiple relevance levels~\cite{wu2004willhunter}. More recently, relative attributes (e.g., ``more formal than these,'' ``shinier than these'') have been exploited as a richer form of feedback for interactive image retrieval \cite{kovashka2012,kovashka2017attributes,kovashka2013attribute,parikh2011relative,yu2017fine}. 
%Along this line of work, techniques have been developed to handle different user perceptions of attributes \cite{shades2014}, actively guide user feedback on selected images \cite{kovashka2013attribute}, and handle fine-grained visual comparisons \cite{yu2017fine}. Zhao et al.~\cite{Zhao_2017_CVPR} proposed a method for interactive fashion apparel search, where feedback is provided in the form of attribute manipulations (e.g., ``blue, instead of pink'').  
All these methods rely on a fixed, pre-defined set of attributes, whereas our method relies on feedback based on natural language, providing more flexible and more precise descriptions of the items to be searched. Further, our approach offers an end-to-end training mechanism which facilitates deployment of the system in other domains, without requiring the explicit effort of building a new vocabulary of attributes.

%\vspace{0.1in}
\paragraph{Image Retrieval with Natural Language Queries.}
%{\bf Image Retrieval with Natural Language Queries. }
Significant progress has been recently made on methods that lie in the intersection of computer vision and natural language processing, such as image captioning~\cite{vinyals2015show,rennie2016self}, visual question-answering~\cite{antol2015vqa,tapaswi2016movieqa}, visual-semantic embeddings~\cite{frome2013devise,wang2016learning},  and grounding phrases in image regions~\cite{rohrbach2016grounding,plummer2015flickr30k}. In particular, our work is related to image or video retrieval methods based on natural language queries~\cite{tellex2009towards,barbu2013saying,li2017person,hu2016natural}. 
%Tellex and Roy~\cite{tellex2009towards} developed a system to search a corpus of surveillance video for clips that match spatial language queries such as ``along the hallway'' and ``across the kitchen.'' Barbu et al.~\cite{barbu2013saying} proposed an approach for video retrieval that uses compositional semantics to encode subtle meaning that is lost in other systems, such as the difference between sentences with identical words: ``the person hit the ball'' and ``the ball hit the person.'' Li et al.~\cite{li2017person} addressed the problem of person search based on natural language description.  Hu et al.~\cite{hu2016natural} proposed a method to localize a target object within a given image based on a natural language query of the object. 
These methods, however, retrieve images and videos in a single turn, whereas our proposed approach aggregates history information from dialog-based feedback and iteratively provides more refined results.

%\vspace{0.1in}
\paragraph{Visual Dialog.} Building conversational agents that can hold meaningful dialogs with humans has been a long-standing goal of Artificial Intelligence.
Early systems were generally designed based on rule-based and slot-filling techniques \cite{williams2007partially}, whereas modern approaches have focused on end-to-end training, 
leveraging the success of encoder-decoder architectures and sequence-to-sequence learning \cite{serban2016building,bordes2016learning,guo2016learning}.
Our work falls into the class of visually-grounded dialog systems \cite{das2016visual,de2016guesswhat,seo2017visual,das2017learning,strub2017end}. Das et al \cite{das2016visual} proposed the task of visual dialog, where
the system has to answer questions about images based on a previous dialog history. 
De Vries et al. \cite{de2016guesswhat} introduced the {\em GuessWhat} game, where a series of questions are asked to pinpoint a specific object in an image, with restricted answers 
consisting of yes/no/NA.
The {\em image guessing} game \cite{das2017learning} %has been used to 
demonstrated emergence of grounded language and communication
among visual dialog agents with no human supervision, using RL to train the agents in a goal-driven dialog setting.
However, these dialogs are purely text-based for both the questioner and answerer agents, whereas we address the interactive image retrieval problem,
with an agent presenting images to the user to seek feedback in natural language.
% whereas in our work we address the interactive image retrieval problem,
% with an agent presenting one or more images to the user at each dialog turn, and the user providing feedback in natural language.

