\documentclass{article}

% if you need to pass options to natbib, use, e.g.:
\PassOptionsToPackage{numbers}{natbib}
% before loading nips_2018

% ready for submission
\usepackage[final]{neurips_2018}
% to compile a preprint version, e.g., for submission to arXiv, add
% add the [preprint] option:
% \usepackage[preprint]{nips_2018}

% to compile a camera-ready version, add the [final] option, e.g.:
% \usepackage[final]{nips_2018}

% to avoid loading the natbib package, add option nonatbib:
% \usepackage[nonatbib]{nips_2018}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
% ours 
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{subcaption}
\usepackage{float}
\usepackage{multirow}
% \usepackage{color}
\usepackage{comment}
\usepackage{wrapfig}

\usepackage[usenames, dvipsnames]{color}

\title{Dialog-based Interactive Image Retrieval}

% The \author macro works with any number of authors. There are two
% commands used to separate the names and addresses of multiple
% authors: \And and \AND.
%
% Using \And between authors leaves it to LaTeX to determine where to
% break the lines. Using \AND forces a line break at that point. So,
% if LaTeX puts 3 of 4 authors names on the first line, and the last
% on the second line, try using \AND instead of \And before the third
% author name.
\begin{comment}
\author{
  %% examples of more authors
  Xiaoxiao Guo$^{*1}$ \thanks{$^{*}$Contributed equally. 
  } \phantom{a} Hui Wu$^{*1}$ \phantom{a} Yu Cheng$^{1}$ \phantom{a}  Steven Rennie$^{2}$ \phantom{a} Rogerio Feris$^{1}$  \\
  $^{1}$IBM Research AI \phantom{AND} $^{2}$Fusemachines Inc. 
  %% Coauthor \\
  %% Affiliation \\
  %% Address \\
  %% \texttt{email} \\
  %% \And
  %% Coauthor \\
  %% Affiliation \\
  %% Address \\
  %% \texttt{email} \\
}
\end{comment}
\author{
  %% examples of more authors
  Xiaoxiao Guo$^{\dagger}$ \thanks{$^{\dagger}$These two authors contributed equally to this work. } \\
  IBM Research AI\\
  \texttt{xiaoxiao.guo@ibm.com}
  \And
  Hui Wu$^{\dagger}$ \\
  IBM Research AI\\
  \texttt{wuhu@us.ibm.com}
  \And
  Yu Cheng \\
  IBM Research AI\\
  \texttt{chengyu@us.ibm.com}
  \And
  Steven Rennie \\
  Fusemachines Inc.  \\
  \texttt{srennie@gmail.com}
  \And
 Gerald Tesauro \\
  IBM Research AI\\
  \texttt{gtesauro@us.ibm.com}
  \And
  Rogerio Schmidt Feris \\
  IBM Research AI\\
  \texttt{rsferis@us.ibm.com}
}

\renewcommand\footnotemark{}
\begin{document}
%\nipsfinalcopy
%is no longer used

\maketitle

\begin{abstract}
\begin{comment}
Inspired by the enormous growth of huge online media collections of many types (e.g. images, audio, video, e-books, etc.), and the paucity of intelligent retrieval systems, 
this paper introduces a novel approach to interactive visual content 
retrieval.  The proposed retrieval framework is guided by free-form natural language feedback from users, allowing for more natural and effective communication. 
Such a framework constitutes a multi-modal dialog protocol where in each dialog turn, a user submits a natural language request to a retrieval agent, which then attempts to retrieve the optimal object.
We formulate the retrieval task
as a reinforcement learning problem, and reward the dialog system for improving the 
rank of the target object during each dialog turn. This
framework can be applied to a variety of visual media types (images, videos, graphics, etc.),
and in this paper, we study in-depth its application on the task of interactive image retrieval.
To avoid the cumbersome and costly process of collecting human-machine conversations 
as the dialog system learns, we train the dialog system with 
a user simulator, which is itself trained to describe the differences between 
target and retrieved images.  The efficacy of our approach is demonstrated in a footwear image retrieval application.  Extensive experiments on both simulated and real-world data show that: 1) our proposed learning framework achieves better accuracy than other supervised and reinforcement learning baselines; and 2) user feedback based on natural language rather than pre-specified attributes leads to more effective retrieval results, and a more natural and expressive communication interface. 
\end{comment}

Existing methods for interactive image retrieval have demonstrated the merit of integrating
user feedback, improving retrieval results. However, most current systems
rely on restricted forms of user feedback, such as binary relevance responses, or
feedback based on a fixed set of relative attributes, which limits their impact. In this
paper, we introduce a new approach to interactive image search that enables users
to provide feedback via natural language, allowing for more natural and effective
interaction. We formulate the task of dialog-based interactive image retrieval as a
reinforcement learning problem, and reward the dialog system for improving the
rank of the target image during each dialog turn. To mitigate the cumbersome and
costly process of collecting human-machine conversations as the dialog system
learns, we train our system with a user simulator, which is itself trained to describe
the differences between target and candidate images. The efficacy of our approach
is demonstrated in a footwear retrieval application. Experiments on
both simulated and real-world data show that 1) our proposed learning framework
achieves better accuracy than other supervised and reinforcement learning baselines
and 2) user feedback based on natural language rather than pre-specified
attributes leads to more effective retrieval results, and a more natural and expressive
communication interface.
 
\end{abstract}

\input{intro}

\input{related_work}

\input{method}

\section{Dataset: Relative Captioning}
\label{sec:dataset}

% In this section, we provide information on the dataset used for training the user simulator. 
Our user simulator aims 
to capture the rich and flexible language describing visual 
differences of any given image pair. The relative captioning dataset 
thus needs this property. We situated the data collection procedure
in a scenario of a shopping chatting session between a shopping assistant and a customer. The annotator was asked to take the role of the customer 
and provide a natural expression to inform the shopping assistant about the desired
product item. 
To promote more regular, specific, and relative user feedback, we provided a sentence prefix for the annotator to complete when composing their response to a retrieved
image. Otherwise the annotator response is completely free-form: no other constraints on the response were imposed.
We used Amazon Mechanical Turk %~\cite{buhrmester2011} 
to crowdsource the relative expressions. After manually removing erroneous annotations, 
we collected in total $10,751$ captions, with one caption per pair of images.

Interestingly, we observed that when the target image and the reference image are sufficiently different, users often directly describe the visual appearance of the target image, rather than using relative expressions (c.f. fourth example in Figure 7(b), Appendix A). This behavior mirrors the \emph{discriminative captioning} problem considered in \cite{vedantam2017}, where a method must take in two images and produce a caption that refers only to one of them. Relative and discriminative captioning are complementary, and in practice, both strategies are used, and so we augmented our dataset by pairing 3600 captions that were discriminative with additional dissimilar images. Our captioner and dialog-based interactive retriever are thus trained on both discriminative and relative captions, so as to be respectively more representative of and responsive to real users. Additional details about the dataset collection procedure and the analysis on dataset statistics are included in Appendix~\ref{sec:app_data} and Appendix~\ref{sec:dataset_analysis}.
% To facilitate future research on relative captioning and reproducible results, we will make our dataset and codes available for public use.

\input{experiment}
\section{Conclusions}
This paper introduced a novel and practical task residing at the intersection 
of computer vision and language understanding: %relative captioning, and 
dialog-based
interactive image retrieval.
Ultimately, techniques that are successful on such tasks will form the basis 
for the high fidelity,  multi-modal, intelligent 
conversational systems of the future, and thus represent important milestones in this quest.  
We demonstrated the value of %these tasks and 
the proposed learning architecture on the application
of interactive fashion footwear retrieval.
Our approach, 
enabling users to provide natural language feedback,
significantly outperforms
traditional methods relying on a pre-defined vocabulary of relative attributes, while offering more natural communication.
As future work, we plan to leverage side information, such as textual descriptions associated with images of product items,
and to develop user models that are
conditioned on dialog histories, enabling more realistic interactions.
We are also optimistic that our approach for image retrieval can be extended to other media types such as audio, video, and e-books, given the 
performance of deep learning on tasks such as speech recognition, machine translation, and activity recognition. 


\textbf{Acknowledgement.}
We would like to give special thanks to Professor Kristen Grauman for helpful discussions. 


\begingroup
    %\setlength{\bibsep}{10pt}
   	\small
    \bibliographystyle{unsrt}
	\bibliography{nips2018}
\endgroup

% \bibliographystyle{unsrt}
% \bibliography{nips2018}

\input{supple.tex}

\end{document}


