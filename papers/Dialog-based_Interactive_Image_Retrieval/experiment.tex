\section{Experimental Results}
\label{sec:experiment}

In Section~\ref{sec:rlResult}, we assess the contribution of each component of our pipeline for policy learning. To evaluate the value of using free-form dialog feedback, 
we show experiments considering both simulated user feedback (Section~\ref{sec:dialogResult}) 
and real-world user feedback (Section~\ref{sec:humanResult}). 

All experiments were performed on the {\em Shoes} 
dataset~\cite{berg2010}, with the same training and testing data split
for all retrieval methods and for training the user simulator. 
$10,000$ database images were used during training, and $4,658$
images for testing. The retrieval models are tested by retrieving images on the testing set, starting from a randomly selected candidate image for the first dialog turn. 
Image retrieval performance is quantified by the average rank percentile 
of the image returned by the dialog manager on the test set.
For details on architectural configurations, parameter settings, 
baseline implementation, please refer to Appendix~\ref{sec:config}. 

\begin{wrapfigure}{r}{7cm}
\vspace*{-1.5em}
\includegraphics[width=7cm]{figs/new-fig.png}
\caption{
Quantitative comparison of our method and two baselines and
methods using feedback based on a pre-defined set of relative attributes. }
\label{fig:quan_result}
\vspace*{-1.5em}
\end{wrapfigure} 

\subsection{Analysis of the Learning Framework}
\label{sec:rlResult}
We use our proposed user simulator to generate data and provide
extensive quantitative analysis %GT
on the contribution
of each model component. 


\paragraph{Results on Relative Captioner.} Figure~\ref{fig:captioner} provides examples of simulator generated 
feedback and the collected user annotations. An interesting 
observation is that even though the user simulator only occasionally 
generates descriptions that exactly match the human annotations 
(the third example in Figure~\ref{fig:captioner}),
it can still summarize the main visual differences
between the images, since inherently there are many ways 
to describe differences between two images. Qualitative 
examination of the generated relative expressions showed that the user 
simulator can approximate feedback of real users at a very low annotation cost 
(more analysis is included in Appendix~\ref{sec:app_relative}).

\paragraph{Policy Learning Results.} 
To investigate how retrieval performance is affected by each component 
of the dialog manager, we compare our approach, denoted as \textbf{Ours}, against 
two variants: (1) \textbf{SL}: supervised learning where the agent is trained only with triplet loss; 
(2) \textbf{RL-SCST}: policy learning using Self-Critical Sequence Training (SCST)~\cite{rennie2016self} after pre-training the network using the triplet loss objective. 
As shown in Figure~\ref{fig:quan_result} (solid lines), the average ranking percentile 
of the target image in all methods increases monotonically as the 
number of dialog turns increases. Both RL-based retrieval algorithms 
outperform the supervised pre-training, \textbf{SL}, which is 
expected since the triplet loss function does not directly optimize the retrieval
ranking objective. Finally, \textbf{Ours} achieves $98\%$ average ranking percentile 
with only two dialog turns and consistently outperforms \textbf{RL-SCST} 
across different dialog turns, which demonstrates the benefit of the model-based
policy improvement component. We have observed similar results on the attribute-based baselines. Each of the SL based attribute model underperforms its RL version by
 $\sim1\%$ in retrieval ranking percentile.


\begin{figure*}
\centering
\includegraphics[width=\textwidth]{figs/caption_result.pdf} 
\caption{Examples of human provided (\textcolor{Green}{green}) and captioner generated relative descriptions 
(\textcolor{NavyBlue}{blue}). 
While generated relative captions don't resemble human annotations in 
most cases, they can nonetheless capture the main visual differences between the
target image and the reference image.}
\label{fig:captioner}
\end{figure*}

\subsection{Effectiveness of Natural Language Feedback}
\label{sec:dialogResult}


In this section, we empirically evaluate the effect of natural language feedback, compared
to pre-defined, relative attribute-based user feedback. 

{\noindent \bf Generating Attribute Feedback.} 
Each image in the 
dataset
maps to a $10$-D attribute vector, as described in~\cite{kovashka2012}.
We adopted a rule-based feedback generator which concatenates the 
respective attribute words with ``more'' or ``less'', depending on the relative attribute
values of a given image pair. For example, 
if the ``shiny'' value of the candidate image and the target image
are $0.9$ and $1.0$ respectively, then the rule-based feedback is 
``more shiny.''  Attributes are randomly sampled, similar to the relative attribute feedback generation in \cite{kovashka2012}. To simulate the scenario when users 
provide feedbacks using multiple attributes, individual attribute phrases are combined.  
% using ``and''. 
We adopted original attribute values in~\cite{kovashka2012}, which
were predicted using hand-crafted image features, as well as attribute values predicted
using deep neural networks in~\cite{7494596}. 
% Besides the original attribute scores, we also compare baselines based on deep learning based attribute estimates in~\cite{souri2016deep}. 

{\noindent \bf Results.} 
We trained the dialog manager using both dialog-based feedback and attribute-based 
feedback (\textbf{Attr\textsubscript{n}} and \textbf{Attr\textsubscript{n} (deep)}), where the subscript number denotes
the number of attributes used in the rule-based feedback generator and \textbf{(deep)} denote baselines using deep learning based attribute estimates as in~\cite{7494596}. The empirical result
is summarized in Figure~\ref{fig:quan_result}, including relative attribute feedback using
one, three and ten attribute phrases. The three attribute case matches the average length of user feedback in free-form texts and the ten case uses all possible pre-defined attributes to provide feedback.   
Across different numbers of dialog turns, the natural language 
based agent produced significantly higher target image average 
ranking percentile than the attribute based
methods. 
The results suggest that feedback based on unrestricted natural language 
is more effective for retrieval than the predefined set of relative attributes used in \cite{kovashka2012}. This is expected as the vocabulary of relative attributes in \cite{kovashka2012} is limited.
Even though deep learning based attribute estimates improve the attribute-based baselines significantly, the performance gap between attribute based baseline and free form texts is still significant. We conjecture that the main reason underlying the  performance gap between attribute and free-form text based models is the effectively open domain for
attribute use, which is difficult to realize in a practical user interface without natural language. In fact, free-form dialog feedback obviates
constructing a reliable and comprehensive attribute taxonomy,
which in itself is a non-trivial task~\cite{maji2012}. 


\subsection{User Study of Dialog-based Image Retrieval}
\label{sec:humanResult}
In this section, we demonstrate the practical use of our system 
with real users. We compare with an existing method, WhittleSearch~\cite{kovashka2012}, on the task of interactive footwear retrieval.  WhittleSearch represents images as feature 
vectors in a pre-defined $10$-D attribute space, and iteratively refines 
retrieval by incorporating the user's relative feedback on 
attributes to narrow down the search space of the target 
image. For each method, we collected $50$ five-turn dialogs; 
at each turn, one image is presented to the user to seek feedback. 
For WhittleSearch, the user can choose to use
any amount of attributes to provide relative feedback on during each
interaction. The resulting average ranking percentile of the dialog manager and 
WhittleSearch are $89.9\%$ and $70.3\%$ 
respectively. In addition to improved retrieval accuracy, users also 
reported that providing dialog-based feedback is more natural compared to selecting
the most relevant attributes from a pre-defined list.

Figure~\ref{fig:userDialogs} shows examples of retrieval dialogs from real users
(please refer to Appendix~\ref{sec:qualitative} for more results and discussions). 
We note that users often start the dialog with a coarse description of the main
visual features (color, category) of the target. As the dialog progresses, 
users give more specific feedback on fine-grained visual differences. 
The benefit of free-form dialog can be seen from the
flexible usage of rich attribute words (``leopard print on straps''),
as well as relative phrases (``thinner'', ``higher heel'').
Overall, these results show that the proposed framework for the
dialog manager exhibits promising behavior on generalizing to real-world applications. 
 
\begin{figure}
\centering
\includegraphics[width=\linewidth]{figs/qualitative_1.pdf} 
\caption{Examples of the user interacting with the proposed dialog-based 
image retrieval framework. }
\label{fig:userDialogs}
\vspace{-1em}
\end{figure}

