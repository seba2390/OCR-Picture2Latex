\label{sec:exp}
\input{tables/qpp-config}

A QPP experiment context \cite{dg22ecir} involves three configuration choices: i) the \textbf{QPP method} itself that is used to predict the relative performance of queries; ii) the \textbf{IR metric} that is used to obtain a ground-truth ordering of the query performances as measured on a set of top-$k$ ($k=100$ in our experiments) documents retrieved by iii) a specific \textbf{IR model}. Table \ref{tab:models_and_metrics} summarizes the IR models and metrics used in our experiments, along with the relevant hyper-parameter values. The objective of our experiments is to investigate the following two key research questions:

\begin{compactitem}
    \item \RQ{1}: Does \proposed~\textit{agree} with the standard listwise correlation metrics?
    \item \RQ{2}: How \textit{robust} is \proposed~with respect to changes in the QPP experiment context?
\end{compactitem}
\input{tables/stability}

An affirmative answer to \RQ{1} would indicate that our proposed metric \proposed~is \textit{consistent} with existing metrics used for QPP evaluation, while an affirmative answer to \RQ{2} would suggest that \proposed~is preferable to existing methods due to its higher stability with respect to different experimental settings.

\input{tables/variation}

We conduct our QPP experiments on the TREC Robust dataset, which consists of $249$ topics. Following the standard practice for QPP experiments \cite{hamed_neuralqpp,query_variants_kurland}, we report results aggregated over a total of 30 randomly chosen equal-sized train-test splits of the data. The training split of each partition was used for tuning the hyper-parameters for the QPP method.

\paragraph{Agreement between listwise and pointwise evaluation}
Firstly, we investigate the consistency of \proposed~with respect to three standard listwise QPP evaluation metrics: Pearson's $r$, Spearman's $\rho$ and Kendall's $\tau$; and a pointwise approach, scaled Absolute Rank Error (sARE) \cite{sare}. Since sARE is an error measure, we measure correlations of \proposed~with $1-\text{sARE}$ measures (which for the sake of simplicity, we refer to as sARE in Table \ref{tab:stable}). We experiment with three different instances of \proposed~obtained by substituting the aggregation functions -- avg, min and max as $\Sigma$ in Equation \ref{eq:avgpwcorr}, denoted respectively as $\pwua{\text{avg}}$, $\pwua{\text{min}}$ and $\pwua{\text{max}}$.

The results presented in Table \ref{tab:stable} answer \RQ{1} in the affirmative. Each reported value here corresponds to the rank correlation (Kendall's $\tau$) between the relative ranks of the QPP systems ordered by their effectiveness as computed via one of the standard metrics (one of $r$, $\rho$, $\tau$ or sARE) and \proposed, i.e., one of $\pwua{\text{avg}}$, $\pwua{\text{min}}$ and $\pwua{\text{max}}$). The high correlation values between the standard listwise and the proposed pointwise metrics show that \proposed~can be used as a substitute for the standard listwise evaluation. Notably, we see that the average aggregate function yields the best results, and hence for the subsequent experiments we use $\pwua{\text{avg}}$ as the pointwise evaluation metric.

\paragraph{Variances in relative effectiveness of QPP methods}

To investigate \RQ{2}, we consider the relative stability of QPP system ranks for variations in QPP contexts (i.e., different IR models and target metrics), comparing both listwise and pointwise approaches (see Table \ref{tab:sdres}). To clarify with an example, if working with three QPP methods, say AvgIDF, NQC, WIG, we observe that $\tau$(NQC) $>$ $\tau$(WIG) $>$ $\tau$(AvgIDF) for LMDir as measured relative to AP@100. We expect to observe a similar ordering for a different choice of the IR model and target IR metric, say BM25 with nDCG@100. As in our previous experiments, here we measure the rank correlations between a total of seven QPP systems (see Table \ref{tab:models_and_metrics}) via Kendall's $\tau$.