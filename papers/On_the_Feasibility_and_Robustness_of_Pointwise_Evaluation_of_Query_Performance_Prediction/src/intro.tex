Query performance prediction (QPP) methods have been proposed to automatically estimate the retrieval effectiveness for queries without making use of any true relevance information (e.g. \cite{qpp_croft_cikm06,uef_kurland_sigir10}).
In practice, a QPP method allows us to dynamically adjust the processing steps for a query, depending on its initial performance estimate. 
Although estimating the performance of individual queries independently is a common requirement in many downstream tasks (e.g., adaptive query processing \cite{adaptive_prf}), the standard QPP evaluation methodology adopted by the IR research community has previously involved a \textbf{listwise} approach, rather than a \textbf{pointwise} one. This is despite the fact that the latter represents a more appropriate strategy for use in downstream applications. To elaborate,
a listwise approach operates on a \emph{set of queries} $\mathcal{Q}$ by first converting it into an ordered set as induced by the QPP estimated scores $\phi(Q)\,\forall Q \in \mathcal{Q}$. It then computes a rank correlation measure, such as Kendall's $\tau$, 
between the ground-truth ordering of the queries as induced by their average precision (AP) values \cite{Cummins14} or by any other IR metric, such as nDCG \cite{hamed_neuralqpp}.

A major disadvantage of listwise QPP approaches is that evaluation is conducted in a relative manner, so the performance of one query is measured relative to the others. However, a downstream performance estimate of an individual query also needs to be evaluated independently of the other queries. In contrast, a pointwise approach measures the effectiveness on individual queries, and then, if required, aggregates the results over a complete set. This is analogous to measuring the retrieval effectiveness metric MAP by computing the average precision values for individual queries and then aggregating them.
Pointwise evaluation also allows us to carry out a per-query analysis of a method often leading to useful insights. For instance, Buckley \cite{ria} found that, by performing an extensive per-topic retrieval analysis, they were able to identify queries where most IR systems fail to retrieve relevant documents. However, a listwise evaluation methodology is not conducive to performing this kind of detailed per-query analysis.

Another drawback of listwise methods is that they can be overly sensitive to the configuration setup used for evaluation. The two most important such configurations are: i) the target retrieval evaluation metric that induces a ground-truth ordering over the set of queries; ii) the retrieval model used to obtain the top-$k$ set of documents for QPP estimation. Indeed, variations in these configurations can lead to both large standard deviations in the reported rank correlation measures and significant differences in the relative ranks of various QPP systems \cite{dg22ecir}.
To address the limitations of listwise methods, we propose a new QPP evaluation framework, \textbf{Aggregated Pointwise Absolute Errors} (\textbf{\proposed}), which is shown to not only be consistent with the existing listwise approaches, but also to be more robust to changes in QPP experimental setup.