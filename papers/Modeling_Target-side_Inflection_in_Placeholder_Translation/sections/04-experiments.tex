\section{Experimental Setups}
We evaluate the proposed model with several baselines to show how well the model can produce the appropriately inflected form of a given lemma.

\subsection{Corpus}
We conduct experiments in a Japanese-to-English translation task with the ASPEC corpus \citep{nakazawa-etal-2016-aspec}.
This corpus consists of abstracts from scientific articles, which tend to contain many technical terms.
Such words are rare and hard for the model to learn the correct translation, and thus this corpus fits the typical use-case of lexically constrained translation.
We use the initial 1M sentence pairs from the training split for training.

\subsection{Word Dictionary}
In this study, lexical constraints in translation are introduced through a source-to-target word dictionary. We construct the dictionary automatically from the ASPEC corpus through the following procedure.

First, we obtain the word alignment by feeding the first 1M sentence pairs of the training split and validation/test splits to \texttt{GIZA++}.\footnote{\url{https://github.com/moses-smt/giza-pp}} We tokenize Japanese sentences with \texttt{Mecab}\footnote{\url{https://taku910.github.io/mecab/}} and English sentences with \texttt{spaCy}.\footnote{\url{https://spacy.io/}} We then construct a phrase table and extract only those with more than 100 occurrences.
Then, we split the dictionary into noun and verb entries to facilitate the analysis of the results and remove noise. If both the Japanese and English phrases are noun phrases, the entry is registered in the noun dictionary. If the Japanese phrase is a nominal verb\footnote{The nominal verb (\ja{サ変動詞}) is the most productive class of verb in Japanese and many new or technical terms fall into this category ({\it e.g.}, \ja{最適化する}-{\it optimize}, \ja{過学習する}-{\it overfit}).} and English is a verb, the entry is registered in the verb dictionary.
In this study, we evaluate the model's ability to inflect a provided lemma. Lemmas for the target language (English) are obtained with \texttt{spaCy}.

\subsection{Models}
As the baseline, we implement a Transformer \citep{NIPS2017_3f5ee243} translation model based on \texttt{AllenNLP} \citep{Gardner2017AllenNLP}.
We configure the model in the Transformer-base setting and sentences are tokenized using \texttt{sentencepiece} \citep{kudo-2018-subword}, which has a shared source-target vocabulary of about 16k sub-words.
The overviews of lexically constrained models are summarized in \fig{fig:baseline}.

\minisection{Placeholder (PH)}
In the placeholder method, the model is trained to translate sentences with a placeholder token and pass that through to the translation. In our experiments, we use different placeholder tokens \texttt{[NOUN]} and \texttt{[VERB]} for nouns and verbs.
Predicted placeholder tokens are replaced by the pre-specified term in the post-processing step.
We evaluate three types of placeholder baselines, each of which differs in what inflected form the target placeholder token is replaced with: {\bf PH (oracle)}, where the pre-specified term is embedded in the same form as in the reference; {\bf PH (lemma)}, always the lemma form; {\bf PH (common)}, the most common inflected forms in the training data, which are the singular form for \texttt{[NOUN]} and the past tense form for \texttt{[VERB]}. The results of PH (lemma) and PH (common) are provided as naive baselines to give a sense of how difficult predicting the correct inflected form is.

We also provide a baseline that performs word inflection through an external resource ({\bf PH (morph)}).
As in \citet{tamchyna-etal-2017-modeling}, words that need inflection are followed by morphological tags, and word formation is realized through an external resource.
We use \texttt{LemmInflect}\footnote{\url{https://github.com/bjascob/LemmInflect}} to decompose the dictionary entries with their lemma and part-of-speech tags and to recover the inflected word form.
As this model uses an external resource to perform inflection, it is not directly comparable with our proposed models but we provide its results as an oracle baseline.

\minisection{Code-switching (CH)}
The code-switching model replaces a phrase in a source sentence with the corresponding target phrase according to a bilingual dictionary.\footnote{\citet{dinu-etal-2019-training} utilize source factors that indicate which tokens are code-switched, but we observe no significant difference by adding source factors. Therefore, we simply report the results from the model with minimal components.}
{\bf CH (oracle)} uses the same target words as in the reference, and {\bf CH (lemma)} uses the lemma form.

\minisection{Proposed Model}
We implement our proposed model described in \cref{sec:approach} on top of the placeholder baseline model.
Compared to the baseline, our proposed model has three additional modules: the target context encoder, target character embeddings, and character-level decoder.
The embedding and hidden sizes are all set to 512, which is the same as in the Transformer-base model.
The additional encoder and decoder have two layers, and the feedforward dimension is 1024.


Note that, for all the models, we restrict the number of constraints to at most one in each sentence as an initial investigation. This favors the placeholder-based models as handling more than one placeholder introduces additional complexity in the system and tends to degrade the performance, while the code-switching methods suffer less from multiple constraints \citep{song-etal-2019-code}.
We leave experiments with multiple constraints to future work.


\begin{figure}[t]
\centering
\includegraphics[width=14.0cm]{data/baselines.png}
\caption{The preprocessing of the lexically constrained baseline models.}
\label{fig:baseline}
\end{figure}



\subsection{Training with Lexical Constraints}
To apply lexical constraints, the models are trained with data augmentation.
Augmented data is created for all sentences that contain any of the source and target phrases found in the dictionary entries.
To control the amount of augmented data to around 10\% of the original training data, we restrict the dictionary entries to infrequent ones.
The restriction to infrequent phrases also simulates real-word use-cases, where user-specified terms are often rare words that typical NMT models struggle with in translation.
Specifically, we restrict the noun entries to ones with a count at most 20, and the verb entries to 2000.
The threshold is chosen to balance the amount of noun and verb entries in the augmented data.


\subsection{Optimization}
We optimize the models using Adam \citep{Kingma2015AdamAM} with the Noam learning rate scheduler with 8000 warmup steps \citep{NIPS2017_3f5ee243}. The training is stopped when the validation BLEU score does not improve for 3 epochs.

For our proposed model, we found that optimizing the word-level modules and character-level modules separately stabilizes the training process and improves the translation quality.
We first train a normal placeholder model, use the weights to initialize those of our proposed model, and then only update the parameters of the additional modules.
In this second training stage, we use the loss value as validation metric and stop the training when the lowest value is updated for 5 epochs.
