\section{Related Work}
\subsection{Placeholder Translation}
To ensure that certain words appear in the translated sentence, previous studies have explored the method of replacing certain classes of words with special placeholder tokens and restore the words in a post-processing step, which we call {\it placeholder translation} in this paper.

\citet{luong-etal-2015-addressing} and \citet{long-etal-2016-translation} employed placeholder tokens to improve the translation of rare words or technical terms.
However, simply replacing words with a unique placeholder token loses the information on the original words. To alleviate this problem, subsequent studies distinguish different types of placeholders, such as named entity types \citep{Crego2016SYSTRANsPN,post-etal-2019-exploration} or parts-of-speech \citep{michon-etal-2020-integrating}.

Instead of replacing the placeholder token with a dictionary entry, some studies propose generating the content of the placeholder with a character-level sequence-to-sequence model to translate words not covered in the bilingual dictionary. \citet{Li2016NeuralNT} and \citet{wang-etal-2017-sogou} incorporated a named entity translator, which is supposed to learn transliteration of named entities.
As in their work, our proposed model also uses a character-level decoder to generate the content of placeholders, but our focus is to inflect a lemma to the appropriately inflected form given the context.

\subsection{The Code-switching Method}
Another way to introduce terminology constraints is the code-switching method \citep{song-etal-2019-code,dinu-etal-2019-training,exel-etal-2020-terminology}. The model is trained with source sentences where some words are replaced or followed by specific target words and expected to copy the words to the translation.

One advantage of the code-switching method is that, unlike the placeholder methods, it preserves the meaning of the original words, which likely leads to better translation quality.
Also, the model can incorporate the specified terminology in a flexible way: a model trained with the code-switching method not only copies the pre-specified target words but can inflect the words according to the target-side context \citep{dinu-etal-2019-training}.
In parallel to our work, \citet{niehues-2021-continuous} offers a quantitative evaluation of how well the code-switching method handles inflection of a pre-specified terminology when the terminology is given in the lemma form.

Although the code-switching method is flexible, one disadvantage is that it tends to ignore the pre-specified terminology more often than the placeholder method (\cref{sec:results}).
We propose a placeholder method that handles inflection of pre-specified terms, aiming for both flexibility and faithfulness to terminology constraints.

\subsection{Constrained Decoding}
Another approach to ensure that a pre-specified term appears in the translation is constrained decoding \citep{anderson-etal-2017-guided,hokamp-liu-2017-lexically,post-vilar-2018-fast}.
Constrained decoding can be applied to any existing NMT models without modifying its architecture and training regime, but imposes a significant cost on the decoding speed.
It is also unclear how to incorporate lexical inflection into constrained decoding.
Therefore, we focus on the placeholder and code-switching methods in this study.

\subsection{Modeling Morphological Inflection in Neural Machine Translation}
Explicitly modeling morphological inflection into NMT models has been studied mainly to enable effective generalization over morphological variation of words.
\citet{tamchyna-etal-2017-modeling} and \citet{weller-di-marco-fraser-2020-modeling} propose to decompose certain classes of words into its lemma and morphological tags to reduce data sparsity.
At decoding time, the inflected form is restored by a morphological analyzer. \citet{Song_Zhang_Zhang_Luo_2018} proposed a model that only requires a stemmer to alleviate the need for linguistic analyzers. The model decomposes the process of word decoding into stem generation and suffix prediction.

In this work, we propose to model morphological inflection in the process of embedding pre-specified terms into placeholders to improve the flexibility of placeholder translation. Our approach requires no external linguistic analyzer at prediction time; instead, inflection is performed via a neural character-based decoder.
