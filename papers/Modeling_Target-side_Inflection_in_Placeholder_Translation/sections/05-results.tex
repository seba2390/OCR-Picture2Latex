\section{Results}
\label{sec:results}
\subsection{Evaluation}
For each model, we evaluate the overall translation quality with BLEU \citep{papineni-etal-2002-bleu}.\footnote{SacreBLEU\citep{post-2018-call} version string: \\\texttt{case.mixed+numrefs.1812+smooth.exp+tok.13a+version.1.5.1}}
We also evaluate the {\it specified term use rate}, a metric to check if the model correctly includes the specified target term.
Note that this is only an approximate measure of what we want to measure: whether the specified term is used in the correct form in the output translation.
Since a single source sentence can be translated into different grammatical constructions, it is possible that the inflected form in the system output is different from the one in the reference but still correct in the context.
Still, we find a substantial overlap in the inflectional form of the specified term between the reference and the system output, and thus report this metric, followed by a more closely inspected manual evaluation.

Also, we are interested in how well the model generalizes to dictionary entries unseen during training. In typical use cases of lexically constrained translation, the specified terms are new or rare words that are not likely to appear in the training data. We construct two kinds of evaluation dictionaries: {\it seen} and {\it unseen}.
We first construct a dictionary by aggregating only entries that appear in the dev/test set.
Then, we randomly split the entries into {\it seen} and {\it unseen} and remove the {\it unseen} entries from the training dictionary. Thus, the {\it seen} split contains entries that appear in the training data while the {\it unseen} not.
We evaluate the model separately using the noun and verb dictionary, which results in a total of four kinds of evaluation configurations.

\begin{table*}[h]
  \centering
  \begin{tabular}{lllll} \toprule
      & \multicolumn{2}{c}{NOUN}                              & \multicolumn{2}{c}{VERB}                              \\
      & \multicolumn{1}{c}{{\it seen}} & \multicolumn{1}{c}{{\it unseen}} & \multicolumn{1}{c}{{\it seen}} & \multicolumn{1}{c}{{\it unseen}} \\ \midrule
      Baseline & 27.1 / 68.3 & 27.1 / 66.5 & 27.1 / 63.4 & 27.1 / 61.2 \\ \midrule
      CS (oracle) & 27.3 / 86.8 & 27.0 / 79.3 & 27.5 / 91.9 & 27.2 / 43.9 \\
      PH (oracle) & 27.2 / 98.8 & 27.0 / 99.2 & 27.4 / 98.7 & 27.5 / 99.4 \\ \midrule
      PH (lemma) & \multirow{2}{*}{27.1 / 84.7}  & \multirow{2}{*}{26.9 / 84.0} & 26.9 / 9.41 & 27.1 / 11.4 \\
      PH (common) &                            &  & 27.3 / 81.8 & 27.3 / 68.9 \\
      CS (lemma) & 27.4 / 81.7 & 27.1 / 74.6 & 27.6 / 81.7 & 27.3 / 42.1 \\
      Proposed & 27.2 / 89.9 & 26.9 / 79.1 & 27.4 / 88.3 & 27.4 / 73.9 \\
      PH (morph) & 27.9 / 84.7 & 27.8 / 81.2 & 28.5 / 91.1 & 28.4 / 87.9 \\
  \bottomrule
  \end{tabular}
\caption{BLUE scores and the specified term use rate of the different models over different evaluation dictionaries. CS: Code-switching, PH: placeholder. For NOUN, PH (lemma) and PH (common) are the same model because the most common inflection for nouns is their lemma.}
\label{table:results-overall}
\end{table*}



\subsection{Main Results}
The results are shown in Table \ref{table:results-overall}. For each configuration, we report the average of three models trained with different random seeds.

First, the lexically constrained models show BLEU scores not significantly different from the baseline.
The only exception is PH (morph): it consistently improves the BLEU score by from 0.7 to 1.4 points from the baseline.
This indicates the strength of injecting the NMT model with morphological knowledge for better generalization in translation.
In the following discussion, we focus on the comparison of the specified term use rate.

PH (oracle) and CS (oracle) models receive the same inflected form of a specified term as in the reference, and thus offer upper bounds for the specified term use rate.
We observe that PH (oracle) exhibits nearly perfect specified term use rates (more than 98\% with all dictionaries).
Also, it is more successful at incorporating the specified term into translation than CS (oracle) in the setting of one constraint, which is in line with previous observations \citep{song-etal-2019-code}.

As for the models that need to handle inflection, the results are quite mixed for NOUN.
A simple strategy of predicting the most common inflection achieves better specified term use rates than most of the other sophisticated models.
We conjecture that some examples allow either singular or plural form and that makes a proper evaluation difficult. Therefore, we turn to the results from VERB for model comparison.

In terms of both {\it seen} and {\it unseen} of the VERB dictionary, PH (morph) performs the best.
Note, however, that this model is not comparable to our model as it assumes access to a high-quality morphological analyzer at training time to obtain morphological tags and the correct inflectional paradigm of user-specified terms at prediction time.

In a more restricted setting, our proposed model outperforms the comparable code-switching model (CS (lemma)) and the other baselines.
In particular, the proposed model is more robust than CS (lemma) to {\it unseen} specified terms: we observe a consistent tendency that the specified term use rate degrades when the entries are unseen during training especially with CS (lemma) and verb entries (81.7 to 42.1), while this tendency is less pronounced in the placeholder model with lemmas (88.3 to 73.9).
Overall, our model exhibits faithfulness to lexical constraints similar to those of the normal placeholder model while having flexibility, which we examine below.

\subsection{Fine-grained Analysis}
The specified term use rate only checks whether specified terms are used in the same form as in the reference. Now we examine the systems' output more closely by manual inspection.
As the problem of inflection matters more in verbs than in nouns in English, here we focus on the translation with the verb dictionary.

We sample from the system's output of the test set 50 sentences with the {\it seen} and {\it unseen} lexical constraints respectively.
We manually check the sampled sentences and annotate each sentence with one of the three tags: {\bf {\it correct}} --- the specified term is used in the translation in the correct inflected form (not necessarily the same as in the reference); {\bf {\it incorrect}} --- the model produces the specified term in some inflected form but that results in an ungrammatical sentence; {\bf {\it null}} --- the model fails to produce the specified term in any form.
The result is shown in Table \ref{table:results-manual}.

\begin{table}[]
  \centering
  \begin{tabular}{lcc} \toprule
                            & VERB {\it seen}  & VERB {\it unseen} \\
  CS (lemma)            & 49 / 0 / 1 & 26 / 0 / 24 \\
  PH with lemmas (proposed) & 48 / 2 / 0 & 39 / 7 / 4  \\
  PH (morph)          & 50 / 0 / 0 & 47 / 3 / 0 \\ \bottomrule
  \end{tabular}
\caption{The manual evaluation of the 50 sampled sentences. The values in each cell indicate {\it correct} / {\it incorrect} / {\it null}.}
\label{table:results-manual}
\end{table}

Firstly, for the words that are seen in the training data, all the models mostly generate the correct word form in the context.
On the other hand, the evaluation with VERB unseen reveals both the advantages and disadvantages of each model, which we discuss with examples below.

\minisection{The placeholder model with morphological tags can handle inflection well}
The model mostly generates the correct inflectional form of the specified terms.
The only three exceptions from VERB seen are errors in choosing the transitive or intransitive usage of the term (Table \ref{fig:ph_pos_wrong}).

\begin{table}[h]
  \centering
  \begin{tabularx}{\textwidth}{X} \toprule
    {\bf Source}: \ja{特発性肺線維症(IPF)患者14例及びIPF急性増悪で\textcolor{red}{入院}した患者8例を対象として,BALF・血漿に関してウィルス検査・免疫血清学的検査を施行した}\\
    {\bf Reference}: The virus inspection and immunoserologic inspection of BALF and blood plasma were carried out for 14 idiopathic pulmonary fibrosis (IPF) patients and of 8 patients \textcolor{red}{hospitalized} for IPF acute aggravation. \\
    {\bf System Output}: Wils inspection and immunoserologic inspection were enforced on BALF blood and blood in 14 patients with idiopathic pulmonary fibrosis (IPF) and 8 patients who \textcolor{red}{hospitalized} in the IPF acute aggravation. \\ \bottomrule
  \end{tabularx}
\caption{A translation example with the placeholder model with morphological tags. The system output should have generated {\it were hospitalized} in the red part.}
\label{fig:ph_pos_wrong}
\end{table}

\minisection{The code-switching method always produces grammatical inflectional forms}
We observe no {\it incorrect} examples from the code-switching model.
Since the output is determined solely by the word decoder with no additional post-editing performed, if the word decoder is well trained, we can expect the output sentences to be grammatical.

\minisection{The code-switching method tends to fail to observe the constraints}
However, the code-switching methods fail to produce the specified term in 24 examples out of 50, which is notably higher than the other methods.
A typical error is the model ignoring the constraint and producing a synonym, for example, generating {\it conclude} instead of {\it judge}, {\it examine} instead of {\it study}.
This is reasonable given the model architecture.
A well-trained NMT model usually assigns similar vector representations to synonyms.
Even when the specified term is given in the source sentence, it is given a representation similar to other synonyms inside the model, and thus the decoder can generate any words with similar meaning.
We also observe a few character decoding errors: wrongly generating {\it hot-spitalized} instead of {\it hospitalized}, {\it move} instead of {\it remove}.

\minisection{The placeholder method almost always produces the specified term, but sometimes fails to inflect it correctly}
The placeholder method fails to observe the constraint much less frequently than the code-switching method (only 4 examples out of 50).
In most cases (39 examples out of 50), the model can successfully predict the correct form as shown in Table \ref{fig:ph_example_correct}.

\begin{table}[h]
  \centering
  \begin{tabularx}{\textwidth}{X} \toprule
    {\bf Source}: \ja{フローセンサーの原理は浮遊式流量計のテーパー管内フロートの位置を差動トランスで検出し,これの電圧制御により流量を\textcolor{red}{管理}する。}\\
    {\bf Reference}: The sensor controls the flow rate by detecting the position of the float in the tepered tube with a differential transformer and \textcolor{red}{controlling} it with the obtained voltage. \\
    {\bf System Output}: The principle of the flow sensor is that the position of the float in the taper tube of the floating flowmeter is detected by the differential transformer, and the flow rate is \textcolor{red}{controlled} by this voltage control. \\ \bottomrule
  \end{tabularx}
  \caption{A translation example with the placeholder model with a character decoder. The model predicts the correct inflectional form of {\it control} that fits in the context.}
  \label{fig:ph_example_correct}
\end{table}

The failures consist of generalization errors of inflectional form: generating {\it maken} for {\it make}.
It is impossible in principle to correctly predict irregular inflectional forms that are unseen in the training data, but this is usually not much of a problem since the specified term is usually a rare or new word, which tends to have a regular inflectional paradigm.
The other kind of error we observe is the model predicting a well-defined word form that is wrong in the context (Table \ref{fig:ph_example_wrong}). We expect that both error types can be addressed by exploiting additional data, either parallel or monolingual, to learn inflection rules in the target language.

\begin{table}[t]
  \centering
  \begin{tabularx}{\textwidth}{X} \toprule
    {\bf Source}: \ja{国立病院機構関門医療センター(国立下関病院)は2002年9月30日に女性総合診療を\textcolor{red}{開設}した。}\\
    {\bf Reference}: A National Hospital System Kanmon Medical Center (A National Shimonoseki Hospital) \textcolor{red}{opened} the comprehensive woman medical care service on September 30th in 2002. \\
    {\bf System Output}: National Hospital Mechanism Kanmon Medical Center ( the national Shimonoseki Hospital ) \textcolor{red}{opening} the woman general medical care on September 30th, 2002. \\ \bottomrule
  \end{tabularx}
\caption{A translation example with the placeholder model with a character decoder. The model predicts a wrong inflectional form for {\it open}.}
\label{fig:ph_example_wrong}
\end{table}
