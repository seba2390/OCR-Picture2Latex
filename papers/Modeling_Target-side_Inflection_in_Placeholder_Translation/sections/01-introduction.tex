\section{Introduction}
Over the last several years, neural machine translation (NMT) has pushed the quality of machine translation to near-human performance \citep{NIPS2014_a14ac55a,NIPS2017_3f5ee243}.
However, due to its end-to-end nature, this comes with the cost of losing a certain degree of control over the produced translation, which once was explicitly modeled, for example, in the form of phrase table \citep{koehn-etal-2003-statistical} in statistical machine translation (SMT).
 % But there are needs for controlled outputs
In practice, users often want to specify how certain words are translated in order to ensure the consistency of document-level translation or to guarantee the model to produce the correct translation for words that may be underrepresented in the training corpus such as proper nouns, technical terms, or novel words.

Given this motivation, a line of previous research has investigated {\it placeholder translation} \citep{post-etal-2019-exploration}.
With a source sentence where certain words are replaced with a special placeholder token, the model produces a translation with the special placeholder token in an appropriate position, and then that placeholder token is replaced with a pre-specified term in a post-processing step.

Although this approach ensures that certain words appear in the translation, one limitation is that the user must specify the term that fits in the context surrounding the placeholder token, or specifically, the term should be properly inflected according to the syntactic structure of the produced translation.
To illustrate the problem, we show an actual output from a normal placeholder translation model in Japanese to English translation in Table \ref{fig:motivation_example}.

The system is supposed to translate the word \ja{管理} into {\it controlling} as in the reference, but the output has a different grammatical construction and thus the progressive form {\it controlling} is invalid in this context; instead, {\it controlled} should be injected in the placeholder.
The appropriate word form is difficult to predict, especially in translation between grammatically distant languages, such as Japanese and English. As manually correcting the inflection in post-editing significantly hurts the convenience of placeholder translation, we need a way to automatically handle inflection.

\begin{table}[t]
  \centering
  \begin{tabularx}{\textwidth}{X} \toprule
  {\bf Specified Translation}: \textcolor{red}{\ja{管理} $\rightarrow$ controlling} \\
  {\bf Source}: \ja{フローセンサーの原理は浮遊式流量計のテーパー管内フロートの位置を差動トランスで検出し,これの電圧制御により流量を\textcolor{red}{[VERB]}する。}\\
  {\bf Reference}: The sensor controls the flow rate by detecting the position of the float in the tepered tube with a a differential transformer and \textcolor{red}{[VERB]} it with the obtained voltage. \\
  {\bf System Output}: The principle of the flow sensor is that the position of the float in the taper tube of the floating flowmeter is detected by the differential transformer, and the flow rate is \textcolor{red}{[VERB]} by this voltage control. \\ \bottomrule
  \end{tabularx}
\caption{A translation example from the ASPEC corpus \citep{nakazawa-etal-2016-aspec} with a placeholder translation model. The specified target term grammatically fits the placeholder in the reference, but not in the system output as it is.}
\label{fig:motivation_example}
\end{table}




One possible approach to this problem is the code-switching methods, in which certain words in the source sentence are replaced with the specific target words, and the model is encouraged to include those specific words in the translation. This approach is flexible in that the model can inflect the specified words according to the context \citep{song-etal-2019-code}, but less faithful to the lexical constraints, often ignoring the specified terms (\cref{sec:results}).

To address this problem, we propose a model that automatically inflects a pre-specified term according to the context of the produced translation.
We extend the sequence-to-sequence encoder and decoder with an additional character-level decoder that predicts the inflected form of the pre-specified term.
Our approach combines the advantages of both the placeholder and the code-switching methods: the faithfulness to lexical constraints and the flexibility of dynamically deciding the word form in the output.

We test our approach with a Japanese-to-English translation task in the scientific-writing domain \citep{nakazawa-etal-2016-aspec}, where the translation of technical terms poses a challenge to a vanilla NMT system.
The results show that the proposed method can include the specified term in the appropriately inflected form in the translation with higher accuracy than a comparable code-switching method.
We also perform a careful error analysis to understand the weaknesses of each system and suggest directions for future work.
