\section{Approach}
\label{sec:approach}
The proposed model builds upon a sequence-to-sequence (seq2seq) model with an attention mechanism.
Specifically, we use the Transformer model \citep{NIPS2017_3f5ee243}.

In the normal placeholder translation, the model is trained to generate placeholder tokens \texttt{[PLACEHOLDER]} when the source sentence includes them.
Then the placeholder tokens are replaced with user-provided terms in post-processing.

We extend the model to be able to handle inflection. Specifically, we consider the scenario where lemmas are provided as a specified term.
On top of the (sub)word-level decoder, we stack a character-level decoder to generate the content of the placeholder token.
The character-level decoder has to predict the correct inflected form of the specified lemma in the surrounding context.
Specifically, given the target tokens $\{w_1,..., w_T\}$ that contain a placeholder token and the specified lemma that consists of $L$ characters $c_{lemma} = \{c_1, .., c_L\}$, the character decoder generates the inflected form $c_{infl} = \{c'_1, .., c'_{L'}\}$.

We model the generation process with a decoder with attention mechanism (\fig{fig:architecture}).
We first summarize the contextual information on the placeholder token by a context encoder.
Specifically, we feed the embeddings of the target tokens $\{\bm{w}_1,..., \bm{w}_T\}$ into another Transformer encoder to contextualize the placeholder token (\eq{eq1}).
Then, the contextualized  representation of the placeholder token $\bm{h}_p$ and the character embeddings of the specified lemma $\{\bm{c}_1, .., \bm{c}_L\}$ with positional encoding \citep{NIPS2017_3f5ee243} are concatenated to form key-value vectors for decoder attention (\eq{eq2}).
Finally, the key-value vectors are passed to the character-level Transformer decoder and it generates the inflected form $\{c'_1, .., c'_{L'}\}$ in an auto-regressive manner (\eq{eq3}).

\begin{align}
  & \bm{h}_{1},..., \bm{h}_{T} = \operatorname{ContextEncoder}([\bm{w}_1,..., \bm{w}_T]) \label{eq1} \\
  & \bm{A} = [\bm{h}_p; \operatorname{Positional}(\bm{c}_1, .., \bm{c}_L)] \text{  where  } w_p = \texttt{[PLACEHOLDER]} \label{eq2} \\
  & c'_{t} = \operatorname{CharacterDecoder}(\bm{c'}_{<t}, \bm{A}) \label{eq3}
\end{align}


\begin{figure}[t]
\centering
\includegraphics[width=14.0cm]{data/model_architecture.png}
\caption{The proposed method: placeholder translation with a character decoder.}
\label{fig:architecture}
\end{figure}
