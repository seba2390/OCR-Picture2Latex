\section{Conclusion and Future Work}
In this study, we point out that the traditional placeholder translation method embeds the specified term into the generated translation without considering the context of the placeholder token, which potentially leads to grammatically incorrect translations.
To address this shortcoming, we proposed a flexible placeholder translation model that handles inflection when the specified term is given in the form of a lemma.
In the experiment of the Japanese-to-English translation task, we showed that the proposed model can inflect user-specified terms more accurately than the code-switching method.

Future work includes testing the proposed method on morphologically-rich languages or extending the model to handle more than one placeholder in a sentence.
Also, the proposed model still has room for improvement to learn inflection.
It is possible that we can improve the model by exploiting monolingual corpora in the target language to provide additional training signals for learning the correct inflection in context.
