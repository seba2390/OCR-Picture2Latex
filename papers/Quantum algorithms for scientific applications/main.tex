\documentclass[10pt]{iopart}
\usepackage[T1]{fontenc}  % \k for author name
\usepackage{iopams}  
\expandafter\let\csname equation*\endcsname\relax
\expandafter\let\csname endequation*\endcsname\relax
\usepackage{amsmath}
\usepackage{mathtools}
\usepackage{bbold}
\usepackage{graphicx,subcaption}
\usepackage{tabularx}
\usepackage{bbm}
\usepackage[dvipsnames]{xcolor}
\usepackage{hhline}
\usepackage{makecell}
\renewcommand{\cellalign}{cl}
\usepackage{multirow}
\usepackage[menucolor=false,linktoc=none]{hyperref}
\usepackage{braket}

\usepackage{natbib}
\newcommand{\newblock}{}
\setcitestyle{square,numbers}
\bibliographystyle{unsrtnat}
%
% ===========================================================
\begin{document}
% =============================
% ===========================================================

\review{Quantum algorithms for scientific applications} 

\author{R. Au-Yeung$^1$, B. Camino$^2$, O. Rathore$^3$ and V. Kendon$^1$}
\address{$^1$ Department of Physics, University of Strathclyde, Glasgow G4 0NG, United Kingdom}

\address{$^2$ Department of Chemistry, UCL, London WC1E 6BT, United Kingdom}

\address{$^3$ Department of Physics, Durham University, Durham DH1 3LE, United Kingdom}

\eads{\mailto{rhonda.au-yeung@strath.ac.uk}, \mailto{viv.kendon@strath.ac.uk}}
\vspace{10pt}
\begin{indented}
\item[]\today
\end{indented}

\begin{abstract}
Quantum computing promises to provide the next step up in computational power for diverse application areas.  In this review, we examine the science behind the quantum hype and breakthroughs required to achieve true quantum advantage in real world applications. Areas that are likely to have the greatest impact on high performance computing (HPC) include simulation of quantum systems, optimisation, and machine learning. We draw our examples from materials simulations and computational fluid dynamics which account for a large fraction of current scientific and engineering use of HPC. Potential challenges include encoding and decoding classical data for quantum devices, and mismatched clock speeds between classical and quantum processors. Even a modest quantum enhancement to current classical techniques would have far-reaching impacts in areas such as weather forecasting, engineering, aerospace, drug design, and realising ``green'' materials for sustainable development. This requires significant effort from the computational science, engineering and quantum computing communities working together.
\end{abstract}

\vspace{2pc}
\noindent{\it Keywords}: quantum algorithms, quantum computing, scientific computing

\maketitle
\ioptwocol

% ===========================================================
\tableofcontents 
\markboth{Quantum algorithms for scientific applications}{Quantum algorithms for scientific applications}

% ===========================================================
\section{Introduction}\label{sec:introduction}
% ===========================================================

The most advanced quantum computers are now approaching exascale classical computing capabilities. For most scientific and engineering applications, ``exascale'' means the system's aggregate performance crosses a threshold of $10^{18}$ IEEE 754 double precision (64-bit) operations (multiplications and/or additions) per second (exaFLOPS). On the other hand, machines just below exascale are used to verify quantum supremacy experiments. For example, random circuit sampling and Gaussian boson sampling methods respectively used the Summit ($\sim$150 petaFLOPs) \cite{Arute2019} and Sunway TaihuLight ($\sim$93 petaFLOPs) \cite{Li2022g} supercomputers. To compare, the Sony PlayStation 5 digital edition is listed as having a peak of $\sim$10 teraFLOPS. These supremacy tasks are not yet known to have any industrial or other useful applications. However, they do indicate that a quantum co-processor running a well-chosen sub-task could provide a huge increase in exascale hardware capabilities.

\begin{figure*}[ht!]
\centering
\includegraphics[width=\linewidth]{microprocessor-trends}
\caption{Microprocessor trend data, 1980-2021. Data up to year 2010 collected by M. Horowitz, F. Labonte, O. Shacham, K. Olukotun, L. Hammond, and C. Batten, data for 2010-2021 by K. Rupp \cite{Rupp2022}.}
\label{fig:microprocessor_trends}
\end{figure*}

Reaching for exascale computing calls for a paradigm shift from traditional computing advances \cite{Mann2020,Betcke2022}. Historical advances in computing were achieved via increasing clock speed and memory performance, adding more cores and parallelisation. Moore's law \cite{Moore1965} predicts that the exponential growth of silicon-based transistors on computer chips is about to hit physical limits \cite{Khan2018,Leiserson2020}. The growth predicted was borne out by major chip and computer system vendors until about 2005, when CPU clock speeds reached around 4 GHz (Figure \ref{fig:microprocessor_trends}). Significantly increasing the clock speed beyond this would require enormous effort to cool the processor to prevent malfunction or permanent hardware damage from overheating.

Launched in 1993, the TOP500 project assembles and maintains a list of the world's 500 most powerful computer systems \cite{top500}. As of November 2023, Oak Ridge National Laboratory's Frontier system is the only true exascale machine worldwide. In recent years, GPUs have enabled a step change in processing power -- all of the GREEN500 \cite{green500} use them. Only Fugaku in the top ten of the current TOP500 list \cite{top500} does not use GPUs. Physical constraints and power requirements are forcing computer vendors to develop new strategies to achieve more compute power. Novel hardware systems, including quantum computing, will be required to significantly extend global computational capacity beyond current capabilities.

The idea of using quantum systems to process information more efficiently than classical von Neumann computers was introduced some 40 years ago \cite{Feynman1982,Deutsch1985}. Since then there has been steady progress in quantum computing, building on the initial achievements of Shor's algorithm for factorisation \cite{Shor1994} that could break current methods of encryption, and Grover's algorithm \cite{Grover1996,Grover1997} for searching unsorted data. Quantum simulation is another promising field of quantum information processing \cite{Georgescu2014,Altman2021}. Many near-term applications of quantum computers fall under this umbrella. Quantum simulation involves modelling the quantum properties of systems that are directly relevant to understanding modern materials science, high-energy physics and quantum chemistry. Building on the successes in algorithm development and experimental work, we now want to examine how quantum devices can solve real-world problems. As the practical relevance of quantum computing becomes clearer, interest has grown beyond the confines of academia and many countries now have national strategies to develop quantum computing and quantum technology more generally, e.g., \cite{DSIT2023,Acin2018,Monroe2019}.

The consensus is that quantum devices will not simply replace HPC. It makes little sense (in terms of engineering and economics) to use quantum methods for problems that classical computers already do well.  Hence a hybrid solution is the most efficient, cost-effective and productive way to approach quantum computation. Instead of simply sending jobs to one or more CPUs, hybrid architectures delegate different parts of the problem to different types of processors, the most common currently being GPUs. Conceptually, quantum processor units (QPUs) are a natural extension to enhance the processing power of HPC. However, increasing the number and diversity of processors compounds the challenges for efficient programming and scheduling. Quantum computing hardware is not yet mature enough to seamlessly integrate with HPC.

\begin{figure*}[ht!]
\centering
\includegraphics[width=\linewidth,trim={0 7.5cm 0 7.5cm},clip]{companies7}
\caption{Quantum computing commercial landscape. Some examples of companies (blue) partnered with quantum tech companies (orange) to advance QC research. Cambridge Quantum Computing (CQC) and Honeywell now merged as Quantinuum (red). We include CQC, Honeywell and Quantinuum separately as some collaborations were formed before the merger in Dec 2021. Universities, research institutes and government organisations not shown. Compiled by R. Au-Yeung from press releases and publicly disclosed interactions with end users.}
\label{fig:commercial_landscape}
\end{figure*}

After several decades of largely academic-based theoretical and experimental development, growing commercial interest has resulted in venture capital \cite{MacQuarrie2020,Gibney2019,Bova2021} providing funding for many existing and start-up companies. The current landscape shows growing collaborations between quantum start-ups, established big players, and with well-established financial, pharmaceutical and automotive companies who wish to integrate quantum technologies into their development and production processes (Figure \ref{fig:commercial_landscape}). Notable players include Google, IBM and Rigetti, who are developing superconducting qubit systems, and Intel who are building systems based on quantum dots, with chips that can be made in its existing foundries. Cloud providers such as Amazon Web Services (AWS), are now selling time on different quantum systems, including QuEra, which uses neutral atoms in tweezer arrays, IonQ ion trap quantum processors, and superconducting qubit systems from Rigetti. Current systems are small and imperfect, known as noisy, intermediate scale quantum computers (NISQ) \cite{Preskill2018,Bharti2022}. They are not yet powerful enough to solve useful problems, but do allow researchers to test and develop quantum algorithms and applications to be ready for more advanced quantum hardware.

Several commercial partnerships are focused on integrating quantum processors with HPC. Finnish quantum start-up IQM is mapping quantum applications and algorithms directly to quantum processors to develop application-specific superconducting computers \cite{IQMtech}. The result is a quantum system optimised to run particular applications such as HPC workloads. The Atos-Pasqal collaboration are working to incorporate Pasqal's neutral-atom quantum processors into HPC environments \cite{Atos2020}. NVIDIA's cuQuantum software development kit (SDK) provides the tools needed to integrate and run quantum simulations in HPC environments using GPUs \cite{NVIDIAcuQuantum2022}, with the goal of adding QPUs to the SDK. Quantum software vendor Zapata anticipates the convergence of quantum computing, HPC, and machine learning, and has created the Orquestra platform to develop and deploy generative AI applications \cite{ZapataOrquestra}.


\subsection{Scope of this review}

Given the growing government and commercial investment in the development of quantum computing, it is timely to consider how it might accelerate computational science and engineering. Now considered the third pillar of scientific research alongside theoretical and experimental science, scientific computing has become ubiquitous in scientific and engineering applications. It leverages the exponential growth in computing power of the last decades, and is itself in receipt of even more government investment than quantum technology, given its essential role in science, engineering, and innovation. 

In the UK, HPC investments by the Engineering and Physical Sciences Research Council (EPSRC) cost \pounds466 million and are anticipated to add between \pounds3 billion and \pounds9.1 billion to the UK economy \cite{DSIT2022}. The European High-Performance Computing Joint Undertaking (EuroHPC JU) is a \texteuro7 billion joint initiative between the EU, European countries and private partners to develop a HPC ecosystem in Europe \cite{EuroHPC}. These examples show that governments are willing to invest significant resources into developing exascale technologies that can provide numerous societal and economic benefits.

Quantum computing has a broad reach across applications, so this review necessarily has to select which areas to focus on. We choose two important applications, namely, materials science and fluid dynamics simulations. In computational chemistry, the hard problems are fully quantum and therefore natural for quantum computers \cite{Feynman1982}. In fluid dynamics, the problem is purely classical. The hardness stems from the need to solve nonlinear differential equations. It is less clear that there will be substantial quantum advantages here, although there are multiple interesting proposals for possible quantum algorithms. Our chosen applications make up a significant fraction of current HPC use. Between them they are diverse enough to offer insights for other areas of computational science and engineering simulations.  Although we discuss quantum machine learning techniques relevant for chemistry and fluid simulations, the wider field of ``big data'' and artificial intelligence (AI) are outside the scope of this review. There are indeed potential quantum enhancements in AI, but they merit their own dedicated reviews (see, e.g., \citeauthor{Dunjko2018} \cite{Dunjko2018}).

In this review, we begin by describing the concepts behind quantum computers and giving examples of important foundational quantum algorithms in section \ref{sec:design}. For conciseness, we minimise the amount of mathematical detail where possible. Interested readers are encouraged to consult references such as \cite{Nielsen2010,Abhijith2022} for details and refer to the original papers for proofs. There are already many excellent reviews that cover different classes of quantum algorithms (see e.g. \cite{Montanaro2016,Albash2018,Childs2010r}), including a ``Quantum Algorithm Zoo'' website \cite{QuantumAlgorithmZoo} which cites over 400 papers. In this review, we aim to emphasise the physical mechanisms and qualitative insights into quantum algorithms and their real-world applications. We will not discuss abstract topics such as complexity analysis \cite{Bernstein1993,Vazirani2002}in depth. We also mention alternative quantum computing methods, such as quantum annealing in section \ref{ssec:qopt} (relevant for quantum simulation applications in section \ref{sec:qsim}) and variational quantum algorithms in section \ref{ssec:VQAs}. We outline some of the many proposed performance benchmarks for quantum computers in section \ref{ssec:benchmark}.

Next we focus on our two example fields: quantum simulation of quantum chemical systems (section \ref{sec:qsim}), and quantum algorithms for classical fluids simulation (section \ref{sec:qfluid}). We summarise the current (classical) methods, their shortcomings, and explore how quantum computers may improve performance. With the theoretical potential established, we turn to the practical challenges of how to combine quantum components into HPC architectures in section \ref{sec:hpc}. This includes the clock speed mismatch problem and how to encode classical data into quantum states, Finally, we summarise and set out future research directions in section \ref{sec:outlook}.

There are other important topics on the road to practical quantum computing that already have their own reviews. These include: quantum error correction \cite{Terhal2015,Roffe2019}; verification and testing \cite{Eisert2020,Carrasco2021} (we only cover benchmarks in section \ref{ssec:benchmark}); and comparisons between different hardware and software platforms available on the market \cite{MacQuarrie2020}.  Quantum annealing has been well-reviewed recently: \citeauthor{Hauke2020} \cite{Hauke2020} cover methods and implementations while \citeauthor{Yarkoni2022} \cite{Yarkoni2022} focus on industrial applications of optimisation problems.

Despite numerous publications hyping the quantum revolution (see, e.g., \cite{Arute2019,Wu2021,Kim2023a}), it pays to read the fine print \cite{Leymann2020,Coveney2020,Aaronson2015}. The methodology in \cite{Chancellor2020} is an instructive example of how to fairly and objectively evaluate different use cases. This helps to avoid evaluations that focus on the positives and ignore weaknesses. 

A key message from the IQM-Atos 2021 white paper \cite{IQM2021} is that HPC centres must create a long-term strategic plan to successfully integrate quantum computing into their workflow. In particular, they recommend establishing a three-step roadmap:
\begin{enumerate}
\item Gap analysis and quantum solution identification (now). Where are the bottlenecks in classical HPC? How can we improve the solution accuracy?
\item Quantum solution design and integration (mid-term). What quantum methods can we use to resolve the issues identified in gap analysis? How do we integrate this into HPC architectures?
\item Quantum computing use-case development and implementation (long-term). Which applications and problems can get the most benefit from quantum speedup?
\end{enumerate}
Our review provides an introduction to the tools and topics needed to make such plans.


% ===========================================================
% ===========================================================

\section{Designing quantum algorithms}\label{sec:design}

Developing useful quantum algorithms is extremely challenging, especially before quantum computing hardware is widely available. It is like building a car without having a road or fuel to take it for a test drive. Since quantum computing has a different logical basis from classical computing, it is not as straightforward as taking a successful classical algorithm and mapping it to the quantum domain. For example, a classical CNOT gate makes a copy of a classical bit. On the other hand, the no-cloning theorem \cite{Wootters1982} states that it is impossible to make copies of unknown quantum states. 

Quantum processors are now becoming increasingly available online through cloud computing services, and being installed as test beds in HPC facilities. This means designing and testing quantum algorithms for scientific applications is becoming timely and -- more importantly -- possible.

In this section, we outline the basic concepts underpinning quantum computing. Then, we describe some existing quantum algorithms that can potentially be adapted for practical scientific applications.

%---------------------------------------%
\subsection{Universal quantum computers}

The most widely used model for quantum computing is the gate or circuit model. The basic unit of quantum information is the quantum bit (qubit), a two-state system. Qubits are the quantum analogue of classical bits. They are two-level quantum systems that represent linear combinations of two basis states 
\begin{equation}
\ket{\psi} = \alpha\ket{0} + \beta\ket{1}
\end{equation} 
where $\alpha,\beta \in \mathbb{C}$ and $\vert\alpha\vert^2 + \vert\beta\vert^2 = 1$. 
In vector notation, we usually choose 
\begin{equation}
\ket{0} = \begin{bmatrix} 1 \\ 0 \end{bmatrix}, \quad \ket{1} = \begin{bmatrix} 0 \\ 1 \end{bmatrix}.
\end{equation}
The superposition property means that an $N$-qubit system can represent $2^N$ states whereas classical bits represent only one state at a time (a single $N$-bit string).  

Qubits can be entangled, forming systems with an exponentially large state space of size $2^N$ for $N$ qubits.  This corresponds to the tensor product of matrices, for example, 
\begin{equation}
\ket{0}\otimes\ket{0} = \begin{bmatrix} 1 \\ 0 \\ 0 \\ 0 \end{bmatrix}.
\end{equation} 

\begin{figure}[t!]
\centering
\includegraphics[width=\linewidth]{bloch-sphere}
\caption{Bloch sphere representation of qubit state $\ket{\psi}$ with examples of gate operations.}
\label{fig:bloch-sphere}
\end{figure}

We visualise qubits using a Bloch sphere (Figure \ref{fig:bloch-sphere}). To perform calculations, we manipulate the qubit state using (reversible) gate operations. These are analogous to the classical logic gates, such as AND, OR and NOT.

\begin{table}[ht!]
\centering
\caption{Examples of commonly used quantum gates. Middle column uses circuit notation where horizontal lines represent qubits. Boxes, crosses and dots indicate the gate applied, with time flowing from left to right. Right-hand column shows the matrix representation, contains spherical polar co-ordinates $\vartheta$ and $\varphi$ (Figure \ref{fig:bloch-sphere}).}
\bgroup
\def\arraystretch{1}
\begin{tabular}{p{1.1in}p{.8in}p{.8in}}
\textbf{Operator} & \textbf{Gate} & \textbf{Matrix} \\[4pt]
\hhline \\
One-qubit gate && \\
\hline \\
\makecell{Pauli-X or NOT \\ {\small (bit flip)}} &
\makecell{\includegraphics[width=.8in]{basic-gates-pauli-x}} &
$\begin{bmatrix} 0 & 1 \\ 1 & 0\end{bmatrix}$
\\
\makecell{Pauli-Z \\ {\small (phase flip)}} &
\makecell{\includegraphics[width=.8in]{basic-gates-pauli-z}} &
$\begin{bmatrix} 1 & 0 \\ 0 & -1\end{bmatrix}$
\\
\makecell{Hadamard \\ {\small (create superposition)}} &
\makecell{\includegraphics[width=.8in]{basic-gates-hadamard}} &
$\dfrac{1}{\sqrt{2}}\begin{bmatrix} 1 & 1 \\ 1 & -1\end{bmatrix}$
\\
T gate &
\makecell{\includegraphics[width=.8in]{basic-gates-t}} &
$\begin{bmatrix} 1 & 0 \\ 0 & e^{i\pi/4} \end{bmatrix}$
\\
Phase shift &
\makecell{\includegraphics[width=.8in]{basic-gates-phase-shift}} &
$\begin{bmatrix} 1 & 0 \\ 0 & e^{i\vartheta} \end{bmatrix}$
\\
Z-rotation &
\makecell{\includegraphics[width=.8in]{basic-gates-z-rotation}} &
$\begin{bmatrix} e^{-i\varphi/2} & 0 \\ 0 & e^{i\varphi/2} \end{bmatrix}$
\\
\hline \\
Two-qubit gates && \\
\hline \\
\makecell{CNOT \\ {\small (flip target qubit if} \\ {\small control qubit is $\ket{1}$)}} &
\makecell{\includegraphics[width=.8in]{basic-gates-cnot}} &
$\begin{bmatrix} 1 & 0 & 0 & 0 \\ 0 & 1 & 0 & 0 \\ 0 & 0 & 0 & 1 \\ 0 & 0 & 1 & 0 \end{bmatrix}$
\\
\makecell{SWAP \\ {\small (swap two qubit} \\ {\small states)}} &
\makecell{\includegraphics[width=.8in]{basic-gates-swap}} &
$\begin{bmatrix} 1 & 0 & 0 & 0 \\ 0 & 0 & 1 & 0 \\ 0 & 1 & 0 & 0 \\ 0 & 0 & 0 & 1 \end{bmatrix}$
\\
\hhline{===}
\end{tabular}
\egroup
\label{tab:basic-gates}
\end{table}

Gate operations are operators applied to the qubits. They are written as matrices (as shown in Table \ref{tab:basic-gates}. In quantum computing literature, we use a convenient graphical way to depict the unitary transformations (gate operations) on a one or $n$-qubit system. The building blocks of the transformations are $2 \times 2$ and $4 \times 4$ unitary matrices on single qubits and two qubits respectively. Applying operator $U$ to a state, then its conjugate transpose $U^\dagger$ brings the system back to its original state ($UU^\dagger = \mathbb{1}$). 
As in classical computing, any computation can be built up from a small set of universal gates.  A universal gate set includes at least one entangling gate acting on two or more qubits, and one or more single qubit rotations.

\subsection{Quantum computing stack}

\begin{figure}[ht!]
\centering
\includegraphics[width=\linewidth]{stack2013}
\caption{Quantum computer architecture among some sub-fields of quantum computation. Reproduced from \cite{vanMeter2013}}
\label{fig:stack2013}
\end{figure}

We decompose the quantum computer architecture in terms of its various levels of abstraction or ``stack''. In gate-based models, this extends from the user interface and compiler down to the low-level gate operations on the physical hardware itself (Figure \ref{fig:stack2013}). The various levels of the quantum stack do not work in the same way as classical computers. Hence it is critical that quantum computers be designed and operated with the entire stack in mind, from algorithms and quantum software down to control engineering and qubit technology \cite{Alexeev2021}. 

At the highest levels, quantum algorithms give instructions for solving the numerical problem. Compiling and translating quantum gates in the middle levels (software stack) compress the algorithms to accelerate performance, while error-correction techniques mitigate quantum computing errors. At the lowest levels, controlling the interactions between qubit technologies may lead to better performance. Finally, future qubit technologies will require tight integration with other layers of the stack to realise their potential. 

Engineering qubits in real physical hardware is highly challenging.  The requirements for scalable quantum computing laid out by \citeauthor{DiVincenzo2000} \cite{DiVincenzo2000} are still relevant today: 
scalable, well-characterized qubits;
the ability to initialize the qubits;
long decoherence times;
a ``universal'' set of quantum gates;
and the ability to measure the qubits.


%------------------------------------------%
\subsection{Foundational quantum algorithms}

In this subsection, we introduce some foundational quantum algorithms which can be used as components in quantum algorithms for a wide variety of applications. They also illustrate how quantum algorithms can provide powerful speed ups over classical algorithms for the same problems.  For readers seeking a comprehensive graduate-level introduction, we recommend \cite{Abhijith2022,Portugal2022,Nielsen2010} plus IBM's Qiskit software documentation \cite{Qiskit}.

%-------------------------------------%
\subsubsection{Quantum Fourier transform}\label{sssec:QFT}\hfill

The quantum Fourier transform (QFT) is a key subroutine in many quantum algorithms, most prominently Shor's factoring algorithm \cite{Shor1994}. The QFT operation changes the quantum state from the computational (Z) basis to the Fourier basis. It is the quantum analogue of the discrete Fourier transform and has exponential speed-up (complexity $O((\log N)^2)$) compared to the fast Fourier transform's $O(N \log N)$ complexity. The QFT performs a discrete Fourier transform on a list of complex numbers encoded as the amplitudes of a quantum state vector. It stores the result ``in place'' as amplitudes of the updated quantum state vector.  Measurements performed on the quantum state identify individual Fourier components -- the QFT is not directly useful for determining the Fourier-transformed coefficients of the original list of numbers, since these are stored as amplitudes.

When applying the QFT to an arbitrary multi-qubit input state, we express the operation as
\begin{equation}
\hat{U}_\textrm{QFT} = \frac{1}{\sqrt{N}} \sum_{n,m=0}^{N-1} e^{2\pi i nm/N} \ket{m}\bra{n}
\end{equation}
which acts on a quantum state in $N$-dimensional Hilbert space $\ket{X} = \sum_{n=0}^{N-1} x_n \ket{n}$ and maps it to $\ket{Y} = \sum_{m=0}^{N-1} y_m \ket{m}$. 
This is a unitary operation on $n$ qubits that can also be expressed as the complex $N \times N$ matrix
\begin{align}
&\textsf{\textbf{F}}_N = \nonumber\\
&\frac{1}{\sqrt{N}} \begin{bmatrix}
1 & 1 & 1 & 1 & ... & 1 \\
1 & \omega & \omega^2 & \omega^3 & ... & \omega^{N-1} \\
1 & \omega^2 & \omega^4 & \omega^6 & ... & \omega^{2(N-1)} \\
1 & \omega^3 & \omega^6 & \omega^9 & ... & \omega^{3(N-1)} \\
\vdots & \vdots & \vdots & \vdots & \ddots & \vdots \\
1 & \omega^{N-1} & \omega^{2(N-1)} & \omega^{3(N-1)} & ... & \omega^{(N-1)^2}
\end{bmatrix} \label{eq:qft-matrix}
\end{align}
where $\omega=e^{2\pi i/N}$ and $N=2^n$.
Note that the Hadamard gate (Table \ref{tab:basic-gates}) acts as the single-qubit QFT, it transforms between the Z-basis states $\ket{0}$ and $\ket{1}$ to the X-basis states 
\begin{equation}
\ket{\pm} = \frac{1}{\sqrt{2}} (\ket{0} \pm \ket{1}).
\end{equation}

\begin{figure*}[ht!]
\centering
\includegraphics[width=\linewidth]{qft-detailed-v2}
\caption{QFT operation \textbf{\textsf{F}}$_N$ on $N$ qubits. Adapted from \cite{Camps2020}.}
\label{fig:qft-detailed}
\end{figure*}

Figure \ref{fig:qft-detailed}(a) shows the quantum circuit for the QFT operation. Each multi-qubit gate block can be decomposed into a sequence of single or two-qubit gates. For example, the $\textsf{\textbf{B}}_{k+1}$ block is written in terms of controlled operations involving two qubits (Figure \ref{fig:qft-detailed}(b)). The $\textsf{\textbf{B}}_{k+1}$ gate is applied to the last $k+1$ qubits. All controlled-$R$ gates are diagonal matrices and commute,
\begin{equation}
R_N = \begin{bmatrix} 1 & 0 \\ 0 & e^{-2\pi i/N} \end{bmatrix}
\end{equation}
so the order in which they are applied does not change the outcome. The bit-reversal permutation matrix $\textsf{\textbf{P}}_N$ can first be written in terms of $\lfloor N/2 \rfloor$ swap gates (Figure \ref{fig:qft-detailed}(c)). Since each swap can be implemented with three CNOT gates, $\textsf{\textbf{P}}_N$ requires $\lfloor 3N/2 \rfloor$ CNOT gates. Hence the QFT requires $O(N^2)$ elementary gates in total.

The semi-classical nature of the QFT circuit means it can be efficiently simulated on classical computers \cite{Griffiths1996,Browne2007} when used at either the start or end of a quantum computation. The idea is to measure a qubit, then use the result to produce a classical signal which controls a one-qubit transformation on the next qubit before it is measured, and so forth.


%--------------------------------------%
\subsubsection{Quantum phase estimation}\label{sssec:QPE}\hfill

Kitaev's quantum phase estimation (QPE) algorithm \cite{Kitaev1996} is a core component of many quantum algorithms and an important technique in algorithm design. While the standard procedure is based on the QFT circuit, there have been various improvements \cite{Ahmadi2012} which try to reduce the dependence on QFT while retaining its accuracy.

Given an equation for some black-box operator $U$,
\begin{equation}
U \ket{\psi} = \lambda \ket{\psi},
\end{equation}
we want to find an eigenvalue $\lambda$ of the eigenvector $\ket{\psi}$. Because $U$ is unitary, the eigenvalue can be expressed as $\lambda = e^{2\pi i \theta}$ and the real aim is to estimate the eigenvalue phase $\theta$.

\begin{figure}[ht!]
\centering
\includegraphics[width=\linewidth]{qpe-all}
\caption{Structure of quantum phase estimation circuit, where iQFT is the inverse of the QFT operation.}
\label{fig:qpe}
\end{figure}

Phase estimation contains two main steps (Figure \ref{fig:qpe}). In the circuit, the top register contains $n$ ``counting'' qubits in state $\vert 0 \rangle$. The second register contains as many qubits as needed to store the eigenstate of $U$, $\ket{\psi}$. We apply a set of Hadamard transforms to the first register (this is equivalent to a QFT on the all zeros state) and controlled-$U$ operations on the second register with $U$ raised to successive powers of two. The final state of the first register is 
\begin{align}
\frac{1}{\sqrt{2^n}} \sum_{k=0}^{2^n-1}e^{2k\pi i\theta} \vert k \rangle.
\end{align}
Applying the inverse QFT (iQFT) gives
\begin{equation}
\frac{1}{\sqrt{2^n}} \sum_{k=0}^{2^n-1}e^{2k\pi i\theta} \vert k \rangle \vert \psi \rangle \to \vert\tilde{\theta}\rangle \vert \psi \rangle.
\end{equation}
The final stage is to read out the state of the first register by measuring in the computational basis. 

Quantum phase estimation is equivalent to simulating quantum system dynamics. We want to estimate the eigenvalue of Hamiltonian $\hat{H}$ by querying $\hat{H}$ via the time-evolution operator $U=e^{-i\hat{H}t}$. When the algorithm implements $U^k$ for increasing powers $k$, this reduces to evolving the system for increasing times $t$ \cite{AspuruGuzik2005}. The phases $\lambda$ are eigenvalues of the time evolution operator $U$ which we can map back on to the eigenenergies of $\hat{H}$. Computational chemistry often requires finding the eigenenergies, as they can help deduce many chemical properties like ionisation potential and equilibrium constants.

The circuit outputs an eigenphase classically and also prepares the corresponding eigenstate. This is a useful starting point for calculating other observables besides energy, or for preparing a reference state to obtain the ground-state energy for a more precise Hamiltonian. Once we have the eigenstates and eigenenergies of interest, we can understand how a (closed) system evolves in time by decomposing the initial state into a sum of eigenstates and evolving each eigenstate according to the phase found in QPE. The final state is an interferometric sum of all components.

There are two important performance metrics for the QPE: maximum run time, and total run time over all repetitions from each circuit in the algorithm. Maximum and total run time approximately measure the circuit depth and total cost of the algorithm respectively. The QPE algorithm in Figure \ref{fig:qpe} is unsuitable for early fault-tolerant quantum computers since these devices have low circuit depths and limited number of logical qubits.


%-------------------------------------%
\subsubsection{Shor's algorithm}\hfill

While Feynman's 1982 arguments \cite{Feynman1982} formalised the idea of using quantum computers to solve problems, what really caught people's attention was Shor's prime factoring algorithm in 1994 \cite{Shor1994,Ekert1996}. It provided a way for large enough (future) quantum computers to break RSA encryption \cite{Rivest1978}, then considered unbreakable due to the exponential computational cost of factoring large numbers. RSA encryption works by multiplying two large prime numbers $N=pq$. The product is so large that the computational cost of factorising -- and hence recovering the encrypted message -- is impractical. Shor's algorithm showed that factoring could be solved by quantum computers in polynomial time. 

\begin{figure*}[ht!]
\centering
\includegraphics[width=\linewidth]{shor-simples}
\caption{Quantum circuit schematics for Shor's algorithm. \textbf{(a)} Circuit for calculating function $f(x)=a^x(\text{mod} N)$, \textbf{(b)} Shor's algorithm for period finding.}
\label{fig:shor-circuit-eq}
\end{figure*}

Shor's algorithm uses two quantum memory registers. The input register contains $n$ qubits and stores an argument of function 
\begin{equation}\label{eq:shor-f}
f(x)=a^x \text{mod}N. 
\end{equation}
The output register stores the values of $f(x)$. The latter takes values between $0$ and $N$, so the output register must contain $n_0$ qubits with initial state $\ket{2^{n_0}-1}_{n_0}$ and each qubit is set to $\ket{1}$. To solve the problem with sufficient accuracy, we need $n \geq 2n_0$ \cite{Shor1994}. The algorithm consists of two parts (Figure \ref{fig:shor-circuit-eq}): modular exponentiation \cite{Vedral1996,Pavlidis2014}, followed by an inverse quantum Fourier transform.

We initially set the input register to state $\ket{0}_n$. Applying a Hadamard gate to each qubit transforms the state into an equally weighted superposition of all basis states of the $n$-qubit system. Then applying the linear operation $U$ to this superposition (Figure \ref{fig:shor-circuit-eq}(a)) gives
\begin{gather}
\ket{0}_n \ket{2^{n_0}-1}_{n_0} \to
\frac{1}{\sqrt{2^n}} \sum_{x=0}^{2^n-1} \ket{x}_n \ket{2^{n_0}-1}_{n_0} \nonumber\\ 
\to
\frac{1}{\sqrt{2^n}} \sum_{x=0}^{2^n-1} \ket{x}_n \ket{f(x)}_{n_0}.
\end{gather}
This procedure prepares a state in the memory register that contains a superposition of the modular expressions for each value of $x$ in superposition in the input register.  Classically, we have to evaluate $a^x\text{mod}N$ for each $x$ in turn. The algorithm achieves quantum advantage here by evaluating for all possible values of $x$ in one go, then extracting the interesting answers through the second part of the algorithm.

Measuring the output register outputs one possible value of the function $a^x\text{mod}N$, say $f'$. Since it is a periodic function, then $a^{x+r} = a^x\text{mod}N$ so $a^r \equiv 1\text{mod}N$ with period $r$. Due to the periodicity, there exist integers $x \in [0,2^n-1]$ where $f$ takes the same value $f'$. The state after measurement is
\begin{equation}
\frac{1}{\sqrt{N}} \sum_{k=0}^{N-1} \ket{x_0+kr}_n \ket{f(x_0)}_{n_0}
\end{equation}
which contains information about the period $r$. However, measuring the input register would output with equal probability in one of the integers $x_0+kr$, where all $x_0$, $k$, and $r$ are unknown. Therefore, one such measurement is insufficient to determine $r$. On the other hand, finding the period can be done efficiently using quantum phase estimation. This is the second part of the circuit for Shor's algorithm (Figure \ref{fig:shor-circuit-eq}(b)) which corresponds to the circuit in subsection \ref{sssec:QPE}. Given the period, classical post-processing can find the prime factors with high probability. If it fails, the algorithm is repeated with a different choice of co-prime $a$.

Classical computers have to date factored semi-primes up to 829 bits long \cite{classicalfactoring}. Each extra bit doubles the classical computational cost. However, quantum computers large enough to beat this are still some way into the future. This is due to the error correction overheads required to carry out the quantum computation accurately enough \cite{vanMeter2016}.


%-------------------------------------%
\subsubsection{Grover's algorithm}\label{sssec:search}\hfill

Grover's algorithm \cite{Grover1996,Grover1997} is a technique for searching unstructured data. The canonical example is finding the name corresponding to a given phone number.  It can be used as a building block in other applications, such as database search and constraint satisfaction problems \cite{Ambainis2006}. Grover's algorithm was subsequently generalised to the framework of quantum amplitude amplification \cite{Brassard2002} which we summarise here.
%
\begin{figure*}[ht!]
\centering
\includegraphics[width=\linewidth]{grover_all}
\caption{Grover's algorithm. \textbf{(a)} Initial state vector $\ket{d}$ with angle $\theta/2$. \textbf{(b)} Vector $\ket{d}$ after inversion and reflection. \textbf{(c)} Final state vector $\ket{\psi}$.}
\label{fig:grover}
\end{figure*}

Suppose there are $N$ items labelled by bit strings $x=0$ to $N-1$, and there is a unique element $x_0$ such that $f(x)=1$ if and only if $x=x_0$. The quantum algorithm uses $n$-qubit basis states $\ket{x}$ corresponding to the labels of the items, with $n=\lceil\log_2 N\rceil$.

Grover's algorithm uses an oracle, which in computer science is a mathematical device for providing part of the computation that is not included in the analysis.  Consequently, Grover's algorithm is only of practical use in situations where we can perform the oracle function efficiently, i.e., $f(x)$ can be evaluated for a superposition of all possible inputs $\ket{x}$.  This can happen when the algorithm is used as a subroutine, but is not generally possible when the data are classical.
The oracle $\mathcal{O} = (-1)^{f(x)}$ has the effect
\begin{equation}
\mathcal{O} \ket{x} =
\begin{cases}
-\ket{x_0}, & x=x_0 \\
\ket{x}, & \textrm{otherwise.} \\
\end{cases}
\end{equation}

Figure \ref{fig:grover} illustrates in two dimensions how amplitude amplification works, through inversion about the mean.  The actual state vector is in an $N$-dimensional Hilbert space.  For large $N$, an initial state $\ket{d}$ that is an equal superposition of all possible $\ket{x}$ is almost orthogonal to the marked state $\ket{x_0}$. In Figure \ref{fig:grover}(a), the small angle $\vartheta/2\simeq 1/\sqrt{N}$.  The oracle $\mathcal{O}$ acts on only $\ket{x_0}$ by inverting the sign of its amplitude. Geometrically, $\mathcal{O}$ reflects vector $\ket{d}$ about the horizontal axis. Then applying the Grover operator
\begin{equation}
U_G = 2 \ket{d} \bra{d} - \mathbbm{1}
\end{equation}
reflects $\mathcal{O}\ket{d}$ about the axis defined by $\ket{d}$. Figure \ref{fig:grover}(b) shows the action of $\mathcal{O}$ and $U_G$.  Since $\vartheta$ is a small angle, this achievement is modest. Solving for $r\vartheta = \pi/2$ gives an approximate number of required iterations $r$,
\begin{equation}
r = 
\Big\lfloor \frac{\pi}{2\vartheta} \Big\rfloor 
=
\Big\lfloor \frac{\pi}{4} \sqrt{N} \Big\rfloor
\end{equation}
to reach the final state within $\vartheta/2$ of $\ket{x_0}$. The probability of measuring $\ket{x_0}$ is better than $1-1/N$.

Since the number of iterations $r$ scales as $\sqrt{N}$, Grover's algorithm generally provides a square root speed up over a classical search, which would need to check on average $N/2$ items to find $x_0$. Among the proposed applications, a Grover search-based method for matched filtering can help detect gravitational waves \cite{Gao2022} by analysing noisy data collected by the Laser Interferometer Gravitational Observatory (LIGO). Matched filtering has detected numerous gravitational waves since LIGO picked up its first signal in September 2015, but it is time-consuming and resource-intensive. The quantum method provides a square-root speedup in the computational cost of searching through a large template bank, assuming the oracle function can be efficiently implemented.

There are alternative quantum algorithms that solve the search problem, such as quantum walks \cite{Shenvi2002}. These are easily generalised to find multiple items \cite{Childs2003} and are intuitive for graph based search problems.


%-------------------------------------%

\subsubsection{Quantum walks}\label{sssec:qw}\hfill

In the last few decades, classical random walks and Markov chains have generated powerful new algorithms in computer science and mathematics \cite{Motwani1995}. They were thus a natural place to look for quantum equivalents. Early in the development of quantum algorithms both discrete-time \cite{Aharonov2000,Ambainis2001} and continuous-time quatum walks \cite{Farhi1998} were introduced with algorithmic applications in mind.  The first proven quantum speed ups for quantum walk algorithms came soon after, with a search algorithm \cite{Shenvi2002} equivalent to Grover's algorithm \cite{Grover1996}, and an algorithm for transport across a particular type of disordered graph \cite{Childs2003b} that provides an exponential speed up with respect to an oracle.  
There are many comprehensive reviews of quantum walks and their applications, including \cite{VenegasAndraca2012} and \cite{Kadian2021}.

Quantum walks are widely studied for their mathematical properties, and for providing simple models of physical phenomena.  In an algorithmic context, they provide quantum speed up for sampling problems (see section \ref{ssec:QMC} on quantum Monte Carlo).  The quantum walk dynamics provide both faster spreading \cite{Ambainis2001} and faster mixing \cite{Aharonov2000}, but also localisation. These are exploited in the search algorithm \cite{Shenvi2002}.  The fast mixing dynamics can also be exploited in a quantum annealing (section \ref{ssec:qopt}) context, see, for example \cite{Callison2019}.  Quantum walks also provide models of universal quantum computing \cite{Childs2009, Lovett2010}, but they are not suitable for physical implementation. Instead these are most useful for complexity proofs.  

Quantum walks with multiple non-interacting walkers and quantum particle statistics include the boson sampling model \cite{Aaronson2011}, a computational model intermediate between classical and quantum computing.
Quantum walks with multiple interacting walkers \cite{Aaronson2011} are a full universal model of quantum computing.  They are a special case of quantum cellular automata (see, for example, \cite{Schumacher2004}) and provide an alternative physical architecture for quantum computing, most suitable for neutral atoms in optical lattices \cite{Karski2009}.

%-------------------------------------%

\subsubsection{Harrow-Hassidim-Lloyd algorithm}\label{sssec:HHL}\hfill

Harrow, Hassidim and Lloyd (HHL) developed their algorithm in 2009 \cite{Harrow2009} to solve one of the most basic classical problems in scientific computing. The HHL algorithm efficiently calculates the solution $\mathbf{x} = A^{-1}\mathbf{b}$ of the linear system $A\mathbf{x} = \mathbf{b}$ for a sparse, regular $N \times N$ Hermitian matrix 
\begin{gather}
A = \sum_j \lambda_j\ket{u_j}\bra{u_j} \\
A^{-1} = \sum_j \lambda_j^{-1}\ket{u_j}\bra{u_j}
\end{gather}
with eigenvectors $\ket{u_j}$ and eigenvalues $\lambda_j$ of $A$, and $N$-dimensional vector 
\begin{equation}
\ket{b} = \sum_j b_j\ket{u_j}.
\end{equation}
The aim is to (approximately) solve
\begin{equation}
\ket{x} = A^{-1} \ket{b} = \sum_j \lambda_j^{-1}b_j\ket{u_j}.
\end{equation}

\begin{figure*}[ht!]
\centering
\includegraphics[width=\linewidth]{hhl-circuit-v2}
\caption{High level HHL circuit. Adapted from \cite{Dervovic2018}.}
\label{fig:hhl_circuit}
\end{figure*}

We now present a high level description of the HHL algorithm. For further details and analysis, see \cite{Dervovic2018,Abhijith2022}. We start with the ancilla $S$, clock $C$, and input $I$ registers (Figure \ref{fig:hhl_circuit}). We load $b$ in the input register $\ket{b}_I = \sum_j \beta_j \ket{u_j}$ with eigenvectors $u_j$ of $A$, and clock register $\ket{0}^{\otimes N}_C$. Then the $\ket{b}_I \ket{0}^{\otimes N}_C$ state is ready for quantum phase estimation (QPE). For this, we use Hamiltonian simulation to transform the Hermitian matrix $A$ into a unitary operator
\begin{equation}
U = e^{iAt}.
\end{equation}
This produces the eigenvalues $\lambda_j$ of $A$ which are stored in the clock register:
\begin{equation}
\ket{b}_I \ket{0}^{\otimes N}_C = 
\sum_j \beta_j \ket{u_j}_I \ket{0}^{\otimes N}_C
\xrightarrow[]{\text{QPE}}
\sum_j \beta_j \ket{u_j}_I \ket{\tilde{\lambda}_j}_C
\end{equation}
where $\tilde{\lambda}_j$ is $\lambda_j$ in binary form.

The next step is to multiply $1/\lambda_j$ by a control rotation using clock qubits, i.e. 
\begin{equation}
\sum_j \beta_j \ket{u_j}_I \ket{\tilde{\lambda}_j}_C \ket{0}_S.
\end{equation}
Hence 
\begin{gather}
\sum_j \beta_j \ket{u_j}_I \ket{\tilde{\lambda}_j}_C \ket{0}_S \nonumber\\
\to
\sum_j \beta_j \ket{u_j}_I \ket{\tilde{\lambda}_j}_C 
\left( 
\sqrt{1-\frac{c^2}{\lambda_j^2}}\ket{0}_S + \frac{c}{\lambda}\ket{1}_S
\right)
\end{gather}
with normalisation constant $c$. The $R$ gate performs a $\sigma_y$ rotation conditioned on the $\tilde{\lambda}_i$ value. Then an inverse QPE restores the clock register,
\begin{equation}
\sum_j \beta_j \ket{u_j}_I \ket{0}^{\otimes N}_C 
\left( 
\sqrt{1-\frac{c^2}{\lambda_j^2}}\ket{0}_S + \frac{c}{\lambda}\ket{1}_S
\right).
\end{equation}

Finally, measuring the ancilla qubit gives
\begin{equation}
\ket{x} \approx
\sum_j \frac{\beta_j c}{\lambda_j} \ket{u_j} 
\end{equation}
if the measurement outcome is $\ket{1}$.

There are many limitations of the HHL algorithm \cite{Aaronson2015,Dervovic2018}. In short, they are:
\begin{itemize}
\item $A$ must be Hermitian.
\item Eigenvalues of $A$ must be in $[0,1)$.
\item We must be able to efficiently implement QPE with $e^{iAt}$.
\item We must be able to efficiently prepare $\ket{b}$.
\item We cannot recover the full solution $\ket{x}$. Instead we may recover the expectation value of some operator $M$, such as $\bra{x}M\ket{x}$.
\end{itemize}

The importance of HHL for practical applications has led to many improvements on the original algorithm \cite{Childs2017}, such as using time variable amplitude amplification to increase the success probability during ancilla measurement \cite{Ambainis2012}. 

While the overall algorithm has seen improvements, so have its individual components such as Hamiltonian simulation \cite{Berry2007,Liu2022h}. Phase estimation remains a bottleneck by requiring $\Omega(1/\epsilon)$ applications of a unitary operation to estimate its eigenvalues to precision $\epsilon$. Hence, even if we use the fastest Hamiltonian simulation methods, the overall improvement would likely be modest with total complexity still being polynomial in $(1/ \epsilon)$. One remedy involves directly applying the matrix inverse to bypass the phase estimation subroutine \cite{Childs2017}, thus exponentially improving dependence on the precision parameter. This approach has subsequently found good use in applications ranging from accelerating finite element methods (FEMs) \cite{Montanaro2016a} to approximating the hitting times of Markov chains \cite{Chowdhury2017}. 

Alternatively, HHL can be ``hybridised'' into a quantum-classical method. Given the resource limitations of NISQ-era hardware, hybrid approaches will likely pave the way for meaningful applications in the near future. For example, an iterative-HHL approach \cite{Saito2021} contains a classical iterative process to improve accuracy beyond the limit imposed by number of qubits in QPE. However, the convergence rate for more general cases remains uncertain. Other approaches include streamlining the algorithm, such as using a classical information feed forward step after the initial QPE to reduce the circuit depth for subsequent steps in HHL \cite{lee2019hybrid}. Of course, if the reduced circuit is not applicable, it would then require repeating QPE to determine an appropriate reduction. A hybrid approach can achieve comparable precision to conventional HHL with fewer qubits and multiple phase estimation modules \cite{Gao2023h}. It can also improve phase estimation itself to mitigate the long coherence times \cite{angara2020hybrid}. 

Solving dense matrices, particularly relevant in machine learning and kernel methods \cite{Wilson2015}, has been tackled by using quantum singular value estimation (QSVE) to gain a polynomial advantage over traditional HHL approaches \cite{Wossnig2018}. There are many other QML algorithms where HHL is an important subroutine, such as data classification using quantum support vector machines \cite{Rebentrost2014}, quantum principal component analysis \cite{Lloyd2014}, or quantum linear discriminant analysis \cite{Cong2016}; quantum ordinary linear regression (QOLR) \cite{Wiebe2012,Wang2017a} and QOLR for prediction \cite{Schuld2016}; quantum ridge regression (QRR) \cite{Yu2021a}; quantum recommendation systems \cite{Kerenidis2017}; quantum singular value thresholding \cite{Duan2018}; and quantum Hopfield neural networks \cite{Rebentrost2018}. More advanced HHL-based methods include solving linear differential systems \cite{Berry2014,Berry2017}; using techniques from adiabatic optimisation to solve linear equations \cite{Subasi2019}; and reformulating least-squares regression as a QUBO problem and solving with adiabatic quantum computation \cite{Date2021}.

%===============================%

\subsection{Quantum optimisation}\label{ssec:qopt}

Combinatorial optimisation problems are widely encountered across industry, academia, and the public sector \cite{Hauke2020,Yarkoni2022,Emani2021}.  Typically, the problem is to find the best of an exponentially large set of solutions. Mathematically, a basic optimisation problem consists of the objective (cost) function which is the output we want to maximise or minimise, the input variables, and optional constraints.  

Many optimisation problems can be expressed as quadratic unconstrained binary optimisation (QUBO) problems.
These are encoded using an upper-triangular $N \times N$ real matrix $Q$ and vector of binary variables $\mathbf{x}$. The aim is to minimise the cost function
\begin{equation}\label{eq:qubo}
f(\mathbf{x}) = \sum_j Q_{jj} x_j + \sum_{j<k} Q_{jk} x_j x_k.
\end{equation}
Many cost functions contain a large number of false local minima, making it difficult for classical algorithms to find the true global minimum.

Classical simulated annealing \cite{Kirkpatrick1983} is based on the idea that thermal fluctuations move the system out of local minima and toward the lowest potential energy state as temperature decreases. The simulated annealing algorithm simulates a random walker that travels through the search space or optimisation landscape.  The rate at which the temperature is reduced determines how fast the system evolves and how likely it is to avoid becoming stuck in local minima.

Quantum annealing \cite{Finilla1994,Kadowaki1998} was conceived as an alternative that adds quantum tunnelling to the system evolution, allowing it to escape from local minima more easily.  Figure \ref{fig:annealing} illustrates the quantum annealing process. 
%
\begin{figure*}[ht!]
\centering
\includegraphics[width=\linewidth]{quantum-anneal-all}
\caption{The full quantum annealing process from problem formulation to solution.  The graphs illustrate how the initially uniform distribution evolves under the driver Hamiltonian to concentrate in the global minimum representing the solution. Adapted from \cite{AuYeung2023}.}
\label{fig:annealing}
\end{figure*}

Conveniently, classical optimisation problems can be mapped to finding the ground state of a classical Ising Hamiltonian \cite{Lucas2014,Choi2010}.  This leads naturally to a quantum version using the transverse field Ising Hamiltonian,
\begin{align}
\hat{H}_\text{Ising} = A(t) \underbrace{\sum_n \hat{\sigma}^x_n}_{\hat{H}_0} + 
B(t) \underbrace{\sum_{n,m} 
\left( 
h_n \hat{\sigma}^z_n + 
J_{nm} \hat{\sigma}^z_n \hat{\sigma}^z_m 
\right)}_{\hat{H}_p}
\end{align}
with are the Pauli $x$- and $z$-matrices $\hat{\sigma}^{x,z}$, symmetric interaction strength $J_{nm}=J_{mn}$ of qubit spins $q_n$ and $q_m$, and on-site energy $h_n$. Note that the qubit spin basis states are $\ket{\pm 1}$. There is a straightforward translation from QUBO $\mathbf{x}$ to Ising $\mathbf{q}$, using $x_n = (q_n+1)/2$. The resulting values of the $\{h_n, J_{mn}\}$ variables then encode the problem into the Ising Hamiltonian $\hat{H}_p$.

The transverse field in the $x$-direction $\hat{H}_0$ provides the dynamics that rotate the qubits from their initial state to the final target state, ideally the ground state of problem Hamiltonian $\hat{H}_p$. The control functions $A(t)$ and $B(t)$ must be specified. Usually $A(t)$ varies from 1 to 0, and $B(t)$ from 0 to 1. The choice of how they vary determines the method used to find the ground state and hence the problem solution.

Adiabatic quantum computing (AQC), introduced by \citeauthor{Farhi2001} \cite{Farhi2001} in \citeyear{Farhi2001}, uses the quantum adiabatic theorem to guarantee that the system remains in the ground state. It assumes that the control functions $A(t)$ and $B(t)$ are varied slowly enough to keep the system in the instantaneous ground state. AQC has several useful properties: it is inherently robust against noise \cite{Childs2001}. It is computationally equivalent to gate-based quantum computers \cite{Aharonov2004}, making it a universal quantum computing paradigm \cite{Aharonov2008}. 

While AQC provides a sound theoretical underpinning for quantum annealing, in reality, the adiabatic conditions imposed by AQC are rarely met. Meeting them would require long run times that are inefficient and impractical. At the other extreme, continuous-time quantum walk algorithms \cite{Farhi1998} can efficiently locate ground states for certain problems \cite{Callison2019} where it is viable to make many short repeats (section \ref{sssec:qw}). Further diabatic methods are reviewed in \cite{Crosson2020}. Many of these, including the quantum approximate optimisation algorithm (QAOA) or quantum alternating operator ansatz \cite{Farhi2014,Hadfield2019} use classical optimisation in the controls to produce an efficient quantum process. We cover this in subsection \ref{sssec:QAOA}. Exploiting the non-adiabatic and open system effects in quantum annealing is the topic of many recent efforts to derive efficient controls \cite{Banks2023,Schulz2023}.
For more details and depth, \citeauthor{Albash2018} \cite{Albash2018} provide a comprehensive introduction to adiabatic quantum computing and quantum annealing. Hardware currently available for quantum annealing includes superconducting systems, trapped ions, and Rydberg atoms (see \cite{Hauke2020} for an overview).


% ================================================================ %
\subsection{Variational quantum algorithms}\label{ssec:VQAs}

\begin{figure}[ht!]
\centering
\includegraphics[width=\linewidth]{vqa}
\caption{High level diagram of hybrid quantum-classical training algorithm for variational circuit. Quantum device (yellow) calculates terms of a cost function, then classical device calculates better circuit parameters $\vec{\theta}$ (blue, green). Cycle is repeated until the desired accuracy is achieved.}
\label{fig:variationalcircuit}
\end{figure}

Variational quantum algorithms (VQAs) distribute the job of solving a problem between a parameterised quantum circuit and conventional classical optimiser, for reviews see, e.g., \cite{Endo2021,Cerezo2021}. Figure \ref{fig:variationalcircuit} shows schematically how it works.  The quantum circuit is defined by a set of parameters $\vec{\theta}$. The measured output of the quantum circuit is used to evaluate a cost function $C(\vec{\theta})$.  Classical optimisation is then used to update the parameters, guided by the cost function. This general framework can be used to solve a wide range of problems, including quantum chemistry and materials science \cite{Kokail2019,Kandala2017} (section \ref{sec:qsim}).

Most VQAs can be grouped into one of two categories: variational quantum optimisation and variational quantum simulation. The optimisation algorithm involves optimising parameters under a static target cost function. For example, when the cost function corresponds to the expectation value of some Hamiltonian, minimising the cost function gives the ground state energy of the Hamiltonian. If the Hamiltonian encodes an optimisation problem, as in section  \ref{ssec:qopt}, the state that minimises the cost function yields the solution to the original problem. If the Hamiltonian corresponds to a quantum system, such as a molecule, the cost function gives the molecule's ground state energy. On the other hand, variational quantum simulation aims to simulate dynamical processes such as the Schr{\"o}dinger time evolution of a quantum state \cite{Yuan2019}. 

The past years have seen rapid developments in the variational approach, and it is clear there are beneficial properties for NISQ-era devices \cite{Cerezo2021}. VQAs can be applied to a wide range of linear optimisation problems and extended to solve nonlinear variational problems \cite{Lubasch2020}. It is possible to use many separate repeats of a shallow (short) quantum circuit. This avoids the need for long coherence times, and the variational parameter tuning compensates for several types of errors. Suitable error mitigation strategies have been developed for NISQ devices \cite{Botelho2022,Ravi2022}. 

There are two important algorithms that make use of the variational principle for optimisation: the quantum approximate optimisation algorithm (QAOA) and the variational quantum eigensolver (VQE). 


%---------------------------------------------------------%
\subsubsection{Quantum approximate optimisation algorithm}\label{sssec:QAOA}\hfill

The quantum approximate optimisation algorithm (QAOA) was originally introduced to approximately solve combinatorial optimisation problems \cite{Farhi2014} on gate-based architecture. This was generalised as the quantum alternating operator ansatz \cite{Hadfield2019}.  Instead of applying the full Hamiltonian $\hat{H} = \hat{H}_M + \hat{H}_P$ continuously in time, the aim is to map some input state to the ground state of a problem Hamiltonian $\hat{H}_P$ by sequentially applying the ansatz
\begin{equation}
U(\gamma,\beta) = \prod_k e^{-i\beta_k\hat{H}_M} e^{-i\gamma_k\hat{H}_P},
\end{equation}
with mixer Hamiltonian $\hat{H}_M$. The ansatz is applied in short discrete steps that approximate the continuous time evolution with parameters $\vec{\theta}=(\vec{\gamma},\vec{\beta})$ to be optimised. In-depth studies of QAOA performance \cite{Zhou2020,Wang2020} developed an efficient parameter-optimisation procedure to find high-quality parameters. It has been shown to converge to quantum annealing in the transverse Ising setting \cite{Brady2021}.  

%----------------------------------------------%
\subsubsection{Variational quantum eigensolvers}\label{sssec:VQE}\hfill

Variational quantum eigensolvers (VQEs) were originally developed to find the ground state energy of molecules \cite{McClean2016,Kandala2017}. They have since been adapted to solve optimisation problems \cite{Cerezo2021}. VQEs are similar to QAOAs, but analyse the Hamiltonian differently. The cost function is defined as $\bra{\psi(\theta)} \hat{H} \ket{\psi(\theta)}$. The aim is to minimise the expectation value of Hamiltonian $\hat{H}$ over a trial state $\ket{\psi(\theta)} = U(\theta) \ket{\psi_0}$ for ansatz $U(\vec{\theta})$ and initial state $\ket{\psi_0}$. 

We can break down the variational methodology into three distinct pieces to approximate the eigenvalues and eigenvectors of the Hamiltonian $\hat{H}$. We outline the method as
\begin{enumerate}
\item Prepare a parameterised initial state as the starting ansatz $\ket{\psi_\text{init}(\vec{\theta})}$.
\item Calculate expectation value $\braket{\hat{H}}(\vec{\theta})$ in the quantum processor (this may require multiple runs using quantum phase estimation).
\item Use a classical optimiser to find a new set of $\vec{\theta}$ values that decreases $\braket{\hat{H}}(\vec{\theta})$.
\item Repeat above procedure to achieve convergence in $\langle\hat{H}\rangle$. Parameters $\vec{\theta}$ at convergence define the desired state.
\end{enumerate}

\citeauthor{Kandala2017} \cite{Kandala2017} discuss the requirements for scaling VQE to larger systems and for bridging the gap between high-performance computing and their implementation on quantum hardware.


%--------------------------------------------%
\subsubsection{Variational quantum simulation}\hfill

Simulating the dynamics of quantum systems is expected to be one of the first useful applications of quantum computers.  Variational approaches make best use of limited quantum hardware by optimising the parameters classically. There are two basic algorithms for variational quantum simulation, specifically for solving real- and imaginary-time evolution of quantum systems. These can be described respectively by the time-dependent Schr{\"o}dinger equation
\begin{equation}
\frac{d\ket{\psi(t)}}{dt} = -i\hat{H} \ket{\psi(t)}
\end{equation}
with Hamiltonian $\hat{H}$ and state $\ket{\psi(t)}$, and normalised Wick-rotated Schr{\"o}dinger equation
\begin{equation}
\frac{d\ket{\psi(\tau)}}{d\tau} = -(\hat{H} - \braket{\hat{H}}) \ket{\psi(\tau)}, 
\end{equation}
where $\tau=-it$ and $\braket{\hat{H}} = \bra{\psi(\tau)} \hat{H} \ket{\psi(\tau)}$ preserve the norm of state $\ket{\psi(\tau)}$.

The direct approach to quantum simulation is to apply a unitary circuit $e^{-i\hat{H}t}$ to the initial state. However this increases the circuit depth polynomially with respect to the evolution time $t$ \cite{low2019hamiltonian} which is difficult to achieve in hardware with short coherence times. On the other hand, variational quantum simulations use an ansatz quantum circuit
\begin{equation}
\ket{\phi(\vec{\theta}(t))} = U(\vec{\theta}(t)) \ket{\phi_0}
\end{equation}
applied to an initial state $\ket{\phi_0}$ to represent $\ket{\psi(t)}$. 
Therefore we must choose the variational parameters to map the time evolution of the Schr{\"o}dinger equation for $\ket{\psi(t)}$ on to the evolution of parameters $\vec{\theta}(t)$. The aim is to obtain a shorter circuit than would be needed for direct simulation. How the parameters evolve and simulate the time evolution depends on the variational principle used \cite{Yuan2019}. 

The algorithm for generalised time evolution can be used for matrix multiplication and solving linear systems of equations \cite{Endo2020}. It is an alternative method to those presented in \cite{BravoPrieto2020,Xu2021v}, or the HHL algorithm (section \ref{sssec:HHL}). 


%------------------------------------------------------------------------%
\subsubsection{Quantum neural networks and parameterised quantum circuits}\hfill

Quantum neural networks (QNNs) combine artificial neural networks \cite{LeCun2015} with quantum principles \cite{Abbas2021}. Classical neural networks are highly nonlinear models whereas QNNs contain operators that always act on linear states. Although there are different variants of QNNs, most follow similar steps \cite{Beer2020}: initialise a neural network architecture; specify a learning task; implement a training algorithm; and simulate the learning task. 

In addition to the fully quantum feed-forward neural networks \cite{Farhi2018}, we can use hybrid models where QNNs are formalised as variational quantum circuits (VQCs) \cite{Schuld2018}, or parameterised quantum circuits (PQCs) \cite{Benedetti2019}. Both offer a concrete way to implement quantum reinforcement learning algorithms in the NISQ era. The approach requires redefining problems as variational optimisation problems then using hybridised quantum-classical devices to find approximate solutions. 

\begin{figure}[ht!]
\centering
\includegraphics[width=\linewidth]{qnn-variational}
\caption{Quantum neural network with variational component.}
\label{fig:qnn_variational}
\end{figure}

For instance, QNNs can be implemented using parameterised gate operations in quantum circuits \cite{Schuld2020} (Figure \ref{fig:qnn_variational}). We encode the input data (usually classical) into a quantum state using the quantum feature map \cite{Schuld2021v} state-preparation routine. The quantum device implements a parameterised circuit $U(\theta)$ that prepares the state $U(\theta)\ket{0} = \ket{\psi(\theta)}$ with a set of classical parameters $\theta$. Then we apply a variational model, which contains parameterised gate operations that are optimised for a particular task, analogous to classical ML techniques \cite{Cong2019}. 

While the approach is straightforward, the state space of a polynomial sized quantum circuit is much smaller than the full Hilbert space.  Hence, the main advantage of variational circuits is the objective function: when $U(\theta)$ cannot be simulated efficiently on classical computers, the overall speedup can be up to exponential. 

In quantum chemistry, PQCs such as the unitary coupled-cluster (UCC) circuits can produce states that approximate the molecular ground states \cite{Lee2019}. UCCs are based on the coupled-cluster method in computational chemistry and are too deep to be easily implemented on today's quantum computers. Instead the \textit{hardware-efficient ansatz} is a shallower PQC composed of gates that are natural operations on the quantum hardware \cite{Kandala2017}. 

The dissipative QNN is analogous to the classical feedforward neural network \cite{Schuld2014,Beer2020}. The QNN is ``dissipative'' because qubits in a layer are discarded after the information forward-propagates to the (new) qubits in the next layer. Many QNNs have been proposed for different types of data analysis, such as quantum generative adversarial networks (QGANs) \cite{Lloyd2018}, continuous variable QNN \cite{Killoran2019}, QNN classification \cite{Farhi2018}, and recurrent QNNs \cite{Bausch2020}. In addition, quantum Boltzmann machines \cite{Amin2018,Kieferova2017} are a natural fit for quantum annealing \cite{Benedetti2016,Xu2021} and quantum computing \cite{Wiebe2016}, with the latter showing polynomial speed-ups over classical deep learning. Constructing QNN architectures is currently an active area of research. 


%---------------------------%
\subsubsection{Limitations of VQAs}\hfill

Variational algorithms have a range of problems (see \cite{Cerezo2021,Cerezo2022} for an overview). Here we note two common problems.

\begin{figure}[ht!]
\centering
\includegraphics[width=\linewidth]{vqa-landscape}
\caption{Ideal solution landscape (blue) corrupted by (a) noise and (b) barren plateau, both orange.}
\label{fig:vqa-landscape}
\end{figure}

Hardware noise is a defining characteristic of NISQ computing. Noise affects all aspects of a quantum model, corrupting information as it forward-propagates in a quantum circuit. It particularly affects deeper circuits with longer runtimes. Noise can hinder the trainability of hybrid algorithms as it leads to noise-induced barren plateaus \cite{Wang2021n}, ultimately deforming the landscape features and shifting the minima positions (Figure \ref{fig:vqa-landscape}(a)). Suggested antidotes include partial quantum error correction \cite{Bultrini2022}, building relatively shallow circuits (depth grows sublinearly in the problem size) \cite{Wang2021n}, and error mitigation techniques to partially alleviate noise-induced trainability issues \cite{Wang2021m,Endo2021}. Alternatively we can create models with noise-resilient properties \cite{Fontana2021} that do not alter the minima positions.

Quantum machine learning and VQAs contain expensive classical optimisation loops. They minimise a cost function by navigating a typically non-convex loss function landscape to find its global minimum. The barren plateau phenomenon occurs when the optimisation landscape becomes exponentially flat (on average) as qubit number increases \cite{McClean2018,Cerezo2021}. At the same time, the valley containing the global minimum also shrinks exponentially with problem size, leading to a narrow gorge \cite{Arrasmith2022} (Figure \ref{fig:vqa-landscape}(b)). The cost function gradient is exponentially suppressed except in an exponentially small region. Hence finding the minimum requires computational resources that scale exponentially with the number of qubits used, which can potentially destroy any quantum speedup.


%========================================================%
\subsection{Performance benchmarks: quality, speed, scale}\label{ssec:benchmark}

Benchmarking for high performance computers is well established, and essential for managing HPC facilities and optimising their throughput. For example, the High Performance Linpack (HPL) benchmark \cite{Petitet2018} solves a dense linear system to measure the throughput of a computing system. Running the same computational tasks on different HPC hardware can provide some level of side-by-side comparison of different facilities. However, the results from standard benchmarks cannot fully predict performance for a specific application, and bespoke tests are often run to tune performance on a per-application basis.

Benchmarks for quantum computers are under active development. Given the diverse types of hardware available, fair comparisons across hardware types are even more challenging. There are five common physical qubit realisations for building quantum processors. Hardware based on superconducting qubits \cite{Krantz2019,Blais2021} is arguably the most popular choice \cite{Kjaergaard2020,Bravyi2022,Blais2020}, followed by ion-trap-based systems \cite{Bruzewicz2019,Malinowski2023,Moses2023}. Semiconductor-based spin qubits \cite{Chatterjee2021,Burkhard2023} are less advanced but potentially offer more scalability in future. Photonic platforms \cite{Pelucci2022,Moody2022} require less stringent cooling regimes but other aspects of the engineering are challenging. Neutral atoms, especially using highly excited Rydberg states \cite{Adams2020}, are versatile and already showing impressive results for directly simulating quantum many-body systems \cite{Surace2020,Ebadi2021}.

The criteria for a good quantum computer are fairly straightforward \cite{DiVincenzo2000}. It should have as many high quality qubits as possible. Qubits can be individually controlled to generate complex (entangled) states. They can then have a large number of sequential operations applied before losing coherence. The control and gate operations are characterised by the quality of single- and two-qubit gates. Scaling up to obtain good performance on large numbers of qubits requires error correction \cite{Terhal2015} to be implemented by the hardware. This needs mid-circuit measurements to be possible, as well as measuring the qubit states at the end of the computation.


\subsubsection{Low- and high-level benchmarks}\label{sec:lowhighbenchmarks}\hfill

Low-level benchmarks measure the performance of individual qubits, single- and two-qubit gate fidelities, state preparation and measurement operations. We can often directly measure the $T_{1,2}$ decoherence times, gate speed or gate operation reliability (gate fidelity). However these characteristics do not provide an accurate benchmark of quantum processor performance. Using low-level benchmarks alone can label a slow high-fidelity quantum computer as equivalent to a fast low-fidelity quantum computer \cite{Campbell2017a}. 

For more thorough testing of gate operations, there is randomised benchmarking \cite{Knill2008,Magesan2011}, cycle benchmarking \cite{Erhard2019a}, or gate set tomography \cite{Nielsen2021}. Randomised benchmarking protocols typically characterise the quality of Clifford operations which are classically simulable. For non-Clifford operations, random circuit sampling and direct randomised benchmarking can be used \cite{Liu2022b,Proctor2019}. These methods may not be scalable to large numbers of qubits as it is difficult to classically simulate non-Clifford operations to check the test outcomes. A class of methods that aims to circumvent this issue is mirror benchmarking \cite{Proctor2021, Mayer2021}. The trivial ideal output of the circuits involved in these benchmarks makes the results easily verifiable and scalable to large numbers of qubits.

At a slightly higher level, specific quantum circuits can be used to test hardware performance for gate sequences. The circuits can be random, application-agnostic circuits \cite{Cross2019,Proctor2022}, or circuits run as part of certain algorithms \cite{Mills2021,Quetschlich2023,Li2022b}. For non-gate-based hardware, a generalised form of randomised benchmarking can be used, for example in programmable analogue quantum simulators \cite{Derbyshire2020,Jackson2023}. \citeauthor{McGeoch2019} \cite{McGeoch2019} proposes guidelines for reporting and analysing quantum annealer performance. 

Algorithm- \cite{Georgopoulos2021} or application-level \cite{Lubinski2023,Donkers2022} benchmarks require knowing the ideal output in order to evaluate the solution quality. For running entire algorithms, there are higher-level benchmarks for how well a quantum computer performs. For example, the one-dimensional Fermi-Hubbard model is exactly solvable using the Bethe ansatz \cite{DallaireDemers2020}. There are benchmarks comprising a suite of many different applications (e.g. \cite{Cornelissen2021,Tomesh2022,Murali2019}). Most include well-known algorithms such as the quantum Fourier transform (section \ref{sssec:QFT}), quantum phase estimation (section \ref{sssec:QPE}), Grover's search algorithm (section \ref{sssec:search}), VQE (section \ref{sssec:VQE}), the Bernstein-Vazirani algorithm \cite{Bernstein1993}, and the hidden shift algorithm \cite{vanDam2006}. In general, device performance on simple problems or circuits may not scale \cite{Lubinski2023}. It is also necessary to define suitable high-level success criteria, eg. for quality \cite{BlumeKohout2020,Mills2021}. 

For benchmarking at the algorithmic level, qubit layout and connectivity affect algorithm efficiency. Some types of hardware (e.g., neutral atoms) can easily be reconfigured between runs. Other platforms, such as superconducting or semiconductor qubits have connectivity baked in at the manufacturing stage. 

It is useful to test at both low and high levels because extrapolating from low level performance to higher level is unreliable. Cross-talk errors can dominate and these are not captured by lower level tests using only one- or two- qubit gates.  Even at a higher level, there may be large differences between performance on random and structured circuits \cite{Lubinski2023,Proctor2022}. 


\subsubsection{Quantifying quantum device performance}\label{sec:quantifyperformance}\hfill

Quantum volume, a benchmark developed by IBM \cite{Cross2019}, allows us to test hardware performance for gate sequences. It can be generalised to more realistic non-square circuits \cite{BlumeKohout2020,Miller2022}. Quantum volume is calculated by running randomised square circuits with different numbers of qubits. The quantum volume is given by the largest square circuit that can be run on the device which passes an acceptance criteria that quantifies the output quality. 

In addition, we want to measure how quickly the quantum processor can execute layers of a parameterised model circuit similar to those used to measure quantum volume. Increased quantum processor speed is critical to support near-term variational algorithms, which require thousands of iterations. Some benchmarks evaluate speed as a sub-score and combine it with other factors to determine a final metric \cite{Donkers2022}. There also exist standalone metrics for speed \cite{Wack2021}. \citeauthor{Lubinski2023} \cite{Lubinski2023} note that evaluating measures of speed for comparing different platforms is difficult. Timing information (when available at all) is reported differently by different hardware vendors, and gate speeds naturally vary by several orders of magnitude between hardware types. Another time-related factor is consistency, or how the performance varies over time. This determines how often hardware must be recalibrated. Consistency can be captured by repeating the same tests, and is an important consideration for current hardware which requires significant downtime for regular recalibration.

Metrics such as Circuit Layer Operations Per Second (CLOPS) \cite{Wack2021}, layer fidelity \cite{McKay2023}, and qubit number have been proposed to collectively quantify the quality, speed, and scale of quantum computers. Single-number metrics are useful to make cross-device comparisons straightforward but may not accurately describe quantum computer performance for all kinds of algorithms.  Other types of gate sequences can be used including identity operations and entanglement generation circuits \cite{Michielsen2017}.

To summarise, the above methods (section \ref{sec:lowhighbenchmarks}) and metrics (section \ref{sec:quantifyperformance}) are useful for benchmarking hardware dependent characteristics.  On top of this, the compilers and software stack efficiency can significantly affect performance, similar to classical HPC.  Quantum software benchmarking is still at an early stage of development.


%-------------------------------------%
\subsubsection{Tensor networks}\label{sec:TNbenchmark}\hfill

Being able to simulate quantum circuits is important for demonstrating quantum advantage. It establishes a classical computational bar that quantum computers must pass to demonstrate advantage. It also verifies that the quantum hardware performs as expected up to the limits of classical computational capabilities. Tensor networks offer an intuitive graphical language to describe quantum circuits \cite{Orus2019t}. Approximating a quantum state using tensor networks is equivalent to applying lossy data compression that preserves the most important properties of the quantum state (see e.g. \cite{Orus2014,Huggins2019}). 

One way to benchmark NISQ devices involves sampling the bit-strings from a random quantum circuit (RQC) \cite{Boixo2018,Villalonga2019}. The result must be within some variational distance of the output distribution defined by the circuit \cite{Bouland2019}. This is a classically intractable problem. There are two main goals on the road towards demonstrating quantum advantage: RQCs should be as difficult as possible to simulate, and classical simulators should be as efficient as possible so that reports of quantum advantage are not exaggerated. 

The tensor network contraction method has been successful in classically simulating RQCs for sizes close to the quantum advantage regime \cite{Wahl2023,Huang2021} and low-depth noisy quantum circuits \cite{Zhou2020x,Orus2019t,Ran2021}. Tensor network states mathematically represent quantum many-body states based on their entanglement structure. Because RQCs (like in Google's work \cite{Arute2019}) contain noisy quantum gates that create a small amount of entanglement, applying tensor-network algorithms is adequate for their simulation \cite{Zhou2020x}. Contracting a whole tensor network involves a contraction tree which describes a series of pairwise merges that turn the network into a single tensor. It is analogous to performing a series of sums and multiplications of the tensor entries to obtain an expectation value. Examples of tensor network-based benchmarking methods include the holoQUADS algorithm \cite{FossFeig2021,FossFeig2022} and qFlex \cite{Villalonga2019}.

%-----------------------------------------%
\subsubsection{Measuring quantum advantage}\hfill

It is not straightforward to determine exactly when a quantum computer provides an advantage over a classical computer. The latter are benchmarked using FLOPS, which measure the number of floating-point operations per second. The equivalent speed benchmark for quantum processors is CLOPS which measures the number of circuit layer operations per second. It does not make sense to convert between the two metrics, since they do not correspond to the same unit of computation. 

At the algorithm level, many quantum machine learning (QML) algorithms make different assumptions from the classical algorithms they are compared with \cite{Aaronson2015}. In some cases, it is possible to find ``dequantised'' versions of QML algorithms \cite{Tang2022}. These are fully classical algorithms that process classical data and perform only polynomially slower than their quantum counterparts. If a dequantised algorithm exists, then its quantum counterpart cannot give an exponential speedup on classical data.  On the other hand, dequantisation shows how replacing quantum linear algebra algorithms with classical sampling techniques can potentially create a classical algorithm that runs exponentially faster than any other known classical algorithm \cite{Tang2019}. Quantum-inspired classical algorithms that provide a significant improvement on current best methods are a useful benefit from quantum algorithm research, if they turn out to be practical to implement on classical HPC. 
 
Many QML algorithms rely on pre-processed data via quantum encoding. Breakthroughs that claim a quantum speed-up often neglect the quantum encoding step in the algorithm runtime, while making comparisons to classical algorithms that operate on raw, unprocessed data. This means it is often possible to achieve equivalent speed-ups using only classical resources, if we perform the same pre-processing for classical algorithms. For example, the pre-processing in \cite{Tang2019} allows us to apply ``dequantised'' classical sampling techniques to the data, and achieve a significant speedup over previous classical methods.  We discuss the issues around encoding classical data into quantum systems in more depth in section \ref{ssec:encoding}.

Comparing state-of-the-art quantum computers with the best classical algorithm for the same problem is not enough to establish a quantum advantage.  For the RQC used in Google's claim of quantum supremacy \cite{Arute2019}, classical algorithms had not been optimised for the problem: it is not a useful problem and there was little research on it.  Subsequent work brought down the classical runtime by some orders of magnitude.  Similar advances have occurred with boson sampling \cite{Zhong2020,Bulmer2022,Oh2023}.

Current research predicts a narrow window in terms of the number of qubits where circuit model NISQ hardware can potentially outperform classical hardware for certain problems \cite{Bouland2021}.  Larger numbers of qubits need longer gate sequences for the same algorithm. Errors inevitably build up and render the computation unreliable without active error correction. Recent proof-of-concept studies of quantum error correction \cite{RyanAnderson2021,Sivak2023} begin to show errors suppressed for extended gate sequences, heralding a new era of fault tolerant quantum computing.


% ===========================================================
% ===========================================================

\section{Quantum simulation of quantum systems}\label{sec:qsim}

Simulating quantum systems on classical computers has long been known for the exponential explosion problem, where the size of the quantum space increases exponentially with the number of qubits. This may be overcome using quantum simulation \cite{Feynman1982}: we use some controllable quantum system to study another less controllable or accessible system. The method is especially relevant for studying many-body problems \cite{Weimer2021} in condensed matter physics, high-energy physics, atomic physics, quantum chemistry, and cosmology. For authoritative reviews, see \cite{Georgescu2014,Altman2021,Daley2022}.

\begin{table*}[ht!]
\centering
\caption{Comparisons of digital vs analogue quantum simulation.}
\bgroup
\def\arraystretch{1.5}
\begin{tabular}{p{.8in}p{.1in}p{2.4in}p{.1in}p{2.4in}}
&&\textbf{Analogue} && \textbf{Digital} \\
\hline
Hamiltonian
 && 
Directly implement system Hamiltonian by experimentally building a controllable quantum system (simulator).
 &&  
Express Hamiltonian in second quantised form, then construct Hamiltonian out of quantum logic gates. 
 \\
System vs simulator
 && 
System may be completely different from simulator, e.g., using atoms in optical lattices to simulate electrons in a crystal.
 &&  
Implements circuits containing quantum gates, therefore has same level of technical difficulty as quantum computer.
 \\
Physical model
 && 
Needs to match the simulator Hamiltonian within the available experimental controls.
 && 
In principle, any quantum system can be simulated using digital (universal) quantum simulation.
 \\
Hardware
 && 
Easier to implement for certain problems well matched to the simulator hardware. Likely to become practicable before universal quantum computers become available.
 &&  
Requires high depth circuits (many quantum gates) therefore unlikely to be practical before fully error corrected quantum computers are available.
 \\
 \hline
\end{tabular}
\egroup
\label{tab:quantumsim}
\end{table*}

Quantum simulation can be implemented using quantum computers (digital quantum simulation) or with analogue devices (analogue quantum simulation) that require less control and are easier to construct. In the latter, ultracold atomic systems are often used to simulate condensed matter physics \cite{Trotzky2012,Choi2016} and quantum chemistry \cite{ArguelloLuengo2019} problems. We summarise the relative merits of the different platforms in Table \ref{tab:quantumsim}.  In the next subsections, we describe the simulation techniques and applications in more detail.

%--------------------------------%
\subsection{Hamiltonian evolution}

The basic problem in most quantum simulations requires time evolution under the system Hamiltonian. For Hamiltonian $\hat{H}$, the unitary operator $U(t)$ evolves the dynamics for a time $t$,
\begin{equation}
U(t) = \exp \left( -i \int_0^t \hat{H}(t) dt \right), 
\end{equation}
using units where $\hbar=1$.
For many physical systems, the system Hamiltonian $\hat{H}$ can be written as a sum of terms that describe local interactions,
\begin{equation}
\hat{H} = \sum_{j=1}^n \hat{H}_j.
\end{equation}
Implementing $U(t)$ on a digital quantum computer usually relies on numerical integration, often called Trotterisation \cite{Hatano2005}, i.e., breaking up the evolution time into small steps of duration $\Delta t$. The first-order Trotter-Suzuki formula gives  
\begin{equation}\label{eq:STformula}
U(\Delta t) 
= e^{-i \sum_j \hat{H}_j \Delta t}
= \prod_j e^{-i\hat{H}_j\Delta t} + \mathcal{O}((\Delta t)^2).
\end{equation}
Here, acceptable precision comes at the expense of small $\Delta t$, which requires many quantum gates, and hence extremely deep circuits, well beyond NISQ capabilities for useful problem sizes.

In analogue quantum simulation, the system Hamiltonian $\hat{H}_\text{sys}$ is directly mapped \cite{Somaroo1999} on to the simulator Hamiltonian $\hat{H}_\text{sys} \leftrightarrow \hat{H}_\text{sim}$.

\begin{figure}[ht!]
\centering
\includegraphics[width=\linewidth]{digital-simulator}
\caption{Conceptual diagram of universal simulator on digital quantum computing device \cite{Tacchino2020}.}
\label{fig:universalsimulator}
\end{figure}

Figure \ref{fig:universalsimulator} gives a conceptual illustration of how to use Trotterisation to perform quantum simulation on a quantum computer. A physical model describes the quantum state evolution in a physical space, $\Psi(t)$; this evolution can be approximated to arbitrary precision by mapping the given model onto a spin-type Hamiltonian. This can be easily encoded onto a qubit-based register. We use the Suzuki-Trotter formula (equation \eqref{eq:STformula}) to perform the time evolution. The sequence of unitary operations can be programmed through a quantum circuit and directly run on a quantum computer, giving the approximated evolved state as output $\psi(t)$ starting from a given input state $\ket{\psi(0)}$.

%---------------------------------%
\subsection{Quantum algorithms for Monte Carlo}\label{ssec:QMC}

Monte Carlo methods have a long scientific history with some of the earliest known applications in the Los Alamos project \cite{Eckhardt1987}. Monte Carlo simulation uses random sampling and statistical modelling to estimate mathematical functions and mimic the operations of complex systems. All Monte Carlo methods follow the same framework: model a system as a series of probability density functions, repeatedly sample from the functions, then calculate the statistics of interest. 
The method requires large numbers of samples to achieve reasonable accuracy, often on the order of millions of samples which can take hours or even days. When applied to quantum systems (quantum Monte Carlo), the classical computation requires less memory than a full quantum simulation, trading memory for the time needed to perform the samples.

Quantum algorithms that can speed up Monte Carlo methods have been discussed since the early 2000s. If sampling is done using a Markov chain, i.e., a random walk, quantum versions of random walks \cite{VenegasAndraca2012} can generally provide a quadratic improvement in the sampling rate \cite{Szegedy2004}.  This faster mixing of quantum processes is at the core of many polynomial quantum speed ups, and can be applied to a wide range of classical sampling problems \cite{Montanaro2015}.

When the problem involves simulating a quantum system, further advantage can be gained with a fully quantum algorithm.  For example, the time-evolution operator can be expressed in the form of a summation, where each term in the sum corresponds to a deterministic quantum-circuit configuration \cite{Berry2015,Childs2012} or by sampling random circuits \cite{Campbell2019r}. 

Methods for quantum systems involve calculating the transition amplitude $\bra{\psi_f} e^{i\hat{H}t^*} \hat{O} e^{-i\hat{H}t} \ket{\psi_i}$ with initial state $\ket{\psi_i}$, final state $\ket{\psi_f}$, operator $\hat{O}$, Hamiltonian $\hat{H}$.  This is used to estimate the expectation value of the operator $\hat{O}$.
Both real ($t$) or imaginary ($it$) time evolution can be used.  Using imaginary time involves decaying rather than oscillating functions, which can converge faster. In the following example using imaginary time, the ground-state energy of an interacting Hamiltonian is
\begin{equation}
\lim_{t\to\infty} \frac{\bra{\psi_0} e^{\hat{H}t/2} \hat{H} e^{-\hat{H}t/2} \ket{\psi_0}}{\bra{\psi_0} e^{\hat{H}t} \ket{\psi_0}}
\end{equation}
where the trial ground state $\ket{\psi_0}$ overlaps with the true ground state. 

The many diverse applications of Monte Carlo to quantum systems provide corresponding opportunities for more efficient computation using quantum computers (for reviews, see for example \cite{Ceperley1986,Foulkes2001}).


%==========================================%
\subsection{Quantum computational chemistry}

One of the original motivations for developing quantum computers was to be able to accurately simulate and characterise systems of interacting fermions \cite{Abrams1997,ortiz2001quantum}. 
Obtaining accurate solutions of the electronic structure of many-body systems is a major challenge in computational chemistry. Research has been carried out to develop algorithms to solve the electronic structure problem using quantum computers (e.g. \cite{Lin2022,Huggins2022,Somma2019}). Fault-tolerant quantum computers are predicted to solve the electronic structure problem for many-body systems \cite{Cao2019} in polynomial time, for example, by using the quantum phase estimation algorithm \cite{AspuruGuzik2005,Abrams1997,Kitaev1996} (see Section \ref{sssec:QPE}). It is possible that a quantum advantage exists even in the absence of a clear polynomial scaling. A detailed overview of the state-of-the-art methods developed in this field can be found in \cite{Cao2019} and \cite{McArdle2020}.

In practice, the possibility of achieving exponential quantum advantage for quantum chemistry problems remains controversial \cite{Lee2023c}.  It is challenging for quantum simulations to achieve quantum advantage over the best classical computations, given the decades of work already done on classical methods for quantum systems. As classical algorithms continue to improve, quantum information processing has been useful in developing advanced ``quantum-inspired'' classical techniques \cite{Arrazola2019}.  

In this regard, we review new techniques for both classical and quantum hardware from different perspectives. We cover a range of methods applied to problems such as \textit{ab initio} simulations, Hubbard models, spin Hamiltonians, interacting Fermi liquids, solid-state systems, and molecular systems. 

Modern computational chemistry aims to simulate molecules and materials within 1 kcal/mol accuracy, which is required to predict chemical phenomena at room temperature. Despite the tremendous progress in quantum chemistry algorithms over the past century, the precision of 1 kcal/mol can only be achieved for small systems due to the polynomial scaling (higher than O($x^{4}$)) of advanced computational chemistry methods \cite{Grimme2018}. Hence, current research has a strong emphasis on developing hybrid quantum-classical and quantum algorithms; estimating quantum resources; and designing quantum hardware architectures for simulations of practical interest. 

Even with noisy hardware, the long-term impact of using quantum simulation for quantum chemistry and materials is promising, given the recent efforts using quantum Monte Carlo methods (see Section \ref{ssec:QMC}). For example, Google's Sycamore quantum processor used a hybrid quantum-classical Monte Carlo method \cite{Arute2019} to calculate the atomisation energy of the strongly correlated square H$_{\text{4}}$ molecule. The results are competitive with those of state-of-the-art classical methods. The algorithm relies on being able to efficiently prepare a good initial guess of the ground state on the quantum hardware.  This is a non-trivial step.

Understanding the strengths and weaknesses of classical algorithms can indicate where we may see a quantum advantage. It also indicates where issues may arise in quantum simulation approaches in the near and long term. For example, \citeauthor{Tubman2018} \cite{Tubman2018} used classical simulations to provide resource estimates for crucially important but often neglected aspects of quantum chemistry simulations. These include creating the initial wave function used in quantum methods such as phase estimation, and efficient state preparation. 

One promising research direction is to improve existing quantum methods for estimating electronic ground-state energies. Two of the most popular methods, quantum phase estimation (QPE) \cite{AspuruGuzik2005} (Section \ref{sssec:QPE}) and VQEs \cite{Kandala2017} (Section \ref{ssec:VQAs}), require significant quantum computing resources \cite{Gonthier2022}. This motivates research to find less intensive alternatives. For example, while VQEs do not require quantum error correction protocols, it is challenging to choose a suitable ansatz \cite{Tang2021a}, optimise potentially thousands of parameters \cite{McClean2018}, and have suitable noise resilience protocols \cite{Fontana2021,Wang2021n}.

Phase estimation is useful for accurately simulating long-time dynamics and determining eigenstate energies \cite{Kitaev1996}, but requires a large number of qubits and quantum gates \cite{Wecker2014,Babbush2018}. It has been experimentally implemented for small molecules on a variety of quantum computing architectures, including superconducting qubits \cite{OMalley2016}. However, larger molecules of practical interest require a gate depth several orders of magnitude beyond what is currently feasible \cite{Reiher2017}. Therefore, chemical researchers are keen to improve techniques for these types of simulations, including considerable efforts to estimate the resources required to perform QPE simulations \cite{Babbush2018}.

%-------------------------------------------------------%
\subsubsection{Quantum chemistry's many-electron problem}\hfill

The core objective of quantum chemistry is to solve the stationary Schr{\"o}dinger equation for an interacting electronic system in the static external field of the nuclei, which describes isolated molecules in the Born-Oppenheimer approximation. The problem can be defined as follows. There are $N$ electrons in the field of stationary $M$ point charges (nuclei). Electrons interact only through electrostatic Coulomb terms and are collectively described by a wave function with $4N$ coordinates $\mathbf{x}_i = (\mathbf{r}_i, \omega_i)$, i.e., $3N$ spatial coordinates $\mathbf{r}_i$ and $N$ spin coordinates $\omega_i$. The antisymmetric electronic wave functions are  solutions to the time-independent Schr{\"o}dinger equation
\begin{gather}
\hat{H} \Psi(\mathbf{r}) = E \Psi(\mathbf{r})
\end{gather}
where we describe the motion of the $N$ electrons ($i$, $j$ indices) in the field of the $M$ nuclei ($k$ index) with Hamiltonian
\begin{gather}
\hat{H} = 
\underbrace{- \sum_i^N \frac{\nabla_i^2}{2}}_{=\hat{H}_K} 
+ \underbrace{\sum_i^N \sum_{j>i}^N \frac{1}{\vert\mathbf{r}_i-\mathbf{r}_j\vert}}_{=\hat{H}_{e-e}}
+ \underbrace{\sum_i^N v(\mathbf{r}_i)}_{=\hat{H}_{e-n}} \\
v(\mathbf{r}_i) = \sum_k^M \frac{z_k}{\vert\mathbf{r}_i-\mathbf{r}_k\vert}
\label{schroedinger_equation}
\end{gather}
in atomic units $z$. The Hamiltonian contains the kinetic term $\hat{H}_K$, the electron-electron interaction $\hat{H}_{e-e}$, and the electron-nuclei interaction $\hat{H}_{e-n}$. When there are two or more electrons, we cannot solve the Schr{\"o}dinger equation exactly. This is the fundamental challenge of computational chemistry.

There are two main families of approximations to solve this problem: \textit{Hartree-Fock} (HF) \cite{slater1951} methods with refinements and \textit{density functional theory} (DFT) \cite{Hohenberg1964,Kohn1965}.  Each has their own opportunities for quantum enhancements.  We summarise the main methods and their limitations before discussing potential quantum enhancements.

%------------------------------------------------------%
\subsubsection{HF and the electron correlation problem}\hfill

The Hartree-Fock (mean-field) method \cite{Echenique2007} describes the potential experienced by each electron resulting from the average field due to all electrons. This means that the $\hat{H}_{e-e}$ term in equation (\ref{schroedinger_equation}) is replaced by a mean-field potential to make the equation solvable. However, this approximation does not correctly describe the 
electron-electron correlation.  Electron correlation accounts for the fact that electrons do not move independently in the average field of other electrons, but rather their movements are influenced by the instantaneous positions and motions of other electrons. We consider an exception to the latter: the particle exchange energy resulting from the anti-symmetry of the wave function (i.e., Pauli principle),
\begin{equation}
\Psi(\mathbf{x}_1, ..., \mathbf{x}_i, \mathbf{x}_j, ..., \mathbf{x}_N) = 
-\Psi(\mathbf{x}_1, ..., \mathbf{x}_j, \mathbf{x}_i, ..., \mathbf{x}_N).
\end{equation}
This means that we cannot express the wave function as a separable product of single-particle wave functions,
\begin{equation}
\Psi(\mathbf{x}_1, \mathbf{x}_2, ..., \mathbf{x}_N) \neq \psi_1(\mathbf{x}_1) \psi_2(\mathbf{x}_2) ... \psi_N(\mathbf{x}_N)
\end{equation}
where the spatial and spin parts are defined as $\psi_i(\mathbf{x}_i) = \phi_i(\mathbf{r}_i) \chi_i(\omega_i)$. Instead, the HF method approximates the many-body wave function with a Slater determinant
\begin{multline}
\Psi(\mathbf{x}_1, \mathbf{x}_2, ..., \mathbf{x}_N) =\\ 
\frac{1}{\sqrt{N!}}
\begin{vmatrix}
\psi_1(\mathbf{x}_1) & \psi_2(\mathbf{x}_1) & ... & \psi_N(\mathbf{x}_1) \\
\psi_1(\mathbf{x}_2) & \psi_2(\mathbf{x}_2) & ... & \psi_N(\mathbf{x}_2) \\
\vdots & \vdots & \ddots & \vdots \\
\psi_1(\mathbf{x}_N) & \psi_2(\mathbf{x}_N) & ... & \psi_N(\mathbf{x}_N)
\end{vmatrix},
\label{slater_determinant}
\end{multline}
where the set of $\{\psi_i(\mathbf{x}_j)\}$ are chosen to minimise the ground state Hartree-Fock energy 
\begin{equation}
E_0 = \bra{\Psi_0} \hat{H} \ket{\Psi_0}.
\end{equation}
The wave function depends on the Hamiltonian, and, in turn, the mean-field part of the Hamiltonian depends on the wave function. This means that the HF method must be solved iteratively using the self-consistent field (SCF) method. We start from an initial guess for the wave function, build an initial Hamiltonian, and then calculate the initial energy. We change the wave function, update the Hamiltonian, and calculate the energy until the difference between cycles is lower than a defined threshold. The SCF method relies on the variational principle, which states that the energy of any approximate wave function is higher than or equal to the exact energy. 
The final energy is the exact HF energy $E_0$. This value is always higher than the true ground-state energy because the formalism cannot fully account for electron correlation effects. Recovering the correlation energy is the main goal of post-HF methods.

For small systems, such as atoms, the unknown $\psi$ functions in equation (\ref{slater_determinant}) can be solved numerically on a three-dimensional grid of points. Larger systems require grids that are too dense to reach convergence. Hence we express the $\psi$ functions as a linear combination of a set of known functions, the basis set:
\begin{equation}
f(x) = \sum_{i=1}^{M}c_{i}\chi_{i}(x).
\end{equation}
In practice, the basis set could be built from any type of function $\chi_{i}$ following two criteria: they should reproduce the behaviour of the electron density (the highest density should be located at the atom positions, and they should monotonically decrease to zero at an infinite distance from the nuclei) and they should be easy to integrate. Among the most widely used basis sets, there are Gaussian and Slater-type basis sets for molecules \cite{basisset} and plane waves for periodic systems \cite{Singh2006}. 

Selecting the appropriate basis set is a critical consideration. It is a balance between achieving the desired accuracy and managing computational resources. Classical computer simulations often employ extensive basis sets, which can provide high levels of accuracy, including up to 30 functions per atom \cite{Peterson2007,Jensen2016}. The computational capacity of classical computers typically means that the basis set size is not the primary limitation on accuracy. On the contrary, NISQ devices impose limitations on the basis set size due to hardware constraints \cite{Chien2022}, making the basis set size a more significant factor affecting accuracy as McArdle \textit{et al.} discuss in this comprehensive review \cite{McArdle2020}.


%------------------------------------------------------%
\subsubsection{Density functional theory (DFT)}\hfill

Density functional theory (DFT) \cite{Capelle2006} focuses on the electronic density 
\begin{gather}
n(\mathbf{r}) = N \int d^3 r_2 \int d^3 r_3 ... \int d^3 r_N \vert\Psi(\mathbf{r}, \mathbf{r}_2, ..., \mathbf{r}_N)\vert^2
\end{gather}
rather than electron coordinates. DFT systematically maps the many-body problem to a single-body problem through the particle density $n(\mathbf{r})$ from which we calculate all other observables.

The Hohenberg-Kohn (HK) theorem lies at the heart of DFT. It states that the system's ground-state energy is uniquely determined by the ground-state density $n_0(\mathbf{r})$. The ground-state wave function $\Psi_0(\mathbf{r}_1, \mathbf{r}_2, ..., \mathbf{r}_N)$ must both reproduce the ground-state density $n_0(\mathbf{r})$ and minimise the energy. Hence the aim is to minimise the sum of kinetic, interaction and potential energies
\begin{align}\label{HK}
&E_0[n(\mathbf{r})] \nonumber\\
&= \min_{\Psi \to n} \bra{\Psi} \hat{H}_K + \hat{H}_{e-e} + \hat{H}_{e-n} \ket{\Psi} \\
&= \underbrace{\min_{\Psi \to n} \bra{\Psi} \hat{H}_K + \hat{H}_{e-e} \ket{\Psi}}_{=: F[n]} + \int d^3 r n(\mathbf{r}) v(\mathbf{r})
\end{align}
with Hohenberg-Kohn density functional $F[n]$. We then minimise $E_0$ with a variational method using Lagrange multipliers to obtain the Kohn-Sham equations \cite{Baseden2014},
\begin{equation}
-\frac{\nabla^2}{2} \psi_i(\mathbf{r}) + v_{\text{eff}}(\mathbf{r})\psi_i(\mathbf{r}) = \epsilon_i \psi_i(\mathbf{r}).
\end{equation}

The effective potential $v_{\text{eff}}(\mathbf{r})$ is defined as the sum of the external potential (the interaction with the nuclei) $v_{\text{ext}}(\mathbf{r})$, the Coulomb interaction among the electrons $v_{\text{H}}(\mathbf{r})$, and the exchange-correlation potential $v_{\text{xc}}(\mathbf{r})$:
\begin{equation}
v_{\text{eff}}(\mathbf{r}) = v_{\text{ext}}(\mathbf{r}) + v_{\text{H}}(\mathbf{r}) + v_{\text{xc}}(\mathbf{r}).
\end{equation}
The exchange-correlation potential takes into account non-classical effects such as the Pauli principle and the electron correlation. This functional, which must be parameterised, determines the quality of the result.

DFT is system-specific and does not reach the desired chemical accuracy. Similarly to the Hartree-Fock method, it does not take into account direct electron correlation, although this can be compensated for by the functional. Additionally, this method has the disadvantage of self-interaction, which occurs when each electron interacts with the electron density generated by all the electrons, including itself. This interaction is exactly cancelled out in the Hartree-Fock approach.

Because DFT is based on the three-dimensional distribution of the electron density, it is possible to directly calculate it on a three-dimensional grid around the atoms and bonds that represent the electron density at different points in space. However, in practical implementations, DFT frequently uses auxiliary functions to discretise the electron density on the grid. This approach ensures precise numerical integration, computational efficiency, management of exchange-correlation functionals, and allows us to apply linear scaling methods for larger systems. Consequently, DFT uses basis set functions similar to those in HF theory.

%------------------------------------------------------%
\subsubsection{Post Hartree-Fock methods}\hfill

Since neither HF nor DFT directly take into account the electron-electron correlations, the repulsive force between electrons needs to be added \textit{a posteriori}. Post-Hartree-Fock (post-HF) methods are more accurate than HF, but at greater computational cost. For example, the full configuration interaction (FCI) method considers all possible electron configurations (excitations) within the chosen basis set and gives the exact solution of the Schr{\"o}dinger equation within a basis set expansion \cite{Eriksen2021}. 
The number of all possible occupancy combinations scales as $O(N^M)$ with $N$ electrons and $M$ basis functions. It can only be performed on classical computers for small systems. Another example is M{\o}ller-Plesset perturbation theory \cite{Moller1934} which takes the core terms $\hat{H}_K+\hat{H}_{e-n}$ as an unperturbed Hamiltonian and treats the electron repulsion terms $\hat{H}_{e-e}$ as a perturbation. Applying the Rayleigh-Schr{\"o}dinger perturbation theory gives an approximate correction of the correlation energy to the HF energy. In comparison, coupled cluster techniques \cite{Cizek1966} apply an exponential cluster operator $\hat{T} = \sum_k \hat{T}_k$ containing $k$-fold excitation operators $\hat{T}_k$ to the HF wave function $\ket{\Psi_0}$. We obtain the exact solution $e^{\hat{T}} \ket{\Psi_0}$ through perturbation theory. 
These methods can be used for larger systems in their truncated form. For example, instead of the FCI approach, we could consider a single excitation to reduce the computational cost. However, this will also decrease the accuracy of the final energy. Another approach is to reduce the number of electrons and orbitals considered in the CI expansion by selecting a complete active space (CAS) where all excitations are considered. This approach relies on the premise that chemical reactions primarily involve higher-energy electrons. Although we may not obtain the exact total energy, energy differences (such as those between reactants and products) should account for the electron correlation. 

The discussion above focuses on the simulation of molecules. The same concepts apply when simulating crystalline materials when the basis set consists of plane waves \cite{Singh2006} or Bloch functions built from local orbitals \cite{Hoffmann1987}. Crystalline orbitals, which are the periodic equivalent of molecular orbitals, are delocalised over the cell and, therefore, are difficult to use in a CAS approach. One method to solve this problem is to use the Wannier single-particle electron basis \cite{Wannier1937}. A detailed analysis of how these functions can be used in the context of quantum algorithms can be found in \cite{Clinton2022}.

%---------------------------------------------------------%
\subsubsection{Post Hartree-Fock quantum algorithms}\hfill

Quantum computing holds significant promise for enhancing two primary categories of chemistry challenges: post-Hartree-Fock (post-HF) methods and optimisation/global minimum search tasks. Both categories deal with the challenges of combinatorial explosion. The former arises when encompassing all possible excitations, as seen in the FCI approach, while the latter emerges in scenarios where a multitude of configurations need to be evaluated, such as in determining the thermodymamic properties of complex materials or the optimum folded structure of particular proteins. Efforts have been made to map chemical problems to quantum computing-based simulations \cite{Cao2019,Kassal2011}; understand the possible benefits of using quantum hardware \cite{Bauer2020}; and explore the possibility of QML \cite{Bernal2022}.

Another difficulty arises when simulating electrons due to their fermionic properties (half-integer spin). When mapping the wave function to the qubit system, we normally start from the second quantised form of the wave function, where the computational basis state is a Slater determinant in the Fock space. Then we use Jordan-Wigner or Bravyi-Kitaev mappings \cite{Tranter2018} to represent the correspondence between each fermionic state and qubit. The latter step is the bottleneck: the proposed methods have a scaling with respect to $N$ basis functions of between $O(N^{4})$ \cite{Xia2018} and $O(N^{2})$ \cite{Babbush2014} depending on the basis set used.  

Gate-based quantum hardware has been so far the technology of choice for performing post-HF simulations. On NISQ devices, FCI is suitable for small molecules such as H$_2$ and LiH \cite{McArdle2020}, while, for larger systems, using an active space is generally preferred. For example, Gao \textit{et al.} \cite{Gao2021} selected an orbital active space derived from an initial HF wave function to study the lithium superoxide dimer rearrangement. The method incorporates relevant orbitals present in reactants, products, and those involved in the transition state. In another instance, active space orbitals were selected from DFT orbitals to study the water splitting reaction on magnesium surfaces \cite{Gujarati2022}.

Accurately calculating electronic excited-state properties is another challenging area of research \cite{Westermayr2021}. This is relevant when we want to simulate spectroscopic experiments involving high-energy transitions above the ionisation potential, non-equilibrium dynamics, light-matter interactions and other excited-state based phenomena. In these cases, explicitly solving the time-dependent Schr{\"o}dinger equation is the most natural route \cite{Goings2018,Li2020c}. 
Such a method requires going beyond DFT and HF to properly account for electron correlation in real time. The time-dependent density matrix renormalisation group method \cite{Baiardi2021} is one possibility but it may not perform well for the dynamics of highly entangled states. This is an ongoing field of research.


%------------------------------------------------%
\subsubsection{Quantum optimisation for chemistry and materials science}\hfill

Quantum annealing (section \ref{ssec:qopt}) has been demonstrated to be a valuable tool for solving optimisation problems in chemistry, biochemistry and materials science. Given a set of variables (for example, atom position in the solid solution lattice or bond angle in protein folding), the aim is to find the configurations of those variables that correspond to the lowest energy. The configuration space to explore is proportional to $\binom{n}{k}$ with $n$ variables and $k$ possible values that each variable can take. This configurational space rapidly becomes very difficult to sample using classical methods for large values of $n$. Quantum annealing can offer an alternative means of finding the lowest energy configurations by exploiting quantum superposition and tunnelling. See \citeauthor{Camino2023}'s \cite{Camino2023} tutorial for more details.

Quantum annealers were used to solve the electronic Hamiltonian eigenvalue-eigenvector problem \cite{Teplukhin2020} and calculate the excited electronic states \cite{Teplukhin2021} for biatomic and triatomic molecules. In industry, researchers at Volkswagen used D-Wave to calculate the ground state energies of H$_2$ and LiH as a proof-of-principle \cite{Streif2018}, where their mixed discrete-continuous optimisation algorithm found the lowest eigenstate of the qubit coupled cluster method and used a quantum annealer to solve the discrete part of the problem \cite{Genin2019}. Menten AI used D-Wave to address the rotamer optimisation problem for protein design without major simplifications or a decrease in accuracy \cite{Mulligan2019}. GlaxoSmithKline studied the mRNA codon optimisation problem to help drug discovery \cite{Fox2021} using quantum annealers. Fujitsu developed a quantum-inspired annealing machine to screen chemicals for materials discovery \cite{Hatakeyama-Sato}. 

Optimisation can also be performed on gate-based hardware when the problem is formulated as a QAOA problem (section \ref{sssec:QAOA}). For example, the design of deuterated molecules for OLED applications has been studied using an approach that combines DFT-derived energies and VQE \cite{Gao2023}. 

%--------------------------------------------------%
\subsection{High-energy physics quantum simulations}

In 2016, the first full quantum simulation of a high-energy physics (HEP) experiment demonstrated the creation of electron-positron pairs from energy \cite{Martinez2016}. 
Gauge field theories are the underlying formalism describing interactions among elementary particles in the Standard Model, and can also be extended to physics beyond the Standard Model. Quantum simulators can be used to study gauge field theories and investigate non-perturbative dynamics at strong coupling. It is generally difficult to investigate the latter using conventional Monte Carlo techniques because of the ``sign problem'' \cite{Alexandru2022}. 
Quantum field theories are also used to describe condensed matter many-body systems.  Work on quantum simulation in this setting, e.g.,
quantum algorithms \cite{Byrnes2006}, and analogue \cite{Zohar2011} and digital simulators \cite{Buchler2005}, has evolved into an experimental field that has attracted the attention of particle and nuclear physicists \cite{Cloet2019,Bauer2023}. 
For example, neutral atom analogue simulators were first used to study the behaviour of condensed-matter systems. The quantum gas microscope is another significant advance, allowing research into quantum-degenerate gas phenomena where particle indistinguishability plays a key role \cite{Bakr2009,Cheuk2015}. Classical numerical methods cannot simulate these quantum phenomena, particularly for strongly-correlated fermionic systems and (non-Landau-Ginzburg) topological matter.  This demonstrated the potential for quantum computation in HEP simulations.
Better HEP simulations may eventually help us understand difficult problems like quark confinement and the properties of dense nuclear matter, e.g., neutron stars. 
\citeauthor{Bauer2023} \cite{Bauer2023} provide a roadmap of quantum simulations for HEP. 

The Large Hadron Collider (LHC) experiments produce enormous amounts of data as they search for new physics. This requires highly sensitive signal detection methods. To discover the Higgs boson particle, researchers analysed data from $10^{15}$ proton-proton collisions per experiment. The high-luminosity LHC (HL-LHC) programme will produce over 300 times as much data, which will need sophisticated automated methods to extract interesting signals. Quantum machine learning techniques may offer new methods for real time big data analysis \cite{Wu2022}. 

While there have been proof-of-concept studies of quantum error correction \cite{RyanAnderson2021,Sivak2023}, digital quantum simulators currently do not have sufficient resources to solve HEP problems at a significant scale. 
In the NISQ era, VQE and QAOA platforms produce meaningful results even with shallow circuits and without active error-correction procedures
(section \ref{ssec:VQAs}). There has been work on simulating field theories, including finding the lowest lying energy spectra of low-dimensional Abelian and non-Abelian lattice gauge theories \cite{Kokail2019,Atas2021,Irmejs2023}. It is important to note that many lattice gauge theory simulations on NISQ hardware require fully or partially removing the redundant degrees of freedom or by imposing symmetries. Simplifications like these are essential in the resource-limited NISQ era.


%=============================================%
\subsection{Important applications in industry}

Quantum simulation may potentially solve many industrial problems and bring about significant societal and environmental benefits. We give two examples in the following section. Researchers are using the Fermi-Hubbard model as an analogue quantum simulator for high-temperature superconductors, specifically copper oxide compounds. On the other hand, digital quantum simulation may help us understand how to produce ammonia fertilisers using less energy intensive processes.


\subsubsection{High-temperature superconductivity}\hfill

Theoretical high-energy and condensed matter physics share common fundamental concepts such as symmetry breaking, renormalisation group, and Feynman diagrams. Interesting phenomena occur in strongly correlated electronic systems where several physical interactions (spin, charge, lattice, and/or orbital) are simultaneously active. In particular, the discovery of high-temperature superconductors (HTS) in the 1980s \cite{Bednorz1986,Sheng1988} launched decades of diligent efforts to understand and use these compounds \cite{Zhou2021,Lilia2022}. 

\begin{figure}[ht!]
\centering
\includegraphics[width=1\linewidth]{YBCO.png}
\caption{Crystal structure of cuprate superconductor YBa$_2$Cu$_{3}$O$_{6+x}$ \cite{Dagotto1994}.}
\label{fig:YBCO}
\end{figure}

The Bardeen-Cooper-Schrieffer (BCS) theory \cite{Bardeen1957,Bardeen1957a} introduced the idea of electron-hole Cooper pairs to explain conventional low-temperature superconductivity. However, the pairing mechanism in `unconventional' superconductors is still unknown. (These materials do not conform to conventional BCS theory or its extensions.) Conventional superconductors are used in many application areas such as MRI machines and high speed trains, but they can only operate at relatively low temperatures. This severely limits their wider use. One of the holy grails of materials science is to develop superconducting materials that operate at room temperature. This would revolutionise many technologies \cite{Lilia2022}. In particular, room-temperature superconductors would decrease the heat wasted from electronic devices and allow them to run more efficiently. On a larger scale, HTS can help achieve the International Energy Agency's roadmap to carbon-free economies \cite{IEA2021} via nuclear fusion-generated electricity \cite{Ball2021}.

Researchers have focused on the two-dimensional Fermi-Hubbard model as an analogue quantum simulator. This model is believed to capture the important behaviour of HTS \cite{Dagotto1994,Fradkin2015}, specifically the cuprate supercondustors (a popular HTS) in the copper-oxygen planes (``CuO$_2$ planes'' in Figure \ref{fig:YBCO}). Despite the Fermi-Hubbard model's apparent simplicity, its theoretical properties are far from fully understood. It is difficult to solve accurately as the model exhibits competing orders in its phase diagram where it is most relevant to cuprates. Experimental work has involved building analogue quantum simulators of the Fermi-Hubbard model using quantum dots \cite{Hensgens2017,Wang2022} and ultracold atoms in optical lattices \cite{Mazurenko2017,Hirthe2023}.

The Fermi-Hubbard model is widely used to benchmark numerical methods for strongly correlated systems \cite{SimonsCollaboration2015}. Recent efforts explored variational approaches \cite{Cade2020,Cai2020,Stanisic2022}, while Google simulated the Fermi-Hubbard model using digital superconducting quantum processors \cite{Google2020}. 

A state-of-the-art circuit-model algorithm for simulating the two-dimensional Fermi-Hubbard time dynamics on an $8 \times 8$ lattice requires roughly $10^7$ Toffoli (CNOT) gates \cite{Kivlichan2020}. This includes the overhead for fault tolerance, given the gate fidelities of current and near-term hardware. A significant contribution to the gate count comes from the phase estimation procedure. \citeauthor{Clinton2021} \cite{Clinton2021} estimate the resources required to simulate the Fermi-Hubbard time dynamics on a $5 \times 5$ square lattice to $\leq 10$ \% accuracy using 50 qubits and 1,243,586 standard two-qubit gates. This (optimistic) estimate assumes the effects of decoherence and errors in the circuit can be neglected.

There is much work to be done before we have a complete microscopic theory of HTS \cite{Lilia2022,Zhou2021}. The combination of theory, simulation, materials synthesis and experiment has been crucial to progress in the last two decades. For example the breakthrough discovery of hydrogen-rich superconductors \cite{Drozdov2015}, may not have happened for another century \cite{Castelvecchi2023s} had there not been significant advances in simulation and algorithms for chemical structure prediction \cite{Li2014}. 

Under high pressure, super-hydrides are hydrogen-rich compounds that exhibit superconductivity at high temperatures. The high pressure pushes atoms so close togather that they begin to interact through both their outer and inner electrons. Understanding this electronic behaviour is considered one of the top problems in the HTS field. In the search for other super-hydrides, researchers are simulating combinations of different elements that combine with hydrogen. These types of simulation problems often require use of HPC and potentially quantum simulators in future. Despite the lack of a clear timeline on when we can expect a fundamental theory for HTS, researchers remain optimistic \cite{Castelvecchi2023s}. 


\subsubsection{Fertilisers}\hfill 

Another strong motivation for quantum simulation is to develop new materials and processes to make significant environmental contributions. For example, the Haber-Bosch process for producing ammonia (NH$_3$) fertilisers is one of the world's most CO$_2$-intensive chemical processes, consuming up to 2\% of global energy output \cite{Reiher2017} and 3-5\% of all natural gas generated globally \cite{BCG2020}. 

This process turns nitrogen in the air into ammonia-based fertiliser for crops and helps sustain about 40 \% of the world's population. The natural process, nitrogen fixation, is a much more efficient process: microorganisms that contain the biological enzyme \textit{nitrogenase} convert atmospheric dinitrogen (N$_2$) to ammonia under ambient conditions. Despite almost a century of research, the reaction mechanism is still unexplained \cite{Burgess1996,Vogiatzis2019}. Understanding how this enzyme works would be an important step towards replacing the Haber-Bosch process and creating less energy-intensive synthetic fertilisers.

\begin{figure}[ht!]
\centering
\includegraphics[width=1\linewidth]{femoco}
\caption{Nitrogenase enzymes convert atmospheric dinitrogen into ammonia. The process involves transferring multiple electrons and protons to dinitrogen and uses multiple metalloclusters found in the nitrogenase enzyme, including the 4Fe--4S cluster (top left), P cluster (top right) and Fe-Mo cofactor M cluster (bottom).}
\label{fig:femoco}
\end{figure}

The transition metal compounds within the enzyme potentially hold the answers. These are the ``4Fe--4S'' cluster (containing iron and sulfur), ``P cluster'' and the iron molybdenum cofactor ``M cluster'' (FeMoco, containing iron, molybdenum, carbon, hydrogen, and oxygen) \cite{Hoffman2014} (Figure \ref{fig:femoco}). The FeMoco active space contains 54 electrons in 108 spin orbitals.  Proposed computational models of FeMoco are beyond the reach of current classical methods but are possible with small error-corrected quantum computers. 

\citeauthor{Reiher2017} \cite{Reiher2017} estimate that a FCI calculation with a Trotterised approach to phase estimation would require around $10^{14}$ T gates. Assuming that we have a low physical error rate ($10^{-3}$), this would require around $2 \times 10^8$ physical qubits. The calculation would take several weeks (10 ns to implement a T gate, including surface code decoding) or months (100 ns per T gate). This work considered a targeted Majorana fermion-based quantum computer: physical error rates are roughly $10^3$ times lower than in trapped ion or superconducting qubits.

Google and collaborators \cite{Tazhigulov2022} calculated the energy states of the nitrogenase 4Fe--4S cluster. Its ground electronic states and low-energy excitations are determined by the electronic spin interactions. These spins could be encoded in single qubits and their interactions simulated by coupling the qubits in circuits that reflect the cluster structure. In practice, the 4Fe--4S cluster has long-range interactions between spins which require many quantum gates to represent accurately. However, simulations containing over 300 gates were overwhelmed by noise. This puts a limitation on the molecule size that could be simulated. The authors concluded that  quantum circuits may not provide much advantage over classical computers until they incorporate noise reduction and/or full quantum error correction methods. These results are both exciting and daunting: quantum computational chemistry and materials science has made great progress over the last decades, but \cite{Tazhigulov2022} also shows how much work is still ahead.


% ===========================================================
% ===========================================================

\section{Quantum algorithms for classical simulation}\label{sec:qfluid}

High-performance computing is used for modelling large-scale systems in many socially and economically important areas, including weather forecasting, cosmology, plasmas, coastal engineering, real time traffic management, and many other applications. These models often have analytically intractable non-linear partial differential equations (NPDEs) at their core, with a large number of variables in two or more dimensions.  Given the large amounts of HPC resources used to solve NPDEs, improved algorithms are highly important and the subject of significant research investment. For example, NASA's Quantum Artificial Intelligence Laboratory (QuAIL) aims to show how quantum algorithms may dramatically improve the agency's computational problem-solving ability \cite{Biswas2017,Rieffel2019}.

The HHL algorithm \cite{Harrow2009} (section \ref{sssec:HHL}) can be used in regimes where it is sufficient to take a linear approximation of the NPDEs. For many applications, it is crucial to model the nonlinear effects more accurately. In this section, we focus on one of the most common NPDEs, the Navier-Stokes equation for computational fluid dynamics (CFD), but the quantum algorithms can be readily adapted to other NPDEs.

%==============================================%
\subsection{Solving the Navier-Stokes equations}

In CFD, the aim is to solve the Navier-Stokes (NS) equations which describe a flow of pure gas or liquid (single- or multi-phase flows)
\begin{equation}\label{eq:navierstokes}
\frac{\partial\mathbf{u}}{\partial t} = \underbrace{-\mathbf{u}\cdot\nabla\mathbf{u}}_\text{advection} - \nabla p + \underbrace{\frac{1}{\text{Re}}\nabla^2\mathbf{u}}_\text{viscous diffusion}
\end{equation}
with continuity equation 
\begin{equation}
\nabla\cdot\mathbf{u}
\begin{cases}
= 0, & \text{incompressible flow,} \\
\neq 0, & \text{compressible flow.} 
\end{cases}
\end{equation}
Velocity $\mathbf{u}(\mathbf{r},t)$ and pressure $p(\mathbf{r},t)$ fields are functions of time $t$ and spatial coordinates in continuous space $\mathbf{r}$. The Reynolds number Re describes the ratio of inertial force to viscous force (viscous diffusion). 

The Navier-Stokes equations consider the contribution of all forces acting on an infinitesimal element of volume and its surface. Given a certain mass of fluid in a region of space, two types of forces act on it: volume (forces outside the region) and surface (internal forces arising from fluid interactions via the boundary surfaces). The Navier-Stokes equations are a system of three balance equations (PDEs) of continuum mechanics which describe a linear viscous fluid. Under this umbrella, Stokes' law (kinematic balance) refers to the specific case of force on a moving sphere in fluid, and Fourier's law (energy balance) is a fundamental law of the material. 

\begin{figure*}[ht!]
\centering
\includegraphics[width=\linewidth]{cfd-methods-v2}
\caption{Examples of numerical methods for simulating CFD problems: finite volume method (FVM), element-based finite volume method (E-FVM), finite element method (FEM), finite difference method (FDM), particle-in-cell (PIC), moving-particle semi-implicit method (MPS), particle-based FEM (P-FEM), smooth particle hydrodynamics (SPH).}
\label{fig:cfd-methods}
\end{figure*}

A straightforward approach to solving the NS equations is direct numerical simulation (DNS) \cite{Moin1998}. DNS directly discretises the NS equations and relies on using a mesh size that provides resolution at all scales of turbulent motion, including Kolmogorov length scales. Thus, there is no need for any subgrid modelling in order to capture turbulent flow dynamics, and an exhaustive description of the fluid is available throughout the domain. However, this understanding comes at high computational cost due to the requirement of a sufficiently fine mesh. DNS is too costly to use in most industrial problems and instead is generally relegated to more fundamental research. Hence, this fuels the need for alternatives, better suited to real world applications. 

Figure \ref{fig:cfd-methods} summarises some methods for solving the NS equations which form the basis of many commercial software packages \cite{Chung2002}. These include mesh-based methods such as the finite difference methods (FDM), finite volume methods (FVM) and finite element methods (FEM). Mesh-based models divide a continuum domain into smaller subdomains where the size of the subdomain is varied depending on the level of detail required around that region. 
Despite their success in science and engineering problems, mesh-based methods encounter difficulties at free surfaces, deformed boundaries, moving interfaces, and extremely large deformation and crack propagation. It is time-consuming and expensive to generate quality meshes for complicated geometries. For example, in FDM, irregular or complex geometries usually require additional transformations (e.g., mesh-rezoning \cite{Liu2003m}) that are more expensive than directly solving the problem itself, and may introduce numerical inaccuracies. 
 
In contrast to more established and mostly mesh-based codes, the smoothed particle hydrodynamics (SPH) method offers a viable alternative, especially for flows with a free surface \cite{Monaghan2005}. SPH is a purely Lagrangian approach that discretises a continuum using a set of point particles. It has become popular in the last few decades as a way to simulate a wide range of applications, e.g., in engineering \cite{Monaghan2012} and astrophysics \cite{Lucy1977,Springel2010}. Another relatively new approach, the lattice-Boltzmann (LB) method, combines both mesh- and particle-based methods.  LB is an advance on lattice gas automata that overcomes the convergence problems of directly using particles on grids.
Both SPH and LB have interesting potential for quantum algorithms or quantum-classical hybrid enhancements.  We discuss both in turn, first summarising the classical method, then explaining the proposed quantum algorithms.

\subsection{Lattice Boltzmann method }\label{ssec:LB}

LB is a mesoscale approach based on the Boltzmann equation. It naturally bridges microscopic phenomena with the continuum macroscopic equations and accommodates a wide variety of boundary conditions \cite{Kruger2017,Succi2001}. LB has good scalability for solving PDEs due to its algorithmic locality, hence is inherently suited for large-scale parallelisation on HPC systems, including use of GPUs \cite{Feichtinger2015,Liu2017l,Tran2015}.

The LB method considers a typical volume element of fluid that contains a collection of particles \cite{Chen1998}. These are characterised by a particle velocity distribution function for each fluid component at each grid point. At discrete time steps, fluid particles can collide with each other as they move, possibly under applied forces. The particle collision behaviour is designed such that their time-average motion is consistent with the Navier-Stokes equation. The fluid is treated as a group of particles that have only mass and no volume. Particles flow in several directions of the lattice and collide with the particles around them. LB uses the collective motion of microscopic particles to describe the macroscopic parameters such as velocity, pressure, and temperature according to kinetic theory.

\begin{figure}[ht!]
\centering
\includegraphics[width=\linewidth]{lbm-grid}
\caption{Schematics of two-dimensional LB model showing nodes (orange), collision (grey arrows) and streaming (blue arrows) processes. Adapted from \cite{Li2005}.}
\label{fig:lbm-grid}
\end{figure}

We use the Lattice Bhatnagar-Gross-Krook (LBGK) notation to describe lattice structures: the \textit{DnQb} classification indicates an $n$-dimensional space where each particle collides with $b$ surrounding particles (including itself). For example, the $D2Q9$ lattice in two-dimensional space contains a particle labelled $c_0$ which collides with the surrounding particles in eight directions (Figure \ref{fig:lbm-grid}). Particle movement is described in two steps: collision and streaming. In the collision step, particles collide with the opposite particles along each axis, changing the velocity. In the streaming step, particles move in the velocity direction to the neighbouring lattice. The evolution equation is
\begin{equation}\label{eq:lbm}
f_k(\mathbf{x}+\mathbf{e}_k, t+\Delta t) - f_k(\mathbf{x}, t) = 
\Omega(f_k^\text{eq}(\mathbf{x}, t) - f_k(\mathbf{x}, t))
\end{equation}
with collision term $\Omega$, fluid distribution $f_k(\mathbf{x}, t)$ at point $\mathbf{x}$ at time $t$, and equilibrium distribution $f_k^\text{eq}$. At the next timestep $t+\Delta t$, the fluid would be at point $\mathbf{x}+\mathbf{e}_k$. The direction vector is $\mathbf{e}_k$ where $k$ represents the nine different directions in $D2Q9$. 

In practice, the $D2Q9$ lattice example (Figure \ref{fig:lbm-grid}) requires up to several million nodes to generate an accurate flow field. Each node is connected to its nearest neighbours with blue vectors, denoted as $\mathbf{e}_k$. (Not shown in the figure is one $\mathbf{e}_k$ pointing to the node itself.) Every $f_k$ moves along its $\mathbf{e}_k$ vector to its neighbour and replaces the neighbour's distribution $f_{k'}$, except the one pointing to the node itself.

\begin{figure*}[ht!]
\centering
\includegraphics[width=\textwidth]{lbm-gpu}
\caption{Sketch of parallelised LB numerical process with CPU and GPU.}
\label{fig:lbm-gpu}
\end{figure*}

A common theme in LB applications is their suitability for parallel computing \cite{Alowayyed2017,Succi2019}. This is primarily due to the intrinsic locality of the method. For each time step, both the collision and streaming operators only require communication between neighbouring cells at most. Recent research has focused on parallelising and optimising the computation using heterogeneous acceleration devices such as GPUs. The idea is to divide the LB lattice and group the distributions $f_k$ into arrays according to their velocity vectors $\mathbf{e}_k$ (Figure \ref{fig:lbm-gpu}). The first iteration starts with the distributions as inputs to calculate the density and velocity outputs. These quantities allow us to find the equilibrium distributions. New distributions are then computed from the input distributions and the equilibrium distributions according to the collision and the streaming operations. Finally, the boundary and outflow conditions are used to update the distributions. The updated distributions are then used as inputs for the next simulation step. For further details, see \cite{Wei2004}. Example open-source software platforms include OpenLB \cite{Krause2021} and Palabos \cite{Latt2021}.


%=======================================%
\subsection{Quantum lattice Boltzmann}

Early attempts at applying quantum methods to CFD generally involved quantum lattice gas models \cite{yepez2001quantum} and ``type II quantum computers'' \cite{Berman2002}. Both contain several quantum nodes connected by classical channels that carry bits instead of qubits \cite{yepez2001type}, while the latter is mathematically equivalent to a classical LB formulation \cite{love2006type}. These lattice models are susceptible to noise, non-isotropic advection and violation of Galilean invariance. Recently a revised quantum algorithm for lattice gas automata \cite{chrit2023fully} managed to eliminate the need for repeated measurements at each time step. However the size of the required quantum register scales linearly with lattice sites, making extension to realistic use cases difficult.

Hence there is strong motivation to build quantum algorithms on top of a LB framework. In fact, many inherent properties of LBs lend themselves naturally to this aim, such as the built-in fine grain parallelism or relaxation of non-linearity present in the full Navier-Stokes equation. Using a microscopic framework to describe macroscopic phenomena greatly simplifies the mathematical constraints and requires us to only evolve a single variable, i.e., the distribution function. 

There is a direct correspondence between the streaming step and quantum walks \cite{succi2015quantum}. \citeauthor{Todorova2020} \cite{Todorova2020} build on the latter and present the first fully quantum (as opposed to hybrid classical-quantum) LB method. The authors consider the simplified case of a collisionless Boltzmann system which reduces to
\begin{equation}\label{e:lbm}
\frac{\partial F (\mathbf{x},\mathbf{c};t)}{\partial t} + \mathbf{c} \frac{\partial F(\mathbf{x},\mathbf{c};t)}{\partial \mathbf{x}}=0
\end{equation}
with single-particle distribution function $F(\mathbf{x},\mathbf{c};t)$ defined in physical $\mathbf{x}$ and velocity $\mathbf{c}$ spaces. Classically, the discrete velocity method discretises the solution state space into potentially three spatial and flow dimensions (equation \ref{e:lbm}), leading to a six dimensional solution space which can be computationally expensive. Therefore a key stepping stone for any appreciable quantum advantage is to represent this higher dimension space as a quantum state using a limited number of qubits. As shown by \citeauthor{Todorova2020}, this can provide an exponential reduction in memory, particularly so for the extension to multiple species. Where classically doubling the species would require doubling the memory, this was shown to be achievable with one extra qubit. Note also that the assumption of a collisionless system essentially decouples equation \ref{e:lbm} for each species, a caveat which is not true for more general systems. 

A primary obstacle to quantum LB algorithms is the need to take measurements. This results in costly initialisation routines when re-preparing the quantum state.
\citeauthor{Todorova2020} \cite{Todorova2020} address this issue by proposing that the purpose of their QLB is different to conventional CFD. The idea was to forego acquiring the complete flow field, i.e., the algorithm does not obtain a complete picture of the classical state. Instead it efficiently obtains specific information such as particle number densities and concentrations for multi-species configurations. Hence, the algorithm allows an uninterrupted temporal iteration with measurement postponed until the end, at the cost of reducing the obtainable information. Similar approaches from quantum simulation and quantum chemistry are discussed in \cite{ortiz2001quantum,somma2002simulating} or techniques to obtain a single amplitude in \cite{Brassard2002}.

Including the collision operator into LB adds considerable complexity but is essential in using the method as a general PDE solver. One approach relies on Carleman linearisation where an equivalent linear system in a higher dimensional space essentially replaces the nonlinear system. 
Recent work \cite{itani2022analysis} explored this in the context of nearly incompressible LB to find a quantum formulation for the collision term. Building on their findings, the authors propose a fully unitary streaming and collision operator \cite{itani2023quantum}. However, their collision step has a sub-optimal dependence on the number of time steps. This makes it unclear whether it is possible to achieve practical quantum advantage, particularly at large Reynolds numbers. 

Alternatively, \citeauthor{Budinski2021} \cite{Budinski2021} formulates a fully quantum algorithm for the complete LB, i.e., including non-unitary collision operators using the standard form encoding approach \cite{low2019hamiltonian} and a quantum walk for the propagation step. Having initially applied this to an advection-diffusion equation, it was later extended to the streamfunction-vorticity formulation of the Navier-Stokes \cite{ljubomir2022quantum}. However, because the collision operator is retained in the equilibrium distribution function, the resulting nonlinearity restricts the algorithm to a single time-step before the state must be reinitialised. 

It is clear that LB offers an exciting avenue for research in terms of developing a quantum counterpart. Being able to choose a particular collision operator provides flexibility in which equation is actually being solved at the continuum level. Hence, the LB method should be considered a versatile equation solver and not just a method for the Navier-Stokes alone. 
In addition, the classical LB's memory intensive state representation may imply that a quantum advantage is trivial to obtain. However, several key challenges remain and perhaps one of the most pressing is the need for a more meaningful encoding. It has been shown \cite{schalkers2023importance} that the conventional amplitude and basis encodings alone are not enough to allow unitary operators for both streaming and collision. 
It is unclear what the best strategy is for algorithm developers. Even so, it is safe to say that the path to obtaining a quantum advantage heavily depends on choosing a particular encoding. It is imperative that this is given due attention in future work. 


%===========================================================%
\subsection{Smoothed particle hydrodynamics}\label{ssec:SPH}

Mathematically, the SPH method expresses a function in terms of its values at locations of the virtual SPH particles. The integral interpolant of any function $A(\mathrm{r})$ is an integral $A(\mathbf{r}) = \int_\Gamma A(\mathbf{r'}) W(\mathbf{r}-\mathbf{r'},h) d\mathbf{r'}$ over the entire space $\Gamma$ for any point $\mathbf{r}$ in space and smoothing kernel $W$ with width $h$. This can be approximated by a summation interpolant,
\begin{equation}\label{eq:sph-interpolant}
A_S(\mathbf{r}) = \sum_i m_i \frac{A(\mathbf{r}_i)}{\rho_i} W(\mathbf{r}-\mathbf{r}_i,h)
\end{equation}
that sums over the set of all SPH particles $\{i\}$. Each particle $i$ has mass $m_i$, position $\mathbf{r}_i$, density $\rho_i$, and velocity $\mathbf{v}_i$. Hence, we can construct a differentiable interpolant of a function from its values at the particle level (interpolation points) by using a differentiable kernel \cite{Monaghan2005}.

\begin{figure}[ht!]
\centering
\includegraphics[width=\linewidth]{sph-kernel}
\caption{Schematics of two-dimensional SPH model with SPH particle $i$, its neighbour $j$, kernel $W$, smoothing length $h$ and compact support domain $\Gamma$. Here, $h$ is equal to radius of support domain (support length). Full support length can typically be between $2h$ and $4h$.}
\label{fig:sph-kernel}
\end{figure}

The general SPH workflow contains three main stages that are repeated at each time step. First, the algorithm creates a neighbour list. Then it calculates the particle interactions for momentum and continuity conservation equations. This involves calculating the smoothing length and SPH kernels. The final step is time integration (or system update). The SPH particle distribution follows the mass density of the fluid while their evolution relies on a weighted interpolation over nearest neighbouring particles. This has several implications. The SPH kernel smoothes the physical properties within the range of interpolation (Figure \ref{fig:sph-kernel}). This is characterised by the smoothing length $h$ which also determines the local spatial resolution. The identity of the neighbouring particles change as the simulated system evolves. There is no computationally efficient method to predict which particles will be neighbours over time, hence we must identify the neighbours at each timestep.

Parallelisation strategies for SPH are strikingly different from those of mesh-based methods. The computational domain is divided into a grid of cells where each SPH particle is assigned a cell. We build a list of its neighbours by searching for particles only in nearby cells. Dynamical neighbour lists require specialised methods for data packing and communicating. Particles migrating between adjacent domains can cause difficulties on memory management especially in distributed-memory architectures, as can large variations in particle density and domain size. The total number of particles can narrow the choice of hardware, as memory space is one of the main limitations of shared memory architectures. Using both shared and distributed memories, most SPH parallelisation schemes are scalable \cite{Ferrari2009,Dominguez2013}. However, if communication latencies increase, scalability decreases rapidly \cite{Oger2016}.

Example software applications demonstrate the versatility of SPH methods in various fields. SPHysics, for instance, is an SPH code tailored for simulating free surface flows which are challenging for other methods but effectively handled by SPH. Similarly, DualSPHysics \cite{Dominguez2022} capitalises on the advantages of SPH by using a hybrid architecture that combines CPUs and GPU accelerators for enhanced performance. The astrophysics community also extensively uses SPH codes, as evidenced by the popularity of SWIFT \cite{schaller2023swift} and GADGET \cite{springel2005cosmological}. These tools have significantly contributed to advancements in cosmological and astrophysical research.

\begin{figure*}[ht!]
\centering
\includegraphics[width=.8\textwidth]{sph-gpu}
\caption{Sketch of parallelised SPH numerical process with CPU and GPU.}
\label{fig:sph-gpu}
\end{figure*}

Figures \ref{fig:lbm-gpu} and \ref{fig:sph-gpu} show the typical workflow for combined CPU-GPU implementation of LB and SPH. We break down the algorithm into parts that are suitable for different types of classical hardware. This illustrates how to incorporate accelerators into large scale simulations. Extending this to incorporate quantum computers used as accelerators can hopefully provide further efficiencies.


%======================%
\subsection{Quantum SPH algorithm}

Recent reviews \cite{Lind2020,Vacondio2021} discuss the computational bottlenecks and grand challenges we face before SPH becomes more widely used for practical problems (e.g. in engineering). The SPH method has traditionally been considered computationally expensive \cite{Lind2020} due to two major factors: a large number of SPH particles are needed for good solution accuracy and time steps must be small enough to obey empirical stability criteria. One strategy to address these bottlenecks involves using quantum subroutines \cite{AuYeung2024}. Below we list examples of how to substitute classical SPH procedures with quantum algorithms.

Rewriting the SPH interpolant (equation \ref{eq:sph-interpolant}) as an inner product of the form
\begin{equation}
\sum_i m_i \frac{A(\mathbf{r}_i)}{\rho_i} W(\mathbf{r}-\mathbf{r}_i,h) \to
\langle m A(\mathbf{r})/\rho
\vert W(\mathbf{r},h)
\rangle
\end{equation}
requires efficiently loading the classical data (floating point numbers) into quantum processors using quantum encoding techniques. There are several possible methods to calculate the inner product: the swap test \cite{Buhrman2001} or one of its variations \cite{Fanizza2020}; the Bell-basis algorithm \cite{Cincio2018} which is efficient on NISQ devices; quantum mean estimation and support vector machines \cite{Liu2022}; or quantum counting algorithm \cite{Brassard1998,Aaronson2019}.

The summation in equation \ref{eq:sph-interpolant} means that the algorithm requires a search algorithm to find all the neighbouring particles inside the compact support domain $\Gamma$ (Figure \ref{fig:sph-kernel}). This is another major bottleneck that we can, in principle, address using Grover's search algorithm \cite{Grover1996} implemented with a quantum walk (sections \ref{sssec:search}, \ref{sssec:qw}). This can be an effective search method when combined with existing SPH neighbour-list approaches, eg. cell-linked or Verlet lists \cite{Dominguez2011}.

The timestepping procedure is subject to empirical stability criteria like the Courant-Friedrichs-Lewy convergence condition \cite{Courant1967}. Physically, in the widely-used weakly-compressible \cite{Violeau2014} and incompressible \cite{Violeau2015} forms of SPH, the timestep is also limited by the speed of sound and maximum velocity respectively. This leads to timesteps of the order of $\leq 10^{-5}$ seconds \cite{Lind2020}, or typically one million time steps to simulate one second of physical time. Most three-dimensional applications require 10-100 million SPH particles. High computational costs have motivated research into timestepping procedures such as the Runge-Kutta Chebyshev scheme \cite{He2021}. This is where quantum algorithms may potentially provide an even greater speed up, since we do not need to read out the data every time step.  A quantum algorithm that can evolve for many time steps between measurements has the best chance of providing real advantages.


\subsection{Other quantum CFD algorithms}

There are quantum algorithms that directly perform numerical integration for linear differential equations and provide some quantum advantages \cite{Childs2021p,Linden2022}. However, specialised quantum algorithms are less well developed, and methods for nonlinear PDEs remain open \cite{Balducci2022}. 
One approach is to discretise the nonlinear PDEs and in doing so essentially replace them with nonlinear ODEs. Subsequently, a quantum nonlinear ODE solver can be applied directly. This approach was adopted in \cite{Gaitan2020,Gaitan2021} using an ODE solver that had gone largely unnoticed until recently \cite{kacewicz1987optimal}. The method comes with stringent requirements on the underlying flow field and its rate of change. Despite this, a regime exists where there may be a quadratic speedup over classical random methods, and an exponential speedup over classical deterministic methods. Replacing the ODE solver with alternative formulations may also provide some benefits within the overall method.

While most discussions of quantum computing focus on when quantum computers will be able to consistently beat their classical counterparts, the reality is more likely to involve hybrid quantum-classical devices. For example, if it is possible, solve the non-linear parts of the calculation on conventional HPC, then send the linear part to a quantum processing unit which uses the HHL algorithm. However, if the classical part then requires consistent feedback from the quantum part, the cost of measurement and reloading input states may significantly reduce any quantum advantage

We can extend variational algorithms (section \ref{ssec:VQAs}) to solve nonlinear variational problems \cite{Lubasch2020,Jaksch2023}. At its heart, this scheme uses quantum nonlinear processing units (QNPUs) to evaluate cost functions that are polynomials of individual trial functions. Hence QNPUs can be applied to a wide range of nonlinear problems including nonlinear PDEs.

The vortex-in-cell method is an example of how the quantum Fourier transform is used to build a Poisson solver \cite{Steijl2018}. However, the main drawback is the assumption that multiplication with the wavenumbers can be done efficiently on classical hardware. There is also the added complexity of accommodating non-periodic boundary conditions with any Fourier-based CFD method, meaning its translation into a quantum setting may not be straightforward. 

Generally, research in this field should consider the range of applicability of the methods being developed. For instance, many existing demonstrations of quantum algorithms for CFD are limited to relatively simple flow cases. The effectiveness of these algorithms at scaling with the Reynolds number is either yet to be established or appears limited. While configurations involving low Reynolds numbers are valuable in areas like geophysics and biophysics, the study of turbulence at high Reynolds numbers is crucial for many disciplines. 

The challenge certainly extends beyond mere flow dynamics. Complex boundary conditions, such as those found in natural and industrial environments, present an additional layer of complexity. Quantum algorithms need to evolve to accommodate these conditions effectively. 


% ===========================================================
% ===========================================================

\section{Interfacing with high performance computing}\label{sec:hpc}

When learning about classical algorithms, it is rare to see discussions of data representation and data transfer to the processing hardware (although memory access considerations become important in big data applications). For quantum algorithms, these issues cannot be ignored. Deciding how to represent information as a quantum state is an important part of designing the quantum algorithm and influences what speedups we can expect. The data encoding procedure into the quantum system is part of the algorithm and may account for a crucial part of the complexity. This is not always included in the headline complexity claimed for the quantum algorithm \cite{Aaronson2015}. Theoretical frameworks that address the interface between classical memory and quantum devices are therefore critical for implementing quantum algorithms. In this section, we summarise the available data encoding approaches for quantum computing, and the challenges they present for interfacing with classical HPC.

%-------------------------------%
\subsection{Clock speed mismatch}

Every computation on a classical computer processor corresponds to a sequence of layers of logic gates. The time taken to execute one layer is the \textit{clock cycle}. Modern CPUs contain several billion transistors operating at around 3 to 4 GHz or 1/3 nanosecond per cycle. When quantifying the speed of quantum computers, we can break down quantum algorithms into a sequence of quantum gate layers. However we cannot define the \textit{quantum clock speed} as an inverse of the quantum gate layer time because of two major variables:
\begin{enumerate}
\item Gate choice. Quantum algorithms can be decomposed into elemental gates, but the choice of gates is flexible. Hence, the quantum gate duration can vary, even for different gate layers in the same hardware.
\item Quantum error correction. Quantum computers make many more errors than classical CPUs. Large-scale quantum computers will likely have a runtime dominated by quantum error correction (QEC) overheads. Each QEC method consists of different routines with different runtimes, that can be dominated by mid-circuit measurements, rather than gate times.
\end{enumerate}
The National Institute of Standards and Technology (NIST) has defined potential speed limits by estimating the upper and lower bounds for how quickly information can travel within quantum systems \cite{FossFeig2015,Jordan2017}. 

\begin{table}[ht!]
\centering
\caption{Quantum hardware platforms ordered by decreasing gate speed (conservative estimates).}
\bgroup
\def\arraystretch{2}
\footnotesize
\begin{tabular}{p{1.2in}p{.8in}p{.9in}}
\textbf{Platform} & \textbf{two-qubit gate speed} & \textbf{measurement time} \\[4pt]
\hline
superconducting qubits$^\text{\cite{Google2023}}$ &
10 MHz & 660 ns
\\
silicon spin qubits$^\text{\cite{Blumoff2022}}$ &
750 kHz & 1.3 $\mu$s
\\
trapped ions$^\text{\cite{Harty2014,Todaro2021}}$ &
6 kHz & 100 $\mu$s
\\
Rydberg arrays$^\text{\cite{Kwon2017}}$ &
170 Hz & 6 ms
\\
\makecell{photons \\ (fusion-based)$^\text{\cite{Istrati2020}}$} &
10 Hz & 100 ms
\\
\hline
\end{tabular}
\egroup
\label{tab:hardware_clock_speed}
\end{table}
%
Table \ref{tab:hardware_clock_speed} shows typical gate operation speeds and measurement times for different hardware platforms.  Note that these are physical gate speeds that error correction overheads will significantly modify.  Slower, but higher quality, trapped ions or Rydberg atoms will need less error correction than more noisy, but faster, superconducting qubits.

Classical CPU clock speeds have remained around 3 GHz over the last decade, which is significantly faster than any current quantum hardware. The steady increase in processor speeds that occurred during several decades of Moore's law scaling \cite{Moore1965} (Figure \ref{fig:microprocessor_trends}) is not possible for quantum hardware platforms: individual atoms, ions or electrons that have their own intrinsic physical frequencies cannot be made smaller.  We therefore need to develop programming models that can leverage processors with vastly different effective clock speeds.  A hybrid algorithm may need to queue 100 or $10^4$ operations on the classical processor for every operation on the quantum processor.  This should be viable for some of the large, multi-scale simulations that dominate today's HPC use, where many parts of the computation can be done classically while the quantum processor solves key components that are expensive for classical computation.


\subsection{Quantum data encoding} \label{ssec:encoding}

In classical computing, floating point numbers are ubiquitous for numerical data in scientific applications.  However, they are not the natural way to encode data into qubits, and the choice of data encoding can have a big impact on quantum algorithm efficiency.

\paragraph{Basis (digital) encoding.} Basis encoding is the simplest encoding. It corresponds to binary encoding in classical bits.  Multi-qubit basis states are often written in a single ``ket'' as
$\{ \ket{0...00}, \ket{0...01}, ... \ket{1...11} \}$ or $\ket{j}$ where it is understood that the integer $j$ corresponds in binary to the qubit values. The bitstring $b_{n-1} ... b_0$ is thus encoded by the $\ket{ b_{n-1} ... b_0}$ state.  Single computational basis states are easy to prepare from the all zero state, by applying bit flips to the appropriate qubits.

\paragraph{Angle encoding.} Angle encoding extends basis state encoding to use the ability of qubits to be in any superposition of zero and one.  A convenient parameterisation of this using angles $\theta$ is
\[
\cos\theta\ket{0} + \sin{\theta}\ket{1}.
\]
An $n$-qubit register can thus encode $n$ angle variables $\{\theta_k\}$.  This is efficient to prepare if arbitrary single qubit rotation gates are available. We xtend this by encoding a second angle $\phi$ in the phase,
\[
\cos\theta\ket{0} + e^{i\phi}\sin{\theta}\ket{1},
\]
and noting that qubits correspond to unit vectors on a Bloch sphere (Figure \ref{fig:bloch-sphere}).  However, it is not possible to read out the value of the angles using a single measurement on a single copy of the qubit.  While the state preparation is easy, readout is only straightforward for results encoded as basis states.

\paragraph{Amplitude (analogue) encoding.} Amplitude encoding loads a vector of real numbers $X=(x_0,x_1\dots,x_{N-1})$, with $x_k\in[0,1]$, into the amplitudes of the quantum state, so they are stored in superposition 
\[
X \to \sum_{k=0}^{N-1} x_k \ket{k}.
\]
The overall normalisation factor is stored separately if it is not equal to one.  An $n$-qubit register stores up to $N=2^n$ real values, so this is an efficient encoding. However, to manipulate individual amplitudes, the number of gates grows exponentially with qubit number. Preparing an arbitrary state is thus an expensive operation \cite{Long2001,Grover2002,Plesch2011,Sanders2019}. Refinements include ``approximate amplitude encoding'' \cite{Nakaji2022,Mitsuda2022} which reduces the preparation cost to $O(\text{poly}(n))$ gates for $n$ qubits at a cost of reduced accuracy. It is also possible to encode data more efficiently for wavepacket dynamical simulations in quantum chemistry \cite{Ollitrault2020,Chan2023}.

\paragraph{QRAM encoding.} Quantum random access memory (QRAM) is a general-purpose architecture for quantum oracles (see \cite{Jaques2023} for an excellent review). It is a generalisation of classical RAM \cite{Giovannetti2008,Giovannetti2008a}: given an address $k$ as input, the RAM returns memory contents $j_k$. Analogously, QRAM takes a quantum superposition of addresses $\ket{\psi_\text{in}}$ and returns an entangled state $\ket{\psi_\text{out}}$ where each address is correlated with the corresponding memory element:
\begin{gather}
\ket{\psi_\text{in}} =
\sum_{k=0}^{\ell-1} \ket{k}^A \ket{0}^B \nonumber\\
\xrightarrow{\text{QRAM}}
\sum_{k=0}^{\ell-1} \ket{k}^A \ket{j_k}^B
= \ket{\psi_\text{out}} \label{eq:qram}
\end{gather}
for loading $\ell$ of $N$ data values. Input and output registers are denoted by $A$ and $B$ respectively. The principle behind QRAM is that, if a query state is a superposition over all addresses, then the circuit responds by performing a memory access operation over all addresses simultaneously. If we imagine the memory laid out in space, a QRAM access must transfer some information to each location in memory for it to correctly perform a superposition of memory accesses.

QRAM performs the operation  in equation \eqref{eq:qram} in $O(\log N)$ time at the cost of $O(\log\ell)$ ancillary qubits. This makes QRAM especially appealing for quantum algorithms that require $O(\log N)$ query times to claim exponential speedup. However, it is still uncertain whether QRAM can be used to achieve quantum speedups, in principle or in practice \cite{Aaronson2015,Ciliberto2018}. 
QRAM is sensitive to decoherence \cite{Arunachalam2015} and is subject to an overhead associated with error correction of ancillary qubits \cite{DiMatteo2020}. Recent efforts have led to developments in highly noise-resilient QRAM using bucket-brigade QRAM architecture \cite{Hann2021}, and this continues to be an active open area of research.

\paragraph{Floating point encodings.}  Classical digital computers encode floating-point numbers using the IEEE 754 international standard, established in 1985 by the Institute of Electrical and Electronics Engineers (IEEE) \cite{IEEE754}.  
Work is currently underway to develop the IEEE P7130 Standard for Quantum Computing Definitions \cite{IEEE7130}.
Basis state encoding is another option for floating point numbers. It uses a mantissa and an exponent in direct analogy to classical floating point numbers.
However this is not the only (or the best) method, depending on the application. 
For example, quantum algorithms that rely on amplitude encoding for their efficiency gains cannot use basis state encoding.
\citeauthor{Seidel2022} \cite{Seidel2022} explore an alternative way to design and handle various types of arithmetic operations. They formulate arithmetic ring operations as semiboolean polynomials, and extend their encoding from intergers to floating point numbers, with significant efficiency gains over basis state encodings.

\paragraph{Block encodings.}  Block encodings of matrices play an important role in quantum algorithms derived from the quantum singular value transformation \cite{Gilyen2019,Sunderhauf2023}, such as Hamiltonian simulation \cite{Berry2015} and QML \cite{Childs2017}. Since it is only possible to implement unitary matrices on quantum computers, we can embed a (scaled) non-unitary matrix $A$ into a larger unitary matrix,
\begin{gather}
U = \begin{bmatrix} A/\alpha & * \\ * & * \end{bmatrix}
\\
\Updownarrow \nonumber
\\
A = \alpha (\bra{0}^{\otimes a} \otimes I_n) U (\ket{0}^{\otimes a} \otimes I_n), \quad a=m-n.
\end{gather}
with arbitrary matrix elements $*$ for $n$-qubit matrix
$A$ and $m$-qubit unitary $U$ (Figure \ref{fig:block-encoding}). 

\begin{figure}[ht!]
\centering
\includegraphics[width=\linewidth]{block-encoding}
\caption{Block encoding $U$ of $A$}
\label{fig:block-encoding}
\end{figure}
The subnormalisation factor necessary for encoding matrices of arbitrary norm is $\alpha$, and the number of ancilla qubits used in the block-encoding is $a$. $\tilde{A}$ is the partial trace of $U$ over the zero state of the ancilla space. Block encodings can be easily combined through tensor products and linear combinations. 


\paragraph{Hamiltonian encoding.}  Hamiltonian encoding is suitable for quantum optimisation.  The problem constraints are encoded in a QUBO or the fields and couplings of an Ising Hamiltonian (section \ref{ssec:qopt}). Loading the classical data on to a quantum device involves setting the values of the couplers and fields through classical control lines. Although straightforward to implement, the required precision increases with the size of the problem. Additionally, not every problem can be easily mapped on to the Ising Hamiltonian parameters. 


\paragraph{One-hot and domain-wall encoding. } One-hot and domain-wall encodings are unary encodings originally designed for higher dimensional variables in quantum optimisation \cite{Chancellor2019,Berwald2023}. 
One-hot encoding sets a single bit in a position that determines the value of the variable.
Domain-wall encoding is based on a topological defect in an Ising spin chain. One side of the chain is in a spin-down state, whereas the other is spin-up. The domain-wall (topological defect) is where the system jumps from spin-up to -down.  The location of the domain wall represents the value of the variable.  The number of qubits in the spin chain determines the number of available values.  Domain wall encoding is efficient in NISQ devices for variables with a small fix range of values \cite{Plewa2021}.

\subsection{Data interconnections}



As HPC users know well, exchanging data between different processing units is where the most bottlenecks occur in large scale computations. 
In addition to the clock speed mismatch and data encoding differences, there are further hurdles to integrating QPUs with CPUs and GPUs. These include the cryogenic conditions required for operating many types of quantum hardware, to reduce the effects of environmental noise, and the timescales for measurements to read out the qubit state and convert into classical data signals.  Table \ref{tab:hardware_clock_speed} lists typical measurement times for different hardware, which are often much longer than typical gate speeds.
These factors all need to be taken into account when designing algorithms, and significantly affect the wall-clock time required.

Nonetheless, there are strong indications that for specific problems, quantum hardware can perform faster, even if it cannot yet solve something useful we cannot do with a classical computer. Solving problems on D-Wave systems take around one second in wall clock time, and the short-time coherent dynamics \cite{King2022} are costly to simulate classically.  Ion traps and Rydberg arrays already contain more qubits than we can simulate fully classically, and the classical approximate simulations we can do are slower than real time.

The most important next step for integrating quantum computers with HPC is making test bed hardware widely available. Demonstrations of their combined capabilities can guide the engineering required to integrate them more efficiently.


% ===========================================================
% ===========================================================

\section{Outlook}\label{sec:outlook}

\begin{figure*}[ht!]
\centering
\includegraphics[width=\linewidth]{qubits-v3}
\caption{Growth in number of qubits per device from 1998 to 2024. Showing SC circuits (red $\times$), trapped ions (green $+$), cold atoms (blue stars), NMR (orange squares) and silicon/spin (pink diamonds) platforms. Results from selected teams annotated with team name and circled in grey. Compiled by R. Au-Yeung from Statista \cite{Statista} and press releases.}
\label{fig:qubits-progress}
\end{figure*}

Looking back over the past two decades, it is already possible to identify a quantum equivalent of Moore's Law through the steady improvements in the qubit number and quality (Figure \ref{fig:qubits-progress}). This is most evident in platforms that have been under development for the longest, such as superconducting qubits and ion traps. Early quantum computers are available now through cloud computing services, such as Amazon Braket, enabling researchers to develop and test of proof-of-concept algorithms. Cloud accessible quantum computing was pioneered by IBM, who released their first cloud accessible five-qubit superconducting quantum computer in 2016 \cite{Mandelbaum2021}. IBM recently announced their newest quantum chip, which provides over a thousand physical qubits \cite{Castelvecchi2023}, an impressive development trajectory.  However, further progress requires error correction \cite{Roffe2019} to reduce the accumulated errors that occur during computation, and hence enable long enough computations for useful applications. This will produce high quality qubits, but each of IBM's thousand-qubit chips will contain only a few of these logical qubits, which will then need to be connected through sophisticated quantum network interfaces.

Ion trap based systems have made similar progress, prioritising qubit quality to achieve the highest fidelity quantum gates of any current platform.  IonQ's latest systems have up to 32 qubits, with 64 qubit systems due in 2025.  This is enough qubits for simulating quantum systems beyond the reach of classical HPC. Again, significantly larger systems will require error correction. Newer entrants, such as Rydberg \cite{Cong2022} and neutral atoms \cite{Wintersperger2023}, are catching up fast in performance and flexibility. Despite the current leading position of superconducting qubits and ion traps, it is by no means clear they will turn out to be the main type of quantum hardware in the longer term.  Photonic, and silicon or other semiconductor-based systems are also firmly in the running. An ecosystem that supports several types of hardware for different applications is both plausible and promising. Despite the significant engineering challenges, there is growing confidence that the hardware will deliver on the anticipated timescales. Companies are forecasting fully error corrected quantum processors on the scale of hundreds or thousands of qubits by 2030, and are investing the resources necessary to achieve this.

However, standalone quantum hardware will support only niche applications.  Some of these are significant: simulating many-body quantum systems (see section \ref{sec:qsim}) is a natural problem for quantum hardware that can potentially deliver many new scientific results with valuable impact in a wide range of commercial sectors.  Nonetheless, to leverage the full potential of quantum computing, interfacing with classical HPC is essential, and already underway \cite{HPCQS2023}. As discussed in section \ref{sec:hpc}, there are major challenges here that require new science and engineering to be developed.  The most important are related to the different data encodings that are natural for quantum computers, making it highly non-trivial to transfer data between quantum and classical processors.  This is compounded by clock speed mismatches that, depending on the type of quantum hardware, can be up to six orders of magnitude.  While data conversion and transfer are largely an engineering challenge, we do not yet have good programming models for how to handle asynchronous computing on these timescales. Commercial promises of seamless software integration are plausible for test bed applications, but do not yet have an assured path for scaling up.

In addition, quantum computing is being developed in a context of significant changes in classical computing.  Physical limits have been reached for scaling up conventional CMOS CPUs: the energy used by a typical large data centre or HPC facility is equivalent to a small town. More compute power requires new hardware that is less power-hungry for the same amount of computation.  GPUs have provided this for the past decade, but they have also shown us that redesigning codes and algorithms to use different types of hardware is extremely challenging and time-consuming for large scale applications \cite{Betcke2022}.  Quantum processors will be significantly more challenging still to deploy alongside classical HPC at scale.  The investment of time and resources necessary to leverage quantum computing is a significant barrier to widespread adoption as quantum computers become more available.  Applications developed and tested in an academic setting are important for opening up areas with commercial potential.

The pace of change in classical computing is also being accelerated by artificial intelligence (AI).  AI is also power hungry. This increases the pressure to develop energy-efficient hardware optimised for the massive data processing required for AI applications like large language models.  Developments in AI will significantly reshape the application areas where quantum computing can deliver real benefits.  While AI may overtake quantum in predicting solutions to scientific computing applications, quantum may in turn provide advantages \cite{Dunjko2018, Cerezo2022} for other types of AI applications, as quantum hardware becomes more capable.

Despite the remaining hurdles on the path to useful quantum computing, this is an exciting time to be doing computational science and engineering. There are multiple potential routes to significantly larger simulations and the scientific and technical breakthroughs they will produce.  Achieving these breakthroughs requires pioneers willing to collaborate across quantum computing and application domains to identify the areas where quantum computers can provide most assistance and develop the algorithms and codes to realise them. 
There are multiple projects aiming to accelerate this process.  Using the UK for examples, we have the
\begin{itemize}
\item National Quantum Computing Centre's SPARQ programme and hackathons \url{https://www.nqcc.ac.uk/engage/sparq-programme/},
\item Quantum Software Lab in Edinburgh \url{https://www.quantumsoftwarelab.com/},
\item Collaborative Computational Project on Quantum Computing \url{https://ccp-qc.ac.uk/}, 
\item ExCALIBUR cross-cutting project QEVEC \url{https://excalibur.ac.uk/projects/qevec/}, 
\item multiple projects funded by the recent Software for Quantum Computing call \url{https://gow.epsrc.ukri.org/NGBOViewPanel.aspx?PanelId=1-FIN46S}, 
\item and the Quantum Technology Hub for Quantum Computing and Simulation \url{https://www.qcshub.org/}, and its successor(s), due to be announced in mid-2024.
\end{itemize}
While many countries do not have their own quantum computing hardware development programmes, 
cloud access to quantum computers enables researchers from any country with internet access to participate in application development.  Initiatives such as QWorld \url{https://qworld.net/} are connecting those interested in an online ecosystem.  This enlarges the overall effort, and ensures that a wide range of relevant problems are tackled.  Amid moves by governments to restrict technology access for national security reasons, continuing cloud access to compute resources is essential to ensure the benefits are available to all.

\vspace{1em}


% ===========================================================
% ===========================================================

\ack
RA, BC, OR, and VK are supported by UK Research and Innovation (UKRI) Grants EP/W00772X/2 (QEVEC). RA, OR, and VK are supported by EP/Y004566/1 (QuANDiE). RA, and VK are supported by EP/T001062/1 (QCS Hub). VK is supported by EP/T026715/2 (CCP-QC). The authors thank John Buckeridge, Nick Chancellor, Steph Foulds and Steve Lind for useful discussions and proofreading the manuscript.


% ===========================================================
% ===========================================================

\addcontentsline{toc}{section}{References}
\input{main.bbl}

% ===========================================================
% ===========================================================

\end{document}