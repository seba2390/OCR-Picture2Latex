\subsection{Problem Abstraction}

\begin{table}[h!]
\caption{Types of network traffic in the LYCOS-IDS dataset and their training and testing sets.}
\label{tab:lycos_train_test_split}
\centering
\begin{tabular}{|lcll|}
\hline
Traffic & Encoding & Train set & Test set \\
\hline
Benign & 0 & 330474 & 110158 \\
Bot & 1 & 550 & 183 \\
DDoS & 2 & 71761 & 23920 \\
DoS Goldeneye & 3 & 5073 & 1691 \\
DoS Hulk & 4 & 119241 & 39747 \\
DoS Slowhttptest & 5 & 3649 & 1216 \\
DoS Slowloris & 6 & 4255 & 1418 \\
FTP Patator & 7 & 3001 & 1000 \\
Heartbleed & 8 & 7 & 2 \\
Portscan & 9 & 119197 & 39732 \\
SSH Patator & 10 & 2218 & 739 \\
Webattack Bruteforce & 11 & 1020 & 340 \\
Webattack Sql Injection & 12 & 9 & 3 \\
Webattack XSS & 13 & 489 & 163 \\
\hline
Total & & 660944 &  220,312 \\
\hline
\end{tabular}
\end{table}

Modern DoS attacks (e.g., UDP, DNS, SYN, NTP) jeopardize networking environments. Existing approaches target DoS detection but struggle with evolving DoS attacks. Addressing this demands advanced classification using fusion, stats, and recent attack data. This tailors defense systems to distinct attack features. Intrusion detection algorithms should evolve beyond old datasets and traditional methods. Dynamic DoS nature mandates a comprehensive approach, moving from detection to classification and effective responses \cite{tian2021lightweight}.

CFA is an emerging approach for combining results from multiple scoring systems. These systems are described by a set of statistical attributes or variables at the data level or by a group of algorithms or models at the computational informatics level. In this paper, we leverage the CFA approach to combine the results from six base ML models, namely Linear Discriminant Analysis (A), Gaussian Naive Bayes (B), Logistic Regression (C), K Nearest Neighbor (D), Decision Trees (E), and Random Forest (F). The CFA approach is a process of ensemble reinforcement learning, where all possible rank and score combinations are considered to find the optimal combination of the candidate models. We apply this approach to address the problem of DoS detection. To determine the most effective model fusion performance, our methodology utilizes the emerging CFA approaches, which involve considering both the score and rank functions for each model (i.e., scoring system) and applying metrics such as average score combination, average rank combination, and weighted combination that factor in the diversity strength or the performance of each scoring system. Last, we employ a 2-model combination to fuse 6 models, pairing them 2 at a time.


\subsection{Dataset}

We use the LYCOS-IDS2017 dataset created through the LycoSTand flow extractor \cite{rosay2021cic}. These datasets contain five days' worth of network flow entries, each comprising 83 features. In total, there are 1,837,498 entries in the dataset. To prepare the data for analysis, we used a 75\% split for the training set and 25\% for the test set as shown in Table \ref{tab:lycos_train_test_split}. The types of network traffic in the dataset and their training and testing sets are summarized by Table \ref{tab:lycos_train_test_split}. After training the six ML models on the training data, we assessed their performance on unseen data by applying the test dataset to each model. The prediction probabilities for each model across the different classes were collected. By convention, the class with the highest probability was selected as the final prediction for each data entry. These probabilities represent the models' confidence level in their predictions for the individual data items. The probabilities generated by the six scoring systems are considered scores for each individual data entry. This score information formed a new dataset, including the original data items and the six probabilities obtained from the models. The CFA algorithm leveraged this new dataset to derive valuable insights using various CFA approaches discussed later in the paper.


\subsection{Data Pre-processing and Exploratory Analysis (Probabilities and Confidence Scores)}

Probabilities and confidence scores are vital in ML models, especially for classification. They offer insight into prediction likelihood and confidence for each label. Models like logistic regression, SVM with probability outputs, and ensembles like Random Forests and Gradient Boosting provide more than just class labels. They yield probabilities or confidence scores for each class, enhancing classification information. Further, probabilities reflect a model's certainty in class predictions. A 0.8 probability for Class A indicates strong belief, while 0.2 suggests lower confidence. This nuanced information aids decisions, especially in uncertain or multi-class scenarios. Evenly distributed probabilities signal ambiguity, while high single-class probabilities show confidence.

%Besides, these probabilities aid post-processing like thresholding, ranking, and model fusion. 
Moreover, these probabilities assist in post-processing tasks such as thresholding, ranking, and model fusion. Thresholds adjust predictions for precision-recall trade-offs. Ranking classes by probabilities helps select top-k likely classes, important for uncertainty. Combining models' predictions via weighted voting or using probabilities as weights enhances ensemble accuracy and robustness. In this paper, we collect the probabilities associated with each prediction, the highest probability among all the classes for the various data points. Treating these probabilities as scores for each data point, we explore different CFA metrics to combine them. Our goal is to identify the optimal approach for combining these scores across multiple models to develop an effective combined model.

\subsection{Performance Evaluation of Scoring Systems}

By employing conventional ensemble methods, we can combine multiple ML classification models and evaluate their performance using specific metrics. The approaches used here are voting-based ensembles and collective confidence.

In voting-based ensembles, multiple classification models are trained independently on the same dataset. During the prediction phase, each model generates its own set of predictions for a given input, and the final prediction is determined based on a voting mechanism. There are two main types of voting-based ensembles, majority voting (each model in the ensemble casts a vote for the predicted class) and weighted voting (each model's prediction is weighted based on its performance). In collective confidence approach, instead of treating predictions as discrete class labels, we can consider each model's confidence scores. One common approach is to calculate the average probability for each class across all models and select the class with the highest average probability as the final prediction. This approach accounts for the collective confidence of the models. Alternatively, the probabilities can be weighted by the performance of each model, similar to the weighted voting approach, to assign more importance to models with better performance.

Choosing between weighted voting and using probabilities depends on the problem and model traits. Finding the right approach might need trial and error. In our paper, we employ the weighted combination of probabilities/confidence scores as the CFA metric, named weighted combination by performance, for two-model fusion. Model weights, assigned by recall scores, prioritize accurate detection of all attack instances. Lastly, Stacking is a advanced ensemble method where a meta-model learns to combine predictions from base models. Ensemble techniques enhance performance by leveraging model strengths, capturing diverse viewpoints and complex patterns. %However, balance the benefits with trade-offs like computation complexity and overfitting. 
Careful experimentation and validation are essential to find the practical model combination for a classification problem. Next, details are provided for the key components leveraged from CFA, namely, rank score characterization (RSC) for the ML models, diversity between RSC functions (scoring systems), and rank combination vs. score combination.

%\subsection{Rank Scoring Characterization (RSC) for ML Models}

A scoring system $A$ on the data set $D = \{d_1, d_2, ...,d_n\}$, comprising a score function $s_A$ and a derived rank function $r_A$, was proposed in \cite{hsu2002methods}. By sorting the values in descending order in the score function $s_A: D \mapsto \mathbb{R}$, a rank function $r_A: D \mapsto \mathbb{N}$, where $\mathbb{N}=\{1, 2, 3, ...n\}$, is obtained. The RSC function $f_A: \mathbb{N} \mapsto \mathbb{R}$ upon scoring system $A$ is expressed as follows:
\begin{equation}\label{eq:RSC}
f_A(i) = s_A(r_A^{-1}(i)) = (s_A \circ r_A^{-1})(i)
\end{equation}

%\subsection{Diversity between RSC Functions (Scoring Systems)}

\begin{figure}[h!]
	\centering
	\includegraphics[height=6.6cm, width=8.8cm]{figures/RSCFunctiongraph.png}
	\caption{Rank score function graph for the six scoring systems. The area between any two RSC functions represents their diversity.}
	\label{fig:rsc_div_plot}
\end{figure}

Cognitive Diversity (CD) between the six systems (${A,B,C,D,E,F}$) is defined as the difference between the RSC functions of these systems \cite{hsu2019cognitive}. We explore the diversity of the RSC functions for each pair of these six scoring systems. Precisely, we will calculate CD between each pair of the six models. The CD between two scoring systems $A_i$ and $A_j$ is denoted by $CD(A_i,A_j)$, which is based on the RSC function of $A_i$ and $A_j$ that are denoted by $f_{A_i}$ and $f_{A_j}$, respectively. Given the rank $k \in \{1, 2, ..., n\}$, CD is defined as:

\begin{equation}
CD(A_i,A_j) = \sqrt{ \frac{1}{n^2-n} \sum_{k=1}^n{(f_{A_i}(k) - f_{A_j}(k))^2}}
\end{equation}

The diversity strength of scoring system $A$ is defined as the average CD between $A$ and all other systems. Let $D_j = \{ d_1, d_2,...,d_n\} \subseteq D$ be the labels of each cross-validation split for the scores $j \epsilon 1, ..., P$ generated by the systems, we obtain the RSC functions of our six scoring systems as: 
%The score function $s_{kj}(d)$ gives a real number to each $d$ in $D_j$, which is the score given by the model $M_k$ to the label for $j_{th}$ split. Sorting $s_{kj}(d)$ into descending order and assigning ranks to each candidate in $D_j$ based on the sorted scores results in a rank function $r_{kj}(d)$.
\begin{enumerate}
    \item The score function, $s_{kj}(d)$, gives a real number to each $d$ in $D_j$, which is the score given by the model $M_k$ to the label for $j_{th}$ split. Having the scores given by each model for each $d$ in $D_j$ provides the score function. 
    \item Sort $s_{kj}(d)$ into descending order and assigning ranks to each candidate in $D_j$ based on the sorted scores results in a rank function $r_{kj}(d)$. We rank the scores for each model/scoring system to obtain the rank function. 
    \item Compare score functions from multiple scoring systems by applying linear normalization, which is the following transformation from $s_{kj}(d):D \rightarrow R$ to $s^*_{kj}(d):D\rightarrow[0,1]$ where $s^*_{kj}(d)=\frac{s_{kj}(d) - s_{min}}{S_{max}-S_{min}}, d \epsilon D$ and $s_{max}=max \{s_{kj}(d)|d \epsilon D\}$ \\ and $s_{min}=min \{s_{kj}(d)|d \epsilon D\}$.
    \item Derive the RSC functions by sorting the normalized scores for each scoring system in descending order, using the rank values as keys (aka \textit{computational derivation of RSC function}).
    \item Plot RSC functions on the same x-y coordinate plane to depict their diversity. The x-axis and the y-axis represent the ranks and normalized scores, respectively (Fig. \ref{fig:rsc_div_plot}).
\end{enumerate}
%To improve results in combining multiple scoring systems, they must have high diversity and relatively high performance, as demonstrated by previous research \cite{hsu2010rank, hsu2006combinatorial,li2009combining}.


%\subsection{Rank Combination vs. Score Combination}

Next, we investigate the superiority between rank combination and score combination. The rank combination can outperform the score combination under rigorous constraints/conditions \cite{hsu2010rank}. We explore the conditions for which efficient performance combinations can be obtained in favor of larger CD values between each pair of the six ML models. Here, we integrate the results of $m$ scoring systems, each with its own score function $s_{kj}(d)$ and rank function $r_{kj}(d)$ for data label $j$, where $k$ represents the scoring system index. Such techniques include score combination (SC), rank combination (RC), voting (V), average combination (AC), and weighted combination (WC). We compute SC, RC, AV, and WC based on the following weighting metrics.
\begin{enumerate}
    \item Average Combination (AC): The average score combination and average rank combination are computed as $s_s(d) = \sum_{i=1}^m [w_is_{ij}(d)]$, and $s_R(d) = \sum_{i=1}^m [w_ir_{ij}(d)]$ where $w_i = \frac{1}{m}$, and $s_s$ and $s_R$ are the score and rank functions of SC and RC respectively.
    \item Weighted combination by diversity strength (WCDS): Weighted score combination by diversity strength (WSCDS) and weighted rank combination by diversity strength (WRCDS) are the two metrics considered here, and the weights are calculated as follows.
\end{enumerate}

\begin{equation}
    W_i = \frac{\textnormal{weight of model  i}}{\textnormal{sum of weights}}  = \begin{cases}
                    \frac{1}{N}, AC \\
                    \frac{ds(A_i)}{\sum_{i=1}^N ds(A_i)}, WCDS
                    
                \end{cases}
\end{equation}
%\textcolor{red}{Where $N$ is the number of scoring systems and $ds(A_i)$ is the diversity strength of scoring system $A_i$. Therefore, we obtain:  
$$WSCDS_{ij}(d) = \frac{(\textnormal{weight of model  i}) * s_{ij}(d)}{\textnormal{sum of weights}}$$
%and 
%$$WRCDS_{ij}(d) = \frac{(\textnormal{weight of model  i}) * r_{ij}(d)}{\textnormal{sum of weights}}$$}
For WRCDS, replace $s_{ij}(d)$ with $r_{ij}(d)$ and $w_i$ with $\frac{1}{w_i}$.