\begin{comment}

\begin{table*}[!ht]
\centering
\caption{Average score combination (ASC) and average rank combination (ARC). The flows represent 10 out of a total of 4187 top-ranked flows based on both ASC and ARC metrics.}
\label{tab:tbl_average_score_combination}
\begin{tabularx}{\textwidth}{|XXXXXXXXXc|}
\hline
$D_i$ & A & B & C  & D  & E  & F & SC/RC & ASC/ARC & Ranking \\
\hline
$d_{27}$  & 1 & 1 & 1 & 1 & 1 & 1 & 6 & 1 & 1 \\
$d_{100}$ & 1 & 1 & 1 & 1 & 1 & 1 & 6 & 1 & 1 \\
$d_{252}$ & 1 & 1 & 1 & 1 & 1 & 1 & 6 & 1 & 1 \\
$d_{274}$ & 1 & 1 & 1 & 1 & 1 & 1 & 6 & 1 & 1 \\
$d_{292}$ & 1 & 1 & 1 & 1 & 1 & 1 & 6 & 1 & 1 \\
$d_{360}$ & 1 & 1 & 1 & 1 & 1 & 1 & 6 & 1 & 1 \\
$d_{444}$ & 1 & 1 & 1 & 1 & 1 & 1 & 6 & 1 & 1 \\
$d_{625}$ & 1 & 1 & 1 & 1 & 1 & 1 & 6 & 1 & 1 \\
$d_{651}$ & 1 & 1 & 1 & 1 & 1 & 1 & 6 & 1 & 1 \\
$d_{654}$ & 1 & 1 & 1 & 1 & 1 & 1 & 6 & 1 & 1 \\
\hline
\end{tabularx}
\end{table*}

\end{comment}

\begin{comment}

\begin{table*}[!ht]
\caption{Weighted score combination (WSC) by diversity strength and weighted rank combination (WRC) by diversity strength. The flows represent the first 10 out of a total of 4187 top-ranked flows. Diversity Strength (DS) of $A$: 82.5656, DS of $B$: 76.1515, DS of $C$: 307.3784, DS of $D$:76.039, DS of $E$: 78.1404, and DS of $F$: 77.6949.}
\label{tab:tbl_weighted_score_combination}
\begin{tabularx}{\textwidth}{|XXXXXXXXXc|}
\hline
$D_i$ & A & B & C  & D  & E  & F & WS/WR & WSC/WRC & Ranking \\ 
\hline
$d_{27}$  & 1 & 1 & 1 & 1 & 1 & 1 & 698.23969 & 1 & 1 \\
$d_{100}$ & 1 & 1 & 1 & 1 & 1 & 1 & 698.23969 & 1 & 1 \\
$d_{252}$ & 1 & 1 & 1 & 1 & 1 & 1 & 698.23969 & 1 & 1 \\
$d_{274}$ & 1 & 1 & 1 & 1 & 1 & 1 & 698.23969 & 1 & 1 \\
$d_{292}$ & 1 & 1 & 1 & 1 & 1 & 1 & 698.23969 & 1 & 1 \\
$d_{360}$ & 1 & 1 & 1 & 1 & 1 & 1 & 698.23969 & 1 & 1 \\
$d_{444}$ & 1 & 1 & 1 & 1 & 1 & 1 & 698.23969 & 1 & 1 \\
$d_{625}$ & 1 & 1 & 1 & 1 & 1 & 1 & 698.23969 & 1 & 1 \\
$d_{651}$ & 1 & 1 & 1 & 1 & 1 & 1 & 698.23969 & 1 & 1 \\
$d_{654}$ & 1 & 1 & 1 & 1 & 1 & 1 & 698.23969 & 1 & 1 \\
\hline
\end{tabularx}
\end{table*}

\end{comment}

\begin{table*}[t]
\caption{Two model combination - Weighted score combinations by performance (recalls) for the data items $d_{1}$ - $d_{10}$.}
\label{tab:wgt_score_comb_by_perf_recall}
\centering
\begin{adjustbox}{width=\textwidth}
\begin{tabular}{|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|}
\hline
$D_i$ & AB & AC & AD & AE & AF & BC & BD & BE & BF & CD & CE & CF & DE & DF & EF \\ 
\hline
$d_{1}$ & 1 & 0.76258 & 1 & 1 & 1 & 0.77052 & 1 & 1 & 1 & 0.77067 & 0.77069 & 0.77077 & 1 & 1 & 1 \\
$d_{2}$ & 1 & 0.99976 & 1 & 1 & 1 & 0.99976 & 1 & 1 & 1 & 0.99976 & 0.99976 & 0.99976 & 1 & 1 & 1 \\
$d_{3}$ & 1 & 0.54627 & 1 & 1 & 1 & 0.56144 & 1 & 1 & 1 & 0.56172 & 0.56175 & 0.56191 & 1 & 1 & 1 \\
$d_{4}$ & 1 & 0.54597 & 1 & 1 & 1 & 0.56115 & 1 & 1 & 1 & 0.56143 & 0.56146 & 0.56162 & 1 & 1 & 1 \\
$d_{5}$ & 0.99996 & 0.55247 & 1 & 1 & 1 & 0.52705 & 0.99996 & 0.99996 & 0.99996 & 0.56771 & 0.56774 & 0.5679 & 1 & 1 & 1 \\
$d_{6}$ & 1 & 0.54641 & 1 & 1 & 1 & 0.56158 & 1 & 1 & 1 & 0.56186 & 0.5619 & 0.56205 & 1 & 1 & 1 \\
$d_{7}$ & 1 & 0.98708 & 1 & 1 & 1 & 0.98635 & 1 & 1 & 1 & 0.98752 & 0.98752 & 0.98753 & 1 & 1 & 1 \\
$d_{8}$ & 1 & 0.98739 & 1 & 1 & 1 & 0.98727 & 1 & 1 & 1 & 0.98741 & 0.98744 & 0.98744 & 1 & 1 & 1 \\
$d_{9}$ & 1 & 0.53821 & 1 & 1 & 1 & 0.53929 & 1 & 1 & 1 & 0.53991 & 0.53991 & 0.53991 & 1 & 1 & 1 \\
$d_{10}$ & 0.76986 & 0.76639 & 0.78936 & 0.78938 & 0.78945 & 0.98505 & 0.99962 & 0.99962 & 0.99962 & 0.99845 & 0.99845 & 0.99845 & 1 & 1 & 1 \\
\hline
\end{tabular}
\end{adjustbox}
\end{table*}

\begin{table*}[t]
\caption{Rankings of the WSCP results presented in Table \ref{tab:wgt_score_comb_by_perf_recall} for the data items $d_{1}$ - $d_{10}$.}
\label{tab:rankings_wgt_comb_by_perf_recall}
\centering
\begin{adjustbox}{width=\textwidth}
\begin{tabular}{|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|}
\hline
$D_i$ & AB & AC & AD & AE & AF & BC & BD & BE & BF & CD & CE & CF & DE & DF & EF \\ 
\hline
$d_{1}$ & 1 & 114936 & 1 & 1 & 1 & 105351 & 1 & 1 & 1 & 115227 & 116010 & 115949 & 1 & 1 & 1 \\
$d_{2}$ & 1 & 16485 & 1 & 1 & 1 & 13230 & 1 & 1 & 1 & 17815 & 17547 & 17270 & 1 & 1 & 1 \\
$d_{3}$ & 1 & 176670 & 1 & 1 & 1 & 157873 & 1 & 1 & 1 & 175125 & 175153 & 175236 & 1 & 1 & 1 \\
$d_{4}$ & 1 & 179070 & 1 & 1 & 1 & 159554 & 1 & 1 & 1 & 177513 & 177561 & 177623 & 1 & 1 & 1 \\
$d_{5}$ & 176598 & 152728 & 1 & 1 & 1 & 207139 & 184970 & 185096 & 184279 & 152473 & 152519 & 15495 & 1 & 1 & 1 \\
$d_{6}$ & 1 & 175530 & 1 & 1 & 1 & 157132 & 1 & 1 & 1 & 174123 & 174147 & 174181 & 1 & 1 & 1 \\
$d_{7}$ & 1 & 37075 & 1 & 1 & 1 & 37065 & 1 & 1 & 1 & 38823 & 39053 & 38739 & 1 & 1 & 1 \\
$d_{8}$ & 1 & 36857 & 1 & 1 & 1 & 36161 & 1 & 1 & 1 & 38925 & 39134 & 38825 & 1 & 1 & 1 \\
$d_{9}$ & 1 & 197884 & 1 & 1 & 1 & 181049 & 1 & 1 & 1 & 198636 & 198655 & 198640 & 1 & 1 & 1 \\
$d_{10}$ & 219277 & 110590 & 218958 & 218912 & 218977 & 38342 & 199530 & 199659 & 198672 & 39536 & 39833 & 39516 & 1 & 1 & 1 \\
\hline
\end{tabular}
\end{adjustbox}
\end{table*}

\begin{table*}[t]
\centering
\caption{Performance evaluation of individual models at the class level of granularity.}
\begin{adjustbox}{width=\textwidth}
\label{tab:model_eval_fusion_tblOne}
\begin{tabular}{|ccc|ccc|ccc|ccc|ccc|ccc|ccc|}
\hline
&&& \multicolumn{3}{c|}{A} & \multicolumn{3}{c|}{B} & \multicolumn{3}{c|}{C} & \multicolumn{3}{c|}{D} & \multicolumn{3}{c|}{E} & \multicolumn{3}{c|}{F} \\
\hline
\textbf{$class$} &&&  Precision & Recall & F1score & Precision & Recall & F1score& Precision & Recall & F1score & Precision & Recall & F1score & Precision & Recall & F1score & Precision & Recall & F1score \\
\hline
$0$ &&& 0.995305 & 0.933387 & 0.963352        &   0.994616 & 0.169366 & 0.289444 & 0.9739 & 0.913225 & 0.942587  & 0.999228 & 0.998502 & 0.998865  & 0.996296 & 0.998638 & 0.997466 & 0.999691 & 0.999328 & 0.99951 \\
$1$ &&&  0.995305 & 0.933387 & 0.963352  &  0.096706 & 0.994536 & 0.176271 & 0 & 0 & 0 & 0.983696 & 0.989071 & 0.986376 & 0.994536 & 0.994536 & 0.994536  & 1 & 1 & 1 \\
$2$ &&&  0.042553 & 0.065574 & 0.051613  &   0.983337 & 0.999164 & 0.991187 & 0.783516 & 0.737625 & 0.759879 & 0.992537 & 0.978554 & 0.985496  & 0.999958 & 1 & 0.999979 & 1 & 1 & 1 \\
$3$ &&&  0.999791 & 0.999833 & 0.999812  &   0.356379 & 0.966292 & 0.520714 & 0.589041 & 0.81372 & 0.683387 & 0.960603 & 0.980485 & 0.970442 & 0.968347 & 0.976937 & 0.972623  & 0.969873 & 0.989947 & 0.979807 \\
$4$ &&&  0.778788 & 0.911886 & 0.840098  &   0.98708 & 0.974564 & 0.980782  & 0.723168 & 0.862027 & 0.786516 & 0.986929 & 0.995396 & 0.991144  & 0.999874 & 0.999723 & 0.999799 & 1 & 0.999824 & 0.999912 \\
$5$ &&&  0.811881 & 0.809211 & 0.810544  &   0.099778 & 0.814967 & 0.17779  & 0.684392 & 0.814967 & 0.743994 & 0.966503 & 0.972862 & 0.969672 & 0.9801 & 0.972039 & 0.976053 & 0.981224 & 0.988487 & 0.984842  \\
$6$ &&&  0.816667 & 0.794781 & 0.805575  &   0.034563 & 0.834274 & 0.066377 & 0.375488 & 0.33921 & 0.356428 & 0.975456 & 0.980959 & 0.9782 & 0.991507 & 0.988011 & 0.989756 & 0.992928 & 0.990127 & 0.991525 \\
$7$ &&&  0.688797 & 0.996 & 0.814391     &   0.996004 & 0.997 & 0.996502  & 0.531576 & 0.968 & 0.686281 & 0.996004 & 0.997 & 0.996502 & 0.999001 & 1 & 0.9995 & 1 & 1 & 1 \\
$8$ &&&  0.333333 & 1 & 0.5              &   1 & 1 & 1  & 0 & 0 & 0 & 1 & 0.5 & 0.666667 & 1 & 1 & 1  & 1 & 1 & 1 \\
$9$ &&&  0.958722 & 0.9926 & 0.975367    &   0.471534 & 0.997257 & 0.64031  & 0.99932 & 0.998339 & 0.998829 & 0.999321 & 0.999799 & 0.99956 & 0.999648 & 0.999924 & 0.999786 & 0.999648 & 0.99995 & 0.999799 \\
$10$ &&& 0.541325 & 0.983762 & 0.698367  &   0.941482 & 0.979702 & 0.960212 & 0.851852 & 0.622463 & 0.719312 & 0.987887 & 0.993234 & 0.990553 & 0.99594 & 0.99594 & 0.99594 & 0.998645 & 0.997294 & 0.997969 \\
$11$ &&& 0.017903 & 0.061765 & 0.027759  &   0.037879 & 0.102941 & 0.05538  & 0 & 0 & 0  & 0.706941 & 0.808824 & 0.754458 & 0.594937 & 0.414706 & 0.488735 & 0.715013 & 0.826471 & 0.766712 \\
$12$ &&& 0.001724 & 0.333333 & 0.003431  &   0.04918 & 1 & 0.09375 & 0 & 0 & 0   & 0 & 0 & 0  & 0 & 0 & 0 & 0 & 0 & 0 \\
$13$ &&& 0.052947 & 0.97546 & 0.100442   &   0.274874 & 1 & 0.431217 & 0 & 0 & 0  & 0.452991 & 0.325153 & 0.378571  & 1 & 0.018405 & 0.036145 & 0.46789 & 0.312883 & 0.375 \\
\hline
\end{tabular}
\end{adjustbox}
\end{table*}

\begin{figure}[!ht]
%\begin{adjustbox}{width=\textwidth}
	\centering
	%\includegraphics[height=6.6cm, width=14cm]{Globecom 2023_ DoS-DDoS/figures/plot_of_model_recalls.png}
    \includegraphics[height=5cm, width=8.8cm]{figures/plotOfModelRecalls.png}
    \caption{Performance of models at the class level of granularity.}
	\label{fig:plot_of_model_recalls_combination}
% \end{adjustbox}
\end{figure}

\begin{figure*}[!ht]
%\begin{adjustbox}{width=\textwidth}
	\centering
	\includegraphics[height=6.6cm, width=14cm]{figures/plotOfModelRecallsCombination.png}
    %\includegraphics[height=5cm, width=8.8cm]{Globecom 2023_ DoS-DDoS/figures/plot_of_model_recalls.png}
    \caption{Recalls of each of the 15 combined models using weighted score combination by performance at the class level of granularity.%\textcolor{red}{where is the weighted rank combination?}
    }
	\label{fig:plot_of_model_recalls}
% \end{adjustbox}
\end{figure*}

%This section showcases the experimental results of the diverse CFA metrics utilized. Also, we provide a performance evaluation of the individual models and their two model combinations. The noteworthy fusion results that emerge from combining these models are also discussed.

%We evaluate the performance and efficiency of each model at the class level of granularity, with particular attention to their performance in the smaller sampled classes. We emphasize the significance of accurately detecting and classifying each attack, as missing even a single attack can have negative consequences. Thus, our analysis focuses on the recall metric as the most important evaluation criterion. Our approach is to use various CFA metrics to combine multiple models to leverage their strengths and mitigate their weaknesses. %(see Tables \ref{tab:tbl_average_score_combination}, \ref{tab:tbl_weighted_score_combination}, \ref{tab:wgt_score_comb_by_perf_recall} and \ref{tab:rankings_wgt_comb_by_perf_recall}).

We assess model performance and efficiency at the class level, particularly in smaller sampled classes. Precise detection and classification of every attack are crucial, as even one missed attack can be harmful. Thus, our analysis emphasizes recall as a vital criterion. We employ diverse CFA metrics to combine models, harnessing strengths and addressing weaknesses. We calculate average score combination (ASC) and average rank combination (ARC) using the top 10 flows from 220,312, selected based on ASC and ARC metrics. These flows show high confidence in the fusion model's predictions. A small subset of the CFA dataset has all models with the highest probability scores, indicating lack of diversity. This tie results in identical top-ranking positions for both metrics. %, shown in Fig. \ref{fig:rsc_div_plot}. 
Although they yield the same outcome at rank 1, differences arise as rankings progress due to score variations. These metrics demonstrate distinct behavior with diverse scoring systems.

%Table \ref{tab:tbl_average_score_combination} gives 
%The average score combination and average rank combination are calculated, where the flows represent the first 10 out of a total of 4187 top-ranked flows based on both ASC and ARC metrics. These flows are the ones in which the fusion model exhibits the highest confidence level in its prediction. Out of a larger set of 220,312 flows, a subset of 4187 flows received the first position ranking. Further, the CFA dataset exhibits a characteristic where 1.9\% of the data items received the highest probability scores across all models. This means that for those data items, the models are not diverse. Such a lack of diversity at the top performing data items, illustrated by Fig. \ref{fig:rsc_div_plot}, resulted in tie ranking at the top and caused the two metrics above to generate identical results at the top ranking position. Typically, separate tables of results would be generated for each metric. However, in this case, both metrics yielded the same outcomes at rank 1. Nevertheless, as we move down the ranking, the results diverge between the metrics due to variations in the scores assigned by the models. When there is diversity among the scoring systems, these two metrics demonstrate distinct behavior.


%Table \ref{tab:tbl_weighted_score_combination} gives 
The weighted score combination by diversity strength and weighted rank combination by diversity strength are calculated, where the flows represent the first 10 out of a total of 4187 top-ranked flows. These flows are the ones in which the fusion model exhibits the highest confidence level in its prediction. Due to a lack of model diversity at the top-performing data items, the two above metrics produced the same output at rank position 1. However, the results vary as we move down the ranks. Table \ref{tab:wgt_score_comb_by_perf_recall} shows the two-model combination, weighted score combinations by performance (recalls) for the data items $d_{1}$ - $d_{10}$. Recall focuses on the model's ability to find all positive instances, measured per each attack class for all models. Last, Table \ref{tab:rankings_wgt_comb_by_perf_recall} summarizes the rankings of the WSCP results presented in the tables mentioned above for the data items $d_{1}$ - $d_{10}$. Building upon these tables, we aim to identify a model that performs well not only on the commonly observed attacks but also on the low-profiled ones.

%Most of the models we evaluated individually demonstrated excellent performance across most attacks. However, they consistently struggled to adequately classify the low-profiled attacks, which posed a significant challenge. Conversely, the few that excelled in accurately identifying the low-profiled attacks did not perform as well as the other types of attacks. This presented an ideal scenario for employing model fusion methods to combine these models in order to leverage their strengths and mitigate their weaknesses. By combining the outputs of the individual models, we aimed to find a robust and comprehensive model that could effectively address both the low-profiled attacks and the other traffic types.

%Our approach is based on a modified version of the soft Voting Classifier as a metric to generate combined predictions from individual models. This is achieved by averaging the probability scores of each model across the different classes for every data item. To ensure an accurate and weighted aggregation, we leveraged the recall values of the models being combined as weights. This metric is the advanced CFA combination technique called weighted combination by performance. The performance we relied upon is the recall of the models being combined, taken at the attack level granularity. Consequently, this allowed us to incorporate the performance of each model for each attack into the fusion process.

While most models individually performed well against many attacks, they faced difficulty with low-profile attacks. Conversely, models excelling at low-profile attacks struggled with other attack types. This prompted us to consider model fusion, leveraging strengths and offsetting weaknesses. Our goal was to create a robust, comprehensive model by combining individual outputs, capable of effectively handling low-profile attacks and other traffic. Our method modifies the soft Voting Classifier as a metric for generating combined predictions. We average probability scores across classes for each model, using model recall values as weights for accurate aggregation. This advanced CFA technique, called weighted combination by performance, incorporates each model's recall performance at the attack level, enhancing fusion with model-specific attack data.

\begin{comment}

\begin{table*}[t]
\centering
\caption{Performance evaluation - Two model combination using WSCP CFA metric.}
\begin{adjustbox}{width=\textwidth}
\label{tab:model_eval_fusion_tblTwo}
\begin{tabular}{|cc|cc|cc|cc|cc|cc|cc|cc|cc|}
\hline
&& \multicolumn{2}{c|}{AB} & \multicolumn{2}{c|}{AC} & \multicolumn{2}{c|}{AD} & \multicolumn{2}{c|}{AE} & \multicolumn{2}{c|}{AF} & \multicolumn{2}{c|}{BC} & \multicolumn{2}{c|}{BD} & \multicolumn{2}{c|}{BE}\\
\hline
\textbf{$class$} &&  Precision & Recall & Precision & Recall & Precision & Recall & Precision & Recall &  Precision & Recall & Precision & Recall & Precision & Recall &  Precision & Recall\\
\hline
$0$ && 0.996966 & 0.921604 & 0.995185 & 0.936219 &  0.99955 & 0.988489 &  0.998743 & 0.973665 & 0.999715 & 0.987999 &  0.97252 & 0.342154 & 0.999464 & 0.998203 & 0.999618 & 0.997676 \\
$1$ && 0.097274 & 0.994536 &  0.042705 & 0.065574 & 0.994505 & 0.989071 & 0.973404 & 1 & 1 & 1 &  0.096654 & 0.994536 & 0.826484 & 0.989071 & 0.973404 & 1  \\
$2$ && 0.998664 & 0.999833 &  0.999833 & 0.999833 & 0.999749 & 0.999749 & 0.999958 & 0.999958 &  0.999958 & 0.999958 & 0.984105 & 0.99908  & 0.998486 & 0.9926 & 0.999958 & 0.999958 \\
$3$ && 0.496003 & 0.917209 & 0.784254 & 0.907156 &  0.957059 & 0.962153 & 0.966903 & 0.967475 &  0.955399 & 0.962744 &  0.603849 & 0.890597 & 0.960327 & 0.973389 & 0.967532 & 0.969249 \\
$4$ && 0.995899 & 0.977583 &  0.997269 & 0.99215 &  0.99917 & 0.999019 & 0.999522 & 0.999119 &  0.999346 & 0.998843 & 0.96418 & 0.965029 & 0.995263 & 0.998994  & 0.999899 & 0.999748  \\
$5$ && 0.598648 & 0.800987 &  0.811881 & 0.809211 & 0.971524 & 0.953947 & 0.978369 & 0.967105 &  0.976112 & 0.974507 & 0.273596 & 0.777138 & 0.965602 & 0.969572 & 0.972995 & 0.977796  \\
$6$ && 0.241209 & 0.827221 & 0.818446 & 0.794781 &  0.978918 & 0.98237 & 0.985945 & 0.989422 &  0.99012 & 0.989422 & 0.040218 & 0.641044 & 0.968553 & 0.977433 & 0.981092 & 0.988011  \\
$7$ && 0.934396 & 0.997 &  0.726477 & 0.996 & 0.997998 & 0.997 &  1 & 1 & 0.99403 & 0.999 &  0.996004 & 0.997 & 0.997998 & 0.997 & 1 & 1 \\
$8$ && 0.333333 & 1 &  0.333333 & 1 &  0.333333 & 1 & 1 & 1 & 0.666667 & 1 &  1 & 1 & 1 & 1 & 0.666667 & 1  \\
$9$ && 0.967115 & 0.99406 &  0.960614 & 0.9926 &  0.998743 & 0.999648 &  0.999344 & 0.997231 & 0.998994 & 0.999874 &  0.4728 & 0.997257 & 0.999245 & 0.998767 & 0.999346 & 0.999924  \\
$10$ && 0.552672 & 0.979702 & 0.550758 & 0.983762 &  0.983871 & 0.990528 & 0.995935 & 0.994587 &  0.979947 & 0.991881 &  0.911392 & 0.97429 & 0.986559 & 0.993234  & 0.991903 & 0.994587\\
$11$ && 0.143836 & 0.061765 & 0.019535 & 0.061765 &  0.751852 & 0.597059 & 0.75 & 0.405882 & 0.736301 & 0.632353 & 0.042527 & 0.102941 & 0.869565 & 0.294118 & 0.772727 & 0.1\\
$12$ && 0.009346 & 0.333333 & 0.001792 & 0.333333 & 0.001876 & 0.333333 & 0.001876 & 0.333333 & 0.001876 & 0.333333 &  0.04918 & 1  & 0.04918 & 1 & 0.04918 & 1 \\
$13$ && 0.294224 & 1 & 0.052877 & 0.97546 & 0.111982 & 0.613497 & 0.047762 & 0.779141 & 0.092275 & 0.527607 &  0.283478 & 1  & 0.380952 & 0.932515 & 0.285464 & 1\\
\hline
\end{tabular}
\end{adjustbox}
\end{table*}

\end{comment}

\begin{comment}

\begin{table*}[t]
\centering
\caption{Performance evaluation - Two model combination using WSCP CFA metric (continuation of Table \ref{tab:model_eval_fusion_tblTwo}).}
\begin{adjustbox}{width=\textwidth}
\label{tab:model_eval_fusion_tblThree}
\begin{tabular}{|cc|cc|cc|cc|cc|cc|cc|cc|cc|}
\hline
&& \multicolumn{2}{c|}{BF} & \multicolumn{2}{c|}{CD} & \multicolumn{2}{c|}{CE} & \multicolumn{2}{c|}{CF} & \multicolumn{2}{c|}{DE} & \multicolumn{2}{c|}{DF} & \multicolumn{2}{c|}{EF} & \multicolumn{2}{c|}{BECE}\\
\hline
\textbf{$class$} &&  Precision & Recall & Precision & Recall & Precision & Recall & Precision & Recall &  Precision & Recall & Precision & Recall & Precision & Recall & Precision & Recall\\
\hline
$0$ && 0.999846 & 0.99892 & 0.998683 & 0.998475  & 0.999183 & 0.998793 & 0.999129 & 0.999201 &  0.999609 & 0.99902  & 0.999664 & 0.999092 & 0.999591 & 0.999165 & 0.999664        & 0.998393\\
$1$ && 1 & 0.994536 & 0.973118 & 0.989071  & 0.963158 & 1  & 1        & 0.994536 &  0.97861  & 1        & 1        & 1 & 0.983871        & 1  & 0.973404        & 1 \\
$2$ && 1 & 1 & 0.99664 & 0.967224  & 0.999916 & 1  & 0.999875 & 1        &  1        & 1        & 0.999875 & 0.999958  & 1 & 1 & 0.999916        & 1 \\
$3$ && 0.95986 & 0.975754 & 0.959906 & 0.962744  & 0.966279 & 0.98285  & 0.966062 & 0.976345 &  0.968134 & 0.988173 & 0.966532 & 0.990538 & 0.972044 & 0.98699 & 0.969376        & 0.973389\\
$4$ && 1 & 0.999522 &  0.979933 & 0.99761  & 0.999874 & 0.999774  & 0.999874 & 0.999723 &  0.999899 & 0.999774 & 0.999925 & 0.999799 & 0.999899 & 0.999744 & 0.999899        & 0.999748\\
$5$ && 0.974756 & 0.984375 & 0.96896 & 0.949836  & 0.974832 & 0.955592  & 0.979184 & 0.967105 &  0.972335 & 0.98273  & 0.971614 & 0.985197 & 0.974735 & 0.983533 & 0.973039        & 0.979441 \\
$6$ && 0.989407 & 0.988011 & 0.973464 & 0.983075  & 0.98662  & 0.988011  & 0.993631 & 0.990127 &  0.987306 & 0.987306 & 0.991489 & 0.985896 & 0.989422 & 0.989422 & 0.98317         & 0.988717 \\
$7$ && 1 & 0.999 & 0.997 & 0.997  & 1        & 1  & 1        & 1        &  1        & 1        & 1        & 0.999 & 1        & 1  & 1               & 1  \\
$8$ && 1 & 1 &  1 & 1  & 0.666667 & 1 & 1        & 1        &  0.666667 & 1        & 1        & 1   & 1        & 1   & 0.666667        & 1   \\
$9$ && 0.999421 & 0.99995 & 0.999472 & 0.999673  & 0.999497 & 0.999849 & 0.999673 & 0.99995  & 0.999472 & 0.999924 & 0.999522 & 0.999924 & 0.999597 & 0.999924 & 0.999472        & 0.999924 \\
$10$ &&  0.990528 & 0.990528 & 0.985195 & 0.990528  & 0.993252 & 0.99594  & 0.998645 & 0.997294 & 0.997286 & 0.994587 & 0.990566 & 0.994587 & 0.998643 & 0.99594 & 0.997286        & 0.994587 \\
$11$ && 0.859155 & 0.358824 & 0.700767 & 0.805882  & 0.659229 & 0.955882  & 0.716495 & 0.817647 &  0.70437  & 0.805882 & 0.713198 & 0.826471 & 0.708955 & 0.838235 & 0.710145        & 0.144118\\
$12$ &&  0.04918 & 1 & 0 & 0  & 0        & 0  & 0      & 0        &  0        & 0        & 0      & 0 & 0.666667     & 0.666667  & 0.04918         & 1      \\
$13$ && 0.389646 & 0.877301 & 0.452991 & 0.325153  & 0.44     & 0.067485  & 0.462264 & 0.300613 &  0.444444 & 0.319018 & 0.46789  & 0.312883 & 0.470588  & 0.294479 & 0.328421        & 0.957055 \\
\hline
\end{tabular}
\end{adjustbox}
\end{table*}

\end{comment}

By assigning higher weights to models with higher recall rates for a given attack, we aimed to prioritize the models that were more adept at correctly identifying all attacks, including the low-profiled ones. This CFA strategy proved to be highly effective in enhancing the overall performance of our methodology. By incorporating the strengths of multiple models, we established a balanced and robust classification system that improved recall and reliability. Consequently, we successfully leveraged the individual strengths of each model, resulting in a more comprehensive and successful approach to attack identification and classification.

Table \ref{tab:model_eval_fusion_tblOne} highlights the performance of individual models, with Model $F$ emerging as the highest performer overall. However, it exhibited significant weaknesses in accurately classifying the low-profile attack class 12. On the other hand, Model $B$ demonstrated a flawless performance in identifying low-profiled attacks. Nonetheless, it fell short of effectively classifying other types of attacks compared to other models. Figures \ref{fig:plot_of_model_recalls_combination} and \ref{fig:plot_of_model_recalls} visually depict the performance of the six individual models and their two-model combinations across various classes, allowing for a comparison of their respective performances. A best performing models for each attack can be found in Table \ref{tab:best_performing_models}.

%Moreover, Tables \ref{tab:model_eval_fusion_tblTwo} and \ref{tab:model_eval_fusion_tblThree} present the performance evaluation of the combined models. 

Among the various models, Model $BE$ consistently outperformed the others, exhibiting the highest recall rates for both low-profiled attacks and other traffic categories. This indicates that Model $BE$ excels in accurately identifying and classifying attacks, particularly in cases where limited training data is available. However, it should be noted that Model $BE$ obtained a recall of only 0.1 for attack class 11 which is a potential weakness. Model $CE$ demonstrated the highest recall of 0.9555882 for attack class 11, surpassing all other models. Motivated by this observation, we combined Models $BE$ and $CE$ to explore the possibility of achieving improved recalls across all attack classes. However, the fusion of these models, Model $BECE$, did not significantly increase the recall for Class 11. 

Table \ref{tab:best_performing_models} presents a summary of the performance for individual and combined models. Notably, Model $DF$ achieved a remarkable 100\% recall for the attack class 4, surpassing the performance of the best individual model in that category. This highlights the effectiveness of the combined model approach in achieving higher recall rates. Based on the CFA metric employed, our recommended fusion model would be to use Model $DF$ for attack class 4 and then Model $BE$ for all attack classes except class 11, for which Model $CE$ is employed. This combination would yield better overall performance in terms of recall and outperform the individual models. %To get the exact top performing models for each category, refer to Table \ref{tab:best_performing_models}. We encourage further exploration of alternative fusion metrics to assess if other models with superior performance can be obtained.

Last, while Model $BE$ and Model $CE$ successfully enhanced the recall rates for all attack classes, developing more advanced fusion techniques may yield even better results. Overall, our approach of combining models proved successful in achieving our goal of developing a model that performs well, particularly in recalling low-profiled attacks.

%\section{Discussions} \label{sec:discussion}

%We evaluated the models by computing metrics based upon multiple multi-class confusion matrices, which capture the performance of each model across different classes in our test dataset as shown in Fig. \ref{tab:lycos_train_test_split}. To gain an in-depth understanding of their performance, we calculated precision, recall, and F1-score for individual models at the class level of granularity. Further, we explored the combination of the models in search of an improved model with better performance, particularly in detecting low-profile attacks. The flows represented in Table \ref{tab:tbl_average_score_combination} are the first 10 out of 4187 top-ranked flows based on ASC and ARC metrics. These flows are the ones in which the fusion model exhibits the highest confidence level in its prediction. Out of a larger set of 220,312 flows, a subset of 4187 flows received the first position ranking. Similarly, the results in Table \ref{tab:tbl_weighted_score_combination} represent the first 10 out of 4187 top-ranked flows based on WSCDS and WRCDS CFA metrics.


%The CFA dataset exhibits a characteristic where 1.9\% of the data items received the highest probability scores across all models. This means that for those data items, the models are not diverse. This lack of diversity at the top performing data items, illustrated by Fig. \ref{fig:rsc_div_plot}, resulted in tie ranking at the top and caused the metrics ASC and ARC, as well as WSCDS and WRCDS, to generate identical results at the top ranking position. Typically, separate tables of results would be generated for each metric. However, in this case, both metrics yielded the same outcomes at rank number 1, which is why we merged them. Nevertheless, as we move down the ranking, the results diverge between the metrics due to variations in the scores assigned by the models. When there is diversity among the scoring systems, these two metrics demonstrate distinct behavior. To establish a connection between the results presented in Tables \ref{tab:wgt_score_comb_by_perf_recall} and \ref{tab:rankings_wgt_comb_by_perf_recall} and the performance evaluation outcomes shown in Tables \ref{tab:model_eval_fusion_tblTwo} and \ref{tab:model_eval_fusion_tblThree}, we employed the same CFA metric. 

%Last, Tables \ref{tab:wgt_score_comb_by_perf_recall} and \ref{tab:rankings_wgt_comb_by_perf_recall} employed the Weighted Score Combination by Performance (WSCP) metric to combine the maximum scores provided by each scoring system for their respective predictions across all data items. Also, the evaluation results in Tables \ref{tab:model_eval_fusion_tblTwo} and \ref{tab:model_eval_fusion_tblThree} utilized the same metric to combine the probabilities/confidence scores across the various classes provided by the models being combined for each data item. This approach yielded a total confidence value for each class in the combined model, with the class having the highest probability being selected as the prediction for the combined model. The results of the evaluation are summarized in the Table \ref{tab:best_performing_models}.


%\begin{comment}

\begin{table}[h!]
\caption{Best performing models for each class (by recalls). Multiple models for a given class represents a tie.}
\label{tab:best_performing_models}
\centering
\begin{tabular}{|l|l|l|}
\hline
Traffic & Individual & Combined using WSCP \\
\hline
0 & F (99.93) & CF (99.92) \\ \hline
1 & F (100) & AE, AF, BE, CE, DE, DF, EF, BECE \\ & & (100) \\ \hline
2 & F (100) &BF, CE, CF, DE, BECE, EF (100 )\\ \hline
3 & A (99.98) & DF (99.05) \\\hline
4 & F (99.98) & DF (100) \\ \hline
5 & F (98.85) & DF (98.52) \\ \hline
6 & F (99.01) & CF (99.01) \\ \hline
7 & E, F (100) & AE, BE, CE, CF, DE, BECE, EF (100) \\ \hline
8 & A, B, E, F & AB, AC, AD, AE, AF, BC, EF, BD (100) \\ 
  &   &  BE, BF, CD, CE, CF, DE, DF, EF, BECE  \\ \hline
9 & F (100) & BF, CF (100) \\ \hline
10 & F (99.73) & CF (99.73) \\ \hline
11 & F (82.65) & CE (95.59) \\ \hline
12 & B (100) & BF, BECE, BC, BD, BE (100)\\ \hline
13 & B (100) & AB, BC, BE (100) \\ \hline
\end{tabular}
\end{table}

%\end{comment}


\begin{comment}
\begin{table*}[!ht]
\centering
\caption{Average score combination (ASC) and Average rank combination (ARC). The flows represent 10 out of a total of 4187 top-ranked flows based on both ASC and ARC metrics.}
\label{tab:tbl_average_score_combination}
\begin{tabularx}{\textwidth}{|XXXXXXXXXc|}
\hline
$D_i$ & A & B & C  & D  & E  & F & SC/RC & ASC/ARC & Ranking \\
\hline
$d_{27}$  & 1 & 1 & 1 & 1 & 1 & 1 & 6 & 1 & 1 \\
$d_{100}$ & 1 & 1 & 1 & 1 & 1 & 1 & 6 & 1 & 1 \\
$d_{252}$ & 1 & 1 & 1 & 1 & 1 & 1 & 6 & 1 & 1 \\
$d_{274}$ & 1 & 1 & 1 & 1 & 1 & 1 & 6 & 1 & 1 \\
$d_{292}$ & 1 & 1 & 1 & 1 & 1 & 1 & 6 & 1 & 1 \\
$d_{360}$ & 1 & 1 & 1 & 1 & 1 & 1 & 6 & 1 & 1 \\
$d_{444}$ & 1 & 1 & 1 & 1 & 1 & 1 & 6 & 1 & 1 \\
$d_{625}$ & 1 & 1 & 1 & 1 & 1 & 1 & 6 & 1 & 1 \\
$d_{651}$ & 1 & 1 & 1 & 1 & 1 & 1 & 6 & 1 & 1 \\
$d_{654}$ & 1 & 1 & 1 & 1 & 1 & 1 & 6 & 1 & 1 \\
\hline
\end{tabularx}
\end{table*}
\end{comment}

\begin{comment}
\begin{table*}[!ht]
\caption{Weighted score (WS) combination by diversity strength and Weighted rank (WS) combination by diversity strength. The flows represent the first 10 out of a total of 4187 top-ranked flows. DS of $A$: 82.5656, DS of $B$: 76.1515, DS of $C$: 307.3784, DS of $D$:76.039, DS of $E$: 78.1404, and DS of $F$: 77.6949.}
\label{tab:tbl_weighted_score_combination}
\begin{tabularx}{\textwidth}{|XXXXXXXXXc|}
\hline
$D_i$ & A & B & C  & D  & E  & F & WS & WSC & Ranking \\ 
\hline
$d_{27}$  & 1 & 1 & 1 & 1 & 1 & 1 & 698.23969 & 1 & 1 \\
$d_{100}$ & 1 & 1 & 1 & 1 & 1 & 1 & 698.23969 & 1 & 1 \\
$d_{252}$ & 1 & 1 & 1 & 1 & 1 & 1 & 698.23969 & 1 & 1 \\
$d_{274}$ & 1 & 1 & 1 & 1 & 1 & 1 & 698.23969 & 1 & 1 \\
$d_{292}$ & 1 & 1 & 1 & 1 & 1 & 1 & 698.23969 & 1 & 1 \\
$d_{360}$ & 1 & 1 & 1 & 1 & 1 & 1 & 698.23969 & 1 & 1 \\
$d_{444}$ & 1 & 1 & 1 & 1 & 1 & 1 & 698.23969 & 1 & 1 \\
$d_{625}$ & 1 & 1 & 1 & 1 & 1 & 1 & 698.23969 & 1 & 1 \\
$d_{651}$ & 1 & 1 & 1 & 1 & 1 & 1 & 698.23969 & 1 & 1 \\
$d_{654}$ & 1 & 1 & 1 & 1 & 1 & 1 & 698.23969 & 1 & 1 \\
\hline
\end{tabularx}
\end{table*}
\end{comment}


\begin{comment}
\begin{table*}[t]
\caption{Two model combination - Weighted score combinations by performance (recalls) for the data items $d_{1}$ - $d_{10}$.}
\label{tab:wgt_score_comb_by_perf_recall}
\centering
\begin{adjustbox}{width=\textwidth}
\begin{tabular}{|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|}
\hline
$D_i$ & AB & AC & AD & AE & AF & BC & BD & BE & BF & CD & CE & CF & DE & DF & EF \\ 
\hline
$d_{1}$ & 1 & 0.76258 & 1 & 1 & 1 & 0.77052 & 1 & 1 & 1 & 0.77067 & 0.77069 & 0.77077 & 1 & 1 & 1 \\
$d_{2}$ & 1 & 0.99976 & 1 & 1 & 1 & 0.99976 & 1 & 1 & 1 & 0.99976 & 0.99976 & 0.99976 & 1 & 1 & 1 \\
$d_{3}$ & 1 & 0.54627 & 1 & 1 & 1 & 0.56144 & 1 & 1 & 1 & 0.56172 & 0.56175 & 0.56191 & 1 & 1 & 1 \\
$d_{4}$ & 1 & 0.54597 & 1 & 1 & 1 & 0.56115 & 1 & 1 & 1 & 0.56143 & 0.56146 & 0.56162 & 1 & 1 & 1 \\
$d_{5}$ & 0.99996 & 0.55247 & 1 & 1 & 1 & 0.52705 & 0.99996 & 0.99996 & 0.99996 & 0.56771 & 0.56774 & 0.5679 & 1 & 1 & 1 \\
$d_{6}$ & 1 & 0.54641 & 1 & 1 & 1 & 0.56158 & 1 & 1 & 1 & 0.56186 & 0.5619 & 0.56205 & 1 & 1 & 1 \\
$d_{7}$ & 1 & 0.98708 & 1 & 1 & 1 & 0.98635 & 1 & 1 & 1 & 0.98752 & 0.98752 & 0.98753 & 1 & 1 & 1 \\
$d_{8}$ & 1 & 0.98739 & 1 & 1 & 1 & 0.98727 & 1 & 1 & 1 & 0.98741 & 0.98744 & 0.98744 & 1 & 1 & 1 \\
$d_{9}$ & 1 & 0.53821 & 1 & 1 & 1 & 0.53929 & 1 & 1 & 1 & 0.53991 & 0.53991 & 0.53991 & 1 & 1 & 1 \\
$d_{10}$ & 0.76986 & 0.76639 & 0.78936 & 0.78938 & 0.78945 & 0.98505 & 0.99962 & 0.99962 & 0.99962 & 0.99845 & 0.99845 & 0.99845 & 1 & 1 & 1 \\
\hline
\end{tabular}
\end{adjustbox}
\end{table*}
\end{comment}

\begin{comment}
\begin{table*}[t]
\caption{Rankings of the WSCP results presented above for the data items $d_{1}$ - $d_{10}$.}
\label{tab:rankings_wgt_comb_by_perf_recall}
\centering
\begin{adjustbox}{width=\textwidth}
\begin{tabular}{|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|}
\hline
$D_i$ & AB & AC & AD & AE & AF & BC & BD & BE & BF & CD & CE & CF & DE & DF & EF \\ 
\hline
$d_{1}$ & 1 & 114936 & 1 & 1 & 1 & 105351 & 1 & 1 & 1 & 115227 & 116010 & 115949 & 1 & 1 & 1 \\
$d_{2}$ & 1 & 16485 & 1 & 1 & 1 & 13230 & 1 & 1 & 1 & 17815 & 17547 & 17270 & 1 & 1 & 1 \\
$d_{3}$ & 1 & 176670 & 1 & 1 & 1 & 157873 & 1 & 1 & 1 & 175125 & 175153 & 175236 & 1 & 1 & 1 \\
$d_{4}$ & 1 & 179070 & 1 & 1 & 1 & 159554 & 1 & 1 & 1 & 177513 & 177561 & 177623 & 1 & 1 & 1 \\
$d_{5}$ & 176598 & 152728 & 1 & 1 & 1 & 207139 & 184970 & 185096 & 184279 & 152473 & 152519 & 15495 & 1 & 1 & 1 \\
$d_{6}$ & 1 & 175530 & 1 & 1 & 1 & 157132 & 1 & 1 & 1 & 174123 & 174147 & 174181 & 1 & 1 & 1 \\
$d_{7}$ & 1 & 37075 & 1 & 1 & 1 & 37065 & 1 & 1 & 1 & 38823 & 39053 & 38739 & 1 & 1 & 1 \\
$d_{8}$ & 1 & 36857 & 1 & 1 & 1 & 36161 & 1 & 1 & 1 & 38925 & 39134 & 38825 & 1 & 1 & 1 \\
$d_{9}$ & 1 & 197884 & 1 & 1 & 1 & 181049 & 1 & 1 & 1 & 198636 & 198655 & 198640 & 1 & 1 & 1 \\
$d_{10}$ & 219277 & 110590 & 218958 & 218912 & 218977 & 38342 & 199530 & 199659 & 198672 & 39536 & 39833 & 39516 & 1 & 1 & 1 \\
\hline
\end{tabular}
\end{adjustbox}
\end{table*}
\end{comment}

\begin{comment}
\begin{table*}[t]
\centering
\caption{Performance Evaluation of Individual Models at the Class Level of Granularity.}
\begin{adjustbox}{width=\textwidth}
\label{tab:model_eval_fusion_tblOne}
\begin{tabular}{|ccc|ccc|ccc|ccc|ccc|ccc|ccc|}
\hline
&&& \multicolumn{3}{c|}{A} & \multicolumn{3}{c|}{B} & \multicolumn{3}{c|}{C} & \multicolumn{3}{c|}{D} & \multicolumn{3}{c|}{E} & \multicolumn{3}{c|}{F} \\
\hline
\textbf{$class$} &&&  Precision & Recall & F1score & Precision & Recall & F1score& Precision & Recall & F1score & Precision & Recall & F1score & Precision & Recall & F1score & Precision & Recall & F1score \\
\hline
$0$ &&& 0.995305 & 0.933387 & 0.963352        &   0.994616 & 0.169366 & 0.289444 & 0.9739 & 0.913225 & 0.942587  & 0.999228 & 0.998502 & 0.998865  & 0.996296 & 0.998638 & 0.997466 & 0.999691 & 0.999328 & 0.99951 \\
$1$ &&&  0.995305 & 0.933387 & 0.963352  &  0.096706 & 0.994536 & 0.176271 & 0 & 0 & 0 & 0.983696 & 0.989071 & 0.986376 & 0.994536 & 0.994536 & 0.994536  & 1 & 1 & 1 \\
$2$ &&&  0.042553 & 0.065574 & 0.051613  &   0.983337 & 0.999164 & 0.991187 & 0.783516 & 0.737625 & 0.759879 & 0.992537 & 0.978554 & 0.985496  & 0.999958 & 1 & 0.999979 & 1 & 1 & 1 \\
$3$ &&&  0.999791 & 0.999833 & 0.999812  &   0.356379 & 0.966292 & 0.520714 & 0.589041 & 0.81372 & 0.683387 & 0.960603 & 0.980485 & 0.970442 & 0.968347 & 0.976937 & 0.972623  & 0.969873 & 0.989947 & 0.979807 \\
$4$ &&&  0.778788 & 0.911886 & 0.840098  &   0.98708 & 0.974564 & 0.980782  & 0.723168 & 0.862027 & 0.786516 & 0.986929 & 0.995396 & 0.991144  & 0.999874 & 0.999723 & 0.999799 & 1 & 0.999824 & 0.999912 \\
$5$ &&&  0.811881 & 0.809211 & 0.810544  &   0.099778 & 0.814967 & 0.17779  & 0.684392 & 0.814967 & 0.743994 & 0.966503 & 0.972862 & 0.969672 & 0.9801 & 0.972039 & 0.976053 & 0.981224 & 0.988487 & 0.984842  \\
$6$ &&&  0.816667 & 0.794781 & 0.805575  &   0.034563 & 0.834274 & 0.066377 & 0.375488 & 0.33921 & 0.356428 & 0.975456 & 0.980959 & 0.9782 & 0.991507 & 0.988011 & 0.989756 & 0.992928 & 0.990127 & 0.991525 \\
$7$ &&&  0.688797 & 0.996 & 0.814391     &   0.996004 & 0.997 & 0.996502  & 0.531576 & 0.968 & 0.686281 & 0.996004 & 0.997 & 0.996502 & 0.999001 & 1 & 0.9995 & 1 & 1 & 1 \\
$8$ &&&  0.333333 & 1 & 0.5              &   1 & 1 & 1  & 0 & 0 & 0 & 1 & 0.5 & 0.666667 & 1 & 1 & 1  & 1 & 1 & 1 \\
$9$ &&&  0.958722 & 0.9926 & 0.975367    &   0.471534 & 0.997257 & 0.64031  & 0.99932 & 0.998339 & 0.998829 & 0.999321 & 0.999799 & 0.99956 & 0.999648 & 0.999924 & 0.999786 & 0.999648 & 0.99995 & 0.999799 \\
$10$ &&& 0.541325 & 0.983762 & 0.698367  &   0.941482 & 0.979702 & 0.960212 & 0.851852 & 0.622463 & 0.719312 & 0.987887 & 0.993234 & 0.990553 & 0.99594 & 0.99594 & 0.99594 & 0.998645 & 0.997294 & 0.997969 \\
$11$ &&& 0.017903 & 0.061765 & 0.027759  &   0.037879 & 0.102941 & 0.05538  & 0 & 0 & 0  & 0.706941 & 0.808824 & 0.754458 & 0.594937 & 0.414706 & 0.488735 & 0.715013 & 0.826471 & 0.766712 \\
$12$ &&& 0.001724 & 0.333333 & 0.003431  &   0.04918 & 1 & 0.09375 & 0 & 0 & 0   & 0 & 0 & 0  & 0 & 0 & 0 & 0 & 0 & 0 \\
$13$ &&& 0.052947 & 0.97546 & 0.100442   &   0.274874 & 1 & 0.431217 & 0 & 0 & 0  & 0.452991 & 0.325153 & 0.378571  & 1 & 0.018405 & 0.036145 & 0.46789 & 0.312883 & 0.375 \\
\hline
\end{tabular}
\end{adjustbox}
\end{table*}
\end{comment}

\begin{comment}
\begin{figure*}[!ht]
%\begin{adjustbox}{width=\textwidth}
	\centering
	\includegraphics[height=6.6cm, width=14cm]{Globecom 2023_ DoS-DDoS/figures/plot_of_model_recalls.png}
    \caption{Performance of Models at the Class Level of Granularity}
	\label{fig:plot_of_model_recalls}
% \end{adjustbox}
\end{figure*}
\end{comment}



\begin{comment}
\begin{figure*}[!ht]
	\centering
        \caption{ Two model combination - Average Score combinations. Results of data items 1 - 10.}
	\includegraphics[height=6.6cm, width=19.8cm]{Globecom 2023_ DoS-DDoS/figures/Two Model Combination SC.png}
 \label{fig:two_model_scores}
 \\
        \caption{ Two model combination - Ranking of the Average scores presented above for the data items 1 - 10. }
	\includegraphics[height=6.6cm, width=19.8cm]{Globecom 2023_ DoS-DDoS/figures/Two Model Combination RC.png}
 \label{fig:two_model_ranking}
\end{figure*}

\end{comment}

\begin{comment}

\begin{figure*}[!ht]
	\centering
        \caption{ Two model combination - Ranking of the Average scores presented in \ref{fig:two_model_scores} for the data items 1 - 10. }
	\includegraphics[height=6.6cm, width=19.8cm]{Globecom 2023_ DoS-DDoS/figures/Two Model Combination RC.png}
	\label{fig:two_model_ranks}
\end{figure*}
\end{comment}