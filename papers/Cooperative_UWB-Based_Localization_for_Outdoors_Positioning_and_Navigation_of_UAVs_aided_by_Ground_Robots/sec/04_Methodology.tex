%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%                                          %%
%%              METHODOLOGY                 %%
%%                                          %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


% \newpage

% \section{Dynamic Network Topologies at the Edge}
\section{Methodology}

This section describes the simulation settings and robotic platforms utilized in the field experiments.
%including sensors and robot platforms,  and the underpinning theories. %Furthermore, our experiments were carried out both in simulation and real outdoor environment.

\subsection{Simulation environment}

The first tests are carried out in a simulation environment using ROS and Gazebo. We simulate the UWB ranging with a standard deviation of the Gaussian noise set to $\sigma_{UWB} = 10\,cm$. This is a conservative value based on the literature~\cite{queralta2020uwb}. We simulate a single transceiver on the UAV and four transceivers on the ground. The latter ones are set are variable distances simulating deployment in small UGVs (0.6\, separation), large UGVs (1.2\,m separation) and different settings based, e.g., on tripods (with separations at 3\,m, 4\,m, 12\,m and 16\,m).

In the simulation experiments, we perform two types of flights. First, a vertical flight where the UAV is set to follow a straight vertical line up to an altitude of 30\,m. Second, a flight following a square pattern with a fixed size of 8 by 8\,m but at different altitudes (5\,m, 10\,m and 20\,m). For each of these flights, we evaluate the UWB positioning performance with flights based on ground truth positioning. Then, we perform the flight using the UWB position estimation as control input and evaluate how well the UAV follows the predefined trajectory (we refer to this as navigation error).

% In order to get the evaluation of cooperative UWB-base relative positioning system, we implemented a sort of experiments in simulation world. In these experiment, Gazebo simulator is applied to simulate the outdoor environment and UAV with model name called iris. Regarding to the UWB ranging, based on the real outdoor experiments, we concluded that UWB ranging error is near 15cm. Accordingly, we made use of the ground truth provided by Gazebo and add a Gaussian noise with standard deviation of 15cm to be the ranging results of UWB technology. The 4 UWB transceivers are put as a rectangle with the robot in the center which is comparable with our real world experiments.

% In terms of simulation experiments, we set a sort of different settings including different anchor distributions and two kinds of fly patterns. The four anchors are deployed over UGV or tripods with the distribution rectangles of the sizes including $0.6m \times 0.6m$,  $1.2m \times 1.2m$ ,  $3.0 \times 3.0m$ , $4.0m \times 4.0m$,  $12.0m \times 12.0m$ and $12.0m \times 12.0m$. For each of the anchor distributions, we flied the drone up and down from 4m to 30m and the squares of 8m at three different heights such as 5m, 10m and 20m. After that, we  calculated both the positioning and navigation errors.

\begin{figure}
    \centering
    \includegraphics[width=0.49\textwidth]{fig/robots.pdf}
    \caption{UAV and companion ground robot utilized in the experiments.}
    \label{fig:robots}
\end{figure}

\subsection{Multi-robot system}

% \red{Describe in detail the robot and drone we are using}
The multi-robot system employed consists of a single ground robot and a UAV. The ground robot is a ClearPath Husky outdoor platform equipped with four UWB responder transceivers for cooperative positioning and a Livox Avia lidar utilized to obtain ground truth. Owing to the lack of a reference system such as a GNSS-RTK receiver, we extract the UAV position from the lidar's point cloud and utilize this as a reference. The point cloud is automatically processed following the steps described in 
Algorithm~\ref{alg:tracking}, and manually validated. We refer the reader to
~\cite{qingqing2021adaptive}%, where we refer the reader
for further details on this method. Based on indoor testing with a reference anchor-based UWB system, we have evaluated the ground truth accuracy to be in the order of 10\,cm. The UGV and the custom UAV are shown in Fig.~\ref{fig:robots}. The UAV is equipped with two UWB transceivers and an Intel RealSense tracking camera T265 that performs VIO estimation. 


\subsection{Experimental settings}

The field experiments are carried out in Turku, Finland (precise location is 60.4557389\textdegree\,N, 22.2843384\textdegree\,E), between a short line of trees and a large building that presumably blocks and reflects GNSS signals. The UAV runs the PX4 autopilot firmware, which is unable to obtain a stable GNSS lock in the test location. This location is chosen as an example of an urban location where GNSS receivers operate in suboptimal mode.


% Livox Avia lidar can achieve $70.4^{\circ}\times77.2^{\circ}$ field of view for non-repetitive scanning up to 450m detection range with 2cm range precision. Therefore, we assume that this lidar can contribute a novel ground truth for the relative position of drone. Based on the ground truth provided by the lidar, we are able to evaluate the performance of GPS, VIO and UWB positioning in ourdoor environment. 

% \subsection{UWB Ranging}

% % \red{Describe in detail how the UWB works (2 initiators on the drone, 4 responders on the UGV, initiators take turns coordinated from onboard computer)}


% \subsection{Positioning modalities}
% In our experiment, we covered multiple positioning sensors including GPS, VIO and UWB.
% GPS have been widely exploited for positioning in outdoor environment. However, its accuracy is not enough for a variety of scenarios especially GNSS denied ones. As a relatively more precise positioning method, VIO is broadly utilized for robots. In this paper, we evaluated the performance of 
% \red{Describe the different positioning modalities in the experiments: GPS, VIO, UWB }


% \subsection{Autonomous navigation}

% \red{
% Here we explain the different trajectories that we are going to try and why (roughly): straight up and down, circle, and square with common orientation / with changing orientation. And we do these circle and square at different altitudes.}


% \subsection{Ground truth / metrics / quantitative analysis}

% The ground truth of drones' position is extracted from lidar data based on methods introduced in~\cite{qingqing2021adaptive}. Let $\mathcal{P}_k$ be the point cloud generated by the lidar with an frequency $I$, and let $\textbf{s}^k_{MAV}$=\{$\textbf{p}^{k}_{MAV}$,$\dot{\textbf{p}}^{k}_{MAV}$\} be the position and speed of the MAV. We use discrete steps represented by $k$ owing to the discrete nature of the set of consecutive point clouds. The output of the main drone position is to extract from $\mathcal{P}_k$ the set of points representing the MAV, which we denote by $\mathcal{P}^{k}_{MAV}$.This process is outlined in Algorithm~\ref{alg:tracking}.

\input{algs/pos_gd}


