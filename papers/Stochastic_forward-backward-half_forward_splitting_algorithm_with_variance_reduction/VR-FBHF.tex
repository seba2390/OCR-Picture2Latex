\documentclass[a4paper,12pt]{article}

\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage{enumitem}
\usepackage{amsmath}
\usepackage{array}
\usepackage{amssymb}
\usepackage{babel}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{amsthm}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{epstopdf}
\usepackage{amsfonts}
\usepackage{enumerate}
\usepackage{color}
%\usepackage{ntheorem}
%\usepackage[thmmarks,amsmath]{ntheorem}

\usepackage{caption}
\captionsetup{justification=centering}

\setlength{\topmargin}{-1.0cm}
\setlength{\textheight}{24cm}
\setlength{\textwidth}{15cm}
\setlength{\oddsidemargin}{8mm}
\setlength{\evensidemargin}{8mm}

%\usepackage[backend=bibtex8]{biblatex}
%\ExecuteBibliographyOptions{url=false,doi=false,isbn=false,eprint=false}
%\ExecuteBibliographyOptions{clearlang=true,firstinits=true,maxbibnames=99}
%\AtEveryBibitem{\ifentrytype{book}{\clearfield{pages}}{}}
%\renewbibmacro{in:}{}
%\addbibresource{QVI_ALM.bib}

\newtheorem{dfn}{Definition}[section]
\newtheorem{lem}[dfn]{Lemma}
\newtheorem{thm}[dfn]{Theorem}
\newtheorem{cor}[dfn]{Corollary}
\newtheorem{alg}[dfn]{Algorithm}
\newtheorem{theorem}{Theorem}[section]
%\theorembodyfont{\normalfont}
\theoremstyle{definition}

\newtheorem{asm}[dfn]{Assumption}
\newtheorem{exm}[dfn]{Example}
\newtheorem{rem}[dfn]{Remark}
\newtheorem{prop}[dfn]{Proposition}
\newtheorem{con}[dfn]{Condition}

\newcommand{\R}{\mathbb{R}}
\DeclareMathOperator{\diag}{diag}
\DeclareMathOperator*{\zer}{zer}
\DeclareMathOperator*{\Fix}{Fix}
\DeclareMathOperator*{\gra}{gra}
\DeclareMathOperator*{\Id}{Id}
\DeclareMathOperator*{\nt}{int}
\DeclareMathOperator{\dom}{dom}
\DeclareMathOperator{\Pro}{Prob}
\allowdisplaybreaks

\title{Stochastic forward-backward-half forward splitting algorithm with variance reduction}
\author{ \sc \normalsize Liqian Qin{\thanks{ email: qlqmath@163.com}},\,\,
Yaxuan Zhang{\thanks{email: bunnyxuan@tju.edu.cn}}\,\,and\,
Qiao-Li Dong{\thanks{Corresponding author. email: dongql@lsec.cc.ac.cn}}\\
\small College of Science,  Civil Aviation University of China, Tianjin 300300, China,\\
}
\date{}


\begin{document}

	\maketitle

	\begin{abstract}
\noindent
\\
In this paper, we present a stochastic forward-backward-half forward splitting algorithm with variance reduction for solving the structured monotone inclusion problem composed of a maximally monotone operator, a maximally monotone and Lipschitz continuous operator and a cocoercive operator. By defining a Lyapunov function, we establish the almost sure convergence of the proposed algorithm, and obtain the linear convergence when one of the maximally monotone operators is strongly monotone. Numerical examples are provided to show the performance of the proposed algorithm.\\

\noindent {\bf Key words}: Variance reduction; Forward-backward-half forward splitting algorithm;  Monotone inclusion problem; The almost sure convergence; Strongly monotone; Linear convergence.

\end{abstract}
	
	
	
\section{Introduction and preliminaries}
\noindent
In this paper, we consider the structured monotone inclusion problem which is to find $x\in \mathbb{R}^d$  such that
\begin{eqnarray}\label{PROB}
 \ 0\in (A+B+C)(x),
\end{eqnarray}
where  $A: \mathbb{R}^d \rightarrow 2^{\mathbb{R}^d}$ is a maximally monotone operator, $B: \mathbb{R}^d \rightarrow \mathbb{R}^d$ is a maximally monotone and $L_B$-Lipschitz operator, and $C: \mathbb{R}^d \rightarrow \mathbb{R}^d$ is a $\beta$-cocoercive operator. Problem \eqref{PROB} arises in various applications such as optimization problems \cite{FBHF,{Davis}}, variational inequalities \cite{Alacaoglu-Malitsky}, deep learning \cite{Barnet} and image deblurring \cite{Tang}.

Numerous iterative algorithms for solving \eqref{PROB} have been presented and analyzed, see, for instance, \cite{{Comb},{Davis},{Latafat},{Malitsky},{Rie},{Ryu},{Ryuu},{Yu},{Zong},{Tang}} and references therein.  In particular, Brice\~{n}o-Arias et al. \cite{FBHF} first proposed a forward-backward-half forward (FBHF) splitting algorithm as follows
\begin{equation}
\label{FBHF}
\left\{
\begin{array}{lr}
p^{k}=J_{\gamma^{k} A}\left(x^{k}-\gamma^{k}(B+C)x^{k}\right), & \\
x^{k+1}=P_{X}(p^{k}+\gamma^{k}(Bx^{k}-Bp^{k})), & \\
\end{array}
\right.
\end{equation}
where $\gamma^{k}$ is step-size, $\gamma^{k}\in [\eta , \chi-\eta]$, $\eta \in (0,\frac{\chi}{2})$, $\chi=\frac{4 \beta}{1+\sqrt{1+16\beta^2L_B^2}}$, $J_{\gamma^{k} A}=({\rm Id}+\gamma^{k} A)^{-1}$ is the resolvent of $A$, and $X$ is a nonempty closed convex subset of $\mathbb{R}^d$ containing a solution of the problem \eqref{PROB}. They obtained the weak convergence of the method \eqref{FBHF} in the real Hilbert space.


In many cases, monotone inclusion problems have a finite sum structure. For example, finite sum minimization is ubiquitous in machine learning where we minimize the  empirical risk \cite{LJC}, and nonlinear constrained optimization problems \cite{FBHF}. Finite sum saddle-point problems and
finite sum variational inequalities can also be transformed into the monotone inclusion
problems \cite{Zhang}.
Given the effectiveness of variance-reduced algorithms for finite sum function minimization, a natural idea is to use similar algorithms to solve the more general finite sum monotone inclusion problems.

Now, we detail our problem setting. Suppose that the maximally monotone operator $B$ in \eqref{PROB} has a finite sum representation $B= \sum_{i=1}^N B_i$, where each $B_i$ is $L_i$-Lipschitz, $B$ is $L_B$-Lipschitz and it is $L$-Lipschitz in mean. Then the problem \eqref{PROB} can be written in the following form
\begin{equation*}\label{PROB2}
\mbox{Find} \ x \in \mathbb{R}^d  \ \ \mbox{such that} \ 0\in (A+\sum_{i=1}^N B_i+C)(x).
\end{equation*}
It might be the case that $L_i$ are easy to compute, but not $L_B$. In this case, $\sum_{i=1}^N L_i \geq L_B  $ gives us a most natural upper bound on $L_B$. On the other hand, the cost of computing $Bx$ is rather expansive  when $N$ is very large.

Throughout this paper, we assume access to a stochastic oracle $B_{\xi}$ such that  $B_{\xi}$ is unbiased, $B(x)=\mathbb{E}[B_{\xi}(x)]$, and then consider utilizing the stochastic oracle $B_{\xi}$ to perform in the half forward step in the \eqref{FBHF} instead of $B$, which yields lower cost per iteration. The two simplest stochastic oracles can be defined as follows
\begin{itemize}
\item[(i)] Uniform sampling: $B_{\xi}(x)=NB_i(x)$, $P_{\xi}(i)=\Pro \{\xi=i\}=\frac{1}{N}$. In this case, $L=\sqrt{N\sum_{i=1}^N L_i^2}$.
\item[(ii)] Importance sampling: $B_{\xi}(x)=\frac{1}{P_i}B_i(x)$, $ P_{\xi}(i)=\Pro\{\xi=i\}=\frac{L_i}{\sum_{j=1}^N L_j}$. In this case, $L=\sum_{i=1}^N L_i$.
\end{itemize}

Recently, Kovalev et al.\cite{KHR} proposed a loopless variant of SVRG \cite{SVRG}
which removes the outer loop present in SVRG and uses a probabilistic update of the full gradient instead. Later, Alacaoglu et al. \cite{Alacaoglu-Malitsky} proposed the loopless version of extragradient method with variance reduction for solving variational inequalities. They also applied the same idea over the forward-backward-forward (FBF) splitting algorithm which was introduced by Tseng \cite{Tseng} to solve the two operators monotone inclusion problem,
\begin{eqnarray*}
\mbox{find} \ x \in \mathbb{R}^d \ \ \mbox{such that} \ 0\in (A+B)(x),
\end{eqnarray*}
where $A: \mathbb{R}^d \rightarrow 2^{\mathbb{R}^d}$ and $B: \mathbb{R}^d \rightarrow \mathbb{R}^d$ are maximally monotone operators. The operator $B: \mathbb{R}^d \rightarrow \mathbb{R}^d$ has a stochastic oracle $B_{\xi}$ that is unbiased and $\bar{L}$-Lipschitz in mean. They proved the almost sure convergence of the forward-backward-forward splitting algorithm with variance reduction when $B_{\xi}$ is continuous for all $\xi$. However,  the cocoercive operator $C$ is required to admit a finite-sum structure as well, if one extends the forward-backward-forward splitting algorithm with variance reduction to solve problem \eqref{PROB}.

%The forward-backward-forward splitting algorithm with variance reduction is given as follows
%\begin{equation}
%\label{FBF}
%\left\{
%\begin{array}{lr}
%\bar{x}^k=\alpha x^k+(1-\alpha)w^k, &\\
%p^{k}=J_{\gamma A}\left(\bar x^{k}-\gamma Gw^{k}\right), & \\
%\hbox{Draw an index $\xi_k$ according to $Q$}, & \\
%x^{k+1}=p^{k}+\gamma(G_{\xi_k}w^{k}-G_{\xi_k}p^{k}), & \\
%w^{k+1}=
%\begin{cases}
%x^{k+1},&\hbox{with probability}\,\,p,\\
%w^k,&\hbox{with probability}\,\,1-p,\\
%\end{cases}
%\end{array}
%\right.
%\end{equation}
%where $\alpha \in (0,1)$, $\gamma$ is step-size, $Q$ is some probability distribution. They proved the almost sure convergence of the method \eqref{FBF} when $G_{\xi}$ is continuous for all $\xi$.

In this paper, we propose a stochastic forward-backward-half forward splitting algorithm with variance reduction (shortly, VRFBHF).  Under some mild assumptions, we establish the almost sure convergence of the sequence $\{x^k\}_{k \in \mathbb{N}}$ generated by our algorithm. Lyapunov analysis of the proposed algorithm is based on the monotonicity inequalities of $A$ and $B$, and the cocoercivity inequality of $C$. Furthermore, we obtain the linear convergence when $A$ or $B$ is strongly monotone. Numerical experiments are conducted to demonstrate the efficacy of the proposed algorithm.

Following, we recall some definitions and known results which will be helpful for further analysis.

Throughout this paper,  $\mathbb{R}^d$ is a $d$-dimensional Euclidean space with inner product  $\langle \cdot, \cdot\rangle$ and induced norm $\|\cdot\|$. The set of nonnegative integers is denoted by $\mathbb{N}$. Probability
mass function $P_{\xi}(\cdot)$ supported on $\{1, . . . , N\}$.

\begin{dfn}{\rm(\cite[Definition 20.1 and Definition 20.20]{BC2011})}
{\rm
 A set-valued mapping $A: \mathbb{R}^d \rightarrow 2^{\mathbb{R}^d}$ is characterized by its graph $\gra(A)=\{(x,u) \in \mathbb{R}^d\times\mathbb{R}^d:u \in Ax\}.$
A set-valued mapping $A: \mathbb{R}^d \rightarrow 2^{\mathbb{R}^d}$ is said to be\\
{\rm (i)}  monotone if $\langle u-v,x-y \rangle \geq 0$ for all $(x,u),(y,v) \in \gra(A).$\vskip 1mm
\noindent
{\rm (ii)}  maximally monotone if there exists no monotone operator $B: \mathbb{R}^d \rightarrow 2^{\mathbb{R}^d}$ such that gra$(B)$ properly contains gra$(A),$ i.e., for every $(x,u) \in \mathbb{R}^d\times \mathbb{R}^d$,
$$
(x,u) \in \gra (A)  \ \ \Leftrightarrow \ \ \langle u-v , x-y \rangle \geq 0,  \ \  \forall(y,v)\in \gra (A).
$$
}
\end{dfn}

\begin{dfn}
{\rm
An operator  $T: \mathbb{R}^d \rightarrow {\mathbb{R}^d}$ is said to be\vskip 1mm
\noindent
{\rm (i)} $L$-Lipschitz continuous, if there exists a constant $ L > 0$, such that
$$
\|Tx-Ty\|\leq  L\|x-y\|,\quad\forall x, y\in \mathbb{R}^d;
$$\vskip 1mm
\noindent
{\rm (ii)} ${\beta}$-cocoercive, if there exists a constant $ \beta > 0$, such that
$$
\langle Tx-Ty,x-y \rangle \geq  {\beta}\|Tx-Ty\|^2,\quad\forall x, y\in \mathbb{R}^d.
$$\vskip 1mm

By the Cauchy--Schwarz inequality, a ${\beta}$-cocoercive operator is $\frac{1}{\beta}$-Lipschitz continuous.
}
\end{dfn}


\begin{lem}{\rm(\cite[Proposition 20.38]{BC2011})}\label{lem3}
Let $A: \mathcal{Z}\rightarrow 2^{\mathcal{Z}}$ be maximally monotone, where $\mathcal{Z}$ is a finite dimensional Euclidean space. Then $\gra(A)$ is closed in ${\mathcal{Z}}^{\rm strong}\times {\mathcal{Z}}^{\rm strong}$, i.e., for every sequence $(x^{k},u^{k})_{k \in \mathbb{N}}$ in $\gra(A)$ and $(x,u)\in \mathcal{Z}\times \mathcal{Z}$, if $x^{k}\rightarrow x$ and $u^{k}\rightarrow u$, then $(x,u) \in \gra(A)$. \vskip 1mm
\end{lem}

%\begin{lem}\label{Opial}{\rm(\cite[Lemma 2.47]{BC2011})}
%Let $C$ be a nonempty set of $\mathbb{R}^d$, and $\{x^k\}_{k \in \mathbb{N}}$ be a sequence in $\mathbb{R}^d$. If the following conditions hold:\vskip 1mm
%\noindent
%{\rm (i)} for every $x \in C$, $\lim_{k \rightarrow \infty}\|x^k-x\|$ exists;\vskip 1mm
%\noindent
%{\rm (ii)} every weak sequential cluster point of $\{x^k\}_{k\in \mathbb{N}}$ belongs to $C$.\vskip 1mm
%\noindent
%Then the sequence $\{x^k\}_{k \in \mathbb{N}}$ converges weakly to a point in $C$.\vskip 1mm
%\end{lem}

%\begin{lem}{\rm \cite{He}}
%\label{lem1}
%Let $D$ be a closed convex subset of a real Hilbert space $\mathcal{H}$ and let $\{u^k\}_{k \in \mathbb{N}}$ be a bounded simple random vector sequence over $\mathcal{H}$. Let $\{u^k\}_{k \in \mathbb{N}}$ satisfy the properties:
%\begin{itemize}
%		\item[{\rm(i)}] $\lim_{k\rightarrow \infty} \mathbb{E}[\|u^k-u\|^2]$ exists for each $u \in D$;
%		\item[{\rm(ii)}] $\omega_w(\mathbb{E}[u^k]) \subset D$.
%\end{itemize}
%Then $\{\mathbb{E}[u^k]\}_{k \in \mathbb{N}}$ converges weakly to a point in $D$.
%\end{lem}
\begin{lem}\label{Robbins} {\rm(\cite[Theorem 1]{Robbins})}
Let $(\Omega, \mathcal{F}, P)$ be a probability space and $\mathcal{F}_1 \subset \mathcal{F}_2 \subset ...$ be a sequence of sub-$\sigma$-algebras of $\mathcal{F}$. For each $k=1,2,...,$ let $z^k$, $\beta^k$, $\xi^k$, and $\zeta^k$ be non-negative $\mathcal{F}_k$-measurable random variables such that
\begin{equation*}
\mathbb{E}(z^{k+1} | \mathcal{F}_k) \leq (1+\beta^k)z^k+\xi^k-\zeta^k.
\end{equation*}
then $\lim_{k \rightarrow \infty}z^k$ exists and $\sum_{k=1}^{\infty}\zeta^k < \infty$ almost surely on  $\sum_{k=1}^{\infty} \beta^k < \infty$ and $\sum_{k=1}^{\infty} \xi^k < \infty$.
\end{lem}
The paper is organized as follows. In Section 2, we introduce the stochastic forward-backward-half forward splitting algorithm with variance reduction to solve the problem \eqref{PROB}, and show the almost sure and linear convergence of the proposed algorithm. Finally, we present the numerical experiments in Section 3.
\section{Main Results}\label{main}
In the sequel, we assume that the following conditions are satisfied:
\begin{asm} \label{CON1}
	\begin{itemize}
		\item[(i)] The operator $A: \mathbb{R}^d\to 2^{\mathbb{R}^d}$ is maximal monotone;
        \item[(ii)] The operator $B: \mathbb{R}^d\to \mathbb{R}^d$ is single-valued, monotone and $L_B$-Lipschitz;
        \item[(iii)] The operator $B$ has a stochastic oracle $B_\xi$ that is unbiased, $B(x)=\mathbb{E}[B_{\xi}(x)]$, and $L$-Lipschitz in mean:
        \begin{equation}
        \label{lip}
        \mathbb{E}[\|B_{\xi}(u)-B_{\xi}(v)\|^2] \leq L^2\|u-v\|^2, \quad \forall u,v \in \mathbb{R}^d;
        \end{equation}
		\item[(iv)] $C: \mathcal{H}\to \mathcal{H}$ is $\beta$-cocoercive;
		\item[(v)] The solution set  of the problem \eqref{PROB}, denoted by ${\rm zer}(A+B+C)$, is nonempty.
	\end{itemize}
\end{asm}

\noindent
We now present the stochastic forward-backward-half forward splitting algorithm with variance reduction to solve the  problem \eqref{PROB}.

\begin{alg}\label{ALG}
{\rm
\hrule\hrule
\vskip 1mm
\noindent\textbf{VRFBHF}
\vskip 1mm
\hrule\hrule


\vskip 1mm


\noindent
1. {\bf Input:} Probability $p \in (0, 1]$, probability distribution $Q$, step-size $\gamma$, $\lambda \in (0, 1).$

Let $x^0 = w^0$.

\noindent
2. {\bf for} $k = 0, 1, \ldots$ {\bf do}

\noindent
3.\quad  $\bar x^k=\lambda x^k+(1-\lambda)w^k$


\noindent
4.\quad
$
y^k = J_{\gamma A}\left(\bar x^k - \gamma(B+C)w^k\right)
$

\noindent
5.\quad
Draw an index $\xi_k$ according to $Q$

\noindent
6.\quad
$
x^{k+1}=y^k+\gamma(B_{\xi_k}w^k-B_{\xi_k}y^k)
$

\noindent
7.\quad
$
w^{k+1}=
\begin{cases}
x^{k+1},&\hbox{with probability}\,\,p\\
w^k,&\hbox{with probability}\,\,1-p\\
\end{cases}
$

\noindent
8:\quad \textbf{end \ for}
\vskip 1mm
}
\hrule\hrule
\hspace*{\fill}
\end{alg}



\begin{rem}
\rm

Algorithm \ref{ALG} is a very general algorithm and it is brand new to the literature. We review how Algorithm \ref{ALG} relates to previous work. Algorithm \ref{ALG} becomes the forward-backward-forward algorithm with variance reduction in \cite{Alacaoglu-Malitsky} if $C=0$. Algorithm \ref{ALG} reduces to loopless SVRG in \cite{KHR} if $\lambda=1$,
$B=\nabla f$, $A=0$ and $C=0$, where $f(x)=\sum_{i=1}^Nf_i(x)$ and $f_i(x)$ is the loss of model $x$ on data point $i$.

\end{rem}
%Every iteration of Algorithm \ref{ALG} requires two stochastic oracles, $B_\xi$ and one $B$ with probability $1-p$.
\begin{rem}
We have two sources of randomness at each iteraton: the index $\xi_k$ which is used for updating $x^{k+1}$, and the reference point $w^k$ which is updated in each iteration with probability $p$ by the iterate $x^{k+1}$, or left unchanged with probability $1-p$. Intuitively, we wish to keep $p$ small to lower the cost per iteration. And different from the FBHF splitting algorithm \eqref{FBHF}, we use the parameter $\lambda $ to add a step of calculating $\bar x^k=\lambda x^k+(1-\lambda)w^k$. This means that we assign some weight to the past iteration points.
\end{rem}
\subsection{The almost sure convergence}

In this subsection, we establish the almost sure convergence of Algorithm \ref{ALG}. We use the following notations for conditional expectations: $\mathbb{E}_k[\cdot]=\mathbb{E}[\cdot|\sigma(\xi_0,...,\xi_{k-1},w^{k})]$ and $\mathbb{E}_{k+\frac{1}{2}}[\cdot]=\mathbb{E}[\cdot|\sigma(\xi_0,...,\xi_{k},w^{k})]$.

For the iterates $\{x^k\}_{k \in \mathbb{N}}$ and $\{w^k\}_{k \in \mathbb{N}}$ generated by Algorithm \ref{ALG}, we define the Lyapunov function
\begin{equation*}
\Phi_{k}(x):=\lambda \|x^k-x\|^2+\frac{1-\lambda}{p}\|w^k-x\|^2,\ \forall x \in \mathbb{R}^d,
\end{equation*}
 which helps to establish the almost sure convergence of the proposed algorithm.
\begin{theorem}
\label{the1}
{
\noindent
Let Assumption \ref{CON1} hold, $\lambda \in [0,1)$, $p \in (0,1]$, and $\gamma \in (0,\frac{4\beta(1-\lambda)}{1+\sqrt{1+16\beta^2L^2(1-\lambda)}})$. Then for $\{x^k\}_{k \in \mathbb{N}}$ generated by Algorithm \ref{ALG} and any $x^* \in \zer(A+B+C)$, it holds that
\begin{equation}
\label{lypunov}
\mathbb{E}_k[\Phi_{k+1}(x^*)] \leq \Phi_{k}(x^*).
\end{equation}
Then, almost surely there exists $x^* \in \zer(A+B+C)$ such that the sequence $\{x^k\}_{k \in \mathbb{N}}$  generated by Algorithm \ref{ALG} converges to $x^* $.
}
\end{theorem}
\begin{proof}
Since $x^* \in \zer(A+B+C),$ we have
\begin{equation}
-\gamma(B+C)x^* \in \gamma Ax^*. \label{1}
\end{equation}
Step 4 in Algorithm \ref{ALG} is equivalent to the inclusion
\begin{equation}
\bar{x}^k-y^k-\gamma(B+C)w^k \in \gamma Ay^k. \label{2}
\end{equation}
Combining \eqref{1}, \eqref{2} and the monotonicity of $A$, we have
\begin{equation*}
\langle y^k-\bar{x}^k+\gamma(B+C)w^k , x^*-y^k \rangle-\gamma \langle(B+C)x^*, x^*-y^k \rangle \geq 0.
\label{3}
\end{equation*}
Then from step 6 in Algorithm \ref{ALG}, we obtain
\begin{equation}
\langle x^{k+1}-\bar{x}^k+\gamma(Bw^k-B_{\xi_k}w^k+B_{\xi_k}y^k)+\gamma Cw^k , x^*-y^k \rangle-\gamma \langle(B+C)x^*, x^*-y^k \rangle \geq 0.
\label{4}
\end{equation}
By the definition of $\bar{x}^k$ and identities $2\langle a , b \rangle=\|a+b\|^2-\|a\|^2-\|b\|^2=\|a\|^2+\|b\|^2-\|a-b\|^2$, we have
\begin{equation}
\aligned
&2\langle x^{k+1}-\bar{x}^k, x^*-y^k \rangle\\
=& 2\langle x^{k+1}-y^k, x^*-y^k \rangle + 2\langle y^k-\bar{x}^k, x^*-y^k \rangle \\
=& \|x^{k+1}-y^k\|^2+\|x^*-y^k\|^2-\|x^{k+1}-x^*\|^2
 + 2\lambda\langle y^k-x^k , x^*-y^k \rangle \\
 &+ 2(1-\lambda)\langle y^k-w^k , x^*-y^k \rangle \\
=& \|x^{k+1}-y^k\|^2+\|x^*-y^k\|^2-\|x^{k+1}-x^*\|^2\\
&+ \lambda(\|x^k-x^*\|^2-\|y^k-x^k\|^2-\|y^k-x^*\|^2)\\
& +(1-\lambda)(\|w^k-x^*\|^2-\|y^k-w^k\|^2-\|y^k-x^*\|^2)\\
=&\|x^{k+1}-y^k\|^2-\|x^{k+1}-x^*\|^2+ \lambda\|x^k-x^*\|^2-\lambda\|y^k-x^k\|^2 \\
& +(1-\lambda)\|w^k-x^*\|^2-(1-\lambda)\|y^k-w^k\|^2.
\endaligned
\label{5}
\end{equation}
By the $\beta$-cocoercivity of $C$ and Young's inequality $\langle a,b \rangle \leq \beta\|a\|^2 + \frac{1}{4\beta} \|b\|^2$ for all $a , b \in \mathbb{R}^d $,  we get
\begin{equation}
\label{7}
\aligned
&2\gamma \langle Cw^k-Cx^* , x^*-y^k \rangle\\
=&2\gamma \langle Cw^k-Cx^* , x^*-w^k \rangle+2\gamma \langle Cw^k-Cx^* , w^k-y^k \rangle \\
\leq& -2\gamma\beta\|Cw^k-Cx^*\|^2+2\gamma\beta\|Cw^k-Cx^*\|^2+\frac{\gamma}{2\beta}\|w^k-y^k\|^2 \\
=&\frac{\gamma}{2\beta}\|w^k-y^k\|^2.
\endaligned
\end{equation}
We use \eqref{5} and \eqref{7} in \eqref{4} to obtain
\begin{equation}
\label{8-}
\aligned & \ \ \ \ 2\gamma \langle Bx^*-(Bw^k-B_{\xi_k}w^k+B_{\xi_k}y^k) , x^*-y^k \rangle + \|x^{k+1}-x^*\|^2 \\
& \leq \lambda\|x^k-x^*\|^2+(1-\lambda)\|w^k-x^*\|^2+\|x^{k+1}-y^k\|^2 \\
&\ \ \ \ -\lambda\|y^k-x^k\|^2-(1-\lambda-\frac{\gamma}{2\beta})\|y^k-w^k\|^2.
\endaligned
\end{equation}
Taking expectation $\mathbb{E}_k$ on \eqref{8-} and using
\begin{equation*}
\mathbb{E}_k [ \langle Bw^k-B_{\xi_k}w^k+B_{\xi_k}y^k, x^*-y^k \rangle]=\langle By^k, x^*-y^k \rangle ,
\end{equation*}
we obtain
\begin{equation*}
\label{8}
\aligned & \ \ \ \ 2\gamma \langle Bx^*-By^k , x^*-y^k \rangle + \mathbb{E}_k \|x^{k+1}-x^*\|^2 \\
& \leq \lambda\|x^k-x^*\|^2+(1-\lambda)\|w^k-x^*\|^2+\mathbb{E}_k \|x^{k+1}-y^k\|^2 \\
&\ \ \ \ -\lambda\|y^k-x^k\|^2-(1-\lambda-\frac{\gamma}{2\beta})\|y^k-w^k\|^2.
\endaligned
\end{equation*}
By the monotonicity of $B,$ we have
\begin{equation}
\label{9}
\langle Bx^*-By^k , x^*-y^k \rangle \geq 0.
\end{equation}
Combining the definition of $x^{k+1}$ and \eqref{lip}, we have
\begin{equation*}
\mathbb{E}_k \|x^{k+1}-y^k\|^2 \leq \gamma^2L^2\|y^k-w^k\|^2.
\end{equation*}
Therefore,
\begin{equation}
\label{8+}
\aligned \mathbb{E}_k \|x^{k+1}-x^*\|^2
& \leq \lambda\|x^k-x^*\|^2+(1-\lambda)\|w^k-x^*\|^2-\lambda\|y^k-x^k\|^2 \\
&\ \ \ \ -(1-\lambda-\gamma^2L^2-\frac{\gamma}{2\beta})\|y^k-w^k\|^2.
\endaligned
\end{equation}
On the other hand, the definition of $w^{k+1}$ and $\mathbb{E}_{k+\frac{1}{2}}$ yield that
\begin{equation}
\label{e2+1}
\frac{1-\lambda}{p}\mathbb{E}_{k+\frac{1}{2}}[\|w^{k+1}-x^*\|^2]=(1-\lambda)\|x^{k+1}-x^*\|^2+(1-\lambda)\frac{1-p}{p}\|w^k-x^*\|^2.
\end{equation}
Then apply to \eqref{e2+1} the tower property $\mathbb{E}_k[\mathbb{E}_{k+\frac{1}{2}}[\cdot]]=\mathbb{E}_k[\cdot]$, we have
\begin{equation}
\label{e2+2}
\frac{1-\lambda}{p}\mathbb{E}_{k}[\|w^{k+1}-x^*\|^2]=(1-\lambda)\mathbb{E}_{k}\|x^{k+1}-x^*\|^2+(1-\lambda)\frac{1-p}{p}\|w^k-x^*\|^2.
\end{equation}
We add \eqref{e2+2} to \eqref{8+} to obtain
\begin{equation}
\label{10}
\aligned
&\ \ \ \ \lambda \mathbb{E}_k \|x^{k+1}-x^*\|^2+\frac{1-\lambda}{p} \mathbb{E}_k\|w^{k+1}-x^*\|^2\\
&\leq \lambda \|x^k-x^*\|^2+\frac{1-\lambda}{p}\|w^k-x^*\|^2-\lambda\|y^k-x^k\|^2 \\
& \ \ \ \ -(1-\lambda-\gamma^2L^2-\frac{\gamma}{2\beta})\|y^k-w^k\|^2.
\endaligned
\end{equation}
Thus, the inequality \eqref{lypunov} holds by $\gamma \in (0,\frac{4\beta(1-\lambda)}{1+\sqrt{1+16\beta^2L^2(1-\lambda)}})$ and $0<\lambda<1$.

Next, we show the almost sure convergence of the sequence $\{x^{k}\}_{k \in \mathbb{N}}$ generated by Algorithm \ref{ALG}.
By Lemma \ref{Robbins}, we have that $\{\Phi_{k}(x^*)\}_{k\in \mathbb{N}}$ converges almost surely and $\{\|y^k-x^k\|^2\}_{k\in \mathbb{N}},$ $\{ \|y^k-w^k\|^2\}_{k\in \mathbb{N}}$ converges to $0$ almost surely.
Then applying Proposition 2.3 in \cite{Comettes}, we can construct a space $\Xi$, with $\mathbb{P}(\Xi)=1$, such that $\forall  \theta \in \Xi$ and  $\forall x^* \in \zer(A+B+C)$,
\begin{equation}
\label{10+}
 \{\lambda \|x^k(\theta)-x^*\|^2+\frac{1-\lambda}{p}\|w^k(\theta)-x^*\|^2\}_{k\in \mathbb{N}} \ \hbox{converges,}
\end{equation}
which implies that the sequence $\{x^{k}(\theta)\}_{k \in \mathbb{N}}$ is bounded. Let $\Xi^{'}$ be the probability 1 set such that $ \forall \theta \in \Xi^{'}$, $y^k(\theta)-x^k(\theta) \rightarrow 0$, $y^k(\theta)-w^k(\theta) \rightarrow 0$,  which implies $y^k(\theta)-\bar{x}^k(\theta) \rightarrow 0$.
Pick $\theta \in \Xi \bigcap \Xi^{'}$ and let $\{x^{k_j}(\theta)\}_{j \in \mathbb{N}}$ be the convergent subsequence of the bounded sequence $\{x^{k}(\theta)\}_{k \in \mathbb{N}}$, say without loss of generality that $x^{k_j}(\theta) \rightarrow \bar{x}(\theta)$ as $j\rightarrow \infty$.  From  $y^{k_j}(\theta)-x^{k_j}(\theta) \rightarrow 0$, it follows that $y^{k_j}(\theta) \rightarrow \bar{x}(\theta)$ as $j\rightarrow \infty$.
Then according to \eqref{2}, we can get
\begin{equation*}
\bar{x}^{k_j}(\theta)-y^{k_j}(\theta)-\gamma((B+C)w^{k_j}(\theta)-(B+C)y^{k_j}(\theta))\in \gamma(A+B+C)y^{k_j}(\theta).
\end{equation*}
We know that $B+C$ is $(L_B+\frac{1}{\beta})$-Lipschitz. Therefore,
\begin{equation*}
\bar{x}^{k_j}(\theta)-y^{k_j}(\theta)-\gamma((B+C)w^{k_j}(\theta)-(B+C)y^{k_j}(\theta)) \rightarrow 0,\ j\rightarrow \infty.
\end{equation*}
Furthermore, based on the assumption that the operator $B$ has a full domain, we have that $A+B$ is maximally monotone. Combining $C$ is cocoercive, one has that $A+B+C$ is maximally monotone. By Lemma \ref{lem3}, $(\bar{x}(\theta) , 0) \in \gra(A+B+C)$, i.e., $\bar{x}(\theta) \in \zer(A+B+C)$.

Hence, all cluster points of $\{x^{k}(\theta)\}_{k \in \mathbb{N}}$ and $\{w^{k}(\theta)\}_{k \in \mathbb{N}}$ belong to $\zer(A+B+C)$. We have shown that at least  one subsequence of $\{\lambda \|x^k(\theta)-\bar{x}(\theta)\|^2+\frac{1-\lambda}{p}\|w^k(\theta)-\bar{x}(\theta)\|^2\}_{k \in \mathbb{N}}$ converges to $0$. Combining \eqref{10+}, we deduce $\lambda \|x^k(\theta)-\bar{x}(\theta)\|^2+\frac{1-\lambda}{p}\|w^k(\theta)-\bar{x}(\theta)\|^2 \rightarrow 0$ and consequently $\|x^k(\theta)-\bar{x}(\theta)\|^2 \rightarrow 0$. This shows that $\{x^k\}_{k \in \mathbb{N}}$ converges almost surely to a point in $\zer(A+B+C)$.
\end{proof}

%\begin{rem}
%In fact, our upper bound on the step-size is smaller than that of the FBHF splitting algorithm. The gap between the two upper bound is
%\begin{equation*}
%\frac{\sqrt{1+16 \beta^2 L^2}-  \sqrt{1+16 \beta^2 L^2(1-\lambda)}}{4\beta L^2},
%\end{equation*}
%it can be seen that as $\lambda$ gets smaller, the two upper bounds get closer.
%\end{rem}
\subsection{Linear convergence}
In this subsection, we show the linear convergence of Algorithm \ref{ALG} for solving the structured monotone inclusion problem \eqref{PROB} when $B$ is $\mu$-strongly monotone. Indeed, assuming that the operator $A$ is strongly monotone also leads to a linear convergence result, and the proof procedure is similar.
\begin{theorem}
\label{theorem2}
{
\noindent
Let Assumption \ref{CON1} hold, $B$ be $\mu$-strongly monotone. If we set $\lambda=1-p$, and $\gamma=\min\{\frac{\sqrt{p}}{2L},\beta p\}$ in Algorithm \ref{ALG}, then for the sequence $\{x^k\}_{k \in \mathbb{N}}$ generated by Algorithm \ref{ALG} and any $x^* \in \zer(A+B+C)$, it holds that
\begin{equation}
\label{xx}
\mathbb{E}\|x^k-x^*\|^2 \leq (\frac{1}{1+c/4})^k \frac{2}{1-p} \|x^0-x^*\|^2,
\end{equation}
with $c=\min \{ \gamma \mu, \frac{p}{(1+\sqrt p)(4+p)}\}$.
}
\end{theorem}
\begin{proof}
If $B$ is $\mu$-strongly monotone, then \eqref{9} becomes
\begin{equation*}
\langle Bx^*-By^k , x^*-y^k \rangle \geq \mu\|x^*-y^k\|^2.
\end{equation*}
We continue as in the proof of Theorem \ref{the1} to obtain, instead of \eqref{10},
\begin{equation}
\label{11}
\aligned
& \ \ \ \ 2\gamma\mu\|y^k-x^*\|^2+\lambda \mathbb{E}_k \|x^{k+1}-x^*\|^2+\frac{1-\lambda}{p} \mathbb{E}_k\|w^{k+1}-x^*\|^2 \\
& \leq \lambda \|x^k-x^*\|^2+\frac{1-\lambda}{p}\|w^k-x^*\|^2-\lambda\|y^k-x^k\|^2 \\
& \ \ \ \ -(1-\lambda-\gamma^2L^2-\frac{\gamma}{2\beta})\|y^k-w^k\|^2.
\endaligned
\end{equation}
By $\|a+b\|^2 \leq 2\|a\|^2 + 2\|b\|^2$, the step 6 and \eqref{lip}, we have
\begin{equation}
\label{12}
\aligned
2\gamma\mu\|y^k-x^*\|^2
& \geq \gamma \mu \mathbb{E}_k[\|x^{k+1}-x^*\|^2]-2\gamma\mu \mathbb{E}_k[\|\gamma(B_{\xi_k}w^k - B_{\xi_k}y^k)\|^2] \\
& \geq \gamma \mu \mathbb{E}_k[\|x^{k+1}-x^*\|^2]-2\gamma^3L^2\mu\|y^k-w^k\|^2.
\endaligned
\end{equation}
Combining \eqref{11}, \eqref{12} and $\lambda=1-p$,  we get
\begin{equation}
\label{14}
\aligned
& \ \ \ \ (1-p+\gamma \mu)\mathbb{E}_k [\|x^{k+1}-x^*\|^2]+\mathbb{E}_k[\|w^{k+1}-x^*\|^2] \\
& \leq (1-p)\|x^k-x^*\|^2+\|w^k-x^*\|^2-(1-p)\|y^k-x^k\|^2 \\
& \ \ \ \ -(p-\gamma^2L^2-\frac{\gamma}{2\beta}-2\gamma^3L^2\mu )\|y^k-w^k\|^2 \\
& \leq (1-p)\|x^k-x^*\|^2+\|w^k-x^*\|^2-(1-p)\|y^k-x^k\|^2 \\
& \ \ \ \ -\frac{p(1-\sqrt{p})}{4}\|y^k-w^k\|^2,
\endaligned
\end{equation}
where the last inequality is obtained by $\gamma=\min\{\frac{\sqrt{p}}{2L},\beta p\}$ and $\mu \leq L$. Similar to \eqref{12}, we have
\begin{equation}
\label{13}
\aligned \frac{c}{2}\mathbb{E}_k[\|x^{k+1}-x^*\|^2]
& \geq \frac{c}{4} \mathbb{E}_k[\|w^{k+1}-x^*\|^2] - \frac{c}{2}\mathbb{E}_k[\mathbb{E}_{k+\frac{1}{2}}\|x^{k+1}-w^{k+1}\|^2] \\
& = \frac{c}{4} \mathbb{E}_k[\|w^{k+1}-x^*\|^2] - \frac{c(1-p)}{2}\mathbb{E}_k[\|x^{k+1}-w^k\|^2] \\
& = \frac{c}{4} \mathbb{E}_k[\|w^{k+1}-x^*\|^2] - \frac{c(1-p)}{2}\mathbb{E}_k[\|y^k-w^k+\gamma(B_{\xi_k}w^k-B_{\xi_k}y^k)\|^2] \\
& \geq \frac{c}{4} \mathbb{E}_k[\|w^{k+1}-x^*\|^2] -c(1-p)(1+\gamma^2L^2)\|y^k-w^k\|^2 \\
& \geq \frac{c}{4} \mathbb{E}_k[\|w^{k+1}-x^*\|^2] - \frac{c(1-p)(4+p)}{4}\|y^k-w^k\|^2.
\endaligned
\end{equation}
Putting \eqref{13} into \eqref{14} and recalling that $c \leq \gamma \mu$,  we have
\begin{equation}
\label{15}
\aligned
& \ \ \ \ (1-p+\frac{c}{2})\mathbb{E}_k [\|x^{k+1}-x^*\|^2]+(1+\frac{c}{4})\mathbb{E}_k[\|w^{k+1}-x^*\|^2] \\
& \leq (1-p)\|x^k-x^*\|^2+\|w^k-x^*\|^2-(1-p)\|y^k-x^k\|^2 \\
& \ \ \ \ -\left[\frac{p(1-\sqrt{p})}{4}-\frac{c(1-p)(4+p)}{4}\right]\|y^k-w^k\|^2\\
& \leq(1-p)\|x^k-x^*\|^2+\|w^k-x^*\|^2,
\endaligned
\end{equation}
where the last inequality comes from $c \leq \frac{p}{(1+\sqrt p)(4+p)}.$
Then, using $1-p+\frac{c}{2} \geq (1-p)(1+\frac{c}{4})$ and taking the full expectation on \eqref{15}, we have
\begin{equation*}
(1+\frac{c}{4})\mathbb{E}[(1-p)\|x^{k+1}-x^*\|^2+\|w^{k+1}-x^*\|^2]  \leq \mathbb{E}[(1-p)\|x^k-x^*\|^2+\|w^k-x^*\|^2].
\end{equation*}
Iterating this inequality, we obain
\begin{equation*}
(1-p)\mathbb{E}\|x^{k}-x^*\|^2  \leq(\frac{1}{1+c/4})^k (2-p)\|x^0-x^*\|^2,
\end{equation*}
showing \eqref{xx}.
\end{proof}

\section{Numerical Simulations}\label{numerics}

\noindent In this section, we compare the Algorithm \ref{ALG} (VRFBHF) with the FBHF splitting algorithm \eqref{FBHF}.
All codes were written in MATLAB R2018b and performed on a PC Desktop
Intel(R) Core(TM) i7-8700 CPU @ 3.20GHz   3.19 GHz, RAM 8.00 GB.
\vskip 2mm
Consider the nonlinear constrained optimization problem of the form
\begin{equation}
\label{fh}
\min_{x\in C}f(x)+h(x),
\end{equation}
where $C= \{ x \in \mathbb{R}^d \,|\, ( \forall i \in \{1,...,q\}) \ g_i(x) \leq 0\} $, $f: \mathbb{R}^d \rightarrow ( -\infty ,+\infty]$ is a proper, convex  and  lower semi-continuous function, for every $i \in \{1,...,q\}$, $g_i : \dom(g_i) \subset \mathbb{R}^d \rightarrow \mathbb{R}$ and $h: \mathbb{R}^d \rightarrow \mathbb{R}$ are $C^1$ convex functions in $\nt \dom g_i$ and $\mathbb{R}^d$, respectively, and $\nabla h$ is $\beta$-Lipschitz.  The solution to the optimization problem \eqref{fh} can be found via the saddle points of the Lagrangian
\begin{equation*}
L(x,u)=f(x)+h(x)+u^{\top}g(x)-\iota_{\mathbb{R}_+^q}(u),
\end{equation*}
where $\iota_{\mathbb{R}_+^q}$ is the indicator function of $\mathbb{R}_+^q$, Under some standard qualifications, the solution to the optimization problem \eqref{fh} can be found by solving the monotone inclusion \cite{{FBHF},RT}: find $x \in Y$ such that $\exists u \in \mathbb{R}_{+}^q$,
\begin{equation}
\label{ABCxu}
(0,0) \in (A+B+C)(x,u),
\end{equation}
where $Y \subset \mathbb{R}^d$ is a nonempty closed convex set modeling the prior information of the solution, $A:(x,u)\mapsto\partial f(x)\times{N_{\mathbb{R}_+^q}u}$ is maximally monotone, $C:(x,u)\mapsto(\nabla h(x),0)$ is $\beta$-cocoercive, and $B:(x,u)\mapsto(\sum_{i=1}^q u_i \nabla g_i(x),-g_1(x),...,-g_q(x))$ is nonlinear, monotone and continuous. In the light of the structure of $B$, we can rewrite $B$ as $B= \sum_{i=1}^q B_i$ where $B_i:(x,u)\mapsto(u_i\nabla g_i(x),0, ...,-g_i(x),...,0)$ for every $i \in \{1,...,q\}$.
\vskip 2mm
In the numerical
results listed in the following table, ``Iter"  denotes the number of iterations.

\begin{exm} \label{ex1}
\rm
\noindent Let $f=\iota_{[0,1]^d}$, $g_i(x)=d_i^{\top}x$ ($\forall i \in  \{1,...,q\}$) with $d_1,\ldots,d_q\in \mathbb{R}^d$, and  $h=\frac{1}{2}\|Gx-b\|^2$ with $G$ being an $t \times d$ real matrix, $d=2t$, $b\in \mathbb{R}^t$. Then the  operators in \eqref{ABCxu} become
\begin{equation}
\label{A1}
\aligned
&A:(x,u)\mapsto\partial{\iota_{[0,1]^d}(x)}\times{N_{\mathbb{R}_+^q}u},\\
& B:(x,u)\mapsto(D^{\top}u,-Dx),\\
& C:(x,u)\mapsto(G^{\top}(Gx-b),0),\\
 \endaligned
\end{equation}
where  $x\in \mathbb{R}^d$, $u\in \mathbb{R}_+^q$, $D=[d_1,\ldots,d_q]^{\top}$. It is easy to see that the operator $A$ is a maximally monotone operator, $C$ is a $\beta$-cocoercive operator with $\beta =\|G\|^{-2}$, and B is a $L$-Lipschitz operator with $L=\|D\|$. According to the structure of the operator $B$, rewrite $B$ as $B= \sum_{i=1}^q B_i$ where $B_i:(x,u)\mapsto(d_i u_i  ,0, ...,-d_i^{\top}(x),...,0)$ for every $i \in \{1,...,q\}$. For uniform sampling, the stochastic oracle $B_{\xi}(x,u)=qB_i(x,u)$, $P_{\xi}(i)=\Pro{ \{\xi=i\}}=\frac{1}{q}$, $i \in \{1,...,q\}$.

Now, we use Algorithm \ref{ALG} to solve the problem \eqref{PROB} with \eqref{A1}, then Algorithm \ref{ALG} reduces to
\begin{equation*}
\label{2step}
\left\lfloor
\aligned
&\bar x^k=\lambda x^k+(1-\lambda)w^k,\\
&\bar u^k=\lambda u^k+(1-\lambda)v^k,\\
&y^k={\rm Prox}_{\gamma\iota_{[0,1]^d}}(\bar x^k-\gamma D^Tv^k-\gamma(G^T(Gw^k-b))),\\
&\hbox{for every}\,\, j=1,\ldots,q,\\
&\left\lfloor
\aligned
&\eta^k_{j}=\max\{0,\bar u^k_j+\gamma
d_j^Tw^k\},\\
\endaligned
\right.\\
&\hbox{Sample} \ \xi_k \ \hbox{uniformly at random from}\,\ \{1,...,q\}\\
&x^{k+1}=y^k+\gamma qd_{\xi_k}(v_{\xi_k}^k-\eta_{\xi_k}^k),\\
&u^{k+1}=\eta^k+\gamma S_{\xi_k}(w^k-y^k),\\
&w^{k+1}=
\begin{cases}
x^{k+1},&\hbox{with probability}\,\,p\\
w^k,&\hbox{with probability}\,\,1-p\\
\end{cases}\\
&v^{k+1}=
\begin{cases}
u^{k+1},&\hbox{with probability}\,\,p\\
v^k,&\hbox{with probability}\,\,1-p\\
\end{cases}\\
\endaligned
\right.
\end{equation*}
where $S_{\xi_k}=[0, ...,-qd_{\xi_k}^{\top}(x),...,0]$.

In the numerical test,  $G,D,b$ and initial value $(x_{0},u_{0})$  are all randomly generated. In VRFBHF, set  $(w_{0},v_{0})=(x_{0},u_{0})$, take $\lambda=0.1$ and $\gamma=\frac{3.999\beta(1-\lambda)}{1+\sqrt{1+16 \beta^2L^2(1-\lambda)}}$. In FBHF, take $\gamma =\frac{3.999 \beta}{1+\sqrt{1+16\beta^2L_B^2}}$. We use
$$
E_k=\frac{\|(x^{k+1}-x^k,u^{k+1}-u^k)\|}{\|(x^k,u^k)\|}<10^{-6},
$$
as the stopping criterion.

\begin{figure}[!h]
\centering
\includegraphics[scale=0.9]{fig.eps}
\caption{Decay of $E_k$ with the number of iterations of different $p$ for
Example \ref{ex1} with $q=1000,d=500$.}
\label{fig2}
\end{figure}

Figure \ref{fig2} illustrates the behavior of VRFBHF for different $p$, from which it can be observed  that $E_n$ oscillates with $k$ and reaches the stopping criterion the fastest when $p=0.2$.
Next we test eight problem sizes and randomly generate 10 instances for each size. The average number of iterations and CPU time for 10 instances are listed in Table \ref{table1}. It is observed from Table \ref{table1} that VRFBHF has remarkably less CPU time and iteration numbers compared to the FBHF splitting algorithm \eqref{FBHF}.



\begin{table}[h]
\label{table1}
\small
%\tiny
%\footnotesize
\renewcommand\arraystretch{2.8}
%\setlength\tabcolsep{1pt}
\centering
\caption{Computational results  with FBHF and VRFBHF with $p=0.2$}
\vskip 2mm
\begin{tabular}{ccccccccccccccccccccccccccccccccccccccc}
\hline
\multicolumn{3}{c}{Problem size}  & \multicolumn{3}{c}{Iter} &&
 \multicolumn{3}{c}{CPU time}  \\
\cline{1-3} \cline{5-6} \cline{8-10}

$q$ && $d$ &&  VRFBHF  & FBHF  &&  VRFBHF&FBHF&   \\
\hline
 \hline
 \multirow{4}{*}{1000}&&500&&75.8&3147&&1.0531&15.4125&\\
 \cline{2-10}
&&750&&92.3&1363&&1.1047&8.8984&\\
\cline{2-10}
&&1000&&45.8&1933&&0.8344&24.825&\\
\cline{2-10}
&&2000&&16.4&1731&&1.5344&77.2984&\\
\hline
\multirow{4}{*}{2000}&&1000&&96.2&1058&&2.4609&28.775&\\
\cline{2-10}
&&1500&&73.8&1020&&8.8984&55.3953&\\
\cline{2-10}
&&2000&&16.9&2022&&14.6687&151.3359&\\
\cline{2-10}
&&2500&&26.6&1268&&22.8078&136.9688&\\
\hline
\hline
\end{tabular}
\label{table1}
\end{table}


\end{exm}


\begin{thebibliography}{99}
%\bibitem{F. Alvarez}
%Alvarez,  F., Attouch, H.: An inertial proximal method for maximal monotone operators
%via discretization of a nonlinear oscillator with damping. \textit{Set-Valued. Anal.} \textbf{9}, 3--11 (2001)

\bibitem{Alacaoglu-Malitsky}
Alacaoglu, A., Malitsky, Y.: Stochastic variance reduction for variational inequality methods. \textit{Mach. Learn.} \textbf{178}, 1–-39 (2022)

\bibitem{BC2011}
Bauschke, H.H., Combettes, P.L.: \textit{Convex Analysis and Monotone Operator Theory in Hilbert Spaces}, 2nd ed. Springer, New York, (2017)

\bibitem{Barnet}
Barnet, S., Rudzusika, J., \"{O}ktem, O., and Adler, J.: Accelerated forward-backward optimization using deep learning.
https://arxiv.org/abs/2105.05210

%\bibitem{BT}
%Beck, A., Teboulle, M.: A fast iterative shrinkage-thresholding algorithm for linear inverse problems. \textit{Siam J. Imaging. Sci.} \textbf{2}, 183--202 (2009)


\bibitem{FBHF}
Brice\~{n}o-Arias, L.M., Davis, D.: Forward-backward-half forward algorithm for solving monotone inclusions. \textit{Siam J. Optimiz.} \textbf{28(4)}, 2839--2871 (2017)

%\bibitem{Comettes}
%Combettes, P.L., Pesquet, J.-C.: Stochastic quasi-Fejér block-coordinate fixed point iterations with random sweeping II: mean-square and linear convergence. \textit{Math. Program.} \textbf{174}, 433–-451 (2019)

\bibitem{Comettes}
Combettes, P.L., Pesquet, J.-C.: Stochastic quasi-Fejér block-coordinate fixed point iterations with random sweeping. \textit{Siam J. Optimiz.} \textbf{25(2)}, 1221--1248 (2015)


\bibitem{Comb}
Combettes, P.L., Pesquet, J.C.: Primal-dual splitting algorithm for solving inclusions with mixtures of composite, Lipschitzian, and parallel-sum type monotone operators. \textit{Set-valued. Var. Anal.} \textbf{20}, 307–-330 (2012)

%\bibitem{SAGA}
%Defazio, A., Bach, F., and Lascoste-Julien, S.: SAGA: A fast incremental gradient method with support for non-strongly convex composite objectives. \textit{Mach. Learn.} 1646--1654 (2014)

\bibitem{Davis}
Davis, D., Yin, W.: A three-operator splitting scheme and its optimization applications. \textit{Set-valued. Var. Anal.} \textbf{25}, 829--858 (2017)

%\bibitem{He}
%He,S.N., et al. title.

%\bibitem{HTF}
%Hastie, T., Tibshirani, R., and Friedman, J.: \textit{The elements of statistical learning: Data mining, inference and prediction}, 2nd ed. Springer, Berlin, (2009)

\bibitem{SVRG}
Johnson, R., Zhang, T.: Accelerating stochastic gradient descent using predictive variance reduction. \textit{Adv. Neural. Inf. Process. Syst.} 315--323 (2013)

\bibitem{KHR}
Kovalev, D., Horvath, S., and Richt\'{a}rik, P.: Don't jump through hoops and remove those loops:
SVRG and katyusha are better without the outer loop. \textit{Mach. Learn.} \textbf{117}, 1--17 (2020)

\bibitem{LJC}
Liu, J.C., Xu, L.L., Shen, S.H., and Ling, Q.: An accelerated variance reducing stochastic method with Douglas-Rachford splitting. \textit{Mach. Learn.} \textbf{108}, 859-–878 (2019)

\bibitem{Latafat}
Latafat, P., Patrinos, P.: Asymmetric forward-backward-adjoint splitting for solving monotone inclusions involving three operators. \textit{Comput. Optim. Appl.} \textbf{68}, 57--93 (2017)

\bibitem{Malitsky}
Malitsky, Y., Tam, M.K.: A forward-backward splitting method for monotone inclusions without cocoercivity. \textit{Siam J. Optim.} \textbf{30(2)}, 1451--1472 (2020)

%\bibitem{Robbins}
%Raguet, H., Fadili, M., and Peyr$\acute{e}$, G.: A generalized forward-backward splitting. \textit{Siam J. Imaging. Sci.} \textbf{6(3)}, 1199--1226 (2013)

\bibitem{Rie}
Rieger, J., Tam, M.K.: Backward-forward-reflected-backward splitting for three operator monotone inclusions. \textit{Appl. Math. Comput.} \textbf{381}, (2020)

\bibitem{Ryu}
Ryu, E.K.: Uniqueness of DRS as the 2 operator resolvent-splitting and impossibility of 3 operator resolvent-splitting. \textit{Math. Program.} \textbf{182}, 233--273 (2020)

\bibitem{Ryuu}
Ryu, E.K., V\~{u}, B.C.: Finding the forward-Douglas–Rachford-forward method. \textit{J. Optimiz. Theory. App.} \textbf{184}, 858--876 (2020)

\bibitem{RT}
Rockafellar, R.T.: Monotone operators associated with saddle-functions and minimax problems, in: Nonlinear Func. Anal., I, F.E. Browder ed., Proc. Pure Mat \textbf{18}, 241–-250 (1970)
%
%\bibitem{SDCA}
%Shalev-S, S., Zhang, T.: Accelerated proximal stochastic dual coordinate ascent for regularized loss minimization. \textit{Mach. Learn.}  (2013)

%\bibitem{SSH}
%Sadeghi, H., Banert, S., and Giselsson, P.: Forward-backward splitting with deviations for monotone inclusions.
%https://arxiv.org/abs/2112.00776
%
%\bibitem{SSH1}
%Sadeghi, H., Banert, S., and Giselsson, P.: Incorporating history and deviations in forward-backward splitting.
%https://arxiv.org/abs/2208.05498
\bibitem{Robbins}
Robbins, H., Siegmund, D.: A convergence theorem for non negative almost supermartingales and some applications. \textit{Optimizing Methods in Statistics.} 233--257 (1971)

\bibitem{Tseng}
Tseng, P.: A modified forward-backward splitting method for maximal monotone mapping.
\textit{Siam J. Control. Optim.} \textbf{38(2)}, (2000)

\bibitem{Yu}
Yu, H., Zong, C., and Tang, Y.: An outer reflected forward-backward splitting algorithm for solving monotone inclusions. https://arxiv.org/abs/2009.12493 (2020)

\bibitem{Zhang}
Zhang, X., Haskell, W. B., and Ye, Z.S.: A Unifying Framework for Variance-Reduced Algorithms
for Findings Zeroes of Monotone operators. \textit{J. Mach. Learn. Res.}
\textbf{23(60)}, 1–-44 (2022)

\bibitem{Zong}
Zong, C., Tang, Y., and Cho, Y.J.: Convergence analysis of an inexact three-operator splitting algorithm. \textit{Symmetry.} \textbf{10(11)}, 563 (2018)

\bibitem{Tang}
Zong, C., Tang, Y., and Zhang, G.: An accelerated forward-backward-half forward splitting algorithm for monotone inclusion with applications to image restoration.  \textit{Optimization.} (2022) DOI: 10.1080/02331934.2022.2107926
\end{thebibliography}


	
\end{document}


