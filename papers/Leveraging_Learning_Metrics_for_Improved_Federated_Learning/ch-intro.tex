\chapter{Introduction }\label{ch:intro} 
The traditional deep-learning (DL) paradigm relies on a central server, where data collected from external sources (mobile phones, laptops, etc) are sent to the server. Massive amounts of data transfer occur between these edge devices and the server, representing a serious communication overhead. In 2016, a new paradigm was proposed by Google \cite{konevcny2016federated}, termed Federated Learning (FL) where edge devices download copies of the model from the central server and train locally. Trained weights are sent back to the central server to be averaged, updating the global model. This novel federated learning system allowed for a completely new method of training deep-learning models, distributing the load of training while providing privacy-preserving features that mitigate modern privacy concerns. \\

\section{Motivation}
As deep learning evolves, the requirements we, as a society, expect of DL increases in parallel. These expectations are built on the behemoth amount of data that exists in both structured and unstructured forms in the real world. Unfortunately, these larges swaths of data that could be leveraged for training DL models, is hidden behind `data islands' where the organizations holding this data tend to be highly protective. A canonical example within the FL community are the highly valuable datasets within hospitals \cite{li2021flsurvey}. Hospitals protect patient privacy through internal policies meaning they often are unable to share patient data without permission. The phenomena of `data islands' permeates multiple verticals such as government, medicine, finance, retail and supply chains. As such, leveraging this large swath of data while maintaining the private nature of the data would be mutually beneficially to all parties involved. \\


\section{Problem Formulation}
\subsection{Federated Learning}\label{ch:fedlearn}
Within federated learning, two paradigms emerge, firstly the ``cross-device'' learning which outlines the initial use-case: for multiple edge devices to collaboratively train networks \cite{konevcny2016federated}. Secondly, as the field progressed, ``cross-silo'' use-cases emerged, such as the hospital scenario, where trusted organizations can serve as reliable clients. In light of these advances, a new definition of federated learning was developed \cite{kairouz2019advances}:


\begin{definition}[Federated Learning]\label{deffl}
Federated learning is a machine learning setting where multiple entities (clients) collaborate in solving a machine learning problem, under the coordination of a central server or service provider. Each clientâ€™s raw data is stored locally and not exchanged or transferred; instead, focused updates intended for immediate aggregation are used to achieve the learning objective.
% \label{deffl}
\end{definition}

As per \autoref{deffl}, the question of coordination, and aggregation of client updates naturally arises. Multiple methods have been proposed based on the initial federated stochastic gradient descent (FedSGD) algorithm \cite{li2020preserving}, which extends stochastic gradient descent (SGD) to a federated setting. One of the most simple and popular techniques is termed federated averaging where clients get the global model shared by the central server, training occurs, and a model is returned after an epoch. The averaging of model weights is done via simple averaging. 
\begin{figure}[htp]
    \centering
    \includegraphics[width=\textwidth]{images/fedavg_setting.png}
    \caption{Federated Averaging where model parameters are shared in consecutive actions by the server-client model. \cite{konevcny2016federated}}
    \label{fig:fedavg}
\end{figure}

Building on this work, more interesting re-weightings of the client weights have taken place, such re-weighting the client parameters according to the inverse of the un-centered variance of the SGD operation at every client \cite{reyes2021precision}. Implicitly this means that models that do not have as many data points to train, will have higher variance, and thus lower weight in model aggregation. Currently, the FL community has not designated one aggregation scheme as the `go-to' method for knowledge aggregation. In this space, exploration in optimizing aggregation schemes is still an open-question allowing for novel model parameter aggregation techniques to be evaluated. \\



% More recently, the artificial intelligence (AI) community has been interested in research around creating `glass-box' models, and the tools surrounding these models in order to analyze and effectively understand how DL models work. In light of this, \cite{jaegerman21genprob} has shown multiple novel explainable metrics \textit{stable rank, condition number, effective rank} and \textit{quality measure}. Building on \cite{jaegerman21genprob}, this work attempts to use a novel re-weighting scheme combined with explainable metrics in order to develop a novel federated learning aggregation method.
\subsection{Learning Metrics}\label{ch:learnmetrics}
As the Artificial Intelligence (AI) community begins more earnest research into explainable AI (XAI), a theme of developing explainable metrics to give human intuition \cite{xai} around the black-box models has risen. A subset of this phenomena is the idea of developing `learning' metrics, to probe the internal layers of a neural network in an effort to analyze how well the model is learning. By assessing knowledge aggregation metrics during training, intuition around model function, mapping space and feature representations can be gained and thus allowing humans to in turn, develop better DL models. \\

In light of this, \cite{jaegerman21genprob} has shown multiple novel learning metrics \textit{stable rank, condition number, effective rank} and \textit{quality measure}. These learning metrics probe the intermediate convolutional layers to better understand the mapping space between any two layers.

\section{Objectives}
The primary purpose of this thesis will be to marry the ideas developed in \autoref{ch:learnmetrics}, `learning metrics' and  \autoref{ch:fedlearn}, `federated learning' to develop a novel re-weighting scheme where model weights will be aggregated in proportion to the Effective Rank \cite{jaegerman21genprob}.