\chapter{Results \& Discussion}
In this section, the results from a series of experiments comparing the federated effective rank averaging method (FedER) to state-of-the-art methods and tested across 3 different optimizes with and without learning rate scheduler StepLR. \\

\textbf{Figure Description: }All figures in this section will have an accuracy and loss plot. The Accuracy title will have the Top-1 Test Accuracy immediately after the title. The Legend will contain numbers \{0, 1, 2, 3 \} denoting Client 0, Client 1, Client 2, Client 3. The pink line in the legend will denote the test-accuracy measured at the end of every round. 

\section{Optimizers}\label{sec:optimizers}
\textbf{Adam:} Adam \cite{kingma2014adam} is the most widely used optimizer in deep learning due to it's ability to easily outperform competing methods. Effectively Adam treats each parameter gradient as a distribution and computes the per-parameter-gradient mean and variance (then some correction terms), and leverages them to derice the  `coefficient of variance' (CV) which is used to update the parameters.\\

\textbf{AdamP:} AdamP \cite{heo2020adamp} is built on top of Adam, but instead of directly using the weights, they notice that for scale-invariant weights like Batch Norm layers (of which $>$80\% of ResNet is comprised) excessive growth occurs. Since the step-size of optimizers is inversely proportional to the norm of these weights, they may yield in sub-optimal weight selections for scale-invariant weights. As such, AdamP instead \textit{regularizes through projection} onto the tangent of the gradient. Thus, AdamP is able to leverage momentum while slowing growth for scale-invariant weights.\\

\textbf{RMSGD:} The Rank-Momentum SGD (RMSGD) method \cite{hosseini2022exploiting} is a novel method recently presented, that introduced learning metrics \textit{stable rank} measuring how well a layer encodes information and \textit{condition number} which measures stability with respect to perturbations \& noise. RMSGD is effectively a per-parameter learning rate scheduler that adaptively chooses a learning rate in order to guarantee a monotonic increase in stable rank. As such it can `force' better encoding of information per layer and thus close the generalization-gap.\\

\section{Computational Setup}
\textbf{Setup:} We evaluate `FedER' against Federated Averaging \cite{konevcny2016federated} as applied to Image Classification task CIFAR100 with cutout and using the Residual Network model with 18 layers (ResNet18)  \cite{he2016deep}. \\
%and Precision-weighted Federated Learning \cite{reyes2021precision}

\textbf{Hardware:} In the distributed setting, we use a single GPU (RTX2080Ti) per client, on 4 different clients meaning 4x RTX2080Ti. The server used an Intel Xeon Gold 6246 processor, and 256 gigabytes of RAM. No experiments were performed using multi-GPU parallelism. The model aggregation was conducted on CPU, and only training was conducted on GPU.\\
% \newpage

\textbf{Standardization:} Since optimizers could exhibit widely different epoch times, we limited training to 250 epochs using SGD. While there are many advantages to federated learning, in this work, for experimental results, we attempt to leverage the knowledge sharing nature to develop better overall models. Full configuration details are available below in \autoref{tab:config}\\

\begin{table}[htp]
\begin{center}
\begin{tabular}{c|c}
\textbf{Hyper-parameter}                                        & \textbf{Configuration}                                           \\ \hline
Batch Size                                                      & 128                                                              \\ \hline
\begin{tabular}[c]{@{}c@{}}Initial Learning\\ Rate\end{tabular} & 1e-3                                                             \\ \hline
Weight Decay                                                    & 1e-4                                                             \\ \hline
Loss Function                                                   & Cross-Entopy                                                     \\ \hline
Scheduler                                                       & \begin{tabular}[c]{@{}c@{}}(if applicable)\\ StepLR\end{tabular} \\
Scheduler Step Size                                             & 25                                                               \\
Scheduler Decay                                                 & 0.5                                                             
\end{tabular}
\end{center}
\caption{Hyper-parameter configurations for Experiments}\label{tab:config}
\end{table}
\newpage

\section{Experiments}
The following experiments are run on ResNet 18 with CIFAR100 in a non-federated setting, with Federated Averaging (FedAvg) and the novel Federated Effective Rank (FedER) method:
\begin{itemize}
    \item Adam without StepLR \vspace{-0.58cm}
    \item Adam with StepLR \vspace{-0.58cm}
    \item AdamP without StepLR \vspace{-0.58cm}
    \item AdamP with StepLR \vspace{-0.58cm}
    \item RMSGD without StepLR \vspace{-0.58cm}
\end{itemize}
\subsection{Non-Federated Baselines}
In this section, the same hyper-parameters ran in the federated setting, are run in the traditional setting to compare as a non-federated baseline. \\

\textbf{Figure Descriptions:} Similar to the federated setting the top-1 test accuracy is displayed in the title. The main difference is that the train loss \& train accuracy are in blue while the test loss \& test accuracy are in green. 
\subsubsection{Adam with no StepLR}
\begin{figure}[H]
    \centering
    \subfigure[Accuracy vs. Epochs]{\includegraphics[width=0.5\textwidth]{images/accuracy/RndVsTrainTestAcc_plain_r18_adam_noneResNet18_CIFAR100.png}}\hspace{-0.80cm}
    \subfigure[Loss vs Epochs]{\includegraphics[width=0.5\textwidth]{images/loss/RndVsTrainTestLoss_plain_r18_adam_noneResNet18_CIFAR100.png}}
    \caption{Non-Federated ResNet with Adam and no StepLR}
    \label{fig:normal-adam-nosteplr}
\end{figure}

\subsubsection{Adam with StepLR}
\begin{figure}[H]
    \centering
    \subfigure[Accuracy vs. Epochs]{\includegraphics[width=0.5\textwidth]{images/accuracy/RndVsTrainTestAcc_plain_r18_adam_steplrResNet18_CIFAR100.png}}\hspace{-0.80cm}
    \subfigure[Loss vs Epochs]{\includegraphics[width=0.5\textwidth]{images/loss/RndVsTrainTestLoss_plain_r18_adam_steplrResNet18_CIFAR100.png}}
    \caption{Non-Federated ResNet18 with Adam and StepLR}
    \label{fig:normal-adam-steplr}
\end{figure}

\subsubsection{AdamP with no StepLR}
\begin{figure}[H]
    \centering
    \subfigure[Accuracy vs. Epochs]{\includegraphics[width=0.5\textwidth]{images/accuracy/RndVsTrainTestAcc_plain_r18_adamp_noneResNet18_CIFAR100.png}}\hspace{-0.80cm}
    \subfigure[Loss vs Epochs]{\includegraphics[width=0.5\textwidth]{images/loss/RndVsTrainTestLoss_plain_r18_adamp_noneResNet18_CIFAR100.png}}
    \caption{Non-Federated ResNet18 with Adam and StepLR}
    \label{fig:normal-adamp-nosteplr}
\end{figure}

\subsubsection{AdamP with StepLR}
\begin{figure}[H]
    \centering
    \subfigure[Accuracy vs. Epochs]{\includegraphics[width=0.5\textwidth]{images/accuracy/RndVsTrainTestAcc_plain_r18_adamp_steplrResNet18_CIFAR100.png}}\hspace{-0.80cm}
    \subfigure[Loss vs Epochs]{\includegraphics[width=0.5\textwidth]{images/loss/RndVsTrainTestLoss_plain_r18_adamp_steplrResNet18_CIFAR100.png}}
    \caption{Non-Federated ResNet18 with AdamP and StepLR}
    \label{fig:normal-adamp-steplr}
\end{figure}

\subsubsection{RMSGD with no StepLR}
\begin{figure}[H]
    \centering
    \subfigure[Accuracy vs. Steps]{\includegraphics[width=0.5\textwidth]{images/accuracy/RndVsTrainTestAcc_plain_r18_rmsgd_noneResNet18_CIFAR100.png}}\hspace{-0.80cm}
    \subfigure[Loss vs Steps]{\includegraphics[width=0.5\textwidth]{images/loss/RndVsTrainTestLoss_plain_r18_rmsgd_noneResNet18_CIFAR100.png}}
    \caption{Non-Federated ResNet18 with RMSGD and no StepLR}
    \label{fig:normal-rmsgd-nosteplr}
\end{figure}

\newpage
\subsection{Federated Averaging \cite{konevcny2016federated}}
\subsubsection{Adam with no StepLR}

\begin{figure}[H]
    \centering
    \subfigure[Accuracy vs. Steps]{\includegraphics[width=0.5\textwidth]{images/accuracy/RndVsTrainTestAcc_r18_c100_adam_nosteplr.png}}\hspace{-0.80cm}
    \subfigure[Loss vs Steps]{\includegraphics[width=0.5\textwidth]{images/loss/RndVsTrainLoss_r18_c100_adam_nosteplr.png}}
    \caption{Federated Averaging with Adam and no StepLR}
    \label{fig:fedavg-adam-nosteplr}
\end{figure}

Here, We can see the traditional accuracy \& loss curves. Due to the knowledge sharing nature we are leveraging in Federated Learning, Adam without StepLR is out-performing plain ResNet18 on CIFAR100 seen in \autoref{fig:normal-adam-nosteplr}.
 
\newpage
\subsubsection{Adam with StepLR}

\begin{figure}[H]
    \centering
    \subfigure[Accuracy vs. Steps]{\includegraphics[width=0.5\textwidth]{images/accuracy/RndVsTrainTestAcc_ResNet18_CIFAR100_baseline.png}}\hspace{-0.80cm}
    \subfigure[Loss vs Steps]{\includegraphics[width=0.5\textwidth]{images/loss/RndVsTrainLoss_ResNet18_CIFAR100_baseline.png}}
    \caption{Federated Averaging with Adam and StepLR}
    \label{fig:fedavg-adam-steplr}
\end{figure}

In comparison to \autoref{fig:fedavg-adam-nosteplr}, we can see a visible increase in the generalization gap with an under-performing test-accuracy. This is contradictory to conventional wisdom, where lowering the learning rate should prevent oscillations in the loss landscape and thus encourage learning, but here as the training accuracy increases there is not a proportional increase in the test accuracy, suggesting overfitting. 

\newpage
\subsubsection{AdamP with no StepLR}

\begin{figure}[H]
    \centering
    \subfigure[Accuracy vs. Steps]{\includegraphics[width=0.5\textwidth]{images/accuracy/RndVsTrainTestAcc_r18_c100_adamp_nosteplr.png}}\hspace{-0.80cm}
    \subfigure[Loss vs Steps]{\includegraphics[width=0.5\textwidth]{images/loss/RndVsTrainLoss_r18_c100_adamp_nosteplr.png}}
    \caption{Federated Averaging with AdamP and no StepLR}
    \label{fig:fedavg-adamp-nosteplr}
\end{figure}

The figure in \autoref{fig:fedavg-adamp-nosteplr} demonstrates a unique phenomena, an incredibly rapid growth within the first regime (first 15 epochs) then a come-down in accuracy (identical increase in loss) then a struggle to lower loss again. This could be due to a bad initial seed, but since all four clients start on different seeds to aggregate knowledge, this seems unlikely.\\

As mentioned in \autoref{sec:optimizers} AdamP is a projection onto the tangent of the gradient. I suspect that when aggregating the results, that AdamP's is \textit{over-regularizing} the weights. This is further supported by the train accuracy being lower than the test-accuracy. \\

Therefore, further research into this phenomena should be considered, changing \textbf{a)} the learning rate to something lower \& perhaps using StepLR (addressed in the next section)  \textbf{b)} lowering the weight-decay parameter (although the weight-decay chosen is actually lower than the default) \textbf{c)} increase the $\beta_1$ \& $\beta_2$ parameters in Adam to encourage averaging over a larger history, allowing for increased smoothness. 

\subsubsection{AdamP with StepLR}

\begin{figure}[H]
    \centering
    \subfigure[Accuracy vs. Steps]{\includegraphics[width=0.5\textwidth]{images/accuracy/RndVsTrainTestAcc_r18_c100_adamp_baseline.png}}\hspace{-0.80cm}
    \subfigure[Loss vs Steps]{\includegraphics[width=0.5\textwidth]{images/loss/RndVsTrainLoss_r18_c100_adamp_baseline.png}}
    \caption{Federated Averaging with AdamP and StepLR}
    \label{fig:fedavg-adamp-steplr}
\end{figure}

Using StepLR has a significant change to AdamP's usage in a federated setting. When comparing to \autoref{fig:fedavg-adamp-nosteplr} we can observe how it simultaneously suffers from the same over-regularization, as the test-accuracy remains higher than the train-accuracy, but the StepLR forcing the train-accuracy up (\& loss down) to overcome the regularization in order to get to a competitive test-accuracy. It appears that the regularization \& overfitting of StepLR are `fighting' to determine which one dominates the model. Furthermore, the fast initial learning suggests value can be extracted from this method.
\newpage
\subsubsection{RMSGD with no StepLR}

\begin{figure}[H]
    \centering
    \subfigure[Accuracy vs. Steps]{\includegraphics[width=0.5\textwidth]{images/accuracy/RndVsTrainTestAcc_r18_c100_rmsgd_nosteplr.png}}\hspace{-0.80cm}
    \subfigure[Loss vs Steps]{\includegraphics[width=0.5\textwidth]{images/loss/RndVsTrainLoss_r18_c100_rmsgd_nosteplr.png}}
    \caption{Federated Averaging with RMSGD and no StepLR}
    \label{fig:fedavg-rmsgd-nosteplr}
\end{figure}

While RMSGD does under-perform the other models, it simultaneously has the lowest generalization-gap across all possible variations. The most fascinating thing about \autoref{fig:fedavg-rmsgd-nosteplr} is the rate at which it learns is extremely slowly. Since it is learning at such a slow pace, it can be ascertained that the model's learning rate was set too low (1e-3) and could benefit from \textbf{a)} longer training times and \textbf{b)} a higher learning rate, possibly with a learning rate scheduler like StepLR. With these key changes, further exploration into how RMSGD could improve in the federated setting should be considered.

\newpage
\subsection{Federated Effective Rank}%(This Method)
\subsubsection{Adam with no StepLR}
\begin{figure}[H]
    \centering
    \subfigure[Accuracy vs. Steps]{\includegraphics[width=0.5\textwidth]{images/accuracy/RndVsTrainTestAcc_ResNet18_CIFAR100_one_epoch.png}}\hspace{-0.80cm}
    \subfigure[Loss vs Steps]{\includegraphics[width=0.5\textwidth]{images/loss/RndVsTrainLoss_ResNet18_CIFAR100_one_epoch.png}}
    \caption{Federated Effective Rank (FedER) with Adam and no StepLR}
    \label{fig:feder-adam-nosteplr}
\end{figure}

The Federated Effective Rank (FedER) method out-performs \autoref{fig:fedavg-adam-nosteplr} by \textcolor{ao}{\textbf{+0.18\%}}. As mentioned in \autoref{sec:er}, the effective rank is a form of measuring the feature mapping representations, and by utilizing the effective rank to weight-convolutional weight aggregations, does indeed prove useful in a federated setting.


% Furthermore, this shows th
\newpage
\subsubsection{Adam with StepLR}

\begin{figure}[H]
    \centering
    \subfigure[Accuracy vs. Steps]{\includegraphics[width=0.5\textwidth]{images/accuracy/RndVsTrainTestAcc_ResNet18_CIFAR100_oe_steplr.png}}\hspace{-0.80cm}
    \subfigure[Loss vs Steps]{\includegraphics[width=0.5\textwidth]{images/loss/RndVsTrainLoss_ResNet18_CIFAR100_oe_steplr.png}}
    \caption{Federated Effective Rank (FedER) with Adam and StepLR}
    \label{fig:feder-adam-steplr}
\end{figure}

When using StepLR, the same issue of over-fitting plagues the FedER as did in FedAvg in \autoref{fig:fedavg-adam-steplr} such as the large generalization gap. That being said, this novel method, FedER outperforms the FedAvg by \textcolor{ao}{+0.26\%}, demonstrating how effective rank's mapping technique is state-of-the-art. 
\newpage
\subsubsection{AdamP with no StepLR}

\begin{figure}[H]
    \centering
    \subfigure[Accuracy vs. Steps]{\includegraphics[width=0.5\textwidth]{images/accuracy/RndVsTrainTestAcc_r18_c100_adamp_feder.png}}\hspace{-0.80cm}
    \subfigure[Loss vs Steps]{\includegraphics[width=0.5\textwidth]{images/loss/RndVsTrainLoss_r18_c100_adamp_feder.png}}
    \caption{Federated Effective Rank (FedER) with AdamP and no StepLR}
    \label{fig:feder-adamp-nosteplr}
\end{figure}

Similar to \autoref{fig:fedavg-adamp-nosteplr}, the FedER method demonstrates similar qualities, but wile the results are similar, the FedER underperforms by \textcolor{red}{-1.46\%}. Further investigation is required into how AdamP can improve FedAvg \& FedER and thus should improve with the recommendations outlined in \autoref{fig:fedavg-adam-steplr} such as lowering the learning rate and lowering the weight-decay parameter. 

\newpage
\subsubsection{AdamP with StepLR}

\begin{figure}[H]
    \centering
    \subfigure[Accuracy vs. Steps]{\includegraphics[width=0.5\textwidth]{images/accuracy/RndVsTrainTestAcc_r18_c100_adamp_steplr_feder.png}}\hspace{-0.80cm}
    \subfigure[Loss vs Steps]{\includegraphics[width=0.5\textwidth]{images/loss/RndVsTrainLoss_r18_c100_adamp_steplr_feder.png}}
    \caption{Federated Effective Rank (FedER) with AdamP and StepLR}
    \label{fig:feder-adamp-steplr}
\end{figure}

Once again, the FedER method performs similarly to FedAvg with AdamP \& StepLR, but this method outperforms by \textcolor{ao}{+0.83\%}. 

\newpage
\subsubsection{RMSGD with no StepLR}
\begin{figure}[H]
    \centering
    \subfigure[Accuracy vs. Steps]{\includegraphics[width=0.5\textwidth]{images/accuracy/RndVsTrainTestAcc_r18_c100_rmsgd_feder.png}}\hspace{-0.80cm}
    \subfigure[Loss vs Steps]{\includegraphics[width=0.5\textwidth]{images/loss/RndVsTrainLoss_r18_c100_rmsgd_feder.png}}
    \caption{Federated Effective Rank (FedER) with AdamP and StepLR}
    \label{fig:feder-rmsgd-nosteplr}
\end{figure}

% \subsection{Federated Variance \cite{reyes2021precision}}
% \subsubsection{Adam with no StepLR}

% \subsubsection{Adam with StepLR}

% \subsubsection{AdamP with no StepLR}

% \subsubsection{AdamP with StepLR}

% \subsubsection{RMSGD with no StepLR}

% \subsection{Plain ResNet18 on CIFAR100}
% \subsubsection{Adam with no StepLR}

% \subsubsection{Adam with StepLR}

% \subsubsection{AdamP with no StepLR}

% \subsubsection{AdamP with StepLR}

% \subsubsection{RMSGD with no StepLR}

% \subsection{Longer Training on RMSGD for Federated Effective Rank}

\section{Final Results}

\begin{table}[H]
\begin{center}
\begin{tabular}{c|c|c|c|l}
\textbf{Configuration}                                                & \textbf{\begin{tabular}[c]{@{}c@{}}Non-Fed\\ Baseline\end{tabular}} & \textbf{FedAvg}               & \textbf{FedER}                & \textbf{Difference}                                  \\ \hline
\textit{\begin{tabular}[c]{@{}c@{}}Adam w/out\\ StepLR\end{tabular}}  & 71.71                                                               & 76.87                         & \cellcolor[HTML]{9AFF99}77.05 & {\color[HTML]{009901} +0.18}                         \\ \hline
\textit{\begin{tabular}[c]{@{}c@{}}Adam with \\ StepLR\end{tabular}}  & 74.54                                                               & 75.71                         & \cellcolor[HTML]{9AFF99}75.97 & {\color[HTML]{009901} +0.26}                         \\ \hline
\textit{\begin{tabular}[c]{@{}c@{}}AdamP w/out\\ StepLR\end{tabular}} & \cellcolor[HTML]{FFFFFF}70.06                                       & \cellcolor[HTML]{9AFF99}50.89 & 49.43                         & \cellcolor[HTML]{FFFFFF}{\color[HTML]{FE0000} -1.46} \\ \hline
\textit{\begin{tabular}[c]{@{}c@{}}AdamP with\\ StepLR\end{tabular}}  & \cellcolor[HTML]{FFFFFF}76.39                                       & 71.85                         & \cellcolor[HTML]{9AFF99}72.68 & {\color[HTML]{009901} +0.83}                         \\ \hline
\textit{\begin{tabular}[c]{@{}c@{}}RMSGD w/out\\ StepLR\end{tabular}} & 60.22                                                               & 60.07                         & \cellcolor[HTML]{9AFF99}60.52 & {\color[HTML]{009901} +0.45}                        
\end{tabular}
\end{center}
\caption{Final Top-1 Test-Accuracies for all experiments. Green coloured boxes represent the `better' federated method, and the differences display by how much. }\label{tab:finalresults}
\end{table}

Above in \autoref{tab:finalresults}, the final results are displayed. While the Federated Effective Rank method under-performs quite a lot using AdamP and no StepLR, it out-performs in every other configuration. While investigating the under performance of FedER using AdamP without StepLR, it showed that heavy regularization was occurring and thus, the model was unable to learn effectively. This is supported by the fact that in both FedAvg and FedER settings, the train-accuracy is never able to overtake the test-accuracy, demonstrating that the learned parameters were having trouble generalizing. When comparing FedER \& FedAvg in AdamP using StepLR, we can see that FedER then out-performs FedAvg by 0.83\%, and in this case the StepLR was attempting to minimize oscillations in the loss-landscape, which has the auxiliary value of attempting to over-fit the on the training data. This competition between over-fitting and regularization is part of the reason the model actually learns.\\

Further analysis into investigating the nature of Federated Effective Rank and how other learning metrics like the stable rank from RMSGD \cite{hosseini2022explainable} can be leveraged in order to improve knowledge aggregation in federated settings.




