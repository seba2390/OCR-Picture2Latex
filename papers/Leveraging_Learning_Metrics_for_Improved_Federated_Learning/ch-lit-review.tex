\chapter{Literature Review}
This literature review assumes some basic knowledge of statistics, vector calculus and distributed systems. It will then focus on the nature of machine learning and deep learning from a theoretical then practical perspective, serving as the basis for the  mathematical explanations formed in later chapters.
\section{Federated Learning Aggregation Methods}
\textbf{Optimization Problem Formulation}\\
Due to the nascent nature of federated learning, multiple weight aggregation methods have been proposed, but they all follow the original problem formulation. In \cite{konevcny2016federated} the optimization scheme to create communication efficient is proposed as:
\begin{equation}
    \min_{\textit{w} \in \mathbb{R}^d} f(w) \text{ \quad where \quad} f(w) = \frac{1}{n} \sum_{i=1}^{n} f_i (w) 
    \label{eq:optform}
\end{equation}
For machine learning problems $f_i (w) = \ell(x_i, y_i; w) $, the loss of prediction sample $(x_i, y_i)$ using weight $w$. With $K$ clients, on selection $k \in K$ are chosen to participate in the model update, whose data $\mathcal{D}_k$ is the data on the $k$th client, where $n_k = |\mathcal{D}_k|$ and $n = \sum n_k$. In this way, we rewrite \autoref{eq:optform} as:
\begin{equation}
    f(w) = \sum_{i = 1}^{n_k} \frac{n_k}{n} F_k (w) \text{ \quad where \quad} F_k(w) = \frac{1}{n_k} \sum_{i \in \mathcal{D}_k} f_i(w)
    \label{eq:optform2}
\end{equation}
% \newpage
\textbf{Stochastic Gradient Descent in a Federated Setting}\\
Gradient updates in traditional machine \& deep learning take place with weight updates using the stochastic gradient descent (SGD) algorithm as follows:
\begin{equation}
    w^{(t+1)} \leftarrow w^{(t)} + \eta_t \nabla f_i(w)
\end{equation}
For some step size, $\eta$, predefined by iteration $t$. 
In a federated setting, the central server aggregates the weights via 
\begin{equation}
    w^{(t+1)} \leftarrow w^{(t)} + \eta_t \sum_{i = 1}^{K} \frac{n_k}{n} g_k  \text{ \quad where \quad} g_k = \nabla F_k(w_t)
    \label{eq:grad1}
\end{equation}
It follows then that $\sum_{i = 1}^{K} \frac{n_k}{n} g_k = \nabla f_i(w) $. We can then rewrite \autoref{eq:grad1} into a simpler formulation:
\begin{align}
    w^k_{t+1} \leftarrow  w^k_{t} + \eta \cdot g_k \qquad \text{Client Update} \label{eq:client_fsgd}\\
    w_{t+1} \leftarrow \sum_{i = 1}^{K} \frac{n_k}{n}w^k_{t+1} \label{eq:server_fsgd} \qquad \text{Server Update}
\end{align}

Since this is the case the local models can update multiple more steps (termed rounds, $E$ - traditionally the minibatch update) before an update occurs on the server.

\subsection{Precision Weighted Federated Learning}
In \cite{reyes2021precision} the authors note that the data heterogeneity may be overestimated in \cite{konevcny2016federated}, and in doing so they mistake the variance within the data. The authors, then propose that leveraging the intra-variability to improve performance through evaluation of the dataset variance in the gradient update. \\

In \cite{konevcny2016federated}, it is noted that for independent and identically distributed (i.i.d) data the variance $\mathbb{E}_{\mathcal{D}_k}[F_k(w)] = f(w)$, but when the data is heterogeneous, this assumption does not hold. Therefore by replacing the weighted averaging with the variance of the maximum likelihood estimator (MLE) in \autoref{eq:server_fsgd}.
\begin{equation}
     w_{t+1} \leftarrow \sum_{i = 1}^{K} \frac{(v_{t+1}^i)^{-1}}{\sum_{i=1}^{K} (v_{t+1}^i)^{-1}}w^k_{t+1} 
\end{equation}
Where $v_{t+1}^k$ denotes the variance of the MLE of $w$, which intuitively considers the model's uncertainty in choosing $w$. 

\subsection{Unbiased Gradient Accumulation}
The authors of \cite{yao2019federated} show that as $E$ increases, the difference between the ideal $g_t$ computed at every iterative update and the $g_t^{k(i)}$ for the $i$th of $E$ steps grows larger. They denote this gap the gradient bias, which initially is small but accumulates for large $E$ as $t$ continues. Furthermore, they also note the differing optimization objectives within a federated setting when choosing a subset of the clients to represent the whole dataset. By showing that the target distribution of data $\mathcal{D}$ may not match the chosen subset from the clients (denoted $\mathcal{D}_{S_t}$) the assumption works well for cross-silo situations but in the cross-device paradigm, the edge compute may not be representative, or able to sustain training and thusly have differing objectives from the true optimization objective.\\

In order circumvent the difference accumulation in the gradients, the computation in \autoref{eq:server_fsgd} of $w^{k(i)}_{t+1}$ can be replaced directly with $w_t$ which violates the client update rule \& requires more than necessary compute. On client for the $i$th step of the $t$th round, the update looks like: 
\begin{equation}
    w^{k(i)}_{t} \leftarrow  w^{k(i-1)}_{t} + \eta g_t^{k(i)}
\end{equation}
Therefore, to minimize compute and allow client updates, the authors in \cite{yao2019federated} propose instead of computing the numerical value of $ w^{k(i)}_{t}$, to instead keep the relationship between $ w^{k(i)}_{t}$ and $ w^{k(i-1)}_{t}$, termed `keep-trace' of gradients. Then, on the final $E$th epoch, evaluate the aggregated gradients. \\

To address the distribution mismatch, the authors propose a meta dataset $\mathcal{D}_{meta}$ which would evaluate the aggregated weights against the meta-dataset, and perform a meta-weight update that would attempt to `re-align' the underlying dataset selection $\mathcal{D}_{S_t}$ to the true dataset distribution $\mathcal{D}$.

\subsection{Federated Proximal Learning}
To address the heterogeneity concerns from \cite{konevcny2016federated}, the authors from \cite{li2020federated} propose including straggling clients, which may adversely affect performance. To mitigate the performance degradation, the authors include a proximal term to improve stability allowing the server to numerically associate the statistical heterogeneity within clients. They achieve this goal through relaxation of the exactness of the local objectives' optimization. By solving the local objectives inexactly in some $\gamma$-bound, where the number of local epochs, $E$ on device can differ. The amount of $\gamma$-inexactness is calculated per device, per round $t$ allowing the central server to include this bound in the aggregation. \\

\begin{definition}[$\gamma_k^t$-inexactness]\label{def:gminexact}
For function $h_k(w;w_t)  = F_k(w) + \frac{\mu}{2}\Vert w - w_t\Vert^2$ and $\gamma \in [0, 1]$ we say $w^*$ is a $\gamma_k^t$-inexact solution of $\min_{w} h_k$ if $\Vert \nabla h_k(w^*; w_t) \Vert \leq \gamma_k^t \Vert \nabla h_k(w_t; w_t) \Vert$, where $ \nabla h_k(w; w_t) = \nabla F_k(w) + \mu(w-w_t)$
\end{definition}

By comparing the variation between the learned $w$ and the original  $w_t$ sent from the server, the optimization chooses a $w_k^{t+1} \approx \argmin_w h_k(w;w_t) = \argmin_w F_k(w) + \frac{\mu}{2}\Vert w - w_t\Vert^2$. This effectively regulates the local choice $w$ from straying too far from the global optimization. 

\section{Learning Metrics}
\textbf{Convolutional Neural Networks}\\
Convolutional Neural Networks (CNNs) were originally developed in the landmark AlexNet paper \cite{krizhevsky2012imagenet} where Graphical Processing Unit (GPU) was used to parallelize the matrix operations required for sliding kernels over an input image. Since then CNNs have developed significantly more, with Residual Networks \cite{he2016deep} showing the topology of a CNN can be interpreted as a Directed Acyclic Graph (DAG) with skip connections. From this, multiple works have attempted to explain how CNNs learn and what knowledge is contained within the feature extraction tensors. 

\subsection{Feature Extraction through Tensor Decomposition}
CNN learned weigths are a 4-dimensional tensor with shape $k\times k \times N_3 \times N_4$ where $N_3 = C_{in}$ is the input channels and $ N_4 = C_{out}$ is the output channels and $k\times k$ denotes the kernel size. During convolution, the output feature map is convoluted in the following fashion: $\etF^{O}_{:,:,i_4} = \sum_{i_3=1}^{N_3}\etF^{I}_{:,:,i_3}\ast\tW_{:,:,i_3,i_4}$, where ${\etF}^{O}\in\mathbb{R}^{H \times W \times N_{4}}$. 

Since the convolutional weight $\tW$ acts as a learned encoder for feature extraction, Fu \textit{et al} \cite{fu2021conet} was able to unfold this 4D tensor along a dimension, $d$ i.e $\tW [\texttt{4D-tensor}] \rightarrow \tW_d [\texttt{2D-matrix}]$. Now that the tensor is in matrix-form, we can utilize the tools within the 2D domain to asses the weights. Specifically, \cite{fu2021conet} we can extract the noisy initialization from the more valuable learnt encodings, though a Variational Bayes Matrix Factorization (VBMF) \cite{nakajima2013global}. Effectively, \cite{fu2021conet} does the following : $\tW_d = \widehat{\tW_d} + E_{noise}$, where $ \widehat{\tW_d}$ is a low-rank Singular Value Decomposition (SVD). This method takes scales with arbitrary size of a matrix, and is computationally fast on CPU, freeing GPU resources for training.

\begin{figure}[htp]
    \centering
    \includegraphics[width=\textwidth]{images/Low_Rank_Factor_with_Distplots.png}
    \caption{Low Rank Factorization for ResNet, showing noise reduction during training and learned encoding knowledge}
    \label{fig:lowrankdist}
\end{figure}

\subsection{Effective Rank}\label{sec:er}
Building on \cite{fu2021conet}, the work proposed by \cite{jaegerman21genprob} shows that multiple derivative explainability, learning \& quality metrics can be extracted from the $\widehat{\tW_d}$ representation. While the authors test multiple metrics, the effective rank $ER$ had the lowest test-train generalization gap, while maintaining the closest correlation to the test-accuracy. In \cite{jaegerman21genprob}, the effective rank is a borrowed metric from \cite{roy2007effective}, representing the dimension of output space from the transformation operated by a deep layer, calculated using Shannon Entropy of the normalized values of a layer. \\

On a layer-by-layer basis Effective Rank, $ER$ is calculated as follows, where $n'$ denotes the rank of matrix $\tW$ and $\sigma_k (\tW)$ denotes the normalized singular values of Matrix $\tW$:
\begin{equation} \label{eqn:er}
    ER = \sum_{i=1}^{n'} \bar{\sigma_k}(\tW_i) \log \bar{\sigma_k}(\tW_i)
\end{equation}
In order to evaluate an entire layer we utilize the aggregation below:
\begin{equation}\label{eqn:modeler}
    Q_{ER} = \log \Bigg( \sqrt{\sum_{i=1}^d ER(\tW_i)^2 } /d \Bigg)
\end{equation}

Since  \autoref{eqn:er} is the Shannon entropy of the singular values of $\tW$, it measures the amount of entropy of the singular values distribution. As weight matrices $\tW$ maps input and output feature representations, the singular values represent the `axes' of this transformation. As such measuring the entropy of the scaling singular values $\bar\sigma_k$ intuitively represents the amount of `information mapping' between two feature representations.
 
% \subsection{Quality Measure}
% In \cite{hosseini2022explainable} the authors extend the SVD analysis to consider the stable rank and the 
