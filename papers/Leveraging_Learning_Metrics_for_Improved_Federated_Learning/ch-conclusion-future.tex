\chapter{Conclusion \& Future Work}
This work has investigated using novel `learning metrics' to improve Federated Learning, in particular using Effective Rank as the conduit for a novel aggregation method. This work has \textbf{(1)} given the first `learning metrics' aggregation method in a federated setting \textbf{(2)} shown that these `learning metrics', particularly effective rank, is well-suited for this class of problems and \textbf{(3)} developed a novel weight-aggregation scheme relying on these metrics. Therefore, this thesis is concluded by highlighting some open problems and interesting directions for future work. 

\section{Future Work}\label{sec:future-work}
\textbf{Utilizing Stable Rank:} As proposed in \cite{hosseini2022explainable}, stable rank is another novel learning metric that can be used to derive useful results. Furthermore, by utilizing the proof of monotonic increase in stable rank as a learning rate selector, an extension can be proposed to show that a monotonic increase in stable rank across models could lead to improved model aggregations. This is directly related to the next section where convergence of weighted averages of any metric should be proven.\\

\textbf{Proof of convergence:} A proof of convergence must be proposed in order to demonstrate that the effective rank (or any learning metrics) as a weighted average will converge. The basis for this work has already been undertaken in \cite{karimireddy2020scaffold}. \\

\textbf{Leveraging Peer-to-Peer knowledge distillation:} As proposed in \cite{sodhani2020closer}, a peer-to-peer knowledge aggregation scheme can be introduced using co-distillation. Leveraging co-distillation as a method of knowledge aggregation, in combination with learning metrics is an area that is currently unexplored and could pose interesting challenges.\\

\textbf{Non-Identical Models:} As demonstrated in this work, when identical ResNet18 models are trained, they are able to share knowledge seamlessly though the Federated Effective Rank scheme. This scheme breaks down in the case of non-identical models as the layer-by-layer effective rank weighting no longer holds. Consequently, using \autoref{eqn:modeler} would allow non-identical models to leverage effective rank, but at a decreased resolution with respect to the individual weights. Further work into how to aggregate learning metrics per-model while maintaining layer-resolution is an interesting area of research. Additional investigation into a method of analyzing where similar feature representations are being generated and inserting learning metrics at those locations could be a valuable avenue of research.