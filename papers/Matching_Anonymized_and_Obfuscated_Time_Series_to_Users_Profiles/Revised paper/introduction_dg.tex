\section{Introduction}
\label{intro}

Various emerging systems and applications work by analyzing
the data submitted by their users in order to serve them; we call such systems  \emph{user-data driven} (UDD) \amir{we need a name. how's this?} services. Examples of UDD services include  smart cities, connected vehicles, smart homes, and connected healthcare devices, which are 
 revolutionizing our lives in different ways.  
Unfortunately, 	the sheer volume of user data collected by these systems 
compromises users' privacy ostensibly~\cite{FTC2015}. 
Even the employment of  standard 
privacy-protection mechanisms (e.g.,  anonymization of user identities and obfuscation of submitted data) does not mitigate the privacy leakage as the adversaries are able to use powerful statistical inference techniques to infer sensitive private information~\cite{FTC2015,0Quest2016, 2nia2016comprehensive, 3ukil2014iot, 4Hosseinzadeh2014,iotCastle,matching}.
 
To illustrate   their privacy leakage, consider the following three popular UDD services. 
(1) {\em Health care:}  Wearable monitors that constantly track user health variables can be invaluable in assessing individual health trends and responding to emergencies.  However, such monitors produce long time-series of user data uniquely matched to the health characteristics of each  user; (2) {\em Smart homes:}  Emerging smart-home technologies such as fine-grained power measurement systems can help users and utility providers to address one of the key challenges of the twenty-first century:  energy conservation.  But the measurements of power by such devices can be  mapped to users and reveal their lifestyle habits; and, (3) 
	{\em Connected vehicles:}  The location data provided by connected vehicles promises to greatly improve everyday lives by reducing congestion and traffic accidents.  However, the matching of such location traces to prior behavior not only allows for user tracking, but also reveals a user's habits.
In summary, despite their critical importance to the society and their emerging popularity, these services have one thing in common: 
they collect  long time-series of user data, for them to operate, 
that puts users' privacy at significant risk.

There are two main approaches to augment privacy in UDD services: 
 \emph{identity perturbation (anonymization)}, and \emph{location perturbation (obfuscation)}.  In anonymization techniques, privacy is obtained by concealing the mapping between users and data, and the mapping is changed periodically to  thwart statistical inference attacks that try to deanonymize the anonymized data traces by matching user data to 
 known  user profiles. 
 On the other hands,  obfuscation mechanisms aim at protecting privacy by perturbing user data, e.g., by adding noise to user locations. 
% Both anonymization and obfuscation mechanisms manipulate user data time-series in a way to not significantly impact the utility of the underlying UDD services. 


Anonymization and obfuscation improve user privacy at the cost of user utility.  In anonymization, we need to change these pseudonym mappings frequently to achieve high privacy by reducing the length of time series exploited by statistical analysis.  However, this frequent change could decrease usability and functionality by concealing the temporal relation between a user's locations, which may be critical in the utility of some systems, e.g., a dining recommendation system that makes suggestions based on the dining places visited by a user in the past.  For obfuscation-based mechanisms, the added noise to the reported values of user locations will degrade the application utility; for example, user utility would be degraded by location obfuscation in ride-sharing systems.  Thus, choosing the right level of privacy-protection mechanism is an important question, and understanding what levels of anonymization and obfuscation can provide theoretical guarantees of privacy is of interest.

In this paper, we derive the fundamental limits of user privacy in UDD services in the presence of both anonymization and obfuscation protection mechanisms. This work is built on our previous work on formalizing privacy in location-based services~\cite{montazeri2016defining, Mont1610Achieving,tifs2016, ciss2017}, by extensively expanding not just the application area but also user models and settings. 
Particularly, our previous work introduced the notion of 
\emph{perfect privacy} for location-based services, and derives the rate at which an anonymization mechanism should change the pseudonyms in order to 
achieve the defined perfect privacy. 
In this work, we expand the notion of perfect privacy to UDD services in general, and derive the conditions for it to hold when \emph{both}
anonymization and obfuscation-based protection mechanisms are employed.   
 We study both achievability and converse results.  

%In~\cite{montazeri2016defining, Mont1610Achieving,tifs2016, ciss2017}, an approach was introduced to understand the fundamental limits of privacy when only anonymization is used in location-based services. There, users are characterized by the statistics of their locations, and the adversary then tries to match traces to those statistics to de-anonymize users.  Per above, anonymization thwarts such statistical analysis by reducing the time series available for such a matching, and thus \cite{montazeri2016defining, Mont1610Achieving,tifs2016} considers the rate at which pseudonyms must be changed so as to preserve perfect location privacy.   In this paper, the perfect privacy set-up of \cite{tifs2016} is again employed; however, in addition to anonymization, perturbation (obfuscation) is also considered; thus, the adversary attempts to infer information about the actual locations by observing the obfuscated \emph{and} anonymized version of the location data.  We study both achievability and converse results.  

\subsection{Summary of the Results}

Given $n$, the total number of the users in a network, their degree of privacy depends 
on two parameters: (1) The number of observations $m=m(n)$ by the adversary per user for a fixed anonymization mapping (i.e., the number of observations before the pseudonyms are changed);  and (2) the value of the noise added by the obfuscation technique (as defined in Section~\ref{sec:framework}, we quantify the obfuscation noise with a parameter $a_n$).  
Intuitively, smaller $m(n)$ and larger $a_n$ result in stronger privacy, at the expense of lower utility for the users. 

Our goal is to identify values of $a_n$ and $m(n)$  that satisfy perfect privacy as the number of users grows ($n \rightarrow \infty$).
%
We show that when the users' data sets are governed by an i.i.d.\ process, 
the $m(n)- a_n$ plane can be divided into two areas.  In the first area, all users have perfect privacy~\cite{tifs2016} (as defined in Section~\ref{sec:framework}), and, in the second area, users have no privacy.  
Figure~\ref{fig:region} shows the limits of privacy in the entire $m(n)- a_n$. 
As the figure shows, in regions $1$, $2$, and $3$, users have perfect privacy, while in region $4$ users have no privacy.
\begin{figure}[h]
	\centering
	\includegraphics[width=0.8\linewidth, height=0.8 \linewidth]{fig/region}
	\caption{Limits of privacy in the entire $m(n)-a_n$ plane: in regions $1$, $2$, and $3$, users have perfect privacy, and in region $4$ users have no privacy.}
	\label{fig:region}
\end{figure}

For the case where the users' data sets are governed by irreducible and aperiodic Markov chains with $r$ states and $|E|$ edges, we show that users will have no privacy if $m =cn^{\frac{2}{|E|-r} +  \alpha}$ and $a_n =c'n^{-\left(\frac{1}{|E|-r}+\beta \right)}$, for some constants $c>0$, $c'>0$, $\alpha>0$, and $\beta>\frac{\alpha}{4}$. We also provide some insights for the opposite direction (under which conditions users have perfect privacy) for the case of Markov chains. 




  




%\subsection{Summary of Results}

%When there are $n$ users in the network, the privacy level of the users depends on two parameters: (1) The %number of observations $m=m(n)$ by the adversary per user for a fixed anonymization mapping (i.e.~the %number of observations before the pseudonyms are changed);  (2) The noise level of the obfuscation %technique.  In this paper, the noise level of the obfuscation is increasing in a parameter $a_n$, which is %defined precisely in Section \ref{sec:framework}.  Intuitively, smaller $m(n)$ and larger $a_n$ result in %stronger location privacy but lower user utility. Thus, the fundamental question is, as the number of users %grows ($n \rightarrow \infty$), what values of $a_n$ and $m(n)$ are required to insure location privacy?

%In this paper, we answer the above question for the entire $m(n)- a_n$ plane for the case where each users' %movement is governed by an i.i.d process. We will prove that the $m(n)- a_n$ plane can be divided into two %areas.  In the first area, all users have perfect location privacy\cite{tifs2016}, as will be defined precisely in %Section \ref{sec:framework}, and, in the second area, users have no location privacy.  Figure~\ref{fig:region} %shows the limits of location privacy in the entire $m(n)- a_n$. In other words, according to %Figure~\ref{fig:region}, in regions $1$, $2$, and $3$, users have perfect location privacy, and  in region $4$ %users have no location privacy.
%\begin{figure}[h]
%	\centering
%	\includegraphics[width=0.9\linewidth, height=0.6 \linewidth]{fig/region}
%	\caption{Limits of location privacy in the entire $m(n)-a_n$ planes: in regions $1$, $2$, and $3$, users %have perfect location privacy, and in region $4$ users have no location privacy.}
%	\label{fig:region}
%\end{figure}
%
%Also, for the case where the users movements are governed by irreducible and aperiodic Markov chains with %$r$ states and $|E|$ edges, we show that users will have no privacy if $m =cn^{\frac{2}{|E|-r} +  \alpha}$ %and $a_n =c'n^{-\left(\frac{1}{|E|-r}+\alpha \right)}$ for some positive constants  $c, c'>0$  and %$0<\alpha<1$. Showing the opposite direction (under which conditions users have perfect privacy) for the %case of Markov chains is among the future work.

%In the rest of the paper we will provide the details of the results. Due to the space limitations, the proofs can %be found in the publicly available longer version of the paper ~\cite{longversion}.
