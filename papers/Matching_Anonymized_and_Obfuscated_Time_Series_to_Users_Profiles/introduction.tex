\section{Introduction}
\label{intro}

A number of emerging systems and applications work by analyzing the data submitted by their users in order to serve them; we call such systems \emph{User-Data Driven} (UDD) services. Examples of UDD services include smart cities, connected vehicles, smart homes, and connected healthcare devices, which have the promise of greatly improving users' lives. Unfortunately, the sheer volume of user data collected by these systems can compromise users' privacy~\cite{FTC2015}. Even the use of standard Privacy-Protection Mechanisms (PPMs), specifically anonymization of user identities and obfuscation of submitted data, does not guarantee users' privacy, as adversaries are able to use powerful statistical inference techniques to learn sensitive private information of the users~\cite{0Quest2016, 3ukil2014iot, 4Hosseinzadeh2014,iotCastle,matching}.

To illustrate the threat of privacy leakage, consider three popular UDD services: (1) {\em Health care:}  Wearable monitors that constantly track user health variables can be invaluable in assessing individual health trends and responding to emergencies.  However, such monitors produce long time-series of user data uniquely matched to the health characteristics of each user; (2) {\em Smart homes:} Emerging smart-home technologies such as fine-grained power measurement systems can help users and utility providers to address one of the key challenges of the twenty-first century:  energy conservation.  But the measurements of power by such devices can be mapped to users and reveal their lifestyle habits; and, (3) {\em Connected vehicles:}  The location data provided by connected vehicles promises to greatly improve everyday life by reducing congestion and traffic accidents.  However, the matching of such location traces to prior behavior not only allows for user tracking, but also reveals a user's habits.  In summary, despite their potential impact on society and their emerging popularity, these UDD services have one thing in common: their utility critically depends on their collection of user data, which puts users' privacy at significant risk.

There are two main approaches to augment privacy in UDD services: \emph{identity perturbation (anonymization)}~\cite{1corser2016evaluating,hoh2005protecting,freudiger2007mix, ma2009location, shokri2011quantifying2, Naini2016,soltani2017towards, soltani2018invisible}, and \emph{data perturbation (obfuscation)}~\cite{shokri2012protecting, gruteser2003anonymous, bordenabe2014optimal}. In anonymization techniques, privacy is obtained by concealing the mapping between users and data, and the mapping is changed periodically to thwart statistical inference attacks that try to de-anonymize the anonymized data traces by matching user data to known user profiles. Some approaches employ $k$-anonymity to keep each user's identity indistinguishable within a group of $k-1$ other users ~\cite{2zhang2016designing,11dewri2014exploiting, gedik2005location, zhong2009distributed, sweeney2002k, kalnis2007preventing,liu2013game}.  Other approaches employ users' pseudonyms within areas called mix-zones~\cite{beresford2003location, freudiger2009optimal, palanisamy2011mobimix}.
Obfuscation mechanisms aim at protecting privacy by perturbing user data, e.g., by adding noise to users' samples of data.   For instance, cloaking replaces each user's sample of data with a larger region~\cite{18shokri2014hiding,8zurbaran2015near,hoh2007preserving, wernke2014classification, chow2011spatial, um2010advanced}, while an alternative approach is to use dummy data in the set of possible data of the users~\cite{kido2005protection, shankar2009privately, chow2009faking, kido2005anonymous, lu2008pad}. In~\cite{randomizedresponse}, a mechanism of obfuscation was introduced where the answer was changed randomly with some small probability. Here we consider the fundamental limits of a similar obfuscation technique for providing privacy in the long time series of emerging applications.

%In~\cite{randomizedresponse}, the mechanism of obfuscation is similar to our work, but the context of this paper is different from our work. First, in~\cite{randomizedresponse}, only a single point of data was considered for each user; however, we consider a long time series of data for each user. Second, in~\cite{randomizedresponse}, no prior distribution for users' sample of data was considered, but we assume there is a prior distribution for each of the users' data samples. In addition, we assume the strongest adversary who has complete statistical knowledge about users. Finally, in~\cite{randomizedresponse}, the privacy of users was provided when there were only two states for users' sample of data, and the obfuscation mechanism for r-states model wasn't studied.

The anonymization and obfuscation mechanisms improve user privacy at the cost of user utility. The anonymization mechanism works by frequently changing the pseudonym mappings of users to reduce the length of time series that can be exploited by statistical analysis.  However, this frequent change may also decrease the usability by concealing the temporal relation between a user's sample of data, which may be critical in the utility of some systems, e.g., a dining recommendation system that makes suggestions based on the dining history of its users. On the other hand, obfuscation mechanisms work by adding noise to users' collected data, e.g., location information. The added noise may degrade the utility of UDD applications.  Thus, choosing the right level of the privacy-protection mechanism is an important question, and understanding what levels of anonymization and obfuscation can provide theoretical guarantees of privacy is of interest.

%Differential privacy aims at protecting queries on aggregated data.
%Particularly, several identity perturbation~\cite{lee2012differential, chatzikokolakis2015geo, nguyen2013differential, machanavajjhala2008privacy} and data perturbation ~\cite{chatzikokolakis2013broadening,shokri2014optimal, chatzikokolakis2015location,andres2013geo} PPMs are based on differential privacy techniques.
%Dewri~\cite{dewri2013local} combines k-anonymity and differential privacy to achieve higher privacy. In ~\cite{info2012}, it  is proved that differential privacy arises out of maximizing entropy principle which is equal to minimizing information leakage and is measured using the mutual information notion.~\cite{yeb17} tries to reduce the expected estimation loss by using differential privacy and constraining the privacy level.

In this paper, we will consider the ability of an adversary to perform statistical analyses on time series and match the series to descriptions of user behavior. In related work, Unnikrishnan~\cite{matching} provides a comprehensive analysis of the asymptotic (in the length of the time series) optimal matching of time series to source distributions. However, there are several key differences between that analysis and the work here. First, Unnikrishnan~\cite{matching} looks at the optimal matching tests, but does not consider any privacy metrics as considered in this paper, and a significant component of our study is demonstrating that mutual information converges to zero so that we can conclude there is no privacy leakage (hence, ``perfect privacy'').  Second, the setting of~\cite{matching} is different, as it does not consider: (a) obfuscation, which is one of the two major protection mechanisms; and (b) sources that are not independent and identically distributed (i.i.d.). Third, the setting of Unnikrishnan~\cite{matching} assumes a fixed distribution on sources (i.e., classical inference), whereas we assume the existence of general (but possibly unknown) prior distributions for the sources (i.e., a Bayesian setting).  Finally, we study the fundamental limits in terms of both the number of users and the number of observations, while Unnikrishnan~\cite{matching} focuses on the case where the number of users is a fixed, finite value.

Numerous researchers have put forward ideas for quantifying privacy-protection. Shokri et al.~\cite{shokri2011quantifying, shokri2011quantifying2} define the expected estimation error of the adversary as a metric to evaluate PPMs.  Ma et al.~\cite{ma2009location} use uncertainty about users' information to quantify user privacy in vehicular networks.
To defeat localization attacks and achieve privacy at the same time, Shokri et al.~\cite{shokri2012protecting} proposed a method which finds optimal PPM for an LBS given service quality constraints.
In~\cite{6li2016privacy} and~\cite{4olteanu2016quantifying}, privacy leakage of data sharing and interdependent privacy risks are quantified, respectively. A similar idea is proposed in \cite{14zhang2014privacy} where the quantification model is based on the Bayes conditional risk. %Yu et al. \cite{diff2017} combine two complementary notations, geo-indstingushability and expected inference error, to achieve location privacy. The geo-indstingushability is derived from differential privacy but cannot adequately protect users' location against inference attacks of adversary by using prior information, and the expected inference error as privacy metric doesn't consider posterior information obtained from  pseudo-location. As a result, combining both notations gives users the opportunity to have location privacy. In addition, users are allowed to personalized error bound for different locations.
Previously, mutual information has been used as a privacy metric in a number of settings,~\cite{kousha3,salamatian2013hide, csiszar1996almost, calmon2015fundamental, sankar2013utility, sankarISIT,sankar, yamamoto1983source, hyposankar}. However, the framework and problem formulation for our setting (Internet of Things (IoT) privacy) are quite different from those encountered in previous works. More specifically, the IoT privacy problem we consider here is based on a large set of time-series data that belongs to different users with different statistical patterns that has gone through a privacy-preserving mechanism, and the adversary is aiming at de-anonymizing and de-obfuscating the data.

The discussed studies demonstrate the growing importance of privacy. What is missing from the current literature is a solid theoretical framework for privacy that is general enough to encompass various privacy-preserving methods in the literature. Such a framework will allow us to achieve provable privacy guarantees, obtain fundamental trade-offs between privacy and performance, and provide analytical tools to optimally achieve provable privacy. We derive the fundamental limits of user privacy in UDD services in the presence of both anonymization and obfuscation protection mechanisms.  We build on our previous works on formalizing privacy in location-based services~\cite{tifs2016, ciss2017}, but we significantly expand those works here not just in application area but also user models and settings. In particular, our previous works introduced the notion of \emph{perfect privacy} for location-based services, and we derived the rate at which an anonymization mechanism should change the pseudonyms in order to achieve the defined perfect privacy. In this work, we expand the notion of perfect privacy to UDD services in general and derive the conditions for it to hold when \emph{both} anonymization and obfuscation-based protection mechanisms are employed.

In this paper, we consider two models for users' data: i.i.d.\ and Markov chains. After introducing the general framework in Section \ref{sec:framework}, we consider an i.i.d.\ model extensively in Section \ref{perfectsec} and the first half of Section \ref{converse}. We obtain achievability and converse results for the i.i.d.\ model. The i.i.d.\ model would apply directly to data that is sampled at a low rate. In addition, understanding the i.i.d.\ case can also be considered the first step toward understanding the more complicated case where there is dependency, as was done for anonymization-only Location Privacy-Preserving Mechanisms (LPPMs) in \cite{tifs2016}, and will be done in Section \ref{subsec:markov}. In particular, in Section \ref{subsec:markov}, a general Markov chain model is used to model users' data pattern to capture the dependency of the user' data pattern over time. There, we obtain converse results for privacy for this model. In Section \ref{sec:perfect-MC}, we provide some discussion about the achievability for the Markov chain case.


\subsection{Summary of the Results}

Given $n$, the total number of the users in a network, their degree of privacy depends
on two parameters: (1) The number of observations $m=m(n)$ by the adversary per user for a fixed anonymization mapping (i.e., the number of observations before the pseudonyms are changed);  and (2) the value of the noise added by the obfuscation technique (as defined in Section~\ref{sec:framework}, we quantify the obfuscation noise with a parameter $a_n$, where larger $a_n$ means a higher level of obfuscation).  Intuitively, smaller $m(n)$ and larger $a_n$ result in stronger privacy, at the expense of lower utility for the users.

Our goal is to identify values of $a_n$ and $m(n)$ that satisfy perfect privacy in the asymptote of a large number of users ($n \rightarrow \infty$).
%
When the users' datasets are governed by an i.i.d.\ process, we show that
the $m(n)- a_n$ plane can be divided into two areas.  In the first area, all users have perfect privacy (as defined in Section~\ref{sec:framework}), and, in the second area, users have no privacy.
Figure~\ref{fig:region} shows the limits of privacy in the entire $m(n)- a_n$ plane.
As the figure shows, in regions $1$, $2$, and $3$, users have perfect privacy, while in region $4$ users have no privacy.
\begin{figure}[h]
	\centering
	\includegraphics[width=0.7\linewidth, height=0.6 \linewidth]{fig/region}
	\caption{Limits of privacy in the entire $m(n)-a_n$ plane: in regions $1$, $2$, and $3$, users have perfect privacy, and in region $4$ users have no privacy.}
	\label{fig:region}
\end{figure}

For the case where the users' datasets are governed by irreducible and aperiodic Markov chains with $r$ states and $|E|$ edges, we show that users will have no privacy if $m =cn^{\frac{2}{|E|-r} +  \alpha}$ and $a_n =c'n^{-\left(\frac{1}{|E|-r}+\beta \right)}$, for any constants $c>0$, $c'>0$, $\alpha>0$, and $\beta>\frac{\alpha}{4}$. We also provide some insights for the opposite direction (under which conditions users have perfect privacy) for the case of Markov chains.









%\subsection{Summary of Results}

%When there are $n$ users in the network, the privacy level of the users depends on two parameters: (1) The %number of observations $m=m(n)$ by the adversary per user for a fixed anonymization mapping (i.e.~the %number of observations before the pseudonyms are changed);  (2) The noise level of the obfuscation %technique.  In this paper, the noise level of the obfuscation is increasing in a parameter $a_n$, which is %defined precisely in Section \ref{sec:framework}.  Intuitively, smaller $m(n)$ and larger $a_n$ result in %stronger location privacy but lower user utility. Thus, the fundamental question is, as the number of users %grows ($n \rightarrow \infty$), what values of $a_n$ and $m(n)$ are required to insure location privacy?

%In this paper, we answer the above question for the entire $m(n)- a_n$ plane for the case where each users' %movement is governed by an i.i.d process. We will prove that the $m(n)- a_n$ plane can be divided into two %areas.  In the first area, all users have perfect location privacy\cite{tifs2016}, as will be defined precisely in %Section \ref{sec:framework}, and, in the second area, users have no location privacy.  Figure~\ref{fig:region} %shows the limits of location privacy in the entire $m(n)- a_n$. In other words, according to %Figure~\ref{fig:region}, in regions $1$, $2$, and $3$, users have perfect location privacy, and  in region $4$ %users have no location privacy.
%\begin{figure}[h]
%	\centering
%	\includegraphics[width=0.9\linewidth, height=0.6 \linewidth]{fig/region}
%	\caption{Limits of location privacy in the entire $m(n)-a_n$ planes: in regions $1$, $2$, and $3$, users %have perfect location privacy, and in region $4$ users have no location privacy.}
%	\label{fig:region}
%\end{figure}
%
%Also, for the case where the users movements are governed by irreducible and aperiodic Markov chains with %$r$ states and $|E|$ edges, we show that users will have no privacy if $m =cn^{\frac{2}{|E|-r} +  \alpha}$ %and $a_n =c'n^{-\left(\frac{1}{|E|-r}+\alpha \right)}$ for some positive constants  $c, c'>0$  and %$0<\alpha<1$. Showing the opposite direction (under which conditions users have perfect privacy) for the %case of Markov chains is among the future work.

%In the rest of the paper we will provide the details of the results. Due to the space limitations, the proofs can %be found in the publicly available longer version of the paper ~\cite{longversion}.
