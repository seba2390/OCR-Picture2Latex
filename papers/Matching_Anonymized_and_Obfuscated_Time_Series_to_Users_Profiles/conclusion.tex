\section{Discussion}
\label{discussion}
\subsection{Markov Chain Model}
\label{markov}
As opposed to the i.i.d.\ case, we see from Section \ref{sec:perfect-MC} that if we do not limit $m=m(n)$, the assumed obfuscation method will not be sufficient to achieve perfect privacy. There are a few natural questions here. First, for a given noise level, what would be the maximum $m(n)$ that could guarantee perfect privacy in this model? The more interesting question is, how can we possibly modify the obfuscation technique to make it more suitable for the Markov chain model? A natural solution seems to be re-generating the obfuscation random variables $R_u$ periodically. This will keep the adversary from easily estimating them by observing a long sequence of data at a small increase in complexity. In fact, this will make the obfuscation much more \emph{robust} to modeling uncertainties and errors. It is worth noting, however, that this change would not affect the other results in the paper. That is, even if the obfuscation random variables are re-generated frequently, it is relatively easy to check that all the previous theorems in the paper remain valid. However, the increase in robustness to modeling errors will definitely be a significant advantage. Thus, the question is how often should the random variable $R_u$ be re-generated to strike a good balance between complexity and privacy? These are all interesting questions for future research.


\subsection{Obfuscating the Samples of Users' Data Using Continuous Noise}
Here we argue that for the setting of this paper, continuous noise such as that drawn from a Gaussian distribution is not a good option to obfuscate the sample of users' data drawn from a finite alphabet when we want to achieve perfect privacy. For a better understanding, let's consider a simple example.
\begin{example}
Consider the scenario where the users' datasets are governed by an i.i.d.\ model and the number of possible values for each sample of the users' data ($r$) is equal to 2 (two-states model). Note that the data sequence for user $u$ is a Bernoulli random variable with parameter $p_u$.

Assume that the actual sample of the data of user $u$ at time $k$ ($X_u(k)$) is obfuscated using noise drawn from a Gaussian distribution ($S_u(k)$), and $Z_u(k)$ is the obfuscated version of $X_u(k)$.  That is, we can write
\[Z_u(k)=X_u(k)+S_u(k); \ \ \ \ \  S_u(k) \sim N\left(0, R_u\right), \]
where $R_u$ is chosen from some distribution. For simplicity, we can consider $R_u\sim N\left(0, a^2_n\right)$ where $a_n$ is the noise level.

We also apply anonymization to $Z_u(k)$, and, as before, $Y_u(k)$ is the reported sample of the data of user $u$ at time $k$ after applying anonymization.  Per Section \ref{sec:framework}, anonymization is modeled by a random permutation $\Pi(u)$ on the set of $n$ users.

Now, the question is as follows: Is it possible to achieve perfect privacy independent of the number of adversary's observation ($m$) while using this continuous noise ($S_u(k)$) to obfuscate the sample of users' data?

Note that the density function of the reported sample of the data of user $u$ after applying obfuscation is
\begin{align}
%\nonumber
\no f_{Z_u}(z)&= p_u f_{S_u(k)}(z-1)+(1-p_u) f_{S_u(k)}(z) \\
\nonumber &= p_u \frac{1}{\sqrt{2\pi}R_u}e^{-\frac{(z-1)^2}{2R_u}}+(1-p_u)\frac{1}{\sqrt{2\pi}R_u}e^{-\frac{z^2}{2R_u}}.\ \
\end{align}
In this case, when the adversary's number of observations is large, the adversary can estimate the values of $P_u$ and $R_u$ for each user with an arbitrarily small error probability. As a result, the adversary can de-anonymize the data and then recover $X_u(k)$. The conclusion here is that a continuous noise distribution gives too much information to the adversary when used for obfuscation of finite alphabet data. A method to remedy this issue is to regenerate the random variables $R_u$ frequently (similar to our previous discussion for Markov chains). Understanding the optimal frequency of such a regeneration and detailed analysis in this case is an interesting future research direction.
\end{example}

%As a result, when we have finite alphabet possibilities for the sample of users' data and use continous noise to obfuscate the sample of users' data, the adversary has too much information to recover the sample of users' data. Thus it is not a good option to achieve perfect privacy.


\subsection{Relation to Differential Privacy}
Differential privacy is mainly used when there is a statistical database of users' sensitive information, and the goal is to protect an individual's data while publishing aggregate information about the database \cite{ lee2012differential, bordenabe2014optimal, chatzikokolakis2015geo, nguyen2013differential, machanavajjhala2008privacy, kousha2}. The goal of differential privacy is publishing aggregate queries with low sensitivity, which means the effect of changes in a single individual on the outcome of the aggregated information is negligible.

In \cite{geo2013} three different approaches for differential privacy are presented. The one that best matches our setting is stated as
\begin{align*}
\frac{P(X_1(k)=x_1|\textbf{Y})}{P(X_1(k)=x_2|\textbf{Y})} \leq e^{\epsilon_r}\frac{P(X_1(k)=x_1)}{P(X_1(k)=x_2)},
\end{align*}
where $\textbf{Y}$ is the set of reported datasets.
It means $\textbf{Y}$ has a limited effect on the probabilities assigned by the attacker. In differential privacy, user $1$ has strongest differential privacy when $\epsilon_r=0$.

In Lemma \ref{lem4}, we proved that if user $1$ has perfect privacy, this implies that asymptotically (for large enough $n$)
\begin{align}\label{eq1}
P\left(X_1(k)=x_1 \big{|} \textbf{Y}\right)\rightarrow P\left(X_1(k)=x_1\right).
\end{align}
\begin{align}\label{eq2}
P\left(X_1(k)=x_2 \big{|} \textbf{Y}\right)\rightarrow P\left(X_1(k)=x_2\right).
\end{align}

As a result, by using (\ref{eq1}) and (\ref{eq2}), we can conclude that if we satisfy the perfect privacy condition given in this paper, we also satisfy differential privacy with $\epsilon_r=0$, i.e., the strongest case of differential privacy.




\section{Conclusions}
In this paper, we have considered both obfuscation and anonymization techniques to achieve privacy.  The privacy level of the users depends on both $m(n)$ (number of observations per user by the adversary for a fixed anonymization mapping) and $a_n$ (noise level). That is, larger $m(n)$ and smaller $a_n$ indicate weaker privacy.  We characterized the limits of privacy in the entire $m(n)-a_n$ plane for the i.i.d.\ case; that is, we obtained the exact values of the thresholds for $m(n)$ and $a_n$ required for privacy to be maintained.  We showed that if $m(n)$ is fewer than $O\left(n^{\frac{2}{r-1}}\right)$, or $a_n$ is larger than $\Omega\left(n^{-\frac{1}{r-1}}\right)$, users have perfect privacy. On the other hand, if neither of these two conditions is satisfied, users have no privacy. For the case where the users' patterns are modeled by Markov chains, we obtained a no-privacy region in the $m(n)-a_n$ plane.

Future research in this area needs to characterize the exact privacy/no-privacy regions when user data sequences obey Markov models. It is also important to consider different ways to obfuscate users' data sets and study the utility-privacy trade-offs for different types of obfuscation techniques.
