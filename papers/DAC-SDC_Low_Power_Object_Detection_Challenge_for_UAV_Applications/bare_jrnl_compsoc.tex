
%% bare_jrnl_compsoc.tex
%% V1.4b
%% 2015/08/26
%% by Michael Shell
%% See:
%% http://www.michaelshell.org/
%% for current contact information.
%%
%% This is a skeleton file demonstrating the use of IEEEtran.cls
%% (requires IEEEtran.cls version 1.8b or later) with an IEEE
%% Computer Society journal paper.
%%
%% Support sites:
%% http://www.michaelshell.org/tex/ieeetran/
%% http://www.ctan.org/pkg/ieeetran
%% and
%% http://www.ieee.org/

%%*************************************************************************
%% Legal Notice:
%% This code is offered as-is without any warranty either expressed or
%% implied; without even the implied warranty of MERCHANTABILITY or
%% FITNESS FOR A PARTICULAR PURPOSE!
%% User assumes all risk.
%% In no event shall the IEEE or any contributor to this code be liable for
%% any damages or losses, including, but not limited to, incidental,
%% consequential, or any other damages, resulting from the use or misuse
%% of any information contained here.
%%
%% All comments are the opinions of their respective authors and are not
%% necessarily endorsed by the IEEE.
%%
%% This work is distributed under the LaTeX Project Public License (LPPL)
%% ( http://www.latex-project.org/ ) version 1.3, and may be freely used,
%% distributed and modified. A copy of the LPPL, version 1.3, is included
%% in the base LaTeX documentation of all distributions of LaTeX released
%% 2003/12/01 or later.
%% Retain all contribution notices and credits.
%% ** Modified files should be clearly indicated as such, including  **
%% ** renaming them and changing author support contact information. **
%%*************************************************************************


% *** Authors should verify (and, if needed, correct) their LaTeX system  ***
% *** with the testflow diagnostic prior to trusting their LaTeX platform ***
% *** with production work. The IEEE's font choices and paper sizes can   ***
% *** trigger bugs that do not appear when using other class files.       ***                          ***
% The testflow support page is at:
% http://www.michaelshell.org/tex/testflow/


\documentclass[10pt,journal,compsoc]{IEEEtran}
%
% If IEEEtran.cls has not been installed into the LaTeX system files,
% manually specify the path to it like:
% \documentclass[10pt,journal,compsoc]{../sty/IEEEtran}

\usepackage{color}
\usepackage{graphicx}
\usepackage{url}
\usepackage{subfigure}

% Some very useful LaTeX packages include:
% (uncomment the ones you want to load)


% *** MISC UTILITY PACKAGES ***
%
%\usepackage{ifpdf}
% Heiko Oberdiek's ifpdf.sty is very useful if you need conditional
% compilation based on whether the output is pdf or dvi.
% usage:
% \ifpdf
%   % pdf code
% \else
%   % dvi code
% \fi
% The latest version of ifpdf.sty can be obtained from:
% http://www.ctan.org/pkg/ifpdf
% Also, note that IEEEtran.cls V1.7 and later provides a builtin
% \ifCLASSINFOpdf conditional that works the same way.
% When switching from latex to pdflatex and vice-versa, the compiler may
% have to be run twice to clear warning/error messages.






% *** CITATION PACKAGES ***
%
\ifCLASSOPTIONcompsoc
  % IEEE Computer Society needs nocompress option
  % requires cite.sty v4.0 or later (November 2003)
  \usepackage[nocompress]{cite}
\else
  % normal IEEE
  \usepackage{cite}
\fi
% cite.sty was written by Donald Arseneau
% V1.6 and later of IEEEtran pre-defines the format of the cite.sty package
% \cite{} output to follow that of the IEEE. Loading the cite package will
% result in citation numbers being automatically sorted and properly
% "compressed/ranged". e.g., [1], [9], [2], [7], [5], [6] without using
% cite.sty will become [1], [2], [5]--[7], [9] using cite.sty. cite.sty's
% \cite will automatically add leading space, if needed. Use cite.sty's
% noadjust option (cite.sty V3.8 and later) if you want to turn this off
% such as if a citation ever needs to be enclosed in parenthesis.
% cite.sty is already installed on most LaTeX systems. Be sure and use
% version 5.0 (2009-03-20) and later if using hyperref.sty.
% The latest version can be obtained at:
% http://www.ctan.org/pkg/cite
% The documentation is contained in the cite.sty file itself.
%
% Note that some packages require special options to format as the Computer
% Society requires. In particular, Computer Society  papers do not use
% compressed citation ranges as is done in typical IEEE papers
% (e.g., [1]-[4]). Instead, they list every citation separately in order
% (e.g., [1], [2], [3], [4]). To get the latter we need to load the cite
% package with the nocompress option which is supported by cite.sty v4.0
% and later. Note also the use of a CLASSOPTION conditional provided by
% IEEEtran.cls V1.7 and later.





% *** GRAPHICS RELATED PACKAGES ***
%
\ifCLASSINFOpdf
  % \usepackage[pdftex]{graphicx}
  % declare the path(s) where your graphic files are
  % \graphicspath{{../pdf/}{../jpeg/}}
  % and their extensions so you won't have to specify these with
  % every instance of \includegraphics
  % \DeclareGraphicsExtensions{.pdf,.jpeg,.png}
\else
  % or other class option (dvipsone, dvipdf, if not using dvips). graphicx
  % will default to the driver specified in the system graphics.cfg if no
  % driver is specified.
  % \usepackage[dvips]{graphicx}
  % declare the path(s) where your graphic files are
  % \graphicspath{{../eps/}}
  % and their extensions so you won't have to specify these with
  % every instance of \includegraphics
  % \DeclareGraphicsExtensions{.eps}
\fi
% graphicx was written by David Carlisle and Sebastian Rahtz. It is
% required if you want graphics, photos, etc. graphicx.sty is already
% installed on most LaTeX systems. The latest version and documentation
% can be obtained at:
% http://www.ctan.org/pkg/graphicx
% Another good source of documentation is "Using Imported Graphics in
% LaTeX2e" by Keith Reckdahl which can be found at:
% http://www.ctan.org/pkg/epslatex
%
% latex, and pdflatex in dvi mode, support graphics in encapsulated
% postscript (.eps) format. pdflatex in pdf mode supports graphics
% in .pdf, .jpeg, .png and .mps (metapost) formats. Users should ensure
% that all non-photo figures use a vector format (.eps, .pdf, .mps) and
% not a bitmapped formats (.jpeg, .png). The IEEE frowns on bitmapped formats
% which can result in "jaggedy"/blurry rendering of lines and letters as
% well as large increases in file sizes.
%
% You can find documentation about the pdfTeX application at:
% http://www.tug.org/applications/pdftex






% *** MATH PACKAGES ***
%
%\usepackage{amsmath}
% A popular package from the American Mathematical Society that provides
% many useful and powerful commands for dealing with mathematics.
%
% Note that the amsmath package sets \interdisplaylinepenalty to 10000
% thus preventing page breaks from occurring within multiline equations. Use:
%\interdisplaylinepenalty=2500
% after loading amsmath to restore such page breaks as IEEEtran.cls normally
% does. amsmath.sty is already installed on most LaTeX systems. The latest
% version and documentation can be obtained at:
% http://www.ctan.org/pkg/amsmath





% *** SPECIALIZED LIST PACKAGES ***
%
%\usepackage{algorithmic}
% algorithmic.sty was written by Peter Williams and Rogerio Brito.
% This package provides an algorithmic environment fo describing algorithms.
% You can use the algorithmic environment in-text or within a figure
% environment to provide for a floating algorithm. Do NOT use the algorithm
% floating environment provided by algorithm.sty (by the same authors) or
% algorithm2e.sty (by Christophe Fiorio) as the IEEE does not use dedicated
% algorithm float types and packages that provide these will not provide
% correct IEEE style captions. The latest version and documentation of
% algorithmic.sty can be obtained at:
% http://www.ctan.org/pkg/algorithms
% Also of interest may be the (relatively newer and more customizable)
% algorithmicx.sty package by Szasz Janos:
% http://www.ctan.org/pkg/algorithmicx




% *** ALIGNMENT PACKAGES ***
%
%\usepackage{array}
% Frank Mittelbach's and David Carlisle's array.sty patches and improves
% the standard LaTeX2e array and tabular environments to provide better
% appearance and additional user controls. As the default LaTeX2e table
% generation code is lacking to the point of almost being broken with
% respect to the quality of the end results, all users are strongly
% advised to use an enhanced (at the very least that provided by array.sty)
% set of table tools. array.sty is already installed on most systems. The
% latest version and documentation can be obtained at:
% http://www.ctan.org/pkg/array


% IEEEtran contains the IEEEeqnarray family of commands that can be used to
% generate multiline equations as well as matrices, tables, etc., of high
% quality.




% *** SUBFIGURE PACKAGES ***
%\ifCLASSOPTIONcompsoc
%  \usepackage[caption=false,font=footnotesize,labelfont=sf,textfont=sf]{subfig}
%\else
%  \usepackage[caption=false,font=footnotesize]{subfig}
%\fi
% subfig.sty, written by Steven Douglas Cochran, is the modern replacement
% for subfigure.sty, the latter of which is no longer maintained and is
% incompatible with some LaTeX packages including fixltx2e. However,
% subfig.sty requires and automatically loads Axel Sommerfeldt's caption.sty
% which will override IEEEtran.cls' handling of captions and this will result
% in non-IEEE style figure/table captions. To prevent this problem, be sure
% and invoke subfig.sty's "caption=false" package option (available since
% subfig.sty version 1.3, 2005/06/28) as this is will preserve IEEEtran.cls
% handling of captions.
% Note that the Computer Society format requires a sans serif font rather
% than the serif font used in traditional IEEE formatting and thus the need
% to invoke different subfig.sty package options depending on whether
% compsoc mode has been enabled.
%
% The latest version and documentation of subfig.sty can be obtained at:
% http://www.ctan.org/pkg/subfig




% *** FLOAT PACKAGES ***
%
%\usepackage{fixltx2e}
% fixltx2e, the successor to the earlier fix2col.sty, was written by
% Frank Mittelbach and David Carlisle. This package corrects a few problems
% in the LaTeX2e kernel, the most notable of which is that in current
% LaTeX2e releases, the ordering of single and double column floats is not
% guaranteed to be preserved. Thus, an unpatched LaTeX2e can allow a
% single column figure to be placed prior to an earlier double column
% figure.
% Be aware that LaTeX2e kernels dated 2015 and later have fixltx2e.sty's
% corrections already built into the system in which case a warning will
% be issued if an attempt is made to load fixltx2e.sty as it is no longer
% needed.
% The latest version and documentation can be found at:
% http://www.ctan.org/pkg/fixltx2e


%\usepackage{stfloats}
% stfloats.sty was written by Sigitas Tolusis. This package gives LaTeX2e
% the ability to do double column floats at the bottom of the page as well
% as the top. (e.g., "\begin{figure*}[!b]" is not normally possible in
% LaTeX2e). It also provides a command:
%\fnbelowfloat
% to enable the placement of footnotes below bottom floats (the standard
% LaTeX2e kernel puts them above bottom floats). This is an invasive package
% which rewrites many portions of the LaTeX2e float routines. It may not work
% with other packages that modify the LaTeX2e float routines. The latest
% version and documentation can be obtained at:
% http://www.ctan.org/pkg/stfloats
% Do not use the stfloats baselinefloat ability as the IEEE does not allow
% \baselineskip to stretch. Authors submitting work to the IEEE should note
% that the IEEE rarely uses double column equations and that authors should try
% to avoid such use. Do not be tempted to use the cuted.sty or midfloat.sty
% packages (also by Sigitas Tolusis) as the IEEE does not format its papers in
% such ways.
% Do not attempt to use stfloats with fixltx2e as they are incompatible.
% Instead, use Morten Hogholm'a dblfloatfix which combines the features
% of both fixltx2e and stfloats:
%
% \usepackage{dblfloatfix}
% The latest version can be found at:
% http://www.ctan.org/pkg/dblfloatfix




%\ifCLASSOPTIONcaptionsoff
%  \usepackage[nomarkers]{endfloat}
% \let\MYoriglatexcaption\caption
% \renewcommand{\caption}[2][\relax]{\MYoriglatexcaption[#2]{#2}}
%\fi
% endfloat.sty was written by James Darrell McCauley, Jeff Goldberg and
% Axel Sommerfeldt. This package may be useful when used in conjunction with
% IEEEtran.cls'  captionsoff option. Some IEEE journals/societies require that
% submissions have lists of figures/tables at the end of the paper and that
% figures/tables without any captions are placed on a page by themselves at
% the end of the document. If needed, the draftcls IEEEtran class option or
% \CLASSINPUTbaselinestretch interface can be used to increase the line
% spacing as well. Be sure and use the nomarkers option of endfloat to
% prevent endfloat from "marking" where the figures would have been placed
% in the text. The two hack lines of code above are a slight modification of
% that suggested by in the endfloat docs (section 8.4.1) to ensure that
% the full captions always appear in the list of figures/tables - even if
% the user used the short optional argument of \caption[]{}.
% IEEE papers do not typically make use of \caption[]'s optional argument,
% so this should not be an issue. A similar trick can be used to disable
% captions of packages such as subfig.sty that lack options to turn off
% the subcaptions:
% For subfig.sty:
% \let\MYorigsubfloat\subfloat
% \renewcommand{\subfloat}[2][\relax]{\MYorigsubfloat[]{#2}}
% However, the above trick will not work if both optional arguments of
% the \subfloat command are used. Furthermore, there needs to be a
% description of each subfigure *somewhere* and endfloat does not add
% subfigure captions to its list of figures. Thus, the best approach is to
% avoid the use of subfigure captions (many IEEE journals avoid them anyway)
% and instead reference/explain all the subfigures within the main caption.
% The latest version of endfloat.sty and its documentation can obtained at:
% http://www.ctan.org/pkg/endfloat
%
% The IEEEtran \ifCLASSOPTIONcaptionsoff conditional can also be used
% later in the document, say, to conditionally put the References on a
% page by themselves.




% *** PDF, URL AND HYPERLINK PACKAGES ***
%
%\usepackage{url}
% url.sty was written by Donald Arseneau. It provides better support for
% handling and breaking URLs. url.sty is already installed on most LaTeX
% systems. The latest version and documentation can be obtained at:
% http://www.ctan.org/pkg/url
% Basically, \url{my_url_here}.





% *** Do not adjust lengths that control margins, column widths, etc. ***
% *** Do not use packages that alter fonts (such as pslatex).         ***
% There should be no need to do such things with IEEEtran.cls V1.6 and later.
% (Unless specifically asked to do so by the journal or conference you plan
% to submit to, of course. )


% correct bad hyphenation here
\hyphenation{op-tical net-works semi-conduc-tor}


\begin{document}
%
% paper title
% Titles are generally capitalized except for words such as a, an, and, as,
% at, but, by, for, in, nor, of, on, or, the, to and up, which are usually
% not capitalized unless they are the first or last word of the title.
% Linebreaks \\ can be used within to get better formatting as desired.
% Do not put math or special symbols in the title.
\title{DAC-SDC Low Power Object Detection Challenge for UAV Applications}
%
%
% author names and IEEE memberships
% note positions of commas and nonbreaking spaces ( ~ ) LaTeX will not break
% a structure at a ~ so this keeps an author's name from being broken across
% two lines.
% use \thanks{} to gain access to the first footnote area
% a separate \thanks must be used for each paragraph as LaTeX2e's \thanks
% was not built to handle multiple paragraphs
%
%
%\IEEEcompsocitemizethanks is a special \thanks that produces the bulleted
% lists the Computer Society journals use for "first footnote" author
% affiliations. Use \IEEEcompsocthanksitem which works much like \item
% for each affiliation group. When not in compsoc mode,
% \IEEEcompsocitemizethanks becomes like \thanks and
% \IEEEcompsocthanksitem becomes a line break with idention. This
% facilitates dual compilation, although admittedly the differences in the
% desired content of \author between the different types of papers makes a
% one-size-fits-all approach a daunting prospect. For instance, compsoc
% journal papers have the author affiliations above the "Manuscript
% received ..."  text while in non-compsoc journals this is reversed. Sigh.

\author{Xiaowei Xu,~\IEEEmembership{Member,~IEEE,}
        Xinyi Zhang,~\IEEEmembership{Student Member,~IEEE,}
        Bei Yu,~\IEEEmembership{Senior Member,~IEEE,}
        Xiaobo Sharon Hu,~\IEEEmembership{Fellow,~IEEE,}
        Christopher Rowen,~\IEEEmembership{Fellow,~IEEE,}
        Jingtong Hu,~\IEEEmembership{Member,~IEEE,}
        and Yiyu Shi,~\IEEEmembership{Senior Member,~IEEE}% <-this % stops a space
\IEEEcompsocitemizethanks
{\IEEEcompsocthanksitem Xiaowei Xu, Xiaobo Sharon Hu and Yiyu Shi are with the Department of Computer Science and Engineering, University of Notre Dame, IN, 46556.\protect\\
% note need leading \protect in front of \\ to get a newline within \thanks as
% \\ is fragile and will error, could use \hfil\break instead.
E-mail: xxu8@nd.edu, shu@nd.edu, yshi4@nd.edu
\IEEEcompsocthanksitem Bei Yu is with the Department of Computer Science and Engineering, The Chinese University of Hong Kong, Hong Kong.\protect\\
E-mail: byu@cse.cuhk.edu.hk
\IEEEcompsocthanksitem Xinyi Zhang and Jingtong Hu are with the Department of Electrical and Computer Engineering,
University of Pittsburgh, PA, 15261.\protect\\
E-mail: xinyizhang@pitt.edu, jthu@pitt.edu
\IEEEcompsocthanksitem Christopher Rowen is the CEO of Cognite Venture,
210 Mission Street, Santa Cruz, CA 95060.\protect\\
E-mail: rowen@cogniteventures.com
}% <-this % stops an unwanted space


\thanks{Manuscript received July 31, 2018; revised August 26, 2018.}}

% note the % following the last \IEEEmembership and also \thanks -
% these prevent an unwanted space from occurring between the last author name
% and the end of the author line. i.e., if you had this:
%
% \author{....lastname \thanks{...} \thanks{...} }
%                     ^------------^------------^----Do not want these spaces!
%
% a space would be appended to the last name and could cause every name on that
% line to be shifted left slightly. This is one of those "LaTeX things". For
% instance, "\textbf{A} \textbf{B}" will typeset as "A B" not "AB". To get
% "AB" then you have to do: "\textbf{A}\textbf{B}"
% \thanks is no different in this regard, so shield the last } of each \thanks
% that ends a line with a % and do not let a space in before the next \thanks.
% Spaces after \IEEEmembership other than the last one are OK (and needed) as
% you are supposed to have spaces between the names. For what it is worth,
% this is a minor point as most people would not even notice if the said evil
% space somehow managed to creep in.



% The paper headers
\markboth{Journal of \LaTeX\ Class Files,~Vol.~14, No.~8, August~2018}%
{Shell \MakeLowercase{\textit{et al.}}: Bare Demo of IEEEtran.cls for Computer Society Journals}
% The only time the second header will appear is for the odd numbered pages
% after the title page when using the twoside option.
%
% *** Note that you probably will NOT want to include the author's ***
% *** name in the headers of peer review papers.                   ***
% You can use \ifCLASSOPTIONpeerreview for conditional compilation here if
% you desire.



% The publisher's ID mark at the bottom of the page is less important with
% Computer Society journal papers as those publications place the marks
% outside of the main text columns and, therefore, unlike regular IEEE
% journals, the available text space is not reduced by their presence.
% If you want to put a publisher's ID mark on the page you can do it like
% this:
%\IEEEpubid{0000--0000/00\$00.00~\copyright~2015 IEEE}
% or like this to get the Computer Society new two part style.
%\IEEEpubid{\makebox[\columnwidth]{\hfill 0000--0000/00/\$00.00~\copyright~2015 IEEE}%
%\hspace{\columnsep}\makebox[\columnwidth]{Published by the IEEE Computer Society\hfill}}
% Remember, if you use this you must call \IEEEpubidadjcol in the second
% column for its text to clear the IEEEpubid mark (Computer Society jorunal
% papers don't need this extra clearance.)



% use for special paper notices
%\IEEEspecialpapernotice{(Invited Paper)}



% for Computer Society papers, we must declare the abstract and index terms
% PRIOR to the title within the \IEEEtitleabstractindextext IEEEtran
% command as these need to go into the title area created by \maketitle.
% As a general rule, do not put math, special symbols or citations
% in the abstract or keywords.
\IEEEtitleabstractindextext{%
\begin{abstract}
%The Design Automation Conference system design contest (DAC-SDC) lower power object detection challenge is a benchmark in object detection on 95 categories and 150k images under the view of unmanned aerial vehicles (UAV) perspective.
%The challenge has been run on the 55th Design Automation Conference in 2018, attracting more than 110 entries from all over the world.
%This paper describes the dataset and evaluation procedure.
%The methods and optimization of the entries are discussed under this particular UAV applications, and lessons are provided with detailed analysis.
%The paper concludes with directions for future improvements.
The 55th Design Automation Conference (DAC) held its first System Design Contest (SDC) in 2018.
SDC'18 features a lower power object detection challenge (LPODC) on designing and implementing novel algorithms based object detection in images taken from unmanned aerial vehicles (UAV).
The dataset includes 95 categories and 150k images, and the hardware platforms include Nvidia's TX2 and Xilinx's PYNQ Z1.
DAC-SDC'18 attracted more than 110 entries from 12 countries.
This paper presents in detail the dataset and evaluation procedure.
It further discusses the methods developed by some of the entries as well as representative results.
The paper concludes with directions for future improvements.

\end{abstract}

% Note that keywords are not normally used for peerreview papers.
\begin{IEEEkeywords}
Dataset, Benchmark, Object Detection, Unmanned Aerial Vehicles, Low Power.
\end{IEEEkeywords}}


% make the title area
\maketitle


% To allow for easy dual compilation without having to reenter the
% abstract/keywords data, the \IEEEtitleabstractindextext text will
% not be used in maketitle, but will appear (i.e., to be "transported")
% here as \IEEEdisplaynontitleabstractindextext when the compsoc
% or transmag modes are not selected <OR> if conference mode is selected
% - because all conference papers position the abstract like regular
% papers do.
\IEEEdisplaynontitleabstractindextext
% \IEEEdisplaynontitleabstractindextext has no effect when using
% compsoc or transmag under a non-conference mode.



% For peer review papers, you can put extra information on the cover
% page as needed:
% \ifCLASSOPTIONpeerreview
% \begin{center} \bfseries EDICS Category: 3-BBND \end{center}
% \fi
%
% For peerreview papers, this IEEEtran command inserts a page break and
% creates the second title. It will be ignored for other modes.
\IEEEpeerreviewmaketitle



\IEEEraisesectionheading{\section{Introduction}\label{sec:introduction}}
% Computer Society journal (but not conference!) papers do something unusual
% with the very first section heading (almost always called "Introduction").
% They place it ABOVE the main text! IEEEtran.cls does not automatically do
% this for you, but you can achieve this effect with the provided
% \IEEEraisesectionheading{} command. Note the need to keep any \label that
% is to refer to the section immediately after \section in the above as
% \IEEEraisesectionheading puts \section within a raised box.




% The very first letter is a 2 line initial drop letter followed
% by the rest of the first word in caps (small caps for compsoc).
%
% form to use if the first word consists of a single letter:
% \IEEEPARstart{A}{demo} file is ....
%
% form to use if you need the single drop letter followed by
% normal text (unknown if ever used by the IEEE):
% \IEEEPARstart{A}{}demo file is ....
%
% Some journals put the first two words in caps:
% \IEEEPARstart{T}{his demo} file is ....
%
% Here we have the typical use of a "T" for an initial drop letter
% and "HIS" in caps to complete the first word.

\IEEEPARstart{T}
%{he} Design Automation Conference system design contest (DAC-SDC) lower power object detection challenge (LPODC) consists of two primary segments: a publicly available dataset of images and annotation with evaluation software; and a competition and session hosted on the 55th Design Automation Conference (DAC).
% You must have at least 2 lines in the paragraph with the drop letter
% (should never be an issue)
%The challenge provides a unified platform to develop and compare state-of-the-art object detection algorithms, and discusses the lessons learned from these participated entries.
The 55th Design Automation Conference (DAC) held its first System Design Contest (SDC) in 2018 which features a lower power object detection challenge (LPODC) on designing and implementing novel algorithms based object detection in images taken from unmanned aerial vehicles (UAV).
This challenge provides a unified platform to develop and compare state-of-the-art object detection algorithms, and discusses the lessons learned from these participated entries.

The LPODC at DAC-SDC'18 focuses on unmanned aerial vehicles (UAV) applications as such applications have stringent accuracy, real-time, and energy requirements.
Specifically, first, the LPODC task is to detect a single object of interest, one of the most important tasks in UAV applications \cite{torres2015automatic}.
%Second, unlike general computer visual challenges such as ImageNet \cite{russakovsky2015imagenet} and  PASCALVOC  Dataset \cite{everingham2010pascal}, which focus only on accuracy, LPODC evaluates the final performance based on a combination of throughput, power, and detection accuracy which takes into full consideration the features of UAV applications: real-time processing, energy-constraint embedded platform, and detection accuracy.
Second, different from general computer visual challenges, such as ImageNet \cite{russakovsky2015imagenet} and PASCAL VOC dataset \cite{everingham2010pascal}, which focused
only on accuracy, LPODC evaluates the final performance based on a combination of throughput, power, and detection accuracy. Thus, LPODC takes into full consideration the features of UAV applications: real-time processing, energy-constrained embedded platform, and detection accuracy.
%general computer visual challenges such as ImageNet \cite{russakovsky2015imagenet} and PASCALVOC dataset \cite{everingham2010pascal} focused only on accuracy.
%LPODC evaluates the final performance based on a combination of throughput, power, and detection accuracy which takes into full consideration the features of UAV applications: real-time processing, energy-constraint embedded platform, and detection accuracy.
Third, the images of the dataset are all captured from actual UAVs which reflect the real circumstances and problems of UAV applications.
Fourth, LPODC provides two hardware platforms: embedded GPU (Jetson TX2 from Nvidia \cite{nvidia}) and FPGA SoC (PYNQ Z-1 board from Xilinx \cite{xilinx}) to all the participating teams to choose from for their implementations.
Note that GPUs and FPGAs are widely adopted for energy-efficient processing on UAVs \cite{chao2013survey}.

The publically released dataset contains a large quantity of manually annotated training images, while the testing dataset is withheld for the evaluation purpose.
There are a total of 150k images provided by a UAV company DJI \cite{dji}.
Participating teams trained their models/algorithms with the training dataset, and sent the trained models/algorithms to the organizers to get the final testing results including throughput, energy, and detection accuracy.
Such evaluation was performed at the end of each month and the detailed rank was released then.
The final rank was released at the end of the competition and the top-3 entries from both GPU and FPGA categories were invited to present their work at a technical session at DAC.

This paper describes the LPODC in detail including: the task, the evaluation method and the dataset.
Furthermore, a comprehensive discussion of the methods and results of the top-3 entries from both GPU and FPGA categories is presented to provide insights and  rich lessons for future development of object detection algorithms especially for UAV applications.
Particularly, we will elaborate hardware-software co-design for efficient processing on embedded platforms.

The training dataset, the source codes of the top-3 entries of both GPU and FPGA categories, and additional information about this challenge can be found at \url{www.github.com/xyzxinyizhang/2018-DAC-System-Design-Contest}.

\subsection{Related Work}
In this section we briefly discuss the related work about benchmark image datasets for object detection.
As segmentation datasets can also be used for object detection, some widely-used segmentation datasets are also included.

Most of the datasets for object detection contain common photographs.
LabelMe \cite{russell2008labelme} has 187k images each of which contains multiple objects annotated with bounding polygon.
It also provides a web-based online annotation tool for easy contribution by the public.
Like LabelMe, PASCAL VOC dataset \cite{everingham2010pascal} further increases the number of images to 500k.
ImageNet \cite{russakovsky2015imagenet} is one of the most popular datasets in computer vision community, and contains more than 14 million images.
It was primarily for classification in 2010 and extended to support object detection and scene recognition in 2013.
Compared with ImageNet, SUN database \cite{xiao2010sun} mainly focuses on scene recognition and contains about 131k images.
Microsoft Common Objects in Context (COCO) \cite{lin2014microsoft} contains complex everyday scenes of common objects in their natural context and has 2.5 million images.
Open Images \cite{openimages} contains more than 9 million real-life images within 6,000 categories which is much larger than that of ImageNet (about 1,000).

There are also some datasets for specific applications, and the images are taken from particular views.
KITTI vision benchmark dataset \cite{Geiger2012CVPR} is specific for autonomous driving, and the images are taken from a autonomous driving platform in a mid-size city.
FieldSAFE \cite{kragh2017fieldsafe} is for agriculture application and has approximately 2 hours of raw sensor data from a tractor-mounted sensor system in a grass mowing scenario.
DOTA \cite{xia2018dota} focuses on aerial applications, and all the images are captured from cameras on aircrafts.

The dataset in this paper is specific for UAV applications.
The images in the dataset are taken from UAVs which operates on a much lower height than general aircrafts in DOTA.
The associated environment in the images is also much more complex than that in DOTA.

\subsection{Paper Layout}

We have given an overview of the LPODC at DAC'18.
The rest of the paper is organized as follows. In Section 2,
%The organization of the paper is as follows.
%The overview of the challenge is presented in Section 1,
%and
the challenge task and its evaluation method, as well as the provided two hardware platforms are described.
The details of the dataset and its analysis are given in Section 3.
The analysis and discussion of the methods for object detection of GPU and FPGA entries are presented in Section 4.
Section 5 details the results of the GPU and FPGA entries.
We conclude the paper with some discussions of the challenge and possible improvements.

% needed in second column of first page if using \IEEEpubid
%\IEEEpubidadjcol





% An example of a floating figure using the graphicx package.
% Note that \label must occur AFTER (or within) \caption.
% For figures, \caption should occur after the \includegraphics.
% Note that IEEEtran v1.7 and later has special internal code that
% is designed to preserve the operation of \label within \caption
% even when the captionsoff option is in effect. However, because
% of issues like this, it may be the safest practice to put all your
% \label just after \caption rather than within \caption{}.
%
% Reminder: the "draftcls" or "draftclsnofoot", not "draft", class
% option should be used if it is desired that the figures are to be
% displayed while in draft mode.
%
%\begin{figure}[!t]
%\centering
%\includegraphics[width=2.5in]{myfigure}
% where an .eps filename suffix will be assumed under latex,
% and a .pdf suffix will be assumed for pdflatex; or what has been declared
% via \DeclareGraphicsExtensions.
%\caption{Simulation results for the network.}
%\label{fig_sim}
%\end{figure}

% Note that the IEEE typically puts floats only at the top, even when this
% results in a large percentage of a column being occupied by floats.
% However, the Computer Society has been known to put floats at the bottom.


% An example of a double column floating figure using two subfigures.
% (The subfig.sty package must be loaded for this to work.)
% The subfigure \label commands are set within each subfloat command,
% and the \label for the overall figure must come after \caption.
% \hfil is used as a separator to get equal spacing.
% Watch out that the combined width of all the subfigures on a
% line do not exceed the text width or a line break will occur.
%
%\begin{figure*}[!t]
%\centering
%\subfloat[Case I]{\includegraphics[width=2.5in]{box}%
%\label{fig_first_case}}
%\hfil
%\subfloat[Case II]{\includegraphics[width=2.5in]{box}%
%\label{fig_second_case}}
%\caption{Simulation results for the network.}
%\label{fig_sim}
%\end{figure*}
%
% Note that often IEEE papers with subfigures do not employ subfigure
% captions (using the optional argument to \subfloat[]), but instead will
% reference/describe all of them (a), (b), etc., within the main caption.
% Be aware that for subfig.sty to generate the (a), (b), etc., subfigure
% labels, the optional argument to \subfloat must be present. If a
% subcaption is not desired, just leave its contents blank,
% e.g., \subfloat[].


% An example of a floating table. Note that, for IEEE style tables, the
% \caption command should come BEFORE the table and, given that table
% captions serve much like titles, are usually capitalized except for words
% such as a, an, and, as, at, but, by, for, in, nor, of, on, or, the, to
% and up, which are usually not capitalized unless they are the first or
% last word of the caption. Table text will default to \footnotesize as
% the IEEE normally uses this smaller font for tables.
% The \label must come after \caption as always.
%
%\begin{table}[!t]
%% increase table row spacing, adjust to taste
%\renewcommand{\arraystretch}{1.3}
% if using array.sty, it might be a good idea to tweak the value of
% \extrarowheight as needed to properly center the text within the cells
%\caption{An Example of a Table}
%\label{table_example}
%\centering
%% Some packages, such as MDW tools, offer better commands for making tables
%% than the plain LaTeX2e tabular which is used here.
%\begin{tabular}{|c||c|}
%\hline
%One & Two\\
%\hline
%Three & Four\\
%\hline
%\end{tabular}
%\end{table}


% Note that the IEEE does not put floats in the very first column
% - or typically anywhere on the first page for that matter. Also,
% in-text middle ("here") positioning is typically not used, but it
% is allowed and encouraged for Computer Society conferences (but
% not Computer Society journals). Most IEEE journals/conferences use
% top floats exclusively.
% Note that, LaTeX2e, unlike IEEE journals/conferences, places
% footnotes above bottom floats. This can be corrected via the
% \fnbelowfloat command of the stfloats package.


\section{LPODC Task and Evaluation Criteria}
In this section, the details of the challenge task are presented, followed with an introduction of the hardware platforms and the evaluation method.

\subsection{Object Detection}
The LPODC task is to perform single object detection in each image with an axis-aligned bounding box indicating the object's position and scale.
As the challenge is targeted at UAV applications, there are several aspects that need to be emphasized.
%First, the object detection task is to find exactly the same object in the testing images as that in the training dataset rather than objects belong to the same category.
First, the object detection task is to locate a specific object from the training dataset, rather than objects from a training category.
For example, if images containing person A are in the training dataset, then the task is to detect person A rather than other persons.
More details about the detection objects are discussed in Section 3.
Second, the object detection task needs to be executed with high throughput and high accuracy which are required by UAV applications.
This requirement is achieved through the weighting of throughput in the scoring system, discussed in Section 2.3.
%by giving a penalty to low throughput implementations, and the details are discussed in Section 2.3.


\subsection{Hardware Platforms}

%\subsubsection{FPGA}
In this challenge, two hardware platforms, either FPGA or GPU, were provided to the participating teams from the challenge sponsors Xilinx \cite{xilinx} and Nvidia \cite{nvidia}, respectively.
Particularly, the \textbf{FPGA platform} is Xilinx PYNQ Z-1 board which is an embedded systems based platform combining Zynq system and Python~\cite{janssen2017dynamic}.
%PYNQ is an embedded systems targeted platform which combine Zynq system and Python~\cite{janssen2017dynamic}.
Participants are allowed to use Cortex-A9 processor and ZYNQ XC7Z020-1CLG400C on the platform to realize their solutions to the challenge.
The embedded FPGA chip contains 53K 6-input look-up-tables, 220 DSPs, and 630KB fast block RAM.
A 512MB DDR3 memory with 16-bit bus at 1,050Mbps is also embedded on the platform which can be accessed by both the processor and the FPGA.
The power consumption of the FPGA platform is about 1-4 watts.

The \textbf{GPU platform} is Nvidia Jetson TX2, which is an embedded AI computing device.
This GPU is very powerful with a 6-core CPU (Dual-core NVIDIA Denver2 + quad-core ARM Cortex-A57), a 256-core Pascal GPU, and 8GB LPDDR4 DRAM.
It can provide more than 1TFLOPS of FP16 compute performance in less than 7.5 watts of power.
Note that both hardware platforms target low-power embedded computation and are suitable for UAV applications.

\begin{figure*}
  \includegraphics[width=1\textwidth]{./figure/dataset.pdf}
  \vspace{-11pt}
  \caption{Overview of the dataset provided by DJI. There are 12 categories, each of which includes several sub-categories as indicated in the bracket, and there are totally 95 sub-categories. Note that there is only one object in each image.}
    \vspace{-1pt}
  \label{fig:dataset}
  \vspace{-16pt}
\end{figure*}

\subsection{Evaluation Method}

The evaluation for the challenge is based on detection accuracy, throughput, and energy consumption.
As energy consumption is relatively straightforward, the following content mainly discusses the metrics for accuracy and throughput.

The metric for object detection accuracy is Intersection over Union (IoU).
Suppose there are two bounding boxes (BB): a predicted BB and the ground-truth BB.
Then the accuracy or IoU of the predicted BB is the ratio between the area of the union of the predicted BB and the ground-truth BB and the area of the overlap encompassed by both the predicted BB and the ground-truth BB, i.e.,
\begin{equation}\label{distance}
%d(c_i'',t_j'')=(c_i''-t_j'')^2.
IoU = \frac{BB_{p}\cap BB_{g}}{BB_{p}\cup BB_{g}},
\end{equation}
where $BB_{p}$ and $BB_{g}$ are the areas of the predicted and ground-truth BBs, respectively.
Note that the challenge only cares the IoU results, but does not care the object categories.

The metric for throughput is frames per second (FPS).
For real-time processing in UAV applications, the minimum throughput requirement in this challenge was set to 20 FPS on the GPU platform and 5 FPS on the FPGA platform.
If the FPS is lower than the requirement, then a penalty to IoU is added, i.e.,
\begin{equation}\label{penalty}
%d(c_i'',t_j'')=(c_i''-t_j'')^2.
IoU_{r} = IoU_{m}\times (min(FPS_{m}, FPS_{r}))/FPS_{r},
\end{equation}
where $IoU_{r}$ and $IoU_{m}$ are the actual and measured IoUs, respectively, and $FPS_{r}$ and $FPS_{m}$ are the required and measured FPSs, respectively.
The $min$ function outputs the minimum one of its inputs.

The final score is a combination of accuracy, throughput, and energy consumption.
Suppose there are $I$ registered entries and the dataset contains $K$ evaluation images.
Let $IoU_{i,k}$ be the IoU score of image $k$ ($k \leq K$) for entry $i$ ($i\leq I$).
Then the IoU score $R_{IoU_{i}}$ for entry $i$ is computed as
\begin{equation}\label{eval1}
R_{IoU_{i}} = \frac{\sum_{k=1}^{K}IoU_{i,k}}{K}.
\end{equation}
Let $E_i$ be the energy consumption of processing all $K$ images for entry $i$.
Let $\bar{E_I}$ be the average energy consumption of the $I$ entries.
$\bar{E_I}$ is computed as
\begin{equation}\label{eval2}
\bar{E_I} = \frac{\sum_{i=1}^{I}E_{i}}{I}.
\end{equation}
Then the energy consumption score $ES_i$ for entry $i$ is
\begin{equation}\label{eval3}
{ES_i} = max\{0, ~1+0.2\times log_x\frac{\bar{E_{I}}}{E_i}\},
\end{equation}
where $x$ is set to 2 and 10 for FPGA category and GPU category, respectively.
Through profiling, we find that the energy-performance Pareto frontier of the GPU platform exhibits much smaller gradient than that of the FPGA platform, reflecting the fact that the FPGA is more resource constrained than the GPU and thus its performance is much more sensitive to energy consumption. As such, we stress more on the accuracy for the GPU, while more on the energy for the FPGA.
The final total score $TS_i$ for entry $i$ is
\begin{equation}\label{eval4}
{TS_i} = R_{IoU_i}\times (1+{ES_i}).
\end{equation}

The factor 0.2 in Equation (\ref{eval3}) is set based on the estimated range of energy consumption variation in participating teams.
Through our profiling of the platform, we anticipate that a team will normally have an energy consumption from 1/4x to 4x the average of all teams for both GPU and FPGA categories.
%we anticipate that $log_{x}\bar{E_I}/E_{i}$ will take a range of -0.25 to 2, and
As such, $log_{x}\bar{E_I}/E_{i}$ will take a range of -0.25 to 2, and a factor of 0.2 will make $ES_{i}$ in the range of 0.6 to 1.4.
Thus, the final (1+$ES_{i}$) in Equation (\ref{eval4}) is in the range of 1.6 to 2.4 which is appropriate to act as a reward factor for energy efficiency.
The $max$ function and the addition of 1 to $ES_{i}$ are for teams with extremely low performance on energy efficiency.
In such condition, $log_{x}\bar{E_I}/E_{i}$ is a large negative number, and the $max$ function ensures that $ES_{i}$ is not a negative number but zero.
Then the addition of 1 to $ES_{i}=0$ further ensures that $TS_{i}$ can still be graded based on accuracy with no multiplying rewarding factor for energy efficiency rather than being frozen to zero even if very high IoUs are obtained.

%As powerful GPUs are usually for complex tasks, while resource-constraint FPGAs are usually for only one specific application, higher weights ($x$=10 and 2) are given to accuracy and energy for GPUs and FPGAs, respectively.

%This is to capture the characteristic of UAV applications where accuracy, throughput and energy/power are all very important.
%It can be noted from the equations that when the real-time processing requirement is met, the detection accuracy (IoUs) plays a more important role in the final evaluation.

\section{Dataset}\label{dataset}

{\color{black}{As shown in Fig. \ref{fig:dataset}, the adopted dataset from DJI \cite{dji} contains 12 categories of images and 95 sub-categories.
For each sub-category, 70\% of the images are provided for training and 30\% are reserved for evaluation.}}
%It should be highlighted that there is a significant difference between our dataset and other general purpose dataset such as
%In each class, the object is captured in a UAV view and with different point of view. The size of the object varies from image to image in a class.
It should be highlighted that compared with existing general purpose datasets such as ImageNet \cite{russakovsky2015imagenet} and  PASCAL VOC dataset \cite{everingham2010pascal},  the object is captured in a UAV view and with different points of view.

The distributions of the training and testing datasets with respect to category, object size ratio, image brightness and amount of information are shown in Figs. \ref{fig:dataset_category}-\ref{fig:dataset_texture}.
Here, object size ratio is the ratio of the object size to the image size.
The brightness of a pixel is defined as in Equation (\ref{brightness})\cite{sanmorino2012clustering} (r, g, b are the three channels of images), where the image brightness is the average brightness of all its pixels.
The amount of information is defined as the average pixel entropy of the object where the pixel entropy is calculated in a 5$\times$5 region.
%Note that for clear demonstration,

\begin{equation}\label{brightness}
%d(c_i'',t_j'')=(c_i''-t_j'')^2.
brightness = \sqrt{0.241\times r^2+0.691\times g^2+0.068\times b^2}
\end{equation}

\begin{figure}
\centering
  \includegraphics[width=1\columnwidth]{./figure/dataset_category.pdf}
  \vspace{-25pt}
  \caption{Distributions of the training and testing datasets with respect to image categories.}
  \vspace{-5pt}
  \label{fig:dataset_category}
\end{figure}

\begin{figure}
\centering
  \includegraphics[width=1\columnwidth]{./figure/dataset_size.pdf}
  \vspace{-25pt}
  \caption{Distributions of the training and testing datasets with respect to object size ratio.}
  \vspace{-5pt}
  \label{fig:dataset_size}
\end{figure}


\begin{figure}
\centering
  \includegraphics[width=1\columnwidth]{./figure/dataset_brightness.pdf}
  \vspace{-25pt}
  \caption{Distributions of the training and testing datasets with respect to image brightness.}
  \vspace{-5pt}
  \label{fig:dataset_brightness}
\end{figure}


\begin{figure}
\centering
  \includegraphics[width=1\columnwidth]{./figure/dataset_texture.pdf}
  \vspace{-25pt}
  \caption{Distributions of the training and testing datasets with respect to amount of information.}
  \label{fig:dataset_texture}
\end{figure}


%of the object varies from image to image in a class
Fig. 2 shows that the ratio of quantity of training and testing images in different categories are almost the same, which is manually segmented to achieve a good balance of training and testing dataset.
The categories \textit{person}, \textit{car}, and \textit{rider} contain much more images than others as they contain more sub-categories than others.
It can be noticed that there is also a good balance between the training dataset and the testing dataset for different object sizes except for some large object size ratio as shown in Fig. 3.
As the object size ratio increases, the image quantity decreases.
Note that the average object size ratio in ILSVRC is 17\% and in PASCAL VOC is 20\%.
However, in this dataset, most of the images have an object size of 1-2\% of the captured images (640x360), which is the main character of UAV-view images.
This good balance still holds for image brightness and amount of information as shown in Fig. \ref{fig:dataset_brightness} and Fig. \ref{fig:dataset_texture}.
Both the two distributions in the two figures have the same shape: most of the images have a moderate brightness/amount of information, while many fewer images contain too large or too small brightness/amount of information, which are like a Gaussian distribution.


%Among all images, most of the images has a object size around 1.8\% of the captured images (640x360), while other dataset such as xxxxx has much bigger object size, xxxxxxxxxxxxxxxx.
%{\color{red}{!!will show 4 figs about quantity vs. (1)category(2)object size(3)brightness(4)texture, and discuss them.!!}}

%Note: The dataset provided for participants to download contains 70\% of the total dataset provided by our sponsor. The remaining 30\% of the dataset is reserved for our evaluation. We will ONLY use the reserved dataset to evaluate and rank all the entries.





\section{Methods for Object Detection}

In this section, the analysis and discussion of the methods for object detection reported by the representative GPU and FPGA entries are presented.
The details of the top-3 entries of GPU and FPGA categories are discussed, and statistical significance analysis is also presented.


\subsection{GPU}
A total of 24 out of the 53 participating teams successfully implemented their designs under the GPU category, and all of them adopted deep learning approach.
The distributions of used neural network models and deep learning frameworks are shown in Fig. \ref{fig:gpu_summary}.
Tiny YOLO \cite{redmon2016you} is the most widely used model in the contest, and a majority of the entries achieve high performance by adding some revision to the network structure.
Darknet is the most popular deep learning framework as tiny YOLO is originally implemented in Darknet.
The top three entries \textbf{ICT-CAS}, \textbf{DeepZ}, and \textbf{SDU-Legend} all adopted the YOLO model as the base design and improved it with structure and computation optimization.
Note that the image size is 640$\times$360 in the challenge, and ICT-CAS and SDU-Legend resized it to improve accuracy and throughput.

\textbf{{ICT-CAS}} adopted the original tiny YOLO as their network structure as shown in Fig. \ref{fig:Network_GPU}(a), and deployed the Tucker decomposition, hard example mining and low-bits computation for fast and accurate processing.
%, while in pursuit of speed, we tried Detection Framework optimization selection, lightweight CNN for feature extractor, tucker decomposition and precision scaling. In Detection Framework selection, we tested Mask/Faster R-CNN, YOLOv2/v3, SSD to find out best Detection Framework for the detection task. As for lightweight CNN, We tried mobilenetv1/v2, shufflenet and Xception as feature extractor to reduce computation and memory consumption.
In Tucker decomposition, they tested different decomposition parameters for optimal precision and throughput.
By extracting the hard examples, re-training was performed to increase accuracy.
In order to speed up the inferencing stage, they deployed half precision float point computation to reduce computation complexity and power consumption.
%In parameter exploration, we mainly tested input size and anchor box parameter.
In the TX2 GPU platform, this entry also adopted TensorRT \cite{tensorrt} as the inference optimizer to speed up the inference.

%Due to the speed limitation of 20FPS, we started with YOLOv2-Tiny as the backbone network for feature extraction and a detection network for candidate bounding box generation. However, with such a simple backbone model, we were soon faced with the challenges of tiny objects, occlusions and distraction from the provided data set. In order to tackle to the aforementioned challenges, we investigated various network architectures for both inference and training.
\textbf{DeepZ} implemented their own network structure as shown in Fig. \ref{fig:Network_GPU}(b).
It combined Feature Pyramid Network \cite{lin2017feature} to fuse fine-grained features with strong semantic features to enhance the ability in detecting small objects. Meanwhile, DeepZ utilized focal loss function to mitigate the imbalance between the single ground truth box and the candidate boxes in the training phase, thereby partially resolving occlusions and distractions.
%With the combined techniques, we achieved the inference network as shown in the figure with an accuracy improvement of ~ 4.2\% while maintaining pretty much the same speed.

\textbf{SDU-Legend} focused on both neural network and architectural level optimization to achieve better balance between system performance and detection accuracy.
SDU-Legend chose YOLO v2 as the starting design point, and performed design space exploration to choose the key training parameters including anchors, coordinates scale in the loss function, batch size, and learning rate policy.
Moreover, SDU-Legend reduced YOLO v2 network architecture from 32 layers to 27 layer (as shown in Fig. \ref{fig:Network_GPU}(c)), and decreased the downsampling rate to strengthen the performance on small targets.
At the architectural level, SDU-Legend aimed at balancing the workload between the GPU and the CPU by executing the last 2 layers on the CPU.
SDU-Legend utilized the half data type (16-bit float) instead of 32-bit float to improve the memory throughput and reduce computation cost with minimum loss in accuracy. %Finally, the object detection system from entry ``SDU-Legend’’ achieves IoU of 0.6847, 10.3w power, and 23.64 FPS.




\begin{figure}
\centering
\subfigure{
\label{fig:gpu_summary_1}
\includegraphics[width=0.23\textwidth]{figure/gpu_summary_1.pdf}}
\subfigure{
\label{fig:gpu_summary_2}
\includegraphics[width=0.23\textwidth]{figure/gpu_summary_2.pdf}}
\vspace{-5pt}
\caption{Number of entries using (a) neural network models and (b) deep learning frameworks.}
\label{fig:gpu_summary}
\vspace{-15pt}
\end{figure}

\begin{figure}
\centering
  \includegraphics[width=1\columnwidth]{./figure/Network_GPU.pdf}
  \caption{Neural network structure of the top-3 GPU entries: (a) ICT-CAS, (b) DeepZ, and (c) SDU-Legend.}
  \label{fig:Network_GPU}
\end{figure}



\subsection{FPGA}


There are a total of seven out of the 61 participating teams that successfully implemented their designs on the FPGA platform provided.
Among these entries, only entry \emph{{TGIIF}} adopted Verilog hardware description language for compact FPGA implementation, while the rest utilized high-level synthesis for fast FPGA implementation.
%synthesis and only entry \emph{\textbf{TGIIF}} adopted Verilog as the programming language. And the majorities of the entries adopted a fixed-point operation in designs to make it hardware friendly.

The top three entries \emph{\textbf{TGIIF}}, \textit{\textbf{SystemsETHZ}}, and \textbf{\textit{iSmart2}} adopted Convolution Neural Network and used variations on the models such as Single Shot MultiBox Detector (SSD)~\cite{liu2016ssd}, SqueezeNet~\cite{iandola2016squeezenet}, MobileNet~\cite{howard2017mobilenets}, and Yolo~\cite{redmon2016you}.
From the original models, the proposed models used 1) fewer deep layers, 2) dynamic precision, 3) pruned topology, and 4) layer-shared Intellectual Property (IP).
To further speed up the inference, these entries downsized the images in the CPU before FPGA processing.
In order to compensate the limited on-chip BRAM, these entries utilized the off-chip 512MB DRAM to store the intermediate data between network layers.

The entry \emph{\textbf{TGIIF}} proposed an optimized SSD network.
It downsized the SSD network topology by removing the last two convolutional layers and quantized the network parameters to eight-bit fixed point.
After modifying the network depth, it also pruned and fine-tuned the resultant network, making it with 14.2x less parameters and 10x less operations.
The network topology proposed by \emph{{TGIIF}} is shown in Fig. \ref{fig:Network_FPGA}(a).
% to increase the speed from 1.5FPS to 12FPS suffering only 3.6\% precision loss from 65.7\% to 62.1\%.

\begin{figure}%[htb]
\centering
  \includegraphics[width=0.95\columnwidth]{./figure/Network_FPGA.pdf}
  \caption{Neural network structure of the top-3 FPGA entries: (a) TGIIF, (b) SystemETHZ, and (c) iSmart2.}
  \label{fig:Network_FPGA}
\end{figure}

%\begin{figure}[htb]
%\centering
  %\includegraphics[width=0.95%\columnwidth]{./figure/TGIIF.pdf}
  %\caption{TGIIF network topology.}
  %\label{TGIIF}
%\end{figure}

%\begin{figure}[htb]
%\centering
  %\includegraphics[width=0.95%\columnwidth]{./figure/SystemsETHZ.pdf}
  %\caption{SystemsETHZ network topology.}
  %\label{SystemsETHZ}
%\end{figure}

%\begin{figure}[htb]
%\centering
  %\includegraphics[width=0.95%\columnwidth]{./figure/iSmart2.pdf}
    %\caption{iSmart2 network topology.}
  %\label{iSmart2}
%\end{figure}



The entry \emph{\textbf{SystemsETHZ}} proposed a variation of SqueezeNet+Yolo networks.
It reduced the fire layer in SqueezeNet to half and binaried the fire layer, and introduced a deep network consisting of 18 convolutional layers where the halfFire layers are binary.
%a total of eighteen binary convolution layers.
The network topology proposed by \emph{{SystemsETHZ}} is shown in Fig. \ref{fig:Network_FPGA}(b).
It also adopted dynamic precision weights among different layers: five-bit fixed point parameters in all activation layers, eight-bit fixed point parameters in the first convolutional layer, and binary weights in all fire layers.
With dynamic precision, it reduced the weight size to 64 KB and the number of multiplication operations to 154 millions.

% achieving a speed of 28.6FPS and precision of 49.19\%.

The entry \emph{\textbf{iSmart2}} proposed a variation on MobileNet+Yolo networks.
It introduced a hardware-friendly network, consisting of multiple depth-wise separable convolution kernels. In each kernel, it adopted a convolution-Relu-Convolution-Relu topology.
The network topology proposed by \emph{{iSmart2}} is shown in the Fig. \ref{fig:Network_FPGA}(c).
By calling one depth-wise separable convolution IP when processing different layers, the proposed network achieved a relatively low resource utilization with minimum look-up-table and Flip-Flop usage.
% It achieved a precision of 57.33\% and speed of 7.35FPS.

The entry \emph{\textbf{traix}} proposed a varied SSD network with sixteen-bit fixed-point parameters.
The entry \emph{\textbf{hwac\_object\_tracker}} proposed a varied Tiny YOLO network topology with half-precision floating point parameters.
The entry \emph{\textbf{Lilou}} proposed a binaried VGG16 network topology with less pooling layers, thus retaining the inference data flow on the FPGA.
The entry \emph{\textbf{Qiu's Team}} proposed a varied PYNQ-BNN \cite{bnnpynq}, adopting 2-bit precision for all layers' parameters.

The FPGA resource utilization of all the entries is shown in Table~\ref{FPGA_resource}.
For entries that adopted parameters with sixteen or eight bit-width (\emph{{TGIIF, SystemsETHZ, iSmart2, traix, hwac\_object\_tracker}}), the DSP utilization is close to 90\%. The top entry \emph{{TGIIF}} achieved a 100\% DSP utilization.
For entries that adopted parameters with one or two bit-width (\emph{{Lilou, Qiu's Team}}), the DSP utilization is as low as 12\% (\emph{{LiLou}}), while the LUT utilization can be as high as 100\% (\emph{{Qiu's Team}}).
%{\color{red}{I may add more?}}




\begin{table}[htbp]
\caption{Resource utilization of FPGA entries}
\label{FPGA_resource}
\centering
\begin{tabular}{ccccc}
\hline
Entries     & LUTs    & Flip-Flop & BRAM    & DSP     \\%\hline
            & (53K)     & (106K)      & (630KB)   & (220)     \\ \hline
TGIIF       & 83.89\% & 54.24\%   & 78.93\% & 100\%   \\
SystemsETHZ & 88\%    & 62\%      & 77\%    & 78\%    \\
iSmart2     & 63\%    & 22\%      & 95\%    & 86\%    \\
traix       & 90\%       & -         & 90\%       & 90\%       \\
hwac\_object\_tracker        & 85.02\% & 41.51\%   & 42.14\% & 87.73\% \\
Lilou       & 75\%    & 38\%      & 98\%    & 12\%    \\
Qiu's Team      & 100\%   & 61\%      & 96\%    & 23\%    \\ \hline
\end{tabular}
\end{table}

% \begin{table}[htbp]
% \caption{Top three entries performance}
% \label{FPGA_performance}
% \centering
% \begin{tabular}{cccc}
% \hline
% Entries     & IoU      & Power(mW) & FPS     \\ \hline
% TGIIF       & 0.623798 & 4200      & 11.9553 \\
% SystemsETHZ & 0.491926 & 2450      & 25.9678 \\
% iSmart2     & 0.5733   & 2590      & 7.3488  \\ \hline
% \end{tabular}
% \end{table}




\section{Results and Analysis}

In this section, we will discuss and analyze the result with respect to method, category, object size, image brightness, and amount of information for both GPU and FPGA entries.
As power and throughput are determined once the method is chosen, we focus on the detection accuracy in this section.


\subsection{Results of GPU Entries}

\subsubsection{Overall Results}

As shown in Fig. \ref{fig:GPU_method}, the optimal IoU, power and FPS are 0.6975, 4834mW, and 58.91 FPS, respectively.
Note that all the top-3 entries (ICT-CAS, DeepZ, SDU-Legend) have high IoUs and FPS.
Only high IoU or FPS is not enough to achieve a high total score/good ranking.
For example, the entry OSSDC obtains a rather high FPS and a low IoU, which only ranks the 15th.
While another entry Talos-G achieved a high IoU and a low FPS (lower than 20), which triggers the penalty as discussed in Equation (\ref{penalty}) and only ranks the 14th.

Actually all the top-8 entries get IoUs higher than 0.60 and throughput higher than 20 FPS.
Moreover, their values are both very close to each other which shows rather fierce competition among the top entries.
Compared with IoUs, the power has a less influence on the final ranking.

In order to analyze whether results of different entries are statistically significantly different from each other, statistical significance analysis is performed.
The bootstrap method is adopted here which is also employed by PASCAL VOC \cite{everingham2010pascal} and ImageNet \cite{russakovsky2015imagenet}.
In each bootstrap round, M images are sampled with replacement from the available M testing images and the average IoU for the sampled images is obtained for one entry.
The above process iterates for each entry until reaching the pre-defined bootstrapping round.
With the results obtained from all the bootstrapping rounds, the lower and upper $a$ fraction are discarded, and the range of the remaining results are the 1-2a confidence interval.
We set the number of bootstrapping rounds to 20,000 and $a$ to 0.0005 (99.9\% confidential interval), and the final results of the entries are shown in Fig. \ref{fig:gpu_statistics}.
It can be observed that almost all the entries are statistically significantly different from each other, and the difference of the top-4 entries are also very obvious even with minor differences.

\begin{figure}
\centering
  \includegraphics[width=1\columnwidth]{./figure/GPU_method.pdf}
  \caption{Overall results of GPU entries. Note that the entries are ranked from high to low in the horizontal axis. The details of the scores can be found on the website of the challenge.}
  \label{fig:GPU_method}
\end{figure}



\begin{figure}
\centering
  \includegraphics[width=1\columnwidth]{./figure/GPU_statistics.pdf}
  \caption{Statistical significance analysis using bootstrapping with 99.9\% confidence intervals for GPU entries. Note that the entries are ranked from high to low in the vertical axis.}
  \label{fig:gpu_statistics}
\end{figure}


\subsubsection{Detection Results by Category}\label{gpu_category}

As shown in Fig. \ref{fig:GPU_class}, the category \textit{boat} is with the highest detection accuracy as it contains moderate quantity of images and its object size is relatively larger than other categories as shown in Fig. \ref{fig:dataset}.
The category \textit{person}, \textit{rider}, and \textit{car} are with a high accuracy as their image quantities are large which can provide a large variety of the object for training as shown in Fig. \ref{fig:dataset_category}.
The category \textit{drone} and \textit{paraglider} also get high accuracy (though their image quantities are small) which is due to the fact that their backgrounds are usually simple such as the sky and their structures are also very special compared with others.

The rest six categories are with relatively lower accuracy as their image quantities are small as shown in Fig. \ref{fig:dataset_category}.
Furthermore, the category \textit{whale} has a very low contrast between the object and the background (the sea) as shown in Fig. \ref{fig:dataset}.
The category \textit{building} is rather challenge as there exists many similar objects which results in a very low accuracy.
The category \textit{group} gets the lowest accuracy as there also exists multiple similar objects which makes it very hard to detect the right one.


\begin{figure*}
\vspace{-6pt}
  \includegraphics[width=1\textwidth]{./figure/gpu_category.pdf}
  \vspace{-20pt}
  \caption{Detection accuracy of GPU entries with respect to image category. Note that the category is sorted in a ranked order and the left-most one is the one with the highest average total score.}
    \vspace{-1pt}
  \label{fig:GPU_class}
  \vspace{-6pt}
\end{figure*}

\begin{figure*}
\vspace{-6pt}
  \includegraphics[width=1\textwidth]{./figure/gpu_size.pdf}
  \vspace{-20pt}
  \caption{Detection accuracy of GPU entries with respect to size.}
    \vspace{-1pt}
  \label{fig:gpu_size}
  \vspace{-10pt}
\end{figure*}


\begin{figure*}
  \includegraphics[width=1\textwidth]{./figure/gpu_brightness.pdf}
  \vspace{-20pt}
  \caption{Detection accuracy of GPU entries with respect to brightness.}
    \vspace{-1pt}
  \label{fig:gpu_brightness}
  \vspace{-16pt}
\end{figure*}

\begin{figure*}
  \includegraphics[width=1\textwidth]{./figure/gpu_texture.pdf}
  \vspace{-20pt}
  \caption{Detection accuracy of GPU entries with respect to amount of information.}
    \vspace{-1pt}
  \label{fig:gpu_texture}
  \vspace{-16pt}
\end{figure*}


\subsubsection{Detection Results by Object Size}\label{gpu_size}

In Fig. \ref{fig:dataset_size} the quantity of images with 1\% object size ratio is larger than that of images with 2\% object size ratio.
However, as shown in Fig. \ref{fig:gpu_size}, the accuracy of images with 1\% object size ratio is much smaller than that of images with 2\% object size ratio.
The main reason is that too small object is much harder to be detected with high accuracy.
The accuracy of the images with 2\%-5\% object size ratios is almost the same and relatively higher than others as their corresponding image quantity is large and the object size is moderate.
However, when the object size ratio increases from 6\%, the accuracy decreases as the corresponding training image quantity is much lower than others.



\subsubsection{Detection Results by Image Brightness}\label{gpu_brightness}
As shown in Fig. \ref{fig:gpu_brightness}, there is a almost linear trend between the accuracy and the image brightness.
As the image brightness increases, the accuracy also increases.
However, if the accuracy is directly correlated with the image brightness, the accuracy with a brightness of about 135 should be the highest as the image quantity with this brightness is the largest.
Thus,
%there is no directly correlation between the image brightness and the accuracy.
we tend to believe that higher brightness will make the object more clear to show its features which will make it relatively easier to be detected with high accuracy.



\subsubsection{Detection Results by Amount of Information}\label{gpu_texture}

As shown in Fig. \ref{fig:gpu_texture}, images with larger amount of information tend to have higher accuracy as larger amount of information contains more features which are relatively easier to be detected.
The images with amount of information of 0.35-0.40 get a slightly higher accuracy than that with amount of information of 0.45-0.50 which may caused by that these images are with unique characteristics and a small quantity.
We can also notice that the entry ICT-CAS can get a much higher accuracy for images with amount of information of 0.35-0.40 than others, and get relatively lower accuracy for images with amount of information of 0.90 than other top-3 entries.

\begin{figure}
\centering
  \includegraphics[width=1\columnwidth]{./figure/FPGA_method.pdf}
    \caption{Overall results of FPGA entries. The details of the scores can be found on the website of the challenge.}
  \label{fig:FPGA_by_method}
\end{figure}

\subsection{Results of FPGA entries}
\subsubsection{Overall Results}

% This section reports and discusses the results of the FPGA task.
% The seven entries were evaluated.
% All entries tackled all of the twelve classes.

Fig.~\ref{fig:FPGA_by_method} summarizes the performance of all FPGA entries.
%The seven entries were evaluated.
Entry TGIIF stands out as the most successful method, obtaining an IoU score of 0.6238 and a throughput of 11.96 FPS. The adopted deep network leads to a high IoU score, and the network pruning ensures the inference throughput.
Entry SystemsETHZ comes second while it achieves a throughput of 25.97 FPS and an IoU score of 0.4919.
Its dynamic precision architecture and binarized layers accelerate the inference throughput at the expense of IoU loss.
iSmart2 comes third with a throughput of 7.35 FPS and an IoU score of 0.5733.
Its uniform-layer deep network design gives rise to IP reuse and fine-grained memory allocation, keeping a good balance between throughput and accuracy.

% The FPGA performance in this challenge is not only influenced by the network topology and depth, but also by the design deployment. The network typologies adopted determine the accuracy of the design and the FPGA design deployment impact the processing speed.


Many entries achieved IoUs of around 0.5-0.6, such as entry $traix$ which achieves an IoU of 0.6100 with sixteen-bit fixed-point network parameters.
Despite the success in accuracy, there is a quite range in inference throughput.
As mentioned in section 2.2 and 4.2, the FPGA platform has 630KB on-chip BRAM, which is not sufficient to retain all parameters and  output feature maps of layers.
%While achieving a high accuracy, the limited on-chip BRAM may not be able to retain all parameters and layer output feature maps on-chip.
Thus, a frequent data transfer between DRAM and FPGA are observed in all designs, and then keeping a proper balance between accuracy and network size is very important in this challenge.
%Thus, keeping a balance between accuracy and network size is very important in this challenge.
%Base on some succeeded networks, t
The standout top-3 entries adopted network pruning, parameters binarization and IP-reuse to ensure the inference throughput.

The statistical significance analysis is shown in Fig. \ref{fig:FPGA_statistics}.
It can be easily noticed that all the entries are statistically significantly different from each other, and the difference of the top-3 entries are even larger and there is also moderate difference between their accuracies.

\begin{figure}
\centering
  \includegraphics[width=1\columnwidth]{./figure/FPGA_statistics.pdf}
  \caption{Statistical significance analysis using bootstrapping with 99.9 confidence intervals for FPGA entries.}
  \label{fig:FPGA_statistics}
\end{figure}






\subsubsection{Detection Results by Category}\label{fpga_category}
Fig.~\ref{fig:FPGA_by_class} summaries the results obtained according to object category. The categories are listed in sequence of average IoU.
There is drastic variation among categories and in different entries, in which, $boat$ keeps the most promising average IoU of 0.5734 and $group$ with an average IoU of 0.2060 is the most challenging category to be detected.
On average IoU, the most promising categories are $boat$, $person$, $rider$, and $car$, while those four categories have moderate quantities of images and their object sizes are relatively larger than other categories.
Among those four categories, $boat$ stands out as the most promising category while it contains 5k (the least) training images compared with $person$ which contains 28k (the most) training images.
This is due to the fact that the background in the $boat$ category is simple (the sea) and the target is distinct, while the $person$ category contains numerous noise such as trees, grass, and even other non-objected persons.
The $rider$ stands ahead of $car$ while the $car$ category contains 25k training images and $rider$ contains only 1.6k training images.
This is due to the fact that the $rider$ category is captured from a relatively close view while $car$ category is captured from a distant view.


For these challenging categories such as $whale$, $truck$, $building$, and $wakeboard$, there is an approximately 20\% accuracy gap between these categories and the promising categories for both maximum IoU and average IoU.
%Rather than the low image quantity, the object features may also worsen the detection accuracy.
Not only the low image quantity, but also the complex object features worsen the detection accuracy.
The features of these four challenging categories can be summarized as follows: the category $whale$ contains blurry object and background, while categories $truck$, $wakeboard$, and $building$ have tiny objects or similar objects to the target objects such as the same color and shadow.




\subsubsection{Detection Results by Object Size}\label{fpga_size}
% Images taken from overlook view have features such as smaller size, different point of view and more irrelevant objects.
Fig. \ref{fig:FPGA_size} summaries the overall IoU of the seven entries with the target size.
%The sequence of the target size is sequenced as it is shown in Fig.\ref{fig:dataset_size}.
It can be observed than the most promising target size is located at 2\%\~{}6\%, and 4\% stands out as the most promising target size.
While the training images with target size lower than 2\% count the largest quantity of the dataset, its accuracy is inferior compared with images with a target sizen of 2\%\~{}6\%.
The accuracy of images with less than 2\% target size is slightly superior than those images with 6\%\~{}10\% target size, while the number of images with less than 2\% target size is almost 100x more than those with 6\%\~{}10\% target size.
The overall accuracy starts downgrading after ratio 4\% while entry $hwac\_object\_tracker$ keeps a relatively high accuracy.
This may due to the fact that its framework has a good capability in extracting features of big objects.

\begin{figure}%[htb]
\centering
  \includegraphics[width=1\columnwidth]{./figure/FPGA_category.pdf}
    \caption{Detection accuracy of FPGA entries with respect to image categories. Note that the category is sorted in a ranked order and the left-most one is with the highest average total score.}
  \label{fig:FPGA_by_class}
\end{figure}
\begin{figure}%[htb]
\centering
  %\includegraphics[width=0.95\columnwidth]{./figure/FPGA_by_size_line.pdf}
  \includegraphics[width=1\columnwidth]{./figure/FPGA_size.pdf}
  \caption{Detection accuracy of FPGA entries with respect to size.}
  \label{fig:FPGA_size}
\end{figure}

\subsubsection{Detection Results by Image Brightness}\label{fpga_brightness}
%The distribution of image brightness is similar to a Gaussian distribution as shown in Fig.~\ref{fig:dataset_brightness}.
In Fig.~\ref{fig:FPGA_brightness}, most of the training images are captured with the brightness between 105 to 165.
%The accuracy of images with different brightness is summarized in Fig.~\ref{fig:FPGA_brightness}.
However, the overall accuracy of seven entries doesn't follow the same trend as shown in Fig. \ref{fig:FPGA_brightness}.
The optimum point first arises when the brightness reaches 90 from 30.
And the trend of accuracy starts slowly climbing until 195.
After 195, the detection results show a drastic variation among entries due to the small quantities of the corresponding training images.
Thereby, it can be deducted that the detection accuracy is correlated to brightness which is the same as that for GPU entries.



\subsubsection{Detection Results by Amount of Information}\label{fpga_texture}
Fig.~\ref{fig:FPGA_texture} summaries the accuracy of all entries with the amount of information.
It can be observed that the overall accuracy trends to increase until the amount of information reaches 0.65, which is the same with the trend shown in Fig.~\ref{fig:dataset_texture}.
However, for images with an amount of information of 0.9, their accuracy is the highest though their quantities are the smallest.
Thus, it can be deducted that larger amount of information tends to achieve higher accuracy which is the same as that for GPU entries.


%\begin{table}[htbp]
%\caption{Performance of different methods}
%\label{FPGA_performance}
%\centering
%\begin{tabular}{ccccc}
%\hline
%Entries      & IoU      & Power(mW) & Energy(mWh) & FPS     \\ \hline
%TGIIF        & 0.623798 & 4200      & 5087        & 11.9553 \\
%SystemsETHZ  & 0.491926 & 2450      & 1432        & 25.9678 \\
%iSmart2      & 0.5733   & 2590      & 5124        & 7.3488  \\
%traix        & 0.610656 & 3110      & 8406        & 5.4448  \\
%hwac         & 0.520074 & 3660      & 10767       & 4.9354  \\
%Lilou        & 0.2084   & 2750      & 2546        & 15.6059 \\
%Qiu's entry   & 0.214842 & 2380      & 6404        & 5.4547  \\
%ApproxiTrack & 0.33766  & 2620      & 61200       & 1.0404  \\ \hline
%\end{tabular}
%\end{table}




%\begin{figure}[htb]
%\centering
%  \includegraphics[width=1\columnwidth]{./figure/FPGA_by_class_max.pdf}
%    \caption{Detection accuracy of GPU FPGA in category--to be deleted.}
%  \label{fig:FPGA_by_class_max}
%\end{figure}




\begin{figure}%[htb]
\centering
  %\includegraphics[width=0.95\columnwidth]{./figure/FPGA_by_bright_line.pdf}
  \includegraphics[width=1\columnwidth]{./figure/FPGA_brightness.pdf}
  \caption{Detection accuracy of FPGA entries with respect to brightness.}
  \label{fig:FPGA_brightness}
\end{figure}

\begin{figure}%[htb]
\centering
  %\includegraphics[width=0.95\columnwidth]{./figure/FPGA_by_bright_line.pdf}
  \includegraphics[width=1\columnwidth]{./figure/FPGA_texture.pdf}
  \caption{Detection accuracy of FPGA entries with respect to amount of information.}
  \label{fig:FPGA_texture}
\end{figure}


\begin{figure*}
\vspace{-6pt}
  \includegraphics[width=1\textwidth]{./figure/hard_examples.pdf}
  \vspace{-20pt}
  \caption{Hard examples with hard categories, small size, low brightness, and low amount of information in the challenge.}
    \vspace{-1pt}
  \label{fig:hardExample}
  \vspace{-10pt}
\end{figure*}
\subsection{Hard Examples and Lessons}

The results for GPU and FPGA entries show almost the same phenomena and trends as they adopted the same method (deep neural networks), and their hard-to-detect images are also almost the same.
Fig. \ref{fig:hardExample} shows some hard examples with hard categories, small sizes, low brightness, and low amount of information for both GPU and FPGA entries.
The most challenge category is \textit{group} as discussed in Section \ref{gpu_category} and Section \ref{fpga_category}.
As shown in Fig. \ref{fig:hardExample}(a) and (b), the objects in the two images are very small, and there are several similar objects around, which makes accurate detection rather hard.
The two images (c) and (d) contain very small objects (\textit{rider} in image (c) and \textit{truck} in image (d)), both of which are hard to be recognized even by humans.
Images (e) and (f) are two images with very low brightness, and the objects are very blurry without clear outlines.
With the context in image (e), humans can infer that the object is a car.
However, the object in image (f) is very hard for humans to recognize as the light is too dim.
The objects in image (h) and (i) are with very low average amount of information, which is also hard to recognize.
Image (h) contains a very small object which is almost invisible and has very limited information, while image (i) has a object which is large however with smooth surfaces and unclear boundaries with the sea.
Within all the hard examples, it can be observed that almost all the images are within very small objects.
In fact small objects are common for UAV applications which is the major challenge for accurate object detection.
Furthermore, similar objects (\textit{persons}, \textit{rider}, \textit{buildings}, \textit{boats}, \textit{cars}, etc.) and special scenarios (\textit{wake board}, operation at night) add more difficulties.

With the above hard examples and previous results discussed, we present the learned lessons as follows.
First, FPGA is much more energy efficient than GPU.
Though FPGA achieves a relatively lower FPS than GPU,  it can achieve almost the same accuracy but with only 1/3-1/2 energy consumption as that of GPU, which is promising for long-term UAV applications.
Second, object detection from UAV views in real world is complicated.
In the contest, there are many images that can not be accurately detected by all the entries.
Dividing the task into well-defined sub-tasks for specific scenarios may improve the performance.
Third, more data is preferred for more accurate detection.
In the challenge we find that many objects get a low detection accuracy when their brightness or view-angle changes.
Training images with more diversity (scale, view-angel, etc.) of the object will further improve the overall accuracy.









\section{Conclusion}
In this paper we present the DAC-SDC low power object detection challenge for UAV applications.
Specifically, we describe the unique dataset from UAV views, and give a detailed discussion and analysis of the participating entries and their adopted methods.
We also discuss the details of methods proposed by the LPODC entries for efficient processing for UAV applications.
The result analysis provides a good practical lesson for researchers and engineers to further improve energy-efficient object detection in UAV applications.





% if have a single appendix:
%\appendix[Proof of the Zonklar Equations]
% or
%\appendix  % for no appendix heading
% do not use \section anymore after \appendix, only \section*
% is possibly needed

% use appendices with more than one appendix
% then use \section to start each appendix
% you must declare a \section before using any
% \subsection or using \label (\appendices by itself
% starts a section numbered zero.)
%


%\appendices
%\section{Proof of the First Zonklar Equation}
%Appendix one text goes here.

% you can choose not to have a title for an appendix
% if you want by leaving the argument blank
%\section{}
%Appendix two text goes here.


% use section* for acknowledgment
\ifCLASSOPTIONcompsoc
  % The Computer Society usually uses the plural form
  \section*{Acknowledgments}
\else
  % regular IEEE prefers the singular form
  \section*{Acknowledgment}
\fi


The authors would like to thank DJI for providing the dataset, Xilinx and Nvidia for providing free GPU and FPGA platforms, and DAC organizers for their support to the DAC SDC challenge.


% Can use something like this to put references on a page
% by themselves when using endfloat and the captionsoff option.
\ifCLASSOPTIONcaptionsoff
  \newpage
\fi



% trigger a \newpage just before the given reference
% number - used to balance the columns on the last page
% adjust value as needed - may need to be readjusted if
% the document is modified later
%\IEEEtriggeratref{8}
% The "triggered" command can be changed if desired:
%\IEEEtriggercmd{\enlargethispage{-5in}}

% references section

% can use a bibliography generated by BibTeX as a .bbl file
% BibTeX documentation can be easily obtained at:
% http://mirror.ctan.org/biblio/bibtex/contrib/doc/
% The IEEEtran BibTeX style support page is at:
% http://www.michaelshell.org/tex/ieeetran/bibtex/
%\bibliographystyle{IEEEtran}
% argument is your BibTeX string definitions and bibliography database(s)
%\bibliography{IEEEabrv,../bib/paper}
%
% <OR> manually copy in the resultant .bbl file
% set second argument of \begin to the number of references
% (used to reserve space for the reference number labels box)

%\begin{thebibliography}{1}
%\bibitem{IEEEhowto:kopka}
%H.~Kopka and P.~W. Daly, \emph{A Guide to \LaTeX}, 3rd~ed.\hskip 1em plus
%  0.5em minus 0.4em\relax Harlow, England: Addison-Wesley, 1999.
%\end{thebibliography}

%\bibliographystyle{abbrv}
%\bibliography{reference}

\begin{thebibliography}{10}

\bibitem{tensorrt}
https://developer.nvidia.com/tensorrt.

\bibitem{bnnpynq}
https://github.com/xilinx/bnn-pynq.

\bibitem{dji}
https://www.dji.com/.

\bibitem{nvidia}
{Nvidia}.
\newblock {http://www.nvidia.com/page/home.html}, 2018.
\newblock [Online; accessed 5-July-2018].

\bibitem{openimages}
{Open Images}.
\newblock {https://www.kaggle.com/bigquery/open-images}, 2018.
\newblock [Online; accessed 4-July-2018].

\bibitem{xilinx}
{Xilinx}.
\newblock {https://www.xilinx.com/}, 2018.
\newblock [Online; accessed 5-July-2018].

\bibitem{chao2013survey}
H.~Chao, Y.~Gu, and M.~Napolitano.
\newblock A survey of optical flow techniques for uav navigation applications.
\newblock In {\em Unmanned Aircraft Systems (ICUAS), 2013 International
  Conference on}, pages 710--716. IEEE, 2013.

\bibitem{everingham2010pascal}
M.~Everingham, L.~Van~Gool, C.~K. Williams, J.~Winn, and A.~Zisserman.
\newblock The pascal visual object classes (voc) challenge.
\newblock {\em International journal of computer vision}, 88(2):303--338, 2010.

\bibitem{Geiger2012CVPR}
A.~Geiger, P.~Lenz, and R.~Urtasun.
\newblock Are we ready for autonomous driving? the kitti vision benchmark
  suite.
\newblock In {\em Conference on Computer Vision and Pattern Recognition
  (CVPR)}, 2012.

\bibitem{howard2017mobilenets}
A.~G. Howard, M.~Zhu, B.~Chen, D.~Kalenichenko, W.~Wang, T.~Weyand,
  M.~Andreetto, and H.~Adam.
\newblock Mobilenets: Efficient convolutional neural networks for mobile vision
  applications.
\newblock {\em arXiv preprint arXiv:1704.04861}, 2017.

\bibitem{iandola2016squeezenet}
F.~N. Iandola, S.~Han, M.~W. Moskewicz, K.~Ashraf, W.~J. Dally, and K.~Keutzer.
\newblock Squeezenet: Alexnet-level accuracy with 50x fewer parameters and< 0.5
  mb model size.
\newblock {\em arXiv preprint arXiv:1602.07360}, 2016.

\bibitem{janssen2017dynamic}
B.~Jan{\ss}en, P.~Zimprich, and M.~H{\"u}bner.
\newblock A dynamic partial reconfigurable overlay concept for pynq.
\newblock In {\em Field Programmable Logic and Applications (FPL), 2017 27th
  International Conference on}, pages 1--4. IEEE, 2017.

\bibitem{kragh2017fieldsafe}
M.~F. Kragh, P.~Christiansen, M.~S. Laursen, M.~Larsen, K.~A. Steen, O.~Green,
  H.~Karstoft, and R.~N. J{\o}rgensen.
\newblock Fieldsafe: dataset for obstacle detection in agriculture.
\newblock {\em Sensors}, 17(11):2579, 2017.

\bibitem{lin2017feature}
T.-Y. Lin, P.~Doll{\'a}r, R.~B. Girshick, K.~He, B.~Hariharan, and S.~J.
  Belongie.
\newblock Feature pyramid networks for object detection.
\newblock In {\em CVPR}, volume~1, page~4, 2017.

\bibitem{lin2014microsoft}
T.-Y. Lin, M.~Maire, S.~Belongie, J.~Hays, P.~Perona, D.~Ramanan,
  P.~Doll{\'a}r, and C.~L. Zitnick.
\newblock Microsoft coco: Common objects in context.
\newblock In {\em European conference on computer vision}, pages 740--755.
  Springer, 2014.

\bibitem{liu2016ssd}
W.~Liu, D.~Anguelov, D.~Erhan, C.~Szegedy, S.~Reed, C.-Y. Fu, and A.~C. Berg.
\newblock Ssd: Single shot multibox detector.
\newblock In {\em European conference on computer vision}, pages 21--37.
  Springer, 2016.

\bibitem{redmon2016you}
J.~Redmon, S.~Divvala, R.~Girshick, and A.~Farhadi.
\newblock You only look once: Unified, real-time object detection.
\newblock In {\em Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pages 779--788, 2016.

\bibitem{russakovsky2015imagenet}
O.~Russakovsky, J.~Deng, H.~Su, J.~Krause, S.~Satheesh, S.~Ma, Z.~Huang,
  A.~Karpathy, A.~Khosla, M.~Bernstein, et~al.
\newblock Imagenet large scale visual recognition challenge.
\newblock {\em International Journal of Computer Vision}, 115(3):211--252,
  2015.

\bibitem{russell2008labelme}
B.~C. Russell, A.~Torralba, K.~P. Murphy, and W.~T. Freeman.
\newblock Labelme: a database and web-based tool for image annotation.
\newblock {\em International journal of computer vision}, 77(1-3):157--173,
  2008.

\bibitem{sanmorino2012clustering}
A.~Sanmorino.
\newblock Clustering batik images using fuzzy c-means algorithm based on
  log-average luminance.
\newblock {\em Computer Engineering and Applications Journal (ComEngApp)},
  1(1):25--31, 2012.

\bibitem{torres2015automatic}
J.~Torres-S{\'a}nchez, F.~L{\'o}pez-Granados, and J.~M. Pe{\~n}a.
\newblock An automatic object-based method for optimal thresholding in uav
  images: Application for vegetation detection in herbaceous crops.
\newblock {\em Computers and Electronics in Agriculture}, 114:43--52, 2015.

\bibitem{xia2018dota}
G.-S. Xia, X.~Bai, J.~Ding, Z.~Zhu, S.~Belongie, J.~Luo, M.~Datcu, M.~Pelillo,
  and L.~Zhang.
\newblock Dota: A large-scale dataset for object detection in aerial images.
\newblock In {\em Proc. CVPR}, 2018.

\bibitem{xiao2010sun}
J.~Xiao, J.~Hays, K.~A. Ehinger, A.~Oliva, and A.~Torralba.
\newblock Sun database: Large-scale scene recognition from abbey to zoo.
\newblock In {\em Computer vision and pattern recognition (CVPR), 2010 IEEE
  conference on}, pages 3485--3492. IEEE, 2010.

\end{thebibliography}

%}



% biography section
%
% If you have an EPS/PDF photo (graphicx package needed) extra braces are
% needed around the contents of the optional argument to biography to prevent
% the LaTeX parser from getting confused when it sees the complicated
% \includegraphics command within an optional argument. (You could create
% your own custom macro containing the \includegraphics command to make things
% simpler here.)
%\begin{IEEEbiography}[{\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{mshell}}]{Michael Shell}
% or if you just want to reserve a space for a photo:

%\begin{IEEEbiography}{Michael Shell}
%Biography text here.
%\end{IEEEbiography}

% if you will not have a photo at all:
%\begin{IEEEbiographynophoto}{John Doe}
%Biography text here.
%\end{IEEEbiographynophoto}

% insert where needed to balance the two columns on the last page with
% biographies
%\newpage

%\begin{IEEEbiographynophoto}{Jane Doe}
%Biography text here.
%\end{IEEEbiographynophoto}

% You can push biographies down or up by placing
% a \vfill before or after them. The appropriate
% use of \vfill depends on what kind of text is
% on the last page and whether or not the columns
% are being equalized.

%\vfill

% Can be used to pull up biographies so that the bottom of the last one
% is flush with the other column.
%\enlargethispage{-5in}



% that's all folks
\end{document}


