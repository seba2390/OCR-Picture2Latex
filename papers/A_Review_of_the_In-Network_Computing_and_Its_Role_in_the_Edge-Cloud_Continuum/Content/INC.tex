% \section{In-Network Computing \textcolor{blue}{(CANADA)}} 
\section{A Primer of In-Network Computing}\label{INC}
\subsection{Beginnings}
\noindent In search of new solutions to improve network functionality, researchers sought mechanisms to inject customized programs -- encapsulated in “active capsules”-- into network nodes together with the classical data. In-packet encapsulated code is the main idea behind \textit{Active Networks}~\cite{ACTIVE} where network nodes actively process and perform calculations on the user data-in-transit.
As cloud-based services dominate the network landscape and ultra-low latency applications are emerging, the idea of active networks has been revived. Software-defined network (SDN) technology  provides excellent flexibility to implement network programming by separating control and data planes.
%
%
%\textit{\textbf{Q1}}: Rather than transmitting only data, why not trasmit small programs?
%
%\textit{\textbf{Q2}}: Why not execute those IN the network instead of going much further to the edge or the cloud?
 %\textit{\textbf{Q1}} was asked by the authors of \cite{ACTIVE} who wanted to improve the network functionalities by adding the programmability aspect to it.
%Yet, with limitation of network hardware, it was not possible to make it real. But it did not stop authors \cite{ACTIVE}  from answering \textit{\textbf{Q1}} by proposing what they called “active capsules” encapsulating a small program that can be transmitted through the network instead of “passive packets”. Transmitted encapsulate instructions to execute predefined programs on a network node (e.g, switch).Afterwards, \textit{\textbf{Q2}} brought the challenge of achieving network programmability. This problem was addressed with the emergence of Software Defined Network (SDN) technology through the separation of control plane from data plane.
%
The SDN Controller interacts with the forwarding plane using the OpenFlow protocol~\cite{mckeown2008openflow}, whereas P4~\cite{bosshart2014p4} has been introduced to overcome some of the initial OpenFlow limitations such as the different traffic processing components and data path hardware designs that depend on switch vendors. A P4 program allows a packet header to be classified and an action to be executed on an incoming packet. Hence, we return to what was previously introduced as an active network and what is now known as in-network computing (INC).

In summary, in addition to approaches related to hardware acceleration, four major milestones have marked the development of INC: 
\begin{enumerate}
    \item Mid-1990s: Active networks introduced programmable functions in the network, paving the way to a paradigm shift in network architectures.
    \item 2004: SDN introduced the concept of control and data plane separation and open interfaces developed between them.
    \item 2010: OpenFlow API and network operating systems established large open interface adoption with advanced and scalable control and data plane techniques. 
    \item 2014: P4 was introduced as a step towards more flexible switches whose functions are specified and modified dynamically in operating networks.
\end{enumerate}

\subsection{An Illustrative Use-case}\label{sec:Usecases}
\noindent In-network computation is used in a variety of fields to meet service requirements, such as packet aggregation, machine learning acceleration, network telemetry, and stream processing \cite{10.11453386367.3431302}.  
Figure \ref{fig:INCusecase} shows an illustrative use-case of the concept of in-network computing for delivering 360° video streaming applications. 360° video streaming has recently drawn increasing attention and publicity as an immersive technology. High bandwidth demand and intensive computational resource consumption make 360° video streaming on mobile devices challenging to achieve acceptable perceived quality. The video's visible area (known as the user's viewport)
 is displayed through a Head-Mounted Display (HMD) with a very high frame rate and high resolution. A pleasant 360° video streaming experience should prevent motion sickness, which can be met by highly responsive connectivity. The resolution of virtual reality (VR) immersive video in HMD needs to be extremely close to the amount of detail the human retina can perceive, which demands ultra-high bandwidth.
Video transcoding is a critical factor in media compress/decompress and up-sampling/down-sampling 360° videos. While centralized cloud-based computing and storage are not adequate for this type of latency-sensitive application. Edge-assisted streaming appears to be a promising solution to alleviate the poor cache hit ratio that increases the traffic volume.
%
\begin{figure}
\centering
\includegraphics[scale=0.5]{Content/Figures/INCusecase.pdf}
\caption{360°video streaming using in-network computing}
\label{fig:INCusecase}
\end{figure}
To cope with this problem, \cite{8756899} has introduced edge transcoding implemented on the in-network computing resource. 
360° videos are transcoded to create multiple quality layers and tiles at the video stream provider's servers. %Transcoding is needed because the captured 360° videos from the broadcaster may not necessarily be encoded in that format. 
The encoded video streams are then transmitted via HTTP to the Content Delivery Network (CDN). The solution proposed in \cite{8756899} allows content sharing by video frame tiling and load balancing through transcoding. In addition to potential latency reduction, a significant reduction (20 to 30\% ) in core network traffic volume can be achieved depending on the location of the transcoder.

Additional examples of in-network computing exist. For example, Vaucher et~\textit{al.} proposed an in-network data (de)compressor working at line-rate speed, allowing the offloading of the computations from end-nodes while still being suitable for time-critical applications \cite{10.11453386367.3431302}.  
%In another example, tile-based panoramic live video streaming focuses on leveraging in-network resources by dividing a video frame into several tiles. Then, transcoders near the consumers are used to decrease the bandwidth usage in the network \cite{8756899}.

\subsection{Key Definitions}
\noindent In this subsection, we classify the various INC definitions and provide a table describing how existing research describes INC.
%\subsubsection*{Taxonomy}
\begin{comment}
\begin{table*}[t]
\setlength\extrarowheight{5pt}
\centering
\caption{INC Definition Classification}
\label{table:1}
\begin{adjustbox}{width=0.7\textwidth,center=\textwidth}
\resizebox{0.75\textwidth}{!}{\begin{tabular}{|c |c|c|ccc|c|c| } 

  \toprule 


\multirow{3}{*}{\textbf{\sffamily Research}}  
       & \multicolumn{5}{c|}{\textbf{\sffamily Definition of INC}} &  \multicolumn{2}{c|}{\textbf{\sffamily Where to Process/Compute}}  \\ 
   \cmidrule(lr){2-6} \cmidrule(lr){7-8}
       &  \textit{In Network Processing}   &  \textit{In Network Computing}    & \multicolumn{3}{c|} {\textit{In Switch x}}  &    \textit{In-fabric}  &    \textit{Out-of-fabric}\\
    \cmidrule(lr){4-6}         &    &  & \textit{Aggregation }& \textit{Caching} & \textit{Traffic Proc} &    &    \\ 
   \midrule 
\multirow{1}{*}{\cite{ports2019should}}   
 
       &      & \checkmark     &  \checkmark  &      &      &     \checkmark  &  \checkmark      \\ 
        %\addlinespace 
        \hline
       \multirow{1}{*}{\cite{blocher2021holistic}}  
      
            &      & \checkmark     &  \checkmark  &   \checkmark   & \checkmark     & \checkmark     &  \checkmark   \\ 
        %\addlinespace 
        \hline
        \multirow{1}{*}{\cite{mustard2019jumpgate}}  
      
           &  \checkmark      &    &    &      &      & \checkmark     &   \checkmark      \\ 
        %\addlinespace   
        \hline
          \multirow{1}{*}{\cite{mai2014netagg}}  
      
      &   \checkmark     &    & \checkmark     &      &      &  \checkmark  &                   \\ 
        %\addlinespace 
        \hline
          \multirow{1}{*}{\cite{anwer2015programming}}  
      
        &   \checkmark     &    &    &      & \checkmark      &      &        \checkmark         \\ 
        %\addlinespace 
        \hline
          \multirow{1}{*}{\cite{10.1145/3152434.3152461}}  
      
            &      & \checkmark     &  \checkmark  &      &      &  \checkmark&     \\ 
        %\addlinespace 
        \hline
          \multirow{1}{*}{\cite{tokusashi2018lake}}  
      
           &      & \checkmark     &    &      &      &   \checkmark  &      \\ 
              %\addlinespace 
                \hline
         \multirow{1}{*}{\cite{Zilberman}}  

             &      & \checkmark     &    &      &    \checkmark   &   \checkmark   &      \\ 
        %\addlinespace 
        \hline
          \multirow{1}{*}{\cite{INC6G}}  
      
           &      & \checkmark     &  \checkmark   &  \checkmark     &   \checkmark    &     & \checkmark      \\ 
        %\addlinespace 
        \hline
          \multirow{1}{*}{\cite{alim2016flick} }  
      
            &    \checkmark   &     & \checkmark   &  \checkmark    &      & \checkmark     &       \\ 
       %\addlinespace 
       \hline
          \multirow{1}{*}{\cite{zhang2014smartswitch}}  
      
            &  \checkmark     &    &      &   \checkmark   & \checkmark     & \checkmark     &              \\ 
             \hline
          \multirow{1}{*}{\cite{InNet}} 
      
            &  \checkmark     &    &      &     & \checkmark     &      &      \checkmark        \\ 
             \hline
          \multirow{1}{*}{\cite{yang2019switchagg} }
      
            &      & \checkmark    & \checkmark      &     &     &  \checkmark     &             \\ 
            \hline
          \multirow{1}{*}{\cite{sapio2019scaling} }
            &   \checkmark   &     & \checkmark      &     &     &  \checkmark     &             \\ 
             \hline
          \multirow{1}{*}{\cite{jin2017netcache}}
          &\checkmark   &     &     &    \checkmark    &    &   \checkmark  &                 \\ 
           \hline
          \multirow{1}{*}{\cite{istvan2016consensus}}
          &\checkmark   &     &     &       & \checkmark    &   \checkmark  &                    \\ 
     
        
   \bottomrule 
\end{tabular}} 
 \end{adjustbox}
\label{tabe:INC Classification}
\end{table*}
\end{comment}


\begin{table*}
\centering

\caption{INC Definition Classification}
\begin{tabular}{|c|c|c|c|c|c|c|c|} 
\hline
\multirow{3}{*}{\textbf{Research}} & \multicolumn{5}{c|}{\textbf{Definition of INC}}                                                                                                                                   & \multicolumn{2}{c|}{\textbf{Where to Process/Compute}}                         \\ 
\cline{2-8}
                                   & \multirow{2}{*}{\textbf{In-Network Processing}} & \multirow{2}{*}{\textbf{In-Network Computing}} & \multicolumn{3}{c|}{\textbf{In Switch x }}                                     & \multirow{2}{*}{\textbf{In-fabric}} & \multirow{2}{*}{\textbf{Out-of-fabric}}  \\ 
\cline{4-6}
                                   &                                                 &                                                & \textbf{Aggregation}     & \textbf{Caching}         & \textbf{Traffic Proc}    &                                     &                                          \\ 
\hline
\cite{ports2019should}                                &                                                 & \checkmark                       & \checkmark &                          &                          & \checkmark            & \checkmark                 \\ 
\hline
\cite{blocher2021holistic}                                 &                                                 & \checkmark                       & \checkmark & \checkmark & \checkmark & \checkmark            & \checkmark                 \\ 
\hline
\cite{mustard2019jumpgate}                                & \checkmark                        &                                                &                          &                          &                          & \checkmark            & \checkmark                 \\ 
\hline
\cite{mai2014netagg}                                & \checkmark                        &                                                & \checkmark &                          &                          & \checkmark            &                                          \\ 
\hline
\cite{anwer2015programming}                                  & \checkmark                        &                                                &                          &                          & \checkmark &                                     & \checkmark                 \\ 
\hline
\cite{10.1145/3152434.3152461}                                &                                                 & \checkmark                       & \checkmark &                          &                          & \checkmark            &                                          \\ 
\hline
\cite{tokusashi2018lake}                                &                                                 & \checkmark                       &                          &                          &                          & \checkmark            &                                          \\ 
\hline
\cite{Zilberman}                                &                                                 & \checkmark                       &                          &                          & \checkmark & \checkmark            &                                          \\ 
\hline
\cite{INC6G}                                 &                                                 & \checkmark                       & \checkmark & \checkmark & \checkmark &                                     & \checkmark                 \\ 
\hline
\cite{alim2016flick}                                  & \checkmark                        &                                                & \checkmark & \checkmark &                          & \checkmark            &                                          \\ 
\hline
\cite{zhang2014smartswitch}                                & \checkmark                        &                                                &                          & \checkmark & \checkmark & \checkmark            &                                          \\ 
\hline
\cite{InNet}                                & \checkmark                        &                                                &                          &                          & \checkmark &                                     & \checkmark                 \\ 
\hline
\cite{yang2019switchagg}                                &                                                 & \checkmark                       & \checkmark &                          &                          & \checkmark            &                                          \\ 
\hline
\cite{sapio2019scaling}                                & \checkmark                        &                                                & \checkmark &                          &                          & \checkmark            &                                          \\ 
\hline
\cite{jin2017netcache}                                 & \checkmark                        &                                                &                          & \checkmark &                          & \checkmark            &                                          \\ 
\hline
\cite{istvan2016consensus}                                 & \checkmark                        &                                                &                          &                          & \checkmark & \checkmark            &                                          \\
\hline
\end{tabular}
\label{tabe:INC Classification}
\end{table*}

The inconsistencies in the names used in the literature to describe INC may cause confusion. To lift the ambiguity of what INC refers to, we present an INC taxonomy based on the existing literature. As illustrated in Table~\ref{tabe:INC Classification}, we determine three main categories bearing resemblance to INC: 
(i) In-network processing, (ii) in-network computing/compute, (iii) in switch $X$, where $X$ is either caching, aggregation, data acceleration, replication protocols, or network sequencing.
We also distinguish two types of network devices for INC ~\cite{blocher2021holistic}: (i) Devices that execute the whole computing function, and (ii) devices that rely on a connected server for function execution. 

\subsubsection*{Taxonomy}
\textit{In-network processing} refers to processing data in network devices along its transmission on the path. Authors frequently use the term on path processing explicitly, like in the works of \cite{mai2014netagg} and \cite{InNet}, or implicitly like in \cite{mustard2019jumpgate} that defined in-network processing as to where data is processed by special-purpose devices as it propagates in the network. In-network processing is not limited to network devices, but it could be extended to middleboxes~\cite{anwer2015programming, alim2016flick}or Field Programmable Gate Arrays (FPGAs)~\cite{istvan2016consensus} which are considered by the authors as the best choice to process data at line rate. 
 %It should be note worthy that papers that used this definition often propose very specific solutions to very narrowed problems in the domain of INC.
 
\textit{In-network computing/compute} refers to executing an operation or code on network devices or in virtual networks. Unlike in-network processing, in-network computing is used as an overarching concept and is often used in works that propose a complete architecture~\cite{10.11453359993.3366649} or a framework~\cite{INC6G}. Also, terms such as ``at line rate processing"~\cite{10.1145/3152434.3152461} and ``in fabric"~\cite{ports2019should} are used as synonyms to in-network computing on network devices. INC has been explicitly defined as the use of ASIC switches, FPGAs, or smartNICs for computing excluding network-linked accelerators in~\cite{NoaZilberman}.   
 
\textit{In switch $X$} refers to executing operations on the transmitted data like aggregation~\cite{InNet, 10.1145/3152434.3152461, sapio2019scaling, yang2019switchagg}, storing data on the network device such as caching~\cite{jin2017netcache}, or acceleration~\cite{tokusashi2018lake}. A switch is not a  conventional device used in the network, it could be virtual switches~\cite{INC6G} or newly developed switches~\cite{zhang2014smartswitch}.  

\subsubsection*{Deployment}
There are two basic types of deployment approaches for in-network computing devices, in-fabric and Out-of-fabric. 
In the \textit{in-fabric deployment}, computation occurs in network devices, such as programmable switches and Network Interface Cards (NICs). This approach offloads a set of computing operations from current computing nodes, e.g., servers, to the network elements such as the switches, routers, and smartNICs~\cite{10.1145/3152434.3152461}. Such an offloading accomplishes two benefits of in-network computing. First, capturing all traffic through a switch and NIC leads to a vision of the whole network. Second, in-fabric deployment does not add additional latency to the path that packets go through. However, as this method may have to support conventional routing and switching, there is a concern about the limitation of computation and memory resources. 

The second type, aka the \textit{out-of-fabric} deployment, is where in-network computing does not occur on network devices but separately from the fabric. The \textit{out-of-fabric} category is implemented using (i) a specialized hardware accelerator or (ii) exploiting Network Function Virtualization (NFV). This method does not give the same latency benefits compared to the in-fabric deployment. However, it leads to lower latency and higher throughput than the conventional approaches, where computation is performed on servers or devices outside the network. The immediate benefit is that this solution can be added without redesigning the entire data center network, i.e., the so-called incremental availability~\cite{Dan2019}.
%Research papers in the INC Domain often propose using programmable network devices instead of conventional ones to perform in-network processing or computing. therefore we need to establish a new type of criteria where we clarify research based on what type of network device is used, 


%that will delegate chunks of function, the execution is done on data plane or via an external attached device?

%A similar classification was done by \cite{blocher2021holistic} where he defined In network Computing based on three Criteria as follows: “INC denotes the general concept of offloading processing and / or storage to a programmable networking devices, which fulfills the following requirement: 
%\begin{itemize}
%\item INC processing is implemented on a network device that has as initial functionalities to %forward network packets at line rate.

%\item INC processing is more than data transmission processing, and it is mainly   tightly %interlinked with certain application running on servers that are connected to the network.

%\item INC is about processing that operates on logical layer and do not take part of data %transmission abstraction.
%\end{itemize}
%Craig et al. \cite{mustard2019jumpgate} defined In Network Processing as  where is data is processed by special purpose devices as it passes over the network. Authors, Proposed a framework that uses IN-Network Processing as A service (NPAAS) to offload operation to processing node (element as they defined it) along or near the path (data path), they also define In Network Processor as:  every device with high performance that can access directly the network. They claim that excessive  analytic operations (not limited to) need heterogeneous processing devices to be used in order to improve performance, alleviate valuable CPU resources, and decrease workload execution for the end user. Authors also defined on path devices and off path devices, where the first operates on packets and can’t buffer data across packet in a network flow. They propose that the whole record should be on one packet “packetizing complexity”.
%
%Continuing with in path definition, \cite{mai2014netagg} Mai et al. They use the term on the path processing for in network processing. Authors propose middle-box like solution to perform to aggregation along the path and no longer on the edge with the proposed solution is not really using and AGGBOX (server connected by high bandwidth link to network switch’s). The middle-boxes are used to execute application specific aggregation function at each hop. To reduce network traffic, loosen up network bottleneck. Increasing application performance.%

%Anwer et al. \cite{anwer2015programming} Define in in network processing as a deployment of monolithic middle box in virtual machines to process traffic.%to elaborate more on their.%

%Ning Hu, et al. \cite{INC6G} Defined the In-network computing as a new type of computing model that delegate application layer processing function to the network data plane where the traffic can be processed during transmission. Authors proposed a completely vitalized platform to enable IN Network computing. Their proposed paradigm, enables data aggregation  and caching in the network. also, network node can execute  a monitoring operations.%

%In-network is a dumb idea:
%Sapio et al. \cite{10.1145/3152434.3152461} uses the term In-Network Computing as define it as offloading a set of compute operation from end host into network devices such as switch and Smart NICs; in spite of using in network computation on the path at line rate, instead of processing. Authors argue that application that follow partition/aggregation workload pattern are the best candidate to use the in-network computing and exploit what it offers.%

%lake 
%Yuta et al. \cite{tokusashi2018lake} define in network computing as a novel area in computing where application execution is accelerated by placing it on network devices switch or NICs. Theses programable devices provides both network functionalities and means to execute the application.%
 
%Zilberman \cite{NoaZilberman/} define in network computing as in network computation or in NETcompute, where the execution of programs typically running on end host within the network device. He argues that INC uses existing devices that are already used for forwarding traffic. As per the author, the definition exclude network attached accelerators and uses ASIC switches, FPGA or SMART NICs. %  

%Port and Neilson \cite{ports2019should} define in network Computing as a computing that is enabled by a hardware platform that provides flexible line rate data processing capabilities on the network path. Authors envision two types of deployments to in network Computing  namely:%
%\begin{itemize}
  %\item In fabric deployment: it is where in network computing is truly emphasised because the computation is placed on the network path using programmable switches or smart NICs instead of deployment on conventional network devices.

  %\item End device deployment: it is processing IN the network using devices that are attached to the network; but separately from what they defined as fabric.
%\end{itemize}%

 %Authors argue after throughout  reflection on both types of deployments, that in fabric is the most suitable to the definition of in network computing and it will be the one used in the future due to the offered benefits.
%Alim et al. propose FLICK a framework for programming and executing application specific network services. Their goal was to allow developers to integrate efficiently application processing into network elements. The proposed framework offers high parallelism where authors argue that to achieve line rate performance, application must take advantage of both data and task parallelism. Finally, Flick is platform and programing language that target application middleboxes programming by providing domain specific language.% 

%Zhang et al. \cite{zhang2014smartswitch} Propose smart switch platform that is built upon commodity server with high speed INCs, the sole purpose of the platform is to simplify the development of services that merge computation and storage into the network. Authors argue that smart switch will enables routers to perform complexes processing on packet or it could be integrated with a networking component to ensure latency. Firstly, they propose memswitch which they claim to be more flexible than the conventional switches, offers a dynamic way to control network redirection and load balancing. Secondly, their framework, introduces platform capable of moving storage from endpoint to software based middlebox. Thirdly, memswitch offer high speed processing and filtering of Data. To summarize briefly, the contribution of this work resides in offering a memsxitch that is Memcached aware load balancing switch that decrease request latency, increase throughput, and allows in network caching.


  %\item Discussion\\
%According to different definitions given to INC, and as a result to the classification illustrated in table 1, we can give a general definition as follows: In Network Computing is a novel domain that is enabled by the appearance of several technologies enablers, and programmable switch and it consist of storing data in the network; and executing Function/code/ Program on network devices. The later could be programmable switch, a smart NICs FPGA, Vswitch, or conventional device that is empowered by attached accelerator or other devices. INC execution should take place on the path, and further from remote servers.%
%The only conflict from the studied paper is that Virtual network, Virtual network function are not considerate in the realm of INC, because virtualization on remote server still far from the user and no benefit will be gained in term of improving performance and processing time.%


%We stated previously that the INC was enabled by different technologies. Before going into the details of each one, we will tackle in the next subsection an overview of INC and shed light on the role of each INC enabler.
%\subsubsection{Overview\\}
%Programmable switches ASICs and the evolution of smart NIC were the first enablers of INC. Traditional network devices had a fixed operation to execute, and their functionalities were defined during manufacturing, so they were not remotely configurable, while programmable switches allow user to define its own functionalities using high level language. The first dominant, and used language is P4 (cf.\ref{Prog}). The latter was initially used to mainly define new protocols and network related functionalities, but researcher did not settle for only these two functions and extend the usability of P4 to build more complex functionalities to network. Not being limited to p4 enhancement, researcher did get inspired by the whole architecture of SDN to propose novel more sophisticated and complexes In-Network services (cf. \ref{SDN}). Network Function Virtualization (NFV) allows implementation of network function as software on both commodity and hardware devices. This Technology integrated with SDN will enable to build a wild range of INC application and services (cf.\ref{}).Another domain, that is integrated with virtualization is named data networking (NDN) . The synergy between NDN and NFV resulted to the Named Function Networking (NFN) Where the execution of virtual function within the network will be based on expression of interest (cf.\ref{NFN}) which will improve the execution of virtual function within the network. In addition, we stated that INC is not only about in network computation but in include the in network storage and this is enabled by NDN (cf.\ref{NDN}).  
%In-network computation enables computation inside the network, instead of restricting computation to servers or devices which are outside the network. This has two main categories, named in-fabric and end-device developments. Although in-network computing enables the capability of computation in network devices, it differs from a general-purpose solution. This works based on offloading primitives, not entire applications. Therefore, it is important to implement a key primitive from the application in the network, and use the conventional implementation for the rest of the application. This would be more practical if primitives are reusable for various applications \cite{Dan2019}.
\section{Technology Enablers} \label{INC_Enablers} 
\noindent In this subsection, we describe how the current advances brought by network softwarization such as Named Data Networking (NDN), SDN, and NFV technologies, enable the INC paradigm. We explain how NDN can be extended to support INC from a networking perspective. Then, we present the role of SDN and NFV in supporting INC from a virtualization perspective.

 \begin{figure}
    \centering
    \includegraphics[scale=0.41]{Content/Figures/PIT_expiry_problem.png}
    \caption{PIT entry expiry time in NDN: (left) the problem of PIT entry expiry time, (right) solution using thunk name}
    \label{fig:PITexpiry}
\end{figure}

\label{NDN}\subsection{NDN as a Networking Technology enabler}
\noindent NDN is a data-centric approach for networks that is based on location-independent names to find a requested content~\cite{NDNsurvey}.
As an alternative to the traditional TCP/IP-based Internet architecture, the NDN paradigm provides exciting features that can be extended to support in-network computing. For instance, NDN allows in-network content caching to respond to queries requesting existing content. This mechanism can be exploited to cache already calculated results to avoid redundant computations. In addition, NDN forwarding strategies can be enhanced to implement a location-independent execution and compute at the best network location~\cite{NFCC}.
There are two basic types of messages in NDN architecture that can be leveraged for in-network computing~\cite{NDNwireless}: \textit{Interests} representing requests for a particular computation to be executed and \textit{Named Data Objects} (NDOs) which can be a calculated result, parameters for the computation task, or a program code. The consumer sends an interest specifying the NDO to be fetched. The router then forwards this interest to the provider hosting the desired NDO. Each NDN forwarder involves three main components: (i) the Content Store (CS) that can cache NDOs (e.g., already calculated results), (ii) the Pending Interest Table (PIT) that is a storage of the interests requesting computation and the interface taken by these interests, and (iii) the Forwarding Information Base (FIB) which stores the name prefix of each execution result along with the following hops to reach the destination.

Notwithstanding the advantages offered by the NDN architecture, a few challenges associated with  NDN components for enabling in-network computing remain to be solved.\\
Firstly, network congestion is possible because of the large number of requests generated in network situations such as those of an IoT environment, and also because the network devices function as both forwarders and computers. Second, a PIT entry may no longer be valid after a specific time if a calculation takes longer time than the expiry time of the PIT entry. To overcome these challenges, \cite{COicn} proposed a framework that uses a computation-centric architecture over NDN with in-network load balancing of incoming computation requests and \textit{thunk names} to take into account the PIT entry expiry time. The thunk is a name used by consumers to retrieve execution results~\cite{RICE}. Figure~\ref{fig:PITexpiry} depicts the problem with the PIT entry expiry time and how it can be avoided using thunk names. In a traditional NDN architecture, a consumer sends a computing interest to the producer. Let us assume that the computation takes 10 ms and the PIT entry expiry time is 3 ms. The consumer first sends an interest that takes 3 ms, which expires later. Since the computation takes 10 ms, the consumer will be sending four interests before receiving the result. If the thunk name is used, the consumer sends the computation interest, and the producer responds with a thunk name and an estimated time. The consumer waits this estimated time and then fetches the result using the thunk name. based on these, the same work also demonstrates how in-network computing is implemented over NDN. The demonstration actors are Android smartphones (i.e., users), an access point providing wireless connectivity and performing in-network load balancing among the workers (e.g., Raspberry Pi). Workers are the devices responsible for in-network computing of users' requests. They retrieve the computation tasks as unikernels from the network. The use of unikernels for task execution over NDN is also proposed in \cite{10.1145/3125719.3125727}, where a framework based on serverless computing is proposed to extend NDN architecture in support of in-network execution of computation tasks (i.e., unikernels) via kernel stores. 
Based on a statistics table (i.e., a structure already existing in NDN), the kernel store decides which unikernels to store, which ones to remove, and which ones to execute.

 \label{NFV} \subsection{SDN/NFV as a Virtualization Technology enabler}
%Virtualization enclose different technologies to manage computing resources, providing an abstraction layer that acts as a translation layer between software and physical hardware. It hides the awareness of the underlying physical resources from the users and application by turning physical resources and hardware into virtual ones\cite{stallings2015foundations}. Applying virtualization in the network was fist discussed by network operators and carriers in order to seek solution on how to improve the network, and it  resulted in the first paper that introduced Network Function virtualization \cite{chiosi2012network}.  Authors summarized the objective of NFV and the over all benefit of leveraging virtualization to integrate different network equipment (e.g.  switches, storage, network nodes) on commodity server \cite{stallings2015foundations}. 
%Benefits of virtualization on the network are limitless, and especially NFV that responded to the negatives consequences of non stopping growth and variety of network appliances. Advantage of NFV compared to traditional network are in terms if reducing both OpEx and CapEx, reducing the time of deploying new network services by offering a standardization and open interfaces which will ease the interoperability issues and providing agility and flexibility etc. With all these attractive features brought by the virtualization it is inevitable to exploit it, to ensure better performance of in network Computing. Thereof, Virtualisation is considered as a major enabler of INC, and several contribution in this area are based on Virtualization one way or another. Bases on contributions proposed for in network Computing, virtualization is often used partially to lift the overhead of network and accelerate network processing by proposing virtual middlboxes \cite{alim2016flick} \cite{mai2014netagg}, or virtual switches \cite{zhang2014smartswitch}; or completely by \cite{hu2021energy} that proposes a complete vitalized platform. 


\noindent By incorporating INC into the Cloud-Edge-Mist continuum, off-loaded computing tasks are executed through an integrated infrastructure that optimizes network and computing resources. Each computational task can be decomposed into a set of subtasks. Each subtask can be realized as a virtualized function through containers or unikernels, deployed across the Cloud-Edge-Mist continuum. Virtualization is considered a major enabler of INC, as shown by Table~\ref{table:INC Framework}. According to previous related works, virtualization is often used to improve network overheads and accelerate network processing through virtual middleboxes. For instance, the authors of \cite{mai2014netagg} have proposed \textit{AGGBOX} to perform the aggregation along the path instead of the edge. \textit{AGGBOX} consists of a server connected by a high bandwidth link to network switches. Middleboxes are used to execute application-specific aggregation functions at each hop to reduce network traffic and solve the network bottleneck, thereby increasing application performance. 
Another use of virtualization is through virtual switches, e.g., in \cite{zhang2014smartswitch} the authors introduce a \textit{SmartSwitch} platform built on commodity servers connected with high-speed NICs. The proposed platform aims at simplifying the development of services that merge computation and storage into the network. The authors argue that \textit{SmartSwitch} can enable routers to perform complex processing on packets, or it could be integrated with a networking component to satisfy latency constraints. The authors first propose \textit{MemSwitch}, which they claim to be more flexible than the conventional switches by offering a dynamic way to control network redirection and load balancing. Secondly, their framework introduces a platform capable of moving storage from an endpoint to software-based middleboxes. Thirdly, \textit{MemSwitch} offers high-speed processing and filtering of data using memcach (i.e, generale distributed memory caching system). To summarize, the resulting \textit{MemSwitch} is a memcached-aware load balancing switch that decreases request latency, increases throughput, and allows in-network caching. 

On the other hand, in the era of cloud/edge computing and containerization, the virtualization of microservices has become very popular in the industry and academia because of its practical benefits. As microservices are self-contained services that perform a specific task, they are excellent candidates for in-network computing. Indeed, network devices such as switches have limited resources and may not be able to handle large monolithic services. Thus, it is better to break these down into microservices to make them smaller and more manageable~\cite{ENTICE}. 

In addition to the fundamental aspect of microservice architecture, namely scalability, it also addresses heterogeneity.  Virtualization allows the deployment of microservices on different devices of various capabilities in a heterogeneous infrastructure. Thus, virtualized microservices have been a key factor in improving cloud application performance~\cite{VirtualizationPerformance, 40}. In many studies, microservices are often implemented in containers because they are lightweight, efficient, and more deployable than traditional virtual machine (VM)-based implementation. In addition, container technology can be used to minimize the effects of virtualization on system resources and reduce its cost~\cite{46}. Implementing microservices as virtual functions taking benefits from INC would highly enhance both network and application performances.  
%Overall, container-based virtualization is considered the best choice for deploying microservices architecture from an efficiency perspective. Since there are multiple things in common such us light weight and high scalability.

%In another work \cite{INC6G}, a whole framework for INC paradigm was proposed basing on virtualization and SDN. Authors introduced a unified scheduling and programming of computing tasks over an infrastructure integrating of both host node computing and network node computing. The proposed solution supports network traffic processing, execution of aggregation, caching, and analysis. 
\begin{figure}[h]
%\centering
\includegraphics[scale=0.47]{Content/Figures/SDNFV.drawio.pdf}
\vspace*{-16mm}
\caption{In-network computing architecture using SDN, NFV and NDN (adapted from \cite{8856241} and \cite{10.11453445814.3446760})}
\label{fig:incArchitecture}
\end{figure}
The authors of \cite{8856241} proposed an SDN-based architecture of named computing services in edge infrastructures composed of three different planes: data, control, and application. The data plane contains the network nodes that make up the managed domain (i.e., ingress/egress nodes, backbone, intermediate, and access routers). The control plane includes the SDN controller managing its domain according to specific orchestration policies defined at the application plane. Moreover, a hierarchical control plane architecture can be envisaged for scalability purposes.\\
Following the architecture proposed in \cite{8856241} figure~\ref{fig:incArchitecture} illustrates the envisaged in-network computing architecture while making use of SDN, NFV, and NDN, where the gray components are the extensions to the conventional NDN proposed by~\cite{10.11453445814.3446760}. The three layers are described as follows.
\begin{itemize}
    \item \textit{Data plane:} In addition to NDN-specific data structures, each node keeps a Function Store (FS), specifically added to store the locally available functions that can be unikernels or containers. The SDN controller can specify a set of actions for the data plane, including (i) forwarding the interest to a given output port; (ii) contacting the controller; (iii) managing the computation locally. 
   \item \textit{Control plane:} For named service orchestration purposes, the controller needs to track the resource status of both the network and the nodes. Thus, the control plane implements two functionalities (i) network resources monitoring, and (ii) storage and computing resources monitoring.
    \item \textit{Application plane:} Adequate resource provisioning in such a system requires additional network applications compared to legacy NDN and SDN design, e.g., for security support and service orchestration. For instance, the authors in \cite{RICE, 8539024} have raised concerns that in-network computation creates new security challenges. In this work, first, the consumers should certify that the new contents are resulting from the computations. This can be ensured by specifying different security policies at the application plane and applied by the controller. Second, service orchestration decides where to execute a named computed request (executor node). The requested processing function can either exist locally at the FS of the selected executor node or be retrieved from a Function Repository (FR) hosted in a remote server known by the controller. Note that each FR is associated with a Function Catalog (FC)~\cite{ETSI}, which includes function descriptors (e.g., deployment template, required resources, etc.). This illustrates
    well how this plane works. 
\end{itemize}
%\textcolor{blue}{Further details on the interaction between the components illustrated in Figure~\ref{fig:incArchitecture} are given in Subsection~\ref{NFN}.}

Overall, in the INC context, the centralized control plane is primarily responsible for network optimization by calculating centrally optimized network policies and installing the corresponding flow tables on data plane devices to support processing and forwarding operations. A good example of such a plane is the context-aware pipeline designed for \textit{In-Switch $X$} targeting increase service satisfaction possibilities~\cite{li2021advancing}. The data plane first utilizes the context-aware caching and computing pipeline for the incoming packets to look for precomputed results that will significantly accelerate the service process and save communication and computation costs. In \cite{li2021advancing}, the SDN controller periodically collects important monitoring information (e.g., topology, traffic load, function lists, computation load, and content) to improve its policies. Moreover, the SDN controller installs extended fine-grained flow tables on data plane devices to support processing and forwarding operations of both interest and data packets.
Similarly, \cite{9350853} proposes an SDN-like collaborative control that periodically calculates the optimized task execution schedule for the entire network. Based on the centralized policies and status reports of the devices, the SDN-like collaborative control adjusts the forwarding strategy of the networking devices.\\
A classification of the aforementioned papers based on the used enablers (i.e., SDN or NDN), is given in Table~\ref{table:INC Framework}.
%\textit{Energy consumption}: The existing INC model is CPU-centric, where the implementation scheme is bound to hardware. Since the energy consumption model of computing nodes is very complex, involving the power consumption of multiple components such as the CPU, memory, and I/O system. Executing each application task on computing node involve the same number of components - setting up an upper bound on the access requests while limiting the CPU utilization. Being a key technology enabler, virtualization improve the overall utilization of hardware resources. With the help of virtual machines and containers, multiple application tasks can be deployed to run on the same physical device, which improves the utilization of hardware resources and reduces energy consumption. With such motivation, the work in \cite{hu2021energy} proposed a new paradigm based on the deployment of virtualization software and a container environment on the host system to achieve a network computing model with better flexibility and energy efficiency.

 
%In addition, data aggregation and in-network caches - are the most prominent efforts in the literature that leveraged SDN based model to implement INC services. In \cite{sapio2017network}, taking the MapReduce \cite{dean2004mapreduce} application as an example, the controller defines the aggregation tree and sends the data aggregation processing rules to the switch in the form of a flow table. Upon receiving the data packet, the switch parses the message and then aggregates the data according to the flow table rules issued by the controller. Similarly, another architecture called SwitchAgg \cite{yang2019switchagg} proposes in-network data aggregation framework that integrate the packet load analysis module and data aggregation processing engine into the switch. The network is used to aggregate results and, in turn, redruce latencies and improve throughput. Besides data aggregation, IncBricks \cite{liu2017incbricks} is an in-network caching fabric with basic computing primitives, primarily proposed for the data center networks. IncBricks requires that the data center has a centralized SDN controller connecting to all switches so that different IncBricks instances can see the same global view of the system state. The centralised controller maintains an updated global registration table on each IncBox. Another work proposed a framework called NetCache \cite{jin2017netcache} that emphasize on cache hit ratio with bounded latencies even under highly-skewed and rapidly-changing workloads. NetCache realizes the application-level function, which handles the key-value query, on the data plane of the network. Since the popularity of each key changes over time, NetCache leveraged controller to handle the dynamic workload on the switches. To be more specific, the switch is responsible for implementing on-path caching for key-value items while the controller is primarily responsible for updating the cache with hot items. 



\begin{table*}
\caption{Current literature on INC frameworks}
\setlength\extrarowheight{5pt}
\label{table:INC Framework}
\begin{adjustbox}{width=\textwidth,center=\textwidth}
\resizebox{0.5\textwidth}{!}{
\begin{tabular}{ |c |c| c| c|}
\hline
\textbf{Papers} & \textbf{Enablers} & \textbf{Contribution}  & \textbf{How the enabler is used}\\
\hline
\cite{10.1145/3125719.3125727}& NDN & Named Function As A Service & Named function\\
\hline
\cite{RICE}& NDN & Remote Method Invocation in ICN & Named routing\\
\hline
\cite{INC6G} & SDN & In-Network Computing paradigm based on virtualized platform & SDN-like collaborative controller \\
\hline
\cite{li2021advancing}&SDN& A software-defined service-centric networking (SDSCN) framework & SDN Controller \\
\hline 
\cite{8856241}& SDN and NDN & \makecell{Managed Provisioning of Named Computing \\ Services in  Edge  Infrastructure} &\begin{tabular}[c]{@{}l@{}}  ~\begin{tabular}{@{\labelitemi\hspace{\dimexpr\labelsep+0.5\tabcolsep}}l@{}}Service Allocation decision mechanism based on SDN\\Request and delivery of Service  by Names\end{tabular}\end{tabular} \\

\hline
\cite{9350853} & SDN and NDN & A shared in network computing infrastructure &\begin{tabular}[c]{@{}l@{}}  ~\begin{tabular}{@{\labelitemi\hspace{\dimexpr\labelsep+0.5\tabcolsep}}l@{}}Name-based computing functions\\SDN-like collaborative schedule
controller\end{tabular}\end{tabular} \\
\hline
\cite{10.11453359993.3366649} &SDN and NDN& Software-Defined Service-Centric Networking (SDSCN) &\begin{tabular}[c]{@{}l@{}}  ~\begin{tabular}{@{\labelitemi\hspace{\dimexpr\labelsep+0.5\tabcolsep}}l@{}}Name-based forwarding functions\\Name-based function\\ Edge controller\\Cloud controller \end{tabular}\end{tabular} \\
\hline
\end{tabular}
} 
 \end{adjustbox}
\end{table*}


\section{Resource Provisioning in INC-enabled systems}\label{ResourceAllocation}
One of the main challenges in INC is where to place computing resources over the forwarding plane, this section aims to answer this question by describing the resource allocation mechanism from an INC functionalities perspective. We first survey existing work on computation entities placement and chaining i.e. VMs, containers, and functions. Next, task scheduling approaches and INC context peculiarities are summarized.

%\textcolor{violet}{INC introduces a new version of a network that offers a variant pool  of resources ranging from storage entities (INC caching) to computational units (PND, access points, or base station). In this section, we examine how existing research tackles the challenges of allocating resources in the network to Virtual Network function, Service Function and Micro-Services. Then, we  accentuate how proposed solutions differ from the ones proposed in the Edge-cloud Continuum. 
%Moreover, we present studies that elaborate on how to schedule tasks and orchestrate resources in INC. }\\

%\st{\noindent We present in this section the resource allocation problem in the network continuum considering the INC paradigm. First, we highlight the particular challenges encountered in the investigation of the optimal placement of components in a specific host environment. In addition to typical resources (caching, networking), the components may also include computational units (VMs, containers, functions). Second, existing research studies on task scheduling and resource orchestration in INC are presented.}
%\subsubsection{Architectural Aspects}
 
\subsection{Placement of computational entities}

\noindent The placement of computational entities has been widely studied in cloud environments with the goal of efficiently distributing services to data centers depending on their requirements in terms of latency, storage, and/or computation~\cite{cloud1}. However, it is largely admitted today that the techniques developed for cloud environments are not applicable to the heterogeneous and dynamic environments of edge computing. Likewise, the placement mechanisms proposed for edge and cloud computing may not be appropriate for in-network computing due to the nature of the underlying network devices, i.e., their significantly limited resources and computation capabilities~\cite{placeDelayINC, distributedINC}.
%The existing works mainly study the placement of virtual network functions (VNFs) or microservices  in different environments.
Application combining In-network and edge computing can be of great variety (e.g., AR/VR applications, online gaming, or autonomous robotics), so the orchestrator must be equipped with an adaptive placement strategy that allows effective decisions. The placement algorithms should take several challenges into consideration~\cite{placementSurvey}: (i) the heterogeneity of computational entities (e.g., virtual machines, containers, functions, and tasks), (ii) the variety of the equipment that can host these entities (e.g., servers, base stations, access points), (iii) the dynamicity of network conditions (e.g., mobility or immobility), and (iv) the different end-user requirements. In recent years, many efforts have been made to address these problems, from simple algorithms and heuristics to solving them with artificial intelligence techniques.

Placing a computing task in the network requires two essential components: (i) an executor node with available processing resources, and (ii) the transfer of the input data from the source to the selected executor node for processing. In highly distributed computing environments, where the application components consume computing, storage, and networking resources, the task and corresponding data placement algorithms substantially impact the performance. The placement strategies face two main challenges. First, (in general, a vast amount of) data exchange  via task executor nodes requires an adequate bandwidth provision. Second, task completion delays are intrinsically linked to end-to-end processing delays imposed by the location of task executor nodes.
%\begin{figure}
%\centering
%\includegraphics[scale=0.5]{Figures/SFCmapping.png}
%\caption{SFC graph embedding across the cloud edge continuum}
%\label{fig:sfcmap}
%\end{figure}

%One of the placement types is service function chain embedding (SFC) also called service graph embedding (SGE) where a service function chain is a sequence of ordered and related functions and the goal is to find a mapping of the service functions and the links that connect them onto the physical network \cite {hybridSFE}. 
%Figure \ref{fig:sfcmap} illustrate an example of SFC  computational graph that consists of functions (piece of code) and data (input/output) connected for message passing and end-to-end service chaining. The computational graph embedding on a heterogeneous not fully connected infrastructure spanning the cloud edge continuum is an open issue. Optimal placement of functions, distributed across the processing graph is of paramount importance. In addition to the afore-mentioned challenges, the placement algorithms should place functions near required data for processing to avoid shifting high data volume to a function running across the cloud edge continuum.


Likewise, service function chaining (SFC) is recognized as an essential technique for connecting a sequence of SDN/NFV-based Service Functions (SFs). Achieving low-latency services entails reducing the SFC completion time through processing acceleration of softwarized SFs. Data plane reconfigurability and line-rate packet processing performance enabled by the Programmable Data Plane are vital to achieving that goal. 
Employing SFC in a programmable data plane can be done in two ways~\cite{FASE}. Firstly, a redundant service function approach that allows the easy processing of ordered SFs that make up an SFC is discussed in~\cite{P4SC}. In this approach, SF tables are redundantly embedded in the programmable data plane switch in order to satisfy the processing order of the SFC at the line rate. However, the limited resources of programmable data plane devices may not efficiently allow embedded service function redundancy. Second, the authors of ~\cite{AccSFC, RESFC} proposed the re-circulation approach when the order of SFC functions is different from the one of embedded SFs. A packet is recirculated from the egress to the ingress port. However, it is hard to guarantee the line-rate performance due to re-circulation. \cite{FASE} proposes a Flow-Aware Service function Embedding (\textit{FASE}) that balances these two drawbacks. It combines the redundant service function and re-circulation approaches to find the optimal service function embedding that minimizes the SFC completion time while the programmable data plane switch resources are efficiently utilized. 

From an architectural perspective, VNFs tend to be monolithic (e.g., load balancers, WAN optimizers), complicating their deployment in network devices. As INC embraces a heterogeneous infrastructure with differently sized hosts and networking devices, many of them with limited resources such as switches or smartNICs, the microservice architecture offers many attractive characteristics (e.g., heterogeneity and scalability) that accommodate service deployment in such environment. Applications can be decomposed into functionalities such as packet header parsing and packet classification~\cite{VNFMicro}. These small components can be easily integrated into network devices with limited capacity.
The authors of \cite{PIAFFE} have proposed a framework, called \textit{PIaFFE} (A Place-as-you-go In-network Framework for Flexible Embedding of VNFs), for VNF placement using in-network processing. In this work, the VNF logic is decomposed into embedded network functions \textit{eNFs} to identify the overlapped functions and efficiently place them in in-network programmable devices. \textit{eNFs} are compact and lightweight VNFs that use a high-level programming language and fit into a smartNIC.  
In addition, \cite{LightNF} proposes \textit{LightNF}, a system that allows network functions to be offloaded into programmable switches. The proposed system in \textit{LightNF} is equipped with an analyzer that examines the characteristics and options of offloading network functions and the resources consumed by each function. Then, based on this analysis, it builds an optimization framework that decides on the placement of SFCs.

%Moreover, Wireless network devices are limited in computing and storage. Thus, Containerizing lightweight microservices is essential \cite{wirelessINC}. One challenge is to map each container to the appropriate fog node without disturbing its basic functionalities (i.e. routing and forwarding packets). To this end, it is required to investigate underlying network characteristics and residual computing capabilities such as the number of CPU cycles, storage, RAM and battery power. Many underlying wireless network characteristics should be considered for mapping microservices as containers over the fog network nodes. These parameters that experience impact due to the dynamic nature of wireless networks are signal-to-noise ratio (SNR), path cost between source and destination, number of associated stations, receiving and transmitting errors, packet drop ratio and transmission speed. Lastly, the placement decision for microservices should optimize the metrics above.


To sum up all these, Table~\ref{placement} classifies the studies related to function placement according to four criteria. The first criterium is computation entities. Service function is a concept mainly used in service function chaining, where the service functions are connected to form a service that responds to a particular request. A task is an entity that focuses on one single action. Finally, embedded network functions are lightweight and small components of a VNF. The second criterium is the computing paradigm (In-network and/or edge computing) considered in each paper. The third criterium is the dependency of entities which refers to the connection (e.g., SFC) or the nonconnection (e.g., a single component) between the components. The last criterium pertains to the objective performance metrics considered in the placement of computational entities.

%% begin table

\begin{table*}
\setlength\extrarowheight{5pt}

\centering

\caption{Classification of works on placement}
\label{placement}
\begin{adjustbox}{width=\textwidth,center=\textwidth}
\resizebox{0.5\textwidth}{!}{
\begin{tabular}{|c|c|c|c|c|c|c|c|c|c|c|c|} 
\hline
\multirow{2}{*}{\textbf{Papers}} & \multirow{2}{*}{\textbf{Computation entities}} & \multicolumn{2}{c|}{\textbf{Computing paradigm}} & \multicolumn{2}{c|}{\textbf{Entities dependency}} & \multicolumn{6}{c|}{\textbf{Performance metrics}}                           \\ 
\cline{3-12}
                        &   & \textbf{INC} & \textbf{EC}  & \textbf{w/CXN} & \textbf{w/o CXN}           & \textbf{Latency} & \textbf{Throughput} & \textbf{Bandwidth} & \textbf{Energy} & \textbf{RU} & \textbf{Cost}  \\ 
\hline
\cite {LightNF}                      & Network Functions                     & \checkmark                  &                  & \checkmark           &                          & \checkmark     & \checkmark        & \checkmark       &        &                &       \\ 
\hline
\cite{distributedINC}                      & Tasks and computing resources         & \checkmark                  & \checkmark              & \checkmark           &                          & \checkmark     & \checkmark        & \checkmark       & \checkmark    &                & \checkmark   \\ 
\hline
\cite{FASE}                      & Service Functions                     & \checkmark                  &                  & \checkmark           &                          & \checkmark     &            &           &        &                &       \\ 
\hline
\cite{placeDelayINC}                      & Tasks                                 & \checkmark                  & \checkmark              &               & \checkmark                      & \checkmark     &            &           &        & \checkmark            &       \\ 
\hline
\cite{PIAFFE}                     & embedded Network Functions            & \checkmark                  &                  & \checkmark           &                          & \checkmark     & \checkmark        &           &        & \checkmark            &       \\
\hline
\multicolumn{12}{l}{INC: In-Network Computing, EC: Edge Computing, CXN: connexion, RU: Resource Usage.}
\end{tabular}
} 
 \end{adjustbox}
\end{table*}




%%end table
 \subsection{Task Scheduling}

\noindent Task scheduling aims to distribute computing resources fairly and efficiently, thereby enhancing resource utilization. However, due to increasing workloads there are several challenges to be addressed. These challenges include load balancing, profit optimization, and software complexity~\cite{TSEC}. Considering INC, together with cloud and edge computing, adds extra complexity to the scheduler~\cite{TSVNF}. Hence, choosing which device to schedule computational tasks is crucial. The randomness of user request arrivals, physical device diversity, and their limited amount of resources should be taken into account when assigning tasks to devices. Additionally, each specific application has its requirements. For instance, it is necessary to consider task processing time, full utilization of resources, and bandwidth costs~\cite{TSVNF}.

Although the resource management problem in traditional data centers (without INC) has been extensively studied, the problem is complex, and there is still no solution that fits all situations. The addition of the INC dimension to the resource pool not only exacerbates the existing challenges but also adds new ones~\cite{10.11453445814.3446760}. First, programmable network components (e.g., ASICs, FPGAs, and NPUs) are heterogeneous in terms of their programming models and interfaces compared to servers that support general Turing-complete computations. Thus, the same INC service exhibits different resource demands when deployed on different components. Second, INC services can request interchangeable resources with different performance properties. This results in extra complexity for scheduling appropriate resources for each INC job. Third, INC service comes with high locality constraints. Thus, the locality decision of a specific device impacts all others. Fourth, sharing INC resources may generate a nonlinear behavior, such as reusing partial INC resources by multiple services on the same network device. The authors of \cite{10.11453445814.3446760} address a few of these challenges by proposing \textit{HIRE} (Holistic INC-aware Resource managEr), a novel data center scheduler solution. \textit{HIRE} includes a scheduler that generates a flow network embedding all the scheduling constraints and objectives based on polymorphic resource requests (see Figure~\ref{fig:hire}). The scheduler considers the alternatives and non-linear resource sharing in the polymorphic requests. These requests represent the input of the \textit{HIRE} scheduler. The scheduler then generates a flow network based on a cost model that translates the current resource status, resource demands in polymorphic requests, and scheduling objectives into a flow network with costs on arcs.


\begin{figure}[!h]
%\centering
\includegraphics[scale=0.7]{Content/Figures/HIRE.drawio.pdf}
\caption{HIRE Architecture (adapted from \cite{10.11453445814.3446760})}
\label{fig:hire}
\end{figure}

%The environment affects the challenges and requirements of task scheduling. In Compute First Network (CFN), scheduling can be a challenge due to the variety of devices and networks, which prevents having a similar forwarding strategy for computing tasks and packets \cite{IEN}.
%Data centers are the main ones affected by this challenge. According to \cite{10.11453445814.3446760}, resource management in data centers without considering in-network computing is already a problem studied in several works. Using massive amounts of data in data centers increases energy consumption. In addition, the task scheduling algorithms adopted cannot fundamentally reduce the amount of data to be processed \cite{INC6G}. Hardware resources can be improved using virtual machines and containers to deploy several tasks on the same device. On the other hand, software resources can be enhanced with the help of scheduling. Thus, an efficient task scheduling mechanism is needed to schedule computing in the network devices, consequently reducing the massive data transmission to the network level. 

Likewise, the authors in \cite{9350853} propose a shared in-network computing infrastructure named Intelligent Eco Networking (IEN). This infrastructure has two layers (i.e., the element and control layers). The control layer is responsible for scheduling the tasks executed in different nodes. However, the element layer is where compute tasks (i.e., name-based computing functions) are deployed. The authors also design an SDN-like collaborative schedule set in the slicing network layer along with name-based computing functions. The use of network slicing allows the proposed framework to coexist with the traditional TCP/IP network. The SDN-like collaborative schedule controller helps task scheduling and works as a global optimizer for the entire network. It runs dynamic programming algorithms for uncertain resources based on two strategies. Firstly, the multi-attribute decision-making strategy is scalable and adapts to the allocation of computing power while considering the static network and the dynamic computing resources. Secondly, information entropy is used for weight assignment to represent the dynamicity of network changes.

Finally, \cite{INC6G} proposes an in-network computing paradigm based on virtualization and software-defined technologies to realize unified scheduling and programming of computing tasks. The proposed paradigm allows the delegation of computing tasks to different nodes, including network devices, and rapid migration of tasks between network nodes. It also supports application-level service scheduling. The task scheduling of in-network computing is modeled as a multi-objective optimization problem. The optimization problem combines task scheduling and network forwarding to minimize the overall idle rate of resources, the overall energy consumption, and the overall data transmission overhead. 

Here again, Table~\ref{taskscheduling} summarizes the comparison between the works related to task scheduling. This comparison is based on the scheduling algorithm, the objective considered for scheduling, and the scheduling type (tasks and/or resource scheduling).

%% begin table
\begin{table*}
\setlength\extrarowheight{5pt}
\centering
\caption{Comparison of various task scheduling papers}
\label{taskscheduling}
\begin{adjustbox}{width=\textwidth,center=\textwidth}
\resizebox{0.5\textwidth}{!}{
\begin{tabular}{|c|l|l|c|c|} 
\hline
\multirow{2}{*}{\textbf{Papers}~} & \multicolumn{1}{c|}{\multirow{2}{*}{\textbf{\textbf{Scheduling algorithm}}}} & \multicolumn{1}{c|}{\multirow{2}{*}{\textbf{Objective}}}                                                                                                                                                                          & \multicolumn{2}{c|}{\textbf{\textbf{\textbf{\textbf{Scheduling}}}}}  \\ 
\cline{4-5}
                                  & \multicolumn{1}{c|}{}                                                        & \multicolumn{1}{c|}{}                                                                                                                                                                                                             & \textbf{Tasks} & \textbf{Resources}                                  \\ 
\hline
\cite{TSEC}                                & Resource Constrained Task Scheduling Profit Optimization algorithm (RCTSPO)  & Maximize profit                                                                                                                                                                                                                   & \checkmark            &                                                     \\ 
\hline
\cite{TSVNF}                               & Greedy Available Fit (GAF) task scheduling algorithm                         & Increase operational efficiency                                                                                                                                                                                                   & \checkmark              & \checkmark                                                   \\ 
\hline
\cite{TSVNF}                               & Heuristic resource management \textit{HealthEdge}                            & Minimize task processing time                                                                                                                                                                                                     & \checkmark              &                                                     \\ 
\hline
\cite{10.11453445814.3446760}                                & Flow-based scheduling using Min-Cost Max-Flow (MCMF) solver                  & Maximize job success rat                                                                                                                                                                                                          & \checkmark              & \checkmark                                                   \\ 
\hline
\cite{9350853}                                & Multi-attribute decision-making and information entropy based algorithm      & Adapt to dynamic changes                                                                                                                                                                                                          & \checkmark              &                                                     \\ 
\hline
\cite{INC6G}                                & Multi-objective evolutionary algorithm based on decomposition                & \begin{tabular}[c]{@{}l@{}}Minimize the overall:~\\\begin{tabular}{@{\labelitemi\hspace{\dimexpr\labelsep+0.5\tabcolsep}}l@{}}Idle rate of resources\\Energy consumption\\Data transmission overhead\end{tabular}\end{tabular} & \checkmark              &                                                     \\
\hline
\end{tabular}
} 
 \end{adjustbox}
\end{table*}
%% end table

\section{Implementation Approaches}\label{Implementation}
\noindent SDN has brought to reality a novel paradigm in networking: programmability. Traditionally, the actions of programmable switches are driven by matching patterns in the tables, which can range from using only MAC addresses to more complex OpenFlow rules. 
\begin{figure}[!h]
\centering
\includegraphics[scale=0.63]{Content/Figures/SDNenabler.drawio.pdf}
\caption{Software defined networking based in-network computing model}
\label{fig:SDNenabler}
\end{figure}
However, this standard match-action paradigm does not apply to the INC context as emerging applications with diverse requirements demand customized matching criteria with corresponding device behavior. 
%whereas SDN (i.e., OpenFlow) programmability is limited to the remote control plane or the decision element only. 
With the emergence of P4 \cite{bosshart2014p4}, the programmability of the network data plane is further extended to allow the network manager to express a novel matching pattern with a corresponding device behavior. As shown in Figure~\ref{fig:SDNenabler}, the incoming network traffic goes through a pipeline process that consists of Reconfigurable Match-Action Tables (RMTs)~\cite{bosshart2013forwarding}. RMTs map keys (e.g., packet headers) to actions (e.g., set egress port), while the control plane designs the optimal forwarding strategies (i.e., tables entries). Moreover, based on the abstract view of the topology, the control plane is capable of designing optimal \textit{In-Switch $X$} policies. Hence, the data plane first utilizes the caching and computing pipeline to look for pre-computed results in content storage that can significantly accelerate the service process and save the communication and computation costs~\cite{li2021advancing}. Hence, the maturity of SDN and P4 makes the vision of INC more practical~\cite{bosshart2014p4}.

\subsection{Named Function Networking (NFN)} \label{NFN}
\noindent NFN is an extension of Information-Centric Networking (ICN) by adding the means to support function definition and application to data while applying the same resolution-by-name process~\cite{sifalakis2014information}. It builds on $\lambda$-functions to execute services anywhere in a network device. However, as NFN is restricted to the basic $\lambda$-functions included in the Interest name, the number of supported services turns out to be limited. Expressing advanced processing or custom code in a network device is challenging when using only $\lambda$-functions. 
 %Following this paradigm to execute named function within the network, will be answering challenges faced by edge, IoT and cloud computing like reverse data flow generated by IoT device and the emergence of new delay sensitive application. 
 
The most important work that brings the benefits of NFN to INC while avoiding the above shortcomings is the Named Function as a Service (\textit{NFaaS}), a framework proposed in \cite{10.1145/3125719.3125727}. NFaaS extends the NDN architecture using explicitly named unikernels to support in-network function execution. 
\textit{NFaaS} includes a Kernel Store component, which is responsible not only for storing functions but also for making decisions on which functions to run locally. In addition, it includes a routing protocol and several forwarding strategies to deploy and dynamically migrate functions within the network. 
The unikernel (i.e., function code) executes the requested action and wraps the response in a data packet that follows the transmission path to the requesting node. This assumes that the network node executing the request has enough computation capacity and storage capacity to cache the unikernel in its own storage. Otherwise, the interest request is simply forwarded to the next node. Figure~\ref{fig:NFNarchitecture} presents an overview of the system. An interest to execute the function \textit{/foo/bar} issued by node A is forwarded to node C, where the corresponding kernel is instantiated. The resulting data is sent back following the same path.
 
\begin{figure}
\centering
\includegraphics[scale=0.75]{Content/Figures/NFNbasic.drawio.pdf}
\caption{NFN architecture}
\label{fig:NFNarchitecture}
\end{figure}
%Data streaming application has INC as a solid basis. This type of application plays an essential role in IoT-based networks providing data for users to analyse and aggregate results. It would be beneficial to do so on the data source, yet its constraints won't allow it. Thereof, it is best to execute near data source as possible which is in the network itself. 

The authors of \cite{8264868} take advantage of NFN to stream and process data in the network leading to what they define as a  \textit{stream aware network}. The network is responsible for deciding when and where to process the stream, and the results are stored in the network nodes. Data streams are processed on the direct path between the source and the requester by optimising location based on mobile function codes (i.e., the functions are mobile and are executed in the best-located processing unit and not at the requester device). The proposed solution (i.e., the mobility of the function code) promises to reduce load compared to the traditional approach, as it distributes mainly mobile cloud codes, so the calculation can be distributed over network nodes.
Another work based on NFN was introduced in  \cite{scherb2017execution} to ensure a mechanism that will enable steering long computations running in the network over NFN. The proposed solution answers the limitation of NFN regarding the incapability of accessing or interacting with the computation once it is launched over the network, which does not stop until it delivers the results. The authors have addressed this problem by proposing \textit{Request 2 Compute} messages as an extension to NFN. Those messages enable users to access 
``process control blocks" and are used to request intermediate results to obtain the current state of a long-running application such as simulation. Table~\ref{INCdeploy} summarizes these studies  highlighting their contributions and design goals.


\subsection{In-Network Programmability}\label{Prog} 

\noindent In the SDN-based network computing model, the application logic fragment can be implemented through P4 with the aim to be deployed on a switch and its flow table. Here, the SDN controller plays a crucial role in deciding the optimal placement of processing and task scheduling \cite{tokusashi2019case}. Several research works propose optimal processing placement strategies by leveraging centralized control plane capabilities. For example, the In-Network Computing Architecture (\textit{INCA}) instantiates the requested function at appropriate places to meet the QoE constraint is described in \cite{10.11453359993.3366649}. In \textit{INCA}, the controller keeps track of the processing workload of the networking devices as well as the optimal routes in between them. Another work \cite{ruth2018towards} utilizes SDN and P4 in industrial control systems to offload delay-sensitive control tasks from the cloud to local network elements which can perform processing with bounded delay.  

Likewise, data aggregation and in-network caching are the most prominent efforts in the literature, leveraging SDN to implement INC services. The controller is primarily responsible for defining the aggregation tree and sending the corresponding processing rules to the switch in the form of table entries~\cite{10.1145/3152434.3152461}. For the incoming data packets, the switch parses the message and then aggregates the data according to the flow rules defined by the controller. Similarly, another framework is proposed in \cite{yang2019switchagg} to integrate the packet load analysis module and data aggregation processing engine into the switch, thus, improving the network throughput with reduced latency. Besides data aggregation, the controller controls the state consistency by maintaining an updated registration table on the switches in the data center network~\cite{liu2017incbricks}. As the popularity of each cached key-value pair changes over time, the central control is used to keep the current hot cached items on the switches. \cite{jin2017netcache}. In practice, network operators use programmable devices to modify the functionality of the data plane as needed while maintaining the ability to interpret, and process data flows at link speed. 
%The idea of employing the data plane for routing and processing flows is an old one, which goes back to the concept of Active Networks~\cite{tennenhouse2002towards}. However, until more recently, the data processing capacity in the data plane was below the needs of the applications, which comes a bottleneck in the network. 
%
% With advances occurring in the data plane to make INC possible, releasing new generations of programmable network hardware and accelerators to the market (SmartNICs~\cite{sanvito2018can}, Graphics Processing Units (GPUs)~\cite{sun2019optimizing} and FPGAs~\cite{woodruff2019p4dns}), computing functions within the network are becoming feasible and flow processing are enabled with high performance and low latency. 
Use cases like data caching~\cite{jin2017netcache}, load balancing~\cite{grigoryan2019iload}, Domain Name System  server~\cite{woodruff2019p4dns}, and data aggregation~\cite{sapio2019scaling, yang2019switchagg} can be implemented within a network device, reducing the traffic in the network and consequently improving the overall performance.


Match-Action Tables allow network operators to use a set of pipeline stages containing a match table of width and depth arbitrary to match a particular field in an incoming packet on the pipeline. Programmable Data Planes (PDPs) enable network devices to perform complex operations on packets allowing the modification of how data is processed at the hardware level. Besides, Domain-Specific Languages created for PDP, such as Protocol-Oblivious Forwarding ~\cite{song2013protocol} and P4~\cite{bosshart2014p4}, allow programming the data plane behavior in a higher-level abstraction. Domain-specific language provides novel approaches in PDP \cite{da2017data}, helping to overcome even more  "ossification" of computer networking and allowing the development of a new generation of network services. Since P4 is the most prevalent enabling technology that is used to define data plane algorithms, the real power of P4 lies in a programmable match-action pipeline, describing tables, lookups, and actions in an abstract, straightforward manner with the freedom of defining any kind of protocol headers.

On the other hand, P4 is a domain-specific language that does not support complex operations such as checksum or hash computation units, random number generators, packet and byte counters, meters, registers, and many others. To make such extern functionalities usable, P4 introduces the so-called externs. These externs are  the external objects describing the interfaces that such objects expose to the data plane. On a separate note, the implementation of complex operations has been investigated as extensions to P4. In~\cite{da2018extern}, a case study is performed on integrating the Robust Header Compression (ROHC) scheme, which leads to an implementation of a new native primitive. Another work, \cite{scholz2019cryptographic}, proposes the extension of P4 by cryptographic hash functions that are required to build secure applications and protocols. The authors of \cite{scholz2019cryptographic} have proposed an extension of P4 Portable Switch Architecture and implemented a PoC for three platforms: CPU, Network Processing Unit (NPU), and FPGA-based P4 targets. In addition, the asynchronous execution of externs has been investigated in \cite{laki2020price, horpacsi2019asynchronous}, showing that packets may be processed by the pipeline during the execution of externs.
%==============================

\begin{figure}[h]
%\centering
\includegraphics[scale=0.7]{Content/Figures/NFNaction.pdf}
\caption{NFN and SDN in action}
\label{fig:NFN}
\end{figure}
\subsection{Summary}\label{summaryEnablers}
\noindent To illustrate the different actions of the INC enablers, we draw a flow chart in Figure~\ref{fig:NFN} summarizing the interest request processing steps from its generation by a consumer to the reception of the final result. 
Suppose that an interest request composed of content $C$ and function $F$ arrives at the first node (node $N$), then the node checks if the results are stored in its content store CS from previously executed computations. If the requested result exists, it is sent back to the consumer; otherwise, the node looks for function $F$ in its function store FS and content $C$ in CS. 
Failure to match function $F$ implies retrieving it from the function store FS. Note that FS is hosted in a remote server known by the controller, as described in Subsection~\ref{INC_Enablers}. 
Likewise, the fail-matching of content $C$ follows the classical NDN procedure for finding content in the network and moving it to node $N$ where the execution of the interest request (content $C$, function $F$) occurs. 
If both $F$ and $C$ exist at node $N$, then the execution can proceed if and only if there is enough available CPU and caching capacity. A request message is sent to the controller to obtain INC resource status from the execution information base (EIB). If there is capacity, the execution proceeds; otherwise, the controller scans the content information base (CIB) to verify if the required result exists in other nodes. Otherwise, the controller will launch the action of finding an executor node through the resource allocation strategy module (see Figure~\ref{fig:SDNenabler}).
%============================
\begin{table*}
\centering
\caption{INC deployment frameworks\label{INCdeploy}}
\begin{tabular}{ |c |c| c| c|}
\hline
\textbf{Papers} & \textbf{Architecture} & \textbf{Design goals} & \textbf{Contribution} \\
\hline
\cite{10.1145/3125719.3125727} & NFN & In-Network Function execution& \begin{tabular}[c]{@{}l@{}}In-Network Function:  :~\\\begin{tabular}{@{\labelitemi\hspace{\dimexpr\labelsep+0.5\tabcolsep}}l@{}}Storing\\Dynamic deploying\\Migration\end{tabular}\end{tabular}\\
\hline
\cite{da2020pipelining} & NFN & In-Network Stream processing (Stream aware network) &  \begin{tabular}[c]{@{}l@{}}In-Network Stream:  :~\\\begin{tabular}{@{\labelitemi\hspace{\dimexpr\labelsep+0.5\tabcolsep}}l@{}}Storing\\Processing\\\end{tabular}\end{tabular} \\
\hline
\cite{scherb2017execution} &NFN& Steering long computation over the network& \makecell{Extension of NFN by adding\\ \textit{Request2Compute} messages}  \\
\hline
\cite{da2018extern} &P4& Supporting header compression and decompression& \makecell{Add new external primitives to P4\\ to support header compression and decompression (ROHC)} \\
\hline
\cite{kim2020unleashing} &P4& \makecell{ Exploiting INC \\ to accelerate Scientific application workload }& NSinC (Network-accelerated ScIeNtific Computing) \\
\hline
\cite{scholz2019cryptographic}&P4& \makecell{ Enabling authentication and resiliency\\ in P4 targets} & Extending P4 with external cryptographic hash function \\
\hline
\end{tabular}
\end{table*}

%new adding
%As previously mentioned, P4 extern instances are used for complex packet processing operations. According to \cite{da2018extern}, theses externs are more suitable for target-specific switching applications where vendors can specify their own specific operations through a standard library without changes in the semantics of p4. Besides, the developers of p4 language want to keep it as simple as possible by implementing only basic operations, hence the use of externs for advanced and complex operations. Extern objects have been shown to be more efficient than native primitives for adding p4 extensions. When using native primitives, it is mandatory to recompile the whole set of primitives whenever the new primitive is modified and they introduce compatibility and modularity problem. In contrast, extern objects are more modular and scalable.
%\cite{da2018extern} propose a case study of header compressing scheme in a p4 switch using extern objects. The packet processing (compression and decompression schemes) is implemented in a library. This library is compiled and instantiated in the behavior switch model. The three main externs are : ROHC compression, ROHC decompression and modify and resubmit a packet. After the header is parsed (using a parser state machine in P4), its identification is stored in the packet metadata used by the P4 extern and then compressed. Identically, the decompression is done after parsing the header and performing identification. \\
%In the same perspective, \cite{ENDN} represents an interesting use case by using P4 externs to enhance NDN architecture. The extern functions allow network services to access or modify the string-based NDN packet content name. The enhanced NDN data plane has two main components : EProcessing module and Forwarding logic module. The EProcessing modules contains a parser and two forwarding tables which are based on the NDN forwarding pipeline. Furthermore, the forwarding logic module contains two components : functions and deparser. The deparser is responsible of updating the packet header field (content name) based on values that the P4 function provide. The function component is a set of independent P4 functions. In the latter component, there are P4 function target where each target contains a P4 program that optionally call one or more extern objects (e.g. meters, registers and counters) and extern functions (e.g. hash functions or encryption). Match-action pipelines can also call extern functions and access or modify extern objects. In addition, many extern functions are added to P4 function target in order to allow manipulation of data from the header field (content name) of packets. For example, a function that takes as input a name component position and returns as output the hash of the component, functions that control the deparser by allowing insertion and deletion of a name component at a specific position, and a function that allow creation of a new data packet with a specific name and content. \\
%It is worth mentioning that programmable data plane can have an impact on network performance. According to \cite{performancePDP}, CPU-based P4 target platforms offer limited performance but are easily to extend comparing with ASIC P4 targets that have dedicated P4 processing pipelines with limited programmability but offer high performance. The work in \cite{performancePDP} analyzes the latency, jitter, throughput and resource consumption of hardware and software based P4 targets. The key performance indicators of networking hardware are : P4 target selection properties and runtime properties. P4 target selection properties are resource consumption, functionality and setup time. Some devices may be limited by resource consumption depending on the complexity of the program they will run. However, A device should be able to define custom functionality in case the required externs are not available (this is what called functionality). Setup time, in turns, is the time needed to deploy and start up a P4 program. Secondly, runtime properties are throughput, latency and jitter. The available throughput should be high in contrast to latency and jitter that should be as low as possible. The investigation shows that ASIC (e.g. Barefoot Tofino) has high throughput with low delay and jitter but memory resources are limited. In contrary, CPU (e.g. t4p4s DPDK) provides high resources but low throughput and high latency and jitter. However, NPU (e.g. NFP-4000 SmartNIC) and FPGA (e.g. NetFPGA SUME) represents a tradeoff between resources, throughput, latency and jitter.
%end new adding


