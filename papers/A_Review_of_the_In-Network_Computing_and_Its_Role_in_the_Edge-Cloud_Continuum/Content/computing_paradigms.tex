\section{A New Era of Massively Scalable Computing:  Cloud-Edge-Fog-Dew-Mist Continuum}\label{ComputingParadigm}

The following presents a short overview of the different computing paradigms present in today's networks and discusses how INC can act as a facilitator of these paradigms.\\
The widespread use of cloud computing has reduced the need for local storage for edge devices and has enabled edge devices to delegate computationally intensive tasks. On the other hand, computing tasks should be transferred to devices near edge devices to reduce latency and improve resistance in the event of heavy traffic in the network backhaul. Thus, in recent years, the interest in edge computing has increased \cite{FIROUZI2021101840}.  One of the main advantages of 5G over earlier mobile network generations is the ability to process data generated by devices on or near mobile network edges via Multi-access Edge Computing (MEC). MEC servers are represented by a set of computing nodes purposefully installed at the network edge by a mobile network operator.  Hence, 5G can contribute greatly to latency assurance and load reduction on the network backhaul. MEC's support for caching, processing, and distributed computing can meet the needs of end-user devices from high mobility vehicles to low latency manufacturing floor machines \cite{7931566}. However, the initial installation of MEC servers may not be appropriate for all applications and may not be flexible. \textit{Fog Computing} (FC), therefore addresses this inadequacy by generally including consumer devices that provide faster installation of available resources \cite{7543455}. \textit{Dew Computing} (DC), closely related to cloud computing, is another paradigm associated with data storage security and reliability \cite{Wang2016DefinitionAC}. The aim of DC is to extend the functionality of the cloud to edge devices, where services can run both independently and in collaboration with the cloud.  
%In the so-called mobile ad hoc networks, the infrastructure is built up of individual edge devices. They operate independent of a central orchestration platform while working on achieving a common goal. 
By forming adhoc clusters, groups of edge devices may mimic a larger processing platform such as a MEC server or an FC node, and process tasks offloaded to them by other edge devices \cite{894385}. While edge devices come together to process a computationally intensive task, resource-rich extreme edge devices can decide to individually provide their processing power to adjacent edge computing nodes \cite{8486685}. Such cases, where data processing is carried out by stationary extreme edge devices, are also referred to as \textit{Mist computing} \cite{lea2020iot}. Mist computing (MC) devices may exist alongside the FC architecture. MC devices are represented by a thin layer of resource-constrained computing devices, wedged between the sensing devices and the first layer of the fog architecture. 
Note that Cloud, FC, DC and MC have different functionalities that help applications achieve stringent service requirements that cannot be met unless they work together. \textit{In-network computing} (INC) has the potential to strengthen the above-mentioned computational paradigms as depicted in Figure \ref{fig:INC}. 

Distributed computing services explained above may employ serverless computing  to easily harness the power of the cloud. With serverless computing, developers simply provide an event-driven function to cloud providers, and the provider seamlessly scales function invocations to meet demands as event triggers occur. Today, successful serverless products fall into the application-specific category and are narrowly targeted, whereas general-purpose serverless abstractions have a better chance of displacing serverful computing. %Application-specific abstractions solve a particular use case, and several of them exist in products today. General-purpose abstractions must work well in a broad variety of uses and remain a research challenge. 
Automatic optimization with machine learning will play an important role in all service models discussed above. It may help decide where to run the code, where to keep the state, when to start up a new execution environment, and how to keep utilization high and costs low while meeting performance objectives \cite{10.1145/3406011}. 


%\subsection{Serverless Computing}
\begin{figure}
    \centering
    \includegraphics[width=0.48\textwidth]{INC.png}
    \caption{In-network computing supported ecosystem of networked devices}
    \label{fig:INC}
\end{figure}

%\subsection{Distributed computing trends in Mist-Edge-Cloud Continuum}

Managing the massive amount of data at the edge requires a distributed and collaborative computing hierarchy. Distributed learning (DL) and collaborative learning (CL) are two learning methods used in the cloud-fog-mist continuum.  DL and CL have been traditionally used for managing and processing a large amount of data and heavily loaded resources. DL focuses on learning mechanisms on different clients through a network system, and CL focuses on the integration of distributed learning on different clients. In particular, the emergence of edge intelligence provides DL and CL with the necessary computational power from the heterogeneous devices located at the network edge.  This optimizes the reliability of the network topology and thus achieves greater efficiency and better performance.

Meanwhile, the training of machine learning (ML) models on edge devices is difficult, especially due to their computational and memory limitations. In general, offloading tasks from a central cloud server to edge nodes results in a substantial reduction of transmitted data; however, ML models need massive computational resources, including energy and memory. 
%For example, Conventional Neural Networks (CNNs) used in computer vision tasks need substantial memory to store intermediate outputs, big datasets, and weight parameters. 
This requirement can create bottleneck issues when ML is used with edge/fog computing. 
When resources are limited, the successful use of ML requires  an edge server to have many different implementations. Each distinct implementation of the network generates the same classification or prediction with varying accuracy versus resource in a way that a higher service level will produce greater accuracy. 
%The idea of “service level” and task offloading should be combined to distribute the intelligence across the edge–fog–cloud to address the power/energy consumption, latency, and performance constraints of the target IoT application. 

%\subsection{Energy/power-neutral computing}
Finally, \textit{energy-driven computing} is a paradigm that empower edge devices with autonomous and low-energy operation \cite{sliper2020energy}.  Note that, the tens of billions of connected devices envisioned in the next generation of networks will require novel solutions to avoid the cost and maintenance requirements imposed by battery-powered operation. Energy harvesting is a promising technology to this end, but harvested energy typically fluctuates, often unpredictably, and with large temporal and spatial variability. Energy-driven computing allows devices to sleep through periods of no energy, endure periods of scarce energy, and capitalize on periods of ample energy. Intermittent operation should be endured since power failures can be frequent and unpredictable. However, by retaining computational progress through power cycles, intermittent computing systems allow long-running applications to progress incrementally whenever energy is available. A key consequence is QoS must often be sacrificed because the devices are not continuously operational \cite{9403911}. Hence, intermittent operation is only applicable to certain applications, e.g., for example, an autonomous solar-powered quadcopter whose mission is to collect environmental sensor data.  Although INC may not be limited by energy, it may also benefit from intermittent computing. For example, a smart NIC may preemptively interrupt an ongoing computation for a flow to forward another incoming flow of packets to satisfy QoS requirements.



%  \subsubsection{Emerging Applications Challenges}

%  \subsubsection{5/6G Context}
 
%  \subsubsection{ML in Distributed Environment}
