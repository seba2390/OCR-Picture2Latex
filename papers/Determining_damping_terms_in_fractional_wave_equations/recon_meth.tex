\section{Reconstruction approaches}\label{sec:recon_meth}
Throughout this section we will a single mode excitation and therefore skip the subscript $n_i$ in $\varphi_{n_i}$, $\lambda_{n_i}$ and $i$ in $B_i$ to simplify notation). Moreover, we focus on the second order model \eqref{eqn:general_2ndorder}.

\subsection{Full time observations or analyticity: Laplace domain reconstruction}

Starting from the equation 
\[
\hat{h}(s)=\frac{\langle f,\varphi\rangle B\varphi}{
s^2+c^2\lambda+\sum_{k=1}^N s^{\alpha_k} b_{k}\lambda^{\beta_k}}
\begin{cases}
1 
&\mbox{ in case $\;u_0=0$, $\;u_1=\varphi$, $\;f=0$}\\
\hat{\sigma}(s)
&\mbox{ in case $\;u_0=0$, $\;u_1=0$, $\;f=\varphi$}\\
s+\sum_{k=1}^N s^{\alpha_k-1} b_{k}\lambda^{\beta_k}
&\mbox{ in case $\;u_0=\varphi$, $\;u_1=0$, $\;f=0$}
\end{cases}
\]
(cf. \eqref{eqn:BaBb_singlemodes} for the slightly simpler setting of \eqref{eqn:general}) and noting that taking the reciprocal and rearranging makes the problem somewhat less nonlinear and the derivative easier to compute, we consider 
\[
F(b_1,\ldots,b_N,\beta_1,\ldots,\beta_N,\alpha_1,\ldots,\alpha_N;s)
= G(s)\,, \quad s\in\{s_1,\ldots,s_M\}
\]
with 
\[
F(b_1,\ldots,b_N,\beta_1,\ldots,\beta_N,\alpha_1,\ldots,\alpha_N;s)
=\sum_{k=1}^N b_k \lambda^{\beta_k} s^{\alpha_k}
\]
and 
\[
G(s)=\begin{cases}
\frac{B\varphi}{\hat{h}(s)}-s^2-c^2\lambda
&\mbox{ in case $\;u_0=0$, $\;u_1=\varphi$, $\;f=0$}\\
\frac{B\varphi\, \hat{\sigma}(s)}{\hat{h}(s)}-s^2-c^2\lambda
&\mbox{ in case $\;u_0=0$, $\;u_1=0$, $\;f=\varphi$}\\
\frac{c^2\lambda}{1-\frac{B\varphi}{s\hat{h}(s)}}-s^2-c^2\lambda
&\mbox{ in case $\;u_0=\varphi$, $\;u_1=0$, $\;f=0$}
\end{cases}
\]
For applying Newton's method, the Jacobian is given by 
\[
\begin{aligned}
&\frac{\partial F}{\partial b_i}(b_1,\ldots,b_N,\beta_1,\ldots,\beta_N,\alpha_1,\ldots,\alpha_N;s):=
\lambda^{\beta_i} s^{\alpha_i}\\
&\frac{\partial F}{\partial \beta_i}(b_1,\ldots,b_N,\beta_1,\ldots,\beta_N,\alpha_1,\ldots,\alpha_N;s):=
b_i \ln \lambda\, \lambda^{\beta_i} s^{\alpha_i}\\
&\frac{\partial F}{\partial \alpha_i}(b_1,\ldots,b_N,\beta_1,\ldots,\beta_N,\alpha_1,\ldots,\alpha_N;s):=
b_i \lambda^{\beta_i} \ln s \, s^{\alpha_i}
\end{aligned}
\] 

Taking the logarithm makes the equation less nonlinear with respect to $\alpha$ and $\beta$ by considering (in case $N=1$)
\[
\begin{aligned}
F_1(b,\beta,\alpha)&:=
b \lambda^\beta s_1^\alpha
=\frac{B\varphi\, \hat{\sigma}(s_1)}{\hat{h}(s_1)}-
s_1^2-c^2\lambda\\
F_m(b,\beta,\alpha)&:=
\log b + \beta\log\lambda +\alpha \log s_m
=\log\left(\frac{B\varphi\, \hat{\sigma}(s_m)}{\hat{h}(s_m)}-
s_m^2-c^2\lambda\right)\,,\quad m=2,\ldots M.
\end{aligned}
\]
Then $F_1$ is linear with respect to $b$ and the $F_m$ are linear with respect to $\alpha$ and $\beta$. This helps to enlarge the convergence radius of Newton's method, as we experienced in our numerical tests.




\subsection{Large time asymptotics}

In case $u_0=\varphi$, $u_1=0$, $f=0$ we have 
\begin{equation}\label{eqn:hath}
\hat{h}(s) = \frac{s + \sum_{k=1}^N b_k\lambda^{\beta_k} s^{\alpha_k-1}}{s^2 + \sum_{k=1}^N b_k\lambda^{\beta_k} s^{\alpha_k} + c^2\lambda} B\varphi
\sim \frac{B\varphi}{c^2} \sum_{k=1}^N b_k\lambda^{\beta_k-1} s^{\alpha_k-1} \quad \mbox{ as }s\to0.
\end{equation}

\revision{
We will heavily make use of a Tauberian Theorem (for $s\to0$, $t\to\infty$, see \cite[Theorem 2 in Chapter XIII.5]{Feller:1971}) which we here quote for the convenience of the reader.
\begin{theorem}\label{thm:Feller_Tauberian}
Let $w:[0,\infty)\to \mathbb{R}$ be a monotone function and assume
its Laplace-Stieltjes transform $\hat w(s) = \int_0^\infty e^{-st} dw(t)$
exists for all $s$ in the right hand complex plane.
% \mathbb{C}^+ := \{z\in \mathbb{C},\ \Re(z) >0\}.
Then for $\rho\geq0$
$$
\hat w(s) \sim \frac{C}{s^{\rho}} \quad\mbox{as\ }\ s\to 0
$$
if and only if
$$
w(t) \sim \frac{C}{\Gamma(\rho+1)} t^\rho \quad\mbox{as\ }\ t \to \infty.
$$
\end{theorem}
Note that here $dw$ is actually a measure, which in case of an absolutely
continuous function $w$ can be written as $dw(t)=w'(t)dt$.
\\
Applying Theorem~\ref{thm:Feller_Tauberian} to \eqref{eqn:hath}} 
we get 
\begin{equation}\label{eqn:S_largetime}
h(t)= \frac{B\varphi}{c^2} \sum_{k=1}^N \frac{b_k\lambda^{\beta_k-1}}{\Gamma(1-\alpha_k)} t^{-\alpha_k}  + O(t^{-2\alpha_1}) \mbox{ as }t\to\infty\,.
\end{equation}
Therefore we get an asymptotic formula (like the one in \cite{HatanoNakagawaWangYamamoto:2013} for the subdiffusion equation) for the smallest order 
\begin{equation}\label{eqn:alpha-larget}
\alpha_1 = -\lim_{t\to\infty}\frac{\log h(t)}{\log t}.
\end{equation}
By l'Hospital's rule and actually thereby removing the constant we can instead also use 
\[
\alpha_1 = -\lim_{t\to\infty}\frac{\tfrac{d}{dt}(\log h(t))}{\tfrac{d}{dt}(\log t)} = -\lim_{t\to\infty}\frac{t h'(t)}{h(t)}.
\]
After having determined $\alpha_1$ this way, we can also compute $b_1$ as a limit
\begin{equation}\label{eqn:b1-larget}
b_1 = \lim_{t\to\infty}(h(t) t^{\alpha_1})\, c^2\lambda^{1-\beta_1}\Gamma(1-\alpha_1).
\end{equation}
So we can successively (by the above procedure and subtracting the recovered terms one after another) reconstruct those terms $k$ for which $\alpha_k < 2\alpha_1$, see the remainder term in \eqref{eqn:S_largetime}. However, if there are terms with  $\alpha_k \geq 2\alpha_1$, they seem to get masked by the $O(t^{-2\alpha_1})$ remainder.

The same holds true if we do the excitation by $u_0=0$, $u_1=\varphi$, $f=0$, or $u_0=0$, $u_1=0$, $f=\varphi$, where \eqref{eqn:hath} changes to 
$\hat{h}(s) = \frac{1}{s^2 + \sum_{k=1}^N b_k\lambda^{\beta_k} s^{\alpha_k} + c^2\lambda} B\varphi$.

In order to avoid the restriction $\alpha_k < 2\alpha_1$, we thus refine the expansion of $\hat{h}(s)$ in terms of powers of $s$ and retain the singular ones, that is, those with negative powers, since we are looking at the limiting case $s\to0$.
Using the geometric series formula and the multinomial theorem, with the abbreviations $\Sigma=\sum_{k=1}^N b_k\lambda^{\beta_k} s^{\alpha_k}$, $q=\frac{s^2+\Sigma}{c^2\lambda}$, yields
\[
\begin{aligned}
\hat{h}(s)&=\frac{s+\Sigma/s}{s^2+\Sigma +c^2\lambda} 
= \frac{1}{s}\, \frac{q}{q+1} 
= -\frac{1}{s}\,\sum_{m=1}^\infty (-q)^m
= -\frac{1}{s}\, \sum_{m=1}^\infty (-\tfrac{1}{c^2\lambda})^m\, 
\sum_{\ell=0}^m s^{2(m-\ell)}\, \Sigma^\ell\\
&= -\sum_{m=1}^\infty (-\tfrac{1}{c^2\lambda})^m\, 
\sum_{\ell=0}^m s^{2(m-\ell)-1}\,
\sum_{i_1+\cdots+i_N=\ell}\left({\ell\atop i_1,\cdots,i_N}\right)
\prod_{j=1}^N b_j^{i_j}\, \lambda^{\sum_{j=1}^N\beta_j i_j}
\, s^{\sum_{j=1}^N\alpha_j i_j}.
\end{aligned}
\]
The terms corresponding to $\ell<m$ are obviously $O(s)$, so to extract the singularities it suffices to consider $\ell=m$. The set of indices leading to singular terms is then
\[
I_m=I_m(\alpha_1,\cdots,\alpha_N)=\{\vec{i}=(i_1,\cdots,i_N)\, : \, 
i_1+\cdots+i_N=m, \, \sum_{j=1}^N\alpha_j i_j <1\}\,, \quad
m\leq m_{\max}=[\tfrac{1}{\alpha_1}]
\]
and the singular part of $\hat{h}$ can be written as
\begin{equation}\label{eqn:hsing}
\hat{h}_{\textup{sing}}(s)
= -\sum_{m=1}^{m_{\max}} (-\tfrac{1}{c^2\lambda})^m\, 
\sum_{\vec{i}\in I} \tilde{b}_{m,\vec{i}}
\,\, s^{\sum_{j=1}^N\alpha_j i_j-1}
\mbox{ with } 
\tilde{b}_{\vec{i}} = \left({m\atop i_1,\cdots,i_N}\right)
\prod_{j=1}^N b_j^{i_j}\, \lambda^{\sum_{j=1}^N\beta_j i_j}.
\end{equation}
In case $N=2$ with $i=i_1$, $i_2=m-i$, this reads as
\[
\hat{h}_{\textup{sing}}(s)
= -\sum_{m=1}^{m_{\max}} (-\tfrac{1}{c^2\lambda})^m\, 
\sum_{i=i_{m,\min}}^m \tilde{b}_{m,i}
\,\, s^{\alpha_1 i + \alpha_2(m-i)-1}
\mbox{ with }
\tilde{b}_{m,i} = \left({m\atop i}\right)
b_1^i\, b_2^{m-i}\, \lambda^{\beta_1 i +\beta_2(m-i)}.
\]
since
\[
I_m=\{i\in\{0,\cdots,m\}\, : \, \alpha_1 i +\alpha_2(m-i) <1\}
=\{i_{m,\min},\cdots,m\} \mbox{ with } i_{m,\min}=\left[\frac{m\alpha_2 -1}{\alpha_2-\alpha_1}\right].
%, \quad m\leq m_{\max}
\] 
For \eqref{eqn:hsing}, by 
\revision{Theorem~\ref{thm:Feller_Tauberian}} 
\[
\hat{h}_{\textup{sing}}(t)
= -\sum_{m=1}^{m_{\max}} (-\tfrac{1}{c^2\lambda})^m\, 
\sum_{\vec{i}\in I} \frac{\tilde{b}_{m,\vec{i}}}{\Gamma(1-\sum_{j=1}^N\alpha_j i_j)}
\,\, t^{-\sum_{j=1}^N\alpha_j i_j}\,.
\]
which confirms \eqref{eqn:alpha-larget}, \eqref{eqn:b1-larget}.

For the first damping term, we also get an asymptotic formula for the smallest order in Laplace domain -- the small $s$ counterpart to \eqref{eqn:alpha-larget}: Due to
\begin{equation}\label{eqn:asymp-smalls}
\log \hat{h}(s) \approx  \log b_1 +(\alpha_1-1)\log s -  \log(c^2\lambda^{1-\beta_1})\mbox{ as } s\to0
\end{equation}
we have 
\begin{equation}\label{eqn:alpha-smalls}
1-\alpha_1 = -\lim_{s\to0}\frac{\log \hat{h}(s)}{\log s}.
\end{equation}
One might realise this limit by extrapolation: Fitting the values $\frac{\log \hat{h}(s_m)}{\log s_m}$ at $M$ sample points $s_1,\ldots,s_M$ to a regression line or low order polynomial $r(s)$ and setting  $\alpha_1=1+r(0)$.
Next, recover $b_1$ from 
\[
\log b_1 \approx \log \hat{h}(s) + \log(c^2\lambda^{1-\beta_1})-(\alpha_1-1)\log s.
\]
The formula \eqref{eqn:asymp-smalls} also shows why recovering $b_1$ and $\alpha_1$ {\it simultaneously} appears to be so hard: The factor multiplied with $\log b_1$ is unity while the factor multiplied with $1-\alpha_1$ tends to infinity. 

\subsection{Small time asymptotics}
In case $u_0=\varphi$, $u_1=0$, $f=0$ where the Laplace transformed observations are given by 
\begin{equation}\label{eqn:hath_u0}
\hat{h}(s) := \frac{s + \sum_{k=1}^N b_k\lambda^{\beta_k} s^{\alpha_k-1}}{s^2 + \sum_{k=1}^N b_k\lambda^{\beta_k} s^{\alpha_k} + c^2\lambda}
\end{equation}
large $s$ / small $t$ asymptotics cannot be exploited for the following reason.
As $s\to \infty$ then $\hat{h}(s) \to 0$ and in fact $s\hat{h}(s) \to 1$.
More precisely, for
$$d(s) := s\hat{h}(s)-1= \frac{s^2 + \sum_{k=1}^N b_k \lambda^{\beta_k}s^{\alpha_k}}{s^2 + \sum_{k=1}^N b_k \lambda^{\beta_k} s^{\alpha_k} + c^2\lambda} - 1 
= -\frac{c^2\lambda}{s^2 + \sum_{k=1}^N b_k \lambda^{\beta_k} s^{\alpha_k} + c^2\lambda}.
$$
Thus clearly $s^{\gamma} [s\hat{h}(s)-1] \to 0$ for any $\gamma\in[0,2)$.

Not even large $\lambda$ values (or some combined asymptotics as $s\to0$, $\lambda_j\to\infty$) seems to work because then the $c^2\lambda$ term takes over and again masks the terms containing $\alpha_k$.

\medskip
However, in case $u_0=0$, $u_1=\varphi$, $f=0$ the situation is different since then 
\begin{equation}\label{eqn:hath_u1}
\hat{h}(s) := \frac{1}{s^2 + \sum_{k=1}^N b_k\lambda^{\beta_k} s^{\alpha_k} + c^2\lambda}
\end{equation}
and therefore 
\begin{equation}\label{eqn:hhat_large_s}
1-(s^2+c^2\lambda)\hat{h}(s) := 
\underbrace{\frac{s^2}{s^2 + \sum_{k=1}^N b_k\lambda^{\beta_k} s^{\alpha_k} + c^2\lambda}}_{\to 1} 
\Bigl(\sum_{k=1}^N b_k\lambda^{\beta_k} s^{\alpha_k-2}\Bigr)
\end{equation}
which via 
\revision{Theorem~\ref{thm:Feller_Tauberian} (where due to \cite[Theorem 3 in Chapter XIII.5]{Feller:1971} we can replace $s\to0$, $t\to\infty$ by $s\to\infty$, $t\to0$) yields 
}
\[
-c^2\lambda h(t)-h''(t) \sim 
\sum_{k=1}^N \frac{b_k\lambda^{\beta_k}}{\Gamma(2-\alpha_k)} t^{1-\alpha_k}
\mbox{ as }t\to0,
\]
where we have used the fact that $h(0)=0$, $h'(0)=B\varphi$ because of $u_0=0$, $u_1=\varphi$.
\\
This yields the asymptotic formulas 
\[
1-\alpha_N = \lim_{t\to0}\frac{\ln(-c^2\lambda h(t)-h''(t))}{\ln(t)}
= \lim_{t\to0}\left|\frac{t(c^2\lambda h(t)+h''(t))}{c^2\lambda h(t)+h''(t)}\right|
\]
where we have used l'Hospital's rule and 
\[
b_N = -\lim_{t\to0}(c^2\lambda h(t)+h''(t)) t^{\alpha_N-1})\, \lambda^{-\beta_N}\Gamma(2-\alpha_N).
\]
Again, in order to also recover $\alpha_{N-1}$, \ldots, $\alpha_1$, one may also take into account mixed terms in the expansion of \eqref{eqn:hhat_large_s}
analogously to the large $t$ asymptotics case.
In case of two damping terms, this results in  
\begin{equation}\label{eqn:smalltime}
\begin{aligned}
&-c^2\lambda h(t)-h''(t) \\
&=\frac{b_1\lambda^{\beta_1}}{\Gamma(2-\alpha_1)} t^{1-\alpha_1}
+\frac{b_2\lambda^{\beta_2}}{\Gamma(2-\alpha_2)} t^{1-\alpha_2}
+\frac{b_1\lambda^{\beta_1}c^2\lambda}{\Gamma(4-\alpha_1)} t^{3-\alpha_1}
+\frac{b_2\lambda^{\beta_2}c^2\lambda}{\Gamma(4-\alpha_2)} t^{3-\alpha_2}
\\&\quad+\frac{b_1^2\lambda^{2\beta_1}}{\Gamma(4-2\alpha_1)} t^{3-2\alpha_1}
+\frac{b_1b_2\lambda^{\beta_1+\beta_2}}{\Gamma(4-\alpha_1-\alpha_2)} t^{3-\alpha_1-\alpha_2}
+\frac{b_2^2\lambda^{2\beta_2}}{\Gamma(4-2\alpha_2)} t^{3-2\alpha_2}
\\&\quad+ O(t^{5-3\alpha_2})\mbox{\ \  as }\ t\to0 .
\end{aligned}
\end{equation}
