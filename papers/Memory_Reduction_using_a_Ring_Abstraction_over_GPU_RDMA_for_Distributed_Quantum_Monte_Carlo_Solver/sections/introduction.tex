\section{Introduction}
\label{sec:intro}

% 1. Introduce DCA++ and mention its memory bound issue
Dynamical Cluster Approximation (DCA++)\footnote{DCA++ is available at \url{https://github.com/CompFUSE/DCA}} is a high-performance research software 
application~\cite{PhysRevB.58.R7475, PhysRevB.61.12739, RevModPhys.77.1027, dca_hpx_2020} that provides a modern
C++ implementation to solve quantum many-body problems. 
DCA++ implements a quantum cluster method with a
Quantum Monte Carlo (QMC) kernel for modeling strongly
correlated electron systems. 
The DCA++ software currently uses three different
programming models---message passing interface (MPI), Compute Unified Device Architecture (CUDA), and High Performance ParalleX (HPX)/C++ threading---together with three numerical libraries---Basic Linear Algebra Subprograms (BLAS), Linear Algebra Package (LAPACK),
and Matrix Algebra on GPU (MAGMA)---to expose the parallel computation. 

In the QMC kernel~\cite{dca2019}, the two-particle Green's function ($G_t$) 
is needed for computing important fundamental quantities, such as
the critical temperature ($T_c$), for superconductivity. 
%
In other words, a larger $G_t$ allows condensed matter
physicists to explore larger and more complex (i.e., higher fidelity)
physics cases.
%
DCA++ currently stores $G_t$  in a single GPU device.
%
However, this limits the largest $G_t$ that can be processed within one
GPU.
%
A new approach for partitioning the large $G_t$ across 
the multiple GPUs can significantly increase 
scientists' capabilities to explore higher fidelity simulations.
%
This paper focuses on how the memory-bound issue in DCA++ was successfully addressed by proposing an effective ``all-to-all'' communication
method---a ring algorithm---to update the distributed $G_t$ device array.
% Section.~\ref{sec:background} introduces physics background of DCA++, emphasizes
% the memory-bound challenge, as well as brief the GPU Remote Direct Memory Access knowledge.
% %
% Section.~\ref{sec:implement} details ring algorithms and various concurrency improvement
% for the algorithm.
% %
% Section.~\ref{sec:results} provides correctness verification, memory reduction analysis 
% and scaling results about our implementation. 
% %
% Section.~\ref{sec:discussion} discusses trade-off strategies for concurrency and memory
% and Section.~\ref{sec:concl} summaries our findings and gives an outlook for our future 
% research directions.
\subsection{Contributions}
The primary contributions of this work are outlined as follows.
\begin{enumerate}
    \item The memory consumption in a QMC solver application was reduced to store a much larger $G_t$ array across multi-GPUs. This significant contribution 
    enables physicists to evaluate larger scientific problem sizes and 
    compute the full $G_t$ array in a single computation, 
    which significantly increases the accuracy/fidelity of the simulation of a certain material.
    \item A ring abstraction layer was designed that updates the
    large distributed $G_t$ array. The ring algorithm was further improved 
    by adding sub-ring communicator and multi-threaded communication to 
    reduce communication overhead and expose more concurrency, respectively.
    \item The ring abstraction layer was implemented on top of NVIDIA 
    GPUDirect remote direct memory
access (RDMA) for fast device memory transfer.
    \item The Autonomic Performance Environment for Exascale (APEX) performance measurement library was extended to support the use case, driving tool development and research.
\end{enumerate}
