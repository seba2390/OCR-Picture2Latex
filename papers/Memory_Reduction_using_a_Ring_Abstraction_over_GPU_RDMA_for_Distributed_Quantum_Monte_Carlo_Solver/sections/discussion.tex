\section{Discussion}
\label{sec:discussion}

\subsection{Concurrency Overlapping}
The multi-threaded ring implementation provides
sufficient concurrency that overlaps communication and computation.
%
The APEX profiling tool was used to collect data on process activities over time and visualize the data in Vampir.

DCA++ was run with multi-threaded ring support and obtained the timeline activities in rank 0 at 49 s (Figure~\ref{fig:master_timeline}). 
%
Some concurrency overlap was observed in the multi-threaded
ring algorithm so that although some threads are
blocked in \texttt{MPI\_Wait}, other threads of the same rank perform useful computation tasks.
%
For example, the short blocks that are not labeled as \texttt{MPI\_Wait} are mostly related to kernel activities.

\begin{figure*}
	\centering
	\includegraphics[width=\textwidth]{images/master_timeline_rank.pdf}
	\caption{Vampir timeline graph shows the processes' activities over the time in rank 0 (DCA++ with multi-threaded ring algorithm).}
	\label{fig:master_timeline}
\end{figure*}

The current ring algorithm was also observed to be a lock-step algorithm in which the 
next computation (update $G_t$) cannot start until the previous 
communication step ($G_{\sigma}$ message exchange) is finished.
%
To expose more currency, HPX~\cite{Kaiser2020}---a task-based programming model---could be used to overlap the communication and computation.
%
For example, DCA++ kernel function can be wrapped into an HPX \textit{future}, which represents
an ongoing computation and asynchronous task. Then, the communication tasks can be attached or chained to the ``futurized'' kernel task. 
%
Wei et al.~\cite{dca_hpx_2020} reported that DCA++ with HPX user-level~\cite{hpxmp} threading 
support achieves a 20\% speedup over the original C++ threading (kernel-level) due to
faster context switching in HPX threading.

\subsection{Trade-Off between Concurrency and Memory}
As walker-accumulator threads increase in the
multi-threaded ring algorithm,
GPU memory usage is also increased because more device memory is needed to store extra thread-private $G_{\sigma,i}$ buffers. 
%
This might cause
a new memory-bound challenge if too many concurrent threads are used.
%
One possible solution is to reduce concurrent threads to achieve more usable device memory.
%

The same configuration was run for the original $G_t$ and distributed $G^d_t$ versions
with seven threads and then with one thread, respectively (Figure~\ref{fig:device_memory_used}).
%

\begin{figure}
\centering
     \begin{subfigure}[b]{\columnwidth}
         \centering
         \includegraphics[width=0.4\textwidth, trim=5cm 0 0 0, clip]{images/gpu_used_regular_7threads_labeled.pdf}
         \caption{Original $G_t$ method.}
         \label{fig:original_7threads}
     \end{subfigure}
     
    \begin{subfigure}[b]{\columnwidth}
         \centering
         \includegraphics[width=0.4\textwidth, trim=5cm 0 0 0, clip]{images/gpu_used_bind3GPUs_7threads_labeled.pdf}
         \caption{Distributed $G^d_t$ method with sub-ring size of three.}
         \label{fig:distributed_7threads}
     \end{subfigure}
     
\caption{Device memory used (GB over time) when using seven walker-accumulator thread. Visualized by Vampir.}
\label{fig:device_memory_used}
\end{figure}

For the comparison on seven threads (Figures~\ref{fig:original_7threads} and ~\ref{fig:distributed_7threads}),
the first spike in memory usage increase is due to $G_t$ allocation, and the second significant wave
is because each thread is allocating $G_{\sigma,i}$. 
%

The original algorithm needs 3.4GB for $G_t$ and 9.6GB in total,
and the new algorithm needs 1.3GB for $G^d_t$ and 11.2 GB in total. 
%
The non-$G_t$ allocation in the original algorithm is 6.2 GB, 
and distributed $G^d_t$ method is 9.9GB,
which leads to the overhead of 3.7 GB in $G^d_t$ version.
%
The $G_{\sigma,i}$ is composed of two same-size matrices 
(spin up and spin down matrix, each matrix is sized at 0.17 GB). 
%
In the original algorithm, the total $G_{\sigma}$ allocation is 
0.17*2*7 = 2.38 GB where 2 is the two matrices (up and down) in 
$G_{\sigma,i}$ and 7 is seven threads. 
%
In the distributed $G^d_t$ method, 
the total $G_{\sigma}$ allocation is 
0.17*2*3*7 = 7.14GB 
where 3 is three allocations 
($G_{\sigma,i}$ itself, \textit{sendBuf}, \textit{recvBuf}) 
per thread.
%
The overhead of overall $G_{\sigma}$ allocation 
in the ring algorithm is 7.14 \textminus 2.38 = 4.76 GB, 
which is about 1 GB more than the non-$G_t$ allocation (3.7GB). 
% 
In Figure~{\ref{fig:original_7threads}}, 
there is a significant reduction of allocated memory in the $42^{nd}$ second, which is 1GB memory deallocation in $G_{\sigma}$. 
%
However, we did not observe a similar drop or wave pattern 
in Figure~{\ref{fig:distributed_7threads}} because those 
\textit{sendBuf}, \textit{recvBuf} matrices are not dynamically 
allocated so that the dip before the allocations was hidden.
%
This explains the 1GB difference.

\begin{figure}[h]
\centering
     \begin{subfigure}[b]{\columnwidth}
         \centering
         \includegraphics[width=0.4\textwidth, trim=5cm 0 0 0, clip]{images/gpu_memory_used_regular_1thread_labeled.pdf}
         \caption{Original $G_t$ method.}
         \label{fig:original_1thread}
     \end{subfigure}
     
    \begin{subfigure}[b]{\columnwidth}
         \centering
         \includegraphics[width=0.4\textwidth, trim=5cm 0 0 0, clip]{images/gpu_memory_used_bindTo3GPU_1thread_labeled.pdf}
         \caption{Distributed $G^d_t$ method with sub-ring size of three.}
         \label{fig:distributed_1thread}
     \end{subfigure}
     
\caption{Device memory used (GB over time) when using one walker-accumulator thread. Visualized by Vampir.}
\end{figure}


However, if only one thread was used (Figures~\ref{fig:original_1thread} and ~\ref{fig:distributed_1thread}), then 
the maximum device usage in the distributed $G^d_t$ version (3.3 GB)
is 1.9 GB less than the one in the original $G_t$ version (5.2 GB). Much more usable
device memory can be gained if concurrent walker-accumulator threads are reduced. For example, the saved
device memory from reduced threads can be used to fit larger $G_t$. Furthermore, a comparison experiment was run on one Summit node (six ranks per node) by using the same input configuration 
(sub-ring size is three, measurement is 4,200 total), except for threading numbers per rank. 
The distributed $G^d_t$ with seven threads (87 s) has 1.3 times more speedup 
than the one with one thread (116 s). This result suggests that if there is insufficient 
device memory, then the code might use fewer threads with some loss (less than 30\%) of run time 
performance. The authors are considering quantifying and modeling this trade-off in their future research development.

To solve the NIC bottleneck issue and the new memory-bound challenge
caused by multi-threaded communication (storing additional
$G_{\sigma}$), the authors are considering another plan to move $G_{\sigma}$
to the CPU host in which the CPU host has more memory.
%
Each Summit node contains 512 GB of DDR4 memory for use by the \textit{IBM POWER9} 
processors, \footnote{Summit User Guide: \url{https://docs.olcf.ornl.gov/systems/summit_user_guide.html} }, 
and there are only 6  * 16 GB = 96 GB of device memory.
%
On Summit, the NICs are connected to the CPU and are not directly connected to the GPU.
%
The NVLINK2 connection between CPU and GPU has peak of 50 GB/s, so
it is faster compared with NIC's peak bandwidth (12.5 GB/s) and might not be the bottleneck.
%
One possible future extension could be to consider keeping $G_t$ on the
CPU side instead of in GPU device memory so that a
smaller sub-ring can be used or so the sub-ring can be kept on the same single node.
%

Additionally, the authors have explored bidirectional ring implementation 
that alternates ring communication directions between threads. 
%
After extensive testing, the authors concluded that the 
bidirectional ring improved performance up to 1.3X across-rack 
(each rack has 18 compute nodes on Summit) over the current unidirectional ring.
%
However, there are no potential performance benefits using the bidirectional ring approach over the current unidirectional ring when reserving the whole rack. 
%
Authors continue to investigate in coordination with hardware vendors to address the performance of bidirectional ring implementation.
