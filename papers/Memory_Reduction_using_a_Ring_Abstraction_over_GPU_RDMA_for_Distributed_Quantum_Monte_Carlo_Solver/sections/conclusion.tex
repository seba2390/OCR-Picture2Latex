\section{Conclusions}
\label{sec:concl}
This paper presents how the authors successfully solved the
memory-bound challenge in DCA++, which will allow physicists 
to explore significantly large science cases and increase
the accuracy and fidelity of simulations of certain materials.
%
An effective ``all-to-all'' communication method---a ring abstraction layer---was designed for this purpose so that the distributed device array $G_t$ can be updated across multi-GPUs.
%
The $G_t$ device array was distributed across 
multi-GPUs so that the allocation size for the most 
memory-intensive data structure per GPU is now reduced to $1/p$ of the original size, where $p$ is number of GPUs in the ring communicator.
%
This significant memory reduction (much larger $G_t$ capability) 
is the primary contribution 
from this work because condensed matter scientists are now able to 
explore much larger science cases.

In calculating the full 4-point correlation function $G_t$, 
the storage of Gt grows as O( $L^3$ $F^3$) 
where $L$ is the number of cluster sites 
and $F$ is the number of frequencies. 
%
This new capability will enable large-scale simulations 
such as  36-site (6x6 cluster) with over 64 frequencies to 
(1) obtain more accurate information, and 
(2) enable resolution of  longer wavelength correlations 
that have longer periodicity 
in real space and which cannot be resolved in smaller clusters. 
%
The system size can grow fairly large and 
depends on how much memory the leadership computing facilities
can provide. 
%
Relevant science problems that the domain specialists 
would like to study range in the orders of 
10s-of-100s of Gigabits of $G_t$, 
potentially opening up more research 
into how we can use the host memory without losing performance. 
%

The ring algorithm implementation takes advantage of 
GPUDirect RDMA technology, which can directly and 
efficiently exchange device memory. 
%
Several optimization techniques were used to improve the ring algorithm performance,
such as sub-ring communicators
and multi-threaded supports. 
%
These optimizations reduced communication overhead and 
expose more concurrency, respectively.
%
Performance profiling tools were also improved, such as APEX, which 
now allows more kernel and communication information
to be captured in-depth.
%
The ring algorithm was demonstrated to effectively reduce the memory
allocation needed for the $G_t$ device array per GPU. 
%
This paper also discusses various trade-offs between concurrency and memory 
usage for the multi-threaded ring algorithm and the NIC bottleneck issue.
%
In the future, the authors plan to explore the HPX run time system to overlap 
the computation and communication in DCA++
to expose more concurrency and asynchronicity.

\balance