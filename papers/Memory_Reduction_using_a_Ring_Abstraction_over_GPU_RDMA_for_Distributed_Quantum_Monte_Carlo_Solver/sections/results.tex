\section{Results}
\label{sec:results}
This section evaluates this work from various perspectives---including correctness, memory analysis, scaling, and function activities---with help from the APEX profiling tool. All experiments were run on 
Summit.

\subsection{Summit Node Configuration}
Summit is a 4,600 node, 200 PFLOPS IBM AC922 
system. 
Each node consists of two \emph{IBM
POWER9} CPUs with 512 GB DDR4 RAM and six NVIDIA V100 GPUs with a total of 96~GB
high-bandwidth memory. 
%
Each Summit node (Figure~\ref{fig:summit_node}) is divided into two sockets, and each socket has one \emph{IBM
POWER9} CPU and three NVIDIA V100 GPUs, all connected through NVIDIA's high-speed NVLINK2. Each NVLINK2 is capable of a 25 GB/s transfer rate in each direction.
%
Two \emph{IBM POWER9} CPUs within a Summit node are connected through 
Peripheral Component Interconnect Express bus (64 GB/s bidirectional). 
%
There is a Mellanox Infiniband EDR network interface connector (NIC) attached to
each Summit node (two ports per NIC, 12.5 GB/s per port).

%%%~\footnote{Summit User Guide: \url{https://docs.olcf.ornl.gov/systems/summit_user_guide.html}}
%%% \footnote{Summit ranked the second place in the TOP500 list in November 2020 \url{https://www.top500.org/}}

\begin{figure}
	\centering
	\includegraphics[width=\columnwidth]{images/summit_node.pdf}
	\caption{Architectural layout of a single node on Summit.}
	\label{fig:summit_node}
\end{figure}

\subsection{APEX}
APEX~\cite{huck2015autonomic} is a 
performance measurement library for distributed, asynchronous multitasking 
systems. It provides lightweight measurements without perturbing high
concurrency through synchronous and asynchronous interfaces.
To support performance measurement in systems that employ operating system- or user-level 
threading, APEX uses a dependency chain in addition to the call stack to
produce traces and task dependency graphs. 
The synchronous APEX instrumentation application programming interface (API) can be used to add instrumentation to a given
run time and includes support for timers and counters.  To support C++ threads
on Linux systems, the underlying POSIX threads are
automatically instrumented by using a preloaded shared object library that intercepts and wraps
pthread calls in the application. The NVIDIA CUDA Profiling Tools Interface~\cite{cuptiweb} provides CUDA host callback and device activity measurements.
Additionally, the hardware and operating system are monitored through an asynchronous measurement that involves the periodic or on-demand interrogation of the operating system, hardware states, or run time states (e.g., CPU use, 
resident set size, memory ``high water mark''). The NVIDIA Management Library interface~\cite{nvmlweb} provides periodic CUDA device monitoring to APEX.  For this work, APEX was 
extended to capture additional timers and counters related to CUDA device-to-device memory transfers, and support for key MPI calls was provided by
a minimal implementation of the MPI Profiling 
Interface~\cite{MPIbook}.
% Interface~\cite{forum1994mpi}.

\subsection{Accuracy Analysis}
To verify that this implementation generates correct results, 
the same input configuration was run  for original and ring algorithm methods, and the differences between the original $G_t$ and accumulated $G^d_t$ arrays were compared. 
%
A normalized L1 loss function (least absolute deviations, Eq.~[\ref{eq:l1error}]) and normalized L2 loss 
function (least square errors, Eq.~[\ref{eq:l2error}]) were used to compute the normalized error between original $G_t$ 
and accumulated $G^d_t$ arrays in which the ``entrywise''  norm was used.%
\footnote{Entrywise norm as defined in \url{https://en.wikipedia.org/wiki/Matrix_norm}}
%
The baseline is that the \textit{L1\_error} and \textit{L2\_error} between two arrays 
should be smaller than 5e-7 after DCA++ testing protocol, where:
%
\begin{equation}
\label{eq:l1error}
    L1\_{\mbox{\textit{error}}} = \frac{\| \mbox{vec}(G_t - G^d_t) \|_{1} }{\| \mbox{vec}(G_t) \|_{1}}
%    \frac{\sum_{i=0}^{N}(\| G_t - G^d_t \|_{1,1})}%
%                                       {\sum_{i=0}^{N}(\| G_t \|_{1,1})}
 ~ ,                                      
\end{equation}
%\end{minipage}%
\hfill
\begin{equation}
\label{eq:l2error}
    L2\_{\mbox{\textit{error}}} = \frac{ \| \mbox{vec}(G_t - G^d_t) \|_{2} }{ \| \mbox{vec}(G_t) \|_{2} }
%    \frac{\sum_{i=0}^{N}{{\|(G_t - G^d_t)}\|_{2,2}}^2}%
%                                       {\sum_{i=0}^{N}{{\|G_t\|_{2,2}}^2}}
~ .
\end{equation}

For input configuration, the single-band Hubbard model was chosen 
because it is a standard model of correlated electron systems 
and is used in almost all the studies of the 
cuprate high-temperature superconductors. 
%
Moreover, the cluster size was configured to 36-site (6x6 cluster), 
which is state-of-the-art simulations size. 
%
100,000 Monte Carlo measurements were chosen to observe runtime 
performance of the ring algorithms as the runtime scales linearly 
with the number of measurements for constant number of ranks. 
%
Since the cluster size was configured to 6*6 and 
four-point-fermionic-frequencies was set to 64, 
this leads 212,336,640 entries in $G_t$. 
%
Since each $G_t$ entry is a 
double-precision complex number, the $G_t$ memory size is about 3.4 GB. 
%
This configuration can produce large $G_t$ but still will not hit memory-bound issues 
on Summit GPUs---in which each GPU has 16 GB---for the regular $G_t$ version. 
%
Such configurations were run on one Summit node five times with six ranks per node and seven walker-accumulator threads per rank. For the distributed $G^d_t$ version, ring size was set to six so there was only one sub-ring during the run. 
%
The results show that the implementation generates correct results (Table~\ref{table:correctness})
because the \textit{L1\_error} and \textit{L2\_error} on accumulated $G^d_t$ are in an acceptable range.
\begin{table}
\caption{Comparison of function differences between the original $G_t$ and accumulated $G^d_t$ over five runs.}
\label{table:correctness}
\centering
\begin{tabular}{|>{\centering\arraybackslash}m{11mm}
                |>{\raggedleft\arraybackslash}m{30mm}
                |>{\raggedleft\arraybackslash}m{30mm}
                |>{\centering\arraybackslash}m{8mm}|}
\hline
\multicolumn{1}{|>{\centering\arraybackslash}m{11mm}|}{\textbf{Error}} 
    & \multicolumn{1}{>{\centering\arraybackslash}m{30mm}|}{\textbf{Real part}} 
    & \multicolumn{1}{>{\centering\arraybackslash}m{30mm}|}{\textbf{Imaginary part}} 
    & \multicolumn{1}{>{\centering\arraybackslash}m{8mm}|}{\textbf{<5e-7 }}\\
     \hline
     L1 & 3.71e-09$\pm$1.74e-18 & 4.61e-09$\pm$2.16e-18 & True \\
     \hline
     L2 & 3.10e-10$\pm$4.19e-18 & 3.37e-10$\pm$3.89e-18 & True \\
    \hline
\end{tabular}
\end{table}

\subsection{Memory Analysis}
\label{subsec:memory_analysis}
The memory analysis results show that device memory required for $G^d_t$ decreases linearly to the size of the sub-ring or the number of MPI ranks in the sub-communicator, which fits
the ring algorithm.
%
The APEX profiling tool was used to collect memory allocation information over the time.
%
The performance results reflect correctly to the ring algorithm method because the $G_t$ was evenly distributed across MPI ranks---in which each rank uses 1 GPU---within one sub-ring communicator. 

%
For example, the requested size in \texttt{cudaMalloc} API was compared between original $G_t$ (Figure~\ref{fig:regular_g4}) and distributed $G^d_t$ 
(sub-ring size of three, Figure~\ref{fig:distributed_g4_bind3GPU}) methods. 
%
This shows that the distributed $G^d_t$ method produced three times less memory allocation 
than the original $G_t$ device array. 
%
At around 7 s in both cases, the distributed $G^d_t$ method allocated 
1.13 GB for $G^d_{t,i}$, and the original $G_t$ method allocated 
3.40 GB for $G_{t,i}$. 

% \begin{figure}
% \centering
% \subfloat[a][Original $G_t$ implementation]{
% \includegraphics[width=0.45\textwidth, trim=5cm 0 0 0, clip]{cudaMalloc_regular_7threads_regular_labeled.pdf}
% \label{fig:regular_g4}}
% \qquad
% \subfloat[b][Distributed $G^d_t$ implementation with sub-ring size of three.]{
% \includegraphics[width=0.45\textwidth, trim=5cm 0 0 0, clip, trim=5cm 0 0 0, clip]{cudaMalloc_bindTo3GPU_7threads_labeled.pdf}
% \label{fig:distributed_g4_bind3GPU}}

% \caption{cudaMalloc requested size (GB) over time visualized by Vampir.}
% \label{fig:cudaMalloc}
% \end{figure}

\begin{figure}
\centering
     \begin{subfigure}[b]{\columnwidth}
         \centering
         \includegraphics[width=0.4\textwidth, trim=5cm 0 0 0, clip]{images/cudaMalloc_regular_7threads_regular_labeled.pdf}
         \caption{Original $G_t$ implementation.}
         \label{fig:regular_g4}
     \end{subfigure}
     
    \begin{subfigure}[b]{\columnwidth}
         \centering
         \includegraphics[width=0.4\textwidth, trim=5cm 0 0 0, clip]{images/cudaMalloc_bindTo3GPU_7threads_labeled.pdf}
         \caption{Distributed $G^d_t$ implementation with sub-ring size of three.}
         \label{fig:distributed_g4_bind3GPU}
     \end{subfigure}
     
\caption{cudaMalloc requested size (GB) over time visualized by Vampir.}
\label{fig:cudaMalloc}
\end{figure}

\subsection{Scaling Results}
% volume scales as S^2, where S is size of subring
% number of messages across each communication link scales as S
% Figure fig:subring_scaling 
% message size is fixed about 170MBytes
% each rank generate 1400 messages
% results shows nearly quadratic nearly identical to linear fit 
% elaspsed time grows linearly as S
% suggests not dominated by total volume of messages
% limiting bottleneck is slowest communication link
% effective bandwidth about 6 GBytes/sec
% peak bandwidth is about 12.5 GBytes/sec
% 
In the pipeline sub-ring algorithm, each rank sends $S-1$ and receives
$S-1$ messages, where $S$ is the size of sub-ring. Thus, the total
number of messages scales quadratically as O($S^2$), but the number
of messages crossing each communication link increases linearly as
O($S$).
%
Figure~\ref{fig:subring_scaling} shows the elapsed computation time
for 1,400 measurements (per rank) of the sub-ring algorithm running with six ranks
per Summit node in which each message
is about 170~MB.
%
The data are well approximated by a linear least-square line that
indicates that the elapsed computation time increases linearly as the sub-ring size increases.
%
This suggests that the sub-ring algorithm is not constrained by the total
volume of messages but is restricted by the slowest communication link.
%
The effective bandwidth of the sub-ring algorithm can be estimated as:
\[
\mbox{effective bandwidth} \approx (170*10^6 * S * 1,400)/(\mbox{ elapsed time}),
\]
and this is about 6~GB/s using the data for $S=60$
on 10 nodes
in Figure~\ref{fig:subring_scaling}.
%
This effective bandwidth is about 50\% of the theoretical peak
bandwidth for the NIC (12.5~GB/s per port)
on the Summit node.
%
\begin{figure}[t]
	\centering
	\includegraphics[width=0.8\columnwidth, trim=2cm 2cm 2cm 2cm, clip]{images/subring_scaling.pdf}
	\caption{Time for 1,400 iterations (per rank) of the sub-ring algorithm using six ranks per Summit node, and 
	         each message size is 170~MB.}
	\label{fig:subring_scaling}
\end{figure}


The authors acknowledge that enabling the ring algorithm 
to solve existing small problem-size 
(single band hubbard model with lower cluster size) 
will be an overkill, since the communication overhead 
will drastically increase the runtime; 
therefore the authors propose, the ring algorithm be 
only used when the $G_t$ cannot fit into one single GPU memory. 
%
When the original DCA++ is executed with a large enough problem size 
(when $G_t$ cannot fit into one single GPU),
the program simply crashes failing to allocate memory on the device. 
%
Moreover, The scalability issue of the ring algorithm 
was the core focus for the authors during the implementation 
and the optimization design strategies of the sub-ring algorithm. 
%
Without sub-ring optimization, the originally proposed ring algorithm 
will potentially take undesirably long period of time 
to finish a run of DCA++, 
especially when requesting thousands of compute nodes. 
%
With the sub-ring optimization, scientists are able to 
run large science cases 
while maintaining acceptable communication overhead. 

Since the current sub-ring size has to be configured manually, 
the authors plan to design a runtime adaptivity optimization 
that will automatically adapt the optimal sub-ring size. 
%
This optimization will distribute $G_t$
into the minimal number of devices as well as 
preserves optimal runtime performance.
%
This runtime adaptivity will be very helpful 
because DCA++ is an iterative convergence algorithm 
and thus $G_t$ size could be 
dynamically changed over multiple DCA++ runs 
for production science runs on leadership computing facilities. 

