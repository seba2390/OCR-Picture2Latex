\section{Related Work}
\label{sec:related_work}
\begin{spacing}{0.9}
Software-managed HMS has been studied in prior work.
Dulloor et. al~\cite{eurosys16:dulloor} introduce a data placement runtime
based on \textit{offline profiling} of application memory access patterns.
Their work targets on enterprise workloads. To decide data placement, they classify memory access patterns into streaming, pointer chasing, and random.
Giardino et. al~\cite{nas16:giardino} rely on OS and application co-scheduling
data placement. In particular, they build APIs that allow programmers
to describe their memory usage characteristics
to OS, through which OS receives and implements
responsive page placement and data migration.
Lin et. al~\cite{asplos16:lin} introduce
a protected OS service for asynchronous memory movement on HMS.
Du et. al~\cite{ismm16:shen} develop a PIN-based \textit{offline profiling} tool
to collect memory traces and provide guidance for placing data on HMS.

Different from the prior efforts, our work requires neither offline profiling as in~\cite{eurosys16:dulloor, ismm16:shen}
nor programmer involvement to identify memory access patterns as in~\cite{nas16:giardino}.
Furthermore, our work does not require the modification of OS, which is different from~\cite{asplos16:lin}.
%Hence, our work provides a better solution for legacy HPC applications and systems.
Our work aims for legacy HPC applications and systems.

Some studies introduce hardware-based data placement solutions for the NVM-based HMS.
Bivens et al.~\cite{hetero_mem_arch} and Qureshi et al.~\cite{qureshi_micro09, ibm_isca09} use DRAM as a set-associative cache logically placed between processor and NVM.
NVM is accessed when DRAM buffer eviction or buffer miss happens. 
Yoon et al.~\cite{row_buffer_pcm_iccd12} place data based on row buffer locality in memory devices.
Wang et al.~\cite{gpu_pcm_pact13} rely on static analysis and advanced memory controller to monitor memory access patterns %at runtime 
to determine data placement on GPU.
Wu et al.~\cite{hpdc16:wu} leverage the knowledge of numerical algorithms to direct data placement. They introduce hardware modifications to support massive data migration and performance optimization. 
Agarwal et al.~\cite{asplos15:agarwal} introduce a bandwidth-aware data placement on GPU, driven by compiler extracted insights and explicit hints from programmers.

A key limitation of the above hardware-based approaches is that they heavily rely on modified hardware to monitor memory access patterns and migrate data.
Some work, such as~\cite{qureshi_micro09, ibm_isca09, gpu_pcm_pact13, row_buffer_pcm_iccd12},
ignores application semantics and triggers data movement based on temporal memory access patterns,
which could cause unnecessary data movement.
Our work avoids any hardware modification,
and explores global optimization on data placement.
%for optimal performance.
\vspace{-15pt}
\end{spacing}

\begin{comment}
Ramos et al.~\cite{Ramos:ics11} rely on MC to monitor write intensity and popularity
of memory pages, which is used to migrate pages between DRAM and PCM.
Wang et al.~\cite{gpu_pcm_pact13} rely on static analysis and MC runtime monitoring to determine data placement on GPU.
A key limitation of these approaches is that they rely heavily on hardware-based monitoring mechanisms and caching policies
to direct data placement without awareness of application semantics.  
%As a result, data saved in non-volatile PCM cannot be used to resume computation after the system
%crashes, because of the data integrity concerns. 
Hence, they can result in inefficient data copy/migration with poor performance and low energy efficiency, because the data management
algorithms are generally based on memory access patterns monitored within a user-defined time period.
Depending on the duration of the time period and other heuristic parameters to trigger data movement,
these algorithms may not work well for a range of workloads and need to be disabled to avoid performance loss.
By contrast, our technique takes a holistic view of algorithm structure and application data-access pattern and hence, it avoids the above problems in the traditional solutions.
A few other works use software-based extensions to implement persistency semantics~\cite{Condit:sosp09, mnemosyne_asplos11, Coburn:asplos11, Venkataraman:fast11}. However, these techniques  cannot work well for HPC domain, due to their large overhead and limited support for massive data movement.

Numerical algorithms play crucial role in HPC and hence, several researchers have explored techniques for utilizing algorithm knowledge to improve application fault tolerance 
(e.g.,~\cite{gmm_scala11, ftcg_ppopp13, ft_lu_hpdc13, ftfactor_ppopp12}),
performance~\cite{Lam:asplos91, Williams:sc07, Faverge:ipdps14}, and energy efficiency~\cite{Garcia:lcpc13, Dorrance:fpga14}. Different from previous efforts, this paper demonstrates the significant benefits of using algorithm knowledge to direct data placement in the future hybrid memory system.
\end{comment}
