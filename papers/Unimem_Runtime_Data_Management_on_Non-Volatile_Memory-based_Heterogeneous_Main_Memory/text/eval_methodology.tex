\section{Evaluation Methodology}
\label{sec:eval_method}
In our evaluation, we use Quartz emulator~\cite{middleware15:volos}.
Quartz enables an efficient emulation of a range
of NVM latency and bandwidth characteristics. 
%for performance evaluation of NVM and their impact
%on applications performance.
%(without modifying or instrumenting
%their source code) by leveraging features available in commodity hardware.
Quartz has low overhead and good accuracy (with emulation
errors 0.2\% - 9\%)~\cite{middleware15:volos}.
We do not use cycle-accurate architecture simulators because of
their slow simulation which cannot scale to large workloads.
Furthermore, Quartz allows us to consider cache eviction effects,
memory-level parallelism, and system-wise memory traffic, which is not available in other state-of-the-art, software-based emulation approaches~\cite{pcmsim, mnemosyne_asplos11}.
However, due to the limitation of Quartz, 
%we cannot distinguish  the latency difference between read and write operations;
we can only emulate either bandwidth limitation or latency limitation,
but cannot emulate both of them.

Using Quartz requires the user to have privilege access to the test system.
We do not have such privilege access on the test platform for our strong scaling tests.
Hence, instead of using Quartz, we leverage NUMA architecture to emulate NVM.
In particular, we carefully manage data placement at the user level, 
such that, given an MPI task, a remote NUMA memory node works as NVM while the NUMA node local to the MPI task works as DRAM.
The latency and bandwidth difference between the remote and local NUMA memory nodes emulates that between NVM and DRAM.
On our test platform for strong scaling tests, the emulated NVM has 60\%
of DRAM bandwidth and 1.89x of DRAM latency.

We have two test platforms for performance evaluation.
One test platform (named ``Platform A'') is a small cluster. Each node of it has two eight-core Intel Xeon E5-2630 processors (2.4 GHz) and 
32GB DDR4. We use this platform for tests in all figures except Figure~\ref{fig:strong_scaling_cg}. %and~\ref{fig:strong_scaling_bt}. 
We deploy Quartz on such platform.
The other test platform is the Edison supercomputer at Lawrence Berkeley National Lab (LBNL). We use this platform for tests in Figure~\ref{fig:strong_scaling_cg}. %and ~\ref{fig:strong_scaling_bt}.
Each Edison node has two 12-core Intel Ivy Bridge processor (2.4 GHz) with 64GB DDR3. 
As discussed before, we perform strong scaling tests and leverage NUMA architecture to emulate NVM on this system. 

We use six benchmarks from NAS parallel benchmark (NPB) suite 3.3.1,
and one production scientific code Nek5000~\cite{27-nek5000}.
For Nek5000, we use eddy input problem with a $256\times256$ mesh.
%\textbf{TODO: which are our target data objects?}
The target data objects of those benchmarks are listed in Table~\ref{tab:data_objects_list}.
Those data objects are the most critical data objects accounting for more than 95\% of
memory footprint except CG and Nek5000. For CG, there are three large data objects ($aelt$, $acol$, and $arow$) only used for problem initialization. They are not treated as target data objects.
For Nek5000, we use main simulation variables and geometry arrays in Nek5000 core. Those are the most important data objects for Nek5000 simulation.
We use GNU compiler (4.4.7 on Platform A and 6.1.0 on Edison) and use default compiler options for building benchmarks. We use the sampling-based approach to collect performance events on the two
platforms. The sampling interval is chosen as 1000 CPU cycles, such that the sampling overhead is ignorable while the sampling is not sparse to lose modeling accuracy. 
%\textbf{TODO}: performance counter sampling interval  is 1000 cycles.
\vspace{-10pt}

\begin{table}
        \begin{center}
        \caption{Target data objects in NPB benchmarks and Nek5000}
        \vspace{-10pt}
        \label{tab:data_objects_list}
        \tiny
        \begin{tabular}{|p{1.2cm}|p{4cm}|p{2cm}|}
        \hline
        \textbf{Benchmark}    & \textbf{Target data objects}  &\textbf{\% of total app mem footprint}                                 \\ \hline \hline
         CG &  $colidx$, $a$, $w$, $z$, $p$, $q$, $r$, $rowst$, $x$ &  42\%  \\ \hline
         FT & $u$, $u0$, $u1$, $u2$, $twiddle$ & 99\%    \\ \hline
         BT & $rhs$, $forcing$, $u$, $us$, $vs$, $ws$, $qs$, $rho\_i$, $square$, $out\_buffer$, $in\_buffer$, $fjac$, $njac$, $lhsa$, $lhsb$, $lhsc$ &  99\% \\ \hline 
         LU & $u$, $rsd$, $frct$, $flux$, $a$, $b$, $c$, $d$, $buf$, $buf1$  &  99\%  \\ \hline
         SP & $u$, $us$, $vs$, $ws$, $qs$, $rho\_i$, $square$, $rhs$, $forcing$, $out\_buffer$, $in\_buffer$, $lhs$ & 98\%  \\ \hline
         MG & $buff$, $u$, $v$, $r$ & 99\%					\\ \hline
         Nek5000(eddy) &  Geometry arrays and main simulation variables (48 data objects in total) & 35\% \\ \hline
        \end{tabular}
        \end{center}
        \vspace{-10pt}
\end{table}
