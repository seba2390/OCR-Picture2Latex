\section{Background}
\label{sec:bg}
%The basic assumption on the memory architecture. (DRAM and NVM side-by-side)
%To overcome the limitation of NVM-only system, future systems
%are likely to build HMS by coupling NVM with a smaller amount of DRAM.
In HMS, we assume that DRAM shares the same physical address space as NVM (but with different addresses) and DRAM memory allocation can be managed at the user level. This assumption has been widely used in the existing work~\cite{eurosys16:dulloor, nas16:giardino, asplos16:lin, ismm16:shen,  hpdc16:wu}. %We assume such system in this paper.

\subsection{Definitions and Basic Assumptions}
%What are phases? Definition of phases (check ICS'09 (LLNL) power "Adagio: DVS practical") \\
%How to handle non-blocking MPI call as phases? How to handle MPI_Wait\\
%Draw a figure to explain the idea of "phases" if necessary;
%We target a SPMD programming model on distributed memory systems using 
%message passing, particularly MPI, for any communication between processes.
%For a parallel application based on such programming model,
We target on the MPI programming model.
For a parallel application based on MPI, we decompose the application into phases.
A phase can be a computation phase delineated by MPI operations;
A phase can also be an MPI communication phase doing collective operations, point-to-point communication operations, 
or synchronization. For a non-blocking communication (e.g., {\fontfamily{qcr}\selectfont MPI\_Isend}), the MPI communication call 
is not a phase. Instead, it is merged into 
the immediately following phase. 
The communication completion operation (e.g., {\fontfamily{qcr}\selectfont MPI\_Wait}) is a communication phase. 

Furthermore, we target on parallel applications from the HPC domain with
an iterative structure. In those applications, each program phase is executed many times.
Such parallel applications are very common. 
As an example, Figure~\ref{fig:phases_gen_desc} depicts a typical iterative structure from CG (an NAS parallel benchmark~\cite{nas}), which dominates the execution time of CG.

\begin{figure}
\centering
\includegraphics[width=0.2\textwidth, height=0.25\textheight]{figures/phases_gen_desc.pdf}
\vspace{-5pt}
\caption{A conceptual description for an MPI-based program (CG) decomposed into phases. $A$ is a 2D matrix, and $q, p, z$, and $r$ are vectors.}
\label{fig:phases_gen_desc}
\vspace{-15pt}
\end{figure}

We claim a data object is~\textit{bandwidth sensitive}, if there is a big performance difference between placing it in %bandwidth-limited NVM and DRAM.
NVM with less memory bandwidth and DRAM.
We claim a data object is~\textit{latency sensitive}, if there is
a big performance difference between placing it in %latency-limited NVM and DRAM.
NVM with longer memory access latency and DRAM.

%\subsection{Performance Characteristics of Numerical Algorithms on NVM}
\subsection{Preliminary Performance Evaluation with NVM-Based Main Memory}
NVM has relatively long access latency and low memory bandwidth.
Table~\ref{tab:nvm_features} shows NVM performance characteristics.
The table is based on~\cite{NVMDB} that gathered a comprehensive survey of 340 non-volatile memory technology papers published between 2000 and 2014 in relevant conferences.
Based on such performance characteristics, we perform preliminary performance study
to quantify the impact of NVM on HPC application performance.
%Also, we want to quantify the performance gap between NVM-only and DRAM-only main memories.
%Such study can quantify the application performance gap between NVM- and DRAM-based main memory.

\begin{comment}
\begin{table*}
        \tiny
        \begin{center}
            \caption{NVM performance characteristics, and comparison between NVM techniques and DRAM~\cite{NVMDB}}
            \vspace{-10pt}
       \label{tab:nvm_features}
       \begin{tabular}{|p{2.4cm}|p{1.8cm}|p{1.8cm}|p{2cm}|p{2.5cm}|p{2cm}|p{2cm}|}
       \hline
              & \textbf{Read time (ns)}  & \textbf{Write time (ns)} &   \textbf{32B random read BW (MB/s)} & \textbf{32B random write BW (MB/s)} & \textbf{Seq read BW (MB/s)} &   \textbf{Seq write BW (MB/s)}                  \\ \hline \hline
                                              DRAM       & 10    & 10 & 1,000 & 900 & 11,000 & 11,000        \\\hline
                                              STT-RAM (ITRS'13)      & 60 & 80 & 800 & 600 & 11,000 & 11,000                                 \\ \hline
                                              PCRAM        & 20-200 & 80-10,000 & 200-800 & 100-800 & 11,000 & 2,000-11,000                            \\ \hline
                                            ReRAM       & 10-1,000 & 10-10,000 & 20-100 & 1-8 & 10,000-11,000 & 900-8,000               \\ \hline
      \end{tabular}
      \end{center}
\end{table*}
\end{comment}

\begin{table}
        \tiny
        \begin{center}
            \caption{NVM performance characteristics, and comparison between NVM techniques and DRAM~\cite{NVMDB}}
            \vspace{-10pt}
       \label{tab:nvm_features}
       \begin{tabular}{|p{0.8cm}|p{1.2cm}|p{1.2cm}|p{1.5cm}|p{1.5cm}|}
       \hline
              & \textbf{Read time}  & \textbf{Write time} &   \textbf{Random read BW} & \textbf{Random write BW}    \\ \hline \hline
                                              DRAM       & 10 ns    & 10 ns & 1,000 MB/s & 900 MB/s        \\\hline
                                              STT-RAM (ITRS'13)      & 60 ns & 80 ns & 800 MB/s & 600 MB/s                                  \\ \hline
                                              PCRAM        & 20-200 ns & 80-10,000 ns & 200-800 MB/s & 100-800 MB/s                            \\ \hline
                                            ReRAM       & 10-1,000 ns & 10-10,000 ns & 20-100 MB/s & 1-8 MB/s            \\ \hline
      \end{tabular}
      \end{center}
      \vspace{-10pt}
\end{table}


We use Quartz, a DRAM-based, lightweight performance emulator for NVM~\cite{middleware15:volos}. %for our study. 
The existing work uses cycle-accurate simulation to study NVM performance~\cite{nvm_ipdps12, hpdc16:wu}.
However, the long simulation time makes impossible simulate~\textit{HPC applications} with large data sets on multiple nodes.
The performance of HPC workloads on NVM is always mysterious.
Using Quartz, we can study performance (execution time) of HPC workloads with much shorter time. 
We deploy our tests on four nodes in Platform A (the configurations of those nodes and Platform A are summarized in Section~\ref{sec:eva}).
We change the emulated NVM bandwidth and latency, and run a set of NAS parallel benchmarks. We use Class D as input and run 16 MPI processes (4 MPI processes per node). For the benchmark FT, we use CLASS C as input because of the long execution time with Class D.  Figures~\ref{fig:bg_bw_sensitivity} and~\ref{fig:bg_lat_sensitivity} show the emulation results.
%In the rest of the paper, we call a data object that has the performance of its data placement sensitive to memory bandwidth (or latency) as \textit{bandwidth sensitive} (\textit{latency sensitive}) data object. 


%Evaluation 1: changes latency and bandwidth and study the performance sensitivity of benchmarks and applications \\
%Evaluation 2: the performance sensitivity of data objects with different data placement 

\begin{figure}[!t]
    \centering
    \includegraphics[width=0.48\textwidth, height=0.13\textheight]{figures/figure2_bw.pdf}
    \caption{The benchmark performance (execution time) on NVM-based main memory (NVM-only) with various bandwidth. The performance is normalized to that of DRAM-only systems.} 
    \label{fig:bg_bw_sensitivity}
\end{figure}

\begin{figure}[!t]
    \centering
    \includegraphics[width=0.48\textwidth, height=0.13\textheight]{figures/bg_lat_sensitivity.pdf}
	    \caption{The benchmark performance (execution time) on NVM-based main memory (NVM-only) with various latency. The performance is normalized to that of DRAM-only systems.} 
    \label{fig:bg_lat_sensitivity}
\end{figure}

\begin{figure}[!t]
    \centering
    \includegraphics[width=0.48\textwidth, height=0.16\textheight]{figures/figure4_edit.pdf}
    \caption{The impact of data placement on performance (execution time) of NVM-based main memory. The performance is normalized to DRAM-only systems. The legend entries ``in\_buffer+out\_buffer'', ``lhs'', and ``rhs'' are the data objects placed in DRAM in the DRAM+NVM system. The $x$ axis shows the configuration of NVM (4x DRAM latency or 1/2 DRAM bandwidth).}
    \label{fig:bg_data_placement_impact}
    \vspace{-15pt}
\end{figure}

\textbf{Observation 1:}
We find a big performance gap between DRAM-only and NVM-only systems.
%for HPC workloads in the two figures. 
This observation is contrary to an existing conclusion (i.e., no big gap) for HPC workloads 
based on a single node simulation~\cite{nvm_ipdps12}.
Furthermore, HPC application performance (execution time) is sensitive to different NVM technologies with various bandwidth and latency. With memory bandwidth reduced by only 1/2 or latency increased by only 2x in NVM, some benchmarks already show big slowdown. For example, LU has 2.19x and 2.14x slowdown with NVM configured with 1/2 DRAM bandwidth (Figure~\ref{fig:bg_bw_sensitivity}) and 2x DRAM latency (Figure~\ref{fig:bg_lat_sensitivity}) respectively.
%the benchmarks have 10\%-218\% performance loss; with 1/8 of DRAM bandwidth, the benchmarks have 68\%-1122\% performance loss. With 4x of DRAM latency, the benchmarks have 2.1\%-56.2\% performance loss.  

%those benchmarks are particularly sensitive to memory bandwidth reduction. 

We further study whether data placement in HMS can bridge the performance gap between DRAM-based and NVM-based systems. We choose SP benchmark and focus on four critical data objects of SP (the arrays $lhs$, $rhs$, $in\_buffer$ and $out\_buffer$). We use two configurations for NVM, one with 1/2 DRAM bandwidth and the other with 4x DRAM latency.
%For each NVM configuration, we do four tests, each of which places one of the four
%data objects in DRAM while placing the other data objects in NVM. 
For each data object with an NVM configuration (either 1/2 DRAM bandwidth or 4x DRAM latency),
we do three tests. In the first test we use DRAM-only system. In the second test we use
a DRAM+NVM system. For this test, a target data object is placed in DRAM (see the legend entries in Figure~\ref{fig:bg_data_placement_impact}), while the rest of
data objects are placed in NVM. In the third test we use an NVM-only system.
In each test, we use 4 nodes with one MPI task per node, and use CLASS C and CLASS D as input.
Figure~\ref{fig:bg_data_placement_impact} shows the results. 
The results are normalized to the performance of DRAM-only.
%With the NVM configurations of 1/2 DRAM bandwidth and 4x DRAM latency, there are 25\% and 92\% performance loss respectively.

\textbf{Observation 2:}
A good data placement can effectively bridge the performance gap.
For example, with the data object $lhs$ placed in DRAM, we bridge the performance gap between DRAM and NVM (using the configuration of 4x DRAM latency and CLASS C) by 31\% (see Figure~\ref{fig:bg_data_placement_impact}).

\textbf{Observation 3:}
%To improvement performance, different data objects have different preference to data placement.
Different data objects manifest different sensitivity to
limited NVM bandwidth and latency, shown in Figure~\ref{fig:bg_data_placement_impact}. 
For example, for the data objects $in\_buffer$ and $out\_buffer$ (CLASS D), 
there is no big performance difference (2.1 vs. 2.15) between placing them in DRAM and placing them in NVM configured with 4X DRAM latency; 
%after moved from NVM (configured with 4X DRAM latency) to DRAM, does not result in performance improvement.
However, 
%if $rhs$ is placed from NVM (configured with 1/2 DRAM bandwidth) to DRAM,
%we bridge the performance gap between NVM-only and DRAM-only by 48\%.
there is a big performance difference (1.14 vs. 1.25) between placing them in DRAM and placing them in NVM configured with 1/2 DRAM bandwidth (CLASS D). 
This indicates that the two data objects are sensitive to memory bandwidth but not memory latency. 
$lhs$ (CLASS D) tells us a different story:
it is sensitive to latency (1.71 vs. 2.15), but not bandwidth (1.21 vs. 1.25). 
Also, $rhs$ is sensitive to both latency and bandwidth.
%due to their bandwidth and latency requirement.

Different data objects have different memory access patterns which manifest
different sensitivity to bandwidth and latency.
A data object with a memory access pattern of bad data locality and massive, concurrent memory accesses (e.g., streaming pattern) is sensitive
to memory bandwidth, while a data object with a memory access pattern of bad data locality and dependent memory accesses (e.g., pointer-chasing) is sensitive to memory latency. 


\begin{comment}
For example, the data object $p$ is not sensitive to bandwidth and latency limitation of NVM: its data placement in NVM and DRAM does not matter a lot to performance ($\le$ 1.2\% performance difference). 
On the other hand, the data object $a$ is sensitive to bandwidth but not latency (Class D): its data placement in DRAM improves performance by 11\%.
Also, the data object $a$ is sensitive to both bandwidth and latency when using
Class C: its data placement in DRAM improves performance by 13\% (1/2 DRAM bandwidth) and 27\% (4x DRAM latency) respectively.

Further study reveals that the memory accesses to $a$ are dominated by
a streaming memory access pattern and such pattern does not have data dependency between memory accesses. A large memory bandwidth is beneficial for such data object with concurrent memory accesses and few data locality.
This above fact is especially pronounced when we use a larger input problem (Class D).
The memory accesses to $p$ are dominated by indirect data references ($p[colidx[i]]$),
and manifest a random memory access pattern with data reuse. 
\textbf{The size of $p$ is also relatively small (comparing with the last level cache size). As a result, a large body of memory references to $p$ happen in the cache hierarchy. Hence, $p$ is not sensitive to the data placement.}
\end{comment}

Our preliminary performance study highlights the importance of capturing memory
access patterns of data objects. It also shows us that it is possible to bridge the performance gap between NVM and DRAM by appropriately directing data placement on HMS.

\vspace{-10pt}


