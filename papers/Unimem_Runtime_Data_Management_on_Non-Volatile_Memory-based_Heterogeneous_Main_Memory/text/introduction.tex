\section{Introduction}
\label{sec:intro}
%The emergence of NVM. \\
Non-volatile memory (NVM), such as phase change memory (PCM) and resistive random-access memory (ReRAM), is a promising technique to build future high performance computing (HPC) systems.
The popularity of many-core platforms in HPC %(such as Intel Xeon Phi on Aurora and Cori HPC)
and large data sets in scientific simulations drive the fast development of NVM,
because NVM can provide a scalable and power-efficient solution as main memory,
alternative to DRAM.
Such solution is based on the attractive characteristics of NVM, such as higher density and near-zero static power consumption.

However, comparing with DRAM, NVM as main memory can be challenging.
The promising NVM solutions (e.g., PCM and ReRAM), 
although providing larger capacity at the similar or lower cost than DRAM,
can have higher latency and lower bandwidth (see Table~\ref{tab:nvm_features}).
Such NVM features can introduce a big performance gap between emerging NVM-based and traditional DRAM-based systems for HPC applications. 
Our initial performance evaluation with HPC workloads (Section~\ref{sec:bg}) shows that there is 1.09x-8.4x slowdown on NVM-based systems, depending on bandwidth and latency features of NVM.
Because of the limitation of NVM, NVM is often paired with a small fraction of DRAM to form a heterogeneous memory system (HMS)~\cite{eurosys16:dulloor, nas16:giardino, asplos16:lin, ismm16:shen, gpu_pcm_pact13, hpdc16:wu}.
%Leveraging such HMS, 
By selectively placing frequently accessed data in the small amount of DRAM available in HMS, we are able to exploit the cost and scaling benefits of NVM while minimizing the limitation of NVM with DRAM.

\begin{comment}
The emergence of the NVM-based HMS breaks the uniformity in the traditional memory system:
%the previous assumption that all memory devices have the similar latency and bandwidth is not true.
%and introduces a gap between application and memory:
As a result, the existing system must be evolved to handle such memory heterogeneity.
Data objects of the application must be carefully placed to NVM and DRAM
for best performance.
%Mapping data objects of the application to memory is not straightforward as in the traditional system. Instead, we must carefully place those data objects on HMS for best performance.
%In fact, such breaking in memory uniformity is not new: we already see that in NUMA systems. But the emergence of NVM exacerbates this problem.
\end{comment}

To manage data placement on HMS for HPC, we have several goals.
%for the system design. 
First, we want to avoid disruptive changes to hardware. The existing hardware-based solutions to manage data placement on HMS~\cite{qureshi_micro09, ibm_isca09, gpu_pcm_pact13, row_buffer_pcm_iccd12} may be difficult to be embraced by
HPC data centers, because of the concerns on hardware cost.
Second, we want to minimize changes to applications and system software.
HPC legacy applications should be easily ported to NVM-based HMS
with few programming efforts.
Third, managing data placement should be as transparent as possible.
We want to enable automatic data placement, and relieve users from
managing data placement details.

%we have hardware- and software-based solutions. 
%The hardware-based solutions introduce disruptive changes to existing hardware and system software. Such changes may result in larger investment to update existing HPC data centers On the other hand, the software-based solution . 
%We have see the success story of using algorithm knowledge~\cite{} and classifying memory access patterns for perf, energy, and programming models for data persistence

In this paper, we introduce a software-based solution to decide
and place data objects on NVM-based HMS. Using a software-based solution
to meet the above goals must address the following research challenges. 
First, how to capture and characterize memory access patterns associated with data objects? This question is important for making data placement decisions.
As we show in Section~\ref{sec:bg},
after we move some data object from %bandwidth-limited 
NVM with less memory bandwidth 
to DRAM, there is a big performance improvement.
However, we do not have such performance improvement after moving
this data object from NVM %latency-limited
with longer access latency to DRAM.
We claim such data object is sensitive to memory bandwidth.
Similarly, we find some data object which is only sensitive to memory latency, or sensitive
to both bandwidth and latency. 
%some data objects have performance of
%their data placement sensitive to memory bandwidth, while some data objects
%have performance of their data placement sensitive to memory latency. 
Characterizing data objects 
based on their sensitivity to bandwidth or latency
is critical to model and predict performance benefit of data placement. 
%Furthermore, to make our software-based solution feasible in HPC, we do not want to modify existing hardware and application, or employ offline analysis~\cite{ismm16:shen, eurosys16:dulloor} to characterize data objects. 

Second, how to strike a balance between different requirements
on the frequency of data movement (i.e., the implementation of data placement)?
On one hand, we want data movement to be frequent, %enough, 
such that data placement is adaptive to variation of memory access patterns across execution phases.
On the other hand, we want to minimize data movement to avoid performance loss.

Third, how to minimize the impact of data movement on application performance? Data movement is known to be expensive
in terms of performance and energy cost. 
Hiding data movement cost and achieving high performance is
a key to be successful in the HPC domain.

In this paper, we introduce a runtime system (named ``Unimem'') that automatically and transparently decides and implements data placement.
This runtime meets the above goals and addresses the above three challenges.
In particular, we employ online profiling based on performance counters
to capture memory access patterns for execution phases, based on which we characterize the sensitivity of data objects in each phase to memory bandwidth and latency.
This addresses the first challenge.
We further introduce lightweight performance models, based on which we predict 
performance benefit and cost if moving data objects between NVM and DRAM.
Given the performance benefit and cost of data movement, we formulate the problem of deciding optimal data placement as a a knapsack problem.
Based on the performance models and formulation, we avoid unnecessary data movement
while maximizing the benefits of data movement.
This addresses the second challenge.

To avoid the impact of data movement on application performance, we introduce a proactive data movement mechanism. Given an execution phase and a data movement plan for the phase,
this mechanism uses a helper thread to trigger data movement before the phase.
The helper thread runs in parallel with the application, overlapping data movement with application execution. This proactive data movement mechanism takes data movement overhead off the critical path, which addresses the third challenge.
To further improve performance, we introduce a series of
techniques, including (1) optimizing initial data placement to reduce data movement cost at runtime, (2) %dynamic phase delineation to 
exploring the tradeoff between phase local search and cross-phase global search for optimal data placement, and (3) decomposing large data objects to enable fine-grained data movement.
All together, those techniques in combination with our performance models
greatly narrow the performance gap between NVM and DRAM:
%The performance difference between DRAM-only and HMS with Unimem
%is 7\% on average and 13\% at most for representative HPC workloads.


In summary, we make the following contributions.
%\linebreak
%\vspace{-4pt}
\begin{itemize}
    \item We study performance of HPC workloads with large data sets on multiple nodes with various NVM bandwidth and latency, which is unprecedented. 
    Our study reveals a big performance gap between NVM-based and DRAM-based main memories. 
    We demonstrate the feasibility of using a runtime-based solution 
    %to direct data placement on HMS
    to narrow such gap for HPC.
    \item We introduce a lightweight runtime system to manage data placement without
    %the requirement of 
    hardware modifications and disruptive changes to applications and system software.
    \item We evaluate Unimem with six representative HPC workloads and one production code (Nek5000). The performance difference between DRAM-only and HMS with Unimem is only 6.2\% on average and 16\% at most. We successfully narrow the performance gap and demonstrate better performance than a state-of-the-art software-based solution.
\end{itemize}

\begin{comment}
Different GPU hetero, there is no programm challening.
But performance challenges
The challenges to use the heterogeneous memory.
(1) bandwidth vs. latency; (2) runtime overhead;
(3) must be adaptive to the application phase changes
These execution phases are sensitive to NVM higher latency or lower bandwidth or both.
Determine the placement is challenging. Runtime overhead
\\


This study has clearly demonstrated that runtime data management


The runtime provides a transparent runtime adaptation of HPC codes,
and requires only a trivial code instrumentation step.
\end{comment}

