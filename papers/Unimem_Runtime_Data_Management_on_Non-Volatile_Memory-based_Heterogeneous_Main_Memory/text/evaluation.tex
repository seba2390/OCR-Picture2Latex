\vspace*{5pt}
\section{Evaluation}
\label{sec:eva}
The goal of our evaluation is multiple-folding. 
First, we want to test if our runtime can effectively direct data placement
to narrow the performance gap between NVM and DRAM;
Second, we want to test if our runtime is lightweight enough;
Third, we want to test the performance of our runtime 
in various system configurations, including different DRAM sizes
and different system scales.
Unless indicated otherwise, performance in this section is normalized to 
that of the DRAM-only system.

\textbf{Basic performance tests.} 
We compare the performance (execution time) of DRAM-only, NVM-only, and HMS with Unimem. We use four nodes in Platform A with one MPI task per node. 
We use CLASS C as input problem for NPB benchmarks. NVM and DRAM sizes are 16GB and 256MB respectively.
Figures~\ref{fig:eval_basic_perf_half_dram_bw} and~\ref{fig:eval_basic_perf_four_dram_lat} show the results. 
NVM is configured with 1/2 DRAM bandwidth (Figure~\ref{fig:eval_basic_perf_half_dram_bw}) or 4x DRAM latency (Figure~\ref{fig:eval_basic_perf_four_dram_lat}). 

\begin{figure}[!t]
    \centering
    \includegraphics[width=0.48\textwidth, height=0.13\textheight]{figures/basic_perf_half_dram_bw_new.pdf}
     \vspace{-20pt}
    \caption{The performance (execution time) comparison between DRAM-only, NVM-only, the existing work (X-Men), and HMS with Unimem. NVM has 1/2 DRAM bandwidth.}
    \label{fig:eval_basic_perf_half_dram_bw}
    \vspace{-10pt}
\end{figure}

\begin{figure}[!t]
    \centering
    \includegraphics[width=0.48\textwidth, height=0.13\textheight]{figures/basic_perf_four_dram_lat_new.pdf}
     \vspace{-20pt}
    \caption{The performance (execution time) comparison between DRAM-only, NVM-only, the existing work (X-Men), and HMS with Unimem. NVM has 4x DRAM latency.}
    \label{fig:eval_basic_perf_four_dram_lat}
    \vspace{-10pt}
\end{figure}

\begin{figure}[!t]
    \centering
    \includegraphics[width=0.48\textwidth, height=0.13\textheight]{figures/eval_basic_op_new.pdf}
     \vspace{-20pt}
    \caption{Quantifying the contributions of our four major techniques to performance improvement.}
    \label{fig:eval_basic_op}
    \vspace{-10pt}
\end{figure}

We first notice that there is a big performance gap between NVM-only
and DRAM-only cases. On average, the gap is 18\% for NVM with 1/2 DRAM bandwidth and 47\% for NVM with 4x DRAM latency. %(see ``Overall'' in the figures). 
However, Unimem greatly narrows the gap and makes performance very
close to DRAM-only cases: the average performance difference between DRAM-only and HMS is only 3\% for NVM with 1/2 DRAM bandwidth and 7\% for NVM with 4x DRAM latency, and the performance difference is no bigger than 10\%
in all cases. This demonstrates that Unimem successfully directs
data placement for those performance-critical data objects.
This also demonstrates that Unimem is very lightweight
after we optimize runtime performance and hide data movement cost.

We compare Unimem and X-Men~\cite{eurosys16:dulloor} (a recent software-based solution for data placement in HMS). The results are shown in Figures~\ref{fig:eval_basic_perf_half_dram_bw} and~\ref{fig:eval_basic_perf_four_dram_lat}. 
X-Men uses PIN-based \textit{offline profiling} to characterize memory access patterns and make the decision on data placement. They do not consider data movement cost and assume a homogeneous memory access pattern within a data object. %static nature,
The results show that Unimem performs similarly to X-Men, but performs 10\%
better than X-Men for Nek5000. Nek5000 is a production code with various memory
access patterns across phases. Unimem adapts to those variations, hence performing better. Also Unimem does not need any offline profiling for applications.

\textbf{Detailed performance analysis}. Based on the results of basic performance tests,  we further quantify the contributions of our runtime techniques to performance improvement on HMS. This quantification study
is important to investigate how effective our techniques are and when
they can be effective.
We study four major techniques: (1) cross-phase global search,
%This technique includes using a global view to examine memory access patterns across phases and using a helper thread to overlap data movement and computation. 
(2) phase local search, 
%Using this technique, we determine data placement at the granularity of individual phase to optimize data placement.
(3) partitioning large data objects, 
%We use this technique to expose more opportunities for data placement and performance improvement.
and (4) initial data placement. 
%We use this technique to reduce data movement cost at runtime.

We apply the four techniques one by one. In particular, we apply (1),
and then apply (2) to (1), and then apply (3) to (1)+(2), and then apply (4) to (1)+(2)+(3). We measure the performance variation after
applying each technique to quantify the contribution of each technique
to performance. 
We use the same system configurations as basic performance tests with NVM 
configured with 1/2 DRAM bandwidth.
Figure~\ref{fig:eval_basic_op} shows the results.

We notice that cross-phase global search can be very effective.
In fact, in benchmarks CG and LU, 
more than 90\% of the contribution comes from this technique.
However, cross-phase global search could lose some opportunities to improve performance on individual phases, because it uses the same data placement decision on all phases. Using phase local search can complement cross-phase global search. For BT and SP, using phase local search we improve performance 
by 19\% and 5\% respectively. 

Initial data placement is very useful. In fact, it takes effect on all benchmarks. For SP, it is the most effective approach (87\% contribution comes from this technique). 
%Note that cross-phase global search and phase local search cannot work very well for SP, because the performance event collection based on the sampling mode introduces high inaccuracy to catch memory access patterns. 

Partitioning large data objects does not take effect except FT,
because it introduces very frequent data movement which lose performance.
%As a result, our runtime is not able to explore the opportunities exposed
%by partitioning large data objects except in FT.
In FT, this technique contributes to 58\% performance improvement,
while the other three techniques make 42\% contribution by manipulating
small data objects. In general, by this study we learn the importance of combining all techniques to maximize performance improvement for various HPC workloads.


\begin{table}
        \begin{center}
        \caption{Data migration details for HMS with Unimem (NVM has 1/2 DRAM bandwidth).}
        \vspace{-10pt}
        \label{tab:data_mig_details}
        \tiny
        \begin{tabular}{|p{1.2cm}|p{1.3cm}|p{2cm}|p{1.5cm}|p{0.8cm}|}
        \hline
        \textbf{Benchmark}    & \textbf{Times of Migration}  & \textbf{Migrated data size (MB)} &\textbf{Pure runtime cost} & \textbf{\% overlap}                                  \\ \hline \hline
         CG & 3 & 132 & 0.5\% & 66.7\%   \\ \hline
         FT & 4 & 201 & 1.5\% & 75\%   \\ \hline
         BT & 24 & 720 & 1\% & 87.5\%   \\ \hline 
         LU & 3 & 187 & 1\% & 60\%     \\ \hline
         SP & 9 & 348 & 1.5\% & 66.7\% \\ \hline
         MG & 1 & 17 & 2\%	& 100\%				\\ \hline
         Nek5000(eddy) & 102 & 1101 & 3\% & 70.6\% \\ \hline
        \end{tabular}
        \end{center}
        \vspace{-15pt}
\end{table}

To further study the effectiveness of Unimem, we collect some detailed data migration information for HMS with Unimem (NVM has 1/2 DRAM bandwidth). Table~\ref{tab:data_mig_details} shows the results. ``Pure runtime cost'' in the table accounts for the overhead of collecting hardware counters, modeling costs, and synchronization cost between the helper thread and main thread.
``Pure runtime cost'' does not include data movement cost and benefit. ``\% overlap'' in the table shows how many percentage of data movement cost is successfully overlapped with the computation.

From Table~\ref{tab:data_mig_details}, we notice that Unimem has very small runtime overhead (less than 3\% in all cases). Directed by Unimem, the data migration can happen very often (e.g., 102 times in Nek5000 and 24 times in BT), and the migrated data size can be very large (e.g., 1.1GB in Nek5000 and 720MB in BT). However, even with the frequent data migration, Unimem successfully overlaps data migration with computation (70.6\% in Nek5000 and 87.5\% in BT). 
Also, performance benefit of data migration outweighs those non-overlapped data migration, and narrows down the performance gap between NVM and DRAM to 9\% at most (see Figure~\ref{fig:eval_basic_perf_half_dram_bw}).

\textbf{Scalability study.} 
To study how Unimem performs in larger system scales. We did
strong scaling tests on Edison at LBNL. For each test,
we use one MPI task per node and use CLASS D as input problem.
We use 256MB for DRAM and 32GB for NVM. 
Figures~\ref{fig:strong_scaling_cg} %and~\ref{fig:strong_scaling_bt}
shows the results for CG. %and BT.
Performance (execution time) in the figures is normalized to the performance of DRAM-only.
%(strong scaling test): use Edison (LBNL); one MPI task per node; Class D;
%use normal NUMA node; 256MB DRAM

As we change the system scale, the sizes of data objects
change. The numbers of main memory accesses also change because of caching effects: Such changes in main memory accesses impact the sensitivity of data object to memory bandwidth and latency.
Because of the above changes, the runtime system must be adaptive enough
to make a good decision on data placement. 
In general, Unimem does a good job for all cases: the
performance difference between DRAM-only and HMS with Uimem
is no bigger than 7\%. 
%\textbf{Discussion: pending on big data objects.}

\begin{figure}[!t]
	\vspace*{5pt}
    \centering
    \includegraphics[width=0.48\textwidth, height=0.13\textheight]{figures/strong_scaling_cg.pdf}
    \vspace{-20pt}
    \caption{CG strong scaling tests on Edison (LBNL).}
    \label{fig:strong_scaling_cg}
    \vspace{-15pt}
\end{figure}

\begin{comment}
\begin{figure}[!t]
    \centering
    \includegraphics[width=0.5\textwidth, height=0.15\textheight]{figures/strong_scaling_bt.pdf}
    \caption{BT strong scaling tests on Edison (LBNL).}
    \label{fig:strong_scaling_bt}
    \vspace{-15pt}
\end{figure}
\end{comment}

\begin{spacing}{0.9}
\textbf{Sensitivity study.}
We use various configurations of DRAM size in HMS and 
test if our runtime can performance well. 
As DRAM size changes, we will have different opportunities 
to place data objects. 
The change of DRAM size will impact the frequency of data movement
and impact whether we should decompose large data objects to improve performance. Figure~\ref{fig:dram_size_sensitivity} shows the results
as we use 128MB, 256MB and 512MB DRAM. In all tests, we use 16GB
NVM configured with 1/2 DRAM bandwidth and CLASS C as input problem. We use Platform A and 4 nodes (1 MPI task per node) to do the tests. In the figure, performance (execution time) is normalized to that of DRAM-only.

In general, Unimem performs well in all cases except one case:
the performance difference between DRAM-only and HMS with Unimem is 
no bigger than 7\% in all cases except MG with 128MB DRAM.
For MG with 128MB DRAM, we have 13\% performance difference between
DRAM-only and HMS with Unimem.
After careful examination, we find that DRAM is not well utilized,
because large data objects cannot be placed in such small DRAM.
We also cannot partition large data objects in MG by using our compiler tool
because of widely employment of memory alias in the benchmark.
But even so, our runtime still narrows performance gap between NVM-only and DRAM-only by 35\%.
\vspace{-5pt}
\end{spacing}

\begin{figure}[!htbp]
    \centering
    \includegraphics[width=0.48\textwidth, height=0.13\textheight]{figures/dram_size_sensitivity.pdf}
    \vspace{-20pt}
    \caption{Unimem performance sensitivity to DRAM size in HMS.}
    \label{fig:dram_size_sensitivity}
    \vspace{-15pt}
\end{figure}

%\vspace{-5pt}

%change the lookahead window size; 
%(1) The impact of phase granularity (local view vs. global view): 
%4 nodes (1 task per node); class C; all 6 NPB benchmarks;
%NVM configuration: (use 1/2 DRAM bandwidth)\\

%(2) The impact of DRAM size: 4 nodes (1 task per node); class C; all 6 NPB %benchmarks; Use 1/2 of DRAM size in Figures 6-7; Use 2X of DRAM size in Figures %6-7; \\

%(3) t1 and t2 thresholds: pending




