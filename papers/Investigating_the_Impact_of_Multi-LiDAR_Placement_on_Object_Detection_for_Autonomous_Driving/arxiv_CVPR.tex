% CVPR 2022 Paper Template
% based on the CVPR template provided by Ming-Ming Cheng (https://github.com/MCG-NKU/CVPR_Template)
% modified and extended by Stefan Roth (stefan.roth@NOSPAMtu-darmstadt.de)

\documentclass[10pt,twocolumn,letterpaper]{article}

%%%%%%%%% PAPER TYPE  - PLEASE UPDATE FOR FINAL VERSION
% \usepackage[review]{cvpr}      % To produce the REVIEW version
% \usepackage{cvpr}              % To produce the CAMERA-READY version
\usepackage[pagenumbers]{cvpr} % To force page numbers, e.g. for an arXiv version

% Include other packages here, before hyperref.
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{booktabs}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{color,soul}
\usepackage{graphicx}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{multirow}
\usepackage{makecell}
\usepackage[ruled,vlined]{algorithm2e}
\usepackage{silence}
\usepackage{multirow}
\usepackage{float}
\usepackage{makecell}
% \usepackage{bbold}
\usepackage{bbm}
\usepackage{amsmath, amssymb, mathtools}
\usepackage[accsupp]{axessibility}

% It is strongly recommended to use hyperref, especially for the review version.
% hyperref with option pagebackref eases the reviewers' job.
% Please disable hyperref *only* if you encounter grave issues, e.g. with the
% file validation for the camera-ready version.
%
% If you comment hyperref and then uncomment it, you should delete
% ReviewTempalte.aux before re-running LaTeX.
% (Or just hit 'q' on the first LaTeX run, let it finish, and you
%  should be clear).
\usepackage[pagebackref,breaklinks,colorlinks]{hyperref}


% Support for easy cross-referencing
\usepackage[capitalize]{cleveref}
\crefname{section}{Sec.}{Secs.}
\Crefname{section}{Section}{Sections}
\Crefname{table}{Table}{Tables}
\crefname{table}{Tab.}{Tabs.}


%%%%%%%%% PAPER ID  - PLEASE UPDATE
\def\cvprPaperID{9541} % *** Enter the CVPR Paper ID here
\def\confName{CVPR}
\def\confYear{2022}

\newcommand{\todo}[1]{\hl{[#1]}}
\makeatletter
\newcommand{\printfnsymbol}[1]{%
  \textsuperscript{\@fnsymbol{#1}}%
}
\newcommand{\printfnsymbolnew}[2]{%
  \textsuperscript{\@fnsymbol{#1}}%
}
\makeatother

\begin{document}

%%%%%%%%% TITLE - PLEASE UPDATE
%
% \title{Improving Perception via Sensor Placement:\\ Designing Multi-LiDAR Systems for Autonomous Vehicles}

\title{Investigating the Impact of Multi-LiDAR Placement  on \\ Object Detection   for Autonomous Driving}

\author{Hanjiang Hu$^{1}$\thanks{equal contribution}\quad  Zuxin Liu$^{1}$\printfnsymbol{1}\,\, Sharad Chitlangia$^{2}$\thanks{work done while interning at CMU, prior to current affiliations}\quad  Akhil Agnihotri$^{3}$\printfnsymbolnew{2}\,\, Ding Zhao$^{1}$\\
$^1$Carnegie Mellon University\quad   $^2$Amazon\quad  $^3$University of Southern California
\\ {\tt\small \{hanjianghu,zuxinl,dingzhao\}@cmu.edu, chitshar@amazon.com, akhil.agnihotri@usc.edu}
% \\
% Institution1 address\\
% {\tt\small firstauthor@i1.org}
% For a paper whose authors are all at the same institution,
% omit the following lines up until the closing ``}''.
% Additional authors and addresses can be added with ``\and'',
% just like the second author.
% To save space, use either the email address or home page, not both
% \and
% Zuxin Liu\\
% Carnegie Mellon University
% \\
% First line of institution2 address\\
% {\tt\small secondauthor@i2.org akhil.agnihotri@usc.edu sharadchitlangia24sc@gmail.com}
}
\maketitle

%%%%%%%%% ABSTRACT
\begin{abstract}
% \todo{todo}
   The past few years have witnessed an increasing interest in improving the perception performance of LiDARs on autonomous vehicles. While most of the existing works focus on developing new deep learning algorithms or model architectures, we study the problem from the physical design perspective, \textit{i.e.}, how different placements of multiple LiDARs influence the learning-based perception. To this end, we introduce an easy-to-compute information-theoretic surrogate metric to  quantitatively and fast evaluate LiDAR placement for 3D detection of different types of objects. We also present a new data collection, detection model training and evaluation framework in the realistic CARLA simulator to evaluate disparate multi-LiDAR configurations. Using several prevalent placements inspired by the designs of self-driving companies, we show the correlation between our surrogate metric and object detection performance of different representative algorithms on KITTI through extensive experiments,
   validating the effectiveness of our LiDAR placement evaluation  approach.
   Our results show that sensor placement is non-negligible in 3D point cloud-based object detection, which will contribute up to $10\%$ performance discrepancy in terms of average precision in challenging 3D object detection settings.
%   , and we analyze the performance with variance of LiDAR placement and scenario object density.
   We believe that this is one of the first studies to quantitatively investigate the influence of LiDAR placement on perception performance.
\end{abstract}

%%%%%%%%% BODY TEXT
\section{Introduction}
\label{sec:intro}
LiDAR sensors are becoming the critical 3D sensors for autonomous vehicles (AVs) since they could provide accurate 3D geometry information and precise distance measures under various driving conditions \cite{liu2019lpd, 9635878}. The point cloud data generated from LiDARs has been used to perform a series of perception tasks, such as object detection and tracking \cite{shi2021pv, shi2020points, shi2019pointrcnn}, SLAM and localization \cite{zhang2014loam,lu2019deepvcp,yu2018ds}.
% Unlike the RGB image data from camera sensors, point cloud data is irregular and sparse, which brings great challenges for the 3D object detection task.
\begin{figure}[h]
\begin{center}
% \fbox{\rule{0pt}{2in} \rule{0.9\linewidth}{0pt}}
    \begin{subfigure}[b]{0.22\textwidth}
                \includegraphics[width=\linewidth]{images/intro/toyota.jpg}
                \caption{Toyota}
                \label{fig:toyota}
    \end{subfigure}
    \begin{subfigure}[b]{0.22\textwidth}
                \includegraphics[width=\linewidth]{images/intro/cruise.jpg}
                \caption{Cruise}
                \label{fig:cruise}
    \end{subfigure}

    \begin{subfigure}[b]{0.22\textwidth}
                \includegraphics[width=\linewidth]{images/intro/ponyai.png}
                \caption{Pony.ai}
                \label{fig:ponyai}
    \end{subfigure}
    \begin{subfigure}[b]{0.22\textwidth}
                \includegraphics[width=\linewidth,height=0.5\linewidth]{images/intro/argoai.jpg}
                \caption{Argo AI}
                \label{fig:argoai}
    \end{subfigure}%

    \begin{subfigure}[b]{0.22\textwidth}
                \includegraphics[width=\linewidth,height=0.61\linewidth]{images/intro/um-perl.jpg}
                \caption{Ford}
                \label{fig:ford}
    \end{subfigure}
  %
    \begin{subfigure}[b]{0.22\textwidth}
                \includegraphics[width=\linewidth,height=0.61\linewidth]{images/intro/zoox.png}
                \caption{Zoox}
                \label{fig:zoox}
    \end{subfigure}
\end{center}
\vspace{-8mm}
  \caption{Different multi-LiDAR configurations used in different autonomous vehicles \cite{toyota,cruise,ponyai,argoai,ford,zoox}.}
\label{fig:baseline-config}
\vspace{-6mm}
\end{figure}


High-quality point cloud data  and effective perception algorithms are usually both required to achieve accurate 3D object detection in practice \cite{xu2021spg}. A number of studies propose to improve the 3D object detection performance for point cloud data by developing novel perception algorithms, which assume that the data is of high quality~\cite{shi2020points,shi2019pointrcnn, lang2019pointpillars, yan2018second, deng2020voxel}. However, only a little literature considers the LiDAR perception problem from the viewpoint of data acquisition \cite{liu2019should,kini2020sensor,ma2021perception} and LiDAR placement. We believe that this new perspective should be equally crucial for real-world AV applications since improper LiDAR placement may cause poor-quality sensing data, and thus corrupt the perception algorithm and lead to poor performance \cite{cao2019adversarial,liu2019extending, xu2021spg, kini2020sensor,liu2019should,mou2018optimal}. In addition, LiDAR is an expensive sensor, so maximizing its utility to save the deployment cost is also important for mass production.
Therefore, we aim to investigate the interplay between LiDAR sensor placement and perception performance for AVs. We use placement and configuration interchangeably throughout this paper.

% We are interested in this topic for several reasons. On the one hand, given the perception algorithms, we want to evaluate the efficacy of different LiDAR placement layouts based on the perception performance of different types of target object. On the other hand, the LiDAR placement may affect the sensing procedure and the point cloud input for the perception algorithm, which influences the perception quality \cite{kini2020sensor,liu2019should,mou2018optimal}.

However, it is not easy to evaluate the efficacy of different LiDAR placement layouts based on the perception performance in the real world, which comes with effort-costly and time-consuming full loops of LiDAR deployment, data collection, model training, and performance evaluation.
Moreover, as shown in Figure \ref{fig:baseline-config}, many companies' self-driving vehicles are equipped with more than 2 LiDARs. As the LiDAR number increases, the cost of LiDAR configuration evaluation and optimization will also increase exponentially.
Therefore, it is a crucial but still open problem to accelerate the quantitative evaluation of the LiDAR configurations regarding perception performance with low cost. Thoroughly studying the interplay between LiDAR sensor placement and perception performance is essential to AV perception systems, saving deployment cost and without sacrificing driving safety.
%efficiently evaluate LiDAR configurations,
% obtain the right balance between the perception performance and affordability of AV perception systems, saving deployment cost and without sacrificing driving safety.
% LiDAR is an expensive sensor, so if sensor placement can be evaluated efficiently, it would be beneficial to minimize the LiDAR sensor deployment cost.
%  As shown in Figure \ref{fig:baseline-config}, many companies' self-driving vehicles are equipped with more than 2 LiDARs for more sensing information, but the deployment cost would be high. Moreover, LiDAR is an expensive sensor, so if sensor placement can be evaluated efficiently, it would be beneficial to minimize the LiDAR sensor deployment cost.
% Different LiDAR configurations may not have the same perception performance on different target objects or scenarios with different densities given current perception algorithms,

% % Decreasing the number of LiDAR sensors or the number of laser beams may affect the perception system,
% but how to efficiently quantify and evaluate such influence of LiDAR placements on perception performance
% % and could we mitigate this loss by optimal LiDAR placement
% are questions that are rarely studied in the literature. Therefore, thoroughly studying the interplay between LiDAR sensor placement and perception performance is an essential bridge to obtain the right balance between the perception performance and affordability of AV perception systems, saving deployment cost and without sacrificing driving safety.
%Specifically, we aim to address the following questions: 1) Does LiDAR placement influence the perception performance

%and developing a general framework to maximize the perception capability by optimal LiDAR placement.
In this paper, we study the perception system from the sensing perspective and focus on investigating the relationship between LiDAR configurations and 3D object detection using our evaluation framework shown in Figure \ref{fig:overview}.
% To the best of our knowledge, we are one of the first works to quantitatively study how LiDAR placement affects the 3D object detection performance.
% in a realistic simulation environment.
The contributions of this paper are summarized as follows:
    % \item We propose and validate a efficient probabilistic surrogate metric to make the optimization of LiDAR placement computationally feasible and evaluate their accuracies with comprehensive tests.
        % \item We conduct extensive ** hours experiments in realistic simulation with top perceptions algorithms ranked by KITTI datasets. We use four prevalent LiDAR placement designs inspired by top self-driving companies as benchmarks and studied detection accuracies with both cars and cyclists. All our code and simulation data will be released to the public to facilitate future studies.
\begin{itemize}
    % , which is heavily grounded in realistic traffic scenarios for strong reproducibility in AV research.
    %The framework is built on top of a well-accepted open-source urban self-driving simulator Carla \cite{dosovitskiy2017carla}, which is useful for AV research.
    \item We establish a systematic framework to evaluate the object detection performance of different LiDAR placements and investigate prevalent LiDAR placements inspired by self-driving companies, showing that LiDAR placement dramatically influences the performance of object detection up to 10\% in challenging 3D detection settings. As far as we know, we are one of the first works to quantitatively study the interplay between LiDAR placement and perception performance.

    \item We propose a novel surrogate metric with maximum information gain (S-MIG) to accelerate the evaluation of LiDAR placement by modeling object distribution through the proposed scalable Probabilistic Occupancy Grids (POG). We show the correlation between the surrogate metric and detection performance, which is theoretically explained via S-MIG and validates its effectiveness for the LiDAR configuration evaluation.
    % the LiDAR placement influence on perception performance
    % , which explains why LiDAR placement influences perception performance  based on information theory.
    % The surrogate cost is easy-to-compute and could greatly accelerate the LiDAR sensor placement optimization procedure.

    \item  We contribute an automated multi-LiDAR data collection and detection model evaluation pipeline in the realistic CARLA simulator and conduct extensive experiments with state-of-the-art LiDAR-based 3D object detection algorithms. The results reveal that LiDAR placement plays an important role in the perception system. The code for the framework is available on \url{https://github.com/HanjiangHu/Multi-LiDAR-Placement-for-3D-Detection}.
    % We will release the code and data as a public evaluation platform to facilitate future studies.

    % show that the surrogate metrics are highly correlated with detection performance.

    % in the proposed framework and validate the effectiveness of surrogate metrics through extensive experiments with variance of placements and scenarios.
    % give insightful hints for future applications under scenarios with different densities of target objects.
    %  examples which could aid in more effective deployment of LiDAR sensors on AVs.
\end{itemize}

% The remainder of the paper is structured as follows: Section \ref{related-work} provides a brief overview of related work and suggests the research gap in the existing literature. Section \ref{method} formulates the LiDAR placement problem and introduces the surrogate metrics. We present our data collection and evaluation framework along with extensive experimental results and analysis in Section \ref{experiments}. Finally,  Section~\ref{conclusion} concludes with a summary of our contributions and directions for future research.




\section{Related Work}

% \todo{I suggest move this part to the end:
% 1. it cuts the reading flow; 2. it may raise unnecessary questions on the comments of lidar detection - there are many new progresses that we did not review. Also, after we moved it back, when reviewing the LiDAR-based 3D object detetion algorithms, we could focus on the algorithms we used in our research and explain why we chose them. It will make the paper more coherent.}

\label{related-work}
The methods and frameworks proposed in this work revolve around evaluating the LiDAR sensor placement for point cloud-based 3D object detection in autonomous driving. Although the literature is scarce in this combined area, there has been some research on the 3D object detection and the LiDAR placement optimization topics independently, which we discuss in this section.

\textbf{LiDAR-based 3D object detection.} To keep up with the surge  in the LiDAR applications in autonomous vehicles, researchers have tried to develop novel point cloud-based 3D object detection from LiDAR. Over the years, there has also been great progress in grid-based and point-based 3D detection methods for point cloud data. 2D grid-based methods project the point cloud data onto a 2D bird-eye-view (BEV) to generate bounding boxes \cite{chen2017multi, liang2019multi, vora2020pointpainting, vora2020pointpainting, Yin_2021_CVPR, lang2019pointpillars}. Another way to deal with point cloud is to use 3D voxels and 3D CNN, which is also called voxel-based methods \cite{zhou2018voxelnet,  yan2018second, shi2020points, graham20183d, deng2020voxel}. However, these grid-based methods are greatly limited by the kernel size of the 2D or 3D convolution, although the region proposal is pretty accurate with fast detection speed. Point-based methods, on the other hand, directly apply on the raw point cloud based on PointNet series \cite{qi2017pointnet, qi2017pointnet++, qi2018frustum}, which enables them to have flexible perception leading to robust point cloud learning \cite{wang2019dynamic,yang2019std, qi2019deep, yang20203dssd}.  More receptive fields bring about higher computation costs compared to grid-based methods. Recent work  \cite{shi2020pv, shi2021pv} combine voxel-based and point-based methods together to efficiently learn features from raw point cloud data, leading to impressive results on KITTI dataset \cite{geiger2013vision}. To evaluate the influence of point cloud of LiDAR configuration, we use some representative voxel-based and point-based methods \cite{openpcdet2020} in Section \ref{experiments}. The majority of the detection methods mentioned above are well-designed and evaluated on high-quality point cloud datasets, but do not consider the influence of the LiDAR sensing system.

% To overcome this, a highly ubiquitous grid-based sliding window approach has also been studied with respect to point cloud detection \cite{wang2015voting,du2020spot}.
% It relies on a popular voting scheme which accelerates the exhaustive 3D window searching.
% However, it is only efficient for sparse data points, similar to the framework by Su \etal \cite{su2018splatnet}, and couldnot be scaled to a dense point cloud, as observed in majority of AV applications.

% we adopt the recent open-source LiDAR-based 3D object detection framework \textit{OpenPCDet} \cite{openpcdet2020} and fine-tune the KITTI pretrained models of multiple representative 3D detection algorithms. For voxel-based methods, we use one-stage SECOND \cite{yan2018second} and two-stage Voxel RCNN  \cite{deng2020voxel}. Also, we use PointRCNN \cite{Shi_2019_CVPR} as point-based method and PV-RCNN  \cite{shi2020pv} as an integration of voxel-based and point-based methods. Moreover, we also compare the models with 3D Intersection-over-Union (IoU) loss for SECOND \cite{yan2018second} and PointRCNN \cite{Shi_2019_CVPR}.




% Therefore, to take advantage of lower computation of grid based methods and higher sensing capabilities of point based methods, Shi \etal \cite{shi2020pv} propose a robust framework, PointVoxel-RCNN (PV-RCNN), to unify the two methods. PV-RCNN utilizes a mixture of voxel Convolutional Neural Network (CNN) and PointNet-based set abstraction to efficiently learn features from raw point cloud data. Compared to conventional point-based methods, this framework provides for richer context information leading to a more accurate estimation of objects' positions and superior performance on the standard KITTI dataset \cite{geiger2013vision}. More details are provided in Section \ref{experiments}.

% \begin{figure}[t!]
% \begin{center}
%  \includegraphics[width=\linewidth]{images/pc-to-pog.pdf}
% \end{center}
% \vspace*{-5mm}
%   \caption{Schematic showing how 3D bounding boxes (Bboxes) are mapped to the ROI to generate a POG. Each cube in the POG has a probability given by Equation {\ref{prob-equation}}.}
% \label{fig:pc-to-pog}
% \vspace{-4mm}
% \end{figure}

\begin{figure*}[t!]
\begin{center}
 \includegraphics[width=0.8\linewidth]{images/overview.pdf}
\end{center}
\vspace*{-5mm}
  \caption{Evaluation framework overview.}
\label{fig:overview}
\vspace*{-5mm}
\end{figure*}


\textbf{LiDAR placement for autonomous vehicles.}
One of the critical factors for autonomous vehicles is the perception and sensing ability. With this respect, LiDARs have been widely used because of their high real-time precision and their ability to extract rich  information from the environment \cite{maddern2014illumination, geiger2013vision, zhang2014loam}.  However, the perception ability of the LiDAR is sensitive to its placement \cite{zhang1995two, durrant1987consistent}, so it is critical to developing a scheme that minimizes the uncertainty among all its possible placements.
To this extent, Dybedal \etal \cite{dybedal2017optimal} proposed to find the optimal placement of 3D sensors using a Mixed Integer Linear Programming approach, which is not scalable to AVs because of the large number of variables involved. Rahimian \etal  \cite{rahimian2016optimal} developed a dynamic occlusion-based optimal placement routine for 3D motion capture systems, but do not consider variable number of sensors during its optimization routine.

There have also been some prominent advances to optimize the placement of multiple LiDARs for AV while considering the perception performance. Mou \etal \cite{mou2018optimal} formulated a min-max optimization problem for LiDAR placement with a cylinder-based cost function proxy to consider the worst non-detectable inscribed spheres formed by the intersection of different laser beams.
% However, their Mixed Integer Linear Programming solver could hardly be applied to a large number of LiDARs or laser beams because of its exponential computational complexity, and moreover, their optimization precision is limited.
Liu \etal \cite{liu2019should} improved the previous work by using an intuitive volume to surface area (VSR) ratio metric and a black-box heuristic-based optimization method to find the optimal LiDAR placement. These methods assume a uniformly weighted region around the AV to minimize the maximum non-detectable area. They do not explicitly reveal the relation between the LiDAR placement and the perception performance. Recent similar work \cite{ma2021perception} proposed the perception entropy metric for multiple sensor configuration evaluation. Although they use conditional entropy to measure the sensor perception, they rely on empirical assumptions from KITTI dataset in the formulation of perception entropy, which weakens the generalizability to other scenes. We formulate the problem under a full probabilistic framework using a data-driven surrogate metric with a ray-tracing acceleration approach to overcome these limitations.
% Furthermore, we combine the two fields - 3D object detection and LiDAR placement - together and propose a systematic framework to quantitatively describe the relation between multi-LiDAR sensing and object detection.

% While their approach aims to minimize the maximum non-detectable area, however, they assume a uniformly weighted region around the AV. In addition, they do not explicitly reveal the relation between the LiDAR placement and the perception performance, which could be done in realistic simulation environment or by real-world testing.

% Our work overcomes these limitations of previous LiDAR placement works by utilizing  data-driven surrogate metrics with a ray-tracing acceleration approach.

% Since different cities may have totally different road infrastructure layouts and traffic patterns, we aim to optimize the LiDAR configurations accordingly by analysing collected data and maximizing the information gain so that the overall perception capability could be improved.

% Furthermore, we combine the two fields - 3D object detection and LiDAR placement - together and propose a systematic framework to quantitatively describe the relation between LiDAR sensing and perception.



% \vspace{-0.5cm}

\section{Methodology}
\label{method}
% \todo{Add a paragraph on the challenges of optimization before introducing the surrogate model: maybe estimate the original computational loads and very non-convex optimization terrain.}
To avoid the huge effort of data collection and complicated analysis of learning-based detection  for evaluating each multi-LiDAR placement, we introduce the methodology to efficiently evaluate LiDAR sensor placement for perception performance based on information theory. The problem of LiDAR placement  evaluation is formulated  with the definitions of Region of Interest (ROI) and Probabilistic Occupancy Grid (POG), by introducing a probabilistic surrogate metric for fast LiDAR configuration evaluation.

\subsection{Problem Formulation}
\label{problem-formulation}
% Introduce LiDAR sensor and the sensor model (cone), introduce perception evaluation metric (mAP), and how do we register different LiDARs' point cloud.

We begin this section by defining the LiDAR perception model and the ROI, which form the basis of our LiDAR configuration evaluation problem.
As shown in Figure \ref{fig:lidar-schematic}, we model a LiDAR sensor as a collection of multiple beams. Each beam owns a pitch angle to the $XY$ plane and rotates at a uniform speed along the positive $Z$ axis. As each beam completes one rotation, it forms a conical surface area, and we assume its perception to be all points in this area.  Thus, the total perception of a LiDAR is the union of all these conical areas formed by the rotation of its beams in the ROI, which we will describe next.

Similar to previous work \cite{liu2019should} and \cite{mou2018optimal}, we define the ROI to be the space where we keep track of objects to be detected. To account for LiDAR's limited range of detection, we denote the cuboid size of the ROI to be $[l, w, h]$ for length $l$ along the x-axis, width $w$ along y-axis and height $h$ along the z-axis in the $XYZ$ coordinate system, as shown in Figure \ref{fig:roi-lidar}. The size of ROI is mainly determined by the distribution of target objects (Car, Truck, Cyclist, \textit{etc.}) in the scenario.
% The ROI width (Y axis) is shorter than length (X axis) because an AV's longitude velocity is usually lesser than its latitude velocity and thus we want the AV to see further along $X$ axis.
We then discretize the ROI into voxels with a fixed resolution $\delta$ to represent ROI as a collection of voxels,
\begin{equation}
\label{ROI_voxels}
\mathcal{V}=\{v_1, v_2, \dots, v_M\}, M=\frac{l}{\delta}\times\frac{w}{\delta}\times\frac{h}{\delta}
\end{equation}
%  For instance, we use cubes of side $0.05$ meters in the experiments, which results in a collection of $\frac{60}{0.05}\times\frac{20}{0.05} \times \frac{4}{0.05} = 38400000$ cubes to represent our ROI, which is denoted by $\{c_1,c_2,...,c_{38400000}\}$.
 The ROI provides us with a fixed perception field around the LiDAR, from which LiDAR beams accumulate most sensing information for the following perception task.
%  Similar LiDAR sensor modeling and ROI definition could also be found in Liu \etal \cite{liu2019should} and Mou \etal \cite{mou2018optimal}.

Therefore, for the LiDAR-based 3D object detection task, we only focus on the objects within ROI for each point cloud frame when calculating the perception metrics.
Then, the problem of LiDAR placement evaluation is formulated as comparing the object detection performance given LiDAR configuration candidate using common point cloud-based 3D object detection algorithms and their metrics \cite{geiger2013vision}. However, directly using 3D object detection metrics to evaluate the LiDAR placement in the real world is inaccurate and extremely inefficient, as it is impossible to make all the scenarios and objects identical to collect point cloud data fairly for evaluated LiDAR configuration candidate in the practical application. Besides, it may take days for each evaluation procedure to collect new data and train the detection models based on new LiDAR placement to get the final detection metrics. Therefore, we propose a new surrogate metric to accelerate the LiDAR placement evaluation procedure based on the Probabilistic Occupancy Grid.
% \todo{This part is important, could you give a numerical estimation of the computational cost and convince the readers that solving the problem needs astronomical efforts therefore designing an effective surrogate model is a key step. Make this to be a separate section or at least a separate paragraph with bold font to highlight it and mention it at the beginning in the overview of this section.}


% such that the cost function is minimized in the ROI.
% To evaluate the perception performance of a LiDAR placement, three commonly used metrics in 3-D Object Detection --- Bird-eye view (BEV) detection, 3-D Intersection Over Union and Orientation Similarity --- are used.  The details of those metrics could be found in Kitti dataset \cite{geiger2013vision}.
% % mAP
\begin{figure}[t!]
\begin{center}
 \includegraphics[width=\linewidth,height=0.4\linewidth]{images/lidar-schematic.pdf}
\end{center}
\vspace*{-5mm}
  \caption{Schematic showing a LiDAR sensor forming perception cones in the ROI to collect point cloud data.}
\label{fig:lidar-schematic}
\end{figure}

\begin{figure}[t!]
\begin{center}
 \includegraphics[width=\linewidth,height=0.65\linewidth]{images/roi-lidar.pdf}
\end{center}
\vspace*{-12mm}
  \caption{LiDAR sensor mounted on an AV samples object voxels according to 3D bounding boxes from the ROI and generate the POG to evaluate detection performance. }
\label{fig:roi-lidar}
\vspace{-4mm}
\end{figure}




\subsection{Probabilistic Occupancy Grid}
% Aim to maximize the information gain, which is represented by our surrogate cost function.
Since we consider the 3D object detection in ROI among all the frames, intuitively, the LiDAR configurations covering more objects will perform better in the object detection task. To this end, we propose to model the joint distribution of voxels in ROI as the Probabilistic Occupancy Grid (POG) by estimating the probability of each voxel to be occupied. For each object of interest, like car, truck and cyclist, the POG is defined as the joint probability of occupied voxels by 3D bounding boxes (BBoxes) among all the frames from $M$ voxels in ROI. Similarly, let the LiDAR configuration be a random variable $C$, given LiDAR configuration $C=C_0$, conditional POG can represent the conditional joint distribution of occupied voxels given the specific LiDAR configuration with the assumption of conditional independence.
% \begin{equation}
\begin{align}
\label{pog}
p_{POG} &= p(v_1, v_2,\dots,v_M)  \\
\label{pog_C}
p_{POG \mid C=C_0} & = p(v_1, v_2,\dots,v_M \mid C=C_0)
% M=\frac{l}{\delta}\times\frac{w}{\delta}\times\frac{h}{\delta}
\end{align}
% \end{equation}
where $v_i \sim  p_{\mathcal{V}}$. To make the notation compact and easy to read, we denote the occupied voxel random variable   $v_i \mid C=C_0  $  as $v_i^{C_0}$ and denote  the conditional distribution $p_{\mathcal{V} \mid C=C_0}$ as $p_{\mathcal{V} \mid C_0}$, so we have $, v_i^{C_0}  \sim p_{\mathcal{V} \mid C_0}$.
\begin{figure}[t!]
\begin{center}
 \includegraphics[width=\linewidth]{images/pc-to-pog.pdf}
\end{center}
\vspace*{-8mm}
  \caption{Schematic showing how 3D bounding boxes (Bboxes) are mapped to the ROI to generate a POG. Each cube in the POG has a probability given by Equation {\ref{prob-equation}}.}
\label{fig:pc-to-pog}
\vspace{-13mm}
\end{figure}

To estimate POG from samples, given a dataset of the ground truth of BBoxes $\mathcal{Y}_{T}=\{y_1, y_2,\dots, y_T\}$, where $T$ represents the number of ground truth frames. Each frame $y_t (t\in\{1,...,T\})$ contains $N^{(t)}$ 3D BBoxes of the target object $\{b^{(t)}_1, b^{(t)}_2, \dots, b^{(t)}_{N^{(t)}}\}$ within ROI of the ego-vehicle, and each BBox is parameterized by its center coordinates, size (length, width, height), and yaw orientation. For each voxel coordinate $v_i=(x_i, y_i, z_i) \in \mathcal{V}$, we denote $v_i \in y_t$ if $v_i$ is within any of the bounding boxes $\{b^{(t)}_1, b^{(t)}_2, \dots, b^{(t)}_{N^{(t)}}\}$. Then, for each voxel $v_i \in \mathcal{V}$ from ROI, we estimate its probability to be occupied among all the $T$ frames as,
\begin{align}
\label{prob-equation}
\hat{p}(v_i) = \frac{ \sum_{t=1}^T \mathbbm{1}(v_i \in y_t) }{T},& i = 1,2, \dots, M \\
v_i \in y_t := \exists ~ b^{(t)} \in y_t, & s.t. ~ v_i \in b^{(t)}
% M=\frac{l}{\delta}\times\frac{w}{\delta}\times\frac{h}{\delta}
\end{align}
where $\mathbbm{1}(\cdot)$ is an indicator function and $M$ is the number of voxels in ROI. The POG can then be estimated by the joint probability of all occupied voxels in ROI. Since the presence of an object in one voxel does not imply presence in other voxels among all the frames in ROI, we could treat these voxels as independent and identically distributed random variables and calculate the joint distribution over all voxels in the set $\mathcal{V}$ as,
\begin{equation}
\label{all_pog_est}
\hat p_{POG} = \hat p(v_1,\dots,v_M) = \prod_{i=1, \hat p(v_i)\neq 0}^{M} \hat p(v_i)
\end{equation}
where $M$ is the total number of non-zero voxels in ROI. One such example of the POG of car is shown in Figure \ref{fig:pc-to-pog}. Note that notations of $\hat p$ with $\hat{hat}$ are the estimated distribution from observed samples, while notations of $ p$ without \textit{hat}  are  the unknown non-random true distribution to be estimated.

% if at a certain frame, a \textit{cube} $c_i$ is occupied by an object, its weight is incremented by one. The final probability $P(\cdot)$ that an object is present in \textit{cube} $c_i$ is its final weight divided by the total number of frames i.e.,

% \begin{equation}
%     P(c_i) = \frac{Weight\ of\ cube\ {c_i}}{Total\ Number\ of\ Frames}
% \end{equation}


For a particular LiDAR configuration $C=C_0$, we employ ray tracing by Bresenham's Line Algorithm \cite{5388473} (denoted as $Bresenham(\mathcal{V},C_0)$) to find all the voxels which intersect with the perception field (ROI) of that LiDAR. We denote the set of these beam-intersected voxels as,
\begin{equation}
\label{bresenham_C}
\mathcal{V}| C_0 = Bresenham(\mathcal{V},C_0) = \{v_1^{C_0}, \dots, v_{M}^{C_0}\}
\end{equation}
% where $M_{C_0}$ is the number of intersected voxels from all the beams in the ROI given LiDAR configuration $C_0$.
where the discretized non-zero voxels from LiDAR beams  give the perception range of LiDAR configuration $C_0$.


The conditional probability of occupied voxels among all frames in ROI given the LiDAR configuration $C_0$ can represent the conditional distribution  from which LiDAR can  get sensing information of the target object. Similar to Equation \ref{all_pog_est},  the conditional POG given LiDAR configuration $C_0$ can be estimated as,
\begin{equation}
\label{all_pog_C_est}
\hat p_{POG \mid C_0} = \hat p(v_1^{C_0}, \dots, v_{M}^{C_0}) = \prod_{i=1, \hat p(v^{C_0}_i)\neq 0}^{M} \hat p(v^{C_0}_i)
    % P(S_C) = P(s_1,s_2,...,s_n) = \prod_{k=1}^{n} P(s_k)
\end{equation}

From the perspective for density estimation to find POG, combine Equation \ref{pog}, \ref{pog_C},  \ref{all_pog_est} and \ref{all_pog_C_est}, the true POG and conditional POG given  configuration $C_0$ can be estimated as,
\begin{align}
% \label{bridge_pog}
p_{POG} =  \hat p_{POG}, ~
% p(v_1,...,v_M)  = \mathbb{E}_{v_i \sim p_{\mathcal{V}}}\prod_{i=1}^{M} \hat p(v_i) \\
\label{bridge_pog_C}
p_{POG |C=C_0}  = \hat p_{POG |C_0}
% p(v_1,...,v_M | C_0) = \mathbb{E}_{v^C_i \sim p_{\mathcal{V}\mid C}}\prod_{i=1}^{M_C} \hat p(v^{C}_i)
% M=\frac{l}{\delta}\times\frac{w}{\delta}\times\frac{h}{\delta}
\end{align}


% Probabilistic grid, information gain/entropy

\subsection{Probabilistic Surrogate Metric}
In this section, we derive our surrogate metric based on information theory using the POG $p(v_1,...,v_M)$ and conditional POG $p(v_1,...,v_M\mid C = C_0)$ given LiDAR configuration $C_0$,  evaluating how well the LiDAR placement can sense objects' location in ROI.
% and reducing the uncertainty of the target objects.

% \textbf{Surrogate metric based on Maximum Likelihood Estimation (S-MLE).} Suppose all the voxels with target objects are observed as POG from the true but unknown LiDAR configuration, our goal is to evaluate how likely the POG can be observed through current LiDAR configuration candidate. One similar case in density estimation is Maximum Likelihood Estimation (MLE), which estimates the parameters as the ones that can have the maximal likelihood of the observed data. Inspired by MLE, we can use the  likelihood of the conditional POG given LiDAR configuration as a surrogate metric, called \textit{S-MLE}, so the LiDAR palcements $C$ with higher surrogate function values are supposed to have more chance to observe the voxels occupied by target objects in POG, \textit{i.e.}. they can focus on sensing the area with higher probability of target objects.
% \begin{align}
% % \label{bridge_pog}
% % p_{POG} &=  p(v_1,...,v_M) \simeq \hat p_{POG} = \prod_{i=1}^{M} \hat p(v_i) \\
% \label{SurM_MLE}
% Surr_{MLE} = \log p_{\mathcal{V}\mid C}
% % \mathbb{E}_{v^C_i \sim p_{\mathcal{V}\mid C}}\log \hat p(v^C_1,v^C_2,...,v^C_M | C) \\
% = \mathbb{E}_{v^C_i \sim p_{\mathcal{V}\mid C}}\sum_{i=1}^{M_C} \log \hat p(v^{C}_i)
% % M=\frac{l}{\delta}\times\frac{w}{\delta}\times\frac{h}{\delta}
% \end{align}
% where we only consider non-zero values and use the logarithm trick to make it easier to compute as MLE does.

% \textbf{Surrogate metric based on Maximum Information Gain (S-MIG).}
% \subsubsection{Maximum Likelihood Estimation Surrogate Metric}
% Based on the POG and conditional POG given LiDAR configuration,
To maximize the perception capability, one intuitive way is to reduce the uncertainty of the joint distribution of voxels given specific LiDAR configuration. The total entropy of POG (\textbf{POG Entropy}) is only determined by the scenarios with bounding boxes.
\begin{align}
\label{totla_entropy}
H_{POG} = H(\mathcal{V}) =  \mathbb{E}_{v_i \sim p_{\mathcal{V}}} \sum_{i=1}^{M} \hat H(v_i)
% M=\frac{l}{\delta}\times\frac{w}{\delta}\times\frac{h}{\delta}
\end{align}
Mutual information (MI) of two random variables can represent uncertainty reduction given the condition. In this case, we  want to evaluate the information gain (\textbf{IG}) of voxels occupied by target objects given the LiDAR configuration candidate. Our insight is that the more IG the LiDAR configuration has, the more information it will contain so the more uncertainty it will reduce. With  $p(C=C_0) = 1$,
\begin{align}
\label{IG}
&IG_{\mathcal{V}, C_0} =  H(\mathcal{V}) - H(\mathcal{V}|C_0) = H(\mathcal{V}) + S_{MIG} \\
\label{SurM_MIG}
&S_{MIG} = -H(\mathcal{V}|C_0) = -\mathbb{E}_{v_i^{C_0} \sim p_{\mathcal{V}| C_0}} \sum_{i=1}^{M} \hat H(v^{C_0}_i)
% M=\frac{l}{\delta}\times\frac{w}{\delta}\times\frac{h}{\delta}
\end{align}
where we introduce the maximum information gain-based surrogate metric \textbf{S-MIG}, ignoring the total entropy $H(\mathcal{V})$ which is a constant given POG and the same for different LiDAR configurations. The entropy of each voxel can be found below.
% \begin{align}
$$\hat H(v_i) =  -\hat p(v_i)\log \hat p(v_i) - (1-\hat p(v_i))\log (1-\hat p(v_i)) $$
% \end{align}

% collect as much information as possible given a budget of LiDARs. Different from the maximum non-detectable subspaces to reflect perception performance \cite{liu2019should}, we propose a data-driven measure to quantify the information gain.
% A key insight here is that AVs are deployed in different cities, so they encounter varied conditions including but not limited to road infrastructures and traffic patterns. Therefore, we aim to optimize LiDAR placement such that the lasers could focus on important areas around the AV based on customized datasets of different cities or scenarios.

% Considering the probabilistic nature of the problem, information about presence of objects in a cube could be quantified using Shannon Entropy of the probability distribution defined over the cube.

% \begin{equation}
%     % H(s_i) = -(\ P(s_i)\log(P(s_i))\ +\ (1-P(s_i))\log(1-P(s_i))\ )
%     H(s_i) = -p\log(p)\ -\ (1-p)\log(1-p) \ ;\ p=P(s_i)
%     % to fit in one line
% \end{equation}

% The uncertainty information of a particular LiDAR configuration $C$ could then be quantified as:
% \begin{equation}
%     H(S_C) = H(s_1,s_2,...,s_n) = \sum_{i=1}^{n}H(s_i)
% \label{shannon}
% \end{equation}

% Finally, we propose two surrogate metrics to evaluate a LiDAR placement $C$ given target object and its POG. Suppose we have $K$ different target objects, like Car, Cyclist, etc., then the weighted surrogate metrics for all different objects given LiDAR placement $C$ can be formulated as,
% \begin{align}
% % \label{SurM_MIG}
% Surr_{MIG}^C &=  \sum_{k=1}^{K} w_k Surr_{MIG}^{(k)} \\
% Surr_{MLE}^C &= \sum_{k=1}^{K} w_k Surr_{MLE}^{(k)}
% % M=\frac{l}{\delta}\times\frac{w}{\delta}\times\frac{h}{\delta}
% \end{align}
% where $\sum_{k=1}^{K} w_k = 1$, $w_k$ is the normalized weight for each target objects in ROI and can be customized in application scenarios.
% and we aim to minimize this cost. This is equivalent to maximizing the amount of information about presence of objects in the cubes of set $S_C$ covered by the LiDAR configuration $C$.


% \subsection{Optimization}
% %To find an optimal LiDAR placement given initial configuration, we first need to estimate the surrogate function to be optimized, which requires the indices of cubes detected by the LiDAR beams. We utilise ray tracing of LiDAR beams to locate these cubes and, in addition, express the coordinate system in integer coordinates to achieve fast integer operations. Therefore, to avoid defining excessively large cubes, we use a scaled ROI.

% A single LiDAR configuration could be expressed as $[x_i, y_i, z_i, \alpha_i, \beta_i, \gamma_i]$, where $(x_i, y_i, z_i)$ is the location and $(\alpha_i, \beta_i, \gamma_i)$ are roll, pitch and yaw angles of the LiDAR in the ROI coordinate system. The yaw angle (represented by $\gamma_i$) is not optimized as the LiDAR beams rotate 360 degrees in the horizontal FOV. Therefore, our goal is to find a configuration $[x_i, y_i, z_i, \alpha_i, \beta_i]$ which minimizes the uncertainty mentioned in Equation \ref{shannon}.

% Since the surrogate cost metric is not differentiable, we could use any black-box optimization method to find the optimal LiDAR configuration.
% A proposed configuration $C$ is considered to be valid if it satisfies some geometric constraints, because the LiDARs could not be installed far away from the vehicle. These geometric constraints are also the boundary points of the search region.
% Denote $C_{min} = [x_{min}, y_{min}, z_{min}, \alpha_{min}, \beta_{min}],
% C_{max} = [x_{max}, y_{max}, z_{max}, \alpha_{max}, \beta_{max}]
% $.
% Then, the optimization objective becomes: \begin{equation}
% \label{eq:optproblem}
% \begin{aligned}
% C^*= \arg & \min_{C} - H(S_C) \\
% &\textrm{such that} \quad C_{\min} \leq C \leq C_{\max}\
% \end{aligned}
% \end{equation}

% The overall pipeline of our LiDAR configuration optimization and evaluation method is shown in Figure~\ref{fig:overview}. We utilize TuRBO \cite{eriksson2019scalable} as the optimizer to minimize the surrogate. TuRBO works by creating multiple local probabilistic models instead of a single global exploration model. Similar to stochastic optimization methods, it creates trust regions (the center of which is usually the best solution) using simple surrogate models that are \textit{believed} to accurately model the function to be optimized. TuRBO proposes configurations which are then evaluated using our ray tracing based fast evaluation methodology on top of a POG.
% The algorithm is detailed in Algorithm \ref{alg:points}.
% %and Bresenham's Line Algorithm is detailed in Algorithm \ref{alg:raytrace}.
% The complete evaluation for a particular configuration is conducted by iterating over all yaw angles to be considered (horizontal FOV), all $N_b$ beams angles and all $N_l$ LiDARs.







\section{Experiments}
\label{experiments}
%  \todo{specify the l w h and delta}
%  \todo{explain how to select BBox, training + test set}
% In this section, we first introduce our multi-lidar configuration evaluation framework based on CARLA simulator \cite{dosovitskiy2017carla}, including point cloud data collection, model training and detection evaluation on test set.
This section aims to address two questions: 1) Does LiDAR placement influence the final perception performance? 2) How can we quantify and evaluate the detection performance of different LiDAR configurations using our easy-to-compute probabilistic surrogate metric? To answer these questions, we conduct extensive experiments in a realistic self-driving simulator --- CARLA \cite{dosovitskiy2017carla}. Due to the requirement of fairly evaluating different LiDAR placements with all other environmental factors fixed, such as the ego-vehicle's and surrounding objects' trajectories,
% current collected real-world public autonomous driving dataset are collected from specific hardware setups and impossible to fix environmental factors as well, so they cannot be used for our experiments.
we choose to use realistic simulation scenarios in CARLA (Figure \ref{fig:carla-maps}), instead of a public real-world dataset.

% We choose to evaluate our method in simulation environment rather than on public datasets or in real world for several reasons. First of all, public point cloud datasets have been collected from specific hardware setups, which could not generate various LiDAR placement configurations as required. Secondly, we need to fix all environment variables, such as the ego-vehicle trajectory and surrounding objects, and only change the LiDAR configurations to fairly compare the perception performances of these configurations. However, it is very hard to control these environment variables for different LiDAR configurations in the real world because vehicles and pedestrians on the road are continuously changing. Therefore, the most convenient and economical way to achieve this is to simulate scenarios and point clouds in realistic simulators such as CARLA.

% In the subsequent subsections we detail the experimental setup in CARLA and show how it connects to the overall pipeline of our methodology, as shown in Figure \ref{fig:overview}.

\begin{figure}[t!]
\begin{center}
    % \begin{subfigure}[b]{0.235\textwidth}
    %             \includegraphics[width=0.98\linewidth]{images/carla/carla-map-4.png}
    %             \caption{}
    %             \label{fig:carla-map-1}
    % \end{subfigure}%
    % \begin{subfigure}[b]{0.235\textwidth}
    %             \includegraphics[width=\linewidth]{images/carla/carla-map-2.png}
    %             \caption{}
    %             \label{fig:carla-map-2}
    % \end{subfigure}
    \begin{subfigure}[b]{0.235\textwidth}
                \includegraphics[width=\linewidth]{images/carla/carla-map-1.png}
                \caption{}
                \label{fig:carla-map-4}
    \end{subfigure}
    \begin{subfigure}[b]{0.235\textwidth}
                \includegraphics[width=\linewidth]{images/carla/carla-map-3.png}
                \caption{}
                \label{fig:carla-map-3}
    \end{subfigure}%
\end{center}
\vspace{-7mm}
  \caption{Realistic simulation environments in CARLA.}
  %(b) A complex town with multi-lane junctions. (c) A small town having only ``T" road intersections.
\label{fig:carla-maps}
\vspace{-5mm}
\end{figure}




\subsection{Experimental Setup}
% Introduce CARLA simulator, scenario runner, Point Voxel RCNN model, evaluation metric etc.
%  A  multi-lidar configuration evaluation framework based on CARLA simulator \cite{dosovitskiy2017carla} is proposed for our LiDAR placement evaluation experiments.
 Given CARLA scenarios and the target object, we obtain the POG based on the bounding box labels and calculate the surrogate metric S-MIG for every LiDAR placement. For each evaluated LiDAR configuration candidate,  we first collect point cloud data in CARLA,  then train and test all the object detection models using the collected data. Finally, we correlate the surrogate metric and the 3D detection performance. More details can be found in Appendix \ref{sec:app}.
%  , and the results explain the LiDAR performance variance from the information theory perspective,
%
% In the subsequent subsections we detail the experimental setup in CARLA and show how it connects to the overall pipeline of our methodology, as shown in Figure \ref{fig:overview}.

\begin{figure}[h]
\begin{center}
% \fbox{\rule{0pt}{2in} \rule{0.9\linewidth}{0pt}}
    \begin{subfigure}[b]{0.235\textwidth}
                \includegraphics[width=.98\linewidth]{images/line_000017.png}
                % \caption{Square}
                % \label{fig:baseline-square}
    \end{subfigure}%
        \begin{subfigure}[b]{0.235\textwidth}
                \includegraphics[width=.98\linewidth]{images/center_000017.png}
                % \caption{Square}
                % \label{fig:baseline-square}
    \end{subfigure}%
    % \begin{subfigure}[b]{0.235\textwidth}
    %             \includegraphics[width=.98\linewidth,height=.6\linewidth]{images/baseline-square.png}
    %             \caption{Square}
    %             \label{fig:baseline-square}
    % \end{subfigure}%
    % \begin{subfigure}[b]{0.235\textwidth}
    %             \includegraphics[width=.98\linewidth,height=.6\linewidth]{images/pointcloud-square.png}
    %             \caption{Pointcloud (Square)}
    %             \label{fig:pointcloud-square}
    % \end{subfigure}

    \begin{subfigure}[b]{0.235\textwidth}
                \includegraphics[width=.98\linewidth,height=.6\linewidth]{images/line.png}
    \end{subfigure}%
    \begin{subfigure}[b]{0.235\textwidth}
                \includegraphics[width=.98\linewidth,height=.6\linewidth]{images/center.png}
    \end{subfigure}%

    \begin{subfigure}[b]{0.235\textwidth}
                \includegraphics[width=.98\linewidth,height=.6\linewidth]{images/baseline-line.png}
                \caption{Line-roll}
                \label{fig:baseline-line}
    \end{subfigure}%
    \begin{subfigure}[b]{0.235\textwidth}
                \includegraphics[width=.98\linewidth,height=.6\linewidth]{images/baseline-center.png}
                \caption{Center}
                \label{fig:baseline-center}
    \end{subfigure}%
    % \begin{subfigure}[b]{0.235\textwidth}
    %             \includegraphics[width=.98\linewidth,height=.6\linewidth]{images/pointcloud-center.png}
    %             \caption{Pointcloud (Center)}
    %             \label{fig:pointcloud-center}
    % \end{subfigure}
    % \begin{subfigure}[b]{0.235\textwidth}
    %             \includegraphics[width=.98\linewidth,height=.6\linewidth]{images/pointcloud-line.png}
    %             \caption{Pointcloud (Line)}
    %             \label{fig:pointcloud-line}
    % \end{subfigure}
\end{center}
\vspace*{-5mm}
  \caption{Point cloud data collected from different LiDAR configurations under the same scenarios with car (in black 3D bounding box) and cyclist (in red 3D bounding  box).
%   Standard baseline and optimal LiDAR configurations along with their generated point clouds, used for Experiment 1. The left column above shows the configurations while the right column displays the corresponding point clouds in the same scene.
  }
\label{fig:compare-config}
\vspace*{-5mm}
\end{figure}

\textbf{CARLA simulator.} We use CARLA as the simulation platform to collect data and evaluate our method. CARLA is a high-definition open-source simulator for autonomous driving research \cite{chen2020learning,chen2019model} that offers flexible scenario setups and sensor configurations. CARLA provides realistic rendering and physics simulation based on Unreal Engine 4. Furthermore, the LiDAR sensor could generate accurate point cloud data and could be easily configured with customized placement.
% Hence, CARLA has been used for a wide range of AV research, including development, training, testing and validating autonomous driving systems~\cite{chen2020learning,chen2019model}.
We use CARLA's inbuilt \texttt{ScenarioRunner} module to simulate  realistic traffic scenarios, only changing  LiDAR configurations for all the experiments. We collect data in \texttt{Town 1, 3, 4, 6} and each town contains 8 manually recorded routes. For \texttt{Sparse}, \texttt{Medium} and \texttt{Dense} scenarios, we spawn 40, 800, 120 Vehicles (including cars, motorcycles, trucks, cyclists, etc.) in each town and the default density  is \texttt{Medium}.

% \textbf{ScenarioRunner.} \todo{how to generate pedestrians (briefly)} The CARLA community contributes many urban maps and scenarios. For example, Figure \ref{fig:carla-maps} shows a sample of the maps provided in the simulator, which encompass various conditions of traffic and road geometries. To fairly compare perception capabilities of different LiDAR configurations we use CARLA's inbuilt \textit{ScenarioRunner} module  A scenario is usually composed of 3D models of static and dynamic objects, where the former could be regarded as the overall map including roads and buildings, and the latter as pedestrians and vehicles. The perception algorithm that is evaluated in this paper aims to detect dynamic objects and predict their 3D bounding boxes. The ego-vehicle with LiDAR sensors and other dynamic objects keep the same moving trajectories in these scenarios. This ensures that the ground-truth 3D bounding boxes for all dynamic objects around the ego-vehicle within the ROI are the same for different LiDAR configurations.

% We collect data from 87 such scenarios with fixed 10 hz sampling frequency, and the overall dataset size is 35000 frames, which contains point clouds and 3D bounding box labels for dynamic objects. The point clouds from different LiDARs are transformed to the ego-vehicle's frame of reference for computational convenience.



\textbf{Data format.}
% \todo{add more types of objects}
The collected dataset for each LiDAR configuration contains about 45000 point cloud frames with 3D bound boxes of normal-size vehicles as Car (including Sedan, Pickup Truck, SUV, etc.) and other abnormal-size vehicles as Van and Cyclist (including Box Truck, Cyclist, Motorcycles, etc.). We use all collected bounding boxes to calculate POG for each object type and estimate the joint distribution of voxels in ROI. We use about 10\% fixed frames as the test set and the remaining data as the training set in all the experiments. The point clouds from different LiDARs are aggregated and transformed to the reference frame of the ego-vehicle   for computational convenience. We make the data format consistent with KITTI 3D object detection benchmark
\cite{geiger2013vision}. To calculate ROI and POG efficiently for each target object, we customize the size of ROI to be $l=80m, w=40m, h=4m$ for CARLA scenario. Following the KITTI format, we use the half front view of point cloud range (40m in length)  for POG and surrogate metric, and the 3D detection model training and testing.
% Using \textit{ScenarioRunner} ensures strong reproducibility of experiments. To track the visibility of objects, a depth camera is utilized. 3-D Bounding boxes are projected back to the camera image using intrinsic and extrinsic matrices for tracking the height and truncation of the object in the bird's eye view. Using occlusion statistics, height and truncation, objects are classified as easy or hard similar to the KITTI dataset.


\textbf{3D object detection algorithms and metrics.} To fairly compare the object detection performance of different LiDAR configurations, we adopt the recent open-source LiDAR-based 3D object detection framework \textit{OpenPCDet} \cite{openpcdet2020} and fine-tune the KITTI pretrained models of multiple representative 3D detection algorithms. For voxel-based methods, we use one-stage SECOND \cite{yan2018second} and two-stage Voxel RCNN  \cite{deng2020voxel}. Also, we use PointRCNN \cite{Shi_2019_CVPR} as point-based method and PV-RCNN  \cite{shi2020pv} as the integration of voxel-based and point-based method. Moreover, we also compare the models with 3D Intersection-over-Union (IoU) loss for SECOND \cite{yan2018second} and PointRCNN \cite{Shi_2019_CVPR}.
For detection metrics, we adopt the strictest metrics of Bird-Eye-View (BEV) and 3D detection with average precision across 40\% recall under IoU thresholds of 0.70 for Car and 0.50 for Van and Cyclist \cite{geiger2013vision, openpcdet2020}.
We follow the default training hyperparameters and fine-tune  all the KITTI pretrained models for 10 epochs on our training set for the fair comparison between different LiDAR placements.


\begin{table*}[]
\centering
\resizebox{0.9\textwidth}{!}{
\begin{tabular}{|c||c|c|c|c||c|c|c|c|}
\hline
\multirow{2}{*}{Models}          & \multicolumn{4}{c||}{ Car-3D  (AP\_R40@0.70)}                                          & \multicolumn{4}{c|}{Car-BEV (AP\_R40@0.70)}                                 \\ \cline{2-9}
                                    & Center & Line & Pyramid & Trapezoid   & Center & Line &  Pyramid & Trapezoid \\ \hline
PV-RCNN  \cite{shi2020pv}
% &  28.97
     &     52.15 &	55.50 &	\textbf{57.44} &	57.37
%  &   34.09
 &	62.64&	65.42&	\textbf{65.81}&	65.54
                    \\ \hline
Voxel RCNN  \cite{deng2020voxel}
% &   26.06
&47.59	&50.85&	\textbf{52.61}&	50.95
%   &                   31.31
   & 57.72&	60.84&	\textbf{63.65}&	61.52
                                                 \\ \hline
% PointPillar  \cite{lang2019pointpillars}                    &   40.95     &    40.55    &   39.68   &    \textbf{40.97}                                                        &   36.62     &   34.72    &   34.46   &     \textbf{36.99}                                                       \\ \hline
PointRCNN \cite{Shi_2019_CVPR}
% & 23.55
&	38.46 &	\textbf{48.25}&	44.28&	47.50
% &      28.97
& 51.41&	\textbf{59.14}&	56.97&	59.08
                 \\ \hline
PointRCNN-IoU \cite{Shi_2019_CVPR}
% & 23.37
&	38.44 &	46.86&	45.34&	\textbf{47.32}
% &   28.70
&	50.56&	58.86&	56.67&	\textbf{59.04}
                         \\ \hline
SECOND \cite{yan2018second}
% & 24.84
& 42.89&	47.13&	46.89&	\textbf{47.53}
% &       31.19
& 56.65&	59.44&	\textbf{61.75}&	59.97
                                \\ \hline
SECOND-IoU  \cite{yan2018second}
% & 25.28
&	44.41&	48.60&	\textbf{51.03}&	49.10
%  &   31.46
 &	56.98&	59.96&	\textbf{62.31}&	60.28
                                 \\ \hline
% Part-$A^2$-Free \cite{shi2020points}  &        &        &      &                                                            &        &        &      &                                                            \\ \hline
% Part-$A^2$-Anchor \cite{shi2020points} &        &        &      &                                                            &        &        &      &                                                            \\ \hline
% \end{tabular}}
% \resizebox{\textwidth}{!}{
% \begin{tabular}{|c||c|c|c|c||c|c|c|c|}
\hline
\multirow{2}{*}{Models}          & \multicolumn{4}{c||}{ Van and Cyclist-3D  (AP\_R40@0.50)}                                          & \multicolumn{4}{c|}{Van and Cyclist-BEV (AP\_R40@0.50)}                                 \\ \cline{2-9} &
% Square   &
Center & Line & Pyramid & Trapezoid &
% Square   &
Center & Line &  Pyramid & Trapezoid \\ \hline
PV-RCNN  \cite{shi2020pv}
% &  \textbf{26.28}
&	40.09 &	39.11 &	40.26 &	\textbf{42.85}
%  &   \textbf{26.67}
 &	41.18&	41.45&	43.09&	\textbf{44.79}
                   \\ \hline
Voxel RCNN  \cite{deng2020voxel}
% &   \textbf{21.59}
&	33.76&	31.60&	33.39&	\textbf{33.91}
%   &                     \textbf{22.11}
   &	35.40&	33.24&	35.11&	\textbf{35.54}
                   \\ \hline
% PointPillar  \cite{lang2019pointpillars}                    &   40.95     &    40.55    &   39.68   &    \textbf{40.97}                                                        &   36.62     &   34.72    &   34.46   &     \textbf{36.99}                                                       \\ \hline
PointRCNN \cite{Shi_2019_CVPR}
% & \textbf{18.82}
&	\textbf{31.43}&	28.00&	27.10&	30.91
%   & \textbf{20.29}
   &	\textbf{33.86}&	30.68&	30.49&	33.75
                      \\ \hline
PointRCNN-IoU \cite{Shi_2019_CVPR}
% & \textbf{18.49}
&	\textbf{30.75}&	27.64&	26.19&	30.54
%  &  \textbf{20.57}
 &	34.04&	30.71&	29.30&	\textbf{34.70}
                  \\ \hline
SECOND \cite{yan2018second}
% & \textbf{22.46}
&	\textbf{36.13}&	32.36&	35.32&	35.95
%  &  \textbf{24.34}
 &	38.19&	30.68&	\textbf{40.84}&	40.01
                     \\ \hline
SECOND-IoU  \cite{yan2018second}
% & \textbf{23.37}
&	35.61&	33.12&	34.95&	\textbf{36.51}
% &    \textbf{24.90}
&	38.99&	36.60&	38.31&	\textbf{40.14}
                        \\ \hline
% Part-$A^2$-Free \cite{shi2020points}  &        &        &      &                                                            &        &        &      &                                                            \\ \hline
% Part-$A^2$-Anchor \cite{shi2020points} &        &        &      &                                                            &        &        &      &                                                            \\ \hline
% \end{tabular}}
% \resizebox{\textwidth}{!}{
% \begin{tabular}{|c||c|c|c|c||c|c|c|c|}
% \hline
% \multirow{2}{*}{Models}          & \multicolumn{5}{c||}{Cyclist-BEV (AP\_R40@0.50)}                                          & \multicolumn{5}{c|}{Cyclist-3D Detection (AP\_R40@0.50)}                                 \\ \cline{2-11}
%                                  & Square   & Center & Line & Pyramid & Trapezoid & Square   & Center & Line &  Pyramid & Trapezoid \\ \hline
% PV-RCNN  \cite{shi2020pv}                        &  56.69     &      &   53.96  & 61.38 &    \textbf{62.25}                                                &   44.42           &   47.06     & 45.92 &   \textbf{51.52 }                   &                              \\ \hline
% Voxel RCNN  \cite{deng2020voxel}                     &   52.38   &            & 50.66 &  56.64   &      \textbf{59.07}                              &                      &    34.65               &  38.45&  40.14  &  \textbf{46.49 }                                                     \\ \hline
% % PointPillar  \cite{lang2019pointpillars}                    &   40.95     &    40.55    &   39.68   &    \textbf{40.97}                                                        &   36.62     &   34.72    &   34.46   &     \textbf{36.99}                                                       \\ \hline
% PointRCNN \cite{Shi_2019_CVPR}               & &        &    43.50   &         36.16 &  47.19   &      \textbf{47.98  }                                                   & 27.30               & 23.82 &  27.83   &   \textbf{30.03 }                                                      \\ \hline
% PointRCNN-IoU \cite{Shi_2019_CVPR}          & &           &  42.07                 & 33.90 & \textbf{47.40}  &   46.90                                                         &  27.53               & 18.00 &    26.73  &  \textbf{31.65 }                                                       \\ \hline
% SECOND \cite{yan2018second}             & &             &    48.01    &           50.68 & 51.27 &       \textbf{56.82 }                                                   &  33.60                 & 38.51 & 32.83   &   \textbf{45.25 }                                                       \\ \hline
% SECOND-IoU  \cite{yan2018second}              & &          &    47.99                &   50.32   & 50.36 &   \textbf{56.92 }                                                 &    36.33             & 40.74 &    35.81  &   \textbf{43.42 }                                                      \\ \hline
% Part-$A^2$-Free \cite{shi2020points}  &        &        &      &                                                            &        &        &      &                                                            \\ \hline
% Part-$A^2$-Anchor \cite{shi2020points} &        &        &      &                                                            &        &        &      &                                                            \\ \hline
\end{tabular}}
% \resizebox{\textwidth}{!}{
% \begin{tabular}{|c||c|c|c|c|c|c||c|c|c|c|c|c|}
% \hline
% \multirow{3}{*}{Models}          & \multicolumn{6}{c||}{Car-BEV (AP\_R40@0.70)}                                          & \multicolumn{6}{c|}{Car-3D Detection (AP\_R40@0.70)}                                 \\ \cline{2-13}
%                                  & Square & \makecell[c]{Vertical \\ Center 2} & Diamod  & \makecell[c]{Vertical \\ Center 4} & Line & \makecell[c]{Optimized \\ Ours} & Square & \makecell[c]{Vertical \\ Center 2} & Diamod  & \makecell[c]{Vertical \\ Center 4} & Line &  \makecell[c]{Optimized \\ Ours} \\ \hline
% PV-RCNN  \cite{shi2020pv}                        &  68.47      &    66.10    &   68.52   &   68.70  & 66.51 &    66.61                                                   &   63.33     &   60.91     &   63.72   &   63.22     & 63.18 &   63.27                                                   \\ \hline
% Voxel RCNN  \cite{deng2020voxel}                     &   65.99     &   63.55     &  66.37 & 68.29 &  65.68   &      65.33                                                     &  60.34      &   58.26     &  62.89  & 62.85 &  60.01  &  60.13                                                        \\ \hline
% % PointPillar  \cite{lang2019pointpillars}                    &   40.95     &    40.55    &   39.68   &    \textbf{40.97}                                                        &   36.62     &   34.72    &   34.46   &     \textbf{36.99}                                                       \\ \hline
% PointRCNN \cite{Shi_2019_CVPR}                       &    61.07    &   52.31     & 61.16  & 58.24 &  56.14    &      58.72                                                      &  53.48      &    43.12    & 55.78  & 50.35 &  47.87   &   50.86                                                         \\ \hline
% PointRCNN-IoU \cite{Shi_2019_CVPR}                     &  60.80      &   55.51     & 54.38    & 57.43 & 57.60  &   58.49                                                         &  53.18      &  50.02      &  49.82 & 49.77 &    49.42  &   52.70                                                         \\ \hline
% SECOND \cite{yan2018second}                          &    66.01    &  63.09      &   63.60  & 67.81 & 65.83 &       65.24                                                     &  59.41      &   57.24     &  57.86   & 61.80 & 57.43   &   59.32                                                         \\ \hline
% SECOND-IoU  \cite{yan2018second}                        &    63.71    &    62.68    &      63.36 &   62.67   & 63.77 &   63.52                                                   &     57.83   &   57.93     & 58.08 & 56.24 &    57.45  &   58.28                                                         \\ \hline
% % Part-$A^2$-Free \cite{shi2020points}  &        &        &      &                                                            &        &        &      &                                                            \\ \hline
% % Part-$A^2$-Anchor \cite{shi2020points} &        &        &      &                                                            &        &        &      &                                                            \\ \hline
% \end{tabular}}
% \resizebox{\textwidth}{!}{
% \begin{tabular}{|c||c|c|c|c|c|c||c|c|c|c|c|c|}
% \hline
% \multirow{3}{*}{Models}          & \multicolumn{6}{c||}{Pedestrian-BEV (AP\_R40@0.50)}                                          & \multicolumn{6}{c|}{Pedestrian-3D Detection (AP\_R40@0.50)}                                 \\ \cline{2-13}
%                                  & Square & \makecell[c]{Vertical \\ Center 2} & Diamod  &\makecell[c]{Vertical \\ Center 4} & Line & \makecell[c]{Optimized \\ Ours} & Square & \makecell[c]{Vertical \\ Center 2} & Diamod  & \makecell[c]{Vertical \\ Center 4} & Line &  \makecell[c]{Optimized \\ Ours} \\ \hline
% PV-RCNN  \cite{shi2020pv}                        &   48.37     &  46.66      &   46.15   & 45.72 & 47.35 &                                                            &   43.98      &   42.24     &   41.70  & 39.62 &   42.48 &                                                            \\ \hline
% Voxel RCNN  \cite{deng2020voxel}                     &  42.15      &   40.19     &  42.65   & 41.13 &  42.48    &                                                          &  37.96     &  36.15      &   38.35   & 36.79 &  37.57 &                                                     \\ \hline
% % PointPillar  \cite{lang2019pointpillars}                    &  20.81     &    20.49    &   20.09   &   \textbf{22.89}                                                         &  13.52      &  16.07      & 15.62     &   \textbf{17.32}                                                         \\ \hline
% PointRCNN  \cite{Shi_2019_CVPR}                       &   27.46      &    22.21    &  26.02 & 22.09 &  24.17    &                                                            & 23.67       &    19.32    & 21.93   & 19.38 &  19.65    &                                                            \\ \hline
% PointRCNN-IoU  \cite{Shi_2019_CVPR}                   &    29.09    &   27.94     & 23.20  & 22.32 &  26.01    &                                                            &   23.66     &    23.81    & 18.16 & 19.25  &    20.23   &                                                            \\ \hline
% SECOND  \cite{yan2018second}                          &   46.27     &   45.23     &  45.59   & 44.95 & 46.04  &                                                            &   40.45     &   40.27     &  39.52   & 38.01 &  38.48  &                                                            \\ \hline
% SECOND-IoU  \cite{yan2018second}                      &    39.26    &    36.67    & 37.12  & 32.45 &   38.09   &                                                            &   34.66     &   32.59     & 32.12  & 26.76 &  31.58    &                                                            \\ \hline
% % Part-$A^2$-Free \cite{shi2020points}  &        &        &      &                                                            &        &        &      &                                                            \\ \hline
% % Part-$A^2$-Anchor \cite{shi2020points} &        &        &      &                                                            &        &        &      &                                                            \\ \hline
% \end{tabular}}
% \resizebox{\textwidth}{!}{
% \begin{tabular}{|c||c|c|c|c|c|c||c|c|c|c|c|c|}
% \hline
% \multirow{3}{*}{Models}          & \multicolumn{6}{c||}{Cyclist-BEV (AP\_R40@0.50)}                                          & \multicolumn{6}{c|}{Cyclist-3D Detection (AP\_R40@0.50)}                                 \\ \cline{2-13}
%                                  & Square & \makecell[c]{Vertical \\ Center 2} & Diamod  & \makecell[c]{Vertical \\ Center 4} & Line & \makecell[c]{Optimized \\ Ours} & Square & \makecell[c]{Vertical \\ Center 2} & Diamod  & \makecell[c]{Vertical \\ Center 4} & Line &  \makecell[c]{Optimized \\ Ours} \\ \hline
% PV-RCNN \cite{shi2020pv}                          &  53.05      &   57.00     &  53.07   & 54.97 &  61.06  &                                                            &    50.86    &   56.41     &   52.70   & 52.75 &  58.96 &                                                            \\ \hline
% Voxel RCNN  \cite{deng2020voxel}                      &  48.46      &   53.57     &   51.27   & 51.71 & 57.24  &                                                           &  46.96     &  52.66      &   49.18  & 49.59 &  56.59  &                                                          \\ \hline
% % PointPillar  \cite{lang2019pointpillars}                    &     26.07   &   \textbf{30.94}     &   27.15   &  28.37                                                          &  23.55      &  \textbf{28.51}      &   24.90   &   25.80                                                         \\ \hline
% PointRCNN  \cite{Shi_2019_CVPR}                      &   38.52    &    33.93    &  37.92  & 33.63 &  43.83   &                                                            &  36.33      &     31.78   &  35.57 & 31.47 &  41.55    &                                                            \\ \hline
% PointRCNN-IoU \cite{Shi_2019_CVPR}                   &   38.53     &    41.81    & 28.99  & 34.27 &   45.46   &                                                            &   36.20     &    38.54    & 27.57   & 31.98 &  41.48   &                                                            \\ \hline
% SECOND  \cite{yan2018second}                          &   48.63     &  53.57      &  53.62   & 53.19 &  56.62  &                                                            &   45.62     &   50.47     &  50.94   & 50.60 &  54.51  &                                                            \\ \hline
% SECOND-IoU \cite{yan2018second}                       &   49.74     &    53.14    & 50.52  & 48.25 &  57.79   &                                                            &    47.33    &   52.11     & 48.32  & 45.77 &   55.59   &                                                            \\ \hline
% % Part-$A^2$-Free  \cite{shi2020points} &        &        &      &                                                            &        &        &      &                                                            \\ \hline
% % Part-$A^2$-Anchor \cite{shi2020points} &        &        &      &                                                            &        &        &      &                                                            \\ \hline
% \end{tabular}}
\caption{Comparison of object detection performance under various LiDAR configurations  using different algorithms.
% Detection metric of mean average precision is computed by averaging precision across 40 recall positions (AP\_R40) for  3D detection (3D IoU) and bird-eye-view (BEV) under IOU thresholds of 0.7 for Car and 0.5 for Cyclist from KITTI \cite{geiger2013vision}.
% \todo{explain what does @70 @50 @50 mean! in the text, caption getting too long}
% }
}
\label{tab:expt1}
\vspace*{-5mm}
\end{table*}



\textbf{Different LiDAR Placements} We evaluate several LiDAR placements as baselines to show the influence on object detection performance. We mainly consider the placement problem of 4 LiDARs and each LiDAR has 20Hz rotation frequency and 90,000 points per frame from the 16 beams.
% Although the beam angles could be optimized (see Section~\ref{sec:beam_optimization} for details),
In our experiments, we set beams to be equally distributed in the vertical FOV $[-25.0, 5.0]$ degrees.
\textit{Trapezoid} LiDAR configuration is simplified from  Toyota's self-driving cars with 4 LiDARs in the parallel front and back (Figure~\ref{fig:toyota}). \textit{Pyramid} placement is motivated by Cruise and Pony AI (Figure~\ref{fig:cruise}, \ref{fig:ponyai}) including 1 front LiDAR and 3 back ones with a higher one in the middle.
 \textit{Center} placement is achieved by vertically stacking four LiDARs together at the center of the roof, which is inspired by Argo AI's autonomous vehicle (Figure~\ref{fig:argoai}). \textit{Line} is motivated by Ford (Figure~\ref{fig:ford}) and 4 LiDARs are placed in a horizontal line symmetrically.  The idea of \textit{Square} comes from Zoox's self-driving cars (Figure~\ref{fig:zoox}), which places the 4 LiDARs on the 4 roof corners.
% Apart from these AV company inspired LiDAR placements, we also add \todo{2 more random baselines to compare. We name them \textbf{Random-1 and Random-2.}}
Furthermore, we investigate the influence of roll rotation on sided LiDARs (\textit{Line-roll}, \textit{Pyramid-roll}).
The visualization of some baseline placements is shown in Figure~\ref{fig:compare-config}
% , where each LiDAR's pose is under ego-vehicle's coordinate system with the origin at the vehicle's geometric center
and more details of LiDAR placement are presented in the Figure \ref{fig:lidar-placement}.

% The training hyper-parameters, such as the number of iterations, learning rate, and batch size, are kept the same for all experiments for fair comparison.
% PV-RCNN  \cite{shi2020pv}    Voxel RCNN  \cite{deng2020voxel}  PointRCNN \cite{Shi_2019_CVPR}   SECOND \cite{yan2018second}
% SECOND Grid-based sparse convolution voxel a voxelbased one stage object detector
% point-based pointrcnn generates 3D proposals directly from the whole point clouds
% e follow [14, 9,
% 26] to adopt the 3D Intersection-over-Union (IoU) between
% the 3D RoIs and their corresponding ground-truth boxes as
% the training targets

% We use PointVoxel-RCNN (PV-RCNN)~\cite{shi2020pv} as the 3D object detection module to evaluate the perception performance of different LiDAR placements. PointVoxel-RCNN combines voxel-based features using anchors of various different sizes and and pointnet-based features using a Voxel Set Abstraction Layer. Following this layer, using multiple receptive fields, the learned discriminative features of keypoints are then aggregated to the ROI-grid points for finer grained region proposals.


% \textbf{Evaluation metrics.} \todo{No AOS (because no camera info for angle angle and angle yaw), no hard, easy, moderate because no 2D detection ground truth, use higher iou 0.7,0.5,0.5 instead of 0.5 0.25 0.25 for strict benchmark} We show results on three commonly used metrics --- Bird-eye view (BEV) detection, 3-D Intersection Over Union (IOU) and Average Orientation Similarity (AOS) --- to evaluate the final perception performance of different LiDAR configurations. 3-D IOU measures the fraction of overlap region divided by total region occupied by the 3-D bounding boxes together. Orientation Similarity considers prediction of the orientation together with object detection. %For all metrics, precision is averaged over 40 recall positions.
% These metrics are reported by averaging precision across 40 recall points so as to roughly approximate the precision recall curve. The evaluation IoU thresholds are set as 0.7 and 0.5 for easy and hard respectively. The detail of the metrics is described in \cite{geiger2013vision}. As a particular instance of this paper, we only evaluate the detection performance for the `Cars' label.


% Note that all the environmental variables (scenarios and model training parameters) except the LiDAR configurations \textbf{are always the same} during the perception performance evaluation procedure, so the three metrics could reflect the point cloud quality or the perception capability for different LiDAR placements.

% \begin{table*}[!htb]
% \centering
% \def\arraystretch{1.2}
% \resizebox{1.98\columnwidth}{!}{
% \begin{tabular}{|c||c|c|c||c|c|c||c|c|c|}
% \hline

% \multirow{3}{*}{Configuration} & \multicolumn{3}{c||}{Car - BEV} & \multicolumn{3}{c||}{Car - 3D Detection} & \multicolumn{3}{c|}{Car - AOS} \\  \cline{2-10}
% & AP\_R40 & @0.70 & @0.50 & AP\_R40 & @0.70 & @0.50 & AP\_R40 & @0.70 & @0.50 \\ \cline{2-10}
% & Overall & Easy & Hard & Overall & Easy & Hard & Overall & Easy & Hard \\ \hline

% Square & \textit{50.54} & 60.11 & 40.97 & \textit{46.81} & 55.48 & 38.14 & \textit{35.6} & 39.57 & 31.76 \\ \hline
% Center & \textit{49.01} & 56.33 & 41.69 & \textit{43.94} & 50.75 & 37.12 & \textit{49.83} & 53.14 & 46.51 \\ \hline
% Line   & \textit{45.18} & 51.68 & 38.68 & \textit{42.79} & 49.25 & 36.32 & \textit{31.02} & 33.98  & 28.07 \\ \hline
% % Random-1 & &  &  &  &  & &  &  &  \\ \hline
% % Configuration 5 (Random-2) & &  &  &  & & &  &  & & &  &  \\ \hline
% % Configuration 6 (Random-3) & & &  &  & & &  &  &  & &  &  \\ \hline

% Optimized (Ours) & \textbf{\textit{55.47}} & {\textbf{60.78}} & \textbf{50.16} & \textbf{\textit{50.53}} & \textbf{56.82} & \textbf{44.25} & \textbf{\textit{53.48}} & \textbf{58.96} & \textbf{47.99}\\ \hline

% \end{tabular}
% }
% \caption{Comparison of object detection performance of various LiDAR configurations. For all 3 metrics, \textbf{mean average precision} is computed by averaging precision across 40 recall positions (\textbf{AP\_R40}). \textbf{Overall} column is the arithmetic mean of \textbf{Easy} and \textbf{Hard} columns. More details about their definitions could be found in the KITTI dataset~\cite{geiger2013vision}. The IOU thresholds in the evaluation are set as 0.7 and 0.5 for easy and hard respectively.
% % \todo{explain what does @70 @50 @50 mean! in the text, caption getting too long}
% % }
% }
% \label{tab:expt1}
% \vspace{-4mm}
% \end{table*}

% & \makecell[c]{Vertical \\ Center} &





% \begin{table*}[]
% \centering
% \resizebox{\textwidth}{!}{
% \begin{tabular}{|c|cccc|cccc|}
% \hline
% \multirow{2}{*}{Variations} & \multicolumn{4}{c|}{Car-3D Detection}
% % & \multicolumn{4}{c|}{Pedestrian-3D Detection}
% & \multicolumn{4}{c|}{Cyclist-3D Detection}                                                          \\ \cline{2-9}
%                             & \multicolumn{1}{c|}{PV-RCNN} & \multicolumn{1}{c|}{PointRCNN} & \multicolumn{1}{c|}{Entropy} & MLE &
%                             % \multicolumn{1}{c|}{PV-RCNN} & \multicolumn{1}{c|}{PointRCNN} & \multicolumn{1}{c|}{Entropy} & MLE &
%                             \multicolumn{1}{c|}{PV-RCNN} & \multicolumn{1}{c|}{PointRCNN} & \multicolumn{1}{c|}{Entropy} & MLE \\ \hline
% Center-2                    & \multicolumn{1}{c|}{}        & \multicolumn{1}{c|}{}          & \multicolumn{1}{c|}{}        &     &
% % \multicolumn{1}{c|}{}        & \multicolumn{1}{c|}{}          & \multicolumn{1}{c|}{}        &     & \multicolumn{1}{c|}{}        &
% \multicolumn{1}{c|}{}          & \multicolumn{1}{c|}{}        &  \multicolumn{1}{c|}{}   &  \\ \hline
% Center-3                    & \multicolumn{1}{c|}{}        & \multicolumn{1}{c|}{}          & \multicolumn{1}{c|}{}        &     &
% % \multicolumn{1}{c|}{}        & \multicolumn{1}{c|}{}          & \multicolumn{1}{c|}{}        &     & \multicolumn{1}{c|}{}        &
% \multicolumn{1}{c|}{}          & \multicolumn{1}{c|}{}        &  \multicolumn{1}{c|}{}   &  \\ \hline
% Center-4                    & \multicolumn{1}{c|}{}        & \multicolumn{1}{c|}{}          & \multicolumn{1}{c|}{}        &     &
% % \multicolumn{1}{c|}{}        & \multicolumn{1}{c|}{}          & \multicolumn{1}{c|}{}        &     & \multicolumn{1}{c|}{}        &
% \multicolumn{1}{c|}{}          & \multicolumn{1}{c|}{}        &   \multicolumn{1}{c|}{}   & \\ \hline
% Line                        & \multicolumn{1}{c|}{}        & \multicolumn{1}{c|}{}          & \multicolumn{1}{c|}{}        &     &
% % \multicolumn{1}{c|}{}        & \multicolumn{1}{c|}{}          & \multicolumn{1}{c|}{}        &     & \multicolumn{1}{c|}{}        &
% \multicolumn{1}{c|}{}          & \multicolumn{1}{c|}{}        &  \multicolumn{1}{c|}{}   &  \\ \hline
% Line-roll                   & \multicolumn{1}{c|}{}        & \multicolumn{1}{c|}{}          & \multicolumn{1}{c|}{}        &     &
% % \multicolumn{1}{c|}{}        & \multicolumn{1}{c|}{}          & \multicolumn{1}{c|}{}        &     & \multicolumn{1}{c|}{}        &
% \multicolumn{1}{c|}{}          & \multicolumn{1}{c|}{}        &  \multicolumn{1}{c|}{}   &  \\ \hline
% Pyramid                     & \multicolumn{1}{c|}{}        & \multicolumn{1}{c|}{}          & \multicolumn{1}{c|}{}        &     &
% % \multicolumn{1}{c|}{}        & \multicolumn{1}{c|}{}          & \multicolumn{1}{c|}{}        &     & \multicolumn{1}{c|}{}        &
% \multicolumn{1}{c|}{}          & \multicolumn{1}{c|}{}        & \multicolumn{1}{c|}{}    &  \\ \hline
% Pyramid-roll                & \multicolumn{1}{c|}{}        & \multicolumn{1}{c|}{}          & \multicolumn{1}{c|}{}        &     &
% % \multicolumn{1}{c|}{}        & \multicolumn{1}{c|}{}          & \multicolumn{1}{c|}{}        &     & \multicolumn{1}{c|}{}        &
% \multicolumn{1}{c|}{}          & \multicolumn{1}{c|}{}        &  \multicolumn{1}{c|}{}    & \\ \hline
% \end{tabular}}
% \caption{Influence on 3D detection along with surrogate function values from variations of LiDAR quantity and rotation angles.  \todo{update data}
% % \todo{explain what does @70 @50 @50 mean! in the text, caption getting too long}
% % }
% }
% \label{tab:expt1}
% \end{table*}

% \begin{table}[]
% \centering
% \resizebox{0.5\textwidth}{!}{
% \begin{tabular}{|c|c|c|c|c|}
% \hline
% Car    & PV-RCNN\cite{shi2020pv}  & PointRCNN\cite{Shi_2019_CVPR}  & \makecell[c]{S-MIG$(10^3)$\\ Per LiDAR} &  \makecell[c]{S-MLE$(10^6)$\\ Per LiDAR}  \\ \hline
% Center-2  &   29.00      &    20.85       &    -1.45         & -0.94   \\ \hline
% Center-3 &    28.62     &     20.86      &       -1.55       &  -0.87 \\ \hline
% Center-4 &    28.09     &     20.54      &     -1.50      & -0.87  \\ \hline
% % \hline
% % Pedestrian    & PV-RCNN & PointRCNN & Entropy & MLE \\ \hline
% % Dense  &         &           &         &     \\ \hline
% % Medium &         &           &         &     \\ \hline
% % Sparse &         &           &         &     \\ \hline
% \hline
% Cyclist     & PV-RCNN\cite{shi2020pv}  & PointRCNN\cite{Shi_2019_CVPR}  & \makecell[c]{S-MIG$(10^3)$\\ Per LiDAR} &  \makecell[c]{S-MLE$(10^6)$\\ Per LiDAR}  \\ \hline
% Center-2  &    17.94     &      12.92     &    -0.56       & -1.10   \\ \hline
% Center-3 &     18.53    &     12.51      &         -0.57       &  -1.03 \\ \hline
% Center-4 &    21.67     &     16.32      &     -0.56       & -1.03 \\ \hline
% \end{tabular}}
% \caption{Influence of different numbers of centered LiDARs on 3D-IoU detection performance.
% % \todo{explain what does @70 @50 @50 mean! in the text, caption getting too long}
% % }
% }
% \label{tab:expt3}
% \end{table}











\subsection{Experimental Results and Analysis}

% To address Q1, we could investigate Which company's LiDAR placement is better? (we should say that inspired by xx company's design, we choose those as baseline placement)
To answer the questions raised in Section \ref{experiments}, we demonstrate our evaluation results first to  show the influence of different LiDAR placements on 3D object detection, then analyze the relation between detection performance and our surrogate metric in detail. Note that we limit the number of input points per frame to make the detection challenging enough for multi-LiDAR configuration, resulting in lower detection metric values than the original KITTI benchmark \cite{geiger2013vision}.  See Appendix \ref{app:more_ressults} for more details.

\begin{table}[]
\centering
\resizebox{0.45\textwidth}{!}{
\begin{tabular}{|c|c|c|c|}
\hline
3D    & PV-RCNN\cite{shi2020pv}  &  PointRCNN\cite{Shi_2019_CVPR}  & \makecell[c]{S-MIG\\ $(10^3)$}  \\ \hline
Line  &   55.50
      &     48.25
      &  -5.02      \\ \hline
Line-roll &  54.53
&    45.12
      &    -6.05       \\ \hline
Pyramid &    57.44
    &   44.28
       &   -5.64       \\ \hline
Pyramid-roll &   53.91
     &    36.91
      &    -6.69      \\ \hline
% \hline
% Pedestrian    & PV-RCNN & PointRCNN & Entropy & MLE \\ \hline
% Dense  &         &           &         &     \\ \hline
% Medium &         &           &         &     \\ \hline
% Sparse &         &           &         &     \\ \hline
\hline
BEV    & PV-RCNN\cite{shi2020pv}   & PointRCNN\cite{Shi_2019_CVPR}  & \makecell[c]{S-MIG\\ $(10^3)$} \\ \hline
Line  &   65.42
      &     59.14
      &     -5.02     \\ \hline
Line-roll &   63.17
     &    56.54
      &   -6.05       \\ \hline
Pyramid &  65.81
       &    56.97
      &    -5.64     \\ \hline
Pyramid-roll &     63.89
    &     50.74
      &   -6.69     \\ \hline
\end{tabular}}
\caption{Influence of roll rotation of sided LiDARs (as Figure \ref{fig:baseline-line} shows) on Car detection performance.
% \todo{explain what does @70 @50 @50 mean! in the text, caption getting too long}
% }
}
\label{tab:expt4}
\vspace*{-5mm}
\end{table}

\textbf{LiDAR placement influence on 3D object detection.} From Table \ref{tab:expt1} and Figure \ref{fig:expt2}, we present the 3D detection (3D IoU) and bird-eye-view (BEV) performance of representative voxel-based and point-based detection algorithms using point cloud collected with different LiDAR configurations. Note that we use the most rigorous detection metrics in our experiments from \cite{openpcdet2020}. It can be seen that different LiDAR placements clearly influence the detection performance for all the algorithms, varying 10\% at most. Moreover, for different target objects, the influence of LiDAR placement is quite different as well. \textit{Pyrimid} configuration perform better on Car detection for most algorithms. In contrast, most models trained with data collected with \textit{Trapezoid} placement has  higher detection precision for Van and Cyclist object. The reason lies in different POG and point cloud distribution between Car, Van and Cyclist, so the most suitable placement for different target objects is different.
% \todo{influence of different placements Table 1}

\textbf{Relation between detection performance and surrogate metric.} Now we will show why different LiDAR placements will affect the detection performance using surrogate metric S-MIG. As shown in Figure \ref{fig:expt2}, we illustrate the relation between Car, Van and Cyclist detection performance and S-MIG with all the LiDAR placements using both 3D and BEV average precision metrics. As S-MIG increases, the 3D and BEV detection metrics under all algorithms generally go up for Car, Van and Cyclist detection. The fluctuation in the plots comes from some noise and outliers in data collection and model training, but the increasing trend of detection performance with respect to S-MIG is  revealed, which explains the influence of LiDAR placement. More specifically, S-MIG is smoother in reflecting detection of small objects like cyclists or extremely large objects like box trucks.
From Figure \ref{fig:compare-config} we can find that  point cloud collected through \textit{Center} is more uniformly distributed so that cyclists get more points  compared to \textit{Line-roll}, while points on large objects may get saturated so the evaluation with S-MIG may be affected.

% and it is beneficial to small object detection like Cyclist.
% For S-MLE, the correlated increasing trend is also quite clear for Cyclist detection and there is some fluctuation for Car detection because the likelihood of Car POG get saturated and less distinguishable with dense car bounding boxes.


% \todo{relationship of performance and surrogate metrics under different placements, Figure7}


\begin{table}[]
\centering
\resizebox{0.45\textwidth}{!}{
\begin{tabular}{|c|c|c|c|}
\hline
Density & Dense  & Medium  & Sparse \\ \hline
PV-RCNN   \cite{shi2020pv}                 &  57.98
     &  56.88
      &  54.48
      \\ \hline
PointRCNN   \cite{Shi_2019_CVPR}                &  48.13
     &  45.46
      &  42.30
      \\ \hline
SECOND \cite{yan2018second}                  &  48.19
     &  47.57
     &  43.96
      \\ \hline
Voxel RCNN  \cite{deng2020voxel}                &  52.15
      &  51.14
    &  48.15
     \\ \hline
\hline
S-MIG $(10^3)$                 &   -10.68    &  -7.63       &   -6.27     \\ \hline
 $H_{POG}$(\ref{totla_entropy}) $(10^3)$                 &   618.40    &  496.70      &   416.96     \\ \hline
IG (\ref{IG}) $(10^3)$                        &   607.072    &  489.06      &  410.70      \\ \hline
% S-MLE $(10^6)$                 &   -5.87    &    -6.23    &    -6.29    \\ \hline
\end{tabular}
% \begin{tabular}{|c|c|c|c|c|}
% \hline
% Car    & PV-RCNN\cite{shi2020pv}  & PointRCNN\cite{Shi_2019_CVPR}  & \makecell[c]{S-MIG\\ $(10^3)$} & \makecell[c]{IG(\ref{IG}) \\ $(10^3)$}\\ \hline
% Dense  &  29.54       &    24.34       &    -10.68   &  607.072  \\ \hline
% Medium &    28.97     &    23.55       &   -7.63       &   489.06   \\ \hline
% Sparse &   28.67      &     22.37      &   -6.27      &  410.70   \\ \hline
% % \hline
% % Pedestrian    & PV-RCNN & PointRCNN & Entropy & MLE \\ \hline
% % Dense  &         &           &         &     \\ \hline
% % Medium &         &           &         &     \\ \hline
% % Sparse &         &           &         &     \\ \hline
% \hline
% Cyclist    & PV-RCNN\cite{shi2020pv}  & PointRCNN\cite{Shi_2019_CVPR}  & \makecell[c]{S-MIG\\ $(10^3)$} & \makecell[c]{IG(\ref{IG}) \\ $(10^3)$} \\ \hline
% Dense  &    18.43     &   10.89        &    -2.88     &   182.86  \\ \hline
% Medium &   26.28      &     18.82      &    -3.01     &    223.11 \\ \hline
% Sparse &   23.51      &     18.29      &    -1.46     &  167.68   \\ \hline
% \end{tabular}
}
\caption{Influence of scenarios with different densities of Car on \textit{Square} 3D detection.
% \todo{explain what does @70 @50 @50 mean! in the text, caption getting too long}
% }
}
\vspace*{-5mm}
\label{tab:expt5}
\end{table}

\subsection{Ablation Study and Application Analysis}
% \todo{what is the difference of this section from the previous one? Need more explaination}
\label{sec:ablation}
In this section, we further investigate the influence on placement-detection correlation from some key factors in self-driving, giving  examples to evaluate the LiDAR configurations using S-MIG towards  potential applications.

% , Consequently, the correlation with object detection means that our surrogate metric can be used to evaluate the detection performance without effort-costly data collection and model training in some applications., along with some potential applications.
% more on how typical placement variances and  scenarios with different object densities affect the the perception performance and surrogate models.

% \textbf{The number of centered LiDARs.} From Table \ref{tab:expt3}, it can be seen that under the same condition and increasing the number of centered LiDARs, the detection performance does not improve too much and even get worse, which is due to the fact that S-MIG and S-MLE per LiDAR are with similar values across 2 to 4 centered LiDARs.

% \todo{how placement variance influence detection performance and sorrogate metrics table 2}

\begin{figure*}[t]
  \centering
  \begin{subfigure}[b]{0.45\textwidth}
%   \fbox{\rule{0pt}{1.5in} \rule{0.9\linewidth}{0pt}}
  \includegraphics[width=\linewidth]{images/plots/car_mig.pdf}
                % \includegraphics[width=.98\linewidth,height=.6\linewidth]{images/baseline-square.png}
                % \caption{Square}
                % \label{fig:baseline-square}
    \end{subfigure}%
    % \begin{subfigure}[b]{0.33\textwidth}
%   \fbox{\rule{0pt}{1.5in} \rule{0.9\linewidth}{0pt}}
% %   \includegraphics[width=0.8\linewidth]{egfigure.eps}
%                 % \includegraphics[width=.98\linewidth,height=.6\linewidth]{images/baseline-square.png}
%                 % \caption{Square}
%                 % \label{fig:baseline-square}
%     \end{subfigure}%
    \begin{subfigure}[b]{0.45\textwidth}
%   \fbox{\rule{0pt}{1.5in} \rule{0.9\linewidth}{0pt}}
  \includegraphics[width=\linewidth]{images/plots/cyc_mig.pdf}
%   \includegraphics[width=0.8\linewidth]{egfigure.eps}
                % \includegraphics[width=.98\linewidth,height=.6\linewidth]{images/baseline-square.png}
                % \caption{Square}
                % \label{fig:baseline-square}
    \end{subfigure}%

    \begin{subfigure}[b]{0.45\textwidth}
%   \fbox{\rule{0pt}{1.5in} \rule{0.9\linewidth}{0pt}}
%   \includegraphics[width=0.4\linewidth]{egfigure.eps}
  \includegraphics[width=\linewidth]{images/plots/car_mig_bev.pdf}
%   \includegraphics[width=0.8\linewidth]{egfigure.eps}
                % \includegraphics[width=.98\linewidth,height=.6\linewidth]{images/baseline-square.png}
                % \caption{Car-3D Detection (AP\_R40@0.70)}
                % \label{fig:baseline-square}
    \end{subfigure}%
%     \begin{subfigure}[b]{0.33\textwidth}
%   \fbox{\rule{0pt}{1.5in} \rule{0.9\linewidth}{0pt}}
% %   \includegraphics[width=0.8\linewidth]{egfigure.eps}
%                 % \includegraphics[width=.98\linewidth,height=.6\linewidth]{images/baseline-square.png}
%                 \caption{Pedestrian-3D Detection (AP\_R40@0.50)}
%                 % \label{fig:baseline-square}
%     \end{subfigure}%
    \begin{subfigure}[b]{0.45\textwidth}
%   \fbox{\rule{0pt}{1.5in} \rule{0.9\linewidth}{0pt}}
  \includegraphics[width=\linewidth]{images/plots/cyc_mig_bev.pdf}
%   \includegraphics[width=0.8\linewidth]{egfigure.eps}
                % \includegraphics[width=.98\linewidth,height=.6\linewidth]{images/baseline-square.png}
                % \caption{Cyclist-3D Detection (AP\_R40@0.50)}
                % \label{fig:baseline-square}
    \end{subfigure}%
    \vspace*{-3mm}
  \caption{The relationship between Car, Van and Cyclist detection performance  and surrogate metric of different LiDAR placements.}
  \label{fig:expt2}
   \vspace*{-5mm}
\end{figure*}

\textbf{Roll angles of sided LiDARs.} Orientation of the sided LiDARs are very common, so we investigate the influence of such placement variance in Table \ref{tab:expt4}.  It can be found that the placements of \textit{Line-roll} and \textit{Pyramid-roll} with roll angles  have worse detection performance as their S-MIG values are lower compared to \textit{Line}  and  \textit{Pyramid} respectively. The finding is consistent with detection metrics of both 3D and bird-eye-view, which further validates the effectiveness of our surrogate  metric and gives an example to fast evaluate the LiDAR placement for object detection.

\textbf{Density of surrounding road objects.}
We investigate how scenarios with different object densities affect detection performance and surrogate metric under \textit{Square} placement. Since under scenarios of different object densities, the POG is different and S-MIG cannot be directly used. So, the original information gain IG (\ref{IG}) is adopted to involve different POG entropy $H_{POG}$(\ref{totla_entropy}). From Table \ref{tab:expt5} we can see that the performance gets better when the density of cars increases because the information gain IG gets larger, which shows that our surrogate metric generalizes well to evaluate detection performance under different scenarios.






\textbf{Potential application analysis.}
% \todo{make it shorter}
Based on the correlation of our surrogate metric and detection performance, the evaluation of different LiDAR placement can be largely accelerated without following the time-consuming procedure: LiDAR installation $\rightarrow$ massive data collection $\rightarrow$ model training $\rightarrow$ evaluation of perception performance. Instead, we only needs a 3D bounding box dataset of object of interest to generate the POG and evaluate the LiDAR placement, which is fast and economical, and can also be customized in different deployment scenarios.

% we could accelerate the evaluation procedure of a particular LiDAR configuration in a city. In other words, using our surrogate cost, one does not need to .

Moreover, using the proposed surrogate metric, it is easy to optimize the LiDAR placement given the number of LiDARs and their beams under specific scenarios with the object of interest.  It can maximize the efficacy of the LiDAR sensor and relevant to the AV research community and industry since the current LiDAR placements are more or less intuition driven.
% Our approach could maximize the efficacy of the LiDAR sensor usage.
% AVs with LiDAR sensors have to process a large amount of point cloud data online, which brings a lot of computational burden and is not energy efficient. Therefore, s
% Since we could evaluate a LiDAR placement fast using our surrogate cost function in Equation \ref{SurM_MIG},
Besides, combining with some recent active perception work  \cite{bartels2019agile,ancha2020active,raaj2021exploiting,ancha2021active}, the proposed surrogate metric in this paper could serve as a guidance for those active sensors to focus on important areas around the AV for different scenarios and target objects.

% Medium case is the default one used in the previous experiments and the performance of Medium
% \todo{how scenarios with different density influence performance and surrogate metrics, or the relationship table 3}

% The results are presented in Table \ref{tab:expt1}, which clearly show that different LiDAR configurations will affect the perception performance because all other variables (number and type of LiDAR, environment, object trajectories, model training parameters, etc) are the same except the LiDAR placement. It is seen that the `\textbf{Square}' configuration, which is inspired by Apple's AV, outperforms other standards baselines in consideration.

% Furthermore, across the various baseline configurations and evaluation metrics, we find that models trained with optimal placement data outperform models trained on baseline configurations. We could see a large performance improvement with the optimized placement when compared with baselines ($10\% \sim 20\%$), which implies that our LiDAR placement optimization method could provide better point cloud quality and thus improve the perception performance. Figure~\ref{fig:iou-surrogate} shows the relation of our surrogate cost value with one of the perception evaluation metric IOU. We could see that the proposed information-theoretic cost is inversely correlated with the evaluation metric, which means that minimizing the surrogate cost could improve the perception capability. The second column in Figure~\ref{fig:compare-config} shows the point clouds of a certain scenario with different LiDAR configurations. We can see that the optimal placement leads to denser points on the car.


% \subsection{Ablation Study}
% \textbf{Different sizes of POG} current [-30,30][-20,20][-3,1]

% \todo{add tables}

% \textbf{Different weight for different objects}


% \todo{add tables}

% \textbf{Different LiDAR beams} current 16 beams

% \todo{add tables}

% \section{Applications}
% \label{applications}

% In this section we detail some of the applications and extensions of our methodology. We believe that the following subsections have direct relevance for AV researchers and manufacturers. We provide more experiments for some applications in the supplementary material.

% \subsection{Accelerated LiDAR placement evaluation}
% Since our surrogate cost is inversely correlated with the perception performance, we could accelerate the evaluation procedure of a particular LiDAR configuration in a city. In other words, using our surrogate cost, one does not need to follow the time-consuming procedure to get an evaluation of a LiDAR placement: LiDAR installation $\rightarrow$ massive data collection $\rightarrow$ perception model training $\rightarrow$ evaluation of perception performance. Instead, one only needs a 3D bounding box dataset of object of interests to generate the POG and evaluate the LiDAR placement, which is fast and economical, and could also be customized by different deployment scenarios.

% \begin{figure}[t!]
% \begin{center}
%  \includegraphics[width=\linewidth,height=0.5\linewidth]{images/iou-surrogate.pdf}
% \end{center}
% \vspace{-6mm}
%   \caption{Relation of overall mAP values of IOU (in \%) and the surrogate cost function.}
% \label{fig:iou-surrogate}
% \vspace{-4mm}
% \end{figure}


% \subsection{LiDAR placement optimization}
% \label{ref:lidar_placement_optimization}
% % Given the number of LiDARs, find the optimal placement
% Since we could evaluate a LiDAR placement fast using our surrogate cost function in Equation \ref{shannon}, it is now easy to optimize a LiDAR setup given the number of LiDARs (and their beams) in the setup. From Table~\ref{tab:expt1}, we could also see a great performance improvement by our optimized LiDAR placement.  This has direct relevance to the AV research community and industry since the current LiDAR placement is more or less intuition driven. Out approach could maximize the efficacy of the LiDAR sensor usage.

% \subsection{LiDAR number selection}
% \label{beam-number-selection}
% % How much LiDARs are enough for a particular perception algorithm
% %As seen in Table \ref{beam-number}, the surrogate cost function and `overall' values vary with number of beams of a LiDAR. This raises the question of how many beams should a LiDAR have for optimal sensing? Referring to Table \ref{beam-number}, we see that there is considerable improvement in IOU and AOS when number of beams increase from 2 to 4, at which point it reaches an elbow point until number beams increase to 7. Although, we observe an overall increase in IOU and AOS with increase in number of beams, the hardware required to process such high frequency data also increases in complexity. Therefore, based on the resources available, one could refer to Table \ref{beam-number} and Figure \ref{fig:iou-surrogate} to select an appropriate beam number for their LiDARs.
% With the proposed framework, we could easily evaluate the perception performance as the number of LiDARs increase and choose the desired one. For instance, given a type of LiDAR, we could find the optimized placement for $n$ LiDARs, where $n=\{1,2,3,...\}$, and compute the surrogate cost easily based on a POG. Then we plot the surrogate cost value versus the number of LiDARs and pick the elbow point number as the optimal choice, because after this point there would be marginal performance improvement by increasing the LiDAR number. Therefore, our method could help the AV manufactures choose the right balance between the sensor cost and perception capability and avoid unnecessary expenditure.

% % \subsection{LiDAR sensor design}
% \subsection{LiDAR beam angle optimization}
% \label{sec:beam_optimization}
% Since the beam angles of a LiDAR are also tunable, a logical follow-up question which arises following subsection \ref{ref:lidar_placement_optimization} is finding the beam angle which maximizes object detection. Specifically, we could regard a multi-beam LiDAR as a collection of multiple single beam LiDARs at the same position, then use our method to solve optimal beam angle design for the LiDARs such that the information gain is maximized. Hence, our method could also help the LiDAR manufactures design their sensor specifications based on different AV deployment scenarios.

% \subsection{Active LiDAR perception}
% AVs with LiDAR sensors have to process a large amount of point cloud data online, which brings a lot of computational burden and is not energy efficient. Therefore, some active depth sensors have been proposed to sense the environment dynamically, such as the light curtain sensor developed by Bartels \etal~\cite{bartels2019agile}. The proposed surrogate cost in this paper could serve as a guidance for those active sensors to focus on important areas around the AV based on different scenarios.


\section{Conclusion}
\label{conclusion}

This paper investigates the interplay between LiDAR placement and 3D detection performance. We proposed a novel efficient framework to evaluate multi-LiDAR placement and configuration for AVs.
We proposed a data-driven surrogate metric that characterizes the information gain in the conical perception areas, which helps accelerate the LiDAR placement evaluation procedure for LiDAR-based detection performance. Finally, we conducted extensive experiments in CARLA, validating the correlation between perception performance and surrogate metric of LiDAR configuration through representative 3D object detection algorithms.
Research in this paper sets a precedent for future work to optimize the placement of multiple LiDARs and co-design the sensor placement and perception algorithms.

% \clearpage

%%%%%%%%% REFERENCES
{\small
\bibliographystyle{ieee_fullname}
\bibliography{egbib}
}
\clearpage
\appendix
\section{Appendix - Details of Experimental Settings}
\label{sec:app}
% First of all, re-run optimized LiDAR config experiment for 4 16-beam LiDARs.
\subsection{Data Collection Details in CARLA}
We choose four different Towns for data collection from CARLA v0.9.10, which are shown in Figure \ref{fig:maps} and the number of frames in each town is about 11000 with 8 different routes covering all the main roads. To split objects, since CARLA itself does not separate Car, Van and Cyclist from Vehicles, we spawn all types of Vehicles and manually denote Van and Cyclist with the actor IDs of \textit{carlamotors.carlacola}, \textit{harley-davidson.low\_rider}, \textit{diamondback.century}, \textit{yamaha.yzf}, \textit{bh.crossbike}, \textit{kawasaki.ninja}  and \textit{gazelle.omafiets} while denoting the remaining as Cars. Note the box truck for \textit{carlamotors.carlacola} is with size over $5.2m \times 2.4m \times 2.6m$, which is the only too large van and categorized into Van and Cyclist, occupying about one tenth of frames in the abnormal-size class.

When collecting point cloud, we choose the frequency of simulation frame to be 20Hz for synchronization and run CARLA on two NVIDIA GeForce RTX 3090 GPUs with RAM 120G in a Ubuntu 18.04 docker container.
% This section provides an example to demonstrate accelerated LiDAR placement evaluation, which is described in Section 5.1. Since the proposed POG-based metric is correlated with perception performance, we can easily compare two LiDAR configurations by their surrogate costs. Note that the two LiDAR configurations could have different number of LiDARs and even different types of LiDARs.

% For instance, consider an AV company which has two LiDAR configuration candidates $A$ and $B$ with the same price, where $A$ is 4 10-beam LiDARs and $B$ is 1 40-beam LiDAR. Since their number of beams is equal, it is hard to tell which one will perform better given the company's point cloud perception algorithm.

% One traditional way to evaluate the two candidates is to install those LiDARs on the vehicle, collect and annotate point cloud data, train the perception model, and finally obtain the perception performance. However, there are two major drawbacks for this approach: 1) Since we have showed that the sensor placement will influence the perception performance, how to install those LiDARs on the vehicle to fairly compare them? 2) The whole procedure requires a lot of time and labor, which introduces additional deployment costs.

% In contrast, with our approach, we can easily optimize the placements for both $A$ and $B$ and compare their optimal surrogate costs. In this way, the evaluation procedure could be done in several hours. For example, we still consider the CARLA simulation environment and we have the same POG as we used in Section 4. Then we optimize the placements for $A$ and $B$ and get the optimal surrogate costs $cost(A) = -1383.685 , cost(B) = -756.337$. Then, we know that $A$ (4 10-beam LiDARs) is better then $B$ (1 40-beam LiDAR) because $cost(A)<cost(B)$ with optimal placement, which signifies that $A$ could gain more information than $B$.




\subsection{LiDAR Placement Details}
The ego-vehicle has its coordinate frame at its geometric center at $[40,20,0,0,0]$ with respect to the ROI frame of reference. All LiDAR configurations are illustrated in Figure \ref{fig:lidar-placement}, and their detailed coordinates are given in Table {\ref{detailed-coords}}. These coordinates are with respect to the ego vehicle's coordinate frame and will be transformed to ROI framework, as shown in Figure {\ref{fig:all-frames}}.  Note that the coordinate frames in CARLA are left-handed, and we make the extra transformation in ROI when calculating the POG and surrogate metric.



\subsection{Training Details of Detection Algorithms}
We use representative algorithms from OpenPCDet \cite{openpcdet2020} to evaluate the performance under different LiDAR placements. We keep all the models' hyperparameters the same as the default KITTI configuration files and change the optimization parameters to fine-tune the pre-trained models using the collected data from different LiDAR placements in CARLA.
Details of the optimization hyper-parameters are given in Table {\ref{table:hyper-param}}. Since only the front half of the point cloud is used to fine-tune the detection models, we change \texttt{POINT\_CLOUD\_RANGE} to be $[0, -20, -3, 40, 20, 1]$ as well. We ensure that the hyper-parameters are the same for all experiments to fairly compare the detection performance, and the detection performance is tested at epoch 10 for all models.

\begin{figure}[t]
  \centering
  \begin{subfigure}[b]{0.2\textwidth}
%   \fbox{\rule{0pt}{1.5in} \rule{0.9\linewidth}{0pt}}
  \includegraphics[width=0.95\linewidth]{images/Town01.jpg}
                % \includegraphics[width=.98\linewidth,height=.6\linewidth]{images/baseline-square.png}
                \caption{Town 1}
                % \label{fig:baseline-square}
    \end{subfigure}%
  \begin{subfigure}[b]{0.2\textwidth}
%   \fbox{\rule{0pt}{1.5in} \rule{0.9\linewidth}{0pt}}
  \includegraphics[width=0.95\linewidth]{images/Town03.jpg}
                % \includegraphics[width=.98\linewidth,height=.6\linewidth]{images/baseline-square.png}
                \caption{Town 3}
                % \label{fig:baseline-square}
    \end{subfigure}%

      \begin{subfigure}[b]{0.2\textwidth}
%   \fbox{\rule{0pt}{1.5in} \rule{0.9\linewidth}{0pt}}
  \includegraphics[width=0.95\linewidth]{images/Town04.jpg}
                % \includegraphics[width=.98\linewidth,height=.6\linewidth]{images/baseline-square.png}
                \caption{Town 4}
                % \label{fig:baseline-square}
    \end{subfigure}%
      \begin{subfigure}[b]{0.2\textwidth}
%   \fbox{\rule{0pt}{1.5in} \rule{0.9\linewidth}{0pt}}
  \includegraphics[width=0.95\linewidth]{images/Town06.jpg}
                % \includegraphics[width=.98\linewidth,height=.6\linewidth]{images/baseline-square.png}
                \caption{Town 6}
                % \label{fig:baseline-square}
    \end{subfigure}%
  \caption{The four diverse town maps we used to collect data and conduct experiments in CARLA v0.9.10.}
  \label{fig:maps}
   \vspace*{-5mm}
\end{figure}


\begin{figure}[t]
\begin{center}
 \includegraphics[width=0.87\linewidth]{images/all-frames.pdf}
\end{center}
\vspace*{-5mm}
   \caption{Coordinate frames of ROI, the ego-vehicle, and the LiDAR. Figure not to scale.}
\label{fig:all-frames}
\vspace*{-4mm}
\end{figure}

\begin{figure*}[t]
  \centering
    % \begin{subfigure}[b]{0.33\textwidth}
%   \fbox{\rule{0pt}{1.5in} \rule{0.9\linewidth}{0pt}}
% %   \includegraphics[width=0.8\linewidth]{egfigure.eps}
%                 % \includegraphics[width=.98\linewidth,height=.6\linewidth]{images/baseline-square.png}
%                 % \caption{Square}
%                 % \label{fig:baseline-square}
%     \end{subfigure}%
    \begin{subfigure}[b]{0.23\textwidth}
%   \fbox{\rule{0pt}{1.5in} \rule{0.9\linewidth}{0pt}}
  \includegraphics[width=\linewidth,height=.58\linewidth]{images/line_norot.png}
%   \includegraphics[width=0.8\linewidth]{egfigure.eps}
                % \includegraphics[width=.98\linewidth,height=.6\linewidth]{images/baseline-square.png}
                \caption{Line}
                % \label{fig:baseline-square}
    \end{subfigure}%
    \begin{subfigure}[b]{0.23\textwidth}
%   \fbox{\rule{0pt}{1.5in} \rule{0.9\linewidth}{0pt}}
  \includegraphics[width=\linewidth,height=.58\linewidth]{images/baseline-center.png}
                % \includegraphics[width=.98\linewidth,height=.6\linewidth]{images/baseline-square.png}
                \caption{Center}
                % \label{fig:baseline-square}
    \end{subfigure}%
    \begin{subfigure}[b]{0.23\textwidth}
%   \fbox{\rule{0pt}{1.5in} \rule{0.9\linewidth}{0pt}}
  \includegraphics[width=\linewidth,height=.58\linewidth]{images/trapezoid.png}
%   \includegraphics[width=0.8\linewidth]{egfigure.eps}
                % \includegraphics[width=.98\linewidth,height=.6\linewidth]{images/baseline-square.png}
                \caption{Trapezoid}
                % \label{fig:baseline-square}
    \end{subfigure}%
    \begin{subfigure}[b]{0.23\textwidth}
%   \fbox{\rule{0pt}{1.5in} \rule{0.9\linewidth}{0pt}}
  \includegraphics[width=\linewidth,height=.58\linewidth]{images/baseline-square.png}
%   \includegraphics[width=0.8\linewidth]{egfigure.eps}
                % \includegraphics[width=.98\linewidth,height=.6\linewidth]{images/baseline-square.png}
                \caption{Square}
                % \label{fig:baseline-square}
    \end{subfigure}%

    \begin{subfigure}[b]{0.23\textwidth}
%   \fbox{\rule{0pt}{1.5in} \rule{0.9\linewidth}{0pt}}
  \includegraphics[width=\linewidth,height=.58\linewidth]{images/baseline-line.png}
%   \includegraphics[width=0.8\linewidth]{egfigure.eps}
                % \includegraphics[width=.98\linewidth,height=.6\linewidth]{images/baseline-square.png}
                \caption{Line-roll}
                % \label{fig:baseline-square}
    \end{subfigure}%
    \begin{subfigure}[b]{0.23\textwidth}
%   \fbox{\rule{0pt}{1.5in} \rule{0.9\linewidth}{0pt}}
  \includegraphics[width=\linewidth]{images/triangle.png}
%   \includegraphics[width=0.8\linewidth]{egfigure.eps}
                % \includegraphics[width=.98\linewidth,height=.6\linewidth]{images/baseline-square.png}
                \caption{Pyramid}
                % \label{fig:baseline-square}
    \end{subfigure}%
    \begin{subfigure}[b]{0.23\textwidth}
%   \fbox{\rule{0pt}{1.5in} \rule{0.9\linewidth}{0pt}}
  \includegraphics[width=\linewidth]{images/triangle_roll.png}
%   \includegraphics[width=0.8\linewidth]{egfigure.eps}
                % \includegraphics[width=.98\linewidth,height=.6\linewidth]{images/baseline-square.png}
                \caption{Pyramid-roll}
                % \label{fig:baseline-square}
    \end{subfigure}%
    \begin{subfigure}[b]{0.23\textwidth}
%   \fbox{\rule{0pt}{1.5in} \rule{0.9\linewidth}{0pt}}
  \includegraphics[width=\linewidth]{images/triangle_pitch.png}
%   \includegraphics[width=0.8\linewidth]{egfigure.eps}
                % \includegraphics[width=.98\linewidth,height=.6\linewidth]{images/baseline-square.png}
                \caption{Pyramid-pitch}
                % \label{fig:baseline-square}
    \end{subfigure}%
%     \begin{subfigure}[b]{0.45\textwidth}
% %   \fbox{\rule{0pt}{1.5in} \rule{0.9\linewidth}{0pt}}
% %   \includegraphics[width=0.4\linewidth]{egfigure.eps}
%   \includegraphics[width=\linewidth]{images/plots/car_mig_bev.pdf}
% %   \includegraphics[width=0.8\linewidth]{egfigure.eps}
%                 % \includegraphics[width=.98\linewidth,height=.6\linewidth]{images/baseline-square.png}
%                 % \caption{Car-3D Detection (AP\_R40@0.70)}
%                 % \label{fig:baseline-square}
%     \end{subfigure}%
% %     \begin{subfigure}[b]{0.33\textwidth}
% %   \fbox{\rule{0pt}{1.5in} \rule{0.9\linewidth}{0pt}}
% % %   \includegraphics[width=0.8\linewidth]{egfigure.eps}
% %                 % \includegraphics[width=.98\linewidth,height=.6\linewidth]{images/baseline-square.png}
% %                 \caption{Pedestrian-3D Detection (AP\_R40@0.50)}
% %                 % \label{fig:baseline-square}
% %     \end{subfigure}%
%     \begin{subfigure}[b]{0.45\textwidth}
% %   \fbox{\rule{0pt}{1.5in} \rule{0.9\linewidth}{0pt}}
%   \includegraphics[width=\linewidth]{images/plots/cyc_mig_bev.pdf}
% %   \includegraphics[width=0.8\linewidth]{egfigure.eps}
%                 % \includegraphics[width=.98\linewidth,height=.6\linewidth]{images/baseline-square.png}
%                 % \caption{Cyclist-3D Detection (AP\_R40@0.50)}
%                 % \label{fig:baseline-square}
%     \end{subfigure}%
\vspace*{-3mm}
  \caption{Illustrations of different multi-LiDAR placements used in the experiments}
  \label{fig:lidar-placement}
   \vspace*{-5mm}
\end{figure*}

\begin{table}[ht!]
\centering
\begin{tabular}{|c||c|c|c|c|c|}
\hline Placement
 & x & y & z & roll & pitch \\ \hline
 \multirow{4}{*}{Line} &
\multirow{4}{*}{\makecell{-0.0 \\ 0.0 \\   0.0 \\ 0.0}}  &  \multirow{4}{*}{\makecell{  -0.6 \\ -0.4 \\ 0.4 \\ 0.6}}  &  \multirow{4}{*}{\makecell{  2.2 \\ 2.2  \\   2.2 \\ 2.2}}  &  \multirow{4}{*}{\makecell{  0.0 \\ 0.0  \\   0.0 \\ 0.0}}  &  \multirow{4}{*}{\makecell{  0.0 \\ 0.0  \\   0.0 \\ 0.0}}  \\
& & & & & \\
& & & & & \\
& & & & & \\ \hline
\multirow{4}{*}{Center} &
\multirow{4}{*}{\makecell{0.0  \\ 0.0  \\ 0.0 \\ 0.0}}  &  \multirow{4}{*}{\makecell{0.0  \\ 0.0  \\ 0.0 \\ 0.0}}  &  \multirow{4}{*}{\makecell{2.4  \\ 2.6  \\ 2.8 \\ 3.0}}  &  \multirow{4}{*}{\makecell{0.0  \\ 0.0 \\ 0.0 \\ 0.0}}  &  \multirow{4}{*}{\makecell{0.0  \\ 0.0  \\ 0.0 \\ 0.0}}  \\
& & & & & \\
& & & & & \\
& & & & & \\ \hline
\multirow{4}{*}{Trapezoid} &
\multirow{4}{*}{\makecell{-0.4\\ -0.4\\ 0.2\\ 0.2}} &  \multirow{4}{*}{\makecell{  0.2\\ -0.2\\ 0.5\\ -0.5}} &  \multirow{4}{*}{\makecell{  2.2  \\   2.2 \\ 2.2 \\   2.2}} &  \multirow{4}{*}{\makecell{  0.0  \\   0.0 \\ 0.0 \\   0.0}} &  \multirow{4}{*}{\makecell{  0.0  \\   0.0 \\ 0.0 \\   0.0}} \\
& & & & & \\
& & & & & \\
& & & & & \\ \hline
\multirow{4}{*}{Square} &
\multirow{4}{*}{\makecell{-0.5  \\ -0.5 \\ 0.5 \\   0.5}} &  \multirow{4}{*}{\makecell{  0.5  \\ - 0.5 \\ 0.5 \\ -0.5}} &  \multirow{4}{*}{\makecell{  2.2  \\   2.2 \\ 2.2 \\   2.2}} &  \multirow{4}{*}{\makecell{  0.0  \\   0.0 \\ 0.0 \\   0.0}} &  \multirow{4}{*}{\makecell{  0.0  \\   0.0 \\ 0.0 \\   0.0}} \\
& & & & & \\
& & & & & \\
& & & & & \\ \hline

 \multirow{4}{*}{Line-roll} &
\multirow{4}{*}{\makecell{- 0.0 \\ 0.0 \\   0.0 \\ 0.0}}  &  \multirow{4}{*}{\makecell{  -0.6 \\ -0.4 \\ 0.4 \\ 0.6}}  &  \multirow{4}{*}{\makecell{  2.2 \\ 2.2  \\   2.2 \\ 2.2}}  &  \multirow{4}{*}{\makecell{  -0.28 \\ 0.0  \\   0.0 \\ 0.28}}  &  \multirow{4}{*}{\makecell{  0.0 \\ 0.0  \\   0.0 \\ 0.0}}  \\
& & & & & \\
& & & & & \\
& & & & & \\ \hline
 \multirow{4}{*}{Pyramid} &
\multirow{4}{*}{\makecell{-0.2\\0.4\\ -0.2\\ -0.2}}  &  \multirow{4}{*}{\makecell{  -0.6\\ 0.0\\ 0.0\\ 0.6}}  &  \multirow{4}{*}{\makecell{  2.2\\ 2.4\\ 2.6\\ 2.2}}  &  \multirow{4}{*}{\makecell{  0.0 \\ 0.0  \\   0.0 \\ 0.0}}  &  \multirow{4}{*}{\makecell{  0.0 \\ 0.0  \\   0.0 \\ 0.0}}  \\
& & & & & \\
& & & & & \\
& & & & & \\ \hline
 \multirow{4}{*}{Pyramid-roll} &
\multirow{4}{*}{\makecell{-0.2\\0.4\\ -0.2\\ -0.2}}  &  \multirow{4}{*}{\makecell{  -0.6\\ 0.0\\ 0.0\\ 0.6}}  &  \multirow{4}{*}{\makecell{  2.2\\ 2.4\\ 2.6\\ 2.2}}  &  \multirow{4}{*}{\makecell{  -0.28 \\ 0.0  \\   0.0 \\ 0.28}}  &  \multirow{4}{*}{\makecell{  0.0 \\ 0.0  \\   0.0 \\ 0.0}}  \\
& & & & & \\
& & & & & \\
& & & & & \\ \hline
 \multirow{4}{*}{Pyramid-pitch} &
\multirow{4}{*}{\makecell{-0.2\\0.4\\ -0.2\\ -0.2}}  &  \multirow{4}{*}{\makecell{  -0.6\\ 0.0\\ 0.0\\ 0.6}}  &  \multirow{4}{*}{\makecell{  2.2\\ 2.4\\ 2.6\\ 2.2}}  &  \multirow{4}{*}{\makecell{  0.0 \\ 0.0  \\   0.0 \\ 0.0}}  &  \multirow{4}{*}{\makecell{  0.0 \\ -0.09  \\   0.0 \\ 0.0}}  \\
& & & & & \\
& & & & & \\
& & & & & \\ \hline
\end{tabular}
\caption{Coordinates of LiDAR sensors with respect to the ego-vehicle coordinate frame. All values of \textit{x,y,z} are in meters and roll and pitch angles are in \textit{rad}.}
\label{detailed-coords}
\vspace*{-3mm}
\end{table}

\begin{table}[h]
\centering
\begin{tabular}{|c|c|}
\hline
Hyperparameter & Value \\ \hline
Epochs & 10 \\ \hline
Optimizer & adam\_onecycle \\ \hline
Learning Rate & 0.01 \\ \hline
Weight Decay: & 0.01 \\ \hline
Momentum: & 0.9 \\ \hline
Learning Rate Clip & 0.0000001 \\ \hline
Learning Rate Decay & 0.1 \\ \hline
Div Factor & 10 \\ \hline
Warmup Epoch & 1 \\ \hline
Learning Rate Warmup & False \\ \hline
Gradient Norm Clip & 10 \\ \hline
MOMS & {[}0.95, 0.85{]} \\ \hline
PCT\_START & 0.1 \\ \hline
% Decay Step List & {[}35, 45{]} \\
\end{tabular}
\caption{Hypeparameters for optimization in model training}
\label{table:hyper-param}
\end{table}


\begin{table*}[]
\centering
\resizebox{0.9\textwidth}{!}{
\begin{tabular}{|c||c|c|c|c||c|c|c|c|}
\hline
\multirow{2}{*}{Models}          & \multicolumn{4}{c||}{ Recall rcnn @0.50 IoU}                                          & \multicolumn{4}{c|}{Recall rcnn @0.70 IoU}                                 \\ \cline{2-9}
                                    & Center & Line & Pyramid & Trapezoid   & Center & Line &  Pyramid & Trapezoid \\ \hline
PV-RCNN  \cite{shi2020pv}
% &  28.97
     &     0.5834&	0.6009&	0.6260&	0.6103
%  &   34.09
 &	0.4192&	0.4330&	0.4610&	0.4404
                    \\ \hline
Voxel RCNN  \cite{deng2020voxel}
% &   26.06
&0.5686&	0.5858&	0.6112&	0.5929
%   &                   31.31
   & 0.3901&	0.4093&	0.4299&	0.4172
                                                 \\ \hline
% PointPillar  \cite{lang2019pointpillars}                    &   40.95     &    40.55    &   39.68   &    \textbf{40.97}                                                        &   36.62     &   34.72    &   34.46   &     \textbf{36.99}                                                       \\ \hline
PointRCNN \cite{Shi_2019_CVPR}
% & 23.55
&0.4462&	0.4584&	0.4722&	0.4593
% &      28.97
& 0.3030&	0.3346&	0.3321&	0.3392
                 \\ \hline
PointRCNN-IoU \cite{Shi_2019_CVPR}
% & 23.37
&	0.4437&	0.4633&	0.4706&	0.4597
% &   28.70
&0.3020&	0.3300&	0.3325&	0.3373
                         \\ \hline
SECOND \cite{yan2018second}
% & 24.84
& 0.4590&	0.4792&	0.5035&	0.4835
% &       31.19
&0.2827&	0.2960&	0.3104&	0.3050
                                \\ \hline
SECOND-IoU  \cite{yan2018second}
% & 25.28
&	0.5788&	0.5944&	0.6249&	0.6052
%  &   31.46
 &	0.3726&	0.3838&	0.4107&	0.3966
                                 \\ \hline
% Part-$A^2$-Free \cite{shi2020points}  &        &        &      &                                                            &        &        &      &                                                            \\ \hline
% Part-$A^2$-Anchor \cite{shi2020points} &        &        &      &                                                            &        &        &      &                                                            \\ \hline
% \end{tabular}}
% \resizebox{\textwidth}{!}{
% \begin{tabular}{|c||c|c|c|c||c|c|c|c|}
% \hline
% \multirow{2}{*}{Models}          & \multicolumn{4}{c||}{ Cyclist-3D  (AP\_R40@0.50)}                                          & \multicolumn{4}{c|}{Cyclist-BEV (AP\_R40@0.50)}                                 \\ \cline{2-9} &
% % Square   &
% Center & Line & Pyramid & Trapezoid &
% % Square   &
% Center & Line &  Pyramid & Trapezoid \\ \hline
% PV-RCNN  \cite{shi2020pv}
% % &  \textbf{26.28}
% &	40.09 &	39.11 &	40.26 &	\textbf{42.85}
% %  &   \textbf{26.67}
%  &	41.18&	41.45&	43.09&	\textbf{44.79}
%                   \\ \hline
% Voxel RCNN  \cite{deng2020voxel}
% % &   \textbf{21.59}
% &	33.76&	31.60&	33.39&	\textbf{33.91}
% %   &                     \textbf{22.11}
%   &	35.40&	33.24&	35.11&	\textbf{35.54}
%                   \\ \hline
% % PointPillar  \cite{lang2019pointpillars}                    &   40.95     &    40.55    &   39.68   &    \textbf{40.97}                                                        &   36.62     &   34.72    &   34.46   &     \textbf{36.99}                                                       \\ \hline
% PointRCNN \cite{Shi_2019_CVPR}
% % & \textbf{18.82}
% &	\textbf{31.43}&	28.00&	27.10&	30.91
% %   & \textbf{20.29}
%   &	\textbf{33.86}&	30.68&	30.49&	33.75
%                       \\ \hline
% PointRCNN-IoU \cite{Shi_2019_CVPR}
% % & \textbf{18.49}
% &	\textbf{30.75}&	27.64&	26.19&	30.54
% %  &  \textbf{20.57}
%  &	34.04&	30.71&	29.30&	\textbf{34.70}
%                   \\ \hline
% SECOND \cite{yan2018second}
% % & \textbf{22.46}
% &	\textbf{36.13}&	32.36&	35.32&	35.95
% %  &  \textbf{24.34}
%  &	38.19&	30.68&	\textbf{40.84}&	40.01
%                      \\ \hline
% SECOND-IoU  \cite{yan2018second}
% % & \textbf{23.37}
% &	35.61&	33.12&	34.95&	\textbf{36.51}
% % &    \textbf{24.90}
% &	38.99&	36.60&	38.31&	\textbf{40.14}
%                         \\ \hline
% Part-$A^2$-Free \cite{shi2020points}  &        &        &      &                                                            &        &        &      &                                                            \\ \hline
% Part-$A^2$-Anchor \cite{shi2020points} &        &        &      &                                                            &        &        &      &                                                            \\ \hline
% \end{tabular}}
% \resizebox{\textwidth}{!}{
% \begin{tabular}{|c||c|c|c|c||c|c|c|c|}
% \hline
% \multirow{2}{*}{Models}          & \multicolumn{5}{c||}{Cyclist-BEV (AP\_R40@0.50)}                                          & \multicolumn{5}{c|}{Cyclist-3D Detection (AP\_R40@0.50)}                                 \\ \cline{2-11}
%                                  & Square   & Center & Line & Pyramid & Trapezoid & Square   & Center & Line &  Pyramid & Trapezoid \\ \hline
% PV-RCNN  \cite{shi2020pv}                        &  56.69     &      &   53.96  & 61.38 &    \textbf{62.25}                                                &   44.42           &   47.06     & 45.92 &   \textbf{51.52 }                   &                              \\ \hline
% Voxel RCNN  \cite{deng2020voxel}                     &   52.38   &            & 50.66 &  56.64   &      \textbf{59.07}                              &                      &    34.65               &  38.45&  40.14  &  \textbf{46.49 }                                                     \\ \hline
% % PointPillar  \cite{lang2019pointpillars}                    &   40.95     &    40.55    &   39.68   &    \textbf{40.97}                                                        &   36.62     &   34.72    &   34.46   &     \textbf{36.99}                                                       \\ \hline
% PointRCNN \cite{Shi_2019_CVPR}               & &        &    43.50   &         36.16 &  47.19   &      \textbf{47.98  }                                                   & 27.30               & 23.82 &  27.83   &   \textbf{30.03 }                                                      \\ \hline
% PointRCNN-IoU \cite{Shi_2019_CVPR}          & &           &  42.07                 & 33.90 & \textbf{47.40}  &   46.90                                                         &  27.53               & 18.00 &    26.73  &  \textbf{31.65 }                                                       \\ \hline
% SECOND \cite{yan2018second}             & &             &    48.01    &           50.68 & 51.27 &       \textbf{56.82 }                                                   &  33.60                 & 38.51 & 32.83   &   \textbf{45.25 }                                                       \\ \hline
% SECOND-IoU  \cite{yan2018second}              & &          &    47.99                &   50.32   & 50.36 &   \textbf{56.92 }                                                 &    36.33             & 40.74 &    35.81  &   \textbf{43.42 }                                                      \\ \hline
% Part-$A^2$-Free \cite{shi2020points}  &        &        &      &                                                            &        &        &      &                                                            \\ \hline
% Part-$A^2$-Anchor \cite{shi2020points} &        &        &      &                                                            &        &        &      &                                                            \\ \hline
\end{tabular}}
% \resizebox{\textwidth}{!}{
% \begin{tabular}{|c||c|c|c|c|c|c||c|c|c|c|c|c|}
% \hline
% \multirow{3}{*}{Models}          & \multicolumn{6}{c||}{Car-BEV (AP\_R40@0.70)}                                          & \multicolumn{6}{c|}{Car-3D Detection (AP\_R40@0.70)}                                 \\ \cline{2-13}
%                                  & Square & \makecell[c]{Vertical \\ Center 2} & Diamod  & \makecell[c]{Vertical \\ Center 4} & Line & \makecell[c]{Optimized \\ Ours} & Square & \makecell[c]{Vertical \\ Center 2} & Diamod  & \makecell[c]{Vertical \\ Center 4} & Line &  \makecell[c]{Optimized \\ Ours} \\ \hline
% PV-RCNN  \cite{shi2020pv}                        &  68.47      &    66.10    &   68.52   &   68.70  & 66.51 &    66.61                                                   &   63.33     &   60.91     &   63.72   &   63.22     & 63.18 &   63.27                                                   \\ \hline
% Voxel RCNN  \cite{deng2020voxel}                     &   65.99     &   63.55     &  66.37 & 68.29 &  65.68   &      65.33                                                     &  60.34      &   58.26     &  62.89  & 62.85 &  60.01  &  60.13                                                        \\ \hline
% % PointPillar  \cite{lang2019pointpillars}                    &   40.95     &    40.55    &   39.68   &    \textbf{40.97}                                                        &   36.62     &   34.72    &   34.46   &     \textbf{36.99}                                                       \\ \hline
% PointRCNN \cite{Shi_2019_CVPR}                       &    61.07    &   52.31     & 61.16  & 58.24 &  56.14    &      58.72                                                      &  53.48      &    43.12    & 55.78  & 50.35 &  47.87   &   50.86                                                         \\ \hline
% PointRCNN-IoU \cite{Shi_2019_CVPR}                     &  60.80      &   55.51     & 54.38    & 57.43 & 57.60  &   58.49                                                         &  53.18      &  50.02      &  49.82 & 49.77 &    49.42  &   52.70                                                         \\ \hline
% SECOND \cite{yan2018second}                          &    66.01    &  63.09      &   63.60  & 67.81 & 65.83 &       65.24                                                     &  59.41      &   57.24     &  57.86   & 61.80 & 57.43   &   59.32                                                         \\ \hline
% SECOND-IoU  \cite{yan2018second}                        &    63.71    &    62.68    &      63.36 &   62.67   & 63.77 &   63.52                                                   &     57.83   &   57.93     & 58.08 & 56.24 &    57.45  &   58.28                                                         \\ \hline
% % Part-$A^2$-Free \cite{shi2020points}  &        &        &      &                                                            &        &        &      &                                                            \\ \hline
% % Part-$A^2$-Anchor \cite{shi2020points} &        &        &      &                                                            &        &        &      &                                                            \\ \hline
% \end{tabular}}
% \resizebox{\textwidth}{!}{
% \begin{tabular}{|c||c|c|c|c|c|c||c|c|c|c|c|c|}
% \hline
% \multirow{3}{*}{Models}          & \multicolumn{6}{c||}{Pedestrian-BEV (AP\_R40@0.50)}                                          & \multicolumn{6}{c|}{Pedestrian-3D Detection (AP\_R40@0.50)}                                 \\ \cline{2-13}
%                                  & Square & \makecell[c]{Vertical \\ Center 2} & Diamod  &\makecell[c]{Vertical \\ Center 4} & Line & \makecell[c]{Optimized \\ Ours} & Square & \makecell[c]{Vertical \\ Center 2} & Diamod  & \makecell[c]{Vertical \\ Center 4} & Line &  \makecell[c]{Optimized \\ Ours} \\ \hline
% PV-RCNN  \cite{shi2020pv}                        &   48.37     &  46.66      &   46.15   & 45.72 & 47.35 &                                                            &   43.98      &   42.24     &   41.70  & 39.62 &   42.48 &                                                            \\ \hline
% Voxel RCNN  \cite{deng2020voxel}                     &  42.15      &   40.19     &  42.65   & 41.13 &  42.48    &                                                          &  37.96     &  36.15      &   38.35   & 36.79 &  37.57 &                                                     \\ \hline
% % PointPillar  \cite{lang2019pointpillars}                    &  20.81     &    20.49    &   20.09   &   \textbf{22.89}                                                         &  13.52      &  16.07      & 15.62     &   \textbf{17.32}                                                         \\ \hline
% PointRCNN  \cite{Shi_2019_CVPR}                       &   27.46      &    22.21    &  26.02 & 22.09 &  24.17    &                                                            & 23.67       &    19.32    & 21.93   & 19.38 &  19.65    &                                                            \\ \hline
% PointRCNN-IoU  \cite{Shi_2019_CVPR}                   &    29.09    &   27.94     & 23.20  & 22.32 &  26.01    &                                                            &   23.66     &    23.81    & 18.16 & 19.25  &    20.23   &                                                            \\ \hline
% SECOND  \cite{yan2018second}                          &   46.27     &   45.23     &  45.59   & 44.95 & 46.04  &                                                            &   40.45     &   40.27     &  39.52   & 38.01 &  38.48  &                                                            \\ \hline
% SECOND-IoU  \cite{yan2018second}                      &    39.26    &    36.67    & 37.12  & 32.45 &   38.09   &                                                            &   34.66     &   32.59     & 32.12  & 26.76 &  31.58    &                                                            \\ \hline
% % Part-$A^2$-Free \cite{shi2020points}  &        &        &      &                                                            &        &        &      &                                                            \\ \hline
% % Part-$A^2$-Anchor \cite{shi2020points} &        &        &      &                                                            &        &        &      &                                                            \\ \hline
% \end{tabular}}
% \resizebox{\textwidth}{!}{
% \begin{tabular}{|c||c|c|c|c|c|c||c|c|c|c|c|c|}
% \hline
% \multirow{3}{*}{Models}          & \multicolumn{6}{c||}{Cyclist-BEV (AP\_R40@0.50)}                                          & \multicolumn{6}{c|}{Cyclist-3D Detection (AP\_R40@0.50)}                                 \\ \cline{2-13}
%                                  & Square & \makecell[c]{Vertical \\ Center 2} & Diamod  & \makecell[c]{Vertical \\ Center 4} & Line & \makecell[c]{Optimized \\ Ours} & Square & \makecell[c]{Vertical \\ Center 2} & Diamod  & \makecell[c]{Vertical \\ Center 4} & Line &  \makecell[c]{Optimized \\ Ours} \\ \hline
% PV-RCNN \cite{shi2020pv}                          &  53.05      &   57.00     &  53.07   & 54.97 &  61.06  &                                                            &    50.86    &   56.41     &   52.70   & 52.75 &  58.96 &                                                            \\ \hline
% Voxel RCNN  \cite{deng2020voxel}                      &  48.46      &   53.57     &   51.27   & 51.71 & 57.24  &                                                           &  46.96     &  52.66      &   49.18  & 49.59 &  56.59  &                                                          \\ \hline
% % PointPillar  \cite{lang2019pointpillars}                    &     26.07   &   \textbf{30.94}     &   27.15   &  28.37                                                          &  23.55      &  \textbf{28.51}      &   24.90   &   25.80                                                         \\ \hline
% PointRCNN  \cite{Shi_2019_CVPR}                      &   38.52    &    33.93    &  37.92  & 33.63 &  43.83   &                                                            &  36.33      &     31.78   &  35.57 & 31.47 &  41.55    &                                                            \\ \hline
% PointRCNN-IoU \cite{Shi_2019_CVPR}                   &   38.53     &    41.81    & 28.99  & 34.27 &   45.46   &                                                            &   36.20     &    38.54    & 27.57   & 31.98 &  41.48   &                                                            \\ \hline
% SECOND  \cite{yan2018second}                          &   48.63     &  53.57      &  53.62   & 53.19 &  56.62  &                                                            &   45.62     &   50.47     &  50.94   & 50.60 &  54.51  &                                                            \\ \hline
% SECOND-IoU \cite{yan2018second}                       &   49.74     &    53.14    & 50.52  & 48.25 &  57.79   &                                                            &    47.33    &   52.11     & 48.32  & 45.77 &   55.59   &                                                            \\ \hline
% % Part-$A^2$-Free  \cite{shi2020points} &        &        &      &                                                            &        &        &      &                                                            \\ \hline
% % Part-$A^2$-Anchor \cite{shi2020points} &        &        &      &                                                            &        &        &      &                                                            \\ \hline
% \end{tabular}}
\caption{Comparison of recall performance under various LiDAR configurations  using different algorithms.
% Detection metric of mean average precision is computed by averaging precision across 40 recall positions (AP\_R40) for  3D detection (3D IoU) and bird-eye-view (BEV) under IOU thresholds of 0.7 for Car and 0.5 for Cyclist from KITTI \cite{geiger2013vision}.
% \todo{explain what does @70 @50 @50 mean! in the text, caption getting too long}
% }
}
\label{tab:recall_compare}
\vspace*{-3mm}
\end{table*}



\section{Appendix - More Experimental Results and Analysis}
\label{app:more_ressults}
\subsection{Recall under Different LiDAR Configurations}
Besides the average precision to show the detection performance, we consider using the overall recall (rcnn) with IoU of 0.5 and 0.7 to show the detection performance of all the objects and validate the relationship between our surrogate metric. The comparison of recall performance under different LiDAR placements can be found in Table \ref{tab:recall_compare}. We can see the recall metric varies a lot under different LiDAR placements for the same detection models. Specifically, \textit{Pyramid} almost gets the best performance for all the algorithms with IoU of both 0.5 and 0.7, which is different from the performance of average precision in Table \ref{tab:expt1}.

From Figure \ref{fig:recall_relation}, we can find the increasing trend between recall and our surrogate metric as well. Note that since the recall is calculated for all Cars, Vans and Cyclists, our total information gain surrogate metric is the sum of S-MIG of Car, Van and Cyclist. Furthermore, it can be seen that the performance variance under different LiDAR placements does not decrease as the IoU is going less, showing that the influence of LiDAR placement is consistent with the recall metrics, as the surrogate metric shows.

\begin{figure*}[t]
  \centering
  \begin{subfigure}[b]{0.48\textwidth}
%   \fbox{\rule{0pt}{1.5in} \rule{0.9\linewidth}{0pt}}
  \includegraphics[width=\linewidth]{images/plots/recall05.pdf}
                % \includegraphics[width=.98\linewidth,height=.6\linewidth]{images/baseline-square.png}
                % \caption{Square}
                % \label{fig:baseline-square}
    \end{subfigure}%
    % \begin{subfigure}[b]{0.33\textwidth}
%   \fbox{\rule{0pt}{1.5in} \rule{0.9\linewidth}{0pt}}
% %   \includegraphics[width=0.8\linewidth]{egfigure.eps}
%                 % \includegraphics[width=.98\linewidth,height=.6\linewidth]{images/baseline-square.png}
%                 % \caption{Square}
%                 % \label{fig:baseline-square}
%     \end{subfigure}%
    \begin{subfigure}[b]{0.48\textwidth}
%   \fbox{\rule{0pt}{1.5in} \rule{0.9\linewidth}{0pt}}
  \includegraphics[width=\linewidth]{images/plots/recall07.pdf}
%   \includegraphics[width=0.8\linewidth]{egfigure.eps}
                % \includegraphics[width=.98\linewidth,height=.6\linewidth]{images/baseline-square.png}
                % \caption{Square}
                % \label{fig:baseline-square}
    \end{subfigure}%
%     \begin{subfigure}[b]{0.45\textwidth}
% %   \fbox{\rule{0pt}{1.5in} \rule{0.9\linewidth}{0pt}}
% %   \includegraphics[width=0.4\linewidth]{egfigure.eps}
%   \includegraphics[width=\linewidth]{images/plots/car_mig_bev.pdf}
% %   \includegraphics[width=0.8\linewidth]{egfigure.eps}
%                 % \includegraphics[width=.98\linewidth,height=.6\linewidth]{images/baseline-square.png}
%                 % \caption{Car-3D Detection (AP\_R40@0.70)}
%                 % \label{fig:baseline-square}
%     \end{subfigure}%
% %     \begin{subfigure}[b]{0.33\textwidth}
% %   \fbox{\rule{0pt}{1.5in} \rule{0.9\linewidth}{0pt}}
% % %   \includegraphics[width=0.8\linewidth]{egfigure.eps}
% %                 % \includegraphics[width=.98\linewidth,height=.6\linewidth]{images/baseline-square.png}
% %                 \caption{Pedestrian-3D Detection (AP\_R40@0.50)}
% %                 % \label{fig:baseline-square}
% %     \end{subfigure}%
%     \begin{subfigure}[b]{0.45\textwidth}
% %   \fbox{\rule{0pt}{1.5in} \rule{0.9\linewidth}{0pt}}
%   \includegraphics[width=\linewidth]{images/plots/cyc_mig_bev.pdf}
% %   \includegraphics[width=0.8\linewidth]{egfigure.eps}
%                 % \includegraphics[width=.98\linewidth,height=.6\linewidth]{images/baseline-square.png}
%                 % \caption{Cyclist-3D Detection (AP\_R40@0.50)}
%                 % \label{fig:baseline-square}
%     \end{subfigure}%
\vspace*{-3mm}
  \caption{The relationship overall recall performance  and overall surrogate metric of different LiDAR placements for  Car, Van and Cyclist.}
  \label{fig:recall_relation}
%   \vspace*{-5mm}
\end{figure*}

\begin{table}[]
\centering
\resizebox{0.45\textwidth}{!}{
\begin{tabular}{|c|c|c|c|}
\hline
3D  AP  & PV-RCNN\cite{shi2020pv}  &  PointRCNN\cite{Shi_2019_CVPR}  & \makecell[c]{S-MIG\\ $(10^3)$}  \\ \hline
% Trapezoid  &    57.37
%       &      47.50
%       &  -6.51      \\ \hline
% Trapezoid-pitch &   57.32
% &     49.09
%       &    -6.52      \\ \hline
Pyramid &    57.44
    &   44.28
       &   -5.64       \\ \hline
Pyramid-pitch &     53.46
     &     37.27
      &    -5.71      \\ \hline
% \hline
% Pedestrian    & PV-RCNN & PointRCNN & Entropy & MLE \\ \hline
% Dense  &         &           &         &     \\ \hline
% Medium &         &           &         &     \\ \hline
% Sparse &         &           &         &     \\ \hline
\hline
BEV AP   & PV-RCNN\cite{shi2020pv}   & PointRCNN\cite{Shi_2019_CVPR}  & \makecell[c]{S-MIG\\ $(10^3)$} \\ \hline
% Trapezoid  &   65.42
%       &     59.14
%       &     -6.51      \\ \hline
% Trapezoid-pitch &   63.17
%      &    56.54
%       &   -6.52       \\ \hline
Pyramid &  65.81
       &    56.97
      &    -5.64     \\ \hline
Pyramid-pitch &     62.33
    &     51.39
      &   -5.71     \\ \hline
\end{tabular}}
\caption{Influence of pitch rotation of front LiDARs on Car AP detection performance.
% \todo{explain what does @70 @50 @50 mean! in the text, caption getting too long}
% }
}
\label{tab:pitch_AP}
\vspace*{-5mm}
\end{table}

\subsection{Sensitiveness Analysis of Detection Algorithms}
In this section, we analyze how sensitive current different LiDAR-based detraction algorithms are to the influence of LiDAR placements. From  Figure \ref{fig:expt2} and Figure \ref{fig:recall_relation}, it can be found that point-based methods, like \textit{PointRCNN} and \textit{PointRCNN-IoU}, are sensitive to different LiDAR placements and have a relatively clear linear relationship with our surrogate metric. On the contrary, the detection performance of voxel-based methods fluctuates with different LiDAR configurations as well, but the linear relationship is less obvious, which is because point-based methods rely on the original point data collected from LiDAR and are highly related to the point cloud distribution and uncertainty revealed by our surrogate metric.

Besides, there are some detection algorithms where the fluctuation caused by LiDAR placement is even more significant than the difference between different algorithms given any LiDAR placement. Specifically, the recall with 0.7 IoU of \textit{Second} is better than \textit{PointRCNN} under \textit{Pyramid-roll}, while  \textit{Second} performs worse than \textit{PointRCNN}  using data collected under other LiDAR placements, showing that LiDAR configuration is also a critical factor in object detection. Therefore, there is still room to improve the 3D detection performance from LiDAR placement.

\begin{figure*}[t]
  \centering
    % \begin{subfigure}[b]{0.33\textwidth}
%   \fbox{\rule{0pt}{1.5in} \rule{0.9\linewidth}{0pt}}
% %   \includegraphics[width=0.8\linewidth]{egfigure.eps}
%                 % \includegraphics[width=.98\linewidth,height=.6\linewidth]{images/baseline-square.png}
%                 % \caption{Square}
%                 % \label{fig:baseline-square}
%     \end{subfigure}%
    \begin{subfigure}[b]{0.23\textwidth}
%   \fbox{\rule{0pt}{1.5in} \rule{0.9\linewidth}{0pt}}
  \includegraphics[width=\linewidth]{images/vis/itti_center_training_image_2_000717.png}
%   \includegraphics[width=0.8\linewidth]{egfigure.eps}
                % \includegraphics[width=.98\linewidth,height=.6\linewidth]{images/baseline-square.png}
                % \caption{RGB image}
                % \label{fig:baseline-square}
    \end{subfigure}%
    \begin{subfigure}[b]{0.23\textwidth}
%   \fbox{\rule{0pt}{1.5in} \rule{0.9\linewidth}{0pt}}
  \includegraphics[width=\linewidth]{images/vis/itti_line_training_image_2_000716.png}
%   \includegraphics[width=0.8\linewidth]{egfigure.eps}
                % \includegraphics[width=.98\linewidth,height=.6\linewidth]{images/baseline-square.png}
                % \caption{RGB image}
                % \label{fig:baseline-square}
    \end{subfigure}%
    \begin{subfigure}[b]{0.23\textwidth}
%   \fbox{\rule{0pt}{1.5in} \rule{0.9\linewidth}{0pt}}
  \includegraphics[width=\linewidth]{images/vis/done_tixing_gpu3_training_training_image_2_000717.png}
%   \includegraphics[width=0.8\linewidth]{egfigure.eps}
                % \includegraphics[width=.98\linewidth,height=.6\linewidth]{images/baseline-square.png}
                % \caption{RGB image}
                % \label{fig:baseline-square}
    \end{subfigure}%
    \begin{subfigure}[b]{0.23\textwidth}
%   \fbox{\rule{0pt}{1.5in} \rule{0.9\linewidth}{0pt}}
  \includegraphics[width=\linewidth]{images/vis/done_kitti_square_training_image_2_000717.png}
%   \includegraphics[width=0.8\linewidth]{egfigure.eps}
                % \includegraphics[width=.98\linewidth,height=.6\linewidth]{images/baseline-square.png}
                % \caption{RGB image}
                % \label{fig:baseline-square}
    \end{subfigure}%

        \begin{subfigure}[b]{0.23\textwidth}
%   \fbox{\rule{0pt}{1.5in} \rule{0.9\linewidth}{0pt}}
  \includegraphics[width=\linewidth,height=.58\linewidth]{images/vis/sup_717_center.png}
%   \includegraphics[width=0.8\linewidth]{egfigure.eps}
                % \includegraphics[width=.98\linewidth,height=.6\linewidth]{images/baseline-square.png}
                % \caption{RGB image}
                % \label{fig:baseline-square}
    \end{subfigure}%
    \begin{subfigure}[b]{0.23\textwidth}
%   \fbox{\rule{0pt}{1.5in} \rule{0.9\linewidth}{0pt}}
  \includegraphics[width=\linewidth]{images/vis/sup_717_line.png}
%   \includegraphics[width=0.8\linewidth]{egfigure.eps}
                % \includegraphics[width=.98\linewidth,height=.6\linewidth]{images/baseline-square.png}
                % \caption{RGB image}
                % \label{fig:baseline-square}
    \end{subfigure}%
    \begin{subfigure}[b]{0.23\textwidth}
%   \fbox{\rule{0pt}{1.5in} \rule{0.9\linewidth}{0pt}}
  \includegraphics[width=\linewidth]{images/vis/sup_717_trapezoid.png}
%   \includegraphics[width=0.8\linewidth]{egfigure.eps}
                % \includegraphics[width=.98\linewidth,height=.6\linewidth]{images/baseline-square.png}
                % \caption{RGB image}
                % \label{fig:baseline-square}
    \end{subfigure}%
    \begin{subfigure}[b]{0.23\textwidth}
%   \fbox{\rule{0pt}{1.5in} \rule{0.9\linewidth}{0pt}}
  \includegraphics[width=\linewidth]{images/vis/sup_717_square.png}
%   \includegraphics[width=0.8\linewidth]{egfigure.eps}
                % \includegraphics[width=.98\linewidth,height=.6\linewidth]{images/baseline-square.png}
                % \caption{RGB image}
                % \label{fig:baseline-square}
    \end{subfigure}%

        \begin{subfigure}[b]{0.23\textwidth}
%   \fbox{\rule{0pt}{1.5in} \rule{0.9\linewidth}{0pt}}
  \includegraphics[width=\linewidth]{images/vis/itti_center_training_image_2_000264.png}
%   \includegraphics[width=0.8\linewidth]{egfigure.eps}
                % \includegraphics[width=.98\linewidth,height=.6\linewidth]{images/baseline-square.png}
                % \caption{RGB image}
                % \label{fig:baseline-square}
    \end{subfigure}%
    \begin{subfigure}[b]{0.23\textwidth}
%   \fbox{\rule{0pt}{1.5in} \rule{0.9\linewidth}{0pt}}
  \includegraphics[width=\linewidth]{images/vis/itti_line_testing_image_2_000264.png}
%   \includegraphics[width=0.8\linewidth]{egfigure.eps}
                % \includegraphics[width=.98\linewidth,height=.6\linewidth]{images/baseline-square.png}
                % \caption{RGB image}
                % \label{fig:baseline-square}
    \end{subfigure}%
    \begin{subfigure}[b]{0.23\textwidth}
%   \fbox{\rule{0pt}{1.5in} \rule{0.9\linewidth}{0pt}}
  \includegraphics[width=\linewidth]{images/vis/done_tixing_gpu3_training_training_image_2_000264.png}
%   \includegraphics[width=0.8\linewidth]{egfigure.eps}
                % \includegraphics[width=.98\linewidth,height=.6\linewidth]{images/baseline-square.png}
                % \caption{RGB image}
                % \label{fig:baseline-square}
    \end{subfigure}%
    \begin{subfigure}[b]{0.23\textwidth}
%   \fbox{\rule{0pt}{1.5in} \rule{0.9\linewidth}{0pt}}
  \includegraphics[width=\linewidth]{images/vis/done_kitti_square_training_image_2_000264.png}
%   \includegraphics[width=0.8\linewidth]{egfigure.eps}
                % \includegraphics[width=.98\linewidth,height=.6\linewidth]{images/baseline-square.png}
                % \caption{RGB image}
                % \label{fig:baseline-square}
    \end{subfigure}%

        \begin{subfigure}[b]{0.23\textwidth}
%   \fbox{\rule{0pt}{1.5in} \rule{0.9\linewidth}{0pt}}
  \includegraphics[width=\linewidth,height=.58\linewidth]{images/vis/sup_264_center.png}
%   \includegraphics[width=0.8\linewidth]{egfigure.eps}
                % \includegraphics[width=.98\linewidth,height=.6\linewidth]{images/baseline-square.png}
                % \caption{RGB image}
                % \label{fig:baseline-square}
    \end{subfigure}%
    \begin{subfigure}[b]{0.23\textwidth}
%   \fbox{\rule{0pt}{1.5in} \rule{0.9\linewidth}{0pt}}
  \includegraphics[width=\linewidth]{images/vis/sup_264_line.png}
%   \includegraphics[width=0.8\linewidth]{egfigure.eps}
                % \includegraphics[width=.98\linewidth,height=.6\linewidth]{images/baseline-square.png}
                % \caption{RGB image}
                % \label{fig:baseline-square}
    \end{subfigure}%
    \begin{subfigure}[b]{0.23\textwidth}
%   \fbox{\rule{0pt}{1.5in} \rule{0.9\linewidth}{0pt}}
  \includegraphics[width=\linewidth]{images/vis/sup_264_trapezoid.png}
%   \includegraphics[width=0.8\linewidth]{egfigure.eps}
                % \includegraphics[width=.98\linewidth,height=.6\linewidth]{images/baseline-square.png}
                % \caption{RGB image}
                % \label{fig:baseline-square}
    \end{subfigure}%
    \begin{subfigure}[b]{0.23\textwidth}
%   \fbox{\rule{0pt}{1.5in} \rule{0.9\linewidth}{0pt}}
  \includegraphics[width=\linewidth]{images/vis/sup_264_square.png}
%   \includegraphics[width=0.8\linewidth]{egfigure.eps}
                % \includegraphics[width=.98\linewidth,height=.6\linewidth]{images/baseline-square.png}
                % \caption{RGB image}
                % \label{fig:baseline-square}
    \end{subfigure}%

    \begin{subfigure}[b]{0.23\textwidth}
%   \fbox{\rule{0pt}{1.5in} \rule{0.9\linewidth}{0pt}}
  \includegraphics[width=\linewidth,height=.58\linewidth]{images/baseline-center.png}
                % \includegraphics[width=.98\linewidth,height=.6\linewidth]{images/baseline-square.png}
                \caption{Center}
                % \label{fig:baseline-square}
    \end{subfigure}%
    \begin{subfigure}[b]{0.23\textwidth}
%   \fbox{\rule{0pt}{1.5in} \rule{0.9\linewidth}{0pt}}
  \includegraphics[width=\linewidth,height=.58\linewidth]{images/line_norot.png}
%   \includegraphics[width=0.8\linewidth]{egfigure.eps}
                % \includegraphics[width=.98\linewidth,height=.6\linewidth]{images/baseline-square.png}
                \caption{Line}
                % \label{fig:baseline-square}
    \end{subfigure}%
    \begin{subfigure}[b]{0.23\textwidth}
%   \fbox{\rule{0pt}{1.5in} \rule{0.9\linewidth}{0pt}}
  \includegraphics[width=\linewidth]{images/trapezoid.png}
%   \includegraphics[width=0.8\linewidth]{egfigure.eps}
                % \includegraphics[width=.98\linewidth,height=.6\linewidth]{images/baseline-square.png}
                \caption{Trapezoid}
                % \label{fig:baseline-square}
    \end{subfigure}%
    \begin{subfigure}[b]{0.23\textwidth}
%   \fbox{\rule{0pt}{1.5in} \rule{0.9\linewidth}{0pt}}
  \includegraphics[width=\linewidth,height=.58\linewidth]{images/baseline-square.png}
%   \includegraphics[width=0.8\linewidth]{egfigure.eps}
                % \includegraphics[width=.98\linewidth,height=.6\linewidth]{images/baseline-square.png}
                \caption{Square}
                % \label{fig:baseline-square}
    \end{subfigure}%
  \caption{Visualization of point cloud distribution from different LiDAR configurations.}
  \label{fig:vis1}
   \vspace*{-3mm}
\end{figure*}


\begin{table}[]
\centering
\resizebox{0.45\textwidth}{!}{
\begin{tabular}{|c|c|c|c|}
\hline
Recall @0.5 IoU    & PV-RCNN\cite{shi2020pv}  &  PointRCNN\cite{Shi_2019_CVPR}  & \makecell[c]{S-MIG\\ $(10^3)$}  \\ \hline
% Trapezoid  &    57.37
%       &      47.50
%       &  -6.51      \\ \hline
% Trapezoid-pitch &   57.32
% &     49.09
%       &    -6.52      \\ \hline
Pyramid &    0.6260
    &   0.4722
       &   -7.90      \\ \hline
Pyramid-pitch &     0.6002
     &     0.4490
      &    -7.99      \\ \hline
% \hline
% Pedestrian    & PV-RCNN & PointRCNN & Entropy & MLE \\ \hline
% Dense  &         &           &         &     \\ \hline
% Medium &         &           &         &     \\ \hline
% Sparse &         &           &         &     \\ \hline
\hline
Recall @0.7 IoU    & PV-RCNN\cite{shi2020pv}   & PointRCNN\cite{Shi_2019_CVPR}  & \makecell[c]{S-MIG\\ $(10^3)$} \\ \hline
% Trapezoid  &   65.42
%       &     59.14
%       &     -6.51      \\ \hline
% Trapezoid-pitch &   63.17
%      &    56.54
%       &   -6.52       \\ \hline
Pyramid &  0.4610
       &    0.3321
      &    -7.90     \\ \hline
Pyramid-pitch &     0.4279
    &     0.3122
      &   -7.99     \\ \hline
\end{tabular}}
\caption{Influence of pitch rotation of front LiDARs on overall recall detection performance.
% \todo{explain what does @70 @50 @50 mean! in the text, caption getting too long}
% }
}
\label{tab:pitch_recall}
\vspace*{-5mm}
\end{table}

\subsection{Influence of Pitch Angles for Front LiDAR}
Along with the analysis of roll angle of sided LiDARs Section \ref{sec:ablation}, we also show the influence of pitch angle for front LiDAR, which is intuitively essential for object detection in the front 180-degree field of view. From Table \ref{tab:pitch_AP} and Table \ref{tab:pitch_recall}, it can be seen that the surrogate metric of \textit{Pyramid-pitch} placement is less than that of \textit{Pyramid} placement. The Car average precision and overall recall metrics are less, which shows that our surrogate can be used to evaluate the detection performance around the local neighborhood of LiDAR placement. Note that the surrogate metric is the sum of all the objects for recall comparison because the recall includes all Car, Van and Cyclist.

\begin{figure}[ht]
  \centering
    % \begin{subfigure}[b]{0.33\textwidth}
%   \fbox{\rule{0pt}{1.5in} \rule{0.9\linewidth}{0pt}}
% %   \includegraphics[width=0.8\linewidth]{egfigure.eps}
%                 % \includegraphics[width=.98\linewidth,height=.6\linewidth]{images/baseline-square.png}
%                 % \caption{Square}
%                 % \label{fig:baseline-square}
%     \end{subfigure}%
    \begin{subfigure}[b]{0.16\textwidth}
%   \fbox{\rule{0pt}{1.5in} \rule{0.9\linewidth}{0pt}}
  \includegraphics[width=\linewidth]{images/vis/done_triangle_norot_gpu3_trained_3_new_training_training_image_2_000048.png}
%   \includegraphics[width=0.8\linewidth]{egfigure.eps}
                % \includegraphics[width=.98\linewidth,height=.6\linewidth]{images/baseline-square.png}
                % \caption{RGB image}
                % \label{fig:baseline-square}
    \end{subfigure}%
    \begin{subfigure}[b]{0.16\textwidth}
%   \fbox{\rule{0pt}{1.5in} \rule{0.9\linewidth}{0pt}}
  \includegraphics[width=\linewidth]{images/vis/done_kitti_pyramid_withrot_gpu1_trained_training_image_2_000048.png}
%   \includegraphics[width=0.8\linewidth]{egfigure.eps}
                % \includegraphics[width=.98\linewidth,height=.6\linewidth]{images/baseline-square.png}
                % \caption{RGB image}
                % \label{fig:baseline-square}
    \end{subfigure}%
    \begin{subfigure}[b]{0.16\textwidth}
%   \fbox{\rule{0pt}{1.5in} \rule{0.9\linewidth}{0pt}}
  \includegraphics[width=\linewidth]{images/vis/triangle_pitch_gpu1_trained_training_image_2_000048.png}
%   \includegraphics[width=0.8\linewidth]{egfigure.eps}
                % \includegraphics[width=.98\linewidth,height=.6\linewidth]{images/baseline-square.png}
                % \caption{RGB image}
                % \label{fig:baseline-square}
    \end{subfigure}%

        \begin{subfigure}[b]{0.16\textwidth}
%   \fbox{\rule{0pt}{1.5in} \rule{0.9\linewidth}{0pt}}
  \includegraphics[width=\linewidth,height=.58\linewidth]{images/vis/sup_048_pyramid.png}
%   \includegraphics[width=0.8\linewidth]{egfigure.eps}
                % \includegraphics[width=.98\linewidth,height=.6\linewidth]{images/baseline-square.png}
                % \caption{RGB image}
                % \label{fig:baseline-square}
    \end{subfigure}%
    \begin{subfigure}[b]{0.16\textwidth}
%   \fbox{\rule{0pt}{1.5in} \rule{0.9\linewidth}{0pt}}
  \includegraphics[width=\linewidth]{images/vis/sup_048_pyr_roll.png}
%   \includegraphics[width=0.8\linewidth]{egfigure.eps}
                % \includegraphics[width=.98\linewidth,height=.6\linewidth]{images/baseline-square.png}
                % \caption{RGB image}
                % \label{fig:baseline-square}
    \end{subfigure}%
    \begin{subfigure}[b]{0.16\textwidth}
%   \fbox{\rule{0pt}{1.5in} \rule{0.9\linewidth}{0pt}}
  \includegraphics[width=\linewidth]{images/vis/sup_048_pyr_pitch.png}
%   \includegraphics[width=0.8\linewidth]{egfigure.eps}
                % \includegraphics[width=.98\linewidth,height=.6\linewidth]{images/baseline-square.png}
                % \caption{RGB image}
                % \label{fig:baseline-square}
    \end{subfigure}%


    \begin{subfigure}[b]{0.16\textwidth}
%   \fbox{\rule{0pt}{1.5in} \rule{0.9\linewidth}{0pt}}
  \includegraphics[width=\linewidth,height=.58\linewidth]{images/triangle.png}
                % \includegraphics[width=.98\linewidth,height=.6\linewidth]{images/baseline-square.png}
                \caption{Pyramid}
                % \label{fig:baseline-square}
    \end{subfigure}%
    \begin{subfigure}[b]{0.16\textwidth}
%   \fbox{\rule{0pt}{1.5in} \rule{0.9\linewidth}{0pt}}
  \includegraphics[width=\linewidth,height=.58\linewidth]{images/triangle_roll.png}
%   \includegraphics[width=0.8\linewidth]{egfigure.eps}
                % \includegraphics[width=.98\linewidth,height=.6\linewidth]{images/baseline-square.png}
                \caption{Pyramid-roll}
                % \label{fig:baseline-square}
    \end{subfigure}%
    \begin{subfigure}[b]{0.16\textwidth}
%   \fbox{\rule{0pt}{1.5in} \rule{0.9\linewidth}{0pt}}
  \includegraphics[width=\linewidth]{images/triangle_pitch.png}
%   \includegraphics[width=0.8\linewidth]{egfigure.eps}
                % \includegraphics[width=.98\linewidth,height=.6\linewidth]{images/baseline-square.png}
                \caption{Pyramid-pitch}
                % \label{fig:baseline-square}
    \end{subfigure}%
  \caption{Comparison of point cloud distribution from LiDAR placement with roll and pitch angles.}
  \label{fig:vis2}
   \vspace*{-5mm}
\end{figure}

\subsection{LiDAR Placement for Pedestrain Detection}
Besides cars, vans and cyclists, e conducted an extra experiment considering the pedestrians mainly from the sidewalk as shown in the table below. The results further consolidate the significance of LiDAR placement to SOTA point cloud-based detection algorithms with the metric of AP at 0.5 IOU. It shows the \textit{Line} option almost performs the best due to its wide horizontal view field. Furthermore, we found that the influence on Pedestrian detection is even larger than Car and Cyclists as shown in Table \ref{tab:expt1} text of the paper, where the four SOTA models are affected by 30\% of \textit{Line} placement.
% including Cars, Cyclists, Pedestrians, etc. However, the amount of pedestrian objects are fairly few on the road in the current CARLA setting. For the pedestrians on the sidewalk, although they might be influenced by different LiDAR placements, they are beyond the motivation of the LiDAR placement for current autonomous driving. Moreover, technically, the performance of pedestrians will be similar to cyclists because they are both small objects and contain much fewer points than cars, resulting in not necessarily involving pedestrians in the experiments.

\begin{table}[H]
\centering
\resizebox{0.47\textwidth}{!}{
\begin{tabular}{|c||c|c|c|c|}
\hline
% \multirow{1}{*}{Pedestrian-3D  (AP\_R40@0.50)}          & \multicolumn{4}{c|}{ Pedestrian-3D  (AP\_R40@0.50)}                                                                   \\ \cline{2-5}
       3D  (AP@0.50)                             & Center & Line & Pyramid & Trapezoid  \\ \hline
PV-RCNN   \cite{shi2020pv}
% &  28.97
      &   12.80 & \textbf{	16.35}& 	 10.95 & 	 13.57
%  &   34.09
%  &	62.64&	65.42&	\textbf{65.81}&	65.54
                    \\ \hline
Voxel RCNN  \cite{deng2020voxel}
% &   26.06
&    10.95 & 	8.17& 	 4.85 	&  \textbf{12.09}
%   &                   31.31
%   & 57.72&	60.84&	\textbf{63.65}&	61.52
                                                 \\ \hline
% PointPillar  \cite{lang2019pointpillars}                    &   40.95     &    40.55    &   39.68   &    \textbf{40.97}                                                        &   36.62     &   34.72    &   34.46   &     \textbf{36.99}                                                       \\ \hline
PointRCNN       \cite{Shi_2019_CVPR}
% & 23.55
&   10.98 & 	\textbf{11.75}& 	 10.20 	&  11.32
% &      28.97
% & 51.41&	\textbf{59.14}&	56.97&	59.08
                 \\ \hline
PointRCNN-IoU      \cite{Shi_2019_CVPR}
% & 23.37
&    10.66 & 	\textbf{12.19}& 	 10.23 & 	 11.30
% &   28.70
% &	50.56&	58.86&	56.67&	\textbf{59.04}
                         \\ \hline
SECOND     \cite{yan2018second}
% & 24.84
 &  \textbf{11.35} & 	8.82& 	 7.46 	&  8.10
% &       31.19
% & 56.65&	59.44&	\textbf{61.75}&	59.97
                                \\ \hline
SECOND-IoU   \cite{yan2018second}
% & 25.28
 &   5.00 & 	\textbf{8.78}& 	 5.23 & 	 7.50
%  &   31.46
%  &	56.98&	59.96&	\textbf{62.31}&	60.28
                                 \\ \hline\hline
                                        BEV  (AP@0.50)                             & Center & Line & Pyramid & Trapezoid  \\ \hline
PV-RCNN   \cite{shi2020pv}
% &  28.97
   &     15.30  & 	\textbf{19.96} & 	 17.27  & 	 17.23
%  &   34.09
%  &	62.64&	65.42&	\textbf{65.81}&	65.54
                    \\ \hline
Voxel RCNN   \cite{deng2020voxel}
% &   26.06
&    11.45  & 	9.32 & 	 7.37  & 	 \textbf{13.48}
%   &                   31.31
%   & 57.72&	60.84&	\textbf{63.65}&	61.52
                                                 \\ \hline
% PointPillar  \cite{lang2019pointpillars}                    &   40.95     &    40.55    &   39.68   &    \textbf{40.97}                                                        &   36.62     &   34.72    &   34.46   &     \textbf{36.99}                                                       \\ \hline
PointRCNN       \cite{Shi_2019_CVPR}
% & 23.55
&    11.86  & 	\textbf{14.44} & 	 11.67  & 	 12.21
% &      28.97
% & 51.41&	\textbf{59.14}&	56.97&	59.08
                 \\ \hline
PointRCNN-IoU    \cite{Shi_2019_CVPR}
% & 23.37
&    11.59  & 	\textbf{14.38}	 &  11.85  & 	 13.61
% &   28.70
% &	50.56&	58.86&	56.67&	\textbf{59.04}
                         \\ \hline
SECOND         \cite{yan2018second}
% & 24.84
&    13.38  & 	\textbf{13.71}	 &  10.08  & 	 12.57
% &       31.19
% & 56.65&	59.44&	\textbf{61.75}&	59.97
                                \\ \hline
SECOND-IoU    \cite{yan2018second}
% & 25.28
&   7.46  & 	\textbf{12.27} & 	 8.49  & 	 10.54
%  &   31.46
%  &	56.98&	59.96&	\textbf{62.31}&	60.28
                                 \\ \hline
\end{tabular}}
\caption{Influence of LiDAR placement on pedestrian detection}
\vspace*{-3mm}
\end{table}


\subsection{Qualitative Visualization and Analysis}
% \todo{small and far good, near: original more uniform, roll side more parallel, pitch near more point and less data on high part}
This section shows some qualitative visualization and analysis of point cloud collected through different LiDAR placements. From Figure \ref{fig:vis1}, we can see that the distribution of point cloud varies a lot in the same scenario, which is directly caused by different LiDAR placements. Specifically, the point distribution of \textit{Center} is more uniform in the vertical direction. However, there are some aggregated points as clear horizontal lines for other placements, which results in the performance improvement for small object detection like Cyclist or extremely large trucks, as shown in Figure \ref{fig:expt2} and Table \ref{tab:expt1}.

 However, since the aggregated lines can better represent the shape and other critical information of the objects,  other plane-placed LiDAR configurations have better performance for large-scale object detection like Car, where there are enough points for the detection task.
In Figure \ref{fig:vis2}, we illustrate the influence of roll and pitch angle in \textit{Pyramid} placement. For the objects far away, the distribution of object point cloud is quite different. The distribution under LiDARs with roll angle is sparser horizontally, making key points harder to aggregate. However, \textit{Pyramid-pitch} placement makes the points in the front object shifted below, resulting in the object not being that clear to detect. Note that there is some slight difference in objects for each scenario, but it does not matter because the distribution of objects is the same for all LiDAR placements. As the number of frames is huge, the detection performance will be fair for all the experiments.



\section{Appendix - Limitation and Discussion}
In this section, we present some limitations of our work and some further discussions. First, when calculating our surrogate metric using POG, we have assumed that all the voxels are independent. Although objects in ROI are independent among all the frames, the voxels may be related to their neighbors and not fully independent. However, since the number of frames is large enough, the related voxels are highly separated, so our assumptions somewhat hold. Besides, we do not consider the case of occlusion. When LiDAR beams meet some object, they will no longer get through it in most cases and reflect. In our Bresenham model, we ignore such occlusion based on observing that the occlusion case is rare in the CARLA town with 40, 80, or 120 vehicles. Also, occlusion often happens far away from the ego vehicle, so it may not influence the surrogate metric too much. Also, we leave the task of finding better surrogate functions to evaluate or even optimize LiDAR configurations as the future work in this community.

Although we have conducted extensive experiments and made comprehensive comparison and analyses, there are still some weaknesses. First, we do not train each model for 80 epochs as default to make the experiment efficient. We have found that  the fine-tuning loss of all the detection algorithms has already converged after 10 epochs,  so we stop it and test the performance for a fair evaluation. Also, the point cloud data is sometimes sparse for objects far away, which is challenging for detection and the detection metrics are not as high as those on KITTI. That is based on the observation that the influence of LiDAR placement is more evident for sparse point cloud to avoid saturation and is also consistent with the practical applications where multiple LiDARs are mainly used towards the challenging cases to detect objects with sparse points. Moreover, the evaluated multi-LiDAR placements are not as complicated as the cases in the company. The real LiDAR placement is confidential for the company, and we believe the simplified case is enough to show the influence of LiDAR configuration to meet the motivation of this work.
% Our motivation stemmed from building a unified framework of optimal LiDAR perception and configuration, which will help balance perception capabilities and design costs of LiDARs, without sacrificing safety in AVs.
% The experimental results indicated that the optimal LiDAR placement solved by our method outperforms those of standard baseline configurations.

% the multi-LiDAR configurations using surrogate metrics and consider occlusion in ROI and POG generation.
% With increasing relevance of adversarial learning, we also aim to investigate if we could mitigate the effect of sensor attacks on the AV perception by changing the LiDAR placement.
% \subsection{Language}

% All manuscripts must be in English.

% \subsection{Dual submission}

% Please refer to the author guidelines on the \confName\ \confYear\ web page for a
% discussion of the policy on dual submissions.

% \subsection{Paper length}
% Papers, excluding the references section, must be no longer than eight pages in length.
% The references section will not be included in the page count, and there is no limit on the length of the references section.
% For example, a paper of eight pages with two pages of references would have a total length of 10 pages.
% {\bf There will be no extra page charges for \confName\ \confYear.}

% Overlength papers will simply not be reviewed.
% This includes papers where the margins and formatting are deemed to have been significantly altered from those laid down by this style guide.
% Note that this \LaTeX\ guide already sets figure captions and references in a smaller font.
% The reason such papers will not be reviewed is that there is no provision for supervised revisions of manuscripts.
% The reviewing process cannot determine the suitability of the paper for presentation in eight pages if it is reviewed in eleven.

% %-------------------------------------------------------------------------
% \subsection{The ruler}
% The \LaTeX\ style defines a printed ruler which should be present in the version submitted for review.
% The ruler is provided in order that reviewers may comment on particular lines in the paper without circumlocution.
% If you are preparing a document using a non-\LaTeX\ document preparation system, please arrange for an equivalent ruler to appear on the final output pages.
% The presence or absence of the ruler should not change the appearance of any other content on the page.
% The camera-ready copy should not contain a ruler.
% (\LaTeX\ users may use options of cvpr.sty to switch between different versions.)

% Reviewers:
% note that the ruler measurements do not align well with lines in the paper --- this turns out to be very difficult to do well when the paper contains many figures and equations, and, when done, looks ugly.
% Just use fractional references (\eg, this line is $087.5$), although in most cases one would expect that the approximate location will be adequate.


% \subsection{Paper ID}
% Make sure that the Paper ID from the submission system is visible in the version submitted for review (replacing the ``*****'' you see in this document).
% If you are using the \LaTeX\ template, \textbf{make sure to update paper ID in the appropriate place in the tex file}.


% \subsection{Mathematics}

% Please number all of your sections and displayed equations as in these examples:
% \begin{equation}
%   E = m\cdot c^2
%   \label{eq:important}
% \end{equation}
% and
% \begin{equation}
%   v = a\cdot t.
%   \label{eq:also-important}
% \end{equation}
% It is important for readers to be able to refer to any particular equation.
% Just because you did not refer to it in the text does not mean some future reader might not need to refer to it.
% It is cumbersome to have to use circumlocutions like ``the equation second from the top of page 3 column 1''.
% (Note that the ruler will not be present in the final copy, so is not an alternative to equation numbers).
% All authors will benefit from reading Mermin's description of how to write mathematics:
% \url{http://www.pamitc.org/documents/mermin.pdf}.

% \subsection{Blind review}

% Many authors misunderstand the concept of anonymizing for blind review.
% Blind review does not mean that one must remove citations to one's own work---in fact it is often impossible to review a paper unless the previous citations are known and available.

% Blind review means that you do not use the words ``my'' or ``our'' when citing previous work.
% That is all.
% (But see below for tech reports.)

% Saying ``this builds on the work of Lucy Smith [1]'' does not say that you are Lucy Smith;
% it says that you are building on her work.
% If you are Smith and Jones, do not say ``as we show in [7]'', say ``as Smith and Jones show in [7]'' and at the end of the paper, include reference 7 as you would any other cited work.

% An example of a bad paper just asking to be rejected:
% \begin{quote}
% \begin{center}
%     An analysis of the frobnicatable foo filter.
% \end{center}

%   In this paper we present a performance analysis of our previous paper [1], and show it to be inferior to all previously known methods.
%   Why the previous paper was accepted without this analysis is beyond me.

%   [1] Removed for blind review
% \end{quote}


% An example of an acceptable paper:
% \begin{quote}
% \begin{center}
%      An analysis of the frobnicatable foo filter.
% \end{center}

%   In this paper we present a performance analysis of the  paper of Smith \etal [1], and show it to be inferior to all previously known methods.
%   Why the previous paper was accepted without this analysis is beyond me.

%   [1] Smith, L and Jones, C. ``The frobnicatable foo filter, a fundamental contribution to human knowledge''. Nature 381(12), 1-213.
% \end{quote}

% If you are making a submission to another conference at the same time, which covers similar or overlapping material, you may need to refer to that submission in order to explain the differences, just as you would if you had previously published related work.
% In such cases, include the anonymized parallel submission~\cite{Authors14} as supplemental material and cite it as
% \begin{quote}
% [1] Authors. ``The frobnicatable foo filter'', F\&G 2014 Submission ID 324, Supplied as supplemental material {\tt fg324.pdf}.
% \end{quote}

% Finally, you may feel you need to tell the reader that more details can be found elsewhere, and refer them to a technical report.
% For conference submissions, the paper must stand on its own, and not {\em require} the reviewer to go to a tech report for further details.
% Thus, you may say in the body of the paper ``further details may be found in~\cite{Authors14b}''.
% Then submit the tech report as supplemental material.
% Again, you may not assume the reviewers will read this material.

% Sometimes your paper is about a problem which you tested using a tool that is widely known to be restricted to a single institution.
% For example, let's say it's 1969, you have solved a key problem on the Apollo lander, and you believe that the CVPR70 audience would like to hear about your
% solution.
% The work is a development of your celebrated 1968 paper entitled ``Zero-g frobnication: How being the only people in the world with access to the Apollo lander source code makes us a wow at parties'', by Zeus \etal.

% You can handle this paper like any other.
% Do not write ``We show how to improve our previous work [Anonymous, 1968].
% This time we tested the algorithm on a lunar lander [name of lander removed for blind review]''.
% That would be silly, and would immediately identify the authors.
% Instead write the following:
% \begin{quotation}
% \noindent
%   We describe a system for zero-g frobnication.
%   This system is new because it handles the following cases:
%   A, B.  Previous systems [Zeus et al. 1968] did not  handle case B properly.
%   Ours handles it by including a foo term in the bar integral.

%   ...

%   The proposed system was integrated with the Apollo lunar lander, and went all the way to the moon, don't you know.
%   It displayed the following behaviours, which show how well we solved cases A and B: ...
% \end{quotation}
% As you can see, the above text follows standard scientific convention, reads better than the first version, and does not explicitly name you as the authors.
% A reviewer might think it likely that the new paper was written by Zeus \etal, but cannot make any decision based on that guess.
% He or she would have to be sure that no other authors could have been contracted to solve problem B.
% \medskip

% \noindent
% FAQ\medskip\\
% {\bf Q:} Are acknowledgements OK?\\
% {\bf A:} No.  Leave them for the final copy.\medskip\\
% {\bf Q:} How do I cite my results reported in open challenges?
% {\bf A:} To conform with the double-blind review policy, you can report results of other challenge participants together with your results in your paper.
% For your results, however, you should not identify yourself and should not mention your participation in the challenge.
% Instead present your results referring to the method proposed in your paper and draw conclusions based on the experimental comparison to other results.\medskip\\

% \begin{figure}[t]
%   \centering
%   \fbox{\rule{0pt}{2in} \rule{0.9\linewidth}{0pt}}
%   %\includegraphics[width=0.8\linewidth]{egfigure.eps}

%   \caption{Example of caption.
%   It is set in Roman so that mathematics (always set in Roman: $B \sin A = A \sin B$) may be included without an ugly clash.}
%   \label{fig:onecol}
% \end{figure}

% \subsection{Miscellaneous}

% \noindent
% Compare the following:\\
% \begin{tabular}{ll}
%  \verb'$conf_a$' &  $conf_a$ \\
%  \verb'$\mathit{conf}_a$' & $\mathit{conf}_a$
% \end{tabular}\\
% See The \TeX book, p165.

% The space after \eg, meaning ``for example'', should not be a sentence-ending space.
% So \eg is correct, {\em e.g.} is not.
% The provided \verb'\eg' macro takes care of this.

% When citing a multi-author paper, you may save space by using ``et alia'', shortened to ``\etal'' (not ``{\em et.\ al.}'' as ``{\em et}'' is a complete word).
% If you use the \verb'\etal' macro provided, then you need not worry about double periods when used at the end of a sentence as in Alpher \etal.
% However, use it only when there are three or more authors.
% Thus, the following is correct:
%   ``Frobnication has been trendy lately.
%   It was introduced by Alpher~\cite{Alpher02}, and subsequently developed by
%   Alpher and Fotheringham-Smythe~\cite{Alpher03}, and Alpher \etal~\cite{Alpher04}.''

% This is incorrect: ``... subsequently developed by Alpher \etal~\cite{Alpher03} ...'' because reference~\cite{Alpher03} has just two authors.


% % Update the cvpr.cls to do the following automatically.
% % For this citation style, keep multiple citations in numerical (not
% % chronological) order, so prefer \cite{Alpher03,Alpher02,Authors14} to
% % \cite{Alpher02,Alpher03,Authors14}.


% \begin{figure*}
%   \centering
%   \begin{subfigure}{0.68\linewidth}
%     \fbox{\rule{0pt}{2in} \rule{.9\linewidth}{0pt}}
%     \caption{An example of a subfigure.}
%     \label{fig:short-a}
%   \end{subfigure}
%   \hfill
%   \begin{subfigure}{0.28\linewidth}
%     \fbox{\rule{0pt}{2in} \rule{.9\linewidth}{0pt}}
%     \caption{Another example of a subfigure.}
%     \label{fig:short-b}
%   \end{subfigure}
%   \caption{Example of a short caption, which should be centered.}
%   \label{fig:short}
% \end{figure*}

% %------------------------------------------------------------------------
% \section{Formatting your paper}
% \label{sec:formatting}

% All text must be in a two-column format.
% The total allowable size of the text area is $6\frac78$ inches (17.46 cm) wide by $8\frac78$ inches (22.54 cm) high.
% Columns are to be $3\frac14$ inches (8.25 cm) wide, with a $\frac{5}{16}$ inch (0.8 cm) space between them.
% The main title (on the first page) should begin 1 inch (2.54 cm) from the top edge of the page.
% The second and following pages should begin 1 inch (2.54 cm) from the top edge.
% On all pages, the bottom margin should be $1\frac{1}{8}$ inches (2.86 cm) from the bottom edge of the page for $8.5 \times 11$-inch paper;
% for A4 paper, approximately $1\frac{5}{8}$ inches (4.13 cm) from the bottom edge of the
% page.

% %-------------------------------------------------------------------------
% \subsection{Margins and page numbering}

% All printed material, including text, illustrations, and charts, must be kept
% within a print area $6\frac{7}{8}$ inches (17.46 cm) wide by $8\frac{7}{8}$ inches (22.54 cm)
% high.
% %
% Page numbers should be in the footer, centered and $\frac{3}{4}$ inches from the bottom of the page.
% The review version should have page numbers, yet the final version submitted as camera ready should not show any page numbers.
% The \LaTeX\ template takes care of this when used properly.



% %-------------------------------------------------------------------------
% \subsection{Type style and fonts}

% Wherever Times is specified, Times Roman may also be used.
% If neither is available on your word processor, please use the font closest in
% appearance to Times to which you have access.

% MAIN TITLE.
% Center the title $1\frac{3}{8}$ inches (3.49 cm) from the top edge of the first page.
% The title should be in Times 14-point, boldface type.
% Capitalize the first letter of nouns, pronouns, verbs, adjectives, and adverbs;
% do not capitalize articles, coordinate conjunctions, or prepositions (unless the title begins with such a word).
% Leave two blank lines after the title.

% AUTHOR NAME(s) and AFFILIATION(s) are to be centered beneath the title
% and printed in Times 12-point, non-boldface type.
% This information is to be followed by two blank lines.

% The ABSTRACT and MAIN TEXT are to be in a two-column format.

% MAIN TEXT.
% Type main text in 10-point Times, single-spaced.
% Do NOT use double-spacing.
% All paragraphs should be indented 1 pica (approx.~$\frac{1}{6}$ inch or 0.422 cm).
% Make sure your text is fully justified---that is, flush left and flush right.
% Please do not place any additional blank lines between paragraphs.

% Figure and table captions should be 9-point Roman type as in \cref{fig:onecol,fig:short}.
% Short captions should be centred.

% \noindent Callouts should be 9-point Helvetica, non-boldface type.
% Initially capitalize only the first word of section titles and first-, second-, and third-order headings.

% FIRST-ORDER HEADINGS.
% (For example, {\large \bf 1. Introduction}) should be Times 12-point boldface, initially capitalized, flush left, with one blank line before, and one blank line after.

% SECOND-ORDER HEADINGS.
% (For example, { \bf 1.1. Database elements}) should be Times 11-point boldface, initially capitalized, flush left, with one blank line before, and one after.
% If you require a third-order heading (we discourage it), use 10-point Times, boldface, initially capitalized, flush left, preceded by one blank line, followed by a period and your text on the same line.

% %-------------------------------------------------------------------------
% \subsection{Footnotes}

% Please use footnotes\footnote{This is what a footnote looks like.
% It often distracts the reader from the main flow of the argument.} sparingly.
% Indeed, try to avoid footnotes altogether and include necessary peripheral observations in the text (within parentheses, if you prefer, as in this sentence).
% If you wish to use a footnote, place it at the bottom of the column on the page on which it is referenced.
% Use Times 8-point type, single-spaced.


% %-------------------------------------------------------------------------
% \subsection{Cross-references}

% For the benefit of author(s) and readers, please use the
% {\small\begin{verbatim}
%   \cref{...}
% \end{verbatim}}  command for cross-referencing to figures, tables, equations, or sections.
% This will automatically insert the appropriate label alongside the cross-reference as in this example:
% \begin{quotation}
%   To see how our method outperforms previous work, please see \cref{fig:onecol} and \cref{tab:example}.
%   It is also possible to refer to multiple targets as once, \eg~to \cref{fig:onecol,fig:short-a}.
%   You may also return to \cref{sec:formatting} or look at \cref{eq:also-important}.
% \end{quotation}
% If you do not wish to abbreviate the label, for example at the beginning of the sentence, you can use the
% {\small\begin{verbatim}
%   \Cref{...}
% \end{verbatim}}
% command. Here is an example:
% \begin{quotation}
%   \Cref{fig:onecol} is also quite important.
% \end{quotation}

% %-------------------------------------------------------------------------
% \subsection{References}

% List and number all bibliographical references in 9-point Times, single-spaced, at the end of your paper.
% When referenced in the text, enclose the citation number in square brackets, for
% example~\cite{Authors14}.
% Where appropriate, include page numbers and the name(s) of editors of referenced books.
% When you cite multiple papers at once, please make sure that you cite them in numerical order like this \cite{Alpher02,Alpher03,Alpher05,Authors14b,Authors14}.
% If you use the template as advised, this will be taken care of automatically.

% \begin{table}
%   \centering
%   \begin{tabular}{@{}lc@{}}
%     \toprule
%     Method & Frobnability \\
%     \midrule
%     Theirs & Frumpy \\
%     Yours & Frobbly \\
%     Ours & Makes one's heart Frob\\
%     \bottomrule
%   \end{tabular}
%   \caption{Results.   Ours is better.}
%   \label{tab:example}
% \end{table}

% %-------------------------------------------------------------------------
% \subsection{Illustrations, graphs, and photographs}

% All graphics should be centered.
% In \LaTeX, avoid using the \texttt{center} environment for this purpose, as this adds potentially unwanted whitespace.
% Instead use
% {\small\begin{verbatim}
%   \centering
% \end{verbatim}}
% at the beginning of your figure.
% Please ensure that any point you wish to make is resolvable in a printed copy of the paper.
% Resize fonts in figures to match the font in the body text, and choose line widths that render effectively in print.
% Readers (and reviewers), even of an electronic copy, may choose to print your paper in order to read it.
% You cannot insist that they do otherwise, and therefore must not assume that they can zoom in to see tiny details on a graphic.

% When placing figures in \LaTeX, it's almost always best to use \verb+\includegraphics+, and to specify the figure width as a multiple of the line width as in the example below
% {\small\begin{verbatim}
%   \usepackage{graphicx} ...
%   \includegraphics[width=0.8\linewidth]
%                   {myfile.pdf}
% \end{verbatim}
% }


% %-------------------------------------------------------------------------
% \subsection{Color}

% Please refer to the author guidelines on the \confName\ \confYear\ web page for a discussion of the use of color in your document.

% If you use color in your plots, please keep in mind that a significant subset of reviewers and readers may have a color vision deficiency; red-green blindness is the most frequent kind.
% Hence avoid relying only on color as the discriminative feature in plots (such as red \vs green lines), but add a second discriminative feature to ease disambiguation.

% %------------------------------------------------------------------------
% \section{Final copy}

% You must include your signed IEEE copyright release form when you submit your finished paper.
% We MUST have this form before your paper can be published in the proceedings.

% Please direct any questions to the production editor in charge of these proceedings at the IEEE Computer Society Press:
% \url{https://www.computer.org/about/contact}.


\end{document}
