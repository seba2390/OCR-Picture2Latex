\section{Introduction}
System of nonlinear equations are ubiquitous in numerical computing.
Solving such nonlinear systems typically depends on efficient iterative
algorithms, as for example Newton-Raphson. In this article, we are interested in the
resolution of a \emph{parametric} system of nonlinear equations, where
the solution depends on a vector of parameters $\bm{p} \in \mathbb{R}^{n_p}$.
These parametric systems are, in their abstract form, written as
\begin{equation}
  \label{eq:nonlinearsystem}
  \text{Find } \bm{x} \text{ such that } g(\bm{x}, \bm{p}) = 0  \; ,
\end{equation}
where the (smooth) nonlinear function $g: \mathbb{R}^{n_x} \times \mathbb{R}^{n_p} \to \mathbb{R}^{n_x}$
depends jointly on an unknown variable $\bm{x} \in \mathbb{R}^{n_x}$ and the parameters $\bm{p} \in \mathbb{R}^{n_p}$.

The solution $x(\bm{p})$ of \eqref{eq:nonlinearsystem} depends \emph{implicitly} on the parameters $\bm{p}$:
of particular interest are the sensitivities of the solution $x(\bm{p})$ with relation to the parameters $\bm{p}$.
Indeed, these sensitivities can be embedded inside an optimization algorithm (if $\bm{p}$ is a design variable)
or in an uncertainty quantification scheme (if $\bm{p}$ encodes an uncertainty).
It is well known that propagating the sensitivities in an iterative algorithm is nontrivial~\cite{gilbert1992automatic}.
Fortunately, there is no need to do so, as we can exploit the mathematical structure of~\eqref{eq:nonlinearsystem}
and compute directly the sensitivities of the solution~$x(\bm{p})$ using the \emph{Implicit Function
Theorem}.

By repeating this process one more step, we are able to extract second-order
sensitivities at the solution $x(\bm{p})$. However, this operation is computationally more demanding
and involves the manipulation of third-order tensors $\nabla^2_{\bm{x}\bm{x}} g, \nabla^2_{\bm{x}\bm{p}} g, \nabla^2_{\bm{p}\bm{p}} g$.
The challenge is to avoid forming explicitly such tensors by using reverse mode accumulation of second-order
information, either explicitly by using the specific structure of the problem
--- encoded by the function $g$ --- or by using automatic differentiation.

\begin{figure}
    \centering
    \begin{tikzpicture}
      \draw (-1.5, 0.7) -- (2, 0.7) -- (2, -3.5) -- (-5.0, -3.5) -- (-5.0, 0.7);
      \node at (-3.2, 0.8) {\textbf{Nonlinear system}};

      \node[frame, label=left:{$g(\bm{x}, \bm{p}) = 0$}] at (0, 0)
        {\textbf{Projection}};
      \filldraw[draw=black, fill=black] (-1, -0.7) -- (1, -0.7) -- (0, -0.9);

      \node[frame, label=left:{$\nabla_{\bm{x}}g, \nabla_{\bm{p}}g$}] at (0, -1.5)
        {\textbf{Reduced gradient}};
      \filldraw[draw=black, fill=black] (-1, -2.2) -- (1, -2.2) -- (0, -2.4);

      \node[hframe, label=left:{$\nabla^2_{\bm{x}\bm{x}}g, \nabla^2_{\bm{x}\bm{p}}g, \nabla^2_{\bm{p}\bm{p}}g$}] at (0, -3)
        {\textbf{Reduced Hessian}};

      \node (p1) at (3.5, 0) {};
     \draw[double,->] (2.1,0.0) -- node[above=.05cm,pos=.6, align=center]{$F$} (p1);

     \node (p2) at (3.5, -1.5) {};
     \draw[double,->] (2.1,-1.5) -- node[above=.05cm,pos=.5, align=center]{$\nabla_{\bm{p}} F$} (p2);

     \node (p3) at (3.5, -3.0) {};
     \draw[double,->] (2.1,-3.0) -- node[above=.05cm,pos=.5, align=center]{$\nabla^2_{\bm{p}\bm{p}} F$} (p3);
    \end{tikzpicture}
    %\includegraphics[width=0.7\linewidth]{figures/overview.png}
    \caption{
      Reduced space algorithm. This article focuses on the last block, in red.
      If $F$ is an objective function,
      the reduced gradient $\nabla_{\bm{p}} F$ and the reduced Hessian $\nabla^2_{\bm{p}\bm{p}} F$
      can be used in any nonlinear optimization algorithm.
    }
    \label{fig:overview}
\end{figure}
As illustrated in Figure~\ref{fig:overview},
this paper covers the efficient computation of the
second-order sensitivities of a nonlinear system~\eqref{eq:nonlinearsystem}. The sparsity structure of the problem is passed to a custom Automatic Differentiation (AutoDiff) backend that automatically generates
all the intermediate sensitivities from the implementation of $g(\bm{x},\bm{p})$. To get a tractable
algorithm, we use an adjoint model implementation of the generated
first-order sensitivities to avoid explicitly forming third-order derivative tensors.
As an application, we compute the reduced Hessian of
the nonlinear equations corresponding to the power flow balance equations of a power grid~\cite{tinney1967power}.
The problem has an unstructured graph structure, leading
to some challenge in the automatic differentiation library, that we discuss extensively.
We show that the reduced Hessian associated to the power flow
equations can be computed efficiently in parallel, by using batches of
Hessian-vector products. The underlying motivation is to embed the reduction
algorithm in a real-time tracking procedure~\cite{tang2017real}, where the reduced Hessian updates
have to be fast to track a suboptimal solution.

In summary, we aim at devising a \emph{portable}, \emph{efficient}, and easily maintainable reduced Hessian algorithm. To this end, we leverage
the expressiveness offered by the Julia programming language. Due to the algorithm's design,
the automatic differentiation backend and the reduction algorithm are transparently implemented on the GPU
without any changes to the algorithm's core implementation, thus realizing a composable software design.

\subsection{Contributions}
Our contribution is a tractable SIMD algorithm and implementation
to evaluate the reduced Hessian from a parametric system of nonlinear equations~\eqref{eq:nonlinearsystem}.
This consists of three closely intertwined components.
%
First, we implement the nonlinear function $g(\bm{x}, \bm{p})$
using the programming language Julia~\cite{bezanson2017julia}
and the portability layer~\KA\ to generate abstract kernels
working on various GPU architectures (CUDA, ROCm).
%
Second, we develop a custom AutoDiff backend on top of the portability
layer to extract automatically the first-order sensitivities $\nabla_{\bm{x}} g, \nabla_{\bm{p}} g$
and the second-order sensitivities $\nabla_{\bm{x}\bm{x}}^2 g, \nabla_{\bm{x}\bm{p}}^2g, \nabla_{\bm{p}\bm{p}}^2 g$.
%
Third, we combine these in an efficient parallel accumulation of the reduced
Hessian associated to a given reduced space problem.
The accumulation involves both Hessian tensor contractions and two sparse linear solves
with multiple right-hand sides.
Glued together, the three components give a generic code
able to extract the second-order derivatives from a power grid problem,
running in parallel on GPU architectures.
Numerical experiments with Volta GPUs (V100)
showcase the scalability of the approach, reaching a 30x faster computation on the largest instances when compared to a reference CPU implementation using UMFPACK. Current researches suggest that a parallel OpenMP power flow implementation using multi-threading (on the CPU alone) potentially achieves a speed-up of 3~\cite{ahmadi2018parallel} or up to 7 and 70 speed-up for Newton-Raphson and batched Newton-Raphson~\cite{2021MTNR}, respectively. However, multi-threaded implementations are not the scope of this paper as we focus on architectures where GPUs are the dominant FLOP contributors for our specific application of second-order space reduction.

\section{Prior Art}
\label{sec:priorart}
In this article we extract the second-order sensitivities from
the system of nonlinear equations using automatic differentiation (AutoDiff).
AutoDiff on Single Instruction, Multiple Data (SIMD) architectures alike the
CUDA cores on GPUs is an ongoing research effort.
Forward-mode AutoDiff effectively adds tangent components to the variables
and preserves the computational flow. In addition, a vector mode can be
applied to propagate multiple tangents or directional derivatives at once.
The technique of automatically generating derivatives of function
implementations has been investigated since the 1950s \cite{nolan1953analytical,beda1959programs}.

Reverse- or adjoint-mode AutoDiff reverses the computational flow and thus
incurs a lot of access restrictions on the final code. Every read of a
variable becomes a write, and vice versa. This leads to application-specific
solutions that exploit the structure of an underlying problem to generate
efficient adjoint code \cite{bluhdorn2020automat,grabner2008automatic,huckelheim2018parallelizable}.
Most prominently, the reverse mode is currently implemented as
backpropagation in machine learning. Indeed, the backpropagation has a long
history (e.g., \cite{bryson1962steepest})  with the reverse mode in AutoDiff
being formalized for the first time in \cite{linnainmaa1976taylor}. Because
of the  limited size and single access pattern of neural networks, current
implementations \cite{NEURIPS2019_9015,tensorflow2015-whitepaper,innes2018flux}
reach a high throughput on GPUs. For the wide field of numerical simulations,
however, efficient adjoints of GPU implementations remain challenging~\cite{10.1145/3458817.3476165}. In
this work we combine the advantages of GPU implementations of the gradient
with the evaluation of Hessian-vector products first introduced in \cite{pearlmutter1994fast}.
% TODO: Note to Michel by Michel: Billy's SC21 Enzyme paper should be here

Reduced-space methods have been applied widely in
uncertainty quantification and partial differential equation (PDE)-constrained
optimization~\cite{biegler2003large}, and their applications in the optimization
of power grids is known since the 1960s~\cite{dommel1968optimal}.
However, extracting the second-order sensitivities in the reduced space
has been considered tedious to implement and hard to motivate on classical CPU architectures (see \cite{kardos2020reduced}
for a recent discussion about the computation of the reduced Hessian on the CPU).
To the best of our knowledge, this paper is the first to present a SIMD focused algorithm leveraging
the GPU to efficiently compute the reduced Hessian of the power flow
equations.

