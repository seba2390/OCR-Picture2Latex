%%%% ijcai21-multiauthor.tex

\typeout{IJCAI--21 Multiple authors example}

% These are the instructions for authors for IJCAI-21.

\documentclass{article}
\pdfpagewidth=8.5in
\pdfpageheight=11in
% The file ijcai21.sty is NOT the same than previous years'
\usepackage{ijcai21}

% Use the postscript times font!
\usepackage{times}
\renewcommand*\ttdefault{txtt}
\usepackage{soul}
\usepackage{url}
\usepackage[hidelinks]{hyperref}
\usepackage[utf8]{inputenc}
\usepackage[small]{caption}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{comment}
\usepackage{booktabs}
\urlstyle{same}
\usepackage{hyperref}
\usepackage{xcolor}
\usepackage[normalem]{ulem}
\newcommand{\cb}[2]{{\sout{#1}}{\color{cyan}#2}}
\newcommand{\am}[2]{{\sout{#1}}{\color{purple}#2}}
\newcommand{\gh}[2]{{\sout{#1}}{\color{blue}#2}}
% the following package is optional:
%\usepackage{latexsym}

% Following comment is from ijcai97-submit.tex:
% The preparation of these files was supported by Schlumberger Palo Alto
% Research, AT\&T Bell Laboratories, and Morgan Kaufmann Publishers.
% Shirley Jowell, of Morgan Kaufmann Publishers, and Peter F.
% Patel-Schneider, of AT\&T Bell Laboratories collaborated on their
% preparation.

% These instructions can be modified and used in other conferences as long
% as credit to the authors and supporting agencies is retained, this notice
% is not changed, and further modification or reuse is not restricted.
% Neither Shirley Jowell nor Peter F. Patel-Schneider can be listed as
% contacts for providing assistance without their prior permission.

% To use for other conferences, change references to files and the
% conference appropriate and use other authors, contacts, publishers, and
% organizations.
% Also change the deadline and address for returning papers and the length and
% page charge instructions.
% Put where the files are available in the appropriate places.

\title{Learning Football Body-Orientation as a Matter of Classification}

\author{
Adrià Arbués Sangüesa$^1$\and
Adrián Martín$^1$\and
Paulino Granero$^{2}$\and \\
Coloma Ballester$^1$\And
Gloria Haro$^1$
\\
\affiliations
$^1$Universitat Pompeu Fabra, $^2$Russian Football Union\\
\emails
\href{mailto:adria.arbues@upf.edu}{adria.arbues@upf.edu} 
}

% \author{
% Anonymous Submission$^1$
% \\
% \affiliations
% $^1$Anonymous Affiliation \\
% \emails
% Anonymous email
% }

\begin{document}

\maketitle

\begin{abstract}
%200 words.
Orientation is a crucial skill for football players that becomes a differential factor in a large set of events, especially the ones involving passes. However, existing orientation estimation methods, which are based on computer-vision techniques, still have a lot of room for improvement. To the best of our knowledge, this article presents the first deep learning model for estimating orientation directly from video footage. By approaching this challenge as a classification problem where classes correspond to orientation bins, and by introducing a cyclic loss function, a well-known convolutional network is refined to provide player orientation data. The model is trained by using ground-truth orientation data obtained from wearable EPTS devices, which are individually compensated with respect to the perceived orientation in the current frame. The obtained results outperform previous methods; in particular, the absolute median error is less than 12 degrees per player. An ablation study is included in order to show the potential generalization to any kind of football video footage. 
\end{abstract}

\section{Introduction}
Although deep learning (DL) has been an active field of research over the last decade, its application on top of sports data has had a slow start. The lack of universal sports datasets made it an impossible challenge for a lot of researchers, professional clubs were not aware about the unlocked potential of data-driven tools, and companies were highly focused on manual video analysis.\\
However, during this last lustrum, the whole paradigm shifted, and for instance, in the case of football, complete datasets like SoccerNet have been publicly shared
\cite{giancola2018soccernet,deliege2020soccernet}, hence providing researchers with valid resources to work with \cite{cioppa2019arthus,cioppa2020context}. At the same time, top European clubs created their own departments of data scientists while publishing their findings \cite{fernandez2018wide,llana2020right}, and companies also shifted to data-driven products based on trained large-scale models. Companies such as SciSports \cite{decroos2019actions,bransen2020player}, Sport Logiq \cite{sanford2020group}, Stats Perform \cite{sha2020end,power21} or Genius Sports \cite{quiroga2020seen} made a huge investment in research groups (in some cases, in collaboration with academia), and other companies are also sharing valuable open data \cite{metricasports,statsbomb,skillcorner}. All these facts prove that, DL is currently both trendy and useful within the context of sports analytics, thus creating a need for \textit{plug-and-play} models that could be exploited either by researchers, clubs or companies. \\
Recently, expected possession value (EPV) and expected goals models proved to produce realistic outcomes \cite{spearman2017physics,spearman2018beyond,fernandez2019decomposing}, which can be directly used by coaches to optimize their tactics. Furthermore, in this same field, Arbués-Sangüesa \textit{et al.} spotted a specific literature gap regarding the presented models: player body-orientation. The authors claimed that by merging existing methods with player orientation, the precision of existing models would improve \cite{arbues2020using}, especially in pass events. By defining orientation as the projected normal vector right in the middle of the upper-torso, the authors propose a sequential computer vision pipeline to obtain orientation data \cite{arbues2020always}. Their model stems from pose estimation, which is obtained with existing models \cite{ramakrishna2014pose,wei2016convolutional,cao2017realtime}, and achieves an absolute median error of 28 degrees, which indicates that, despite being a solid baseline, there is still room for improvement. \\ %Besides, the presented contribution has a collateral drawback that penalizes the method's generalization capability: the authors trained a coarse model to perform a double-check validation on the orientation based on color features. Since all games of their dataset include video footage from the same team, this corroboration step will only work for that specific team.\\
\begin{figure*}
\begin{center}
  \includegraphics[width=0.8\linewidth]{ReferencesThin2HD-min.jpg}
  \caption{Several domains are merged in this research: (left) sensor-, (middle) field-, and (right) image-domain. By using corners and intersection points of field lines, the corresponding homographies are used to map data across domains into one same reference system.}
  \label{fig:ref1}
 \end{center}
\end{figure*}
Therefore, in this article, a novel deep learning %\cb{fine-tuned}{} 
model to obtain orientation out of any player's bounding boxes is presented. By: (1) using sensor-based orientation data as ground truth, (2) turning this estimation into a classification problem, (3) compensating angles with respect to the camera's viewing direction, and (4) introducing a cyclic loss function based on soft labels, the network is able to estimate orientation with a median error of fewer than 12 degrees per player. \\ %Moreover, since images are converted into grayscale, the dependency on jersey-color features is reduced. \\
The rest of the paper is organized as follows: in Section \ref{sec:data} the main data structures and types of datasets are detailed; the proposed fine-tuning process is explained in Section \ref{sec:prop} together with the appropriate details about the loss function and angle compensation. Results are shown in Section \ref{sec:res}, and finally, conclusions are drawn in Section \ref{sec:conc}. 

%\section{Related Work}
\section{Data Sources} \label{sec:data}
Before introducing the proposed method, a detailed description of the required materials to train this model is given. Similarly, since we are going to mix data from different sources, their corresponding domains should be listed as well: 
\begin{itemize}
    \item \textbf{Image-domain}, which includes all kinds of data related to the associated video footage. That is: (\textit{i1}) the video footage itself, (\textit{i2}) player tracking and (\textit{i3})  corners' position. Note that the result of player tracking in the image-domain consists of a set of bounding boxes, expressed in pixels; similarly, corners' location is also expressed in pixels. In this research, full HD resolution (1920 x 1080) is considered, together with a temporal resolution of 25 frames per second.
    \item \textbf{Sensor-domain}, which gathers all pieces of data generated by wearable EPTS devices. In particular, data include: (\textit{s4}) player tracking, and (\textit{s5}) orientation data. In this case, players are tracked according to the universal latitude and longitude coordinates, and orientation data are captured with a gyroscope in all XYZ Euler angles. In this work, sensor data were gathered with RealTrack Wimu wearable devices \cite{realtrack}, which generate GPS/Orientation data at 100/10 samples per second respectively. 
    \item \textbf{Field-domain}, which expresses all variables in terms of a fixed two-dimensional football field, where the top-left corner is the origin.
\end{itemize}

\begin{figure*}
\begin{center}
  \includegraphics[width=0.95\linewidth]{PipelineMixedHD-min.jpg}
  \caption{Proposed pipeline to match sensor orientation data with bounding boxes. Different input sources are merged: (top, image-domain) video footage, which is used for player detection and jersey filtering; the resulting bounding boxes are later mapped into the field-domain. (middle, image-domain) Corner's location, which is used for building the corresponding mapping homographies, and (bottom, sensor-domain) ground-truth data, which are also mapped into the field-domain. Finally, players in the 2D-domain are matched through pairwise distances.}
  \label{fig:pip1}
 \end{center}
\end{figure*}

Once data are gathered and synchronized from different sources, two possible scenarios are faced:
\begin{itemize}
    \item The complete case, in which all variables (\textit{i1}, \textit{i2}, \textit{i3}, \textit{s4}, \textit{s5}) are available. Note that both image- and sensor-data include unique identifiers, which are easy to match by inspecting a small subset of frames.
    \item The semi-complete case, where only part of the information is available (\textit{i1},\textit{s4},\textit{s5}). In order to estimate the missing pieces (\textit{i2, i3}) and match data across domains, a sequential pipeline is proposed in Section \ref{sec:Pip1}.
\end{itemize}
In this article, both a complete and a semi-complete datasets are used, each one containing data from single games. In particular, the complete dataset contains a full game of F.C. Barcelona's Youth team recorded with a tactical camera with almost no panning and without zoom; this dataset will be named $\text{FCB}_{DS}$. The semi-complete dataset contains a full preseason match of CSKA Moscow's professional team, recorded in a practice facility (without fans) with a single static camera that zooms quite often and has severe panning.  Similarly, this second dataset will be named $\text{CSKA}_{DS}$. Furthermore, intersection and corner image-coordinates were manually identified and labelled in more than 4000 frames of $\text{CSKA}_{DS}$ (1 frame every 37, \textit{i.e.} 1.5 seconds), with a mean of 8.3 ground-truth field-spots per frame (34000 annotations).\\

\subsection{Homography Estimation}
Since the reference system of the image- and the sensor-domain is not the same, corners' positions (or line intersections) are used to translate all coordinates into the field-domain. On the one hand, obtaining field locations in the sensor domain is pretty straightforward: since its gathered coordinates are expressed with respect to the universal latitude/longitude system, the corners' locations are fixed. By using online tools such as the \textit{Satellite View} of Google Maps, and by accurately picking field intersections, the corners' latitude and longitude coordinates are obtained. On the other hand, corner's positions in the image domain (in pixels) depend on the camera shot and change across the different frames; although several literature methods \cite{citraro2020real} can be implemented in order to get the location of these field spots or the camera pose, our proposal leverages homographies computed from manual annotations. %Given at least 4 correspondences between the sensor-/image-domain and a template of the field-domain, homographies are computed with the DLT algorithm \cite{hartley2003multiple}. %; note that, depending on the type of camera shot and its perspective, it is highly suggested to include more than 4 points, specially if the dataset is not ideal. 
From now on, the homography that maps latitude/longitude coordinates into the field will be named $H_{SF}$, whereas the one that converts pixels in the image into field coordinates will be named $H_{IF}$. The complete homography-mapping process is illustrated in Figure \ref{fig:ref1}.\\

\subsection{Automatic Dataset Completion} \label{sec:Pip1}
In this Subsection, the complete process to convert a semi-complete dataset into a complete one is described. It has to be remarked that the aim is to detect players in the image-domain and to match them with sensor data, hence pairing orientation and identified bounding boxes. Note that this procedure has been applied to $\text{CSKA}_{DS}$, which did not contain ground-truth data in the image-domain. The proposed pipeline is also displayed in Figure \ref{fig:pip1}. \\
\textbf{Player Detection}: the first step is to locate players' position in the image. In order to do so, literature detection models can be used, such as OpenPose \cite{cao2017realtime} (used in this research) or Mask R-CNN \cite{maskrcnn}. Once identified all different targets in the scene, detections are converted into bounding boxes. Note that this step does not exploit any temporal information across frames.\\ 
%\paragraph{Player Detection:} 
\textbf{Jersey Filtering}: since sensor data are only acquired for one specific team, approximately half of the detected bounding boxes (opponents) are filtered out. Given that the home/away teams of football matches are required to wear distinguishable colored jerseys, a simple clustering model can be trained. Specifically, by computing and by concatenating quantized versions of the HSV / LAB histograms, a single 48-feature vector is obtained per player. Having trained a $K$-Means model, with $K=3$, boxes with three different types of content are obtained: (1) home team, (2) away team, and (3) outliers.\\
\textbf{Mapping}: in order to establish the same reference system for both sensor and image data, all tracking coordinates are mapped into the field domain. More specifically, corner-based homographies $H_{SF}$ and $H_{IF}$ are used; in the latter, since we are dealing with bounding boxes, the only point being mapped for each box is the middle point of the bottom boundary of the box.\\
\textbf{Matching}: once all points are mapped into the field-domain, a customized version of the Hungarian method \cite{kuhn1955hungarian} is implemented, thus matching sensor and image data in terms of pairwise field-distances. \\ 
%$Even though the complete set of matches may contain noisy miss-matches or missed players, this automatic pipeline is a straightforward procedure that can cross-match several data sources with notable precision.

\begin{figure}
\begin{center}
  \includegraphics[width=0.8\linewidth]{AngleRefsideCHD-min.jpg}
  \caption{Orientation references in the field-domain.}
  \label{fig:ref2}
 \end{center}
\end{figure}

\begin{figure*}
\begin{center}
  \includegraphics[width=0.7\linewidth]{OrientationMixedHD-min.jpg}
  \caption{(Top) Three players oriented towards 0º can look really different depending on the camera pose and orientation. (Bottom) Proposed technique for angle compensation: (left) detected player together with his orientation \{red\} and \textit{apparent zero-vector} \{cyan\}; (middle-left) mapped  \textit{apparent zero-vector} in the field-domain \{dashed axes - apparent reference system, continous axes - absolute reference system\} (middle-right) Applied compensation on the original orientation \{purple\}; (right) resulting compensated absolute orientation \{purple\}. }
  \label{fig:angCompAx}
 \end{center}
\end{figure*}

\section{Proposed Method} \label{sec:prop}
In this Section, the complete adaptation and fine-tuning procedure of a state-of-the-art convolutional neural network are detailed, hence resulting in a model capable of estimating body orientation directly from bounding boxes containing players. By default, all orientations are expressed in the field-domain reference system. In this bi-dimensional field it can be assumed that 0º / 90º / 180º / 270º are the corresponding orientations of players facing towards the right / top / left / bottom sides of the fields, respectively, as shown in Figure \ref{fig:ref2}. %Assuming that the recording camera is placed at the center of the field (behind one of the sidelines) and its viewing direction points towards its front, if a player is located in the same viewing direction, his/her back/chest will be spotted if he/she is oriented towards 90º/270º. \\


\subsection{Angle Compensation}
%\cb{Although the overall reference system seems to be clear enough, the absolute orientation of each player}{Although the apparent orientation of each player in the whole image frame (where field lines are frequently visible) could be enough to estimate the his/her absolute orientation, the apparent orientation of each player within his/her bounding box} 
The apparent orientation of each player is influenced by the current image content, which is drastically affected by the camera pose and its orientation. This means that, if a bounding box of a particular player is cropped without taking into account any kind of field-reference around him/her, it is not possible to obtain an absolute orientation estimation. As displayed in Figure \ref{fig:angCompAx} (top), the appearance of three players oriented towards the same direction (0 degrees) can differ a lot. Since the presented classification model only takes a bounding box as input, we propose to compensate angles \textit{a priori}, thus assuming that all orientations have been obtained under the same camera pose; \textit{i.e.} the pink camera displayed in Figure \ref{fig:ref2}. For instance, if the full chest of a player is spotted in a particular frame, its orientation must be approximately 270, no matter what the overall image context is. \\ 
In order to conduct this compensation, as seen in the bottom row of Figure \ref{fig:angCompAx}, the orientation vector of the player is first mapped into the field-domain. Then, the \textit{apparent zero-vector} is considered in the image-domain; for the reference camera, \textit{i.e.} the pink one in Figure \ref{fig:ref2}, this vector would point to the right side of the field whilst being parallel to the sidelines. By using $H_{IF}$, the \textit{apparent zero-vector} is mapped into the field-domain, and the corresponding compensation is then found by computing the angular difference between the mapped \textit{apparent zero-vector} and the reference zero-vector in the field-domain. According to Figure \ref{fig:ref2}, this difference indicates how much does the orientation vector differ from the \textit{apparent zero-vector}. \\
Formally, for a player $i$ with non-compensated orientation $\alpha_{i}'$ at position $P_{i} = (P_{i,x}, P_{i,y})$ and being the (unitary) \textit{apparent zero-vector} $Z$ described by ($1,0$), another point is defined towards the zero direction: 
\begin{equation}
    P_{i}^0 = P_{i} + Z = (P_{i,x} + 1, P_{iy})
\end{equation}
Both points $P_{i}$ and $P_{i}^0$ are mapped into the field domain by using $H_{IF}$, thus obtaining their 2D position $F_{i}$ and $F_{i}^0$, respectively.
The final compensated angle is then found as: 
\begin{equation}
    \alpha_{i} = \alpha_{i}'-\angle(\overrightarrow{F_{i}F_{i}^0}),
\end{equation}
where $\angle$ expresses the angle of the vector $\overrightarrow{F_{i}F_{i}^0}$ with respect to the reference zero-vector.
%Finally, the absolute compensated angle is obtained as: $\alpha_{i} = \alpha_{i}' - \beta_{i}$.


\begin{figure*}
\begin{center}
  \includegraphics[width=0.8\linewidth]{VGGStructHD-min.jpg}
  \caption{Proposed architecture for fine-tuning a VGG according to the main blocks of the original network.}
  \label{fig:vgg}
 \end{center}
\end{figure*}


\subsection{Network}
Once all bounding boxes have an associated compensated body-orientation value, the model is set to be trained. In this work, orientation estimation has been approached as a classification task, where each bounding box is classified within a certain number of orientation bins. 
%\textit{A priori}, it would make sense to define a regression problem, where the output of the network returns an orientation value between 0 and 360. However, several papers in the literature approach this challenge as a matter of classification \cite{diaz2019soft}, as they prove that regression scenarios are more difficult to handle with a \cb{Convolutional Network}{convolutional neural network} architecture. Therefore, an effective alternative is to transform the angle regression into classification within a certain number of orientation bins.
As detailed in Section \ref{sec:res}, orientation data are grouped into 12 bins, each one containing an orientation range of 30 degrees (\textit{e.g.} bin 1 goes from 0º to 30º, bin 2 form 30º to 60º, until bin 12, which goes from 330º to 360º). Consequently, the above-mentioned bounding boxes in the image-domain were automatically labeled with their corresponding class according to their compensated orientation. Another reason for grouping similar angles into the same class is the noisy raw orientation signals generated by the EPTS devices. \\ 
%Since the core code of OpenPose \cite{cao2017realtime} uses the output of a VGG convolutional layer to generate the set of feature maps that will describe the detected humans in terms of their pose, the chosen network to be fine-tuned in this research is also a VGG-19. 
In particular, the chosen network to be fine-tuned in this research is a VGG-19 \cite{simonyan2014very}; this type of network has also been used as a backbone in existing literature methods such as OpenPose \cite{cao2017realtime}. However, in order to further analyze and to justify our choice, alternative results are shown in Section \ref{sec:res} when using Densenet \cite{huang2017densely}. The original architecture of VGG-19 is composed of 5 convolutional blocks -each one containing either 2 or 4 convolutional layers-, and a final set of fully connected layers with a probability output vector of 1000 classes. For the presented experiments, as seen in Figure \ref{fig:vgg}, the architecture adaptation and the proposed method consists of: (1) changing the dimensions of the final fully-connected layer, thus obtaining an output with a length equal to 12, the desired number of classes, (2) freezing the weights of the first couple of convolutional blocks, (3) re-training the convolutional layers of the third block and the fully connected layers of the classifier, and (4) omitting both the fourth and fifth convolutional blocks.
By visualizing the final network weights with Score-CAM \cite{wang2020score} (Figure \ref{fig:weightViz}), it can be spotted how the most important body parts regarding orientation (upper-torso) are already being vital for the sake of classification after the third block; in fact, the responses of the fourth block do not provide useful information in terms of orientation. Therefore, omitting blocks 4 and 5 is a safe choice to have an accurate model whilst decreasing the total number of parameters to be trained. \\ 
Let us finally remark that bounding boxes' values are converted into grayscale, thus improving the overall capability of generalization, since the model will not be learning the specific jersey colors, which seemed to be one of the drawbacks in \cite{arbues2020always}. In terms of data augmentation, brightnes, and contrast random changes are performed for all boxes in the training set. 

\begin{figure}
\begin{center}
  \includegraphics[width=0.8\linewidth]{newConvLayersHD-min.jpg}
  \caption{Obtained ScoreCam responses. While the 1st block detects mainly edges and shapes, the 3rd one has a high response over the upper-torso of players. The last row shows how the 4th block learns specific features that have little to with orientation.}
  \label{fig:weightViz}
 \end{center}
\end{figure}

\begin{comment}
\begin{figure*}
\begin{center}
  \includegraphics[width=0.65\linewidth]{lossesHD-min.jpg}
  \caption{Losses.}
  \label{fig:losses}
 \end{center}
\end{figure*}
\end{comment}

\subsection{Cyclic Loss}
An important aspect of the training process is the definition of the loss function. %Once the network architecture and its global training scheme are defined, a loss function has to be chosen. 
\textit{A priori}, state-of-the-art loss functions such as binary cross-entropy could be a valid resource, but in general classification scenarios, the order and the distance within classes is not taken into account. Nonetheless, in this particular scenario, we have 12 ordered-cyclic classes and a distance between them that can be well-defined. Besides, in this classification problem, since similar orientations have been grouped into bins, enforcing a one-hot encoding is not the best solution. For example, imagine a player $P_{1}$ oriented towards 31º and another $P_{2}$ oriented towards 59º; both players are included in the second bin, which encompasses all orientations between 30-60. With one-hot encoding, it would be assumed that since both $P_{1}$ and $P_{2}$ are in the second bin, both of them have the same orientation (45º). However, alternatives such as soft labels \cite{diaz2019soft} can describe the players' class as a mixture; in the given example, the soft labels of $P_{1}$/$P_{2}$ would indicate that these players are right between the first-second/second-third bins, respectively. The other challenge to be solved is the need for this loss function to be cyclic, as the first bin (number 1, 0-30º) and the last one (12, 345-360º) are actually really close. \\
%Formally, for a player $i$ with compensated orientation $\alpha_{i}$, and being $B_{C}$ a vector of 12 positions containing the central bin values, the soft labels are defined as: 
% \begin{equation}
%     y_{i} = \frac{exp(-\phi (B_{c},\alpha_{i}))}{\sum_{k=1}^{K}{exp(-\phi(B_{c},\alpha_{i}))}}
% \end{equation}
%\cb{Formally, for a player $i$ with compensated orientation $\alpha_{i}$, and being $\chi = \{b_{1}, b_{2}, ... , b_{12}\}$, where $b_{i}$ contains the corresponding central bin value.}
Let $\{b_{1}, b_{2}, ... , b_{12}\}$ be the set of orientation classes and let $\chi = \{r_{1}, r_{2},\dots, r_{12}\}$ be the set such that each $r_j$ denotes the central angle of bin $b_{j}$, for all $j\in\{ 1,\dots,12\}$. Then, for a player $i$ with compensated ground-truth orientation $\alpha_{i}$, the soft labels representing the ground-truth probability distribution is defined as the vector with coordinates:
\begin{equation}
    y_{ij} = \frac{exp(-\phi (\alpha_{i}, r_{j}))}{\sum_{k=1}^{K}{exp(-\phi (\alpha_{i},r_{k}))}},\; \text{for}\, j=1,\dots,12
\end{equation}
where $\phi$ is cyclic distance between the ground-truth player's orientation  $\alpha_i$ and the angle corresponding to the $j$th bin, $r_{j}$:
\begin{equation}
    \phi(\alpha_{i},r_{j}) = \frac{\text{min}(|\alpha_{i}-r_{j}|,360-|\alpha_{i}-r_{j}|)^{2}}{90}.
\end{equation}
Let us denote as $x_i$ the estimated probability distribution of orientation of player $i$ obtained by applying the softmax function to the last layer of the network. Finally, our loss is the cross-entropy between $x_i$ and the ground-truth soft labels $y_i$. 

%\cb{}{Em sembla que aquesta formula no es la loss que fem, veritat? Pot ser val mes que ho parlem en directe, pero el que entenc (Adria, revisa sisplau el que he retocat fins a la formula (4)) es que ara el vector $y_i$ es el soft label ground truth, es a dir, la distribucio de probabilitat ground truth associada a la orientacio ground truth $\alpha_i$. Ara, hem de comparar lo que ens surt de la network, que "es" una distribucio de probabilitat despres d'aplicar softmax (ho has anomenat $x_i$, pero no se si volies dir $x_i=\text{softmax(de lo que surt de la network)}$)), emprant una loss que  comparai la distribucio de probabilitat estimada $x_i$ amb la distribucio ground truth o softlabel $y_i$. Veritat? Aixo es podria fer directament amb una Kullback-Leibler divergence o cross-entropy. Nosaltres com ho fem?}

\subsection{Training Setting}
%Before detailing and discussing the obtained results, let us recall the goal of this research is to classify players, according to their corresponding bounding boxes, into separate body-orientation bins. %\textcolor{olive}{Since these bounding boxes were only gathered from two different games in the given datasets (Section \ref{sec:data}), the overall training scheme has to be thought in a \textit{fair} way to show the potential generalization capability; \textit{i.e.} the network should be able to perform notably on data from both datasets despite being mainly trained with data from only one of them.}\cb{}{Alternativa a lo olive (per exemple): These bounding boxes were gathered from the only two games with different recording conditions at our disposal (Section \ref{sec:data}). Thus, a strategy that generalizes enough from the learning on it needs to be designed.}
As mentioned in Section \ref{sec:data}, bounding boxes were gathered from the only two games at our disposal (Section \ref{sec:data}), both recorded under different camera shot conditions. %Hence, a strategy that generalizes enough from the learning on it needs to be designed. 
Consequently, as seen in Figure \ref{fig:diffPl}, the content inside both bounding boxes differs a lot: while in $FCB_{DS}$ players are seen from a tactical camera and have small dimensions, players in $CSKA_{DS}$ are spotted from a camera that is almost in the same height as the playing field, thus resulting in big bounding boxes. Although all bounding boxes are resized as a preprocessing stage of the network, the raw datasets suffer from concept drift \cite{webb2016characterizing}.
\begin{figure}
\begin{center}
  \includegraphics[width=0.65\linewidth]{DiffPlayersHD-min.jpg}
  %\caption{Resized bounding boxes of both datasets; players of $FCB_{DS}$ have smaller resolution, thus several artifacts can be spotted (\textit{e.g.} JPEG, ringing, aliasing).}
    \caption{Resized bounding boxes of both datasets; several artifacts can be spotted in $FCB_{DS}$ (\textit{e.g.} JPEG, ringing, aliasing).}
  \label{fig:diffPl}
 \end{center}
\end{figure}

The proposed solution in this article %By default, and bearing in mind the limited data being used, the three raw sets to train the model would be: (1) train and (2) validation set, both including data from the ideal $FCB_{DS}$ (boxes from the first and second half footage, respectively), and (3) test set, with gathered data from $CSKA_{DS}$. However,  \\
is to build an unbalanced-mixed train set; that is, merging bounding boxes from both datasets with an unbalanced distribution in the train set, whilst using the remaining instances from $FCB_{DS}$ and $CSKA_{DS}$ on their own to build the validation and the test set, respectively. In particular, the presented experiments have been carried out with a ~90-10 distribution in the training set; for each class, a total of 4500 bounding boxes -corresponding to the first half of the games- are included, where 4000 of them are obtained from $FCB_{DS}$ and the 500 remaining ones are gathered from $CSKA_{DS}$. Both the validation and test set include 500 samples per class, all of them belonging to data from the second half. 

\section{Results} \label{sec:res}
The obtained classification results will be shown in terms of angular difference and confusion matrices. Nonetheless, it has to be remarked that, when grouping orientations, an intrinsic error is introduced: assuming that each bin contains $d$ degrees, and that a players' orientation is equal to the central bin value, properly classified players may have an associated absolute error up to $d/2$. % In the case of 12 orientation bins, $d/2 = 15$. \\
The results of six different experiments are shown in Table \ref{tab:res}: (1) $t_{12}$ and (2) $t_{24}$ use a VGG architecture that classifies into 12 and 24 orientation bins, respectively, both trained with compensated angles; (3) $t_{12nC}$ uses the same network as in $t_{12}$ but trained without angle compensation, and (4) $t_{12den}$ uses a DenseNet architecture -fine-tuning of the fourth dense block- that performs a 12-bin classification, (5) $t_{12CE}$ uses binary cross-entropy instead of the proposed cyclic loss, and (6) $t_{12CV}$ shows the performance of the existing \textit{state-of-the-art} method \cite{arbues2020always} (12 bins as well).  %The obtained losses are seen in Figure \ref{fig:losses}, where $t_{12}$ is the \gh{only}{} experiment that \gh{actually learns in}{better generalizes to} the test set; on  the contrary, the remaining experiments, apart from reaching higher losses both in train and in validation, increase the initial test loss after the first epoch.
Table \ref{tab:res} contains the mean absolute error (MEAE) and the median absolute error (MDAE) of the estimated angles in each experiment. As it can be spotted, the test of 12 classes is the one providing the most reliable test results in terms of generalization; in particular, classifying orientation into 24 classes produces better results in the validation set, but seemingly the model overfits and learns specific features that do not seem to generalize properly. Moreover, the model benefits from the cyclic loss implementation, as binary cross-entropy introduces errors both in the validation and in the test set due to the unknown distance between classes and the non-cyclic angular behavior.  Actually, the obtained boost with this cyclic loss is displayed in the confusion matrices of Figure \ref{fig:cmTests}. The addition of angle compensation also proves to be vital, especially in the test set, where the corresponding video footage contained a lot of panning and zooming. Besides, the performance of DenseNet does not seem to generalize either; however, it is likely that with an exhaustive trial-error procedure of freezing weights of particular layers and performing small changes in the original DenseNet structure, this architecture should be able to generalize as well. Finally, the existing computer-vision-based method, implemented without the model in charge of the coarse corroboration, performs the worst, obtaining more than 32 degrees in both absolute errors. 

\begin{table}[]
\begin{center}
\resizebox{0.35\textwidth}{!}{%
\begin{tabular}{|c|c|c|c|c|}
\hline
            & $\text{MEAE}_{v}$ & $\text{MDAE}_{v}$ & $\text{MEAE}_{t}$ & $\text{MDAE}_{t}$ \\ \hline
$t_{12}$    & 17.37             & 9.90              & \textbf{18.92}    & \textbf{11.60}    \\ \hline
$t_{24}$    & \textbf{13.13}    & \textbf{7.70}     & 24.34             & 13.01             \\ \hline
$t_{12CE}$  & 22.34             & 17.00             & 28.98             & 23.00             \\ \hline
$t_{12nC}$  & 21.47             & 14.16             & 31.75             & 24.54             \\ \hline
$t_{12den}$ & 15.22             & 10.46             & 25.27             & 17.29             \\ \hline
$t_{CV}$  & -                 & -                 & 38.23             & 32.09             \\ \hline
\end{tabular}}
\caption{Obtained results in all experiments, expressed in terms of the mean/median absolute error, both in the validation and test set.}
\label{tab:res}
\end{center}
\end{table}

% \begin{comment}
% \begin{table}[]
% \resizebox{0.48\textwidth}{!}{%
% \begin{tabular}{|c|c|c|c|c|c|c|c|c|c|c|c|c|}
% \hline
%              & \textbf{b1} & \textbf{b2} & \textbf{b3} & \textbf{b4} & \textbf{b5} & \textbf{b6} & \textbf{b7} & \textbf{b8} & \textbf{b9} & \textbf{b10} & \textbf{b11} & \textbf{b12} \\ \hline
% \textbf{b1}  & \textbf{314}         & 73          & 8           & 1           & 0           & 1           & 2           & 4           & 1           & 1            & 15           & 99           \\ \hline
% \textbf{b2}  & 83          & \textbf{290}         & 75          & 8           & 2           & 4           & 1           & 3           & 1           & 2            & 1            & 18           \\ \hline
% \textbf{b3}  & 10          & 99          & \textbf{333}         & 86          & 17          & 4           & 0           & 0           & 1           & 1            & 1            & 2            \\ \hline
% \textbf{b4}  & 0           & 7           & 65          & \textbf{320}         & 88          & 10          & 1           & 1           & 2           & 0            & 1            & 2            \\ \hline
% \textbf{b5}  & 0           & 1           & 9           & 71          & \textbf{293}         & 75          & 8           & 2           & 1           & 1            & 1            & 0            \\ \hline
% \textbf{b6}  & 1           & 4           & 2           & 4           & 82          & \textbf{298}         & 78          & 13          & 3           & 1            & 0            & 3            \\ \hline
% \textbf{b7}  & 1           & 0           & 3           & 2           & 9           & 89          & \textbf{298}         & 74          & 15          & 2            & 1            & 0            \\ \hline
% \textbf{b8}  & 2           & 1           & 0           & 0           & 4           & 10          & 96          & \textbf{291}         & 71          & 6            & 1            & 0            \\ \hline
% \textbf{b9}  & 1           & 1           & 3           & 2           & 1           & 6           & 11          & 98          & \textbf{310}         & 72           & 9            & 6            \\ \hline
% \textbf{b10} & 4           & 0           & 0           & 3           & 0           & 1           & 0           & 13          & 77          & \textbf{324}          & 76           & 17           \\ \hline
% \textbf{b11} & 7           & 4           & 0           & 0           & 1           & 1           & 2           & 0           & 9           & 74           & \textbf{308}          & 83           \\ \hline
% \textbf{b12} & 77          & 20          & 2           & 3           & 2           & 1           & 3           & 1           & 8           & 17           & 86           & \textbf{270}          \\ \hline
% \end{tabular}}
% \caption{Confusion Matrix test set}
% \label{tab:cmTest}
% \end{table}
% \end{comment}

% \begin{table}[]
% \resizebox{0.48\textwidth}{!}{%
% \begin{tabular}{|c|c|c|c|c|c|c|c|c|c|c|c|c}
% \hline
%                               & \textbf{b1}  & \textbf{b2}  & \textbf{b3} & \textbf{b4} & \textbf{b5} & \textbf{b6} & \textbf{b7} & \textbf{b8} & \textbf{b9} & \textbf{b10} & \textbf{b11} & \multicolumn{1}{c|}{\textbf{b12}} \\ \hline
% \textbf{b1}  & \textbf{314} & 73                            & 8                            & 1                            & 0                            & 1                            & 2                            & 4                            & 1                            & 1                             & 15                            & \multicolumn{1}{c|}{99}                            \\ \hline
% \textbf{b2}  & 83                            & \textbf{290} & 75                           & 8                            & 2                            & 4                            & 1                            & 3                            & 1                            & 2                             & 1                             & \multicolumn{1}{c|}{18}                            \\ \hline
% \textbf{(...)}                       & \multicolumn{12}{c|}{(...)}                                                                                                                                                                                                                                                                                                                                                                                                                \\ \hline
% \textbf{b11} & 7                             & 4                             & 0                            & 0                            & 1                            & 1                            & 2                            & 0                            & 9                            & 74                            & \textbf{308} & \multicolumn{1}{c|}{83}                            \\ \hline
% \textbf{b12} & 77                            & 20                            & 2                            & 3                            & 2                            & 1                            & 3                            & 1                            & 8                            & 17                            & 86                            & \multicolumn{1}{c|}{\textbf{270}} \\ \hline
% \end{tabular}}
% \caption{First and last rows of the obtained confusion matrix (test set) when using the proposed cyclic loss function ($t_{12}$).}
% \label{tab:cmTest}
% \end{table}

% \begin{comment} 
% \begin{table}[]
% \resizebox{0.48\textwidth}{!}{%
% \begin{tabular}{|c|c|c|c|c|c|c|c|c|c|c|c|c|}
% \hline
%              & \textbf{b1}  & \textbf{b2}  & \textbf{b3} & \textbf{b4}  & \textbf{b5}  & \textbf{b6}  & \textbf{b7}  & \textbf{b8}  & \textbf{b9}  & \textbf{b10} & \textbf{b11} & \textbf{b12} \\ \hline
% \textbf{b1}  & \textbf{182} & 102          & 21          & 118          & 36           & 4            & 2            & 4            & 10           & 2            & 12           & 7            \\ \hline
% \textbf{b2}  & 109          & \textbf{196} & 4           & 41           & 105          & 13           & 9            & 6            & 5            & 7            & 4            & 1            \\ \hline
% \textbf{b3}  & 45           & 12           & 123         & \textbf{146} & 5            & 5            & 2            & 4            & 9            & 13           & 48           & 88           \\ \hline
% \textbf{b4}  & 129          & 30           & 56          & \textbf{195} & 15           & 2            & 4            & 6            & 9            & 4            & 17           & 33           \\ \hline
% \textbf{b5}  & 26           & 116          & 0           & 17           & \textbf{218} & 67           & 19           & 15           & 7            & 2            & 7            & 6            \\ \hline
% \textbf{b6}  & 10           & 25           & 2           & 5            & 132          & \textbf{164} & 95           & 37           & 16           & 6            & 5            & 3            \\ \hline
% \textbf{b7}  & 7            & 5            & 3           & 1            & 45           & 98           & \textbf{141} & 128          & 56           & 9            & 5            & 2            \\ \hline
% \textbf{b8}  & 4            & 6            & 2           & 3            & 5            & 17           & 83           & \textbf{212} & 110          & 34           & 18           & 6            \\ \hline
% \textbf{b9}  & 2            & 3            & 3           & 4            & 3            & 2            & 27           & 129          & \textbf{193} & 102          & 26           & 6            \\ \hline
% \textbf{b10} & 4            & 5            & 4           & 4            & 3            & 6            & 12           & 33           & 127          & \textbf{164} & 116          & 22           \\ \hline
% \textbf{b11} & 3            & 1            & 23          & 13           & 5            & 1            & 2            & 15           & 53           & 89           & \textbf{208} & 87           \\ \hline
% \textbf{b12} & 8            & 2            & 89          & 44           & 6            & 3            & 1            & 7            & 11           & 33           & 131          & \textbf{165} \\ \hline
% \end{tabular}}
% \caption{Confusion Matrix test set - Cross-entropy}
% \label{tab:cmTestCE}
% \end{table}
% \end{comment}

% \begin{table}[]
% \resizebox{0.48\textwidth}{!}{%
% \begin{tabular}{|c|c|c|c|c|c|c|c|c|c|c|c|c}
% \hline
%                               & \textbf{b1}  & \textbf{b2}  & \textbf{b3} & \textbf{b4} & \textbf{b5} & \textbf{b6} & \textbf{b7} & \textbf{b8} & \textbf{b9} & \textbf{b10} & \textbf{b11} & \multicolumn{1}{c|}{\textbf{b12}} \\ \hline
% \textbf{b1}  & \textbf{182} & 102                           & 21                           & 118                          & 36                           & 4                            & 2                            & 4                            & 10                           & 2                             & 12                            & \multicolumn{1}{c|}{7}                             \\ \hline
% \textbf{b2}  & 109                           & \textbf{196} & 4                            & 41                           & 105                          & 13                           & 9                            & 6                            & 5                            & 7                             & 4                             & \multicolumn{1}{c|}{1}                             \\ \hline
% (...)                         & \multicolumn{12}{c|}{(...)}                                                                                                                                                                                                                                                                                                                                                                                 \\ \hline
% \textbf{b11} & 3                             & 1                             & 23                           & 13                           & 5                            & 1                            & 2                            & 15                           & 53                           & 89                            & \textbf{208} & \multicolumn{1}{c|}{87}                            \\ \hline
% \textbf{b12} & 8                             & 2                             & 89                           & 44                           & 6                            & 3                            & 1                            & 7                            & 11                           & 33                            & 131                           & \multicolumn{1}{c|}{\textbf{165}} \\ \hline
% \end{tabular}}
% \caption{First and last rows of the obtained confusion matrix (test set) when using cross-entropy as a loss function ($t_{12CE}$).}
% \label{tab:cmTestCE}
% \end{table}

\begin{comment}
\begin{table}[]
\resizebox{0.35\textwidth}{!}{%
\begin{tabular}{ccccccccccccc}
\hline
\multicolumn{1}{|c|}{}                              & \multicolumn{1}{c|}{\textbf{b1}}  & \multicolumn{1}{c|}{\textbf{b2}}  & \multicolumn{1}{c|}{\textbf{b3}} & \multicolumn{1}{c|}{\textbf{b4}} & \multicolumn{1}{c|}{\textbf{b5}} & \multicolumn{1}{c|}{\textbf{b6}} & \multicolumn{1}{c|}{\textbf{b7}} & \multicolumn{1}{c|}{\textbf{b8}} & \multicolumn{1}{c|}{\textbf{b9}} & \multicolumn{1}{c|}{\textbf{b10}} & \multicolumn{1}{c|}{\textbf{b11}} & \multicolumn{1}{c|}{\textbf{b12}} \\ \hline
\multicolumn{1}{|c|}{\textbf{b1}}  & \multicolumn{1}{c|}{\textbf{314}} & \multicolumn{1}{c|}{73}                            & \multicolumn{1}{c|}{8}                            & \multicolumn{1}{c|}{1}                            & \multicolumn{1}{c|}{0}                            & \multicolumn{1}{c|}{1}                            & \multicolumn{1}{c|}{2}                            & \multicolumn{1}{c|}{4}                            & \multicolumn{1}{c|}{1}                            & \multicolumn{1}{c|}{1}                             & \multicolumn{1}{c|}{15}                            & \multicolumn{1}{c|}{99}                            \\ \hline
\multicolumn{1}{|c|}{\textbf{b2}}  & \multicolumn{1}{c|}{83}                            & \multicolumn{1}{c|}{290}                           & \multicolumn{1}{c|}{75}                           & \multicolumn{1}{c|}{8}                            & \multicolumn{1}{c|}{2}                            & \multicolumn{1}{c|}{4}                            & \multicolumn{1}{c|}{1}                            & \multicolumn{1}{c|}{3}                            & \multicolumn{1}{c|}{1}                            & \multicolumn{1}{c|}{2}                             & \multicolumn{1}{c|}{1}                             & \multicolumn{1}{c|}{18}                            \\ \hline
\multicolumn{1}{|c|}{(...)}                         & \multicolumn{12}{c|}{(...)}                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        \\ \hline
\multicolumn{1}{|c|}{\textbf{b11}} & \multicolumn{1}{c|}{7}                             & \multicolumn{1}{c|}{4}                             & \multicolumn{1}{c|}{0}                            & \multicolumn{1}{c|}{0}                            & \multicolumn{1}{c|}{1}                            & \multicolumn{1}{c|}{1}                            & \multicolumn{1}{c|}{2}                            & \multicolumn{1}{c|}{0}                            & \multicolumn{1}{c|}{9}                            & \multicolumn{1}{c|}{74}                            & \multicolumn{1}{c|}{308}                           & \multicolumn{1}{c|}{83}                            \\ \hline
\multicolumn{1}{|c|}{\textbf{b12}} & \multicolumn{1}{c|}{77}                            & \multicolumn{1}{c|}{20}                            & \multicolumn{1}{c|}{2}                            & \multicolumn{1}{c|}{3}                            & \multicolumn{1}{c|}{2}                            & \multicolumn{1}{c|}{1}                            & \multicolumn{1}{c|}{3}                            & \multicolumn{1}{c|}{1}                            & \multicolumn{1}{c|}{8}                            & \multicolumn{1}{c|}{17}                            & \multicolumn{1}{c|}{86}                            & \multicolumn{1}{c|}{270}                           \\ \hline
\multicolumn{1}{l}{}                                & \multicolumn{1}{l}{}                               & \multicolumn{1}{l}{}                               & \multicolumn{1}{l}{}                              & \multicolumn{1}{l}{}                              & \multicolumn{1}{l}{}                              & \multicolumn{1}{l}{}                              & \multicolumn{1}{l}{}                              & \multicolumn{1}{l}{}                              & \multicolumn{1}{l}{}                              & \multicolumn{1}{l}{}                               & \multicolumn{1}{l}{}                               & \multicolumn{1}{l}{}                               \\ \hline
\multicolumn{1}{|c|}{}                              & \multicolumn{1}{c|}{\textbf{b1}}  & \multicolumn{1}{c|}{\textbf{b2}}  & \multicolumn{1}{c|}{\textbf{b3}} & \multicolumn{1}{c|}{\textbf{b4}} & \multicolumn{1}{c|}{\textbf{b5}} & \multicolumn{1}{c|}{\textbf{b6}} & \multicolumn{1}{c|}{\textbf{b7}} & \multicolumn{1}{c|}{\textbf{b8}} & \multicolumn{1}{c|}{\textbf{b9}} & \multicolumn{1}{c|}{\textbf{b10}} & \multicolumn{1}{c|}{\textbf{b11}} & \multicolumn{1}{c|}{\textbf{b12}} \\ \hline
\multicolumn{1}{|c|}{\textbf{b1}}  & \multicolumn{1}{c|}{\textbf{182}} & \multicolumn{1}{c|}{102}                           & \multicolumn{1}{c|}{21}                           & \multicolumn{1}{c|}{118}                          & \multicolumn{1}{c|}{36}                           & \multicolumn{1}{c|}{4}                            & \multicolumn{1}{c|}{2}                            & \multicolumn{1}{c|}{4}                            & \multicolumn{1}{c|}{10}                           & \multicolumn{1}{c|}{2}                             & \multicolumn{1}{c|}{12}                            & \multicolumn{1}{c|}{7}                             \\ \hline
\multicolumn{1}{|c|}{\textbf{b2}}  & \multicolumn{1}{c|}{109}                           & \multicolumn{1}{c|}{\textbf{196}} & \multicolumn{1}{c|}{4}                            & \multicolumn{1}{c|}{41}                           & \multicolumn{1}{c|}{105}                          & \multicolumn{1}{c|}{13}                           & \multicolumn{1}{c|}{9}                            & \multicolumn{1}{c|}{6}                            & \multicolumn{1}{c|}{5}                            & \multicolumn{1}{c|}{7}                             & \multicolumn{1}{c|}{4}                             & \multicolumn{1}{c|}{1}                             \\ \hline
\multicolumn{1}{|c|}{(...)}                         & \multicolumn{12}{c|}{(...)}                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        \\ \hline
\multicolumn{1}{|c|}{\textbf{b11}} & \multicolumn{1}{c|}{3}                             & \multicolumn{1}{c|}{1}                             & \multicolumn{1}{c|}{23}                           & \multicolumn{1}{c|}{13}                           & \multicolumn{1}{c|}{5}                            & \multicolumn{1}{c|}{1}                            & \multicolumn{1}{c|}{2}                            & \multicolumn{1}{c|}{15}                           & \multicolumn{1}{c|}{53}                           & \multicolumn{1}{c|}{89}                            & \multicolumn{1}{c|}{\textbf{208}} & \multicolumn{1}{c|}{87}                            \\ \hline
\multicolumn{1}{|c|}{\textbf{b12}} & \multicolumn{1}{c|}{8}                             & \multicolumn{1}{c|}{2}                             & \multicolumn{1}{c|}{89}                           & \multicolumn{1}{c|}{44}                           & \multicolumn{1}{c|}{6}                            & \multicolumn{1}{c|}{3}                            & \multicolumn{1}{c|}{1}                            & \multicolumn{1}{c|}{7}                            & \multicolumn{1}{c|}{11}                           & \multicolumn{1}{c|}{33}                            & \multicolumn{1}{c|}{131}                           & \multicolumn{1}{c|}{\textbf{165}} \\ \hline
\end{tabular}}
\caption{First and last rows of the obtained confusion matrix (test set) when using the (top) proposed cyclic and (bottom) binary cross-entropy as a loss function ($t_{12}$ and $t_{12CE}$ respectively).}
\label{tab:cmTests}
\end{table}
\end{comment}

\begin{figure}
\begin{center}
  \includegraphics[width=0.75\linewidth]{ConfMats-min.jpg}
  %\caption{Resized bounding boxes of both datasets; players of $FCB_{DS}$ have smaller resolution, thus several artifacts can be spotted (\textit{e.g.} JPEG, ringing, aliasing).}
    \caption{First and last rows of the obtained confusion matrix (test set) when using the (top) proposed cyclic and (bottom) binary cross-entropy as a loss function ($t_{12}$ and $t_{12CE}$ respectively).}
  \label{fig:cmTests}
 \end{center}
\end{figure}

\section{Conclusions} \label{sec:conc}
In this article, a novel DL model to estimate the orientation of football players is presented. More concretely, the fine-tuned model learns how to classify players' crops into orientation bins. The core of this method combines a VGG structure with frozen and re-trained layers, an angle compensation strategy to get rid of the camera behavior, and a cyclic loss function based on soft labels that take the intra-class distance into account. The obtained results outperform (by a large margin of more than 20 degrees) the existing state-of-the-art computer-vision method with a MDAE of 11.60 degrees in the test set. Moreover, since complete datasets are difficult to gather, a sequential-based pipeline has also been proposed, which fuses data from different domains in order to establish the ground truth orientation of the player (sensor-domain) in each bounding box (image-domain). 
%, where data from the sensor-domain can be matched with image-based one, thus creating complete datasets. \\
The main limitation of the presented model is that only two different games were used in the given dataset, as ground-truth sensor data (together with high-quality frames) are difficult to obtain. This research shows that even with unbalanced training sets it is possible to train a model with certain generalization capabilities, hence promising results should be obtained with a more varied and balanced dataset in terms of different games. As future work, apart from (a) analyzing the model extension to other football datasets and (b) testing the same approach in a regression-based fashion, its potential intra-sport generalization will be studied as well. Besides, the inclusion of fine-grained orientation into existing EPV models could lead to better performance. \\ 

\section*{Acknowledgements}
The authors acknowledge partial support by MICINN/FEDER UE project, ref. PGC2018-098625-B-I00, H2020-MSCA-RISE-2017 project, ref. 777826 NoMADS, EU H-2020 grant 952027 (project AdMiRe), and RED2018-102511-T. Besides, we also acknowledge RealTrack Systems' (in particular, Carlos Padilla's) and F.C. Barcelona's data support.
%WIMU

%\bibliographystyle{named}
%\bibliography{ijcai21}

\begin{thebibliography}{}

\bibitem[\protect\citeauthoryear{Arbu{\'e}s-Sang{\"u}esa \bgroup \em et
  al.\egroup }{2020a}]{arbues2020using}
Adri{\`a} Arbu{\'e}s-Sang{\"u}esa, Adrian Martin, Javier Fern{\'a}ndez, Coloma
  Ballester, and Gloria Haro.
\newblock Using player's body-orientation to model pass feasibility in soccer.
\newblock In {\em Proceedings of the IEEE/CVF Conference on Computer Vision and
  Pattern Recognition Workshops}, pages 886--887, 2020.

\bibitem[\protect\citeauthoryear{Arbu{\'e}s-Sang{\"u}esa \bgroup \em et
  al.\egroup }{2020b}]{arbues2020always}
Adri{\`a} Arbu{\'e}s-Sang{\"u}esa, Adri{\'a}n Mart{\'\i}n, Javier
  Fern{\'a}ndez, Carlos Rodr{\'\i}guez, Gloria Haro, and Coloma Ballester.
\newblock Always look on the bright side of the field: Merging pose and
  contextual data to estimate orientation of soccer players.
\newblock In {\em Proceedings of the International Conference on Image
  Processing}, 2020.

\bibitem[\protect\citeauthoryear{Bransen and Haaren}{2020}]{bransen2020player}
Lotte Bransen and Jan~Van Haaren.
\newblock Player chemistry: Striving for a perfectly balanced soccer team,
  2020.

\bibitem[\protect\citeauthoryear{Cao \bgroup \em et al.\egroup
  }{2017}]{cao2017realtime}
Zhe Cao, Tomas Simon, Shih-En Wei, and Yaser Sheikh.
\newblock Realtime multi-person 2d pose estimation using part affinity fields.
\newblock In {\em Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pages 7291--7299, 2017.

\bibitem[\protect\citeauthoryear{Cioppa \bgroup \em et al.\egroup
  }{2019}]{cioppa2019arthus}
Anthony Cioppa, Adrien Deliege, Maxime Istasse, Christophe De~Vleeschouwer, and
  Marc Van~Droogenbroeck.
\newblock Arthus: Adaptive real-time human segmentation in sports through
  online distillation.
\newblock In {\em Proceedings of the IEEE/CVF Conference on Computer Vision and
  Pattern Recognition Workshops}, pages 0--0, 2019.

\bibitem[\protect\citeauthoryear{Cioppa \bgroup \em et al.\egroup
  }{2020}]{cioppa2020context}
Anthony Cioppa, Adrien Deliege, Silvio Giancola, Bernard Ghanem, Marc~Van
  Droogenbroeck, Rikke Gade, and Thomas~B Moeslund.
\newblock A context-aware loss function for action spotting in soccer videos.
\newblock In {\em Proceedings of the IEEE/CVF Conference on Computer Vision and
  Pattern Recognition}, pages 13126--13136, 2020.

\bibitem[\protect\citeauthoryear{Citraro \bgroup \em et al.\egroup
  }{2020}]{citraro2020real}
Leonardo Citraro, Pablo M{\'a}rquez-Neila, Stefano Savar{\`e}, Vivek Jayaram,
  Charles Dubout, F{\'e}lix Renaut, Andr{\'e}s Hasfura, Horesh~Ben Shitrit, and
  Pascal Fua.
\newblock Real-time camera pose estimation for sports fields.
\newblock {\em Machine Vision and Applications}, 31(3):1--13, 2020.

\bibitem[\protect\citeauthoryear{Decroos \bgroup \em et al.\egroup
  }{2019}]{decroos2019actions}
Tom Decroos, Lotte Bransen, Jan Van~Haaren, and Jesse Davis.
\newblock Actions speak louder than goals: Valuing player actions in soccer.
\newblock In {\em Proceedings of the 25th ACM SIGKDD International Conference
  on Knowledge Discovery \& Data Mining}, pages 1851--1861, 2019.

\bibitem[\protect\citeauthoryear{Deli{\`e}ge \bgroup \em et al.\egroup
  }{2020}]{deliege2020soccernet}
Adrien Deli{\`e}ge, Anthony Cioppa, Silvio Giancola, Meisam~J Seikavandi,
  Jacob~V Dueholm, Kamal Nasrollahi, Bernard Ghanem, Thomas~B Moeslund, and
  Marc Van~Droogenbroeck.
\newblock Soccernet-v2: A dataset and benchmarks for holistic understanding of
  broadcast soccer videos.
\newblock {\em arXiv preprint arXiv:2011.13367}, 2020.

\bibitem[\protect\citeauthoryear{Diaz and Marathe}{2019}]{diaz2019soft}
Raul Diaz and Amit Marathe.
\newblock Soft labels for ordinal regression.
\newblock In {\em Proceedings of the IEEE/CVF Conference on Computer Vision and
  Pattern Recognition}, pages 4738--4747, 2019.

\bibitem[\protect\citeauthoryear{Fernandez and Bornn}{2018}]{fernandez2018wide}
Javier Fernandez and Luke Bornn.
\newblock Wide open spaces: A statistical technique for measuring space
  creation in professional soccer.
\newblock In {\em Sloan Sports Analytics Conference}, volume 2018, 2018.

\bibitem[\protect\citeauthoryear{Fern{\'a}ndez \bgroup \em et al.\egroup
  }{2019}]{fernandez2019decomposing}
Javier Fern{\'a}ndez, Luke Bornn, and Dan Cervone.
\newblock Decomposing the immeasurable sport: A deep learning expected
  possession value framework for soccer.
\newblock In {\em 13 th Annual MIT Sloan Sports Analytics Conference}, 2019.

\bibitem[\protect\citeauthoryear{Giancola \bgroup \em et al.\egroup
  }{2018}]{giancola2018soccernet}
Silvio Giancola, Mohieddine Amine, Tarek Dghaily, and Bernard Ghanem.
\newblock Soccernet: A scalable dataset for action spotting in soccer videos.
\newblock In {\em Proceedings of the IEEE Conference on Computer Vision and
  Pattern Recognition Workshops}, pages 1711--1721, 2018.

\bibitem[\protect\citeauthoryear{He \bgroup \em et al.\egroup
  }{2017}]{maskrcnn}
Kaiming He, Georgia Gkioxari, Piotr Doll{\'a}r, and Ross Girshick.
\newblock Mask r-cnn.
\newblock In {\em Proceedings of the IEEE international conference on computer
  vision}, pages 2961--2969, 2017.

\bibitem[\protect\citeauthoryear{Huang \bgroup \em et al.\egroup
  }{2017}]{huang2017densely}
Gao Huang, Zhuang Liu, Laurens Van Der~Maaten, and Kilian~Q Weinberger.
\newblock Densely connected convolutional networks.
\newblock In {\em Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pages 4700--4708, 2017.

\bibitem[\protect\citeauthoryear{Kuhn}{1955}]{kuhn1955hungarian}
Harold~W Kuhn.
\newblock The hungarian method for the assignment problem.
\newblock {\em Naval research logistics quarterly}, 2(1-2):83--97, 1955.

\bibitem[\protect\citeauthoryear{Llana \bgroup \em et al.\egroup
  }{2020}]{llana2020right}
Sergio Llana, Pau Madrero, and Javier Fern{\'a}ndez.
\newblock The right place at the right time: Advanced off-ball metrics for
  exploiting an opponent’s spatial weaknesses in soccer.
\newblock In {\em Proceedings of the 14th MIT Sloan Sports Analytics
  Conference}, 2020.

\bibitem[\protect\citeauthoryear{MetricaSports}{2020}]{metricasports}
MetricaSports.
\newblock {Metrica Sports - Football Open Data}.
\newblock \url{https://metrica-sports.com/football-open-data/}, 2020.
\newblock Accessed: 23-04-21.

\bibitem[\protect\citeauthoryear{Quiroga \bgroup \em et al.\egroup
  }{2020}]{quiroga2020seen}
Julian Quiroga, Henry Carrillo, Edisson Maldonado, John Ruiz, and Luis~M
  Zapata.
\newblock As seen on tv: Automatic basketball video production using
  gaussian-based actionness and game states recognition.
\newblock In {\em Proceedings of the IEEE/CVF Conference on Computer Vision and
  Pattern Recognition Workshops}, pages 894--895, 2020.

\bibitem[\protect\citeauthoryear{Ramakrishna \bgroup \em et al.\egroup
  }{2014}]{ramakrishna2014pose}
Varun Ramakrishna, Daniel Munoz, Martial Hebert, James~Andrew Bagnell, and
  Yaser Sheikh.
\newblock Pose machines: Articulated pose estimation via inference machines.
\newblock In {\em European Conference on Computer Vision}, pages 33--47.
  Springer, 2014.

\bibitem[\protect\citeauthoryear{RealTrack}{2018}]{realtrack}
RealTrack.
\newblock {RealTrack Systems}.
\newblock \url{http://www.realtracksystems.com/}, 2018.
\newblock Accessed: 23-04-21.

\bibitem[\protect\citeauthoryear{Sanford \bgroup \em et al.\egroup
  }{2020}]{sanford2020group}
Ryan Sanford, Siavash Gorji, Luiz~G Hafemann, Bahareh Pourbabaee, and Mehrsan
  Javan.
\newblock Group activity detection from trajectory and video data in soccer.
\newblock In {\em Proceedings of the IEEE/CVF Conference on Computer Vision and
  Pattern Recognition Workshops}, pages 898--899, 2020.

\bibitem[\protect\citeauthoryear{Sha \bgroup \em et al.\egroup
  }{2020}]{sha2020end}
Long Sha, Jennifer Hobbs, Panna Felsen, Xinyu Wei, Patrick Lucey, and Sujoy
  Ganguly.
\newblock End-to-end camera calibration for broadcast videos.
\newblock In {\em Proceedings of the IEEE/CVF Conference on Computer Vision and
  Pattern Recognition}, pages 13627--13636, 2020.

\bibitem[\protect\citeauthoryear{Simonyan and
  Zisserman}{2014}]{simonyan2014very}
Karen Simonyan and Andrew Zisserman.
\newblock Very deep convolutional networks for large-scale image recognition.
\newblock {\em arXiv preprint arXiv:1409.1556}, 2014.

\bibitem[\protect\citeauthoryear{SkillCorner}{2020}]{skillcorner}
SkillCorner.
\newblock {SkillCorner - Open Data}.
\newblock \url{https://github.com/SkillCorner/opendata}, 2020.
\newblock Accessed: 23-04-21.

\bibitem[\protect\citeauthoryear{Spearman \bgroup \em et al.\egroup
  }{2017}]{spearman2017physics}
William Spearman, Austin Basye, Greg Dick, Ryan Hotovy, and Paul Pop.
\newblock Physics-based modeling of pass probabilities in soccer.
\newblock In {\em Proceeding of the 11th MIT Sloan Sports Analytics
  Conference}, 2017.

\bibitem[\protect\citeauthoryear{Spearman}{2018}]{spearman2018beyond}
William Spearman.
\newblock Beyond expected goals.
\newblock In {\em Proceedings of the 12th MIT sloan sports analytics
  conference}, pages 1--17, 2018.

\bibitem[\protect\citeauthoryear{StatsBomb}{2020}]{statsbomb}
StatsBomb.
\newblock {StatsBomb - Open Data}.
\newblock \url{https://github.com/statsbomb/open-data}, 2020.
\newblock Accessed: 23-04-21.

\bibitem[\protect\citeauthoryear{Stöckl \bgroup \em et al.\egroup
  }{2022}]{power21}
Michael Stöckl, Thomas Seidl, Daniel Marley, and Paul Power.
\newblock Making offensive play predictable - using a graph convolutional
  network to understand defensive performance in soccer.
\newblock In {\em Proceedings of the 15th MIT Sloan Sports Analytics
  Conference}, 2022.

\bibitem[\protect\citeauthoryear{Wang \bgroup \em et al.\egroup
  }{2020}]{wang2020score}
Haofan Wang, Zifan Wang, Mengnan Du, Fan Yang, Zijian Zhang, Sirui Ding, Piotr
  Mardziel, and Xia Hu.
\newblock Score-cam: Score-weighted visual explanations for convolutional
  neural networks.
\newblock In {\em Proceedings of the IEEE/CVF Conference on Computer Vision and
  Pattern Recognition Workshops}, pages 24--25, 2020.

\bibitem[\protect\citeauthoryear{Webb \bgroup \em et al.\egroup
  }{2016}]{webb2016characterizing}
Geoffrey~I Webb, Roy Hyde, Hong Cao, Hai~Long Nguyen, and Francois Petitjean.
\newblock Characterizing concept drift.
\newblock {\em Data Mining and Knowledge Discovery}, 30(4):964--994, 2016.

\bibitem[\protect\citeauthoryear{Wei \bgroup \em et al.\egroup
  }{2016}]{wei2016convolutional}
Shih-En Wei, Varun Ramakrishna, Takeo Kanade, and Yaser Sheikh.
\newblock Convolutional pose machines.
\newblock In {\em Proceedings of the IEEE conference on Computer Vision and
  Pattern Recognition}, pages 4724--4732, 2016.

\end{thebibliography}

\end{document}

