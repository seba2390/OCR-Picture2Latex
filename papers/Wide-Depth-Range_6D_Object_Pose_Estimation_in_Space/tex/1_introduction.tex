% !TEX root = ../top.tex
% !TEX spellcheck = en-US

\section{Introduction}
\label{sec:introduction}

Reliable 6D pose estimation is key to automating many spatial maneuvers, such
as docking or capturing inert objects as shown in Fig.~\ref{fig:docking}. An
important consequence of such maneuvers is that they dramatically change the
scale and aspect of the observed target. Although 6D pose estimation is an
active area of research in computer vision and robotics, this important aspect
has not received significant attention thus far---for example, most benchmark
datasets~\cite{Hinterstoisser12b,Krull15,Xiang18b,Hodan18} feature objects whose depth varies within a limited range.
The lack of atmospheric scattering enabling observation from great
distances also leads to other challenges: harsh contrast, under- and
over-exposed areas, and significant specular reflections from reflective
materials used in space engineering (aluminium and carbon fiber panels, etc.).
% \WJ{(Many edits here, and in the abstract -- remove this comment, if they are okay.)}
% \YH{Seems great}

\input{fig/clearspace.tex}
% \input{fig/approach.tex}

To address such challenges, the European Space Agency (ESA) and Stanford
University recently organized a satellite pose estimation challenge based on
the \emph{Spacecraft Pose Estimation Dataset} (SPEED)~\cite{Kisantal20}. The best-performing
methods in this competition use a two-step approach to handle large depth
variation: a detector finds an axis-aligned box bounding the target, which is
resampled to a uniform size and finally processed by a 6D pose estimator.

This approach is suboptimal in several ways. First, detection and pose
estimation are treated as separate processes, which precludes joint training.
Second, it provides supervisory signals only to the final layer of the
encoder-decoder architecture being used instead of to all levels of the
decoding pyramid, which would increase robustness. Third, many similar feature
extraction computations are performed by both processes, which results in an
unnecessary duplication of effort. Finally, these methods rely on the dominant approach
to deep learning based 6D object pose estimation~\cite{Rad17,Hu19a,Chen19DLR} consisting of
training a network to minimize the 2D reprojection error of predefined 3D
keypoints, which cannot cope with large depth range variations: As shown
in Fig.~\ref{fig:cube_problem}, reprojection error is strongly affected by the
distance of individual keypoints to the camera, and not explicitly taking this
into account degrades performance. 
% \WJ{Use of ``keypoint'' in this paragraph seems unnecessarily technical, why not ``position''? Also applies to Fig 2 caption.}\YH{It may be confused with other points on the object surface. We only use the 8 corners of the 3D object bounding box of the object. So we say they are ``key'' points. And most literature uses this terminology.}

To address these shortcomings, we introduce a single hierarchical end-to-end trainable network depicted by Fig.~\ref{fig:arch} that yields robust and scale-insensitive 6D poses. 
To use information across scales, it progressively downscales the learned features,  derives 3D-to-2D correspondences for each level of the resulting pyramid, and finally uses a RANSAC-based PnP strategy to infer a single reliable pose from these sets of correspondences. This is a departure from most networks that estimate pose only from the final layer. 
To address the issue in Fig.~\ref{fig:cube_problem}, we minimize a training loss based on 3D positions instead of 2D projections, making the method invariant to the target distance.
We use a Feature Pyramid Network (FPN)~\cite{Lin17e} as our backbone but, unlike in most approaches relying on such networks, we assign each training instance to multiple pyramid levels to promote the joint use of multi-scale information. 

 %
In short, our contribution is a new 6D pose estimation architecture that reliably handles large scale changes under challenging conditions. We will show that it outperforms all state-of-the-art methods on the established SPEED dataset while also being much faster. Furthermore, we introduce a larger-scale satellite pose estimation dataset featuring more realistic and more complex images than SPEED, and we show that our method delivers the same benefits in this more challenging scenario. Finally, we demonstrate that our method outperforms the state of the art even on images with smaller depth variations, such as those of the challenging Occluded LINEMOD dataset. Our code and new dataset will be publicly released.

\input{fig/cube_problem.tex}
\input{fig/arch.tex}

%\PF{Based on our zoom meeting, I would recommend showing 
%\begin{itemize}
% \item quantitative results on SPEED. 
% \item qualitative results on VESPA to match Fig. 1. 
% \item qualitative results on real images of the space cube with a discussion about doing better in future work using DA. 
%\end{itemize}
%}
%
%\YH{Thank you. I will do it asap.}

%-------
% OLD
%-------

%Estimating 6D object pose has became an essential component of many real-world vision applications, including robotics,  machine perception, and augmented reality etc~\cite{xx}. On the other hand, estimating 6D pose of space objects, such as satellites and orbit debris, is drawing significant attention as the increasing congestion in Earth orbits~\cite{xx}.

%In a typical wide-depth-range scenario, estimating 6D object pose will be much more challenge due to the scaling problems as the estimation of farther samples will be dominated by closer ones~\cite{xx}. The most straightforward way to handle this probelm is to introduce a object detection network as a preprocessing component, and scale all the detected bounding boxes to the same size before feeding them into another pose regression network. However, this type of strategy equipping with two separated networks is heavy and suboptimal in practice~\cite{xx} (see Fig.~\ref{fig:pyramid_vs_twonetworks}).
