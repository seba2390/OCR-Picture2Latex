% !TEX root = ../top.tex
% !TEX spellcheck = en-US

\section{Related Work}
\label{sec:related}

%Mounting a depth sensor or LiDAR onto a spacecraft is still not very common due to economic, stability, or power consumption reasons. In this work, we will focus on RGB-based 6D object pose estimation.
The most commonly-used sensors for 6D pose estimation in space remain cameras, may they be RGB, monochromatic, or, although more rarely, infrared. 
% \WJ{That seems potentially dubious without presenting any evidence. I could imagine that a faster monochromatic sensor may preferable sometimes.
% How about saying that you focus on RGB data, but the method works in principle for any kind of image-based sensor including monochromatic
% visible and infrared cameras?} \MS{Correct. In fact the original statement contrasted cameras with LiDAR, and my edit lost this point. I rephrased.}
We therefore focus on image-based 6D pose estimation in both our work and the discussion below.

The standard framework to perform 6D pose estimation consists of first establishing 3D-to-2D correspondences, and then compute the pose using a PnP solver~\cite{Lu00,Tulsiani15,Pavlakos17a}. While many hancrafted methods have been designed to extract the required correspondences~\cite{Lowe04,Tola10,Trzcinski12c}, they tend to produce low-quality output under challenging conditions (objects lacking spatial variation, strong highlights, etc.). As such, most modern 6D object pose estimation methods establish such correspondences using a neural network. This network is usually trained to predict the image location of the 3D object bounding box corners, either in a single global fashion~\cite{Kehl17,Rad17,Tekin18a,Xiang18b}, or by aggregating multiple local predictions to improve robustness to occlusions~\cite{Oberweger18,Jafari18,Hu19a,Peng19a,Zakharov19a,Li19a}. 
%Segmentation-driven 6D object pose estimation~\cite{Hu19a} first claim that local predictions from the feature cell located within the segmentation mask can be fused to a single robust 6D pose results. PVNet~\cite{Peng19a} adopts a similar manner while using a different representation of local predictions. Most of these methods only consider the case in a fixed image scale and can not handle the cases with apparent scale drastically changed. Our method pushes the limit of this type of method to wide-depth-range scenarios in a single compact hierarchical framework. 
Whether global or local, these methods were designed to be effective on standard computer vision benchmarks, which feature minimal scale changes. As we will show in our experiments, they therefore perform poorly when the depth range at which the object is depicted varies dramatically across different images.

The few works that have attempted to handle the scale issue rely on an object detection network as a preprocessing component~\cite{Li18a,Li19a,Chen19DLR}. While the zoom sampling strategy introduced in~\cite{Li19a} aims to account for the object detection noise when training the pose network, it still does not reflect the true distribution of the patches output by the detection network, and the resulting framework does not unify the detection and pose estimation stages.  While this could in principle be achieved via a Spatial Transformer Network~\cite{Jaderberg15}, such a change would significantly complicate the architecture, introducing redundant operations across the detection and pose estimation modules and eventually precluding real-time inference. Our main contribution entails using the inherent hierarchical structure of a single network with shared weights across the levels to handle the scale problem. We demonstrate this to be both robust and efficient.

Hierarchical processing, such as image pyramids~\cite{Bartoli08,Hu16,Jing18}, is a classical idea for multi-scale image understanding~\cite{Hu16a,Hu18d}. Recently, this idea has been translated to the deep learning realm via Feature Pyramid Networks (FPNs)~\cite{Lin17e}, which are now a standard component of many object detection frameworks~\cite{Lin17f,Tian19b,Zhang20d}. Here, we leverage this idea for 6D object pose estimation. However, unlike most object detection methods that explicitly associate each pyramid level to a single, predefined scale, we introduce a dynamic sampling strategy where each training instance leverages all pyramid levels, albeit with different weights. This allows us to fuse the predictions from the different levels at inference, leading to more robust 6D pose estimates.

\input{fig/swisscube_vs_speed_demo.tex}

We focus our experiments on 6D pose estimation of space-borne objects, because robustness to scale is highly important in that context, particularly when approaching non-cooperative targets (e.g. space trash)
% \WJ{Removed the word "non-cooperative" here and in the intro. It sounds too much like the object actively fights being approached :-)} \MS{But this is the term commonly used in the space engineering literature.}\YH{Fixed}
that require motion synchronization. The space engineering community has its own literature on the topic of 6D pose estimation. While it has evolved in a manner that resembles progress in computer vision, it has mostly focused on handcrafted methods~\cite{Zhang05b,Amico14,Petit11,Sharma18b}, with only a few works proposing deep learning based approaches~\cite{Chen19DLR}. The main reason for this is the lack of large amounts of annotated data for space-borne objects. 
Recently, this was addressed by the SPEED dataset~\cite{Kisantal20} released by ESA and Stanford University as part of a satellite pose estimation challenge. This dataset, however, has several limitations. First, it does not provide the 3D model of the satellite, and while it can be reconstructed from the images, the final pose estimate will depend not only on the pose estimation algorithm but also on the quality of this reconstruction. 
% \WJ{It's not immediately apparent why that is a problem, other than maybe wanting to generate more data? It would be useful to have the model for a differentiable rendering-based optimization to improve an initial guess by a neural network, but that is probably beyond the scope of this discussion.} \YH{This is not related to the scale of the data, but to fair comparisons. Without an accurate 3D model, the evaluation of the 6D object pose will depend on many other factors, especially how you reconstruct the 3D model yourself. So we need a dataset with released accurate 3D model to get rid of these prerequisites.} 
Second, the SPEED images were synthesized by a non-physics-based rendering technique, only poorly reflecting the complexity of illumination in space, as illustrated in Fig.~\ref{fig:swisscube_vs_speed_demo}. 
% \WJ{is there something more concrete that could be said about this lack of realism? I would say that light transport in space is actually *much* easier than on earth, because there is no atmospheric scattering, and most directions are simply black. In some sense, even realistically generated space-based images look "fake" :-)} \YH{Have added a new figure}
% Finally, the SPEED dataset remains of relatively small size, with only XXX training images. \MS{Complete.} 
Finally, the depth distribution of the SPEED dataset is not uniform, with only few images depicting the satellite at a large distance from the camera. However, accurate pose for farther objects can be critical for space rendezvous; they give the docker or chaser enough time to adjust its own motion and prepare for the actual operations.
% and do other preparations at the very first time. 
% \WJ{Beginning of this sentence doesn't parse for me. First time a "grabber" is mentioned in this paper, should this occur earlier?}\YH{rephrased.}
We propose a novel satellite pose estimation dataset that addresses this bias,
and constitutes the second contribution of this article. The images in
this dataset were created using a physically-based spectral light transport simulation
involving an accurate reference 3D model of a cube satellite that accounts for
the effects of the Sun, Earth, stars, etc.
