% !TEX root = ../top.tex
% !TEX spellcheck = en-US

\section{Experiments}
\label{sec:experiments}

In this section, we first evaluate our framework on the SPEED dataset, and then introduce the SwissCube dataset, which contains accurate 3D mesh and physically-modeled astronomical objects, and perform thorough ablation studies on it. We further show results on real images of the same satellite. Finally, to demonstrate the generality of our approach we evaluate it on the standard Occluded-LINEMOD dataset depicting small depth variations. 
% \WJ{"CubeSat" leaks author nationalities and even institution (it's an EPFL project). In my community, this would be perceived negatively during peer review. I'd suggest using a temporary name ("NanoSat") with an asterisk/footnote saying that the dataset name is temporarily anonymized as not to reveal authorship.} \YH{I am not sure if it is in CV, while it does not hurt to change to a temporary name.} \MS{I agree, but I suggest "CubeSat", which is the standard term for this type of satellite.}

% \yh{
We train our model starting from a backbone pre-trained on ImageNet~\cite{Deng09}, and, for any 6D pose dataset, feed it 3M unique training samples obtained via standard online data augmentation strategies, such as random shift, scale, and rotation. To evaluate the accuracy, we will report the individual performance under different depth ranges, using the standard ADI-0.1d~\cite{Hu19a,Hu20a} accuracy metrics, which encodes the percentage of samples whose 3D reconstruction error is below 10\% of the object diameter. On the SPEED dataset, however, we use a different metric, as we do not have access to the 3D SPEED model, making the computation of ADI impossible. Instead, we use the metric from the competition, that is, ${\bf e}_{\bf q}+{\bf e}_{\bf t}$, where ${\bf e}_{\bf q}$ is the angular error between the ground-truth quaternion and the predicted one, and ${\bf e}_{\bf t}$ is the normalized translation error. Furthermore, because the depth distribution of SPEED is not uniform, with only few images depicting the satellite at a large distance from the camera, we only report the average error on the whole test set, as in the competition.
% }
The source code and dataset are publicly available at \href{https://github.com/cvlab-epfl/wide-depth-range-pose}{https://github.com/cvlab-epfl/wide-depth-range-pose}.

\subsection{Evaluation on the SPEED Dataset}
Although the SPEED dataset has several drawbacks, discussed in Section~\ref{sec:related}, it remains a valuable benchmark, and we thus begin by evaluating our method on it. As the test annotations are not publicly available, and the competition is not ongoing, we divide the training set into two parts, 10K images for training and the remaining 2K ones for testing.
We evaluate the two top-performing methods from the competition,~\cite{Chen19DLR} (DLR) 
% \MS{Don't they have a better name?}
and~\cite{Hu19a} (SegDriven-Z), on these new splits using the publicly-available code, and find their errors to be of similar magnitude to the ones reported online during the challenge.
Note that our method, as DLR and SegDriven-Z, uses the 3D model to define the keypoints whose image location we predict. We therefore exploit a method of~\cite{Hartley00} to first reconstruct the satellite from the dataset. 

Table~\ref{tab:speed_stoa} compares our results to those of the two top-performing methods on this dataset. Note that DLR combines the results of 6 pose estimation networks, followed by an additional pose refinement strategy to improve accuracy. We therefore also report the results of our method with and without this pose refinement strategy. Note, however, that we still use a single pose estimation network. Furthermore, for our method, we report the results of two separate networks trained at different input resolutions. 
At the resolution of 960$\times$, we outperform the two state-of-the-art methods, while our architecture is much smaller and much faster. To further speed up our approach, we train a network at a third (640$\times$) of the raw image resolution. This network remains on par with DLR but runs 20+ times faster.
% \MS{What resolution does Chen use? If they use 960, I would tend to turn this the other way around: Say that, at the same resolution, we outperform the two state-of-the-art methods, while our architecture is much smaller and much faster. To further speed up our approach, we train a network at a third of the raw image resolution. This network remains on par with Chen but runs 50 times faster.}
% \YH{As they use two networks, we can not compare the resolution directly. In more detail. they train a 768x768 detector first and resize all the detected bounding box to 768x768 again to feed into the next pose network. And they use the second pose network 6 times for results ensemble.}
% The faster version can already on par with the top performer but runs 10+ times faster. Our slower version performs the best and still runs 5+ times faster than the competitors.
% \WJ{Can you explain the rationale for these versions with different resolutions? Just speed? Wasn't clear from the text.}\YH{Yes, mainly for speed, also for GPU memory consumption, especially during training.}
% \WJ{You could more prominently point out speed as one of the benefits in the introduction. Practical usage in an autonomous satellite will require low-latency low-compute answers.}\YH{Fixed}

% ~\footnote{\href{https://github.com/BoChenYS/satellite-pose-estimation}{https://github.com/BoChenYS/satellite-pose-estimation}}$^{,}$\footnote{\href{https://github.com/cvlab-epfl/segmentation-driven-pose}{https://github.com/cvlab-epfl/segmentation-driven-pose}}

% \input{fig/swisscube_statistics.tex}

\input{table/speed_stoa.tex}
\subsection{Evaluation on the SwissCube Dataset}
To facilitate the evaluation of 6D object pose estimation methods in the wide-depth-range scenario, we
introduce a novel SwissCube dataset. The renderings in this dataset account for the
precise 3D shape of the satellite and include realistic models of the star backdrop, Sun, Earth,
and target satellite, including the effects of global illumination, mainly
glossy reflection of the Sun and Earth from the satellite's surface.
To create the 3D model of the SwissCube, we modeled every mechanical part from
raw CAD files, including solar panels, antennas, and screws, and we
carefully assigned material parameters to each part.

The renderings feature a space environment based on the relative placement and
sizes of the Earth and Sun. Correct modeling of the Earth is most important, as
it is often directly observed in the images and significantly affects the
appearance of the satellite via inter-reflection. We extract a high-resolution
spectral texture of the Earth's surface and atmosphere from published data
products acquired by the NASA Visible Infrared Imaging Radiometer Suite (VIIRS)
instrument. These images account for typical cloud coverage and provide
accurate spectral color information on 6 wavelength bands. Illumination from
the Sun is also modeled spectrally using the extraterrestrial solar irradiance
spectrum. The spectral simulation performed using the open source Mitsuba 2
renderer~\cite{Nimier19} finally produces an RGB output that
can be ingested by standard computer vision tools.

\input{fig/render_setting.tex}

The renderings also include a backdrop of galaxies, nebulae, and star clusters
based on the HYG database star catalog~\cite{hygdatabase} containing around
120K astronomical objects along with information about position and
brightness. The irradiance due to astronomical objects is orders of magnitude
below that of the Sun. To increase the diversity of the dataset, and to ensure
that the network ultimately learns to ignore such details, we boost the
brightness of astronomical objects in renderings to make them more apparent.

Following these steps, we place the SwissCube into its actual orbit located
approximately 700 km above the Earth's surface along with a virtual observer
positioned in a slightly elevated orbit. We render sequences with different
relative velocities, distances and angles. To this end, we use a wide field-of-view (100$^{\circ}$) camera whose distance to the target ranges uniformly between $1d$ to $10d$, where $d$ indicates the diameter of the SwissCube without taking the antennas into accounts.
% \MS{Do you use a wide field-of-view camera? With what angle? Does the diameter $d$ include the antennas, or is it just the cube edge length?}
% \YH{Yes, we use the virtual camera with a FOV of 100. the diameter is computed from only the cube body and does not take the antennas into accounts. And, we treat the Swisscube as an asymmetrical object.}
The high-level
setup is illustrated in Fig.~\ref{fig:render_setting}. Note that the renderings
are essentially black when the SwissCube passes into the earth's shadow, and we
detect and remove such configurations.

We generate 500 scenes each consisting of a 100-frame sequence, for a total of
50K images. We take 40K images from 400 scenes for training and the 10K
image from the remaining 100 scenes for testing. 
%We make the depth range of the %CubeSat dataset approximately uniformly distributed from 1d to 10d, as
We render the images at a 1024$\times$1024
resolution, a few of which are shown in Fig.~\ref{fig:results_demo}. During network processing, we resize the
input to 512$\times$512. 
%Although higher input resolution often means higher
%accuracy, as shown by the SPEED experiments, we will focus on this resolution
%setting for a detailed ablation study in this experiment. 
We report the ADI-0.1d accuracy at three
depth ranges, which we refer to as {\it near}, {\it medium}, and {\it far}, corresponding to the depth ranges [1d-4d],
[4d-7d], and [7d-10d], respectively.

\input{fig/results_demo.tex}

\subsubsection{Effect of our Ensemble-Aware Sampling}
We first evaluate the effectiveness of our ensemble-aware sampling strategy, further comparing our approach with the single-scale baseline SegDriven~\cite{Hu19a}, which uses the same backbone as us. Note that the original SegDriven method did not rely on a detector to zoom in on the object, but was extended with a YOLOv3~\cite{Redmon18} one in the SPEED competition, resulting in the SegDriven-Z approach evaluated above. For our comparison on the SwissCube dataset to be fair, we therefore also report the results of SegDriven-Z.
% \MS{Could we also evaluate Chen on this dataset? This would be more convincing, although probably too late.} \YH{We had the result, I will add it back.}
Moreover, we also evaluate the top performer on the SPEED dataset, DLR~\cite{Chen19DLR}, on our dataset.

Fig.~\ref{fig:param_study} demonstrates the effectiveness of our sampling strategy.
Our results with different $\lambda$ values, which controls the ensemble-aware sampling, show that large values, such as $\lambda>10$, yield lower accuracies. With such large values, our sampling strategy degenerates to the one commonly-used in FPN-based object detectors. This therefore evidences the importance of encouraging every pyramid level to produce valid estimates at more than a single object scale. 
%adopts is much inferior to other settings. That big $\lambda$ makes every pyramid level working on unoverlapped training instances, making different pyramid levels uncombinable during inference for a specific instance. On the other hand, the case of 
Note also that $\lambda=0$, which corresponds to distributing every training instance uniformly to all levels, does not yield the best results, suggesting that forcing every level to produce high-accuracy at all the scales is sub-optimal. In other words, each level should perform well in a reasonable scale range, but these ranges should overlap across the pyramid levels. 
%The imposing of large variation difficulties to every pyramid level makes their performance deteriorate, leading to a worse fusion accuracy. 
This is achieved approximately with $\lambda=1$, which we will use in the following experiments.

Table~\ref{tab:parameters_study} summarizes the comparison results with other baselines. Because it does not explicitly handle scale, SegDriven performs poorly on far objects. This is improved by the detector used in SegDiven-Z. However, the performance of this two-stage approach remains much worse than that of our framework.
Our method outperforms DLR as well, even though our method is 20+ times faster than DLR.
% , independently of the hyper-parameter value $\lambda$, controlling the ensemble-aware sampling. 
Fig.~\ref{fig:results_demo} depicts a few rendered images and corresponding poses estimated with our approach. 
% \MS{I would tend to show this at the end of the first subsection, and potentially compare with SegDriven-Z.}

\input{table/parameters_study.tex}
\input{fig/parameters_study.tex}
\input{table/fusion_effect.tex}

\subsubsection{Effect of our Multi-Scale Fusion}

To better understand the role of each pyramid level during multi-scale fusion, we study the accuracy obtained using the predictions of each individual pyramid level.
Intuitively, we expect the levels with a larger receptive field (feature maps with low spatial resolution) to perform well for close objects, and those with a small receptive field (feature maps with high spatial resolution) to produce better results far-away ones. While the results in Table~\ref{tab:fusion_effect} confirm this intuition for Levels L1, L2 and L3, we observe that the performance degrades at L4 and L5. We believe this to be due to the very low spatial resolution of the corresponding feature maps, 8$\times$8, and 4$\times$4, respectively, making it difficult for these levels to output precise poses. Nevertheless, the accuracy after multi-scale fusion outperforms every individual level, and we leave the study of a different number of pyramid levels to future work.
% \MS{This suggests that we should probably just stop at L3...}\YH{Although the performance of L4 and L5 alone is bad, we are not sure if the L4 or L5 can contribute to the final loss via ensemble. We need more experiments to verify it, so leave it as it is right now.}

%performance with the results combined only from feature cells within each level's segmentation mask. Table~\ref{tab:fusion_effect} shows the results. In intuition, levels with larger reception fields perform better for closer objects and vice versa. However, we find that this is not always true. The performance of level 4 on near objects can not match the one on level 3, and level 5 becomes even more worse. Note that, the spatial feature dimensions for L1, L2, L3, L4, and L5 are 64$\times$64, 32$\times$32, 16$\times$16, 8$\times$8, and 4$\times$4, respectively. Although L4, especially L5, has larger reception fields, the lower spatial resolution makes them less discriminable against 2D keypoints and introduces more visual noises for each cell. Nevertheless, the accuracy after multi-scale fusion outperforms every single level and we leave the study of a different number of pyramid levels to future work.

\subsubsection{Effect of the 3D Loss}

\input{table/error_3d_vs_2d.tex}
\input{fig/error_vs_positions.tex}

%The popular 2D reprojection loss has server problems in the wide-depth-range scenarios as discussed in Fig.~\ref{fig:cube_problem}. To fairly compare the proposed 3D loss against the 2D loss, we train our framework two times from the same initial states and with the same other settings except for the adopted regression loss. 
In Table~\ref{tab:error_3d_vs_2d}, we compare the results obtained by training our approach with either the commonly-used 2D reprojection loss or our loss function in 3D space. Note that our 3D loss outperforms the 2D one in all depth ranges, and the farther the object, the larger the gap between the results of the two loss functions.
In Fig.~\ref{fig:error_vs_positions}, we plot the average accuracy as a function of the object image location. The performance of the 2D loss degrades significantly when the object is located near the image center, whereas the accuracy of our 3D loss remains stable for most object positions. Note that, The reason both of them become worse in the right part of the figure is due to the object truncation by image borders.

\subsection{Results on Real Images}

In Fig.~\ref{fig:domain_adaptation}, we illustrate the performance of our approach on real images. Note that these real images were not captured in space but in a lab environment using a mock-up model of the target and an OptiTrack motion capture system to obtain ground-truth pose information for a few images. We then fine-tuned our model pre-trained on our synthetic SwissCube dataset using only 20 real images with pose annotations. Because this procedure only requires small amounts of annotated real data, it would be applicable in an actual mission, where images can be sent to the ground, annotated manually, and the updated network parameters uploaded back to space.
%Although our CubeSat dataset is rendered by a computer, thanks to its high realism, it can be easily adapted to real data. For the real data, we obtain it by capturing a real-size mock-up of the target. We use a simple finetune~\cite{1}, which is a very basic domain adaptation technique, to adapt our model to the read data. shows some real results on two different satellites, CubeSat and VESPA as well. Although the real data is not captured from the ``real'' space and we are sure we can find better domain adaptation methods, it shines a bright light for the preparation of the real launching in the future.

% \input{table/swisscube_stoa.tex}
\input{fig/domain_adaptation.tex}
\input{table/occ_linemod_stoa.tex}

\subsection{Evaluation on Occluded-LINEMOD}

Finally, to demonstrate that our approach is general, and thus applies to datasets depicting small depth variations, we evaluate it on the standard Occluded-LINEMOD dataset~\cite{Krull15}. Following~\cite{Hu20a}, we use the raw images at resolution 640$\times$480 as input to our network, train our model on the LINEMOD~\cite{Hinterstoisser12b} dataset and test it on Occluded-LINEMOD without overlapped data. Although our framework supports multi-object training, for the evaluation to be fair, we train one model for each object type and compare it with methods not relying on another refinement procedure.
Considering the small depth variations in this dataset, we remove the two pyramid levels with the largest reception fields from our framework, leaving only ${\cal F}_1$, ${\cal F}_2$ and ${\cal F}_3$. As shown in Table~\ref{tab:occ_linemod_stoa}, our model outperforms the state of the art even in this general 6D object pose estimation scenario.

%shows the comparison results of our framework against the state-of-the-art methods. It shows that our multi-scale fusion framework also works pretty well in general 6D object pose estimation.

