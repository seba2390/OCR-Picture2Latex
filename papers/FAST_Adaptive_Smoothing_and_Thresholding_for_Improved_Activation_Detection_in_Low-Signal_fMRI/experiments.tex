\section{Performance Evaluations} \label{sec:simulation}
We studied performance of FAST relative to the most popular and
relevant alternatives. Our 
evaluations were on real and simulated datasets and compared FAST with
cluster thresholding (CT) applied with $\alpha = 0.001$ per
\citet{wooetal14}, a second-order
neighborhood and number of voxels in cluster determined by ~\citet{cox96}'s {\tt
  3dClustSim} function, 
threshold-free cluster enhancement (TFCE) \citep{smithandnichols09},
permutation-based 
testing~(PBT)~\citep{winkleretal14}, AS and PS (applied as AWS or
adaptive-weighted smoothing \citep{polzehl2006propagation}). We used
$\alpha=0.01$ and $\alpha=0.05$ in FAST to obtain insight 
into the role of $\alpha$. We
used R packages {\sc RFASTfMRI} for FAST, {\sc fMRI} for AS and AWS,
{\sc AnalyzeFMRI} for CT and  {\sc  permuco} for TFCE and PBT.

%We use a 2D setup to make it computationally possible to evaluate performance across many settings. A reviewer asked us to perform FAST on a resting state dataset, as well compare performance of AM-FAST and AR-FAST under the same experimental set up as the adaptive segmentation approach.  The associated editor asked to compare our newly methodology with other real fMRI data. For this case we used the fMRI finger tapping experiment from \citep{roweandlogan05}.

\subsection{Finger-Tapping Experiments}
Our first set of evaluations used the 12 replicated SPMs 
\citet{maitraetal02,maitra09b} 
from the right-hand (RH) and left-hand (LH) finger tapping
study  of a RH-dominant male.
%Although 12 replications are available for both sets of experiments,
%we do not use the eighth replication for the LH experiments because
%of concerns about its quality \citet{maitra10}.
For each method, Figure~\ref{fig:jaccard.index.finger} displays
\begin{figure}[h]
  \centering
\includegraphics[width=\columnwidth]{figs/Jaccard-Index-Finger-Tapping}
\caption{The summarized Jaccard index ($\ddot\omega$) of activations obtained by
  each method over the replications for the RH and LH
    experiments. In this and other figures, AL$_a$, AR$_a$ and
    AM$_a$ denote ALL-, AM- and AR-FAST methods with $\alpha = a$, while
    TFC is used to abbreviate TFCE.}
  \label{fig:jaccard.index.finger}
\end{figure}
the summarized Jaccard similarity ($\ddot\omega$) between the
activation maps from the 12 replications. % and have values of up to
                                % 0.51 and 0.38 
%for AR-FAST with $\alpha = 0.01$ (RH) and 0.38 (LH). %for the left).
%the summarized multiple Jaccard coefficient ($\ddot\omega$) of
%\%citep{maitra10} to index the similarity across the activation maps
%obtained by each method across replications.
%(Figure~\ref{JI-overlap}  displays the pairwise Jaccard indices
%($\omega$) between the FAST-obtained activation maps, with values
%going up to 0.51 for AR-FAST with $\alpha = 0.01$ for the right hand
%and 0.38 for the left).
For the RH experiments, AR-FAST showed the highest reliability of
detected activation with $\alpha=0.05$. AR-FAST at $\alpha=0.01$ 
and TFCE were marginally behind and AS and AWS also doing reasonably. TFCE
was a bit better than AR-FAST for the LH experiments. 
%Notwithstanding FAST's  relatively superior performance,
The generally low $\ddot\omega$ for all methods points to potential issues in data  quality and processing
%(for instance,  ignoring correlated  errors in fitting ~\ref{eq:lm}
%to obtain the SPM).
\citep{maitra10}.

\begin{comment}

Finger tapping experiment for the right hand consist in a$128\times 128 \times 22 \times 12$ with 12 replicates.  To compare consistency on the overlap measure, we reported the generalized overlap of the firsts sixth replicates, similar as \citep{maitra10}. Figure \ref{fig:Overlapmatrix} we display the overlap matrix for each hand using both AR-FAST and AM-FAST. The generalized overlap for the right hand $\ddot{\omega}_R = 0.1906$ and for the left hand $\ddot{\omega}_L = 0.2597$ Supplemental material contained the results for all 12 replicates for the right hand and results for 10 replicates of the left hand. Due some bogus in the left hand recopilation, replicates 8 and 11 were removed.

\begin{figure*}[h]
  \centering
  \subfloat[]{\includegraphics[width=0.33\textwidth]{figures/RightHand-OverlapMatrix-Robust-6}}
  \subfloat[]{\includegraphics[width=0.33\textwidth]{figures/RightHand-OverlapMatrix-Likelihood-6}}
  \\
\subfloat[]{\includegraphics[width=0.33\textwidth]{figures/LeftHand-OverlapMatrix-Robust-6}}
\subfloat[]{\includegraphics[width=0.33\textwidth]{figures/LeftHand-OverlapMatrix-Likelihood-6}}
\caption{Overlap map for left and right hand finger tapping. The first two are are right hand robust and likelihood smoothing.}
\label{fig:Overlapmatrix}
\end{figure*}
\end{comment}
\subsection{Experiments on Simulated Phantom Data}
Our next set of examples evaluated performance on phantom data
simulated using \eqref{eq:lm} under different conditions. 
\subsubsection{Motif and Stripes}
\label{tabelow}
%A reviewer's comment led us to study performance on simulated phantom
%data similar to those used in
\begin{figure}[h]
  \mbox{
    \subfloat[Motif]{
      \includegraphics[width=0.245\columnwidth]{figs/True-Activation-Motif-128_x_128}\label{motif}}
    \subfloat[$16\!\!\times\!\!16$ stripes]{
    \includegraphics[width=0.245\columnwidth]{figs/True-Activation-L-R-16_x_16}\label{str16}}
    \subfloat[$32\!\!\times\!\!32$ stripes]{
    \includegraphics[width=0.245\columnwidth]{figs/True-Activation-L-R-32_x_32}\label{str32}}
    \subfloat[$64\!\!\times\!\! 64$ stripes]{
    \includegraphics[width=0.245\columnwidth]{figs/True-Activation-L-R-64_x_64}\label{str64}}
}
\mbox{
\subfloat[Performance using $\mJ$ for the two-sided phantom experiments]{\includegraphics[width=\columnwidth]{figs/Jaccard-Index-Tabelow-Motif-Art-Stripes}\label{JI-Tabelow}}}
\caption{(a)-(d) The phantoms from \citep{polzehletal10} and (e)
  performance of the  methods  under  different
  settings.}
\label{fig:tabelow}
\end{figure}
We first study performance using the simulation setup of 
\citet{polzehletal10}. We thank K. Tabelow for readily sharing code
that created  the motif and three striped ($16\times 16$, $32\times 32$,
$64\times 64$) phantoms of
Figures~\ref{fig:tabelow}\subref{motif}-\subref{str64}.
The phantoms have 14, 50, 28 and 47\% truly activated voxels, or
more than the 1-3\% expected in typical fMRI experiments. 
We used $\beta$s as per \citet{polzehletal10} and  CNRs
of between  0.75 to 2.68 for the motif and  1 to 2 for
the stripes. These examples are of two-sided alternatives. All simulations  had AR(1) errors with
$\rho=0.3$. For AS and AWS, we adopted the maximum bandwidth sequence values 
($h_k^*=3.06$, 1, 2 and 3)  in \citet{polzehletal10} for the four
respective phantoms as 
%\citet{polzehletal10} used  
%bandwidth sequences with maximum at $h_k^*=3.06$, 1, 2 and 3 for the
%four phantoms and we adopt these values % It is generally unclear how
                                % to choose such specific values, but
                                % we use them
the best-case specific choices.
 Figure~\ref{fig:tabelow}\subref{JI-Tabelow} summarizes
 performance. There is no clear overall winner but AM-FAST,
 PBT and TFCE find no activation at
 all~(Figure~\ref{fig:tabelow-all}). ALL-FAST and AR-FAST, in that order,
 perform creditably  in some situations but not in others.
\subsubsection{Large-scale study with modified Hoffman phantom}
\label{hoff}
The phantoms in \citet{polzehletal10}, with uniform underlying
structure ($\beta_0$) and no drift, but varying CNR, are not particularly
representative of cerebral activation and do not provide much
insight into performance of different activation methods. So we
performed a large 
simulation study using a more realistic phantom and experimental setup
that matches \eqref{eq:lm}. 
We used a modified version of the digitized
$128\!\times\!128$ 2D Hoffman phantom~\citep{hoffmanetal90}  of
Positron Emission Tomography, with 
 3465 in-brain pixels, representing two types (say, A and B) of
anatomic structures -- the latter has 138 deemed truly activated  pixels in  two
distinct regions (Figure~\ref{fig:Hoff}).  
\begin{figure}[h]
    \centering
    \subfloat{
      \raisebox{-.5\height}{\includegraphics[width=0.12\textwidth]{figs/Hoff}}
    }
    \subfloat{
 \begin{tabular}{|c|ccc|}
   \hline
   Region & $\beta_{i0}$ & $\beta_{i1}$ & $\beta_{i2}$\\%& $\sigma_0$ \\
   \hline
   Background & 0  & 0 & 0 \\%& $(240,300,400)$ \\
   Inactivated, A & 4500 & 0 & -155.32\\% & $(240,300,400)$ \\
   Inactivated, B & 6000 & 0 & -155.32 \\%& $(240,300,400)$  \\
   Activated & 6000 & 600 & -155.32 \\%& $(240,300,400)$ \\
    \hline
  \end{tabular}
    }
    \caption{The modified Hoffman phantom. Putative anatomic  regions (A and B) are in
      shades of grey, with truly activated pixels  in red. The
      table lists the $\bbeta_i$s used in our simulations.}
\label{fig:Hoff}
\end{figure}
The $i$th pixel in the phantom had values 
$\bbeta_i\!=\!(\beta_{i0},\beta_{i1},\beta_{i2})$ in \eqref{eq:lm} as
per its location (see Figure~\ref{fig:Hoff}).
%The phantom provided the values (see Figure~\ref{fig:Hoff}) for $\bbeta_i\!=\!(\beta_{i0},\beta_{i1},\beta_{i2})$ in \eqref{eq:lm} as per the location of the $i$th pixel in the phantom.

As in~\eqref{eq:lm}, the design matrix $\bX$ had the intercept in the
first column. The second column had the hemodynamic response function
(HRF)~\citep{lindquist08} convolved with the input stimulus time
series that alternated as 16 on-off blocks of 6 time-points
each. %The ``on'' block   signaled rest, with input stimulus 0, while
      %the ``off'' block  was 1 to signify activation. %These blocks alternated, yielding values
% at 96 time-points.
The third column of $\bX$ represented linear drift and was set to $t$
($t\!=\!1,2,\ldots,96$). As per~\eqref{eq:lm}, AR($p$) Gaussian errors were
simulated for different $p$ and at each pixel. Specifically, for each $p$, we considered AR
coefficients for a range of $\bphi\!\equiv\!\bphi_i$s with  coefficients
$(\phi_1,\phi_2,\ldots,\phi_p)$ that were, with lag,  (a) all equal, (b)
decreasing, (c) increasing, (d) first decreasing, 
then increasing and (e) first increasing and then decreasing. We
restricted $\sum^p_{j=1}\phi_{j} = 0.9$ to ensure stationary
solutions. So $\phi_1\equiv0.9$ for all AR(1) cases. For $p\!=\!2,3,4$,
$\phi_i\equiv 0.9/p$ for the equal AR coefficients scenario and as per 
Table~\ref{tab:Phivalues} for the other cases.
\begin{table}[htbp]
\caption[PhiValuesSimulation]{$\bphi$s for the AR($p$)
  scenarios used in our simulations.}\label{tab:Phivalues}
\centering
\begin{tabular}{|c|c|c|}
\hline
$p$ & Decreasing  & Increasing \\  
\hline
%1  & 0.9 &  0.9 \\
2  & (0.6, 0.3) & (0.3, 0.6) \\
3  & (0.4, 0.3, 0.2) & (0.2, 0.3, 0.4) \\
4  & (0.3, 0.25, 0.20, 0.15)& (0.15, 0.20, 0.25, 0.3) \\
5  & (0.3, 0.25, 0.20, 0.10, 0.05) & (0.05, 0.10, 0.20, 0.25, 0.30) \\
\hline
\hline
$p$ & Decreasing-Increasing & Increasing-Decreasing  \\  
\hline
%1  & 0.9 &  0.9 \\
2  & (0.6,0.3) & (0.3,0.6)\\
3  & (0.4,0.1,0.4) & (0.1,0.7,0.1) \\
4  & (0.4,0.05,0.05,0.4)&(0.05,0.4,0.4,0.05) \\
5  & (0.25,0.15,0.1,0.15,0.25) & (0.1,0.15,0.4,0.15,0.1)\\
\hline
\end{tabular}
\end{table}
Finally,  $\sigma_0 = 1200, 800, 600$ to correspond to very low to
moderate CNR = 0.50, 0.75, and 1.0. By design, our SNRs were 10 times
our CNRs. Time series images were
simulated using~\eqref{eq:lm} and the setup of Fig.~\ref{fig:Hoff} and
Table \ref{tab:Phivalues}. 


For each pixel, we fit \eqref{eq:lm}  with $\hat p$
chosen from \{0,1,2,3,4,5\} using BIC. SPMs were generated as per
Section~\ref{method:prelim}. Figure~\ref{fig:sim.phant} provides
sample SPMs and activation maps with the three top-performing methods: AR-FAST, AM-FAST and AS, for the three CNR
settings with AR(2) errors and coefficients decreasing with order.
\begin{figure}[h]
  \begin{center}
\includegraphics[width=0.98\columnwidth]{figs/Hoff_3_x_4_act_SPM}
\caption{Left-to-right: sample SPMs and best performers (AM-F$_{0.05}$,
  AR-F$_{0.05}$ and AS) for experiments with CNR = 0.5 (top row), CNR
  = 0.75 (middle) and CNR = 1.0 (bottom). AR-F$_{0.01}$ and
  AM-F$_{0.01}$ out-performed AS but are not displayed.}
\label{fig:sim.phant}
  \end{center}
\end{figure}
\begin{comment}
\begin{figure}[h]
  \centering
\includegraphics[width=0.45\textwidth,height=0.25\textwidth]{figures/Hoff_3x6_act}
  \caption{SPM for a  sample simulation case of decreasing AR coefficients, for the sixth methods settings (from left to right) of AWS, AS, AM-FAST, AR-FAST, TFCE, permutation test. The three rows (from top to bottom) correspond   to simulation settings with CNR of 0.5, 0.75 and 1.0, respectively.}
  \label{fig:sim.phant}
\end{figure}
\end{comment}
\begin{comment}
was done with AM-FAST and AR-FAST and also the commonly-used (in fMRI) cluster-thresholding (CT) at ~\citet{wooetal14}'s suggested significance level ($\alpha\!=\!0.001$) with a second-order neighborhood and number of voxels determined by ~\citet{cox96}'s {\tt 3dClustSim} function. The R package  {\tt AnalyzeFMRI}~\citep{bordieretal11} was used to perform cluster-wise
thresholding. We also performed adaptive weight smoothing, (AWS) ~\citep{tabelowetal06}, structural adaptive segmentation, (AS)~\citep{polzehletal10}~ using the R package {\tt fmri}~\citep{tabelowandpolzehl11}. A reviewer asked us to compare our methodology with distribution-free approaches. We performed permutation test and thresholding-free cluster enhacement (TFCE) ~\citep{smithandnichols09}, these function are in the R package {\tt permuco}.  In each case, performance was evaluated in terms of the overlap Jaccard Index \citep{jaccard1901,maitra10} between the estimated activation map and the truth as per Figure~\ref{fig:Hoff}. The same reviewer asked us to add false-positive rate, we also reported the false-negative rate, true-positive rate and true negative rate for our proposed methodology as well competeting methods.
\end{comment}
\begin{figure*}
  \begin{center}
\includegraphics[width=\textwidth]{figs/Jaccard_Index_Decreasing_All_Cases}
\caption{Performance of the activation detection algorithms for different
  settings and when the AR coefficient decreases with order.
For clarity, each setting displays performance of the methods in the
same order as in the legend. 
}
\label{fig:Jaccard.index.decreasing}
  \end{center}
\end{figure*}
(See Figures~\ref{fig:ActMapDec1} for activation maps using all
methods and Figures~\ref{fig:ActMapDec1}--\ref{fig:ActMapInc2} for cases with
other $p$ and/or decreasing order.) All methods do poorly for CNR=0.5
in this example, but AS correctly finds a few activated voxels. AM-
and AR-FAST perform 
very well for CNR=0.75 and 1.0. Other methods -- in
particular ALL-FAST, CT, TFCE and PBT -- barely find  activation on
this example. 

To more fully understand performance, we replicated our experiment
25 times for each simulation setting. Figure~\ref{fig:ARp.estimated.order} shows performance in
estimating $p$, with over-estimation and mild under-estimation  
for large and small values of the true AR order. The pattern
broadly holds at all CNRs and $\phi$s. We now discuss performance of 
activation detection methods on SPMs obtained upon fitting AR($\hat p$). 
Figure~\ref{fig:Jaccard.index.decreasing} displays  overall
performance, in terms of $\omega$, of all methods in the
decreasing-$\phi$ cases. Performances with other types of $\phi$s
are similar
(Figure~\ref{fig:Jaccard.complete}). Figures~\ref{fig:ActivatedVoxels.complete}--\ref{fig:TrueNegative.complete} display the number of activated 
voxels and the false and true positive rates (FPRs and TPRs). % These
%results % follow our initial observations for Figure~\ref{fig:sim.phant}
% and
% show
FAST methods are among the top performers at all CNRs: AM- and
AR-FAST do very well for experiments with orders other than
AR(1). (All methods do poorly at CNR=0.50 in the AR(1) case: we 
surmise that is because of highly-correlated noise obtained with  $\phi=0.9$.)
AS and AWS are the next best  performers but CT, TFCE and PBT perform  
very poorly with  TPRs of below 25\%. FAST
methods have very high TPRs but FPRs of up to 
0.2, 0.3 and 0.5\% for ALL-, AM- and AR-FAST  for $\alpha = 0.05$, 
low CNR and $p$.
% Interestingly enough, AR-FAST with $\alpha=0.01$ has negligible
% false positive rates but also slightly higher false negative
% rates.
Overall, the slightly higher FPRs of the  FAST methods are overwhelmed
by their vastly higher TPRs, leading to their having the highest $\mJ$s.

Our threshold $\alpha$ has a role, with smaller values performing
better at higher CNRs and conversely. We suggest $\alpha\approx0.05$
for low-CNR tasks 
% such as finger-tapping
and $\alpha\approx0.01$ for high-CNR tasks. We suggest determining low or high CNR scenarios
accordingly as whether the upper percentile of the estimated
voxel-wise CNRs is less than the standard normal upper percentile
(2.33) or not: the upper percentile of the estimated CNRs 
is chosen to include an activated voxel (if such exists) in the
CNR determination. In our studies, ALL-FAST required more Step~\ref{step2}
iterations, but was computationally faster than AM-FAST AR-FAST and
had lower TPR, FPR and $\omega$, especially at low 
CNRs. Regardless, our methods were the fastest among all  
methods. %one may use AR-FAST for lower expected CNRs and AM-FAST for other cases.  
\begin{comment}



For low CNRs, both CT and TP11 perform poorly, but 
AM-FAST and AR-FAST perform quite well. As with
Figure~\ref{fig:sim.activ}, both CT and TP11 improve with
increased CNR: indeed CT is marginally the best performer for when
CNR=2.0 and with no autocorrelation ($p=0$). Interestingly, for
CNR=2.0, CT does worse than AM-FAST or AR-FAST for the decreasing
AR coefficients case but not necessarily for the other
cases. AM-FAST and AR-FAST perform very similarly at all settings,
however AM-FAST is computationally faster than AR-FAST. 
The poorer performance of CT relative to the FAST algorithms
is not surprising because {\tt 3dClustSim} and other such functions
are not very adaptive in their execution. TP11 is however, somewhat
more adaptive, through the choice of the sequence of smoothing
parameters that is left to the user. Specifying this sequence
appropriately may, however, require considerable skill, and the
default values provided in the {\tt fmri} package do not appear to be
adequate enough for lower CNR situations. On the other hand, our approach
determines the optimal smoothing from the SPM at each iteration through likelihood maximization (AM-FAST) or robust methods (AR-FAST). Interestingly though, we have found that $\alpha$ plays a
role in the FAST algorithms with smaller values performing marginally
better with higher CNRs and higher values performing substantially better for lower CNRs.
As a via media, we use $\alpha=0.025$ in our application.
\end{comment}
%In summary, the FAST algorithms perform very well over a wide range
%of low-to-modest CNRs. 


\begin{comment}
\subsection{Experimental Set-up: Phantom Signal}
These phantom datasets are from the experimental section of ~\citep{polzheletal10} and the example of the {\tt fmri} package. The art-stripe dataset consist in a numerical simulation of an artificial dataset. This phantom simulated a situation where the fMRI response depends on two stimuli, left/right (L/R), and analyze their contrast. The phantom consists of 10 slices with only the fifth slice containing activation. The designed stripes with different widths (1-4)at an in-plane matrix of $64\times 64$. This are divide in $16 \times 16$, $32 \times 32$ and $64 \times 64$. We thanks the authors for providing their code, we used the same experimental set-up their used in their work. Same as the modified Hoff experiment, we compared our methodology as well the competing algorithms for this Phantom Signal and art-strip data sets. 
\end{comment}
\subsection{Performance in Null Activation Scenarios}
\subsubsection{Resting-State Dataset}
\label{resting}
A reviewer's suggestion led us to apply our FAST algorithms on SPMs
obtained upon 
fitting \eqref{eq:lm} to a  resting state 
dataset~\citet{barberetal2011,nebeletal2014}, with no  activation
identified even at $\alpha=0.05$. This zero FPR (when CNR=0) as
opposed to the small FPR in low-CNR experiments may be due
to  Step~\ref{stop} of our algorithm correctly allowing more
Step~\ref{step2} iterations to
attenuate stray high-valued SPM voxels -- termination is earlier
in the low CNR cases %of Section~\ref{hoff} given the 
given the spatially located weaker-signal peaks. For higher CNRs,
Step~\ref{stop} again adaptively admits more smoothing iterations
that  dampen stray high values in the SPM without substantially
degrading the true high-signal peaks. %For higher CNRs, more
                                %Step~\ref{step2} iterations are again
                                %adaptively admitted because the
                                %high-signal peaks are not  as degraded by the increased smoothing that dampens stray high-valued SPM voxels.
%allow for more smoothing to be admitted before the stopping criterion
%is met, again allowing greater dampening out of  stray high-value
%voxels in the SPM.

\subsubsection{Null-simulated SPMs} Another reviewer was concerned
about multiple significance. Our use of $\alpha$ is to set a
threshold and not as a significance level: still, experiments on
simulated null SPMs~(Section~\ref{null}) indicate no practical
concerns.%false positives. 


\begin{comment}
\subsection{Experimental Setup: Activation during Finger tapping experiments}
Finger tapping experiment for the right hand consist in a $128\times 128 \times 22 \times 12$ with 12 replicates.  To compare consistency on the overlap measure, we reported the generalized overlap of the firsts sixth replicates, similar as \citep{maitra10}. Figure \ref{fig:Overlapmatrix} we display the overlap matrix for each hand using both AR-FAST and AM-FAST. The generalized overlap for the right hand $\ddot{\omega}_R = 0.1906$ and for the left hand $\ddot{\omega}_L = 0.2597$ Supplemental material contained the results for all 12 replicates for the right hand and results for 10 replicates of the left hand. Due some bogus in the left hand recopilation, replicates 8 and 11 were removed.


\section{Results}

\subsection{Results: Hoffman Phantom}


Figure~\ref{fig:sim.activ} provides activation maps detected using AM-FAST (with $\alpha = 0.025$), CT and TP11 for each case of Figure~\ref{fig:sim.phant}. AR-FAST results were essentially
indistinguishable from AM-FAST and so are not displayed. It is
encouraging to note that AM-FAST  visually tracks very closely
to the activation regions of Figure~\ref{fig:Hoff}. Both CT and TP11
also identify the true regions as activated but they also identify
several other pixels as activated, with more false positives detected
at lower CNR levels. 

\begin{figure*}[!h]
\begin{center}
\includegraphics[width=0.95\textwidth]{figures/Jaccard_IndexAR0123}
\caption[JaccardIndex]{Jaccard Index perfomance for each of the algorithms}
  \label{fig:Jaccard2}
\end{center}
\end{figure*}

\begin{figure*}[!h]
\begin{center}
\includegraphics[width=0.95\textwidth]{figures/ActivatedVoxelsAR0123}
\caption[ActivatedVoxels]{Activated number of voxels AM-FAST, AR-FAST, AS, AWS, CT, permutation and TFCE algorithms for the different simulation settings.}
  \label{fig:ActivatedVoxels}
\end{center}
\end{figure*}

\begin{figure*}[!h]
\begin{center}
\includegraphics[width=0.95\textwidth]{figures/FalsePositiveAR0123}
\caption[FalsePositive]{False Positive Rate of the AM-FAST, AR-FAST, AS, AWS, CT, permutation and TFCE algorithms for the different simulation settings.}
  \label{fig:FalsePositive}
\end{center}
\end{figure*}

\begin{figure*}[!h]
\begin{center}
\includegraphics[width=0.95\textwidth]{figures/FalseNegativeAR0123}
\caption[FalseNegative]{False Negative Rate of the AM-FAST, AR-FAST, AS, AWS, CT, permutation and TFCE algorithms for the different simulation settings.}
  \label{fig:FalseNegative}
\end{center}
\end{figure*}

\begin{figure*}[!h]
\begin{center}
\includegraphics[width=0.95\textwidth]{figures/TruePositiveAR0123}
\caption[TruePositive]{True Positive Rate of the AM-FAST, AR-FAST, AS, AWS, CT, permutation and TFCE algorithms for the different simulation settings.}
  \label{fig:TruePositive}
\end{center}
\end{figure*}

\begin{figure*}[!h]
\begin{center}
\includegraphics[width=0.95\textwidth]{figures/TrueNegativeAR0123}
\caption[TrueNegative]{TrueNegative Rate of the AM-FAST, AR-FAST, AS, AWS, CT, permutation and TFCE algorithms for the different simulation settings.}
  \label{fig:TrueNegative}
\end{center}
\end{figure*}


Figure~\ref{fig:Jaccard2} displays  performance of our algorithms and its competitors in the large-scale simulation study. For low CNRs, both CT and TP11 perform poorly, but
AM-FAST and AR-FAST perform quite well. As with
Figure~\ref{fig:sim.activ}, both CT and TP11 improve with
increased CNR: indeed CT is marginally the best performer for when
CNR=2.0 and with no autocorrelation ($p=0$). Interestingly, for
CNR=2.0, CT does worse than AM-FAST or AR-FAST for the decreasing
AR coefficients case but not necessarily for the other
cases. AM-FAST and AR-FAST perform very similarly at all settings,
however AM-FAST is computationally faster than AR-FAST. 
The poorer performance of CT relative to the FAST algorithms
is not surprising because {\tt 3dClustSim} and other such functions
are not very adaptive in their execution. TP11 is however, somewhat
more adaptive, through the choice of the sequence of smoothing
parameters that is left to the user. Specifying this sequence
appropriately may, however, require considerable skill, and the
default values provided in the {\tt fmri} package do not appear to be
adequate enough for lower CNR situations. On the other hand, our approach
determines the optimal smoothing from the SPM at each iteration through likelihood maximization (AM-FAST) or robust methods (AR-FAST). Interestingly though, we have found that $\alpha$ plays a
role in the FAST algorithms with smaller values performing marginally
better with higher CNRs and higher values performing substantially better for lower CNRs.
As a via media, we use $\alpha=0.025$ in our application. In summary, however, both FAST algorithms perform very well and over a wide range of low-to-moderate CNR settings.


%%% NEW SECTION results experiment suggested by reviewers.

\subsection{Results: Phantom Signal}

A reviewer asked us how our methodology compare against the adaptive segmentation approach in they same experiments. We thanked the authors for sharing the code of their simulation experiments. Using their same set-up value we apply FAST.
\begin{table*}[ht]
\centering
\begin{tabular}{rrrrrrrrr}
  \hline
 & AM-FAST & AR-FAST & AS & AWS & CT & Non Adaptive Smoothing & Permutation test & TFCE \\ 
  \hline
  L-R-$16\times 16$ & 0.1453 & 0.0710 & 0.1294 & 0.0000 & 0.1302 & 0.0000 & 0.0000 & 0.0000 \\
  R-L-$16\times 16$ & 0.1803 & 0.0769 & 0.1294 & 0.0000 & 0.1302 & 0.0000 & 0.0000 & 0.0000 \\ 
  L-R-$32\times 32$ & 0.1206 & 0.0019 & 0.3520 & 0.0000 & 0.1572 & 0.0265 & 0.0000 & 0.0000 \\
  R-L-$32\times 32$ & 0.1406 & 0.0038 & 0.3520 & 0.0000 & 0.1572 & 0.0246 & 0.0000 & 0.0000 \\ 
  L-R-$64\times 64$ & 0.1370 & 0.0000 & 0.1112 & 0.0016 & 0.0099 & 0.0151 & 0.0000 & 0.0000 \\ 
  R-L-$64\times 64$ & 0.1165 & 0.0000 & 0.1112 & 0.0016 & 0.0099 & 0.0109 & 0.0000 & 0.0000 \\ 
  Phantom Signal & 0.2749 & 0.1408 & 0.1845 & 0.0443 & 0.1691 & 0.0583 & 0.1722 & 0.0417 \\ 
   \hline
\end{tabular}
\caption{Jaccard Index for Art-stripes and Phantom Signal} 
\label{tab:phantom}
\end{table*}


Similarly for the left hand is the size although replicate 8 and 11 were removed from the analysis due some irregularities. For the left hand, this reduced the analysis to only 10 replicates. In Table \ref{tab:genOverlap}, generalized overlap of \citep{maitra10} is reported.

%Accelerated bootstrap confidence interval is also reported for each method in both hands. The confidence intervals reported in Table \ref{tab:genOverlap} are adjusted. For each case, the corresponding level are for robust and left hand tapping the adjusted confidence interval levels are 1.82927\% and 96.749\%. Left hand and likelihood adjusted level are 1.905558\% and 96.84233\%. For the right hand, the adjusted confidence level for the robust method are 2.226937\% and  97.21893\% and for likelihood are 1.817158\% 96.74073\%.

\end{comment}