\section{Theory and Methods} \label{sec:methodology}
\subsection{Preliminaries}\label{method:prelim}
Let $\bY_i$ be the time series vector of the observed BOLD response at
the $i$th voxel obtained after preprocessing for registration and
other corrections. It is  common to relate 
$\bY_i$ to the expected BOLD response via the general linear model
\begin{equation} 
  {\bY}_{i} = \bX\bbeta_i + \bepsilon_i,
  \label{eq:lm}
\end{equation}
where $\bepsilon_i$ is a $p$th-order auto-regressive (AR) Gaussian error
vector with AR coefficients $\bphi_i 
\!\!=\!\!(\phi_{i1},\phi_{i2},\ldots,\phi_{ip})$ and marginal variance
$\sigma_i^2$. Without loss of generality (w.l.o.g.), assume that the 
design matrix $\bX$ has the intercept in the first column, the 
expected BOLD response for the $k$ stimulus levels in the next $k$
columns, and polynomial terms for the drift parameter in the remaining
$m$ columns. Therefore, $\bbeta$ is a coefficient vector of length
$d\!=\!k\!+\!m\!+\!1$. We assume that the image volume has 
$n$ voxels, so $i\!=\!1,2,\ldots,n$ in \eqref{eq:lm}.
The parameters $(\hat\bbeta_i,\hat\sigma_i^2,\hat\bphi_i)$s are
usually estimated via generalized least squares 
or restricted maximum likelihood. A typical analysis
approach then applies (voxel-wise) hypothesis tests with 
the null hypothesis specifying no activation owing to the stimulus or
task. SPMs of the form $\bGamma =
\{c'\hat{\boldsymbol{\beta}}_i\}_{i \in V}$ with
appropriate contrasts 
$c'{\boldsymbol{\beta}}_i$ are then formulated at each voxel.

Many researchers use models that assume independent or AR(1) errors,
while others pre-whiten the time series before fitting
\eqref{eq:lm} under independence. Misspecified models can yield less  
precise SPMs~\citep{monti11,luoandnichols03,lohetal08,lindquistetal09}
so here we assume AR($p$) errors, with $p$ assessed by the
Bayes Information 
Criterion (BIC)~\citep{schwarz78,shumwayandstoffer06} that trades  a
fitted model's  complexity against its fidelity to the data.  
Tests on the SPM $\bGamma$ identify voxels that are activated with the
application of the stimulus. %Specifically, a voxel is declared as
                             %activated  if a suitable test rejects
                             %the hypothesis of no
                             %activation. %$H_0\!:\!c'{\boldsymbol{\beta}}_i\!=\!0$.   
\begin{comment}
As mentioned in Section~\ref{sintro}, issues of multiple testing arise
in activation detection, and they are addressed by approaches~\citep{benjaminietal06,helleretal06,benjaminiandheller07,smithandfahrmeir07,smithandnichols09,
  wooetal14} that account little for spatial
resolution \citet{tabelowetal06}. 
\begin{comment}
~\citep{resnick2013,sibuya1960} computed the limiting distribution of a maximum random variable when this are i.i.d. Let $X_{(n)} = \max_{1\leq i \leq n} \{X_i\}$ that if there exist sequence $b_n \in \mathbb{R}$ and $a_n \in \mathbb{R}^+$, such that
\begin{equation}
  P\left(\frac{X_{(n)}-b_n}{a_n} \leq x\right) = P(X_{(n)} \leq a_n x + b_n) = [F_X(a_n  x + b_n)]^n,
\end{equation}
where $F^n_X$ is the cumulative distribution function of the random variable $X$ to the $n$-th power. In fact, if $X$ is normally distributed, then the limiting distribution of the maximum $X_{(n)}$ is a Gumbel random variable;
\begin{equation}
  \begin{split}
   [F_1(a_n x + b_n)]^n = G_{3,0}(x),\\
  \end{split}
\end{equation}
Let $X_1,\ldots X_n$ be independent identically distributed (IID) random
variables from the standard normal density truncated at 1.

random variables t with mean 0 and variance 1. Let these observations
$X_i$ be truncated normal random variable with $x < \eta$ for some
$\eta \in \mathbb{R}$. Our first goal is to obtain the distribution
funcion of $X_{(n)} = \max_{1\leq i \leq n} \{ X_i \}$, for that using
basic probability definition, 
\begin{equation}
  F_{X_{(n)}}(x) = P(X_{(n)} \leq x) = [P(X_1 \leq x)]^n = [F_{X_1}(x)]^n
\end{equation}
The cumulative distribution function of a truncated normal with support $(\infty,\eta)$,
\begin{equation}
F_{X_{(n)}}(x) = \left(\frac{\Phi(x)}{\Phi(\eta)}\right)^n I(x \in (-\infty,\eta))
\end{equation}
There exists a relationship between the limiting distribution of the maximum of random variable a the extreme value theory. To achieve the idea is to find constants $a_n$, $b_n$ such that $F_{X_{(n)}}(a_n x + b_ n) \rightarrow G(x)$ where $G(\cdot)$ is a nondegenerate distribution of the extreme value family.
\begin{equation}
  \begin{split}
F_{X_{(n)}}(a_n x+b_n) &= \left(\frac{\Phi(a_n x+ b_n)}{\Phi(\eta)}\right)^n \\
& = \left(\frac{1-(1-\Phi(a_n x+ b_n))}{\Phi(\eta)}\right)^n \\
&= \left( \frac{1}{\Phi(\eta)} - \left(  \frac{1-\Phi(a_n x +b_n)}{\Phi(\eta)} \right) \right)^n \\
  \end{split}
\end{equation}
For normalizing values $a_n$ and $b_n$, this result arise from the gumbel derivations,  $\Phi(a_n x + b_n) = 1 - \frac{1}{n} \exp \{ -x + o((\log n)^{-1/2})\}$,
\begin{equation}
  \begin{split}
    &= \left( \frac{1}{\Phi(\eta)} - \left( \frac{1}{\Phi(\eta)} - \frac{[\Phi(\eta)]^{-1}}{n} \exp \{ -x + o((\log n)^{-1/2})\} \right) \right)^n\\
    & = \left( \frac{1}{\Phi(\eta)} - 1 \right) + \left( 1 - \frac{1}{n} \exp \{ -x -\log(\Phi(\eta)) + o((\log n)^{-1/2})\} \right)^n
  \end{split}
\end{equation}
For a fixed $\eta > 0 $, the first term $\frac{1}{\Phi(\eta)} - 1\rightarrow 0$.
\begin{equation}
  \begin{split}
  \lim_{n\rightarrow \infty}  & \left( 1 - \frac{1}{n} \exp \{ -x -\log(\Phi(\eta)) + o((\log n)^{-1/2})\} \right)^n\\
  &= \exp\{ -e^{-x - \log(\Phi(\eta))}\}
  \end{split}
\end{equation}
which is a gumbell distribution with location $\log(1/\Phi(\eta))$,
\end{comment}
\begin{comment}
~\citep{resnick2013,sibuya1960} computed the limiting distribution of a maximum random variable when this are i.i.d. Let $X_{(n)} = \max_{1\leq i \leq n} \{X_i\}$ that if there exist sequence $b_n \in \mathbb{R}$ and $a_n \in \mathbb{R}^+$, such that
\begin{equation}
  P\left(\frac{X_{(n)}-b_n}{a_n} \leq x\right) = P(X_{(n)} \leq a_n x + b_n) = [F_X(a_n  x + b_n)]^n,
\end{equation}
where $F^n_X$ is the cumulative distribution function of the random variable $X$ to the $n$-th power. In fact, if $X$ is normally distributed, then the limiting distribution of the maximum $X_{(n)}$ is a gumbel random variable;
\begin{equation}
  \begin{split}
   [F_1(a_n x + b_n)]^n\rightarrow G_{3,0}(x),\\
  \end{split}
\end{equation}
Now consider the case when this random variables are correlated, in here we focus in the case that ${\bf X} = (X_1,\ldots,X_n)'$ is a vector of normal random variables with mean $\boldsymbol{0}$ and covariance-variance matrix ${\bf R}$. The above result does not apply direclty because $X_i$ are not independent. Let $X_{(n)} = \max_{1\leq i \leq n} \{ X_i \}$ be the maximun of ${\bf X} = (X_1,\ldots,X_n)' \sim N_n(\boldsymbol{0},{\bf R})$, where ${\bf R}$ follows are special structure, it is circulant matrix. To obtain a limiting distribution of the maximum for a correlated normal random variable we will start with the normalizing sequence $a_n$ and $b_n$,
\begin{equation}
  \begin{split}
    P\left(\frac{X_{(n)}-b_n}{a_n} \leq x\right) &= P(X_{(n)}\leq a_n x + b_n) \\
    %    &= P(X_1 \leq a_n x + b_n, X_2 \leq a_n x + b_n,\ldots, X_n \leq a_n x + b_n) \\
    &= P({\bf X} \leq (a_n x + b_n) \boldsymbol{1}_{n\times1}); \\
    &= P({\bf R}^{-1/2}{\bf X} \leq (a_n x + b_n) {\bf R}^{-1/2} \boldsymbol{1}_{n\times1}) \\
  \end{split}
  \end{equation}
Let ${\bf Y} = {\bf R}^{-1/2} {\bf X}$  and  ${\bf R}^{-1/2} \boldsymbol{1}_{n\times1} = \delta \boldsymbol{1}_{n\times1}$;
\begin{equation}
  \begin{split}
   P({\bf Y} \leq (a_n x + b_n ) \delta \boldsymbol{1}_{n\times1}) &  \\
    &=P({\bf Y} \leq (a^*_n x + b^*_n \boldsymbol{1}); \mbox{ where } a^*_n = \delta a_n; b^*_n = \delta b_n;\\
    \mbox{ note that } {\bf Y} \sim N_n(\boldsymbol{0}, {\bf I}) \\
    =  P(Y_1 \leq a^*_n x + b^*_n, Y_2 \leq a^*_n x + b^*_n,\ldots, Y_n \leq a^*_n x + b^*_n) = [P(Y_1 \leq a^*_n x + b^*_n )]^n \\
    = [F_Y(a^*_n x + b^*_n)]^n.
  \end{split}
\end{equation}
where $F_Y(\cdot)$ is the cumulative distribution of a normal random variable with mean 0 and variance 1. The limiting distribution of $[F_y(\cdot)]^n$ is Gumbel distribution, the normalizing sequence, in this case $a^*_n$ and $b^*_n$ can be chose based on ~\citep{nadaraya2003}
\begin{equation}
  \begin{split}
    a^*_n = \frac{1}{n\phi(\Phi^{-1}(1-1/n))}\\
    b^*_n = \Phi^{-1}(1-1/n)
    \end{split}
\end{equation}
where $\phi(\cdot)$ and $\Phi(\cdot)$ are the density function and distribution function of a normal distribution. Using the above sequence we can obtain, the normalizing sequences valuee for the maximum random variable $X_{(n)}$,
\begin{equation}
  \begin{split}
    a^*_n = \delta a_n \Rightarrow  a_n = a^*_n/\delta = \frac{1}{\delta n \phi(\Phi^{-1}(1-1/n))} \\
    b^*_n = \delta b_n \Rightarrow b_n = b^*_n/\delta = \frac{\Phi^{-1}(1-1/n)}{\delta}
    \end{split}
  \end{equation}
Replacing these constants $a^*_n$ and $b^*_n$, this give us $a_n$ and $b_n$ from $X_{(n)}$.
\end{comment}
\begin{comment}
%--------- Extreme value introduction
\citep{polzehletal10} proposed a multi-scaling test based on extreme
value theory~\citep{resnick13,sibuya60} using the SPM $\bGamma$. Their
approach uses a sequence of 
pre-specified smoothing operations on $\bGamma$ and sequential
thresholding operations based on extreme value theory. However, their
development ignores the 
correlation structure in the SPMs, and also that the thresholding
results in additional changes in the structure. Here, we
\end{comment}
Our objective is to develop an approach
that adaptively and automatically smooths and thresholds the SPM while
incorporating spatial correlation and the fact that the
sequential thresholding results in SPMs from truncated
distributions. Before detailing  our methods, we provide  some
theoretical development.   
\begin{comment}
while also having a smoothing
sequence 

By assuming this, the spatial structure is ignore and even though activated regions seems to reasonable the presence of this spatial structure will be notice. To address the issues under spatial structure we derived the distribution of the maximum of normal random variables with correlated structure, ${\bf X} \sim N_m(\bmu,{\bf R})$, where ${\bf X} = (X_1,\ldots,X_m)'$.  
\end{comment}
\subsection{Theoretical Development}
\label{method:theory}
We assume $t$-distributed SPMs with degrees of
freedom large enough for them to be approximately standard normally
distributed under the hypothesis of no activation. The SPM 
has a homogeneous correlation structure, a reasonable
assumption with our use of radially 
symmetric smoothing kernels. % for smoothing.
We have  
\begin{theorem}
\label{theo:evt}
  Let ${\bX} \sim \mN_n(\bzero,{\bR})$ where $\bX =
(X_1,\ldots,X_n)'$ and ${\bR}$ is a circulant correlation
matrix with only nonnegative elements such that $\bR^{1/2}$ also has
no negative entries. Writing $\bone = (1,1,\ldots,1)'$, we
let $\varrho$ be the sum 
of the elements in any row of ${\bR}^{\frac12}$. Further, let
$X_{(n)}$ be the maximum value of ${\bX}$, that is, $X_{(n)} =
\max{\{X_1,X_2,\ldots,X_n\}}\equiv\max\bX$. Then  the cumulative distribution
function (CDF) $F_{(n)}(x)$ of $X_{(n)}$ is given by $F_{(n)}(x) = P(X_{(n)} \leq
x)\geq %P(Y_{(m)} \leq  \varrho x) =
[\Phi(x/\varrho)]^n$, where
%$Y_{(m)} =     %\max\{ Y_1,\ldots,Y_m\}$, $\bY \sim N_m(\bzero,\bI_m)$, $\bI_m$ is the identity matrix of order $m$, and
$\Phi(\cdot)$ is the CDF of the
standard normal random variable. The equality holds when
$\bR^{-1/2}$ also has no negative entries.
\end{theorem}

\begin{proof}
  Let $\bZ\!\sim\! \mN_n(\bzero,\bI_n)$ and $Z_{(n)} =
  \max{\{Z_1,Z_2,\ldots,Z_n\}}$ have CDF $\Phi_{(n)}(z)\equiv [\Phi(z)]^n$. 
  Then $\Phi_{(n)}(x/\varrho)  =
  \mP[Z_{(n)} \leq x/\varrho] = \mP[\bZ \leq   x \bone/\varrho]$ so that
%  the RHS
  \begin{equation}
    \begin{split}
\Phi_{(n)}(x/\varrho) & \leq\mP[{\bR}^{1/2}\bZ \leq x {\bR}^{1/2}\bone/\varrho]\\ %,\mbox{ where } \bY\sim N_n(\bzero,\bI_n)\\
      & =   \mP[\bX \leq x \bone], \mbox{ where } \bX\sim N_n(\bzero,\bR)\\
      &    = \mP[X_{(n)}\leq x] = F_{(n)}(x).\\
      \label{th1}
  \end{split}
\end{equation}
Now $\bR^{-1/2}$ is also circulant and 
$\bR^{1/2}\bR^{-1/2}\bone = \tilde\varrho\bR^{1/2}\bone =
\tilde\varrho\varrho\bone$ where $\tilde\varrho$ is the
sum of the elements of any row of  
$\bR^{-1/2}$ and 
$\tilde\varrho=1/\varrho$. If $\bR^{-1/2}$ has no negative elements, $F_{(n)}(x)=\mP[\bX \leq x \bone]\leq \mP[{\bR}^{-1/2}\bX \leq x
{\bR}^{-1/2}\bone] =\mP[\bZ \leq x\tilde\varrho\bone] =
\Phi_{(n)}(x/\varrho) $ and then equality holds in \eqref{th1}.
\end{proof}
\begin{corollary}
  \label{theo:gumbel}
  For $\bX$ and $X_{(n)}$ as in Theorem~\ref{theo:evt}, the limiting
  distribution of $X_{(n)}$ is bounded below by one that lies in the
  domain of  attraction of   the Gumbel distribution, 
  and satisfies 
  \begin{equation}
    \lim_{n\rightarrow \infty}[F_n(a_n x + b_n)] \geq \exp\{-\exp(-x)\},
\end{equation}
where $a_n = \varrho/[ n \phi(b_n/\varrho)]$ and $b_n =\varrho
\Phi^{-1}(1-1/n)$, with $\phi(\cdot)$ the standard normal
probability density function (PDF).
\end{corollary}
\begin{proof}
  % From  Theorem~\ref{theo:evt}, we have
Each element of $\bW\sim \mN_n(\bzero,\varrho^2\bI_n)$ has CDF
$\Phi(\frac w\varrho)$ and PDF
  $\phi(\frac w\varrho)/\varrho$. Then
  $W_{(n)}\!\doteq\!\max\bW$  has CDF $G_{(n)}(\cdot)$ that satisfies
  $\lim_{n\rightarrow     \infty}G_{(n)}(a_n x + b_n)\!=\! 
  \exp\{-\exp(-x)\}$ with $a_n\!=\!\varrho/[n \phi(b_n/\varrho)]$ and
  $b_n\!=\!\varrho\Phi^{-1}(1-1/n)$~\citet{resnick13}.
  The result follows from   Theorem~\ref{theo:evt}.
\end{proof}
This paper uses $\bR$ for which $\bR^{1/2}$ can be
shown, using Theorem 2 of \citet{maitra19}, to have no negative
entries. Then Corollary~\ref{theo:gumbel}  provides a
conservative bound for the quantiles of the limiting distribution of
$\max\bX\sim\mN(\bzero,\bR)$ with the conservatism 
determined by the negative elements of $\bR^{-1/2}$.

The thresholding steps yield {\em truncated} (and correlated) random
variables for potential thresholding  in subsequent
steps. We account for this added complication by deriving the limiting 
distribution of the maximum of a correlated sample from a
right-truncated normal distribution. %We first develop the case for a
                                %limiting distribution of independent
%right-truncated normals.

Suppose that 
\begin{comment}


%Obtaining the above and based on results on Extreme value theory ~\citep{resnick2013}, we compute the limiting distribution of the maximum $\bY$.
%Let ${\bf R} = {\bf R}_1 \otimes {\bf R}_2 \otimes {\bf R}_3$,be the tensor product of the covariance matrix with each ${\bf R}_i$ representing th-dimension,

To achieve the above, from a given vector of bandwidth, ${\bf h}=(h_x,h_y,h_z)'$, and using a Gaussian kernel 3D obtain ${\bf S}^{-1}_{\bf h}$ and let ${\bf R} = {\bf S}^{-1}_{\bf h}$.  Our goal now is to obtain, ${\bf R}^{-1/2}$ one way to achieve this is by using the Fast Fourier Transform (FFT). From ${\bf S}$  obtained $\check{{\bf S}}^{-1}_{\bf h} $ and obtain ${\bf S}^{1/2}_{\bf h}$. Then for a given ${\bf h}=(h_1,h_2,h_3)'$ compute ${\bf Y} = {\bf S}^{-1/2}_{\bf h} {\bf X}$.   
From ~\citep{resnick2013}; a non-degenerate limiting distribution function $F$ of a sample maximal $X_{(n)}$, 
\begin{enumerate}
\item Fr\'echet: $G_{1,\alpha}(x) = \exp\{-x^{-\alpha}\} I(x \geq 0)$.
\item Weibull:  $G_{2,\alpha}(x) = \exp\{-(-x^{-\alpha})\} I(x \leq 0)$.
\item Gumbel:  $G_{3,\alpha = 0}(x) = \exp\{-e^{-x}\} I(x \in \R)$.
\end{enumerate}
\end{comment}
\begin{comment}
Since we are assuming that the statistical parametric map $\bY$ follows a normal distribution with. To determine the normalizing constants $a_n$ and $b_n$  of the limiting distribution, we compare the approximation of limiting distribution against the true one, we follow ~\citep{polzehletal10} to choose the normalizing constants $a_n$ and $b_n$ by minimizing the mean square relative error. %The relative error is a measure $(E - T)/T$

\begin{equation}
  \begin{split}
    \log([F(a_n \delta_x + b_n)]^n) = n\log ( \Phi(a_n \delta_x +b_n ) \\
    \approx \log(\exp\{ -e^{-\delta_x}\}) = -\exp\{-\delta_x\}
  \end{split}
\end{equation}
The mean relative error when assuming gaussian distribution;
\begin{equation}
  \begin{split}
   Q(a_n,b_n)= \left(\frac{n \log( \Phi(a_n \delta_x + b_n)) - (-\exp\{ -\delta_x \})}{-\exp\{-\delta_x\}}\right)^2 \\
   = \left( n \log(\Phi(a_n \delta_x +b_n))\exp\{\delta_x \}+1 \right)^2 
  \end{split}
\end{equation}
The limiting distribution is satisfy for normalizing constant such that;$G^n_{i,\alpha}(a_nx+b_n) \rightarrow G_{i,\alpha}$. If $a_n = n^{1/\alpha}$ and $b_n=0$ the limiting distribution is Fr\'echet, if $a_n^{-1/\alpha}$ and $b_n=0$ the limiting distribution is Weibull and if $a_n=1,b_n =\log n$, the limiting distribution is Gumbel. Suppose $\bX_1,\ldots, \bX_n$  are iid random vectors in $\R^m$, for $m\geq2$, with common distribution $F$. Suppose the marginal distributions of $F$ are the same and equal to some $F_1(x)$ which belong to some univariate extreme value distribution $G_{i,\alpha}(x)$, there exists constants $a_n \in \R^+, b_n \in \R$, such that $F^n_1(a_n x + b) \rightarrow G_{i,\alpha}$. For a multivariate case, using \citep{resnick2013}; let $\bX^{(n)}$ be the maximum of a random vector. If $F$ is multivariate; 
\end{comment}
$Y_1,Y_2,\ldots Y_n$ are independent identically distributed (IID) random
variables from $\mN(0,\varrho^2)$ but truncated at $\eta$, then
each $Y_i$ has PDF $\phi_\eta^{\bullet}(y;\varrho)$ and
CDF $\Phi_\eta^{\bullet}(y;\varrho)$, where
\begin{equation}
  \label{truncnormal}
  \phi_\eta^{\bullet}(y;\varrho) = \frac{\phi(\frac y\varrho)}{\varrho\Phi(\frac\eta\varrho)} I(y < \eta);\:\:\: \Phi_\eta^{\bullet}(y;\varrho) = \frac{\Phi[\frac{\min(y,\eta)}\varrho]}{\Phi(\frac\eta\varrho)},
\end{equation}
where $I(\cdot)$ is the indicator function. Then
$Y_{(n)}=\max{\{Y_1,Y_2,\ldots,Y_n\}}$ has CDF 
$\Phi_{\eta,(n)}^\bullet(y;\varrho)\!= \!
\left[{\Phi(\frac{\min(y,\eta)}\varrho)}/{\Phi(\frac\eta\varrho)}\right]^n $ with limiting
distribution as follows.%by Theorem \ref{theo:indeptruncnormal}.
\begin{comment}
  A sufficient condition to determine if $G^\bullet(\cdot) \in
D(H)$, where $H(\cdot)$ is a CDF is to show that,
\begin{equation}
\lim_{x\rightarrow x_0} \frac{d}{dx}\left( \frac{1-F(x)}{f(x)}\right) = 0
\end{equation}
Recall that $x_0 = x_0(F) = \sup\{x|F(x) < 1\}$, for our case $x_0(F) =\sup\{x|F(x) < 1\} = \sup\{x| \Phi(x) < \Phi(\eta)\}$, clearly $x_0 = \eta$. If, 
\begin{equation}
\lim_{x\rightarrow \eta} \frac{d}{dx}\left( \frac{1-F(x)}{f(x)}\right) = 0
\end{equation}
then the truncated normal distribution belong to the Gumbel domain.
%------ Proof
\begin{equation}
\begin{split}
\frac{d}{dx}\left( \frac{1-F(x)}{f(x)}\right) = \frac{d}{dx}\left((1-F(x))[f(x)]^{-1}\right)\\ 
 = -\frac{f'(x)(1-F(x))}{[f(x)]^2} - 1
\end{split}
\end{equation}
Using the above derivation, on the limit
\begin{equation}
\begin{split}
    \lim_{x\rightarrow \eta} \left(-\frac{f'(x)(1-F(x))}{[f(x)]^2} - 1\right)\\ 
    = \lim_{x\rightarrow \eta} \left( -\frac{f'(x)}{f(x)}\frac{(1-F(x))}{f(x)} - 1 \right)
\end{split}
\end{equation}
Using the corresponding density and distribution function in our case,$f(x) = \phi(x)/\Phi(\eta) I(x < \eta)$, $f'(x) =  \phi'(x)/\Phi(\eta) I(x < \eta)$and $F(x) = \Phi(x)/\Phi(\eta)I(x  < \eta)$
\begin{equation}
  \lim_{x\rightarrow \eta} \left( -\frac{\phi'(x)/\Phi(\eta)}{\phi(x)/\Phi(\eta)}\frac{(1-\Phi(x)/\Phi(\eta))}{\phi(x)/\Phi(\eta)} - 1 \right);
\end{equation}
The first expression $\frac{\phi'(x)}{\phi(x)} = x$,%$\frac{-\phi'(x)}{\phi(x)} = -\frac{\frac{1}{\sqrt{2\pi}}\exp\{-x^2/2\}(-x)}{\frac{1}{\sqrt{2\pi}}\exp\{-x^2/2\} }=x$,
\begin{equation}
\begin{split}
  \lim_{x\rightarrow \eta} \left( x\frac{(1-\Phi(x)/\Phi(\eta))}{\phi(x)/\Phi(\eta)} - 1 \right) \\ 
  = \left( \eta\frac{(1-\Phi(\eta)/\Phi(\eta))}{\phi(\eta)/\Phi(\eta)} - 1 \right) = -1.
\end{split}
\end{equation}
As we just showed the truncated normal distribution did not satisfy
the von-Mises criterion to determine if $F \in D(\Lambda)$. This was
expected since the convergence to Gumbel depends on the behavior of
the tails, if the tails are large enough, i.e., $\eta$ large, then the
above goes approximately to 0. The Mills ratio will be proportional to
$1/x$. In a sense we can assume that at the beginning we can have
values large enough to approach the Gumbel distribution. To find a
limiting distribution for the truncated normal case. Now we move to
the second limiting distribution in the extreme value family, the
reverse Weibull.
\end{comment}
\begin{theorem}
\label{theo:indeptruncnormal}
Let $Y_1,Y_2,\ldots,Y_n$ be a sample from \eqref{truncnormal}. Then
the limiting distribution of $Y_{(n)} $ % = \max{\{Y_i:i=1,2,\ldots,n\}}$
  belongs to the domain of attraction of the reverse Weibull
distribution and satisfies
\begin{equation}
  \label{rev.weibull}
  \lim_{n\rightarrow\infty}[\Phi^{\bullet}_{\eta,(n)}(a^\bullet_nx+b_n^\bullet;\varrho)] = \exp{\{-(-x)^{-\nu}\}} I(x \leq 0).
\end{equation}
for some $\nu >0$. Here $a_n^\bullet = \eta - {\Phi_\eta^\bullet}^{-1}(1-1/n;\varrho)$ and $b_n^\bullet = \eta$.
\end{theorem}
\begin{proof}
Note that $\eta = \sup\{x\mid \Phi_\eta^\bullet(x) < 
1\}$.
%for our case $x_0(F) =\sup\{x|F(x) < 1\} = \sup\{x| \Phi(x) <
%\Phi(\eta)\}$, clearly $x_0 = \eta$.
From Theorem 10.5.2 in \citet{davidandnagaraja03}, for $Y_{(n)}$ to be
in the domain of attraction of the reverse Weibull, it sufficient to
show that 
\begin{equation*}
\lim_{y \rightarrow \eta}
\frac{(\eta-y)\phi^\bullet(y;\varrho)}{1-\Phi^\bullet(y;\varrho)} = \nu
\end{equation*}
for some $\nu>0$~\citep{vonMises36}. In our case, the limit holds
because  $\eta <\infty$. Then upon using  L'H\^opital's 
rule, we have 
\begin{equation*}
\begin{split}
  \lim_{y \rightarrow \eta}
  \frac{(\eta-y)\phi_\eta^\bullet(y;\varrho)}{1-\Phi_\eta^\bullet(y;\varrho)}  & =
  \lim_{y \rightarrow \eta} \frac{(\eta-y)\frac{d}{dx}\phi_\eta^{\bullet}(y;\varrho)-\phi_\eta^\bullet(y;\varrho)}{-\phi_\eta^\bullet(y;\varrho)}  \\
& = \lim_{y\rightarrow \eta} \frac{(\eta-y)\phi'(\frac
  y\varrho)/\varrho-\phi(\frac y\varrho)/\varrho}{-\phi(\frac y\varrho)/\varrho }=1.\\
\end{split}
\end{equation*}
Thus the right-truncated normal distribution satisfies the reverse Weibull condition and
converges to the reverse Weibull distribution with %The derivation
%shows that
$\nu\! \equiv\! 1$ in \eqref{rev.weibull}. The constants in the
theorem are as per extreme value
theory~\citep{davidandnagaraja03,resnick13}. % provides the constants
                                % in the theorem.
%This means we can adjust a given SPM map by its maximum based on the reverse Weibull.
\end{proof}
\begin{theorem}
  \label{theo:trunc.corr.normal}
  Let $\bX$ % = (X_1,X_2,\ldots,X_n)$
  be a random vector from the
  $\mN_n(\bzero, \bR)$ density but that is right-truncated in each coordinate
  at $\eta$, with ${\bR}$ and $\varrho$ as in Theorem ~\ref{theo:evt}. %the maximum element in $\bX$. % =\max{\{X_1,X_2,\ldots,X_n\}}$.
Then  the CDF $F^\bullet_{\eta,(n)}(x)$ of $X_{(n)}$ is $F^\bullet_\eta(x) = \mP[X_{(n)} \leq
x] \geq  %P(Y_{(m)} \leq x/ \varrho) =
\Phi_{\eta,(n)}^\bullet(x;\varrho)$.
\end{theorem}
\begin{proof}
  % Proceeding as in the proof of Theorem~\ref{theo:evt} yields
  For IID random variables  $Y_1,Y_2,\ldots,Y_n$ from \eqref{truncnormal},
  $Y_{(n)}$ has
CDF $\Phi^\bullet_{\eta,(n)}(x;\varrho) = \mP[Y_{(n)}\leq x]
=\mP[U_{(n)}\leq \frac x\varrho] =
\Phi^\bullet_{\frac\eta\varrho,(n)}(\frac x\varrho;1)$, where $U_{(n)}
=\max\bU$ with $\bU$ a vector of $n$ IID random variables from $\bPhi^\bullet_{\frac\eta\varrho}$. Then 
  \begin{equation*}
    \begin{split}
      \Phi^\bullet_{\frac\eta\varrho,(n)}(x/\varrho;1)  = \mP[\bU \leq x \bone/\varrho]
      &\leq  \mP[{\bf R}^{1/2}\bY \leq x {\bf R}^{1/2}\bone/\varrho]\\
      &       = \mP[\bX \leq x \bone] \equiv   F^\bullet_\eta(x), %& = P(X_{(n)} \leq x)\
%    & = P(X_i\leq x\quad \forall\quad i =1,2,\ldots,n)\\
%    & = P(\bX \leq x \bone) \\
%    & = P({\bf R}^{-1/2}\bX \leq x {\bf R}^{-1/2}\bone)\\
%    & =   P(\bY \leq \varrho x \bone),\mbox{ where $Y_i$s are i.i.d. $\Phi^\bullet_{\varrho\eta}$(y)}\\
%&    = P(Y_i\leq \rho x \forall i=1,2,\ldots, n) \\
%&    = P(\max \{ Y_1,\ldots,Y_m\} \leq \rho x) \\
%&    = P(Y_{(n)} \leq \rho x) = [\Phi(\rho x)]^n.  \\
%    & = G^\bullet_{\varrho\eta}(\varrho x) = [\Phi(\varrho
%    x)/\Phi(\varrho \eta)]^nI(x<\eta).
    \end{split}
  \end{equation*}
  proving the statement of the theorem.
\end{proof}

\begin{corollary}
  \label{theo:evt.weibull}
  Let $\bX$ and $X_{(n)}$ be as in Theorem~\ref{theo:trunc.corr.normal}. Then the limiting
  distribution of $X_{(n)}$ belongs to the domain of attraction of
  the reverse Weibull distribution, and satisfies:
  \begin{equation}
    \lim_{n\rightarrow \infty}[F_{\eta}^\bullet (a^\bullet_nx
      + b^\bullet_n)] \geq \exp{\{-(-x)^{-\nu}\}} I(x \leq 0).
\end{equation}
where $a_n^\bullet = \eta - {\Phi_{\eta}^\bullet}^{-1}(1-1/n;\varrho)$ and $b_n^\bullet = \eta$.
\begin{proof}
The result is immediate from Theorems~\ref{theo:indeptruncnormal} 
and %  the limiting distribution of $Y_{(n)}$
% =\max{(Y_i:i=1,2,\ldots,n)}$. is \eqref{rev.weibull} so
%$F^\bullet_\eta(x)\!  \geq
%\!\Phi^\bullet_{\frac\eta\varrho,(n)}(x/\varrho)$ in    
 \ref{theo:trunc.corr.normal}. 
\end{proof}
\end{corollary}


\subsection{Fast Adaptive Smoothing and Thresholding}
\label{fast}
We propose our FAST algorithm that adaptively and, in sequence, smooths and
identifies activated regions by thresholding. We estimate the
amount of smoothing robustly or from the correlation structure that we
assume is well-approximated by an ellipsoidally-symmetric 3D Gaussian
kernel oriented along the three axes and with 
parameters $\bh=(h_1,h_2,h_3)$. That is, under the 
null hypothesis (of no activation anywhere), we assume that the 
SPM $\bGamma\sim \mN_n(\bzero, \sigma^2\bR)$ where $\bR\! =\! \bS_{\bh}$, with
$\bS_{\bh}$ a circulant smoothing matrix~\citep{maitraandosullivan98}.
Let $\bGamma_{(-{\bh})} \sim \bS_{\bh}^{-\frac12}\bGamma$. We estimate
$\bh$ and $\sigma$
by maximizing 
% by decorrelating $\bGamma$ using   $\bGamma_{(-h)} \sim
% \bS_h^{-\frac12}\bGamma$ and maximize
the loglikelihood function
\begin{equation}
  \label{llhd}
  \ell(\bh\mid\sigma,\bGamma_{(-\bh)}) = %-\frac{n}{2} \log(2 \pi)-
  constant -
  \frac12\log|\sigma^2\bS_{\bh}| - \frac1{2\sigma^2}\bGamma_{(-\bh)}'\bGamma_{(-\bh)}.
\end{equation}
Both $\bGamma_{(-\bh)}$ and $|\bS_{\bh}|$ are
speedily computed using Fast Fourier Transforms (FFTs). 
Starting with the
SPM $\bGamma$, obtained
as discussed in Section~\ref{method:prelim}, we  propose the algorithm:
\begin{enumerate}
%\item Obtained fMRI time series data. Spatially smooth the image to increase the SNR.
%\item At each voxel in the region of interest $V$, fit a set of models ${\cal M}$ and obtained the best model based on a model selection and/or model diagnostic tool.
\item {\em Initial Setup}. At this point, assume that $\zeta_i
  \equiv 0$ $\forall$ $i$, where $\zeta_i$ is the activation status of the $i$th
  voxel. That is, assume that all voxels are inactive. Set
  $\zeta_i^{(0)} \equiv \zeta_i$. Also denote $\bGamma^{(0)} =
  \bGamma$, and $n_0=n$, where $n_k$ denotes the number of voxels for which
  $\zeta_i^{(k)} = 0$.  
\item {\em Iterative Steps}, \label{step2}
  For $k\!=\!1,2,\ldots,$ iterate as follows:
  \begin{enumerate}
  \item 
    {\em Smoothing}. Smooth  $\bGamma^{(k-1)}$ in one of three
    ways:
    \begin{enumerate}
    \item\label{ALL-FAST} {\em Adaptive Likelihood maximization
        (ALL-FAST, pronounced {\em\^ol-fast})}:
    Maximize~\eqref{llhd} given $\bGamma^{(k-1)}$     to obtain 
    ${\bh}^{(k)}$. Smooth $\bGamma^{(k-1)}$ with
    $\bS_{{\bh}^{(k)}}$ to get $\bGamma^{(k)}$. 
  \item\label{AM-FAST} {\em Adaptive Model-based smoothing
      (AM-FAST, pronounced {\em\u{a}m-fast})}:
    Use a Markov Random Field (MRF) prior model with parameters
    estimated using empirical Bayes methods as described in
    Section~\ref{mb.smooth} to get $\bGamma^{(k)}$ from
    $\bGamma^{(k-1)}$. 
    \item\label{AR-FAST} {\em Adaptive Robust smoothing (AR-FAST,
        {\em ahr-fast})}: Use \citep{garcia10} to smooth
      $\bGamma^{(k-1)}$ and get    $\bGamma^{(k)}$.
    \end{enumerate}
  \item\label{std} {\em Standardization.} Maximize~\eqref{llhd} given
    $\bGamma^{(k)}$ to obtain $\sigma_{(k)}$, ${\bh}^{(k)}$ and $\varrho_k=
    {\bS}_{{\bh}^{(k)}}^{\frac12}{\bone}$. Standardize
      $\bGamma^{(k)}$ by scaling with a robust version of
      $\sigma_{(k)}$, say $\tilde\sigma_{(k)}$.
\item {\em Adaptive Thresholding}. This consists of two steps:
  \ben
\item
  For $k=1$, use Corollary \ref{theo:gumbel} to obtain $(a_n,b_n)$,
  otherwise ({\em i.e.} for $k > 1$)
  use Corollary \ref{theo:evt.weibull} to get
  $(a^\bullet_{n_{k-1}},b_{n_{k-1}}^\bullet)$. In both cases,
use $\bR=\bS_{{\bh}^{(k)}}$.
%  we use the spatial correlation   estimated by $\bS_{{\bh}^{(k-1)}}$. 
\item From the Gumbel (for $k=1$) or reverse Weibull distributions (for
  $k>1$), get
  \begin{equation}
    \label{gumbel.weibull.cutoff}
    \eta_k = \begin{cases} {a_{n_0}}\iota_{\alpha}^\mG+ b_{n_0} &
      \mbox{ for }k=1\\
    {a^\bullet_{n_{k-1}}}  \iota_{\alpha}^\mW + b^\bullet_{n_{k-1}}&
      \mbox{ otherwise. }\end{cases}
  \end{equation}
  where $\iota_{\alpha}^\mG$ and $\iota_{\alpha}^\mW$ are the
  upper-tail $\alpha$-values for the Gumbel and the reverse Weibull
  (with $\nu = 1$) distributions, respectively. 
\item Set $\zeta^{(k)}_i = 1$ if $\zeta^{(k-1)}_i = 0$ and if the
  $i$th coordinate of $\bGamma^{(k)}$ exceeds $\eta_k$. Let $n_k=\sum_{i=1}^n\zeta_i^{(k)}$.
    \een
    \een
  \item {\em Termination}.
    \label{stop}
    Declare no activation and terminate if $\bzeta^{(1)}\equiv
    0$. Otherwise, let
    $J(\boldsymbol{\zeta}^{(k)},\boldsymbol{\zeta}^{(k-1)})$ be 
    the Jaccard Index~\citep{jaccard1901,maitra10} of the activation
    maps in the  $k$th and $(k-1)$th iterations.
    If $J(\boldsymbol{\zeta}^{(k)},\boldsymbol{\zeta}^{(k-1)}) \geq
    J(\boldsymbol{\zeta}^{(k+1)},\boldsymbol{\zeta}^{(k)})$,  the
    algorithm terminates -- the final activation map is $\boldsymbol{\zeta}^{(k)}$.
    \een
    
%   The Jaccard Index is a similarity measurement, in this case voxels that are activated and those not activated. To compute the Jaccard Index for this activation map, we replaced those voxels that were negatived activated,i.e., $\zeta_i = -1$, we replaced them with the value 1.
%\end{enumerate}
% In this algorithm, the critical value $\nu$ it's changing at each step. At the first $\nu$ is based on the critical value of a Gumbel with significance level $1-\alpha$. For the reverse Weibull approach the critical value will be adjust it $\eta^{(k)}$, i.e $\nu^{(k)}_{1-\alpha}= \nu_{1-\alpha} + \eta^{(k)}$
    \subsubsection*{Comments} A few comments are in order:
   \paragraph{Correlation structure} A circulant
    correlation structure allows for spatial context in the association
    between the values of the voxel-wise test statistics, while having
    the added benefit of speedy computations via the use of
    FFTs.
    \paragraph{Robust Estimation of $\sigma_{(k)}$} The estimate  
    $\hat\sigma_{(k)}$ from \eqref{llhd} assumes no  activation in
    $\bGamma_{(k)}$. Ignoring the activation can inflate the estimate,
    so we obtain $\tilde\sigma_{(k)}=\hat\sigma_{(k)}\tilde s_{(k;w)}/s_{(k)}$
    where $s_{(k)}$ is the estimated SD of $\bGamma_{(k)}$, and
    $\tilde s_{(k;w)}$ is its biweight-estimated
    SD~\citep{hoaglinetal00}, both assuming a zero mean. Specifically,
    if $\bGamma_{(k)}$ has $i$th component $\Gamma_{i(k)}$, we calculate 
 \begin{equation*}
\tilde s_{(k,w)}=\frac{\left(n\right)^{\frac{1}{2}}\left[\sum_{|e_{i(k),j}|<1}\Gamma_{i(k)}^2\left(1-e_{i(k)}^2\right)^4\right]^\frac{1}{2}}{\abs{\sum_{\abs{e_{i(k)}<1}}\left(1-e_{i(k)}^2\right)\left(1-5e_{i(k)}^2\right)}}
\end{equation*}
where $e_{i(k),j}={\Gamma_{i(k)}}/(w\tilde s_{(k)})$,  $\tilde
s_{(k)}$ the median absolute deviation of $\bGamma_{(k)}$ from 0, and
$w=\argmin_{w\in(0,6)} \tilde s_{(k,w)}$.   
  
   \paragraph{Comparison with AS} Our FAST algorithms are 
   similar to AS \citet{polzehletal10} in %    provide an
                                %    adaptive weighted smoothing
                                %    approach based on
                                %    multi-scale testing. Both AM-FAST
                                %    and AR-FAST have
                                %    similarities with AS, in
   that they also smooth and threshold 
    iteratively. But there are a few 
    fundamental differences. The AS approach has a set
    user-specified sequence of bandwidths that smooths
    $\bGamma^{(k)}$ at
    each step. In contrast, ALL-, AM- and AR-FAST use likelihood, empirical Bayes and robust methods to optimally determine
    $\bh$ at each step. AS also 
    thresholds but uses a general
    Fr\'echet extreme value distribution that ignores  
    spatial context and the correlated truncated nature of the random
    variables that arise from the smoothing and thresholding at each
    iteration. Our
    development represents the procedure more accurately because we
    account for both the correlation structure (with the initial
    cut-off decided as per the Gumbel distribution) and the
    truncation (with subsequent cut-offs determined by the
    reverse Weibull distribution). Our more general $\bh$ 
    allows for different amounts of
    smoothing in each     axis. Finally, our method is     entirely
    data-driven, with termination declared only if there is no initial
    activation or when $\mJ$ between subsequent activation maps decreases.
    \begin{comment}
    \paragraph{Thresholding} A reviewer wondered about 
    multiple comparisons because of the connection of hypothesis
    testing with thresholding. Our  cutoff  at each iteration depends
    on the conditional distribution given 
    prior thresholding. Therefore, for $I$ iterations, the overall
    significance   is $\alpha^I$ which is not more than $ \alpha$. We study the role of $\alpha$ in the
    next section.
    \end{comment}
   \paragraph{Two-sided alternatives} Our development here builds from 
    one-sided tests where large values are the extreme
    values of the SPM. For two-sided alternatives, we  use the
    algorithm individually on the SPM and its negative, but replacing
    $\alpha$ in     \eqref{gumbel.weibull.cutoff} with
    $\alpha/2$. This provides two (disjoint) activation  
    maps, the union of which is the two-sided activation map.
 %We investigate performance based on Further, our investigations in the next section suggest using low and high values of $\alpha $  inversely  as CNR is high or low. 