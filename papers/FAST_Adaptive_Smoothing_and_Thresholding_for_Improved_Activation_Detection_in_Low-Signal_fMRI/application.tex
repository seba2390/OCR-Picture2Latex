\section{Activation During Perception of Noisy Speech}\label{sec:Apps}
The dataset, provided as  {\tt data6} in the AFNI tutorial~\citet{cox96},
is originally from an fMRI study~\citet{nathandbeauchamp11} where
\begin{figure}[h]
\subfloat[]{\includegraphics[width=0.5\columnwidth]{figs/AR-FAST-001-Visual-crop}}
\subfloat[]{\includegraphics[width=0.5\columnwidth]{figs/AR-FAST-001-Audio-crop}}
%\subfloat[]{\includegraphics[width=0.33\textwidth]{figs/AM-FAST-005-diff}}
\caption{ AR-FAST-identified activation regions on SPMs obtained by
  fitting ~\ref{eq:lm} with   AR($\hat{p}$) to AFNI's {\tt data6} for
  (a) visual-reliable stimulus and (b) audio-reliable
  stimulus.}
\label{fig:AMSmoothingAFNI}
\end{figure}
\begin{comment}
\begin{figure*}[h]
\subfloat[]{\includegraphics[width=0.25\textwidth]{figures/Visual_AM-crop}}
\subfloat[]{\includegraphics[width=0.25\textwidth]{figures/Audio_AM-crop}}
\subfloat[]{\includegraphics[width=0.25\textwidth]{figures/Visual_Audio_AM-crop}}
\caption{Activation areas obtained using AR-FAST with in {\it AFNI data6} on the SPM obtained
  after fitting AR($\hat{p}$) of the (a) Visual-reliable, (b) Audio-reliable and (c) the difference contrast between Visual-reliable and Audio-reliable.}
\label{fig:AMSmoothingAFNI}
\end{figure*}
\end{comment}
a subject heard and saw a female volunteer speak words, separately, in
two different formats. The audio-reliable setting had the subject
clearly hear the spoken word but see a degraded image of the speaker
while the visual-reliable case had the subject clearly see the speaker
vocalize the word but the audio was of reduced quality.  There were
three experimental runs, each 
consisting of a randomized design of 10 blocks, equally divided into blocks of
audio-reliable and visual-reliable stimuli. %An echo-planar imaging
% sequence (TR=2s) was used to obtain
$\mbox{T}_2^*$-weighted images with volumes of $80 \times 80 \times
33$ (with voxels of dimension $2.75 \times  2.75 \times 3.0\  mm^3$)
from  echo-planar sequences (TR=2s) 
were obtained  over $152$ time-points. Our interest was in determining 
activation corresponding to the audio
($H_0:\beta_{a}=0$) and visual
($H_0:\beta_{v}=0$) tasks.
%, as well as their contrast  ($H_0:\beta_{v} - \beta_{a}=0$). The
%first two cases have one-sided alternatives while the contrast in
%activation corresponds to a two-sided alternative.
At each voxel, we fitted AR models for 
$p=0,1,2,3,4,5$ and chose $p$ with the highest BIC. 
Figure \ref{fig:AMSmoothingAFNI} uses AFNI and Surface Mapping (SUMA)
to display activated regions obtained using AR-FAST on the SPM:
see  Figure~\ref{fig:Visual-Audio} for  maps drawn from ALL-FAST, AS,
AWS and CT. We used $\alpha = 0.01$ because of the high (greater than
4) upper percentile of the voxel-wise estimated CNRs. Most of the activation 
occurs in Brodmann areas 18 and 19 (BA18 and BA19)
which comprise the
occipital cortex %in the human brain,  accounting for the bulk of the
                 %volume of the occipital lobe. Both areas form part
                 %of the visual association area while BA 19, also the occipital lobe cortex as well. Along with BA18, it comprises
and the extrastriate (or peristriate) cortex. In humans with normal
sight,  this area is for visual association where 
feature-extraction, shape recognition, attentional and multimodal
integrating functions occur. We also see increased activation in
the STS, which recent
studies~\citep{grossman2001brain} have related to  distinguishing
voices from environmental sounds, 
stories versus nonsensical speech, moving faces versus moving objects,
biological motion and so on. ALL-FAST performs similarly as AR-FAST,
while the other methods also identify the same regions but they identify
a lot more activated 
voxels, some of which appear to be false positives. Although a 
detailed analysis of the results of this study is beyond the purview
of this paper, we note that AR-FAST 
finds interpretable results even when applied to a single
subject high-level cognition experiment. 
\begin{comment}
\begin{table}[h!]
\centering
\caption{Coordinates of the maximum $t$-statistic and its corresponding value}\label{tab:maxt}
\begin{tabular}{c|c}
Task & $(x,y,z) mm$  \\
\hline
Visual-Audio & (-30.162,86.221,6.349) \\
Audio & (-27.412,75.221,-5.651)  \\
 Visual & (-30.162, 80.721, 15.349) \\
\hline
\end{tabular}
\end{table}
\end{comment}
