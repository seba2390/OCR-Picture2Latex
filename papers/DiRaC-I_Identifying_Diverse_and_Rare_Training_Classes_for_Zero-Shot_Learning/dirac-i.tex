%%
%% This is file `sample-acmsmall.tex',
%% generated with the docstrip utility.
%%
%% The original source files were:
%%
%% samples.dtx  (with options: `acmsmall')
%% 
%% IMPORTANT NOTICE:
%% 
%% For the copyright see the source file.
%% 
%% Any modified versions of this file must be renamed
%% with new filenames distinct from sample-acmsmall.tex.
%% 
%% For distribution of the original source see the terms
%% for copying and modification in the file samples.dtx.
%% 
%% This generated file may be distributed as long as the
%% original source files, as listed above, are part of the
%% same distribution. (The sources need not necessarily be
%% in the same archive or directory.)
%%
%%
%% Commands for TeXCount
%TC:macro \cite [option:text,text]
%TC:macro \citep [option:text,text]
%TC:macro \citet [option:text,text]
%TC:envir table 0 1
%TC:envir table* 0 1
%TC:envir tabular [ignore] word
%TC:envir displaymath 0 word
%TC:envir math 0 word
%TC:envir comment 0 0
%%
%%
%% The first command in your LaTeX source must be the \documentclass command.
\documentclass[acmsmall]{acmart}

%%
%% \BibTeX command to typeset BibTeX logo in the docs
\AtBeginDocument{%
  \providecommand\BibTeX{{%
    Bib\TeX}}}

%% Rights management information.  This information is sent to you
%% when you complete the rights form.  These commands have SAMPLE
%% values in them; it is your responsibility as an author to replace
%% the commands and values with those provided to you when you
%% complete the rights form.
\setcopyright{acmcopyright}
\copyrightyear{2018}
\acmYear{2018}
\acmDOI{XXXXXXX.XXXXXXX}


%%
%% These commands are for a JOURNAL article.
\acmJournal{JACM}
\acmVolume{37}
\acmNumber{4}
\acmArticle{111}
\acmMonth{8}

%%
%% Submission ID.
%% Use this when submitting an article to a sponsored event. You'll
%% receive a unique submission ID from the organizers
%% of the event, and this ID should be used as the parameter to this command.
%%\acmSubmissionID{123-A56-BU3}

%%
%% For managing citations, it is recommended to use bibliography
%% files in BibTeX format.
%%
%% You can then either use BibTeX with the ACM-Reference-Format style,
%% or BibLaTeX with the acmnumeric or acmauthoryear sytles, that include
%% support for advanced citation of software artefact from the
%% biblatex-software package, also separately available on CTAN.
%%
%% Look at the sample-*-biblatex.tex files for templates showcasing
%% the biblatex styles.
%%

%%
%% The majority of ACM publications use numbered citations and
%% references.  The command \citestyle{authoryear} switches to the
%% "author year" style.
%%
%% If you are preparing content for an event
%% sponsored by ACM SIGGRAPH, you must use the "author year" style of
%% citations and references.
%% Uncommenting
%% the next command will enable that style.
%%\citestyle{acmauthoryear}



%%
%% end of the preamble, start of the body of the document source.

%added by me
\usepackage{amsmath} % define this before the line numbering.
\usepackage{color}
%\usepackage{subfigure}
%\usepackage{includegraphics}
% included by me
\usepackage{multirow}
\usepackage{subcaption}
\begin{document}

%%
%% The "title" command has an optional parameter,
%% allowing the author to define a "short title" to be used in page headers.
\title{DiRaC-I: Identifying Diverse and Rare Training Classes for Zero-Shot Learning}

%%
%% The "author" command and its associated commands are used to define
%% the authors and their affiliations.
%% Of note is the shared affiliation of the first two authors, and the
%% "authornote" and "authornotemark" commands
%% used to denote shared contribution to the research.
\author{Sandipan Sarma}
%\authornote{Both authors contributed equally to this research.}
\email{sandipan.sarma@iitg.ac.in}
%\orcid{1234-5678-9012}
\author{Arijit Sur}
%\authornotemark[1]
\email{arijit@iitg.ac.in}
\affiliation{%
  \institution{Indian Institute of Technology Guwahati}
  \streetaddress{Multimedia Lab, Department of Computer Science and Engineering}
  \city{Guwahati}
  \state{Assam}
  \country{India}
  \postcode{781039}
}

%\author{Lars Th{\o}rv{\"a}ld}
%\affiliation{%
%  \institution{The Th{\o}rv{\"a}ld Group}
%  \streetaddress{1 Th{\o}rv{\"a}ld Circle}
%  \city{Hekla}
%  \country{Iceland}}
%\email{larst@affiliation.org}
%
%\author{Valerie B\'eranger}
%\affiliation{%
%  \institution{Inria Paris-Rocquencourt}
%  \city{Rocquencourt}
%  \country{France}
%}
%
%\author{Aparna Patel}
%\affiliation{%
% \institution{Rajiv Gandhi University}
% \streetaddress{Rono-Hills}
% \city{Doimukh}
% \state{Arunachal Pradesh}
% \country{India}}
%
%\author{Huifen Chan}
%\affiliation{%
%  \institution{Tsinghua University}
%  \streetaddress{30 Shuangqing Rd}
%  \city{Haidian Qu}
%  \state{Beijing Shi}
%  \country{China}}
%
%\author{Charles Palmer}
%\affiliation{%
%  \institution{Palmer Research Laboratories}
%  \streetaddress{8600 Datapoint Drive}
%  \city{San Antonio}
%  \state{Texas}
%  \country{USA}
%  \postcode{78229}}
%\email{cpalmer@prl.com}
%
%\author{John Smith}
%\affiliation{%
%  \institution{The Th{\o}rv{\"a}ld Group}
%  \streetaddress{1 Th{\o}rv{\"a}ld Circle}
%  \city{Hekla}
%  \country{Iceland}}
%\email{jsmith@affiliation.org}
%
%\author{Julius P. Kumquat}
%\affiliation{%
%  \institution{The Kumquat Consortium}
%  \city{New York}
%  \country{USA}}
%\email{jpkumquat@consortium.net}

%%
%% By default, the full list of authors will be used in the page
%% headers. Often, this list is too long, and will overlap
%% other information printed in the page headers. This command allows
%% the author to define a more concise list
%% of authors' names for this purpose.
\renewcommand{\shortauthors}{Sandipan Sarma and Arijit Sur}

%%
%% The abstract is a short summary of the work to be presented in the
%% article.
\begin{abstract}
  Inspired by strategies like Active Learning, it is intuitive that intelligently selecting the training classes from a dataset for Zero-Shot Learning (ZSL) can improve the performance of existing ZSL methods. In this work, we propose a framework called Diverse and Rare Class Identifier (DiRaC-I) which, given an attribute-based dataset, can intelligently yield the most suitable ``seen classes'' for training ZSL models. DiRaC-I has two main goals -- constructing a diversified set of seed classes, followed by a visual-semantic mining algorithm initialized by these seed classes that acquires the classes capturing both diversity and rarity in the object domain adequately. These classes can then be used as “seen classes” to train ZSL models for image classification. We adopt a real-world scenario where novel object classes are available to neither DiRaC-I nor the ZSL models during training and conducted extensive experiments on two benchmark data sets for zero-shot image classification — CUB and SUN. Our results demonstrate DiRaC-I helps ZSL models to achieve significant classification accuracy improvements.
\end{abstract}

%%
%% The code below is generated by the tool at http://dl.acm.org/ccs.cfm.
%% Please copy and paste the code instead of the example below.
%%
%\begin{CCSXML}
%<ccs2012>
% <concept>
%  <concept_id>10010520.10010553.10010562</concept_id>
%  <concept_desc>Computer systems organization~Embedded systems</concept_desc>
%  <concept_significance>500</concept_significance>
% </concept>
% <concept>
%  <concept_id>10010520.10010575.10010755</concept_id>
%  <concept_desc>Computer systems organization~Redundancy</concept_desc>
%  <concept_significance>300</concept_significance>
% </concept>
% <concept>
%  <concept_id>10010520.10010553.10010554</concept_id>
%  <concept_desc>Computer systems organization~Robotics</concept_desc>
%  <concept_significance>100</concept_significance>
% </concept>
% <concept>
%  <concept_id>10003033.10003083.10003095</concept_id>
%  <concept_desc>Networks~Network reliability</concept_desc>
%  <concept_significance>100</concept_significance>
% </concept>
%</ccs2012>
%\end{CCSXML}

%\ccsdesc[500]{Computer systems organization~Embedded systems}
%\ccsdesc[300]{Computer systems organization~Redundancy}
%\ccsdesc{Computer systems organization~Robotics}
%\ccsdesc[100]{Networks~Network reliability}

\begin{CCSXML}
	<ccs2012>
	<concept>
	<concept_id>10010147.10010257.10010258.10010262.10010277</concept_id>
	<concept_desc>Computing methodologies~Transfer learning</concept_desc>
	<concept_significance>500</concept_significance>
	</concept>
	</ccs2012>
\end{CCSXML}

\ccsdesc[500]{Computing methodologies~Transfer learning}

%%
%% Keywords. The author(s) should pick words that accurately describe
%% the work being presented. Separate the keywords with commas.
\keywords{Zero-shot learning, deep learning, object recognition, image classification}

%%
%% This command processes the author and affiliation and title
%% information and builds the first part of the formatted document.
\maketitle

\section{Introduction}
\label{sec:intro}
Object recognition has witnessed a significant improvement in the recent past using deep learning methods~\cite{simonyan2014very,szegedy2015going,he2016deep,huang2017densely,howard2017mobilenets,liu2021medical} trained on large, annotated data sets in a supervised fashion. However, such methods fail when novel concepts are encountered. For example, an underwater robot exploring deep-sea biodiversity should trigger an alert if it encounters a novel or rare species --- like a \textit{Manocherian's Catshark} (Fig.~\ref{fig:work_idea}) --- but would probably fail as its recognition model is not trained on visual images of that species. A human, on the contrary, can recognize it if he/she has a visual perception about sharks and is given additional information that it looks like a small shark with some characteristic {\it attributes} --- a whitish, porcelain-colored body with a white spot on the tail tip. The idea of zero-shot learning (ZSL)~\cite{lampert2013attribute,xian2018zero} stems from this ability of humans to recognize unseen objects by learning a mapping function associating the visual samples from the seen classes with their {\it semantics} (or attributes). This function is then used to recognize both seen and unseen objects. 

\begin{figure}[t]
	\centering
	\includegraphics[width=\linewidth]{latex/phases.pdf}
	
	\caption{\textbf{ZSL community and the contributors.} Dataset constructors manually define split of seen-unseen classes which is received by ZSL researchers and used for training and evaluating ZSL models. High-performance models would be used for real-world deployment for image classification. {\bf DiRaC-I targets the work of the constructors ({\it green dotted box}) and aims to replace manual splits by intelligent splits automatically.}}
	\Description{Different actors of the ZSL community}
	\label{fig:zsl-comm}
\end{figure}


The ZSL community can be divided into three groups (Fig.~\ref{fig:zsl-comm}) based on their contributions towards ZSL -- (1) \textit{Dataset constructors}, who collect labeled data and semantics for a fixed number (say $k$) of object classes. Before releasing the dataset for ZSL research, they define a disjoint seen-unseen split of the $k$ classes {\bf manually}; (2) \textit{ZSL researchers}, who use these {\it predetermined} seen classes for training the ZSL models they propose, and the {\it predetermined} unseen classes to evaluate these models and simulate their ability to classify unseen classes {\it of the wild} when deployed in future; (3) \textit{AI-aided industries}, which deploy the state-of-the-art models to solve real-world problems using ZSL. For ZSL-based classification models to be widely accepted in the future by industries, the need for high-performance ZSL models trained with classes that capture rarity and diversity of the \textit{object domain} (defined in Sec.~\ref{dirac-i}) is paramount. However, ZSL models proposed by researchers today only {\it passively} learn from the predetermined set of seen classes provided by dataset constructors, like Xian et al.~\cite{xian2018zero}. They claim that class diversity is maintained while manually defining the seen-unseen sets; however, such splits are not designed for best zero-shot performance~\cite{lampert2013attribute}. Hence, an intelligent seen-unseen split of the $k$ classes of the collected dataset should be designed such that the designated seen classes automatically capture the diversity and rarity of the object domain. Only a few studies have addressed this issue in ZSL, mostly using Active Learning (AL) approaches. \cite{xie2016active,xie2017active} experiment with textual datasets only. Recently, in the image classification area,~\cite{wang2021graph} proposed an GCN-based AL framework for selecting the most crucial classes as seen classes for training. However, all these works initialize the AL algorithm with a randomly selected set of classes with labeled examples, called the {\it seed set}. Additionally, they do not consider the rare attributes for enriching the training set.

%new change
In this work, we propose a two-stage framework named \textit{Diverse and Rare Class Identifier (DiRaC-I)} inspired by AL which targets the attribute-based dataset constructors. From the $k$-class dataset provided by the constructors, DiRaC-I aims to select the most suitable seen classes for training ZSL models while trying to capture visual diversity and semantic rarity. The first stage is seed-set construction, where the $k$ classes are clustered based on semantic similarity. A single representative is picked from each cluster to ensure diversity while jointly prioritizing semantic rarity, forming a seed set. Intuitively, doing so would incorporate a generalized initial understanding of the object domain within the seed set, which is used as input for the next stage --- Visual-Semantic Mining (VSM). Here, we seek to maximize the diversity between visual samples of the seed and those of the other classes by estimating the distribution of {\it related} classes, based on the work by~\cite{bendale2016towards}, and select a few candidate non-seed classes. We define a {\it semantic score} to be computed for each of them, and a few classes having the highest semantic scores are added to the seed set. This process continues iteratively till we get a fixed number of seed classes, which become our seen classes to be provided as an output to researchers for training ZSL models.



We ensure a fair comparison of the knowledge gained while training using our seen classes (from Proposed seen-unseen Splits (PS)) and the predetermined (from Existing seen-unseen Splits (ES)) by evaluating the performance of existing ZSL models on a common set of unseen classes. For a given data set, this set is derived by randomly picking 50\% classes from all the unseen classes (for reasons given in Sec.~\ref{sec:3.2}) used in ES. These classes are not used during the entire operation of DiRaC-I or ZSL model training but only during the evaluation of ZSL models. Extensive experiments conducted on two challenging benchmark data sets --- CUB and SUN --- demonstrate that zero-shot accuracy of most models are enhanced when trained with seen classes acquired by DiRaC-I. In real-world situations, our framework should be able to select seen classes from a given attribute-based dataset to help improve the training of the ZSL models to be deployed for image classification. 

We summarize our contributions as follows. (1) We design a framework, DiRaC-I, that intelligently captures the diversity and rarity of the object domain within a set of seen classes on which ZSL models can be trained to have a comprehensive idea of the domain. (2) For initializing the VSM algorithm, a few diverse seed classes are selected as per some attribute-based scores, instead of just selecting them randomly. The rare attributes play a key role in computing these scores. (3) ZSL model performance is evaluated on a set of unseen classes common to both existing splits (ES) and proposed splits (PS) that are unavailable to DiRaC-I and the model during training (simulating practical scenarios of encountering novel classes in the wild). Hence, a fair comparison between the knowledge gained by the model trained with predetermined seen classes of ES and the acquired seen classes of PS is ensured. (4) Unlike~\cite{wang2021graph}, DiRaC-I can be used by ZSL researchers as a predecessor to select the seen classes from a given attribute-based dataset, and therefore can adapt to real-world scenarios. 

%\begin{figure}[t]
%	\centering
%	\includegraphics[width=0.4\linewidth]{latex/test classes split.pdf}
%	
%	\caption{In a real-world scenario, data for a certain number of object categories are collected by dataset constructors. Existing Split (ES) proposed in \cite{xian2018zero} defined some fixed classes to be used as seen ($\mathcal{S}\textsubscript{E}$) and unseen ($\mathcal{U}\textsubscript{E}$) while performing ZSL. We randomly split the set $\mathcal{U}\textsubscript{E}$ and obtain a set of common unseen classes ($\mathcal{U}_{com}$). The proposed framework has access to only the remaining classes belonging to set $\Tilde{C}$, which constitute the known object domain for the framework. After training several existing ZSL models with seen classes both from ES and PS (our Proposed Splits), model performance is evaluated only on classes from $\mathcal{U}_{com}$ for fair comparison, because these classes are common to both ES and PS. We can consider classes from $\mathcal{U}_{com}$ to act as novel classes encountered {\it in the wild}}
%	\Description{How to split the classes from a dataset into disjoint sets and select a suitable set of seen classes for ZSL}
%	\label{fig:test splits}
%\end{figure}


\begin{figure}[t]
	\centering
	\begin{subfigure}[t]{0.35\textwidth}
		\includegraphics[scale=0.45]{latex/test_classes_split.pdf}
		\caption{Splitting the available set of classes in a collected dataset to obtain an object domain to work with and the novel classes for model evaluation}
		\label{fig:test split}
	\end{subfigure} 
	\hfil
	\begin{subfigure}[t]{0.45\textwidth}
		\includegraphics[scale=0.45]{latex/work_idea.pdf}
		\caption{``Seen classes'' designated by DiRaC-I for training ZSL models -- an application}
		\label{fig:work_idea}
	\end{subfigure}
	\caption{\textbf{The target of DiRaC-I and its application}. \textit{(Left)} In a real-world scenario, data for a certain number of object categories are collected by dataset constructors. For example, Existing Split (ES) proposed by Xian et al.~\cite{xian2018zero} defines some fixed classes to be used as ``seen'' ($\mathcal{S}\textsubscript{E}$) and ``unseen'' ($\mathcal{U}\textsubscript{E}$) while performing ZSL. We randomly split the set $\mathcal{U}\textsubscript{E}$ and obtain a set of common unseen classes ($\mathcal{U}_{com}$). The proposed framework has access to only the remaining classes belonging to set $\Tilde{C}$, which constitute the known object domain for the framework. After training several existing ZSL models with seen classes both from ES and PS (our Proposed Splits), model performance is evaluated only on classes from $\mathcal{U}_{com}$ for fair comparison, because these classes are common to both ES and PS. We can consider classes from $\mathcal{U}_{com}$ to act as novel classes encountered {\it in the wild}; \textit{(Right)} Leveraging the DiRaC-I framework for selecting suitable seen classes from the object domain of {\it fish} -- a real-world application}
	\Description{How to split the classes from a dataset into disjoint sets and select a suitable set of seen classes for ZSL. An application of our framework DiRaC-I ZSL model while exploring underwater biodiversity is also shown, leveraging visual and semantic spaces of the object domain.}
	\label{fig:split-n-idea}
\end{figure}


\section{Related Work}
\label{sec:literature}
\subsection{Zero-shot learning (ZSL)}
\label{sec:lit-zsl}
Motivated by the problems faced in supervised learning, several new learning paradigms have been proposed in the last decade or so, such as few-shot~\cite{cao2022learning,jiang2020few} and one-shot learning~\cite{fei2006one,liu2021single}. However, methods under these paradigms are still not able to cope with scenarios where we have ``zero" training samples of certain classes -- e.g. a rare fish species. Therefore, in the recent years there has been an increasing amount of interest in zero-shot learning, which defines a setting where visual features for unseen classes are unavailable during model training. However, seen and unseen classes can be linked through their semantics. ZSL has been applied to a wide array of computer vision tasks such as object detection~\cite{bansal2018zero}, action recognition~\cite{chen2021elaborative} and cross-modal retrieval~\cite{xu2021zero} to name a few. We, however, focus on the zero-shot image classification task in the following discussion.

The early works on ZSL~\cite{rohrbach2010helps,rohrbach2011evaluating,kankuekul2012online,lampert2013attribute} tried to learn intermediate attribute classifiers to transfer knowledge from the seen to unseen. Several other approaches that followed~\cite{frome2013devise,socher2013zero,akata2015label,akata2015evaluation,romera2015embarrassingly,xian2016latent,kodirov2017semantic} directly set up bi-linear and non-linear compatibility functions between the visual and semantic spaces. At test time, unseen visual features are projected to the semantic space using the learned functions, and the predicted class is the one achieving maximum compatibility score. The approach of learning a mixture model of seen classes to represent the images and semantic embeddings is taken up in~\cite{norouzi2013zero,zhang2015zero,changpinyo2016synthesized}. While these approaches work well in the conventional setting (CZSL) of unseen test classes only, in practice, a model should be able to classify samples from both seen and unseen classes when deployed. Generalized zero-shot learning (GZSL) is a setting that considers such a scenario. Most existing works that show improvements in GZSL have incorporated generative models~\cite{mishra2018generative,xian2018feature,felix2018multi,xian2019f,vyas2020leveraging,narayan2020latent,feng2020transfer,tang2021zero}, where the aim is to synthesize high-quality unseen class samples or visual features, converting the ZSL problem into a simple supervised classification task. A benchmark providing standard evaluation protocols and seen-unseen splits for some of the widely-used data sets in ZSL is given by~\cite{xian2018zero}. However, manually created seen-unseen splits might not capture the diversity and rarity well enough for training ZSL models, affecting their knowledge about the object domain.



\subsection{Selecting suitable seen classes for ZSL}
\label{sec:lit-al-zsl}
The idea of training ZSL models with seen classes more informative than the predetermined ones is relatively new. Recently, Active Learning (AL)~\cite{hanneke2014theory} strategies have been employed in this direction. However, contrary to the traditional way of acquiring the most informative {\it instances} from a data set for training classifiers, in the zero-shot setting, the objective of AL changes to acquiring informative {\it classes}. \cite{xie2016active} proposes a probabilistic method that focuses on two properties --- informativeness of the seen classes and their connectivity to the unseen. An extension of this work~\cite{xie2017active} demonstrates the impact of AL on ZSL for extreme multi-label classification. 
%(having millions of different labels). 
However, it experiments with textual data sets only. \cite{wang2021graph} adopts an AL approach for GCN-based zero-shot image classification. Their work extends the k-center algorithm with a Laplacian energy-based strategy for selecting the most crucial classes as seen classes. However, it is limited to GCN frameworks for ZSL and initializes the algorithm with a randomly selected seed set, like the other works on AL-based ZSL. 

Research in traditional AL has shown that instead of selecting the seed set {\it randomly}, an intelligent selection can propel AL in better directions. In an effort to justify this,~\cite{tomanek-etal-2009-proper} proposes to manually prepare the seed set, artificially enriched with rare class examples. \cite{dligach-palmer-2011-good} gives an automatic approach that follows the same principle. Nevertheless, to the best of our knowledge, no work has shown the combined benefits of intelligently acquiring seed classes and using them to obtain seen classes that capture the diversity and rarity from the object domain. Our proposed framework (DiRaC-I) first constructs a seed set with a diverse initial representation of the object domain. It then initializes an AL-inspired algorithm with this seed set and iteratively acquires a fixed number of seen classes using which ZSL models are to be trained. The most recent work with a similar objective picks the seed set randomly and is compatible only with GCN-based zero-shot frameworks~\cite{wang2021graph}. Moreover, their experiments are on a single dataset, and evaluation metrics are not comparable to the standard ones~\cite{xian2018zero}. On the other hand, DiRaC-I can work with any attribute-based data set in practical scenarios. We also evaluate the prediction accuracy of several existing ZSL models trained with seen classes from Existing and Proposed Splits using the standard metrics and obtain encouraging results. 


\section{Problem setting and notations} 
\label{sec:ps-not}

\subsection{Object recognition using zero-shot learning}
\label{sec:3.1}

In a typical zero-shot setting, we have sets $\mathcal{S}$ and $\mathcal{U}$ of $N_{s}$ and $N_{u}$ number of seen and unseen classes respectively, such that $\mathcal{S} \, \cap \, \mathcal{U} = \emptyset$. Let $\mathcal{C} = \mathcal{S} \cup \mathcal{U}$ denote the set of all classes for a given data set. The associated semantic embeddings for these sets can be represented by $\mathcal{P}(\mathcal{S}) \in \mathbb{R}^{N_{s} \times d}$ and $\mathcal{P}(\mathcal{U}) \in \mathbb{R}^{N_{u} \times d}$ respectively, where attributes of a class $c$ are represented by a $d-$dimensional vector $\langle a_c^1, a_c^2,...a_c^d\rangle$. These embeddings are available in several forms like human-annotated attributes~\cite{xian2018zero}, word embeddings like Word2Vec~\cite{mikolov2013distributed} and GloVe~\cite{pennington-etal-2014-glove}, or hierarchical embeddings like WordNet~\cite{miller1995wordnet}. $\mathcal{X}^{s} \in \mathbb{R}^{m \times k}$ and $\mathcal{X}^{u} \in \mathbb{R}^{n \times k}$ represent the visual data for seen and unseen samples respectively, usually available in the form of visual features extracted from a CNN like ResNet-101~\cite{he2016deep,xian2018zero}, pretrained on a large-scale visual dataset like ImageNet~\cite{russakovsky2015imagenet}. $m$ and $n$ denote the number of seen and unseen class samples respectively, with each image being represented by a $k-$dimensional feature vector. Then, given training data $\mathcal{D} = \{(x_{j}^{s}, y_{j}^{s}) \in \mathcal{X}^{s} \times \mathcal{S} \}$ along with $\mathcal{P}(\mathcal{S})$ and $\mathcal{P}(\mathcal{U})$, the task in CZSL is to learn a classifier $f_{czsl} : \mathcal{X}^{u} \rightarrow \mathcal{U}$. In GZSL, a small subset of $\mathcal{X}^s$ ($\mathcal{X}^s_{sub}$) is used as the set of seen samples at test time. Then, the objective changes to learning a classifier $f_{gzsl} : \mathcal{X}^{s}_{sub} \cup \mathcal{X}^{u} \rightarrow \mathcal{S} \cup \mathcal{U}$ to classify both seen and unseen objects. 

\subsection{Practical insights into seen-unseen splits} 
\label{sec:3.2}
Scarcity of labeled data and dealing with unseen concepts are two potential areas where ZSL can contribute significantly in the future when deployed in practical applications. To name a few, with ZSL models: (1) autonomous vehicles~\cite{rezaei2014look,rajasekhar2015autonomous,ishihara2021multi} should be able to recognize unseen {\it concept cars} while driving; (2) previously unseen diseases like COVID-19 could be diagnosed based on their novel characteristics combined with the similarity to other known diseases like asthma~\cite{chen2021deep,rahman2021multimodal,rezaei2020zero}; (3) Autonomous Underwater Vehicles (AUVs) deployed in underwater explorations~\cite{kunz2008deep,kennedy2019unknown} should be able to recognize new fish or coral species if encountered (Fig.~\ref{fig:work_idea}).
However, for a target application, data for only a fixed number of available categories (comprising a set $\Tilde{\mathcal{C}}$) can be collected by the dataset constructors. Although they have access to labeled examples of all the $|\Tilde{\mathcal{C}}|$ classes and can provide them to ZSL researchers, the researchers cannot train their models with all $|\Tilde{\mathcal{C}}|$ classes as they would always need a disjoint set of unseen classes to evaluate ZSL models, as per ZSL criteria. Consequently, training ZSL models requires a subset $\mathcal{S} \subset \Tilde{\mathcal{C}}$ (i.e. the set of seen classes), which DiRaC-I helps the constructors to obtain. Classes from the other subset of the collected dataset($\mathcal{U} = \Tilde{\mathcal{C}} \setminus \mathcal{S}$) can be considered unseen classes by researchers to evaluate their model performance. Finally, the trained model can be deployed in the future to recognize novel classes (with known attributes) {\it in the wild} (Fig.~\ref{fig:work_idea}). 

For zero-shot classification, current researchers use seen-unseen splits predetermined by~\cite{xian2018zero}. However, unlike these {\bf Existing Splits (ES)}, we try to emulate the real-world scenario via our {\bf Proposed Splits (PS)}, where the seen classes exhibiting diversity and rarity can be automatically acquired from $\Tilde{\mathcal{C}}$ itself. For fair comparison of the knowledge gained by existing ZSL models when trained with seen classes from ES and PS, they should be evaluated on the same set of unseen classes. Since we do not have data from classes that are completely unknown to us during experimentation, we extract a few classes from the unseen set originally given by ES, and make them unavailable to both DiRaC-I and the ZSL models during their training. We first dissociate the set $\mathcal{U}$ of ES ($\mathcal{U}$\textsubscript{E}) into two halves randomly --- $\mathcal{U}_{com}$ becomes the set of $N_{u_{com}}$ unseen classes {\it of the wild} and $\Tilde{\mathcal{U}\textsubscript{E}}$ the other half, having $N_{\Tilde{u}}$ classes. Then, the proposed framework acquires seen classes from the set $\Tilde{\mathcal{C}} = \mathcal{S}\textsubscript{E} \, \cup \, \Tilde{\mathcal{U}\textsubscript{E}}$ and ZSL model can train on the acquired classes ($\mathcal{S}$\textsubscript{P}). Figure~\ref{fig:test split} gives a better understanding of this process. Finally, let $\Phi_E = \{ \Phi_{E}^{M_1}, \Phi_{E}^{M_2},... \Phi_{E}^{M_n}\}$ and $\Phi_P = \{ \Phi_{P}^{M_1}, \Phi_{P}^{M_2},...\Phi_{P}^{M_n} \}$ denote the sets of models $M_1,M_2,...M_n$ trained using seen classes from ES and PS respectively. We compare the performance of the models $\Phi_{E}^x$ and $\Phi_{P}^x$ on the test set $\mathcal{U}_{com}$ that is unseen to both $\Phi_{E}^x$ and $\Phi_{P}^x (x = M_1, M_2,...M_n)$. Note that in our framework, classes in $\mathcal{U}_{com}$ do not overlap with the ImageNet 1K classes used for pretraining ResNet-101, following the ZSL assumption provided by~\cite{xian2018zero}. Moreover, PS is not fixed -- since we induce randomness while splitting $\mathcal{U}$\textsubscript{E}, we repeat the entire process (from initializing DiRaC-I to evaluating ZSL models trained with the acquired seen classes) three times so that three different sets of classes are available to our framework at its inception. We show our results in each case, demonstrating the robustness of our framework to the available object domain. 

%for acm tomm
%\section{D\MakeLowercase{i}R\MakeLowercase{a}C-I: Diverse and Rare Class Identifier}

%for arxiv
\section{DiRaC-I: Diverse and Rare Class Identifier}
\label{dirac-i}
In this work, we focus on data sets having homogeneous categories only --- e.g. having all bird categories. For such a data set, we say that its {\it object domain} is birds. Heuristically, training a ZSL model with seen classes that capture both the diversity in the visual space and rarity in the semantic space would provide it with a more generalized idea of the object domain. Hence, the key to our approach is exploring the entire available object domain (defined by classes from $\Tilde{\mathcal{C}}$) for diversity and rarity using a method {\it inspired by} Active Learning. Adopting such a principle enhances the capability of ZSL models for knowledge transfer from the seen to unseen classes during evaluation. Moreover, the novel classes exhibiting rare attributes have a better chance of being recognized accurately, as suggested by the results of our experiments on two benchmark data sets (Tab.~\ref{tab:2}). DiRaC-I consists of two stages, which are discussed in the following sections.

\subsection{Stage 1: Seed-set construction}
\label{sec:seed}

Let $\Tilde{\Psi}_i = \{ \Psi_{i}^1, \Psi_{i}^2...\Psi_{i}^i\}$ denote the set of $i$ clusters, where $\Psi_{i}^j$ denotes the $j^{th}$ cluster of classes represented by their semantic vectors ($j \leq i$) when $i$ clusters are obtained. We run hierarchical agglomerative clustering (HAC) multiple times to decide the optimal number of clusters by evaluating the goodness of clusters in each $\Tilde{\Psi}_i$ as:
\begin{equation} \label{eq:7}
	N_z = \underset{2 \leq \, i \, \leq (N_s + N_{\Tilde{u}} - 1) }{argmax}\; 
	MSC(\Tilde{\Psi}_i)
\end{equation}
where $MSC(.)$ is the {\it mean silhouette coefficient}~\cite{ROUSSEEUW198753,kaufman2009finding}:
\begin{equation}
	\label{eq:8}
	MSC(\Tilde{\Psi}_i) =     \frac{1}{(N_s + N_{\Tilde{u}})}  \sum_{k \in \Tilde{\mathcal{C}}} \frac{b_k^i - a_k^i}{max \{a_k^i, b_k^i \}} 
\end{equation}
where $a_k^i$ and $b_k^i$ are the mean intra-cluster distance and mean nearest-cluster distance for semantic vector of class $k$ when $i$ clusters are formed by HAC. From the optimal set of clusters($\Tilde{\Psi}_{N_z}$), a single representative is selected from each $\Psi_{N_z}^j$ based on information from the cluster-specific semantic space.
%new change
\begin{figure}[t]
	\centering
	\includegraphics[width=\linewidth]{latex/dirac-i.pdf}
	\caption{{\bf DiRaC-I workflow.} In stage 1, clustering in the semantic space selects a representative from each cluster while filtering out irrelevant and unremarkable attributes, and this forms the seed set. In stage 2, the seed set expands iteratively while considering visual diversity and semantic rarity, until it contains a fixed number of classes. The resulting set can act as seen set for training ZSL models}
	\Description{Two stages of our framework accounting for visual diversity and acknowledging semantic rarity in the object domain.}
	\label{fig:dirac-i}
	
\end{figure}
However, for a cluster, some attributes might not be present at all ({\bf irrelevant}) or may occur in minimal amounts ({\bf unremarkable}) and hence can be ignored while searching for its suitable representative. Therefore, for a cluster $\Psi_{N_z}^j$, we formally recognize these two groups of attributes respectively from the semantic space ($\mathcal{P}({\Psi_{N_z}^j})$) spanned by its member classes:
\begin{equation} \label{eq:9}
	IA(\Psi_{N_z}^j) = \{ a^l \in \mathbb{R} \mid a_c^l = 0, \; \forall \; c \in \Psi_{N_z}^j \}
\end{equation}
\begin{equation} \label{eq:10}
	UA(\Psi_{N_z}^j) = \{ a^l \in \mathbb{R} \mid \mathcal{B}_{c}^l(\Psi_{N_z}^j) = 0, \; \forall \; c \in \Psi_{N_z}^j \}
\end{equation}
where for an attribute $a^l$:
\begin{equation}
	\label{eq:11}
	\mathcal{B}_{c}^l(\Psi_{N_z}^j) = 
	\begin{cases}
		0,        &\text{if $a_c^l \leq \frac{1}{| \{c \in \Psi_{N_z}^j | \, a_c^l \neq 0 \} |} \sum \limits_{c \in \Psi_{N_z}^j} a_c^l  $} \\
		1,        &\text{otherwise}
	\end{cases}
\end{equation}
Here, for cluster $\Psi_{N_z}^j$, $IA(.)$ and $UA(.)$ denote the sets of irrelevant and unremarkable attributes respectively, and $\mathcal{B}(.)$ is a binary class-attribute matrix procured from $\mathcal{P}(.)$ after ignoring the irrelevant attributes. Unremarkable attributes are also discarded from both $\mathcal{B}(.)$ and $\mathcal{P}(.)$. To account for the rarity in semantic space $\mathcal{P}({\Psi_{N_z}^j})$, we calculate per-attribute frequencies with the help of the corresponding $\mathcal{B}(\Psi_{N_z}^j)$ --- rarer the attribute, more the importance given to it by sampling weights from the function:
\begin{equation} \label{eq:1}
	f(\theta_{a^l}) = - \log (\theta_{a^l})
\end{equation}
where we obtain attribute frequencies from the diagonal values ($d_l^l$) of matrix $(\mathcal{B}(\Psi_{N_z}^j))^T \cdot \mathcal{B}(\Psi_{N_z}^j)$ as:
\begin{equation}
	\label{eq:6}
	\theta_{a^l} = \frac{d_l^l}{| \Psi_{N_z}^j |}
\end{equation}
According to Eq.~\ref{eq:6}, $\theta \in (0, 1]$ and $f(\theta_{a^l}) \in [0, \infty)$. We use $- \log (\theta_{a^l})$ to sample attribute-weights as it is strictly decreasing on the interval $(0, 1]$, providing a higher weight if $a^l$ is rare (i.e., $\theta_{a^l}$ is low), and a lower weight otherwise. Finally, we get the seed set as $\mathcal{Z} = \{ \kappa(\Psi_{N_z}^1), \kappa(\Psi_{N_z}^2),...\kappa(\Psi_{N_z}^{N_z}) \}$ in which a representative class from each cluster is selected as:
\begin{equation} \label{eq:2}
	\kappa(\Psi_{N_z}^j) = \underset{c \in \Psi_{N_z}^j}{argmax}\;  (\mathcal{B}(\Psi_{N_z}^j)\odot \mathcal{P}(\Psi_{N_z}^j)) \cdot \mathcal{W}
\end{equation}
where $\odot$ denotes element-wise matrix multiplication and $\mathcal{W}$ is a vector of weights for attributes present in $\mathcal{P}(.)$. Such representatives from different clusters boost diversity while promoting semantic rarity via Eqs.~\ref{eq:1} and~\ref{eq:2}. We consider an outlier class (not a member of any cluster) to be diverse enough from the other classes and take it directly into the seed set.



\subsection{Stage 2: Visual-Semantic Mining (VSM)}
\label{sec:vsm}
The samples belonging to classes from $\mathcal{Z}$ act as a labeled data set used to initialize our VSM algorithm. VSM is inspired by Active Learning (AL), where inputs from an {\it Oracle} (the source of ground truth labels, e.g. a human expert) are used to label some of the most informative samples from an unlabeled pool ($\mathcal{A}$) to train machine learning models. For our framework, this pool corresponds to the samples not belonging to classes from the seed set (for a given VSM iteration only; dataset constructors actually provide labels for all samples available to DiRaC-I). We aim to adopt a similar strategy to iteratively acquire $N_s$ classes exhibiting diversity and rarity for training ZSL models.

In each iteration of VSM, we retrain a ResNet-101 ($\mathcal{M}$)~\cite{he2016deep} pretrained on ImageNet~\cite{russakovsky2015imagenet} to behave as a feature extractor for the seed class samples using a transfer learning approach. We capitalize on the work done by~\cite{bendale2016towards} and use the scores from the penultimate layer of a CNN (Activation Vectors or AVs) to estimate the distribution of the {\it related} classes, establishing a relationship between the unlabeled and labeled samples in the AV space. Each class $c \in \mathcal{Z}$ is represented by its {\bf Mean Activation Vector (MAV)} computed using the AVs of the training samples classified correctly by $\mathcal{M}$, obtaining the MAV set $\mathcal{V} = \{ \mu_1, \mu_2,...\mu_{| \mathcal{Z} |} \}$. For the unlabeled samples, we extract the AVs using the trained $\mathcal{M}$ to obtain $\mathcal{F} = \{ f_1, f_2,...f_{| \mathcal{A} |} \}$. We intend to capture the visually most diverse samples leveraging the AV space by first obtaining the set:
\begin{equation}
	\label{eq:12}
	\Pi = \{ k \in \mathbb{R} \mid k = \underset{\mu_c \in \mathcal{V}}{min} \; \delta(\mu_c, f_j), \; \forall f_j \in \mathcal{F} \}
\end{equation}
and then selecting $t$ samples from $\mathcal{A}$ corresponding to the largest values in $\Pi$, where $\delta$ denotes the Euclidean-cosine distance~\cite{bendale2016towards}. A set of unique {\it candidate classes} ($\mathcal{H}$) is formed by querying the ground truths of these $t$ samples. VSM then explores the rarity in the semantic space spanned by these candidate classes ($\mathcal{P}(\mathcal{H})$). A class-wise estimate of the number of images from the seed classes exhibiting each attribute can be obtained in a matrix $\mathcal{I}$, where:
\begin{equation}
	\label{eq:13}
	\mathcal{I}_c^l = a_c^l \cdot IC_c, \; c \in \mathcal{Z}
\end{equation}
Here, $a_c^l$ is an element from the semantic space of the seed classes ($\mathcal{P}(\mathcal{Z})$) and $IC_c$ gives the number of images for seed class $c$. New attribute-weights are calculated based on the proportion of each attribute within the currently ``known object domain ($\mathcal{Z}$)'' for VSM using Eq.~\ref{eq:1}, except that now:
\begin{equation}
	\label{eq:4}
	\theta_{a^l} = \frac{\sum \limits_{c \in \mathcal{Z}} \mathcal{I}_c^l}{\sum \limits_{c \in \mathcal{Z}} IC_c}
\end{equation}
Finally, we calculate {\bf semantic scores} for each candidate class as follows:
\begin{equation}
	\alpha_h = \mathcal{P}(\mathcal{H}) \cdot \mathcal{W}
	\label{eq:5}
\end{equation}
where $\mathcal{W}$ denotes the vector of obtained attribute-weights. Top-$q$ candidate classes having the highest semantic scores are added to $\mathcal{Z}$. Ground truth of the samples from the added classes are also queried at the end of an iteration so that samples belonging to classes in $\mathcal{Z}$ always remain labeled. We repeat this entire process (Fig.~\ref{fig:dirac-i}) until $| \mathcal{Z} | = N_s$ (kept as the same value as in Existing Split for a fair comparison).

It is important to note that VSM is {\bf only inspired by} Active Learning (AL). We do acknowledge the structural similarities with AL, such as seeds and acquisition functions. However, the problem setup and goal of VSM are quite different from AL. AL theoretically aims to select the most informative samples from a huge unlabeled pool of data, whereas that is not the case for DiRaC-I’s target group (dataset constructors). DiRaC-I can query the labels once a class is added to the seed set since labeled data for all the collected classes are available with the constructors. Hence, for this task, the AL assumption (model should not have access to labels) does not hold, and its absence does not make VSM impractical to use.


\section{Experiments} \label{expt}

\subsection{Datasets and seen-unseen splits}
\label{sec:5.1}
{\bf CUB}~\cite{WahCUB_200_2011} and {\bf SUN}~\cite{patterson2014sun} are two challenging fine-grained data sets, both with several classes but limited data per class. CUB contains 11,788 images from 200 bird categories, each of which is defined using 312 human-annotated attributes. SUN contains 14,340 images from 717 scene categories annotated with 102 attributes. $N_s$ is 150 for CUB and 645 for SUN for both ES and PS. Moreover, before initiating DiRaC-I, we obtain $N_{u_{com}}$ as 25 and 36 for CUB and SUN. The random split of $\mathcal{U}\textsubscript{E}$ is done three times and model evaluation is done on three different $\mathcal{U}_{com}$ sets --- $\mathcal{U}_{com}^1$, $\mathcal{U}_{com}^2$ and $\mathcal{U}_{com}^3$ --- where $X$ in $\mathcal{U}_{com}^X$ denotes the split number. Consequently, DiRaC-I runs three times with different object domains ($\Tilde{\mathcal{C}}$) at its inception. We report the image count of classes belonging to sets $\mathcal{S}\textsubscript{E}$, $\mathcal{S}\textsubscript{P}$ and $\mathcal{U}_{com}$ in Tab.~\ref{tab:1}, where the slight difference in image count for ES and PS can be attributed to the different seen classes considered in ES and PS. For visual features, we follow previous work~\cite{xian2018zero} and use CNN features extracted from pretrained ResNet-101~\cite{he2016deep}.

\subsection{Implementation details}
\label{sec:5.2}
During stage 1, HAC uses {\it Ward's method}~\cite{ward1963hierarchical} to calculate cluster similarity. Obtaining too few clusters (and hence, seed classes) using HAC would initialize the deep model ($\mathcal{M}$) in VSM with too few training samples. Additionally, some data sets have very few images per class --- e.g. 20 for SUN. Therefore, we set the lower bound of number of clusters to be formed as 5 to achieve effective model training. While retraining $\mathcal{M}$, weights of all the layers are frozen except the last fully-connected layer. The learning rates for optimizing $\mathcal{M}$ are set to 0.01 and 0.001 for CUB and SUN, respectively. $q$ is set to 2 for CUB and 4 for SUN. We need $t$ to be low so that in a practical scenario, only a small percentage of the unlabeled images from $\mathcal{A}$ need to be queried for their class labels while inferring the candidate classes during the entire process of VSM. In our experiments, $t = \max (5, \lceil(3 \log a) \rceil)$, where $a = $ average number of images per class for a given data set, according to which $t = 13$ for CUB and $t = 9$ for SUN. Across the three runs of DiRaC-I, VSM runs for 58 iterations on an average for CUB and queries the labels for 754 samples, i.e., 6.39\% of the total samples in CUB. For SUN, labels are queried for 10.04\% of the total samples (1440) over an average of 160 iterations. Furthermore, keeping in mind the real-life scenarios, we prioritize a candidate class to be included in $\mathcal{Z}$ if it is an overlapping class (Sec.~\ref{sec:3.2}), so that classes from the set $\Tilde{C} \setminus \mathcal{S}\textsubscript{P}$ can also serve as test set if required without violating zero-shot assumptions~\cite{xian2018zero}. We observe that these classes mostly have the highest semantic scores too, and hence conclude that the inclusion is fair. 


\begin{table}[t]
	\begin{center}
		\caption{Image count in seen and common unseen classes for ES and PS for different random splits of $\mathcal{U}\textsubscript{E}$}
		\label{tab:1}      
		\begin{tabular}{lccccc}    
			\hline
			\multirow{2}{*}{\textbf{Dataset}}   & 
			\multirow{2}{*}{\textbf{Split}}  & \multicolumn{2}{c}{\textbf{ES}} & \multicolumn{2}{c}{\textbf{PS}}
			\\ 
			
			& & $\boldsymbol{\mathcal{S}\textsubscript{E}}$ & $\boldsymbol{\mathcal{U}_{com}}$ & $\boldsymbol{\mathcal{S}\textsubscript{P}}$ & $\boldsymbol{\mathcal{U}_{com}}$\\
			\hline
			
			\multirow{3}{*}{CUB} 
			&1 &7057 &1489 &7068 &1489 \\
			&2 &7057 &1488 &7068 &1488 \\
			&3 &7057 &1471 &7075 &1471 \\
			
			\hline
			
			\multirow{3}{*}{SUN} 
			&1 &10320 &720 &10320 &720 \\
			&2 &10320 &720 &10320 &720 \\
			&3 &10320 &720 &10320 &720 \\
			
			\hline
			
		\end{tabular}
	\end{center}
\end{table}




\begin{table}[t]
	\begin{center}
		\caption{Comparative results (top-1 accuracy in \%) of Conventional ZSL with models from sets $\Phi_E$ and $\Phi_P$ (defined in Sec.~\ref{sec:3.2}) on the CUB and SUN data sets. Results on test classes having at least one common attribute corresponds to results on all classes (left), since all classes in CUB and SUN exhibit at least one common attribute (see Tab.~\ref{tab:3}). Enhanced results achieved with PS are in {\bf BOLD}}
		\label{tab:2}    
		\begin{tabular}{lccccc|cccc}
			\hline
			\multirow{4}{*}{{\bf Method}}   & 
			\multirow{4}{*}{{\bf Test set}}  &
			\multicolumn{4}{c}{{\bf ZSL for all}} &
			\multicolumn{4}{c}{{\bf ZSL for classes having}} \\
			
			& & \multicolumn{4}{c}{{\bf test classes}} &
			\multicolumn{4}{c}{{\bf at least 1 rare attribute}} 
			\\ 
			
			& & \multicolumn{2}{c}{{\bf CUB}} & \multicolumn{2}{c}{{\bf SUN}} & \multicolumn{2}{c}{{\bf CUB}} &
			\multicolumn{2}{c}{{\bf SUN}} 
			\\
			& & {\bf ES} & {\bf PS} & {\bf ES} & {\bf PS} & {\bf ES\textsubscript{R}} & {\bf PS\textsubscript{R}} & {\bf ES\textsubscript{R}} & {\bf PS\textsubscript{R}} \\
			\hline
			
			\multirow{3}{*}{\textbf{ALE} \cite{akata2015label}} 
			&$\mathcal{U}_{com}^1$ &47.85 &\textbf{50.76} &61.25 &59.17 &46.58 &\textbf{49.89} &63.63 &60.45 \\
			&$\mathcal{U}_{com}^2$ &43.76 &\textbf{47.62} &63.19 &59.86 &41.32 &\textbf{48.36} &61.84 &58.68 \\
			&$\mathcal{U}_{com}^3$ &48.06 &\textbf{56.10} &60.28 &57.92 &47.99 &\textbf{54.24} &61.66 &\textbf{63.33} \\
			
			\hline
			
			
			\multirow{3}{*}{\textbf{SAE} \cite{kodirov2017semantic}} 
			&$\mathcal{U}_{com}^1$ &40.69 &\textbf{44.07} &47.78 &\textbf{53.19}  &39.26 &\textbf{42.85} &52.5 &\textbf{59.54}\\
			&$\mathcal{U}_{com}^2$ &32.07 &\textbf{39.64} &54.17 &\textbf{55.69} &30.30 &\textbf{40.12} &53.68 &\textbf{55.26}\\
			&$\mathcal{U}_{com}^3$ &40.41 &\textbf{43.64} &50.69 &\textbf{52.92} &40.42 &\textbf{44.11} &50.00 &\textbf{55.66}\\
			
			\hline
			
			\multirow{3}{*}{\textbf{SJE} \cite{akata2015evaluation}} 
			&$\mathcal{U}_{com}^1$ &48.01 &\textbf{50.74} &53.19 &53.06  &47.16 &\textbf{49.37} &54.31 &\textbf{55.90} \\
			&$\mathcal{U}_{com}^2$ &40.26 &\textbf{47.98} &53.19 &\textbf{55.42} &38.32 &\textbf{47.32} &51.57 &\textbf{56.57}\\
			&$\mathcal{U}_{com}^3$ &54.05 &\textbf{54.81} &51.67 &50.28 &48.20 &\textbf{48.72} &55.00 &53.66 \\
			
			\hline
			
			\multirow{3}{*}{\textbf{DeViSE} \cite{frome2013devise}} 
			&$\mathcal{U}_{com}^1$ &47.18 &\textbf{50.27} &53.89 &\textbf{54.31} &45.46 &\textbf{48.96} &57.04 &\textbf{60.22}\\
			&$\mathcal{U}_{com}^2$ &42.55 &\textbf{45.63} &58.47 &55.00  &38.88 &\textbf{45.56} &56.31 &56.31 \\
			&$\mathcal{U}_{com}^3$ &44.58 &\textbf{45.09 }&55.14 &54.17 &41.32 &\textbf{43.15} &62.00 &\textbf{66.00}\\
			
			\hline
			
			\multirow{3}{*}{\textbf{ESZSL} \cite{romera2015embarrassingly}} 
			&$\mathcal{U}_{com}^1$ &53.62 &52.37 &53.89 &49.31 &52.79 &51.01 &59.54 &56.81 \\
			&$\mathcal{U}_{com}^2$ &44.46 &\textbf{50.80} &55.42 &52.78 &44.38 &\textbf{52.27} &57.36 &53.15\\
			&$\mathcal{U}_{com}^3$ &56.81 &\textbf{59.84} &57.78 &50.83 &51.63 &\textbf{55.96} &62.66 &54.00\\
			
			\hline
			
			\multirow{3}{*}{\textbf{LsrGAN} \cite{vyas2020leveraging}} 
			&$\mathcal{U}_{com}^1$ &57.91 &\textbf{60.19} &59.44 &\textbf{64.58} &57.61 &\textbf{59.64} &62.72 &\textbf{65.90}\\
			&$\mathcal{U}_{com}^2$ &56.03 &\textbf{59.40} &61.67 &\textbf{64.17} &56.01 &\textbf{61.28} &61.57 &\textbf{63.94}\\
			&$\mathcal{U}_{com}^3$ &64.43 &61.46 &60.69 &\textbf{61.81} &60.54 &57.84 &63.66 &\textbf{66.00}\\
			
			\hline
			
			\multirow{3}{*}{{\bf TF-VAEGAN} \cite{narayan2020latent}} 
			&$\mathcal{U}_{com}^1$ &63.48 &\textbf{66.32} &64.58 &\textbf{65.83} &63.68 &\textbf{65.89} &65.00 &\textbf{67.04}\\
			&$\mathcal{U}_{com}^2$ &61.82 &\textbf{64.98} &69.03 &66.11 &60.55 &\textbf{65.13} &68.15 &67.10 \\
			&$\mathcal{U}_{com}^3$ &67.56 &\textbf{68.35} &65.83 &65.00 &64.44 &\textbf{65.06} &67.66 &\textbf{70.33} \\
			
			
			\hline
		\end{tabular}
	\end{center}
\end{table}

\subsection{Performance comparison with Existing Splits} \label{sec:5.3} 
We report the average per-class top-1 accuracy~\cite{xian2018zero} in the CZSL setting to evaluate ZSL methods based on different approaches like compatibility learning and generative frameworks and compare the performance of $\Phi_E$ and $\Phi_P$ (Sec.~\ref{sec:3.2}). All the methods are implemented in PyTorch. Among them, official codes in PyTorch are available for LsrGAN~\cite{vyas2020leveraging} and TF-VAEGAN~\cite{narayan2020latent}, and the rest are re-implemented versions based on the original publications. The hyperparameters for the official codes are directly used, whereas they are set on the validation sets for the rest. Note that model accuracy on ES reported in Tab.~\ref{tab:2} is not comparable with that of the original papers because the test set is different in our framework. Table~\ref{tab:2} shows that in the CZSL setting, models in $\Phi_P$ show significant improvements on CUB over $\Phi_E$ across all three splits. For SUN, we notice a mix of improved and similar results with models in $\Phi_E$. This might be because DiRaC-I leverages information from the semantic space, which lacks attributes with discriminative strength in the case of SUN, as explained in Sec.~\ref{sec:6.1}. Comparing results for ES and PS in the GZSL setting would not be fair in the current work because the seen classes in ES and PS might be different. Consequently, they might have different influences on the unseen predictions and the harmonic mean of seen and unseen accuracy (GZSL evaluation metric). This is true for most models because of their bias towards the seen classes.

\section{Framework analysis}\label{fr-ana}
In this section, we demonstrate some qualitative and quantitative results and analyze the performance of the two stages of our framework, as well as the impact of incorporating diversity and rarity of the object domain in the training of ZSL models. In some of the subsections that follow, we show some qualitative results on the {\bf CUB}~\cite{WahCUB_200_2011} data set when $\Tilde{\mathcal{C}} = \mathcal{S}\textsubscript{E} \, \cup \, (\mathcal{U}\textsubscript{E} \setminus \mathcal{U}_{com}^2)$ ($\mathcal{U}_{com}^2$ denotes the set $\mathcal{U}_{com}$ after the randomly splitting $\mathcal{U}\textsubscript{E}$ for the second time). For brevity, we denote this set as $\Tilde{\mathcal{C}^2}$.
All the results are shown for the object domain $\Tilde{\mathcal{C}^2}$ (unless stated otherwise) to maintain a correlation between results obtained from various stages of the framework. We choose the CUB dataset for qualitative results because the attributes that characterize each class in CUB are visually interpretable and hence can be easily verified with the visual results we provide here.

\begin{figure}[t]
	\centering
	\includegraphics[width=\linewidth]{latex/cubclus_all.pdf}
	
	\caption{Results of clustering during seed-set construction (for object domain $\Tilde{\mathcal{C}^2}$). Each row corresponds to the members of a specific cluster obtained. The attribute descriptions for each cluster written below each row are formed by combining some of the most frequent attributes, procured after discarding the attributes adjudged as \textit{irrelevant} and \textit{unremarkable} for the cluster (such discarded attributes can be found in Tab.~\ref{tab:ia_ua}) }
	\Description{Verifying the visual images within clusters with their corresponding semantic descriptions}
	\label{fig:clusters_with_att}
\end{figure}

\subsection{Seed-set construction} \label{sec:6.1} 
%Clustering the classes in $\Tilde{\mathcal{C}}$ brought together the semantically related classes. 
We aim to achieve a good quality of clusters during this stage and acquire seed classes that provide a comprehensive initial idea of the object domain to the next stage of DiRaC-I. For CUB, obtained clusters are visually more interpretable since the semantic space consists of several {\it groups} of discriminative properties like {\it wing color, bill shape, head pattern}, etc. Hence, {\it hummingbirds, kingfishers, and gulls} get clustered separately, and picking a representative from each cluster captures the object domain diversity well enough. However, the SUN attributes come from a variety of contexts~\cite{patterson2014sun}, many of which are applicable to several classes with the same attribute strength --- e.g. $\mathcal{P}(\mathcal{S}\textsubscript{E} \cup (\mathcal{U}\textsubscript{E} \setminus \mathcal{U}_{com}^1))$ shows attributes {\it warm} and {\it eating} have a non-zero value for 669 and 346 classes but have only 50 and 49 unique values. This results in a large number of classes clustering together in the semantic space (Fig.~\ref{fig:semantic_vsm}). Hence, our experiments suggest that data sets which characterize classes using more discriminative attribute strengths would help in selecting better seed classes. Figures~\ref{fig:semantic1_CUB} and~\ref{fig:semantic1_SUN} show the seed classes (numbered in `black') for CUB and SUN respectively. Randomly selecting such classes could pick all of them from a particular region (when every class is semantically very similar) or from very different regions. However, our approach leverages the semantic relationships between classes and ensures that it picks diverse representatives, as evident from Fig.~\ref{fig:semantic_vsm}.


\begin{figure}[t]
	\centering
	\includegraphics[width=0.5\linewidth]{latex/seeds0_new.pdf}
	
	\caption{An example from every seed class at the end of stage 1 for CUB with object domain $\Tilde{\mathcal{C}^2}$. Notice the birds exhibiting rare attributes of the domain $\Tilde{\mathcal{C}^2}$ (see Tab.~\ref{tab:rare_common}) such as needle-shaped bill (pink box) and orange eye (green box)}
	\Description{Prioritizing rare attributes to identify object domain}
	\label{fig:seeds0}
\end{figure}




\begin{table}[t]
	\begin{center}
		\caption{Sets of irrelevant attributes ($IA$) and unremarkable attributes ($UA$) obtained for the clusters corresponding to the top, middle and bottom rows of Fig.~\ref{fig:clusters_with_att}, respectively. $\Psi^c_{25}$ denotes cluster number $c$ out of the 25 clusters obtained using HAC. (+$p$) indicates set contains $p$ more attributes}
		\label{tab:ia_ua}
		\begin{tabular}{cll}
			\hline
			\multirow{1}{*}{{\bf Cluster (c)}}   & 
			\multirow{1}{*}{\bf $\boldsymbol{IA(\Psi_{25}^c)}$}   & 
			\multirow{1}{*}{\bf $\boldsymbol{UA(\Psi_{25}^c)}$}  \\
			
			
			\hline
			
			\multirow{5}{*}{1} 
			&Purple bill &Yellow back \\
			&Olive bill &Purple eye \\
			&Green bill &Pink forehead \\
			&Rufous crown &Orange nape \\
			& &Green leg (+14) \\
			\hline
			
			\multirow{5}{*}{2} 
			&Spatulate-shaped bill &Purple wing \\
			&Green wing &Green throat \\
			&Purple leg &Blue under-tail \\
			&Olive crown &Red nape \\
			&Pink eye (+19) &Blue belly (+24) \\
			\hline
			
			\multirow{5}{*}{3} 
			&Purple back &Orange wing \\
			&Pink underparts &Red upper-tail \\
			&Green upper-tail &Brown eye \\
			&Red forehead &Blue nape \\
			&Olive breast (+57) &Pink bill (+53) \\
			
			
			\hline
		\end{tabular}
	\end{center}
\end{table}


\begin{figure}[t]
	\centering
	\includegraphics[width=0.45\linewidth]{latex/hyperparam.pdf}
	\caption{Sensitivity to $q$ for TF-VAEGAN~\cite{narayan2020latent}}
	\Description{Number of classes selected each time do not hamper model accuracy too much}
	\label{fig:hyperparam}
\end{figure}

\subsubsection{Clustering in the semantic space} \label{sec:seed.1}
Hierarchical Agglomerative Clustering (HAC) in the semantic space spanned by classes from  set $\Tilde{\mathcal{C}^2}$ provides 25 clusters, and a single representative is designated as the seed class from each cluster. Fig.~\ref{fig:clusters_with_att} elucidates the clustering quality by presenting the cluster members of three clusters found by HAC. It can be seen that all the cluster members of the bottom row belong to the family of {\it terns} --- hence picking a seed class from this cluster ensures that the final seed set at the end of seed-set construction has a member from this family of birds. For the other rows, although the cluster members come from several families, they share common visual properties. For example, {\it crows, cormorants, blackbirds} and others have been clustered together in the top row, whereas the middle row consists of small-sized birds like {\it sparrows, finches,} etc. This suggests that selecting a member from each cluster would provide a good visual representation of that cluster to the next stage (VSM). Fig.~\ref{fig:seeds0} shows a sample image from each of the seed classes of CUB dataset corresponding to Fig.~\ref{fig:semantic1_CUB} with object domain $\Tilde{\mathcal{C}^2}$, providing a visual idea of the domain diversity captured within the seed classes.

\subsubsection{Designating cluster representatives as seeds}
Once the clusters are obtained, seed classes are selected based on the cluster-specific semantic space using Eqs.~\ref{eq:1},~\ref{eq:6}, and~\ref{eq:2}. However, to ensure that the computations are devoid of the effects of {\it irrelevant} and {\it unremarkable} attributes of a cluster, we defined sets $IA(.)$ and $UA(.)$ for every cluster. Table~\ref{tab:ia_ua} shows these two sets obtained corresponding to the three clusters (out of 25) exhibited in Fig.~\ref{fig:clusters_with_att}. Combining the information from Fig.~\ref{fig:clusters_with_att} and Tab.~\ref{tab:ia_ua}, we can see that the attributes belonging to the obtained sets $IA(.)$ and $UA(.)$ are indeed not descriptive enough of the cluster members. However, associating some of the most frequently occurring attributes in a given cluster, we construct some cluster descriptions (Fig.~\ref{fig:clusters_with_att}) and find them to be consistent with the visual images of the cluster members, providing a general description of the cluster.


%In Sec.~6.4 of the main paper, we devised a criteria to designate attributes as {\it rare} or {\it common} in order to understand the impact of acknowledging rarity on the domain knowledge gained by zero-shot models. While testing the effectiveness of DiRaC-I when exposed to different object domains at its inception, we use these criteria to figure out the rare and common attributes, and report a few rare and common attributes for each object domain (i.e. $\Tilde{\mathcal{C}^1}, \Tilde{\mathcal{C}^2}$ and $\Tilde{\mathcal{C}^3}$) in Tab.~\ref{tab:rare_common}. Thereafter, looking back at the seed classes acquired from $\Tilde{\mathcal{C}^2}$ at the end of stage 1 (Fig.~\ref{fig:seeds0}), we find that our seed-set construction process indeed picks an initial seed set which is not only diverse enough, but also captures the rarity from the semantic space of the object domain. For example, Fig.~\ref{fig:seeds0} shows birds exhibiting {\it needle-shaped bill} and {\it orange eye} (in colored boxes), which are rare attributes considering the domain $\Tilde{\mathcal{C}^2}$ (see Tab.~\ref{tab:rare_common}).




\setlength{\tabcolsep}{1.4pt}

\begin{figure}[t]
	\centering
	\begin{subfigure}{0.7\textwidth}
		\includegraphics[scale = 0.18]{latex/u_split2_CUB_vsm_iter1.pdf}
		\caption{After iteration 1}
		\label{fig:vsm1}
	\end{subfigure}
	
	\begin{subfigure}{0.7\textwidth}
		\includegraphics[scale = 0.18]{latex/u_split2_CUB_vsm_iter2.pdf}
		\caption{After iteration 2}
		\label{fig:vsm2}
	\end{subfigure}
	
	\begin{subfigure}{0.7\textwidth}
		\includegraphics[scale = 0.18]{latex/u_split2_CUB_vsm_iter3.pdf}
		\caption{After iteration 3}
		\label{fig:vsm3}
	\end{subfigure}
	
	\begin{subfigure}{0.7\textwidth}
		\includegraphics[scale = 0.18]{latex/u_split2_CUB_vsm_iter4.pdf}
		\caption{After iteration 4}
		\label{fig:vsm4}
	\end{subfigure}
	
	
	
	
	\caption{Top $t$ samples visually most diverse from the seed classes at the start of iteration $i$, based on a Euclidean-cosine distance~\cite{bendale2016towards}}
	\Description{Visual diversity captured within the object domain of birds}
	\label{fig:vsm_candidates}
\end{figure}




\begin{figure}[t]
	\centering
	\begin{subfigure}[t]{0.4\textwidth}
		\includegraphics[scale=0.2]{latex/u_split2_CUB_tsne_clusters_q1.png}
		\caption{CUB, $i_1$}
		\label{fig:semantic1_CUB}
	\end{subfigure} \hfil
	\begin{subfigure}[t]{0.4\textwidth}
		\includegraphics[scale=0.2]{latex/u_split2_CUB_tsne_clusters_q2.png}
		\caption{CUB, $i_2$}
		\label{fig:semantic2_CUB}
	\end{subfigure} \hfil
	\begin{subfigure}[t]{0.4\textwidth}
		\includegraphics[scale=0.2]{latex/u_split2_CUB_tsne_clusters_q3.png}
		\caption{CUB, $i_3$}
		\label{fig:semantic3_CUB}
	\end{subfigure} \hfil
	\begin{subfigure}[t]{0.4\textwidth}
		\includegraphics[scale=0.2]{latex/u_split2_CUB_tsne_clusters_q4.png}
		\caption{CUB, $i_4$}
		\label{fig:semantic4_CUB}
	\end{subfigure}
	
	\begin{subfigure}[t]{0.4\linewidth}
		\includegraphics[scale=0.2]{latex/u_split1_SUN_tsne_clusters_q1.png}
		\caption{SUN, $i_1$}
		\label{fig:semantic1_SUN}
	\end{subfigure}
	\hfil
	\begin{subfigure}[t]{0.4\linewidth}
		\includegraphics[scale=0.2]{latex/u_split1_SUN_tsne_clusters_q2.png}
		\caption{SUN, $i_2$}
		\label{fig:semantic2_SUN}
	\end{subfigure}
	\hfil
	\begin{subfigure}[t]{0.4\linewidth}
		\includegraphics[scale=0.2]{latex/u_split1_SUN_tsne_clusters_q3.png}
		\caption{SUN, $i_3$}
		\label{fig:semantic3_SUN}
	\end{subfigure}
	\hfil
	\begin{subfigure}[t]{0.4\linewidth}
		\includegraphics[scale=0.2]{latex/u_split1_SUN_tsne_clusters_q2.png}
		\caption{SUN, $i_4$}
		\label{fig:semantic4_SUN}
	\end{subfigure}
	\caption{Visualization of the classes in the semantic space acquired during different iterations of VSM for both CUB and SUN by t-SNE method~\cite{van2014accelerating} (best viewed in color). Each class is represented by its attribute vector in the semantic space, and classes in the same cluster are shown in the same color. The top $q$ classes acquired in the $k^{th}$ iteration are marked with `red' numbers, and the rest of the numbered classes are the seed classes before starting iteration named $i_k$}
	\Description{A representation of how classes spanning various regions of the semantic space are captured in each VSM iteration}
	\label{fig:semantic_vsm}
\end{figure}

\subsection{Visual-Semantic Mining (VSM)} \label{sec:6.2} 
The idea of capturing diversity and rarity in the object domain via an iterative VSM algorithm is pivotal to our work and has been shown in action in Fig.~\ref{fig:semantic_vsm}. We notice that classes are captured from several regions of the semantic space, maximizing the distance from the existing seed classes in most cases. In a few cases, the acquired classes are closer to quite a few existing seed classes, like classes labeled as 29 in Fig.~\ref{fig:semantic4_CUB} and 8 in Fig.~\ref{fig:semantic3_SUN}. These cases arise when the generated semantic scores exceed the visual diversity factor (Eq.~\ref{eq:12}) by virtue of the rarity of attributes.  

\subsubsection{Qualitative analysis: VSM}
\label{sec:vsm-qual}
Figure~\ref{fig:vsm_candidates} shows the $t$ samples for the first four VSM iterations considered visually most diverse from the existing seed classes at the start of every iteration. For CUB data set, $t = 13$ (Sec.~\ref{sec:5.2}). New classes added to the initial seed set at every iteration have been shown in Fig.~\ref{fig:vsm_seen}, where classes at the end of iteration $i$ serve as the seed classes at the start of iteration $i+1$ (for $i = 1,2,3$). The initial 25 seed classes (Fig.~\ref{fig:seeds0}) are used for acquiring new classes in iteration 1 of VSM. 

\begin{figure}
	\centering
	\begin{subfigure}{0.4\textwidth}
		\includegraphics[scale=0.4]{latex/seeds1.pdf}
		\caption{After iteration 1}
		\label{fig:seeds1}
	\end{subfigure}
	\hfil
	\begin{subfigure}{0.4\textwidth}
		\includegraphics[scale=0.4]{latex/seeds2.pdf}
		\caption{After iteration 2}
		\label{fig:seeds2}
	\end{subfigure}
	
	
	\begin{subfigure}{0.4\textwidth}
		\includegraphics[scale=0.4]{latex/seeds3.pdf}
		\caption{After iteration 3}
		\label{fig:seeds3}
	\end{subfigure}
	\hfil
	\begin{subfigure}{0.4\textwidth}
		\includegraphics[scale=0.4]{latex/seeds4.pdf}
		\caption{After iteration 4}
		\label{fig:seeds4}
	\end{subfigure}
	
	\caption{New classes (in red boxes) added to the initial seed set (Fig.~\ref{fig:seeds0}) after the first four iterations of VSM. Corresponding results in the semantic space can be found in Fig.~\ref{fig:semantic_vsm}. Visual diversity can be observed as representatives of various families like {\it sparrows}, {\it albatrosses}, {\it cormorants}, {\it kingfishers} and others have been acquired by VSM. Rare attributes like {\it purple underparts}, {\it needle-shaped bill} and others have also been captured within these classes}
	\Description{Capturing diversity and rarity every iteration}
	\label{fig:vsm_seen}
\end{figure}







For iteration 1, Fig.~\ref{fig:vsm1} presents different kinds of {\it swallows, albatrosses} and {\it sparrows} which are different from the birds captured in the seed set. At the start of iteration 1, the top-5 rare and common attributes captured from the existing seed classes are shown in Tab.~\ref{tab:vsm_rare_common}. These lists are obtained according to the fraction of seed class images that the attributes appear in, and hence can approximately be verified from the visual images of the seed classes (Fig.~\ref{fig:seeds0}). The added classes after iteration 1 are {\it Sooty albatross} and {\it Dark-eyed junco}. Referring to the semantic vectors for these two classes, we find that both of them marginally exhibit the top-5 rare attributes, except {\it green leg}. Moreover, as expected, both the added classes exhibit huge amounts of some of the top-5 common attributes like {\it black eye} and {\it solid belly pattern}. After adding these new classes to the previous seed set, the list of top-5 rare attributes changes in the second iteration of VSM, indicating that the seed set has now been enriched with rare attributes. The list of top-5 common attributes remains mostly the same, as these attributes are already the most abundant ones in the object domain.

\begin{table}[t]
	\begin{center}
		\caption{The top five rare and common attributes captured by analyzing the semantic space of the classes in the seed set at the start of the first four iterations of VSM. For each iteration, the attributes are shown in descending order of weights assigned to them, computed using Eqs.~\ref{eq:6},~\ref{eq:13}, and~\ref{eq:4}}
		\label{tab:vsm_rare_common}
		\begin{tabular}{lll}
			\hline
			\multirow{1}{*}{{\bf Iteration}}   & 
			\multirow{1}{*}{{\bf Top Rare}}   & 
			\multirow{1}{*}{{\bf Top Common}}  \\
			
			% {\bf domain} &{\bf attributes} &{\bf attributes} \\
			
			\hline
			
			\multirow{5}{*}{1}
			&Purple under-tail &Small size \\
			&Primarily purple &Solid belly pattern \\
			&Purple nape  &Bill shorter than head\\
			&Green leg &Solid breast pattern \\
			&Purple underparts &Black eye \\
			\hline
			\multirow{5}{*}{2}
			&Pink eye &Small size \\
			&Purple breast &Bill shorter than head \\
			&Green leg &Solid breast pattern\\
			&Purple under-tail &Solid belly pattern \\
			&Green bill &Black eye \\
			\hline
			
			\multirow{5}{*}{3}
			&Pink eye &Small size \\
			&Purple eye &Bill shorter than head  \\
			&Green leg &Solid breast pattern \\
			&Purple under-tail &Solid belly pattern \\
			&Pink under-tail &Black eye\\
			
			\hline
			
			\multirow{5}{*}{4}
			&Owl-like shape &Small size \\
			&Pink eye &Bill shorter than head  \\
			&Purple eye &Solid breast pattern \\
			&Purple under-tail &Solid belly pattern \\
			&Green leg &Black eye\\
			
			
			\hline
		\end{tabular}
		
	\end{center}
\end{table}




\subsection{Parameter sensitivity} \label{sec:6.3}
During VSM, attribute-weights are computed based on the semantics of classes in $\mathcal{Z}$ only (Eq.~\ref{eq:4}). Diversity and rarity expressed by such a small portion of the object domain should not dictate the selection of too many classes at a time. Figure~\ref{fig:hyperparam} suggests that $q$ is not very sensitive to ZSL model performance, so we set low values of $q$ for VSM to steadily explore the object domain while expanding the set $\mathcal{Z}$. Since the average image count per class for SUN is relatively lower than CUB, we set $q$ to be higher for SUN to train the feature extractor effectively. 







\begin{figure}
	\centering
	\begin{subfigure}{0.34\textwidth}
		\includegraphics[scale=0.111]{latex/u_split1_CUB_rare_accs_DEVISE.pdf}
		\caption{CUB, DeViSE}
		\label{fig:adcub}
	\end{subfigure}
	\hfil
	\begin{subfigure}{0.34\textwidth}
		\includegraphics[scale=0.111]{latex/u_split1_CUB_rare_accs_TFVAEGAN.pdf}
		\caption{CUB, TF-VAEGAN}
		\label{fig:atcub}
	\end{subfigure}
	\hfil
	\begin{subfigure}{0.34\textwidth}
		\includegraphics[scale=0.111]{latex/u_split1_SUN_rare_accs_DEVISE.pdf}
		\caption{SUN, DeViSE}
		\label{fig:adsun}
	\end{subfigure}
	\hfil
	\begin{subfigure}{0.34\textwidth}
		\includegraphics[scale=0.111]{latex/u_split1_SUN_rare_accs_TFVAEGAN.pdf}
		\caption{SUN, TF-VAEGAN}
		\label{fig:atsun}
	\end{subfigure}
	
	\begin{subfigure}{0.34\textwidth}
		\includegraphics[scale=0.111]{latex/u_split1_CUB_common_accs_DEVISE.pdf}
		\caption{CUB, DeViSE}
		\label{fig:acdsub}
	\end{subfigure}
	\hfil
	\begin{subfigure}{0.34\textwidth}
		\includegraphics[scale=0.111]{latex/u_split1_CUB_common_accs_TFVAEGAN.pdf}
		\caption{CUB, TF-VAEGAN}
		\label{fig:actcub}
	\end{subfigure}
	\hfil
	\begin{subfigure}{0.34\textwidth}
		\includegraphics[scale=0.111]{latex/u_split1_SUN_common_accs_DEVISE.pdf}
		\caption{SUN, TF-VAEGAN}
		\label{fig:acdsun}
	\end{subfigure}
	\hfil
	\begin{subfigure}{0.34\textwidth}
		\includegraphics[scale=0.111]{latex/u_split1_SUN_common_accs_TFVAEGAN.pdf}
		\caption{SUN, TF-VAEGAN}
		\label{fig:actsun}
	\end{subfigure}
	
	
	
	\caption{Class-wise accuracy (in \%) in the CZSL setting for test classes from $\mathcal{U}_{com}^1$ (obtained after randomly splitting set $\mathcal{U}$ of Existing Split (ES)), containing at least one rare attribute ((a)--(d)) or one common attribute ((e)--(h)). The curves are obtained by evaluating the performance of two trained models -- DeViSE~\cite{frome2013devise} and TF-VAEGAN~\cite{narayan2020latent}. Common unseen classes are the test classes on which we evaluate models trained with seen classes from Existing Splits (ES) and Proposed Splits (PS)}
	\Description{Impact on class-wise test accuracy of ZSL models by selecting suitable seen classes for ZSL model training}
	\label{fig:classwise}
\end{figure}




\begin{table}[t]
	\begin{center}
		\caption{Distribution of rare and common attributes for different random splits of $\mathcal{U}\textsubscript{E}$. $A = $ total number of attributes; $N_{\Tilde{C}} = N_{s} + N_{\Tilde{u}}$; A\textsubscript{R} and A\textsubscript{C} are the number of rare and common attributes; Y\textsubscript{R} and Y\textsubscript{C} are the number of common unseen classes having at least one rare and one common attribute respectively}
		\label{tab:3}
		% Some packages, such as MDW tools, offer better commands for making tables
		% than the plain LaTeX2e tabular which is used here.
		\begin{tabular}{lcccc}
			\hline
			\multirow{1}{*}{{\bf Dataset}}   & 
			\multirow{1}{*}{\bf A / N\textsubscript{$\Tilde{C}$} / N\textsubscript{u\textsubscript{com}}}   & 
			\multirow{1}{*}{\textbf{Split}}  & 
			\multirow{1}{*}{\bf A\textsubscript{R} / A\textsubscript{C}}  & 
			\multirow{1}{*}{\bf Y\textsubscript{R} / Y\textsubscript{C}}  \\
			
			%\multicolumn{2}{c}{\textbf{Attribute count}}
			% \\ \cline{3-4}
			%  & & \textbf{Rare} & \textbf{Common}\\
			\hline
			
			\multirow{3}{*}{CUB} &
			\multirow{3}{*}{312 / 175 / 25}
			&1 &24 / 9 &24 / 25  \\
			& &2 &22 / 9 &22 / 25  \\
			& &3 &22 / 10 &19 / 25  \\
			
			\hline
			
			\multirow{3}{*}{SUN} &
			\multirow{3}{*}{102 / 681 / 36} 
			&1 &7 / 2 &22 / 36  \\
			& &2 &7 / 2 &19 / 36  \\
			& &3 &6 / 2 &15 / 36  \\
			
			\hline
		\end{tabular}
	\end{center}
\end{table}

\begin{table}[t]
	\begin{center}
		\caption{Understanding object domain via attributes. Five rare and common attributes (designated as described in Sec.~\ref{sec:6.4}) are listed for the three different object domains for which we show our results. {$\Tilde{\mathcal{C}^X}$} denotes the classes in the object domain acquired as $\Tilde{\mathcal{C}^X} = \mathcal{S}\textsubscript{E} \, \cup \, (\mathcal{U}\textsubscript{E} \setminus \mathcal{U}_{com}^X$). Here, $\mathcal{U}_{com}^{X}$ denotes the set of common unseen classes for both ES and PS separated out for fair evaluation, created by $X^{th}$ random split of set $\mathcal{U}\textsubscript{E}$.  (+$p$) indicates set contains $p$ more attributes}
		\label{tab:rare_common}
		\begin{tabular}{lll}
			\hline
			\multirow{1}{*}{{\bf Domain}}   & 
			\multirow{1}{*}{{\bf Rare}}   & 
			\multirow{1}{*}{{\bf Common}}  \\
			
			% {\bf domain} &{\bf attributes} &{\bf attributes} \\
			
			\hline
			
			\multirow{5}{*}{$\Tilde{\mathcal{C}^1}$} 
			&Needle-shaped bill &Black bill \\
			&Pink throat &Notched tail \\
			&Red back &Small size \\
			&Purple eye &Rounded wings \\
			&Owl-like shape (+19) &Black eye (+4) \\
			\hline
			
			\multirow{5}{*}{$\Tilde{\mathcal{C}^2}$} 
			&Needle-shaped bill &Rounded wing \\
			&Purple underparts &Solid breast pattern \\
			&Pink forehead &Notched tail \\
			&Green leg &Bill shorter than head \\
			&Orange eye (+17) &Solid belly pattern (+4) \\
			\hline
			
			\multirow{5}{*}{$\Tilde{\mathcal{C}^3}$}
			&Red upper-tail &Rounded wing \\
			&Pink crown &Small size \\
			&Green crown &Solid back pattern \\
			&Purple breast &Black eye \\
			&Red underparts (+17) &Black bill (+5) \\
			
			
			\hline
		\end{tabular}
		
	\end{center}
\end{table}



\subsection{Acknowledging rarity in the object domain}
\label{sec:6.4}
For attribute-based data, an object class is uniquely characterized by its attributes, so it can be reasoned that the more rare attributes a class exhibits, the higher its probability of being a rare class. To test the semantic knowledge gained by our framework about the object domain, we develop a notion for designating attributes as either {\it rare} or {\it common} using semantic information from $\mathcal{P}(\Tilde{\mathcal{C}})$. Sets $IA(\Tilde{\mathcal{C}})$ and $UA(\Tilde{\mathcal{C}})$ are developed using Eqs.~\ref{eq:9} and~\ref{eq:10} and their member attributes are discarded. Then, we analyze $\mathcal{B}({\Tilde{C}})$ (obtained using Eq.~\ref{eq:11}) and designate the attributes which appear in less than 5\% of all classes in $\Tilde{\mathcal{C}}$ as rare, and those appearing in more than 50\% of the classes as common attributes. Table~\ref{tab:3} indicates that there are fewer rare attributes in SUN as compared to CUB. This was expected as the attributes in SUN are observed in many different contexts~\cite{patterson2014sun} and hence appear for many classes. On the other hand, several attributes in CUB are visual variants of a single, broader attribute~\cite{WahCUB_200_2011}. Hence, several of these attributes are exhibited by a few classes only.

We report a few rare and common attributes for each object domain (i.e. $\Tilde{\mathcal{C}^1}, \Tilde{\mathcal{C}^2}$ and $\Tilde{\mathcal{C}^3}$) in Tab.~\ref{tab:rare_common}. Thereafter, looking back at the seed classes acquired from $\Tilde{\mathcal{C}^2}$ at the end of stage 1 (Fig.~\ref{fig:seeds0}), we find that our seed-set construction process indeed picks an initial seed set which is not only diverse enough, but also captures the rarity from the semantic space of the object domain. For example, 
Fig.~\ref{fig:seeds0} shows birds exhibiting {\it needle-shaped bill} and {\it orange eye} (in colored boxes), which are rare attributes considering the domain $\Tilde{\mathcal{C}^2}$ (see Fig.~\ref{tab:rare_common}).

Table~\ref{tab:2} conveys that training ZSL models with seen classes that capture rarity in the object domain well enough enhance the models' capability to recognize novel classes exhibiting rare attributes. The class-wise top-1 accuracy after training with two ZSL models --- DeViSE~\cite{frome2013devise} (a compatibility learning framework) and TF-VAEGAN~\cite{narayan2020latent} (a generative model-based framework) --- is depicted in Fig.~\ref{fig:classwise} for test classes from $\mathcal{U}_{com}^1$ exhibiting at least one rare or common attribute. It is evident that models in $\Phi_P$ (those trained by seen classes selected by DiRaC-I) recognize novel classes more accurately. 




\section{Conclusion} \label{disc}
In this paper, we propose a novel framework called DiRaC-I for identifying the most suitable classes from the available database that can be used to train zero-shot models. Specifically, we emphasize capturing both visual diversity and semantic rarity of an object domain through our framework, inspired by Active Learning. Extensive experiments on two challenging fine-grained data sets verified that zero-shot models trained with classes acquired by DiRaC-I perform better than models trained with predetermined classes. We limit our work to these data sets for fair comparison as they have a balanced image count across all the classes, unlike certain others like AwA2~\cite{xian2018zero}. This ensures that even if seen classes for ES and PS are different, it does not adversely affect ZSL model performance just due to a huge difference in number of training images. Additionally, we work only with human-annotated attributes to account for rarity as they are more semantically descriptive and interpretable than word vector representations of classes. Such an attribute space is consistent with our real-life goal of zero-shot methods working in a specific object domain. However, manually defining attribute ontology is expensive. Hence, an extension of DiRaC-I that can work with word vector spaces is worth investigating.




















%\section{Template Overview}
%As noted in the introduction, the ``\verb|acmart|'' document class can
%be used to prepare many different kinds of documentation --- a
%double-blind initial submission of a full-length technical paper, a
%two-page SIGGRAPH Emerging Technologies abstract, a ``camera-ready''
%journal article, a SIGCHI Extended Abstract, and more --- all by
%selecting the appropriate {\itshape template style} and {\itshape
%  template parameters}.
%
%This document will explain the major features of the document
%class. For further information, the {\itshape \LaTeX\ User's Guide} is
%available from
%\url{https://www.acm.org/publications/proceedings-template}.
%
%\subsection{Template Styles}
%
%The primary parameter given to the ``\verb|acmart|'' document class is
%the {\itshape template style} which corresponds to the kind of publication
%or SIG publishing the work. This parameter is enclosed in square
%brackets and is a part of the {\verb|documentclass|} command:
%\begin{verbatim}
%  \documentclass[STYLE]{acmart}
%\end{verbatim}
%
%Journals use one of three template styles. All but three ACM journals
%use the {\verb|acmsmall|} template style:
%\begin{itemize}
%\item {\texttt{acmsmall}}: The default journal template style.
%\item {\texttt{acmlarge}}: Used by JOCCH and TAP.
%\item {\texttt{acmtog}}: Used by TOG.
%\end{itemize}
%
%The majority of conference proceedings documentation will use the {\verb|acmconf|} template style.
%\begin{itemize}
%\item {\texttt{acmconf}}: The default proceedings template style.
%\item{\texttt{sigchi}}: Used for SIGCHI conference articles.
%\item{\texttt{sigchi-a}}: Used for SIGCHI ``Extended Abstract'' articles.
%\item{\texttt{sigplan}}: Used for SIGPLAN conference articles.
%\end{itemize}
%
%\subsection{Template Parameters}
%
%In addition to specifying the {\itshape template style} to be used in
%formatting your work, there are a number of {\itshape template parameters}
%which modify some part of the applied template style. A complete list
%of these parameters can be found in the {\itshape \LaTeX\ User's Guide.}
%
%Frequently-used parameters, or combinations of parameters, include:
%\begin{itemize}
%\item {\texttt{anonymous,review}}: Suitable for a ``double-blind''
%  conference submission. Anonymizes the work and includes line
%  numbers. Use with the \texttt{\acmSubmissionID} command to print the
%  submission's unique ID on each page of the work.
%\item{\texttt{authorversion}}: Produces a version of the work suitable
%  for posting by the author.
%\item{\texttt{screen}}: Produces colored hyperlinks.
%\end{itemize}
%
%This document uses the following string as the first command in the
%source file:
%\begin{verbatim}
%\documentclass[acmsmall]{acmart}
%\end{verbatim}
%
%\section{Modifications}
%
%Modifying the template --- including but not limited to: adjusting
%margins, typeface sizes, line spacing, paragraph and list definitions,
%and the use of the \verb|\vspace| command to manually adjust the
%vertical spacing between elements of your work --- is not allowed.
%
%{\bfseries Your document will be returned to you for revision if
%  modifications are discovered.}
%
%\section{Typefaces}
%
%The ``\verb|acmart|'' document class requires the use of the
%``Libertine'' typeface family. Your \TeX\ installation should include
%this set of packages. Please do not substitute other typefaces. The
%``\verb|lmodern|'' and ``\verb|ltimes|'' packages should not be used,
%as they will override the built-in typeface families.
%
%\section{Title Information}
%
%The title of your work should use capital letters appropriately -
%\url{https://capitalizemytitle.com/} has useful rules for
%capitalization. Use the {\verb|title|} command to define the title of
%your work. If your work has a subtitle, define it with the
%{\verb|subtitle|} command.  Do not insert line breaks in your title.
%
%If your title is lengthy, you must define a short version to be used
%in the page headers, to prevent overlapping text. The \verb|title|
%command has a ``short title'' parameter:
%\begin{verbatim}
%  \title[short title]{full title}
%\end{verbatim}
%
%\section{Authors and Affiliations}
%
%Each author must be defined separately for accurate metadata
%identification.  As an exception, multiple authors may share one
%affiliation. Authors' names should not be abbreviated; use full first
%names wherever possible. Include authors' e-mail addresses whenever
%possible.
%
%Grouping authors' names or e-mail addresses, or providing an ``e-mail
%alias,'' as shown below, is not acceptable:
%\begin{verbatim}
%  \author{Brooke Aster, David Mehldau}
%  \email{dave,judy,steve@university.edu}
%  \email{firstname.lastname@phillips.org}
%\end{verbatim}
%
%The \verb|authornote| and \verb|authornotemark| commands allow a note
%to apply to multiple authors --- for example, if the first two authors
%of an article contributed equally to the work.
%
%If your author list is lengthy, you must define a shortened version of
%the list of authors to be used in the page headers, to prevent
%overlapping text. The following command should be placed just after
%the last \verb|\author{}| definition:
%\begin{verbatim}
%  \renewcommand{\shortauthors}{McCartney, et al.}
%\end{verbatim}
%Omitting this command will force the use of a concatenated list of all
%of the authors' names, which may result in overlapping text in the
%page headers.
%
%The article template's documentation, available at
%\url{https://www.acm.org/publications/proceedings-template}, has a
%complete explanation of these commands and tips for their effective
%use.
%
%Note that authors' addresses are mandatory for journal articles.
%
%\section{Rights Information}
%
%Authors of any work published by ACM will need to complete a rights
%form. Depending on the kind of work, and the rights management choice
%made by the author, this may be copyright transfer, permission,
%license, or an OA (open access) agreement.
%
%Regardless of the rights management choice, the author will receive a
%copy of the completed rights form once it has been submitted. This
%form contains \LaTeX\ commands that must be copied into the source
%document. When the document source is compiled, these commands and
%their parameters add formatted text to several areas of the final
%document:
%\begin{itemize}
%\item the ``ACM Reference Format'' text on the first page.
%\item the ``rights management'' text on the first page.
%\item the conference information in the page header(s).
%\end{itemize}
%
%Rights information is unique to the work; if you are preparing several
%works for an event, make sure to use the correct set of commands with
%each of the works.
%
%The ACM Reference Format text is required for all articles over one
%page in length, and is optional for one-page articles (abstracts).
%
%\section{CCS Concepts and User-Defined Keywords}
%
%Two elements of the ``acmart'' document class provide powerful
%taxonomic tools for you to help readers find your work in an online
%search.
%
%The ACM Computing Classification System ---
%\url{https://www.acm.org/publications/class-2012} --- is a set of
%classifiers and concepts that describe the computing
%discipline. Authors can select entries from this classification
%system, via \url{https://dl.acm.org/ccs/ccs.cfm}, and generate the
%commands to be included in the \LaTeX\ source.
%
%User-defined keywords are a comma-separated list of words and phrases
%of the authors' choosing, providing a more flexible way of describing
%the research being presented.
%
%CCS concepts and user-defined keywords are required for for all
%articles over two pages in length, and are optional for one- and
%two-page articles (or abstracts).
%
%\section{Sectioning Commands}
%
%Your work should use standard \LaTeX\ sectioning commands:
%\verb|section|, \verb|subsection|, \verb|subsubsection|, and
%\verb|paragraph|. They should be numbered; do not remove the numbering
%from the commands.
%
%Simulating a sectioning command by setting the first word or words of
%a paragraph in boldface or italicized text is {\bfseries not allowed.}
%
%\section{Tables}
%
%The ``\verb|acmart|'' document class includes the ``\verb|booktabs|''
%package --- \url{https://ctan.org/pkg/booktabs} --- for preparing
%high-quality tables.
%
%Table captions are placed {\itshape above} the table.
%
%Because tables cannot be split across pages, the best placement for
%them is typically the top of the page nearest their initial cite.  To
%ensure this proper ``floating'' placement of tables, use the
%environment \textbf{table} to enclose the table's contents and the
%table caption.  The contents of the table itself must go in the
%\textbf{tabular} environment, to be aligned properly in rows and
%columns, with the desired horizontal and vertical rules.  Again,
%detailed instructions on \textbf{tabular} material are found in the
%\textit{\LaTeX\ User's Guide}.
%
%Immediately following this sentence is the point at which
%Table~\ref{tab:freq} is included in the input file; compare the
%placement of the table here with the table in the printed output of
%this document.
%
%\begin{table}
%  \caption{Frequency of Special Characters}
%  \label{tab:freq}
%  \begin{tabular}{ccl}
%    \toprule
%    Non-English or Math&Frequency&Comments\\
%    \midrule
%    \O & 1 in 1,000& For Swedish names\\
%    $\pi$ & 1 in 5& Common in math\\
%    \$ & 4 in 5 & Used in business\\
%    $\Psi^2_1$ & 1 in 40,000& Unexplained usage\\
%  \bottomrule
%\end{tabular}
%\end{table}
%
%To set a wider table, which takes up the whole width of the page's
%live area, use the environment \textbf{table*} to enclose the table's
%contents and the table caption.  As with a single-column table, this
%wide table will ``float'' to a location deemed more
%desirable. Immediately following this sentence is the point at which
%Table~\ref{tab:commands} is included in the input file; again, it is
%instructive to compare the placement of the table here with the table
%in the printed output of this document.
%
%\begin{table*}
%  \caption{Some Typical Commands}
%  \label{tab:commands}
%  \begin{tabular}{ccl}
%    \toprule
%    Command &A Number & Comments\\
%    \midrule
%    \texttt{{\char'134}author} & 100& Author \\
%    \texttt{{\char'134}table}& 300 & For tables\\
%    \texttt{{\char'134}table*}& 400& For wider tables\\
%    \bottomrule
%  \end{tabular}
%\end{table*}
%
%Always use midrule to separate table header rows from data rows, and
%use it only for this purpose. This enables assistive technologies to
%recognise table headers and support their users in navigating tables
%more easily.
%
%\section{Math Equations}
%You may want to display math equations in three distinct styles:
%inline, numbered or non-numbered display.  Each of the three are
%discussed in the next sections.
%
%\subsection{Inline (In-text) Equations}
%A formula that appears in the running text is called an inline or
%in-text formula.  It is produced by the \textbf{math} environment,
%which can be invoked with the usual
%\texttt{{\char'134}begin\,\ldots{\char'134}end} construction or with
%the short form \texttt{\$\,\ldots\$}. You can use any of the symbols
%and structures, from $\alpha$ to $\omega$, available in
%\LaTeX~\cite{Lamport:LaTeX}; this section will simply show a few
%examples of in-text equations in context. Notice how this equation:
%\begin{math}
%  \lim_{n\rightarrow \infty}x=0
%\end{math},
%set here in in-line math style, looks slightly different when
%set in display style.  (See next section).
%
%\subsection{Display Equations}
%A numbered display equation---one set off by vertical space from the
%text and centered horizontally---is produced by the \textbf{equation}
%environment. An unnumbered display equation is produced by the
%\textbf{displaymath} environment.
%
%Again, in either environment, you can use any of the symbols and
%structures available in \LaTeX\@; this section will just give a couple
%of examples of display equations in context.  First, consider the
%equation, shown as an inline equation above:
%\begin{equation}
%  \lim_{n\rightarrow \infty}x=0
%\end{equation}
%Notice how it is formatted somewhat differently in
%the \textbf{displaymath}
%environment.  Now, we'll enter an unnumbered equation:
%\begin{displaymath}
%  \sum_{i=0}^{\infty} x + 1
%\end{displaymath}
%and follow it with another numbered equation:
%\begin{equation}
%  \sum_{i=0}^{\infty}x_i=\int_{0}^{\pi+2} f
%\end{equation}
%just to demonstrate \LaTeX's able handling of numbering.
%
%\section{Figures}
%
%The ``\verb|figure|'' environment should be used for figures. One or
%more images can be placed within a figure. If your figure contains
%third-party material, you must clearly identify it as such, as shown
%in the example below.
%%\begin{figure}[h]
%%  \centering
%%  \includegraphics[width=\linewidth]{sample-franklin}
%%  \caption{1907 Franklin Model D roadster. Photograph by Harris \&
%%    Ewing, Inc. [Public domain], via Wikimedia
%%    Commons. (\url{https://goo.gl/VLCRBB}).}
%%  \Description{A woman and a girl in white dresses sit in an open car.}
%%\end{figure}
%
%Your figures should contain a caption which describes the figure to
%the reader.
%
%Figure captions are placed {\itshape below} the figure.
%
%Every figure should also have a figure description unless it is purely
%decorative. These descriptions convey what’s in the image to someone
%who cannot see it. They are also used by search engine crawlers for
%indexing images, and when images cannot be loaded.
%
%A figure description must be unformatted plain text less than 2000
%characters long (including spaces).  {\bfseries Figure descriptions
%  should not repeat the figure caption – their purpose is to capture
%  important information that is not already provided in the caption or
%  the main text of the paper.} For figures that convey important and
%complex new information, a short text description may not be
%adequate. More complex alternative descriptions can be placed in an
%appendix and referenced in a short figure description. For example,
%provide a data table capturing the information in a bar chart, or a
%structured list representing a graph.  For additional information
%regarding how best to write figure descriptions and why doing this is
%so important, please see
%\url{https://www.acm.org/publications/taps/describing-figures/}.
%
%\subsection{The ``Teaser Figure''}
%
%A ``teaser figure'' is an image, or set of images in one figure, that
%are placed after all author and affiliation information, and before
%the body of the article, spanning the page. If you wish to have such a
%figure in your article, place the command immediately before the
%\verb|\maketitle| command:
%\begin{verbatim}
%  \begin{teaserfigure}
%    \includegraphics[width=\textwidth]{sampleteaser}
%    \caption{figure caption}
%    \Description{figure description}
%  \end{teaserfigure}
%\end{verbatim}
%
%\section{Citations and Bibliographies}
%
%The use of \BibTeX\ for the preparation and formatting of one's
%references is strongly recommended. Authors' names should be complete
%--- use full first names (``Donald E. Knuth'') not initials
%(``D. E. Knuth'') --- and the salient identifying features of a
%reference should be included: title, year, volume, number, pages,
%article DOI, etc.
%
%The bibliography is included in your source document with these two
%commands, placed just before the \verb|\end{document}| command:
%\begin{verbatim}
%  \bibliographystyle{ACM-Reference-Format}
%  \bibliography{bibfile}
%\end{verbatim}
%where ``\verb|bibfile|'' is the name, without the ``\verb|.bib|''
%suffix, of the \BibTeX\ file.
%
%Citations and references are numbered by default. A small number of
%ACM publications have citations and references formatted in the
%``author year'' style; for these exceptions, please include this
%command in the {\bfseries preamble} (before the command
%``\verb|\begin{document}|'') of your \LaTeX\ source:
%\begin{verbatim}
%  \citestyle{acmauthoryear}
%\end{verbatim}
%
%
%  Some examples.  A paginated journal article \cite{Abril07}, an
%  enumerated journal article \cite{Cohen07}, a reference to an entire
%  issue \cite{JCohen96}, a monograph (whole book) \cite{Kosiur01}, a
%  monograph/whole book in a series (see 2a in spec. document)
%  \cite{Harel79}, a divisible-book such as an anthology or compilation
%  \cite{Editor00} followed by the same example, however we only output
%  the series if the volume number is given \cite{Editor00a} (so
%  Editor00a's series should NOT be present since it has no vol. no.),
%  a chapter in a divisible book \cite{Spector90}, a chapter in a
%  divisible book in a series \cite{Douglass98}, a multi-volume work as
%  book \cite{Knuth97}, a couple of articles in a proceedings (of a
%  conference, symposium, workshop for example) (paginated proceedings
%  article) \cite{Andler79, Hagerup1993}, a proceedings article with
%  all possible elements \cite{Smith10}, an example of an enumerated
%  proceedings article \cite{VanGundy07}, an informally published work
%  \cite{Harel78}, a couple of preprints \cite{Bornmann2019,
%    AnzarootPBM14}, a doctoral dissertation \cite{Clarkson85}, a
%  master's thesis: \cite{anisi03}, an online document / world wide web
%  resource \cite{Thornburg01, Ablamowicz07, Poker06}, a video game
%  (Case 1) \cite{Obama08} and (Case 2) \cite{Novak03} and \cite{Lee05}
%  and (Case 3) a patent \cite{JoeScientist001}, work accepted for
%  publication \cite{rous08}, 'YYYYb'-test for prolific author
%  \cite{SaeediMEJ10} and \cite{SaeediJETC10}. Other cites might
%  contain 'duplicate' DOI and URLs (some SIAM articles)
%  \cite{Kirschmer:2010:AEI:1958016.1958018}. Boris / Barbara Beeton:
%  multi-volume works as books \cite{MR781536} and \cite{MR781537}. A
%  couple of citations with DOIs:
%  \cite{2004:ITE:1009386.1010128,Kirschmer:2010:AEI:1958016.1958018}. Online
%  citations: \cite{TUGInstmem, Thornburg01, CTANacmart}.
%  Artifacts: \cite{R} and \cite{UMassCitations}.
%
%\section{Acknowledgments}
%
%Identification of funding sources and other support, and thanks to
%individuals and groups that assisted in the research and the
%preparation of the work should be included in an acknowledgment
%section, which is placed just before the reference section in your
%document.
%
%This section has a special environment:
%\begin{verbatim}
%  \begin{acks}
%  ...
%  \end{acks}
%\end{verbatim}
%so that the information contained therein can be more easily collected
%during the article metadata extraction phase, and to ensure
%consistency in the spelling of the section heading.
%
%Authors should not prepare this section as a numbered or unnumbered {\verb|\section|}; please use the ``{\verb|acks|}'' environment.
%
%\section{Appendices}
%
%If your work needs an appendix, add it before the
%``\verb|\end{document}|'' command at the conclusion of your source
%document.
%
%Start the appendix with the ``\verb|appendix|'' command:
%\begin{verbatim}
%  \appendix
%\end{verbatim}
%and note that in the appendix, sections are lettered, not
%numbered. This document has two appendices, demonstrating the section
%and subsection identification method.
%
%\section{Multi-language papers}
%
%Papers may be written in languages other than English or include
%titles, subtitles, keywords and abstracts in different languages (as a
%rule, a paper in a language other than English should include an
%English title and an English abstract).  Use \verb|language=...| for
%every language used in the paper.  The last language indicated is the
%main language of the paper.  For example, a French paper with
%additional titles and abstracts in English and German may start with
%the following command
%\begin{verbatim}
%\documentclass[sigconf, language=english, language=german,
%               language=french]{acmart}
%\end{verbatim}
%
%The title, subtitle, keywords and abstract will be typeset in the main
%language of the paper.  The commands \verb|\translatedXXX|, \verb|XXX|
%begin title, subtitle and keywords, can be used to set these elements
%in the other languages.  The environment \verb|translatedabstract| is
%used to set the translation of the abstract.  These commands and
%environment have a mandatory first argument: the language of the
%second argument.  See \verb|sample-sigconf-i13n.tex| file for examples
%of their usage.
%
%\section{SIGCHI Extended Abstracts}
%
%The ``\verb|sigchi-a|'' template style (available only in \LaTeX\ and
%not in Word) produces a landscape-orientation formatted article, with
%a wide left margin. Three environments are available for use with the
%``\verb|sigchi-a|'' template style, and produce formatted output in
%the margin:
%\begin{description}
%\item[\texttt{sidebar}:]  Place formatted text in the margin.
%\item[\texttt{marginfigure}:] Place a figure in the margin.
%\item[\texttt{margintable}:] Place a table in the margin.
%\end{description}
%
%%%
%%% The acknowledgments section is defined using the "acks" environment
%%% (and NOT an unnumbered section). This ensures the proper
%%% identification of the section in the article metadata, and the
%%% consistent spelling of the heading.
%\begin{acks}
%To Robert, for the bagels and explaining CMYK and color spaces.
%\end{acks}

%%
%% The next two lines define the bibliography style to be used, and
%% the bibliography file.
\bibliographystyle{ACM-Reference-Format}
\bibliography{dirac-i-ref}


%%
%% If your work has an appendix, this is the place to put it.
%\appendix
%
%\section{Research Methods}
%
%\subsection{Part One}
%
%Lorem ipsum dolor sit amet, consectetur adipiscing elit. Morbi
%malesuada, quam in pulvinar varius, metus nunc fermentum urna, id
%sollicitudin purus odio sit amet enim. Aliquam ullamcorper eu ipsum
%vel mollis. Curabitur quis dictum nisl. Phasellus vel semper risus, et
%lacinia dolor. Integer ultricies commodo sem nec semper.
%
%\subsection{Part Two}
%
%Etiam commodo feugiat nisl pulvinar pellentesque. Etiam auctor sodales
%ligula, non varius nibh pulvinar semper. Suspendisse nec lectus non
%ipsum convallis congue hendrerit vitae sapien. Donec at laoreet
%eros. Vivamus non purus placerat, scelerisque diam eu, cursus
%ante. Etiam aliquam tortor auctor efficitur mattis.
%
%\section{Online Resources}
%
%Nam id fermentum dui. Suspendisse sagittis tortor a nulla mollis, in
%pulvinar ex pretium. Sed interdum orci quis metus euismod, et sagittis
%enim maximus. Vestibulum gravida massa ut felis suscipit
%congue. Quisque mattis elit a risus ultrices commodo venenatis eget
%dui. Etiam sagittis eleifend elementum.
%
%Nam interdum magna at lectus dignissim, ac dignissim lorem
%rhoncus. Maecenas eu arcu ac neque placerat aliquam. Nunc pulvinar
%massa et mattis lacinia.

\end{document}
\endinput
%%
%% End of file `sample-acmsmall.tex'.
