%\appendices
%\section*{Appendix}
\section{Proof of Supporting Lemmas of Theorem~\ref{thm:LP_instantly_decodable}}

%\color{red}

% --------------------------------------------------------
% Lemma: LP solves Min
% --------------------------------------------------------
\begin{myLemma}
\label{lem:LPToMin}
	Let $(\bT{1}, \bT{2}, \bT{3})$ be the optimal solution of the linear program in~\eqref{eq:LP}.  Then $(\bT{1}, \bT{2}, \bT{3})$ satisfies~\eqref{eq:Ti_min}.
\end{myLemma}
\begin{proof}
To show that the optimal solution of~\eqref{eq:LP} must satisfy \eqref{eq:Ti_min}, we note that in order to maximize the objective function in \eqref{eq:LP}, one of the inequality constraints for \bT{i} must be met with equality.  Otherwise, if \bT{i}, \bT{j}, \bT{k} is a purported optimal solution where \bT{i} does not meet an inequality constraint with equality, we can find some $\delta > 0$ such that $\bT{i} + \delta$, \bT{j}, \bT{k} is also feasible and gives a strictly larger objective function value.
\end{proof}

% --------------------------------------------------------
% Lemma: Min Solves LP
% --------------------------------------------------------

\begin{myLemma}
\label{lem:MinToLP}
	Let $(\bT{1}, \bT{2}, \bT{3})$ satisfy~\eqref{eq:Ti_min}, where $\bT{i} > 0$ for all $i \in \mathcal{U}$.  Then $(\bT{1}, \bT{2}, \bT{3})$ is an optimal solution of~\eqref{eq:LP}.
\end{myLemma}
\begin{proof}
	We show that $(\bT{1}, \bT{2}, \bT{3})$ is a local maximum of the linear program in~\eqref{eq:LP}.  Consider a neighbouring  element in the feasible region of~\eqref{eq:LP} of the form $(\bT{1} + \Delta_1, \bT{2} + \Delta_2, \bT{3} + \Delta_3)$.  We show that the objective function evaluated at this new element is not greater than the objective function evaluated at $(\bT{1}, \bT{2}, \bT{3})$.  We do this by showing that the linear program

% --------------------------------------------------------
% Equation: Formulate Optimization for Local Max
% --------------------------------------------------------
	
\begin{equation}
\label{eq:local}
\begin{aligned}
	& \underset{\Delta_{1}, \Delta_{2}, \Delta_{3}}{\text{max}}
	&& \!\! \Delta_{1} + \Delta_{2} + \Delta_{3}\\
	& \text{subject to}
%\end{aligned}
%\end{equation}
%\[
%\begin{aligned}
	&& 	\!\! \bT{i} + \Delta_i \leq \frac{\Qp{i}{\bT{j} + \Delta_j}{\bT{k} + \Delta_k} }{1 - \epsilon_i},\\
	&&& \!\!\bar{T}_i + \Delta_i \leq \frac{\Q{j,k}^{+}}{1 - \epsilon_j\epsilon_k} \quad \forall i \in \mathcal{U}, j,k \in \mathcal{U}\setminus \{i\} , j \neq k.
\end{aligned}
%\]
\end{equation}
%	
has an optimal value no greater than zero.  

By assumption, $(\bT{1}, \bT{2}, \bT{3})$ satisfies~\eqref{eq:Ti_min}, and so for all $i \in \mathcal{U}$, $\bT{i}$ meets one of the inequality constraints in~\eqref{eq:LP} with equality. Let $\mathcal{T}(\bT{1}, \bT{2}, \bT{3})$ be the set of $i$ for which \bT{i} meets the first inequality constraint with equality, i.e., 
%\begin{equation}
%\begin{eqnarray}
% --------------------------------------------------------
% Equation: Define T Set
% --------------------------------------------------------
\begin{flalign}
\label{eq:Tset}
	&\mathcal{T}(\bT{1}, \bT{2}, \bT{3}) = \\ \nonumber
	& \left\{i \in \mathcal{U} \mid \bT{i} = \frac{\Qp{i}{\bT{j}}{\bT{k}} }{1 - \epsilon_i}, \bT{i} > 0, j, k \in \mathcal{U} \setminus \{i\}, j \neq k\right\}.
%\end{equation}
%\end{eqnarray}
\end{flalign}
%
When the context is clear, we will at times use $\mathcal{T}$ to refer to $\mathcal{T}(\bT{1}, \bT{2}, \bT{3})$.  Furthermore, for $i \in \mathcal{T}$, we will at times find it convenient to emphasize the linear dependence of \bT{i} on \bT{j}, and \bT{k} by writing (c.f.~\eqref{eq:Qi_plus})
\begin{equation}
\label{eq:linear_Qi_plus}
	\frac{Q_i^{+}(\bT{j}, \bT{k})}{1 - \epsilon_i} = k_i + a_{ik}\bT{j} + a_{ij} \bT{k},
\end{equation}
where 
\begin{equation}
\label{eq:a_ik}
%	a_{ik} = \epsilon_i(1 - \epsilon_k)/(1 - \epsilon_i) > 0.
	a_{ik} = \frac{\epsilon_i}{(1 - \epsilon_i)}(1 - \epsilon_k) > 0,
\end{equation}
\begin{equation}
\label{eq:k_i}
	k_i = \frac{\bT{0}\epsilon_i(1 - \epsilon_j)(1 - \epsilon_k)}{1 - \epsilon_i} > 0,
\end{equation}
and \bT{0} is given by~\eqref{eq:T0}.

Now, we have that if $i \notin \mathcal{T}$, then $\bT{i} = \Q{j,k}^{+}/(1 - \epsilon_j\epsilon_k)$.  From the second inequality of~\eqref{eq:local}, we have then that for these values of $i$, $\Delta_i \leq 0$.  Notice  however, that an optimal solution of~\eqref{eq:local} must have all of its components non-negative.  Otherwise, a larger-valued objective function could be obtained by setting any negative component to zero.  
We can therefore upper bound the optimal value for the linear program in~\eqref{eq:local} with the negated optimal value for the \emph{relaxed} linear program

%\begin{equation}
%\label{eq:delta_half}
%\begin{aligned}
%	& \underset{\Delta_{1}, \Delta_{2}, \Delta_{3}}{\text{max}}
%	&& \!\! \Delta_{1} + \Delta_{2} + \Delta_{3}\\
%	& \text{subject to}
%%\end{aligned}
%%\end{equation}
%%\[
%%\begin{aligned}
%	&& 	\!\! \bT{i} + \Delta_i \leq \frac{\Qp{i}{\bT{j} + \Delta_j}{\bT{k} + \Delta_k} }{1 - \epsilon_i}, \\
%	&&& \quad \forall i \in \mathcal{T}, j,k \in \mathcal{T} \setminus\{i\}, j\neq k\\
%	&&& \Delta_j = 0, \quad j \notin \mathcal{T}.
%\end{aligned}
%%\]
%\end{equation}
%
%We further simplify the constraints of~\eqref{eq:delta_half} by using~\eqref{eq:Tset} and explicitly evaluating \Qp{i}{\cdot}{\cdot} to arrive at the linear program
% --------------------------------------------------------
% Equation: Optimization for Deltas
% --------------------------------------------------------
\begin{equation}
\label{eq:delta}
\begin{aligned}
	& \underset{\Delta_{1}, \Delta_{2}, \Delta_{3}}{\text{min}}
%	& \underset{\Delta_{1}, \Delta_{2}, \Delta_{3}}{\text{max}}
	&& \!\! -(\Delta_{1} + \Delta_{2} + \Delta_{3})\\
%	&& \!\! \Delta_{1} + \Delta_{2} + \Delta_{3}\\
	& \text{subject to}
%\end{aligned}
%\end{equation}
%\[
%\begin{aligned}
	&& 	\!\! \Delta_i \leq a_{ik}\Delta_{j} + a_{ij}\Delta_{k} \\
	&&& \quad \forall i \in \mathcal{T}, j,k \in \mathcal{U} \setminus\{i\}, j\neq k\\
	&&& \Delta_j = 0, \quad \forall j \notin \mathcal{T}.
\end{aligned}
%\]
\end{equation}
%
where we have used~\eqref{eq:Tset} and \eqref{eq:a_ik} to simplify the constraints of~\eqref{eq:local}.

We now consider several cases.  Notice first that if $0 \leq |\mathcal{T}| \leq 1$, the constraints of~\eqref{eq:delta} clearly show that the optimal value of~\eqref{eq:delta} is zero. Alternatively, if $2 \leq |\mathcal{T}| \leq 3$, we consider the Lagrange dual function, $g(\lambda)$, given by~\cite[Section~5.2.1]{boyd2004convex}
\begin{equation}
g(\lambda) = 
	\begin{cases}
   		0 & \text{if } A^T \lambda = -c, \\
   		-\infty & \text{else,}
	\end{cases}
\end{equation}  
where we have assumed that the objective function and inequality constraints of~\eqref{eq:delta} have been written as $c^T\Delta$ and $A\Delta \leq 0$ respectively for some $\Delta$, $A$ and $c$ that will be subsequently defined depending on $|\mathcal{T}|$.  For any $\lambda \succeq 0$, $g(\lambda)$ gives a lower bound on the optimal value of~\eqref{eq:delta}.  If we can therefore find a $\lambda \succeq 0$ such that 
\begin{equation}
\label{eq:lambda}
A^T \lambda = -c, 
\end{equation}
we will have thus shown that the optimal value of~\eqref{eq:local} is no greater than zero, as was our original goal.

%\begin{enumerate}[\label=\bfseries $|\mathcal{T}| = 2$ \arabic*:]
%\begin{enumerate}[label=\bfseries Exercise \arabic*:]
\begin{enumerate}

	% --------------------------------------------------------
	% Case T = 2
	% --------------------------------------------------------
	\item Consider now, if $|\mathcal{T}| = 2$.  Let $i, j$  and $k$ be distinct elements in $\mathcal{U}$, where we assume without loss of generality that $k$ is the only element not in~$\mathcal{T}$.
	In this case, we have
%	\begin{eqnarray}
	\begin{equation}
		A =  A_2 \triangleq\begin{bmatrix}
			       	1 & -a_{ik}           \\
				-a_{jk} & 1  
			\end{bmatrix},
	\end{equation}
	\begin{equation}
	\label{eq:c2}
		c = c_2 \triangleq \begin{bmatrix}
				-1 \\
				-1 
			\end{bmatrix}, \qquad
		\Delta = \hat{\Delta} \triangleq \begin{bmatrix}
				\Delta_i \\
				\Delta_j 
			\end{bmatrix},
	\end{equation}
%	\end{eqnarray}
	where we remind the reader that we have assumed that the constraints in~\eqref{eq:delta} are written as $A\Delta \leq 0$.  
%	where we have assumed that $\Delta = [\Delta_i, \Delta_j]^T$, and the inequality constraints of~\eqref{eq:delta} have been written as $A\Delta \leq 0$.
	Since the off-diagonal entries of $A_2$ are negative by~\eqref{eq:a_ik}, we have that $A_2$ is a $Z$-matrix~\cite{P77}.  Furthermore, since $|\mathcal{T}| = 2$ by assumption, we further have that there exists an $x \triangleq [\bT{i}, \bT{j}] \succeq 0$ such that $A_2x = k \succ 0$, where $k = [k_i + a_{ij}\bT{k}, k_j + a_{ji}\bT{k}]$ and $\bT{k} = \Q{i, j}^{+}/(1 - \epsilon_i\epsilon_j)$, (see~\eqref{eq:Tset}, \eqref{eq:linear_Qi_plus} and \eqref{eq:k_i}).  Therefore, by Condition $K_{34}$ of~\cite{P77}, $A_2$ is also a non-singular $M$-matrix.  
%	By Condition $A_1$ of~\cite{P77}, we conclude that $A_{2}^{T}$ is also a non-singular $M$-matrix and therefore 
	By Condition~$F_{15}$ of~\cite{P77}, we thus infer that the inverse matrix $A_{2}^{-1}$ has all positive elements.  Finally, we conclude from~\eqref{eq:c2} that indeed $[\lambda_1, \lambda_2]^T = -(A_2^T)^{-1} c_2 \succeq 0$ since all elements involved in the matrix multiplication are positive.

%	We explicitly invert the system of equations for $\lambda$ in~\eqref{eq:lambda} to get that $[\lambda_1, \lambda_2]^T = 		-(A_2^T)^{-1} c_2 $ where
%	\begin{equation}
%%	\begin{eqnarray}
%	\label{eq:T_equals_two}
%		(A_2^{T})^{-1}
%%		\begin{bmatrix}
%%			\lambda_1 \\
%%			\lambda_2 
%%		\end{bmatrix} 
%%			&=& 
%%		-(A_2^T)^{-1} c \\
%%			&=&
%			=
%		\frac{1}{1 - a_{ik}a_{jk}}
%			\begin{bmatrix}
%			       	1 & a_{jk}           \\
%				a_{ik} & 1  
%			\end{bmatrix}.
%%			\begin{bmatrix}
%%				1 \\
%%				1
%%			\end{bmatrix} .
%	\end{equation}
%%	\end{eqnarray}
%	Lemma~\ref{lem:one_minus_a} shows that if $i$ and $j$ are in $\mathcal{T}$, the denominator in~\eqref{eq:T_equals_two} is positive.  We use this along with~\eqref{eq:a_ik} and~\eqref{eq:c2} to conclude that indeed $\lambda \succeq 0$.
	
	% --------------------------------------------------------
	% Case T = 3
	% --------------------------------------------------------
	\item If $|\mathcal{T}| = 3$, we have that
	\begin{equation}
	\label{eq:A3}
		A = A_3 \triangleq \begin{bmatrix}
			       	1 & -a_{ik} & -a_{ij}  \\
				-a_{jk} & 1 & -a_{ji}  \\
				-a_{kj} & -a_{ki} & 1 
			\end{bmatrix},
	\end{equation}
	\begin{equation}
	\label{eq:c3}
		c = c_3 \triangleq \begin{bmatrix}
				-1 \\
				-1 \\
				-1
			\end{bmatrix}, \qquad
		\Delta = \mathring{\Delta} \triangleq \begin{bmatrix}
				\Delta_i \\
				\Delta_j \\
				\Delta_k
			\end{bmatrix}.
	\end{equation}
%	Assuming that $\det(A_3) \neq 0$, we again invert the system of linear equations in~\eqref{eq:lambda} to get that
	
	Similar to the case in which $|\mathcal{T}| = 2$, we can again argue that $A_{3}$ is a non-singular $M$-matrix and so it is inverse-positive~\cite{P77}.  Consequently, we again have that indeed $[\lambda_1, \lambda_2, \lambda_3]^T = -(A_3^{T})^{-1} c_3 \succeq 0$.
	
%	In Lemma~\ref{lem:det}, we establish that if the system of linear equations in the definition of $\mathcal{T}$ are satisfied when $|\mathcal{T}| = 3$,  
%%	and the inequalities defined by $A_3\Delta \leq 0$ are also satisfied, 
%	then $\det(A_3) > 0$.  We can therefore again invert the system of linear equations in~\eqref{eq:lambda} to get that $[\lambda_1, \lambda_2, \lambda_3]^T = -(A_3^{T})^{-1} c_3 $ where
%%	Lemma~\ref{lem:det} establishes that a trivial determinant for $A_3$ contradicts the assumption that $|\mathcal{T}| = 3$, i.e., that we have found a $(\bT{1}, \bT{2}, \bT{3}) \succeq 0$ satisfying~\eqref{eq:linear_Qi_plus} for $i \in \mathcal{U}, j, k \in \mathcal{U} \setminus \{i\}, j\neq k$.  A trivial determinant can result in a system of equations having either no solution, or infinitely many solutions.  However, by showing that a trivial determinant results in an inconsistent system of linear equations, Lemma~\ref{lem:det} shows that only the former is possible.
%
%	\begin{flalign}
%	\label{eq:A3_inverse}
%	& (A_3^{T})^{-1} = %\\ \nonumber
%	 \frac{1}{\det(A_3)}
%		\begin{bmatrix}
%		       	1 - a_{ji}a_{ki} & a_{jk} + a_{ji}a_{kj} & a_{kj} + a_{jk}a_{ki}  \\
%			a_{ik}+ a_{ij}a_{ki} & 1 - a_{ij}a_{kj} & a_{ki} + a_{ik}a_{kj}  \\
%			a_{ij} + a_{ik}a_{ji} & a_{ji}+a_{ij}a_{jk} & 1  - a_{ik}a_{jk}
%		\end{bmatrix}.
%	\end{flalign}
%	By Lemmas~\ref{lem:one_minus_a} and \ref{lem:det} and Equations~\eqref{eq:a_ik} and \eqref{eq:c3}, we again conclude that $\lambda \succeq 0$.
\end{enumerate}

%\begin{equation}
%\label{eq:matrix}
%\begin{aligned}
%	& \underset{\Delta}{\text{max}}
%	&& \!\! c^T \Delta\\
%	& \text{subject to}
%%\end{aligned}
%%\end{equation}
%%\[
%%\begin{aligned}
%	&& 	\!\! A \Delta \leq 0 \\
%\end{aligned}
%%\]
%\end{equation}


\end{proof}

% --------------------------------------------------------
% Lemma: Uniqueness
% --------------------------------------------------------

\begin{myLemma}
\label{lem:unique}
	Let $(\bT{1}, \bT{2}, \bT{3})$ be the optimal solution of the linear program in~\eqref{eq:LP}.  Then $(\bT{1}, \bT{2}, \bT{3})$  is unique.
\end{myLemma}

\begin{proof}
%Therefore, we conclude that the solution to \eqref{eq:LP} is a solution to \eqref{eq:Ti_min}.  
We show that the solution to~\eqref{eq:LP} is unique via Theorem~1 of~\cite{OLM}.  
Intuitively, its reasoning is that to maximize the objective function of a linear program, we follow the objective function's gradient vector until we reach the boundary of the feasible region.  If this stopping occurs at a face of the region rather than a single point, we have a non-unique solution.  In this case, following the gradient vector after it has undergone a small perturbation in its direction will lead us to a different solution on the face of the boundary region.  On the other hand, if any small perturbation in the gradient vector's direction leads us to the same boundary point as the unperturbed gradient vector, we know we have a unique solution.  This is stated in the following fact.

\begin{fact}[{\hspace{1sp}\cite[Theorem 1]{OLM}}]
%\begin{fact}\hspace{1sp}\cite[Theorem~1]{OLM}
\label{fact:unique}
	A solution $\bar{x}$ to the linear programming problem
	\begin{equation}
	\label{eq:LP_fact}
\begin{aligned}
	& \underset{x}{\text{max}}
	&& c^{T}x\\
	& \text{subject to}
	&& 	Ax \leq b\\
\end{aligned}
\end{equation}
is unique iff for all $q$, there exists a $\delta >0$ s.t.\ $\bar{x}$ is still a solution when the objective function is replaced by $(c + \delta q)^{T}x$.
\end{fact}

For our purposes, we have that $c = [1, 1, 1]^{T}$ in Fact~\ref{fact:unique}. Furthermore, for any given $q$, we choose $\delta$ large enough so that all coefficients of the newly-perturbed objective function remain positive.  

Let $\bar{T}^{*} = (\bT{1}^{*}, \bT{2}^{*}, \bT{3}^{*})$ be the optimal solution of~\eqref{eq:LP}, and let $\bar{T} = (\bT{1}, \bT{2}, \bT{3})$ be another element in the feasible region of~\eqref{eq:LP}.  We will show that $\bar{T}^{*}$ remains the optimal solution of~\eqref{eq:LP} when the objective function is perturbed in a way that maintains the positivity of its coefficients by showing that  
\begin{equation}
\label{eq:unique_positive}
	\bT{i}^{*} - \bT{i} \geq 0, \qquad \textrm{for all } i \in \mathcal{U}.
\end{equation}
%
Assume, by way of contradiction, that~\eqref{eq:unique_positive} does not hold, i.e., there exists a non-empty set $\mathcal{V} \subseteq \mathcal{U}$ such that for all $j \in \mathcal{V}$, $\bT{j}^{*} - \bT{j} < 0$.  We will show that the existence of such a set violates the optimality assumption of $\bar{T}^{*}$.  In particular, we construct a feasible solution of~\eqref{eq:LP} with a strictly larger value for the objective function than when it is evaluated at $\bar{T}^{*}$.  This new solution retains all $\bT{i}^{*}$ for $i \in \mathcal{U} \setminus \mathcal{V}$ and replaces all $\bT{j}^{*}$ with $\bT{j}$ for all $j \in \mathcal{V}$.  

For all $i \in \mathcal{U} \setminus \mathcal{V}$, we now show the feasibility of $\bT{i}^{*}$ within the newly-constructed solution by writing
\begin{equation}
\label{eq:still_feasible1}
	\bT{i}^{*} \leq \frac{Q_{\{j, k\}}^{+}}{1 - \epsilon_j\epsilon_k},
\end{equation}
and 
%\newcounter{cnt}
\setcounter{cnt}{1}
\begin{eqnarray}
	\bT{i}^{*} 
		&\stackrel{(\alph{cnt})}{\leq}& k_i + a_{ik} \bT{j}^{*} + a_{ij}\bT{k}^{*} \\
		\addtocounter{cnt}{1}
		&=& k_i + \sum_{\substack{u \in \mathcal{U} \setminus \mathcal{V}, \\ v \in \mathcal{U} \setminus \{i, u\}}} a_{iv} \bT{u}^{*} + \sum_{\substack{v \in \mathcal{V} \\ u \in \mathcal{U} \setminus \{i, v\}}} a_{iu} \bT{v}^{*} \\
		\label{eq:still_feasible}
		&\stackrel{(\alph{cnt})}{<}& k_i + \sum_{\substack{u \in \mathcal{U} \setminus \mathcal{V}, \\ v \in \mathcal{U} \setminus \{i, u\}}}a_{iv} \bT{u}^{*} + \sum_{\substack{v \in \mathcal{V} \\ u \in \mathcal{U} \setminus \{i, v\}}} a_{iu} \bT{v},
\end{eqnarray}
where 
\begin{enumerate}[(a)]
	\item and \eqref{eq:still_feasible1} follow from~\eqref{eq:linear_Qi_plus} and the feasibility of $\bar{T}^{*}$
	\item follows from~\eqref{eq:a_ik} and the definition of $\mathcal{V}$.
\end{enumerate}

Hence, \eqref{eq:still_feasible} shows that $\bT{i}^{*}$ remains feasible within the newly-constructed solution.  Similarly, we can show that $\bT{j}$ remains feasible within the newly-constructed solution for all $j \in \mathcal{V}$.
%Fact~\ref{fact:unique} is true for our situation since for any $q$ that perturbs our objective function, we can find a sufficiently small $\delta > 0$ that keeps all coefficients of the objective function in \eqref{eq:LP} positive.  With positive coefficients for each $\bar{T}_i$ term, we can similarly argue that optimality is achieved when \eqref{eq:Ti_min} holds.

%\textcolor{red}{
%In summary, we have shown that the unique solution of~\eqref{eq:LP} is also a solution to~\eqref{eq:Ti_min}.  If we could conversely show that a solution to~\eqref{eq:Ti_min} is also a solution to~\eqref{eq:LP}, then we would have that there is a unique number of instantly decodable, distortion-innovative symbols that can be sent.  As of yet, however, we have not done so, although it is an ongoing effort and simulation results in Section~\ref{sec:simulations} strongly suggest this.  Consequently, \bT{i} in~\eqref{eq:Ti_min} may actually be a function of some scheduler that determines which linear combination to send at any given time based on the state of the queues.  Furthermore, each scheduler may give a different solution to~\eqref{eq:Ti_min}.
%}
%
%The proof is in preparation and will be available in the extended version of this paper.  Section~\ref{sec:simulations} however, will describe simulations that support the plausibility of this result.  

\end{proof}

% --------------------------------------------------------
% Lemma: 1 - a_{ik}a_{jk}
% --------------------------------------------------------

%%\begin{myLemma}
%%\label{lem:one_minus_a}
%%	Let $i, j$ and $k$ be distinct elements in $\mathcal{U}$, and let $(\bT{1}, \bT{2}, \bT{3})$ satisfy~\eqref{eq:Ti_min}.  If $i, j \in \mathcal{T}(\bT{1}, \bT{2}, \bT{3})$, then $1 - a_{ik}a_{jk} > 0$, where $\mathcal{T}$ is given by~\eqref{eq:Tset} and $a_{ik}$ is given by~\eqref{eq:a_ik}.
%%\end{myLemma}
%%
%%\begin{proof}
%%%	We use~\eqref{eq:Qi_plus} to evaluate $Q_{i}^{+}(\cdot, \cdot)$ and write that 
%%%	\begin{equation}
%%%		a_{jk} \bT{i} + a_{ji} \bT{k} + k_j = \frac{1}{a_{ik}} (\bT{i} - a_{ij} \bT{k} - k_i)
%%%	\end{equation}
%%%	and so
%%	We eliminate $T_j$ from the two linear equations $\bT{i} = Q_i^{+}(\bT{j}, \bT{k})/(1 - \epsilon_i)$ and $\bT{j} = Q_j^{+}(\bT{i}, \bT{k})/(1 - \epsilon_j)$ to get that
%%	\begin{equation}
%%	\label{eq:positive}
%%		(1 - a_{ik}a_{jk})\bT{i} - (a_{ij} + a_{ji}a_{ik})\bT{k} - k_{i} - k_{j}a_{ik} = 0
%%%		\left( a_{jk} - \frac{1}{a_{ik}} \right) \bT{i} + a_{ji} \bT{k} + k_j + \frac{1}{a_{ik}}(\bT{k} + k_i) = 0.
%%	\end{equation}
%%	We observe that the coefficient of \bT{i} in~\eqref{eq:positive} must be positive, as all the other terms in~\eqref{eq:positive} are strictly negative.
%%%	We observe that the coefficient of \bT{i} in~\eqref{eq:positive} must be negative, as all the other terms in~\eqref{eq:positive} are strictly positive.
%%%	We observe first that $\bT{i} > 0$.  Furthermore, since every term in~\eqref{eq:positive} is positive, we conclude that the coefficient of \bT{i} in~\eqref{eq:positive} must be negative.
%%
%%\end{proof}

% --------------------------------------------------------
% Lemma: det(A3) > 0
% --------------------------------------------------------

%%\begin{myLemma}
%%\label{lem:det}
%%	Let $i, j$ and $k$ be distinct elements in $\mathcal{U}$.  If $\bar{T} = [\bT{i}, \bT{j}, \bT{k}]^T \succ 0$ satisfies 
%%	$A_3 \bar{T} = k$,  where $A_3$ is given by~\eqref{eq:A3}, and $k = [k_i, k_j, k_k]$ is given by~\eqref{eq:k_i}, then $\det(A_3) > 0$.
%%\end{myLemma}
%%\begin{proof}
%%	We first show that $\det(A_3) \neq 0$ by assuming, by way of contradiction, that $\det(A_3) = 0$.  A trivial determinant can result in a system of equations having either no solution, or infinitely many solutions.  For our specific situation however, we will show that the system of linear equations $A_3 \bar{T} = k$ is inconsistent if $\det(A_3) = 0$.
%%%	a trivial determinant results in an inconsistent system of linear equations, Lemma~\ref{lem:det} shows that only the former is possible.	
%%%	We will show that the system of equations $A_3 \bar{T} = k$ is inconsistent.
%%
%%	For $i \in \mathcal{U}$, let $\alpha_i$ be the $i^{\textrm{th}}$ row of $A_3$.  Now, a trivial determinant implies that for some $\gamma_1$ and $\gamma_2$,
%%	\begin{equation}
%%	\label{eq:alpha}
%%%		\alpha_1 = \gamma_2 \alpha_2 + \gamma_3 \alpha_3.
%%		\alpha_3 = \gamma_1 \alpha_1 + \gamma_2 \alpha_2.
%%	\end{equation}
%%	Equation~\eqref{eq:alpha} is an overspecified system of linear equations, since we have three equations in the two unknown variables $\gamma_1$ and $\gamma_2$.  This in turn imposes an additional constraint for the elements of $A_3$ if~\eqref{eq:alpha} is to be true.  We assume that these additional constraints hold, and notwithstanding this issue, we solve for $\gamma_2$ and $\gamma_3$ to get that 
%%%	$[\gamma_2, \gamma_3]^T = (A_2^{T})^{-1} [-a_{kj}, -a_{ki}]^{T}$
%%	\begin{equation}
%%	\label{eq:gamma}
%%		\begin{bmatrix}
%%			\gamma_1 \\
%%			\gamma_2 
%%		\end{bmatrix} 
%%			= 
%%%		\frac{1}{1 - a_{ik}a_{jk}}
%%%			\begin{bmatrix}
%%%			       	1 & a_{ik}           \\
%%%				a_{jk} & 1  
%%%			\end{bmatrix}
%%%			\begin{bmatrix}
%%%				1 \\
%%%				1
%%%			\end{bmatrix} .
%%		(A_2^{T})^{-1} 
%%		\begin{bmatrix}
%%			-a_{kj} \\
%%			-a_{ki}
%%		\end{bmatrix},
%%	\end{equation}
%%	where $(A_2^{T})^{-1}$ is given by~\eqref{eq:T_equals_two}.  Since each element of $(A_2^{T})^{-1}$ is positive, we conclude from \eqref{eq:a_ik} and \eqref{eq:gamma} that $\gamma_1$ and $\gamma_2$ are both negative (c.f.\ Lemma~\ref{lem:MinToLP} for the case when $|\mathcal{T}| = 2$).
%%%	With similar arguments to the ones found for the case when $|\mathcal{T}| = 2$, we can similarly conclude that $\gamma_1$ and $\gamma_2$ are negative.  Now, let $\bar{T} = [\bT{i}, \bT{j}, \bT{k}]^T$.  
%%
%%	We now proceed towards a contradiction by writing
%%	\begin{eqnarray}
%%	\label{eq:}
%%		k_k &=& \alpha_3 \bar{T} \\
%%			&=& (\gamma_1 \alpha_1 + \gamma_2 \alpha_2) \bar{T} \\
%%			\label{eq:contradiction}
%%			&=& \gamma_1 k_i + \gamma_2 k_j.
%%	\end{eqnarray}
%%	The contradiction results from the fact that by~\eqref{eq:k_i} and~\eqref{eq:gamma}, the left-hand-side of~\eqref{eq:contradiction} is positive, while the right-hand-side of~\eqref{eq:contradiction} is negative.
%%	
%%	Having shown that $\det(A_3)$ is non-zero, we now show that it is, furthermore, positive.  We do this by first inverting the system of linear equations $A_3 \bar{T} = k$ to get that 
%%	\begin{equation}
%%	\label{eq:barT}
%%		\bar{T} = A_3^{-1}k,
%%	\end{equation}
%%	where the transpose of $A_3^{-1}$ is given by \eqref{eq:A3_inverse}.  From~\eqref{eq:k_i}, \eqref{eq:A3_inverse}, \eqref{eq:barT}, and Lemma~\ref{lem:one_minus_a} we see that each element of $\bar{T}$ has the same sign, and so if $\bar{T} \succ 0$, we must have that $\det(A_3) > 0$.
%%	
%%\end{proof}
