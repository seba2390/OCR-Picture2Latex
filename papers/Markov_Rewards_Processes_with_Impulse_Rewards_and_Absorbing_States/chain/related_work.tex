%\section{Related Work}
\section{Introduction}

Markov reward models have been a well-studied area of research for decades~\cite{howard1971dynamicv2} particularly in the literature for performance and dependability~\cite{Trivedi93, Muppala96, Blake1988,GayKetelsen1979,Beaudry78}.  Variations of the problem formulation have primarily been based on whether the Markov chain is discrete-time or continuous-time, whether there are any absorbing states in the state space, and whether rewards are assigned for the occupancy of a state (\emph{rate-based} Markov reward models) or for the transition to a state (\emph{impulse-based} Markov reward models).  Within any problem formulation, there have also been variations on the quantity of interest, with some authors calculating the expected instantaneous reward rate~\cite{Blake1988}, while others find the steady-state expected reward rate~\cite{GayKetelsen1979}, the expected accumulated reward~\cite{Blake1988}, the distribution of the accumulated reward until absorption~\cite{Beaudry78}, etc. (see~\cite{Trivedi93} for a review of the literature for different reward-based measures).  In terms of numerical computation, the topic of model checking for Markov chains has been used to verify whether certain properties hold such as~\cite{HanssonJonsson94}, ``after a request for service there is at least a 98\% probability that the service will be carried out within 2 seconds.''  Such a framework has also been extended for Markov reward models~\cite{KKZ05}.
%different permutations of problem formulations).

%, or whether or not there are absorbing states. 
%and its many variations have been studied for decades~\cite{howard1971dynamicv2} particularly in the literature for performance and dependability~\cite{Trivedi93}.  The variations include continuous and discrete-time markov chains, markov chains with and without absorbing states, and assigning rewards to the states and/or transitions between states.  The goal in previous works has been to calculate the expected rewards at any time, or the distribution of the expected rewards (see~\cite{Trivedi93} for a review on the different permutations of problem formulations).

Surprisingly, one formulation that has gone unstudied is that of finding the expected accumulated reward for a discrete-time Markov reward model with absorbing states and impulse rewards.  The continuous-time counterpart of this problem has been studied~\cite{Trivedi93}.  For a discrete-time model, to the author's knowledge, the results have either involved a steady-state analysis that excludes absorbing states~\cite{howard1971dynamicv2}, or a transient analysis for \emph{rate-based} models that include absorbing states but exclude impulse rewards~\cite{howard1971dynamicv2}.  

% with only state rewards, and without impulse rewards~\cite{howard1971dynamicv2}, the results of which have also been applied to an information theoretic problem in~\cite{GGT}.

Granted, an impulse-based Markov reward model can be translated into a rate-based model by introducing an intermediary state between the transitioning states.  Specifically, suppose that in an impulse-based model, state~$i$ transitions to state~$j$ with probability $\trans{i}{j}$ and a reward of $\rew{i}{j}$ is assigned for such a transition.  Then in the rate-based counterpart to this model, for every such transition, we create an auxiliary state $k$ such that state~$i$ transitions to state~$k$ with probability~\trans{i}{j} and state~$k$ transitions to state~$j$ with probability one.  In this rate-based model, we assign the same reward~\rew{i}{j} for occupying state~$k$ and solve for the expected accumulated reward for rate-based models as in~\cite{howard1971dynamicv2} .  %Such an approach could potentially increase 

While this approach is hypothetically possible, for a state space of size $|\myState|$, such an approach could potentially add an additional $|\myState|^2$ intermediate states if the states in the Markov model form a complete digraph.  As the solution in~\cite{howard1971dynamicv2} involves the inverting of a $|\myState| \times |\myState|$ matrix, the computational cost of this approach could be prohibitive.  In our work, we instead derive a closed-form solution for the expected accumulated reward without having to resort to introducing intermediary states.


%While this approach is hypothetically possible, for a state space of size $|\myState|$, such an approach could potentially add an additional $|\myState| (|\myState| - 1)/2$ states if the states in the Markov chain are fully connected.  As the solution in~\cite{howard1971dynamicv2} involves the inverting of a $|\myState| \times |\myState|$ matrix, the computational cost of this approach could be prohibitive.  In our work, we will instead derive a closed-form solution without having to resort to intermediary states.

%steady-state behaviour 