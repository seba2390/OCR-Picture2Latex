\section{Preliminaries and notations}

\subsection{Revisiting self-attention}

For a given input sequence $\mtx{X} \in \mb R^{n \times d_0}$ of length $n$ and embedding dimension $d_0$,
The dot-product attention for a single head in Transformer~\cite{DBLP:conf/nips/VaswaniSPUJGKP17} is defined as
\begin{equation}
    \nonumber
    \text{Attention}(\mtx{Q},\mtx{K},\mtx{V}) = \text{softmax}\left(\frac{\mtx{Q} \mtx{K}^{T}}{\sqrt{p}}\right) \mtx{V}
\end{equation}
where $\mtx{Q} = \mtx{X} \mtx{W}_Q$, $\mtx{K} = \mtx{X} \mtx{W}_K$, and $\mtx{V} = \mtx{X} \mtx{W}_V$,
and $\mtx{W}_Q$, $\mtx{W}_K$ and $\mtx{W}_V$ are the query, key, and value weight metrics that linearly project the input $\mtx X$ of $d_0$ dimension to an output tensor of $p$ dimensions.

To simplify the future analysis, the left softmax term can be rewritten into $\mtx{D}^{-1} \mtx{A}$,
where $\mtx{A} \defeq \exp(\mtx{Q} \mtx{K}^{T} / \sqrt{p})$ is the un-normalized attention score matrix; 
$\mtx{D}$ is a diagonal matrix whose diagonal is $\exp(\mtx{Q} \mtx{K}^{T} / \sqrt{p}) \cdot \mtx{1}$ (by convention $\mtx{1}$ is a size-$n$ vector with all elements being $1$).
Following the notation in Performer \citep{DBLP:journals/corr/abs-2009-14794},
we define $\text{SM}(\mtx{q}, \mtx{k}) \defeq \exp(\mtx{q}^T \mtx{k} / \sqrt{p})$ as the softmax kernel function,
and represent $\mtx{A}$ by the notation $\text{SM}(\mtx{Q}, \mtx{K})$, which means the element $a_{ij}$ from the $i$-th row and $j$-th column in $\mtx{A}$ is equal to $\text{SM}(\mtx{q}_i, \mtx{k}_j)$.
Throughout this paper $\mtx{q}_i$ (resp. $\mtx{k}_j$) means the $i$-th (resp. $j$-th) row in $\mtx{Q}$ (resp. $\mtx{K}$).

We close this subsection with a short lemma to show $\text{SM}(\cdot, \cdot)$ is a positive semidefinite (PSD) kernel function \citep[Definition~12.6]{wainwright2019high} by relating it to Gaussian kernels.
\begin{lem}
$\text{SM}(\cdot, \cdot)$ is a PSD kernel function. 
Equivalently, for all integers $n \geq 1$ and elements $\{\mtx{q}_i\}_{i=1}^n \subseteq \mb R^{p}$,
the $n$-by-$n$ matrix $\mtx{C} = \text{SM}(\mtx{Q}, \mtx{Q})$ is PSD.
\end{lem}
\begin{proof}
We first state an important equation to connect the softmax kernel and Gaussian kernels as follows:
\begin{align*}
\text{SM}(\mtx{q}_i, \mtx{q}_j) = \exp\left(\frac{\mtx{q}_i^T \mtx{q}_j}{\sqrt{p}}\right) 
= \exp\left(\frac{\|\mtx{q}_i\|^2}{2 \sqrt{p}}\right) \exp\left(-\frac{\|\mtx{q}_i - \mtx{q}_j\|^2}{2 \sqrt{p}}\right) \exp\left(\frac{\|\mtx{q}_j\|^2}{2 \sqrt{p}}\right).
\end{align*}
The middle part $\exp\left(\frac{\|\mtx{q}_i - \mtx{q}_j\|^2}{2 \sqrt{p}}\right)$ is exactly a Gaussian kernel with bandwidth $p^{\frac14}$.
(\citet{DBLP:journals/corr/abs-2009-14794} have more discussion on the findings.)

Through this equation, we can rewrite $\mtx{C}$ as
\begin{align}
\label{eqn:sm_rbf}
\mtx{C} = \mtx{D}_Q^{1/2} \cdot \kappa\left(\frac{\mtx{Q}}{p^{1/4}}, \frac{\mtx{Q}}{p^{1/4}} \right) \cdot \mtx{D}_Q^{1/2},
\end{align}
where $\mtx{D}_Q$ is a diagonal matrix with elements $(\mtx{D}_Q)_{ii} = \exp\left(\frac{\|\mtx{q}_i\|^2}{\sqrt{p}}\right), \forall i \in [n]$,
and $\kappa(\mtx{q}_i, \mtx{q}_j) \defeq \exp\left(-{\|\mtx{q}_i - \mtx{q}_j\|^2}/{2}\right)$ is the standard Gaussian kernel function.

We prove the lemma by using the fact that $\kappa$ is a PSD kernel and $\kappa\left(\frac{\mtx{Q}}{p^{1/4}}, \frac{\mtx{Q}}{p^{1/4}} \right)$ is a PSD matrix.
\end{proof}


\subsection{\nystrom method}

Due to the intrinsic low-rankness of an empirical kernel matrix $\mtx{B}$, the so-called \nystrom method that replaces $\mtx{B}$ with its low-rank approximation $\tilde{\mtx{B}}$, has been applied to accelerate kernel methods \citep{gittens2016revisiting, kumar2009sampling, williams2001using}. 
% statistical dimension (eigenvalue decaying rate of empirical kernel matrix.)
Specifically, the \nystrom approximation of $\mtx{B}$ is the matrix $\tilde{\mtx{B}} = \mtx{B} \mtx{S}(\mtx{S}^T \mtx{B} \mtx{S})^\dagger \mtx{S}^T \mtx{B}$,
where $(\cdot)^\dagger$ denotes the Moore-Penrose pseudoinverse of a matrix, and $\mtx{S} \in \mb R^{n \times d}$ is a zero-one sub-sampling matrix whose columns are a subset of the columns in $\mtx{I}$, indicating which $d$ observations have been selected. 
The formal definition of the uniform sub-sampling matrix is given as follows:
\begin{definition}[Uniform sub-sampling matrix]
\label{def:subsampling}
% Consider a uniform discrete distribution which draws $i$ with probability $\frac1n$.
For a random matrix $\mtx{S} \in \mb R^{n \times d}$, if $\mtx{S}$ has i.i.d. columns and the $j$-th column $\mtx{S}^{(j)}$ can randomly be $\sqrt{\frac{1}{d}} \mtx{e}_i$ with probability $\frac1n$,
where $\mtx{e}_i$ is the $i$-th column of the $n$-by-$n$ identity matrix $\mtx{I}_n$, 
then $\mtx{S}$ is called a uniform sub-sampling.
\end{definition}

We close this subsection with a remark that it is not appropriate to directly extend the \nystrom method from kernel method to self-attention due to a core requirement that $\mtx{B}$ should be PSD with consideration of approximation performance improvement.
% , which is critical in improving the approximation performance.
We will show in the next section how to address this challenge and properly adapt \nystrom method to non-PSD matrices.

% \heng{maybe it would be good to add a table to summarize the pros and cons of these methods and your solution}

\subsection{Approximation evaluation}

Beyond the time and space complexity, attention acceleration methods have been mostly evaluated with empirical experiment results, such as the perplexity of pretrained language models and the fine-tuned performance on downstream natural language understanding tasks. 
Specifically, Long Range Arena benchmark~\citep{DBLP:journals/corr/abs-2011-04006} has been proposed to systematically evaluate the performance of efficient transformers with ten NLP tasks in long-context scenarios.
However, such empirical results are indirect for theoretical analysis.
% Many different methods have been proposed to approximate the self-attention.
% However, some of the existing methods solely evaluate the approximation performance by the downstream transformer model accuracy, 
% which is indirect and hard to perform theoretical analysis.
Therefore, we introduce a common criterion used in matrix approximation, spectral norm, to ease the future discussion on performance.
\begin{definition}[Spectral norm guarantee for matrix approximation (MA)]
\label{def:ma}
Given a matrix $\mtx{M} \in \mb R^{n_1 \times n_2}$, two constants $\varepsilon > 0, \delta < \frac12$, we say that its approximation matrix $\wt{\mtx{M}} \in \mb R^{n_1 \times n_2}$ satisfies $(\varepsilon, \delta)$-MA property for $\mtx{M}$, if
\begin{align}
\Prob{\|\mtx{M} - \wt{\mtx{M}}\| > \varepsilon \|\mtx{M}\|} < \delta.
\end{align}
\end{definition}

In previous works, the direct analysis of the approximation error to the entire output $\mtx{D}^{-1} \mtx{A} \mtx{V}$ in the $(\varepsilon, \delta)$-MA manner is usually spared due to the difficulty caused by the complex softmax structure.
In this paper, with the new kernelized attention, we are allowed to perform the analysis through the existing theoretical results in kernel methods.
Consequently, in Section~\ref{sec:norm} we are able to give a relatively precise error analysis on the approximation of Skyformer to the entire kernelized attention,
which eases the future comparison with other methods approximating kernelized attention.
% focus on the un-normalized attention score matrix $\mtx{A}$ study the $(\varepsilon, \delta)$-MA property for ,
% and from this perspective we are give precisely in Section~\ref{sec:related_work}.
% Concretely, in Section~\ref{sec:norm} we evaluate the approximation performance for the whole self-attention $\mtx{D}^{-1} \mtx{A} \mtx{V}$ from real BERT models.