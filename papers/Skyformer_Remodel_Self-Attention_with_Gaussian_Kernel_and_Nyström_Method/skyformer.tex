\documentclass{article}

% if you need to pass options to natbib, use, e.g.:
%     \PassOptionsToPackage{numbers, compress}{natbib}
% before loading neurips_2021

% ready for submission
% \usepackage{neurips_2021}

% to compile a preprint version, e.g., for submission to arXiv, add add the
% [preprint] option:
    % \usepackage[preprint]{neurips_2021}

% to compile a camera-ready version, add the [final] option, e.g.:
    \usepackage[final]{neurips_2021}

% to avoid loading the natbib package, add option nonatbib:
%    \usepackage[nonatbib]{neurips_2021}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{paperpkg}

% \usepackage{xr}
% \externaldocument{sky_supp}

\input{init.tex}

% \title{Sketched Attention for Fast Transformer}
% \title{Accelerate Attention with Nystr\"om Method\\ for Long Document}
% \title{Spectral Analysis and Fast Approximation of Self-attention for Long Documents}
\title{Skyformer: Remodel Self-Attention with Gaussian Kernel and Nystr\"om Method}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to LaTeX to determine where to break the
% lines. Using \AND forces a line break at that point. So, if LaTeX puts 3 of 4
% authors names on the first line, and the last on the second line, try using
% \AND instead of \And before the third author name.

\author{
Yifan Chen\thanks{\; Equal contribution. },~   
Qi Zeng\footnotemark[1],~ 
Heng Ji,~ 
Yun Yang \\ 
University of Illinois Urbana-Champaign \\
% \email{\texttt{\{yifanc10, qizeng2,  hengji, yy84\}@illinois.edu }}
\texttt{\{yifanc10, qizeng2,  hengji, yy84\}@illinois.edu }
}



\begin{document}

\maketitle


\begin{abstract}
\input{000abstract}
\end{abstract}


\input{010introduction}
\input{020related}
\input{030preliminary}
\input{040method}

\input{060experimental}
\input{070conclusion}


\begin{ack}

This research is based upon work in part supported by the Office of the Director of National Intelligence (ODNI), Intelligence Advanced Research Projects Activity (IARPA), via contract No. FA8650-17-C-9116, and U.S. DARPA KAIROS Program No. FA8750-19-2-1004. This work is also in part supported by NSF grant DMS-1810831. The views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the official policies, either expressed or implied, of DARPA, ODNI, IARPA, or the U.S. Government. The U.S. Government is authorized to reproduce and distribute reprints for governmental purposes notwithstanding any copyright annotation therein.


\end{ack}

\clearpage
\bibliographystyle{plainnat}
\bibliography{ref}
\clearpage
\appendix

\section{Validation loss}
\label{sec:dev_loss}
\input{figures/lra_devloss}

Figure~\ref{fig:devloss} shows the validation loss changes with respect to training time for $50$k steps as supplementary results for the experiments in Section~\ref{sec:exp}.
In general, Skyformer converges faster and finishes $50$k steps earlier than vanilla Attention and Kernelized Attention over all tasks.
We further remark that on Text Classification, all models quickly fall into over-fitting, and thus the validation losses rise quickly.
On Pathfinder, due to the difficulty of training, in the trial shown in the figure vanilla Attention fails to reach the best long-time limit under a certain setting.



\section{Singular value decay rate}
\input{figures/svd}


% We propose to measure the task difficulty with the singular value decay rate in attention output. 


Figure~\ref{fig:svd} shows the singular value distribution of attention output from the second layer of a trained vanilla transformer.
Results are averaged across one random batch from the test set in each LRA task. 

The singular values decay fast and thus justify the low-rank approximation, as analyzed by \citet{DBLP:journals/corr/abs-2006-04768, dong2021attention}.
We propose to measure the task difficulty with the singular value decay rate in attention output, as higher intrinsic task difficulty forces the model to output a matrix with more large singular values.
Such matrices are considered more informative since they are harder to approximate, requiring more ranks even in the truncated SVD approximation.
With the observation in Figure~\ref{fig:svd}, we conclude that the singular values in Document Retrieval and Pathfinder tasks decay slower, and those two tasks are more difficult than Text Classification and ListOps.
% We suggest to focus on those two tasks as a metric to evaluate the performance of efficient transformers to capture long-distance dependency.




\section{Useful facts}
\label{sec:facts}

This section introduces some useful facts, which are key in the proof in the next section. 
To start with, we provide a matrix concentration inequality as follows.
% The following theorem is copied from the tutorial \citep[Theorem~1.4,~1.6]{tropp2012user} for the reader's convenience.
\begin{lemma}[Matrix Bernstein Inequality \citep{tropp2012user}]
\label{lem:intro-bernstein}
Consider a finite sequence $\{ \mtx{X}_k \}$ of independent, random, self-adjoint matrices with dimension $n$.  
Assume that each random matrix satisfies
$$
\Expect \mtx{X}_k = \mtx{0}
\quad\text{and}\quad
\| \mtx{X}_k \| \leq R
\quad\text{almost surely}.
$$
Then, for all $t \geq 0$,
$$
\Prob{ \| \sum\nolimits_k \mtx{X}_k \| \geq t }
	\leq 2n \cdot \exp\left( \frac{-t^2/2}{\sigma^2 + Rt/3} \right)
	\quad\text{where}\quad
	\sigma^2 \geq \norm{ \sum\nolimits_k \Expect \big(\mtx{X}_k^2 \big) }.
$$
\end{lemma}

For a certain $n$-by-$n$ orthogonal matrix $\mtx{H}$ ($\mtx{H} \mtx{H}^T$ is a diagonal matrix) 
and an $n$-by-$d$ uniform sub-sampling matrix $\mtx{S}$ (as defined in Definition~\ref{def:subsampling} in the main paper),
we denote the sketching matrix $\mtx{\Pi} \defeq \sqrt{n} \mtx{S}$.
We aim to show $\mtx{H} \mtx{\Pi} \mtx{\Pi}^T \mtx{H}^T$ can satisfy $(\frac12, \delta)$-MA property for $\mtx{H} \mtx{H}^T$ by the following lemma.
\begin{lemma}
\label{lem:frac12MA}
Denote the stable rank $s \defeq \frac{\|\mtx{H}\|_F^2}{\|\mtx{H}\|^2} \geq 1$, and a constant $\delta < 1/2$. 
Suppose there exists a constant $\beta \in (0, 1]$ such that $\beta \leq \frac{\|\mtx{H}\|_F^2}{n \|\mtx{H}^{(i)}\|^2}, \forall i = 1,\dots,n$,
where $\mtx{H}^{(i)}$ is the $i$-th column of $\mtx{H}$. 
There exists a constant $C_0$ that if 
\begin{align*}
d \geq C_0 \frac{s}{\beta} \log \frac{n}{\delta}, 
\end{align*}
then $\mtx{H} \mtx{\Pi} \mtx{\Pi}^T \mtx{H}^T$ satisfies $(\frac12, \delta)$-MA property for $\mtx{H} \mtx{H}^T$.
\end{lemma}

\begin{proof}
The main idea is to utilize Lemma~\ref{lem:intro-bernstein} by setting $t = \frac12 \|\mtx{H} \mtx{H}^T\| = \frac12 \|\mtx{H}\|^2$.
Specifically, we denote the matrices 
\begin{align*}
\mtx{X}_k &= \mtx{H} \mtx{\Pi}^{(i)} \left(\mtx{\Pi}^{(i)}\right)^T \mtx{H}^T - \frac1d \mtx{H} \mtx{H}^T, \quad \text{so that} \\
\sum\nolimits_k \mtx{X}_k &= \mtx{H} \mtx{\Pi} \mtx{\Pi}^T \mtx{H}^T - \mtx{H} \mtx{H}^T.
\end{align*}

% To prove the lower bound of $d$ in the lemma, 
We still need two steps to give control of $R$ and $\sigma^2$.
For $R$, we have
\begin{align*}
\|\mtx{X}_k\| &= \norm{\frac1d \sum_{i=1}^n (n z_{ki} -1) \mtx{H}^{(i)} \left(\mtx{H}^{(i)}\right)^T}
\leq \frac1d \max \left\{\max_i (n - 1) \|\mtx{H}^{(i)}\|^2, \|\mtx{H}\|^2 \right\} \\
&\leq \frac1d n \max_i \|\mtx{H}^{(i)}\|^2,
\end{align*}
where $\{z_{ki}\}_{i=1}^n$ are the indicators of whether the $i$-th column is chosen.
The first inequality of the preceding display holds due to the fact that $\mtx{H}$ is an orthogonal matrix.
Using the condition $n \leq \frac{\|\mtx{H}\|_F^2}{\beta \|\mtx{H}^{(i)}\|^2}, \forall i = 1,\dots,n$, we further have
\begin{align*}
\|\mtx{X}_k\| \leq \frac{\|\mtx{H}\|_F^2}{d \beta},
\end{align*}
and we thus set $R \defeq \frac{\|\mtx{H}\|_F^2}{d \beta}$.
On the other hand,
\begin{align*}
\Expect \mtx{X}_k^2 
= \frac{1}{d^2} \sum_{i=1}^n \Expect \left( (n z_{ki} -1)^2 \right) \|\mtx{H}^{(i)}\|^2 \mtx{H}^{(i)} \left(\mtx{H}^{(i)}\right)^T
= \frac{1}{d^2} \sum_{i=1}^n (n-1) \|\mtx{H}^{(i)}\|^2 \mtx{H}^{(i)} \left(\mtx{H}^{(i)}\right)^T.
\end{align*}
Again using the condition that $n \|\mtx{H}^{(i)}\|^2 \leq \frac{\|\mtx{H}\|_F^2}{\beta}, \forall i = 1,\dots,n$, we reach
\begin{align*}
\norm{\Expect \sum_{k=1}^d \mtx{X}_k^2} \leq \frac1d \frac{\|\mtx{H}\|_F^2}{\beta} \norm{\mtx{H} \mtx{H}^T} 
= \frac{\|\mtx{H}\|_F^2}{d \beta} \norm{\mtx{H}}^2,
\end{align*}
and set $\sigma^2 \defeq \frac{\|\mtx{H}\|_F^2}{d \beta} \norm{\mtx{H}}^2$.

Finally we plug $R$ and $\sigma^2$ into Lemma~\ref{lem:intro-bernstein} and obtain:
\begin{align*}
\Prob{ \norm{\mtx{H} \mtx{\Pi} \mtx{\Pi}^T \mtx{H}^T - \mtx{H} \mtx{H}^T} \geq \frac12 \|\mtx{H}\|^2}
	\leq 2n \cdot \exp\left( \frac{-\|\mtx{H}\|^4/8}{\frac{s \|\mtx{H}\|^4}{d \beta} + \frac{s \|\mtx{H}\|^4}{6 d \beta}} \right).
\end{align*}
To ensure the right-hand-side is smaller than $\delta$, we just need
\begin{align*}
d \geq \frac{28}{3} \frac{s}{\beta} \log \frac{2n}{\delta},
\end{align*}
which validates the lemma.
\end{proof}


\section{Proof of Theorem~\ref{thm:tilde_K} in the main paper}
\label{sec:thm_error}

\begin{proof}
The conclusion in the lemma can be divided into two parts, 
that $\tilde{\bar{\mtx{C}}} \psdle \bar{\mtx{C}}$ 
and $\bar{\mtx{C}} \psdle \tilde{\bar{\mtx{C}}} + \lambda \mtx{I}$.
To prove them we first introduce some notations and auxiliary results.
Since $\bar{\mtx{C}}$ is PSD, there exists a matrix $\mtx{B}$ satisfying $\mtx{B} \mtx{B}^T = \bar{\mtx{C}}$.
We further denote $\mtx{B}$'s SVD decomposition as $\mtx{B} = \mtx{U} \mtx{\Sigma}^{\frac12} \mtx{V}^T$ 
($\bar{\mtx{C}} = \mtx{U} \mtx{\Sigma} \mtx{U}^T$),
where both $\mtx{U}$ and $\mtx{V}$ are $2n$-by-$2n$ orthonormal matrices.
(In this section we slightly abuse the notation that $\mtx{V}$ represents the matrix of right-singular vectors, instead of the value matrix in self-attention.)
Define $\bar{\mtx{\Sigma}} \defeq \mtx{\Sigma} + \lambda \mtx{I}, \mtx{\Psi} \defeq \mtx{U} \mtx{\Sigma}^{\frac12} \bar{\mtx{\Sigma}}^{-\frac12}$, which implies $\bar{\mtx{C}} (\bar{\mtx{C}} + \lambda \mtx{I})^{-1} = \mtx{\Psi} \mtx{\Psi}^T$.
Also following the notations in the last section,
we define the $2n$-by-$d$ matrix $\mtx{\Pi} \defeq \sqrt{2n} \mtx{S}$
With those notations, $\tilde{\bar{\mtx{C}}}$ can be rewritten as $\mtx{B} \mtx{B}^T \mtx{\Pi} (\mtx{\Pi}^T \mtx{B} \mtx{B}^T \mtx{\Pi})^\dagger \mtx{\Pi} \mtx{B} \mtx{B}^T = \mtx{B} \mtx{P_\Pi} \mtx{B}^T$,
where $\mtx{P_\Pi}$ is the orthogonal projection matrix for the column space of $\mtx{B}^T \mtx{\Pi}$.
It is easy to check that $\bar{\mtx{C}} - \tilde{\bar{\mtx{C}}} = \mtx{B} (\mtx{I} - \mtx{P_\Pi}) \mtx{B}^T$.
Since $\mtx{I} - \mtx{P_\Pi}$ is an orthogonal projection matrix (which is PSD), 
we have $\bar{\mtx{C}} - \tilde{\bar{\mtx{C}}} \psdge \mtx{0}$,
which proves the first conclusion that $\tilde{\bar{\mtx{C}}} \psdle \bar{\mtx{C}}$.

For the second conclusion, we utilize the following important identity:
\begin{align*}
\mtx{B}^T \mtx{\Pi} \mtx{\Pi}^T \mtx{B} - \mtx{B}^T \mtx{B} 
&= \mtx{V} \bar{\mtx{\Sigma}}^{\frac12} \big( \bar{\mtx{\Sigma}}^{-\frac12} \mtx{V}^T (\mtx{B}^T \mtx{\Pi} \mtx{\Pi}^T \mtx{B} - \mtx{B}^T \mtx{B}) \mtx{V} \bar{\mtx{\Sigma}}^{-\frac12} \big) \bar{\mtx{\Sigma}}^{\frac12} \mtx{V}^T \\
&= \mtx{V} \bar{\mtx{\Sigma}}^{\frac12} \big( \mtx{\Psi}^T \mtx{\Pi} \mtx{\Pi}^T \mtx{\Psi} - \mtx{\Psi}^T \mtx{\Psi} \big) \bar{\mtx{\Sigma}}^{\frac12} \mtx{V}^T.
\end{align*}
For $\mtx{\Psi}$, we have that its squared Frobenius norm $\|\mtx{\Psi}\|_F^2 = d_{stat}$, and $\|\mtx{\Psi}\|^2 
= \|\bar{\mtx{C}} (\tilde{\bar{\mtx{C}}} + \lambda \mtx{I})^{-1}\| \geq 1/2$,
indicating that $\mtx{\Psi}$'s stable rank $s = \norm{\mtx{\Psi}}_F^2 / \norm{\mtx{\Psi}}^2$ is at most $2 d_{stat}$.

Taking $\varepsilon = \frac12$ and applying Lemma~\ref{lem:frac12MA}, we can conclude that with the conditions on $d$ in the theorem, 
$\mtx{\Psi}^T \mtx{\Pi} \mtx{\Pi}^T \mtx{\Psi}$ satisfies $(\frac12, \delta)$-MA property for $\mtx{\Psi}^T \mtx{\Psi}$.
Therefore it holds with probability $1-\delta$ that, 
\begin{align*}
\|\mtx{\Psi}^T \mtx{\Pi} \mtx{\Pi}^T \mtx{\Psi} - \mtx{\Psi}^T \mtx{\Psi}\| 
\leq \frac12 \|\mtx{\Psi}\|^2
\leq \frac12.
\end{align*}
From identity $\mtx{V} \bar{\mtx{\Sigma}}^{\frac12} \bar{\mtx{\Sigma}}^{\frac12} \mtx{V}^T = \mtx{B}^T \mtx{B} + \frac{\lambda}{2} \mtx{I}$, we obtain
\begin{align}
\frac12 \mtx{B}^T \mtx{B} - \frac{\lambda}{2} \mtx{I} \psdle \mtx{B}^T \mtx{\Pi} \mtx{\Pi}^T \mtx{B} 
\psdle \frac32 \mtx{B}^T \mtx{B} + \frac{\lambda}{2} \mtx{I},
\end{align}
which implies
\begin{align}
\label{eqn:BTB_bound}
\mtx{B}^T \mtx{B} \psdle 2 \mtx{B}^T \mtx{\Pi} \mtx{\Pi}^T \mtx{B} + \lambda \mtx{I}.
\end{align}

Finally, we multiply two sides of Eq.~(\ref{eqn:BTB_bound}) by $(\mtx{I} - \mtx{P_\Pi})$ to obtain
\begin{align*}
(\mtx{I} - \mtx{P_\Pi}) \mtx{B}^T \mtx{B} (\mtx{I} - \mtx{P_\Pi}) \psdle 2 \cdot \mtx{0} + \lambda (\mtx{I} - \mtx{P_\Pi}) \psdle \lambda \mtx{I},
\end{align*}
where the second inequality is due to the fact that $(\mtx{I} - \mtx{P_\Pi})$ is an orthogonal projection matrix.
The equation above implies $\|(\mtx{I} - \mtx{P_\Pi}) \mtx{B}^T \mtx{B} (\mtx{I} - \mtx{P_\Pi})\| = \|\mtx{B} (\mtx{I} - \mtx{P_\Pi}) \mtx{B}^T\| \leq \lambda$, 
which completes the proof for the second conclusion $\bar{\mtx{C}} \psdle \tilde{\bar{\mtx{C}}} + \lambda \mtx{I}$.

Based on the conclusion above, the last implication is direct:
\begin{align*}
\|\tilde{\mtx{C}} - \mtx{C}\| = \norm{(\mtx{I}, \mtx{0}) \left( \tilde{\bar{\mtx{C}}} - \bar{\mtx{C}} \right) (\mtx{0}, \mtx{I})^T}
\leq \norm{(\mtx{I}, \mtx{0})} \norm{\left( \tilde{\bar{\mtx{C}}} - \bar{\mtx{C}} \right)} \norm{(\mtx{0}, \mtx{I})^T}
\leq \lambda = \varepsilon \|\mtx{C}\|,
\end{align*}
which completes the proof.
\end{proof}


\section{Proof of Lemma~\ref{lem:iterative} in the main paper}
\label{sec:lem_iter}

\begin{proof}
As $\bar{\mtx{C}}$ is constructed based on a PSD kernel, $\bar{\mtx{C}}$ is also PSD.
Consequently $\mtx{M} = \mtx{S}^{T} \bar{\mtx{C}} \textbf{S}$ is PSD, and $\mtx{D}_M^{-1/2} (\mtx{M} + \gamma \mtx{I}) \mtx{D}_M^{-1/2}$ is positive definite, with all eigenvalues positive.
To prove the claim in the lemma we only need to show the eigenvalues of $\mtx{D}_M^{-1/2} (\mtx{M} + \gamma \mtx{I}) \mtx{D}_M^{-1/2}$ are bounded from above by $1$.
It is equivalent to prove that $\mtx{I} - \mtx{D}_M^{-1/2} (\mtx{M} + \gamma \mtx{I}) \mtx{D}_M^{-1/2}$ is PSD,
which can be induced by another statement that $\mtx{L} \defeq \mtx{D}_M - (\mtx{M} + \gamma \mtx{I})$ is PSD.

The proof of the statement above is similar to the proof of the well-known conclusion that graph Laplacian matrix is PSD.
For simplicity we denote $\mtx{W} \defeq \mtx{M} + \gamma \mtx{I}$, and given any vector $\mtx{x} \in \mb R^d$ we have
\begin{align*}
\mtx{x}^T \mtx{L} \mtx{x} &= \mtx{x}^T \mtx{D}_M \mtx{x} - \mtx{x}^T \mtx{W} \mtx{x}
= \sum_{i=1}^d (\mtx{D}_M)_{ii} \mtx{x}_i^2 - \sum_{i,j=1}^d \mtx{W}_{ij} \mtx{x}_i \mtx{x}_j \\
&= \frac12 \left( \sum_{i=1}^d (\mtx{D}_M)_{ii} \mtx{x}_i^2 - 2\sum_{i,j=1}^d \mtx{W}_{ij} \mtx{x}_i \mtx{x}_j + \sum_{j=1}^d (\mtx{D}_M)_{jj} \mtx{x}_j^2 \right) \\
&= \frac12 \sum_{i,j=1}^d \mtx{W}_{ij} (\mtx{x}_i - \mtx{x}_j)^2 \geq 0,
\end{align*}
where the last equation holds due to the fact that $(\mtx{D}_M)_{ii} = \sum_{j=1}^d \mtx{W}_{ij}$.

Combining the pieces above we can conclude that $\|\mtx{I} - \mtx{D}_M^{-1/2} (\mtx{M} + \gamma \mtx{I}) \mtx{D}_M^{-1/2}\| < 1$.
\end{proof}


\section{Additional discussions about the stability in model training}
\label{sec:exp_stability}

For our argument about stability, we mainly refer to the paper~\citep{DBLP:conf/emnlp/LiuLGCH20}, 
which identifies that the amplification of small parameter perturbations in the self-attention module is the root cause of training instability. 
We take kernelized attention as mitigation since it contains an automatic normalization. 
We have empirically used Figure~\ref{fig:devacc} and Figure~\ref{fig:devloss} in Appendix~\ref{sec:dev_loss} to support our claim. 

For further analysis we conduct a toy experiment adapting from Figure~4 in the aforementioned paper~\citep{DBLP:conf/emnlp/LiuLGCH20}.
We aim to show that in kernelized attention (and Skyformer) the output changes $f(x, W^*) - f(x, W)$ for parameter changes $W^* - W$ is smaller than in self-attention (and its approximation Nystr\"omformer). 
This concept involved is somewhat similar to condition number and below we will formalize it as ``instability score".

We show a table of the averaged ratios between the instability scores of kernelized attention (we also add Skyformer and Nystr\"omformer for reference) and self-attention to conclude our statement about stability. 
A ratio smaller than $1$ means higher stability compared to self-attention. 
We follow all the settings in Table~\ref{table:lra_acc} in the main paper except here we only update the model for 20 steps (we limit the number of steps as suggested by \citet{DBLP:conf/emnlp/LiuLGCH20} to make the results of the same step comparable among different models). 
In step $i$ for each model we compute the instability score $\tau_i = \frac{\|f(x_i, W_i) - f(x_i, W_{i-1})\|^2_F}{\|W_i - W_{i-1}\|^2_F}, i=1, \cdots, 20$, 
where $f()$ gives the embedding after two layers, $x_i$ is the $i$-th input sequence batch, $W_0$ represents the initial parameters, and $W_i$ represents the parameters after step $i$. 
In each step we compute the ratio of a certain method’s $\tau_i$ to the $\tau_i$ of self-attention, and finally average the $20$ ratios in Table~\ref{table:ratio_instability} in the appendix. 

As we can observe, both kernelized attention and Skyformer consistently have a lower instability score than self-attention, 
while the instability score of Nystr\"omformer, an approximation to self-attention, fluctuates around $1$ in all the tasks.
The results support our claim that the proposed kernelized attention can improve stability.

\begin{table}[t]
\caption{Ratios of instability score on LRA benchmark.}
\label{table:ratio_instability}
\centering
\begin{tabular}{lccccc}
\toprule
Model & Text    & ListOps & Retrieval & Pathfinder &  Image \\
\midrule
Nystr\"omformer         & 1.03 & 1.01 &	0.97 & 	0.99 & 1.02	\\
Kernelized Attention    & 0.83 & 0.77 & 0.64 & 	0.74 & 0.62	\\
Skyformer               & 0.81 & 0.79 &	0.64 & 	0.79 & 0.65 \\
\bottomrule
\end{tabular}
\end{table}


\end{document}

