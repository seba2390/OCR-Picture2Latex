\section{Experimental Results}
\label{sec:exp}

\input{figures/lra_devacc}

\input{tables/lra}

% In this section our goal is to show whether the proposed method is sufficient in getting comparable performance to the full self-attention with much fewer computation resources. 

% We first compute the spectrum norm, a common evaluation metric and indicator of matrix multiplication approximation, to test the effectiveness of self-attention approximation comparing to other low-rank approximation methods as a sanity check.
% Next we consider two representative use cases of pretrained transformer-based language models, pre-training a Masked Language Model and fine-tuning a pretrained RoBERTa model on downstream tasks.
% , as the evidence of whether our proposed approximation method achieves comparable performance to the original self-attention with significant smaller computation cost.
% Next, we exam the performance on Long Range Arena (LRA) benchmark, which focuses on model quality under long-context scenarios.  

\textbf{Tasks and Datasets.}
We evaluate the proposed methods on five classification tasks on LRA benchmark~\citep{DBLP:journals/corr/abs-2011-04006}, which focuses on model quality under long-context scenarios: ListOps \citep{DBLP:conf/naacl/NangiaB18}, Text Classification on IMDb review dataset \citep{DBLP:conf/acl/MaasDPHNP11}, Document Retrieval on AAN dataset \citep{DBLP:journals/lre/RadevMQA13}, Pathfinder \citep{DBLP:conf/nips/LinsleyKVWS18}, and Image Classification on CIFAR-10 \citep{krizhevsky2009learning}.
The LRA benchmark covers diverse long-sequence tasks in sequence length, task difficulty, and inspected model abilities. 
For example, ListOps and Pathfinder evaluate the abilities to capture the long-range hierarchical dependency and spatial dependency, respectively, which poses challenges for sparse attention pattern based methods.
We report the classification accuracy on the test set, training time, and peak memory usage during training for each task.


\textbf{Baselines.}
Aside from the vanilla quadratic self-attention, we compare with Big Bird~\citep{DBLP:conf/nips/ZaheerGDAAOPRWY20}, Performer~\citep{DBLP:journals/corr/abs-2009-14794}, Linformer~\citep{DBLP:journals/corr/abs-2006-04768}, Nystr\"omformer \citep{DBLP:journals/corr/abs-2102-03902}, Informer~\citep{DBLP:journals/corr/abs-2012-07436},
and Reformer~\citep{DBLP:conf/iclr/KitaevKL20}.
Most methods are approximating the vanilla full attention for efficiency and thus are not expected to have better performance.
As it is not realistic to exhaustively fine-tune all models and search for the best performance under limited computation resources, we instead only replace the self-attention module with the various attention methods and keep other experimental settings the same for fair comparisons. 




% \heng{change 'almost impossible' to 'not realistic'}
% \heng{suggest to change 'aggressively' to exhaustively' because the former is too subjective about attitude }

% goals of exp:
% step num needed to converge, 
% less training time and space, 
% not necessarily better performance,
% longer seq may benefit some tasks

% hyper selection - number of features (two lengths)

% To show the approximation method really improves the efficiency, we need to train the models until convergence, and report the steps and time needed.
% Otherwise, it is possible that original transformer though needs more time in one step, but it needs less total steps, and finally instead requires less time.















\textbf{Implementation Details.}
We conduct each experiment on one Tesla V100 SXM2 16GB.
We use the LRA evaluation benchmark reimplemented in PyTorch by~\citet{DBLP:journals/corr/abs-2102-03902}.
We use a 2-layer transformer model with $64$ embedding dimension, $128$ hidden dimension, $2$ attention heads, and mean pooling for classification. 
Batch size is selected conditioned on the memory requirements of the standard self-attention method, which leads to $16$ for Text Classification, $32$ for ListOps, $16$ for Document Retrieval, $128$ for Pathfinder, and $256$ for Image Classification.
Learning rate is set to $1e-4$ for Text Classification, ListOps, and Image Classification, and $2e-4$ for Retrieval and Pathfinder.
Each model on each task is trained for $50k$ steps, during which the best checkpoint with the highest accuracy on the development set will be saved for evaluation.
For comparable computation complexity, we control the number of features to be $128$ used in all methods (except Big Bird), under which setting the models will visit $128\cdot n$ elements in the attention matrix.
For numerical consistency, all experiment results are averaged across three runs with different random seeds.


We do not follow all settings in \citep{DBLP:journals/corr/abs-2102-03902} due to the hardware limitation.
The compromises, such as approximation dimension and gradient accumulations steps, might bring performance differences comparing to results reported in \citep{DBLP:journals/corr/abs-2102-03902}.
The training instability problem also helps explain the performance gap.



\textbf{Results.}
The training process of the standard softmax-based method is unstable as observed in Figure~\ref{fig:devacc}: it takes more steps to reach the stationary distribution of its long-time limit, and it is more easily getting stuck in a local minimum. 
Runs with different random seeds may bring divergent performances, and probably leads to lower averaged scores.
We have also tried directly approximating the self-attention method with the \nystrom method and observed numerical instability during training.

Replacing the softmax structure with Gaussian kernel somehow alleviates this instability problem with boosted performance as shown in Table~\ref{table:lra_acc}. 
However, the time and space requirement of Kernelized Attention is not significantly improved compared to the original version, which serves as the motivation to approximate Kernelized Self-Attention with \nystrom method.

Though not necessarily the fastest, our proposed Skyformer can efficiently converge to the long-time limit with comparable general performance in classification accuracy (Table~\ref{table:lra_acc}) and resource consumption (Table~\ref{table:lra_timespace}).
The advantages over the standard self-attention are significant with consistently less training time and generally better performance.
For example, Skyformer brings nearly 4 times speed-up on text classification and document retrieval while with 2.75\% and 1.37\% accuracy improvement over the standard self-attention.


\textbf{Limitations.}
The applications of Skyformer might be limited to long sequence tasks because for small sequence length $n$ the statistical dimension $d_stat$ might be close to $n$.

To make the claim above clear, we first reiterate that the efficiency of Skyformer is related to $d_stat$.
As implied by Theorem~\ref{thm:tilde_K}, the intrinsic difficulty of approximating a raw attention score matrix is concluded as $d_{stat}$, which corresponds to the effective rank of matrix $\bar{C}$. 
The complexity of Skyformer depends on the sub-sample size $d$ (the size of the sub-sampling matrix $S$). A large $d_{stat}$ leads to a large $d$ , and an inefficient application of the \nystrom method. 

The classical theory for statistical dimension only guarantees that $d_{stat}$ is small (compared to $n$) when $n$ is large enough, and it is possible the statistical dimension associated with a short sequence might be even close to the sequence $n$. 
Therefore a large $n$ serves as a condition to make the method work. 
Figure \ref{fig:norm} empirically shows that our method performs better with larger $n$’s. 








% \input{figures/svd}
% Figure~\ref{fig:svd} shows the singular value distribution of attention output from the second layer of standard transformer. 
% Results are averaged across one batch from test set.
% We conclude that Document Retrieval and Pathfinder tasks are more difficult, on which tasks our models outperforming other models.
% \vicki{add conclusions from figure 4}













% \begin{comment}
% \subsection{Language Model}

% \input{tables/lm}

% goal: (1) comparable performance to the original self-attention with significant smaller computation cost. (2) whether this task can benefit from long seq compare 512 and 4096


% \textbf{Tasks and Datasets.}

% settings: continuing training from the roberta released checkpoint (12-layer), and from-scratch training a toy 2-layer model. correspond to the init and pretrain settings in exp1 (motivation: different distribution). 


% % MLM pretraining is expensive, we continue pretraining from the RoBERTa (Liu et al., 2019) released checkpoint, while only making the minimal changes necessary to support Longformer’s attention mechanism. 

% two data settings (seq 512 and 4096 but same dataset)


% BookCorpus\citep{DBLP:conf/iccv/ZhuKZSUTF15}

% Wikipedia, wikiextractor\citep{Wikiextractor2015}

% Realnews\citep{DBLP:conf/nips/ZellersHRBFRC19}




% \textbf{Implementation Details.}

% load and toy

% number of features


% \textbf{Results.}

% \subsection{GLUE Benchmark}

% \input{tables/glue}

% Following the pretrain-finetuning transfer learning settings

% motivation: following pretrain-finetuning transfer learning setting, check the performance of finetuning on downsteam nlu tasks. 
% goal see converge speed and results

% \textbf{Tasks and Datasets.}

% glue

% cola, mnli, mrpc, qnli, qqp, rte, sst2, stsb, wnli

% \textbf{Implementation Details.}

% similar to the continuing training setting in language modeling experiment, we load the Roberta checkpoint

% batch size is selected conditioned on the requirements of the vanilla method (bigbird may require more space )

% \textbf{Results.}
% \end{comment}