\begin{figure}[htbp]
\centering
\includegraphics[width=1.0\linewidth]{figures/sky_norm8.png}
\caption{
Spectral norm results with different sequence lengths under different $W_Q, W_K, W_V$ settings, either from initialized or pretrained BERT models. 
All methods are approximating the original self-attention output.
Y axis: Lower spectral loss means better approximation. 
X axis: Higher $d$ (number of features) means visiting more elements in the original matrix and bringing more computation costs.
The label ``Skyformer" here means that we use the algorithm behind Skyformer, mainly Eq. (5), to approximate the raw attention score matrix $A$ in self-attention. In this experiment, ``Skyformer" also needs to first approximate $A$, and then approximate $D$, as Performer does. 
}
\label{fig:norm}
\end{figure}