\section{Related Work}
\label{sec:related_work}


% \subsection{Relate efficient transformers to kernel methods}

Among all the transformer acceleration methods, including attention layer simplification by pruning redundant attention heads~\citep{DBLP:conf/acl/VoitaTMST19, DBLP:conf/nips/MichelLN19} and
model size reduction with knowledge distillation~\citep{DBLP:conf/emnlp/JiaoYSJCL0L20, DBLP:journals/corr/abs-1903-12136, DBLP:conf/acl/LiuZWZDJ20}, 
we focus on attention approximation models, which are closely related to kernel methods. 


% \textbf{Pattern-based Attention Approximation. }  
To reduce the time and space complexity by avoiding exhaustive computation over the attention metric, recent studies propose to apply sparse attention patterns to limit the numbers of elements participating in matrix multiplications~\citep{DBLP:conf/emnlp/QiuMLYW020, DBLP:journals/corr/abs-1904-10509, DBLP:conf/nips/ZaheerGDAAOPRWY20, DBLP:journals/corr/abs-2004-05150}. 
Beyond limiting the attention to fixed patterns, some approaches learn the patterns by determining
token assignments to relevant groups ~\citep{DBLP:conf/iclr/KitaevKL20, DBLP:journals/tacl/RoySVG21}. 
Those models utilize local and global information in the attention score matrix to perform approximation,
which coincides with the attempt to accelerate the computation in Gaussian processes \citep{snelson2007local}.


% \textbf{Low-Rank Attention Approximation. } 
The attention score matrix is known to exhibit a very fast rate of singular value decay \citep{bhojanapalli2020low, dong2021attention}, similar to that of an empirical kernel matrix \citep{yang2017randomized}.
This near singular property motivates many low-rank attention approximation methods to skillfully leverage the computation techniques in kernel methods.
Among them, Linformer~\citep{DBLP:journals/corr/abs-2006-04768} compresses the size of the key and value matrix with random projections based on the Johnson–Lindenstrauss transform, a common randomized sketching method in Gaussian processes \citep{yang2017randomized};
Reformer~\citep{DBLP:conf/iclr/KitaevKL20} applies locality-sensitive hashing (LSH) \citep{har2012approximate} to simplify the computation of the attention score matrix,
which is widely used in kernel density estimation \citep{charikar2017hashing, DBLP:conf/nips/BackursIW19};
Performer~\citep{DBLP:journals/corr/abs-2009-14794} projects both query and key matrix through random Fourier features~\citep{rahimi2007random},
heavily exploiting Bochner Theorem for stationary kernels. 

The most related papers to ours are linear attention~\citep{katharopoulos2020transformers}, Synthesizer~\citep{DBLP:journals/corr/abs-2005-00743}, and Nystr\"omformer~\citep{DBLP:journals/corr/abs-2102-03902}.
Linear attention takes the softmax structure in self-attention as a measure of similarity and replaces it with the dot product of separately activated query and key matrices;
Synthesizer aims to modify the original self-attention by replacing the dot product before softmax with Synthetic Attention, which generates the alignment matrix independent of token-token dependencies.
Their attempts indicate that the softmax structure in self-attention is not the only feasible choice, and justify our usage of kernelized attention.
Rather than remodeling self-attention, Nystr\"omformer applies the \nystrom method \citep{williams2001using, drineas2005nystrom}, a powerful and effective method for large-scale kernel machines acceleration, to approximate the attention score matrix.
However, Nystr\"omformer applies the \nystrom method to a non-PSD matrix, and thus fails to utilize the full potential of the \nystrom method.
This issue is resolved in our proposed Skyformer by instead lifting the kernelized attention score matrix into a large PSD matrix which contains the target non-PSD matrix as its off-diagonal block.
For more details on attention approximation methods,
we refer readers to a survey paper on efficient transformers~\citep{DBLP:journals/corr/abs-2009-06732}.



\iffalse
% Note that some models are not listed here because they do not aim to approximate the original self-attention, such as .

\subsection{\nystrom Method for Empirical Kernel Matrix Approximation}

% \william{Plan to combine the two subsections. Introduce a former paper, following a kernel method paper.}


Previous studies have shown the efficiency of \nystrom method~\citep{} in empirical kernel matrix approximation, such as such as Recursive Sampling for the Nyström Method~\citep{DBLP:conf/nips/MuscoM17} and ---more---.
% Our method mainly utilize the \nystrom method~\citep{}, which is very efficient for empirical kernel matrix approximation.
% There are a bundle of studies on its usage in kernel methods, such as Recursive Sampling for the Nyström Method~\citep{DBLP:conf/nips/MuscoM17}, and ---more---
% two paper from aistat\citep{chen2021fast, chen2021accumulations}.


% We will come back to the important property in the following sections.

\fi