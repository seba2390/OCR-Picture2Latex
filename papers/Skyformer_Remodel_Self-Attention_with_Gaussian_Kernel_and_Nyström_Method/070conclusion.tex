\section{Conclusions and future work}

Motivated by the connection between kernel methods and self-attention, 
we introduce Kernelized Attention, which replaces the softmax structure in self-attention with a Gaussian kernel.
We also propose Skyformer, which adapts the \nystrom method to Kernelized Attention to improve its efficiency.
We expect the new model can enjoy more stable training while inheriting the strong performance from self-attention.
Extensive experiments verify our intuitions and show that both Kernelized Attention and its \nystrom approximation variant have comparable accuracy to the original Transformer on the LRA benchmark.

Direct development of this work is the incorporation of further computation tricks in kernel methods, such as the local and global approximation for gram matrix \citep{snelson2007local} and the importance sampling in \nystrom methods \citep{DBLP:conf/nips/MuscoM17, chen2021fast, chen2021accumulations}.
Other related questions include the choice of the kernel other than the Gaussian kernel in our kernelized attention model.
It is expected that for different tasks there will be specific kernels more proper than the original self-attention.
The results in this work also shed new light on the design of the attention mechanism, which may benefit board downstream NLP tasks.





% A possible trick would be correcting the diagonal / band / block part of $\tilde A$ (from "local and global sparse Gaussian process approximation"):
% \begin{align*}
% \tilde A + \text{a certain part of}(A - \tilde{A}),
% \end{align*}
% which ensures the entries in the certain part are exact.

% Diagonal Correction on $\Tilde{\textbf{A}}$: $\Tilde{\textbf{A}} + \text{diag}(\textbf{A}-\Tilde{\textbf{A}})$.
% Here the correction can be of any low rank form, including diagonal, banded, or block-wise.


%  can be further combined with previous tricks in BigBird

%  optimize based on the properties of document (sentence-wise block)