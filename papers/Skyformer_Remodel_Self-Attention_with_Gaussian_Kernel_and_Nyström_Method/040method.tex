\section{Method}
\label{Sec:Method}

\subsection{Kernelized Attention}
\label{sec:kernel_attn}

Kernelized Attention replaces the softmax structure in vanilla self-attention with a Gaussian kernel,
and the new attention model is stated as:
\begin{align}
\text{Kernelized-Attention}(\mtx{Q},\mtx{K},\mtx{V}) = \mtx{C} \mtx{V} \defeq \kappa\left(\frac{\mtx{Q}}{p^{1/4}}, \frac{\mtx{K}}{p^{1/4}} \right) \mtx{V},
\end{align}
where we define the $n$-by-$n$ matrix $\mtx{C}$ as the kernelized attention score matrix $\kappa(\mtx{Q} / p^{1/4}, \mtx{K} / p^{1/4})$.

The justification for using the kernelized attention model is as follows.
A significant advantage of softmax attention is that tokens are allowed to attend to a limited number of other important tokens in the sequence.
We observe that Gaussian kernel function can play a similar role.
The expression of a Gaussian kernel is $\kappa(\mtx{q}_i,  \mtx{k}_j) \defeq \exp \left(-\|\mtx{q}_i - \mtx{k}_j\|^2 / 2 \right)$.
Via this expression, for token $i$ in the query, Gaussian kernel assigns a large attention score to the token $j$ when $\mtx{k}_j$ is close to $\mtx{q}_i$.
The distance-based weight assignment is indeed considered as a major reason why kernel methods are powerful.
The form of kernelized attention also leads to an automatic normalization. 
Based on Equation~(\ref{eqn:sm_rbf}), the new attention model can be rewritten in terms of the un-normalized attention score matrix $\mtx{A}$ as
\begin{align*}
\text{Kernelized-Attention}(\mtx{Q},\mtx{K},\mtx{V}) = \left( \mtx{D}_Q^{-1/2} \cdot \mtx{A} \cdot \mtx{D}_K^{-1/2} \right) \mtx{V},
\end{align*}
where $\mtx{D}_Q$ (resp. $\mtx{D}_K$) is a diagonal matrix with elements $(\mtx{D}_Q)_{ii} = \exp\left( \frac{\|\mtx{q}_i\|^2}{\sqrt{p}} \right)$ 
(resp. $(\mtx{D}_K)_{ii} = \exp \left( \frac{\|\mtx{k}_i\|^2}{\sqrt{p}} \right)$), $\forall i \in [n]$.
We remark the kernelized attention model can thus be formally taken as a variant of the original self-attention, 
which instead normalizes the matrix $\mtx{A}$ in a form of $\mtx{D}^{-1} \mtx{A}$.
The intrinsic normalization allows kernelized attention to have a more reasonable condition number than self-attention, 
which benefits the stability of model training.
To demonstrate the improvement in stability, we additionally provide a toy experiment in Appendix~\ref{sec:exp_stability},
which shows the ``condition number" of kernelized attention is smaller than self-attention.
Moreover, empirical evaluation in Section~\ref{sec:exp} supports our claim that the new attention model can attain a comparable performance to the original attention model.


\subsection{Skyformer: a modified \nystrom method}
\label{sec:nystrom}


Before jumping into details of Skyformer, 
we first propose a method to apply \nystrom method to approximate an asymmetric (and thus non-PSD) empirical kernel matrix $\mtx{B}$ constructed with any PSD kernel $\phi(\cdot, \cdot)$.
Specifically, with two different $n$-by-$p$ design matrices $\mtx{Q}$ and $\mtx{K}$,
its element $b_{ij}$ from the $i$-th row and $j$-th column in $\mtx{B}$ is equal to $\phi(\mtx{q}_i, \mtx{k}_j)$,
where $\mtx{q}_i$ (resp. $\mtx{k}_j$) is the $i$-th (resp. $j$-th) row in $\mtx{Q}$ (resp. $\mtx{K}$).
We remark this type of empirical kernel matrices involves the un-normalized attention score matrix $\mtx{A} \defeq \text{SM}(\mtx{Q}, \mtx{K})$,
and the empirical Gaussian kernel matrix $\mtx{C} \defeq \kappa(\mtx{Q} / p^{1/4}, \mtx{K} / p^{1/4})$.
% (The proof to show $\kappa(\mtx{q}_i, \mtx{k}_j) = \exp(\mtx{q}_i^T \mtx{k}_j / \sqrt{p})$ is a PSD kernel function is deferred to Appendix~\ref{sec:facts}.)
Therefore this method leads to a low-rank approximation to the output of either self-attention $\mtx{D}^{-1} \mtx{A} \mtx{V}$ or Kernelized Attention $\mtx{C} \mtx{V}$.
($\mtx{D}$ in self-attention can be obtained by computing $\mtx{A} \cdot \mtx{1}$, and thus a low-rank approximation to $\mtx{A}$ also implies an approximation to $\mtx{D}$.)
% with a low dimensional randomized sketch of the complete symmetric matrix $\bar{\mtx{A}}$.

% Figure~\ref{fig:model} shows our proposed method. 
% mark: delete figure

Computational details are stated as follows.
To tackle the challenge of approximating a non-PSD matrix $\mtx{B}$, our first step is to complete the matrix into a PSD matrix $\bar{\mtx{B}}$:
\begin{align}
\label{eqn:concat}
    \bar{\mtx{B}} \defeq \phi \left(
        \begin{pmatrix}
        \mtx{Q}  \\
        \mtx{K} 
        \end{pmatrix},
        \begin{pmatrix}
        \mtx{Q}  \\
        \mtx{K} 
        \end{pmatrix} \right).
\end{align}
Then we approximate $\bar{\mtx{B}}$ with $\tilde{\bar{\mtx{B}}}$ through
\begin{align}
\label{eqn:tilde_bar}
\tilde{\bar{\mtx{B}}} = \bar{\mtx{B}} \mtx{S} (\mtx{S}^{T} \bar{\mtx{B}}\textbf{S})^{\dagger} \textbf{S}^{T}\bar{\mtx{B}},
\end{align}
where $\mtx{S}$ is a $2n$-by-$d$ uniform sub-sampling matrix as defined in Definition~\ref{def:subsampling}.
The final approximation will be given as 
\begin{align}
\label{eqn:approx}
\tilde{\mtx{B}} \defeq (\mtx{I}, \mtx{0}) \tilde{\bar{\mtx{B}}} (\mtx{0}, \mtx{I})^T.
\end{align}
The original matrix $\mtx{B}$ can be well-approximated by $\tilde{\mtx{B}}$ due to the following inequality
\begin{align*}
\|\mtx{B} - \tilde{\mtx{B}}\| = \|(\mtx{I}, \mtx{0}) (\bar{\mtx{B}} - \tilde{\bar{\mtx{B}}}) (\mtx{0}, \mtx{I})^T\| \leq \|\bar{\mtx{B}} - \tilde{\bar{\mtx{B}}}\|,
\end{align*}
and thus we show our task of approximating the non-PSD matrix $\mtx{B}$ boils down to well approximating the PSD matrix $\bar{\mtx{B}}$.

% \william{We empirically show the performance of the modified \nystrom method on approximation error in Figure~\ref{},}
% through comparing the method with other approximation methods used by existing efficient Transformers.
% The complete settings can be found in Appendix~\ref{sec:exp_norm}.

\textbf{Remark.} 
The reason why $\mtx{B}$ can be well approximated by a low-rank $\tilde{B}$ is that as an empirical kernel matrix the eigenvalues in $\bar{\mtx{B}}$ usually decay fast,
and thus there are many small eigenvalues in the long tail. 
In this case, theoretically a low-rank matrix (e.g. truncated singular value decomposition (SVD) of $\bar{\mtx{B}}$) has enough potential to well approximate the original matrix $\bar{\mtx{B}}$ (and $\mtx{B}$ accordingly) in terms of spectral norm.
% \input{figures/model}


With the derivation above, we officially introduce our proposed Skyformer as an approximation to Kernelized Attention, which applies the modified \nystrom method to the kernelized attention score matrix $\mtx{C}$.
The next two subsections will continue our discussion on it, and respectively state the theoretical analysis of its approximation error and some details of its implementation in practice.


\subsection{Error analysis of Skyformer}


As mentioned, an implicit advantage of using Kernelized Attention is that we can leverage the existing conclusions for kernel methods to analyze the theoretical properties of the model.
In this subsection, we aim to provide some theoretical analysis of its approximation error.

We state a high probability bound on the size $d$ of the sub-sampling matrix used in Skyformer to attain $(\varepsilon, \delta)$-MA property for the kernelized attention score matrix $\mtx{C}$ by the following theorem.
We refer the readers to the proof in Appendix~\ref{sec:thm_error} to take a closer look at our claim that the matrix to be approximated should be PSD is a key to the theoretical guarantee of \nystrom method.
\begin{thm}[Adapted from Lemma~9 and Theorem~3 \citep{DBLP:conf/nips/MuscoM17}]
\label{thm:tilde_K}
Consider the query, key, and value matrix $\mtx{Q}, \mtx{K}, \mtx{V} \in \mb R^{n \times p}$ and two positive constants $\varepsilon < 1, \delta < \frac12$.
For the empirical Gaussian kernel matrix $\mtx{C} \defeq \kappa(\mtx{Q} / p^{1/4}, \mtx{K} / p^{1/4})$ defined above, 
we let $\lambda \defeq \varepsilon \|\mtx{C}\| < \|\bar{\mtx{C}}\|$, where $\bar{\mtx{C}}$ is the completion of $\mtx{C}$ (similar to $\bar{\mtx{B}}$, constructed as substituting the Gaussian kernel with bandwidth $p^{1/4}$ for the arbitrary kernel function $\phi$ in Equation~(\ref{eqn:concat})).
We comment $\lambda$ serves as the regularization coefficient as well as the approximation error bound. 
To ease the analysis, we specifically define the $i$-th diagonal element of $\bar{\mtx{C}} (\bar{\mtx{C}} + \lambda \mtx{I}_{2n})^{-1}$ as leverage score $\ell_i, \forall i = 1,\dots, 2n$, 
and define their sum $\Tr \big( \bar{\mtx{C}} (\bar{\mtx{C}} + \lambda \mtx{I})^{-1} \big)$ as the statistical dimension $d_{stat}$,
which increases with $1 / \varepsilon$ as $\lambda \propto \varepsilon$. 
Suppose $\mtx{S}$ is a uniform sub-sampling matrix, 
and assume there exists a constant $\beta \in (0, 1]$ such that $\beta \leq \frac{d_{stat}}{2n \ell_i}, \forall i = 1,\dots,2n$. 
For the approximation matrix $\tilde{\bar{\mtx{C}}}$ constructed with $\bar{\mtx{C}}, \mtx{S}$ as in Equation~(\ref{eqn:tilde_bar}), 
there exists a constant $C$ such that if
\begin{align*}
d \geq C \frac{d_{stat}}{\beta} \log \frac{n}{\delta}
\end{align*}
then $\tilde{\bar{\mtx{C}}} \psdle \bar{\mtx{C}} \psdle \tilde{\bar{\mtx{C}}} + \lambda \mtx{I}$ with probability $1-\delta$.
Here $\psdle$ denotes the Loewner ordering: $\mtx{B} \psdle \mtx{A}$ means $\mtx{A} - \mtx{B}$ is positive semidefinite.
Furthermore, for our approximation $\tilde{\mtx{C}}$ in Equation~(\ref{eqn:approx}) to the kernelized attention score $\mtx{C}$, we have 
\begin{align*}
\|\tilde{\mtx{C}} - \mtx{C}\| \leq \lambda = \varepsilon \|\mtx{C}\|.
\end{align*}
\end{thm}

This theorem implies the time and space complexity of our proposed approximation depends on the statistical dimension $d_{stat}$.
If we directly use the conclusion from Gaussian kernels, $d_{stat}$ should be $\wt{\m O}(1)$ (complexity modulo poly-log term) \citep{yang2017randomized} due to the exponential eigenvalue decay rate of Gaussian kernels,  
which is comparable to the complexity of most other efficient transformers.
However, different than the case in the classical kernel methods, the distribution of the query and key matrix $\mtx{Q}$ and $\mtx{K}$ changes during the training procedure,
which may invalidate the conclusion about $d_{stat}$.
We leave the exact non-asymptotic analysis of the computational complexity for future work.
% We also remark the complexity analysis is discussed under a high probability bound setting, 
% which is not comparable to the linear $\m O(n)$ complexity of some previous methods. 



% The training of the approximation model with \nystrom method also benefits from the reduced condition number of $\mtx{S}^T (\mtx{D}_Q^{-1/2} \mtx{A} \mtx{D}_K^{-1/2}) \mtx{S}$.
% Some empirical analysis are reported in Appendix~\ref{sec:nyst_kernel_attn} to further validate the statement above.


\subsection{Workaround in implementation}
\label{sec:method_implementation}

A potential limitation with the implementation of the proposed method lies in the tricky fact that the matrix inversion on GPU is much slower and numerically less stable than the same operation on CPU due to the different back-end libraries in the two platforms.
We attempt to circumvent the problem by adapting the strategy in Nystr\"{o}mformer \citep{DBLP:journals/corr/abs-2102-03902} to our setting.
Specifically, we use the matrix-product-based iterative method \citep{razavi2014new} for finding approximate inverses, instead of some division-based methods (such as the conjugate gradient method) which induces some instability in model training.

To apply the iterative method and inverse matrix $\mtx{M} = \mtx{S}^{T} \bar{\mtx{C}} \textbf{S}$, we need to satisfy its assumption \citep[Theorem~2]{razavi2014new} that $\|\mtx{I} - \mtx{M}\| < 1$.
In practice, we instead pass the matrix $\mtx{D}_M^{-1/2} (\mtx{M} + \gamma \mtx{I}) \mtx{D}_M^{-1/2}$ as an input to the iterative method,
where $\gamma>0$ is a small constant and the diagonal matrix $\mtx{D}_M$ is defined as $\text{diag}\left( (\mtx{M} + \gamma \mtx{I}) \mtx{1} \right)$.
We give the following lemma to justify our practical usage of the method. The proof is deferred to Appendix~\ref{sec:lem_iter}.
\begin{lem}
\label{lem:iterative}
Given a constant $\gamma > 0$, if matrices $\mtx{M}$ is constructed as $\mtx{S}^{T} \bar{\mtx{C}} \textbf{S}$, and $\mtx{D}_M$ are defined as above,
then all the singular values of $\mtx{D}_M^{-1/2} (\mtx{M} + \gamma \mtx{I}) \mtx{D}_M^{-1/2}$ are within $(0, 1)$,
which implies that $\|\mtx{I} - \mtx{D}_M^{-1/2} (\mtx{M} + \gamma \mtx{I}) \mtx{D}_M^{-1/2}\| < 1$.
\end{lem}
We further comment that numerically an implicit risk of the Schulz-type iterative method we use is the unintended consequence of ``zero fill-in".
If we use some sparse kernels (e.g. test functions with bounded support) other than Gaussian kernels,
the empirical kernel matrices are sparse while the approximate inverse will converge to a dense matrix,
which increases the computational cost.



\subsection{Empirical approximation evaluation}
\label{sec:norm}

Spectral norm, the maximum singular value of a matrix, is a computation-light indicator of matrix approximation performance. 
In this work, we compare the spectral norm of the difference between the outputs from attention functions and the output from vanilla self-attention with the same input.

We use the initialized and pretrained bert-base-cased models from Huggingface's implementation~\citep{DBLP:journals/corr/abs-1910-03771} .
The input vector $X$ is embedded from the tokenized raw text in Wikitext-2 dataset~\citep{DBLP:conf/iclr/MerityX0S17}.
The query, key and value weight matrices in initialized or pretrained models  transform input $X$ into $Q, K, V$ of different distributions.
We compare the results with different sequence lengths and different numbers of features used in attention approximation methods. 
We set the number of features in the range of $2^4$ to $2^8$. 
More features usually require more computation resources.
% Note that the number of features used in Big Bird is roughly ten times of \textit{block\_size} ($(r+b+g)*b$ in their notations and by default set $b$ to 64), which means it is much larger than the number of features used in other methods.

Figure~\ref{fig:norm} shows the performance of the modified Nystr\"om method on approximation error with regards to the number of features.
We conclude that for Skyformer the approximation is significantly better with the increased number of features, while for other methods the gain is not obvious.
The good performance of the modified \nystrom method also validates our previous claim that the \nystrom method is currently one of the most powerful methods in large-scale kernel machines acceleration.

\textbf{Remark.} 
Although in a single step the modified \nystrom method in Section~\ref{sec:nystrom} can give low approximation error, we do not recommend directly applying it to the original self-attention.
With some exploratory experiments on classification tasks, we find the variant suffers a more severe gradient explosion issue than usual transformers.
We speculate that it is because the matrix $\mtx{S}^{T} \bar{\mtx{A}}\textbf{S}$ (in the middle of Equation~(\ref{eqn:approx})) inherits the high condition number of the original attention score matrices $\mtx{A}$, 
while the derivative of matrix inverse ($\left( \mtx{A}^{-1} \right)' = - \mtx{A}^{-1} \mtx{A}' \mtx{A}^{-1}$) further amplifies the condition number during backpropagation.


\input{figures/norm_results}



\iffalse
% \william{
% Point out Nystromformer indeed use twice our landmarks}

% (c.f. Section~\ref{Sec:Method})
% In short, this method approximate the softmax kernel matrix by a low dimensional randomized sketch of the complete symmetric matrix, which is expected to have outstanding performance.

\textbf{Symmetrization}. We use $\mtx{A}$ to denote the term $\exp(\mtx{Q} \mtx{K}^{T} \sqrt{d})$ and try to approximate $\mtx{A}$ with $\wt{\mtx{A}}$.
We first consider $\textbf{A}$ as the result of a kernel function $\kappa$
\begin{equation}
    \nonumber
    \textbf{A} = \kappa (\textbf{Q},\textbf{K}),
\end{equation}
and symmetrize it by extending into
\begin{equation}
    \nonumber
    \bar{\textbf{A}} = \kappa (
        \begin{pmatrix}
        \textbf{Q}  \\
        \textbf{K} 
        \end{pmatrix},
        \begin{pmatrix}
        \textbf{Q}  \\
        \textbf{K} 
        \end{pmatrix}).
\end{equation} 
Note our target $A = (I, 0) \bar A (0, I)^T$, and the approximation term can be correspondingly attained as
\begin{equation}
    \nonumber
    \Tilde{\textbf{A}} = 
        \begin{pmatrix}
            \textbf{I}_{n \times n},
            \textbf{0}
        \end{pmatrix}
        \widetilde{\bar{\textbf{A}}}
        \begin{pmatrix}
            \textbf{0} \\
            \textbf{I}_{n \times n}  
        \end{pmatrix},
\end{equation}
where $\bar{\textbf{A}}$ can be approximated by
\begin{equation}
    \nonumber
    \Tilde{\bar{\textbf{A}}} = \bar{\textbf{A}} \textbf{S} (\textbf{S}^{T}\bar{\textbf{A}}\textbf{S})^{+}\textbf{S}^{T}\bar{\textbf{A}}.
\end{equation}
$\textbf{S}$ here is a rescaled uniform sub-sampling matrix.
\subsection{Empirical approximation evaluation}

norm experiment
\fi

\iffalse

\begin{align*}
\int_{\mb R^p} \kappa(\mtx{q}_i,  \mtx{k}) \dd \mtx{k} = 1,
\end{align*}
and taking the matrix multiplication as a numerical integration we can simply use $\frac{1}{nh} \kappa(\mtx{q}_i/h,  \mtx{k}_j/h)$, where $h$ is the bandwidth used in Gaussian kernel, similar to the factor $p^{1/4}$ in the original attention.


The new kernelized attention also allows the improvement on the singular value decay rate.
As motivated by the classical kernel density estimation, we can set the bandwith $h = \cdots$, 
so that according to Equation~(\ref{}) the decay rate becomes $\cdots$.
This change also reduces the condition number of the new attention score matrix $\frac{1}{nh} \kappa(\mtx{Q} / h, \mtx{K} / h)$,
which benefits our proposed approximation method involving matrix inversion.


\fi