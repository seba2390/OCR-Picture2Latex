\section{Introduction}
\label{sec:intro}


The cost of language model training increases exponentially.
Among different models, Transformer-based language models ~\citep{DBLP:conf/nips/VaswaniSPUJGKP17, DBLP:journals/corr/abs-1810-04805, DBLP:journals/corr/abs-1907-11692, DBLP:conf/acl/LewisLGGMLSZ20} are shown to enjoy state-of-the-art (SOTA) performances on many Natural Language Processing (NLP) tasks despite their enormous training cost. %  that was unthinkable years ago
% For example, it may theoretically take around $355$ years to train GPT-3~\citep{DBLP:conf/nips/BrownMRSKDNSSAA20} on a Tesla V100.
One of the computation bottlenecks lies in the self-attention mechanism, which is known to be resource-intensive with quadratic time and space complexity ($O(n)$ where $n$ is the input sequence length).
Consequently, Transformers cannot support long sequence processing and large batch size with limited resources.

The challenge of improving computational efficiency of Transformers has motivated several recent studies on attention acceleration, using either sparse attention pattern \citep{DBLP:conf/emnlp/QiuMLYW020, DBLP:journals/corr/abs-1904-10509, DBLP:conf/nips/ZaheerGDAAOPRWY20, DBLP:journals/corr/abs-2004-05150, DBLP:conf/iclr/KitaevKL20} or low-rank approximation \citep{DBLP:journals/corr/abs-2009-14794, DBLP:journals/corr/abs-2006-04768}. 
However, there is usually a lack of theoretical analysis on the approximation error of these methods due to the complex softmax structure, which makes the theoretical comparison between the efficiency of each method infeasible.
It is also unclear in theory how to set the hyper-parameters of those methods to attain a desired level of approximation accuracy.


Another issue of Transformers is the training instability that small perturbations in parameter updates tend to be amplified, resulting in significant disturbances in the model output~\citep{DBLP:conf/emnlp/LiuLGCH20}.
Transformers on some NLP tasks have shown to be sensitive to hyper-parameters, learning schedulers, or even random seeds, which usually demands a time-costly grid search for the best configuration in real-world applications.
It has also been observed in our experiments that a slight change in the learning rate may cause the failure of convergence for some models.
We conjecture that the instability in Transformer training comes from the softmax structure,
as the un-normalized attention score matrices before softmax tend to have extremely large condition numbers due to its fast singular value decay.
% In addition to the high computational complexity, another potential issue of Transformer is its difficult training \citep{DBLP:conf/emnlp/LiuLGCH20}.
% -------------------------------------


To alleviate the instability issue, an extra factor of $1 / \sqrt{p}$ in the softmax kernel $\text{SM}$ is suggested by \citet{DBLP:conf/nips/VaswaniSPUJGKP17} to restrain the scale variation;
\citet{DBLP:conf/emnlp/LiuLGCH20} proposes a new scheme to control the magnitude of output change and stabilize the training in early stages.
In practice, we also need to consider the lower numerical precision of GPU implementation in model training, which further deteriorates the stability.
% This issue substantially influences the usage of \nystrom method, which requires to inverse an ill-conditioned matrix $\mtx{S}^T \mtx{A} \mtx{S}$.
% The large condition number of $\mtx{S}^T \mtx{A} \mtx{S}$ results in extremely large spectral norm of the pseudoinverse $(\mtx{S}^T \mtx{A} \mtx{S})^\dagger$,
% which causes large parameter updates and make the training process even more unstable.


Kernel methods may be the answer to both challenges.
As pointed out by \citet{DBLP:journals/corr/abs-2009-14794}, 
the softmax structure is closely related to Gaussian kernels up to diagonal matrix multiplications,
as the pairwise dot products naturally appear when expanding the squared $\ell_2$ distance.
We further notice some important connections between self-attention and Gaussian kernels.
First, the un-normalized attention score matrix can be formed via basic matrix operations on an empirical Gaussian kernel matrix.
Moreover, the form of Gaussian kernels has the natural interpretation of  assigning ``attention'' to different tokens.
Compared to the softmax function, Gaussian kernels automatically perform the normalization as softmax does (c.f. Section~\ref{sec:kernel_attn}).
These observations motivate us to replace the softmax structure with Gaussian kernels.
As we demonstrated in this paper, the new attention model, \textbf{Kernelized Attention}, empirically stabilizes the model training while being comparable to self-attention in model accuracy.
% and theoretically allows a more precise analysis considering the abundant studies on kernel methods. 

To further improve the efficiency, we propose \textbf{Skyformer} (\textbf{S}ymmetrization of \textbf{K}ernelized attention for N\textbf{Y}str\"om method) to accelerate kernelized attention.
Skyformer adapts the \nystrom method \citep{williams2001using, drineas2005nystrom} to the non-PSD empirical Gaussian kernel matrix
(as query matrices in general do not equal to key matrices), 
by instead lifting the kernelized attention score matrix into a large PSD matrix that contains the un-normalized attention score matrix as the off-diagonal block.
We further conduct theoretical analysis by showing that Skyformer has a small matrix approximation error on kernelized attention in the spectral norm.
Our experiments on the LRA benchmark show that Skyformer consistently uses less space and time while achieving better accuracy than other baseline methods.
% In addition, our method can be easily applied to some new variants of attention, such as Lambda Network~\citep{lambdanetwork}. 


In summary, our main contributions are:


(1) We revisit the intrinsic connection between self-attention and kernel methods, and explore a new kernel-based structure, kernelized attention, to stabilize the training of Transformers.


(2) We propose Skyformer, which approximates the kernelized attention via low dimensional randomized sketches by adapting the \nystrom method to a non-PSD matrix. 
We provide the theoretical guarantee that the matrix multiplication error is small in term of spectral norm. 


(3) Extensive experiments show that Skyformer achieves comparable performance to the original self-attention with fewer computational costs.\footnote{Our code is released at \url{https://github.com/pkuzengqi/Skyformer}}





%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%






\begin{comment}

\iffalse
A potential issue in Transformer is its difficult training \citep{DBLP:conf/emnlp/LiuLGCH20},
which requires huge amounts of efforts to tune the hyper-parameters in training, including carefully choosing the optimizers and learning rate schedulers.
Specifically, the fluctuation caused by parameter updates will be amplified and thus destabilizes model training.
We conjecture this amplification effect comes from the softmax structure itself, as in applying softmax we need to enforce the row normalization to the matrix $\mtx{A}$, which greatly alters the scale of some rows.
\fi

This similarity motivates us to apply the most widely used computation method for large-scale kernel machines, the \nystrom method \citep{williams2001using, drineas2005nystrom}, to approximate the output of self-attention.
To realize the intuitive idea, we need to first address the thorny fact that the un-normalized attention score matrix is not positive semidefinite (PSD), 
as in general the query matrix unnecessarily equal the key matrix.
In this work, 

Furthermore, as we conjecture the instability in Transformer training comes from the softmax structure, we propose to replace the softmax structure with Gaussian kernel, which has a similar form to the original self-attention.

We conclude the two improvements with a new proposed model, Skyformer, 
Motivated by the computation methods for kernel machines that alleviates the high computational cost of pairwise dot products, we introduce Skyformer which replaces the softmax structure with Gaussian kernel to stabilize the model training and adapts the \nystrom method to a non-PSD matrix to accelerate the computation.





\textbf{general motivation}:
\begin{itemize}
\item Difficult to train the original Transformers with softmax structures.
\item resource-intensive: both memory consumption and training/inference time, party of labs rich in computational resource
\item lack of long document support: The dot-product self-attention operation is quadratic to seq length and fundamentally limits the maximum length of the input sequence. Truncation or sliding window may lose some local context and  long-distance dependency. 



\end{itemize}



\textbf{motivation}:
\begin{itemize}
\item resource-intensive: both memory consumption and training/inference time, party of labs rich in computational resource
\item lack of long document support: The dot-product self-attention operation is quadratic to seq length and fundamentally limits the maximum length of the input sequence. Truncation or sliding window may lose some local context and  long-distance dependency. 
\item Some previous methods coincide with the past literature to approximate gram matrix in kernel methods.
It is natural to introduce the powerful \nystrom method to self-attention approximation. \heng{explain why it's natural}
\item The current discussion on time complexity is groundless.
Some methods claim they have linear complexity solely because they keep using a fixed rank regardless of the sequence length in practice. 
Of course the original attention cannot be well approximated,
otherwise they theoretically outperform the SOTA kernel density estimation method.
To ease the efficiency comparison between different methods, we utilize the concepts from kernel methods to give a normative complexity criterion.

\end{itemize}

new claim: less dim_head, more num_head
original method cannot afford

do q and k dim must have the same dim as v?
does better approximation mean better performance?

\textbf{Some important discussion (to be deleted)}

Add some discussion on the metric (spectral norm loss), and remark on the complexity analysis.
\textbf{The first experiment} should be comparing the spectral norm loss of different methods.

why new setting can avoid rank collapse
\william{by setting $d=1$, maybe not very practical since the number of heads will blow up.}


\textbf{challenges in / advantages over existing methods}: 
\textbf{Not exactly mention the name of the following models.}
\begin{itemize}
\item  limitation of previous work (Performer):
% the projection dimension (number of random features) is required to be less than $d$, which is a concern when $d$ is tiny relative to $n$ and limits the highest possible accuracy.
Also, the efficiency of approximation could be further improved: it is known that random feature-based methods would require much more projection dimensions than needed.
% Specifically, for an empirical Gaussian kernel (equivalent to softmax kernel) matrix, the statistical dimension (the least rank needed to approximate the original matrix) should be much less than $d$ given usually $d = O(n)$ in applications.
Besides, random features are not easy to be extended to non-translation-invariant kernels, as Mercer's theorem only applies to invariant kernels.
In contrast, our method can be easily applied to some new variants of attention, such as lambda network~\cite{lambdanetwork}. 
\item lack of theoretical guarantee on approximation error in term of spectral norm, which is the most common metric in computation method,
theoretical guarantee that error is small in term of spectral norm
% \item evaluation of methods is based on the expensive experiments
% \item utilizing information from all tokens; separate blockwise or window-based attn will lose context
\item Nystr√∂mformer is also based on \nystrom method, but they do not utilize a key property that \nystrom method should be applied to PSD matrix.
Thus their performance can still be greatly improved, and their theoretical results are loose.

\end{itemize}


\end{comment}
