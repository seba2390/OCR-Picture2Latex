% !TEX root = 0-20SPAWC_SAND.tex
% DO NOT REMOVE THE ABOVE COMMENT!

\section{SAND: Sparsity-Adaptive oNe-bit Denoiser} \label{sec:SAND}

As a generalized variant of $\alpha$-BEACHES, we next develop a sparsity-adaptive method that \emph{simultaneously} learns a prefactor~$\gamma$ and a denoising threshold~$\tau$ in order to minimize the MSE.
%
By defining our two-parameter estimator\footnote{This estimator is equivalent to $\hhattick = \eta(\gamma'\rhat,\tau')$ for $\gamma = \gamma'$ and $\tau = \frac{\tau'}{\gamma}$.} as $\hhattick = \gamma\eta(\rhat,\tau)$, we aim to find the parameters $\gamma^\star$ and $\tau^\star$ that minimize the MSE in \fref{eq:MSE}. 
%
Since the MSE depends on the unknown vector~$\hhat$, we select the optimal parameters $\gamma^\star$ and $\tau^\star$ that minimize SURE, which (i) is an unbiased estimator of the MSE so that $\E{\SURE}=\MSE$ and $\lim_{B\to\infty}\SURE = \MSE$ (see \cite{ghods19a} for the details) and (ii) does \emph{not}  depend on $\hhat$. 
%
For any weakly differentiable estimator $\mur$, using the decomposition \fref{eq:bussgang_decomposition_beamspace} and assuming that $\dhat$ is i.i.d.\ Gaussian,  SURE is given by  
\begin{align}
\SURE = \, & \textstyle \frac{1}{B} {\|\mur\|^2}+ \frac{2-\Do}{\alpha^2}  -  \frac{1}{B} {\frac{2}{\alpha}\realpart{{\rhat}^\Herm \mur}} \notag\\
& + \textstyle \frac{1}{B} \sum_{b=1}^B \frac{\Do}{\alpha}\left(\frac{\partial\realindex{\mure{b}}}{\partial\realindex{\rhate{b}}}+\frac{\partial\imagindex{\mure{b}}}{\partial\imagindex{\rhate{b}}}\right)\!.  \label{eq:sure}
\end{align}
Refer to \fref{app:sand_sure} for the proof.
%
%
Since SURE is an unbiased estimator of the MSE, we use SURE in \fref{eq:sure} with $\mur = \gamma\eta(\rhat,\tau)$, in order to find the optimal parameters $\gamma^\star$ and $\tau^\star$.
%
While a na\"ive approach could perform a two-dimensional grid search over the tuple $(\gamma,\tau)$, we next show that we can efficiently find $\gamma^\star$ and $\tau^\star$ with $\setO(B\log(B))$ complexity.

%
Let $\rshat$ be a vector containing the absolute values of $\rhat$ sorted in ascending order. For a given $\tau$, let $k$ be the number of entries in $\rshat$ that are smaller than $\tau$.
For the denoiser $\mur = \gamma\eta(\rhat,\tau)$, following the derivations in \cite[App.~B]{ghods19a}, SURE in \fref{eq:sure} is 
\begin{align} \label{eq:SureShrink1}
\SURE = \, & \textstyle \frac{1}{B} \gamma^2\sum_{b=k+1}^{B}{(\rshate{b}-\tau)^2} + \frac{2-\Do}{\alpha^2} \\
& - \textstyle \frac{1}{B} \frac{\gamma}{\alpha}\sum_{b=k+1}^{B}\left({{2}{\rshate{b}(\rshate{b}-\tau)}}-{\Do}\left(2-\textstyle\frac{\tau}{\rshate{b}}\right)\right) \notag.
\end{align}
%
By defining the quantities $\sumabsysquared = \sum_{b=k+1}^{B} (\rshate{b})^2$, $\sumabsy = \sum_{b=k+1}^{B} \rshate{b}$ and $\sumabsyinverse = \sum_{b=k+1}^{B} (\rshate{b})^{-1}$, we can rewrite \fref{eq:SureShrink1} as
\begin{align} 
\SURE = \, & \textstyle \frac{1}{B}\gamma^2\left(\sumabsysquared-2\tau\sumabsy+\tau^2\sumone\right) + \frac{2-\Do}{\alpha^2} \notag \\
& - \textstyle \frac{1}{B}\frac{\gamma}{\alpha}\left(2\left(\sumabsysquared-\tau\sumabsy\right)  
- \Do\left(2\sumone-\tau\sumabsyinverse\right)\right)\!.
\label{eq:SureShrinkabc}
\end{align}
%
For a fixed $\tau$, the optimal $\gamma^\star\in\reals_{\geq0}$ that minimizes \fref{eq:SureShrinkabc} is% given by
\begin{align} \label{eq:gammastar}
\gamma^\star = \max\{0,\textstyle \frac{2\left(\sumabsysquared-\tau\sumabsy\right)  
	- \Do\left(2\sumone-\tau\sumabsyinverse\right)}{2\alpha\left(\sumabsysquared-2\tau\sumabsy+\tau^2\sumone\right)}\}.
\end{align}
%
The optimal threshold $\tau^\star$ could take any value between $0$ and~$\rshate{B}$. However, as in the derivation of BEACHES~\cite{mirfarshbafan19a}, we restrict the search to values in $\rshat$, as it significantly reduces the complexity, without sacrificing performance. We also set an upper limit for $\tau$ of $\sqrt{2\Do\log(B)}$, which ensures (with high probability) that the threshold is lower than the largest noise realization~\cite{donoho95}.
%
For each $\tau = \rshate{k}$, $k = 0,\ldots,B$ (with $\rshate{0} = 0$), and for its associated $\gamma^\star$ given by \fref{eq:gammastar}, we evaluate SURE in \fref{eq:SureShrinkabc}, and then pick $\gamma^\star$ and $\tau^\star$ that result in the minimum value of SURE. We call the resulting algorithm Sparsity-Adaptive oNe-bit Denoiser (SAND), which is summarized in \fref{alg:SAND}. 
Since the complexity of a fast Fourier transform (FFT) and sorting scale with $\mathcal{O}(B\log(B))$, and the operations in each iteration (lines 6 to 11) have complexity $\setO(1)$, the overall complexity of SAND scales with $\mathcal{O}(B\log(B))$.

