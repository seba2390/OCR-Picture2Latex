\section{Privacy Exploited Localized Poisoning Attack}
\label{s:problemFormulation}
In this section, we dissect the DP noise exploitation mechanism, formulating adversarial noise profile challenges. We also articulate our threat model and proposed PeLPA algorithm.

\subsection{How can LDP-noise be Exploited for Poisoning Attacks?} \hskip1em \textbf{DP not included.} Considering a non-LDP advising scenario, the agents exchange Q-value knowledge, facilitating learning. We formulate the knowledge as Q-values instead of the recommended actions since the Q-value advising, unlike the action advising, does not impair the performance of the agentâ€™s learning directly \cite{zhu2021q}. Let us assume an anomaly detector at $p_i$'s end that monitors Q-values sequences from advisor agents for all actions in a specific state, $s$. Generally, for a received Q-value, $Q_{p_k}(s)$, from advisor $p_k$, the condition $|Q_{p_{k}}(s)-Q_{0}(s)|\leq \tau$ is consistently maintained, where $\tau$ is a detection threshold and $Q_{0}(s)$, a historical standard Q-value. Any deviation raises an alarm, implying a potential malicious advisor $p_a \in \left[p_k\right]$ with biased Q-values. Nonetheless, to evade detection, the attacker can introduce a bias up to a maximum of $\tau$ relative to the standard value, i.e., $Q_{p_a}(s) \leq Q_{0}(s) + \tau$.

\textbf{DP included.} With an LDP mechanism safeguarding knowledge exchange, any received Q-value, $\bar{Q}{p_k}(s) = Q{p_k}(s) + \eta$, includes noise, $\eta$ following a zero-mean Laplace distribution, $\mathcal{N}(0, b)$, where $b$ is the distribution scale. To prevent false-positive alarms for benign differentially private Q-values, the detector adjusts the previous detection condition to $|\bar{Q}{p_k}(s)-Q_{0}(s)|\leq \tau'$ with $\tau' = \tau \times \kappa; \forall \kappa \in \mathbb{R}$, where $\kappa$ is the tolerance multiplier. This adjustment creates a poisoning window of $\lvert\tau(1-\kappa)\rvert$ that an attacker can exploit, enabling a larger bias in knowledge (i.e., Q-values) without detection. Formally, the attacker shares malicious knowledge, $\bar{Q}{p_a}(s) = Q{p_a}(s)+\eta_a; \forall \eta_a \in \lvert\tau(1-\kappa)\rvert$, where $\eta_a$ denotes the malicious noise drawn from an adversarial noise profile, $\mathcal{N}_a$. Hence, an increase in noise for privacy enhancement also expands the detection and the poisoning window.

\subsection{Challenges in Formulating Adversarial Noise Profile}
\label{noiseProfile} Crafting an adversarial noise profile, $\mathcal{\eta}_a$, that optimizes attack gain while evading anomaly detection poses a technical conundrum. A previous methodology \cite{fang2020local} attempted this by maximizing utility degradation, although this leads to a paradoxical situation in the face of an anomaly detector - more noise aids detection but less noise diminishes the attack gain. A sophisticated alternative, as proposed by \cite{giraldo2020adversarial}, models this as a multi-objective optimization problem, i.e., $\underset{\mathcal{A}}{max}\;\mathpzc{G}(\mathcal{A}, \mathcal{D}) \ni |\bar{Q}_{p_{a}}(s)-Q_{0}(s)|\leq \tau'$ where $\mathcal{A}, \mathcal{D}, \text{and } \mathpzc{G}$ denote the attack, the detect, and the gain function, respectively. The solution of this multi-criteria optimization problem is derived in \cite{giraldo2020adversarial}, where the authors presented an attack impact, $\mu^*_a$, and an optimal adversarial distribution, $\mathcal{N}_a^*(\mu_a^*, b)$ having the probability density function, $f^*_a$ as

\begin{equation}
    f_a^*(x) = \frac{k^2 - b^2}{2bc^2}e^{-\frac{|x-\theta|}{b} + \frac{(x-\theta)}{c}} \;\text{and}\; \mu_a^* = \frac{b^2(\theta-2c)-\theta c^2}{b^2 - c^2}
    \label{eqn:attackDist}
\end{equation}

where $\theta$ is the mean, $b^2$ is the variance, and $c$ is the Lagrange multiplier. $c$ can be solved numerically from \cite{giraldo2020adversarial}: 

\begin{equation}
    \frac{2b^2}{c^2 - b^2} + \ln{(1-\frac{b^2}{c^2})}=\gamma.
    \label{eqn:c}
\end{equation} 

Here, $\gamma$ is the degree of knowledge poisoning; a high $\gamma$ implies a large malicious noise injection (i.e., a higher attack gain) and vice versa. In particular, choosing a high $\gamma$ can lead to unrealistically large Q-values whereas choosing a minuscule $\gamma$ can result in negligible to almost zero attack gain. Consequently, tuning $\gamma$ for an optimal attack is non-trivial but challenging, which, unfortunately, overlooked by literature so far. We address this research gap in section~\ref{s:methodology}. Figure~\ref{fig:outlier_rmse}(a) demonstrates the influence of $\kappa$ and $\gamma$ on detected outliers and RMSE. By adding LDP-noise to $100$ uniform random values, non-DP Q-values detect a steady number of outliers for a fixed $\tau$, whereas LDP implementation significantly increases outlier detection due to benign DP Q-values flagged as false positives. This can be mitigated by setting $\tau' = \tau\times\kappa$. Moreover, an optimal attack approach as per (\ref{eqn:attackDist}) allows successful detection evasion, maintaining the baseline outlier count while inflating the system's RMSE, as shown in Fig.~\ref{fig:outlier_rmse}(b).
\begin{figure}[!ht]
\centerline{\includegraphics[width=1.0\linewidth]{Figures/Outlier_RMSE.pdf}}
\caption{\small (a) Impact of tolerance multiplier, $\kappa$ over detected outliers in both non-DP and DP settings, (b) Impact of degree of knowledge poisoning, $\gamma$ over attack evasion (difference in outlier count between non-attack and attack scenario) and attack gain (System's RMSE).}
\label{fig:outlier_rmse}
\end{figure}

\subsection{Attacker's Capability and Knowledge} We contemplate an attacker manipulating knowledge submissions to an advisee, either by exploiting susceptible agents (internal threats) or by compromising communication channels (external threats) (Fig.~\ref{fig:poisoning_framework}a, b). The attacker, in line with SOTA research \cite{dwork2019differential}, is presumed to know the publicly available $\varepsilon$-value and noise distribution.
\begin{figure}[!t]
\centerline{\includegraphics[width=\linewidth]
{Figures/poisoning_framework.pdf}}
\caption{\small (a) Internal poisoning: Attacker compromises advisors and replaces benign LDP process with adversarial LDP process, (b) External poisoning: Attacker compromises the communication path and injects additional malicious noise.} 
\label{fig:poisoning_framework}
\end{figure}
\begin{figure*}[!t]
\centerline{\includegraphics[width=\textwidth]{Figures/comparison_convergence.pdf}}
\caption{\small Average steps to goal ($\bar{\Pi}$) and obtained reward ($\bar{\Phi}$) analysis for (a) small ($\mathpzc{H}\times\mathpzc{W}=5\times 5, \mathpzc{N}=5, \mathpzc{O}=1$), (b) medium ($\mathpzc{H}\times\mathpzc{W}=10\times 10, \mathpzc{N}=10, \mathpzc{O}=3$), and (c) large-scale ($\mathpzc{H}\times\mathpzc{W}=15\times15, \mathpzc{N}=20, \mathpzc{O}=5$) environments. The number of steps is increased as well as the maximum reward achievement is delayed with more attacks (large attacker ratio). Also, (d) convergence is delayed for both $20\%$ and $40\%$ attacks compared to the no-attack baseline.} 
\label{fig:comparison_convergence}
\end{figure*}
\subsection{Proposed PeLPA Algorithm} \label{s:methodology}
A malevolent advisor, $p_a \in \mathpzc{N}$, could disrupt $p_i$'s convergence by transmitting erroneous information during the knowledge-sharing phase. Having knowledge of $\mathpzc{A, S}, \Phi, (x_{\mathpzc{G}},y_{\mathpzc{G}})$ and $p_i$'s state, $s$, $p_a$ might manipulate larger $Q$-values for a misleading action $a_m$ versus an ideal action $a_h$. This would steer $p_i$ towards a malicious point. Yet, anomalous Q-values could either invite detection or result in an insignificant attack impact. The optimal attack method in section~\ref{noiseProfile} addresses this trade-off. Our proposed PeLPA attack for LDP-CMARL is detailed in Algorithm~\ref{algo:Attack}. $p_a$ continually injects adversarial noises ($\eta_a$) to its Q-values ($Q_a(s,a)$) until    either the malicious Q-values drop below $p_i$'s maximum Q-value for an action $a$, or $\gamma$ exceeds a predetermined poisoning threshold ($\tau_{\gamma}$). Additionally, $p_a$ ensures malicious advice stays within the reward range $\bar{Q}_{p_a}(s, a) \in [l,u]$ to evade detection.

\begin{algorithm}[!ht]
    \SetKwInOut{KwIn}{Input}
    \SetKwInOut{KwOut}{Output}
    \SetKwRepeat{Do}{do}{while}
    \KwIn{$\varepsilon, b, \alpha, Q_{p_i}(s), Q_{p_a}(s)$}
    Initialize $\bar{Q}_{p_a}(s) = \text{[}\;\text{]}$ and set $\gamma\leftarrow 0, \Psi\leftarrow True, \theta\leftarrow 0$\\
    \While{$\Psi$ is True}{
        $\gamma= \gamma+1$\\
        With $b$ and $\gamma$, find $c$ numerically from (\ref{eqn:c})\\
        Then, with $c, \theta$ and $b$, find $\mu^*_{p_a}$ from (\ref{eqn:attackDist})\\
        \For{each $a\in\mathpzc{A}_i$ in state, $s$}{
            \While{$\bar{Q}_{p_a}(s, a) \notin (l, u)$}{
                $\bar{Q}_{p_a}(s,a) = Q_{p_a}(s,a) +\eta_a\sim\mathcal{N}(\mu^*_{p_a}, b)$}
            Append $\bar{Q}_{p_a}(s,a)$ to $\bar{Q}_{p_a}(s)$\\
        }
        $\bar{Q}^*_{p_a}(s) $ =
            $\begin{cases}
                \begin{aligned}
                    & \bar{Q}_{p_a}(s) \;\text{and} \\
                    & \hskip1em\Psi \leftarrow False,
                \end{aligned} & 
                \begin{array}{rr}
                    & \text{if }\bar{Q}_{p_a}(s,a)<Q_{p_i}(s,a) \text{ s.t. }\\
                    &  a \text{ for }{}_{max}Q_{p_i}(s) \text{ or } \gamma>\tau_\gamma
                \end{array}\\
                
                % \bar{Q}_{p_a}(s,a)<Q_{p_i}(s,a) | a \text{ for }{}_{max}Q_{p_i}(s)\\
            Continue\text{,} & \text{Otherwise until } \gamma\leq\tau_\gamma
        \end{cases}$\\

        Set $\bar{Q}_{p_a}(s) = \text{[ ]}$   
    }
    \KwRet{$\bar{Q}^*_{p_a}(s)$}
    
    \caption{Proposed PeLPA Algorithm}
    
    \label{algo:Attack}
    
\end{algorithm}





