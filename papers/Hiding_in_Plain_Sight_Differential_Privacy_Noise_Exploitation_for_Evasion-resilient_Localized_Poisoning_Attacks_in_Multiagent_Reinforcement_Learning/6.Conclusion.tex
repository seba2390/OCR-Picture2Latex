\section{Conclusions} \label{s:Conclusion}

This paper highlights the potential security risks of using DP in CMARL algorithms and proposes a new adaptive and localized knowledge poisoning attack technique (PeLPA) to exploit DP-noise and prevent optimal convergence of the CMARL model. The proposed PeLPA technique is designed to evade SOTA anomaly detection techniques and degrade the multiagent learning performance. The effectiveness of the proposed attack technique is demonstrated through extensive experimental analysis in varying environment scales. The study fills a research gap in the literature and sheds light on the need for stronger security measures in LDP-CMARL systems.