\section{Introduction} \label{s:Introduction}

Cooperative multiagent reinforcement learning (CMARL) has been acknowledged for its proficiency in orchestrating complex tasks, such as automated robotic swarming and distributed power system optimization, through multi-agent collaboration \cite{Zhou2022,da2017simultaneously,le2017coordinated}. However, the inherent nature of data sharing in CMARL can trigger potential privacy infringements, as the shared experiences often encompass sensitive data \cite{li2022privacy,zou2020privacy}. To combat this, differential privacy (DP) mechanisms \cite{dwork2006}, which employ stochastic noise addition to obfuscate sensitive data, are posited as effective countermeasures \cite{li2022privacy,ye2022differential, Abahussein2023,ye2022one}.

Yet, we conjecture that adversaries could exploit DP's noise-adding mechanism to craft their own malicious noise in CMARL, thereby degrading the learning efficacy while remaining undetected by hiding behind the DP-noise, leading to catastrophic implications in sectors like robotics, cyber-physical systems, automotive industries, etc. \cite{scheikl2021cooperative,la2014multirobot,prasad2019multi,kiran2021deep}. For example, false advising with DP-exploited misleading knowledge from advisor cars in autonomous driving may make lane-changing ambiguous and lead to severe road accidents. Contemporary state-of-the-art (SOTA) poisoning attacks typically focus on voluminous malicious data injection, which is prone to detection, leaving the creation of subtle, stealthy adversarial instances as a formidable challenge \cite{cao2021data,fang2020local, mohammadi2023implicit,cheu2021manipulation}.

Addressing this challenge, our research proposes a novel adversarial model tailored for CMARL that exploits DP-induced noise to facilitate stealthy, localized poisoning attacks \cite{Hossain2021Desmp,giraldo2020adversarial,hossain2022adversarial}. To our knowledge, this is the first investigation into DP-noise exploitation for conducting local poisoning attacks while evading detection in CMARL. Our contributions are:

\begin{itemize}
\item Uncovering the susceptibility of DP mechanisms to adversarial poisoning attacks, illustrating how adversaries can adaptively perturb knowledge to remain undetected.
\item Proposing a novel privacy-exploiting local poisoning attack (PeLPA), contrasting general poisoning attacks that overlook the importance of attack stealthiness.
\item Experimentally evaluating the potential ramifications of DP-exploited stealthy attacks in safety-critical sectors.
\end{itemize}

The terms knowledge', experience', advice', and Q-value' are used interchangeably throughout the paper.