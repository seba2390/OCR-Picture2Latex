\section{Experimental Analysis} \label{s:experimentalAnalysis}
In this section, we implement our proposed PeLPA attack in a modified predator-prey domain, following the environmental specifications detailed in section~\ref{subsec:CMARL} \cite{le2017coordinated}. The environment consists of multiple predator agents and one prey. The environment is reset if the initial agent doesn't achieve the goal within a specified number of steps. Table~\ref{tab:parameter} presents the experimental parameters. For comparative insight, we investigate three environment scales: \textbf{small-scale (5x5)}, \textbf{medium-scale (10x10)}, and \textbf{large-scale (15x15)}, exploring $0\%$, $20\%$, and $40\%$ attacker percentages in each. Each experiment is repeated 10 times to average results. We use a privacy budget $\varepsilon = 1.0$ for all results presented, even though a smaller $\varepsilon$ would indicate stronger privacy protection, albeit with larger attack gains.

\textbf{Steps to Goal ($\Pi$) Analysis.} 
The $\bar{\Pi}$-values represent the average steps an agent takes to achieve the goal, with lower values indicating efficient learning. The top three charts of Fig. \ref{fig:comparison_convergence}(a-c) reveals an increase in the required step count to reach the goal as the attacker ratio rises and the environment expands. For example, after $5000$ episodes in a medium-scale environment, $\bar{\Pi} = \{7.52, 11.332, 12.364\}$ for $\{0\%, 20\%, 40\%\}$ attackers, leading to a $\frac{(11.332-7.52)\times 100}{7.52}\approx 50.69\%$ and $\frac{(12.364-7.52)\times 100}{7.52}\approx 64.41\%$ increase in average \textit{steps to goal} for $20\%$ and $40\%$ attackers, respectively.

\textbf{Reward ($\Phi$) Analysis.} Similarly, the $\bar{\Phi}$-values represent average rewards obtained by agents as shown in the bottom three charts of Fig. \ref{fig:comparison_convergence}(a-c). Our experiments exhibit a decrease in the speed of obtaining optimal rewards as the attacker ratio escalates. For instance, in a medium-scale environment, $\{2500, 3500, 4000\}$ episodes are requisite to attain the optimal $\bar{\Phi}$, for $\{0\%, 20\%, 40\%\}$ attackers, respectively. This leads to a $\frac{3500}{2500} \approx 1.4$x and $\frac{4000}{2500} \approx 1.6$x time increase in optimal $\bar{\Phi}$ acquisition for $20\%$ and $40\%$ attackers, respectively.
\setlength{\textfloatsep}{8pt}% 
\begin{table}[!ht]
\centering
\caption{\small Parameter value. $\alpha$: learning rate, $\epsilon$: exploration-exploitation probability, $\Gamma$: discount factor, $B$: communication budget, $w$: aggregation factor, $\tau, \tau', \tau_{\gamma}$: predefined threshold, $\phi$: reward, $\varepsilon$: privacy budget.}
\label{tab:parameter}
\begin{adjustbox}{max width=\linewidth}
\begin{tabular}{c|c|c|c|c|c|c|c}
\toprule
Parameter &
$\alpha$                                                      & $\epsilon$                                                    & $\Gamma$                                                      & \multicolumn{1}{c|}{$B^{tot}_{p_i}$}           & $B^{tot}_{p_a}$                          & $w$                                          & $\tau_{\gamma}$   \\ 
\midrule
Value &
0.10                                                                           & 0.08                                                                           & 0.80                                                                           & 100,000                           & 10,000                                              & 0.90                                           & 12                       \\
\midrule
\multicolumn{1}{c|}{Parameter} &
\multicolumn{1}{c|}{$\phi_{\mathpzc{G}}$} & \multicolumn{1}{c|}{$\phi_{\mathpzc{F}}$} & \multicolumn{1}{c|}{$\phi_{\mathpzc{O}}$} & $\phi_{\mathpzc{W}}$ & \multicolumn{1}{l|}{$\varepsilon$} & \multicolumn{1}{c|}{$\tau$} & $\tau'$  \\ 
\midrule
\multicolumn{1}{c|}{Value}  & 
\multicolumn{1}{c|}{10.0}                                                      & \multicolumn{1}{l|}{0.50}                                                      & \multicolumn{1}{c|}{-1.50}                                                     & -0.50                                                     & \multicolumn{1}{l|}{1.0}                            & \multicolumn{1}{l|}{100}                      & 100,000   \\
\bottomrule
\end{tabular}
\end{adjustbox}
\end{table}

\textbf{Convergence ($\Delta Q$) Analysis.} 
To gauge the effectiveness of our proposed attack, we conduct a convergence analysis based on $\overline{\Delta Q}$ values, i.e., the average of the deviation of Q-values from the optimal value ($Q^*$). An optimal learning process would have $\overline{\Delta Q}$ values tending to zero, and our analysis confirms this behavior is impeded as the attacker ratio increases. This delay in convergence correlates with the increase in attacker prevalence. Specifically, in a medium-scale environment, $\overline{\Delta Q}$ falls below $10e^{-6}$ following $\{2360, 2800, 3280\}$ episodes for $\{0\%, 20\%, 40\%\}$ attackers. Consequently, convergence is delayed by $\frac{2800}{2360}\approx 1.18$x and $\frac{3280}{2360}\approx 1.38$x for attacker ratios of $20\%$ and $40\%$, respectively.

\textbf{Adaptive Degree of Knowledge Poisoning ($\gamma$).}
\label{degreeOfPoisoning}
Finally, we consider the degree of knowledge poisoning, $\gamma$, demonstrating its distribution and symmetry in various scenarios as shown in Fig.~\ref{fig:degree}. This parameter is adjusted following line $10$ in Algorithm~\ref{algo:Attack}, showing varied instances of its manipulation across different episodes. We only present the episodes in which the attacker adjusted the $\gamma$ value more than $20$ times. For example, in episode $1146$, the attacker maintained the $\gamma$ value under $5$ for most of the steps but increased it to more than $10$ for a few steps. Contrarily, in episode $2027$, the attacker never sets $\gamma$ in the range of $\left[5,10\right]$.

\begin{figure}[!t]
\centerline{\includegraphics[width=\linewidth]{Figures/distributionOfPoisoning.pdf}}
\caption{\small Distribution of the degree of knowledge poisoning, ($\gamma$) in some example episodes. For instance, in episode $1146$, the attacker maintained the $\gamma$ value under $5$ for most of the steps but increased it to more than $10$ for a few steps.}
\label{fig:degree}
\end{figure}


