\section{Local Differentially Private Cooperative Multiagent Reinforcement Learning}
\label{s:DP_CMARL}

We present a local differentially private CMARL (LDP-CMARL) framework akin to the one adopted in \cite{hossain2023BRNES}. However, for demonstration simplicity, instead of a generalized randomized response (GRR) technique, we leverage a Bounded Laplace (BLP) mechanism \cite{Neera2023} to model our LDP framework that also achieves the same $\varepsilon$-LDP guarantee.

\subsection{Cooperative Multiagent Reinforcement Learning (CMARL)}
\label{subsec:CMARL} \textbf{Environment model.} Our research formalizes a cooperative reinforcement learning context with a Markov game $\mathcal{M} =(\mathpzc{N}, \mathpzc{S}, \mathpzc{A}, \Phi, \Gamma, \mathpzc{T})$ incorporating $\mathpzc{N}$ robots navigating an environment $\mathbb{E}$ of dimensions height ($\mathpzc{H}$) and width ($\mathpzc{W}$) towards a goal $\mathpzc{G}$. It introduces obstacles $\mathpzc{O}$ and freeway $\mathpzc{F}$ with corresponding reward penalties and incentives, $\phi_{\mathpzc{O}}$ and $\phi_{\mathpzc{F}}$. Dynamic obstacle positioning adds complexity to learning, which concludes when the first agent reaches $\mathpzc{G}$.

\textbf{Learning objectives.} Agent $p_i$'s objective is to take the fewest steps, $\Pi$ to reach $\mathpzc{G}$, collect $\phi_f$, avoid hitting $o_x \in \mathpzc{O}$, and earn as much as rewards, $\phi_{\mathpzc{F},\mathpzc{G}}$. In short, the objectives can be formalized as
\begin{equation}
\begin{alignedat}{2}
(a)&\;\Pi{p_i} = \underset{\mathcal{M}}{min}\;\Pi\\
(b)&\;\phi_{p_i} = \phi_{\mathpzc{F}} + \phi_{\mathpzc{G}} + \left[\phi_{\mathpzc{O}} = 0\right]\\
(c)&\;\lVert (x_{p_i}, y_{p_i})- (x_{\mathpzc{G}}, y_{\mathpzc{G}}) \rVert = 0
 \end{alignedat}
 \forall \phi_{\mathpzc{G}, \mathpzc{F}, \mathpzc{O}} \in \Phi \text{ and } \phi_{\mathpzc{G}} > \phi_{\mathpzc{F}}
 \label{eqn:objective1}
\end{equation}

where $(x_{p_i}, y_{p_i})$, and $(x_{\mathpzc{G}}, y_{\mathpzc{G}})$ are $p_i$'s and $\mathpzc{G}$'s positions.

\subsection{Integrating Local Differential Privacy (LDP) in CMARL}
LDP protocols encapsulate two main stages: perturbation and aggregation. The Q-values domain, denoted as $\mathbb{Q} = \left[q\right]$, undergoes local perturbation before being relayed to the advisee, $p_i$, ensuring $p_i$'s inability to infer the original Q-value of the advisor, $p_k$. The aggregation phase facilitates $p_i$'s estimation of optimal advice utilizing the perturbed values received from all $p_k$, with perturbation function for Q-values of all actions, $a$ in state $s$ represented as $P(Q(s))$. Following the definition of $\varepsilon$-LDP \cite{cao2021data}, a protocol achieving LDP must ensure the probabilistic resemblance between any pair of perturbed Q-values.

LDP offers plausible deniability to $p_k$, restraining $p_i$ from determining the origin of the output confidently. This ambiguity is regulated by the privacy budget, $\varepsilon$ \cite{dwork2006}. To actualize $(\varepsilon, 0)$-DP, the Laplace mechanism, a noise-addition technique, is applied as follows \cite{dwork2006}:

\begin{equation}
    \mathpzc{M}(D) = f(D) + \eta \sim \mathcal{N}(0, b)
\end{equation}

where the added
noise, $\eta$ is drawn from a zero-mean Laplace distribution with
scale parameter, $b \geq \frac{\Delta}{\varepsilon}$. Here, $\Delta$ denotes the sensitivity of the query function. Nonetheless, the same Laplace mechanism that satisfies $(\varepsilon,0)$-DP,  can be deployed in a distributed fashion for achieving $\varepsilon$-LDP \cite{wang2020comprehensive, Neera2023}, by integrating randomized Laplace noise into each state-action pair's Q-values of an advisor. We leverage the higher noise sensitivity offered by the Laplace mechanism to attain stronger privacy protection as compared to Gaussian or Exponential mechanism. The advisee, $p_i$ computes the average value from all the noisy Q-values \cite{wang2020comprehensive}. We utilize the following BLP technique for input perturbation \cite{Neera2023}: 

\begin{definition}[Bounded Laplace Mechanism (BLP)]
   Given an input $q \in \left[l, u\right] \subset \mathbb{R}$, and scale $b>0$, the BLP technique, $\mathpzc{M}: \Omega \rightarrow \left[l, u\right]$ over output $\bar{q}$ can be represented by the following conditional probability density function (pdf):
   
   \begin{equation}
       f\mathpzc{M}(\bar{q}) = 
       \begin{cases}
       0 & \text{ if } \bar{q} \notin \left[l,u\right]\\
       \frac{1}{C_q} \frac{1}{2b}e^{-\frac{\lvert \bar{q}- q\rvert}{b}} & \text{ if } \bar{q} \in \left[l,u\right]
       \end{cases}
   \end{equation}
\end{definition}
where $l$ and $u$ are the lower and upper range, and $C_q = \int_{l}^{u} \frac{1}{2b}e^{-\frac{|\bar{q}- q|}{b}} \,d\bar{q}$ is a normalization constant. The proof and further details can be found in \cite{Neera2023}. BLP constrains noise sampling within a predefined range, avoiding values that may detriment learning performance. Hence, the sensitivity of the combined LDP mechanism is $\Delta = \lvert u-l\rvert$. Similar to \cite{ye2022differential}, within our LDP-CMARL framework, the sensitivity $\Delta$ needs to be calculated carefully. The LDP-CMARL framework training stages utilizing the BLP mechanism are outlined in Algorithm~\ref{algo:DP_CMARL}. During advice request dispatch, $p_i$ specifies a neighbor zone, $\mathpzc{Z}$, and sends advice requests only to advisors within $\mathpzc{Z}$. Both $p_i$ and $p_k$ calculate their advice requesting ($\varrho_{p_i}$) and advice giving ($\varrho_{p_k}$) probabilities as per \cite{ye2022differential}. After receiving advice from the neighbors, $p_i$ aggregates all the advice following a weighted linear aggregation technique, controlled by a predefined weight parameter, $w$ \cite{hossain2023BRNES}. Then, $p_i$ selects and executes an optimal action followed by a final Q-table update.

\setlength{\textfloatsep}{0pt}% 
\begin{algorithm}[!t]
    \SetKwFunction{LDP}{LDP}
    \SetKwProg{Fn}{Function}{:}{}
    \SetKwInOut{KwIn}{Input}
    \SetKwInOut{KwOut}{Output}

    \KwIn{$\mathbb{E}, \mathpzc{N}, \mathpzc{A}, \mathpzc{S}, \Phi \rightarrow (l, u)$}
    \KwOut{Trained LDP-CMARL model}

    Initialize Q-table, set $\varepsilon, \alpha, \Gamma$, and compute $b =\frac{\alpha\lvert u-l\rvert}{\varepsilon}$

    \For{each agent, $p_i \in \mathpzc{N}$}{
        \For{each episode}{
            Initialize state, $s$
            \For{each state}{
                Send advice request to $p_k$ in $\mathpzc{Z}$ with $\varrho_{p_i}$\\
                Receive LDP-advice, $\LDP(s, \varepsilon, b)\rightarrow \bar{Q}_{p_i}(s) = \left[\bar{Q}_i(s)\right]_{i=1}^k$\\ 
                \For{each action $a\in \mathpzc{A}_i$ in state, $s$}{
                    Find weighted Q-value, $Q_{p_i}^*(s,a) = w\cdot Q_{p_i}(s, a) + (1-w)(\frac{1}{k}\sum_{i=1}^{k} \bar{Q}_i(s, a))$\\
                    Append $Q_{p_i}^*(s,a)$ to $Q_{p_i}^*(s)$
                }
            Update Q-table with $Q_{p_i}^*(s)$\\
            Choose $a^*\in \mathpzc{A}_i$ for $s$ using $\epsilon$-greedy policy\\
            Execute action, $a^*$, observe $\phi_{p_i}, s'$\\
            Perform $Q_{p_i}(s,a) \leftarrow (1-\alpha)Q_{p_i}(s,a) + \alpha \left[\phi_{p_i}+\Gamma \; \underset{a'}{max}\;Q(s', a')\right]$\\
            Set, $s\leftarrow s'$
            }
            \textbf{If }$\lVert(x_{p_i}, y_{p_i})-(x_{\mathpzc{G}, y_{\mathpzc{G}}})>0\rVert$ \textbf{then}
                continue\\
                \textbf{else} end episode and reset $\mathbb{E}$
        }
    }
    \KwRet{Trained LDP-CMARL model}\\
    \Fn{\LDP{$s, \varepsilon$, b}}{
        \For{$i = 1, 2, ..., k$ advisors}{
            Receive advice request for the state, $s$\\
            With $\varrho_{p_k}$, \For{each action $a\in \mathpzc{A}_i$}{
                find $Q_i(s, a)$ and generate $\eta_i\sim\mathcal{N}(0, b)$
                %\; \forall\; b \geq \frac{\alpha\lvert u-l\rvert}{\varepsilon}$
                \\
                Add LDP-noise,
                $\bar{Q}_i(s, a) = Q_i(s, a)+\eta_i$\\
                \eIf{$\bar{Q}_i(s, a) \notin (l, u)$}{
                    Repeat loop until $\bar{Q}_i(s, a) \in (l, u)$
                }{
                Append $\bar{Q}_i(s, a)$ to $\bar{Q}_i(s)$
                }
            }
            \KwRet{$\bar{Q}_i(s)$}
        }
        \KwRet{$\left[\bar{Q}_i(s) \right]_{i=1}^k$}  
        
    }
    \caption{LDP-CMARL Framework}
    \label{algo:DP_CMARL}
\end{algorithm}