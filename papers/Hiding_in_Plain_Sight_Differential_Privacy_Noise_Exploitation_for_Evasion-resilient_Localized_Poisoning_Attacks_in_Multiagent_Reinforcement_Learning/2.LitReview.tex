\section{Related Works} \label{s:litReview}
In this section, we address the SOTA poisoning techniques.
\subsection{Application of Differential Privacy for Knowledge Sharing}

DP, a prominent method for privacy preservation, has been extensively employed in private knowledge sharing within the realm of CMARL \cite{ye2022differential,wang2019privacy,li2022privacy, Wei2022, Abahussein2023,ye2022one}. The scope of its application includes DP-guided Q-learning models to maintain the privacy of reward data \cite{wang2019privacy}, privacy-centric multi-agent frameworks leveraging federated learning (FL) and DP to obstruct illegitimate access to data statistics \cite{li2022privacy}, and harnessing ($\beta, \phi$)-DP to counteract offloading preference inference attacks in vehicular ad-hoc networks (VANET) \cite{Wei2022}. Apart from protecting user information during private knowledge sharing, DP has also been proposed for differential advising. In particular, Ye et al. \cite{ye2022differential} propose a DP-based advising method for CMARL that enables agents to use the advice in a state even if the advice is created in a slightly different state. \textit{Nevertheless, they overlook the susceptibility of DP to poisoning attacks during knowledge sharing \cite{ye2022differential, Abahussein2023,ye2022one}.}

\subsection{Poisoning Attacks in Cooperative Multiagent Learning} The infiltration of poisoning attacks in CMARL, which can alter training datasets and consequently disrupt learning outcomes, is a pertinent research concern \cite{figura2021adversarial,fang2020local, Xie2022,mohammadi2023implicit}. Research has delved into scenarios where adversarial agents can manipulate network-wide policies \cite{figura2021adversarial}, scrutinized targeted poisoning attacks in dual-agent frameworks where one agentâ€™s policy is modified \cite{mohammadi2023implicit}, and investigated the implications of soft actor-critic algorithms in CMARL for executing poisoning attacks \cite{Xie2022}. For instance, Figura et al. \cite{figura2021adversarial} demonstrate that an adversarial agent can persuade all other agents in the network to implement policies that optimize its desired objective. Another approach for performing poisoning attacks by any malicious advisor in multiagent Q-learning as demonstrated in \cite{hossain2023BRNES}, is to shuffle the Q-values for all actions corresponding to the requested state
and inject false noise that is similar to the maximum reward using reward poisoning method. \textit{However, the ramifications of these SOTA poisoning techniques against anomaly detection and privacy-preserving knowledge-sharing technologies remain largely unexplored. Our work endeavors to model a DP noise-exploiting poisoning attack that remains resilient to detection algorithms.}

\subsection{Differential Privacy Exploitation Techniques} Another domain of interest focuses on the possible exploitation of DP in classification challenges, even though it does not necessarily concentrate on adversarial onslaughts on CMARL algorithms \cite{giraldo2017security_2, giraldo2020adversarial, hossain2021privacy,cao2021data,cheu2021manipulation,hossain2022adversarial, Hossain2021Desmp}. This research trajectory involves the systemic degradation of utility by exploiting DP noise \cite{giraldo2020adversarial}, gauging the impact of DP manipulation in smart grid networks \cite{hossain2021privacy}, and designing stealthy model poisoning attacks on an FL model \cite{Hossain2021Desmp, hossain2022adversarial}. Similarly, \cite{hossain2021privacy} investigates the impact of DP exploitation in a smart grid network and introduces a correlation among DP parameters to enable the system designer to calibrate the privacy level and reduce the attack surface. To examine the effect of DP-exploiting attacks on an FL model, \cite{Hossain2021Desmp} proposes a stealthy model poisoning attack leveraging DP noise added to ensure privacy. They improve their attack technique in \cite{hossain2022adversarial}, investigating how the degree of model poisoning can be adjusted dynamically through episodic loss memorization in FL and demonstrating how their attack can evade some SOTA defense techniques, such as norm, accuracy, and mix detection. \textit{However, these attack models face constraints in multi-agent environments or decentralized CMARL platforms.} Contrarily, Cao et al. \cite{cao2021data} propose an attack on the Local Differential Privacy (LDP) protocol by introducing fraudulent users. \textit{Our research, however, targets legitimate yet compromised users infusing false noise into shared data, also aiming to dodge anomaly detectors - a critical objective for a successful attack.}