\section{Study Design}

\subsection{Stimuli: Data}

We evaluated a single network with $258$ nodes and $1090$ edges, representing cooking ingredients connected by edges when frequently used together in recipes. The density of the network was $0.016$ (computed as $\#edges/\#nodes^2$). This network had been explored previously by Ahn {\it et al.}~\cite{ahn2011flavor}. In its original form, the network is larger ($381$ nodes) but we reduced it slightly to ensure it could be visualized smoothly in a browser. We did so by removing disconnected components and low-weight edges.  
Evaluating a single dataset allowed us to cover a broad spectrum of tasks while keeping the size of the study manageable, but naturally, this choice has several limitations, discussed in section 5.

\vspace{2mm}
\noindent\textbf{Rationale:} Our motivation for choosing our network was three-fold. First, it is {\it different than those evaluated already}. Our network is $2.5$ and $5$ times larger than those evaluated by Ghoniem {\it et al.} and Keller {\it et al.}. 
%It is also exemplary for small-scale, real-world networks, which are more common in real-word applications. 
%Furthermore, Ghoniem et al. conducted their study using  randomly generated.  
%one and exhibits non-random structure.
Second, our network was chosen as a {\it representative of several types of real-world networks}. Specifically,  we reviewed $17$ relational datasets (e.g.,  trade exchanges between countries,
%similarities between music artists and books, 
the Les Miserable dataset, 
TVCG paper co-authorships, 
protein-interaction networks). We selected one from this set that was representative in terms of structure and density, while at the same time sufficiently small to be evaluated in a browser. Our network has about $4$ times more edges than nodes. This was close to the average edge/node ratio in the $17$ networks we reviewed and 
%The edge-node ratio captures density variations in real-world networks better than more traditional density functions as it is less sensitive to number of nodes~\cite{melancon2006just}. Furthermore, following an analysis of $19$ real-world networks, Melancon notes that small-world networks with approximately 2-4 times as many edges are 
representative of many networks commonly found in practice~\cite{melancon2006just}. 
%This provided further motivation for the choice of dataset.
Third, we believe a dataset revolving around cooking ingredients would have a {\it greater appeal to participants}. Ingredients were shown as node labels and several tasks referred to ingredients by name. Relatable, concrete dataset may help users understand tasks better~\cite{dagstuhl}.


\subsection{Stimuli: Visual Encoding}

We evaluated two visual encodings: a node-link diagram (NL) drawn using the neato algorithm from graphviz~\cite{graphviz},  and an adjacency matrix (AM), sorted to reveal clusters using the barycenter algorithm available in the Reorder.js library~\cite{fekete2015reorder}. We clustered the network using modularity clustering from GMap~\cite{pacvis10} and encoded this information in the two visual representations using color, as shown in Fig.~1. Both visualizations were developed using the D3 web-library.

\vspace{2mm}
\noindent
\textbf{Rationale:} The neato algorithm is provided in popular layout tools such as graphviz and
frequently part of NL evaluations~\cite{ghoniem2004comparison,jianu2014display}.
%Ghoniem et al.~\cite{ghoniem2004comparison} and Jianu et al.~\cite{jianu2014display}.
We ordered our AM to reveal structure, as we considered this more representative of how matrices are used in practice, unlike  Ghoniem {\it et al.}~\cite{ghoniem2004comparison}, who used a lexicographical order.

\subsection{Stimuli: Interactions}

Both visualizations support panning and zooming, using the mouse-wheel. Multiple nodes can be selected by clicking on them, and deselected with an additional click. Selecting a node in NL colors both the node and its outgoing edges in purple. Selections in AM operate on node labels but change the color of the corresponding node's row or column. Similarly, node mouse-over in NL turns the node and its edges green and shows the node label via tooltips. Node mouse-over in AM colors the row or column. Note that for both node selection and node mouse-over in AM, if a row (column) is colored the complementary column (row) is not.  We chose this approach since both Ghoniem {\it at al.} and Okoe {\it et al.} mention that multiple markings for the same node can confuse users~\cite{ghoniem2004comparison,jianu2014display}.

To select a node as the answer to a task, the participants double-click it. This marks the node with a thick black contour. In both NL and AM this marking was restricted to nodes and labels, without extending to edges or rows/columns. The participants could also deselect an answer by double-clicking it again.

Similar interactions apply to edge selection: An edge mouse-over in NL turns the edge green, and if clicked it is selected and so turns purple. In AM, hovering over an edge-cell highlights its corresponding row and column in green, and clicking it selects the edge.
%and make the visual configuration permanent. 

\vspace{2mm}
\noindent
\textbf{Rationale:} We chose to evaluate interactive visualizations as interactivity is typical in real-world applications. Previous studies, such as those of Ghoniem {\it et al.} or Keller {\it et al.}, also used basic interactions for the same reason. Interactivity can significantly change the effectiveness of a visual encoding, however, and a careful choice of interactive techniques is warranted. 

Our goal was to use interactions that are {\it ecologically valid} (i.e., representative of interactions typical of NL or AM visualizations) and {\it fair} (i.e., providing similar functionality and power in both visualizations). To this end, we reviewed $9$ systems for network visualization (e.g., Gephi~\cite{bastian2009gephi}, Cytoscape~\cite{shannon2003cytoscape}, Tulip~\cite{auber2004tulip}), $12$ network evaluation papers (e.g., Ghoniem {\it et al.}\cite{ghoniem2004comparison}, Keller {\it et al.}~\cite{keller2006matrices}, Okoe {\it et al.}\cite{okoeecological}) 
%, Holten et al.\cite{holten2009user}),
and $6$ systems and papers for adjacency matrices (e.g., ZAME~\cite{elmqvist2008zame},TimeMatrix~\cite{yi2010timematrix}, work by Perin {\it et al.}~\cite{perin2014revisiting}, work by Henry {\it et al.}~\cite{henry2006matrixexplorer}). We cataloged the interactions described or available in these systems, as well as their particular implementation, and then selected the set of most common interactions.

This resulted in a set of interactions that both overlapped and differed slightly from those implemented in previous studies. Overlapping interactions were described above. New interactions included zooming and panning, which was required to solve some of the tasks. %tasks accurately participants had to use these interactions. 
We believe the addition of zooming and panning is valuable since such basic navigation is an integral part of real-life systems. Our node-link diagrams also allowed users to move nodes, an interaction that can be used to disambiguate cases in which nodes or edges overlap, and is ubiquitous in NL systems. This interaction does not have an equivalent in AM but is also not necessary as rows and columns are evenly spaced.

\begin{figure}[t]
  \centering
  \includegraphics[width=.43\linewidth]{images/Stimulus1_interaction.png}\hspace{.5cm} \includegraphics[width=.43\linewidth]{images/Stimulus2_interaction.png}
  \caption{Participants mouse-over nodes to highlight them (green) and click on nodes to select them (purple). Designating a node as the answer for a task answer is accomplished via a double-click, which draws a black contour around the node.}
	\label{fig:interactions}
\end{figure}

\subsection{Tasks}

We evaluated the $14$ tasks described in Table 1. Participants solved multiple repeats (generally $5$ or $10$) of each task. Task repeats were selected manually on the network so as to cover multiple levels of complexity. For example, our repeats included nodes with both low and large degrees (e.g., $T1$, $T2$), short and long paths (e.g., $T10$, $T13$), or nodes with few and many neighbors (e.g., $T4$). 

Three of our tasks warrant a more detailed discussion. We included two memorability tasks, ($T11$, $T14$). The former tested the ability of participants to recall data they had looked for or accessed at an earlier time, and is similar to memorability tasks evaluated by Saket {\it et al.}~\cite{Memorability_Saket2015}. The latter tested the ability of participants to recognize visual configurations they had viewed previously and is more similar to tasks used by Jianu {\it et al.} and Borkin {\it et al.}~\cite{jianu2014display,borkin2013makes}. Both memorability tasks were based on questions that the participants had to answer early in their session (i.e., $T9$ in group 4, and $T12$ in group 5) to prime the participants with a particular piece of information or visual configuration. A few minutes later, after performing a set of other  tasks (i.e, $T10$ in group 4, $T13$ in group 5), the participants were asked about the information from the earlier task. Finally, we added a path-estimation task ($T5$), which required the participants to estimate how far two nodes are, in terms of the shortest path between them. Timing constraints ensured that participants used perceptual mechanisms to give a best-guess response instead of ``computing'' the correct answer.  

\vspace{2mm}
\noindent
\textbf{Rationale:} Our overarching goal in selecting our tasks was to cover a wide spectrum of different and realistic network tasks. We selected tasks to cover the graph objects they provide answers about (i.e., nodes, edges, paths), as well as cover Lee {\it et al.}'s categories of graph-reading tasks, and Amar {\it et al.}'s general types of visualization tasks. Several of our tasks have been used before but under slightly different conditions. Additionally, 
we included tasks that go beyond previous studies comparing NL and AM, such as tasks involving clusters. 
We also included memorability tasks as they are a topic of growing interest in the visualization community~\cite{borkin2013makes,Memorability_Saket2015}. We also hypothesized there would be differences between the two visualizations in this respect. 
We included a path-estimation task~\cite{jianu2014display}, as it is a good representative of the ``Overview'' category of graph tasks, and underlies perceptual queries that users make on relational data. 


\begin{table*}[t]						{\tiny
	\centering					
		\begin{tabular}{|l|l|l|l|l|l|l|}				
		\hline			
Task&	Target&	Task tax.~\cite{lee2006task}&	Task tax.~\cite{amar2005low}&	Group&	\#Repeats&	Time\\\hline
\shortstack[l]{1. Given two highlighted nodes, select the \\one with the larger degree.}&	\shortstack[l]{node}&	\shortstack[l]{Topology\\ (adjacency)}&	\shortstack[l]{Retrieve value,\\ Sort}&	\shortstack[l]{1}&	\shortstack[l]{10}&	\shortstack[l]{15}\\\hline
\shortstack[l]{2. Given a highlighted node, select all its \\neighbors}&	\shortstack[l]{edge}&	\shortstack[l]{Topology \\(adjacency, \\accessability)}&	\shortstack[l]{Retrieve value, \\Filter}&	\shortstack[l]{1}&	\shortstack[l]{10}&	\shortstack[l]{25}\\\hline
\shortstack[l]{3. Given two clusters of highlighted nodes, \\which one is more interconnected?}&	\shortstack[l]{clusters,\\ cliques}&	\shortstack[l]{Overview \\(connectivity)}&	\shortstack[l]{Filter, Sort, \\Cluster}&	\shortstack[l]{1}&	\shortstack[l]{10}&	\shortstack[l]{10}\\\hline
\shortstack[l]{4. Given two highlighted nodes, select all \\of the common neighbors.}&	\shortstack[l]{edge}&	\shortstack[l]{Topology\\ (shared\\ neighbor)}&	\shortstack[l]{Retrieve value,\\ Filter}&	\shortstack[l]{2}&	\shortstack[l]{10}&	\shortstack[l]{30}\\\hline
\shortstack[l]{5. Given two pairs of highlighted nodes \\(red and blue) and limited time, estimate\\ which pair is closer in terms of graph \\topology?}&	\shortstack[l]{path, \\edge}&	\shortstack[l]{Overview\\ (connectivity)}&	\shortstack[l]{Derive value,\\ Sort}&	\shortstack[l]{2}&	\shortstack[l]{10}&	\shortstack[l]{10}\\\hline
\shortstack[l]{6. How many clusters are there in the\\ visualization? \\ $^\ast$clusters shown via color (section 3.2)}&	\shortstack[l]{clusters}&	\shortstack[l]{Overview\\ (connectivity)}&	\shortstack[l]{Derive \\value}&	\shortstack[l]{3}&	\shortstack[l]{1}&	\shortstack[l]{10}\\\hline
\shortstack[l]{7. Given two groups of highlighted nodes \\(e.g., red and blue) and limited time, \\estimate which group is larger. }&	\shortstack[l]{clusters}&	\shortstack[l]{Attribute \\based}&	\shortstack[l]{Filter, Sort,\\ Derive value, \\Correlate}&	\shortstack[l]{3}&	\shortstack[l]{10}&	\shortstack[l]{10}\\\hline
\shortstack[l]{8. Given two highlighted nodes decide \\whether they belong to the same cluster. \\ $^\ast$clusters shown via color (section 3.2)}&	\shortstack[l]{clusters,\\ nodes}&	\shortstack[l]{Attribute\\ based}&	\shortstack[l]{Cluster, \\Filter}&	\shortstack[l]{3}&	\shortstack[l]{10}&	\shortstack[l]{10}\\\hline
\shortstack[l]{9. Given one highlighted node and one \\named node, are they connected?}&	\shortstack[l]{edge}&	\shortstack[l]{Topology\\ (adjacency)}&	\shortstack[l]{Retrieve value}&	\shortstack[l]{4}&	\shortstack[l]{5}&	\shortstack[l]{20}\\\hline
\shortstack[l]{10. Given two highlighted nodes, how long \\is the shortest path between them?}&	\shortstack[l]{path, \\edge}&	\shortstack[l]{Topology\\ (connectivity)}&	\shortstack[l]{Retrieve value, \\Derived value,\\ filter}&	\shortstack[l]{4}&	\shortstack[l]{5}&	\shortstack[l]{60}\\\hline
\shortstack[l]{11. Memorability: After spending several \\minutes on task 10, can participants\\ remember the answers they gave to \\task 9, without access to the visualization?}&	\shortstack[l]{}&	\shortstack[l]{See section 3.4}&	\shortstack[l]{See section 3.4}&	\shortstack[l]{4}&	\shortstack[l]{5}&	\shortstack[l]{unlim}\\\hline
\shortstack[l]{12. Given two highlighted nodes and three \\named ones, which of the named nodes \\is connected to both highlighted nodes? \\(exemplified in Figure 3)}&	\shortstack[l]{edge}&	\shortstack[l]{Topology \\(shared neighbor)}&	\shortstack[l]{Retrieve value,\\ Filter}&	\shortstack[l]{5}&	\shortstack[l]{5}&	\shortstack[l]{60}\\\hline
\shortstack[l]{13. Given a selected node, how many nodes \\are within two edges' reach?}&	\shortstack[l]{edge}&	\shortstack[l]{Topology\\ (accessibility)}&	\shortstack[l]{Retrieve value,\\ Derive value, \\Filter}&	\shortstack[l]{5}&	\shortstack[l]{5}&	\shortstack[l]{60}\\\hline
\shortstack[l]{14. Memorability: After spending several \\minutes on tasks 13, can participants remember \\(i.e., select) which nodes were highlighted as \\part of task 12, if showed the visualization \\with the answers they gave to task 13 \\highlighted?}&	\shortstack[l]{}&	\shortstack[l]{**See paper \\body}&	\shortstack[l]{**See paper \\body}&	\shortstack[l]{5}&	\shortstack[l]{5}&	\shortstack[l]{unlim}\\\hline
\end{tabular}	
\vspace{.1cm}
		\caption{Tasks: the columns describe (i) the task, (ii) targeted network element, (iii-iv) task categories in Lee {\it et al.}'s and Amar {\it et al.}'s taxonomies, (v) group number the task was evaluated in, (vi) number of instances of this task type, (vii) task time limit (sec).	}			
		\label{tab:Table1}				}
\end{table*}

\subsection{Hypotheses}

Based on previous results by Ghoniem {\it et al.}~\cite{ghoniem2004comparison}, Keller {\it et al.}~\cite{keller2006matrices}, Okoe {\it et al.}~\cite{okoe2015graphunit}, Jianu {\it et al.}~\cite{jianu2014display}, and Saket {\it et al.}~\cite{saket2014node} we devised the null hypotheses:

\begin{itemize}
\item[] \textbf{H1}: There is no statistically significant difference in time and accuracy performance between using NL and AM for tasks involving the retrieval of information about nodes and direct connectivity ($T1$, $T2$, $T4$, $T9$, $T12$). 

\item[] \textbf{H2}: There is no statistically significant difference in time and accuracy performance between using NL and AM for connectivity and accessibility tasks involving paths of length greater than two ($T5$, $T10$, $T13$). 

\item[] \textbf{H3}: There is no statistically significant difference in time and accuracy performance between using NL and AM on group tasks ($T3,T6,T7,T8$).

\item[] \textbf{H4}: There is no statistically significant difference in memorability between using NL and AM.

\end{itemize}

\noindent We expected H1 to hold and H2 not to hold. We also thought H3 would hold, except for estimating group interconnectivity ($T6$), since estimating the number of non-overlapping dots in a square (AM) should be easier than estimating overlapping edges in an irregular 2D area (NL). Finally, we anticipated memorability would be higher in node-link diagrams due to its more distinguishable features.

\subsection{Design}

We used a between-subjects experiment with two conditions. We divided our $14$ task types into $5$ experimental groups, as shown in Table 1, and we evaluated each group separately. Each participant was allowed to participate in a single group and used just one of the two visualizations. We assigned participants to groups and conditions in a round-robin fashion. We aimed to collect data from around $50$ participants per condition. As  some participants did not complete the study, the total number of participants for whom we collected data varies slightly between conditions. 
All tasks were timed as shown in Table 1, with time limits determined experimentally through a pilot-study and chosen to allow most participants to complete the tasks, while moving the study along.  

\vspace{2mm}
\noindent
\textbf{Rationale}: Between-subject experiments are frequently used in the visualization community~\cite{jianu2014display,saket2014node,ziemkiewicz2008shaping,borkin2011evaluation,robertson2008effectiveness,kosara2010mechanical,micallef2012assessing}. One advantage of this design is the absence of learning effects between evaluated conditions. A disadvantage is the need for large numbers of participants, which is easily mitigated in a crowdsourced setting. Moreover, between-subjects designs are quicker (since only one condition is evaluated at a time) and  online participants prefer shorter studies.
%due to the lower  time and risk commitment. 

We divided the tasks into groups for the same reason. Having each participant evaluate all tasks 
would have resulted in excessively long sessions that participants would have found tiring. Having participants solve only subsets of tasks allowed us to reduce their time commitment. 
We used estimated task completion times to group tasks, aiming for an expected duration of about $15$ minutes.

We aimed for $50$  participants per condition, matching the numbers used in earlier crowdsourced studies~\cite{chapman2014visualizing,jianu2014display}. We decided to enforce short time-limits in order to limit and make uniform the total session duration across participants.

\subsection{Procedure}

We used Amazon's Mechanical Turk (MTurk) to crowdsource our study to a broad population. To account for variations in participant demographics during the day, we published study batches throughout the day. We ran conditions in parallel and directed incoming participants to them using a round-robin assignment, to ensure that the two conditions sampled participants from the same populations. The demographics of MTurk users are reported by Ross {\it et al.}~\cite{ross2010crowdworkers}.

Each incoming participant was first presented with an introduction to the study, dataset, the visualization they would see and use, and the tasks they would perform. Each task was exemplified in the introduction, as shown in Figure 2. Since our interactions relied on color, participants were administered a color-blindness test. Next came a training session which involved solving two instances of each type of task in their assigned group. During the training session the participants could check the correctness of their answers.

Finally, the participants were lead to the main part of the study. 
In the main part of the study, task instances of each type in an assigned group were shown to the participants. %in succession. 
For example, since group $1$ involved three distinct task types, participants assigned to it solved three consecutive sections of ten task-instances each. %Once these tasks were completed, 
At the end, we asked the participants for comments.
%report any inconvenience during the study, and to finalize their session.

We used GraphUnit~\cite{okoe2015graphunit} to create the study, deploy it, and collect data. Visualizations were shown on the left, while task instructions and answer widgets were shown on the right. Depending on each task, users answered by selecting nodes or by using interactive widgets (e.g., text-boxes, check-boxes). Time limits were enforced by showing  a count-down timer and hiding the visualization once the counter expired. To increase the chances of collecting clean data we awarded a bonus to the best result in each group and told participants that two of the task-instances were control tasks easy enough for anyone to solve. 


%\textbf{Choice rationale}: Crowdsourcing has been validated as an experimental tool~\cite{heer2010crowdsourcing}, and crowdsourced visualization experiments are becoming increasingly common. 
%We relied on studies of MTurk workers and practices~\cite{heer2010crowdsourcing} to deploy the study, control for data quality, and consider participant demographics.

