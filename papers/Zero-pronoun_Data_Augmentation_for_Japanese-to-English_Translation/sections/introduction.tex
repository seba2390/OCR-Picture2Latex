\section{Introduction}
While neural machine translation (NMT) has demonstrated high performance in single-sentence translation, it is still challenging to handle linguistic phenomena involving discourse contexts.
One such issue is the translation of {\it zero pronouns} (ZP) in Japanese-to-English translation.
In Japanese, subjects and objects are often omitted when the listener can infer them from the context. However, when translating them into English, the omitted words must be explicitly translated in most cases. For example, in the following sentence, the subject omitted in Japanese is the first person, and \textit{I} has to be output in English.

\begin{table}[h]
  \centering
  \begin{tabular}{ll}
  \ja{うなぎが}             & \ja{食べたいな}    \\
  unagi-ga         & tabe-tai-na              \\
  eel-\texttt{OBJ}          & eat-want-\texttt{PARTICLE}  \\
  \multicolumn{2}{l}{I feel like eating eel.}
\end{tabular}
\end{table}

The prediction of ZPs, essentially, requires understanding the topic and old information in the discourse, or referring to the world knowledge. On the other hand, linguistic information within the sentence may provide some clues \citep{kudo-etal-2015-anlp}.
For example, in the sentence above, the auxiliary verb \ja{たい} ({\it want}) suggests that the sentence expresses a subjective statement and thus the missing pronoun is the first person. Here we refer to such information as {\it local context}.

Correlations between local context and ZPs can be learned by the standard single-sentence neural machine translation, but it may not be possible under low-resource conditions. For example, the translation of conversations, which usually contain a large number of ZPs, is currently one of the under-resourced domains.

\begin{figure}[t]
\centering
\includegraphics[width=7.0cm]{data/zu.png}
\caption{The proposed method: ZP data augmentation}
\label{fig:zu}
\end{figure}

To address this problem, we propose {\bf zero pronoun data augmentation} to facilitate learning correlations between local context and ZPs (Figure \ref{fig:zu}).
We augment the training data by deleting personal pronouns in the source Japanese sentence.
This creates parallel data that include ZPs and provides additional training signals to learn to predict ZPs.
Our method is simple yet effective: it does not require any modification to the model architecture nor additional computation at inference time, but significantly improves the accuracy of the ZP translation.

