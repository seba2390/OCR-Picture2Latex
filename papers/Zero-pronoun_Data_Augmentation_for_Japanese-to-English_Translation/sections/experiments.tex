\section{ZP Data Augmentation}
\label{section:experiments}

In the previous section, we confirmed that local context is useful for predicting ZPs.
In this section, we examine the usefulness of ZP data augmentation for machine translation.

The method artificially creates training data containing ZPs by deleting pronouns in the source Japanese sentence along with the following particles.
The pronouns to be deleted are detected by string matching with manually created lists (Appendix \ref{appendix:list}).
The augmented data is supposed to provide useful training signals for learning correlations between ZPs and local context.


\subsection{Experimental Setups}
\minisection{Corpus}
We use the Document-aligned Japanese-English Conversation Parallel Corpus \citep{rikters-EtAl:2020:WMT}.
We also add an in-house conversational parallel corpus to the training data. The statistics of the corpus are shown in Table \ref{corpus:stat}.

\begin{table}[!h]
\centering
\begin{tabular}{cccc} \toprule
train    & train+pro\_aug     & dev       & test     \\ \midrule
\multicolumn{1}{c}{246,541} & \multicolumn{1}{c}{282,952} & \multicolumn{1}{c}{2,051} & \multicolumn{1}{c}{2,020} \\ \bottomrule
\end{tabular}
\caption{The number of sentences in the corpus.}
\label{corpus:stat}
\end{table}

\minisection{Model}
Transformer \citep{NIPS2017_3f5ee243} was used as the translation model. We adopt the hyperparameters recommended for the corpus of our size in \citet{araabi-monz-2020-optimizing} (Appendix \ref{appendix:hyperparameters}). In addition to the single-sentence translation, we also experimented with the 2to1 setting \citep{tiedemann-scherrer-2017-neural}, in which the previous sentence in the document is added to the input.

\minisection{Evaluation}
We evaluate the overall translation quality on the test set with BLEU \citep{papineni-etal-2002-bleu}.
We also conduct a targeted evaluation with the ZP evaluation dataset for Japanese-to-English translation \citep{shimazu-etal-2020-evaluation}. The ZP evaluation dataset contains 724 triples of a source sentence, a target sentence with a correct pronoun, and one with an incorrect pronoun.
To evaluate a translation model, we see if the model assigns a lower perplexity to the correct target sentence, and calculate the accuracy.

\subsection{Results}
The results of the experiment are shown in Table \ref{result:mt}.
We can observe that ZP data augmentation does not improve the BLEU score, but significantly improves the accuracy of ZP evaluation in both the 1to1 (83.6\% to 92.3\%) and 2to1 settings (89.3\% to 92.1\%). Our method yields a similar degree of improvement to the 2to1 setting in the ZP evaluation without any computational overhead at the inference time.

We also confirm that adding the previous context (2to1) does not improve BLEU but pronoun translation (83.6\% to 89.3\%), which conforms to observations in the previous study \citep{Jean2017DoesNM,shimazu-etal-2020-evaluation}.
However, this is not the case with the ZP data augmentation (92.3\% to 92.1\%).
We speculate that this is because longer inputs in the 2to1 setting make it more difficult for the model to find correlations between ZPs and local context.
