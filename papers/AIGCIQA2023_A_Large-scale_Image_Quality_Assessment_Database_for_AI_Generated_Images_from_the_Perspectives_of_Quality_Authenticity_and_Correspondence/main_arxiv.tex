% This is samplepaper.tex, a sample chapter demonstrating the
% LLNCS macro package for Springer Computer Science proceedings;
% Version 2.20 of 2017/10/04
%
\documentclass[runningheads]{llncs}
%
\usepackage{comment}
\usepackage{graphicx}
\usepackage{cite}
\usepackage{colortbl}
\usepackage{indentfirst}
\usepackage{color}
\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
%\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
%\usepackage{xcolor}         % colors
\usepackage{authblk}
\usepackage{hyperref}
\usepackage[cmyk]{xcolor}
% Used for displaying a sample figure. If possible, figure files should
% be included in EPS format.
%
% If you use the hyperref package, please uncomment the following line
% to display URLs in blue roman font according to Springer's eBook style:
% \renewcommand\UrlFont{\color{blue}\rmfamily}

\begin{document}
%
% \title{AIGCIQA2023:  A Large-scale Image Quality Assessment Database for AI Generated Images: from the Perspectives of Quality, Authenticity \\ and Correspondence }

% }


%\end{comment}
%******************

% CAMERA READY SUBMISSION
% \begin{comment}

\title{AIGCIQA2023:  A Large-scale Image Quality Assessment Database for AI Generated Images: from the Perspectives of Quality, Authenticity \\ and Correspondence}
% If the paper title is too long for the running head, you can set
% an abbreviated paper title here
%
\author{Jiarui Wang\inst{1}\and
Huiyu Duan\inst{1}\and
Jing Liu\inst{2}\and
Shi Chen\inst{3}\\
Xiongkuo Min\inst{1}$^*$, and Guangtao Zhai\inst{1}\thanks{Corresponding Authors.}
}

\authorrunning{F. Author et al.}

\institute{Shanghai Jiao Tong University, Shanghai, China \\
\email{\{wangjiarui,huiyuduan,minxiongkuo,zhaiguangtao\}@sjtu.edu.cn}
\and Tianjin University, Tianjin, China\\
\and Shanghai Second Polytechnic University, Shanghai, China\\}
% \end{comment}
%******************
\maketitle              % typeset the header of the contribution
\begin{abstract}
    
    Recent years have witnessed a rapid growth of Artificial Intelligence Generated Content (AIGC), among which with the development of text-to-image techniques, AI-based image generation has been applied to various fields.
    However, AI Generated Images (AIGIs) may have some unique distortions compared to natural images, thus many generated images are not qualified for real-world applications. 
    Consequently, it is important and significant to study subjective and objective Image Quality Assessment (IQA) methodologies for AIGIs. 
    %The human preference is a missing dimension of image quality that is not well tracked by existing mainstream evaluation metrics
    In this paper, in order to get a better understanding of the human visual preferences for AIGIs, a large-scale IQA database for AIGC is established, which is named as AIGCIQA2023. We first generate over 2000 images based on 6 state-of-the-art text-to-image generation models using 100 prompts.
    Based on these images, a well-organized subjective experiment is conducted to assess the human visual preferences for each image from three perspectives including \emph{\textbf{quality}}, \emph{\textbf{authenticity}} and \emph{\textbf{correspondence}}. 
    Finally, based on this large-scale database, we conduct a benchmark experiment to evaluate the performance of several state-of-the-art IQA metrics on our constructed database. The AIGCIQA2023 database and benchmark will be released to facilitate future research on \textcolor{magenta!100}{\url{https://github.com/wangjiarui153/AIGCIQA2023}}
    %%

\keywords{AI generated content (AIGC) \and text-to-image generation \and image quality assessment \and human visual preference }
\end{abstract}

%provides a more complete evaluation for individual text-to-image generations, making it better aligned with human preferences.
%
%
\section{Introduction}
% introduction of AIGC and GIQA
Artificial Intelligence Generated Content (AIGC) refers to the content, including texts, images, audios, or videos, \textit{etc.}, that is created or generated with the assistance of AI technology.
Many impressive AIGC models have been developed in recent years, such as ChatGPT %\cite{kulkarni11} 
and DALLE\cite{ramesh2022hierarchical}, which have been utilized in various application scenarios. 
As an important part of AIGC, AI Generated Images (AIGIs) have also gained significant attention in recent years due to advancement in generative models including Generative Adversarial Network (GAN) \cite{goodfellow2020generative}, Variational Autoencoder (VAE) \cite{kingma2013auto}, diffusion models \cite{Rombach2021HighResolutionIS}, \textit{etc.}, and language-image pre-training techniques including CLIP\cite{radford2021learning}, BLIP\cite{li2022blip}, \textit{etc.}

%current dataset
However, the development of AIGI models also raises new problems and challenges. 
One significant challenge is that not all generated images are qualified for real-world applications, which often require to be processed, adjusted, refined or filtered out before being applied to practical scenes.
However, unlike common image content, such as Natural Scene Images (NSIs)\cite{duan2017ivqad,duan2018perceptual}, screen content images\cite{min2017unified,duan2022confusing}, graphic images\cite{min2017unified,duan2022saliency}, \textit{etc.}, which generally encounters some common distortions including  noise, blur, compression, \emph{etc.} \cite{duan2023masked,duan2022develop}, AIGIs may suffer from some unique degradations such as unreal structures, unreasonable combinations, \textit{etc}. Moreover, the generated images may not correspond to the semantics of the text prompts \cite{lee2023aligning,kirstain2023pick,xu2023imagereward}.
Therefore, it is important to study the human visual preferences for AIGIs and design corresponding objective Image Quality Assessment (IQA) metrics for these images.

%current method
Many subjective IQA studies have been conducted for human captured or created images, and many objective IQA models have also been developed.
However, these models are designed for assessing low-level distortions, while AIGIs generally contain both low-level artifacts and high-level semantic degradations.
Some quantitative evaluation metrics such as Inception Score (IS)\cite{gulrajani2017improved} and Fréchet Inception Distance (FID)\cite{heusel2017gans} have been proposed to assess the performance of generative models and have been widely used to evaluate the authenticity of the generated images.
However, these methods cannot evaluate the authenticity of a single generated image, and cannot measure the correspondence between the generated images and the text-prompts.
As a new type of image content, previous IQA methods may fail to assess the image quality of AIGIs and cannot align well with human preferences due to the irregular distortions. 

%Our dataset and IQA method
To gain a better understanding of human visual preferences for AIGIs and guide the design process of corresponding objective IQA models, in this paper, we conduct a comprehensive subjective and objective IQA study for AIGIs.
We first establish a large-scale IQA database for AIGIs termed AIGCIQA2023, which contains 2,400 diverse images generated by 6 state-of-the-art AIGI models based on 100 various text prompts. 
Based on these images, a well-organized subjective experiment is conducted to assess the human visual preferences for each individual generated image from three perspectives including 
\textbf{\textit{quality}}, \textbf{\textit{authenticity}}, and \textbf{\textit{correspondence}}. Based on the constructed AIGCIQA2023 database, we evaluate the performance of several state-of-the-art IQA models and establish a new benchmark. Experimental results demonstrate that current IQA methods cannot well align with human visual preferences for AIGIs, and more efforts should be made in this research field in the future. The main contributions of this paper are summarized as follows:

\begin{itemize}


\item We propose to disentangle the human visual experience for AIGIs into three perspectives including \textbf{\textit{quality}}, \textbf{\textit{authenticity}}, and \textbf{\textit{correspondence}}.
\item Based on the above theory, we establish a novel large-scale database, \textit{i.e.,} AIGCIQA2023, to better understand the human visual preferences for AIGIs and guide the design of objective IQA models.
\item We conduct a benchmark experiment to
evaluate the performance of several current state-of-the-art IQA algorithms in measuring the quality, authenticity, and text-image correspondence of AIGIs.

\end{itemize}

The rest of the paper is organized as follows. In Section 2 we introduce the details of our constructed AIGCIQA2023 database, including the generation of AIGIs and the subjective quality assessment methodology and procedures.
In section 3 we present the benchmark experiment for current state-of-the-art IQA algorithms based on the established database. 
Section 4 concludes the whole paper and we discuss possible future research that can be conducted with the database.


\section{Database Construction and Analysis}
In order to get a better understanding of human visual preferences for AI-generated images based on text prompts, we construct a novel IQA database for AIGIs, termed AIGCIQA2023, which is a collection of generated images derived from six state-of-the-art deep generative models based on 100 text prompts, and corresponding subjective quality ratings from three different perspectives.
Then we further analyze the human visual preferences for AIGIs based on the constructed database.

\subsection{AIGI Collection}
\input{figures/circle}
We adopt six latest text-to-image generative models, including Glide\cite{Nichol2021GLIDETP}, Lafite\cite{Zhou_2022_CVPR}, DALLE\cite{ramesh2022hierarchical}, Stable-diffusion\cite{Rombach2021HighResolutionIS}, Unidiffuser\cite{Bao2023OneTF}, Controlnet\cite{Zhang2023AddingCC}, to produce AIGIs by using open source code and default weights.
To ensure content diversity and catch up with the practical application requirements, we collect diverse texts from the PartiPrompts website \cite{yu2022scaling} as the prompts for AIGI generation.
The text prompts can be simple, allowing generative models to produce imaginative results.
They can also be complex, which raises the challenge for generative models.
We select 10 scene categories from the prompt set, and each scene contains 10 challenge categories.
Overall, we collect 100 text prompts (10 scene categories $\times$ 10 challenge categories) from PartiPrompts\cite{yu2022scaling}.
The distribution of the selected scene and challenge categories is displayed in pie chart of Fig.1.
It can be observed that the dataset exhibits a high level of scene diversity, with images generated covering a broad range of challenges.
Then we perform the text-to-image generation based on these models and prompts. Specifically, for each prompt, we generate 4 various images randomly for each generative model. Therefore, the constructed AIGCIQA2023 database totally contains 2400 AIGIs (4 images $\times$ 6 models $\times$ 100 prompts) corresponding to 100 prompts.
\input{figures/imgall}

\subsection{Subjective Experiment Setup}

Subjective IQA is the most reliable way to evaluate the visual quality of digital images perceived by the users. 
It is generally used to construct image quality datasets and served as the ground truth to optimize or evaluate the performance of objective quality assessment metrics. 
Due to the unnatural property of AIGIs and different text prompts having different target image spaces, it is unreasonable to just use one score, \textit{i.e.,} ``quality'' to represent human visual preferences.
In this paper, we propose to measure the human visual preferences of AIGIs from three perspectives including \textbf{\textit{quality}}, \textbf{\textit{authenticity}}, and text-image \textbf{\textit{correspondence}}.
For an image, these three visual perception perspectives are related but different.
\input{figures/top10}

The first dimension of AIGI evaluation is ``quality'' evaluation, \textit{i.e.,} evaluating an AIGI from its clarity, color, lightness, contrast, \textit{etc.}, which is similar to the assessment of NSIs.
During the experiment procedure, subjects are instructed to evaluate whether the image outline is clear, whether the content can be distinguished, and the richness of details, \textit{etc.} 
Fig.3 (a) shows 10 high quality examples and 10 low quality examples of the images generated by the prompt of “a corgi”.

Considering the generation nature of AIGIs, an important problem of these images is that they may not look real compared to NSIs.
Therefore, we introduce a second dimension of evaluation metrics for the generated images, \textit{i.e.,} ``authenticity'' evaluation.
For this dimension, subjects are instructed to assess the image from the authenticity aspect, \textit{i.e.,} whether it looks real or whether they can distinguish that the image is AI-generated or not. 
Fig.3 (b) shows 10 high authenticity and 10 low authenticity examples of images generated by the prompt of ``a girl''.

Since an AIGI is generated from a text, it is also important to evaluate its correspondence with the original prompt, \textit{i.e.,} the third dimension, text-image ``correspondence''. 
For this purpose, subjects are instructed to consider textual information provided with the image and then give the correspondence score from 0 to 5 to assess the relevance between the generated image and its prompt. 
Fig.3 (c) shows 10 high text-image correspondence and 10 low correspondence examples of images generated by the prompt of “a grandmother reading a book to her grandson and granddaughter''.
\input{figures/UI}
\vspace{-2pt}
\subsection{Subjective Experiment Procedure}
To evaluate the quality of the images in the AIGCIQA2023 and obtain Mean Opinion Scores (MOSs), a subjective experiment is conducted following the guidelines of ITU-R BT.500-14 \cite{duan2022confusing}. 
The subjects are asked to rate their visual preference degree of exhibited AIGIs from the quality, authenticity and text-image correspondence.
The AIGIs are presented in a random order on an iMac monitor with a resolution of up to 4096 × 2304, using an interface designed with Python Tkinter, as shown in Fig.4. The interface allows viewers to browse the previous and next AIGIs and rate them using a quality scale that ranges from 0 to 5, with a minimum interval of 0.01. A total of 28 graduate students (14 males and 14 females) participate in the experiment, and they are seated at a distance of around 60 cm in a laboratory  environment with normal indoor lighting.





\subsection{Subjective Data Processing}

We follow the suggestions recommended by ITU to conduct the outlier detection and subject rejection. 
The score rejection rate is 2\%.
In order to obtain the MOS for an AIGI, we first convert the raw ratings into Z-scores, then linearly scale them to the range $[0,100]$ as follows:
$$z_i{}_j=\frac{r_i{}_j-\mu_i{}_j}{\sigma_i},\quad z_{ij}'=\frac{100(z_{ij}+3)}{6},$$
$$\mu_i=\frac{1}{N_i}\sum_{j=1}^{N_i}r_i{}_j, ~~ \sigma_i=\sqrt{\frac{1}{N_i-1}\sum_{j=1}^{N_i}{(r_i{}_j-\mu_i{}_j)^2}}$$ 
where $r_{ij}$ is the raw ratings given by the $i$-th subject to the $j$-th image. $N_i$ is the number of images judged by subject $i$. 

Next, the mean opinion score (MOS) of the image j is computed by averaging the rescaled z-scores as follows:
$$MOS_j=\frac{1}{M}\sum_{i=1}^{M}z_{ij}'$$
where $MOS_j$ indicates the MOS for the $j$-th AIGI, $M$ is the number of valid subjects, and $z'_i{}_j$ are the rescaled z-scores. 
\input{figures/sample}

\subsection{AIGI Analysis from Three Perspectives}
\vspace{-2pt}

To further illustrate the differences of the three perspectives, we demonstrate several example images and their corresponding subjective ratings from three aspects in Fig.5.
For each subfigure, it can be noticed that the right AIGI outperforms the left AIGI on two evaluation dimensions but is much worse than the left AIGI on another dimension, which demonstrates that each evaluation perspective (quality, authenticity, or text-image correspondence) has its own unique perspective and value.


\input{figures/score}
Fig.6 demonstrates the MOS and score distribution for quality evaluation, authenticity evaluation, and text-image correspondence evaluation, respectively, which demonstrate the images in AIGCIQA 2023 cover a wide range of perceptual quality. 
% Fig. 11-13 displays the subjective evaluation results with top 20 high score and top 20 low score from 3 different aspects.





\section{EXPERIMENT}
\subsection{Benchmark Models}
Since the AIGIs in the proposed AIGCIQA2023 database are generated based on text prompts and have no pristine reference images, they can only be evaluated by no-reference (NR) IQA metrics.
In this paper, we select fifteen state-of-the-art IQA models for comparison. The selected models can be classified into two groups:


\begin{itemize}
\item \textbf{Handcrafted-based} models, including: NIQE\cite{mittal2012making}, BMPRI\cite{min2018blind}, BPRI\cite{min2017blind}, BRISQUE\cite{mittal2012no}, HOSA\cite{xu2016blind}, BPRI-LSSn\cite{min2017blind}, BPRI-LSSs\cite{min2017blind}, BPRI-PSS\cite{min2017blind}, QAC\cite{xue2013learning}, HIGRADE-1 and HIGRADE-2\cite{kundu2017large}. 

These models extract handcrafted features based on prior knowledge about image quality. 
%\item \textbf{SVR-based} models: HIGRADE-1 and HIGRADE-2\cite{kundu2017large} . 

%These models combine hand crafted features from a Support Vector Regression(SVR) to represent perceptual quality.
\item \textbf{Deep learning-based} models, including: CNNIQA\cite{kang2014convolutional}, WaDIQaM-NR\cite{bosse2017deep}, VGG (VGG-16 and VGG-19)\cite{simonyan2014very} and ResNet (ResNet-18 and ResNet-34)\cite{he2016deep}.  

These models characterize quality-aware information by training deep neural networks from labeled data.
\end {itemize}
%\input{figures/algorithm}

\subsection{Evaluation Criteria}





In this study, we utilize the following four performance evaluation criteria to evaluate
the consistency between the predicted scores and the corresponding ground-truth MOSs, including Spearman Rank Correlation Coefficient (SRCC), Pearson Linear Correlation Coefficient (PLCC), Kendall’s Rank Correlation Coefficient (KRCC), and Root Mean Squared Error (RMSE).

% The SRCC metric measures the similarity between two sets of rankings, while the PLCC metric computes the linear correlation between two groups of rankings. SRCC and PLCC are used to measures the monotonicity and accuracy of prediction respectively. The KRCC metric, on the other hand, estimates the ordinal relationship between two measured quantities. The closer the result of SRCC, PLCC and KRCC is to 1, the better the prediction performance. The RMSE measures the distance between the ground-truth MOSs and the predicted MOSs. The closer the result of RMSE is to 0, the better the prediction performance.


\subsection{ Experimental Setup}
All the benchmark models are validated on the proposed AIGCIQA2023 database. 
For traditional handcrafted-based models, they are directly evaluated based on the database.
For deep trainable models, we first randomly split the database into an 4:1 ratio for training/testing while ensuring the image with the same prompt label falls into the same set. 
The partitioning and evaluation process is repeated several times for a fair comparison while considering the computational complexity, and the average result is reported as the final performance. 
%For SVR-based models, the repeating time is 1,000, implemented by LIBSVM  with radial basis function (RBF) kernel. 
For deep learning-based models, we applied CNNIQA\cite{kang2014convolutional}, WaDIQaM-NR\cite{bosse2017deep}, VGG (VGG-16 and VGG-19)\cite{simonyan2014very} and ResNet (ResNet-18 and ResNet-34)\cite{he2016deep} to predict the MOS of image quality. 
The repeating time is 10, the training epochs are 50 with an initial learning rate of 0.0001 and batch size of 4.


\subsection{Performance Discussion}
\input{figures/relation}
The performance results of the state-of-the-art IQA models mentioned above on the proposed AIGCIQA2023 database are exhibited in Table 1, from which we can make several conclusions:
\begin{itemize}
\item The handcrafted-based methods achieve poor performance on the whole database, which indicates the extracted handcrafted features are not effective for modeling the quality representation of AIGIs. This is because most employed handcrafted features of
these methods are based on the prior knowledge learned from NSIs, which are not effective for evaluating AIGIs.

\item The deep learning-based methods achieve relatively more competitive performance results on three evaluation perspectives. However, they are still far away from satisfactory.

\item Most of the IQA models achieve better performance on quality evaluation and worse on text-image correspondence score assessment.
The reason is that the text prompts for image generation are not utilized for the IQA model training. 
This makes it more challenging for the IQA models to extract relation features from AIGIs, which inevitably leads to performance drops.
\end {itemize}


\section{Conclusion and Future Work}
In this paper, we study the human visual preference problem for AIGIs. 
We first construct a new IQA database for AIGIs, termed AIGCIQA2023, which includes 2400 AIGIs generated based on 100 various text-prompts, and corresponding subjective MOSs evaluated from three perspectives (\textit{i.e., quality, authenticity, and text-image correspondence}).
Experimental analysis demonstrates that these three dimensions can reflect different aspects of human visual preferences on AIGIs, which further manifests that the evaluation of Quality of Experience (QoE) for AIGIs should be considered from multiple dimensions.
Based on the constructed database, we evaluate the performance of several state-of-the-art IQA models and establish a new benchmark to facilitate future research.

In future work, we will further explore the human visual perception for AIGIs and develop corresponding objective evaluation models for better assessing the quality of AIGIs from the three perspectives proposed in this paper.

%\input{figures/top20}




\newpage

\bibliographystyle{splncs04}
\bibliography{ref.bib}
\end{document}








\end{document}
