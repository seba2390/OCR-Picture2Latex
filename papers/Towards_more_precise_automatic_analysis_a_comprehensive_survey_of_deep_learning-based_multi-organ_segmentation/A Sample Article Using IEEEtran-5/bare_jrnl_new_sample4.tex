\documentclass[lettersize,journal]{IEEEtran}
\usepackage{amsmath,amsfonts}
\usepackage{algorithmic}
\usepackage{algorithm}
\usepackage{array}
\usepackage[caption=false,font=normalsize,labelfont=sf,textfont=sf]{subfig}
\usepackage{textcomp}
\usepackage{stfloats}
\usepackage{url}
\usepackage{verbatim}
\usepackage{graphicx}
\usepackage{cite}
\usepackage{booktabs}
\usepackage{multirow}
\hyphenation{op-tical net-works semi-conduc-tor IEEE-Xplore}
% updated with editorial comments 8/9/2021

\begin{document}

\title{Towards More Precise Automatic Analysis: A Comprehensive Survey of Deep Learning-based Multi-organ Segmentation}

\author{Xiaoyu Liu, Linhao Qu, Ziyue Xie, Jiayue Zhao, Yonghong Shi, and Zhijian Song
\thanks{X. Liu, L. Qu, Z. Xie, J. Zhao, Y. Shi and Z. Song are with the Digital Medical Research Center, School of Basic Medical Sciences, Fudan University, and also with the Shanghai Key Laboratory of Medical Imaging Computing and Computer Assisted Intervention, Shanghai, 200032, China. (e-mail: (liuxiaoyu21, zyxie22, jiayuezhao22) @m.fudan.edu.cn; (lhqu20, yonghong.shi, zjsong) @fudan.edu.cn);}
\thanks{X. Liu and L. Qu are co-first authors. Y. Shi and Z. Song are co-corresponding authors.}}

% \author{IEEE Publication Technology,~\IEEEmembership{Staff,~IEEE,}
%         % <-this % stops a space
% \thanks{This paper was produced by the IEEE Publication Technology Group. They are in Piscataway, NJ.}% <-this % stops a space
% \thanks{Manuscript received April 19, 2021; revised August 16, 2021.}}

% The paper headers
\markboth{Journal of \LaTeX\ Class Files,~Vol.~14, No.~8, August~2021}%
{Shell Xiaoyu Liu {\textit{et al.}}: Towards More Precise Automatic Analysis: A Comprehensive Survey of Deep Learning-based Multi-organ Segmentation}

% \IEEEpubid{0000--0000/00\$00.00~\copyright~2021 IEEE}
% Remember, if you use this you must call \IEEEpubidadjcol in the second
% column for its text to clear the IEEEpubid mark.

\maketitle

\begin{abstract}
Accurate segmentation of multiple organs of the head, neck, chest, and abdomen from medical images is an essential step in computer-aided diagnosis, surgical navigation, and radiation therapy. In the past few years, with a data-driven feature extraction approach and end-to-end training, automatic deep learning-based multi-organ segmentation method has far outperformed traditional methods and become a new research topic. This review systematically summarizes the latest research in this field. For the first time, from the perspective of full and imperfect annotation, we comprehensively compile 161 studies on deep learning-based multi-organ segmentation in multiple regions such as the head and neck, chest, and abdomen, containing a total of 214 related references. The method based on  full annotation summarizes the existing methods from four aspects: network architecture, network dimension, network dedicated modules, and network loss function. The method based on imperfect annotation summarizes the existing methods from two aspects: weak annotation-based methods and semi annotation-based methods. We also summarize frequently used datasets for multi-organ segmentation and discuss new challenges and new research trends in this field.
\end{abstract}

\begin{IEEEkeywords}
abdomen multi-organ, chest multi-organ, deep learning, head and neck multi-organ, multi-organ segmentation.
\end{IEEEkeywords}

\section{Introduction}
\IEEEPARstart{A}{ccurate} segmentation of multiple organs of the head and neck, chest, abdomen as well as other parts from medical images is crucial in computer-aided diagnosis, surgical navigation, and radiotherapy \cite{1,2}. For example, radiotherapy, which targets tumour masses and microscopic areas with high risk of tumour proliferation, is a common treatment option for cancer patients \cite{3}. However, radiotherapy can bring great risk to the normal organs around the tumour, which are known as organs at risk (OARs). Thus, accurate segmentation of tumour contours and OARs is necessary \cite{4,5}.

The early segmentation process relied heavily on manual labelling by physicians, which is a labour-intensive and time-consuming process. For example, a trained specialist may spend more than four hours manually labelling a case, which not only places a heavy burden on the healthcare system but also likely causes a delay in the radiotherapy for a patient. Moreover, different physicians or hospitals will have different results of labelling \cite{6,7,8,9}. Therefore, accurate automatic multi-organ segmentation method is urgently needed in clinical practice.

Multi-organ segmentation is a challenging task. First, the contour of the anatomical structure in image is highly variable, which is difficult to expressed by a unified mathematical rule. Second, the boundaries between different organs or tissue regions in an image are often blurred due to image noise and low intensity contrast, and these boundaries are difficult to identify using techniques of traditional digital image processing. Third, the use of different scanners, scanning protocols, and contrast agents will lead to different intensity distributions of organs in the obtained images, which poses a great challenge to the generalizability of the model. Finally, considering safety and ethical issues, many hospitals do not disclose their datasets. Many segmentation methods are trained and validated on private datasets, making it difficult to compare different methods. Therefore, designing accurate and robust multi-organ segmentation models is a very difficult and expensive task.

Traditional methods \cite{10,11,12,13} usually utilize manually extracted image features for image segmentation, such as the threshold \cite{14} method, graph cut \cite{15} method, and region growth \cite{16} method. Limited by a large number of manually extracted image features and the selection of non-robust thresholds or seeds, the segmentation results of these methods are usually unstable, and often yield only a rough segmentation result or only apply to specific organs. Knowledge-based methods can obtain anatomical information of different organs from labelled datasets, reduce the burden of manual feature extraction, and improve the robustness and accuracy of multi-organ segmentation, which commonly include multi-atlas label fusion \cite{17,18} and statistical shape models \cite{19,20}. The method based on multi-atlas label fusion-based uses image alignment to align predefined structural contours to the image to be segmented, and this method typically includes multiple steps. Therefore, the performance of this method may be influenced by various relevant factors involved in each step. The atlas-based method is still very popular, but due to the use of fixed atlases, it is difficult to handle the anatomical variation of organs between patients. In addition, it is computationally intensive and takes a long time to complete an alignment task. The statistical shape model uses the positional relationships between different organs, and uses the shape of the organs in the statistical space as a constraint to regularize the segmentation results. However, the accuracy of this approach is largely dependent on the reliability and extensibility of the shape model, and  the model based on normal anatomical structures has  very limited effect  in the segmentation of irregular structures.

Using data-driven feature extraction approach and end-to-end training, the methods based on deep learning (DL) have been widely studied in the fields of image classification \cite{21}, object detection \cite{22} and image segmentation \cite{23,24}, image fusion \cite{25}, image registration \cite{26}, etc. The segmentation method based on deep learning has become a mainstream method in the field of medical image processing. However, there are two main difficulties in multi-organ deep learning segmentation tasks. First, as shown in the head and neck in Fig. \ref{fig1}, the abdomen in Fig. \ref{fig2}, the chest in Fig. \ref{fig3}, and the statistics of the multi-organ size in each part in Fig. \ref{fig4}, there are very large differences between the organs sizes, and the serious imbalances of different organs sizes will lead to a poor segmentation performance of the trained segmentation network for small organs. Second, due to the imaging principle of CT technology and the complex anatomical structure of the human body, the contrast between organs and their surrounding tissues is often low, which leads to the inaccurate segmentation of organ boundaries by segmentation networks. Therefore, it has become a new hot research topic to develop deep multi-organ segmentation methods that can accurately segment small and large organs at the same time.
\begin{figure}[ht!]
    \centering
    \includegraphics[width=\columnwidth]{fig1}
    \caption{Schematic diagram of the organs of the head and neck, where the numbers are arranged in order: (1) brainstem, (2) left eye, (3) right eye, (4) left lens, (5) right lens, (6) left optic nerve, (7) right optic nerve, (8) Optical chiasm, (9) left temporal lobe, (10) right temporal lobe, (11) pituitary gland, (12) left parotid gland, (13) right parotid gland, (14) left temporal bone rock, (15) right temporal bone rock, (16) left temporal bone, (17) right temporal bone, (18) left mandibular condyle, (19) right mandibular condyle, (20) spinal cord, (21) left mandible, (22) right mandible.}
    \label{fig1}
\end{figure}
\begin{figure}[ht!]
    \centering
    \includegraphics[width=\columnwidth]{fig2}
    \caption{Schematic diagram of the thoracic organs, where the numbers are arranged in order: (1) left lung, (2) right lung, (3) heart, (4) esophagus, (5) trachea, and (6) spinal cord.}
    \label{fig2}
\end{figure}
\begin{figure}[ht!]
    \centering
    \includegraphics[width=\columnwidth]{fig3}
    \caption{Schematic diagram of the abdominal organs, where the numbers are arranged in order: (1) liver, (2) kidney, (3) spleen, (4) pancreas, (5) aorta, (6) inferior vena cava, (7) stomach, (8) gallbladder, (9) esophagus, (10) right adrenal gland, (11) left adrenal gland, and (12) celiac artery.}
    \label{fig3}
\end{figure}
\begin{figure}[ht!]
    \centering
    \includegraphics[width=\columnwidth]{fig4a}\\ 
    \includegraphics[width=\columnwidth]{fig4b}
    \caption{Illustration of the percentage of voxels in each organ of the head and neck (a), chest (b), and abdomen (c), respectively.}
    \label{fig4}
\end{figure}

Recently, a large number of deep learning-based multi-organ segmentation methods with significantly improved performance have been proposed \cite{27}. Fu {\it{et al.}} \cite{28} systematically reviewed the medical image multi-organ segmentation methods based on deep learning by 2020 according to the network architecture. However, with the rapid development of deep learning technology, more representative new techniques and methods have been proposed, such as transformer-based multi-organ segmentation methods and imperfect annotation-based methods. A more comprehensive review and summary of these techniques and methods are very important for the development of this field.

This paper reviews deep learning-based multi-organ segmentation method of the head, neck, chest and abdomen published from 2016 to 2022. On Google Scholar, a search using the keywords `Multi Organ Segmentation' and `Deep Learning' yielded an initial 287 articles, 73 articles were removed according to abstract and keywords, and 161 highly relevant studies containing a total of 214 relevant references were obtained. Fig. \ref{fig5} summarizes all current state-of-the-art deep learning-based multi-organ segmentation methods according to full annotation and imperfect annotation architectures. In full annotation-based methods, we summarize the existing methods in four aspects: network architecture, network dimension, network dedicated modules, and network loss function. In imperfect annotation-based methods, we summarize the existing methods in two aspects, weak annotation and semi annotation, to investigate their innovation, contribution, and challenges.
\begin{figure}[ht!]
    \centering
    \includegraphics[width=\columnwidth]{fig5}
    \caption{Framework diagram of the overview.}
    \label{fig5}
\end{figure}

This article is organized as follows. Section \ref{sec2} expounds the mathematical definition of multi-organ segmentation and the corresponding evaluation metrics. Section \ref{sec3} summarizes the multi-organ segmentation datasets. Section \ref{sec4} describes the literature based on full annotation-based methods, involving four parts: network architecture (section \ref{sec4}.A), network dimension (section \ref{sec4}.B), network dedicated modules (section \ref{sec4}.C), and network loss function (section \ref{sec4}.D). Section V analyzes the articles based on imperfect annotation methods, including two parts: weak annotation-based methods (section \ref{sec5}.A) and semi annotation-based methods (section \ref{sec5}.B). We discuss the existing methods and their future outlooks in section \ref{sec6}, and conclude the whole paper in section \ref{sec7}.

\section{Definition and Evaluation Metrics}
\label{sec2}
Let $\boldsymbol{X}$ represent the union of multiple organ regions in the input images, $\boldsymbol{G}$ represent the union of ground truth labels of multiple organs in the input images, $\boldsymbol{P}$ represent the union of predicted labels of multiple organs in the output images, $\boldsymbol{x_i^c \in X, g_i^c \in G, p_i^c \in P, i=1, \cdots N}$, and $\boldsymbol{c=1, \cdots C}$, where $\boldsymbol{N}$ represents the number of pixel in the image, $\boldsymbol{C}$ represents the number of categories to which the pixels belong, $\boldsymbol{f}$ represents the neural network, and $\boldsymbol{\theta}$ represents the parameters of the neural network optimization, where $\boldsymbol{P=f(X; \theta)}$.

The loss function represents the gap between the predicted and true values. In the multi-organ segmentation task, common loss functions include the cross-entropy loss and Dice loss. Section \ref{sec4_4} provides specific details about the loss function.

Given a multi-organ segmentation task, $\left\{\boldsymbol{\Psi}\right\}$ represents the represents the class set of organs to be segmented. $\left\{\boldsymbol{x}\right\}_\ast$ represents the set of organs annotated in $\boldsymbol{x}$. According to the available annotations, multi-organ segmentation can be implemented according to three learning paradigms: full annotation-based learning, weak annotation-based learning, and semi annotation-based learning. The last two are called imperfect annotation-based methods, as shown in Fig. \ref{fig6}. Full annotation-based learning means that the labels of all organ are given, which indicates that $\forall \boldsymbol{x} \in \boldsymbol{X},\{\boldsymbol{x}\}_*=\{\boldsymbol{\Psi}\}$ . Weak annotation often means that the data come from $\boldsymbol{n}$ different datasets. However, each dataset provides the annotations of one or more organs but not all organs, which means that $\boldsymbol{X=X_1 \cup X_2 \cup \cdots \cup X_n, \forall x_{k, i} \in X_k, k=1,2, \ldots n}$, $\boldsymbol{\left\{x_{k, i}\right\}_*<\{\Psi\}, \bigcup_{k=1}^n\left\{x_{k, i}\right\}_*=\{\Psi\}}$. Here, $\boldsymbol{x_{k, i}}$ \textbf{denotes the} $\boldsymbol{i}$\textbf{-th image in} $\boldsymbol{X_k}$. Semi annotation-based methods indicate that some of the training datasets are fully labelled and others are unlabelled, $\boldsymbol{X}=\boldsymbol{X}_{\boldsymbol{l}} \cup \boldsymbol{X_u} \cdot \boldsymbol{X}_{\boldsymbol{l}}$. $\boldsymbol{X_l}$ represents the fully labelled dataset, $\boldsymbol{X_u}$ represents the unlabelled dataset, which indicates that $\forall \boldsymbol{x_l \in X}_{\boldsymbol{l}},\left\{\boldsymbol{x}_{\boldsymbol{l}}\right\}_*=\{\boldsymbol{\Psi}\}$ and $\forall \boldsymbol{x}_{\boldsymbol{u}} \in \boldsymbol{X}_{\boldsymbol{u}},\left\{\boldsymbol{x}_{\boldsymbol{u}}\right\}_*=\boldsymbol{\phi}$, and the size of $\boldsymbol{X_l}$ is far less than the one of $\boldsymbol{X_u}$.
\begin{figure}[ht!]
    \centering
    \includegraphics[width=0.8\columnwidth]{fig6}
    \caption{General overview of the learning paradigms reviewed in this paper.}
    \label{fig6}
\end{figure}

Usually using the \textit{\textbf{Dice}} similarity coefficient (\textit{\textbf{DSC}}), 95\% \textit{\textbf{Hausdorff}} distance (\textit{\textbf{HD95}}) and mean surface distance (\textit{\textbf{MSD}}) to evaluate the performance of the segmentation methods. \textit{\textbf{DSC}} is a measure of the volume overlap between the predicted labels and ground truth labels, \textit{\textbf{HD95}} and \textit{\textbf{MSD}} are measures of the surface distance between the predicted labels and ground truth labels.
\begin{equation}\label{eq1}
\boldsymbol{D S C=\dfrac{2 \times\left|P^c \cap G^c\right|}{\left|P^c\right|+\left|G^c\right|}}
\end{equation}
\begin{equation}\label{eq2}
\boldsymbol{H D 95=\max _{95 \%}\left[d\left(P_s^c, G_s^c\right), d\left(G_s^c, P_s^c\right)\right]}
\end{equation}
\begin{equation}\label{eq3}
M S D=\frac{1}{\left|P_s^c\right|+\left|G_s^c\right|}\left(\sum_{p_s^c \in P_s^c} d\left(p_s^c, G_s^c\right)+\sum_{g_s^c \in G_s^c} d\left(g_s^c, P_s^c\right)\right)
\end{equation}
where $\boldsymbol{P}^{\boldsymbol{c}}$ and $\boldsymbol{G}^{\boldsymbol{c}}$ represent the set of predicted pixels and the set of real pixels of the $\boldsymbol{c}$ class organ, respectively; $\boldsymbol{P_s^c}$ and $\boldsymbol{G_s^c}$ represent the set of predicted pixels and the set of real pixels of the surface of the $\boldsymbol{c}$ class organ, respectively; and $\boldsymbol{d\left(p_s^c, G_s^c\right)=\min _{g_s^c \in G_s^c}^c\left\|p_s^c-g_s^c\right\|_2}$ represents the minimal distance from point $\boldsymbol{p_s^c}$ to surface $\boldsymbol{G_s^c}$. The review reports various methods based on \textit{\textbf{DSC}} values.

\section{Multi-organ Segmentation Datasets}
\label{sec3}
To obtain high-quality organ segmentation datasets, many research teams have undertaken several collaborations with medical organizations. Table \ref{tab1} summarizes the common head and neck, thorax, and abdomen datasets used for the development and validation of multi-organ segmentation method. Table I also shows that the quantity of annotated data available for deep learning studies is still very low.
\begin{table*}[ht!]
\centering 
\caption{Frequently Used Dataset for Multi-organ Segmentation}
\label{tab1}
\begin{tabular}{@{}lm{0.11\textwidth}llm{0.2\textwidth}m{0.1\textwidth}m{0.1\textwidth}l@{}}
\toprule
Year                  & Dataset                                                                                                                                          & Modality            & Part                     & Number of organs (specific organs)                                                                                                                                                                                                                                                                                                                                                      & Quantity                                                                 & Labelling status                                              & Image size                                     \\ \midrule
2015                  & MICCAI Multi-Atlas Labelling Beyond the Cranial Vault (BTCV) \cite{29}                                                                            & CT                  & Abdomen                  & 13 (spleen, right kidney, left kidney, gallbladder, esophagus, liver, stomach, aorta, inferior vena cava, portal and splenic veins, pancreas, right adrenal gland, left adrenal gland)                                                                                                                                                                                                  & 50 (30 training and 20 testing)                                          & The training set are labelled, the test set are not labelled  & 512 × 512 × {[}85$\sim$198{]}                  \\
2015                  & MICCAI head and neck Auto Segmentation Challenge (HNC) \cite{30}                                                                                  & CT                  & Head and neck            & 9 (brainstem, mandible, chiasm, left optic nerves, right optic nerves, left parotid glands, right parotid glands, left submandibular glands, right submandibular glands)                                                                                                                                                                                                                & 35 (25 training, 10 off-site tests, 5 on-site tests)                     & Labelled                                                      & 512 × 512 × {[}110$\sim$190{]}                 \\
2015                  & \begin{tabular}[c]{@{}l@{}}Synapse multi-\\ organ segmentation\\ dataset (Synapse)\end{tabular}                                                    & CT                  & Abdomen                  & 13 (spleen, right kidney, left kidney, gallbladder, esophagus, liver, stomach, aorta, inferior vena cava, portal vein and splenic vein, pancreas, right adrenal gland, left adrenal gland)                                                                                                                                                                                              & 50 (30 training, 20 testing)                                             & Labelled                                                      & 512 × 512 × {[}85$\sim$198{]}                  \\
2015                  & Public Domain Database for Computational Anatomy (PDDCA) \cite{30}                                                                                & CT                  & Head and neck            & 9 (brainstem, mandible, chiasm, left optic nerves, right optic nerves, left parotid glands, right parotid glands, left submandibular glands, right submandibular glands)                                                                                                                                                                                                                & 48 (25 training, 8 additional training, 10 off-site and 5 on-site tests) & Labelled                                                      & 512 × 512 × {[}110$\sim$190{]}                 \\
2017                  & Thoracic Auto-segmentation Challenge (AAPM) \cite{31}                                                                                             & CT                  & Thorax                   & 5 (left lung, right lung, heart, Esophagus, spinal cord)                                                                                                                                                                                                                                                                                                                                & 60 (36 training, 12 off-site tests, 12 on-site tests)                    & Labelled                                                      & 512 × 512 × {[}103$\sim$279{]}                 \\
\multirow{2}{*}{2019} & \multirow{2}{*}{\begin{tabular}[c]{@{}l@{}}Combined (CT-MR)\\ Healthy Abdominal\\ Organ Segmentation\\ (CHAOS) \cite{32}\end{tabular}}                                                          & CT                  & \multirow{2}{*}{Abdomen} & \multirow{2}{*}{\begin{tabular}[c]{@{}l@{}}4 (left kidney, right kidney, liver,\\ spleen)\end{tabular}}                                                                                                                                                                                                                                                                                                                           & 40 (20 training and 20 testing)                                          & Labelled training set and unlabeled test set                  & 512 × 512 × {[}78$\sim$294{]}                  \\
                      &                                                                                                                                                  & MR                  &                          &                                                                                                                                                                                                                                                                                                                                                                                         & 40 (20 training, 20 testing) × 3 sequences                               & Labelled training set and unlabeled test set                  & 256 × 256 × {[}26$\sim$50{]}                   \\
2019                  & SegTHOR Challenge: Segmentation of Thoracic Organs at Risk in CT Images (SegTHOR) \cite{33}                                                      & CT                  & Thorax                   & 5 (left and right lungs, heart, esophagus, spinal cord)                                                                                                                                                                                                                                                                                                                                 & 60 (36 training, 12 off-site tests, 12 on-site tests)                    & Labelled                                                      & 512 × 512 × N                                  \\
2019                  & Annotations for Body Organ Localization based on MICCAI LITS Dataset \cite{34}                                                                    & CT                  & Thorax                   & 11 (heart, left lung, right lung, liver, spleen, pancreas, left kidney, right kidney, bladder, left femoral head, right femoral head)                                                                                                                                                                                                                                                   & 201 (131 training and 70 testing)                                        & Bounding boxes labelled                                       & 512 × 512 × N                                  \\
\multirow{2}{*}{2019} & \multirow{2}{*}{\begin{tabular}[c]{@{}l@{}}Automatic Structure\\ Segmentation for\\ Radiotherapy Planning\\ Challenge 2019\\ (StructSeg)\end{tabular}} & \multirow{2}{*}{CT} & Head and neck            & 22 (left eye, right eye, left lens, right lens, left optical nerve, right optical nerve, chiasm, pituitary, brainstem, left temporal lobes, right temporal lobes, spinal cord, left parotid gland, right parotid gland, left inner ear, right inner ear, left middle ear, right middle ear, left temporomandibular joint, right temporomandibular joint, left mandible, right mandible) & 60 (50 training, 10 testing)                                             & \multirow{2}{*}{\begin{tabular}[c]{@{}l@{}}Labelled\\ training\\ set and\\ unlabeled\\ test set\end{tabular}} & \multirow{2}{*}{512 × 512 × {[}98$\sim$140{]}} \\
                      &                                                                                                                                                  &                     & Thorax                   & 6 (left lung, right lung, spinal cord, esophagus, heart, trachea)                                                                                                                                                                                                                                                                                                                       & 60 (50 training, 10 testing)                                             &                                                               &                                                \\
2020                  & OpenKBP: The open-access knowledge-based planning grand challenge and dataset \cite{35}                                                           & CT                  & Head and neck            & 7 (brainstem, spinal cord, right parotid, left parotid, larynx, esophagus, mandible)                                                                                                                                                                                                                                                                                                    & 340 (200 training, 40 validating, 100 testing)                           & Labelled                                                      & 128×128×128                                    \\
2021                  & Abdomenct-1k \cite{36}                                                                                                                            & CT                  & Abdomen                  & 5 (liver, right and left kidneys, spleen, pancreas)                                                                                                                                                                                                                                                                                                                                     & 1112                                                                     & Labelled                                                      & 512 × 512 × N                                  \\ \bottomrule
\end{tabular}
\end{table*}

\section{Full Annotation-based Methods}
\label{sec4}
The method based on full annotation means that all organs of the multi-organ segmentation task are fully annotated. The existing methods can be analysed from four parts: network architecture, network dimension, network dedicated modules, and network loss function. Among these methods, the network architecture part summarizes the common neural network architectures and the combination or cascade of different architectures. In the network dimension part, the existing methods are classified into 2D, 3D, and multi-view methods according to the image dimension used. The part of network dedicated modules describes modules that are commonly used in multi-organ segmentation to improve the segmentation performance, The part of network loss function summarizes how common loss functions are innovated around multi-organ segmentation.

\subsection{Network Architecture}
\label{sec4_1}
Based on the design of the network architecture, multi-organ segmentation methods can be classified according to single-stage and multistage implementations. Single-stage methods include those based on CNN (Convolutional Neural Network), GAN (Generative Adversarial Network), transformer or hybrid networks. Multistage approaches include coarse-to-fine methods, localization and segmentation methods, or other cascade approaches. Tables \ref{tab_S_1}- \ref{tab_S_3} summarize the literature related to single-stage methods for the segmentation of multi-organ in the head and neck, abdomen and chest based on DSC metrics. Since there are too many organs in the head and neck as well as abdomen, this paper mainly reports 9 organs in the head and neck and 7 organs in the abdomen. Tables  \ref{tab_S_10}- \ref{tab_S_11} in the supplementary materials summarize the DSC values of other organs.

\subsubsection{CNN-Based Methods}
Convolutional Neural Network (CNN) is a feedforward neural network which can automatically extract deep features of the image. Multiple neurons are connected to each neuron in next layer, where each layer can perform complex tasks such as convolution, pooling, or loss computation \cite{92}. CNNs have been successfully applied to medical images, such as brain \cite{93,94} and pancreas \cite{55} segmentation tasks.

\paragraph{Early CNN-Based Methods}
Earlier CNN-based methods mainly used convolutional layers to extract features and then went through pooling layers and fully connected layers to obtain the final prediction results. Ibragimov and Xing \cite{37} used deep learning methods to segment OARs in head and neck CT images for the first time, training 13 CNNs for 13 OARs, and showed that the CNNs outperformed or were comparable to advanced algorithms in segmentation accuracy for organs such as the spinal cord, mandible, larynx, pharynx, eye, and optic nerve, but performed poorly in the segmentation of organs such as the parotid gland, submandibular gland, and optical chiasm. Fritscher {\it{et al.}} \cite{38} combined the shape location as well as the intensity with CNN for segmentation of the parotid gland, submandibular gland and optic nerve. Moeskops {\it{et al.}} \cite{95} investigated whether a single CNN can be used to segment six tissues in brain MR images, pectoral muscles in breast MR images, and coronary arteries in heart CTA images. The results showed that a single CNN can segment multiple organs not only on a single modality but also on multiple modalities.

\paragraph{FCN-Based Methods}
Early CNN-based methods made some improvements in segmentation accuracy compared to traditional methods. However, CNN involves multiple identical computations of overlapping voxels during the convolution operation, which may cause some performance loss. Moreover, the spatial information of the image is lost when the convolutional features are input into the final fully connected network layer. Thus, Shelhamer {\it{et al.}} \cite{96} proposed the Fully Convolutional Network (FCN), which enables end-to-end segmentation by using transposed convolutional layers that allow the size of the predicted image to match the size of the input image. Wang {\it{et al.}} \cite{97} used FCN combined with a new sample selection strategy to segment 16 organs in the abdomen, and Trullo {\it{et al.}} [83] used a variant of FCN, SharpMask \cite{98}, to segment the esophagus, heart, trachea, and aorta in the thorax, which showed the segmentation results of all four organs were improved compared with the normal FCN.

\paragraph{U-Net-Based Methods}
Based on FCN, Ronneberger {\it{et al.}} \cite{99} proposed a classical U-Net architecture, which is consisted of an encoder for the down sampling layer and a decoder for the up-sampling layer, and connects them layer by layer with skip connections, so that the features extracted from the down sampling layer can be directly transmitted to the up-sampling layer to fuse multiscale features for segmentation. U-Net has become one of the most commonly used architectures in the field of multi-organ segmentation \cite{39,40,42,48,61,69,78}. Roth {\it{et al.}} \cite{61} applied the U-Net architecture to segment the abdominal aorta, portal vein, liver, spleen, stomach, gallbladder, and pancreas. The advanced segmentation performance of multiple organs was achieved with an average Dice value of 0.893 for seven organs. Lambert {\it{et al.}} \cite{86} proposed a simplified U-Net for segmenting the heart, trachea, aorta, and esophagus of the chest. The results showed that adding dropout and using bilinear interpolation can significantly improve the segmentation performance of the heart, aorta, and esophagus compared with the ordinary U-Net. In addition to U-Net, V-Net \cite{100} proposes 3D image segmentation method based on volumetric, fully convolutional neural network. This method can directly process 3D medical data by introducing residual connections and using convolutional layers instead of pooling layers in the original U-Net. Gibson {\it{et al.}} \cite{60} used dense V-Networks to segment the pancreas, esophagus, stomach, liver, spleen, gallbladder, left kidney, and duodenum of the abdomen. Xu {\it{et al.}} \cite{77} proposed a new probabilistic V-Net model which combines a conditional variational autoencoder (cVAE) and hierarchical spatial feature transform (HSPT) for abdominal multi-organ segmentation. nnU-Net [101] is a novel framework based on U-Net architecture with the addition of adaptive pre-processing, data enhancement, and postprocessing techniques, and has shown state-of-the-art results on many publicly available datasets for different biomedical segmentation challenges \cite{50,102,103,104}. Podobnik {\it{et al.}} \cite{50} reported the results of segmentation of 31 OARs of the head and neck using the nnU-Net architecture combined with CT and MR images.

\subsubsection{GAN-Based Methods}
A typical Generative Adversarial Network (GAN) \cite{105} includes a pair of competitive networks, which are generators and discriminators. The generator attempts to deceive the discriminator by generating the artificial data, and the discriminator strives to discriminate the artificial data without being deceived by the generator; after alternate optimization training, the performance of both networks can eventually be improved. In recent years, many GAN-based multi-organ segmentation methods have been proposed and achieved high segmentation accuracy \cite{41,62,84,106,107,108,109}.

Dong {\it{et al.}} \cite{84} jointly trained GAN with a set of U-Nets as a generator and a set of FCNs as a discriminator for segmenting the left lung, right lung, spinal cord, esophagus and heart from chest CT images. The results showed that the segmentation performance of most of the organs were improved with the help of adversarial networks, and the average DSC values of the above five OARs were finally obtained as 0.970, 0.970, 0.900, 0.750 and 0.870. Tong {\it{et al.}} \cite{41} proposed a Shape-constraint GAN for automatic head and neck OARs segmentation (SC-GAN) from CT and low-field MRI images. It uses DenseNet, a deep supervised fully convolutional network to segment organs for prediction, and uses a CNN as discriminator network to correct the error of prediction. The results show that the combination of GAN and DenseNet can further improve the segmentation performance of CNN based on the original shape constraints.

GAN can improve accuracy with its adversarial losses. However, the training of GAN network is difficult and time-consuming since the generator needs to achieve Nash equilibrium with the discriminator. And its adversarial loss as a shape modifier can only achieve higher segmentation accuracy when segmenting organs with regular and unique shapes (such as liver and heart), but may not work well for irregular or tubular structures (such as pancreas and aorta).

\subsubsection{Transformer-Based Methods}
CNN-based methods can perform well for segmenting multiple organs in many tasks, but the inherent shortcomings of the perceptual field of the convolutional layers lead to the inability of CNNs to model global relationships, hindering the performance of the models. The self-attentive mechanism of the transformer \cite{110} can solve the long-term dependency problem well, achieving better results than CNNs in many tasks such as natural language processing (NLP) or computer vision \cite{111}. The performance of the medical image segmentation networks using transformer is also close or even better than the one of current state-of-the-art methods \cite{112,113,114,115}.

Cao {\it{et al.}} \cite{71} integrated the transformer with a U-shaped architecture to explore the potential of the pure transformer model for abdominal multi-organ segmentation. The results showed that the method has good segmentation accuracy. However, the method needs to initialize the network encoder and decoder using the training weights of the Swin transformer on ImageNet. Huang {\it{et al.}} \cite{75} introduced an efficient and powerful medical image segmentation architecture, MISSFormer, where the proposed enhanced mixed block can effectively overcome the feature recognition limitation problem caused by convolution. Moreover, compared with Swin-UNet, this model does not require pre-training on large-scale datasets to achieve comparable segmentation results.

Transformer-based approaches can capture long-range dependencies and achieve better performance than CNNs in many tasks. However, multi-organ segmentation problem involves the segmentation of many tiny organs, and the pure transformer network focuses on the global context modelling. This leads to the lack of detailed localization information of low-resolution features. Thus, a coarser segmentation result is usually obtained.

\subsubsection{Hybrid Networks}
CNN convolution operation can extract local features well, but it is difficult to obtain global features. The self-attentive mechanism of the transformer can effectively capture feature dependencies over long distances, but it loses local feature details, which may obtain poor results for the segmentation accuracy of small organs. Therefore, some researchers have combined the CNN and transformer to overcome the limitations of both architectures \cite{82,113,116,117,118,119}.

Suo {\it{et al.}} \cite{76} proposed an intra-scale and inter-scale collaborative learning network (I2-Net) by combining features extracted by the CNN and transformer to segment multiple organs of the abdomen, which improved the segmentation performance of small and medium-sized organs by 4.19\% and 1.83\%-3.8\%, respectively. Kan {\it{et al.}} \cite{51} proposed ITUnet, which adds the features extracted by the transformer to the output of each block of the CNN-based encoder, which can obtain segmentation results provided by both the local and global information of the image. ITUnet has better accuracy and robustness than other methods, especially on difficult organs such as the lens. Chen {\it{et al.}} \cite{72} proposed a new network architecture, TransUNet, which uses a transformer to further encode CNN encoders to build stronger encoders and report competitive results for multi-organ segmentation of the head and neck. Hatamizadeh {\it{et al.}} \cite{66} proposed a new architecture U-net transformer (UNETR) using a transformer as an encoder and the CNN as a decoder, which achieves better segmentation accuracy by capturing global and local dependencies.

In addition to the methods combining CNN and transformer, there are some other hybrid frames. For example, Chen {\it{et al.}} \cite{120} combined U-Net and long short-term memory (LSTM) to realize the segmentation of five organs in the chest, and the DSC values of all five organs were above 0.8. Chakravarty {\it{et al.}} \cite{121} proposed a hybrid architecture combining CNN and RNN to segment the optic disc, nucleus, and left atrium. The hybrid architecture-based approach can combine and utilize the advantages of the two architectures for the accurate segmentation of small and medium-sized organs, which is a key research direction for the future.

\subsubsection{Cascade Networks}
Due to most organs occupy only a small volume in images, the segmentation models are easy to segment large organs and ignore small organs, which prompted researchers to propose cascade multistage methods. Multistage methods can be divided into two main categories, depending on the information provided by the primary network to the secondary network. The first category is called coarse-to-fine multi-organ segmentation method, where the first network performs coarse segmentation, and its results are passed to another network to achieve fine segmentation. The second category is called multi-organ segmentation method based on localization and segmentation, where candidate boxes for the location of each organ are identified by registration methods or localization networks, and then input into the second-level network for fine segmentation. In addition, the first network can provide other information, such as the shape location or proportion, to better guide the segmentation of the second network. Tables \ref{tab_S_4}-\ref{tab_S_9} summarize the relevant literature of the cascade methods for head and neck, chest and abdomen based on DSC metrics, and tables \ref{tab_S_7}-\ref{tab_S_8} in the supplementary materials summarize the DSC metrics of other organs.

\paragraph{Coarse-to-Fine-Based Methods}
The coarse-to-fine-based methods first inputs the original image and its corresponding labels into the first network. After training, the first-level network obtains the coarse segmentation probability map, which will be multiplied by the original image, and the results will be input into the second network to refine the rough segmentation. This process is shown in Fig. \ref{fig7}. In recent years, a number of coarse-to-fine methods have been proposed for multi-organ segmentation \cite{123,124,125,126,129,130,133,152,153,154}, and the references are shown in Tables \ref{tab_S_5}-\ref{tab_S_7}.
\begin{figure}[ht!]
    \centering
    \includegraphics[width=\columnwidth]{fig7}
    \caption{Coarse-to-fine-based segmentation method.}
    \label{fig7}
\end{figure}

Trullo {\it{et al.}} \cite{83} proposed two synergistic depth architectures to jointly segment all organs, including the esophagus, heart, aorta, and trachea. Probabilistic maps obtained in the first stage were passed to the second stage to learn anatomical constraints, and then four networks were trained for four structures in the second stage to distinguish the background from each target organ in separate refinements. Zhang {\it{et al.}} \cite{129} proposed a new cascaded network model with Block Level Skip Connections (BLSC) between two cascaded networks. This architecture enabled the second-stage network to capture the features learned by each block in the first-stage network and accelerated the convergence of the second-stage network. Xie {\it{et al.}} \cite{130} proposed a new framework called the Recurrent Saliency Transformation Network (RSTN). This framework enabled coarse scale segmentation masks to be passed to the fine stage as spatial weights, while gradients can be backpropagated from the loss layer to the whole network, so as to realize the joint optimization of the two stages, thus improving the segmentation accuracy of small targets. Ma {\it{et al.}} \cite{125} proposed a new end-to-end coarse-to-fine segmentation model to automatically segment multiple OARs in head and neck CT images. This model used a predetermined threshold to classify the initial results of the coarse stage into large and small OARs, and then designed different modules to refine the segmentation results.

This coarse-to-fine approach effectively reduces the complexity of the background and enhances the discriminative information of the target structures. Compared with the single-stage approach, this coarse-to-fine-based method improves the segmentation results for small organs, but there are limitations in memory and training time because at least two networks need to be trained.

\paragraph{Localization and Segmentation Based Methods}
The localization and segmentation methods are also multistage cascade methods. Here, the first-level network provides location information, returns a candidate frame, and crops the region of interest of the image according to the location information, and uses it as the input of the second network. In this way, when the second network performs segmentation, one organ can be targeted, excluding the interference of other organs or background noise and improving the segmentation accuracy. The process is shown in Fig. \ref{fig8}. The organ location in the first stage can be obtained through registration or localization network. The relevant literature of multi-stage method based on location and segmentation are listed in Tables \ref{tab_S_7}-\ref{tab_S_9}, and the DSC values of other organs are listed in tables \ref{tab_S_14}-\ref{tab_S_15}.
\begin{figure}[ht!]
    \centering
    \includegraphics[width=\columnwidth]{fig8}
    \caption{Localization and segmentation based method.}
    \label{fig8}
\end{figure}

Wang {\it{et al.}} \cite{137}, Men {\it{et al.}} \cite{138}, Lei {\it{et al.}} \cite{144}, Francis {\it{et al.}} \cite{151}, Tang {\it{et al.}} \cite{139} proposed decomposing OARs segmentation into two stages of localization and segmentation. The first stage localizes the target OARs using the bounding box, the second stage segments the target OARs within the bounding box, and both stages use neural networks. Among them, Wang {\it{et al.}} \cite{137} and Francis {\it{et al.}} \cite{151} used a 3D U-net in both stages. Lei {\it{et al.}} \cite{144} used Faster RCNN to automatically locate the ROI of organs in the first stage. Korte {\it{et al.}} \cite{146} demonstrated that the CNN is a suitable method for automatically segmenting parotid and submandibular glands in MRI images of HNC patients. The segmentation accuracy of the parotid and submandibular glands can be improved by cascading localizing CNNs, cropping and segmenting high-resolution CNNs. FocusNet \cite{108,142} presented a novel deep neural network to solve the class imbalance problem in the segmentation of head and neck OARs. The small organs are first localized by the organ localization network. Then, combined with the high-resolution information of each small organ, multiscale features are input to the segmentation network together to accurately segment the small organs.

The organ localization by Larsson {\it{et al.}} \cite{148}, Zhao {\it{et al.}} \cite{149}, Ren {\it{et al.}} \cite{122} and Huang {\it{et al.}} \cite{145} was obtained with registration method followed by the application of convolutional neural networks for segmentation. Among them, Ren {\it{et al.}} \cite{122} designed interleaved cascades of 3D-CNNs to segment each structure of interest. Since adjacent tissues are usually highly correlated from a physiological and anatomical perspective, using the initial segmentation results of a specific tissue can help refine the segmentation of other neighbouring tissues. Zhao {\it{et al.}} \cite{149} proposed a new flexible knowledge-assisted convolutional neural network which combine deep learning and traditional methods to improve the segmentation accuracy in the second stage.

The vast majority of approaches require to determine the target areas prior to segmentation network training by different localization methods. For example, Ren {\it{et al.}} \cite{122} localized organ regions through a multi-atlas-based method. Wang {\it{et al.}} \cite{137} used separate CNNs to localize candidate areas. That is, their target organ region localization is constructed independently of organ segmentation, which will hinder the transmission of information between these two related learning tasks. On this basis, Liang {\it{et al.}} \cite{141} proposed a multi-organ segmentation framework based on multi view spatial aggregation, which combines the learning of the organ localization subnetwork and the segmentation subnetwork to reduce the influence of background regions and neighbouring similar structures in the input data. Additionally, the proposed fine-grained representation based on ROIs can improve the segmentation accuracy of organs with different sizes, especially the segmentation results of small organs.

The type of multistage method improves the organ segmentation accuracy, especially for small organs, which largely reduces the interference of the background. However, this two-step process has certain requirements for memory and training time, and the segmentation accuracy also depends largely on the regional localization accuracy. Better localization of organs and improvement of segmentation accuracy are still directions to be investigated in the future.

\paragraph{	Other Cascade Methods}
In addition to probability maps and localization information, the first network can also provide other types of information, such as scale information and shape priors. For example, Tong {\it{et al.}} \cite{155} combines the FCNN and a shape representation model (SRM) for head and neck OARs segmentation. The first-level network is the SRM for learning highly representative shape features in head and neck organs. The direct comparison of the FCNN with and without SRM shows that the SRM significantly improves the segmentation accuracy of nine organs with different sizes, morphological complexity, and different CT contrasts. Roth {\it{et al.}} \cite{127} proposed a multiscale 3D FCN approach which is accomplished by two cascaded FCNs, where low-resolution 3D FCN predictions are upsampled, cropped, and connected to higher-resolution 3D FCN inputs. In this case, the primary network provides scale information to the secondary network. And the method uses the scale space pyramid with automatic context to perform high-resolution semantic image segmentation, while considering large contextual information from the lower resolution levels.

\subsection{Network Dimension}
\label{sec4_2}
Considering the dimensionality of input images and convolutional kernels, multi-organ segmentation neural networks can be classified into 2D, 2.5D and 3D architectures, as shown in Fig. \ref{fig9}, and the differences between the three architectures will be discussed in follows.
\begin{figure}[ht!]
    \centering
    \includegraphics[width=\columnwidth]{fig9}
    \caption{Different network dimensions.}
    \label{fig9}
\end{figure}

\subsubsection{2D- \& 3D-Based Methods}
The input of the 2D multi-organ segmentation neural network is slices from a three-dimensional medical image, and the convolution kernel is also two-dimensional. Men {\it{et al.}} \cite{58}, Trullo {\it{et al.}} \cite{83}, Gibson {\it{et al.}} \cite{60}, Chen {\it{et al.}} \cite{44}, Zhang {\it{et al.}} \cite{48}, Chen {\it{et al.}} \cite{67} used 2D networks for multi-organ segmentation. 2D architectures can reduce the GPU memory burden, but CT or MRI images are inherently 3D. Moreover, slicing images into 2D tends to ignore the rich information in the entire image voxel, so 2D models are insufficient for analysing the complex 3D structures in medical images.

3D multi-organ segmentation neural network architectures use 3D convolutional kernels, which can directly extract feature information from 3D medical images. Roth {\it{et al.}} \cite{61}, Zhu {\it{et al.}} \cite{39}, Gou {\it{et al.}} \cite{42}, and Jain {\it{et al.}} \cite{156} used 3D architectures for multi-organ segmentation. However, due to GPU memory limitations, 3D architectures may face computationally intensive and memory shortage problems, so the majority of 3D network methods use sliding windows acting on patches. Zhu {\it{et al.}} \cite{39} proposed a deep learning model called AnatomyNet, which receives full-volume head and neck CT images as the inputs and generates masks of all organs to be segmented at once. AnatomyNet only uses a down sampling layer in the first encoding block to consider the trade-off between GPU memory usage and network learning capability, which can occupy less GPU memory than other network structures while preserving information about small anatomical structures.

\subsubsection{Multi-View-Based Methods}
In medical image segmentation, it is crucial to make good use of the spatial information between medical image slices. Directly input 3D images into the network, the 3D images will occupy huge memory, or convert 3D images to 2D images, the spatial information between medical image slices will be directly discarded. Thus, the idea of multiple views has appeared, which means using 2.5D neural networks with multiple 2D slices and combining 2D convolution and 3D convolution.

The 2.5D multi-organ segmentation neural network architecture still uses 2D convolutional kernels, but the input of the network is multiple slices, either a stack of adjacent slices using interslice information \cite{64,89}, or slices along three orthogonal directions (axial, coronal, and sagittal) \cite{37,38,124,143}. This 2.5D approach saves computational resources and makes good use of spatial information. It is also widely used in semi supervised-based methods, which are reviewed in Section \ref{sec5_2}. Zhou {\it{et al.}} \cite{157} segmented each 2D slice using the FCN by sampling a 3D CT case on three orthogonally oriented slices (2D images) and then assembled the segmented output (i.e., 2D slice results) back into 3D. Chen {\it{et al.}} \cite{67} developed a multi-view training method at the ratio of 4:1:1 on different views (axial, coronal, and sagittal) and applied a majority voting strategy to combine the three predictions into a final segmentation. The results show that the method can remove some wrong segmentation areas in the single-view output, especially for the small intestine and duodenum. Wang {\it{et al.}} \cite{128} used a statistical fusion approach to combine segmentation results from three views and relate the structural similarity of 2D views to the original 3D image. Liang {\it{et al.}} \cite{143} performed context-based iterative refinement training on each of the three views and aggregated all the predicted probability maps of the three orthogonal views in the last iteration to obtain the final segmentation results. Experiments show that this multi-view framework outperforms the segmentation results of the three separate views.

Tang {\it{et al.}} \cite{68} proposed a new framework for combining 3D and 2D models, which implements segmentation through high-resolution 2D convolution and extracting spatial contextual information through low-resolution 3D convolution. The corresponding 3D features used to guide 2D segmentation are controlled by a self-attentive mechanism, and the results show that this method consistently outperforms existing 2D and 3D models. Chen {\it{et al.}} \cite{44} proposed a hybrid convolutional neural network, OrganNet2.5D, which can make full use of 3D image information to process different planar and depth image resolutions. OrganNet2.5D integrates 2D convolution and 3D convolution to extract both clear underlying edge features and rich high-level semantic features.

Some current studies only deal with 2D image, which avoids memory and computation problems but does not make full use of 3D image information. 2.5D methods can make better use of information from multiple views and improve single-view segmentation compared to 2D networks, but the spatial contextual information they can extract is still limited. Moreover, the current 2.5D methods using in multi-organ segmentation are the aggregation of three perspectives at the outcome level, and the intermediate processes are independent of each other; better use of the intermediate learning process is also the direction to be investigated \cite{158,159,160}. Some studies have performed 3D convolution, but local patches need to be processed. For example, Networks that process full-volume 3D CT images, similar to AnatomyNet, use only a down sampling layer to preserve information about small anatomical structures, so the receptive field of these networks is limited. To solve this problem, DenseASPP with four expansion rates (3,6,12,18) is introduced into FocusNet \cite{142}; however, when the expansion rates of the cascaded expanded convolution have a common factor relationship, grid problems affecting the segmentation accuracy may occur. Pure 3D networks also face the problem of increased parameter and computational burden, which limits the depth and performance of the network. Therefore, considering the memory and computational burden, better combination of multi-view information for more accurate multi-organ segmentation is still the future research direction.

\subsection{Network Dedicated Modules}
\label{sec4_3}
The network architecture is very important to improve the multi-organ segmentation accuracy, but its design process is complex. In multi-organ segmentation tasks, there are many special mechanisms to improve the accuracy of organ segmentation, such as the dilation convolution module, feature pyramid module, and attention module. They improve multi-organ segmentation accuracy by increasing the perceptual field, aggregating features of different scales, and focusing the network on the segmented region. Cheng {\it{et al.}} \cite{184} studied the performance improvement of each module of the network compared with the basic U-Net network in the head and neck segmentation task.

\subsubsection{Shape Prior Module}
Shape prior is more suitable for medical images than natural images because the spatial relationships between internal structures in medical images are relatively fixed. Therefore, considering anatomical priors in a multi-organ segmentation task will significantly improve the performance of multi-organ segmentation.

The current methods using anatomical priors fall into two main categories. One category is based on the idea of statistics, which calculates the average distribution of organs in a fully labelled dataset so that the prediction results can be as close as possible to the average distribution of organs \cite{38,55,84,161,162}. The other is to train a shape representation model, which pretrains the shape representation model using the annotation of the training dataset, and then uses it as a regularization term to constrain the predictions of the segmentation network during training \cite{41,155}. It has also been shown that generative models can learn anatomical priors \cite{163}. Therefore, it is a future research direction to consider using generative models (e.g., diffusion models, which are popular in the last two years \cite{164,165}) to better obtain anatomical prior knowledge to improve segmentation performance.

\subsubsection{Dilated Convolutional Module}
In traditional CNNs, down sampling and pooling operation are usually used several times to reduce the computation and expand the field of perception, which will lose the spatial information and make image reconstruction difficult. Dilated convolution (also known as ``Atrous'') introduces another parameter to the convolution layer, namely, the expansion rate, which can expand the field of perception to extract features across a larger spatial range without increasing the computational cost. Dilated convolution is a commonly used method in multi-organ segmentation tasks \cite{54,55,69,88,89} that increases the size of the sampling space, allowing the neural network to extract features in a larger receptive field that captures multiscale contextual information. These contextual features can capture finer structural information, which is important for pinpointing organ location. Gibson {\it{et al.}} \cite{55} used CNN networks with dilated convolution to accurately segment the liver, pancreas, stomach, and esophagus from abdominal CT. Men {\it{et al.}} \cite{58} proposed a new method based on deep extended convolutional neural network (DDCNN) for fast and consistent automatic segmentation of clinical target volumes (CTVs) and OARs. Vesal {\it{et al.}} \cite{88} introduced dilated convolution to 2D U-Net for segmenting the esophagus, heart, aorta, and thoracic trachea.

\subsubsection{Multiscale Module}
Neural networks extract the features of the target layer by layer. The lower layer networks have smaller perceptual fields and stronger representation of geometric detail information, and they have higher resolution but weaker representation of semantic information. The higher layer networks have larger perceptual fields and stronger representation of semantic information, but they have lower resolution of feature maps and weaker representation of geometric information, leading to the information loss of small targets. Common multiscale fusion modules include bottom-up, top-down, and laterally connected feature pyramids (FPNs) \cite{166}, spatial pooling pyramids (ASPPs) \cite{167} combining dilated convolution and multiscale fusion, and others. In multi-organ segmentation tasks, multiscale feature fusion has been widely used in multi-organ segmentation due to the different sizes of the organs of interest. Jia and Wei \cite{69} introduced the feature pyramid into the multi-organ segmentation network using two opposite feature pyramids, top-down and bottom-up forms, which can effectively handle multiscale changes and improve the segmentation accuracy of small targets. Shi {\it{et al.}} \cite{89} used the pyramidal structure of lateral connections between encoders and decoders to capture contextual information at multiple scales. Srivastava {\it{et al.}} \cite{49} proposed a new segmentation architecture named OARFocalFuseNet, which uses a focal modulation scheme to aggregate multiscale contexts in a specific resolution stream when performing multiscale fusion.

\subsubsection{Attention Module}
The attention module can highlight important features by dynamically weighting them. This novel attention mechanism allows exploring the inherent self-attentiveness of the network and is essential for multi-organ segmentation tasks \cite{62,168}. Common attention mechanisms include channel attention, spatial attention, and self-attention.

Squeeze-and-excitation (SE) module \cite{169} is a typical channel attention module which can focus on key parts of an image by generating a channel attention tensor. AnatomyNet \cite{39} uses 3D SE residual blocks to segment the OARs of the head and neck, enabling the extraction of 3D features directly from CT images and adaptively calibrating the mapping of residual features within each feature channel. Liu {\it{et al.}} \cite{45} proposed a new cross-layer spatial attentional map fusion network (CSAF-CNN) to segment multiple organs in the chest, which can effectively integrate the weights of different spatial attentional maps in the network, thus obtain more useful attentional maps. The average DSC of 22 organs in the head and neck was 72.50\%, which was significantly better than U-Net (63.9\%) and SE-UNet (67.9\%). Gou {\it{et al.}} \cite{42} designed a self-channel-spatial-attention neural network (SCSA-Net) for 3D head and neck OARs segmentation, which can adaptively enhance both channel and spatial features. Compared with SE-Res-Net and SE-Net, SCSA-Net improved the DSC of the optic nerve and submandibular gland by 0.06 and 0.03 and 0.05 and 0.04, respectively. Lin {\it{et al.}} \cite{70} suggested to embed the variance uncertainty into the attention architecture and proposed a variance-aware attention U-Net network to improve the attention to error-prone regions (e.g., boundary regions) in multi-organ segmentation. Compared with existing methods, the segmentation results of small organs and organs with irregular structures (e.g., duodenum, esophagus, gallbladder, and pancreas) are significantly improved. Zhang {\it{et al.}} \cite{48} proposed a new hybrid network (Weaving attention U-Net, WAU-Net) with a U-Net++ \cite{170} structure that uses CNNs to extract the underlying features, and uses axial attention blocks to efficiently model global relationships at different levels of the network, which achieve competitive performance in the head and neck multi-organ segmentation task.

\subsubsection{Other Modules}
The dense block \cite{171} can efficiently use the information of the intermediate layer, and the residual block \cite{172} can prevent gradient disappearance during backpropagation. These two modules are often embedded in the basic segmentation framework. The convolution kernel of the deformable convolution \cite{173} can adapt itself to the actual situation and better extract the input features. Heinrich {\it{et al.}} \cite{63} proposed a 3D abdominal multi-organ segmentation architecture with sparse deformable convolutions (OBELISK-Net) and showed that the combination with conventional CNNs can further improve the segmentation of small organs with large shape variations (e.g., pancreas, esophagus). The deformable convolutional block proposed by Shen {\it{et al.}} \cite{80} can handle variations in the shape and size of different organs by generating reasonable receptive fields for different organs with additional trainable offsets. The strip pooling (strip pooling) \cite{174} module can target long strip structures (e.g., esophagus and spinal cord) by using long pooling instead of traditional square pooling to avoid merging contaminated information from unrelated regions and better capture anisotropic and remote contextual information. For example, Zhang {\it{et al.}} \cite{131} used a pool of anisotropic strips with three different directional receptive fields to capture the spatial relationships between multiple organs in the abdomen. Compared to network architectures, network modules have been widely utilized because of their relatively simple design process and the relative ease of embedding them into various architectures.

\subsection{Network Loss Function}
\label{sec4_4}
As we all known, in addition to the network architecture or network modules, the segmentation accuracy also depends on the selected loss function. In multi-organ segmentation tasks, selecting a suitable loss function can reduce the class imbalance in deep learning and improve the segmentation accuracy of small organs.

Jadon \cite{175} summarized the commonly used loss functions in semantic segmentation, which are classified into distribution-based loss functions, region-based loss functions, boundary-based loss functions, and compound-based loss functions. Common loss functions used for multi-organ segmentation include CE loss \cite{176}, Dice loss \cite{177}, Tversky loss \cite{178}, focal loss \cite{179} and their combined loss functions.

\subsubsection{CE Loss}
The CE loss (cross-entropy loss function) \cite{176} is an information theoretic measure that calculates the difference between the prediction of the network and the ground truth. Men {\it{et al.}} \cite{58}, Moeskops {\it{et al.}} \cite{95}, Zhang {\it{et al.}} \cite{48} used CE loss for multi-organ segmentation. However, when the number of foreground pixels is much smaller than the background, CE loss will heavily bias the model towards the background, resulting in poor segmentation results. The weighted CE loss \cite{180} adds weight parameters to each category based on CE loss. so that it can obtain better results in the case of unbalanced sample sizes compared to the original CE loss. Since there is a significant class imbalance problem in multi-organ segmentation, i.e., a very large difference in the number of voxels in different organs, using weighted CE loss will achieve better results than using only the CE loss. Trullo {\it{et al.}} \cite{83} used a weighted CE loss to segment the heart, esophagus, trachea, and aorta in thechest image; Roth {\it{et al.}} \cite{61} applied a weighted CE loss to abdomen multi-organ segmentation.

\subsubsection{Dice Loss}
Milletari {\it{et al.}} \cite{100} proposed the Dice loss as a volume-based overlap measure, converting the voxel measure to the semantic label overlap measure, and becoming a commonly loss function in the segmentation task. Ibragimov and Xing \cite{37} used the Dice loss to segment multiple organs of the head and neck. However, the use of the Dice loss alone does not eliminate the problem that the inherent nature of neural networks is beneficial to large volume organs. Inspired by the weighted CE loss, Sudre {\it{et al.}} \cite{177} introduced the weighted Dice score (GDSC), which adaptively weighed its Dice values according to the current class size. Shen {\it{et al.}} \cite{59} investigated three different types of GDSC based on class label frequencies (uniform, simple, and square) and evaluated their effects on segmentation accuracy. Gou {\it{et al.}} \cite{42} used GDSC for head and neck multi-organ segmentation. Tappeiner {\it{et al.}} \cite{181} introduced the class adaptive Dice loss to further compensate for high imbalances based on nnU-Net, and the results showed that the method could improve the performance of class imbalance segmentation tasks.

\subsubsection{Other Losses}
The Tversky loss \cite{178} is a generalization of the Dice loss and can be optimized by adjusting the parameters to control the balance between false positives and false negatives. The focal loss \cite{179} was proposed in the field of object detection to enhance the attention on samples that are difficult to segment. Similar to the focal loss, the focal Tversky loss \cite{182} focuses on segmenting difficult samples by reducing the weights of simple sample losses. Berzoini {\it{et al.}} \cite{78} used the focal Tversky loss on smaller organs, thus balancing the indices between organs of different sizes, increasing the weights of small samples that are difficult to segment and finally solving the class imbalance problem caused by the kidney and bladder. Inspired by the exponential logarithmic loss (ELD-Loss) \cite{183}, Liu {\it{et al.}} \cite{45} introduced the top-k exponential logarithmic loss (TELD-Loss) to solve the class imbalance problem in the head and neck. The results showed that using this loss function has a strong ability to handle mislabelling.

\subsubsection{Combined Loss}
Each type of loss function has its own advantages and disadvantages. Combining multiple functions can be used for multi-organ segmentation. A more common method is the weighted sum of the Dice loss and CE loss, which attempts to solve the class imbalance problem with the Dice loss while using the CE loss for curve smoothing. Isensee {\it{et al.}} [101] proposed combining the Dice loss and CE loss to measure the overlap of voxel-like predicted outcomes and ground truth. Isler {\it{et al.}} \cite{54}, Srivastava {\it{et al.}} \cite{49}, Xu {\it{et al.}} \cite{77}, Lin {\it{et al.}} \cite{70}, and Song {\it{et al.}} \cite{73} used the weighted combination of the Dice loss and CE loss for multi-organ segmentation. When small objects are involved, using only the Dice loss leads to a lower accuracy; when the predicted region does not overlap with the labelled region, using the CE loss allows the prediction to be as close to the label as possible. Zhu {\it{et al.}} \cite{39} specifically studied different loss functions for the unbalanced head and neck region, and pointed out that the combination of the Dice loss and focal loss was superior to the ordinary Dice loss. Both Cheng {\it{et al.}} \cite{184} and Chen {\it{et al.}} \cite{44} used this combined loss function.

The conventional Dice loss is detrimental for smaller structures because a small amount of voxel misclassification leads to a large decrease in the Dice score. Applying the exponential logarithmic loss or combining the focal loss with the Dice loss can solve this problem. Using this kind of loss function does not require much adjustment to the network, however, it reduces the segmentation accuracy of the hard voxels in the region. On this basis, Lei {\it{et al.}} \cite{47} proposed a new hardness-aware loss function that can focus more on hard voxels to achieve accurate segmentation. The ultimate goal of neural network optimization is the loss function, and designing a suitable loss function so that the network can improve the segmentation accuracy of various organs is still a research direction.

\section{Imperfect Annotation-based Methods}
\label{sec5}
Currently, most of the methods in the multi-organ segmentation field are based on fully annotated methods. However, medical image data is usually hard to acquire and annotate. In particular, for multi-organ segmentation tasks, obtaining fully annotated datasets is quite difficult, which inspired the idea of using imperfect annotation. In this paper, imperfect annotations are classified into two categories. The first category is weak annotation-based methods, where weak annotation indicates that the data annotation is incomplete or imprecise in each case. For example, in multi-organ segmentation, each image has only one kind of organ annotated; each image has no pixel-level annotation but only category annotation; or the annotation is scribbled or contains noise. Another category is semi supervised-based methods, where semi supervision indicates that only a small portion of the total data is annotated and most of the remaining is unannotated. In the following, we introduce the application of these two types of methods in multi-organ segmentation.

\subsection{Weak Annotation-Based Methods}
\label{sec5_1}
In medical image segmentation, it is a difficult task to obtain the annotation of multiple organs simultaneously on the same set of images. For example, many existing single-organ datasets, such as LiTS \cite{185}, KiTS \cite{186} (p19), and pancreas datasets \cite{187}, can only provide annotations for a single organ. However, multi-organ segmentation networks cannot be effectively trained solely based on these single-organ annotated datasets. Therefore, many studies have started to explore learning unified multi-organ segmentation networks from partially labelled datasets. Based on the implementation methods, we divide the current studies into model-based approaches and pseudo label-based approaches.

\subsubsection{Model-Based Methods}
The idea of the model-based approach is to realize a unified network for multiple partially labelled organs. Chen {\it{et al.}} \cite{188} introduced a multi-branch decoder structure with a shared encoder and eight decoders to address the partial labelling problem. However, this structure is not flexible enough to be extended to new classes. Dmitriev and Kaufman \cite{189} proposed conditional CNNs for learning multi-organ segmentation models, which integrate information of organ categories into the segmentation network. Zhang and Xie {\it{et al.}} \cite{190,191} proposed the idea of DoDNet. Similar to conditional CNN, they spliced the task encoding with the features extracted by the encoder, and introduced a dynamic parameter mechanism in the segmentation head. Zhang {\it{et al.}} [103] used the leading framework nn-UNet \cite{101} as the backbone model, adding task encoding as supporting information to the decoder of nn-UNet, and combined the deep supervision mechanism to further refine the output of organs of different sizes. Wu {\it{et al.}} \cite{192} proposed TGNet composed of task-guided attention module and task-guided residual block, which can highlight task-relevant features while suppressing task-irrelevant information during feature extraction. Liu {\it{et al.}} \cite{193} first introduced incremental learning (IL) to aggregate partially labelled datasets in stages, and verified that the distribution of different partially labelled datasets misleads the process of IL. Xu and Yan \cite{194} proposed a new federated multi-encoding U-Net (Fed-MENU) method that can effectively use independent datasets with different partial labels to train a unified model for multi-organ segmentation. The model outperformed any model trained on a single dataset as well as the model trained on all datasets combined. Fang and Yan \cite{195} and Shi {\it{et al.}} \cite{196} trained uniform models on partially labelled datasets by designing new network and proposing specific loss function.

\subsubsection{Pseudo Label-Based Methods}
The pseudo label-based methods generate pseudo labels of unlabelled organs by using partial-organ segmentation models trained in partially labelled datasets, which can be converted to fully supervised methods. Zhou {\it{et al.}} \cite{161} proposed an a Prior-aware Neural Network (PaNN), which utilized prior statistics obtained from a fully labelled dataset to guide the training process based on partially labelled datasets. Huang {\it{et al.}} \cite{197} proposed a weight-averaging joint training framework, which can correct the noise in the pseudo labels, so as to learn a more robust model. Zhang {\it{et al.}} \cite{198} proposed a multi-teacher knowledge distillation framework that utilizes pseudo labels predicted by teacher models trained on partially labelled datasets to train student models for multi-organ segmentation. Lian {\it{et al.}} \cite{162} proposed a multi-organ segmentation model (PRIMP) based on single and multiple organs anatomical priors. The model first generates pseudo labels for each partially labelled dataset so as to obtain a set of multi-organ datasets with pseudo label. Then the multi-organ segmentation model is trained on this dataset, and tested on another new dataset. For the first time, this method considers the domain discrepancy between partially labelled datasets and the tested multi-organ datasets.

In addition to partial annotation, weak annotation also includes image-level annotation, sparse annotation, and noisy annotation \cite{199}. Regarding multi-organ segmentation tasks, Kanavati {\it{et al.}} \cite{200} proposed a weakly supervised organ segmentation method based on classification forests for the liver, spleen, and kidney, in which the labels are scribbled on the organs.

\subsection{Semi Supervised-Based Methods}
\label{sec5_2}
Semi supervised multi-organ segmentation methods make full use of unlabelled data to improve the segmentation performance, thus reducing the need for extensive annotation. In recent years, semi supervised learning has been widely used in medical image segmentation, such as heart segmentation \cite{201,202,203}, pancreas segmentation \cite{204}, and tumour target region segmentation \cite{205}. A detailed review of semi supervised learning in medical images was presented by Jiao {\it{et al.}} \cite{206}, who classified semi supervised medical image segmentation methods into three paradigms: pseudo label-based methods, consistency regularization-based methods, and knowledge prior-based methods. In this review, we focus on semi supervised multi-organ segmentation methods.

Ma {\it{et al.}} \cite{36} established a new benchmark for semi supervised abdominal multi-organ segmentation, which developed a method based on pseudo labelling. The teacher model was first trained on the labelled data, and generated the pseudo labels for the unlabelled data. Then, the student model was trained on both the real labelled and pseudo labelled data. Finally, the teacher model was substituted with the student model to complete the training. The results on the liver, kidney, spleen, and pancreas show that using unlabelled data can improve the performance of multi-organ segmentation.

Multi-view methods are also widely used in semi supervised multi-organ segmentation, where the model is made to learn in a collaborative training manner to extract useful information from multiple planes (e.g., sagittal, coronal, and axial planes), and then use multi-plane fusion to generate more reliable pseudo labels, and thus train better segmentation networks. Zhou {\it{et al.}} \cite{207} designed a system framework, DMPCT, for multi-organ segmentation of abdominal CT scans by fusing multi-planar information on unlabelled data during training. The framework uses a multi-planar fusion module to synthesize inferences and iteratively update pseudo labels for multiple configurations of unlabelled data. Xia {\it{et al.}} \cite{208} proposed an uncertainty-aware multi-view collaborative training (UMCT) method based on uncertainty perception, which first obtains multiple views by spatial transformations such as rotation and alignment, then trains a 3D deep segmentation network on each view, and performs joint training by implementing multi-view consistency on unlabelled data.

In addition to the collaborative training approach, multi-organ segmentation is also suitable for consistency-based learning due to the large number of prospect categories and dense distribution of organs. Consistency learning encourages consistent output through networks with different parameters. Lai {\it{et al.}} \cite{209} developed a semi supervised learning-based DLUNet for abdominal multi-organ segmentation, which consists of two lightweight U-Nets in the training phase. Moreover, regarding unlabelled data, the outputs obtained from two networks are used to supervise each other, which can improve the accuracy of these unlabelled data. It eventually achieves an average DSC of 0.8718 for 13 organs in the abdomen.

In addition, there are other semi supervised multi-organ segmentation-based methods. Lee {\it{et al.}} \cite{210} proposed a discriminator module based on human-in-the-loop quality assurance (QA) to supervise the learning of unlabelled data. They used QA scores as a loss function for unlabelled data. Raju Cheng {\it{et al.}} \cite{211} proposed a powerful semi supervised organ segmentation method, CHASe, for liver and lesion segmentation. It integrates co-training and heteromodality learning into a co-heterogeneous training framework. The framework is trained on a small single-phase dataset and can be adapted to label-free multicentre and multiphase clinical data.

\section{Discussion and Future Trends}
\label{sec6}
In this paper, a systematic review of deep learning methods for multi-organ segmentation is presented from the perspectives of both full annotation and imperfect annotation. The main innovations of the full annotation method focus on the design of network architectures, the combination of network dimensions, the innovation of network modules and the proposal of new loss functions. In terms of the network architecture design, with the development of the transformer \cite{114} architectures, better utilization of these advanced architectures for multi-organ segmentation is a promising direction, as well as the automatic search for the optimal architecture for each organ through neural network architecture search (NAS) \cite{212}. In the network dimension, optimally combining 2D and 3D architectures is a worthwhile research direction. In terms of network module, more dedicated modules need to design to improve the segmentation accuracy according to the multi-organ segmentation task. In terms of the loss functions, targeting the class imbalance, geometric prior or introducing adversarial learning loss will have great potential for designing more comprehensive and diverse loss functions.

Full annotation methods rely on fully annotated and high-quality datasets. Many imperfect annotation-based methods have been proposed for medical image segmentation in the last two years, including the aforementioned multi-organ segmentation based on weak annotation-based methods and semi annotation-based methods. However, compared to full annotation-based methods, the imperfect annotation-based methods have been less studied. It is a future research focus if imperfect annotation-based methods can be used more adequately to achieve the performance close to that of the full annotation-based methods.

Deep learning has already played a significant role in multi-organ segmentation task, but many challenges remain to be explored in the future, which are summarized in follows:

\subsection{Higher Segmentation Accuracy}
\label{sec6_1}
The current multi-organ segmentation method is more effective in solving the segmentation of large organs and organs with standard contours, such as the brainstem and mandible in the head and neck; the left and right lungs and heart in the chest; and the liver, spleen, and stomach in the abdomen. Moreover, the DSC of various methods can basically reach 0.8 or higher, while for small organs, such as the optical chiasm in the head and neck (see Fig. \ref{fig1}(8)), the left and right optic nerves (see Fig. \ref{fig1}(6 and 7)), the DSC can only reach about 0.7; irregular organs such as the pancreas in the abdomen (Fig. \ref{fig2}(4)), and long striped organs such as the spinal cord (Fig. \ref{fig2}(6)), the segmentation results are also not very satisfactory. The future research direction is to enhance the segmentation accuracy of these types of organs using more advanced automatic segmentation frameworks.

\subsection{More Comprehensive Public Datasets}
\label{sec6_2}
Currently, public datasets covering multiple organs are not sufficient. And the vast majority of methods are validated on their private datasets, making it difficult to verify the generalizability of the models. Therefore, there is a need to establish multicentre public datasets of multi-organ segmentation with large data volumes, wide coverage, and strong clinical relevance in the future.

\subsection{Better Use of Imperfect Annotations}
\label{sec6_3}
The vast majority of current methods are based on full annotation methods. Since medical image data are usually not easy to collect and annotating all the organs on the same image is a time-consuming and laborious work. Further studies can be performed to better utilize imperfect annotations \cite{213,214}, including the use of weakly annotated datasets and semi annotated datasets.

\subsection{Study of Transfer Learning Models}
\label{sec6_4}
Existing deep learning models usually trained on one part of the body, which usually tend to obtain poor results when migrated to other datasets or applied to other parts of the body. Therefore, transfer learning models need to be explored in the future. For example, Fu {\it{et al.}} \cite{65} proposed a new method called domain adaptive relational reasoning (DARR). It is used to generalize 3D multi-organ segmentation models to medical data from different domains. In addition, a very significant problem with medical images compared to other natural images is that many private datasets are not publicly available, and many hospitals only release trained models. Therefore, source free domain adaptation problem will be a very important research direction in the future. For example, Hong {\it{et al.}} \cite{81} proposed a source free unsupervised domain adaptive cross-modal approach for abdomen multi-organ segmentation.

\section{Conclusion}
\label{sec7}
In this paper, we systematically review 214 deep learning-based multi-organ segmentation studies in two broad categories, namely full annotation-based methods and imperfect annotation-based methods for multiple parts, including the head and neck, thorax and abdomen. In the fully labelled methods, we summarize the existing methods according to network architectures, network modules, network dimensions, and loss functions. In the imperfect annotation-based methods, we summarize both weak annotation-based methods and semi annotated-based methods. On this basis, we also put forward tailored solutions for some current difficulties and shortcomings in this field, and illustrate the future trends. The comprehensive survey shows that multi-organ segmentation algorithm based on deep learning is rapidly developing towards a new era of more accurate, more detailed and more automated analysis.






% \section{Introduction}
% \IEEEPARstart{T}{his} file is intended to serve as a ``sample article file''
% for IEEE journal papers produced under \LaTeX\ using
% IEEEtran.cls version 1.8b and later. The most common elements are covered in the simplified and updated instructions in ``New\_IEEEtran\_how-to.pdf''. For less common elements you can refer back to the original ``IEEEtran\_HOWTO.pdf''. It is assumed that the reader has a basic working knowledge of \LaTeX. Those who are new to \LaTeX \ are encouraged to read Tobias Oetiker's ``The Not So Short Introduction to \LaTeX ,'' available at: \url{http://tug.ctan.org/info/lshort/english/lshort.pdf} which provides an overview of working with \LaTeX.

% \section{The Design, Intent, and \\ Limitations of the Templates}
% The templates are intended to {\bf{approximate the final look and page length of the articles/papers}}. {\bf{They are NOT intended to be the final produced work that is displayed in print or on IEEEXplore\textsuperscript{\textregistered}}}. They will help to give the authors an approximation of the number of pages that will be in the final version. The structure of the \LaTeX\ files, as designed, enable easy conversion to XML for the composition systems used by the IEEE. The XML files are used to produce the final print/IEEEXplore pdf and then converted to HTML for IEEEXplore.

% \section{Where to Get \LaTeX \ Help --- User Groups}
% The following online groups are helpful to beginning and experienced \LaTeX\ users. A search through their archives can provide many answers to common questions.
% \begin{list}{}{}
% \item{\url{http://www.latex-community.org/}} 
% \item{\url{https://tex.stackexchange.com/} }
% \end{list}

% \section{Other Resources}
% See \cite{ref1,ref2,ref3,ref4,ref5} for resources on formatting math into text and additional help in working with \LaTeX .

% \section{Text}
% For some of the remainer of this sample we will use dummy text to fill out paragraphs rather than use live text that may violate a copyright.

% Itam, que ipiti sum dem velit la sum et dionet quatibus apitet voloritet audam, qui aliciant voloreicid quaspe volorem ut maximusandit faccum conemporerum aut ellatur, nobis arcimus.
% Fugit odi ut pliquia incitium latum que cusapere perit molupta eaquaeria quod ut optatem poreiur? Quiaerr ovitior suntiant litio bearciur?

% Onseque sequaes rectur autate minullore nusae nestiberum, sum voluptatio. Et ratem sequiam quaspername nos rem repudandae volum consequis nos eium aut as molupta tectum ulparumquam ut maximillesti consequas quas inctia cum volectinusa porrum unt eius cusaest exeritatur? Nias es enist fugit pa vollum reium essusam nist et pa aceaqui quo elibusdandis deligendus que nullaci lloreri bla que sa coreriam explacc atiumquos simolorpore, non prehendunt lam que occum\cite{ref6} si aut aut maximus eliaeruntia dia sequiamenime natem sendae ipidemp orehend uciisi omnienetus most verum, ommolendi omnimus, est, veni aut ipsa volendelist mo conserum volores estisciis recessi nveles ut poressitatur sitiis ex endi diti volum dolupta aut aut odi as eatquo cullabo remquis toreptum et des accus dolende pores sequas dolores tinust quas expel moditae ne sum quiatis nis endipie nihilis etum fugiae audi dia quiasit quibus.
% \IEEEpubidadjcol
% Ibus el et quatemo luptatque doluptaest et pe volent rem ipidusa eribus utem venimolorae dera qui acea quam etur aceruptat.
% Gias anis doluptaspic tem et aliquis alique inctiuntiur?

% Sedigent, si aligend elibuscid ut et ium volo tem eictore pellore ritatus ut ut ullatus in con con pere nos ab ium di tem aliqui od magnit repta volectur suntio. Nam isquiante doluptis essit, ut eos suntionsecto debitiur sum ea ipitiis adipit, oditiore, a dolorerempos aut harum ius, atquat.

% Rum rem ditinti sciendunti volupiciendi sequiae nonsect oreniatur, volores sition ressimil inus solut ea volum harumqui to see\eqref{deqn_ex1a} mint aut quat eos explis ad quodi debis deliqui aspel earcius.

% \begin{equation}
% \label{deqn_ex1a}
% x = \sum_{i=0}^{n} 2{i} Q.
% \end{equation}

% Alis nime volorempera perferi sitio denim repudae pre ducilit atatet volecte ssimillorae dolore, ut pel ipsa nonsequiam in re nus maiost et que dolor sunt eturita tibusanis eatent a aut et dio blaudit reptibu scipitem liquia consequodi od unto ipsae. Et enitia vel et experferum quiat harum sa net faccae dolut voloria nem. Bus ut labo. Ita eum repraer rovitia samendit aut et volupta tecupti busant omni quiae porro que nossimodic temquis anto blacita conse nis am, que ereperum eumquam quaescil imenisci quae magnimos recus ilibeaque cum etum iliate prae parumquatemo blaceaquiam quundia dit apienditem rerit re eici quaes eos sinvers pelecabo. Namendignis as exerupit aut magnim ium illabor roratecte plic tem res apiscipsam et vernat untur a deliquaest que non cus eat ea dolupiducim fugiam volum hil ius dolo eaquis sitis aut landesto quo corerest et auditaquas ditae voloribus, qui optaspis exero cusa am, ut plibus.


% \section{Some Common Elements}
% \subsection{Sections and Subsections}
% Enumeration of section headings is desirable, but not required. When numbered, please be consistent throughout the article, that is, all headings and all levels of section headings in the article should be enumerated. Primary headings are designated with Roman numerals, secondary with capital letters, tertiary with Arabic numbers; and quaternary with lowercase letters. Reference and Acknowledgment headings are unlike all other section headings in text. They are never enumerated. They are simply primary headings without labels, regardless of whether the other headings in the article are enumerated. 

% \subsection{Citations to the Bibliography}
% The coding for the citations is made with the \LaTeX\ $\backslash${\tt{cite}} command. 
% This will display as: see \cite{ref1}.

% For multiple citations code as follows: {\tt{$\backslash$cite\{ref1,ref2,ref3\}}}
%  which will produce \cite{ref1,ref2,ref3}. For reference ranges that are not consecutive code as {\tt{$\backslash$cite\{ref1,ref2,ref3,ref9\}}} which will produce  \cite{ref1,ref2,ref3,ref9}

% \subsection{Lists}
% In this section, we will consider three types of lists: simple unnumbered, numbered, and bulleted. There have been many options added to IEEEtran to enhance the creation of lists. If your lists are more complex than those shown below, please refer to the original ``IEEEtran\_HOWTO.pdf'' for additional options.\\

% \subsubsection*{\bf A plain  unnumbered list}
% \begin{list}{}{}
% \item{bare\_jrnl.tex}
% \item{bare\_conf.tex}
% \item{bare\_jrnl\_compsoc.tex}
% \item{bare\_conf\_compsoc.tex}
% \item{bare\_jrnl\_comsoc.tex}
% \end{list}

% \subsubsection*{\bf A simple numbered list}
% \begin{enumerate}
% \item{bare\_jrnl.tex}
% \item{bare\_conf.tex}
% \item{bare\_jrnl\_compsoc.tex}
% \item{bare\_conf\_compsoc.tex}
% \item{bare\_jrnl\_comsoc.tex}
% \end{enumerate}

% \subsubsection*{\bf A simple bulleted list}
% \begin{itemize}
% \item{bare\_jrnl.tex}
% \item{bare\_conf.tex}
% \item{bare\_jrnl\_compsoc.tex}
% \item{bare\_conf\_compsoc.tex}
% \item{bare\_jrnl\_comsoc.tex}
% \end{itemize}





% \subsection{Figures}
% Fig. 1 is an example of a floating figure using the graphicx package.
%  Note that $\backslash${\tt{label}} must occur AFTER (or within) $\backslash${\tt{caption}}.
%  For figures, $\backslash${\tt{caption}} should occur after the $\backslash${\tt{includegraphics}}.

% % \begin{figure}[!t]
% % \centering
% % \includegraphics[width=2.5in]{fig1}
% % \caption{Simulation results for the network.}
% % \label{fig_1}
% % \end{figure}

% Fig. 2(a) and 2(b) is an example of a double column floating figure using two subfigures.
%  (The subfig.sty package must be loaded for this to work.)
%  The subfigure $\backslash${\tt{label}} commands are set within each subfloat command,
%  and the $\backslash${\tt{label}} for the overall figure must come after $\backslash${\tt{caption}}.
%  $\backslash${\tt{hfil}} is used as a separator to get equal spacing.
%  The combined width of all the parts of the figure should do not exceed the text width or a line break will occur.
% %
% \begin{figure*}[!t]
% \centering
% \subfloat[]{\includegraphics[width=\columnwidth]{fig1}%
% \label{fig_first_case}}
% \hfil
% \subfloat[]{\includegraphics[width=\columnwidth]{fig1}%
% \label{fig_second_case}}
% \caption{Dae. Ad quatur autat ut porepel itemoles dolor autem fuga. Bus quia con nessunti as remo di quatus non perum que nimus. (a) Case I. (b) Case II.}
% \label{fig_sim}
% \end{figure*}

% Note that often IEEE papers with multi-part figures do not place the labels within the image itself (using the optional argument to $\backslash${\tt{subfloat}}[]), but instead will
%  reference/describe all of them (a), (b), etc., within the main caption.
%  Be aware that for subfig.sty to generate the (a), (b), etc., subfigure
%  labels, the optional argument to $\backslash${\tt{subfloat}} must be present. If a
%  subcaption is not desired, leave its contents blank,
%  e.g.,$\backslash${\tt{subfloat}}[].


 

% \section{Tables}
% Note that, for IEEE-style tables, the
%  $\backslash${\tt{caption}} command should come BEFORE the table. Table captions use title case. Articles (a, an, the), coordinating conjunctions (and, but, for, or, nor), and most short prepositions are lowercase unless they are the first or last word. Table text will default to $\backslash${\tt{footnotesize}} as
%  the IEEE normally uses this smaller font for tables.
%  The $\backslash${\tt{label}} must come after $\backslash${\tt{caption}} as always.
 
% \begin{table}[!t]
% \caption{An Example of a Table\label{tab:table1}}
% \centering
% \begin{tabular}{|c||c|}
% \hline
% One & Two\\
% \hline
% Three & Four\\
% \hline
% \end{tabular}
% \end{table}

% \section{Algorithms}
% Algorithms should be numbered and include a short title. They are set off from the text with rules above and below the title and after the last line.

% \begin{algorithm}[H]
% \caption{Weighted Tanimoto ELM.}\label{alg:alg1}
% \begin{algorithmic}
% \STATE 
% \STATE {\textsc{TRAIN}}$(\mathbf{X} \mathbf{T})$
% \STATE \hspace{0.5cm}$ \textbf{select randomly } W \subset \mathbf{X}  $
% \STATE \hspace{0.5cm}$ N_\mathbf{t} \gets | \{ i : \mathbf{t}_i = \mathbf{t} \} | $ \textbf{ for } $ \mathbf{t}= -1,+1 $
% \STATE \hspace{0.5cm}$ B_i \gets \sqrt{ \textsc{max}(N_{-1},N_{+1}) / N_{\mathbf{t}_i} } $ \textbf{ for } $ i = 1,...,N $
% \STATE \hspace{0.5cm}$ \hat{\mathbf{H}} \gets  B \cdot (\mathbf{X}^T\textbf{W})/( \mathbb{1}\mathbf{X} + \mathbb{1}\textbf{W} - \mathbf{X}^T\textbf{W} ) $
% \STATE \hspace{0.5cm}$ \beta \gets \left ( I/C + \hat{\mathbf{H}}^T\hat{\mathbf{H}} \right )^{-1}(\hat{\mathbf{H}}^T B\cdot \mathbf{T})  $
% \STATE \hspace{0.5cm}\textbf{return}  $\textbf{W},  \beta $
% \STATE 
% \STATE {\textsc{PREDICT}}$(\mathbf{X} )$
% \STATE \hspace{0.5cm}$ \mathbf{H} \gets  (\mathbf{X}^T\textbf{W} )/( \mathbb{1}\mathbf{X}  + \mathbb{1}\textbf{W}- \mathbf{X}^T\textbf{W}  ) $
% \STATE \hspace{0.5cm}\textbf{return}  $\textsc{sign}( \mathbf{H} \beta )$
% \end{algorithmic}
% \label{alg1}
% \end{algorithm}

% Que sunt eum lam eos si dic to estist, culluptium quid qui nestrum nobis reiumquiatur minimus minctem. Ro moluptat fuga. Itatquiam ut laborpo rersped exceres vollandi repudaerem. Ulparci sunt, qui doluptaquis sumquia ndestiu sapient iorepella sunti veribus. Ro moluptat fuga. Itatquiam ut laborpo rersped exceres vollandi repudaerem. 
% \section{Mathematical Typography \\ and Why It Matters}

% Typographical conventions for mathematical formulas have been developed to {\bf provide uniformity and clarity of presentation across mathematical texts}. This enables the readers of those texts to both understand the author's ideas and to grasp new concepts quickly. While software such as \LaTeX \ and MathType\textsuperscript{\textregistered} can produce aesthetically pleasing math when used properly, it is also very easy to misuse the software, potentially resulting in incorrect math display.

% % IEEE aims to provide authors with the proper guidance on mathematical typesetting style and assist them in writing the best possible article. As such, IEEE has assembled a set of examples of good and bad mathematical typesetting \cite{ref1,ref2,ref3,ref4,ref5}. 

% Further examples can be found at \url{http://journals.ieeeauthorcenter.ieee.org/wp-content/uploads/sites/7/IEEE-Math-Typesetting-Guide-for-LaTeX-Users.pdf}

% \subsection{Display Equations}
% The simple display equation example shown below uses the ``equation'' environment. To number the equations, use the $\backslash${\tt{label}} macro to create an identifier for the equation. LaTeX will automatically number the equation for you.
% \begin{equation}
% \label{deqn_ex1}
% x = \sum_{i=0}^{n} 2{i} Q.
% \end{equation}

% \noindent is coded as follows:
% \begin{verbatim}
% \begin{equation}
% \label{deqn_ex1}
% x = \sum_{i=0}^{n} 2{i} Q.
% \end{equation}
% \end{verbatim}

% To reference this equation in the text use the $\backslash${\tt{ref}} macro. 
% Please see (\ref{deqn_ex1})\\
% \noindent is coded as follows:
% \begin{verbatim}
% Please see (\ref{deqn_ex1})\end{verbatim}

% \subsection{Equation Numbering}
% {\bf{Consecutive Numbering:}} Equations within an article are numbered consecutively from the beginning of the
% article to the end, i.e., (1), (2), (3), (4), (5), etc. Do not use roman numerals or section numbers for equation numbering.

% \noindent {\bf{Appendix Equations:}} The continuation of consecutively numbered equations is best in the Appendix, but numbering
%  as (A1), (A2), etc., is permissible.\\

% \noindent {\bf{Hyphens and Periods}}: Hyphens and periods should not be used in equation numbers, i.e., use (1a) rather than
% (1-a) and (2a) rather than (2.a) for subequations. This should be consistent throughout the article.

% \subsection{Multi-Line Equations and Alignment}
% Here we show several examples of multi-line equations and proper alignments.

% \noindent {\bf{A single equation that must break over multiple lines due to length with no specific alignment.}}
% \begin{multline}
% \text{The first line of this example}\\
% \text{The second line of this example}\\
% \text{The third line of this example}
% \end{multline}

% \noindent is coded as:
% \begin{verbatim}
% \begin{multline}
% \text{The first line of this example}\\
% \text{The second line of this example}\\
% \text{The third line of this example}
% \end{multline}
% \end{verbatim}

% \noindent {\bf{A single equation with multiple lines aligned at the = signs}}
% \begin{align}
% a &= c+d \\
% b &= e+f
% \end{align}
% \noindent is coded as:
% \begin{verbatim}
% \begin{align}
% a &= c+d \\
% b &= e+f
% \end{align}
% \end{verbatim}

% The {\tt{align}} environment can align on multiple  points as shown in the following example:
% \begin{align}
% x &= y & X & =Y & a &=bc\\
% x' &= y' & X' &=Y' &a' &=bz
% \end{align}
% \noindent is coded as:
% \begin{verbatim}
% \begin{align}
% x &= y & X & =Y & a &=bc\\
% x' &= y' & X' &=Y' &a' &=bz
% \end{align}
% \end{verbatim}





% \subsection{Subnumbering}
% The amsmath package provides a {\tt{subequations}} environment to facilitate subnumbering. An example:

% \begin{subequations}\label{eq:2}
% \begin{align}
% f&=g \label{eq:2A}\\
% f' &=g' \label{eq:2B}\\
% \mathcal{L}f &= \mathcal{L}g \label{eq:2c}
% \end{align}
% \end{subequations}

% \noindent is coded as:
% \begin{verbatim}
% \begin{subequations}\label{eq:2}
% \begin{align}
% f&=g \label{eq:2A}\\
% f' &=g' \label{eq:2B}\\
% \mathcal{L}f &= \mathcal{L}g \label{eq:2c}
% \end{align}
% \end{subequations}

% \end{verbatim}

% \subsection{Matrices}
% There are several useful matrix environments that can save you some keystrokes. See the example coding below and the output.

% \noindent {\bf{A simple matrix:}}
% \begin{equation}
% \begin{matrix}  0 &  1 \\ 
% 1 &  0 \end{matrix}
% \end{equation}
% is coded as:
% \begin{verbatim}
% \begin{equation}
% \begin{matrix}  0 &  1 \\ 
% 1 &  0 \end{matrix}
% \end{equation}
% \end{verbatim}

% \noindent {\bf{A matrix with parenthesis}}
% \begin{equation}
% \begin{pmatrix} 0 & -i \\
%  i &  0 \end{pmatrix}
% \end{equation}
% is coded as:
% \begin{verbatim}
% \begin{equation}
% \begin{pmatrix} 0 & -i \\
%  i &  0 \end{pmatrix}
% \end{equation}
% \end{verbatim}

% \noindent {\bf{A matrix with square brackets}}
% \begin{equation}
% \begin{bmatrix} 0 & -1 \\ 
% 1 &  0 \end{bmatrix}
% \end{equation}
% is coded as:
% \begin{verbatim}
% \begin{equation}
% \begin{bmatrix} 0 & -1 \\ 
% 1 &  0 \end{bmatrix}
% \end{equation}
% \end{verbatim}

% \noindent {\bf{A matrix with curly braces}}
% \begin{equation}
% \begin{Bmatrix} 1 &  0 \\ 
% 0 & -1 \end{Bmatrix}
% \end{equation}
% is coded as:
% \begin{verbatim}
% \begin{equation}
% \begin{Bmatrix} 1 &  0 \\ 
% 0 & -1 \end{Bmatrix}
% \end{equation}\end{verbatim}

% \noindent {\bf{A matrix with single verticals}}
% \begin{equation}
% \begin{vmatrix} a &  b \\ 
% c &  d \end{vmatrix}
% \end{equation}
% is coded as:
% \begin{verbatim}
% \begin{equation}
% \begin{vmatrix} a &  b \\ 
% c &  d \end{vmatrix}
% \end{equation}\end{verbatim}

% \noindent {\bf{A matrix with double verticals}}
% \begin{equation}
% \begin{Vmatrix} i &  0 \\ 
% 0 & -i \end{Vmatrix}
% \end{equation}
% is coded as:
% \begin{verbatim}
% \begin{equation}
% \begin{Vmatrix} i &  0 \\ 
% 0 & -i \end{Vmatrix}
% \end{equation}\end{verbatim}

% \subsection{Arrays}
% The {\tt{array}} environment allows you some options for matrix-like equations. You will have to manually key the fences, but there are other options for alignment of the columns and for setting horizontal and vertical rules. The argument to {\tt{array}} controls alignment and placement of vertical rules.

% A simple array
% \begin{equation}
% \left(
% \begin{array}{cccc}
% a+b+c & uv & x-y & 27\\
% a+b & u+v & z & 134
% \end{array}\right)
% \end{equation}
% is coded as:
% \begin{verbatim}
% \begin{equation}
% \left(
% \begin{array}{cccc}
% a+b+c & uv & x-y & 27\\
% a+b & u+v & z & 134
% \end{array} \right)
% \end{equation}
% \end{verbatim}

% A slight variation on this to better align the numbers in the last column
% \begin{equation}
% \left(
% \begin{array}{cccr}
% a+b+c & uv & x-y & 27\\
% a+b & u+v & z & 134
% \end{array}\right)
% \end{equation}
% is coded as:
% \begin{verbatim}
% \begin{equation}
% \left(
% \begin{array}{cccr}
% a+b+c & uv & x-y & 27\\
% a+b & u+v & z & 134
% \end{array} \right)
% \end{equation}
% \end{verbatim}

% An array with vertical and horizontal rules
% \begin{equation}
% \left( \begin{array}{c|c|c|r}
% a+b+c & uv & x-y & 27\\ \hline
% a+b & u+v & z & 134
% \end{array}\right)
% \end{equation}
% is coded as:
% \begin{verbatim}
% \begin{equation}
% \left(
% \begin{array}{c|c|c|r}
% a+b+c & uv & x-y & 27\\
% a+b & u+v & z & 134
% \end{array} \right)
% \end{equation}
% \end{verbatim}
% Note the argument now has the pipe "$\vert$" included to indicate the placement of the vertical rules.


% \subsection{Cases Structures}
% Many times cases can be miscoded using the wrong environment, i.e., {\tt{array}}. Using the {\tt{cases}} environment will save keystrokes (from not having to type the $\backslash${\tt{left}}$\backslash${\tt{lbrace}}) and automatically provide the correct column alignment.
% \begin{equation*}
% {z_m(t)} = \begin{cases}
% 1,&{\text{if}}\ {\beta }_m(t) \\ 
% {0,}&{\text{otherwise.}} 
% \end{cases}
% \end{equation*}
% \noindent is coded as follows:
% \begin{verbatim}
% \begin{equation*}
% {z_m(t)} = 
% \begin{cases}
% 1,&{\text{if}}\ {\beta }_m(t),\\ 
% {0,}&{\text{otherwise.}} 
% \end{cases}
% \end{equation*}
% \end{verbatim}
% \noindent Note that the ``\&'' is used to mark the tabular alignment. This is important to get  proper column alignment. Do not use $\backslash${\tt{quad}} or other fixed spaces to try and align the columns. Also, note the use of the $\backslash${\tt{text}} macro for text elements such as ``if'' and ``otherwise.''

% \subsection{Function Formatting in Equations}
% Often, there is an easy way to properly format most common functions. Use of the $\backslash$ in front of the function name will in most cases, provide the correct formatting. When this does not work, the following example provides a solution using the $\backslash${\tt{text}} macro:

% \begin{equation*} 
%   d_{R}^{KM} = \underset {d_{l}^{KM}} {\text{arg min}} \{ d_{1}^{KM},\ldots,d_{6}^{KM}\}.
% \end{equation*}

% \noindent is coded as follows:
% \begin{verbatim}
% \begin{equation*} 
%  d_{R}^{KM} = \underset {d_{l}^{KM}} 
%  {\text{arg min}} \{ d_{1}^{KM},
%  \ldots,d_{6}^{KM}\}.
% \end{equation*}
% \end{verbatim}

% \subsection{ Text Acronyms Inside Equations}
% This example shows where the acronym ``MSE" is coded using $\backslash${\tt{text\{\}}} to match how it appears in the text.

% \begin{equation*}
%  \text{MSE} = \frac {1}{n}\sum _{i=1}^{n}(Y_{i} - \hat {Y_{i}})^{2}
% \end{equation*}

% \begin{verbatim}
% \begin{equation*}
%  \text{MSE} = \frac {1}{n}\sum _{i=1}^{n}
% (Y_{i} - \hat {Y_{i}})^{2}
% \end{equation*}
% \end{verbatim}

% \section{Conclusion}
% The conclusion goes here.


\section*{Acknowledgments}
This work was supported by the National Natural Science Foundation of China under grant 82072021. This work was also supported by the medical-industrial integration project of Fudan University under grant XM03211181.



% {\appendix[Proof of the Zonklar Equations]
% Use $\backslash${\tt{appendix}} if you have a single appendix:
% Do not use $\backslash${\tt{section}} anymore after $\backslash${\tt{appendix}}, only $\backslash${\tt{section*}}.
% If you have multiple appendixes use $\backslash${\tt{appendices}} then use $\backslash${\tt{section}} to start each appendix.
% You must declare a $\backslash${\tt{section}} before using any $\backslash${\tt{subsection}} or using $\backslash${\tt{label}} ($\backslash${\tt{appendices}} by itself
%  starts a section numbered zero.)}




% \section{References Section}
% You can use a bibliography generated by BibTeX as a .bbl file.
%  BibTeX documentation can be easily obtained at:
%  http://mirror.ctan.org/biblio/bibtex/contrib/doc/
%  The IEEEtran BibTeX style support page is:
%  http://www.michaelshell.org/tex/ieeetran/bibtex/
 
%  % argument is your BibTeX string definitions and bibliography database(s)
% %\bibliography{IEEEabrv,../bib/paper}
% %
% \section{Simple References}
% You can manually copy in the resultant .bbl file and set second argument of $\backslash${\tt{begin}} to the number of references
%  (used to reserve space for the reference number labels box).

% \bibliographystyle{IEEEtran}
% \bibliography{myref}
\input{bare_jrnl_new_sample4.bbl}


% \newpage

% \section{Biography Section}
% If you have an EPS/PDF photo (graphicx package needed), extra braces are
%  needed around the contents of the optional argument to biography to prevent
%  the LaTeX parser from getting confused when it sees the complicated
%  $\backslash${\tt{includegraphics}} command within an optional argument. (You can create
%  your own custom macro containing the $\backslash${\tt{includegraphics}} command to make things
%  simpler here.)
 
% \vspace{11pt}

% \bf{If you include a photo:}\vspace{-33pt}
% \begin{IEEEbiography}[{\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{fig1}}]{Michael Shell}
% Use $\backslash${\tt{begin\{IEEEbiography\}}} and then for the 1st argument use $\backslash${\tt{includegraphics}} to declare and link the author photo.
% Use the author name as the 3rd argument followed by the biography text.
% \end{IEEEbiography}

% \vspace{11pt}

% \bf{If you will not include a photo:}\vspace{-33pt}
% \begin{IEEEbiographynophoto}{John Doe}
% Use $\backslash${\tt{begin\{IEEEbiographynophoto\}}} and the author name as the argument followed by the biography text.
% \end{IEEEbiographynophoto}

\clearpage

{\appendices
% \section*{Supplementary Materials}
\begin{table*}[ht!]
\section*{Supplementary Materials}
\centering 
\caption{DSC-Based Summary of the Literature on Multi-Organ Single-State Segmentation Methods for the Head and Neck}
\label{tab_S_1}
\resizebox{\textwidth}{!}{
\begin{tabular}{@{}llllllllllllll@{}}
\toprule
\multirow{2}{*}{Ref}                                     & \multirow{2}{*}{Backbone}                  & \multirow{2}{*}{Datasets}              & \multirow{2}{*}{Quantity} & \multirow{2}{*}{Organ type} & \multirow{2}{*}{Brainstem} & \multirow{2}{*}{Mandible} & \multicolumn{2}{l}{Parotid gland} & \multicolumn{2}{l}{Submandibular gland} & \multicolumn{2}{l}{Optic nerve} & \multirow{2}{*}{Chiasm} \\ \cmidrule(lr){8-13}
                                        &                           &                       &          &            &           &          & Left            & Right           & Left               & Right              & Left           & Right          &        \\ \midrule
Ibragimov and Xing \cite{37}             & 2.5D CNN                  & Private (CT)          & 50       & 13         & -         & 0.895    & 0.766           & 0.779           & 0.697              & 0.730              & 0.639          & 0.645          & 0.374  \\
Fritscher {\it{et al.}} \cite{38}              & 2.5D CNN                  & HNC (CT) \cite{30}     & 30       & 3          & -         & -        & 0.810           & -               & 0.650              & -                  & -              & -              & 0.520  \\
Zhu {\it{et al.}} \cite{39}                      & 3D U-Net                  & Private (CT)          & 261      & 9          & 0.867     & 0.925    & 0.881           & 0.874           & 0.814              & 0.813              & 0.721          & 0.706          & 0.532  \\
Van Rooij {\it{et al.}} \cite{40}               & 3D U-Net                  & Private (CT)          & 157      & 11         & 0.640     & -        & 0.830           & 0.830           & 0.820              & 0.810              & -              & -              & -      \\
Tong {\it{et al.}} \cite{41}                     & 3D GAN                    & PDDCA (CT) \cite{30}   & 48       & 9          & 0.867     & 0.939    & 0.855           & 0.858           & 0.807              & 0.819              & 0.664          & 0.699          & 0.592  \\
Tong {\it{et al.}} \cite{41}                     & 3D GAN                    & Private (MRI)         & 25       & 9          & 0.916     & 0.816    & 0.865           & 0.825           & -                  & -                  & 0.717          & 0.693          & 0.589  \\
Gou {\it{et al.}} \cite{42}                       & 3D U-Net                  & HNC (CT) \cite{30}     & 48       & 9          & 0.880     & 0.940    & 0.870           & 0.860           & 0.780              & 0.810              & 0.720          & 0.710          & 0.610  \\
Liu {\it{et al.}} \cite{43}                       & 3D U-Net                  & Private (MRI \& CT)   & 45       & 19         & 0.880     & 0.890    & 0.890           & 0.880           & -                  & -                  & 0.720          & 0.720          & 0.760  \\
Liu {\it{et al.}} \cite{43}                       & 3D U-Net                  & HNC (CT) \cite{30}     & 48       & 9          & 0.910     & 0.960    & 0.880           & 0.880           & 0.860              & 0.850              & 0.780          & 0.780          & 0.730  \\
Chen {\it{et al.}} \cite{44}                     & 2.5D U-Net                & Private (CT)          & 307      & 24         & -         & -        & -               & -               & -                  & -                  & 0.711          & 0.712          & 0.598  \\
Chen {\it{et al.}} \cite{44}                      & 2.5D U-Net                & HNC (CT) \cite{30}     & 48       & 9          & 0.872     & 0.922    & 0.867           & 0.858           & 0.821              & 0.821              & 0.750          & 0.741          & 0.663  \\
Liu {\it{et al.}} \cite{45}                      & 2D U-Net                  & StructSeg (CT)        & 50       & 22         & 0.864     & 0.906    & 0.802           & 0.826           & -                  & -                  & 0.770          & 0.647          & 0.712  \\
Cros {\it{et al.}} \cite{46}                     & 3D U-Net                  & Private (CT)          & 200      & 12         & -         & 0.900    & 0.760           & 0.760           & 0.740              & -                  & -              & -              & -      \\
Lei {\it{et al.}} \cite{47}                      & 2.5D U-Net                & StructSeg (CT)        & 50       & 22         & 0.897     & 0.914    & 0.857           & 0.873           & -                  & -                  & 0.680          & 0.663          & 0.566  \\
Lei {\it{et al.}} \cite{47}                      & 2.5D U-Net                & Hybrid HAN (CT)       & 165      & 7          & 0.874     & 0.900    & 0.847           & 0.846           & -                  & -                  & 0.624          & 0.621          & 0.290  \\
Zhang {\it{et al.}} \cite{48}                     & 2D U-Net                  & HNC (CT) \cite{30}     & 48       & 9          & 0.840     & 0.900    & 0.820           & 0.830           & 0.820              & 0.810              & 0.670          & 0.710          & 0.660  \\
Srivastava {\it{et al.}} \cite{49}              & 3D U-Net                  & OpenKBP (CT) \cite{35} & 188      & 5          & 0.803     & 0.883    & 0.799           & 0.773           & -                  & -                  & -              & -              & -      \\
Podobnik {\it{et al.}} \cite{50}                 & 2D nnU-net                & Private (CT\&MR)      & 56       & 31         & 0.836     & 0.898    & 0.817           & 0.765           & 0.716              & 0.670              & 0.572          & 0.604          & 0.387  \\
Kan {\it{et al.}} \cite{51}                      & 3D Transformer and U-Net  & Private (CT)          & 94       & 18         & 0.871     & 0.925    & 0.821           & 0.844           & -                  & -                  & 0.717          & 0.679          & 0.328  \\
Jiang {\it{et al.}} \cite{52}                     & 2D U-Net                  & PDDCA (CT) \cite{30}   & 16       & 6          & 0.920     & 0.950    & 0.880           & 0.880           & 0.820              & 0.830              &                &                &        \\
\multirow{2}{*}{Francis {\it{et al.}} \cite{53}}  & \multirow{2}{*}{3D U-Net} & Private (CT)          & 232      & 7          & 0.890     & 0.932    & 0.852           & 0.870           &                    &                    & 0.744          & 0.764          & 0.635  \\
                                        &                           & HNC (CT)  \cite{30}     & 48       & 7          & 0.862     & 0.940    & 0.885           & 0.885           &                    &                    & 0.728          & 0.723          & 0.620  \\
\multirow{2}{*}{Isler {\it{et al.}} \cite{54}}    & \multirow{2}{*}{2D U-Net} & HNC (CT)  \cite{30}     & 48       & 6          & 0.830     & -        & 0.790           & 0.760           & -                  & -                  & 0.580          & 0.540          & 0.520  \\
                                        &                           & OpenKBP (CT) \cite{35}  & 188      & 5          & 0.800     & 0.860    & 0.750           & 0.760           & -                  & -                  & -              & -              & -      \\ \bottomrule
\end{tabular}}
\end{table*}

\begin{table*}[ht!]
\caption{DSC-Based Summary of the Literature on Multi-Organ Single-Stage Segmentation Methods for the Abdomen}
\label{tab_S_2}
\resizebox{\textwidth}{!}{%
\begin{tabular}{@{}llllllllllll@{}}
\toprule
\multirow{2}{*}{Ref}                                       & \multirow{2}{*}{Backbone}                  & \multirow{2}{*}{Datasets}                                                            & \multirow{2}{*}{Quantity}                            & \multirow{2}{*}{Organ type} & \multirow{2}{*}{Liver}        & \multirow{2}{*}{Spleen}       & \multicolumn{2}{l}{Kidney}             & \multirow{2}{*}{Pancreas}       & \multirow{2}{*}{Gallbladder}       & \multirow{2}{*}{Stomach}       \\ \cmidrule(lr){8-9}
                                          &                           &                                                                     &                                     &            &              &              & Left               & Right             &                &                   &               \\ \midrule
Gibson {\it{et al.}} \cite{55}                     & 3D CNN                    & \begin{tabular}[c]{@{}l@{}}TCIA \cite{56,57} \& \\ BTCV \cite{29} (CT)\end{tabular}                        & 72                                  & 4          & 0.920        & -            & -                  & -                 & 0.660          & -                 & 0.830         \\
Men {\it{et al.}} \cite{58}                        & 2D CNN                    & Private (CT)                                                        & 278                                 & 5          & -            & -            & -                  & -                 & -              & -                 & -             \\
Shen {\it{et al.}} \cite{59}                      & 3D U-Net                  & Private (CT)                                                        & 377                                 & 7          & 0.965        & 0.947        & -                  & -                 & 0.847          & 0.808             & 0.963         \\
Gibson {\it{et al.}} \cite{60}                     & 3D V-Net                  & \begin{tabular}[c]{@{}l@{}}TCIA \cite{56,57} \& \\ BTCV \cite{29} (CT)\end{tabular}                        & 90                                  & 8          & 0.960        & 0.960        & 0.950              & -                 & 0.780          & 0.840             & 0.900         \\
Roth {\it{et al.}} \cite{61}                       & 3D U-Net                  & Private (CT)                                                        & 377                                 & 7          & 0.971        & 0.977        & -                  & -                 & 0.849          & 0.851             & 0.961         \\
Cai {\it{et al.}} \cite{62}                       & 2D FCN                    & Private (CT)                                                        & 120                                 & 16         & 0.96         & 0.951        & 0.956              & 0.954             & 0.785          & 0.797             & 0.909         \\
Cai {\it{et al.}} \cite{62}                       & 3D GAN                    & Private (CT)                                                        & \begin{tabular}[c]{@{}l@{}}131(liver)+281(spleen)\\ +41(pancreas)\end{tabular} & 3          & 0.944        & 0.960        & -                  & -                 & 0.743          & -                 & -             \\
\multirow{2}{*}{Heinrich {\it{et al.}} \cite{63}} & \multirow{2}{*}{3D U-Net} & TCIA (CT) \cite{56,57}                                        & 43                                  & 8          & 0.954        & 0.944        & -                  & -                 & 0.702          & 0.753             & 0.868         \\
                                          &                           & Private (CT)                                                        & 10                                  & 7          &              &              &                    &                   &                &                   &               \\
\multirow{2}{*}{Ahn {\it{et al.}} \cite{64}}       & \multirow{2}{*}{2.5D CNN} & Private (CT)                                                        & 813+150                             & 2          & 0.973        & 0.974        & -                  & -                 & -              & -                 & -             \\
                                          &                           & Private (CT)                                                        & 813+50                              & 2          & 0.983        & 0.968        & -                  & -                 & -              & -                 & -             \\
Fu {\it{et al.}} \cite{65}                        & 3D V-Net                  & Synapse (CT) \cite{2}                                                & 90                                  & 8          & \multicolumn{7}{l}{\begin{tabular}[c]{@{}l@{}}Aorta, gallbladder, left kidney, right kidney, liver, pancreas, spleen and\\ stomach average DSC: 0.698\end{tabular}} \\
Hatamizadeh {\it{et al.}} \cite{66}               & \begin{tabular}[c]{@{}l@{}}3D Transformer\\ and U-Net\end{tabular}  & BTCV (CT) \cite{29}                                                  & 30                                  & 13         & 0.983        & 0.972        & 0.954              & 0.942             & 0.799          & 0.825             & 0.945         \\
Chen {\it{et al.}} \cite{67}                       & 2.5D U-Net                & Private (MR)                                                        & 102                                 & 10         & 0.963        & 0.946        & 0.954              & 0.954             & 0.880          & 0.732             & 0.923         \\
Tang {\it{et al.}} \cite{68}                      & 2.5D U-Net                & ABD-110 (CT) \cite{68}                                               & 110                                 & 11         & 0.964        & 0.959        & 0.960              & 0.957             & 0.821          & 0.822             & 0.875         \\
Jia and Wei \cite{69}                      & 3D U-Net                  & CHAOS (CT \cite{32}                                                 & 20                                  & 4          & 0.934        & 0.896        & 0.937              & 0.949             & -              & -                 & -             \\
Lin {\it{et al.}} \cite{70}                       & 3D U-Net                  & \begin{tabular}[c]{@{}l@{}}TCIA \cite{56,57} \& \\ BTCV \cite{29} (CT)\end{tabular}                        & 90                                  & 8          & 0.953        & 0.920        & 0.902              & -                 & 0.742          & 0.760             & 0.862         \\
Cao {\it{et al.}} \cite{71}                        & 2D Transformer            & Synapse (CT) \cite{2}                                                & 30                                  & 8          & 0.943        & 0.907        & 0.833              & 0.796             & 0.566          & 0.665             & 0.766         \\
Chen {\it{et al.}} \cite{72}                       & \begin{tabular}[c]{@{}l@{}}2D Transformer\\ And U-Net\end{tabular}  & Synapse (CT) \cite{2}                                                & 30                                  & 8          & 0.941        & 0.851        & 0.819              & 0.770             & 0.559          & 0.631             & 0.756         \\
Song {\it{et al.}} \cite{73}                       & 2D CNN                    & Synapse (CT) \cite{2}                                                & 30                                  & 8          & 0.959        & 0.926        & 0.906              & 0.892             & 0.687          & 0.671             & 0.839         \\
Kumar {\it{et al.}} \cite{74}                      & 2D GAN                    & CHAOS (CT) \cite{32}                                                 & 40                                  & 4          & \multicolumn{7}{l}{Liver, left kidney, right kidney and spleen average DSC: 0.970}                                        \\
Huang {\it{et al.}} \cite{75}                      & 2D Transformer            & Synapse (CT) \cite{2}                                                & 30                                  & 8          & 0.944        & 0.919        & 0.852              & 0.820             & 0.657          & 0.687             & 0.808         \\
Suo {\it{et al.}} \cite{76}                        & \begin{tabular}[c]{@{}l@{}}2D Transformer\\ And U-Net\end{tabular}  & Synapse (CT) \cite{2}                                                & 30                                  & 8          & 0.951        & 0.914        & 0.890              & 0.851             & 0.699          & 0.720             & 0.826         \\
\multirow{2}{*}{Xu {\it{et al.}} \cite{77}}       & \multirow{2}{*}{3D V-Net} & \begin{tabular}[c]{@{}l@{}}AbdomenCT-1K  \cite{36}\\ +Private (CT)\end{tabular}                                 & 1112+100                            & 4          & 0.953        & 0.920        & \multicolumn{2}{l}{0.914}              & 0.747          & -                 & -             \\
                                          &                           & \begin{tabular}[c]{@{}l@{}}AbdomenCT-1K  \cite{36}\\ +TCIA \cite{56,57} \& \\ BTCV (CT) \cite{29}\end{tabular} & 1112+90                             & 4          & 0.961        & 0.954        & 0.918              & -                 & 0.784          & -                 & -             \\
Berzoini {\it{et al.}} \cite{78}                   & 2D U-Net                  & \begin{tabular}[c]{@{}l@{}}Open-source CT-org\\ dataset \cite{79}\end{tabular}                                 & 140                                 & 5          & 0.922        & -            & \multicolumn{2}{l}{0.837}              & -              & -                 & -             \\
Shen {\it{et al.}} \cite{80}                       & 2D U-Net                  & TCIA (CT) \cite{56,57}                                        & 42                                  & 5          & 0.960        & -            & -                  & -                 & 0.754          & 0.805             & 0.889         \\
Hong {\it{et al.}} \cite{81}                       & 2D U-Net                  & \begin{tabular}[c]{@{}l@{}}BTCV \cite{29} \& CHAOS\\ \cite{32} (CT)\end{tabular}                                 & 30+20                               & 4          & 0.884        & 0.911        & 0.864              & 0.891             & -              & -                 & -             \\
Xie {\it{et al.}} \cite{82}                       & \begin{tabular}[c]{@{}l@{}}3D CNN And\\ Transformer\end{tabular}    & BTCV \cite{29} (CT)                                                  & 30                                  & 11         & 0.971        & 0.963        & \multicolumn{2}{l}{0.939}              & 0.831          & 0.666             & 0.882         \\
Srivastava {\it{et al.}} \cite{49}                & 3D U-Net                  & Synapse (CT) \cite{2}                                                & 30                                  & 8          & 0.950        & 0.870        & 0.842              & 0.824             & 0.681          & 0.675             & 0.760         \\
Jiang {\it{et al.}} \cite{52}                      & 2D U-Net                  & BTCV (CT) \cite{29}                                                  & 30                                  & 12         & 0.969        & 0.958        & 0.943              & 0.921             & 0.798          & 0.786             & 0.906         \\ \bottomrule
\end{tabular}%
}
\end{table*}

\begin{table*}[ht!]
\caption{DSC-Based Summary of the Literature on Multi-Organ Single-State Segmentation Methods for the Thorax}
\label{tab_S_3}
\resizebox{\textwidth}{!}{%
\begin{tabular}{@{}llllllllllll@{}}
\toprule
\multirow{2}{*}{Ref}                                                                                    & \multirow{2}{*}{Backbone}                                                    & \multirow{2}{*}{Dataset}                                                                               & \multirow{2}{*}{Quantity}                                        & \multirow{2}{*}{Organ type}                                    & \multirow{2}{*}{Heart}                                                 & \multirow{2}{*}{Esophagus}                                             & \multicolumn{2}{l}{Trachea}                                                                           & \multirow{2}{*}{Aorta}                                         & \multirow{2}{*}{Spinal cord}                                           & \multirow{2}{*}{Stomach}                                       \\ \cmidrule(lr){8-9}
                                                                                       &                                                             &                                                                                       &                                                 &                                               &                                                       &                                                       & Left                                                  & Right                                         &                                               &                                                       &                                               \\ \midrule
Trullo {\it{et al.}} \cite{83}                                                                 & 2D FCN                                                      & Private (CT)                                                                          & 30                                              & 4                                             & 0.900                                                 & 0.670                                                 & 0.820                                                 & -                                             & -                                             & 0.860                                                 & -                                             \\
Dong {\it{et al.}} \cite{84}                                                                   & 3D GAN                                                      & AAPM (CT) \cite{31}                                                                    & 35                                              & 4                                             & 0.870                                                 & 0.750                                                 & -                                                     & 0.970                                         & 0.970                                         & -                                                     & 0.900                                         \\
Vu {\it{et al.}} \cite{85}                                                                     & 2D U-Net                                                    & Private (CT)                                                                          & 22411(2D)                                       & 5                                             & 0.910                                                 & 0.630                                                 & -                                                     & 0.960                                         & 0.960                                         & -                                                     & 0.710                                         \\
\begin{tabular}[c]{@{}l@{}}Lambert {\it{et al.}} \cite{86}\\ Gali {\it{et al.}} \cite{87}\end{tabular} & \begin{tabular}[c]{@{}l@{}}2D U-Net\\ 2D U-Net\end{tabular} & \begin{tabular}[c]{@{}l@{}}SegTHOR (CT) \cite{33}\\ SegTHOR (CT) \cite{33}\end{tabular} & \begin{tabular}[c]{@{}l@{}}60\\ 60\end{tabular} & \begin{tabular}[c]{@{}l@{}}4\\ 4\end{tabular} & \begin{tabular}[c]{@{}l@{}}0.930\\ 0.860\end{tabular} & \begin{tabular}[c]{@{}l@{}}0.820\\ 0.469\end{tabular} & \begin{tabular}[c]{@{}l@{}}0.850\\ 0.643\end{tabular} & \begin{tabular}[c]{@{}l@{}}-\\ -\end{tabular} & \begin{tabular}[c]{@{}l@{}}-\\ -\end{tabular} & \begin{tabular}[c]{@{}l@{}}0.910\\ 0.854\end{tabular} & \begin{tabular}[c]{@{}l@{}}-\\ -\end{tabular} \\
Vesal {\it{et al.}} \cite{88}                                                                  & 2D U-Net                                                    & SegTHOR (CT) \cite{33}                                                                 & 60                                              & 4                                             & 0.941                                                 & 0.858                                                 & 0.926                                                 & -                                             & -                                             & 0.938                                                 & -                                             \\
Shi {\it{et al.}} \cite{89}                                                                    & 2.5D U-Net                                                  & StructSeg (CT) \cite{1}                                                                & 50                                              & 5                                             & 0.941                                                 & 0.821                                                 & 0.882                                                 & 0.968                                         & 0.971                                         & -                                                     & 0.902                                         \\
Mahmood {\it{et al.}} \cite{90}                                                                & 2D U-Net                                                    & AAPM (CT) \cite{31}                                                                    & 60                                              & 5                                             & 0.880                                                 & 0.660                                                 & -                                                     & 0.970                                         & 0.970                                         & -                                                     & 0.800                                         \\
Zhang {\it{et al.}} \cite{91}                                                                 & 2D FCN                                                      & Private (CT)                                                                          & 36                                              & 6                                             & 0.860                                                 & 0.670                                                 & 0.910                                                 & 0.950                                         & 0.960                                         & -                                                     & 0.890       \\ 
\bottomrule
\end{tabular}%
}
\end{table*}

\begin{table*}[ht!]
\caption{DSC-Based Summary of the Literature on Multi-Organ Coarse-to-Fine Segmentation Methods for the Head and Neck}
\label{tab_S_4}
\resizebox{\textwidth}{!}{%
\begin{tabular}{@{}lllllllllllllll@{}}
\toprule
\multirow{2}{*}{Ref}                        & \multirow{2}{*}{Coarse}     & \multirow{2}{*}{Fine}     & \multirow{2}{*}{Datasets}          & \multirow{2}{*}{Quantity} & \multirow{2}{*}{Organ type} & \multirow{2}{*}{Brainstem} & \multirow{2}{*}{Mandible} & \multicolumn{2}{l}{Parotid gland}               & \multicolumn{2}{l}{Submandibular gland}         & \multicolumn{2}{l}{Optic Nerve}                 & \multirow{2}{*}{Chiasm} \\ \cmidrule(lr){9-14}
                                            &                             &                           &                                    &                           &                             &                            &                           & Left                   & Right                  & Left                   & Right                  & Left                   & Right                  &                         \\ \midrule
Ren {\it{et al.}} \cite{122}                        & 3D CNN                      & 3D CNN                    & HNC (CT) \cite{30}                  & 48                        & 3                           & -                          & -                         & -                      & -                      & -                      & -                      & 0.720                  & 0.700                  & 0.580                   \\
\multirow{2}{*}{Tappeiner {\it{et al.}} \cite{123}} & \multirow{2}{*}{3D CNN}     & \multirow{2}{*}{3D CNN}   & \multirow{2}{*}{HNC (CT) \cite{30}} & \multirow{2}{*}{40}       & \multirow{2}{*}{7}          & \multirow{2}{*}{0.820}     & \multirow{2}{*}{0.910}    & \multirow{2}{*}{0.800} & \multirow{2}{*}{0.810} & \multirow{2}{*}{-}     & \multirow{2}{*}{-}     & \multirow{2}{*}{0.640} & \multirow{2}{*}{0.630} & \multirow{2}{*}{0.420}  \\
                                            &                             &                           &                                    &                           &                             &                            &                           &                        &                        &                        &                        &                        &                        &                         \\
\multirow{2}{*}{Pu {\it{et al.}} \cite{124}}        & \multirow{2}{*}{2.5D U-Net} & \multirow{2}{*}{3D U-Net} & \multirow{2}{*}{HNC (CT) \cite{30}} & \multirow{2}{*}{48}       & \multirow{2}{*}{9}          & \multirow{2}{*}{0.880}     & \multirow{2}{*}{0.940}    & \multirow{2}{*}{0.860} & \multirow{2}{*}{0.865} & \multirow{2}{*}{0.788} & \multirow{2}{*}{0.802} & \multirow{2}{*}{0.743} & \multirow{2}{*}{0.768} & \multirow{2}{*}{0.612}  \\
                                            &                             &                           &                                    &                           &                             &                            &                           &                        &                        &                        &                        &                        &                        &                         \\
\multirow{2}{*}{Ma {\it{et al.}} \cite{125}}       & \multirow{2}{*}{3D U-Net}   & \multirow{2}{*}{}         & \multirow{2}{*}{HNC (CT) \cite{30}} & \multirow{2}{*}{48}       & \multirow{2}{*}{9}          & \multirow{2}{*}{0.879}     & \multirow{2}{*}{0.945}    & \multirow{2}{*}{0.892} & \multirow{2}{*}{0.884} & \multirow{2}{*}{0.829} & \multirow{2}{*}{0.815} & \multirow{2}{*}{0.753} & \multirow{2}{*}{0.747} & \multirow{2}{*}{0.659}  \\
                                            &                             &                           &                                    &                           &                             &                            &                           &                        &                        &                        &                        &                        &                        &                         \\
\multirow{2}{*}{Fang {\it{et al.}} \cite{109}}      & \multirow{2}{*}{2D FCN}     & \multirow{2}{*}{3D U-Net} & HNC (CT) \cite{30}                  & 32                        & 9                           & 0.849                      & 0.924                     & 0.842                  & 0.849                  & 0.734                  & 0.782                  & 0.676                  & 0.684                  & 0.547                   \\
                                            &                             &                           & Private (CT)                       & 56                        & 14                          & 0.863                      & 0.905                     & 0.582                  & 0.687                  & 0.668                  & 0.575                  &                        &                        &                         \\ \bottomrule
\end{tabular}%
}
\end{table*}

\begin{table*}[ht!]
\caption{DSC-Based Summary of the Literature on Multi-Organ Coarse-to-Fine Segmentation Methods for the Abdomen}
\label{tab_S_5}
\resizebox{\textwidth}{!}{%
\begin{tabular}{@{}lllllllllllll@{}}
\toprule
\multirow{2}{*}{Ref}    & \multirow{2}{*}{Coarse} & \multirow{2}{*}{Fine} & \multirow{2}{*}{Datasets}                                           & \multirow{2}{*}{Quantity} & \multirow{2}{*}{Category} & \multirow{2}{*}{Liver} & \multirow{2}{*}{Spleen} & \multicolumn{2}{l}{Kidney} & \multirow{2}{*}{Pancreas} & \multirow{2}{*}{Gallbladder} & \multirow{2}{*}{Stomach} \\ \cmidrule(lr){9-10}
                        &                         &                       &                                                                     &                           &                           &                        &                         & Left         & Right       &                           &                              &                          \\ \midrule
Hu {\it{et al.}} \cite{126}     & 3D FCN                  & Refinement Model      & Private (CT)                                                        & 140                       & 4                         & 0.960                  & 0.942                   & 0.954        &             & -                         & -                            & -                        \\
Roth {\it{et al.}} \cite{127}   & 3D FCN                  & 3D FCN                & Private (CT)                                                        & 331                       & 3                         & 0.932                  & 0.906                   & -            & -           & 0.631                     & 0.706                        & 0.843                    \\
Wang {\it{et al.}} \cite{128}   & 2.5D FCN                & 2.5D FCN              & Private (CT)                                                        & 236                       & 13                        & 0.980                  & 0.971                   & 0.968        & 0.984       & 0.878                     & 0.905                        & 0.952                    \\
Zhang {\it{et al.}} \cite{129}  & 3D V-Net                & 3D V-Net              & BTCV (CT) \cite{29}                                                  & 30                        & 13                        & 0.945                  & 0.915                   & 0.909        & 0.919       & 0.694                     & 0.682                        & 0.784                    \\
Xie {\it{et al.}} \cite{130}    & 2.5D FCN                & 2.5D FCN              & Private (CT)                                                        & 200                       & 16                        & 0.969                  & 0.968                   & 0.962        & 0.960       & 0.877                     & 0.894                        & 0.951                    \\
Zhang {\it{et al.}} \cite{131}  & 3D U-Net                & 3D U-Net              & \begin{tabular}[c]{@{}l@{}}FLARE\\ 2021 (CT) \cite{132}\end{tabular} & 511                       & 4                         & 0.954                  & 0.942                   & \multicolumn{2}{l}{0.936}  & 0.753                     & -                            & -                        \\
Lee {\it{et al.}} \cite{133}    & 3D U-Net                & 3D U-Net              & Private (CT)                                                        & 100                       & 13                        & 0.960                  & 0.965                   & 0.945        & 0.920       & 0.766                     & 0.793                        & 0.833                    \\
Kakeya {\it{et al.}} \cite{134} & 3D U-Net                & 3D U-Net              & Private (CT)                                                        & 47                        & 8                         & 0.971                  & 0.969                   & 0.984        & 0.975       & 0.861                     & 0.918                        & -                        \\ \bottomrule
\end{tabular}%
}
\end{table*}

\begin{table*}[ht!]
\caption{DSC-Based Summary of the Literature on Multi-Organ Coarse-to-Fine Segmentation Methods for the Thorax}
\label{tab_S_6}
\resizebox{\textwidth}{!}{%
\begin{tabular}{@{}lllllllllllll@{}}
\toprule
\multirow{2}{*}{Ref}    & \multirow{2}{*}{Coarse} & \multirow{2}{*}{Fine} & \multirow{2}{*}{Datasets} & \multirow{2}{*}{Quantity} & \multirow{2}{*}{Category} & \multirow{2}{*}{Heart} & \multirow{2}{*}{Esophagus} & \multirow{2}{*}{Trachea} & \multicolumn{2}{l}{Lung} & \multirow{2}{*}{Aorta} & \multirow{2}{*}{Spinal cord} \\ \cmidrule(lr){10-11}
                        &                         &                       &                           &                           &                           &                        &                            &                          & Left       & Right       &                        &                              \\ \midrule
Trullo {\it{et al.}} \cite{135} & 2D FCN                  & 2D FCN                & Private (CT)              & 30                        & 4                         & 0.900                  & 0.690                      & 0.870                    & -          & -           & 0.89                   & -                            \\
Cao {\it{et al.}} \cite{136}    & 2D U-Net                & 2D U-Net              & SegTHOR (CT) \cite{33}     & 50                        & 6                         & 0.945                  & 0.850                      & 0.807                    & 0.97       & 0.966       & -                      & 0.91                         \\
Zhang {\it{et al.}} \cite{129}  & 3D V-Net                & 3D V-Net              & SegTHOR (CT) \cite{33}     & 50                        & 4                         & 0.930                  & 0.785                      & 0.890                    & -          & -           & 0.916                  & -                            \\ \bottomrule
\end{tabular}%
}
\vspace{80pt}
\end{table*}

\begin{table*}[ht!]
\caption{DSC-BASED SUMMARY OF THE LITERATURE ON MULTI-ORGAN LOCALIZATION AND SEGMENTATION METHODS FOR THE HEAD AND NECK}
\label{tab_S_7}
\resizebox{\textwidth}{!}{%
\begin{tabular}{@{}lllllllllllllll@{}}
\toprule
\multirow{2}{*}{Ref}   & \multirow{2}{*}{Localization} & \multirow{2}{*}{Segmentation} & \multirow{2}{*}{Datasets}             & \multirow{2}{*}{Quantity} & \multirow{2}{*}{Category} & \multirow{2}{*}{Brainstem} & \multirow{2}{*}{Mandible} & \multicolumn{2}{l}{Parotid gland} & \multicolumn{2}{l}{Submandibular gland} & \multicolumn{2}{l}{Optic nerve} & \multirow{2}{*}{Chiasm} \\ \cmidrule(lr){9-14}
                       &                               &                               &                                       &                           &                           &                            &                           & Left            & Right           & Left               & Right              & Left           & Right          &                         \\ \midrule
Wang {\it{et al.}} \cite{137}   & 3D U-Net                      & 3D U-Net                      & HNC (CT) \cite{30}                     & 48                        & 9                         & 0.875                      & 0.930                     & 0.864           & 0.848           & 0.758              & 0.733              & 0.737          & 0.736          & 0.451                   \\
Men {\it{et al.}} \cite{138}    & 3D U-Net                      & 3D U-Net                      & TCIA (CT) \cite{56,57}          & 100                       & 7                         & 0.900                      & 0.920                     & 0.860           & 0.860           & -                  & -                  & -              & -              & -                       \\
Tang {\it{et al.}} \cite{139}   & 3D U-Net                      & 3D U-Net                      & Private (CT)                          & 215                       & 28                        & 0.863                      & 0.931                     & 0.849           & 0.849           & 0.807              & 0.825              & 0.757          & 0.761          & 0.642                   \\
Tang {\it{et al.}} \cite{139}   & 3D U-Net                      & 3D U-Net                      & PDDCA (CT) \cite{30}                   & 48                        & 9                         & 0.875                      & 0.95                      & 0.887           & 0.875           & 0.823              & 0.815              & 0.748          & 0.723          & 0.615                   \\
Yang {\it{et al.}} \cite{140}   & 3D CNN                        & 2D U-Net                      & Private (CT)                          & 88                        & 17                        & 0.831                      & 0.875                     & 0.807           & 0.811           & -                  & -                  & 0.638          & 0.675          & -                       \\
Liang {\it{et al.}} \cite{141}  & 2D CNN                        & 2D CNN                        & Private (CT)                          & 185                       & 18                        & 0.896                      & \begin{tabular}[c]{@{}l@{}}left: 0.914;\\ right: 0.912\end{tabular} & 0.852           & 0.85            & -                  & -                  & 0.661          & 0.717          & -                       \\
Gao {\it{et al.}} \cite{142}    & 3D CNN                        & 3D CNN                        & Private (CT)                          & 50                        & 18                        & 0.858                      & -                         & 0.772           & 0.800           & -                  & -                  & 0.639          & 0.617          & 0.638                   \\
Gao {\it{et al.}} \cite{142}   & 3D CNN                        & 3D CNN                        & HNC (CT) \cite{30}                     & 48                        & 9                         & 0.875                      & 0.935                     & 0.863           & 0.879           & 0.798              & 0.801              & 0.735          & 0.744          & 0.596                   \\
Liang {\it{et al.}} \cite{143}  & 2.5D CNN                      & 2.5D CNN                      & HNC (CT) \cite{30}                     & 48                        & 9                         & 0.923                      & 0.941                     & \multicolumn{2}{l}{0.876}         & \multicolumn{2}{l}{0.808}               & \multicolumn{2}{l}{0.736}       & 0.713                   \\
Liang {\it{et al.}} \cite{143} & 2.5D CNN                      & 2.5D CNN                      & Private (CT)                          & 96                        & 11                        & -                          & \begin{tabular}[c]{@{}l@{}}Left: 0.911;\\ right: 0.914\end{tabular} & 0.883           & 0.868           & -                  & -                  & 0.871          & 0.874          & -                       \\
Lei {\it{et al.}} \cite{144}   & 3D CNN                        & 3D U-Net                      & Private (CT)                          & 15                        & 8                         & -                          & 0.850                     & 0.820           & 0.810           & -                  & -                  & -              & -              & -                       \\
Huang {\it{et al.}} \cite{145}  & 3D CNN                        & 3D CNN                        & HNC (CT) \cite{30}                     & 48                        & 9                         & 0.879                      & 0.916                     & 0.884           & 0.878           & 0.801              & 0.776              & 0.677          & 0.706          & 0.643                   \\
Huang {\it{et al.}} \cite{145} & 3D CNN                        & 3D CNN                        & StructSeg (CT) \cite{1}                & 15                        & 7                         & 0.769                      & 0.807                     & 0.802           & 0.802           & -                  & -                  & 0.499          & 0.534          & 0.211                   \\
Huang {\it{et al.}} \cite{145} & 3D CNN                        & 3D CNN                        & Private (CT)                          & 15                        & 9                         & 0.957                      & 0.848                     & 0.962           & 0.946           & 0.846              & 0.808              & 0.824          & 0.843          & 0.434                   \\
Korte {\it{et al.}} \cite{146}  & 3D U-Net                      & 3D U-Net                      & \begin{tabular}[c]{@{}l@{}}Public RT-MAC dataset\\ (MRI) \cite{147}\end{tabular} & 31                        & 8                         & -                          & -                         & 0.860           & 0.857           & 0.830              & 0.785              & -              & -              & -                       \\
Korte {\it{et al.}} \cite{146} & 3D U-Net                      & 3D U-Net                      & Private (MRI)                         & 10                        & 8                         & -                          & -                         & 0.730           & 0.775           & 0.537              & 0.435              & -              & -              & -                       \\
Gao {\it{et al.}} \cite{108}    & 3D CNN                        & 3D CNN                        & Private (CT)                          & 1164                      & 22                        & 0.891                      & \begin{tabular}[c]{@{}l@{}}Left: 0.924;\\ right: 0.925\end{tabular} & 0.846           & 0.87            & -                  & -                  & 0.713          & 0.753          & 0.612                   \\
Gao {\it{et al.}} \cite{108}   & 3D CNN                        & 3D CNN                        & HNC (CT) \cite{30}                     & 48                        & 9                         & 0.882                      & 0.947                     & 0.898           & 0.881           & 0.840              & 0.838              & 0.790          & 0.817          & 0.713                   \\ \bottomrule
\end{tabular}%
}
\end{table*}

\begin{table*}[ht!]
\caption{DSC-Based Summary of the Literature on Multi-Organ Localization and Segmentation Methods for the Abdomen}
\label{tab_S_8}
\resizebox{\textwidth}{!}{%
\begin{tabular}{@{}lllllllllllll@{}}
\toprule
\multirow{2}{*}{Ref}    & \multirow{2}{*}{Localization} & \multirow{2}{*}{Segmentation} & \multirow{2}{*}{Dataset}                                   & \multirow{2}{*}{Quantity} & \multirow{2}{*}{Category} & \multirow{2}{*}{Liver} & \multirow{2}{*}{Spleen} & \multicolumn{2}{l}{Kidney} & \multirow{2}{*}{Pancreas} & \multirow{2}{*}{Gallbladder} & \multirow{2}{*}{Stomach} \\ \cmidrule(lr){9-10}
                        &                               &                               &                                                            &                           &                           &                        &                         & Left         & Right       &                           &                              &                          \\ \midrule
Larsson {\it{et al.}} \cite{148} & Multi-Atlas                   & 3D FCN                        & BTCV (CT) \cite{29}                                         & 30                        & 13                        & 0.949                  & 0.936                   & 0.911        & 0.897       & 0.646                     & 0.613                        & 0.764                    \\
Zhao {\it{et al.}} \cite{149}    & Registration                  & 2D U-Net                      & \begin{tabular}[c]{@{}l@{}}VISCERAL challenge dataset Nonenhanced\\ CT (CTwb) \cite{150}\end{tabular} & 20                        & 4                         & -                      & -                       & -            & -           & 0.583                     & 0.473                        & -                        \\
Zhao {\it{et al.}} \cite{149}   & Registration                  & 2D U-Net                      & \begin{tabular}[c]{@{}l@{}}VISCERAL challenge dataset enhanced\\ CT (CTce) \cite{150}\end{tabular}    & 20                        & 4                         & -                      & -                       & -            & -           & 0.588                     & 0.624                        & -                        \\ \bottomrule
\end{tabular}%
}
\end{table*}

\begin{table*}[ht!]
\caption{DSC-Based Summary of the Literature on Multi-Organ Localization and Segmentation Methods for the Thorax}
\label{tab_S_9}
\resizebox{\textwidth}{!}{%
\begin{tabular}{@{}lllllllllllll@{}}
\toprule
\multirow{2}{*}{Ref}     & \multirow{2}{*}{Localization} & \multirow{2}{*}{Segmentation} & \multirow{2}{*}{Dataset} & \multirow{2}{*}{Quantity} & \multirow{2}{*}{Category} & \multirow{2}{*}{Heart} & \multirow{2}{*}{Esophagus} & \multirow{2}{*}{Trachea} & \multicolumn{2}{l}{Lung} & \multirow{2}{*}{Aorta} & \multirow{2}{*}{Spinal cord} \\ \cmidrule(lr){10-11}
                         &                               &                               &                          &                           &                           &                        &                            &                          & Left        & Right      &                        &                              \\ \midrule
Francis {\it{et al.}} \cite{136} & 3D U-Net                      & Two 3D U-Net                  & AAPM (CT) \cite{31}       & 60                        & 5                         & 0.941                  & 0.738                      & -                        & 0.979       & 0.973      & -                      & 0.899                        \\
Feng {\it{et al.}} \cite{151}    & 3D U-Net                      & 3D U-Net                      & AAPM (CT) \cite{31}       & 60                        & 5                         & 0.925                  & 0.726                      & -                        & 0.979       & 0.972      & -                      & 0.893                        \\
Feng {\it{et al.}} \cite{151}    & 3D U-Net                      & 3D U-Net                      & Private (CT)             & 30                        & 5                         & 0.86                   & 0.685                      & -                        & 0.976       & 0.977      & -                      & 0.852                        \\ \bottomrule
\end{tabular}%
}
\end{table*}

\begin{table*}[ht!]
\caption{DSC-Based Summary of the Literature on Multi-Organ Single-State Segmentation Methods for the Head and Neck-Supplementary Material}
\label{tab_S_10}
\resizebox{\textwidth}{!}{%
\begin{tabular}{@{}lllllm{0.6\textwidth}@{}}
\toprule
\multirow{2}{*}{Ref}        & \multirow{2}{*}{Backbone} & \multirow{2}{*}{Datasets} & \multirow{2}{*}{Quantity} & \multirow{2}{*}{Organ type} & \multirow{2}{*}{Other organs}                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             \\
                            &                           &                           &                           &                             &                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           \\ \midrule
Ibragimov and Xing \cite{37} & 2.5D CNN                  & Private (CT)              & 50                        & 13                          & Pharynx: 0.856; Left eyeball: 0.884; Right eyeball: 0.877; Spinal cord: 0.870; Larynx: 0.693                                                                                                                                                                                                                                                                                                                                                                                                                                              \\
Van Rooij {\it{et al.}} \cite{40}   & 3D U-Net                  & Private (CT)              & 157                       & 11                          & Larynx: 0.780; Pharyngeal Constrictor: 0.680; Cricopharynx: 0.730; Upper esophageal sphincter: 0.810; esophagus: 0.600; Oral Cavity: 0.780                                                                                                                                                                                                                                                                                                                                                                                                \\
Tong {\it{et al.}} \cite{41}        & 3D GAN                    & Private (MRI)             & 25                        & 9                           & Pharynx: 0.706; Larynx: 0.799                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             \\
Liu {\it{et al.}} \cite{43}         & 3D U-Net                  & Private (MRI \& CT)       & 45                        & 19                          & Pharynx: 0.740; spinal cord: 0.840; left cochlea: 0.760; right cochlea: 0.750; esophagus: 0.850; oral cavity: 0.900; left eye: 0.890; right eye: 0.870; left lens: 0.730; right lens: 0.730; larynx: 0.900; brain: 0.950                                                                                                                                                                                                                                                                                                                  \\
Chen {\it{et al.}} \cite{44}        & 2.5D U-Net                & Private (CT)              & 307                       & 24                          & Pituitary: 0.756; left middle ear: 0.869; right middle ear: 0.859; left lens: 0.844; right lens: 0.839; left temporomandibular joint: 0.838; right temporomandibular joint: 0.829                                                                                                                                                                                                                                                                                                                                                         \\
Liu {\it{et al.}} \cite{45}         & 2D U-Net                  & StructSeg (CT)            & 50                        & 22                          & Left eye: 0.858; right eye: 0.882; spinal cord: 0.804; pituitary: 0.503; left middle ear: 0.825; right middle ear: 0.717; left lens: 0.898; right lens: 0.786; left temporomandibular joint: 0.723; right temporomandibular joint: 0.824                                                                                                                                                                                                                                                                                                  \\
Cros {\it{et al.}} \cite{46}        & 3D U-Net                  & Private (CT)              & 200                       & 12                          & Medullary canal: 0.870; Outer medullary canal: 0.860; oral cavity: 0.660; esophagus: 0.600; trachea: 0.670; trunk: 0.670; outer trunk: 0.700; inner ears: 0.710; eyes: 0.770; sub-maxillary glands: 0.740                                                                                                                                                                                                                                                                                                                                 \\
Lei {\it{et al.}} \cite{47}         & 3D U-Net                  & StructSeg (CT)            & 50                        & 22                          & Left eye: 0.886; right eye: 0.873; spinal cord: 0.830; pituitary: 0.661; left middle ear: 0.826; right middle ear: 0.783; left lens: 0.815; right lens: 0.754; left temporomandibular joint: 0.757; right temporomandibular joint: 0.772                                                                                                                                                                                                                                                                                                  \\
Srivastava {\it{et al.}} \cite{49}  & 3D U-Net                  & OpenKBP (CT) \cite{36}     & 188                       & 5                           & Spinal cord: 0.740                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        \\
Podobnik {\it{et al.}} \cite{50}    & 2D nnU-net                & Private (CT \& MRI)         & 56                        & 31                          & Spinal cord: 0.812; Pharyngeal constrictor muscles: 0.617; oral cavity: 0.845; Larynx-supraglottis: 0.728; Larynx-glottis:0.615; Lips:0.728; Thyroid: 0.721; pituitary gland: 0.658; Lacrimal glands (left): 0.621; Lacrimal glands (right): 0.636; left eye: 0.887; right eye: 0.884; left lens: 0.723; right lens: 0.763; Cervical esophagus: 0.559; Cricopharyngeal inlet: 0.517; Cochleae (left):0.558; Cochleae (right):0.514; Carotid arteries (left):0.624; Carotid arteries (right):0.618; Buccal mucosa: 0.661; Arytenoids:0.474 \\
Kan {\it{et al.}} \cite{51}         & 3D Transformer and U-Net  & Private (CT)              & 94                        & 18                          & Spinal cord: 0.897; pituitary gland: 0.608; oral cavity: 0.908; left eye: 0.907; right eye: 0.902; left lens: 0.724; right lens: 0.689; left TMJ: 0.789; right TMJ: 0.778; left temporal lobe: 0.803; right temporal lobe: 0.802                                                                                                                                                                                                                                                                                                          \\
Isler {\it{et al.}} \cite{54}       & 2D U-Net                  & OpenKBP (CT) \cite{36}     & 188                       & 5                           & Spinal cord: 0.750                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        \\ \bottomrule
\end{tabular}%
}
\end{table*}

\begin{table*}[ht!]
\caption{DSC-Based Summary of the Literature on Multi-Organ Single-State Segmentation Methods for the Abdomen-Supplementary Material}
\label{tab_S_11}
\resizebox{\textwidth}{!}{%
\begin{tabular}{@{}lllllm{0.5\textwidth}@{}}
\toprule
\multirow{2}{*}{Ref}        & \multirow{2}{*}{Network} & \multirow{2}{*}{Datasets}                    & \multirow{2}{*}{Quantity} & \multirow{2}{*}{Organ type} & \multirow{2}{*}{Other organs}                                                                                                                                                      \\
                            &                          &                                              &                           &                             &                                                                                                                                                                                    \\ \midrule
Gibson {\it{et al.}} \cite{55}     & 3D CNN                   & TCIA \cite{56,57} \&BTCV \cite{29} (CT) & 72                        & 4                           & Esophagus: 0.730                                                                                                                                                                   \\
Men {\it{et al.}} \cite{58}         & 2D CNN                   & Private (CT)                                 & 278                       & 5                           & Bladder: 0.934; Intestine: 0.653; Left femoral head: 0.921; Right femoral head: 0.923; Colon: 0.618                                                                                \\
Shen {\it{et al.}} \cite{59}        & 3D U-Net                 & Private (CT)                                 & 377                       & 7                           & Artery: 0.892; Vein: 0.793                                                                                                                                                         \\
Gibson {\it{et al.}} \cite{60}      & 3D V-Net                 & TCIA \cite{56,57} \&BTCV \cite{29} (CT) & 90                        & 8                           & Duodenum 0.630; Esophagus: 0.760                                                                                                                                                   \\
Roth {\it{et al.}} \cite{61}        & 3D U-Net                 & Private (CT)                                 & 377                       & 7                           & Artery: 0.835; Vein 0.805                                                                                                                                                          \\
Cai {\it{et al.}} \cite{62}         & 2D FCN                   & Private (CT)                                 & 120                       & 16                          & Aorta: 0.810; Adrenal gland: 0.368; Celiac AA: 0.385; Duodenum:0.649; Colon: 0.776; Inferior vena cava: 0.786; Superior mesenteric artery: 0.496; Small bowel: 0.729; Veins: 0.651 \\
Heinrich {\it{et al.}} \cite{63}    & 3D U-Net                 & TCIA (CT) \cite{56,57}                 & 43                        & 8                           & Left adrenal gland: 0.942; duodenum: 0.538; Esophagus: 0.633                                                                                                                       \\
                            &                          & Private (CT)                                 & 10                        & 7                           & Spleen, pancreas, kidney, gallbladder, esophagus, liver, stomach and duodenum average DSC: 0.823                                                                                   \\
Hatamizadeh {\it{et al.}} \cite{66} & 3D Transformer And U-Net & BTCV (CT) \cite{29}                           & 30                        & 13                          & Esophagus: 0.864; aorta: 0.948; Inferior vena cava: 0.890; vein: 0.858                                                                                                             \\
Chen {\it{et al.}} \cite{67}        & 2.5D U-Net               & Private (MR)                                 & 102                       & 10                          & Duodenum: 0.801; Small Intestine: 0.870; Spinal Cord: 0.904; Vertebral Body: 0.900                                                                                                 \\
Tang {\it{et al.}} \cite{68}        & 2.5D U-Net               & ABD-110 (CT) \cite{68}                        & 110                       & 11                          & Large intestine: 0.825; small intestine 0.765; duodenum 0.707; spinal cord 0.908                                                                                                   \\
Jia and Wei \cite{69}        & 3D U-Net                 & CHAOS (CT) \cite{32}                          & 20                        & 4                           & -                                                                                                                                                                                  \\
Lin {\it{et al.}} \cite{70}         & 3D U-Net                 & TCIA \cite{56,57} \& BTCV(CT) \cite{29} & 90                        & 8                           & Duodenum 0.637; esophagus 0.733                                                                                                                                                    \\
Cao {\it{et al.}} \cite{71}         & 2D Transformer           & Synapse (CT) \cite{2}                         & 30                        & 8                           & Aorta 0.855                                                                                                                                                                        \\
Chen {\it{et al.}} \cite{72}        & 2D Transformer And U-Net & Synapse (CT) \cite{2}                         & 30                        & 8                           & Aorta 0.872                                                                                                                                                                        \\
Song {\it{et al.}} \cite{73}        & 2D CNN                   & Synapse (CT) \cite{2}                         & 30                        & 8                           & Aorta 0.903                                                                                                                                                                        \\
Huang {\it{et al.}} \cite{75}       & 2D Transformer           & Synapse (CT) \cite{2}                         & 30                        & 8                           & Aorta: 0.870                                                                                                                                                                       \\
Suo {\it{et al.}} \cite{76}         & 2D Transformer And U-Net & Synapse (CT) \cite{2}                         & 30                        & 8                           & Aorta: 0.881                                                                                                                                                                       \\
Berzoini {\it{et al.}} \cite{78}    & 2D U-Net                 & Open-source CT-org dataset \cite{79}          & 140                       & 6                           & Lung: 0.967; bladder: 0.836; bone: 0.944                                                                                                                                           \\
Shen {\it{et al.}} \cite{80}        & 2D U-Net                 & TCIA (CT) \cite{56,57}                 & 42                        & 5                           & Duodenum: 0.615                                                                                                                                                                    \\
Xie {\it{et al.}} \cite{82}         & 3D CNN And Transformer   & BTCV (CT) \cite{29}                           & 30                        & 11                          & Esophagus: 0.780; aorta: 0.912; Inferior vena cava: 0.880; Portal vein and splenic vein: 0.781;                                                                                    \\
Srivastava {\it{et al.}} \cite{49}  & 3D U-Net                 & Synapse (CT) \cite{2}                         & 30                        & 8                           & Aorta: 0.909                                                                                                                                                                       \\
Jiang {\it{et al.}} \cite{52}       & 2D U-Net                 & BTCV (CT) \cite{29}                           & 30                        & 12                          & Esophagus: 0.807; aorta: 0.913; Inferior vena cava: 0.850; Portal vein and splenic vein: 0.809; adrenal gland: 0.691                                                               \\ \bottomrule
\end{tabular}%
}
\end{table*}

\begin{table*}[ht!]
\caption{DSC-Based Summary of the Literature on Multi-Organ Coarse-to-Fine Segmentation Methods for the Head and Neck-Supplementary Material}
\label{tab_S_12}
\resizebox{\textwidth}{!}{%
\begin{tabular}{@{}llllllm{0.6\textwidth}@{}}
\toprule
\multirow{2}{*}{Ref} & \multirow{2}{*}{Coarse} & \multirow{2}{*}{Fine} & \multirow{2}{*}{Datasets} & \multirow{2}{*}{Quantity} & \multirow{2}{*}{Organ type} & \multirow{2}{*}{Other organs}                                                                                                                         \\
                     &                         &                       &                           &                           &                             &                                                                                                                                                       \\ \midrule
Fang {\it{et al.}} \cite{109} & 2D FCN                  & 3D U-Net              & Private (CT)              & 56                        & 14                          & Right eyeball: 0.634; Left eyeball: 0.636; Lips: 0.676; Oral Cavity: 0.829; throat: 0.389; Esophagus: 0.735; Thyroid gland: 0.642; spinal cord: 0.782 \\ \bottomrule
\end{tabular}%
}
\end{table*}

\begin{table*}[ht!]
\caption{DSC-Based Summary of the Literature on Multi-Organ Coarse-to-Fine Segmentation Methods for the Abdomen}
\label{tab_S_13}
\resizebox{\textwidth}{!}{%
\begin{tabular}{@{}llllllm{0.6\textwidth}@{}}
\toprule
\multirow{2}{*}{Ref} & \multirow{2}{*}{Coarse network} & \multirow{2}{*}{Fine network} & \multirow{2}{*}{Dataset} & \multirow{2}{*}{Quantity} & \multirow{2}{*}{Category} & \multirow{2}{*}{Other organs}                                                                                                                                              \\
                     &                                 &                               &                          &                           &                           &                                                                                                                                                                            \\ \midrule
Roth \cite{144}              & 3D FCN                          & 3D FCN                        & Private (CT)             & 331                       & 3                         & Artery: 0.796; vein: 0.731                                                                                                                                                 \\
Wang \cite{145}              & 2.5D FCN                        & 2.5D FCN                      & Private (CT)             & 236                       & 13                        & Aorta: 0.918; colon: 0.830; duodenum: 0.754; Inferior vena cava: 0.870; small intestine: 0.801; vein: 0.807                                                                \\
Zhang \cite{128}             & 3D V-Net                        & 3D V-Net                      & BTCV (CT) \cite{29}              & 30                        & 13                        & Esophagus: 0.691; aorta: 0.877; Inferior vena cava: 0.865; Portal vein and splenic vein: 0.688; right adrenal gland: 0.651; left adrenal gland: 0.619                      \\
Xie \cite{129}               & 2.5D FCN                        & 2.5D FCN                      & Private (CT)             & 200                       & 16                        & Aorta: 0.937; adrenal gland: 0.630; abdominal cavity: 0.620; duodenum: 0.735; Inferior vena cava: 0.837; Vascular: 0.742; small intestine: 0.751; vein: 0.748; Colon: 0.800 \\
Lee \cite{127}               & 3D U-Net                        & 3D U-Net                      & BTCV (CT) \cite{29}              & 47                        & 8                         & Esophagus: 0.783; aorta: 0.916; Inferior vena cava: 0.856; Portal vein and splenic vein: 0.762; RAD: 0.741; LAD: 0.746                                                     \\
Kakeya \cite{148}            & 3D U-Net                        & 3D U-Net                      & Private (CT)             & 47                        & 8                         & Inferior vena cava: 0.908; Aorta: 0.969                                                                                                                                    \\ \bottomrule
\end{tabular}%
}
\vspace{100pt}
\end{table*}

\begin{table*}[ht!]
\caption{DSC-Based Summary of the Literature on Multi-Organ Localization and Segmentation Methods for the Head and Neck-Supplementary Material}
\label{tab_S_14}
\resizebox{\textwidth}{!}{%
\begin{tabular}{@{}llllllm{0.6\textwidth}@{}}
\toprule
\multirow{2}{*}{Ref}                    & \multirow{2}{*}{Localization} & \multirow{2}{*}{Segmentation} & \multirow{2}{*}{Dataset}              & \multirow{2}{*}{Quantity} & \multirow{2}{*}{Category} & \multirow{2}{*}{Other organs}                                                                                                                                                                                                                                                                                                                                                                                                              \\
                                        &                               &                               &                                       &                           &                           &                                                                                                                                                                                                                                                                                                                                                                                                                                            \\ \midrule
Men {\it{et al.}} \cite{138}                    & 3D U-Net                      & 3D U-Net                      & TCIA (CT) \cite{56,57}          & 100                       & 7                         & Spinal cord: 0.910; Left eye: 0.930; Right eye: 0.920                                                                                                                                                                                                                                                                                                                                                                                      \\
Tang {\it{et al.}} \cite{139}                   & 3D U-Net                      & 3D U-Net                      & Private (CT)                          & 215                       & 28                        & Brachial plexus: 0.562; pharyngeal constrictor: 0.755; left ear: 0.773; right ear: 0.786; left eye: 0.925; right eye: 0.925; pituitary gland: 0.639; larynx: 0.893; left lens: 0.819; right lens: 0.830; oral cavity: 0.908; spinal cord: 0.856; sublingual gland: 0.460; left temporal lobe: 0.848; right temporal lobe: 0.841; thyroid: 0.856; left temporomandibular joint: 0.880; right temporomandibular joint: 0.869; trachea: 0.813 \\
Yang {\it{et al.}} \cite{140}                   & 3D CNN                        & 2D U-Net                      & Private (CT)                          & 88                        & 17                        & Left eye: 0.875; right eye: 0.889; left lens: 0.747; right lens: 0.698; cerebellum: 0.936; pituitary: 0.672; thyroid: 0.844; Temporal lobe left: 0.762; Temporal lobe right: 0.784; brain: 0.976; head: 0.943                                                                                                                                                                                                                              \\
Liang {\it{et al.}} \cite{141}                  & 2D CNN                        & 2D CNN                        & Private (CT)                          & 185                       & 18                        & Left eye: 0.932; right eye: 0.936; left lens: 0.930; right lens: 0.842; larynx: 0.870; oral cavity: 0.928; left mastoid: 0.821; right mastoid: 0.824; spinal cord: 0.884; left TMJ: 0.846; right TMJ: 0.844;                                                                                                                                                                                                                               \\
\multirow{2}{*}{Gao {\it{et al.}} \cite{142}}   & \multirow{2}{*}{3D CNN}       & \multirow{2}{*}{3D CNN}       & Private (CT)                          & 50                        & 18                        & Left eye: 0.876; right eye: 0.912; oral cavity: 0.792; larynx: 0.658; spinal cord: 0.874; left lens: 0.808; right lens: 0.790; pituitary gland: 0.769; left middle ear: 0.567; right middle ear: 0.522; left TMJ: 0.584; right TMJ: 0.572                                                                                                                                                                                                  \\
                                        &                               &                               & Private (CT)                          & 96                        & 11                        & Left eye: 0.930; right eye: 0.930; spinal cord: 0.900; left lens: 0.872; right lens: 0.883;                                                                                                                                                                                                                                                                                                                                                \\
Lei {\it{et al.}} \cite{144}                    & 3D CNN                        & 3D U-Net                      & Private (CT)                          & 15                        & 8                         & Esophagus: 0.840; Throat: 0.790; Oral: 0.890; Pharynx: 0.850; spinal cord: 0.890                                                                                                                                                                                                                                                                                                                                                           \\
\multirow{2}{*}{Korte {\it{et al.}} \cite{146}} & \multirow{2}{*}{3D U-Net}     & \multirow{2}{*}{3D U-Net}     & \begin{tabular}[c]{@{}l@{}}Public RT-MAC dataset\\ (MRI) \cite{147}\end{tabular} & 43                        & 8                         & Secondary lymph nodes (left): 0.708; secondary lymph nodes (right): 0.715; tertiary lymph nodes (left): 0.561; tertiary lymph nodes (right): 0.573;                                                                                                                                                                                                                                                                                        \\
                                        &                               &                               & Private (MRI)                         & 10                        & 8                         & Secondary lymph nodes (left): 0.553; Secondary lymph nodes (right): 0.525; Tertiary lymph nodes (left): 0.304; Tertiary lymph nodes (right): 0.189;                                                                                                                                                                                                                                                                                        \\
Gao {\it{et al.}} \cite{108}                    & 3D CNN                        & 3D CNN                        & Private (CT)                          & 1164                      & 22                        & Left eye: 0.897; right eye: 0.895; left lens: 0.819; right lens: 0.825; pituitary gland: 0.722; left temporal lobe: 0.877; right temporal lobe: 0.883; spinal cord: 0.831; left inner ear: 0.864; right inner ear: 0.855; left middle ear: 0.857; right middle ear: 0.843; left temporomandibular joint: 0.764; right temporomandibular joint: 0.789;                                                                                      \\ \bottomrule
\end{tabular}%
}
\end{table*}

\begin{table*}[ht!]
\caption{DSC-Based Summary of the Literature on Multi-Organ Localization and Segmentation Methods for the Abdomen-Supplementary Material}
\label{tab_S_15}
\resizebox{\textwidth}{!}{%
\begin{tabular}{@{}llllllm{0.5\textwidth}@{}}
\toprule
Ref                                    & Localization                  & Segmentation              & Dataset                                                    & Quantity & Category & Other organs                                                                                                                                          \\ \midrule
Larsson {\it{et al.}} \cite{148}               & Multi-Atlas                   & 3D FCN                    & BTCV (CT) \cite{29}                                         & 30       & 13       & Esophagus: 0.588; aorta: 0.870; Inferior vena cava: 0.758; Portal vein and splenic vein: 0.715; Right adrenal gland: 0.630; Left adrenal gland: 0.631 \\
\multirow{2}{*}{Zhao {\it{et al.}} \cite{149}} & \multirow{2}{*}{Registration} & \multirow{2}{*}{2D U-Net} & \begin{tabular}[c]{@{}l@{}}VISCERAL challenge dataset Nonenhanced\\ CT (CTwb) \cite{150}\end{tabular} & 20       & 4        & Left adrenal gland: 0.472; Right adrenal gland: 0.390                                                                                                 \\
                                       &                               &                           & \begin{tabular}[c]{@{}l@{}}VISCERAL challenge dataset enhanced\\ CT (CTce) \cite{150}\end{tabular}    & 20       & 4        & Left adrenal gland: 0.403; Right adrenal gland: 0.434                                                                                                 \\ \bottomrule
\end{tabular}%
}
\vspace{300pt}
\end{table*}


}




% \vfill

\end{document}


