\newpage

\begin{table}
    \caption{Parameter settings for \alg on different datasets. Apart from the hyperparameters mentioned in the table below, all other hyperparameters were held constant across datasets. All ReLU layers were followed by a dropout layer with 50\% drop-rate in Module-I and 20\% for the rest of the modules. Learning rate was decayed by a decay factor of 0.5 after interval $0.5\times$ epoch length. Batch size was taken to be 255 for all datasets. Module I used 20 epochs with initial learning rate of 0.01. In Module II, 10 epochs were used with an initial learning rate of 0.008 for all datasets.}
	\label{tab:hyperparameters}
	\centering
	%\resizebox{\linewidth}{!}{
	\begin{tabular}{l|ccc}
            \toprule
            \textbf{Dataset} &  \textbf{\begin{tabular}[c]{@{}c@{}}Beam\\ Size\end{tabular}} & \textbf{\begin{tabular}[c]{@{}c@{}}Embedding\\ Dimension\end{tabular}} & \textbf{\begin{tabular}[c]{@{}c@{}}Cluster\\ Size\end{tabular}}\\ 
            \midrule
            LF-AmazonTitles-131K &  200 & 300 & $2^{15}$\\
            LF-WikiSeeAlsoTitles-320K & 160 & 300 & $2^{17}$\\
            LF-AmazonTitles-1.3M & 100 & 512 & $2^{17}$\\
            LF-Amazon-131K &  200 & 512 & $2^{15}$\\
            LF-WikiSeeAlso-320K & 160 & 512 & $2^{17}$\\
            \midrule
            LF-P2PTitles-300K & 160 & 300 & $2^{17}$\\
            LF-P2PTitles-2M & 40 & 512 & $2^{17}$\\
            \midrule
            LF-WikiTitles-500K & 100 & 512 & $2^{17}$\\
            LF-Wikipedia-500K & 100 & 512 & $2^{17}$\\
            \bottomrule
        \end{tabular}
    %}
\end{table}

\textbf{Please go to the next page for detailed experimental results.}
