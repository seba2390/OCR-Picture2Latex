\begin{table*}
\caption{A comparison of \alg on publicly available product-to-product datasets. The first 3 rows are short-text datasets whereas the last two rows are long-text versions of the first two. \alg offers predictions that are the most accurate based on all evaluation metrics, and an order of magnitude faster as compared to existing deep learning based approaches. Methods marked with a `-' sign could not be scaled for the given dataset within the available resources.}
    \label{tab:sup:prod}
    \centering
    \resizebox{\textwidth}{!}{
    \begin{tabular}{@{}c|l|ccccc|ccccc|ccc@{}}
    \toprule
    \textbf{Dataset} & \textbf{Method} & \textbf{P@1} & \textbf{P@3} & \textbf{P@5} & \textbf{N@3} & \textbf{N@5} &  \textbf{PSP@1} & \textbf{PSP@3}& \textbf{PSP@5} & \textbf{PSN@3} & \textbf{PSN@5}
    & \multicolumn{1}{c}{\begin{tabular}[c]{@{}c@{}}\textbf{Model}\\ \textbf{Size (GB)}\end{tabular}}
    & \multicolumn{1}{c}{\begin{tabular}[c]{@{}c@{}}\textbf{Training}\\ \textbf{Time (hr)}\end{tabular}}
    & \multicolumn{1}{c}{\begin{tabular}[c]{@{}c@{}}\textbf{Prediction}\\ \textbf{Time (ms)}\end{tabular}} \\
    \midrule
  \multirow{13}{*}{\textbf{\rotatebox{90}{LF-AmazonTitles-131K}}}	& \alg	 & \textbf{38.4}	 & \textbf{25.84}	 & \textbf{18.65}	 & \textbf{39.43}	 & \textbf{41.46}	 & \textbf{30.85}	 & \textbf{36.44}	 & \textbf{41.42}	 & \textbf{34.69}	 & \textbf{37.13}	 & 0.81	 & 2.16	 & 0.1\\
	& Astec	 & 37.12	 & 25.2	 & 18.24	 & 38.17	 & 40.16	 & 29.22	 & 34.64	 & 39.49	 & 32.73	 & 35.03	 & 3.24	 & 1.83	 & 2.34\\
	& AttentionXML	 & 32.25	 & 21.7	 & 15.61	 & 32.83	 & 34.42	 & 23.97	 & 28.6	 & 32.57	 & 26.88	 & 28.75	 & 2.61	 & 20.73	 & 5.19\\
	& MACH	 & 33.49	 & 22.71	 & 16.45	 & 34.36	 & 36.16	 & 24.97	 & 30.23	 & 34.72	 & 28.41	 & 30.54	 & 2.35	 & 3.3	 & 0.23\\
	& X-Transformer	 & 29.95	 & 18.73	 & 13.07	 & 28.75	 & 29.6	 & 21.72	 & 24.42	 & 27.09	 & 23.18	 & 24.39	 & -	 & -	 & 15.38\\
	& Siamese	 & 13.81	 & 8.53	 & 5.81	 & 13.32	 & 13.64	 & 13.3	 & 12.68	 & 13.36	 & 12.69	 & 13.06	 & 0.6	 & 6.92	 & 0.2\\
	& Parabel	 & 32.6	 & 21.8	 & 15.61	 & 32.96	 & 34.47	 & 23.27	 & 28.21	 & 32.14	 & 26.36	 & 28.21	 & 0.34	 & 0.03	 & 0.69\\
	& Bonsai	 & 34.11	 & 23.06	 & 16.63	 & 34.81	 & 36.57	 & 24.75	 & 30.35	 & 34.86	 & 28.32	 & 30.47	 & 0.24	 & 0.1	 & 7.49\\
	& DiSMEC	 & 35.14	 & 23.88	 & 17.24	 & 36.17	 & 38.06	 & 25.86	 & 32.11	 & 36.97	 & 30.09	 & 32.47	 & 0.11	 & 3.1	 & 5.53\\
	& PfastreXML	 & 32.56	 & 22.25	 & 16.05	 & 33.62	 & 35.26	 & 26.81	 & 30.61	 & 34.24	 & 29.02	 & 30.67	 & 3.02	 & 0.26	 & 2.32\\
	& XT	 & 31.41	 & 21.39	 & 15.48	 & 32.17	 & 33.86	 & 22.37	 & 27.51	 & 31.64	 & 25.58	 & 27.52	 & 0.84	 & 9.46	 & 9.12\\
	& Slice	 & 30.43	 & 20.5	 & 14.84	 & 31.07	 & 32.76	 & 23.08	 & 27.74	 & 31.89	 & 26.11	 & 28.13	 & 0.39	 & 0.08	 & 1.58\\
	& AnneXML	 & 30.05	 & 21.25	 & 16.02	 & 31.58	 & 34.05	 & 19.23	 & 26.09	 & 32.26	 & 23.64	 & 26.6	 & 1.95	 & 0.08	 & 0.11\\
	 \midrule
	 \multirow{13}{*}{\textbf{\rotatebox{90}{LF-WikiSeeAlsoTitles-320K}}}	& \alg	 & \textbf{25.14}	 & \textbf{16.9}	 & \textbf{12.86}	 & \textbf{24.99}	 & \textbf{25.95}	 & \textbf{16.73}	 & \textbf{18.99}	 & \textbf{21.01}	 & \textbf{19.18}	 & \textbf{20.75}	 & 1.76	 & 11.16	 & 0.09\\
	& Astec	 & 22.72	 & 15.12	 & 11.43	 & 22.16	 & 22.87	 & 13.69	 & 15.81	 & 17.5	 & 15.56	 & 16.75	 & 7.3	 & 4.17	 & 2.67\\
	& AttentionXML	 & 17.56	 & 11.34	 & 8.52	 & 16.58	 & 17.07	 & 9.45	 & 10.63	 & 11.73	 & 10.45	 & 11.24	 & 6.02	 & 56.12	 & 7.08\\
	& MACH	 & 18.06	 & 11.91	 & 8.99	 & 17.57	 & 18.17	 & 9.68	 & 11.28	 & 12.53	 & 11.19	 & 12.14	 & 2.51	 & 8.23	 & 0.52\\
	& X-Transformer	 & -	 & -	 & -	 & -	 & -	 & -	 & -	 & -	 & -	 & -	 & -	 & -	 & -\\
	& Siamese	 & 10.69	 & 6.28	 & 4.51	 & 9.79	 & 9.91	 & 10.1	 & 9.43	 & 9.59	 & 10.22	 & 10.47	 & 0.67	 & 11.58	 & 0.17\\
	& Parabel	 & 17.68	 & 11.48	 & 8.59	 & 16.96	 & 17.44	 & 9.24	 & 10.65	 & 11.8	 & 10.49	 & 11.32	 & 0.6	 & 0.07	 & 0.8\\
	& Bonsai	 & 19.31	 & 12.71	 & 9.55	 & 18.74	 & 19.32	 & 10.69	 & 12.44	 & 13.79	 & 12.29	 & 13.29	 & 0.37	 & 0.37	 & 14.82\\
	& DiSMEC	 & 19.12	 & 12.93	 & 9.87	 & 18.93	 & 19.71	 & 10.56	 & 13.01	 & 14.82	 & 12.7	 & 14.02	 & 0.19	 & 15.56	 & 11.02\\
	& PfastreXML	 & 17.1	 & 11.13	 & 8.35	 & 16.8	 & 17.35	 & 12.15	 & 12.51	 & 13.26	 & 12.81	 & 13.48	 & 6.77	 & 0.59	 & 2.59\\
	& XT	 & 17.04	 & 11.31	 & 8.6	 & 16.61	 & 17.24	 & 8.99	 & 10.52	 & 11.82	 & 10.33	 & 11.26	 & -	 & 5.28	 & 12.86\\
	& Slice	 & 18.55	 & 12.62	 & 9.68	 & 18.29	 & 19.07	 & 11.24	 & 13.45	 & 15.2	 & 13.03	 & 14.23	 & 0.94	 & 0.2	 & 1.85\\
	& AnneXML	 & 16.3	 & 11.24	 & 8.84	 & 16.19	 & 17.14	 & 7.24	 & 9.63	 & 11.75	 & 9.06	 & 10.43	 & 4.22	 & 0.21	 & 0.13\\
	 \midrule
	\multirow{11}{*}{\textbf{\rotatebox{90}{LF-AmazonTitles-1.3M}}}	& \alg	 & \textbf{50.67}	 & \textbf{44.49}	 & \textbf{40.35}	 & \textbf{48.05}	 & \textbf{46.85}	 & 22.07	 & 26.54	 & 29.3	 & 25.06	 & 26.85	 & 9.62	 & 74.47	 & 0.16\\
	& Astec	 & 48.82	 & 42.62	 & 38.44	 & 46.11	 & 44.8	 & 21.47	 & 25.41	 & 27.86	 & 24.08	 & 25.66	 & 26.66	 & 18.54	 & 2.61\\
	& AttentionXML	 & 45.04	 & 39.71	 & 36.25	 & 42.42	 & 41.23	 & 15.97	 & 19.9	 & 22.54	 & 18.23	 & 19.6	 & 28.84	 & 380.02	 & 29.53\\
	& MACH	 & 35.68	 & 31.22	 & 28.35	 & 33.42	 & 32.27	 & 9.32	 & 11.65	 & 13.26	 & 10.79	 & 11.65	 & 7.68	 & 60.39	 & 2.09\\
	& X-Transformer	 & -	 & -	 & -	 & -	 & -	 & -	 & -	 & -	 & -	 & -	 & -	 & -	 & -\\
	& Siamese	 & -	 & -	 & -	 & -	 & -	 & -	 & -	 & -	 & -	 & -	 & -	 & -	 & -\\
	& Parabel	 & 46.79	 & 41.36	 & 37.65	 & 44.39	 & 43.25	 & 16.94	 & 21.31	 & 24.13	 & 19.7	 & 21.34	 & 11.75	 & 1.5	 & 0.89\\
	& Bonsai	 & 47.87	 & 42.19	 & 38.34	 & 45.47	 & 44.35	 & 18.48	 & 23.06	 & 25.95	 & 21.52	 & 23.33	 & 9.02	 & 7.89	 & 39.03\\
	& DiSMEC	 & -	 & -	 & -	 & -	 & -	 & -	 & -	 & -	 & -	 & -	 & -	 & -	 & -\\
	& PfastreXML	 & 37.08	 & 33.77	 & 31.43	 & 36.61	 & 36.61	 & \textbf{28.71}	 & \textbf{30.98}	 & \textbf{32.51}	 & \textbf{29.92}	 & \textbf{30.73}	 & 29.59	 & 9.55	 & 23.64\\
	& XT	 & 40.6	 & 35.74	 & 32.01	 & 38.18	 & 36.68	 & 13.67	 & 17.11	 & 19.06	 & 15.64	 & 16.65	 & 7.9	 & 82.18	 & 5.94\\
	& Slice	 & 34.8	 & 30.58	 & 27.71	 & 32.72	 & 31.69	 & 13.8	 & 16.87	 & 18.89	 & 15.62	 & 16.74	 & 5.98	 & 0.79	 & 1.45\\
	& AnneXML	 & 47.79	 & 41.65	 & 36.91	 & 44.83	 & 42.93	 & 15.42	 & 19.67	 & 21.91	 & 18.05	 & 19.36	 & 14.53	 & 2.48	 & 0.12\\
	\midrule
	\multirow{11}{*}{\textbf{\rotatebox{90}{LF-Amazon-131K}}}	& \alg	 & \textbf{42.94}	 & 28.79	 & \textbf{21}	 & \textbf{44.25}	 & \textbf{46.84}	 & \textbf{34.52}	 & \textbf{41.14}	 & \textbf{47.33}	 & \textbf{39.35}	 & \textbf{42.48}	 & 1.86	 & 1.8	 & 0.1\\
	& Astec	 & -	 & -	 & -	 & -	 & -	 & -	 & -	 & -	 & -	 & -	 & -	 & -	 & -\\
	& AttentionXML	 & 42.9	 & \textbf{28.96}	 & 20.97	 & 44.07	 & 46.44	 & 32.92	 & 39.51	 & 45.24	 & 37.49	 & 40.33	 & 5.04	 & 50.17	 & 12.33\\
	& MACH	 & 34.52	 & 23.39	 & 17	 & 35.53	 & 37.51	 & 25.27	 & 30.71	 & 35.42	 & 29.02	 & 31.33	 & 4.57	 & 13.91	 & 0.25\\
	& X-Transformer	 & -	 & -	 & -	 & -	 & -	 & -	 & -	 & -	 & -	 & -	 & -	 & -	 & -\\
	& Bonsai	 & 40.23	 & 27.29	 & 19.87	 & 41.46	 & 43.84	 & 29.6	 & 36.52	 & 42.39	 & 34.43	 & 37.34	 & 0.46	 & 0.4	 & 7.41\\
	& DiSMEC	 & 41.68	 & 28.32	 & 20.58	 & 43.22	 & 45.69	 & 31.61	 & 38.96	 & 45.07	 & 36.97	 & 40.05	 & 0.45	 & 7.12	 & 15.48\\
	& PfastreXML	 & 35.83	 & 24.35	 & 17.6	 & 36.97	 & 38.85	 & 28.99	 & 33.24	 & 37.4	 & 31.65	 & 33.62	 & 0.01	 & 1.54	 & 3.32\\
	& XT	 & 34.31	 & 23.27	 & 16.99	 & 35.18	 & 37.26	 & 24.35	 & 29.81	 & 34.7	 & 27.95	 & 30.34	 & 0.92	 & 1.38	 & 7.42\\
	& Slice	 & 32.07	 & 22.21	 & 16.52	 & 33.54	 & 35.98	 & 23.14	 & 29.08	 & 34.63	 & 27.25	 & 30.06	 & 0.39	 & 0.11	 & 1.35\\
	& AnneXML	 & 35.73	 & 25.46	 & 19.41	 & 37.81	 & 41.08	 & 23.56	 & 31.97	 & 39.95	 & 29.07	 & 33	 & 4.01	 & 0.68	 & 0.11\\
	 \midrule
	 \multirow{11}{*}{\textbf{\rotatebox{90}{LF-WikiSeeAlso-320K}}}	& \alg	 & \textbf{41.36}	 & \textbf{28.04}	 & \textbf{21.38}	 & \textbf{41.55}	 & \textbf{43.32}	 & \textbf{25.72}	 & \textbf{30.93}	 & \textbf{34.89}	 & \textbf{30.69}	 & \textbf{33.69}	 & 4.84	 & 13.4	 & 0.09\\
	& Astec	 & -	 & -	 & -	 & -	 & -	 & -	 & -	 & -	 & -	 & -	 & -	 & -	 & -\\
	& AttentionXML	 & 40.5	 & 26.43	 & 19.87	 & 39.13	 & 40.26	 & 22.67	 & 26.66	 & 29.83	 & 26.13	 & 28.38	 & 7.12	 & 90.37	 & 12.6\\
	& MACH	 & 27.18	 & 17.38	 & 12.89	 & 26.09	 & 26.8	 & 13.11	 & 15.28	 & 16.93	 & 15.17	 & 16.48	 & 11.41	 & 50.22	 & 0.54\\
	& X-Transformer	 & -	 & -	 & -	 & -	 & -	 & -	 & -	 & -	 & -	 & -	 & -	 & -	 & -\\
	& Bonsai	 & 34.86	 & 23.21	 & 17.66	 & 34.09	 & 35.32	 & 18.19	 & 22.35	 & 25.66	 & 21.62	 & 23.84	 & 0.84	 & 1.39	 & 8.94\\
	& DiSMEC	 & 34.59	 & 23.58	 & 18.26	 & 34.43	 & 36.11	 & 18.95	 & 23.92	 & 27.9	 & 23.04	 & 25.76	 & 1.28	 & 58.79	 & 75.52\\
	& PfastreXML	 & 28.79	 & 18.38	 & 13.6	 & 27.69	 & 28.28	 & 17.12	 & 18.19	 & 19.43	 & 18.23	 & 19.2	 & 14.02	 & 4.97	 & 2.68\\
	& XT	 & 30.1	 & 19.6	 & 14.92	 & 28.65	 & 29.58	 & 14.43	 & 17.13	 & 19.69	 & 16.37	 & 17.97	 & 2.2	 & 3.27	 & 4.79\\
	& Slice	 & 27.74	 & 19.39	 & 15.47	 & 27.84	 & 29.65	 & 13.07	 & 17.5	 & 21.55	 & 16.36	 & 18.9	 & 0.94	 & 0.2	 & 1.18\\
	& AnneXML	 & 30.79	 & 20.88	 & 16.47	 & 30.02	 & 31.64	 & 13.48	 & 17.92	 & 22.21	 & 16.52	 & 19.08	 & 12.13	 & 2.4	 & 0.11\\
    \bottomrule
    \end{tabular}
    }
\end{table*}

\begin{table*}
\caption{A comparison of \alg's performance on product-to-category datasets. The first row is a short-text dataset and the second row its long-text counterpart. Although \alg focuses on product-to-product tasks, it is nevertheless competitive in terms of accuracy, as well as an order of magnitude faster in prediction as compared to leading deep learning approaches. Methods marked with a `-' sign could not be scaled for the given dataset within the available resources. The AttentionXML method used a non-standard version of the Wikipedia-500K dataset. All other methods, including \alg, used the standard version of the dataset.}
    \label{tab:sup:category}
    \centering
    \resizebox{\textwidth}{!}{
    \begin{tabular}{@{}c|l|ccccc|ccccc|ccc@{}}
    \toprule
    \textbf{Dataset} & \textbf{Method} & \textbf{P@1} & \textbf{P@3} & \textbf{P@5} & \textbf{N@3} & \textbf{N@5} &  \textbf{PSP@1} & \textbf{PSP@3}& \textbf{PSP@5} & \textbf{PSN@3} & \textbf{PSN@5}
    & \multicolumn{1}{c}{\begin{tabular}[c]{@{}c@{}}\textbf{Model}\\ \textbf{Size (GB)}\end{tabular}}
    & \multicolumn{1}{c}{\begin{tabular}[c]{@{}c@{}}\textbf{Training}\\ \textbf{Time (hrs)}\end{tabular}}
    & \multicolumn{1}{c}{\begin{tabular}[c]{@{}c@{}}\textbf{Prediction}\\ \textbf{Time (ms)}\end{tabular}} \\
    \midrule
  
	\multirow{11}{*}{\textbf{\rotatebox{90}{LF-WikiTitles-500K}}}	& \alg	 & 44.21	 & 24.64	 & 17.36	 & \textbf{33.55}	 & \textbf{31.92}	 & \textbf{19.29}	 & \textbf{19.82}	 & \textbf{19.96}	 & \textbf{21.26}	 & \textbf{22.95}	 & 4.53	 & 42.26	 & 0.09\\
	& Astec-3	 & \textbf{44.4}	 & \textbf{24.69}	 & \textbf{17.49}	 & 33.43	 & 31.72	 & 18.31	 & 18.25	 & 18.56	 & 19.57	 & 21.09	 & 15.01	 & 13.5	 & 2.7\\
	& AttentionXML	 & 40.9	 & 21.55	 & 15.05	 & 29.38	 & 27.45	 & 14.8	 & 13.97	 & 13.88	 & 15.24	 & 16.22	 & 14.01	 & 133.94	 & 9\\
	& MACH	 & 37.74	 & 19.11	 & 13.26	 & 26.63	 & 24.94	 & 13.71	 & 12.14	 & 12	 & 13.63	 & 14.54	 & 4.73	 & 22.46	 & 0.8\\
	& X-Transformer	 & -	 & -	 & -	 & -	 & -	 & -	 & -	 & -	 & -	 & -	 & -	 & -	 & -\\
	& Siamese	 & -	 & -	 & -	 & -	 & -	 & -	 & -	 & -	 & -	 & -	 & -	 & -	 & -\\
	& Parabel	 & 40.41	 & 21.98	 & 15.42	 & 29.89	 & 28.15	 & 15.55	 & 15.32	 & 15.35	 & 16.5	 & 17.66	 & 2.7	 & 0.42	 & 0.81\\
	& Bonsai	 & 40.97	 & 22.3	 & 15.66	 & 30.35	 & 28.65	 & 16.58	 & 16.34	 & 16.4	 & 17.6	 & 18.85	 & 1.63	 & 2.03	 & 17.38\\
	& DiSMEC	 & 39.42	 & 21.1	 & 14.85	 & 28.87	 & 27.29	 & 15.88	 & 15.54	 & 15.89	 & 16.76	 & 18.13	 & 0.68	 & 48.27	 & 11.71\\
	& PfastreXML	 & 35.71	 & 19.27	 & 13.64	 & 26.45	 & 25.15	 & 18.23	 & 15.42	 & 15.08	 & 17.34	 & 18.24	 & 20.41	 & 3.79	 & 9.37\\
	& XT	 & 38.19	 & 20.74	 & 14.68	 & 28.15	 & 26.64	 & 14.2	 & 14.14	 & 14.41	 & 15.18	 & 16.45	 & 3.1	 & 8.78	 & 7.56\\
	& Slice	 & 25.48	 & 15.06	 & 10.98	 & 20.67	 & 20.52	 & 13.9	 & 13.33	 & 13.82	 & 14.5	 & 15.9	 & 2.3	 & 0.74	 & 1.76\\
	& AnneXML	 & 39	 & 20.66	 & 14.55	 & 28.4	 & 26.8	 & 13.91	 & 13.38	 & 13.75	 & 14.63	 & 15.88	 & 11.18	 & 1.98	 & 0.13\\
	\midrule
	\multirow{11}{*}{\textbf{\rotatebox{90}{LF-Wikipedia-500K}}}	& \alg	 & 73.96	 & 54.17	 & 42.43	 & 66.31	 & 64.81	 & 32.13	 & 40.13	 & 44.59	 & 39.57	 & 43.7	 & 9.34	 & 44.23	 & 0.09\\
	& Astec	 & -	 & -	 & -	 & -	 & -	 & -	 & -	 & -	 & -	 & -	 & -	 & -	 & -\\
	& AttentionXML	 & \textbf{82.73}	 & \textbf{63.75}	 & \textbf{50.41}	 & \textbf{76.56}	 & \textbf{74.86}	 & \textbf{34}	 & \textbf{44.32}	 & \textbf{50.15}	 & \textbf{42.99}	 & \textbf{47.69}	 & 9.73	 & 221.6	 & 12.38\\
	& MACH	 & 52.48	 & 31.93	 & 23.34	 & 41.7	 & 39.43	 & 17.92	 & 18.16	 & 18.66	 & 19.45	 & 20.77	 & 28.12	 & 220.07	 & 0.82\\
	& X-Transformer	 & -	 & -	 & -	 & -	 & -	 & -	 & -	 & -	 & -	 & -	 & -	 & -	 & -\\
	& Siamese	 & -	 & -	 & -	 & -	 & -	 & -	 & -	 & -	 & -	 & -	 & -	 & 0.03	 & -\\
	& Parabel	 & 70.14	 & 50.62	 & 39.45	 & 61.86	 & 59.89	 & 27.25	 & 32.52	 & 35.93	 & 32.29	 & 35.31	 & 5.51	 & 3.02	 & 2.01\\
	& Bonsai	 & 70.56	 & 51.11	 & 39.86	 & 62.47	 & 60.61	 & 28.18	 & 33.86	 & 37.55	 & 33.58	 & 36.86	 & 3.94	 & 17.22	 & 22.23\\
	& DiSMEC	 & -	 & -	 & -	 & -	 & -	 & -	 & -	 & -	 & -	 & -	 & -	 & -	 & -\\
	& PfastreXML	 & 61.24	 & 41.59	 & 31.75	 & 52.26	 & 50.34	 & 33.3	 & 32.56	 & 33.67	 & 33.77	 & 35.25	 & 48.26	 & 24.71	 & 7.69\\
	& XT	 & 66.98	 & 48.33	 & 37.82	 & 58.94	 & 57.19	 & 24.78	 & 30.06	 & 33.46	 & 29.63	 & 32.51	 & 3.9	 & 16.73	 & 3.81\\
	& Slice	 & 47.51	 & 32.34	 & 25.07	 & 40.56	 & 39.51	 & 19.6	 & 21.99	 & 24.6	 & 22.2	 & 24.53	 & 2.3	 & 0.67	 & 1.58\\
	& AnneXML	 & 64.77	 & 43.24	 & 32.79	 & 54.63	 & 52.51	 & 24.08	 & 28.25	 & 31.2	 & 28.47	 & 31.3	 & 49.25	 & 14.97	 & 5.15\\
	 \bottomrule
    \end{tabular}
    }
\end{table*}

\begin{table*}
\caption{A comparison of \alg's performance on the proprietary datasets. \alg can be an order of magnitude faster in prediction as compared to existing deep learning approaches.}
    \label{tab:sup:p2p}
    \centering
    \resizebox{\textwidth}{!}{
    \begin{tabular}{@{}c|l|ccccc|ccccc@{}}
    \toprule
    \textbf{Dataset} & \textbf{Method} & \textbf{P@1} & \textbf{P@3} & \textbf{P@5} & \textbf{N@3} & \textbf{N@5} &  \textbf{PSP@1} & \textbf{PSP@3}& \textbf{PSP@5} & \textbf{PSN@3} & \textbf{PSN@5}\\
    \midrule
    \multirow{5}{*}{\textbf{\rotatebox{45}{P2PTitles-300K}}}	& \alg	 & \textbf{47.17}	 & \textbf{30.67}	 & \textbf{22.69}	 & \textbf{53.62}	 & \textbf{57.06}	 & \textbf{42.43}	 & \textbf{55.07}	 & \textbf{62.3}	 & \textbf{49.86} &  \textbf{53.27}\\
	& Astec	 & 44.3	 & 28.95	 & 21.56	 & 50.36	 & 53.67	 & 39.44	 & 50.9	 & 57.83	 & 45.99	 & 49.12	 \\
	& Parabel	 & 43.14	 & 28.34	 & 20.99	 & 48.73	 & 51.75	 & 37.26	 & 48.87	 & 55.32	 & 43.45	 & 46.32\\
	& PfastreXML	 & 39.4	 & 25.6	 & 18.77	 & 44.59	 & 46.98	 & 35.79	 & 45.13	 & 49.9	 & 40.98	 & 43.03	 \\
	& Slice	 & 31.27	 & 28.91	 & \textbf{25.19}	 & 31.5	 & 33.2	 & 27.03	 & 30.44	 & 34.95	 & 28.54	 & 30.77 \\
	\midrule
  \multirow{5}{*}{\textbf{\rotatebox{45}{P2PTitles-2M}}}	& \alg	 & \textbf{40.27}	 & \textbf{36.65}	 & \textbf{31.45}	 & \textbf{40.4}	 & \textbf{42.49}	 & \textbf{36.65}	 & \textbf{40.14}	 & \textbf{45.15}	 & \textbf{38.23}	 & \textbf{40.99}	\\
	& Astec	 & 36.34	 & 33.33	 & 28.74	 & 36.63	 & 38.63	 & 32.75	 & 36.3	 & 41	 & 34.43	 & 36.97\\
	& Parabel	 & 35.26	 & 32.44	 & 28.06	 & 35.3	 & 36.89	 & 30.21	 & 33.85	 & 38.46	 & 31.63	 & 33.71\\
	& PfastreXML	 & 30.52	 & 28.68	 & 24.6	 & 31.5	 & 33.23	 & 28.84	 & 32.1	 & 35.65	 & 30.56	 & 32.52	 \\
	& Slice	 & 31.27	 & 28.91	 & 25.19	 & 31.5	 & 33.2	 & 27.03	 & 30.44	 & 34.95	 & 28.54	 & 30.77\\
	 \bottomrule
    \end{tabular}
    }
\end{table*}
