\section{Ablation experiments}
\label{sec:ablation}

This section includes ablation experiments to further understand the utility of \alg's components, {\it i.e.,} initialization strategy, label metadata, and classifier. The results are also included for the existing classifiers with explicit label metadata components.

\subsection{Metadata in existing XML algorithms}
\textbf{Learning metadata with base XML classifier}: In principle, \alg's formulation could be efficiently deployed with existing algorithms where collaborative learning among labels is possible. Incorporating label metadata component lead to $1.5\%$ gain in the leading classifier including DiSMEC, Parabel, Bonsai, and DeepXML as compared to their counterparts without the label metadata component. For details please refer to table~\ref{tab:bowmeta} in the supplementary. This demonstrates that label metadata can lead to gains even for current XML algorithms. However, \alg could be upto $3\%$ more accurate as compared to these algorithms even after incorporating label metadata. This demonstrates the utility of \alg's formulation, which efficiently encodes label metadata and encourages collaborative learning among the labels.

\textbf{Re-ranking labels based on label meta-data}:
Classifiers such as DeepXML~\citep{dahiya2020} utilized a re-ranker to improve classification accuracy. \alg's metadata enriched classifier can also be used to re-rank the base classifier's predictions. The re-ranker is the same as the \alg classifier for the leaf nodes (Section~\ref{sec:training}) on the PLT with a difference that top $r$ shortlist of labels are sampled from the base model. In particular, re-ranking Parabel's and DeepXML's predictions could lead to 2\% and 1\% gains compared to the respective base classifiers. However, \alg's label metadata enriched PLT classifier could be upto 4\% more accurate than existing leading XML classifiers, even if they include metadata re-ranker, indicating efficiency as well as the accuracy of the proposed PLT. Please refer to table~\ref{tab:sub:xmlclass} and section~\ref{sup:xmlmeta} in supplementary for more details. 

\subsection{Varying \alg's classifier}
Experiments were also performed to understand the importance of both components of the classifier, \ie, the traditional weight-vector $\v u$, labels metadata weight-vector $\v v$, and the initialization of $\v u$. \alg computes $\v w$ as $\v w=\mathbf{f}(\v u, \v v)$, where $\mathbf{f}(.)$ can be non-linear or a linear operator. Please refer to Table~\ref{tab:combouv} in the supplementary material for different variation of $\mathbf{f}(.)$. First, \alg lead to 1\% gains as compared to the variation where $\v u$ was initialized randomly~\citep{glorot10} while other components were kept same as \alg. Please note that even without initialization \alg can be upto $3\%$ more accurate than state-of-the-art XML classifiers. This indicates metadata $\v v$ is essential to improve accuracy over current XML classifiers. Furthermore, configuration $\v w = \v u$, $\v w = \v u$-no-init, refers to case when metadata component $\v v$ is absent like all XML classifiers. It can be observed that $\v w = \v u$ can be upto $3\%$ more accurate than existing XML classifiers while $\v w = \v u$-no-init performs worse than DeepXML. This indicates the importance of initialization for the traditional classifier $\v u$. \alg's current formulation, {\it i.e.,} $\v w = \sigma(\v \alpha) \odot \v u + \sigma(\v \beta) \odot \v v$ could be upto $2\%$ more accurate as compared to simple sum \ie, $\v w = \v u + \v v$. Indicating current, non-linear combination is essential for better accuracy. Figure~\ref{fig:sub:init} in the supplementary divided labels into five equal size bins of increasing frequency. It can be observed from figure~\ref{fig:sub:init} that the accuracy of labels in the bins having low frequency labels increases significantly when metadata $\v v$ is collaboratively learned along with $\v u$. This supports that collaborative learning can lead to significant gains for data improvised labels.

\subsection{Varying \alg's tree configuration}
Experiments were also performed to understand the impact of \alg's tree hyper-parameters, including depth ($D$) and width ($\max(\v B)$) of the tree, where $\v B$ is the set containing the number of nodes at each depth. Shallow and wide trees were empirically observed to be up to $4\%$ more accurate as compared to slim and deep trees. This can be attributed to the fact that the errors made by nodes at shallow depths can prove to be more costly than errors made by nodes near full depth due to the cascading effect of errors in tree-based settings. Furthermore,~\citep{grave17} demonstrated that shallow and wide trees could be more suitable for GPU based implementation. Different tree configurations are included in Table~\ref{tab:sub:tree_sctruc} in the supplementary material. Furthermore, increasing the number of learners could lead to up to $1.7\%$ increase in accuracy. The accuracy plateaued at three learners (or trees), as demonstrated in Figure~\ref{fig:numlearner} in the supplementary material. It should be reiterated that \alg was observed to be an order of magnitude faster at prediction, even with multiple learners relative to leading XML algorithms.
