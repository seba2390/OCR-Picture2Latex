\subsection{Dataset Preparation and Evaluation Details}
\label{app:dataprep}
Train-test splits were generated using a random 70:30 split keeping only those labels that have at least 1 test as well as 1 train point. For sake of validation, 5\% of training data points were randomly sampled.

\textbf{Reciprocal pair removal}:  It was observed that in certain datasets, documents were mapped to themselves. For instance, the product with title ``Dinosaur'' was tagged with the label ``Dinosaur'' itself in the LF-AmazonTitles-131K dataset. Algorithms could achieve disproportionately high P@$1$ by making such trivial predictions without learning anything useful. Additionally, in product-to-product and related webpage recommendation tasks, both documents and labels come from the same set/universe. This allows for \emph{reciprocal pairs} to exist where a data point has document A and label B in its ground truth but a separate data point has document B and label A in its ground truth. We affectionately call these AB and BA pairs respectively. If these pairs are split across train and test sets, an algorithm could simply memorize the AB pair while training and predict the BA pair during testing to achieve very high P@1. Moreover, such predictions did not add to the quality of predictions in real-life applications. Hence, methods were not rewarded for making such trivial predictions. Table~\ref{tab:baelines_eval} reports numbers as per this very evaluation strategy.
% We note that trivial and reciprocal pairs were removed from the test sets alone. 
Additionally, coverage (C@20) is reported in Table~\ref{tab:p2p} to verify that prediction accuracy is not being achieved at the expense of label coverage.
