\section{Experiments}
\label{sec:results}

\input{tables/results_main_filtered_titles}
\input{tables/results_main_filtered_text}

\textbf{Datasets}: Experiments were conducted on product-to-product and related-webpage recommendation datasets. These were short-text tasks with only the product/webpage titles being used to perform prediction. Of these, LF-AmazonTitles-131K, LF-AmazonTitles-1.3M, and LF-WikiSeeAlsoTitles-320K are publicly available at The Extreme Classification Repository~\cite{XMLRepo}. Results are also reported on two proprietary product-to-product recommendation datasets (LF-P2PTitles-300K and LF-P2PTitles-2M) mined from click logs of the Bing search engine, where a pair of products was considered similar if the Jaccard index of the set of queries which led to a click on them was found to be more than a certain threshold. We also considered some datasets' long text counterparts, namely LF-Amazon-131K and LF-WikiSeeAlso-320K, which contained the entire product/webpage descriptions. Note that LF-AmazonTitles-131K and LF-AmazonTitles-1.3M (as well as their long-text counterparts) are subsets of the standard AmazonTitles-670K and AmazonTitles-3M datasets respectively, and were created by restricting the label set to labels for which label-text was available. Please refer to Appendix~A.3 and Table~7 in the \suppl for dataset preparation details and dataset statistics.

% \textbf{Product-to-Product}: The Product-to-Product similarity datasets are mined from the product-click logs of a popular search engine, where we consider a pair of products similar if the Jaccard index of the set of queries that lead to a click on them is more than the threshold of 0.5. 

\textbf{Baseline algorithms}: \alg was compared to leading deep extreme classifiers including the X-Transformer~\cite{Chang20}, Astec~\cite{Dahiya21}, XT~\cite{Wydmuch18}, AttentionXML~\cite{You18}, and MACH~\cite{Medini2019}, as well as standard extreme classifiers based on fixed or sparse BoW features including Bonsai~\cite{Khandagale19}, DiSMEC~\cite{Babbar17}, Parabel~\cite{Prabhu18b}, AnnexML~\cite{Tagami17}. Slice~\cite{Jain19}. Slice was trained with fixed FastText~\cite{Bojanowski17} features, while other methods used sparse BoW features. Unfortunately, GLaS~\citep{Guo2019} could not be included in the experiments as their code was not publicly available. Each baseline deep learning method was given a 12-core Intel Skylake 2.4 GHz machine with 4 Nvidia V100 GPUs. However, \alg was offered a 6-core Intel Skylake 2.4 GHz machine with a single Nvidia V100 GPU. A training timeout of 1 week was set for every method. Please refer to Table~9 in the \suppl for more details.

\clearpage

\input{tables/P2P}

\textbf{Evaluation}: Standard extreme classification metrics~\cite{Babbar19, Prabhu14, Prabhu18b, You18, Liu17}, namely Precision (P@$k$) and propensity scored precision (PSP@$k$) for $k= 1, 3, 5$ were used and are detailed in Appendix~A.4 in the \suppl.

\textbf{Hyperparameters}: \alg has two tuneable hyperparameters a) beam-width $B$ which determines the shortlist length $LB/K$ and b) token embedding dimension $D$. $B$ was chosen after concluding Module II training by setting a value that ensured a recall of $>85\%$ on the training set (note that choosing $B = K$ trivially ensures $100\%$ recall). Doing so did not require \alg to re-train Module II yet ensured a high quality shortlisting. Token embedding dimension $D$ was kept at 512 for larger datasets to improve the network capacity for large output spaces. For the small dataset LF-AmazonTitles-131K, clusters size $K$ was kept at $2^{15}$ and for other datasets it was kept at $2^{17}$. All other hyperparameters including learning rate, number of epochs were set to their default values across all datasets. Please refer to Table~8 in the \suppl for details.

\input{figures/fig_tex/results_contrib}

\textbf{Results on public datasets}: Table~\ref{tab:baelines_eval} compares \alg with leading XML algorithms on short-text product-to-product and related-webpage tasks. For details as well as results on long-text versions of these datasets, please refer to Table~9 in the \suppl. Furthermore, although \alg focuses on product-to-product applications, results on product-to-category style datasets such as product-to-category prediction on Amazon or article-to-category prediction on Wikipedia are reported in Table~10 in the \suppl. Parabel~\citep{Prabhu18}, Bonsai~\citep{Khandagale19}, AttentionXML~\citep{You18} and X-Transformer~\citep{Chang20} are the most relevant methods to \alg as they shortlist labels based on a tree learned in the label centroid space. \alg was found to be $4-10\%$ more accurate than methods such as Slice~\cite{Jain19}, PfastreXML~\cite{Jain17}, DiSMEC~\cite{Babbar17}, and AnnexML~\cite{Tagami17} that use fixed or pre-learnt features. This demonstrates that learning tasks-specific features can lead to significantly more accurate predictions. \alg was also compared with other leading deep learning based approaches like MACH~\citep{Medini2019}, and XT~\cite{Wydmuch18}. \alg could be up to $7\%$ more accurate while being more than 150$\times$ faster at prediction as compared to attention based models like X-Transformer and AttentionXML. \alg was also compared to Siamese networks that had similar access to label metadata as \alg. However, \alg could be up to $15\%$ more accurate than a Siamese network at an extreme scale. \alg was also compared to Astec~\citep{Dahiya21} that was specifically designed for short-text applications but does not utilize label metadata. \alg could be up to 3\% more accurate than Astec. This further supports \alg's claim of using label meta-data for improving prediction accuracy. Even on long-text tasks such as the LF-WikiSeeAlso-320K dataset (please refer to Table~9 in the \suppl), \alg can be more accurate in propensity scored metrics compared to the second best method AttentionXML, in addition to being vastly superior in terms of prediction time. This indicates the suitability of \alg's frugal architecture to product-to-product scenarios. The frugal architecture also allows \alg to make predictions on a CPU within a few milliseconds even for large datasets such as LF-AmazonTitles-1.3M while other deep extreme classifiers can take an order of magnitude longer time even on a GPU. \alg's prediction times on a CPU are reported within parentheses in Table~\ref{tab:baelines_eval}.

\textbf{Results on proprietary datasets}: Table~\ref{tab:p2p} presents results on proprietary product-to-product recommendation tasks (with details presented in Table~11 in the \suppl). \alg could easily scale to the LF-P2PTitles-2M dataset and be upto 2\% more accurate than leading XML algorithms including Bonsai, Slice and Parabel. Unfortunately, leading deep learning algorithms such as X-Transformer could not scale to this dataset within the timeout. \alg offers label coverage similar to state-of-the-art XML methods yet offers the best accuracy in terms of P@1. Thus, \alg's superior predictions do not come at a cost of coverage.

\input{tables/examples_table_decaf}

\textbf{Analysis}: Table~\ref{tab:decaf_examples} shows specific examples of \alg predictions. \alg encourages collaborative learning among labels which allows it to predict the labels ``Australian dollar" and ``Economy of New Zealand'' for the document ``New Zealand dollar'' when other methods failed to do so. This example was taken from the LF-WikiseeAlsoTitles-320K dataset (please refer to Table~12 in the \suppl for details). It is notable that these labels do not share any common training instances with other ground truth labels but are semantically related nevertheless. \alg similarly predicted a rare label ``Panzer Dragoon Orta'' for the (video game) product ``Panzer Dragoon Zwei' whereas other algorithms failed to do so. To better understand the nature of \alg's gains, the label set was divided into five uniform bins (quantiles) based on frequency of occurrence in the training set. \alg's collaborative approach using label text in classifier learning led to gains in every quantile, the gains were more prominent on the data-scarce tail-labels, as demonstrated in Figure~\ref{fig:sup:contrib}.

\textbf{Incorporating metadata into baseline XML algorithms}: In principle, \alg's formulation could be deployed with existing XML algorithms wherever collaborative learning is feasible. Table \ref{tab:bowmeta} shows that introducing label text embeddings to the DiSMEC, Parabel, and Bonsai classifiers led to upto $1.5\%$ gain as compared to their vanilla counterparts that do not use label text. Details of these augmentations are given in Appendix~A.5 in the \suppl. Thus, label text inclusion can lead to gains for existing methods as well. However, \alg continues to be upto $7\%$ more accurate than even these augmented versions. This shows that \alg is more efficient at utilizing available label text.

\input{tables/ablation}

\begin{figure}
    \centering
    \begin{minipage}{\linewidth}
		\centering
	   \includegraphics[width=0.65\linewidth]{figures/p5_with_inst-crop.pdf}
    \caption{Impact of the number of instances in \alg's ensemble on performance on the LF-AmazonTitles-131K dataset. \alg offers maximum benefits using a small ensemble of 3 instances after which benefits taper off.}
    \label{fig:numlearner}
	\end{minipage}
	\begin{minipage}{\linewidth}
		\centering
	   \includegraphics[width=0.65\linewidth]{figures/recall-crop.pdf}
    \caption{A comparison of recall when using moderate or large fanout on the LF-WikiSeeAlso-320K dataset. The x-axis represents various values of beam-width $B$ and training recall offered by each. A large fanout offers superior recall with small beam width, and hence small shortlists lengths.}
    \label{fig:recall_clusters}
	\end{minipage}
\end{figure}

\textbf{Shortlister}: \alg's shortlister distinguishes itself from previous shortlisting strategies \cite{Chang20,Khandagale19,You18,Prabhu18b} in two critical ways. Firstly, \alg uses a massive fanout of $K = 2^{17} \approx 130$K clusters whereas existing approaches either use much fewer (upto 8K) clusters \cite{Chang20,Bhatia15} or use hierarchical clustering with a small fanout (upto 100) at each node \cite{Khandagale19,You18}. Secondly, in contrast to other methods that create shortlists from generic embeddings (e.g. bag-of-words or FastText~\citep{Joulin17}), \alg fine-tunes its shortlister in Module II using task-specific embeddings learnt in Module I. Tables \ref{tab:sub:xmlclass} and \ref{tab:combouv} show that \alg's shortlister offers much better performance than shortlists computed using a small fanout or else computed using ANNS-based negative sampling \cite{Jain19}. Fig~\ref{fig:recall_clusters} shows that a large fanout offers much better recall even with small shortlist lengths than if using even moderate fanouts e.g. $K = 8$K.

\textbf{Ablation}: As described in Section \ref{sec:method}, the training pipeline for \alg is divided into 4 modules mirroring the DeepXML pipeline \cite{Dahiya21}. Table~\ref{tab:combouv} presents the results of extensive experiments conducted to analyze the optimality of algorithmic and design choices made in these modules. We refer to Appendix~A.5 in the \suppl for details. \textbf{a)} To assess the utility of learning task-specific token embeddings in Module I, a variant \alg-FFT was devised that replaced these with pre-trained FastText embeddings: \alg outperforms \alg-FFT by 6\% in PSP@1 and 3.5\% in P@1. \textbf{b)} To assess the impact of a large fanout while learning the shortlister, a variant \alg-8K was trained with a smaller fanout of $K = 2^{13} \approx 8$K clusters that is used by methods such as AttentionXML and X-Transformer. Restricting fanout was found to hurt accuracy by 3\%. This can be attributed to the fact that the classifier's final accuracy depends on the recall of the shortlister (see Theorem~\ref{thm:thm}). Fig.~\ref{fig:recall_clusters} indicates that using $K = 2^{13}$ results in significantly larger shortlist lengths (upto $2\times$ larger) being required to achieve the same recall as compared to using $K = 2^{17}$. Large shortlists make Module IV training and prediction more challenging, especially for large datasets involving millions of labels, thereby making a large fan-out $K$ more beneficial. \textbf{c)} Approaches other than \alg's shortlister $\cS$ were considered for shortlisting labels, such as nearest neighbor search using HNSW \cite{Jain19} or PLTs with small fanout such as Parabel \cite{Prabhu18b} learnt over dense document embeddings. Table \ref{tab:sub:xmlclass} shows that both alternatives lead to significant loss, upto 15\% in recall, as compared to that offered by $\cS$. These sub-optimal shortlists eventually hurt final prediction which could be 2\% less accurate as compared to \alg. \textbf{d)} To assess the importance of label classifier initialization in Module III, a variant \alg-no-init was tested which initialized $\hat\vz^2_l$ randomly instead of with $\vE\vz_l$. \alg-no-init was found to offer 1-1.5\% less PSP@1 than \alg, therefore indicating importance of proper initialization in Module III. \textbf{e)} Modules II and IV learn OvA classifiers as a combination of the label embedding vector and a refinement vector. To investigate the need for both components, Table~\ref{tab:combouv} considers two \alg variants: the first variant, named \alg-$\hat\vz^1$, discards the refinement vector in both modules i.e. using $\vw_l = \hat\vz^1_l$ and $\vh_m = \hat\vu^1_m$ whereas the second variant, named \alg-$\hat\vz^2$, rejects the label embedding component altogether and learns the OvA classifers from scratch using only the refinement vector i.e. using $\vw_l = \hat\vz^2_l$ and $\vh_m = \hat\vu^2_m$. Both variants take a hit of up to 5\% in prediction accuracy as compared to \alg. Incorporating label-text in the classifier is critical to achieve superior accuracies. \textbf{f)} Finally, to assess the utility of fine-tuning token embeddings in each successive module, a frugal version \algl was considered which freezes token embeddings after Module I and shares token embeddings among the three instances in its ensemble. \algl offers 0.5-1\% loss in performance as compared to \alg but is noticeably faster at training.