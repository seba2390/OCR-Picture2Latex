\subsection{Proof of Theorem~\ref{thm:thm}}
\label{app:proof}
We recall from the main text that $\cL(\Theta)$ denotes the original likelihood expression and $\tilde\cL(\Theta \cond \cS)$ denotes the approximate likelihood expression that incorporates the shortlister $\cS$. Both expression are reproduced below for sake of clarity.
\begin{align*}
\cL(\Theta) &= \frac1{NL}\sum_{i \in [N]}\sum_{l \in [L]}\ell_{il}(\Theta)\\
\tilde\cL(\Theta \cond \cS) &= \frac{K}{NLB}\sum_{i \in [N]}\sum_{l \in \cS(\hat\vx_i)}\ell_{il}(\Theta)
\end{align*}

\begin{theorem}[Theorem~\ref{thm:thm} Restated]
\label{thm:thm-restated}
Suppose the training data has label sparsity at rate $s$ i.e. $\sum_{i \in [N]}\sum_{l \in [L]} \bI\bc{y_{il} = +1} = s\cdot NL$ and the shortlister offers a recall rate of $r$ on the training set i.e. $\sum_{i \in [N]}\sum_{l \in \cS(\hat\vx_i)}\bI\bc{y_{il} = +1} = rs\cdot NL$. Then if $\hat\Theta$ is obtained by optimizing the approximate likelihood function $\tilde\cL(\Theta \cond \cS)$, then the following always holds
\[
\cL(\hat\Theta) \leq \min_{\Theta} \cL(\Theta) + \bigO{s(1-r)\ln\br{\frac1{s(1-r)}}}.
\]
\end{theorem}

Below we prove the above claimed result. For the sake of simplicity, let $\Theta^\ast = \argmin_\Theta\ \cL(\Theta)$ denote the optimal model that could have been learnt using the original likelihood expression. As discussed in Sec~\ref{sec:method}, OvA methods with linear classifiers assume a likelihood decomposition of the form $\P{\vy_i \cond \vx_i, \Theta} = \prod_{l=1}^L\P{y_{il} \cond \hat\vx_i, \vw_l} = \prod_{l=1}^L\br{1 + \exp\br{y_{il}\cdot\ip{\hat\vx_i}{\vw_l}}}^{-1}$ where $\hat\vx_i = \text{ReLU}(\cE(\vx_i))$ is the document embedding obtained using token embeddings $\vE$ and embedding block parameters taken from $\Theta$, and $\vw_l$ is the label classifier obtained as shown in Fig~\ref{fig:embedding}. Thus, for a label-document pair $(i,l) \in [N] \times [L]$, the model posits a likelihood
\[
\P{y_{il} \cond \hat\vx_i, \vw_l} = \br{1 + \exp\br{y_{il}\cdot\ip{\hat\vx_i}{\vw_l}}}^{-1}
\]
However, in the presence of a shortlister $\cS$, the above model fails to hold since for a document $i$, a label $l \notin \cS(\hat\vx_i)$ is never predicted. This can cause a catastrophic collapse of the model likelihood if even a single positive label fails to be shortlisted by the shortlister, i.e. if the shortlister admits even a single false negative. To address this, and allow \alg to continue working with shortlisters with high but still imperfect recall, we augment the likelihood model as follows
\[
\P{y_{il} \cond \hat\vx_i, \vw_l} = \begin{cases}
\br{1 + \exp\br{y_{il}\cdot\ip{\hat\vx_i}{\vw_l}}}^{-1} & l \in \cS(\hat\vx_i)\\
y_{il}\br{\eta - \frac12} + \frac12 & l \notin \cS(\hat\vx_i)
\end{cases},
\]
where $\eta \in (0,1]$ is some default likelihood value assigned to positive labels that escape shortlisting (recall that $y_{il} \in \bc{-1,+1}$). Essentially, for non-shortlisted labels, we posit their probability of being relevant as $\eta$. The value of $\eta$ will be tuned later.

Note that we must set $\eta \ll 1$ so as to ensure that these default likelihood scores do not interfere with the prediction pipeline which discards non-shortlisted labels. We will see that our calculations do result in an extremely small value of $\eta$ as the optimal value. However, also note that we cannot simply set $\eta = 0$ since that would lead to a catastrophic collapse of the model likelihood to zero if the shortlister has even one false negative. Although our shortlister does offer good recall even with shortists of small length (e.g. 85\% with a shortlist of length $\approx 200$), demanding 100\% recall would require exorbitantly large beam sizes that would slow down prediction greatly. Thus, it is imperative that the augmented likelihood model itself account for shortlister failures.

To incorporate the above augmentation, we also redefine our log-likelihood score function to handle document-label pairs $(i,l) \in [N] \times [L]$ such that $l \notin \cS(\hat\vx_i)$
\[
\ell_{il}(\Theta \cond \cS) = \begin{cases}
\ln\br{1 + \exp\br{y_{il}\cdot\ip{\hat\vx_i}{\vw_l}}} & l \in \cS(\hat\vx_i)\\
-\ln\br{y_{il}\br{\eta - \frac12} + \frac12} & l \notin \cS(\hat\vx_i)
\end{cases},
\]
Note the negative sign in the second case since $\ell_{ij}$ is the negative log-likelihood expression. We will also benefit from defining the following \emph{residual} loss term
\[
\Delta(\Theta \cond \cS) = \sum_{i \in [N]}\sum_{l \notin \cS(\hat\vx_i)}\ell_{il}(\Theta)
\]
Note that $\Delta$ simply sums up loss terms corresponding to all labels omitted by the shortlister. We will establish the result claimed in the theorem by comparing the performance offered by $\hat\Theta$ and $\Theta^\ast$ on the loss terms given by $\tilde\cL$ and $\Delta$. Note that for any $\Theta$ we always have the following decomposition
\[
\cL(\Theta) = \frac1{NL}\br{\frac{NLB}K\cdot\tilde\cL(\Theta \cond \cS) + \Delta(\Theta \cond \cS}
\]
\newcommand{\fnr}{\text{FPR}}
\newcommand{\tnr}{\text{TNR}}
Now, since $\hat\Theta$ optimizes $\tilde\cL$, we have $\tilde\cL(\hat\Theta \cond \cS) \leq \tilde\cL(\Theta^\ast \cond \cS)$ which settles the first term in the above decomposition. To settle the second term, we note that as per the recall $r$ and label sparsity $s$ terms defined in the statement of the theorem, the number of positive labels not shortlisted by the shortlister $\cS$ throughout the dataset is $\fnr\cdot NL$ where $\fnr = (1-r)s$ is the false negative rate of the shortlister. Similarly, the number of negative labels not shortlisted by the shortlister throughout the dataset by $(L-B)N$ can be seen to be $\tnr\cdot NL$ where $\tnr = \br{(1-s) - \frac BK + rs}$ is the true negative rate of the shortlister. This gives us
\[
\Delta(\hat\Theta \cond \cS) = \br{\fnr\cdot\ln\frac1\eta + \tnr\cdot\ln\frac1{1-\eta}}\cdot NL
\]
It is easy to see that the optimal value of $\eta$ for the above expression is $\eta = \frac{\fnr}{\fnr + \tnr}$. For example, in the LF-WikiSeeAlsoTitles-320K dataset, which has $s \approx 6.75 \times 10^{-6}, r \approx 0.85, B = 160, K = 2^{17}$, this gives a value of $\fnr \approx 1.01 \times 10^{-6}, \tnr \approx 0.999$ which gives $\eta \approx 1.01 \times 10^{-6}$. This confirms that the augmentation indeed does not interfere with the prediction pipeline and labels not shortlisted can be safely ignored. However, moving on and plugging this optimal value of $\eta$ into the expression tells us that
\[
\Delta(\hat\Theta \cond \cS) = \br{\frac\fnr\tnr\ln\br{1 + \frac\tnr\fnr} + \ln\br{1+\frac\fnr\tnr}}\cdot NL.
\]
Since $\tnr \rightarrow 1$ (for example, we saw $\tnr \approx 0.999$ above), we simplify this to $\frac\fnr\tnr = \bigO{\fnr}$ and use the inequality $\ln(1+v) \leq v$ for all $v > 0$ to conclude that $\Delta(\hat\Theta \cond \cS) \leq \bigO{\fnr\ln\frac1\fnr + \fnr} = \bigO{s(1-r)\ln\br{\frac1{s(1-r)}}}$. Using $\Delta(\Theta^\ast \cond \cS) \geq 0$ settles the second term in the decomposition by establishing that $\Delta(\hat\Theta \cond \cS) - \Delta(\Theta^\ast \cond \cS) \leq \bigO{s(1-r)\ln\br{\frac1{s(1-r)}}}\cdot NL$. Combining the two terms in the decomposition above gives us
\[
\cL(\hat\Theta) - \cL(\Theta^\ast) = \frac1{NL}\br{\frac{NLB}K\cdot(\tilde\cL(\Theta \cond \cS) - \tilde\cL(\Theta^\ast \cond \cS)) + (\Delta(\Theta \cond \cS) - \Delta(\Theta^\ast \cond \cS))} \leq \bigO{s(1-r)\ln\br{\frac1{s(1-r)}}},
\]
which finishes the proof of the theorem.

We conclude this discussion by noting that since $\cL$ and $\tilde\cL$ are non-convex objectives due to the non-linear architecture encoded by the model parameters $\Theta$, it may not be able to solve these objectives optimally in practice. Thus, in practice, all we may be ensure is that
\[
\tilde\cL(\hat\Theta \cond \cS) \leq \min_\Theta \tilde\cL(\Theta \cond \cS) + \epsilon_{\text{opt}}
\]
where $\epsilon_{\text{opt}}$ is the sub-optimality in optimizing the objective $\tilde\cL$ due to factors such as sub-optimal initialization, training, premature termination, etc. It is easy to see that the main result of the theorem continues to hold since we now have $\tilde\cL(\hat\Theta \cond \cS) \leq \tilde\cL(\Theta^\ast \cond \cS) + \epsilon_{\text{opt}}$ which gives us the modified result as follows
\[
\cL(\hat\Theta) \leq \min_\Theta \cL(\Theta) + \bigO{s(1-r)\ln\br{\frac1{s(1-r)}}} + \frac BK\epsilon_{\text{opt}}.
\]
