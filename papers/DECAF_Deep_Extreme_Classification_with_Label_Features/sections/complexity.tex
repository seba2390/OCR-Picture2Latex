\newcommand{\vdoc}{\hat V_x}
\newcommand{\vlab}{\hat V_y}
\newcommand{\hL}{\hat L}
\newcommand{\hN}{\hat N}

\allowdisplaybreaks

\subsection{Time Complexity Analysis for \alg}
\label{app:complexity}
In this section, we discuss the time complexity of the various modules in \alg, as well as derive the prediction and training complexities.

\textbf{Notation}: Recall from Section~\ref{sec:method} that \alg learns $D$-dimensional representations for all $V$ tokens ($\ve_t, t \in [V]$), that are used to create embeddings for all $L$ labels $\hat\vz^1_l, l \in [L]$, and all $N$ training documents $\hat\vx_i, i \in [N]$. We introduce some additional notation to facilitate the discussion: we use $\vdoc$ to denote the average number of unique tokens present in a document i.e. $\vdoc = \frac1N\sum_{i=1}^N\norm{\vx_i}_0$ where $\norm{\cdot}_0$ is the sparsity ``norm'' that gives the number of non-zero elements in a vector. We similarly use $\vlab = \frac1L\sum_{l=1}^L\norm{\vz_l}_0$ to denote the average number of tokens in a label text. Let $\hL =  \frac1N\sum_{i=1}^N\norm{\vy_i}_0$ denote the average number of labels per document and also let $\hN = \frac{N\hL}L$ denote the average number of documents per label. We also let $M$ denote the mini-batch size (\alg used $M = 255$ for all datasets -- see Table~\ref{tab:hyperparameters}).

\textbf{Embedding Block}: Given a text with $\hat V$ tokens, the embedding block requires $\hat VD$ operations to aggregate token embeddings and $D^2 + 3D$ operations to execute the residual block and the combination block, for a total of $\bigO{\hat VD + D^2}$ operations. Thus, to encode a label (respectively document) text, it takes $\bigO{\vlab D + D^2}$ (respectively $\bigO{\vdoc D + D^2}$) operations on average.

\textbf{Prediction}: Given a test document, assuming that it contain $\vdoc$ tokens, embedding takes $\bigO{\vdoc D + D^2}$ operations, executing the shortlister by identifying the top $B$ clusters takes $\bigO{KD + K\log K}$ operations. These clusters contain a total of $\frac{LB}K$ labels. The ranker takes $\bigO{\frac{LB}KD + \frac{LB}K\log\br{\frac{LB}K}}$ operations to execute the $\frac{LB}K$ OvA linear models corresponding to these shortlisted labels to obtain the top-ranked predictions. Thus, prediction takes $\bigO{\vdoc D + D^2 + KD + K\log K} = \bigO{KD}$ time since usually $\frac{LB}K \leq K, \vdoc \leq K$ and $\log K \leq D \leq K$.

\textbf{Module I Training}: Creation of all $L$ label centroids $\vc_l$ takes $\bigO{L\hN\vdoc}$ time. These centroids are $\bigO{\hN\vdoc}$-sparse on average. Clustering these labels using hierarchical balanced binary clustering for $\log K$ levels to get $K$ balanced clusters takes time $\bigO{L\hN\vdoc\log K}$. Computing meta label text representations $\vu_m$ for all meta labels takes $\bigO{L\vlab}$ time. The vectors $\vu_m$ are $\frac{\vlab L}K$-sparse on average. To compute the complexity of learning the $K$ OvA meta-classifiers, we calculate below the cost of a single back-propagation step when using a mini-batch of size $M$. Computing the document and meta-label features of all $M$ documents in the mini-batch and $K$ meta-labels takes on average $\bigO{(D^2 + \vdoc D)M}$ and $\bigO{\br{D^2 + \frac{\vlab L}K\cdot D}K}$ time respectively. Computing the scores for all the OvA meta classifiers for all documents in the mini-batch takes $\bigO{MKD}$ time. Overestimating that the $K$ meta label texts together cover all $V$ tokens, updating the residual layer parameters $\vR$, the combination block parameters, and the token embeddings $\vE$ using back-propagation takes at most $\bigO{(D^2 + V)MK}$ time.

% \textbf{Caching Document and Label Pre-representations}: Since the default version of \alg does not change the token representations $E$ once they are fine-tuned along with the coarse shortlister, it is beneficial for us to pre-compute the $\vg$ vectors in the embedding block for all document and text so that they do not need to be computed again and again. Computing $\vg^i_x := E\vx^i, i \in [N]$ takes $\bigO{N\vdoc d}$ time whereas computing $\vg^l_y, l \in [L]$ takes $\bigO{L\vlab d}$ time.

% The fine-tuning phase for the shortlister offers faster training since the token embeddings are frozen in this stage in the default version of \alg, as well as because document pre-representations were also pre-computed as outlined above.

\textbf{Module II Training}: Recreating all $L$ label centroids $\vc_l$ now takes $\bigO{L\hN\vdoc D}$ time. Clustering the labels takes time $\bigO{LD\log K}$. Computing document features in a mini-batch of size $M$ takes $\bigO{(\vdoc D + D^2)M}$ time as before. Computing the meta-label representations $\hat\vu^1_m$ for all $K$ meta-labels now takes $\bigO{(\vlab D + D^2)L}$ time. Computing the scores for all the OvA meta classifiers for all documents in the mini-batch takes $\bigO{MKD}$ time as before. Next, updating the model parameters as well as the refinement vectors $\hat\vu^2_m, m \in [K]$ takes at most $\bigO{(D^2 + V)MK}$ time time as before. The added task of updating $\hat\vu^2_m$ does not affect the asymptotic complexity of this module. Generating the shortlists for all $N$ training points is essentially a prediction step and takes $\bigO{NKD}$ time.

\textbf{Module II Initializations}: Model parameter initializations take $\bigO{D^2}$ time. Initializing the refinement vectors $\hat\vz^2_l$ takes $\bigO{L\vlab D}$ time. 

\textbf{Module IV Training}: Given the shortlist of $LB/K$ labels per training point generated in Module II, training the OvA classifiers by fine-tuning the model parameters and learning the refinement vectors $\hat\vz^2_l, l \in [L]$ is made much less expensive than $\bigO{NLD}$. Computing document features in a mini-batch of size $M$ takes $\bigO{(\vdoc D + D^2)M}$ time as before. However, label representations $\hat\vz^1_l$ of only shortlisted labels need be computed. Since there are atmost $\br{\frac{LB}K + \hL}M$ of them (accounting for hard negatives and all positives), this takes $\bigO{(\vlab D + D^2)M\br{\frac{LB}K + \hL}}$ time. Next, updating the model parameters as well as the refinement vectors $\hat\vz^2_l$ for shortlisted takes at most $\bigO{(D^2 + (\vdoc +  \vlab)D)M\br{\frac{LB}K + \hL}}$ time. This can be simplified to $\bigO{M\br{\frac{LB}K + \hL}D^2} = \bigO{MD^2\log^2L}$ time per mini-batch since $\vdoc, \vlab \leq D$, usually $\hL \leq \bigO{\log L}$ and \alg chooses $\frac BK \leq \bigO{\frac{\log^2L}L}$ for large datasets such as LF-AmazonTitles-1.3M and LF-P2PTitles-2M (see Table~\ref{tab:hyperparameters}), thus ensuring an OvA training time that scales at most as $\log^2L$ with the number of labels.

