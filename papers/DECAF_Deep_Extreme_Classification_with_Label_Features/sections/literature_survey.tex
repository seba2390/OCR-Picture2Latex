\section{Related Work}
\label{sec:related}

\textbf{Summary}: XML techniques can be categorized into 1-vs-All, tree, and embedding methods. Of these, one-vs-all methods such as Slice \cite{Jain19} and Parabel \cite{Prabhu18b} offer the most accurate solutions. Recent advances have introduced the use of deep-learning-based representations. However, these techniques mostly do not use label metadata. Techniques such as the X-Transformer \cite{Chang20} that do use label text either do not scale well with millions of labels or else do not offer state-of-the-art accuracies. The \alg method presented in this paper effectively uses label metadata to offer state-of-the-art accuracies and scale to tasks with millions of labels.

\textbf{1-vs-All classifiers}: 1-vs-All classifiers PPDSparse~\citep{Yen17}, DiSMEC~\cite{Babbar17}, ProXML~\cite{Babbar19} are known to offer accurate predictions but risk incurring training and prediction costs that are linear in the number of labels, which is prohibitive at extreme scales. Approaches such as negative sampling, PLTs, and learned label hierarchies have been proposed to speed up training~\citep{Jain19, Khandagale19, Prabhu18b, Yen18a}, and predictions~\citep{Jasinska16, Niculescu17} for 1-vs-All methods. However, they rely on sub-linear search structures such as nearest-neighbor structures or label-trees that are well suited for fixed or pre-trained features such as bag-of-words or FastText~\citep{Joulin17} but not support jointly learning deep representations since it is expensive to repeatedly update these search structures as deep-learned representations keep getting updated across learning epochs. Thus, these approaches are unable to utilize deep-learned features, which leads to inaccurate solutions. \alg avoids these issues by its use of the \emph{shortlister} which offers a high recall filtering of labels allowing training and prediction costs that are logarithmic in the number of labels.

\textbf{Tree classifiers}: Tree-based classifiers typically partition the label space to achieve logarithmic prediction complexity. In particular, MLRF~\citep{Agrawal13}, FastXML~\citep{Prabhu14}, PfastreXML~\citep{Jain16} learn an ensemble of trees where each node in a tree is partitioned by optimizing an objective based on the Gini index or nDCG. CRAFTML~\cite{Siblini18a} deploys random partitioning of features and labels to learn an ensemble of trees. However, such algorithms can be expensive in terms of training time and model size.

\textbf{Deep feature representations}: Recent works MACH~\citep{Medini2019}, X-Transformer \cite{Chang20}, XML-CNN~\citep{Liu17}, and AttentionXML~\citep{Liu17} have graduated from using fixed or pre-learned features to using task-specific feature representations that can be significantly more accurate. However, CNN and attention-based mechanisms were found to be inaccurate on short-text applications (as shown in \cite{Dahiya21}) where scant information is available (3-10 tokens) for a data point. Furthermore, approaches like X-Transformer and AttentionXML that learn label-specific document representations do not scale well.

\textbf{Using label metadata}: Techniques that use label metadata e.g. label text include SwiftXML \citep{Prabhu18} which uses a pre-trained Word2Vec \citep{Mikolov13} model to compute label representations. However, SwiftXML is designed for \emph{warm-start} settings where a subset of ground-truth labels for each test point is already available. This is a non-standard scenario that is beyond the scope of this paper. \citep{Guo2019} demonstrated, using the GlaS regularizer, that modeling label correlations could lead to gains on tail labels. Siamese networks~\citep{Wu17} are a popular framework that can learn representations so that documents and their associated labels get embedded together. Unfortunately, Siamese networks were found to be inaccurate at extreme scales. The X-Transformer method \cite{Chang20} uses label text to generate shortlists to speed up training and prediction. \alg, on the other hand, makes much more direct use of label text to train the 1-vs-All label classifiers themselves and offers greater accuracy compared to X-Transformer and other XML techniques that also use label text.
