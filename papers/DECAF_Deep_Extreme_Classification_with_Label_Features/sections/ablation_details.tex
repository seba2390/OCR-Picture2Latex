\subsection{Further Details about Experiments and Ablation Studies}
\label{sup:xmlmeta}
\textbf{Recap of Notation:} Let us recall from Section~\ref{sec:method}, that $L$ denotes the number of labels and $V$ denotes the total number of tokens appearing across label and document texts. The training set of $N$ documents is presented as $\bc{(\vx_i,\vy_i)_{i=1}^{N}}$ with each document represented as a bag of tokens $\vx_i \in \bR^V$ with $x_{it}$ representing the TF-IDF weight of token $t \in [V]$ in the $i\nth$ document, and the ground truth label vector $\vy_i \in \bc{-1,+1}^L$ such that $y_{il} = +1$ if label $l \in [L]$ is relevant to document $i$ and $y_{il} = -1$ otherwise. For each label $l \in [L]$, its label text is similarly represented as a bag of TF-IDF scores $\vz_l \in \bR^V$. \alg learns $D$-dimensional embeddings for tokens, documents as well as labels.

\textbf{Incorporating Label text into existing BoW XML methods}: XML classifiers such as Parabel, DiSMEC, Bonsai, \etc, use a fixed BoW (bag-of-words)-based representation of documents to learn their classifiers. Label text was incorporated into these classifiers as follows: for every document $i$, let $s_{il} \in \bR$ be the relevance score the XML classifier predicted for label $l$ for document $i$. We augmented this score to incorporate label text by computing $\tilde s_{il} = \alpha \cdot s_{il} + (1-\alpha)\sigma\br{\ip{\vx_i}{\vz_l}}$. Here, $\alpha \in [0, 1]$ was fine tuned to offer the best results. Table~\ref{tab:bowmeta} shows that incorporating label text, even in this relatively crude way, still benefits accuracy.

% \textbf{Incorporating Label text into contemporary deep XML algorithms}: State-of-the-art deep XML classifiers such as MACH, AttentionXML and X-Transformer, could also have been modified to incorporate label metadata and learn label representation. Unfortunately, training every one of them with label meta-data, such as label text, could not scale within available resources, for example, in case of the X-Transformer method \cite{Chang20}.

\textbf{Generating alternative shortlists for \alg}: \alg learns a shortlister to generate a subset of labels with high recall, from an extremely large output space. Experiments were also conducted to use existing scalable XML algorithms \eg~Parabel or ANNS data structures \eg~HNSW as possible alternatives to generating this shortlist. Label centroids using learnt intermediate feature representations were provided to Parabel and HNSW in order to partition the label space. However, as Table~\ref{tab:sub:xmlclass} shows, this leads to significant reduction in precision as well as recall (upto 2\%) which adversely impacted the performance of the final ranking by \alg.
% We also experimented with random shortlists i.e. using random negative sampling (see Table~\ref{tab:combouv} \alg-RNS). Even if used just in stage-I while learning coarse-pre ranker and token embeddings (stage-II continued to use the default settings), this cheap strategy offered a drastic reduction in prediction accuracy. Specifically, \alg-RNS was found to be upto 10\% worse in terms of prec@1 as compared to \alg.

\textbf{Varying the shortlister fan-out in \alg}: \alg uses  Modules I and II to learn a shortlister. In Module I, \alg clusters the extremely large label space (in millions) to a smaller number of $K = 2^{17} \approx 130K$ meta-labels. In Module II, \alg fine-tunes the re-ranker to generate a shortlist of labels. For details of training please refer to section~\ref{sec:method} in the main paper. Experiments were conducted to observe the impact of the fan-out $K$. In particular fan-out was restricted to $2^{13} \approx 8K$ which is also a value used by contemporary algorithms such as AttentionXML and the X-Transformer. It was observed that to maintain a high recall (of around 85\%) during training \alg had to increase the beam-size by 2$\times$ which leads to increase in training time as well as a drop in accuracy (see Table~\ref{tab:combouv} \alg-8K). AttentionXML and X-Transformer were found to be computationally expensive and could not be scaled to use $2^{17}$ clusters to check whether increasing fan-out benefits them as it does \alg.

% \textbf{Withholding shortlister fine-tuning}: In another set of experiments, the shortlister was not fine-tuned in stage-II. We call this configuration \alg-NFT (see Table~\ref{tab:combouv}). Empirically we observed a 1\% loss in accuracy as compared to \alg.

\textbf{Varying the label classifier components in \alg}: As outlined in Section~\ref{sec:method}, \alg makes crucial use of label text embeddings while learning its label classifiers $\vw_l, l \in [L]$, with two components for each label $l$ a) $\hat\vz^1_l$ that is simply the label text embedding, and b) $\hat\vz^2_l$ that is a refinement vector. $\hat\vz^2_l$ was  initialized with $\vE\vz_l$ and then fine-tuned jointly with other model parameters such as those within the residual and combination blocks, etc. An experiment was conducted in which the label embedding component $\hat\vz^1_l$ was removed from the label classifier (effectively done by setting $\hat\vz^1_l = \v0, \forall l \in [L]$) and $\hat\vz^2_l$ was randomly initialized instead. We call this configuration \alg-$\hat\vz^2$ (see Table~\ref{tab:combouv}). Another experimented was conducted to understand the importance of the refinement vector $\hat\vz^2_l$. In this experiment, $\hat\vz^2_l$ was explicitly set to $\v0$ and we used $\vw_l = \hat\vz^1_l$. We call this configuration \alg-$\hat\vz^1$ (see Table~\ref{tab:combouv}).\alg was found to be upto 5\% more accurate as compared to these variants. These experiments suggest that the novel combination of two label classifier components as proposed by \alg, namely $\hat\vz^1_l$ and $\hat\vz^2_l$ is essential for achieving high accuracy.\\

\noindent\textbf{Please go to the next page for dataset statistics and hyperparameter details.}