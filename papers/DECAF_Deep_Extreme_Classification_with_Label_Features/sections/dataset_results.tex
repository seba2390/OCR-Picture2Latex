\section{Results}
\label{sec:results}

\input{tables/results_main_filtered}
\textbf{Datasets and features}: Experiments were conducted on the following four benchmark datasets available at The Extreme Classification Repository~\cite{XMLRepo}: AmazonTitles-300K, AmazonTitles-2M, WikiSeeAlsoTitles-350K, and WikiTitles-500K. It should be noted that AmazonTitles-300K and AmazonTitles-2M are a subset of standard AmazonTitles-670K and AmazonTitles-3M, respectively, created by restricting the label set to the labels for which meta-data was available. Please refer to Table~\ref{tab:stats} for dataset statistics and data processing scripts in the supplementary material.

\textbf{Evaluation}: Performance was evaluated using popular extreme classification metrics~\cite{Babbar19, Prabhu14, Prabhu18b, dahiya2020, You18, Liu17}, {\it i.e.,} Precision ($P@k$), nDCG ($N@k$) for $k= 1, 3, 5$. These metrics are defined in section~\ref{sup:eval} in the supplementary material. Furthermore, it was observed that documents were mapped to itself in some of the datasets. For instance, `Dinosaur' was tagged with `Dinosaur' label in the AmazonTitles-300K dataset. An algorithm can achieve disproportionately high precision@$1$ by just predicting such labels {\it a.k.a.} trivial predictions, without learning anything useful. Furthermore, such predictions do not add any utility to the quality of predictions in real-life applications. Hence, methods were not rewarded for trivial predictions. Table~\ref{tab:baelines_eval} reports numbers as per this evaluation strategy. All training times have been reported on a 6-core Intel Skylake 2.4 GHz machine with a single Nvidia V100 GPU.

\textbf{Baseline algorithms}: \alg was compared to deep extreme classifiers including XT~\cite{Wydmuch18}, DeepXML~\cite{dahiya2020}, AttentionXML~\cite{You18}, and MACH~\cite{Medini2019}. Furthermore, \alg was also compared to standard extreme classifiers based on sparse BoW features including Bonsai~\cite{Khandagale19}, DiSMEC~\cite{Babbar17}, Parabel~\cite{Prabhu18b}, AnnexML~\cite{Tagami17} and Slice~\cite{Jain19}. Note that Slice was trained with FastText~\cite{Bojanowski16} features while other methods used sparse BoW features. \alg was also compared to a Siamese network learned using triplet loss to embed similar items together. Unfortunately, X-Bert~\citep{Chang19} and GLaS~\citep{Guo2019} could not be included in the analysis as their code was not publicly available.

\textbf{Results on publicly available datasets}: Table~\ref{tab:baelines_eval} compares \alg with leading XML algorithms. Parabel~\citep{Prabhu18}, Bonsai~\citep{Khandagale19}, and AttentionXML~\citep{You18} are the most relevant methods to \alg as they generate a shortlist of negatives based on tree learned in label centroid space. \alg was found to be $5\%$ more accurate than Parabel, Bonsai, and AttentionXML. This indicates that metadata based probabilistic label trees can lead to significant accuracy gains as compared to the counterparts which do not utilize the label meta-data. \alg was also compared with other deep learning based approaches like DeepXML~\cite{dahiya2020}, MACH~\citep{Medini2019}, and XT~\cite{Wydmuch18}. \alg could be upto $4\%$ more accurate while being upto $10\times$ faster at prediction as compared to deep extreme classifiers on a single GPU. Siamese networks included in this analysis had access to the same label metadata information as \alg. However, \alg could be up to $12-18\%$ more accurate than a Siamese network at an extreme scale. Finally, \alg was also compared against other XML classifiers including Slice~\cite{Jain19}, PfastreXML~\cite{Jain17}, DiSMEC~\cite{Babbar17}, AnnexML~\cite{Tagami17}. \alg lead to $4-10\%$ more accurate predictions as compared to these algorithms. This demonstrates that learning tasks-specific features can lead to significantly more accurate predictions. It should be re-iterated that the focus of the paper is short-text documents. Nevertheless, the results on standard full-text versions of the datasets are included in Table~\ref{tab:sup:fulltext} in the supplementary material for the sake of completeness.

\textbf{Analysis}:
\alg encourages collaborative learning among labels which results in more accurate classifiers. Table~\ref{tab:examples} in the supplementary material include examples which demonstrate that \alg was able to predict labels such as ``DC connector", ``RCA connector," and ``Optical fiber connector" when other methods failed to do so. Note that these labels do not share any common training instance but are semantically similar. To better understand the nature of \alg's gains, the label set is divided into five uniform bins~(quantiles) based on the frequency of labels occurring in the training set. While \alg lead to gains in every quantile, the gains were more prominent on data-scarce labels, as demonstrated in Figure~\ref{fig:sup:contrib} in the supplementary material. In conclusion, data improvised tail labels are benefited from collaborative learning and give significant gains in overall accuracy.