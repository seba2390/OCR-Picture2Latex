\section{\alg: \algfull}
\label{sec:method}

\begin{figure}
    \centering
    \includegraphics[width=\columnwidth]{figures/fig03.pdf}
    \caption{\alg's frugal prediction pipeline scales to millions of labels. Given a document $\vx$, its text embedding $\hat\vx$ (see Fig~\ref{fig:embedding} (Left)) is first used by the shortlister $\cS$ to shortlist the most probable $\bigO{\log L}$ labels while maintaining high recall. The ranker $\cR$ then uses label classifiers (see Fig~\ref{fig:embedding} (Right)) of only the shortlisted labels to produce the final ranking.}
    \label{fig:architecture}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=0.91\columnwidth]{figures/fig01.pdf}
    \caption{(Left) \alg uses a lightweight architecture with a residual layer to embed both document and label text (see Fig.~\ref{fig:embedding}). (Right) Combination blocks are used to combine various representations (separate instances are used in the text embedding blocks ($\cE_D, \cE_L$) and in label classifiers ($\cC_L$)).}
    \label{fig:blocks}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=\columnwidth]{figures/fig02.pdf}
    \caption{(Left) Document text is embedded using an instance $\cE_D$ of the text embedding block (see Fig.~\ref{fig:blocks}). Stop words (e.g. \emph{and, the}) are discarded. (Right) \alg critically incorporates label text into classifier learning. For each label $l \in [L]$, a one-vs-all classifier $\vw_l$ is learnt by combining label text embedding $\hvz^1_l$ (using a separate instance $\cE_L$ of the text embedding block) and a refinement vector $\hvz^2_l$. Note that $\cE_D,\cE_L,\cC_L$ use separate parameters. However, all labels share the blocks $\cE_L, \cC_L$ and all documents share the block $\cE_D$.}
    \label{fig:embedding}
\end{figure}

\textbf{Summary}: \alg consists of three components 1) a lightweight text embedding block suitable for short-text applications, 2) 1-vs-All classifiers per label that incorporate label text, and 3) a shortlister that offers a high recall label shortlists for data points, allowing \alg to offer sub-millisecond prediction times even with millions of labels. This section details these components, and an approximate likelihood model with provable recovery guarantees, using which \alg offers a highly scalable yet accurate pipeline for jointly training text embeddings and classifier parameters.

%Accurate classification at the scale of millions of labels and documents is a challenging task, made more demanding by the prospect of jointly learning embeddings for those millions of document and label texts. In particular, its frugal text embedding module and a novel label classifier model encourage collaborative learning by exploiting label text information. 

\textbf{Notation}: Let $L$ be the number of labels and $V$ be the dictionary size. Each of the $N$ training points is presented as $(\vx_i,\vy_i)$. $\vx_i \in \bR^V$ is a bag-of-tokens representation for the $i\nth$ document i.e. $x_{it}$ is the TF-IDF weight of token $t \in [V]$ in the $i\nth$ document. $\vy_i \in \bc{-1,+1}^L$ is the ground truth label vector with $y_{il} = +1$ if label $l \in [L]$ is relevant to the $i\nth$ document and $y_{il} = -1$ otherwise. For each label $l \in [L]$, its label text is similarly represented as $\vz_l \in \bR^V$.

\textbf{Document and label-text embedding}: \alg learns $D$-dim embeddings for each vocabulary token i.e. $\vE = \bs{\ve_1,\ldots,\ve_V} \in \bR^{D \times V}$ and uses a light-weight embedding block (see Fig~\ref{fig:embedding}) to encode label and document texts. The embedding block $\cE = \bc{\vR,\valpha,\vbeta}$ is parameterized by a residual block $\vR \in \bR^{d \times d}$ and scaling constants $\valpha,\vbeta \in \bR^D$ for the combination block (see Fig~\ref{fig:blocks}). The embedding for a bag-of-tokens vector, say $\vr \in \bR^V$, is $\cE(\vr) = \sigma(\valpha) \odot \hat\vr^0 + \sigma(\vbeta) \odot (\vR\cdot \text{ReLU}(\hat\vr^0)) \in \bR^D$ where $\hat\vr^0 = \vE\vr$, $\odot$ denotes component-wise multiplication, and $\sigma$ is the sigmoid function. Document embeddings, denoted by $\hat\vx_i$, are computed as $\hat\vx_i = \text{ReLU}(\cE_D(\vx_i))$. Label-text embeddings, denoted by $\hat\vz^1_l$ are computed as $\hat\vz^1_l = \cE_L(\vz_l)$. Note that document and labels use separate instantiations $\cE_D, \cE_L$ of the embedding block. We note that \alg could also be made to use alternate text representations such as BERT~\cite{Devlin19}, attention~\cite{You18}, LSTM~\citep{hochreiter97} or convolution~\cite{Liu17}. However, such elaborate architectures negatively impact prediction time and moreover, \alg outperforms BERT, CNN and attention based XML techniques on all our benchmark datasets indicating the suitability of \alg's frugal architecture to short-text applications. 

\textbf{1-vs-All Label Classifiers}: \alg uses high capacity 1-vs-All (OvA) classifiers $\vW = [\vw_1,\ldots,\vw_L] \in \bR^{D \times L}$ that outperform tree- and embedding-based classifiers \cite{Chang20,Jain19,Babbar19,Prabhu18b,Yen17,Babbar17}. However, \alg distinguishes itself from previous OvA works (even those such as \cite{Chang20} that do use label text) by directly incorporating label text into the OvA classifiers. For each label $l \in [L]$, the label-text embedding $\hat\vz^1_l = \cE_L(\vz_l)$ (see above) is combined with a \emph{refinement} vector $\hat\vz^2_l$ that is learnt separately per label, to produce the label classifier $\vw_l = \sigma(\valpha_L) \odot \hat\vz^1_l + \sigma(\vbeta_L) \odot \hat\vz^2_l \in \bR^D$ where $\valpha_L, \vbeta_L \in \bR^D$ are shared across labels (see Fig~\ref{fig:embedding}). Incorporating $\hat\vz^1_l$ into the label classifier $\vw_l$ allows labels that never co-occur, but nevertheless share tokens, to perform learning in a collaborative manner since if two labels, say $l,m \in [L]$ share some token $t \in [V]$ in their respective texts, then $\ve_t$ contributes to both $\hat\vz^1_l$ and $\hat\vz^1_m$. In particular, this allows rare labels to share classifier information with popular labels with which they share a token. Ablation studies (Tab~\ref{tab:bowmeta},\ref{tab:sub:xmlclass},\ref{tab:combouv}) show that incorporating label text into classifier learning offers \alg significant gains of over 2-6\% compared to methods that do not use label text. Incorporating other forms of label metadata, such as label hierarchies, could also lead to further gains.

\textbf{Shortlister}: OvA training and prediction can be prohibitive, $\Om{NDL}$ and $\Om{DL}$ resp., if done naively. A popular way to accelerate training is to, for every data point $i \in [N]$, use only a \emph{shortlist} containing all positive labels (that are relatively fewer around $\bigO{\log L}$) and a small subset of the, say again $\bigO{\log L}$, most challenging negative labels \cite{Chang20,Jain19,Khandagale19,Prabhu18b,Yen17,Bhatia15}. This allows training to be performed in $\bigO{ND\log L}$ time instead of $\bigO{NDL}$ time. \alg learns a \emph{shortlister} $\cS$ that offers a label-clustering based shortlisting. We have $\cS = \bc{\cC, \vH}$ where $\cC = \bc{C_1,\ldots,C_K}$ is a balanced clustering of the $L$ labels and $\vH = [\vh_1,\ldots,\vh_K] \in \bR^{D \times K}$ are OvA classifiers, one for each cluster. Given the embedding $\hat\vx$ of a document and \emph{beam-size} $B$, the top $B$ clusters with the highest scores, say $\ip{\vh_{m_1}}{\hat\vx} \geq \ip{\vh_{m_2}}{\hat\vx} \geq \ldots$ are taken and labels present therein are shortlisted i.e. $\cS(\hat\vx) := \bc{m_1,\ldots,m_B}$. As clusters are balanced, we get, for every datapoint, $LB/K$ shortlisted labels in the clusters returned. \alg uses $K = 2^{17}$ clusters for large datasets.

\textbf{Prediction Pipeline}: Fig~\ref{fig:architecture} shows the frugal prediction pipeline adopted by \alg. Given a document $\vx \in \bR^V$, its embedding $\hat\vx = \text{ReLU}(\cE_D(\vx))$ is used by the shortlister to obtain a shortlist of $B$ label clusters $\cS(\hat\vx) = \bc{m_1,\ldots,m_B}$. Label scores are computed for every shortlisted label i.e. $l \in C_m, m \in \cS(\hat\vx)$ by combining shortlister and OvA classifier scores as $s_l := \sigma(\ip{\vw_l}{\hat\vx})\cdot\sigma(\ip{\vh_m}{\hat\vx})$. These scores are sorted to make the final prediction. In practice, even on a dataset with 1.3 million labels, \alg could make predictions within 0.2 ms using a GPU and 2 ms using a CPU.

\subsection{Efficient Training: the DeepXML Pipeline}
\textbf{Summary}: \alg adopts the scalable DeepXML pipeline \cite{Dahiya21} that splits training into 4 \emph{modules}. In summary, Module I jointly learns the token embeddings $\vE$, the embedding modules $\cE_D, \cE_L$ and shortlister $\cS$. Module II fine-tunes $\cE_D, \cE_L, \cS$, and retrieves label shortlists for all data points. After performing initialization in Module III, Module IV jointly learns the OvA classifiers $\vW$ and fine-tunes $\cE_D, \cE_L$ using the shortlists generated in Module II. Due to lack of space some details are provided in the \suppl\footnote{Supplementary Material Link: \color{blue}{\url{http://manikvarma.org/pubs/mittal21.pdf}}}

\textbf{Module I}: Token embeddings $\vE \in \bR^{D \times V}$ are randomly initialized using~\cite{he2015delving}, residual blocks within the blocks $\cE_D, \cE_L$ are initialized to identity, and label \emph{centroids} are created by aggregating document information for each label $l \in [L]$ as $\vc_l = \sum_{i:y_{il} = +1}\vx_i$. Balanced hierarchical binary clustering \cite{Prabhu18b} is now done on these label centroids for 17 levels to generate $K$ label clusters. Clustering labels using label centroids gave superior performance than using other representations such as label text $\vz_l$. This is because the label centroid carries information from multiple documents and thus, a diverse set of tokens whereas $\vz_l$ contains information from only a handful of tokens. The hierarchy itself is discarded and each resulting cluster is now treated as a \emph{meta-label} that gives us a \emph{meta} multi-label classification problem on the same training points, but with $K$ meta-labels instead of the original $L$ labels. Each meta label $m \in [K]$ is granted meta-label text as $\vu_m = \sum_{l \in C_m}\vz_l$. Each datapoint $i \in [N]$ is assigned a meta-label vector $\tilde\vy_i \in \bc{-1,+1}^K$ such that $\tilde y_{im} = +1$ if $y_{il} = +1$ for any $l \in C_m$ and $\tilde y_{im} = -1$ if $y_{il} = -1$ for all $l \in C_m$. OvA meta-classifiers $\vH = [\vh_1,\ldots,\vh_K] \in \bR^{D \times K}$ are learnt to solve this meta multi-label problem but are constrained in Module I to be of the form $\vh_m = \cE_L(\vu_m)$. This constrained form of the meta-classifier forces good token embeddings $\vE$ to be learnt that allow meta-classification without the assistance of powerful refinement vectors. However, this form continues to allow collaborative learning among meta classifiers based on shared tokens. Module I solves the meta multi-label classification problem while jointly training $\cE_D, \cE_L, \vE$ (implicitly learning $\vH$ in the process).

\textbf{Module II}: The shortlister is fine-tuned in this module. Label centroids are recomputed as $\vc_l = \sum_{i:\vy^i_l = +1}\vE\vx_i$ where $\vE$ are the task-specific token embeddings learnt in Module I. The meta multi-label classification problem is recreated using these new centroids by following the same steps outlined in Module I. Module II uses OvA meta-classifiers that are more powerful and resemble those used by \alg. Specifically, we now have $\vh_m = \sigma(\tilde\valpha_P)\odot\hat\vu^2_m + \sigma(\tilde\vbeta_P)\odot\hat\vu^1_m$ where $\hat\vu^1_m = \sum_{l \in C_m}\cE_L(\vz_l)$ is the meta label-text embedding, $\hat\vu^2_m$ are meta label-specific refinement vectors, and $\cC_P = \bc{\tilde\valpha_P, \tilde\vbeta_P}$ is a fresh instantiating of the combination block. Module II solves the (new) meta multi-label classification problem, jointly learning $\cC_P, \hat\vu^2_m$ (implicitly updating $\vH$ in the process) and fine-tuning $\cE_D, \cE_L, \vE$. The shortlister $\cS$ so learnt is now used to retrieve shortlists $\cS(\vx_i)$ for each data point $i \in [N]$.

\textbf{Module III}: Residual blocks within $\cE_D, \cE_L$ are re-initialized to identity, $\cS$ is frozen and combination block parameters for the OvA classifiers are initialized to $\valpha_L = \vbeta_L = \vzero$ (note that $\sigma(\vzero) = 0.5\cdot\vone$ where $\vone$ is the all-ones vector). Refinement vectors for all $L$ labels are initialized to $\hat\vz^2_l = \vE\vz_l$. Ablation studies (see Tab~\ref{tab:combouv}) show that this refinement vector initialization offers performance boosts of up to 5-10\% compared to random initialization as is used by existing methods such as AttentionXML \cite{You18} and the X-Transformer \cite{Chang20}. 

\textbf{Module IV}: This module performs learning using an approximate likelihood model. Let $\Theta = \bc{\vE, \cE_D, \cE_L, \cC_L, \vW}$ be the model parameters in the \alg architecture. We recall that $\cC_L$ are combination blocks used to construct the OvA classifiers and meta classifiers, and $\vE$ are the token embeddings. OvA approaches assume a likelihood decomposition such as $\P{\vy_i \cond \vx_i, \Theta} = \prod_{l=1}^L\P{y_{il} \cond \hat\vx_i, \vw_l} = \prod_{l=1}^L\br{1 + \exp\br{-y_{il}\cdot\ip{\hat\vx_i}{\vw_l}}}^{-1}$. Here $\hat\vx_i = \text{ReLU}(\cE_D(\vx_i))$ is the document-text embedding and $\vw_l$ are the OvA classifiers as shown in Fig~\ref{fig:embedding}. Let us abbreviate $\ell_{il}(\Theta) = \ln\br{1 + \exp\br{-y_{il}\cdot\ip{\hat\vx_i}{\vw_l}}}$. Then, our objective is to optimize $\argmin_{\Theta} \cL(\Theta)$ where
\[
\cL(\Theta) = \frac1{NL}\sum_{i \in [N]}\sum_{l \in [L]}\ell_{il}(\Theta)
\]
However, performing the above optimization exactly is intractable and takes $\Om{NDL}$ time. \alg's solves this problem by instead optimizing $\argmin_\Theta \tilde\cL(\Theta \cond \cS)$ where
\[
\tilde\cL(\Theta \cond \cS) = \frac{K}{NLB}\sum_{i \in [N]}\sum_{l \in \cS(\hat\vx_i)}\ell_{il}(\Theta)
\]
Recall that for any document, $\cS(\hat\vx_i)$ is a shortlist of $B$ label clusters (that give us a total of $LB/K$ labels). Thus, the above expression contains only $NLB/K \ll NL$ terms as \alg uses a large fanout of $K \approx 130$K and $B \approx 100$. The result below assures us that model parameters and embeddings obtained by optimizing $\tilde\cL(\Theta \cond \cS)$ perform well w.r.t. the original likelihood $\cL(\Theta)$ if the dataset exhibits label sparsity, and the shortlister assures high recall. 

\begin{theorem}
\label{thm:thm}
Suppose the training data has label sparsity at rate $s$ i.e. $\sum_{i \in [N]}\sum_{l \in [L]} \bI\bc{y_{il} = +1} = s\cdot NL$ and the shortlister offers a recall rate of $r$ on the training set i.e. $\sum_{i \in [N]}\sum_{l \in \cS(\hat\vx_i)}\bI\bc{y_{il} = +1} = rs\cdot NL$. Then if $\hat\Theta$ is obtained by optimizing the approximate likelihood function $\tilde\cL(\Theta \cond \cS)$, then the following always holds
\[
\cL(\hat\Theta) \leq \min_{\Theta} \cL(\Theta) + \bigO{s(1-r)\ln(1/(s(1-r)))}.
\]
\end{theorem}
Please refer to Appendix~A.1 in the \suppl for the proof. As $s \rightarrow 0$ and $r \rightarrow 1$, the excess error term vanishes at rate at least $\sqrt{s(1-r)}$. Our XML datasets do exhibit label sparsity at rate $s \approx 10^{-5}$ and Fig~\ref{fig:recall_clusters} shows that \alg's shortlister does offer high recall with small shortlists (80\% recall with $\approx 50$-sized shortlist and 85\% recall with $\approx 100$-sized shortlist). Since Thm~\ref{thm:thm} holds in the completely agnostic setting, it establishes the utility of learning when likelihood maximization is performed only on label shortlists with high-recall. Module IV uses these shortlists to jointly learn the $L$ OvA classifiers $\vW$ and $\cC_L$, as well as fine-tune the embedding blocks $\cE_D, \cE_L$ and token embeddings $\vE$.

%\textbf{Efficient Two-Stage Training}: \alg performs training in two stages to efficiently optimize model parameters and token embeddings. In Stage I, firstly, token embeddings $E$ are initialized using fastText~\citep{Joulin17} and the coarse shortlister clusters are obtained. Next, the shortlister model parameters $\tilde\Theta$ and the embeddings $E$ are jointly optimized to maximize accuracy on the meta-label classification task. From hereon, these (task-specific) token embeddings are frozen. In Stage II, first the shortlister is fine-tuned using the frozen embeddings $E$ and frozen. Finally, conditioned on the shortlister and the embeddings, the model parameters in $\Theta$ are optimized by solving $\argmin_{\Theta} \tilde\cL(\Theta \cond E, \cS)$. Below, we describe a variant \algp that fine-tunes $E$ jointly with the model parameters $\Theta$ in Stage II i.e. optimizes $\argmin_{\Theta, E} \tilde\cL(\Theta, E \cond \cS)$ instead. This variant offers greater accuracy but is more expensive to train.

\textbf{Loss Function and Regularization}: Modules I, II, IV use the logistic loss and the Adam~\cite{Kingma14} optimizer to train the model parameters and various refinement vectors. Residual layers used in the text embedding blocks $\cE_D, \cE_L$ were subjected to spectral regularization \cite{Miyato18b}. All ReLU layers were followed by a dropout layer with 50\% drop-rate in Module-I and 20\% for the rest of the modules.

%\textbf{Initialization and regularization}: model parameters $\Theta, \tilde\Theta$ as well as token embeddings $E$ were learnt using the Adam~\cite{Kingma14} optimizer. We observed that learning the token embedding vectors $\ve^t, t \in [T]$ and the refinement vectors $\vu^l, l \in [L]$ and $\tilde\vu^m, m \in [K]$ for the OvA classifier and shortlister respectively is challenging and offers poor performance if initialization is done randomly. To mitigate this, token embeddings were initialized using FastText~\citep{Joulin17} in Stage I, as well as $\vu^l, \tilde\vu^m$ were always initialized to $\vv^l, \tilde\vv^m$ respectively so that these refinement vectors could be learnt speedily. We note that existing methods such as AttentionXML \cite{You18} and X-Transformer \cite{Chang20} also initialize token embeddings using Glove~\citep{pennington14} and XLNet \cite{Ye20}, but they initialize classifier model vectors randomly. Ablation studies (see Tab~\ref{tab:combouv}) show that non-random initialization can offer performance boosts of upto 6-8\%. The residual block $R$ was subjected to spectral regularization \cite{Miyato18b} and all ReLU layers in the architecture also included a dropout layer with 20\% rate.

\textbf{Ensemble Learning}: \alg learns an inexpensive ensemble of 3 instances (see Figure~\ref{fig:numlearner}). The three instances share Module I training to promote scalability i.e. they inherit the same token embeddings. However, they carry out training Module II onwards independently. Thus, the shortlister and embedding modules get fine-tuned for each instance.

% We also experiment with a more elaborate ensemble \algp where instances carry out both stages independently and fine-tune token embeddings jointly with model parameters in stage II. \algp consistently offers better performance than \alg but is also more expensive to train.

\textbf{Time Complexity}: Appendix~A.2 in the \suppl presents time complexity analysis for the \alg modules.