%\pdfoutput=1

\documentclass{article}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage[colorlinks=true,citecolor=blue,linkcolor=black]{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{xcolor}         % colors

%\usepackage{geometry}
\usepackage{float}
\usepackage{proof}
\usepackage{amsmath,amssymb,amsthm}
\usepackage{float}
\usepackage{mathrsfs}
%\usepackage[alphabetic]{amsrefs}%alphabetical reference
\usepackage{bbm}%\mathbbm{f}
\usepackage{stmaryrd}%\llbracket
\usepackage{braket}%inner product
\usepackage{graphicx}
\usepackage{comment}
\usepackage[ruled,vlined]{algorithm2e}
\usepackage{subfigure}
\usepackage{wrapfig}

\usepackage{fullpage}
%\geometry{a4paper,scale=0.75}
%\newcommand{\Rmnum}[1]{\expandafter\@slowromancap\romannumeral #1@}
\newcommand{\Rnum}[1]{\uppercase\expandafter{\romannumeral #1} }
\makeatletter
\newcommand{\rmnum}[1]{\romannumeral #1}
\newcommand{\Rmnum}[1]{\expandafter\@slowromancap\romannumeral #1@}
\makeatother


\newtheorem{definition}{Definition}
\newtheorem{theorem}{Theorem}
\newtheorem{lemma}{Lemma}
\newtheorem{proposition}[lemma]{Proposition}
\newtheorem{claim}[lemma]{Claim}
\newtheorem{fact}[lemma]{Fact}
\newtheorem{remark}[lemma]{Remark}
\newtheorem{corollary}[lemma]{Corollary}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to LaTeX to determine where to break the
% lines. Using \AND forces a line break at that point. So, if LaTeX puts 3 of 4
% authors names on the first line, and the last on the second line, try using
% \AND instead of \And before the third author name.
\title{Robust and Fully-Dynamic Coreset for Continuous-and-Bounded Learning (With Outliers) Problems}

\author{%
	Zixiu Wang$^1$\thanks{Part of this work was done when Zixiu Wang was an intern under the supervision of Yiwen Guo at ByteDance.} \quad Yiwen Guo$^2$ \quad Hu Ding$^1$\thanks{Corresponding author.} \\
	{\small$^1$School of Computer Science and Technology,}\\
	{\small University of Science and Technology of China}\\
	{\small$^2$ByteDance AI Lab}\\
	{\small \texttt{wzx2014@mail.ustc.edu.cn, guoyiwen89@gmail.com, huding@ustc.edu.cn}}
}
\date{\vspace{-1cm}}


\begin{document}
\allowdisplaybreaks 

\maketitle
\begin{abstract}
In many machine learning tasks, a common approach for dealing with large-scale data is to build a small summary, {\em e.g.,} coreset,  that can efficiently represent the original input. However, real-world datasets usually contain outliers and most existing coreset construction methods are not resilient against outliers (in particular, an outlier can be located arbitrarily in the space by an adversarial attacker). In this paper, we propose a novel robust coreset method for the {\em continuous-and-bounded learning} problems (with outliers) which includes a broad range of popular optimization objectives in machine learning, {\em e.g.,} logistic regression and $ k $-means clustering. Moreover, our robust coreset  can be efficiently maintained in fully-dynamic environment. To the best of our knowledge, this is the first robust and fully-dynamic coreset construction method for these optimization problems. Another highlight is that our coreset size can depend on the doubling dimension of the parameter space, rather than the VC dimension of the objective function which could be very large or even challenging to compute. Finally, we conduct the experiments on real-world datasets to evaluate the effectiveness of our proposed robust coreset method. 

\end{abstract}


\section{Introduction}
%With the increasing scale of datasets used in machine learning, traditional algorithms with super-linear running time are no longer suitable. 

As the rapid increasing of data volume in this big data era, we often need to develop low-complexity ({\em e.g.,} linear or even sublinear) algorithms for machine learning tasks.  
Moreover, our dataset is often maintained in a dynamic environment so that we have to consider the  issues like  data insertion and deletion. For example, as mentioned in the recent article~\cite{GinartGVZ19}, Ginart {\em et al.} discussed the scenario that some sensitive training data  have to be deleted due to the reason of privacy preserving. Obviously, it is prohibitive to re-train our model when the training data is changed dynamically, if the data size is extremely large. To remedy these issues, a natural way is to construct a small-sized summary of the training data so that we can run existing algorithms on the summary rather than the whole data. \textbf{Coreset}~\cite{core-survey}, which was originally studied in the community of computational geometry~\cite{AgarwalHV04}, has become a widely used data summary for many large-scale machine learning problems~\cite{BravermanFL16,HugginsCB16,LucicFKF17,MunteanuSSW18,MirzasoleimanCL20,DBLP:conf/icml/HuangHLFD21}. 
%It is reasonable to compress the data before running the algorithm for less time/space burden. Coreset~\cite{core-survey}, as a succinct data compression technique, has been widely used together with many machine learning algorithms. 
As a succinct data compression technique, coreset also enjoys several nice properties. For instance, coreset is usually composable and thus can be applied in the environment like distributed computing~\cite{DBLP:conf/pods/IndykMMM14}. Also, it is  usually able to obtain small coresets  for streaming algorithms~\cite{Har-PeledM04,Chen09} and fully-dynamic algorithms  with data insertion and deletion~\cite{DBLP:journals/dcg/Chan09,HenzingerK20}. 


However, the existing coreset construction methods are still far from being satisfactory in practice. A major bottleneck is that most of them are sensitive to outliers. We are aware that 
real-world dataset is usually noisy and may contain outliers; note that the outliers can be located arbitrarily in the space and even a single outlier can significantly destroy the final machine learning result.  A typical example is poisoning attack, where an adversarial attacker may inject several specially crafted samples into the training data which can make the decision boundary severely deviate and cause unexpected misclassification~\cite{BiggioR18}. In the past decades, a number of algorithms have been proposed for solving optimization with outliers problems, like clustering~\cite{CharikarKMN01,Chen08,Chawla2013kmeansAU,GuptaLSM2017,k-Means+++}, regression~\cite{books/wi/RousseeuwL87,MountNPSW14,DBLP:conf/icml/DingW20}, and PCA~\cite{DBLP:journals/jacm/CandesLMW11}. 

To see why the existing coreset methods are sensitive to outliers, we can take the popular sampling based coreset framework~\cite{FeldmanL11} as an example. The framework needs to compute a ``sensitivity'' for each data item, which measures the importance degree of the data item to the whole data set; however, it tends to assign high sensitivities to the points who are far from the majority of the data, that is, an outlier is likely to have a high sensitivity and thus has a high chance to be selected to the coreset. Obviously, the coreset obtained by this way is not pleasant since we expect to contain more inliers rather than outliers in the coreset. It is also more challenging to further construct a fully-dynamic robust coreset. The existing robust coreset construction methods~\cite{FeldmanL11,HuangJLW18} often rely on simple uniform sampling and are efficient only when the number of outliers is a constant factor of the input size (we will discuss this issue in Section~\ref{sec-wu}). 
%At the same time, the existing coresets construction methods, such as the popular sensitivity-based method~\cite{core-survey}, are still far from satisfactory when dealing with outliers. It is worth noting that even one outlier can significantly destroy the final result (like clustering and classification)~\cite{BiggioR18}. 
Note that other 
%importance sampling framework to construct coreset can not deal with outliers well. Also, a 
outlier-resistant data summary methods like~\cite{GuptaLSM2017,ChenA018} usually yield large approximation factors 
%does not have some advantages like 
and are not easy to be maintained in a fully dynamic scenario, to our knowledge.

%\textcolor{gray}{Coreset is a tiny data summary such that the model trained on the coreset are competitive with the one trained on the entire data. Thanks to the small size compared to the original data, coresets can reduce the training time significantly. In addition, coreset has the following three benefits: (i) Coreset construction is modularized. i.e., the construction is independent of the algorithms running on it. Thus all algorithms can be accelerated via the coreset. (ii) Coreset is usually decomposable. Informally speaking, the union of coresets is a coreset of the union of the original data. Hence we can divide the total data into several disjoint parts, and build the coreset in the distributed and parallel setting. (iii) Coreset has hierarchical property. If $ C_1 $ is a coreset of $ C_2 $ and $ C_2 $ is a coreset of $ C_3 $, then $ C_1 $ is a coreset of $ C_3 $. This enables us to build the coreset hierarchically which implying a quick reconstruction. All these benefits convince us that coreset is a hopeful technique for handling dynamic data.}\par  
%Large dataset has becoming increasingly common in the machine learning task over the past few years. Massive data helps us train models better but bring about heavy computational burden at the same time. In this context, some traditional algorithms with quadratic or more time complexity are not efficient enough. Faster algorithms with linear or even sub-linear (scalable) complexity are required. 
%Most training tasks in machine learning are loss based. More specifically, there is an objective function of the model, whose value is dependent on the parameter to learn and the input training data. Moreover, the objective function usually has the form that is the sum (average) of the costs of individual data points. (e.g., mean squared error (MSE), maximum likelihood estimation (MLE) and empirical risk minimization (ERM).)  Therefore, we can run the existing algorithms on the coreset instead. This really reduces both the space and time complexity. it means that if we merge the coresets of disjoint datasets, then we will obtain the coreset of total data. 

\subsection{Our Contributions}
\label{sec-ourcon}
%\textcolor{gray}{Before designing a coreset-based framework for dynamic model, we should overcome two shortages of most current coreset methods. First, coresets are often sensitive to outliers. Research on the outlier-resistant coreset is quite limited. Second, high quality coreset are mainly restrained to clustering-style problems. Furthermore, there are optimization problems that admit no small coreset~\cite{MunteanuSSW18}. We are going to briefly show that the two obstacles can be solved simultaneously with a rational relax by the example.}

%\begin{figure}[ht]
%	\vskip -0.01in
%	\centering
%	\includegraphics[width=\linewidth]{fig/region-outlier}
%	\vskip -0.15in
%	\caption{For the illustrated $1$-median clustering problem with three outliers, it is sufficient and more robust to consider solutions only in the orange region. If a center closed to the outliers is chosen, the outliers will be mistakenly considered as inliers and vice versa. It shows that (b) the clustering center in a rational region helps to detect outliers (colored in red) correctly, (a) while a improperly chosen clustering center leads to incorrect outlier detection.}
%	\label{region-outlier}
%	%	\vskip -0.2in
%\end{figure}

In this paper, we propose a  \textbf{unified fully-dynamic robust coreset framework} for a class of optimization problems which is termed \emph{continuous-and-bounded (CnB) learning}. This type of learning problems covers a broad range of optimization objectives in machine learning~\cite[Chapter 12.2.2]{understandingML}. Roughly speaking, ``CnB learning'' requires that the optimization objective is a continuous function ({\em e.g.,} smooth or Lipschitz), and meanwhile the solution is restricted within a bounded region. 
%a large range of learning problems named  \emph{continuous-and-bounded learning}. The solution space of the continuous-and-bounded learning problem is 
%a ball of bounded radius. 
We emphasize that this ``bounded'' assumption  is quite natural in real machine learning scenarios.  
%of this assumption is inspired by the following two key observations. First, for many optimization problems, their solution domains usually are able to be restricted to some reasonably small regions, though their global feasible domains can be large. 
To shed some light, we can consider running an iterative algorithm ({\em e.g.,} the popular gradient descent or expectation maximization) for optimizing some objective; the solution is always restricted within a local region except for the first few rounds. 
%the unsupervised optimization problem, {\em facility location}, as an example. Suppose we are going to build several supermarkets in a city. Though there are many available candidate locations in the city, we often restrict our choice within some specific districts due to some  practical factors. 
Moreover, it is also reasonable to bound the solution range in a dynamic environment because one single update (insertion or deletion) is not likely to dramatically change the solution.  
%Although the available locations is the entire city, the truly feasible locations are inside those densely populated areas.
%To shed some light on that, we consider the simple $1$-median problem with outliers. The median point can be located arbitrarily in the space, however, it is sufficient to consider a local range only (see Figure~\ref{region-outlier}). 
%In addition, the widely used regularization can be also viewed as a restriction on the solution domain. 
%Another observation comes from the nature of dynamic data stream. 
%The updates often arrive one by one, where a single update is not quite likely to dramatically change the solution. 
%So it makes sense to assume a small region (similar with the first observation) for the solution at each time spot of a dynamic data stream. 

Our coreset construction is a novel \textbf{hybrid framework}. First, we suppose that there exists an ordinary coreset construction method $\mathcal{A}$ for the given CnB optimization objective (without considering outliers). Our key idea is to classify the input data into two parts: the ``suspected'' inliers and the ``suspected'' outliers, where the ratio of the sizes of these two parts is a carefully designed parameter $\lambda$. For the ``suspected'' inliers, we run the method $\mathcal{A}$ (as a black box); for the ``suspected'' outliers, we directly take a small sample uniformly at random; finally, we prove that these two parts together yield a robust coreset. Our framework can be also efficiently implemented under the merge-and-reduce framework for dynamic setting (though the original merge-and-reduce framework is not designed for the case with outliers)~\cite{BentleyS80,Har-PeledM04}. A cute feature of our framework is that we can easily tune the parameter $\lambda$ for updating our coreset dynamically, if the fraction of outliers is changed in the dynamic environment.


The other contribution of this paper is that we propose two different coreset construction methods for CnB optimization objectives ({\em i.e.,} the aforementioned black box $\mathcal{A}$). 
The first method is based on the importance sampling framework~\cite{FeldmanL11}, and the second one is based on a space partition idea. Our coreset sizes depend on the doubling dimension of the solution space rather than the VC (shattering) dimension. This property is particularly useful if the VC dimension is too high or not easy to compute, or considering the scenarios like sparse optimization (the domain of the solution vector has a low doubling dimension). To our knowledge, the only existing coreset construction methods that depend on doubling dimension are from Huang {\em et al.}~\cite{HuangJLW18} and Cohen-Addad {\em et al.}~\cite{DBLP:conf/stoc/Cohen-AddadSS21}, but their results are only for clustering problems. Our methods can be applied for a broad range of widely studied optimization objectives, such as  
logistic regression~\cite{MunteanuSSW18}, Bregman clustering~\cite{BanerjeeMDG05}, and truth discovery~\cite{DBLP:journals/sigkdd/LiGMLSZFH15}. It is worth noting that although some coreset construction methods for them have been proposed before ({\em e.g.,}~\cite{DBLP:conf/aistats/LucicBK16,MunteanuSSW18,tolochinsky2018generic,DBLP:conf/icml/DingW20,DBLP:conf/icml/HuangHLFD21}), they are all problem-dependent and we are the first, to the best of our knowledge, to study them from a unified ``CnB'' perspective. 


%We design a general framework to compute the robust coreset for continuous-and-bounded learning problems, which can resolve the aforementioned issues. Given a coreset method, we show how to turn it into a robust coreset method. We also give several coreset methods for the continuous-and-bounded learning problems as a complement. Furthermore, our robust coreset is convenient to be adapted to be fully-dynamic for streaming data (the number of outliers can be also updated at any time). Hence it is worth mentioning that the fully-dynamic coreset provides a practicable technique for those learning and unlearning problems.
%Our framework also enjoys several other benefits. First, our method can handle a broad range of objective functions. Second, the parameter space is not necessary a Euclidean space, e.g., in a clustering problem, the data items may come from an abstract metric and the parameters (the cluster centers) are also located in this metric space.
%Our coreset size depends on the doubling dimension of the parameter space, which is a natural generalization of Euclidean dimension~\cite{ColeG06}.


%The size of our coreset is dependent on the doubling dimension of the solution space.
%In addition, we prove that, for smooth functions, a constructed coreset can be utilized to further build a robust (i.e., outlier-resistant) coreset over the rational region. Robust coreset cannot directly be hierarchically constructed by using the merge-reduce technique because we do not know the accurate number of outliers in each part. The hybird robust coreset in this work overcome this defect. To our knowledge, this is the first coreset method that can handling outlier-including dynamic data. Furthermore, the number of outliers can also be updated in the streaming operation.



%In this paper, we study how coreset construction can be efficient and effective with dynamic data pipelines and outliers.
%In many practical machine learning tasks, the solution is constrained in a small region. e.g., the optimization problem with regularization can be considered as an optimization problem (without regularization) demanding a solution with bounded norm. Besides, the solution we are interested in is often a small subset of all feasible solutions.
%We illustrate our motivation by the following instance of clustering.
%It is a $1$-median clustering~\cite{CharikarGTS99} problem whose objective function is $L(c,X)=\sum_{x\in X} \|x-c\|_1$, where $c$ is a possible solution center. A typical coreset $C$ requires $L(c,C)\approx L(c,X)$ for any feasible $c$ in the solution space. 
%We advocate relaxing the requirement of ``any feasible solution'' for coreset to a ball of radius $l$ centered at $\tilde{\theta}$ in the solution space, termed a ``rational region''. How such a rational region is beneficial in practice can be seen in Figure~\ref{region-outlier}, and it also makes great sense with the dynamic data pipeline where $\tilde{\theta}$ can be considered as a solution obtained previously.  \par

%Motivated by these observations, we propose a novel method for constructing coresets with additive error, for machine learning problems 


%Figure \ref{region-outlier} is an intance of $ 1$-median whose objective function is $ L(c,X)=\sum_{x\in X} \mathtt{dist}(c,x) $, where $ c $ is the solution center. A typical coreset $ C $ requires that $ L(c,C)\approx L(c,X) $ for any solution $ c $ in the total space. But sometimes this could be too strong and a smaller solution region (the orange shade in the figure) is sufficient in practice. The influence of solution region is more apparent when outliers exist. As shown in Figure \ref{region-outlier}, for such center that is closed to the outliers, the real outliers will be considered as inliers and real inliers that is far from $ c $ will be mistaken as outliers.\par 
%In this paper, we relax the solution requirement of the coreset to a ball of radius $ l $ centered at $ \tilde{\theta} $ in the solution space, which we call ``rational region''. This really makes sense in the dynamic model where $ \tilde{\theta} $ can be considered as a pre-solution computed on the previous datasets. And $ l $ measures how big the rational region is. Under this rational relaxing, we propose a method to construct coreset with additive error, for optimizations having smooth objective functions. The size of our coreset is dependent on the doubling dimension of the solution space, different from common pseudo-dimension~\cite{FeldmanL11} or VC dimension~\cite{BravermanFL16}. Also, we prove that for smooth functions, a coreset leads to a robust (i.e., outlier-resistant) coreset over the resonable region. And we propose a framework for handling the dynamic model efficiently based on our coreset. To our knowledge, this is the first framework for dealing with outlier-including dynamic data pipelines.

%\subsection{Related Work}
%Also, the problems for outliers is still not well understood. Algorithms dealing with outliers are usually heuristic~\cite{Chawla2013kmeansAU}, quite involved~\cite{Chen08,GuptaLSM2017} or have specific constraints~\cite{MountNPSW14,ChenA018,BhaskaraVX19,DingW20}. 
%There is no constant factor approximation algorithm for $ k $-means with outliers until~\cite{KrishnaswamyLS18}.

%In the past decades, the coreset technique has been extensively studied   for the problems like clustering~\cite{BadoiuHI02,Har-PeledM04,Har-PeledK05,Chen09,FeldmanL11,FeldmanSS13,SohlerW18,HuangJLW18,HuangV20}, regression~\cite{DasguptaDHKM08,BoutsidisDM13,HugginsCB16,ZhengP17,MunteanuSSW18,Chhaya0S20}, and many other optimization problems~\cite{LucicFKF17,PhillipsT18a,PhillipsT18b,BravermanFLR19}. 
%Some robust coreset construction methods were also proposed, such as~\cite{HuangJLW18,MountNPSW14,ChenA018,DingW20}. But most of them are problem-dependent and cannot be used a general tool; also, as mentioned before, it is challenging to adapt them to be full-dynamic.  


%Nevertheless, the research on robust coreset handling outliers is relative limit and is confined to uniform sampling~\cite{HuangJLW18,MountNPSW14}. Existing methods for dealing with outliers include~\cite{Chawla2013kmeansAU,Chen08,GuptaLSM2017,MountNPSW14,ChenA018,BhaskaraVX19,DingW20} are often heuristic or have extra constraints.

% the other hand, ~\cite{BachemL018} presented another kind of coreset for hard and soft clustering based on Bregman divergences, appending an additional additive error. \par
%The connection between coreset and streaming algorithm is natural. The construction of small coreset can lead to a streaming algorithm in the insertion-only input model due to~\cite{Har-PeledM04,BentleyS80}. 

%The algorithms for dynamic data stream have been studied in many articles like~\cite{KarasuyamaT09,TsaiLL14,Schelter20}. In particular,


%Ginart {\em et al.}~\cite{GinartGVZ19} formally defined the ``deletion efficient problem'' in machine learning.
%And in that work, they presented deletion-efficiently algorithms for $ k $-means. 
%However, the research on deletion-efficiently algorithms is still quite limited compared with the results for incremental (insertion-only) streaming model.  
%For clustering problems in  Euclidean space, Frahling and   Sohler~\cite{FrahlingSohler05} designed a dynamic coreset method that can support deletion; their work was recently improved by~\cite{BravermanFLSY17} and ~\cite{hu2019nearly}. Very recently, the general fully-dynamic (but not robust) coreset  was proposed by Henzinger and   Kale~\cite{HenzingerK20}. 
%As for the space complexity, recently~\cite{HenzingerK20} showed that  even for $ 1 $-median/means, $ \Omega(n) $ space is necessary if allowing deletion.

\section{Preliminaries}\label{sec:pre}
%\paragraph{Problem Settings}

We introduce several important notations used throughout this paper. 
Suppose $\mathcal{P}$ is the parameter space. Let $X$ be the input data set that contains $n$ items in a metric space $\mathcal{X}$, and each $x\in X$ has a weight $w(x)\geq 0$. Further, we use $ (X,z) $ to  denote a given instance $X$ with $z$ outliers. 
We always use $|\cdot|$ and $\llbracket \cdot \rrbracket$ respectively to denote the number of data items and the total weight of a given data set. We consider the learning problem whose objective function is the weighted sum of the cost over $X$, {\em i.e.}, 
\begin{eqnarray}
 f(\theta,X):=\sum_{x\in X} w(x) f(\theta,x),\label{for-obj} 
\end{eqnarray}
where $f(\theta,x)$ is the non-negative cost contributed by $x$ with the parameter vector $\theta\in \mathcal{P}$. The goal is to find an appropriate $\theta$ so that the objective function $f(\theta,X)$ is minimized. Usually we assume each $x\in X $ has unit weight ({\em i.e.,}  $ w(x)=1 $), and it is straightforward  to extend our method to weighted case. Given the pre-specified number $ z\in\mathbb{Z}^+ $ of outliers in $ X $ (for weighted case, ``$z$'' refers to the total weight of outliers), we then define the ``robust'' objective function: 
\begin{equation}
    f_{z}(\theta,X) := \min_{O\subset X, \llbracket O \rrbracket = z}\ {f}(\theta,X\backslash O).\label{for-obj2} 
\end{equation}
Actually, the above definition~(\ref{for-obj2}) comes from the popular ``trimming'' idea~\cite{books/wi/RousseeuwL87} that has been widely used for robust optimization problems. 
%Sometimes we also use the average forms of (\ref{for-obj}) and (\ref{for-obj2}): $\mathbbm{f}(\theta,X):=\frac{f(\theta,X)}{\llbracket X \rrbracket}$ and $ \mathbbm{f}_{z}(\theta,X):=\frac{1}{|X|-z}f_{z}(\theta,X) $. 

%Several other notations used throughout this paper is shown in Table \ref{notation}. 

%which does not change rapidly. It is common to quantitatively measure such functions by using the language of Lipschitz continuity.
%\begin{table}[h]
%	\caption{List of Notations}
%	\label{notation}
%	%\vskip 0.08in
%	\centering
%	\begin{tabular}{|c|l|}
%		\hline
%	%	$X$   & the input dataset   \\ \hline
%	%	$x$    & any data item of $X$        \\ \hline
%	%	$w(x)\in\mathbb{R}_+$  & the weight of  $x$   \\ \hline
%		$ |Y|$  & the number of data items in a set $Y$  \\ \hline
%		$\llbracket Y \rrbracket$   & the total weight of the data items in $Y$ \\ \hline
%	%	$\theta$ &  the parameter vector to learn in the model \\ \hline
%		%$\mathcal{P}$  & parameter space  \\ \hline
%	%	$\mathcal{B}$  & the ball of radius $ \ell $ centered at $ \tilde{\theta} $ as described in Definition~\ref{def:CB-learning}  \\ \hline
%	%	$f(\theta,x) $ & the cost of $x$ w.r.t. $\theta$  \\ \hline
%	%	$f(\theta,X):=\sum_{x\in X} w(x) f(\theta,x)$  & the objective function \\ \hline
%		$\mathbbm{f}(\theta,X):=\frac{f(\theta,X)}{\llbracket X \rrbracket}$ & the average form of $f(\theta,X)$  \\ \hline
%		%$f_{z}(\theta,X) := \min_{\llbracket O \rrbracket = z}\ {f}(\theta,X\backslash O) $ & the objective  with $ z $ outliers \\ \hline
%		$ \mathbbm{f}_{z}(\theta,X):=\frac{1}{|X|-z}f_{z}(\theta,X) $ & the average form of $f_{z}(\theta,X)$ \\ \hline
%	\end{tabular}
%\end{table}

%Below we introduce three types of continuous-and-bounded learning problems.

Below we present the formal definition of continuous-and-bound learning problem. A function $ g:\mathcal{P}\rightarrow \mathbb{R} $ is \emph{$ \alpha $-Lipschitz continuous} if for any $ \theta_1,\theta_2 \in \mathcal{P} $, $ |g(\theta_1)-g(\theta_2)|\leq \alpha \|\Delta\theta \| $, where $ \Delta\theta= \theta_1-\theta_2 $ and $ \|\cdot\| $ is some specified norm in $ \mathcal{P} $. 

\begin{definition}[Continuous-and-Bounded (CnB) Learning~\cite{understandingML}]\label{def:CB-learning} 
Let $\alpha$, $\ell>0$, and $\tilde{\theta}\in \mathcal{P}$. Denote by $\mathbb{B}(\tilde{\theta}, \ell)$ the ball centered at $\tilde{\theta}$ with radius   $ \ell $ in the parameter space $\mathcal{P}$.	An objective (\ref{for-obj}) is called a CnB  learning problem with the parameters $( \alpha,\ell, \tilde{\theta} )$ if (\rmnum{1}) the loss function $ f(\cdot,x) $ is $\alpha $-Lipschitz continuous for any $x\in X$, and (\rmnum{2})  $\theta $ is always restricted within $\mathbb{B}(\tilde{\theta}, \ell)$. 
%More specifically, if the loss function $ f(\cdot,x) $ is $ \alpha $-Lipschitz continuous, we call it a CnB$_{\text{Lip}}$  learning problem.
%%
%%the following two conditions hold:
%%	\begin{enumerate}
%%		\item There exists a fixed parameter vector $\tilde{\theta}$ such that   $\theta $ is always restricted within 	$\mathcal{B}$,
%%		\item  For each $ x\in X $, the loss function $ f(\cdot,x) $ is $ \alpha $-Lipschitz.
%%	\end{enumerate} 
%Similarly, we can define the CnB$_{\text{smooth}}$ and CnB$_{\text{Hes}}$  learning problems, if the condition (\rmnum{1}) is replaced by the assumptions (2) and (3) of Definition~\ref{def-assumption}, respectively. 
%In general, we  call all these three problems ``\emph{continuous-and-bounded learning}''.
\end{definition}
\begin{remark}
\label{rem-cnb}
We can also consider other variants for CnB learning with replacing the ``$\alpha $-Lipschitz continuous'' assumption. For example, a differentiable   function $ g $ is ``$ \alpha $-Lipschitz continuous gradient''   if its gradient  $ \nabla g $  is $ \alpha $-Lipschitz continuous (it is also called ``$ \alpha $-smooth'').   Similarly, a  twice-differentiable function $ g$ is  ``$ \alpha $-Lipschitz continuous Hessian'' if its  Hessian matrix  $ \nabla^2 g $ is $ \alpha $-Lipschitz continuous.   In this paper, we mainly focus the problems under the ``$\alpha $-Lipschitz continuous'' assumption, and our analysis can be also applied to these two variants via slight modifications. Please see more details in Section~\ref{sec-appcbl}.
\end{remark}

Several examples for the CnB learning problem are shown in Section~\ref{supp:instance}. 
We also define the coreset for the CnB learning problems below. 
\begin{definition}[$ \varepsilon $-coreset]
\label{def-coreset}
	Let $\varepsilon>0$. Given a dataset $ X\subset \mathcal{X} $ and the objective function $ f(\theta,X) $,  we say that a weighted set $ C\subset\mathcal{X} $ is an \emph{$ \varepsilon $-coreset} of $ X $ if for any $ \theta\in\mathbb{B}(\tilde{\theta}, \ell) $, we have
\begin{equation}\label{coreset}
	|f(\theta,C)-f(\theta,X)| \leq \varepsilon f(\theta,X).
\end{equation} 
\end{definition}

If $C$ is an  $ \varepsilon $-coreset of $ X $, we can run an existing optimization algorithm on $C$ so as to obtain an approximate solution. Obviously, we expect that the size of $C$ to be as small as possible. Following Definition~\ref{def-coreset}, we  also define the corresponding \emph{robust coreset} (the similar definition was also introduced in~\cite{FeldmanL11,HuangJLW18} before).

\begin{definition}[robust coreset]\label{robust-coreset}
	Let $\varepsilon>0$, and $\beta\in [0,1)$. Given the dataset $ X\subset \mathcal{X} $ and the objective function $ {f}(\theta,x) $, we say that a weighted dataset $ C\subset \mathcal{X} $ is a $ (\beta,\varepsilon) $-robust coreset of $ X $ if for any $ \theta\in \mathbb{B}(\tilde{\theta}, \ell) $, we have
	\begin{equation}
		(1-\varepsilon) f_{(1+\beta)z}(\theta,X)  \leq
		 {f}_{z}(\theta,C)  \leq (1+\varepsilon) f_{(1-\beta)z}(\theta,X).
	\end{equation}

\end{definition}
Roughly speaking, if we obtain an approximate solution $\theta'\in\mathcal{P}$ on $C$, its quality can be preserved on the original input data $X$. The parameter $\beta$ indicates the error on the number of outliers if using $\theta'$ as our solution on $X$. If we set $\beta=0$, that means we allow no error on the number of outliers.  In Section~\ref{supp:quality}, we present our detailed discussion on the quality loss (in terms of the objective value and the number of outliers) of this transformation from $C$ to $X$.

\textbf{The rest of this paper is organized as follows.} In Section~\ref{robust}, we introduce our robust coreset framework and show how to implement it in a fully-dynamic environment. In Section~\ref{standard-coreset}, we propose two different ordinary coreset (without outliers) construction methods for CnB learning problems, which can be used as the black box in our robust coreset framework of Section~\ref{robust}. Finally, in Section~\ref{sec-app} we illustrate the application of our coreset method in practice.

\section{Our Robust Coreset Framework}
\label{robust}   
We first consider the simple uniform sampling as the robust coreset in Section~\ref{sec-wu} (in this part, we consider the general learning problems without the CnB assumption). To improve the result, we further introduce our major contribution, the hybrid framework for robust coreset construction and its fully-dynamic realization in Section~\ref{sec:robust-coreset} and \ref{sec-dynamic}, respectively. 

%will show how to construct robust coreset for Continuous-Bounded learning problems.
%First, we study that whether the $ \delta $-sample is a good candidate of the robust coreset. Second, we propose an algorithm which can compute robust coreset if an $ \varepsilon $-coreset method has been given. 

\subsection{Uniform Sampling for General Case}
\label{sec-wu}
As mentioned before, the existing robust coreset construction methods~\cite{FeldmanL11,HuangJLW18} are based on uniform sampling. Note that their methods are only for the clustering problems ({\em e.g.,} $k$-means/median clustering). Thus a natural question is that whether the uniform sampling idea also works for the general  learning problems in the form of (\ref{for-obj}). Below we answer this question in the affirmative.  To illustrate our idea, we need the following  definition for range space. 

\begin{definition}[$f$-induced range space]
\label{def-range}
	%A range space is a tuple $ (\mathcal{X},\mathfrak{R}) $ such that $ \mathfrak{R} $ is a family of subsets of $ \mathcal{X} $.
	Suppose $\mathcal{X}$ is an arbitrary metric space. 
	Given the cost function $ f(\theta,x)$ as (\ref{for-obj}) over $\mathcal{X}$, we let
	\begin{equation}\label{range}
		\mathfrak{R}=\Big\{ \{  x\in \mathcal{X}:f(\theta,x)\leq r \}\mid \forall r \geq 0, \forall \theta\in\mathcal{P} \Big\} ,
	\end{equation} 
	then $ (\mathcal{X},\mathfrak{R}) $ is called the $f$-induced range space. Each $R\in \mathfrak{R}$ is called a range of $\mathcal{X}$.
	\end{definition}
The following ``$ \delta $-sample'' concept comes from the theory of VC dimension~~\cite{li2001improved}. 
%in the following.
Given a range space $ (\mathcal{X},\mathfrak{R}) $, let $ C$ and $ X $ be two finite subsets of $ \mathcal{X} $. Suppose $\delta\in(0,1)$. We say $ C $ is a \emph{$ \delta $-sample} of $ X $ if $ C\subseteq X $ and   
\begin{equation}
  \left| \frac{|X\cap R|}{|X|}-\frac{|C\cap R|}{|C|} \right|\leq \delta  \text{ for any } R\in\mathfrak{R}.\label{for-deltasample}
  \end{equation} 
%In this paper, the $ \delta $-sample is a weighted set. i.e., if $ B $ is a $ \delta $-sample of $ A $, then the weight of every point of $ B $ is $ |A|/|B| $.
%We can obtain a $ \delta $-sample by uniform sampling. 
%The sampling size is dependent on the $ \delta $ and VC dimension of the range space induced from $ f(\theta,x) $.
%$ k $-means with outliers is just a spectial case of last section with $ \mathcal{X}=\mathbb{R}^d $, $ C $ is $ k $ points in $ \mathbb{R}^d $ and $ f_C(x)=\min_{c\in C}\| x-c \|^2 $. Note that for $ f_C(x)=\min_{c\in C}\| x-c \|^2  $, ranges are just the union of $ k $ balls with nonegative radius, whose VC dimension is $ O(kd\log k) $. \par 
%The cost function in linear regression is $ f_h(x)=Res(x,h) $ where $ h $ is a $ (d-1) $-hyperplane in $ \mathbb{R}^d $. Range in this case is the intersection of two halfspaces.  Its VC dimension is $ O(d) $.\par 
Denote by $ \mathtt{vcdim} $ the VC dimension of the range space of Definition~\ref{def-range}, then we can achieve a $ \delta $-sample with probability $ 1-\eta $ by uniformly sampling $ O(\frac{1}{\delta^2}(\mathtt{vcdim}+\log \frac{1}{\eta})) $ points from $X$~\cite{li2001improved}. The value of $ \mathtt{vcdim} $ depends on the function ``$ f$''. For example, if ``$ f$'' is the loss function of logistic regression in $ \mathbb{R}^d $, then $ \mathtt{vcdim}$ can be as large as $ \Theta(d) $~\cite{MunteanuSSW18}. 
The following theorem shows that a $ \delta $-sample can serve as a robust coreset if $z$ is a constant factor of $n$. Note that in the following theorem, the objective $f$ can be any function without following Definition~\ref{def:CB-learning}. 
%Due to the space limit, we place its proof to our %supplement. 

%which admits a $ \delta n$ violation on the number of the outliers.
\begin{theorem}\label{lemma-eps-sample}
Let $(X,z)$ be an instance of the robust  learning problem (\ref{for-obj2}). 
	If $ C $ is a $ \delta $-sample of $ X $ in  the $f$-induced range space. We assign $w(c)=\frac{n}{|C|}$ for each $c\in C$. Then we have 
	\begin{equation}
		f_{z+\delta n}(\theta,X)\leq
		f_{z}(\theta,C)\leq f_{z-\delta n}(\theta,X) \label{for-the-uniform}
	\end{equation}
	for any $ \theta\in\mathcal{P} $ and any $ \delta \in (0,z/n] $. In particular, if $\delta=\beta z/n$, $C$ is a $(\beta,0) $-robust coreset of $X$ and the size of $C$ is $O(\frac{1}{\beta^2}(\frac{n}{z})^2(\mathtt{vcdim}+\log \frac{1}{\eta}))$.  
\end{theorem}
\begin{proof}
%Let $ A $ and $ B $ be two subsets of $ \mathcal{X} $ and we assume that they are $ \delta $-approximate, where the range space is induced by $ f(\theta,x) $.
%For convenience, we use the notation $ f_{[\gamma]}(\theta,X) $ in this proof. The below expressions are equivalent.
%\[ f_{[\gamma]}(\theta,X)=f_{(1-\gamma)n}(\theta,X)=f_{z}(\theta,X). \]
%We first give an average form.
%Given two $ \delta $-approximate subsets $ A $ and $ B $ of $ \mathcal{X} $, where the range space is induced by $ f(\theta,x) $. Then for any $ \gamma $ and any $ \theta $, $ \mathbbm{f}_{[\gamma]}(\theta,B)\geq  (1-\frac{\delta}{\gamma})\mathbbm{f}_{[\gamma-\delta]}(\theta,A) $.
Suppose the size of $C$ is $m$ and let $N=nm$. To prove Theorem~\ref{lemma-eps-sample}, we imagine to generate two new sets as follows. For each point $c\in C$, we generate $n$ copies; consequently we obtain a new set $C'$ that actually is the union of $n$ copies of $C$. Similarly, we generate a new set $X'$ that is the union of $m$ copies of $X$. Obviously, $|C'|=|X'|=N$. Below, we fix an arbitrary $ \theta\in\mathcal{P} $ and show that (\ref{for-the-uniform}) is true. 

We order the points of $X'$ based on their objective values; namely, $X'=\{x'_i\mid 1\leq i\leq N\}$ and $f(\theta, x'_1)\leq f(\theta, x'_2)\leq\cdots\leq f(\theta, x'_N)$. Similarly, we have $C'=\{c'_i\mid 1\leq i\leq N\}$ and $f(\theta, c'_1)\leq f(\theta, c'_2)\leq\cdots\leq f(\theta, c'_N)$. Then we claim that for any $ 1\leq i\leq (1-\delta)N $, the following inequality holds: 
\begin{equation}\label{eq-1}
    f(\theta,c'_{i+\delta N})\geq f(\theta,x'_{i}). 
\end{equation}
Otherwise,  there exists some $i_0$ that 
%
%%
%%
%%
%%Let $ |X|=a, |B|=b$ and $ O=ab $. We create a new weighted set $ A' $ by setting the weight of each element in $ A $ to be $ b $; it can be simply thought as making $ b $ copies of each element in $ A $. Similarly, we create $ B' $ that each point has weight $ a $. Then we have $ \llbracket A' \rrbracket = \llbracket B' \rrbracket = O $. 
%
%
%%We arrange elements in $ Y $ in the order of $ f(\theta,y) $ and use $ y_i $ to denote the $ i $-th element.
%%First we claim that for all $ 1\leq i\leq (1-\delta)O $, we have
%%\begin{equation}\label{eq-1}
%%    f(\theta,a'_{i+\delta O})\geq f(\theta,b'_{i})
%%\end{equation}
%If not, there must be some $ j $ with 
$ f(\theta,c'_{i_0+\delta N})< f(\theta,x'_{i_0}) $. Consider the range $R_0=\{x\in \mathcal{X}\mid f(\theta,x)\leq f(\theta,c'_{i_0+\delta N})  \} $. Then we have
%
%
% We have $ R\in\mathfrak{R} $,  $ \llbracket A'\cap R\rrbracket \geq j+\delta O $ and $\llbracket B'\cap R\rrbracket <j $. (We define the intersection of a weighted set and a unit-weighted set to be a weighted set such that each element should belong to both sets and it keeps its weight from the weighted set.) Then we have 
\begin{eqnarray}
	\frac{|C\cap R_0|}{|C|}=\frac{|C'\cap R_0|/n}{|C|}&\geq& \frac{(i_0+\delta N)/n}{m}=\frac{i_0}{N}+\delta; \\
	\frac{|X\cap R_0|}{|X|}=\frac{|X'\cap R_0 | /m}{|X|}&<&\frac{i_0/m}{n}=\frac{i_0}{N}. 
\end{eqnarray}
That is, $\big|\frac{|C\cap R_0|}{|C|}-\frac{|X\cap R_0|}{|X|}\big|>\delta$ which is in  contradiction with the fact that $ C $ is a $ \delta $-sample of $ X $. Thus (\ref{eq-1}) is true. As a consequence, we have
\begin{eqnarray}
	{f}_{z}(\theta,C)&=&\frac{n}{m}\sum_{i=1}^{m-\frac{m}{n}z}f(\theta,c_i) = \frac{1}{m}\sum_{i=1}^{N-mz}f(\theta,c'_i)  \geq \frac{1}{m}\sum_{i=1+\delta N}^{N-mz}f(\theta,c'_i)   \\
	&\underbrace{\geq}_{\text{by (\ref{eq-1})}} &\frac{1}{m}\sum_{i=1}^{(1-\delta)N-mz}f(\theta,x'_i)=\sum_{i=1}^{(1-\delta)n-z}f(\theta,x_i)=f_{z+\delta n}(\theta,X).
\end{eqnarray}
So the left-hand side of (\ref{for-the-uniform}) is true, and the right-hand side can be proved by using the similar manner. 
%
%
%
%We see that $ A $ and $ B $ are not $ \delta $-approximate, which contradicts the assumption. Thus we proved (\ref{eq-1}) .\newline
%We denote by $ \gamma = (n-z)/n $ the proportion of inliers, then we have
%\begin{eqnarray}
%	{f}_{(1-\gamma)|A|}(\theta,A)&=\sum_{i=1}^{\gamma a}f(\theta,a_i)=\frac{1}{b}\sum_{i=1}^{\gamma O}f(\theta,a'_i)\\
%	&\geq \frac{1}{b}\sum_{i=1+\delta O}^{\gamma O}f(\theta,a'_i)\quad  \text{because } f(\theta,\cdot)\geq 0  \\
%	&=\frac{1}{b}\sum_{i=1}^{(\gamma-\delta)O}f(\theta,a'_{i+\delta O})\geq \frac{1}{b}\sum_{i=1}^{(\gamma-\delta)O}f(\theta,b'_i)\quad  \text{by the claim $ f(\theta,a'_{i+\delta O})\geq f(\theta,b'_{i}) $}   \\
%	&=\frac{a}{b}\sum_{i=1}^{(\gamma-\delta)b}f(\theta,y_i)\\
%	&={f}_{(1-(\gamma-\delta))|B|}(\theta,B).
%\end{eqnarray}
%For simplicity we assume that $ \delta O $ is an integer, and it is easy to see that this result still holds when $ \delta O $ is not integral.
%
%
%$ C $ is a $ \delta $-sample of $ X $, so they are $ \delta $-approximate.
%Let $ A $ be $ C $ and $ B $ be $ X $, we have $ f_{z+\delta n}(\theta,X)\leq
%		f_{z}(\theta,C) $. Let $ B $ be $ C $, $ A $ be $ X $ and $ \gamma= \gamma+\delta $, we have $f_{z}(\theta,C)\leq f_{z-\delta n}(\theta,X)$.
\end{proof}


\begin{remark}
Our  proof is partly inspired by the ideas of \cite{MountNPSW14,DBLP:journals/ml/MeyersonOP04} for analyzing uniform sampling. 
Though the uniform sampling is simple and easy to implement, it has two major drawbacks. First, it always involves an error ``$\delta$'' on the number of outliers (otherwise, if letting $\delta=0$, the sample should be the whole $X$). Also, the result is interesting only when $z$ is a constant factor of $n$. For example, if $z=\sqrt{n}$, the obtained sample size can be as large as $n$. Our hybrid robust framework proposed in Section~\ref{sec:robust-coreset} can resolve these two issues for CnB learning problems. 
\end{remark}
%However, the robust coreset in Definition~\ref{robust-coreset} only admits $ \beta z $ violations. To make this we should let $ \delta = \beta z /n $. If $ z $ is $ o(n) $, then we cannot achieve a small robust coreset. To solve this, we will give a simple robust coreset method for continuous-and-bounded learning problems in section~\ref{sec:robust-coreset}. Our method requires a coreset method (without outliers) as a blackbox to be called. The standard coreset method is introduced in Section \ref{standard-coreset}.


\subsection{The Hybrid Framework for $ (\beta,\varepsilon) $-Robust Coreset}\label{sec:robust-coreset}
%Remind that $ |X|=n $ and the number of outliers is $ z $. The key idea of our proof is as follows. As the existence of $ \tilde{\theta} $, we can use it to split total data into two parts: suspected inliers and suspected outliers. Then we have an important observation: Because of the  smoothness of the objective function, outliers whose cost values are much larger than the normal one, can be correctly recognized. At the same time, those true outiers that was mistaken as inliers, must have a value close to the inliers'. Therefore, the error brought by the mistaking can be bounded in this case. We need to split dataset $ X $ first and process respectively. (Coreset Modularity) \par 
Our idea for building the robust coreset comes from the following intuition. In an ideal scenario, if we know who are the inliers and who are the outliers, we can simply construct the coresets for them separately. In reality, though we cannot obtain such a clear classification, the CnB property (Definition~\ref{def:CB-learning}) can guide us to obtain a ``coarse'' classification. Furthermore, together with some novel insights in geometry, we prove that such a hybrid framework can yield a $ (\beta,\varepsilon) $-robust coreset. 

Suppose $\varepsilon\in(0,1)$ and  the objective $f$ is continuous-and-bounded as Definition~\ref{def:CB-learning}. Specifically, the parameter vector $\theta$ is always restricted within the ball $\mathbb{B}(\tilde{\theta}, \ell)$.  
First, we classify $ X $ into two parts according to the value of $ f(\tilde{\theta},x) $. 
Let  $\varepsilon_0 := \min \left\{ \frac{\varepsilon}{16},\frac{\varepsilon\cdot\inf_{\theta\in\mathbb{B}(\tilde{\theta}, \ell)} f_z(\theta,X)}{16(n-z)\alpha \ell} \right\} $  and $\tilde{z}:=  \left( 1+1/\varepsilon_0 \right)z$; also assume $x_{\tilde{z}}\in X$ is the point who has the $ \tilde{z}$-th largest cost $ f(\tilde{\theta},x) $ among $X$.  %We let  
%Data point $ x $ with $ f(\tilde{\theta},x) $ being the $ Z $-th largest value in all $ f(\tilde{\theta},x)  $s is defined as critical point $ x_c $. And 
We let $ \tau=f(\tilde{\theta},x_{\tilde{z}}) $, and thus we obtain the set 
\begin{equation}\label{markov}
	 \{x\in X\mid f(\tilde{\theta},x)\geq \tau \}.
\end{equation}
that has size $\tilde{z}$. We call these $\tilde{z}$ points as the ``suspected outliers'' (denoted as $X_{\mathtt{so}}$) and the remaining $ n-\tilde{z}$ points as the ``suspected inliers'' (denoted as $X_{\mathtt{si}}$). If we fix $\theta=\tilde{\theta}$, the set of the ``suspected outliers'' contains at least $\frac{1}{\varepsilon_0}z$ real inliers (since $\tilde{z}=\left( 1+1/\varepsilon_0 \right)z$). This immediately implies the following inequality: 
%Let $ Z=Z(\varepsilon)=  \left( 1+1/\varepsilon \right)z $ so we have
\begin{align}\label{notmarkov}
	&\tau  z \leq \varepsilon_0 f_{z}(\tilde{\theta},X).
\end{align}

Suppose we have an ordinary coreset construction method $\mathcal{A}$ as the black box (we will discuss it in Section~\ref{standard-coreset}).  \textbf{Our robust coreset construction} is as follows:

\vspace{0.1in}
{\em \hspace{0.2in} We build an $ \varepsilon_1 $-coreset ($\varepsilon_1=\varepsilon/4 $) for $X_{\mathtt{si}}$ by $\mathcal{A}$ and  take a $ \delta $-sample for $X_{\mathtt{so}}$ with setting $ \delta=\frac{\beta\varepsilon_0}{1+\varepsilon_0}$. We denote these two sets as $ C_{\mathtt{si}} $ and $ C_{\mathtt{so}} $ respectively. If we set $\beta=0$ ({\em i.e.,} require no error on the number of outliers), we just directly take all the points of  $X_{\mathtt{so}}$ as $ C_{\mathtt{so}} $.  Finally, we return  $ C =C_{\mathtt{si}}\cup C_{\mathtt{so}} $ as the robust coreset. }

%We conclude that $ C $ is an $ (\beta,5\varepsilon,2\xi(\ell)) $-robust coreset for the continuous-and-bounded learning problem.


\begin{theorem}
	\label{thm-coreset-outlier}
	Given a CnB learning instance $(X,z)$, the above coreset construction method returns a $ (\beta, \varepsilon) $-robust coreset (as Defintion~\ref{robust-coreset}) of size 
	\begin{equation}
	 |C_{\mathtt{si}}|+\min \Big\{ O\left( \frac{1}{\beta^2\varepsilon^2}(\mathtt{vcdim}+\log\frac{1}{\eta}) \right),O\left(\frac{z}{\varepsilon_0}\right) \Big\}  
	 \end{equation}
	with probability at least $ 1-\eta $. In particular, when $ \beta=0 $, our coreset has no error on the number of outliers and its size is $ |C_{\mathtt{si}}|+ O\left( \frac{z}{\varepsilon_0} \right) $.
	%we obtain a $ (0,5\varepsilon,2\xi(\ell)) $-robust coreset with additional $ O(\frac{z}{\varepsilon}) $ size.
\end{theorem}
We present the sketch of the proof below and leave the full proof to Section~\ref{sec-appthe2}. 

\begin{proof}\textbf{(sketch)} 
It is easy to obtain the coreset size. So we only focus on proving the quality guarantee below. 

Let $\theta$ be any parameter vector in the ball $ \mathbb{B}(\tilde{\theta}, \ell) $. Similar with the aforementioned classification $X_{\mathtt{si}}\cup X_{\mathtt{so}}$, $\theta$ also yields a classification on $X$. Suppose $\tau_\theta$ is the $z$-th largest value of $\{f(\theta, x)\mid x\in X\}$. Then we use  $X_{\mathtt{ri}}$ to denote the set of $n-z$ ``real'' inliers with respect to $\theta$,  {\em i.e.,} $\{x\in X\mid f(\theta, x)<\tau_\theta\}$; we also use   $X_{\mathtt{ro}}$ to denote the set of $z$ ``real'' outliers with respect to $\theta$,  {\em i.e.,} $\{x\in X\mid f(\theta, x)\geq\tau_\theta\}$. Overall, the input set $X$ is partitioned into $4$ parts: 
\begin{equation}
\text{$ X_{\mathrm{\Rnum{1}}}=X_{\mathtt{si}}\cap X_{\mathtt{ri}}$, $ X_{\mathrm{\Rnum{2}}}=X_{\mathtt{so}}\cap X_{\mathtt{ri}}$, $ X_{\mathrm{\Rnum{3}}}=X_{\mathtt{so}}\cap X_{\mathtt{ro}}$, and $ X_{\mathrm{\Rnum{4}}}=X_{\mathtt{si}}\cap X_{\mathtt{ro}}$. }
\end{equation}
Similarly, $\theta$ also yields a classification on $C$ to be $C_{\mathtt{ri}}$ (the set of ``real'' inliers of $C$) and $C_{\mathtt{ro}}$ (the set of ``real'' outliers of $C$). Therefore we have 
\begin{equation}
\text{$ C_{\mathrm{\Rnum{1}}}=C_{\mathtt{si}}\cap C_{\mathtt{ri}}$, $ C_{\mathrm{\Rnum{2}}}=C_{\mathtt{so}}\cap C_{\mathtt{ri}}$, $ C_{\mathrm{\Rnum{3}}}=C_{\mathtt{so}}\cap C_{\mathtt{ro}}$, and $ C_{\mathrm{\Rnum{4}}}=C_{\mathtt{si}}\cap C_{\mathtt{ro}}$. }
\end{equation}



%In this proof we omit technical details and the entire proof is put in the %supplement.
%
%
%
%To prove the correctness of our construction, we imagine the following partition on $X$. 
%For any parameter vector $ \theta $ in the parameter space, $X$ is also partitioned into the real inliers ({\em i.e.,} the set of $n-z$ points who have the smallest cost $f(\theta,x)$)  and the real outliers ({\em i.e.,} the set of $z$ points who have the largest cost $f(\theta,x)$). Therefore, together with the suspected inliers and outliers, $X$ is partitioned into $4$ parts: 
%%we call the  
%%$ z $ points with largest $ f(\theta,x) $ are outliers (according to $ \theta $) and the others are inliers (according to $ \theta $). 
%%$ \theta $ and $ \tilde{\theta} $ jointly yield a partition of $ X $: 
%\begin{itemize}
%	\item $ X_{\mathrm{\Rnum{1}}} $: the points belonging to  the real inliers and also the suspected inliers. $ X_{\mathrm{\Rnum{1}}} $ is not empty because we assume that suspected outliers are less than inliers ($ Z< n-z $).
%	\item $ X_{\mathrm{\Rnum{2}}} $: the points belonging to the real inliers and the suspected outliers. $ X_{\mathrm{\Rnum{2}}} $ is not empty because the number of the suspected outliers is larger than the number of outliers ($ Z>z $).
%	\item $ X_{\mathrm{\Rnum{3}}} $: the points belonging to the real outliers and also the suspected outliers. $ X_{\mathrm{\Rnum{3}}} $ can be empty iff all the real outliers are suspected inliers.
%	\item $ X_{\mathrm{\Rnum{4}}} $: the points belonging to the real  outliers and the suspected inliers. $ X_{\mathrm{\Rnum{4}}} $ can be empty iff all the real outliers are suspected outliers.
%\end{itemize}
%Easy to see that $ f_{z}(\theta,C)=f(\theta,\mathrm{\Rnum{1}^C+\Rnum{2}^C})=f(\theta,\mathrm{\Rnum{1}^C})+f(\theta,\mathrm{\Rnum{2}^C}) $.
%Given an existing $ (\varepsilon,\nu) $-coreset of $ \mathrm{\Rnum{1}^X+\Rnum{4}^X} $
%for the $ \alpha $-parametric Lipschitz continuous function $ f(\theta,x) $ over rational region $ \mathcal{R} $, 
%combining the results from Claim \ref{claim-divide} with (\ref{markov}) and (\ref{notmarkov}), we can bound $ f(\theta,\mathrm{\Rnum{1}^C}) $ and $ f(\theta,\mathrm{\Rnum{2}^C}) $ respectively and have
%\begin{equation*}
%	\mathbbm{f}_{z}(\theta,C)\in (1\pm5\varepsilon)\mathbbm{f}_{z}^{\pm \beta}(\theta,X)\pm 5\varepsilon(\nu+\xi{\ell}).
%\end{equation*}	
%for any $ \theta\in\mathcal{B} $.

%Similarly, $ C $ is partitioned into $4$ parts in the same way.
%%The follow key lemma reveals the relations of these $4$ parts (see the proof in the %supplement). 
%
%For continuous-and-bounded learning problems, we can bound $|f(\theta,x)-f(\tilde{\theta},x)| $ by a low degree polynomial function. For example, if $ f(\cdot,x) $ is $ \alpha $-Lipschitz, the polynomial is $ \xi(\ell)=\alpha\ell $; if $ f(\cdot,x) $ is $ \alpha $-smooth, the polynomial is $ \xi(h;\ell)=h\ell+\alpha\ell^2/2 $, where $ h=\max_{x\in X} \|\nabla f(\theta,x)_{|\tilde{\theta}} \| $. For conciseness, we use the $ \xi(\ell) $ in the following statement.
 
%\begin{lemma}\label{lemma:robust}
%As for different parts defined above, we have these following results.
%        \item \begin{equation}\label{ineq-1}
%            f(\theta,C_\mathrm{\Rnum{2}})\leq 
%            \begin{cases}
%                f_{(1-\beta)z}(\theta,X_\mathrm{\Rnum{2}}+X_\mathrm{\Rnum{3}}) \quad& \text{if } C_\mathrm{\Rnum{4}}=\varnothing \\
%                f(\theta,X_\mathrm{\Rnum{2}})+2z(\tau+\xi(\ell)) \quad& \text{if } C_\mathrm{\Rnum{4}}\ne\varnothing
%            \end{cases}
%        \end{equation}
%        \begin{align}\label{ineq-2}
%            &f(\theta,X_\mathrm{\Rnum{1}}+X_\mathrm{\Rnum{4}})+ f_{(1-\beta)z}(\theta,X_\mathrm{\Rnum{2}}+X_\mathrm{\Rnum{3}})
%		\leq f_{(1-\beta)z}(\theta,X)+2z\xi(\ell) \\
%	        \label{ineq-3}
%            &\left| f_{z}(\theta,X)-f_{z}(\tilde{\theta},X) \right|\leq (n-z)\xi(\ell)
%        \end{align}

%\end{lemma}



Our goal is to show that $ f_{z}(\theta,C)$ is a qualified approximation of $ f_{z}(\theta,X)$ (as Defintion~\ref{robust-coreset}).  
Note that $ f_{z}(\theta,C)=f(\theta,C_\mathrm{\Rnum{1}}\cup C_\mathrm{\Rnum{2}})=f(\theta,C_\mathrm{\Rnum{1}})+f(\theta,C_\mathrm{\Rnum{2}}) $. Hence we can bound $ f(\theta,C_\mathrm{\Rnum{1}}) $ and $ f(\theta,C_\mathrm{\Rnum{2}}) $ separately. We consider their upper bounds first, and the lower bounds can be derived by using the similar manner. 


The upper bound of $ f(\theta,C_\mathrm{\Rnum{1}}) $ directly comes from the definition of $\varepsilon$-coreset, {\em i.e.,} $ f(\theta,C_\mathrm{\Rnum{1}})\leq f(\theta,C_\mathrm{\Rnum{1}}\cup C_\mathrm{\Rnum{4}})\leq (1+\varepsilon_1) f(\theta,X_\mathrm{\Rnum{1}}\cup X_\mathrm{\Rnum{4}}) $ since $ C_\mathrm{\Rnum{1}}\cup C_\mathrm{\Rnum{4}} $ is an $ \varepsilon_1 $-coreset of $ X_\mathrm{\Rnum{1}}\cup X_\mathrm{\Rnum{4}} $.


It is more complicated to derive the upper bound of $ f(\theta,C_\mathrm{\Rnum{2}}) $. We consider two cases. \textbf{(1)} If $ C_\mathrm{\Rnum{4}}=\emptyset $,  then we know that all the suspected inliers of $C$ are all real inliers (and meanwhile, all the real outliers of $C$ are suspected outliers); consequently, we have  
\begin{equation}
f(\theta,C_\mathrm{\Rnum{2}})\leq f_{z}(\theta,C_\mathrm{\Rnum{2}}\cup C_\mathrm{\Rnum{3}})\leq f_{(1-\beta)z}(\theta,X_\mathrm{\Rnum{2}}\cup X_\mathrm{\Rnum{3}}) 
\end{equation}
from Theorem~\ref{lemma-eps-sample}. \textbf{(2)}  If $ C_\mathrm{\Rnum{4}}\neq\emptyset $, by using the triangle inequality and the $\alpha$-Lipschitz assumption, we have $ f(\theta,C_\mathrm{\Rnum{2}})\leq f(\theta,X_\mathrm{\Rnum{2}})+2z(\tau+\alpha \ell))$. We merge these two cases and overall obtain the following upper bound:
%We give two different upper bounds depending on $ C_\mathrm{\Rnum{4}} $ is empty or not:
%\begin{equation}\label{ineq-1}
%        f(\theta,C_\mathrm{\Rnum{2}})\leq 
%        \begin{cases}
%            f_{(1-\beta)z}(\theta,X_\mathrm{\Rnum{2}}+X_\mathrm{\Rnum{3}}) \quad& \text{if } C_\mathrm{\Rnum{4}}=\varnothing \\
%            f(\theta,X_\mathrm{\Rnum{2}})+2z(\tau+\xi(\ell)) \quad& \text{if } C_\mathrm{\Rnum{4}}\ne\varnothing
%        \end{cases}
%\end{equation}
%By adding the upper bound of $ f(\theta,C_\mathrm{\Rnum{1}}) $ and $ f(\theta,C_\mathrm{\Rnum{2}}) $, we obtain an upper bound of $ f_{z}(\theta,C) $:
%\begin{equation}\label{ineq-2}
%        f_{z}(\theta,C)\leq
%        \begin{cases}
%            (1+\varepsilon)f_{(1-\beta)z}(\theta,X)+4z\xi(\ell) \quad& \text{if } C_\mathrm{\Rnum{4}}=\varnothing \\
%            (1+\varepsilon)f_{z}(\theta,X)+4z\tau+4z\xi(\ell) \quad& \text{if } C_\mathrm{\Rnum{4}}\ne\varnothing
%        \end{cases}
%\end{equation}
%Merging two cases together, we have that
\begin{equation}\label{ineq-3}
   f_{z}(\theta,C)\leq (1+\varepsilon_1)f_{(1-\beta)z}(\theta,X)+4z\tau+4z\alpha\ell.
\end{equation}
Moreover, from (\ref{notmarkov}) and the $\alpha$-Lipschitz assumption, we have  $ \tau z\leq \varepsilon_0 f_{z}(\theta,X)+\varepsilon_0(n-z)\alpha\ell $. Then the above (\ref{ineq-3}) implies
 \begin{equation}
    f_{z}(\theta,C) \leq (1+\varepsilon) f_{(1-\beta)z}(\theta,X).
\end{equation}
Similarly, we can obtain the lower bound
\begin{equation}
     f_{z}(\theta,C) \geq (1-\varepsilon) f_{(1+\beta)z}(\theta,X).
\end{equation}
Therefore $ C $ is a $(\beta,\varepsilon)$-robust coreset of $ X $.
\end{proof}


\subsection{The Fully-Dynamic Implementation}
\label{sec-dynamic}
In this section, we show that our robust coreset of Section~\ref{sec:robust-coreset} can be efficiently implemented in a fully-dynamic environment, even if the number of outliers $z$ is dynamically changed. 



The standard $ \varepsilon $-coreset usually has two important properties. 
If $ C_1 $ and $ C_2 $ are respectively the $ \varepsilon $-coresets of two disjoint sets $ X_1 $ and $ X_2 $, their union $ C_1\cup C_2 $ should be an $ \varepsilon$-coreset of $ X_1\cup X_2 $. Also, 
if $ C_1 $ is an $ \varepsilon_1$-coreset of $ C_2 $ and $ C_2 $ is an $ \varepsilon_2$-coreset of $ C_3 $,  $ C_1 $ should be an $ (\varepsilon_1+\varepsilon_2+\varepsilon_1\varepsilon_2) $-coreset of $ C_3 $. Based on these two properties, one can build a coreset for incremental data stream by using the ``merge-and-reduce'' technique~\cite{BentleyS80,Har-PeledM04}. Very recently, Henzinger and Kale~\cite{HenzingerK20} extended it to the more general fully-dynamic setting, where data items can be deleted and updated as well.


Roughly speaking, the merge-and-reduce technique uses a sequence of ``buckets'' to maintain the coreset for the input streaming data, and the buckets are merged by a bottom-up manner. However, it is challenging to directly adapt this strategy to the case with outliers, because we cannot determine the number of outliers in each bucket. 
%Even if we can derive similar results for the robust coreset, they can not be directly applied to the merge-and-reduce technique. Because we still do not know the exact number of outliers in each separated part although the total number of outliers is given~\cite{ChenA018,GuptaLSM2017}. Moreover, in the situation containing outliers, the fully-dynamic setting is more complex: besides the data, the number of outliers $ z $ should be able to be updated.
A cute aspect of our hybrid robust coreset framework is that we can easily resolve this obstacle by using an $O(n)$ size auxiliary table $\mathscr{L}$ together with the merge-and-reduce technique (note that even for the case without outliers, maintaining a fully-dynamic coreset already needs $\Omega(n)$ space~\cite{HenzingerK20}). We briefly introduce our idea below and leave the full details to Section~\ref{sec-appdynamic}. 

\begin{wrapfigure}{r}{7cm}
	\vspace{-0.4cm}
	\centering
	\includegraphics[width=1\linewidth]{fig/coreset-tree-outlier}
	\caption{The illustration for our fully-dynamic robust coreset construction.}
	\vspace{-0.3cm}
	\label{fig-coreset-tree-outlier}
\end{wrapfigure}


Recall that we partition the input data $X$ into two parts: the $n-\tilde{z}$ ``suspected inliers'' and the $\tilde{z}$ ``suspected outliers'', where $\tilde{z}=(1+1/\varepsilon_0)z$. We follow the same notations used in Section~\ref{sec:robust-coreset}. For the first part, we just apply the vanilla merge-and-reduce technique to obtain a fully-dynamic coreset $C_{\mathtt{si}}$; for the other part, we can just take a $\delta$-sample or take the whole set (if we require $\beta$ to be $0$), and denote it as $C_{\mathtt{so}}$. Moreover, we maintain a table $\mathscr{L}$ to record the key values $ x.\mathtt{value}= f(\tilde{\theta},x)$ and its position $ x.\mathtt{position} $ in the merge-and-reduce tree, for each $x\in X$; they are sorted by the $x.\mathtt{value}$s in the table. To deal with the dynamic updates ({\em e.g.,} deletion and insertion), we also maintain a critical pointer $ p $ pointing to the data item $ x_{\tilde{z}} $ (recall $x_{\tilde{z}}$ has the $ \tilde{z}$-th largest cost $ f(\tilde{\theta},x) $ among $X$ defined in Section~\ref{sec:robust-coreset}). 

When a new data item $x$ is coming or an existing data item $x$ is going to be deleted, we just need to compare it with $ f(\tilde{\theta},x_c) $ so as to decide to update $C_{\mathtt{si}}$ or $C_{\mathtt{so}}$ accordingly; after the update, we also need to update $ x_{\tilde{z}} $ and the pointer $p$ in $\mathscr{L}$. If the number of outliers $z$ is changed, we just need to update $ x_{\tilde{z}} $ and  $p$ first, and then update $C_{\mathtt{si}}$ and $C_{\mathtt{so}}$ (for example, if $z$ is increased, we just need to delete some items from $C_{\mathtt{so}}$ and insert some items to $C_{\mathtt{si}}$). To realize these updating operations, we also set one bucket as the ``hot bucket'', which serves as a shuttle to execute all the data shifts. See Figure~\ref{fig-coreset-tree-outlier} for the illustration. Let $ M(\varepsilon) $ be the size of the vanilla $\varepsilon$-coreset. In order to achieve an $ \varepsilon $-coreset overall, we need to construct an $ \frac{\varepsilon}{\log n} $-coreset with size $ M(\varepsilon/\log n) $ in every reduce part~\cite{AgarwalHV04}. We use $ M $ to denote $ M(\varepsilon/\log n) $ for short and assume that we can compute a coreset of $ X $ in time $ \mathsf{t}(|X|) $~\cite{DBLP:books/daglib/0035668}, then we have the following result.
\begin{theorem}
\label{the-dynamic}
In our dynamic implementation, the time complexity for insertion and deletion is $O(\mathsf{t}(M)\log n)$. To update $z$ to $z\pm \Delta z$ with $\Delta z\geq 0$, the time complexity is $O(\frac{\Delta z}{\varepsilon}\mathsf{t}(M)\log n)$, where $\varepsilon$ is the error bound for the robust coreset in Definition~\ref{robust-coreset}. 
\end{theorem}

%Our robust coreset can overcome both problems at the same time due to our construction method. As shown in Figure~\ref{fig-coreset-tree-outlier}, we divide total dataset into two parts and construct traditional $ \varepsilon $-coreset in suspected inliers and take uniform sampling from suspected outliers. Then we only apply the merge-and-reduce technique in the suspected inliers. Therefore, we do not have to know the number of outliers in each separated part. When $ z $ is updated in the dynamic data stream, we only have to shift the critical point and move points between two parts, which can be reduced to the data insertion and deletion.
%Specifically speaking, we can achieve an fully-dynamic robust coreset such that the updating time is $ O(M\log n) $. $ M $ is the size of the $ \varepsilon $-coreset.


%We presume that at any time the number of data is no more than $ n $. We maintain a list of data $ \mathscr{L} $, which is sorted by $ x.value= f(\tilde{\theta},x) $. Due to the construction method of our robust coreset, we maintain a critical pointer $ p $ pointing to the critical point $ x_c $. Hence all data points after the critical point are suspected outliers. Then we build the tree for the suspected inliers. Note that for a suspected inlier $ x $, we need $ x.position $ to record its position in the tree. 


%We shortly denote by $ M $ the size of the coreset used for the problem. 
%Let every leaf be a bucket with volume $ M $ thus the number of leaves is less than $ n/M $ and the height of the tree is $ O(\log n) $. Because our coreset construction time is linear (see theorem~\ref{thm:standard-coreset}), the re-construction time along a bottom-up path is $ O(M\log n) $. And we insert data points from left to right and delete from right to left. We call the right-end leaf of the current data \emph{hot bucket}, which could be shifted with the number of currunt data. All insertions and deletions will take place in the hot bucket. Clearly, all leaves on the left of hot bucket are full and all leaves on the right are empty. The following is the concrete reaction to each operation.
%\begin{itemize}
%	\item \textsc{Insert($ y $)} A new point $ y $ is inserted. We insert it into the list $ \mathscr{L} $ according to $y.value=f(\tilde{\theta},y) $. If $ y.value>p\rightarrow value $, shift $ p $ one place to the right. i.e., $ p\gets p+1 $. If $ y.value \leq p\rightarrow value $, insert it into the hot bucket and set $ y.position $, then update the coreset tree from hot bucket to the root. Updating time: $ O(M \log n) $.
%	\item \textsc{Delete($ y $)} A point $ y $ is deleted. If $ y $ is a suspected inlier, delete it from its bucket $ y.position $ and find another point $ y' $ from the hot bucket to re-fill the bucket $ y.position $. Then update the tree from these two buckets. Finally we delete $ y $ from list $ \mathscr{L} $. If $ y $ is a suspected outlier, delete it from $ \mathscr{L} $. Then $  p\gets p-1 $ and delete the current critical point from the tree, which is a suspected outlier now. Updating time: $ O(M\log n) $.
%	\item \textsc{Update($ y $)} A point $ y$ is updated. First we update $ y.value $ and its place in $ \mathscr{L} $. If the attribute of $ y $ (suspected inlier or suspected outlier) is not changed, we just update the tree form bucket $ y.position $. Otherwise we \textsc{Delete($ y $)} and \textsc{Insert($ y $)}. Updating time: $ O(M\log n) $.
%	\item \textsc{Change($ \Delta z $)} The number of outliers $ z\gets z+\Delta z $. Note that the number of suspected outliers $ Z(z) $ is an increasing function of $ z $. $ p\gets p+Z(z+\Delta z)-Z(z) $. If $ \Delta z>0 $, delete these $ Z(z+\Delta z)-Z(z) $ points from the tree. If $ \Delta z<0 $, insert these points from suspected outliers into the tree. Note that we do not need to update $ \mathscr{L} $ in this case. Updating time: $ O(\frac{\Delta z}{\varepsilon}M\log n) $.
%\end{itemize}


%Since the objective function computed on the coreset can approximate the one computed on the full data, running algorithms on the $ (\varepsilon,\nu) $-coreset will yield a competitive solution. We have the following conclusion.
%\begin{lemma}\label{lemma-competitive}
%	If $ C $ is an $ (\varepsilon,\nu) $-coreset of $ X $ with $ 0\leq \varepsilon \leq 1/3 $, $ \theta_C^* $ and $ \theta_X^* $ are the optimal solution for $ f(\theta,C) $ and $ f(\theta,X) $ respectively. $ \theta_C^\rho $ is a $ \rho $-approximate solution of $ C $ (i.e., $ f(\theta_C^\rho,C)\leq \rho \cdot f(\theta_C^*,C) $). Then we have
%	\begin{equation}
%		f(\theta_C^\rho,X)\leq (1+3\varepsilon)\rho \cdot f(\theta_X^*,X)+3\varepsilon\cdot \frac{\rho +1}{2}\nu.
%	\end{equation}
%	Esepcailly, 
%	\begin{equation}
%		f(\theta_C^*,X)\leq (1+3\varepsilon) f(\theta_X^*,X)+3\varepsilon\nu.
%	\end{equation}
%\end{lemma}
%Therefore, in dynamic data pipelines, we only need to update the coreset after each operation. When the solution is required, we just run the algorithm on the updated coreset. The coreset-based method is a lazy method~\cite{GinartGVZ19}. 

\section{Coreset for Continuous-and-Bounded Learning Problems}\label{standard-coreset}
As mentioned in Section~\ref{sec:robust-coreset}, we need a black-box ordinary coreset (without considering outliers) construction method $\mathcal{A}$ in the hybrid robust coreset framework. In this section, we provide two different $\varepsilon $-coreset construction methods for the CnB learning problems. 



%In last section, our robust coreset is based on the standard $ \varepsilon $-coreset. We give two $\varepsilon $-coreset construction methods for continuous-and-bounded learning problems in this section. The authors emphasize that coreset method presented in this section is a general method for completeness, and we recommend specific method design for particular problems. We first give an algorithm based on importance sampling framework. Then we propose another algorithm inpired by~\cite{Chen09}. The size of our coreset depends on the doubling dimension of the parameter space, which is not related to the specific form of the objective function.

\subsection{Importance Sampling Based Coreset Construction}\label{sec:sensitity}
We follow the importance sampling based approach~\cite{DBLP:conf/soda/LangbergS10}. 
%Our first coreset method is based on the importance sampling, which is a standard sampling scheme to construct coreset.
 Suppose $X=\{x_1, \cdots, x_n\}$. For each data point $ x_i $, it has a sensitivity $  \sigma_i=\sup_{\theta} \frac{f(\theta,x)}{f(\theta,X)}$ that measures its importance to the whole input data $X$. 
%on the objective function as \begin{equation}
%	\label{sensitivity}
%	\sigma(x)=\sup_{\theta} \frac{f(\theta,x)}{f(\theta,X)}.
%\end{equation} 
Computing the sensitivity is often challenging but an upper bound of the sensitivity actually is  already sufficient for the coreset construction. Assume $ s_i $ is an upper bound of $ \sigma_i $ and let $ S=\sum_{i=1}^n s_i $. The coreset construction is as follows. We sample a subset $ C $ from $ X $, where each element of $ C $ is sampled {\em i.i.d.} with probability $ p_i=s_i/S $; we assign a weight $ w_i = \frac{S}{s_i|C|} $ to each sampled data item $x_i$ of $C$. Finally, we return $C$ as the coreset. 


%Then we have the following conclusion. 
\begin{theorem}[\cite{BravermanFL16}]
\label{the-vccoreset}
Let $ \mathtt{vcdim} $ be the VC dimension (or shattering dimension) of the range space induced from $ f(\theta,x) $. 
	If the size of $ C $ is $ \Theta\left( \frac{S}{\varepsilon^2}\left( \mathtt{vcdim}\cdot\log S+\log\frac{1}{\eta}  \right)  \right) $, then $ C $ is an $ \varepsilon $-coreset with probability at least $ 1-\eta $. 
\end{theorem}


%{\color{gray}  If $ |C| $ is $ O\left( \frac{S^2}{\varepsilon^2}\log(\frac{1}{\eta}) \right) $, then we have $ |\mathbbm{f}(\theta,C)-\mathbbm{f}(\theta,X)|\leq \varepsilon \mathbbm{f}(\theta,X) $ for any fixed $ \theta $ with probability $ 1-\eta $ by Hoeffdings inequality. Therefore, the remained thing is to compute suitable $ s_i $s.}

%\begin{wrapfigure}{r}{8cm}[t]
%	\begin{algorithm}[H]
%		\DontPrintSemicolon
%		\caption{Bounding Sensitivity over an Ellipsoid}
%		\label{alg-fp}
%		\KwIn{data $ X\subset \mathbb{R}^{n\times d} $, pre-solution $ \tilde{\theta} $, a prior coefficient $ c $ and $ l $: a number to capture the volume of $ \mathcal{P} $.} 
%		\For{$ x_i\in X $}{Compute the gradient and Hessian matrix of $ f(\theta,x_i) $ at $ \tilde{\theta} $, denoted by $ \mathbf{D}_i $ and $ \mathbf{H}_i $.}
%		
%%		$ \mathbf{D}\gets \sum_{i=1}^{n}\mathbf{D}_i $ \;
%%		$ \mathbf{H}\gets \sum_{i=1}^{n}\mathbf{H}_i $ \;
%		
%		\For{$ i\gets 1 $ {\bfseries to} $ n $}
%		{Solve the following fractional programming:
%%			\begin{eqnarray}
%%				\sup &\frac{f(\tilde{\theta},x_i)+\mathbf{D}_i^T \mathbf{t}+\mathbf{t}^T(\mathbf{H}_i+c\mathbf{I})\mathbf{t}}{f(\tilde{\theta},X)+\mathbf{D}^T \mathbf{t}+\mathbf{t}^T(\mathbf{H}-cn\mathbf{I})\mathbf{t}} \\
%%				&\text{s.t. }\|\mathbf{Et}\|\leq l 
%%			\end{eqnarray}
%			\texttt{\small /* $ \mathbf{E} $ captures the shape of the ellipsoid and it must be full rank. The ellipsoid is a ball when $ \mathbf{E=I} $. */}
%			\vskip 0.1in
%			$ s_i\gets $ the result of the programming.\;
%		}
%		$ S= \sum_{i=1}^n s_i $\;
%		\KwResult{$S$ and all $ s_i $s} 
%	\end{algorithm}
%\end{wrapfigure}

Therefore the only remaining issue is how  to compute the upper bounds $ s_i $s.  Recall that we assume our cost function is $\alpha$-Lipschitz (or $\alpha$-smooth, $ \alpha $-Lipschitz continuous Hessian) in Definition~\ref{def:CB-learning}. That is, we can bound the difference between $f(\theta, x_i) $ and $f(\tilde{\theta}, x_i)$, and such a bound can help us to compute $s_i$. 
%For example, if we assume the cost function to be $ \alpha $-smooth, 
% we have $ f_i(\theta) \leq f_i(\tilde{\theta})+\braket{\nabla f_i(\tilde{\theta}),\Delta \theta}+\frac{\alpha}{2}\|\Delta\theta\|^2 $ and $ f_i(\theta) \geq f_i(\tilde{\theta})+\braket{\nabla f_i(\tilde{\theta}),\Delta \theta}-\frac{\alpha}{2}\|\Delta\theta\|^2 $, where $\Delta \theta=\theta-\tilde{\theta}$. Consequently, we obtain an upper bound of $ \sigma_i $: 
% \begin{eqnarray}
% \sup_{\Delta\theta\in\mathbb{R}^d,\|\Delta\theta\|\leq\ell} \frac{f_i(\tilde{\theta})+\braket{\nabla f_i(\tilde{\theta}),\Delta \theta}+\frac{\alpha}{2}\|\Delta\theta\|^2}{\sum_{i=1}^n f_i(\tilde{\theta})+\braket{\nabla f_i(\tilde{\theta}),\Delta \theta}-\frac{\alpha}{2}\|\Delta\theta\|^2}.\label{for-uppersens}
% \end{eqnarray}
In Section~\ref{sec-appqfp}, we show that computing $s_i$  is equivalent to solving a \textbf{quadratic fractional programming}. This programming can be reduced to a semi-definite programming (SDP)~\cite{BeckT09}, which can be solved in polynomial time up to any desired accuracy~\cite{aasdp}. We denote the solving time of SDP by $ \mathsf{T}(d) $, where $ d $ is the dimension of the data point. So the total running time of the coreset construction is $ O(n\cdot \mathsf{T}(d)) $.



A drawback of Theorem~\ref{the-vccoreset} is that the coreset size depends on $ \mathtt{vcdim}$ induced by $ f(\theta,x) $. For some objectives, the value $ \mathtt{vcdim}$ can be very large or difficult to obtain. Here, we prove that for a continuous-and-bounded cost function, the coreset size can be independent of $ \mathtt{vcdim}$; instead, it depends on the doubling dimension $\mathtt{ddim}$~\cite{DBLP:journals/talg/ChanGMZ16} of the parameter space $\mathcal{P}$. Doubling dimension is a widely used measure to describe the growth rate of the data, which can also be viewed as a generalization of the Euclidean dimension. For example, the doubling dimension of a $d$-dimensional Euclidean space is $\Theta(d)$. 
The proof of Theorem~\ref{the-coreset-1} is placed in Section~\ref{sec:proof-thm-4}. 

\begin{theorem}
    \label{the-coreset-1}
    Given a CnB learning instance $X$ with the objective function $ f(\theta,X) $  as described in Definition~\ref{def:CB-learning},  let $\mathtt{ddim}$ be the doubling dimension of the parameter space. Then, if we run the importance sampling based coreset construction method with the sample size $|C|=\Theta\left(\frac{S^2}{\varepsilon^2}\left(\mathtt{ddim}\cdot\log\frac{1}{\varepsilon}+\log\frac{1}{\eta}\right) \right)$, $ C $ will be an $ \varepsilon $-coreset with probability $ 1-\eta $. The hidden constant of $|C|$ depends on the Lipschitz constant $\alpha$ and $\inf_{\theta\in\mathbb{B}(\tilde{\theta}, \ell)} \frac{1}{n}f(\theta,X)$\footnote{In practice, we often add a positive penalty item to the objective function for regularization, so we can assume that  $\inf_{\theta\in\mathbb{B}(\tilde{\theta}, \ell)} \frac{1}{n}f(\theta,X)$ is not too small.}. 
\end{theorem}
%\begin{remark}
    The major advantage of Theorem~\ref{the-coreset-1} over Theorem~\ref{the-vccoreset} is that we do not need to know the VC dimension induced by the cost function. On the other hand, the doubling dimension is often much easier to know (or estimate), {\em e.g.,} the doubling dimension of a given instance in $\mathbb{R}^d$ is just $\Theta(d)$, even the cost function can be very complicated. Another motivation of  Theorem~\ref{the-coreset-1} is from sparse optimization. Let the parameter space be $\mathbb{R}^D$, and we restrict $\theta$ to be $k$-sparse ({\em i.e.}, at most $k$ non-zero entries with $k\ll D$). It is easy to see the domain of $\theta$ is a union of ${D\choose k}$ $k$-dimensional subspaces, and thus its doubling dimension is $O(k\log D)$ which is much smaller than $D$ (each ball of radius $r$ in the domain can be covered by ${D\choose k}\cdot 2^{O(k)}=2^{O(k\log D)}$ balls of radius $r/2$).
    
    
    The reader is also referred to \cite{HuangJLW18} for a more detailed discussion on the relation between VC (shattering) dimension and doubling dimension.
%\end{remark}

%This programming-based algorithm is a general method to compute $ s_i $s for continuous-and-bounded learning problems. However, the VC dimension induced by $ f(\theta,x) $ depends on the specific expression of $ f(\theta,x) $ and cannot be bounded uniformly.


%The ideally small $ S $ could be non-existent for some problems when $ \theta $ in (\ref{sensitivity}) is over all feasible solutions ~\cite{tolochinsky2018generic,MunteanuSSW18}. This is also the reason we substitute feasible solutions for the rational solutions: The smaller solution region is, the lower $ \sigma_i $s ($ s_i $s) can be.  
%To overcome this for continuous-and-bounded learning problems, we give a method to construct a coreset whose size depends on the doubling dimension of the space of $ \theta $. The doubling dimension is undependent on the specific expression of $ f(\theta,x) $. 


%By Hoeffding's inequality, we can prove that if $ C $ satisfies coreset property (\ref{coreset}) for a single $ \theta $ with probability $ 1-\eta $, then $ O\left(\frac{S^2}{\varepsilon^2}\log(\frac{1}{\eta} ) \right) $ sample size is enough.
%Let $ \mathcal{B}^{\varepsilon \ell} $ be an $ \varepsilon \ell $-net of $ \mathcal{B} $, which means that for any $ \theta\in\mathcal{B} $, there is a $ \theta'\in\mathcal{B}^{\varepsilon \ell} $ such that $ \|\theta-\theta' \|\leq \varepsilon \ell $. Let (\ref{coreset}) hold for all $ \theta\in \mathcal{B}^{\varepsilon \ell} $, which can be done by replace $ \eta $ with $ \eta/|\mathcal{B}^{\varepsilon \ell}| $. Now we have
%\begin{eqnarray}
%	\left|\mathbbm{f}(\theta,X)-\mathbbm{f}(\theta,C)\right|\leq %\left|\mathbbm{f}(\theta,X)-\mathbbm{f}(\theta',X) \right|
%	+\left|\mathbbm{f}(\theta',X)-\mathbbm{f}(\theta',C) \right|
%	+\left|\mathbbm{f}(\theta',C)-\mathbbm{f}(\theta,C) \right|
%\end{eqnarray}  
%The first and last item are both $ \leq \xi(\varepsilon\ell)\leq \varepsilon\xi(\ell) $. The middle term can be bounded by $ \varepsilon \mathbbm{f}(\theta,X) $. Consequently such $ C $ is an $ (\varepsilon,\xi(\ell)) $-coreset of $ X $. Finally we need to estimate $ |\mathcal{B}^{\varepsilon l}| $ by using the doubling dimension of the parameter space.


%$ \mathcal{M} $ is a metric space, we say it has \emph{doubling dimension} $ \mathtt{ddim} $ ($\mathtt{ddim}$ is an integer) if any ball in $ \mathcal{M} $ can be covered by at most $ 2^{\mathtt{ddim}} $ balls of half the radius but not by $ 2^{\mathtt{ddim}-1} $ ones. It is not difficult to derive that a ball of radius $ r $ can be covered by at most $ (\frac{r}{\varepsilon})^{\mathtt{ddim}} $ balls of radius $ \varepsilon $, and, obviously, all centers of these balls constitute an $ \varepsilon $-net of the original ball. So we derive that 
%\begin{lemma}\label{ddim}
%	If $ B $ is a ball with radius $ r $, in the metric space of doubling dimension $ \mathtt{ddim} $, then it have an $ \varepsilon $-net of size $ (\frac{r}{\varepsilon})^{\mathtt{ddim}}$. 
%\end{lemma}
%$ |\mathcal{B}^{\varepsilon l}|=\left( \frac{\ell}{\varepsilon\ell}\right)^{\mathtt{ddim}}=\left( \frac{1}{\varepsilon}\right)^{\mathtt{ddim}} $, where $ \mathtt{ddim} $ is the doubling dimension of the parameter space.   
%Hence, the sample size of this additve coreset is $ O\left(\frac{S^2}{\varepsilon^2}\left(\mathtt{ddim}\log(\frac{1}{\varepsilon})+\log(\frac{1}{\eta})\right) \right) $.

\subsection{Spatial Partition Based Coreset Construction}\label{GSP}
The reader may realize that the coreset size presented in Theorem~\ref{the-coreset-1} (and also Theorem~\ref{the-vccoreset}) is \textbf{data-dependent}. That is, the coreset size depends on the value $S$, which can be different for different input instances. To achieve a \textbf{data-independent} coreset size, we introduce the following method based on spatial partition, which is partly inspired by the previous $k$-median/means clustering coreset construction idea of~\cite{Chen09,DBLP:conf/icml/DingW20,DBLP:conf/icml/HuangHLFD21}.   
We generalize their method to the continuous-and-bounded learning problems and call it as {\em Generalized Spatial Partition (GSP)} method. 

%but ours is different from his in several points. 
%Our method creates coreset with extra additive error but has better generality and his method works only for $ k $-clustering. Our method needs a pre-solution and divides the data due to their cost w.r.t. the pre-solution instead of their distance to a bi-criteria approximate solution. 
%In fact, it is hard to define the bi-criteria approximate solution for general objective functions but the definition of pre-solution is quite natural. Furthermore, we slightly extend Chen's result in the language of doubling dimension.

\textbf{GSP coreset construction.} We set $ \varrho=\min_{x\in X} f(\tilde{\theta},x) $ and $ T = \frac{1}{|X|}f(\tilde{\theta},X) $. Then,
we partition all the data points to different layers according to their cost with respect to $ \tilde{\theta} $. Specifically, we assign a point $ x $ to the   $0$-th layer if $ f(\tilde{\theta},x)-\varrho< T $; otherwise, we assign it to the $ \lfloor\log ( \frac{f(\tilde{\theta},x)-\varrho}{T}) \rfloor$-th layer. Let $L$ be the number of layers, and it is easy to see $ L $ is at most $\log n+1$. For any $0\leq j\leq L$, we denote the set of points falling in the $j$-th layer as $X_j$. From each $X_j$, we take a small sample $C_j$ uniformly at random, where each point of $C_j$ is assigned the weight $ |X_j|/|C_j| $. Finally, the union set $\bigcup^L_{j=0} C_j$ form our final coreset. 



%For each layer $ i $, set the weights of all points in $ C_i $ to be $ |X_i|/|C_i| $. Let the union of $ C_i $ be $ C $ and we can prove that $ C $ is an $ (\varepsilon,\xi(\ell)) $-coreset for a fixed parameter. 



%Let $ X_i $ be the set of points of $ X $ at layer $ i $. This indicates that if $ x\in X_i (i\geq 1)$, then $ f(\tilde{\theta},x) $ is between $ 2^{i-1} T + \varrho $ and $ 2^{i}T + \varrho $ (between $\varrho$ and $ T+\varrho $ when $ i=0 $). Since $ |f(\theta,x)-f(\tilde{\theta},x)|\leq \xi(\ell)  $ for any $ \theta\in \mathcal{B} $ and any point $ x $, if $ x $ is in layer $ i $, we have
%\begin{equation}\label{value-range}
%	f(\theta,x)\in\begin{cases}
%		&[2^{i-1}T-\xi(\ell)+\varrho,2^iT+\xi(\ell)+\varrho] \quad i>0 \\
%		&[\varrho,T+\xi(\ell)+\varrho]\quad i=0
%	\end{cases}
%\end{equation}
%\begin{lemma}[\cite{Haussler92}]\label{Haussler92}
%	Let $ g(\cdot) $ be a function defined on a set $ V $ and for all $ x\in V $, $ g(x)\in [a,b] $ such that $ b-a\leq h $. Let $ U\subseteq V $ be a set independently and uniformly sampled from $ V $. If $ |U|\geq (h^2/2\varepsilon^2)\ln(\frac{2}{\eta}) $, then $ \Pr\left[\left| \frac{g(V)}{|V|}-\frac{g(U)}{|U|} \right|\geq \varepsilon \right]\leq \eta $. 
%\end{lemma}
%For every layer, we use the lemma \ref{Haussler92} and (\ref{value-range}). To make all inequalities hold at the same time, we need to replace $ \eta $ with $ \eta/(L+1) $. 
%Applying (\ref{value-range}) to lemma \ref{Haussler92}, we can prove:
%\begin{lemma}
%	If we sample $ O\left(\frac{1}{\varepsilon^2}\left(\log(\frac{1}{\eta})+\log\log n \right)\right) $ points independently and uniformly from $ X_i $, denoted by $ C_i $, then for a fixed parameter $ \theta $, we have
%	\begin{equation}\label{1layer-1}
%		\left| \frac{f(\theta,X_i)}{|X_i|}-\frac{f(\theta,C_i)}{|C_i|} %\right|\leq
%		\begin{cases}
%			\varepsilon(2^{i-1}T+2\xi(\ell)) \ (i\geq 1) \\ %\varepsilon(T+\xi(\ell)) \ (i=0)
%		\end{cases}
%	\end{equation}
%	with probability $ 1-\eta $.
%\end{lemma}
%The total running time is $ O(n) $. Hence we have the following:
%\begin{lemma}\label{lemma-layer}
%	We can compute a weighted subset $ C\subseteq X $ of size $ O\left(\frac{\log n}{\varepsilon^2}\log(\frac{1}{\eta})\right) $\footnote{We omit the $ \log\log(\cdot) $ item.} such that 
%	\begin{equation}\label{fixed-parameter}
%		|\mathbbm{f}(\theta,C)-\mathbbm{f}(\theta,X)|\leq %\varepsilon(\mathbbm{f}(\theta,X)+\xi(\ell))
%	\end{equation}
%	holds for a fixed parameter $ \theta $ with probability $ 1-\eta $.
%\end{lemma}

%\begin{remark}
%	We set the value of $ T $ to be $ f(\tilde{\theta},X)/|X| $ for bounding the number of layers. In practice we usually set $ T $ much smaller than this and will not increase the number of layer very much. When we delete points, $ |X_0|T<f(\tilde{\theta},X) $ will hold. If the data updating is not acute, we do not need to adjust $ T $. Also, we can evaluate $ f(\tilde{\theta},X)/|X| $ by taking a random sampling instead of compute the sum of all $ f(\tilde{\theta},x) $. 
%\end{remark}

%We have shown how to construct a set $ C $ satisfying the coreset property for any fixed parameter. Analogously, We are able to obtain the uniform guarantee by following steps in \ref{sec:sensitity}. We have
%To make this, in the framework of importance sampling before, besides $ \varepsilon,S $ and $ \eta $, the size of $ C $ depends on the pseudo-dimension of the objective function~\cite{FeldmanL11} or the VC dimension of the range space induced by the objective function~\cite{BravermanFL16}. In this section, we give a simple result depending on the doubling dimension of the parameter space.
\begin{theorem}\label{thm:standard-coreset}
 Given a CnB learning instance $X$ with the objective function $ f(\theta,X) $  as described in Definition~\ref{def:CB-learning},  let $\mathtt{ddim}$ be the doubling dimension of the parameter space.  The above coreset construction method GSP can achieve an $ \varepsilon $-coreset of size $ \Theta\left(\frac{\log n}{\varepsilon^2}\left(\mathtt{ddim}\cdot\log\frac{1}{\varepsilon}+\log\frac{1}{\eta}\right)\right) $ in linear time. The hidden constant of $|C|$ depends on the Lipschitz constant $\alpha$ and $\inf_{\theta\in\mathbb{B}(\tilde{\theta}, \ell)} \frac{1}{n}f(\theta,X)$.  
 \end{theorem}
 To prove Theorem~\ref{thm:standard-coreset}, the key is  show that each $C_j$ can well represent the layer $X_j$ with respect to any $\theta$ in the bounded region $\mathbb{B}(\tilde{\theta}, \ell)$. First, we use the continuity property to bound the difference between $f(\theta, x)$ and $f(\tilde{\theta}, x)$ for each $x\in X_j$ with a fixed $\theta$; then, together with the doubling dimension, we can generalize this bound to any $\theta$ in the bounded region. 
 The full proof is shown in Section~\ref{sec-appthestancoreset}. 
 
 
%Finally we just need to convert the additive coreset to the standard $\varepsilon$-coreset.
%\begin{theorem}[$ (\varepsilon,\nu) $-coreset]
%	\label{additive-coreset}
%	Given a set $ X $ of $ n $ points, a parameter region $ \mathcal{R} $ centered at $ \tilde{\theta} $ with radius $ l $ and $ \varepsilon,\eta\in (0,1) $. $ f(\theta,x) $ is an $ \alpha $-parametric Lipschitz continuous function over $ \mathcal{R} $. We can compute a weighted set $ C $ of size $ O\left(\frac{\log n}{\varepsilon^2}\left(\log(\frac{1}{\eta})+\mathtt{ddim}\cdot\log(\frac{1}{\varepsilon})\right)\right)  $ such that $ C $ is an $ (\varepsilon,\xi{\ell}) $-coreset of $ X $ for $ f(\theta,x) $ over $ \mathcal{R} $ with probability $ 1-\eta $. If $ f(\theta,x) $ is differentiable, then the size can be improved to $ O\left(\frac{\min\{\log n,S^2\}}{\varepsilon^2}\left(\log(\frac{1}{\eta})+\mathtt{ddim}\log(\frac{1}{\varepsilon})\right)\right) $.
%\end{theorem}



\section{Experiments}
\label{sec-app}
In this section, we illustrate the applications of our proposed robust coreset method in machine learning. 
% More experimental results are placed in Section \ref{supp:instance} of the %supplement.
% We leave the more detailed experimental results to the supplement.

\textbf{Logistic regression (with outliers).} 
Given $ x,\theta \in\mathbb{R}^d $ and $ y\in\{\pm 1 \} $, the loss function of Logistic regression is 
\begin{equation}
	f(\theta,x) = \ln (1+\exp(-y\cdot\braket{\theta,x} )).
\end{equation}

%\textcolor{red}{this sentence is not necessary here: This function is $ \|x\| $-Lipschitz and $ \frac{\|x\|^2}{4} $-smooth.}

\textbf{$k$-median/means clustering (with outliers).} 
The goal is to find $k$ cluster centers $ \mathtt{Cen}=\{c_1,c_2,\cdots,c_k \} $ $\subset\mathbb{R}^d$; the cost function of $ k $-median ({\em resp.} $k$-means) clustering for each $x\in \mathbb{R}^d$ is $ f(\mathtt{Cen},x)=\min_{c_i\in \mathtt{Cen}}d(c_i,x)$ $ (\text{\em resp.}\ d(c_i,x)^2) $, where $d(c_i,x)$ denotes the Euclidean distance between $c_i$ and $x$.

%\textcolor{red}{this sentence is not necessary here, we just explain it in the full version: With a small modification of the definition of CnB learning problems (detailed explanation is put in the %supplement), we can consider that the cost function of $k$-median is $1$-Lipschitz and that of $ k $-means is $ 2\|x\| $-Lipschitz.}

%\textbf{Truth discovery (with outliers).} {\color{red} show the definition and explain why it is a CnB problem.}

%\subsection{Experiments}
%\label{experiment}
%We evaluate the practical performance of our proposed coreset construction methods in this section. 

All the algorithms were implemented in Python  on a PC with 2.3GHz Intel Core i7 CPU and 32GB of RAM. All the results were averaged across $ 5 $ trials.

\textbf{The algorithms.} We use the following three representative coreset construction methods as the black box in our hybrid framework for outliers. (1) \textsc{Uniform}: the simple uniform sampling method; (2) \textsc{GSP}: the generalized spatial partition method proposed in section \ref{GSP}; (3) \textsc{QR}: a QR-decomposition based importance sampling method proposed by~\cite{MunteanuSSW18} for logistic regression. For each coreset method name, we add a suffix ``$+$'' to denote the   corresponding robust coreset enhanced by our hybrid framework proposed in section~\ref{robust}. 



For many optimization with outliers problems, a commonly used strategy is alternating minimization ({\em e.g.,} \cite{Chawla2013kmeansAU}). In each iteration, it detects the $ z $ outliers with the largest losses and run an existing algorithm (for ordinary logistic regression or $k$-means clustering) on the remaining $ n-z $ points; then updates the $ z $ outliers based on the obtained new solution. The algorithm repeats this strategy until the solution is stable. For logistic regression with outliers, we run the codes from the scikit-learn package\footnote{\url{https://scikit-learn.org/stable/}} together with the alternating minimization. For $ k $-means with outliers, we use the local search method~\cite{GuptaLSM2017} to seed initial centers and then run the $ k $-means{-}{-} algorithm~\cite{Chawla2013kmeansAU}. We apply these algorithms on our obtained coresets. 
To obtain the initial solution $ \tilde{\theta} $, we just simply run the algorithm  on a small sample (less than 1\%) from the input data.
\par 




\textbf{Datasets.} 
We consider the following two real datasets in our experiments. The dataset \textbf{Covetype}~\cite{BLACKARD1999131} consists of $ 581012 $ instances  with $54$ cartographic features for predicting forest cover type. There are $ 7 $ cover types and we set the dominant one (49\%) to be the positive samples and the others to be negative samples. We randomly take $ 10000 $ points as the test set and the remaining data points form the training set. The dataset \textbf{3Dspatial}~\cite{Kaul0J13} comprises $434874$ instances with $ 4 $ features for the road information. To generate outliers for the unsupervised learning task $k$-means clustering, we randomly generate $ 10000 $ points in the space as the outliers, and add the gaussian noisy $ \mathcal{N}(0,200) $ to each dimension for these outliers. For the supervised learning task logistic regression, we add Gaussian noise to a set of randomly selected $10000$ points (as the outliers) from the data and also randomly shuffle their labels.
%together with adding Gaussian noise, we randomly shuffle the labels of these selected outliers.
%

%{\color{gray} We are also aware that several coreset based methods for learning with noise were proposed recently~\cite{glister, MirzasoleimanCL20}. But their objective is different from the conventional (robust) coreset~\cite{Har-PeledM04,FeldmanL11,HuangJLW18}. 
 %Rather than the weighted subset strictly satisfying some mathematical approximation requirements, Their ``roubst coreset is more likely a noise-resistent training technique used in the optimization procedure. The usage of their coresets method have different purpose compared with ours. 
%We believe that our coreset can be used to accelerate their training process, as a substitute of the big training set.} {\color{red} I don't think this paragraph is necessary to our paper, so maybe remove it.}


\begin{table}[ht]
\scriptsize
\begin{minipage}{0.45\linewidth}
\centering
\caption{\small Logistic regression  on Covetype. $|C|$ denotes the coreset size.}\label{lr-time}
	\begin{tabular}{|c|c|c|c|}
		\hline
		Method     & $|C|$  & Loss ratio & Speed-up    \\ \hline
		\textsc{GSP+}        & $4\times 10^3$  &   1.046        & $\times 26.9$  \\ \hline
		\textsc{GSP+}        & $8\times 10^3$  &   1.031     & $\times 19.19 $   \\ \hline
		\textsc{Uniform+ }   & $4\times 10^3$  &   1.134     & $\times 45.8$   \\ \hline
		\textsc{Uniform+}    & $8\times 10^3$  &   1.050     & $\times 29.1 $    \\ \hline
		\textsc{QR+} & $4\times 10^3$  &   1.025     & $\times 23.4$    \\ \hline
		\textsc{QR+} & $8\times 10^3$  &   1.012     & $\times 17.9 $   \\ \hline
	\end{tabular}
\end{minipage}
\hfill
\begin{minipage}{0.45\linewidth}
\centering
\caption{\small $ k $-means clustering on 3Dspatial with $ k=10 $. $|C|$ denotes the coreset size. }\label{clustering-time}
	\begin{tabular}{|c|c|c|c|}
		\hline
		Method    & $|C|$  & Loss ratio & Speed-up    \\ \hline
		\textsc{GSP+  }      & $5\times 10^3$  &    1.016      & $\times 41.1$ \\ \hline
	\textsc{	GSP+ }       & $ 10^4$ &    1.008        & $\times 15.4$  \\ \hline
		\textsc{Uniform+ }   & $5\times 10^3$  &     1.029      & $\times 78.9$  \\ \hline
		\textsc{Uniform+ }   & $ 10^4$  &     1.011        & $\times 46.9$  \\ \hline
	\end{tabular}
\end{minipage}
\end{table}

\begin{figure}[ht]
	\centering
	\vspace{-0.5cm}
	\subfigure[{\small Loss}]{\includegraphics[width=0.49\linewidth]{fig/lr-loss}\label{fig:lr-a}}
	\subfigure[{\small Accuracy} ]{\includegraphics[width=0.49\linewidth]{fig/lr-score}\label{fig:lr-b}}
	\subfigure[{\small Speed-up with the Merge-and-Reduce tree in the dynamic setting.}]{\includegraphics[width=0.49\linewidth]{fig/dynamic-time}\label{fig:lr-c}}
	\caption{The performances of different coreset methods for logistic regression on Covetype. The results are normalized over the results obtained from the original data (without using coreset).}
	\label{fig:lr-loss}
\end{figure}

\textbf{Results.} 
Table \ref{lr-time} and Table \ref{clustering-time} illustrate the loss ratio (the obtained loss over the loss without using coreset) and speed-up ratio of different robust coreset methods. We can see that the robust coreset methods can achieve significant speed-up, and meanwhile the optimization qualities can be well preserved (their loss ratios are very close to $1$). 
Figure~\ref{fig:lr-a} and \ref{fig:lr-b} illustrate the performance of the (robust) coreset methods with varying the coreset size. In general, our robust coreset can achieve better performance (in terms of the loss and accuracy) compared with its counterpart  without considering outliers. 
Figure~\ref{fig:lr-c} illustrates the speed-up ratio of running time in the dynamic setting. Our robust coreset construction uses the merge-and-reduce tree method. When the update happens in one bucket, we perform a ``bottom-up'' re-construction for the coreset. We let the bucket size be $ n/2^{h-1} $, where $ h $ is the height of the tree; thus the higher the tree, the smaller the bucket size (and the speed-up is more significant). The results reveal that using the coreset yields considerable speed-up compared to re-running the algorithm on the entire updated dataset.  


\section{Conclusion}\label{conclusion}
In this paper, we propose a novel robust coreset framework for the continuous-and-bounded learning problems (with outliers). Also, our framework can be efficiently implemented in the dynamic setting. In future, we can consider generalizing our proposed (dynamic) robust coreset method to other types of optimization problems ({\em e.g.,} privacy-preserving and fairness); it is also interesting to consider implementing our method for distributed computing or federated learning. 

\section*{Acknowledgment}
We would like to thank the anonymous reviewers for their helpful suggestions and comments.


\bibliographystyle{alpha}
\bibliography{reference}



\input{supp}




\end{document}