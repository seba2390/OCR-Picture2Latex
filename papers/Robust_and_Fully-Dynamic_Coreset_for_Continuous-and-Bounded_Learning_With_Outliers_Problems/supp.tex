%auto-ignore 
%!TEX root = arxiv-core.tex

\newpage
\appendix
\begin{comment}
\begin{center}
    {\LARGE\bf Supplement for Robust and Fully-Dynamic Coreset for Continuous-and-Bounded Learning (With Outliers) Problems}
\end{center}
\bigskip
\end{comment}



\section{Examples for Continuous-and-Bounded Learning Problem}
\label{supp:instance}

\paragraph{Logistic Regression}
For $ x,\theta \in\mathbb{R}^d $ and $ y\in\{\pm 1 \} $, the loss function of Logistic regression is 
\begin{equation}
	f(\theta,x) = \ln (1+\exp(-y\cdot\braket{\theta,x} )).
\end{equation}
We denote the upper bound of $ \|x\| $ by $ \Delta $, then this loss function is $ \Delta $-Lipschitz and $ \frac{\Delta ^2}{4} $-smooth. 
%We denote the diameter of the dataset by $ \Delta $, which means that $ \|x\|\leq \Delta $ for all data point $ x $.
%{\color{red} This loss function is $ \|x\| $-Lipschitz and $ \frac{\|x\|^2}{4} $-smooth. (should we set an upper bound for $||x||$?)}


\paragraph{Bregman Divergence~\cite{BanerjeeMDG05}}
Let function $ \phi:\mathbb{R}^d\rightarrow \mathbb{R} $ be strictly convex and differentiable, then the Bregman divergence between $ x,y\in\mathbb{R}^d $ respect to $ \phi $ is 
\begin{equation}
	d_\phi(y,x) = \phi(y)-\phi(x)-\braket{\nabla\phi(x),y-x}.
\end{equation}
%Bregman clustering is clustering with Bregman divergence $ d_\phi(y,x) $.


If we assume $ \|\nabla\phi(x)\| \leq L $ for any $x $ with some $L>0$, then we have 
\[ d_\phi(y,x)\leq  L\|y-x\|+\|\nabla \phi(x)\|\|y-x\| \leq 2L\|y-x\|.  \]
So in this case the Bregman divergence function is $ 2L $-Lipschitz.

% \paragraph{Kernel Estimation~\cite{}}
\paragraph{Truth Discovery~\cite{LiXY20,DBLP:journals/sigkdd/LiGMLSZFH15}} Truth discovery is used to aggregate multi-source information to achieve a more reliable result; the topic has been extensively studied in the area of data mining (especially for crowdsourcing). Suppose
$ \theta,x\in\mathbb{R}^d $ where $\theta$ is the ``truth vector'' and $x$ is the vector provided by a source, and then the loss function of $x$ is $ f(\theta,x)=f_{truth}(\|\theta-x\|) $, where
\begin{equation}
	f_{truth}(t)=
	\begin{cases}
		t^2\quad &0 \leq t < 1;  \\
		1+\log t^2 & t \geq 1.
	\end{cases}
\end{equation}
We can prove that $ f(\theta,x) $ is $ 2 $-Lipschitz.

\paragraph{$ k $-median/means clustering in $\mathbb{R}^d$} Suppose $x\in\mathbb{R}^d$. 
Let $ \mathtt{Cen}=\{c_1,c_2,\cdots,c_k \} $ where $ c_i\in \mathbb{R}^d $  for $1\leq i\leq k$; the cost function of $ k $-medians ({\em resp.,} $k$-means) on $x$ is $ f(\mathtt{Cen},x)=\min_{c_i\in \mathtt{Cen}}d(c_i,x)\ (resp.,\ d(c_i,x)^2) $, where $d(c_i,x)$ denotes the Euclidean distance between $c_i$ and $x$. We also need to define the distance between two center sets. Let   $ \mathtt{Cen}'=\{c'_1,c'_2,\cdots,c'_k \} $ be another given center set. 
We let $ d(\mathtt{Cen},\mathtt{Cen}') := \max_{1\leq i \leq k} d(c_i,c'_i) $. It is easy to see that this defines a metric for center sets. Therefore, the bounded region  considered here is the Cartesian product
 of $k$ balls. i.e., $ \mathbb{B}(\widetilde{\mathtt{Cen}},\ell) =\bigoplus_{i=1}^k \mathbb{B}(\tilde{c}_i,\ell) $ with $ \widetilde{\mathtt{Cen}}=\{\tilde{c}_1,\tilde{c}_2,\cdots,\tilde{c}_k \} $.


We take the $k$-median clustering problem as an example. Suppose the nearest neighbors of $x$ among $\mathtt{Cen}$ and $\mathtt{Cen}'$ are $c_i$ and $c'_j$, respectively.  Then we have
\begin{align*}
    f(\mathtt{Cen},x)-f(\mathtt{Cen}',x)=d(c_i,x)- d(c'_j,x)\leq d(c_j,x)-d(c'_j,x)\leq d(c_j,c'_j) \leq d(\mathtt{Cen},\mathtt{Cen}');\\
    f(\mathtt{Cen}',x)-f(\mathtt{Cen},x)=d(c'_j,x)- d(c_i,x)\leq d(c'_i,x)-d(c_i,x)\leq d(c'_i,c_i)\leq d(\mathtt{Cen},\mathtt{Cen}'),
\end{align*}
which directly imply $ |f(\mathtt{Cen},x)-f(\mathtt{Cen}',x)|\leq d(\mathtt{Cen},\mathtt{Cen}') $.
Hence the $k$-median problem is $ 1 $-Lipschitz.


\section{Quality Guarantee Yielded from  Robust Coreset}
\label{supp:quality}

For the case without outliers, it is easy to see that the optimal solution of the $\varepsilon$-coreset is a $(1+3\varepsilon)$-approximate solution of the full data. Specifically, let $ \theta_C^* $ be the optimal solution of an $\varepsilon$-coreset and $ \theta_X^* $ be the optimal solution of the original dataset $X$; then for any  $\varepsilon \in (0, 1/3)$, we have
\begin{equation}
    f(\theta_C^*,X)\leq (1+3\varepsilon)f(\theta_X^*,X).
\end{equation}
But when considering the case with  outliers, this result only holds for $ (0,\varepsilon) $-robust coreset ({\em i.e.,} $\beta=0$). 
If $ \beta > 0 $, we can obtain a slightly weaker result (exclude slightly more than $z$ outliers). 

\begin{lemma}
Given two parameters $ 0<\varepsilon<1/3 $ and $ 0\leq \beta < 1/2 $, suppose $C$ is a $(\beta,\varepsilon)$-robust coreset of $ X $ with $(1+2\beta)z $ outliers. Let
$ \theta_C^* $ be the optimal solution of the instance $ (C,(1+2\beta)z) $, $ \theta_X^* $ be the optimal solution of $ (X,z) $ respectively.
Then we have
\begin{equation}
    f_{(1+4\beta)z}(\theta_C^*,X)\leq (1+3\varepsilon)f_z(\theta_X^*,X).
\end{equation}
\end{lemma}
\begin{proof}
Since $ \beta\in [0,1/2) $ and $ \varepsilon \in (0,1/3) $, we have $ {1+4\beta}\geq (1+2\beta)(1+\beta) $, $ (1+2\beta)(1-\beta) \geq 1 $ and $ \frac{1+\varepsilon}{1-\varepsilon}<1+3\varepsilon $. Thus we can obtain the following bound:
    \begin{align*}
    f_{(1+4\beta)z}(\theta_C^*,X)&\leq f_{(1+2\beta)(1+\beta)z}(\theta_C^*,X)\leq \frac{1}{1-\varepsilon}f_{(1+2\beta)z}(\theta_C^*,C)\\
    &\leq \frac{1}{1-\varepsilon}f_{(1+2\beta)z}(\theta_X^*,C)\leq \frac{1+\varepsilon}{1-\varepsilon}f_{(1+2\beta)(1-\beta)z}(\theta_X^*,X)\\
    &\leq (1+3\varepsilon)f_z(\theta_X^*,X).
    \end{align*}
\end{proof}






\section{Fully-Dynamic Coreset with Outliers}
\label{sec-appdynamic}


In this section, we show that our robust coreset of Section~\ref{sec:robust-coreset} can be efficiently implemented in a fully-dynamic environment, even if the number of outliers $z$ is dynamically changed. 



The standard $ \varepsilon $-coreset usually has two important properties. 
If $ C_1 $ and $ C_2 $ are respectively the $ \varepsilon $-coresets of two disjoint sets $ X_1 $ and $ X_2 $, their union $ C_1\cup C_2 $ should be an $ \varepsilon$-coreset of $ X_1\cup X_2 $. Also, 
if $ C_1 $ is an $ \varepsilon_1$-coreset of $ C_2 $ and $ C_2 $ is an $ \varepsilon_2$-coreset of $ C_3 $,  $ C_1 $ should be an $ (\varepsilon_1+\varepsilon_2+\varepsilon_1\varepsilon_2) $-coreset of $ C_3 $. Based on these two properties, one can build a coreset for incremental data stream by using the ``merge-and-reduce'' technique~\cite{BentleyS80,Har-PeledM04} as shown in Figure~\ref{fig:merge-reduce-tree}. Very recently, Henzinger and Kale~\cite{HenzingerK20} extended it to the more general fully-dynamic setting, where data items can be deleted and updated as well.


Roughly speaking, the merge-and-reduce technique uses a sequence of ``buckets'' to maintain the coreset for the input streaming data, and the buckets are merged by a bottom-up manner. However, it is challenging to directly adapt this strategy to the case with outliers, because we cannot determine the number of outliers in each bucket. 
%Even if we can derive similar results for the robust coreset, they can not be directly applied to the merge-and-reduce technique. Because we still do not know the exact number of outliers in each separated part although the total number of outliers is given~\cite{ChenA018,GuptaLSM2017}. Moreover, in the situation containing outliers, the fully-dynamic setting is more complex: besides the data, the number of outliers $ z $ should be able to be updated.
A cute aspect of our hybrid robust coreset framework is that we can easily resolve this obstacle by using an $O(n)$ size auxiliary table $\mathscr{L}$ together with the merge-and-reduce technique (note that even for the case without outliers, maintaining a fully-dynamic coreset already needs $\Omega(n)$ space~\cite{HenzingerK20}). 

\begin{figure}[ht]
	\subfigure[Merge-and-Reduce (without outliers)]{\includegraphics[width=0.45\linewidth]{fig/coreset-tree}\label{fig:merge-reduce-tree}}
	\subfigure[Merge-and-Reduce with outliers]{\includegraphics[width=0.45\linewidth]{fig/coreset-tree-outlier}\label{fig:merge-reduce-tree-outlier}}
	\caption{}
\end{figure}

Recall that we partition the input data $X$ into two parts: the $n-\tilde{z}$ ``suspected inliers'' and the $\tilde{z}$ ``suspected outliers'', where $\tilde{z}=(1+1/\varepsilon_0)z$. We follow the same notations used in Section~\ref{sec:robust-coreset}. For the first part, we just apply the vanilla merge-and-reduce technique to obtain a fully-dynamic coreset $C_{\mathtt{si}}$; for the other part, we can just take a $\delta$-sample or take the whole set (if we require $\beta$ to be $0$), and denote it as $C_{\mathtt{so}}$. Moreover, we maintain a table $\mathscr{L}$ to record the key values $ x.\mathtt{value}= f(\tilde{\theta},x)$ and its position $ x.\mathtt{position} $ in the merge-and-reduce tree, for each $x\in X$; they are sorted by the $x.\mathtt{value}$s in the table. To deal with the dynamic updates ({\em e.g.,} deletion and insertion), we also maintain a critical pointer $ p $ pointing to the data item $ x_{\tilde{z}} $ and $p\rightarrow \mathtt{value}:=x_{\tilde{z}}.\mathtt{value} $  (recall $x_{\tilde{z}}$ has the $ \tilde{z}$-th largest cost $ f(\tilde{\theta},x) $ among $X$ defined in Section~\ref{sec:robust-coreset}). 

When a new data item $x$ is coming or an existing data item $x$ is going to be deleted, we just need to compare it with $ f(\tilde{\theta},x_c) $ so as to decide to update $C_{\mathtt{si}}$ or $C_{\mathtt{so}}$ accordingly; after the update, we also need to update $ x_{\tilde{z}} $ and the pointer $p$ in $\mathscr{L}$. If the number of outliers $z$ is changed, we just need to update $ x_{\tilde{z}} $ and  $p$ first, and then update $C_{\mathtt{si}}$ and $C_{\mathtt{so}}$ (for example, if $z$ is increased, we just need to delete some items from $C_{\mathtt{so}}$ and insert some items to $C_{\mathtt{si}}$). To realize these updating operations, we also set one bucket as the ``hot bucket'', which serves as a shuttle to execute all the data shifts. See Figure~\ref{fig:merge-reduce-tree-outlier} for the illustration. Let $ M(\varepsilon) $ be the size of the vanilla $\varepsilon$-coreset. In order to achieve an $ \varepsilon $-coreset overall, we need to construct an $ \frac{\varepsilon}{\log n} $-coreset with size $ M(\varepsilon/\log n) $ in every reduce part~\cite{AgarwalHV04}. We use $ M $ to denote $ M(\varepsilon/\log n) $ for short and assume that we can compute a coreset of $ X $ in time $ \mathsf{t}(|X|) $~\cite{DBLP:books/daglib/0035668}. The height of the tree is $ O(\log n) $. 

%Thus the re-construction time along a bottom-up path is $ O(\mathsf{t}(M)\log n) $. We let each data item be inserted from left to right and deleted from right to left. We call the right-end leaf of the current data \emph{hot bucket}, which could be shifted with the number of current data. All insertions and deletions will take place in the hot bucket. Clearly, all leaves on the left of hot bucket are full boxes and all leaves on the right are empty boxes as shown in Figure~\ref{fig:merge-reduce-tree-outlier}.
%The next thing we need to do is to expound the concrete reactions to the operations and an analysis of the updating time. 

\begin{itemize}
	\item \textsc{Insert($ y $)} (a new point $ y $ is inserted.) We insert it into the list $ \mathscr{L} $ according to $y.\mathtt{value}=f(\tilde{\theta},y) $. If $ y.\mathtt{value}>p\rightarrow \mathtt{value} $, shift $ p $ one place to the right. i.e., $ p\gets p+1 $. If $ y.\mathtt{value} \leq p\rightarrow \mathtt{value} $, insert it into the hot bucket and set $ y.\mathtt{position} $, then update the coreset tree from hot bucket to the root. The updating time is $ O(\mathsf{t}(M) \log n) $.
	\item \textsc{Delete($ y $)} (a point $ y $ is deleted.) If $ y $ is a suspected inlier, delete it from its bucket $ y.\mathtt{position} $ and find another point $ y' $ from the hot bucket to re-fill the bucket $ y.\mathtt{position} $. Then update the tree from these two buckets. Finally we delete $ y $ from list $ \mathscr{L} $. If $ y $ is a suspected outlier, delete it from $ \mathscr{L} $. Finally $  p\gets p-1 $ and delete the current critical point from the tree, which is a suspected outlier now. The updating time is $ O(\mathsf{t}(M)\log n) $.
	\item \textsc{Update($ y $)} (a point $ y$ is updated.) 
	%First we update $ y.value $ and its place in $ \mathscr{L} $. If the attribute of $ y $ (suspected inlier or suspected outlier) is not changed, we just update the tree {\color{red} from the bucket $ y.\mathtt{position} $ (is it correct?)}. Otherwise 
	We just need to run \textsc{Delete($ y $)} and \textsc{Insert($ y $)} for this updating. The updating time is $ O(\mathsf{t}(M)\log n) $.
	\item \textsc{Change($ \Delta z $)} (the number of outliers is updated as $ z\gets z+\Delta z $.) 
	%Note that the number of suspected outliers $ \tilde{z} $ is an increasing function of $ z $. 
	We set $ p\gets p+\tilde{z}(z+\Delta z)-\tilde{z}(z) $. If $ \Delta z>0 $, we delete these $ \tilde{z}(z+\Delta z)-\tilde{z}(z) $ points from the tree. If $ \Delta z<0 $, we insert these points from suspected outliers into the tree. Note that we do not need to update $ \mathscr{L} $ in this case. The updating time is $ O(\frac{\Delta z}{\varepsilon}\mathsf{t}(M)\log n) $.
\end{itemize}










%then we have the following result.


\begin{theorem}
\label{the-dynamic}
In our dynamic implementation, the time complexity for insertion and deletion is $O(\mathsf{t}(M)\log n)$. To update $z$ to $z\pm \Delta z$ with $\Delta z\geq 0$, the time complexity is $O(\frac{\Delta z}{\varepsilon}\mathsf{t}(M)\log n)$, where $\varepsilon$ is the error bound for the robust coreset in Definition~\ref{robust-coreset}. 
\end{theorem}







%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%In the setting of dynamic data, we need to re-construct the coreset once an update happens. To save the re-construction time, we apply the properties of coreset to build a tree as shown in Figure~\ref{fig:merge-reduce-tree}. This method is commonly called merge-and-reduce framework~\cite{BentleyS80,Har-PeledM04}. If we only update a leaf node, we just need to re-construct along the path from this leaf to root. However, the problem becomes much more complicated for the case  with outliers. For example, we  cannot know the exact number of outliers in each part of the tree even the total number of outliers $z$ is given. In some scenarios, we also need to consider the case that the value of $z$ is changed dynamically. 

%Second, the fully-dynamic setting should be more flexible in the situation containing outliers. Besides the dataset, the number of outliers $ z $ is also able to change. 


%Besides, to make the root an $ (\varepsilon,\nu) $-coreset we should repalce $ \varepsilon $ by $ \varepsilon/2H $.
%Let the size of the $ (\varepsilon,\nu) $-coreset be $ M(\varepsilon) $. The root of the tree is a coreset of size $ M(\varepsilon/2H) $. We can remove the $ H $ by a small trick. We construct $ (\varepsilon/6H,\nu) $-coreset at each node thus the root is an $ (\varepsilon/3,\nu) $-coreset. Then we compute the $ (\varepsilon/2,\nu) $-coreset of the root. Then we obtain an $ (\varepsilon,\nu) $-coreset of size $ M(\varepsilon/2) $ since $ (1+\varepsilon/2)(1+\varepsilon/3)=1+\varepsilon $.




%For continuous-and-bounded learning problems, we show that it is possible to overcome the challenges for building a fully-dynamic data structure. The key point is that our construction framework is hybrid. As shown in Figure~\ref{fig:merge-reduce-tree-outlier}, we divide whole dataset into two parts (i.e., the suspected inliers and the suspected outliers); then we construct $ \varepsilon $-coreset for the first  part and take a uniform sample for the second part. In particular we do not need to know the number of outliers in each separated part (each data item can be simply  classified to one of these two parts by the ``threshold'' distance $\tau$ as (\ref{markov})). When $ z $ changes, we can shift the critical point (which corresponds to the number $\tilde{z}$) in Figure~\ref{fig:merge-reduce-tree-outlier}, then the problem is reduced to the data insertion and deletion. 
% In fact, we achieve a fully-dynamic robust coreset such that the updating time is $ O(M\log n) $. $ M $ is the size of the nodes in the tree.


%We assume that the number $n$ is known. We maintain a list of data $ \mathscr{L} $ sorted by the value  $ x.value= f(\tilde{\theta},x) $ for each data point $ x $. According to the construction method of Theorem \ref{thm-coreset-outlier}, we maintain a critical pointer $ p $ pointing to the critical point $ x_c $. Hence all data points after the critical point are suspected outliers. Then we build the tree for the suspected inliers. Note that for a suspected inlier $ x $, we need $ x.position $ to record its position in the tree.  
%We shortly denote by $ M $ the size of the coreset used for the problem. 
%To bound the accumulative error from the overall bottom-up reduce operation, we need to construct $ \frac{\varepsilon}{\log n} $-coreset in each reduce step~\cite{AgarwalHV04}. 
%Let $ M(\varepsilon) $ be the size of the vanilla $\varepsilon$-coreset and we will use $ M $ to denote $ M(\varepsilon/\log n) $ for short in the later statement. 







%And our robust coreset method is helpful to boost the performance. Figure \ref{fig:lr-loss-b} is the accuracy-size line chart. The performance here is not very clear, and the trend is positive. And robust coreset also outperform here.


%	The proof of the merge property is straightforward, we only prove the latter one.
%	\begin{align*}
%		\mathbbm{f}(\theta,C_1)&\leq (1+\varepsilon_1)(\mathbbm{f}(\theta,C_2))+\varepsilon_1\nu \\
%		&\leq (1+\varepsilon_1)\left( (1+\varepsilon_2)\mathbbm{f}(\theta,C_3)+\varepsilon_2\nu \right)+\varepsilon_1\nu\\
%		&=(1+\varepsilon_1)(1+\varepsilon_2)\mathbbm{f}(\theta,C_3)\\
%		&\quad\quad +((1+\varepsilon_1)(1+\varepsilon_2)-1)\nu\\
%		\mathbbm{f}(\theta,C_1)&\geq (1-\varepsilon_1)(\mathbbm{f}(\theta,C_2))-\varepsilon_1\nu\\
%		&\geq (1-\varepsilon_1)\left( (1-\varepsilon_2)\mathbbm{f}(\theta,C_3)-\varepsilon_2\nu \right)-\varepsilon_1\nu\\
%		&=(1-\varepsilon_1-\varepsilon_2+\varepsilon_1\varepsilon_2)\mathbbm{f}(\theta,C_3)\\
%		&\quad\quad-(\varepsilon_1+\varepsilon_2-\varepsilon_1\varepsilon_2)\nu\\
%		&\geq (1-((1+\varepsilon_1)(1+\varepsilon_2)-1))\mathbbm{f}(\theta,C_3)\\
%		&\quad\quad-((1+\varepsilon_1)(1+\varepsilon_2)-1)\nu
%	\end{align*}
%\end{proof}
\section{Quadratic Fractional Programming}
\label{sec-appqfp}

In this section, we present an algorithm to compute the upper bound of the sensitivity for the CnB learning problems via the quadratic fractional programming. 
Recall that $ \sigma_i=\sigma(x_i)=\sup_\theta\frac{f(\theta,x_i)}{f(\theta,X)} $ is the sensitivity of data point $ x_i $. We denote $ f(\theta,x_i) $ by $ f_i(\theta) $ for convenience. Because the loss function is $\alpha$-Lipschitz (or $\alpha$-smooth, $ \alpha $-Lipschitz continuous Hessian), we can bound the difference between $f_i(\theta) $ and $f_i(\tilde{\theta})$. For example, if we assume the cost function to be $ \alpha $-smooth, 
 we have $ f_i(\theta) \leq f_i(\tilde{\theta})+\braket{\nabla f_i(\tilde{\theta}),\Delta \theta}+\frac{\alpha}{2}\|\Delta\theta\|^2 $ and $ f_i(\theta) \geq f_i(\tilde{\theta})+\braket{\nabla f_i(\tilde{\theta}),\Delta \theta}-\frac{\alpha}{2}\|\Delta\theta\|^2 $, where $\Delta \theta=\theta-\tilde{\theta}$. Consequently, we obtain an upper bound of $ \sigma_i $: 
 \begin{eqnarray}
 \sup_{\Delta\theta\in\mathbb{R}^d,\|\Delta\theta\|\leq\ell} \frac{f_i(\tilde{\theta})+\braket{\nabla f_i(\tilde{\theta}),\Delta \theta}+\frac{\alpha}{2}\|\Delta\theta\|^2}{\sum_{i=1}^n f_i(\tilde{\theta})+\braket{\sum_{i=1}^n\nabla f_i(\tilde{\theta}),\Delta \theta}-\frac{\alpha n}{2}\|\Delta\theta\|^2}.\label{for-uppersens}
 \end{eqnarray}
 Note that $f_i(\tilde{\theta})$, $\nabla f_i(\tilde{\theta})$, $\sum_{i=1}^n f_i(\tilde{\theta})$, and $\sum_{i=1}^n\nabla f_i(\tilde{\theta})$ in (\ref{for-uppersens}) are all constant. 
Thus it is a standard $d$-dimensional quadratic fractional programming over a bounded ball, which can be reduced to an instance of the semi-definite programming~\cite{BeckT09}.

\section{Omitted Proofs}

\subsection{More Details for Definition~\ref{def:CB-learning}}
\label{sec-appcbl}
Recall that we have $ |g(\theta_1)-g(\theta_2)|\leq \alpha \|\Delta\theta \| $ for $ \alpha $-Lipschitz continuous function $ g $. Similarly, the $\alpha $-Lipschitz continuous gradient and $ \alpha $-Lipschitz continuous Hessian also imply the bounds of the difference between $ g(\theta_1) $ and $ g(\theta_2) $: 
\begin{align*}
	&|g(\theta_1)-g(\theta_2)-\braket{\nabla g(\theta_2),\Delta \theta}|\leq \frac{\alpha}{2}\|\Delta \theta\|^2,\\
	&|g(\theta_1)-g(\theta_2)-\braket{\nabla g(\theta_2),\Delta \theta}-\frac{1}{2}\braket{\nabla^2 g(\theta_2)\Delta\theta,\Delta \theta}|\leq \frac{\alpha}{6}\|\Delta \theta\|^3.
\end{align*}
In general, as for continuous-and-bounded learning problems, we can bound $|f(\theta,x)-f(\tilde{\theta},x)| $ by a low degree polynomial function. As shown above, if $ f(\cdot,x) $ is $ \alpha $-Lipschitz, the polynomial function is $ \xi(\ell)=\alpha\ell $; if $ f(\cdot,x) $ is $ \alpha $-smooth, the polynomial function is $ \xi(h;\ell)=h\ell+\alpha\ell^2/2 $, where $ h=\max_{x\in X} \|\nabla f(\theta,x)_{|\tilde{\theta}} \| $. The following proofs are presented for the general case. Namely, we always use a unified polynomial $ \xi(\ell) $ to represent this difference instead of specifying the type of continuity of $f(\cdot,x)$.


\subsection{Proof of Theorem \ref{thm-coreset-outlier}}
\label{sec-appthe2}

\begin{figure}[h]
	%\vspace{-0.4cm}
	\subfigure[$ X_\mathrm{\Rnum{3}} $ and $ X_\mathrm{\Rnum{4}} $ are both non-empty]{\includegraphics[width=0.3\linewidth]{fig/arrange-1}}
	\subfigure[$ X_\mathrm{\Rnum{4}}=\varnothing $]{\includegraphics[width=0.3\linewidth]{fig/arrange-2}}
	\subfigure[$ X_\mathrm{\Rnum{3}}=\varnothing $]{\includegraphics[width=0.3\linewidth]{fig/arrange-3}}\vskip -0.1in
	\caption{The partition of dataset determined by $ \theta $ and $ \tilde{\theta} $.}
	\label{fig:arrange}
%	\vskip -0.1cm
\end{figure}

Recall that $ X_{\mathtt{so}} $ comprises $ \tilde{z} $ suspected outliers w.r.t. $ \tilde{\theta} $; $ X_{\mathtt{si}} $ comprises $ n-\tilde{z} $ suspected inliers w.r.t. $ \tilde{\theta} $; $ X_{\mathtt{ro}} $ comprises $ z $ ``real'' outliers w.r.t. $ \theta $; $ X_{\mathtt{ri}} $ comprises $ n-z $ ``real'' inliers w.r.t. $ \theta $. As mentioned in Section \ref{sec:robust-coreset}, parameters $ \tilde{\theta} $ and $ \theta $ partition $ X $ into at most 4 parts as shown in Figure \ref{fig:arrange}:
\begin{equation}
\text{$ X_{\mathrm{\Rnum{1}}}=X_{\mathtt{si}}\cap X_{\mathtt{ri}}$, $ X_{\mathrm{\Rnum{2}}}=X_{\mathtt{so}}\cap X_{\mathtt{ri}}$, $ X_{\mathrm{\Rnum{3}}}=X_{\mathtt{so}}\cap X_{\mathtt{ro}}$, and $ X_{\mathrm{\Rnum{4}}}=X_{\mathtt{si}}\cap X_{\mathtt{ro}}$.}
\end{equation}
Similarly, $ \tilde{\theta} $ and $ \theta $ yield a classification on $ C $:
\begin{equation}
\text{$ C_{\mathrm{\Rnum{1}}}=C_{\mathtt{si}}\cap C_{\mathtt{ri}}$, $ C_{\mathrm{\Rnum{2}}}=C_{\mathtt{so}}\cap C_{\mathtt{ri}}$, $ C_{\mathrm{\Rnum{3}}}=C_{\mathtt{so}}\cap C_{\mathtt{ro}}$, and $ C_{\mathrm{\Rnum{4}}}=C_{\mathtt{si}}\cap C_{\mathtt{ro}}$. }
\end{equation}

Note that $ C_\mathrm{\Rnum{1}}+C_\mathrm{\Rnum{4}} $ is an $ \varepsilon_1 $-coreset of $ X_\mathrm{\Rnum{1}}+X_\mathrm{\Rnum{4}} $, $ C_\mathrm{\Rnum{2}}+C_\mathrm{\Rnum{3}} $ is a $ \delta $-sample of $ X_\mathrm{\Rnum{2}}+X_\mathrm{\Rnum{3}} $, where $ \delta=\frac{\beta\varepsilon_0}{1+\varepsilon_0} $ with $ \varepsilon_0=\min \left\{ \frac{\varepsilon}{16},\frac{\varepsilon\cdot\inf_{\theta\in\mathbb{B}(\tilde{\theta}, \ell)} f_z(\theta,X)}{16(n-z)\xi{\ell}} \right\} $. We have $ f_z(\theta,X)=f(\theta,X_\mathrm{\Rnum{1}}+X_\mathrm{\Rnum{2}}) $ and $ f_z(\theta,C)=f(\theta,C_\mathrm{\Rnum{1}}+C_\mathrm{\Rnum{2}}) $. So our aim is to prove
\[f(\theta,C_\mathrm{\Rnum{1}}+C_\mathrm{\Rnum{2}})=f(\theta,C_\mathrm{\Rnum{1}})+f(\theta,C_\mathrm{\Rnum{2}})\approx f(\theta,X_\mathrm{\Rnum{1}}+X_\mathrm{\Rnum{2}}).\]
%We have to bound $ f(\theta,\mathrm{\Rnum{1}^C+\Rnum{2}^C}) $ in two directions, but we only concentrate on the $ \leq $ case here and the proof of another direction is similar.
%We will bound $ f(\theta,C_\mathrm{\Rnum{1}}) $ and $ f(\theta,C_\mathrm{\Rnum{2}}) $ repectively first and then bound their sum. %The key idea is that: Outliers having very large cost are in region $ \mathrm{\Rnum{3}^X} $ and they will be properly detected and processed in $ \mathrm{\Rnum{2}^C+\Rnum{3}^C} $. Outliers in region $ \mathrm{\Rnum{4}^X} $ may be mistaken as inliers but the influence of the mistake is relatively manageable.
%We present the following claim.
\begin{claim}
	\label{claim-divide}
	We have the following results for the partition yielded by $ \theta $ and $ \tilde{\theta} $.
	\begin{enumerate}
		\item 
		%These are from the definition of $ X_\mathrm{\Rnum{1}},X_\mathrm{\Rnum{2}},X_\mathrm{\Rnum{3}},X_\mathrm{\Rnum{4}}$ and $ C_\mathrm{\Rnum{1}},C_\mathrm{\Rnum{2}},C_\mathrm{\Rnum{3}},C_\mathrm{\Rnum{4}} $.
		\begin{align*}
			\left|X_\mathrm{\Rnum{1}}+X_\mathrm{\Rnum{2}}\right|=\left\llbracket C_\mathrm{\Rnum{1}}+C_\mathrm{\Rnum{2}}\right\rrbracket = n-z,\quad
			\left|X_\mathrm{\Rnum{3}}+X_\mathrm{\Rnum{4}}\right|=\left\llbracket C_\mathrm{\Rnum{3}}+C_\mathrm{\Rnum{4}}\right\rrbracket = z,\\
			\left|X_\mathrm{\Rnum{1}}+X_\mathrm{\Rnum{4}}\right|=\left\llbracket C_\mathrm{\Rnum{1}}+C_\mathrm{\Rnum{4}} \right\rrbracket = n-\tilde{z},\quad
			\left|X_\mathrm{\Rnum{2}}+X_\mathrm{\Rnum{3}}\right|=\left\llbracket C_\mathrm{\Rnum{2}}+C_\mathrm{\Rnum{4}}\right\rrbracket = \tilde{z}.
		\end{align*}
		\item 
		%These are from the construction method immediately.
		\begin{align*}
			C_\mathrm{\Rnum{1}}+C_\mathrm{\Rnum{4}}\subseteq X_\mathrm{\Rnum{1}}+X_\mathrm{\Rnum{4}},\quad
			C_\mathrm{\Rnum{2}}+C_\mathrm{\Rnum{3}}\subseteq X_\mathrm{\Rnum{2}}+X_\mathrm{\Rnum{3}}
		\end{align*}
		\item we use $ x_{\mathrm{\Rnum{1}}} $ to denote any element in set $ \mathrm{\Rnum{1}} $ ($ \mathrm{\Rnum{1}} $ can be displaced by $ X_{\mathrm{\Rnum{1}}} $ or $ C_{\mathrm{\Rnum{1}}} $, and the meanings of other notations are similar.), then we have \begin{align*}
			f(\theta,x_{\mathrm{\Rnum{1}+\Rnum{2}}})\leq f(\theta,x_{\mathrm{\Rnum{3}+\Rnum{4}}}) ,\quad
			f(\tilde{\theta},x_{\mathrm{\Rnum{1}+\Rnum{4}}})\leq f(\tilde{\theta},x_{\mathrm{\Rnum{2}+\Rnum{3}}}).
		\end{align*}
		\item 
		%Because of the definition of $ \tau $ and the continuity of $ f(\theta,x) $, we have
		\begin{align*}
			f(\tilde{\theta},x_{\mathrm{\Rnum{1}}})\leq \tau,\  f(\theta,x_{\mathrm{\Rnum{1}}})\leq \tau+\xi(\ell),  \\
			f(\tilde{\theta},x_{\mathrm{\Rnum{3}}})\geq \tau,\  f(\theta,x_{\mathrm{\Rnum{3}}})\geq \tau-\xi(\ell), \\
			f(\tilde{\theta},x_{\mathrm{\Rnum{2}}})\geq \tau,\ f(\theta,x_{\mathrm{\Rnum{2}}})\geq \tau-\xi(\ell).
		\end{align*}
		\item If set $ \mathrm{\Rnum{4}} $ is not empty, then
		\begin{align*}
			\tau - 2\xi(\ell) \leq f(\tilde{\theta},x_{\mathrm{\Rnum{4}}})\leq \tau,\quad
			\tau -\xi(\ell)\leq f(\theta,x_{\mathrm{\Rnum{4}}})\leq \tau+\xi(\ell),\\
			\tau \leq f(\tilde{\theta},x_{\mathrm{\Rnum{2}}})\leq \tau+2\xi(\ell),\quad
			\tau -\xi(\ell) \leq f(\theta,x_{\mathrm{\Rnum{2}}})\leq \tau+\xi(\ell).
		\end{align*}
	\end{enumerate}
	
\end{claim}

\begin{proof}
	The proofs of the item $ 1$ to $ 4 $ are straightforward. As for the item 5, if set $ \mathrm{\Rnum{4}} $ is not empty, through combining the results of the item 3 and 4, we can attain the lower and upper bounds of $ f(\theta,x_{\mathrm{\Rnum{4}}}) $, 
	\begin{align*}
		f(\theta,x_{\mathrm{\Rnum{4}}})\geq f(\theta,x_{\mathrm{\Rnum{2}}})\geq \tau-\xi(\ell),\\
		f(\theta,x_{\mathrm{\Rnum{4}}})\leq f(\tilde{\theta},x_{\mathrm{\Rnum{4}}})+\xi(\ell)\leq \tau+\xi(\ell),
	\end{align*}
	and those of $ f(\tilde{\theta},x_{\mathrm{\Rnum{4}}}) $,
	\begin{align*}
	    f(\tilde{\theta},x_{\mathrm{\Rnum{4}}})\leq f(\tilde{\theta},x_{\mathrm{\Rnum{2}}})\leq \tau, \\
	    f(\tilde{\theta},x_{\mathrm{\Rnum{4}}})\geq 	f(\theta,x_{\mathrm{\Rnum{4}}})-\xi(\ell)\geq \tau-2\xi(\ell). 
	\end{align*}
	Similarly, we have
	\begin{align*}
	f(\tilde{\theta},x_{\mathrm{\Rnum{2}}})\leq f(\theta,x_{\mathrm{\Rnum{2}}})+\xi(\ell) \leq \tau+2\xi(\ell),\\
		f(\theta,x_{\mathrm{\Rnum{2}}})\leq f(\theta,x_{\mathrm{\Rnum{4}}})\leq \tau+\xi(\ell),\\
		f(\theta,x_{\mathrm{\Rnum{2}}})\geq f(\tilde{\theta},x_{\mathrm{\Rnum{2}}})-\xi(\ell)\geq \tau-\xi(\ell).
	\end{align*}
\end{proof}

Claim~\ref{claim-divide} will be used in the proofs of the following key lemmas before proving theorem \ref{thm-coreset-outlier}.

\begin{figure}
	\centering
	\includegraphics[width=0.4\linewidth]{fig/arrange-supp}
	\caption{Illustration of Lemma \ref{lemma-l}. $\theta$ and $ \tilde{\theta} $ divide $X$ into different inliers and outliers. And we have that $ f_z(\theta,X)= f(\theta,X_\mathrm{\Rnum{1}}+X_\mathrm{\Rnum{2}}) $ and $ f_z(\tilde{\theta},X)= f(\tilde{\theta},X_\mathrm{\Rnum{1}}+X_\mathrm{\Rnum{4}}) $. }
	\label{fig:region-2}
\end{figure}

\begin{lemma}\label{lemma-l}
%	We have 
%	\begin{equation}
		$\left| f_z(\theta,X)-f_z(\tilde{\theta},X) \right|\leq (n-z)\xi(\ell).$
%	\end{equation}
\end{lemma}


\begin{proof}
	Note that $ f_z(\theta,X)= f(\theta,X_\mathrm{\Rnum{1}}+X_\mathrm{\Rnum{2}}) $ and $ f_z(\tilde{\theta},X)= f(\tilde{\theta},X_\mathrm{\Rnum{1}}+X_\mathrm{\Rnum{4}})$ (see Figure~\ref{fig:region-2}). 
	Because $ |X_\mathrm{\Rnum{2}}+X_\mathrm{\Rnum{3}}|=|X_\mathrm{\Rnum{3}}+X_\mathrm{\Rnum{4}}|=z $, we have $ |X_\mathrm{\Rnum{2}}|=|X_\mathrm{\Rnum{4}}|\leq z $. 
	Then we have 
	\begin{align*}
		f_z(\theta,X)&=f(\theta,X_\mathrm{\Rnum{1}}+X_\mathrm{\Rnum{2}})\\ &\geq f(\tilde{\theta},X_\mathrm{\Rnum{1}}+X_\mathrm{\Rnum{2}})-\xi(\ell)(n-z)\\ &=f(\tilde{\theta},X_\mathrm{\Rnum{1}})+f(\tilde{\theta},X_\mathrm{\Rnum{2}})-f(\tilde{\theta},X_\mathrm{\Rnum{4}})+f(\tilde{\theta},X_\mathrm{\Rnum{4}})-\xi(\ell)(n-z)\\
		&\geq f(\tilde{\theta},X_\mathrm{\Rnum{1}}+X_\mathrm{\Rnum{4}})-\xi(\ell)(n-z)+\underbrace{\left( f(\tilde{\theta},X_\mathrm{\Rnum{2}})-f(\tilde{\theta},X_\mathrm{\Rnum{4}}) \right)}_{\geq 0} \\
		&\geq f_z(\tilde{\theta},X)-\xi(\ell)(n-z).
	\end{align*}
	Similarly, we have $ f_z(\theta,X)\leq f_z(\tilde{\theta},X)+\xi(\ell)(n-z) $. Therefore we have 	$\left| f_z(\theta,X)-f_z(\tilde{\theta},X) \right|\leq (n-z)\xi(\ell).$
\end{proof}


\begin{lemma}\label{lemma-4}
	\begin{equation}\label{C_II-Bound}
		f(\theta,C_\mathrm{\Rnum{2}})\leq 
		\begin{cases}
			f_{(1-\beta)z}(\theta,X_\mathrm{\Rnum{2}}+X_\mathrm{\Rnum{3}}) \quad& \text{if } C_\mathrm{\Rnum{4}}=\varnothing \\
			f(\theta,X_\mathrm{\Rnum{2}})+2z(\tau+\xi(\ell)) \quad& \text{if } C_\mathrm{\Rnum{4}}\ne\varnothing
		\end{cases}
	\end{equation}
\end{lemma}
\begin{proof}
It is easy to show that
$ \tilde{z}-z \leq \left| X_\mathrm{\Rnum{2}} \right|\leq \tilde{z} $ and $ \tilde{z}-z \leq \left\llbracket C_\mathrm{\Rnum{2}} \right\rrbracket \leq \tilde{z} $.
Then it implies that
\begin{equation}
	\Big| \llbracket C_\mathrm{\Rnum{2}}\rrbracket -|X_\mathrm{\Rnum{2}}| \Big| \leq z.
\end{equation}
If $ \iota $ is the the proportion of inliers of a dataset $ A $ then we define $ f_{[\iota]}(\theta,A):= f_{(1-\iota)n}(\theta,A) $.
Let the proportion of inliers of $ X_\mathrm{\Rnum{2}}+X_\mathrm{\Rnum{3}} $ and $ C_\mathrm{\Rnum{2}}+C_\mathrm{\Rnum{3}} $ be $ \gamma $ and $ \gamma' $ respectively. Then we have
\begin{align}
	f(\theta,X_\mathrm{\Rnum{2}})&= f_{[\gamma]}(\theta,X_\mathrm{\Rnum{2}}+X_\mathrm{\Rnum{3}}),\\
	f(\theta,C_\mathrm{\Rnum{2}})&=f_{[\gamma']}(\theta,C_\mathrm{\Rnum{2}}+C_\mathrm{\Rnum{3}}).
\end{align}
Note that $ \gamma,\gamma'\geq \frac{1}{1+\varepsilon_0} $ and thus $ |\gamma-\gamma'|\leq \frac{\varepsilon_0}{1+\varepsilon_0} $.


If $ C_\mathrm{\Rnum{4}}=\varnothing $, i.e., $ \gamma'=\frac{1}{1+\varepsilon_0} $, we have 
\begin{align} 	
	f(\theta,C_\mathrm{\Rnum{2}})&=f_{[\gamma']}(\theta,C_\mathrm{\Rnum{2}}+C_\mathrm{\Rnum{3}})\\
	&\leq f_{[\gamma'+\delta]}(\theta,X_\mathrm{\Rnum{2}}+X_\mathrm{\Rnum{3}})\\
	&=f_{(1-\beta)z}(\theta,X_\mathrm{\Rnum{2}}+X_\mathrm{\Rnum{3}}).
	\label{II-1}
\end{align}
Otherwise $ C_\mathrm{\Rnum{4}}\ne\varnothing $, we have $\tau-\xi(\ell)\leq x_{C_\mathrm{\Rnum{2}}}\leq \tau+\xi(\ell) $. Moreover, if $ \gamma'\leq \gamma $, then
\begin{align}
	f_{[\gamma']}(\theta,C_\mathrm{\Rnum{2}}+C_\mathrm{\Rnum{3}})&\leq f_{[\gamma'-\delta]}(\theta,C_\mathrm{\Rnum{2}}+C_\mathrm{\Rnum{3}})+z(\tau+\xi(\ell))\\
	&\leq f_{[\gamma']}(\theta,X_\mathrm{\Rnum{2}}+X_\mathrm{\Rnum{3}})+z(\tau+\xi(\ell))\\
	&\leq f_{[\gamma]}(\theta,X_\mathrm{\Rnum{2}}+X_\mathrm{\Rnum{3}})+z(\tau+\xi(\ell)).\label{C_iv-ne-empty-1}
\end{align}
If $ \gamma' > \gamma $, since $ \gamma'-\gamma\leq \frac{\varepsilon_0}{1+\varepsilon_0} $, we have
\begin{align}
	f_{[\gamma']}(\theta,C_\mathrm{\Rnum{2}}+C_\mathrm{\Rnum{3}})&\leq f_{[\gamma-\delta]}(\theta,C_\mathrm{\Rnum{2}}+C_\mathrm{\Rnum{3}})+2z(\tau+\xi(\ell))\\
	&\leq f_{[\gamma]}(\theta,X_\mathrm{\Rnum{2}}+X_\mathrm{\Rnum{3}})+2z(\tau+\xi(\ell)).\label{C_iv-ne-empty-2}
\end{align}
Combining (\ref{C_iv-ne-empty-1}) and (\ref{C_iv-ne-empty-2}), if $ C_\mathrm{\Rnum{4}}\ne\varnothing $, we have
\begin{equation}
	f(\theta,C_\mathrm{\Rnum{2}})\leq f_{[\gamma]}(\theta,X_\mathrm{\Rnum{2}}+X_\mathrm{\Rnum{3}})+2z(\tau+\xi(\ell))
	\label{II-2}
\end{equation}
	
\end{proof}

%If $ \gamma'\leq\gamma $, we have the following by applying Lemma 7.
%\begin{align*}
%	f_{[\gamma']}(\theta,\mathrm{\Rnum{2}^{C}+\Rnum{3}^{C}})&\leq f_{[\gamma'+\delta]}(\theta,\mathrm{\Rnum{2}^{X}+\Rnum{3}^{X}})\\
%	&\leq f_{[\gamma+\delta]}(\theta,\mathrm{\Rnum{2}^{X}+\Rnum{3}^{X}})\\
%	&=f_{(1-\beta)z}(\theta,\mathrm{\Rnum{2}^{X}+\Rnum{3}^{X}})
%\end{align*}
%Otherwise, $ \gamma'>\gamma $, we claim that $ \mathrm{\Rnum{4}^{C}}\ne \varnothing $ in this case. Because if $ \mathrm{\Rnum{4}^{C}} = \varnothing $, then $ \gamma'=\frac{1}{1+\varepsilon}\leq \gamma $, which makes a contradiction. We have $ x_{\mathrm{\Rnum{2}^{C}}}\leq \tau+\alpha l $ from $ \mathrm{\Rnum{4}^{C}}\ne \varnothing $. 
%
%There are at most $ (\gamma'-\gamma+\delta)\tilde{z}\leq (1+\beta)z \leq 2z  $ weight points of $ f_{[\gamma']}(\theta,\mathrm{\Rnum{2}^{C}+\Rnum{3}^{C}}) $ are not counted in $ f_{[\gamma-\delta]}(\theta,\mathrm{\Rnum{2}^{C}+\Rnum{3}^{C}}) $. Thus we have
%\begin{align*}
%	f_{[\gamma']}(\theta,\mathrm{\Rnum{2}^{C}+\Rnum{3}^{C}})&\leq 	f_{[\gamma-\delta]}(\theta,\mathrm{\Rnum{2}^{C}+\Rnum{3}^{C}})\\
%	&\qquad+2z(\tau+\alpha l)\\
%	&\leq f_{[\gamma]}(\theta,\mathrm{\Rnum{2}^{X}+\Rnum{3}^{X}})\\
%	&\qquad+2z(\tau+\alpha l)
%\end{align*}
%Finally we conclude that
%\begin{equation}\label{II}
%	f(\theta,\mathrm{\Rnum{2}^C})\leq f_{(1-\beta)z}(\theta,\mathrm{\Rnum{2}^{X}+\Rnum{3}^{X}})+2z(\tau+\alpha l)
%\end{equation}



\begin{lemma}
	\label{lemma-5}
	$f(\theta,X_\mathrm{\Rnum{1}}+X_\mathrm{\Rnum{4}})+ f_{(1-\beta)z}(\theta,X_\mathrm{\Rnum{2}}+X_\mathrm{\Rnum{3}})
		\leq f_{(1-\beta)z}(\theta,X)+2z\xi(\ell)$.
 \end{lemma}
\begin{proof}
	In $ f_{(1-\beta)z}(\theta,X) $, we delete $ (1-\beta)z $ points that denoted by $ O $ from $ X_\mathrm{\Rnum{3}}+X_\mathrm{\Rnum{4}} $.
	If $ O\cap X_\mathrm{\Rnum{4}}=\varnothing $, then we have \[ f(\theta,X_\mathrm{\Rnum{1}}+X_\mathrm{\Rnum{4}})+ f_{(1-\beta)z}(\theta,X_\mathrm{\Rnum{2}}+X_\mathrm{\Rnum{3}})= f_{(1-\beta)z}(\theta,X).\]
	Otherwise, $ O\cap X_\mathrm{\Rnum{4}}\ne \varnothing $, which implies $ X_\mathrm{\Rnum{4}}\ne \varnothing $, then we have $ f(\theta,x_{X_\mathrm{\Rnum{2}}+X_\mathrm{\Rnum{3}}})\geq \tau-\xi(\ell) $ and $ f(\theta,x_{X_\mathrm{\Rnum{4}}})\leq \tau+\xi(\ell) $. Then 
	\begin{align*}
	    f(\theta,X_\mathrm{\Rnum{1}}+X_\mathrm{\Rnum{4}})&+ f_{(1-\beta)z}(\theta,X_\mathrm{\Rnum{2}}+X_\mathrm{\Rnum{3}})\leq f(\theta,X)-(\tau-\xi(\ell))(1-\beta)z,\\
	    f(\theta,X)&\leq f_{(1-\beta)z}(\theta,X)+(\tau+\xi(\ell))(1-\beta)z.
	\end{align*}
	Combining the above two inequalities finally we conclude 
	\[ f(\theta,X_\mathrm{\Rnum{1}}+X_\mathrm{\Rnum{4}})+ f_{(1-\beta)z}(\theta,X_\mathrm{\Rnum{2}}+X_\mathrm{\Rnum{3}})\leq f_{(1-\beta)z}(\theta,X)+2z\xi(\ell). \]
\end{proof}

\begin{proof}[\textbf{Proof of Theorem \ref{thm-coreset-outlier}}]
It is easy to obtain the coreset size. So we only focus on proving the quality guarantee below. 

Our goal is to show that $ f_{z}(\theta,C)$ is a qualified approximation of $ f_{z}(\theta,X)$ (as Defintion~\ref{robust-coreset}).  
Note that $ f_{z}(\theta,C)=f(\theta,C_\mathrm{\Rnum{1}}+ C_\mathrm{\Rnum{2}})=f(\theta,C_\mathrm{\Rnum{1}})+f(\theta,C_\mathrm{\Rnum{2}}) $. Hence we can bound $ f(\theta,C_\mathrm{\Rnum{1}}) $ and $ f(\theta,C_\mathrm{\Rnum{2}}) $ separately. We consider their upper bounds first, and the lower bounds can be derived by using the similar manner. 


The upper bound of $ f(\theta,C_\mathrm{\Rnum{1}}) $ directly comes from the definition of $\varepsilon$-coreset, i.e., $ f(\theta,C_\mathrm{\Rnum{1}})\leq f(\theta,C_\mathrm{\Rnum{1}}+C_\mathrm{\Rnum{4}})\leq (1+\varepsilon_1) f(\theta,X_\mathrm{\Rnum{1}}+X_\mathrm{\Rnum{4}}) $ since $ C_\mathrm{\Rnum{1}}+C_\mathrm{\Rnum{4}} $ is an $ \varepsilon_1 $-coreset of $ X_\mathrm{\Rnum{1}}+X_\mathrm{\Rnum{4}} $.

Together with Lemma~\ref{lemma-4} and Lemma~\ref{lemma-5}, we have an upper bound for $ f_z(\theta,C) $: 
\begin{equation}\label{f_z-Bound}
	f_z(\theta,C)\leq
	\begin{cases}
		(1+\varepsilon_1)f_{(1-\beta)z}(\theta,X)+4z\xi(\ell) \quad& \text{if } C_\mathrm{\Rnum{4}}=\varnothing \\
		(1+\varepsilon_1)f_z(\theta,X)+4z\tau+4z\xi(\ell) \quad& \text{if } C_\mathrm{\Rnum{4}}\ne\varnothing
	\end{cases}
\end{equation}
By taking the upper bound of these two bounds, we have 
\begin{equation}\label{f_z-Bound-2}
	f_z(\theta,C)\leq (1+\varepsilon_1)f_{(1-\beta)z}(\theta,X)+4z\tau+4z\xi(\ell).
\end{equation}
Also, we have $ z\tau\leq \varepsilon_0 f_z(\theta,X)+\varepsilon_0(n-z)\xi(\ell) $ and $ z\xi(\ell)\leq \varepsilon_0(n-z)\xi(\ell) $ due to lemma~\ref{lemma-l}. Through plugging these two bounds into (\ref{f_z-Bound-2}), we have
\begin{equation}
	f_z(\theta,C)\leq (1+4\varepsilon_0+\varepsilon_1)f_{(1-\beta)z}(\theta,X)+8\varepsilon_0\xi(\ell)(n-z).
\end{equation}
Since $ \varepsilon_1=\varepsilon/4 $ and $ \varepsilon_0=\min \left\{ \frac{\varepsilon}{16},\frac{\varepsilon\cdot\inf_{\theta\in\mathbb{B}(\tilde{\theta}, \ell)} f_z(\theta,X)}{16(n-z)\xi{\ell}} \right\} $, we have
\begin{equation}\label{upper-result}
    f_z(\theta,C)\leq (1+\varepsilon)f_{(1-\beta)z}(\theta,X).
\end{equation}
Similarly, we can obtain the lower bound
\begin{equation}\label{lower-result}
     f_{z}(\theta,C) \geq (1-\varepsilon) f_{(1+\beta)z}(\theta,X).
\end{equation}
Overall, from (\ref{upper-result}) and (\ref{lower-result}), we know that $ C $ is a $(\beta,\varepsilon)$-robust coreset of $ X $.
\end{proof}

\subsection{Proof of Theorem \ref{the-coreset-1}}\label{sec:proof-thm-4}
As for the importance sampling framework, we have the following lemma by using the Hoeffding's inequality~\cite{bachem2017practical}.
\begin{lemma}
\label{lem-proof-thm-4-1}
	Let $\theta$ be a fixed parameter vector. We sample $ m $ points from $ X $, denoted by $ C $, with the importance sampling framework. If $ m\geq \frac{S^2}{2\varepsilon^2}\log\frac{2}{\eta} $, then $ |{f}(\theta,C)-{f}(\theta,X)|\leq \varepsilon {f}(\theta,X) $ holds with probability at least $ 1-\eta $.
\end{lemma}
Further,   we need to prove that $ |{f}(\theta,C)-{f}(\theta,X)|\leq \varepsilon {f}(\theta,X) $ holds for all $ \theta\in\mathbb{B}(\tilde{\theta},\ell) $. Let $ \mathbb{B}^{\varepsilon \ell} $ be an $ \varepsilon \ell $-net of $ \mathbb{B}(\tilde{\theta},\ell) $; so for any $ \theta\in\mathbb{B}(\tilde{\theta},\ell) $, there exists a $ \theta'\in\mathbb{B}^{\varepsilon \ell} $ such that $ \|\theta-\theta' \|\leq \varepsilon \ell $. To guarantee  $ |{f}(\theta',C)-{f}(\theta',X)|\leq \varepsilon {f}(\theta',X) $  for any $ \theta' $s in $ \mathbb{B}^{\varepsilon \ell} $, we can sample $ \frac{S^2}{2\varepsilon^2}\log\frac{2|\mathbb{B}^{\varepsilon \ell}|}{\eta} $ points instead of $\frac{S^2}{2\varepsilon^2}\log\frac{2}{\eta}$ as shown in Lemma~\ref{lem-proof-thm-4-1} (just take the union bound over all the points of $\mathbb{B}^{\varepsilon l}$). 
Also we can obtain the size $ |\mathbb{B}^{\varepsilon l}| $ by using the doubling dimension of the parameter space. Let $ \mathcal{M} $ be a metric space, and we say it has the \emph{doubling dimension} $ \mathtt{ddim} $ if $ \mathtt{ddim} $ is the smallest value satisfying that any ball in $ \mathcal{M} $ can be always covered by at most $ 2^{\mathtt{ddim}} $ balls of half the radius.
%It is not difficult to derive that a ball of radius $ r $ can be covered by at most $ (\frac{r}{\varepsilon})^{\mathtt{ddim}} $ balls of radius $ \varepsilon $, and, obviously, all centers of these balls constitute an $ \varepsilon $-net of the original ball. 
So we have  
%\begin{lemma}\label{ddim}
%	If $ B $ is a ball with radius $ r $, in the metric space of doubling dimension $ \mathtt{ddim} $, then it have an $ \varepsilon $-net of size $ (\frac{r}{\varepsilon})^{\mathtt{ddim}}$. 
%\end{lemma}
$ |\mathbb{B}^{\varepsilon l}|=\left( \frac{\ell}{\varepsilon\ell}\right)^{\mathtt{ddim}}=\left( \frac{1}{\varepsilon}\right)^{\mathtt{ddim}} $, where $ \mathtt{ddim} $ is the doubling dimension of the parameter space.

Now the only remaining issue is to prove that $ |{f}(\theta,C)-{f}(\theta,X)|\leq \varepsilon {f}(\theta,X) $ holds for any $ \theta\in\mathbb{B}(\tilde{\theta},\ell)\setminus \mathbb{B}^{\varepsilon l}$. 
By using the triangle inequality,  we have
%To make this, in the framework of importance sampling before, besides $ \varepsilon,S $ and $ \eta $, the size of $ C $ depends on the pseudo-dimension of the objective function~\cite{FeldmanL11} or the VC dimension of the range space induced by the objective function~\cite{BravermanFL16}. In this section, we give a simple result depending on the doubling dimension of the parameter space.
\begin{align*}
	\left|{f}(\theta,X)-{f}(\theta,C)\right|\leq \left|{f}(\theta,X)-{f}(\theta',X) \right|
	+\left|{f}(\theta',X)-{f}(\theta',C) \right|
	+\left|{f}(\theta',C)-{f}(\theta,C) \right|.
\end{align*}
The first and last item are both no more than $ \varepsilon n\xi(\ell) $ since $ \xi(\varepsilon\ell)\leq \varepsilon\xi(\ell) $. The middle term can be upper-bounded by $ \varepsilon ({f}(\theta,X)+n\xi(\ell)) $. Hence $ \left|{f}(\theta,X)-{f}(\theta,C)\right|\leq \varepsilon f(\theta,X)+3\varepsilon n\xi(\ell) $.
If replacing $ \varepsilon $ by $\min\{\varepsilon/2,\varepsilon\inf_{\theta\in\mathbb{B}(\tilde{\theta},\ell)}f(\theta,X)/6n\xi(\ell) \} $, through simple calculations we can obtain an $ \varepsilon $-coreset of size $|C|=\Theta\left(\frac{S^2}{\varepsilon^2}\left(\mathtt{ddim}\log\frac{1}{\varepsilon}+\log\frac{1}{\eta}\right) \right)$ with probability $ 1-\eta $. Since $ \xi(\ell) $ has an implicit parameter $\alpha$, the hidden constant of $|C|$ depends on $\alpha$ and $\inf_{\theta\in\mathbb{B}(\tilde{\theta},\ell)}\frac{f(\theta,X)}{n}$.



\subsection{Proof of Theorem \ref{thm:standard-coreset}}
\label{sec-appthestancoreset}
It is easy to see that the construction time is linear. So we only focus on the quality guarantee below. Recall that for any $ x $ in the $ i $-th layer, we have
\begin{equation}\label{value-range}
	f(\theta,x)\in\begin{cases}
		&[2^{i-1}T-\xi(\ell)+\varrho,2^iT+\xi(\ell)+\varrho] \quad i>0 \\
		&[\varrho,T+\xi(\ell)+\varrho]\quad i=0
	\end{cases}
\end{equation}
\begin{lemma}[\cite{Haussler92}]\label{Haussler92}
	Let $ g(\cdot) $ be a function defined on a set $ V $ and for all $ x\in V $, $ g(x)\in [a,b] $ such that $ b-a\leq h $. Let $ U\subseteq V $ be a set independently and uniformly sampled from $ V $. If $ |U|\geq (h^2/2\varepsilon^2)\ln(\frac{2}{\eta}) $, then $ \Pr\left[\left| \frac{g(V)}{|V|}-\frac{g(U)}{|U|} \right|\geq \varepsilon \right]\leq \eta $. 
\end{lemma}
%For every layer, we use the lemma \ref{Haussler92} and (\ref{value-range}). To make all inequalities hold at the same time, we need to replace $ \eta $ with $ \eta/(L+1) $. 
We apply  (\ref{value-range}) to lemma \ref{Haussler92} and obtain the following result. 
\begin{lemma}
	If we sample a set of $ O\left(\frac{1}{\varepsilon^2}\left(\log\frac{1}{\eta}+\log\log n \right)\right) $ points uniformly at random from $ X_i $, which is denoted by $ C_i $, then for any fixed parameter $ \theta $, we have
	\begin{equation}\label{1layer-1}
		\left| \frac{f(\theta,X_i)}{|X_i|}-\frac{f(\theta,C_i)}{|C_i|} \right|\leq
		\begin{cases}
			\varepsilon(2^{i-1}T+2\xi(\ell)), \ i\geq 1; \\ \varepsilon(T+\xi(\ell)),  \ i=0 
		\end{cases}
	\end{equation}
	with probability $ 1-\eta $.
\end{lemma}
For each $i$-th layer, we set the weight of each sampled point  to be $|X_i|/|C_i|$. Let $ C $ be the union of $ C_i $s and we can prove the following lemma.

%The total running time is $ O(n) $. Hence we have the following result.
\begin{lemma}\label{lemma-layer}
	Let $\theta$ be a fixed parameter vector. We can compute a weighted subset $ C\subseteq X $ of size $ \Theta\left(\frac{\log n}{\varepsilon^2}\left(\log\frac{1}{\eta}+\log\log n \right) \right) $ such that 
	\begin{equation}\label{fixed-parameter}
		|{f}(\theta,C)-{f}(\theta,X)|\leq 4\varepsilon({f}(\theta,X)+n\xi(\ell))
	\end{equation}
	holds  with probability $ 1-\eta $.
\end{lemma}
\begin{proof}
	Based on the construction of $ C $, we have
	\begin{align*}
		|f(\theta&,C)-f(\theta,X)|\leq 
		\sum_{i=0}^L|f(\theta,C_i)-f(\theta,X_i)| \\
		&=|f(\theta,C_0)-f(\theta,X_0)|+\sum_{i=1}^L|f(\theta,C_i)-f(\theta,X_i)| \\
		&\leq  \varepsilon |X_0|(T+\xi(\ell)) +\sum_{i=1}^L\varepsilon|X_i|(2^{i-1}T+2\xi(\ell))\\
		&\leq \sum_{i=0}^L\varepsilon|X_i|2\xi(\ell)+ \varepsilon |X_0|T+\sum_{i=1}^L\varepsilon|X_i|(2^{i-1}T).	
	\end{align*}	
	Due to the definition of $ T $, we have $ |X_i|2^{i-1}T \leq f(\tilde{\theta},X_i) $ for $ i\geq 1 $, then 
		\begin{align*}
		|{f}(\theta,C)-{f}(\theta,X)|& \leq \varepsilon\cdot 2n\xi(\ell)+\varepsilon {f}(\tilde{\theta},X)+\varepsilon {f}(\tilde{\theta},X)\\
		&=2\varepsilon(n\xi(\ell)+{f}(\tilde{\theta},X))\\
		&\leq 2\varepsilon(2n\xi(\ell)+{f}(\theta,X))\\
		&\leq 4\varepsilon(n\xi(\ell)+{f}(\theta,X)).
		\end{align*}
	The second inequality holds because $ |f(\tilde{\theta},x)-f(\theta,x)|\leq \xi(\ell) $ holds for every $ x $.
\end{proof}


Then we can apply the similar idea of Section~\ref{sec:proof-thm-4} to achieve a union bound over $ \mathbb{B}(\tilde{\theta},\ell) $. Finally, we obtain the coreset size 
$ \Theta\left(\frac{\log n}{\varepsilon^2}\left(\mathtt{ddim}\log\frac{1}{\varepsilon}+\log\frac{1}{\eta}\right) \right)$\footnote{We omit the $ \log\log n $ item.}.
