%===============================================================================
% ifacconf.tex 2022-02-11 jpuente  
% Template for IFAC meeting papers
% Copyright (c) 2022 International Federation of Automatic Control
%===============================================================================
\documentclass{ifacconf}

\usepackage{graphicx}      % include this line if your document contains figures
\usepackage{epstopdf}
\usepackage{natbib}        % required for bibliography
\usepackage{soul, color}
\usepackage{amsmath}
\usepackage{subfigure}
\usepackage{amsfonts} 
%\usepackage{stfloats}
%\usepackage[ruled,vlined]{algorithm2e}
\usepackage{algpseudocode}
\usepackage[ruled,vlined,linesnumbered]{algorithm2e}

\usepackage{pgf}
\usepackage{url}
\usepackage{tikz}
\usetikzlibrary{positioning, arrows.meta,automata}
\usetikzlibrary{calc}


\newtheorem{theorem}{Theorem}
\newtheorem{lemma}{Lemma}
\newtheorem{assumption}{Assumption}
\newtheorem{exam}{Example}
\newtheorem{corollary}{Corollary}
\newtheorem{definition}{Definition}
\newtheorem{proposition}{Proposition}
\newtheorem{problem}{Problem}
\newtheorem{remark}{Remark} 

\def\XY#1{{\textcolor{red}{ {\bf XY:} #1}}} 
\def\ZJN#1{{\textcolor{blue}{#1}}} 

\pagestyle{plain} 


\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

\def \AP{\mathcal{AP}}
\def \A{\mathcal{A}}
\def \I{\mathcal{I}}
\def \L{\mathcal{L}}
\def \K{\mathcal{K}}
\def \<{\langle}
\def \>{\rangle}
\def \P{\mathcal{P}}
\def \NN{\mathbb{N}}
\def \RR{\mathbb{R}}
\def \k{\emph{\text{k}}}
\def \pref{\emph{\text{pref}}}
\def \stop{\emph{\text{stop}}}
\def \stra{\textsf{Stra}}
\def \last{\textsf{last}}
\def \reg{\text{reg}}
\def \kw{\textsf{Kw}}
\def \hist{\textsf{Hist}}
\def \lib{\textsf{Lib}}
\def \WTS{\textsf{WTS}}
\def \proj{\textsf{proj}}
\def \uk{\emph{\text{uk}}}
\def \uke{\emph{\text{uke}}}
\def \uka{\emph{\text{uka}}}
\def \succ{\text{Succ}}
\def \trace{\textsf{Trace}}
\def \path{\textsf{Path}}
\def \cost{\textsf{cost}}
\def \play{\textsf{Play}}
\def \S{\mathfrak{S}}
\def \T{\mathbb{T}}
\def \cVal{\text{cVal}}
\def \aVal{\text{aVal}}
\def \Reg{\textsf{Reg}}
\def \Range{\text{Range}}
\definecolor{region4}{RGB}{238 238 224}
\definecolor{region1}{RGB}{193 255 193}
\definecolor{region3}{RGB}{255 255 224}
\definecolor{region0}{RGB}{245 245 245}
\definecolor{region2}{RGB}{240 248 255}
\definecolor{region5}{RGB}{255 240 245}
%===============================================================================
\begin{document}
\begin{frontmatter}

\title{\large To Explore or Not to Explore:  Regret-Based LTL Planning    in Partially-Known Environments\thanksref{footnoteinfo}} 
% Title, preferably not more than 10 words.

\thanks[footnoteinfo]{This work was supported by the National Natural Science Foundation of China (62061136004, 62173226, 61803259) and by the National Key Research and Development Program of China (2018AAA0101700).}

\author[First]{Jianing Zhao} 
\author[First]{Keyi Zhu}
\author[First]{Shaoyuan Li}
\author[First]{Xiang Yin}

\address[First]{Department of Automation, Shanghai Jiao Tong University\\
   Shanghai 200240, China\\ (e-mail: \{jnzhao,bai12wp,syli,yinxiang\}@sjtu.edu.cn)}
\begin{abstract}                % Abstract of not more than 250 words.
In this paper, we investigate the optimal robot path planning problem for high-level specifications described by co-safe linear temporal logic (LTL) formulae.  We consider the scenario where the map geometry of the workspace is \emph{partially-known}. Specifically, we assume that there are some unknown regions, for which the robot does not know their successor regions \emph{a priori} unless it reaches these regions physically. In contrast to the standard game-based approach that optimizes the worst-case cost, in the paper, we propose to use \emph{regret} as a new metric for  planning  in such a partially-known environment. The regret of a plan under a fixed but unknown environment is the difference between the actual cost incurred and the best-response cost the robot could have achieved if it realizes the actual environment with hindsight. We provide an effective algorithm for finding an optimal plan that satisfies the LTL specification while minimizing its regret. A case study on  firefighting  robots is provided to illustrate the proposed framework. We argue that the new metric is more suitable for the scenario of partially-known environment since it captures the trade-off between the actual cost spent and the potential benefit one may obtain for exploring an unknown region. 
\end{abstract}

\begin{keyword}
Discrete event systems, regret, autonomous robots, LTL planning.
\end{keyword}

\end{frontmatter}
%===============================================================================

\section{Introduction}


Path planning is one of the central problems in autonomous robots. In this context, one needs to design a finite or infinite path for the robot, according to its dynamic and the underlying environment, such that some desired requirements can be fulfilled. In many robotics applications such as search and rescue, persistent surveillance or warehouse delivery, the planning tasks are usually complicated evolving spatial and/or temporal constraints. Therefore,  in the past years,   robot path planning for \emph{high-level specifications} using formal logics has been drawing increasingly more attentions in the literature; see, e.g., \cite{kress2018synthesis,mahulea2020path,kloetzer2020path,yu2022security}. 

Linear temporal logic (LTL) is one of the most popular languages for describing high-level specifications, which supports temporal operators such as ``always", ``eventually" or ``next". In the context of robotic applications, path planning and decision-making for LTL specifications have been investigated very extensively recently. 
For example, \cite{smith2011optimal} studied how to generate an optimal open-loop plan, in the so-called ``prefix-suffix" structure, such that a given LTL formula is fulfilled.
When the results of control actions are non-deterministic, algorithms for synthesizing reactive strategies have been developed using two-player games \cite{fu2016synthesis}. The LTL path planning problem has also been studied for stochastic systems \cite{guo2018probabilistic} to provide probabilistic guarantees and  for multi-robot systems \cite{yu2022distributed} under both global and local tasks.  

The aforementioned works on LTL path planning all assume that the environment is known in the sense that the map geometry and the semantic structure are both available at the planning stage. In practice, however, the environment may be partially-known such that the robot needs to explore the map geometry as well as the region semantics on-the-fly. To this end, in \cite{guo2015multi}, the authors provided a re-planning algorithm based on the system model updated online.  In  \cite{lahijanian2016iterative}, the authors proposed an iterative planning algorithm in uncertain environments where unknown obstacles may appear. A learning-based algorithm is proposed in \cite{cai2021learning} for LTL planning in stochastic environments with unknown transition probabilities. Recently, \cite{kantaros2022perception} investigated the LTL planning problem under environments with known map geometries  but with semantic uncertainties.   
  
  

\newcommand\kk{1.2}
\begin{figure}
\centering
\input{figure1}
	\caption{A motivating example, where a robot needs to reach region $5$ from regin $0$ with partially-known environment information.}
	\label{fig:motiexam}
\end{figure}   


In this paper, we also investigate the LTL path planning for robots in \emph{partially-known} environments. Specifically, here we assume that the location of each region in the map is perfectly known but, for some regions, the robot does not know their successor regions \emph{a priori} unless it reaches these regions physically. For example, in Figure~1, the dashed line between regions $2$ and $5$ denotes a possible wall that may prevent the robot from reaching region $5$ directly from region $2$. Initially, the robot knows the possibility of the wall, but it will actually know the (non-)existence of the wall only when reaching region $2$.  Here, we distinguish between the terminologies of \emph{non-determinsitic} environments and \emph{partially-known} environments. Specifically, the former is referred to the scenario where the outcome of the environment is purely random in the sense that even for the same visit, the environment may behave differently. However, the partially-known environment is referred to the case where the robot has \emph{information uncertainty} regarding the true world initially, but the underlying actual environment is still fixed and deterministic. 

To solve the path planning problem in partially-known environments, a direct approach is to follow the same idea for planning in non-deterministic environments, where game-based approaches are usually used to minimize the \emph{worst-case cost}. Still, let us consider Figure~1, where the robot aims to reach target region $5$ with shortest distance. 
Using a worst-case-based approach, the robot will follow the red trajectory. This is because the short-cut from regions $2$ to $5$ may not exist; if it goes to region $2$, then in the worst-case, it will spend additional effort to go back. However, by taking the red trajectory, the robot may \emph{heavily regret} by thinking that it should have taken the short-cut at region $2$ if it knows \emph{with hindsight} that the wall does not exist. Therefore, a more natural and human-like plan  is to first go to region $2$ to take a look at whether there is a wall. If not, then it can take the short-cut, which saves 7 units cost.  Otherwise, the robot needs to go back to the red trajectory.  Compared with the red path, although this approach may have two more units cost than the worst-case, it takes the potential huge advantage of exploring the unknown regions. 

In this paper, we formulate and solve a new type of LTL optimal path planning problem for robots working in an aforementioned partially-known environment. We adopt the notion of \emph{regret} from game theory \cite{yannakakis2018artificial} as the optimality metric. We propose the structure of \emph{partially-known weighted transition systems} (PK-WTS) as the model that contains the set of all possible actual environments. The regret of a plan under a fixed but unknown environment is defined as the difference between its actual cost and the best-response cost it could have achieved after it knows the actual environment with hindsight. A value iteration algorithm is developed for computing an optimal  strategy such that (i) it satisfies the LTL requirement under any possible environment; and (ii) minimizes its regret. 
We illustrate by case studies that, compared with the worst-case-based synthesis for non-deterministic environments, the proposed regret-based synthesis is more suitable for partially-known environments. 

The regret minimization problem is an emerging topic in the context of graph games; see, e.g.,  \cite{filiot2010iterated,hunter2017reactive,cadilhac2019impatient}. 
Particularly, \cite{filiot2010iterated} is most related to our problem setting, where it solves a reachability game, via a graph-unfolding algorithm, with minimal regret for a player while the other player plays unrestricted strategies or regret-minimizing strategies as well. 
Furthermore, in the context of robotic applications, the recent work \cite{muvvala2022let} uses regret to optimize  human-robot collaboration strategies, based on the algorithm in \cite{filiot2010iterated}.
However, 
when it refers to the exploration problem, since the actual environment is fixed, two critical issues arise in the two-player graph game: 
i) the environment-player plays a positional strategy rather than the unrestricted manner in \cite{filiot2010iterated, muvvala2022let}; and
ii) to this end, the agent-player needs to ``memorize" the choices of the environment-player, which renders the mechanism of \emph{knowledge update}.
Therefore, it still remains open in the literature of reactive synthesis/graph game to solve a regret-minimizing reachability game against the environment-player playing positional strategies.
In this work, we use regret to capture the issue of exploration in partially-known environments and present an efficient algorithm to solve the regret-minimizing planning problem.



\section{LTL Planning in Fully-Known Environments}\label{sec:plan-standard} 
% In this section, we briefly review some necessary preliminaries and the standard approach for solving the LTL planning problem in a fully-known environment. 
%
\subsection{Weighted Transition Systems}
When the environment of the workspace is fully-known, the mobility of the agent (or map geometry) is usually modeled as a \emph{weighted transition system} (WTS)
\[
T=(X,x_0,\delta_T,w_T,\AP,L), 
\]
where 
$X$ is a set of states representing different regions of the workspace;
$x_0\in X$ is the initial state representing the starting region of the agent; $\delta_T:X\to 2^X$ is the transition function  such that, 
starting from each state $x\in X$, the agent can move directly to any of its successor state $x'\in\delta_T(x)$. We also refer $\delta_T(x)$ to as the \emph{successor states} of $x$; 
$w:X\times X\to\NN$ is a cost function such that 
  $w(x,x')$ represents the cost incurred when the agent moves from $x$ to $x'$;
  $\AP$ is the set of atomic propositions; and
   $L:X\to2^\AP$ is a labeling function assigning each state a set of atomic propositions.
 
Given a WTS $T$, an infinite \emph{path} of $T$ is an infinite sequence of states 
$\rho=x_0x_1x_2\cdots \in X^\omega$ 
such that $x_{i+1}\in\delta_T(x_i), i\geq 0$. 
A finite path is defined analogously. 
%With a slight abuse of notation, we write $x\in\rho$ if  state  $x$ appears in path $\rho$. 
We denote by $\path^\omega(T)$ and $\path^*(T)$ the sets of all infinite paths and finite paths in $T$, respectively.  
Given a finite path $\rho=x_0x_1\cdots x_n\in\path^*(T)$, its cost is defined as the sum of all transition weights in it, which is denoted by 
$\cost(\rho)=\sum_{i=0}^{n-1}w(x_i,x_{i+1})$. 
The \emph{trace} of  an infinite path $\rho=x_0x_1x_2\cdots \in X^\omega$ is an infinite sequence over $2^\AP$ denoted by $L(\rho)=L(x_0)L(x_1)\cdots$. Analogously, we denote by $\trace^\omega(T)$ and $\trace^*(T)$ the sets of all infinite traces and finite traces in $T$, respectively.

\begin{remark}\label{rmk-connected}
Throughout this paper, we assume that $T$ is fixed and connected, that is, for any $x,x'\!\in\! X$, there exists a path fragment $x_1x_2\cdots x_n\!\in\! X^*$ such that $x_1\!=\!x$, $x_{i+1}\!\in\!\delta_T(x_i),i\!=\!1,\ldots,n\!-\!1$ and $x_n\!=\!x'$. Such a setting is common and without loss of generality, (c.f. \cite{macdonald2019active}) when
the agent is able to ``turn around" between any two adjacent regions.
\end{remark}
 
 
 
\subsection{Linear Temporal Logic Specifications}
%
The syntax of general LTL formula is given as follows
\[
\phi=\top\mid a \mid\neg \phi \mid\phi_1\wedge\phi_2 \mid\bigcirc\phi\mid\phi_1 U \phi_2, 
\]
where 
$\top$ stands for the ``true" predicate; 
$a\!\in\!\mathcal{AP}$ is an atomic proposition; 
$\neg$ and $\wedge$ are Boolean operators ``negation" and ``conjunction", respectively; 
$\bigcirc$ and $U$ denote temporal operators ``next" and ``until", respectively. 
One can also derive other temporal operators such as 
``eventually"  by $\lozenge \phi \!=\!\top U \phi$.   
LTL formulae are evaluated over infinite words; the readers are referred to \cite{baier2008principles} for the semantics of LTL.   
Specifically, an infinite word $\tau\!\in\! (2^{\mathcal{AP}})^\omega$ is an infinite sequence over alphabet $2^{\mathcal{AP}}$. 
We write $\tau\!\models\! \phi$ if $\tau$ satisfies LTL formula $\phi$. 

In this paper, we focus on a widely used fragment of LTL formulae called the \emph{co-safe LTL} (scLTL) formulae.   
Specifically, an scLTL formula requires that  the negation operator $\neg$ can only be applied in front of atomic propositions. 
Consequently, one cannot use ``always" $\square$ in scLTL. 
Although the semantics of LTL are defined over infinite words,  it is well-known that any infinite word satisfying a co-safe LTL formula has a \emph{finite good prefix}.   
Specifically,  a good prefix is a finite word $\tau'=\tau_1\cdots \tau_n\in (2^{\mathcal{AP}})^*$ such that $\tau'\tau''\models\phi$ for any $\tau'' \in (2^{\mathcal{AP}})^\omega$.   
We denote by $\L_{\pref}^\phi$ the set of all finite good prefixes of scLTL formula $\phi$.

For any scLTL formula $\phi$, its good prefixes $\L_{\pref}^\phi$ can be accepted by a \emph{deterministic finite automaton} (DFA). Formally, a DFA is a 5-tuple
$\A=(Q,q_0,\Sigma,f,Q_F)$, where
$Q$ is the set of states; $q_0\in Q$ is the initial state; $\Sigma$ is the alphabet;
$f\!:\!Q\times \Sigma\!\to\! Q$ is a transition function; and $Q_F\subseteq Q$ is the set of accepting states. 
The transition function can also be extended  to $f\!:\!Q\times \Sigma^*\!\to\! Q$ recursively. 
A finite word $\tau\in \Sigma^*$ is said to be \emph{accepted} by $\A$  if $f(q_0,\tau)\in Q_F$; we denote by $\L(\A)$ the set of all accepted words. 
Then for any scLTL formula $\phi$ defined over $\AP$,   we can always build a DFA over alphabet $\Sigma=2^{\AP}$, denoted by  $\A_\phi\!=\!(Q,q_0,2^{\AP},f,Q_F)$, such that $\L(\A_\phi)\!=\!\L_\pref^\phi$.    

\subsection{Path Planning for scLTL Specifications}\label{subsec-product}
%
Given a WTS $T$ and an scLTL formula $\phi$, the path planning problem is to find an finite path (a.k.a.\ a plan) $\rho\in \path^*(T)$ such that 
$L(\rho) \in \L_\pref^\phi$ and, at the same time, its cost $\cost(\rho)$ is minimized.  

To solve the scLTL planning problem, the standard approach is to build the \emph{product system} between  WTS $T\!=\!(X,x_0,\delta_T,w,\AP,L)$ and DFA $\A_\phi\!=\!(Q,q_0,\Sigma,f,Q_F)$, which is a new (unlabeled) WTS
\[
P=T\otimes\A_\phi=(S,s_0,\delta_P,w_P,S_F), 
\]
where 
  $S\!=\!X\!\times\! Q$ is the set of states;
  $s_0\!=\!(x_0,q_0)$ is the initial state;
  $\delta_P\!:\!S \!\to\! 2^S$ is the transition function defined by:  
  for any $s\!=\!(x,q)\!\in\! S$,  we have
  $\delta_P(  s )\!=\!\{    (x',q')\!\in\! S \mid  x'\!\in\!\delta_T(x)\wedge q'\!=\! f(q,L(x)) \}$;
  $w_P\!:\! S\!\times\! S\!\to\!\NN$ is the weight function defined by: 
  for any $s\!=\!(x,q),s'\!=\!(x',q')\!\in\! S$, we have $w_P( s,s' )\!=\!w(x,x')$; and
  $S_F\!=\!X\times Q_F$ is the set of accepting states. 
By construction, 
for any path $\rho\!=\!(x_0,q_0)\cdots(x_n,q_n)$ in the product system,  
$(x_n,q_n)\!\in\! S_F$ implies  $\rho\!=\!x_0\cdots x_n\!\in\! \path^*(T)$  and  $L(\rho)\!\in\! \L_{\pref}^\phi$. 
Therefore, to solve the scLTL planning  problem, it suffices to find a path with minimum weight from the initial state to accepting states $S_F$ in the product system.  
 

\section{Planning in Partially-Known Environments}\label{sec:parti}
%
The above reviewed shortest-path-search-based LTL planning method crucially  depends on that the mobility of the robot, or the environment map $T$ is perfectly known. This method, however, is not suitable for the case of  \emph{partially-known} environments. 
To be specific,  we consider a partially-known environment in the following setting:
\begin{itemize}
    \item[A1] 
    The agent knows the existence of all regions in the environment as well as their semantics (atomic propositions hold at each region); 
    \item[A2] 
    The successor regions of each region are fixed, but the agent may not know, \emph{a priori}, what are the actual successor regions it can move to;
    \item[A3]
    Once the agent physically reaches a region, it will know the successor regions of this region precisely.  
\end{itemize}

In this section, we will   provide a formal model for such a partially-known environment using the new structure of \emph{partially-known weighted transition systems} and use \emph{regret} as a new metric for evaluating the performance of the  agent's plan in a partially-known environment.


\subsection{Partially-Known Weighted Transition Systems}

\begin{definition}[Partially-Known WTS]
A partially-known weighted transition system (PK-WTS) is a 6-tuple 
\[
\T=(X,x_0,\Delta,w,\AP,L), 
\]
where, similar  to a WTS, 
$X$ is the set of states with initial state $x_0\!\in\! X$, 
$w\!:\!X\times X\!\to\!\NN$ is the cost function 
and $L\!:\!X\!\to\! 2^\AP$ is a labeling function that assigns each state a set of atomic propositions.  
Different from the WTS, 
$\Delta:X\to 2^{2^X}$
is called a \emph{successor-pattern function} that assigns each state $x\in X$ a family of successor states.  \vspace{3pt}
\end{definition} 


The intuition of the PK-WTS $\T$ is explained as follows. 
Essentially, PK-WTS is used to describe the \emph{possible world} from the perspective of the agent. Specifically, under  assumptions A1-A3, the agent has some prior information regarding the successor states of each unknown region but does not know which one is true before it actually visits the region. Therefore, in PK-WTS $\T$, for each state $x\!\in\! X$,  we have $\Delta(x)=\{o_1,\dots,o_{|\Delta(x)|}\}$, where
each $o_i\in 2^X$ is called a \emph{successor-pattern} representing a possible  set of actual successor states at state $x$.  
Hereafter, we will also refer each $o_i\in \Delta(x)$ to as an \emph{observation} at state $x$ since the agent ``observes" its successor states when exploring state $x$. 
Therefore, for each state $x\in X$, we say $x$ is a
\begin{itemize}
    \item 
    \emph{known state}  if $|\Delta(x)|=1$; and 
    \item
    \emph{unknown state}  if $|\Delta(x)|>1$. 
\end{itemize}
We assume that the initial state $x_0$ is known since the agent has already stayed at $x_0$ so that it has the precise information regarding the successor states of $x_0$. 
Therefore, we can partition the state space as 
$X=X_{kno}\dot{\cup}X_{un}$,
where $X_{kno}$ is the set of known states and $X_{un}$ is the set of unknown states.  

In reality, the agent is moving in a specific environment that is compatible with the possible world $\T$, although itself does not know this a priori. 
Formally,  we say a WTS $T=(X,x_0,\delta_T,w,\AP,L)$ is \emph{compatible} with PK-WTS  $\T$, denoted by $T\in\T$, if  $\forall x\in X: \delta_T(x)\in\Delta(x)$. 
Clearly, if all states in $\T$ are known, then its compatible WTS is unique. 
Similar to Remark \ref{rmk-connected}, we still assume that each compatible environment $T\in\T$ is fixed and connected.

\subsection{History and Knowledge Updates}
In the partially-known setting, the agent cannot make decision only based on the finite sequence of states it has visited. In addition, it should also consider what it observed (successor-pattern) at each state visited. Note that, when the agent visits a known state $x\in X_{kno}$, it will not gain any useful information about the environment since $\Delta(x)$ is already a singleton.  Only when  the agent visits an unknown state, it will gain new information and successor-pattern at this state will become known from then on. Therefore, we refer the visit to an unknown state to as an \emph{exploration}. 

To capture the result of an exploration, 
we call a tuple $\kappa=\< x,o\>\in X\times 2^X$, where $o \in\Delta(x)$, 
a \emph{knowledge} obtained when exploring state $x$, which means the agent knows that the successor states of $x$ are $o$.   
For each knowledge $\kappa$, we denote by $\kappa(x)$ and $\kappa(o)$, respectively, its first and second components, i.e., $\kappa=\<\kappa(x),\kappa(o)\>$. 
We denote by 
\begin{equation}\label{eq:know-domain}
\textsf{Kw}=\{  \kappa \in X\times 2^X \mid  \kappa(o) \in\Delta(\kappa(x))\}, 
\end{equation}
the set of all possible knowledges. 

A \emph{history} in $\T$ is a finite sequence of knowledges  
\begin{equation}\label{eq:hist}
\hbar= \kappa_0\kappa_1\cdots\kappa_n=  \<x_0,o_0\>\<x_1,o_1\>\cdots\<x_n,o_n\> \in \textsf{Kw}^*
\end{equation}
such that 
\begin{enumerate} 
    \item 
    for any $i=0,\dots, n-1$, we have $x_{i+1}\in o_i$; and 
    \item 
    for any $i,j=1,\dots,n$, we have $x_i=x_j\Rightarrow o_i=o_j$.
\end{enumerate}
Intuitively, the first condition says that the agent can only go to one of its actual successor states in $o_i$. 
The second condition captures the fact that the actual environment is partially-known but \emph{fixed}; hence, the agent will observe the same successor-pattern for different visits of the same state. 
For history $\hbar=\kappa_0\kappa_1\cdots\kappa_n\in\textsf{Kw}^*$, we call $\kappa_0(x)\kappa_1(x)\cdots\kappa_n(x)$ its path. 
We denote by $\path^*(\T)$ and $\textsf{Hist}^*(\T)$ the set of all paths and histories of $\T$, respectively. 

Along each history $\hbar=\kappa_0\kappa_1\cdots\kappa_n\in\textsf{Kw}^*$, 
the agent obtain a set of knowledges (or simply, a knowledge-set) 
\begin{equation}
\mathcal{K}=\{ \kappa_i \mid i=0,\dots,n\}\subseteq \textsf{Kw},
\end{equation}
which is an \emph{unordered} set of knowledges.  We define 
\begin{equation}\label{eq:knowset}
\mathbb{KW}\!=\!\{  \K\!\in\!   2^{\textsf{Kw}} \mid  \forall \kappa,\kappa'\!\in\! \K:  \kappa(x)\!=\! \kappa'(x) \Rightarrow \kappa(o)\!=\! \kappa'(o)           \} 
\end{equation}
as the set of knowledge-sets.  
Therefore, given  a knowledge-set $\K\in \mathbb{KW}$, 
we say state $x\in X$ has been explored in $\K$,  if  $\<x,o\>\in \K$ for some $o$; we denote by $X(\K)$ the set of explored states in $\K$. 
If $x$ has been explored   in $\K$,
we denote by $o_\K(x)\in 2^X$ the unique observation such that 
$\<x,o_\K(x)\> \in \K$.



Then the agent can maintain a finer possible world by incorporating with the knowledges it obtained. Specifically, 
by having knowledge-set $\K \in \mathbb{KW}$, the agent can update the PK-WTS 
$\T=(X,x_0,\Delta,w,\AP,L)$  to a finer PK-WTS 
\[
\T'=\textsf{update}(\T,\K)\!=\!(X,x_0,\Delta',w,\AP,L),
\]
where  for any $x\in X$, we have
\[
\Delta'(x) = 
\left\{ \begin{array}{ll}
\{ o_\K(x)   \} & \text{if }  x\in X(\K) \\
\Delta(x) & \text{if }  x\notin X(\K) 
\end{array} 
\right..
\]
Note that, the above update function is well-defined since 
conflict knowledges $\<x,o\>,\<x,o'\>\in \textsf{Kw}$ such that  $o\neq o'$ cannot belong to the same knowledge-set by the definition of history and Equation~\eqref{eq:knowset}. 

\begin{remark}
Note that in the domain of knowledges as defined in Equation~\eqref{eq:know-domain}, 
we can omit those tuples $\<x,o\>$ when $x\in X_{kno}$ is a known state since $o$ here does not provide any additional information. Here, we still leave this reduction information when constructing a knowledge-set for the sake of unified description. One can easily omit this part when implementing our algorithm. 
\end{remark}


\subsection{Strategy and Regret}
%
Under the setting of partially-known environment, the plan is no longer an open-loop sequence. Instead, it is a \emph{strategy} that determines the next state the agent should go to based what has been visited and what has known, which are environment dependent. Formally, a strategy is a function
$\xi:\hist^*(\T)\to X\cup \{\stop\}$
such that  
for any $\hbar= \kappa_1 \cdots \kappa_n$,  where $\kappa_i=\<x_i,o_i\>$, 
either 
(i)  $\xi(\hbar)\in o_n$, i.e., it decides to move to some successor state;  or (ii) $\xi(\hbar)=\stop$, i.e., the plan is terminated.  
We denote by $\stra(\T)$ the set of all strategies for $\T$.

Although a strategy is designed to handle all possible actual environments in the possible world $\T$, when it is applied to an actual environment $T\in \T$, the outcome of the strategy can be completely determined. 
We denote by 
$\rho^{T}_\xi\!=\!x_0x_1\cdots x_n\!\in\! X^*$
the finite path induced by strategy $\xi$ in environment $T\in \T$, which is the unique path such that 
\begin{itemize}
    \item 
     $\forall i< n:\xi( \<x_0,\delta_T(x_0)\>\cdots    \<x_i,\delta_T(x_i)\>    )=x_{i+1}$; 
    and 
    \item
    $\xi( \<x_0,\delta_T(x_0)\>\cdots    \<x_n,\delta_T(x_n)\>    )=\stop$.
\end{itemize}
Note that the agent does not know \emph{a priori} which $T\in \T$ is the actual environment. 
To guarantee the accomplishment of the LTL task, a strategy $\xi$ should satisfy  
\begin{equation}\label{eq:LTL-sa}
\forall T\in \T:  \rho^{T}_\xi\in \L_{\pref}^\phi. 
\end{equation}
We denote by $\stra_\phi(\T)$   all strategies satisfying~\eqref{eq:LTL-sa}.

To evaluate the performance of strategy $\xi$, one approach is to consider the \emph{worst-case cost} of the strategy among all possible environment, i.e., $\textsf{cost}_{\text{\emph{worst}}}(\xi):=\max_{T\in \mathbb{T}}\cost(\rho^T_\xi)$.
However, as we have illustrated by the example in Figure~1, this metric cannot capture the potential benefit obtained from exploring unknown states and the agent may regret due to the unexploration.  To capture this issue, in this work, we adopt the notion of \emph{regret} as the metric to evaluate the performance of a strategy.  

\begin{definition}[Regret]\label{def:regret-in-T}
Given a partially-known environment described by PK-WTS $\T$ and a task described by an scLTL $\phi$,  the \emph{regret} of strategy $\xi$ is defined by
\begin{equation}
    \reg_\T(\xi)=\max_{T\in\T}\left(\cost(\rho_{\xi}^T)-\min_{\xi'\in\stra_\phi(\T)}\cost(\rho_{\xi'}^T)\right)
\end{equation}\vspace{0pt} 
\end{definition}

The intuition of the above notion of regret is explained as follows. 
For each strategy $\xi$ and each actual environment $T$, 
$\cost(\rho_{\xi}^T)$ is the actual cost incurred when applying this strategy to this specific environment, while $\min_{\xi'\in\stra_\phi(\T)}\cost(\rho_{\xi'}^T)$ is  cost of the best-response strategy the agent should have  taken if it knows the actual environment $T$  with hindsight. 
Therefore, their difference is the regret of the agent  when applying strategy $\xi$ in environment $T$. 
Note that, the agent does not know the actual environment $T$ precisely \emph{a priori}. Therefore, the regret of the strategy is considered as the worst-case regret among all possible environments $T\in \T$.  

\subsection{Problem Formulation}
%
After presenting the PK-WTS modeling framework as well as the regret-based performance metric, we are now ready to formulate the problem that we solve in this work. 

\begin{problem}[Regret-Based  LTL Planning]\label{pro-1}
Given a possible world represented by PK-WTS $\T$ and an scLTL task $\phi$, 
synthesize a strategy $\xi$ such that i) $ \rho^{T}_\xi\in \L_{\pref}^\phi$ for any $T\in \T$; and ii) $\reg_\T(\xi)$ is minimized. 
\end{problem} 


\section{Game-Based Synthesis Algorithm}\label{sec:game}
In this section, we solve the regret-based LTL planning problem. 
Our approach is to first build a knowledge-based game arena that is consistent with all possible histories. Then, based on the game arena, an effective value iteration algorithm is developed, which provides the optimal strategy with minimum regret. 

\subsection{Knowledge-Based Game Arena}
Given  PK-WTS  $\T=(X,x_0,\Delta,w,\AP,L)$,  its skeleton system is a WTS 
\[
\mathcal{T}=(X,x_0,\delta_{\mathcal{T}},w,\AP,L),
\]
where  for any $x\in X$, we have 
$\delta_{\mathcal{T}}(x)=\bigcup\{ o\in \Delta(x)\}$, i.e., the successor states of $x$ is defined as the union of all possible successor-patterns. 
To incorporate with the task information,  let $\A_\phi\!=\!(Q,q_0,\Sigma,f,Q_F)$ be the DFA that accepts all good-prefixes of scLTL formula $\phi$. We construct the product system between  $\mathcal{T}$ and $\A_\phi$, denoted by 
\[
\P=\mathcal{T}\otimes \A_\phi=(S,s_0,\delta_\P,w_\P,S_F), 
\]
where the product ``$\otimes$" has been defined in Section \ref{subsec-product} and recall that its state-space is $S=X\times Q$.
 
However, the state-space of $\P$ is still not sufficient for the purpose of decision-making since the explored knowledges along the trajectory are missing. Therefore, we further incorporate the knowledge-set into the product state-space and explicitly split the movement  choice of the agent and the non-determinism of the environment. This leads to the following \emph{knowledge-based game arena}.  

\begin{definition}[Knowledge-Based Game Arena]
Given PK-WTS $\T$, the knowledge-based game arena is a bipartite graph
\[
G=(V=V_a \dot{\cup} V_e,v_0,E),
\]
where
\begin{itemize}
\item 
$V_a\!=\!X\times Q\times \mathbb{KW}$ is the set of \emph{agent vertices}; 
\item 
$V_e\!=\! X\times Q\times \mathbb{KW} \times X$ is the set of \emph{environment vertices}; 
\item 
$v_0= (x_0,q_0,  \kappa_0 ) \in V_a$ is the initial (agent) vertex, where  $\kappa_0\in \kw$ is the initial knowledge of the agent, i.e., 
$\kappa_0(x)=x_0$ and   $\Delta(x_0)=\{ \kappa_0(o)\}$; 
\item 
$E\subseteq V\times V$ is the set of edges defined by:  
for any $v_a=(x_a,q_a,\K_a)\in V_a$ and $v_e=(x_e,q_e,\K_e,\hat{x}_e)\in V_e$, we have
\begin{itemize}
    \item 
    $\< v_a,v_e\>\in E$ whenever
    \begin{enumerate}
        \item[(i)]
        $(x_e,q_e,\K_e)=(x_a,q_a,\K_a)$; and 
        \item[(ii)] 
        $\hat{x}_e\in o_{\K_a}(x_a)$. 
    \end{enumerate} 
    \item 
    $\< v_e,v_a\>\in E$ whenever
    \begin{enumerate}
        \item[(i)]
        $x_a=\hat{x}_e$; and 
        \item[(ii)] 
        $(x_a,q_a)\in \delta_\P(x_e,q_e) $; and 
        \item[(iii-1)] 
        if $x_a \in X(\K_e) $, then $\K_a=\K_e$; 
        \item[(iii-2)] 
        if $x_a \notin X(\K_e) $, then  we have  
        \[
            \K_a  \in
      \{  \K_e\cup \{\<x_a ,o\>\}  \mid  o\in \Delta( x_a )\}. 
        \]
    \end{enumerate}  
\end{itemize} 
\end{itemize}{\vspace{3pt}}
\end{definition}

The intuition of the knowledge-based game arena $G$ is explained as follows. The graph is bipartite with two types of vertices: agent vertices from which the agent chooses a feasible successor state to move to and 
environment vertices from which the environment chooses the actual successor-pattern in the possible world. More specifically, for each agent vertex $v_a=(x_a,q_a,\K_a)$, the first component $x_a$ represents its physical state in the system, the second component $q_a$ represents the current DFA state for task $\phi$ and the third component $\K_a$ represents the knowledge-set of the agent obtained along the trajectory.  
At each agent vertex, the agent chooses to move to a successor state. Note that since $x_a$ is the current state, it has been explored and we have $x_a\in X(\K_a)$, i.e., we know that the actual successor states of $x_a$ are $o_{\K_a}(x_a)$. Therefore,  it can move to any environment state $v_e=(x_a,q_a,\K_a, \hat{x})$ by ``remembering" the successor state $\hat{x}\in o_{\K_a}(x_a)$ it chooses. 
Now, at each environment state  $v_e=(x_e,q_e,\K_e,\hat{x}_e)$, the meanings of the first three components are the same as those for agent state. The last component $\hat{x}_e$ denotes the state it is moving to.  
Therefore, $v_e$ can reach agent state $v_a=(x_a,q_a,\K_a)$, where the first two components are just the transition in the product system synchronizing the movements of the WTS and the DFA. Note that we have $x_a=\hat{x}_e$ since the movement has been decided by the agent. However, 
for the last component of knowledge-set $\K_a$, we need to consider the following two cases:
\begin{itemize}
    \item 
    If state $x_a\!=\!\hat{x}_e$ has already been explored, then the agent must observe the same successor-pattern as before. Therefore, the knowledge-set is not updated;  
    \item 
    If state $x_a\!=\!\hat{x}_e$ has not yet been explored, then the new explored knowledge $\<x_a, o\>\in \kw$ should be added to the knowledge-set $\K_e$. However, since this is the first time the agent visits $x_a$, any possible observations $o\in \Delta(x_a)$ consistent with the prior information are possible. Therefore, the resulting knowledge-set $\K_e$ is non-deterministic. 
\end{itemize}


\subsection{Strategies and Plays in the Game Arena}
 We call  a finite sequence of vertices 
$\pi=v_0v_1\cdots v_n\in V^*$ a \emph{play} on $G$ if $\< v_i,v_{i+1} \>\!\in\! E$ and we denote by $\textsf{Play}^*(G)$ the set of all finite plays on $G$.
We call $\pi$ a \emph{complete  play} if $\textsf{last}(\pi)\in V_a$, where $\textsf{last}(\pi)$ denotes the last vertex in  $\pi$.  Then for a  complete play 
$\pi=v_0v_1\cdots v_{2n}\in V^*V_a$, 
where $v_{2i}=(x_i, q_i, \K_i ), i=0,\dots, n$, it  induces a path denoted by 
$\pi_{\text{\emph{path}}}= x_0 x_1\cdots x_{n}$ as well as a history 
\begin{equation}
    \pi_{his}= \<x_0, o_{\K_0}(x_0)\>\<x_1, o_{\K_1}(x_1)\> \cdots 
    \<x_{n}, o_{\K_{n}}(x_{n})\>.   \nonumber
\end{equation}
Note that, in the above, we have $\K_0\subseteq \K_1\subseteq\cdots\subseteq \K_n$ 
and the knowledge-set constructed along history $\pi_{his}$ is exactly $\K_n$. 
On the other hand, for any history 
$\hbar= \kappa_0\kappa_1\cdots\kappa_n\in \kw^*$, there exists a unique complete play in $G$, denoted by $\pi_{\hbar}$,  
such that  its induced history   is $\hbar$.


Since the first two components of $G$ are from the product of $\mathcal{T}$ and $\A_\phi$, 
for any complete play $\pi$, we have $L(\pi_{\text{\emph{path}}})\in \L_{\pref}^\phi$ iff the second component of $\textsf{last}(\pi)$ is an accepting state in the DFA. Therefore, we define 
\begin{equation}
V_F=\{ (x_a,q_a,\K_a)\in V_a \mid q_a\in Q_F \}\nonumber
\end{equation}
the set of accepting vertices representing the satisfaction of the scLTL task. Also, since only  edges from $V_e$ to $V_a$ represent   actual movements, we   define a weight function for $G$ as 
$w_G\!:\!V\!\times\! V\!\to\! \mathbb{N}$, 
where for any $v_a\!=\!(x_a,q_a,\K_a)$ and $v_e\!=\!(x_e,q_e,\K_e, \hat{x}_e)$, 
we have $w_G(v_a,v_e)\!=\!0$ and $w_G(v_e,v_a)\!=\!w(x_e,x_a)$. 
The the cost of a play  $\pi\!=\!v_0v_1\cdots v_n\!\in\! V^*$ is defined as 
  $\cost_G(\pi)\!=\!\sum_{i=0}^{n-1}w_G ( v_i,v_{i+1} )$.


A strategy for the agent-player is a function 
$\sigma_a\!:\!  V^*V_a \!\to\! V_e \cup\{\stop\}$ 
such that for any $\pi \!\in\! V^*V_a$, either $\< \textsf{last}(\pi),\sigma_a(\pi) \>\!\in\! E$ or $\sigma_a(\pi)\!=\!\stop$.
Analogously, a strategy for the environment-player is a function $\sigma_e\!:\! V^*V_e\!\to\! V_a$ such that for any $\pi\!\in\! V^* V_e$, we have $\< \textsf{last}(\pi),\sigma_e(\pi) \>\!\in\! E$.
We denote by $\Sigma_a(G)$ and $\Sigma_e(G)$ the sets of all strategies for the agent and the environment respectively.
In particular, a strategy $\sigma\in\Sigma_a(G)\cup\Sigma_e(G)$ is said to be positional if $\forall \pi,\pi'\!:\!\textsf{last}(\pi)\!=\!\textsf{last}(\pi')\Rightarrow \sigma(\pi)\!=\!\sigma(\pi')$ and we denote by $\Sigma_a^1(G)$ and $\Sigma_e^1(G)$ the corresponding sets of all positional strategies respectively. 
Given strategies $\sigma_a\!\in\!\Sigma_a(G)$ and $\sigma_e\!\in\!\Sigma_e(G)$, the \emph{outcome play} $\pi_{\sigma_a,\sigma_e}$ is the unique sequence 
$v_0v_1\cdots v_n\!\in\! V^* V_a$ s.t.\
\begin{itemize}
    \item 
    $\forall i<n: v_i\in V_a \Rightarrow \sigma_a(v_0v_1\cdots v_i) =v_{i+1}$; and
    \item 
    $\forall i<n: v_i\in V_e\Rightarrow \sigma_e(v_0v_1\cdots v_i) =v_{i+1}$; and
    \item 
    $\sigma_a(v_0v_1\cdots v_n)=\stop$.  
\end{itemize}
For the environment-player, under assumption A3, we can further characterize its set of strategies as
\begin{equation}
    \S_e=\left\{
    \sigma_e\in\Sigma_e^1(G): 
    \begin{aligned}
    &\forall v_e,v_e'\in V_e:X(v_e)=X(v_e')\\
    &\Rightarrow X(\sigma_e(v_e))=X(\sigma_e(v_e'))
    \end{aligned}
    \right\}
\end{equation}
where we denote by $X(\cdot)$ the first component of an agent vertex $v_a\!=\!(x_a,q_a,\K_a)\!\in\! V_a$ as well as an environment vertex $v_e\!=\!(x_e,q_e,\K_e,\hat{x}_e)\!\in\!V_e$, i.e., for such $v_a,v_e$, we have $X(v_a)\!=\!x_a$ and $X(v_e)\!=\!x_e$.
For the agent-player, we say $\sigma_a$ is winning  if for any $\sigma_e\in\S_e$, we have $\textsf{last}(\pi_{\sigma_a,\sigma_e})\in V_F$. 
We denote by $\S_a\subseteq \Sigma_a(G)$ the set of all winning strategies. 
By Remark~\ref{rmk-connected}, we know $|\S_a^F|\geq 1$.
Similarly to Definition~\ref{def:regret-in-T}, we can also define the \emph{regret} of strategy $\sigma_a\!\in\!\S_a$ in $G$ by 
\begin{equation}\label{eq:reg-G}
    \reg_G(\sigma_a)\!=\!\!\!\max_{\sigma_e\in\S_e}\!\left(\! \cost_G (\pi_{\sigma_a,\sigma_e})-\min_{\sigma_a'\in\S_a}\!\!\!\cost_G(\pi_{\sigma_a,\sigma_e}) \!\!\right)
\end{equation} 
In particular, we denote by $\reg_G^{\sigma_e}(\sigma_a)\!:=\!\cost_G (\pi_{\sigma_a,\sigma_e})\!-\!\min_{\sigma_a'\in\S_a}\!\!\!\cost_G(\pi_{\sigma_a,\sigma_e})$ the regret of agent strategy $\sigma_a\!\in\!\S_a$ w.r.t. an environment strategy $\sigma_e\!\in\!\S_e$.


Essentially,  an agent-player's strategy $\sigma_a\in \S_a$ uniquely defines a corresponding strategy  in $\T$, denoted by $\xi_{\sigma_a}\in \stra_\phi(\T)$ as follows:  
for any $\hbar\in \hist^*(\T)$, we have $ \xi_{\sigma_a}(\hbar)=\sigma_a(\pi_\hbar)$. The environment-player's strategy  $\sigma_e\in \S_e$ essentially corresponds to a possible actual environment $T\in \T$ since it needs to specify an observation $o\in \Delta(x)$ for each unexplored $x$, and once $x$ is explored, the observation is fixed based on the construction of $G$.  Since $\cost_G(\cdot)$ is defined only according to its first component, for any play $\pi\in V^*V_a$, we have 
$\cost_G(\pi) =  \cost( \pi_{\text{\emph{path}}} )$.

In summary,   in order to solve Problem~1, it suffices to find a winning strategy $\sigma_a\!\in\! \S_a$ of the agent-player in the knowledge-based game arena $G$, such that 
$\reg_G(\sigma_a)$ as defined in~\eqref{eq:reg-G} is minimized. 
Then, based on the optimal strategy $\sigma_a\!\in\! \S_a$, we can induce a strategy $\xi_{\sigma_a}\!\in\! \stra(\T)$ for PK-WTS $\T$. Therefore, hereafter, we will only focus on the game graph $G$ without considering $\T$. 
Furthermore, it suffices to find a \emph{positional winning strategy} $\sigma_a\!\in\!\S_a\cap\Sigma_a^1(G)\!:=\!\S_a^1$ for the agent-player since it is unnecessary to record the choices of the environment-player based on the construction of the knowledge-based game arena $G$.

\subsection{Synthesis via Min-Max Games}
Since the optimal strategy on the arena $G$, we denote by $\sigma_a^*$, satisfies $\reg(\sigma_a^*)=\min_{\sigma_a\in\S_a}\reg_G(\sigma_a)$, it is natural for us to reduce the strategy synthesis problem to the solution of a \emph{min-max reachability game} \citet{brihaye2017pseudopolynomial} in which the optimal strategy $\sigma^*$ satisfies 
\begin{equation}
    \max_{\sigma_e\in\S_e}W(\pi_{\sigma^*,\sigma_e})=\min_{\sigma_a\in\S_a}\max_{\sigma_e\in\S_e}W(\pi_{\sigma_a,\sigma_e})
\end{equation}
where $W\!:\!\textsf{Play}^*(G)\!\to\! \NN$ is some cost function.
To this end, given knowledge-based game arena $G$, we first define the \emph{best response} w.r.t. a knowledge $\K_a\!\in\!\mathbb{KW}$ as 
\begin{equation}
    br(\K_a)=\textsc{ShortestPath}(V_{x_0}(\K_a),V_F)
\end{equation}
where $V_{x_0}(\K_a)\!:=\!\{v_a\!\in\! V_a\mid X(v_a)\!=\!x_0\wedge \K(v_a)\!=\!\K_a\}$ is the set of agent vertices whose first component are $x_0$ and third component is $\K_a$ and we denote by $\K(\cdot)$ the third component of an agent vertex. Given two subsets $V_1,V_2\subseteq V$ on $G$, the shortest path between them $\textsc{ShortestPath}(V_1,V_2)$ can be directly computed via a Dijkstraâ€™s algorithm.


For the agent-player, the cost of all its positional winning strategies is bounded by a constant $B\!=\!M^G|V|\!\in\!\NN$, where $M^G\!:=\!\max_{(v,v')\in E}w_G(v,v')$ denotes the largest cost of the transitions in $G$. To this end, we define $\hat{G}\!=\!(\hat{V},\hat{v}_0,\hat{E})$ as the  \emph{information game arena}, where $\hat{V}\!\subseteq\! V\!\times\! [0,B]$, $\hat{v}_0\!=\!(v_0,0)$, $\hat{E}\subseteq \hat{V}\!\times\! \hat{V}$ is defined by: for any $\hat{v}\!=\!(v,u),\hat{v}'\!=\!(v',u')\!\in\!\hat{V}$, we have $(\hat{v},\hat{v}')\!\in\!\hat{E}$ if i) $(v,v')\!\in\! E$ and ii) $u'\!=\!u+w_G(v,v')$.
We also have partition $\hat{V}\!=\!\hat{V}_a\dot{\cup}\hat{V}_e$, based on whether the first component of each vertex belongs to $V_a$ or $V_e$.
Given $\hat{G}$, we define cost function $\mu\!:\!\hat{E}\!\to\!\NN$ by 
\begin{itemize}
    \item\hspace{-4pt} for any $(\hat{v},\hat{v}')\!\in\!\hat{E}\!:\!\hat{v}'\!\notin\!\hat{V}_F$, we have $\mu(\hat{v},\hat{v}')\!=\!0$;
    \item\hspace{-4pt} for any $(\hat{v},\hat{v}')\!\in\!\hat{E}\!:\!\hat{v}'\!=\!((x,q,\K),u)\!\in\!\hat{V}_F$, we have 
    \begin{equation}
        \mu(\hat{v},\hat{v}')\!=\!u-br(\K).
    \end{equation}
\end{itemize}

Based on the above construction, one can easily build the ``one-to-one correspondence" between a play $\rho$ in $G$ and the play $\hat{\rho}$ in $\hat{G}$, as well as strategies $\sigma$ and $\hat{\sigma}$, which is omitted here.
Given a play $\hat{\rho}\!=\!\hat{v}_0\hat{v}_1\cdots \hat{v}_n\!\in\! \textsf{Play}^*(\hat{G})$, we denote its cost w.r.t $\mu$ by $\cost_{\hat{G}}^\mu(\hat{\rho})\!:=\!\sum_{i=0}\mu(\hat{v}_i,\hat{v}_{i+1})$. Then we obtain the following result and the detailed proof is given in Appendix \ref{apdx:pf-proposition}.
 
\begin{proposition}\label{proposition}
Given game arena $G$ with the information game arena $\hat{G}$ and cost function $\mu\!:\!E\!\to\! \NN$, for each $\sigma_a\in\S_a^1$,
we have
\begin{equation}\vspace{-1pt}
    \reg_G(\sigma_a)=\max_{\hat{\sigma}_e\in\hat{\S}_e}\cost_{\hat{G}}^\mu(\pi_{\hat{\sigma}_a,\hat{\sigma}_e})
\end{equation}
\end{proposition}


With the result of Proposition~\ref{proposition}, to obtain the optimal regret-minimizing strategy, it suffices for us to solve a min-max reachability game on $\hat{G}$ w.r.t. cost function $\mu$. Now, we summarize our solution as Algorithm~\ref{alg:1} and obtain the following result and the proof is given in Appendix \ref{apdx:thm}.

\begin{theorem}\label{thm}
The strategy $\xi^*$ obtained from Algorithm~\ref{alg:1} correctly solves the Regret-Based LTL Planning Problem defined in Problem~\ref{pro-1}.
\end{theorem}
 
 
\begin{algorithm} 
\caption{Optimal Regret-Minimizing LTL Plan}\label{alg:1}
\KwIn{PK-WTS $\T$ and scLTL $\phi$;}
\KwOut{Optimal Regret-Minimizing Plan $\xi^*$}

Construct skeleton-WTS $\mathcal{T}$ and DFA $\A_\phi$ from $\phi$;\\
Construct product system $\P=\mathcal{T}\otimes\A_\phi$;\\
Construct knowledge-based game arena $G$;\\
Construct information game arena $\hat{G}$;\\

\For{$(\hat{v},\hat{v}')\!\in\! \hat{E}$} 
{
define its cost $\mu(\hat{v},\hat{v}')$;
}

Obtain strategy $\hat{\sigma}_a^*=\texttt{SolveMinMax}(\hat{G},\mu)$;\\
Obtain strategy $\sigma_a^*$ from $\hat{\sigma}_a^*$;\\
Obtain strategy $\xi^*=\xi_{\sigma_a^*}$;


\textbf{procedure} $\texttt{SolveMinMax}(\hat{G},\mu)$\\
\For{$\hat{v}\!\in\! \hat{V}$}{
    \eIf{$\hat{v}\in \hat{V}_F$}{
        $W^{(0)}(\hat{v})= 0$ and $\hat{\sigma}_a^{(0)}(\hat{v})=\stop$;}
        {$W^{(0)}(\hat{v})= \infty$;}
}
\While{$\exists \hat{v}\!\in\! \hat{V}\!:\!W^{(k+1)}(\hat{v})\!\neq\! W^{(k)}(\hat{v})$}
{
\For{$\hat{v}_e\in \hat{V}_e$} 
{
$W^{(k+1)}(\hat{v}_e)\!=\!\displaystyle\!\!\!\max_{\hat{v}_a\in\succ(\hat{v}_e)}\left(W^{(k)}(\hat{v}_a)+\mu(\hat{v}_e,\hat{v}_a)\right)$;
}
\For{$\hat{v}_a\in \hat{V}_a$} 
{
\eIf{$\hat{v}_a\in \hat{V}_a$}{
        $W^{(k+1)}(\hat{v}_a)=0$ and $\hat{\sigma}_a^{(k+1)}(\hat{v}_a)=\stop$;}
        {$\!\!\!W^{(k+1)}(\hat{v}_a)\!=\!\!\!\!\!\displaystyle\!\!\!\min_{\hat{v}_e\in\succ(\hat{v}_a)}\left(W^{(k)}(\hat{v}_e)\!+\!\mu(\hat{v}_e,\hat{v}_a)\right)$;\\
        $\!\!\hat{\sigma}_a^{(k+1)}(\hat{v}_a)\!=\displaystyle\!\!\!\argmin_{\hat{v}_e\in\succ(v_a)}\!\!\left(W^{(k)}(\hat{v}_e)\!+\!\mu(\hat{v}_e,\hat{v}_a)\right)$;}
} 
$k\leftarrow k+1$;
}
\textbf{return} $\sigma_a^{(k)}$;
\end{algorithm}
 
 
 
\section{Case Study: A team of firefighting robots}\label{sec:case}

In this section, we present a case study to illustrate the proposed framework. 
We consider a team of firefighting robots consisting of a ground robot and a UAV working in an urban district shown in Figure~\ref{fig:sim-map}. 
Specifically,  the blue regions are  \emph{rivers}, the black regions are \emph{bridges}, 
the grey regions are \emph{squares},  the green regions are \emph{parks},
the brown regions are \emph{buildings} and the yellow region is the \emph{base} of the team of robots. 
The firefighting mission   in this district is undertaken by the collaboration of the UAV and the ground robot. Specifically, we assume that the district map is completely unknown to the robotic system initially. When a fire alarm is reported, the UAV takes off first and reconnoiters over the district, which allows the system to obtain some rough information of the distinct and leads to a possible world map. More detailed connectivities for some unknown regions in the possible world still remain to be explored by the ground robot. 
In  order to accomplish the firefighting mission, the ground robot needs to first go to some regions with \textsf{extinguisher} to get fire-extinguishers and then move to the region with \textsf{fire}. Let $\AP=\{\textsf{fire} , \textsf{extinguisher}\}$. The mission can be described by the following   scLTL formula:
\begin{equation}
 \phi= (\neg \textsf{fire} \ U\  \textsf{extinguisher})\wedge   \lozenge  \textsf{fire} 
\end{equation} 

Suppose that, after the reconnaissance, the UAV will get a look down picture of the entire district as shown in Figure~\ref{fig:sim-look}. 
Based on the distinct picture, the system will know the map geometry and the semantics. Specifically, it knows that there is a \textsf{fire} in Square~4 and there are \textsf{extinguishers} in Parks~1-3. The connectivities of all open regions including rivers, bridges, squares, parks and the base are known. However, the UAV cannot tell if each building has corresponding doors  connecting with its adjacent open regions.  In order to figure out the (non-)existence of those potential doors, the ground robot has to move to the adjacent areas to explore. Therefore, the possible world model $\T$ of the robots is shown in Figure~\ref{fig:sim-poss}, where the  dashed arrows denote some potential doors, whose existences are unknown. 


\begin{figure} 
\centering
\subfigure[Firefighting Scenario.]{
\includegraphics[scale=0.25]{fig-scenario.eps}\label{fig:sim-map}
}
\subfigure[Perspective from UAV.]{
\includegraphics[scale=0.2]{fig-UAV.eps}\label{fig:sim-look}
}
\subfigure[Possible World $\T$.]{
\input{figure2}\label{fig:sim-poss}
}.\vspace{-12pt}
\caption{Simulation Setting.}\label{fig:simulation-setting}
\end{figure} 


\begin{figure} 
\centering
\subfigure[Trajectory in Environment $T_1$]{
\includegraphics[scale=0.2]{fig-trajectory1.eps}\label{fig:sim-T-1}
}
\subfigure[Trajectory in Environment $T_2$]{
\includegraphics[scale=0.2]{fig-trajectory2.eps}\label{fig:sim-T-2}
}\vspace{-12pt}
\caption{Simulation Results}\label{fig:simulation-results}
\end{figure} 


Now, the environment is partially-known in the sense that the accessibility of the buildings are unknown to the robotic system until the ground robot reaches their adjacent regions. Then based   on the possible world model $\T$ and the scLTL $\phi$, we can synthesize an strategy $\xi^*$ that minimizes the regret while achieving $\phi$. We have implemented our algorithm in  robot simulator \texttt{V}-\texttt{REP} (Videos are available at  \ZJN{\url{https://youtu.be/lLRT2pLfABA}}). 
As shown in Figure~\ref{fig:simulation-results}, we consider two different actual environments $T_1,T_2  \!\in\!\T$ compatible to the possible world $\T$ when applying the same strategy $\xi^*$.  
To minimize its regret, the ground robot needs to first go to Square~2 to explore if there exists a door connecting Building~3. Environment $T_1$  in Figure~\ref{fig:sim-T-1} actually corresponds to the case of the existence of such a door. Then by passing through this door, the ground robot can easily find a \textsf{extinguisher} in Park~3 and then quench the \textsf{fire} in Square~4. This actual path is shorter than the path planned based on the worst-case by  assuming  no such a door exists.  
However, environment $T_2$ in Figure~\ref{fig:sim-T-2} corresponds to the case, where there is no door between Square~2 and Building~3. Note that, the ground robot will still first go to Square~2. However, when it realizes that there is no such a door, it will turn back and go to Park~1, where it finds a \textsf{extinguisher} and luckily, there is a door to Building~2. Then the ground robot will cross through Building~2 and finally reaches Square~4 to quench the \textsf{fire}.    
Although this actual path is longer than the path planned based on the worst-case, the robot will not regret that much since it does not know, \emph{a priori}, the non-existence of such a door. 



\section{Conclusions}\label{sec:conclu}
In this paper, we proposed a new approach for optimal path planning for scLTL specifications under partially-known environments. We adopted the notion of regret to evaluate the trade-off between cost incurred in an actual environment and the potential benefit of exploring unknown regions. A   knowledge-based model was developed to formally describe the partially-known scenario and an effective algorithm was proposed to synthesize an optimal strategy with minimum regret.  In the future, we would like to extend our results to multi-agent systems with general LTL specifications.   



\bibliography{myref}

\newpage
\appendix
\section{Proof of Proposition \ref{proposition}}\label{apdx:pf-proposition}

In what follows, we first present three lemmas based on which we finish the proof of Proposition \ref{proposition}.

\begin{lemma}\label{lem-positional}
A \emph{positional} winning strategy $\sigma_a\!\in\!\S_a^1$ can minimize the regret of the agent-player in $G$.
\end{lemma}


\begin{pf}
Consider a winning strategy for the agent-player $\sigma_a^F\!\in\!\S_a$, which means that for any $\sigma_e\!\in\!\S_e(G)$, the \emph{outcome play} $\pi_{\sigma_a^F,\sigma_e}$ satisfies $\textsf{last}(\pi_{\sigma_a^F,\sigma_e})\!\in\! V_F$. The existence of such winning strategy $\sigma_a^F$ can be guaranteed by Remark 1. 
It is obvious that strategy $\sigma_a^F$ needs at most finite memory since all the outcome plays $\pi_{\sigma_a^F,\sigma_e},\forall\sigma_e\!\in\!\S_e$ are finite. 
Then we show the existence of a memoryless strategy $\sigma_a\!:\!V_a\!\to\! V_e$ corresponding to $\sigma_a^F$ such that 
\begin{equation}\label{eq-costleq}
    \forall \sigma_e\in\S_e:\cost_G(\pi_{\sigma_a,\sigma_e})\leq\cost_G(\pi_{\sigma_a^F,\sigma_e}).
\end{equation}
We define $\sigma_a$ by: for any $\sigma_e\!\in\!\S_e$ with $\pi_{\sigma_a^F,\sigma_e}\!=\!v_0v_1\cdots v_n$ being the output play and for all $v_k\!\in\! V_a$, we have
\begin{equation}
    \not\exists i\geq k:v_i=v_k\Rightarrow\sigma_a(v_k)=v_{k+1}.
\end{equation}
It is obvious that strategy $\sigma_a$ is well defined.
Furthermore, for any $\sigma_e\!\in\!\S_e$, 
the output play 
$\pi_{\sigma_a,\sigma_e}$ only visits the states in $\pi_{\sigma_a^F,\sigma_e}$,
since the environment-player plays positional strategies.
Then \eqref{eq-costleq} holds since we obtain $\pi_{\sigma_a,\sigma_e}$ by removing all cycles in $\pi_{\sigma_a^F,\sigma_e}$ based on the above construction.
It directly follows that
\begin{equation}
    \forall \sigma_e\in\S_e:\reg_G^{\sigma_e}(\sigma_a)\leq \reg_G^{\sigma_e}(\sigma_a^F).
\end{equation}
Then we have
\begin{equation}\label{eq-regleq}
    \reg_G(\sigma_a)\leq\reg_G(\sigma_a^F).
\end{equation}
That is, given any winning strategy for the agent-player, we can always find a positional strategy making \eqref{eq-regleq} hold. The proof is thus completed.
\end{pf}

Therefore, to synthesize a strategy that minimizes the regret, it suffices to consider positional winning strategies on $G$. 
For each positional winning strategy, we aim to compute its regret on game arena $G$. However, given an agent-strategy $\sigma_a\!\in\!\S_a$,
it is typically to compute its the cost against all environment-player strategies $\sigma_e\!\in\!\S_e$. In fact, given an agent-player strategy, there may be more than one environment-player strategies resulting in the same output play. For those environment-player strategies, it suffices to consider an $\sigma_e'$ that minimizes the regret $\reg_G^{\sigma_e}(\sigma_a)$. Therefore, we have the following result.

\begin{lemma}\label{lem-env}
Given $\sigma_a\!\in\!\S_a^1$, for any $\sigma_e\!\in\!\S_e$ resulting in the same output play $\pi_{\sigma_a,\sigma_e}$, we have
\begin{equation}\label{eq-1}
    \reg_G^{\sigma_e}(\sigma_a)\geq\cost_G(\pi_{\sigma_a,\sigma_e}) -br(\K(\last(\pi_{\sigma_a,\sigma_e'})))
\end{equation}
Specifically, there exists $\sigma_e'\!\in\!\S_e$ such that %$\pi_{\sigma_a,\sigma_e'}\!=\!\pi_{\sigma_a,\sigma_e}$,
\begin{equation}\label{eq-2}
    \min_{\sigma_a\in\S_a}\cost_G(\pi_{\sigma_a,\sigma_e'})= br(\K(\last(\pi_{\sigma_a,\sigma_e'})))
\end{equation}
and thus $\reg_G^{\sigma_e'}(\sigma_a)\!=\!\cost_G(\pi_{\sigma_a,\sigma_e'}) -br(\K(\last(\pi_{\sigma_a,\sigma_e'})))$.
\end{lemma}

\begin{pf}
Based on the construction of the knowledge-based game arena, we know that, there is a ``one-to-one" correspondence between an environment-player strategy $\sigma_e\!\in\!\S_e$ and an actual environment $T\!\in\!\T$. For each actual environment $T$, we denote by $\K^T$ the knowledge obtained by the agent after exploring all the unknown states. Given a play $\pi=v_0v_1\!\cdots\! v_n\in\play^*(G)$, each $v_i\!\in\! V_e$ such that $\succ(v_e)\!\geq\! 2$ represents an update of the knowledge, i.e., for such $v_i\!=\!(x_i,q_i,\K_i,\hat{x}_i)$ and $v_{i+1}\!=\!(x_{i+1},q_{i+1},\K_{i+1})$, we have $\K_i\subset \K_{i+1}$. Accordingly, we know that, for each knowledge $\K\!\in\!\mathbb{KW}$, there are a sequence of knowledge updates $\K_1,\K_2,\ldots$ such that
\begin{itemize}
    \item[i)] $\K\subset\K_1\subset\cdots\subset \K_k$;
    \item[ii)] $\K_k=\K^T$ for some $T\in\T$.
\end{itemize}
In particular, we have $k\!=\!0$ if $\K\!=\!\K^T$ for some $T\!\in\!\T$ initially. Therefore, given the two strategies $\sigma_a$ and $\sigma_e$ with $\last(\pi_{\sigma_a,\sigma_e}):=(x,q,\K)$, we denote by $T_1,T_2,\ldots,T_m\!\in\!\T$ the actual environments such that ii) could hold. Furthermore, we denote by $\sigma_e^1,\sigma_e^2,\ldots,\sigma_e^m\!\in\!\S_e$ the corresponding environment-player strategies. Then it follows that 
\begin{equation}
    \pi_{\sigma_a,\sigma_e^i}=\pi_{\sigma_a,\sigma_e},\forall i\!=\!1,\ldots,m
\end{equation}
Obviously, we have $\cost_G(\pi_{\sigma_a,\sigma_e^i})\!=\!\cost_G(\pi_{\sigma_a,\sigma_e})$ and thus
\[
\reg^{\sigma_e^i}(\sigma_a)=\cost_G(\pi_{\sigma_a,\sigma_e})-\min_{\sigma_a'\in\S_a}\cost_G(\pi_{\sigma_a',\sigma_e^i})
\]
Based on the construction of $br(\K)$, we have
\begin{equation}
    br(\K)=\min_{i\in\{1,\ldots,m\}}\min_{\sigma_a'\in\S_a}\cost_G(\pi_{\sigma_a',\sigma_e^i})
\end{equation}
It directly follows that 
\begin{equation}
    \reg^{\sigma_e^i}(\sigma_a)\geq\cost_G(\pi_{\sigma_a,\sigma_e})-br(\K)
\end{equation}
Therefore, \eqref{eq-1} holds. Furthermore, \eqref{eq-2} also holds by denoting $\sigma_e'\!=\!\argmin_{i\in\{1,\ldots,m\}}\reg^{\sigma_e^i}(\sigma_a)$. The proof is thus completed.
\end{pf}

To this end, given an agent-player strategy $\sigma_a\in\S_a$, since $\reg(\sigma_a)\!=\!\max_{\sigma_e\in\S_e}\reg_G^{\sigma_e}(\sigma_a)$, it suffices to consider the environment-player strategies $\sigma_e'$ such that \eqref{eq-2} holds.

On the other hand, based on the construction of $\hat{G}$, we directly have the following result.

\begin{lemma}\label{lem-one2one}
Given knowledge-based game arena $G$ with its information game arena being $\hat{G}$, for any $\sigma_a\in\S_a^1$ and any $\sigma_e\in\S_e$, we have
\begin{equation}
    \cost_G(\pi_{\sigma_a,\sigma_e})=\cost_{\hat{G}}(\hat{\pi}_{\hat{\sigma}_a,\hat{\sigma}_e})
\end{equation}
where $\hat{\sigma}_a$ and $\hat{\sigma}_e$ denote the corresponding strategies of $\sigma_a$ and $\sigma_e$ in $\hat{G}$, respectively.
\end{lemma}

By Lemma \ref{lem-env} and Lemma \ref{lem-one2one}, we know that, given a strategy $\sigma_a\in\S_a^1$, for any $\sigma_e'\in\S_e$ satisfying \eqref{eq-1}, we have
\begin{equation}
    \reg_G^{\sigma_e'}(\sigma_a)=\cost_{\hat{G}}(\hat{\pi}_{\hat{\sigma}_a,\hat{\sigma}_e'})-br(\K(\last(\pi_{\sigma_a,\sigma_e'})))
\end{equation}
Based on the construction of $\mu\!:\!\hat{E}\!\to\!\NN$, we have
\begin{equation}
    \reg_G^{\sigma_e'}(\sigma_a)=\cost_{\hat{G}}^\mu(\hat{\pi}_{\hat{\sigma}_a,\hat{\sigma}_e'})
\end{equation}
Accordingly, we have
\begin{equation}
    \reg_G(\sigma_a)=\max_{\hat{\sigma}_e\in\hat{\S}_e}\cost_{\hat{G}}^\mu(\hat{\pi}_{\hat{\sigma}_a,\hat{\sigma}_e})
\end{equation}

Above all, the proof of Proposition \ref{proposition} is thus completed.



\section{Proof of Theorem \ref{thm}}\label{apdx:thm}

Based on the result of Proposition \ref{proposition}, we know that, to synthesize a regret-minimizing strategies on $G$, it suffices to synthesize a strategy that solves the \emph{min-max game} on the information game arena $\hat{G}$ with respect to cost function $\mu:\hat{E}\to\NN$. Furthermore, the correctness of the procedure $\texttt{SolveMinMax}(\hat{G},\mu)$ could be easily validated according to \cite{brihaye2017pseudopolynomial}. The proof is thus completed.

\end{document}
