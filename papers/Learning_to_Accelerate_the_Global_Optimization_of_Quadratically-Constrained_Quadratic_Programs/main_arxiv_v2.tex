\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage{fullpage}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{epsfig}
\usepackage{epstopdf}
\usepackage{mathtools}
\usepackage{relsize}
\usepackage{hyperref}
\hypersetup{
	colorlinks=true, 
	breaklinks=true,
	urlcolor= blue, 
	linkcolor= blue, 
	citecolor=red, 
}
\usepackage[numbers,sort&compress]{natbib}
\usepackage{enumitem}
\usepackage{algorithm,algpseudocode}
\usepackage{setspace}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{xspace}
\usepackage{authblk}
\usepackage{dsfont}
\usepackage{multicol}
\usepackage[export]{adjustbox}
\usepackage{thmtools,thm-restate}


\usepackage{tikz}
\usepackage{pgfplots}
\pgfplotsset{compat = newest}

%\usepgfplotslibrary{external}
%\tikzexternalize

\ifpdf
  \DeclareGraphicsExtensions{.eps,.pdf,.png,.jpg}
\else
  \DeclareGraphicsExtensions{.eps}
\fi



\title{Learning to Accelerate the Global Optimization of \\ Quadratically-Constrained Quadratic Programs}


\date{Version 2 (this document): February 22, 2023 \\[0.1in] \hspace*{-1.18in} Version 1: December 31, 2022}

\author[1,2]{Rohit Kannan}
\author[2]{Harsha Nagarajan}
\author[2]{Deepjyoti Deka}
\affil[1]{Center for Nonlinear Studies (T-CNLS), Los Alamos National Laboratory, Los Alamos, NM, USA.}
\affil[2]{Applied Mathematics \& Plasma Physics (T-5), Los Alamos National Laboratory, Los Alamos, NM, USA. \protect\\ E-mail: \{rohitk@alum.mit.edu, harsha@lanl.gov, deepjyoti@lanl.gov\}}
\renewcommand\Affilfont{\small}
 
 


\newcounter{mycounter}

\renewcommand{\labelenumii}{\alph{enumii}.}
\setlength{\leftmarginii}{4mm}



\newcommand{\pmc}{piecewise McCormick}
\newcommand{\pp}{partitioning points}
 
\newcommand{\Set}[2]{\left\lbrace #1 : #2 \right\rbrace}
\newcommand{\minf}[1]{\min\left\lbrace #1 \right\rbrace}
 
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\argmax}{arg\,max}

\DeclareMathOperator*{\esssup}{ess\,sup}

\DeclarePairedDelimiter\ceil{\lceil}{\rceil}
\DeclarePairedDelimiter\floor{\lfloor}{\rfloor}

\newcommand{\Mod}[1]{\ (\mathrm{mod}\ #1)}


%\numberwithin{equation}{section}

\newcommand{\tr}[1]{\ensuremath{{#1}^\text{T}}}

\newcommand{\uset}[2]{\ensuremath{\underset{#1}{#2}}}

\newcommand{\ml}[1]{\ensuremath{\mathlarger{#1}}}


\DeclarePairedDelimiter\abs{\lvert}{\rvert}%
\DeclarePairedDelimiter\norm{\lVert}{\rVert}%



\newcommand{\Q}{\mathcal{Q}}
\newcommand{\R}{\mathbb{R}}


\providecommand{\keywords}[1]
{
  \small	
  \textbf{Key words:} #1
}



%%%%%%% THEOREMS %%%%%%%

\newtheorem{theorem}{Theorem}[]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{assumption}{Assumption}[]
\newtheorem{remark}{Remark}
\newtheorem{definition}{Definition}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{example}{Example}
\newtheorem{conjecture}[theorem]{Conjecture}

\let\oldtheorem\theorem
\renewcommand{\theorem}{\oldtheorem\normalfont}

\let\oldlemma\lemma
\renewcommand{\lemma}{\oldlemma\normalfont}

\let\oldassumption\assumption
\renewcommand{\assumption}{\oldassumption\normalfont}

\let\oldremark\remark
\renewcommand{\remark}{\oldremark\normalfont}

\let\olddefinition\definition
\renewcommand{\definition}{\olddefinition\normalfont}

\let\oldcorollary\corollary
\renewcommand{\corollary}{\oldcorollary\normalfont}

\let\oldproposition\proposition
\renewcommand{\proposition}{\oldproposition\normalfont}

\let\oldexample\example
\renewcommand{\example}{\oldexample\normalfont}

\let\oldconjecture\conjecture
\renewcommand{\conjecture}{\oldconjecture\normalfont}



\usepackage{listofitems} % for \readlist to create arrays
\usetikzlibrary{arrows.meta} % for arrow size
\usepackage[outline]{contour} % glow around text
\contourlength{1.4pt}

\tikzset{>=latex} % for LaTeX arrow head
\usepackage{xcolor}
\colorlet{myred}{red!80!black}
\colorlet{myblue}{blue!80!black}
\colorlet{mygreen}{green!60!black}
\colorlet{myorange}{orange!70!red!60!black}
\colorlet{mydarkred}{red!30!black}
\colorlet{mydarkblue}{blue!40!black}
\colorlet{mydarkgreen}{green!30!black}
\tikzstyle{node}=[thick,circle,draw=myblue,minimum size=22,inner sep=0.5,outer sep=0.6]
\tikzstyle{node in}=[node,green!20!black,draw=mygreen!30!black,fill=mygreen!25]
\tikzstyle{node hidden}=[node,blue!20!black,draw=myblue!30!black,fill=myblue!20]
\tikzstyle{node convol}=[node,orange!20!black,draw=myorange!30!black,fill=myorange!20]
\tikzstyle{node out}=[node,red!20!black,draw=myred!30!black,fill=myred!20]
\tikzstyle{connect}=[thick,mydarkblue] %,line cap=round
\tikzstyle{connect arrow}=[-{Latex[length=4,width=3.5]},thick,mydarkblue,shorten <=0.5,shorten >=1]
\tikzset{ % node styles, numbered for easy mapping with \nstyle
  node 1/.style={node in},
  node 2/.style={node hidden},
  node 3/.style={node out},
}
\def\nstyle{int(\lay<\Nnodlen?min(2,\lay):3)} % map layer number onto 1, 2, or 3






\setlength{\bibsep}{3pt}



\begin{document}

\maketitle

\begin{abstract}
We learn optimal instance-specific heuristics for the global minimization of nonconvex quadratically-constrained quadratic programs (QCQPs).
Specifically, we consider partitioning-based mixed-integer programming relaxations for nonconvex QCQPs and propose the novel problem of \textit{strong partitioning} to optimally partition variable domains \textit{without} sacrificing global optimality.
We design a local optimization method for solving this challenging max-min strong partitioning problem and replace this expensive benchmark strategy with a machine learning (ML) approximation for homogeneous families of QCQPs.
We present a detailed computational study on randomly generated families of QCQPs, including instances of the pooling problem, using the open-source global solver Alpine.
Our numerical experiments demonstrate that strong partitioning and its ML approximation significantly reduce Alpine's solution time by factors of $3.5 - 16.5$ and $2 - 4.5$ \textit{on average} and by maximum factors of $15 - 700$ and $10 - 200$, respectively, over the different QCQP families.
\\[0.1in]
\keywords{Quadratically-Constrained Quadratic Program, Piecewise McCormick Relaxations, Global Optimization, Machine Learning, Strong Partitioning, Sensitivity Analysis, Pooling Problem}
\end{abstract}










\section{Introduction}


Many real-world applications involve the repeated solution of the same underlying quadratically-constrained quadratic program (QCQP) with slightly varying model parameters.
Examples include the pooling problem with varying input qualities~\cite{misener2013glomiqo} and the cost-efficient operation of the power grid with varying loads and renewable sources~\cite{bienstock2022mathematical}.
These hard optimization problems are typically solved using off-the-shelf global optimization software~\cite{belotti2009branching,bestuzheva2021scip,lin2009global,misener2014antigone,nagarajan2019adaptive,sahinidis1996baron} that do not exploit the shared problem structure---heuristics within these implementations are engineered to work well \textit{on average} over a diverse set of instances and may perform sub-optimally for instances from a specific application~\cite{liu2019tuning}.
Recent work~\cite{bengio2021machine,lodi2017learning} has shown that tailoring branching decisions can significantly accelerate branch-and-bound (B\&B) algorithms for mixed-integer linear programs (MILPs).
In contrast, only a few papers (see Section~\ref{subsec:learning_minlps}) attempt to use machine learning (ML) to accelerate the guaranteed global solution of nonconvex \textit{nonlinear} programs (NLPs).


We use ML to accelerate the global minimization of nonconvex QCQPs.
We focus on accelerating partitioning algorithms~\cite{bergamini2008improved,saif2008global,wicaksono2008piecewise} since they are effective in solving this challenging class of problems~\cite{castro2016normalized,nagarajan2019adaptive}.
Partitioning algorithms determine lower bounds on the optimal value of a nonconvex QCQP using piecewise convex relaxations.
They begin by selecting a subset of the continuous variables participating in nonconvex terms for partitioning.
At each iteration, they refine the partitions of the domains of these variables and update the piecewise convex relaxations in their lower bounding formulation.
They then solve a convex mixed-integer program (MIP)~\cite{lu2018tight,sundar2021piecewise} to determine a lower bound.
Partitioning algorithms typically use heuristics to specify the locations of {\pp} and continue to refine their variable partitions until the lower bounds converge to the optimal objective value.
Since the complexity of their MIP relaxations can grow significantly at each iteration, the choice of {\pp} in the initial iterations can have a huge impact on the overall performance of these algorithms~\cite{nagarajan2019adaptive}.
Despite their importance, the optimal choice of partitioning points is not well understood and current approaches resort to heuristics such as bisecting the active partition~\cite{castro2016normalized,saif2008global,wicaksono2008piecewise}, or adding {\pp} at/around the lower bounding solution~\cite{bergamini2008improved,nagarajan2019adaptive} to refine variable partitions.




\paragraph{Proposed approach.} 
We learn to optimally partition the domains of variables in a given QCQP \textit{without} sacrificing global optimality.
Similar to the concept of \textit{strong branching} in B\&B algorithms for MIPs~\cite{achterberg2007constraint,belotti2009branching}, we propose the novel concept of \textit{strong partitioning} to choose {\pp}.
The key idea of strong partitioning is to determine a specified number of {\pp} per variable such that the resulting piecewise convex relaxation-based lower bound is maximized.
We formulate strong partitioning as a max-min problem, where the outer-maximization chooses the {\pp} and the inner-minimization solves the piecewise relaxation-based lower bounding problem for a given partition.
We solve this \mbox{max-min} problem to local optimality by using generalized gradients of the value function of the inner-minimization problem within a bundle solver for nonsmooth nonconvex optimization.
Because each iteration of the bundle method requires the solution of a MIP, solving this \mbox{max-min} problem may be computationally expensive.
Therefore, we use ML to learn the strong partitioning strategy for homogeneous QCQP instances.
We evaluate the performance of strong partitioning and an off-the-shelf ML approximation on randomly generated QCQPs, including instances of the pooling problem, using the open-source global solver Alpine~\cite{nagarajan2019adaptive,nagarajan2016tightening}. 
Our experiments demonstrate that using strong partitioning to initialize Alpine's variable partitions can reduce its solution time by a factor of $\mathit{3.5 - 16.5}$ on average and by a maximum factor of $\mathit{15 - 700}$.
They also illustrate that our ML model is able to learn the strong partitioning strategy approximately, reducing Alpine's solution time by a factor of $\mathit{2 - 4.5}$ on average and by a maximum factor of $\mathit{10 - 200}$ over the same set of instances.


\paragraph*{}
This paper is organized as follows.
Section~\ref{sec:relatedwork} reviews approaches that use ML to accelerate the guaranteed global solution of MILPs, NLPs, and mixed-integer NLPs (MINLPs).
Section~\ref{sec:partitioning_bounds} outlines partitioning-based lower bounding methods for nonconvex QCQPs.
Section~\ref{sec:strongpart} introduces strong partitioning and designs an algorithm for its solution with theoretical guarantees.
Section~\ref{sec:computexp} presents detailed computational results demonstrating the effectiveness of using strong partitioning and an off-the-shelf ML approximation within Alpine for randomly generated QCQPs, including instances of the pooling problem.
We conclude with directions for future research in Section~\ref{sec:conclusion}.


\paragraph{Notation.}
Let $[n] := \{1,2,\dots,n\}$ and $\mathbb{R}_+$ denote the set of non-negative reals.
We write $v_i$ to denote the $i$th component of vector $v = (v_1,v_2,\dots,v_n) \in \mathbb{R}^n$, $M_{ij}$ to denote the $(i,j)$th component of matrix $M$, and $\abs{S}$, $\text{int}(S)$, and $\text{conv}(S)$ to denote the cardinality, interior, and convex hull of a set~$S$, respectively.



\section{Related work}
\label{sec:relatedwork}


Optimization solvers tune key algorithmic parameters by extensive testing on benchmark libraries.
However, they typically only consider a narrow family of efficiently computable heuristics.
Moreover, they do not tailor these heuristics to each problem instance but seek a universal setting for good \textit{average} solver performance.
Machine learning, on the other hand, can enable efficient approximations of better performing but expensive heuristics, potentially leading to significant computational gains for hard test instances.
Several recent papers survey the burgeoning field of using ML to accelerate MILP and combinatorial optimization algorithms~\cite{bengio2021machine,cappart2021combinatorial,kotary2021end,lodi2017learning}.
In the next sections, we review related approaches on learning to branch for MILPs and learning to accelerate the guaranteed solution of (MI)NLPs.






\subsection{Learning to branch for MILPs}

Branch-and-bound and its variants form the backbone of modern MILP solvers.
Besides heuristics for determining good feasible solutions, selecting the branching variable at each node of the B\&B tree is probably the most impactful decision in terms of the run time of the algorithm~\cite{achterberg2007constraint}.
Typically, a subset of the integer variables with fractional lower bounding solutions at a particular node are considered as the candidate branching variables for that node.
The gold standard branching variable selection heuristic is {\it full strong branching} (FSB), which chooses as the branching variable the one that leads to the maximum product of improvements in the lower bounds of the two children nodes (assuming they are both feasible).
FSB results in a $65\%$ reduction in the number of nodes explored by the B\&B tree (relative to the default branching strategy) on average over standard test instances; however, this comes with a $44\%$ increase in the cost-per-node that is not tenable~\cite{achterberg2007constraint}.
MILP solvers therefore tend to use computationally cheaper heuristic approximations of FSB such as reliability, pseudocost, or hybrid branching.
Motivated by FSB's promise, most approaches for learning to branch for MILPs aim to develop a computationally efficient ML approximation that retains its attractive performance.


Alvarez et al.\ \cite{alvarez2017machine} propose hand-crafted features to construct an ML approximation of FSB using Extremely Randomized Trees.
Khalil et al.\ \cite{khalil2016learning} propose an instance-specific on-the-fly ML approximation of FSB using Support Vector Machines (SVMs) and use the learned approximation for subsequent branching decisions in the same problem instance.
Gasse et al.\ \cite{gasse2019exact} and Nair et al.\ \cite{nair2020solving} design graph neural network (GNN) approximations of FSB that leverages the bipartite graph representation of MILPs and removes the need for feature engineering.
Zarpellon et al.\ \cite{zarpellon2021parameterizing} seek to learn branching policies that generalize to heterogeneous MILPs by explicitly parametrizing the state of the B\&B tree.
A few works (see, e.g.,~\cite{etheve2020reinforcement,he2014learning}) study online and reinforcement learning approaches for making branching decisions.
Some others~\cite{balcan2018learning,di2016dash} learn combinations of existing heuristics to come up with better branching decisions.




\subsection{Learning for (MI)NLPs}
\label{subsec:learning_minlps}


There are relatively fewer approaches in the literature for accelerating the \textit{guaranteed global solution} of nonconvex NLPs and MINLPs using ML. 
To the best of our knowledge, none of these approaches use ML to accelerate partitioning-based global optimization algorithms.


Baltean-Lugojan et al.\ \cite{baltean2019scoring} consider the global solution of nonconvex QCQPs using semi-definite programming (SDP) relaxations.
They use ML to construct good outer-approximations of these SDP relaxations to mitigate the computational burden of SDP solvers.
They train a neural network to select cuts based on their sparsity and predicted impact on the objective, 
and show that their approach results in computationally cheap relaxations that can be effectively integrated into global solvers.

Ghaddar et al.\ \cite{ghaddar2022learning} consider branching variable selection for a B\&B search tree that is embedded within the reformulation-linearization technique (RLT) for solving polynomial problems.
They use ML to choose the ``best branching strategy'' from a portfolio of branching rules (cf.\ Di Liberto et al.\ \cite{di2016dash}) based on violations of RLT-defining identities.
They design several hand-crafted features and pick the branching strategy that optimizes a quantile regression forest-based approximation of their performance indicator.
Gonz{\'a}lez-Rodr{\'\i}guez et al.\ \cite{gonzalez2022polynomial} consider a portfolio of second-order cone and SDP constraints to strengthen the RLT formulation for polynomial problems and use ML to select constraints to add within a B\&B framework.


Bonami et al.\ \cite{bonami2018learning} train classifiers to predict whether linearizing products of binary variables or binary and bounded continuous variables may be computationally advantageous for solving MIQPs.
Nannicini et al.\ \cite{nannicini2011probing} train an SVM classifier to decide if an expensive optimality-based bounds tightening (OBBT) routine should be used in lieu of a cheaper feasibility-based routine for nonconvex MINLPs.
Cengil et al.\ \cite{cengil2022learning} consider the AC optimal power flow  problem and train a deep neural network to identify a small subset of lines and buses for which a reduced-cost OBBT routine is applied.
Finally, Lee et al.\ \cite{lee2020accelerating} use classification and regression techniques to identify effective cuts for the generalized Benders decomposition master problem. 


We review partitioning algorithms for the global minimization of QCQPs in the next section before outlining our proposal to accelerate them using ML.








\section{Partitioning-based bounds for QCQPs}
\label{sec:partitioning_bounds}


Consider the nonconvex QCQP
\begin{alignat}{2}
\label{eqn:orig_qcqp}
&\min_{x \in [0,1]^{n}} \:\: && x^{\textup{T}} Q^0 x + (r^0)^{\textup{T}} x \\
&\quad\: \text{s.t.} && x^{\textup{T}} Q^i x + (r^i)^{\textup{T}} x \leq b_i, \quad \forall i \in [m_I], \nonumber
\end{alignat}
where $b \in \R^{m_I}$ and for each $k \in \{0\} \cup [m_I]$, $r^k \in \R^n$ and $Q^k \in \R^{n \times n}$ are symmetric but not necessarily positive semi-definite.
QCQPs with equality constraints and different variable bounds can be handled using simple transformations.
Polynomial optimization problems may also be reformulated as QCQPs through the addition of variables and constraints.
It is well known that nonconvex QCQPs are NP-hard~\cite{vavasis1990quadratic}. 

QCQPs arise in several applications (see~\cite{misener2013glomiqo} for a detailed list) such as facility location~\cite{koopmans1957assignment}, refinery optimization~\cite{kannan2018algorithms,yang2016integrated}, electric grid optimization \cite{bienstock2022mathematical}, and circle packing~\cite{costa2013impact}.
By introducing variables and constraints, we can reformulate~\eqref{eqn:orig_qcqp} into the following equivalent form:
\begin{alignat}{2}
\label{eqn:qcqp}
v^* := \: &\min_{x \in [0,1]^n, w} \:\: && c^{\textup{T}} x + d^{\textup{T}} w \tag{QCQP} \\
&\quad\:\:\: \text{s.t.} && A x + B w \leq b, \nonumber\\
& && w_{ij} = x_i x_j, \quad \forall (i, j) \in \mathcal{B}, \nonumber\\
& && w_{kk} = x^2_k, \quad \forall k \in \mathcal{Q}, \nonumber
\end{alignat}
for some vectors $c$ and $d$, matrices $A$ and $B$, and index sets $\mathcal{B} \subset \{(i,j) \in [n]^2 : i \neq j \}$ and $\mathcal{Q} \subset [n]$ of (pairs of) variables participating in bilinear and univariate quadratic terms.
We assume~\eqref{eqn:qcqp} is feasible for simplicity, and define the set $F := \{(x,w) : x \in [0,1]^n, \: Ax + Bw \leq b\}$ for convenience.


One of the earliest approaches for constructing lower bounds on the optimal value of~\eqref{eqn:qcqp} uses termwise McCormick relaxations~\cite{al1983jointly,mccormick1976computability} to yield the following lower bounding problem:
\begin{alignat}{2}
\label{eqn:mccormick}
&\min_{(x,w) \in F} \:\: && c^{\textup{T}} x + d^{\textup{T}} w \\
&\quad\: \text{s.t.} && (x_i, x_j, w_{ij}) \in \mathcal{M}^{B}_{ij}, \quad \forall (i, j) \in \mathcal{B}, \nonumber\\
& && (x_k, w_{kk}) \in \mathcal{M}^{Q}_k, \quad \forall k \in \mathcal{Q}, \nonumber
\end{alignat}
where the bilinear and quadratic equality constraints in~\eqref{eqn:qcqp} have been replaced with the following valid convex relaxations on $x \in [0,1]^n$ for each $(i, j) \in \mathcal{B}$ and $k \in \mathcal{Q}$:
\begin{align*}
\mathcal{M}^{B}_{ij} &:= \{ (x_i, x_j, w_{ij}) \: : \: 0 \leq w_{ij} \leq x_i, \: x_i + x_j -1 \leq w_{ij} \leq x_j \}, \\
\mathcal{M}^{Q}_{k} &:= \{ (x_k, w_{kk}) \: : \: x^2_k \leq w_{kk} \leq x_k \}.
\end{align*}
Problem~\eqref{eqn:mccormick} can be used within a spatial B\&B framework for solving~\eqref{eqn:qcqp} to global optimality.
Several papers (see, e.g.,~\cite{bao2011semidefinite,bergamini2008improved,billionnet2012extending,burer2008finite,nohra2021spectral,saif2008global,sherali2013reformulation,wicaksono2008piecewise}) improve upon the termwise McCormick bound, usually at an increase in the computational cost but with the goal of reducing the overall time for the B\&B algorithm to converge.
In this work, we consider the so-called {\pmc} relaxation approach~\cite{bergamini2005logic,bergamini2008improved,castro2016normalized,karuppiah2006global,kolodziej2013global,meyer2006global,nagarajan2019adaptive,saif2008global,wicaksono2008piecewise} for strengthening the termwise McCormick relaxations.



\input{figures/tikz_bilinear_quadratic_arxiv.tex}



Piecewise McCormick relaxations begin by partitioning the domains of variables participating in nonconvex terms into sub-intervals.
Assume for simplicity that we wish to partition the domain of each $x_i$ into $d+1$ sub-intervals with $d \geq 1$ (general partitioning schemes can be handled similarly).
For each $i \in [n]$, let 
\[
\mathcal{P}_i := [p^i_0, p^i_1, p^i_2,\dots,p^i_d, p^i_{d+1}], \quad \text{with} \quad 0 =: p^i_0 \leq p^i_1 \leq p^i_2 \leq \dots \leq p^i_d \leq p^i_{d+1} := 1,
\]
denote the array of $d+2$ {\pp} for variable $x_i$, including the original variable bounds $0$ and $1$.
Given partitions $\mathcal{P}_i$, $i \in [n]$, the {\pmc} relaxation-based lower bounding problem to~\eqref{eqn:qcqp} can be abstractly written as
\begin{alignat}{2}
\label{eqn:piecewise_mccormick}
&\min_{(x,w) \in F} \:\: && c^{\textup{T}} x + d^{\textup{T}} w \\
&\quad\: \text{s.t.} && (x_i, x_j, w_{ij}) \in \mathcal{PMR}^{B}_{ij}(\mathcal{P}_i,\mathcal{P}_j), \quad \forall (i, j) \in \mathcal{B}, \nonumber\\
& && (x_k, w_{kk}) \in \mathcal{PMR}^{Q}_k(\mathcal{P}_k), \quad \forall k \in \mathcal{Q}, \nonumber
\end{alignat}
where $\mathcal{PMR}^B_{ij}(\mathcal{P}_i, \mathcal{P}_j)$ and $\mathcal{PMR}^{Q}_{k}(\mathcal{P}_k)$
denote the feasible regions corresponding to the {\pmc} relaxations of the bilinear equation $w_{ij} = x_i x_j$ and the quadratic equation $w_k = x^2_k$, respectively.
While there are several ways of formulating these {\pmc} relaxations, we use the so-called ``convex combination'' or ``lambda'' formulation below (see~\cite{kim2022piecewise} for enhancements in the multilinear setting).
The piecewise \mbox{McCormick} relaxation for the bilinear constraint $w_{ij} = x_i x_j$ can be represented as follows~\cite{sundar2021piecewise}:
\begin{align*}
\mathcal{PMR}^B_{ij}(\mathcal{P}_i, \mathcal{P}_j) &:= \Big\{ (x_i, x_j, w_{ij}) \: : \: \exists \lambda^{ij} \in \R^{(d+2)^2}_+, \: y^{i} \in \{0,1\}^{(d+1)}, \: y^{j} \in \{0,1\}^{(d+1)} \\
&\hspace*{1.8in} \text{s.t.} \:\: (x_i, x_j, w_{ij}, \lambda^{ij}, y^{i}, y^{j}) \:\: \text{satisfies} \:\: \eqref{eqn:lambda_form_bilinear_conv_comb}-\eqref{eqn:lambda_form_bilinear_active2} \Big\},
\end{align*}
where
\begin{subequations}
\begin{alignat}{3}
&x_i = \sum_{k=0}^{d+1} \sum_{l=0}^{d+1} \lambda^{ij}_{k(d+2)+l+1} p^i_l, \quad\:\:\:  && x_j = \sum_{k=0}^{d+1} \sum_{l=0}^{d+1} \lambda^{ij}_{k(d+2)+l+1} p^j_k, \quad\:\:\:  && w_{ij} = \sum_{k=0}^{d+1} \sum_{l=0}^{d+1} \lambda^{ij}_{k(d+2)+l+1} p^i_l p^j_k, \label{eqn:lambda_form_bilinear_conv_comb}\\
&\sum_{k=1}^{d+1} y^{i}_k = 1,   &&\sum_{k=1}^{d+1} y^{j}_k = 1,   &&\sum_{k=1}^{(d+2)^2} \lambda^{ij}_k = 1, \label{eqn:lambda_form_bilinear_bin} \\
&\sum_{k=0}^{d+1} \lambda^{ij}_{k(d+2) + 1} \leq y^i_1,  && \sum_{k=1}^{d+2} \lambda^{ij}_{k(d+2)} \leq y^i_{d+1},  && \sum_{k=0}^{d+1} \lambda^{ij}_{k(d+2) + l+1} \leq y^i_l + y^i_{l+1}, \:\: \forall l \in [d], \label{eqn:lambda_form_bilinear_active} \\
&\sum_{k=1}^{d+2} \lambda^{ij}_{k} \leq y^j_1,  && \sum_{k=1}^{d+2} \lambda^{ij}_{(d+2)^2 - k} \leq y^j_{d+1},  && \sum_{k=1}^{d+2} \lambda^{ij}_{l(d+2) + k} \leq y^j_l + y^j_{l+1}, \:\: \forall l \in [d]. \label{eqn:lambda_form_bilinear_active2}
\end{alignat}
\end{subequations}
Note that only equations~\eqref{eqn:lambda_form_bilinear_conv_comb} depend on the partitions $\mathcal{P}_i$ and $\mathcal{P}_j$ of $x_i$ and $x_j$, respectively, which are parameters in these constraints. The binary vectors $y^i$ and $y^j$ denote the active partition of $x_i$ and $x_j$---these variables may be reused in the {\pmc} relaxations of other nonconvex terms involving $x_i$ or~$x_j$. 


The {\pmc} relaxation for the quadratic equality constraint $w_{kk} = x^2_k$ can be represented as follows~\cite{lu2018tight,nagarajan2019adaptive}:
\begin{align*}
\mathcal{PMR}^Q_{k}(\mathcal{P}_k) &:= \Big\{ (x_k, w_{kk}) \: : \: \exists \lambda^{k} \in \R^{(d+2)}_+, \: y^{k} \in \{0,1\}^{(d+1)}, \:\: \text{s.t.} \:\: (x_k, w_{kk}, \lambda^{k}, y^{k}) \:\: \text{satisfies} \:\: \eqref{eqn:lambda_form_quadratic_conv_comb}-\eqref{eqn:lambda_form_quadratic_active} \Big\},
\end{align*}
where
\begin{subequations}
\begin{alignat}{3}
&x_k = \sum_{l=0}^{d+1} \lambda^{k}_{l+1} p^k_l, \qquad\qquad  && w_{kk} \leq \sum_{l=0}^{d+1} \lambda^{k}_{l+1} (p^k_l)^2, \qquad\qquad && \sum_{l=1}^{d+1} y^k_l p^k_{l-1} \leq x_k \leq \sum_{l=2}^{d+2} y^k_{l-1} p^k_{l-1}, \label{eqn:lambda_form_quadratic_conv_comb}\\
&\sum_{l=1}^{d+1} y^{k}_l = 1,   &&\sum_{l=1}^{d+2} \lambda^{k}_l = 1, && w_{kk} \geq x^2_k, \label{eqn:lambda_form_quadratic_bin} \\
&\lambda^{k}_{1} \leq y^k_1,  && \lambda^{k}_{d+2} \leq y^k_{d+1},  &&\lambda^{k}_{l+1} \leq y^k_l + y^k_{l+1}, \:\: \forall l \in [d]. \label{eqn:lambda_form_quadratic_active}
\end{alignat}
\end{subequations}
Note that only equations~\eqref{eqn:lambda_form_quadratic_conv_comb} depend on the partition $\mathcal{P}_k$ of $x_k$. 
Additionally, equations~\eqref{eqn:lambda_form_quadratic_bin} involve convex quadratic functions of $x_k$. 
Figure~\ref{fig:piecewise_mccormick} illustrates the {\pmc} relaxations for a bilinear and a univariate quadratic term.

Using the above representations of $\mathcal{PMR}^B_{ij}$ and $\mathcal{PMR}^Q_{k}$, we obtain the following extended convex MIP formulation for the {\pmc} relaxation-based lower bounding problem to~\eqref{eqn:qcqp}:
\begin{alignat}{2}
\label{eqn:piecewise_mccormick_mip}
&\min_{(x,w) \in F, \:\lambda \geq 0, \: y \in Y} && c^{\textup{T}} x + d^{\textup{T}} w \tag{PMR}\\
&\qquad\quad\: \text{s.t.} && (x_i, x_j, w_{ij}, \lambda^{ij}, y^i, y^j) \:\: \text{satisfies} \:\: \eqref{eqn:lambda_form_bilinear_conv_comb}-\eqref{eqn:lambda_form_bilinear_active2}, \:\: \forall (i,j) \in \mathcal{B}, \nonumber\\
& && (x_k, w_{kk}, \lambda^k, y^k) \:\: \text{satisfies} \:\: \eqref{eqn:lambda_form_quadratic_conv_comb}-\eqref{eqn:lambda_form_quadratic_active}, \:\: \forall k \in \mathcal{Q}, \nonumber
\end{alignat}
where $Y := \{y \in \{0,1\}^{\abs{\mathcal{NC}} \times (d+1)} : \sum_{l=1}^{d+1} y^{i}_l = 1, \: \forall i \in \mathcal{NC} \}$ is a special-ordered set of type~1, the set $\mathcal{NC} := \{i \: : \: \exists j \in [n] \:\: \text{s.t.} \:\: (i,j) \in \mathcal{B}\} \cup \mathcal{Q}$ collects the indices of variables participating in nonconvex terms, variables $\lambda$ comprise $\lambda^{ij}$, $(i,j) \in \mathcal{B}$, and $\lambda^k$, $k \in \mathcal{Q}$, and variables $y$ comprise $y^i$, $i \in \mathcal{NC}$.
Problem~\eqref{eqn:piecewise_mccormick_mip} is a convex mixed-integer QCQP (MIQCQP).









Algorithm~\ref{alg:partitioning_algorithm} outlines a partitioning-based global optimization algorithm that solves problem~\eqref{eqn:piecewise_mccormick_mip} to determine a sequence of lower bounds on the optimal value of~\eqref{eqn:qcqp}.
Note that we do not consider bound tightening steps in this work, but our approach can be readily adapted to the setting where bounds tightening is employed.
As mentioned in the introduction, the choice of heuristics on line~\ref{step:refine_part} of Algorithm~\ref{alg:partitioning_algorithm} for refining the partitions $\{\mathcal{P}_i\}_{i \in [n]}$ can greatly impact the number of iterations and time for convergence.
We detail an adaptive partition refinement strategy proposed in the literature~\cite{nagarajan2019adaptive} in Section~\ref{sec:alpinepart}, and introduce the concept of strong partitioning in Section~\ref{sec:strongpart} to optimally specify variable partitions.









\begin{algorithm}[t]
\caption{Partitioning algorithm for the global optimization of~\eqref{eqn:qcqp}}
\label{alg:partitioning_algorithm}
{
\begin{algorithmic}[1]

\State \textbf{Input}: relative optimality tolerance $\varepsilon_r > 0$

\State \textbf{Initialization}: partitions $\mathcal{P}^0_i = [0, 1]$, $i \in [n]$, best found solution $\{\hat{x}\} = \emptyset$ with objective $UBD = +\infty$, lower bound $LBD = -\infty$, iteration number $l = 0$

\vspace*{0.05in}
\State Solve~\eqref{eqn:qcqp} locally. Update incumbent $\hat{x}$ and upper bound $UBD$ if relevant

\vspace*{0.05in}
\State Solve the termwise McCormick relaxation~\eqref{eqn:mccormick}. Update lower bound $LBD$


\vspace*{0.05in}
\While{$\dfrac{UBD - LBD}{\abs{UBD} + 10^{-6}} > \varepsilon_r$ \textbf{or} $UBD = +\infty$}

\State Update $l \leftarrow l+1$


\State\label{step:refine_part}Refine partitions $\{\mathcal{P}^{l-1}_i\}_{i \in [n]}$ to $\{\mathcal{P}^{l}_i\}_{i \in [n]}$ by adding partitioning points

\State Solve piecewise McCormick relaxation~\eqref{eqn:piecewise_mccormick_mip} with partitions $\{\mathcal{P}^{l}_i\}_{i \in [n]}$. Update lower bound $LBD$
% \Statex \hspace*{0.25in} Update lower bound $LBD$

\State Solve~\eqref{eqn:qcqp} locally. Update incumbent $\hat{x}$ and upper bound $UBD$ if needed 

\EndWhile


\vspace*{0.05in}
\State Return $\varepsilon_r$-optimal solution $\hat{x}$, upper bound $UBD$, and lower bound $LBD$


\end{algorithmic}
}
\end{algorithm}


We further relax~\eqref{eqn:piecewise_mccormick_mip} by outer-approximating the convex quadratic terms in equation~\eqref{eqn:lambda_form_quadratic_bin} to obtain the following MILP relaxation for strong partitioning.
Note that for purely bilinear problems (i.e., $\abs{\Q} = 0$), problem~\eqref{eqn:piecewise_mccormick_mip} is already an MILP.
{
% \small
\begin{alignat}{2}
\label{eqn:piecewise_mccormick_mip_oa}
\underline{v}(P) := &\min_{(x,w) \in F, \:\lambda \geq 0, \: y \in Y} && c^{\textup{T}} x + d^{\textup{T}} w \tag{PMR-OA} \\
&\qquad\quad\: \text{s.t.} && (x_i, x_j, w_{ij}, \lambda^{ij}, y^i, y^j) \:\: \text{satisfies} \:\: \eqref{eqn:lambda_form_bilinear_conv_comb}-\eqref{eqn:lambda_form_bilinear_active2}, \:\: \forall (i,j) \in \mathcal{B}, \nonumber\\
& && (x_k, w_{kk}, \lambda^k, y^k) \:\: \text{satisfies} \:\: \eqref{eqn:lambda_form_quadratic_conv_comb} \text{ and } \eqref{eqn:lambda_form_quadratic_active}, \:\: \forall k \in \mathcal{Q}, \nonumber \\
& && \sum_{l=1}^{d+2} \lambda^{k}_l = 1, \quad w_{kk} \geq 2 \alpha^k_j x_k - (\alpha^k_j)^2, \:\: \forall j \in \mathcal{J}_k, \:\: k \in \mathcal{Q}. \label{eqn:piecewise_mccormick_mip_oa_outer}
\end{alignat}%
}%
We explicitly indicate the dependence of the {\pmc} lower bound $\underline{v}$ on the $d \times n$ matrix of {\pp} $P := (p^1,p^2,\dots,p^n)$, excluding points $p^i_0 := 0$ and $p^i_{d+1} := 1$.
Constraints~\eqref{eqn:piecewise_mccormick_mip_oa_outer} outer-approximate the inequalities $w_{kk} \geq x^2_k$ in equation~\eqref{eqn:lambda_form_quadratic_bin} at the points $\{\alpha^k_j\}_{j \in \mathcal{J}_k} \subset [0,1]$ (we assume $\{p^k_0,p^k_1,\dots,p^k_{d+1}\} \subset \{\alpha^k_j\}$).
\textit{We only use the outer-approximation} \eqref{eqn:piecewise_mccormick_mip_oa} \textit{for strong partitioning} and revert to solving problem~\eqref{eqn:piecewise_mccormick_mip} while computing lower bounds (see Algorithm~\ref{alg:partitioning_algorithm}).




In preparation for Section~\ref{sec:strongpart}, we recast~\eqref{eqn:piecewise_mccormick_mip_oa} in the following abstract form for suitably defined vectors $\bar{b}$ and $\bar{c}$, matrix $\bar{B}$, matrix-valued function $M$ with co-domain $\mathbb{R}^{n_r \times n_c}$,
and variables $z$ (that includes $x$, $w$, $\lambda$, and slack variables):

\noindent
\begin{minipage}{0.2\linewidth}
\vspace*{0.5in}
\end{minipage}%
\hspace*{0.75in}
\begin{minipage}{0.3\linewidth}
\vspace*{-0.2in}
\begin{align}
\label{eqn:piecewise_mccormick_mip_abstract}
\underline{v}(P) &:= \min_{y \in Y} \: v(P,y),
\end{align}
\end{minipage}%
\hspace*{0.1in}
\begin{minipage}{0.45\linewidth}
\begin{alignat}{2}
\label{eqn:piecewise_mccormick_mip_abstract_inner}
v(P,y) &:= \min_{z \geq 0} \:\: && \bar{c}^{\textup{T}} z \\
&\quad\quad \text{s.t.} \:\: && M(P,y) z = \bar{B} y + \bar{b}. \nonumber
\end{alignat}
\end{minipage}

\vspace*{0.1in}
\noindent We omit in problem~\eqref{eqn:piecewise_mccormick_mip_abstract_inner} the third set of constraints in equation~\eqref{eqn:lambda_form_quadratic_conv_comb} for simplicity because they only strengthen the LP relaxation of~\eqref{eqn:piecewise_mccormick_mip_oa} and are redundant for the {\pmc} relaxations $\mathcal{PMR}^Q_k$.
Note that for any $y \in Y$, at most four of the $\lambda^{ij}$ variables in the formulation of each $\mathcal{PMR}^B_{ij}$ and at most two of the $\lambda^k$ variables in the formulation of each $\mathcal{PMR}^Q_k$ may be nonzero.
Consequently, for each $y \in Y$, we eliminate the variables and equations corresponding to the $\lambda$ variables that are fixed to zero and let $M(P,y)$ denote the coefficient matrix of the remaining equations (the coefficients of the matrix $M(P,y)$ themselves \textit{do not} depend on $y$).








\begin{algorithm}[t]
\caption{Adaptive partition refinement strategy at iteration $l$}
\label{alg:adaptive_partitioning}
{
\begin{algorithmic}[1]

\State \textbf{Input}: parameter $\Delta \geq 4$ and $\forall i \in [n]$: partition $\mathcal{P}^{l-1}_i := [\hat{p}^i_0, \hat{p}^i_1, \dots, \hat{p}^i_{m_{l-1,i}}]$, index $\mathcal{A}(i,l-1) \in [m_{l-1,i}]$ of the active partition at iteration $l-1$, and reference point $\bar{x}^{l-1}_i$ in the active partition, i.e., the point $\bar{x}^{l-1}_i \in [\hat{p}^i_{\mathcal{A}(i,l-1)-1}, \hat{p}^i_{\mathcal{A}(i,l-1)}]$

\State \textbf{Initialization}: partition $\mathcal{P}^l_i = \mathcal{P}^{l-1}_i$, $i \in [n]$


\vspace*{0.05in}
\For{$i \in \mathcal{NC}$}


\State Refine $\mathcal{P}^{l-1}_i$ to $\mathcal{P}^l_i$ as follows:
\begin{align*}
\mathcal{P}^l_i &= \biggl[ \hat{p}^i_0, \dots, \hat{p}^i_{\mathcal{A}(i,l-1)-1}, \: \max\biggl\{\hat{p}^i_{\mathcal{A}(i,l-1)-1}, \: \bar{x}^{l-1}_i - \frac{\text{width}(\mathcal{A}(i,l-1))}{\Delta}\biggr\}, \\ 
&\qquad\qquad\qquad\qquad\qquad \min\biggl\{\hat{p}^i_{\mathcal{A}(i,l-1)}, \: \bar{x}^{l-1}_i + \frac{\text{width}(\mathcal{A}(i,l-1))}{\Delta}\biggr\}, \: \hat{p}^i_{\mathcal{A}(i,l-1)}, \dots, \hat{p}^i_{m_{l-1,i}} \biggr],
\end{align*}

\Statex \hspace*{0.18in} where $\text{width}(\mathcal{A}(i,l-1)) := \hat{p}^i_{\mathcal{A}(i,l-1)} - \hat{p}^i_{\mathcal{A}(i,l-1)-1}$

\EndFor


\vspace*{0.05in}
\State \textbf{Output}: refined partitions $\{\mathcal{P}^l_i\}_{i \in [n]}$


\end{algorithmic}
}
\end{algorithm}





\subsection{Adaptive partitioning strategy}
\label{sec:alpinepart}


Algorithm~\ref{alg:adaptive_partitioning} outlines the adaptive partitioning strategy proposed in Nagarajan et al.\ \cite{nagarajan2019adaptive} that is implemented within the solver Alpine.
This adaptive strategy empirically performs well on numerous test instances~\cite{nagarajan2019adaptive} and is motivated by the fact that uniformly partitioning variable domains (proposed, e.g., in~\cite{castro2016normalized,saif2008global,wicaksono2008piecewise}) creates many partitions that do not contribute significantly to improving the piecewise McCormick lower bound. 
Instead of selecting a subset of the variables participating in nonconvex terms for partitioning as in~\cite{nagarajan2019adaptive}, Algorithm~\ref{alg:adaptive_partitioning} partitions the domains of all variables in $\mathcal{NC}$.
This is because partitioning only a subset of variables in $\mathcal{NC}$ may result in an analogue of the so-called cluster problem in reduced-space global optimization~\cite{kannan2017cluster,kannan2018convergence}, potentially resulting in a significant increase in the number of iterations for convergence.


At each iteration $l \in \mathbb{N}$, Algorithm~\ref{alg:adaptive_partitioning} adds (up to) two {\pp} per variable around a reference point $\bar{x}^{l-1}$.
At the first iteration, $\bar{x}^0$ is either set to the feasible local solution from presolve if one is found, or to a solution to the termwise McCormick relaxation~\eqref{eqn:mccormick} otherwise.
At subsequent iterations $l > 1$, $\bar{x}^{l-1}$ is specified as the $x$-component of a solution to the {\pmc} relaxation lower bounding problem~\eqref{eqn:piecewise_mccormick_mip} at iteration $l-1$.
Relative to the solution $\bar{x}^{l-1}$, the $j$th partition $[\hat{p}^i_{j-1}, \hat{p}^i_j]$ of variable $x_i$ is said to be active at iteration $l-1$ (i.e., $\mathcal{A}(i,l-1) = j$) if $\hat{p}^i_{j-1} \leq \bar{x}^{l-1}_i \leq \hat{p}^i_j$, or, equivalently, if there exists an optimal solution to~\eqref{eqn:piecewise_mccormick_mip} such that $y^i_j = 1$.
The parameter~$\Delta$ (default value $= 10$) in Algorithm~\ref{alg:adaptive_partitioning} is a dimensionless scaling factor for the size of the partition constructed around the reference point~$\bar{x}^{l-1}$.
Larger values of $\Delta$ result in a narrower partition around $\bar{x}^{l-1}$.



In the next section, we propose strong partitioning to specify the partitions $\{\mathcal{P}^1_i\}_{i \in [n]}$ in the first iteration instead of using the heuristic in Algorithm~\ref{alg:adaptive_partitioning}.
If $UBD$ and the lower bound $LBD$ obtained using strong partitioning are not converged, we revert to using the adaptive partitioning strategy in Algorithm~\ref{alg:adaptive_partitioning} to specify the partitions $\{\mathcal{P}^l_i\}_{i \in [n]}$ in the subsequent iterations $l \geq 2$ of our partitioning algorithm.





\section{Strong partitioning for nonconvex QCQPs}
\label{sec:strongpart}


The choice of partitioning points in the initial iterations can greatly impact the strength of lower bounds, number of iterations for convergence, and overall solution time.
While there are some motivations for the adaptive partitioning strategy in Section~\ref{sec:alpinepart}, it is still ad hoc for a few reasons: it uses the same parameter $\Delta$ to partition the domains of \textit{all} variables, and it only considers symmetric partitions around the reference point $\bar{x}^{l-1}$.
The quality of the partitions $\{\mathcal{P}^1_i\}_{i \in [n]}$ in the first iteration also depend on the quality of the feasible solution determined during presolve, with sub-optimal presolve solutions potentially leading to sub-optimal initial partitions and slow convergence overall.
We propose \textit{strong partitioning} (SP) to address the above limitations of Algorithm~\ref{alg:adaptive_partitioning}.



The concept of strong partitioning is akin to strong branching in B\&B algorithms for MI(N)LPs. 
Strong branching for MILPs only chooses the branching variable (a discrete choice) at a node to maximize some function of the lower bound improvements at its two children nodes. 
Strong partitioning, on the other hand, chooses {\pp} for each partitioned variable (continuous choices within the variable domains) such that the resulting {\pmc} relaxation lower bound is maximized.
It can be formulated as the following max-min problem:
\begin{align}
\label{eqn:strong_part}
P^* &\in \argmax_{P \in \mathfrak{P}} \underline{v}(P), \tag{SP}
\end{align}
where $\underline{v}(P)$ is the value function of~\eqref{eqn:piecewise_mccormick_mip_oa} and the set $\mathfrak{P}$ is defined as
\[
\mathfrak{P} := \bigl\{ P := (p^1,p^2,\dots,p^n) \in \R^{d \times n}_+ \: : \: 0 \leq p^i_1 \leq p^i_2 \leq \dots \leq p^i_d \leq 1, \: \forall i \in [n] \bigr\}.
\]
The strong partitioning problem~\eqref{eqn:strong_part} is challenging to solve even to local optimality because the inner-minimization problem~\eqref{eqn:piecewise_mccormick_mip_oa} includes binary decisions and its feasible region depends on~$P$ (variables of the outer-maximization).
While~\eqref{eqn:strong_part} can be formulated as a generalized semi-infinite program~\cite{vazquez2008generalized}, state-of-the-art global optimization algorithms for this problem class do not scale even for moderate problem dimensions.
Therefore, we design a local optimization method for~\eqref{eqn:strong_part} with the hope of determining {\pp} $\bar{P} \in \mathfrak{P}$ that yield a tight lower bound $\underline{v}(\bar{P})$.
We use the local solution of~\eqref{eqn:strong_part} to specify the initial partitions $\{\mathcal{P}^1_i\}_{i \in [n]}$ within Algorithm~\ref{alg:partitioning_algorithm}.
If the resulting bounds $LBD$ and $UBD$ are not converged after one iteration of Algorithm~\ref{alg:partitioning_algorithm}, we use Algorithm~\ref{alg:adaptive_partitioning} to specify the partitions $\{\mathcal{P}^l_i\}_{i \in [n]}$ from the second iteration.


We use generalized gradients of the value function of the inner-minimization \eqref{eqn:piecewise_mccormick_mip_oa} within a bundle solver for nonsmooth nonconvex optimization to solve problem~\eqref{eqn:strong_part} to local optimality.
Although the value function of an MILP might be discontinuous in general,~\eqref{eqn:piecewise_mccormick_mip_oa} possesses special structure because (outer-approximations of) {\pmc} relaxations are nonconvex piecewise-linear continuous functions (cf.\ Figure~\ref{fig:piecewise_mccormick}). This allows for the computation of sensitivity information in this setting. The bundle solver, MPBNGC~\cite{makela2003multiobjective}, that we use, requires function and generalized gradient evaluations at points $P \in \mathfrak{P}$ during the course of its algorithm.
Each function evaluation $\underline{v}(P)$ requires the solution of the MILP~\eqref{eqn:piecewise_mccormick_mip_oa}.
Under suitable assumptions, a generalized gradient $\partial_P \underline{v}(P)$ can be obtained by fixing~$y$ to an optimal $y$-solution of~\eqref{eqn:piecewise_mccormick_mip_oa} and computing a generalized gradient of the resulting LP~\eqref{eqn:piecewise_mccormick_mip_abstract_inner} using parametric sensitivity theory~\cite{de2021generalized}.
We formalize these details in the next section.
Before we proceed, we include the convergence guarantees of MPBNGC~\cite{makela2003multiobjective} below for the sake of completeness.




\begin{definition}
Let $Z \subset \mathbb{R}^N$ be open.
A locally Lipschitz function $f: Z \to \mathbb{R}$ is said to be weakly semismooth if the directional derivative
$f'(z,d) = \lim_{t \downarrow 0} \frac{f(z+td) - f(z)}{t}$
exists for all $z \in Z$, $d \in \R^N$ and $f'(z,d) = \lim_{t \downarrow 0} \tr{\xi(z+td)} d$ for $\xi(z+td) \in \partial f(z+td)$.
\end{definition}

\begin{definition}
Let $f: \R^N \to \R$, $g : \R^N \to \R^M$ be locally Lipschitz continuous.
Consider the problem $\min_{z: g(z) \leq 0} f(z)$.
A feasible point $z^*$ is said to be substationary if there exist multipliers $\lambda \geq 0$ and $\mu \in \mathbb{R}^M_+$, with $(\lambda, \mu) \neq (0, 0)$, such that
\[
0 \in \lambda \partial f(z^*) + \sum_{j=1}^{M} \mu_j \partial g_j(z^*), \qquad \mu_j g_j(z^*) = 0, \:\: \forall j \in [M].
\]
\end{definition}

\begin{theorem}
Suppose the function $\underline{v}$ is weakly semismooth.
Then MPBNGC either terminates finitely with a substationary point to~\eqref{eqn:strong_part}, or any accumulation point of a sequence of MPBNGC solutions is a substationary point to~\eqref{eqn:strong_part}.
\end{theorem}
\begin{proof}
See Theorem~9 of~\cite{makela2003multiobjective}.
\end{proof}



The example below shows the value function $\underline{v}$ of~\eqref{eqn:piecewise_mccormick_mip_oa} may be nonsmooth.

\begin{example}
\label{exm:nonsmooth_value_fn}
Consider the following instance of the QCQP~\eqref{eqn:orig_qcqp}:
\begin{align*}
\min_{x \in [0,1]} \:\: & x \:\: \text{ s.t. } \:\: x^2 \geq (0.4)^2.
\end{align*}
The optimal solution is $x^* = 0.4$ with optimal value $v^* = 0.4$.
Suppose we wish to partition the domain of $x$ into two sub-intervals (i.e., $d = 1$).
Let $\mathcal{P} = [0, p, 1]$ denote the partition of $x$ with $0 \leq p \leq 1$.
After some algebraic manipulation, the outer-approximation problem~\eqref{eqn:piecewise_mccormick_mip_oa} can be reformulated as
\begin{equation*}
\underline{v}(p) = \uset{x \in [0,1]}{\min} \:\: x \:\: \text{s.t.} \:\: w \geq (0.4)^2, \:\: w \leq \max\{px, (1+p)x - p\}, \:\: w \geq 2\alpha_j x - \alpha^2_j, \: \forall j \in \mathcal{J},
\end{equation*}
where $\{\alpha^k_j\}_{j \in \mathcal{J}} \subset [0,1]$
and we write $\underline{v}(p)$ to indicate the dependence on the partitioning point~$p$.
We can derive the {\pmc} lower bound to be 


\begin{minipage}[t]{0.4\textwidth}
\vspace*{-0.85in}
\[
\underline{v}(p) = 
\begin{cases}
\frac{0.16+p}{1+p}, & \text{if $0 \leq p \leq 0.4$}\\
\frac{0.16}{p}, & \text{if $0.4 < p \leq 1$}
\end{cases},
\]
\end{minipage}
\hfill
\begin{minipage}[t]{0.5\textwidth}
\centering
\input{figures/tikz_value_function.tex}
\end{minipage}

\noindent which shows that $\underline{v}$ is continuous and piecewise differentiable at $p = 0.4$.
\end{example}





\subsection{Computing generalized gradients of \texorpdfstring{$\underline{v}$}{the value function}}


We identify conditions under which a generalized gradient of the value function $\underline{v}$ may be computed in practice (we consider generalized gradients of $\underline{v}$ on $\mathfrak{P}$ instead of the open set $\text{int}(\mathfrak{P})$ for simplicity).
We begin with the following useful result; its assumption that the $y$-solution of~\eqref{eqn:piecewise_mccormick_mip_abstract} is unique can be verified by adding a ``no-good cut'' and re-solving~\eqref{eqn:piecewise_mccormick_mip_abstract} to check if the second-best solution for $y$ has a strictly greater objective than $\underline{v}(P)$.

\begin{lemma}
\label{lem:value_fn_active_part}
Suppose problem~\eqref{eqn:piecewise_mccormick_mip_abstract} has a unique $y$ solution~$y^* \in Y$ at $P \in \mathfrak{P}$ and $v(\cdot,y^*)$ is continuous at $P$.
Then $\underline{v}(\tilde{P}) = v(\tilde{P},y^*)$, $\forall \tilde{P} \in \mathfrak{P}$ in a neighborhood of $P$.
\end{lemma}
\begin{proof}
Because $y^*$ is the unique $y$ solution to~\eqref{eqn:piecewise_mccormick_mip_abstract} at $P \in \mathfrak{P}$, $v(P,y^*) < v(P,y)$, $\forall y \in Y \backslash \{y^*\}$.
To see $\underline{v}(\cdot) \equiv v(\cdot,y^*)$ in a neighborhood of $P$, we show that the value function $v(\cdot,y)$ is lower semicontinuous on~$\mathfrak{P}$ for each $y \in Y$.
The stated result then holds since $v(\cdot,y^*)$ is assumed to be continuous at $P$.

The set-valued mapping $P \in \mathfrak{P} \mapsto \{z \geq 0 : M(P,y) = \bar{B}y + \bar{b}\}$ is locally compact for each $y \in Y$ by virtue of the continuity of the mapping $M(\cdot,y)$ and the finite bounds that can be deduced on all of the variables in problem~\eqref{eqn:piecewise_mccormick_mip_oa}.
Lemma~5.3 of Still~\cite{still2018lectures} then implies that $v(\cdot,y)$ is lower semicontinuous on~$\mathfrak{P}$ for each $y \in Y$.
\end{proof}



The next result characterizes the gradient of $\underline{v}$ in the non-degenerate case.



\begin{theorem}
\label{thm:smooth_case}
Suppose $P \in \mathfrak{P}$ and problem~\eqref{eqn:piecewise_mccormick_mip_abstract} has a unique $y$ solution $y^* \in Y$.
Consider the LP~\eqref{eqn:piecewise_mccormick_mip_abstract_inner} with $y$ fixed to $y^*$.
If this LP has a unique primal solution $z^*$ and a unique dual solution $\pi^*$, then
\[
\frac{\partial \underline{v}}{\partial p^i_j}(P) = \frac{\partial v}{\partial p^i_j}(P,y^*) = \sum_{k=1}^{n_r} \sum_{l=1}^{n_c} \pi^*_k z^*_l \frac{\partial M_{kl}}{\partial p^i_j}(P,y^*), \quad \forall i \in [n], \: j \in [d].
\]
\end{theorem}
\begin{proof}
Lemma~\ref{lem:value_fn_active_part} implies $\underline{v}(\cdot) \equiv v(\cdot,y^*)$ in a neighborhood of $P$ provided $v(\cdot,y^*)$ is continuous at $P$.
Theorem~1 of Freund~\cite{freund1985postoptimal} (cf.\ Proposition~4.1 of~\cite{de2021generalized}) and the fact that the function $M(\cdot,y^*)$ is continuously differentiable on $\mathfrak{P}$ together imply $v(\cdot,y^*)$ is continuously differentiable at $P$ and the stated equalities hold.
\end{proof}


Next, we derive a formula for the generalized gradient $\partial_P \underline{v}(P)$ when the assumption that the LP~\eqref{eqn:piecewise_mccormick_mip_abstract_inner} is non-degenerate fails to hold.



\begin{theorem}
\label{thm:nonsmooth_case}
Suppose $P \in \mathfrak{P}$ and problem~\eqref{eqn:piecewise_mccormick_mip_abstract} has a unique $y$ solution $y^* \in Y$.
Consider the LP~\eqref{eqn:piecewise_mccormick_mip_abstract_inner} with $y$ fixed to $y^*$.
Suppose $v(\cdot,y^*)$ is finite and locally Lipschitz in a neighborhood of $P$.
Then
\begin{align*}
\partial_P \underline{v}(P) &= \partial_P v(P,y^*) \subset \text{conv}\biggl(\biggl\{\sum_{k=1}^{n_r} \sum_{l=1}^{n_c} \pi^*_k x^*_l \partial_P M_{kl}(P) \: : \: (x^*,\pi^*) \text{ is a primal-dual optimal pair for } \eqref{eqn:piecewise_mccormick_mip_abstract_inner}\biggr\}\biggr).
\end{align*}
\end{theorem}
\begin{proof}
Lemma~\ref{lem:value_fn_active_part} implies $\underline{v}(\cdot) \equiv v(\cdot,y^*)$ in a neighborhood of $P$.
The stated equalities hold by mirroring the proof of Theorem~5.1 of De Wolf and Smeers~\cite{de2021generalized} and noting that the function $M(\cdot,y^*)$ is continuously differentiable on $\mathfrak{P}$.
\end{proof}


De Wolf and Smeers~\cite{de2021generalized} (see Assumption~5.1) and Im~\cite{im2018sensitivity} argue the following result with $\bar{y} = y^*$ ensures $v(\cdot,y^*)$ is locally Lipschitz in a neighborhood of $P \in \mathfrak{P}$.

\begin{lemma}
\label{lem:genl_grad_assumptions}
Suppose $P \in \mathfrak{P}$ and $\bar{y} \in Y$.
Consider the LP~\eqref{eqn:piecewise_mccormick_mip_abstract_inner} with $y$ fixed to $\bar{y}$.
If the matrix $M(P,\bar{y})$ has full row rank and $\bar{B}\bar{y} + \bar{b} \in \text{int}\bigl( \{ M(P,\bar{y})z : z \geq 0 \}  \bigr)$, then $v(\cdot,\bar{y})$ is finite and locally Lipschitz in a neighborhood of $P$.
\end{lemma}
\begin{proof}
See Proposition~5.3 of~\cite{de2021generalized} and pages~73 to~76 of~\cite{im2018sensitivity}.
\end{proof}


We now verify that the full rank assumption in Lemma~\ref{lem:genl_grad_assumptions} holds in general.


\begin{lemma}
\label{lem:fullrank}
The matrix $M(P, y)$ has full row rank, $\forall P \in \text{int}(\mathfrak{P})$ and $y \in Y$.
\end{lemma}
\begin{proof}
Fix $y \in Y$.
Since $P \in \text{int}(\mathfrak{P})$, we have $0 < p^i_1 < p^i_2 < \dots < p^i_d < 1$ for each $i \in [n]$.
We show that for each $(i,j) \in \mathcal{B}$ and $k \in \mathcal{Q}$, the equality constraints in equations~\eqref{eqn:lambda_form_bilinear_conv_comb}-\eqref{eqn:lambda_form_bilinear_active2} and equations~\eqref{eqn:lambda_form_quadratic_conv_comb}-\eqref{eqn:lambda_form_quadratic_active} have full row rank, which readily imply that $M(P,y)$ has full row rank.
We ignore inequality constraints because they are transformed into equality constraints by the addition of unique slack variables.

We begin by focusing on the equality constraints in~\eqref{eqn:lambda_form_bilinear_conv_comb}-\eqref{eqn:lambda_form_bilinear_active2} involving the $x$, $w$, and $\lambda$ variables. 
Consider a fixed $(i,j) \in \mathcal{B}$.
Since at most four of the $\lambda^{ij}$ variables may be nonzero, we can rewrite these equality constraints as follows after a change of variables (here, $\mathcal{A}(i)$ denotes the active partition of $x_i$):
{
\footnotesize
\[
\begin{pmatrix}
-1 & 0 & 0 & p^i_{\mathcal{A}(i)-1} & p^i_{\mathcal{A}(i)-1} & p^i_{\mathcal{A}(i)} & p^i_{\mathcal{A}(i)} \\
0 & -1 & 0 & p^j_{\mathcal{A}(j)-1} & p^j_{\mathcal{A}(j)} & p^j_{\mathcal{A}(j)-1} & p^j_{\mathcal{A}(j)} \\
0 & 0 & -1 & p^i_{\mathcal{A}(i)-1} p^j_{\mathcal{A}(j)-1} & p^i_{\mathcal{A}(i)-1} p^j_{\mathcal{A}(j)} & p^i_{\mathcal{A}(i)} p^j_{\mathcal{A}(j)-1} & p^i_{\mathcal{A}(i)} p^j_{\mathcal{A}(j)} \\
0 & 0 & 0 & 1 & 1 & 1 & 1
\end{pmatrix}
\begin{pmatrix}
x_i \\
x_j \\
w_{ij} \\
\lambda^{ij}_1 \\
\lambda^{ij}_2 \\
\lambda^{ij}_3 \\
\lambda^{ij}_4
\end{pmatrix}
= \begin{pmatrix}
0 \\
0 \\
0 \\
1
\end{pmatrix}.
\]
}
We argue that the following sub-matrix is of full rank whenever $P \in \text{int}(\mathfrak{P})$:
\[
\begin{pmatrix}
p^i_{\mathcal{A}(i)-1} & p^i_{\mathcal{A}(i)-1} & p^i_{\mathcal{A}(i)} & p^i_{\mathcal{A}(i)} \\
p^j_{\mathcal{A}(j)-1} & p^j_{\mathcal{A}(j)} & p^j_{\mathcal{A}(j)-1} & p^j_{\mathcal{A}(j)} \\
p^i_{\mathcal{A}(i)-1} p^j_{\mathcal{A}(j)-1} & p^i_{\mathcal{A}(i)-1} p^j_{\mathcal{A}(j)} & p^i_{\mathcal{A}(i)} p^j_{\mathcal{A}(j)-1} & p^i_{\mathcal{A}(i)} p^j_{\mathcal{A}(j)} \\
1 & 1 & 1 & 1
\end{pmatrix}.
\]
Subtracting the first column from the second column, the third from the fourth column, and finally the first from the third column yields the column vectors 
\begin{align*}
&\bigl(p^i_{\mathcal{A}(i)-1}, \:\: p^j_{\mathcal{A}(j)-1}, \:\: p^i_{\mathcal{A}(i)-1} p^j_{\mathcal{A}(j)-1}, \:\: 1\bigr), \\ 
&\bigl(0, \:\: (p^j_{\mathcal{A}(j)} - p^j_{\mathcal{A}(j)-1}), \:\: p^i_{\mathcal{A}(i)-1} (p^j_{\mathcal{A}(j)} - p^j_{\mathcal{A}(j)-1}), \:\: 0\bigr), \\
&\bigl( (p^i_{\mathcal{A}(i)} - p^i_{\mathcal{A}(i)-1}), \:\: 0, \:\: p^j_{\mathcal{A}(j)-1} (p^i_{\mathcal{A}(i)} - p^i_{\mathcal{A}(i)-1}), \:\: 0\bigr), \\
& \bigl(0, \:\: (p^j_{\mathcal{A}(j)} - p^j_{\mathcal{A}(j)-1}), \:\: p^i_{\mathcal{A}(i)} (p^j_{\mathcal{A}(j)} - p^j_{\mathcal{A}(j)-1}), \:\: 0\bigr).
\end{align*}
It is easy to see these vectors are linearly independent if $0 < p^i_{\mathcal{A}(i)-1} < p^i_{\mathcal{A}(i)} < 1$, $\forall i$.

Next, we focus on the equality constraints in~\eqref{eqn:lambda_form_quadratic_conv_comb}-\eqref{eqn:lambda_form_quadratic_active} involving the $x$, $w$, and $\lambda$ variables for a fixed $k \in \mathcal{Q}$.
Since at most two of the $\lambda^{k}$ variables may be nonzero, we can rewrite these equality constraints as follows after a change of variables:
\[
\begin{pmatrix}
-1 & p^k_{\mathcal{A}(k)-1} & p^k_{\mathcal{A}(k)} \\
0 & 1 & 1
\end{pmatrix}
\begin{pmatrix}
x_k \\
\lambda^{k}_1 \\
\lambda^{k}_2
\end{pmatrix}
= \begin{pmatrix}
0 \\
1
\end{pmatrix}.
\]
The last two matrix columns are linearly independent if $0 < p^k_{\mathcal{A}(k)-1} < p^k_{\mathcal{A}(k)} < 1$.
\end{proof}


Finally, we show that for almost every (a.e.) $P \in \mathfrak{P}$ with respect to the uniform measure, problem~\eqref{eqn:piecewise_mccormick_mip_abstract} either has a unique $y$ solution (which ensures that we can use Theorem~\ref{thm:smooth_case} or~\ref{thm:nonsmooth_case} to compute a generalized of $\underline{v}$ under mild conditions), or $\underline{v}(P) = v^*$ (i.e., the optimal values of~\eqref{eqn:qcqp} and problem~\eqref{eqn:piecewise_mccormick_mip_abstract} are equal, which implies that partitioning points $P$ are sufficient for convergence of the lower bound), or both.

\begin{theorem}
At least one of the following holds for a.e.\ $P \in \mathfrak{P}$:
\begin{enumerate}
\item $\underline{v}(P) = v^*$,

\item Problem~\eqref{eqn:piecewise_mccormick_mip_abstract} has a unique $y$ solution.
\end{enumerate}
\end{theorem}
\begin{proof}
Consider $P \in \mathfrak{P}$, and let $\hat{y} \in Y$ and $\hat{x} \in [0,1]^n$ denote the $y$ and $x$ components of an optimal solution to problem~\eqref{eqn:piecewise_mccormick_mip_abstract}.
We consider the following cases:
\begin{enumerate}[label=(\alph*)]
\item For each $k \in \mathcal{Q}$, we have $\hat{x}_k \in \{p^k_0, p^k_1, \dots, p^k_{d+1}\}$. Additionally, for each $(i,j) \in \mathcal{B}$, we either have $\hat{x}_i \in \{p^i_0, p^i_1, \dots, p^i_{d+1}\}$, or $\hat{x}_j \in \{p^j_0, p^j_1, \dots, p^j_{d+1}\}$, or both.

\item Case (a) does not hold, i.e., there either exists at least one index $k \in \mathcal{Q}$ such that $\hat{x}_k \not\in \{p^k_0, p^k_1, \dots, p^k_{d+1}\}$, or there exists at least one pair of indices $(i,j) \in \mathcal{B}$ such that  both $\hat{x}_i \not\in \{p^i_0, p^i_1, \dots, p^i_{d+1}\}$ and $\hat{x}_j \not\in \{p^j_0, p^j_1, \dots, p^j_{d+1}\}$.
\end{enumerate}


Suppose case (a) holds.
Since we assume that $\{p^k_0,p^k_1,\dots,p^k_{d+1}\} \subset \{\alpha^k_j\}$ for each $k \in \mathcal{Q}$, our outer-approximation of the piecewise McCormick relaxation \eqref{eqn:lambda_form_quadratic_conv_comb}-\eqref{eqn:lambda_form_quadratic_active} for the quadratic constraint $w_{kk} = x^2_k$ is exact (i.e., there is no relaxation gap) at the partitioning points $x_k \in \{p^k_0, p^k_1, \dots, p^k_{d+1}\}$.
Additionally, the piecewise McCormick relaxation \eqref{eqn:lambda_form_bilinear_conv_comb}-\eqref{eqn:lambda_form_bilinear_active2} for the bilinear constraint $w_{ij} = x_i x_j$ is exact either when $x_i \in \{p^i_0, p^i_1, \dots, p^i_{d+1}\}$, or when $x_j \in \{p^j_0, p^j_1, \dots, p^j_{d+1}\}$, or both.
Therefore, the point $\hat{x}$ is feasible to the original QCQP~\eqref{eqn:orig_qcqp}, which implies $\underline{v}(P) = v^*$.


Suppose instead that case (b) holds.
Additionally, suppose there exist multiple $y$ solutions to problem~\eqref{eqn:piecewise_mccormick_mip_abstract}.
Let $\tilde{y} \in Y$ and $\tilde{x} \in [0,1]^n$ denote the $y$ and $x$ components of \textit{another} optimal solution to problem~\eqref{eqn:piecewise_mccormick_mip_abstract} with $\tilde{y} \neq \hat{y}$.
Since case (a) does not hold and $\tilde{y} \neq \hat{y}$, we have $\tilde{x} \neq \hat{x}$.
Moreover, there exist non-singular basis matrices $\hat{M}(P,\hat{y})$ and $\tilde{M}(P,\tilde{y})$ for the LPs~\eqref{eqn:piecewise_mccormick_mip_abstract_inner} corresponding to $\hat{y}$ and $\tilde{y}$, respectively, such that 
\begin{equation}
\label{eqn:polynomial_eqn}
\underline{v}(P) = v(P,\hat{y}) = \tr{\hat{c}} [\hat{M}(P,\hat{y})]^{-1} (\bar{B} \hat{y} + \bar{b}) = \tr{\tilde{c}} [\tilde{M}(P,\tilde{y})]^{-1} (\bar{B} \tilde{y} + \bar{b}) = v(P,\tilde{y})
\end{equation}
for suitable vectors $\tilde{c}$ and $\hat{c}$, which only include the components of $\bar{c}$ corresponding to the basic variables of these LPs.
Since not all components of $\hat{x}$ equal $0$ or $1$, at least some of the entries of $\hat{M}(P,\hat{y})$ are functions of the partitioning points~$P$.
Moreover, $v(P,\hat{y})$ and $v(P,\tilde{y})$ are not identical functions of $P$ since $\tilde{x} \neq \hat{x}$.
Equation~\eqref{eqn:polynomial_eqn} thus yields a polynomial equation in~$P$.
Therefore, the set of all $P \in \mathfrak{P}$ such that~\eqref{eqn:polynomial_eqn} holds has measure zero.
Noting that $\abs{Y} < +\infty$ and the number of possible bases is finite for each $y \in Y$ concludes the proof.
\end{proof}



\begin{algorithm}[t]
\caption{Preprocessing and postprocessing steps for strong partitioning}
\label{alg:enhancements}
{
\begin{algorithmic}[1]
\Statex \textbf{Preprocessing steps}

\vspace*{0.05in}
\State \textbf{Initialization}: partitions $\hat{\mathcal{P}}^0_i := [0, 1]$, $\forall i \in [n]$ \label{algo:preprocess_begin}

\State Solve the McCormick relaxation~\eqref{eqn:mccormick} to compute a lower bounding solution $\bar{x}^0$



\For{$k = 1, 2, \dots, d$}

\For{$i = 1, 2, \dots, n$} 

\If{$i \not\in \mathcal{NC}$ \textbf{or} $\bar{x}^{k-1}_i \approx \tilde{x}_i$ for some $\tilde{x}_i \in \hat{\mathcal{P}}^{k-1}_i$}

\State Set $\hat{\mathcal{P}}^k_i = \hat{\mathcal{P}}^{k-1}_i$

\Else

\State Insert $\bar{x}^{k-1}_i$ in $\hat{\mathcal{P}}^{k-1}_i$ to obtain $\hat{\mathcal{P}}^k_i$

\EndIf


\EndFor


\State Solve~\eqref{eqn:piecewise_mccormick_mip_oa} with partitions $\{\hat{\mathcal{P}}^k_i\}_{i \in [n]}$ to determine solution $\bar{x}^k$


\EndFor


\vspace*{0.05in}

\State Let $n_i := \abs{\hat{\mathcal{P}}^d_i} - 2$, $\forall i \in [n]$

\State Let $[0, p^{i0}_{d-n_i+1}, p^{i0}_{d-n_i+2}, \dots, p^{i0}_d, 1]$ denote $\hat{\mathcal{P}}^d_i$ and set $p^{i0}_j := 0$, $\forall j \in [d-n_i]$

\State Set the initial guess for~\eqref{eqn:strong_part} to $P^0$, where $P^0_{ji} := p^{i0}_j$, $\forall i \in [n]$, $j \in [d]$

\State Fix variables $p^i_j$, $j \in [d-n_i]$, to $0$ while solving~\eqref{eqn:strong_part}

\State \textbf{Output}: initial guess (and variable fixings) $P^0$ for problem~\eqref{eqn:strong_part} \label{algo:preprocess_end}


\vspace*{0.15in}

\State Solve max-min problem~\eqref{eqn:strong_part} to obtain a solution $\bar{P} \in \mathfrak{P}$ with objective $\bar{v} := \underline{v}(\bar{P})$  \label{algo:maxmin}

\vspace*{0.15in}

\noindent
\textbf{Postprocessing steps} 


\vspace*{0.05in}

\For{$j = 1, 2, \dots, d$} \label{algo:postprocess_begin}

\For{$i \in \mathcal{NC}$}

\State Set $\hat{P} = \bar{P}$ and replace the element $\hat{P}_{ji}$ with zero

\State Solve~\eqref{eqn:piecewise_mccormick_mip_oa} with partitioning points $\hat{P}$ to obtain bound $\hat{v} := \underline{v}(\hat{P})$

\If{$\hat{v} \leq \bar{v} + 10^{-6} \abs{\bar{v}}$}

\State Update $\bar{P} = \hat{P}$ and sort it such that $\bar{P}_{ki} \leq \bar{P}_{(k+1)i}$, $\forall k \in [d-1]$

\EndIf


\EndFor

\EndFor

\vspace*{0.05in}
\State \textbf{Output}: partitions $\{\mathcal{P}^1_i\}_{i \in [n]}$ for Algorithm~\ref{alg:partitioning_algorithm} based on postprocessed $\bar{P}$ \label{algo:postprocess_end}


\end{algorithmic}
}
\end{algorithm}





\subsection{Algorithmic enhancements}

We design preprocessing and postprocessing steps that can be used to mitigate the computational burden of solving~\eqref{eqn:strong_part} locally and enable our ML model to more effectively learn its solution.


The outer-maximization in problem~\eqref{eqn:strong_part} involves $n \times d$ partitioning variables.
Since larger problem dimensions may increase both the per-iteration cost and number of iterations taken by the bundle solver to converge, we propose preprocessing heuristics to fix a subset of the {\pp} $P$ and to compute an initial guess $P^0$ for the bundle method.
After solving the max-min problem~\eqref{eqn:strong_part} (line \ref{algo:maxmin}), we propose postprocessing steps to eliminate {\pp} in its solution $\bar{P}$ that do not significantly affect the lower bound $\underline{v}(\bar{P})$.
Algorithm~\ref{alg:enhancements} includes detailed pseudocode of our preprocessing (lines~\ref{algo:preprocess_begin}--\ref{algo:preprocess_end}) and postprocessing steps (lines~\ref{algo:postprocess_begin}--\ref{algo:postprocess_end}).









\section{Numerical experiments}
\label{sec:computexp}


We study the impact of using strong partitioning to specify Alpine's partitions at the first iteration and investigate an off-the-shelf ML model for learning these partitions for homogeneous QCQPs.
We begin by describing the setup for our computational experiments.
In Section~\ref{subsec:test_instances}, we outline the procedure for generating families of random QCQP instances, including instances of the pooling problem.
We detail our ML approximation of strong partitioning in Section~\ref{subsec:ml_approx}, and compare the performance of strong partitioning and its ML approximation against Alpine's default partitioning strategy (see Section~\ref{sec:alpinepart} for details) in Section~\ref{subsec:results}.



Our strong partitioning code is written in Julia 1.6.3 and implemented within Alpine.jl v0.4.1\footnote{\url{https://github.com/lanl-ansi/Alpine.jl}}.
We use JuMP.jl v1.1.1 and use Gurobi 9.1.2 via Gurobi.jl v0.11.3 for solving LPs, MILPs, and convex MIQCQPs (with $\texttt{MIPGap} = 10^{-6}$). 
We use either Ipopt 3.14.4 via Ipopt.jl v1.0.3 (with $\texttt{max\_iter} = 10^4$), or Artelys Knitro 12.4.0 via KNITRO.jl v0.13.0 (with $\texttt{algorithm} = 3$) to solve NLPs locally within Alpine\footnote{We switch Alpine's local solver between Ipopt for the random bilinear and QCQP instances and Knitro for the random pooling instances because Ipopt is less effective for the pooling instances.}.
We use the bundle solver MPBNGC 2.0~\cite{makela2003multiobjective} via MPBNGCInterface.jl\footnote{\url{https://github.com/milzj/MPBNGCInterface.jl}} (with options $\texttt{OPT\_LMAX} = 20$, $\texttt{OPT\_EPS} = 10^{-9}$, and $\texttt{OPT\_NITER} = \texttt{OPT\_NFASG} = 500$) to solve the max-min problem~\eqref{eqn:strong_part} to local optimality.
We consider strong partitioning with either two or four {\pp} per partitioned variable ($d = 2$ or $d = 4$) in addition to the variable bounds and use scikit-learn v0.23.2~\cite{scikit-learn} to design its ML approximation.
To demonstrate the non-trivial nature of our nonconvex test instances, we also solve them  to global optimality using BARON 22.11.3 via BARON.jl v0.8.0 and provide BARON with the option of using CPLEX 22.1.0 as an MILP solver.


All of our experiments were run 
on nodes of the Darwin cluster at LANL with dual socket Intel Broadwell 18-core processors (E5-2695\_v4 CPUs, base clock rate at 2.1GHz), EDR InfiniBand, and 125GB of memory.
Each instance was run exclusively on a single node and different solution approaches were run in sequence to limit the impact of variability in machine performance.
All Alpine and BARON runs were given a time limit of $2$ hours with target relative and absolute optimality gaps of $10^{-4}$ and $10^{-9}$, respectively\footnote{Alpine's definition of relative gap differs slightly from BARON's definition, see Section~\ref{subsubsec:baron_benchmark}.}. 
No time limit was specified for solving the max-min problem~\eqref{eqn:strong_part}.
The rest of BARON's options, including range reduction options, were kept to default.
We deactivate bounds tightening techniques within Alpine because it is largely ineffective for our medium and large-scale instances (our approaches are easily adapted to the setting where bounds tightening is employed).
We partition the domains of all variables participating in nonconvex terms within Alpine, and set the rest of Alpine's options to default, including the partition scaling factor to $\Delta = 10$.



\subsection{Test instances}
\label{subsec:test_instances}

We describe how we generate homogeneous families of random QCQPs, including instances of the pooling problem, based on the literature.
Scripts for generating the different families of instances can be found at \url{https://github.com/lanl-ansi/Alpine.jl/tree/master/examples/random_QCQPs}.


\subsubsection{Random bilinear programs}
\label{subsubsec:random_bilinear}

We consider parametric bilinear programs of the form~\cite{bao2011semidefinite}:
\begin{alignat*}{2}
v(\theta) := &\min_{x \in [0,1]^{n}} \:\: && x^{\textup{T}} Q^0(\theta) x + (r^0(\theta))^{\textup{T}} x \\
&\quad\: \text{s.t.} && x^{\textup{T}} Q^i(\theta) x + (r^i(\theta))^{\textup{T}} x \leq b_i, \quad \forall i \in [m_I], \\
& && (a^j)^{\textup{T}} x = d_j, \quad \forall j \in [m_E],
\end{alignat*}
where $\theta \in [-1,1]^{d_{\theta}}$ are parameters,
$r^k(\theta) \in \R^n$, $k \in \{0\} \cup [m_I]$, $Q^k(\theta) \in \R^{n \times n}$, $k \in \{0\} \cup [m_I]$, are symmetric but not necessarily positive semi-definite, $a^j \in \R^n$, $j \in [m_E]$, $b \in \R^{m_I}$, and $d \in \R^{m_E}$.


We generate $1000$ instances for each of $n \in \{10, 20, 50\}$ variables with $\abs{\mathcal{B}} = \min\{5n, \binom{n}{2}\}$ bilinear terms (we count $x_i x_j$ and $x_j x_i$ as the same bilinear term; all instances for a fixed dimension $n$ have the same set of $\abs{\mathcal{B}}$ bilinear terms), $\abs{\mathcal{Q}} = 0$ quadratic terms, $m_I = n$ bilinear inequalities, and $m_E = 0.2n$ linear equalities~\cite{bao2011semidefinite}.
We let the dimension $d_{\theta} = 3 \times (0.2m_I + 1)$ (see below for why we make this choice).
The problem data is generated as follows (cf.\ \cite{bao2011semidefinite}).
All entries of the vectors $a^j$ and $d$ are generated i.i.d.\ from the uniform distribution $U(-1,1)$, and all entries of the vector $b$ are generated i.i.d.\ from $U(0,100)$.
The components of $\theta$ are generated i.i.d.\ from $U(-1,1)$.
Each $Q^k$ and $r^k$, $k \in \{0,1,\dots,0.2m_I\}$, are of the form:
\[
Q^k(\theta) = \bar{Q}^k + \displaystyle\sum_{l=3k+1}^{3k+3} \theta_{l} \tilde{Q}^{k,l-3k}, \qquad r^k(\theta) = \bar{r}^k + \displaystyle\sum_{l=3k+1}^{3k+3} \theta_{l} \tilde{r}^{k,l-3k}.
\]
The nonzero entries of the ``nominal matrices'' $\bar{Q}^k$ and ``nominal vectors'' $\bar{r}^k$ are generated i.i.d.\ from $U(-1,1)$. 
For each tuple $(i,j) \in \mathcal{B}$, indices $k \in \{0,1,\dots,0.2m_I\}$ and $l \in \{1,2,3\}$, we set $\tilde{Q}^{k,l}_{ij} := \gamma^{k,l}_{ij} \bar{Q}^k_{ij}$, where $\gamma^{k,l}_{ij}$ are generated i.i.d.\ from $U(0,0.5)$. 
Similarly, for each index $i \in \{1,\dots,n\}$, $k \in \{0,1,\dots,0.2m_I\}$, and $l \in \{1,2,3\}$, we set $\tilde{r}^{k,l}_i := \delta^{k,l}_i \bar{r}^k_i$, where $\delta^{k,l}_i$ are generated i.i.d.\ from $U(0,0.5)$. 
Since each $\tilde{Q}^{k,l}$ and $\tilde{r}^{k,l}$ is a different perturbation of $\bar{Q}^k$ and $\bar{r}^k$, the expansions of $Q^k$ and $r^k$ may be motivated using principal components analysis.
The nonzero entries of the remaining matrices $Q^k$ and vectors $r^k$, $k \in \{0.2m_I+1,\dots,m_I\}$, are the same across all $1000$ instances and generated i.i.d.\ from $U(-1,1)$.
Finally, the constraint coefficients are re-scaled such that the vectors $b = d = 1$.
Note that for a fixed dimension~$n$, each instance is uniquely specified by the parameters~$\theta$ of dimension $d_{\theta} = 3 (0.2n + 1)$.






\subsubsection{Random QCQPs with bilinear \textit{and} univariate quadratic terms}
\label{subsubsec:random_qcqps}

We also generate $1000$ random QCQPs with $\abs{\mathcal{B}} = \min\{5n, \binom{n}{2}\}$ bilinear terms and $\abs{\mathcal{Q}} = \lfloor0.25n\rfloor$ univariate quadratic terms for each of $n \in \{10,20,50\}$ variables (all instances for a fixed $n$ have the same set of bilinear and univariate quadratic terms).
The coefficients of quadratic terms in the objective and constraints are generated similarly to the coefficients of bilinear terms in Section~\ref{subsubsec:random_bilinear}.
The rest of the model parameters and problem data (including $\theta$) are also generated similarly as in Section~\ref{subsubsec:random_bilinear}.







\subsubsection{The pooling problem}
\label{subsubsec:pooling_problem}


The pooling problem is a classical example of a bilinear program introduced by Haverly~\cite{haverly1978studies}.
It has several important applications in process systems engineering, including petroleum refining~\cite{kannan2018algorithms,yang2016integrated}, natural gas production~\cite{kannan2018algorithms,li2011stochastic}, and water treatment network design~\cite{bergamini2008improved,misener2013glomiqo,saif2008global}.
Its goal is to blend inputs of differing qualities at intermediate pools to produce outputs that meet quality specifications while satisfying capacity constraints at inputs, pools, and outputs.
Solving the pooling problem is in general NP-hard~\cite{alfaki2013multi}.


We consider instances of the pooling problem with $45$ inputs, $15$ pools, $30$ outputs, and a single quality.
Each instance has $116$ input-output arcs, $71$ input-pool arcs, and $53$ pool-output arcs, yielding $572$ variables and $621$ constraints, including $360$ linear constraints and $261$ bilinear equations (with $124$ variables involved in bilinear terms).
We use the $pq$-formulation of the pooling problem outlined in Section~2 of~\cite{luedtke2020strong}.
Note that unlike the random bilinear instances in Section~\ref{subsubsec:random_bilinear} where all of the original ``$x$ variables'' participate in bilinear terms, only $124$ out of the $311$ original variables in the pooling model participate in bilinear terms.


We first generate a nominal instance using the ``random Haverly'' instance generation approach\footnote{\url{https://github.com/poolinginstances/poolinginstances}} in~\cite{luedtke2020strong} that puts together $15$ perturbed copies of one of Haverly's pooling instances~\cite{haverly1978studies} and adds $150$ edges to it.
We modify the target output quality concentrations generated by~\cite{luedtke2020strong} to construct harder instances.
For each output $j$, we compute the minimum $c^{\text{min}}_j$ and maximum $c^{\text{max}}_j$ input concentrations of the quality over the subset of inputs from which there exists a path to output $j$.
We then specify the lower and upper bound on the quality concentration at output $j$ to be $c^{\text{min}}_j + \alpha_j (c^{\text{max}}_j - c^{\text{min}}_j)$ and $c^{\text{min}}_j + \beta_j (c^{\text{max}}_j - c^{\text{min}}_j)$, respectively, where $\alpha_j \sim U(0.2,0.4)$ and $\beta_j \sim U(0.6,0.8)$ are generated independently.
We also rescale the capacities of the inputs, pools, and outputs and the costs of the arcs for better numerical performance.
Note that while all variables in the formulation are non-negative, upper bounds on the variables are not necessarily equal to one after rescaling.
After constructing a nominal instance using the above procedure, we use it to generate $1000$ random pooling instances by randomly perturbing each input's quality concentration (parameters $\theta$ for this problem family) by up to $20\%$, uniformly and independently.








\subsection{Machine learning approximation of strong partitioning}
\label{subsec:ml_approx}

Because solving the max-min problem~\eqref{eqn:strong_part} even to local optimality may be time consuming, we propose to learn the SP strategy for homogeneous QCQP instances.
We detail our off-the-shelf ML approximation of strong partitioning in this section.
Although our ultimate goal is to optimize the ML model so that its predictions yield good performance when they are used to inform Alpine's partitions at the first iteration, we instead choose our ML model solely based on its accuracy of predicting the strong {\pp}.
We do so because tuning the hyperparameters of the ML model directly for good performance within Alpine incurs a huge computational expense due to the need to re-evaluate the performance of the ML predictions within Alpine for each choice of the hyperparameters. 
While the choice of the ML model can have a significant impact on the performance of its predictions within Alpine, we leave the design of more sophisticated ML architectures for future work.


We use scikit-learn's AdaBoost regressor\footnote{\url{https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.AdaBoostRegressor.html}}~\cite{freund1997decision} that implements the ``\texttt{AdaBoost.R2} algorithm''~\cite{drucker1997improving} to learn a mapping from each QCQP instance to the strong {\pp}.
Our base estimator is a scikit-learn regression tree\footnote{\url{https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeRegressor.html}}~\cite{breiman2017classification} with maximum depth equal to $25$, and we set the maximum number of weak learners for the boosting algorithm to $1000$.
The rest of scikit-learn's \texttt{AdaBoostRegressor} options are set to default.
We use \textit{$10$-fold cross-validation} to generate out-of-sample ML predictions for all $1000$ QCQP instances in each problem family.
Specifically, we randomly split the $1000$ instances in each family into $10$ folds, use $9$ out of $10$ folds for training the ML model, predict the strong {\pp} for the omitted fold, and loop through different choices of the omitted fold to generate predictions for all $1000$ instances.
We \textit{emphasize} that we fit our ML model for prediction accuracy and do not perform much hyperparameter tuning since our ultimate goal is good performance of the ML predictions when used within Alpine. 


\paragraph{ML model inputs and outputs.}
The choice of features for the ML model can greatly impact its performance. 
We use the following problem features as \textit{inputs} to the ML model: i.\ parameters~$\theta$, which uniquely parametrize each nonconvex QCQP instance, ii.\ the best found feasible solution during Alpine's presolve step (which involves a single local solve), and iii.\ the McCormick lower bounding solution (obtained by solving a single convex program). 
Although it is theoretically sufficient to use only the parameters~$\theta$ as features because they uniquely identify each QCQP instance, we also use the features (ii) and (iii) since they are relatively cheap to compute and intuitively can help inform the partitioning strategy. 
These additional features are also complicated transformations of the instance parameters~$\theta$ that may otherwise be challenging to uncover.
The \textit{outputs} of our ML model are the $d$ {\pp} (excluding variable bounds) for each of the $n$ partitioned variables, resulting in an output dimension of $d \times n$.
In contrast with much of the literature on learning for MILPs, we train separate ML models for each family of $1000$ instances since both the feature and output dimensions of our ML models depend on the problem dimensions.
While we plan to design more advanced ML architectures that can accommodate variable feature and output dimensions as part of future work, we do not consider the need to train a different ML model for each problem family to be a major limitation.
This is because decision-makers often care about solving instances of the \textit{same} problem family with only a few varying parameters, which means they only need to train a single ML model with fixed feature and output dimensions for their application.







\begin{table}[t]
\centering
\begin{tabular}{ r | c c c c c }
\hline
Scaled MAE & $<0.01$ & $<0.02$ & $<0.05$ & $<0.1$ & $<0.2$ \\ 
& \multicolumn{5}{c}{\% Partitioning Points} \\ \hline
Bilinear $n = 10$ &  60  &  75   &  80   &  95  &  100  \\
Bilinear $n = 20$ &  15  &  22.5   &  60  &  87.5  &  97.5  \\
Bilinear $n = 50$ &  31  &  39   &  70  &  94  &  100  \\[0.05in]
QCQP $n = 10$ &  65  &  80   &  95   &  100  &  100  \\
QCQP $n = 20$ &  35  &  37.5   &  77.5  &  92.5  &  100  \\
QCQP $n = 50$ &  56  &  66   &  85  &  99  &  100  \\[0.05in]
Pooling           &  65.7  &  70.9   &  78.6   &  89.5  &  97.2  \\ \hline
\end{tabular}
\caption{Statistics of scaled MAEs of the out-of-sample predictions of the ML model.}
\label{tab:scaled_maes}
\end{table}







\paragraph*{}
We now summarize the out-of-sample prediction errors of our trained ML models when they are used to predict two strong {\pp} per partitioned variable (excluding variable bounds).
Table~\ref{tab:scaled_maes} provides statistics of the scaled mean absolute errors (MAEs) of the out-of-sample predictions of the $2n$ {\pp} ($248$ points for the pooling problem) produced by the ML model for each problem family.
The MAEs of the predicted {\pp} are averaged over the $1000$ instances in each family and scaled by the upper bounds of the corresponding $x$ variables---these upper bounds are simply equal to one for the random bilinear and QCQP instances, but are greater than one for some of the partitioned variables in the pooling instances.
Roughly $90\%$ or higher of the {\pp} predicted using ML have a scaled MAE of less than $10\%$ for each problem family, which indicates that the same underlying ML model is able to generate reasonable predictions of the strong {\pp} across these different problem families.







\subsection{Results and discussion}
\label{subsec:results}




\begin{table}
\centering
% \resizebox{\columnwidth}{!}{%
\begin{tabular}{ r | c c c c | c c }
\hline
Problem Family & \multicolumn{4}{c|}{BARON Solution Time (seconds)} \\
 & Shifted GM & Median & Min & Max & \# TLE & TLE Gap (GM) \\ \hline
Bilinear $n = 10$  &  0.2  &  0.2  &  0.1  &  0.4  &  0  & $\underline{\hspace{0.3cm}}$  \\
Bilinear $n = 20$  &  3.5  &  3.6  &  1.4  &  7.0  &  0  & $\underline{\hspace{0.3cm}}$ \\
Bilinear $n = 50$  &  257.1  &  260.6  &  54.3  &  4637.4  &  0  & $\underline{\hspace{0.3cm}}$ \\[0.1in]
QCQP $n = 10$  &  0.3  &  0.3  &  0.1  &  0.6  &  0  & $\underline{\hspace{0.3cm}}$  \\
QCQP $n = 20$  &  4.8  &  4.6  &  1.6  &  10.8  &  0  & $\underline{\hspace{0.3cm}}$ \\
QCQP $n = 50$  &  268.2  &  246.2  &  14.7  &  6897.9  &  19  & $2.3 \times 10^{-2}$ \\[0.1in]
Pooling  &  441.4  &  422.1  &  16.6  &  7114.5  &  432  &  $2.7 \times 10^{-2}$ \\  \hline
\end{tabular}
% }
\caption{Statistics of BARON solution times,
including the shifted geometric mean, median, minimum, and maximum times over the subset of $1000$ instances for which BARON does not hit the time limit. 
The last two columns denote the number of instances for which BARON hits the time limit and the corresponding geometric mean of residual optimality gaps at termination, respectively.
}
\label{tab:baron_times}
\end{table}







\begin{table}
\centering
\begin{tabular}{ c | c c c c | c }
\hline
Solution Method & \multicolumn{4}{c|}{Solution Time (seconds)} \\
& Shifted GM & Median & Min & Max & \# TLE \\ \hline
Alpine (default)  &  30.7  &  14.1  &  0.4  &  4020.9  &  2  \\
Alpine+SP2  &  9.4  &  1.7  &  0.2  &  3864.9  &  1  \\
Alpine+SP4  &  5.8  &  1.4  &  0.2  &  3871.3  &  0  \\ \hline
\end{tabular}
\caption{
(\textbf{Benchmark QCQPs}) Statistics of solution times. 
Columns correspond to the shifted geometric mean, median, minimum, and maximum times over the subset of $140$ instances that do not hit the time limit. 
The last column denotes the number of instances for which each method hits the time limit.
}
\label{tab:baron_library}
\end{table}




\begin{table}[t]
\centering
% \resizebox{\columnwidth}{!}{%
\begin{tabular}{ c | c | c c c c | c }
\hline
Problem Family & Solution Method & \multicolumn{4}{c|}{Solution Time (seconds)} & \\
& & Shifted GM & Median & Min & Max & \# TLE \\ \hline
&  Alpine (default)  &  0.51  &  0.47  &  0.14  &  2.41  &  0  \\
Bilinear $n = 10$  &  Alpine+SP2  &  0.11  &  0.10  &  0.06  &  0.28  &  0  \\
&  Alpine+ML2  &  0.15  &  0.10  &  0.06  &  1.64  &  0  \\[0.1in]
&  Alpine (default)  &  21.4  &  21.9  &  5.1  &  161.5  &  0  \\
&  Alpine+SP2  &  4.2  &  2.0  &  0.8  &  132.6  &  0  \\
Bilinear $n = 20$  &  Alpine+ML2  &  10.0  &  7.8  &  1.1  &  116.0  &  0  \\
&  Alpine+SP4  &  2.4  &  1.9  &  0.8  &  94.2  &  0  \\
&  Alpine+ML4  &  9.3  &  7.2  &  1.0  &  117.4  &  0  \\[0.1in]
&  Alpine (default)  &  405.9  &  336.2  &  48.0  &  7135.9  &  24  \\
Bilinear $n = 50$  &  Alpine+SP2  &  52.8  &  34.9  &  4.2  &  5705.1  & 4  \\
&  Alpine+ML2  &  101.6  &  83.6  &  6.6  &  7071.7  &  5  \\[0.1in]
&  Alpine (default)  &  0.85  &  0.81  &  0.62  &  2.29  &  0  \\
QCQP $n = 10$  &  Alpine+SP2  &  0.10  &  0.09  &  0.07  &  0.27  &  0  \\
&  Alpine+ML2  &  0.27  &  0.12  &  0.07  &  2.89  &  0  \\[0.1in]
&  Alpine (default)  &  40.1  &  35.6  &  4.6  &  241.1  &  0  \\
&  Alpine+SP2  &  7.7  &  1.7  &  0.8  &  135.4  &  0  \\
QCQP $n = 20$  &  Alpine+ML2  &  13.0  &  9.5  &  1.0  &  180.1  &  0  \\
&  Alpine+SP4  &  2.4  &  1.5  &  0.7  &  125.7  &  0  \\
&  Alpine+ML4  &  9.4  &  6.4  &  0.9  &  101.2  &  0  \\[0.1in]
&  Alpine (default)  &  391.5  &  289.1  &  36.6  &  7198.2  &  0  \\
QCQP $n = 50$  &  Alpine+SP2  &  63.3  &  51.9  &  4.2  &  6055.2  &  0 \\
&  Alpine+ML2  &  100.5  &  118.2  &  5.3  &  6514.2  &  0  \\[0.1in]
&  Alpine (default)  &  242.8  &  212.5  &  25.9  &  7091.9  &  7  \\
Pooling  &  Alpine+SP2  &  66.7  &  49.7  &  1.6  &  6127.1  & 5  \\
&  Alpine+ML2  &  117.1  &  101.9  &  11.4  &  6097.0  &  1  \\ \hline
\end{tabular}
% }
\caption{
(\textbf{Solution Times}) Statistics of solution times. 
Columns correspond to the shifted geometric mean, median, minimum, and maximum times over the subset of $1000$ instances that did not hit the time limit. 
The last time column denotes the number of instances for which each method hits the time limit.
}
\label{tab:alpine_times}
\end{table}






We begin by benchmarking the hardness of our instances using BARON.
We then compare the performance of default Alpine with the use of strong partitioning and its ML approximation (described in Section~\ref{subsec:ml_approx}) within Alpine through a few metrics.
All reported times are in seconds and do not include the time for solving the max-min problem~\eqref{eqn:strong_part} or training the ML model. 



\subsubsection{Benchmarking using BARON}
\label{subsubsec:baron_benchmark}


To illustrate the non-trivial nature of our instances, we present statistics of their run times using BARON in Table~\ref{tab:baron_times}.
BARON solves the $10$ variable and $20$ variable random bilinear and QCQP instances within seconds, but takes over $4$ minutes on average to solve the $50$ variable instances and times out on $19/1000$ of the $50$ variable instances with univariate quadratic terms.
BARON finds the random pooling instances to be significantly harder, timing out on $432/1000$ instances\footnote{BARON finds global solutions but is unable to prove global optimality within the time limit.} and taking roughly $7$ minutes to solve the remaining $568/1000$ instances on average.
As suggested in the literature, we use the shifted geometric mean to compare the solution times of different algorithms on a family of test instances.
The shifted geometric mean (shifted GM) of a positive vector $t \in \R^N_+$ is defined as\footnote{\url{http://plato.asu.edu/ftp/shgeom.html}}:
\[
\text{Shifted GM}(t) = \exp\Bigl(\frac{1}{N}\sum_{i=1}^{N} \ln\bigl(\max(1, t_i + \text{shift})\bigr) \Bigr) - \text{shift},
\]
where we set $\textit{shift} = 10$ when comparing solution times in seconds.
The last column in Table~\ref{tab:baron_times} notes the GM of the relative optimality gap at termination for instances where BARON hits the time limit.
Following the definition of relative optimality gap in Alpine, this residual optimality gap is defined as $\frac{\text{UB} - \text{LB}}{10^{-6} + \abs{\text{UB}}}$, where $\text{UB}$ and $\text{LB}$ are the upper and lower bounds returned by BARON at termination. 
We \textit{emphasize} that our goal is not to compare the different versions of Alpine with BARON but rather to illustrate that our instances and accelerations of Alpine are non-trivial.







\subsubsection{Evaluating strong partitioning on benchmark QCQPs}

We compare Alpine's default partitioning strategy with the use of two/four strong {\pp} excluding the bounds (Alpine+SP2 and Alpine+SP4, respectively, see Algorithm~\ref{alg:enhancements}) on a subset of BARON's QCQP test library\footnote{These 140 ``\texttt{qcqp2}'' instances are from \url{https://minlp.com/nlp-and-minlp-test-problems}}~\cite{bao2011semidefinite}.
Specifically, we only consider the $140$ QCQP instances from Bao et al.\ \cite{bao2011semidefinite} with $20$ variables in order to keep the time for solving the max-min problem manageable.
Table~\ref{tab:baron_library} presents statistics of the run times of default Alpine, Alpine+SP2, and Alpine+SP4 on these $140$ instances.
Alpine+SP2 and Alpine+SP4 are able to reduce the shifted GM of Alpine's solution time by factors\footnote{These factors correspond to $69.7\%$ and $81.1\%$ average reductions in Alpine's solution times.} of $3.3$ and $5.3$, respectively, which indicates that strong partitioning has the potential to result in significant speedups on broad families of QCQPs.
Table~\ref{tab:maxmin_times} reports statistics of the solution times for the max-min problem over these $140$ instances.







\begin{figure}
\centering
\begin{subfigure}[b]{0.33\textwidth}
\includegraphics[width=\textwidth]{figures/var10_solution_profile_2.eps}
\end{subfigure}%
\hfill
\begin{subfigure}[b]{0.33\textwidth}
\includegraphics[width=\textwidth]{figures/var20_solution_profile_5.eps}
\end{subfigure}%
\hfill
\begin{subfigure}[b]{0.33\textwidth}
\includegraphics[width=\textwidth]{figures/var50_solution_profile_2.eps}
\end{subfigure}\\[0.1in]
\begin{subfigure}[b]{0.33\textwidth}
\includegraphics[width=\textwidth]{figures/var10_hist_gap.eps}
\end{subfigure}%
\hfill
\begin{subfigure}[b]{0.33\textwidth}
\includegraphics[width=\textwidth]{figures/var20_hist_gap.eps}
\end{subfigure}%
\hfill
\begin{subfigure}[b]{0.33\textwidth}
\includegraphics[width=\textwidth]{figures/var50_hist_gap.eps}
\end{subfigure}
\caption{
(\textbf{Bilinear Instances}) Top row: solution profiles indicating the percentage of instances solved by the different methods within time $T$ seconds (higher is better). 
Bottom row: histogram plots of the ratios of the effective optimality gaps~\eqref{eqn:effective_gap} of default Alpine with Alpine+SP2 and with Alpine+ML2 after one iteration (larger gap reduction factors are better).}
\label{fig:plots_bilinear}
\end{figure}







\subsubsection{Evaluating the performance of strong partitioning and its ML approximation}


We compare Alpine's default partitioning strategy with the use of two strong {\pp} (excluding variable bounds) per partitioned variable (Alpine+SP2) and its ML approximation (Alpine+ML2) in Alpine's first iteration.
For the cases with $n=20$, we also compare the above approaches with the use of four strong {\pp} (excluding the bounds) per partitioned variable (Alpine+SP4) and its ML approximation (Alpine+ML4)  at Alpine's first iteration.
We compare these methods for each family of instances using two metrics: i.\ statistics of solution times, and ii.\ statistics of the effective optimality gap after Alpine's first iteration.
We define the \textit{effective} relative optimality gap as
\begin{align}
\label{eqn:effective_gap}
\text{Effective Optimality Gap } &= \max \biggl\{ 10^{-4}, \frac{v^* - v^{\text{LBD}}}{10^{-6} + \abs{v^*}} \biggr\},
\end{align}
where $v^*$ is the optimal objective value, $v^{\text{LBD}}$ is Alpine's lower bound after one iteration (using any one of the different approaches for specifying partitions), and $10^{-4}$ is the target optimality gap.
By measuring the gap of $v^{\text{LBD}}$ relative to the optimal objective value $v^*$ instead of the best found feasible solution, we do not let the performance of the local solver impact our evaluation of the different partitioning methods.
Thresholding the optimality gap at $10^{-4}$ also lends equal importance to all optimality gaps less than the target since all such gaps are sufficient for Alpine to converge.



Table~\ref{tab:alpine_times} presents statistics of run times of default Alpine, Alpine with the different versions of strong partitioning at the first iteration, and Alpine with the different ML approximations of strong partitioning at the first iteration for the different problem families.
Table~\ref{tab:alpine_speedup} records the speedup/slowdown of the different versions of Alpine+SP and Alpine+ML over default Alpine. 
Table~\ref{tab:alpine_gaps} presents statistics of the effective optimality gaps~\eqref{eqn:effective_gap} of the different approaches after one iteration, whereas Table~\ref{tab:alpine_tle_gaps} notes the GM of the residual effective optimality gaps on instances for which the different approaches hit the time limit.
Table~\ref{tab:maxmin_times} reports statistics of the solution times for the max-min problem for the different problem families.
Figures~\ref{fig:plots_bilinear},~\ref{fig:plots_qcqp}, and~\ref{fig:plots_pooling} plot solution profiles and histograms of the factor improvements of the effective optimality gaps 
for the bilinear, QCQP, and pooling families.
We do not plot performance profiles due to their known issues (see \url{http://plato.asu.edu/bench.html}).







\begin{figure}
\centering
\begin{subfigure}[b]{0.33\textwidth}
\includegraphics[width=\textwidth]{figures/qcqps_var10_solution_profile.eps}
\end{subfigure}%
\hfill
\begin{subfigure}[b]{0.33\textwidth}
\includegraphics[width=\textwidth]{figures/qcqps_var20_solution_profile.eps}
\end{subfigure}%
\hfill
\begin{subfigure}[b]{0.33\textwidth}
\includegraphics[width=\textwidth]{figures/qcqps_var50_solution_profile.eps}
\end{subfigure}\\[0.1in]
\begin{subfigure}[b]{0.33\textwidth}
\includegraphics[width=\textwidth]{figures/qcqps_var10_hist_gap.eps}
\end{subfigure}%
\hfill
\begin{subfigure}[b]{0.33\textwidth}
\includegraphics[width=\textwidth]{figures/qcqps_var20_hist_gap.eps}
\end{subfigure}%
\hfill
\begin{subfigure}[b]{0.33\textwidth}
\includegraphics[width=\textwidth]{figures/qcqps_var50_hist_gap.eps}
\end{subfigure}
\caption{
(\textbf{QCQP Instances}) Top row: solution profiles indicating the percentage of instances solved by the different methods within time $T$ seconds (higher is better). 
Bottom row: histogram plots of the ratios of the effective optimality gaps~\eqref{eqn:effective_gap} of default Alpine with Alpine+SP2 and with Alpine+ML2 after one iteration (larger gap reduction factors are better).}
\label{fig:plots_qcqp}
\end{figure}





\begin{figure}
\centering
\begin{subfigure}[b]{0.48\textwidth}
\centering
\includegraphics[width=0.7\textwidth,right]{figures/pooling_solution_profile.eps}
\end{subfigure}%
\hfill
\begin{subfigure}[b]{0.48\textwidth}
\centering
\includegraphics[width=0.7\textwidth,left]{figures/pooling_hist_gap.eps}
\end{subfigure}
\caption{\textbf{(Pooling Instances)} Left plot: solution profile indicating the percentage of instances solved by the different methods within time $T$ seconds (higher is better). Right plot: histogram plots of the ratios of the effective optimality gaps~\eqref{eqn:effective_gap} of default Alpine with Alpine+SP2 and with Alpine+ML2 after one iteration (larger gap reduction factors are better).}
\label{fig:plots_pooling}
\end{figure}







\begin{table}[t]
\centering
\resizebox{\textwidth}{!}{%
\begin{tabular}{ c | c | c c c c c c c c }
\hline
Problem Family & Solution Method & \multicolumn{8}{c}{Speedup/Slowdown Factor} \\
& & $<0.5$ & $0.5 - 1$ & $1 - 2$ & $2 - 5$ & $5 - 10$ & $10 - 20$ & $20 - 50$ & $> 50$ \\ \hline
Bilinear $n = 10$ & \% Alpine+SP2 inst.\  &  $\underline{\hspace{0.3cm}}$  &  $\underline{\hspace{0.3cm}}$  &  1.1  &  57.6  &  40.1  &  1.2  &  0  &  0 \\
& \% Alpine+ML2 inst.\  &  0.2  &  2.1  &  7.7  &  49.9  &  40.0  &  0.1  &  0  &  0 \\[0.1in]
& \% Alpine+SP2 inst.\  &  0.2  &  3.3  &  7.2  &  18.2  &  31.2  &  29.9  &  10.0  &  0.0 \\
Bilinear $n = 20$ & \% Alpine+ML2 inst.\  &  3.3  &  9.8  &  25.5  &  39.2  &  15.3  &  6.0  &  0.9  &  0.0 \\
& \% Alpine+SP4 inst.\  &  0.2  &  0.7  &  1.3  &  13.4  &  32.7  &  37.1  &  14.5  &  0.1 \\
& \% Alpine+ML4 inst.\  &  2.8  &  10.5  &  23.3  &  41.4  &  15.2  &  5.9  &  0.9  &  0.0 \\[0.1in]
Bilinear $n = 50$ & \% Alpine+SP2 inst.\  &  0.4  &  1.3  &  7.2  &  18.7  &  26.3  &  24.3  &  14.9  &  6.9 \\
& \% Alpine+ML2 inst.\  &  0.7  &  4.7  &  16.9  &  32.5  &  25.3  &  13.7  &  5.4  &  0.8 \\[0.1in]
QCQP $n = 10$ & \% Alpine+SP2 inst.\  &  $\underline{\hspace{0.3cm}}$  &  $\underline{\hspace{0.3cm}}$  &  0.1  &  3.3  &  76.1  &  20.4  &  0.1  &  0 \\
& \% Alpine+ML2 inst.\  &  1.0  &  3.9  &  20.9  &  8.5  &  53.4  &  12.3  &  0  &  0 \\[0.1in]
& \% Alpine+SP2 inst.\  &  0.1  &  3.2  &  12.2  &  18.4  &  11.5  &  19.4  &  32.6  &  2.6 \\
QCQP $n = 20$ & \% Alpine+ML2 inst.\  &  0.5  &  5.1  &  19.0  &  40.7  &  23.1  &  9.6  &  1.9  &  0.1 \\
& \% Alpine+SP4 inst.\  &  0  &  0.2  &  1.3  &  3.8  &  5.5  &  28.2  &  53.7  &  7.3 \\
& \% Alpine+ML4 inst.\  &  0  &  2.9  &  11.6  &  33.3  &  27.0  &  17.2  &  7.6  &  0.4 \\[0.1in]
QCQP $n = 50$ & \% Alpine+SP2 inst.\  &  0.9  &  1.3  &  10.7  &  22.0  &  23.0  &  32.5  &  7.2  &  2.4 \\
& \% Alpine+ML2 inst.\  &  1.4  &  4.0  &  19.5  &  32.4  &  22.7  &  16.6  &  3.4  &  0 \\[0.1in]
Pooling & \% Alpine+SP2 inst.\  &  2.2  &  6.4  &  19.7  &  26.0  &  21.8  &  16.8  &  6.7  &  0.4 \\
& \% Alpine+ML2 inst.\  &  2.1  &  11.5  &  34.5  &  40.4  &  9.8  &  1.4  &  0.3  &  0 \\ \hline
\end{tabular}%
}
\caption{
(\textbf{Speedup/Slowdown}) Statistics of the speedup/slowdown of the different versions of Alpine with SP and its ML approximation (relative to default Alpine).
}
\label{tab:alpine_speedup}
\end{table}





\paragraph{Bilinear instances.}
Table~\ref{tab:alpine_times} implies Alpine+SP2 is able to reduce the shifted GM of default Alpine's solution time by factors of $4.5$, $5.1$, and $7.7$, respectively, for $n = 10$, $n = 20$, and $n = 50$ over $1000$ instances.
Alpine+ML2 is able to generate a moderate approximation of Alpine+SP2 overall, reducing the shifted GM of default Alpine's solution time by factors of $3.5$, $2.1$, and $4$, respectively, for $n = 10$, $n = 20$, and $n = 50$ over the same $1000$ instances.
For the $n = 20$ instances, Alpine+SP4 and Alpine+ML4 reduce the shifted GM of default Alpine's solution time by factors of $9$ and $2.3$, respectively.
Table~\ref{tab:alpine_speedup} implies Alpine+SP2 results in at least $5\times$ speedup over default Alpine on $41.3\%$ of the $n = 10$ instances, and results in at least $10\times$ speedup on $39.9\%$ and $46.1\%$ of the $n = 20$ and $n = 50$ instances, respectively.
On the other hand, Alpine+ML2 yields at least $5\times$ speedup over default Alpine on $40.1\%$, $22.2\%$, and $45.2\%$ of the $n = 10$, $n = 20$, and $n = 50$ instances.
Alpine+SP4 results in at least $10\times$ speedup over default Alpine on $51.7\%$ of the $n = 20$ instances.
Finally, Alpine+SP2 results in a maximum speedup of $15\times$, $49\times$, and $685\times$ for the $n = 10$, $n = 20$, and $n = 50$ instances, whereas Alpine+ML2 results in a maximum speedup of $13\times$, $38\times$, and $197\times$ for the same sets of instances.

Table~\ref{tab:alpine_gaps} implies Alpine+SP2 reduces the GM of default Alpine's effective optimality gap~\eqref{eqn:effective_gap} after the first iteration by factors of $5.5$, $2200$, and $80$, respectively, for $n = 10$, $n = 20$, and $n = 50$.
Alpine+ML2 reduces the GM of default Alpine's effective gap after the first iteration by factors of $4.6$, $180$, and $15$, respectively, for the $n = 10$, $n = 20$, and $n = 50$ instances.
Interestingly, Alpine+SP2 is able to close the effective gap in the first iteration for $100\%$, $82.3\%$, and $46\%$ of the $n = 10$, $n = 20$, and $n = 50$ instances, whereas default Alpine is able to close the gap in the first iteration for at most $0.1\%$ of the instances for these different problem families, which demonstrates the effectiveness of the strong partitioning strategy.
Finally, 
Table~\ref{tab:alpine_tle_gaps} shows that Alpine+SP2 and Alpine+ML2 terminate with smaller average optimality gaps on the $n = 50$ instances where they time out compared to Alpine.











\begin{table}[t]
\centering
% \resizebox{\textwidth}{!}{%
\begin{tabular}{ c | c | c c c c | c }
\hline
Problem Family & Solution Method & \multicolumn{4}{c|}{Effective Optimality Gap after $1$ iteration} & \% Instances \\
& & GM & Median & Min & Max & Gap Closed \\ \hline
&  Alpine (default)  &  $5.5 \times 10^{-4}$  &  $4.5 \times 10^{-4}$  &  $10^{-4}$  &  $3.4 \times 10^{-2}$  &  $0.1$ \\
Bilinear $n = 10$  &  Alpine+SP2  &  $10^{-4}$  &  $10^{-4}$  &  $10^{-4}$  &  $10^{-4}$  &  $100$ \\
&  Alpine+ML2  &  $1.2 \times 10^{-4}$  &  $10^{-4}$  &  $10^{-4}$  &  $4.4 \times 10^{-2}$  &  $88.3$ \\[0.1in]
&  Alpine (default)  &  $2.9 \times 10^{-1}$  &  $3.3 \times 10^{-1}$  &  $7.3 \times 10^{-2}$  &  $4.8 \times 10^{-1}$  &  $0$ \\
&  Alpine+SP2  &  $1.3 \times 10^{-4}$  &  $10^{-4}$  &  $10^{-4}$  &  $6 \times 10^{-3}$  &  $82.3$ \\
Bilinear $n = 20$  &  Alpine+ML2  &  $1.6 \times 10^{-3}$  &  $1.9 \times 10^{-3}$  &  $10^{-4}$  &  $1.4 \times 10^{-1}$  &  $18.9$ \\
&  Alpine+SP4  &  $1.0 \times 10^{-4}$  &  $10^{-4}$  &  $10^{-4}$  &  $4.7 \times 10^{-4}$  &  $96.0$ \\
&  Alpine+ML4  &  $2.2 \times 10^{-3}$  &  $3.6 \times 10^{-3}$  &  $10^{-4}$  &  $9.9 \times 10^{-2}$  &  $14.5$ \\[0.1in]
&  Alpine (default)  &  $1.4 \times 10^{-2}$  &  $1.7 \times 10^{-2}$  &  $10^{-4}$  &  $6.9 \times 10^{-2}$ & $0.1$  \\
Bilinear $n = 50$ &  Alpine+SP2  &  $1.7 \times 10^{-4}$  &  $1.2 \times 10^{-4}$  &  $10^{-4}$  &  $5.4 \times 10^{-1}$ & $46.0$ \\
&  Alpine+ML2  &  $9.5 \times 10^{-4}$  &  $9.4 \times 10^{-4}$  &  $10^{-4}$  &  $4.9 \times 10^{-1}$ & $5.6$ \\[0.1in]
&  Alpine (default)  &  $1.3 \times 10^{-3}$  &  $1.2 \times 10^{-3}$  &  $7.5 \times 10^{-4}$  &  $1.9 \times 10^{-2}$  &  $0$ \\
QCQP $n = 10$  &  Alpine+SP2  &  $10^{-4}$  &  $10^{-4}$  &  $10^{-4}$  &  $10^{-4}$  &  $100$ \\
&  Alpine+ML2  &  $3.0 \times 10^{-4}$  &  $10^{-4}$  &  $10^{-4}$  &  $1.3 \times 10^{-1}$  &  $71.8$ \\[0.1in]
&  Alpine (default)  &  $6.3 \times 10^{-2}$  &  $7.8 \times 10^{-2}$  &  $3.0 \times 10^{-3}$  &  $2.1 \times 10^{-1}$  &  $0$ \\
&  Alpine+SP2  &  $2.1 \times 10^{-4}$  &  $10^{-4}$  &  $10^{-4}$  &  $6.6 \times 10^{-3}$  &  $52.2$ \\
QCQP $n = 20$  &  Alpine+ML2  &  $2.0 \times 10^{-3}$  &  $2.5 \times 10^{-3}$  &  $10^{-4}$  &  $5.8 \times 10^{-2}$  &  $2.0$ \\
&  Alpine+SP4  &  $1.1 \times 10^{-4}$  &  $10^{-4}$  &  $10^{-4}$  &  $3.6 \times 10^{-3}$  &  $92.6$ \\
&  Alpine+ML4  &  $1.5 \times 10^{-3}$  &  $1.7 \times 10^{-3}$  &  $10^{-4}$  &  $6.7 \times 10^{-2}$  &  $14.7$ \\[0.1in]
&  Alpine (default)  &  $8.1 \times 10^{-3}$  &  $1.0 \times 10^{-2}$  &  $6.3 \times 10^{-4}$  &  $2.8 \times 10^{-2}$ & $0$  \\
QCQP $n = 50$ &  Alpine+SP2  &  $1.6 \times 10^{-4}$  &  $1.3 \times 10^{-4}$  &  $10^{-4}$  &  $1.0 \times 10^{-3}$ & $39.0$ \\
&  Alpine+ML2  &  $4.8 \times 10^{-4}$  &  $5.3 \times 10^{-4}$  &  $10^{-4}$  &  $1.5 \times 10^{-2}$ & $14.9$ \\[0.1in]
&  Alpine (default)  &  $6.8 \times 10^{-3}$  &  $6.4 \times 10^{-3}$  &  $1.2 \times 10^{-3}$  &  $4.4 \times 10^{-2}$ & $0$  \\
Pooling &  Alpine+SP2  &  $2.4 \times 10^{-4}$  &  $1.4 \times 10^{-4}$  &  $10^{-4}$  &  $3.1 \times 10^{-3}$ & $45.2$ \\
&  Alpine+ML2  &  $1.5 \times 10^{-3}$  &  $1.6 \times 10^{-3}$  &  $10^{-4}$  &  $6.3 \times 10^{-3}$ & $0.1$ \\ \hline
\end{tabular}%
% }
\caption{
(\textbf{Effective Optimality Gaps}) Statistics of effective optimality gaps~\eqref{eqn:effective_gap} after one iteration (note: minimum possible value $= 10^{-4}$, the target gap). 
Columns record the geometric mean, median, minimum, and maximum effective gaps over $1000$ instances. 
The last column is the percentage of instances for which each method results in the minimum possible effective optimality gap of $10^{-4}$ after one iteration.
}
\label{tab:alpine_gaps}
\end{table}








\begin{table}[t]
\centering
\resizebox{\columnwidth}{!}{%
\begin{tabular}{ c | c c c | c c c }
\hline
Problem Family & \multicolumn{3}{c|}{Bilinear $n = 50$} & \multicolumn{3}{c}{Pooling} \\[0.02in]
Method & Alpine (default) & Alpine+SP2 & Alpine+ML2 & Alpine (default) & Alpine+SP2 & Alpine+ML2 \\ \hline 
& & & & & & \\[-0.1in]
TLE Gap (GM) &  $4.4 \times 10^{-4}$  &  $1.6 \times 10^{-4}$  &  $1.8 \times 10^{-4}$  &  $2.9 \times 10^{-4}$  &  $2.1 \times 10^{-4}$  &  $2.8 \times 10^{-4}$ \\[0.02in] \hline
\end{tabular}
}
\caption{
(\textbf{Effective TLE Optimality Gaps}) Geometric mean of residual effective optimality gaps (target $= 10^{-4}$) on instances for which methods hit the time limit.
}
\label{tab:alpine_tle_gaps}
\end{table}









\begin{table}
\centering
% \resizebox{\columnwidth}{!}{%
\begin{tabular}{ r | c | c c c c c }
\hline
Problem Family & Solution Method & \multicolumn{5}{c}{Max-Min Solution Time (seconds)} \\
& & Shifted GM & Median & Min & Max & Std.\ Dev.\ \\ \hline
Bilinear $n = 10$  &  SP2  &  16  &  14  &  6  &  96  &  13  \\[0.05in]
Bilinear $n = 20$ &  SP2  &  528  &  445  &  136  &  2389  &  544  \\
&  SP4  &  1244  &  1117  &  374  &  4360  &  893  \\[0.05in]
Bilinear $n = 50$  &  SP2  &  7070  &  7404  &  1271  &  23166  &  3268  \\[0.1in]
QCQP $n = 10$  &  SP2  &  8  &  8  &  6  &  53  &  3  \\[0.05in]
QCQP $n = 20$ &  SP2  &  1731  &  1826  &  171  &  4244  &  654  \\
&  SP4  &  2152  &  2740  &  471  &  5965  &  961  \\[0.05in]
QCQP $n = 50$  &  SP2  &  16964  &  17074  &  8626  &  23551  &  2319  \\[0.05in]
Pooling  &  SP2  &  15658  &  15148  &  1088  &  77029  &  8657  \\[0.05in]
Benchmark QCQPs  &  SP2  &  413  &  364  &  7  &  27907  &  4432  \\
&  SP4  &  895  &  651  &  12  &  136320  &  15444  \\ \hline
\end{tabular}
% }
\caption{
(\textbf{Max-Min Solution Times}) Statistics of max-min solution times. 
Columns correspond to the shifted geometric mean, median, minimum, maximum, and standard deviation of times for solving the max-min problem~\eqref{eqn:strong_part}.
}
\label{tab:maxmin_times}
\end{table}






\paragraph{QCQP instances.}
Table~\ref{tab:alpine_times} implies Alpine+SP2 is able to reduce the shifted GM of default Alpine's solution time by factors of $8.4$, $5.2$, and $6.2$, respectively, for $n = 10$, $n = 20$, and $n = 50$.
Alpine+ML2 is able to generate a moderate approximation of Alpine+SP2, reducing the shifted GM of default Alpine's solution time by factors of $3.1$, $3.1$, and $3.9$, respectively, for $n = 10$, $n = 20$, and $n = 50$ over the same $1000$ instances.
For the $n = 20$ instances, Alpine+SP4 and Alpine+ML4 reduce the shifted GM of default Alpine's solution time by factors of $16.4$ and $4.3$, respectively.
Table~\ref{tab:alpine_speedup} implies Alpine+SP2 results in at least $10\times$ speedup over default Alpine on $20.5\%$, $54.6\%$, and $42.1\%$ of the $n = 10$, $n = 20$, and $n = 50$ instances, respectively.
On the other hand, Alpine+ML2 yields at least $5\times$ speedup over default Alpine on $65.7\%$, $34.7\%$, and $42.7\%$ of the $n = 10$, $n = 20$, and $n = 50$ instances.
Alpine+SP4 results in at least $20\times$ speedup over default Alpine on $61\%$ of the $n = 20$ instances.
Finally, Alpine+SP2 results in a maximum speedup of $22\times$, $87\times$, and $98\times$ for the $n = 10$, $n = 20$, and $n = 50$ instances, whereas Alpine+ML2 results in a maximum speedup of $19\times$, $56\times$, and $32\times$ for the same sets of instances.

Table~\ref{tab:alpine_gaps} implies Alpine+SP2 reduces the GM of default Alpine's effective optimality gap~\eqref{eqn:effective_gap} after the first iteration by factors of $13$, $300$, and $50$, respectively, for the $n = 10$, $n = 20$, and $n = 50$ instances.
On the other hand, Alpine+ML2 reduces the GM of default Alpine's effective gap after the first iteration by factors of $4.3$, $31$, and $17$, respectively, for $n = 10$, $n = 20$, and $n = 50$.
Note that Alpine+SP2 is able to close the effective gap in the first iteration for $100\%$, $52.2\%$, and $39\%$ of the $n = 10$, $n = 20$, and $n = 50$ instances, whereas default Alpine is unable to close the gap in the first iteration for any instance in these problem families.







\paragraph{Pooling instances.}
Table~\ref{tab:alpine_times} implies Alpine+SP2 and Alpine+ML2 reduce the shifted GM of default Alpine's solution time by factors of $3.6$ and $2.1$ over the $1000$ instances.
Table~\ref{tab:alpine_speedup} implies Alpine+SP2 and Alpine+ML2 result in at least $5\times$ speedup over default Alpine on $45.7\%$ and $11.5\%$ of the instances, respectively.
Table~\ref{tab:alpine_gaps} implies Alpine+SP2 and Alpine+ML2 reduce the GM of default Alpine's effective optimality gap~\eqref{eqn:effective_gap} after the first iteration by factors of $28$ and $4.5$, respectively.
After the first iteration, Alpine+SP2 closes the effective optimality gap for $45.2\%$ of the instances, whereas default Alpine is unable to close the gap for any of the $1000$ instances.
Finally, Alpine+SP2 and Alpine+ML2 result in maximum speedups of $120\times$ and $41\times$.



\paragraph{Summary.} Tables~\ref{tab:alpine_times} to~\ref{tab:alpine_gaps} and Figures~\ref{fig:plots_bilinear} to~\ref{fig:plots_pooling} clearly show the benefits of strong partitioning and its ML approximation over Alpine's default partitioning strategy.
They also demonstrate that Alpine+SP and Alpine+ML are able to match or even outperform (particularly on the pooling instances) the performance of the state-of-the-art solver BARON (with default options) on average over the different problem families.
While our off-the-shelf ML model is able to yield a moderate approximation of SP across these different problem families, there is a clear scope for significant improvement with tailored ML approaches.









\section{Future work}
\label{sec:conclusion}



There are several interesting avenues for future work.

First, instead of prespecifying the number of {\pp} per variable for SP, we could allocate a different number of partitions per variable based on their relative impact on the lower bound.
Suppose we wish to specify \textit{at most} $d+2$ {\pp} for each variable and are given a budget $B \in [d \times n]$ for the total number of {\pp} across all variables (excluding variable bounds).
We can solve the following max-min problem to determine both the optimal allocation of partitions \textit{and} the optimal specification of {\pp} across the partitioned variables:
\begin{align*}
&\max_{(P,Z) \in \mathfrak{P}_z} \underline{v}(P),
\end{align*}
where $P := (p^1, p^2, \dots, p^n)$ denotes the (potential) {\pp}, $\underline{v}(P)$ is defined in~\eqref{eqn:piecewise_mccormick_mip_oa}, and $Z := (z^1, z^2, \dots, z^n)$ is a $d \times n$ matrix of binary decisions. The partitioning point $p^i_j$ is added to the partition $\mathcal{P}_i$ of $x_i$ only if the variable $z^i_j = 1$.
Finally, the MILP representable set $\mathfrak{P}_Z$ is defined as
{
\small
\[
\mathfrak{P}_Z := \biggl\{ (P,Z) \in \mathfrak{P} \times \{0,1\}^{d \times n} \: : \: \sum_{i=1}^{n} \sum_{j=1}^{d} z^i_j = B, \:\: z^i_j = 0 \implies p^i_j = 0, \: \forall (i,j) \in [n] \times [d] \biggr\}.
\]
}%
If $z^i_j = 0$, then the partitioning point $p^i_j$ is made redundant by forcing it to $0$.
Note that the above outer-maximization problem involves binary decision variables $Z$ unlike the strong partitioning problem~\eqref{eqn:strong_part}, which necessitates new techniques for its solution.

Second, designing more efficient approaches for solving the strong partitioning problem~\eqref{eqn:strong_part} would help scale our approach to larger problem dimensions (and also make it easier to generating more training data).
Third, designing tailored ML architectures that can achieve similar speedups as strong partitioning and can accommodate variable feature and output dimensions merits investigation.
Fourth, motivated by the cluster problem~\cite{kannan2017cluster,kannan2018convergence}, it would be interesting to explore variants that choose a different subset of variables to be partitioned at each iteration within Alpine.
Finally, using strong partitioning to choose Alpine's partitions at the second iteration and beyond can help promote convergence of its bounds in fewer iterations.










\section*{Acknowledgments}
The authors gratefully acknowledge funding from Los Alamos National Laboratory's (LANL's) ``Center for Nonlinear Studies'' and the U.S.\ Department of Energy's ``Laboratory Directed Research and Development (LDRD)'' program under the projects ``20230091ER: Learning to Accelerate Global Solutions for Non-convex Optimization'' and ``20210078DR: The Optimization of Machine Learning: Imposing Requirements on Artificial Intelligence.''
This research used resources provided by LANL's Darwin testbed, which is funded by the Computational Systems and Software Environments subprogram of LANL's Advanced Simulation and Computing program (NNSA/DOE).








{
\footnotesize
\section*{References}
\begingroup
\renewcommand{\section}[2]{}%
\bibliographystyle{abbrvnat}
\bibliography{main}
\endgroup
}


\end{document}
