% \section*{Acknowledgements}

% This document has been adapted
% by Steven Bethard, Ryan Cotterell and Rui Yan
% from the instructions for earlier ACL and NAACL proceedings, including those for 
% ACL 2019 by Douwe Kiela and Ivan Vuli\'{c},
% NAACL 2019 by Stephanie Lukin and Alla Roskovskaya, 
% ACL 2018 by Shay Cohen, Kevin Gimpel, and Wei Lu, 
% NAACL 2018 by Margaret Mitchell and Stephanie Lukin,
% Bib\TeX{} suggestions for (NA)ACL 2017/2018 from Jason Eisner,
% ACL 2017 by Dan Gildea and Min-Yen Kan, 
% NAACL 2017 by Margaret Mitchell, 
% ACL 2012 by Maggie Li and Michael White, 
% ACL 2010 by Jing-Shin Chang and Philipp Koehn, 
% ACL 2008 by Johanna D. Moore, Simone Teufel, James Allan, and Sadaoki Furui, 
% ACL 2005 by Hwee Tou Ng and Kemal Oflazer, 
% ACL 2002 by Eugene Charniak and Dekang Lin, 
% and earlier ACL and EACL formats written by several people, including
% John Chen, Henry S. Thompson and Donald Walker.
% Additional elements were taken from the formatting instructions of the \emph{International Joint Conference on Artificial Intelligence} and the \emph{Conference on Computer Vision and Pattern Recognition}.

\section{Results}
\respace
\label{sec:results}

\subsection{Comparing Acquisition Methods}
\respace
    
    Results in Figure~\ref{fig:performances} show the experiments described in Section~\ref{sec:experiments}: benchmarking each acquisition method for multi-domain active learning.
    We observe for both question answering (QA) and sentiment analysis (SA), most methods manage to outperform the no-extra-labelled data baseline (0\% at the y-axis) and very narrowly outperform the random selection baseline (\textcolor{myred}{red} line).
    %Consistent with prior work \citep{lowell2019practical} random selection is a very robust baseline.
    % Consistent with prior work \citep{lowell2019practical}, many active learning strategies have brittle performance and random selection is difficult to beat consistently.
    Consistent with prior work \citep{lowell2019practical}, active learning strategies in NLP have brittle and inconsistent improvements over random selection.
    % The mean improvements are larger for QA than SA, because QA models are more brittle with few data points.
    Our main empirical findings, described in this section, include:
    \respace
    \begin{itemize}\itemsep0em
        \item \textbf{$\mathcal{H}$-Divergence}, and particularly \dale{} variants, consistently outperform baselines and other families of methods.
        \item The ordering of examples in \textbf{Uncertainty methods} depend significantly on the diversity in source domains.
        \bald{} variants perform best among available options.
        \item Task-agnostic representations, used in \textbf{\knn{}} or \textbf{\dal{}} variants, provide consistently strong results on average, but task-specific representations significantly benefit certain target sets.
        \item Different families of methods rely on orthogonal notions of \emph{relevance} in producing their example rankings.
    \end{itemize}
    
\begin{figure*}
    \vspace*{-.6cm}
        \centering
        \begin{subfigure}{.83\textwidth}
          \includegraphics[width=\textwidth]{figures/sent-perfs-legend-v3.pdf}
          \caption{\label{fig:sent-performances}
          \small \textbf{Sentiment Analysis} performance improvement (Accuracy \%) by acquisition method.}
        \end{subfigure}
        
        \begin{subfigure}{.83\textwidth}
          \includegraphics[width=\textwidth]{figures/mrqa-perfs-legend-v3.pdf}
          \caption{\small \textbf{Question Answering} performance improvement (F1 \%) by acquisition method.}
          \label{fig:mrqa-performances}
        \end{subfigure}
        
        \caption{\textbf{Performance by Method:} The improvement of each acquisition method over the model given no extra labelled data.
        Boxplot and whiskers denote the median, quartiles and min/max scores aggregated across each target domain and sample sizes ($n=\{8000, 18000, 28000\}$).
        The \textcolor{myred}{red} line represents the median performance of a baseline that randomly selects examples to annotate.
        }
    \label{fig:performances}
    \vspace*{-.7cm}
    \end{figure*}
    
    \textbf{$\mathcal{H}$-Divergence} methods categorically achieved the highest and most reliable scores, both as a family and individual methods, represented in the top 3 individual methods 11 / 18 times for QA, and 20 / 24 times for SA.
    For QA, \balda{} and \dales{} had the best mean and median scores respectively, and for SA \dale{} achieved both the best mean and median scores. 
    Among these methods, our proposed \dale{} variants routinely outperform \dalt{} variants by a small margin on average, with equivalent training and tuning procedures.
    We believe this is because \dale{} captures both notions of domain similarity and uncertainty. 
    By design it prioritizes examples that are similar to in-domain samples, but also avoids those which are uninformative, because the model already performs well on them. 
    
    Among \textbf{Uncertainty methods}, for SA methods which select for higher uncertainty vastly outperformed those which selected for low uncertainty.
    The opposite is true for QA.
    This suggests the diversity of QA datasets contain more extreme (harmful) domain shift than the (mostly Amazon-based) SA datasets.\footnote{Accordingly, we attempt to derive a relationship between domain distance and method performance in Appendix~\ref{appendix:domain-distances}, but find intuitive calculations of domain distance uninterpretable.}
    In both settings, the right ordering of examples with \bald{} (\emph{epistemic} uncertainty) achieves the best results in this family of methods, over the others, which rely on \emph{total} uncertainty.
    
    Among \textbf{Reverse Classification Accuracy} methods, our \textbf{\rcas{}} variant also noticeably outperforms standard \rca{} and most other methods, aside from \dal{} and \bald{}.
    Combining \rcas{} with an example ranking method is a promising direction for future work, given the performance it achieves selecting examples randomly as a \textbf{Domain Budget Allocation} strategy.
    % As we discuss in Section~\ref{sec:opt-example-selection}, this demonstrates the potential for \textbb{Domain Budget Allocation} methods over \textbb{Single Pool Strategies}.
    
    Lastly, the \textbf{Semantic Similarity Detection} set of methods only rarely or narrowly exceed random selection.
    Intuitively, task-agnostic representations (\knn{}) outperform \knns{}, given the task-agnostic sentence encoder was optimized for cosine similarity.
    
    \respace
    \paragraph{Embedding Ablations}
    To see the effects of embedding space on \knn{} and \dal{}, we used both a task-specific and task-agnostic embedding space. 
    While a task-specific embedding space reduces the examples to features relevant for the task, a task-agnostic embedding space produces generic notions of similarity, unbiased by the task model.
    
    According to Figure~\ref{fig:performances}, \knn{} outperforms \knns{}. 
    In the QA setting, \knns{}'s median is below the random baseline's. 
    In both plots, \knns{}'s whiskers extend below 0, indicating that in some cases the method actually chooses source examples that are harmful to target domain performance. 
    
    For \dal{} methods, task-agnostic and task-specific embeddings demonstrated mostly similar median performances. 
    Notably, the boxes and whiskers are typically longer for task-specific methods than task-agnostic methods.
    This variability indicates certain target datasets may benefit significantly from task-specific embeddings, though task-agnostic embeddings achieve more consistent results.
 
 \begin{figure*}[t]
    \centering
    \includegraphics[width=0.85\textwidth]{figures/KendallsTau_combined.pdf}
    \caption{\label{fig:kendall-tau-heatmap} \textbf{Similarities of Example Rankings} Measured by Kendall's Tau Coefficients, for QA (above diagonal) and SA (below diagonal). 
    Kendall's Tau coefficient is computed between the example rankings of each pair of methods. 
    The heatmap contains these coefficients averaged over each target dataset (some cells are crossed out for SA since SA's \knn{} methods don't have C/Q/QC variants). 
    1 indicates a perfect relationship between the rankings, 0 means no relationship, and -1 means an inverse relationship.}
    \respace
    \respace
\end{figure*}
    
    \respace
    \paragraph{Comparing Example Rankings}
    For each setting, we quantify how similar acquisition methods rank examples from $D_S$.
    In Figure \ref{fig:kendall-tau-heatmap}, for each pair of methods, we calculate the Kendall's Tau coefficient between the source example rankings chosen for a target domain, then average this coefficient over the target domains. 
    Kendall's Tau gives a scores $[-1, 1]$, with -1 meaning perfect anti-correlation, 0 meaning no correlation, and 1 meaning perfect correlation between the rankings.
    Methods from different families show close to no relationship, even if they achieve similar performances, suggesting each family relies on orthogonal notions of similarity to rank example relevance.
    This suggests there is potential for combining methods from different families for this task in future work.
    
    In Sentiment tasks, all uncertainty methods had highly correlated examples.
    In QA, \entr{} had little correlation with any method. 
    This is likely due to the significantly larger output space for QA models.
    Compared to only 5 label classes in SA, question answering models distribute their start and end confidences over sequences of up to 512, where there can be multiple valid answer candidates.
    Embedding space also largely influences the examples that methods chose. 
    \dal{} methods had higher correlations with each other when they share the same embedding space; \emph{i.e.} \dale{}'s ranking has a higher correlation with \dalt{} than with \dales{}.

\respace
\subsection{Properties of Optimal Example Selection}
\label{sec:opt-example-selection}
\respace
    
    We examine three properties of optimally selected examples: (i) whether selecting from many diverse or one single domain leads to better performance, (ii) whether the selection of a domain or the individual examples matters more to performance, and (iii) whether selection strategies can benefit from source domain information rather than treating samples as drawn from a single pool?
    Our findings regarding properties of optimal selection, as described in this section, include:
    \respace
    \begin{itemize}\itemsep0em
        \item Selecting a diversity of domains usually outperforms selecting examples from a single domain. 
        \item Acquisition functions such as \dales{} do rely on example selection, mainly to avoid the possibility of large negative outcomes. 
        \item \textbf{Domain Budget Allocation} during selection may improve performance. Surprisingly, even random selection from an ``optimal'' balance of domains beats our best performing acquisition methods most of the time.  
    \end{itemize}

    \respace
    \paragraph{Are Many Diverse or One Single Domain Preferable?}
    To answer this question we conduct a full search over all combinations of source datasets.
    For each target set, we fix 2k in-domain data points and sample all combinations of other source sets in 2k increments, such that altogether there are 10k training data points.
    For each combination of source sets, we conduct a simple grid search, randomly sampling the source set examples each time, and select the best model, mimicking standard practice among practitioners.
    
    The result is a comprehensive search of all combinations of source sets (in 2k increments) up to 10k training points, so we can rank all combinations of domains per target, by performance.
    Tables \ref{tbl:opt-domain-mrqa} and \ref{tbl:opt-domain-sent} show the optimal selections, even as discrete as 2k increments, typically select at least two or more domains to achieve the best performance.
    However, 1 of 6 targets for QA, or 2 of 8 for the SA tasks achieve better results selecting all examples from a single domain, suggesting this is a strong baseline, if the right source domain is isolated.
    We also report the mean score of all permutations to demonstrate the importance of selecting the right set of domains over a random combination.
    
    \input{tables/optimal-domain-search}
    
    \respace
    \paragraph{Domains or Examples?}
    Which is more important, to select the right domains or the right examples within some domain?
    From the above optimal search experiment we see selecting the right combination of domains regularly leads to strong improvements over a random combination of domains.
    Whether example selection is more important than domain selection may vary depending on the example variety within each domain.
    % While this is a limitation of our experiments we believe these tasks still comprise a broad and realistic benchmark with which to shed light on this question.
    We narrow our focus to how much example selection plays a role for one of the stronger acquisition functions: \dales{}.
    
    We fix the effect of domain selection (the number of examples from each domain) but vary which examples are specifically selected.
    Using \dales{}'s distribution of domains, we compare the mean performance of models trained on it's highest ranked examples against a random set of examples sampled from those same domains.
    We find a \textcolor{mygreen}{+0.46} $\pm$ 0.25\% improvement for QA, and \textcolor{mygreen}{+0.12} $\pm$ 0.19\% for SA.
    We also compare model performances trained on random selection against the lowest ranked examples by \dales{}.
    Interestingly, we see a \textcolor{myred}{-1.64} $\pm$ 0.37\% performance decrease for QA, and \textcolor{myred}{-1.46} $\pm$ 0.56\% decrease for sentiment tasks.
    These results suggest that example selection is an important factor beyond domain selection, especially for avoiding bad example selections.
    % Though not optimal, random selection is also surprisingly robust.

    \respace
    \paragraph{Single Pool or Domain Budget Allocation}
    Does using information about examples' domains during selection lead to better results than treating all examples as coming from a single unlabeled pool?
    Originally, we hypothesized \textbf{Single Pool Strategy} methods would perform better on smaller budget sizes as they add the most informative data points regardless of domain. 
    On the other hand, we thought that if the budget size is large, \textbf{Domain Budget Allocation} would perform best, as they choose source domains closest to the target domain. 
    Based on Figures \ref{fig:mrqa-performances} and \ref{fig:sent-performances}, we were not able to draw conclusions about this hypothesis, as each sample size $n=\{8000,18000,28000\}$ produced roughly similar winning methods. 
    Future work should include a wider range of budget sizes with larger changes in method performance between sizes.
    
    In our main set of experiments, the \rca{} acquisition functions follow the \textbf{Domain Budget Allocation} strategy, while all other acquisition functions follow the \textbf{Single Pool} strategies. 
    Based on median performance, \rcas{} outperformed all other methods (we're including \bald{} here due to inconsistency in performance between QA and SA) except for those in the $\mathcal{H}$-Divergence family. 
    This suggests that using domain information during selection can lead to performance gains.  
    
    The Optimal Domain Search experiments, shown in Tables \ref{tbl:opt-domain-mrqa} and \ref{tbl:opt-domain-sent}, further suggest that allocating a budget from each domain can improve performance. 
    For 8 out of our 14 experiments, selecting random samples according to the optimal domain distribution outperform any active learning strategy. 
    While the optimal domain distributions were not computed a priori in our experiments, this result shows the potential for \textbf{Domain Budget Allocation} strategies. 
    Future work could reasonably improve our results by developing an acquisition function that better predicts the optimal domain distributions than \rcas{}, or to even have greater performance gains by budgeting each domain, then applying an active learning strategy (e.g. \dale{}) within each budget.
   
    % Altogether, these findings suggest there is ample room for improvement in the best acquisition functions, both in domain and example selection, to even reach our coarse lower-bound on optimal selection for multi-domain active learning.
    % Future work might consider combining orthogonal notions of relevance for more robust example selection.
    % Alternatively, combining \textbf{Domain Budget Allocation} and \textbf{Single Pool} strategies (e.g.\rcas{} and \dale{}) could select domain distributions, then relevant examples within each domain.
    
    