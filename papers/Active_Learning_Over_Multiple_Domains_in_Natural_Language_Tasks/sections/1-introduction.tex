% \section{Introduction}

% These instructions are for authors submitting papers to *ACL conferences using \LaTeX. They are not self-contained. All authors must follow the general instructions for *ACL proceedings,\footnote{\url{http://acl-org.github.io/ACLPUB/formatting.html}} and this document contains additional instructions for the \LaTeX{} style files.

% The templates include the \LaTeX{} source of this document (\texttt{acl.tex}),
% the \LaTeX{} style file used to format it (\texttt{acl.sty}),
% an ACL bibliography style (\texttt{acl\_natbib.bst}),
% an example bibliography (\texttt{custom.bib}),
% and the bibliography for the ACL Anthology (\texttt{anthology.bib}).

\respace
\section{Introduction}
\respace
\label{sec:introduction}

New natural language problems, outside the watershed of core NLP, are often strictly limited by a dearth of labeled data.
While unlabeled data is frequently available, it is not always from the same \emph{source} as the \emph{target} distribution.
This is particularly prevalent for tasks characterized by (i) significant distribution shift over time, (ii) personalization for user subgroups, or (iii) different collection mediums (see examples in Section~\ref{sec:appendix-task}).

A widely-used solution to this problem is to bootstrap a larger training set using active learning (AL): a method to decide which unlabeled training examples should be labeled on a fixed annotation budget \citep{cohn1996active, settles2012active}.
However, most active learning literature in NLP assumes the unlabeled \textit{source} data is drawn from the same distribution as the \textit{target} data \citep{dor2020active}.
This simplifying assumption avoids the frequent challenges faced by practitioners in \emph{multi-domain active learning}.
In this realistic setting, there are multiple sources of data (\emph{i.e.} domains) to consider.
In this case, it's unclear whether to optimize for homogeneity or heterogeneity of selected examples.
Secondly, is it more effective to allocate an example budget per domain, or treat examples as a single unlabeled pool?
Where active learning baselines traditionally select examples the model is least confident on \citep{settles2009active}, in this setting it could lead to distracting examples from very dissimilar distributions.

In this work we empirically examine four separate families of methods (uncertainty-based, $\mathcal{H}$-Divergence, reverse classification accuracy, and semantic similarity detection) over several question answering and sentiment analysis datasets, following \citep{lowell2019practical, elsahar-galle-2019-annotate}, to provide actionable insights to practitioners facing this challenging variant of active learning for natural language.
We address the following questions:
\respace
\begin{enumerate}\itemsep0em
  \item What family of methods are effective for multi-domain active learning?
  \item What properties of the example and domain selection yield strong results?
%   \item How do the characteristics of these diverse methods compare (i.e. the similarity of domain and example rankings)? 
%   \item What are the properties of an effective method, in terms of domain and example selection?
\end{enumerate}
\respace
While previous work has investigated similar settings \citep{10.1007/978-3-642-23808-6_7, Liu_Reyzin_Ziebart_2015, pmlr-v130-zhao21b, kirsch2021active} we contribute, to our knowledge, the first rigorous formalization and broad survey of methods within NLP.
We find that many families of techniques for active learning and domain shift detection fail to reliably beat random baselines in this challenging variant of active learning, but certain $\mathcal{H}$-Divergence methods are consistently strong.
Our analysis identifies stark dissimilarities of these methods' example selection, and suggests domain diversity is an important factor in achieving strong results.
%These results may serve as a guide to practitioners facing this problem, suggesting particular methods depending on the heterogeneity of the source domains to choose from.
These results may serve as a guide to practitioners facing this problem, suggesting particular methods that are generally effective and properties of strategies that increase performance.