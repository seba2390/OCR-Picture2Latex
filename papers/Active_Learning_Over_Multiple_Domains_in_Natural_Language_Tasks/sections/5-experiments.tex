% \section{Bib\TeX{} Files}
% \label{sec:bibtex}

% Unicode cannot be used in Bib\TeX{} entries, and some ways of typing special characters can disrupt Bib\TeX's alphabetization. The recommended way of typing special characters is shown in Table~\ref{tab:accents}.

% Please ensure that Bib\TeX{} records contain DOIs or URLs when possible, and for all the ACL materials that you reference.
% Use the \verb|doi| field for DOIs and the \verb|url| field for URLs.
% If a Bib\TeX{} entry has a URL or DOI field, the paper title in the references section will appear as a hyperlink to the paper, using the hyperref \LaTeX{} package.
\respace
\respace
\section{Experiments}
\respace
\label{sec:experiments}

Experiments are conducted on two common NLP tasks: question answering (QA) and sentiment analysis (SA), each with several available domains.

\respace
\paragraph{Question Answering} 
We employ 6 diverse QA datasets from the MRQA 2019 workshop \citep{fisch2019mrqa}, shown in Table~\ref{datasets}.\footnote{The workshop pre-processed all datasets into a similar format, for fully answerable, span-extraction QA: \url{https://github.com/mrqa/MRQA-Shared-Task-2019}.}
We sample 60k examples from each dataset for training, 5k for validation, and 5k for testing. 
Questions and contexts are collected with varying procedures and sources, representing a wide diversity of datasets.

\input{tables/datasets}

\paragraph{Sentiment Analysis}
For the sentiment analysis classification task, we follow \citep{blitzer-etal-2007-biographies} and \citep{ruder2018strong} by randomly selecting 6 Amazon multi-domain review datasets, as well as Yelp reviews \citep{asghar2016yelp} and IMDB movie reviews datasets \citep{maas-EtAl:2011:ACL-HLT2011}.~\footnote{\url{https://jmcauley.ucsd.edu/data/amazon/}, \url{https://www.yelp.com/dataset}, \url{https://ai.stanford.edu/~amaas/data/sentiment/}.}
Altogether, these datasets exhibit wide diversity based on review length and topic (see Table~\ref{datasets}).
We normalize all datasets to have 5 sentiment classes: very negative, negative, neutral, positive, and very positive. 
We sample 50k examples for training, 5k for validation, and 5k for testing.

\respace
\paragraph{Experimental Setup}
\label{sec:ex-setup}
To evaluate methods for the multi-domain active learning task, we conduct the experiment described in Section~\ref{sec:task} for each acquisition method, rotating each domain as the target set.
Model $M$, a BERT-Base model ~\citep{devlin2019bert}, is chosen via hyperparameter grid search over learning rate, number of epochs, and gradient accumulation.
The large volume of experiments entailed by this search space limits our capacity to benchmark performance variability due to isolated factors (the acquisition method, the target domain, or fine-tuning final models).
However, our hyper-parameter search closely mimics the process of an ML practitioner looking to select a best method and model, so we believe our experiment design captures a fair comparison among methods.
See Algorithm~\ref{alg:experiment} in Appendix Section \ref{sec:appendix-expdesign} for full details.



