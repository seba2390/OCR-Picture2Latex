% \section{Example Appendix}
% \label{sec:appendix}

% This is an appendix.

\clearpage
\appendix

\section{Multi-Domain Active Learning Task}
\label{sec:appendix-task}

In this section, we would enumerate real-world settings in which a practitioner would be interested in multi-domain active learning methods.
We expect this active learning variant to be applicable to cold starts, rare classes, personalization, and settings where the modelers are constrained by privacy considerations, or a lack of labelers with domain expertise.

\respace
\begin{itemize}\itemsep0em
    \item In the \textbf{cold start} scenario, for a new NLP problem, there is often little to no target data available yet (labeled or unlabelled), but there are related sources of unlabelled data to try. 
    Perhaps an engineer has collected small amounts of training data from an internal population. Because the data size is small, the engineer is considering out-of-domain samples, collected from user studies, repurposed from other projects, scraped from the web, etc..
    \item In the \textbf{rare class} scenario, take an example of a new platform/forum/social media company classifying hate speech against a certain minority group. 
    Perhaps the prevalence of positive, in-domain samples on the social media platform is small, so an engineer uses out-domain samples from books, other social media platforms, or from combing the internet.
    \item In a \textbf{personalization} setting, like spam filtering or auto-completion on a keyboard, each user may only have a couple hundred of their own samples, but out-domain samples from other users may be available in greater quantities.
    \item In the \textbf{privacy constrained} setting, a company may collect data from internal users, user studies, and beta testers; however, a commitment to user privacy may incentivize the company to keep the amount of labeled data from the target user population low.
    \item Lastly, labeling in-domain data may require certain \textbf{domain knowledge}, which would lead to increased expenses and difficulty in finding annotators. 
    As an example, take a text classification problem in a rare language. 
    It may be easy to produce out-domain samples by labeling English text and machine translating it to the rare language, whereas generating in-domain labeled data would require annotators who are fluent in the rare language.
\end{itemize}
    
In each of these settings, target distribution data may not be amply available, but semi-similar unlabelled domains often are. 
This rules out many domain adaptation methods that rely heavily on unlabelled target data.

We were able to simulate the base conditions of this problem with sentiment analysis and question answering datasets, since they are rich in domain diversity. 
We believe these datasets are reasonable proxies to represent the base problem, and yield general-enough insights for a practitioner starting on this problem.

% Our evaluation setup is also designed for realistic measurements of cold-start active learning scenarios.
% In these settings, practitioners are likely to conduct one large active learning experiment to bootstrap the redo not continuously sampling 50-200 examples for labelling
% , where target-domain labelled data is limited in a cold start scenario.
% Whereas traditional active learning is sometimes evaluated by , over several iterations, we assume there is one big labelling batch
% for this setting, where target unlabelled data is not abundantly available.

\section{Reproducibility}
\label{sec:appendix-expdesign}

\subsection{Datasets and Model Training}

We choose question answering and sentiment analysis tasks as they are core NLP tasks, somewhat representative of many classification and information-seeking problems.
Multi-domain active learning is not limited to any subset of NLP tasks, so we believe these datasets are a reasonable proxie for the problem.

For question answering, the MRQA shared task \citep{fisch2019mrqa} includes SQuAD \citep{rajpurkar2016squad}, NewsQA \citep{trischler2016newsqa}, TriviaQA \citep{joshi2017triviaqa}, SearchQA \citep{dunn2017searchqa}, HotpotQA \citep{yang2018hotpotqa}, and Natural Questions \citep{kwiatkowski2019natural}.

For the sentiment analysis classification task, we use Amazon datasets following \citep{blitzer-etal-2007-biographies} and \citep{ruder2018strong}, as well as Yelp reviews \citep{asghar2016yelp} and IMDB movie reviews datasets \citep{maas-EtAl:2011:ACL-HLT2011}.~\footnote{\url{https://jmcauley.ucsd.edu/data/amazon/}, \url{https://www.yelp.com/dataset}, \url{https://ai.stanford.edu/~amaas/data/sentiment/}.}
Both question answering and sentiment analysis datasets are described in Table~\ref{datasets}.

For reproducibility, we share our hyper-parameter selection in Table~\ref{model-hyperparams}. 
Hyper-parameters are taken from \citet{longpre2019exploration} for training all Question Answering (QA) models since their parameters are tuned for the same datasets in the MRQA Shared Task. 
We found these choices to provide stable and strong results across all datasets.
For sentiment analysis, we initially experimented on a small portion of the datasets to arrive at a strong set of base hyper-parameters to tune from.

Our BERT question answering modules build upon the standard PyTorch \citep{NEURIPS2019_9015} implementations from HuggingFace, and are trained on one NVIDIA Tesla V100 GPU.\footnote{\url{https://github.com/huggingface/transformers}}.

\input{tables/task-model-parameters}

\subsection{Experimental Design}

For more detail regarding the experimental design we include Algorith~\ref{alg:exp-design}, using notation described in the multi-domain active learning task definition.

\input{tables/experiment-design}

\section{Acquisition functions}

\subsection{Task Agnostic Embeddings}

To compute the semantic similarity between two examples, we computed the example embeddings using the pre-trained model from a sentence-transformer \citep{reimers-2019-sentence-bert}. 
We used the RoBERTa large model, which has 24 layers, 1024 hidden layers, 16 heads, 355M parameters, and fine tuning on the SNLI \citep{snli:emnlp2015}, MultiNLI \citep{N18-1101}, and STSBenchmark \citep{cer2017semeval} datasets. 
Its training procedure is documented in \url{https://www.sbert.net/examples/training/sts/README.html}. 

\subsection{Bayesian active learning by disagreement (BALD)}
We note that Siddant and Lipton's presentation of BALD is more closely related to the Variation Ratios acquisition function described in \citet{gal2017deep} than the description of dropout as a Bayesian approximation given in \citet{gal2016}. 
In particular, \citet{gal2017deep} found that Variation Ratios performed on par or better than Houlsby's BALD on MNIST but was less suitable for ISIC2016.

\subsection{Discriminative Active Learning Model (DAL) Training}
\dal{}'s training set is created using the methods detailed in Section~\ref{sec:h-divergence-methods}. 
The training set is then partitioned into five equally sized folds. 
In order to predict on data that is not used to train the discriminator, we use 5-fold cross validation. 
The model is trained on four folds, balancing the positive and negative classes using sample weights. 
The classifier then predicts on the single held-out fold. 
This process is repeated five times so that each example is in the held out fold exactly once. 
Custom model parameters are shown in Table~\ref{tbl:dal-hyperparams}; model parameters not shown in the table are the default XGBClassifier parameters in xgboost 1.0.2.
The motivations for choice in model and architecture are the small amount of target domain examples requiring a simple model to prevent overfitting and the ability of decision trees to capture collective interactions between features. 

\section{Full Method Performances}
We provide a full breakdown of final method performances in Tables~\ref{tab:MRQA-performance} and ~\ref{tab:sent-performance}.

\input{tables/dal-model-paramaters}

\input{tables/mrqa-performances}
\input{tables/sent-performances}

% \section{Comparing Example Rankings}
% \begin{figure*}
%   \includegraphics[width=\textwidth]{figures/KendallsTau_combined.pdf}
%   \caption{\label{fig:kendall-tau-heatmap} Kendall's Tau Coefficients for MRQA (above diagonal) and Sentiment (below diagonal). 
%   Kendall's Tau coefficient is computed between the example rankings of each pair of methods. 
%   The heatmap contains these coefficients averaged over each target dataset. 
%   1 indicates a perfect relationship between the rankings, 0 means no relationship, and -1 means an anti-relationship.}
% \end{figure*}


\section{Kendall's Tau}
\subsection{Definition}
Kendall's Tau is a statistic that measures the rank correlation between two quantities. 
Let $X$ and $Y$ be random variables with $(x_1, y_1), (x_2, y_2), ..., (x_n, y_n)$ as observations drawn from the joint distribution. 
Given a pair $(x_i, y_i)$ and $(x_j, y_j)$, where $i\neq j$, we have:

$\frac{y_j-y_i}{x_j-x_i}>0:$ pair is concordant

$\frac{y_j-y_i}{x_j-x_i}<0:$ pair is discordant

$\frac{y_j-y_i}{x_j-x_i}=0:$ pair is a tie

Let $n_c$ be the number of concordant pairs and $n_d$ the number of discordant pairs. Let ties add 0.5 to the concordant and discordant pair counts each. Then, Kendall's Tau is computed as:\footnote{\url{https://www.itl.nist.gov/div898/software/dataplot/refman2/auxillar/kendell.htm}}

$\tau = \frac{n_c-n_d}{n_c+n_d}$

\begin{figure}[h]
\centering
\begin{subfigure}{\linewidth}
  \centering
  \includegraphics[width=\linewidth]{figures/KendallsTauMRQANormalized.pdf}
  \caption{MRQA}
  \label{fig:sub1}
\end{subfigure}
\begin{subfigure}{\linewidth}
  \centering
  \includegraphics[width=\linewidth]{figures/KendallsTauSentNormalized.pdf}
  \caption{Sentiment}
  \label{fig:sub2}
\end{subfigure}
\caption{Kendall Tau scores normalized by intra-family scores according to the family of the method on the y-axis (with uncertainty-ascending and uncertainty-descending as distinct families). If the cell's corresponding Kendall Tau score  is within the intra-family range, it's value will be in $[0, 1]$. Below the range is negative, and above the range is greater than 1.} 
\label{fig:kt-normalized}
\end{figure}

\subsection{Inter-Family Comparison}
Here, we extend on our comparison of example rankings by presenting plots of Kendall Tau scores normalized by intra-family scores in \ref{fig:kt-normalized}. 
For the sentiment setting, the ranges of intra-family Kendall Tau coefficients are smaller than the MRQA setting. 
Methods in the uncertainty family have especially strong correlations with each other and much weaker with methods outside of the family. 
For H-divergence based methods, intra-family correlations are notâ€™t as strong as for the uncertainty family; in fact, the Kendall Taus between \dale{}/\knn{} and \dalt{}/\knn{} appear to be slightly within the H-divergence intra-family range. 

Furthermore, intra-family ranges are quite large for all families in the MRQA setting. 
For each method, there is at least one other method from a different family with which it had a higher Kendall Tau coefficient than the least similar methods of its own family.

\section{Relating Domain Distances to Performance}
\label{appendix:domain-distances}

% From results in Figures \ref{fig:mrqa-performances} and \ref{fig:sent-performances}, we observe the best method for a given target set is often difficult to predict.
% In an attempt to understand why certain methods work well, we hoped to derive some relationship between the distribution of examples selected (close or far from the target), and the final performance of this selection.
    
% We estimated the \textit{distance} between two domains by computing the wasserstein distance between their constitutent example embeddings (we tried with both task-agnostic and task-specific encoders).
% Unfortunately, we found no discernable relationship between these distances and the performances of single domain baselines.
% We believe this is either because our estimated distances were simply not reliable measures of domain relevance, since each target clearly benefited more from some domains than others during training, but the distances had no correlation.
% We hypothesize that a single scalar value, for domain distance, is not sufficient to represent the relevance, informativeness, and diversity of potential training sources.
    
We investigated why certain methods work better than others. 
One hypothesis is that there exists a relationship between between target-source domain distances and method performance. 
We estimated the distance between two domains by computing the Wasserstein distance between random samples of 3k example embeddings from each domain. 
We experimented with two kinds of example embeddings: 1. A task agnostic embedding computed by the sentence transformer used in the \knn{} method, and 2. A task specific embedding computed by a model trained with the source domain used in the \dals{} method. 
Given that there are $k - 1$ source domains for each target domain, we tried aggregating domain distances over its mean, minimum, maximum, and variance to see if Wasserstein domain distances could be indicative of relative performance across all methods.

Figure \ref{fig:avg_distance_perf}, Figure \ref{fig:min_distance_perf}, Figure \ref{fig:max_distance_perf}, and Figure \ref{fig:var_distance_perf} each show, for a subset of methods, the relationship between each domain distance aggregation and the final performance gap between the best performing method. 
Unfortunately, we found no consistent relationship for both MRQA and the sentiment classification tasks. 
We believe that this result arose either because our estimated domain distances were not reliable measures of domain relevance, or because the aggregated domain distances are not independently sufficient to discern relative performance differences across methods.

\begin{figure*}[ht]
  \centering
     \caption*{Figures 4-7: The average domain distance is calculated by finding the distance between 3k examples from $D_T$ and the combined set made from choosing 3k examples from each domain in $D_S$. 
     Since the Wasserstein metric is symmetric, this yields $k$ points for comparison.}
\includegraphics[width=\textwidth]{figures/avg_distance_perf_flatten.pdf}
    \caption{\label{fig:avg_distance_perf} Average Wasserstein domain distance vs performance.}
\end{figure*}
\respace
\begin{figure*}[ht]
  \centering
    \includegraphics[width=\textwidth]{figures/min_distance_perf_v2.pdf}
    \caption{\label{fig:min_distance_perf} Minimum Wasserstein domain distance vs method performance.}
\end{figure*}
\respace
\begin{figure*}[ht]
  \centering
    \includegraphics[width=\textwidth]{figures/max_distance_perf_flatten.pdf}
    \caption{\label{fig:max_distance_perf} Maximum Wasserstein domain distance vs method performance.}
\end{figure*}
\clearpage
\begin{figure*}[ht]
  \centering
    \includegraphics[width=\textwidth]{figures/var_distance_perf_flatten.pdf}
    \caption{\label{fig:var_distance_perf} Wasserstein Domain distance variance vs performance.}
\end{figure*}