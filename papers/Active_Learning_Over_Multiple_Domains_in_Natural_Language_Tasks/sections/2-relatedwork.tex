% \section{Engines}

% To produce a PDF file, pdf\LaTeX{} is strongly recommended (over original \LaTeX{} plus dvips+ps2pdf or dvipdf). Xe\LaTeX{} also produces PDF files, and is especially suitable for text in non-Latin scripts.

\section{Related Work}
\respace
\label{sec:relatedwork}

\paragraph{Active Learning in NLP}
\citet{lowell2019practical} shows how inconsistent active learning methods are in NLP, even under regular conditions. 
However, \citet{dor2020active, siddhant-lipton-2018-deep} survey active learning methods in NLP and find notable gains over random baselines.
\citet{kouw2019review} survey domain adaptation without target labels, similar to our setting, but for non-language tasks.
We reference more active learning techniques in Section~\ref{sec:methods}.

\respace
\paragraph{Domain Shift Detection}
\citet{elsahar-galle-2019-annotate} attempt to predict accuracy drops due to domain shifts and \citet{rabanser2018failing} surveys different domain shift detection methods. 
\citet{arora2021types} examine calibration and density estimation for textual OOD detection.
% Both evaluate domain discriminators for detecting domain shift using task agnostic and specific embeddings, which motivates our \dal{} methods.

\respace
\paragraph{Active Learning under Distribution Shift}
A few previous works investigated active learning under distribution shifts, though mainly in image classification, with single source and target domains.
\citet{kirsch2021active} finds that \bald{}, which is often considered the state of the art for unshifted domain settings, can get stuck on irrelevant source domain or junk data. 
\citet{pmlr-v130-zhao21b} investigates \emph{label shift}, proposing a combination of predicted class balanced subsampling and importance weighting. 
\citet{10.1007/978-3-642-23808-6_7}, whose approach corrects joint distribution shift, relies on the \emph{covariate shift assumption}. 
However, in practical settings, there may be general distributional shifts where neither the \emph{covariate shift} nor \emph{label shift} assumptions hold.

\respace
\paragraph{Transfer Learning from Multiple Domains}
Attempts to better understand how to handle shifted domains for better generalization or target performance has motivated work in question answering \citep{talmor2019multiqa, fisch2019mrqa, longpre2019exploration, kamath2020selective} and classification tasks \citep{ruder2018strong, sheoran2020recommendation}.
\citet{ruder2017learning} show the benefits of both data similarity and diversity in transfer learning.
\citet{ruckle2020multicqa} find that sampling from a wide-variety of source domains (data scale) outperforms sampling similar domains in question answering.
\citet{he2021multi} investigate a version of multi-domain active learning where models are trained and evaluated on examples from all domains, focusing on robustness across domains.