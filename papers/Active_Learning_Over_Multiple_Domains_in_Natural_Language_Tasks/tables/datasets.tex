\begin{table*}[ht]
\centering
\begin{tabular}{@{}lllcc|lcccc@{}}
\toprule
\multicolumn{5}{c}{\textbf{MRQA Datasets}} & \multicolumn{5}{c}{\textbf{Sentiment Datasets}}  \\
\toprule
\textbf{Dataset} & \textbf{Q} & \textbf{C} & \textbf{$|$Q$|$} & \textbf{Q $\perp$ C} & \textbf{Dataset} & \textbf{$|$R$|$} & \textbf{-} & \textbf{N} & \textbf{+}\\
\midrule
SQuAD  & Crowd     & Wiki    & 11 & \xmark & Amzn-Books 	&	144 & 12.1 & 8.8 & 79.1\\
NewsQA & Crowd      & News    & 8 & \cmark & Amzn-Health  	&	80 & 9.3 & 7.0 & 83.7\\
TriviaQA & Trivia   & Web   & 16 & \cmark & Amzn-Music  	&	132 & 36.2 & 9.1 & 54.7\\
SearchQA & Jeopardy  & Web  & 17 & \cmark & Amzn-Software  	&	126 & 14.2 & 8.1 & 77.6\\
HotpotQA & Crowd      & Wiki     & 22 & \xmark & Amzn-Sports  	&	84 & 49.9 & 0.0 & 50.1\\
Natural-QS  & Search       & Wiki    & 9 & \cmark & Amzn-Tools  	& 89 &	15.3 & 7.9 & 76.8\\ 
  &        &      &      &    & Imdb 	&	230 & 16.4 & 7.5 & 76.1\\ 
  &        &      &      &     & Yelp 	&	109 & 24.3 & 10.7 & 65.0\\ 

\midrule
\end{tabular}
\caption{\textbf{Datasets:} The question answering (left) and sentiment analysis (right) datasets in our experiments.
Left: Query source (Q), Context source (C), mean query length ($|Q|$), and whether the query was written independently from the context ($Q \perp C$).
Right: mean review length ($|R|$) and the percent representation of negative (-), neutral (N) and positive (+) labels.}
\label{datasets}
\end{table*}


% \begin{table*}[t]
% \centering
% \begin{tabular}{@{}lllccclccccc@{}}
% \toprule
% \multicolumn{6}{c}{\textbf{MRQA Datasets}} & \multicolumn{5}{c}{\textbf{Sentiment Datasets}}  \\
% \toprule
% \textbf{Dataset} & \textbf{Q} & \textbf{C} & \textbf{$|$Q$|$} & \textbf{$|$C$|$} & \textbf{Q $\independent$ C} & \textbf{Dataset} & \textbf{$|$R$|$} & \textbf{+} & \textbf{-} & \textbf{-/+}\\
% \midrule
% SQuAD  & Crowd     & Wiki  & 11  &  137  & \xmark & amzn-book &	144	& 39550	& 6032	  & 4418\\
% NewsQA & Crowd      & News    & 8 & 599  & \cmark & amzn-health  &	80	& 41874	& 4635	  & 3491\\
% TriviaQA & Trivia            & Web   & 16 & 784 & \cmark & amzn-music  &	132	& 27367	& 18099  & 	4534\\
% SearchQA & Jeopardy          & Web & 17 & 749 & \cmark & amzn-software  &	126	& 38820	& 7115	  & 4065\\
% HotpotQA & Crowd      & Wiki    &  22 & 232 & \xmark & amzn-sports  &	84	& 20030	& 19970  & 	0 \\
% Natural-QS  & Search       & Wiki     & 9  & 153 & \cmark & amzn-tools  &	89	& 38117	& 7953	  & 3930\\ 
%   &        &      &   &  &  & imdb &	230	& 38034	& 8222	  & 3744\\ 
%   &        &      &   &  &  & yelp &	109	& 32498	& 12139  & 	5363\\ 

% \midrule
% \end{tabular}
% \caption{\label{question-answering-datasets} A comparison of MRQA datasets, emphasizing domain diversity.
% $Q \independent C$ indicates if the question is written independently from the context passage, and $|\cdot|$ measures sequence length.}
% \end{table*}