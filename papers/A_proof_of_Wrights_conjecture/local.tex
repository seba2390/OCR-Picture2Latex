%!TEX root = hopfwright.tex

\subsection{Constructing a Newton-like operator}
\label{s:newtonlike}

In this section and in the appendices we often suppress the subscript in $F=F_\epsilon$.
We will find solutions to the equation $F(\alpha ,\omega , c)=0$ by the
constructing a Newton-like operator $T$ such that fixed points of $T$
corresponds precisely to zeros of $F$. In order to construct the map $T$ we
need an operator $A^{\dagger}$ which is an approximate inverse of 
$DF(\bx_\epsilon)$. 
% Since
% $\bx_\epsilon$ is an approximate zero of $F_\epsilon$ up to order
% $\cO(\epsilon^2)$ correction terms,
We will use an approximation $A$ of 
$DF( \bx_\epsilon )$ that is linear in~$\epsilon$ and correct up to $\cO(\epsilon^2)$.
% (recall that $F(\bx_\epsilon)=\cO(\eps^2)$). 
Likewise, we define $A^{\dagger}$ to be linear in $\epsilon$ (and again correct up to $\cO(\epsilon^2)$). 

It will be convenient to use the usual identification $i_\C : \R^2 \to \C$ given by $i_\C (x,y) = x+iy $. We also use $\omega_0 := \pi/2$.
 % order
% Since $x(\epsilon)$ is only correct up to order $\cO(\epsilon^2)$, then it only makes sense to compute our approximate derivative up to order $\cO( \epsilon^2)$.

% \marginpar{Jonathan: I tried to be careful about the spaces here, but it all seems a bit of a distraction since everything is explicit in coordinates}
\begin{definition}\label{def:A}
We introduce the linear maps $A:  \R^2 \times \ell^K_0 \to \ell^1$ and 
$ A^{\dagger}:  \ell^1 \to  \R^2 \times \ell^K_0 $ by
\begin{alignat*}{1}
A &:= A_0 + \epsilon A_1 \, , \\
A^{\dagger} &:= A_0^{-1} - \epsilon A_0^{-1} A_1 A_0^{-1} \,  ,
%\label{eq:ADagger}
\end{alignat*}
where the linear maps $ A_0 , A_1 : \R^2 \times \ell^K_0 \to \ell^1$  are defined below. Writing $x=(\alpha,\omega,c)$, we set
\begin{alignat*}{1}
A_0	x = A_0 (\alpha,\omega,c) & := i_\C A_{0,1} 
\!\left[\!\! \begin{array}{c} \alpha \\ \omega \end{array} \!\!\right]  \e_1
 + A_{0,*}  c , \\
A_1 x =	A_1 (\alpha,\omega,c) & := i_\C  A_{1,2}
\!\left[\!\! \begin{array}{c} \alpha \\ \omega \end{array} \!\!\right]  \e_2
 + A_{1,*}  c .
%\label{eq:ApproxDFdef}
\end{alignat*}
Here the matrices $A_{0,1}$ and $A_{1,2}$ are given by
\begin{equation}
A_{0,1} := 
\left[
\begin{matrix}
0 & - \pp \\
-1  & 1 
\end{matrix} 
\right]
\qquad\text{and}\qquad
A_{1,2} := \frac{1}{5}
\left[
\begin{matrix}
-2 & 2-\tfrac{3 \pi}{2} \\
-4  & 2(2+\pi) 
\end{matrix}  
\right]  ,
\label{eq:defA12}
\end{equation}
and the linear maps $A_{0,*} : \ell^K_0 \to \ell^1_0$ and
$A_{1,*} : \ell^K_0 \to \ell^1$
are given by
\begin{equation*}
% A_{0,*} :& \ell^1_0 \to \ell^1_0
% &
% A_{1,*} :& \ell^1_0 \to \ell^1  \\
% %%%%%%
% A_{0,1} :& \{ \alpha, \omega\} \to \{ Re \, F_1 , Im\, F_1 \}
% &
% A_{1,2} :& \{ \alpha, \omega\} \to \{ Re \,F_2 , Im \, F_2 \}
% \end{align*}
% and given by the equations below, taking $ \omega_0 = \pp$.
% \begin{align}
A_{0,*} 	 := \tfrac{\pi}{2} ( i K^{-1} + U_{\omega_0}) 
\qquad\text{and}\qquad
A_{1,*} 	:= \tfrac{\pi}{2} L_{\omega_0} .
\end{equation*}
%%%%%%%%%%%%%%%%%%%%
% A_{0,1} := &
% \left[
% \begin{matrix}
% 0 & - \pp \\
% -1  & 1
% \end{matrix}
% \right]
% &
% A_{1,2} :=& \frac{1}{5}
% \left[
% \begin{matrix}
% -2 & 2-\tfrac{3 \pi}{2} \\
% -4  & 2(2+\pi)
% \end{matrix}
% \right]
% \label{eq:defA12}
% \end{align}
\end{definition}

Since $K$ and $U_{\omega_0}$ both act as diagonal operators, the inverse 
$A_{0,*}^{-1} : \ell^1_0 \to \ell^K_0$ of $A_{0,*}$ is given by
\begin{equation*}
	  (A_{0,*}^{-1} a)_k = \frac{2}{\pi} \frac{a_k}{ik+e^{-ik\omega_0}} 
	  \qquad\text{for all } k \geq 2.
\end{equation*} 
An explicit computation, which we leave to the reader, shows that these approximations are indeed correct up to $\cO(\epsilon^2)$. 
In particular, $A^{\dagger} = \left[ DF( \bx_\epsilon ) \right]^{-1} + \cO(\epsilon^2)$.
In Appendix~\ref{sec:OperatorNorms} several additional properties of these operators are derived. The most important one is the following.
% \note[J]{I've tried to make this change for the new injectivity bound throughout.} \note[JB]{Seems fine, but wouldn't it be nicer to write $\tfrac{\sqrt{10}}{4}$ instead of $\tfrac{5}{2 \sqrt{10}}$?} \note[J]{Yes it would, made changes below. }
\begin{proposition}
	\label{prop:Injective}
	For 
%\change[J]{$0 \leq \epsilon < \tfrac{5}{2} ( 4 + \sqrt{10})^{-1} \approx 0.349$}
	$0 \leq \epsilon < \tfrac{\sqrt{10}}{4} \approx 0.790$
	 the operator $ A^{\dagger}$ is injective. 
\end{proposition}
\begin{proof}
	In order to show that $ A^{\dagger}$ is injective we show that 
	it has a left inverse. 
	Note that $ A A^{\dagger} = I - \epsilon^2 ( A_1 A_0^{-1})^2$. 
	By Proposition \ref{prop:A1A0} it follows that 
%	\change[J]{$ \| A_1 A_0^{-1} \| \leq \tfrac{2}{5} ( 4 + \sqrt{10})$}
	 $ \| A_1 A_0^{-1} \| \leq \tfrac{2 \sqrt{10}}{5} $.  
	By choosing 
%\change[J]{$ \epsilon < \tfrac{5}{2} ( 4 + \sqrt{10})^{-1}$}
$ \epsilon < \tfrac{\sqrt{10}}{4}$ 
we obtain 
	$\|  \epsilon^2 ( A_1 A_0^{-1})^2 \| < 1$, whereby $ A A^{\dagger}$ is 
	invertible, and so $ A^{\dagger}$ is injective. 
\end{proof}


\begin{definition}
We define the operator $ T: \R^2 \times \ell^K_0 \to \R^2 \times \ell^K_0 $ by
\begin{equation*}
	T(x) :=  x - A^{\dagger} F(x) ,
\end{equation*}
	where  $F$ is defined in Equation~\eqref{eq:FDefinition}  and $A^{\dagger}$ in Definition~\ref{def:A}.
	We note that $F$, $A^{\dagger}$ and $T$ depend on the parameter $\epsilon \geq 0$, although we suppress this in the notation.
\end{definition}

% \note[J]{When we got the better bound on $\|A_1 A_{0}^{-1}\|$, then $A^{\dagger}$ being injective ceased to be a bottleneck for doing a Hopf bifurcation. I don't think we'd lose much if we just delete this remark.  }
% \begin{remark}
% 	\label{r:Injective}
% \remove[J]{
% 	If $A^{\dagger}$ is injective, which is true for
% 	$0 \le \epsilon <  \tfrac{5}{2} ( 4 + \sqrt{10})^{-1}$ by Proposition 3.2, then the fixed points of $T$ correspond bijectively with the zeros of $F$.
% 	Since the periodic solution having $ \epsilon_0 = \tfrac{5}{2} ( 4 + \sqrt{10})^{-1}$ corresponds approximately to $\bar{\alpha}_{\epsilon_0} = \pp + 0.090$, above this value we cannot use the Newton-like operator $T$ to reliably study the SOPS to Wright's equation.
% 	Hence $ \alpha = \pp + 0.09$ represents an upper bound for doing an $\cO(\epsilon^2)$ Hopf bifurcation analysis.}
% \end{remark}
%


\subsection{Explicit contraction bounds}
\label{s:contraction}


The map $T$ is not continuous on all of $\R^2 \times \ell^K_0$,
since $ U_{\omega} c $ is not continuous in $\omega$.
While continuity is ``recovered'' for terms of the form $A^{\dagger} U_{\omega} c$,  this is not the case for the nonlinear part $ - \alpha \epsilon A^{\dagger} [ U_{\omega} c ] * c$.  
% The problem is that while in the $ U_{\omega} c$ term and that  $ \tfrac{\partial}{\partial \omega} U_{\omega} = - i K^{-1} U_{\omega}$.
% Since  the map $ A^{\dagger}$ is approximately $\tfrac{2 }{\pi i} K$, then the  $  A^{\dagger}  U_{\omega} c$ component of $ T$ is continuous in $\omega$.
%
%For any $ \omega_1, \omega_2\in \R$  then $ \| U_{\omega_1} - U_{\omega_2} \|  = 2$ when $ 2 \pi $ does not divide $ \omega_1 - \omega_2$.  
%This is not a problem for the $ A^{\dagger} ( i \omega K^{-1} + \alpha U_{\omega} ) c$ component of $T$; 
%the and then $ A^{\dagger } U_{\omega}$ is continuous in $\omega$.  
% However, since $ U_{\omega}$ is inside a convolution in the nonlinear part, this type of simplification cannot happen.
% 
We overcome this difficulty by fixing some $ \rho > 0$ and restricting the domain of $T$ to sets of the form 
%$\R^2 \times X_\rho$, where
\[
  \R^2 \times  \{ c \in \ell^K_0 : \|K^{-1} c\| \leq \rho \} = \R^2 \times \ell_\rho.
\]
Since we wish to center the domain of $T$ about the approximate solution~$\bx_\epsilon$, we introduce the following definition, which uses a triple of radii $r \in \R^3_+$, for which it will be convenient to use two different notations:
\[
  r = ( r_{\alpha } , r_{\omega} , r_c) = (r_1,r_2,r_3).
\]
\begin{definition}
	Fix   $ r \in \R^3_+$ and $ \rho > 0$ and let  $ \bx_\epsilon = ( \balpha_\epsilon , \bomega_\epsilon , \bc_\epsilon )$ be as defined in Definition~\ref{def:xepsilon}. 
    We define the $\rho$-ball $B_\epsilon(r,\rho) \subset \R^2 \times \ell^1_0$
    of radius $r$ centered at $\bx_\epsilon$ to be the set of points satisfying 
\begin{alignat*}{1}
	|  \alpha -\balpha_\epsilon | & \leq  r_\alpha  \\
	| \omega - \bomega_\epsilon  | & \leq  r_{\omega} \\
	\| c - \bc_\epsilon  \| & \leq r_c \\
	\|K^{-1} c\| & \leq  \rho .
\end{alignat*}
\end{definition}

We want to show that $T$ is a contraction map on some $\rho$-ball 
$B_\epsilon(r,\rho) \subset \R^2 \times \ell^1_0$ using a Newton-Kantorovich argument. 
This will require us to develop a bound on $DT$ using some norm on  $ X$.  
Unfortunately there is no natural choice of norm on the product space $ X$. 
Furthermore, it will not become apparent if one norm is better than another until after  significant calculation.  
For this reason, we use a notion of an ``upper bound'' which allows us to delay our choice of norm. 
We first introduce the operator $\zeta:  X  \to \R^3_{+}$
which consists of the norms of the three components:
\[
  \LL(x) :=   ( |\pi_\alpha x|, |\pi_\omega x|, \|\pi_c x\| )^T \in \R^3_{+}
  \qquad\text{for any } x \in X.
\]
% \note[JB]{Propose to revert to using single overlines for the upper bound; the double overlines I had introduced just look ridiculous to me now. There is a potential for confusion with $\bx_\epsilon$, but I can live with it.} \note[J]{I would also prefer using only one overline. }
\begin{definition}[upper bound]\label{def:upperbound}
We call $\upperbound{x} \in \R^3_+$ an upper bound on $x$ if $\LL(x) \leq \upperbound{x}$, where the inequality is interpreted componentwise in $\R^3$. 
Let $X'$ be a subspace of $X$ and let $X''$ be a subset of $X'$.   
An upper bound on a linear operator $A' : X' \to X $ over $X''$  is 
a $3 \times 3$ matrix $\upperbound{A'} \in \text{\textup{Mat}}(\R^3 , \R^3)$ such that
\[
   \LL(A' x ) \leq \upperbound{A'} \cdot \LL(x)  
     \qquad\text{for any }  x \in X'',
\]
where the inequality is again interpreted componentwise in $\R^3$. 
The notion of upper bound conveniently encapsulates bounds on the different components of the operator $A'$ on the product space $X$. Clearly the components of the matrix $\upperbound{A'}$ are nonnegative.


		% 	Let $ (\alpha , \omega , c) \in \R^2 \times \ell^1_0$ and $  \upperbound{x} = ( x_{ \alpha } , x_{\omega} , x_{c}) \in \R^3_+$.
		% 	Then $ \upperbound{x}$ is an \emph{upper bound} on $(\alpha , \omega , c)$ if $ | \alpha | \leq x_\alpha$ and $ | \omega | \leq x_{\omega}$ and $ \| c\|\leq x_{c}$.
		% Similarly, suppose that $ A' : X \to \R^2 \times \ell^1_0$ is a linear operator, defined on some domain $ X \subset \R^2 \times \ell^1_0$.
		% Then $ \upperbound{A'} \in Mat(\R^3 , \R^3)$ is an \emph{upper bound } on $ A'$  if $ \upperbound{A'} \cdot \upperbound{x} $ is an upper bound on $ A' x$ whenever $ \upperbound{x} \in \R^3_+$ is an upper bound on all $ x \in X$.
\end{definition}
		% The notion of upper bounds commutes with vector addition and matrix multiplication.
		% That is, if $\upperbound{x} , \upperbound{y} \in \R^3_+ $ are upper bounds on $ x,y \in \R^2 \times \ell^1_0$, then $ \upperbound{x} + \upperbound{y}$ is an upper bound on $x +y$.
		% Similarly, if we have two linear operators $A'$ and $A''$ with upper bounds
		% $\upperbound{A'}$ and $\upperbound{A''}$, respectively, then $ \upperbound{A'} \cdot \upperbound{A''}$ is an upper bound for $ A' \circ A''$.
		% Furthermore, if $\upperbound{A'} \in Mat(\R^3,\R^3)$ is an upper bound, then the entries of this matrix are necessarily non-negative.
For example, in Proposition \ref{prop:A0A1} we calculate an upper  bound on the map $A_0^{-1} A_1$.  
As for the domain of definition of $T$, in practice we use $X' = \R^2 \times  \ell^K_0  $ and  $X'' = \R^2 \times  \ell_\rho  $.
The subset $X''$ does not always affect the upper bound calculation (such as in Proposition \ref{prop:A0A1}). 
However, operators such as $U_{\omega} - U_{\omega_0}$ have upper bounds which contain $\rho$-terms (see for example Proposition \ref{prop:OmegaDerivatives}).

Using this terminology, we state a ``radii polynomial'' theorem, which allows us to check whether $T$ is a contraction map. This technique has been used frequently in a computer-assisted setting in the past decade. Early application include~\cite{daylessardmischaikow,lessardvandenberg}, while a previous implementation in the context of Wright's delay equation can be found in~\cite{lessard2010recent}. 
Although we use radii polynomials as well, our approach differs significantly from the computer-assisted setting mentioned above. 
While we do engage a computer (namely the Mathematica file~\cite{mathematicafile}) to optimize our quantitative results, the analysis is performed essentially in terms of pencil-and-paper mathematics (in particular, our operators do not involve any floating point numbers).
In our current setup we employ \emph{three} radii as a priori unknown variables,
which builds on an idea introduced in~\cite{vandenberg}.
We note that in most of the papers mentioned above the notation of $A$ and $A^\dagger$ is reversed compared to the current paper.

As preparation, the following lemma (of which the proof can be found in Appendix~\ref{sec:CompactDomain})  provides an explicit choice for $\rho$, as a function of $\epsilon$ and $r$, for which we have proper control on the image of $B_\epsilon(r,\rho)$ under $T$.
\begin{lemma}\label{lem:Crho}
For any $\epsilon \geq 0$ and $r \in\R^3_+$, let $C=C(\epsilon,r)$ be given  by Equation~\eqref{eq:RhoConstant}. 
If $C(\epsilon,r) >0$  then 
% Proposition~\ref{prop:DerivativeEndo} states that
\begin{equation}\label{e:Cepsr} 
  \| K^{-1} \pi_c  T(x) \| \leq \rho 
  \quad\text{whenever } x \in B_\epsilon(r,\rho) \text{ and } \rho \geq C(\epsilon,r).
\end{equation}
%%%%\marginpar{Jonathan: please fix appendix to reflect this (and define $C$ there)}
Moreover, $C(\epsilon,r)$ is nondecreasing in $\epsilon$ and $r$. 
\end{lemma}

\begin{proof}
See Proposition~\ref{prop:DerivativeEndo}.
\end{proof}


\begin{theorem}
	\label{thm:RadPoly}
	Let  
%\change[J]{$0 \leq \epsilon < \tfrac{5}{2} ( 4 + \sqrt{10})^{-1} $}
 $0 \leq \epsilon < \tfrac{\sqrt{10}}{4} $  
 and fix $r = (r_\alpha, r_\omega, r_c) \in \R^3_+$. Fix $\rho > 0$ such that $ \rho \geq C(\epsilon,r)$, as given by Lemma~\ref{lem:Crho}.
 % as in Proposition \ref{prop:DerivativeEndo} (REFORMULATE TO POINT TO THE PREPARATION ABOVE).
%
Suppose that $Y(\epsilon) $ is an upper bound on $ T(\bx_\epsilon) - \bx_\epsilon$ and $Z(\epsilon , r ,\rho) $ a (uniform) upper bound on $ DT(x) $ for all $ x \in B_\epsilon(r,\rho)$. 
Define the \emph{radii polynomials}
$P :\R^5_+ \to \R^3 $  by 
 \begin{equation}
 \label{eq:RadPolyDef}
  P(\epsilon,r,\rho) := Y(\epsilon) - \left[ I - Z( \epsilon,r,\rho) \right] \cdot r  \,  .
 \end{equation}
If each component of $P(\epsilon,r,\rho)$ is negative, then there is  a unique $\hat{x}_\epsilon \in B_\epsilon( r , \rho)$ such that $F(\hat{x}_\epsilon) =0$. 
\end{theorem}

\begin{proof}    
Let $r \in \R^3_+$ be a triple such that $P(\epsilon,r,\rho)<0$.
By Proposition \ref{prop:Injective}, if 
%\change[J]{$\epsilon < \tfrac{5}{2} ( 4 + \sqrt{10})^{-1} $}
$\epsilon <\tfrac{\sqrt{10}}{4} $
then $ A^{\dagger}$ is injective. 
Hence $ \hat{x}_{\epsilon} $ is a fixed point of $T$ if and only if $ F( \hat{x}_{\epsilon}) = 0$.  
In order to show  there is a unique fixed point $ \hat{x}_{\epsilon}$, we show that $T$ maps  $ B_{\epsilon}(r,\rho) $ into itself and that $ T $ is a contraction mapping. 

We first show that $T: B_\epsilon(r,\rho) \to B_\epsilon(r,\rho)$. 
Since $ \rho \geq C(\epsilon,r)$ then by Equation~\eqref{e:Cepsr} it follows that $ \| K^{-1} \pi_c T( x) \| \leq \rho$ for all $ x \in B_\epsilon(r,\rho)$.
In order to show that $T(x) \in B_\epsilon(r,\rho)$, it suffices to show that $ r=(r_\alpha , r_\omega, r_c)$ is an upper bound on $ T(x) - \bx_\epsilon$
for all $ x \in B_\epsilon(r,\rho)$.  
We decompose 
%by breaking $T(x) - \bx_\epsilon$ into two parts: 
\begin{equation}\label{e:Tsplit}
	T(x) - \bx_\epsilon = [T(\bx_\epsilon) -\bx_\epsilon] +
	[T(x) - T(\bx_\epsilon)],
\end{equation}
and estimate each part separately. Concerning the first term,
by assumption, $Y(\epsilon)$ is an upper bound on $T(\bx_\epsilon) - \bx_\epsilon$. 
%
Concerning the second term, we claim that $ Z(\epsilon,r,\rho) \cdot r$ is an upper bound on $T(x) - T(\bx_\epsilon)$.
Indeed, we have the following somewhat stronger bound: 
% if $ x,y\in B_\epsilon(r,\rho)$ and $\upperbound{\xi}$ is an upper bound on $y-x$, then
% $Z(\epsilon,r,\rho) \cdot \upperbound{\xi}$ is an upper bound on $T(y) - T(x)$,
% i.e.,
\begin{equation}\label{e:DTisboundedbyZ}
	\LL(T(y) - T(x)) \leq Z(\epsilon,r,\rho) \cdot \LL(y-x)
	\qquad\text{for all } x,y \in B_\epsilon(r,\rho) .
\end{equation}
The latter follows from the mean value theorem, since 
$T$ is continuously Fr\'echet differentiable on $B_\epsilon(r,\rho)$.
%
% MORE DETAILED ARGUMENT PROBABLY NOT NEEDED?
% \begin{equation}
% \label{eq:ZIntegrationBound}
% 	T( y) - T(x) \leq Z(\epsilon,r,\rho) \cdot \upperbound{\xi}
% \end{equation}
% Since $T$ is continuously Frechet differentiable on $B_\epsilon(r,\rho)$,
% then it follows that $T(y) - T(x) = \int_x^y DT( z) dz  $.
% %Since $ B_\epsilon(r,\rho)$ is convex, then $z= x+ s(y-x) \in B_\epsilon(r,\rho)$ for all $s \in [0,1]$.
% By assumption $Z(\epsilon,r,\rho)$ is an upper bound on $DT(z)$ for all $z \in B_\epsilon(r,\rho)$.
% If $ \upperbound{\xi} $ is an upper bound on $ y-x$, then we obtain the inequality $ T(y) - T(x) \leq \int_0^1 Z(\epsilon,r,\rho) \cdot \upperbound{\xi} \, ds$, from which Equation \ref{eq:ZIntegrationBound} follows.
%
Since $r$ is an upper bound on $x - \bx_\epsilon$ for all $ x \in B_\epsilon(r,\rho)$, we find, by using~\eqref{e:Tsplit}, that  
% have obtained that
% \begin{eqnarray}
% 	T(\bx_\epsilon) - \bx_\epsilon &=& \left[ T(\bx_\epsilon) - \bx_\epsilon \right] +
% 	\left[ T(x) - T(\bx_\epsilon) \right] \\
% 	&\leq& Y(\epsilon) + Z(\epsilon,r,\rho) \cdot r
% \end{eqnarray}
% By assumption each component of Equation \ref{eq:RadPolyDef} is negative, so
$Y(\epsilon) + Z(\epsilon,r,\rho) \cdot r \leq r$ (with the inequality, interpreted componentwise, following from $P(\epsilon,r,\rho)<0$) is an upper bound on $T(x) - \bx_\epsilon$
for all $ x \in B_\epsilon(r,\rho)$.  
%It follows that $r$ is an upper bound on $T(x) - \bx_\epsilon $. 
That is to  say, if all of the  radii polynomials are negative, 
then  $T$ maps $B_\epsilon(r,\rho) $ into itself.

To finish the proof we show that $T$ is a contraction mapping. 
We abbreviate $Z=Z(\epsilon,r,\rho)$ and  recall that $r=(r_\alpha,r_\omega,r_c)=(r_1,r_2,r_3) \in \R^3_+$
is such that $Z \cdot r < r$, hence for some $\kappa <1$ we have
\begin{equation}\label{e:defkappa}
  \frac{(Z \cdot r)_i}{r_i} \leq \kappa  \qquad\text{for } i=1,2,3.
\end{equation}

We now need to choose a norm on $X$. 
We define a norm $ \| \cdot \|_r$ on elements $x = (\alpha,\omega,c) \in X$
by
\[  
\| (\alpha, \omega, c) \|_r := \max 
\left\{  		  
	 \frac{|\alpha|}{r_\alpha},
	 \frac{|\omega|}{r_\omega},
	 \frac{\|c\|}{r_{c}} \right\} , 
\]
or
\[
  \|x\|_r = \max_{i=1,2,3} \frac{ \LL(x)_i}{r_i}
  \qquad \text{for all } x \in X.
\]
% We also introduce the compatible norm $ \| \cdot \|_{\tilde{r}}$ on $\R^3$ by
% $ \| (y_1, y_2, y_3) \|_{\tilde{r}} = \max_{i=1,2,3 }
% \{\frac{|y_i|}/{r_i} \}$, so that $\|x\|_r = \| \LL(x) \|_{\tilde{r}}$ for all $x \in X$.
%
By using the upper bound $Z$, we bound the Lipschitz constant of $T$ on $B_\epsilon(r, \rho)$ as follows:
\begin{alignat*}{1}
 \| T(y) - T(x) \|_r 
 % &= \|\LL(T(y) - T(x)) \|_{\tilde{r} } \\
    &= \max_{i=1,2,3} \frac{\LL(T(y) - T(x))_i} {r_i} \\
    &\leq  \max_{i=1,2,3}  \frac{(Z \cdot \LL(y-x))_i}{r_i} \\
    &\leq  \max_{i=1,2,3} \max_{j=1,2,3}\frac{\LL(y-x)_j}{r_j}  
				   \frac{(Z \cdot r )_i}{r_i} \\
    & = \| y-x \|_r \max_{i=1,2,3} \frac{(Z \cdot r )_i}{r_i} \\
    & \leq \kappa \| y-x \|_r,
\end{alignat*}
where we have used~\eqref{e:DTisboundedbyZ} and~\eqref{e:defkappa} with $\kappa<1$.
% \sup_{x,y \in B_\epsilon(r,\rho)} \frac{\|T(y) - T(x) \|_r}{\| y- x\|_r}
% \leq
% \sup_{ x,y \in B_\epsilon(r,\rho)}
% \frac{\left \|
% Z(\epsilon,r,\rho) \cdot \upperbound{\xi}
% \right\|_{\tilde{r}} }{  \| y -x\|_{r}} ,
% \]
% where $\upperbound{\xi}$ is any upper bound on $y-x$, as before.
% \marginpar{THIS IS STILL NOT ENTIRELY CLEAR!}
% If $u \in \R^3$ and $ \|u\|_{\tilde{r}} =1$, then $ \| Z \cdot u\|_{\tilde{r}}$ is maximized when $u=r$.
% Hence $ Lip(T) \leq \| Z(\epsilon,r,\rho) \cdot r \|_r$.
% Since all of the radii polynomials are negative, then $ Z \cdot r < r$component wise, thus proving that $ \|Z \cdot r\|_r <1$ and
Hence $T:B_{\epsilon}(r,\rho) \to B_{\epsilon}(r,\rho)$ is a contraction with respect to the $\| \cdot \|_r$ norm.

% We have thereby proved that  $T:B_{\epsilon}(r,\rho) \to B_{\epsilon}(r,\rho)$ is a contraction mapping.
Since $B_\epsilon(r,\rho)$ with this norm is a complete metric space, by the Banach fixed point theorem $T$~has a unique fixed point $ \hat{x}_\epsilon \in B_\epsilon(r,\rho)$. 
Since $A^\dagger$ is injective,  it follows that $ \hat{x}_\epsilon$   is the unique point in $B_\epsilon(r,\rho)$ for which $ F(\hat{x}_\epsilon) =0$. 
\end{proof}

\begin{remark}\label{r:boundDT}
Under the assumptions in Theorem~\ref{thm:RadPoly},
essentially the same calculation as in the proof above
leads to the estimate
\[
  \| DT(x) y \|_r \leq \kappa \|y\|_r 
  \qquad \text{for all } y \in \R^2 \times \ell^K_0 , 
  \, x \in B_\epsilon(r,\rho),
\]
where $\kappa := \max_{i=1,2,3} (Z\cdot r)_i / r_i$.
\end{remark}


In Appendix \ref{sec:YBoundingFunctions} and Appendix \ref{sec:BoundingFunctions} we construct explicit upper bounds 
$Y(\epsilon)$ and $ Z(\epsilon,r,\rho)$, respectively.  
These functions are constructed such that their components are (multivariate) polynomials in $\epsilon$, $r$ and $ \rho$ with nonnegative coefficients, hence they are increasing in these variables. 
This construction enables us to make use of the uniform contraction principle. 

\begin{corollary}\label{cor:eps0}
Let 
%\change[J]{$0 <\epsilon_0 < \tfrac{5}{2} ( 4 + \sqrt{10})^{-1} $}
 $0 < \epsilon_0 < \tfrac{\sqrt{10}}{4} $ 
and fix some $r = (r_\alpha, r_\omega, r_c) \in \R^3_+$.  
Fix $\rho > 0$ such that $ \rho \geq C(\epsilon_0,  r)$, as given by Lemma~\ref{lem:Crho}.
% as in Proposition \ref{prop:DerivativeEndo}. 
%
Let $Y(\epsilon)$ and $Z(\epsilon,r,\rho)$ be the upper bounds as given in  Propositions~\ref{prop:Ydef} and~\ref{prop:Zdef}. 
Let the radii polynomials $P$ be defined by Equation~\eqref{eq:RadPolyDef}.


If each component of  $P(\epsilon_0, r,\rho)$ is negative, 
then for all $ 0 \leq \epsilon \leq \epsilon_0$ there exists a unique $ \hat{x}_\epsilon \in B_\epsilon(  r , \rho)$ such that $ F(\hat{x}_\epsilon) =0$.  
The solution $\hat{x}_\epsilon$ depends smoothly on $\epsilon$.
\end{corollary}
\begin{proof} 
	Let $0 \leq  \epsilon \leq \epsilon_0$ be arbitrary.
	Because $\rho \geq C(\epsilon_0, r) \geq C(\epsilon, r)$ by Lemma~\ref{lem:Crho},
	Theorem~\ref{thm:RadPoly} implies that it suffices to show that $ P(\epsilon, r ,\rho) <0$. 	
Since  the bounds 
$Y(\epsilon)$ and $ Z(\epsilon,r,\rho)$ are monotonically increasing in their arguments, it follows that $ P(\epsilon,r,\rho) \leq P(\epsilon_0,r,\rho) <0$.  
Continuous and smooth dependence on $\epsilon$ of the fixed point follows from the uniform contraction principle (see for example~\cite{ChowHale}). 
\end{proof}


Given the upper bounds $ Y(\epsilon)$ and $ Z( \epsilon ,r , \rho)$, 
trying to apply Corollary~\ref{cor:eps0} amounts to finding values of $ \epsilon, r_\alpha, r_\omega, r_c,\rho$ for which the radii polynomials are negative.
Selecting a value for $ \rho$ is straightforward: all estimates improve with smaller values of $\rho$, and Proposition \ref{prop:DerivativeEndo} (see also Lemma~\ref{lem:Crho}) explicitly describes the smallest allowable choice of $\rho$ in terms of $ \epsilon,r_\alpha,r_\omega,r_c$. 

Beyond selecting a value for $ \rho$, it is difficult to pinpoint what constitutes an ``optimal'' choice of these variables. 
In general it is interesting to find such  viable radii (i.e.\ radii such that $P(r)<0$) which are both large and small.  
The smaller radius tells us how close the true solution is to our approximate solution. 
The larger radius tells us in how large a neighborhood our solution is unique.  With regard to $\epsilon$, larger values allow us to describe functions whose first Fourier mode is large. However this will ``grow'' the smallest viable radius and ``shrink'' the largest viable radius. 

Proposition \ref{prop:bigboxes} presents two selections of variables which satisfy the hypothesis of Corollary~\ref{cor:eps0}.  
We check the hypothesis is indeed satisfied by using interval arithmetic.
All details are provided in the Mathematica file~\cite{mathematicafile}. 
While the specific numbers used may appear to be somewhat arbitrary (see also the discussion in Remark~\ref{r:largeradii})  they have been chosen to be used later in Theorem 
\ref{thm:WrightConjecture} and Theorem \ref{thm:UniqunessNbd}.  


%%%
%%%BY DOING SOME CHOICES THAT HAVE NO MOTIVATION AT THIS POINT, BUT THAT WILL TURN OUT TO BE USEFUL IN SECTION~\ref{s:global} WE PROVE THE FOLLOWING USING MATHEMATICA  FILES.\marginpar{todo}

\begin{proposition}
		\label{prop:bigboxes}
Fix the constants $ \epsilon_0$, $(r_\alpha, r_\omega,r_c)$  and $\rho$ according to one of the following choices:
% \begin{enumerate}
% \item[\textup{(a)}]  $ \epsilon_0 = 0.029 $ and $ (r_\alpha , r_ \omega , r_c) = (  0.21, \, 0.16 , \, 0.09 ) $ and $\rho = 1.01$;
% \item[\textup{(b)}]  $ \epsilon_0 = 0.087 $ and $ (r_\alpha , r_ \omega , r_c) = (  0.1501, \, 0.0626 , \, 0.2092 ) $ and $\rho = 0.5672$.
% \end{enumerate}
% \note[J]{Version with new numbers below}
\begin{enumerate}
	\item[\textup{(a)}]  $ \epsilon_0 = 0.029 $ and $ (r_\alpha , r_ \omega , r_c) = (  0.13, \, 0.17 , \, 0.17 ) $ and $\rho = 1.78$; 
	\item[\textup{(b)}]  $ \epsilon_0 = 0.09 $ and $ (r_\alpha , r_ \omega , r_c) = (  0.1753, \, 0.0941 , \, 0.3829 ) $ and $\rho = 1.5940$. 
\end{enumerate}
For either of the choices (a) and (b) we have the following: 
for all $0 \leq \epsilon \leq \epsilon_0$ there exists a unique point 
$(\hat{\alpha}_\epsilon,\hat{\omega}_\epsilon,\hat{c}_\epsilon) \in B_{\epsilon}(r,\rho)$ 
satisfying $F_\epsilon(\hat{\alpha}_\epsilon,\hat{\omega}_\epsilon,\hat{c}_\epsilon) = 0$ and 
\[ 	
 | \hat{\alpha}_\epsilon - \balpha_\epsilon| \leq r_\alpha , 
 \quad
 |\hat{\omega}_\epsilon - \bomega_\epsilon| \leq  r_\omega  ,
 \quad
 \| \hat{c}_\epsilon - \bc_\epsilon\| \leq r_c     ,
 \quad
 \| K^{-1} \hat{c}_\epsilon \| \leq \rho  .
\]
\end{proposition}
\begin{proof}
In the Mathematica file~\cite{mathematicafile}  we check, using interval arithmetic, that  $\rho \geq C(\epsilon_0, r)$ and  the radii polynomials $P(\epsilon_0,r,\rho)$ are negative for the choices (a) and (b). The result then follows from Corollary~\ref{cor:eps0}.	
\end{proof}


\begin{remark}\label{r:largeradii}	
In Proposition~\ref{prop:bigboxes} we aimed for large balls on which the solution is unique.
Even for a fixed value of $ \epsilon$, it is not immediately obvious how to find a ``largest'' viable radius $r$, 
since $r$ has three components. In particular, there is a trade-off between the different components of $r$. On the other hand, as explained in Remark~\ref{r:smallradii}, no such difficulty arises when looking for a ``smallest'' viable radius.
\end{remark}




We will also need a rescaled version of the radii polynomials, which takes into account the asymptotic behavior of the bound $Y$ on the residue $T(\bar{x}_\epsilon) -\bar{x}_\epsilon = - A^\dagger F(\bx_\epsilon)$  as $\epsilon \to 0$, namely it is of the form
$Y(\epsilon)= \epsilon^2 \tilde{Y}(\epsilon)$,
see Proposition~\ref{prop:Ydef}.
The proofs of the following monotonicity properties can be found in 
Appendices~\ref{sec:YBoundingFunctions} and~\ref{sec:BoundingFunctions}. 
\begin{lemma}\label{lem:YZ}
Let $\epsilon \geq 0$, $\rho >0$ 
and $r \in\R^3_+$. 
Then there are upper bounds
$Y(\epsilon) =\epsilon^2 \tilde{Y}(\epsilon)$ on $ T(\bx_\epsilon) - \bx_\epsilon$ and a (uniform) upper bound 
$Z(\epsilon , r ,\rho) $  on $ DT(x) $ for all $ x \in B_\epsilon(r,\rho)$.
These bounds are given explicitly by Propositions~\ref{prop:Ydef} and~\ref{prop:Zdef}, respectively. Moreover, $\tilde{Y}(\epsilon)$ is nondecreasing in $\epsilon$,
while $Z(\epsilon , r ,\rho) $ is nondecreasing in $\epsilon$, $r$ and $\rho$.
\end{lemma}

This implies, roughly speaking, that if we are able to show that $T$ is a contraction map on 
$B_{\epsilon_0}( \epsilon_0^2 \rr,\rho)$ for a particular choice of $ \epsilon_0$, then it will be a contraction map on $B_\epsilon( \epsilon^2 \rr,\rho)$ for all $ 0 \leq \epsilon \leq \epsilon_0$. Here, and in what follows, we use the notation $r = \epsilon^2 \rr$ for the $\epsilon$-scaled version of the radii. 



\begin{corollary}
	\label{cor:RPUniformEpsilon}
	Let  
	 $0 < \epsilon_0 < \tfrac{\sqrt{10}}{4} $ 
	and fix some $\rr = (\rr_\alpha, \rr_\omega, \rr_c) \in \R^3_+$. 
	Fix $\rho > 0$ such that $ \rho \geq C(\epsilon_0, \epsilon_0^2 \rr)$, as given by Lemma~\ref{lem:Crho}. 
	Let $Y(\epsilon)$ and $Z(\epsilon,r,\rho)$ be the upper bounds as given by Lemma~\ref{lem:YZ}.  
Let the radii polynomials $P$ be defined by~\eqref{eq:RadPolyDef}. 

	If each component of  $P(\epsilon_0,\epsilon_0^2 \rr,\rho)$ is negative, 
	then for all $ 0 \leq \epsilon \leq \epsilon_0$ 
	there exists a unique $ \hat{x}_\epsilon \in B_\epsilon(\epsilon^2  \rr , \rho)$ 
	such that $ F(\hat{x}_\epsilon) =0$. 
	Furthermore, $\hat{x}_\epsilon$ depends smoothly on $\epsilon$.
\end{corollary}

\begin{proof}
	 Let $0 \leq  \epsilon < \epsilon_0$ be arbitrary.
	 Because $\rho \geq C(\epsilon_0,\epsilon_0^2 \rr) \geq C(\epsilon,\epsilon^2 \rr)$ by Lemma~\ref{lem:Crho},
	Theorem~\ref{thm:RadPoly} implies that it suffices to show that $ P(\epsilon,\epsilon^2 \rr ,\rho) <0$. 
	By using the monotonicity provided by Lemma~\ref{lem:YZ}, we obtain
	\begin{alignat*}{1}
		P(\epsilon,\epsilon^2 \rr ,\rho) &= Y(\epsilon) 
- \left[ I - Z(\epsilon,\epsilon^2 \rr,\rho)\right] \cdot \epsilon^2 \rr \\
		&=  (\epsilon / \epsilon_0)^{2} \left[ \epsilon_0^2   
		  \tilde{Y}(\epsilon) - \epsilon_0^2 \rr 
		+  Z(\epsilon,\epsilon^2 \rr,\rho) \cdot \epsilon_0^2 \rr  \right] \\
		&\leq  (\epsilon / \epsilon_0)^{2} \left[ \epsilon_0^2  
		  \tilde{Y}(\epsilon_0)  - \epsilon_0^2 \rr 
   +  Z(\epsilon_0,\epsilon_0^2 \rr,\rho) \cdot \epsilon_0^2 \rr  \right] \\
		&= (\epsilon / \epsilon_0)^{ 2} P(\epsilon_0 , \epsilon_0^2 \rr,\rho) \\
		& < 0,
	\end{alignat*}
where inequalities are interpreted componentwise in $\R^3$, as usual.
\end{proof}




%%%%%
%%%%%		THIS IS THE OLD VERSION OF THE UNIFORM \EPSILON^2 THEOREM
%%%%%
%%%%%\begin{corollary}
%%%%%	\label{prop:RPUniformEpsilon}
%%%%%	Let $ 0 < \epsilon_0 < \tfrac{5}{2} ( 4 + \sqrt{10})^{-1}$ and fix some $r = (r_\alpha, r_\omega, r_c) \in \R^3_+$ and 
%%%%%	fix $ k \in \{ 0,1,2\}$.  
%%%%%	Fix $\rho > 0$ such that $ \rho \geq C(\epsilon_0, (\epsilon_0)^2 r)$, as given by Lemma~\ref{lem:Crho}. 
%%%%%	Let $Y(\epsilon)$ and $Z(\epsilon,r,\rho)$ be the upper bounds as given by~\ref{lem:YZ}.  
%%%%%	Let the radii polynomials $P$ be defined by~\eqref{eq:RadPolyDef}.
%%%%%	If each component of  $P(\epsilon_0,{\epsilon_0}^k r,\rho)$ is negative, 
%%%%%	then for all $ 0 \leq \epsilon \leq \epsilon_0$ there exists a unique $ \hat{x}_\epsilon \in B_\epsilon(\epsilon^k  r , \rho)$ such that $ F(\hat{x}_\epsilon) =0$. Furthermore, $\hat{x}_\epsilon$ depends smoothly on $\epsilon$.
%%%%%\end{corollary}
%%%%%
%%%%%\begin{proof}
%%%%%	Let $0 \leq  \epsilon < \epsilon_0$ be arbitrary.
%%%%%	Because $\rho \geq C(\epsilon_0,\epsilon_0^k r) \geq C(\epsilon_0,\epsilon_0^k r)$ by Lemma~\ref{lem:Crho},
%%%%%	Theorem~\ref{thm:RadPoly} implies that it suffices to show that $ P(\epsilon,\epsilon^k r ,\rho) <0$. 
%%%%%	By using the monotonicity provided by Lemma~\ref{lem:YZ}, we obtain
%%%%%	\begin{alignat*}{1}
%%%%%	P(\epsilon,\epsilon^k r ,\rho) &= Y(\epsilon) - \left[ I - Z(\epsilon,\epsilon^k r,\rho)\right] \cdot \epsilon^k r \\
%%%%%	&=  (\epsilon / \epsilon_0)^{k} \left[ \epsilon_0^k  \epsilon^{2-k}  \tilde{Y}(\epsilon) - \epsilon_0^k r +  Z(\epsilon,\epsilon^k r,\rho) \cdot \epsilon_0^k r  \right] \\
%%%%%	&\leq  (\epsilon / \epsilon_0)^{k} \left[ \epsilon_0^k  \epsilon_0^{2-k}  \tilde{Y}(\epsilon_0)  - \epsilon_0^k r +  Z(\epsilon_0,\epsilon_0^k r,\rho) \cdot \epsilon_0^k r  \right] \\
%%%%%	&= (\epsilon / \epsilon_0)^{ k} P(\epsilon_0 , \epsilon_0^k r,\rho) \\
%%%%%	& < 0,
%%%%%	\end{alignat*}
%%%%%	where inequalities are interpreted componentwise in $\R^3$, as usual.
%%%%%\end{proof}
%%%%%




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\subsection{Application of Radii Polynomials}

%\begin{remark}


These $\epsilon$-rescaled variables are used in
Proposition~\ref{prop:TightEstimate} below to derive \emph{tight} bounds on the
solution (in particular, tight enough to conclude that the bifurcation is
supercritical). The following remark explains that the monotonicity properties of
the bounds $Y$ and $Z$ imply that looking for small(est) radii which satisfy $P(r)<0$, is
a well-defined problem.


\begin{remark}\label{r:smallradii}
The set $R$ of radii for which the radii polynomials are negative is given by 
\[
  R := \{ r \in \R^3_+ : r_j > 0,  P_i(r) < 0 \text{ for } i,j=1,2,3 \} .
\] 
This set has the property that if
	$r,r' \in R$, then $r''\in R$, where $r''_j=\min\{ r_j,r'_j\}$.
Namely, the main observation is that we can write 
	$P_i(r)= \tilde{P}_i(r)-r_i$, where $\partial_{r_j} \tilde{P}_i \geq 0$ for all $i,j=1,2,3$.
Now fix any $i$; we want to show that $P_i(r'') < 0$.	
We have either $r''_i=r_i$ or $r''_i=r'_i$, hence assume $r''_i=r_i$ (otherwise
just exchange the roles of $r$ and $r'$). We infer that $P_i(r'') \leq P_i(r) <
0$, since $\partial_{r_j} P_i \geq 0$ for $j \neq i$.
We conclude that there are no trade-offs in looking for minimal/tight radii, as
opposed to looking for large radii, see Remark~\ref{r:largeradii}.
\end{remark}

%%%
%%%The optimization problem is simplified to a degree because the region $ P(\epsilon_0,r,\rho_0) <0$ is convex for fixed $\epsilon_0$ and $ \rho_0 $.  
%%%This is because the function $Z(\epsilon,r,\rho)$ is constructed out of polynomials with non-negative coefficients, whereby $\tfrac{\partial}{ \partial r_i} \tfrac{\partial }{\partial r_j} P_{r_k}(\epsilon_0,r,\rho_0) >0$ for all $ i,j,k \in \{ \alpha, \omega,c\}$. 
%%%\marginpar{I believe this is true, right? -JJ}


\begin{proposition}
		\label{prop:TightEstimate}
	Fix $ \epsilon_0 = 0.10$ and 
%\change[J]{$ (\rr_\alpha , \rr_ \omega , \rr_c) = (  0.1149, \, 0.0470 , \, 0.4711 ) $}
$ (\rr_\alpha , \rr_ \omega , \rr_c) = (  0.0594, \, 0.0260 , \, 0.4929 ) $ 
and 
%\change[J]{$\rho = 0.0279$}
$\rho = 0.3191$. 
	For all $0< \epsilon \leq \epsilon_0$ there exists a unique point $\hat{x}_\epsilon = (\hat{\alpha}_\epsilon,\hat{\omega}_\epsilon,\hat{c}_\epsilon)$ 
	satisfying $F(\hat{x}_\epsilon) = 0$ and 
	\begin{align}
	\label{eq:TightBound}
 | \hat{\alpha}_\epsilon - \balpha_\epsilon| <& \rr_\alpha \epsilon^2 , 
 %
 &|\hat{\omega}_\epsilon - \bomega_\epsilon| <&  \rr_\omega \epsilon^2 ,
 %
 &
 \| \hat{c}_\epsilon - \bc_\epsilon\| <& \rr_c  \epsilon^2   ,
  %
  &
  \| K^{-1} \hat{c}_\epsilon \| <& \rho  .
	\end{align}
Furthermore, $\hat{\alpha}_\epsilon > \pp$ for $ 0 < \epsilon < \epsilon_0$.
\end{proposition}

\begin{proof}
	In the Mathematica file~\cite{mathematicafile}  we check, using interval arithmetic, that  $\rho \geq C(\epsilon_0, \epsilon_0^2 \rr)$ and  the radii polynomials $P(\epsilon_0,\epsilon_0^2 \rr,\rho)$ are negative.  
	%I DO NOT UNDERSTAND THE NEXT SENTENCE
 The inequalities in Equation~\eqref{eq:TightBound} follow from Corollary~\ref{cor:RPUniformEpsilon}. 
 Since $\hat{\alpha}_\epsilon \geq \balpha_\epsilon - \rr_\alpha \epsilon^2
 = \pp +\frac{1}{5}(\frac{3\pi}{2}-1)\epsilon^2 - \rr_\alpha \epsilon^2$ and $ \rr_\alpha < \tfrac{1}{5} ( \tfrac{3 \pi}{2} -1) $, it follows that $ \hat{\alpha}_\epsilon > \pp $ for all $ 0 < \epsilon \leq \epsilon_0$. 
%%
%%STILL NEEDS AN EXPLANATION
%%\marginpar{Jonathan: I am not sure what the argument is \dots}
%% WHY IT IS UNIQUE IN THE BALL GIVEN BY~\eqref{eq:TightBound}. CLEARLY IT IS UNIQUE IN $B_\epsilon(r,\rho)$
%%WITH $\rho= C( \epsilon_0,\epsilon_0^2 r)$. WHY CAN THERE BE NO SOLUTIONS WITH
%%$\| K^{-1} c \| > \rho$ SATISFYING~\eqref{eq:TightBound} ? 
\end{proof}

\begin{remark}\label{r:nested}
% The pivotal result in Proposition~\ref{prop:TightEstimate} is that $\hat{\alpha}_\epsilon > \pp$, which implies that the bifurcation is subcritical.
Since $\epsilon_0^2\rr < r$ for the choices (a) and (b) in Proposition~\ref{prop:bigboxes},
and the choices of $\rho$ and $\epsilon_0$ are compatible as well, the solutions found in Proposition~\ref{prop:bigboxes} are the same as those described by Proposition~\ref{prop:TightEstimate}. While the former proposition provides large isolation/uniqueness neighborhoods for the solutions,
the latter provides tight bounds and confirms the  supercriticality of the bifurcation suggested in Definition \ref{def:xepsilon}.

% The bifurcation is supercritical (see eg.  \cite{faria2006normal} p 252
	
	
	
%		We note that for each (appropriate) $\epsilon$, the ball 
%	$ B_{\epsilon}(r,\rho)$ from Proposition \ref{prop:TightEstimate} is contained within the balls   
%	$ B_{\epsilon}(r_a,\rho_a)$  and 
%	$ B_{\epsilon}(r_b,\rho_b)$ from Proposition \ref{prop:bigboxes}. 
%	This means that the fixed points $  \hat{x}_{\epsilon} \in B_{\epsilon}(r,\rho)$ is the same fixed point $\hat{x}_{\epsilon} \in B_{\epsilon}(r_a,\rho_a)$ .

\end{remark}


%
%The method of radii polynomials is versatile. 
%With the goal of later proving Corollary \ref{prop:UniqunessNbd}, we added additional constraints to \emph{Mathematica}'s function \emph{NMaximize} to find the parameters for Proposition \ref{prop:WideEstimate}.
%
%When searching for the largest viable radius we add an additional constraint. 
%In Proposition \ref{prop:Cone}, we showed that for a given selection of $ \epsilon$, $r_\alpha$ and $ r_\omega$, then the unscaled variable $ \|c\|$ is either $\cO(1)$ or $\cO(\epsilon^2)$. 
%When we scale $c \to \epsilon c$, then we are only able to prove uniqueness of our solution in an $ \epsilon-$cone about the approximate solution. 
%We use this Proposition to select $r_c = ????$ in terms of $\epsilon$, $r_\alpha$ and $r_\omega$ so that any unscaled  solution $c$ is either $\cO(1)$ or $ c \in B_{\epsilon}(r,\rho)$.
%
%Even still, the larger we choose $ \epsilon$, the smaller we will need to take $ r_\omega$ in order to have a proof. 
%For the following theorem, we fixed $ \epsilon_0 =0.085$ and used \emph{NMaximize} to find a choice of variables $(\epsilon,r_\alpha,r_\omega,r_c)$  which maximized the objective function $r_w$ and for which all the radii polynomials were negative. 
%By slightly shrinking the estimate for the optimal radii, we obtain the following theorem.

