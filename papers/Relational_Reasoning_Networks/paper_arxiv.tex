\documentclass[journal]{IEEEtran}

%\usepackage[retainorgcmds]{IEEEtrantools}
%\usepackage{bibentry}  
\usepackage{xcolor,soul,framed} %,caption
\colorlet{shadecolor}{yellow}
% \usepackage{color,soul}
\usepackage[pdftex]{graphicx}
\graphicspath{{../pdf/}{../jpeg/}}
\DeclareGraphicsExtensions{.pdf,.jpeg,.png}

\usepackage[cmex10]{amsmath}
%Mathabx do not work on ScribTex => Removed
%\usepackage{mathabx}
\usepackage{array}
\usepackage{xcolor}
\usepackage{color, colortbl}
\usepackage{mdwmath}
\usepackage{mdwtab}
\usepackage{eqparbox}
\usepackage{url}
\usepackage{amsmath,amssymb,amsfonts,amsthm,mathrsfs}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{subcaption}


\newtheorem{problemdef}{Problem Definition}
\newtheorem{notation}{Notation}
\newtheorem{result}{Result}
\newtheorem{definition}{Definition}
\newtheorem{summary}{Summary}
\newtheorem{theorem}{Theorem}
\newtheorem{lemma}{Lemma}
%\newtheorem*{proof}{Proof}
\newtheorem{corollary}{Corollary}
\newtheorem{example}{Example}
\newtheorem{proposition}{Proposition}
\newtheorem{remark}{Remark}
\newcommand{\B}[1]{\boldsymbol #1}
\definecolor{Gray}{gray}{0.9}
\definecolor{dgreen}{rgb}{0.25, 0.23, 0.1}
\newcommand{\mc}{\mathcal}
\newcommand{\gm}[1]{\textcolor{cyan}{#1}}
\newcommand{\fg}[1]{\textcolor{blue}{#1}}
\newcommand{\md}[1]{\textcolor{dgreen}{#1}}

%use this for final changes in the text:
\newcommand{\ar}[1]{\textcolor{black}{#1}}
\newcommand{\arr}[1]{\textcolor{blue}{#1}}



\newtheorem{innercustomthm}{Theorem}
\newenvironment{appxthm}[1]
  {\renewcommand\theinnercustomthm{#1}\innercustomthm}
  {\endinnercustomthm}

%=== TITLE & AUTHORS
\begin{document}
%\bstctlcite{IEEEexample:BSTcontrol}
    \title{
    % Learning Representations for \\ Sub-Symbolic Reasoning 
    Relational Reasoning Networks
    \thanks{}}
  \author{Giuseppe Marra, Michelangelo Diligenti, Francesco Giannini \textbf{}
\thanks{G. Marra is with the Department of Computer Science, KULeuven, Leuven, Belgium, (e-mail: giuseppe.marra@kuleuven.be). M. Diligenti and F. Giannini are with the Department of Information Engineering and Mathematics, University of Siena, Siena, Italy, (e-mail: \{michelangelo.diligenti,francesco.giannini\}@unisi.it).}}


% The paper headers
% \markboth{IEEE TRANSACTIONS ON MICROWAVE THEORY AND TECHNIQUES, VOL.~60, NO.~12, DECEMBER~2012
% }{Roberg \MakeLowercase{\textit{et al.}}: High-Efficiency Diode and Transistor Rectifiers}


% ====================================================================
\maketitle



% === ABSTRACT ====================================================================
\begin{abstract}
Neuro-symbolic methods integrate neural architectures, knowledge representation and reasoning. However, they have been struggling at both dealing with the intrinsic uncertainty of the observations and scaling to real-world applications.
This paper presents Relational Reasoning Networks (R2N), a novel end-to-end model that performs relational reasoning in the latent space of a deep learner architecture, where the representations of constants, ground atoms and their manipulations are learned in an integrated fashion.
Unlike flat architectures like Knowledge Graph Embedders, which can only represent  relations between entities, R2Ns define an additional computational structure, accounting for higher-level relations among the ground atoms.
The considered relations can be explicitly known, like the ones defined by logic formulas, or defined as unconstrained correlations among groups of ground atoms. 
R2Ns can be applied to purely symbolic tasks or as a neuro-symbolic platform to integrate learning and reasoning in heterogeneous problems with both symbolic and feature-based represented entities. The proposed model overtakes the limitations of previous neuro-symbolic methods that have been either limited in terms of scalability or expressivity.
The proposed methodology is shown to achieve state-of-the-art results in different experimental settings.
\end{abstract}

% === KEYWORDS ===========
\begin{IEEEkeywords}
Neuro-Symbolic methods, \and First-Order Logic, \and Knowledge Graph Embeddings, \and Relational Reasoning
\end{IEEEkeywords}

% For peerreview papers, this IEEEtran command inserts a page break and
% creates the second title. It will be ignored for other modes.
\IEEEpeerreviewmaketitle

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% === I. INTRODUCTION

% This led to influential
% debates on the impact of AI both on academia and industry



\section{Introduction}
\label{sec:introduction}
%Enriching neural architectures with the ability of developing data representations in presence of relational knowledge is a major goal of neuro-symbolic methods\ar{~\cite{hitzler2022neuro}}. 
\ar{Enriching deep learning models with explicit reasoning capabilities is a fundamental challenge in the AI agenda to realize semantically sound, robust and trustworthy AI systems~\cite{hitzler2022neuro,garcez2020neurosymbolic,marcus2020next,de2020statistical}. This is especially relevant for industrial applications, where (potentially large) relational databases are generally available as input knowledge to solve different tasks like query answering or knowledge base completion.} 
A powerful class of methodologies \cite{raedt2016statistical} uses logic formulas as templates for Probabilistic Graphical Models \ar{(PGM)}, which explicitly encode statistical dependencies among entities and their relationships. However, the application of these methods has been strongly limited due to their computational complexity. %, which prevents them to be applied to large scale domains. 
An interesting line of research is represented by solutions focusing
on reasoning over a set of objects represented via trainable embeddings~\cite{wang2017knowledge}, like Knowledge Graph Embeddings (KGE). Once the entities and relations are embedded, it is possible to use classical distance-based or neural-based decision functions to generalize the knowledge to new embedded facts, while scaling to very large domains.
However, a limitation of these approaches is that they model statistical regularities among relations and/or entities but they fail to detect and exploit higher level logical knowledge, like formulas or programs. %This limitation is particularly relevant in complex domains where inferring the truth of a query may require multiple hops to get to a correct deduction.
\ar{
% Integrating embedding representations, probabilistic logic inference and neural architectures has been an important goal of the AI research~\cite{manhaeve2018deepproblog,sourek2018lifted,zhang2019efficient}
Devising a model that can effectively and efficiently preserve and integrate the benefits of embedded representations, probabilistic logic inference and neural architectures is still an open research problem.}
\ar{In this regard, the presented work represents a relevant step forward towards this integration,
% in the integration of embedded representations, probabilistic logic inference and neural architecture, 
as it enables to perform probabilistic logic reasoning on embedding representations by means of neural computations.}
\ar{This claim is also supported by achieving state-of-the-art results in the experimental analysis.}

\ar{This paper introduces} Relational Reasoning Networks (R2N), a class of models that exploits \ar{logic} knowledge in a \ar{relational domain} to define its structure and to produce \ar{semantically refined embedding} representations.
% By compiling the relations into the model structure,
R2Ns jointly develop integrated representations for constants, ground atoms and ground knowledge by computing multiple sub-symbolic reasoning steps in a latent space. 
The learning of \ar{embedding} representations allows the model to \ar{automatically} select whether \ar{and how much} some specific \ar{logic} knowledge is useful for the specific task. As a result, R2Ns maintain enough expressive power to perform relational reasoning under uncertainty, while allowing to scale up to larger relational \ar{settings} compared to PGM-based approaches. 
R2N inference mechanism is divided into three phases. \ar{Firstly}, symbolic ground atoms
are embedded in a latent space using representation learning techniques, e.g. KGE. \ar{Secondly}, the relations among atoms provided in the \ar{logic theory} are used to sub-symbolically aggregate and refine the atom embeddings in a multi-layer fashion. This phase resembles multi-hop automated reasoning\ar{, as we discuss in Sec. \ref{sec:Relwork},} but in a latent space. \ar{Embedding} representations of the \ar{cor}related groups of atoms, i.e. \ar{ground logic rules},
are also computed \ar{in} this phase.
\ar{Lastly}, the final embedding \ar{representations} are used to make predictions. These predictions can be used to train the model end-to-end with respect to a supervised task.
% figure~\ref{fig:model} shows an overview of the model. 

\ar{\textbf{Contributions.}} The main contribution of the paper is the introduction of a neuro-symbolic architecture performing relational reasoning in a latent space. Distilling the relational structure at the embedding level allows to perform both learning and reasoning in a more scalable way. Experimental results show the effectiveness of the approach achieving state-of-the-art results on different experimental settings.
In addition, the proposed methodology is very general, as it can be used  both in relational tasks where the prior \ar{logic} knowledge is explicitly available, even if noisy (e.g. predictions can deviate from the knowledge with a variable degree that must be co-learned), and in tasks where the relational structure is present but left latent. Last but not least, the methodology provides a general platform for neuro-symbolic integration~\cite{de2020statistical}, as it can be transparently applied to pure symbolic inputs, or to cases where the inputs have a feature-based representation, like images.
The experimental section provides examples of applications of all these cases.

The outline of the paper is the following:
\ar{Sec.~\ref{sec:preliminary} summarizes basic notions relevant to define our model, while Sec.~\ref{sec:problem} explicitly formulates the problem statement that is solved by the proposed model.} Sec. \ref{sec:model} presents the model
and how it approximates the reasoning process,
Sec. \ref{sec:bp} provides insights and theoretical guarantees for the reasoning process implemented by R2Ns, and Sec. \ref{sec:exp_results} shows the experimental results on multiple datasets and learning tasks. 
\ar{Finally, Sec.~\ref{sec:Relwork} presents an overview of related works, while} some conclusions and remarks on future works are drawn in Sec. \ref{sec:conclusions}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\ar{\section{Preliminary}
\label{sec:preliminary}}
\ar{\subsection{First-Order Logic}
\label{sec:fol}}
\ar{We consider a} function-free First-Order Logic (FOL) language \ar{$\mathcal{L}$, composed of
a} finite set of constants \ar{$\mathcal{C}$} for specific domain entities, a set \ar{$\mathcal{X}$} of variables for anonymous entities  and a set \ar{$\mathcal{P}$ of} $n$-ary predicates for relations among constants. 
Given an $n$-ary predicate $P$ and a tuple $(t_1,\ldots,t_n)$ of constants or variables, $P(t_1,\ldots,t_n)$ is called an \emph{atom}. If $t_1,\ldots,t_n$ are only constants, $P(t_1,\ldots,t_n)$ is called a \emph{ground atom}. 
The set of all the possible ground atoms of a FOL language is called Herbrand Base (HB). 
\ar{\begin{example}[FOL Language]
\label{ex:fol}
Consider a set of constants $\mathcal{C} = \{a, b, c\}$ representing people (e.g. (a)lice, (b)ob and (c)arl), and the predicate set $\mathcal{P} = \{S(\cdot), F(\cdot,\cdot)\}$ stating if a person (S)mokes or if two people are (F)riends, respectively. The Herbrand Base of this language is composed of 12 ground atoms, obtained by grounding the predicate $S$ over the constants in $\mathcal{C}$ and the predicate $F$ over the pair of constants in $\mathcal{C}$, i.e. $HB=\{S(a),S(b),S(c),F(a,a),F(a,b),\ldots, F(c,c)\}$.
\end{example}}
\ar{Our approach considers a logic theory $\mathcal{T}$
%\footnote{Quantifiers are omitted as R2Ns automatically learn how frequently satisfy any logic rule, according to the available training data. \fg{forse fare dopo e un po' più esteso questo discorso? ogni R2N è come apprendesse exists\_n, se n va a inf è perogni, sennò è exist 1}} 
as a set of unquantified FOL rules $r_i$, each depending on a set of variables $X_i\subset\mathcal{X}$, i.e. $\mathcal{T} =\{r_1(X_1), r_2(X_2), \ldots, r_m(X_m)\}$.} 
\ar{A substitution $\theta$ for $X_i$ is a replacement of constants in $\mathcal{C}$ to the variables in $X_i$}.
\ar{Given a rule $r_i(X_i)$ and a substitution $\theta$ for $X_i$, we obtain a ground rule %$r_i(X_i\theta)$
by applying the substitution $\theta$ to the variables in $X_i$.
\textit{Grounding} a rule $r_i(X_i)$ refers to the process of applying all the possible substitutions $\Theta$ for $X_i$ given $\mathcal{C}$, where $|\Theta| =|\mathcal{C}|^{|X_i|}$.}
\ar{We indicate by $R_i$ the set of all the groundings of $r_i(X_i)$. Grounding the entire $\mathcal{T}$ refers to grounding all its rules and the resulting set is indicated by $R$.}
\ar{\begin{example}[Logic Theory]
\label{ex:logic_theory}
Consider $\mathcal{T}=\{r_1(X_1)\}$, with $r_1(X_1) = S(x) \land F(x,y) \rightarrow S(y)$ stating that ``if a person $x$ smokes and $x$ and $y$ are friends, then also $y$ smokes". Here, $X_1 = \{x,y\}$ is the set of variables of $r_1$. For instance, we can ground $r_1(X_1)$ by the substitution $\theta = \{x:a,\ y:b\}$ for $X_1$, thus obtaining $S(a) \land F(a,b) \rightarrow S(b)$. If $\mathcal{C}=\{a,b,c\}$ we get $R_1=\{S(a) \land F(a,a) \rightarrow S(a), S(a) \land F(a,b) \rightarrow S(b),\ldots, S(c) \land F(c,c) \rightarrow S(c)\}$, with $|R_1|=3^2=9$.
\end{example}}
\ar{Logic theories can benefit from graph representations to define inference and learning algorithms according to their underlying relational structures. In this paper, we encode two components of our logic language as graphs: (i) the set HB of ground atoms as an n-ary knowledge graph (Sec. \ref{sec:kge}); (ii) the set of ground rules $R$ as a factor graph (Sec. \ref{sec:fg}).}
% \gm{
% \subsection{POSSIBILE SEZIONE UNIFICANTE: Graph representations for FOL}
% \begin{itemize}
%     \item It is often useful to provide a graph representation of logical theories since it allows to more easily define inference, reasoning and learning algorithms. This is a common practice in the communities like KGE and StarAI.
%     \item In this paper, we are interested in encoding two different components of our logical language as graphs: the Herbrand Base (set of ground atoms) and the Logic Theory (set of ground rules).
%     \item Since a (binary) HB is equivalent to a knowledge graph, knowledge graphs are a first candidate for such representations. For HBs with general arities, a tranditional strategy is \textit{reification}, where both entities and facts (ground atoms) are turned into nodes. A bipartite graph is then created, where there exists an edge between a entity node and an atom node if the the fact refers to that entity (e.g. an edge between $a$ and $S(a)$).
%     \item Logic Theoris are similarly represented as bipartite graphs, where nodes are split into atom nodes and ground rule nodes, and there exists an edge between an atom node and a ground rule node if the atom occurs in the ground rule.  This is a common strategy in StarAI, where logic theories are turned into PGMs. Here, the bipartite graph is often referred to as factor graphs, because ground rules are used to \textit{factorize} the correspondent probability distribution.
% \end{itemize}
% }

\begin{figure}[t]
  \begin{subfigure}[t]{0.49\linewidth}
    \centering\includegraphics[width=\linewidth,trim={0 0 1500 0},clip]{nary-KGE.png}
    \caption{$n$-ary KGE}
  \end{subfigure}
   \begin{subfigure}[t]{0.49\linewidth}
    \centering\includegraphics[width=\linewidth,trim={0 0 1500 0},clip]{logic-based-factor-graph.png}
    \caption{Logic-based Factor graph}
  \end{subfigure}
    \caption{\ar{\textbf{(a)} An $n$-ary KGE where different arrows correspond to different predicates, here we used the dashed one for $S$ and the solid one for $F$.}
    % A KGE learns the embeddings of both constants $a,b,c$ and predicates $S,F$. 
    \ar{\textbf{(b)} A factor graph for the logic theory  $\mathcal{T} = \{S(x) \land F(x,y) \rightarrow S(y)\}$, grounded for the constants $\{a, b, c\}$, where each ground formula is highlighted with a different colour. In both graph representations some nodes/edges have been omitted for readability reasons.}}
    \label{fig:graphical_model_to_transformer}
\end{figure}


\subsection{Knowledge Graph Embeddings}
\label{sec:kge}

Knowledge graphs (KG) \ar{are graph data structures representing relational knowledge} consisting of facts (ground atoms) in form of triples formed by two entities (constants) and a relation (\ar{binary} predicate). In a KG each considered entity is a node and each fact is a relation establishing an edge between two entities. KGs are
incomplete and Knowledge Graph Embeddings (KGE) are a powerful approach for populating KGs by mapping entities and relations to a latent representation, which generalizes the assignments to unknown facts. \ar{Indeed}, KGE methods learn the entity and relation embeddings by defining scoring functions that are trained to match the supervisions.
\ar{\begin{example}[KGE]
\label{ex:kge}
Let $F(a,b)$ be a fact of a KG with $e_a,e_b,W_F$ indicating trainable embedding vectors for entities $a,b$ and the relation $F$, respectively. \emph{TransE}~\cite{bordes2013translating} is a well-known KGE tha models relations as translation operations on the embeddings of the entities, where the internal atom representation $e_a + W_F - e_b$ is scored as $1 / (1 + \|e_a + W_F - e_b\|)$.
\end{example}}
\ar{%A HB with only binary predicates can be trivially expressed by a knowledge graph, but
KGs can be extended to an HB with generic $n$-ary relations, where the bipartite graph representation is generalized such that an $n$-ary atom establishes $n$ edges from the atom node to the $n$ entities appearing in the atom, see e.g. Fig. \ref{fig:graphical_model_to_transformer}-(a).
%Even if beyond the scope of this paper, 
Many KGE approaches can also be trivially extended to $n$-ary relations as discussed by Fatemi et al.~\cite{fatemi2021knowledge}.
}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\ar{\subsection{Encoding grounded FOL Formulas as Factor Graphs}
\label{sec:fg}}
\ar{A common practice in the field of Statistical Relational AI (StarAI)~\cite{de2008survey} is to map FOL theories to undirected probabilistic graphical models, represented via factor graphs~\cite{haykin1994}, to get advantage of the available standard tools for model training and inference. This paper exploits the translation used by Markov Logic Networks (MLN)~\cite{richardson2006markov}, mapping a logic theory into a factor graph by grounding all the rules.}
%%%%%%%%%%%%%%% 

\ar{A factor graph $(V,F,\mc{E})$ is a bipartite undirected graph, whose vertices are divided into two disjoint sets: the variable nodes $V$ and the factor nodes $F$. Edges in $\mc{E}$ connect nodes in $V$ to nodes in $F$. A \textit{logic-based factor graph} can be built given the HB of a FOL language and a logic theory $\mathcal{T}$ by:
\begin{itemize}
\item adding a variable node $a$ for each ground atom: $V=HB$; %; we call this variable node an \textit{atom} node.
\item adding a factor node $g$ for each ground rule: $F=R$; %we call this factor node a \textit{ground rule} node.
\item adding an edge $(a,g)$ in $\mc{E}$ if the atom associated to $a$ occurs in the ground rule associated to $g$.
\end{itemize} }
% \fg{rimosso cosa quantificatori! ricordarsi di dire qualcosa da qualche parte}
\noindent\ar{Given an atom $a$ and a ground rule $g$, $ne(a)$ and $ne(g)$ indicate the list of neighbor nodes of $a$ and $g$, respectively. $ne(a,i)$ and $ne(g,i)$ denote the $i$-th elements of such lists, respectively.}
\ar{\begin{example}[Factor Graph]
The factor graph corresponding to the FOL theory $\mathcal{T}$ in Example \ref{ex:logic_theory} is represented in Fig. \ref{fig:graphical_model_to_transformer}-(b). 
\end{example}}

\ar{\section{The Problem}
\label{sec:problem}}
\noindent\ar{\textbf{Problem Definition. }}
\ar{\textbf{Given}:
\begin{itemize}
    \item a FOL language $\mathcal{L} = (\mathcal{C}, \mathcal{P})$ and a logic theory $\mathcal{T}$;
    \item the ground truth $y_e$ for $E \subset$ HB (i.e. the evidence)
    % (or equivalently the set of ground rules $R$);
    \item (optional) a feature representation $I$ of constants in $\mathcal{C}$;
\end{itemize}}

\ar{\textbf{Find}:
\begin{itemize}
    \item the truth values $y_Q$ for $Q \subset$ HB (i.e. the queries).
    \item[]
\end{itemize}
}

\ar{Examples of instances of the problem:}

\begin{itemize}
    \item \ar{\textit{Classification:} $\mathcal{C}$ is the set of input patterns and $I$ their feature representation, $\mathcal{P}$ is the set of  classes, $\mathcal{T}=\emptyset$. $y_E$ are the class labels for the subset of the patterns in $\mathcal{C}$ (i.e. the training set). $y_Q$ are the class predictions for another subset of the patterns in $\mathcal{C}$ (i.e. the test set).}
    \item \ar{\textit{Link Prediction:} $I=\mathcal{T}=\emptyset$. $E$ is the set of of the known links on the graph, $Q$ is the set of unknown links to predict.}
    \item \ar{\textit{Logical Reasoning:} $I=\emptyset$, $\mathcal{T}$ is the logic theory. $E$ is the set of known logic facts.  $Q$ is a subset of the conclusions of the reasoning process.}

    \item \ar{\textit{Neuro-Symbolic:} all the elements of the previous points can be integrated to add logical reasoning or link prediction on top of a pattern recognition task.}
\end{itemize}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{The Model}
\label{sec:model}
\ar{Relational Reasoning Networks (R2N) are neuro-symbolic reasoning models composed of three high-level components.}
\ar{\begin{enumerate}
\item The \textbf{Input Atom Embedding Layer} maps each ground atom $a_i\in$ HB into an embedding vector.
\item \textbf{Reasoning Layers} (recursively) update the atom embeddings according to the structure imposed by the logic-based factor graph associated to $\mathcal{T}$. %As an intermediate representation, also factor embeddings will be developed. We detail this component in Section \ref{sec:infemb}.
\item \textbf{Output Layers} use the final atom embeddings to take decisions on the query atoms in $Q$. %both at the level of the atoms and of the factors, as discussed in Section \ref{sec:output}.
\end{enumerate}
}
The overall model is shown in Fig. \ref{fig:model}.
\begin{figure*}[t]
    \centering
   \includegraphics[width=\textwidth,trim={0 180 0 0}]{architecture.png}
\caption{\ar{\textbf{Overall structure of the model}. The model can be seamlessly used to: \textit{(i)} \textit{classify} whether $a \in \mathcal{C}$ smokes by computing $out(S(a))$; \textit{(ii)} \textit{predict the link} stating whether $a,b \in \mathcal{C}$ are friends by computing $out(F(a,b))$; and \textit{(iii)} \textit{reason} about $b \in \mathcal{C}$ smoking because being a friend of the smoker $a$ by exploiting the rule $F(a,b) \wedge S(a) \rightarrow S(b)$.}
}
    \label{fig:model} 
\end{figure*}
\subsection{Input Atom Embedding Layer}
% \subsection{From Atom Scores to Atom Embeddings}
\label{sec:atemb}
\ar{The first layer of an R2N assigns a numeric representation to all the ground atoms in the HB. In particular, each ground atom is assigned an embedding vector in some \textit{latent} space whose semantics is not defined a-priori. %\footnote{Notice that we can also directly assign boolean values (i.e. $True$ or $False$) to ground atoms and thus simulate a standard logical reasoning process.}. % Io non lo direi perche' non vorrei che rivenisse fuori la cosa della semantica...
Given a ground atom $a = P(c_1,\ldots,c_n)$ and $Emb(c_i)$ the embedding of $c_i\in\mc{C}$, the input atom embedding layer computes the embedding $Emb^0(a)$ as:
\begin{equation}
    Emb^0(a) = f_P( Emb(c_1), ..., Emb(c_n))
\end{equation}} 
A vast range of representation learning approaches can be employed to implement and co-train \ar{the function $f_P$. In particular, in this paper we} consider  Knowledge Graph Embeddings and latent representations of \ar{deep neural networks.}
\paragraph{Knowledge Graph Embeddings}
\ar{KGEs (Sec. \ref{sec:kge}) encode entities and relations with real-valued embedding vectors in order to represent the relational structure among different objects in a meaningful latent space.}
All KGE methods internally perform two operations: reconstruct an atom representation from the representations of the constants and \ar{the predicate}, and compute a score from this representation via a scoring function. 
% \ar{The atom representation provided by a KGE is a natural choice as input for an R2N.}
\paragraph{Latent Representations of \ar{deep networks}}\ar{
In case a feature representation $I$ of the constants (cf. Sec. \ref{sec:problem}) is provided, such as with images, time series, videos, text, etc., we can obtain
% KGE-based
the atom embedding representations by direct extraction form hidden representations of neural architectures.}
% However, in this case the embeddings are computed as hiddedeep functions of the feature representation.

% Regardless of the specific selection of the input embedding for a single atom, all the ground atoms in the HB are embedded and passed to the reasoning layers.


% \begin{figure}[t]
%     \centering
%     \includegraphics[width=0.40\textwidth]{model_inputs1.pdf}
%     \caption{An R2N is a neuro-symbolic model, which can reason on top of different input representations: {\bf a)} the input is provided by a KGE, whose internal representation of an atom before the scoring function is used as input to the reasoning layers; {\bf b)} a classifier processes the feature-based representations of the patterns and the embeddings from the last layer of the classifier are used as initial representations for the reasoning layers.
%     Both the classifier and the KGE are co-trained with the reasoning layers.
%     %{\bf c)} the output predictions of the KGE are passed to the transformer. In this case the Boolean vales are directly manipulated by the model, which refines the predictions using the domain knowledge.
%     }
% \label{fig:model_inputs}
% \end{figure}

% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%





\subsection{Reasoning Layers}
\label{sec:infemb}

% \gm{Dire che siamo una GNN sul factor graph ma che utilizziamo un processo asincrono. Prima calcoliamo le features dei fattori dagli atomi e poi il viceversa. Inoltre, non possiamo essere considerati una semplice GNN sul factor graph dal momento che mentre il nostro schema e' tale per cui le rappresentazioni degli atomi sono invarianti a permutazioni dei nodi fattori, il viceversa non vale. In altri termini, l'aggiornamento dei nodi variabili e fattori non e' simmetrico. Il motivo e' che noi ipotizziamo che questi fattori  implementino formule logiche qualsiasi, che non sono rappresentabili in generale dai layer di message passing delle reti neurali (citare il frammento counting 2). Al contrario, questo e' quello che e' assunto da altri modelli che usano GNN su factor graph (quale FGNN).}




\ar{Reasoning layers recursively take a HB representation as input and provide an updated HB representation as output.}
\ar{Inspired by recent advances in the Graph Neural Network (GNN) community \cite{scarselli2009graph, wu2020comprehensive}, % bronst???
we implement reasoning layers as the unfolding network of a message passing process \cite{gilmer2017neural} over the logic-based factor graph. }
%\ar{A message passing neural net is characterized by two functions \cite{gilmer2017neural}: a message function, which computes messages between nodes, and an update function, which aggregates incoming messages and update the embedding of the target node. However, given the bipartite nature of our input factor graph $G$, we implement an asynchronous \cite{pearl1982reverend} message passing process. Here, atom nodes send messages to ground rule nodes, which, in turn, send messages back to atom nodes. In our asynchronous scheme, both message and update processes are specialized for atom and ground rule nodes.}
\ar{The message  from an atom node $a$ to a ground rule node $g$ at layer $\ell$ is defined as: 
\begin{equation}
    M^{\ell}_{a \rightarrow g} = Emb^{\ell-1}(a)
    \label{eq:message_a_r}
\end{equation}
where $Emb^{\ell-1}(a)$ is the embedding of the atom $a$ at layer $\ell-1$, which resolves to the output of the input atom embedding layer for the first reasoning layer, i.e. for $\ell=1$.}

\ar{The embedding of the ground rule node $g \in R_j$ for rule $r_j$ at layer $\ell$ is then computed as an MLP on the messages of its neighbor atom nodes:
\begin{equation}
    Emb^\ell(g) = MLP^1_{j}( M^\ell_{ne(g,1) \rightarrow g}, ..., M^\ell_{ne(g,d_j) \rightarrow g})
    \label{eq:update_r}
\end{equation}
where $d_j = |ne(g)|$, i.e. the number of atoms occurring in $r_j$.
Notice that we have a different function $MLP^1_j$ for each rule $r_j$. This is important so that different rules can aggregate differently the incoming messages from their neighbor atoms.} 

\ar{The message from a ground rule node $g \in R_j$ to its $i$-th neighbor atom node $a = ne(g,i)$ is defined as:
\begin{equation}
    M_{g \rightarrow a}^\ell = MLP^{2}_{j,i}(Emb^\ell(g))
    \label{eq:message_r_a}
\end{equation}
where, the message function $MLP^{2}_{j,i}$ is different for each rule $r_j$ and for each position $i$ of the recipient node of the message.}

\ar{Finally, the embedding of an atom node $a$ at layer $\ell$ is updated by aggregating the messages of all the ground rule nodes $g$, where $a$ is a neighbor of $g$:
\begin{equation}
   Emb^\ell(a) = \underset{(g,i):\ a = ne(g,i)}{\mathcal{A}} M^\ell_{g \rightarrow a}
   \label{eq:update_a}
\end{equation}
% \begin{equation}
%     Emb^\ell(a) = \underset{g:\ a \in ne(g)}{\mathcal{A}} M^\ell_{g \rightarrow a}
%     \label{eq:update_a}
% \end{equation}
where $\mathcal{A}$ is an aggregation operator like sum, mean or max. In the experiments we used a summation aggregator.}

\ar{The message passing equations~(\ref{eq:message_a_r})--(\ref{eq:update_a}) implement an update scheme for the atom embeddings, which can be repeated multiple times in a recursive fashion. Each atom embedding is initialized by the input atom embedding layer (see \ref{sec:atemb}) and then updated multiple times by the message passing scheme.}
\ar{Our message passing process is also inspired by standard logic reasoning, e.g. forward chaining, where the truth degree of the atoms is iteratively updated from the truth degree of other atoms appearing in the same rules}. 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\ar{\subsection{Output Layers and Loss Functions}
\label{sec:outlay}}
\ar{The output layers consist in an \textit{atom output layer} and (possibly) a \textit{rule output layer}.}

\ar{For each atom $a$, the atom output layer takes its last available embedding, i.e. $Emb^{L}(a)$, and returns the prediction
\[
out(a) = MLP^{A}(Emb^{L}(a))
\]
A similar output can be obtained for a ground rule embedding, i.e. $Emb^{L}(g)$, to get the ground rule prediction:
\[
out(g) = MLP^{R}(Emb^{L}(g))
\]
The overall model is then optimized by minimizing the loss:
\begin{align}
    \mathcal{L} = & \sum_{a \in E} L_{sup}(out(a), y_a)+  \\
     & + \lambda \sum_{r_j\in\mc{T}} \sum_{g \in R_j^E} L_{sem}(out(g), r_j({\bf y}_{ne(g)}))
     \label{eq:losssem}
\end{align}
%%%old
% \begin{align}
%     \mathcal{L} = & \sum_{a \in E} L_{sup}(out(a), y_a)+  \\
%      & + \lambda \sum_{r_j} \sum_{g \in R_j: \forall a \in ne(g) a \in E} L_{sem}(out(g), r_j({\bf y}_{ne(g)}))
%      \label{eq:losssem}
% \end{align}
where $\lambda \ge 0$ is a meta-parameter, $L_{sup}$ and $L_{sem}$ are standard supervised classification losses, which force the outputs of the model on the evidence $E$. In particular, $L_{sup}$ enforces each $out(a)$ with $a \in E$ to match the available ground truth $y_a$. For instance for KGE tasks, where only positive examples are provided, the \emph{negative softmax-loss} is used with negative examples automatically obtained through corruptions of the positive ones. On the other hand, $L_{sem}$ enforces each $out(g)$ with $g\in R_j^E=\{g: a \in E,\ \forall a \in ne(g)\}$, i.e. for the ground rules whose all neighbor atoms $ne(g)$ are in the evidence $E$, to match the evaluation of the rule $r_j$ on their overall ground truth ${\bf y}_{ne(g)}$. This can be done by simply evaluating the logic formula $r_j({\bf y}_{ne(g)}) = r_j(y_{ne(g,1)}, \ldots, y_{ne(g,d_j)})$.}
%MICHI: PER ME STO DISCORSO E" APPROPRIATO MA NON E' MEGLIO METTERLO IN RELATED WORKS? MI SEMBRA UN PO UNA SCUSA PREVENTIVA MESSO QUI

\ar{\subsection{Discussion}\label{sec:discussion}}
%{\bf Position-aware message passing. } R2N improves over other message passing GNNs working on factor graphs, like FGNN ~\cite{zhen2020nips}. In particular, R2Ns are aware of the order of the in-going and outgoing edges in the factor graph. In particular,  $MLP^1_j$ concatenates its in-going messages following the order in which atoms appear in a rule, and one different $MLP^2_{j,i}$ is specialized to correctly predict the embedding for the $i$-th element in the list of the outgoing neighbors.\\
\ar{{\bf Complexity Analysis. } An R2N implements a message passing scheme over the factor graph obtained by grounding the logical theory. Such message passing is linear in the size of the factor graph and quadratic on the embedding size: $O(|R| \cdot d_{max} \cdot |Emb|^2)$, where $|Emb|$ is the atom embedding size, $|R|$ is the total number of ground rule nodes and $d_{max} = \max_{g \in R} |ne(g)|$ is the maximum number of atoms in the rules.
If we can consider $d_{max} \cdot |Emb|^2$ a fixed constant, the computational cost is mainly determined by the $|R|$ value. As explained in Sec. \ref{sec:fol}, the number of ground rules is a polynomial depending on the number of constants in each rule $|R| = \sum_{r_j} |\mathcal{C}|^{|X_j|}$, where $|X_j|$ is the number of variables in $r_j$.
The resulting complexity is in between KGE inference, which has a complexity quadratic in the number of constants (and linear or quadratic in the embedding size depending on the selected KGE), and other statistical neuro-symbolic methods, which are exponential in the number of constants. However, one could rely on the sparsity of the input graph to selectively ground the logical theory around the query atoms and largely improve the scalability. We leave this study to future work and we focus on the full exact grounding in this paper.
\\
{\bf The role of Logic Theories. } The employed logic theory $\mc{T}=\{r_1(X_1),\ldots,r_m(X_m)\}$ %(see Section \ref{sec:fol})
has the fundamental role of determining the structure of an R2N.
An R2N fully grounds the rules $r_i$ on $\mc{C}$, without assuming that a rule holds true for a grounding, as the satisfaction or the dissatisfaction of a rule on a grounding is determined according to the available evidence. Therefore, the model learns the contexts where a rule is satisfied, driven by the ground truth. A rule that is always satisfied by the training data it will be enforced like if it was universally quantified. On the other hand, a partially violated rule will be enforced only in the contexts where it is expected to hold true. % and, finally, it could learn to apply a rule as if it was existential quantified if needed. % Non so se dirlo e' necessario? Ci e' stato chiesto, senno eviterei visto che la quantificazione esistenziale e' un po tirata per i capelli in questo contesto.
This property makes R2Ns very versatile and suitable to deal with either hard or soft rules whose percentage of fulfillment is not known in advance.
\\
{\bf Implicit Logic Rules. } Let us suppose that we know a group of atoms to be semantically connected, but without having explicitly available the logic expression of the rule correlating them, that we refer as an \textit{implicit} rule. 
For instance, let $X_i$ be the set of variables for an implicit rule $r_i(P_1(\bar{x}_1),\ldots,P_l(\bar{x}_l))$, where each $\bar{x}_k\subseteq X_i$ denotes the set of variables $P_k$ depends on, for all $k\in\{1,\ldots,l\}$.
% For each possible ground assignment to the variables, it is generated a group of ground atoms that are known to be dependent, even if the logic semantics are unknown. 
The grounding process and the construction of the corresponding logic-based factor graph for $r_i$ (as described in Sec. \ref{sec:fg}) can take place like with fully specified FOL formulas. However, since the semantics of the rule is not specified (the logical connectives are unknown), it is not possible to determine the truth-value of the evidence groundings of $r_i$, hence the loss components given by equation~(\ref{eq:losssem}) can not be considered, e.g. $\lambda=0$. In the experimental results, we show a use case of implicit logic rules used by R2N in Sec. \ref{sec:kgc}. Even if beyond the scope of this paper, we notice that it is possible to use explainability methods to explicitly extract the logic rules a posteriori~\cite{ciravegna2020human}.}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{R2N as a Probabilistic Graphical Model}
\label{sec:bp}
One natural question that arises is which kind of reasoning can be approximated by an R2N.
As shown in Fig.~\ref{fig:graphical_model_to_transformer}-(b), the underlying structure of an R2N can be regarded as a factor graph. Let we call ${\bf x}$ a possible truth assignment to all the ground atoms in HB. A factor graph corresponds to the factorized probability distribution:
\[
p({\bf x}) = \frac{1}{Z} \exp\left( \sum_{g \in R} \phi_g({\bf x}_{ne(g)}) \right)
\]
where $\phi_g({\bf x}_{ne(g)})$ is a potential function corresponding to one ground rule (factor) node correlating the neigboring atom variables ${\bf x}_{ne(g)} \subset {\bf x}$.
This section shows that the Belief Propagation (BP) algorithm, commonly applied to factor graphs, can be approximated by a forward pass within the R2N model.
\\
{\bf Belief Propagation. }
The (loopy) Belief Propagation algorithm defines an efficient inference schema for Probabilistic Graphical Models, like Markov Random Fields, that is based on iterative message passing. The algorithm computes an approximation of the marginal distribution for each unobserved variable, given the observed ones.
Let us consider a logic based factor graph as in Sec. \ref{sec:fg}.
The max-product belief propagation algorithm can be defined as follows: 
\[
b(x_i) = \displaystyle\prod_{g:\ i \in ne(g)} m_{g\rightarrow i}(x_i) \ ,
\]
where $x_i$ is a discrete random variable with $i\in$ HB an atom node, whose belief $b(x_i)$ is computed as an aggregation over each neighbor factor node $g$.
% the set of its nodes $S(c)$ contains $x_i$. 
The message from a factor $g$ to the atom node $i$ ($m_{g\rightarrow i}$) defines the effect of that factor on the belief for that atom:
\[
m_{g\rightarrow i}(x_i) = \max_{\mathbf{x}_g:\  \mathbf{x}_{i}=x_i} \phi_g(\mathbf{x}_g) \displaystyle\prod_{j \in ne(g):\ i \ne j} b({\bf x}_j) 
\]
where $\phi_g(\mathbf{x}_g)$ is the potential associated to the factor $g$ when instantiated with the variable values $\mathbf{x}_g$ and the max is performed over all possible assignments to the variables in the factor, for a fixed $x_i$.

Lets now consider binary potentials and variables, which are the ones used in networks representing classic logic knowledge, i.e. $\phi_g({\bf x}_g) \in\{0,1\}$.
% and for which it holds that $p(x_i=1) + p(x_i=0) = 1$.
Hence, we have:
\begin{align}
b(x_i) &= \displaystyle\prod_{g:\ i \in ne(g)} \max_{\mathbf{x}_g:\ \mathbf{x}_i=x_i} \left[ \phi_g(\mathbf{x}_g) \cdot \displaystyle\prod_{j \in ne(g):\ i \ne j} b({\bf x}_j) \right] = \nonumber \\
&= \displaystyle\prod_{g:\ i \in ne(g)} \max_{\mathbf{x}_g \in \mathcal{T}_g:\  \mathbf{x}_i=x_i} \left[ \displaystyle\prod_{j \in ne(g):\ i \ne j} b({\bf x}_j) \right] \label{eq:belief_as_input_function}
\end{align}
where $\mathcal{T}_g$ is the set of  assignments $\mathbf{x}_g$ such that $\phi_g(\mathbf{x}_g)=1$, i.e. that satisfy the formula associated to factor $g$.
The same equation can be expressed in terms of log-beliefs, which highlights the connections with neural-based implementations:
\begin{equation}
\log b(x_i) \!=\!\!\!\!\sum_{g:\ i \in ne(g)} \max_{\mathbf{x}_g\in \mathcal{T}_g:\ \mathbf{x}_i=x_i} \left[ \sum_{j \in ne(g):\ i \ne j} \!\!\!\! \log b({\bf x}_j) \right] \label{eq:log_belief_as_input_function}
\end{equation}

Some examples of the computation of believes are shown in the appendix.

\noindent {\bf BP and R2N. } The BP computation can be factorized in terms of the R2N modules as follows:
\begin{align*}
\log b(x_i) &=  \sum_{g:\ i \in ne(g)} \max_{\mathbf{x}_g\in \mathcal{T}_g:\  {\bf x}_i=x_i} \left[ \displaystyle\sum_{j \in ne(g):\ i \ne j} \log b({\bf x}_j) \right] = \\
%&=& \displaystyle\sum_{c: i \in ne(g)} 
%\max_{\mathbf{x}_g: x_{i}=x_i} \left[
%MLP^1_\varphi(g)\left(x_1,\ldots,x_{|ne(g)|}\right) \right] = \\
%&=& \sum_{c: i \in ne(g)}MLP^2_{ci}\left[
%MLP^1_\varphi(g)\left(x_1,\ldots,x_{|ne(g)|}\right)\right] = \\
&=
\underbrace{\sum_{g:\ i \in ne(g)}\underbrace{MLP^2_{\varphi(g),i}
\Big(\underbrace{MLP^1_{\varphi(g)} \left(log\_b({\bf x}_{g}))\right)}_{\mbox{nodes to factor eqs. (\ref{eq:message_a_r})-(\ref{eq:update_r})}}\Big)}_{\mbox{factor to node equation (\ref{eq:message_r_a})}}}_{\mbox{aggregation equation~(\ref{eq:update_a})}}
\label{eq:fgnn_decomposition2}
\end{align*}
where $\varphi(g)$ is the index of the formula for which $g$ has been instantiated, i.e. $g$ is a grounding of $r_{\varphi(g)} \in \mathcal{T}$ and $log\_b({\bf x}_{g}) = [\log b({\bf x}_1), \ldots, \log b({\bf x}_{|ne(g)|}) ]$ indicates the log beliefs of neighbors of $g$ factor. In the context of an R2N, each variable $x_i$ corresponds to one ground atom e.g. $F(a,b)$. This iterative inference schema is initialized within an R2N with the beliefs computed by the input layer, which provides a solid initial guess of the values.

The following theorem shows how this factorization allows us to approximate BP with arbitrary precision, provided sufficient computational power.
\begin{theorem}
	\label{the:r2n_approx_bp}
	An R2N reasoning block can exactly compute one iteration of the max-product belief propagation algorithm as a forward step within the architecture.
\end{theorem}
\begin{proof} The proof is in the appendix.
\end{proof}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\noindent
{\bf Discussion. } When belief propagation can find an optimal solution, an R2N can perfectly reconstruct the same optimal output. 
Please note that loopy BP is often initialized with random or constant initial beliefs and it is known to not always converge to a satisfactory solution. However, thanks to the neuro-symbolic integration of R2Ns, the atom beliefs are instantiated by the input layer, like a KGE, which can often provide an accurate initialization of the beliefs, which are later corrected and improved by the reasoning process.
For example, as described earlier in this paper, the output of a KGE is a scoring function $f_{KGE}(x_a, \theta)$, where the parameters $\theta$ and a hidden latent representation $x_a$ of the atom are used to expresses the confidence that an atom is true.
The quantities $\sigma (f_{KGE}(x_a, \theta))$ and $1-\sigma (f_{KGE}(x_a, \theta))$ can be interpreted as the initial beliefs for atom $a$, as shown 
by Nickel at al.~\cite{nickel2015review}, where the KGE assignments are used to define a Bernoulli probability distribution. In particular, the probability of an assignment ${\bf y}$ with $y_a$ indicating the assignment to atom $a$ can be computed as:
\begin{align*}
p({\bf y}| {\bf x}, \theta) &= \prod_a p(y_a|x_a, \theta) =  & \\
&= \prod_a \left\{ \begin{array}{ll}
\sigma (f_{KGE}(x_a, \theta))         & if~y_a=1\\
1 - \sigma (f_{KGE}(x_a, \theta)) & if~y_a=0
\end{array} \right.  \\
%&=& \prod_a \left[ y_a \cdot \sigma (f_{KGE}(x_a, \theta)) + (1-y_a) \cdot (1-\sigma (f_{KGE}(x_a, \theta))) \right] = \\
&= \prod_a \left[ 1 - y_a + \sigma (f_{KGE}(x_a, \theta)) (2y_a - 1) \right]
\end{align*}
In practice, an R2N usually employs a larger atom embedding space than the one required to  represent only the beliefs of a binary variable. As a consequence, the model can perform complex atom transformations and can learn how each formula should be accounted depending on the input constants and predicates. In this scenario, the internal representation $x_a$ of the atom of a KGE can be directly passed to the reasoning steps, without the information bottleneck of compressing it to scalar beliefs.


\section{Experiments}
\label{sec:exp_results}
The experiments explore the performances of the model in different contexts\footnote{Code/data will be released upon publication.}: symbolic reasoning with explicit logic-knowledge (Sec. \ref{sec:countries}), symbolic reasoning with \ar{implicit logic rules} (Sec. \ref{sec:kgc}) and, finally, symbolic and sub-symbolic integration (Sec. \ref{sec:cora}).
Table~\ref{tab:symbolic_datasets} reports the basic statistics about the datasets. Additional details on the experimental settings can be found in the appendix.

% In Section \ref{sec:countries}, the methodology is tested in a symbolic reasoning task with explicit logic knowledge. In Section \ref{sec:kgc}, the model is applied on top of KGEs to perform relational reasoning tasks, when the relational knowledge is implicit and the model can exploit the higher-order correlations among groups of atoms.
% Finally, in Section \ref{sec:cora} the model is used to perform symbolic/sub-symbolic integration, by learning to take collective decisions by reasoning over the output of a set of co-trained neural classifiers.
%We ensured that all the baselines use the same splits and evaluation routines, which are the ones in Minervini et al.~\cite{minervini2020learning}, for Sections \ref{sec:countries} and \ref{sec:kgc}, and the ones in Zhang et al.~\cite{zhang2020efficient} for \ref{sec:cora}.



\begin{table}[t]
\centering
\ar{
{\small
\begin{tabular}{l|cccc}
{\bf Dataset} & \#{\bf Entities} & \#{\bf Relations} & \#{\bf Facts} & \#{\bf Degree}\\
\hline
Countries & 272 & 2 & 1158 & 4.35\\
Nations & 14 & 55 & 1992 & 142.3\\
Kinship & 104 & 25 & 8544 & 85.15 \\
UMLS & 135 & 46 & 5216 & 38.63 \\
Cora & 700 & 9 & 18561 & 26.51
\end{tabular}
}}
\caption{\ar{Basic statistics of datasets. For Cora, we report the average over the five splits.}}
\label{tab:symbolic_datasets}
\end{table}


\ar{\subsection{Competitors}
The proposed model is compared against state-of-the-art knowledge graph embeddings or neuro-symbolic approaches. The following baselines have been used in the experiments:
\begin{itemize}
    \item KGEs like ComplEx \cite{trouillon2016complex} and DistMult \cite{yang2015embedding}, and query oriented versions, like MINERVA \cite{das2018gofor};
    \item Neural Theorem Provers (NTP), with their greedy attention (GNTP-Attn, \cite{minervini2018towards}) and conditional (CTP, \cite{minervini2020learning}) versions;
    \item a differentiable rule learning system based on stochastic logic programs, i.e. NeuralLP \cite{yang2017differentiable};
    \item GNN methods on factor graphs, like FGNN \cite{zhen2020nips};
    \item ExpressGNN, a GNN-based variational approach to inference in MLNs~\cite{zhang2020efficient}.
\end{itemize}
In the following, to distinguish if an R2N uses explicit logic rules ($\lambda>0$) with also the semantic loss provided by equation~(\ref{eq:losssem}) or just implicit logic rules ($\lambda=0$), we will use the notation R2NS and R2NC, respectively.}
% In the following, we will refer to an R2N using only implicit logic rules as R2NC ($\lambda=0$), and with an explicit logic theory and, therefore, a semantic loss ($\lambda>0$) provided by equation~(\ref{eq:losssem}) as R2NS.
% When the R2N model is trained with both the supervised loss and the regularization term with $\lambda>0$, we refer it as R2NS.}

\begin{table*}[th]
\centering
\caption{AUC-PR metric and average train/inference times on the $3$ tasks of the Countries dataset using different KGE and neural reasoning systems, R2NS indicates the relational reasoner network proposed in this paper. A bold font indicates the best method for each task.}
{\footnotesize
\begin{tabular}{l|ccccccccc}
Task & %TransE &
ComplEx & %ConvE &
DistMult & NTP-$\lambda$ & GNTP-Attn & CTP & NeuralLP & Minerva & FGNN & R2NS\\
\hline
S1 & %0.870$\pm 0.03$ &
$0.993\pm 0.00$ & %{\bf 1.000}&
0.922$\pm 0.05$ & {\bf 1.000}$\pm 0.00$ & {\bf 1.000}$\pm 0.00$ & {\bf 1.000}$\pm 0.00$ & {\bf 1.000}$\pm 0.00$& {\bf 1.000}$\pm 0.00$&  0.935$\pm 0.04$ & {\bf 1.000}$\pm 0.00$\\
S2 & %0.527$\pm 0.07$ &
0.880$\pm0.03$ & %0.990 &
0.571$\pm 0.09$ & 0.930$\pm 0.00$ & 0.930$\pm 0.03$ & 0.918$\pm 0.01$ & 0.751$\pm 0.00$ & 0.924$\pm 0.02$ & 0.823$\pm 0.09$ & {\bf 0.992}$\pm 0.00$\\
S3 & % 0.509$\pm 0.08$ &
0.484$\pm 0.06$ & %0.860 &
0.554$\pm 0.05$ & 0.773$\pm 0.17$ & 0.913$\pm 0.04$ & 0.948$\pm 0.00$ & 0.922$\pm 0.00$ & {\bf 0.951}$\pm 0.01$ & 0.560$\pm 0.06$ & {\bf 0.951}$\pm 0.03$\\
Time & %23s / 0.1s &
67s/0.4s & 31s/0.2s & - & - & - & - & - & 71s/0.2s & 125s/1.2s
\end{tabular}
}
\label{tab:countries_dataset}
\end{table*}


\begin{table}[th]
\centering
\caption{\ar{Ablation study for R2NS in the $3$ tasks of the Countries dataset, varying the number of reasoning blocks and using ComplEx as input layer. The results report the AUC-PR metric computed as average over 10 runs.}}
{\footnotesize
\ar{\begin{tabular}{l|cccc}
\multicolumn{5}{c}{\hspace{1cm}Number Reasoning Blocks}\\
Task & 0 & 1 & 2 & 3 \\
\hline
S1 & 0.922$\pm 0.05$ & 1.000$\pm 0.00$ & 1.000$\pm 0.00$ & 1.000$\pm 0.00$\\
S2 & 0.880$\pm 0.03$ & 0.975$\pm 0.01$ & 0.987$\pm 0.01$ & 0.992$\pm 0.00$\\
S3 & 0.484$\pm 0.06$ & 0.739$\pm 0.06$ & 0.848$\pm 0.15$ & 0.951$\pm 0.03$\\
\end{tabular}}
}
\label{tab:countries_dataset_ablation}
\end{table}



\begin{table*}[th]
\caption{Results and training/inference times for KGE datasets. Missing values indicate values not reported in the original papers, for which it was not possible to re-run the experiments. The best model for each metric is shown in bold.}
\centering
{\small
\begin{tabular}{l|l|cccccccc}
Dataset & Metric & ComplEx & DistMult & NTP & GNTP & CTP &  NeuralLP & Minerva & R2NC\\
\hline
Nations & Hits@1 & 0.627 & 0.617 & 0.45 & 0.493 & 0.562 & - &  - & {\bf 0.793}\\
        & Hits@3 & 0.858 & 0.868 & 0.73 & 0.781 & 0.813 & - & - & {\bf 0.930}\\
        & Hits@10 & 0.998 & {\bf 1.000} & 0.87 & 0.985 & 0.995 & - & - & {\bf 1.000}\\
        & MRR     & 0.749 & 0.754 & 0.61 & 0.658 & 0.709 & - & - & {\bf 0.862}\\
        & Time & 63s/0.02s & 53s/0.02s  & - & - & - & - & - & 170s/0.03s \\
\hline
Kinship & Hits@1 & 0.623 & 0.352 & 0.24 & 0.586 & 0.646 & 0.475 & 0.605 & {\bf 0.814}\\
        & Hits@3 & 0.843 & 0.574 & 0.37 & 0.815 & 0.859 & 0.707 & 0.812 & {\bf 0.942}\\
        & Hits@10 & 0.965 & 0.967 & 0.57 & 0.959 & 0.958 & 0.912 & 0.924 & {\bf 0.978}\\
        & MRR & 0.745 & 0.508 & 0.35 & 0.658 & 0.709 & 0.619 & 0.720 & {\bf 0.881}\\
        & Time & 827s/0.02s & 507s/0.02s  & - & - & - & - & - & 2442s/0.04s \\
\hline
UMLS    & Hits@1 & 0.877 & 0.341 & 0.70 & 0.761  &  0.752& 0.643 & 0.728 & \textbf{0.924}\\
        & Hits@3 & \textbf{0.987} & 0.542 & 0.88 & 0.947  &  0.947 & 0.869 & 0.900 & \textbf{0.987}\\
        & Hits@10 & \textbf{0.998} & 0.736 & 0.95 & 0.983  & 0.984 & 0.962 & 0.968 & 0.996\\
        & MRR & 0.923 & 0.479  & 0.80 & 0.857  & 0.852 & 0.778 & 0.825 & \textbf{0.952} \\
        & Time & 109s/0.02s & 125s/0.03s  & - & - & - & - & - & 877s/0.12s \\
% with ntn-$\lambda$
%Kinship & HITS@1 & 0.754 & 0.697 & 0.808 & 0.500 & {\bf 0.759} & 0.475 & 0.605 & 0.755\\
%        & HITS@3 & 0.910 & 0.886 & 0.942 & 0.700 & 0.798 & 0.707 & 0.812 & {\bf 0.925}\\
%        & HITS@10 & 0.980 & 0.974 & 0.979 & 0.777 & 0.878 & 0.912 & 0.924 & {\bf 0.985}\\
%        & MRR & 0.838 & 0.797 & 0.878 & 0.612 & 0.793 & 0.619 & 0.720 & {\bf 0.843}\\
%\hline
%UMLS    & HITS@1 & 0.823 & 0.894 & 0.916 & 0.817 & 0.843 & 0.643 & 0.728 & 0.831\\
%        & HITS@3 & 0.962 & 0.964 & 0.967 & 0.906 & 0.983 & 0.869 & 0.900 & ?\\
%        & HITS@10 & 0.995 & 0.992 & 0.992 & 0.970 & 1.000 & 0.962 & 0.968 & ?\\
%        & MRR & 0.894 & 0.933 & 0.944 & 0.872 & 0.912 & 0.778 & 0.825 & ?
\end{tabular}
}
\label{tab:nations_kinship_umls_dataset}
\end{table*}

\subsection{Countries Dataset}
\label{sec:countries}
The Countries dataset (ODbL licence)~\cite{bouchard2015approximate}
defines a set of countries, regions and sub-regions as basic entities. We used splits and setup from \cite{rocktaschel2017end}, which reports the basic statistics of the dataset and defines $3$ tasks named $S1,S2,S3$, each requiring reasoning chains of increasing length. 
The task consists in predicting the unknown facts $LocIn(country, continent)$, stating country location within a continent, given the evidence in form of country neighbourhoods and some known country/region locations.
The model is provided with knowledge about the task like the hierarchical nature of the $LocIn$ (located in) relation,
\begin{equation}
\forall c \forall r \forall k\  LocIn(c,r) \land LocIn(r,k) \rightarrow LocIn(c,k)
\label{eq:country1}
\end{equation}
and the manifold created across neighbouring countries:
\begin{equation}
\forall c \forall c_1 \!\forall k NeighOf(c, c_1) \!\land\! LocIn(c,k) \!\rightarrow\! LocIn(c_1,k) 
\label{eq:country2}
\end{equation}
    
\ar{The task S1 can be solved exactly by applying the first rule. In task \textit{S2}, some facts supporting the first rule are removed, and one has to rely on a soft application of the second rule. In task \textit{S3}, facts supporting directly the second rule are removed, making multiple recursive applications of the second rule necessary.} 
% The first rule is always correct in the data, while the second is only partially correct and the probabilistic reasoning has to determine how and where to employ it.
Table~\ref{tab:countries_dataset} reports the area under the precision-recall curve (AUC-PR) metric and one standard error for the different reasoning tasks as an average over $5$ different runs.
In order to ensure that the best possible results are obtained for the baseline methods, the values are taken from the original papers whenever available, for example theorem provers results have been extracted from Minervini et al.~\cite{minervini2020learning}. \ar{R2NS shows better performance both w.r.t. plain KGEs and state-of-the-art neural theorem provers. The advantage w.r.t. KGE approaches is due to their lack of multi-hop reasoning. In fact, multi-hop reasoning is more important in S2 and S3 tasks where KGE methods clearly underperform. The advantage w.r.t. NTP is due to the fact that the grounding process in R2N is symbolic. In fact, we instantiate the same ground rules that a standard forward chaining solver will instantiate. On the contrary, NTP uses soft-unification, which introduces more ground rules than those required by the symbolic solver (i.e. backward chaining in their case). Such ground rules are a source of noise, which becomes worse when multiple hops of reasoning are required. This is also signalled by the fact that extensions with sparser grounding techniques (e.g. CTP) tend to perform better when longer reasoning paths are required}.

\ar{
{\bf Ablation Study. } This experiment studies how the results on the Countries dataset are affected by the number of stacked reasoners. Table~\ref{tab:countries_dataset_ablation} reports the detailed results as average over $10$ different runs. Since the S1 task is constructed to be solved by the simple application of equation~(\ref{eq:country1}), one reasoner block is enough to exactly solve the task. Moving to the more difficult S2 task, making more complex inference paths help the generalization. Finally, in the harder S3 task, which can be solved only via a longer reasoning chaining formed by the recursive application of equation~(\ref{eq:country2}), the results are significantly improved by adding more reasoning layers. Even if not reported in the table, we observed a saturation effect when moving beyond $3$ reasoning layers on this task.
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{KGE datasets: Nations, Kinship, UMLS}
\label{sec:kgc}
The Nations, Kinship, and UMLS datasets~\cite{kok2007statistical} (CC0 licence) are popular datasets for relational reasoning, where a set of triples (entity, relation, entity) expresses known true facts and the goal is to infer the unknown true facts. We used the setup and splits defined by~\cite{minervini2020learning}, which also reports the basics statistics for the datasets.
In particular, the Kinship dataset is based on the kinship relationships recorded among the members of te Central Australia Alyawarra tribe, and the Unified Medical Language System (UMLS) dataset defines a set of entities representing biomedical concepts and relations.
These tasks can be addressed using KGE approaches, which learn the atom representations based on the entities and relation correlations over the true facts.
The main limitation of these approaches is that they fail to represent higher-order correlations among the predicates like taxonomic relations or among the constants like in transitive relations.
No logic knowledge is explicitly provided for these datasets, therefore an R2NC model is instructed to correlate all predicates over each pair of constants: \ar{$r_1(P_1(x,y), P_2(x,y), \ldots, P_n(x,y))$}.
The model exploits the correlations to correct and improve the KGE predictions via an implicit and latent reasoning process.


Table~\ref{tab:nations_kinship_umls_dataset} reports training/inference times and the standard metrics for these tasks, where the proposed methodology outperforms KGEs and state-of-the-art reasoning systems on all datasets.
The model co-trains a KGE layer, therefore adding some time overhead. In spite of the extra reasoning, running times were in the same order of magnitude for all experiments.
In order to ensure that the best possible results are obtained for the baseline methods, the values are taken from the original papers whenever available, for example NTP/GNTP results have been extracted from Minervini et al.~\cite{minervini2020learning}. \ar{R2N outperforms the competitors, due to its ability to perform a more complex higher level reasoning. In particular, when explicit symbolic knowledge is not available, like in these Knowledge Graph Completion (KGC) tasks, the correlations exploited by R2NC should not be restricted to definite clauses templates exploited in backward chaining theorem provers.}

\begin{table}[th]
\centering
\caption{AUC-PR on test for the $5$ folds and global average, training/inference mean running time for the Cora dataset for different tested models.
A dash indicates a missing result not reported in the original paper.
Statistically significant ($95\%$) best results reported in bold.}
%{\small
\begin{tabular}{c|cccc}
{\bfseries Split} & ExpressGNN \!\!\!\!\!\!\!\!\!\!& MLP & R2NS & R2NC \\
\hline
S1  & 0.62 \!\!\!\!\!\!\!\!\!\! & $0.830\pm0.007$ & {\bf 0.898}$\pm0.021$ & $0.823\pm0.015$\\
S2  & 0.79 \!\!\!\!\!\!\!\!\!\!& $0.781\pm0.007$ & {\bf 0.920}$\pm0.007$ 
& $0.819\pm0.023$\\
S3  & 0.46 \!\!\!\!\!\!\!\!\!\!& $0.843\pm0.009$ & {\bf 0.957}$\pm0.003$ & $0.888\pm0.015$\\
S4  & 0.57 \!\!\!\!\!\!\!\!\!\!& $0.774\pm0.011$ & {\bf 0.923}$\pm0.011$ & $0.837\pm0.039$\\
S5  & 0.75 \!\!\!\!\!\!\!\!\!\!& $0.838\pm0.006$ & {\bf 0.913}$\pm0.010$ & $0.800\pm0.019$\\
Avg & 0.64 \!\!\!\!\!\!\!\!\!\!& $0.812\pm0.007$ & {\bf 0.922}$\pm0.012$ & $0.833\pm0.024$\\
Time & - & $120.2s/0.8s$  & $1142s/3.6s$ & $809.8s/3.6s$
\end{tabular}
% 70 neurons
%0.904/0.009,0.877/0.015,0.942/0.004,0.924/0.008,0.908/0.012,GLOBAL:0.911/0.010
%}
\label{tab:cora}
\end{table}

\begin{table}[t]
\centering
\caption{Logic knowledge used by the presented models for the Cora dataset. $SamePaper(x,y)$ is defined on the union of \emph{Author, Title} and \emph{Venue} domains and expresses that two entities $x,y$ are related to a common paper in the known facts.}
{\tiny
\begin{tabular}{l}
\small $SamePaper(x,y) \rightarrow SameAuthor(x,y)$ \\
\small $SamePaper(x,y) \rightarrow SameTitle(x,y)$ \\
\small $SamePaper(x,y) \rightarrow SameVenue(x,y)$ \\
\small $SameVenue(x,y) \rightarrow SameVenue(y,x)$ \\
\small $SameAuthor(x,y) \rightarrow SameAuthor(y,x)$ \\
\small $SameTitle(x,y) \rightarrow SameTitle(y,x)$\\
\small $SamePaper(x,y) \!\land\! SamePaper(y,z) \!\rightarrow\! SameAuthor(x,z)$ \\
\small $SamePaper(x,y) \!\land\! SamePaper(y,z) \!\rightarrow\! SameTitle(x,z)$ \\
\small $SamePaper(x,y) \!\land\! SamePaper(y,z) \!\rightarrow\! SameVenue(x,z)$ \\      
\small $SameAuthor(x,y) \!\land\! SameAuthor(y,z) \!\!\rightarrow\!\! SameAuthor(x,z)$\\
\small $SameTitle(x,y) \!\land\! SameTitle(y,z) \!\rightarrow\! SameTitle(x,z)$\\
\small $SameVenue(x,y) \!\land\! SameVenue(y,z) \!\rightarrow\! SameVenue(x,z)$
\end{tabular}
}
\label{tab:cora_rules}
\end{table} 

\subsection{Symbolic and Sub-Symbolic Integration: Cora}
\label{sec:cora}
The Cora dataset \cite{singla2005discriminative} (CC0 licence) defines a deduplication task which aims at reconciling small differences in paper citations. Each paper is associated to its author, title and venue attributes.
We used the setup defined by~\cite{zhang2020efficient}, which
measures the performance of the model in terms of the prediction accuracy of the predicates $SameAuthor(author, author)$, $SameTitle(title, title)$, $SameVenue(venue, venue)$, detecting whether two entries refer to the same author, title and venue, respectively.
Neuro-symbolic methods, which focus only on the logical representation like ExpressGNN, map the textual information contained in titles/authors/venues to one binary feature modeling the presence of each term in the corresponding text. A main limitation of this class of models is that they can not easily capture complex term correlations, required to construct powerful text similarity distances.
Pure sub-symbolic models, like neural networks, can learn arbitrary complex functions to approximate the $SameAuthor,SameTitle,SameVenue$ predicates. However, they can not easily model the relational dependencies among tasks and/or groundings.

The proposed model reasons over the latent representations generated by the underlying classifiers processing the textual information. Hence, they can simultaneously optimize the classifiers learning the text similarity predicates and the inference process defined by the available logic knowledge.
Table~\ref{tab:cora_rules} shows the logic knowledge used for this task, which is equivalent to the one used by ExpressGNN.


Table~\ref{tab:cora} reports the average AUC-PR scores and $95\%$ confidence error over $10$ different runs for the five folds. The $Avg$ split is the average of all the runs on all the folds.
R2NS outperforms all the other baselines. ExpressGNN is separated by a large gap, which provides further evidence on the importance of exploiting low-level embedding functions to correlate the terms in each title, venue or author name. This is evident by also looking at the good performances of a simple MLP with no relational information. R2NS outperforms also the R2NC model, which however performs better than the MLP classifier.  This is a confirmation of what previously discussed on the advantages of using more specific knowledge, when available.
%R2NS defines factors by using the available knowledge shaped as logical formulas. However, it does not provide any information about the semantics of the formula. An interesting question is whether the model is able to learn the semantics of the rule. For this reason, we evaluated the logical value of each factor on the atoms prediction at the end of the learning. We computed the same logical values on the atoms ground truth.
The accuracy of the factor predictions is $0.822 \pm 0.031$ ($95\%$ confidence error) for factors of grounded formulas with at least one test atom, which means that the predictions of the network are consistent with the semantics of the provided formulas.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Related Work}
\label{sec:Relwork}

\ar{Relational Reasoning Networks bridge ideas from different research areas, like logic, neural networks, probability, and, as a result, they present several connections with different AI models. In the following, the links between R2Ns and prominent work in different areas are highlighted.}

\ar{{\bf R2Ns and (Probabilistic) Reasoning. } The capability of make inference is a fundamental property of AI systems whose behaviour could be considered intelligent, and possible ways on how to automatize the process of reasoning have received a lot of attention over the years \cite{robinson2001handbook}. In forward chaining (or forward reasoning) \cite{russell2010artificial} novel true facts are produced as a result of the application of one or more inference steps, starting with a set of known true facts. In the same spirit, an R2N performs one or more sub-symbolic reasoning steps by manipulating a set of input fact embeddings into a set of refined new ones, accounting for the relational structure of the task.}
%prob
Probabilistic reasoners define a more flexible inference process than their counterparts based on standard logic. For example, Statistical Relational Learning approaches like Markov Logic Networks (MLN)~\cite{richardson2006markov} translate a  First-Order Logic (FOL) logic theory into an undirected graphical model.
Probabilistic Soft Logic~\cite{bach2017hinge} tried to overtake the scalability limitations of MLNs by defining a tractable fraction of FOL and relaxing inference using fuzzy logic. \ar{R2Ns exploit the same graph construction of MLNs but inference is performed faster as a forward step of a neural architecture.}

\ar{{\bf R2Ns and Neuro-Symbolic AI. }} Neuro-symbolic methods~\cite{de2020statistical} bridge the reasoning capabilities of logic reasoners with the ability of sub-symbolic systems to deal with the feature-based representations that typically represent sensorial inputs like video, images or sounds.
In particular, neuro-symbolic distillation methods like Semantic-based Regularization~\cite{diligenti2017semantic}, teacher-student setups~\cite{hu2016harnessing} and Logic Tensor Networks~\cite{donadello2017logic} inject logic knowledge into the network weights by enforcing the knowledge on the predictor outputs.
In spite of their simplicity, these methods are powerful when the logic knowledge should be applied to the entire output space in a flat fashion, unlike what is needed to obtain a full probabilistic reasoning process.
R2Ns define a more flexible employment of the logic knowledge, since reasoning over latent representation learns how and where to apply the available knowledge.
Neural Logic Machines (NLM)~\cite{dong2018neural} exploit neural networks and logic programming to carry out both inductive learning and logic reasoning tasks. However, NLM could not be applied to Knowledge Graph Embeddings tasks, where the relational information is not fully known. Further NMLs directly compute output Boolean values, which strongly limits the flexibility of reasoning under uncertainty, while reasoning in R2Ns happens at latent level.
%
Another class of neuro-symbolic approaches, like Deep ProbLog~\cite{manhaeve2018deepproblog},
Semantic Loss~\cite{xu2017semantic}, 
Deep Logic Models~\cite{marra2019integrating} and Relational Neural Machines~\cite{marra2020rnm} define directed or undirected graphical models to jointly train predicates approximated by deep learners and reasoning layers in a single differentiable architecture.
Exact inference over the graphical models is generally intractable and these systems rely on heuristics to apply to any real-world problem~\cite{marra2020inference,manhaeve2021approximate}.
Lifted Relational Neural Networks~\cite{sourek2018lifted} and Neural Theorem Provers (NTP)~\cite{rocktaschel2017end,minervini2018towards,minervini2020learning}
realize a soft forward or backward chaining via an end-to-end gradient-based scheme. NTP allows soft-unification among symbols using a tensorial representation to allow a more flexible matching. However, this introduces scalability issues as reasoning paths can not be easily pruned without relying on heuristics. 
Neural LP~\cite{yang2017differentiable} learns rules using differentiable operators defined in TensorLog~\cite{cohen2020tensorlog}, which is a framework to get advantage of deep learning computational power to perform probabilistic logical reasoning.
Unlike this class of models, R2N approximates relational inference using a forward step in a neural architecture and can scale up to much larger problems.

% ExpressGNN spostato in GNN. Chiaramente anche nesy o KG andavano bene, ma visto il nome che contiene GNN eviterei confusioni
\ar{{\bf R2Ns and KGEs. }}
With the emergence of Knowledge Graphs (KG), representation learning has played a pivotal role in learning embedding spaces, which allow efficient inference and query answering.
Knowledge Graph Embeddings (KGE), initially built from the statistical co-occurrences of entities and relations, have been later enriched with semantic information carried by higher-level logic knowledge to improve embedding accuracy and explainability. As an example of this line of research, both probabilistic Logic Neural Networks~\cite{qu2019probabilistic} and the work by Niu at al.~\cite{niu2020rule} inject logic knowledge into a KGE. However, both approaches require strong assumptions on the form of the distributions to make inference tractable.
Another related class of approaches uses KGs for question answering by modeling multi-hop reasoning in latent space. For example, MINERVA~\cite{das2018gofor} learns to do query answering by walking on a knowledge graph conditioned on a query, and then by stopping when reaching the answer node. 
DeepPath~\cite{xiong2017deeppath} uses reinforcement learning to find paths in a KG but, like \cite{ren2020beta}, its applicability is limited to cases where the target entity is known in advance.

\ar{{\bf R2Ns and GNNs. }} Finally, R2N is related to a recent line of research using Graph Neural Networks (GNNs)~\cite{scarselli2009graph,wu2020comprehensive} to approximate inference in PGMs.
Standard GNNs are examples of Message Passing Neural Networks~\cite{gilmer2017neural}, whose expressiveness can be interpreted in terms of two variable counting logic~\cite{grohe2021logic}, a small fragment of First-Order Logic.
In order to overtake the limitations of standard GNNs in approximating inference, some authors have proposed hybrid models where GNNs are integrated to replace one step of a traditional inference algorithm like Belief Propagation~\cite{kuck2020belief,satorras2021neural}.
Other work has instead attempted at defining higher-order GNNs~\cite{morris2019weisfeiler} which yield expressive models at the cost of a combinatorial explosion of the higher-order edges. R2Ns stand in the middle of standard GNNs and higher-order GNNs as the logic provides a bias on which ``higher-order edges'' (i.e. factors) to instantiate.
The proposed model can indeed be seen as a higher-order GNN, where the layers computing the factor representations process the higher-order information. In this regard, R2N is related to Factor Graph Neural Networks (FGNN)~\cite{zhen2020nips}, but applied to structures representing a grounded FOL logic theory.
Unlike FGNNs, R2Ns are aware of the order of the in-going and outgoing edges in the factor graph. In particular,  $MLP^1_j$ concatenates its in-going messages following the order in which atoms appear in a rule, and one different $MLP^2_{j,i}$ is specialized to correctly predict the embedding for the $i$-th element in the list of the outgoing neighbors.
Furthermore, unlike what done by FGNNs, R2N is designed to work in a neuro-symbolic environment, where the input representations are also co-developed during training. \ar{R2Ns are also related to graph recurrent neural networks (like graph LSTMs) with constrained gating mechanisms \cite{tang2019coherence,yan2020social}. Here, activation in both the spatial and temporal domain are constrained to be coherent.}

Other GNNs have been defined to process the KG topological structure. For example, ExpressGNN~\cite{zhang2020efficient} attempts at overtaking the scalability limitations of probabilistic reasoning approaches using a GNN defined over the KG to embed the constants. Since the employed GNN does not have enough expressive power to perform reasoning, as there are isomorphic substructures in the KG that can not be distinguished, reasoning is performed via an external variational process.
A similar relational structure is considered by Discriminative Gaifman models~\cite{niepert2016discriminative}, which translate FOL formulas into a set of features.
Unlike this class of models, the proposed architecture directly compiles the relational structures required to approximate the reasoning process into the model architecture. Therefore, R2N can embed the constants and the atoms via the knowledge in the forward step of the network without employing an external inference step. 


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Conclusions}
\label{sec:conclusions}
This paper presents a neural architecture to perform relational reasoning using latent representations. The model can be applied on top of different input feeds like KGEs or the embeddings computed by \ar{deep neural networks}. The presented model provides a flexible and expressive platform for neuro-symbolic integration, which can be used either when \ar{logic} knowledge is explicitly available or when the \ar{expression} of the relational knowledge is not fully known.

While the scalability of the presented model is a huge step forward to classical Statistical Relational Learning methods, application on large domains can still be impractical when working on a fully grounded HB, as the number of possible ground atoms grows polynomially on the arity of the considered relations. %For example a relation $R(x,y)$ has a quadratic number of groundings with respect of the size of the domains of $x,y$.
%This issue is not specific to the presented methodology but it is common to all neuro-symbolic methods based on grounded logic, like LTN or SBR.
An accurate approximation of the inference process on the relevant subset of ground atoms is a promising line of research, currently explored by the authors.

\section*{ Acknowledgments} We thank Filippo Guerranti for his help in running one experiment presented in this paper. This project has received funding from the European Union's Horizon 2020 research and innovation program under grant agreement No 825619. This work was also supported by TAILOR, a project funded by EU Horizon 2020 research and innovation programme under GA No 952215. Giuseppe Marra is funded by Research Foundation-Flanders (FWO-Vlaanderen, 1239422N).

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\bibliographystyle{IEEEtran}
\bibliography{references}




\appendix
% \section * {Input modes in R2N }
% \fg{Occhio anyway! che la figura è cambiata!}
% figure~\ref{fig:model_inputs} shows different ways of applying the R2N over different inputs. In figure~\ref{fig:model_inputs}-a,
% KGE embeddings are used as input to refine them according to the logic knowledge structure. In figure~\ref{fig:model_inputs}-b,
% a classifier processes a set of vector-based representations of the constants and predicts the atoms output values; then the reasoning layers take as input the atom embeddings from the last layer of the classifier and refine them by using the higher-level logic knowledge.
% Finally, figure~\ref{fig:model_inputs}-c shows that it is also possible to take the  Boolean outputs of a KGE/classifier as input to the R2N.
% In this case the Boolean values are directly manipulated by the model, which recursively refines the prediction for an atom based on the Boolean values of the surrounding atoms. This latter modality provides a very efficient reasoning schema, which is powerful when the logic knowledge is generally correct in the data and the R2N should provide a translation correcting sparse errors of the KGE based on the relational structure of the knowledge. However, performing reasoning over the latent space like in a) is generally more powerful, as the machine has the additional flexibility of deciding on which portion of the embedded space it should use the relational information.

\ar{\section*{R2N as Belief Propagation}
\subsection*{Logic-based Belief Propagation}
In this section we report a couple of examples of the computation of beliefs of atoms occurring in logic formulas, as detailed in Sec. \ref{sec:bp}.}
\begin{example}
We show how to compute the belief in case of factors expressing ground formulas. To keep the notation simple, we assume here that any variable only occurs in one single factor. %When a variable appears in multiple factors the beliefs are summed across all factors and re-normalized.
Given two atoms $a = p_1(a_1,a_2)$, $b = p_2(b_1,b_2)$ co-occurring in a same factor $g$, the following expressions allow to recompute the beliefs for different semantics:
%\textbf{DA RIVEDERE}
\begin{itemize}
\item Factor for logical OR between atoms: $g = a \lor b$:
% \[
% b(x_a) =\!\!\!\!\!\!\!\!
% \displaystyle\max_{x_a^\prime,x_b^\prime \in [01,10,11]} \prod_{x_i \in \{x_b^\prime\}} \!\!b(x_i) = \max[1-b(x_b), b(x_b)]
% \]
\[
b(x_a)= \! \displaystyle\max_{{\bf x}_a{\bf x}_b \in \{01,10,11\}:\ {\bf x}_a=1} \!\prod_{j \in \{b\}} \!b({\bf x}_j) = \!\! \displaystyle\max_{{\bf x}_b\in\{0,1\}} b({\bf x}_b)
\]
\[
b(x_b)= \! \displaystyle\max_{{\bf x}_a{\bf x}_b \in \{01,10,11\}:\ x_b=1} \!\prod_{j \in \{a\}} \!b({\bf x}_j) = \!\! \displaystyle\max_{{\bf x}_a\in\{0,1\}} b({\bf x}_a)
\]
\item Factor for logical AND among atoms $a \land b$: % \land x_g$:
\[
%b(x_a) =\!\! \displaystyle\max_{(x_a^\prime,x_b^\prime,x_g^\prime) \in [111]} \prod_{i\in (x_b^\prime,x_g^\prime)} b(i) = b(x_b) \cdot b(x_g)
b(x_a) =\!\! \displaystyle\max_{{\bf x}_a{\bf x}_b \in \{11\}:\ {\bf x}_a=1} \prod_{j\in \{b\}} b({\bf x}_j) = b({\bf x}_b=1)
\]
\[
%b(x_b) = \!\! \displaystyle\max_{(x_a^\prime,x_b^\prime,x_g^\prime) \in [111]} \prod_{i\in (x_a^\prime,x_g^\prime)} b(i) = b(x_a) \cdot b(x_g)
b(x_b) = \!\! \displaystyle\max_{{\bf x}_a{\bf x}_b \in \{11\}} \prod_{j\in \{a\}} b({\bf x}_j) = b({\bf x}_a=1)
\]
\end{itemize}
\end{example}

As shown by the previous examples, the belief of a variable appearing in a single factor can be computed as function of the beliefs of the input nodes to the factor. Even if the above examples consider only simple basic formulas, the beliefs of any logic formula can be considered by following the same pattern, just considering a different set of admissible assignments. Therefore, the structure of the function is dictated by the semantic of the ground logic formula represented by the factor.

\ar{\subsection*{Proof of Theorem \ref{the:r2n_approx_bp}}}

In this section, we recall Theorem \ref{the:r2n_approx_bp} and provide its proof.
\begin{appxthm}{1}
	An R2N reasoning block can exactly compute one iteration of the max-product belief propagation algorithm as a forward step within the architecture.
\end{appxthm}
\begin{proof} 
	Assume without loss of generality that a two-dimensional embedding space is used to represent the atoms, such that these representations can be the log-beliefs for the $\{0,1\}$-assignments for each atom at the previous iteration. %Regarding the initialization of the belief at the first reasoning iteration , for example BP typically starts with random or constant variable beliefs, therefore any initial representation of the variables can be used.
	%The contribution of each factor is the maximum over the admissible assignments of the summation of the log-beliefs of the variables in the assignment, as shown by equation~(\ref{eq:log_belief_as_input_function}). In more detail.
	For any variable $x_i$, an R2N reasoning block can reproduce one iteration of the max-product BP algorithm if:
	\begin{enumerate}
	\item for any $g$,  the  factor update function  ($MLP^1_{\varphi(g)}$) takes the log-beliefs of all the variables as input and computes a factor representation that collects for each assignment of ${\bf x}_g$ and variable $x_i\in {\bf x}_g$:
	   % the vector:
	   %\[
	   % \left[\sum_{x \in {\bf x}_g \setminus x_i} \log b(x), \ldots  ~ \forall {\bf x}_g \in \mathcal{T}_g \right] \ ;
	   % \]
	    \[
	    \sum_{j \in ne(g):\ i\neq j} \log b({\bf x}_j) \ .
	    \]
	    which is a simple function of the input log-beliefs.
	    The resulting factor representation collects these values in a vector of size $|ne(g)| \cdot 2^{|\mathcal{T}_g| - 1}$.

        \item The factor-to-node Multi-Layer Perceptron ($MLP^2_{\varphi(g),i}$) selects the maximum over the input values, corresponding to sum of the log-belief of the assignments $\mathbf{x}_g\in\mc T_g$, with ${\bf x}_i=x_i$.

        \item The final belief of a node is computed as summation over a per-factor contribution in the BP algorithm.
	\end{enumerate}
	Since an MLP with at least one hidden layer and a sufficient number of hidden neurons is a universal function approximator~\cite{cybenko1992approximation, lu2017expressive}, it trivially follows that it is possible to train the MLPs to approximate the first two steps with arbitrary precision, if provided with sufficient computational power, and the factor representation has at least $|ne(g)| \cdot 2^{|\mathcal{T}_g| - 1}$ values.
	Finally, if the aggregation operator in equation~\ref{eq:update_a} is selected to be the summation over the factor contribution, the factor aggregation step of BP is perfectly replicated 
% 	at  structural level 
	by an R2N.
\end{proof}

In addition, it is possible to provide a tighter definition of the minimum computational power of the MLP architectures, instead of the over-estimates provided by the universal approximation theorem. In particular, the $MLP^1$ has to compute different summations over its inputs, such that a single layer network with linear neuron activation already provides sufficient computation power.
On the other hand, the following proposition provides a direct estimate of the number of layers and neurons needed by the $MLP^2$ networks to compute the selection of the maximum element in equation \ref{eq:log_belief_as_input_function} using RELU neuron activations.
\begin{proposition}
	\label{the:max_approx}
	For arbitrary real valued feature matrix $\mathbf{X} \in \mathbb{R}^{m \times n}$ with $x_{ij}$ as its entry in the $i$-th row and $j$-th column, the feature mapping operation $x^\star = [\max_j x_{ij} ]$ can be exactly parameterized with a $2 \log_2 n$-layer neural network with RELU as activation function and at most $2n$ hidden units. See Zhen et al.~\cite{zhen2020nips} for a proof.
\end{proposition}

\ar{\section*{Experimental Details}
\subsection*{Experimental settings}}
All the employed datasets do not include personal data. The inference graphs have been pruned using standard techniques~\cite{shavlik2009speeding} to remove the portions having no effect on the returned predictions.
Experiments have been executed on a machine running Linux with a Volta-V100 GPU, Xeon 2.1Ghz CPU and 48GB of RAM.
The size of the embedded space and the $\lambda$ value, when used, has been selected by maximizing the target metric on the validation set for each experiments, using the grids reported in the provided code package.

\ar{\subsection*{Countries experiment}}
In the Countries experiment (Sec. \ref{sec:countries}), an R2NS model with $3$ stacked reasoning layers was trained for $300$ epochs, Adam optimizer with initial learning rate equal to $0.01$ and the reasoning embedding size was selected via a validation set in the $[5, 50]$ range. Since FGNN requires Boolean inputs, the KGE output (i.e. the true belief for each atom) is directly fed to the FGNN, which performs the reasoning process using the same knowledge used by the R2NS. 

\ar{\subsection*{Knowledge graph completion experiment}}
In the knowledge graph completion experiment (Sec. \ref{sec:kgc}), the different KGE methods have been tested for each dataset (ComplEx, DistMult and TransE), searching the embedding size maximizing the Mean Reciprocal Rank (MRR) metric on the validation set over a grid in the $[10,100]$ range. The best performing KGE was employed as first layer of the model and co-trained for $700$ epochs with an initial learning rate equal to $0.01$, Adam optimizer and reasoning embedding size selected using the validation set on the same grid $[10,100]$.

\ar{\subsection*{Cora experiment}}
In the Cora experiment (Sec. \ref{sec:cora}),
the input text was represented by a bag-of-word representation and processed by an input MLP with 2 layers and $35$ neurons per layer. The classifier and reasoning layers have been co-trained for $400$ epochs, with an initial learning rate equal to $0.01$, Adam optimizer and reasoning embedding size equal to $35$ (validated on the validation set). \ar{The same architecture has been used for the competitor MLP.}
An R2NC model was also built by discarding the available logic knowledge by defining the factors correlating all predicates over each pair of constants, as done for the experiment in Sec.~\ref{sec:kgc}.




% \clearpage

% \input{summary_of_changes}

\end{document}
