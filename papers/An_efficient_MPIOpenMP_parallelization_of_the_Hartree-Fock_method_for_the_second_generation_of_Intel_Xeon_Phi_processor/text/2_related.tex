\section{Related work}
\label{sec:related}
The HF algorithm has been a primary parallelization target since the onset of parallel computing. The primary computational components of the HF algorithm are construction of the density and Fock matrices, that are described in \cref{sec:hf} of this work. The irregular task and data access patterns during Fock matrix construction bring significant challenges to efficient parallel distribution of the computation. The poor scaling of Fock matrix diagonalization is a major expense as well. Linear scaling methods like the fragment molecular orbital method (FMO) have been successfully applied to thousands of atoms and CPU cores \cite{alexeev2012heuristic,umeda2010parallel}, but such methods introduce additional approximations \cite{fedorov2007extending,fedorov2009}. In any case, fragmentations methods may benefit from optimizations of the core HF algorithm as well.

Early HF parallelization efforts focused on the distributed computation of the many electron repulsion integrals (ERIs) required for Fock matrix construction via MPI or other message passing libraries. The Fock and density matrices were often replicated for each rank, and load balancing algorithms were a primary optimization target. Blocking and clustering techniques were explored in depth in a landmark paper by \citet{foster1996toward}. Their contributions were implemented in the quantum chemistry package NWChem~\cite{valiev2010nwchem}. In a follow-up paper by \citet{harrison1996toward}, a node-distributed HF implementation was introduced. In this work, both the density and Fock matrices were distributed across nodes using globally addressable array (GA). In a more recent work UPC++ library was used to achieve this goal \cite{ozog2016hartree}. A similar approach was used to implement distributed data parallel HF by \cite{alexeev2002distributed,alexeev2007parallel} in the GAMESS code. This implementation utilizes the Distributed Data Interface (DDI) message passing library \cite{fletcher2000distributed}. To further address the load balancing issues, a work stealing technique was introduced by \citet{liu2014new}.

A detailed study and analysis of the scalability of Fock matrix construction and density matrix construction \cite{chow2015parallel}, including the effects of load imbalance, was explored in a work by \mbox{\citet{chow2015scaling}}. In this work, density matrix construction was achieved by density purification techniques and the resulting implementation was scaled up to 8,100 Tianhe-2 Intel Xeon Phi first generation co-processors. In fact, a number of attempts have been made to design efficient implementations of HF for accelerators \cite{asadchev2012new,chow2015scaling,ufimtsev2008quantum,ufimtsev2009quantum,wilkinson2011acceleration} and other post-HF methods \cite{apra2014efficient}. A major issue in this context is the management of shared data structures between cores -- in particular, the density and Fock matrices. OpenMP HF implementations with a replicated Fock matrix and shared density matrix have been explored in the work of \citet{ishimura2010mpi} and \citet{mironov2015quantum}. The differences between these works are in the workload distribution among MPI ranks and OpenMP threads. The current work borrows some techniques from these previous works which implement HF for accelerators. The result is a hybrid MPI/OpenMP implementation that is designed to scale well on a large number of Intel Xeon Phi processors, while at the same time managing the memory footprint and maintaining compatibility with the original GAMESS codebase.
