\appendix
\section{Artifact Description: An efficient MPI/OpenMP parallelization of the Hartree-Fock method for the second generation of \intelphireg\ processor}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Abstract}

This description contains all the information needed to run the simulations described in the SC17 paper ``An efficient MPI/OpenMP parallelization of the Hartree-Fock method for the second generation of \intelphireg\ processor''. More precisely, we explain how to compile and run GAMESS simulations to reproduce results presented in \Cref{sec:results} of the paper. The PDB files, GAMESS inputs, and submission scripts used in this work can be downloaded from the Git repository available online\footnote{\label{gitrepo}\url{https://github.com/gms-bbg/gamess-papers}, folder Mironov\_Vladimir-2017-SC17}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Description}

\subsubsection{Check-list (artifact meta information)}

{\em Fill in whatever is applicable with some informal keywords and remove the rest}

{\small
\begin{itemize}
  \item {\bf Algorithm: } Self Consistent Field (SCF) algorithm in Hartree-Fock (HF) method
  \item {\bf Program: } GAMESS binary, Fortran and C libraries
  \item {\bf Compilation: } \intelreg\ Parallel Studio XE 2016 v3.210
  \item {\bf Data set: } graphene bilayer systems from 0.5 nm to 5 nm; details are in the
  		main paper \cref{sec:benchmark} and in \Cref{tab:ad:chem}
  \item {\bf Run-time environment: } when running on Cray XC40 the following modules were loaded:
  		\begin{itemize}
        	\item craype/2.5.9
            \item PrgEnv-intel/6.0.3
            \item craype-mic-knl  
        \end{itemize}
  \item {\bf Hardware: } all single node benchmarked were run on JLSE cluster on \intelphireg\ 7210 processor; all multi-node benchmarks were run on Cray XC40 with up to 3,000 \intelphireg\ 7230 processors
  \item {\bf Experiment customization: } varied number of MPI ranks from 1 to 12,000 and number of OpenMP threads from 1 to 256 on single node.
  \item {\bf Publicly available?: } yes (partially)
\end{itemize}
}

\subsubsection{How software can be obtained (if available)}

In this work, we used GAMESS version dated August 18, 2016 Revision 1. The original GAMESS code can be downloaded (at no cost) from the official GAMESS website (registration required)\footnote{\url{http://www.msg.ameslab.gov/gamess/download.html}}.

Patches for GAMESS developed in this work can be obtained from the authors by request.


\subsubsection{Hardware dependencies}
All measurements in this work were performed on the \second\ generation of \intelphireg\ processor. However, the same benchmarks can be run on any other architectures.

\subsubsection{Software dependencies}
C (C99 standard) compiler, Fortran 95 compatible compiler with OpenMP 3.0 support, MPI library. The code has been extensively tested only for \intelreg\ Parallel Studio XE 2016 update 3 compilers and libraries. MKL BLAS library was used to link GAMESS, but it does not affect the performance of the SCF code and thus, it is optional.

\subsubsection{Datasets}
All structures of chemical systems and corresponding input files used for benchmarking code can be downloaded from the Git repository\cref{gitrepo}. They are easily reconfigurable bi-layer graphene systems. The problem size (computation time, memory footprint) depends on the number of basis functions for the system. These numbers are provided in \Cref{tab:ad:chem}. The basis set used in all calculations is 6-31G(d).

\begin{table}[ht]
	\centering
  	\caption{Chemical systems used in benchmarks and their size characteristics. BF stands for basis function.}
  	\label{tab:ad:chem}
  	\begin{tabular}{lccc}
    \toprule
    Name & \# atoms & \# shells & \# basis functions\\		 
    \midrule
    0.5 nm	&	44		&	176		&	660 \\
    1 nm	&	120		&	480		&	1,800 \\
	1.5 nm	&	220		&	880		&	3,300 \\
	2 nm	&	356		&	1,424	&	5,340 \\
	5 nm	&	2,016	&	8,064	&	30,240 \\
  	\bottomrule
\end{tabular}
\end{table}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Installation}

We followed the standard installation procedure outlined in the file \texttt{PROG.DOC} in GAMESS root directory. GAMESS uses a custom build configuration system. The first step is configuration of the \texttt{install.info} file. To perform the basic configuration one need to run \texttt{\$\{GMS\_HOME\}/config} script and specify compilers and libraries for the compilation. When the script asks whether to compile GAMESS with LIBCCHEM one need to refuse. After that the \texttt{install.info} file can be edited directly to get the desired configuration. After setup is finished, GAMESS compilation can be done with \texttt{make} command. At successful conclusion, the file \texttt{gamess.\$(VERNO).x} will appear in GAMESS home directory, where \texttt{\$(VERNO)} is a variable specified at basic configuration step with \texttt{\$\{GMS\_HOME\}/config} script. \texttt{VERNO="00"} by default.

We used two different \intelphireg systems: JLSE and XC40. The actual \texttt{install.info} configurations are accessible at Git repository in folders \texttt{JLSE} and \texttt{CRAYXC40}.  The key parameters of the \texttt{install.info} file for both clusters are summarized in \Cref{tab:ad:clust}.

Moreover, we manually added the flag \texttt{-xMIC-AVX512} to the compilation line in \texttt{\$\{GMS\_HOME\}/comp} script and added \texttt{-mkl} flag to the link line in \texttt{\$\{GMS\_HOME\}/lked} script. On Cray XC40 system we also modified \texttt{\$\{GMS\_HOME\}/comp}, \texttt{\$\{GMS\_HOME\}/compall}, \texttt{\$\{GMS\_HOME\}/lked}, and \texttt{\$\{GMS\_HOME\}/ddi/compddi} scripts to add a new target ``cray-xc''. The modified files are accessible at Git repository in folders \texttt{JLSE} and \texttt{CRAYXC40}.

For DDI library, we used an experimental version of software to run all benchmarks for the single node \intelphireg\ performance on JLSE cluster. This DDI library uses one-sided communication features of MPI-3 which does not spawn data servers. On the Cray XC40 system, we used the standard DDI library.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Experiment workflow}

We used the standard workflow of the experiment: compile code and run it for different \intelphireg\ system. We varied number of MPI ranks from 1 to 3,000, number of threads from 1 to 256 per rank, and varied different \iphireg\ clustering and memory modes.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Evaluation and expected result}

We ran GAMESS on the Joint Laboratory for System Evaluation (JLSE) cluster using this submission line for OpenMP code:
\begin{lstlisting}[language=bash]
mpirun -n $2 \
  -env OMP_NUM_THREADS $3 \
  -env I_MPI_SHM_LMT shm \
  -env KMP_STACKSIZE 200m \
  -env I_MPI_PIN_DOMAIN auto \
  -env KMP_AFFINITY verbose \
  -env <GAMESS options> gamess.00.x
\end{lstlisting}

On the XC40 system, we used the following submission line using Cobalt queuing system for OpenMP code:

\begin{lstlisting}[language=bash]
rpn=4
allranks=$((COBALT_JOBSIZE*rpn))
aprun -n $allranks -N $rpn \
  -d $threads -cc depth -j 4 \
  --env OMP_NUM_THREADS=64 \
  --env I_MPI_PIN_DOMAIN=64:compact \
  --env KMP_STACKSIZE=2000m \
  --env <GAMESS options> gamess.00.x
\end{lstlisting}

For MPI code, we used this submission line:
\begin{lstlisting}[language=bash]
rpn=4
allranks=$((COBALT_JOBSIZE*rpn))
aprun -n $allranks -N $rpn \
  -cc depth -j 4 \
  --env I_MPI_PIN_DOMAIN=64:compact \
  --env <GAMESS options> gamess.00.x
\end{lstlisting}

The submission and run scripts are accessible at Git repository in folders \texttt{JLSE} and \texttt{CRAYXC40}.

The results of the computation are printed to \texttt{STDOUT} or redirected to the file specified in submission/run script. Time for the SCF part of code can be obtained with the following command:
\begin{lstlisting}[language=bash]
$ grep "TIME TO FORM FOCK" <logfile> \
	| awk '{print $9}'
\end{lstlisting}
It will return the time in seconds for the Fock matrix construction step.

\begin{table}[ht]
	\centering
  	\caption{Key configuration parameters of the install.info file for the supercomputers used in this work}
  	\label{tab:ad:clust}
  	\begin{tabular}{lll}
    \toprule
    Parameter 			& JLSE		& Cray XC40 \\
    \midrule
    GMS\_TARGET			&	linux64	&	cray-xc	\\
    GMS\_FORTRAN		&	ifort	&	ftn		\\
	GMS\_MATHLIB		&	mkl		&	mkl 	\\
    GMS\_DDI\_COMM		&	mpi		&	mpi		\\
    GMS\_MPI\_LIB		&	impi	&	impi	\\
    GMS\_LIBCCHEM		&	false	&	false	\\
    GMS\_PHI			&	true	&	true	\\
    GMS\_SHMTYPE		&	sysv	&	posix	\\
    GMS\_OPENMP			&	true	&	true	\\
  	\bottomrule
\end{tabular}
\end{table}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Experiment customization}
During the experiment we changed the following runtime parameters:
\begin{itemize}
	\item The total number of MPI processes with batch script parameters
    \item The number of OpenMP threads per MPI process by adjusting \verb|OMP_NUM_THREADS| environmental variable
    \item Memory and clustering modes of \intelphireg\ nodes (BIOS parameters, restart is required)
    \item The environmental variables for Intel MPI and Intel OpenMP libraries (\verb|KMP_AFFINITY|, \verb|I_MPI_PIN_DOMAIN|)
\end{itemize}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Notes}
Some of the GAMESS timer routines use CPU time instead of wall clock time without informing the user about it. Timing results from these routines will be incorrect for multithreaded code. Therefore, we used  \verb|omp_get_wtime()| function to measure the performance of OpenMP code.

%The links to the GitHub project were removed for the blind review.