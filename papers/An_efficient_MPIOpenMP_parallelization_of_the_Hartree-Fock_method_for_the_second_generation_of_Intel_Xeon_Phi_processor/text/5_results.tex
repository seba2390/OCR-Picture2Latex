\section{Methodology}
\label{sec:benchmark}

\subsection{Description of hardware and software}
\label{ssec:supercomp}
The benchmarks reported in this paper were performed on the Intel Xeon Phi systems provided by the Joint Laboratory for System Evaluation (JLSE) and the Theta supercomputer at the Argonne Leadership Computing Facility (ALCF) \cite{alcf}, which is a part of the U.S. Department of Energy (DOE) Office of Science (SC) Innovative and Novel Computational Impact on Theory and Experiment (INCITE) program \cite{incite}. Theta is a 10-petaflop Cray XC40 supercomputer consisting of 3,624 Intel Xeon Phi 7230 processors. Hardware details for the JLSE and Theta system are shown in \Cref{tab:hw}.

The Intel Xeon Phi processor used in this paper has 64 cores each equipped with L1 cache. Each core also has two Vector Processing Units, both of which need to be used to get peak performance. This is possible because the core can execute two instructions per cycle. In practical terms, this can be achieved by using two threads per core. Pairs of cores constitute a tile. Each tile has an L2 cache symmetrically shared by the core pair. The L2 caches between tiles are connected by a two dimensional mesh. The cores themselves operate at 1.3 GHz. Beyond the L1 and L2 cache structure, all the cores in the Intel Xeon Phi processor share 16 GBytes of MCDRAM (also known as High Bandwidth Memory) and 192 GBytes of DDR4. The bandwidth of MCDRAM is approximately 400 GBytes/sec while the bandwidth of DDR4 is approximately 100 GBytes/sec. 

\begin{table}
  \caption{Hardware and software specifications}
  \label{tab:hw}

  \begin{tabularx}{\columnwidth}{XX}
  \toprule
			\multicolumn{2}{c}{\textbf{\intelphi\ node characteristics}} \\
    \midrule 
    \intelphi\ models				&	7210 and 7230 (64~cores, 1.3~GHz, 
    									2,622 GFLOPs) \\
    Memory per node					&	16 GB MCDRAM, \newline 192 GB DDR4 RAM \\
    Compiler						&	Intel Parallel Studio XE 2016v3 \\
    \midrule
    		\multicolumn{2}{c}{\textbf{JLSE \iphi\ cluster (26.2 TFLOPS peak)}} \\
    \midrule
    \# of \intelphi\ nodes	&	10 \\
    Interconnect type				&	Intel Omni-Path\textsuperscript{TM} \\
    \midrule
    		\multicolumn{2}{c}{\textbf{Theta supercomputer (9.65~PFLOPS peak)}} \\
    \midrule
    \# of \intelphi\ nodes				&	3,624 \\
    Interconnect type				&	Aries interconnect with \newline Dragonfly topology \\
  \bottomrule
\end{tabularx}

\end{table}

\begin{table}
\begin{threeparttable}
  \caption{Chemical systems used in benchmarks and their size characteristics}
  \label{tab:chem}

  \begin{tabularx}{\columnwidth}{XYYYYY}

  \toprule

  \multirow{2}{*}{Name}	&	\multirow{2}{*}{\# atoms}	&	\multirow{2}{*}{\# BFs\tnote{a}}	&	\multicolumn{3}{c}{Memory footprint\tnote{b}, GB} \\
  \cmidrule(l){4-6}
  		& & &	{MPI\tnote{c}}	&	{Pr.F.\tnote{d}}	&	{Sh.F.\tnote{e}} \\
  \midrule
  	0.5~nm	&	44			&	660			&	7		&	0.13		&	0.03	\\
	1.0~nm	&	120			&	1800		&	48		&	1			&	0.2	\\
	1.5~nm	&	220			&	3300		&	160		&	3			&	0.8	\\
	2.0~nm	&	356			&	5340		&	417		&	8			&	2	\\
	5.0~nm	&	2016		&	30240		&	9869	&	257			&	52	\\
	\bottomrule
  \end{tabularx}

  \begin{tablenotes}
  	\item [a] BF -- basis function
    \item [b] Estimated using \crefrange{eqn:mem:mpi}{eqn:mem:shr}
 	\item [c] MPI-only SCF code
    \item [d] Private Fock SCF code
    \item [e] Shared Fock SCF code
  \end{tablenotes}
\end{threeparttable}
\end{table}

These two levels of memory can be configured in three different ways (or modes). The modes are referred to as Flat mode, Cache mode, and Hybrid mode. Flat mode treats the two levels of memory as separate entities. The Cache mode treats the MCDRAM as a direct mapped L3 cache to the DDR4 layer. Hybrid mode allows the user to use a fraction of MCDRM as L3 cache allocate the rest of the MCDRAM as part of the DDR4 memory.
In Flat mode, one may choose to run entirely in MCDRAM or entirely in DDR4. The "numactl" utility provides an easy mechanism to select which memory is used. It is also possible to choose the kind of memory used via the "memkind" API, though as expected this requires changes to the source code.

Beyond memory modes, the Intel Xeon Phi processor supports five cluster modes. The motivation for these modes can be understood in the following manner: to maintain cache coherency the Intel Xeon Phi processor employs a distributed tag directory (DTD). This is organized as a set of per-tile tag directories (TDs), which identify the state and the location on the chip of any cache line. For any memory address, the hardware can identify the TD responsible for that address. The most extreme case of a cache miss requires retrieving data from main memory (via a memory controller). It is therefore of interest to have the TD as close as possible to the memory controller. This leads to a concept of locality of the TD and the memory controllers.
It is in the developer's interest to maintain the locality of these messages to achieve the lowest latency and greatest bandwidth of communication with caches. Intel Xeon Phi supports all-to-all, quadrant/hemisphere and sub-NUMA cluster SNC-4/SNC-2 modes of cache operation.

For large problem sizes, different memory and clustering modes were observed to have little impact on the time to solution for the three versions of the GAMESS code. For this reason, we simply chose the mode most easily available to us. In other words, since the choice of mode made little difference in performance, our choice of Quad-Cache mode was ultimately driven by convenience (this being the default choice in our particular environment). Our comments here apply to large problem sizes, so for small problem sizes, the user will have to experiment to find the most suitable mode(s).


\subsection{Description of chemical systems}
\label{ssec:chemical}
For benchmarks, a system consisting of parallel series of graphene sheets was chosen. This system is of interest to researchers in the area of (micro)lubricants \cite{kawai2016superlubricity}. A physical depiction of the configuration is provided in \Cref{fig:graphene}. The graphene-sheet system is ideal for benchmarking, because the size of the system is easily manipulated. Various Fock matrix sizes can be targeted by adjusting the system size.

\begin{figure}
	\includegraphics[width=\columnwidth]{Figure2}
	\caption{Model system of a C$_{2016}$ graphene bilayer. In the text, we refer to this system as 5~nm.
    		 There are two layers with size 5~nm by 5~nm.
             Each graphene layer consists of 1,008 carbon atoms.}
    \label{fig:graphene}
\end{figure}

\subsection{Characteristics of datasets}
\label{ssec:datasets}
In all, five configurations of the graphene sheets system were studied. The datasets for the systems studied are labeled as follows: 0.5~nm, 1.0~nm, 1.5~nm, 2.0~nm, and 5.0~nm.  \Cref{tab:chem} lists size characteristics of these configurations. The same 6-31G(d) basis set (per atom) was used in all calculations. For N basis functions, the density, Fock, AO overlap, one-electron Fock matrices and the matrix of MO coefficients are N$\times$N in size. These are the main data structures of significant size. Therefore, the benchmarks performed in this work process matrices which range from 660$\times$660 to 30,240$\times$30,240. For each of the systems studied, \Cref{tab:chem} lists the memory requirements of the three versions of GAMESS HF code.
Denoting $N_{BF}$ as the number of basis functions, the following equations describe the asymptotic $(N_{BF}\to\infty)$ memory footprint for the studied HF algorithms:
\begin{subequations}
	\label{eqn:mem}
	\begin{align}
		M_{MPI} =& 5/2 \cdot N_{BF}^2 \cdot N_{MPI\_per\_node}, 				\label{eqn:mem:mpi} \\
		M_{PrF} =& (2+N_{threads}) \cdot N_{BF}^2 \cdot N_{MPI\_per\_node}, 	\label{eqn:mem:prv} \\
		M_{ShF} =& 7/2 \cdot N_{BF}^2 \cdot N_{MPI\_per\_node},					\label{eqn:mem:shr}
	\end{align}
\end{subequations}
where $M_{MPI}$, $M_{PrF}$, $M_{ShF}$ denote the memory footprint of MPI-only, private Fock, and shared Fock algorithms respectively; $N_{threads}$ denotes the number of threads per MPI process for the OpenMP code, and $N_{MPI\_per\_node}$ denotes the number of MPI processes per KNL node. For OpenMP runs $N_{MPI\_per\_node}=4$, while for MPI runs the number of MPI ranks was varied from 64 to 256.

If one compares columns MPI versus Pr.F and Sh.F. in \Cref{tab:chem}, you will see that the private Fock code has about a 50 times less footprint compared to the stock MPI code. For the shared Fock code, the difference is even more dramatic with a savings of about 200 times. The ideal difference is 256 times since we compare 256 MPI ranks in the stock MPI code where all data structures are replicated versus 1 MPI rank with 256 threads for the hybrid MPI/OpenMP codes. But we introduced additional replicated structures (see \Cref{fig:buffer}) and many relatively small data structures are replicated also in the MPI/OpenMP codes. This explains the difference between the ideal and observed footprints.

Each of the aforementioned datasets was used to benchmark three versions of the GAMESS code. The first version is the stock GAMESS MPI-only release that is freely available on the GAMESS website~\cite{gamesswebsite}. The second version is a hybrid MPI/OpenMP code, derived from the stock release. This version has a shared density matrix, but a thread-private Fock matrix. The third version of the code is in turn derived from the second version; it has shared density and Fock matrices. A key objective was to see how these incremental changes allow one to manage (i.e., reduce) the memory footprint of the original code while simultaneously driving higher performance.

\section{Results}
\label{sec:results}

\subsection{Single node performance}
\label{ssec:singlenode}
The second generation Intel Xeon Phi processor supports four hardware threads per physical core. Generally, more threads per core can help hide latencies inherent in an application. For example, when one thread is waiting for memory, another can use the processor. The out-of-order execution engine is beneficial in this regard as well. To manipulate the placement of processes and threads, the \verb|I_MPI_DOMAIN| and \verb|KMP_AFFINITY| environment variables were used. 
We examined the performance picture when one thread per core is utilized and when four threads per core are utilized. As expected, the benefit is highest for all versions of GAMESS for two threads (or processes) per core. For three and four threads per core, some gain is observed, albeit at a diminished level. \Cref{fig:afty} shows the scaling curves with respect to the number of hardware threads utilized observed by us.

\begin{figure}
	\includegraphics[width=\columnwidth]{Figure3}
	\caption{Performance dependence on OpenMP thread affinity type for the shared Fock version of the GAMESS code
    		 on a single \intelphireg\ processor using the 1.0 nm benchmark.
             All calculations are performed in quad-cache mode.
             Four MPI ranks were used in all cases.
             The number of threads per MPI rank was varied from 1 to 64.}
    \label{fig:afty}
\end{figure}

\begin{figure}
	\includegraphics[width=\columnwidth]{Figure4}
	\caption{Scalability with respect to the number of hardware threads of the original MPI code
    		and two OpenMP versions on a single \intelphireg\ processor using the 1.0~nm benchmark.}
    \label{fig:singlescaling}
\end{figure}

As a first test, single-node scalability was examined with respect to hardware threads of all three versions of GAMESS. For the MPI-only version of GAMESS, the number of ranks was varied from~4 to~256. For the hybrid versions of GAMESS, the number of ranks times the number of threads per rank is the number of hardware threads targeted. The larger memory requirements of the original MPI-only code restrict the computations to, at most, 128 hardware threads. In contrast, the two hybrid versions can easily utilize all 256 hardware threads available. Finally, in general terms, on cache based memory architectures, it is expected that larger memory footprints potentially lead to more cache capacity and cache line conflict effects. These effects can lead to diminished performance, and this is yet another motivation to look at a hybrid MPI+X approach.

The results of our single-node tests are plotted in \Cref{fig:singlescaling}. It is found that using the private Fock version leads to the best time to solution for the 1.0~nm dataset, for any number of hardware threads. This version of the code is much more memory-efficient than the stock version but, because the Fock matrix data structure is private, it has a much larger memory footprint than the shared Fock version of GAMESS. Nevertheless, because the Fock matrix is private, there is less thread contention than the shared Fock version.

It was mentioned in \Cref{ssec:omp} that shared Fock algorithm introduces additional overhead for thread synchronization. For small numbers of Intel Xeon Phi threads, this overhead is expected to be low. Therefore the shared Fock version is expected to be on par with the other versions. Eventually, as the overhead of the synchronization mechanisms begins to increase, the private Fock version of the code is found to dominate. In the end, the private Fock version outperforms stock GAMESS because of the reduced memory footprint, and outperforms the shared Fock version because of a lower synchronization overhead.
Therefore, on a single node, the private Fock version gives the best time-to-solution of the three codes, but the shared Fock version strikes a (better) balance between memory utilization and performance.

\begin{figure}
	\includegraphics[width=\columnwidth]{Figure5}
	\caption{Time to solution (x axis, time in seconds) for different clustering and memory modes.
    		 Left column displays the small chemical system -- 0.5~nm bilayer graphene and
             right column displays one of the largest molecules bilayer graphene -- 2.0~nm.}
    \label{fig:tts}
\end{figure}

Beyond this, one must consider the choice of memory mode and cluster mode of the Intel Xeon Phi processor. It should be noted that, depending on the compute and memory access patterns of a code, the choice of memory and cluster mode can be a potentially significant performance variable. The performance impact of different memory and cluster modes is examined for the 0.5~nm (small) and~2.0~nm (large) datasets. The results are shown in \Cref{fig:tts}. For both datasets, some variation in performance is apparent when different cluster modes and memory modes are used. The smaller dataset indicates more sensitivity to these variables than the larger dataset. Also, for both data sizes the private Fock version performs best in all cluster and memory modes tested. Also, except in the All-to-All cluster mode, the shared Fock version significantly outperforms the MPI-only stock version. In the All-to-All mode, the MPI-only version actually outperforms the shared Fock version for small datasets, and the two versions are close to parity for large datasets. In total, it is concluded that the quadrant-cache cluster-memory mode is best suited to the design of the GAMESS hybrid codes.

\subsection{Multi-node performance}
It is very important to note that the total number of MPI ranks for GAMESS is actually twice the number of compute ranks because of the DDI. The DDI layer was originally implemented to support one-sided communication using MPI-1. For GAMESS developers, the benefit of DDI is convenience in programming. The downside is that each MPI compute process is complemented by an MPI data server~(DDI) process, which clearly results in increased memory requirements. Because data structures are replicated on a rank-by-rank basis, the impact of DDI on memory requirements is particularly unfavorable to the original version of the GAMESS code. To alleviate some of the limitations of the original implementation, an implementation of DDI based on MPI-3 was developed \cite{pruitt2016private}. Indeed, by leveraging the ``native'' support of one-sided communication in MPI-3, the need for a DDI process alongside each MPI rank was eliminated. For all three versions of the code benchmarked here, no DDI processes were needed.

\begin{figure}
	\includegraphics[width=\columnwidth]{Figure6}
	\caption{Multi-node scalability of the Private Fock and the Shared Fock hybrid MPI-OpenMP
    		 and the MPI-only stock GAMESS codes on the Theta machine with the 2.0~nm dataset.
             The quad-cache cluster-memory mode was used for all data points.}
    \label{fig:2nm}
\end{figure}

\Cref{fig:2nm} shows the multi-node scalability of the MPI-only version of GAMESS versus the private Fock and the shared Fock hybrid versions. It is important to appreciate at the outset that the multi-node scalability of the original MPI-only version of GAMESS is already reasonable. For example, the code scales linearly to 256 Xeon Phi nodes, and it is really the memory footprint bottleneck that limits how well all the Xeon Phi cores on any given node can be used. This pressure is reduced in the private Fock version of the code, and it is essentially eliminated in the shared Fock version. Overall, for the 2~nm dataset, the shared Fock code runs about six times faster than stock GAMESS on 512 Xeon Phi processors. It resulted from the better load balance of the shared Fock algorithm that uses all four shell indices -- two are used in MPI and two are used in OpenMP workload distribution. The actual timings and efficiencies are listed in \Cref{tab:efficiency}.

\begin{table}
\begin{threeparttable}
  \caption{Parallel efficiency of the three different HF algorithms using 2.0~nm dataset}
  \label{tab:efficiency}
  \begin{tabularx}{\columnwidth}{XYYYYYY}

  \toprule
    	\multirow{2}{*}{\# Nodes}		&	\multicolumn{3}{c}{Time-to-solution, s} &
                            \multicolumn{3}{c}{Parallel efficiency, \%} \\
        \cmidrule(rl{0.75em}){2-4} \cmidrule(l){5-7}
  					&	{MPI\tnote{a}} &	{Pr.F.\tnote{a}} &	{Sh.F.\tnote{a}} &
                        {MPI\tnote{a}} &	{Pr.F.\tnote{a}} &	{Sh.F.\tnote{a}} \\

  	\midrule
		4	&	2661	&	1128	&	1318	&	100	&	100	&	100 \\
		16	&	685		&	288		&	332		&	97	&	98	&	99 \\
		64	&	195		&	78		&	85		&	85	&	90	&	97 \\
		128	&	118		&	49		&	43		&	70	&	72	&	96 \\
		256	&	85		&	44		&	23		&	49	&	40	&	90 \\
		512	&	82		&	44		&	13		&	25	&	20	&	79 \\
    \bottomrule
   \end{tabularx}

 	\begin{tablenotes}
 		\item [a] MPI-only SCF code
    	\item [b] Private Fock SCF code
    	\item [c] Shared Fock SCF code
 	\end{tablenotes}
\end{threeparttable}
\end{table}

\begin{figure}
	\includegraphics[width=\columnwidth]{Figure7}
	\caption{Scalability of the Shared Fock hybrid MPI-OpenMP version of GAMESS on the Theta machine
    		 for the 5.0~nm (i.e. large) dataset in quadrant cache mode on 3,000 \intelphireg\ processors.
             The results here are for 4~MPI ranks per node with 64~threads per rank,
             giving full saturation (in terms of hardware threads) on every \intelphireg\ node. For each point in the figure, we show the time in seconds.}
    \label{fig:5nm}
\end{figure}

\Cref{fig:5nm} shows the behavior of the shared Fock version of GAMESS for the 5~nm dataset. It is the largest dataset we could fit in memory on Theta. Since we run on 4~MPI ranks the memory footprint is approximately 208~GB per node. This figure shows good scaling of the code up to 3,000 Xeon Phi nodes, which is equal to 192,000 cores (64~cores per node).