
% Panoptic segmentation

% 3D segmentation

% Multi-object tracking

% Online 3D panoptic:

% PanopticFusion: (IROS 2019)
% https://arxiv.org/pdf/1903.01177.pdf
%
% - most similar to ours
% - PSPNet + M-RCNN + 2D fusion
% - volumetric mapping, 
% - greedy matching with IoU -> optimal only with 0.5 threshold
% - voxel & class weighting
% - CRF regularisation
%
% - good:
%
% - bad:
%  - CRF post-processing step
%  - greedy data-association
%    - can't be tuned for lower overlap ratios -> has to have high framerate, large changes in viewpoint could break this
%    - IoU: sensitive to 2D labels projecting over object borders (CRF and voxel weighting seem to alleviate this)

% Voxblox++: (Robotics & automation letters 2019)
% https://arxiv.org/pdf/1903.00268.pdf
% https://github.com/ethz-asl/voxblox-plusplus
%
% - M-RCNN + geometric segmentation + fusion 
% - data association of geometric segments with 3D overlap (no. points inside volume), fixed threshold for min number of points
% - instance label is assigned to a segment based on highest overlap
% - only one detected segment per reference label, as in PanopticFusion and Ours
% - TSDF Integration 
%
% good: 
% - because of geometric segmentation objects with no associated semantic class can also be segmented
% bad:
% - two different object segment types -> confusing, overly complicated ?
% - quite inaccurate (fixed below)

% Reconstructing Interactive 3D Scenes by Panoptic Mapping and CAD Model Alignments (ICRA 2021)
% https://arxiv.org/pdf/2103.16095.pdf
% https://github.com/hmz-15/Interactive-Scene-Reconstruction
%
% - based heavily on Voxblox++, much more accurate
% - Scene-graph ("contact graph") for mapping object relations
% - Search & replace voxels with CAD models, with geometrical and physical constraints
% - Object 6D pose
% - Format for robot interaction
%
% - Segmentation: bilateral fusion of geomatric and semantic segments -> reduce segmentation noise compared to Voxblox++
% - Fusion: triplet count improves consistency over Voxblox++ pairwise count strategy (take semantic label into account in addition to instance and geometry)
% - Fusion: instance labels are also combined if there is enough overlap with common geometric label for long enough time
%   - this means multiple detections can match the same reference unlike ours, voxblox++ and PanopticFusion ?
%

% Panoptic-MOPE: (ROBOTICS AND AUTOMATION LETTERS 2020)
% https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8977356
% https://github.com/hoangcuongbk80/Object-RPE/tree/panoptic-mope
%
% - novel RGB-D semantic segmentation model + M-RCNN
% - camera tracking based on "addaptively weighted optimization of geometric, appearance, and semantic cues"
% - surfel map: 
%   - how does it scale ? authors satate they tested on room-sized environments, but could be applied in larger scale as well ...
%     - could maybe be applied as VO in a SLAM algorithm ...
%   - demo only on a small pallet + surroundings, might not be applicable in large-scale SLAM

% US VS THEM:
%
% - based heavily on PanopticFusion, with modifications:
%   - instead of greedy data-association (which seems to be the case in others as well), we solve LAP (JPDA?)
%     - overlap threshold can be tuned, which renders the algorithm more flexible
%     - could be extended to dynamic tracking ?
%   - multiple options for association likelihood
%   - outlier rejection (either clustering or probabilistic)
%   - test different options for decreasing processing time
%   - no post-processing
%
% - model-agnostic:
%   - completely separated from segmentation
%   - does not care how point clouds are obtained -> applicable for LIDAR segmentation (e.g. EfficientLPS) as well
%
% - also agnostic to localisation method
%   - could, however, be utilised to find landmark locations / poses

% More compact version of this paragraph to introduction to save space?
%Panoptic segmentation -- proposed in \cite{panoptic_segmentation} -- aims to solve the unified task of semantic- and instance segmentation. Semantic classes are separated to \textit{stuff} -- amorphous, unquantifiable regions like sky, road or floor -- and \textit{things} -- quantifiable objects. The distinction between the two can vary depending on the application, but a semantic class can only belong to one or another. The article also proposes a unified panoptic evaluation metric, coined \textbf{Panoptic Quality} (PQ). Many 2D approaches to panoptic segmentation -- \textit{e.g.} \cite{panopticfpn,seamless,panoptic_deeplab,efficientps} -- have since been proposed. Deep neural networks for performing semantic- or instance segmentation directly on the 3D reconstruction -- \textit{e.g.} on \cite{scannet,s3dis,paris_lille_3d} -- have also been proposed, but since they require the reconstructed 3D scene, they are mostly offline approaches and therefore out of scope for this work. Some recent works also apply panoptic segmentation to point clouds -- \textit{e.g.} methods in the SemanticKITTI panoptic segmentation competition \cite{semantic_kitti} -- mostly aimed at segmenting LiDAR output. They are suitable for online processing, but similar to RGB-D images require a method for tracking object instances persistent in both time and space. In fact, our proposed method, as well as some others mentioned in this work, could use segmented LiDAR point clouds as an input similarly to RGB-D images.

PanopticFusion \cite{panopticfusion} is the first work to propose online integration of panoptic image segmentations to a 3D reconstruction. They integrate point clouds generated from segmented images to a TSDF voxel volume \cite{tsdf,voxblox} by greedily matching detected segments with the reconstruction and regulating each voxel's corresponding instance with a weighting function. Semantic labels are inferred in a bayesian manner based on confidence scores provided by the segmentation model. They also apply a Conditional Random Field (CRF) to regularise the reconstruction, improving results significantly. Voxblox++ \cite{voxblox++} -- introduced later the same year -- is a similar approach that also integrates segmented RGB-D images into a TSDF volume. It leverages geometric segmentation of depth images to improve instance segmentation accuracy. Both geometric and semantic segments are used to compute a pair-wise weight, which is used to greedily match them with segments in the reconstruction. Because of the geometric segmentation, the method allows segmentation of objects with no known semantic class in addition to objects recognised by the instance segmentation model. 

Recently, \cite{interactive_3d_scenes} built upon the idea of Voxblox++. They apply Voxblox++ for 3D instance integration, with two small but effective modifications: the pair-wise weight is replaced by a triplet weight that also takes semantic labels into account in the fusion, and -- in addition to geometric segments -- instance segments are fused if they overlap by a significant amount. The article introduces a method for searching and aligning CAD models to reconstructed objects based on geometry and semantic class, as well as geometrical and physical rules. With the CAD models, a contact graph and interactive virtual scene are reconstructed to allow a robot to simulate its interaction with the environment. SceneGraphFusion \cite{scenegraphfusion} is another approach that forms a scene graph online from a stream of RGB-D images, but unlike the above-mentioned approach, it generates the graph with a deep neural network, after which the panoptic labels for geometrically segmented portions of the 3D reconstruction are produced a side product.

Panoptic-MOPE \cite{panoptic_mope} is another recent approach, which integrates sequences of RGB-D images into a surfel reconstruction. Unlike other mentioned approaches -- which assume the camera pose either known or estimated elsewhere -- it also tracks camera movements based on geometric-, appearance- and semantic cues. The method also applies a novel RGB-D panoptic segmentation model. Although it is only tested on room-sized environments, the authors claim it could be scaled to larger environments as well.