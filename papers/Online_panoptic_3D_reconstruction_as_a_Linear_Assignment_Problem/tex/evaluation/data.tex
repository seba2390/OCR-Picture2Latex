
All training and evaluation in this manuscript are performed on the ScanNet \cite{scannet} dataset. It contains 2.5 million RGB-D video frames and ground-truth poses from different indoor scenes. Each scene has both 2D and 3D ground-truth annotations with two stuff classes ('wall' and 'floor'), as well as 18 thing classes of objects commonly found in indoor environments. A test set with hidden ground-truth is provided for evaluation of semantic segmentation and object segmentation both in 2D and 3D, but since no panoptic ground-truth is available for the hidden set, a part of the training data is applied as an open test set in this work.

A subset of 25 000 images -- frames sub-sampled from video sequences approximately every 100 frames -- are provided with 2D ground truth for training image segmentation models, which are used to train the panoptic segmentation model. A part of the training data -- consisting of roughly five percent of the images in the 2D training set -- is applied as a validation set. The randomly picked validation scenes are separated from training scenes: locations present in validation dataset are not found in training data. The same scenes found in 2D validation set are also used in evaluating 3D performance. However, since multiple scenes are sometimes captured from the same locations and the reconstructions are usually quite similar, only one instance of each location is stored for 3D evaluation. Therefore, 3D evaluation is performed on 35 randomly picked unique indoor scenes not found in training data.
