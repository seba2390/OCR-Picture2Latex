
\begin{table}[t]
\caption[Average 3D Panoptic Quality on open ScanNet validation set.]
{Average 3D Panoptic Quality on open ScanNet validation set. Mean Panoptic Quality (PQ), Segmentation Quality (SQ) and Recongnition Quality (RQ) are reported over all classes, as well as over \textit{things} and \textit{stuff} separately.}
\label{tab:scannet_pq}
\begin{center}

\resizebox{\textwidth}{!}{
\begin{threeparttable}
\setlength\tabcolsep{3pt}
\setlength{\belowcaptionskip}{-10pt}

\begin{tabular}{c c c c | c c c | c c c | c}

       & \multicolumn{3}{c}{all} & \multicolumn{3}{c}{stuff} & \multicolumn{3}{c}{things} \\
\multicolumn{1}{c}{} & PQ & SQ & RQ & PQ & SQ & RQ & PQ & SQ & RQ & framerate (Hz)\tnote{2} \\
 
%\hline
\multicolumn{10}{l}{} \\

\multicolumn{10}{l}{Results reported in PanopticFusion\tnote{1} \cite{panopticfusion}} \\
\hline
\multicolumn{1}{l|}{offline, with CRF} & 33.5 & 73.0 & 45.3 & 58.4 & 70.7 & 80.9 & 30.8 & 73.3 & 41.3 & - \\
\multicolumn{1}{l|}{online, without CRF}  & 29.7 & \textbf{71.2} & 41.1 & \textbf{56.7} & 69.5 & \textbf{79.6} & 26.7 & \textbf{71.4} & 36.8 & 4.30 \\

\multicolumn{10}{l}{} \\
\multicolumn{10}{l}{Results with our method} \\
\hline
\multicolumn{1}{l|}{Greedy, 2.4 cm voxels}   & 29.6 & 66.0 & 41.4 & 51.1 & 69.5 & 71.4 & 27.2 & 65.6 & 38.0 & 0.32 \\
\multicolumn{1}{l|}{Greedy,  5 cm voxels}    & 20.0 & 54.1 & 28.9 & 46.3 & 64.6 & 68.9 & 17.1 & 53.0 & 24.5 & 2.66 \\
\multicolumn{1}{l|}{Greedy,  10 cm voxels}   & 28.9 & 57.0 & 42.4 & 47.3 & 64.7 & 70.3 & 26.9 & 56.1 & 39.3 & 9.83 \\

\multicolumn{10}{c}{} \\

\multicolumn{1}{l|}{Hungarian, 2.4 cm voxels}     & 33.5 & 65.4 & 47.6 & 53.4 & \textbf{70.0} & 74.3 & 31.3 & 64.9 & 44.6 & 0.33 \\
\multicolumn{1}{l|}{Hungarian,  5 cm voxels}       & \textbf{34.0} & 68.0 & \textbf{47.8} & 52.4 & 69.0 & 74.3 & \textbf{31.9} & 67.9 & \textbf{44.5} & 3.53 \\
\multicolumn{1}{l|}{Hungarian,  10 cm voxels}      & 31.7 & 62.3 & 47.0 & 47.2 & 66.5 & 68.8 & 30.0 & 61.9 & 44.5 & \textbf{11.63}\\
%\hline
\end{tabular}
\begin{tablenotes}\footnotesize
\item[1] Open validation set of the PanopticFusion paper contains different ScanNet scenes than the validation set of this work.
\item[2] PanopticFusion and our method were evaluated on different hardware, thus framerates should not be directly compared between them. They are nevertheless reported to clarify the effect of reducing image and voxel-grid resolutions.
\end{tablenotes}
\end{threeparttable}}

\end{center}
\end{table}

Table \ref{tab:scannet_pq} compares our method to PanopticFusion on the Panoptic Quality (PQ) metric \cite{panoptic_segmentation}, which is a combination of Segmentation Quality (SQ) and Recognition Quality (RQ). The results have been reported as an average over all classes, as well as over stuff and thing classes separately. The results of the original PanopticFusion \cite{panopticfusion} have been reported both with and without Conditional Random Field (CRF) regularisation. Because the test data and panoptic segmentation model are not the same as reported in \cite{panopticfusion}, we have also re-implemented the Greedy data-association algorithm of PanopticFusion: detection segments are processed in descending order based on size, by choosing the target corresponding to highest IoU. IoU threshold for the greedy algorithm is set to $0.25$ as in the original work. The results on the test closest to the original PanopticFusion results -- Greedy algorithm with $2.4~cm$ voxels -- seem to be quite close to the results reported in the original paper.

\begin{figure}[t]
\setlength{\belowcaptionskip}{-10pt}
\centering
\begin{subfigure}[b]{0.32\textwidth}  
        \includegraphics[width=\textwidth]{fig/288_seg.pdf}
\end{subfigure}
\begin{subfigure}[b]{0.32\textwidth}  
        \includegraphics[width=\textwidth]{fig/616_seg.pdf}
\end{subfigure}
\begin{subfigure}[b]{0.32\textwidth}  
        \includegraphics[width=\textwidth]{fig/655_seg.pdf}
\end{subfigure}
\hfill
\begin{subfigure}[b]{0.32\textwidth}  
        \centering 
        \includegraphics[width=\textwidth]{fig/288_gt.pdf}
        \caption[]%
        {\small scene $0288\_00$}
        \label{fig:288}
\end{subfigure}
\begin{subfigure}[b]{0.32\textwidth}  
        \centering 
        \includegraphics[width=\textwidth]{fig/616_gt.pdf}
        \caption[]%
        {\small scene $0616\_00$}    
        \label{fig:616}
\end{subfigure}
\begin{subfigure}[b]{0.32\textwidth}  
        \centering 
        \includegraphics[width=\textwidth]{fig/655_gt.pdf}
        \caption[]%
        {\small scene $0655\_00$}  
        \label{fig:655}
\end{subfigure}
\caption[]%
{Qualitative examples of typical scenes in the ScanNet dataset. Top: panoptic reconstructions with our method, bottom: ground truth. Colors of 'thing' instances can be different from ground truth because they are randomised.}    
\label{fig:examples}
\end{figure}

The visual quality of the reconstruction varies mostly based on the resolution of the voxel grid: the smaller the voxels are, the more detailed the meshes become. Voxel size seems to affect computational requirements the most as well. While image resolution could be reduced up to $0.1$ times the original after segmentation with voxels larger than $5~cm$ without affecting the quality of the outcome much, with smaller voxels there is a significant loss of quality. When image resolution is only reduced to half of the original, the quality is much better, but computational requirements grow respectively. In tests with $5~cm$ and $10~cm$ voxels point cloud resolution was reduced to $0.1$ times the original, and in tests with $2.4~cm$ voxels the resolution was reduced to $0.5$ times the original.

While our method seems to be roughly on par with the version of PanopticFusion applying CRF, the one without CRF seems to perform significantly worse than ours on PQ. This is further supported by the tests on our re-implementation of their method. While both PanopticFusion configurations perform better on 'stuff' classes and Segmentation Quality (SQ) than ours, they both seem to fall behind on 'thing' classes and Recognition Quality (RQ). This implies that while our method's segmentation seems to be slightly worse, our data association strategy seems to be performing better. Differences in SQ most likely originate from differences in the image segmentation approaches.

Some examples of panoptic reconstruction on typical scenes found in ScanNet are presented in Figure \ref{fig:examples}. Object instances seem to have been found and separated from one another correctly most of the time, and segmentations look pretty accurate, even though larger objects are often only seen partially in the video sequences.

Volumetric integration slows down a lot when voxel size is decreased taking on average $46$, $106$ or $1188$ milliseconds for $10$, $5$ and $2.4$ centimetre voxels respectively. Data-association with the Hungarian algorithm took on average $47$, $177$ or $1875$ milliseconds for the same sizes, while greedy matching took on average $53$, $247$ or $1949$ milliseconds. Most of this time is spent on computing the IoUs, which takes more time with larger amounts of voxels. Compared to the relations of these timings reported in the PanopticFusion paper, our association algorithm is somewhat slower than the original. Our implementation of the greedy algorithm is a bit slower than the Hungarian method, most likely because the segments need to be sorted every iteration. We can reach reconstruction quality similar to the version of PanopticFusion with CRF regularisation even when decimating point clouds, reducing voxel grid resolution and processing less frames. While the CRF is only run every $10$ seconds, it is reported to take around $4.5$ seconds and to slow down along increasing reconstruction size. For a real-time application to get the benefits of the regularisation, one would need to tolerate long delays during operation, thus we see it more useful as a post-processing tool for cleaning the reconstruction offline. 
