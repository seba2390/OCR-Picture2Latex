Because we represent the environment with a voxel volume, each target object is a set of voxels. Voxel size is assumed uniform across the whole scene. On the other hand -- while detections are originally point clouds -- they are transformed into sets of voxels in the same grid to make them consistent with the targets, and to reduce the number of points used to represent them. We employ a simple yet effective weighting strategy inspired by PanopticFusion \cite{panopticfusion} to determine a voxel's instance ID. For each timestep $t$ and voxel $v$ in the camera view the weight is computed as
\begin{equation}
W(t,v) =
    \begin{cases}
       W(t-1,v) + w(t, v) & l_d(t, v) = l_\tau(t, v), \\
       W(t-1,v) - w(t, v) & l_d(t, v) \neq l_\tau(t, v) \land W(t-1,v) \geq w(t, v) \\
       w_t(v, d) & l_d(t, v) \neq l_\tau(t, v) \land W(t-1,v) < w(t, v) 
    \end{cases}
\end{equation}
where $l_\tau(t, v) \in L$ is the persistent ID of a target instance currently associated with the voxel, $l_d(t, v) \in L$ a persistent ID of a matched detection segment and $w(t, v)$ the TSDF weight of the voxel. If a weight is reduced significantly -- \textit{i.e.} when $l_d(t, v) \neq l_\tau(t, v) \land W(t-1,v) < w(t, v)$, the last case above -- the voxel's persistent instance ID is reset as $l_\tau(t, v) := l_d(t, v)$. This way, only one ID has to be stored, and one does not need to keep track of all the instances associated with each voxel.

With an approach similar to \cite{panopticfusion}, each time a detected segment is matched with a target, the target's confidence scores are integrated in a probabilistic manner. However, because confidence is hard to define in panoptic segmentation, and EfficientPS therefore does not output confidence scores, each detection's confidence is assumed to be one for the detected class and zero for others. Therefore, the semantic class of a target ID $l \in L$ at timestep $t$ is determined by
\begin{equation}
    class(t,l) = 
    \begin{cases}
        argmax_c(n_c^l / n^l) & max_c(n_c^l / n^l) > \theta \\
        void                  & max_c(n_c^l / n^l) \leq \theta
    \end{cases}
\end{equation}
where $n_c^l$ is the number of times the target has been associated with the semantic class $c$, while $n^l$ total number of associations for the target across all classes. If the score is lower than a given threshold $\theta$, it will be assigned to the $void$ semantic class, representing objects that the segmentation model does not recognise.

Imprecise borders between segments in the image can cause problems when back-projecting them to 3D. For instance, if an object's segmentation surpasses the actual object's borders, a part of the detection can appear behind the object \textit{e.g.} on a wall. An accurate segmentation model can alleviate this issue, but nonetheless effective outlier rejection can make the system more robust to segmentation errors. Both confidence intervals on Gaussian probability distributions and clustering with the DBScan algorithm \cite{dbscan} were found to be effective. However, when the voxel weighting approach introduced in \cite{panopticfusion} was employed, the accuracy gains from both approaches were almost insignificant. Therefore, we believe that with a sufficiently precise segmentation model, simply introducing voxel weights as described above is enough. With noisier detections, however, one could benefit from the more complex approaches as well.