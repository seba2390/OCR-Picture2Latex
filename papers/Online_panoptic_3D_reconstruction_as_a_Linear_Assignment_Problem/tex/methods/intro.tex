
Figure \ref{fig:methods_intro_diagram} depicts a flow diagram of our panoptic 3D reconstruction pipeline. First, instance IDs and semantic classes are acquired from RGB images with a panoptic segmentation model. The IDs and classes are combined with depth information to form a panoptic point cloud, which is transformed to a global coordinate frame with camera pose information and quantised to a voxel grid. Voxel clusters corresponding to detections are matched with ones found in the current reconstruction. Afterwards, the new voxels are integrated into the volume. If there are no matches for some of the detected segments, new targets are generated accordingly.

\begin{figure}[t]

    \centering
    \includegraphics[width=\textwidth]{fig/metsem_system.pdf}
    \caption[]{A flow diagram of the reconstruction pipeline}
\label{fig:methods_intro_diagram}
\end{figure}

Similar to \cite{panopticfusion}, \cite{voxblox++} and \cite{interactive_3d_scenes}, we apply the popular Voxblox ROS library \cite{voxblox} to produce a TSDF voxel volume online. Our panoptic tracking system is built alongside the TSDF integrator, separate from the segmentation model and the integrator itself. Although our implementation isn't highly optimised, some parallelisation has been utilised to reach competitive speeds. We apply EfficientPS \cite{efficientps} for segmentation because of its efficient design and impressive performance on benchmark datasets.
