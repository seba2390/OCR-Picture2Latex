
\documentclass{article} % For LaTeX2e
\usepackage{iclr2023_conference,times}

% Optional math commands from https://github.com/goodfeli/dlbook_notation.
% \input{math_commands.tex}

\usepackage{hyperref}
\usepackage{url}
\usepackage{float}
\usepackage{caption}
% \usepackage{subfigure}
% \usepackage{subcaption}
% \usepackage{floatrow}
\usepackage{amssymb}
\usepackage{booktabs}
\usepackage[pdftex]{graphicx}

\title{StrucTexTv2: Masked Visual-Textual Prediction for Document Image Pre-training}
\iclrfinalcopy
\hypersetup{hidelinks}
% Authors must not appear in the submitted version. They should be hidden
% as long as the \iclrfinalcopy macro remains commented out below.
% Non-anonymous submissions will be rejected without review.

\author{Yuechen Yu\textsuperscript{\rm $\dag$}, Yulin Li\textsuperscript{\rm $\dag$}, Chengquan Zhang\textsuperscript{\rm $\dag$}\thanks{\rm $\dag$ Equal contribution. Correspondence to: Chengquan Zhang$<$zhangchengquan@baidu.com$>$.}, Xiaoqiang Zhang, Zengyuan Guo, \\
\textbf{Xiameng Qin, Kun Yao, Junyu Han, Errui Ding, Jingdong Wang} \\
%        \textbf{Xiaoqiang Zhang, Zengyuan Guo, Xiameng Qin,} \\
%        \textbf{Kun Yao, Junyu Han, Errui Ding, Jingdong Wang} \\
Department of Computer Vision Technology (VIS), Baidu Inc.\\
%\texttt{\{yuyuechen,liyulin03,zhangchengquan\}@baidu.com} \\
%\texttt{\{zhengxiaoqiang,guozengyuan,qinxiameng\}@baidu.com} \\
%\texttt{\{hanjunyu,dingerrui,wangjingdong\}@baidu.com} \\
% \texttt{\{yuyuechen,liyulin03,zhangchengquan,zhengxiaoqiang,guozengyuan,qinxiameng,yaokun01,hanjunyu,dingerrui,wangjingdong\}@baidu.com} \\
\texttt{\href{https://github.com/PaddlePaddle/VIMER/tree/main/StrucTexT}{https://github.com/PaddlePaddle/VIMER/tree/main/StrucTexT}
}
}
\makeatletter
\def\thanks#1{\protected@xdef\@thanks{\@thanks
        \protect\footnotetext{#1}}}
\makeatother
% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to \LaTeX{} to determine where to break
% the lines. Using \AND forces a linebreak at that point. So, if \LaTeX{}
% puts 3 of 4 authors names on the first line, and the last on the second
% line, try using \AND instead of \And before the third author name.

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

%\iclrfinalcopy % Uncomment for camera-ready version, but NOT for submission.
\begin{document}


\maketitle

\begin{abstract}
%In the paper, we present StrucTexTv2, an effective multi-modal pre-training model, by predicting visual-textual masks for document image representation. 
%In this paper, we present Masked Visual-Textual Prediction for document image pre-training, called StrucTexTv2. 
In this paper, we present StrucTexTv2, an effective document image pre-training framework, by performing masked visual-textual prediction. It consists of two self-supervised pre-training tasks: masked image modeling and masked language modeling, \emph{based on text region-level image masking}. The proposed method randomly masks some image regions according to the bounding box coordinates of text words. The objectives of our pre-training tasks are reconstructing the pixels of masked image regions and the corresponding masked tokens simultaneously. Hence the pre-trained encoder can capture more textual semantics in comparison to the masked image modeling that usually predicts the masked image patches. Compared to the masked multi-modal modeling methods for document image understanding that rely on both the image and text modalities, StrucTexTv2 models image-only input and potentially deals with more application scenarios free from OCR pre-processing. Extensive experiments on mainstream benchmarks of document image understanding demonstrate the effectiveness of StrucTexTv2. It achieves competitive or even new state-of-the-art performance in various downstream tasks such as image classification, layout analysis, table structure recognition, document OCR, and information extraction under the end-to-end scenario. 
%{Our code and models will be released soon.}



% In this paper, we present Masked Visual-Textual Prediction for document image representation pre-training, called StrucTexTv2. It comprises of two self-supervised pre-training tasks: Masked Image Modeling and Masked Language Modeling, \emph{based on region-level image masking}.
% Firstly, our approach randomly masks some words or texts and accordingly the corresponding image regions. Then, we pretrain the encoder by reconstructing not only the masked image regions but also the corresponding words, to learn visual-textual representations of document images. Thus, our approach considers more textual semantics than the masked image modeling approaches which usually predict the image patches or tokens. 
% Furthermore, compared to the masked multi-modal modeling methods for document image understanding, e.g., LayoutLM and StrucTexT, that need both the image and text inputs, our approach is able to model image-only input, and potentially can deal with more application scenarios free from OCR pre-processing. 
% %To our best knowledge, StrucTexTv2 is the first end-to-end self-supervised pre-training framework in the field of document image understanding that does not rely on multi-modality input. 
% To our best knowledge, StrucTexTv2 is the first end-to-end visual-textual representation pre-training framework for document image understanding that does not rely on multi-modality input. 
% % At the same time, for supporting the learning of visual-textual representations, we propose a novel Convolutional Vision Transformers (ConViT) and pretrain it on a large-scale unlabelled document images.  
% We perform the pre-trained model on several document image understanding tasks such as image classification, layout analysis, table structure recognition, document OCR, and end-to-end information extraction. Experimental results show that StrucTexTv2 achieves state-of-the-art performance. \emph{Our code and models will be released soon.}
\end{abstract}

\section{Introduction}
In the Document Artificial Intelligence, how to understand visually-rich document images and extract structured information from them has gradually become a popular research topic. Its main associated tasks include document image classification~\cite{icdar15cdip}, layout analysis~\cite{zhong2019publaynet}, form understanding~\cite{jaume2019funsd}, document OCR (also called text spotting)~\cite{li2017towards,lyu2018mask}, and end-to-end information extraction (usually composed of OCR and entity labelling phrase)~\cite{WangLJT0ZWWC21}, etc. To solve these tasks well, it is necessary to fully exploit both visual and textual cues. Meanwhile, large-scale self-supervised pre-training~\cite{li2021structurallm,appalaraju2021docformer,xu2020layoutlm,xu2020layoutlmv2,huang2022layoutlmv3,nips2021udoc} is a recently rising technology to enhance multi-modal knowledge learning of document images.
%In the current era of Intelligent Process Automation (IPA), how to make the computer understand visually-rich document images and extract structured OCR from them has become an important artificial intelligence technology. The common tasks of document image understanding include document image classification~\cite{icdar15cdip}, layout analysis~\cite{zhong2019publaynet}, document OCR (also called text spotting)~\cite{li2017towards,lyu2018mask}, end-to-end information extraction (usually consists of OCR and entity labeling)~\cite{jaume2019funsd}, and so on. To solve almost all these tasks well, it is necessary to take into account both the understanding of visual cues and semantic information.

\begin{figure*}
 \centering
 \includegraphics*[width=1.0\textwidth]{imgs/intro_fig_v6.pdf}
\caption{Comparisons with the main-stream pre-training models of document image understanding. (a) It shows the masked multi-modal modeling methods which input both OCR results and image embeddings. (b) The framework that inputs image-only embeddings is suitable for vision-dominated tasks like document image classification and layout analysis. (c) StrucTexTv2 learns visual-textual representations using only the information from images in the pre-training step and then optimizes various downstream tasks of document image understanding end-to-end.}
\label{fig:intro_arch}
\vspace{-1em}
\end{figure*}

There are two mainstream self-supervised pre-training frameworks for document image understanding. As illustrated in Fig.~\ref{fig:intro_arch}: (a) The first category is the masked multi-modal modeling such as the proposed pre-training tasks: MLM~\cite{devlin2018bert}, MVLM~\cite{xu2020layoutlmv2}, MM-MLM~\cite{appalaraju2021docformer} and MSM~\cite{nips2021udoc}, whose inputs mainly consists of OCR-extracted texts and image embeddings. The methods collect semantic information from text and image, depending heavily on front-end OCR engines with certain computing costs. Additionally, the two components of OCR engine and document understanding module are separately optimized, which is hard to ensure performance of the whole system. (b) The second category is the masked image modeling (MIM) that inherits the concept of vision-based self-supervised learning such as BEiT~\cite{bao2021beit}, SimMIM~\cite{Xie00LBYD022}, MAE~\cite{he2021masked}, CAE~\cite{chen2022context}, and DiT~\cite{li2022dit}, etc. MIM is a powerful image-only pre-training technique to learn the visual contextualized representations of document images. Because of the insufficient consideration of textual contents, MIM often applies to some vision-dominated tasks~\cite{li2022dit} like image classification, layout analysis and table detection. (c) To take full advantage of multi-modality knowledge on the basis of MIM, we propose the third pre-training framework by learning visual-textual representations with image-only inputs, to optimize the performance of document image understanding tasks in an end-to-end manner.
%There are two mainstream self-supervised frameworks for document image understanding. As shown in Fig.~\ref{fig:intro_arch}(a) and (b), the first is the multi-modal modeling methods like layoutLM series~\cite{xu2020layoutlm,xu2020layoutlmv2}, UDoc~\cite{nips2021udoc}, DocFormer~\cite{appalaraju2021docformer} and StrucTexT~\cite{li2021structext}, whose inputs consist OCR results and image embeddings.
%These methods are heavily dependent on the front-end OCR results, which makes it hard to generalize well to other tasks mentioned above.
%The second framework is masked image modeling methods which inherit the concept of vision-based self-supervised learning such as BEiT~\cite{bao2021beit}, MAE~\cite{he2021masked}, CAE~\cite{chen2022context}, DiT~\cite{li2022dit}, Donut~\cite{kim2021donut}, etc. The input modal is image-only, which is simple yet powerful to learn visual contextualized representations for some weak semantic-aware tasks like document image classification and layout analysis. However, there is still a large gap between pretrain tasks and downstream tasks which limits the application in different fields and affects the performance of applications. For example, DiT is likely to be deficient in semantic understanding for lack of guidance for text understanding tasks and Donut cannot handle tasks that require precise text location.
%Our work follows a new idea illustrated in Fig.~\ref{fig:intro_arch}(c), which attempts to learn visual-textual representations using only the information from images in the pre-training phase. The pre-trained model can be used as the backbone for almost strong and weak semantic-aware tasks of document image understanding.

 Due to the great disparities between vision and language, the existing document understanding methods either consider a single modality or introduce an OCR engine to capture textual content in advance. Researchers used text tokens as the input in language modeling, or selected fixed-size image patches as the granularity of vision pre-training tasks. However, the textual content is visually-situated in a document and extracted from the image. Thus, we propose the text region-level image masking scheme corresponding to document content to bridge vision modeling to language modeling with the shared representations.

%Thus, we propose the third pre-training framework as illustrated in Fig.~\ref{fig:intro_arch}(c) with performing a text region-based image masking scheme at document images, which can which can bridge vision modeling to language modeling with the shared representations.

 %We propose the third pre-training framework, which learns visual-textual representations using only the images, to optimize the performance of various downstream tasks in an end-to-end manner

%The self-supervised pretaining task based on the Mask-Predict mechanism~\cite{ghazvininejad2019mask} enables the encoder to learn rich contextualized representations, which is widely adopted in the two mainstream frameworks mentioned above. However, due to the large differences in modalities, in the past, researchers used text tokens as the basic granularity of the language-based self-supervised tasks like masked language modeling (MLM), while the granularity selection for vision-based self-supervised tasks often used predefined image patches(such as BEiT~\cite{bao2021beit} and MAE~\cite{he2021masked}). Roughly migrating the latter to document images is prone to a patch containing multiple text tokens or incomplete tokens. 
%%Returning to the analysis of document images, we find that text or word is the most basic and common unit, whether from the perspective of visual perception or natural language understanding.
%Therefore, we creatively perform text region-based image masking at the document images, enabling the encoder to learn rich visual-textual representations from dual parallel self-supervised tasks of image reconstruction and language modeling.
%%One of the most direct re-thinking, if we perform text region based image masking at the document image, can it make the encoder learn rich multi-modal knowledge from dual parallel sel-supervised tasks of image reconstruction and language modelling? This feasibility has been fully verified in our work.

This paper proposes StrucTexTv2, a novel multi-modal knowledge learning framework for document image understanding by performing text region-level image masking with dual parallel self-supervised tasks of image reconstruction and language modeling (as shown in Fig.~\ref{fig:method_arch}). First off, we adopt an off-the-shelf OCR toolkit to perform word-level text detection and text recognition on the pre-training dataset (IIT-CDIP Test Collection~\cite{sigir06cdip}). Next, we randomly mask some text word regions given the input images and fed them into the encoder. Finally, the pre-training objectives of StrucTexTv2 learn to reconstruct image pixels and text content of the masked words. In support of the proposed pre-training tasks, we introduce a new backbone network for StrucTexTv2. In particular, a CNN-based network with the RoI-Align~\cite{he2017mask} operation produces visual features for the masked regions. Inspired by ViBERTGrid~\cite{lin2021vibertgrid}, the backbone uses FPN~\cite{lin2017feature} to integrate features of CNN. The following transformer model enables capturing semantical and contextualized representations from the visual features. We evaluate and verify our pre-trained model in five tasks including document image classification, layout analysis, table structure recognition, document OCR, and end-to-end information extraction, all of which have achieved significant gains. The experimental results have also confirmed that the framework of StrucTexTv2 can construct fundamental pre-trained models for document image understanding.

The major contributions of our work can be summarized as following:
\begin{itemize}
 \item A novel self-supervised pre-training framework by performing text region-level document image masking, named StrucTexTv2, is proposed to learn visual-textual representations in an end-to-end manner.
 
 \item Superior performance in five downstream tasks demonstrates the effectiveness of the StrucTexTv2 pre-trained model in document image understanding.
\end{itemize}

\section{Related Work}
\noindent\textbf{Self-supervised Learning} 
Thanks to the development of self-supervised tasks and Transformer architectures, in the past few years, the computer vision (CV) and natural language processing (NLP) have achieved breakthroughs in knowledge learning from large-scale unlabeled data. In the domain of NLP, Masked Language Modeling (MLM) task has been widely used in pre-trained models~\cite{devlin2018bert,radford2018gpt} due to its efficiency and effectiveness. The MLM task randomly masks a set of text tokens of input and reconstructs them according to the context around the masked tokens. In the CV domain, taking a similar idea, Masked Image Modeling (MIM) has also been successfully verified. There are several variants of MIM, for example, BEiT~\cite{bao2021beit} and CAE~\cite{chen2022context} randomly mask a percentage of image patches and predict discrete image patch tokens learned by dVAE~\cite{ramesh2021dvae}. MAE~\cite{he2021masked} and SimMIM~\cite{Xie00LBYD022} take a simpler behavior, predicting RGB values of raw pixels by direct regression.

\noindent\textbf{Document pre-trained models} 
As stated above, the cutting-edge document pre-training models can be roughly separated into two categories: masked multi-modal modeling and masked image modeling approaches. The representative works of the former are LayoutLM series~\cite{xu2020layoutlm,xu2020layoutlmv2,huang2022layoutlmv3}, Docformer~\cite{appalaraju2021docformer}, UDoc~\cite{nips2021udoc} and StrucTexT~\cite{li2021structext}, they usually depend on a mature OCR engine to extract textual content from document images, and fed them to the encoder combined with image embeddings. These OCR-based methods promise excellent performance, but they are cumbersome to solve some vision-dominated tasks. The latter methods, such as DiT~\cite{li2022dit} almost following the idea of BEiT~\cite{bao2021beit}, directly use a general CV pre-training framework to learn on large-scale document image data. Due to the lack of guidance for text information, the model is likely to be deficient in semantic understanding. Recently, prompt-learning~\cite{LiuYFJHN23} has been studied for adapting pre-trained vision-language models. Those prompt-based methods~\cite{kim2021donut,DavisMPTWM22} directly generate textual output from documents and achieve competitive performance on downstream tasks.

In this paper, the proposed StrucTexTv2 is a new solution that integrates the advantages of CV and NLP pre-training methods in an end-to-end manner. Benefiting from image-only input of the encoder, our framework can avoid the interference of false OCR results compared with the OCR-based pre-trained models. For our pre-training, although the supervision labels partially come from OCR results, only the high-confidence words from OCR results are randomly selected. The impacts of the OCR quality on our pre-trained model is alleviated to a certain extent.
% In terms of model structure design, we adopt the structure of efficient fusion of CNN and Transformer which can not only capture local information efficiently, but also achieve a rapid decrease in the amount of calculation.

\section{Approach}
% The overall illustration of the proposed StrucTexTv2 framework is shown in Figure ~\ref{fig:method_arch}. Given an document image, StrucTexTv2 extract visual-textual representations through the novel Convolutional Vision Transformers (called ConViT), and pre-trained on a large scale document dataset using Masked Language Modeling (MLM) task and Masked Image Modeling (MIM) task. Finally, according to different down stream tasks, StrucTexTv2 uses various prediction heads to solve the problems.

\subsection{Model Architecture}
As illustrated in Fig.~\ref{fig:method_arch}, there are two main components of StrucTexTv2: a encoder network using FPN to integrate visual features and semantic features, and the pre-training framework containing two objectives: Mask Language Modeling and Mask Image Modeling.
%Fig.~\ref{fig:method_arch} illustrates the overall framework of our StrucTexTv2. There are two main components: a backbone network consist of Convolutional Neural Networks and Vision Transformers and two prediction branches, Mask Language Modeling and Mask Image Modeling.

% \noindent\textbf{Convolutional Neural Networks & Vision Transformers.} 
The proposed encoder consists of a visual extractor (CNN) and a semantic module (Transformer). Given an input document image, StrucTexTv2 extracts visual-textual representations through this backbone network. Specifically, the features of the last four down-sampled stages on CNN are extracted from the visual extractor. In the semantic module, following ViT~\cite{DBLP:journals/corr/abs-2010-11929} to handle 2D feature maps, the features of the last stage in CNN are flattened in patch-level and are linearly projected to obtain 1D sequence of patch token embeddings, which also serves as the effective input for the Transformer. A relative position embedding representing the token index is added to the token embeddings. 
% Each token embedding represents a image patch .
% Following ViT~\cite{DBLP:journals/corr/abs-2010-11929}, to handle 2D feature maps, the features of the last stage $\text{X} \in \mathbb{R}^{H \times W \times C}$ in CNN are firstly flattened 2D patches $\text{x}^{v} \in R^{N \times C}$, where $(H, W)$ is the resolution of the input feature maps, $C$ is the number of channels, and $N = H \times W$ is the resulting number of flattened patches, which also serves as the effective input sequence length for the Transformers. Then, the flattened patches are fed to a Transformer Network and enhanced by multiple Multi-head self-attention (MSA) and output enhanced features $\text{x}^{s} \in R^{N \times C}$. We reshape the shape of $\text{x}^{s}$ back to $H \times W \times C$ and up-sample the context feature map with the factors of 8$\times$. We adopt FPN strategy~\cite{lin2017feature} to merge visual features of different resolutions and then concatenate context features with them, deriving a feature map with 1/4 size of the input image. Finally, two successive 1$\times$1 convolutional layers are performed to take a further fusion.
Then, the standard Transformer receives the input token embeddings and outputs the semantic enhanced features. We reshape the output features back to context feature maps in the 2D visual space and up-sample the feature maps with the factors of 8. We adopt FPN strategy~\cite{lin2017feature} to merge visual features of different resolutions from CNN and then concatenate context feature maps with them, deriving a feature map with 1/4 size of the input image. Finally, a fusion network which consists of two successive 1$\times$1 convolutional layers is introduced to take a further multi-modal fusion.

\begin{figure*}[t]
 \centering
 \includegraphics[width=1.0\textwidth]{imgs/arch_fig_v2.pdf}
%\caption{The architecture and pre-training tasks of StrucTexTv2. It consists of a visual branch and a context branch. The features of document image are extracted by CNN and modulated by Transformer. The multi-model features of each text region are fed into MIM branch and MLM branch to reconstructs the text region in pixels and content, respectively.}
\caption{The overview of StrucTexTv2. Its encoder network consists of a visual extractor (CNN) and a semantic module (Transformer). Given a document image, the encoder extracts the visual feature of the whole image by CNN and obtains the semantic enhanced feature through a Transformer. Subsequently, a lightweight fusion network is utilized to generate the final representation of the image. With the help of ROI Alignment, the multi-modal feature of each masked text region is processed by the MIM branch and the MLM branch to reconstruct the pixels and text, respectively.}
\label{fig:method_arch}
\vspace{-0.5em}
\end{figure*}
%Word embeddings of MLM predications are then fed into MIM features to enhance "content" information of restored images.

% the minimum feature map has a size of 1/32 of the input image, which is then 2$\times$ up-sampled and concatenated with 1/16 feature map. Following above operation until 

% Similarly, the resulting feature map is further 2$\times$ up-sampled and concatenated with 1/8 feature map. This operation continues until we derive a feature map with 1/4 size of the input image. Additionally, the 1/32 feature map is up-sampled by 8$\times$ and 1/16 by 4$\times$, respectively, and which are further concatenated with the 1/4 feature map to capture higher level information. 


% For Mask Image Modeling, we further develop a Fully Convolutional Networks with Transpose Convolution to improve the document recovery performance. Specifically, on one hand, we first apply global average pooling to each text region features, generating the embeddings $Emb_{style}$ that mainly represent 'style' information. On the other hand, we encode the MLM result $P_{i}^{mlm}$ to $Emb_{content}$ through a embedding layer, representing 'content' information. Then we concatenate $Emb_{style}$ and $Emb_{content}$ and feed it to a Fully Convolutional Networks, generating the final image prediction $P_{i}^{mim}$. The Mask Image Modeling process is formulated as follow:

% \begin{align}
% \begin{aligned}
% Emb_{style} &= GlobalAveragePooling(ROI-Align(F_{fuse}, b_{i}))  \\
% Emb_{content} &= EmbeddingLayer(P_{i}^{mlm}) \\
% P_{i}^{mim} &= FCN(Concat(Emb_{style}, Emb_{content}))
% \end{aligned}
% \end{align}

% \subsection{Prediction Targets}
% Following MAE[], we predict the missing pixels of the masked text region by regression through Mask Image Modeling prediction head. For example, we firstly resize the spatial resolutions of each masked text-region to $64 \times 64$, that means each text-region's regression target is $12288=64 \times 64 \times 3$ standing for the RGB values of $64 \times 64$ pixels. We use the mean squared error(MSE) loss between the reconstructed text-regions and original text-regions in the pixel space. We compute the loss only on masked text-regions.

\subsection{Pre-training}

To enhance the document image understanding of StrucTexTv2, we perform pre-training on a large-scale document dataset IIT-CDIP Test Collection 1.0 dataset~\cite{sigir06cdip}. Unlike MAE~\cite{he2021masked}, BEiT~\cite{bao2021beit}, and DIT~\cite{li2022dit}, we do not use patch-level masking strategy for pre-training. Instead, we use a novel text region-level masking strategy and employ two self-supervised pre-training tasks to learn visual-textual representations. The subsequent experiment results suggest that the fine-grained text region-level masking strategy is more suitable than the coarse-grained patch-level masking strategy in document understanding.

\subsubsection{Task \#1: Masked Language Modeling}
To encourage the model to learn contextual representation, similar to BERT~\cite{devlin2018bert}, we mask a portion of the text region of the input document with RGB values [255, 255, 255] at random. We find that 30\% of masking ratio is the best choice in our experiments. According to the contextual information of the unmasked regions in a document, the encoder-decoder architecture learns to predict the indexes of text tokens (in a pre-defined vocabulary) of the masked text regions with cross-entropy loss. To avoid the interference of sub-words produced by WordPiece~\cite{song2020fast}, we only choose the first sub-word in each word for keeping the number of words in the document images. Notice that the Masked Language Modeling task in StrucTexTv2 does not require the text input, however, which is essential for the NLP domain.
%the content of text regions as input which are necessary for the NLP domain.

\noindent{\textit{Decoder for Task \#1.}}
For Mask Language Modeling, we employ an extremely 2-layer MLP as a decoder to project the encoding feature. The multi-model feature of each text region is extracted from the fused feature maps $\mathbf{F}_{fuse}$ by ROI-Align~\cite{ren2015faster}, which is computed as follows,
\begin{equation}
    \mathbf{P}_{i}^{mlm} = \textit{MLP}(\textit{ROI-Align}(\mathbf{F}_{fuse}, b_{i})),
\end{equation}
where $b_{i}$ is the bounding box of the $i$th text region, and the $\mathbf{P}_{i}^{mlm}$ is optimized by cross-entropy loss with 30,522 token categories.

\subsubsection{Task \#2: Masked Image Modeling}
In MAE~\cite{he2021masked}, BEiT~\cite{bao2021beit}, and SimMIM~\cite{Xie00LBYD022}, patch-level Masked Image Modeling has shown strong potential in representation learning. However, in the document understanding domain, patch-level feature learning is too coarse to represent the details of text or word region. Therefore, we introduce a text region-level visual representation learning task called Masked Image Modeling to enhance document representation and understanding. Instead of classifying the classes defined by tokenization like LayoutLMv3 and BEiT, we regress the raw pixel values of the masked text region with mean square error loss following SimMIM and MAE. Specifically, we mask the rectangle text regions and predict the RGB values of missing pixels, leading to significant improvement for performance in representation learning.

\noindent{\textit{Decoder for Task \#2.}} 
%It is worth noticing that MLP is difficult to achieve impressive inpainting results in Mask Image Modeling task. Thus, we 
We develop a Fully Convolutional Network (FCN) with Transpose Convolution to carry out the document image reconstruction of masked text regions. Specifically, we apply the global average pooling to aggregate each text region's feature and generate the embedding $\mathbf{Emb}_{style}$ that mainly represents the visual "style" for each masked text region. To strengthen its text information, we encode the MLM prediction $\mathbf{P}_{i}^{mlm}$ to $\mathbf{Emb}_{content}$ by using an embedding layer, denoting the "content" knowledge. At last, we concatenate $\mathbf{Emb}_{style}$ and $\mathbf{Emb}_{content}$ and feed it to a FCN, generating the final restored image prediction $\mathbf{P}_{i}^{mim}$. The procedure of Mask Image Modeling can be formulated as follows,
\begin{equation}
\mathbf{Emb}_{style} = \textit{GAP}(\textit{ROI-Align}(\mathbf{F}_{fuse}, b_{i})),
\end{equation}
\begin{equation}
\mathbf{Emb}_{content} = \textit{EmbeddingLayer}(\mathbf{P}_{i}^{mlm}),
\end{equation}
\begin{equation}
\mathbf{P}_{i}^{mim} = \textit{FCN}(\textit{Concat}(\mathbf{Emb}_{style}, \mathbf{Emb}_{content})),
\end{equation}
where GAP is the global average pooling operator. In MIM, we follow MAE and predict the missing pixels of masked text regions. For example, we resize the spatial resolutions of each masked text region to fixed 64$\times$64, and each text-region's regression target is 12,288=64$\times$64$\times$3 pixels of RGB values. The $\mathbf{P}_{i}^{mim}$ is optimized by MSE loss in the pre-training phrase.

\subsection{Downstream Tasks}

The StrucTexTv2 pre-training scheme contributes to a visual-textual representation with input of image-only. The multi-modal representation is available to model fine-tuning and profits numerous downstream tasks.

\subsubsection{Task \#1: Document Image Classification}
Document image classification aims to predict the category of each document image, which is one of the fundamental tasks in office automation. For this task, we downsample the output feature maps of backbone network by four 3$\times$3 convolutional layers with stride 2. Next, the image representation is fed into the final linear layer with softmax to predict the class label.
%a factor of four with stride-two 3$\times$3 convolutional layers

\subsubsection{Task \#2: Document Layout Analysis}
Document layout analysis aims to identify the layout components of document images by object detection. Following DiT, we leverage Cascade R-CNN~\cite{cai2018cascade} as the detection framework to perform layout element detection and replace its backbone with StrucTexTv2. Thanks to the multi-scale context design of backbone networks, four resolution-modifying features (P2$\sim$P5) of FPN fusion layers on backbone networks are sent into the iterative structure heads of the detector.

\subsubsection{Task \#3 Table Structure Recognition}
Table structure recognition aims to recognize the internal structure of a table which is critical for document understanding. Specifically, We employ Cascade R-CNN for cell detection in our table structure recognition framework while replacing the feature encoder with backbone networks. Since some table images are collected by cameras and many cells are deformed, we modify the final output of Cascade R-CNN to the coordinate regression of four vertexes of cells.

\subsubsection{Task \#4: Document OCR}
We tend to read the text in an end-to-end manner based on StrucTexTv2.
% Document OCR is also known as Text Reading and which can be formally divided into two sub-task: text detection and text recognition. 
Our OCR method consists of both the word-level text detection and recognition modules. They share the features of backbone networks and are connected by ROI-Align operations. The text detection module adopts the standard DB~\cite{liao2020real} algorithm, which predicts the binarization mask for word-level bounding boxes. Similar to NRTR~\cite{sheng2019nrtr}, the text recognition module is composed of multi-layer Transformer decoders to predict character sequences for each word.

\subsubsection{Task \#5: End-to-end Information Extraction}
The aim of the task is to extract entity-level content of key fields from given documents without predefined OCR information. We evaluate the StrucTexTv2 model based on the architecture of document OCR and devise a new branch for semantic entity extraction. Concretely, another DB detection is developed to identify the entity bounding boxes. An additional MLP block is performed with the ROI features to classify entity label. These bounding boxes are utilized for word grouping to merge the text content from Task \#4. At length the key information is obtained by grouping words according to the reading order.
% Recent OCR-based approaches of information extraction receiving the official annotations of text bounding-box and transcript promise remarkable performance. However, they all focus on the design of information extraction and neglect the computational costs and OCR error propagation in a documental system with text reading and key information parsing.

\section{Experiments}
% In order to demonstrate the effectiveness of StrucTexTv2, we pretrain the model on the large-scale public dataset and conduct extensive experiments on five downstream tasks mentioned above. Additionally, we provide ablation study to explore the impacts of each strategy and component.

\subsection{Datasets} 
\noindent\textbf{Pre-training Data} By following DiT~\cite{li2022dit}, we pretrain StrucTexTv2 on the IIT-CDIP Test Collection 1.0 dataset~\cite{sigir06cdip}, whose 11 million multi-page documents are split into single pages, totally 42 million document images. 
%In MIM task, following MAE~\cite{He_2022_CVPR}, the missing pixels of the masked text region is predicted by regression through Mask Image Modeling prediction head. 
% For example, we firstly resize the spatial resolutions of each masked text-region to $64 \times 64$, that means each text-region's regression target is $12288=64 \times 64 \times 3$ standing for the RGB values of $64 \times 64$ pixels. 

\noindent\textbf{RVL-CDIP}~\cite{icdar15cdip} contains 400,000 grayscale document images in 16 classes, with 25,000 images per class. 
% The 16 classes include {letter, form, email, handwritten, advertisement, scientific report, scientific publication, specification, file folder, news article, budget, invoice, presentation, questionnaire, resume, memo}. There are 320,000 training images, 40,000 validation images, and 40,000 test images.
We adopt RVL-CDIP as the benchmark to carry out experiments on document classification task. Average classification accuracy is used evaluate model performance.

\noindent\textbf{PubLayNet}~\cite{zhong2019publaynet} consists of more than 360,000 paper images built by automatically parsing PubMed XML files. The five typical document layout elements (text, title, list, figure, and table) are annotated with bounding boxes. Mean average precision (mAP) @ intersection over union (IOU) is used as the evaluation metric of document layout analysis.

\noindent\textbf{WTW}~\cite{long2021parsing} covers unconstrained table in natural scene, requiring table structure recognizer to have both discriminative and generative capabilities. It has a total of 14581 images in a wide range of real business scenarios and the corresponding full annotation (including cell coordinates and row/column information) of tables. 

\noindent\textbf{FUNSD}~\cite{jaume2019funsd} is a form understanding dataset that contains 199 forms, which refers to extract four predefined semantic entities (questions, answers, headers, and other) and their linkings presented in the form. We focus on two tasks of document OCR and end-to-end information extraction on FUNSD. For evaluation, we compute the normalized Levenshtein similarity (1-NED) between the predictions and the ground truth.


\subsection{Implementation Details}
\noindent\textbf{Pre-training on IIT-CDIP} The proposed encoder network of StrucTexTv2 is composed mainly of the CNN and Transformer. To balance efficiency and effectiveness, $\rm StrucTexTv2_{Small}$ consists of ResNet-50 and 12-layer Transformers (128 hidden size and 8 attention heads) and introduces only 28M parameters. A larger version of $\rm StrucTexTv2_{Large}$ is set up as ResNeXt-101~\cite{xie2017aggregated} and 24-layer Transformers (768 hidden size and 8 attention heads), which total parameters are 238M. We use the networks trained with ImageNet~\cite{deng2009imagenet} as the initialization of CNNs. The Transformers are initialized from the language models~\cite{SunWLFTWW20}. 
$\rm StrucTexTv2_{Small}$ and $\rm StrucTexTv2_{Large}$ take 17 hours and 52 hours to train one epoch of the IIT-CDIP data, respectively. The whole pre-training phase takes nearly a week with 32 Nvidia Tesla 80G A100 GPUs.

\noindent\textbf{Fine-tuning on RVL-CDIP} We evaluate StrucTexTv2 for document image classification. We fine-tune the model on RVL-CDIP for 20 epochs with cross-entropy loss. The learning rate is set for 3e-4 and the batch size is 28. The input images are resized to 960$\times$960 and maintain its aspect ratio. We use label smoothing=0.1 in the loss function. Besides, the data augmentation methods such as CutMix and MixUp with 0.3 probability are applied on the training phase.

\noindent\textbf{Fine-tuning on PubLayNet} We evaluate on the validation set of PubLayNet for document layout analysis. We fine-tune the Cascade R-CNN and initialize the backbone with our pre-trained model. The detector is trained 8 epochs with Momentum optimizer and a batch size of 8. The learning rate is set to 1e-2, while it decays to 1e-3 on 3 epoch and decays 1e-4 on 6 epoch. We use random resized cropping to augment the training images while the short edges does not exceed 800 pixels.

\noindent\textbf{Fine-tuning on WTW} We conduct experiments on WTW for table structure recognition. We also employ Cascade R-CNN to detect the cells of table whose backbone is replaced by pre-trained StrucTexTv2. We fine-tune our model end-to-end using ADAM~\cite{kingma2014adam} optimizer for 20 epochs with a batch size of 16 and a learning rate of 1e-4. The input images are resized to 640 $\times$ 640 after random scaling and the long size being resized to 640 pixels.

\noindent\textbf{Fine-tuning on FUNSD} On account of the full annotations, both the document OCR and end-to-end information extraction tasks are measured on FUNSD. We set the text recognition network as a 6-layers Transformer and fine-tune the whole model for 1200 epochs with a batch size of 32. We follow a cosine learning rate policy and set the initial learning rate to 5e-4. Extra position embeddings are appended to roi-features and we pass it to each layer of decoders. The training losses except DB detector are the cross-entropy function. Additionally, the same loss is estimated for each decoder layer in the text recognition module for better convergence training.

\subsection{Comparisons with the State-of-the-art}
To investigate the effect of visual-textual representations, we benchmark StrucTexTv2 with several state-of-the-art techniques on different downstream tasks. Since only small datasets (149 training documents on FUNSD) and to avoid exceeding GPU memory (a tremendous number of table cells on WTW), we only evaluate $\rm StrucTexTv2_{Small}$ on the FUNSD and WTW datasets.

% such as BEiT~\cite{bao2021beit}, LayoutLM*~\cite{xu2020layoutlm,xu2020layoutlmv2,huang2022layoutlmv3},  StrucTexT~\cite{li2021structext}, DiT~\cite{li2022dit}, DocFormer~\cite{appalaraju2021docformer} and so forth

\begin{table}[t]
% \vspace{-1em}
\caption{Performance comparisons on the RVL-CDIP dataset. We report classification accuracy on the test set. $T$ and $I$ denote the text and image modality of input. The proposed StrucTexTv2 achieves a comparable accuracy to the state-of-the-art models with image-only input.}
\vspace{-1em}
\label{RVL-CDIP}
\begin{center}
\setlength{\tabcolsep}{8pt}{
\renewcommand{\arraystretch}{1.15}
\begin{tabular}{l c c c}
\toprule
\textbf{Methods} & \textbf{Modalities} & \textbf{Accuracy} & \textbf{\#Param.} \\
\midrule
$\rm BERT_{Base}$~\cite{devlin2018bert} & $T$ & 89.81\% & 110M \\
$\rm BERT_{Large}$~\cite{devlin2018bert} & $T$ & 89.92\% & 340M \\
$\rm StructuralLM_{Large}$~\cite{li2021structurallm} & $T$ & 96.08\% & 355M \\
\midrule
SelfDoc~\cite{li2021selfdoc} & $ V + T $ & 92.81\% & - \\
$\rm LayoutLM_{Large}$~\cite{xu2020layoutlm} & $V + T$ & 94.43\% & 390M \\
UDoc~\cite{nips2021udoc} & $V + T$ & 95.05\% & 272M \\
$\rm TILT_{Large}$~\cite{PowalskiBJDPP21} & $V  + T$ & 95.52\% & 780M \\
$\rm LayoutLMv2_{Large}$~\cite{xu2020layoutlmv2} & $V + T$ & 95.64\% & 426M \\
$\rm LayoutLMv3_{Large}$~\cite{huang2022layoutlmv3} & $V + T$ & 95.93\% & 368M \\
$\rm \textbf{DocFormer}_{Base}$~\cite{appalaraju2021docformer} & $V + T$ & \textbf{96.17\%} & 183M \\
\midrule
$\rm BEiT_{Base}$~\cite{bao2021beit} & $V$ & 91.09\% & 87M \\
$\rm DiT_{Base}$~\cite{li2022dit} & $V$ & 92.11\% & 87M \\
$\rm DiT_{Large}$~\cite{li2022dit} & $V$ & 92.69\% & 304M \\
$\rm \textbf{StrucTexTv2}_{Small}$ & $V$ & 93.40\% & \textbf{28M} \\
$\rm StrucTexTv2_{Large}$ & $V$ & 94.62\% & 238M \\
\bottomrule
\end{tabular}}
\end{center}
\vspace{-1em}
\end{table}

\noindent\textbf{RVL-CDIP} As the Tab.~\ref{RVL-CDIP} shows, OCR-based approaches such as DocFormer~\cite{appalaraju2021docformer} with multi-modal modeling takes superior performance on RVL-CDIP. Compared with image-only methods, it relies on stand-alone OCR systems which are low efficiency for practical applications. Our $\rm StrucTexTv2_{Small}$ achieves an accuracy of 93.40\% and outperforms previous image-only methods with a much smaller model size. Besides, $\rm StrucTexTv2_{Large}$ drives an accuracy of 94.62\%, which brings a 1.93\% improvement over $\rm DiT_{Large}$~\cite{li2022dit}. Intuitively, the results show that StrucTexTv2 is effective for semantic understanding on document image classification.

\noindent\textbf{PubLayNet} The experiment results on PubLayNet are presented in Tab.~\ref{PubLayNet}. 
It is observed that StrucTexTv2 achieves new state-of-the-art performance of 95.4\% and 95.5\% mAP scores for both small and large settings. $\rm StrucTexTv2_{Small}$ beats even $\rm LayoutLMv3_{Base}$~\cite{huang2022layoutlmv3} (the result of $\rm LayoutLMv3_{Large}$ is not released in the paper) which contains multi-modal inputs by 0.3\%. We suggest that our dual-modal pre-training tasks can learn rich visual-textual representations of document images and performs excellently in confusing situations. Notably, $\rm StrucTexTv2_{Large}$
gets 0.1\% mAP gain over that on $\rm StrucTexTv2_{Small}$. We attribute this subtle improvement to the over-fitting to a certain extent with increasing model size.

\noindent\textbf{WTW} Tab.\ref{WTW} shows quantitative results of table structure recognition on the WTW dataset. StrucTexTv2 achieves the physical 78.9\% F1-score among all published methods. We reconstruct the table structure based on the detection results of table cells. The superior performance of StrucTexTv2 largely due to the proposed pre-training framework.

\begin{table}[t]
\begin{minipage}[t]{0.4\linewidth}
\caption{Performance comparisons on the PubLayNet validation set. The mAP @ IOU [0.50:0.95] is used as the metric.}
\label{PubLayNet}
\vspace{-1em}
\begin{center}
\setlength{\tabcolsep}{1mm}{
\renewcommand{\arraystretch}{1.1}
\begin{tabular}{l c}
\toprule
\textbf{Methods} & \textbf{mAP} \\
\midrule
$\rm BEiT_{Base}$ & 93.1\% \\
UDoc & 93.9\% \\
$\rm DiT_{Large}$ & 94.9\% \\
$\rm LayoutLMv3_{Base}$ & 95.1\% \\
$StrucTexTv2_{Small}$ & 95.4\%  \\
$\rm \textbf{StrucTexTv2}_{Large}$ & \textbf{95.5\%}  \\
\bottomrule
\end{tabular}}
\end{center}
\end{minipage}
\hspace{0.02\linewidth}
\begin{minipage}[t]{0.57\linewidth}
\caption[0.9\linewidth]{Performance comparisons on the WTW dataset. The F1-Score is used to measure the accuracy of cell coordinate when IOU=0.9.}
\label{WTW}
\vspace{-1em}
\begin{center}
\setlength{\tabcolsep}{1mm}{
\renewcommand{\arraystretch}{1.1}
\begin{tabular}{l c}
\toprule
\textbf{Methods} & \textbf{F1-Score} \\
\midrule
Split+Heuristic~\cite{tensmeyer2019deep} & 3.4\% \\
Faster-RCNN~\cite{ren2015faster}  & 64.4\% \\
TridenNet~\cite{li2019scale} & 65.0\% \\
CenterNet~\cite{duan2019centernet} & 73.1\% \\
Cycle-CenterNet~\cite{long2021parsing}  & 78.3\%  \\
$\rm \textbf{StrucTexTv2}_{Small}$ & \textbf{78.9\%} \\
\bottomrule
\end{tabular}}
\end{center}
\end{minipage}
\vspace{-0.5em}
\end{table}

\noindent\textbf{FUNSD} We evaluate $\rm StrucTexTv2_{Small}$ on both the document OCR and end-to-end information extraction tasks. As shown in Tab.~\ref{FUNSD}, $\rm StrucTexTv2_{Small}$ achieves outstanding performance, 84.1\% 1-NED for document OCR and 55.0\% 1-NED for information extraction. Significantly, the whole network is end-to-end trainable. Compared to StrucTexT~\cite{li2021structext} and LayoutLMv3 with need of separately stage-wise training strategies, our model alleviates the error propagation in a documental system with key information parsing.

\begin{table}[ht]
\caption{Performance comparisons on FUNSD. We present the Normalized Edit Distance (1-NED) for the word-level document OCR and the entity-level information extraction. The \textbf{*} denotes a multi-stage process in which the methods are applied using our OCR results and entity boxes for word grouping in information extraction.}
\label{FUNSD}
% \vspace{-1em}
\begin{minipage}[c]{0.53\linewidth}
\begin{center}
\setlength{\tabcolsep}{1mm}{
\renewcommand{\arraystretch}{1.1}
\begin{tabular}{l c}
\multicolumn{2}{c}{The results of Document OCR} \\
\toprule
\textbf{Methods} & \textbf{1-NED} \\
\midrule
% Tesseract~\cite{smith2007overview} & 3.4\% \\
DB+NRTR~\cite{sheng2019nrtr} & 73.8\% \\
Google Vision~\cite{goolevision2019} & 76.4\% \\
$\rm \textbf{StrucTexTv2}_{Small}$ & \textbf{84.1\%} \\
\bottomrule
\end{tabular}}
\end{center}
\end{minipage}
\begin{minipage}[c]{0.45\linewidth}
\begin{center}
\setlength{\tabcolsep}{1mm}{
\renewcommand{\arraystretch}{1.1}
\begin{tabular}{l c} 
\multicolumn{2}{c}{The results of Information Extraction} \\
\toprule
\textbf{Methods} & \textbf{1-NED} \\
\midrule
$\rm StrucTexT_{Base}^{*}$ & 46.8\% \\
$\rm LayoutLMv3_{Base}^{*}$ & 53.1\% \\
$\rm \textbf{StrucTexTv2}_{Small}$ & \textbf{55.0\%} \\
\bottomrule
\end{tabular}}
\end{center}
\end{minipage}
\vspace{-0.5em}
\end{table}

\subsection{Ablation Study}
To further examine the different contributions of StrucTexTv2, we conduct several ablation experiments, such as document layout analysis on PubLayNet, document image classification on RVL-CDIP, and end-to-end information extraction on FUNSD. All models in ablation study are pre-trained for 1 epoch with only 1M documents sampled from the IIT-CDIP dataset .

\begin{table}[ht]
\caption{The ablation study on pre-training tasks and different encoding structures.}
\label{table:Ablation_BackboneArchitectures_Tasks}
\vspace{-1em}
\begin{center}
\setlength{\tabcolsep}{1mm}{
\renewcommand{\arraystretch}{1.1}
\begin{tabular}{l l l c c c c}
\toprule
\textbf{Model} & \textbf{\#Param.} & \textbf{MIM} & \textbf{MLM} & \textbf{RVL-CDIP} & \textbf{PubLayNet} \\
\midrule
$\rm SwinT_{base} +FPN $ & 176M & \checkmark & \checkmark   & 86.6\%  & 92.2\% \\
$\rm ViT_{base} + FPN $   & 116M & \checkmark & \checkmark   & 88.6\%  & 93.2\% \\

$\rm {StrucTexTv2}_{Large}$     & 238M & \checkmark & \checkmark & \textbf{94.1\%}  & \textbf{95.6\%} \\
\midrule
$\rm {StrucTexTv2}_{Small}$ & 28M & \checkmark &      & 91.8\%  & 94.1\% \\
$\rm {StrucTexTv2}_{Small}$ & 28M & & \checkmark  & 92.0\%  & 94.5\% \\
$\rm {StrucTexTv2}_{Small}$ & 28M & \checkmark & \checkmark  & \textbf{92.5\%}  & \textbf{94.9\%} \\
\bottomrule
\end{tabular}}
\end{center}
\vspace{-0.5em}
\end{table}

\noindent\textbf{Encoding Structures} In this study, we evaluate the impact of encoding structures by replacing the backbone of StrucTexTv2 with ViT~\cite{DBLP:journals/corr/abs-2010-11929} and SwinTransformer~\cite{liu2021swin}. As shown in Tab.~\ref{table:Ablation_BackboneArchitectures_Tasks}, the proposed network of $\rm StrucTexTv2_{Small}$ even achieves the better results of 92.5\% accuracy and 94.9\% mAP on RVL-CDIP and PubLayNet, respectively. The performance of two benchmarks dropped by 3.9\% accuracy and 1.7\% mAP with the $\rm ViT_{Base}$. Replaced with the $\rm SwinTransformer_{Base}$, the degradation is more obvious. In addition, $\rm StrucTexTv2_{Large}$ improves performance by 1.6\% on RVL-CDIP and 0.7\% on PubLayNet.

\noindent\textbf{Pre-training Tasks} In this study, we identify the contributions of different pre-training tasks. As shown in the bottom of Tab.~\ref{table:Ablation_BackboneArchitectures_Tasks}, the MIM-only pre-trained model achieves an accuracy of 91.8\% on RVL-CDIP and an mAP of 94.1\% on PubLayNet. The MLM-only pre-trained model achieves 92.0\% and 94.5\% for the two datasets. The MLM and MIM can jointly exploit the multi-modal feature representations in StrucTexTv2. By combining both the proposed pre-training tasks, the accuracy is improved to 92.5\% on RVL-CDIP and the mAP achieves 94.9\% on PubLayNet.

\begin{table}[ht]
\begin{minipage}[t]{0.35\linewidth}
\caption{The ablation study on the influence of masking ratios (MR.) with StrucTexTv2$_{Small}$ on RVL-CDIP and PubLayNet.}
\label{table:Ablation_MaskingRates}
\vspace{-1em}
\begin{center}
\setlength{\tabcolsep}{1mm}{
\renewcommand{\arraystretch}{1.1}
\begin{tabular}{l c c}
\toprule
\textbf{MR.}  & \textbf{RVL-CDIP} & \textbf{PubLayNet} \\
\midrule
0.15 & 92.1\%  & 94.7\% \\
\textbf{0.30} & \textbf{92.5\%}  & \textbf{94.9\%} \\
0.45 & 91.7\%  & 94.8\% \\
0.60 & 92.4\%  & 94.8\% \\
\bottomrule
\end{tabular}}
\end{center}
\end{minipage}
\hspace{0.02\linewidth}
\begin{minipage}[t]{0.62\linewidth}
\caption{Consumption analysis on RVL-CDIP. We re-implement $\rm LayoutLMv3^*_{Base}$ with open-source OCR engines to provide text. {$^\dag$} denotes the cost of OCR process. All the models are inferred on a NVIDIA Tesla 80G A100.}
\label{table:Ablation_Consumption}
\vspace{-1em}
\begin{center}
\setlength{\tabcolsep}{1mm}{
\renewcommand{\arraystretch}{1.1}
\begin{tabular}{l l c c}
\toprule
\textbf{Methods} & \textbf{OCR} & \textbf{GPU(MB)} & \textbf{Time(ms)}\\
\midrule
$\rm {LayoutLMv3}^*_{Base}$ & Tesseract & 2,184+n/a\textsuperscript{\rm $\dag$} & 22+1,105\textsuperscript{\rm $\dag$} \\
$\rm {LayoutLMv3}^*_{Base}$ & PaddleOCR & 2,184+3,450\textsuperscript{\rm $\dag$} & 22+252\textsuperscript{\rm $\dag$} \\
$\rm {StrucTexTv2}_{Small}$ & None & 2,276 & 56 \\
$\rm {StrucTexTv2}_{Large}$ & None & 4,058 & 284 \\

\bottomrule
\end{tabular}}
\end{center}
\end{minipage}
\end{table}

\noindent\textbf{Masking Ratios} We investigate the effect of training with different masking ratios. As shown in Tab.\ref{table:Ablation_MaskingRates}, by replacing the masking ratio with 0.15, 0.30, 0.45 and 0.60, the accuracy of RVL-CDIP is 92.1\%, 92.5\%, 91.7\% and 92.4\%, respectively. We also report the results on PubLayNet, the mAP of PubLayNet is 94.7\%, 94.9\%, 94.8\%, and 94.8\%, respectively. It suggests that the best masking ratio of our pre-training tasks is 0.30. At the same time, It also suggests that the performance of downstream tasks is less sensitive to the selection of masking ratio.

\noindent\textbf{Consumption Analysis} As shown in Tab.~\ref{table:Ablation_Consumption}, $\rm StrucTexTv2_{Small}$ consumes 56ms and 2,276MB GPU memory to infer one image on RVL-CDIP, while $\rm LayoutLMv3_{Base}$ spends more GPU memory or time with different OCR engines. It is observed that the OCR process of the two-stage method accounts for the vast majority of computation overhead. Thus, our OCR-free framework can achieve a better trade-off between performance and efficiency.

\noindent\textbf{Masking Strategies} The impact of adjusting text region-level masking to patch-level masking is evaluated in Tab.\ref{table:Ablation_MaskingStrategies}. The performance drops a 4.2\% accuracy score on RVL-CDIP and a 1.0\% mAP on PubLayNet, which demonstrates the effectiveness of the proposed Text-Region masking strategy.

% \setlength{\tabcolsep}{4pt}
\begin{table}[ht]
% \vspace{-0.5em}
\caption{Comparison between performance of different masking strategies on RVL-CDIP and PubLayNet. The model is only pre-trained with the MIM task.}
\label{table:Ablation_MaskingStrategies}
\vspace{-1em}
\begin{center}
\setlength{\tabcolsep}{1mm}{
\renewcommand{\arraystretch}{1.1}
\begin{tabular}{l c c c }
\toprule
\textbf{Model} & \textbf{Masking strategy} & \textbf{RVL-CDIP} & \textbf{PubLayNet} \\
\midrule
% {SwinT}_{base} & 176M & text-region & 82.4\% & 92.2\% \\
% {ViT}_{base} & 116M & text-region & 88.6\% & 93.2\% \\
% \midrule
$\rm {StrucTexTv2}_{Small}$ & patch-level & 87.4\% & 93.1\% \\
$\rm {StrucTexTv2}_{Small}$ & region-level & \textbf{91.8}\% & \textbf{94.1\%} \\
\bottomrule
\end{tabular}}
\end{center}
\vspace{-1em}
\end{table}

%Notice that we both use base configurations of ViT and SwinTransformer with similar parameters for a fair comparison.
%With ViT backbone architecture, the accuracy on RVL-CDIP is 88.6\% and with SwinTransformer backbone architecture, the accuracy on RVL-CDIP is 82.4\%. The proposed ConViT get the best 92.5\%  accuracy on RVL-CDIP, suggesting that the proposed ConViT backbone architecture is the primary contributors to the performance boost.

%In Tab.\ref{table:Ablation_MaskingStrategies} we evaluate the the impact of text-region to patch-level on masking strategy. It is observed that the performance drops from 91.8\% to 87.4\% accuracy score on RVL-CDIP and the performance drops from 94.1\% to 93.1\% mAP on PubLayNet. This suggests that the proposed Text-Region masking strategy is critical to document classification.
%Notice that we both use ViT without any features fusion strategy as the backbone for a fair comparison.

% \setlength{\tabcolsep}{4pt}
% \begin{table}[ht]
% \caption{Ablation for Different Image Resolution.}
% \label{table:Ablation_ImageResolution}
% \vspace{-0.5em}
% \begin{center}
% \setlength{\tabcolsep}{1mm}{
% \renewcommand{\arraystretch}{1.1}
% \begin{tabular}{c c c c}
% \toprule
% \textbf{Image Resolution} & \textbf{Pretrain Tasks} & \textbf{RVL-CDIP} & \textbf{PubLayNet} \\
% \midrule
% 224*224       & MLM+MIM     & 89.5\%  & 85.9\% \\
% 512*512       & MLM+MIM     & 90.4\%  & 92.1\% \\
% 960*960       & MLM+MIM     & 92.5\%  & 94.9\% \\
% \bottomrule
% \end{tabular}}
% \end{center}
% \vspace{-0.5em}
% \end{table}

% \noindent\textbf{Ablation for Different Image Resolution} In this study, we investigate the impact of training with different image resolution. As shown in \ref{table:Ablation_ImageResolution}, by replacing the resolution of input images with $224*224$, $512*512$ and $960*960$, the accuracy of RVL-CDIP is 89.5\%, 90.4\%, and 92.5\%, respectively.  We also report the results on PubLayNet, the F1-Score of PubLayNet is 94.9\%, 92.1\%, and 85.9\%, respectively. This suggests that the best image resolution of our pretrain tasks is $960*960$. At the same time, It also suggests that the performance of document layout analysis is more sensitive to the selection of the resolution of input images.

\section{Conclusion}
This work successfully explores a novel pre-training framework named StrucTexTv2 to learn visual-textual representations for document image understanding with image-only input. By performing text region-based image masking, and then predicting both corresponding visual and textual content, the proposed encoder can benefit from large-scale document images efficiently. Extensive experiments on five document understanding tasks demonstrate the superiority of StrucTexTv2 over state-of-the-art methods, especially an improvement in both efficiency and effectiveness.

\bibliography{iclr2023_conference}
\bibliographystyle{iclr2023_conference}

% Moreover, this work has far-reaching influence and inspiration, making the visual-textual representation learning more straightforward and making the pre-trained model easier to use.

\appendix
\clearpage
\section{Appendix}
\subsection{StrucTexTv2 for Form Understanding}
To more accurately verify the capability of StrucTexTv2 on semantic learning, we evaluate the performance of our pre-trained model on FUNSD for form understanding. We build a classification layer to predict four categories (question, answer, header, or other) for each semantic entity. As shown in Tab.~\ref{form_understanding}, the entity-level F1-score of $\rm StrucTexTv2_{Small}$ is 89.23\% and the entity-level F1-Score of $\rm StrucTexTv2_{Large}$ is 91.82\%, which achieve comparable performance with the latest state-of-the-art methods. StrucTexTv2 achieves great semantic learning while the ground-truth OCR is utilized on FUNSD. However, it needs to be explained that our method can present better end-to-end information extraction results than the two-stage methods with the actual OCR engine, which is more referential for practical applications.

\begin{table}[ht]
\caption{Experimental results and performance comparison of the form understanding task on FUNSD. The entity-level F1-Score is used to measure model accuracy.}
\label{form_understanding}
\vspace{-0.5em}
\begin{center}
\setlength{\tabcolsep}{1mm}{
\renewcommand{\arraystretch}{1.1}
\begin{tabular}{l l c}
\toprule
\textbf{Methods} & \textbf{\#Params.} & \textbf{FUNSD} \\
\midrule
$\rm DocFormer_{Base}$~\cite{appalaraju2021docformer} & 183M & 83.34\% \\
SelfDoc~\cite{li2021selfdoc} & - & 83.36\% \\
$\rm LayoutLMv2_{Large}$~\cite{xu2020layoutlmv2} & 426M & 84.20\% \\
$\rm DocFormer_{Large}$~\cite{appalaraju2021docformer} & 536M & 84.55\% \\
$\rm StructuralLM_{Large}$~\cite{li2021structurallm} & 355M & 85.14\% \\
UDoc~\cite{nips2021udoc} & 272M	& 87.93\% \\
$\rm StrucTexTv2_{Small}$ & \textbf{28M} & 89.23\% \\
XDoc~\cite{chen2022xdoc} & 146M	& 89.40\% \\
$\rm LayoutLMv3_{Base}$~\cite{huang2022layoutlmv3} & 133M & 90.29\% \\
$\rm StrucTexTv2_{Large}$ & 238M & 91.82\% \\
$\rm LayoutLMv3_{Large}$~\cite{huang2022layoutlmv3} & 368M & \textbf{92.08\%} \\
\bottomrule
\end{tabular}}
\end{center}
\vspace{-0.5em}
\end{table}

\subsection{StrucTexTv2 in Chinese}
To further analyze StrucTexTv2 in the Chinese language, we pre-train $\rm StrucTexTv2^{\dag}_{Small}$ with extra collected Chinese document images and evaluate the model on XFUND-ZH~\cite{XuL0WLFZW22}. XFUND is a multi-lingual extended dataset of the FUNSD~\cite{jaume2019funsd} in 7 languages. $\rm StrucTexTv2^{\dag}_{Small}$ directly adopts the text recognition model from PaddleOCR since only 149 train images of XFUND-ZH can not fulfill the recognition branch training for the end-to-end information extraction task. Experiment results in Tab.~\ref{chinese_ie} show that StrucTexTv2 can benefit the task of document image understanding in Chinese.

\begin{table}[ht]
\caption{Experimental results on XFUND-ZH. We re-implement $\rm LayoutXLM^*_{Base}$ with PaddleOCR to provide OCR results. The Normalized Edit Distance (1-NED) is used to evaluate for the end-to-end information extraction task.}
\label{chinese_ie}
\vspace{-0.5em}
\begin{center}
\setlength{\tabcolsep}{1mm}{
\renewcommand{\arraystretch}{1.1}
\begin{tabular}{l l c}
\toprule
\textbf{Methods} & \textbf{OCR} & \textbf{1-NED} \\
\midrule
$\rm LayoutXLM^*_{Base}$~\cite{abs-2104-08836} & PaddleOCR & 60.40\% \\
$\rm StrucTexTv2^{\dag}_{Small}$ & Text Recognition Only & 67.46\% \\
\bottomrule
\end{tabular}}
\end{center}
\vspace{-0.5em}
\end{table}


\subsection{Ablation Study for Image Resolution}
In this ablation, we investigate the impact of training with different image resolutions. As shown in Tab.\ref{table:Ablation_ImageResolution}, by replacing the resolution of input images with $224*224$, $512*512$, and $960*960$, the accuracy of RVL-CDIP is 89.5\%, 90.4\%, and 92.5\%, respectively. We also report the results on PubLayNet, the F1-Score of PubLayNet is 94.9\%, 92.1\%, and 85.9\%, respectively. It suggests that the best image resolution of our pre-training tasks is $960*960$. The performance of image-only vision tasks is more sensitive to the selection of resolution of the input.

\begin{table}[ht]
\caption{Ablation study for Different Image Resolution.}
\label{table:Ablation_ImageResolution}
\vspace{-0.5em}
\begin{center}
\setlength{\tabcolsep}{1mm}{
\renewcommand{\arraystretch}{1.1}
\begin{tabular}{c c c c}
\toprule
\textbf{Image Resolution} & \textbf{Pre-train Tasks} & \textbf{RVL-CDIP} & \textbf{PubLayNet} \\
\midrule
224*224       & MLM+MIM     & 89.5\%  & 85.9\% \\
512*512       & MLM+MIM     & 90.4\%  & 92.1\% \\
960*960       & MLM+MIM     & 92.5\%  & 94.9\% \\
\bottomrule
\end{tabular}}
\end{center}
\vspace{-0.5em}
\end{table}


\subsection{Ablation Study for OCR Selection}
In this ablation, we look at the impact of various thresholds used in OCR selection for pre-training. We choose a commercial OCR engine to get the OCR results of the pre-training data. Besides the coordinates and text content, we keep the confidence score for each OCR word. For our pre-training, only the high-confidence word with a score above 0.8 was retained. Then the masked words are selected within the preserved text. We conduct experiments to analyze the impacts of the quality of OCR results on our pre-trained model via performing different score thresholds. As shown in Tab.\ref{table:Ablation_Threshold}, the performance decreases from 55.0\% to 54.9\% and 54.8\% by replacing the score threshold from 0.8 to 0.6 and 0.4 on FUNSD, respectively. Similarly, on RVL-CDIP, only slight modifications in accuracy by replacing the score threshold from 0.8 to 0.6 and 0.4. The subtle performance changes in downstream tasks suggest that StrucTexTv2 is robust against the selection of word's thresholds.

\begin{table}[ht]
\caption{Ablation study for the impacts of different thresholds used in OCR selection. The model is pre-trained with only MIM task.}
\label{table:Ablation_Threshold}
\vspace{-0.5em}
\begin{center}
\setlength{\tabcolsep}{1mm}{
\renewcommand{\arraystretch}{1.1}
\begin{tabular}{l c c}
\toprule
\textbf{OCR} & \textbf{RVL-CDIP} & \textbf{FUNSD} \\
\midrule
threshold=0.8 & 93.4\% & 55.0\% \\
threshold=0.6 & 93.4\% & 54.9\% \\
threshold=0.4 & 93.3\% & 54.8\% \\
\bottomrule
\end{tabular}}
\end{center}
\vspace{-0.5em}
\end{table}


\begin{figure*}
 \centering
 \includegraphics[width=0.9\textwidth]{appendix_imgs/appendix_fig.pdf}
 \caption{The qualitative results of Mask Image Modeling. From left to right: the masked document image, document reconstruction without content information from MLM, and document reconstruction with content information from MLM.}
\label{fig:grid_4figs_1cap_4subcap}
\end{figure*}

\subsection{Qualitative Results of Mask Image Modeling}
The qualitative results of Mask Image Modeling, which are shown in Fig. ~\ref{fig:grid_4figs_1cap_4subcap}, express that StrucTexTv2 is capable of recovering the RGB value of the randomly selected masked text regions according to the surrounding context.

\end{document}