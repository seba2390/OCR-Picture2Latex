%%
%% This is file `sample-authordraft.tex',
%% generated with the docstrip utility.
%%
%% The original source files were:
%%
%% samples.dtx  (with options: `authordraft')
%%
%% IMPORTANT NOTICE:
%%
%% For the copyright see the source file.
%%
%% Any modified versions of this file must be renamed
%% with new filenames distinct from sample-authordraft.tex.
%%
%% For distribution of the original source see the terms
%% for copying and modification in the file samples.dtx.
%%
%% This generated file may be distributed as long as the
%% original source files, as listed above, are part of the
%% same distribution. (The sources need not necessarily be
%% in the same archive or directory.)
%%
%% The first command in your LaTeX source must be the \documentclass command.
\documentclass[sigconf,dvipsnames]{acmart}
% \documentclass[sigconf,authorversion,dvipsnames]{acmart}
% \documentclass[sigconf,authordraft,dvipsnames]{acmart}
\usepackage{booktabs}
%\usepackage{threeparttable}
\usepackage{listings}
\usepackage{lipsum}
%\usepackage{xargs}

% \usepackage[colorinlistoftodos,prependcaption,textsize=tiny]{todonotes}
% \usepackage[colorinlistoftodos,prependcaption,textsize=scriptsize,color=orange!30]{todonotes}
% \newcommandx{\task}[2][1=]{\todo[inline,size=normalsize,backgroundcolor=CornflowerBlue!25,bordercolor=CornflowerBlue!25,#1]{#2}}
% \newcommandx{\fix}[2][1=]{\todo[linecolor=red,backgroundcolor=red!20,bordercolor=red,#1]{#2}}
% \newcommandx{\unsure}[2][1=]{\todo[linecolor=blue,backgroundcolor=blue!20,bordercolor=blue,#1]{#2}}
% \newcommandx{\info}[2][1=]{\todo[linecolor=ForestGreen,backgroundcolor=green!25,bordercolor=ForestGreen,#1]{#2}}
% \newcommandx{\improve}[2][1=]{\todo[linecolor=Plum,backgroundcolor=Orchid!20,bordercolor=Plum,#1]{#2}}
% \newcommandx{\thiswillnotshow}[2][1=]{\todo[disable,#1]{#2}}

\newcommand{\revision}[1]{\textcolor{red}{#1}}

\lstset{basicstyle=\footnotesize\ttfamily,breaklines=true}
% \lstset{framextopmargin=50pt,frame=bottomline}

\graphicspath{{figs/}}

%%
%% \BibTeX command to typeset BibTeX logo in the docs
\AtBeginDocument{%
  \providecommand\BibTeX{{%
    \normalfont B\kern-0.5em{\scshape i\kern-0.25em b}\kern-0.8em\TeX}}}

%% Rights management information.  This information is sent to you
%% when you complete the rights form.  These commands have SAMPLE
%% values in them; it is your responsibility as an author to replace
%% the commands and values with those provided to you when you
%% complete the rights form.
% \setcopyright{acmcopyright}
% \copyrightyear{2021}
% \acmYear{2021}
%\acmDOI{10.1145/1122445.1122456}
\setcopyright{licensedusgovmixed}
\copyrightyear{2021}
\acmYear{2021}
\acmISBN{978-1-4503-8691-3/21/07}
\acmDOI{10.1145/3477145.3477155}

%% These commands are for a PROCEEDINGS abstract or paper.
% \acmConference[Woodstock '18]{Woodstock '18: ACM Symposium on Neural
  % Gaze Detection}{June 03--05, 2018}{Woodstock, NY}
\acmConference[ICONS '21]{ICONS 2021: International Conference on Neuromorphic Systems}{July 27--29, 2021}{PREPRINT}
\acmBooktitle{PREPRINT}
%\acmBooktitle{Proceedings of ICONS 21: International Conference on Neuromorphic Systems(ICONS â€™21)}
% \acmPrice{15.00}



%%
%% Submission ID.
%% Use this when submitting an article to a sponsored event. You'll
%% receive a unique submission ID from the organizers
%% of the event, and this ID should be used as the parameter to this command.
%%\acmSubmissionID{123-A56-BU3}

%%
%% The majority of ACM publications use numbered citations and
%% references.  The command \citestyle{authoryear} switches to the
%% "author year" style.
%%
%% If you are preparing content for an event
%% sponsored by ACM SIGGRAPH, you must use the "author year" style of
%% citations and references.
%% Uncommenting
%% the next command will enable that style.
%%\citestyle{acmauthoryear}

%%
%% end of the preamble, start of the body of the document source.
\begin{document}

%%
%% The "title" command has an optional parameter,
%% allowing the author to define a "short title" to be used in page headers.
\title{Neko: a Library for Exploring Neuromorphic Learning Rules}

%%
%% The "author" command and its associated commands are used to define
%% the authors and their affiliations.
%% Of note is the shared affiliation of the first two authors, and the
%% "authornote" and "authornotemark" commands
%% used to denote shared contribution to the research.
% \author{Ben Trovato}
% \email{webmaster@marysville-ohio.com}
% \affiliation{%
%   \institution{Institute for Clarity in Documentation}
%   \streetaddress{P.O. Box 1212}
%   \city{Dublin}
%   \state{Ohio}
%   \country{USA}
%   \postcode{43017-6221}
% }

\author{Zixuan Zhao}
\affiliation{%
  \institution{University of Chicago}
  \country{}
}

\author{Nathan Wycoff}
\affiliation{%
  \institution{Virginia Tech}
  \country{}
}

\author{Neil Getty}
\affiliation{%
  \institution{Argonne National Laboratory}
  \country{}
}

\author{Rick Stevens}
\affiliation{%
 \institution{Argonne National Laboratory \& University of Chicago}
  \country{}
}

\author{Fangfang Xia}
\affiliation{%
 \institution{Argonne National Laboratory \& University of Chicago}
  \country{}
}


%%
%% By default, the full list of authors will be used in the page
%% headers. Often, this list is too long, and will overlap
%% other information printed in the page headers. This command allows
%% the author to define a more concise list
%% of authors' names for this purpose.
% \renewcommand{\shortauthors}{Zhao, et al.}

%%
%% The abstract is a short summary of the work to be presented in the
%% article.
\begin{abstract}
The field of neuromorphic computing is in a period of active exploration.
While many tools have been developed to simulate neuronal dynamics or convert deep networks to spiking models, general software libraries for learning rules remain underexplored.
This is partly due to the diverse, challenging nature of efforts to design new learning rules, which range from encoding methods to gradient approximations, from population approaches that mimic the Bayesian brain to constrained learning algorithms deployed on memristor crossbars.
To address this gap, we present Neko, a modular, extensible library with a focus on aiding the design of new learning algorithms.
We demonstrate the utility of Neko in three exemplar cases: online local learning, probabilistic learning, and analog on-device learning.
Our results show that Neko can replicate the state-of-the-art algorithms and, in one case, lead to significant outperformance in accuracy and speed.
Further, it offers tools including gradient comparison that can help develop new algorithmic variants.
Neko is an open source Python library that supports PyTorch and TensorFlow backends.
\end{abstract}

%%
%% The code below is generated by the tool at http://dl.acm.org/ccs.cfm.
%% Please copy and paste the code instead of the example below.
%%
\begin{CCSXML}
<ccs2012>
   <concept>
       <concept_id>10010147.10010257.10010321</concept_id>
       <concept_desc>Computing methodologies~Machine learning algorithms</concept_desc>
       <concept_significance>500</concept_significance>
       </concept>
   <concept>
       <concept_id>10010583.10010786.10010792.10010798</concept_id>
       <concept_desc>Hardware~Neural systems</concept_desc>
       <concept_significance>500</concept_significance>
       </concept>
   % <concept>
   %     <concept_id>10002950.10003648.10003670.10003675</concept_id>
   %     <concept_desc>Mathematics of computing~Variational methods</concept_desc>
   %     <concept_significance>500</concept_significance>
   %     </concept>
 </ccs2012>
\end{CCSXML}

\ccsdesc[500]{Computing methodologies~Machine learning algorithms}
\ccsdesc[500]{Hardware~Neural systems}
% \ccsdesc[500]{Mathematics of computing~Variational methods}

%%
%% Keywords. The author(s) should pick words that accurately describe
%% the work being presented. Separate the keywords with commas.
\keywords{Neuromorphic computing, learning rules, approximate gradients, Bayesian inference, Manhattan rule, open-source library}

%% A "teaser" image appears between the author and affiliation
%% information and the body of the document, and typically spans the
%% page.
\begin{teaserfigure}
  \includegraphics[width=\textwidth]{overview.png}
  \caption{Neko overview. \textmd{Key components in the neuromorphic learning library.}}
  \Description{Key components in the Neko neuromorphic learning library}
  \label{fig:teaser}
\end{teaserfigure}

%%
%% This command processes the author and affiliation and title
%% information and builds the first part of the formatted document.
\maketitle

%\listoftodos\relax

\section{Introduction}
Deep learning is the prevailing paradigm for machine learning.
Over the course of its meteoric rise, its many differences from human learning have become increasingly clear.
Chief among these are gaps in data efficiency, robustness, generalizability, and energy efficiency --- all unlikely to narrow with growing computation power alone.
This has motivated a renewed search for brain-inspired learning algorithms.
However, the current software infrastructure needs improvement to support productive exploration.

Two common choices today for designing novel learning algorithms are TensorFlow \cite{abadi2016tensorflow} and PyTorch \cite{paszke2019pytorch}.
These general deep learning frameworks provide powerful abstractions for calculating gradients and building deep neural networks, but there is no intermediate layer between these two levels.
For high-level development, backpropagation is the only learning algorithm offered and is in fact coupled with the training process.

Software in neuromorphic computing, on the other hand, has traditionally focused more on simulating neurons and spiking neural networks \cite{carnevale2006neuron,gewaltig2007nest,bekolay2014nengo,stimberg2019brian}, interfacing with neuromorphic hardware \cite{davison2009pynn,sawada2016truenorth,lin2018programming,rueckauer2021nxtf}, and converting pre-trained deep learning models to spiking neural networks for inference \cite{rueckauer2017conversion,rueckauer2018conversion}.
Learning has not been a key part of these libraries.
The few supported learning rules such as spike-timing-dependent plasticity are not competitive on large problems.
As a result, new learning algorithms are developed in independent codebases that are not easily reusable.

In this work, we present Neko, a software library under active development for exploring learning rules.
We build on the popular autograd frameworks, and our goal is to implement key building blocks to boost researcher productivity.
By decoupling the learning rules from the training process, we aim to provide an abstraction model that enables mixing and matching of various design ideas.
To arrive at the right abstraction level, we need to sample a wide range of learning algorithm research.
Below are the three directions and exemplars we have prioritized in this initial code release.

The first class of learning rules are gradient-based methods.
They approximate backpropagation with various levels of biological plausibility \cite{lillicrap2020backpropagation,lee2016training,sacramento2018dendritic,neftci2019surrogate,zenke2018superspike,marschall2020unified,lillicrap2016random,akrout2019deep,sornborger2019pulse}.
From this category, we study the e-prop algorithm \cite{bellec2020solution} in detail and provide a complete reimplementation.
The second direction is based on the hypothesis that the brain keeps track of probabilistic distributions over weights and rewards \cite{aitchison2021synaptic,dabney2020distributional}.
This line of exploration may offer important clues towards achieving learning efficiency and robustness in the face of uncertainty.
We develop a sampling-based learning rule on spiking neural networks (SNN).
The third class is concerned with hardware constraints on plasticity mechanisms.
For this class, we include the classic example of Manhattan rule training for memristive crossbar circuits.
In all three exemplars, we seek consistent implementation in the Neko library.

\section{Library design}
The Neko library is designed to be modular, extensible, and easy to use.
Users can select from a collection of neuron models and encoding methods to build a spiking or regular artificial neural network, and train it with one of the implemented learning rules.
Alternatively, they could supply their own networks from PyTorch or Keras \cite{chollet2015keras} or develop new learning algorithms based on the provided intrinsics.
The following code snippet provides an example of solving MNIST \cite{lecun1998mnist} with the e-prop algorithm on a recurrent network of 128 hidden adaptive leaky integrate-and-fire (ALIF) neurons.

\begin{lstlisting}[caption={Train an SNN model of ALIF neurons with e-prop. },captionpos=b,frame=single, language=python,breaklines]
from neko.backend import pytorch_backend as backend

rsnn = ALIF(128, 10, backend, task_type='classification')
model = Evaluator(rsnn, loss='categorical_crossentropy', metrics=['accuracy', 'firing_rate'])
learning_rule = Eprop(model, mode='symmetric')
trainer = Trainer(learning_rule)
trainer.train(x_train, y_train, epochs=30)
\end{lstlisting}
%\unsure{ZZ: SNN->RSNN}

The training process illustrated in this example can be broken down into a series of high-level Neko modules:
the \emph{layer} includes pre-implemented recurrent SNNs and adaptors for existing Keras and PyTorch models;
the \emph{evaluator} associates a model with a loss function and optional metrics;
the \emph{learning rule} implements backpropagation and a growing list of neuromorphic learning rules;
and the \emph{trainer} handles training logistics as well as special logic to apply multiple learning rules for gradient comparison between models.
Besides these core components, auxiliary modules include the data loader, spike encoder, optimizer, and functions for loss, activation, and pseudo-derivatives calculations.

To help users define custom algorithms, Neko also provides a unified API for accessing frequently used features in TensorFlow and PyTorch such as low-level tensor operations.
Switching the backend is straightforward.
This feature can detect occasional framework-dependent behavior and is useful for code verification and performance analysis.
The multi-backend support is reminiscent of the earlier Keras framework.
However, Neko is different in that it provides more fine-grained abstraction layers such that users can replace the learning algorithm by changing a single line of code.
Taken together, these features also simplify the process of porting code to hardware accelerators, since implementing a backend for the hardware is sufficient to run all models in Neko on it.

\section{Use cases}
In this section, we present results on the three representative learning rules introduced earlier.
We also provide gradient analysis as an example of Neko's cross-cutting utilities that we are building to help design, debug, and compare new learning algorithms.

\subsection{Credit assignment with local signals}
A key mystery in the brain is how it implements credit assignment.
The standard backpropagation through time (BPTT) algorithm is unrealistic as we cannot expect a biological neuron to be aware of all past synaptic strengths.
Bellec et al. \cite{bellec2020solution} proposed e-prop, a local online learning algorithm for recurrent SNNs.
The method exploits the mathematical formula of BPTT, deriving an approximation which only requires a recursive accumulative \emph{eligibility trace} and a local \emph{learning signal}.
These properties make the algorithm one step closer to biologically realistic on-chip learning.

In Neko, we implemented full-featured e-prop algorithms including the three variants: symmetric, random, and adaptive.
Whereas the paper manually derived the e-prop formulas for some networks, we took a different approach: separating the model from the learning rules.
In the layer module, the regular recurrent neural networks and recurrent SNNs, with leaky integrate-and-fire (LIF) or ALIF neurons, were all defined as standard models.
Meanwhile, they inherited from an \emph{Epropable} class, which defined general symbolic gradient formulas according to recurrent cell dynamics.
Specifying this extra information was all it took to perform e-prop, and in a network-agnostic way.
This design enabled the error-prone formula derivation to be automated.
It also sped up experiments with new network architectures or e-prop variants.
% It reduced the number of implementation tasks in terms of models ($m$) and learning algorithms ($n$) from $m\cdot n$ to $m+n$.
% Despite the three main versions (symmetric, random, and adaptive) of e-prop, we also included switches to small tweaks like the firing rate regularization, weight decay regularization and more. Aside from more biological plausibility, those modifications can alter the training behavior remarkably.

We compared the Neko implementation of e-prop to the original implementation on the TIMIT benchmark \cite{garofolo1992timit} for framewise speech recognition.
The authors reported the results on a hybrid network of 100 ALIF and 300 LIF neurons \cite{bellec2020solution}.
In our experiment, we used an ALIF-only network of 200 neurons and otherwise kept the setup identical.
We report close reproduction accuracy in Fig. \ref{fig:timit}.
Notably, Neko's error rate dropped by $27\%$, after tuning regularization and batch size, while keeping the firing rate low at 10 Hz.
To the best of our knowledge, this is the best SNN accuracy obtained with a local learning rule, which in fact reaches the level of an LSTM baseline trained with the precise gradients from BPTT (\cite{bellec2020solution} Fig. S4).
Additionally, Neko is faster (training time from Nvidia V100) and convenient for iterative development.


\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{timit.png}
    \caption{TIMIT results.
      \textmd{We reproduce e-prop accuracy on speech recognition in Neko with a smaller network. Neko is faster with slight tuning and reduces error by $27\%$ to reach the nonspiking baseline performance of a BPTT-trained LSTM model.}
    }
    \label{fig:timit}
\end{figure}


\subsection{Probabilistic learning}
Bayesian statistics has captured much attention in the computational neuroscience community, both as an explanation for neural behavior \cite{Knill2004} as well as a means of performing inference in neural networks. In Neko, we develop a Hybrid Monte Carlo, or HMC \citep{Neil2011HMC}, algorithm to perform Bayesian inference on spiking neural networks based on Metropolis-adjusted Langevin diffusion \cite{Rossky1978}.

Fundamentally, HMC algorithms are simply Metropolis-Hastings samplers \cite{Hoff2009Bayes} where the proposal distribution is based on the gradient. Though spiking neurons are non-differentiable by definition, \textit{surrogate gradients} can be defined by considering smoothed versions of the spiking activation function \cite{neftci2019surrogate}. State of the art learning algorithms for spiking neurons have used these surrogate gradients successfully, and we also find success in deploying them in HMC to form our proposal. In fact, this two-stage approach is especially appealing for spiking neurons, since the theoretical underpinnings of HMC place only very weak restrictions on what the proposal direction should be, and certainly do not require an exact gradient to be satisfied. Thus, from a theoretical perspective, running our algorithm for sufficiently long will result in a sample from our true posterior. Empirically, of course, it is not practical to explore the entire nonconvex, high-dimensional posterior. We therefore verify our implementation numerically.

The MNIST-1D \cite{greydanus2020scaling} data is a derivative of the popular MNIST dataset of handwritten digits which transforms the image recognition problem into a sequence learning problem (See Figure \ref{fig:hmc}, Left). We train a spiking neural network with 1,000 hidden neurons using our proposed HMC algorithm\footnote{Using an adaptive step size \cite{Andrieu2008Adaptive} with a diffusion standard deviation of 0.01 scaled by the norm of the surrogate gradient, which was obtained via standard backpropagation.}, and recorded the posterior mean as well as uncertainty for the train set examples. As shown in Figure 3 (Right), we find that the model displayed significantly more uncertainty on test examples for which its best guess was incorrect than when it was correct. This validates our algorithm, as we would like errors to be associated with high uncertainty.

%Visualization of predictive uncertainty as a function of predictive accuracy. The y axis gives the deviation of the posterior sample from the posterior mean as measured by the average cross entropy between the posterior mean of class probabilities and individual draws on test samples. These are split into two groups, based on whether the \textit{a posteriori} most probable class was correct. We observe higher uncertainty on average when the model is incorrect than when it is correct.
As future work, we intend to compare HMC and other MCMC algorithms to other probabilistic learning approaches such as Variational Bayes \cite{Graves2011} and Monte Carlo Dropout \cite{gal2016} within the Neko framework.

\begin{figure}
    \centering
    \includegraphics[scale=0.193]{figs/oned_example.png}
    \includegraphics[scale=0.8]{figs/hmc_boxplot.pdf}
    \caption{ Uncertainty Quantification.
    \textmd{\textbf{Left:} An example input representing the number 3 for the MNIST-1D data. \textbf{Right:} Posterior uncertainty among test examples which were correctly versus incorrectly predicted. Uncertainty is higher when errors are made.}}
    \label{fig:hmc}
\end{figure}

\subsection{Analog neural network training}
Memristors have emerged as a new platform for neuromorphic learning \cite{thomas2013memristor,hu2014memristor}.
These devices represent the synapse weights in the tunable conductance states of large crossbar architectures.
Compared with digital implementations of neural networks, these analog circuits offer promising advantages in parallel processing, in-situ learning, and energy efficiency \cite{fuller2019parallel,li2018efficient}.
However, they also place constraints on how the weights can be updated.

A classic way to train these networks is with the Manhattan rule learning algorithm \cite{7139171}.
% Since the device in a single column is updated with the same current, we cannot apply per-entry weight changes.
Although training with backpropagation on device is theoretically possible, the time consumption of tuning individual weights with feedback algorithm can be prohibitive, especially for larger scale neural networks \cite{Alibart_2012}.
As an alternative, the Manhattan rule simply updates network weights by a fixed amount according to the sign of the gradients, where the actual change magnitude may depend on the state of the material.
This learning rule has been applied successfully to simple machine learning  benchmarks in simulated or fully hardware-implemented analog neural networks \cite{yao2020fully}.

Neko implements a family of Manhattan rules to simulate the training process.
It includes the basic algorithm and an extended version that supports a specified range of material conductance constraints.
Because these learning rules do not have special requirements for the network architecture, users can directly supply existing Keras and PyTorch models with Neko's adaptors.
Our preliminary results show that both the simple Manhattan rule and the constrained version could train the MNIST dataset up to 96\% accuracy on a simple 2-layer (with 64, 32 neurons) multi-layer perceptron, which is 2\% lower than backpropagation.


\subsection{Gradient comparison analysis}
Many learning rules depend on gradients explicitly or implicitly.
Yet, gradient estimates are not intuitive to developers.
Debugging learning rules sometimes require noticing the subtle differences in gradient estimates and follow their trends over the course of training.
In Neko, we have designed a gradient comparison tool that can enumerate the gradients or weight changes for multiple learning rules with the same model state and input data.
It can also track this information batch by batch.
Visualizing this information can help inspect approximation quality differences caused by algorithm tweaks and identify equivalence in formula transformations.
Outside the context of debugging, the change in gradient estimates throughout the training process can also reveal potential biases and other properties of the learning algorithm.

The gradient comparison tool is made possible by Neko's separation of the learning algorithm and trainer module. It is implemented as a special trainer that takes multiple learning rules and clones of the same model. While the primary model follows the usual training process, the others' parameters are synced with the primary at each training step, and the weight changes are saved. The equivalence of gradient changes and weight changes can be established using the built-in \emph{naive optimizer} which applies gradients directly without learning rate.

Gradient analysis offers insights into how learning rules behave relative to each other and backpropagation.
Fig. \ref{fig:grads} illustrates this with an example of training spiking MNIST models with three variants of e-prop.
While symmetric e-prop was the best at gradient approximation, the relationship between random and adaptive versions was somewhat unexpected.
The adaptive version produced gradients with larger deviation and bias, which could explain its weaker performance on the benchmark (not shown).


\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{grads.png}
    \caption{Gradient analysis tool.
      % snapshot from MNIST
      \textmd{This example illustrates the differences in approximate gradients among e-prop variants for training MNIST: (top) a snapshot of the distributions of gradient deviations, (bottom) how the gradient deviations change over time.}
    }
    \label{fig:grads}
\end{figure}


\section{Supporting utilities}
To further enable neuromorphic centric exploration, we integrate the SpikeCoding toolbox \cite{SpikeCoding2021} which enables simple encoding of continuous value sequences into spikes with nearly a dozen algorithms.
We present experimental results (Table \ref{tab:surg-ecg_table}) on two temporal data applications using three encoding schemes \cite{Petro2020}:
\begin{itemize}
\item \emph{Temporal contrast (TC)} encoding compares the absolute value of a signal with a threshold derived by the derivative and standard deviation of the full sequence multiplied by a tunable parameter.
\item \emph{Step-forward (SF)} encoding generates positive/negative spikes by comparing values in a sequence to a moving baseline plus a tunable threshold, which is initially the first value of the sequence and updated each spike.
\item \emph{Moving window (MW)} encoding uses a similar moving baseline and threshold to determine spiking but which is set to the mean of values in a tunable time window.
\end{itemize}

All models were trained with e-prop learning except for the Benchmark RNN model trained with BPTT.
While we note that there was often a sizable decrease in accuracy using these encodings, the sparsity of the input signal was significantly increased.
Spike encodings may enable the use and development of learning algorithms more suited to or dependent on event based input.

\begin{table}
\setlength{\tabcolsep}{7pt}
\centering
\caption{Testing two classification exemplars using temporal spike encoding schemes}
\label{tab:surg-ecg_table}
\begin{tabular}{lccccc}
\toprule
Encoding & None            & TC & SF & MW & Benchmark       \\
\midrule
Surgery$^{1}$  & 0.675           & 0.620   & 0.687   & 0.563   & \textbf{0.766} \\
ECG$^{2}$      & \textbf{0.813}  & 0.763   & 0.699   & 0.685   & 0.811    \\
\bottomrule
\end{tabular}
%\begin{tablenotes}%[flushleft]
\begin{flushleft}
%\item
$^{1}$A surgery kinematic dataset measuring the positions and orientations of surgical instruments during labeled simulated exercises. Data available upon request.

%\item 
$^{2}$A public ECG heartbeat categorization dataset \cite{kachuee2018ecg} subsampled for class balance.
\end{flushleft}
%\end{tablenotes}
\end{table}

\section{Conclusions}
We presented the design of a coding library for researching learning algorithms.
Through three examples, we demonstrated its capability and ease of use in diverse scenarios.
Our reference implementations introduced a new state-of-the-art in local temporal credit assignment with SNNs, a sampling-based learning rule for estimating weight and prediction posteriors, as well as simulations for constrained training of analog neural networks on memristive hardware.
Additionally, we showed a cross-cutting example to support learning rule inspection with gradient comparison analysis.

Two directions emerge for future work.
First, we will extend learning rules to complex neuron models (e.g., dendritic computation, structured neurons) and network architecture.
Second, we will port learning algorithms to emerging hardware platforms.
Both processes will be facilitated by the abstraction of learning algorithms and the multi-backend support in the Neko library\footnote{https://github.com/cortical-team/neko}.

\pagebreak
\begin{acks}
We thank Sihong Wang and Shilei Dai for helpful discussions. 
This work is partially supported by Laboratory Directed Research and Development (LDRD) funding from Argonne National Laboratory, provided by the Director, Office of Science, of the U.S. Department of Energy under Contract No. DE-AC02-06CH11357.
\end{acks}

%%
%% The next two lines define the bibliography style to be used, and
%% the bibliography file.
\bibliographystyle{ACM-Reference-Format}
\bibliography{main}

%%
%% If your work has an appendix, this is the place to put it.
% \appendix

% \section{Research Methods}

% \section{Online Resources}

% \todo[inline]{The original todo note withouth changed colours.\newline Here's another line.}
% \lipsum[11]\unsure{Is this correct?}\unsure{I'm unsure about also!}
% \lipsum[11]\fix{Change this!}
% \lipsum[11]\info{This can help me in chapter seven!}
% \lipsum[11]\improve{This really needs to be improved!\newline\newline What was I thinking?!}
% \lipsum[11]
% \improve[inline]{The following section needs to be rewritten! 1 2 3}
% \lipsum[11]
% \lipsum[20]
% \todo[inline]{make a cake}
% \improve[inline]{The following section needs to be rewritten!}
% \lipsum[20]
% \newpage
% % \listoftodos[Notes]


\end{document}
\endinput
%%
%% End of file `sample-authordraft.tex'.
