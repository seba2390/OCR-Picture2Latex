
\documentclass[11pt]{article}
\usepackage{fullpage}
\usepackage{amsmath,amsthm,amssymb}
\usepackage{graphicx,float,psfrag,epsfig,amssymb}
\usepackage[top=1in, bottom=1.25in, left=1in, right=1in]{geometry}
\usepackage[usenames,dvipsnames,svgnames,table]{xcolor}
\definecolor{darkgreen}{rgb}{0.0,0,0.9}
\usepackage[pagebackref,letterpaper=true,colorlinks=true,pdfpagemode=none,citecolor=OliveGreen,linkcolor=BrickRed,urlcolor=BrickRed,pdfstartview=FitH]{hyperref}
%\usepackage{wrapfig}
\usepackage{relsize}
\usepackage{color}
\usepackage{pict2e}
%\usepackage[tight]{subfigure}
\usepackage{caption}

\usepackage{nameref}
\usepackage{makecell}
\usepackage[font={small}]{caption} 
\usepackage{subcaption}
%\usepackage{float}
\usepackage{mathtools}
\usepackage{enumitem}

%%================  Algorithm Package
%
%%\usepackage{algorithm}
\let\chapter\section
\usepackage[ruled,vlined]{algorithm2e}
\usepackage[noend]{algorithmic}
\usepackage{diagbox}
%\usepackage[skins]{tcolorbox}
\renewcommand{\algorithmicrequire}{\textbf{Input:}}
\renewcommand{\algorithmicensure}{\textbf{Output:}}

\DeclareMathAlphabet{\mathpzc}{OT1}{pzc}{m}{it}

%\newenvironment{policy}[1][htb]
%  {\renewcommand{\algorithmcfname}{Table}% Update algorithm name
%   \begin{algorithm}[#1]%
%  }{\end{algorithm}}
  
\newtheorem{propo}{Proposition}[section]
\newtheorem{lemma}[propo]{Lemma}

\newtheorem{assumption}[propo]{Assumption}
\newtheorem{proposition}[propo]{Proposition}
\newtheorem{defi}[propo]{Definition}
\newtheorem{coro}[propo]{Corollary}
\newtheorem{thm}[propo]{Theorem}
\newtheorem{rmk}[propo]{Remark}
\newtheorem{conj}[propo]{Conjecture}
\newtheorem{fact}[propo]{Fact}
\newtheorem{consequence}[propo]{Consequence}


% if you need to pass options to natbib, use, e.g.:
% \PassOptionsToPackage{numbers, compress}{natbib}
% before loading nips_2018

% ready for submission
%\usepackage[nonatbib]{nips_2018}

% to compile a preprint version, e.g., for submission to arXiv, add
% add the [preprint] option:
% \usepackage[preprint]{nips_2018}

% to compile a camera-ready version, add the [final] option, e.g.:
% \usepackage[final]{nips_2018}

% to avoid loading the natbib package, add option nonatbib:
% \usepackage[nonatbib]{nips_2018}

%\usepackage[utf8]{inputenc} % allow utf-8 input
%\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}   
%\usepackage{slashbox}
%\usepackage{amsmath}         % simple URL typesetting
%\usepackage{booktabs}       % professional-quality tables
%\usepackage{amsfonts}       % blackboard math symbols
%\usepackage{nicefrac}       % compact symbols for 1/2, etc.
%\usepackage{microtype}      % microtypography


\newcommand{\aj}[1]{\noindent{\textcolor{blue}{\textsf{#1} \#\#\#}}}
\newcommand{\am}[1]{\noindent{\textcolor{blue}{\textbf{\#\#\# AM:} \textsf{#1} \#\#\#}}}
\newcommand{\hnz}[1]{\noindent{\textcolor{red}{\textbf{\#Hamid:} \textsf{#1}}}}
\newcommand{\hnzu}[1]{\noindent{\textcolor{red}{#1}}}
\newcommand{\jnote}[1]{{[\color{blue}JL: #1]}}
\addtocontents{toc}{\protect\setcounter{tocdepth}{2}}
\newcommand{\indep}{\perp \!\!\! \perp}
\newcommand{\notindep} {\not\!\perp\!\!\!\perp}
 \input{notation}
\def\vw {\underline{w}}
\def\bias{{\sf b}}
\def\noise{{\sf v}}
\def\Std{{\rm{Std}}}
\def\heta{{\hat{\eta}}}
\def\tP{{\widetilde{P}}}
\def\cD{\mathcal{D}}
\def\tcD {\widetilde{\mathcal{D}}}
\def\hC{{\widehat{C}}}
\def\tr{{\widetilde{r}}}
\def\cJ{\mathcal{J}}
\def\wC{\widetilde{C}}
\def\hw{\widehat{w}}
\def\bX{\mathbf{X}}
\def\bY{\mathbf{Y}}
\def\bZ{\mathbf{Z}}
\def\hbet{\widehat{\beta}}
\def\tbet{\widetilde{\beta}}
\def\tw{\widetilde{W}}
\def\wth{\widetilde{\theta}}
\def\wsigma{\widetilde{\sigma}}
\def\quant{{\rm{Quantile}}}
\def\tz{\widetilde{Z}}
\def\bD{\textbf{D}}
\DeclarePairedDelimiter\ceil{\lceil}{\rceil}
\DeclarePairedDelimiter\floor{\lfloor}{\rfloor}
\def\AR{\mathsf{AR}}
\def\SR{\mathsf{SR}}
\def\mfM{\mathfrak{M}}
\def\hP{\widehat{P}}
\def\hX{\widehat{X}}
\def\normal{\mathsf{N}}
\def\tv{\mathsf{TV}}
\def\hlg{\mathsf{H}}
\def\tbX{\widetilde{\bX}}
\def\partu{\frac{\partial}{\partial u}}
\def\partt{\frac{\partial}{\partial t}}
%====================== Title and authors ==========================
\title{\bf Pearson Chi-squared Conditional Randomization Test}


\author{ 
Adel Javanmard\thanks{Data Sciences and Operations Department, University
of Southern California} \thanks{A.~Javanmard is partially supported by the Sloan Research Fellowship
in mathematics, an Adobe Data Science Faculty Research Award and the
NSF CAREER Award DMS-1844481.} \and 
Mohammad Mehrabi\footnotemark[1] \thanks{The names of the authors are in alphabetical order. }
}
\begin{document}

\maketitle
\begin{abstract}
Conditional independence (CI) testing arises naturally in many scientific problems and applications domains. The goal of this problem is to investigate the conditional independence between a response variable $Y$ and another variable $X$, while controlling for the effect of a high-dimensional confounding variable $Z$. In this paper, we introduce a novel test, called `Pearson Chi-squared Conditional Randomization' (PCR) test,  which uses the distributional information on covariates $X,Z$ and constructs randomizations to test conditional independence.  Our proposal is motivated by some of the hard alternatives for the vanilla conditional randomization test~\cite{candes2018panning}.  

We also provide a power analysis of the PCR test, which captures the effect of various parameters of the test, the sample size and the distance of the alternative from the set of null distributions, measured in terms of a notion called `conditional relative density'.  In addition, we propose two extensions of the PCR test, with important practical implications: $(i)$ parameter-free PCR, which uses Bonferroni's correction to decide on a tuning parameter in the test; $(ii)$ robust PCR, which avoids inflations in the size of the test when there is slight error in estimating the conditional law $P_{X|Z}$. 
\end{abstract}

\section{Introduction}

Understanding the statistical relationship between random variables is a cornerstone of many scientific experiments. During the last few decades, various measures of dependency were developed to capture the association between two random variables, such as the mutual information and information theoretic coefficients \cite{reshef2011detecting}, the kernel-based measures \cite{pfister2018kernel, zhang2018large}, the correlation coefficients that are based on sample ranks 
\cite{drton2018high, deb2021multivariate, weihs2018symmetric}, and the dependency metrics that are based on copulas \cite{zhang2019bet, shih2021copula}; We refer to this survey \cite{jossea2013measures} for other dependency coefficients.

 In many modern data science problems, answering profound questions, such as inferring causal links between different variables, requires a more thorough analysis. In particular, a desired analysis must control for the presence of what is known as  confounding factors.  This happens when an (often unmeasured) factor $Z$ effects both of the variables of interest (say $X$ and $Y$), and hence  can lead to misleading conclusions about the association of the variables. For example, in genome-wide association studies (GWAS), researchers are interested in finding loci that are causal for the trait. However, spurious association can arise due to ancestry-induced correlations between causal and non-causal loci, or when ancestry is correlated with both the genotype and the trait~\cite{campbell2005demonstrating,bhaskar2017novel}.
 
 Conditional independence (CI) testing controls for the effect of such confounding factors. To further highlight the significance of the CI problem, it is worth noting that many important problems in statistics can indeed be cast as a CI testing problem, with examples ranging from the classic concepts of sufficient and ancillary statistics \cite{dawid1979conditional}, to the well-known concepts in graphical models \cite{koller2009probabilistic, friedman2004inferring, dobra2004sparse}, and the causal discovery problems \cite{pearl2000models, zhang2012kernel, peters2017elements}, where at the heart of all these settings, one can find a CI testing problem. 

In the recent work of \cite{shah2020hardness}, it is argued that the CI testing is provably a hard problem without assumptions being placed on the distribution of variables. Concretely, \cite{shah2020hardness} shows that no uniformly valid test\footnote{a test that controls the type I error at a predetermined significance level $\alpha$ for all absolutely continuous (with respect to the Lebesgue measure)  random variables $(X,Z,Y)$ that are conditionally independent} can have nontrivial power (power  exceeding $\alpha$) against any alternative hypothesis (a triple $(X,Z,Y)$ that are not conditionally independent).  By and large, this impossibility result can be perceived as a consequence of an interesting phenomenon that happens in the CI testing problem: while the space of the null distributions are separated from the alternatives, in fact the convex hull of the null space is a dense set in the alternative space with respect to the total variation metric \cite{shah2020hardness}.


The discouraging result of \cite{shah2020hardness} highlights the crucial role of the assumptions on the distribution of $(X,Z,Y)$ in the CI testing problem. This is a noteworthy observation that such assumptions may make the null space smaller, so the aforementioned no-free-lunch theorem can not be applied anymore. 
%In fact, focusing merely on a null subset, instead of the entire set of conditionally independent triples, sheds light on the existence of statistical tests with valid type I  error control and non-trivial statistical power in the CI testing problem. 
During the past few years, several methods have been developed for CI testing under different setups, such as  \cite{neykov2020minimax} for one-dimensional variables satisfying certain smoothness assumptions, and \cite{canonne2018testing} for discrete variables. Also there exists  quite a large body of work on model-specific methods, where a parametric model is assumed between the response and the covariates (assumptions on the law $\cL(Y|X,Z)$) \cite{liang2018bayesian, crawford2018bayesian, belloni2014inference}. There is also other concurrent work which goes beyond  testing for the conditional independence and aims at measuring the strength of dependency when the CI hypothesis does not hold (e.g.,  \cite{zhang2020floodgate, azadkia2019simple})

%\aj{Simultaneously, there has also been great effort to go beyond CI testing and try to capture the conditional dependency  power \cite{zhang2020floodgate, azadkia2019simple}.}
%e.g. \cite{shah2020hardness} assumes that the conditional regression models $\E[Y|Z],\E[Y|X]$ can be estimated perfectly.  


Another complimentary line that has been pursued in the past few years is the model-X perspective \cite{candes2018panning}.  In this framework, contrary to the classic setup no assumption is made on the conditional law $\cL({Y|X,Z})$, rather it shifts the focus on $(X,Z)$ and requires an extensive knowledge on the law $\cL({X,Z})$. To emphasize the importance of the model-X setup, one should note that a set of CI tests that have been developed for a certain family of distribution $\cL({Y|X,Z})$ leads to type I error inflation under model misspecification. On the other hand, in many settings, you may have access to abundant unlabeled data which allows for good approximation of $\cL({X,Z})$. For example, in genetic studies \cite{peters2016comprehensive, cong2013multiplex} the joint distribution of covariates can be well approximated. In particular, \cite{wen2010using} proposed an estimator to approximate the covariance matrix of covariates for the genome-wide association study (GWAS), in which genetic distance information is used.

In this paper, we will focus on CI testing in the model-X setup. In this setting, we would like to examine the independence of a covariate $X\in \reals$ and a response value $Y\in \reals$,  while controlling for the effect of a potentially high-dimensional confounding covariate vector  $Z\in \reals^q$. This is formalized via a hypothesis testing problem:
\begin{equation}\label{eq: CI hypothesis}
 H_0: X\indep Y|Z\,,\quad\quad H_A: X\notindep Y|Z\,.
\end{equation}
%The model-X framework implies that an ample amount of information on covariates is available, while the dependency rules $Y|XZ$ or $Y|Z$ remain unknown. In short, 
In the model-X CI testing problem, we are given access to the conditional law $P_{X|Z}$  along with $n$ i.i.d. observations $(X_i,Y_i,Z_i)$ as data, while the conditional laws $Y|X,Z$ or $Y|Z$ are unknown.  A large body of proposed CI tests in the model-X setup, such as the conditional randomization test (CRT) \cite{candes2018panning},  and the holdout-randomization test (HRT) \cite{tansey2018holdout} are based on constructing counterfeit data sets, and then scoring them by a certain score function $T$. In the next step, the normalized rank of the original data score among the counterfeits is considered as the test statistic.  Under the null hypothesis, the constructed test statistic follows a uniform distribution, provided that the number of counterfeits is sufficiently large. The reason is that under $H_0$, and conditional on $Y, Z$, the original and counterfeit scores are i.i.d and so exchangeable. Also, working with $M$ counterfeits, the smallest value the normalized rank can achieve is $1/(M+1)$. Therefore, we need sufficiently large $M$ to test at small significance level $\alpha$. 
%Having a uniform distribution under the null hypothesis, it is natural to count extreme values ( close to 0 and 1) 
The CRT perceives the extreme values of the normalized rank (close to 0 or 1) as evidence against the null hypothesis; however, as we discuss later, it is possible that under alternative hypothesis the deviation from the uniform distribution occurs at the central range; so less likely to observe extreme values, hurting the power of CRT.  We next illustrate this point by a concrete example.

\subsection{Why CRT may fail?: An illustrative example  }\label{subsec: CRT fails}

 For a data set $(\bX,\bZ,\bY)$ consisting of $n$ independent samples with $\bX\in \reals^n$, $\bZ\in\reals^{n\times q}$ an $\bY\in\reals^n$, the CRT constructs $M$ counterfeits $(\widetilde{\bX}_1,\bZ,\bY),...,(\widetilde{\bX}_M,\bZ,\bY)$ where $\widetilde{\bX}_j$ is sampled independently from the conditional law $P_{\bX|\bZ}(\cdot|\bZ)$. (By independence of samples, this means that the entries $\tX_{j,\ell}$ are drawn independently from the law $X|Z$, for $\ell \in\{1,2,\dotsc, n\}$.) Then, for score function $T$ define the normalized rank: 
 %
 \begin{equation}\label{eq: p-crt}
p=\frac{1+\sum\limits_{j=1}^{M}\ind_{\{T(\bX,\bZ,\bY)\geq T(\widetilde{\bX}_j,\bZ,\bY)\} } }{M+1}\,.
\end{equation}  
 %
 We can either reject the null hypothesis for $p$ smaller than $\alpha$ (interpret it as a $p$-value) or reject two extreme $\alpha/2$ tails (two-sided CRT \cite{wang2020power}). Given that $\widetilde{\bX}|\bZ,\bY\sim\cL(\bX|\bZ)$, under the null hypothesis we have 
 \[
 T(\bX,\bZ,\bY) \stackrel{d}{=} T(\tbX,\bZ,\bY) | \bZ,\bY\,,
 \]
so $p$ is uniformly distributed in $[0,1]$, and the (one-sided) cutoff $\alpha$ or the (two-sided) cutoff $\alpha/2$ give valid tests. On the other hand, the power of the test relies merely on observing extremely small or large values of $p$, which reflects a restricted outlook on deviation from the uniform distribution. In particular, this class of CI tests falls short in capturing the inflations on central values and results in a lower statistical power (See Figure~\ref{fig:schemtatic} for a schematic representation). To illustrate this point, we provide a simple independence testing problem with two real-valued random variables $X,Y$ from the following probability laws (for the sake of simplicity, we assume $Z=\emptyset$): 
%\begin{align}
%&Z\sim\normal(0,1)\,,\nonumber \\
%&X=Z+\normal(0,1)\,, \label{eq:simple-example}\\
%&Y=X\arctan(X)-0.5\log(X^2+1)+Z-2+\normal(0,1)\,.\nonumber
%\end{align}
\begin{align} \label{eq: example}
X&\sim \normal(0,1)\,, \nonumber \\
Y&=\frac{1}{\sqrt{10^{-6}+X^2}} + \eps, \quad \eps\sim\normal(0,1)\,.
%Y&=\frac{20}{1+\exp(10X^2)}+\normal(0,1)\,,
\end{align}
Obviously $X$, $Y$ are dependent, and the null hypothesis \ref{eq: CI hypothesis} is not true. Assume the original data set $(\bX,\bY)$ consists of $n=1000$ data points, and let the score function $T$ be the $\ell_2$-square loss $T(\bX,\bY)=\|\bY-\bX\|^2_2$. We run the two-sided CRT at significance level $\alpha=0.1$ with $M=1000$ counterfeits. The statistical power of CRT, averaged over $N=10,000$ experiments, is zero! The histogram for the normalized rank \eqref{eq: p-crt} is depicted in Figure \ref{fig: hist-crt}. 
As can be seen from the plot, most of the p-values are around 0.5. In other words, the deviation from a uniform distribution is occurring at the central range instead of the $\alpha/2$ tails, and CRT fails to discern the dependency between $X,Y$. We refer to Section \ref{poof: example} for further details and the intuition behind the construction of this example. We extend this example in section \ref{sec: crt-failure} in that we show for a class of regression problems $y=g(x)+\eps$, with $\eps\sim\normal(0,1)$ and $g$ satisfying certain conditions, CRT provably has power smaller than $\alpha/2$. The  ratio $1/2$ here is for simplicity, and can be made arbitrarily small for specific functions $g$.



\begin{figure}[]
\centering
\includegraphics[scale=0.24]{new_new_new_hist.pdf}
\caption{  Histogram of the normalized rank $p$ given by \eqref{eq: p-crt}, with $M=1000$ and the $\ell_2$-square loss $T(\bX,\bY)=\|\bY-\bX\|^2_2$ as the score function. The original data set have $n=1000$ data points $\{(X_i,Y_i)\}_{i=1}^n$ which are generated from \eqref{eq: example}. The histogram is obtained by considering $N = 10,000$ realizations of the data. The horizontal purple line represents the uniform distribution. As observed, the normalized ranks are mostly in the central range, leading the CRT to have zero statistical power. }\label{fig: hist-crt}
\end{figure}


\begin{figure*}
	%\centering
	\begin{subfigure}[b]{0.32\textwidth}
		%\centering
		\includegraphics[scale=0.217]{one_sided_comp.pdf}
		\caption{One-sided inflation}
		\label{fig:one_sided}
	\end{subfigure}%
	%\hfill
	\begin{subfigure}[b]{0.32\textwidth}
		%\centering
		\includegraphics[scale=0.2]{two_sided_comp.pdf}
		\caption{Two-sided inflation}
		\label{fig:two_sided}
	\end{subfigure}%
	%\hfill
	\begin{subfigure}[b]{0.32\textwidth}
		%\centering
		\includegraphics[scale=0.2]{middle_comp.pdf}
		\caption{Middle-value inflation}
		\label{fig:middle}
	\end{subfigure}
	\caption{Schematic representations of the distribution of the normalized rank $p$ under alternative hypothesis. The horizontal purple line indicates the uniform distribution. In (a), (b), the non-null p-value has higher tail compared to the null uniform distribution and the CRT (one-sided for (a) and two-sided for (b)) would have descent power. In (c), the non-null p-value is more likely to fall in the middle range which causes the CRT to have low power}
	\label{fig:schemtatic}
\end{figure*}
\subsection{ Related work on model-X CI tests}
The Conditional Randomization Test (CRT) was originally proposed by \cite{candes2018panning} as a generic framework that exploits the distributional information $X|Z$ to control the type I error for any score function, and number of data points (valid finite-sample results). This flexibility of the CRT allows for using any advanced black box predictive model, which plays a key role in achieving high statistical power for the CI testing problem. In \cite{wang2020power} the authors analyze the power of CRT in a high-dimensional linear regression setting for three different score functions: marginal covariance based scores,  the ordinary least square coefficient and the LASSO \cite{tibshirani1996regression}. Further, \cite{katsevich2020theoretical} shows that the CRT with a likelihood-based score function is the most powerful model-X CI test against a point alternative. 

On the computational side, using  advanced black box predictive models in the CRT can be prohibitively daunting, due to the repetitive fittings of the score function on the resampled data. This issue is even exacerbated in multiple testing, where the CRT is used for the feature selection problem. In this approach, the CRT is run for each covariate separately to test its relevance to the response, conditioned on the other covariates. Such multiple usage of the CRT is computationally prohibitive in high-dimensional problems. Alternatively, one can use the model-X knockoff approach  proposed  by \cite{candes2018panning} to circumvent this issue, which of course assumes the knowledge of the covariates  joint distribution. Several recent works extended this procedure beyond the multivariate Gaussian distribution for a broader range of the covariates joint population, see \cite{sesia2017gene} for hidden Markov models,  and \cite{bates2020metropolized} which introduced the Metropolis knockoff sampling for cases where
the covariates are continuous and follow a graphical model. Despite the fact that the model-X knockoff procedure has alleviated the CRT computational burden, this benefit often comes at the cost of a lower statistical power \cite[Section 5.3]{candes2018panning}.  For high-dimensional linear models,~\cite{wang2020power} shows that the CRT provably dominates model-X knockoffs in the variable selection problem. More precisely, they show that under the high-dimensional linear setup, when the Benjaminiâ€“Hochberg (BH) procedure \cite{benjamini1995controlling}, or the adaptive p-value thresholding (AdaPT) procedure \cite{lei2018adapt} is applied on the CRT p-values, a higher statistical power is achieved in comparison to the model-X framework.


Several other methods have also been proposed recently to improve the heavy computational cost of  CRT, such as the Holdout Randomization Test (HRT) \cite{tansey2018holdout} and the Conditional Randomization Test with Distillation (dCRT) \cite{liu2020fast}.  In \cite{berrett2020conditional} the authors   have proposed the Conditional Permutation Test (CPT)  to enhance the robustness of CRT with respect to approximation errors in the law of $X|Z$.

\subsection{Summary of contributions}
In this paper, we introduce a novel method called the {\bf P}earson Chi-squared {\bf C}onditional {\bf R}andomization (PCR) test, which is a general method for investigating the conditional independence of two variables $X,Y$ in the presence of a high-dimensional confounding variable $Z$.  The proposed method is designed for model-X framework where the conditional law $X|Z$ is assumed to be known, but the method assumes nothing whatsoever about  $Y|(X,Z)$. The PCR test is inspired by shortcoming of the conditional randomization test (CRT)~\cite{candes2018panning} in rejecting specific alternative distributions. Similar to CRT, the PCR test uses randomization to construct multiple counterfeits of the data and rank the original data among their counterfeits according to a score function. The score function can be based on arbitrary (potentially complex) predictive models. In contrast to CRT, the PCR test works at the sample levels and label each sample based on its rank among the corresponding counterfeits while CRT works at dataset level. The PCR test includes a Pearson $\chi^2$-test at heart which allows to scrutinize the entire support of the ranks distribution to detect statistically significant deviations from the discrete uniform, while the CRT focuses on the deviations at the tails of the distribution.
As we show in numerical experiments, the PCR test achieves relatively high power even with a small number of counterfeits, while other model-X CI tests require substantially a large number of counterfeits to achieve high power. 

The rest of the paper presents the following contributions:

\begin{enumerate}
\item Section~\ref{sec: crt-failure}: We discuss some alternatives which while stands far from the null distribution (i..e, the response value $Y$ and the covariate $X$ are highly dependent), yet the CRT is provably powerless in rejecting them, even with arbitrarily large sample size and number of counterfeits.

\item Section~\ref{sec:PCR}: We present the PCR test statistic, and provide two rejection thresholds for it to control the size of the test under a target level 
$\alpha$. One threshold indicated by $\th_{L,\alpha}^{\mathsf{finite}}$ is guaranteed to control the size even in finite-sample regime, while the other threshold $\th_{L,\alpha}^{\mathsf{asymp}}$ controls the size for large enough sample size (asymptotic regime). Of course, the former turns out to be more conservative and in our numerical study we observe that for $n$ of order a few hundreds, the size of test is already controlled using the  threshold $\th_{L,\alpha}^{\mathsf{asymp}}$.    

\item Section~\ref{sec: pwr}: We provide a power analysis of the PCR test. Distance of alternative distributions to the set of null distributions is measured via a notion called `conditional relative density', which depends on both the joint law $\cL(X,Y,Z)$ as well as the score function. Our analysis reveals the role of different factors, such as sample size, number of counterfeits and number of labels which are the input parameters for the PCR test.  

\item Section~\ref{sec:pf}: As our power analysis reveals, the number of labels ($L$) used in the PCR test effects its power in a non-trivial ways. Here, we suggest to run PCR test for different choices of $L$ and then use Bonferroni's correction to combine the resulting $p$-values into a valid $p$-valid for the conditional independence hypothesis.

\item Section~\ref{sec:robust}: While in the model-X framework it is assumed that the conditional law $\cL(X|Z)$ is known, in practice one may need to estimate this distribution (e.g., from unlabeled data). In this section, we provide a more conservative version of the PCR test which is more robust to errors in estimating $\cL(X|Z)$, and avoids inflation in the type I error.
\item Section~\ref{sec: numerical}: We evaluate the performance of the PCR test on several  simulated data and on the Capital Bikeshare data set.
\end{enumerate}

%while it always preserves a valid finite-sample type I error level. In particular, the Pearson $\chi^2$-CI test works well even with very few number of randomizations, where this is in contrast with the other model-X CI tests, such as the CRT, HRT, PRT, and dCRT, where their output statistic is intrinsically discrete, and large number of randomizations are required to get statistically valid outputs. From the computational perspective, the Pearson $\chi^2$-CI test can be perceived as a new line of solutions to reduce the heavy computational burden of CRTs. 

%Further, we provide explicit finite-sample expressions for the power of the Pearson $\chi^2$-CI test. The obtained expression is in terms of a new notion called the \textit{conditional dependency power}, where it is a function of the primary distribution $p(x,z,y)$ and the score function $T$. We provide a set of CI testing problems, in which the response value $Y$ and the covariate $X$ are highly dependent, but we theoretically show that with infinite number of samples and unlimited computational power (infinite number of randomizations) the CRT has zero statistical power. On the flip side, we show that, by using the similar score function that was used for the CRT, the Pearson $\chi^2$-CI test achieves power $1$. To make the Pearson $\chi^2$-CI test more practical, we propose a set of extensions of the primary method, in particular we introduce an extension of the Pearson $\chi^2$-CI test, that can be used in scenarios when there is a mismatch between the true $P_{X|Z}$ and the available approximation $\widehat{P}_{X|Z}$. \\

\noindent\textbf{Notations.} Throughout the paper, we use the shorthands $[n]=\{1,2,...,n\}$ for an integer $n\geq1$,  also $a\wedge b=\min\{a,b\}$. We use the capital letters for random variables and the small letters for the specific values they may take. We use bold symbols for vectors and matrices. %For a function $f$ over $[0,1]$, let
%$\||f\||_{\infty}=\text{ess} \sup_{x\in [0,1]}^{} f(x)$, and $L^{\infty}([0,1]$ be the space of all measurable functions on $[0,1]$ with bounded $\|\cdot\|_\infty$ norm. 
%Further, let $\Phi(t)=\int\limits_{-\infty}^{t}e^{-x^2/2}dx/\sqrt{2\pi}$-- standard normal cdf,
% and $\Phi_{\chi^2_m}=P(m/2,x/2)$-- cdf of the Chi-squared distribution with $m$ degrees of freedom, with $P(.,.)$ being the regularized gamma distribution. 
 For random variables or vectors $U,V$, $\cL(U)$ represents the probability law (distribution) of $U$ and $\cL(U|V)$ represents the conditional distribution of $U$ given V. We write $U\stackrel{d}{=}V$ to indicate that $U$ and $V$ have the same distribution. 
 For an event $E$, we denotes its probability by $\prob(E)$.  We use $\stackrel{p}{\Rightarrow}$ to indicate convergence `in probability' and $\stackrel{d}{\Rightarrow}$ for convergence `in distribution'. 
 %Formally,  for a sequence of random variables $\{X_n\}_{n\in\naturals}$ and a random variable $X$, we write $X_n\stackrel{p}{\to}X$ if $\forall \eps>0$, we have $\lim_{n\to\infty}\prob(|X_n - X|> \eps) = 0$. 
 Throughout, $\phi(t) = e^{-t^2/2}/\sqrt{2\pi}$ is the Gaussian density and $\Phi(u) = \int_{-\infty}^u \phi(t)\de t$ is the Gaussian distribution.

\section{Why CRT may fail?: A formal result}\label{sec: crt-failure}
In this section, we delineate the two problems associated with CRTs for CI testing in a model-X setup. Before getting into details, let us briefly recall the CRT procedure and introduce some notations. Assume we are given $n$ i.i.d. data points $(X_i,Y_i,Z_i)_{i=1}^n$, and denote the original data set by $\cD=(\bX,\bY,\bZ)$. We use a score function $T$ to score each data set. This can be a simple squared error loss $T(\cD)=||\bY-\bX||^2$, or a more complicated function, for example the absolute value of the coefficient for $\bX$ fitted by the Lasso on $\cD$, or even more advanced models, such as deep neural networks or random forests. In model-X setup, we assume having access to the conditional law $\cL_{\bX|\bZ}$, and construct counterfeit data sets $\tcD_1=(\tbX_1,\bY,\bZ),...,\tcD_M=(\tbX_M,\bY,\bZ)$, for sufficiently large $M$, where $\tbX_j$, $j\in[M]$ are sampled independently from $\cL({\bX|\bZ})$.  In the next step, we construct the normalized rank statistic given by
%
\[
p=\frac{1+\sum\limits_{j=1}^{M}\ind_{ \{T(\cD)\geq T(\cD_i) \} }}{M+1}\,.
\]
%
Under the null hypothesis $X \indep Y| Z$, and $\bX$ can be perceived as another draw from $\cL(\bX|\bZ)$ independently from $\bY$. This implies exchangeability of the scores and that the statistic $p$ follows a uniform distribution over the set $\{1/(M+1),2/(M+1),...,M/(M+1),1\}$. The first problem can be noticed here: in order to make any rejection we require $M$ to be at least $1/\alpha$, and for small significance $\alpha$ this may cause computational hurdle since you need to compute the score of each counterfeit. The second problem about the CRT  is the rejection region. Since under the null hypothesis, the statistic $p$ follows a uniform distribution, observing its extreme values (close to 0 or 1) can be interpreted as evidence against the CI hypothesis. 
Now if under an alternative, the statistic $p$ deviates from uniform distribution by showing an inflation on the central range and deflation on the tails, then this deviation is not detected by CRT. In particular this may cause CRT to have power less than $\alpha$ (worse than random guessing).

Our next theorem demonstrates such behavior in a regression setting. For simplicity, it considers $Z= \emptyset$ and proposes a setting where CRT provably gets power less than $c_0 \alpha$, for any arbitrarily small but fixed $c_0>0$. Before proceeding to the theorem, we prove a concentration result for the CRT p-statistic given by~\eqref{eq: p-crt}.

%When the statistic $p$ follows a concave probability density function, e.g. in Figure \ref{fig: hist-crt}, this indeed can be problematic in inferring the conditional dependency. In this scenario (concave density), the likelihood of observing extremely small (near $0$) or large (near $1$) are pretty low. In fact, because of the concavity, the probability of attaining these extreme values, such as $\alpha/2$ upper and lower regions, is going to be smaller than $\alpha$. This means the statistical power is even smaller that the predetermined tolerable type 1 error level. To understand this better, we introduce a general set of CI testing problems that this phenomena ( $p$ with concave density) can happen.  We investigate the CRT performance on a regression setting $y=g(x)+\normal(0,1)$, for two one-dimensional random variables $x,y$, where obviously the covariate $x$ and the response $y$ are highly dependent. In the next theorem, we establish a concentration result for the p-statistic of the CRT in \eqref{eq: p-crt}.

\begin{propo}\label{thm: crt-failure}
Consider an even function $g$, and a dataset $(\bX,\bY)$ of $n$ i.i.d pairs $\{(X_i,Y_i)\}_{i=1}^n$, with $X_i, Y_i\in \reals$, generated from the following regression model:
\begin{align} \label{eq: example:thm}
X&\sim \normal(0,1)\,, \nonumber \\
Y&=g(X)+\eps\,, \quad \eps\sim\normal(0,1)\,.
\end{align}
 For the score function $T(\bX,\bY)= \||\bX-\bY\||_2^2$, and counterfeit datasets $\tbX_{j}\sim\normal(0,I_{n})$, recall the CRT $p$ statistic:
\begin{align}\label{eq:pMn}
p^{(M)}_n=\frac{1+\sum\limits_{j=1}^{M}\ind_{ \{T(\bX,\bY)\geq T(\tbX_{j},\bY)\} } }{M+1}\,.
\end{align}
Then, the statistic $p^{(M)}_n$ concentrates around $1/2$. In particular, for any $\delta>0$ and $M>1/\delta$, we have
\[
\lim\limits_{n\rightarrow \infty}^{}\prob\left(\left|  p^{(M)}_n -1/2 \right| \geq \delta \right) \leq  \frac{1}{\left( \delta-1/M\right)^2} \left(   \frac{1}{4M} + \frac{M-1}{M} \Big(\E_{Z\sim\normal(0,1)}[\Phi^2(\eta Z)] -\frac{1}{4}\Big)  \right)\,,
\]
with $\eta=\left(\frac{3+2\E[X^2g(X)^2]}{3+2\E[g(X)^2]}\right)^{1/2}$.
 \end{propo}
 We refer to Section~\ref{proof:thm: crt-failure} for the proof of this proposition.
  
 The next corollary considers a set of even functions parametrized by $\theta$ and shows that for every significance level  $\alpha$, by having $\theta$ small enough (depending on $\alpha$), the power of CRT is smaller than $\alpha/2$.
\begin{thm}\label{coro: failure-crt}
 %Under similar settings as in Theorem \ref{thm: crt-failure}
  For an even function $g$,  consider the following model between the response variable $Y$ and covariate $X$:
  \begin{align*} 
X&\sim \normal(0,1)\,, \nonumber \\
Y&=g(X)+\eps\,,\quad \eps\sim \normal(0,1)\,,
\end{align*}
with the regression function $g(x)=\frac{1}{\sqrt{\theta^2+x^2}}$ 
%for the infinite number of data points and unlimited computational power ( infinite number of samplings and  counterfeits) 
Then, the followings hold:
 \begin{itemize} 
  \item[$(a)$]For any $\alpha\in (0,1/2)$, and small enough $\theta$ such that  $\theta\leq \alpha^2/500$,  the two-sided CRT at significance level $\alpha$ (reject $\alpha/2$-th upper and lower quantiles) has power smaller than $\alpha/2$. Formally, 
\[
\lim\limits_{M\rightarrow \infty}^{} \lim\limits_{n\rightarrow \infty}^{}\prob\Big(  \Big|p_{n}^{(M)}  -\frac{1}{2} \Big|   \geq  \frac{1-\alpha}{2} \Big)\leq  \alpha/2 \,.
\]    
   
    \item[$(b)$] For any $\alpha\in (0,1/2-\gamma)$ with $\gamma>0$, and for small enough $\theta$ such that $\theta\leq \gamma^4\alpha^2/2$, the one-sided CRT at significance level $\alpha$ (rejecting either $\alpha$-th upper or lower quantile) has power smaller than $\alpha/2$. Formally,
    %
    \begin{align*}
\lim\limits_{M\rightarrow \infty}^{} \lim\limits_{n\rightarrow \infty}^{}\prob\left(  p_{n}^{(M)}   \geq  1-\alpha \right) \leq \frac{\alpha}{2}\,,\quad
\lim\limits_{M\rightarrow \infty}^{} \lim\limits_{n\rightarrow \infty}^{}\prob\left(  p_{n}^{(M)}   \leq  \alpha \right) \leq \frac{\alpha}{2}\,.
\end{align*}
\end{itemize}
 %\[
 %3\sqrt{\pi}-4+\sqrt{2}\pi/c \geq \frac{80}{\alpha\sqrt{\pi}} 
 %\]

 %the probability that the test statistic $p$  lies in the interval $(\alpha/2, 1-\alpha/2)$ is larger than $1-\alpha/2$. This implies that under this setting
\end{thm}
We refer to Section~\ref{proof:coro: failure-crt} for the proof of Theorem~\ref{coro: failure-crt}.

%\subsection{Contributions}
 %For this model-X setup, we propose the Pearson-$\chi^2$-conditional independence (CI) testing. Further, in Theorem \ref{thm: power_balls_bins}, we evaluate the size and power of this test... 
%\section{Problem formulation}






%\section{Main results}
 % Algorithm \ref{algorithm: model-free} proposes a test statistics that under the null hypothesis will contro thel false positive rate. More detail can be seen in Theorem \ref{thm: model-free}.
%\subsection{Model-$X$ conditional independence testing}
%CI testing in the semi-supervised learning framework, such as genome-wide association studies (GWAS), has recently attracted much attention; in this setting, a scarce number of labeled data points $(X,Z,Y)$ are available, while an abundant amount of data on unlabeled covariates $(X,Z)$ is in hand. The model-X approach to CI assumes access to the law of $X|Z$ and proposes tests based on the discrepancy between the behavior of the initial triple $(X,Z,Y)$ and its counterfeits, constructed by replacing $X$ with an independent draw from $X|Z$ law. 








\section{Pearson Chi-squared randomization (PCR) test}\label{sec:PCR}
Motivated by the issues of CRT discussed in the previous section, in this work we propose a novel test, called Pearson $\chi^2$ conditional   randomization (PCR) test.
%Con an arbitrary score function $T:\reals^{q+2} \rightarrow \reals$ that satisfies certain regularity conditions (will be provided later),  we propose the Pearson $\chi^2-$CI testing procedure. 
We start by describing the PCR test and its test statistic. We then characterize the null distribution of its statistic by which we propose two rejection thresholds, for finite and infinite sample regimes.
%We will characterize the asymptotic distribution of the Pearson $\chi^2$-CI test statistic, and provide two rejection thresholds that each controls the type I error rate in finite and asymptotic number of samples.  We start by describing the Pearson $\chi^2-$CI test statistic construction.
 
 
 
 
\subsection{PCR test statistic}
We construct the PCR test statistic in three main steps: 
\begin{description}
\item[Counterfeit sampling.] This is a common step in model-X conditional independence testing methods, where for each data point $(X_j,Y_j, Z_j)$ several counterfeits of the form $(\tX_j,Y_j,Z_j)$ are constructed by sampling $\tX_j\sim \cL_{X|Z}$ while keeping $Y_j, Z_j$ intact.  As we will discuss a main distinction of our PCR test with other CRT approach is that the PCR test works with few number of counterfeits while, in CRT approach, one requires a large number of counterfeits (at least of order $1/\alpha$), given that the normalized rank statistic \eqref{eq: p-crt} is intrinsically discrete.   
%
% a significant distinction with other methods is the number of counterfeits required per each sample. In particular, the Pearson CI test works well with very few number of counterfeits per each sample, while in the CRTs, the final test statistic \eqref{eq: p-crt} is intrinsically discrete and requires a large number of counterfeits ($M$) to have valid statistical results.

\item[ Score and label.] Given a score function $T$, we score each data point $(X_j,Y_j,Z_j)$ and then label the data points based on the relative location of the score of the original data point among the scores of its counterfeits. Specifically, we partition the range of possible ranks in to $L$ subsets, $S_1, \dotsc, S_L$, of equal size and assign label $\ell$ to data points whose score rank falls in $S_{\ell}$.  
Special cases of this idea (with $L=2$ labels and unbalanced groups) can be traced in conformal inference literature \cite{vovk2005algorithmic, lei2018distribution, lei2014distribution, romano2019conformalized}, where the sample quantile of the non-conformity scores are compared to a certain threshold to construct prediction intervals.
 
\item[Uniformity testing in a multinomial model.] Under the null hypothesis~\eqref{eq: CI hypothesis}, by using the exchangeability of data scores and their counterfeits scores, it is straightforward to see that each label occurs with equal frequency (with expected count of each label  being $n/L$). In this step, we use the Pearson Chi-squared test statistic $U_{n,L}$ to test uniformity of label occurrences in a multinomial model with $n$ samples and $L$ labels. Note that, in general $L$ can scale with $n$, and as discussed in  \cite{balakrishnan2019hypothesis}, the $\chi^2$ test can have bad power due to the fact that the variance of the $\chi^2$ statistics is dominated by small entries of the multinomial. A truncated version of $\chi^2$ statistic has been proposed by \cite{balakrishnan2019hypothesis} to mitigate this issue by limiting the contribution to the variance from each label. However, when testing for a uniform distribution, as in our case, the truncation becomes superfluous. This implies that in this case, the usual $\chi^2$ statistic inherits several appealing properties of the truncated $\chi^2$ statistic. In particular, \cite{balakrishnan2019hypothesis} showed that
truncated $\chi^2$ test is globally minimax optimal for the multinomial problem. It is worth noting that for the multinomial testing problem in high dimension ($L$ growing with $n$), the upper and lower bounds on the  critical radius $\eps$ has been established in \cite{paninski2008coincidence, valiant2017automatic}. 
Concretely,    
%Similar phenomenon for uniformity testing in the high-dimensional setting $(\ell>n)$ in multinomial models is also noted in \cite{paninski2008coincidence, valiant2017automatic}, where 
it has been shown that $O(\sqrt{L}/\eps^2)$ number of samples are sufficient and information-theoretically necessary for distinguishing uniform distributions from alternatives that are $\eps$ far in the $\ell_1$-ball, with success probability larger than $2/3$. 
\end{description}

A detailed description for construction of the PCR statistic is given in Algorithm~\ref{algorithm: model-xz}.

%1) counterfeit sampling;  2) scoring and labeling data points;  3) constructing a test statistic for uniformity testing in multinomial models.   
%In the second step, after scoring data points by the score function, based on the relative locations of the original scores among the counterfeits, each data point will be labeled. The special case of this step can be seen in conformal inference \cite{vovk2005algorithmic, lei2018distribution, lei2014distribution, romano2019conformalized}, where they rank non-conformity scores and choose a sample quantile at a certain level $\alpha$ as a threshold to construct prediction intervals. Here, we partition the whole sample quantile interval to $\ell$ homogenous regions (later refer to them as labels), in contrast to two non-uniform regions in predictive inference methods.     
% 
%In the last step, we use the Pearson Chi-squared test statistic $U_{n,\ell}$ for uniformity testing in a multinomial model with $n$ samples and $\ell$ labels, i.e. each sample accepts value over the set $[\ell]$.  In fact, the Pearson Chi-squared test statistic matches the truncated Chi-squared
%test statistic which is defined in \cite{balakrishnan2019hypothesis}, moreover the same work shows that Pearson Chi-squared test statistic is globally minimax optimal for uniformity testing in multinomials. A similar phenomenon for uniformity testing in the high-dimensional setting $(\ell>n)$ in multinomial models is also noted in \cite{paninski2008coincidence, valiant2017automatic}, where it's been reported that $O(\sqrt{\ell}/\eps^2)$ number of samples are sufficient and information-theoretically necessary for distinguishing uniform distributions from the neighbors in the $\ell_1$-ball with radius $\eps$ with success probability larger than $2/3$. 
\begin{algorithmic}[!h]
	\begin{algorithm}
		\SetAlgoLined
		\REQUIRE $n$ data points $(X_j,Y_j,Z_j)\in \reals \times \reals\times \reals^q$, a real-valued score function $T: \reals \times \reals\times \reals^q\mapsto \reals$, and integers $K,L\geq1$ (let $M=KL-1$).
		\ENSURE Test statistics $U_{n,L}$ for testing the conditional independence hypothesis \eqref{eq: CI hypothesis}. \\
			\For{$j\in [n]$}{
			\begin{itemize}
			\item  Draw $M$ i.i.d. samples $\tX_j^{(1)},..., \tX_j^{(M)}$ from $\cL_{X|Z}(\cdot|Z_j)$.
			\item Use $T$ to score the initial data point $(X_j,Y_j,Z_j)$ and its $M$ counterfeits $(\tX_j^{(1:M)},Y_j,Z_j)$:
			\begin{eqnarray*}
			T_j &=&T(X_j,Y_j,Z_j)\,,\\
			\widetilde{T}_j^{(i)} &=& T(\tX_j^{(i)},Y_j,Z_j),\quad\text{ for } i \in[M]\,.
		    \end{eqnarray*}
	    \item Let $R_j$ denote the rank of $T_j$ among $\{T_j,\tT_j^{(1)},...,\tT_j^{(M)}\} $:
	    \[
	  R_j=  1+\sum\limits_{i=1}^{M}\ind_{\left\{T_j \geq \tT_j^{(i)}\right \}}
	    \]
		\item Partition $[M+1] = S_1\cup \dotsc \cup S_L$ with $S_{\ell} := \{(\ell-1)K+1, \dotsc, \ell K\}$.
		Assign label $\ell_j\in \{ 1,2,...,L\}$ to sample $j$ if $R_j\in S_{\ell_j}$.
		
				\end{itemize}
}

\For{$\ell\in \{1,2,\dotsc,L\}$}{
			\begin{itemize}
				\item Let $W_{\ell}$ be the number of samples with label $\ell$: \,	$W_{\ell}:=\Big|\big \{j\in \{1,2,...,n\}: \ell_j=\ell \big\}  \Big|\,.$
				
				\end{itemize}
		}
	\begin{itemize}[leftmargin = *]
\item
	Define the test statistic $U_{n,L}$ as follows
		\begin{align}\label{eq:U_nell}
		U_{n,L}=\frac{L}{n}\sum\limits_{\ell=1}^{L}\left(W_{\ell}-\frac{n}{L}\right)^2\,.
		\end{align}	
%\item At significance level $\alpha$, reject $H_0$ if $U_{n,\alpha}\geq \ell+\sqrt{\frac{2\ell}{\alpha}}$. 

%with $c_{m,1-\alpha}$ being the $\alpha$-th upper quantile of Chi-squared distribution with $m$ degrees of freedom. 
	\end{itemize}		
				\caption{PCR test statistic}\label{algorithm: model-xz}
	\end{algorithm}
\end{algorithmic}


\subsection{Decision rule} We introduce two rejection thresholds for the hypothesis testing problem \eqref{eq: CI hypothesis} with the statistic $U_{n,L}$ given by~\eqref{eq:U_nell}. At significance level $\alpha$, the decision rule is based on the test statistic:
\begin{equation}\label{eq:decision rule}
\phi\left(\bX,\bZ,\bY\right)=\begin{cases} 1 & U_{n,L}\geq \th_{L,\alpha}\quad (\text{reject } H_0)\,,\\
	0&  \text{otherwise}\quad ~~(\text{accept }H_0)\,. \
\end{cases}
\end{equation}
For the threshold $\theta_{L,\alpha}$ we consider two proposals:
\begin{align}\label{eq:thresholds}
 \th_{L,\alpha}^{\mathsf{asym}}:= \chi^2_{L-1}(1-\alpha), 
 \quad \th_{L,\alpha}^{\mathsf{finite}}=L+\sqrt{\frac{2L}{\alpha}}\,,
 \end{align}
 where $\chi^2_{L-1}(1-\alpha)$
denotes the $1-\alpha$ quantile of a $\chi^2$ distribution with $L-1$ degrees of freedom.
%\item $\th_{\ell,\alpha}^{\mathsf{finite}}=\ell+\sqrt{\frac{2\ell}{\alpha}}$
As we show in the next section, the size of PCR test is controlled asymptotically (as $n\to \infty$) with using $\th_{L,\alpha}^{\mathsf{asym}}$. In addition, by using $\th_{L,\alpha}^{\mathsf{finite}}$, we prove that the size is controlled at finite sample settings.


As clear form its description, and similar to the CRT, the PCR test looks for statistically significant deviations between the distribution of the rank of original scores and the uniform distribution. While CRT only examines the tails of the distributions, the PCR test examines the entire support by comparing the two distributions on $L$ bins (corresponding to labels) of equal size and is able to capture deviations occurring in the middle range as well as at the tails.
\bigskip

 \noindent
 \textbf{Revisiting the numerical example in Section \ref{subsec: CRT fails}. }
Recall the numerical example in Section \ref{subsec: CRT fails}, with $n=1000$ data points $\{(X_i,Z_i,Y_i)\}_{i=1}^n$ generated from the conditional laws \eqref{eq: example}. As we saw, the CRT is powerless in this setting. Here, we run the PCR test on the same example, with $L=5$ and different values for $K$.  The number of counterfeits per each sample is therefore $M=5K-1$. We consider both of the rejection thresholds $\th^{\mathsf{asym}},\th^{\mathsf{finite}}$ for decision rule \eqref{eq:decision rule}. Table \ref{table: pearson-example} reports the power of PCR test for different values of $K$ (so different values of counterfeits.) As we see, it has high power with both of the thresholds and for various choices of $K$. This is an illustration of the PCR test overcoming the issues with the CRT test as discussed in Section~\ref{subsec: CRT fails}.

\begin{table}
\begin{center}
\begin{tabular} {|c|c|c|c|c|}
	\hline
	  Setup         & $K=1$  & $K=4$ & $K=10$ & $K=20$ \\   \hline
 $\chi^2$-CI test with $\th^{\mathsf{finite}}_{L,\alpha}$&0.8856 &   0.9941 &0.9981&0.9993 \\\hline 
$\chi^2$-CI test with $\th^{\mathsf{asym}}_{L,\alpha}$&0.9926 &  1.00 &1.00& 1.00 \\ \hline
	\end{tabular}
\end{center}
\caption{Statistical power of the PCR test applied to a dataset of size $n=1000$ generated according to the regression setting in Section \ref {subsec: CRT fails}.  PCR test is used with two rejection thresholds $\th_{L,\alpha}^{\mathsf{finite}}$, $\th_{L,\alpha}^{\mathsf{asymp}}$ with number of labels $L=5$ and at significance level $\alpha=0.1$. Different values of $K$ are chosen which determine the number of counterfeits per each sample as $M=KL-1$. Reported values are averaged out over $10000$ experiments. }\label{table: pearson-example}
\end{table}

%
% Let $\th_{\ell,\alpha}^{\mathsf{asym}}$ be equal to the $\alpha-$th upper quantile of a Chi-squared distribution with $\ell-1$ degrees of freedom, and
% $\th_{\ell,\alpha}^{\mathsf{finite}}=\ell+\sqrt{\frac{2\ell}{\alpha}}$. We can use either $\th_{\ell,\alpha}^{\mathsf{asym}}$  or  $\th_{\ell,\alpha}^{\mathsf{finite}}$ in the above decision rule. In the next section, we provide guarantees on the rates of falsely rejecting the null hypothesis when the null hypothesis indeed holds (Type I error) for decision rule \eqref{eq:decision rule}, when either $\th_{\ell,\alpha}^{\mathsf{asym}}$ or $\th_{\ell,\alpha}^{\mathsf{finite}}$ is being deployed as the rejection threshold.
 
 
 \iffalse
 We start by proposing some regularity assumptions on score function $T$. First, suppose each data point $(X_i,Y_i,Z_i)$ is generated independently from a density function $q(x,z,y)$ with the following decomposition: $q(x,z,y)=p_{Z}(z)p_{X|Z}(x|z)p_{Y|XZ}(y|xz)$. Further, define $p(x,z,y):=p_{Z}(z)p_{X|Z}(x|z)p_{Y|Z}(y|z)$ with 
$$p_{Y|Z}(y|z):=\int p_{Y|XZ}(y|x,z)p_{X|Z}(x|z)\nu(dx)\,,$$
for a common measure $\nu(.)$. It is easy to observe that, for $(X,Z,Y)$ coming from the density function $p(.)$, we have $X \indep Y|Z$. 
\fi


 \subsection{Size of the PCR test}\label{sec:size}
 
 
 Under the null hypothesis, the original and counterfeit scores are coming from a similar population. Our next assumption on the continuity of random variables ensures that the different data points achieve distinct score values, with probability one. This symmetry on distinct values implies that each data point gets label $\ell \in[L]$ uniformly at random. In short, we change the problem of conditional independence testing into the uniformity testing problem on data points coming from a multinomial distribution. 
 %this is due to the aforementioned fact that under the conditional independence, data points are distributed uniformly on $\ell$ labels. 

\begin{assumption}\label{assum: mu_mapping}
	For a score function $T$, assume that the following conditional cumulative distribution functions are continuous, for every pair $(y,z)$:
	\begin{eqnarray}
	F_{T|ZY}(t;z,y)&:=&\prob_{{X|ZY}}\left(T(X,z,y)\leq t|Z=z,Y=y\right)\,,\label{eq:FZY}\\
	F_{T|Z}(t;z,y)&:=&\prob_{{X|Z}}\left(T(X,z,y)\leq t|Z=z,Y=y\right)\,.\label{eq:FZ}
	\end{eqnarray}
	Note that both $F_{T|ZY}$ and $F_{T|Z}$ are conditional on $Y,Z$, and randomness is coming from $X$. The difference is that in $F_{T|ZY}$, we have $X\sim \cL(X|ZY)$, while in $F_{T|Z}$, we have $X\sim \cL(X|Z)$.  
\end{assumption}	


It is worth noting that the above assumption, which is used to transform the conditional independence testing problem into a multinomial uniformity testing problem, is indeed a weak assumption. It is used to avoid ties when ranking the scores, and alternatively one can use a random tie-breaking decision rule and remove this assumption.  

In the next theorem, we show that by using $\th_{L,\alpha}^{\mathsf{asym}}$ in the decision rule \eqref{eq:decision rule} asymptotic control on type I error is guaranteed. It is an immediate consequence of characterizing the asymptotic distribution of $U_{n,L}$ statistic in Algorithm \ref{algorithm: model-xz}. Furthermore,  we show that deploying the rejection threshold  $\th_{L,\alpha}^{\mathsf{finite}}$ results in finite-sample control on the type I error.  
 %\begin{align*}
%&\lim\limits_{n\rightarrow \infty}^{}\prob\left( U_{n,\ell} \geq \th_{\ell,\alpha}^{\mathsf{asym}}| H_0 \text{ holds }\right) \leq \alpha\,,\\
% &~~\prob\left( U_{n,\ell} \geq \th_{\ell,\alpha}^{\mathsf{finite}}| H_0 \text{ holds }\right) \leq \alpha\,.
%\end{align*}
 \begin{thm}\label{thm: chi^2-CI-size}
 	Under the null hypothesis \eqref{eq: CI hypothesis} and Assumption \eqref{assum: mu_mapping} , the statistic $U_{n,L}$ constructed in Algorithm \ref{algorithm: model-xz} will converge to $\chi^2$ distribution with $L-1$ degrees of freedom, for $L\ge 2$: 
 	\[
 	U_{n,L}\overset{d}{\Rightarrow} \chi^2_{L-1}, \quad\text{ as }n\rightarrow\infty\,.
 	\] 
 In addition, for every $n\ge 1$, we have 
 \[
 \prob\left( U_{n,L} \geq \th_{L,\alpha}^{\mathsf{finite}} \right) \leq \alpha\,,
 \]
 with $\th_{L,\alpha}^{\mathsf{finite}}=L+\sqrt{\frac{2L}{\alpha}}$.
\end{thm} 
We refer to Section~\ref{proof:thm: chi^2-CI-size} for the proof of Theorem~\ref{thm: chi^2-CI-size}.


Based on the above characterization of the null distribution, in finite sample and asymptotic regimes, we can construct the following $p$-values for the testing problem~\eqref{eq: CI hypothesis}:
%
% statistic $U_{n,\ell}$ and the decision rule~\eqref{eq:decision rule} with $\th^{\mathsf{asym}}_{n,\ell}$ and $ \th^{\mathsf{finite}}_{n,\ell}$ thresholds  we can construct the following $p-$values $P^{\mathsf{asym}}_{n,\ell}$ and $P^{\mathsf{finite}}_{n,\ell}$:
\begin{equation}\label{eq: p-val}
P^{\mathsf{finite}}_{n,L}=\begin{cases}
	1,& U_{n,L} \leq L\,,\\
	\min\left\{\dfrac{2L}{(U_{n,L}-L)^2},1 \right\},&\text{otherwise}\,.
\end{cases}
\end{equation}
\begin{equation}\label{eq: p-val-asymptotic}
P^{\mathsf{asym}}_{n,L} =1-{\sf F}_{L-1}(U_{n,L})\,,
\end{equation}
%
where ${\sf F}_k$ is the cdf of a chi-squared random variable with $k$ degrees of freedom.

Note that under the null hypothesis, $p$-value $P^{\mathsf{asym}}_{n,L}$ is asymptotically uniform, whereas $P^{\mathsf{finite}}_{n,L}$ is super-uniform for finite $n$. Formally, for all $t\in [0,1]$
\begin{align*}
&\lim\limits_{n\rightarrow \infty}^{} \prob\left( P^{\mathsf{asym}}_{n,L} \leq t \right)=t\,, \\ 
&\prob\left( P^{\mathsf{finite}}_{n,L} \leq t \right)\le t\,, \quad \forall n\ge 1\,.
\end{align*}
%i.e., $\lim\limits_{n\rightarrow \infty}^{} \prob\left( P^{\mathsf{asym}}_{n,\ell} \leq \alpha \right)=\alpha$, for all $\alph\in [0,1]$.  the p-value $P^{\mathsf{finite}}_{n,\ell}$ stochastically dominates the uniform distribution, and  it is more conservative. 
% Note that p-value $P$ constructed above is valid for all number of sample values $n$,  more precisely under the null hypothesis \eqref{eq: CI hypothesis} we have $\prob\left(P\leq \alpha\right)\leq \alpha$.  This is an immediate result of Theorem \ref{thm: chi^2-CI-size}. It worth noting that p-value $P$ is extracted from decision rule \eqref{eq:decision rule} with rejection threshold $\th_{\ell,\alpha}^{\mathsf{finite}}$. Using rejection threshold $\th_{\ell,\alpha}^{\mathsf{asym}}$ gives $p-$value $\tP=1-F_{\chi^2_{\ell-1}}(U_{n,\ell})$ which is  asymptoticly uniform, i.e $\lim\limits_{n\rightarrow \infty}^{}\prob(\tP\leq \alpha)=\alpha$ (here $F_{\chi^2_{k}}(.)$ is cdf of Chi-squared distribution with $k$ degrees of freedom.)\\\\
 








\section{A power analysis of the PCR test}\label{sec: pwr}
We next provide a power analysis of the PCR test . To this end, we need a notion of distance between a probability density function $p_{XZY}(x,z,y)$ and its corresponding conditional independence density $p_X(x)p_{Z|X}(z|x)p_{Y|Z}(y|z)$, where $p_{Y|Z}(y|z)$ is obtained by marginalizing out $X$, i.e.,
$p_{Y|Z}(y|z)= \int p_{Y|XZ}(y|x,z)p_{X|Z}(x|z) \de x$. As expected, the larger this distance, the easier to discern the conditional dependency. The metric that we use here to analyze the power of PCR test is a generalization of the notion of \emph{ordinal dominance curve} (ODC) \cite{hsieh1996nonparametric, bamber1975area}. For two densities $p$ and $q$ defined on the real line, the ODC is given by $F_p(F_q^{-1}(t))$, where $F_p, F_q$ respectively denote the cdfs corresponding to $p$ and $q$. In other words, the ODC is the population analogous of the PP plot. The derivative of the ODC (if exists) is given by $f_p(F_q^{-1}(t))/f_q(F^{-1}_q(t))$ and is called the \emph{relative density function} (\cite{thas2010comparing}, Section 2.4). 

We next define the conditional ODC and the conditional relative density function, along with two assumptions. Let us emphasize that the upcoming assumptions are made to facilitate the power analysis, and the validity of the PCR test (control on type I error) holds even without these assumptions. 
\begin{defi}\label{def: conditonal-odc}(Conditional ODC and relative density function). For a score function $T$, recall the conditional cdfs $F_{T|ZY}(t;z,y)$ and $F_{T|Z}(t;z,y)$ given by equations~\eqref{eq:FZY} and \eqref{eq:FZ}. We define the conditional ordinal dominance curve $\sR_T:[0,1]\rightarrow [0,1]$ as follows:
	\[
	\sR_{T}(u)=\E_{(Z,Y) \sim \cL(Z,Y)}\left[ F_{T|ZY}\left( F_{T|Z}^{-1}\big(u; Z,Y\big) ; Z,Y\right)  \right]\,.
	\]
	For Differentiable $\sR_{T}$, we 
	 call its derivative the conditional relative density function:
	 $\sr_{T}(u):=\frac{\partial}{\partial u}\sR_{T}(u)\,,$ for $u\in (0,1)$.
	 	
\end{defi}

\begin{assumption}\label{assum: sr_continuity}
	Assume the conditional relative density function $\sr_{T}(u)$ is  $C$-Lipschitz continuous. This also implies that $\sr_{T}(u)$ is uniformly bounded, i.e.,
	\[\sup\limits_{u \in [0,1]} ^{} |\sr_{T}(u)| \leq B\,,\]
	for some constant $B>0$.
	\end{assumption}
	
Our next assumption is a sufficient condition to replace the order of the expectation and the derivative in the definition of $\sr_{T}(u)$ (see~\eqref{eq:interchange}). 
\begin{assumption}\label{assum: dR-L1-integrable}
We assume that
\[
\int_0^1 \E_{(Z,Y) \sim \cL(Z,Y)}\Big[ \Big|\frac{\partial}{\partial u}F_{T|ZY}\Big( F_{T|Z}^{-1}\big(u; Z,Y\big) ; Z,Y\Big)  \Big|  \Big] \de u <\infty \,.
\]
\end{assumption}

We are now ready to define a distance between the distribution of $(X,Z,Y)$ and $(\tX,Z,Y)$ where $\tX\sim\cL(X|Z)$, independently of $Y$. Note that the two densities match under the null hypothesis~\eqref{eq: CI hypothesis}. 


\begin{defi}\label{def: conditional-dependency-power}
	For a score function $T$ and its relative density function $\sr_{T}(.)$, define \textit{conditional dependency power}  as 
	\[
	\Delta_T(\cL(X,Z,Y))=\int\limits_{0}^{1}|\sr_{T}(u)-1|\de u\,.
	\]
\end{defi}	
%
	We next state some properties of the measure $\Delta_T(\cL(X,Z,Y))$. Recall that for two random variables $U, V$ with density functions $p, q$ (with respect to the Lebesgue's measure), the total variation distance is defines as
	\[
	d_{\tv} = \frac{1}{2}\int_{-\infty}^\infty |p(t) - q(t)|\de t\,.
	\]
	
	\begin{rmk}\label{rmk: Delta upper bound} The followings hold for the measure $\Delta_T(\cL(X,Z,Y))$.
	\begin{itemize}
	\item[(a)] Under the null hypothesis \eqref{eq: CI hypothesis}, for any score function $T$ satisfying Assumption \ref{assum: mu_mapping} we have $\Delta_{T}(\cL(X,Z,Y))= 0$.
	\item[(b)] The following upper bound holds in general:
	\[
		\Delta_T(\cL(X,Z,Y))\leq \E_{(Z,Y) \sim \cL(Z,Y)}\left[2 d_{\tv}\left(  (T(\tX,Z,Y)|Z,Y), (T(X,Z,Y)|Z,Y) \right)\right],		\]
		with $X\sim \cL(X|Z,Y)$ and $\tX\sim \cL(X|Z)$.
	\end{itemize}
	\end{rmk}
We refer to Section~\ref{proof:rmk: Delta upper bound} for the proof of Remark~\ref{rmk: Delta upper bound}.



%The basic idea for Algorithm $\ref{algorithm: model-xz}$ lies in the fact that under the null hypothesis $H_0$ so that $p=q$, the primary value $\mu(X,Z,Y)$ along with the other $m$ extracted versions $\mu(\tX^{(1)},Z,Y), \mu(\tX^{(2)},Z,Y),...,\mu(\tX^{(m)},Z,Y)$ are exchangeable, where $\tX_j^{(\ell)}$ are i.i.d. from $P_{X|Z}(.|Z)$. In case of dependency between $X,Y$ in the presence of $Z$, there is a mismatch between distribution of $\mu(X,Z,Y)$ and $m$ other $\mu(\tX^{\ell},Z,Y)$ values, Theorem \ref{thm: power_balls_bins} depicts the effect of this mismatch on the power of test statistics $T$. 


	%It is worth noting that in Algorithm \ref{algorithm: model-xz}, the value $S_j$ denotes the location of $\mu(X_j,.)$ among ascendingly sorted values of the set $\left\{\mu(X,.), \mu(\tX_j^{1},.),...,\mu(\tX_j^{(M)},.)\right\}$. This location admits a value from the set $\{1,2,...,M+1 \}$; later $Y_\ell$ represents normalized number of data points $j$ that $S_j=\ell$.


%Split randomly on hand data points into two disjoint groups of lengths $n_1,n_2$ with $n_1+n_2=n$ (guidelines for choosing $n_1, n_2$ will be provided later in the next section). Apply mapping $\mu$ on each one of $n_1$ members and call them  $A_i:=\mu(X_i,Y_i,Z_i)$. For each data point $j$ in the second group (length $n_2$), replace feaure $X_j$ with a new value $\tX_j$ coming from  probability law $P_{X|Z}(.|Z_j)$, and put $B_j:=\mu(\tX_j,Z_j,Y_j)$.
%It is easy to see that each $A_i$ ( for $1\leq i \leq n_1$) has cdf $F_Q$ and each $B_j$( for $1\leq j \leq n_2$) has cdf $F_P$. 
%\begin{align*}
%\{A_i\}_{i\in \cI_1} \sim Q^{\otimes n_1}\,,\\
%\{B_j\}_{j \in \cI_2} \sim P^{\otimes n_2}\,.
%\end{align*}
%It is clear that under the null hypothesis that $X\indep Y|Z$, we have $p=q$, hence $F_P=F_Q$. Therefore, we can consider the following hypothesis testing problem
%\begin{align}\label{eq: hypothesis}
  %  H_0: F_P=F_Q\,, \quad\quad\quad H_A: F_P\neq F_Q.
%\end{align}
\noindent
 
 


As discussed earlier, the PCR test transforms the conditional independence problem into the problem of uniformity testing under a multinomial model. That said, in order to analyze the power of PCR test we focus on the later problem.
We use the results of~\cite{balakrishnan2019hypothesis} which characterize the power of truncated $\chi^2$-test for a high-dimensional multinomial model, in terms of the $\ell_1$ distance between the nominal probabilities and the uniform distribution over the categories. However, it is not clear how the nominal probabilities in the multinomial model are related to the distribution of $(X,Z,Y)$ in the original conditional independence testing problem. Our next proposition answers this question and relates the $\ell_1$ distance between the nominal probabilities and the discrete uniform distribution, in the multinomial problem, to the measure $\Delta_T(\cL(X,Z,Y))$ given in Definition~\ref{def: conditional-dependency-power}.


% and relate the  we need to capture the deviation of nominal probabilities from the uniformity in the multinomial model. The next proposition demonstrates a lower bound on $\ell_1$ distance of the transformed multinomial model from the uniform multinomial distribution. In fact, this lower bound is highly dependent on the conditional dependency power of the primary conditional independence problem, and implies that the higher the dependency power, the larger is the deviation.
%We first propose a closed-form relation on the probability that one triple $(X,Z,Y)$ gets label $s\in \{1,2,...,\ell\}$, then we will use this and establish the aforementioned  lower bound. 
\begin{propo}\label{propo: lambda_bound}
Under Assumption \ref{assum: dR-L1-integrable}, in Algorithm \ref{algorithm: model-xz}, each data point $(X_i,Z_i,Y_i)\sim \cL(X,Y,Z)$ admits label $s\in \{1,2,...,L\}$, independently from other data points with probability
	\begin{equation}\label{eq: tp_j}
		{p}_s= \sum\limits_{j=(s-1)K}^{sK-1} \binom{M}{j}\int\limits_{0}^{1}u^{j}\big(1-u\big)^{M-j}r_{T}(u)\de u\,,
	\end{equation} 
where $\sr_T(.)$ is the conditional relative density function given by Definition \ref{def: conditonal-odc}. Under the null hypothesis \eqref{eq: CI hypothesis}, we have $p_{s}=\frac{1}{L}$. In addition, under Assumption \ref{assum: sr_continuity}, the partial sums of $\{p_s\}_{s=1}^\ell$  satisfies the following bounds:
	\begin{enumerate} [label=\roman*)]
		\item For every $\ell\in [L]$, we have 
		\begin{equation}\label{eq:sum-p_ell-lower}
			\sum\limits_{s=1}^{\ell}p_s \geq \sR_{T}\left(\frac{\ell}{L}\right)\,,
		\end{equation}
where $\sR_T(u)$ is the conditional dominance curve given by Definition \ref{def: conditonal-odc}.
		\item Let $D=C/2+2B$ with $B,C$ given according to Assumption \ref{assum: sr_continuity} and introduce $\nu_K:= 2\left(\frac{4D^2\log{K}}{\sqrt{K}}  \right)^{2/5}$. Then for $K$ sufficiently large such that $\nu_K<1$, we have 
		\begin{equation}\label{eq:sum-p_ell-upper}
			\sum\limits_{s=1}^{\ell}p_s \leq \sR_{T}\left(\frac{\ell}{L}\right)+\nu_K
			\,.
		\end{equation}
		\item 
		We have
		\begin{equation}\label{eq: tmp10}
		\sum\limits_{s=1}^{L}\Big|p_s-\frac{1}{L}\Big|\geq 
		\left( \Delta_T(\cL(X,Z,Y)) -L\nu_k-\frac{C}{L} \right)\,.	
		\end{equation}
%\item 
%If the relative density function $r_T(u)$ satisfies a higher order smoothness condition, then in terms of the number of samples $\ell$, a better rate can be established in \eqref{eq: tmp10}. More precisely, if 
% $\sr_T(u)\in C^m{[0,1]}$ for some integer $m\geq 1$, then $C/\ell$ can be replaced with $C_m/\ell^m$ with  $C_m=\max\limits_{u \in [0,1] }^{} \sr_T(u)^{(m)}$ in $\eqref{eq: tmp10}$.
\end{enumerate}
\end{propo}
Proof of Proposition~\ref{propo: lambda_bound} is given in Section~\ref{proof:propo: lambda_bound}.

%Having a lower bound on the discrepancy between the nominal probabilities in the transformed multinomial model and the uniform multinomial distribution, 
With Proposition~\ref{propo: lambda_bound} in place, we are now ready to state the main result about the statistical power of our PCR test. We start by analyzing the power of the PCR test when it is used with the finite-sample threshold $\th^{\mathsf {finite}}_{L,\alpha}$.

\begin{thm}\label{thm: power_balls_bins}
Let $U_{n,L}$ be the PCR test statistic-- output of Algorithm \ref{algorithm: model-xz}, with the number of labels $L$, and number of counterfeits per sample $M$, where $M= KL-1$, and a score function $T$ that satisfies Assumptions  \ref{assum: sr_continuity} and \ref{assum: dR-L1-integrable} with parameters $B,C$.  
%the Pearson $\chi^2-$CI test has size at most $\alpha$, i.e.
%\[
%\prob\left( U_{n,\alpha} \geq \ell+\sqrt{\frac{2\ell}{\alpha}} \right) \leq \alpha,\quad \text{ for all  } P_{X,Z,Y} \text{ s.t } X\indep Y|Z\,.
%\]
Suppose that for some $\beta>0$, the conditional dependency power $\Delta_T(\cL(X,Z,Y))$ satisfies the following:
 \begin{equation}\label{eq: lower Delta}
\Delta_{T}(\cL(X,Z,Y)) \geq \frac{32{L}^{1/4}}{\sqrt{n}}\left( \frac{1}{\sqrt{\alpha}} \vee \frac{1}{\beta} \right)^{1/2} + \frac{C}{L}+L \nu_K \,,
\end{equation}
with $\nu_K=2\left(\frac{4(C/2+2B)^2\log{K}}{\sqrt{K}}  \right)^{2/5}$, for $K$ sufficiently large such that $\nu_K<1$. Then the PCR test, used with the finite-sample threshold
$\th^{\mathsf{finite}}_{L,\alpha}$, achieves a power of at least $1-\beta$. More precisely, for all distributions $\cL(X,Z,Y)$ satisfying \eqref{eq: lower Delta}, we have 
%
\[
\prob\left(U_{n,L} \geq L+\sqrt{\frac{2L}{\alpha}} \right) \geq 1-\beta\quad\,.
\]
\end{thm}
The proof of Theorem~\ref{thm: power_balls_bins} follows from Proposition~\ref{propo: lambda_bound}  and is given in Section~\ref{proof:thm: power_balls_bins}.
 
 We next analyze the PCR test power when it is employed with the asymptotic threshold $\th^{\mathsf{asym}}_{L,\alpha}$.
\begin{thm}\label{thm: power_balls_bins_asympt}
Let $U_{n,L}$ be the PCR test statistic-- output of Algorithm \ref{algorithm: model-xz}, with the number of labels $L$, and number of counterfeits per sample $M$, where $M= KL-1$, and a score function $T$ that satisfies Assumptions  \ref{assum: sr_continuity} and \ref{assum: dR-L1-integrable} with parameters $B,C$.  In addition, suppose that the following lower bound holds for the conditional dependency power $\Delta_{T}(\cL(X,Z,Y) $:
\begin{equation}\label{eq: tmp: power-asympt}
%\Delta_{T}(\cL(X,Z,Y)\geq \frac{\sqrt{2}L^{1/4}}{\sqrt{n}}\cdot\left(\sqrt{\log\frac{1}{\beta}}+\left( \log\frac{1}{\beta} +  \sqrt{\log\frac{1}{\alpha}} \right)^{1/2}  \right) +\frac{C}{L}+L\nu_k\,
\Delta_{T}(\cL(X,Z,Y))\geq \frac{L^{1/4}}{\sqrt{n}}
\cdot\max\left(\sqrt{3\log\frac{1}{\beta}}+\left( 3\log\frac{1}{\beta} +  2\sqrt{\log\frac{1}{\alpha}}+2\log\frac{1}{\alpha} \right)^{1/2} ,~1  \right)+\frac{C}{L}+L\nu_K\,,
\end{equation}
with $\nu_K=2\left(\frac{4(C/2+2B)^2\log{K}}{\sqrt{K}}  \right)^{2/5}$, for $K$ sufficiently large such that $\nu_K<1$.  Then the PCR test deployed with the asymptotic threshold $\th_{L,\alpha}^{\mathsf{asym}}$ has asymptotic statistical power at least $1-\beta$. Formally, for all distributions $\cL(X,Z,Y)$ satisfying \eqref{eq: tmp: power-asympt}, the following holds
% 
 \[
 \lim\limits_{n\rightarrow \infty}^{}\prob\left(U_{n,\ell} \geq \chi^2_{L-1}(1-\alpha) \right) \geq 1-\beta\,.
 \]
 \end{thm}
 
Proof of Theorem~\ref{thm: power_balls_bins_asympt} also uses the results of Proposition~\ref{propo: lambda_bound} and is deferred to Section~\ref{proof:thm: power_balls_bins_asympt}.
 

The next result provides guidelines on the choice of the number of labels $L$, as the number of samples $n$ grows to infinity. 
\begin{rmk}\label{rmk: optimal-L}
%Let $U_{n,L}$ be the PCR test statistic-- output of Algorithm \ref{algorithm: model-xz}, with the number of labels $L$, and number of counterfeits per sample $M$, where $M= KL-1$. Then for both the asymptotic and finite thresholds, the growth rate $L\asymp n^{2/5}$ for number of labels $L$ in the PCR test is optimal .  Specifically, for fixed significance level $\alpha$ and type II error tolerance $\beta$, for $K$ sufficiently large, among all the values of number of labels $L$, the choice $L\asymp n^{2/5}$ has power of at least $1-\beta$ against the largest sets of alternatives. In addition, this set of alternatives is all the laws $\cL(X,Z,Y)$ such that  $\Delta(\cL(X,Z,Y))\gtrsim n^{-2/5}$.
Note that the lower bounds~\eqref{eq: lower Delta} and~\eqref{eq: tmp: power-asympt} on $\Delta_{T}(\cL(X,Z,Y))$ are minimized for $L\asymp n^{2/5}$. This suggests that optimal scaling for the number of labels $L$ in the PCR test (with both the asymptotic threshold and the finite-sample threshold) is $L\asymp n^{2/5}$ which achieves non-trivial power when $\Delta(\cL(X,Z,Y))\gtrsim n^{-2/5}$.
% In addition, the PCR test with the optimal number of labels $L\asymp n^{2/5}$ for all can have the power of at least $1-\beta$.    
\end{rmk}  
%The proof of Remark \ref{rmk: optimal-L} is given in Section \ref{sec: proofs: rmk-optimal-L}.
%
%\begin{equation}\label{eq: lambda_n}
%\lambda_n=n(m+1)\sum\limits_{\ell=1}^{m+1} \left(p_\ell-\frac{1}{m+1}\right)^2\,.
%\end{equation}

%Further,, therefore $T$ converges to central Chi-squared distribution with $m$ degrees of freedom. 

%Further, under the null Hypothesis $H_0: X\indep Y | Z$, the following finite-sample type I error bound holds: 
%\[
%\prob(\text{Reject Null }| H_0 \text{ holds }) \leq \alpha +\frac{250(m+1)^{2.5}}{\sqrt{n}}
%\]



%\begin{equation}\label{eq: lambda}
%\prob\{\text{Reject } H_0 \}=\prob\{W \geq c_{1-\alpha,m}\}\,,
%\end{equation}
%where $W\sim \chi^2(\lambda;m)$ with non-central parameter $\lambda=N(m+1)\sum\limits_{j=1}^{m+1}({p}_j-\frac{1}{m+1})^2$.

%\begin{defi}(Conditional Pearson $\chi^2$-divergence)
%	For two density functions $p(x,z,y), q(x,z,y)$ defined on space of triples $(X,Z,Y)$, introdue conditional $\chi^2$-divergence as follows:
%	....
%	\[
%D_{\chi^2|Z,Y}(p||q)=\E_{(Z,Y)\sim p(z,y)} \left[ \left(  \int\frac{p(x,z,y)}{q(x,z,y)}  \right)^2-1 \right]  
%	\]
%\end{defi}



%Note that when $P=Q$, we have $g(t)=t$. In this case, 
%$\tilde{p}_j=\binom{m}{j-1}B(j,m-j+2)$ with $B$ being the beta function.  By simplifying this expression, we get $\tilde{p}_j=\frac{1}{m+1}$, as we expected.

\iffalse
Figure \ref{fig: theoretical_power} compares  empirical power and theoretical power obtained from equation~\eqref{eq: lambda}.
\begin{figure}
    \centering
    \includegraphics[scale=0.5]{emp-theory.pdf}
    \caption{Comparison of empirical and theoretical power obtained from~\eqref{eq: lambda} for different $Q$ measures and $m$ values. Settings are similar to Figure \ref{fig: power}.} 
    \label{fig: theoretical_power}
\end{figure}
\fi

\section{Parameter-free PCR test }\label{sec:pf}
The PCR test statistic described in Algorithm \ref{algorithm: model-xz} takes the parameters $K$ and $L$ as input. In general, having a large $K$ (for fixed value of $L$) results in large value of $M$ (the number of counterfeits) and hence increases the statistical power of the test because we can better discern the discrepancy between the distribution of the ranks and the discrete uniform distribution. This benefit of course comes at a higher computational cost for constructing the test statistic. The choice of $L$  (total number of labels) is however more subtle. On the one hand, a large value of $L$ implies that many of the labels occur rarely, which makes it challenging to point out significant deviations from the discrete uniform distribution (too many weak effects). On the other hand, a small value of $L$ results in a few bins over which we are comparing the test statistic with discrete uniform. In this case the test may miss sharp deviations as they are aggregated by the relatively large number of other points in the same bin. Similar observation can be made from the results of Theorem~\ref{thm: power_balls_bins} (and Theorem~\ref{thm: power_balls_bins_asympt}) where the right-hand side of~\eqref{eq: lower Delta} (and~\eqref{eq: tmp: power-asympt}) has a term decreasing in $L$ and a term increasing in $L$. Therefore, the parameter $L$ should be perceived as a tuning parameter in Algorithm \ref{algorithm: model-xz}.  

As we showed in Theorem \ref{thm: chi^2-CI-size}, any choice of $L$ results in a test with type I error control; however different choices of $L$ gives different statistical powers. %One would like to choose an $\ell$  value that gives a lower $p$-value, therefore higher number of discoveries. 
A natural approach is to run the PCR test multiple times, each time with a different value of $L$, and then `pick' the one that results in the smallest (most significant) $p$-value. However, this approach clearly violates the validity of the reported $p$-value, as we should account for the `cherry-picking'. Also, note that the obtained $p$-values (with different choices of $L$) are dependent as they are constructed from a common data set. To properly combine the $p$-values, we use the Bonferroni's method. Algorithm \ref{algorithm: model-xz-simes} describes this idea and presents a parameter-free version of Algorithm \ref{algorithm: model-xz}. 

\begin{algorithmic}[t]
	\begin{algorithm}
		\SetAlgoLined
		\REQUIRE $n$ data points $(X_j,Y_j,Z_j)\in \reals \times \reals\times \reals^q$, significance level $\alpha\in(0,1)$, a real-valued score function $T:\reals\times\reals\times\reals^q\mapsto \reals$, $K\ge 1$ and a gird of $N$ values $\{L_1,...,L_N\}$.\\
		\ENSURE Decision on the conditional independence hypothesis \eqref{eq: CI hypothesis}.\\
	
		\For{$i\in [N]$}{
		\begin{itemize}
			\item Run Algorithm \ref{algorithm: model-xz} with $L=L_i$ labels to get test statistic $U_{n,L_i}$.
			\item Construct $p$-value $P_i$ using \eqref{eq: p-val} (for finite sample)  or \eqref{eq: p-val-asymptotic} (for asymptotic case).
			\end{itemize}
	}
	\begin{itemize}[leftmargin=*]
	\item Reject the null hypothesis if $P^*:= N\min\limits_{i\in[N]}^{} P_i\leq \alpha$.
	\end{itemize}

	%\item Sort $P_1,...,P_N$ and denote them by $P^{(1)}\leq P^{(2)}\leq...\leq P^{(N)}$.

	

		%\ENSURE {Reject the null hypothesis if $P^*:=\min\limits_{s\geq 1}^{} P_s\leq \alpha/N$. } 

		\caption{Parameter-free PCR test}\label{algorithm: model-xz-simes}
	\end{algorithm}

%Having p-value $P^*$ allows us to naturally consider the following decision rule at significance level $\alpha$.
%
%\begin{equation}\label{eq:decision rule-simes}
%	\phi\left(X,Z,Y\right)=\begin{cases} 1 & P^*\leq \alpha \quad ~(\text{reject } H_0)\,,\\
%		0&  \text{otherwise}\quad (\text{accept }H_0)\,. \
%	\end{cases}
%\end{equation}
\end{algorithmic}

The next theorem follows readily from Theorem \ref{thm: chi^2-CI-size} along with union bounding for the Bonferroni's correction. 
\begin{thm}
Under the null hypothesis \eqref{eq: CI hypothesis}, the p-value $P^*$ constructed in Algorithm \ref{algorithm: model-xz-simes} is super-uniform, i.e $\prob\left(P^*\leq t\right) \leq t$, for all $t\in [0,1]$. 
  \end{thm}






\section{Robustness of the PCR test}\label{sec:robust}
In this section, we investigate the conditional independence problem when the exact conditional distribution $P_{X|Z}$ is not available; rather we use $\hP_{X|Z}(\cdot|Z)$  an estimate of $P_{X|Z}(\cdot|Z)$ for sampling the counterfeits. We would like to modify the PCR test so it still controls the type I error, when access to the exact conditional law $P_{X|Z}(\cdot|Z)$ is not feasible. To this end, the next theorem introduces a new test statistic which is based on the discrepancy between conditional laws $P_{X|Z}(\cdot|Z)$ and  $\hP_{X|Z}(\cdot|Z)$ along with the rejection thresholds for both the asymptotic setting and the finite-sample setting. We use the expected total variation metric to assess the distance between conditional laws. 
%
\begin{thm}\label{thm:robust-model X}
Let $W_s$, for $s\in [\ell]$, be the number of  data points with label $s$ as defined in Algorithm \ref{algorithm: model-xz}. For $\delta$ such that $\E_{Z}\left[d_{\tv}\left(P_{X|Z}(.|Z),\hP_{X|Z}(.|Z)  \right)\right]\leq \delta$,
introduce
\begin{equation}\label{eq:robust}
	\begin{split}
		U_{n,L}(\delta):=\min_{\{p_s\}_{s\in[L]}} \quad & \frac{L}{n(1+L\delta)}\sum\limits_{s=1}^{L}{\left(W_s-np_s\right)^2}\\
		{\rm s.t.} \quad\quad  &p_s\geq 0,\quad  |p_s-1/{L}|\leq \delta,\quad \textrm{ for } s\in[L]\,,\\
		&{\rm and }\;\;\sum\limits_{s=1}^{L}p_s=1\,. \\
	\end{split}
	\end{equation}
Recall the thresholds $\th^{\mathsf{finite}}_{L,\alpha} $ and $\th^{\mathsf{asym}}_{L,\alpha}$ from~\eqref{eq:thresholds}. Under the null hypothesis, we have the following relations:
\begin{align}
&\prob\left( U_{n,L}(\delta) \geq  \th^{\mathsf{finite}}_{L,\alpha}   \right) \leq \alpha\,,\label{eq:robust-fin}\\
&\lim\limits_{n\rightarrow \infty}^{}  \prob\left( U_{n,L}(\delta) \geq \th^{\mathsf{asym}}_{L,\alpha} \right) \leq \alpha\,.\label{eq:robust-asym}
\end{align}
%\[
%V_\delta=\min \limits_{p \in \reals^\ell }^{} \sum\limits_{s=1}^{\ell}\left(W_s-{n}p_j\right)^2
%\]
 \end{thm}
We refer to Section~\ref{proof:thm:robust-model X} for the proof of Theorem~\ref{thm:robust-model X}.
Note that optimization~\eqref{eq:robust} is a quadratic programming and can be solved efficiently. Also, statistic $U_{n,L}(\delta)$, given as the optimal value of this optimization, is a decreasing function with respect to $\delta$ and when there is no mismatch between the true and the approximate version ($\delta = 0$), we recover the primary statistic $U_{n,L}$ that was given by Algorithm \ref{algorithm: model-xz}.

As an immediate corollary of Theorem~\ref{thm:robust-model X} we can construct valid $p$-value for testing the conditional independence (i.e., super-uniform under the null hypothesis~\eqref{eq: CI hypothesis}), following the same recipe given by~(\ref{eq: p-val}-\ref{eq: p-val-asymptotic}), but using $U_{n,L}(\delta)$ instead of $U_{n,L}$.

\section{Discussion}\label{sec:discussion}
In this work, we introduced the PCR test procedure to examine conditional independence of two variables in the presence of a high-dimensional confounding variable, in a model-$X$ setup where the distributional information on the covariate population is available.  The proposal of the PCR test was inspired by some of the alternative distributions for which the CRT (and its variants) are powerless. The PCR test is generally more flexible in capturing the conditional dependency, and under some alternatives can result in much higher statistical power compared to the CRT. We also provided a power analysis of the PCR test  in terms of the so-called conditional dependency power of the joint law $\cL(X,Z,Y)$, sample size $n$ and the number of labels $L$ used in constructing the PCR test statistic.  

We also proposed two extensions of the PCR test: $(i)$ \emph{Parameter-free PCR test}, which consists of multiple runs of PCR test with different choices of number of labels $L$, and then using Bonferroni's method to combine the obtained $p$-values. $(ii)$    \emph{Robust PCR test}, which improves the robustness of the test against errors in estimating the conditional distribution $P_{X|Z}$.  Both of these extensions would have important practical implications. 

It is worth noting that many of the extensions made to improve the computational complexity of CRT (e.g., hold-out randomization test (HRT)~\cite{tansey2018holdout} or the distilled CRT~\cite{liu2020fast}) can also be adapted and applied to our proposed PCR test.

We next describe a generalization which unifies both the PCR test and the CRT. Specifically, instead of running PCR at sample level, we first partition the samples into $N$ smaller datasets of equal size. On each of these datasets, we follow a similar approach to CRT in that we draw counterfeits  and rank the original dataset among its counterfeits using a score function. We then apply the PCR test at the sub-datasets level, where we label each of these `sub-datasets' based on their ranks and use the test statistic in Algorithm~\ref{algorithm: model-xz} to examine differences between ranks distribution and the discrete uniform. Notably, when the sub-datasets are singletons this reduces to the PCR test, while in the other extreme where there is no partitioning $(N=1)$, this recovers the CRT. We will implement and discuss this generalization further in the BikeShare example in  Section \ref{sec: numerical}.  

%one can combine the Pearson $\chi^2-$CI test with many othe well-developed tricks in the model-X CI litrature to get more extensions. In particular, instead of considering the $\chi^2$-CI test at the data point level, it is possible to partition the data set into smaller data sets, and fit a certain fixed model on each group, e.g. a LASSO coefficient, a random forest residual error, etc., and finally apply the classic $\chi^2$-CI test on the obtained values--the final number of data points will be the primary number of paritioned groups. For another example, inspired by the HRT, one can hold a certain number of data points as training samples, and then train a predictive model on it. The obtained model can be used further as the regular score function $T$ in Algorithm \ref{algorithm: model-xz}. We will use the combination of the mentioned tricks in the BikeShare data set in Section \ref{sec: numerical}.

\section{Numerical Experiments}\label{sec: numerical}

In this section, we evaluate the performance of PCR test and its extensions on synthetic datasets. 
\medskip

\noindent \textbf{Size of PCR test.} We start by showing that the size of PCR test is controlled at the desired level, under various choices of input parameters $L$ and $K$. Assume $n=100$ data points $\{(X_i,Z_i,Y_i)\}_{i=1}^{n}$ are generated i.i.d. from the following model: First draw two vectors $v, u\in \reals^p$ with i.i.d standard normal entries and $p=20$. Then,   
\begin{align}
  %v,u& \overset{\text{ i.i.d. }}{\sim} \normal(0,I_p),\quad\text{ with } p=20\,,\nonumber\\
	Z&\sim \normal(0,I_p),\quad \text{ for }Z\in \reals^p\,,\nonumber\\
	X|Z&\sim \normal(v^\sT Z,1),\quad\text{ for }X\in \reals\,,\label{eq:example-size}\\
	Y|X,Z&\sim \normal\left((u^\sT Z)^2,1\right)\nonumber\,.
\end{align} 

Clearly $X\indep Y|Z$ and the null hypothesis holds. We assume that the dependency rule $X|Z$ and the vector $v$ are known, and  therefore for every given $Z$ we can easily sample from $\normal(v^\sT Z,1)$ to construct the counterfeit variables.  Tables \ref{table: test-size-finite} and \ref{table: test-size-asym} exhibit the performance of the PCR test with thresholds $\th^{\mathsf{finite}}_{\ell,\alpha}$ and $\th^{\mathsf{asym}}_{\ell,\alpha}$, respectively. As expected, the $\th^{\mathsf{finite}}_{\ell,\alpha}$ threshold is conservative and controls the size at a level lower than $\alpha$. The $\th^{\mathsf{asym}}_{\ell,\alpha}$ threshold also controls the size, albeit $n$ being only $100$.  





	\begin{table}[]
%\begin{center}
			\scalebox{0.9}{
	\begin{tabular}{|c|c|c|c|c||c|c|c|c||c|c|c|c| }\hline
&\multicolumn{4}{c||}{$\alpha=0.05$}&\multicolumn{4}{c||}{$\alpha=0.1$}&\multicolumn{4}{c|}{$\alpha=0.15$}  \\\hline
 \diagbox{$K$}{$L$}&\makebox{$2$}&\makebox{$3$}&\makebox{$5$}&\makebox{$10$}&\makebox{$2$}&\makebox{$3$}&\makebox{$5$}&\makebox{$10$}&\makebox{$2$}&\makebox{$3$}&\makebox{$5$}&\makebox{$10$}          	\\ \hline
	$1$&0.0009 &0.0012 &0.0009 & 0.0006&0.0035 &0.0042 &0.0042 &0.0049& 0.0062 &0.0094 &0.0101 &0.0109 \\\hline
	$4$&0.0008&0.0006&0.0006 &0.0002 & 0.0029&0.0035 &0.0039 &0.0047 &0.0063 &0.0090 &0.0105 &0.0110 \\\hline
	$20$& 0.0007& 0.0010& 0.0006& 0.0004& 0.0038&0.0045 &0.0042 &0.0036 & 0.0064& 0.0082&0.0107 &0.0088\\\hline
	$100$&0.0012  &0.0007  &0.0005 & 0.0004 &0.0035 &0.0032 &0.0044 &0.0043   &0.0065 &0.0087 &0.0095 &0.0107\\\hline
\end{tabular}
}
%\end{center}
\caption{Size of PCR test applied on a dataset consisting of $n=100$ samples generated from model \eqref{eq:example-size}, where $X\indep Y|Z$. Three significance levels $\alpha=0.05,0.1,$ and $0.15$ are considered. Statistic $U_{n,L}$ is obtained from Algorithm \ref{algorithm: model-xz} by using the score function $T(x,z,y)=(y-x-z^\sT\mathbf{1})^2$, and the decision rule \eqref{eq:decision rule} is employed with the threshold $\th^{\mathsf{finite}}_{\ell,\alpha}$. Reported numbers are averaged out over $10,000$ independent realizations.\vspace{0.4cm}} \label{table: test-size-finite}
\end{table}

	
	
	
	\begin{table}[]
	%	\begin{center}
			\scalebox{0.9}{
			\begin{tabular}{|c|c|c|c|c||c|c|c|c||c|c|c|c| }\hline
				&\multicolumn{4}{c||}{$\alpha=0.05$}&\multicolumn{4}{c||}{$\alpha=0.1$}&\multicolumn{4}{c|}{$\alpha=0.15$}  \\\hline
				\diagbox{$K$}{$L$}&\makebox{$2$}&\makebox{$3$}&\makebox{$5$}&\makebox{$10$}&\makebox{$2$}&\makebox{$3$}&\makebox{$5$}&\makebox{$10$}&\makebox{$2$}&\makebox{$3$}&\makebox{$5$}&\makebox{$10$}          	\\ \hline
				$1$&0.0560  &0.0510 &0.0504 &0.0493 &          0.0874 &0.0930 &0.1018 & 0.0968 &    0.1321 &0.1515 & 0.1508& 0.1457 \\\hline
				$4$& 0.0583& 0.0545& 0.0509 &0.0527 &           0.0899& 0.1000  &0.0960 &0.0992 &   0.1320 &0.1583  & 0.1468 &0.1492 \\\hline
				$20$&0.0561 & 0.0488& 0.0536 &0.0457 &        0.0874&0.0923 &0.0981 &0.0933 &   0.1346  &0.1516 & 0.1470& 0.1455\\\hline
				$100$&0.0564  &0.0504  &0.0470 &0.0521 &      0.0885& 0.0936 & 0.0925 &0.1004 &    0.1332   & 0.1543 &0.1421 &0.1491\\\hline
			\end{tabular}
		}
%		\end{center}
		\caption{Size of PCR test applied on a dataset consisting of $n=100$ samples generated from model \eqref{eq:example-size}, where $X\indep Y|Z$. Three significance levels $\alpha=0.05,0.1,$ and $0.15$ are considered. Statistic $U_{n,L}$ is obtained from Algorithm \ref{algorithm: model-xz} by using the score function $T(x,z,y)=(y-x-z^\sT\mathbf{1})^2$, and the decision rule \eqref{eq:decision rule} is employed with the threshold $\th^{\mathsf{asym}}_{\ell,\alpha}$. Reported numbers are averaged out over $10,000$ independent realizations.} \label{table: test-size-asym}
	\end{table}
	
	
	\bigskip
	
	\noindent \textbf{Statistical Power of PCR test.} Consider a setup similar to \eqref{eq:example-size}, but with $n=1000$ data points and the conditional law 
	\begin{equation}\label{eq: power-example-conditional-law}
		Y|X,Z \sim \normal  \left((u^\sT Z)^2+2 X,1 \right)\,,
	\end{equation}
	Our power analysis in section \ref{sec: pwr} suggests that larger values of $M=KL-1$ would results in higher power. We fix $K=100$ and let $L$ vary in the set $L=\{2,3,\dotsc,30\}$. The significance level is fixed at $\alpha=0.1$. Figure  \ref{fig: powers} showcases the power of PCR test with both choices of rejection thresholds  $\th^{\mathsf{finite}}_{\ell,\alpha}$ and $\th^{\mathsf{asym}}_{\ell,\alpha}$. As we see, when $n$ doubles not only the power increases but also it becomes more stable with respect to the choice of $L$.  
\begin{figure}[]
\centering
\begin{subfigure}{0.5\textwidth}
		\centering
		\includegraphics[scale=0.45]{plot_power.pdf}
		\caption{$n=1000$ data points}
	\end{subfigure}%
\begin{subfigure}{0.5\textwidth}
			\includegraphics[scale=0.45]{plot_power_n_2000.pdf}
	\caption{$n=2000$ data points}
\end{subfigure}
\caption{ Power of PCR test for (left) $n=1000$  and (right) $n=2000$ data points. Data points are generated under the setup \eqref{eq:example-size} and the conditional law \eqref{eq: power-example-conditional-law}. We consider the score function $(y-x-z^\sT\mathbf{1})^2$ and choose the significance level $\alpha=0.1$. We consider the decision rule \eqref{eq:decision rule} with  both of the rejection thresholds $\th^{\mathsf{asym}}_{\ell,\alpha}$ and $\th^{\mathsf{finite}}_{\ell,\alpha}$. Each reported power is obtained by averaging over $1000$ trials.}\label{fig: powers}
\end{figure}
	\bigskip
	
	\noindent
	\textbf{Parameter-free PCR test.}
	We consider a setup similar to the previous experiment \eqref{eq: power-example-conditional-law} and run the PCR test with different choices of $L\in\{2,4,8,16,32\}$. 
	We combine the obtained $p$-values using the Bonferroni's correction, as described in Algorithm~\ref{algorithm: model-xz-simes}.  With $n=1000$ data points, we get a statistical power of $0.192$ (with the finite-sample threshold), and $0.815$ (with the asymptotic threshold). Note that in this case, the power of the PCR test with different individual choices of $L$ (without combining the $p$-values) ranges in $(0.13-0.53)$, for the finite-sample threshold, and in $(0.576-0.887)$, for the asymptotic-threshold. 
	
	For $n=2000$ data points, and with the Bonferroni's correction, we get a power of $0.613$, with the finite-sample threshold, and a power of $0.972$, with the asymptotic threshold. Here, the power of the PCR test with individual choices of $L$  ranges in $(0.477-0.83)$, for the finite-sample threshold, and in $(0.8560-0.981)$, for the asymptotic threshold. 
	\bigskip
	
	\noindent
	\textbf{Robustness of the PCR test.}	In this part, we consider cases where the exact dependency law $P_{X|Z}$ is not available, and we use an estimate of it denoted by $\hP_{X|Z}$ (see Section~\ref{sec:robust} for the details and the description of the robust PCR test). 
	%To this end, we show that, using the robust test statistic, which is constructed in \eqref{eq:robust}, allows us to control Type I error.
	 %however, using the regular statistic $T$ from Algorithm \ref{algorithm: model-xz}, will significantly exceed the predetermined significance level $\alpha$.
	  %In addition, we investigate Chi-squared CI test power, when the robust test statistic is used. 
	  Consider a setup similar to \eqref{eq:example-size}, but with $n=5000$ data points and the conditional law 
	\begin{equation}\label{eq:model-aj}
		Y|X,Z \sim \normal  \left((u^\sT Z)^2+a X,1 \right)\,.
	\end{equation}
	When $a=0$, then the null hypothesis is true ($X\indep Y|Z$) and the rejection rate amounts to the type I error. For $a\neq 0$, the null hypothesis is false and the rejection rate amounts to the power of the test.
	
	 In the current experiment, we assume that the counterfeits are sampled from $\hP_{X|Z}$ with
	 \begin{equation}\label{eq: ex-robust}
	 \hX|Z\sim\normal(v^\sT Z,(1+\eta)^2)\,.
	 \end{equation}
	Note that when $\eta=0$, we get the true distribution $P_{X|Z}$ defined in \eqref{eq:example-size}. We use the Pinsker's inequality\footnote{$d_{\tv}(P,Q)\leq \sqrt{\dfrac{1}{2} d_{\mathsf{KL}}(P,Q)} $}to bound the expected total variation distance $\E_{Z}\left[d_{\tv}\left(P_{X|Z}(\cdot|Z),\hP_{X|Z}(\cdot|Z)  \right)\right]$. Note that for two 1-dimensional Gaussian distributions we have $$d_{\mathsf{KL}}\left(\normal(\mu,\sigma^2_1), \normal(\mu,\sigma^2_2)\right) =\log \frac{\sigma_2}{\sigma_1} +\frac{\sigma_1^2}{2\sigma_2^2}-\frac{1}{2}\,,$$
	which combined with Pinsker's inequality implies that
	\begin{equation}\label{eq: robust-delta}
	 \E_{Z}\left[d_{\tv}\left(P_{X|Z}(\cdot|Z),\hP_{X|Z}(\cdot|Z)  \right)\right] \leq \delta:= \frac{1}{\sqrt{2}} \left(\log(1+\eta)+\frac{1}{2(1+\eta)^2}-\frac{1}{2}\right)^{1/2}. 
	 \end{equation}

The results for $a = 0$ and $a=4$ are summarized in Table~\ref{tbl:robust}. As we see the robust PCR test controls the type I error under the level $\alpha = 0.1$ for different choices of $\eta$. In addition, it achieves a high power for $a = 4$.
 
It is also worth noting that if we use the test statistics $U_{n,L}$ (instead of $U_{n,L}(\delta)$) we observe an inflation in type I errors. Concretely, when $\eta= 0.04$ we obtain an inflated type I error of $0.595$ (with the finite-sample threshold $\th^{\mathsf{finite}}_{\ell,\alpha}$) and an inflated type I error of  $0.1860$ (with the asymptotic threshold $\th^{\mathsf{asym}}_{\ell,\alpha}$), while the target level is $\alpha = 0.1$. This highlights the importance of adjusting for the errors in estimating the model-X conditional distribution, as proposed in Section~\ref{sec:robust}.

%in the presence of discrepancy between the actual law $P_{X|Z}$ and its estimate $\hP_{X|Z}$  , using the regular test statistic $T_{n,\ell}$ in the presence of discrepancy of value $\eta=0.04$ will result in inflated Type I error  $0.595$ and $0.1860$ for asymptotic and finite thresholds, respectively.
	
	
	

	\begin{table}
\begin{center}
			\scalebox{0.9}{
	\begin{tabular}{|c|c|c|c|c||c|c|c|c| }\hline
       &\multicolumn{4}{c||}{$a=0$} & \multicolumn{4}{c|}{$a=4$}\\  \hline
 \diagbox{setting}{$\eta$}&\makebox{$0$}&\makebox{$0.01$}&\makebox{$0.02$}&\makebox{$0.04$}&\makebox{$0$}&\makebox{$0.01$}&\makebox{$0.02$}&\makebox{$0.04$} \\ \hline
   
  %$T_{n,\ell}$ with $\th^{\mathsf{finite}}_{\ell,\alpha}$ & 0.008&0.139  &0.2380 &0.595 & 0.846 & & &\\ \hline
  %$T_{n,\ell}$ with $\th^{\mathsf{asym}}_{\ell,\alpha}$ & 0.1050& 0.01 &0.0290 & 0.1860&0.973 & & & \\ \hline
  $U_{n,L}(\delta)$ with $\th^{\mathsf{finite}}_{L,\alpha}$  &0.008 &  0&0 & 0&1 &0.998 & 0.973& 0.63\\ \hline
    $U_{n,L}(\delta)$ with $\th^{\mathsf{asym}}_{L,\alpha}$ &0.1050 &  0.003&0 &0 &1 & 1&0.995 &0.8790 \\ \hline


   
\end{tabular}
}
\end{center}
\caption{Size ($a=0$) and power ($a=4$) of the robust PCR test for the setting of \eqref{eq:model-aj}  and the approximate distribution \eqref{eq: ex-robust} available for sampling the counterfeits. We consider $a=0,4$ and $n=5000$ data points. The PCR  test is run with $L=4$ number of labels. For each value of the discrepancy level $\eta$, we use \eqref{eq: robust-delta} to get an upper bound $\delta$ on the expected total variation distance  $\E_{Z}\left[d_{\tv}\left(P_{X|Z}(\cdot|Z),\hP_{X|Z}(\cdot|Z)  \right)\right]$, and use it in constructing the robust statistic $U_{n,L}(\delta)$. Note that $\eta=0$ implies $\delta=0$, and therefore the robust statistic $U_{n,L}(\delta)$ matches the statistic $U_{n,L}$ in Algorithm \eqref{algorithm: model-xz}. For $a=0$ (true null hypothesis), the size is controlled at the significance level $\alpha=0.1$. Reported numbers are obtained by averaging over $1000$ trials.}\label{tbl:robust}
\end{table}
	
	
	
\subsection{Real data experiment: Capital Bikeshare dataset }
	
In this section, we evaluate the performance of the PCR test on real data from the Capital Bikeshare\footnote{The dataset is publicly available at \url{https://www.capitalbikeshare.com/system-data}.}. Capital Bikeshare is bike-sharing system in Washington, D.C, and releases its trips data on a quarterly basis. The data includes each trip taken, start date and time, end date and time,
start and end stations, Bike ID, and the  user type indicating whether the rider was a registered member or if it was a casual ride (one-time rental or a short pass). 
%which have taken place sometime in September, October, or November 2011 in Washington, DC.  In particular, each entry incorporates information about the trip day (Monday to Friday), the start and end time, the trip duration, the rider's membership status (i.e. having a long term membership, or it is a temporary rental ride), the bicycle ID, and the start and end station. 

In this experiment, we use our proposed PCR test to study the independence of the trip duration ($X$), and other variables, such as the user type ($Y$), and provide $p-$values for their associations. A similar data and question has been studied by \cite{berrett2020conditional} using the Conditional Permutation Test (CPT). As can be imagined, the trip duration ($X$) heavily depends on the route (length of the rout, elevation ,etc) and the time of the day at the start of the ride (due to varying traffic and the rush hours). To control for the effect of such variables, we condition on the start and end locations and the day hour $Z=(Z_{\text{start loc}}, Z_{\text{end loc}}, Z_{\text{hour}})$. 

In order to implement the PCR test, we use the conditional normal distribution $X|Z \sim \normal(\mu(Z), \sigma^2(Z))$ as an approximation of $P_{X|Z}$.  We follow the procedure of \cite{berrett2020conditional} to estimate the mean $\mu(z)$ and variance $\sigma^2(z)$. We outline the procedure here for the reader's convenience. We consider a test data, consisting of the rides taken on weekdays in Oct 2011, and a training data consisting of the rides taken on weekdays in Sep 2011 and Nov 2011. The test data is used to for testing conditional independence between factors of interest, and the training data is used to estimate the conditional mean and variance ($\mu(z)$,$\sigma^2(z)$). To have reliable estimation, we eliminate the records in the test data for which the corresponding route in the training data has less than 20 rides. After this preprocessing step, the test data includes $7,346$ samples. Finally, the conditional functions $\mu(z)$ and $\sigma^2(z)$ are estimated using a Gaussian kernel with a bandwidth of $20$ minute, on the training data. (See \cite[Appendix B]{berrett2020conditional}for further details on this part.) 

% The dataset into two training and test sections, where the training data set will be used for conditional mean and variance estimations $\widehat{\mu}(z)$ and  $\widehat{\sigma}(z)$, and the test data set will be used for CI testing.  The training dataset incorporates rides in September and November, and the test data set consists of rides in October.  . After this screening step, we will end up with a test data set of size $7346$.  Finally, a Gaussian kernel with bandwidth of $20$ minutes is used on the training data set to estimate the conditional functions $\mu(z)$ and $\sigma^2(z)$. For more details about the kernel fitting and screening steps, we refer readers to \cite{berrett2020conditional},  Appendix B.  


We test the null hypothesis \eqref{eq: CI hypothesis} with $X$ being the duration of the ride, and three different response variables $Y$: (1) User type-- registered members have acquaintance with the routes and are likely to have lower trip durations, (2) Date of the month (continuous variable from $1-30$)--  this can be used to capture effect of factors such as weather and sunlight hours. (3) Weekday (categorical variable from Monday to Friday)-- rides on the early days of the week are likely to be more work-related.  
For score function to be used in the PCR test, we consider the squared residual from regressing $Y$ on $X$. As an example, when $Y$ is the user type, we encode it  as a binary variable $Y=\ind{\{ \text{the user is a registered member}\}}$, and fit the linear model $Y=b_0+b_1X$ to the training data to obtain the estimates $\widehat{b}_0, \widehat{b}_1$. Finally, for any test data point $(x,y)$, we consider the score function $T(x,y)=(y-\widehat{b}_0-\widehat{b}_1x)^2$.

In this experiment, we use the PCR test with $L=10$ number of labels and the counterfeit ratio $K=200$, and therefore $M=1999$. Further, in order to reduce the variation between the true distribution $P_{X|Z}$ distribution and its estimate  $\normal(\widehat{\mu}(Z), \widehat{\sigma}^2(Z))$, we use the generalization of the PCR framework as discussed at the end of Section~\ref{sec:discussion}. Concretely, we partition the $n=7346$ test data samples into $N= \lfloor n/4 \rfloor$ smaller groups, each of size $4$, and label each group based on the rank of its score, among the scores of its counterfeits, as proposed in the PCR test.  Formally, for test data vectors $X_{1:n}$, $Z_{1:n}$, we construct counterfeits $\tX_{1:n}^{(1)},\tX_{1:n}^{(2)},...,\tX_{1:n}^{(M)}$, and score each sample and its counterfeits by:
\[
T=[(\widehat{b}_0+\widehat{b}_1X_j-Y_j)^2]_{j=1:n}\,, \quad \widetilde{T}_i=[(\widehat{b}_0+\widehat{b}_1\tX_j^{(i)}-Y_j)^2]_{j=1:n}\,.
\]
We then partition the samples into $N=\lfloor n/4 \rfloor$ groups of equal size and score each group (and its counterfeits) by the average score of their four members. Finally, we follow Algorithm \eqref{algorithm: model-xz} to label the groups and construct the statistic $U_{N,\ell}$. We calculate the $p$-values for each of the conditional independence tests, using \eqref{eq: p-val}. The results are summarized in Table \ref{table: real-data}. As we see among the three response variables considered in this experiment, user type has the most significant (conditional) dependence to duration of the ride.
\begin{table}
\begin{center}
\begin{tabular}{|c|c|c|}
\hline
Response $Y$ & $p$-value (finite) &$p$-value (asym)\\ \hline 
User type &   $0.0014$ & 0   \\ \hline
Date & $0.3855$ & 0.0456 \\ \hline
Week day &  0.2094 & 0.0194  \\ \hline
\end{tabular}
\caption{$P$-values that are computed from the PCR test on the Capital Bikeshare dataset. The null hypothesis \eqref{eq: CI hypothesis} is considered with $X$ being the duration of the ride, and the confounder variable $Z$ encoding the start and end locations, as well as the time of day at the start of the ride. We consider three different response values $Y$: (1) User type, (2) Date of the month, (3) Weekday. The $p$-values are obtained as per~\eqref{eq: p-val} with the number of labels $L=10$, and the counterfeit ratio $K=200$.}\label{table: real-data}
\end{center}
\end{table}

\section{Proof of theorems and technical lemmas}

\subsection{Technical preliminaries}


\begin{lemma}[  Pearson's $\chi^2-$test size and power] \label{lemma: multinomial-main}
Consider a multinomial model with $L$ labels $\{1,2,...,L\}$, and $n$ number of samples. For $s\in [L]$, let $W_s$ denote the number of samples with label $s$ and $p_s$  be the occurrence probability of label $s$ in one realization of the multinomial model .  Consider the following uniformity hypothesis, at the significance level $\alpha\in (0,1)$:
\begin{equation}\label{eq: multi-unif-hypothesis}
H_0: p_s=\frac{1}{L},\quad \text{for } 1\leq s \leq L\,,
\end{equation}
with the following decision rule $\Psi_{n,L}$, which is based on the Pearson's Chi-squared statistic $U_{n,L}$:
\[
\Psi_{n,L}= \ind\left( U_{n,L}: =\frac{L}{n} \sum\limits_{s=1}^{L} \left(W_s-\frac{n}{L}\right)^2  \geq L+\sqrt{\frac{2L}{\alpha}} \right )\,.
\]
The following statements hold:
\begin{enumerate}
\item  Under the null hypothesis \eqref{eq: multi-unif-hypothesis}, $U_{n,L}\overset{d}{\Rightarrow} \chi^2_{L-1}$, as $n\rightarrow\infty$. % the Chi-squared distribution with $\ell-1$ degrees of freedom.
%
\item  Under the null hypothesis \eqref{eq: multi-unif-hypothesis},  the size of this test is controlled at level $\alpha$:
\[
\prob(\Psi_{n,L}=1)\leq \alpha\,.
\]
%
\item  If for some $\beta>0$, we have the following:

\[
\sum\limits_{s=1}^{L}\left|p_s-\frac{1}{L}\right| \geq\frac{32{L^{1/4}}}{\sqrt{n}}\left[\frac{1}{\sqrt{\alpha}}\vee\frac{1}{\beta}  \right]^{1/2}\,,
\]
then the type II error does not exceed $\beta$:
\[
\prob(\Psi_{n,L}=0)\leq \beta\,.
\]
\end{enumerate}
\end{lemma}
%\subsection{Proof of Lemma \ref{lemma: multinomial-main}}
Regarding the proof of Lemma \ref{lemma: multinomial-main}, note that the first part is a classic result on the asymptotic null distribution of the Pearson's Chi-squared test (See e.g. \cite{lehmann2006testing}, Theorem 14.3.1.)
For the proof of parts 2 and 3, we refer to \cite{balakrishnan2019hypothesis}. More specifically, \cite{balakrishnan2019hypothesis} proves similar claims for the `truncated' $\chi^2$-test statistic and for more general hypotheses regarding the nominal probabilities of the labels under multinomial models. For the special case of the uniformity testing problem~\eqref{eq: multi-unif-hypothesis}, the truncated Chi-squared statistic reduces to the classic Pearson's Chi-squared test statistic.



% The threshold $\ell+\sqrt{2\ell/\alpha}$ in the decision rule $\Psi_{n,\ell}$ is presented in \cite{balakrishnan2019hypothesis} for the truncated $\chi^2$-test statistic for a general multinomial hypothesis testing problem with $n$ samples and $\ell$ categories. The important point that we'd like to emphasize here is that the 





The next lemma is the Berry-Esseen theorem for non-identical independent random variables and its statement is borrowed from 
\cite[Section 5]{barbour2005introduction}.
%
\begin{lemma}(\cite[Section 5]{barbour2005introduction})\label{lemma: berry-essen}
For zero-mean independent random variables $\xi_1,...,\xi_n$ with $\sum\limits_{i=1}^{n}\E[\xi_i^2]=1$, let $W=\sum\limits_{i=1}^{n}\xi_i$. If $\sum\limits_{i=1}^{n}\E[|\xi_i^3|]\leq \gamma$, then we have
%
\[
\sup\limits_{-\infty\leq z\leq \infty}^{}\left|\prob\left(W \leq z  \right)-\Phi(z) \right| \leq \gamma\,.
\]
\end{lemma}

\subsection{The intuition behind the example of Section~\ref{subsec: CRT fails}}\label{poof: example}
Consider the following regression setting for a measurable function $g$:
\[
Y=g(X) +\eps\,, \quad \eps\sim \normal (0,1)  \text{ and } X \sim \normal(0,1)\,.
\]
Assume that the data consists of $n$ samples $(\bX,\bY)=(X_i,Y_i)_{i=1}^{n}$. For $j\in[M]$, generate counterfeits $\tbX_j\sim \normal({0},I_{n\times n})$, and define 
 $U=\|\bY-\bX\|_2^2/n, \widetilde{U}_j=\|\bY-\widetilde{\bX}_j\|_2^2/n$.
 % We have
 %\begin{align*}
 %U&=\frac{1}{n}\sum\limits_{i=1}^{n} (g(x_i)-x_i+\eps_i)^2\\
 %&=\frac{1}{n}\sum\limits_{i=1}^{n} \left(g(x_i)^2+x_i^2-2g(x_i)x_i+2\eps_i g(x_i)-2\eps_ix_i+ \eps_i^2 \right)\,.
 %&=\frac{1}{n}\sum\limits_{i=1}^{n}x_i^2+g(x_i)^2-2x_ig(x_i)+ \eps_i^2+2\eps_i(g(x_i)-x_i)\\
 %\end{align*}
%We can get a similar expression for $\widetilde{U}$: 
% \begin{align*}
%\widetilde{U}=\frac{1}{n}\sum\limits_{i=1}^{n} \left(g(x_i)^2+\tx_i^2-2g(x_i)\tx_i+2\eps_i g(x_i)-2\eps_i\tx_i+ \eps_i^2 \right)\,.
%\end{align*}

From the Central limit theorem, we know that $U\sim \normal(\mu_1, \sigma_1^2/n)$ and $\widetilde{U}_j\sim \normal(\mu_2, \sigma_2^2/n)$, where
\begin{align*}
&\mu_1=\E[ (g(X)-X+\eps)^2 ], \quad\;\;\quad\; \mu_2=\E[ (g(X)-\tX+\eps)^2 ]\,,\\
&\sigma_1^2=\mathsf{Var}((g(X)-X+\eps)^2 ) , \quad\;\;\; \sigma_2^2= \mathsf{Var}((g(X)-\tX+\eps)^2 )  \,.
\end{align*}  
Roughly speaking, if 
\begin{align}\label{eq:mu-sigma}
\mu_1=\mu_2 \quad \text{and } \quad \sigma_1<\sigma_2\,,
\end{align} 
then the counterfeits scores $\widetilde{U}_j$ will be distributed around the  original data score $U$, and therefore the distribution of the normalized rank of the original data score gets an inflation in the central range.
 With this intuition in mind, we seek properties of the function $g$ to fulfill the conditions \ref{eq:mu-sigma}.
   
Note that, because $X,\tX$ are exchangeable and independent, we have \[\mu_1-\mu_2=\E[Xg(X)]-\E[\tX g(X)]=\E[Xg(X)],\]
since $\E[\tX]=0$. For even functions $g$, $xg(x)$ is odd and $\E[Xg(X)]=0$ by symmetry of the normal distribution. Hence, $\mu_1=\mu_2$. 

Next, we rewrite the variance terms:
\begin{align*}
\sigma_1^2-\sigma_2^2&=\E[ (g(X)-X+\eps)^4 ] - \E[ (g(X)-\tX+\eps)^4 ] -\mu_1^2+\mu_2^2\\
&=\E[ (g(X)-X+\eps)^4 ] - \E[ (g(X)-\tX+\eps)^4 ]\\
&=\E[(g(X)+\eps)^4] +\E[X^4] - 4\E[X^3(g(X)+\eps)]-4\E[X(g(X)+\eps)^3]+6\E[X^2(g(X)+\eps)^2]\\
&-\E[(g(X)+\eps)^4] -\E[\tX^4] +4\E[\tX^3(g(X)+\eps)]+4\E[\tX(g(X)+\eps)^3]-6 \E[\tX^2(g(X)+\eps)^2]\\
&\stackrel{(a)}{=} 6 \E[X^2(g(X)+\eps)^2]-6\E[\tX^2(g(X)+\eps)^2]\\
& \stackrel{(b)}{=} 6 \E[X^2g(X)^2]-6\E[g(X)^2]\,,
\end{align*}
where $(a)$ follows from the exchangeability of $X$, $\tX$ in conjunction  with the fact that $x^3g(x)$ and  $x(g(x)+\eps)^3$ are odd functions. Step (b) follows from $\E[X^2] = \E[\tilde{X}^2]=1$, $\E[\eps] = 0$ and the fact  $\eps, X,\tX$ are independent.  

In summary,  conditions~\eqref{eq:mu-sigma} are satisfied if $g$ is an even function and $\E[X^2g(X)^2]<\E[g(X)^2]$, for $X\sim \normal(0,1)$.  By choosing $g$ to be 
\[
g(x)=\frac{1}{\sqrt{10^{-6}+x^2}}\,,
\]
we get $\E[g(X)^2] =6.110$, $\E[X^2g(X)^2]=0.798$, and so it satisfies these conditions. 


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Proof of Proposition \ref{thm: crt-failure}} \label{proof:thm: crt-failure}
\def\tbX{\widetilde{\bX}}
 Consider $M$ counterfeits $\tbX_1,\tbX_2,...,\tbX_M$ sampled independently from $\normal(0,I_n)$. For $ j\in [M]$, let $I_j= \ind_{\{T(\bX,\bY)  \geq T(\tbX_j,\bY)\}} $. Let $T(\bX,\bY)=\|\bX-\bY\|_2^2/n$ and
 \[
 \mu_n=\E\left[ \ind_{ \{T(\bX,\bY) \geq T(\tbX_1,\bY) \}}  \right]\,,\quad \sigma_n^2=\var\left[  \ind_{\{T(\bX,\bY) \geq T(\tbX_1,\bY)\} } \right]\,.
 \]
It is easy to see that $\sigma_n^2=\mu_n(1-\mu_n)$. Before proceeding further we establish a lemma which will be used in proving the result.
%
\begin{lemma}\label{lemma: crt-fails0}
The followings hold:
\begin{align*}
&\lim_{n\to\infty} \mu_n =  \lim\limits_{n\rightarrow \infty}^{}\prob(T(\tbX,\bY) \leq T(\bX,\bY)) =1/2 \,,\\
&\lim\limits_{n\rightarrow \infty}^{}\E \left[\prob\left( \{T(\bX,\bY)\geq T(\tbX,\bY)\}|\bX,\bY \right)^2 \right] = \E_{Z\sim\normal(0,1)}[ \Phi^2(\eta Z) ] \,,
\end{align*}
where $\eta= \frac{3+2\E[X^2g(X)^2]}{3+2\E[g(X)^2]}$.
\end{lemma}
%
By applying Chebyshev's inequality we get
%
\begin{align}
\prob\Big( \Big| \sum\limits_{j=1}^{M}\frac{I_j}{M} -1/2  \Big| \geq \delta \Big) &\leq  \prob\Big( \Big| \sum\limits_{j=1}^{M}\frac{I_j}{M} -\mu_n  \Big| \geq \delta-|\mu_n-1/2| \Big) \nonumber\\
%\prob\left( \left| \sum\limits_{j=1}^{M}\frac{I_j}{M} -\mu_n  \right| \geq \eps \right)
 &\leq \frac{1}{(\delta-|\mu_n-1/2|)^2}\cdot\E\Big[  \Big| \sum\limits_{j=1}^{M} \frac{I_j}{M}-\mu_n   \Big|^2  \Big]\,,\nonumber\\
&=\frac{1}{(\delta-|\mu_n-1/2|)^2}\cdot\E\Big[ \frac{1} {M^2} \sum\limits_{j=1}^{M} (I_j-\mu_n)^2 + \frac{1}{M^2}\sum_{i\neq j}(I_i-\mu_n)(I_j-\mu_n)) \Big]\nonumber\\
&= \frac{1}{(\delta-|\mu_n-1/2|)^2}\cdot\left( \frac{\sigma_n^2}{M}+\frac{M-1}{M}\E [ (I_1-\mu_n)(I_2-\mu_n) ] \right)\nonumber \\
&=\frac{1}{(\delta-|\mu_n-1/2|)^2}\cdot\left( \frac{1}{M}\mu_n(1-\mu_n)+\frac{M-1}{M}\E [ I_1I_2-\mu_n^2 ] \right)\,. \label{eq: tmp2} 
\end{align}

We next compute $\E[I_1I_2-\mu_n^2]$.  
\begin{align*}
\E[ I_1I_2]&=\prob\left( \{T(\bX,\bY)\geq T(\tbX_1,\bY)\} \cap \{T(\bX,\bY)\geq T(\tbX_2,\bY) \}  \right)\nonumber \\
&=\E\left[\prob\left( \{T(\bX,\bY)\geq T(\tbX_1,\bY)\} \cap \{T(\bX,\bY)\geq T(\tbX_2,\bY) \}  | \bX,\bY \right)\right]\nonumber \\
&=\E \left[\prob\left( \{T(\bX,\bY)\geq T(\tbX_1,\bY)\}|\bX,\bY \right) \prob\left(\{T(\bX,\bY)\geq T(\tbX_2,\bY) \}  | \bX,\bY \right)\right] \nonumber \\
&=\E \left[\prob\left( \{T(\bX,\bY)\geq T(\tbX,\bY)\}|\bX,\bY \right)^2 \right]\,,
\end{align*}
where we used the fact that conditioned on $\bX$ and $\bY$, score values $T(\tbX_1,\bY), T(\tbX_2,\bY)$ are independent. Therefore, by using Lemma \ref{lemma: crt-fails0}, we write
% 
\begin{align}
\lim\limits_{n\rightarrow \infty}^{}\E[ (I_1 I_2-\mu_n^2)]&= \lim\limits_{n\rightarrow \infty}^{}\E \left[\prob\left( \{T(\bX,\bY)\geq T(\tbX,\bY)\}|\bX,\bY \right)^2 \right]-\mu_n^2 \nonumber\\
&= \lim\limits_{n\rightarrow \infty}^{}\E \left[\prob\left( \{T(\bX,\bY)\geq T(\tbX,\bY)\}|\bX,\bY \right)^2 \right]-1/4\nonumber \\
&=\E_{Z\sim\normal(0,1)}[ \Phi^2(\eta Z) ]-1/4  \,.\label{eq: tmp3}
\end{align}
%where (a) is followed by \eqref{eq: lim-mu-n}, and the last equation comes from Lemma \ref{lemma: crt-fails0}. 

To summarize, we let $S_M=\sum\limits_{j=1}^{M}  \ind_{\{ T(\bX,\bY)\geq T(\tbX_j,\bY) \} }$ and use \eqref{eq: tmp3} in \eqref{eq: tmp2} along with $\lim_{n\to\infty}\mu_n = 1/2$ per Lemma \ref{lemma: crt-fails0} to obtain
%
\begin{equation}\label{eq: tmp4}
\lim\limits_{n\rightarrow \infty}^{}\prob\left(\left|  \frac{S_M}{M}  -\frac{1}{2} \right| \geq \delta \right) \leq  \frac{1}{4M\delta^2} + \frac{M-1}{M\delta^2} \cdot(\E_{Z\sim\normal(0,1)}[\Phi^2(\eta Z)] -1/4)\,,\quad \forall \delta>0 \,.
\end{equation}
Recalling the $p$ statistic \eqref{eq:pMn}, we have $p^{(M)}_n=\frac {1+S_M}{M+1}$. As $S_M\leq M$, we have
\[
\frac{S_M}{M} \leq \frac{S_M+1}{M+1} \leq \frac{S_M}{M}+\frac{1}{M}\,,
\]
which implies that $\left|p^{(M)}_n-\frac{1}{2}\right| \leq \left| \frac{S_M}{M}-\frac{1}{2} \right|+\frac{1}{M}$. Using this relation along with triangle inequality in \eqref{eq: tmp4} to arrive at the following:
%
\begin{align*}
\prob\Big(\Big|p^{(M)}_n-\frac{1}{2}\Big|\geq \delta\Big) &\leq \prob\Big(\Big|\frac{S_M}{M}-\frac{1}{2}\Big|\geq \delta-\frac{1}{M}\Big)\\
&\leq \frac{1}{\left( \eps-1/M\right)^2} \left(   \frac{1}{4M} + \frac{M-1}{M} \cdot(\E_{Z\sim\normal(0,1)}[\Phi^2(\eta Z)] -1/4)  \right)\,.
\end{align*}
This completes the proof of Proposition \ref{thm: crt-failure}.

\subsubsection{Proof of Lemma \ref{lemma: crt-fails0}}
We start by establishing a lemma which characterizes the conditional probability that the original data score exceeds a counterfeit score.

Define the shorthands
\begin{align}\label{eq:mv}
m_n(\bX,\bY):=\frac{\|\bY\|_2^2}{n}+1\,,\quad
v_n(\bX,\bY):=\frac{4 \|\bY\|_2^2}{n^2}+\frac{2}{n}\,.
\end{align}
%
\begin{lemma}\label{lemma:indicators-cond-exp} 
 For $m_n(\bX,\bY)$ and $v_n(\bX,\bY)$ given by \eqref{eq:mv} we have
 \begin{equation}\label{eq: berry-2}
\Big|\prob( T(\tbX,\bY) \leq T(\bX,\bY)  | \bX,\bY ) -  \Phi\Big( \frac{T(\bX,\bY)-m_n(\bX,\bY)}{v_n(\bX,\bY)^{1/2}} \Big) \Big| \leq   \gamma_n(\bX,\bY)
\end{equation}
with 
\[
 \gamma_n(\bX,\bY) :=  \frac{1}{n^2 v_n(\bX,\bY)^{1.5}   } \left( C_0+\sum\limits_{i=1}^{n}\frac{C_1|Y_i|+C_2|Y_i|^2+C_3|Y_i|^3}{n} \right)\,,
\]
where $C_i$s are absolute constants.
\end{lemma}
We next show that $\E[\gamma_n(\bX,\bY)] \to 0$ as $n\to\infty$.
\begin{align*}
\E\left[\gamma_n(\bX,\bY)\right] &=  \E\left[\frac{1}{\sqrt{n}\left(2+4||\bY||_2^2/n\right)^{1.5}   } \left( C_0+\sum\limits_{i=1}^{n}\frac{C_1|Y_i|+C_2|Y_i|^2+C_3|Y_i|^3}{n} \right) \right] \,,\\
& \leq \frac{1}{2^{3/2}\sqrt{n}} \E\left[ \left( C_0+\sum\limits_{i=1}^{n}\frac{C_1|Y_i|+C_2|Y_i|^2+C_3|Y_i|^3}{n} \right) \right] \\
&= \frac{1}{2^{3/2}\sqrt{n}} \left( C_0+ C_1\E[|Xg(X)+\eps|] +C_2\E[|Xg(X)+\eps|^2]+C_3\E[|Xg(X)+\eps|^3] \right) \,,
\end{align*}
which brings us to 
\begin{align}\label{eq:Ezero}
\lim\limits_{n\rightarrow \infty}^{} \E\left[\gamma_n(\bX,\bY)\right] =0.
\end{align}
In the next lemma, we characterize the distribution of the other quantity in~\eqref{eq: berry-2}.  


\begin{lemma}\label{lemma:crt-failure-1}
 For $m_n(\bX,\bY)$ and $v_n(\bX,\bY)$ given by \eqref{eq:mv} we have
\[
 \frac{T(\bX,\bY)- m_n(\bX,\bY)}{ v_n(\bX,\bY)^{1/2} } \overset{d}{\Rightarrow} \normal(0,\eta^2), \quad \text{as } n \rightarrow \infty\,.
\]
with  $\eta=\left(\frac{3+2\E[X^2g(X)^2]}{3+2\E[g(X)^2]}\right)^{1/2} $.
\end{lemma}
%
Using the result of Lemma \ref{lemma:crt-failure-1} and by an application of the Portmanteau theorem for the bounded continuous function $\Phi$ we get 
\begin{equation}\label{eq: tmp0}
\lim\limits_{n\rightarrow \infty}^{}  \E \left[ \Phi\left(   \frac{T(\bX,\bY)- m_n(\bX,\bY)}{ v_n(\bX,\bY)^{1/2} }  \right) \right]=\E_{Z\sim \normal(0,1)}[\Phi(\eta Z)]\,.
\end{equation}
Combining~\eqref{eq:Ezero} and \eqref{eq: tmp0} with \eqref{eq: berry-2} we arrive at
\[
\lim_{n\to\infty}\prob(T(\tbX,\bY)\leq T(\bX,\bY))=\E_{Z\sim \normal(0,1)}\left[\Phi\left(\eta Z\right) \right] \,.
\]
%Rewrite the main expression as the following:
%\begin{align*}
%\prob(T(\tbX,\bY)\leq T(\bX,\bY))=\E\left[ \prob\left(T(\tbX,\bY)\leq T(\bX,\bY) \right)| \bX,\bY \right]\,.
%\end{align*}
%We want to use Lemma \ref{lemma: indicators-cond-exp} to establish the result of Lemma \ref{lemma: crt-fails0}. To this end, we need to prove that $E[\gamma_n(\bX,\bY)] \rightarrow 0$, and the expectation of the inner quantity in \eqref{eq: berry-2} converges to  $ \E_{Z\sim \normal(0,1)}\left[ \Phi(\eta Z)\right] $. We start by the inner quantity, where by using Lemma \ref{ lemma: crt-failure-1} in along with the Portmanteau theorem for the bounded continuous function $\Phi$ we get 
%
%
%then use this and  \eqref{eq: tmp0} in Lemma \ref{lemma: indicators-cond-exp} to get
%\[
%\prob(T(\tbX,\bY)\leq T(\bX,\bY))=\E_{Z\sim \normal(0,1)}\left[\Phi\left(\eta Z\right) \right] \,.
%\]
In the next lemma we show that $\E_{Z\sim \normal(0,1)}\left[\Phi\left(\eta Z\right) \right] = 1/2$, which completes the proof of the first part.
\begin{lemma}\label{lem:phi-etaZ}
Let $\Phi(\cdot)$ denote the distribution of standard normal variable. Then, for any constant $\eta$ we have
\[
\E_{Z\sim \normal(0,1)}\left[\Phi\left(\eta Z\right) \right] = 1/2.
\]
\end{lemma}

The second part of the lemma follows by a similar argument.
We have 
\begin{align*}
\E\left[\gamma_n^2(\bX,\bY)\right] 
& \leq \frac{1}{8n} \E\left[ \left( C_0+\sum\limits_{i=1}^{n}\frac{C_1|Y_i|+C_2|Y_i|^2+C_3|Y_i|^3}{n} \right)^2 \right] \\
& \leq \frac{1}{4n} \E\left[  C_0^2+ \left(\sum\limits_{i=1}^{n}\frac{C_1|Y_i|+C_2|Y_i|^2+C_3|Y_i|^3}{n} \right)^2 \right] \\
& \leq \frac{1}{4n} \left(C_0^2+ \frac{3}{n} \E\Big[\sum_{i=1}^n C_1^2|Y_i|^2+C_2^2|Y_i|^4+C_3^2|Y_i|^6 \Big] \right) \\
&= \frac{1}{4n} \left( C_0^2+ 3C_1^2\E[|Xg(X)+\eps|^2] +3C_2^2\E[|Xg(X)+\eps|^4]+3C_3^2\E[|Xg(X)+\eps|^6] \right) \,,
\end{align*}
 which implies that $ \lim\limits_{n\rightarrow \infty}^{} \E\left[\gamma_n^2(\bX,\bY)\right] =0$.
 
Also, by using Lemma \ref{lemma:crt-failure-1} and an application of the Portmanteau theorem for the bounded continuous function $\Phi^2$ we obtain
\[
\lim\limits_{n\rightarrow \infty}^{}  \E \left[ \Phi^2\left(    \frac{T(\bX,\bY)- m_n(\bX,\bY)}{ v_n(\bX,\bY)^{1/2} }   \right) \right]=\E_{Z\sim \normal(0,1)}[\Phi^2(\eta Z)]\,,
\]
which by invoking Lemma~\ref{lemma:indicators-cond-exp} completes the proof of Lemma \ref{lemma: crt-fails0} second part.








\subsubsection{Proof of Lemma \ref{lemma:indicators-cond-exp}}
We focus on the distribution of $T(\tbX,\bY)|\bX,\bY$ and treat $\bX,\bY$ as deterministic values, so the only source of randomness is $\tbX$.  To lighten the notation, we write 
\[
v_n: = v_n(\bX,\bY) = \frac{2}{n} + \frac{4\|\bY\|_2^2}{n^2}\,,
\]
and introduce  
\[
\xi_i=\frac{(Y_i-\tX_i)^2-(Y_i^2+1)}{n v_n^{1/2}}\,,\quad\text{for } i\in [n]\,.
\]

By simple algebraic computations, we get that $\E[\xi_i|\bX,\bY]=0$ and $\sum\limits_{i=1}^{n} \E[\xi^2|\bX,\bY]=1$. Also, conditioned on $\bX,\bY$, random variables $\xi_i$ are independent. We next use the Berry-Essen theorem to characterize the distribution of $\sum_{i=1}^n \xi_i$. For the reader's convenience, the version of the Berry-Esseen theorem for non-identical random variables is provided in Lemma \ref{lemma: berry-essen}.  First, we need to bound the sum of third moments:
\begin{align*}
\sum\limits_{i=1}^{n}\E[|\xi_i^3|]&=\frac{1}{n^3v_n^{3/2}}\sum\limits_{i=1}^{n}\E \left[\left | (Y_i-\tX_i)^2-(Y_i^2+1) \right|^3 \right]\\
&= \frac{1}{n^3v_n^{3/2}}\sum\limits_{i=1}^{n}\E \left[\left | \tX_i^2-2\tX_iY_i -1 \right|^3 \right]\\
&\overset{(a)}{\leq} \frac{1}{n^3v_n^{3/2}}\sum\limits_{i=1}^{n} (C_0+C_1|Y_i|+C_2|Y_i|^2+C_3|Y_i|^3)\\
&= \frac{1}{n^2 v_n^{3/2} } \left( C_0+ \sum\limits_{i=1}^{n}\frac{C_1|Y_i|+C_2|Y_i|^2+C_3|Y_i|^3}{n} \right)\,,
\end{align*}
where in (a), for $i=0,1,2,3$, coefficients $C_i$ are universal constants that can be precisely computed by using moments of the half-normal distribution. Note that here the expectation is with respect to $\tX_i$.
%\begin{align*}
%m_n(\bX,\bY)&:=\E[ T(\tbX,\bY)|\bX,\bY]\,,\\
%v_n(\bX,\bY)&:=\var[T(\tbX,\bY)|\bX,\bY]\,, \\
%\rho_n(\bX,\bY)&:=\E\left[\left(\left|T(\tbX,\bY)-m_n(\bX,\bY) \right|\right)^3\big|\bX,\bY\right]\,.
%\end{align*}

%By simple algebraic calculations we can get

%\[
% m_n(T(\bX,\bY)|\bX,\bY)=\frac{||\bY||_2^2}{n}+1\,,\quad   v_n(T(\bX,\bY)|\bX,\bY)=\frac{4 ||\bY||_2^2}{n^2}+\frac{2}{n}\,.
%\] 
Now, we employ the Berryâ€“Esseen theorem \ref{lemma: berry-essen} to get:
%
\begin{align}\label{eq: berry-1}
\sup\limits_{z}^{}\left|\prob(  \sum\limits_{i=1}^{n} \xi_i \leq z  | \bX,\bY )- \Phi\left( z  \right) \right |  \leq \gamma_n(\bX,\bY)\,.
\end{align}
%
From the definition of $\xi_i$ and recalling the definition of score $T(\tbX,\bY)$ we have
\begin{align*}
 T(\tbX,\bY)& = \frac{1}{n}\|\tbX-\bY\|_2^2 = \frac{\|\tbX\|_2^2}{n}+\frac{\|\bY\|_2^2}{n} - \frac{2\bX^\sT\bY}{n}\\
 & = 1+ \frac{\|\bY\|_2^2}{n} + \frac{\|\tbX\|_2^2}{n}- \frac{2\bX^\sT\bY}{n} - 1\\
 & = m_n(\bX,\bY)  + v_n(\bX,\bY)^{1/2}\sum_{i=1}^n \xi_i\,.
\end{align*}

Using the above relation and choosing $z = \tfrac{T(\bX,\bY)-m_n(\bX,\bY)}{v_n(\bX,\bY)^{1/2}}$ in equation\eqref{eq: berry-1} (note that $z$ is a measurable function of $\bX,\bY$), we get
%
\begin{align*}
\Big|\prob({ T(\tbX,\bY) \leq T(\bX,\bY) } | \bX,\bY ) -  \Phi\left(  \frac{T(\bX,\bY)-m_n(\bX,\bY)}{v_n(\bX,\bY)^{1/2}} \right) \Big| \leq   \gamma_n(\bX,\bY)\,.
\end{align*}

%Use the conditional expectation tower property to arrive at
%\[
%\left| \mu_n- \E\left[\Phi\left( \frac{T(\bX,\bY)- m_n(\bX,\bY)}{ v_n(\bX,\bY)^{0.5} } \right)\right] \right| \leq   \E\left[\frac{C\rho_n(\bX,\bY)}{v_n(\bX,\bY)^{1.5} \sqrt{n}}\right]
%\]






\subsubsection{Proof of Lemma \ref{lemma:crt-failure-1} }
Substituting for $T(\bX,\bY)$, $m_n(\bX,\bY)$ and $v_n(\bX,\bY)$ we get
\begin{align}
 \frac{T(\bX,\bY)- m_n(\bX,\bY)}{ v_n(\bX,\bY)^{1/2} }&=  \frac{ \|\bX-\bY\|_2^2/n - {\|\bY\|_2^2}/{n}-1 }{ \left(  {4 \|\bY\|_2^2}/{n^2}+{2}/{n} \right)^{1/2} } \nonumber\\
 &=\frac{\sqrt{n}\left(  \|\bX\|_2^2/n-2\bY^T\bX/n-1  \right)} { \left( 2+4\|\bY\|_2^2/n  \right)^{1/2} }\,, \label{eq: mu_n-inside-term}
\end{align}
By an application of the central limit theorem, the numerator converges in distribution to a normal random variable. More precisely,
 \begin{align*}
 \E[X^2-2YX-1]&=-2\E[XY]\\
 &=-2\E[Xg(X)]-2\E[X\eps]=0\,,
 \end{align*}
where in the last relation we used the property that  $g$ is an even function and $X\sim\normal(0,1)$. In addition, $\var[(X^2-2YX-1)]=6+4\E[X^2g(X)^2]$ by simple calculation. Therefore, by CLT we have
\begin{equation} \label{eq:numerator-slutsky}
\sqrt{n} \left(||\bX-\bY||_2^2/n - {||\bY||_2^2}/{n}+1\right) \overset{d}{\Rightarrow}\normal(0,6+4\E[X^2g(X)^2])
\end{equation}

On the other hand, from the weak law of large numbers we have that the denominator in \eqref{eq: mu_n-inside-term} converges in probability to $6+4\E[g(X)^2]$. The proof is completed by using the Slutsky's theorem. %in conjunction with equations \eqref{eq:numerator-slutsky} and \eqref{eq:denominator-slutsky}.

%=========================
\subsubsection{Proof of Lemma~\ref{lem:phi-etaZ}}
Let $\Phi(\cdot)$ and $\phi(\cdot)$ respectively denote the distribution and the density function of the standard normal random variable. We write
\begin{align}
\frac{\partial}{\partial \eta} \E[\Phi(\eta Z)] = \E[Z \phi(\eta Z)]
\stackrel{(a)}{=} \E[\eta \phi'(\eta Z)] \stackrel{(b)}{=} \E[-\eta^2 Z \phi(\eta Z)] = -\eta^2  \frac{\partial}{\partial \eta} \E[\Phi(\eta Z)]\,, 
\end{align}
where we used Stein's lemma in step $(a)$, and the identity $\phi'(t) = -t\phi(t)$ in step (b). Therefore, $\frac{\partial}{\partial \eta} \E[\Phi(\eta Z)] = 0$ and so $ \E[\Phi(\eta Z)] =  \E[\Phi(0)] = 1/2$.

%%%%%%%%%%%%%%%%%%
\subsection{Proof of Theorem \ref{coro: failure-crt}}\label{proof:coro: failure-crt}
From Proposition \ref{thm: crt-failure} we know that

\begin{equation}\label{eq: tmp5}
\lim\limits_{M\rightarrow \infty}\lim\limits_{n\rightarrow \infty}^{}\prob\left(\left|  p^{(M)}_n -1/2 \right| \geq \delta \right) \leq  \frac{1}{\delta^2} \left(  \E_{Z\sim\normal(0,1)}[\Phi^2(\eta Z)] -1/4) \right)\,.
\end{equation}
In order to find the probability of divergence from $1/2$, we need to upper bound  the right-hand-side of the above expression. Obviously, it is zero at $\eta=0$ and we will show that for small enough $\eta$ we get good concentration around $1/2$.  To this end, note that for the function $g(x)=1/\sqrt{\theta^2+x^2}$ the following holds
\begin{equation}\label{eq:bound-x2g2}
\E[X^2g(X)^2]=\E\left[ \frac{X^2}{\theta^2+X^2} \right]\leq 1\,.
\end{equation}
On the other hand, we have:
\begin{align}
\E[g(X)^2]&=\frac{1}{\sqrt{2\pi}}\int\limits_{-\infty}^{\infty} \frac{e^{-x^2/2}}{\theta^2+x^2}dx \nonumber\\
&\overset{(a)}{\geq} \frac{1}{\sqrt{2\pi}}\int\limits_{-\sqrt{2}}^{\sqrt{2}} \frac{1-x^2/2}{\theta^2+x^2}dx\nonumber\\ 
&\geq \frac{1}{\sqrt{2\pi}}\int\limits_{-\sqrt{2}}^{\sqrt{2}} \frac{dx}{\theta^2+x^2} -{\frac{1}{\sqrt{\pi}}}\nonumber\\ 
&=\frac{2}{\theta\sqrt{2\pi}}\arctan\frac{\sqrt{2}}{\theta}-{\frac{1}{\sqrt{\pi}}}\nonumber\\
&\overset{(b)}{\geq} \frac{2}{\theta\sqrt{2\pi}} \left(  \frac{\pi}{2}-\frac{\theta}{\sqrt{2}} \right)  -{\frac{1}{\sqrt{\pi}}}\nonumber\\
&\geq \frac{1}{\theta}\sqrt{\frac{\pi}{2}} -\frac{2}{\sqrt{\pi}}\label{eq:bound-g2} \,,
\end{align}
where in (a) we used the $e^x\geq 1+x$ inequality, and in (b) we used the $\arctan(x)\geq \pi/2-1/x$ inequality. Next, 
use \eqref{eq:bound-x2g2} and \eqref{eq:bound-g2} to get
%
\begin{equation}\label{eq:eta-upper}
\eta^2 = \frac{3+2\E[X^2g(X)^2]}{3+2\E[g(X)^2]} \leq \frac{5\sqrt{\pi}}{3\sqrt{\pi}-{4} +{\sqrt{2}\pi}/{\theta} } < \frac{5\theta}{\sqrt{2\pi}}\,,
\end{equation}
which implies that by choosing $\theta$ small enough, we can make $\eta$ arbitrarily small. 

We next show that the deviation of the $p$-statistic from $1/2$ can be controlled by the choice of $\eta$. 
%By recalling our initial observation, it implies that by having $c$ close enough to $0$, we can get highly concentrated values of the  CRT statistic around $1/2$. To accurately capture this behavior, we want to use this upper bound on $\eta$ in Theorem \ref{thm: crt-failure} and bound the deviation of the $p$ statistic. 

Note that for the normal distribution function $\Phi$ and the normal density $\phi$ we have %has bounded derivative $1/\sqrt{2\pi}$, for every $z$ this yields 
 \[
0\leq  \Phi(\eta z)= \Phi(0)+ \int_{0}^{\eta z} \phi(t)\de t \le \frac{1}{2}+ \frac{\eta |z|}{\sqrt{2\pi}} \,.
 \]
Consequently for $Z\sim\normal(0,1)$,
\[
\E\left[\Phi^2(\eta Z)\right] \leq \E\Big[ \Big(\frac{1}{2}+\frac{\eta |Z|}{\sqrt{2\pi}}\Big)^2\Big] = \frac{1}{4}+\frac{\eta^2}{2\pi} + \frac{\eta}{\pi}\,.
\]
%by simplifying this expression and using the half-normal distribution mean value, we finally have
Therefore,
\[
\E\left[\Phi^2(\eta Z)\right] - \frac{1}{4} \leq \frac{\eta^2+2\eta}{2\pi}\,,
\]
which along with \eqref{eq: tmp5} imply
%
\[
\lim\limits_{M\rightarrow \infty}^{} \lim\limits_{n\rightarrow \infty}^{}\prob\Big(  \Big|p_{n}^{(M)}  -\frac{1}{2} \Big|   \geq \delta\Big) \leq \frac{\eta^2+2\eta}{2\pi\delta^2}\,.
\]
Next be choosing $\delta=(1-\alpha)/2$ and using~\eqref{eq:eta-upper}, we obtain that for $\theta<\alpha^2/500$ and $\alpha\le 1/2$,
%
\[
\lim\limits_{M\rightarrow \infty}^{} \lim\limits_{n\rightarrow \infty}^{}\prob\Big(  \Big|p_{n}^{(M)}  -\frac{1}{2} \Big|   \geq  \frac{1-\alpha}{2} \Big)\leq  \frac{\alpha}{2}\,,
\]
This completes the proof of part (a) for two-sided CRT. 

Proof of part (b) follows along the same lines. The only modification is that time we set $\delta = 1/2-\alpha$. Invoking \eqref{eq:eta-upper} with assumptions $\alpha \le 1/2-\gamma$ and $\theta\leq \gamma^4\alpha^2/2$, we get 

\begin{align*}
\lim\limits_{M\rightarrow \infty}^{} \lim\limits_{n\rightarrow \infty}^{}\prob\left(  p_{n}^{(M)}   \geq  1-\alpha \right) \leq \frac{\alpha}{2}\,,\quad
\lim\limits_{M\rightarrow \infty}^{} \lim\limits_{n\rightarrow \infty}^{}\prob\left(  p_{n}^{(M)}   \leq  \alpha \right) \leq \frac{\alpha}{2}\,.
\end{align*}
%
This completes the proof of part (b) for one-sided CRT. 




























%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Proof of Theorem \ref{thm: chi^2-CI-size}}\label{proof:thm: chi^2-CI-size}
Because of Assumption \ref{assum: mu_mapping} (continuity of probability laws),  with probability one all the score values are distinct and so there is no ambiguity (tie) in labeling the data points.
Recall $W_\ell$ as the number of data points with label $\ell$.
By construction, the joint distribution of $(nW_1,nW_2,...,nW_{L})$ is a multinomial distribution with $L$ distinct values (number of labels). Denote by $p_\ell$ the probability of getting label $\ell$. 
%
% where for $1\leq s\leq \ell$, the category $s$ occurs with probability $p_s$, and .  We computed a closed-form value of $p_s$ in Proposition \ref{propo: lambda_bound}. 
Then, the statistic $U_{n,L}$, given by~\eqref{eq:U_nell}, is the standard Pearson's $\chi^2$ test statistic for testing the null hypothesis 
\begin{equation}\label{eq: tmp11}
H'_0: p_{\ell}=\frac{1}{L}, \text{ for } \ell\in[L]\,.
\end{equation}
Now by using Lemma \ref{lemma: multinomial-main} (Part 1), we get $U_{n,L} \overset{d}{\Rightarrow} \chi^{2}_{L-1}$, as $n\to\infty$.  The claim about $\th_{L,\alpha}^{\mathsf{finite}}$ follows from Part 2 of Lemma \ref{lemma: multinomial-main}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Proof of Remark \ref{rmk: Delta upper bound} }\label{proof:rmk: Delta upper bound}
Define 
\[
g(u,z,y) : = \frac{\partial}{\partial u}F_{T|ZY}\Big( F_{T|Z}^{-1}\big(u; Z,Y\big) ; Z,Y\Big)  \,.
\]
Then by Assumption \ref{assum: dR-L1-integrable} we have $\int_0^1 \E_{(Z,Y) \sim \cL(Z,Y)}[|g(u,Z,Y)|]<\infty$ and as an application of the Fubini's theorem, we can change the order of integration and the expectation and get:
\begin{align*}
\int_0^u \E_{(Z,Y) \sim \cL(Z,Y)}[g(v,Z,Y)]\de v &= 
 \E_{(Z,Y) \sim \cL(Z,Y)}\Big[\int_0^u g(v,Z,Y) \de v\Big]\\
 &= \E_{(Z,Y) \sim \cL(Z,Y)}\Big[F_{T|ZY}\Big( F_{T|Z}^{-1}\big(u; Z,Y\big) ; Z,Y\Big) - F_{T|ZY}\Big( F_{T|Z}^{-1}\big(0; Z,Y\big) ; Z,Y\Big)\Big]\\
 &=  \E_{(Z,Y) \sim \cL(Z,Y)}\Big[F_{T|ZY}\Big( F_{T|Z}^{-1}\big(u; Z,Y\big) ; Z,Y\Big)
\end{align*}
Now taking derivative of both sides with respect to $u$, we arrive at
\begin{align}\label{eq:interchange}
\E_{(Z,Y) \sim \cL(Z,Y)}[g(v,Z,Y)] =  \sr_T(u)\,.
\end{align}
	
	We next prove part (b) of the remark. Part(a) follows readily from part (a) since under the null hypothesis $T(X,Z,Y)$ and $T(\tX,Z,Y)$ have the same distribution.
	
	By definition of the conditional dependency power $\Delta_T(\cL(X,Z,Y))$, cf. Definition \ref{def: conditional-dependency-power} we have
	\begin{align*}
		\Delta_T(P_{X,Z,Y})&= \int_0^1 |\sr_T(u)-1|\de u\\
		&\overset{(a)}{=}\int\limits_{0}^{1}\left|\E_{(Z,Y) \sim \cL(Z,Y)}\left[ \frac{\partial}{\partial u} F_{T|ZY}\left( F_{T|Z}^{-1}\big(u;Z,Y\big); Z,Y\right)  \right] -1\right| \de u \\
		&\overset{(b)}{\leq} \int\limits_{0}^{1} \E_{(Z,Y) \sim \cL(Z,Y)}\left[ \left| \frac{\partial}{\partial u} F_{T|ZY}\left( F_{T|Z}^{-1}\big(u;Z,Y\big); Z,Y\right)-1\right|\right]\de u \\
		&\overset{(c)}{=} \E_{(Z,Y) \sim \cL(Z,Y)} \left[ \int\limits_{0}^{1}  \left| \frac{\partial}{\partial u} F_{T|ZY}\left( F_{T|Z}^{-1}\big(u; Z,Y\big); Z,Y\right)-1\right|\right]\de u \\
		&= \E_{(Z,Y) \sim \cL(Z,Y)}  \left[ \int\limits_{0}^{1} \left| \frac{f_{T|ZY}\left( F_{T|Z}^{-1}\big(u; Z,Y\big); Z,Y\right)}{ f_{T|Z}\left( F_{T|Z}^{-1}\big(u; Z,Y\big); Z,Y\right) }-1\right|\right]\de u \\
		&= \E_{(Z,Y) \sim \cL(Z,Y)}  \left[ \int\limits_{-\infty}^{\infty} \left| \frac{f_{T|Z,Y}\left(t;Z,Y \right)}{ f_{T|Z}( t;Z,Y) }-1\right|f_{T|Z}(t;Z,Y)\de t\right]\\
		%&\overset{(d)}{=}\E_{(Z,Y)\sim P_{Z,Y}}\left[  \E_{X\sim P_{X|Z}}\left[\left| \frac{  f_{T|ZY}\left(T(X,Z,Y)|Z,Y\right)  }{  f_{T|Z} \left(T(X,Z,Y)|Z,Y\right)  }-1\right| \right]\right]\\
		%&=\E_{(X,Z,Y)\sim P_{X|Z}\otimes P_{Y,Z} } \left[ \left| \frac{  f_{T|ZY}\left(T(X,Z,Y)|Z,Y\right)  }{  f_{T|Z} \left(T(X,Z,Y)|Z,Y\right)  }-1\right|     \right]\\
		%&\overset{(e)}{=}\E_{(X,Z,Y)\sim P_{X|Z}\otimes P_{Y,Z} } \left[ \left| \frac{  f_1(T(X,Z,Y))  }{  f_2(T(X,Z,Y)) }-1\right|     \right]\\
		&=\E_{(Z,Y) \sim \cL(Z,Y)}\left[2d_{\tv}\left(  (T(\tX,Z,Y)|Z,Y), (T(X,Z,Y)|Z,Y) \right)\right], 
	\end{align*}
	with $X\sim \cL(X|Z,Y)$, $\tX\sim \cL(X|Z)$, and $f_{T|Z,Y}$ and $f_{T|Z}$ representing the density functions corresponding to cdfs  $F_{T|Z,Y}$ and $F_{T|Z}$ . Note that in (a) we used \eqref{eq:interchange}; (b) is a direct result of Jenson's inequality, and (c) follows from Assumption \ref{assum: dR-L1-integrable} in conjunction with the Fubini's theorem.
	%(d) is a direct result from the definition of $f_{T|Z}(.|Z,Y)$ which is the density function of $T(X,Z,Y)$ when $X\sim P_{X|Z}(.|Z)$, and finally in $(e)$, $f_1(t), f_2(t)$ denote the probability density functions of random variable $T(X,Z,Y)$ when $X\sim P_{X|Z}$, and $X\sim P_{X|Z,Y}$, respectively; it is not hard to see that $f_1(t)=f_{T|Z}(t|z,y)f_{Z,Y}(z,y)$
	%he fact that $f_{T|ZY}(.|Z,Y)$ stands for the density function of  $T(X,Z,Y)$ when 
	
	
	
	
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Proof of Proposition \ref{propo: lambda_bound}}\label{proof:propo: lambda_bound}

	%It is easy to see that each $(X_j,Z_j,Y_j)$ can be categorized precisely in one of the $m+1$ possible values $1\leq \ell\leq m+1$ and  $Y_\ell$ denotes the normalized number of $\ell-$type triples.
Based on the Pearson $\chi^2$-CI statistic $U_{n,L}$ construction that is described in Algorithm \ref{algorithm: model-xz}, for a sample point $(X,Z,Y)$ and its $M$ constructed counterfeits $\{(\tX^{(i)},Y,Z)\}_{i=1}^M$ we have the following rank value 
\[
R=1+\sum\limits_{i=1}^{M}\ind_{ \{ T(X,Z,Y) \geq T(\tX^{(i)},Z,Y)  \}} \,.
\]
This allows us to compute the  probability of $(X,Z,Y)$ getting label $s\in[L]$:
\begin{align}
	\prob\left((X,Z,Y)\text{ has label } s \right)&=\prob\left((s-1)K+1 \leq R\leq sK\right)\nonumber\\
	&=\sum\limits_{j=K(s-1)+1}^{Ks}\prob(R=j)\nonumber\\
	&=\sum\limits_{j=K(s-1)+1}^{Ks}\E_{ZY}[\prob(R=j|Y=y,Z=z)]\label{eq:Ryz}\,.
\end{align}
%Introduce
%
%\[
%\gamma(z,y):= \prob\big(\mu(X,Z,Y)\geq \mu(\tX,Z,Y)|Z=z,Y=y\big),\quad\quad \text{with } X\sim P_{X|ZY}\text{ and } \tX\sim P_{X|Z}\,. 
%\]
Note that by conditioning on $(Z,Y)=(z,y)$, random variables $T(X,Z,Y)$ and $T(\tX_j,Z,Y)$ are independent. To lighten the notation, we use the shorthands $T:=T(X,Z,Y)$ and $\tT_i=T(\tX^{(i)},Z,Y)$, and proceed as follows:
%and $R$ has binomial distribution with $M$ trials and success probability $\gamma(z,y)$, therefore we get
\begin{align}
	\prob\left( R=j|Y=y,Z=z \right)&= \prob\left( T \text{ is exactly larger than } j-1\text{ of } \tT_i  |Z=z,Y=y \right)\nonumber\\
	&\overset{(a)}{=}\int \prob\big( t \text{ is exactly larger than } j-1\text{ of } \tT_i  |Z=z,Y=y \big)\; \de F_{T|ZY}(t;z,y)\nonumber\\
	&\overset{(b)}{=}\binom{M}{j-1} \int F_{T|Z}(t;z,y)^{j-1}(1-F_{T|Z}(t;z,y))^{M-j+1}\de F_{T|ZY}(t;z,y)\nonumber \\
	&{=}\binom{M}{j-1}\int\limits_{0}^{1} u^{j-1}(1-u)^{M-j+1}\de F_{T|ZY}\big(F_{T|Z}^{-1}(u;z,y);z,y\big)\label{eq: tmp6} \,,
\end{align}
where (a) comes from the fact that $T|ZY$ has density $F_{T|ZY}(.)$, and (b) holds since $\tT_i|ZY$ is distributed according to $F_{T|Z}(.)$, independent of $T$.  Note that for a function $f(t)$, the notation $\de f(t)  =f'(t)\de t$ denotes the differential of $f(t)$.
%In step $(c)$ we use the following identity which follows from the chain rule: 
%\[
%\partt f(t) = \partu f(g^{-1}(u))\,,
%\]
%if $u = g(t)$, for any differentiable functions $g,f$ with $g$ being an invertible function.

We next plug in equation \eqref{eq: tmp6} into \eqref{eq:Ryz} to get

\begin{align}
	\prob\left((X,Z,Y)\text{ has label } s \right)&=
	\sum\limits_{j=K(s-1)+1}^{Ks}\binom{M}{j-1}\E_{ZY}
	\left[ \int\limits_{0}^{1} u^{j-1}(1-u)^{M-j+1}\de F_{T|ZY}\big(F_{T|Z}^{-1}(u;Z,Y);Z,Y\big)       \right]\nonumber\\
	&\overset{(a)}{=}	\sum\limits_{j=K(s-1)+1}^{Ks}\binom{M}{j-1} \int\limits_{0}^{1} u^{j-1}(1-u)^{M-j+1}\E_{ZY}
	\left[\de F_{T|ZY}\big(F_{T|Z}^{-1}(u;Z,Y);Z,Y\big) \right]\nonumber\\
	&\overset{{(b)}}{=} \sum\limits_{j=K(s-1)+1}^{Ks}\binom{M}{j-1} \int\limits_{0}^{1} u^{j-1}(1-u)^{M-j+1}\de\E_{ZY}
	\left[F_{T|ZY}\big(F_{T|Z}^{-1}(u;Z,Y);Z,Y\big) \right]\nonumber\\
	&=\sum\limits_{j=K(s-1)+1}^{Ks}\binom{M}{j-1} \int\limits_{0}^{1} u^{j-1}(1-u)^{M-j+1}\de \sR_{T}(u)\nonumber\\
	&=\sum\limits_{j=K(s-1)}^{Ks-1}\binom{M}{j} \int\limits_{0}^{1} u^{j}(1-u)^{M-j} \sr_T(u)\de u\label{eq: tmp7}\,,
\end{align}
in (a) we used the Fubini's theorem along with Assumption \ref{assum: dR-L1-integrable} and the fact that for every $0\leq u\leq 1 $ we have $|u^{j}(1-u)^{M-j}|\leq 1$.  Also, (b) is a direct result of Assumption \ref{assum: dR-L1-integrable} and dominated convergence theorem. This completes the proof of claim~\eqref{eq: tp_j}.

It is worth noting that, when $X\indep Y|Z$, we have $P_{X|ZY}=P_{X|Z}$ which implies $\sR_{T}(u)=u$, so the conditional relative density function $\sr_{T}(u)$ always attains the constant value $1$. In this case, we have 
\begin{align}
	p_{s}&= \sum\limits_{j=K(s-1)}^{Ks-1}\binom{M}{j} \int\limits_{0}^{1} u^{j}(1-u)^{M-j}\de u\nonumber\\
	&=\sum\limits_{j=K(s-1)}^{Ks-1}\binom{M}{j} B(j+1,M-j+1)\nonumber\\
	&=\sum\limits_{j=K(s-1)}^{Ks-1}\binom{M}{j} \frac{\Gamma(j+1)\Gamma(M-j+1)}{\Gamma(M+2)}\nonumber\\
	&=\sum\limits_{j=K(s-1)}^{Ks-1}\binom{M}{j} \frac{j!(M-j)!}{(M+1)!}\nonumber\\
	&=\sum\limits_{j=K(s-1)}^{Ks-1}\frac{1}{M+1}\nonumber\\
	&=\frac{k}{M+1}=\frac{1}{L}\label{eq: tmp8}\,,
\end{align}	
where $B(a,b)$ is the Beta function and $\Gamma(a)$ is the Gamma function. 


Now, we are ready to prove Part $(i)$. First note that deriving a more explicit characterization of $p_s$ from~\eqref{eq: tmp7} is in general intractable, due to the relative density term $\sr_{T}(u)$ in the inner integral expression . However, it is useful to note that if $\sr_{T}(u)$ is a polynomial of $u$, then this probability can be easily computed by absorbing that into the integral formulation of the Beta function and then leveraging the connection between the Gamma function and Beta function for integer values.  Inspired by this observation, our strategy is to  approximate $\sr_{T}(u)$ with polynomials.  To this end, note that by Assumption \ref{assum: sr_continuity}, $\sr_{T}(u)$ is a continuous function over $[0,1]$ interval, which allows us to use the Weierstrass theorem to uniformly approximate $\sr_T(u)$ as closely as desired by polynomials.  Formally, for any $\eps>0$ there exists a polynomial $\widetilde{r}(u)$ with real coefficients such that 
%
\begin{equation}\label{eq: def-eps_i}
\sup\limits_{u\in [0,1]}^{} |\widetilde{r}(u)-\sr_T(u)| <\eps \,.
\end{equation}
%we have 
%\begin{equation}\label{eq: lim-eps_j}
%\lim\limits_{i\rightarrow \infty}^{}\eps_i=0\,.
%\end{equation}
Further, from \eqref{eq: tmp7} for every $\ell\in[L]$ we have
\begin{align}
	\sum\limits_{s=1}^{\ell}p_s&= \sum\limits_{j=0}^{\ell K-1} \binom{M}{j}\int\limits_{0}^{1}u^{j}\big(1-u\big)^{M-j}\sr_{T}(u)\de u\nonumber\\
	&\geq \sum\limits_{j=0}^{\ell K-1} \binom{M}{j}\int\limits_{0}^{1}u^{j}\big(1-u\big)^{M-j}\widetilde{r}(u)\de u -\eps  \sum\limits_{j=0}^{\ell K-1} \binom{M}{j}\int\limits_{0}^{1}u^{j}\big(1-u\big)^{M-j}\de u\nonumber\\
	&= \sum\limits_{j=0}^{ \ell K-1} \binom{M}{j}\int\limits_{0}^{1}u^{j}\big(1-u\big)^{M-j}\widetilde{r}(u)\de u -\frac{\ell\eps}{L}\label{eq:lower-p_ell}\,,
\end{align}
where in the last equality we used the result in \eqref{eq: tmp8} that when $\sR_{T}(u)=u$, we have $p_{s}=1/L$. We are left with lower bounding the right-hand side summation in \eqref{eq:lower-p_ell}. Let $\widetilde{r}(u)$ be a polynomial of degree $N$ and coefficients $a_t$, i.e. $\widetilde{r}(u)=\sum\limits_{t=0}^{N}a_tu^t$. We have 
 \begin{align}
 	&\sum\limits_{j=0}^{\ell K-1} \binom{M}{j}\int\limits_{0}^{1}u^{j}\big(1-u\big)^{M-j}\widetilde{r}(u)\de u\nonumber\\
 	&=\sum\limits_{j=0}^{\ell K-1} \binom{M}{j}\int\limits_{0}^{1}u^{j}\big(1-u\big)^{M-j}\sum\limits_{t=0}^{N}a_tu^t\de u\nonumber\\
 	&=\sum\limits_{j=0}^{ \ell K-1}
 	\sum\limits_{t=0}^{N}a_t \binom{M}{j}\int\limits_{0}^{1}u^{j+t}\big(1-u\big)^{M-j}\de u\nonumber\\
 	&=\sum\limits_{j=0}^{\ell K-1}
 	\sum\limits_{t=0}^{N}a_t \binom{M}{j}B(j+t+1,M-j+1)\nonumber\\
 	&=\sum\limits_{j=0}^{\ell K-1}
 	\sum\limits_{t=0}^{N}a_t \binom{M}{j} \frac{(j+t)!(M-j)!}{(M+t+1)!\nonumber}\\
 	&=	\sum\limits_{t=0}^{N} a_t\frac{M!t!}{(M+t+1)!}\sum\limits_{j=0}^{\ell K-1}
 \binom{j+t}{t}\nonumber\\
 &=	\sum\limits_{t=0}^{N} a_t\frac{M!t!}{(M+t+1)!}
 \binom{\ell K+t}{t+1}\nonumber\\
 &=	\sum\limits_{t=0}^{N} \frac{a_t}{t+1}\prod\limits_{h=0}^{t}\frac{\ell K +h}{M+1+h}\label{eq:sum-p_ell-polynomial}\,,
 \end{align}
where in the penultimate equation, we used the Hockey-stick identity. Next, use the following simple inequality in \eqref{eq:sum-p_ell-polynomial}
 $$\frac{\ell K+h}{M+1+h}\geq \frac{\ell K}{M+1}=\frac{\ell}{L}\,,$$
to arrive at
\begin{align*}
\sum\limits_{j=0}^{\ell K-1} \binom{M}{j}\int\limits_{0}^{1}u^{j}\big(1-u\big)^{M-j}\widetilde{r}(u)\de u\nonumber\geq \sum\limits_{t=0}^{N} \frac{a_t}{t+1}\left( \frac{\ell}{L} \right)^{t+1}=\int\limits_{0}^{\frac{\ell}{L}} \widetilde{r}(u)\de u\,.
\end{align*}
Next we plug the above lower bound into \eqref{eq:lower-p_ell} to get
\[
\sum\limits_{s=1}^{\ell}p_{s} \geq \int\limits_{0}^{\frac{\ell}{L}} \widetilde{r}(u)\de u-\frac{\ell\eps}{L}\,,
\]
which along with \eqref{eq: def-eps_i} implies that
%
\[
\sum\limits_{s=1}^{\ell}p_{s} \geq \int\limits_{0}^{\frac{\ell}{L}} \sr_T(u)du-\frac{2\ell\eps}{L}=\sR_{T}\left(\frac{\ell}{L} \right)-\frac{2\ell\eps}{L}\,.
\]
Finally, since $\eps>0$ can be chosen arbitrarily small, by letting $\eps\to 0$ we get the desired claim of \eqref{eq:sum-p_ell-lower}.

We next proceed to Part $(ii)$. In Part $(i)$, we use a general form of the Weierstrass approximation theorem, to uniformly approximate $\sr_T$ as closely as desired, while the rate of convergence (in terms of the polynomial degree) was not needed.
For establishing an upper bound on the sum of labels probabilities,  $\sum\limits_{s=1}^{\ell}p_s$, we need to upper bound the polynomial-approximation error, and knowing the convergence rate becomes important. For this reason, we use a more refined  version of the Weierstrass approximation theorem. For the reader's convenience, we state this version in the following lemma, borrowed from~\cite{gzyl1997weierstrass}:
\begin{lemma}[ \cite{gzyl1997weierstrass}, Theorem 1]\label{lem:poly}
	Let $f$ be a $B$-bounded and  $C-$Lipschitz continuous function on $[0,1]$. Then, for every positive integer $N$, there exists a polynomial $\widetilde{f}_N$ of degree $N$ such that
	\[
\sup\limits_{u\in[0,1]}^{}|f(u)- \widetilde{f}_N(u)|\leq(C/2+2B) \sqrt{\frac{\log{N}}{N}}\,.
	\]
\end{lemma}
	Recall that by Assumption \ref{assum: sr_continuity}, $\sr_T(u)$ is $B$-bounded and $C$-Lipschitz, and therefore, by an application of Lemma~\ref{lem:poly} there exists a polynomial $\widetilde{r}_N$ of degree $N$, such that for $D=C/2+2B$ we have
	\begin{equation}\label{eq:unif-Bernstein-polynomials}
	\|\sr_{T}-\widetilde{r}_N\|_\infty \leq D\sqrt{\frac{\log{N}}{N}}\,.
	\end{equation}
	Let $\widetilde{r}_N(u)=\sum\limits_{t=0}^{N}a_tu^t$. By a similar argument used in deriving \eqref{eq:lower-p_ell} and \eqref{eq:sum-p_ell-polynomial}, we get
	\begin{align}\label{eq:dum0}
	\sum\limits_{s=1}^{\ell}p_s&\leq \sum\limits_{t=0}^{N} \frac{a_t}{t+1}\prod\limits_{h=0}^{t}\frac{\ell K +h}{M+1+h} + \frac{\ell D}{L}\sqrt{\frac{\log{N}}{N}}\,.
	\end{align}
To further simplify the right-hand side, we use the following simple algebraic manipulations. Since $h\le t\le N$ and $M+1 = LK\ge \ell K$ we have $(M+1-\ell K)(N-h)\ge0$, from which we get 
\begin{align*}
\frac{\ell K+h}{M+1+h} &\leq  \frac{\ell K+N}{M+1+N}\\
&= \frac{\ell K+N}{LK+N} =\frac{\ell}{L}\left(\frac{K+\tfrac{N}{\ell}}{K+\tfrac{N}{L}}\right)\le \frac{\ell}{L}\left(1+\frac{N}{K}\right)\,.
\end{align*}
Using this bound in~\eqref{eq:dum0}, for $h\ge 1$, we arrive at
\begin{align*}
	\sum\limits_{s=1}^{\ell}p_s &\leq\left(1+\frac{N}{k}\right)^N \sum\limits_{t=0}^{N} \frac{a_t}{t+1}\left(\frac{\ell}{L}\right)^{t+1}+ \frac{\ell D}{L}\sqrt{\frac{\log{N}}{N}}\\
	&=\left(1+\frac{N}{K}\right)^N\int\limits_{0}^{\frac{\ell}{L}}\widetilde{r}_N(u)\de u+ \frac{\ell D}{L}\sqrt{\frac{\log{N}}{N}}\\
	&\leq e^{N^2/K}\int\limits_{0}^{\frac{\ell}{L}}\widetilde{r}_N(u)\de u+ \frac{\ell D}{L}\sqrt{\frac{\log{N}}{N}}\,.
	\end{align*}
By using \eqref{eq:unif-Bernstein-polynomials} again, we obtain 
%
\begin{align*}
	\sum\limits_{s=1}^{\ell}p_s&\leq e^{N^2/K}\int\limits_{0}^{\frac{\ell}{L}}\widetilde{r}_N(u)\de u+ \frac{2\ell D}{L}\sqrt{\frac{\log{N}}{N}}\\
	&=e^{N^2/k}\;\sR_{T}\left(\frac{\ell}{L} \right)+ \frac{2\ell D}{L}\sqrt{\frac{\log{N}}{N}}\ \,.
\end{align*}
Set $N=\sqrt{k\log(1+\delta) }$ for a fixed $0<\delta<1$ and rewrite the above bound as
\[	\sum\limits_{s=1}^{\ell}p_s \leq
(1+\delta)\sR_{T}\left(\frac{\ell}{L} \right)+ \frac{2\ell D}{L}\left( \frac{\log\left(K\log(1+\delta)\right)}{2\sqrt{K\log(1+\delta)}}\right)^{1/2}\,.
\]
By using the relations  $L\leq\ell$, $\delta<1$, $\sR_T(u)\leq 1$, and $\log(1+\delta)\geq\delta/2$, for $\delta\in[0,1]$, we obtain

\[	\sum\limits_{s=1}^{\ell}p_s \leq
\sR_{T}\left(\frac{\ell}{L} \right)+ \delta +2D\left( \frac{\log{K}}{\sqrt{K\delta}}\right)^{1/2}\,.
\]
Minimizing the right-hand side over $\delta$, we get $\delta = \left(\frac{4D^2\log{k}}{\sqrt{k}}  \right)^{2/5}$ , which is smaller than one for $k$ sufficiently large. Plugging in for this value of $\delta$ we obtain 
\[
\sum\limits_{s=1}^{\ell}p_s \leq \sR_{T}\left(\frac{\ell}{L} \right)+\nu_K\,,
\]
with $\nu_K=2\left(\frac{4D^2\log{K}}{\sqrt{K}}  \right)^{2/5}$.
%%%%%%%%%%5
%%%%%%%%%%%%%

We next proceed to prove Part $(iii)$. 
%Equation \eqref{eq: lambda_n} gives us 
%\begin{align}
%\lambda_n&=n(m+1)\sum\limits_{\ell=1}^{m+1}\left( p_\ell-\frac{1}{m+1} \right)^2\nonumber\\
%&=n\left( (m+1) \sum\limits_{\ell=1}^{m+1}p_\ell^2-1\right)\label{eq:lambda_n-simplified}\,.
%\end{align}
For $1\leq s\leq \ell$, let 
\begin{equation}\label{eq: tmp9}
q_s:= \sR_{T}\left( \frac{s}{L}\right)- \sR_{T}\left( \frac{s-1}{L}\right)\,,
\end{equation}
By employing the results of parts $(i)$ and $(ii)$ we have
\begin{align*}
\left| p_s-q_s \right| \le \Big|\sum_{j=1}^{s} p_j - \sR_{T}\left( \frac{s}{L}\right) - \sum_{j=1}^{s-1} p_j
+ \sR_{T}\left( \frac{s-1}{L}\right) \Big| \leq \nu_K\,.
\end{align*}
Therefore,
\begin{align}
\sum\limits_{s=1}^{L}\left|p_s-\frac{1}{L}\right|
\geq \sum\limits_{s=1}^{L}\left|q_s-\frac{1}{L}\right| -\sum\limits_{s=1}^{L}\left|p_s-q_s\right|
\geq -L\nu_K+ \sum\limits_{s=1}^{L}\left|q_s-\frac{1}{L}\right|\label{eq:lambda_n_lower_1}\,.
%\sum\limits_{\ell=1}^{m+1}p_\ell^2&=\sum\limits_{\ell=1}^{m+1}q_\ell^2+\sum\limits_{\ell=1}^{m+1}(p_\ell^2-q_\ell^2)\\
%&\geq \sum\limits_{\ell=1}^{m+1}q_\ell^2 -\nu_k \sum\limits_{\ell=1}^{m+1}(p_\ell+q_\ell)\\
%&=\sum\limits_{\ell=1}^{m+1}q_\ell^2 -2\nu_k\,,
\end{align}
%\begin{equation}\label{eq:lambda_n_lower_1}
%\sum\limits_{\ell=1}^{m+1}\left|p_\ell-\frac{1}{m+1}\right| \geq \sum \limits_{\ell=1}^{m+1}\left|q_\ell-\frac{1}{m+1} f\right|
%\end{equation}
Next, by applying the mean value theorem in the definition of $q_s$ in \eqref{eq: tmp9}, for every $s\in [L]$, there exists $\xi_s \in \left(\frac{s-1}{\ell},\frac{s}{\ell}  \right)$, such that $q_s=\sr_{T}(\xi_s)/L$. Therefore,
\begin{align*}
	\sum\limits_{s=1}^{L} \left|q_s-\frac{1}{L}\right| &=\frac{1}{L}\sum\limits_{s=1}^{L} |\sr_{T}(\xi_s)-1|\\
	&=\sum\limits_{s=1}^{L}\int\limits_{\frac{s-1}{L}}^{\frac{s}{L}}|\sr_{T}(\xi_s)-1|\de u\\
	&\geq\sum\limits_{s=1}^{L}\int\limits_{\frac{s-1}{L}}^{\frac{s}{L}}|\sr_{T}(u)-1|\de u-\sum\limits_{s=1}^{L}\int\limits_{\frac{s-1}{L}}^{\frac{s}{L}}\left|\sr_{T}(u)-\sr_{T}(\xi_s)\right|\de u\\
	&\geq \int\limits_{0}^{1}|\sr_{T}(u)-1|\de u-  \sum\limits_{s=1}^{L} \int\limits_{\frac{s-1}{L}}^{\frac{s}{L}} {C|u-\xi_s|}{} \de u\\
	&\geq \int\limits_{0}^{1}|\sr_{T}(u)-1|\de u-\sum\limits_{s=1}^{L}\frac{C}{L^2}=\int\limits_{0}^{1}|\sr_{T}(u)-1|\de u-\frac{C}{L}\,.
\end{align*}	
Using the above lower bound into \eqref{eq:lambda_n_lower_1} gives  

\[
\sum\limits_{s=1}^{L}\left|p_s-\frac{1}{L} \right|\geq 
\int\limits_{0}^{1}|\sr_{T}(u)-1|\de u-\ell\nu_K-\frac{C}{L}\,.
\]

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Proof of Theorem \ref{thm: power_balls_bins}}\label{proof:thm: power_balls_bins}

The primary arguments here are similar to the initial reasonings in the proof of Theorem \ref{thm: chi^2-CI-size}, where we arrived at the point that the joint distribution of $(nW_1,nW_2,...,nW_{L})$ is a multinomial distribution with $L$ categories, such that category $s\in[L]$ happens with probability $p_s$. Next, recall Lemma \ref{lemma: multinomial-main}, part 3, where it implies that if for some $\beta>0$, the following holds:
%
\begin{equation}\label{eq: tmp12}
\sum\limits_{s=1}^{L}\left|p_s-\frac{1}{L}\right| \geq\frac{32{L^{1/4}}}{\sqrt{n}}\left[\frac{1}{\sqrt{\alpha}}\vee\frac{1}{\beta}  \right]^{1/2}\,,
\end{equation}
then the type II error is bounded by $\beta$. On the other hand, from Proposition \ref{propo: lambda_bound} we have
\begin{align}\label{eq:dum1}
\sum\limits_{s=1}^{L} \left| p_s-\frac{1}{L}\right| \geq 
\int\limits_{0}^{1}|\sr_T(u)\de u-1|-L \nu_L-\frac{C}{L}\,.
\end{align}
Combining equations~\eqref{eq: tmp12} and \eqref{eq:dum1},  in conjunction with the definition of the conditional dependency in Definition \ref{def: conditional-dependency-power} completes the proof.




 %For the reader's convenience,  we present this theorem here: 

 %Theorem 2 of \cite{balakrishnan2019hypothesis} states that

%This theorem for uniformity testing in multinomial distributions implies that
%\[
%P_0\left( n(m+1)\sum\limits_{\ell=1}^{m+1}(Y_\ell-\frac{1}{m+1})^2 \geq 
%(m+1)+\sqrt{\frac{2}{\alpha}(m+1)}   \right) \leq \alpha\,.
%\]
%Further, multinomial distributions with probability mass values   $(p_\ell)_{\ell=1:m+1}$ that for some $\beta>0$ satisfies

%\[
%\frac{1}{4}\left(\sum\limits_{\ell=1}^{m+1}\left|p_\ell-\frac{1}{m+1}\right| \right)^2\geq\frac{1024\sqrt{d}}{n}\left[\frac{1}{\sqrt{\alpha}}\vee\frac{1}{\beta}  \right]\,,
%\]
%have the Type II error smaller than $\beta$.


 
%From classical results on Pearson's statistic for Hypothesis $H'_0$ (  \cite{lehmann2006testing}, Theorem 14.3.1)  we know that $T\overset{d}{\rightarrow}\chi^2_{m}(\lambda_n)$ with the same $\lambda_n$ defined in Theorem \ref{thm: power_balls_bins}.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Proof of Theorem \ref{thm: power_balls_bins_asympt}} \label{proof:thm: power_balls_bins_asympt} 
%In this section, we analyze the power of the PCR test deployed with the asymptotic threshold $\th^{\mathsf{asym}}_{L,\alpha}=\chi^2_{L-1}(1-\alpha)$.
 The first step is similar to initial arguments of the proof of Theorem \ref{thm: chi^2-CI-size}, where we showed that the joint distribution of $(nW_1,nW_2,...,nW_{L})$ is a multinomial distribution with $L$ categories, such that the category $\ell\in [L]$ occurs with probability $p_\ell$. We then use the following asymptotic result on the Pearson's $\chi^2$ test statistic for multinomial models (see e.g., \cite[Theorem 14.3.1]{lehmann2006testing}):
	\begin{equation}\label{eq: asympt-power-multi-pearson}
	U_{n,L} \overset{(d)}{\Rightarrow} \chi^2_{\lambda,L-1}\,,
	\end{equation}   
where $\chi^2_{\lambda_n,L-1}$ stands for the $\chi^2$ distribution with $L-1$ degrees of freedom and the non-central parameter $\lambda=\sum\limits_{\ell=1}^{L}nL\left(p_\ell-\frac{1}{L}\right)^2$.  This gives us

\begin{align}\label{eq: tmp-power}
\lim\limits_{n\rightarrow \infty}^{} \prob\left(U_{n,\ell}\geq \th^{\mathsf{asym}}_{L,\alpha} \right)&= \lim\limits_{n\rightarrow \infty}^{} \prob_{Q\sim  \chi^2_{\lambda,L-1}} \left(Q\geq \th^{\mathsf{asym}}_{L,\alpha} \right)\,.
\end{align}
Using the Cauchy inequality for $\lambda=\sum\limits_{\ell=1}^{L}nL\left(p_\ell-\frac{1}{L}\right)^2$ yields
$$\sqrt{\lambda}\geq \sqrt{n}\sum\limits_{\ell=1}^{L}\left|p_\ell-\frac{1}{L}\right|\,. $$
We then recall the result from Proposition \ref{propo: lambda_bound} to get
\begin{align}\label{eq:dum1_5}
\sum\limits_{s=1}^{L} \left| p_s-\frac{1}{L}\right| \geq 
\int\limits_{0}^{1}|\sr_T(u)\de u-1|-L \nu_L-\frac{C}{L}\,.
\end{align}

In the next step, combine \eqref{eq:dum1_5} and the lower bound on the conditional dependency power in Theorem \eqref{thm: power_balls_bins} to get $\lambda \geq \widetilde{\lambda}$ with $\widetilde{\lambda}=A^2 L^{1/2}$, where $A$ is given by:

\begin{equation}\label{eq: tmp-A}
A=\max\left(\sqrt{3\log\frac{1}{\beta}}+\left( 3\log\frac{1}{\beta} +  2\sqrt{\log\frac{1}{\alpha}}+2\log\frac{1}{\alpha} \right)^{1/2} ,~1  \right)\,.
\end{equation}
Using $\lambda\geq \widetilde{\lambda}$ in \eqref{eq: tmp-power} yields

\begin{equation}\label{eq: tmp-power-2}
\lim\limits_{n\rightarrow \infty}^{} \prob\left(U_{n,\ell}\geq \th^{\mathsf{asym}}_{L,\alpha} \right)\geq \prob_{Q\sim  \chi^2_{\widetilde{\lambda},L-1}} \left(Q\geq \th^{\mathsf{asym}}_{L,\alpha} \right)\,.
\end{equation}

We then provide the following inequality borrowed from \cite{birge2001alternative} on tails of non-central $\chi^2$ random variables.

\begin{lemma}[\cite{birge2001alternative}, Lemma 8.1]\label{lemma: chi-tails}
Suppose that  random variable $X$ has a $\chi^2$ distribution with $m$ degrees of freedom and non-central parameter $\lambda$. Then for every $t\geq 0$  we have 
\begin{align*}
&\prob\left(X\leq m+\lambda -2\sqrt{(m+2\lambda)t}\right) \leq \exp(-t)\,,\\
&\prob\left(X\geq m+\lambda +2\sqrt{(m+2\lambda)t}+2t\right) \leq \exp(-t)\,.
\end{align*}
 \end{lemma}
As an immediate consequence of Lemma \ref{lemma: chi-tails}, we can obtain the following upper bound on the  $(1-\alpha)$-th quantile of the central $\chi^2$ distribution with $m$ degrees of freedom:
 \begin{equation}\label{eq: tmp-chi-quantile-bound}
\chi^2_{m}(1-\alpha) \leq m+2\sqrt{m\log\frac{1}{\alpha}} + 2\log\frac{1}{\alpha}\,.
 \end{equation}
By exploiting \eqref{eq: tmp-chi-quantile-bound} in $\th^{\mathsf{asym}}_{L,\alpha}=\chi^2_{L-1}(1-\alpha)$ we arrive at
\begin{equation}\label{eq: tmp-upper-th-asymp}
\th^{\mathsf{asym}}_{L,\alpha}\leq L-1+2\sqrt{(L-1)\log\frac{1}{\alpha}} + 2\log\frac{1}{\alpha}\,.
\end{equation}
Using  \eqref{eq: tmp-upper-th-asymp} in \eqref{eq: tmp-power-2} brings us
\begin{equation} \label{eq: tmp-power-3}
\lim\limits_{n\rightarrow \infty}^{} \prob\left(U_{n,\ell}\geq \th^{\mathsf{asym}}_{L,\alpha} \right)\geq \prob_{Q\sim  \chi^2_{\widetilde{\lambda},L-1}} \left(Q\geq L-1+2\sqrt{(L-1)\log\frac{1}{\alpha}} + 2\log\frac{1}{\alpha}  \right)\,.
\end{equation}
We next claim that 

\begin{equation}\label{eq: tmp-claim-power}
 2\sqrt{(L-1)\log\frac{1}{\alpha}} + 2\log\frac{1}{\alpha}  \leq A^2L^{1/2}-2\sqrt{(L-1+2A^2L^{1/2})\log\frac{1}{\beta}}\,.
\end{equation}
Deploying \eqref{eq: tmp-claim-power} (we provide the proof of claim \eqref{eq: tmp-claim-power} later) in \eqref{eq: tmp-power-3} yields 

\begin{equation} \label{eq: tmp-power-4}
\lim\limits_{n\rightarrow \infty}^{} \prob\left(U_{n,\ell}\geq \th^{\mathsf{asym}}_{L,\alpha} \right)\geq \prob_{Q\sim  \chi^2_{\widetilde{\lambda},L-1}} \left(Q\geq L-1+ A^2L^{1/2}-2\sqrt{(L-1+2A^2L^{1/2})\log\frac{1}{\beta}} \right)\,.
\end{equation}
Next by using the first tail bound of Lemma \ref{lemma: chi-tails} (for values $m=L-1, \widetilde{\lambda}=A^2L^{1/2}$, and $t=\log\frac{1}{\beta}$) in \eqref{eq: tmp-power-4} we obtain
\[
\lim\limits_{n\rightarrow \infty}^{} \prob\left(U_{n,\ell}\geq \th^{\mathsf{asym}}_{L,\alpha} \right)\geq 1-\beta\,.
\]
This completes the proof. Finally, we are left to prove the claim \eqref{eq: tmp-claim-power}. As $L\geq 1$, we have
\begin{align*}
&\widetilde{\th}:=A^2L^{1/2}-2\sqrt{(L-1+2A^2L^{1/2})\log\frac{1}{\beta}}\geq \sqrt{L}\left(A^2-  2\sqrt{(1+2A^2)\log\frac{1}{\beta}}  \right)\,.
\end{align*}
In the next step, by using $A\geq 1$, we get
\begin{align}
\widetilde{\th}&\geq \sqrt{L}\left(A^2-  2A\sqrt{3\log\frac{1}{\beta}}  \right)\nonumber\\
&\geq \sqrt{L}\left(A - \sqrt{3\log\frac{1}{\beta}}  \right)^2-3\sqrt{L}\log\frac{1}{\beta}\nonumber\\
&\geq\sqrt{L}\left( 2\sqrt{\log\frac{1}{\alpha}}+2\log\frac{1}{\alpha}\right)\label{eq: tmp-power-claim-2} \,,
\end{align}
where the last inequality is followed by the definition of $A$ in \eqref{eq: tmp-A}. We then use $L\geq 1$ in \eqref{eq: tmp-power-claim-2} to arrive at the following:
\[
\widetilde{\th}\geq 2\sqrt{(L-1)\log\frac{1}{\alpha}}+2\log\frac{1}{\alpha}\label{eq: tmp-power-claim} \,.
\]
This proves the claim \eqref{eq: tmp-claim-power}.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Proof of Theorem \ref{thm:robust-model X} }\label{proof:thm:robust-model X}

	Consider a data point $(X,Z,Y)$ and its $M=KL-1$  counterfeits $(\tX^{(1:M)},Z,Y)$ where $\tX^{(j)}$ is sampled from $\hP_{X|Z}(\cdot|Z)$, for $j\in [M]$.  Assume $\hX$ is also drawn from $\hP_{X|Z}(\cdot|Z)$,  independently of $\tX^{(1:M)},X,$ and $Y$.
	We fix the values of $Z,Y$, and for $s\in [L]$ define
	
	
	\[
	A_s=\left\{(x,\tx^{(1)},...,\tx^{(M)}): (s-1)K\leq \sum\limits_{j=1}^{M}\ind_{\{T(x,Z,Y)\geq T(\tx^{(j)},Z,Y)\}} \leq sK-1  \right\}\,.
	\]
	
	We have
	\begin{align}
		&\left|\prob\left( (X,Z,Y) \text{ has label } s|Z,Y  \right)- \frac{1}{L} \right|\nonumber\\
		&\overset{(a)}{=}\left|\prob\left( (X,\tX^{(1)},...,\tX^{(M)}) \in A_s|Z,Y  \right)-\frac{1}{L} \right|\nonumber\\
		%&\overset{(a)}{=}\left|\prob\left( (X,Z,Y) \text{ is type } s|Z,Y  \right)-  \prob\left( (\tX,Z,Y) \text{ is type } s|Z,Y  \right) \right|\\
		&\overset{(b)}{=}\left|\prob\left( (X,\tX^{(1)},...,\tX^{(M)}) \in A_s|Z,Y  \right)-\prob\left( (\hX,\tX^{(1)},...,\tX^{(M)}) \in A_s|Z,Y  \right) \right|\nonumber\\
		&\overset{(c)}{\leq} d_{\tv}\left(((X,\tX^{(1)},...,\tX^{(M)})|Z,Y),  ((\hX,\tX^{(1)},...,\tX^{(M)})|Z,Y)  \right)\nonumber\\
		&\overset{(d)}{=}d_{\tv}\left((X|Z,Y),(\hX|Z,Y)   \right)\nonumber\\
		&\overset{(e)}{=}d_{\tv}\left((X|Z),(\hX|Z)   \right)=d_{\tv}\left( P_{X|Z}(\cdot|Z),\hP_{X|Z}(\cdot|Z)  \right)\label{eq:robust-prob-upper}\,,
	\end{align}
	where $(a)$ comes from the process of labeling the data points; in (b) we used the fact that conditioned on $Z,Y$ random variables $\tX,\tX^{(1)},...,\tX^{(M)}$ are i.i.d.,  hence $\sum\limits_{j=1}^{M}\ind_{\{T(x,Z,Y)\geq T(\tx^{(j)},Z,Y)\}}$ takes values  $\{0,1,...,M\}$, uniformly at random; (c) is a direct result from the total variation definition; in $(d)$ we used the property that conditioned on $(Z,Y)$, random variables $(X,\tX,\tX^{(1)},...,\tX^{(M)})$ are independent; $(e)$ comes from the fact that the under null hypothesis, $X\indep Y|Z$ and also $\hX\indep Y|Z$ by construction of $\hX$. 
	
	In the current scenario that counterfeits are drawn from the approximate law $\hP_{X|Z}(.|Z)$, define $q_s$ to be the probability that under the null hypothesis, a typical data point $(X,Z,Y)$ gets label $s$. Then, by marginalizing over $Z$, we can upper bound the  deviation of $q_s$ from $1/L$, as follows: 
	%$$q_s=\prob\left( (X,Z,Y) \text{ is type } s \right),\quad \text{ for } s=1,2,...,\ell\,. $$
	%By marginalizing over $Z$, \eqref{eq:robust-prob-upper} gives us 
	\begin{align*}
		\left|q_s-\frac{1}{L}\right|	&=\left|\prob\left( (X,Z,Y) \text{ has label } s  \right)- \frac{1}{L} \right|\nonumber\\
	&= \left|\int \prob\left( (X,Z,Y) \text{ has label } s|Z,Y  \right)\de P_{ZY}- \frac{1}{L} \right|\nonumber\\
		&= \left|\int \left(\prob\left( (X,Z,Y) \text{ has label } s|Z,Y  \right)-\frac{1}{L}\right)\de P_{ZY}\right|\nonumber\\
		&\leq  \int \left|\prob\left( (X,Z,Y) \text{ has label } s|Z,Y  \right)-\frac{1}{L}\right|\de P_{ZY}\nonumber\\
		&\overset{(a)}{\leq} \int d_{\tv}\left( P_{X|Z}(\cdot|Z),\hP_{X|Z}(\cdot|Z)  \right) \de P_{ZY}\\
		&=  \E_{Z}\left[ d_{\tv}\left( P_{X|Z}(\cdot|Z),\hP_{X|Z}(\cdot|Z)  \right) \right]\leq \delta\,,
	\end{align*}
	where (a) comes from \eqref{eq:robust-prob-upper}.  In summary we get 
	\begin{equation}\label{eq:robust-prob-upper2}
		\left|q_s-\frac{1}{\ell}\right|\leq \delta,\quad \text{ for } s=1,2,...,\ell\,.
	\end{equation}
	Recall $W_s$ as the number of data points getting label $s$. Clearly,
	\[
	(W_1,...,W_L)=\mathsf{multi}\left(n;q_1,...,q_\ell\right)\,.
	\]
	
	We next use a result on the size of truncated $\chi^2$ test from \cite[Theorem 3.2]{balakrishnan2019hypothesis}, which implies the first inequality in the chain of inequalities below:
	 %as a result, from the multinomial hypothesis testing (\cite{balakrishnan2019hypothesis}, Theorem 2) we get that
	\begin{align*}
		\alpha&\geq \prob\left(\sum\limits_{s=1}^{L} \frac{(W_s-nq_s)^2-W_s}{\max\{q_s,\frac{1}{L} \}} \geq n\sqrt{ \frac{2}{\alpha} \sum\limits_{s=1}^{L} \left( \frac{q_s}{\max\{q_s,1/L \}} \right)^2 }    \right)\\
		&\geq \prob\left(\sum\limits_{s=1}^{L} \frac{(W_s-nq_s)^2-W_s}{\max\{q_s,\frac{1}{L} \}} \geq n\sqrt{ \frac{2}{\alpha}L}     \right)\\
		&= \prob \left(\sum\limits_{s=1}^{L} \frac{(W_s-nq_s)^2}{\max\{q_s,\frac{1}{L} \}} \geq \sum\limits_{s=1}^{L}\frac{W_s}{\max\{q_s,\frac{1}{L} \}} + n\sqrt{ \frac{2}{\alpha}L}     \right)\\\
		&\geq \prob \left( \sum\limits_{s=1}^{L} \frac{(W_s-nq_s)^2}{\max\{q_s,\frac{1}{L} \}} \geq L\sum\limits_{s=1}^{L}{W_s} +n\sqrt{ \frac{2}{\alpha}L}     \right)\\
		&=\prob \left(\sum\limits_{s=1}^{L} \frac{(W_s-nq_s)^2}{\max\{q_s,\frac{1}{L} \}} \geq n L+n\sqrt{ \frac{2}{\alpha}L}     \right)\\
		&\overset{(a)}{\geq} \prob \left( \sum\limits_{s=1}^{L} \frac{(W_s-nq_s)^2}{\frac{1}{L}+\delta } \geq n L+n\sqrt{ \frac{2}{\alpha}L}     \right)\\
		&\geq  \prob\left( \frac{L}{n(1+L\delta)}\sum\limits_{s=1}^{L}{\left(W_s-nq_s\right)^2}  \geq  L+\sqrt{ \frac{2}{\alpha}L}   \right)\\
		&\geq  \prob\left( U_{n,L}(\delta)  \geq  L+\sqrt{ \frac{2}{\alpha}L}   \right)\,,
	\end{align*}
	where $(a)$ comes from \eqref{eq:robust-prob-upper2} and the last inequality follows from the definition of $U_{n,L}$. This concludes the proof of claim \eqref{eq:robust-fin}.
	
	 For the claim~\eqref{eq:robust-asym}, we use the following  asymptotic result on the Pearson's $\chi^2$ test statistic for multinomial models (see e.g, \cite[Theorem 14.3.1]{lehmann2006testing}):
	\begin{equation}\label{eq: asympt-size-multi-pearson}
	\lim\limits_{n\rightarrow \infty}^{}\prob\left ( \sum\limits_{s=1}^{L}\frac{(W_s-nq_s)^2}{nq_s} \geq \th^{\mathsf{asym}}_{L,\alpha}  \right) \leq \alpha\,, 
	\end{equation}
where $\th^{\mathsf{asym}}_{L,\alpha} $ is the $\alpha$-th upper quantile of a Chi-squared distribution with $L-1$ degrees of freedom. By definition of $U_{n,L}(\delta)$, we have
\begin{align*}
\prob\left(U_{n,\ell}(\delta) \geq \th^{\mathsf{asym}}_{L,\alpha}    \right)&\leq 
\prob\left(\frac{L}{n(1+L\delta)}\sum\limits_{s=1}^{L} (W_s-nq_s)^2 \geq \th^{\mathsf{asym}}_{L,\alpha}    \right)\\
&\leq \prob\left(\sum\limits_{s=1}^{L} \frac{(W_s-nq_s)^2}{nq_s} \geq \th^{\mathsf{asym}}_{L,\alpha}    \right)\,,
\end{align*}
where in the last inequality we used \eqref{eq:robust-prob-upper2}. Finally, plug the above relation into \eqref{eq: asympt-size-multi-pearson} to get the following relation:
\[
\lim\limits_{n\rightarrow \infty}^{}\prob\left( U_{n,L}(\delta) \geq  \th^{\mathsf{asym}}_{L,\alpha}   \right) \leq \alpha\,.
\]
This concludes the proof.




\newpage
\bibliographystyle{alpha}
\bibliography{mybib}

\end{document}
    