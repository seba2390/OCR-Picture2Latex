%%%%%%%%%%%%%%%%%%%%%%% file template.tex %%%%%%%%%%%%%%%%%%%%%%%%%
%
% This is a general template file for the LaTeX package SVJour3
% for Springer journals.          Springer Heidelberg 2010/09/16
%
% Copy it to a new file with a new name and use it as the basis
% for your article. Delete % signs as needed.
%
% This template includes a few options for different layouts and
% content for various journals. Please consult a previous issue of
% your journal as needed.
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
% First comes an example EPS file -- just ignore it and
% proceed on the \documentclass line
% your LaTeX will extract the file if required
%\begin{filecontents*}{example.eps}
%%!PS-Adobe-3.0 EPSF-3.0
%%%BoundingBox: 19 19 221 221
%%%CreationDate: Mon Sep 29 1997
%%%Creator: programmed by hand (JK)
%%%EndComments
%gsave
%newpath
%  20 20 moveto
%  20 220 lineto
%  220 220 lineto
%  220 20 lineto
%closepath
%2 setlinewidth
%gsave
%  .4 setgray fill
%grestore
%stroke
%grestore
%\end{filecontents*}
%
\RequirePackage{fix-cm}
%
%\documentclass{svjour3}                     % onecolumn (standard format)
%\documentclass[smallcondensed]{svjour3}     % onecolumn (ditto)
\documentclass{amsart}    % onecolumn (second format)
%\documentclass[twocolumn]{svjour3}          % twocolumn
%
% \smartqed  % flush right qed marks, e.g. at end of proof
%
\usepackage{bbm}
\usepackage{graphicx}
\usepackage{amsmath}
%\usepackage{amsthm}
\usepackage{amssymb}
\usepackage{wasysym}
\usepackage{mathrsfs}
\usepackage{geometry} 
\usepackage{mathtools}
%\usepackage[numbers]{natbib}
%\usepackage{hyperref}

\DeclareMathOperator{\R}{\mathbb{R}}
\DeclareMathOperator{\vol}{vol}
\DeclareMathOperator{\as}{as}
\DeclareMathOperator{\dist}{dist}
\DeclareMathOperator{\conv}{conv}
\DeclareMathOperator{\ldel}{ldel}
\DeclareMathOperator{\ldiv}{ldiv}
\DeclareMathOperator{\dell}{del}
\DeclareMathOperator{\divv}{div}
\DeclareMathOperator{\diam}{diam}
\DeclareMathOperator{\var}{Var}
\DeclareMathOperator{\intt}{int}

\let\originalleft\left
\let\originalright\right
\renewcommand{\left}{\mathopen{}\mathclose\bgroup\originalleft}
\renewcommand{\right}{\aftergroup\egroup\originalright}

\newtheorem{theorem}{Theorem}[section]
 \newtheorem{prop}[theorem]{Proposition}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{remark}[theorem]{Remark}
\newtheorem{conj}[theorem]{Conjecture}
\newtheorem{ques}[theorem]{Question}
%\theoremstyle{definition}
%\newtheorem{remark}{Remark}
\newcommand{\NN}{N^{-\frac 2{n-1}}}
\newcommand{\NNN}{N^{-\frac{1}{n-1}}}
\newcommand{\M}{M^{-\frac 2{n-1}}}
\newcommand{\Sn}{\mathbb{S}^{n-1}}
\newcommand\inneri[2]{\langle #1, #2 \rangle}
\newcommand{\Sp}{\mathbb{S}^{n-1}}
\newcommand{\qleq}{\accentset{*}{\leq}}
\newcommand{\1}{\mathbbm{1}}
%\DeclareMathOperator{\conv}{conv}
%\DeclareMathOperator{\aff}{aff}
\newcommand{\E}{{\mathbb{E}}}
\newcommand{\prob}{{\mathbb{P}}}
%\newcommand{\Pr}{\mathbb P}
%
% \usepackage{mathptmx}      % use Times fonts if available on your TeX system
%
% insert here the call for the packages your document requires
%\usepackage{latexsym}
% etc.
%
% please place your own definitions here and don't use \def but
% \newcommand{}{}
%
% Insert the name of "your journal" with
% \journalname{myjournal}
%
\begin{document}

\title[Random Polytopes, Dirichlet-Voronoi Tiling Numbers and Geometric Balls and Bins  ]{A Concentration Inequality for Random Polytopes, Dirichlet-Voronoi Tiling Numbers and the Geometric Balls and Bins Problem}
%\title{An almost sharp concentration inequality for random polytopes and  Dirichlet-Voronoi tiling numbers%\thanks{Grants or other notes
%about the article that should go on the front page should be
%placed here. General acknowledgments should be placed at the end of the article.}

%\subtitle{Do you have a subtitle?\\ If so, write it here}

%\titlerunning{Short form of title}        % if too long for running head

\author{Steven Hoehner}     
\address{Longwood University, Department of Mathematics and Computer Science}
\email{hoehnersd@longwood.edu}
\author{Gil Kur} %etc.
\address{Weizmann Institute of Science,
               Department of Computer Science and Mathematics}
               \email{gilkur1990@gmail.com}


%\authorrunning{Short form of author list} % if too long for running head

% \institute{Steven Hoehner \at
%               Longwood University \\
%               Department of Mathematics \& Computer Science\\
%               \email{hoehnersd@longwood.edu}           %  \\
% %             \emph{Present address:} of F. Author  %  if needed
%           \and
%           Gil Kur \at
%               Weizmann Institute of Science \\
%               Department of Computer Science and Mathematics\\
% 			\email{gilkur1990@gmail.com}	
% }
% \date{Received: date / Accepted: date}
% The correct dates will be entered by the editor
\subjclass[2000]{Primary 52A22, 52A27, 52C17, 52C22}
\maketitle

\begin{abstract}
	Our main contribution is a concentration inequality for the symmetric volume difference of a $ C^2 $ convex body with  positive Gaussian curvature and a circumscribed random polytope with a restricted number of facets, for any probability measure on the boundary with a  positive density function.
	 	 
We also show that the Dirichlet-Voronoi tiling numbers satisfy $ \text{div}_{n-1} = (2\pi e)^{-1}(n+\ln n) + O(1)$, which improves a classical result of Zador by a factor of $o(n)$. In addition, we provide a remarkable open problem which is the natural geometric generalization of the famous and fundamental ``balls and bins" problem from probability.   This problem  is tightly connected to the optimality of random polytopes in high dimensions.
%These problems concern  random partial sphere coverings, which were studied by Erd{\H{o}}s, Few and Rogers, .

Finally, as an application of the aforementioned results, we derive a lower bound for the maximal Mahler volume product of polytopes with a restricted number of vertices or facets.
\keywords{ Random polytopes \and approximation\and convex bodies \and sphere covering \and tiling}
% \PACS{PACS code1 \and PACS code2 \and more}
% \subclass{ 52A22 \and 52A27 \and 52C17 \and 52C22 }
\end{abstract}



%\begin{acknowledgements}
%If you'd like to thank anyone, place your comments here
%and remove the percent signs.
%\end{acknowledgements}

% BibTeX users please use one of
%\bibliographystyle{spbasic}      % basic style, author-year citations
%\bibliographystyle{spmpsci}      % mathematics and physical sciences
%\bibliographystyle{spphys}       % APS-like style for physics
%\bibliography{}   % name your BibTeX data base
\section{Introduction and main results}

The approximation of convex bodies by polytopes is of significant theoretical interest in convex geometry, and it has applications in a wide variety of areas, including tomography (e.g., \cite{gardner1995geometric}), computational geometry (e.g., \cite{edelsbrunner,edelsbrunnermesh}), geometric algorithms (e.g., \cite{GKM}) and statistical learning theory (e.g., \cite{diakonikolas2016learning}). The accuracy of the approximation  is often measured by the symmetric volume difference $d_S$ (also called the symmetric difference metric, or the Nikodym metric), which equals the volume of the symmetric difference of a given convex body $K$ in $\R^n$ and an approximating polytope $P$. It is defined by
$d_S(K,P) := |K\triangle P|,$ where $|\cdot|$ denotes $n$-dimensional volume. 

Typically, conditions are prescribed on the approximating polytopes, such as a restricted number of vertices or facets. Moreover, the approximating polytopes may be inscribed in $K$, circumscribed around $K$, or positioned arbitrarily. In this paper, we focus on the approximation of convex bodies by circumscribed and arbitrarily positioned polytopes with a restricted number of facets under the symmetric volume difference. 

Due to the difficulty of finding  best-approximating polytopes (even when $n = 3$), random polytopes can and have been used to derive sharp estimates for the approximation of convex bodies. In fact, it turns out that, asymptotically, random polytopes are almost as good as best-approximating polytopes under the symmetric difference metric \cite{boroczky2004approximation,GruberII,kur2017approximation,ludwig1999asymptotic,Lud06,muller1990approximation,schutt2003polytopes}. 
% Specifically, it has been shown that the expectation of the volume difference is optimal up to an absolute constant (compare the results in)
In our first theorem, we derive a sharp concentration inequality (up to logarithmic factors)  for the volume difference of a $ C^2 $ convex body $K$ with  positive Gaussian curvature and a random circumscribed polytope with $N$ facets. %(see Eq. (\ref{Eq:def}) below). 
The expected volume of these polytopes was estimated in \cite{boroczky2004approximation}. 
% The facets of the random polytope are chosen independently and  identically distributed (i.i.d.) according to a probability measure $\mu$ on $\partial K$ that has a  density which is  positive at each point of  $\partial K$.
%We show that this concentration also holds for the random polytopes in \cite{kur2017approximation}, which are arbitrarily positioned and give an optimal approximation to the Euclidean ball with respect to the symmetric volume difference.  As a corollary, we deduce an almost sharp estimate for the variance of the volume difference. 

%Throughout this section, we assume that $ K $ is a $ C^2 $ convex body with  positive Gaussian curvature, and $ \mu  $ is a probability measure on $ \partial K $ such that $ d\mu(x) =  g(x)\,d\sigma(x) $ and  $ g(x) >0 $ for all $ x\in\partial K$. % Let   

%\vskip .1in

%The following theorem is the main result of this paper.
\begin{theorem}{\label{geneThm}}
	Let $ P_{n,N} $ be a random circumscribed polytope in $ \R^{n} $ with at most $N$ facets that is defined by
	\begin{equation}\label{Eq:def}
	P_{n,N} := \bigcap_{i=1}^{N}\{x\in\R^n: \inneri{x}{\nu(X_i)} \leq \inneri{X_i}{\nu(X_i)} \}, \quad X_1,\ldots,X_N \stackrel{\text{i.i.d.}}{\sim} \mu,
	\end{equation} where   $ \nu:\partial K \to \Sp $ denotes the Gauss map of $K$. 
	Then when $ N$  is large enough, for any $ \epsilon > 0 $ the following holds:
	\begin{equation}{\label{maineq}}
	\Pr_A(\big||P_{n,N} \setminus K| - \E[|P_{n,N}\setminus K|]\big| \geq \epsilon) \leq 2\exp\left(-c(K,\mu)N^{1+\frac{4}{n-1}}f(N)\epsilon^2\right),
	\end{equation}
	where $A$ is an event that holds with probability at least $1- \exp\left(-c_1(K,\mu)N^{0.5-\frac{2}{n-1}}\ln N\right)$, $c(K,\mu)$ and $c_1(K,\mu)$ are  positive constants that depend on $K$ and $\mu$, and $ f(N) = (\ln N)^{-(2+\frac{4}{n-1})}$. 
\end{theorem}
This result implies that the conditional variance is bounded above by $C(K,\mu)N^{-(1+\frac{4}{n-1})}f(N)^{-1}$.
% Our next theorem shows that the concentration bound \eqref{maineq} is tight, up to  logarithmic factors in the exponent  (which we believe may be removed). %We show that by proving the following theorem.
% \begin{theorem}{\label{varThm}}
% 	Let $ P_{n,N} $ be the polytope defined in Theorem \ref{geneThm}.  Then when $N$ is large enough, it holds that 
% 	\begin{equation}{\label{vareq}}
% 	\mathrm{Var}_A(|P_{n,N} \setminus K|) \geq c(K,\mu)N^{-(1+\frac{4}{n-1})},
% 	\end{equation}
% 	where the variance is conditioned on the event $A$ defined in Theorem \ref{geneThm}. 
% \end{theorem}

The proof of Theorem \ref{geneThm} also gives a concentration inequality for the volume of the arbitrarily positioned random polytopes that were defined in \cite{kur2017approximation}; these polytopes give an optimal approximation to the Euclidean unit ball $B_n$ when $\mu=\sigma$ is the uniform measure on the sphere.
\begin{corollary}{\label{mainThm}}
	Let $ P_{n,N} $ be the random polytope in $ \R^{n} $ that is defined by
	\[
	P_{n,N} := \bigcap_{i=1}^{N}\{x\in\R^n: \inneri{X_i}{x} \leq t_{n,N} \}, 
	\quad X_1,\ldots,X_N \stackrel{\text{i.i.d.}}{\sim}  \sigma,%\mu,
	\] 
	where $t_{n,N} := {\sqrt{1-\varepsilon_{n,N}^2}}$ and $\varepsilon_{n,N} := \left(\frac{(\ln 2)|\partial B_n|}{N|B_{n-1}|}\right)^{\frac{1}{n-1}}$.	 
	Then when $N$ is large enough, the random variable $|P_{n,N} \triangle B_n|$ satisfies the concentration inequality
	\[
	\Pr_A(\big||P_{n,N}\triangle B_n|-\E[|P_{n,N}\triangle B_n|]\big| \geq \epsilon) 
	\leq 2\exp\left(-c(n)N^{1+\frac{4}{n-1}}f(N)\epsilon^2\right),
	\]
	where $A$ is an event that holds with probability at least $\exp\left(-c_1(n)N^{0.5-\frac{2}{n-1}}\ln N\right)$, $c(n)$ and $c_1(n)$ are positive constants that depend only on the dimension, and $ f(N)=(\ln N)^{-(2+\frac{4}{n-1})}$. %same concentration bound as in Theorem \ref{geneThm}.  
%	\[
%	\Pr(\big| - \E[|P_{n,N}\triangle B_n|]\big| \geq \epsilon) \leq  2e^{-c(n,\mu)N^{1+\frac{4}{n-1}}f(N)\epsilon^2}+e^{-c_1(n,\mu)(\ln N)N^{0.5-\frac{2}{n-1}}},
%	\]
%	where $ f(N) = (\ln N)^{-(2+\frac{4}{n-1})}$. %*** Log part *** 
%	*** Var ***
\end{corollary}

Using %(\ref{boereitzner1}) and 
 an argument of Reitzner \cite{reitzner2003random}, we derive the following corollary to Theorem \ref{geneThm}.
\begin{corollary}\label{corollary10}
	%	 Let $K$ be a $C^2$ convex body in $\R^n$, and let $X_i$, $1\leq i<\infty$, be chosen i.i.d. from $\partial K$ with respect to a positive, continuous density function $g$.
	Consider the sequence of circumscribed random polytopes $(P_{n,N})_{N}$ defined  in Theorem \ref{geneThm}. Then with probability 1,
	\[
	\lim_{N\to\infty} N^{\frac{2}{n-1}}|P_{n,N}\setminus  K| = \frac{1}{2}|B_{n-1}|^{-\frac{2}{n-1}} \Gamma\left(\frac{2}{n-1}+1\right)\int_{\partial K}g(x)^{-\frac{2}{n-1}} \kappa(x)^{\frac{1}{n-1}} \,dx,
	\] 
	where $g(x)\,dx = d\mu(x)$ and $g(x)>0$.
%	and  the integral is minimized by the density $g_{\min}$ defined in (\ref{optimaldensity1}).
\end{corollary}


Our next result is an application of \cite{kur2017approximation} and provides an improvement to a result of Zador \cite{zador1982asymptotic} on the asymptotic behavior of the Dirichlet-Voronoi tiling number in $\R^n$, denoted by $\divv_{n-1}$.
These numbers have numerous definitions. From a geometric point of view, Gruber \cite{GruberII} proved that for any convex body $ K $ in $\R^n$ with  $C^2$ boundary and  positive Gaussian curvature $\kappa$, the following asymptotic formulas hold:
\begin{align}\label{GruberOutEqn}
\lim_{N\to\infty}N^{\frac{2}{n-1}}\min\{|P\setminus K|: P\supset K, P\text{ has at most }N \text{ facets}\} &= \frac{1}{2}\divv_{n-1}(\as(K))^{\frac{n+1}{n-1}}\\
%\end{align}
%Also he proved that $\dell_{n-1}$ the Delone triangulation number in dimension $n$. 
%\begin{align}
\label{GruberInEqn}
\lim_{N\to\infty}N^{\frac{2}{n-1}}\min\{|K\setminus P|: P\subset K, P\text{ has at most }N \text{ vertices}\} &= \frac{1}{2}\dell_{n-1}(\as(K))^{\frac{n+1}{n-1}}.
\end{align}
Here $\as(K)=\int_{\partial K}\kappa(x)^{\frac{1}{n+1}}\,dS(x)$ is the affine surface area of $K$ (see, e.g., \cite{SchneiderBook}). 
% $\mu_{\partial K}$ is the surface measure on $\partial K$ and $|K|$ is the $n$-dimensional volume of a compact set $K$.
The term $\dell_{n-1}$ is a positive constant that depends only on the dimension and is known as the Delone triangulation number in $\R^n$. A strong connection between  optimal Delone triangulations, sphere covering, and asymptotic best approximation of convex bodies by inscribed polytopes with a restricted number of vertices was exhibited by Chen \cite{chen}.
%The constants $\dell_{n-1}$ and $\divv_{n-1}$ are connected with Delone triangulations and Dirichlet-Voronoi tilings in $\R^n$, respectively.
% More %, and they arise in applications in, e.g., geometric algorithms (e.g., \cite{edelsbrunner}), medical imaging (e.g., \cite{cyto}), terrain reconstruction (e.g., \cite{kok}), and computer vision (e.g., \cite{edelsbrunnermesh}). 

The exact values of $\dell_{n-1}$ and $\divv_{n-1}$ are unknown for $ n \geq 4 $. For $ n=2$, it is known\footnote{Please note that for $n=2$, Eqs. (\ref{GruberOutEqn}) and (\ref{GruberInEqn})  were stated by T\'oth \cite{toth} and proved by McClure and Vitale \cite{McVi}; for $n=3$, they were also stated in \cite{toth}, and later proved by Gruber in \cite{GruberIn,GruberOut}.} that $\dell_1=\frac{1}{6}$ and $\divv_{1}=\frac{1}{12}$ (see, e.g., \cite{ludwig1999asymptotic}), and for $n=3$ the values  $\dell_2=\frac{1}{2\sqrt{3}}$ and $\divv_2=\frac{5}{18\sqrt{3}}$ were determined by Gruber in \cite{GruberIn} and \cite{GruberOut}, respectively. Surprisingly, for large $n$, the constants $\dell_{n-1}$ and $\divv_{n-1}$ are nearly equal. First,  in the groundbreaking paper \cite{zador1982asymptotic}, Zador proved that $\divv_{n-1} =(2\pi e)^{-1}n +o(n)$.  On the other hand, $\dell_{n-1}$ was estimated in several papers. The best known estimate was provided by Mankiewicz and Sch\"utt \cite{MaS1}, who proved that %\begin{equation}\label{delll}
$\dell_{n-1} = (2\pi e)^{-1}n + O(\ln n)$. In the next theorem, we provide an almost sharp estimate for $\divv_{n-1}$. %We provide an almost sharp estimate of $\divv_{n-1}$, namely 
%\end{equation} 
%For more details, please see Subsection \ref{delone}.

\begin{theorem}\label{deldiv}
	Let  $\divv_{n-1} $  be defined as in Eq. \eqref{GruberOutEqn}. Then %the following holds:
	\begin{equation}\label{deldiv2}
	\divv_{n-1} = (2\pi e)^{-1}(n+\ln n)+O(1).
	\end{equation}
\end{theorem}

\noindent Theorem \ref{deldiv} and the estimate for $\dell_{n-1}$ imply that, asymptotically, $ \dell_{n-1} $ and $ \divv_{n-1} $ differ by at most $O(\ln n)$.

\vskip 2mm
 
%\begin{equation}\label{zador1}
% 
%\end{equation}
%For more background on $\divv_{n-1}$ and $\dell_{n-1}$, please see Subsection \ref{} below. *******

%For more details, please see Subsection \ref{mahlersection} below.
 
% absolutely continuous with respect to the uniform measure and has a density that is  positive at each point in the boundary. 
%This concentration inequality is a  ``dual'' analogue of the main result in \cite{vu2005sharp}, 
%\section{Main results}
%\subsection{Random approximation of convex bodies}
%	First we prove the result when the convex body is the $ n$-dimensional Euclidean ball and for e  distribution on the sphere. Then we generalize it to any $ C^2 $ convex body with  positive curvature.
% 
%	 When the uniform distribution is chosen, the expectation of these random polytopes, gives optimal approximating polytopes for the Euclidean ball in arbitrary position. We show an almost sharp concentration inequality for the symmetric volume  difference of the ball and such a random polytope.



%Our next result is regards ********

 The following result is a remarkable application of Theorem \ref{deldiv} and the main result in  \cite{boroczky2004approximation} (the asymptotic notation $O(\cdot)$  is with respect to the dimension $n$).
\begin{corollary}\label{randomvsbest}
	Let $ P_{K,N}^b $ be a best-approximating polytope with at most $N$ facets that circumscribes $K$, and let $ P_{n,N} $ be a random polytope that is generated as in Theorem \ref{geneThm} with the density that minimizes the expectation of the volume difference, which is  $ (\as(K))^{-1}\kappa^{\frac{1}{n+1}}\mathbbm{1}_{\partial K}$. Then
		\begin{equation}\label{noCovexlim}
	\lim_{N\to\infty}\frac{\E[|P_{n,N}\setminus K|]}{|P_{K,N}^b \setminus K|} 
	%= \frac{\divv_{n-1}}{|B_{n-1}|^{-\frac{2}{n-1}} \Gamma\left(\frac{2}{n-1}+1\right)}
	= 1+O(\tfrac 1n).
	\end{equation}
\end{corollary}
%\noindent{}}. 
%	This result implies that random approximation is ``as good as" best-approximation as the dimension tends to infinity.

Our last result is a lower bound for the  functional
\[
F(n,N) := \max_{P_{n,N} \subset \R^{n} \text {has at most $N$ vertices}} |P_{n,N}|\cdot|P^{s(P_{n,N})}_{n,N}|,
\] where $s(P_{n,N})  $ denotes the Santal\'o point of the polytope $ P_{n,N} $ and  $ P^{s(P_{n,N})}_{n,N} $ is the polar body of $P_{n,N}$ with respect to $s(P_{n,N})$ (when the Santal\'o point is the origin, we use the notation of $K^\circ$ to denote the polar body of $K$). Recently, Alexander, Fradelizi and Zvavitch \cite[Theorem 3.4]{alexander2017polytopes} proved that the maximum of $F(n,N)$  is achieved by a simplicial polytope with precisely $N$ vertices. We use all of the results in this paper to prove the following theorem. % show that when $N$ is large enough,
%\[
%F(n,N) \geq (1-c\ln n \cdot \NN)|B_n|^2 ,
%\]
%where $c>0$ is an absolute constant. 
\begin{theorem}\label{mahler}
	Let $ P_{n,N} $ be the centrally symmetric random polytope
	\[
	P_{n,N} := \conv\{\pm X_i\}_{i=1}^{\frac{N}{2}} \subset B_n, \quad X_1,\ldots,X_{\frac{N}{2}} \stackrel{\text{i.i.d.}}{\sim} \sigma(\Sn).
	\]
	%where $ X_i \stackrel{\text{i.i.d.}}{\sim} \sigma(\Sn).$ 
	Then there exists an absolute constant $C>0$ such that when $ N \geq C^nn^{0.5n}$, with overwhelming probability it holds that % the following holds:
	\[
	F(n,N)  \geq |P_{n,N}|\cdot|P_{n,N}^{\circ}| \geq \E[|P_{n,N}|\cdot|P_{n,N}^{\circ}|] + c\NN|B_n|^2 \geq (1 - (1.5\ln n + O(1))\NN)|B_n|^2.
	\]
% $ (1 - (1.5\ln n + O(1))\NN)|B_n|^2 $. *******fix the constant*****
%	\[
%	
%	\]
\end{theorem}

We conclude this section with a few remarks.
%\begin{remark}
%	For large $\epsilon$, using the arguments of the proof one can get a better bound for deviations from the expectation. Specifically, for $ \epsilon > c\NN|B_n| \sim c_1\E[|P \triangle B_n|] $,% the following holds:
%	\[
%	\Pr(\big||P_{n,N} \triangle B_n| - \E[|P_{n,N}\triangle B_n|]\big| \geq \epsilon) \leq 2 e^{-c(n)N^{1+\frac{4}{n-1}}f(N)\epsilon^2}+e^{-c_1(n)N^{1-\frac{2}{n-1}}},
%	\]where $ f(N) = (\ln N)^{-(2+\frac{4}{n-1})}.$
%\end{remark}

%As a corollary, we prove the following 
\begin{remark}
	%Corollary \ref{mainThm} implies a ``large deviation inequality'' for the surface area deviation. 
	For convex bodies $K$ and $L$ in $\R^n$, the surface area deviation $\triangle_s(K,L)$ is defined by (see, e.g., \cite{Wer15,kur2017approximation})
	\[
	\triangle_s(K,L) := |\partial (K\cup L)| - |\partial (K\cap L)|.
	\]
	Kur \cite{kur2017approximation} showed that the polytopes $P_{n,N}$ of Corollary \ref{mainThm} satisfy $\triangle_s(P_{n,N},B_n) \leq 4n|P_{n,N}\triangle B_n|$. %$ \frac{\triangle_s(P_{n,N},B_n)}{|\partial B_n|} \leq  4\frac{|P_{n,N}\triangle B_n|}{|B_n|}. $ 
This inequality and Corollary \ref{mainThm} together imply that for all sufficiently large $N$, the following ``large deviation" inequality holds:
	\[
	\Pr\left( \triangle_s(B_n,P_{n,N}) > C \cdot \E\left[\triangle_s(B_n,P_{n,N})\right] \right) \leq 2 \exp\left(-c(n)N^{1-\frac{2}{n-1}}\right).
	\]
\end{remark}
\begin{remark}
	In the case of inscribed random polytopes whose vertices are chosen randomly from the boundary of the body $K$, one can use the parallel results of \cite{MaS2,schutt2003polytopes} (with density $\kappa^{\frac{1}{n+1}}\mathbbm{1}_{\partial K}$) to deduce a limit formula like \eqref{noCovexlim} with a convergence rate that is bounded by $ 1+O(n^{-1}\ln n)$ as $N\to\infty$.
\end{remark}
%B\"or\"oczky and Reitzner \cite{boroczky2004approximation} proved that ********8
%\begin{equation}\label{boereitzner1}
%%\lim_{N\to\infty} N^{\frac{2}{n-1}}\E[|P_{n,N}\setminus K|] = \frac{1}{2}|B_{n-1}|^{-\frac{2}{n-1}} \Gamma\left(\frac{2}{n-1}+1\right)\int_{\partial K}g(x)^{-\frac{2}{n-1}} \kappa(x)^{\frac{1}{n-1}} \,dx .
%\end{equation}
\section{Prior work}

\subsection{Random polytopes} 

There is a rich literature on the statistical properties of inscribed random polytopes with a restricted number of vertices. It would be impossible to list all of the results in this direction, so we will highlight those  which are most relevant to this paper. First, M\"uller \cite{muller1990approximation} showed that when the vertices are chosen uniformly and independently from the boundary of the Euclidean ball, the expectation of the volume difference is optimal up to an absolute constant. Sch\"utt and Werner \cite{schutt2003polytopes} generalized this result to any $ C^2 $ convex body with %everywhere 
positive Gaussian curvature, and for any continuous, positive density on the boundary of the body. They also derived an explicit formula for the optimal density function that minimizes the expected volume difference over all choices of positive densities, and showed that if the optimal density is chosen, then as the dimension tends to infinity, random approximation is asymptotically as good as best approximation.% **** dimension tends to infinity ****

From results on the expectation, an immediate question that follows is to investigate higher moments of the volume difference. K\"ufer \cite{kufer1994approximation} proved an inequality for the variance of the volume of a random polytope that is the convex hull of points chosen uniformly and independently from the boundary of the Euclidean ball. Reitzner \cite{reitzner2003random} later extended this result to all $C^2$ convex bodies $K$ with  positive generalized Gaussian curvature by showing that the variance of the volume of a random polytope whose vertices are chosen uniformly and independently from $\partial K$ is at most  $C(K)N^{-(1+\frac{4}{n-1})}$, where $C(K)$ is a positive constant that depends on $K$.  In \cite{reitzner2003random} it was also shown that the variance of the volume of a random polytope whose vertices are chosen uniformly and independently from $ K$  is at most $C(K)N^{-(1+\frac{2}{n+1})}$. Later,  Reitzner \cite{reitzner2005central} also proved a matching  lower bound for the variance of the same order.
%In \cite{reitzner2005central}, Reitzner also proved that his estimates for the variance are optimal.

%Moreover, he estimated the variance in the case that the points are chosen from the boundary. In this case, he showed that the variance of the volume of the random polytope is of the order $ N^{-(1+\frac{4}{n-1})}$. 

Another question that arises is to investigate concentration of the volume  of random polytopes. Vu \cite{vu2005sharp} used a ``boosted" martingale method to prove a sharp concentration inequality for the symmetric volume difference of a smooth convex body $K$ with $C^2$ boundary and $|K|=1$, and a random inscribed polytope $P_{n,N}$ that is the convex hull of $N$ points chosen uniformly and independently from $ K.$  %Specifically, suppose that $K$ has $C^2$ boundary and $|K|=1$, and let $ P_{n,N} $ be the convex hull of $ N$  points chosen uniformly and independently from $ K.$ 
More specifically, it was shown that there exist positive constants $c_1(K)$ and $c_2(K)$ depending only on $K$ such that for any $\lambda\in\left(0,\frac{1}{4}c_1(K)N^{-\frac{(n-1)(n+3)}{(n+1)(3n+5)}}\right]$,% the following concentration inequality holds:% for the random variable $|K\setminus P_{n,N}|$:
\begin{align}
\Pr\bigg(\big||K\setminus P_{n,N}|-\E[|K\setminus P_{n,N}|]\big| &\geq \sqrt{c_1(K)\lambda N^{-\left(1+\frac{2}{n+1}\right)}}\bigg)  \nonumber \\
%&\qquad \qquad \qquad \qquad \qquad \qquad \qquad
&\leq 2\exp(-\lambda/4)+\exp\left(-c_2(K) N^{\frac{n-1}{3n+5}}\right).{\label{bbb}}
\end{align}Later, Vu \cite{vu2006central}  used a central limit theorem of Reitzner \cite{reitzner2005central} for Poisson point processes and the concentration inequality \eqref{bbb} to prove a central limit theorem for random polytopes inscribed in a $C^2$ convex body with positive curvature.

Although there are numerous results on the statistical properties of random polytopes with a restricted number of vertices,  much less is known about the case of random polytopes with a restricted number of facets. In this direction, B\"or\"oczky and Reitzner \cite{boroczky2004approximation} calculated the expectation of the volume difference of a smooth convex body $K$ and a random circumscribed polytope with $N$ facets. The random polytope was generated as follows: Choose $ N$ i.i.d. random points from the boundary of $K$  with respect to a given density function, and take the intersection of the supporting hyperplanes at these points. In \cite{boroczky2004approximation}, the optimal density that minimizes the expected volume difference was also determined explicitly in terms of $K$. 
%Furthermore, it was shown that if the optimal density is chosen, then as the  dimension tends to infinity, random approximation is asymptotically as good as best approximation.
%**** As in vertices case random is optimal ***** %They calculated the expectation of the volume difference of this random polytope and $ K $.

Recently, Fodor, Hug, and Ziebarth \cite{fodor2016volume} computed the expectation and the variance of the volume difference of $K$ and the polar  $ P_{n,N}^{\circ}$ of the random polytope from (\ref{bbb})  for a general density on $K$. 
%Furthermore, an estimate for the variance of $|P_{n,N}^\circ|$ was also given. 
Surprisingly, the estimates for the variance in \cite{reitzner2003random} and  \cite{fodor2016volume} are equal, up to a constant that depends on $ K.$ 

To the best of the authors' knowledge, this paper is the first  to show concentration results for the volume of random polytopes generated by a probability measure which is not necessarily  uniform, and it is also the first to show concentration results for the volume of random polytopes with a restricted number of facets. We believe that this work is a natural extension of \cite{boroczky2004approximation}.

%This result is a dual analogue of the variance estimate in \cite{reitzner2003random}. 
%Our proof uses geometry and combinatorics, and its intuition follows from  random partial sphere ``coverings'', which will be discussed below.
%\subsection{triangulations} 
%For general $n$, it is probably very difficult, if not impossible, to determine the exact values of $\dell_{n-1}$ and $\divv_{n-1}$. However, recall from formulas  (\ref{zador1}) and (\ref{delll}) that $\divv_{n-1}$ and $\dell_{n-1}$  behave like $\frac{n}{2\pi e}$ as $n$ tends to infinity, up to error terms of order $o(n)$ and $O(\ln n)$, respectively
\subsection{The Mahler volume product of polytopes}

Let $K$ be a convex body in $\R^n$. The Santal\'o point $s(K)$ of $K$ is the unique point that satisfies $|K^{s(K)}|=\min_{z\in\intt(K)}|K^z|$, where  $K^z:=(K-z)^\circ$ is the polar body of $K$ with center of polarity $z$ and $\intt(K)$ denotes the interior of $K$. %The polar body $K^z$ of $K$ with  center of polarity $z$ is defined by $K^z:=(K-z)^\circ$, %$K^z:=\{y\in\R^n: \langle y-z,x-z\rangle \leq 1 \text{ for all }x\in K\}$. 
%When $z$ is the origin, $K^z$ is called the polar body of $K$ and is denoted $K^\circ$. 
The Mahler volume product  of $K$ is then defined by $|K|\cdot|K^{s(K)}|$. The maximizers of the volume product are precisely the ellipsoids; this celebrated result is called the Blaschke-Santal\'o inequality. It was proved for $n\leq 3$ by Blaschke \cite{Blaschke1917,Blaschke1918} and extended to all dimensions by Santal\'o \cite{santalo1949}. The equality conditions were proved by Saint-Raymond \cite{saintraymond1981} in the symmetric case, and by Petty \cite{petty1985} for the general case.  

On the other hand, finding the minimizers of the  volume product is a major open problem in convex geometry, which has attracted considerable interest. {\it Mahler's conjecture} \cite{mahler1} states  that the simplex is the minimizer. It was proven for $n=2$ by Mahler \cite{mahler2}, and Meyer \cite{meyer1991} proved that equality is attained only for triangles. The conjecture remains open for $n\geq 3$. In the case the body $K$ is centrally symmetric, Mahler \cite{mahler1} conjectured that the minimizer is the unit cube, and he proved it in \cite{mahler2} for $n=2$. For $n=3$, the symmetric case was proved in the affirmative by Iriyeh and Shibata \cite{iriyehshibata} (recently, the proof was simplified in \cite{equipartition}). In general dimensions, it was shown by Nazarov et. al. \cite{NPRZ} that the unit cube is a strict local minimizer for the volume product among symmetric bodies. Nevertheless, Mahler's conjecture for symmetric bodies remains open for $n\geq 4$.

%Naturally, one wishes to study some of the geometric properties of the volume product. 
Recently, the volume product of polytopes with a fixed number of vertices was considered by Alexander, Fradelizi and Zvavitch \cite{alexander2017polytopes}. They showed that the functional
\[
F(n,N):=\max_{P_{n,N} \subset \R^{n} \text {has at most $N$ vertices}} |P_{n,N}|\cdot|P^{s(P_{n,N})}_{n,N}|,
\]
achieves its maximum for simplicial polytopes. 
 %showed that the maximum of $F(n,N)$ is achieved by a simplicial polytope with precisely $N$ vertices. %Our next result provides a lower bound for $F(n,N)$ that holds with high probability.
In Theorem \ref{mahler}, we use all of the results in this paper, as well as  results of M\"uller \cite{muller1990approximation} and Reitzner \cite{reitzner2003random}, to derive a lower bound for the asymptotic behavior of $ F(n,N)$.  For more background, as  please see Section \ref{resultstable}. The proof of Theorem \ref{mahler} is in Section \ref{mahlerproof1}.

\section{Discussion: Optimality of random polytopes in high dimensions}\label{resultstable}
%First, we summarize all the known results and our new result, when we assume that $K=B_n$ and  the optimal density is the uniform one.

The following table summarizes known results on the asymptotic behavior of the quantity $$\frac{\E[d(B_n,P_{n,N})]}{ d(B_n,P_{n,N}^b)},$$ 
where $P_{n,N}^b$ is the polytope with $N$ facets or $N$ vertices that is best-approximating with respect to a metric $d$ under a constraint that appears in the first column of the table. The distance $d$ is either the volume difference or Hausdorff metric, and the expectation is taken with respect to the uniform distribution (with the assumption that all the facets or vertices have the same height). In the inscribed and circumscribed cases the height is one, whereas in the arbitrary case the height is chosen to minimize the distance (in expectation).

%or the Mahler volume ``distance" (i.e. maximazing the product). 
The values in the table are given for the following types of polytopes: circumscribed with $N$ facets, inscribed with $N$ vertices  and arbitrarily positioned with either $N$ facets or $N$ vertices.
\begin{table}[h]
\centering
	\begin{tabular}{|l|l|l|}
		\hline
		& \multicolumn{1}{|c|}{Volume difference}                                      & \multicolumn{1}{|c|}{Hausdorff metric}                          \\ \hline
		Facets circumscribed           & $1+O(n^{-1})$   \,\,\,\, \cite{boroczky2004approximation,GruberII}, Thm. \ref{randomvsbest}         & $\Omega(1)\ln N$ \,\,\,\,\cite{boroczky2000polytopal,glasauer1996asymptotic,janson1986random} 
		\\ \hline
		Vertices inscribed      & $1+O(n^{-1}\ln n)$ \,\,\,\,\cite{GruberII,MaS1,MaS2,schutt2003polytopes} &            $\Omega(1)\ln N$ \,\,\,\,\cite{boroczky2000polytopal,glasauer1996asymptotic,janson1986random}                \\ \hline
		Arbitrary facets & $\Theta(1)$     \,\,\,\,\cite{kur2017approximation,Lud06}   &  $\Omega(1)\ln N$ \,\,\,\,\cite{boroczky2000polytopal,glasauer1996asymptotic}\\
		\hline
		Arbitrary vertices & $O(1)$  \,\, \cite{Lud06}, \,\, $\Omega(n^{-1})$ \,\, \cite{boroczky2000polytopal}  &  $\Omega(1)\ln N$ \,\,\,\,\cite{boroczky2000polytopal,glasauer1996asymptotic}\\
		\hline
	\end{tabular}
\end{table}
% \begin{table}[h]
% 	\begin{tabular}{|l|l|l|}
% 		\hline
% 		& Vertices inscribed                                       & Facets circumscribed                          \\ \hline
% 		Volume difference      & $1+O(\ln(n)n^{-1})$ \,\,\,\,\,\cite{GruberII}, \cite{MaS1}, \cite{MaS2}, \cite{schutt2003polytopes} & $1+O(n^{-1})$   \,\,\,\,\, \cite{boroczky2004approximation}, \cite{GruberII}, Theorem \ref{randomvsbest}                             \\ \hline
% 		Hausdorff metric           & $\Omega(1)\ln N$     \,\,\,\,\,\cite{janson1986random,boroczky2000polytopal,glasauer1996asymptotic}       & $\Omega(1)\ln N$ \,\,\,\,\,\cite{janson1986random,boroczky2000polytopal,glasauer1996asymptotic} 
% 		\\ \hline
% 		%Mahler volume product  & $O(\ln n)$    \,\,\,\,\, Thm. \ref{mahler}                          & $O(\ln n)|B_n|^2$    \,\,\,\,\, Thm. \ref{mahler}                     \\ \hline
% 	\end{tabular}
% \end{table}
% \begin{table}[h]
% 	\begin{tabular}{|l|l|l|}
% 		\hline
% 		                       & Arbitrary facets                   & Arbitrary vertices                  \\ \hline
% 		Volume difference         & $\Theta(1)$     \,\,\,\,\,\cite{kur2017approximation}              & $O(1)$      \,\,\,\,\, \cite{Lud06}                                  \\ \hline
% 		Hausdorff metric            & $\Omega(1)\ln N$ \,\,\,\,\,\cite{boroczky2000polytopal,glasauer1996asymptotic} & $\Omega(1)\ln N$ \,\,\,\,\,\cite{boroczky2000polytopal,glasauer1996asymptotic} \\ \hline
% 	%	Mahler volume product& $O(\ln n)|B_n|^2$   \,\,\,\,\, Thm. \ref{mahler}         & $O(\ln n)|B_n|^2$    \,\,\,\,\, Thm. \ref{mahler}            \\ \hline
% 	\end{tabular}
% \end{table}

%\begin{table}[h]
%	\begin{tabular}{|l|l|l|l|l|}
%		\hline
%		& Vertices inscribed                                       & Facets circumscribed                         & Arbitrary facets                   & Arbitrary vertices                  \\ \hline
%		Volume difference      & $1+O(\frac{\ln n}{n})$ \,\,\,\,\,\cite{GruberII}, \cite{MaS1} & $1+O(\frac{1}{n})$   \,\,\,\,\,Cor. \ref{randomvsbest}       & $\Theta(1)$     \,\,\,\,\,\cite{}              & $O(1)$      \,\,\,\,\,\cite{cite}                                  \\ \hline
%		Hausdorff metric           & $\Theta_n(\ln N)$     \,\,\,\,\,\cite{cite}       & $\Theta_n(\ln N)$ \,\,\,\,\,\cite{cite} & $\Theta_n(\ln N)$ \,\,\,\,\,\cite{cite} & $\Theta_n(\ln N)$ \,\,\,\,\,\cite{cite} \\ \hline
%		Mahler volume product  & $\Omega(\ln n)$     \,\,\,\,\,\cite{cite}                           & $\Omega(\ln n)$    \,\,\,\,\,\cite{cite}                 & $\Omega(\ln n)$           \,\,\,\,\,\cite{cite}          & $\Omega(\ln n)$          \,\,\,\,\,\cite{cite}           \\ \hline
%	\end{tabular}
%\end{table}

% Our first observation is the difference between the Hausdorff distance and the Mahler volume product.
%  The best-approximating inscribed polytope for the Hausdorff distance is generated by taking a minimal $\delta$-net with $N$ points. However, with high probability a random polytope does not give an optimal approximation under the Hausdorff metric \cite{janson1986random} ***. In the Mahler volume product, it seems that a random polytope is not optimal as well (up to a dimensional constant). An interesting problem is how to find an optimal polytope for this metric. Moreover, even showing a lower bound of order $c(n)\NN$ seems like a hard task.
%Observe that the Mahler product is invariant to the these constraints, and we believe that the sharp bound is $\Theta(1)|B_n|^2$. Unfortunately, even showing a lower bound of order $c(n)\NN$ seems like a hard task, since it is hard to ``control" this product.

Our main question on the optimality of random polytopes in high dimensions is in the spirit of Corollary \ref{randomvsbest}. In order to formulate it,  we use the main result of Kur \cite{kur2017approximation}, where he found the arbitrarily positioned random polytopes with $N$ facets that minimize the expectation of the symmetric volume difference with the Euclidean ball. Moreover, these random polytopes are optimal up to absolute constants. Namely, by \cite[Theorem 2]{Lud06} and \cite[Remark 2.2]{kur2017approximation}, for all $N \geq 10^n$ it holds that
\[
	c\NN|B_n| \leq |P^b_{n,N} \triangle B_n| \leq \E[|P_{n,N} \triangle B_n|] \leq C\NN|B_n|,
\]
where $P^b_{n,N}$ is the best-approximating polytope with $N$ facets under the constraint that its facets have roughly the same height\footnote{We mean that for every $n \geq 3, N > 10^{n}$ there exists a polytope $P_{n,N}^h$ in $\R^n$ with $N$ facets that all have (roughly) the same height $h$, which satisfies $|P^b_{n,N}|-|P^h_{n,N}| = o(1)\NN$.} (see Corollary~\ref{mainThm}). Thus, it is natural to ask if the following formula holds:
\begin{equation}{\label{Eq:Conj}}
	\lim_{N\to\infty}\frac{\E[|P_{n,N} \triangle B_n|]}{|P^b_{n,N} \triangle B_n|} = 1+o(1).
	%\lim_{n\to\infty}\lim_{N\to\infty}\frac{\E[|P_{n,N} \triangle B_n|]}{|P^b_{n,N} \triangle B_n|} = 0.
\end{equation}
\noindent{In other words, Eq. \eqref{Eq:Conj} asks if random polytopes become ``more optimal" as the dimension increases.}

We strongly believe that Eq. \eqref{Eq:Conj} holds. For example, these random polytopes satisfy the property that as $N$ increases, half of their surface area is outside, which is a condition that best-approximating polytopes must satisfy (as was observed in \cite[Lemma 9]{Lud06}).

%Without getting too much into the details, 
Let $X_1,\ldots,X_N\stackrel{\text{i.i.d.}}{\sim} \sigma$ and $r_{\gamma} :=  {\sqrt{1-\left(\frac{\gamma|\partial B_n|}{N|B_{n-1}|}\right)^{\frac{2}{n-1}}}}$ for any fixed $\gamma \in (0,\infty).$ By Eqs. (\ref{Eq:refine}) and (\ref{Eq:refine2}) below, the quantity 
\begin{equation}\label{quantity1}
     \left|\E\left[\int_{\mathbb{S}^{n-1}}\mathbbm{1}_{\{\max\langle X_{i},x\rangle\leq r_\gamma\}}(x)\,d\sigma(x)\right] - \max_{x_1,\ldots,x_N}\int_{\mathbb{S}^{n-1}}\mathbbm{1}_{\{\max\langle x_{i},x\rangle\leq r_\gamma\}}(x)\,d\sigma(x)\right|
\end{equation}
plays a key role in understanding the question \eqref{Eq:Conj}. This quantity also has a beautiful interpretation that is connected to the ``geometric balls and bins problem", which we define in the next section. Finally, we conjecture that if for every fixed $\gamma \in (0,\infty)$ the quantity \eqref{quantity1} is of the order $o(1)$, then Eq. \eqref{Eq:Conj} holds.

%then under the assumption that there is $\tilde{P}^b_{n,N}$ that all its facets have the same height that has the fo
%\[
%\tilde{P}^b_{n,N} \triangle P^b_{n,N}| = o(1)\NN.
%\] 
 


\section{Geometric generalization of the ``balls and bins" problem}{\label{sphercovering}}

It turns out that the optimality of random polytopes is closely connected to the optimality of random partial sphere coverings. As we shall see, the latter problem is the geometric generalization of the famous ``balls and bins" problem from probability, which we now recall.

Assume that we have $\lceil \alpha N \rceil$ balls ($\alpha \in (0,\infty)$) and $N$ bins. For each ball, we draw a bin uniformly and independently, and we place the ball inside of this bin. 
\begin{enumerate}
	\item By the linearity of expectation, a proportion of $1-e^{-\alpha} + 0.5\alpha e^{-\alpha}N^{-1} + O( c(\alpha)N^{-2})$ of the bins are occupied.
	\item Trivially, when $\alpha=1$ the best configuration of balls places one ball in one bin, i.e., all the bins are occupied.
	\item  If we have $CN\ln N$ balls that are drawn uniformly and independently, where $C>1$ is an absolute constant, then with high probability all of the bins will be full (provided $C$ is large enough).
\end{enumerate}

Next, we can think about the natural and equivalent ``continuous" question. Namely, we replace the $N$ bins by the interval $[0,1]/\{0 \sim 1\} =: \mathbb{S}^{1}$, and the $\lceil \alpha N \rceil$ balls by $\lceil \alpha N \rceil$ subintervals of length $N^{-1}$. Then, we draw a point uniformly and independently from $\mathbb{S}^{1}$ and  place a subinterval with length $N^{-1}$ centered at the drawn point. This ``continuous" version of the balls and bins problem has the same properties as the original: %In this continuous setting, it has the same properties as the previous problem:
\begin{enumerate}
	\item By Fubini's Theorem, the expectation of the length of the union of the subintervals equals $1-e^{-\alpha} + 0.5\alpha e^{-\alpha}N^{-1} + O( c(\alpha)N^{-2})$ (see Eq. \eqref{indepRV} below).% is the same expectation from the discrete problem. (see Lemma **)
	\item Trivially, when $\alpha=1$ the best configuration places $N$ subintervals end-to-end, and the length of their union is $1$.
	\item By \cite[Theorem 1.2]{janson1986random}, if we have $CN\ln N$ disjoint intervals with length $N^{-1}$, then with high probability their union will be the entire sphere $\mathbb{S}^{1}$.
\end{enumerate}

%However, the last problem, 
The  balls and bins problem for $\Bbb{S}^1$ can be extended naturally to the sphere $\Sn$ of  arbitrary dimension $n$ by replacing the subintervals of length $N^{-1}$ with geodesic balls of normalized surface measure $N^{-1}$. In this setting, the geometry of the sphere starts to have an effect. %"kick in".
For example, (up to a set of measure zero) we cannot have $N$ disjoint geodesic balls of volume $N^{-1}$ when $n \geq 3$. Therefore, some of the aforementioned properties may change.
\begin{enumerate}
	\item Remarkably, the expectation of the union of the geodesic balls is dimension-free, and it equals $1-e^{-\alpha} + 0.5\alpha e^{-\alpha }N^{-1} + O( c(\alpha) N^{-2})$ in all dimensions  (see Eq. \eqref{indepRV} below).
	\item  Erd{\H{o}}s, Few and Rogers \cite{erdHos1964amount} essentially proved that when $n$ and $N$ are large enough, the best configuration of $\lceil\alpha N\rceil$ (for $\alpha \in (0.99,1.01)$) caps captures roughly $90\%$ of the measure of the sphere. Thus, in high dimensions the overlap is significant.
	\item  The minimal number of caps (with volume $N^{-1}$) required to cover the entire sphere is at most $Cn\ln n \cdot N$ (\cite{rogers1957note,boroczky2003covering}) and at least $cn\cdot N$ (\cite{rogers1963covering}). 
	\item By \cite[Theorem 1.2]{janson1986random}, if we have $C(n)N\ln N$ disjoint balls of measure $N^{-1}$ (where $C(n)$ grows exponentially fast in $n$), then with high probability their union will be the entire sphere $\mathbb{S}^{n-1}$. 
\end{enumerate}

In some sense, Items 1 and 2 are remarkable. Item 1 states that, amazingly, the expected measure of a random partial covering is invariant to the geometry of the sphere. Item 2 states that in high dimensions, the geometry of the sphere causes an ``intrinsic'' overlap of the balls that cannot be avoided, and is independent of the randomness of the points (unlike the case of $\mathbb{S}^{1}$).  Intuitively, one should expect that this overlap will also reduce the expectation as well. From this discussion, the following questions can be raised:

\begin{enumerate}
	\item What is the variance (or a sharp concentration inequality) for the measure of the union of the geodesic balls?
	%	\item In the flavor of ******. As the dimension tends to infinity, is there a subset of geodesic balls of size $ N$ such that the measure of the overlap between the balls is negligible (i.e., tends to zero as the dimension tends to infinity)? If the answer to this question is negative, then the natural question will be: \emph{Are random ``coverings" optimal?} %\emph{Is random optimal ?}
	\item Does the measure of the overlap increase as the dimension grows? Formally, for any fixed $\alpha \in (0,\infty)$ define the sequence   
	\[
C_{n,\alpha}:=\lim_{N\to\infty}\max_{x_1,\ldots,x_{\lceil \alpha N \rceil}}\sigma(\cup_{i=1}^{\lceil \alpha N \rceil}B_G(x_i,N^{-1})), \quad n\in\Bbb N,
		%\{\underbrace{\lim_{N\to\infty}\max_{x_1,\ldots,x_N}\sigma(\cup_{i=1}^{\lceil \alpha N \rceil}C(x_i,N))}_{C_{n,\alpha}}\}_{n \in \mathbb{N}}
	\]
	where $B_G(x_i,N^{-1})$ is the geodesic ball with center $x_i$ and normalized surface measure $N^{-1}$. Is this sequence decreasing in $n$ for every fixed $\alpha$?
	%\item The deepest question, in the flavor of this paper. 
\item Is the expectation of the random partial covering optimal as the dimension tends to infinity? More specifically, does it hold that\footnote{From personal communication, Prof. Alexey Glazyrin has also observed this phenomenon.}
% at the March 2019 AMS sectional meeting at Auburn University, 
%	 if $C(x_i,N)\subset\Sp$ is the geodesic ball with normalized volume $\sigma(C(x_i,N))=N^{-1}$ and center $x_i\in\Sp$, does the following equality also hold true?% Namely, is the followi 
\[
\lim_{n \to \infty}C_{n,\alpha} = 1-e^{-\alpha}
\]	
for every fixed $\alpha \in (0,\infty)$?
	
	%	In other words, is there a configuration of caps, such that captures at least $ 1-e^{-\alpha} + c_1$ of the measure of the sphere, where $ c_1>0 $ is an absolute constant that does not depend on $ \epsilon $ nor $ n$?
\end{enumerate} 
For the first question, we  give a sharp estimate for the variance, which implies that the deviations decrease as the dimension grows. Also, an immediate use of McDiarmid's inequality yields a concentration inequality. We summarize these results in the next theorem.
\begin{theorem}\label{partialthm}
	Choose $X_1,\ldots,X_{\lceil \alpha N \rceil} \stackrel{\text{i.i.d.}}{\sim} \sigma(\Sn)$,  let $\mathbf{X}_{\alpha N}:=\{X_1,\ldots,X_{\lceil \alpha N \rceil}\}$, and define 
\[
\mathbb{V}(\mathbf{X}_{\alpha N}) :=  \sigma(\cup_{i=1}^{\lceil \alpha N \rceil}B_G(X_i,N^{-1})).
\]
%	$$\mathbb{V}(\underbrace{X_1,\ldots,X_{\lceil \alpha N \rceil}}_{=:\mathbf{X}_{\alpha N}}) :=  \sigma(\cup_{i=1}^{\lceil \alpha N \rceil}C(x_i,N)). $$ 
	Assume that $ N \geq C^{n}$. Then for all $\alpha \in (0,\infty)$,
	 \[
	 	e^{-C_1\alpha} c_1^{n-1}N^{-1} \leq \var(\mathbb{V}(\mathbf{X}_{\alpha N})) \leq e^{-\alpha} c_2^{n-1}N^{-1},
	 \]
	  where $c_1,c_2 \in (0,1)$ and $C_1 \in (1,2)$. Moreover, for any $ N \in \mathbb{N}$ and $\epsilon>0$, it holds that 
	 \[
	 	\Pr(|\mathbb{V}(\mathbf{X}_{\alpha N}) - (1-e^{-\alpha})| \geq \epsilon) \leq 2\exp\left(-2\lfloor\alpha^{-1}\rfloor N\epsilon^2\right).
	 \]
\end{theorem}
\noindent{Unfortunately, our concentration inequality is ``dimension-free'', and we don't know how to utilize the dimension in order to achieve the optimal dimensional constant in the exponent. However, our variance estimate utilizes the dimension, and shows that the variance decreases as the dimension grows.}

We don't know how to answer questions 2 and 3 above, which, in our opinion are fundamental. We believe that the answers to these questions will help provide an understanding of the behavior of volumes in high dimensions.  We conclude this section with a conjecture and a question.
% If the answer to the last question is positive, then it would imply a deep result on the optimality of arbitrarily positioned polytopes for the approximation of convex bodies (see the previous section).
\begin{conj}\label{mainconj1}
	For any $\alpha \in (0,\infty)$, the sequence $\{C_{n,\alpha}\}_{n \in \mathbb{N}}$ is decreasing in $n$. Moreover, for every $ \alpha \in (0,\infty)$,
	\[	
		\lim_{n \to \infty}C_{n,\alpha} = \lim_{n \to \infty}\lim_{N \to \infty}\mathbb{V}(\mathbf{X}_{\alpha N}) = 1-e^{-\alpha}.
	\]
% 	which also implies that under the minimal assumption *** in the previous section, Eq. () holds. Namely, random polytopes in arbitrary position are optimal.
\end{conj}
Note that if Conjecture \ref{mainconj1}  holds (say, for $\alpha=1$), then it gives a remarkable geometric definition of  Euler's number $e$, namely,
\begin{equation}\label{eulerconj}
1-e^{-1} = \lim_{n\to\infty}\lim_{N\to\infty}\max_{x_1,\ldots,x_N}\sigma\left(\cup_{i=1}^N B_G(x_i,N^{-1})\right).
\end{equation}
Finally, we mention a perhaps simpler question, which is to utilize the dimension to find a one-sided concentration inequality that improves as the dimension increases, i.e., as $n\to\infty$ the probability of capturing surface area decreases. 
\begin{ques}
		For any $\alpha \in (0,\infty)$ and any $\epsilon > C(n)/\sqrt{N}$, does it hold that
		\[
			\Pr( \mathbb{V}(\mathbf{X}_{\alpha N}) - (1-e^{-\alpha}) \geq \epsilon) \leq 2\exp\left(-C_1(\alpha)C(n)N\epsilon^2\right)?
		\]
% 		where $C(n),C_1(\alpha)$ are increasing with $\alpha$.
\end{ques}



% Recently, an upper bound for this functional was derived  by  Alexander, Fradelizi and Zvavitch in \cite{alexander2017polytopes}. It was shown there that*****. We prove that******* 
% 	
%	\vspace{2mm}

%In Subsection  \ref{sphercoveringformal}, we give a detailed formulation of our results regarding these problems. 

%  it turns out that in high dimensions, there is a significant overlap. This follows from a result   that when the dimension is large enough any collection of geodesic balls of size $ N(\epsilon,n)$ captures at most $ 1-c_2 $ of the surface area, for some $ c_2>0.$  	 	 
%Unfortunately, we don't the answer to the last question, but we conjecture that the expectation is optimal up to a factor of $ o(1) $. For more details on partial sphere ``coverings", as well as formal conjectures and results, please see 

%The last section of the paper will be dedicated to a conjecture on partial sphere ``coverings''. 
%First, we present some definitions. Let $ M(\epsilon,n) $  be the minimal number of caps with radius $ \epsilon $ that are needed to cover the $ n$-dimensional unit sphere $ \Sn$. A trivial lower bound for $ M(\epsilon,n)$ is $ N(\epsilon,n) := \lceil\frac{|\partial B_n|}{|B_G(x,\epsilon)|}\rceil$, where $ |B_G(x,\epsilon)| $ is the surface area of the geodesic ball $B_G(x,\epsilon)$ with center $ x\in\Sp $ and geodesic radius $ \epsilon>0 $. 

%It turns out that this bound is nearly optimal. More specifically, Rogers  \cite{rogers1957note,rogers1963covering}  proved that if $\epsilon $ is small enough, then $ M(\epsilon,n) \leq (cn\ln n)N(\epsilon,n) $ and $ M(\epsilon,n) \geq  \omega(n)N(\epsilon,n)$. Up to absolute constants, these are the best bounds known today. From these bounds we can see that as the dimension tends to infinity, it becomes more difficult to cover the sphere. The curse of dimensionality also holds for the famous problem of sphere packing (see, e.g., \cite{aubrun2017alice}).  
%
%On the other hand, we noticed a remarkable phenomenon: Let $ \epsilon $ be small enough, and choose $ N(\epsilon,n)$ random geodesic balls of the sphere (i.e., their centers are chosen independently according to the uniform probability measure $\sigma$ on $\Sp$). Then in expectation, the balls capture $ 1-e^{-1} + O(N(\epsilon,n)^{-2}) $ of the measure of the sphere, \emph{for all dimensions.} Consequently, the expected measure of the overlap, i.e. the difference between the sum of the volume of the balls and the volume of their union, is  $ e^{-1}+ O(N(\epsilon,n)^{-2}).$ 
%	 Let $ c \in \R_{+} $ and choose $\lceil c \cdot M(\epsilon,n) \rceil  $ random caps (i.e. their centers are chosen independently and uniformly from the sphere), then in expectation the caps capture $ 1-e^{-c} + O(M(\epsilon,n)^{-2}) $ of the measure of the sphere, \emph{for all dimensions} when $ \epsilon $ is small enough. Consequently, the expected measure of the overlap, i.e. the difference between:the sum of the volume of the caps and the volume of the union, is  $$ 2(c-(1-e^{-c}))+ O(M(\epsilon,n)^{-2}).$$  

%In dimension two,  random geodesic balls are far from optimal in expectation because there are constructions with no overlap. Moreover, from computer simulations the authors observed that in low dimensions, the measure of the overlap is extremely small.  Nevertheless, we tend to believe that the measure of the overlap increases as the dimension tends to infinity because random constructions are often optimal in high dimensions (up to $ o(1) $); see, e.g., \cite{boroczky2004approximation,kur2017approximation,MaS2,schutt2003polytopes}. From these observations, several questions can be raised:
%	\begin{enumerate}
%		\item What is the probability that choosing $ cM(\epsilon,n) $ caps captures $ 1-e^{-c}$ of the measure %surface area 
%of the sphere?
%		\item Let $ c \in (0,1), $ as the dimension tends to infinity, , are there a subsets of caps of size $ cM(\epsilon,n)$ such that the measure of the overlap between the caps is negligible (i.e. tend to zero as the dimension tends to infinity) ? If the answer to this question is negative, the follow-up question will be: \emph{Are random ``coverings" optimal?} %\emph{Is random optimal ?}
%		\item Is the expectation optimal as the dimension tends to infinity? In other words, is there a sequence of subsets of caps of size $ M(\epsilon,n)$ that capture at least $ 1-e^{-c} + c_1(c)$ of the sphere, where $ c_1(c)>0 $ is an absolute constant that does not depend on $ \epsilon $ nor $ n$?
%	\end{enumerate} 
%\begin{enumerate}
%	\item What is the probability that choosing $ N(\epsilon,n) $ geodesic balls captures $ 1-e^{-1}$ of the measure 
%	of the sphere?
%	\item	there a subset of geodesic balls of size $ M(\epsilon,n)$ such that the measure of the overlap between the balls is negligible (i.e., tends to zero as the dimension tends to infinity)? If the answer to this question is negative, then the natural question will be: \emph{Are random ``coverings" optimal?} %\emph{Is random optimal ?}
%	\item Is the expectation optimal as the dimension tends to infinity? In other words, is there a sequence of geodesic balls of size $ N(\epsilon,n)$ that captures at least $ 1-e^{-1} + c_1$ of the measure of the sphere, where $ c_1>0 $ is an absolute constant that does not depend on $ \epsilon $ nor $ n$?
%\end{enumerate}   
%		Surprisingly, the use of a simple concentration inequality gives an affirmative answer to the first question. More specifically, if $ \epsilon $ is small enough, then with extremely high probability $ M(\epsilon,n) $ caps capture $ 1-e^{-c}$ of the surface area of sphere.
%	The answer for the second question negative, when $ c $ is close enough to one, in high dimensions there is a significant overlap. It was shown in \cite{erdHos1964amount} Erd{\H{o}}s, Few and Rogers essentially proved that when $ n $ is large enough and $ c $ is around one , any subset of caps of size $ M(\epsilon,n)$ captures at most $ 1-c_2 $ of the surface area, for some $ c_2>0.$  	 	 
%	Unfortunately, we don't the answer to the last question. The authors conjecture that the expectation is optimal for all $ c \in \R_{+}.$ For more details on partial sphere ``coverings", as well as formal conjectures and results, please see Section  \ref{sphercoveringformal}.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%


%For the following theorems, we define the quantities

%\begin{remark}\label{densityball}
%	Corollary \ref{mainThm} can be extended to all probability measures $\mu$ on $\Sp$ that have a continuous, positive density function $g$, i.e. $d\mu(x) = g(x)\,d\sigma(x)$ and $g(x)>0$ for all $x\in\Sp$. The constants appearing in the exponents will then depend on both the dimension and the density. This claim is proved in section \ref{proofdensityball}.
%\end{remark}

%\begin{equation}\label{epsilonN}
%\end{equation}
%\begin{equation}\label{tnN}
%t_{n,N} := {\sqrt{1-\varepsilon_{n,N}^2}}= 1-\tfrac{1}{2}\left(1+\tfrac{\ln n}{n}+O\left(\big(\tfrac{\ln n}{n}\big)^2\right)\right)\NN. 
%\end{equation}

%. In the next result, we improve the  estimate for $\divv_{n-1}$. %to:%Thus, $\divv_{n-1}$ and $\dell_{n-1}$ differ asymptotically by a term of order at most $o(n)$. % In particular, formulas (\ref{zador1}) and (\ref{delll}) imply the following remarkable result:
%\[
%	\lim_{n\to\infty} \frac{\dell_{n-1}}{\divv_{n-1}} = 1.
%\]  
%Our next result improves the  order of the error term for $\divv_{n-1}$ to $O(1)$. More specifically, our result is the following:
%We prove the following:
%	\section{Proof of Corollary \ref{randomvsbest}}
%	%It was also shown in \cite{boroczky2004approximation} that the  integral on the right-hand side of (\ref{boereitzner1}) is minimized by the density 
%	%\begin{equation}\label{optimaldensity1}
%	%g_{\min}(x):=(\as(K))^{-1}\kappa(x)^{\frac{1}{n+1}}\mathbbm{1}_{\partial K}(x).
%	%\end{equation}
%	Corollary \ref{corollary10} and Theorem \ref{deldiv}  show the power of random constructions in high dimensions. Specifically, let $ K $ be a  $ C^{2} $ convex body in $\R^n$ and   Then by (\ref{GruberOutEqn}) and Theorem \ref{deldiv}, 
%	\[
%	\lim_{N\to\infty}N^{\frac{2}{n-1}}|P_{K,N}^b \setminus K| = \frac{1}{2}\divv_{n-1}(\as(K))^{\frac{n+1}{n-1}}=\frac{1}{2}\big((2\pi e)^{-1}(n+\ln n)+O(1)\big)(\as(K))^{\frac{n+1}{n-1}} %\sim Cn(\as(K))^{\frac{n+1}{n-1}}\NN,
%	\]
%	whereas by Theorem 1 in \cite{boroczky2004approximation},
%	\begin{align*}
%	\lim_{N\to\infty}N^{\frac{2}{n-1}}\E[|P_{n,N} \setminus K|] &= \frac{1}{2}|B_{n-1}|^{-\frac{2}{n-1}} \Gamma\left(\frac{2}{n-1}+1\right)(\as(K))^{\frac{n+1}{n-1}}\\&=\frac{1}{2}\big((2\pi e)^{-1}(n+\ln n)+O(1)\big)(\as(K))^{\frac{n+1}{n-1}}. 
%	\end{align*}%\sim C_1n(\as(K))^{\frac{n+1}{n-1}}\NN.
%	Therefore, combining the last two equations the claim follows. \qed
%
%\vskip .1in
%	
%	The natural question that follows from Corollary \ref{randomvsbest} is: Can this result be extended to arbitrarily positioned polytopes?\footnote{*****} In other words, does the following formula hold?
%	\begin{equation}\label{noCovexlimArb}
%	\lim_{N\to\infty}\frac{\E[|P_{n,N} \triangle K|]}{|P^{b}_{K,N} \triangle K|} 
%	%= \frac{\divv_{n-1}}{|B_{n-1}|^{-\frac{2}{n-1}} \Gamma\left(\frac{2}{n-1}+1\right)}
%	= 1+o(1).
%	\end{equation}
%	Using \cite{kur2017approximation,ludwig1999asymptotic}, we know that for all sufficiently large $N$,
%	\[
%	\frac{\E[|P_{n,N} \triangle B_n|]}{|P^{b}_{B_n,N} \triangle B_n|} = O(1),
%	\]
%	where **** . In order to try to answer this question, we need first to assume that w.l.o.g. that  $P^{b}_{B_n,N}$ facets have the same height. Then, using **** we see that it is enough to answer the following question   
%	
%	%where we have used Theorem \ref{deldiv} (see also \cite{boroczky2004approximation, schutt2003polytopes}).
%	% Moreover, for the symmetric volume difference of a convex body and a circumscribed polytope, using Theorem \ref{geneThm} one can remove this expectation formally
%	%s   limit holds w.h.p. for a random polytope, which implies that ``random approximation is as good as best-approximation'' as the dimension tends to infinity.
%	%Yet, using our estimation on $ \divv_{n-1},$ we derive that 
%	%\[
%	%\big|\E[|P_{n,N} \setminus K|] - |P_{K,N}^b \setminus K|\big| \sim C(\as(K))^{\frac{n+1}{n-1}}\NN,
%	%\]
%%\end{remark}
%Rather than asking for the extremal polytopes that maximize $F(n,N)$, one may ask for estimates for the maximum value that $F(n,N)$ can achieve. By the Blaschke-Santal\'o inequality (see, e.g., \cite{SchneiderBook}), we know that $F(n,N) < |B_n|^2$. We believe that this estimate can be improved to:
%
%\begin{conj}
%	Let $ P_{n,N} $ be any polytope with $ N $ vertices in $ \R^{n}.$ Then there exists a universal constant $ c>0 $ such that
%	\[
%	|P_{n,N}|\cdot|P_{n,N}^{\circ}| \leq (1-c\NN)|B_n|^2 .
%	\]
%	Moreover, there exists a polytope $Q_{n,N}$ with $N$ vertices in $\R^n$ such that
%	\[
%	|Q_{n,N}|\cdot|Q_{n,N}^{\circ}| \geq (1-C\NN)|B_n|^2
%	\]for some universal constant $C>0$.
%\end{conj}
%\subsection{Partial random sphere ``covering"}{\label{sphercoveringformal}}
%%****** Read the Intro first  *****
%In this subsection we formalize the discussion from Subsection \ref{sphercovering} on random partial coverings of the unit sphere. Now we consider a ``covering" of $\Sp$ by $N_{c,\epsilon(n)}:=  \left\lceil c\cdot\frac{|\partial B_n|}{|B_G(x,\epsilon(n))|}\right\rceil $ random geodesic balls, where $  c\in\R_{+}$. We now also assume that the radius of the geodesic balls $B_G(X_i,\epsilon)$ may  depend on the dimension, i.e., $\epsilon=\epsilon(n)$. 
%Choose $N_{c,\epsilon(n)}$ points uniformly and independently from $\Sp$ and denote them by $X_1,\ldots,X_{N_{c,\epsilon(n)}}$.
%Define the random variable\begin{equation}\label{xRV}
%g(X_1,\ldots,X_{N_{c,\epsilon(n)}}) := 1-\int_{\mathbb S^{n-1}} \mathbbm{1}_{\{\max\langle X_i,x\rangle\leq \epsilon(n)\}}(x)\,d\sigma(x),% =\sigma(h_P>1),
%\end{equation} and observe that  
%\[ 
%g(\mathbf{X}) :=g(X_1,\ldots,X_{N_{c,\epsilon(n)}})  = \sigma\left(\cup_{i=1}^{N_{c,\epsilon(n)}}B_G({X_i,\epsilon(n)})\right).
%\]
%%independently and identically distributed with respect to the uniform measure $ \sigma $
%
%The next result is an immediate corollary of McDiarmid's inequality. It gives an answer to the first question on sphere ``covering" in the introduction, and it proves that the probability of generating a bad ``cover" is extremely low. Moreover, its proof provides the intuition for the proofs of Corollary \ref{mainThm} and its generalization Theorem \ref{geneThm}. Thus, we encourage the reader to first read the proof of Corollary \ref{Thm1} in Section \ref{spherethm} at the end of the paper  before reading the proofs of Theorems \ref{mainThm} and \ref{geneThm}****check numbering***.
%
%\begin{corollary}{\label{Thm1}}
%	Let $g(\mathbf{X})$ be the random variable defined in (\ref{xRV}). Then $ g(\mathbf{X}) $ is tightly concentrated around its expectation, i.e., for all $\epsilon>0$,
%	\[
%	\Pr(|g(\mathbf{X}) - (1-e^{-c}) | > \epsilon) \leq 2e^{-2c^{-1}N_{1,\epsilon(n)}\epsilon^2}.
%	\]
%\end{corollary}
%This concentration inequality is optimal for big $ \epsilon $. Indeed, an immediate application of the result in \cite{wendel1962problem} shows that
%\[
%\Pr(g(\mathbf{X}) \leq \tfrac 12) \sim 2^{-N}.
%\]
%%On the other hand it's dimension free which is hiding the geometry of this problem, based on the result of ...... Thus we believe that there exists a one side concentration inequality, **** side of increasing ****  
%
%%Theorem \ref{Thm1} can be proven in a more generalized way:
%%\begin{corollary}\label{COVERcor}
%%	Let $r\in (1+N^{-\frac{2}{n-1}},1+N^{-\frac{1.1}{n-1}}),$ and define 
%%	\[
%%	Y_r := 1-\int_{\mathbb S^{n-1}} \mathbbm{1}_{\{\max\langle X_i,x\rangle\leq r^{-1}\}}\,d\sigma(x).
%%	\] Then for any $ \epsilon > 0,$ the following holds:
%%	\[
%%	\Pr(|Y_r-\E[Y_r]| > \epsilon) \leq 2\exp\left(-c_1(n)N\left(\frac{r-1}{\NN}\right)^{\frac{n-1}{2}}\epsilon^2\right).
%%	\]  
%%\end{corollary}
%%\begin{remark}
%%	This bound is optimal up to an absolute constant.
%%\end{remark}
%
%%\begin{remark}
%%The proof of Theorem \ref{Thm1} and the Efron-Stein inequality (see, e.g., \cite{BLM}  p. 56) imply \[\var(g(X)) \leq \tfrac{1}{4}N^{-1}.\]
%%\end{remark}
%
%\vspace{1mm}
%
%The main disadvantage of Corollary \ref{Thm1} is that the same concentration bound holds for increasing and decreasing the volume of the covering. The authors believe (based on asymptotic geometry) that as dimension tends to infinity, it becomes more difficult to increase the volume of the covering. We conjecture that a sharper one-sided concentration inequality holds when the radius of the geodesic balls is small enough.
%\begin{conj}
%	For all $ \epsilon > 0 $, it holds that
%	\[
%	\Pr(g(\mathbf{X})- (1-e^{-c}) > \epsilon) \leq Ce^{-c(n)N_{c,\epsilon(n)}\epsilon^2},
%	\]
%	where $ c(n) \to \infty $ as $ n\to\infty.$
%\end{conj}
%%\begin{conj}
%%	The result in Theorem \ref{Thm1} can be improved to  
%%	\[
%%	\Pr(|g(X)-\E[g(X)]| > \epsilon) \leq 2e^{-c(n)N\epsilon^2}.
%%	\]	
%%\end{conj}
%
%%As a corollary to Theorem \ref{Thm1}, we derive an upper bound for the variance of $g(X)$.
%
%%\begin{corollary}\label{vargx}
%%\[\var(g(X)) \leq \frac{1}{4}N^{-1}\]
%%\end{corollary}
%
%Our final conjecture formalizes the last question from Subsection \ref{sphercovering}.
%\begin{conj}{\label{conj}}
%	Let $ c \in \mathbb{R}_{+}$. When the dimension is large enough and $ \epsilon $ is small enough, we define the quantile
%	%	\vspace{2mm}and let $ B_G(y_1,\varepsilon_{n,N}),\ldots,  B_G(y_{c\cdot M(\varepsilon_{n,N},n)},\varepsilon_{n,N}) $ be any subset of caps of $\Sp$.
%	\[c_{n,\epsilon} := %\max_{\ldots,x_i,\ldots}\sigma\left(\cup_{i=1}^{N_{c,\epsilon(n)}}B_G({x_i,\epsilon})\right) \] 
%	%\max_{x_1,\ldots,x_{N_{c,\epsilon(n)}} \in \Sp}\sigma\left(\cup_{i=1}^{N_{c,\epsilon(n)}}B_G({x_i,\epsilon})\right). \] 
%	\max \left\{\sigma\left(\cup_{i=1}^{N_{c,\epsilon(n)}}B_G({x_i,\epsilon})\right): x_1,\ldots,x_{N_{c,\epsilon(n)}} \in \Sp\right\}. \] 
%	Then for fixed $ \epsilon $,  the sequence $\{c_{n,\epsilon}\}_{n=1}^{\infty}$ is decreasing and $ c_{n,\epsilon} \to 1-e^{-c}$ as $ n \to\infty $.
%	%	The second question in the introduction regards the case when $ c_1=1.$
%\end{conj}

%We believe that the variance of $ |P_{n,N} \setminus K| $ is at most $ c(K,\mu)N^{-(1+\frac{4}{n-1})}$. Moreover, we conjecture that that inequality (\refeq{maineq}) can be improved to:
%\begin{conj}
%	Let $P_{n,N}$ be the random circumscribed polytope that was defined in Theorem \ref{geneThm}. Then when $N$ is large enough, for any $\epsilon>0$ it holds that
%	\[
%	\Pr(\big||P_{n,N} \setminus K| - \E[|P_{n,N}\setminus K|]\big| \geq \epsilon) \leq2 e^{-c(K,\mu)N^{1+\frac{4}{n-1}}\epsilon^2} + e^{-c(K,\mu)N^C}.
%	\]
%\end{conj}
% From this and Corollary \ref{mainThm}, we get the following result.
%\begin{corollary}{\label{Sufracedev}}
%Let $ P_{n,N} $ be the polytope defined in Corollary \ref{mainThm}. 
%\end{corollary}

%	As a consequence of the proof of Corollary \ref{mainThm}, we derive the following almost sharp concentration inequality for  approximation by circumscribed polytopWes with a restricted number of facets. 
% 	\begin{corollary}{\label{secThm}}
% 		Let $ Q_{n,N} $ be a random polytope in $ \R^{n} $ that is defined by
% 		\[
% 		Q_{n,N} := \bigcap_{i=1}^{N}\{x\in\R^n: \inneri{X_i}{x} \leq 1 \},
% 		\] 
% 		where $ X_i \stackrel{\text{i.i.d.}}{\sim} \sigma.$ Then when $ N$ is large enough, for any $ \epsilon > 0 $ the following holds:
% 		\[
% 		\Pr(||Q_{n,N} \setminus B_n| - \E[|Q_{n,N}\setminus B_n|]| \geq \epsilon) \leq e^{-c(n)N^{1+\frac{4}{n-1}}\ln(N)^{-1}\epsilon^2} + e^{-c_1(n)N\ln(N)^{-1}}.
% 		\]
% 	\end{corollary}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\vspace{5mm}	 

%Using the ideas of the proof of Corollary \ref{mainThm}, we derive the main result of this paper, which is a ``dual" analogue of a result of Vu \cite{vu2005sharp}.



%		 It follows from a result of Wendel \cite{wendel1962problem}, where it is shown that the probability, when the uniform distribution is chosen, the event that $ P_{n,N} $ which is defined on \ref{secThm}, is unbounded equals to $ e^{-c(n)N}.$ 
%Our belief is based both on the proof and on the results in \cite{reitzner2003random,vu2005sharp} for the variance (under the uniform distribution) of the volume difference $ |K \setminus P^{\circ}_{n,N}|, $  where $P_{n,N}^\circ\subset K$ denotes the polar of the polytope $P_{n,N}$ defined in (\ref{geneThm}). The  variance results in these two papers state that 
%\[
%\var(|K \setminus P^{\circ}_{n,N}|) \leq c(K)N^{-(1 + \frac{4}{n-1})}.
%\]
%Theorem \ref{geneThm} and some basic probability arguments imply that under the event  $ P \subset  2K,$ that holds with probability of at least $ 1-e^{-c(K)N},$ that   
%s 	In \cite{fodor2016volume}, a better estimate for the variance was shown, under the event $ Q_{n,N} $ is bounded:
%\[
%\var(|P_{n,N} \setminus K|) \leq c(K)N^{-(1 +\frac{4}{n-1})}(\ln N)^{2+\frac{4}{n-1}}.
%\] 

%\vspace{2mm}
% 	\begin{corollary}\label{dualvureitzner}
% 		Given a convex Let $ P_{n,N} $ to be defined as in Theorem (\ref{geneThm}). Then for any $ \lambda > 0 $,
% \[
% 	\Pr(|P_{n,N} \setminus B_n| - \E[|P_{n,N} \setminus B_n|]| \geq \sqrt{V(K)\lambda}) \leq e^{-\lambda},
% \]
% where $ V(K) = c(K)N^{-1 - \frac{4}{n-1}}\ln(N)^{-1}.$ Following Section 2.2 in \cite{vu2005sharp}, we give the following estimate for the variance of $ P_{n,N},$ under the event that $ Q_{n,N} $ is bounded:  
% \[
% \var(|P_{n,N} \setminus B_n|) \leq c(n)N^{-1 + \frac{2}{n-1}}\ln(N)^{-2(1-\frac{1}{n-1})}.
% \]
% 	\end{corollary}

% \begin{remark}
% 	Corollaries \ref{secThm} and \ref{dualvureitzner} also hold for the arbitrarily positioned polytopes $ P_{n,N}$ that are defined in Corollary \ref{mainThm}.
% \end{remark}

%\subsection{Delone triangulation numbers and Dirichlet-Voronoi tiling numbers}{\label{delone}}

%Let $K$ be a convex body with $C^2$ boundary and Gauss curvature $\kappa>0$. Gruber \cite{GruberII} proved that\begin{align}
%\lim_{N\to\infty}\frac{\min\{|K\setminus P|: P\subset K, P\text{ has at most }N \text{ vertices}\}}{\NN} &= \frac{1}{2}\dell_{n-1}(\as(K))^{\frac{n+1}{n-1}}
%\end{align}and
%\begin{align}
%\lim_{N\to\infty}\frac{\min\{|P\setminus K|: P\supset K, P\text{ has at most }N \text{ facets}\}}{\NN} &= \frac{1}{2}\divv_{n-1}(\as(K))^{\frac{n+1}{n-1}},
%\end{align}where $\as(K)=\int_{\partial K}\kappa(x)^{\frac{1}{n+1}}\,d\mu_{\partial K}(x)$ is the affine surface area of $K$, and $\dell_{n-1}$ and $\divv_{n-1}$ are positive constants that depend only on the dimension. 

%This corollary and Corollary 3**,  show the power of random construction in higher dimensions. Let $ K $ be a convex body and $ P_{K,N} $ be the best circumscribed approximating polytope with $ N $ facets with respect to the symmetric volume difference. Clearly from Eq **** and corrolary when $ N $ is large enough
%\[
%	|P_{K,N} \setminus K| = ***
%\]
%Also by Theorem *** We also know that for choosing the density **** with extremely high probability the random polytope satisfies
%\[
%	|Q_{K,N } \setminus K| = *****
%\]   
%Observe that the approximation order is of order **** but random construction gives us optimality of a small perturbation of order **** which proves that random is extremely close to optimal in high dimensions. 
%The following corollary follows from Theorem \ref{deldiv} and Theorem 2 in \cite{MaS2}, where it was proven that $ \dell_{n-1} =  (2\pi e)^{-1}n + O(\ln(n)) $. 

%Formulas (\ref{zador1}) and (\ref{delll}) also show that $\divv_{n-1}$ and $\dell_{n-1}$ differ asymptotically by a term of order at most $o(n)$. As a corollary to Theorem \ref{deldiv}, we improve the order of this term to $O(\ln(n))$.% and the last theorem
%\begin{corollary}
%	Let $ \dell_{n-1}$ and $\divv_{n-1} $  be defined as in \eqref{GruberInEqn} and \eqref{GruberOutEqn}. Then the following holds:
%	\begin{equation}
%	|\divv_{n-1} - \dell_{n-1}| = O(\ln(n)) 
%	\end{equation}
%%	\end{equation}where $C>0$ is an absolute constant.
%\end{corollary}


%From Theorem \ref{deldiv} and the asymptotic formulas (\ref{GruberInEqn}) and (\ref{GruberOutEqn}), it follows that when the dimension is sufficiently large, the ratio between $\min\{|K\setminus P|: P\subset K, P\text{ has at most }N\text{ vertices}\}$ and $\min\{|P\setminus K|: P\supset K, P\text{ has at most }N\text{ facets}\}$   tends to one. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\subsection{Application of all our results}
%\subsection{The Mahler volume product of random polytopes}\label{mahlersection}


%Given two convex bodies $K$ and $L$ in $\R^n$, the symmetric volume difference $d_S(K,L)$ is defined by
%\[
%d_S(K,L):= |K\triangle L| = |K\cup L|-|K\cap L|=|K\setminus L|+|L\setminus K|,
%\]where $|\cdot|$ denotes $n$-dimensional volume.


%
%Given a convex body $K$ in $\R^n$ and an integer $N\geq n+1$, there is at least one optimal polytope $P_{n,N}^b$ with $N$ facets that minimizes $|P\triangle K|$ over all choices of polytopes $P$ in $\R^n$ with $N$ facets. The polytope $P_{n,N}^b$ is called a best-approximating polytope of $K$ with respect to the symmetric volume difference, and it is defined by% the condition
%\[
% P_{n,N}^b := \text{argmin}\{|P\triangle K|: P\text{ is a polytope in }\R^n\text{ with at most }N\text{ facets}\}.
%\]
%Similarly,  a best-approximating circumscribed polytope $  Q_{n,N}^b \supset K  $ is defined by   
%\[
%	Q_{n,N}^b := \text{argmin}\{|P\triangle K|: P \supset K  \text{ is a polytope in }\R^n\text{ with at most }N\text{ facets}\}.
%\]

%In most cases, the explicit construction of best-approximating polytopes is difficult or out of reach. However, 
%An immediate application of this conjecture is true, is an improvement to the a $ \divv_{n-1} $

%*****For readability, state here explicitly the connection between the previous optimality conjecture and the optimality of arbitrarily positioned polytopes for approximation of convex bodies*******


\subsection*{Acknowledgements}
		The authors would like to thank the Mathematical Sciences Research Institute (MSRI) for its hospitality. It was during the authors' visit there when significant progress was achieved. The second-named author wants to thank Prof. Emanuel Milman for his useful suggestions, and also his great advisor Prof. Boaz Nadler and Prof. Yuval Filmus. The authors would also like to thank Prof. Monika Ludwig, Prof. Carsten Sch\"utt and Prof. Elisabeth Werner for the enlightening discussions and insightful suggestions.
		 Finally, we would like to thank the anonymous referees for  thoroughly reading this article, and for their helpful comments and corrections.
		
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection*{Background and notation}

Here we provide some background information on convex sets and sphere coverings, and we fix the notation that will be used throughout the paper. 

\vspace{2mm}

%A convex body $K\subset\R^n$ is a convex, compact subset of $\R^n$ with nonempty interior. 
The $n$-dimensional volume of a compact set $K\subset \R^n$ is denoted $|K|$. The boundary of $K$ is denoted $\partial K$, and the surface area of $K$ is $|\partial K|$. The Gaussian curvature of $K$ at $x\in\partial K$ is denoted $\kappa(x)$. 
The polar of $K$ is the set $K^\circ:=\{x\in\R^n: \langle x,y\rangle \leq 1, \,\forall y\in K\}$. %A polytope is the convex hull of a finite point set in $\R^n$; it is well-known that every polytope can also be represented as a (bounded) intersection of halfspaces, and vice versa.
%	A {\it convex body} $K\subset\R^n$ is a convex, compact set with nonempty interior
%The support function of $K$ is denoted by $h_K.$ %and the radial function of $K$ is denoted by $\rho_K$. The polar body of $K$ is denoted by  $K^\circ$. 
%From the definit iions of the support and radial function and of the polar body, $\rho_{K^\circ}=1/h_K$ and, if the origin lies in the interior of $K$, then $K^{\circ\circ}=K$. A {\it polytope} is the convex hull of a finite point set in $\mathbb R^n$

The Euclidean unit ball in $\mathbb R^n$ centered at the origin is denoted by $B_n=\{x\in\R^n: \|x\|\leq 1\}$, where $\|x\|= (\sum_{i=1}^n x_i^2)^{1/2}$ is the Euclidean norm of $x=(x_1,\ldots,x_n)\in\R^n$. The boundary of $B_n$ is the unit sphere $\Sp=\{x\in\R^n: \|x\|=1\}$, and $\sigma$ denotes the uniform probability measure on $\Sp $, i.e., $\sigma(A) = \frac{|A|}{|\partial B_n|}$ for any Borel set $A\subset\Sp$. The volume of the unit ball is $|B_n| = \frac{\pi^{n/2}}{\Gamma(\frac{n}{2}+1)}$, where $\Gamma(x)=\int_0^\infty t^{x-1}e^{-t}\,dt$ denotes the gamma function. In particular, the volume and surface area of the unit ball satisfy the cone-volume formula $|\partial B_n|=n|B_n|$. For more information on convex sets, see, e.g., the monograph of Schneider \cite{SchneiderBook}.

\vspace{2mm}

Let $ K$ be a $ C^2 $ convex body with  positive generalized Gaussian curvature. Consider the metric space $(\partial K, d_G)$, where $d_G$ denotes the geodesic distance on $\partial K$. We shall use the notation $B_G(x,r):=\{y\in\partial K: d_G(x,y)\leq r\}$ to denote the geodesic ball with center $x$ and radius $r$. A finite subset $\mathcal N\subset \partial K$ is called a {\it $\delta$-net} of $\partial K$ if for every $x\in \partial K$ there exists $y\in \mathcal N$ such that $d_G(x,y) \leq \delta$. A $\delta$-net $\mathcal N$ of $\partial K$ is also called a {\it covering} of $\partial K$  since $\cup_{x\in\mathcal N}B_G(x,\delta)\supset \partial K$. We use the term ``covering" in quotes to mean a partial covering, i.e.,  the union of the corresponding geodesic balls is a proper subset of $\partial K$. When $ K = B_n $, a $\delta$-net of $\Sp$ is also called a {\it sphere covering}. % since $\cup_{x\in\mathcal N}B_G(x,\delta)\supset \Sp$. %The minimal cardinality of a $\delta$-net on $\Sp$ is called the {\it covering number} of $\Sp$ (at scale $\delta$).  %and is denoted $M(\delta,n)$.  
For more information on sphere coverings, see, e.g., the monograph of B\"or\"oczky \cite{boroczky2004finite}. 

\vspace{2mm}

Throughout the paper, $c,c_1,C,c_0,c_1,c_2,\ldots$ will denote positive absolute constants that may change from line to line. The dependence of a positive constant on  the dimension $n$ or a convex body $K$ or a probability measure $\mu$ will always be stated explicitly as  $ c(n),C(n),c_0(n),c_1(n),\ldots$ or $ c(K), C(K), c_1(K)\ldots $ or $c(\mu), c(\mu,n),\ldots$, respectively; moreover, these constants  may also change from line to line. Finally, please note that all instances of the asymptotic notations $O(\cdot),o(\cdot)$ are meant with respect to the dimension $n$ only.

\section{Auxiliary lemmas}
%The following lemma is used in the proofs of Theorem \ref{Thm1} %and Corollary \ref{COVERcor}. First we introduce Gromov's comparison theorem (see, e.g., Theorem 5.38 in \cite{aubrun2017alice}, and for a proof see \cite{milman2009asymptotic}), applied to the boundary of a $ C^2 $  convex body.
%\begin{lemma}{\label{Gromov}}
%	Let $g:\otimes_{i=1}^N \partial K \to\R$ be an $ L$-Lipschitz  function with respect to the metric $ \sqrt{\sum_{i=1}^{N}d_G(x_i,y_i)^2 }$.  Then% the following holds:
%	\[
%	\Pr_{x_1,\ldots,x_N}(|g(X) - \E[g(X)]| > \epsilon) \leq 2e^{-C(K)\epsilon^2 L^{-2}},
%\]
%	where the probability is with respect to the measure $ \otimes_{i=1}^N Uni(\partial K).$ 
%\end{lemma}

The following is the classical McDiarmid's inequality \cite{McDiarmid1989}. It is used in the proofs of Theorem \ref{geneThm}, Corollary \ref{mainThm} and Theorem  \ref{partialthm}.
\begin{lemma}[McDiarmid's inequality]
	Let $ X_1,\ldots,X_m $ be independent random variables all taking values in the set $ \mathcal{X}.$ Further, let $ f:\mathcal{X}^m\to \R $ be a
	function of $ X_1,\ldots,X_m $ such that for $1\leq i\leq m$, %$ \forall i \  \forall x_1,\ldots,x_i,x'_i,\ldots,x_m \in \mathcal{X} $
	\begin{align}\label{boundeddiff}
	%\forall i \  \forall x_1,\ldots,x_i,x'_i,\ldots,x_m \in \mathcal{X}: \,\,
	%|f(x_1,\ldots,x_i,\ldots,x_m) - f(x_1,\ldots,x'_i,\ldots,x_m)| \leq c_i.
	\sup_{x_1,\ldots,x_m,x_i'\in\mathcal{X}}|f(x_1,\ldots,x_i,\ldots,x_m) - f(x_1,\ldots,x'_i,\ldots,x_m)| \leq c_i .
	\end{align}
	Then
	\[
	\Pr(|f(X_1,\ldots,X_m)-\E[f]| \geq \epsilon) \leq 2\exp\left(-\frac{2\epsilon^2}{\sum_{i=1}^{m}c_i^2}\right).
	\]
\end{lemma}
%The next lemma is the Efron-Stein inequality (see, e.g., \cite{BLM}  p. 56). It is used in the proof of Corollary \ref{vargx}.
%\begin{lemma}
%Suppose that $f$ satisfies the bounded differences property (\ref{boundeddiff}) with constants $c_1,\ldots,c_m$. Then 
%\[\var(f(X_1,\ldots,X_m)) \leq \frac{1}{4}\sum_{i=1}^m c_i^2.\]
%\end{lemma}
%This holds because $\{X+Y \geq \epsilon\} \subset \{X\geq \epsilon\}\cup\{Y\geq\epsilon\}$.
%Now we introduce Logarithmic Sobolev Inequality, which defined as follows **********  

%We can extend Lemma \ref{Gromov} to a larger class of probability measures in the following way: Let $K$ be a $C^2$ convex body with  positive curvature, and let $\mu$ denote a probability measure on $\otimes_{i=1}^N\partial K$. We say that $(\otimes_{i=1}^N\partial K, \mu)$ satisfies a Logarithmic Sobolev Inequality (LSI) if there exists a constant $C_{n,\mu}$ (that depends on $n$ and $\mu$) such that for every smooth function $f:\otimes_{i=1}^N\partial K\to\R$ with $\int_{\otimes_{i=1}^N\partial K} f\, d\mu=1$,\begin{equation}
%\int_{\otimes_{i=1}^N\partial K}f^2(x)\log f^2(x)\,d\mu(x) \leq C_{n,\mu}\int_{\otimes_{i=1}^N\partial K}\|\nabla f(x)\|^2\,d\mu(x).
%\end{equation}
%The LSI  is connected to concentration of measure phenomena via Herbst's argument (see, e.g., \cite{aubrun2017alice} p. 132). We use a special case of this result, when it is applied to probability measures on the boundary of a smooth convex body. %The following lemma  Lemma \ref{Gromov} to a class of probability measures on $\partial K$ which satisfy an LSI, which includes the uniform measure.

%\begin{lemma}%[Herbst's argument for smooth convex bodies]
%Let $K$ be a $C^2$ convex body with  positive curvature, and let $\mu$ denote a probability measure on $\otimes_{i=1}^N\partial K$ that satisfies an LSI with constant $C_{n,\mu}$. Then every $L$-Lipschitz function $F:\otimes_{i=1}^N\partial K\to\R$ satisfies, for every $\epsilon>0$, the concentration inequality
%\[
%\mu_{x_1,\ldots,x_N}\left(|F(x_1,\ldots,x_N)-\E_\mu[F(x_1,\ldots,x_N)]|> \epsilon\right) \leq e^{-c\epsilon^2 C_{n,\mu}^{-1}L^{-2}}.
%\]
%\end{lemma}

%		\begin{lemma}{\label{Sobolev}}
%		Let $g:\otimes_{i=1}^N \partial K \to\R$ be an $ L$-Lipschitz  function with respect to the metric $ \sqrt{\sum_{i=1}^{N}d_G(x_i,y_i)^2 }$.  Then% the following holds:
%		\[
%		\Pr_{x_1,\ldots,x_N}(|g(X) - \E[g(X)]| > \epsilon) \leq 2e^{-C(K)n\epsilon^2 L^{-2}},
%		\]
%		where the probability is with respect to the measure $ \otimes_{i=1}^N Uni(\partial K).$ 
%	\end{lemma}	 
%
\vspace{2mm}

We will also need Stirling's inequality (see, e.g., \cite{artin}).

\begin{lemma}[Stirling's inequality]\label{stirlingineq}
	For $x>0$,
	\[x^x e^{-x}\sqrt{2\pi x} < \Gamma(x+1) < x^x e^{-x} e^{\frac{1}{12x}}\sqrt{2\pi x} .\]
\end{lemma}

An immediate consequence of Stirling's inequality is the following well-known lemma.

\begin{lemma}\label{stirling}
	\[
	\frac{\sqrt{2\pi}}{\sqrt{n+2}} \leq \frac{|B_n|}{|B_{n-1}|} \leq  \frac{\sqrt{2\pi}}{\sqrt{n}}
	\]
\end{lemma}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\section{Proofs}\label{proofssec}
%	This section is divided into three parts: First we prove the results that are related to random sphere ``covering'', then we prove the results that are related to the random polytopes, and finally we prove the results that are related to the Dirichlet-Voronoi tiling numbers. 
%\subsection*{Random sphere ``covering''}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%\begin{proof}[Proof of Corollary \ref{vargx}]
%The Efron-Stein inequality implies \begin{align*}
%\var(g(X)) \leq \frac{1}{4}\sum_{i=1}^N c_i^2 \leq \frac{1}{4}N^{-1}.
%\end{align*}
%\end{proof}
%	\begin{proof}[Proof of Corollary \ref{COVERcor}]
%		In the proof of Theorem \ref{Thm1}, we only used the value of $ \varepsilon_{n,N} $ for the estimation of the surface area of a spherical cap. Thus we get
%				\[
%\Pr(|f(X_1,\ldots,X_N) -\E[f]| > \epsilon|\partial B_n|) \leq 2e^{-2\frac{\epsilon^2|\partial B_n|^2}{\sum_{i=1}^{N}(r-1)^{(n-1)}}} \leq 2e^{-c_1(n) N\left(\frac{r-1}{\NN}\right)^{\frac{n-1}{2}}\epsilon^2}.
%\]
%%		Thus, applying Lemma \ref{Gromov} yields
%%		\[
%%		\Pr(|f(X_1,\ldots,X_N) -\E[f(X_1,\ldots,X_N)]| > \epsilon) \leq 2\exp\left(-c_1(n)N^{1 - \frac{2}{n-1}}\left(\frac{r-1}{\NN}\right)^{n-2}\epsilon^2\right),
%%		\]as desired.
%	\end{proof}  
%\subsection*{Random polytopes}

\vskip .1in

For simplicity, we first prove Corollary \ref{mainThm}. This proof captures the main ideas of the proof of Theorem \ref{geneThm}. Later, in Section \ref{pfthm1}, we show how to formally extend this proof from the ball to all smooth convex bodies with positive curvature in order to obtain Theorem \ref{geneThm}.
 
\section{Proof of Corollary \ref{mainThm}}
%	We now split the proof of Corollary \ref{mainThm} into two main lemmas. The first of these two lemmas follows.
%	\begin{lemma}{\label{a1}}
%		Let $\epsilon > c(n)N^{-1}$. Then the following concentration inequality holds:
%		\[
%			\Pr(||P_{n,N} \setminus B_n| - \E[|P_{n,N}\setminus B_n|]| \geq \epsilon) \leq  e^{-c(n)N^{1-\frac{2}{n-1}}\ln(N)^{-2(1-\frac{1}{n-1})}}
%		\]
%	\end{lemma}
%	As a first step in the proof of Corollary \ref{mainThm}, we show that when $ N $ is large enough,  with a probability of $ 1-e^{-c_1(n)N} $ the polytope is inscribed in $ (1+\frac{2}{n})B_n.$
%To show this inclusion, we prove the following lemma:
%\vspace{2mm}
%		\subsubsection{Proof of Lemma \ref{a1}}
We first prove the corollary for the uniform measure on the sphere, i.e., $\mu := \sigma$. After this, we extend the proof to an arbitrary density at the end of this section. %Then, in Subsection **** we extend it to an arbitrary density. 

\vskip .1in

By the definitions of $t_{n,N}$ and $P_{n,N}$, we have $t_{n,N}B_n\subset P_{n,N}$. Using this and Fubini's theorem, we express the volume of the symmetric difference $P_{n,N} \triangle B_n$ as
\begin{equation}\label{Eq:refine}
\begin{aligned}|P_{n,N}\triangle B_n| & =\int_{\R^{n}\setminus B_n}\mathbbm{1}_{\{\max\langle X_{i},x\rangle\leq t_{n,N}\}}(x)\,dx+\int_{B_n\setminus t_{n,N}B_n}(1-\mathbbm{1}_{\{\max\langle X_{i},x\rangle\leq t_{n,N}\}}(x))\,dx\\
& =|\partial B_n|\bigg(\int_{\mathbb{S}^{n-1}}\int_{1}^{\infty}r^{n-1}\mathbbm{1}_{\{\max\langle X_{i},x\rangle\leq r^{-1}t_{n,N}\}}(x)\,dr\,d\sigma(x)\\
& +\int_{\mathbb{S}^{n-1}}\int_{t_{n,N}}^{1}r^{n-1}(1-\mathbbm{1}_{\{\max\langle X_{i},x\rangle\leq r^{-1}t_{n,N}\}}(x))\,dr\,d\sigma(x)\bigg)\\
%& =|\partial B_n|\int_{1}^{\infty}r^{n-1}\int_{\mathbb{S}^{n-1}}\mathbbm{1}_{\{\max\langle X_{i},x\rangle\leq r^{-1}t_{n,N}\}}d\sigma(x)\,dr\\
& =|\partial B_n|\bigg(\int_{t_{n,N}}^{1}r^{n-1}\int_{\Sp}(1-\mathbbm{1}_{\{\max\langle X_{i},x\rangle\leq r^{-1}t_{n,N}\}}(x))\,d\sigma(x)\,dr\\
& +\int_{1}^{1+c_{n,N}}r^{n-1}\int_{\mathbb{S}^{n-1}}\mathbbm{1}_{\{\max\langle X_{i},x\rangle\leq r^{-1}t_{n,N}\}}(x)\,d\sigma(x)\,dr\\
& +\int_{1+c_{n,N}}^{1+\frac{2}{n}}r^{n-1}\int_{\mathbb{S}^{n-1}}\mathbbm{1}_{\{\max\langle X_{i},x\rangle\leq r^{-1}t_{n,N}\}}(x)\,d\sigma(x)\,dr\\
& +\int_{1+\frac{2}{n}}^{\infty}r^{n-1}\int_{\mathbb{S}^{n-1}}\mathbbm{1}_{\{\max\langle X_{i},x\rangle\leq r^{-1}t_{n,N}\}}(x)\,d\sigma(x)\,dr\bigg)\\
& =Y+Z+W.
\end{aligned}
\end{equation}
Here $ c_{n,N}:=c(n)\NN(\ln N)^{\frac{2}{n-1}} $ and $c(n)$ will be defined at the end of Subsection \ref{Znegligiblesection}, and  
\begin{equation}\label{Eq:refine2}
\begin{aligned}Y & :=|\partial B_n|\bigg(\int_{1}^{1+c_{n,N}}r^{n-1}\int_{\mathbb{S}^{n-1}}\mathbbm{1}_{\{\max\langle X_{i},x\rangle\leq r^{-1}t_{n,N}\}}(x)\,d\sigma(x)\,dr\\
& +\int_{t_{n,N}}^{1}r^{n-1}\int_{\mathbb{S}^{n-1}}(1-\mathbbm{1}_{\{\max\langle X_{i},x\rangle\leq r^{-1}t_{n,N}\}}(x))\,d\sigma(x)\,dr\bigg) \\%= |\partial B_n|(Y_1+Y_2)\\
Z & :=|\partial B_n|\int_{1+c_{n,N}}^{1+\frac{2}{n}}r^{n-1}\int_{\mathbb{S}^{n-1}}\mathbbm{1}_{\{\max\langle X_{i},x\rangle\leq r^{-1}t_{n,N}\}}(x)\,d\sigma(x)\,dr\\
W & :=|\partial B_n|\int_{1+\frac{2}{n}}^{\infty}r^{n-1}\int_{\mathbb{S}^{n-1}}\mathbbm{1}_{\{\max\langle X_{i},x\rangle\leq r^{-1}t_{n,N}\}}(x)\,d\sigma(x)\,dr
=|P_{n,N}\cap((1+\tfrac{2}{n})B_n)^{c}|.
\end{aligned}
\end{equation}

%The main idea of the proof is to derive concentration inequalities for each of the random variables $Y$, $Z$ and $W$, and then to apply a union bound to derive a concentration inequality for the sum $|B_n\setminus P_{n,N}| = Y+Z+W$. 
%In high level description, we  
We split the proof of Corollary \ref{mainThm} into three lemmas, one for each random variable $Y$, $Z$ and $W$. First, we apply McDiarmid's inequality %, in a nontrivial way, 
to  derive a concentration inequality for $Y$. For $Z$, we apply %a ``balls and bins" argument 
a ``pigeonhole principle" to certain  coverings of the sphere to show that $Z$ is negligible with high probability. Finally, we use a standard sphere covering argument to show that $W=0$ with extremely high probability. In the final analysis, we condition on the event that $Z+W$ is extremely small to derive the desired concentration inequality for $| P_{n,N} \triangle B_n|$. 

%%%%

\subsection{Concentration for the random variable $Y$}

\begin{lemma}\label{Yprop}
	Let $\epsilon>0$. There is a constant $c_1(n)>0$ such that for all sufficiently large $N$,
	\[
	\Pr(|Y-\E[Y]| \geq \epsilon) \leq
	\exp\left(-c_1(n)N^{1+\frac{4}{n-1}}f(N)\epsilon^2\right)
	\]
	where $f(N)=(\ln N)^{-(2+\frac{4}{n-1})}$.
\end{lemma}

\begin{proof}%[Proof of Lemma \ref{Yprop}]
	%The next step in the proof of Corollary \ref{mainThm} is to 
	%Our next step is to derive a concentration inequality for the random variable $Y$. We will handle the first integral $Y_1$ in $ Y $, and the second integral $Y_2$ is handled in a similar way. 
	%Observe that by definition, $Y_1 %:= |\partial B_n| \int_{1}^{1+c_{n,N}}r^{n-1}\int_{\mathbb{S}^{n-1}}\mathbbm{1}_{\{\max\langle X_{i},x\rangle\leq r^{-1}t_{n,N}\}}d\sigma(x)\,dr
	%=|P_{n,N}\cap (1+c_{n,N})B_n \cap B_n^c|.$ 
     We aim to apply McDiarmid's inequality to the  random variables
	\begin{align*}
	Y_1(X_1,\dots,X_N) &:=|P_{n,N}^c\cap(1+c_{n,N})B_n\cap B_n^c| \\%\text { and } 
	Y_2(X_1,\dots,X_N) &:=|B_n \setminus P_{n,N} |,
	\end{align*}
	where $Y = \left(|(1+c_{n,N})B_n\cap B_n^c| - Y_1\right) +Y_2$. For all $1\leq i\leq N$, we have %Now %, ($Y_1$ is the volume that is removed from $ (1+c_{n,N})B_n \setminus B_n  $ so that). Now,
% 	 
% 	\[
% 	Y_1:=|(1+c_{n,N})B_n\setminus B_n| - f(X_1,\ldots,X_N).
% 	\]
	% by $ |(1+c_{n,N})B_n| - f(X_1,\ldots,X_N), $ where $ f(X_1,\ldots,X_N) $ measures the volume that we removed from $ (1+c_{n,N})B_n \setminus B_n  $. 
	\begin{align*}
	c_i:&=\sup_{x_1,\ldots,x_N,x'_i}|Y_1(x_1,\ldots,x_i,\ldots ,x_N) - Y_1(x_1,\ldots,x'_i,\ldots x_N)| \\
	&\leq \sup_{x_1,\ldots,x_N,x'_i} |(x_i^\perp)^{-}\cap (1+c_{n,N})B_n|,
	\end{align*}
	where $x_i^\perp$ is the hyperplane orthogonal to $x_i$ that contains $x_i$, $(x_i^\perp)^{-}$ is the halfspace of $x_i^\perp$ that does not contain the origin, and $(x_i^\perp)^{-}\cap (1+c_{n,N})B_n$ is the cap of $(1+c_{n,N})B_n$ with height $1+c_{n,N}-t_{n,N}$ and base $x_i^\perp\cap(1+c_{n,N})B_n$.
	We can bound the volume of each cap by %it by $ C(n)N^{-(1+\frac{2}{n-1})}\log(N)^{1+\frac{2}{n-1}} $. 
	\begin{align*}|(x_{i}^{\perp})^{-}\cap(1+c_{n,N})B_n| & =|B_{n-1}|\int_{t_{n,N}}^{1+c_{n,N}}\left((1+c_{n,N})^{2}-x^{2}\right)^{\frac{n-1}{2}}\,dx\\
	& \leq2^{\frac{n-1}{2}}(1+c_{n,N})^{\frac{n-1}{2}}|B_{n-1}|\int_{t_{n,N}}^{1+c_{n,N}}(1+c_{n,N}-x)^{\frac{n-1}{2}}\,dx\\
%	& = c(n)\left[(1+c_{n,N}-x)^{\frac{n+1}{2}}\right]_{1+c_{n,N}}^{t_{n,N}}\\
	&=c(n)(1+c_{n,N}-t_{n,N})^{\frac{n+1}{2}}\\
	%& \leq C(n)(1+c_{n,N}-t_{n,N}(1+c_{n,N})^{-1})^{\frac{n+1}{2}}\\
	&\leq c_{1}(n)N^{-(1+\frac{2}{n-1})}(\ln N)^{1+\frac{2}{n-1}}.
	\end{align*}
	The last inequality follows since
	\begin{align*}
	1+c_{n,N}-t_{n,N} &\leq 1+c(n)\NN(\ln N)^{\frac{2}{n-1}}-\left[1-\frac{1}{2}\left(1+\frac{c\ln n}{n}\right)\NN\right]\\
	&\leq 2c(n)\NN(\ln N)^{\frac{2}{n-1}}.
	%&\leq c_1(n)\NN(\ln N)^{\frac{2}{n-1}}.
	\end{align*}
	%In the last two inequalities we used the definition of $c_{n,N}$ and
	%\begin{align*}
	%t_{n,N}(1+c_{n,N})^{-1}&= \left[1-\frac{1}{2}\left(1+\frac{\ln(n)}{n}+O\bigg(\big(n^{-1}\ln(n)\big)^2\bigg)\right)\NN\right]\times(1+c_{n,N})^{-1} \\
	%&\geq \left(1-\frac{1}{2}c_{n,N}\right)(1-c_{n,N}) > 1-\frac{3}{2}c_{n,N}.
	%\end{align*} 
	Thus, from McDiarmid's inequality it follows that for any $\epsilon > 0$,
	\begin{align}\label{Yconcentration}
	\Pr(|Y_1-\E[Y_1]| \geq \epsilon|B_n|) &\leq 2\exp\left(-\frac{2\epsilon^2}{4 N c(n)^2 N^{-(2+\frac{4}{n-1})}(\ln N)^{2+\frac{4}{n-1}}}\right) \nonumber\\
	%2\exp\left(-\frac{2\epsilon^2}{N C(n)N^{-(2+\frac{4}{n-1})}\ln(N)^{2+\frac{4}{n-1}}}\right) \nonumber\\
	&= 2\exp\left(-c_1(n)N^{1+\frac{4}{n-1}}f(N)\epsilon^2\right)
	\end{align}where $f(N)=(\ln N)^{-(2+\frac{4}{n-1})}$.
	The second variable $Y_2$   
% 	\[
% 	Y_2:=\int_{t_{n,N}}^{1}r^{n-1}\int_{\mathbb{S}^{n-1}}(1-\mathbbm{1}_{\{\max\langle X_{i},x\rangle\leq r^{-1}t_{n,N}\}}(x))\,d\sigma(x)\,dr 
% 	\]
	is handled in a similar way, and it can be derived that 
	\begin{equation}\label{concY2}
	\Pr(|Y_2-\E[Y_2]| \geq \epsilon |B_n|) \leq 2\exp\left(-c_2N^{1+\frac{4}{n-1}}\epsilon^2\right).
	\end{equation}
	Finally, we apply a union bound and use Eqs. (\ref{Yconcentration}) and (\ref{concY2}) to derive the desired concentration inequality for $Y$.
% 	\begin{align*}
% 	\Pr(|Y-\E[Y]| \geq \epsilon) %&= \Pr(\big| |\partial B_n|(Y_1+Y_2) - |\partial B_n|(\E[Y_1]+\E[Y_2])\big| \geq \epsilon)\\
% 	&= \Pr \left(\big| |Y_1-\E[Y_1]|+|Y_2-\E[Y_2]|\big| \geq \epsilon/2 \cdot |B_n|\right)\\
% 	&\leq \Pr \left(|Y_1-\E[Y_1]|\geq \epsilon/2 \cdot |B_n|\right)+\Pr\left(|Y_2-\E[Y_2]| \geq \epsilon/2 \cdot |B_n|\right) \\
% 	&\leq 2\exp\left(-c_3(n)N^{1+\frac{4}{n-1}}f(N)\epsilon^2\right) + 2\exp\left(-c_4N^{1+\frac{4}{n-1}}\epsilon^2\right).
% 	\end{align*}
\end{proof}

%%%%

\subsection{The random variable $Z$ is negligible}\label{Znegligiblesection}

First, we mention that McDiarmid's inequality does not provide the desired concentration inequality for $ Z $ because the volume of each geodesic ball is too big. Furthermore, despite the fact that the expectation of $Z$ is extremely small, other standard concentration inequalities  do not yield the desired inequality for $Z$. We  instead use a direct geometric and combinatorial argument to show that $ Z $ is negligible with high probability. %More specifically, we show that with probability  at least $ 1-\exp(-c_2(n)N^{-(0.5-\frac{2}{n-1})}\ln N) $, the random variable $Z$ is bounded from above by a negligible factor.

\begin{lemma}\label{Zprop}
	For all sufficiently large $N$,
	\[\Pr\left(Z \geq N^{-(0.5+\frac{2}{n-1})}\right) \leq \exp\left(-c_2(n)N^{0.5-\frac{2}{n-1}}\ln N\right).\]
\end{lemma}
\begin{proof}%[Proof of Lemma \ref{Zprop}]
	Recall that $t_{n,N}:=\sqrt{1-\left(\frac{(\ln 2)|\partial B_n|}{N|B_{n-1}|}\right)^{\frac{2}{n-1}}}$. Since $t_{n,N}<1$,  for all $x\in\Sp$ and all $r> 0$ it holds that 
	\[
	\mathbbm{1}_{\max\langle X_i,x\rangle \leq r^{-1}t_{n,N}} \leq \mathbbm{1}_{\max\langle X_i,x\rangle \leq r^{-1}}.
	\]
	Therefore, 
	\begin{align*} Z &\leq |\partial B_n|\int_{1+c_{n,N}}^{1+\frac{2}{n}}r^{n-1}\int_{\mathbb{S}^{n-1}}\mathbbm{1}_{\{\max\langle X_{i},x\rangle\leq r^{-1}\}}(x)\,d\sigma(x)\,dr\\
	& \leq|\partial B_n|\int_{1+c_{n,N}}^{1+\frac{2}{n}}r^{n-1}\int_{\mathbb{S}^{n-1}}\mathbbm{1}_{\{\max\langle X_{i},x\rangle\leq(1+c_{n,N})^{-1}\}}(x)\,d\sigma(x)\,dr\\
	& =|\partial B_n|\left[\frac{r^{n}}{n}\right]_{1+c_{n,N}}^{1+\frac{2}{n}}\times\int_{\Sp}\mathbbm{1}_{\{\max\langle X_{i},x\rangle\leq(1+c_{n,N})^{-1}\}}(x)\,d\sigma(x)\\
	& \leq3|B_n|\int_{\Sp}\mathbbm{1}_{\{\max\langle X_{i},x\rangle\leq(1+c_{n,N})^{-1}\}}(x)\,d\sigma(x)=\frac{3}{n}\tilde{Z},
	\end{align*}
	where $\tilde Z:=|\partial B_n|\int_{\mathbb{S}^{n-1}}\mathbbm{1}_{\{\max\langle X_{i},x\rangle\leq(1+c_{n,N})^{-1}\}}(x)\,d\sigma(x)$. %Next, we will bound $ \tilde Z.$ 
	The random variable $ \tilde Z $ has a geometric meaning: it measures the missing surface area of a random sphere ``covering" with $ N $ random geodesic balls of measure $ c(n)\frac{\ln N}{N}$. Observe that the base of each spherical  cap of the ``covering" 
	\[
	\textstyle \Sp \cap \left(\bigcap_{i=1}^N \{x\in\R^n: \langle X_i,x\rangle \leq (1+c_{n,N})^{-1}\}\right)^c
	\]
	has radius $\sqrt{1-(1+c_{n,N})^{-2}}>  \sqrt{c_{n,N}}$, where we used the inequality $(1+x)^{-2} < 1-x$ for $x\in(0, (\sqrt{5}-1)/2)$. 
	
	To get some intuition, we first show that the expectation of $ \tilde Z $ is extremely small. By  independence and Lemma \ref{stirling},%$ \E[\tilde Z] = c(n)N^{-1}.$ 
	\begin{align*}
	\E[\tilde Z] &=\int_{\Sp} \E[\mathbbm{1}_{\max\langle X_i,x\rangle \leq (1+c_{n,N})^{-1}}(x)]\,d\sigma(x)\\
	&=\int_{\Sp} \prod_{i=1}^N\Pr(\langle X_i,x\rangle \leq (1+c_{n,N})^{-1})\,d\sigma(x)\\
	&<\left(1-\frac{c_{n,N}^{\frac{n-1}{2}}|B_{n-1}|}{|\partial B_n|}\right)^N
	\leq \left(1-\frac{c(n)^{\frac{n-1}{2}}\ln N}{\sqrt{2\pi n}N}\right)^N\\
	&\leq \exp\left(-\frac{c(n)^{\frac{n-1}{2}}}{\sqrt{2\pi n}}\cdot\ln N\right)=N^{-c(n)^{\frac{n-1}{2}}/\sqrt{2\pi n}},
	\end{align*}where  we used the elementary inequality $(1-x)^N \leq e^{-Nx}$ for $x\in(0,1)$. 
	At the end of the proof of the lemma, we will choose $ c(n) $ to be large enough so that this expectation is negligible.
	
	Recall that our goal is to show that with high probability, $ \tilde{Z} $ is negligible. We can reduce the problem to the following  random sphere ``covering" of $ c(n)N\ln N$ random geodesic balls with volume $ N^{-1}.$	Let $ \mathbf{X} := \{X_1,\ldots, X_{c(n)N\ln N}\} $ and define 
	\[
	A_{\mathbf X} :=\bigcup_{i=1}^{c(n)N\ln N} B_{G}(X_i,N^{-1})
	\]
	to be the random ``covering" generated by $\mathbf{X}$. We want to estimate the probability of the  event 
	\[
	B :=\{ |A^c_{\mathbf X}| = \tilde Z > |\partial B_n|N^{-(0.5+\frac{2}{n-1})}\}.
	\]
	To do so, we will need a sphere covering such that each point of the sphere is not counted ``too many" times, i.e., each point must not belong to ``too many" balls in the covering. A remarkable result of B\"or\"oczky and Wintsche \cite[Theorem 1.1]{boroczky2003covering} provides such a covering,  showing that there exists a $ \frac{1}{2}N^{-\frac{1}{n-1}}$-net  
	$\mathcal{N}$ of size $400 n\ln n\cdot 2^n N$ %such that $\{B_G(y_i,\tfrac{1}{2}N^{-\frac{1}{n-1}}): 1 \leq i \leq Cn\ln(n)\cdot 2^n N \}$
	such that each point $x\in\Sn$ lies inside of at most $ 400 n\ln n$ caps. %***$C=400$, no?***%***technically, the finite point set $\{y_1,\ldots, y_{Cn\ln(n)\cdot 2^n N}\}$ is actually the net, not the union of the geodesic balls...we need to fix the language here...**** Please note that $B_G(y,\tfrac{1}{2}\NNN)$ denotes the geodesic ball centered at $y$ with radius $\tfrac{1}{2}\NNN$.
	
	Next, we use the covering $\mathcal N$ to define the random  set % $ S_{\max} $ denote the random set that consists of the points $ y\in\mathcal{N} $ that satisfy 
	\[S_{\max} := \left\{y\in\mathcal N: \, d_G(X_i,y) \geq \tfrac{1}{2}N^{-\frac{1}{n-1}} , \, \forall 1 \leq i\leq c(n)N\ln N \right\}.\] 
	Observe that if $ y \notin S_{\max} $, then by the triangle inequality $ B_G(y, 2^{-(n-1)}N^{-1}) \subset A_{\mathbf X}$, and hence
	\[
	A_{\mathcal N\setminus S_{\max}} := \bigcup_{y\in\mathcal N\setminus S_{\max}} B_G(y, 2^{-(n-1)}N^{-1}) 
	\subset A_{\mathbf X}.
	\] 
	We claim that this inclusion implies  $ |S_{\max}| \geq 2^{n-1}N^{0.5-\frac{2}{n-1}}$ holds under the event $ B.$ To see this, we prove the contrapositive statement and assume that $|S_{\max}| < 2^{n-1}N^{0.5-\frac{2}{n-1}}$. Then 
	\begin{align*} 
	|A^c_{\mathbf X}| \leq |A^c_{\mathcal N\setminus S_{\max}}|
	\leq |A_{S_{\max}}| 
	&= \big|\bigcup_{ y \in S_{\max}}B_G(y,2^{-(n-1)}N^{-1})\big| \\
	&= |S_{\max}| \cdot 2^{-(n-1)}N^{-1}|\partial B_n| \\
	&< N^{-(0.5+\frac{2}{n-1})}|\partial B_n|,
	\end{align*} 
	so the complementary event $B^c$ holds. %Now clearly the following holds: *****avoid the word ``clearly" or ``obviously" at all costs in math writing as it tacitly assumes the reader's knowledge, which we do not know, and also is probably the main source of errors in papers*****
	Therefore,
	\begin{align}\label{starineq}
	\Pr_{\mathbf X}(B) &\leq \Pr_{\mathbf X}(|S_{\max}| \geq   2^{n-1}N^{0.5-\frac{2}{n-1}}) \nonumber
	\\&= \Pr_{\mathbf X}(\exists S \subset S_{\max}: \, |S| = 2^{n-1}N^{0.5-\frac{2}{n-1}}).
	%	:\\&  &\stackrel{(\star)}{\leq}
	\end{align}
Since each point of the sphere lies inside of at most $400 n\ln n$ balls of the covering $\mathcal{N}$,
	\begin{align}\label{Asmaxlower}
	|A_{S_{\max}}| = \int_{\Sn}\mathbbm{1}_{A_{S_{\max}}}(x)\,dS(x) %\\
	&\geq \frac{1}{400 n\ln n}\int_{\Sn}\sum_{y\in S_{\max}}\mathbbm{1}_{B_G(y,2^{-(n-1)}N^{-1})}(x)\,dS(x) \nonumber \\
	%&\geq \frac{1}{Cn\ln(n)}\int_{\Sn}\sum_{i=1}^{2^nN^{0.5-\frac{2}{n-1}}}\mathbbm{1}_{B_G(x_i,2^{-(n-1)}N^{-1})}(x)\,dS(x)  \\
	&=\frac{1}{400 n\ln n}|S_{\max}|\cdot|B_G(y,2^{-(n-1)}N^{-1})| \nonumber\\
	&\geq\frac{2^nN^{-(0.5+\frac{2}{n-1})}}{400 n\ln n}|\partial B_n|.
	\end{align} 
	Thus, by (\ref{starineq}), (\ref{Asmaxlower}), a union bound and independence, we conclude that
	\begin{equation}\label{Zfinal}
	\begin{aligned}
	\Pr_{\mathbf X}(B) &\leq \Pr_{\mathbf X}(\exists S\subset S_{\max}: |S|=2^{n-1}N^{0.5-\frac{2}{n-1}}) \\%&\leq \bigcup_{S\subset S_{\max}\\ |S|=2^{n-1}N^{0.5-\frac{2}{n-1}}}\\
	&\leq \binom{400 n\ln n\cdot2^nN}{ 2^nN^{0.5-\frac{2}{n-1}}}\cdot\Pr_{\mathbf X}(X_{1},\ldots, X_{c(n)N\ln N}\notin A_{S_{\max}})
	\\& \leq \exp\left(c_2(n)(\ln N)N^{0.5-\frac{2}{n-1}}\right)\cdot\left(1-\frac{2^nN^{-(0.5+\frac{2}{n-1})}}{400 n\ln n}\right)^{c(n)N\ln N} 
	\\&\leq \exp\left(c_2(n)(\ln N)N^{0.5-\frac{2}{n-1}}-c(n)c_1(n)(\ln N)N^{0.5-\frac{2}{n-1}}\right).
	\end{aligned}
	\end{equation}
	Choosing $ c(n) $  large enough yields the lemma.
\end{proof}

%******************What is wrong with the following argument? 
%
%Since $Z \leq 3n^{-1} \tilde Z \leq \tilde Z$, by Markov's inequality $\Pr(X\geq a) \leq a^{-1}\E[X]$ we have
%\begin{align*}
%\Pr(Z \geq N^{-(0.5+\frac{2}{n-1})} ) %&\leq \Pr(\tilde Z \geq N^{-(0.5+\frac{2}{n-1})}) \\
%&\leq  N^{0.5+\frac{2}{n-1}}\E[ Z]\\
%&\leq  3n^{-1}N^{0.5+\frac{2}{n-1}}\E[\tilde Z]\\
%&\leq 3n^{-1}N^{0.5+\frac{2}{n-1}} N^{-c(n)^{\frac{n-1}{2}}/\sqrt{2\pi n}}\\
%&= 3n^{-1}N^{-c(n)^{\frac{n-1}{2}}/\sqrt{2\pi n}+0.5+\frac{2}{n-1}}
%\end{align*}
%Choosing $c(n)$ to be large enough yields the lemma. (and we can do the same for general $K$?)******


%Choose $N^{0.9}$ random points $Y_1,\ldots,Y_{N^{0.9}}$ from $\Sp$ independently and identically distributed with respect to $\sigma$, and define the random variable 
%\[
% M :=\sum_{i=1}^{N^{0.9}}\mathbbm{1}_{Y_i\in A^c_{\bf X}}.
%\] By the definition of $B$ and linearity of the expectation, it follows that $\E_{\bf X,\bf Y}[M|B] \geq  N^{0.4-\frac{2}{n-1}}.$ 
%
%Using a standard Bernoulli trials argument and the independence of $ \bf X , \bf Y $, we derive
%\[
%\Pr(M > N^{0.3}|B) \geq 1 - e^{-c(n)N^{0.4-\frac{2}{n-1}}}.
%\]
%
%%We now use an inequality of Chernoff for Bernoulli trials: If $X_1,\ldots,X_m$ are i.i.d. Bernoulli random variables and $X=\sum_{i=1}^m X_i$, then $\Pr(X> \E[X] - \epsilon) > 1-\exp(-\epsilon^2/(2\E[X]))$. From this we derive
%%\begin{align*}
%	%\Pr(M > N^{0.3}|B) &= \Pr(M>N^{0.4-\frac{2}{n-1}}(1-(1-N^{-0.1}))|B) \\
%%&\geq \Pr(M>\E[M|B]-N^{0.4-\frac{2}{n-1}}(1-N^{-0.1}))|B) \\
%%&> 1-\exp\left(-\frac{N^{0.8-\frac{4}{n-1}}(1-N^{-0.1})^2}{2\E[M|B]}\right)
%%\geq 1-\exp\left(-\frac{1}{4}N^{0.4-\frac{2}{n-1}}\right).
%%1 - e^{-c(n)N^{0.4-\frac{2}{n-1}}}.
%%\end{align*}
%Now we want to estimate how much surface area is captured by $ N^{0.3} $ geodesic balls of volume $ |\partial B_n|N^{-1}.$ For this purpose we will estimate the probability that we get a separated set, which also implies that $ |A_{Y_1,\ldots ,Y_{N^{0.3}}}| = N^{-0.7}.$ By independence
%Using a similar argument as in the proof of Theorem \ref{Thm1} (essentially it's repeating the proof with $ N^{0.3} $ balls instead of $ N $ balls), we derive that
%\begin{align*}
%	\Pr(|A_{Y_1,\ldots ,Y_{N^{0.3}}} -\E[A_{Y_1,\ldots ,Y_{N^{0.3}}}]| > \epsilon|\partial B_n|) &= \Pr(|A_{Y_1,\ldots ,Y_{N^{0.3}}} -N^{-0.7}| > 2\epsilon|\partial B_n|)  \\& \leq e^{-cN^{1.7}\epsilon^2}.
%\end{align*}
%%\begin{align*}
%%\Pr(\forall \ 1\leq i\ne j\leq N \  d(Y_i,Y_j) \geq N^{-\frac{1}{n-1}}) &\geq \Pi_{i=1}^{N^{0.3}}(1-i\cdot 2^{n-1}N^{-1}) \\& \geq (1-2^{n-1}N^{-0.7})^{N^{0.3}}&\\\geq 1- e^{-c_1(n)N^{-0.4}},
%%\end{align*}
%Thus
%\begin{equation}{\label{volume}}
%\Pr(|A_{Y_1,\ldots ,Y_{N^{0.3}}}| \geq c_1(n)N^{-0.7}) \geq 1-e^{-c_1(n)N^{0.4}}. 
%\end{equation}
%Denote the event from \eqref{volume} by $ C.$   Finally using the independence of $ \bf Y,\bf  X$
%\begin{align*}\Pr_{{\bf X}}(B) & \leq\Pr_{{\bf Y,{\bf X}}}(\exists|S|\geq N^{0.3}s.t.Y_{i}\notin A_{{\bf X}}\forall i\in S)+e^{-c(n)N^{0.4-\frac{2}{n-1}}}\\
%& \leq\binom{N^{0.9}}{N^{0.3}}\Pr_{{\bf Y,{\bf X}}}(Y_{1}\ldots Y_{N^{0.3}}\notin A_{{\bf X}})+e^{-c(n)N^{0.4-\frac{2}{n-1}}}
%\end{align*}
%Now we can reverse the rolls, i.e. if $ Y_1 \notin A_{{\bf X}} $ it implies that $ d_G(Y_i,X_i) > N^{-\frac{1}{n-1}}$ for all $ 1\leq i \leq N^{0.3}, $ thus $ X_1,\ldots X_{c(n)N\ln(N)} \notin A_{Y_{1,\ldots,N^{0.3}}}.$ Hence, 
%\begin{align*} & =\binom{N^{0.9}}{N^{0.3}}\Pr_{{\bf Y,{\bf X}}}(X_{1}\ldots X_{c(n)N\ln N}\notin A_{Y_{1,\ldots,N^{0.3}}})+e^{-c(n)N^{0.4-\frac{2}{n-1}}}\\
%& \leq\binom{N^{0.9}}{N^{0.3}}\big(\Pr_{{\bf Y}}(\Pr_{{\bf X}|{\bf Y}}(X_{1}\ldots X_{c(n)N\ln N}\notin A_{Y_{1,\ldots,N^{0.3}}})|C)\Pr_{{\bf Y}}(C)+\Pr_{{\bf Y}}(C^{c})\big)+e^{-c(n)N^{0.4-\frac{2}{n-1}}}\\
%& \leq\binom{N^{0.9}}{N^{0.3}}\Pr_{{\bf Y}}(\Pr_{{\bf X}|{\bf Y}}(X_{1}\ldots X_{c(n)N\ln N}\notin A_{Y_{1,\ldots,N^{0.3}}})|C)+2e^{-c(n)N^{0.4-\frac{2}{n-1}}}\\
%& =\binom{N^{0.9}}{N^{0.3}}\Pr_{{\bf Y}}(\Pr_{{\bf X}}(X_{1}\ldots X_{c(n)N\ln N}\notin A_{Y_{1,\ldots,N^{0.3}}})|C)+2e^{-c(n)N^{0.4-\frac{2}{n-1}}}
%\end{align*}
%Finally by \eqref{volume},
%\begin{align*}
%& \leq\binom{N^{0.9}}{N^{0.3}}(1-N^{-0.7})^{c(n)N\ln N}+2e^{-c(n)N^{0.4-\frac{2}{n-1}}}\\
%& \leq e^{-c(n)N^{0,3}\ln(N)+0.6N^{0,3}\ln(N)}+2e^{-c(n)N^{0.4-\frac{2}{n-1}}}
%\end{align*}
%\E[Z_A] = \sum_{i=1}^{N^{0.9}}\Pr(Z_i\in A) \geq N^{0.9}N^{-0.5}=N^{0.4},
%\]where in the inequality we used the definition of $A$. By a version of Chernoff's upper bound (see, e.g., \cite{} p. xxx),% Now we will use the following multiplicative form of Chernoff's upper bound:
%%{\it Let $X_1,\dots,X_N$ be independent random variables with $X_i\in[0,1]$, and let $X:=\sum_{i=1}^N X_i$. Then for any $\delta\in(0,1)$, }
%%\[
%%\Pr(X\leq(1-\delta)\E[X]) \leq e^{-\delta^2\E[X]/2}.
%%\]Applying the previous inequality with $X=Z_A$, $X_i=\mathbbm{1}_{Z_i\in A}$ and $\delta=1-N^{-0.1}$, we obtain
%\begin{align*}
%\Pr(Z_A < N^{0.3}) &= \Pr(Z_A < (1-(1-N^{-0.1}))N^{0.4})\\
%&\leq  \Pr(Z_A < (1-(1-N^{-0.1}))\E[Z_A])\\
%&\leq e^{-(1-N^{-0.1})^2 N^{0.4}/2}
%\end{align*}
%and the claim follows.
%Observe that $ \E[\tilde Z]$ is at most $ c(n)N^{-1}|B_n|.$ This follows from 
%\begin{align*}
%|\partial B_n|\E[\tilde{Z}] & = |\partial B_n|\E_{x\sim\sigma}\E_{X_1,\ldots,X_N}[\mathbbm{1}_{\{\max\langle X_{i},x\rangle\leq(1+\ln(N)^{\frac{2}{n-1}}N^{-\frac{2}{n-1}})^{-1}\}}] \\& =|\partial B_n|\left(\frac{1-|B_{n-1}|\ln(N)N^{-1}}{|\partial B_n|}\right)^N \leq c(n)N^{-1}|\partial B_n|,
%\end{align*} 
%where we used Fubini and the independence of the $ X_i $.
%Thus, by Corollary \ref{COVERcor} we obtain  %the claim follows. 
%\begin{align}\label{Zconcentration}
%\Pr(Z > c(n)N^{-1} + \epsilon) &\leq \Pr\left(\frac{3}{n}|\tilde Z -\E[\tilde{Z}]| \geq \epsilon\right) \leq e^{-c_1(n)N^{1-\frac{2}{n-1}}\ln(N)^{-2(1-\frac{1}{n-1})}\epsilon^2}.
%\end{align}

% In this direction, we estimate the integral in $Y$ using a Riemann sum. Partition the interval $ [{1},1+\ln(N)^{\frac{2}{n-1}}N^{-\frac{2}{n-1}}] $ into a collection of subintervals with endpoints $ r_0 = 1,\ldots ,r_i,\ldots, r_M = {1+\ln(N)^{\frac{2}{n-1}}N^{-\frac{2}{n-1}}},$ where $ r_{i+1} - r_{i} = N^{-1}.$   It follows that
%\begin{align*}Y & =|\partial B_n|\int_{\mathbb{S}^{n-1}}\int_{1}^{1+\ln(N)^{\frac{2}{n-1}}N^{-\frac{2}{n-1}}}r^{n-1}\mathbbm{1}_{\{\max\langle X_{i},x\rangle\leq r^{-1}t_{n,N}\}}dr\,d\sigma(x)\\
%& =|\partial B_n|\int_{\mathbb{S}^{n-1}}\sum_{j=0}^{M-1}\int_{r_{j}}^{r_{j+1}}r^{n-1}\mathbbm{1}_{\{\max\langle X_{i},x\rangle\leq r^{-1}t_{n,N}\}}dr\,d\sigma(x)\\
%& \leq|\partial B_n|\int_{\mathbb{S}^{n-1}}\sum_{j=0}^{M-1}\int_{r_{j}}^{r_{j+1}}(r_{j}+N^{-1})^{n-1}\mathbbm{1}_{\{\max\langle X_{i},x\rangle\leq r_{j}^{-1}t_{n,N}\}}dr\,d\sigma(x)\\
%& \leq|\partial B_n|\int_{\mathbb{S}^{n-1}}\sum_{j=0}^{M-1}\int_{r_{j}}^{r_{j+1}}r_{j}^{n-1}\left(1+\frac{cn}{N}\right)\mathbbm{1}_{\{\max\langle X_{i},x\rangle\leq r_{j}^{-1}t_{n,N}\}}dr\,d\sigma(x)\\
%& =\left(1+\frac{cn}{N}\right)\frac{|\partial B_n|}{N}\sum_{j=0}^{M-1}r_{j}^{n-1}\int_{\Sp}\mathbbm{1}_{\{\max\langle X_{i},x\rangle\leq r_{j}^{-1}t_{n,N}\}}d\sigma(x)\\
%& \leq\frac{|\partial B_n|}{N}\sum_{j=0}^{M-1}r_{j}^{n-1}\int_{\Sp}\mathbbm{1}_{\{\max\langle X_{i},x\rangle\leq r_{j}^{-1}t_{n,N}\}}d\sigma(x)\\
%& +\frac{cn|\partial B_n|}{N^{2}}M\left(1+\ln(N)^{\frac{2}{n-1}}N^{-\frac{2}{n-1}}\right)^{n-1}.
%\end{align*}
%Now using the fact that $M\leq N$, we obtain
%\begin{align}\label{riemannsum}
% Y &=\frac{|\partial B_n|}{N}\sum_{j=0}^{M-1}r_{j}^{n-1}\int_{\mathbb{S}^{n-1}}\mathbbm{1}_{\{\max\langle X_{i},x\rangle\leq r_{j}^{-1}t_{n,N}\}}\,d\sigma(x)+O(N^{-1}),
%\end{align}where the implied constant depends on $n$. 
%
%
%	 By Theorem \ref{Thm1}, the random variable $ X_r:= \int_{\mathbb{S}^{n-1}}\mathbbm{1}_{\{\max\langle X_{i},x\rangle\leq r^{-1}t_{n,N}\}}d\sigma(x)$ is tightly concentrated around its mean; more specifically, when $ r\leq 1+\ln(N)^{\frac{2}{n-1}}N^{-\frac{2}{n-1}}$ we know from Corollary \ref{COVERcor} that  
%	\begin{equation}\label{Xr}
%	\Pr( |X_r - \E[X_r]| > \delta) \leq e^{-c(n)N^{1-\frac{2}{n-1}}\ln(N)^{-2(1-\frac{1}{n})}\delta^2}.
%	\end{equation}
% By the triangle inequality and the union bound,
%	\begin{align*}
%\Pr(|Y-\E [Y]| \geq \delta+c/N) 
%&=\Pr\left(\frac{|\partial B_n|}{N}\left|\sum_{j=0}^{M-1} r_j^{n-1}(X_{r_j}-\E [X_{r_j}])\right|\geq \delta\right)\\
%&\leq \Pr\left(\sum_{j=0}^{M-1} r_j^{n-1}|X_{r_j}-\E [X_{r_j}]| \geq \frac{N\delta}{|\partial B_n|}\right)\\
%&\leq \sum_{j=0}^{M-1} \Pr\left(|X_{r_j}-\E [X_{r_j}]| \geq \frac{N\delta}{M r_j^{n-1}|\partial B_n|}\right).
%	\end{align*}
%Since $M\leq N$ and $\min_{0\leq j\leq M}r_j^{n-1}=1$, the previous inequality and (\ref{Xr}) together yield the following concentration inequality for $Y$:
%\begin{align}\label{Yconcentration}
%\Pr(|Y-\E [Y]| \geq \delta+c/N) &\leq  \sum_{j=0}^{M-1} \Pr\left(|X_{r_j}-\E [X_{r_j}]| \geq \frac{\delta}{|\partial B_n|}\right) \nonumber\\
%&\leq Me^{-c(n)N^{1-\frac{2}{n-1}}\ln(N)^{-2}\ln(N)^{-2(1-\frac{1}{n})}\delta^2} \nonumber\\
%&\leq e^{-c_1(n)N^{1-\frac{2}{n-1}}\ln(N)^{-2}\ln(N)^{-2(1-\frac{1}{n})}\delta^2}.
%\end{align}

\subsection{The random variable $W$ equals zero with high probability}

Finally, we turn our attention to the random variable $W=|P_{n,N}\cap ((1+\frac{2}{n})B_n)^c|$. The next lemma implies that $W=0$ with high probability. %for the proof of Corollary \ref{mainThm}, it's enough to restrict our attention to the event in which $ P_{n,N}  \subset (1+\frac{2}{n})B_n. $ More specifically, we  $ W=|P_{n,N}\cap ((1+2/n)B_n)^c|= 0 $  with probability at least $1-e^{-c_1(n)N}.$ %This proposition is proven using Lemma \ref{deltanet} at the end of this section. Now we have all the tools necessary to prove Corollary \ref{mainThm}.


\begin{lemma}{\label{Wconcentration}}
	When $N$ is large enough, the polytope $ P_{n,N} $ lies inside the ball $ (1+\frac{2}{n})B_n $ with probability at least $1-e^{-c_1(n)N}.$
\end{lemma}

%\subsection{Proof of Proposition \ref{Wconcentration}}

%\vspace{1mm}

%Our goal is to show that $P_{n,N}\subset (1+\frac{2}{n})B_n$ holds with probability $1-e^{-c(n)N}$. 
We break the proof of Lemma \ref{Wconcentration} into two steps. First, we show that if $\mathcal N$ is a $\frac{1}{\sqrt{n}}$-net of the sphere, then the inclusion $P_{n,N} \subset (1+\frac{2}{n})B_n$ holds. In the second step, we show that if the points $X_1,\ldots, X_N$ are chosen uniformly and independently from $\Sp$, then the random set $\{X_1,\ldots,X_N\}$ is a $\frac{1}{\sqrt{n}}$-net of $\Sp$ with probability at least $1-e^{-c_1(n)N}$. 

%Observe from the definition of $ W $ that if $ P_{n,N}  \subset (1+\frac{2}{n})B_n,$ then for every $ x\in\Sn $ there exists an $ X_i $ such that $ \langle X_{i},x\rangle\geq r^{-1}t_{n,N}$. Thus it's enough to show that with probability of $1-e^{-c(n)N},$ the points  $ X_i $ are a $ \frac{c}{\sqrt{n}}$-net of the unit sphere. 

%Observe that if $\mathcal N:=\{X_1,\ldots,X_N\}$ is a $c/\sqrt{n}$-net of the sphere, then $\Sp\subset \mathcal N+\frac{c}{\sqrt{n}}B_n$

\begin{lemma}\label{lemma1W}
	Suppose that $\mathcal N=\{X_1,\ldots,X_N\}$ is a $\frac{1}{\sqrt{n}}$-net of $\Sp$. Then $P_{n,N} \subset (1+\frac{2}{n})B_n$.
\end{lemma}

\begin{proof}
	Suppose by way of contradiction that there exists a point $v\in P_{n,N}$ such that $\|v\| \geq 1+\frac{2}{n}$. Without loss of generality, we may assume that  $v = (1+\frac{2}{n})e_1$. We claim that if $v\in P_{n,N}$, then $P_{n,N}$ has no facet $\{\langle X_i,x\rangle = t_{n,N}\}$ with $\langle X_i, e_1\rangle > 1-\frac{1}{n}$. Otherwise, if such a facet exists then there is an index $j$ such that
	\[
	\langle X_j, v\rangle = \langle X_j, e_1\rangle v_1 
	\geq \left(1-\frac{1}{n}\right)\left(1+\frac{2}{n}\right) = 1+\frac{1}{n}-\frac{2}{n^2} > t_{n,N},
	\]which is a contradiction. Thus, none of the outer normals $X_1,\ldots,X_N$ of the facets of $P_{n,N}$ lie in the cap $ \{x\in\Sp: \langle x, e_1\rangle > 1-\frac{1}{n}\}$. Let $r$ denote the radius of this cap. Then for all $n\geq 2$,
	\[
	r= \sqrt{1-\left(1-\frac{1}{n}\right)^2} = \sqrt{\frac{2}{n}-\frac{1}{n^2}} = \frac{\sqrt{2}}{\sqrt{n}}\cdot\sqrt{1-\frac{1}{2n}} \geq \frac{\sqrt{3}}{\sqrt{2n}} > \frac{1}{\sqrt{n}}.
	\]
	This implies that $\mathcal N$ is not a $\frac{1}{\sqrt{n}}$-net, a contradiction.
\end{proof}

\begin{lemma}\label{deltanet}
	Let $X_1,\ldots,X_N \stackrel{\text{i.i.d.}}{\sim} \sigma$. For all sufficiently large $N$, the set $\mathcal N:=\{X_1,\ldots,X_N\}$ is a $ \frac{1}{\sqrt{n}}$-net of the unit sphere with probability at least $ 1-e^{-c_1(n)N}.$ 
\end{lemma}
\begin{proof}
	%Now we return to the proof that $\mathcal N_0$ is a $\frac{c}{\sqrt{n}}$-net. 
	By definition, $\mathcal N$ is a $\frac{1}{\sqrt{n}}$-net of $\Sp$ if and only if for any $x\in\Sp$ there exists $X_i\in\mathcal N$ such that $d_G(x,X_i)\leq \frac{1}{\sqrt{n}}$. We estimate the probability that $\mathcal N$ is not a $\frac{1}{\sqrt{n}}$-net, which holds if and only if  there exists $z\in\Sp$ such that for all $1\leq i\leq N$ we have $d_G(z,X_i)>\frac{1}{\sqrt{n}}$.  By independence and Lemma \ref{stirling}, for any fixed  $ z\in\Sn $ and all sufficiently large $N$ we obtain
	\begin{align}\label{deltanet1}
	\Pr\left(\forall X_i\in\mathcal{N} :\ d_{G}(X_{i},z)>\frac{1}{2\sqrt{n}}\right) & \leq\left(1-\frac{(\frac{1}{2\sqrt{n}})^{n-1}|B_{n-1}|}{|\partial B_n|}\right)^{N} \nonumber\\
	&\leq \left(1-\frac{1}{(\sqrt{\pi/2})^{n-1}n^{n/2}}\right)^N \nonumber\\
	& \leq e^{-c(n)N}.%e^{-c(n)N}.
	\end{align}
	Now using \cite[Cor. 5.5]{aubrun2017alice}, we can find a (deterministic) $ \frac{1}{2\sqrt{n}}$-net of $ \Sn $ of size \\
	$ c_1 n\ln n\cdot\frac{|\partial B_n|}{|C(x,\frac{1}{2\sqrt{n}})|}$; denote it by $ \mathcal{N}_0.$ Therefore, when $N$ is large enough, we can bound the probability that $\mathcal N$ is not a $\frac{1}{\sqrt{n}}$-net by
	\begin{align*}
	\Pr\bigg(\exists x\in\mathbb{S}^{n-1}\,\text{s.t.}\,\forall X_i\in\mathcal N:\ d_{G}(X_{i},x)&>\frac{1}{\sqrt{n}}\bigg) \\
	& \leq\Pr\left(\exists z\in\mathcal{N}_0\,\,\text{s.t.}\,\forall X_i\in\mathcal N:\ d_{G}(z,X_i)>\frac{1}{2\sqrt{n}}\right)\\
	& \leq|\mathcal{N}_0|\Pr\left(\forall X_i\in\mathcal N:\,d_{G}(z,X_{i})>\frac{1}{2\sqrt{n}}\right)\\
	& \leq c_1(n)e^{-N}\\%c_{2}(n)e^{-c(n)N}\\
	& \leq e^{-c_2(n)N}.
	\end{align*}The first inequality follows from the triangle inequality, the second from a union bound and the third from (\ref{deltanet1}). The claim follows.%Therefore, the probability that $\mathcal N$ is a $\frac{c}{\sqrt{n}}$-net is at least $1-e^{-c_1(n)N}$, and the claim follows.
\end{proof}

%%%%%%%%%%%%

\subsection{Concluding Corollary \ref{mainThm}}\label{proofsectionmainthm}

Finally, we put everything together to prove Corollary \ref{mainThm}. Recall that $|P_{n,N} \triangle B_n| = Y+Z+W.$  We  use  the following ingredients:
\begin{enumerate}
	\item[1.] $ \E[|P_{n,N} \triangle B_n|] = c(n)\NN|B_n|$ (see \cite[Theorem 2.1]{kur2017approximation})
	\item[2.] $ \E[Z] \leq \frac{3}{n}\E[\tilde{Z}] = N^{-c(n)^{\frac{n-1}{2}}/\sqrt{2\pi n}} $
	\item[3.] $ \E[W] = 0$ with probability $ 1-e^{c(n)N} $. 
\end{enumerate}	
By  Lemmas  \ref{Zprop} and \ref{Wconcentration} as well as Items 2 and 3,  
\begin{align*}
\Pr(|Z+W| \geq N^{-(0.5+\frac{2}{n-1})}\pm \E[Z]|W = 0) + \Pr(W \ne 0) &\leq\exp\left(-c_1(n)N^{0.5-\frac{2}{n-1}}\right).
\end{align*}
%In order to conclude the proof we use the following that were proved in this paper 
Conditioning on the event $W=0$ and using Lemma \ref{Yconcentration} again, we derive that
\begin{align*}
&\Pr\left(\big||P_{n,N}\triangle B_n| - \E[|P_{n,N}\triangle B_n|]\big| \geq \epsilon \right) \leq
\\&\Pr\left(\left\{\big||P_{n,N}\triangle B_n| - \E[|P_{n,N}\triangle B_n|]\big| \geq \epsilon\right\} \cup \{W \ne 0\} \right) \leq
\\& \Pr\left(\big|Y - \E[Y]\big| \geq \epsilon| W=0 \right) + \Pr\left(|Z| \geq c_1(n)N^{-(0.5+\frac{2}{n-1})} \pm \E[Z] \big| W=0\right) +\Pr(W \ne 0)
\\& \leq 2\exp\left(-c(n)N^{1+\frac{4}{n-1}}f(N)\epsilon^2\right)+\exp\left(-c_1(n)N^{0.5-\frac{2}{n-1}}\right).
\end{align*}
Observe that for $ \epsilon < N^{-(0.5+\frac{2}{n-1})},$ our concentration inequality doesn't give something meaningful. Thus, we can assume that this inequality holds for all $ \epsilon > 0.$
\qed

%%%%%%%%%%%%%%%%%%%%%%%%

\subsection*{Extending to an arbitrary density.}
%\label{proofdensityball}
We now describe how to extend the proof of Corollary \ref{mainThm} from the uniform measure $\sigma$ to any probability measure $\mu$ on $\Sp$ for which there exists a density $g$ such that $d\mu(x) = g(x)\,d\sigma(x)$ and $g(x)>0$ for all $x\in\Sp$. In this case, $|P_{n,N}\triangle B_n| = Y_\mu + Z_\mu + W_\mu$, where 
\begin{align*}Y_{\mu} & :=|\partial B_n|\bigg(\int_{1}^{1+c_{n,\mu,N}}r^{n-1}\int_{\mathbb{S}^{n-1}}\mathbbm{1}_{\{\max\langle X_{i},x\rangle\leq r^{-1}t_{n,N}\}}(x)\,g(x)\,d\sigma(x)\,dr\\
&  +\int_{t_{n,N}}^{1}r^{n-1}\int_{\mathbb{S}^{n-1}}(1-\mathbbm{1}_{\{\max\langle X_{i},x\rangle\leq r^{-1}t_{n,N}\}}(x))\,g(x)\,d\sigma(x)\,dr\bigg)\\ %=|\partial B_n|(Y_{1}+Y_{2})\\
Z_{\mu} & :=|\partial B_n|\int_{1+c_{n,\mu,N}}^{1+\frac{2}{n}}r^{n-1}\int_{\mathbb{S}^{n-1}}\mathbbm{1}_{\{\max\langle X_{i},x\rangle\leq r^{-1}t_{n,N}\}}(x)\,g(x)\,d\sigma(x)\,dr\\
W_{\mu} & :=|\partial B_n|\int_{1+\frac{2}{n}}^{\infty}r^{n-1}\int_{\mathbb{S}^{n-1}}\mathbbm{1}_{\{\max\langle X_{i},x\rangle\leq r^{-1}t_{n,N}\}}(x)\,g(x)\,d\sigma(x)\,dr.%
%
\end{align*}
Here $ c_{n,\mu,N}:=c(n,\mu)\NN(\ln N)^{\frac{2}{n-1}} $ and $ c(n,\mu) $ is a large constant that will be defined later.

Since McDiarmid's inequality holds for any probability measure, it follows that for any $\epsilon>0$,
% we can use the same argument from the proof of Lemma \ref{Yprop} to derive that there is a constant $c(n,g)>0$ such that for any $\epsilon>0$,
\[
\Pr(|Y_\mu-\E[Y_\mu]| \geq \epsilon) \leq
\exp\left(-c_1(n)N^{1+\frac{4}{n-1}}f(N)\epsilon^2\right)
\]
where $f(N)=(\ln N)^{-(2+\frac{4}{n-1})}$. Thus, we only need to discuss how to extend the proofs of Lemmas \ref{Zprop} and \ref{Wconcentration} to the random variables $Z_\mu$ and $W_\mu$, respectively.

Following the proof of Lemma \ref{Zprop},  observe that the only time  the probability measure was used was in  (\ref{Zfinal}). Since
\begin{equation}\label{musigma}
\mu(A_{S_{\max}})=\int_{A_{S_{\max}}} g(x)\,d\sigma(x) \geq \sigma(A_{S_{\max}})\min_{x\in\Sp}g(x),
\end{equation}
we can modify \eqref{Zfinal} to get
\begin{align*}\Pr(B)&\leq\binom{400n\ln n\cdot2^{n}N}{2^{n}N^{0.5-\frac{2}{n-1}}}\Pr_{X_{i}\sim\mu}(X_{1},\ldots,X_{c(n,\mu)N\ln N}\not\in A_{S_{\max}})  \\&=\binom{400n\ln n\cdot2^{n}N}{2^{n}N^{0.5-\frac{2}{n-1}}}\left(1-\mu(A_{S_{\max}})\right)^{c(n,\mu)N\ln N}\\
& \leq\binom{400n\ln n\cdot2^{n}N}{2^{n}N^{0.5-\frac{2}{n-1}}}\left(1-\frac{2^{n}N^{-(0.5+\frac{2}{n-1})}\min_{x\in\Sp}g(x)}{Cn\ln n}\right)^{c(n,\mu)N\ln N}\\
& \leq\exp\left(c_2(n)(\ln N)N^{0.5-\frac{2}{n-1}}-c(n,\mu)c(g,\mu)(\ln N)N^{0.5-\frac{2}{n-1}}\right).
\end{align*}
Thus, choosing $c(n,\mu)$ large enough shows that $Z_\mu$ is negligible with high probability.

Similarly, in the proof of Lemma \ref{Wconcentration} we only used the probability measure once, in (\ref{deltanet1}). Thus, by independence and (\ref{musigma}), for any fixed $z\in\Sp$ and all sufficiently large $N$ we derive
\begin{align*}
\Pr_{X_i\sim\mu}\left(\forall X_i\in\mathcal{N} :\ d_{G}(X_{i},z)>\frac{1}{\sqrt{n}}\right) &\leq \left(1-\mu(B_G(X_i,\tfrac{1}{\sqrt{n}}))\right)^{N} \\
&\leq \left(1-\sigma(B_G(X_i,\tfrac{1}{\sqrt{n}}))\cdot\min_{x\in\Sp}g(x)\right)^N\\
&\leq e^{-c(n)c(g)N},
\end{align*}where $c(g)$ is a positive constant that depends only on $g$. The rest of the proof for $W_\mu$ is similar to that of Lemma \ref{Wconcentration}.

%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%

% Proposition \ref{Zprop} for the probability measure $\mu$.

%Finally, we discuss how to modify the proof of Proposition \ref{Wconcentration} so that it holds for the probability measure $\mu$.****************

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\section{Proof of Theorem *** and Remark ***}
%First, we  prove that the variance of the ball and bins problem ($N$ balls and $N$ bins) is bounded below by $c(n)N^{-1}$. For this purpose, we will apply a local argument. First, we estimate the probability that only $n+1$ points will fall in a spherical cap with center $z$ and measure $N^{-1}$, and that the rest of the points will fall outside the cap with four times the radius; denote this event by $A_z$. Using the fact that the measure is uniform and $X_1,\ldots,X_N$ are i.i.d., we derive that
%\begin{equation}\label{problower1}
%\Pr(A_z) = \binom{N}{n+1}(c^{n}N^{-1})^{n+1} (1-4^{n}N^{-1})^{N-(n+1)} \geq  c(n).
%\end{equation}
%Using a standard packing argument we know that we have $c^n N$ disjoint spherical caps with volume $N^{-1}$; denote their centers by $X_1,\ldots,X_{c^n N}$. By \eqref{problower1},
%\[
%\E\left[\sum_{i=1}^{N}\mathbbm{1}_{A_i}\right] \geq c(n)N.
%\]
%By definition, we know that when $A_i$ holds then the portion from the volume of the spherical cap with center $z_i$ with volume $2^{n-1}N^{-1}$  only depends on $d+1$ points that lay in the center $z_i$ with volume $N^{-1}$ . Since all the caps involved have a volume of $2N^{-1},N^{-1}$ by scaling we know that the variance of this portion is $c_1(d)N^{-2}$. Clearly, the portions in the event $A_i$ and the $A_j$ are independent. Thus, we
%define $\mathcal{F}$ as fixing all the points except the ones when $A_i$ holds. Also, we denote by $V(\mathbb{X})$ the volume of the union of the caps $X_1,\ldots,X_n$.  By conditional variance formula
%\[
%	Var(V(\mathbf{X}) \geq \E[Var(V(\mathbf{X}) | \mathcal{F})] = \E[\sum_{\mathbbm{1}_{A_i}=1}Var(local(Cap))] \geq c(d)N^{-2}\E[\sum_{\mathbbm{1}_{A_i}=1}1] \geq c_1(d)N^{-1}.  
%\]
% 
%Using, roughly the same idea we are going prove Theorem 2, for $K= B_n$ and $\mu$ is the uniform measure on the sphere. Define the same event as in the previous part but now consider
%\[
%	(1+c(d)N^{-\frac{2}{d+1}})B_n \cap P_{n,N}  
%\]
%The same argument will work when $c(d)$ is chosen carefully,i.e. we have $d+1$ caps with volume $N^{-\frac{d+1}{n-1}}$ that moves on a subset of volume $C(d)N^{-\frac{d+1}{n-1}}$. Hence, the variance will be $N^{-(2+\frac{4}{n-1})}$. Now extending to $2B_n \setminus P_{n,N}$ will be also okay. 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Proof of Theorem \ref{deldiv}}
Recall  we aim to show that 
\[
|\divv_{n-1} - (2\pi e)^{-1}(n+\ln n)| =O(1).
\]
First, let us prove that  $\divv_{n-1} \leq (2\pi e)^{-1}(n+\ln n) + c.$  By \cite[Theorem 2.1]{kur2017approximation}, there is a polytope $ P $ with $ N $ facets, all of which have the same height $ t_{n,N} $, such that when $N$ is sufficiently large
\[
|P \triangle B_n| \leq C|B_n|\NN.
%\left(1+e^{-1}+\frac{c}{\sqrt{n}}\right)\NN|B_n|.
\]
Now we inflate the polytope $P$ by a factor of $ t_{n,N}^{-1} $ to get a polytope $\tilde P:=t_{n,N}^{-1}P$ that circumscribes $B_n$ (i.e., $\tilde P\supset B_n$ and each facet of $\tilde P$ touches $\partial B_n$). By the homogeneity of volume,
\begin{align*}
|\tilde{P} \setminus B_n|&=|t_{n,N}^{-1}P| - |B_n|\\
& \leq \left(1+C\NN\right)t_{n,N}^{-n}|B_n|-|B_n|\\
& \leq\frac{1}{2}(n+\ln n+c_{0})|B_n|\NN.
\end{align*}
From this and a result of Gruber \cite[Eq. (4)]{GruberOut}, for all sufficiently large  $N$  we obtain
\begin{align*}
&\phantom{=}\,\,\,\frac{1}{2}\left(1-\frac{1}{n^2}\right)\divv_{n-1}|\partial B_n|^{\frac{n+1}{n-1}}\NN\\
&=\frac{1}{2}\left(n+2\ln n +O\bigg(\frac{(\ln n)^2}{n}\bigg)\right)\divv_{n-1}|B_n|^{\frac{n+1}{n-1}}\NN
\\& \leq |\tilde P\setminus B_n| \leq \frac{1}{2}(n+\ln n+c_0)|B_n|\NN,
\end{align*}which implies that
\begin{align}\label{ballvolume1}
\divv_{n-1} &\leq |B_n|^{-\frac{2}{n-1}}\cdot\frac{n+\ln n+c_0}{n+2\ln n +O\bigg(\frac{(\ln n)^2}{n}\bigg)}\nonumber
\\&=
\frac{n+\ln n+c_0}{n+2\ln n +O\bigg(\frac{(\ln n)^2}{n}\bigg)}\left(\frac{{\pi}^{\frac{n}{2}}}{\Gamma(\frac{n}{2}+1)}\right)^{-\frac{2}{n-1}}.
\end{align}
%We use Lemma \ref{stirlingineq} (Stirling's inequality) to estimate the term involving the volume of the Euclidean ball:
From Lemma \ref{stirlingineq} (Stirling's inequality) we obtain the estimate
\begin{align}\label{ballvolume2}
\left(\frac{{\pi}^{\frac{n}{2}}}{\Gamma(\frac{n}{2}+1)}\right)^{-\frac{2}{n-1}} &= \pi^{-1}(1+O(\tfrac{1}{n}))\left(\Gamma\left(\frac{n}{2}+1\right)\right)^{\frac{2}{n-1}} \nonumber\\
&=\pi^{-1}(1+O(\tfrac{1}{n}))\left(\sqrt{2\pi\cdot\frac{n}{2}}\left(\frac{n}{2}\right)^{\frac{n}{2}}e^{-\frac{n}{2}}\right)^{\frac{2}{n-1}}\nonumber\\
&=(2\pi e)^{-1}(n+2\ln n+O(1)).
\end{align}
Combining (\ref{ballvolume1}) and (\ref{ballvolume2}) yields
\begin{align}\label{divleq}
%\divv_{n-1} \leq (1-n^{-2})^{-1}n^{-\frac{2}{n-1}}|B_n|^{-\frac{2}{n-1}}\left(1+\frac{\ln(n)}{n}+\frac{c_0}{n}\right).
\divv_{n-1} &\leq (2\pi e)^{-1}(n+2\ln n+O(1))\cdot\frac{n+\ln n+c_0}{n+2\ln n +O\bigg(\frac{(\ln n)^2}{n}\bigg)} 
\nonumber\\&= (2\pi e)^{-1}(n+2\ln n+O(1))\left(1-\frac{\ln n+O(1)}{n+2\ln n +O\bigg(\frac{(\ln n)^2}{n}\bigg)}\right)
\nonumber\\& = (2\pi e)^{-1}(n+\ln n)+O(1).
\end{align}

%By Theorem 2 in \cite{MaS2},
%\begin{align}\label{msdel}
%\frac{n-1}{n+1}|B_n|^{-\frac{2}{n-1}} \leq \dell_{n-1}  &\leq \left(1+\frac{c\ln(n)}{n}\right)\frac{n-1}{n+1}|B_{n-1}|^{-\frac{2}{n-1}} \nonumber\\
%& \leq n+c_1\ln(n).
%\end{align}

%Thus, using (\ref{divdel3}), (\ref{msdel}), and the fact that $(1-n^{-2})^{-1}n^{-\frac{2}{n-1}}\leq \frac{n-1}{n+1}$ for all $n\geq 2$, we deduce
%%Thus by \cite{MaS1} and \cite{MaS2}, we get the following:
%\begin{equation}\label{divleqdel}
%	\divv_{n-1} \leq \dell_{n-1}  + \frac{c\ln(n)}{n}
%\end{equation}for some absolute constant $c>1$.

\vspace{2mm}

In the other direction, we show that $\divv_{n-1} \geq (2\pi e)^{-1}(n+\ln n) - c_1$, where $ c_1 > 0  $ is an absolute constant that will be defined later. Suppose that there exists a polytope $P_b \supset B_n$ with $N$ facets such that \[ |P_b| \leq \left(1+\frac{1}{2}(n + \ln n- c_1)\NN\right)|B_n|.\]   Without loss of generality, we may assume that all of the facets of $P_b$ touch the unit ball.
Now  shrink $P_b$ so that its volume equals $ |B_n|$, and  denote the resulting shrunken polytope by $ \hat{P_b}.$ Then $\hat{P_b}$ can be represented as %in the form
\[
\hat{P_b} = \bigcap_{i=1}^{N}\{x\in\R^{n}:\ \inneri{y_i}{x} \leq w_{n,N}\},
\] where $y_1,\dots,y_N$ are the normals of the facets of $\hat{P_b}$ and $w_{n,N} \geq t_{n,N} +\frac{c_1}{2n}\NN$. We express the volume of $B_n\setminus \hat{P_b}$ as
\begin{align}\label{eq:a1}
|B_n \setminus \hat{P_b}| &= \left|\cup_{i=1}^{N}\{x\in\R^{n}:\inneri{y_i}{x} > w_{n,N} \} \cap B_n\right|  \nonumber\\
&\leq N|\{x\in\R^{n}:\inneri{y_1}{x} \in (w_{n,N},1]\} \cap B_n| .
% N|\underbrace{\{x\in\R^{n}:\inneri{y_1}{x} \in (w_{n,N},1]\} \cap B_n|}_{C} .
\end{align}
By the definitions of $\varepsilon_{n,N}$ and $t_{n,N}$,  we have
\[ 
w_{n,N} >1-\frac{1}{2}\left(\frac{|\partial B_n|}{|B_{n-1}|N}\right)^{\frac{2}{n-1}}+\frac{c_1}{2n}\NN
=1-\frac{1}{2}\varepsilon_{n,N}^2+\frac{c_1}{2n}\NN.
\]
Hence, we can estimate the volume of each cap as% follows:
\begin{align}{\label{eq:a2}}
|\{x\in\R^{n}:\inneri{y_1}{x} \in (w_{n,N},1]\} \cap B_n|
&=|B_{n-1}|\int_{w_{n,N}}^{1}\left(1-x^{2}\right)^{\frac{n-1}{2}}dx \nonumber\\
%& \leq|B_{n-1}|\int_{1-\frac{1}{2}\varepsilon_{n,N}^2+\frac{c_1}{2n}\NN}^{1}\left(1-x^{2}\right)^{\frac{n-1}{2}}dx \nonumber\\
%& =|B_{n-1}|\int_{1-\frac{1}{2}\varepsilon_{n,N}^2+\frac{c_1}{2n}\NN}^{1}\left((1-x)(1+x)\right)^{\frac{n-1}{2}}dx \nonumber\\
& \leq 2^{\frac{n-1}{2}}|B_{n-1}|\int_{1-\frac{1}{2}\varepsilon_{n,N}^2+\frac{c_1}{2n}\NN}^{1}\left(1-x\right)^{\frac{n-1}{2}}dx\nonumber\\
%& =|B_{n-1}|2^{\frac{n-1}{2}}\frac{2}{n+1}\left(1-x\right)^{\frac{n+1}{2}}\Big|_{1}^{1-\frac{1}{2}\varepsilon_{n,N}^2+\frac{c_1}{2n}\NN} \nonumber\\
& =\frac{|B_{n-1}|}{n+1}2^{\frac{n+1}{2}}\bigg(\frac{1}{2}\left(\frac{|\partial B_n|}{|B_{n-1}|N}\right)^{\frac{2}{n-1}}-\frac{c_1}{2n}\NN\bigg)^{\frac{n+1}{2}} \nonumber\\
& \leq\frac{|B_{n-1}|}{n+1}\bigg(\left(1-\frac{c_1}{n}\right)\left(\frac{|\partial B_n|}{|B_{n-1}|N}\right)^{\frac{2}{n-1}}\bigg)^{\frac{n+1}{2}} \nonumber\\
& < 9e^{-c_1/2}|B_n|N^{-1}N^{-\frac{2}{n-1}}.
\end{align}
Please note that in the last inequality, we used Lemma \ref{stirling} and the elementary inequality $\frac{n}{n+1}\sqrt{2\pi n}<9$, $n\geq 2$. Thus, from (\ref{eq:a1}) and (\ref{eq:a2}) we obtain
\[
|B_n \setminus \hat{P_b}| %\leq N|C|
\leq 9e^{-c_1/2}|B_n|\NN.
\] 
However, by \cite[Theorem 2]{Lud06} it is known that
\[
|\hat{P_b} \triangle B_n| \geq c|B_n|\NN.
\]
Therefore, when $ c_1 $ is large enough we get
\[
|\hat{P_b}| \geq \left(1+\frac{c}{2}\NN\right)|B_n|,
\]
which contradicts the fact that $ |\hat{P_b}| = |B_n|. $ Hence, for all polytopes $P_{n,N}\supset B_n$ with $N$ facets, when $N$ is large enough we have $|P_{n,N}| > \left(1+\tfrac{1}{2}(n+\ln n- c_1)\NN\right)|B_n|$, i.e., 
\[
|P_{n,N}\setminus B_n| > \left(1+\frac{1}{2}(n+\ln n- c_1)\NN\right)|B_n|\NN. 
\]
Thus, from (\ref{GruberOutEqn}) we get that for all sufficiently large $N$,
\[
\frac{1}{2}\left(1-\frac{1}{n^2}\right)\divv_{n-1}|\partial B_n|^{\frac{n+1}{n-1}}\NN \geq \frac{1}{2}(n+\ln n-c_1)|B_n|\NN.
\]
Finally, another application of Stirling's inequality yields
\begin{align}\label{divgeq}
\divv_{n-1} \geq (2\pi e)^{-1}(n+\ln n) -c_1.
\end{align}
The theorem now follows from \eqref{divleq} and \eqref{divgeq}. \qed
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Proof of Theorem \ref{mahler}}\label{mahlerproof1}

First note that it is well-known that a centered convex body has the origin as its Santal\'o point (see, e.g., \cite{SchneiderBook}). Also, recall that $ P_{n,N} = \conv\{\pm X_i\}_{i=1}^{\frac{N}{2}}$. By an argument of M\"uller \cite{muller1990approximation}, it follows  that\footnote{The original proof in \cite{muller1990approximation} is for $\conv\{X_1,\ldots,X_{N}\}$; simple modifications to the arguments there show that the expected volume of $\conv\{\pm X_1,\ldots,\pm X_{N/2}\}$ equals the expected volume of $\conv\{ X_1,\ldots,X_{N}\}$ (up to a negligible factor).}
%Using all of Theorems we know that when $ N $ is large enough the following holds: with high probability $ P'^{\circ}_{n,\frac{N}{2}} $ satisfies the following: 
%\[
%	|P'^{\circ}_{n,\frac{N}{2}}| = (1+(\frac{N}{2})^{-\frac{2}{n-1}}(2\pi e)^{-1}(n+\ln(n)+O(1)))|B_n|
%\]
\[
\E[|P_{n,N}|] = \left(1-2^{-1}\left(n+4\ln n+O(1)\right)N^{-\frac{2}{n-1}}\right)|B_n|.
\]
Furthermore, Reitzner \cite{reitzner2003random} showed that when $ N $  is large enough,% the variance of $|P_{n,N}|$ is of the order $ c(n)N^{-(1+\frac{4}{n-1})} $. Thus with high probability, the following holds: 
\[
\var(|P_{n,N}|) \leq c(n)N^{-(1+\frac{4}{n-1})}.
\]
Thus, by Chebyshev's inequality the event
\begin{equation}\label{chevy1}
|P_{n,N}| \approx \left(1-2^{-1}(n+ 4\ln n+O(1))N^{-\frac{2}{n-1}}\right)|B_n|  
\end{equation}
holds with probability at least $1-C(n)N^{-1}$, where $\approx$ denotes asymptotic equality up to a factor of $O(1)\NN$. Now the proof of Theorem \ref{deldiv}, specifically the argument showing that $\divv_{n-1} \geq (2\pi e)^{-1}(n+\ln n) - c_1,$ implies that when $ N$ is large enough the following inequality holds for every realization of $ P_{n,N}$: 
\begin{equation}\label{polarlower1}
|P^{\circ}_{n,N}| \approx \left(1+2^{-1}(n+\ln n+O(1))\NN\right)|B_n| .
\end{equation}
Using (\ref{chevy1}) and (\ref{polarlower1}), we conclude that with high probability
\[
F(n,N) \geq \E[|P_{n,N}|\cdot|P^{\circ}_{n,N}|] \approx (1-(1.5\ln n + O(1))\NN)|B_n|^2.
\]
\qed
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Proof of Theorem  \ref{partialthm}}\label{spherethm} %and Theorem \ref{Thm1}}

\subsection{Estimating the expectation of a random partial covering}\label{expectationsection}
%We prove Theorem \ref{Thm1} on sphere ``covering" first because it provides some of the intuition for the proof of Corollary \ref{mainThm} on random polytopes.
% Denote by $ N_{\epsilon(n)} := N_{1,\epsilon(n)}. $  Let $ X_1,\ldots,X_{N_{\epsilon(n)}} \stackrel{\text{i.i.d.}}{\sim} \sigma$ and consider the  random variable 
%\[
%%\frac{1}{|\partial B_n|}
%V() := |\partial B_n|^{-1}\times\left|\bigcup_{i=1}^{\lceil \alpha N \rceil}B_G({X_i},\epsilon)\right|.
%\]
Here we show that the expectation of the random variable $\mathbb{V}(\mathbf{X}_{\alpha N})$, which is the proportion of the surface area of $\Sn$ that the caps capture, is about $ 1-e^{-\alpha}.$ Let $\epsilon$ be the height of a spherical cap with a surface area $N^{-1}|\partial B_n|$. By Fubini's theorem and the independence of the $X_i$,
\begin{equation}\label{indepRV}
\begin{aligned} \E[\mathbb{V}(\mathbf{X}_{\alpha N})]=\E\left[\int_{\mathbb{S}^{n-1}}(1 -\mathbbm{1}_{\max\langle X_{i},x\rangle\leq 1-\epsilon}(x))\,d\sigma(x)\right]  &=1 -\int_{\mathbb{S}^{n-1}}\prod_{i=1}^{\lceil \alpha N \rceil}\E\big[\mathbbm{1}_{\langle X_{i},x\rangle\leq 1-\epsilon}(x)\big]d\sigma(x)\\
&= 1 - \left(1-\frac{1}{N}\right)^{\lceil \alpha N \rceil}
\\& =1 - e^{\lceil\alpha N\rceil  \ln(1-1/N)}
\\
%&\leq 1 - e^{\alpha N  \ln(1-1/N)}\\
&=1 - e^{-\alpha} + \frac{1}{2}\alpha e^{-\alpha} N^{-1} +O( \alpha e^{-\alpha} N^{-2}).
\end{aligned}
\end{equation}
Observe that each cap increases $ \mathbb{V}(\mathbf{X}_{\alpha N}) $ by at most $ N^{-1}$. Hence, for all $ 1\leq i \leq \lceil \alpha N \rceil $,
	\[
	c_i:=\sup_{ x_1,\ldots,x_{\lceil \alpha N \rceil},x'_i} | \mathbb{V}(\mathbf{X}_{\alpha N})- \mathbb{V}(\mathbf{X}_{\alpha N}^\prime)| \leq N^{-1}
	\]
	where $\mathbf{X}_{\alpha N}^\prime:=(X_1,\ldots,X_{i-1},X_i^\prime,X_{i+1},\ldots,X_{\lceil \alpha N \rceil})$. Thus, by McDiarmid's inequality, 
	\[
	\Pr(|\mathbb{V}(\mathbf{X}_{\alpha N}) - (1-e^{-\alpha})| > \epsilon) \leq 2\exp\left(-\frac{2\epsilon^2}{\sum_{i=1}^{\lceil \alpha N \rceil}N^{-2}}\right) \leq 2e^{-2\lfloor \alpha^{-1}\rfloor N\epsilon^2}.
	\]
	\subsection{Estimating the variance of a random partial covering}
	For this proof, we use the notation
	$
	    \epsilon=\epsilon(n,N) := \left(\frac{|\partial B_{n}|}{|B_{n-1}|N}\right)^{\frac{1}{n-1}},
	$
	and 
	$
	    C(x,N):= B_G(x,N^{-1})
	$
	denotes the spherical cap with center $x$ and normalized surface area $N^{-1}$. 
	We shall also use the fact that $  (1-\E[\mathbb{V}(\mathbf{X}_{\alpha N})])^2 = e^{-2\alpha}(1+0.5 \alpha N^{-1})^2 =  e^{-2\alpha}(1+\alpha N^{-1}) + O(C(\alpha)N^{-2}) $. 
%	Since the variance is invariant to shifts in constants,  we obtain
	By elementary properties of the variance and the linearity of expectation,
	\begin{align}\label{variancecaps}
	    \var(\mathbb{V}(\mathbf{X}_{\alpha N})) %&= \var(1-\mathbb{V}(\mathbf{X}_{\alpha N})) \nonumber\\
	    &=\E[(1-\mathbb{V}(\mathbf{X}_{\alpha N}))^2]-(1-\E[\mathbb{V}(\mathbf{X}_{\alpha N})])^2.
	\end{align}
Expanding the product in the first term, we obtain
	\begin{align*}
	    (1-\mathbb{V}(\mathbf{X}_{\alpha N}))^2 &= \left(\int_{\Sn}\mathbbm{1}_{\max\langle X_{i},x\rangle\leq 1-\epsilon}(x)\,d\sigma(x)\right)^2
	    \\&=
	    \int_{\Sn}\int_{\Sn}\prod_{i=1}^{\lceil\alpha N\rceil}\mathbbm{1}_{\langle X_{i},x\rangle\leq 1-\epsilon,\langle X_{i},y\rangle\leq 1-\epsilon}(x)\,d\sigma(y)\,d\sigma(x).
	\end{align*}
By independence and the rotational invariance of the uniform measure, we get
	\begin{align*}
	    \E[ (1-\mathbb{V}(\mathbf{X}_{\alpha N}))^2] &= \int_{\Sn}\int_{\Sn}\Pr( x \notin C(X_i,N),y \notin C(X_i,N))^{\lceil\alpha N\rceil}\,d\sigma(y)\,d\sigma(x) 
	    \\&= \int_{\Sn}\Pr( e_n \notin C(X_i,N),y \notin C(X_i,N))^{\lceil\alpha N\rceil}\,d\sigma(y)
	    \\&= \int_{\Sn}\Pr( X \notin C(y,N), X \notin C(e_n,N))^{\lceil\alpha N\rceil}\,d\sigma(y).
   \end{align*}
The last integrand measures the probability that $e_n$ and $y$ are not inside the same cap centered at $X$. Since $X$ is drawn uniformly from the sphere, this probability equals measure of the union of these two spherical caps; moreover, if $d_G(y,e_n) \geq 2\epsilon$, then it equals the sum of the measures of the two caps. Thus,
    \begin{align}\label{2ndmoment}
        \E[(1-\mathbb{V}(\mathbf{X}_{\alpha N}))^2] &= (1-2^{n-1}N^{-1})(1-2N^{-1})^{\lceil\alpha N\rceil} \nonumber\\
&+ \int_{C(e_n,2^{n-1}N^{-1})}\Pr( X \notin C(y,N), X \notin C(e_n,N))^{\lceil\alpha N\rceil}\,d\sigma(y)  
        % \\& = e^{-2\alpha}(1+\alpha N^{-1})(1-2^{n-1}N^{-1})+ \int_{B_G(e_n,2\epsilon)}\Pr( X \notin C(y,N), X \notin C(e_n,N))^{\alpha N}\,d\sigma(y)
       \nonumber \\
       & = (1+\alpha N^{-1}-2^{n-1}N^{-1})e^{-2\alpha} \nonumber\\
&+ \underbrace{\int_{C(e_n,2^{n-1}N^{-1})}\left(1 - \frac{| C(y,N) \cup C(e_n,N) |}{|\partial B_n|}\right)^{\lceil\alpha N\rceil}\,d\sigma(y)}_{=:I}.
    \end{align}
    Now when $N$ is large enough, we may assume that the caps are $(n-1)$-dimensional balls with the same radius. Indeed, if their distance is $t < 2\epsilon$, then the measure of the intersection of the two balls equals the measure of two $(n-1)$-balls with height $\epsilon - t/2$ (up to a negligible perturbation). Using polar coordinates, we  estimate the  integral $I$ by
    \begin{align}\label{integralI}
         I &=  \frac{|\partial B_{n-1}|}{|\partial B_n|}\int_{0}^{2\epsilon}t^{n-2}(1-2N^{-1}+\sigma(C(e_n,\epsilon-t/2)))^{\lceil\alpha N\rceil}\, dt
        \nonumber \\& \leq  \frac{|\partial B_{n-1}|}{|\partial B_n|}\int_{4^{-1}\epsilon}^{2\epsilon} t^{n-2}(1-(2+c^{n-1})N^{-1})^{\lceil\alpha N\rceil}\,dt + e^{-\alpha}4^{-(n-1)}N^{-1}
       \nonumber\\& \leq    2^{n-1}N^{-1}(1-(2+ c^{n-1})N^{-1})^{\lceil\alpha N\rceil}+e^{-\alpha}C_1^{n-1}N^{-1} 
       \nonumber\\&\leq 2^{n-1}N^{-1}e^{-2\alpha} + e^{-\alpha}C^{n-1}N^{-1}. 
    \end{align}
  Combining \eqref{2ndmoment} and \eqref{integralI}, we get 
    \begin{equation}\label{2ndmomentupper}
        \E[(1-\mathbb{V}(\mathbf{X}_{\alpha N}))^2] \leq e^{-2\alpha}(1+\alpha N^{-1}) + e^{-\alpha}C^{n-1}N^{-1}. %\leq \E[(1-\mathbb{V}(\mathbf{X}_{\alpha N})]^2 + c_1^{n-1} e^{-\alpha} N^{-1}.
    \end{equation}
    Putting everything together, we use \eqref{variancecaps} and \eqref{2ndmomentupper} to derive 
    \begin{align*}
\var(\mathbb{V}(\mathbf{X}_{\alpha N}))%\leq \E[(1-\mathbb{V}(\mathbf{X}_{\alpha N}))^2] +(1-\E[\mathbb{V}(\mathbf{X}_{\alpha N})])^2\\
&\leq e^{-2\alpha}(1+\alpha N^{-1}) + e^{-\alpha}C^{n-1}N^{-1}-\left(e^{-\alpha} - \frac{1}{2}\alpha e^{-\alpha} N^{-1} -O( \alpha e^{-\alpha} N^{-2})\right)^2\\
&\leq e^{-\alpha}C^{n-1}N^{-1}.
    \end{align*}
Following a similar analysis, one can also show that
    \[
         \E[(1- \mathbb{V}(\mathbf{X}_{\alpha N}))^2] \geq e^{-2\alpha}(1+\alpha N^{-1}) + c_2^{n-1} e^{-C_2\alpha} N^{-1},
    \]
    where $C_2 \in (1,2)$.
    The claim follows.
    %     \begin{align*}
    %   & C_d\int_{0}^{2\epsilon}\Pr( e_n \notin C(X_i,N),y \notin C(X_i,N))^{\alpha N}\,d\sigma(y) \\&=  C_d\int_{0}^{2\epsilon}t^{d-2}(1-2N^{-1}+2Cap(\epsilon-t/2))^{\alpha N} dt
    %   \\&= 
    %   C_d\int_{0}^{2\epsilon} t^{d-2}(1-2N^{-1} - 2|B_{d-2}|\int_{t/2}^{\epsilon}(\epsilon-x)^{(d-2)/2}dx)^{\alpha N}dt \leq
    %   \\&  C_d\int_{4^{-1}\epsilon}^{2\epsilon} t^{d-2}(1-2N^{-1} - |B_{d-2}|\frac{C(\epsilon-t^2/4)^{(d-2)/2}(\epsilon - t/2)}{n-1} )^{\alpha N}dt + 4^{-d}N^{-1}
    %   \\&= C_d\int_{4^{-1}\epsilon}^{2\epsilon} t^{d-2}(1-2N^{-1} - |B_{d-2}|\frac{C(\epsilon-t^2/4)^{(d-2)/2}(\epsilon - t/2)}{n-1} )^{\alpha N}dt + 4^{-d}N^{-1}
    %   \\&= N^{-1}\int_{4^{-1}}^{1}t^{d-2}(1-2N^{-1} + N^{-1}|B_{d-2}|\frac{C(1-t^2/4)^{(d-2)/2}(1 -t/2)}{n-1})^{\alpha N}dt + 4^{-d}N^{-1} 
    % \end{align*}

% 	\subsection{Lower bound of variance}
% 	First, we  prove that the variance of the geometric ball and bins problem (with $N$ balls and $N$ bins) is bounded below by $c(n)N^{-1}$. For this purpose, we will apply a local argument. First, we estimate the probability that only $n+1$ points will fall in a spherical cap with center $z$ and measure $N^{-1}$, and that the rest of the points will fall outside the cap with four times the radius; denote this event by $A_z$. Using the fact that the measure is uniform and $X_1,\ldots,X_N$ are i.i.d., we derive that
% 	\begin{equation}\label{problower1}
% 	\Pr(A_z) = \binom{N}{n+1}(c^{n}N^{-1})^{n+1} (1-4^{n}N^{-1})^{N-(n+1)} \geq  c(n).
% 	\end{equation}
% 	Using a standard packing argument, we know that there exist $c^n N$ disjoint spherical caps with volume $N^{-1}$; denote their centers by $X_1,\ldots,X_{c^n N}$. Letting $A_i:=A_{X_i}$, by \eqref{problower1} and the linearity of expectation we get
% 	\[
% 	\E\left[\sum_{i=1}^{c^n N}\mathbbm{1}_{A_i}\right] \geq c(n)N.
% 	\]
% 	By definition, we know that when $A_i$ holds  the portion of the volume of the spherical cap with center $z_i$ and volume $2^{n}N^{-1}$  only depends on the $n+1$ points that lie in the geodesic ball with center $z_i$ and volume $N^{-1}$ . Since all the caps involved have a volume of $2N^{-1}$ or $N^{-1}$, by scaling we know that the variance of this local portion is $c_1(n)N^{-2}$. Clearly, the local portions in the events $A_i$ and $A_j$ are independent whenever $i\neq j$. Thus, we
% 	define $\mathcal{F}$ as the sub-$\sigma$-algebra that fixes all the points except the ones when $A_i$ holds. Also, we denote by $V(\mathbf{X})$ the volume of the union of the caps $X_1,\ldots,X_N$.  By the conditional variance formula,
% 	\[
% 	\var(V(\mathbf{X})) \geq \E[\var(V(\mathbf{X}) | \mathcal{F})] = \E[\sum_{\mathbbm{1}_{A_i}=1}\var(local(Cap))] \geq c(n)N^{-2}\E[\sum_{\mathbbm{1}_{A_i}=1}1] \geq c_1(n)N^{-1}.  
% 	\]
	
%\end{proof}
%\]which implies
%\begin{equation}\label{divlowern}
%\divv_{n-1} > (1-o(1))n.
%\end{equation}
%Hence, from (\ref{divlowern}) and (\ref{msdel}) we deduce that 
%\begin{equation}\label{divgeqdel}
% \divv_{n-1} > (1-o(1))n > \dell_{n-1} - c_1\ln(n).
%\end{equation}
%Now from (\ref{divleqdel}) and (\ref{divgeqdel}) we get the desired inequality, i.e.
%\begin{equation}\label{divdelineq}
%	|\divv_{n-1} - \dell_{n-1}| \leq c\ln(n).
%\end{equation}
%
%\vspace{2mm}
%For the conclusion, we show that $\lim_{n\to\infty}\frac{\divv_{n-1}}{\dell_{n-1}}=1$. By the left-hand inequality in (\ref{msdel}), we have\begin{equation}\label{dellower6}
%\dell_{n-1}> c_0 n.
%\end{equation}
%Using (\ref{divdelineq}), (\ref{dellower6}), and $\frac{\divv_{n-1}}{\dell_{n-1}}=1+\frac{\divv_{n-1}-\dell_{n-1}}{\dell_{n-1}}$, we  obtain
%\begin{align*}
%1-\frac{c\ln(n)}{c_0 n}\leq \frac{\divv_{n-1}}{\dell_{n-1}} \leq 1+\frac{c\ln(n)}{c_0 n}.
%\end{align*}Taking limits throughout the previous inequality, the desired result follows.

%*****
%To see why  inequality $(\star)$ holds, first observe that the complementary event of the right-hand side is $ |S_{\max}| <  2^{n-1}N^{0.5-\frac{2}{n-1}}.$ Using the triangle inequality and the definition of  $S_{\max}$, we derive that
%\[ 
%A_{\mathcal{N} \setminus S_{\max}}:=\bigcup_{y\in\mathcal N\setminus S_{\max}}B_G(y,\tfrac{1}{2}N^{-\frac{1}{n-1}}) \subset A_{\emph X}.
%\]
%Therefore the missing surface area of that the random covering of $ \bf X,$  lies on the balls of $ S_{\max} $ with radius $ \frac{1}{2}N^{-\frac{1}{n-1}} $. 
%Thus, by De Morgan's law
%\begin{align*}
%A^c_{\mathcal N\setminus S_{\max}} %&= \left(\bigcup_{y\in\mathcal N\setminus S_{\max}}B_G(y,\tfrac{1}{2}N^{-\frac{1}{n-1}})\right)^c \\
%&= \bigcap_{y\in\mathcal N\setminus S_{\max}} B_G(y,\tfrac{1}{2}N^{-\frac{1}{n-1}})^c 
%\subset \bigcup_{y\in S_{\max}} B_G(y,\tfrac{1}{2}N^{-\frac{1}{n-1}}) =: A_{S_{\max}},
%\end{align*}%Hence \begin{align*}
%x\in A^c_{\mathcal N\setminus S_{\max}, 2^{-(n-1)}N^{-1}} &\Longleftrightarrow \forall y\in\mathcal N\setminus S_{\max}:\, x\not\in B_G(y,\tfrac{1}{2}N^{-\frac{1}{n-1}})\\
%&\Longrightarrow x\in\bigcup_{y\in S_{\max}} B_G(y,\tfrac{1}{2}N^{-\frac{1}{n-1}})=A_{S_{\max},2^{-(n-1)}N^{-1}},
%\end{align*}
%where the inclusion follows since $\mathcal N$ is a covering of $\Sp$. The previous two inclusions yield %under the complementary event we obtain 
%****
%	\begin{align*} 
%	|A^c_{\emph X,N^{-1}}| \leq |A^c_{\mathcal N\setminus S_{\max}}|
%	\leq |A_{S_{\max}}| 
%	&= |\bigcup_{ y \in S_{\max}}B_G(y,\tfrac{1}{2}N^{-\frac{1}{n-1}})| \\
%	& \leq \sum_{y\in S_{\max}} |B_G(y,\tfrac{1}{2}\NNN)| \\
%	&\leq |S_{\max}| \times |B_G(y,\tfrac{1}{2}\NNN)| \\
%	&= |S_{\max}| \times 2^{-(n-1)}N^{-1}|\partial B_n| \\
%	&< N^{-(0.5+\frac{2}{n-1})}|\partial B_n|,
%	\end{align*}which is a contradiction to the definition of the event $B$. This establishes inequality $(\star)$. 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \section{Proof of Theorem \ref{varThm}}
% %We are going to prove the result for the case $K = B_n$ and $\mu = \sigma$. 
% Recall that we aim to show $\var(|P_{n,N}\setminus B_n|) \geq c(n)N^{-\left(1+\frac{4}{n-1}\right)}$. The proof follows along the same lines as the variance estimate for the balls and bins problem. As was the case there, the main step is again to estimate the second moment from below. Consider the event $E:=\{P_{n,N} \subset (1+c_{n,N})B_n\}$, where $c_{n,N}=c(n)\left(\frac{\ln N}{N}\right)^{\frac{2}{n-1}}$ was defined in Theorem ******. This event holds with probability at least ***** Moreover, under this event, each facet $\{\langle X_i,x\rangle =1\}$ of $P_{n,N}$ cuts off a cap $C(X_i)$  of height $c_{n,N}$ from $(1+c_{n,N})B_n$.  For the rest of the proof, we shall assume  that $E$ holds. Then
% \[
% |P_{n,N}\setminus B_n| = |(1+c_{n,N})B_n\setminus B_n| - |\cup_{i=1}^N C(X_i)|,
% \]
% which implies $\var_E(|P_{n,N}\setminus B_n|) = \var_E(|\cup_{i=1}^N C(X_i)|)$. Thus, we need to estimate the second moment of $|\cup_{i=1}^N C(X_i)|$ from below. Converting to polar coordinates, we have
% \begin{align*}
%     |\cup_{i=1}^N C(X_i)|^2 &= \left(\int_{(1+c_{n,N})B_n\setminus B_n}\mathbbm{1}_{\max\langle X_i,x\rangle \leq 1}(x)\,dx\right)^2\\
%     &=|\partial B_n|^2\left(\int_{\Sp}\int_1^{1+c_{n,N}} r^{n-1}\mathbbm{1}_{\max\langle X_i,x\rangle \leq r^{-1}}(x)\,dr\,d\sigma(x)\right)^2\\
%     &=|\partial B_n|^2\int_{\Sp}\int_{\Sp}\int_1^{1+c_{n,N}}\int_1^{1+c_{n,N}} (rs)^{n-1}\\
%     &\times \prod_{i=1}^N \mathbbm{1}_{\langle X_i,x\rangle \leq r^{-1}, \langle X_i,y\rangle \leq s^{-1}}(x,y)\,dr\,ds\,d\sigma(x)\,d\sigma(y)
% \end{align*}
% %If the height of the cap $C(x)$ centered at $x\in \partial(1+c_{n,N})B_n$ is $h=\text{dist}(o,C(x))$, then we write $C(x)=C(x,h)$. Thus by Fubini's theorem, the second moment of the volume of the caps can be expressed as
% By taking expectation as in the proof of Theorem ***, we conclude that 
% \begin{align*}
%     \E[|\cup_{i=1}^N C(X_i)|^2] &= |\partial B_n| \int_1^{1+c_{n,N}}\int_1^{1+c_{n,N}}(rs)^{n-1}\\
%     &\times\int_{\Sp}\Pr(X\not\in C(x,r^{-1}), X \not\in C(e_n,s^{-1}))^N d\sigma(x)\,dr\,ds,
% \end{align*}
% where $C(X, r)$ is a spherical cap that is centered on $X$ with radius $r$. In Theorem ***, we considered a similar case, when the caps had the same radius. By similar considerations, we conclude that
% \begin{align*}
% 	(*) &= |\partial B_n| \int_1^{1+c_{n,N}}\int_1^{1+c_{n,N}}(rs)^{n-1}\int_{\Sn}(1 - |C(X_i,r^{-1})| + |C(X_i,s^{-1})|)^N \,dr\,ds,  + 	
% \end{align*}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Proof of Theorem \ref{geneThm}}\label{pfthm1}
%Let $ c_{n,N} = 1+c_0(n,K)\ln(N)^{\frac{2}{n-1}},$ where $ c_0(n,K) $ ****
In this section, we show how to modify the proof of Corollary \ref{mainThm}  to extend the result from the Euclidean ball to all smooth convex bodies $K$ with positive curvature. The proof that is given holds for the uniform distribution on the boundary; the extension to arbitrary densities follows from arguments similar to those in the proof of Corollary \ref{mainThm}. 

\vspace{1mm}

First, recall that the random polytope $ P_{n,N} $ is defined by
\[
P_{n,N} := \bigcap_{i=1}^{N}\{x\in\R^n: \inneri{x}{\nu(X_i)} \leq \inneri{X_i}{\nu(X_i)} \}, \hspace{.2in} X_1,\ldots,X_N \stackrel{\text{i.i.d.}}{\sim} \sigma_{\partial K} ,
\]
where $ \sigma_{\partial K} $ denotes the uniform probability measure on the boundary of $ K $. We use a ``polar'' coordinates formula for a convex body with the origin in its interior (see, e.g., \cite{nazarov2003maximal}) to express the volume of the set difference $P_{n,N}\setminus K$ as
\begin{align*}
|P_{n,N}\setminus K| &=\int_{\R^{n} \setminus K} \mathbbm{1}_{\max\inneri{\nu(X_i)}{x} \leq \inneri{\nu(X_i)}{X_i}}(x)\, dx\\
%&\qquad \qquad
&= |\partial K|\int_{1}^{\infty}r^{n-1}\int_{\partial K}\|y\|\alpha(y)\mathbbm{1}_{\max\inneri{\nu(X_i)}{y}\leq r^{-1}\inneri{\nu(X_i)}{X_i}}(y) \,d\sigma_{\partial K}(y)\,dr,
\end{align*}
where  $ \nu(y) $ is the outer unit normal to $\partial K$ at the point $y$ and $ \alpha(y) $ is the cosine of  the angle between the normal $ \nu(y) $ and the ``radial" vector $ y\in\partial K.$ We now split this integral into three parts as we did in the proof of Corollary \ref{mainThm}:
\begin{align}|P_{n,N}\setminus K| & =|\partial K|\bigg(\int_{1}^{1+c_{K,N}}r^{n-1}\int_{\partial K}\|x\|\alpha(x)\mathbbm{1}_{\max\langle\nu(X_{i}),x\rangle\leq r^{-1}\langle\nu(X_{i}),X_{i}\rangle}(x)\,d\sigma_{\partial K}(x)\,dr\nonumber\\
& +\int_{1+c_{K,N}}^{1+\frac{2}{n}}r^{n-1}\int_{\partial K}\|x\|\alpha(x)\mathbbm{1}_{\max\langle\nu(X_{i}),x\rangle\leq r^{-1}\langle\nu(X_{i}),X_{i}\rangle}(x)\,d\sigma_{\partial K}(x)\,dr\nonumber\\
& +\int_{1+\frac{2}{n}}^{\infty}r^{n-1}\int_{\partial K}\|x\|\alpha(x)\mathbbm{1}_{\max\langle\nu(X_{i}),x\rangle\leq r^{-1}\langle\nu(X_{i}),X_{i}\rangle}(x)\,d\sigma_{\partial K}(x)\,dr\bigg)\nonumber\\
& =Y+Z+W.
\end{align}
Here $ c_{K,N}:=c(K)\NN(\ln N)^{\frac{2}{n-1}} $ and $ c(K) $ is a large constant that is defined at the end of Subsection \ref{sectionZnegligibleK}. As in the proof of Corollary \ref{mainThm}, we will divide the proof into three lemmas, considering each random variable $Y$, $Z$ and $W$ separately. The proofs of these lemmas are similar to those of Lemmas  \ref{Yprop}, \ref{Zprop} and \ref{Wconcentration} for the Euclidean unit ball. The modifications needed to extend the proofs to all smooth convex bodies  involve elementary differential geometry. 
%We use elementary differential geometry to extend  Lemmas \ref{Yprop}, \ref{Zprop} and \ref{Wconcentration} for the Euclidean unit ball to all smooth convex bodies with positive curvature.

%****what if $P_{n,N}$ is arbitrarily positioned? Can we write the integral as $Y_1+Y_2+Z+W$ as in the proof of Corollary \ref{mainThm}?****

%%%%%%%%%%

\subsection{Concentration for the random variable $Y$}

\begin{lemma}\label{YpropK}
	Let $\epsilon>0$. There is a constant $c_1(K)>0$ such that for all sufficiently large $N$,
	\[
	\Pr(|Y-\E[Y]| \geq \epsilon) \leq
	2\exp\left(-c_1(K)N^{1+\frac{4}{n-1}}f(N)\epsilon^2\right)
	\]
	where $f(N)=(\ln N)^{-(2+\frac{4}{n-1})}$.
\end{lemma}

\begin{proof}%[Proof of Lemma \ref{YpropK}]
	
	We follow along the same lines as the proof of Lemma \ref{Yprop}, where we used McDiarmid's inequality to derive a concentration inequality for the random variable $Y$ by analyzing a random partial ``covering" of the sphere by geodesic balls of a fixed radius. However, in the setting of smooth convex bodies with positive curvature, we will instead consider a random partial ``covering" of $\partial K$ by geodesic ellipsoids. Moreover, unlike the sphere ``covering" setting, the  shape of each geodesic ellipsoid can vary depending on the curvature of $K$ at the ellipsoid's center.
	
	\vspace{1mm}
	
	First, we define $ f(X_1,\dots,X_N):=|P_{n,N}^c\cap(1+c_{K,N})K\cap K^c| $ to be the volume that we remove from $ (1+c_{K,N})K \setminus K $, so that $Y=|(1+c_{K,N})K\setminus K| - f(X_1,\ldots,X_N)$. Then for all $1\leq i\leq N$,
	\begin{align*}
	c_i:&=\sup_{x_1,\ldots,x_N,x'_i}|f(x_1,\ldots,x_i,\ldots ,x_N) - f(x_1,\ldots,x'_i,\ldots x_N)| \\
	&\leq \sup_{x_1,\ldots,x_N,x'_i} |(x_i^\perp)^{-}\cap (1+c_{K,N})K|.
	\end{align*}
	
	Since $K$ has $C^2$ boundary with  positive curvature, each point in the boundary of $K$ is an elliptic point. Thus for each $x\in\partial K$, we can represent the cap $(x_i^\perp)^{-} \cap  K$ in local coordinates as a cap of the ellipsoid  $\mathcal{E}(x)$ with axes length as the principal radii of curvature $\kappa_j(x)^{-1}$:
	\[
	\mathcal{E}(x):=\left\{\left(z_1,\ldots,z_{n-1},\sqrt{1 - \sum_{j=1}^{n-1}\frac{z^2_j}{\kappa_j(x)^{-2}}}\right): z_1,\ldots,z_{n-1}\in\R\right\}.
%\left\{(z_1,\ldots,z_n)\in\R^n: \left(z_1,\ldots,z_{n-1},\sqrt{1 - \sum_{j=1}^{n-1}\frac{z^2_j}{\kappa_j(x)^{-2}}}\right)\right\}.
	\]Please note that when $N$ is large enough, for each $1\leq i\leq N$ the volume of the cap $(x_i^\perp)^{-} \cap  K$ equals the volume of  $(x_i^\perp)^{-}\cap \mathcal{E}(x_i)$, up to a term of negligible order in $N$. 
	Using a computation similar to the one in the proof of Corollary \ref{mainThm}, we derive that
	\begin{align}\label{ellipsoidcap}
	|(x_i^\perp)^{-}\cap (1+c_{K,N})\mathcal{E}(x_i)| &=|B_{n-1}|\int_{\langle\nu(x_i),x_i\rangle}^{1+c_{K,N}}\left((1+c_{K,N})^2-x^2\right)^{\frac{n-1}{2}}\prod_{j=1}^{n-1}\kappa_j(x)^{-1}\,dx \nonumber\\
	&\leq 3^{\frac{n-1}{2}}(1+c_{K,N})^{\frac{n-1}{2}}|B_{n-1}|\kappa(x_i)^{-1}\int_{\langle\nu(x_i),x_i\rangle}^{1+c_{K,N}}\left(1+c_{K,N}-x\right)^{\frac{n-1}{2}}\,dx \nonumber\\
	&\leq C(K)N^{-(1+\frac{2}{n-1})}(\ln N)^{1+\frac{2}{n-1}}, \nonumber
	\end{align}where $\kappa(x)$ denotes the Gaussian curvature of $K$ at $x\in\partial K$. The rest of the proof is similar to that of Lemma \ref{Yprop}. 
\end{proof}

%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{The random variable $Z$ is negligible}\label{sectionZnegligibleK}

Next, we extend Proposition \ref{Zprop} from the ball to all smooth convex bodies with positive curvature. The final ingredient we need follows from the papers \cite{erdHos1964amount,erdos1953covering} of Erd{\H{o}}s and Rogers. 

\begin{lemma}{\label{ErodsRogeres}}
	Let $ K $ be a $ C^2 $ convex body, and for fixed $\delta>0$ let $ \mathcal{N} $ be  a minimal $\delta$-net of $\partial K$, i.e., $\bigcup_{x\in\mathcal N}B_G(x,\delta) \supset \partial K$ and every $\delta$-net of $\partial K$ contains at least $|\mathcal N|$ elements. %$\partial K$ cannot be covered by fewer than $|\mathcal N|$ geodesic balls of radius $r$. %$ |\mathcal N|$ equals the minimal number of geodesic balls of radius $ r $ that are needed to cover $\partial K$. 
	Then when $ \delta $ is small enough, each point of $\partial K$ lies in the interior of no more than $ 4^n\frac{|\partial K|}{|\partial B_n|} $ balls of the covering.
\end{lemma}
This result is far from optimal; recall that B\"or\"oczky and Wintsche \cite{boroczky2003covering} showed that for a Euclidean ball of any radius, there is a covering such that each point of its boundary lies in the interior of no more than $400n\ln n$ geodesic balls.

\vskip .1in

The extension of Lemma \ref{Zprop} is given in the next lemma.
\begin{lemma}\label{ZpropK}
	\begin{equation*}	
	\Pr\left(Z \geq N^{-(0.5+\frac{2}{n-1})}\right) \leq \exp\left(-c_1(K)N^{0.5-\frac{2}{n-1}}\right).
	\end{equation*}
\end{lemma}

\begin{proof}%[Proof of Lemma \ref{ZpropK}]
	Without loss of generality, we can assume that the origin lies in the interior of $K$, so that $\max_{x\in \partial K}\|x\| \leq \diam(K)$. 
	Moreover, $\alpha(x) := \cos(\measuredangle(\nu(x),x)) \leq 1$ for any $x\in\partial K$. Thus,
	\begin{align*}Z & =|\partial K|\int_{1+c_{K,N}}^{1+\frac{2}{n}}r^{n-1}\int_{\partial K}\|x\|\alpha(x)\mathbbm{1}_{\max\langle\nu(X_{i}),x\rangle\leq r^{-1}\langle\nu(X_{i}),X_{i}\rangle}(x)\,d\sigma_{\partial K}(x)\,dr\\
	& \leq\diam(K)|\partial K|\int_{1+c_{K,N}}^{1+\frac{2}{n}}r^{n-1}\,dr\int_{\partial K}\mathbbm{1}_{\max\langle\nu(X_{i}),x\rangle\leq(1+c_{K,N})^{-1}\langle\nu(X_{i}),X_{i}\rangle}(x)\,d\sigma_{\partial K}(x)\\
	& \leq Cn^{-1}\diam(K)|\partial K|\int_{\partial K}\mathbbm{1}_{\max\langle\nu(X_{i}),x\rangle\leq(1+c_{K,N})^{-1}\langle\nu(X_{i}),X_{i}\rangle}(x)\,d\sigma_{\partial K}(x).%\\
%	& =C_{1}(K)\tilde{Z},
	\end{align*}
	%when $ N $ is large enough everything is nearly ''flat'',
%	where $C_1(K):=Cn^{-1}\diam(K)$ and 
As in the proof of Lemma \ref{Zprop}, we  define 
\[
\tilde Z := |\partial K|\int_{\partial K} \mathbbm{1}_{\max\langle\nu(X_{i}),x\rangle\leq(1+c_{K,N})^{-1}\langle\nu(X_{i}),X_{i}\rangle}(x)\,d\sigma_{\partial K}(x).
\] 
The random variable $\tilde Z$  measures the missing surface area of a random  ``covering" of $K$ by $ N $ random geodesic ellipsoids of volume $ c(K)\frac{\ln N}{N}$. By independence, its expected value  can be estimated by
	\begin{align*}\E[\tilde{Z}] & = |\partial K| \prod_{i=1}^{N}\Pr\left(\langle\nu(X_{i}),x\rangle\leq(1+c_{K,N})^{-1}\langle\nu(X_{i}),X_{i}\rangle\right)\\
	%C(K)\times\prod_{i=1}^{N}\Pr\left(\langle\nu(X_{i}),x\rangle\leq(1+c_{K,N})^{-1}\langle\nu(X_{i}),X_{i}\rangle\right)\\
	& \leq |\partial K|\left(1-\frac{c(K)^{n-1}C(K)^{\frac{n-1}{2}}|B_{n-1}|(\ln N)}{N|\partial K|}\right)^{N}\\
	& \leq |\partial K|\exp\left(-\frac{c(K)^{n-1}C(K)^{\frac{n-1}{2}}|B_{n-1}|}{|\partial K|}\cdot\ln N\right)\\
	& =|\partial K| N^{-C(K)^{\frac{n-1}{2}}c(K)^{n-1}|B_{n-1}|/|\partial K|}.
	\end{align*}
	
	Next, we show that $ \tilde{Z} $ is negligible with high probability. As in the proof of Lemma \ref{Zprop}, we can reduce the problem to the following random ``covering" of $K$ by $C(K)N\ln N$ random ellipsoids of volume $c_i(K)N^{-1}$, $1\leq i\leq C(K)N\ln N$. Let $\mathbf{X} := \{X_1,\ldots, X_{C(K)N\ln N}\} $ and define 
	\[
	A_{\mathbf{X}} :=\bigcup_{i=1}^{C(K)N\ln N} \mathcal{E}(X_i,c_i(K)N^{-1}) ,
	\]
	where $ \mathcal{E}(X_i,c_i(K)N^{-1})  \subset \partial K$ denotes the geodesic ellipsoid centered at $X_i$ with volume $c_i(K)N^{-1}$.
	In particular, every ellipsoid contains a ball of radius 
	\[
	r_{N,K}:= c_1(K)\min_{x\in\partial K}\min_{1\leq i\leq n-1}\kappa_i(x)^{-1}N^{-\frac{1}{n-1}}.
	\]
	Now as in Lemma \ref{Zprop}, it suffices to prove that
	\[
	\Pr\left(\tilde Z \geq N^{-(0.5+\frac{2}{n-1})}\right) \leq \exp\left(-c_2(K)N^{-(0.5+\frac{2}{n-1})}\right) .
	\]
	In order to apply the same proof of Lemma \ref{Zprop},  we need  a covering of $\partial K$ by geodesic balls of radius $ \frac{1}{2}r_{N,K} $ such that each point is counted no more than $ c_4(K)  $ times. Indeed, Lemma \ref{ErodsRogeres} provides such a covering. Now the rest of the proof proceeds similarly to that of Lemma \ref{Zprop}, and we ultimately derive that
	\[
	\Pr(B) \leq  \exp\left(c_2(K)(\ln N)N^{0.5-\frac{2}{n-1}}-c(K)c_1(K)(\ln N)N^{0.5-\frac{2}{n-1}}\right).
	\]
	Choosing $c(K)$ to be large enough yields the lemma.
\end{proof}

%%%%%%%%%%%%%%%%%

% \subsection{The random variable $W$ equals zero with high probability}

Finally, we turn our attention to the random variable $W$. 

\begin{lemma}\label{WpropK}
	When $N$ is large enough, the polytope $P_{n,N}$ lies in $(1+2/n)K$ with probability at least $1-e^{-c(K,\mu)N}$.
\end{lemma}
The proof is similar to that of Lemma~ \ref{Wconcentration}, where now we replace the spherical caps by geodesic ellipsoids. We leave the details to the interested reader. 
% In the first step, we follow Lemma \ref{lemma1W} and show that if $\{X_1,\ldots,X_N\}$ is a $\frac{1}{n^2}$-net of $\partial K$, then $P \subset (1+\frac{2}{n})K$. In the second step we follow Lemma \ref{deltanet} and show that $\{X_1,\ldots,X_N\}$ is a $\frac{1}{n^2}$-net with probability $1-e^{-c_1(K)N}$. 


% \begin{lemma}
% 	Suppose that $\{X_1,\ldots,X_N\}$ is a $\frac{1}{n^2}$-net of $K$. Then $P_{n,N} \subset (1+\frac{2}{n})K$.
% \end{lemma}

% \begin{proof}
% 	Without loss of generality, we may assume that $o$ lies in the interior of $K$ (and thus $o$ also lies in the interior of $P_{n,N}$).  %Then $P\subset (1+\frac{2}{n})K$ if and only if $\partial((1+\frac{2}{n})K) \cap P_{n,N} = \varnothing$. 
% Suppose by way of contradiction that  there exists $v_0\in \partial K$ such that $(1+\frac{2}{n})v_0 \in P_{n,N}$. By the definition of $P_{n,N}$, this means that for all $1\leq i\leq N$,
% 	\[
% 	\langle (1+\tfrac{2}{n})v_0, \nu(X_i)\rangle \leq \langle X_i, \nu(X_i)\rangle.
% 	\]
% 	There exists $X_j$ such that $\|v_0 - X_j\| = \min_{1\leq i\leq N}\|v_0 - X_i\|$. Hence, the previous inequality yields
% 	\[
% 	0 \geq \langle (1+\tfrac{2}{n})v_0-X_j, \nu(X_j)\rangle
% 	=\langle (1+\tfrac{2}{n})(v_0-X_j+X_j)-X_j, \nu(X_j)\rangle.
% 	\]
% 	Thus, by the Cauchy-Schwarz inequality and the fact that $\{X_i\}_{i=1}^N$ is a $\frac{1}{n^2}$-net,
% 	\[
% 	\tfrac{2}{n} \langle X_j, \nu(X_j)\rangle \leq (1+\tfrac{2}{n})\langle X_j - v_0, \nu(X_j)\rangle \leq (1+\tfrac{2}{n})\|X_j-v_0\|<\tfrac{2}{n^2},
% 	\]
% 	a contradiction.
% \end{proof}

% \begin{lemma}
% 	Let $X_1,\ldots,X_N \stackrel{\text{i.i.d.}}{\sim} \sigma_{\partial K}$. For all sufficiently large $N$, the set $\mathcal N:=\{X_1,\ldots,X_N\}$ is a $ \frac{1}{n^2}$-net of $\partial K$ with probability at least $ 1-e^{-c_3(K)N}.$ 
% \end{lemma}
% The proof of this Lemma is similar to Lemma \ref{l

%%%%%%%%%%%%%}
\subsection*{Conclusion of the proof of Theorem \ref{geneThm}.}

Recalling that $|P_{n,N} \setminus K| = Y + Z+W$,  the rest of the proof of Theorem \ref{geneThm} proceeds in the same way as the proof of Corollary \ref{mainThm} in Subsection \ref{proofsectionmainthm}. \qed

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% BibTeX users please use one of
%\bibliographystyle{spbasic}      % basic style, author-year citations
\bibliographystyle{spmpsci}      % mathematics and physical sciences
%\bibliographystyle{spphys}       % APS-like style for physics
\bibliography{biblo}   % name your BibTeX data base


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\end{document}
%\end{document}
%% end of file template.tex
%\section{Introduction}
%\label{intro}
%Your text comes here. Separate text sections with
%\section{Section title}
%\label{sec:1}
%Text with citations \cite{RefB} and \cite{RefJ}.
%\subsection{Subsection title}
%\label{sec:2}
%as required. Don't forget to give each section
%and subsection a unique label (see Sect.~\ref{sec:1}).
%\subsection*{Paragraph headings} Use paragraph headings as needed.
%\begin{equation}
%a^2+b^2=c^2
%\end{equation}
%
%% For one-column wide figures use
%\begin{figure}
%	% Use the relevant command to insert your figure file.
%	% For example, with the graphicx package use
%	\includegraphics{example.eps}
%	% figure caption is below the figure
%	\caption{Please write your figure caption here}
%	\label{fig:1}       % Give a unique label
%\end{figure}
%%
%% For two-column wide figures use
%\begin{figure*}
%	% Use the relevant command to insert your figure file.
%	% For example, with the graphicx package use
%	\includegraphics[width=0.75\textwidth]{example.eps}
%	% figure caption is below the figure
%	\caption{Please write your figure caption here}
%	\label{fig:2}       % Give a unique label
%\end{figure*}
%%
%% For tables use
%\begin{table}
%	% table caption is above the table
%	\caption{Please write your table caption here}
%	\label{tab:1}       % Give a unique label
%	% For LaTeX tables use
%	\begin{tabular}{lll}
%		\hline\noalign{\smallskip}
%		first & second & third  \\
%		\noalign{\smallskip}\hline\noalign{\smallskip}
%		number & number & number \\
%		number & number & number \\
%		\noalign{\smallskip}\hline
%	\end{tabular}
%\end{table}
