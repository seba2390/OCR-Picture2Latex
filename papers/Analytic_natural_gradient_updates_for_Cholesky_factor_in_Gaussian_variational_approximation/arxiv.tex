\documentclass{statsoc}


\usepackage[a4paper]{geometry}
\usepackage{amssymb, amsmath, multirow}
\usepackage{etoolbox}

\makeatletter
\patchcmd{\@makecaption}
  {\parbox}
  {\advance\@tempdima-\fontdimen2\font\parbox} % decrease the width!
  {}{}
\makeatother  

\usepackage{graphicx, afterpage}
\usepackage[caption = false]{subfig} 
\usepackage{enumerate}
\usepackage{comment}
\usepackage{natbib, float}


\usepackage{epsfig, latexsym, url, xcolor, bbm}

\makeatletter
\patchcmd{\@makecaption}
  {\parbox}
  {\advance\@tempdima-\fontdimen2\font\parbox} % decrease the width!
  {}{}
\makeatother 


\usepackage{float} 
\floatstyle{plain}
\newfloat{Algorithm}{thp}{lop}
\floatname{Algorithm}{Algorithm}
\def\bibfont{\fontsize{9}{10}\selectfont}
\setlength{\bibsep}{0.0pt}
\RequirePackage[colorlinks,citecolor=blue,urlcolor=blue]{hyperref}
%\sectionfont{\large}
%\subsectionfont{\normalsize}

\newtheorem{theorem}{Theorem}
\newtheorem{corollary}{Corollary}
\newtheorem{definition}{Definition}
\newtheorem{lemma}{Lemma}
\newtheorem{proposition}{Proposition}

\newtheorem{thma}{Bonnet's Theorem (Stein's Lemma)}
\renewcommand\thethma{\unskip}

\newtheorem{thmb}{Price's Theorem}
\renewcommand\thethmb{\unskip}

% Definitions of handy macros can go here
\newcommand\ev{{\text{ev}} }   
\newcommand\df{{\text{d}}}   
\newcommand\e{{\rm e}}                             
\newcommand\E{{\text{E}}}           
\newcommand\dg{{\text{dg}}}         
\newcommand\Var{{\text{Var}}}   
\newcommand\blockdiag{{\text{blockdiag}}}                           
\newcommand\mL{{\mathcal{L}}}
\newcommand\mT{{\mathcal{T}}}
\newcommand\Cov{{\text{Cov}}}   
\newcommand\Cor{{\text{Cor}}}   
\newcommand\diag{{\text{diag}}}   
\newcommand\N{{\text{N}}} 
\newcommand\M{{\mathcal{M}}} 
\newcommand\G{{\mathcal{G}}} 
\newcommand\F{{\mathcal{F}}} 
\newcommand\tr{\text{tr}}   
\newcommand\logit{\text{logit}} 
\newcommand\KL{\text{KL}}   
\newcommand{\vc}{\text{vec}}  
\newcommand{\vech}{\text{vech}}   
\newcommand{\Prob}{\text{P}}   
\newcommand{\dH}{\bar{\bar{H}}}   

\newcommand{\ra}[1]{\renewcommand{\arraystretch}{#1}}
\sloppy
%\allowdisplaybreaks

\graphicspath{{img/}}


\title[Natural gradient updates in Gaussian variational approximation]{Analytic natural gradient updates for Cholesky factor in Gaussian variational approximation}
\author[Linda Tan]{Linda S. L. Tan}
\address{Department of Statistics and Data Science, National University of Singapore,
117546 Singapore.}
\email{statsll@nus.edu.sg}



\begin{document}

\begin{abstract}
Stochastic gradient methods have enabled variational inference for high-dimensional models. However, the steepest ascent direction in the parameter space of a statistical model is actually given by the natural gradient which premultiplies the widely used Euclidean gradient by the inverse Fisher information. Use of natural gradients can improve convergence, but inverting the Fisher information matrix is daunting in high-dimensions. In Gaussian variational approximation, natural gradient updates of the mean and precision of the normal distribution can be derived analytically, but do not ensure that the precision matrix remains positive definite. To tackle this issue, we consider Cholesky decomposition of the covariance or precision matrix, and derive analytic natural gradient updates of the Cholesky factor, which depend on either the first or second derivative of the log posterior density. Efficient natural gradient updates of the Cholesky factor are also derived under sparsity constraints representing different posterior correlation structures. As Adam's adaptive learning rate does not work well with natural gradients, we propose stochastic normalized natural gradient ascent with momentum. The efficiency of proposed methods are demonstrated using logistic regression and generalized linear mixed models.
\end{abstract}

\keywords{Gaussian variational approximation, Natural gradients, Cholesky factor, Positive definite constraint, Sparse precision matrix, Normalized stochastic gradient descent}


\section{Introduction}
Variational inference is fast and provides an attractive alternative to Markov chain Monte Carlo (MCMC) methods for approximating intractable posterior distributions in the Bayesian framework. Stochastic gradient methods \citep{Robbins1951} have further enabled variational inference for high-dimensional models and large data sets \citep{Hoffman2013, Salimans2013}. While Euclidean gradients are commonly used in optimizing the variational objective function, the direction of steepest ascent in the parameter space of statistical models, where distance between probability distributions is measured using the Kullback-Leibler (KL) divergence, is actually given by the natural gradient \citep{Amari1998}. Stochastic optimization based on natural gradients has been found to be more robust with the ability to avoid or escape plateaus, resulting in faster convergence \citep{Rattray1998}. \cite{Martens2020} shows that natural gradient descent can be seen as a second order optimization method, with the Fisher information taking the place of the Hessian and having  more favorable properties.

The natural gradient is obtained by premultiplying the Euclidean gradient with the inverse Fisher information, whose computation can be complex. However, sometimes natural gradient updates can be simpler than Euclidean ones, such as for conjugate exponential family models \citep{Hoffman2013}. If the variational density is in the minimal exponential family \citep{Wainwright2008}, then the natural gradient of the objective function with respect to the natural parameter is just the Euclidean gradient with respect to the mean of the sufficient statistics \citep{Khan2017}. In Gaussian variational approximation \citep{Opper2009}, the true posterior is approximated by a normal density lying in the minimal exponential family. Hence, natural gradient update of the natural parameter can be derived analytically. Combined with the theorems of \cite{Bonnet1964} and \cite{Price1958}, stochastic natural gradient updates of the mean and precision, which depend respectively on the first and second derivatives of the log posterior density \citep{Khan2018} are obtained. However, the update for the precision matrix does not ensure that it remains positive definite. 

Various approaches have been proposed to handle the positive definite constraint. \cite{Khan2017} use a back-tracking line search, which can lead to slow convergence. \cite{Ong2018a} parametrize the Gaussian in terms of the mean and Cholesky factor of the precision  and derive the Fisher information analytically, but compute the natural gradients by solving a linear system numerically. Using chain rule, \cite{Salimbeni2018} show that the inverse Fisher information in parametrizations which are one-one transformations of the natural parameter can be computed as a Jacobian-vector product via automatic differentiation. \cite{Tran2020} consider a factor structure for the covariance and compute natural gradients using a conjugate gradient linear solver based on a block diagonal approximation of the Fisher information. \cite{Lin2020} use Riemannian gradient descent with a retraction map (derived using a second-order approximation of the geodesic) to obtain an update of the precision that includes an additional term to ensure positive definiteness. \cite{Tran2020} optimize on the manifold of symmetric positive definite matrices and derive an update for the covariance based on an approximation of the natural gradient and a popular retraction for the manifold.

We consider Cholesky decompositions of the covariance or precision and derive the inverse Fisher information in closed form. Analytic natural gradient updates for the Cholesky factor are then obtained in terms of either the first or second order derivative of the log posterior via Stein's Lemma \citep{Stein1981}.  A powerful tool in statistics, Stein's Lemma relates the mean of the function of a normally distributed random variate with the mean of its derivative. \cite{Lin2019a} showed how Stein's Lemma can be used to derive the identities in Bonnet's and Price's theorems, and reparametrizable gradient identities for exponential family mixture distributions. Extending the results of \cite{Lin2019a}, we use Stein's Lemma to derive unbiased  first or second order gradient estimates of the variational objective with respect to the Cholesky factor of the covariance or precision. Close to the mode, when the log posterior can be approximated quadratically, second order gradient estimates have smaller variance than first order estimates, that is almost negligible. Hence second order gradient estimates can improve convergence, although first order estimates are more efficient computationally and storage wise. Compared with updates of the mean and Cholesky factor based on Euclidean gradients \citep{Titsias2014}, natural gradient updates require additional computation, but can potentially improve the convergence rate significantly. 

Gaussian variational approximation has been widely applied in many contexts such as likelihood-free inference using synthetic likelihood \citep{Ong2018a}, Bayesian neural networks in deep learning \citep{Khan2018}, exponential random graph models for network modeling \citep{Tan2020a} and factor copula models \citep{Nguyen2020}. To accommodate constrained, skewed or heavy-tailed variables, a Gaussian variational approximation can be specified for variables which have first undergone independent parametric transformations, resulting in a Gaussian copula variational approximation. \cite{Han2016} use a Bernstein polynomial transformation while \cite{Smith2020} employ the transformation of \cite{Yeo2000} and the Tukey g-and-h distribution \citep{Yan2019} to improve the normality and symmetry of original variables. Our natural gradient updates can also be used in these contexts. 

In high-dimensional models, sparsity constraints can be imposed on the covariance matrix by assuming a (block) diagonal structure according to the variational Bayes restriction \citep{Attias1999}. Alternatively, the precision matrix can be assumed to adopt a structure reflecting conditional independence in the true posterior. The automatic differentiation variational inference algorithm in Stan \citep{Kucukelbir2017} allows the user to fit Gaussian variational approximations with a diagonal or full covariance matrix and provides a library of transformations to convert constrained variables onto the real line. However, it does not permit other sparsity structures and uses Euclidean gradients to update the Cholesky factor in stochastic gradient ascent. While sparsity constraints can be easily imposed on Euclidean gradients by setting relevant entries to zero, the same may not apply to natural gradients due to premultiplication by the Fisher information. We further derive efficient natural gradient updates in two cases, (i) the covariance matrix has a block diagonal structure corresponding to the product density assumption in variational Bayes and (ii) the precision matrix has a sparse structure mirroring the posterior conditional independence in a hierarchical model where local variables are independent conditional on global variables \citep{Tan2018}. 

Finally, we demonstrate that adaptive learning rate computed using Adam \citep{Kingma2015}, which has achieved widespread success in deep learning, is incompatible with natural gradients. This is because Adam, which can be interpreted as a sign-based approach with per dimension variance adaptation \citep{Balles2018}, neglects largely the scale information contained in natural gradients. As an alternative, we propose stochastic normalized natural gradient ascent coupled with heavy-ball momentum \citep{Polyak1964}. The same stepsize is used for all variables so that scaling information in the natural gradients is preserved, and the stepsize increases automatically as the algorithm converges to a local mode due to reduction in norm of the gradients. \cite{Hazan2015} showed that stochastic normalized gradient descent is suited to non-convex optimization problems as it is able to overcome plateaus and cliffs in the objective function. While \cite{Cutkosky2020} also considers normalized stochastic gradient descent with momentum, our approach differs in the consideration of natural rather than Euclidean gradients and normalization of the natural gradient instead of the momentum. The proposed algorithm is shown to converge if the objective function is $L$-Lipschitz smooth with bounded gradients. We investigate the performance of natural gradient updates using logistic regression and generalized linear mixed models. 

Section \ref{sec_notation} introduces the notation and Section \ref{sec_SVI} describes stochastic variational inference. We introduce the natural gradient in Section \ref{sec_natgrad} and Section \ref{sec_GVA} presents natural gradient updates of the mean and covariance/precision matrix in Gaussian variational approximation. Section \ref{sec_Gauss} derives natural gradient updates in terms of the mean and Cholesky factor of the covariance/precision matrix, while Section \ref{sec imposing sparsity} consider various sparsity constraints. The normalized stochastic natural gradient ascent algorithm with momentum is described in Section \ref{sec_Normalized SGD}, and Section \ref{sec_applications} presents the experimental results. We conclude with a discussion in Section \ref{sec_Conclusion}. 



\section{Notation} \label{sec_notation}
For a square matrix $A$, let $\bar{A}$ and $\dg(A)$ be the lower triangular and diagonal matrix derived from $A$ respectively by replacing all supradiagonal and  non-diagonal elements by zero. We define
\[
\bar{\bar{A}} = \bar{A} - \dg(A)/2.
\]
Let $\vc(A)$ denote the vector obtained by stacking the columns of $A$ in order from left to right, and $K$ be the commutation matrix such that $K \vc(A) = \vc(A^T)$. Let $\vech(A)$ be the vector obtained from $\vc(A)$ by omitting supradiagonal elements. If $A$ is symmetric, then $D \vech(A) = \vc(A)$, where $D$ is the duplication matrix, and $D^+ \vc(A) = \vech(A)$ where $D^+ = (D^T D)^{-1} D^T$ is the Moore-Penrose inverse of $D$. Let $L$ be the elimination matrix such that $L \vc(A) = \vech(A)$. If $A$ is lower triangular, then $L^T \vech(A) = \vc(A)$. More details can be found in \cite{Magnus1980, Magnus2019}. 



\section{Stochastic variational inference} \label{sec_SVI}
Let $p(y|\theta)$ denote the likelihood of unknown variables $\theta \in \mathbb{R}^d$ given observed data $y$. Suppose $p(\theta)$ is a prior for $\theta$ and the posterior distribution $p(\theta|y) = p(y|\theta)p(\theta)/p(y)$ is intractable. In variational inference, $p(\theta|y)$ is approximated by a more tractable density $q_\lambda(\theta)$ with parameter $\lambda$, that is chosen to minimize the KL divergence between $q_\lambda(\theta)$ and $p(\theta|y)$. As
\[
\log p(y) = \underbrace{\int q_\lambda(\theta) \log \frac{q_\lambda(\theta)}{p(\theta|y)} \df \theta}_{\text{KL divergence}} + \underbrace{\int q_\lambda(\theta) \log \frac{p(y, \theta)}{q_\lambda(\theta)} \df \theta}_{\text{Evidence lower bound}},
\]
minimizing the KL divergence is equivalent to maximizing the lower bound on the log marginal likelihood, $\mL(\lambda) = \E_q [h(\theta)]$, with respect to $\lambda$, where $h(\theta) =\log p(y, \theta) - \log   q_\lambda (\theta)$. When $\mL$ is intractable, stochastic gradient ascent can be used for optimization. Starting with an initial estimate of $\lambda$, an update 
\[
\lambda \leftarrow \lambda + \rho_t \, \widehat{\nabla}_{\lambda} \mL,
\]
is performed at iteration $t$, where $\widehat{\nabla}_{\lambda} \mL$ is an unbiased estimate of the Euclidean gradient $\nabla_{\lambda} \mL$. Applying chain rule, the Euclidean gradient is $\nabla_\lambda \mL = \int \{\nabla_\lambda q_\lambda (\theta)\} h(\theta) \df \theta$, since $\E_q[\nabla_\lambda \log q_\lambda (\theta)] = 0$. Under regularity conditions, the algorithm will converge to a local maximum of $\mL$ if the stepsize $\rho_t$ satisfies $\sum_{t=1}^\infty \rho_t = \infty$ and $\sum_{t=1}^\infty \rho_t^2 < \infty$ \citep{Spall2003}.



\section{Natural gradient} \label{sec_natgrad}
Our search for the optimal $\lambda$ is performed in the parameter space of $q_\lambda(\theta)$, which is not flat but has its own curvature, and the Euclidean metric may not be appropriate for measuring the distance between densities indexed by different $\lambda$s. For instance, although N(0, 1000.1) and N(0, 1000.2) are similar, while N(0, 0.1) and N(0, 0.2) are vastly different, both pairs have the same Euclidean distance \citep{Salimbeni2018}. \cite{Amari2016} defines the distance between $\lambda$ and $\lambda +d\lambda$ as $2\KL(q_\lambda \| q_{\lambda + d\lambda})$ instead for a small $d\lambda$. Using a second order Taylor series expansion, this is approximately equal to 
\[
2\E_q \left[ \log q_\lambda(\theta) - \{ \log q_\lambda(\theta) + d\lambda^T  \nabla_\lambda \log q_\lambda(\theta) + \tfrac{1}{2} d\lambda^T  \nabla_\lambda^2 \log q_\lambda (\theta) d\lambda \} \right] 
= d\lambda^T  F_\lambda d\lambda,
\]
where $F_\lambda= -\E_q [\nabla_\lambda^2 \log q_\lambda (\theta)]$ is the Fisher information of $q_\lambda(\theta)$. Thus, the distance between $\lambda$ and $\lambda + d\lambda$ is not $d\lambda^T  d\lambda$ as in a Euclidean space, but $d\lambda^T  F_\lambda d\lambda$. The set of all distributions $q_\lambda(\theta)$ is a manifold and the KL divergence provides the manifold with a Riemannian structure, with norm $\| d\lambda \|_{F_\lambda} = \sqrt{d\lambda^T  F_\lambda d\lambda}$ if $F_\lambda$ is positive definite.

The steepest ascent direction of $\mL$ at $\lambda$ is defined as the vector $a$ that maximizes $\mL(\lambda + a)$, where $\|a\|_{F_\lambda}$ is equal to a small constant $\epsilon$ \citep{Amari1998}. Using Lagrange multipliers, let
\[
\begin{aligned}
\mathfrak{L} &= \mL(\lambda + a) - \alpha (\|a\|^2_{F_\lambda} - \epsilon^2) 
\approx \mL(\lambda) +  a^T  \nabla_\lambda \mL - \alpha (a^T  F_\lambda a -\epsilon^2).
\end{aligned}
\]
Setting $\nabla_{a} \mathfrak{L}  \approx \nabla_\lambda \mL - 2\alpha F_\lambda a$ to zero, we obtain $a = \epsilon (\widetilde{\nabla}_\lambda \mL)/ \|\widetilde{\nabla}_\lambda \mL \|_{F_\lambda}$, and thus the steepest ascent direction in the parameter space is given by the natural gradient, 
\[
\widetilde{\nabla}_\lambda \mL = F_\lambda ^{-1} \nabla_\lambda \mL.
\]
Replacing the unbiased Euclidean gradient estimate with that of the natural gradient results in the stochastic natural gradient update,
\[
\lambda \leftarrow \lambda + \rho_t \, F_{\lambda}^{-1} \,\widehat{\nabla}_{\lambda} \mL.
\]

Another motivation for the use of natural gradient is that, provided $q_\lambda (\theta)$ is a good approximation to $p(\theta|y)$, then close to the mode,
\[
\nabla^2_\lambda \mL = \int \nabla_\lambda^2 q_\lambda(\theta) \left\{\log p(y, \theta) - \log q_\lambda(\theta) \right\}  \df \theta - F_\lambda \approx -F_\lambda.
\]
since the first term is approximately zero. Thus the natural gradient update resembles Newton-Raphson, a {\em second-order} optimization method, where $\lambda \leftarrow \lambda - (\nabla_\lambda^2 \mL)^{-1} \nabla_\lambda \mL$. Finally, if $\xi \equiv \xi(\lambda)$ is a smooth invertible reparametrization of $q_\lambda(\theta)$, then 
\begin{equation} \label{reparametrized natural gradient}
\widetilde{\nabla}_\xi \mL = F_\xi^{-1} \nabla_\xi \mL 
= ( J F_\lambda J^T )^{-1} J \nabla_\lambda \mL
= (\nabla_\lambda \xi)^T  \widetilde{\nabla}_\lambda \mL,
\end{equation}
where $J = \nabla_\xi \lambda$ \citep{Lehmann1998}. 



\section{Gaussian variational approximation} \label{sec_GVA}
A popular option for $q_\lambda(\theta)$ is the multivariate Gaussian $\N(\mu, \Sigma)$, which is a member of the exponential family, and can be written as
\begin{equation} \label{exp family}
q_\lambda(\theta) = \exp \left\{ s(\theta)^T  \lambda -  A(\lambda) \right\}, 
\end{equation}
where $s(\theta) = (\theta^T ,\vech(\theta\theta^T )^T )^T $ is the sufficient statistic, $\lambda = (\mu^T \Sigma^{-1},  -\frac{1}{2} \vc(\Sigma^{-1})^T D)^T $ is the natural parameter and $A(\lambda) = \tfrac{1}{2} \mu^T  \Sigma^{-1} \mu + \tfrac{1}{2} \log |\Sigma| + \frac{d}{2} \log(2\pi)$ is the log-partition function. For a density in \eqref{exp family}, $m = \E [s(\theta)] = \nabla_\lambda A(\lambda)$ and $\Var[s(\theta)] = \nabla_\lambda^2 A(\lambda)  = \nabla_\lambda m = F_\lambda$. \cite{Khan2017} showed that by applying chain rule,  $\nabla_{\lambda} \mL  = \nabla_\lambda m  \nabla_m \mL = F_\lambda \nabla_m \mL$, which implies that the natural gradient with respect to the {\em natural parameter},
\begin{equation*} 
\widetilde{\nabla}_\lambda \mL = \nabla_m \mL = \begin{bmatrix} \nabla_\mu \mL - 2(\nabla_{\Sigma}\mL) \mu \\ D^T  \vc(\nabla_{\Sigma} \mL)
 \end{bmatrix},
\end{equation*}
where $\vc(\nabla_{\Sigma} \mL) = \nabla_{\vc(\Sigma) } \mL$. Thus $\widetilde{\nabla}_\lambda \mL$ can be obtained without finding the inverse Fisher information explicitly. Derivation details are given in the supplement S1 and the natural gradient update for $\lambda$ is shown in Table \ref{Tab a}. Note that $\Sigma$ must be updated first as the subsequent update of $\mu$ depends on the updated $\Sigma$. 

\begin{table}
\caption{\label{Tab a} Euclidean and natural gradient updates of $\mu$ and $\Sigma$ for different parametrizations.} 
\centering \ra{1.2}
\begin{footnotesize}
\fbox{\begin{tabular}{cccc} 
Euclidean   & \multicolumn{3}{c}{Natural gradient update for} \\
gradient update &  $\kappa$ & $\xi$ & $\lambda$ (natural parameter)   \\ \hline
$\mu \leftarrow \mu + \rho_t \nabla_\mu \mL$ & $\mu \leftarrow \mu + \rho_t \Sigma \nabla_\mu \mL$ & $\mu \leftarrow \mu + \rho_t \Sigma \nabla_\mu \mL$ & $\Sigma^{-1} \leftarrow \Sigma^{-1} - 2 \rho_t  \nabla_\Sigma \mL$
\\
$\Sigma \leftarrow \Sigma + \rho_t  \nabla_\Sigma \mL$ &
$\Sigma \leftarrow \Sigma + 2 \rho_t  \Sigma \nabla_\Sigma \mL \Sigma$ & $\Sigma^{-1} \leftarrow \Sigma^{-1} - 2 \rho_t  \nabla_\Sigma \mL$ & $\mu \leftarrow \mu + \rho_t \Sigma \nabla_\mu \mL$
\end{tabular}}
\end{footnotesize}
\end{table}

This update of $\Sigma$ is not guaranteed to be positive definite but it performs well in practice, if the starting point is sufficiently close to the optimum and $\mL$, or more specifically, $\E_q[\log p(y, \theta)]$, can be computed in closed form or  estimated using quadrature. In fact, the stepsize can be as large as one. In nonconjugate variational message passing \citep[][]{Knowles2011}, the variational density is of the form in \eqref{exp family} and $\nabla_\lambda \mL = \nabla_\lambda \E_q [\log p(y, \theta)] - F_\lambda \lambda$, is set to zero to derive the  update, $\lambda \leftarrow F_\lambda^{-1} \E_q [\log p(y, \theta)]$. When $q_\lambda(\theta)$ is Gaussian, this fixed point iteration update is identical to the natural gradient update with stepsize one \citep{Tan2013, Wand2014}.

Suppose we consider some other parametrizations, $\kappa = (\mu^T , \vech(\Sigma)^T )^T $ and $\xi = (\mu^T , \vech(\Sigma^{-1})^T )^T $, which are one-one transformations of $\lambda$. The natural gradients $\widetilde{\nabla}_\kappa \mL$ and $\widetilde{\nabla}_\xi \mL$ are derived using \eqref{reparametrized natural gradient} in the supplement S1, and corresponding updates for $\kappa$ and $\xi$ are shown in Table \ref{Tab a}. The update for $\xi$ is almost identical to $\lambda$, except that the updates of $\mu$ and $\Sigma$ for $\xi$ are independent and can be performed simultaneously, while the update of $\mu$ for $\lambda$ relies on the updated $\Sigma$. The Fisher information of $\kappa$ and $\xi$ are block diagonal matrices, which imply that $\kappa$ and $\xi$ are the (usually desired) orthogonal parametrizations. However, it is only through the non-orthogonal parametrization $\lambda$, that we discover that the updated $\Sigma$ can be used to improve the update of $\mu$, due to the correlation between $\Sigma^{-1} \mu$ and $\Sigma^{-1}$. 


\subsection{An illustration using Poisson  loglinear model} \label{sec_Poisson}
To gain insights on how the updates in Table \ref{Tab a} compare in performance, we consider the loglinear model for counts. Suppose $y_i \sim \text{Poisson}(\delta_i)$ for $i=1, \dots, n$, and $\log \delta_i = x_i^T  \theta$, where $x_i$  and $\theta$ are $d \times 1$ vectors of covariates and regression coefficients respectively. Consider a prior, $\theta \sim \N(0, \sigma_0^2 I)$, and a Gaussian approximation $\N(\mu, \Sigma)$ of the true posterior of $\theta$. The lower bound $\mL$ is tractable and hence its curvature can be studied easily. Expressions of $\mL$, $\nabla_\mu \mL$ and $\nabla_\Sigma \mL$ are given in the supplement S2. 

To visualize the gradient vector field, we consider intercept-only models and write $\Sigma$ as $\sigma^2$. Variational parameters $(\mu, \sigma^2)$ are estimated using gradient ascent and the largest possible stepsize $\rho_t \in \{1, 0.1, 0.01, ...\}$ is used in each iteration, provided that the update of $\sigma^2$ is positive and $\mL$ is increasing. We use as observations the number of satellites ({\tt Sa}) of 173 female horseshoe crabs given in Table 3.2 of \cite{Agresti2018} and set $\sigma_0^2 = 100$. For each approach, Figure \ref{Fig a} shows the gradient vector field and gradient ascent trajectories from three starting points marked by squares. $\mL$ is maximized at $(\mu, \sigma^2) = (1.07, 0.002)$, which is marked by a circle. The number of iterations to converge and the smallest value of $\rho_t$ used are reported in Table \ref{Tab b}.

\begin{figure}[htb!]
\centering
\includegraphics[height=107pt, angle=0]{satellite.pdf}
\caption{Gradient vector field and trajectories for gradient ascent from three starting points.}
\label{Fig a}
\end{figure}

\begin{table}
\caption{\label{Tab b} Number of iterations in gradient ascent and smallest $\rho_t$ used.} 
\centering
\fbox{\begin{tabular}{ccccc}
Starting point & Euclidean ($\kappa$)  & Natural ($\kappa$) & Natural ($\xi$) & Natural ($\lambda$) \\ \hline
(0, 0.1) & 141 (1e-05) & 15 (0.01) & 11 (0.01) & 6 (1) \\ 
(0.5, 0.02) & 107 (1e-06) & 12 (0.1) & 8 (0.1) & 5 (1) \\ 
(2, 0.01) & 115 (1e-06) & 9 (0.01) & 8 (0.1) & 5 (1) \\ 
\end{tabular}}
\end{table}

The first two plots show the contrast between Euclidean and natural gradient vector fields for the $(\mu, \sigma^2)$ parametrization, particularly when $\sigma^2$ is close to zero. While natural gradients are collectively directed at the mode, Euclidean gradients have much stronger vertical components, causing zigzag trajectories that result in longer routes. The number of iterations required for Euclidean gradient ascent is an order of magnitude larger than natural gradient ascent, and a much smaller stepsize has to be used (at some point) to avoid a negative $\sigma^2$ update or $\mL$ decreasing. Natural gradient ascent for $\lambda$ is most efficient, where a stepsize as large as 1 can be used from all starting points, indicating that {\em reversing} the order of $\mu$ and $\sigma^2$ updates leads to significant improvement.



\section{Natural gradient updates for mean and Cholesky factor} \label{sec_Gauss}
In many applications, the lower bound cannot be computed analytically and stochastic gradient ascent based on updating the mean and Cholesky factor of the covariance/precision matrix is performed, as this allows optimization without positive definite constraints, more flexibility in choice of stepsize and reduction in computation/storage costs. However, existing updates for Cholesky factors are based on Euclidean gradients \citep{Titsias2014, Tan2018}, and we seek to derive the natural gradient counterparts. We consider two parametrizations based on Cholesky decomposition of the covariance or precision matrix. The first is $\lambda = (\mu^T , \vech(C)^T )^T$ where $\Sigma = CC^T$ and the second is $\lambda = (\mu^T , \vech(T)^T )^T$ where $\Sigma^{-1} = TT^T$, and $C$ and $T$ are lower triangular matrices. In these cases, $\lambda$ is not the natural parameter and we need to find the inverse Fisher information explicitly to get the natural gradient. The Fisher information for both parametrizations are block diagonal matrices with the same form. Hence the inverse can be found using a common result in (ii) of Lemma \ref{lem inv}, while (iii) is useful in simplifying the natural gradient $F_\lambda^{-1} \nabla_\lambda \mL$. The inverse Fisher information and natural gradient for these two parametrizations are presented in Theorem \ref{thm inv}. 

\begin{lemma} \label{lem inv}
Let $\Lambda$ be a $d \times d$ lower triangular matrix  and 
\[
\mathfrak{I}(\Lambda) = L\{ (\Lambda^{-1} \otimes  \Lambda^{-T })K + I_d \otimes \Lambda^{-T }\Lambda^{-1}  \} L^T.
\]
If $N = (K+ I_{d^2})/2$, then 
\begin{enumerate}[(i)]
\item $\mathfrak{I}(\Lambda) = 2 L ( I_d \otimes \Lambda^{-T })N ( I_d \otimes \Lambda^{-1})L^T $,
\item $\mathfrak{I}(\Lambda)^{-1} = \tfrac{1}{2}L ( I_d \otimes \Lambda) L^T (L N L^T )^{-1}L (I_d \otimes \Lambda^T ) L^T $ and 
\item $\mathfrak{I}(\Lambda)^{-1} \vech(G) = \vech(\Lambda \dH)$ for any $d \times d$ matrix $G$, where $H = \Lambda^T  \bar{G}$.
\end{enumerate}
\end{lemma}

\begin{theorem} \label{thm inv}
Let $\nabla_{\vech(\Lambda)} \mL = \vech(G)$ and $H = \Lambda^T  \bar{G}$, where $\Lambda = C$ for $\lambda = (\mu^T , \vech(C)^T )^T$ and $\Lambda = T$ for $\lambda = (\mu^T , \vech(T)^T )^T$. Then 
\[
F_\lambda= \begin{bmatrix} \Sigma^{-1} & 0 \\ 0 & \mathfrak{I}(\Lambda) \end{bmatrix}, \quad 
F_\lambda^{-1}= \begin{bmatrix} \Sigma & 0 \\ 0 & \mathfrak{I}(\Lambda)^{-1} \end{bmatrix} \quad \text{and} \quad
\widetilde{\nabla}_\lambda \mL = \begin{bmatrix} \Sigma \nabla_\mu \mL \\ \vech(\Lambda \dH) \end{bmatrix}.
\]
Hence, the natural gradient update at iteration $t$ is 
\[
\Lambda^{(t+1)} = \Lambda^{(t)} + \rho_t \Lambda^{(t)} \dH^{(t)},
\quad 
\mu^{(t+1)} = \mu^{(t)} + \rho_t \Sigma^{(t)} \nabla_\mu \mL^{(t)}.
\]
\end{theorem}

Inspired by the superior performance the natural parameter has compared to other orthogonal parametrizations in Section \ref{sec_Poisson}, we consider a one-one transformation of $\lambda = (\mu^T , \vech(T)^T )^T$ in Corollary \ref{cor inv} akin to the natural parameter. It reveals that instead of updating $\mu$ and $T$ {\em independently} as in Theorem \ref{thm inv}, the updated $T$ can be used to update $\mu$. Unfortunately, it is not possible to obtain similar improved updates for $C$. Proofs of Lemma \ref{lem inv}, Theorem \ref{thm inv} and Corollary \ref{cor inv} are given in the supplement S3. 

\begin{corollary} \label{cor inv}
Let $\xi = ((T^T \mu)^T, \vech(T)^T)^T$, $\nabla_{\vech(T)} \mL = \vech(G)$ and $H = T^T \bar{G}$. Then $\widetilde{\nabla}_\xi \mL= ((T^{-1} \nabla_\mu \mL + \dH^T T^T \mu)^T, \vech(T \dH)^T)^T$ and the natural gradient update at iteration $t$ is 
\[
T^{(t+1)} = T^{(t)} + \rho_t T^{(t)}\dH^{(t)},  
\quad 
\mu^{(t+1)} = \mu^{(t)} + \rho_t {T^{(t+1)}}^{-T} {T^{(t)}}^{-1} \nabla_\mu \mL^{(t)}.
\]
\end{corollary}

To investigate the difference between updates of $\mu$ and $T$ in Theorem \ref{thm inv} and Corollary \ref{cor inv}, we consider the Poisson loglinear model in Section \ref{sec_Poisson} again, this time fitting model 1: {\tt Sa $\sim$ Width}, and model 2: {\tt Sa $\sim$  Color + Width}. The largest stepsize $\rho_t \in \{1, 0.1, 0.01, ...\}$ is used provided $\mL$ is increasing. Figure \ref{Fig b} shows that updates in Corollary \ref{cor inv} are superior as they converge faster and are more resilient to larger stepsize.

\begin{figure}[htb!]
\centering
\includegraphics[height=107pt, angle=0]{satellite2.pdf}
\caption{Stepsize $\rho_t$ used at each iteration of natural gradient update of $\mu$ and $T$ based on Theorem \ref{thm inv} and Corollary \ref{cor inv}.}
\label{Fig b}
\end{figure}

\subsection{Stochastic natural gradient updates}
 In applications where $\nabla_\lambda \mL$ is not analytic, we can perform stochastic natural gradient ascent using an unbiased estimate.  For updates of $\mu$ and $\Sigma$, \cite{Khan2018} invoke the Theorems of \cite{Bonnet1964} and \cite{Price1958} to obtain unbiased estimates of $\nabla_\mu \mL$ and $\nabla_{\Sigma}\mL$. The theorems below and their proofs are given in \cite{Lin2019a}, and ACL is an abbreviation for absolute continuity on almost every straight line \citep{Leoni2017}. The second equality  in Bonnet's Theorem is also known as Stein's Lemma \citep{Stein1981}. As $\mL = \E_q [h(\theta)]$, if $\theta$ is a sample generated from $q_\lambda(\theta)$ at iteration $t$, then the stochastic natural gradient update of the natural parameter $\lambda$ is   
\[
\Sigma^{-1} \leftarrow \Sigma^{-1} - \rho_t \nabla^2_\theta h(\theta), 
\quad 
\mu \leftarrow \mu + \rho_t \Sigma\nabla_{\theta} h(\theta).
\]


\begin{thma} If $\theta \sim N(\mu, \Sigma)$ and $h: \mathbbm{R}^d \rightarrow \mathbbm{R}$ is locally ACL and continuous, then $\nabla_\mu  \E_q [h(\theta)]  = \E_q \left[\Sigma^{-1} (\theta-\mu) h(\theta) \right] =  \E_q[\nabla_\theta h(\theta)]$.
\end{thma}

\begin{thmb}
If $\theta \sim N(\mu, \Sigma)$, $h: \mathbbm{R}^d \rightarrow \mathbbm{R}$ is continuously differentiable, $\nabla_\theta h(\theta)$ is locally ACL and $E_q[h(\theta)]$ is well-defined, then
\[
\nabla_\Sigma  \E_q [h(\theta)] = \tfrac{1}{2} \E_q \left[\Sigma^{-1}  (\theta-\mu) \nabla_\theta h(\theta)^T \right] = \tfrac{1}{2} \E_q \left [\nabla_\theta^2 h(\theta) \right].
\]
\end{thmb}


For updates of Cholesky factors, we cannot apply Price's Theorem directly but there are several alternatives. The score function method uses $ \nabla_\lambda q_\lambda (\theta) = q_\lambda (\theta) \nabla_\lambda \log q_\lambda (\theta)$ to write $\nabla_\lambda \mL =  \E_q[\nabla_\lambda \log q_\lambda (\theta) h(\theta)]$. While widely applicable, such estimates tend to have high variance leading to slow convergence, and further variance reduction techniques are required \citep{Paisley2012, Ranganath2014, Ruiz2016b}. The reparametrization trick \citep{Kingma2014} introduces a differentiable transformation $\theta = \mathcal{T}_\lambda(z)$ so that the density $\phi(z)$ of $z$ is independent of $\lambda$. Making a variable substitution and applying chain rule,
$\nabla_\lambda \mL = \E_{\phi} [\nabla_\lambda \theta \,\nabla_\theta h(\theta)]$. Gradients estimated using the reparametrization trick typically have lower variance than the score function method \citep{Xu2019}, but yields unbiased estimates of $\nabla_{\vech(C)} \mL$ and $\nabla_{\vech(C)} \mL$ that only make use of the {\em first derivative} of $h(\theta)$ unlike Price's Theorem. 

We propose alternative unbiased estimates in terms of the {\em second derivative} of $h(\theta)$ in Theorem \ref{thm stein}. Our results extend  Bonnet's and Price's Theorems to gradients with respect to the Cholesky factor of the covariance/precision matrix. Lemma \ref{lem stein} is instrumental in proving Theorem \ref{thm stein} and all proofs are given in the supplement S3. 

\begin{lemma} \label{lem stein}
 If $\theta \sim N(\mu, \Sigma)$ and $h: \mathbbm{R}^d \rightarrow \mathbbm{R}$ is locally ACL and continuous, then $\E_q \left[ \{\Sigma^{-1} (\theta-\mu)  (\theta - \mu) ^T  - I_d \}  h(\theta)\right] =  \E_q \left[ \nabla_\theta h(\theta)(\theta - \mu) ^T \right] $.
\end{lemma}


\begin{theorem} \label{thm stein}
Suppose $\theta \sim N(\mu, \Sigma)$, and $CC^T$ and $TT^T$ are Cholesky decompositions of $\Sigma$ and $\Sigma^{-1}$ respectively, where $C$ and $T$ are lower triangular matrices. Let $h: \mathbbm{R}^d \rightarrow \mathbbm{R}$ be continuously differentiable,  and $h$ and $\nabla_\theta h(\theta)$ be locally ACL. Then
\begin{enumerate} [(i)]
\item $\nabla_{\vech(C)} \mL = \E_q \vech(\G_1) =  \E_q \vech(\F_1)$, where $\G_1 =  \nabla_\theta h(\theta) (\theta-\mu)^T C^{-T} $ and $\F_1 = \nabla_\theta^2 h(\theta) C$.
\item $\nabla_{\vech(T)}  \mL = \E_q \vech(\G_2) = \E_q   \vech(\F_2)$, where  $\G_2 = - (\theta - \mu) v^T$, $v = T^{-1} \nabla_\theta h(\theta)$ and $\F_2 = -\Sigma \nabla_\theta^2 h(\theta) T^{-T}$. 
\end{enumerate} 
\end{theorem}

Results in Theorem \ref{thm stein} are obtained by first finding $\nabla _{\vech(C)} \mL$ and $\nabla _{\vech(C)} \mL$ using the score function method, which yields unbiased estimates in terms of $h(\theta)$. Applying Bonnet's Theorem (Stein's Lemma), we get estimates in terms of $\nabla_\theta h(\theta)$, which are {\em identical} to those obtained from the reparametrization trick. Finally, estimates in terms of $\nabla_\theta^2 h(\theta)$ are obtained using Price's Theorem. The reparametrization trick is thus connected to the score function method via Stein's Lemma. Since Price's Theorem can be derived from Bonnet's Theorem by applying Stein's Lemma, we are applying Stein's Lemma repeatedly to obtain unbiased estimates in terms of even higher derivatives of $h(\theta)$. Second order estimates are undoubtedly more expensive computationally, but they can be advantageous in some situations where $\nabla^2_\theta h(\theta)$ is not excessively complex, as they are more stable when close to the optimum. Suppose $\ell(\theta) = \log p(y, \theta)$ is well approximated by a second order Taylor expansion about its mode $\hat{\theta}$. Then 
\[
\begin{gathered}
h(\theta) \approx \ell(\hat{\theta}) + \tfrac{1}{2} (\theta - \hat{\theta})^T \nabla_{\theta}^2 \ell(\hat{\theta}) (\theta - \hat{\theta}) + \tfrac{d}{2}\log(2\pi) + \tfrac{1}{2}\log|\Sigma| + \tfrac{1}{2}(\theta - \mu) \Sigma^{-1} (\theta - \mu), \\
\nabla_\theta h(\theta) \approx \nabla_{\theta}^2 \ell(\hat{\theta}) (\theta - \hat{\theta})  + \Sigma^{-1} (\theta - \mu),  \qquad 
\nabla_{\theta}^2 h(\theta) \approx \nabla_{\theta}^2 \ell(\hat{\theta}) + \Sigma^{-1},
\end{gathered}
\]
which leads to the estimators,
\[
\begin{gathered}
\G_1 \approx \{\nabla_{\theta}^2 \ell(\hat{\theta}) (\theta - \hat{\theta})   + \Sigma^{-1} (\theta - \mu)\}(\theta-\mu)^T C^{-T}, \quad 
\F_1 \approx \nabla_{\theta}^2 \ell(\hat{\theta})C + C^{-T}, \\
\G_2 \approx - (\theta - \mu) \{ (\theta - \hat{\theta})^T \nabla_{\theta}^2 \ell(\hat{\theta})T^{-T}  +  (\theta - \mu)^T T \} , \quad 
\F_2 \approx - \Sigma\nabla_{\theta}^2 \ell(\hat{\theta})T^{-T} - T^{-T}.
\end{gathered}
\]
While $\F_1$ and $\F_2$ are independent of $\theta$ and hence have (close to) zero variances, $\G_1$ and $\G_2$ are subjected to variation arising from simulation of $\theta$ from $q_\lambda(\theta)$. Hence $\F_1$ and $\F_2$ are more stable close to the optimum where $\mL$ will be approximately quadratic.

Stochastic variational algorithms obtained using Theorem \ref{thm stein} are outlined in Tables \ref{Alg1} and \ref{Alg2}, and we have applied Corollary \ref{cor inv} in the update of $\mu$ in Algorithm 2N. Algorithms based on Euclidean and natural gradients are placed side-by-side for ease of comparison. Those based on natural gradients have an additional step for computing $\dH$ and the updates involve some form of scaling, which can help to improve convergence. 


\begin{table} 
\caption{Stochastic variational algorithms for updating $\mu$ and $C$. \label{Alg1}} 
\centering
\fbox{\begin{tabular}{l|l}
Algorithm 1E (Euclidean gradient) &  Algorithm 1N (Natural gradient) \\ \hline
\multicolumn{2}{c}{
\begin{minipage}{0.76\textwidth}
\vspace{1mm}
Initialize $\mu$ and $C$. For $t=1,2,\dots$,
\begin{enumerate} [1.]
\item Generate $z \sim \N(0, I_d)$ and compute $\theta = C z + \mu$.
\item Find $\bar{G}$ where $G = \nabla_\theta h(\theta) z^T$ or 
$G = \nabla_\theta^2 h(\theta) C$. 
\end{enumerate}
\end{minipage}
} \\ \hline
\begin{minipage}[t]{0.3\textwidth}
\vspace{-2mm}
\begin{enumerate}[1.]
\setcounter{enumi}{2}
\item Update $\mu \leftarrow \mu + \rho_t \nabla_\theta h(\theta)$.
\item Update $C \leftarrow C + \rho_t \bar{G}$.
\end{enumerate}
 \end{minipage} &
\begin{minipage}[t]{0.36\textwidth}
\vspace{-3mm}
\begin{enumerate} [1.]
\setcounter{enumi}{2}
\item Update $\mu \leftarrow \mu + \rho_t  C C^T \nabla_\theta h(\theta)$.
\item Find $\dH$ where $H =  C^T \bar{G}$.
\item Update $C \leftarrow C+ \rho_t C \dH$.
\end{enumerate}
 \end{minipage}
\vspace{1mm}
\end{tabular}}
\end{table}

\begin{table} 
\caption{\label{Alg2} Stochastic variational  algorithms for updating $\mu$ and $T$.}
\centering
\fbox{\begin{tabular}{l|l}
Algorithm 2E (Euclidean gradient) & Algorithm 2N (Natural gradient) \\ \hline
\multicolumn{2}{c}{
\begin{minipage}{0.83\textwidth}
\vspace{1mm}
Initialize $\mu$ and $T$. For $t=1,2,\dots$,
\begin{enumerate}[1.]
\item Generate $z \sim \N(0, I_d)$ and compute $\theta = T^{-T}z + \mu$.
\item Find $\bar{G}$ where $G = -T^{-T} z v^T$, $v = T^{-1} \nabla_\theta h(\theta)$ or $G = -T^{-T} T^{-1} \nabla_\theta^2 h(\theta) T^{-T}$. 
\end{enumerate}
\end{minipage}
} \\ \hline
\begin{minipage}[t]{0.34\textwidth}
\vspace{-2mm}
\begin{enumerate}[1.]
\setcounter{enumi}{2}
\item Update $\mu \leftarrow \mu + \rho_t \nabla_\theta h(\theta)$.
\item Update $T \leftarrow T + \rho_t \bar{G}$.
\end{enumerate}
 \end{minipage} &
\begin{minipage}[t]{0.33\textwidth}
\vspace{-3mm}
\begin{enumerate}[1.]
\setcounter{enumi}{2}
\item Find $\dH$ where $H =  T^T \bar{G}$. 
\item Update $T \leftarrow T + \rho_t T \dH$.
\item Update $\mu \leftarrow \mu + \rho_t  T^{-T} v$.
\end{enumerate}
\end{minipage} 
\vspace{0.5mm}
\end{tabular}}
\end{table}



\section{Imposing sparsity} \label{sec imposing sparsity}
For high-dimensional models, it may be useful to impose sparsity on the covariance or precision matrix, and hence on their Cholesky factors to increase efficiency. For Algorithms 1E and 2E, updates of sparse Cholesky factors can be obtained simply by extracting entries in the Euclidean gradients that correspond to nonzero entries in the Cholesky factor, but the same may not apply to natural gradients due to premultiplication by the Fisher information. As illustration, suppose $\lambda = (\lambda_1^T , \lambda_2^T )^T $ and the Fisher information, Euclidean gradient and natural gradient for this partitioning are respectively
\begin{align*}
F_\lambda = \begin{bmatrix} F_{11} & F_{12} \\ F_{21} & F_{22} \end{bmatrix}, \quad 
g_\lambda = \begin{bmatrix} g_1 \\ g_2 \end{bmatrix}, \quad 
\widetilde{g}_\lambda = F_\lambda^{-1} g_\lambda =  \begin{bmatrix} \widetilde{g}_1 \\ \widetilde{g}_2 \end{bmatrix}.
\end{align*}
By block matrix inversion, $\widetilde{g}_1 = F_{11}^{-1}g_1 - F_{11}^{-1} F_{12} \widetilde{g}_2$. If we fix $\lambda_2 = 0$ ($\lambda_2$ is no longer an unknown parameter), then the natural gradient for updating $\lambda_1$ is just $F_{11}^{-1} g_1$, which is equal to $\widetilde{g}_1 + F_{11}^{-1} F_{12} \widetilde{g}_2$, not $\widetilde{g}_1$. Thus, we cannot update $\lambda_1$ simply by extracting $\widetilde{g}_1$. 

In this section, we derive efficient natural gradient updates of the Cholesky factors in two cases, (i) the covariance matrix has a block diagonal structure corresponding to the product density assumption in variational Bayes and (ii) the precision matrix reflects the posterior conditional independence structure in a hierarchical model where local variables are independent conditional on the global variables.



\subsection{Block diagonal covariance structure}
Let $q_\lambda (\theta) = \prod_{i=1}^N q_{\lambda_i} (\theta_i)$ for some partitioning $\theta = (\theta_1^T , \dots, \theta_N^T )^T$, where $\theta_i \sim \N(\mu_i, \Sigma_i)$. Then $\Sigma = \blockdiag(\Sigma_1, \dots, \Sigma_N)$ and $\mu = (\mu_1^T , \dots, \mu_N^T )^T $. Let $C_i C_i^T $ be the Cholesky decomposition of $\Sigma_i$, where $C_i$ is a lower triangular matrix for $i=1, \dots, N$, and $C =  \blockdiag(C_1, \dots, C_N)$. For the parametrization $\lambda = (\mu^T , \vech(C_1)^T , \dots,  \vech(C_N)^T )^T$, the Fisher information, $F_\lambda = \blockdiag(\Sigma^{-1}, \mathfrak{I}(C_1), \dots, \mathfrak{I}(C_N))$, where $\mathfrak{I}(\cdot)$ is defined in Lemma \ref{lem inv}. Let $\nabla_{\vech (C_i)} \mL = \vech(G_i)$ and $H_i = C_i^T  \bar{G}_i$ for $i=1, \dots, N$. Then it follows from Lemma \ref{lem inv} that the natural gradient,
\[
\widetilde{\nabla}_{\lambda} \mL = F_\lambda^{-1} \nabla_\lambda \mL =  \begin{bmatrix}
\Sigma & 0 & \dots & 0 \\
0 & \mathfrak{I}(C_1)^{-1} & \dots & 0 \\
\vdots & \vdots & \ddots & \vdots \\
0 & 0 & \dots & \mathfrak{I}(C_N)^{-1} \\
\end{bmatrix}
\begin{bmatrix} \nabla_\mu \mL \\ \vech(\bar{G}_1)  \\ \vdots \\ \vech(\bar{G}_N) \end{bmatrix}
= \begin{bmatrix} \Sigma \nabla_\mu \mL \\ \vech (C_1 \dH_1) \\  \vdots \\  \vech (C_N \dH_N) \end{bmatrix}.
\]

This explicit expression of the natural gradient reveals the sparse structures of matrices that underlie its computation. Let $\bar{G} = \blockdiag(\bar{G}_1, \dots, \bar{G}_N)$, $H = \blockdiag(H_1, \dots, H_N)$ and $\dH = \blockdiag(\dH_1, \dots, \dH_N)$. Then $H = C^T \bar{G}$ and $C\dH = \blockdiag(C_1 \dH_1, \dots, C_N \dH_N)$. Thus $C$, $\bar{G}$, $\dH$ and $C\dH$ have the same sparse block lower triangular structure (see Figure \ref{Fig1}), which is useful in improving storage and computational efficiency.
\begin{figure}[htb!]
\centering
\includegraphics[height=60pt, angle=0]{Fig1.pdf}
\caption{Shaded regions represent nonzero entries in $C^T $, $\bar{G}$ and $H = C^T \bar{G}$ ($N=4$).}
\label{Fig1}
\end{figure}


If the Euclidean gradient $\nabla_\lambda \mL$ is intractable, then unbiased estimates of $\nabla_{\vech(C_i)} \mL$ can be obtained using Theorem \ref{thm stein} (i). As $C =  \blockdiag(C_1, \dots, C_N)$, we only have to extract the entries in $\G_1$ and $\F_1$ that correspond to $C_1, \dots, C_N$ on the block diagonal. For $i=1, \dots, N$,
\[
\nabla_{\vech(C_i)} \mL = \E_q \vech\{\nabla_{\theta_i} h(\theta) (\theta_i - \mu_i)^T C_i^{-T}\} =  \E_q \vech(\nabla_{\theta_i}^2 h(\theta) C_i).
\]
The resulting stochastic variational algorithm 1S is outlined in Table \ref{Algc}.


\begin{table}
\caption{\label{Algc}  Stochastic natural gradient algorithms incorporating sparsity.}
\centering
\fbox{\begin{tabular}{l|l}
Algorithm 1S (Update $\mu$ and $C$) & Algorithm 2S (Update $\mu$ and $T$) \\ \hline
\begin{minipage}[t]{0.46\textwidth}
\vspace{-1mm}
Initialize $\mu$ and $C= \blockdiag(C_1, \dots, C_N)$. \\
For $t=1,2,\dots$,
\begin{enumerate}[1.]
\item Generate $z \sim \N(0, I_d)$ and \\
compute $\theta = Cz + \mu$.
\item Find $\bar{G} = \blockdiag(\bar{G}_1, \dots, \bar{G}_N)$ where $G_i = \nabla_{\theta_i} h(\theta) (\theta_i - \mu_i)^T C_i^{-T}$ or $\nabla_{\theta_i}^2 h(\theta) C_i$.
\item Compute $\dH$ where $H = C^T  \bar{G}$.
\item Update
$\mu \leftarrow \mu + \rho_t  C C^T  \nabla_\theta h(\theta)$.
\item Update $C \leftarrow C + \rho_t C \dH$.
\end{enumerate}
\end{minipage} &
\begin{minipage}[t]{0.46\textwidth}
\vspace{-1mm}
Initialize $\mu$ and $T$ in \eqref{sparse T}. 
For $t=1,2,\dots$,
\begin{enumerate}[1.]
\item Generate $z \sim \N(0, I_d)$ and \\
compute $\theta = T^{-T}z + \mu$.
\item Find $\bar{G}$ where $G$ comprises blocks in $uv^T$ or $T_d^{-T} T^{-1} \nabla_\theta^2 h(\theta)T^{-T}$ that correspond to nonzero blocks in $T$.
\item Compute $\dH$ where $H = T_d^{T}\bar{G}$.
\item Update $T \leftarrow T + \rho_t T \dH$.
\item Update $\mu \leftarrow \mu + \rho_t  T^{-T} v$.
\end{enumerate}
 \end{minipage}
\end{tabular}}
\end{table}



\subsection{Sparse precision matrix} \label{sec_sparse precision matrix}
Consider a hierarchical model where the local variables specific to individual observations, $\theta_1, \dots, \theta_n$, are independent of each other conditional on the global variables shared across all observations, $\theta_g$. Then the joint density is of the form,
\[
p(y,\theta) = p(\theta_g) \prod_{i=1}^n p(y_i|\theta_i, \theta_g) p(\theta_i|\theta_g),
\]
where $y=(y_1, \dots, y_n)^T$, $\theta = (\theta_1^T , \dots, \theta_n^T , \theta_g^T )^T $ and $p(\theta_g)$ is a prior density for the global variables. For this model, $\theta_1, \dots, \theta_n$ are conditionally independent given $\theta_g$ a posteriori. To mirror this conditional independence structure in the posterior distribution, let the the Cholesky factor of precision matrix $\Sigma^{-1}$ be of the form
\begin{equation} \label{sparse T}
T = \begin{bmatrix}
T_{1}  & \dots & 0 & 0\\
\vdots & \ddots & \vdots & \vdots \\
0  & \dots & T_{n} &0 \\
T_{g1} & \dots & T_{gn} & T_{g}
\end{bmatrix},
\end{equation}
where $T_1, \dots, T_n, T_g$ are lower triangular matrices. Let $\mu = (\mu_1^T , \dots, \mu_n^T , \mu_g^T )^T $ be the corresponding partitioning, $\Sigma_g^{-1} = T_g T_g^T$ and $\Sigma_i^{-1} = T_i T_i^T$ for $i=1, \dots, n$. Then $q_\lambda (\theta) = q(\theta_g) \prod_{i=1}^n q(\theta_i|\theta_g)$, where $q(\theta_g)$ is $\N(\mu_g, \Sigma_g)$ and $q(\theta_i|\theta_g)$ is $\N(\mu_i - T_i^{-T} T_{gi}^T (\theta_g - \mu_g), \Sigma_i)$ for $i=1, \dots, n$. 


Consider
\[
\lambda = (\mu^T , \vech(T_1)^T , \vc(T_{g1})^T , \dots,  \vech(T_n)^T ,\vc(T_{gn})^T , \vech(T_g)^T )^T . 
\]
For this ordering, the Fisher information has a block diagonal structure and can be inverted easily. By using Lemma \ref{lem inv} and block matrix inversion, we show in the supplement S4 that $F_\lambda^{-1} = \blockdiag(\Sigma, F_1^{-1}, \dots, F_n^{-1}, \mathfrak{I}(T_g)^{-1} )$, where
\[
F_i^{-1} = \begin{bmatrix}
\mathfrak{I}(T_i)^{-1} & \mathfrak{I}(T_i)^{-1} L (I \otimes T_i^{-T} T_{gi}^T)  \\
\cdot &  (I \otimes \Sigma_g^{-1}) +  (I \otimes T_{gi} T_i^{-1}) L^T \mathfrak{I}(T_i)^{-1} L (I \otimes T_i^{-T} T_{gi}^T)
\end{bmatrix}.
\]
Let $\nabla_{\vech(T_i)} \mL = \vech(A_i)$, $\nabla_{\vc(T_{gi})} \mL = \vc(G_{gi})$ for $i=1, \dots, n$ and $\nabla_{\vech(T_g)} \mL = \vech(G_g)$.  Applying Lemma \ref{lem inv}, the natural gradient is 
\[
\widetilde{\nabla}_\lambda \mL = F_\lambda^{-1}  \begin{bmatrix}
\nabla_\mu \mL \\ 
\begin{bmatrix} \vech (A_i)  \\  \vc(G_{gi})  \\  \end{bmatrix}_{i=1:n} \\ 
\vech(G_g)
\end{bmatrix} = \begin{bmatrix}
\Sigma \nabla_\mu \mL \\
\begin{bmatrix}
\vech (T_i \dH_i) \\
\vc( T_{gi}\dH_i +  \Sigma_g^{-1} G_{gi}) \\
\end{bmatrix}_{i=1:n} \\
\vech(T_g \dH_g)
\end{bmatrix},
\]
where $[a_i]_{i=1:n} = (a_1^T , \dots, a_n^T )^T $, $H_g = T_g^T  \bar{G}_{g}$, $G_i = A_i + T_i^{-T} T_{gi}^T G_{gi}$ and $H_i = T_i^T  \bar{G}_i$ for $i=1, \dots, n$. To compute the natural gradient in practice, we can define 
\begin{equation*}
G = \begin{bmatrix}
G_1  &  \dots & 0 & 0 \\
\vdots & \ddots & \vdots & \vdots \\
0 & \dots  & G_n  & 0 \\
G_{g1} & \dots & G_{gn}  & G_g 
\end{bmatrix}, \quad 
H = T_d^T \bar{G} =  \begin{bmatrix}
H_1 &  \dots & 0 & 0 \\
\vdots & \ddots & \vdots & \vdots \\
0 & \dots  & H_n & 0 \\
T_g^T  G_{g1}  & \dots & T_g^TG_{gn}  & H_g
\end{bmatrix},
\end{equation*}
where $T_d = \blockdiag(T_1, \dots, T_n, T_g)$ consists only of the diagonal blocks in $T$. Then 
\[
T\dH =  \begin{bmatrix}
T_1 \dH_1 &  \dots & 0 & 0 \\
\vdots & \ddots & \vdots & \vdots \\
0 & \dots  & T_n \dH_n & 0 \\
T_{g1}\dH_1 + T_g T_g^T G_{g1}  & \dots & T_{gn}\dH_n +  T_g T_g^T G_{gn}  & T_g \dH_g
\end{bmatrix},
\]
yields all the necessary components in the natural gradient. In addition, $\bar{G}$, $\dH$ and $T\dH$ all have the same sparse structure as $T$.



\subsection{Stochastic natural gradient for sparse precision matrix}
If $\nabla_\lambda \mL$ is not analytic, unbiased estimates of $\nabla_\lambda \mL$ can be obtained from Theorem \ref{thm stein} (ii) by extracting entries in $\G_2$ and $\F_2$ that correspond to nonzero entries in $T$. As the $\theta_i$s are conditionally independent in both $p(y, \theta)$ and $q_\lambda (\theta)$, $\nabla_{\theta_i, \theta_j}^2 h(\theta) = 0$ if $i \neq j$ and $\nabla_\theta^2 h(\theta)$ is also sparse. Let $u = (u_1^T, \dots, u_n^T, u_g^T)^T = T_d^{-T} z$ where $z =  T^T (\theta - \mu)$, $v = (v_1^T , \dots, v_n^T , v_g^T )^T = T^{-1} \nabla_\theta h(\theta)$ and 
\[
\begin{aligned}
U_{gg} &= \Sigma_g \bigg( \nabla_{\theta_g}^2 h(\theta) - \sum_{i=1}^n T_{gi} T_i^{-1} \nabla_{\theta_i, \theta_g}^2 h(\theta) \bigg) T_g^{-T}, \\
U_{gi} &= \Sigma_g \{\nabla_{\theta_g, \theta_i}^2 h(\theta) - T_{gi} T_i^{-1} \nabla_{\theta_i}^2 h(\theta)\}T_i^{-T} \quad (i=1, \dots, n).
\end{aligned}
\]
In the supplement S4, we show that for $i=1, \dots, n$, 
\[
\begin{aligned}
\nabla_{\vech(T_i)} \mL &= \E_q \vech \{ (T_i^{-T} T_{gi} u_g - u_i)  v_i^T\} 
= \E_q \vech (T_i^{-T} T_{gi}^T U_{gi} - \Sigma_i  \nabla_{\theta_i}^2 h(\theta)  T_i^{-T} ), \\
\nabla_{\vc(T_{gi})} \mL &= -\E_q \vc (u_g v_i^T)
= -\E_q \vc(U_{gi}), \\
\nabla_{\vech(T_g)} \mL &= -\E_q \vech(u_g v_g^T) = \E_q \vech \bigg(  \sum_{i=1}^n U_{gi}T_{gi}^TT_g^{-T} - U_{gg} \bigg) .
\end{aligned}
\]
Using the above results, we can obtain unbiased estimates of the natural gradient in terms of $\nabla_\theta h(\theta)$ by setting
\[
G_g = -u_g v_g^T, \;\; G_{gi} = -u_gv_i^T, \;\;G_i = - u_i v_i^T \quad ( i=1, \dots, n).
\]
On the other hand, unbiased estimates in terms of $\nabla_\theta^2 h(\theta)$ can be obtained by setting
\[
G_g = \sum_{i=1}^n U_{gi} T_{gi}^TT_g^{-T} - U_{gg}, \;\; 
G_{gi} = -U_{gi} \;\;
G_i = - \Sigma_i  \nabla_{\theta_i}^2 h(\theta)  T_i^{-T} \quad ( i=1, \dots, n).
\]
In practice, we can compute $G$ by finding the blocks in $uv^T$ or $T_d^{-T} T^{-1} \nabla_\theta^2 h(\theta)T^{-T}$  that correspond to nonzero blocks in $T$. 
The overall procedure is outlined in Algorithm 2S (Table \ref{Algc}). Compared with 2N, the computation of $\bar{G}$ and $\dH$ differ in sparsity and some usage of $T_d$ instead of $T$. Further derivation details are given in the supplement S4.



 
\section{Stochastic normalized natural gradient ascent with momentum} \label{sec_Normalized SGD}
Next, we discuss the choice of stepsize $\rho_t$ in stochastic natural gradient ascent. For high-dimensional models, it is particularly important to use an adaptive stepsize that is robust to noisy gradient information. Some popular approaches include Adagrad \citep{Duchi2011}, Adadelta \citep{Zeiler2012} and Adam \citep{Kingma2015}, which compute {\em elementwise} adaptive learning rates using past gradients. Notably, Adam  introduces momentum by computing the exponential moving average of the gradient ($m_t$) and elementwise squared gradient ($v_t$), and corrects for the bias due to initializing $m_t$ and $v_t$ at 0 using $\widehat{m}_t$ and $\widehat{v}_t$ (Table \ref{Adam and Snngm}). The effective step is $\alpha \widehat{m}_t/(\sqrt{\widehat{v}_t} + \epsilon)$, where $\epsilon$ is a small constant that is added to avoid division by zero. 

\begin{table}
\caption{\label{Adam and Snngm} Adam and Snngm (scalar functions are performed elementwise on vectors).}
\centering
\fbox{\begin{tabular}{l|l}
Adam \citep{Kingma2015} & Snngm \\ \hline
\begin{minipage}[t]{0.46\textwidth}
\vspace{-2mm}
Default: $\alpha =0.001$, $\beta_1=0.9$, $\beta_2=0.999$, $\epsilon =10^{-8}$.
Initialize $m_0 = 0$, $v_0=0$ and $\lambda^{(1)}$. \\
For $t=1,2,\dots$,
\begin{enumerate}[1.]
\item Compute gradient estimate $\widehat{g}_t$.
\item $m_t = \beta_1 m_{t-1} + (1- \beta_1) g_t$.
\item $v_t = \beta_2 v_{t-1} + (1- \beta_2) g_t^2$.
\item $\widehat{m}_t = m_t/(1-\beta_1^t)$ and $\widehat{v}_t = v_t/(1-\beta_2^t)$.
\item $\lambda^{(t+1)} = \lambda^{(t)} + \alpha \widehat{m}_t/(\sqrt{\widehat{v}_t} + \epsilon)$.
\end{enumerate}
\end{minipage} &
\begin{minipage}[t]{0.43\textwidth}
\vspace{-2mm}
Default: $\alpha = 0.001\sqrt{\ell_\lambda}$ where $\ell_\lambda$ is length of $\lambda$, $\beta = 0.9$. Initialize $m_0 = 0$ and $\lambda^{(1)}$. \\
For $t=1,2,\dots$,
\begin{enumerate}[1.]
\item Compute natural gradient estimate $\widetilde{g}_t$.
\item $m_t = \beta m_{t-1} + (1-\beta) \widetilde{g}_t/\|\widetilde{g}_t\|$.
\item $\widehat{m}_t = m_t/(1-\beta^t)$.
\item $\lambda^{(t+1)} = \lambda^{(t)} + \alpha \widehat{m}_t$.
\end{enumerate}
 \end{minipage}
\end{tabular}}
\end{table}


\subsection{Motivation of Snngm and its difference from Adam}
Despite Adam's wide applicability, we observe that use of natural gradients with Adam fails to yield significant improvement in convergence compared to Euclidean gradients. There are several factors that contribute to this phenomenon. Adam can be interpreted as a {\em sign-based} variance adapted approach \citep{Balles2018}, since (ignoring $\epsilon$) the update step can be expressed as 
\[
\alpha \frac{\text{sign}(\widehat{m}_t)}{\sqrt{\widehat{v}_t/\widehat{m}_t}} \approx \alpha \frac{\text{sign}(\widehat{m}_t)}{\sqrt{\E(g_t^2)/\E(g_t)^2}} 
= \alpha \frac{\text{sign}(\widehat{m}_t)}{\sqrt{1 + \Var(g_t)/\E(g_t)^2}}.
\]
The update direction is thus given by the sign of $\widehat{m}_t$, while the per dimension magnitude is bounded by $\alpha$ and reduced correspondingly when there is high uncertainty (measured by the relative variance $\Var(g_t)/\E(g_t)^2$). Moreover, experiments conducted by \cite{Balles2018} indicate that the sign aspect is dominant. If we replace Euclidean gradient estimate $\widehat{g}_t$ by natural gradient estimate $\tilde{g}_t$, then Adam will update by focusing on the sign information in $\tilde{g}_t$ while the scaling obtained via the Fisher information will be neglected to a large extent. Loss of scale information is compounded by variance adaption performed on a per dimension basis.

To overcome these issues, we propose {\em stochastic normalized natural gradient ascent with momentum} (Snngm), which is outlined in Table \ref{Adam and Snngm}. If we exclude momentum by setting $\beta = 0$, then the update step is $\alpha \widetilde{g}_t /\|\widetilde{g}_t \|$, where $\| \cdot \|$ represents the Euclidean norm. The norm of this step is fixed at $\alpha$, while the effective stepsize is $\rho_t = \alpha /\|\widetilde{g}_t \|$. The same stepsize is used for all parameters to preserve the scaling by the inverse Fisher information. In the initial stage of optimization when $\lambda$ is far from the mode, $\rho_t$ will be small as the gradient tends to be large in magnitude. This is important for stability especially if the initialization is far from the mode. As $\lambda$ approaches the optimum, $\rho_t$ increases as the gradient tends to zero. Normalized natural gradient ascent is thus able to avoid slow convergence close to the mode and is also effective at evading saddle points \citep{Hazan2015}. As the true natural gradient is unknown, we inject momentum using the exponential moving average for robustness against noisy gradients. In Table \ref{Adam and Snngm}, we set the default $\alpha = 0.001 \ell_\lambda$ where $\ell_\lambda$ is the length of $\lambda$, because $\|\widetilde{g}_t\|$ tends to increase with $\ell_\lambda$ and scaling up $\alpha$ proportionally prevents the stepsize from becoming too small in high-dimensional problems. Further tuning of $\alpha$ may be desired depending on the problem but 0.001$\ell_\lambda$ is a good starting point. 

To illustrate the difference in performance between Adam and Snngm, we consider the intercept-only loglinear model in Section \ref{sec_Poisson} again. Figure \ref{Fig c} shows the gradient vector fields and trajectories of Adam or Snngm run using default settings in Table \ref{Adam and Snngm} from the same starting points. Use of natural gradients did not lead to any improvements in Adam. Instead, more iterations were required and the run starting from (2, 0.01) was terminated due to a negative $\sigma^2$ update. The second plot also indicates that Adam does not follow the flow of natural gradients closely unlike Snngm in the third plot. This is likely caused by the loss of scale information in Adam as discussed previously. On the other hand, the number of iterations was reduced by about three times using Snngm.

\begin{figure}[htb!]
\centering
\includegraphics[height=110pt, angle=0]{satellite1.pdf}
\caption{Gradient vector field and trajectories of Adam and Snngm from three starting points. Legend shows the total number of iterations.}
\label{Fig c}
\end{figure}


\subsection{Convergence of Snngm}
Next, we analyze the convergence of Snngm under four assumptions, of which the first three are similar to that made by \cite{Defossez2020} in proving the convergence of Adam. Let $g_t = \nabla_\lambda \mL(\lambda^{(t)})$ be the Euclidean gradient at $\lambda^{(t)}$ and  $\widehat{g}_t = \widehat{\nabla}_\lambda \mL(\lambda^{(t)})$ be an unbiased estimate such that $\E_q(\widehat{g}_t) = g_t$. In addition, let $F_t = F_\lambda(\lambda^{(t)})$ and $\widetilde{g}_t = F_t^{-1} \widehat{g}_t $ be an unbiased estimate of the natural gradient. The assumptions are as follows.
\begin{description}
\item[(A1)] $\mL(\lambda) \leq \mL^*$ $\forall \lambda \in \mathbb{R}^d$.
\item[(A2)] $\| \widehat{\nabla}_\lambda \mL (\lambda) \| \leq R$ $\forall \lambda \in \mathbb{R}^d$.
\item[(A3)] $\mL(\lambda)$ is $L$-Lipschitz smooth: $\exists$ a constant $L > 0$ such that $\| \nabla_\lambda \mL(\lambda') - \nabla_\lambda \mL(\lambda) \| \leq L \| \lambda' - \lambda \|$ $\forall \; \lambda, \lambda' \in \mathbb{R}^d$.
\item[(A4)] $0 < R_1 \leq \ev(F_\lambda) \leq R_2$ $\forall \lambda \in \mathbb{R}^d$, where $\ev(F_\lambda)$ denotes the eigenvalues of $F_\lambda$.
\end{description}
Following \cite{Defossez2020}, let $\tau$ be a random index such that $\Prob(\tau=j) \propto 1 - \beta^{T-j+1}$ for $j  \in \{1, \dots, T\}$. The proportionality constant,
\[
C = \sum_{j=1}^T  (1-\beta^{T-j+1}) = T - \frac{\beta(1-\beta^T )}{1-\beta} \geq  T - \frac{\beta}{1-\beta} = \widetilde{T}.
\]
For the distribution of $\tau$, almost all values of $j$ are sampled uniformly except that the last few are sampled less often. For instance, Figure \ref{Fig2} shows the value of $1 - \beta^{T-j+1}$ for $T=10000$ and $\beta = 0.9$. All values are greater than 0.99 except for the last 43 of them. 
\begin{figure}[htb!]
\centering 
\includegraphics[height=100pt]{tau.pdf}
\caption{Value of $1 - \beta^{T-j+1}$ for $j=1, \dots, T$ where $T=10000$.}
\label{Fig2}
\end{figure}

Theorem \ref{thm NSNGAM} provides bounds for the expected squared norm of the gradient at iteration $\tau$ in Snngm under assumptions (A1)--(A4). The proof of Theorem \ref{thm NSNGAM} is given in the supplement S5. If we assume $T \gg \beta/(1-\beta)$, then $\widetilde{T} \approx T$. Setting $\alpha = 1/\sqrt{T}$ then yields an $O(1/\sqrt{T})$ convergence rate. 

\begin{theorem} \label{thm NSNGAM}
In Snngm, under assumptions (A1)--(A4) and for any $T > \beta/(1-\beta)$, 
\begin{equation*} 
\E\|g_\tau \|^2 \leq \frac{R R_2}{ R_1} \left\{ \frac{\mL^* - \mL(\lambda^{(1)})}{\widetilde{T} \alpha} +  \frac{TL\alpha}{\widetilde{T}}  \left( \frac{\beta}{1-\beta} + \frac{1}{2} \right) \right\}.
\end{equation*}
\end{theorem}



\section{Applications} \label{sec_applications}
We apply proposed methods to logistic regression and generalized linear mixed models (GLMMs). A Gaussian variational approximation with a full or diagonal covariance matrix is considered for logistic regression, while a block diagonal covariance or sparse precision matrix is used for GLMMs. The stepsize is computed using Adam or Snngm, and we compare the efficiency and accuracy of algorithms based on Euclidean versus natural gradient. Parameters for Adam and Snngm are set at default values in Table \ref{Adam and Snngm}, except that $\alpha$ for Snngm is adjusted to $0.01\sqrt{\ell_\lambda}$ for algorithms 2N and 2S that update the Cholesky factor of the precision matrix. This adjustment is required likely due to the difference in scale between covariance and precision matrices. We initialize $\mu = 0$ and $C = 0.1 I$ or equivalently $T = 10 I$ unless stated otherwise.

At each iteration $t$, we compute an unbiased estimate $\widehat{\mL}_t = h(\theta^{(t)})$ of $\mL$. To assess convergence, we average these estimates over every 1000 iterations and fit a least square regression line to the past three means \citep{Tan2021}. The algorithm is terminated once the gradient is less than 0.01. At termination (iteration $T$), we compute an estimate $\hat{\mL}$ of $\mL$, as the mean of $h(\theta^{(t)})$ over 1000 simulations of $\theta^{(t)}$ from $q_{\lambda^{(T)}}(\theta)$. Since our goal is to maximize $\mL$, an algorithm with a higher $\hat{\mL}$ is regarded as providing a better estimate of $\lambda$. The code is written in Julia \citep{Bezanson2017} and is available as supplementary material. All experiments are run on an Intel Core i9-9900K CPU @ 3.60GHz.


\subsection{Logistic regression}
Given a dataset $\{(x_i, y_i)|i=1, \dots, n\}$ where $x_i \in \mathbb{R}^d$ and $y_i \in \{0,1\}$, we consider the model, $\logit(p_i) = x_i^T  \theta$, where $y_i \sim \text{Bernoulli}(p_i)$. It is assumed that $x_i$ contains an intercept. The regression coefficient $\theta$ is assigned a prior $\N(0, \sigma_0^2 I)$, where $\sigma_0 = 10$, and the posterior density of $\theta$ is approximated by $\N(\mu, \Sigma)$. The expression of $\log p(y, \theta)$ and its first and second order derivatives are given in the supplement S6.

\begin{table}
\caption{\label{Tab5} Logistic regression: Number of iterations ($T$) in thousands, average lower bound $\hat{\mL}$ at termination and runtime in seconds.} 
\centering
\begin{footnotesize}
\fbox{\begin{tabular}{ll|rcr|rcr|rcr}
& & \multicolumn{3}{c|}{German} & \multicolumn{3}{c|}{Heart} & \multicolumn{3}{c}{ICU} \\ 
\multicolumn{2}{l|}{First order} & $T$ & $\hat{\mL}$ & time & $T$ &  $\hat{\mL}$ & time & $T$ & $\hat{\mL}$ & time  \\   \hline
\multirow{3}{*}{\begin{minipage}{0.03\textwidth} Full \\ Cov \end{minipage}} 
    & Euclidean (Adam) & 14 & -627.5 &  6.1 & 13 & -144.1 & 1.0 & 17 & -115.3 & 1.4 \\
    & Natural (Adam) & 8 & -625.9 &  5.7 & 9 & -144.1 & 0.8 & 10 & -115.2 & 1.0 \\
    & Natural (Snngm) & 5 & -625.7 &  2.9 & 6 & -144.0 & 0.4 & 7 & -115.2 & 0.5 \\
\hline
\multirow{3}{*}{\begin{minipage}{0.03\textwidth} Diag \\ Cov \end{minipage}} 
    & Euclidean (Adam) & 16 & -640.5 &  2.1 & 23 & -148.6 & 0.3 & 22 & -123.0 & 0.3 \\
    & Natural (Adam) & 15 & -640.7 &  2.0 & 23 & -148.6 & 0.4 & 22 & -123.0 & 0.4 \\
    & Natural (Snngm) & 14 & -639.7 &  1.9 & 13 & -148.8 & 0.2 & 16 & -122.9 & 0.2 \\
\hline
\multirow{3}{*}{\begin{minipage}{0.03\textwidth} Full \\ Prec \end{minipage}} 
    & Euclidean (Adam) & 44 & -626.0 & 18.5 & 21 & -144.0 & 1.6 & 24 & -115.2 & 1.9 \\
    & Natural (Adam) & 27 & -625.6 & 18.5 & 22 & -144.0 & 2.2 & 23 & -115.2 & 2.4 \\
    & Natural (Snngm) & 8 & -625.7 &  4.8 & 7 & -144.1 & 0.5 & 6 & -115.3 & 0.4 \\
\hline
\multicolumn{2}{l|}{Second order} &&&&&&&&&  \\   \hline
\multirow{3}{*}{\begin{minipage}{0.03\textwidth} Full \\ Cov \end{minipage}} 
    & Euclidean (Adam) & 13 & -625.6 & 11.5 & 13 & -144.0 & 1.3 & 16 & -115.2 & 1.5 \\
    & Natural (Adam) & 8 & -625.6 &  9.9 & 10 & -144.1 & 1.3 & 13 & -115.2 & 1.7 \\
    & Natural (Snngm) & 4 & -625.6 &  4.8 & 4 & -144.0 & 0.4 & 4 & -115.2 & 0.4 \\
\hline
\multirow{3}{*}{\begin{minipage}{0.03\textwidth} Diag \\ Cov \end{minipage}} 
    & Euclidean (Adam) & 15 & -640.6 &  3.9 & 23 & -148.6 & 0.6 & 22 & -123.0 & 0.6 \\
    & Natural (Adam) & 15 & -640.5 &  3.7 & 23 & -148.6 & 0.7 & 22 & -122.9 & 0.6 \\
    & Natural (Snngm) & 14 & -639.9 &  3.5 & 13 & -148.8 & 0.3 & 18 & -122.6 & 0.5 \\
 \hline
\multirow{3}{*}{\begin{minipage}{0.03\textwidth} Full \\ Prec \end{minipage}} 
    & Euclidean (Adam) & 17 & -625.6 & 16.6 & 16 & -144.0 & 3.0 & 21 & -115.2 & 3.9 \\
    & Natural (Adam) & 15 & -625.6 & 20.9 & 16 & -144.0 & 4.3 & 16 & -115.2 & 4.3 \\
    & Natural (Snngm) & 4 & -625.6 &  4.9 & 4 & -144.0 & 0.9 & 6 & -115.3 & 1.4 \\
\end{tabular}}
\end{footnotesize}
\end{table}

We consider three datasets. The German credit ($n = 1000$, $d=49$) and Heart ($n = 270$, $d=19$) data are from the UCI Machine Learning Repository and have been analyzed by \cite{Chopin2017}, while the ICU data ($n = 200$, $d = 20$) from \cite{Hosmer2013} can be downloaded from the book website. All continuous predictors are rescaled to have mean 0 and standard deviation 1, while categorical predictors are coded using dummy variables. For the ICU data, we further convert the RACE and LOC variables to binary variables. As $d$ is large for these datasets, we consider Cholesky decompositions of the full covariance, full precision or diagonal covariance matrix. It is easy to compute $\nabla_\theta^2 h(\theta)$ for this problem and hence we also compare results obtained using either the first or second order gradient estimates for each algorithm.

\begin{figure}[htb!]
\centering 
\includegraphics[height=280pt]{logreg.pdf}
\caption{Logistic regression: Average lower bound attained over past 1000 iterations.}
\label{Fig3}
\end{figure}

The progress of each algorithm is represented in Figure \ref{Fig3} through the average lower bound attained over the past 1000 iterations, while Table \ref{Tab5} shows the total number of iterations required (in thousands), average lower bound $\hat{\mL}$ attained at termination and runtime in seconds. When the learning rate is computed using Adam, natural gradients provide some improvement relative to Euclidean gradients in terms of a higher lower bound or shorter runtime in about half of the cases. However, such improvements are much more prevalent and pronounced with Snngm. In particular, natural gradient with Snngm is often able to achieve a much higher lower bound within the first few iterations. While using natural gradients requires more computation, the improved convergence makes up for it and overall runtime can be reduced by up to a factor of 3 to 4.

The value of $\hat{\mL}$ is mostly about the same across different algorithms for the same level of approximation, and $\hat{\mL}$ is naturally lower for the much more restrictive diagonal covariance case. However, for the German credit data, natural gradient with Snngm is able to achieve a distinctly higher lower bound than other approaches for the full and diagonal covariance cases using first order gradient estimates. Algorithms based on second order gradient estimates often require less number of iterations to converge but the runtime is still longer due more intensive computations per iteration. First order gradient estimates are more efficient computationally, although the use of second order estimates for Algorithms 1E and 2E with Adam led to higher lower bounds than what could be achieved using first order estimates for the German credit data. Finally, while a Cholesky decomposition of the full precision instead of covariance matrix leads to similar results, computation time is increased significantly as the size of the data increases due to the matrix inversion operations that are required.



\subsection{Generalized linear mixed models}
Let $y_i = (y_{i1}, \dots, y_{in_i})^T $ denote the $i$th observation for $i=1, \dots, n$. Each $y_{ij}$ is assumed to follow some distribution in the exponential family and $g(\E(y_{ij})) = \eta_{ij}$ for some link function $g(\cdot)$, where the linear predictor, $\eta_{ij} = X_{ij}^T  \beta + Z_{ij}^T  \theta_i$. Here $X_{ij}$ and $Z_{ij}$ denote covariates of length $p$ and $r$ respectively, $\beta$ denotes the fixed effects and $\theta_i \sim \N(0, B^{-1})$ denote the random effects. We assume the priors, $\beta \sim \N(0, \sigma_\beta^2I_p)$ and $B \sim \text{W}(\nu, S)$, where $\text{W}(\nu, S)$ represents the Wishart distribution. We set $\sigma_\beta = 10$ while $\nu$ and $S$ are determined using the default conjugate prior of \cite{Kass2006}. To transform all variables onto $\mathbb{R}$, consider  the Cholesky decomposition $B = WW^T $ where $W$ is lower triangular with positive diagonal entries, and define $W^*$ such that $W_{ii}^* = \log(W_{ii})$ and $W_{ij}^* = W_{ij}$ if $i \neq j$. Then the joint distribution of the GLMM is of the form in Section \ref{sec_sparse precision matrix}, where $\theta_g = [\beta^T , \omega^T ]^T $ and $\omega = \vech(W^*)$. 

We consider two variational approximations. The first is GVA \citep{Tan2018}, where conditional independence structure in the posterior distribution is captured using a sparse precision matrix, whose Cholesky factor $T$ is of the form in \eqref{sparse T}. Thus GVA can be found using Algorithm 2S. The second is reparametrized variational Bayes \cite[RVB,][]{Tan2021}, where posterior dependence between local and global variables is first minimized by applying an invertible affine transformation on the local variables. \cite{Tan2021} considers two transformations, which lead to the approaches RVB1 and RVB2. RVB1 is more suited to Poisson and binomial GLMMs while RVB2 works better for Bernoulli models. Let $\tilde{\theta} = (\tilde{\theta}_1^T, \dots, \tilde{\theta}_n^T, \theta_g^T)^T $, where $\tilde{\theta}_1, \dots, \tilde{\theta}_n$ are the transformed local variables. Variational Bayes is then applied by assuming $q(\tilde{\theta}) = q(\theta_g) \prod_{i=1}^n q(\tilde{\theta}_i)$, and additionally that $q(\theta_g)$ and each $q(\tilde{\theta}_i)$ are Gaussian. Thus $q(\tilde{\theta}) = \N(\mu, \Sigma)$ where $\Sigma$ is a block diagonal matrix with $n+1$ blocks. If we consider a Cholesky decomposition $CC^T  = \Sigma$, then RVB1 and RVB2 can be obtained using Algorithm 1S. In RVB, the local variables are transformed to be approximately Gaussian with mean 0 and variance 1. Hence, we initialize $C$ as a diagonal matrix where diagonal elements corresponding to local variables and global variables are set at 1 and 0.1 respectively.

We study three benchmark datasets analyzed in \cite{Tan2021}. The first is the Epilepsy data \citep{Thall1990}, where $n=59$ epileptics are randomly assigned a new drug Progabide or a placebo, and $y_{ij}$ is the number of seizures of patient $i$ in the two weeks before clinic visit $j$ for $j=1, \dots, 4$. Consider the Poisson random slope model,
\begin{align*}
\log \mu_{ij} = \beta_1+\beta_2 \text{Base}_i+\beta_3 \text{Trt}_i + \beta_4 \text{Base}_i \times \text{Trt}_i +\beta_5 \text{Age}_i +\beta_6 \text{Visit}_{ij} +b_{i1} + b_{i2} \text{Visit}_{ij},
\end{align*}
where the covariates for patient $i$ are $\text{Base}_i$ (log(number of baseline seizures/4)), $\text{Trt}_i$  (1 for drug and 0 for placebo), $\text{Age}_i$ (log(age of patient at baseline) centered at zero) and $\text{Visit}_{ij}$ (coded as $-0.3$, $-0.1$, $0.1$, $0.3$ for $j=1, \dots, 4$). For the prior hyperparameters, $\nu = 3$, $S_{11} = 11.0169$, $S_{12} = -0.1616$ and $S_{22} = 0.5516$.

\begin{figure}[b!]
\centering 
\includegraphics[height=280pt]{glmm.pdf}
\caption{GLMMs: Average lower bound attained over past 1000 iterations.}
\label{Fig4}
\end{figure}

\begin{table}
\caption{\label{Tab6} GLMMs: Number of iterations ($T$) in thousands that each algorithm was run, average lower bound $\hat{\mL}$ attained at termination and runtime in seconds.} 
\centering
\begin{small}
\fbox{\begin{tabular}{llrcrrcrrcr}
\hline
& & \multicolumn{3}{c}{Epilepsy} & \multicolumn{3}{c}{Toenail} & \multicolumn{3}{c}{Hers} \\ 
& & $T$ & $\hat{\mL}$ & time & $T$ &  $\hat{\mL}$ & time & $T$ & $\hat{\mL}$ & time  \\   \hline
\multirow{3}{*}{GVA} 
  & Euclidean (Adam) & 37 & 3135.1 & 11.0 & 34 & -646.2 & 17.0 & 38 & -5042.2 & 130.1 \\
    & Natural (Adam) & 38 & 3136.0 & 17.0 & 31 & -646.3 & 21.2 & 36 & -5042.3 & 165.5 \\
    & Natural (Snngm) & 11 & 3139.4 &  4.4 & 16 & -646.4 &  9.8 & 28 & -5041.8 & 118.9 \\
\hline
\multirow{3}{*}{RVB1} 
    & Euclidean (Adam) & 7 & 3140.1 &  2.2 & 19 & -646.7 &  7.2 & 7 & -5041.3 &  15.4 \\
    & Natural (Adam) & 7 & 3140.1 &  2.5 & 19 & -646.7 &  8.2 & 10 & -5041.3 &  26.0 \\
    & Natural (Snngm) & 4 & 3140.1 &  1.4 & 20 & -646.7 &  8.4 & 5 & -5041.3 &  12.9 \\
\hline
\multirow{3}{*}{RVB2} 
    & Euclidean (Adam) & 6 & 3140.2 &  4.5 & 12 & -645.6 & 25.5 & 7 & -5041.0 &  67.5 \\
    & Natural (Adam) & 6 & 3140.2 &  4.7 & 9 & -645.6 & 19.3 & 7 & -5041.0 &  68.5 \\
    & Natural (Snngm) & 6 & 3140.2 &  4.6 & 9 & -645.6 & 19.7 & 5 & -5041.0 &  49.3 \\
\hline
\end{tabular}}
\end{small}
\end{table}

The second is the Toenail data \citep{DeBacker1998}, where two treatments for toenail infection are compared for $n = 294$ patients. The binary response $y_{ij}$ of patient $i$ at the $j$th visit is 1 if degree of separation of nail plate from nail bed is moderate or severe and 0 if none or mild. Consider the random intercept model,
\begin{equation*}
\text{logit} (p_{ij}) = \beta_1 + \beta_2 \text{Trt}_i + \beta_3 t_{ij} + \beta_4 \text{Trt}_i \times t_{ij} + \theta_i, \quad 1 \leq j \leq 7,
\end{equation*}
where for the $i$th patient, $\text{Trt}_i =1$ if 250mg of terbinafine is taken each day and 0 if 200mg of itraconazole is taken, and $t_{ij}$ is the time in months when the patient is evaluated at the $j$th visit. The prior for $B$ is Gamma(0.5, 0.4962). 

The third dataset which is available at \url{www.biostat.ucsf.edu/vgsm/data.html} comes from the Heart and Estrogen/Progestin Study \citep[HERS,][]{Hulley1998}. We examine 2031 women whose data for all covariates are available. The binary response $y_{ij}$ of patient $i$ at the $j$th visit indicates whether the systolic blood pressure is above 140. Consider the random intercept model,
\begin{equation*}
\begin{aligned}
\text{logit} (p_{ij}) &= \beta_1 + \beta_2 \text{age}_i +\beta_3 \text{BMI}_{ij} +\beta_4 \text{HTN}_{ij} + \beta_5 \text{visit}_{ij} + \theta_i, \quad 0 \leq j \leq 5,
\end{aligned}
\end{equation*}
where for patient $i$, $\text{age}_i$ is the age at baseline, $\text{BMI}_{ij}$ is the body mass index at the $j$th visit, $\text{HTN}_{ij}$ indicates whether high blood pressure medication is taken at the $j$th visit and $\text{visit}_{ij}$ is coded as $-1$, $-0.6$, $-0.2$, 0.2, 0.6, 1 for $j=0, 1, \dots, 5$ respectively. We normalize BMI and age to have mean 0 and standard deviation 1 and the prior for $B$ is Gamma(0.5, 0.5079).


Figure \ref{Fig4} shows the average lower bounds attained over the past 1000 iterations for each algorithm and dataset, while Table \ref{Tab6} shows the total number of iterations,  average lower bound attained and runtime. These results are based on first order gradient estimates as $\nabla_\theta^2 h(\theta)$ is highly complex for GVA and RVB and it is unlikely that second order gradient estimates will be more efficient than first order ones. The use of natural gradients with Adam did not bring about significant improvement in convergence relative to Euclidean gradients. In almost all cases, about the same number of iterations is required. However, natural gradients with Snngm yields much better results and runtime can be improved by up to a factor of 2.5 in the case of GVA for the Epilepsy data (together with a distinctly higher lower bound).

Figure \ref{Fig4} also shows that natural gradients with Snngm seem to be able to escape suboptimal local modes more effectively in the case of GVA for the Toenail and Hers datasets. Generally, GVA takes much more iterations to converge than RVB because the local variables in RVB are transformed so that they are approximately distributed as standard normals a posteriori. Hence, by initializing their mean as 0 and variance as 1, the algorithm is already closer to convergence. While RVB1 is the fastest to converge, RVB2 yields the highest lower bound. All three approaches are able to benefit from the use of natural gradients with Snngm, with GVA seeing the biggest reduction in number of iterations required.


\section{Conclusion} \label{sec_Conclusion}
Gaussian variational approximation is widely used and natural gradients provide a direct means of improving the convergence in stochastic gradient ascent, which is particularly important when suboptimal local modes are present. However, the natural gradient update of the precision matrix does not ensure positive definiteness. To tackle this issue, we consider Cholesky decomposition of the covariance or precision matrix. We show that the inverse Fisher information can be found analytically and present natural gradient updates of the Cholesky factors in closed form. We also derive unbiased gradient estimates in terms of the first or second derivative of the log posterior when the gradient of the lower bound is not available analytically. While second order gradient estimates are more stable and can lead to more accurate variational approximations, they require intensive computations and first order gradient estimates are still more efficient in most cases. For high-dimensional models, we impose sparsity constraints on the covariance or precision matrix to incorporate assumptions in variational Bayes or conditional independence structure in the posterior, and we show that efficient natural gradient updates can also be derived in these cases. Finally, we observe that Adam does not always perform well with natural gradients and we propose stochastic normalized natural gradient ascent with momentum (Snngm) as an alternative. We prove the convergence of this approach for $L$-Lipschitz smooth functions with bounded gradients and demonstrate its efficiency in logistic regression and GLMMs for several real datasets.


\vskip 0.2in
\bibliographystyle{chicago}
\bibliography{refnatgrad}



\newpage

\setcounter{section}{0} \renewcommand{\thesection}{S\arabic{section}}
\setcounter{figure}{0} \renewcommand{\thefigure}{S\arabic{figure}}
\setcounter{table}{0} \renewcommand{\thetable}{S\arabic{table}}
\setcounter{equation}{0} \renewcommand{\theequation}{S\arabic{equation}}
\setcounter{lemma}{0} \renewcommand{\thelemma}{S\arabic{lemma}}



\noindent
{\bf \Large Supplementary material}



\section{Natural gradient updates in terms of mean and covariance/precision matrix} \label{S1}

First we derive the natural gradient of $\mL$ with respect to the natural parameter $\lambda$ using $\widetilde{\nabla}_\lambda \mL = \nabla_m \mL$. For the Gaussian, $m = \E[s(\theta)] = (m_1^T, m_2^T )^T$, where $m_1 = \mu$, $m_2 = \vech(\Sigma + \mu \mu^T) $. We introduce $\zeta = (\zeta_1^T, \zeta_2^T)^T$, where
\[
\zeta_1 = \mu = m_1, \quad \zeta_2 = \vech(\Sigma) = m_2 - \vech(m_1 m_1^T).
\]
Then
\[
\nabla_m \zeta 
= \begin{bmatrix} I_d & -2(I_d \otimes \mu^T){D^+}^T \\ 0_{d(d+1)/2 \times d} & I_{d(d+1)/2}  \end{bmatrix}.
\]
Applying chain rule, the natural gradient is 
\[
\begin{aligned}
\widetilde{\nabla}_\lambda \mL 
&= \nabla_m \mL =\nabla_m \zeta \nabla_\zeta \mL \\
&= \begin{bmatrix} I_d & -2(I_d \otimes \mu^T){D^+}^T \\ 0_{d(d+1)/2 \times d} & I_{d(d+1)/2}  \end{bmatrix} \begin{bmatrix} \nabla_\mu \mL \\ \nabla_{\vech(\Sigma)} \mL  \end{bmatrix} \\
&= \begin{bmatrix} \nabla_\mu \mL -2(I_d \otimes \mu^T) \vc(\nabla_{\Sigma} \mL) \\  D^T \vc(\nabla_{\Sigma} \mL) \end{bmatrix} \\
&= \begin{bmatrix} \nabla_\mu \mL - 2(\nabla_{\Sigma}\mL) \mu \\  D^T \vc(\nabla_{\Sigma} \mL) \end{bmatrix}.
\end{aligned}
\]


Next, consider the parametrization, $\kappa = (\mu^T, \vech(\Sigma)^T)^T$. The Fisher information matrix and its inverse are respectively, 
\begin{equation*}
F_\kappa = \begin{bmatrix}
\Sigma^{-1} & 0 \\
0 & \frac{1}{2}D^T (\Sigma^{-1} \otimes \Sigma^{-1}) D
\end{bmatrix}, 
\quad
F_\kappa^{-1} = \begin{bmatrix}
\Sigma & 0 \\
0 & 2D^+ (\Sigma \otimes \Sigma) {D^+}^T
\end{bmatrix}.
\end{equation*}
Hence the natural gradient is 
\[
\widetilde{\nabla}_\kappa \mL = \begin{bmatrix}
\Sigma & 0 \\
0 & 2D^+ (\Sigma \otimes \Sigma) {D^+}^T
\end{bmatrix}
\begin{bmatrix}
\nabla_\mu \mL \\ \nabla_{\vech(\Sigma)} \mL
\end{bmatrix} 
= \begin{bmatrix}
\Sigma \nabla_\mu \mL \\ 2 \vech(\Sigma \nabla_\Sigma \mL \Sigma)
\end{bmatrix}.
\]
Alternatively, $\widetilde{\nabla}_\kappa \mL =  (\nabla_\lambda \kappa)^T \widetilde{\nabla}_\lambda \mL$, which is equal to
\[
\begin{bmatrix}
\Sigma & - \Sigma (\mu^T \Sigma^{-1} \otimes \Sigma^{-1}) D D^+ (\Sigma \otimes \Sigma) {D^+}^T \\
0 & - D^+(\Sigma \otimes \Sigma) {D^+}^T \end{bmatrix}
\begin{bmatrix} \nabla_\mu \mL - 2(\nabla_{\Sigma}\mL) \mu \\ -2 D^T \vc(\nabla_{\Sigma} \mL) \end{bmatrix}  
= \begin{bmatrix} \Sigma \nabla_\mu \mL \\ 2 \vech (\Sigma \nabla_{\Sigma} \mL \Sigma ) \end{bmatrix}.
\]


For the parametrization $\xi = (\mu^T, \vech(\Sigma^{-1})^T)^T$, the Fisher information and its inverse are respectively,
\begin{equation*}
F_\xi = \begin{bmatrix}
\Sigma^{-1} & 0 \\
0 & \frac{1}{2}D^T (\Sigma \otimes \Sigma) D
\end{bmatrix}, 
\quad 
F_\xi^{-1} = \begin{bmatrix}
\Sigma & 0 \\
0 & 2D^+ (\Sigma^{-1} \otimes \Sigma^{-1}) {D^+}^T
\end{bmatrix}.
\end{equation*}
Hence the natural gradient is 
\begin{equation*}
\widetilde{\nabla}_\xi \mL= \begin{bmatrix}
\Sigma & 0 \\
0 & 2D^+ (\Sigma^{-1} \otimes \Sigma^{-1}) {D^+}^T
\end{bmatrix}\begin{bmatrix}
\nabla_\mu \mL \\ \nabla_{\vech(\Sigma)} \mL
\end{bmatrix} = \begin{bmatrix}
\Sigma \nabla_\mu \mL \\ - 2 \vech(\nabla_\Sigma \mL)
\end{bmatrix},
\end{equation*}
Alternatively, $\widetilde{\nabla}_\xi \mL =  (\nabla_\lambda \xi)^T \widetilde{\nabla}_\lambda \mL $, which is equal to
\begin{equation*}
= \begin{bmatrix}
\Sigma & - \Sigma (\mu^T \otimes I){D^+}^T \\
0 & (D^T D)^{-1} \end{bmatrix}
\begin{bmatrix} \nabla_\mu \mL - 2(\nabla_{\Sigma}\mL) \mu \\ -2 D^T \vc(\nabla_{\Sigma} \mL) \end{bmatrix} = \begin{bmatrix} \Sigma \nabla_\mu \mL \\ -2 \vech (\nabla_{\Sigma} \mL) \end{bmatrix}.
\end{equation*}


\section{Loglinear model for Poisson counts}  \label{S2}
Let $y = (y_1, \dots, y_n)^T$ an $X = (x_1, \dots, x_n)^T$. We have $\log \delta_i = x_i^T \theta$ and $\delta_i = \exp(x_i^T \theta)$ for $i=1, \dots, n$. The lower bound can be evaluated analytically and it is given by 
\[
\begin{aligned}
\mL(\lambda) &= \E_q \{\log p(y, \theta) - \log q_\lambda(\theta)\} \\
&= \E_q \bigg[  y^T X \theta - \sum_{i=1}^n \{\exp(x_i^T \theta)  + \log(y_i!)\} - \frac{d}{2} \log(2\pi) - \frac{d}{2} \log(\sigma_0^2)  - \frac{\theta^T \theta}{2\sigma_0^2} \bigg] \\
& \quad - \E_q \bigg[ -\frac{d}{2} \log(2\pi) - \frac{1}{2} \log|\Sigma|- \frac{1}{2} (\theta - \mu)^T \Sigma^{-1} (\theta - \mu) \bigg] \\
&= y^T X \mu - \sum_{i=1}^n \{w_i + \log(y_i!) \} - \frac{\mu^T \mu + \tr(\Sigma)}{2\sigma_0^2} + \frac{1}{2} \log|\Sigma| + \frac{d}{2} \{1 -  \log(\sigma_0^2)\},
\end{aligned}
\]
where $w_i =  \exp (x_i^T \mu + \frac{1}{2} x_i^T \Sigma x_i )$. Let $w=(w_1, \dots, w_n)^T$ and $W = \diag(w)$. Then the Euclidean gradients are given by 
\[
\begin{aligned}
\nabla_\mu \mL &= X^T (y - w)  - \mu/\sigma_0^2, \quad
\nabla_{\vc(\Sigma)} \mL = \tfrac{1}{2} \vc ( \Sigma^{-1}  - I/\sigma_0^2 - X^T W X ) \\
\nabla_{\vech(T)} &= \vech ( \{ \Sigma X^T W X  + \Sigma/\sigma_0^2 - I \} T^{-T} ).
\end{aligned}
\]



\section{Natural gradient updates in terms of mean and Cholesky factor} \label{S3}
First we present the proof of Lemma 1. As this proof requires several results from \cite{Magnus1980} concerning the elimination matrix $L$, these are collected here in Lemma S1 for ease of reference.  
\begin{lemma} \label{Lem_L}
If $P$ and $Q$ are lower triangular $d \times d$ matrices and $N = (I+K)/2$, then
\begin{enumerate}[(i)]
\item $L L^T = I_{d(d+1)/2}$,
\item $(L N L^T)^{-1} = 2I_{d(d+1)/2} -LKL^T$,
\item $N = D L N$,
\item $L^T L (P^T \otimes Q)L^T = (P^T \otimes Q) L^T$ and its transpose, $L (P \otimes Q^T)L^T L = L (P \otimes Q^T)$,
\item $L (P^T \otimes Q) L^T = D^T (P^T \otimes Q) L^T$ and its transpose, $L (P \otimes Q^T) L^T = L (P \otimes Q^T) D$.
\end{enumerate}
\begin{proof}[Lemma S1] 
The proofs can be found in Lemma 3.2 (ii), Lemma 3.4 (ii), Lemma 3.5 (ii) and Lemma 4.2 (i) and (iii) of \cite{Magnus1980} respectively.
\end{proof}
\end{lemma}


\subsection{Proof of Lemma 1}
First, we prove (i):
\begin{align*}
\mathfrak{I}(\Lambda) 
&= L \{ K(\Lambda^{-T} \otimes \Lambda^{-1} ) + I_d \otimes \Lambda^{-T}\Lambda^{-1} \} L^T \\ 
&= L \{ K(\Lambda^{-T} \otimes I_d)(I_d \otimes \Lambda^{-1} ) + ( I_d \otimes \Lambda^{-T}) ( I_d \otimes \Lambda^{-1}) \}L^T\\ 
&= L \{ ( I_d \otimes \Lambda^{-T}) K+ ( I_d \otimes \Lambda^{-T}) \} ( I_d \otimes \Lambda^{-1})L^T\\ 
&= L ( I_d \otimes \Lambda^{-T})(K+ I_{d^2}) ( I_d \otimes \Lambda^{-1})L^T\\ 
&= 2 L ( I_d \otimes \Lambda^{-T})N ( I_d \otimes \Lambda^{-1})L^T.
\end{align*}
Next we prove (ii) and (iii) by using the results in Lemma S1. The roman letters in square brackets on the right indicate which parts of Lemma S1 are used. For (ii),
\begin{align*}
&\{ 2 L  ( I_d \otimes \Lambda^{-T})N ( I_d \otimes \Lambda^{-1})L^T \} \left\{\tfrac{1}{2} L ( I_d \otimes \Lambda) L^T (L N L^T)^{-1} L (I_d \otimes \Lambda^T)  L^T \right\} \\
&= L  ( I_d \otimes \Lambda^{-T})(DL N) ( I_d \otimes \Lambda^{-1}) ( I_d \otimes \Lambda) L^T (L N L^T)^{-1} L (I_d \otimes \Lambda^T)  L^T && \text{[(iii) \& (iv)]} \\
&= L  ( I_d \otimes \Lambda^{-T})L^T (L N L^T)  (L N L^T)^{-1} L (I_d \otimes \Lambda^T)  L^T &&  \text{[(v)]}\\
&= L  ( I_d \otimes \Lambda^{-T})L^T L (I_d \otimes \Lambda^T) L^T \\
&= L  ( I_d \otimes \Lambda^{-T}) (I_d \otimes \Lambda^T) L^T && \text{[(iv)]}\\
&= L  L^T = I_{d(d+1)/2}. && \text{[(i)]}
\end{align*}
For (iii),
\begin{align*}
\mathfrak{I}(\Lambda)^{-1} \vech(G)  &= \tfrac{1}{2}L ( I_d \otimes \Lambda) L^T(L N L^T)^{-1}L (I_d \otimes \Lambda^T) L^T\vech( \bar{G})  \\
&= \tfrac{1}{2}L ( I_d \otimes \Lambda) L^T(2I_{d(d+1)/2} -LKL^T) L (I_d \otimes \Lambda^T) \vc(\bar{G})  && \text{[(ii)]} \\
&= \tfrac{1}{2}L ( I_d \otimes \Lambda)  (2I_{d^2} -L^TLK) L^T L \vc( \Lambda^T \bar{G}) \\
&= \tfrac{1}{2}L ( I_d \otimes \Lambda) (2I_{d^2} -L^TLK) L^T\vech(\bar{H})  \\
& = \tfrac{1}{2}L ( I_d \otimes \Lambda) (2I_{d^2} -L^TLK) \vc(\bar{H})  \\
& = L ( I_d \otimes \Lambda) \vc(\bar{H})  -  \tfrac{1}{2}L ( I_d \otimes \Lambda)  L^TLK \vc(\bar{H})  \\
& = L\vc(\Lambda\bar{H}) - \tfrac{1}{2}L ( I_d \otimes \Lambda)  L^T \vech(\bar{H}^T)  \\
& = \vech(\Lambda\bar{H}) - \tfrac{1}{2}L ( I_d \otimes \Lambda) \vc(\dg (\bar{H}))  \\
& = \vech(\Lambda\bar{H}) - \tfrac{1}{2} \vech(\Lambda\dg (\bar{H}))  
= \vech(\Lambda \dH).
\end{align*}



\subsection{Proof of Theorem 1} 
First we derive the Fisher information and its inverse for each of the two parametrizations. We have 
\[
\ell_q = \log q_\lambda(\theta) = -\tfrac{d}{2} \log(2\pi) - \tfrac{1}{2} \log|\Sigma| - \tfrac{1}{2}(\theta-\mu)^T \Sigma^{-1} (\theta-\mu).
\]

For the first parametrization, $\lambda = (\mu^T , \vech(C)^T )^T$,  let $z = C^{-1} (\theta -\mu)$ to simplify expressions. The first order derivatives are
\[
\nabla_\mu \ell_q = \Sigma^{-1} (\theta - \mu), \quad 
\nabla_{\vech(C)} \ell_q = \vech(C^{-T} z z^T - C^{-T}),
\] 
and $ -\nabla_\lambda^2 \ell_q$ is given by
\[
\begin{bmatrix}
\Sigma^{-1} & \{(C^{-T} \otimes z^T C^{-1}) + (z^T \otimes \Sigma^{-1})\}L^T \\
\cdot  &  L[\{ (C^{-1} \otimes C^{-T} zz^T) + (zz^T C^{-1} \otimes C^{-T}) - (C^{-1} \otimes C^{-T})  \}K + zz^T \otimes \Sigma^{-1}]L^T
\end{bmatrix}.
\]
Taking the negative expectation of $\nabla_\lambda^2 \ell_q$ and applying the fact that $\E(z) = 0$ and $\E(z z^T) = I_d$, we obtain 
\begin{equation*} 
F_\lambda = \begin{bmatrix}
\Sigma^{-1} & 0 \\
0 & L \{  (C^{-1} \otimes C^{-T }) K +  (I_d \otimes \Sigma^{-1}) \}L^T 
\end{bmatrix} =
\begin{bmatrix}
\Sigma^{-1} & 0 \\ 0 & \mathfrak{I}(C)
\end{bmatrix}.
\end{equation*}
Thus $F_\lambda^{-1} = \blockdiag(\Sigma, \mathfrak{I}(C)^{-1})$.


For the second parametrization, $\lambda = (\mu^T, \vech(T)^T)^T$, the first order derivative, 
\[
\nabla_{\vech(T)} \ell_q =\vech(T^{-T} - (\theta -\mu) (\theta -\mu)^T T) 
\]
and 
\[
\nabla_\lambda^2 \ell_q= -
\begin{bmatrix}
\Sigma^{-1} & - \{(\theta -\mu)^T T \otimes I_d + T \otimes (\theta -\mu)^T\}L^T \\
\cdot  &   L\{ (T^{-1} \otimes T^{-T} ) K + I_d \otimes (\theta -\mu)(\theta -\mu)^T \}L^T
\end{bmatrix}.
\]
Taking the negative expectation of $\nabla_\lambda^2 \ell_q$ and applying the fact that $\E(\theta) = \mu$ and $\E[(\theta -\mu)(\theta -\mu)^T] = \Sigma$, we obtain 
\begin{equation*} 
F_\lambda = \begin{bmatrix}
\Sigma^{-1} & 0 \\
0 & L\{ (T^{-1} \otimes T^{-T } ) K + I_d \otimes \Sigma \}L^T 
\end{bmatrix} =
\begin{bmatrix}
\Sigma^{-1} & 0 \\ 0 & \mathfrak{I}(T)
\end{bmatrix}.
\end{equation*}
Thus $F_\lambda^{-1} = \blockdiag(\Sigma, \mathfrak{I}(T)^{-1})$.

Suppose $\nabla_{\vech(\Lambda)} \mL = \vech(G)$. Then, for each parametrization, the natural gradient is 
\[
\widetilde{\nabla} _{\lambda} \mL = F_{\lambda}^{-1} \nabla_{\lambda} \mL 
= \begin{bmatrix} \Sigma & 0 \\ 0 & \mathfrak{I}(\Lambda)^{-1} \end{bmatrix}
\begin{bmatrix} \nabla_\mu \mL \\ \vech(G) \end{bmatrix} 
= \begin{bmatrix} \Sigma \nabla_\mu \mL  \\  \vech(\Lambda \dH) \end{bmatrix},
\]
where we have applied Lemma 1 (iii) in the last step.


\subsection{Proof of Corollary 1}
If $\xi = ((T^T \mu)^T, \vech(T)^T)^T$, then 
\begin{equation*}
\begin{aligned}
\widetilde{\nabla}_\xi \mL =  (\nabla_\lambda \xi)^T \widetilde{\nabla}_\lambda \mL 
= \begin{bmatrix} T^T & (I \otimes \mu^T) L^T \\ 0 & I \end{bmatrix}
\begin{bmatrix} \Sigma \nabla_\mu \mL \\  \vech(T \dH) \end{bmatrix} 
= \begin{bmatrix}
T^{-1} \nabla_\mu \mL + \dH^T T^T \mu \\  \vech(T \dH) \end{bmatrix}.
\end{aligned}
\end{equation*}
The natural gradient ascent update is 
\[
\begin{aligned}
T^{(t+1)T} \mu^{(t+1)} &=T^{(t)T} \mu^{(t)} + \rho_t \{ T^{(t)-1} \nabla_\mu \mL  + \dH^{(t)T} T^{(t)T} \mu^{(t)}  \},  \\
T^{(t+1)} &= T^{(t)} + \rho_t T^{(t)}  \dH^{(t)},
\end{aligned}
\]
The first line simplifies to 
\[
\begin{aligned}
T^{(t+1)T} \mu^{(t+1)} &=\{T^{(t)} + \rho_t T^{(t)}\dH^{(t)} \} ^T \mu^{(t)} + \rho_t T^{(t)-1} \nabla_\mu \mL  \\
\implies \mu^{(t+1)} &= \mu^{(t)} + \rho_t T^{-(t+1)T} T^{(t)-1} \nabla_\mu \mL.
\end{aligned}
\]


\subsection{Proof of Lemma 2}
Define $g(\theta) = (\theta - \mu) ^T e_j h(\theta)$ to be a function from $\mathbbm{R}^d$ to $\mathbbm{R}$ and $e_j$ to be a $d \times 1$ vector with the $j$th element equal to one and zero elsewhere. Then
\[
\nabla_\theta g(\theta) = h(\theta) e_j  +  (\theta - \mu) ^T e_j \nabla_\theta h(\theta).
\] 
Replacing $f(\theta)$ by $g(\theta)$ in Stein's Lemma, we obtain
\[
\E_q[\Sigma^{-1} (\theta-\mu)  (\theta - \mu) ^T e_j h(\theta)] =  \E_q[h(\theta) e_j  +  (\theta - \mu) ^T e_j \nabla_\theta h(\theta)].
\]
This implies that for any $1 \leq i,j \leq d$, 
\[
e_i^T \E_q[\{\Sigma^{-1} (\theta-\mu)  (\theta - \mu) ^T - I_d\}  h(\theta)]e_j =  e_i^T \E_q[ \nabla_\theta h(\theta)(\theta - \mu) ^T] e_j.
\]
Thus $\E_q[\{\Sigma^{-1} (\theta-\mu)  (\theta - \mu) ^T - I_d\}  h(\theta)] =  \E_q[ \nabla_\theta h(\theta)(\theta - \mu) ^T]$ since every $(i,j)$ element of these two matrices agree with each other.


\subsection{Proof of Theorem 2}
First we prove (i) of Theorem 2. Post-multiplying the identity in Lemma 2 by $C^{-T}$, we obtain
\[
\E_q[\{\Sigma^{-1} (\theta-\mu)  (\theta - \mu) ^T C^{-T} - C^{-T}\}  h(\theta)] =  \E_q[ \nabla_\theta h(\theta)(\theta - \mu) ^T C^{-T}] = \E_q(\G_1).
\]
Hence
\[
\begin{aligned}
\nabla_{\vech(C)} \mL &= \int \nabla_{\vech(C)} q_\lambda (\theta)  h(\theta) \df \theta \\
& =  \int q_\lambda (\theta) \vech \left\{\Sigma^{-1}(\theta -\mu)(\theta-\mu)^TC^{-T} -  C^{-T} \right\} h(\theta) \df \theta \\
& = \E_q \left[ \vech \{\Sigma^{-1}(\theta -\mu)(\theta-\mu)^TC^{-T} -  C^{-T}\} h(\theta) \right] 
= \E_q \vech(\G_1),
\end{aligned}
\]
and the first part of the identity in Theorem 2 (i) is shown. For the second part of the identity, we have $\E_q[\Sigma^{-1} (\theta-\mu) \nabla_\theta h(\theta)^T]= \E_q[\nabla_\theta^2 h(\theta) ]$ from Price's Theorem. Taking the transpose and post-multiplying by $C$, we obtain 
\[
\E_q(\G_1) = \E_q[ \nabla_\theta h(\theta)  (\theta-\mu)^T C^{-T}]= \E_q[\nabla_\theta^2 h(\theta)C ] = \E_q(\F_1).
\]

Next, First we prove (ii) of Theorem 2. Taking the transpose of the identity in Lemma 2 and post-multiplying by $T^{-T}$,
\[
\E_q[\{ (\theta-\mu)  (\theta - \mu) ^T T - T^{-T}\}  h(\theta)] = \E_q[ (\theta - \mu) \nabla_\theta h(\theta)^T T^{-T}] = -\E_q(\G_2).
\] 
Hence
\[
\begin{aligned}
\nabla_{\vech(T)} \mL &= \int \nabla_{\vech(T)} q_\lambda (\theta)  h(\theta) \df \theta \\
& = \int q_\lambda (\theta) \vech \{T^{-T} -  (\theta-\mu)(\theta - \mu)^T T\}  h(\theta) \df \theta \\
& = \E_q[ \vech \{T^{-T} -  (\theta-\mu)(\theta - \mu)^T T\} h(\theta)] 
= \E_q \vech (\G_2) ,
\end{aligned}
\]
and the first part of the identity in Theorem 2 (ii) is shown. For the second part of the identity, we have $\E_q[\Sigma^{-1} (\theta-\mu) \nabla_\theta h(\theta)^T]= \E_q[\nabla_\theta^2 h(\theta) ]$ from Price's Theorem. Pre-multiplying by $\Sigma$ and post-multiplying by $T^{-T}$, we obtain 
\[
- \E_q (\G_2) = \E_q[(\theta-\mu) \nabla_\theta h(\theta)^TT^{-T}]= \E_q[\Sigma \nabla_\theta^2 h(\theta) T^{-T}] = - \E_q(\F_2).
\]



\section{Natural gradient for sparse precision matrix} \label{S4}
We derive the natural gradient where the precision matrix is sparse.  In this case,
\[
\begin{aligned}
\ell_q &= \log q_\lambda (\theta) = -\frac{d}{2} \log (2\pi) + \log|T_g| + \sum_{i=1}^N \log |T_i| - \frac{1}{2} \sum_{i=1}^n  (\theta_i - \mu_i)^T T_i T_i^T (\theta_i - \mu_i) \\
&- \frac{1}{2} (\theta_g - \mu_g)^T \left( \sum_{i=1}^n T_{gi} T_{g_i}^T + T_g T_g^T \right)  (\theta_g - \mu_g) - (\theta_g - \mu_g)^T \sum_{i=1}^n T_{gi} T_i^T  (\theta_i - \mu_i).
\end{aligned}
\]
In addition, if we integrate out all other variables from $q_\lambda(\theta) = \prod_{i=1}^n q(\theta_i|\theta_g) q(\theta_g)$ except $\theta_i$ and $\theta_g$, then we have $q(\theta_i, \theta_g) = q(\theta_i|\theta_g) q(\theta_g)$, whose covariance matrix is 
\[
\begin{aligned}
\begin{bmatrix} T_i & 0 \\ T_{gi} & T_g \end{bmatrix}^{-T} 
&\begin{bmatrix} T_i & 0 \\ T_{gi} & T_g \end{bmatrix}^{-1} 
= \begin{bmatrix} T_i^{-T} & -T_i^{-T}T_{gi}^T T_g^{-T}   \\ 0 & T_g^{-T} \end{bmatrix}
\begin{bmatrix} T_i^{-1} & 0 \\ -T_g^{-1} T_{gi} T_i^{-1} & T_g^{-1} \end{bmatrix} \\
&= \begin{bmatrix} T_i^{-T}  T_i^{-1} + T_i^{-T}T_{gi}^T T_g^{-T} T_g^{-1} T_{gi} T_i^{-1} & -T_i^{-T}T_{gi}^T T_g^{-T}  T_g^{-1} \\[1mm]
 -T_g^{-T} T_g^{-1} T_{gi} T_i^{-1} & T_g^{-T} T_g^{-1} \end{bmatrix}.
\end{aligned}
\]
Hence $\Cov(\theta_g) = \E\{ (\theta_g - \mu_g)(\theta_g - \mu_g)^T\} = T_g^{-T} T_g^{-1}$, 
\[
\begin{aligned}
\Cov(\theta_i) &= \E\{(\theta_i - \mu_i) (\theta_i - \mu_i)^T\} = T_i^{-T}  T_i^{-1} + T_i^{-T}T_{gi}^T T_g^{-T} T_g^{-1} T_{gi} T_i^{-1}, \\
\Cov(\theta_i, \theta_g) &= \E\{(\theta_i - \mu_i)(\theta_g - \mu_g)^T \}
= -T_i^{-T}T_{gi}^T T_g^{-T}  T_g^{-1}.
\end{aligned}
\]


First, we find the elements in the Fisher information matrix. Differentiating $\ell_q$ with respect to $T_i$ and taking expectation with respect to $q_\lambda (\theta)$, 
\[
\begin{aligned}
\nabla_{\vech(T_i)} \ell_q &= \vech\{T_i^{-T} - (\theta_i - \mu_i) (\theta_i - \mu_i)^T T_i - (\theta_i - \mu_i)(\theta_g - \mu_g)^T T_{gi} \}, \\
\nabla_{\vech(T_i)}^2 \ell_q &= - L \{ (T_i^{-1} \otimes T_i^{-T})K +  I \otimes (\theta_i - \mu_i)  (\theta_i - \mu_i)^T \}L^T, \\
\E[\nabla_{\vech(T_i)}^2 \ell_q] &= -L \{  (T_i^{-1} \otimes T_i^{-T})K + I \otimes T_i^{-T} (I + T_{gi}^T T_g^{-T} T_g^{-1} T_{gi}) T_i^{-1} \}L^T \\
&= - \mathfrak{I}(T_i) -  L ( I \otimes T_i^{-T} T_{gi}^T T_g^{-T} T_g^{-1} T_{gi} T_i^{-1} ) L^T.
\end{aligned}
\]
Differentiating $\nabla_{\vech(T_i)} \ell_q$ with respect to $T_{gi}$ and taking expectation with respect to $q_\lambda (\theta)$, 
\[
\begin{aligned}
\nabla_{\vech(T_i), \vc(T_{gi})}^2 \ell_q &= - L \{I \otimes (\theta_i - \mu_i)(\theta_g - \mu_g)^T  \}, \\
\E [\nabla_{\vech(T_i), \vc(T_{gi})}^2 \ell_q]&= L (I \otimes T_i^{-T} T_{gi}^T T_g^{-T} T_g^{-1} ).
\end{aligned}
\]
Differentiating $\ell_q$ with respect to $T_{gi}$ and taking expectation with respect to $q_\lambda (\theta)$, 
\[
\begin{aligned}
\nabla_{\vc(T_{gi})} \ell_q &= - \vc[ (\theta_g - \mu_g)(\theta_g - \mu_g)^T  T_{gi} + (\theta_g - \mu_g)(\theta_i - \mu_i)^T T_i]. \\
\nabla_{\vc(T_{gi})}^2 \ell_q &= - (I \otimes  (\theta_g - \mu_g)(\theta_g - \mu_g)^T ) . \\
\E[\nabla_{\vc(T_{gi})}^2 \ell_q] &= - (I \otimes T_g^{-T}T_g^{-1}).
\end{aligned}
\]
Differentiating $\ell_q$ with respect to $T_g$ and taking expectation with respect to $q_\lambda (\theta)$, 
\begin{align*}
\nabla_{\vech (T_g)} \ell_q &= \vech[ T_g^{-T} - (\theta_g - \mu_g)(\theta_g - \mu_g)^T T_g]. \\
\nabla_{\vech (T_g)}^2  \ell_q &= - L [ (T_g^{-1} \otimes T_g^{-T})K + I \otimes (\theta_g - \mu_g)(\theta_g - \mu_g)^T ]  L^T \\
-\E[\nabla_{\vech (T_g)}^2  \ell_q ]&= - L [ (T_g^{-1} \otimes T_g^{-T})K + I \otimes T_g^{-T} T_g^{-1} ]  L^T = - \mathfrak{I}(T_g).
\end{align*}
Thus the Fisher information matrix is $F_\lambda = \blockdiag(\Sigma^{-1}, F_1, \dots, F_n, \mathfrak{I}(T_g) )$, where 
\begin{equation*}
\begin{aligned}
F_i = \begin{bmatrix}
F_{11i} & F_{12i} \\
F_{12i}^T & F_{22i}
\end{bmatrix}
\end{aligned}
\qquad  \text{and} \qquad
\begin{aligned}
F_{11i} &= \mathfrak{I}(T_i) +  L ( I \otimes T_i^{-T} T_{gi}^T T_g^{-T} T_g^{-1} T_{gi} T_i^{-1} ) L^T, \\
F_{12i} &= - L (I \otimes T_i^{-T} T_{gi}^T T_g^{-T} T_g^{-1}) , \\
F_{22i} &= (I \otimes T_g^{-T}T_g^{-1}). 
\end{aligned}
\end{equation*}
Since $F_{22i}^{-1} = I \otimes T_gT_g^T$, and $F_{12i}F_{22i}^{-1} = - L (I \otimes T_i^{-T} T_{gi}^T)$, $F_{11i} - F_{12i} F_{22i}^{-1} F_{12i}^T = \mathfrak{I}(T_i)$. Hence using block matrix inversion,
\[
F_i^{-1} = \begin{bmatrix}
\mathfrak{I}(T_i)^{-1} & \mathfrak{I}(T_i)^{-1} L (I \otimes T_i^{-T} T_{gi}^T)  \\
\cdot &  (I \otimes T_g T_g^T) +  (I \otimes T_{gi} T_i^{-1}) L^T \mathfrak{I}(T_i)^{-1} L (I \otimes T_i^{-T} T_{gi}^T)
\end{bmatrix}.
\]

Next, we simplify the expression for the natural gradient. For $i=1, \dots, n$,
\[
\begin{aligned}
\begin{bmatrix}
\widetilde{\nabla}_{\vech(T_i)} \mL \\ \widetilde{\nabla}_{\vc(T_{gi})} \mL
\end{bmatrix} 
&= F_i^{-1} \begin{bmatrix}  \vech(A_i)  \\  \vc(G_{gi})  \end{bmatrix}.
\end{aligned}
\]
Applying Lemma 1, 
\begin{align*}
\widetilde{\nabla}_{\vech(T_i)} \mL &= \mathfrak{I}(T_i)^{-1}  \vech(A_i) +  \mathfrak{I}(T_i)^{-1}  L (I \otimes T_i^{-T} T_{gi}^T) \vc(G_{gi}) \\
&= \mathfrak{I}(T_i)^{-1} \vech(A_i + T_i^{-T} T_{gi}^T G_{gi})  \\
&= \mathfrak{I}(T_i)^{-1} \vech(G_i )  \\
&= \vech(T_i \dH_i). \\
\widetilde{\nabla}_{\vc(T_{gi})} \mL 
&= (I \otimes T_gT_g^T)  \vc(G_{gi})+  (I \otimes T_{gi} T_i^{-1}) L^T \widetilde{\nabla}_{\vech(T_i)} \mL \\
&= \vc(T_gT_g^T G_{gi}) +  (I \otimes T_{gi} T_i^{-1}) L^T \vech (T_i \dH_i) \\
&= \vc(\Sigma_g^{-1} G_{gi} + T_{gi}\dH_i).
\end{align*}



\subsection{Stochastic natural gradients}
Let $\theta_a = (\theta_1^T, \dots, \theta_n^T)^T$,  $\mu_a = (\mu_1^T, \dots, \mu_n^T)^T$, $v_a=(v_1^T, \dots, v_n^T)^T$  and
\[
T = \begin{bmatrix} T_a & 0 \\ T_{ga} & T_g \end{bmatrix}, \quad 
T^{-1} = \begin{bmatrix} T_a^{-1} & 0 \\ - T_g^{-1} T_{ga}  T_a^{-1} & T_g^{-1} \end{bmatrix}, 
\]
where $T_a = \blockdiag(T_1, \dots, T_n)$ and $T_{ga} = [T_{g1} \dots T_{gn}]$. 
Note that 
\[
\begin{aligned}
v &= \begin{bmatrix} T_a^{-1} & 0 \\ - T_g^{-1} T_{ga}  T_a^{-1} & T_g^{-1} \end{bmatrix} \begin{bmatrix} \nabla_{\theta_a} h(\theta)  \\ \nabla_{\theta_g} h(\theta)  \end{bmatrix}
= \begin{bmatrix}
T_a^{-1} \nabla_{\theta_a} h(\theta) \\
T_g^{-1} \{\nabla_{\theta_g} h(\theta) - T_{ga}  T_a^{-1} \nabla_{\theta_a} h(\theta)\} \end{bmatrix}
= \begin{bmatrix} v_a \\ v_g \end{bmatrix}, \\
u &=T_d^{-T} T^T (\theta -\mu)  = \begin{bmatrix}
\begin{bmatrix} (\theta_i - \mu_i) + T_i^{-T} T_{gi} (\theta_g - \mu_g) \end{bmatrix}_{i=1:n} \\
\theta_g - \mu_g \end{bmatrix} = 
\begin{bmatrix} [u_i]_{i=1:n} \\ u_g \end{bmatrix}.
\end{aligned}
\]


First, we consider extracting entries in $\G_2 = - (\theta - \mu)  \nabla_\theta h(\theta)^T T^{-T}$ corresponding to nonzero entries in $T$. We have
\[
\begin{aligned}
\G_2 &= - \begin{bmatrix}
(\theta_a - \mu_a) \nabla_{\theta_a} h(\theta)^T &  (\theta_a - \mu_a) \nabla_{\theta_g} h(\theta)^T \\
(\theta_g - \mu_g) \nabla_{\theta_a} h(\theta)^T &  (\theta_g - \mu_g) \nabla_{\theta_g} h(\theta)^T \\
\end{bmatrix}
\begin{bmatrix} T_a^{-T} &  - T_a^{-T} T_{ga}^T  T_g^{-T} \\ 0 & T_g^{-T} \end{bmatrix}
\\
&= - \begin{bmatrix}
(\theta_a - \mu_a) \nabla_{\theta_a} h(\theta)^T T_a^{-T} & \cdot \\
(\theta_g - \mu_g) \nabla_{\theta_a} h(\theta)^T  T_a^{-T} &  (\theta_g - \mu_g) \{\nabla_{\theta_g} h(\theta) -  T_{ga} T_a^{-1} \nabla_{\theta_a} h(\theta) \}^T   T_g^{-T}
\end{bmatrix} \\
&= - \begin{bmatrix} (\theta_a - \mu_a) v_a^T & \cdot \\
(\theta_g - \mu_g) v_a^T &  (\theta_g - \mu_g) v_g^T \end{bmatrix}.
\end{aligned} 
\]

Thus
\[
\begin{aligned}
\nabla_{\vech(T_i)} \mL &=- \E_q \vech \{ (\theta_i - \mu_i)  v_i^T\} 
=- \E_q \vech \{ (u_i - T_i^{-T} T_{gi} u_g)  v_i^T\} , \\
\nabla_{\vech(T_g)} \mL & = -\E_q \vech \{ (\theta_g - \mu_g) v_g^T \} 
= -\E_q \vech \{ u_g v_g^T \}, \\
\nabla_{\vc(T_{gi})} \mL &= -\E_q \vc\{  (\theta_g - \mu_g) v_i^T \} 
= -\E_q \vc\{  u_g v_i^T \}.
\end{aligned}
\]



Next, we consider extracting entries in $\F_2 = -\Sigma \nabla_\theta^2 h(\theta) T^{-T}$ corresponding to nonzero entries in $T$. If $\Sigma_g = T_g^{-T} T_g^{-1}$, $\Sigma_i = T_i^{-T} T_i$ and $\Sigma_a = T_a^{-T} T_a^{-1}$, then  
\[
\begin{aligned}
\Sigma = T^{-T} T^{-1} 
&=  \begin{bmatrix} T_a^{-T} &  - T_a^{-T}  T_{ga}^T  T_g^{-T} \\ 0 & T_g^{-T} \end{bmatrix} \begin{bmatrix} T_a^{-1} & 0 \\ - T_g^{-1} T_{ga}  T_a^{-1} & T_g^{-1} \end{bmatrix} \\
&= \begin{bmatrix} \Sigma_a  + T_a^{-T}  T_{ga}^T  \Sigma_g T_{ga}  T_a^{-1} &  - T_a^{-T}  T_{ga}^T  \Sigma_g  \\[1mm]  - \Sigma_g T_{ga} T_a^{-1} & \Sigma_g  \end{bmatrix} 
\end{aligned} 
\]
and 
\[
\begin{aligned}
\nabla_\theta^2 h(\theta) T^{-T}& = \begin{bmatrix} \nabla_{\theta_a}^2 h(\theta)  &   \nabla_{\theta_a, \theta_g}^2 h(\theta)  \\  \nabla_{\theta_g, \theta_a}^2 h(\theta) &  \nabla_{\theta_g}^2 h(\theta)  \end{bmatrix}
\begin{bmatrix} T_a^{-T} &  - T_a^{-T}  T_{ga}^T  T_g^{-T} \\ 0 & T_g^{-T} \end{bmatrix} \\
&= \begin{bmatrix} \nabla_{\theta_a}^2 h(\theta) T_a^{-T}   &   \{\nabla_{\theta_a, \theta_g}^2 h(\theta) - \nabla_{\theta_a}^2 h(\theta) T_a^{-T} T_{ga}^T \} T_g^{-T}      \\  \nabla_{\theta_g, \theta_a}^2 h(\theta) T_a^{-T}   &  \{\nabla_{ \theta_g}^2 h(\theta) - \nabla_{\theta_g, \theta_a}^2 h(\theta) T_a^{-T} T_{ga}^T \} T_g^{-T}    \end{bmatrix}.
\end{aligned}
\]
Hence
\[
\begin{aligned}
\F_{2, 11} &=  - \{\Sigma_a \nabla_{\theta_a}^2 h(\theta)  + T_a^{-T} T_{ga}^T \Sigma_g T_{ga}  T_a^{-1} \nabla_{\theta_a}^2 h(\theta) - T_a^{-T} T_{ga}^T \Sigma_g  \nabla_{\theta_g, \theta_a}^2 h(\theta)  \} T_a^{-T},  \\
\F_{2,21} &= -  \Sigma_g \{\nabla_{\theta_g, \theta_a}^2 h(\theta) - T_{ga} T_a^{-1} \nabla_{\theta_a}^2 h(\theta) \} T_a^{-T}, \\
\F_{2,22} &= -  \Sigma_g \{ \nabla_{ \theta_g}^2 h(\theta)  - \nabla_{\theta_g, \theta_a}^2 h(\theta) T_a^{-T} T_{ga}^T -  T_{ga} T_a^{-1} \nabla_{\theta_a, \theta_g}^2 h(\theta)  \\
& \quad +  T_{ga} T_a^{-1} \nabla_{\theta_a}^2 h(\theta) T_a^{-T} T_{ga}^T \} T_g^{-T}.
\end{aligned}
\]
Then, we obtain
\[
\begin{aligned}
\nabla_{\vech(T_i)} \mL &= - \E_q \vech (\Sigma_i  \nabla_{\theta_i}^2 h(\theta)  T_i^{-T} - T_i^{-T} T_{gi}^T U_{gi} ), \\
\nabla_{\vc(T_{gi})} \mL &= -\E_q \vc (U_{gi}), \\
\nabla_{\vech(T_g)} \mL &= - \E_q \vech\bigg[ \Sigma_g \bigg\{  \nabla_{\theta_g}^2 h(\theta) - \sum_{i=1}^n T_{gi} T_i^{-1} \nabla_{\theta_i, \theta_g}^2 h(\theta)  - \sum_{i=1}^n \nabla_{\theta_g, \theta_i}^2 h(\theta)T_i^{-T} T_{gi}^T \\
& +  \sum_{i=1}^n T_{gi} T_i^{-1} \nabla_{\theta_i}^2 h(\theta) T_i^{-T} T_{gi}^T \bigg\} T_g^{-T}\bigg] =- \E_q \vech \bigg\{ U_{gg} -  \sum_{i=1}^n U_{gi} T_{gi}^T T_g^{-T}\bigg\} .
\end{aligned}
\]




\section{Proof of Theorem 3} \label{S5}
First we derive some intermediate results that are needed for the proof of Theorem 3. Let $\langle \cdot, \cdot\rangle$ denote the inner product and $\bar{g}_{t}  = \widetilde{g}_t/\|\widetilde{g}_t\|$ so that $\|\bar{g}_t\| = 1$. 


\begin{lemma} \label{lem_beta}
\[
\sum_{i=0}^{t-1} i \beta^i \leq \frac{\beta(1-\beta^t)}{(1-\beta)^2}.
\]
\begin{proof}
\[
\begin{aligned}
\sum_{i=0}^{t-1} i \beta^i &= \beta \{1 + 2\beta + \dots (t-1) \beta^{t-2}\}
= \beta \frac{\df}{\df \beta} (\beta + \beta^2 + \dots + \beta^{t-1}) \\
&= \beta \frac{\df}{\df \beta} \left\{ \frac{\beta(1-\beta^{t-1})}{1-\beta} \right\}
= \frac{\beta \{1 - \beta^t - t \beta^{t-1} (1-\beta)\} }{(1-\beta)^2} \leq \frac{\beta(1-\beta^t)}{(1-\beta)^2}.
\end{aligned}
\]
\end{proof}
\end{lemma}


\subsection{Bounds for norm of natural gradient} 
We have $\| \widetilde{g}_t \|^2 = \widehat{g}_t^T  F_t^{-2}\widehat{g}_t$.  By (A2), $\|\widehat{g}_t\| \leq R$ and by (A4), $R_1 \leq \ev(F_t) \leq R_2$. This implies that $1/R_2 \leq \ev(F_t^{-1}) \leq 1/R_1$. Using the result on page 18 of \cite{Magnus2019},
\[
\begin{aligned}
\frac{1}{R_2} &\leq \frac{g_t^T  F_t^{-1}g_t}{g_t^T g_t} \leq \frac{1}{R_1}  
\implies \frac{\|g_t\|^2}{R_2} \leq \langle g_t, F_t^{-1}g_t \rangle  \leq \frac{\|g_t\|^2}{R_1}, \\
\frac{1}{R_2^2} &\leq \frac{\widehat{g}_t^T  F_t^{-2}\widehat{g}_t}{\widehat{g}_t^T  \widehat{g}_t} \leq \frac{1}{R_1^2}  
\implies \frac{\|\widehat{g}_t\|}{R_2} \leq \| \widetilde{g}_t \| \leq \frac{\|\widehat{g}_t\|}{R_1} \leq \frac{R}{R_1}. 
\end{aligned}
\]



\subsection{Bound on momentum} \label{bound on momentum}
Since $m_0 = 0$, $m_t = \beta m_{t-1} + (1-\beta) \bar{g}_{t} 
= (1-\beta) \sum_{i=0}^{t-1} \beta^i \bar{g}_{t-i}$ for  $t=1, \dots, T$. Thus,
\begin{equation} \label{bd on m_t}
\|m_t\|  \leq (1-\beta) \sum_{i=0}^{t-1} \beta^{i} \|\bar{g}_{t-i}\|
\leq (1-\beta) \sum_{i=0}^{t-1} \beta^i 
= 1-\beta^t. 
\end{equation}


\subsection{Inequality from $L$-Lipschitz smooth assumption}
Define $G(t) = \mL(\lambda + t(\lambda'-\lambda))$. Then $G'(t) = \nabla_\lambda \mL(\lambda + t(\lambda'-\lambda))^T (\lambda'-\lambda)$. Now,
\[
G(1)  = G(0) + G'(0) + \int_0^1 G'(t) - G'(0) \df t.
\]
Therefore, by Cauchy-Schwarz inequality, 
\begin{align} \label{Lsmooth}
|G(1) - G(0) -G'(0) | &\leq \int_0^1 | \langle \nabla_\lambda \mL(\lambda + t(\lambda'-\lambda)) - \nabla_\lambda \mL(\lambda), \lambda' - \lambda \rangle |\, \df t \nonumber \\
& \leq  L \|\lambda'-\lambda \|^2 \int_0^1 t \, \df t 
= L \|\lambda'-\lambda \|^2/2.\nonumber \\
\therefore  |\mL(\lambda') - \mL(\lambda) &-\langle \nabla_\lambda \mL(\lambda), \lambda' - \lambda \rangle |  \leq L \|\lambda'-\lambda \|^2/2 \nonumber \\ 
\implies  -\mL(\lambda') + \mL(\lambda) &+ \langle \nabla_\lambda \mL(\lambda), \lambda' - \lambda \rangle \leq L \|\lambda'-\lambda \|^2/2.
\end{align}


\subsection{Proof of Theorem 3}
Set $\lambda=\lambda^{(t)}$ and $\lambda'=\lambda^{(t+1)}$ in \eqref{Lsmooth}. Since $\lambda^{(t+1)} = \lambda^{(t)} + \alpha m_t/(1-\beta^t)$, $m_t = (1-\beta) \sum_{i=0}^{t-1} \beta^i \bar{g}_{t-i}$ and $\|m_t\| \leq 1 -\beta^t$ from \eqref{bd on m_t},
\begin{align} \label{E1}
\mL(\lambda^{(t)}) & \leq \mL(\lambda^{(t+1)}) - \langle \nabla_\lambda \mL(\lambda^{(t)}), \lambda^{(t+1)}- \lambda^{(t)} \rangle  + L \| \lambda^{(t+1)}- \lambda^{(t)} \|^2/2 \nonumber \\
&= \mL(\lambda^{(t+1)}) - \frac{\alpha}{1-\beta^t} \langle \nabla \mL(\lambda^{(t)}),m_t \rangle  + \frac{L \alpha^2}{2(1-\beta^t)^2} \| m_t \|^2 \nonumber \\
& \leq \mL(\lambda^{(t+1)}) - \frac{\alpha (1-\beta)}{1-\beta^t} \sum_{i=0}^{t-1} \beta^i \langle \nabla_\lambda \mL(\lambda^{(t)}), \bar{g}_{t-i} \rangle + \frac{L \alpha^2}{2}.
\end{align}


Write $\langle \nabla_\lambda \mL(\lambda^{(t)}), \bar{g}_{t-i} \rangle =  \langle \nabla_\lambda \mL(\lambda^{(t)}) - \nabla_\lambda \mL(\lambda^{(t-i)}), \bar{g}_{t-i} \rangle  +  \langle \nabla_\lambda \mL(\lambda^{(t-i)}), \bar{g}_{t-i} \rangle$. For the first term, applying the Cauchy–Schwarz inequality and $L$-Lipschitz smooth assumption (A3),
\[
|\langle \nabla_\lambda \mL(\lambda^{(t)}) - \nabla_\lambda \mL(\lambda^{(t-i)}), \bar{g}_{t-i} \rangle| \leq \|\nabla_\lambda \mL(\lambda^{(t)}) - \nabla_\lambda \mL(\lambda^{(t-i)}) \| \|\bar{g}_{t-i}\| \leq L \|\lambda^{(t)} - \lambda^{(t-i)} \|,
\]
where $\lambda^{(t)} - \lambda^{(t-i)} = \alpha \sum_{j=t-i}^{t-1} \{\lambda^{(j+1)} - \lambda^{(j)}\} = \alpha \sum_{j=t-i}^{t-1} m_j/(1-\beta^j)$. Hence
\begin{align*}
\|\lambda^{(t)} - \lambda^{(t-i)} \|_2  \leq \alpha \sum_{j=t-i}^{t-1} \frac{\| m_j \|}{1-\beta^j} \leq \alpha i.
\end{align*}


For the second term,
\[
\langle \nabla_\lambda \mL(\lambda^{(t-i)}), \bar{g}_{t-i} \rangle 
= \langle g_{t-i}, F_{t-i}^{-1}\widehat{g}_{t-i} \rangle/\|\widetilde{g}_{t-i} \|
\geq \frac{R_1}{R}  \langle g_{t-i}, F_{t-i}^{-1}\widehat{g}_{t-i} \rangle.
\]
Substituting these back into \eqref{E1} and applying Lemma \ref{lem_beta},   
\begin{align*}
\mL(\lambda^{(t)}) & \leq \mL(\lambda^{(t+1)}) + \frac{\alpha (1-\beta)}{1-\beta^t} \sum_{i=0}^{t-1} \beta^i \left( L\alpha i - \frac{R_1}{R}\langle g_{t-i}, F_{t-i}^{-1}\widehat{g}_{t-i} \rangle \right) + \frac{L \alpha^2}{2} \\ 
& \leq \mL(\lambda^{(t+1)}) + \frac{L\alpha ^2 \beta }{(1-\beta)} - \frac{R_1 \alpha (1-\beta)}{R(1-\beta^t)} \sum_{i=0}^{t-1} \beta^i\langle g_{t-i}, F_{t-i}^{-1}\widehat{g}_{t-i} \rangle + \frac{L \alpha^2}{2}.
\end{align*}
Taking expectation,
\[
\begin{aligned}
 \frac{R_1 \alpha (1-\beta)}{R} \sum_{i=0}^{t-1} \beta^i \langle g_{t-i}, F_{t-i}^{-1} g_{t-i} \rangle 
&\leq  \frac{R_1 \alpha (1-\beta)}{R(1-\beta^t)} \sum_{i=0}^{t-1} \beta^i \langle g_{t-i}, F_{t-i}^{-1} g_{t-i} \rangle \\
&\leq \mL(\lambda^{(t+1)}) - \mL(\lambda^{(t)}) + \frac{L\alpha ^2 \beta }{(1-\beta)}  + \frac{L \alpha^2}{2}.
\end{aligned}
\]
Summing over $t=1$ to $t=T$ and applying $\mL(\lambda^{T+1}) \leq \mL^*$ by (A1),
\[
\begin{aligned}
 \frac{R_1 \alpha (1-\beta)}{R} \sum_{t=1}^T \sum_{i=0}^{t-1} \beta^i \langle g_{t-i}, F_{t-i}^{-1} g_{t-i} \rangle 
&\leq \mL(\lambda^{(T+1)}) - \mL(\lambda^{(1)}) + \frac{TL\alpha ^2 \beta }{(1-\beta)}  + \frac{TL \alpha^2}{2} \\
&\leq \mL^* - \mL(\lambda^{(1)}) + \frac{TL\alpha ^2 \beta }{(1-\beta)}  + \frac{TL \alpha^2}{2}.
\end{aligned}
\]
Since $\langle g_{t-i}, F_{t-i}^{-1} g_{t-i} \rangle\geq \|g_{t-i}\|^2/R_2$,
\[
\begin{aligned}
 \frac{R_1 \alpha (1-\beta)}{R_2 R} \sum_{t=1}^T \sum_{i=0}^{t-1} \beta^i \|g_{t-i}\|^2 
&\leq \mL^* - \mL(\lambda^{(1)}) + \frac{TL\alpha ^2 \beta }{(1-\beta)}  + \frac{TL \alpha^2}{2}.
\end{aligned}
\]
Let $j = t-i$ and interchanging the summation,
\begin{align*}
 \sum_{t=1}^T \sum_{i=0}^{t-1} \beta^i \| g_{t-i} \|^2
&= \sum_{t=1}^T \sum_{j=1}^t \beta^{t-j} \| g_{j} \|^2  
= \sum_{j=1}^T \sum_{t=j}^T \beta^{t-j} \| g_{j} \|^2  
= \frac{\sum_{j=1}^T (1-\beta^{T-j+1})\| g_{j} \|^2}{1-\beta}.
\end{align*}
Therefore,
\begin{align*}
\E\|g_\tau \|^2 &= \sum_{j=1}^T \frac{1 - \beta^{T-j+1}}{C} \|g_j \|^2  \leq  \frac{1}{\widetilde{T}}\sum_{j=1}^T (1-\beta^{T-j+1})\| g_{j} \|^2\\
&= \frac{1-\beta}{\widetilde{T}}  \sum_{t=1}^T \sum_{i=0}^{t-1} \beta^i \| g_{t-i} \|^2 \\
& \leq \frac{R R_2}{\widetilde{T} R_1 \alpha} \left\{ \mL^* - \mL(\lambda^{(1)}) + \frac{TL\alpha ^2 \beta }{(1-\beta)}  + \frac{TL \alpha^2}{2} \right\}.
\end{align*}

\section{Logistic regression}
Let $y=(y_1, \dots, y_n)^T $, $X= (x_1^T , \dots, x_n^T )$ and $w = (w_1, \dots, w_n)^T $, where $w_i = \exp(x_i^T  \theta)/\{1+\exp(x_i^T  \theta)\}$. Let $W$ be a diagonal matrix with the $i$th element given by $w_i(1-w_i)$ for $i=1, \dots, d$. Then
\[
\begin{gathered}
\log p(y, \theta) = y^T  X \theta - \sum_{i=1}^n \log \{1 + \exp(x_i^T  \theta)\} - \frac{d}{2} \log (2\pi \sigma_0^2) - \frac{\theta^T  \theta}{2 \sigma_0^2}, \\
\nabla_\theta \log p(y, \theta)  = X^T  (y- w) - \theta/\sigma_0^2, 
\quad
\nabla_\theta^2  \log p(y, \theta) = - X^T W X - I_d/\sigma_0^2,
\end{gathered}
\]


\end{document}