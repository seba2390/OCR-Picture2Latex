\documentclass[fleqn,usenatbib]{mnras}
\usepackage{txfonts}
\let\iint\relax
\let\iiint\relax
\let\iiiint\relax
\let\idotsint\relax
\usepackage[T1]{fontenc}
\usepackage{ae,aecompl}
\usepackage{graphicx}	% Including figure files
\usepackage{amsmath}	% Advanced maths commands
\usepackage{amssymb}	% Extra maths symbols
\usepackage{mathtools}
%\usepackage{siunitx}
\usepackage{subcaption}
\usepackage[dvipsnames]{xcolor}
%\usepackage{tabularx}
\usepackage{multirow}
% \usepackage{caption}
\usepackage{pdfpages}
\usepackage{stfloats}
\usepackage{tabularx}
\usepackage{float}
\usepackage[export]{adjustbox}
\restylefloat{table}

\DeclarePairedDelimiter{\evdel}{\langle}{\rangle}
\newcommand{\ev}{\operatorname{}\evdel}
\newcommand{\be}{\begin{equation}}
\newcommand{\ee}{\end{equation}}
\newcommand\fnurl[2]{\href{#2}{#1}\footnote{\url{#2}}}
\newcommand{\mf}[1]{{\textcolor{cyan}{#1}}}
\newcommand{\ali}[1]{{\textcolor{red}{#1}}}
\newcommand{\bt}[1]{{\textcolor{Plum}{#1}}}

\newcommand{\order}[1]{\mathcal{O}\!\left(#1\right)}
\newcommand{\uS}{\mathrm{s}}
\newcommand{\uc}{\mathrm{c}}
\newcommand{\ud}{\mathrm{d}}
\newcommand{\bfmath}[1]{\ensuremath{\boldsymbol{#1}}}
\newcommand{\SNR}{SNR}

\def \hfillx {\hspace*{-\textwidth} \hfill}

\title[CS detection by ML]{{\it Planck} Limits on  Cosmic String Tension Using Machine Learning}
\author[Torki et al.]{
M. Torki$^1$\thanks{M. Torki and H. Hajizadeh contributed equally to this work as first authors.},
H. Hajizadeh$^1$\footnotemark[1],
M. Farhang$^1$\thanks{corresponding author: m\_farhang$@$sbu.ac.ir},
A. Vafaei Sadr$^{2,3}$ \&
S. M. S. Movahed$^{1}$
\\
% List of institutions
$^{1}$Department of Physics, Shahid Beheshti University,  1983969411, Tehran, Iran\\
$^{2}$Department de Physique Theorique and Center for Astroparticle Physics, University Geneva, 1211 Geneva, Switzerland\\
$^{3}$Institute for Research in Fundamental Sciences (IPM), P. O. Box 19395-5531, Tehran, Iran
}
\pubyear{2021}

% Don't change these lines
\begin{document}
\label{firstpage}
\pagerange{\pageref{firstpage}--\pageref{lastpage}}
\maketitle

% Abstract of the paper
\begin{abstract}
We develop two parallel machine-learning pipelines to estimate the contribution of cosmic strings (CSs), conveniently encoded in  their tension ($G\mu$), to the anisotropies of the cosmic microwave background radiation observed by {\it Planck}. 
The first approach is tree-based and feeds on certain map features derived by image processing and statistical tools. The second uses convolutional neural network with the goal to explore possible non-trivial features of the CS imprints. 
The two pipelines are trained on {\it Planck} simulations and when applied to {\it Planck} \texttt{SMICA} map yield the $3\sigma$ upper bound of $G\mu\lesssim 8.6\times 10^{-7}$. 
We also train and apply the pipelines to make forecasts for futuristic CMB-S4-like surveys and conservatively find their minimum detectable tension to be $G\mu_{\rm min}\sim 1.9\times 10^{-7}$.
\end{abstract}

%---------------------------------------------------------------------------

\begin{keywords}
Cosmic Strings - Cosmic Microwave Background(CMB) - Convolutional Neural Network(CNN) - Machine Learning

\end{keywords}

%%%%%%%%%%%%%%%%% BODY OF PAPER %%%%%%%%%%%%%%%%%%
\section{Introduction}
%---------------------------------------------------------------------------
Cosmic strings (CSs) are hypothetical one-dimensional topological defects possibly formed in certain cosmic phase transitions in the early Universe \citep{Kibble:1976sj,Zeldovich:1980gh,Vilenkin:1981iu,
Vachaspati:1984dz,Vilenkin:1984ib,Shellard:1987bv,
Hindmarsh:1994re,Vilenkin:2000jqa,Sakellariadou:2006qs,
Bevis:2007gh,Depies:2009im,Bevis:2010gj,
Copeland:1994vg,Sakellariadou:1997zt,Sarangi:2002yt,
Copeland:2003bj,Pogosian:2003mz,Majumdar:2002hy,
Dvali:2003zj,Kibble:2004hq,HenryTye:2006uv}.
Their detection would give a unique opportunity to study the high energy physics of the relevant phase transition. 
The CS network, if existing, can source inhomogeneities in the distribution of matter and background photons. 
Among the different CS imprints is the Gott-Kaiser-Stebbins (KS) effect \citep{Kaiser:1984iv}. This effect describes the line-like discontinuities left by moving strings on the Cosmic Microwave Background radiation (CMB) temperature  through the integrated Sachs-Wolfe effect
\citep{lazanu2015constraints,Gott:1985,Stebbins:1988,bouchet1988microwave,allen1997cmb,pen1997power,ringeval2012all}.
The induced anisotropy has the form \citep{Hindmarsh:1993pu,Stebbins:1995}
%----------------------------
\begin{equation}
\frac{\delta T}{T} \sim 8 \pi G\mu v_{\rm s}
\end{equation}
%----------------------------
where $G\mu$ is the main parameter characterizing the string network with  $\mu$ being the string tension and  $v_{\rm s}$ the transverse velocity of the string. 
The string tension  is intimately related to 
the energy scale of the phase transition through 
%----------------------------
\begin{equation}
\frac{G\mu}{c^2}=\order{\frac{\varpi^2}{M_{\rm Planck}^2}},
\end{equation}
%----------------------------
where   $\varpi$ represents the energy scale of the transition. The constants $G$  and $c$ are the Newton constant and the speed of light and $M_{\rm Planck}\equiv\sqrt{\hbar c/G}$ is the Planck mass. 

There have been a lot of efforts to model and simulate these imprints and various statistical tools have been developed for their study.
In particular, CMB-based studies using {\it Planck} temperature and polarization power spectra yield 
 $G\mu < 2.0\times 10^{-7}$ for Higgs strings (Lizarraga et al. 2016), $G\mu < 1.5\times 10^{-7}$  for  Nambu-Goto strings (Lazanu  Shellard 2015) and  $G\mu < 1.1\times 10^{-7}$  for a multi-parameter fit to the unconnected segment model \citep{cha16}. 
 %
Methods based on the non-Gaussianity of the string-induced anisotropies  \citep{rin10,duc12} such as bispectrum and Minkowski functional measurements and wavelet analysis  give 
$G\mu < 8.8\times10^{-7}$, $ G\mu < 7.8\times10^{-7}$ and $G\mu < 7\times10^{-7}$ respectively \citep{hin09,hin10,reg15,ade2014planck,duc12}.
%
Map-based approaches use real-space statistics  or  edge-like properties of the KS imprints to tighten the detectability limit of the strings.  
%
For instance, there are detectability claims from  ideal noise-free simulations for CSs with $G\mu > 4.0\times 10^{-9}$  using crossing statistics \citep{mov10}
and with  $G\mu > 1.2 \times 10^{-8} $ with the unweighted Two-Point Correlation Function (TPCF) of CMB peaks \citep{mov12}. A recent analysis of the peak clustering  provided the upper bound of  $G\mu\lesssim 5.59\times 10^{-7}$  in \texttt{SMICA} \citep{vafaei2021clustering}.

%For instance, \cite{mov10} claim detectability of  strings with $G\mu > 4.0\times 10^{-9}$ using crossing statistics  for an ideal noise-free experiment and \cite{mov12} claim detectability of $G\mu > 1.2 \times 10^{-8} $ with the unweighted Two-Point Correlation Function (TPCF) of CMB peaks.   
%
For simulations of the South Pole Telescope, \cite{ste08} set the minimum detectability bound of  $G\mu >  5.5\times10^{-8}$ with edge-detection algorithms and  \cite{her16} find a detection limit of $G\mu >  1.4\times10^{-7}$ using wavelets and curvelets. 
%
\cite{vaf17} also used various combinations of image processing tools and statistical measures,  followed by tree-based learning algorithms \citep{vaf18}  to forecast the detectability limit of different observational scenarios. 
%
There have been relatively recent neural network-based algorithms to 
locate the position of the strings for ideal noiseless experiments \citep{ciu17} or to put information-theoretic bounds on the CS tension for various observational setups \citep{ciu20}.
%
There are  also $G\mu$ measurements and forecasts from other observational routes. For example see \cite{rin17,bla17a,bla17b,arz18,bla18,auc19,len15,jen06,psh09,tun10,dam05,bat10,kur12} for bounds on string loops from pulsar timing constraints on the amplitude of the stochastic gravitational wave background, \cite{imt20} for bounds on CS network from fast radio bursts, \cite{bra10,her12,pag12} for forecasts from  21-cm signatures, \cite{lal19} for CS imprints on ionization fraction in the Universe, \cite{fer20} for the effect of CS on the filament structure in the cosmic web and \cite{cun18} for the imprint of CSs on the dark matter distribution.
%\cite{cam18} for the imprint of CS on dark matter distribution at $z=3$. 

In this manuscript, we propose to use convolutional neural networks (CNN) to search for the CS footprints on CMB data. 
Deep learning methods, i.e. neural networks containing many layers, have led to significant improvements in performance over the past decade in almost all machine learning problems. 
%
Here, we study the possibility of achieving tighter upper bounds on the CS tension by deep learning methods from {\it Planck}18 data  and compare the results with the limits from an improved version of the previous approaches of \cite{vaf17,vaf18}.
%
%using an improved version of the previous approaches of \cite{vaf17,vaf18} and comparing the results with direct usage of CNN.
%
We also apply the same two pipelines, properly trained, to other observational setups to compare their performance and make forecast for future surveys. 

The outline of the rest of this paper is as follows. 
Section~\ref{sec:ml} is a brief introduction to the machine learning algorithms used. An overview of the CMB simulations and data  is given in Section~\ref{sec:sim}. In Section~\ref{sec:det} the methodology and pipelines are introduced and discussed in detail. In Section~\ref{sec:res} we discuss and compare the performance of the models and apply them to {\it Planck}18 data. We conclude with a summary and a short discussion in Section~\ref{sec:sum}.

%---------------------------------------------------------------------------
\section{Machine Learning}\label{sec:ml}%---------------------------------------------------------------------------

Machine learning is the branch of artificial intelligence with the goal of empowering computers to learn and improve from experience. 
%
In machine learning, given some data, a machine is trained to construct a model which is used to make predictions on new datasets. 
%
If supervised, the target value of the learning algorithm is provided by an instructor. In unsupervised learning, on the other hand, the algorithm learns to make sense of  data with no guide. This happens by experiencing a dataset with many features which are not pre-labeled and trying to learn its inherent structure, usually through clustering \citep{goodfellow2016deep}. 


In the rest of this section we briefly review the two main learning algorithms used in this paper, namely the gradient boosting method and neural networks.
%------------------------------------------------------------------------
\subsection{Gradient Boosting Method}
Gradient boosting models are supervised algorithms based on decision trees and are commonly used in classification and regression problems. 
A decision tree has a tree-like structure with nodes, branches and leaves.
The tree starts with a question, corresponding to a node, and depending on the answer, the data are split into branches. The process continues until the algorithm reaches a leaf that predicts the label of the data.
Gradient boosting combines many weak learners into a strong learner and is based, not on a single model, but on gradually improving the model toward models with lower loss function and higher prediction power at each step.
The performance of a model is evaluated by the loss or cost function through  
comparing predictions with fiducial values. 

In this work we use the two gradient boosting libraries of XGBoost\footnote{https://xgboost.readthedocs.io/en/latest/} \citep{che16} and LightGBM\footnote{https://lightgbm.readthedocs.io/en/latest/} \citep{lig17}, or LGBM for short. 
XGBoost is an advanced and powerful implementation of gradient boosting and due to its high accuracy, is the popular boosting method for many learning problems.  
%
However,  XGBoost can be very time-consuming  for  problems with huge datasets. 
In LightGBM, the tree grows leaf-wise in contrast to XGBoost and other tree-based algorithms where  the tree grows depth-wise. The leaf-wise split surprisingly increases the speed and accuracy of the model.
However, due to increased complexity, there is a high risk of being overfit. This is usually avoided by setting a maximum depth to which splitting can occur. 
%
The leaf with a larger loss function is chosen to grow and  further reduce  loss. 
It is therefore faster, the reason it is called {\it light}.
%------------------------------------------------------------------------
\subsection{Neural Networks}
%---------------------------------------------------------------------------

Artificial Neural Networks (ANNs), inspired by biological neural networks,  consist of neurons with cell body and dendrites to receive inputs and axons to output a signal \citep{jain1996artificial,hassoun1995fundamentals,yegnanarayana2009artificial}. The neural network takes the data in its first layer and calculates a weighted sum over them.  The weights are initialized  randomly, and are updated  during the training process.
  The higher the assigned weight to an input, the more significant it has been considered compared to other features. Through learning,  the model finds weights that result in the desired behavior of  the network. 
    A bias is usually added to the weighted sum of the input to change the output range and lead to predictions with better fit to data.
 %   
 A non-linear differentiable transformation is applied to weighted data by an activation function. This step increases the complexity of the model and without it the model would be a simple linear regressor.
 %
Among the activation functions commonly used are sigmoids (commonly $\frac{1}{1+e^{-x}}$), softmax functions or $\frac{e^{x_i}}{\sum_j{e^{x_j}}}$,  rectifiers or ${\rm max}(0,x)$ and $\tanh(x)$ \citep{nwankpa2018activation}.
%
The output of a neuron is fed into the next layer and these steps are repeated until combined into the final layer where a prediction is made.
%
The algorithm is called deep learning if many hidden layers are used with long causal chains of computational steps. 
The learning process is basically finding the parameters  of the network
that optimize the cost function. 
%
In each iteration the learning rate (LR) controls the weight update with respect to the loss gradient,  new weights = old weights - LR $\times$ gradient of loss. LR is often a small number between 0 and 1. A good idea is to start training with a large LR and gradually decrease it in the training process.

Convolutional neural networks (ConvNet or CNN) are deep learning algorithms
 that use convolution instead of general matrix multiplication in at least one of their layers \citep{krizhevsky2012imagenet,gu2018recent}.
Convolution can be interpreted as a filter, kernel or weight matrix that multiplies part of the input data 
to produce a convolved feature map.
Neurons that lie in the same feature map share the weight.   
The network complexity is thereby reduced by keeping the number of parameters low \citep{hinton2012improving}.
% 
Similar to weights in regular neural networks, the filters are initiated randomly but 
are updated in the training process to optimize the cost value.
%
Different hyperparameters  that control the output size include 1- number of filters at a layer that specifies the output depth of a convolution layer, 2- kernel-size that specifies the dimension of the convolution matrix and controls the width and height of output, 3- stride or step of filter movement, and 4- zero-padding  that adds extra layers of zeros across the images to keep the output size the same as the input \citep{aloysius2017review}.

CNNs may also contain {\it pooling} layers that reduce the dimension of the data by combining  the outputs of certain neurons of a layer into a single 
neuron in the next layer. 
This is achieved by, e.g., taking the maximum output of the neuron cluster in the pooling region (max pooling, see \cite{cir12}) or by computing their average (average pooling, see \cite{mit20}). 
Pooling would reduce the computational complexity of the model.  

A well-trained machine should easily generalize to unseen (test) datasets in the sense that it has small errors when making predictions on them. A gap can occur between the predicted errors of train and test datasets  if the model is overfit by learning insignificant and noisy  features of the train sets and having partially missed the main underlying structure of data. {\it Dropout} is one of the techniques proposed to prevent overfitting by randomly and temporarily removing  a unit from the network \cite{sri14}. This technique forces the machine to learn about more robust features.  

{\it Batch Normalization} (BN) is another technique to make the model more generalizable \citep{iof15}.
In BN the model is trained on data chunks with equal size, or batches, instead of the whole dataset in a single go. The BN layer increases the learning rate  and reduces the sensitivity to initialization. 
%-------------------------------------------------------------------------
\section{Simulations and Data}
%---------------------------------------------------------------------------
\label{sec:sim}
In this work, we use simulations to test, train and verify the robustness of the proposed CS search pipelines.
%
The simulated maps contain contributions from inflationary Gaussian CMB anisotropies and different levels of CS-induced fluctuations and also include experimental effects.
We use Nambo-Gutto string  simulations in this work \citep{bennett1990high,ringeval2007cosmological}.
% 

The  simulation for the observed CMB sky map, $d$, would then be
%--------------------
\begin{equation}
\label{eq:d}
d = T+ B \left(G\mu \times S\right) + n
\end{equation}
%---------------------
 where $S$, $n$ and $B$ represent the template for the CS-induced anisotropies, the instrumental noise and beam respectively. 
 The simulated anisotropies $T$ represent the observed inflationary Guassian fluctuations and in this work in divided into three main sets. 
 %
 %$T$, $S$ and $n$ represent the observed inflationary CMB anisotropies, the template for cosmic string-induced anisotropies and the instrumental noise. The beam $B$ encodes the effect of instrumental beam and $G\mu$ parametrizes the level of contribution of cosmic strings to CMB fluctuations. 
%
% primordial and noise 
%We use three sets of inflationary CMB simulations. 
The first set consists of HEALPix\footnote{https://healpix.sourceforge.io}-generated CMB maps, $g$, with $T=Bg$ in Equation~\ref{eq:d}.
This is used in this work with simulations of the two phases of CMB-S4, here referred to as CMB-S4-like(I) and (II).  
 %
The second and third sets of simulations are {\it Planck}18 simulations, labeled by the Full Focal Plane 10\footnote{pla.esac.esa.int $\to$ maps $\to$ simulations $\to$ cmb} (FFP10)  and the end-to-end\footnote{pla.esac.esa.int $\to$ advanced\;search\;and\;map\;operations $\to$ simulated\;maps\;search $\to$ comp-separation} (E2E) simulations \citep{ade2016planck}.
%
The FFP10 maps are generated using the best characterizations of instrumental properties, as well as astrophysical foreground residuals. 
The instrumental noise, $n$, for all these cases is assumed  Gaussian and white  with the desired noise level. 
 For the details of experimental characterization for the simulations used in this work see \cite{vaf18}. 
%
The E2E maps are constructed based on the FFP10 simulations, where these maps go through the same pipeline as the data \citep{akrami2019planck,ade2016planck}. The E2E CMB and noise  maps, $T$ and $n$ respectively, are therefore the closest to observations. %
 
 %---------------------------------------------------------------------
\begin{figure*}
	\centering	
	\includegraphics[width=\textwidth]{fig1.pdf}
	\caption{Up: The full sky map, with $N_{\rm side}=2048$, divided into four patches to represent different simulations. The upper half correspond to E2E (right) and CS-induced (left) simulations. The lower parts show the sum of the E2E and string simulations with $G\mu$ = $10^{-6}$ (left) and $G\mu$ = $10^{-7}$ (right).
	Middle: The zoomed-in view of two $256\times256$ patches with different string contributions. 
	Bottom: The same patches as the middle row, after having passed through parts of the image-processing steps of the proposed CS detection pipeline. Specifically, they correspond to the seventh curvelet component (labeled as C7), the Scharr-filtered image and the combination of the two.}
	\label{fig:CMB}
\end{figure*}
%------------------------------------------------------------------------

The pipelines developed based on the HEALPix simulations are used to make futuristic forecasts on CS detectability from CMB data and works with the HEALPix resolution $N_{\rm side}=4096$.
 The second and third sets of pipelines establish an estimate for the anticipated level of detectability with the {\it Planck} data, and work with  $N_{\rm side}=2048$.
The E2E maps are also used to build machine learning-based models which are then directly applied to {\it Planck}18 observations to measure or set an upper bound on $G\mu$. 
 %-----------------------------------------------------------------------
\section{detection pipelines}
\label{sec:det}
%---------------------------------------------------------------------------

In the following, we divide the $G\mu$ range of interest into $n_{\rm cls}=11$ classes, consisting of a null case with $G\mu=0$ and ten logarithmically-scaled classes in the range $(6\times 10^{-9},6\times 10^{-6})$.  
Then $N_{\rm sim}$ simulations are generated with different levels of string contribution  for each observational case.
The two search methods are patch-based.  After removing the sky mask\footnote{pla.esac.esa.int $\to$ maps $\to$ mask $\to$ cmb $\to$ 2018\;component\;separation\;common\;mask\;in\;intensity}, the maps are divided into $N_{\rm patch}$ non-overlapping patches to reduce the computational cost. This is not  expected to lead to any noticeable  information loss on CS network
as the strings mainly leave small-scale imprints.  

To exploit the available information in the full observed sky, the results from all patches need to be interpreted simultaneously. 
For this purpose, we treat the patches as independent realizations of a single cosmological model with the same $G\mu$ and multiply their probability vectors to give the full-(masked-)sky probability vector as the final prediction of the machine. 


In our search for CSs, we follow two main parallel approaches.
 The first is feature-engineered and is guided by previous experience \citep{vaf17,vafaei2021clustering}. In this approach certain features, chosen based on earlier works on the topic, are extracted from the maps 
 and fed as input to machines for classification. 
%
The second approach, on the other hand, uses deep learning methods to estimate string contribution to CMB anisotropies. 

%---------------------------------------------------------------------
\subsection{Educated Search}
\label{sec:edu}
%---------------------------------------------------------------------------

 The CS search pipeline introduced in this section is an extension of \cite{vaf17,vafaei2021clustering} to more realistic {\it Planck} simulations of FFP10 and E2E maps to allow for direct comparison with {\it Planck} data. 
 Here the maps are processed and  compressed into features with the goal to  enhance the CS signal contribution compared to primordial CMB anisotropies. 
In this branch of the pipeline we use $N_{\rm sim}=10$ maps.
%at  $N_{\rm side}=2048$ for {\it Planck} simulations and $4096$ otherwise.
 The maps are divided into $N_{\rm patch}=192$ patches for all cases but E2E where we take $N_{\rm sim}=100$. 
That is because the machines trained on E2E maps will be applied to {\it Planck} observations, and,  as will be discussed later,  the performance of the machines  slightly improves when $N_{\rm sim}$ increases from ten to $100$. 
For the other observational scenarios, however, as no direct comparison with data is made, we use $N_{\rm sim}=10$ to keep the computation cost low.

%-------------------------------------------------------------------- 
\subsubsection{Processing maps}
%---------------------------------------------------------------------------

In the first step, the sky patches go through two image processing steps of curvelet decomposition
\citep{don00,can00,can01,can02,can06} and Canny filtering\citep{can86},  to increase the detectability of the CS signature. 
Curvelet transformation disintegrates the maps into layers with scales relevant to the signal of interest. We use three small-scale curvelet components, labeled as C5, C6 and C7, so that only components with the highest contribution from CSs are kept for further analysis.
%
The curvelet decomposed maps are then filtered using Canny algorithm which is suitable for edge detection in images. In previous analysis we found the best results from Scharr and Sobel filters. These two are therefor used as filters here to produce the gradient maps.
Figure~\ref{fig:CMB} illustrates the effect of these steps on the input map, for various combinations of primordial, CS-induced and noise fluctuations.

%-------------------------------------------------------- 
\subsubsection{Analysis of  processed maps}
%---------------------------------------------------------
 \begin{figure}
	\includegraphics[width=\linewidth]{fig2.pdf}
	\caption{The {\it P}-values for the $G\mu$ classes, measured by the LightGBM method, quantify the difference between  the distributions of the predictions for each class from the null case, for the E2E simulations. The vertical line marks the minimum detectable $G\mu$   ($G\mu_{\rm min}$) where the {\it P}-value crosses the detectability threshold (taken to be $0.05$ here) and is found by interpolation.}
	\label{fig:pvallgbm}
\end{figure}
%-------------------------------------------------------------
The goal of the map processing step is to enhance the detectability of the imprints of the CS network. 
Here we quantify the possible imprints by applying various statistical measures on the processed maps. 
In previous analysis of \cite{vaf17} it was shown that, among the different measures explored, 
the tightest constraints were reached by  the  standard deviation and one-point probably distribution function (hereafter, the PDF).
We thus focus on the same measures in the current work.
The standard deviation returns a number for each patch.  We subtract from this number the mean standard deviation of the  baseline patches with $G\mu = 0$.
The PDF of each patch, on the other hand, is a function. We subtract from these functions the baseline PDF and then integrate over it to associate a single number to each patch. 

At the end of the processing steps we are left with 12 features for each patch, obtained from combinations of three curvelet components (C5, C6 and C7), two filters (Scharr and Sobel) and two statistical measures (PDF and standard deviation). 
%
In the next section we explain how we follow a learning-based approach using these features to estimate the level of string-induced anisotropies.  
We also calculate and report  the two-tail ${\it P}$-value statistics as a traditional measure to assess  the performance of the algorithms and quantify their strength. 
With the ${\it P}$-value as the criterion, we define the detectability limit, $G\mu_{\rm min}$, as the 
minimum $G\mu$ with a distribution distinguishable from the null class with a maximum ${\it P}$-value of $0.05$. Figure~\ref{fig:pvallgbm} shows the ${\it P}$-values for different levels of $G\mu$ contribution for E2E simulations. 
For the details of the quantification of the CS-induced deviation and the corresponding {\it P}-value calculation see \cite{vaf17}.

%
One could also follow a purely statistical path and calculate the ${\it P}$-value to compare the distribution of the above statistical measures (pdf and variance) 
 with the null class, using $N_{\rm sim} \times N_{\rm patch}$ for different $G\mu$ classes and for all observational cases of interest. 
This is similar to the approach of \cite{vaf18}. 
Figure~\ref{fig:pvallgbm} shows the {\it P}-values of the $G\mu$s for the E2E case from this approach.
%--------------------------------------------------------------------------
\subsubsection{Machine-learning methods}\label{sec:MLmethod}
%--------------------------------------------------------------------------
We use the 12 map features of the previous section, derived from combinations of various statistical measures and image processing tools, as inputs to different tree-based learning algorithms for CS imprint detection.
For each observational scenario we use $75\%$ of the simulated patches for training  and the remaining as test sets to assess the power of the trained model.
%
We compare the performance of different machine-learning methods including Naive Bayes \citep{ris01}, Decision Tree \citep{qui86}, Random Forest \citep{bre01}, K-Nearest Neighbours \citep{kel85}, XGBoost \citep{che16} and LightGBM \citep{ke17} .
%
We found the boosting methods of XGBoost and LightGBM as the most accurate, with LightGBM being the fastest of the two. The LightGBM is therefore the main learning algorithm in this work for the tree-based search, and the reported results are based on this algorithm.
The performance of the model after training is summarized in the confusion matrix.
Figure~\ref{fig:conflgbm} shows the confusion matrix of the 11 $G\mu$ classes for the E2E simulations. The columns and rows of this matrix represent the predicted and actual classes respectively. 
We see that the upper right ($5\times5$) diagonal block of this matrix has a dominant diagonal element with diminishing values as one moves to either side. This feature is the illustration of the low confusion of the machine in learning about the signal we are after in this regime, and making relatively powerful predictions about the $G\mu$ classes. 
On the other hand, the lower left ($6\times6$)  block of the matrix and the scattered distribution of the predictions for these classes show that the machine has not learned about these classes and the predictions are not reliable. We therefore consider the lowest class above this {\it confused block} as the minimum detectable $G\mu$ or $G\mu_{\rm min}$. 
It should, however, be noted that detectability requires not only robust predictions based on powerful learning by the machine, but also distinguishability from the null class with  $G\mu=0$. 
Therefore, for a (non-zero) $G\mu$ to be chosen as the detection limit,  the majority (e.g., $95\%$) of the machine predictions for the patches are non-null. Figure~\ref{fig:conflgbm} shows that $G\mu_{\rm min}= 8.6 \times 10^{-7}$ is the detection limit for the educated method in this work based on the confusion matrix.
This is a conservative choice due to the discrete nature of the classes. One could possibly decrease this limit by zooming into the neighborhood of this $G\mu_{\rm min}$ and further train the machine on fine-gridded classes. 

%-----------------------------------------------------------------------
\begin{figure}
	\centering
	\includegraphics[width=0.47\textwidth,height=0.35\textheight]{fig3.pdf}
	\caption{The confusion matrix summarizes  the results of the classification of the  LightGBM algorithm on unseen E2E simulations. The diagonal elements are the fractions of correct predictions, while the off-diagonals represent the mislabelled. We see the model has not learned much about the first six classes and the  predictions in the $6\times6$ block in the lower left are quite scattered and hardly centered around fiducial values. We, conservatively, report the first class above this confused block as the minimum detectable class by the model that can be distinguished from the null class, i.e., $G\mu_{\rm min}=8.6\times10^{-7}$.}
	\label{fig:conflgbm}
\end{figure}
%--------------------------------------------------------------------------
The process of machine learning in this section is separate and totally different from the deep search that will follow in section \ref{Deep_sec}. 
The  features here are chosen a priori and handed in to the machine as inputs, while in the deep search, the network itself selects the best features for training. The only preprocessing used in the deep search is  edge-enhancement through filtering the map.

%------------------------------------------------------
In the educated search
the machines are trained based on our prior assumption that certain features effectively represent or enhance the desired signal. So the maps are initially transformed into these features to improve the $G\mu$ detection limit   
of the models. 
%
However, there is no guarantee that  our search in the space of statistical measures and image processors is exhaustive. 
 For instance,  there could exist nonlinear features that are most sensitive to the string network and are left out in our limited and linear image processors. 
Therefore, as a parallel approach, we use CNNs to classify the input maps according to their predicted string contribution. 
The main advantage of deep learners over the educated algorithms is that the network itself learns to extract features in the training process. 
In this section we introduce our deep string detector. 

%-------------------------------------------------------------------------
\subsubsection{Input preparation for the CNN}
%-------------------------------------------------------------------------
We train the network with patches of $256\times 256$ pixels  ($\sim7.5\times7.5 ~  \rm{deg}^2$),  randomly chosen  from $N_{\rm sim}\sim 60$ full sky simulations with $N_{\rm side}=2048$. The string contribution in each simulation is set by a randomly chosen $G\mu$. The maps are first Scharr-filtered to enhance their edge-like features and then standardized by subtracting their mean and normalizing their standard deviation to unity. 

As for any supervised algorithm, the targets should be labeled. One could use one-hot vectors as the labels for the $n_{\rm cls}$ classes of $G\mu$, i.e., $n_{\rm cls}$ arrays with a single one and $n_{\rm cls}-1$ zeros.
% 
 The one component in each vector represents the $G\mu$ class of the input map. 
 The  prediction for each map is also an array with 
 $n_{\rm cls}$  components between 0 and 1, specifying the probability of the data  belonging to each class.  If the targets are labeled  by their $G\mu$ values instead, the network will treat the problem as a regression. 
 
 %
\begin{table}
%\begin{center}
\caption{The final architecture of CNN. The abbreviations used in this table are as follows. f: number of filters, k-size: kernel size, s: number of strides, p: pooling-size, AF: Activation Function, BN: Batch Normalization and Conv.: convolution layer.}
\label{Table:arch}
\centering
\begin{tabular}{c}
\hline
$4 \times$ \Bigg\{ Conv.(f=4 , k-size = 5 , s=1) \\
% \downarrow \\
$\rightarrow$ BN \\
% \downarrow \\
$\rightarrow$ 
CReLU activation layer \Bigg\} \\
$\downarrow$ \\
$6 \times$ \Bigg\{ Conv.(f=8 , k-size = 5 , s=1) \\
$\rightarrow$ BN \\
$\rightarrow$ CReLU activation layer \\
\\
$\downarrow$ \\
\\
Conv.(f=16 , k-size = 5 , s=2) \\
$\rightarrow$ BN \\
$\rightarrow$ CReLU activation layer  \\
$\rightarrow$ Max-Pooling(p=2 , s=1) \Bigg\} \\ 
$\downarrow$ \\
Flatten \\
$\downarrow$ \\
Dropout(rate = 0.5) \\
$\downarrow$ \\
Dense(40 , AF= CReLU) \\
$\downarrow$ \\
Dropout(rate = 0.5) \\
$\downarrow$ \\
Dense(20 , AF= CReLU) \\
$\downarrow$ \\
Dropout(rate = 0.5) \\
$\downarrow$ \\
Dense(11 , AF= Softmax) \\
\hline
\end{tabular}
%\end{center}
\end{table}
%-------------------------------------------------------------------------

%-------------------------------------------------------------------------
\subsubsection{Network architecture and analysis}
%-------------------------------------------------------------------------
Network design is about choosing a proper architecture and tuning its hyperparameters to control the learning process, such as the number and size of kernels in each layer and the stride values.  
%
We try various architectures and parametrizations in two separate and parallel paths of regression, with continuous  $G\mu$ labels, and classification,  with $n_{\rm cls}=11$ . 
%
The former regression problem, not surprisingly, turned out to be more challenging for the network to solve. We therefore report the classifier results.
%
We also find that certain features of the network have significant impact on its performance and the accuracy of its predictions. 
For example, using min-max pooling layer rather than average pooling, using 
z-score normalization as the scaling choice, assigning more strides to the convolution layer than to the pooling layer, and use of batch normalization layers greatly impact the results.

%---------------------------------------------------------
\begin{figure*}
\centering
	\includegraphics[width=\textwidth , height=0.3\textheight]{fig4.pdf}
	\caption{Visualization of the CNN architecture and variations in the input dimension through different layers (see Table~\ref{Table:arch}).  The CNN feeds on $256\times256$ images. Dimension reduction in the network results in 1D arrays with $n_{\rm cls}=11$ elements as the machine predictions. Above each layer the height and width of its output are shown. The different layer depths reflect differences in their hyperparameters. Note that only one of the four repetitive layer patterns of the first block is shown for more clarity.	}
	\label{fig:arch}
\end{figure*} 
%-------------------------------------------------------------------------


%-------------------------------------------------------------------------

\subsection{Deep search}
\label{Deep_sec}
%---------------------------------------------------------


%----------------------------------------------------------------------
As the activation function, and in a tradeoff between performance and time-consumption, we use  the Concatenated Rectified Linear Unit or Concatenated  ReLU or CReLU that concatenates a ReLU function \citep{shang2016understanding}, in all but the output layer where Softmax is  used.
 The  ReLU function preserves only the positive part of the activation, $f(x)=\max(x,0)$. 
 CReLU, on the other hand,  concatenates a ReLU which selects only the positive part of the activation with a ReLU which selects only the negative part of the activation and has the notable feature of information preservation.
%
For the loss function, among the various  functions tried, we find the Huber-Loss to lead to best predictions,  
%-------------------------------------------------------------------------
\begin{align}
L_{\delta}(y,f(x)) = \left\{ \begin{array}{cl}
\frac{1}{2} \left[y-f(x)\right]^2 & \text{for }|y-f(x)| \le \delta, \\
\delta \left(|y-f(x)|-\delta/2\right) & \text{otherwise.}
\end{array}\right.
\label{huber_loss}
\end{align}
%---------------------------------------------------------------------
For the learning rate or LR, we start with $0.001$ and gradually decrease it to $5\times 10^{-5}$ as the cost value settles down during the training process.
To deal with the destructive impact of 
 instrumental noise on the network accuracy, we 
can also use Transfer Learning methods \citep{pan2009survey}, where the learning process is transferred from noise-free  to noisy data. 
In these methods the network is first trained on noise-free datasets. 
Then some of the pre-trained layers are deactivated while the rest are allowed to be trained with the noisy data. 
%

The network coding of this work is done in the python environment, and heavily uses the TensorFlow \citep{abadi2016tensorflow} python library.%\footnote{The codes are available at \url{https://github.com/vafaei-ar/DeePlanck} .}. 
Details of the network were explained before. The final model has around $90,000$ free parameters describing the architecture and  the hyperparameters. 
The training process with the goal to find the best fit of all these parameters is done during about 4500 iterations where in each iteration the network sees around 100 images. So the total amount of data fed to the network is about 450000 patches, or close to $3\times10^{10}$ pixels\footnote{This training process takes about four hours to run on 12GB  Tesla K80 GPU.}.


%
The final architecture for the classifier used in this work is described in Table~\ref{Table:arch}. 
Figure~\ref{fig:arch} visualizes the network and how it reduces the dimension through 16 convolution layers of the input data (a $256\times 256$ image) into an array with $11$ components corresponding to the $11$ classes of $G\mu$.
The first two layers of this network are repeated four and six times respectively and they both have convolution and BN layers. The second block has  pooling and strides as well, and therefor reduces the input dimension. These convolution blocks are followed by a flatten layer to transform the 2D image into a 1D array which is fed into the fully connected layer (Dense). Also, $50\%$ of nodes are also dropped in dropout layers to avoid overfitting. The output of the very last Dense layer, with a Softmax activation, is a 1D array with 11 elements corresponding to the network predictions for the $G\mu$ probability of the input maps.


To report the capability of the network in measuring the cosmic string signature, we use both the {\it P}-value statistics and the confusion matrix, similar to the case of educated search (Section~\ref{sec:edu}).
\iffalse 
\bt{-- Note: Not sure if it is the right place to say this, but anyway:) -- As mentioned before for both educated and deep search we use patch-based analysis. Since the patches don't overlap they can be assumed as independent events, ie. knowing the probability of a patch belonging to each G$\mu$ class doesn't change the probability of the other patch. In order to use the full sky information, by assuming that the neighbour patches of a CMB sky which refer to a unique G$\mu$ are independent events, we can multiply the probability vectors of the patches together. After a normalization step, this yields to another probability vector which can be considered as the prediction of machine for G$\mu$ of the sky.}\fi
Figures~\ref{fig:cnnpval} and~\ref{fig:cnnconf} illustrate the performance of the final network  for unseen E2E simulations. From this confusion matrix, we find that the network can powerfully distinguish the class of $G\mu=8.6\times 10^{-7}$ and above from the null case and from each other, while it has not learned much about the $6\times6$ lower left block of the matrix. In this confused block, the majority of the predictions happen to be stacked in the third class. We therefore report $G\mu_{\rm min}=8.6\times10^{-7}$ based on the E2E simulations. 
%------------------------------------------------------------------------- 
\begin{figure}
	\includegraphics[width=\linewidth]{fig5.pdf}
	\caption{Similar to Figure~\ref{fig:pvallgbm} measured by the CNN on E2E simulations.}
	\label{fig:cnnpval}
\end{figure}
%------------------------------------------------------------------------
%------------------------------------------------------
\begin{figure}
	\centering
	\includegraphics[width=0.47\textwidth ,height=0.35\textheight]{fig6.pdf}
	\caption{Similar to Figure~\ref{fig:conflgbm} measured by the CNN on E2E simulations.}
	\label{fig:cnnconf}
\end{figure} 
%--------------------------------------------------------------------


%-----------------------------------------------------------------
\section{Results}\label{sec:res}
%-----------------------------------------------------------------
The pipelines for educated and deep cosmic string search were described in section~\ref{sec:det} and developed based on simulations as introduced in Section~\ref{sec:sim}.
An overview of these algorithms is presented in Figure~\ref{fig:overview}.
The results of the analysis and training processes and the performance of the various paths in the pipeline for these different sets of simulations are presented in Table~\ref{table:res}. The middle and last columns  are based on the confusion matrix and  {\it P}-value measurements respectively. 
We see that the detectability limit from the confusion matrix is more conservative than the {\it P}-value. 
As was discussed in Section~\ref{sec:MLmethod}, while a direct comparison between the distribution of predictions for different classes can distinguish the class with  lower $G\mu$ imprints from the null case, the confusion matrix implies that the predictions for this low $G\mu$
region are not fully reliable. 
We therefore conservatively use the bound from the confusion matrix as the main limit.
With this measure the deep search has often a better performance for cases with relatively low noise level. %\bt{Note--This sentences is not valid anymore: However, with E2E simulations the LightGBM method can explore  the $G\mu=4.1\times 10^{-7}$ class which is below the detectability limit of the neural network.} 

\begin{figure}
\centering
  \includegraphics[width=1.\columnwidth]{fig7.pdf}
  \caption{Overview of the data preparation and training pipelines of this work.}
  \label{fig:overview}
\end{figure}


We apply the pipelines developed for the E2E {\it Planck} simulations to the \texttt{ SMICA} temperature map.
We divide the full (except for the mask) sky {\it Planck} map into smaller patches, as was done for the simulations, and 
compare the histogram of predictions for both pipelines with the histograms of the individual $G\mu$ classes from simulations through calculating the {\it P}-value for each case. The measured {\it P}-values for all cases are below  $0.0027$ which corresponds to the $3\sigma$ frequentist level.
We therefore conclude that $G\mu$ is below the minimum detectability limit of the network with $3\sigma$ significance.  The $3\sigma$ upper bound on the  $G\mu$ as estimated from  {\it Planck} sky is $G\mu \lesssim 8.6 \times 10^{-7}$, for both LightGBM and deep search methods.
\iffalse 
%-------------------------
\begin{equation}\label{eq:finlgbm}
G\mu \lesssim 8.6 \times 10^{-7}  ~~~(3\sigma)
\end{equation}  
%-------------------------
for both LightGBM and deep search methods. }
%--------------------------
\fi
\iffalse
\begin{equation}\label{eq:fincnn}
G\mu \lesssim 8.6  \times 10^{-7} ~~~(3\sigma)
\end{equation}  
for the deep search.
\fi
%-------------------------

We also explored binary classifications with two classes of $G\mu=0$ as  class zero, and $G\mu \gtrsim G\mu_{\rm min}$ as class one, to see whether the upper bounds can be improved. However, we found no significant improvement over the results  of Table~\ref{table:res} for the E2E case and therefore the $G\mu \lesssim 8.6 \times 10^{-7}$ bound is our final result.
%-------------------------------------------------------
  %------------------------------------------------------------------------
\begin{table}
	\captionof{table}{The minimum detectable $G\mu_{\rm min}(\times 10^{-7})$ from LightGBM and CNN pipelines based on the confusion matrix and {\it P}-value measurements for different sets of simulations.}
	%\centering
	\label{table:res}
	\begin{tabularx}{0.5\textwidth}{>{\raggedleft}X|>{\centering}XX|>{\centering}XX|}
		\cline{2-5}
		& \multicolumn{2}{c|}{ Confusion Matrix} & \multicolumn{2}{c|}{ {\it P}-value}  \\ \hline
		\multicolumn{1}{|c|}{Experiment}            & LGBM  & CNN  & LGBM & CNN        \\
		\hline
		
		\multicolumn{1}{|c|}{CMB-S4-like(II)}       &  4.0 & 1.9  & 0.5  & 0.8   \\
		\multicolumn{1}{|c|}{CMB-S4-like(I)}        &  4.0  & 1.9  & 0.8  & 1.2   \\
		\multicolumn{1}{|c|}{noise-free FFP10}      &  4.0  & 0.4  & 0.3  & 0.3  \\
		\multicolumn{1}{|c|}{FFP10}                 &  8.6  & 8.6  & 1.7  & 3.6   \\
		\multicolumn{1}{|c|}{noise-free E2E}        &  8.6  & 8.6  &  2.1 & 3.8   \\
        \multicolumn{1}{|c|}{ E2E}                  &  8.6  & 8.6  & 2.7  & 5.9   \\       \hline
	\end{tabularx}
\end{table}
%------------------------------------------------------------------------
%------------------------------------------------------------------
 \section{Summary and discussion}\label{sec:sum}
 %---------------------------------------------------------------------------
In this work we developed two parallel tree-based and deep pipelines with the goal to estimate or put tight upper bounds on the level of cosmic string contribution
to the observed CMB anisotropies by {\it Planck}18 data. We also made forecasts for the $G\mu$ detectability by next generation CMB experiments.  
Table~\ref{table:res} summarizes the performance of the pipelines  for these various observational scenarios using two different criteria, here labeled by {\it P}-value and confusion matrix. 
The limits from the confusion matrix are more conservative by making sure that the predictions of the trained models are not driven  by confused decisions. 
Overall, the detectability limits based on the {\it P}-value and confusion matrix of Table~\ref{table:res} are respectively comparable with and slightly higher than the previous machine-based forecasts of \cite{vaf18}. In particular, both the LightGBM and CNN pipelines put tighter bounds on string tension in the case of noisy {\it Planck} simulations. 
%

We used the machines trained with E2E {\it Planck} simulations to search for the CS imprints on the observed \texttt{SMICA} map. Both LightGBM and CNN models found no observable trace  and yielded $G\mu \lesssim 8.6 \times 10^{-7}$  with $3\sigma$ confidence level.
It should be noted that these bounds correspond to the discrete class labels in the confusion matrix and are therefore conservative. The true upper bound could be in between the current bound and the class label right below it, which needs to be  investigated with finer grids in the vicinity of the current upper bound. 
% Training the machines on finer grids in the vicinity of the current upper bounds would also possibly improve the machines' performance and tighten the bounds. 
Further improvement could also be achieved through improving the string simulations. In our analysis it would require the string maps to go  through the very same pipeline as the primordial CMB maps, such as E2E and FFP10. 
Here, instead, we have  used  an effective Gaussian beam.  This could have slightly deteriorated the machines power in learning about the true string signal.
Training on more string simulations is also expected to help the machines in distinguishing the string imprints from background and instrumental noise. Accurate simulations of CS-induced anisotropies are quite expensive, which motivates trying producing simulations from the Generative  Adversarial Network methods. 

%
In parallel to the geometrical and statistical features and image processing tools applied in this work, one could 
 explore topological measures and invariants through, e.g., investigating the $k$-homology classes
\citep{matsubara2003statistics,adler1981geometry,adler2011topological,adler2010persistent,kozlov2007combinatorial,bubenik2015statistical}.
The topological measures could then be included in the feature vector to improve the learning process and the precision of the predictions. The full exploration of these steps are left to future work.


%\ali{As a prospective idea, we would like to include the topological and geometrical measures, particularly considering the homology groups which represents the topological invariants. These features like $k$-homology classes enumerating the $k$-dimensional topological holes of underlying field \cite{matsubara2003statistics,adler1981geometry,adler2011topological,adler2010persistent,kozlov2007combinatorial,bubenik2015statistical}, may provide new information if one includes them into the features vector leading to improvement in precision and learning performance. Accordingly, it would be useful to elucidate which measures are more sensitive for cosmic string detection. We will leave the full examination of such issue to future work.} 


\section*{Acknowledgment}
The authors are grateful to  C. Ringeval, F. R. Bouchet, and the {\it Planck} collaboration as they generously provided the simulations of the cosmic string maps used in this work.
The numerical computations were carried out on the Baobab cluster at University of Geneva and the Google collaboratory facilities.
 %C. Ringeval for generously providing the full sky cosmic string simulations used in this work.  
%------------------------------------------------------------------

\bibliographystyle{mnras}
\bibliography{bib} % if your bibtex file is called example.bib

% Don't change these lines
\bsp	% typesetting comment
\label{lastpage}
\end{document}

%--------------------------------------------------------------------------
