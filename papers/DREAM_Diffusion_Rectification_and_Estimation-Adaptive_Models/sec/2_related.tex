\section{Related work}
\label{sec:related}
\noindent\textbf{Super-resolution.} 
In single-image super-resolution, substantial efforts~\cite{liang2022details, jo2021tackling, ledig2017photo, zhang2018image, soh2019natural,
ding2022sparsity, ding2021cdfi, geng2022rstt,
 zhang2020deep, zhang2021designing, zhang2018residual, anwar2020densely} have been devoted to two primary categories: regression-based and generation-based. Regression-based methods, such as EDSR~\cite{lim2017enhanced}, RRDB~\cite{wang2018esrgan}, and SWinIR~\cite{liang2021swinir}, focus on a direct mapping from LR to HR images, employing pixel-wise loss to minimize differences between SR images and their HR references. While effective in reducing distortion, these methods often yield overly smooth, blurry images. Generation-based methods, on the other hand, aim to produce more realistic SR images. GAN-based models, like SRGAN~\cite{ledig2017photo}, combine adversarial and perceptual losses~\cite{zhang2018unreasonable} to enhance visual quality. Methods of this line include SFTGAN~\cite{wang2018recovering} and GLEAN~\cite{chan2021glean}, which integrate semantic information to improve texture realism. ESRGAN~\cite{wang2018esrgan} further refines SRGANâ€™s architecture and loss function. However, GAN-based methods often face challenges like complex regularization and optimization to avoid instability. Autoregressive models (\eg, Pixel-CNN~\cite{van2016pixel}, Pixel-RNN~\cite{oord-nips-2016}, VQVAE~\cite{van2017neural}, and LAR-SR~\cite{guo2022lar}) are computationally intensive and less practical for HR image generation. Normalizing Flows (NFs)~\cite{dinh2016density,Kingma2018} and VAEs~\cite{Kingma2013,vahdat2021nvae} also contribute to the field, but these methods sometimes struggle to produce satisfactory results.

\textbf{Diffusion model.}
Inspired by non-equilibrium statistical physics, \cite{sohl2015deep} first proposes Diffusion Probabilistic Models (DPMs) to learn complex distributions. These models have since advanced significantly~\cite{ho2020denoising, song2021denoising, nichol2021improved, dhariwal2021diffusion}, achieving state-of-the-art results in image synthesis.  Beyond general image generation, diffusion models have shown remarkable utility in low-level vision tasks, particularly in SR. Notable examples include SR3~\cite{saharia2022image}, which excels in image super-resolution through iterative refinement, and IDM~\cite{gao2023implicit}, which blends DPMs with explicit image representations to enable flexible generation across various resolutions. SRDiff~\cite{li2022srdiff} uniquely focuses on learning the residual distribution between HR and LR images through diffusion processes. LDM~\cite{rombach2022high} deviates from traditional pixel space approaches, employing cross-attention conditioning for diffusion in latent space. Building upon LDM, ResShift~\cite{yue2023resshift} employs a refined transition kernel for sequentially transitioning the residual from LR embeddings to their HR counterparts. 

\textbf{Training-sampling discrepancy.} 
% Orthogonal to the line of works \cite{song2021denoising, lu2022dpm} of improving sampling efficiency, 
\cite{ning2023input} first analyzes the training-sampling discrepancy in unconditional diffusion models, proposing to represent estimation errors with a Gaussian distribution for improved DPM training.  This discrepancy was later attributed by~\cite{yu2023debias} to a constant training weight strategy, suggesting a reweighted objective function based on the signal-to-noise ratio at different diffusion steps.  In addition, \cite{li2023alleviating} adjusts the distribution during the sampling process by choosing the optimal step within a predefined windows for denoising at each stage. \cite{ning2023elucidating} applies a predefined linear function to adjust noise variance during sampling, and~\cite{everaert2023exploiting} recommends starting the sampling from an approximate distribution that mirrors the training process in terms of frequency and pixel space.

Our approach, distinct from previous unconditional methods, addresses discrepancies based on predictions relative to the conditional input data, ensuring a tailored and accurate solution for complex visual prediction tasks like SISR. Our method also draws inspiration from step-unrolling techniques in depth estimation~\cite{saxena2023monocular, ji2023ddp} and text generation~\cite{savinov2022stepunrolled}, leveraging the model's own predictions for error estimation. However, we uniquely integrate self-estimation with adaptive incorporation of ground-truth data. This integration, guided by the pattern of estimation errors, effectively balances perceptual quality and distortion, enhancing generated image qualities.
% This training-sampling mismatch is akin to the ``exposure bias" observed in autoregressive models for text generation, where the models are trained on actual ground-truth sequences but transition to operating on their previous predicted words during inference \cite{bengio2015scheduled, RanzatoCAZ16Sequence, rennie2017self, Schmidt19closer}. Moreover, 