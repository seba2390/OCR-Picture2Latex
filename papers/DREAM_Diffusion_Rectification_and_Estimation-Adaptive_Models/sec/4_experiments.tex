\section{Experiments}
\label{sec:exp}


% Beyond the comprehensive experiments presented in this section, additional results and analysis are available in the supplementary materials.


\subsection{Implementation details}


\textbf{Baselines and datasets.}
Our experiments involve three diffusion-based SR methods as baselines, spanning datasets for faces, general scenes, and natural images. For face image datasets, we adopt SR3\footnote{Due to the unavailability of official code, we use a widely-recognized implementation \href{https://github.com/Janspiry/Image-Super-Resolution-via-Iterative-Refinement}{[link]}.}~\cite{saharia2022image}  and IDM~\cite{gao2023implicit} as baselines, with  training conducted on FFHQ~\cite{karras2019style} and evaluations on CelebA-HQ~\cite{karras2017progressive}.  For general scenes, we use the DIV2K dataset~\cite{agustsson2017ntire}, employing SR3~\cite{saharia2022image} and ResShift~\cite{yue2023resshift} as baseline models. Notably, SR3 and IDM operate in pixel space, whereas ResShift\footnote{To ensure consistency across baselines, we standardize the transition kernel to align with DDPM's approach for noise prediction.} conducts diffusion process in latent space. In addition, to assess out-of-distribution (OOD) performance, we train SR3 as baseline on the DIV2K dataset and evaluate on CAT~\cite{zhang2008cat} and LSUN datasets~\cite{yu2015lsun}. 
% More implementation details can be found in the appendix.
% Our experiments are carried out using three distinct baselines across both face datasets, natural image datasets, and a general scene dataset (DIV2K \cite{agustsson2017ntire}). Specifically for face datasets, we adopt SR3~\cite{saharia2022image} \footnote{Because the official code is unavailable, our experiments adopt the popular implementation \href{https://github.com/Janspiry/Image-Super-Resolution-via-Iterative-Refinement}{[link]}.} and IDM~\cite{gao2023implicit} as our baseline models for comparison, adhering to the same settings of training on FFHQ ~\cite{karras2019style} and evaluation on CelebA-HQ ~\cite{karras2017progressive}. For general scenes, we adopt SR3~\cite{saharia2022image} and ResShift~\cite{yue2023resshift} as our baseline models for comparison on DIV2K dataset~\cite{agustsson2017ntire}. Finally, to evaluate the out-of-distribution (OOD) performance, we train on DIV2K dataset and evaluate on the LSUN~\cite{yu2015lsun} dataset. More implementation details can be found in the appendix.

\begin{table}[]\small
\centering
\caption{Quantitative comparison for 16$\times$16  to 128$\times$128 face super-resolution on CelebA-HQ~\cite{karras2017progressive}. Consistency measures the MSE $(\times 10^{-5})$ between LR and downsampled SR images.}
\vspace{-.1in}
\label{tab:face}
\begin{tabular}{lccc}
\hline
Method     & { PSNR$\uparrow$} & { SSIM$\uparrow$} & {Consistency$\downarrow$} \\ \hline
PULSE~\cite{menon2020pulse}     & 16.88                       & 0.44                        & 161.1                              \\
FSRGAN~\cite{chen2018fsrnet}   & 23.85                       & 0.71                        & 33.8                               \\
Regression~\cite{saharia2022image} & 23.96                       & 0.69                        & 2.71                               \\
SR3~\cite{saharia2022image}        & 23.85                       & 0.71                        & 2.33\\
IDM~\cite{gao2023implicit}     & 24.01                       & 0.71                        & 2.14  
\\ \hline
SR3~\cite{saharia2022image}+DREAM        & \cellcolor{red!20}24.63                       & \cellcolor{red!20}0.74                       & \cellcolor{orange!20}2.12\\
IDM~\cite{gao2023implicit}+DREAM     & \cellcolor{orange!20}24.50                       & \cellcolor{orange!20}0.73                        & \cellcolor{red!20}1.26  
\\ \hline
\end{tabular}
\vspace{-.1in}
\end{table}



\subsection{Results and analysis}

\textbf{Effect of $p$ in $\lambda_t$.} In DREAM implementation, we set $\lambda_t = (\sqrt{1-\bar{\alpha}_t})^p$, where $p$ manages the balance between ground-truth and self-estimation data as in~\cref{eq:blend-hat-y0}.  We conduct experiments with three baselines (SR3, IDM and ResShift) for $8\times$ face SR on CelebA-HQ and $4\times$ general scene SR on DIV2K at various $p$ settings, as shown in~\Cref{tab:face-sr3-peffect}. Baselines use the standard diffusion process ($p\to\infty$). For $p=0$ ($\lambda_t\equiv1$), corresponding to the DRM model in~\cref{eq:drm-objective}, there is a notable reduction in distortion (higher PSNR and SSIM), but at the cost of perceptual quality (lower LPIPS and FID), confirming our findings in~\Cref{fig:drm-visual}. Increasing $p$ to $1$ (our full DREAM approach) leads to a slight decrease in distortion but significantly improves the balance between distortion and perception. Further increase in $p$ shows continual distortion degradation, while perceptual quality initially improves then declines. \emph{DREAM demonstrates clear advantages over baseline models across all metrics.} We found $p=1$ yields the \emph{best overall performance} compared to other $p$ values and baselines, making it our choice for subsequent experiments.

% The DREAM strategy in \cref{eq:dream-objective},  integrates a technique for dynamically combining diffusion rectification with standard diffusion. For simplicity in practice, we set $\lambda_t = (\sqrt{1-\bar{\alpha}_t})^p$, where the hyperparameter $p$ allows for adjustable balancing of information sourced from actual data versus that from self-estimation during the Markov process.  For $p=1$, $\lambda_t=1$ through the diffusion process, degrading the method to DRM. On the other hand, as $p\rightarrow \infty$, $\lambda_t\rightarrow0$, transforming the method to the standard diffusion. In \Cref{tab:face-sr3-peffect}, we  we present a comparative analysis against three baseline models for $8\times$ face super-resolution and $4\times$ general  scene (DIV2K) dataset  across various $p$ settings. The outcomes are evaluated based on distortion quality, using PSNR and SSIM, and on perceptual quality, using LPIPS and FID. As the parameter $p$ increases, there is a continuous degradation in the distortion of the SR images, while perceptual quality initially shows improvement but subsequently experiences a decline. When $p$ lies within the interval  $[1,3]$, our method attains superior results in terms of both distortion reduction and perceptual enhancement. Therefore, in this work, we have chosen to set $p=1$. 




\textbf{Face super-resolution.}
\Cref{fig:drm-visual,,fig:face-idm} show qualitative comparisons for face super-resolution from $16\times16$ to $128\times128$, applying our DREAM approach to state-of-the-art diffusion-based methods, SR3 and IDM. While SR3 and IDM generally have decent image qualities, they often miss intricate facial details like hair and eyes, resulting in somewhat unrealistic appearance, and even omit accessories like rings. In contrast, our DREAM approach operated on the these baseline  more faithfully preserves facial identity and details. \Cref{tab:face} shows a quantitative comparison of our DREAM approach applied to SR3 and IDM against other methods, using metrics such as PSNR, SSIM, and consistency~\cite{saharia2022image}. While GAN-based models are known for their fidelity to human perception at higher SR scales, their lower consistency scores suggest a notable deviation from the original LR images. Applying DREAM to SR3 and IDM, we observe considerable enhancements across all metrics. Notably, the simpler SR3, a pure conditional DDPM, when augmented with DREAM, outperforms the more complex IDM, underscoring DREAM's effectiveness.

\begin{figure}[t]
    \centering
    \includegraphics[width=0.99\columnwidth]{figures/dr_visuals/idm_visual.pdf}
    \vspace{-.1in}
    \caption{
    Qualitative comparison for $8\times$ SR using IDM~\cite{gao2023implicit} on the CelebA-HQ dataset~\cite{karras2017progressive}. Results highlight DREAM's superior fidelity and enhanced identity preservation, leading to more realistic detail generation in features like hair, eyes, and rings.} 
        \label{fig:face-idm}
    \vspace{-.1in}
\end{figure}



% \noindent
\textbf{General scene super-resolution.} \Cref{fig:div} shows a visual comparison of $4\times$ SR results on the DIV2K dataset~\cite{agustsson2017ntire}, using our DREAM approach against standard diffusion methods, with SR3 and ResShift as baselines. Standard training tends to produce images with blurred details and compromised realism, evident in unclear window outlines and distorted shirt textures. In contrast, DREAM maintains structural integrity and delivers more realistic textures. Following~\cite{guo2022lar}, we conduct a comprehensive comparison with various regression-based and generative methods on the DIV2K dataset. The results, detailed in~\Cref{tab:div2k} and benchmarked against models from~\cite{liang2021hierarchical}, demonstrate DREAM's effectiveness. Notably, DREAM has led to an increase of $1.08$dB and $3.14$dB in PSNR, and improvements of $0.03$ and $0.11$ in SSIM for SR3 and ResShift, respectively, outperforming other generative methods. Moreover, these methods demonstrate comparable or superior performance in perceptual quality metrics, marked by a 0.087 reduction in LPIPS for ResShift.  Although LPIPS scores are not as favorable as those obtained by HCFlow++, even with DREAM applied,  further improvements in image quality could be achieved through advanced network designs and incorporating GAN loss, as in HCFlow++.  However, such approaches are orthogonal to DREAM, and we leave these explorations for future work.

\begin{figure}[t]
    \centering
    \includegraphics[width=0.99\columnwidth]{figures/dr_visuals/DIV.pdf}
    \vspace{-.1in}
    \caption{
    Qualitative comparison for $4\times$ SR on DIV2K~\cite{agustsson2017ntire}. Top with SR3~\cite{saharia2022image} the baseline; bottom with ResShift~\cite{yue2023resshift} the baseline.} 
        \label{fig:div}
    \vspace{-.1in}
\end{figure}

\begin{table}[]
% \label{tab:div2k}
\caption{Quantitative comparison for 4$\times$ SR on DIV2K. All models are trained on DIV2K plus Flickr2K~\cite{timofte2017ntire}. The \colorbox{red!20}{best} and \colorbox{orange!20}{second-best} results among generative models are colorized.
% {\color{red}Red} and {\color{blue}blue} colors indicate the best and the second-best performance among generative models, respectively.
% The bold values indicate the best results among generative models.
}
\vspace{-.1in}
\setlength{\tabcolsep}{0.8mm}{
\small
\begin{tabular}{clccc}
\hline
\multicolumn{2}{c}{Method}                                               & PSNR$\uparrow$                     & SSIM$\uparrow$            & LPIPS$\downarrow$\\ \hline
&{Bicubic}                                              & 26.7                      & 0.77       & 0.409                                   \\ \hline
\multirow{2}{*}{Reg.-based}   & EDSR \cite{lim2017enhanced}             & 28.98                     & 0.83    & 0.270                                     \\
                              & RRDB \cite{wang2018esrgan}               & 29.44                     & 0.84          & 0.253                              \\ \hline\hline
\multirow{2}{*}{GAN-based}    & ESRGAN \cite{wang2018esrgan}             & 26.22                     & 0.75   &    0.124                                 \\
                              & RankSRGAN \cite{zhang2019ranksrgan}       & 26.55                     & 0.75        &0.128                               \\ \hline
\multirow{2}{*}{Flow-based}   & SRFlow \cite{lugmayr2020srflow}          & 27.09                     & \cellcolor{orange!20}0.76      & \cellcolor{orange!20}0.121                                 \\
                              & HCFlow \cite{liang2021hierarchical}      & 27.02    & \cellcolor{orange!20}0.76      & 0.124\\ \hline
Flow+GAN                      & HCFlow++ \cite{liang2021hierarchical}    & 26.61                     & 0.74      & \cellcolor{red!20}0.110                                 \\ \hline
\multirow{4}{*}{Diffusion} &  SR3~\cite{saharia2022image} & 27.02 & \cellcolor{orange!20}0.76 & \cellcolor{orange!20}0.121     \\
 &  SR3~\cite{saharia2022image}+DREAM & \cellcolor{orange!20}28.10 & \cellcolor{red!20}0.79 & \cellcolor{orange!20}0.121     \\
 & ResShift~\cite{yue2023resshift} & 25.30 & 0.68 & 0.211\\
 & ResShift~\cite{yue2023resshift}+DREAM & \cellcolor{red!20}28.44 & \cellcolor{red!20}0.79 & 0.124\\\hline
\end{tabular}
\label{tab:div2k}}
\vspace{-.1in}
\end{table}

\subsection{Training and sampling acceleration}

The DREAM strategy not only improves SR image quality but also accelerates the training.  As shown in~\Cref{fig:teaser}, DREAM reaches convergence at around 100k to 150k iterations, a significant improvement over the standard diffusion-based SR3's 400k iterations. Moreover, \Cref{fig:training} illustrates the  evolution of training in terms of distortion metrics (PSNR and SSIM) and perception metrics (LPIPS and FID) using SR3 as the baseline on the DIV2K dataset. DREAM not only converges faster but also surpasses SR3’s final results before its own convergence. For example, DREAM achieves a PSNR of $28.07$ and FID of $14.72$ at just $470$k iterations, while the baseline SR3 with standard diffusion reaches PSNR $27.02$ and FID $16.72$ after full convergence at $980$k iterations, indicating a \emph{$2\times$ speedup in training}.  Additional experiments with different baselines and datasets can be found in the appendix.
% More experiments using different baselines on various datasets are in the appendix. 

% \subsection{Sampling efficiency}

Moreover, DREAM considerably accelerates the sampling process, outperforming standard diffusion training with fewer sampling steps.  \Cref{fig:sampling} demonstrates this using SR3 on the CelebA-HQ dataset, comparing SR images generated with varying sampling steps in terms of both distortion and perception metrics. While the standard baseline typically requires an entire 2000 sampling steps, DREAM achieves improved distortion metrics  ($0.73$ v.s. $0.71$ in SSIM) and comparable perceptual quality ($0.189$ v.s. $0.184$ in LPIPS) with only 100 steps. This marks a substantial \emph{$20\times$ speedup in sampling}. More details and results are available in the appendix. 


\begin{figure}[t]
     \centering
     \begin{subfigure}[b]{0.23\textwidth}
         \centering
         \includegraphics[width=\textwidth]{figures/training-efficiency/sr3-div-training-efficiency-PS.pdf}
         \caption{Distortion}
         \label{fig:training-ps}
     \end{subfigure}
     \begin{subfigure}[b]{0.242\textwidth}
         \centering
         \includegraphics[width=\textwidth]{figures/training-efficiency/sr3-div-training-efficiency-FL.pdf}
         \caption{Perception}
         \label{fig:training-fl}
     \end{subfigure}
     \vspace{-.28in}
         \caption{Evolution of distortion metrics (left) and perceptual metrics (right) using SR3 as a baseline on the DIV2K dataset.}
        \label{fig:training}
        \vspace{-.1in}
\end{figure}


\begin{figure}[t]
     \centering
     \begin{subfigure}[b]{0.23\textwidth}
         \centering
         \includegraphics[width=\textwidth]{figures/sampling-efficiency/sr3-face-sampling-efficiency-PS.pdf}
         \caption{Distortion}
         \label{fig:sampling-ps}
     \end{subfigure}
     \begin{subfigure}[b]{0.242\textwidth}
         \centering
         \includegraphics[width=\textwidth]{figures/sampling-efficiency/sr3-face-sampling-efficiency-FL.pdf}
         \caption{Perception}
         \label{fig:sampling-fl}
     \end{subfigure}
     \vspace{-.28in}
         \caption{Comparison of distortion metrics (left) and perception metrics (right) with varying sampling steps, using  SR3 as a baseline on the CelebA-HQ dataset.}
        \label{fig:sampling}
        \vspace{-.1in}
\end{figure}




\subsection{Out-of-distribution (OOD) evaluations}


To evaluate our approach's OOD performance, we train the SR3 model on DIV2K for $4\times$ SR scaling, then evaluate its performance on various natural image datasets from the CAT~\cite{zhang2008cat} and LSUN~\cite{yu2015lsun} benchmarks, covering multiple SR scales. This OOD evaluation encompasses both dataset diversity and scaling differences. As shown in~\Cref{fig:lsun}, our DREAM training approach significantly enhances model robustness, producing more realistic and clearer images across different scales. For instance, it captures finer details such as the beard of cats at $2\times$ and $5\times$ scales, the structural integrity of a tower at $3\times$ scale, and the intricate wrinkles on a bed at $4\times$ scale.  Following~\cite{gao2023implicit}, \Cref{tab:nature} presents the average PSNR and LPIPS metrics for 100 selected images from these validation datasets. Our findings show that the DREAM training framework consistently improves baseline model across diverse datasets and scales. 


\begin{figure}[t]
\centering
\includegraphics[width=0.45\textwidth]{figures/dr_visuals/lsun.pdf} % Reduce the figure size so that it is slightly narrower than the column.
\vspace{-.1in}
\caption{Visual comparison of OOD SR. We use SR3 as a baseline, pretrain it on DIV2K and evaluate on CAT and LSUN, across various scales. The top row is obtained using standard training for SR3; the bottom row is generated using DREAM on SR3. }
\label{fig:lsun}
% \vspace{-.1in}
\end{figure}

\begin{table}[t]\small
\caption{Quantitative comparison of OOD SR on CAT and LSUN Bedroom and Tower validation sets at various scales.}
\vspace{-.1in}
\centering
\label{tab:nature}
\setlength{\tabcolsep}{0.8mm}{
\begin{tabular}{ccccc}
\hline
Scale &  & Cats & Towers & Bedrooms \\\hline
\multirow{2}{*}{$2\times$} & Standard & 19.72/0.398 &  18.82/0.333& 20.20/0.314 \\
 & DREAM & \textbf{22.50}/\textbf{0.337} & \textbf{20.89}/\textbf{0.288} & \textbf{22.15}/\textbf{0.278} \\\hline
 \multirow{2}{*}{$3\times$} & Standard & 22.48/0.281 & 18.42/0.266 & 20.14/0.235 \\
 & DREAM & \textbf{23.90}/\textbf{0.265} & \textbf{19.35}/\textbf{0.252} & \textbf{20.65}/\textbf{0.231} \\\hline
\multirow{2}{*}{$4\times$} & Standard & 26.49/0.257 & 24.03/0.217 & 26.89/0.187 \\
 & DREAM & \textbf{27.19}/\textbf{0.246} & \textbf{24.94}/\textbf{0.212} & \textbf{27.53}/\textbf{0.183} \\\hline
\multirow{2}{*}{$5\times$} & Standard & 24.52/0.381 & 21.79/0.331 & 23.18/0.313 \\
 & DREAM & \textbf{24.58}/\textbf{0.373} & \textbf{21.84}/\textbf{0.324} & \textbf{23.19}/\textbf{0.310}
 \\\hline
\end{tabular}}
% \vspace{-.1in}
\end{table}

