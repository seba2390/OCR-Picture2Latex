\clearpage
\setcounter{page}{1}
\maketitlesupplementary
In this supplementary material, we begin by describing more details of the evaluation metrics and experiment setup in \Cref{sec:app-setup}. In following \Cref{sec:app-exp}, we present more quantitative comparisons and visualization results on various baselines and datasets, which further demonstrates the effectiveness of our DREAM strategy. We conclude with a discussion of the ethical implications in \Cref{sec:app-ethic}. 

\section{Metrics and setups}
We provide a more comprehensive explanation of the metrics and the experiment settings employed in the main text of the paper.
\label{sec:app-setup}
\subsection{Metrics}
In this section, we will detail the metrics applied to measure image distortion and perception quality. The distortion metrics encompass Peak Signal-to-Noise Ratio (PSNR) and Structural Similarity Index Measure (SSIM), as well as Consistency the the perception measurement include the Learned Perceptual Image Patch Similarity (LPIPS) and the Fréchet Inception Distance (FID).

\textbf{Peak Signal-to-Noise Ratio (PSNR).} PSNR is an indicator of image reconstruction quality. However, its value in decibels (dB) presents certain constraints when assessing super-resolution tasks \cite{menon2020pulse}. Thus, it acts merely as a referential metric of image quality, comparing the maximum possible signal to the level of background noise. Generally, a higher PSNR suggests a lower degree of image distortion.

\textbf{Structure Similarity Index Measure (SSIM).} Building on the image distortion modeling framework~\cite{wang2004image}, the SSIM applies the principles of structural similarity, mirroring the functionality of the human visual system. It is adept at detecting local structural alterations within an image. SSIM measures image attributes such as luminance, contrast, and structure by employing the mean for luminance assessment, variance for contrast evaluation, and covariance to gauge structural integrity.

\textbf{Consistency.} Consistency is measured by calculating the MSE ($\times10^{-5}$) between the low-resolution inputs and their corresponding downsampled super-resolution outputs.

\textbf{Learned Perceptual Image Patch Similarity (LPIPS).} LPIPS evaluates the perceptual resemblance between generated images and their authentic counterparts by analyzing deep feature representations.

\textbf{Fréchet Inception Distance score (FID).} FID~\cite{heusel2017gans} assesses image quality by emulating human judgment of image resemblance. This is achieved by utilizing a pre-trained Inception-V3 network~\cite{szegedy2016rethinking} to contrast the distribution patterns of the generated images against the distributions of the original, ground-truth images.
\subsection{Setups}

In this section, we will provide detailed descriptions of the configurations for various baseline models as well as the datasets utilized in our experiments. 

\textbf{SR3 model on face dataset.} We train the SR3~\cite{saharia2022image} model on an upscaled $8\times$ FFHQ dataset for 1M iterations and evaluate on 100 images from the CelebA~\cite{karras2017progressive} validation dataset. During training, the LR images are consistently resized to $16\times16$ pixels, while the HR counterparts are scaled to $128\times128$ pixels. For the SR image generation, the LR images are first upscaled to $128\times128$ pixels using bicubic interpolation and serve as the conditioning input. In alignment with the DDPM~\cite{ho2020denoising},  the Adam optimizer is utilized with a fixed learning rate of 1e-4 through the training phase. The training employs a batch size of 4, incorporates a dropout rate of 0.2, and utilizes a linear beta scheduler over 2000 steps with a starting value of 
$10^{-6}$ and a final value of $10^{-2}$. A single
24GB NVIDIA RTX A5000 GPU is used under this situation.

\textbf{IDM model on face dataset.} Adhering to the offical implementation of  the IDM~\cite{gao2023implicit}, the model is trained on a $8\times$ FFHQ dataset for 1M iterations and evaluated on 100 images from the CelebA~\cite{karras2017progressive} validation dataset. Specifically, throughout training, LR images are consistently resized to $16\times16$ pixels, while their HR counterparts are scaled to $128\times128$ pixels. These LR images are then processed through a specialized LR conditioning network, which is stacked with a series of convolutional layers, bilinear downsampling filtering, and leaky ReLU activation to extract a hierarchy of multi-resolution features. These features are then employed as the conditioning input for the denoising network. The training employs the Adam optimizer with a constant learning rate of $10^{-4}$, a batch size of 32, and a dropout rate of 0.2. We implement a linear beta scheduler that advances over 2000 steps, starting from $10^{-6}$ and escalating to $10^{-2}$. This setup is supported by two 24GB NVIDIA RTX A5000.

\textbf{SR3 model on general scene dataset.} We train the SR3~\cite{saharia2022image} model on upscaled $4\times$ the training dataset comparising DIV2K~\cite{agustsson2017ntire} and Flicker2K~\cite{timofte2017ntire} for 1M iterations. Consistent with the SRDiff~\cite{li2022srdiff}, each image is cropped into patches 
of $160 \times 160$ as the HR ground truths. To produce the corresponding LR image patches of $40\times40$ pixels, the HR image patches are downscaled using a bicubic kernel. These LR image patches are then resized back to the HR dimensions using bicubic interpolation and are used as the conditioning input for the super-resolution process. For evaluation, the entire DIV2K validation set, consisting of 100 images, is utilized. The HR images are downsampled using a bicubic kernel to generate LR images, which are then cropped into $40\times40$ pixel patches with a 5-pixel overlap between adjacent patches. The SR3 model is applied to these LR patches to yield the SR predictions which are subsequently merged to form the final SR images. The model's training utilizes the Adam optimizer with a steady learning rate of $10^{-4}$, a batch size of 32 patches, and a dropout rate of 0.2. A linear beta scheduler is applied over 1000 steps, initiating at $10^{-6}$ and culminating at $10^{-2}$. This configuration is executed on two 24GB NVIDIA RTX A5000 GPUs.
% \begin{figure} [t]
%      \centering
%     \includegraphics[width=0.4\textwidth]{figures/training_vs_sampling_with_drm.pdf}
%      \caption{Evaluation of training-sampling discrepancy under our DRM framework. The mean curve over 100 samples at each time step $t$ is plotted, with the shaded area representing the standard deviation of each metric.  Here, $T=2000$.}
%     \label{fig:error-dynamic-drm}
%     \vspace{-.1in}
% \end{figure}

\textbf{ResShift on general scene datatset.} Training the ResShift model~\cite{yue2023resshift}uses a $4\times$ dataset, combining the training sets from DIV2K~\cite{agustsson2017ntire} and Flickr2K~\cite{timofte2017ntire} over 0.5M iterations. Similar as data process in the previous SR3 setting, each image is partitioned into patches of 256x256 pixels to serve as HR ground truths. The LR image patches, resized to 64x64 pixels, are derived by downscaling the HR patches with a bicubic kernel. The VQGAN encoder, pre-trained on the ImageNet dataset, processes these LR patches to distill salient features, furnishing the necessary conditioning input for the following latent denoiser network. For performance evaluation, we use the entire DIV2K validation set, which comprises 100 images. The HR images are downsampled to LR with a bicubic kernel, and then segmented into 64x64 pixel patches, maintaining an 8-pixel overlap between adjacent patches. The latent denoiser model is applied to the LR patches to generate the corresponding SR latent codes. These latent codes are subsequently processed by the VQGAN decoder to reconstruct the SR patches, thereby producing the final high-resolution super-resolution images. The training regimen employs the Adam optimizer with a consistent learning rate of $5\times10^{-5}$ and a batch size of 32 patches. A linear beta scheduler is utilized over 50 steps, selected evenly from a linearly spaced 2000-steps schedule beginning at $10^{-6}$ and increasing to $10^{-2}$. The training is conducted using two 24GB NVIDIA RTX A5000.

\begin{table}[t]
\caption{Comparison of training time by using different baselines and datasets. The value denotes the ratio of DREAM$/$standard.}
\vspace{-.1in}
\centering
\label{tab:training-time}
\setlength{\tabcolsep}{1.6mm}{
\begin{tabular}{ccc|cc}
& \multicolumn{2}{c}{Face} & \multicolumn{2}{c}{DIV2K} \\\hline
\multirow{2}{*}{Ratio}& SR3~\cite{saharia2022image} & IDM~\cite{gao2023implicit} & SR3~\cite{saharia2022image} & ResShift~\cite{yue2023resshift} \\\cline{2-5} 
 & 1.38 & 1.21 & 1.24 & 1.08\\\hline
\end{tabular}}
\vspace{-.1in}
\end{table}

\section{Additional experimental results} 
In this section, we begin by providing additional results on the acceleration of training and sampling across various baselines and datasets in \Cref{sec:app-efficiency}. Lastly, in \Cref{sec:app-vis}, we offer a more comprehensive visual comparison on the general scene dataset, using the SR3~\cite{saharia2022image} and ResShift~\cite{yue2023resshift} models as baselines. 
\label{sec:app-exp}
% \subsection{Further analysis of DRM}\label{sec:app-drm-exp}

\begin{figure}[t]
     \centering
     \begin{subfigure}[b]{0.23\textwidth}
         \centering
         \includegraphics[width=\textwidth]{figures/training-efficiency/sr3-face-training-efficiency-PS.pdf}
         \caption{Distortion}
         \label{fig:sr3-face-training-ps}
     \end{subfigure}
     \begin{subfigure}[b]{0.242\textwidth}
         \centering
         \includegraphics[width=\textwidth]{figures/training-efficiency/sr3-face-training-efficiency-FL.pdf}
         \caption{Perception}
         \label{fig:sr3-face-training-fl}
     \end{subfigure}
     \vspace{-.28in}
         \caption{Evolution of distortion metrics (left) and perceptual metrics (right) using SR3 as a baseline on the face dataset.}
        \label{fig:sr3-face-training}
        \vspace{-.1in}
\end{figure}

\begin{figure}[t]
     \centering
     \begin{subfigure}[b]{0.23\textwidth}
         \centering
         \includegraphics[width=\textwidth]{figures/training-efficiency/idm-face-training-efficiency-PS.pdf}
         \caption{Distortion}
         \label{fig:idm-face-training-ps}
     \end{subfigure}
     \begin{subfigure}[b]{0.242\textwidth}
         \centering
         \includegraphics[width=\textwidth]{figures/training-efficiency/idm-face-training-efficiency-FL.pdf}
         \caption{Perception}
         \label{fig:idm-face-training-fl}
     \end{subfigure}
     \vspace{-.28in}
         \caption{Evolution of distortion metrics (left) and perceptual metrics (right) using IDM as a baseline on the face dataset.}
        \label{fig:idm-face-training}
        \vspace{-.1in}
\end{figure}

\begin{figure}[h]
     \centering
     \begin{subfigure}[b]{0.23\textwidth}
         \centering
         \includegraphics[width=\textwidth]{figures/training-efficiency/resshift-div-training-efficiency-PS.pdf}
         \caption{Distortion}
         \label{fig:res-div-training-ps}
     \end{subfigure}
     \begin{subfigure}[b]{0.242\textwidth}
         \centering
         \includegraphics[width=\textwidth]{figures/training-efficiency/resshift-div-training-efficiency-FL.pdf}
         \caption{Perception}
         \label{fig:res-div-training-fl}
     \end{subfigure}
     \vspace{-.28in}
         \caption{Evolution of distortion metrics (left) and perceptual metrics (right) using ResShift as a baseline on the DIV2K dataset.}
        \label{fig:res-div-training}
        \vspace{-.1in}
\end{figure}

\subsection{Training and sampling acceleration}\label{sec:app-efficiency}

\textbf{Training efficiency.} In \Cref{tab:training-time}, we detail the training time ratio between our DREAM methodology and standard training approaches across a variety of baselines and datasets. Our DREAM method, which includes only a single additional forward computation, results in a marginal increase in training time. However, it offers a considerable advantage by significantly accelerating training convergence. We further illustrate the evolution of training through distortion metrics, namely PSNR and SSIM, as well as perception metrics such as LPIPS and FID. Utilizing SR3 and IDM as baselines for the face dataset, the improvements are evident in \Cref{fig:sr3-face-training} and \Cref{fig:idm-face-training}. The ResShift model, used as a baseline for the DIV2K dataset, demonstrates similar enhancements in \Cref{fig:res-div-training}. Notably, DREAM not only facilitates quicker convergence but also outperforms the final outcomes of several baselines after they fully converge. For example, with the face dataset, the SR3 model using DREAM achieves a PSNR of 24.49 and an FID of 61.02 in just 490k iterations, whereas the standard diffusion baseline reaches a PSNR of 23.85 and an FID of 61.98 after 880k iterations. This underlines a substantial training speedup by roughly $2 \times$ with DREAM. Similarly, the IDM model with DREAM reaches a PSNR of 23.54 and an FID of 55.81 in only 330k iterations, compared to the baseline achieving a PSNR of 23.85 and an FID of 61.98 after 760k iterations, reinforcing the significant efficiency of DREAM.

\begin{figure}[t]
     \centering
     \begin{subfigure}[b]{0.23\textwidth}
         \centering
         \includegraphics[width=\textwidth]{figures/sampling-efficiency/sr3-div-sampling-efficiency-PS.pdf}
         \caption{Distortion}
         \label{fig:idm-face-sampling-ps}
     \end{subfigure}
     \begin{subfigure}[b]{0.242\textwidth}
         \centering
         \includegraphics[width=\textwidth]{figures/sampling-efficiency/idm-face-sampling-efficiency-FL.pdf}
         \caption{Perception}
         \label{fig:idm-face-sampling-fl}
     \end{subfigure}
     \vspace{-.28in}
         \caption{Comparison of distortion metrics (left) and perception metrics (right) with varying sampling steps, using  IDM as a baseline on the CelebA-HQ dataset.}
        \label{fig:idm-face-sampling}
        \vspace{-.1in}
\end{figure}

\begin{figure}[t]
     \centering
     \begin{subfigure}[b]{0.23\textwidth}
         \centering
         \includegraphics[width=\textwidth]{figures/sampling-efficiency/idm-face-sampling-efficiency-PS.pdf}
         \caption{Distortion}
         \label{fig:sr3-div-sampling-ps}
     \end{subfigure}
     \begin{subfigure}[b]{0.242\textwidth}
         \centering
         \includegraphics[width=\textwidth]{figures/sampling-efficiency/sr3-div-sampling-efficiency-FL.pdf}
         \caption{Perception}
         \label{fig:sr3-div-sampling-fl}
     \end{subfigure}
     \vspace{-.28in}
         \caption{Comparison of distortion metrics (left) and perception metrics (right) with varying sampling steps, using  SR3 as a baseline on the DIV2K dataset.}
        \label{fig:sr3-div-sampling}
        \vspace{-.2in}
\end{figure}
\noindent\textbf{Sampling acceleration.} Furthermore, DREAM significantly enhances the efficiency of the sampling process, surpassing the performance of standard diffusion training with a reduced number of sampling steps. \Cref{fig:idm-face-sampling} showcases the capabilities of DREAM using the IDM model on the CelebA-HQ dataset. It compares super-resolution images generated with different numbers of sampling steps, evaluating them against both distortion and perception metrics. While the conventional baseline necessitates up to 2000 sampling steps, DREAM attains superior distortion metrics (an SSIM of 0.73 compared to 0.71) and comparable perceptual quality (an LPIPS of 0.179 versus 0.172) with merely 100 steps, leading to an impressive $20\times$ increase in sampling efficiency. In a similar vein, \Cref{fig:sr3-div-sampling-ps} illustrates the impact of DREAM using the SR3 model on the DIV2K dataset. Here, the images produced with varying sampling steps are again evaluated using both sets of metrics. Standard baselines typically require 1000 sampling steps, but with DREAM, improved distortion metrics (an SSIM of 0.79 versus 0.76) and similar perceptual quality (an LPIPS of 0.127 versus 0.121) are achieved with just 100 steps, resulting in a substantial $10\times$ sampling speedup.

\subsection{Visualization}\label{sec:app-vis}
\begin{figure}[t]
    \centering
    \includegraphics[width=0.99\columnwidth]{figures/dr_visuals/add-sr3-face.pdf}
    \vspace{-.1in}
    \caption{
    Qualitative comparison for $8\times$ SR using SR3~\cite{saharia2022image} on the CelebA-HQ dataset~\cite{karras2017progressive}. Results highlight DREAM's superior fidelity and enhanced identity preservation, leading to more realistic details, such as eye and teeth.} 
        \label{fig:app-face-sr3}
    \vspace{-.1in}
\end{figure}

\begin{figure}[t]
    \centering
    \includegraphics[width=0.99\columnwidth]{figures/dr_visuals/add-idm-face.pdf}
    \vspace{-.1in}
    \caption{
    Qualitative comparison for $8\times$ SR using IDM~\cite{gao2023implicit} on the CelebA-HQ dataset~\cite{karras2017progressive}. Results highlight DREAM's superior fidelity and enhanced identity preservation, leading to more realistic detail generation in features like nose, and wrinkles.} 
        \label{fig:app-face-idm}
    \vspace{-.2in}
\end{figure}

\paragraph{Face dataset.} In \Cref{fig:app-face-sr3} and  \Cref{fig:app-face-idm}, we provide more representative examples from CelebA-HQ dataset~\cite{karras2017progressive}, employing SR3 and IDM as baselines, respectively. These results again
validate the remarkable ability of DREAM strategy in synthesizing high-fidelity face images. 

\noindent\textbf{General scene dataset.} To further illustrate the broad effectiveness of our DREAM  strategy, we present a selection of examples from the DIV2K~\cite{agustsson2017ntire}. These examples showcase complex image elements such as intricate textures, repeated symbols, and distinct objects. We conduct a comparative visualization of our DREAM strategy against standard training practices, employing the SR3 model as a baseline in \Cref{fig:sr3-div-exp1}, \Cref{fig:sr3-div-exp2} and \Cref{fig:sr3-div-exp3}. Similarly, we use the ResShift model as a baseline in \Cref{fig:res-div-exp1}, \Cref{fig:res-div-exp2} and \Cref{fig:res-div-exp3}. The comparisons unequivocally demonstrate the superior performance of our DREAM strategy.  
\section{Ethic impact}
\label{sec:app-ethic}
This research is applicable to the task of enhancing human facial resolution, a frequent requirement in mobile photography. It does not inherently contribute to negative social consequences. However, given concerns surrounding personal security, it is crucial to safeguard against its potential misuse for harmful intentions.
\begin{figure*}[t]
    \centering
    \includegraphics[width=2.08\columnwidth]{figures/dr_visuals/sr3-div-exp1.pdf}
    \vspace{-.1in}
    \caption{
    Qualitative comparison for $4\times$ SR on DIV2K~\cite{agustsson2017ntire} using SR3~\cite{saharia2022image} model as baseline. \textbf{Left Image:} standard training; \textbf{Right Image:} DREAM training. The model trained under DREAM framework exhibits enhanced fine-grained details and rendering more realistic results, as indicated by the magnified section of the synthesized SR images.}
        \label{fig:sr3-div-exp1}
    \vspace{-.1in}
\end{figure*}

\begin{figure*}[htb]
    \centering
    \includegraphics[width=2.08\columnwidth]{figures/dr_visuals/sr3-div-exp2.pdf}
    \vspace{-.1in}
    \caption{
    Qualitative comparison for $4\times$ SR on DIV2K~\cite{agustsson2017ntire} using SR3~\cite{saharia2022image} model as baseline. \textbf{Left Image:} standard training; \textbf{Right Image:} DREAM training. The model trained under DREAM framework exhibits enhanced fine-grained details and rendering more realistic results, as indicated by the magnified section of the synthesized SR images.}
        \label{fig:sr3-div-exp2}
    \vspace{-.1in}
\end{figure*}

\begin{figure*}[]
    \centering
    \includegraphics[width=2.08\columnwidth]{figures/dr_visuals/sr3-div-exp3.pdf}
    \vspace{-.1in}
    \caption{
    Qualitative comparison for $4\times$ SR on DIV2K~\cite{agustsson2017ntire} using SR3~\cite{saharia2022image} model as baseline. \textbf{Left Image:} standard training; \textbf{Right Image:} DREAM training. The model trained under DREAM framework exhibits enhanced fine-grained details and rendering more realistic results, as indicated by the magnified section of the synthesized SR images.}
        \label{fig:sr3-div-exp3}
    \vspace{-.1in}
\end{figure*}

\begin{figure*}[t]
    \centering
    \includegraphics[width=2.08\columnwidth]{figures/dr_visuals/resshift-div-exp1.pdf}
    \vspace{-.1in}
    \caption{
    Qualitative comparison for $4\times$ SR on DIV2K~\cite{agustsson2017ntire} using ResShift~\cite{yue2023resshift} model as baseline. \textbf{Left Image:} standard training; \textbf{Right Image:} DREAM training. The model trained under DREAM framework exhibits enhanced fine-grained details and rendering more realistic results, as indicated by the magnified section of the synthesized SR images.}
        \label{fig:res-div-exp1}
    \vspace{-.1in}
\end{figure*}

\begin{figure*}[htb]
    \centering
    \includegraphics[width=2.08\columnwidth]{figures/dr_visuals/resshift-div-exp2.pdf}
    \vspace{-.1in}
    \caption{
    Qualitative comparison for $4\times$ SR on DIV2K~\cite{agustsson2017ntire} using ResShift~\cite{yue2023resshift} model as baseline. \textbf{Left Image:} standard training; \textbf{Right Image:} DREAM training. The model trained under DREAM framework exhibits enhanced fine-grained details and rendering more realistic results, as indicated by the magnified section of the synthesized SR images.}
        \label{fig:res-div-exp2}
    \vspace{-.1in}
\end{figure*}

\begin{figure*}[]
    \centering
    \includegraphics[width=2.08\columnwidth]{figures/dr_visuals/resshift-div-exp3.pdf}
    \vspace{-.1in}
    \caption{
    Qualitative comparison for $4\times$ SR on DIV2K~\cite{agustsson2017ntire} using ResShift~\cite{yue2023resshift} model as baseline. \textbf{Left Image:} standard training; \textbf{Right Image:} DREAM training. The model trained under DREAM framework exhibits enhanced fine-grained details and rendering more realistic results, as indicated by the magnified section of the synthesized SR images.}
        \label{fig:res-div-exp3}
    \vspace{-.1in}
\end{figure*}