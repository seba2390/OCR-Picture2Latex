\section{Can we train evaluators to better identify machine-generated text?}\label{sec:exp_2}

Given evaluators' inability to distinguish GPT3- and human-authored text and their inconsistent reasoning for their decisions, we investigated whether there were simple ways of improving evaluators' ability to spot attributes of GPT3-generated text.
Inspired by crowdsourcing research on guiding workers on writing or other subjective tasks~\cite{kim2017mechanical, mitra_crowdsourcing}, we tested three \emph{lightweight} evaluator-training methods to see if we could improve people's ability to identify machine-generated text while maintaining the short, low-cost nature of the evaluations. 


\subsection{Evaluator Training Methods}
We considered 3 evaluator trainings that can be added to the beginning of a human evaluation task, at most requiring only 3 extra samples of human- and machine-generated text. 
To test the effectiveness of each type of training, we re-ran the experiments from \S\ref{sec:exp_1}, but this time, we prepended one of three  evaluator-training methods to the evaluation task: an \emph{instruction-based} training, an \emph{example-based} training, and a \emph{comparison-based} training.
Screenshots of the training interfaces are in Appendix \ref{app:training+instructions}; the full set of training materials are in the supplementary materials and at \url{ark.cs.washington.edu/human_evals_ACL21}.

Other than the training, the task setup was identical to the GPT3-based tasks in \S\ref{sec:exp_1}.
We again ran the task on Amazon Mechanical Turk across three domains (stories, news, and recipes), using the same texts.
As each individual participant was only permitted to complete one set of evaluations, the set of evaluators who received these trainings was completely disjoint from the set of evaluators from our first study.
The participants were subject to the same restrictions described in \S\ref{sec:participants} and excluded according the same criteria; we did not use the trainings to filter out evaluators.
For each domain and training method pair, we had 130 unique evaluators complete the task, giving us 5,850 text annotations from 1,170 evaluators.

\subsubsection{Training with Instructions}
% explain method (point to screenshot, any references)
To give evaluators a better sense of which parts of the text to pay attention to, we extended the original task instructions to include dimensions of the text that could be helpful for identifying machine-generated text (repetition and factuality) and ones that could be misleading (grammar, spelling, and style).
We chose these dimensions based on previous work \citep{ippolito-etal-2020-automatic} and evaluators' comments in a pilot study (see Appendix \ref{app:pilot}). 

% explain pros/cons
The Instructions training was the simplest of our 3 evaluator training methods.
It was general enough to be applied across the 3 domains but provided little information about the quality and domain of text the evaluator would be rating.
It did not increase the cost of collecting evaluations (US\$1.25 per HIT) because it does not require any extra work on the part of the evaluator, though this also made it the easiest training to ignore.
The instruction-based training is the most prescriptive of the training methods, as the researcher has to choose the dimensions they want the evaluators to focus on.

\subsubsection{Training with Examples}
% explain method (point to screenshot, any references)
Our Examples training consisted of 3 practice rounds of the actual task: given a text, guess if it is machine- or human-authored.
We collected 3 additional texts in the same manner described in \S\ref{sec:data} and wrote a short explanation of which aspects of the text hinted at its source.
After an evaluator makes their guess, the correct answer and explanation are shown.
Each domain had its own set of examples and explanations.

% explain pros/cons
By showing examples, this training helps set the evaluators' expectations about the quality of the human- and machine-generated text.
We paid evaluators more for completing this task (US\$1.75 per HIT) to compensate for the extra texts they needed to read.
As with the instruction-based training, while pointing out specific text dimensions can help evaluators focus on important features, it may also restrict their search space.

\subsubsection{Training with Comparison}
% explain method (point to screenshot, any references)
In the Comparison training, we took the example passages from the Examples training and paired them with a text from the opposite source (machine or human) that began with the same prompt. 
We asked evaluators to guess which of the two texts was the machine-generated one.
We then provided the correct answer to the evaluator, along with the same explanations used in the Examples training.

% explain pros/cons
This training allows evaluators to directly compare human and machine texts written from the same prompt.
It is also the most expensive training, as it required evaluators to read three more passages than the Examples training; we paid evaluators US\$2.25 per HIT.

\begin{table*}[ht]
\centering
\begin{tabular}{lrlrrrrrrr}
\toprule
Training & \begin{tabular}[c]{@{}l@{}}Overall\\ Acc.\end{tabular} & Domain & Acc. & $F_1$ & Prec. & Recall & Kripp. $\alpha$ & \begin{tabular}[c]{@{}l@{}}\%\\ human\end{tabular} & \begin{tabular}[c]{@{}l@{}}\%\\ confident\end{tabular} \\
\midrule
\multirow{3}{*}{None} & \multirow{3}{*}{0.50} & Stories & 0.48 & 0.40 & 0.47 & 0.36 & 0.03 & 62.15 & 47.69 \\
 &  & News & 0.51 & 0.44 & 0.54 & 0.37 & 0.05 & 65.54 & 52.46 \\
 &  & Recipes & 0.50 & 0.41 & 0.50 & 0.34 & 0.00 & 66.15 & 50.62 \\
 \midrule
\multirow{3}{*}{Instructions} & \multirow{3}{*}{0.52} & Stories & 0.50 & 0.45 & 0.49 & 0.42 & 0.11 & 57.69 & 45.54 \\
 &  & News & 0.56 & 0.48 & 0.55 & 0.43 & 0.05 & 62.77 & 52.15 \\
 &  & Recipes & 0.50 & 0.41 & 0.52 & 0.33 & 0.07 & 67.69 & 49.85 \\
 \midrule
\multirow{3}{*}{Examples} & \multirow{3}{*}{*0.55} & Stories & 0.57 & 0.55 & 0.58 & 0.53 & 0.06 & 53.69 & 64.31 \\
 &  & News & 0.53 & 0.48 & 0.52 & 0.45 & 0.05 & 58.00 & 65.69 \\
 &  & Recipes & 0.56 & 0.56 & 0.61 & 0.51 & 0.06 & 55.23 & 64.00 \\
 \midrule
\multirow{3}{*}{Comparison} & \multirow{3}{*}{0.53} & Stories & 0.56 & 0.56 & 0.55 & 0.57 & 0.07 & 48.46 & 56.62 \\
 &  & News & 0.52 & 0.51 & 0.53 & 0.48 & 0.08 & 53.85 & 50.31 \\
 &  & Recipes & 0.51 & 0.49 & 0.52 & 0.46 & 0.06 & 54.31 & 53.54 \\
 \bottomrule
\end{tabular}
\caption{\S\ref{sec:exp_2} results, broken down by domain and training method, along with the $F_1$, precision, and recall at identifying machine-generated text, Krippendorff's $\alpha$, \% human-written guesses, and \% confident guesses (i.e., \textit{Definitely} machine- or human-authored). ``None'' training refers to the GPT3 results from \S\ref{sec:exp_1}. * indicates accuracies significantly better than None (no training; two-sided $t$-test, for Bonferroni-corrected $p<0.00333$).}
\label{tab:exp_2_results}
\end{table*}

\subsection{Results}
We found that while all 3 training methods improved evaluators' accuracy at identifying machine- vs. human-authored text over the no-training accuracy, the Examples training was the only one that showed significant improvement (see Table \ref{tab:exp_2_results}).\footnote{Tukey's HSD adjusted $p<0.003$ for distinguishing between the Examples training and no training, $d=0.25$}

Breaking down the results by domain, however, we find the Examples accuracy did not significantly increase over the no-training accuracy when considering any of the three domains individually.
Even so, the significant difference in overall performance is mainly contributed by the story domain;
when comparing evaluators' performance with no training to its Examples training counterpart, we see a change of 0.019 and 0.062 mean accuracy in the news and recipe domains, respectively, versus 0.086 on the story domain. 
This is perhaps due to the examples helping override the preconception that machines cannot generate ``creative'' text.

Across all 3 domains, the Examples and Comparison trainings produced the highest recall and $F_1$ scores for evaluators' judgments
and decreased the percentage of texts they guessed were human-written, which indicate that evaluators were willing to consider a broader set of texts to be machine-generated than the evaluators in \S\ref{sec:exp_1}. However, despite the trainings and the increased proportion of confident responses, evaluator agreement remained low across domain and training settings ($\alpha \leq 0.11)$, and higher agreement did not correspond to higher accuracy.

\subsection{Analysis}

We again annotated 150 comments along the dimensions listed in Appendix \ref{app:annotation}, divided into \textit{form}, \textit{content}, and \textit{machine capabilities} categories, this time from evaluators who received the best-performing Examples training.
As shown in Table \ref{tab:annotation}, we found that the proportion of \textit{form} comments dropped in the sample of evaluators who went through the Examples training, while the proportion of \textit{content} comments doubled. 
We also saw a drop in the number of comments mentioning evaluators' expectations of machine-generated text. While this change in focus doesn't necessarily correspond to correct judgments, \textit{content} reasons are more in-line with current NLG model capabilities~\cite{gpt3}.


\begin{table}
\centering
\begin{tabular}{llll}
\toprule
Training & \multicolumn{1}{l}{Form} & Content & \begin{tabular}[c]{@{}l@{}}Machine\\ capabilities\end{tabular} \\
\midrule
None & 47.1 & 24.6 & 28.3 \\
Examples & 32.5 & 50.0 & 17.5 \\
\bottomrule
\end{tabular}
\caption{\% of annotation labels that reference the text's form and content and the evaluator's perception of machines' capabilities}
\label{tab:annotation}
\end{table}