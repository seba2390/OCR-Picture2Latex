\clearpage\newpage
\appendix
\section{Appendices}
\label{sec:appendix}


\subsection{Newspapers}\label{app:newspapers}
Each newspaper came from a randomly chosen U.S. state and was selected from Wikipedia's lists of newspapers by state (\url{en.wikipedia.org/wiki/List_of_newspapers_in_the_United_States\#By_state_and_territory}).
The human-authored news articles and prompts came from the following states and websites:
\begin{itemize}[noitemsep]
    \item HI: \url{www.westhawaiitoday.com}
    \item CT: \url{www.greenwichtime.com/}
    \item WA: \url{www.vashonbeachcomber.com/}
    \item SD: \url{www.argusleader.com/}
    \item CA: \url{www.redding.com/}
    \item MA: \url{www.lowellsun.com/}
    \item NE: \url{starherald.com/}
    \item VA: \url{dailyprogress.com/}
    \item WV: \url{www.theintermountain.com/}
    \item NM: \url{www.lcsun-news.com/}
    \item LA: \url{www.nola.com/}
    \item IA: \url{qctimes.com/}
    \item NY: \url{www.pressconnects.com/}
    \item IN: \url{www.pal-item.com/}
    \item NJ: \url{www.northjersey.com/}
\end{itemize}


\begin{comment}
\subsection{Participant Demographics}\label{app:demographics}
The participant demographics for \S\ref{sec:exp_1} and \S\ref{sec:exp_2} are shown in Table \ref{tab:demographics}.

% Please add the following required packages to your document preamble:
% \usepackage{multirow}
\begin{table}[h]
\centering
\small
\begin{tabular}{llll}
\toprule
 &  & \S\ref{sec:exp_1} & \S\ref{sec:exp_2} \\
 \midrule
\multirow{8}{*}{Age} & 0-19 & 0 & 9 \\
 & 20-29 & 154 & 199 \\
 & 30-39 & 273 & 485 \\
 & 40-49 & 142 & 241 \\
 & 50-59 & 127 & 137 \\
 & 60-69 & 68 & 75 \\
 & 70-79 & 16 & 23 \\
 & 80-89 & 0 & 1 \\
 \midrule
\multirow{5}{*}{\begin{tabular}[c]{@{}l@{}}English\\ proficiency\end{tabular}} & Elementary & 4 & 11 \\
 & \begin{tabular}[c]{@{}l@{}}Limited\\ working\end{tabular} & 3 & 6 \\
 & \begin{tabular}[c]{@{}l@{}}Professional\\ working\end{tabular} & 19 & 25 \\
 & \begin{tabular}[c]{@{}l@{}}Full\\ professional\end{tabular} & 53 & 96 \\
 & \begin{tabular}[c]{@{}l@{}}Native/\\ bilingual\end{tabular} & 701 & 1032 \\
 \midrule
\multirow{3}{*}{\begin{tabular}[c]{@{}l@{}}Evaluation\\ experience\end{tabular}} & None & 284 & 457 \\
 & Some ($\leq10$) & 346 & 522 \\
 & Many ($>10$) & 150 & 191 \\
 \midrule
\multirow{4}{*}{Gender} & Female & 402 & 583 \\
 & Male & 360 & 573 \\
 & Non-binary & 7 & 11 \\
 & Prefer not to say & 11 & 3 \\
 \bottomrule
\end{tabular}
\caption{Participant demographics for \S\ref{sec:exp_1} and \S\ref{sec:exp_2}.}
\label{tab:demographics}
\end{table}
\end{comment}

\subsection{Score Frequencies}\label{app:exp1_histograms}
The frequency of the scores (out of 5) received by evaluators is shown in Figures \ref{fig:gpt2_histograms} (for GPT2 experiments) and \ref{fig:gpt3_histograms} (for GPT3 experiments).
\begin{figure*}[h]
     \centering
     \begin{subfigure}[b]{0.24\textwidth}
         \centering
         \includegraphics[width=\textwidth]{images/hist_gpt2_all.png}
         \caption{GPT2 overall}
         \label{fig:hist_gpt2_all}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{0.24\textwidth}
         \centering
         \includegraphics[width=\textwidth]{images/hist_gpt2_story.png}
         \caption{GPT2 story}
         \label{fig:hist_gpt2_story}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{0.24\textwidth}
         \centering
         \includegraphics[width=\textwidth]{images/hist_gpt2_news.png}
         \caption{GPT2 news}
         \label{fig:hist_gpt2_news}
     \end{subfigure}
      \hfill
     \begin{subfigure}[b]{0.24\textwidth}
         \centering
         \includegraphics[width=\textwidth]{images/hist_gpt2_recipe.png}
         \caption{GPT2 recipe}
         \label{fig:hist_gpt2_recipe}
     \end{subfigure}
        \caption{Histogram of scores classifying human and GPT2 texts.}
        \label{fig:gpt2_histograms}
\end{figure*}

\begin{figure*}[h]
     \centering
     \begin{subfigure}[b]{0.24\textwidth}
         \centering
         \includegraphics[width=\textwidth]{images/hist_gpt3_all.png}
         \caption{GPT3 overall}
         \label{fig:hist_gpt3_all}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{0.24\textwidth}
         \centering
         \includegraphics[width=\textwidth]{images/hist_gpt3_story.png}
         \caption{GPT3 story}
         \label{fig:hist_gpt3_story}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{0.24\textwidth}
         \centering
         \includegraphics[width=\textwidth]{images/hist_gpt3_news.png}
         \caption{GPT3 news}
         \label{fig:hist_gpt3_news}
     \end{subfigure}
      \hfill
     \begin{subfigure}[b]{0.24\textwidth}
         \centering
         \includegraphics[width=\textwidth]{images/hist_gpt3_recipe.png}
         \caption{GPT3 recipe}
         \label{fig:hist_gpt3_recipe}
     \end{subfigure}
        \caption{Histogram of scores classifying human and GPT3 texts.}
        \label{fig:gpt3_histograms}
\end{figure*}

\subsection{Annotation Details}\label{app:annotation}
The authors annotated 300 comments (150 from the No Training experiment and 150 from the Examples experiment).
For each experiment, we randomly chose 50 authors from each setting and randomly added 1 of their responses to the annotation set.
Each comment was annotated by 2 of the authors.
The annotation labels are shown in Table \ref{tab:annotation_labels}.
To create the set of annotation labels, the authors created a candidate list of labels, annotated a subset of the data collected in the pilot study (Appendix \ref{app:pilot}) together, then another subset separately, and finally refined the labels based on feedback from that process.
Because evaluators' responses often contained more than one reason for their choice, comments could receive more than one label.


\subsection{Evaluators' Expectations of Generated Text}\label{app:HUM}
Because we asked evaluators whether they thought the text was human- or machine-authored, they often justified their choices by explaining what types of human language they believed machines could (or could not) generate.
We took note of these comments and annotated for them in our data annotation process (Appendix \ref{app:annotation}) because they demonstrate the expectations evaluators have for the quality of machine-generated text.
Some example comments shown in Table \ref{tab:HUM}.


\begin{table*}[t!]
\centering
\small
\begin{tabular}{p{.09\textwidth}p{0.1\textwidth}p{.3\textwidth}p{.4\textwidth}}
\toprule
\textbf{Category} & \textbf{Label} & \textbf{Description} & \textbf{Example} \\
\midrule
\multirow{3}{*}{Form} & Grammar & The spelling and grammar of the text, punctuation/formatting issues & I would make the text more grammatical by adding more punctuation where necassary. \\
 & Level of detail & Is the text simple or does it go more in-depth? & i would include more examples and explanations of the statements. The author need to elaborate more on the topic. \\
 & Genre & If the text is the genre/domain/style/formality that the reader expects, adheres to style norms & written exactly the way a human will tell a story \\
 \midrule
\multirow{5}{*}{Content} & Repetition & Words/phrases/content repeated itself & Repeating ``or some would say'' seemed very unnatural. \\
 & Factuality & The accuracy of the text, whether it describes things that are ``true.'' & The article lists many facts that make the information seem like it was machine-generated. \\
 & Consistency & How the text relates to the context and other pieces of the text & The subject of the article follows the headline well without repeating it exactly \\
 & Common sense & Whether the text ``makes sense'' within the world that it is written & Change the ``bake in the preheated oven for 20 minutes on top of the stove.'' You can't bake on top of the stove but to bake in the oven. \\
 & Coherence & The structure and coherence of the text. Order issues go here. & More cohesion between sentences. Feel loosely related, but wording is strange. \\
 \midrule
Machine capabilities & Writer intent and expression & Speculating about writer's intent or capabilities (e.g., ability to express emotions) & The text is thorough and tries to cover all basis of the situation. It is very inclusive and humans worry about being inclusive not machines. \\
\midrule
\multirow{2}{*}{Null} & Miscellaneous & Everything else & too many dialogue-like things, and make it less gender-dicey. \\
 & Null/Vague & No reasons given, or too vague to be considered a real reason & i selected this rating because it is definitely written by human \\
 \bottomrule
\end{tabular}
\caption{The annotation labels, along with an example of each label. Note that some example sentences would also be labeled with additional labels. We did not use the Null category in the paper's analyses.}
\label{tab:annotation_labels}
\end{table*}

\begin{table*}[t!]
\centering
\small
\begin{tabular}{p{\linewidth}}
\toprule
Punctuation is perfect as well as the flow of the text. There is also more complex punctuation, such as quotes, that I think a computer would get wrong. \\
\midrule
``fried anyone to a crisp.'' That is a human if I've ever seen one. a bot or AI is more proper, they wouldn't write so casual. \\
\midrule
Because it talked about love which robots know nothing about. \\
\midrule
Lack of oxford comma. A computer would know better. \\
\midrule
The article flows properly, has appropriate English and multiple quotes. This would seem to be more than a bot could create. How would a bot create random quotes? \\
\midrule
This was more of a ramble which humans do, not computers. \\
\midrule
There are details and key phrases used in this article that computer generated text would not have in it, such as ``came up short'', ``put together a solid drive'', ``put up any points''. These are human specific terms and are not generally able to be programmed into a text program. \\
\midrule
This piece quotes the host and I don't believe AI can interview people yet so this has to be human written. \\
\midrule
It has a lot of detail in an emotional description that a machine isn't capable of giving to its readers. \\
\midrule
The way some words are phrased here again shows the human uncertainty, ``let the apples marinate for about 30 minutes''. If this was machine-generated, it would most likely just say marinate for 30 minutes. \\
\midrule
It seems to know when to use semicolns very well. This could be a human or a really smart computer. \\
\midrule
I donâ€™t think AIs are capable of writing recipes on their own just yet. \\
\midrule
I don't believe a machine could come up with this level of whimsy or creativity and have it make sense. \\
\midrule
I don't think AI would use the term `literally'. \\
\midrule
There is a lot of every day language written in this recipe that I couldn't see a machine possibly replicating. \\
\midrule
It adds that she is both nervous and excited whereas a machine wouldn't care what emotions are involved. \\
\midrule
The writer used proper grammar and punctuation. No bot could write this, \\
\midrule
I'm not sure if a computer would get the concept or use the word ``your'' where the recipe begins with ``Start by doing your prep.'' \\
\bottomrule
\end{tabular}
\caption{Example reasons evaluators gave for their decisions that spoke to their beliefs about current NLG capabilities.}
\label{tab:HUM}
\end{table*}

\subsection{Pilot Study}\label{app:pilot}
Before running the experiments described in the paper, we ran a smaller-scale version with both Amazon Mechanical Turk ($n=22$) and ``expert'' evaluators (NLP graduate students; $n=11$).
We asked the evaluators to distinguish between stories authored by humans, GPT2, and GPT3 and to explain their reasoning.
When we coded and analyzed their responses, we found that the most accurate evaluators focused on textual aspects like repetition and were less likely to mention aspects like style.
The AMT evaluators mentioned grammar and spelling far more frequently than the expert evaluators, who were more likely to mention the repetition, factuality, and commonsense of the passage.

\subsection{Training and Instructions}\label{app:training+instructions}
Figure \ref{fig:basic_instr} shows the basic instructions that were shown to all evaluators, in both \S\ref{sec:exp_1} and \S\ref{sec:exp_2}, regardless of training or domain.
All training information occurred after receiving the basic instructions.

\begin{figure*}
\centering
\fbox{\includegraphics[scale=0.4, angle=270, trim={6.5cm 1cm 8cm 0.75cm},clip]{images/NONE}}
\caption{Basic instructions shown to all evaluators.}
\label{fig:basic_instr}
\end{figure*}

\subsubsection{Instruction Training}\label{app:train_instructions}
The training shown to evaluators in the Instruction training condition is shown in Figure \ref{fig:training_instr}.
\begin{figure*}
\centering
\fbox{\includegraphics[scale=0.4, angle=270, trim={9.5cm 1cm 3.5cm 1cm},clip]{images/INSTR}}
\caption{The Instruction training.}
\label{fig:training_instr}
\end{figure*}


\subsubsection{Example Training}\label{app:train_examples}
A screenshot of the Examples and Comparison training is in Figure \ref{fig:training_example_compare}. The full set of examples and annotations used in the Examples and Comparison trainings can be found in the supplementary materials and at \url{ark.cs.washington.edu/human_evals_ACL21}.

\begin{figure*}[]
\centering
\includegraphics[scale=0.2, trim={1cm 0cm 1cm 1cm},clip]{images/Figure7_8.pdf}
\caption{The Example training (left) and Comparison training (right) in the story domain. The instructions are the same for both, except ``Choose the one you think was written by a machine.'' was in Comparison only.}
\label{fig:training_example_compare}
\end{figure*}