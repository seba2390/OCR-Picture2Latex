\section{Introduction}
\begin{figure}[ht]
\centering
\includegraphics[width=.48\textwidth]{images/fig1_tall_light.png} % removed the trim formatting  scale=0.152, trim={4cm 1cm 4cm 1cm,}
\caption{Excerpts from human evaluators' explanations for why they believe a GPT3-generated story (also excerpted) was written by a human (left) or a machine (right). The evaluators point to a wide range of text attributes to make their decisions, sometimes using the same aspect of the text to come to opposite conclusions.}
\label{fig:intro_fig}
\end{figure}

% Why do human evals matter in NLG? What role does human-authored text play in human evals?
Human-quality text has long been a holy grail for the output of natural language generation (NLG) systems, serving as an upper bound on their performance. %, since these systems typically learn from human-written text. 
Since we lack a good way of encoding many aspects of what constitutes human-quality output in an automated method, we often must rely on human evaluation for our models.
Though evaluations with end-users in an applied setting are encouraged~\cite{belz-reiter-2006-comparing}, in practice, most human evaluations instead ask people to rate generated text's \emph{intrinsic} quality
\citep{van-der-lee-etal-2019-best, howcroft-etal-2020-twenty}.
Sometimes the generated text is explicitly compared to human-authored text \citep[e.g.,][]{liu-etal-2016-evaluate,zellers_turingadvice, Zhang2020PEGASUSPW}, but even when no human-authored text is evaluated, evaluators implicitly compare the generated text to their knowledge of language and norms within specific domains. %\ta{, all of which is defined by human-written text}.


Evaluators are often asked to assess a text holistically, e.g., based on its overall quality, naturalness, or humanlikeness \citep{vanderlee_journal, howcroft-etal-2020-twenty}, 
where the exact evaluation criteria is left to the discretion of the evaluator.
Though other evaluations are broken down along specific dimensions of text quality (e.g., grammaticality, coherence, etc.), \citet{novikova-etal-2017-need,novikova-etal-2018-rankme} and \citet{callison-burch-etal-2007-meta} found that these dimensions are often correlated and may be conflated in some evaluation settings.
This is concerning because, as NLG models improve, evaluators are asked to read longer passages of text conditioned on large amounts of context. In these cases, fluency-related aspects of quality (i.e., the ones that don't require careful reading of the context and meaning of the passage) are the easiest to assess, particularly in small-batch evaluations with non-expert evaluators where speed is incentivized.
This poses a challenge when collecting human evaluations for state-of-the-art language models, as errors are often content-based (e.g., factual inaccuracies or inconsistencies with the context) rather than fluency-based \citep{gpt3}, so a superficial read may not be sufficient to catch model errors.
For accurate assessments of generated text, we need human evaluations that are designed to encourage a sufficiently careful reading of the text to examine these subtler aspects of text quality.

% What did we do in this work? What did we find?
We asked non-expert evaluators to assess the humanlikeness (operationalized as how believably human an evaluator finds a text) of text generated by current NLG models (GPT2 and GPT3) to test what current human evaluation practices can reveal about the models' quality (\S\ref{sec:exp_1}).
We found that evaluators were unable to distinguish between GPT3- and human-authored text across story, news, and recipe domains.
However, when we categorized the aspects of text the evaluators used to make their judgments, we found they primarily focused on the grammar, spelling, and style of the text.
The evaluators' responses also indicated that they underestimated the quality of text current models are capable of generating (as seen in Figure \ref{fig:intro_fig}).
To our knowledge, this paper is the first to evaluate human evaluations of GPT3-generated text across multiple domains.

We then looked at three different evaluator training methods---providing detailed instructions, annotated examples, and human-machine paired examples---to test whether we could improve evaluators' accuracy (\S\ref{sec:exp_2}). While we found including examples in the task increased the set of texts evaluators thought could be machine-generated and increased their focus on textual content, no training method significantly increased evaluators' performance consistently across domains.

Based on our results (discussed in \S\ref{sec:discussion}), we recommend moving away from small-batch evaluations with little training when collecting human evaluations of NLG models (\S\ref{sec:recommendations}).
We also encourage practitioners to consider alternative evaluation frameworks that capture the usefulness of generated text in downstream settings rather than its humanlikeness.