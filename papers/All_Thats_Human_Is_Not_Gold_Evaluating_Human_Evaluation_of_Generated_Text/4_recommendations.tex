\section{Recommendations}\label{sec:recommendations}
Based on our findings, if NLG researchers must run human evaluations as small-batch evaluations on Amazon Mechanical Turk or similar platforms, we recommend they train evaluators with examples.
This will help calibrate the evaluators' expectations of generated text and indicate the careful reading they may need to do to properly assess the text's quality.
Our experiments also indicate the importance of confirming with evaluators why they have made the decisions they have, as the criteria they might implicitly be evaluating may be mismatched with researchers' intended criteria.
However, other evaluation setups may be more successful on Amazon Mechanical Turk, such as long-term evaluations with qualified evaluators who have gone through an extended training (like those in \citealp{10.1145/1460563.1460572}; \citealp{zellers-etal-2019-hellaswag}) or third-party evaluator quality tools (e.g., Positly, used by \citealp{gpt3}).

However, given the increasing length of text NLG models can handle and the careful reading needed to detect many errors in generated text, we encourage NLG researchers to move away from standalone, intrinsic human evaluation tasks.
We found that, by default,
our evaluators in this evaluation setting were most likely to focus on surface-level, fluency-related aspects of quality.
We join past work \citep{belz-reiter-2006-comparing, vanderlee_journal} in recommending a move towards evaluation settings where evaluators are better motivated to carefully consider the content and usefulness of generated text.
For example, TuringAdvice \citep{zellers_turingadvice} asks evaluators to rate NLG models by their ability to generate helpful advice, and RoFT \citep{dugan-etal-2020-roft} engages evaluators through a guessing game to determine the boundary between human- and machine-generated text. Other evaluation methods ask the evaluators to directly interact with the generated text; for example, Choose Your Own Adventure \citep{clark-smith-2021-choose} and Storium \citep{akoury-etal-2020-storium} evaluate story generation models by having people write stories with the help of generated text.\footnote{Note that we initially tried a fourth training condition along these lines, where we asked evaluators to directly interact with the generated text by rewriting it to be more humanlike. We found we were unable to successfully recruit evaluators to complete this task. The rate of retention was less than 30\%, and the rejection rate was over 50\%. We found AMT was not a good platform for this type of task, at least not for the format and the price point we explored in this work.}
We see that GPT3 can successfully mimic human-authored text across several domains, renewing the importance of evaluations that push beyond surface-level notions of quality and consider whether a text is helpful in a downstream setting or has attributes that people would want from machine-generated text.

Finally, given the mixed effect we found different trainings can have on evaluators' performance and the lack of human evaluation details typically presented in NLG papers \citep{van-der-lee-etal-2019-best, howcroft-etal-2020-twenty}, we encourage NLG researchers to include details of any instructions and training they gave evaluators in their publications.
This, along with efforts to standardize human evaluation design \citep{belz-etal-2020-disentangling, howcroft-etal-2020-twenty} and deployment \citep{genie, gem}, will support future development of evaluator training procedures and the comparison of human evaluation results in future NLG evaluation work.