%
% File acl2021.tex
%
%% Based on the style files for EMNLP 2020, which were
%% Based on the style files for ACL 2020, which were
%% Based on the style files for ACL 2018, NAACL 2018/19, which were
%% Based on the style files for ACL-2015, with some improvements
%%  taken from the NAACL-2016 style
%% Based on the style files for ACL-2014, which were, in turn,
%% based on ACL-2013, ACL-2012, ACL-2011, ACL-2010, ACL-IJCNLP-2009,
%% EACL-2009, IJCNLP-2008...
%% Based on the style files for EACL 2006 by 
%%e.agirre@ehu.es or Sergi.Balari@uab.es
%% and that of ACL 08 by Joakim Nivre and Noah Smith

\documentclass[11pt,a4paper]{article}
\usepackage[hyperref]{acl2021}
\usepackage{times}
\usepackage{latexsym}
\renewcommand{\UrlFont}{\ttfamily\small}

% This is not strictly necessary, and may be commented out,
% but it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}
\usepackage{multirow}
\usepackage{booktabs}
\usepackage{enumitem}
\usepackage[normalem]{ulem}
\useunder{\uline}{\ul}{}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{graphicx}
\usepackage{verbatim}
\usepackage{ifthen}


\aclfinalcopy % Uncomment this line for the final submission
\def\aclpaperid{3461} %  Enter the acl Paper ID here

%\setlength\titlebox{5cm}
% You can expand the titlebox if you need extra space
% to show all the authors. Please do not make the titlebox
% smaller than 5cm (the original size); we will check this
% in the camera-ready version and ask you to change it back.
% \newcommand{\draftonly}[1]{#1}
% % Uncomment for submission
% % \renewcommand{\draftonly}[1]{}
% \newcommand{\draftcomment}[1]{\draftonly{#1}}

\newcommand\BibTeX{B\textsc{ib}\TeX}

\newboolean{showcomments}

\setboolean{showcomments}{true}  
\newcommand{\ta}[1]{\ifthenelse{\boolean{showcomments}}{{\color{orange}tal:[#1]}}{}}
\newcommand{\nik}[1]{\ifthenelse{\boolean{showcomments}}{{\color{cyan}nik:[#1]}}{}}
\newcommand{\sof}[1]{\ifthenelse{\boolean{showcomments}}{{\color{red}sof:[#1]}}{}}
\newcommand{\liz}[1]{\ifthenelse{\boolean{showcomments}}{{\color{blue}liz:[#1]}}{}}
\newcommand{\suchin}[1]{\ifthenelse{\boolean{showcomments}}{{\color{magenta}sg:[#1]}}{}}
\newcommand{\nas}[1]{\ifthenelse{\boolean{showcomments}}{{\color{brown}nas:[#1]}}{}}


\title{All That's `Human' Is Not Gold: \\ Evaluating Human Evaluation of Generated Text}

\author{Elizabeth Clark$^1$ \qquad Tal August$^1$ \qquad Sofia Serrano$^1$ \qquad Nikita Haduong$^1$ \\ \bf{Suchin Gururangan$^1$} \qquad \bf{Noah A. Smith$^{1,2}$} \\
$^1$Paul G. Allen School of Computer Science \& Engineering, University of Washington \\
$^2$Allen Institute for Artificial Intelligence \\
\texttt{\{eaclark7,taugust,sofias6,qu,sg01,nasmith\}@cs.washington.edu}}


\begin{document}
\maketitle
\begin{abstract}
Human evaluations are typically considered the gold standard in natural language generation, but as models' fluency improves,
how well can evaluators detect and judge machine-generated text?
We run a study assessing non-experts' ability to distinguish between human- and machine-authored text (GPT2 and GPT3) in three domains (stories, news articles, and recipes).
We find that, without training, evaluators distinguished between GPT3- and human-authored text at random chance level. 
We explore three approaches for quickly training evaluators to better identify GPT3-authored text (detailed instructions, annotated examples, and paired examples) and find that while evaluators' accuracy improved up to 55\%, it did not significantly improve across the three domains.
Given the inconsistent results across text domains and the often contradictory reasons evaluators gave for their judgments, 
we examine the role untrained human evaluations play in NLG evaluation
and provide recommendations to NLG researchers for improving human evaluations of text generated from state-of-the-art models.


\end{abstract}



\input{0_intro}
\input{1_experiment_1}
\input{2_experiment_2}
\input{3_discussion}
\input{4_recommendations}
\input{5_related_work}
\input{6_conclusion}

\section*{Acknowledgments}
This research was supported in part by the Office of Naval Research under the MURI grant N00014-18-1-2670. The authors would like to thank OpenAI, specifically Bianca Martin and Miles Brundage, for providing access to GPT3 through the OpenAI API Academic Access Program. The authors would also like to thank Katharina Reinecke, the members of the CSE 599 crowdsourcing class, and the ARK group for their feedback, the reviewers for their helpful comments, and the participants who took part in our study.

\paragraph{Ethical considerations}
All experiments in this paper were approved by our institution's internal review board.
Evaluators' responses were collected and stored anonymously.
Evaluators were paid based on an estimated US\$10 per hour rate; we raised the price of the task in proportion to the added difficulty of our 3 training methods.
For each dataset we considered, its source and language are included, along with any other details we believed would be relevant to evaluators' ability to read and understand the text.
Evaluators were warned about possible risks before starting the task, namely that NLG models can generate text with harmful language or themes, and were able to leave comments about their experience at the end of the study.




\bibliographystyle{acl_natbib}
\bibliography{anthology,non_acl}

\appendix
\input{7_appendix}
%\input{8_supplementary}


%\section{Supplemental Material}
%\label{sec:supplemental}



\end{document}
