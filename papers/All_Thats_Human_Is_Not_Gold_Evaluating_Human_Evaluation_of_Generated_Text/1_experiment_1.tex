\section{How well can untrained evaluators identify machine-generated text?}\label{sec:exp_1}
In our first study, we ask how well untrained evaluators can distinguish between human- and machine-generated text.
This task format, inspired by the \citet{turing_test} Test, is used to compare the quality of machine-generated text to human-authored text and, as models' fluency improves, to analyze NLG models' ability to ``fool'' readers \citep{garbacea-etal-2019-judge,ippolito-etal-2020-automatic,gpt3}. 

By asking evaluators to assess the humanlikeness of the text
with only minimal instructions (see Figure \ref{fig:task}),
we observe how well untrained evaluators can detect state-of-the-art machine-generated text and which attributes evaluators focus on and think are important for detecting machine-generated text.

\subsection{The Task}
We gave evaluators 5 text passages, some of which were written by people and some generated by a model.
We asked them to rate the text on a 4-point scale \citep{ippolito-etal-2020-automatic}:
\begin{enumerate}[noitemsep]
    \item Definitely human-written
    \item Possibly human-written
    \item Possibly machine-generated
    \item Definitely machine-generated
\end{enumerate}
If they selected option 1, we asked them: ``Why did you select this rating?'' Otherwise, they were asked, ``What would you change to make it seem more human-like?''
The interface is shown in Figure \ref{fig:task}.

\begin{figure}[ht]
\centering
\includegraphics[scale=0.45, trim={4cm 1.7cm 4cm 2cm},clip]{images/task}
\caption{The task interface (story domain)}
\label{fig:task}
\end{figure}

\subsection{Data}\label{sec:data}
We considered human- and machine-generated text in three different domains: stories, news articles, and recipes. In all three cases, we collected 50 human-authored texts in English and generated 50 texts from both the 175B parameter GPT3 model (also known as Davinci; \citealp{gpt3})\footnote{\url{beta.openai.com/}} and GPT2-XL \citep{gpt2}.\footnote{\url{huggingface.co/gpt2-xl}}
Evaluators were assigned to one domain and one model; the texts read by any given evaluator included some human-authored texts and some texts generated by their assigned model.
We only considered texts 100 words or longer,
and after reaching 100 words, all texts were truncated at the end of the next sentence.\footnote{Using NLTK; \url{www.nltk.org/}}

%For the machine-generated text,
To generate text, we used the ``three-shot'' setting described in \citet{gpt3}, conditioning the text on three additional samples of in-domain, human-authored text, which we refer to as the \textit{priming texts} (all priming texts are in the supplementary materials and at \url{ark.cs.washington.edu/human_evals_ACL21}).
While this setting is not typically how GPT2 is used in practice, we held this approach constant to directly compare how model quality changes evaluators' ability to distinguish between texts.
For each domain, each generated text was conditioned on the same set of priming texts.
The texts were delimited with an $\langle$EOS$\rangle$ token and generated using the default GPT3 generation settings (i.e., sampling with temperature $=0.7$).

\subsubsection{Stories}
The human-authored texts came from the Reddit WritingPrompts dataset \citep{fan-etal-2018-hierarchical}.\footnote{\url{github.com/pytorch/fairseq/tree/master/examples/stories}} 
We collected all the stories that began with \textit{Once upon a time} (255 stories total) and randomly chose 50 human-authored stories from this set. For the machine-generated text, we conditioned the models on the three priming texts and on the phrase \textit{Once upon a time}.
We removed generated stories that directly copied a priming text (with $>80\%$ overlap) and regenerated those texts (9 instances with GPT2, 2 with GPT3).

% dataset attributes
This is the most open-ended of the three domains, as the story's content is virtually unrestricted, and the only creative domain.
It is also the noisiest of the human-authored datasets, as the stories were originally collected from social media comments with no quality-based filtering.

\subsubsection{News Articles}
We collected 2,111 recent local news articles from 15 different newspapers using Newspaper3k\footnote{\url{github.com/codelucas/newspaper}} (details in Appendix \ref{app:newspapers}).
After filtering out articles under 100 words, we manually filtered out articles that weren't local news or that referenced the coronavirus pandemic.
We randomly chose 50 articles to use as our human-authored news articles and another 50 to use as prompts for our generation models. 
We conditioned each generated text on the headline and first sentence from the prompt articles, along with the three priming texts.

% dataset attributes
Because the title and the first sentence of a news article often summarize its contents, the generated content must adhere to the topics they introduce. By using local, recent news, we also limit the models' ability to copy from their training data.
The models seemed to have the most trouble with this dataset structurally, e.g., generating new headlines without ending the current article or outputting invalid end-of-file tags.

\subsubsection{Recipes}
We collected 50 human-authored recipes from the RecipeNLG dataset \citep{bien-etal-2020-recipenlg}, which contains 2,231,142 recipes scraped from the web. We randomly chose an additional 50 recipes and used their titles and ingredient lists as prompts, appending them to the end of the priming texts.

% dataset attributes
This is the most closed of the three domains, as the recipe must incorporate the listed ingredients and result in the dish described by the title. Recipes are typically written in clear commands, leaving little room for surprising or unexpected text.


\subsection{Participants}\label{sec:participants}
We used Amazon Mechanical Turk (AMT) to collect the text evaluations with non-expert evaluators, commonly used in NLG evaluations \citep{van-der-lee-etal-2019-best}.
To have adequate power in our analyses (based on a power analysis with $\beta=0.8$; \citealp{card-etal-2020-little}), we had 130 different evaluators for each of the 6 task settings (3 domains $\times$ 2 models).
Each participant evaluated 5 texts each, giving us a total of 780 participants and 3,900 text evaluations.

We paid evaluators US\$1.25 for completing the task.
Following common best practice on AMT~\cite{berinsky2012evaluating}, evaluators had to have over a 95\% acceptance rate, be in the United States, and have completed over 1,000 HITs (AMT tasks).
We excluded evaluators' work if their explanations were directly copied text from the task, 
did not match their responses, did not follow the instructions, or were short, vague, or otherwise uninterpretable.
Across experiments, 445 participants (18.6\%) were rejected and not included in the \S\ref{sec:exp_1} results (780 approved participants) and \S\ref{sec:exp_2} results (1,170 approved participants).

\begin{table*}
\centering
\begin{tabular}{lrlrrrrrrr}
\toprule
Model & \begin{tabular}[c]{@{}l@{}}Overall\\ Acc.\end{tabular} & Domain & Acc. & $F_1$ & Prec. & Recall & Kripp. $\alpha$ & \begin{tabular}[c]{@{}l@{}} \% \\ human\end{tabular} & \begin{tabular}[c]{@{}l@{}} \% \\ confident\end{tabular} \\
\midrule
\multirow{3}{*}{GPT2} & \multirow{3}{*}{*0.58} & Stories & *0.62 & 0.60 & 0.64 & 0.56 & 0.10 & 55.23 & 52.00 \\
 &  & News & *0.57 & 0.52 & 0.60 & 0.47 & 0.09 & 60.46 & 51.38 \\
 &  & Recipes & 0.55 & 0.48 & 0.59 & 0.40 & 0.03 & 65.08 & 50.31 \\
 \midrule
\multirow{3}{*}{GPT3} & \multirow{3}{*}{0.50} & Stories & 0.48 & 0.40 & 0.47 & 0.36 & 0.03 & 62.15 & 47.69 \\
 &  & News & 0.51 & 0.44 & 0.54 & 0.37 & 0.05 & 65.54 & 52.46 \\
 &  & Recipes & 0.50 & 0.41 & 0.50 & 0.34 & 0.00 & 66.15 & 50.62 \\
 \bottomrule
\end{tabular}
\caption{\S\ref{sec:exp_1} results, broken down by domain and model, along with the $F_1$, precision, and recall at identifying machine-generated text, Krippendorff's $\alpha$, \% human-written guesses, and  \% confident guesses (i.e., \textit{Definitely} machine- or human-authored). * indicates the accuracies significantly better than random (two-sided $t$-test, 
for Bonferroni-corrected $p<0.00333$).}
\label{tab:exp_1_results}
\end{table*}

\subsection{Results}
Overall, evaluators choosing between human and GPT2-generated text correctly identified the author of the text 57.9\% of the time,\footnote{Unless otherwise noted, all analyses binned the responses into 2 categories (\textit{human} and \textit{machine}).} but the evaluators choosing between human- and GPT3-generated text only guessed correctly 49.9\% of the time (Table \ref{tab:exp_1_results}), compared to 50\% random chance.
While the accuracy of classifying GPT2- vs. human-authored text is significantly\footnote{$t_{388}=6.58$, $p<0.0001$} different from chance, evaluators' accuracy distinguishing GPT3- and human-authored text is not.\footnote{$t_{388}=-0.09$, $p=0.93$}
This remains the case regardless of text domain; we failed to find any evidence that evaluators' accuracy on any one domain for GPT3 differs from the overall GPT3 accuracy of $\approx50$\%.\footnote{ANOVA with $F_{2,390}=0.78$, $p=0.46$}
The story texts saw the biggest drop in evaluator accuracy from GPT2 to GPT3 (62\% to 48\%, Cohen's $d=0.57$).
The distribution of evaluators' scores are shown in Appendix \ref{app:exp1_histograms}.

In Table \ref{tab:exp_1_results}, we see other statistics worsen as well between GPT2 and GPT3: how well evaluators identified the machine-generated text ($F_1$, precision, and recall), evaluators' agreement (Krippendorff's $\alpha$, a measure of annotator agreement that corrects for the probability of random agreement), and the percent of guesses that the text was human-written (\% human).
Given that the texts are equally likely to be human- and machine-written, there are disproportionately many \textit{human} guesses, making up two thirds of the responses in the GPT3 experiments.
Despite the significantly lower scores, evaluators' confidence (the percent of \textit{Definitely} responses) remains fairly constant across conditions.

\subsection{Analysis}
Taken on its own, the evaluators' difficulty identifying GPT3-generated text compared to GPT2 points to the improvement of new NLG models.
However, it also points to concerns about extending current human evaluation methodologies to state-of-the-art text generation.
In particular, the evaluators' explanations reveal underlying confusion and misconceptions about state-of-the-art NLG.

To better understand what untrained evaluators focused on in the text to make their decisions, the authors annotated 150 random responses from the evaluators who distinguished between human- and GPT3-generated text (see Appendix \ref{app:annotation} for annotation details).
We divided the text annotation labels into three categories: \textit{form}, \textit{content}, and \textit{machine capabilities}. \textit{Form} qualities focus on the format, style, and tone of the text, while \textit{content} focuses on the text's meaning. We also coded for comments that explicitly referenced people's perceptions of what types of language machines are capable (or incapable) of generating (\textit{machine capabilities}).

We found nearly twice as many comments about the form of the text than the content (\textit{form}: 47\% of labels, \textit{content}: 25\%). Evaluators in our sample focused most on the spelling, grammar, or punctuation of the texts (45 out of 150 comments) and the style or tone of the text (24 out of 150 comments). However, these dimensions of text are unlikely to be helpful in identifying text generated by current models, considering that GPT3 has already been shown to generate fluent text and to adapt easily to new generation domains \citep{gpt3}.

We also found that the reasons evaluators gave for their answers often contradicted each other. The formality of the text, spelling and grammar errors, and clarity were all cited to justify both \textit{human} and \textit{machine} judgments.
This was also reflected in the low agreement scores between evaluators, with Krippendorff's $\alpha\approx0$ across domains.

Evaluators' expectations about what NLG models are capable of ranged from thinking their text is already indistinguishable from human-authored text (``I have no idea if a human wrote anything these days. No idea at all.'') to doubting machines' ability to use basic language (``Usually AI has terrible grammer [sic] and messes up.'').
But overall we found most evaluators' beliefs about generated language underestimated or misunderstood current NLG models, as seen in Appendix \ref{app:HUM}.