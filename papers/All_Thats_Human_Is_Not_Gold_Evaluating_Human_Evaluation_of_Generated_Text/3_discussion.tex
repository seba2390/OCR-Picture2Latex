\section{Discussion}\label{sec:discussion}
Overall, none of our three training methods significantly improved evaluators' ability to detect machine-generated text reliably across text domains while still maintaining the small-batch nature of Amazon Mechanical Turk.
This speaks to the improving quality of NLG models, but we also found that untrained evaluators mainly focused on the format of the text, deciding if it was human or machine-generated based on whether the text was grammatically or stylistically correct. This, combined with the high percentage of \textit{human} guesses, the low recall scores for the \textit{machine} guesses,
and the evaluators' comments on their expectations of NLG models, indicates a systematic underestimation by the evaluators of the quality of machine-generated text. 
Evaluators who were trained with examples had higher expectations of machine-generated text and focused more on the text's content; however, the training was not sufficient to significantly raise evaluators' scores across all three domains.

Many of the explanations given by evaluators included references to the text that reflected human attributes or intent that they suspected machines could not generate (e.g., ``personal description a machine wouldn't understand, [like a pirate] wanting to be home with his wife and son'' from Figure \ref{fig:intro_fig} and the examples in Appendix \ref{app:HUM}).
However, current NLG models are capable of generating text with at least superficial reference to human attributes or intent, as seen in the generated story in Figure \ref{fig:intro_fig}. This assumption that machines can't generate text with these aspects of humanlikeness led many evaluators astray, and we suspect it is one cause of the low accuracy we found. 

Crowdsourcing studies dealing only with human-authored texts often include extensive training, quality checks, or coordination~\cite{10.1145/1460563.1460572, kim2017mechanical, bernstein2010soylent}.
NLG evaluations usually forego such structures, based, we suspect, on the assumption that evaluating machine-generated text requires only fluency in the language the text is generated in. 
Our results suggest otherwise. Evaluators often mistook machine-generated text as human, citing superficial textual features that machine generation has surpassed~\cite{gpt3}. 
One potential remedy for this is to focus evaluator training on debunking this misconception. We did see evidence that the increase in accuracy we saw with our Examples training was associated with fewer explanations mistakenly referencing machine capabilities, even though the training did not specifically focus on this.