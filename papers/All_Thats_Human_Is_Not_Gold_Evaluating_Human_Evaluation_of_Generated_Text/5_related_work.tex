\section{Related Work}
A subfield of NLG analyzes the role of human evaluations, including discussions of the tradeoffs of human and automatic evaluations \citep{belz-reiter-2006-comparing, hashimoto-etal-2019-unifying}.
There are critiques and recommendations for different aspects of human evaluations, like the evaluation design \citep{novikova-etal-2018-rankme, santhanam-shaikh-2019-towards}, question framing \citep{schoch-etal-2020-problem}, and evaluation measures like agreement \citep{amidei-etal-2018-rethinking}, as well as analyses of past NLG papers' human evaluations \citep{vanderlee_journal, howcroft-etal-2020-twenty}.
Additionally, crowdsourcing literature has work on effectively using platforms like Amazon Mechanical Turk \citep[e.g.,][]{florian_crowdsourcing,oppenheimer_crowdsourcing,weld_crowdsourcing,mitra_crowdsourcing}.
In this work, we focus on the role evaluator training can play for producing better accuracy at distinguishing human- and machine-generated text, though other quality control methods are worth exploring.

Previous work has asked evaluators to distinguish between human- and machine-authored text. For example, \citet{ippolito-etal-2020-automatic} found that trained evaluators were able to detect open-ended GPT2-L-generated text 71.4\% of the time, \citet{garbacea-etal-2019-judge} reported that individual evaluators guessed correctly 66.6\% of the time when evaluating product reviews, and \citet{gpt3} found evaluators could guess GPT3-davinci-generated news articles' source with 52\% accuracy, though these results are not directly comparable to ours due to differences in the evaluation setup, data, and participants.

Finally, our findings that untrained evaluators are not well equipped to detect machine-generated text point to the importance of researching the safe deployment of NLG systems. \citet{gehrmann-etal-2019-gltr} proposed visualization techniques to help readers detect generated text, and work like \citet{grover_zellers}, \citet{ippolito-etal-2020-automatic}, and \citet{uchendu-etal-2020-authorship} investigated large language models' ability to detect generated text.