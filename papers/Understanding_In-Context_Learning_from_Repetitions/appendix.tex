\clearpage
\addcontentsline{toc}{section}{Appendix} % Add the appendix text to the document TOC
\part{Appendix} % Start the appendix part
\parttoc % Insert the appendix TOC
% \setcounter{tocdepth}{1} % Show sections
% \addtocontents{toc}{\setcounter{tocdepth}{1}} % Set depth to 1

\section{Limitations}
While this study provides some insightful findings in the field of in-context learning, there are some limitations that should be noted.
First, the experiments in this work constrain themselves to repeating surface patterns. More sophisticated patterns are not discussed. 
Second, our work mainly focuses on revealing the token co-occurrence reinforcement and understanding its influence on in-context learning. Its detrimental influences on in-context learning suggest that resolving the spurious connections would be helpful to either chain-of-thought or in-context learning. 
% Third, 
% Third, 


\section{Experimental Details}
\label{sec:exp_detail}
% \subsection{Experimental Setting of The Desired}

\subsection{Dataset Descriptions}
\label{sec:datasets}
In this paper, we mainly use the following five datasets, and we introduce each of them and describe our preprocess of these datasets individually. 
We present cases from each dataset to demonstrate their characteristics in Table \ref{tab:datasets-cases}.

\begin{table}[htbp]
    \centering
    \caption{Datasets and Cases}
    \label{tab:datasets-cases}
    \begin{tabular}{lcl}
      \toprule
      \textbf{Dataset} & \textbf{Number of Cases}& \textbf{Examples} \\
      \midrule
      Wikitext-103 & 1000 & \begin{tabular}{@{}p{8cm}@{}}\emph{The Bintulu Government Secondary School was built in 1964.}\end{tabular} \\
      \midrule
      BookCorpus & 1000 & \emph{``A little, ''he admitted.}\\
      \midrule
      Random & 1000 & \begin{tabular}{@{}p{8cm}@{}}\emph{Green Ou incarcer hijab ura na Edmonton regardless iken  Mayor} \end{tabular}\\
      \midrule
      GSM8K & 1000 & \begin{tabular}{@{}p{8cm}@{}}\emph{Question: Janet's ducks lay 16 eggs per day. She eats three for breakfast every morning and bakes muffins for her friends every day with four. She sells the remainder at the farmers' market daily for \$2 per fresh duck egg. How much in dollars does she make every day at the farmers' market?} \\
      \emph{Let's think step by step.} \\
      \emph{Janet sells 16 - 3 - 4 = <<16-3-4=9>>9 duck eggs a day. She makes 9 * 2 = \$<<9*2=18>>18 every day at the farmer's market. \#\#\# 18}
      \end{tabular} \\
      \midrule
    %   Random & 1000 & ilar mier elsewhere Ur ates нии ollow République inda\\
    %   GSM8K & 1000 & \begin{tablular}{@{}p{4cm}@{}} Question: A raspberry bush has 6 clusters of 20 fruit each and 67 individual fruit scattered across the bush. How many raspberries are there total? Let's think step by step. \end{tabular}\\
      MMLU & 1140 & \begin{tabular}{@{}p{8cm}@{}}\emph{As more lamps are connected in parallel in a circuit, the current in the power source} \\
      \emph{A. increases} \\
      \emph{B. decreases} \\
      \emph{C. remains the same} \\
      \emph{D. Not enough information to say} \\
      \emph{Answer: A}
      \end{tabular}\\
      \bottomrule
    \end{tabular}
\end{table}


\paragraph*{Wikitext-103}
The Wikitext-103 dataset, introduced by \cite{merity2016pointer}\footnote{\url{https://blog.salesforceairesearch.com/the-wikitext-long-term-dependency-language-modeling-dataset/}}, is a language modeling dataset that contains a collection of over 100 million tokens extracted from the set of verified Good and Featured articles on Wikipedia. 
We use a randomly sampled collection of 1000 sentences provided by \cite{xu2022learning}\footnote{\url{https://github.com/Jxu-Thu/DITTO/blob/main/data/wiki_sentences.txt}}. 
The provided version is pre-tokenized to words and we use the moses detokenizer\footnote{\url{https://github.com/moses-smt/mosesdecoder/blob/master/scripts/tokenizer/detokenizer.perl}} to restore the untokenized version for compatibility with the tokenizer of transformers. 

\paragraph*{BookCorpus}
BookCorpus~\citep{Zhu_2015_ICCV} is originally introduced to align the books to their movie releases in order to provide rich descriptive explanations for visual content. It contains 74M sentences from various sources of books, and we randomly sample 1000 sentences and also detokenize with moses. 

\paragraph*{Random Generated Sentences}
Here, we follow \cite{xu2022learning} and construct 1000 randomly generated sentences. For each sentence, we first sample a random length from 5 tokens to 10 tokens, and then sample the tokens from the whole vocabulary. 

\paragraph*{GSM8K}
GSM8K~\citep{cobbe2021training} is a dataset of high-quality grade school math problems created by human problem writers. The dataset contains 7500 training problems and 1000 test problems. All the problems are answered with between 2 and 8 steps to solve. It is a frequently used benchmark to evaluate the reasoning ability of large language models~\citep{touvron2023llama,chowdhery2022palm}. To analyze the chain-of-thought~\citep{wei2022chain} effect of LLMs, we add ``Let's think step by step" followed by the question and right before the answer.


\paragraph*{MMLU}
The MMLU~\citep{hendrycks2021measuring} dataset is another commonly used benchmark to evaluate the knowledge of large language models. 
It covers 57 subjects across STEM, the humanities, the social sciences, and more. It ranges in difficulty from an elementary level to an advanced professional level, and it tests both world knowledge and problem-solving ability. We randomly sample 20 test cases from each task, resulting in 1140 test queries in total. In the `undesired' section, we uniformly redistribute the answers and options to isolate the selection bias of LLMs. 




% Different from previous approach, when analyzing the output space, we randomly choose demonstrations for each test query, avoiding the inductive bias of pre-defined demonstrations. 

\subsection{Experimental Details}
\begin{table}[t]
    \centering
    \caption{Semantically equivalent substitutes.}
    \label{tab:semantic_substitutes}
    \begin{tabular}{lcc}
      \toprule
      \textbf{Category} & \textbf{Original}& \textbf{Substitutes} \\
      \midrule
       Option Names & A.; B.; C.; D. & 
       \begin{tabular}{c@{}p{8cm}@{}}\emph{I.; II.; III.; IV.} \\
       \emph{E.; F.; G.; H.} \\
       \emph{1.; 2.; 3.; 4.} \\
       \emph{(a).; (b).; (c).; (d).} \\
    \end{tabular}\\
    \midrule
    Answer Indicator & Answer: & 
    \begin{tabular}{c@{}p{8cm}@{}}\emph{Solution} \\
    \emph{Reply} \\
    \emph{Response} \\
    \emph{Result} \\
    \emph{Choice} \\
    \end{tabular}\\
    \bottomrule
    \end{tabular}
\end{table}

\paragraph*{Random Substitutes}
Throughout the paper, we adopt random substitutes to isolate the effects of tokens, specific formats, and other components. The random substitutions are conducted in the following manner. 
To avoid the effect of different sequence lengths, we first tokenize the original sentence or demonstration using the Transformers tokenizer. Then, we replace the part to substitute with random tokens from the corresponding vocabulary. With the substitutes, we exclude the special tokens to ensure that all random tokens are valid. 

\paragraph*{Self-Reinforcement}
For each experiment, we report the averaged results across three runs with different seeds. 
% \subsection{Experiment Settings}
We mainly conduct our experiments over two model families, LLaMA and OPT. For LLaMA models, we use 4 models sized in [7b, 13b, 30b, 65b]. 
For OPT models, we use 7 models with sizes ranging in [125m, 350m, 1.3b, 2.7b, 6.7b, 13b, and 30b].
Each sentence is repeated 20 times in our experiments.
Following \cite{xu2022learning}, we randomly concatenate a prefix before all repeated sentences. 

% \subsection{Experimental Details for }
\paragraph{In-Context Learning}

In the MMLU experiments, we replace option names and answer indicators to study the importance of token reinforcement in directing output space. 
Specifically, for each replacement, we choose semantically equivalent substitutes from a pool and randomly replace the original option name/answer indicator with the chosen substitute. In this way, we break the token reinforcement from the demonstrations. 
We put the pool of our replacement in Table \ref{tab:semantic_substitutes}. 

% \paragraph{The Undesired}

\paragraph{Significance test}
We conduct the significance test using paired t-tests, where we randomly split the test set into 5 folds and compute the significance over accuracies on these 5 subsets. In Table \ref{tab:undesired}, we compute the significance levels for [A, B, C] and D separately. 

% \paragraph*{Experimental Details of Section \ref{}}
% How we replace with semantically equivalent substitutes.


\section{Supporting Experiments of Token Co-occurrence Reinforcement}
\label{sec:support_exps}
In this section, we provide more experimental evidence related to the token co-occurrence reinforcement. 
The experimental settings in this section follow the same setting as in Section \ref{sec:token-reinforce}, except for models to use, datasets to use, and the choices of tokens. 

\begin{figure}[htbp]
    \centering
    \includegraphics[width=1.0\textwidth]{figs/opt-token-self-reinforce.pdf}
  \caption{\textbf{Token co-occurrence reinforcement of OPT models.} }
  \label{fig:opt-token-reinforce}
\end{figure}

\subsection{Token Reinforcement of OPT Models}

In Figure \ref{fig:opt-token-reinforce}, we plot the token reinforcement of all 7 OPT models. The results are consistent with our results of LLaMA in the main context, validating the generality of token reinforcement across different families of large language models.  

\subsection{Token Reinforcement on Other Datasets}
\begin{figure}[t!]
    \centering
    \includegraphics[width=1.0\textwidth]{figs/wiki-token-self-reinforce.pdf}
  \caption{\textbf{Token co-occurrence reinforcement on Wikitext-103.} }
  \label{fig:wiki-token-reinforce}
\end{figure}
\begin{figure}[t!]
    \centering
    \includegraphics[width=1.0\textwidth]{figs/book-token-self-reinforce.pdf}
  \caption{\textbf{Token co-occurrence reinforcement on BookCorpus.} }
  \label{fig:book-token-reinforce}
\end{figure}

Here, we plot the token reinforcement on Wikitext-103 and BookCorpus. As we can see, the probability of reinforced tokens is quite different in the two datasets. In BookCorpus, with 4 kept tokens, the probability can be boosted to about 0.8, whereas in Wikitext-103, the probability can only reach about 0.5. 
Note that compared to the results on randomly generated sentences, tokens in these two datasets are more likely to co-occur in the pre-training data. 
This indicates the semantic relationship between tokens affects how token reinforcement performs. 

\subsection{Semantic Relationships of Token Reinforcement}

\begin{figure}[t!]
    \centering
    \includegraphics[width=1.0\textwidth]{figs/token-reinforce-semantic.pdf}
  \caption{\textbf{Token reinforcement against semantic relationships.} }
  \label{fig:semantic}
\end{figure}

We further conduct experiments to investigate how the semantic relationship between two tokens affects the token co-occurrence reinforcement. We choose three relationships: (1) Random two tokens. (2) The same two tokens. (3) Two tokens that are similar in the embedding space.

Figure \ref{fig:semantic} plots our results for different LLaMA models. We observe clear gaps among different semantic relationships. Two tokens that are the same can build a strong connection stronger than two tokens that are similar in the embedding space. Further investigating the reasons behind this is interesting and may unravel the internal biases of large language models. We leave it as the future work. 

\subsection{Improved Ratio of Token Reinforcement}
In this section, we solidify our findings with the ratio of improved token probability. Formally, the improved ratio~(IR; \citet{xu2022learning}) is defined as follows: 
\begin{gather}
    \hat{\mathbf{IR}} = \hat{\mathbf{TP}_N} > \hat{\mathbf{TP}_0}.
\end{gather}
Our metric of improved ratio is defined over the setting in Section \ref{sec:token-reinforce}. Figure \ref{fig:ir} plots our results of IR on three datasets. We only consider the case of two tokens. 

As we can see, the improved ratios for all three datasets quickly reach almost 1.0 with only several repetitions, indicating token reinforcement exists in most of the cases. In addition, we observe that IRs in Wikitext-103 and BookCorpus have larger variances than those in randomly generated sentences, because tokens in Wikitext-103 and BookCorpus are more likely to co-occur in the same context and thus get larger probabilities without reinforcement. 

\begin{figure}[t!]
    \centering
    \includegraphics[width=1.0\textwidth]{figs/token-reinforce-ir.pdf}
  \caption{\textbf{Improved Ratio on three datasets.} }
  \label{fig:ir}
\end{figure}


\section{Other Experiments on Detrimental Effects}
\subsection{Generation of Cot Answer}
\label{sec:cot_answer}

% \begin{figure}[t]
%   \centering
%     \includegraphics[width=0.5\linewidth]{figs/gsm8k-output-cot-answer.pdf}
%   \caption{\textbf{After generating CoT prompt, the probability of generating the CoT answer.} }
%   \label{fig:cot_answer}
% \end{figure}

\begin{figure}[t]
  \centering
  \begin{subfigure}[b]{0.48\linewidth}
    % \raggedright
    \includegraphics[width=0.9\linewidth]{figs/gsm8k-output-cot-answer.pdf}
    % \caption{}
    % \label{fig:subfigure1}
  \end{subfigure}
  \hfill
  \begin{subfigure}[b]{0.48\linewidth}
    % \raggedleft
    \includegraphics[width=0.9\linewidth]{figs/gsm8k-output-cot-answer-first.pdf}
  \end{subfigure}
  \caption{\emph{Left:} \textbf{The probability of generating the CoT answer.} \emph{Right: }\textbf{The probability of generating the first token in CoT answer.}}
  \label{fig:cot_answer}
  % \vspace{-1pt}
\end{figure}

In this section, we study how the CoT answer is generated with respect to our discovered surface patterns. We study the problem on the GSM8K dataset. 
Recall the demonstrations in GMS8K, i.e., ``Question: [Question] // Let's think step by step. // [CoT Answer]''. 
We plot the probability of [CoT Answer] in Figure \ref{fig:cot_answer}. 
We would expect the [Question] to have a huge effect on generatin the CoT answer. 
Thus, we involve the probability with random substitution of the [Question] part. 
Interestingly, we find that even without the [Question] part, the probability of [CoT Answer] still increases with demonstrations, suggesting the patterns play a crucial role in learning how to generate the CoT answers. 

So what does the [Question] part do in in-context learning of GSM8K? 
We further plot the probability of the first token of [CoT Answer]. 
The probability of the first token significantly decreases when we mask out [Question]. 
Hence, even though the [Question] part does not affect much the probability of [CoT Answer], it substantiates the generation of [CoT Answer] at the very beginning.



\subsection{Selection Bias of LLMs}
In experiments illustrated in table \ref{tab:undesired}, we observe a selection bias of LLaMA-65B. 
We show the results for each class in Table \ref{tab:selection_bias}.
Note that we randomly permute the test set to make sure all questions are balanced with [A,B,C,D]. 

First, we see that the zero-shot performances in different class are quite different. Class `A' gets an accuracy as high as 71.58\% and class `D' only gets 52.28\%. Second, with more demonstrations, the overall accuracy is improved, from 60.61\% to 63.16\%, demonstrating the effectiveness of ICL. However, the accuracy of class `A' largely decreases, while the accuracies for `B', `C', and `D' all increase. 
The above findings indicate that LLaMA-65B has a selection bias in both zero-shot and few-shot scenarios. 

\begin{table}[htbp]
    \centering
    \caption{\textbf{Accuracies for each class on our balanced MMLU.} We use the LLaMA-65B and vary the number of demonstrations from 0 to 5.}
    \label{tab:selection_bias}
    \begin{tabular}{c|cccccc}
        Class & 0 & 1 & 2 & 3 & 4 & 5 \\
        \midrule
        A & 71.58 & 57.31 & 58.25 & 57.31 & 56.96 & 54.74 \\
        B & 59.65 & 66.90 & 69.94 & 70.41 & 69.59 & 70.76 \\
        C & 58.95 & 67.02 & 65.50 & 64.80 & 66.78 & 67.60 \\
        D & 52.28 & 59.65 & 60.00 & 59.30 & 59.88 & 59.53 \\
        \hline
        Avg. & 60.61 & 62.72 & 63.42 & 62.95 & 63.30 & 63.16 \\
\end{tabular}
\end{table}