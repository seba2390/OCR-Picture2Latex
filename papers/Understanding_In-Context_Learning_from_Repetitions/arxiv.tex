
\documentclass{article} % For LaTeX2e
\usepackage{iclr2024_conference,times}

% Optional math commands from https://github.com/goodfeli/dlbook_notation.
% \input{math_commands.tex}

% \usepackage{hyperref}
% \usepackage{url}

% \documentclass{article} % For LaTeX2e
% \usepackage{iclr2023_conference,times}

% Optional math commands from https://github.com/goodfeli/dlbook_notation.
\input{math_commands.tex}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{xcolor}  
\usepackage{booktabs}       % professional-quality tables
\usepackage{enumitem}
\usepackage{wrapfig}% colors
\usepackage{multirow}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{subcaption}
% \usepackage{subfigure}
\usepackage{bbm}
\usepackage{color}
\usepackage{tabularx}
% \DeclareMathOperator*{\argmax}{arg\,max}
\usepackage{textcomp}
\usepackage{mathtools}
\usepackage{minitoc}

\usepackage[toc,page,header]{appendix}

% Make the "Part I" text invisible
\renewcommand \thepart{}
\renewcommand \partname{}

\doparttoc % Tell to minitoc to generate a toc for the parts
\faketableofcontents % Run a fake tableofcontents command for the partocs

% \newcommand{\jin}[1]{{\color{blue}#1}}

\def\elliott#1{{\color{cyan}{\bf [Elliott:} {{#1}}{\bf ]}}}

\def\chenming#1{{\color{red}{\bf [Chenming:} {{#1}}{\bf ]}}}
\def\jin#1{{\color{blue}{\bf [Xujin:} {{#1}}{\bf ]}}}


% \definecolor{custompurple}{rgb}{0.439, 0.188, 0.627}
% \author{
%   Jianhao Yan \\ Zhejiang University \\ School of Engineering, Westlake University \\ \texttt{elliottyan37@gmail.com} 
%   \And Jin Xu \\ Alibaba Group 
%   \And Chiyu Song \\ Zhejiang University \\ School of Engineering, Westlake 
%   \And Chenming Wu \\ Baidu Research
%   \And Yafu Li \\ Zhejiang University \\ School of Engineering, Westlake 
%   \And Yue Zhang \\ Institute of Advanced Technology, Westlake Institute for Advanced Study
%   }
\author{%
\centerline{Jianhao Yan$^{1,2}$ ~~ Jin Xu$^{4}$ ~~ Chiyu Song$^{1,2}$ ~~ Chenming Wu$^{5}$ ~~ Yafu Li$^{1,2}$ ~~ Yue Zhang$^{2,3,*}$} \\
\centerline{\normalfont{$^1$Zhejiang University} \quad \quad \normalfont{$^2$School of Engineering, Westlake University}} \\
\centerline{\normalfont{$^3$ Institute of Advanced Technology, Westlake Institute for Advanced Study}}\\
\centerline{\normalfont{$^4$ Tsinghua University} \quad \quad \normalfont{$^5$ Baidu Research}} \\
\centerline{\texttt{elliottyan37@gmail.com}}
}
% % \quad\texttt{\{leyangcui,victoriabi,shumingshi\}@tencent.com} \\
% % \quad\texttt{\{yanjianhao,yinyongjing,zhangyue\}@westlake.edu.cn}\\
% }
 


% NOTE: how to change new line???
% \title{Double-Edged Sword of Analogy: Understanding In-Context Learning from Repeats}
\title{Understanding In-Context Learning from Repetitions}

{
\renewcommand{\thefootnote}{\fnsymbol{footnote}}
\footnotetext[1]{\ Corresponding author.}
}

% Authors must not appear in the submitted version. They should be hidden
% as long as the \iclrfinalcopy macro remains commented out below.
% Non-anonymous submissions will be rejected without review.

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to \LaTeX{} to determine where to break
% the lines. Using \AND forces a linebreak at that point. So, if \LaTeX{}
% puts 3 of 4 authors names on the first line, and the last on the second
% line, try using \AND instead of \And before the third author name.

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\iclrfinalcopy % Uncomment for camera-ready version, but NOT for submission.
\begin{document}

% \doparttoc % Tell to minitoc to generate a toc for the parts
% \faketableofcontents % Run a fake tableofcontents command for the partocs

\part{} % Start the document part
% \parttoc % Insert the document TOC


\maketitle
\begin{abstract}
  This paper explores the elusive mechanism underpinning in-context learning in Large Language Models (LLMs). 
  Our work provides a novel perspective by examining in-context learning via the lens of surface repetitions. 
  We quantitatively investigate the role of surface features in text generation, and empirically establish the existence of \emph{token co-occurrence reinforcement}, a principle that strengthens the relationship between two tokens based on their contextual co-occurrences.
  By investigating the dual impacts of these features, our research illuminates the internal workings of in-context learning and expounds on the reasons for its failures. This paper provides an essential contribution to the understanding of in-context learning and its potential limitations, providing a fresh perspective on this exciting capability.
\end{abstract}
\section{Introduction}
% LLMs are great, ICL is perfect.
% In the rapidly evolving field of artificial intelligence, large language models (LLMs) are transforming the landscape of machine learning and natural language processing. These vast AI models, such as GPT4~\citep{} by OpenAI, exhibit impressive capabilities in understanding and generating human-like text, making them revolutionary tools in countless applications. 
% A standout characteristic of large language models (LLMs) is their ability to perform \emph{in-context learning}~(ICL), where given only a few demonstrations, models are able to predict on unseen test queries for various tasks~\citep{brown2020language}. 
% maybe I can mention it is surprising to gain such ability by open-domain pre-training.
% why this is intriging
% This behavior deeply connects to human being's ability of learning and reasoning with analogy, where we recall a similar situation, we match them up and we learn~\citep{winston1980learning}.
% This behavior mirrors human learning and reasoning from \emph{analogy}, where `we recall a similar situation, we match them up, and we learn'~\citep{winston1980learning}.

% What makes ICL even surprising is its occurrence in models \emph{without being explicitly pretrained to learn}~\citep{brown2020language,zhang2022opt,chowdhery2022palm}.
% However, understanding the occurrence of ICL remains an open problem. 
% Previous work mainly discusses in-context learning either from pretraining data~\citep{chan2022data,xie2022an} or from implicit learning mechanism~\citep{garg2023transformers,vonoswald2023transformers,dai2023gpt}, where synthetic dataset and small sized models are used.

% \elliott{change occurrence to inner working}
% \elliott{mention other works to analyze ICL}
% Nonetheless, there exists a significant gap between the experimental settings of these studies, with their smaller model sizes or synthetic learning objectives.\jin{add citations, which work is smaller size, which one is synthetic learning.}
% the small size of their models and specific goals of Inter-Cluster Learning (ICL).
% These studies are typically constrained by smaller model sizes and specific ICL objectives. 

% NOTE: also use analogy to guide the motivation.
% In a distinct vein of research, recent studies~\citep{holtzman2019curious,fu2020theoretical,xu2022learning} have shed light on the notorious issue of repetitive text generation. This could potentially be attributed to the 'self-reinforcement effect'~\cite{xu2022learning}, a phenomenon characterized by an increased likelihood for language models (LMs) to continue generating a sentence that frequently appears in the context. In this paper, we propose a significant link between the remarkable in-context learning ability of these models and the aforementioned self-reinforcement effect. 
% \begin{figure}[t!]
%   \centering
%   \includegraphics[width=0.9\textwidth]{figs/icl_and_repetitive_egs.pdf}
% \caption{We showcase the difference and resemblance between in-context learning and repetitive generation. We additionally involve the reinforced/learned connections from demonstrations, and the demonstrations are from the task of sentiment analysis. 
% % discuss resemblance and difference
% Both can be seen as learning from analogy, i.e., learning connections from the demonstrations and making decisions based on these connections.
% In the case of in-context learning, the model learns reliable connections and hopefully several of these connections result in the function of sentiment analysis. 
% On the other hand, in repetitive demonstrations, the model gets stuck to spurious connections and miss the key information \emph{`decreased'}, leading to a wrong prediction.}
% \label{fig:examples}
% \vspace{-10pt}
% \end{figure}

\begin{figure}[h]
  \centering
  
  \begin{subfigure}[b]{0.85\linewidth}
    \centering
    \includegraphics[width=\linewidth]{figs/success.pdf}
    \caption{A correct prediction of in-context learning.}
    % \label{fig:success}
    % \label{fig:subfigure1}
  \end{subfigure}
  \hfill
  \begin{subfigure}[b]{0.85\linewidth}
    \centering
    \includegraphics[width=\linewidth]{figs/failure.pdf}
    \caption{An in-correct prediction of in-context learning.}
    % \label{fig:falure}
  \end{subfigure}
  \caption{We showcase correct and incorrect predictions of in-context learning of LLaMA-65B. 
  The shown task is to identify whether the given sentence presents a positive sentiment. 
  We involve the token reinforced connections from demonstrations.
  In both cases, LLMs learn connections from the demonstrations and make decisions based on these connections.
  In the case of in-context learning, the model learns reliable connections and hopefully several of these connections result in the function of sentiment analysis. 
  On the other hand, in repetitive demonstrations, the model gets stuck to spurious connections and misses the key information \emph{`decreased'}, leading to a wrong prediction.
  }
  \vspace{-10pt}
  \label{fig:examples}
\end{figure}

The impressive ability of Large Language Models (LLMs; \citet{touvron2023llama,chowdhery2022palm,openai2023gpt4}) to execute in-context learning (ICL) is a standout characteristic. 
This behavior mirrors human learning and reasoning from \emph{analogy}~\citep{winston1980learning}, enabling LLMs to rapidly adapt to a range of downstream tasks.
Without being explicitly pretrained to learn from demonstrations, LLMs can predict responses to unseen test queries from a few demonstrations and without any instruction given~\citep{brown2020language,zhang2022opt,chowdhery2022palm}. 
An example of in-context learning can be found in Figure \ref{fig:examples}(a), where a pre-trained LLaMA model is given demonstrations for a binary classification task, and learns to make predictions correctly.
% As a result, it is an important research question to understand the underlying reason for such capabilites. 
Despite the success in applications, the working mechanism of in-context learning is still an open question.


% The impressive ability of Large Language Models (LLMs; \citet{touvron2023llama,chowdhery2022palm,openai2023gpt4}) to execute in-context learning (ICL) is a standout characteristic. These models can predict responses to unseen test queries from a few demonstrations alone~\citep{brown2020language}. 
% An example of in-context learning can be found in Figure \ref{fig:examples}(a).
% This behavior mirrors human learning and reasoning from \emph{analogy}~\citep{winston1980learning}, enabling LLMs to rapidly adapt to a range of downstream tasks without the extra cost of fine-tuning.


% ICL occurs in models that have not been explicitly pretrained to learn from demonstrations\citep{brown2020language,zhang2022opt,chowdhery2022palm}. 
% Prior research has mainly explored the occurrence and mechanism of in-context learning from two angles: either focusing on data for pretraining\citep{chan2022data,xie2022an} or suggesting an implicit learning algorithm~\citep{garg2023transformers,vonoswald2023transformers,dai2023gpt}. Others are devoted to analyzing factors affecting in-context learning, including input-label mapping~\citep{min2022rethinking,yoo2022groundtruth}, diversity~\citep{an2023incontext}, ordering~\citep{lu-etal-2022-fantastically} and etc. 
% However, the inner working mechanism of in-context learning remains unclear. 

% \jin{Prior works have shown that the performance of LLM's in-context learning ability has a strong relationship with the data format in demonstrations~\citep{RethinkICL, InverseScaling} and has riskch been misleading towards a different solution from true tasks by distractor tasks ~\citep{InverseScaling}. Motivated by these findings, we aim to study the relationship between the phrases in demonstrations with the probability of desired outputs.} 
We take a feature-centric view to understand ICL, analyzing the key patterns in the input context that correlate with ICL behavior. In particular, as Figure \ref{fig:examples}(b) shows, in-context demonstrations can result not only in desired effects but also cause errors. 
In this example, the same LLaMA model makes the incorrect prediction `\emph{True}' given the input ``\emph{Circulation revenue has decreased by 5\% in
Finland.}'', which is likely because of the repeated pattern ``Answer:'' -> ``True'' from the demonstrations. 
In the same perspective, the success case in Figure \ref{fig:examples}(a) can be attributed to learning desired patterns such as ``Answer:'' -> ``True|False'' in the demonstrations. 
Such patterns are apparently used as features in the autoregressive inference process by the model.

The patterns we discussed above can be viewed as generalizations to repetition patterns~\citep{holtzman2019curious,fu2020theoretical} and self-reinforced patterns~\citep{xu2022learning} which have been discussed in the literature.
The `self-reinforcement effect' describes a phenomenon where the model tends to perpetuate the generation of sentences that have frequently appeared in its context. 
These effects are regarded as harmful to text generation and previous work puts efforts to mitigate it. 
However, the above observation implies that they play the role of both angels and demons in LLMs.

Building upon our observation, we quantitatively investigate in-context learning from the perspective of surface patterns, illustrating the inherent correlation among surface patterns, self-reinforcement, and repetitive generation.
By devising complex self-reinforcement templates, we study their crucial roles as surface pattern guides within text generation.
We empirically establish the existence of the \emph{token co-occurrence reinforcement}, where the connection between any two tokens gets reinforced with the number of contextual co-occurrences, a primary principle in learning surface-level patterns. 

Furthermore, we scrutinize the beneficial and detrimental effects of these surface patterns on in-context learning.
On the one hand, our experiments on MMLU and GSM8K show that the reinforcement helps constrain the output space and format outputs to follow demonstrations like outputting `Let's think step by step.'. On the other hand, our experiments with non-informative connections and reordered answers in MMLU demonstrate that intentionally constructed connections make LLMs lean towards specific answers, revealing the risk of unintended, spurious connections. 
Our work not only reveals the intrinsic workings of in-context learning to some extent, providing a perspective not analyzed in the literature but also explains the underlying reasons for the failure of in-context learning\footnote{Our code can be found at \url{https://github.com/ElliottYan/understand-icl-from-repetition}}.


% Furthermore, we scrutinized the beneficial and detrimental effects of these surface patterns on in-context learning.
% On the one hand, our experiments on MMLU and GSM8K show that the reinforcement helps guide or constrain the output space and format outputs to follow patterns like `Let's think step by step.'. On the other hand, we build non-informative connections and reordered answers in MMLU to demonstrate that manually constructed connections make LLMs lean towards specific answers, revealing the potental risk of unintended, spurious connections. 
% Our work not only reveals some intrinsic workings of in-context learning, providing a perspective not analyzed in the literature, but also explains the underlying reasons for the failure of in-context learning.

% Simultaneously, it has been pointed out that language models often prone to the issue of repetitive text generation~\citep{holtzman2019curious,fu2020theoretical,xu2022learning}. This issue persists like a stubborn hurdle in natural language processing, even with large language models. At the heart of this problem is the `self-reinforcement effect'~\cite{xu2022learning}, a phenomenon where the model tends to perpetuate the generation of sentences that have frequently appeared in its context. 


% Figure \ref{fig:examples} presents a successful case and a failed case of in-context learning. 
% Both cases involve the model understanding associations between tokens, effectively learning from in-context, and finally making decisions based on learned connections.
% However, key differences lie in which connection the model has learned.  
% The case of success is characterized by the discernment of specific, desired connections. 
% For example, format tokens like `Input:' and `Answer:' are used to delineate and shape the output space, directing the model towards specific outcomes. 
% Conversely, in the failed case, LLMs identify more arbitrary, or \emph{spurious}, connections. 
% These connections highlight the risk of `overfitting', even though there is no real optimization involved, and these spurious connections can result in a model that performs poorly on new, unseen data.
% \emph{The ability of learning from in-context is a double-edged sword.}

% In this work, we put forth an surprising link between two phenomena: the in-context learning capability of LLMs and the effect of repetitive generation. The association is notably unexpected due to the contradictory characteristics of these phenomena. While in-context learning is a valued characteristic that improves the model's capacity to produce context-appropriate responses, repetitive generation, which may be a byproduct of the same process, leads to unwanted degenerate text
% \elliott{here we should contrast prevous work.}
% The disparities discussed above echo the optimization process in conventional machine learning tasks, where the in-context learning demonstrations correspond to a balanced dataset, while those of repetitive generation align with an imbalanced and homogeneous dataset.
% In this paper, we claim that learning from in-context is a double-edged sword.
% We propose a novel perspective to understand both in-context learning and repetitive generation via analogy. 
% Both examples in Figure \ref{fig:examples} can be viewed as a learning process.
% Following the ``recall, match, and learn'' procedure, it becomes evident that both cases employ learned connections from their respective demonstrations. 
% However, the quality of these connections profoundly influences the results. Connections within in-context learning are robust and effective, whereas those in repetitive generation are paradoxical and spurious, as illustrated when 'Answer' yields 'Positive'. 
% These disparities echo the optimization process in conventional machine learning tasks, where the in-context learning demonstrations correspond to a balanced dataset, while those of repetitive generation align with an imbalanced and homogeneous dataset.


% \elliott{should emphasize that}
% Figure \ref{fig:examples} presents the two processes: in-context learning and repetitive generation. 
% Both processes involve the model understanding associations between tokens, effectively learning by analogy, and finally making decisions based on learned connections.
% However, key differences lie in which connection the model has learned.  
% In-context learning is characterized by the discernment of specific, desired connections. 
% For example, format tokens like `Input:' and `Answer:' are used to delineate and shape the output space, directing the model towards specific outcomes. 
% Conversely, the repetitive generation often results in the model identifying more arbitrary, or \emph{spurious}, connections, potentially leading the model astray from desired learning pathways. 
% The learned connections of repetitive generation highlight the risk of `overfitting', even though there is no real optimization involved, and these spurious connections can result in a model that performs poorly on new, unseen data.
% leading to an over-reliance on these surface patterns. 
% and the probability increases monotonically with the number of the co-occurrences.

% In the above cases, we see that the surface similarities of the demonstrations introduce spurious connections, causing the model to overlook the intrinsic patterns within the data and learn solely from surface-level connections. 
% Approaching from this issue, we quantitatively investigate in-context learning from the perspective of surface patterns, illustrating the inherent correlation between surface patterns, self-reinforcement~\citep{xu2022learning}, and notorious repeated generation~\citep{holtzman2019curious,fu2020theoretical}.
% By devising complex self-reinforcement templates, we studying their crucial roles as surface patterns guides within text generation.
% We discover the existence of the \emph{token co-occurrence reinforcement} -- the connection between any two tokens gets reinforced with the number of contextual co-occurrence, the first principle to learning of surface-level patterns. 


% How does the model choose which connection to learn? 
% We start from the repetitive side and dig into the first principles of the self-reinforcement effect. 
% We first use a synthetic dataset to conduct the analyses of self-reinforcement effect on current LLMs, i.e., LLaMA and OPT, confirming the existence of self-reinforcement in these models. 
% Then, we delve into the fine grain of self-reinforcement.
% From repetitive phrases to tokens, we inject random perturbations in between to isolate reinforcement effect of given granularities. 
% From our findings, we empirically prove the existence of the \emph{token co-occurrence reinforcement} -- the connection between any two tokens gets reinforced with contextual co-occurrence, and the probability increases monotonically with the number of the co-occurrences.


% discuss the root of these two phenomena
% We posit that the two phenomena, both driven by the analogy, are affected by the same mechanism of language models. \chenming{what mechanism, should we give the answer here? or be more specific, are both affected  by the xxx mechanism of language models}
% To validate our claim, 

% Then, we

% The generality of token co-occurrence reinforcement demonstrates that learning by analogy is a double-edged sword in language modeling. Except for leading to both in-context learning and repetitive generation, we further demonstrate even in in-context learning itself, the reinforcement both produces desired outcomes and undesired outcomes. Specifically, on one hand, we show it helps limiting the output space and formatting outputs to follow patterns like `Let's think step by step.'. On the other hand, it as well make ICL susceptible to manually constructed spurious connections. 


% We posit that the two phenomena, both driven by the analogy, are affected by the same mechanism of language models. \chenming{what mechanism, should we give the answer here? or be more specific, are both affected  by the xxx mechanism of language models}
% To validate our claim, we first use a synthetic dataset to conduct the analyses of self-reinforcement effect on contemporary LLMs\chenming{why  contemporay?}, i.e., LLaMA and OPT, confirming the existence of self-reinforcement in these models. 
% In order to investigate the first principle\jin{what is first principle?}, we delve into the fine grain of self-reinforcement.
% From repetitive phrases to tokens, we inject random perturbations in between to isolate reinforcement of given granularities. 
% From our findings, we identify \chenming{identify is too weak. maybe propose a method to identify, or prove the existing of ...} the \textbf{token co-occurrence reinforcement} -- {the connection between any two tokens gets reinforced with contextual co-occurrence, and the probability increases monotonically with the number of the co-occurrences.}  \chenming{don't use texit}



% Our contributions can be summarized as follows:
% \begin{itemize}
%   \item We propose a novel view of in-context learning as a double-edged sword and analyze in-context learning from complex surface connections. 
%   \item We quantitatively analyze surface co-occurrence patterns of demonstrations with progressive granularities, and empirically find the token co-occurrence reinforcement. 
%   \item We demonstrate the double-edged sword of token co-occurrence on in-context learning, where it brings both beneficial effects, e.g., directing output space and learning to follow patterns, and detrimental effects, i.e., proning to spurious connections. 
% \end{itemize}

% \chenming{Maybe use bullet points to emphasize the contributions one by one.}

% Based on our findings, we further demonstrate how  affects real-world in-context learning scenarios. 

% Finally, we show by desired and undesired outcomes how in-context learning is affected by the token co-occurence reinforcement. 

% We investigate this mechanism by first proving the existence of self-reinforcement effect in contemporary LLMs, then digging into the finer-graine of self-reinforcement effect, and finally revealing how this mechanism affects in-context learning. 


% This seemingly contradictory relationship between a valuable capability (ICL) and a persistent problem (self-reinforcement effect) in the same mechanism unveils a complex facet of large language models. Therefore, it presents a fascinating and crucial area for further research.

% What makes this connection particularly surprising is the paradoxical nature of it. On one hand, in-context learning is hailed as a desirable trait, enhancing the model's ability to generate contextually relevant responses. On the other hand, the self-reinforcement effect, potentially a by-product of the same mechanism, leads to an undesirable outcome – repetitive text. This unexpected intertwining of a valuable capability and a persistent issue in the same process uncovers a complex layer of large language models, presenting an intriguing and critical area for further exploration.


% We present solid and extensive experiments to support this hypothesis.  


% On the other hand, degenerate text from language models, especially repeated text, has always been one of the main focuses of text generation literature. 
% The self-reinforcement effect, recently introduced by \citep{}, could be the root cause for the repeating issue. 
% It describes a phenomenon that language models increase the probability when encountering the same sentence time after time. Due to this effect, the generated text receives even higher probabilities the next time, and it finally leads to repeating.   

% acts as a double-edged sword in in-context learning. 
% The resemblance is shown in Figure 1. 
% We provide extensive supporting evidence that the self-reinforcement effect 
% To validate our hypothesis, we first extend the self-reinforcement effect. 
% From language models to LLMs, we show that the current state-of-the-art LLMs like LaMMA or Alpaca retain the self-reinforcement effect and the effect even becomes stronger when scaling the model size. 

% We show that even with interfering content in between, this effect still occurs with successive phrases. 
% By doing so, we connect self-reinforcement with the formatting ability in ICL. 

% Deep reinforcement and shallow reinforcement. 


\section{ICL and Reptitions}
\label{sec:preliminary}
% \paragraph*{Motivating Example}
% We propose a novel perspective to understand both in-context learning and repetitive generation via analogy. 
% % By viewing both examples in Figure \ref{fig:examples} as a learning process from the demonstrations, the true differences emerges. 
% Both examples in Figure \ref{fig:examples} can be viewed as a learning process.
% Following the ``recall, match, and learn'' procedure, it becomes evident that both cases employ learned connections from their respective demonstrations. 
% However, the quality of these connections profoundly influences the results. Connections within in-context learning are robust and effective, whereas those in repetitive generation are paradoxical and spurious, as illustrated when 'Answer' yields 'Positive'. 
% These disparities echo the optimization process in conventional machine learning tasks, where the in-context learning demonstrations correspond to a balanced dataset, while those of repetitive generation align with an imbalanced and homogeneous dataset.

% With this perspective, we effectively unify ICL and repetitive generation into the same process of learning by analogy. 
% This perspective enables us to coalesce in-context learning and repetitive generation into a singular process of learning by analogy.
% Yet, the question of how the model learns from analogy remains nebulous.
% How does the model learn from analogy? 
% After establishing the existence of self-reinforcement in LLMs, 
% Thus, we first dig into the self-reinforcement effect to uncover the fundamental principle of learning by analogy. Then, we show how this principle explains and affects the behavior of in-context learning.
% We propose to study the following two unkowns:
% \begin{itemize}
    % \item RQ1: How does self-reinforcement effect work/contribute in/to in-context learning? (\elliott{Improve this RQ.})
    % \item RQ2: As a result, how does this affect real-world in-context learning?
% \end{itemize}

% Next, we present the preliminaries.


% By examining the connections we can learn from the demonstrations, 

% what do we learn
% By viewing the decision making of 

% From the demonstrations of in-context learning scenario, we can 



% Exploring this hypothesis\chenming{no formal definition of the hypothesis?} is both compelling and significant for two reasons: (1) it links the in-context learning, arguably the most intriguing feature of large language models, with the annoying issue of repetitions lots of approaches dedicated to resolving to date back to the age of LSTMs. (2) It could expose a fundamental mechanism governing the language modeling task/model.



% JIANHAO: We should first discuss the motivation about why we cares the connection and why these would be interesting. 

% 1. What motivates us to investigate these two? 
% 2. How interesting is the connection? How would such an investigation be important to us?



% In this section, we briefly discuss the motivation why we draw connections between these two phenomenons and 
% In this section, we discuss the similarity and difference between in-context learning and repetitive generation. 

% (Need abstract summarization)


% Next, we briefly introduce the formulation of two scenarios. 
% \paragraph*{In-Context Learning}
In ICL, given a desired task $f$, we feed an LLM with $K$ demonstrations $\{(x_k, y_k), n\in[1,K]\}$, where $y_k=f(x_k)$. Here, $y_k$ can be a label word or a free text phrase.
% , and $x_k$ is a test query. 
Each demonstration $d^k=(\mathbf{F_I}, x_k, \mathbf{F_O}, y_k)$ can be divided into four parts. 
$\mathbf{F_I}$ and $\mathbf{F_O}$ denote formatting tokens for inputs and outputs, e.g., \emph{`Input:'} and \emph{`Answer:'}. 
% $x_k$ and $y_k$ are the input and output for each demonstration. 
Note that $x_k$ and $y_k$ can consist of several tokens. 

A pretrained language model $\mathcal{M}$ predicts the output $y$ conditioned on the concatenation of both demonstrations and the test query $x$,
\begin{gather}
    \mathcal{P}_{\text{ICL}}(y|x,k) \coloneqq \mathcal{M}(y|(\mathbf{F_I}, x_1, \mathbf{F_O}, y_1, \cdots, \mathbf{F_I}, x_k, \mathbf{F_O}, y_k, \mathbf{F_I}, x, \mathbf{F_O})).
    \label{eq:icl}
\end{gather}
% Here, $\mathbf{F_I}$ and $\mathbf{F_O}$ are formating tokens that split each part in the concatenation, e.g., \emph{`Input:'} and \emph{`Output:'}.
% There are a variety of work trying to explain the working mechanism of in-context learning. 



% introduce repetitive generation.

% \begin{wrapfigure}{l}{0.5\textwidth}
%   \centering
%   \includegraphics[width=\linewidth]{figs/example_self_reinforce.pdf}
%   \caption{
%     % Manually repeat a given sentence several times, feed to the model and observe the same
%   % token's probability (in {\color{red}red}). 
%   }
%   % \vspace{-20pt}
%   % \label{fig:example}
% \end{wrapfigure}
\begin{figure}[t]
\centering
\begin{minipage}[b]{0.43\textwidth}
  \centering
  \includegraphics[width=\textwidth]{figs/example_self_reinforce.pdf}
  % \caption{}
  % \label{fig:sr_example}
\end{minipage}%
\hfill
\begin{minipage}[b]{0.43\textwidth}
  \centering
  \includegraphics[width=\textwidth]{figs/sentence-self-reinforce.pdf}
  % \caption{}
  % \label{fig:figure1}
\end{minipage}
\caption{\emph{Left: }\textbf{An example of the self-reinforcement effect.}  We choose a normal sentence (`Answer is {\color{red}A}'), repeat it several times, and present the probability of the token {\color{red}`A'}. The model used is LLaMA-7B. \emph{Right: }\textbf{Sentence-level self-reinforcement over LLMs.} We plot all sizes of OPT and LLaMA with colors from light to dark. All sizes of LLaMA and OPT models demonstrate strong sentence-level self-reinforcement effects. }
% \vspace{-15pt}
\label{fig:sentence-self-reinforce}
\end{figure}


To further understand the influence of surface repetitions from Figure \ref{fig:examples}(b), we show a case of sentence repetition and plots the probability of the sentence as the number of repetition in the previous context increases (Figure \ref{fig:sentence-self-reinforce}).
When we manually repeat the sentence `Answer is A', the probability of generating `A' after `Answer is' gets boosted from 0.03 to almost 1.0. 

The right part of Figure \ref{fig:sentence-self-reinforce} demonstrates our preliminary study with two families of large language models quantitatively, namely OPT~\citep{zhang2022opt} and LLaMA~\citep{touvron2023llama}. 
% With the colors from light to dark, we plot the average sentence probability after 10 repeats for LLMs from smaller sizes to larger sizes
We plot the average sentence probability after 10 repeats for LLMs of varying sizes, arranged by light to dark colors.
We use 1,000 sentences from each of the three different datasets -- Wikitext-103~\citep{merity2016pointer}, BookCorpus~\citep{Zhu_2015_ICCV}, and sequences of random words. More experimental details can be found in Appendix \ref{sec:exp_detail}.
% We can see that, with 10 repeats, the probability of generating the sentence is significantly boosted, for all tested LLMs. 
With 10 repeats, the probability of generating the sentence is significantly increased across all tested LLMs.
% The existence of self-reinforcement effect across all LLaMA and OPT models is clear and definite\chenming{The self-reinforcement effect is evident in all LLaMA and OPT models.}
% This finding reconcile previous work on the existence of self-reinforcement effect on LLMs. 
% Intriguingly, we observe phenomena that are different from previous work. 
Current LLMs amplify the occurrence of previously presented sentences, even \emph{sequences of random tokens.}\footnote{\cite{xu2022learning} reported that sentences with a low initial probability — such as sentences composed of random tokens — have a smaller self-reinforcement effect.
In our experiments, even sentences with random tokens (which initially have a near-zero probability) become reinforced to a probability nearing one. 
The difference may come from different model sizes (150M vs. maximum 65B) and pretrained corpus size (hundred millions of tokens vs. trillions of tokens). }

% \paragraph*{Repetitive Generation}
The above observations are related to the study of \emph{self-reinforcement effect}~\citep{xu2022learning} in literature. 
% In self-reinforcement, 
Formally, the conditional probability we model here is $\mathcal{P}_{\text{REP}}(w) \coloneqq \mathcal{M}(w|[s^1;s^2;\cdots;s^{n-1};w_1 \cdots w_{i-1}])$, where $s$ is the repeating sentence, $n$ denotes the number of occurrences, and $w_i$ is the $i$-th token in the sentence $s$.
Previous research finds the self-reinforcement effect, where the probability of generating the sentence $s$ of length $L_s$ occurred $N$ times in context, $\text{TP}_N = \frac{1}{L_s}\sum_{i}\mathcal{P}_{\text{REP}}(w|w=w_i; n=N)$, almost monotonically increases with the number of $N$. 
Apparently, the generation of repetitive sentences can be understood as the influence of a single surface feature. It does not fully reveal the influence of more sophisticated features. 
Consequently, we investigate more sophisticated surface patterns and their influence on in-context learning. 
% \chenming{Consequently, our work aims to further investigate more sophisticated surface patterns and how they impact learning within a given context.}


% \chenming{what is the purpose of this paragraph? You just explained the preliminaries and then right away jump to an example.} 
% describe the experimental results. 
% Across all four LLaMA models and three datasets, the probability of a sentence soars with the increasing number of reptitions. 
% The trend of steadily increasing probabilities with
% The existence of self-reinforcement effect across all LLaMA models is clear and definite. 
% This clearly and definitively establishes the persistence of the self-reinforcement effect across all LLaMA models.

% The study conducted by \cite{xu2022learning} is on the self-reinforcement effect in a 16-layer Transformer decoder-only model (approximately 150 million trainable parameters) trained on the Wikitext-103 corpus (approximately 100 million tokens). The scale of both the model and the training corpus are markedly different from those of current LLMs, which typically range from several billion to several hundred billion parameters and are trained on trillions of tokens. 
% It is a necessity to replicate the experiments with contemporary LLMs.
% These differences possibly lead to different performances.

% In essence, contemporary LLMs amplify the occurrence of any previously presented sentence.



% \subsection{Confirm of Existence}


% We define the formulation for two settings:
% \begin{align}
    % P_{\text{ICL}}(y) = \text{Model}([\mathbf{F_I}; x_1; \mathbf{F_O}; y_1; \cdots; \mathbf{F_I}; x_k; \mathbf{F_O}; y_k \cdots; \mathbf{F_I}; x_K; \mathbf{F_O}]) \\
    % P_{\text{REP}}(y) = \text{Model}([\mathbf{F_I}; x_1; \mathbf{F_O}; y_1; \cdots; \mathbf{F_I}; x_1; \mathbf{F_O}; y_1; \cdots; \mathbf{F_I}; x_1; \mathbf{F_O}])
% \end{align}

% In previous section, we establish that 
% If a similar mechanism also holds for repeating formatting patterns, it would explain some key characteristics of in-context learning, such as limiting output space or learning to follow examplars' formats. 

% \paragraph*{Importance}


% \elliott{improve the motivation part}
% At first glance, this appears to be a simple case of repetition. 
% However, if we delve deeper\chenming{how?}, we can see that the repetition is more than a mere redundancy; it's a reflection of the model's capacity for learning with analogy.\jin{Changed to `different from xxx focusing a redundancy xx, we delve deeper to xxx (phrase,token analogy).'}
% Despite we know the example is drawn from the sentiment analysis, the underlying function of repetitive demonstrations could be if the input is "Circulation revenue has increased by 5\% in Finland.", output "Positive".
% Repetition is an over-specification of functions. 
% This way of thinking motivates us to investigate the mechanism behind two phenomena. 
% Moreover, it reveals\chenming{too simple?} that reasoning with analogy is a double-edge sword, which we will discuss detailedly in Section \ref{sec:real_world}.\jin{The motivation part is weak. Should be improved.}


% \cite{xu2022learning} prove that the repeated occurrence of the same sentence increases the probability of its next occurrence, an illustration of the self-reinforcement effect. 
% If this principle also applies to repeated formatting patterns, it could elucidate key attributes of in-context learning, such as the constriction of output space or the adherence to the demonstrations' formats.


% why investigating this is important?


% write some of the intuition and motivation.
% In this section, we demonstrate the surprising link between in-context learning and the self-reinforcement effect. 
% We hypothesize that 

% The two research questions mentioned above have a progressive relationship, with each question building upon the previous one.

% \section{Existence of Self-Reinforcement Effect}


% \begin{figure}[t!]
%   \centering
%   \includegraphics[width=1.0\textwidth]{figs/sentence-self-reinforce.pdf}
% \caption{}
% \label{fig:sentence-reinforce}
% \end{figure}


% In this section, we investigate RQ1: \emph{Does the self-reinforcement effect still exist in current large language models?} This research question serves as the foundation of subsequent research, given that the cability for in-context learning is predominantly manifested in LLMs.




% \section{How does self-reinforcement effect work/contribute in/to in-context learning?}
\section{Self-Reinforced Suraface Features for In-context Learning}
% \section{How does surface patterns contribute to in-context learning?}

% Building upon the existence in LLMs, we further investigate RQ1: \emph{Can we extend the current form of self-reinforcement effect to analyze in-context learning?} \elliott{RQ and section title are not appropriate. Here, we should more focus on repetitive generation not ICL.}

In accordance with Figure \ref{fig:examples}, we set $s=[\mathbf{F_I}; x_1; \mathbf{F_O}; y_1]$ and have,
\begin{gather*}
  \mathcal{P}_{\text{REP}}(w|n=K) = \mathcal{M}(y|(\overbrace{\mathbf{F_I}, x_1, \mathbf{F_O}, y_1, \cdots, 
  \mathbf{F_I}, x_1, \mathbf{F_O}, y_1}^{\text{K times}}, \mathbf{F_I}, x_1, \mathbf{F_O})). \\
  \mathcal{P}_{\text{ICL}}(y|x,k=K) = \mathcal{M}(y|(\mathbf{F_I}, x_1, \mathbf{F_O}, y_1, \cdots, \mathbf{F_I}, x_K, \mathbf{F_O}, y_K, \mathbf{F_I}, x, \mathbf{F_O})).
  \label{eq:rep}
\end{gather*}

% similarity
Comparing $\mathcal{P}_{\text{REP}}(w)$ to $\mathcal{P}_{\text{ICL}}(y)$, we find: (1) $\mathbf{F_I}$ and $\mathbf{F_O}$ are both repeated across demonstrations; (2) In repetitive generation, $x_1$ and $y_1$ are repeated, while in ICL, $x$ and $y$ are changing. 

% To study the connection between surface patterns and the final generated answer $y$, we progressively extend self-reinforcement patterns toward in-context learning. \chenming{
To investigate the correlation between surface patterns and the resulting answer $y$, we gradually expand self-reinforcement patterns toward in-context learning.
We achieve this by introducing random perturbations to each demonstration, imitating the role of changing $x$ while leaving certain components, such as $\mathbf{F_O}$ and $y$, unchanged.

The experiments in this section are conducted over the dataset of randomly generated sentences as in Section \ref{sec:preliminary} and with the four LLaMA models. The results on Wikitext-103 and BookCorpus, and results with OPT models can be found in Appendix \ref{sec:support_exps}. For each experiment, we repeat the pattern 20 times and report the mean and standard deviation of the probabilities for the kept tokens. 
% We aim to study the connection between surface patterns and the final generated answer $y$. 
% Therefore, we first study the surface connection between $\mathbf{F_O}$ and $y$. 
% Therefore, we dig further into self-reinforcement effect and claim that the effect can be extended to scenarios that are close to in-context learning. 
% We achieve this by first proving random perturbations across each demonstration do not break the reinforcement loop, then identifying distant and successive reinforcement between any two tokens. 
% \elliott{modify these two sentences.}

% NOTE: maybe we should discuss scaling?

% \subsection{From Sentence to Suffix}
\subsection{Self-Reinforcement Retains with Random Perturbations}

\begin{figure}[t!]
  \centering
  \includegraphics[width=1.0\textwidth]{figs/suffix-self-reinforce.pdf}
\caption{\textbf{Phrase co-occurrence reinforcement.} When disrupting the reinforcement loop with random prefixs, the self-reinforcement effect still retains. (A)-(C) plot the scaling of LLaMA models with a certain phrase length (e.g., ``XY'' denotes length of 2). (D) plots LLaMA-65B's probability varying phrase length. The gray shadow denotes the standard deviation across 1,000 samples.}
\label{fig:suffix-reinforce}
\end{figure}

% Recall our examples in Figure \ref{fig:examples}. The core distinction between the ICL and the repetitive generation is that ICL presents variability in both the input and output across demonstrations, thereby disrupting the sentence-level loop of self-reinforcement. 

% To bridge the gap, we 
% We first extend the self-reinforcement analyses to repeated and discontinuous phrases.
We first discuss repeated and discontinuous phrases in this section. 
Specifically, while maintaining a phrase repeated across demonstrations, we randomly substitute other tokens with random tokens drawn from the vocabulary. 
Without loss of generality, we place the phrase at the end of each demonstration. 

Formally, given a sentence $s=(w_{1}, \cdots, w_{L_{\mathbf{s}}})$ from a corpus $D$, we construct a binary mask sequence $m=(\overbrace{0,\cdots,0}^{L_s-L_p},\overbrace{1,\cdots,1}^{L_p})$, where $L_p$ is the length of the kept phrase, and we define a replacement operation $\mathbf{R}(w, m)=\begin{cases}
  w_r,& \text{if } m = 0\\
  w, & \text{if } m = 1
\end{cases}$ that replaces $w$ with a randomly sampled token $w_r$ from the vocabulary if $m=0$ and keep it unchanged when $m=1$. Note that $w_r$ is different for each sentence and each position. 
Then, we can define a randomly perturbed sentence $\tilde{s}=(\mathbf{R}(w_1, m_1), \cdots, \mathbf{R}(w_{L_s}, m_{L_s}))$. Effectively, we keep a phrase of length $L_p$ unchanged for each sentence, and replace other tokens with random tokens. 
We compute $\mathcal{P}_{\text{REP-P}}(w) = \mathcal{M}(w|[\tilde{s}^1;\tilde{s}^2;\cdots;\tilde{s}^{n-1};\mathbf{R}(w_{1},m_1),\cdots,\mathbf{R}(w_{i-1}, m_{i-1})])$
and the average token probability that only considers the kept tokens $\tilde{\text{TP}}_N=\frac{1}{L_p}\sum_{i=L_s-L_p+1}^{L_{s}}{\mathcal{P_{\text{REP-P}}}(w|w=w_i, n=N)} \times m_i$. 
As a concrete example, we take a sentence $s=(\text{Apple}, \text{is}, \text{red})$ and $m=(0, 1, 1)$, and the target token $w=\text{red}$. Then, the demonstrations discussed will be like `Apple {\color{red}is red} // Orange {\color{red}is red} // Banana {\color{red}is red}'. The pattern we kept unchanged is in color {\color{red}red}.
In the context of in-context learning, this pattern corresponds to demonstrations like `...{\color{red}Answer: D}' or `...{\color{red}Response: Positive}'.
% Then, the 
% this pattern corresponds to demonstrations like `\emph{...Answer: D}'.
% Formally, given a sentence $s=(w_{1}, \cdots, w_{L_{\mathbf{s}}})$ from a corpus $D$, we construct ${\tilde{s}^n} = (\mathbf{R}_n(w_{1}), \cdots, \mathbf{R}_n(w_{L_s-L_p}), w_{L_s-L_p+1}, \cdots, w_{L_s})$. 
% $\mathbf{R}(\cdot)$ denotes a replacement with a randomly sampled token from the vocabulary and $L_p$ is the length of the kept phrase. Then, we construct a sequence of these sentences $(\tilde{s}^1, \tilde{s}^2, \cdots, \tilde{s}^n)$. 
% In this manner, each demonstration retains a common suffix, but is distinguished by a unique prefix, which more closely aligns with the ICL approach by allowing for variations in inputs across demonstrations.
% \begin{align}
%   \mathcal{P}_{\text{REP-P}}(y) = \mathcal{M}(y|(\mathbf{F_I}, x_1, \mathbf{F_O}, y_1, \cdots, \mathbf{F_I}, x_k, \mathbf{F_O}, y_1 \cdots, \mathbf{F_I}, x_K, \mathbf{F_O}))
% \end{align}


Figure \ref{fig:suffix-reinforce} depicts our experimental results for repeated and discontinuous phrases. We use randomly generated sentences here. We see that disrupting the sentence-level loop with random prefixes does not break the self-reinforcement effect. 
The effect persists across various LLaMA models.
% The effect persists across various LLaMA models and our results confirm a further link to ICL. 
% Translating to the context of in-context learning, output formatting tokens like `Output:' and the successive label word like `A' in demonstrations will form a self-reinforcement phrase, and thus increase the probability of outputting `A' in the test example. 
% \emph{It is both limiting the output space and injecting bias in label selection.}
In addition, we observe a stronger reinforcement when increasing the phrase length. More tokens are kept unchanged, higher probability the model assigns to the phrase. 
Particularly, Figure \ref{fig:suffix-reinforce}(D) serves as an ablation study, as each line progressively adds one more unchanged token. 
A repeating phrase of `...XYZ' gets reinforced more compared to the phrase `...XY', indicating that each repetitive token bolsters its subsequent tokens.
% providing valuable insight for future examination of token-level effects.
% NOTE: what is the meaning of this? What is the implication of this ?
% add an example to describe the effect in real-world ICL
% e.g., Answer: -> A


\subsection{Any Two Tokens Form a Token Reinforced Loop}
\label{sec:token-reinforce}

\begin{figure}[t!]
  \centering
  \includegraphics[width=1.0\textwidth]{figs/token-self-reinforce.pdf}
  \vspace{-15pt}
  \caption{\textbf{Token co-occurrence reinforcement.} Even if only one token repeats in context, the self-reinforcement loop triggers. ``..X..Y..'' denotes 2 tokens are kept unchanged. The mean and variance are computed over 1,000 randomly generated samples. }
\label{fig:token-reinforce}
\vspace{-10pt}
\end{figure}

% Thus far, we have already extended self-reinforcement from sentence-level to phrase-level. 
% Next, we go smaller in granularity and study tokens. 
Inspired by our results above, our focus now turns to an even finer level of detail, examining individual tokens. By assessing the self-reinforcement effect among tokens, we understand the self-reinforcement effect from first principles. 
% How is this connected to ICL?

We expand our scope from phrases to any tokens found within a sentence. 
In particular, we stop requiring the kept tokens to be continuous and randomly sample positions to put the 0s and 1s of the mask sequence $\hat{m}$ and control the number of kept tokens $L_t$. 
% In particular, we stop require the kept tokens to be continuous and randomly choose positions to put 0s and 1s in the mask sequence
In this way, a sentence $s$ is transformed into $\hat{s^n}=(\mathbf{R}(w_1, \hat{m}_1), \cdots, \mathbf{R}(w_{L_s}, \hat{m}_{L_s}))$, where $\sum_{l\in[1,L_s]} \hat{m}_l = L_t$.
% In each repetition, $m$ is individually sampled. 
Then, we report the average token probability $\hat{\text{TP}}_N$ as in the previous section. 
Suppose we have a sentence $s=(\text{Apple}, \text{there}, \text{is}, \text{red})$ and $m=(1, 0, 1, 1)$, and the target token $w=\text{red}$. 
Then, the demonstrations in this section will be like `{\color{red}Apple} there {\color{red}is red} // {\color{red}Apple} logo {\color{red}is red} // {\color{red}Apple} juice {\color{red}is red}'. 
% The pattern we kept unchanged is in color {\color{red}red}. 

% We compute $\mathcal{P}_{\text{REP-P}}(w) = \mathcal{M}(w|[\tilde{s}^1;\tilde{s}^2;\cdots;\tilde{s}^{n-1};\mathbf{R}(w_{1},m^n_1),\cdots,\mathbf{R}(w_{i-1}, m^n_{i-1})])$
% and the average token probability that only considers the kept tokens $\tilde{\text{TP}}_N=\frac{1}{L_p}\sum_{i=L_s-L_p+1}^{L_{s}}{\mathcal{P_{\text{REP-P}}}(w|w=w_i, n=N)} \times m_i$. 

% In particular, we construct a sentence ${\hat{s}^n} = (\mathbf{\hat{R}}(w_{n,1}), \cdots, \mathbf{\hat{R}}(w_{n,L_{{s}}}))$. 
% $\mathbf{\hat{R}}(\cdot)$ denotes a function of either keeping the token unchanged or replacing the token with a replacement operation, $\mathbf{R}(\cdot)$, the same as in the previous section. 
% For each sentence, we randomly select tokens to remain unaltered, while controlling the number of preserved tokens and the distance between them.

\paragraph{Number of Tokens} As depicted in Figure \ref{fig:token-reinforce}, even only one single token shared across demonstrations elicits self-reinforcement. 
% discuss compositional increases of such effect.
% As we increase the number of preserved tokens from 1 to 4, we observe a strengthening of the reinforcement effect.
% \elliott{add more illustrations}
We are particularly interested in the scenario with two tokens kept unchanged, as it reveals a fundamental rule of one token triggers the generation of the other one. 
We find that \emph{the connection between any two tokens gets reinforced and the probability increases monotonically with the number of their contextual co-occurrences.}
We refer to this base effect as the token co-occurrence reinforcement. 
When we increase the number of preserved tokens from 2 to 4, we observe a strengthening of the reinforcement effect. 
This is because each former token forms a reinforced connection with all the latter ones. 
% this example is not aligned with increased length of probs.
% Polish this example. Make it more intriguing.

% This suggests that even formatting tokens like \emph{`Input:'}, which is distant from the label word, can boost output probability.

\paragraph{Distance In-Between}
\begin{figure}[t!]
  \centering
  \includegraphics[width=1.0\textwidth]{figs/token-distance-self-reinforce.pdf}
  \vspace{-15pt}
\caption{\textbf{Successive and distant reinforcement.} The self-reinforcement effect is the strongest when two tokens are successive, i.e., distance=0. Otherwise, the reinforcement is smaller and appears insensitive to the distance. ``..X.Y..'' denotes the distance between two tokens is 1. }
\label{fig:token-distance-reinforce}
\vspace{-15pt}
\end{figure}

% Furthermore, we study how distance affects the self-reinforcement effect between tokens. Here, we constrain experiments to two tokens as it is the base scenario.
We further examine the role of distance in token reinforcement. This analysis is confined to two tokens. Figure \ref{fig:token-distance-reinforce} distinctly differentiates between successive tokens (distance$=0$) and distant tokens (distance$>=1$), which we term as \emph{successive reinforcement} and \emph{distant reinforcement}, respectively.
The successive reinforcement significantly elevates the probability, from 0 to 0.4, with only several repetitions. Conversely, the distant reinforcement provides a moderate boost to the probability, from 0 to 0.2, and appears to be indifferent to the distance between tokens.

% \paragraph*{}

% Figure \ref{fig:token-distance-reinforce} demonstrate that 
% \paragraph{Discussion} 
% \elliott{add more discussion about the mechanism of self-reinforcement and how it connects to ICL.}
% In all experiments, we notice stronger self-reinforcement effects when scaling the model size. 
% This monotonic increases indicate that the effect is non-trivial and is potentially a fundamental mechanism in LLMs.  
Across all experiments, we observe a marked increase in reinforcement as model size scales, especially in the distant token reinforcement. 
This consistent escalation suggests that larger LLMs are more capable of following complex patterns in in-context demonstrations, which is consistent with results from \cite{wei2023larger}. 
We provide more supporting evidence of token reinforcement in Appendix \ref{sec:support_exps}.

% \paragraph*{Discussion}
The link we found between any two tokens forms the foundation of the sentence-level self-reinforcement effect. Each token is strengthened by the ones before it. 
In in-context learning, common elements in demonstrations, such as pattern words, form connections with label words like "A, B, C, D". These connections are not merely incidental but foundational to the learning process. 
% \elliott{We should add some implications here and verify in the next sections.}
% This structure is key to learning, as it helps link new information to what we already know.



% Further, when we increase the number of kept tokens from 1 to 4, we observe stronger reinforcement effects. 

% Then, we construct a similar sequence of $\mathbf{x = (s'_1, s'_2, \cdots, s'_N)}$ and compute $\text{TP'}(\mathbf{s'_N})=\frac{1}{L_{\mathbf{s}}-L_p}\sum_{l=L_p+1}^{L_{\mathbf{s}}}{\mathcal{P}_{\theta}(x_{n, l}|\mathbf{x}_{<n,l})}$ for each sentence. 

% \section{Double-Edged Sword in Real-world In-Context Learning}
% After extensively studying the characteristics of the self-reinforcement effect, we wonder how it affects real-world applications, aka, RQ3.

% After establishing in-context learning and repetitive generation as two sides of self-reinforcement effect, we hypothesize that self-reinforcement effect also plays a role of double-edged sword even just in in-context learning. 
% In this section, we demonstrate both the good side and the bad side of self-reinforcement effect causes. 

% \subsection{Good Side}
% \subsection{bad Side}

\section{The effects of Surface Patterns to In-Context Learning}
% \section{The Double-Edged Sword of Real-World In-Context Learning}
\label{sec:real_world}
% \jin{At first glance, I thought this part lacked experiments and sufficient statistical evidence. I think, in this section, we should emphasize the results are from statistical calculation on substantial samples thus they are solid results. Then, I thinks we should change some figures to tables.}
% Having conducted an extensive analysis of the self-reinforcement effect, we have turned our attention to its connections with in-context learning, i.e., RQ2. 
% In this section, we provide the desired and the undesired outcomes from token co-occurrence reinforcement. 
% The token co-occurrence reinforcement we found in previous section 
In this section, we quantitatively study how these self-reinforced surface patterns lead to both beneficial functions and detrimental effects in in-context learning.
The experiments in this section are conducted over MMLU~\citep{hendrycks2021measuring} and GSM8K~\citep{cobbe2021training}. 
Due to the limit of computing resources, we randomly sampled 20 samples for each of the 57 tasks of MMLU, resulting in a collection of 1140 test samples. 
The demonstrations are independently drawn for each of the test samples. All experiments are conducted across three random seeds. For further experimental details, we refer our readers to Appendix \ref{sec:exp_detail}.

\subsection{Beneficial Effects}


\begin{figure}[t]
  \centering
  
  \begin{subfigure}[b]{0.48\linewidth}
    \centering
    \includegraphics[width=\linewidth]{figs/mmlu_mask_example.pdf}
    % \caption{}
    % \label{fig:subfigure1}
  \end{subfigure}
  \hfill
  \begin{subfigure}[b]{0.48\linewidth}
    \centering
    \includegraphics[width=\linewidth]{figs/mmlu-output-space.pdf}
    % \caption{}
    % \label{fig:subfigure2}
  \end{subfigure}
  \caption{\emph{Left: }\textbf{An example demonstration from MMLU's high school statistics dataset.} Colors indicate the parts to be masked. \emph{Right: }\textbf{Probability of MMLU's label space. } We find that: (1) Masking question contents and answer contents of the demonstration does not influence directing the label space. (2) Both replacing the option names and the answer indicator significantly hurt the ability to constrain the label space. The gray shadow denotes the standard deviation across three runs. }
  \vspace{-10pt}
  \label{fig:mmlu}
\end{figure}

% \hfill
% \begin{subfigure}[b]{0.32\linewidth}
%   \centering
%   \includegraphics[width=\linewidth]{figs/cot_probs.pdf}
%   \caption{Sub-figure 3}
%   \label{fig:subfigure3}
% \end{subfigure}

\paragraph*{Constraining Output Space.} 
An important advantage brought by the reinforced effect is that it helps constrain the output space --- with several demonstrations, connections between formatting tokens (`Input:' and `Answer:') and label words in each demonstration (`ABCD') are reinforced, through distant and successive reinforcement, respectively. In this way, the LLM learns to predict either one of `ABCD' as the final answer, instead of `Oh, an interesting question. I hope I know the answer.'.

We verify this advantage with the MMLU dataset, which is widely used to evaluate the language understanding of real-world large language models.
To isolate the effect of self-reinforcement, we construct masked demonstrations for analyses. 
An example of how we mask demonstrations is shown in the left part of Figure \ref{fig:mmlu}. 
Particularly, a demonstration in the MMLU dataset can be divided into five parts: question content, option name~(e.g., `A.'), option content~(e.g., `114, 27.35'), answer indicator~(e.g., `Answer:') and final answer~(e.g., `D').  
Based on token reinforcement, we hypothesize that the option names, i.e., "A.", "B.", reinforce outputting "A,B,C,D" via distant reinforcement. The answer indicator, i.e., ``Answer: '', reinforces outputting within label space via successive reinforcement. 
% question indicator would build a successive connection with the final answer, and the repeating option names would build a distant reinforcement with the final answer. 
To validate our hypothesis, we first mask all the questions and option contents in all demonstrations and keep the formatting words, final answer, and test query unchanged. 
Then, we further ablate option names and answer indicators by replacing them with semantically equivalent substitutes. 
More experimental details can be found in the Appendix. 
% We exclude task prompt in MMLU dataset to avoid additional inductive bias. 

% In this way, we ablate in-contetx patterns from input-output mapping
% describe the reason why we did this.

We use LLaMA-65B and plot the probability of choosing "A, B, C, D" as the predicted answer. The results are shown in the right of Figure \ref{fig:mmlu}. 
We find that masking the question and option contents, typically regarded as the most informative parts, does not influence the ability to direct the label space. 
Option names and answer indicators repeated several times in the demonstrations fulfill the job. 
% With only one masked demonstration, LLMs concentrate the output space with 95\% probability mass. 
Our conclusion is further solidified after ablating the option names and answer indicators individually. We see a huge decrease when replacing both option names and answer indicators, validating our hypothesis. 

\paragraph*{Learning to Follow Patterns.}


% \begin{figure}[t]
%   \centering
%   \begin{subfigure}[b]{0.48\linewidth}
%     % \raggedright
%     \includegraphics[width=0.9\linewidth]{figs/gsm8k-output-cot-pattern.pdf}
%     % \caption{}
%     % \label{fig:subfigure1}
%   \end{subfigure}
%   \hfill
%   \begin{subfigure}[b]{0.48\linewidth}
%     % \raggedleft
%     \includegraphics[width=0.9\linewidth]{figs/gsm8k-output-cot-answer.pdf}
%   \end{subfigure}
%   \caption{\emph{Left: }\textbf{Ability to follow patterns from demonstrations. } \emph{Right:} \textbf{After generating CoT prompt, the probability of generating the CoT answer.} }
%   \label{fig:gsm8k}
%   % \vspace{-1pt}
% \end{figure}

\begin{wrapfigure}{l}{0.4\textwidth}
  \centering
  \includegraphics[width=\linewidth]{figs/gsm8k-output-cot-pattern.pdf}
  \caption{\textbf{Ability to follow patterns from demonstrations.}}
  \label{fig:gsm8k}
  \vspace{-15pt}
\end{wrapfigure}

% \elliott{first divide pattern following in GMS8k into two parts.}
Another distinctive feature of in-context learning is to follow the patterns of demonstrations.
This is exemplified in techniques such as the Few-shot Chain-of-thought (CoT) prompting~\citep{wei2022chain}, frequently employed in reasoning tasks of LLMs like the GSM8K~\citep{cobbe2021training}.

Here, we illustrate how the reinforced features in Section \ref{sec:token-reinforce} affect the pattern following of ICL, by showing how LLMs follow the chain-of-thought demonstrations in the GSM8K high school math dataset.
Each demonstration in the dataset follows the form ``Question: [Question] // Let's think step by step. // [CoT Answer]''. 
We demonstrate how models learn to say the CoT pattern, i.e., ``Let's think step by step.''. We further discuss the connection between surface patterns and [CoT Answer] in Appendix \ref{sec:cot_answer}.
% \autoref{sec:cot_answer}.


Based on the findings in previous sections, we hypothesize that the common parts in the demonstrations teach the LLM to generate the CoT pattern. More specifically, `Question:' builds a distant reinforcement, and the new liner `//' builds a successive reinforcement with the CoT pattern. 
% We design similar masked experiments as previously discussed with MMLU. 
We mask out each part in each demonstration \emph{progressively} with random tokens to ablate the influences. 
%  and keep the formattings, the CoT pattern, the CoT answer, and the test query unchanged. 

The probability of the CoT pattern is shown in Figure \ref{fig:gsm8k}. 
After masking out ``//", the probability gains obtained from demonstrations almost diminish, verifying our hypothesis of successive reinforcement.
Another interesting finding is masking [Question] reduces the probability of generating the CoT pattern, indicating the [Question] part to some extent builds a connection with the CoT pattern. 
Since the questions in demonstrations are different but lie in the same group of grad math problems, there might be some common patterns among these questions. 

% \elliott{discuss more about concept generalization}

% \eliiott{mask "//" too in left part. question is normal.}

% \eliiott{maybe we should split the two experiments.}

% \elliott{both left and right parts can prove the generalization.}

% \elliott{it looks like the right part shows the question is not useful.}
% \eliiott{maybe replace "<<" to "(" in some demonstrations, directing outputs to follow a certain format? }

% We present two experiments: (1) How token-reinforcement helps generate the phrase ``Let's think step by step.''; (2) How token connections generalize to concept connections. 
% The results are shown in the left and right parts of Figure \ref{fig:gsm8k}, respectively. 
% First, we observe that the probabilities of both generating CoT pattern and CoT answer are largely improved with demonstrations. This corroborates the success of CoT prompting in previous work. 
% Second, we find after masking out the question in each demonstration, the probabilities of generating CoT pattern and CoT answer are still very high. 

% Our findings verify our hypothesis that learning to follow patterns like ``Let's think step by step'' in demonstrations is driven by token reinforcement, and these patterns further constrain the outputs to a certain output space, ``A B C D'' in previous MMLU experiments and chain-of-thought like answers in GSM8k.

% The results suggest that increasing the number of demonstrations gives a higher probability to follow demonstrations and output "Let's think step by step."




\subsection{Detrimental Effects}

% \begin{figure}[h]
%   \centering
  
%   \begin{subfigure}[b]{0.48\linewidth}
%     \centering
%     \includegraphics[width=\textwidth]{figs/nonsense_exps.pdf}
%     % \caption{}
%     % \label{fig:subfigure1}
%   \end{subfigure}
%   \hfill
%   \begin{subfigure}[b]{0.48\linewidth}
%     \centering
%     \includegraphics[width=\textwidth]{figs/reorder_exps.pdf}
%     % \caption{}
%     % \label{fig:subfigure2}
%   \end{subfigure}
%   \caption{\emph{Left: }\textbf{Effect of Non-Informative Connections~(NC).} The accuracy of D increases with the cost of A, B and C. \emph{Right: }\textbf{Effect of Reordered Answers.} With more reordered demonstrations, the outputs are more leaned towards D.}
%   \label{fig:undesired}
% \end{figure}

\renewcommand{\arraystretch}{1.2} % Increase row spacing

\begin{table}[htbp]
  \small
  \centering
  \caption{\emph{Top:} \textbf{Effect of Non-Informative Connections~(NC).} The accuracy of D increases with the cost of A, B and C. \emph{Bottom: }\textbf{Effect of Reordered Answers.} With more reordered demonstrations, the outputs are more leaned toward D. The delta values on the superscript denote the improvement compared with zero-shot scenarios. `Avg. [A,B,C]' denotes the average accuracy of samples whose golden answer is A, B or C. `D' denotes the accuracy of samples whose golden answer is D. ``$\dagger$": significantly different compared to its corresponding ICL baseline (p < 0.05).}
  \label{tab:undesired}
  \begin{tabular}{c|lllllll}
    \toprule
    % \textbf{Column 1} & \textbf{Column 2} & \textbf{Column 3} \\
    \multicolumn{7}{c}{\em Non-informative Connections} \\
    \hline
    \# Demos & ~~ 0 & ~~1 & ~~2 & ~~3 & ~~4 & ~~5 \\
    \hline
    Avg. [A,B,C] & 63.39 &	63.74\textsuperscript{+0.35} & 64.56\textsuperscript{+1.17} & 64.17\textsuperscript{+0.78} & 64.44\textsuperscript{+1.05} & 64.37\textsuperscript{+0.97} \\
    Avg. [A,B,C] w/ NC & 63.04 &	61.21\textsuperscript{-1.83$\dagger$} & 62.57\textsuperscript{-0.47$\dagger$} & 63.27\textsuperscript{+0.23$\dagger$} & 63.00\textsuperscript{-0.04$\dagger$} & 63.47\textsuperscript{+0.43} \\
    \hline
    D & 52.28 &	59.65\textsuperscript{+7.37} & 60.00\textsuperscript{+7.72} & 59.30\textsuperscript{+7.02} & 59.88\textsuperscript{+7.60} & 59.53\textsuperscript{+7.25} \\
    D w/ NC & 49.47 &	63.63\textsuperscript{+14.15$\dagger$} & 64.09\textsuperscript{+14.62$\dagger$} & 63.51\textsuperscript{+14.04$\dagger$} & 62.57\textsuperscript{+13.10$\dagger$} & 61.64\textsuperscript{+12.16} \\
    % $\Delta$ & -2.81 & 3.98 & 4.09 & 4.21 & 2.69 & 2.11 \\
    % \hline\hline
    \midrule
    \multicolumn{7}{c}{\em Reordered Answers} \\
    \hline
    % Avg. [A,B,C] & 63.39 & 63.74 & 64.56 & 64.17 & 64.44 & 64.37  \\
    % Avg. [A,B,C] w/ 50\% RA & 63.39\textsuperscript{+0.00} & 64.56\textsuperscript{+0.82} & 64.44\textsuperscript{-0.12} & 64.05\textsuperscript{-0.12} & 63.94\textsuperscript{-0.51} & 63.86\textsuperscript{-0.51} \\
    % Avg. [A,B,C] w/ 75\% RA  & 63.39\textsuperscript{+0.00} & 64.52\textsuperscript{-0.04} & 62.65\textsuperscript{-1.79} & 62.53\textsuperscript{-1.52} & 61.95\textsuperscript{-1.99} & 62.34\textsuperscript{-1.52} \\  
    % Avg. [A,B,C] w/ 100\% RA  & 63.39\textsuperscript{+0.00} & 64.80\textsuperscript{+0.27} & 62.03\textsuperscript{-0.62} & 61.17\textsuperscript{-1.36} & 59.10\textsuperscript{-2.85} & 58.05\textsuperscript{-4.29} \\
    Avg. [A,B,C] & 63.39 & 63.74\textsuperscript{+0.35} & 64.56\textsuperscript{+1.17} & 64.17\textsuperscript{+0.78} & 64.44\textsuperscript{+1.05} & 64.37\textsuperscript{+0.97} \\
    Avg. [A,B,C] w/ 50\% RA &63.39 & 64.56\textsuperscript{+1.17} & 64.44\textsuperscript{+1.05} & 64.05\textsuperscript{+0.66} & 63.94\textsuperscript{+0.55} & 63.86\textsuperscript{+0.47} \\
    Avg. [A,B,C] w/ 75\% RA  &63.39 & 64.52\textsuperscript{+1.13} & 62.65\textsuperscript{-0.74$\dagger$} & 62.53\textsuperscript{-0.86$\dagger$} & 61.95\textsuperscript{-1.44$\dagger$} & 62.34\textsuperscript{-1.05$\dagger$} \\
    Avg. [A,B,C] w/ 100\% RA  &63.39 & 64.80\textsuperscript{+1.40$\dagger$} & 62.03\textsuperscript{-1.36$\dagger$} & 61.17\textsuperscript{-2.22$\dagger$} & 59.10\textsuperscript{-4.29$\dagger$} & 58.05\textsuperscript{-5.34$\dagger$} \\
    \hline
    D & 52.28 & 59.65\textsuperscript{+7.37} & 60.00\textsuperscript{+7.72} &  59.30\textsuperscript{+7.02} & 59.88\textsuperscript{+7.60} & 59.53\textsuperscript{+7.25} \\
    D w/ 50\% RA &52.28 & 59.30\textsuperscript{+7.02} & 60.82\textsuperscript{+8.54} &  61.40\textsuperscript{+9.12}  & 61.75\textsuperscript{+9.47} & 61.64\textsuperscript{+9.36} \\
    D w/ 75\% RA &52.28 & 59.06\textsuperscript{+6.78} & 62.81\textsuperscript{+10.53} & 65.50\textsuperscript{+13.22$\dagger$} & 67.02\textsuperscript{+14.74$\dagger$} & 66.90\textsuperscript{+14.62$\dagger$} \\
    D w/ 100\% RA &52.28 & 58.71\textsuperscript{+6.43} & 66.20\textsuperscript{+13.92$\dagger$} & 71.35\textsuperscript{+19.06$\dagger$} & 75.67\textsuperscript{+23.39$\dagger$} & 77.19\textsuperscript{+24.91$\dagger$} \\
    \hline
  \end{tabular}
  \vspace{-10pt}
\end{table}

% \begin{figure}[t!]
%   \centering
%   \includegraphics[width=0.85\textwidth]{figs/nonsense_exps.pdf}
% \caption{\textbf{Effect of Non-Informative Connections~(NC).} The accuracy of D increases with the cost of A, B and C.}
% \label{fig:non-informative}
% \vspace{-10pt}
% \end{figure}

% \begin{figure}[t!]
%   \centering
%   \includegraphics[width=0.85\textwidth]{figs/reorder_exps.pdf}
%   \caption{\textbf{Effect of Reordered Answers.} With more reordered demonstrations, the outputs are more leaned towards D.}
% % \caption{When disrupting the reinforcement loop with random prefixs, the self-reinforcement effect still retains. (A)-(C) plot the scaling of LLaMA models with a certain suffix length. (D) plots LLaMA-65B's probability varying suffix length.}
% \label{fig:reorder}
% \vspace{-10pt}
% \end{figure}

% Token reinforcement does not always yield beneficial results. In this section, we explore several unintended consequences that may arise from it. As previously demonstrated, any two tokens can activate both distant and successive reinforcement. This might result in unexpected patterns across demonstrations, which could potentially go unnoticed by end-users.


Token reinforcement is not always helpful. 
In this section, we explore the detrimental consequences that arise from it. 
As shown in Section \ref{sec:token-reinforce}, distant and successive reinforcement are activated with even two random tokens.
This could potentially lead to spurious patterns across demonstrations, which might be completely unforeseen by end users.
% \paragraph*{Spurious Pattern.} The reinforcement sometimes  
% We present by two case studies of manually construct spurious patterns in the MMLU dataset.
We illustrate this point using two experiments where we manually construct spurious patterns in the MMLU dataset.
% \elliott{reorder this part}


\paragraph*{Non-informative Connections.}
% \elliott{rephrase first three sentences. Not very clear. too long.}
Our first approach is adding connections between a phrase and a certain choice. 
We append a reasonable but non-informative phrase such as \emph{`Please kindly provide your answer.'} or \emph{`Looking forward to your choice.'} right before the template \emph{`Answer:'} each time the question's answer is `D'. 
In testing, we also append the same phrase and check whether the outputs are navigated toward `D'.
By doing so, we construct a distant reinforcement loop from the non-informative phrase to the answer `D'. 
We ensure the test set is balanced with equal numbers of questions having golden answers "A,B,C,D", and we report the accuracies at the top of Table \ref{tab:undesired}. 

We first see a gap even without demonstrations, where adding non-informative phrases lowers the accuracy of choices D. We further discuss the selection bias~\citep{zheng2023large} of different choices in the Appendix. 
% Then, compared to original 
Then, we see that the non-informative connection overcomes the selection bias and significantly elevates the accuracy choice D with a noticeable gap, in the cost of accuracy of A, B, and C.
% Then, we compare the probabilities of `A' between with and without these manually constructed spurious patterns. 
These results show the potential risk of manually injecting spurious connections and directing in-context learning toward unintended outcomes. 


\paragraph*{Answer Indicator Connections.} 
In our second experiment, we show that reinforcing the connection between `Answer:' and a certain choice, e.g., D, navigates the outputs.
To this end, we randomly replace $r$ percent of demonstrated answers with D. Simultaneously, we exchange the option contents of the original golden answer and D, to keep the demonstrations valid.
In this way, we gradually reinforce the connection between `Answer:' and `D', with successive reinforcement. 

The results are presented at the bottom of Table \ref{tab:undesired}. 
Our baseline is 25\%, where the demonstrations are balanced. 
With the increased ratio of answer D in demonstrations, the accuracy of D is largely improved, from 0.52 to 0.78, while the accuracy of A, B, and C decreases from 0.63 to 0.58. 
Our findings corroborate with ~\cite{an2023incontext}, where they show how diversity affects the performance of in-context learning. Our findings demonstrate how unbalanced demonstrations bring an unfair advantage for certain outputs. 

% \paragraph*{Discussion}
In conclusion, our studies in this section emphasize the risk that the same reinforced surface patterns, which give beneficial effects to ICL, can also cause spurious connections, and without even noticing, LLMs can get misled by unintended and unforeseen connections.
% Our studies in this section reveals that in-context learning is prone to spurious connections in demonstrations. 
% LLMs easily get stuck in some unintended and unforeseen connections, which brings unfair biases. 

\section{Related Work}
% \subsection{In-Context Learning}

\paragraph{Explaining In-Context Learning.}
A range of contributions has deepened our understanding of In-Context Learning (ICL). \cite{chan2022data} and \cite{xie2022an} explore the emergence of ICL from the perspective of training data and Bayesian inference, respectively. 
Implicit learning of ICL over demonstrations is further highlighted by \cite{garg2023transformers}, \cite{li2023transformers}, and \cite{hahn2023theory}. The similarity between gradient descent learner and in-context learner is demonstrated by \cite{gd1,vonoswald2023transformers}, while \cite{dai2023gpt} explain language models as meta-optimizers and likens ICL to implicit finetuning. 
Our work differs from this line of work with a novel perspective via repetitions and reinforced features, and our findings could potentially explain the mechanism of how LLMs achieve implicit learning. For instance, the step by step reinforcement across demonstrations intuitively resembles the gradient descent process of ICL described in previous work. 
\cite{olsson2022context} introduce induction heads of copying patterns and provide evidence for their relationship with ICL. Differently, our work investigates the LLM as a whole~\citep{anderson1972more}, studies sophisticated patterns, and scrutinizes both the benefits and drawbacks of surface patterns.
%  further investigates the ICL in LLMs from the perspective of repetitions. 

% Closest to our work, \cite{olsson2022context} introduce induction heads and provide evidence for their relationship with in-context learning (ICL). However, our research diverges from theirs in several significant ways:
% (1) Their work concentrate on the attention layer and mainly on small models. In contrast, we study the complex phenomena of real-world large language models (LLMs), such as LLaMA and OPT, as a whole. As posited by Anderson (1972), with each increment in complexity, entirely new properties emerge.
% (2) While their work is centered on successive patterns, our study uncovers the role of token reinforcement in ICL, considering \emph{both} successive and distant reinforcements.
% (3) Rather than relying on a simple heuristic measure for in-context learning, we establish a direct and explicit connection between our findings and ICL. Furthermore, we scrutinize both the benefits and drawbacks of surface patterns.


%  and mainly on small models, while we investigate the probability of large 
% while their work also study the surface patterns 

\paragraph{Analyzing In-Context Learning.}
Several studies have analyzed ICL properties. \cite{min2022rethinking} identifies key factors that influence ICL capability, with \cite{yoo2022groundtruth} suggesting the importance of input-label mapping varies with experimental settings. \cite{wei2023larger} proposes that learning input-label mapping is an emergent ability. The importance of structural similarity, diversity, and simplicity in in-context demonstrations is emphasized by \cite{an2023incontext}. Order and embedding distribution of the demonstrations are critical, according to \cite{lu-etal-2022-fantastically} and \cite{liu-etal-2022-makes}. \cite{pan2023incontext} partitions ICL ability into task recognition and task learning, observing different phenomena with varying model sizes. Lastly, \cite{si2023measuring} unveils the presence of inductive biases in ICL by designing underspecified demonstrations.

Our findings corroborate with multiple previous analyses of in-context learning. 
For example, the scaling for distant reinforcement echoes \cite{wei2023larger,pan2023incontext}'s findings of different phenomena when varying model sizes. 
The importance of demonstration ordering and diversity in \cite{an2023incontext,lu-etal-2022-fantastically} can be explained by avoiding spurious connections. 

% Our results of 
% \elliott{add differences}

% \subsection{In-Context Learning}

% \paragraph{Understanding In-Context Learning.}
% \cite{chan2022data} study the pretraining data and show that ICL emerges when the transformer architecture works alongside particular properties of the training data.
% \cite{xie2022an} explain ICL from the perspective of Bayesian inference. They construct a synthetic dataset to prove ICL emerges when the training data follows a mixture of hidden Markov distribution. 

% Another line of research hints that ICL performs implicit learning over demonstrations. 
% \cite{garg2023transformers} propose to directly train to in-context learn the linear function and demonstrate Transformers can effectively learn unseen linear functions from in-context examples.
% \cite{li2023transformers} formalize ICL as an algorithm learning problem and theoretically discuss the stability and generalization of ICL. 
% \cite{hahn2023theory} derive an information-theoretic bound showing and show the compositional nature of language is the key to inducing ICL ability.

% \cite{gd1,vonoswald2023transformers} demonstrate by construction and by comparison the similarity between gradient descent learner and in-context learner, and present evidence that in-context learners share algorithmic features with exact least-squares regression predictors. 
% \cite{dai2023gpt} explain language models as meta-optimizers and understand in-context learning as implicit finetuning. They provide theoretical and empirical evidence to show the resemblance of the behaviors of in-context learning and explicit fine-tuning.

% More related to our work, \cite{olsson2022context} introduce induction heads, that is attention heads responsible for completing sequences like [A][B] ... [A] -> [B].
% They provide macro- and microscopic evidence that supports the relationship between induction head and in-context learning.  
% The induction head mechanism corroborates our finding of successive reinforcement. 
% Different from their work, we focus on the LLM as a whole and induction heads could potentially be a reason why token occurrence reinforcement occurs, though they have a huge gap to fill.


% \paragraph{Analyzing In-Context Learning.}
% Extensive work has been devoted to analyzing ICL properties. 
% \cite{min2022rethinking} scrutinize four demonstrative factors that affect ICL capability: formatting tokens, label space, input distribution, and input-label mapping. Surprisingly, they find all factors except the input-label mapping matter a lot to ICL.
% Contrastingly, \cite{yoo2022groundtruth} determine that the importance of input-label mapping is contingent in particular experimental settings.
% Further research by \cite{wei2023larger} suggests that learning input-label mapping is an emergent ability, as revealed through their investigation of semantically unrelated ICL.
% \cite{an2023incontext} emphasize that in-context demonstrations should exhibit structural similarity to the test case, diversity amongst themselves, and individual simplicity.
% \cite{lu-etal-2022-fantastically} and \cite{liu-etal-2022-makes} claim the order and embedding distribution of the demonstrations crucial to the performance. 
% \cite{pan2023incontext} partition the ICL ability into two subcategories, task recognition and task learning. They design controlled experiments to disentangle two subcategories and discover different phenomena when scaling model sizes. 
% \cite{si2023measuring} confuse ICL ability by intentionally designing underspecified demonstrations where two possible mapping functions are available. Their findings unravel the existence of inductive biases in ICL. 



% \subsection{Repetitive Generation and Self-Reinforcement Effect}

\paragraph{Repetitive Generation and Self-Reinforcement Effect.}

Repetition is a notorious issue in neural text generation, affecting tasks like open-ended and directed generation~\citep{holtzman2019curious,welleck2019neural,lin2021straight,see2017get,liu2019text}. Maximization-based decoding strategies lead to bland, consecutive repetitions at word, phrase, and sentence levels~\citep{holtzman2019curious,welleck2019neural,li2016simple,karpathy2015deep,guan2021long}. Despite advancements in large-scale pre-training with Transformer architecture~\citep{vaswani2017attention,radford2019language,lewis2020bart}, unexpected sentence-level repetitions persist~\citep{radford2019language, brown2020language, fu2020theoretical}. 

% \paragraph{Self-Reinforcement Effect.}
The repetition issue is puzzling given the lack of repetitive sentences in the training data. A series of studies investigate the cause, with both from the theoretical perspective~\citep{fu2020theoretical} and empirical findings~\citep{holtzman2019curious}. Recently, \cite{xu2022learning} proposes the \emph{self-reinforcement effect}, suggesting a repetitive loop when combined with maximization decoding. Our study extends the effect to large language models and token reinforcement, and bridges the excellent ability of in-context learning to this notorious issue. 

% \subsection*{Discussion}
% Both examples in Figure \ref{fig:examples} can be viewed as a learning process.
% Following the ``recall, match, and learn'' procedure, it becomes evident that both cases employ learned connections from their respective demonstrations. 
% However, the quality of these connections profoundly influences the results. Connections within in-context learning are robust and effective, whereas those in repetitive generation are paradoxical and spurious, as illustrated when 'Answer' yields 'Positive'. 
% These disparities echo the optimization process in conventional machine learning tasks, where the in-context learning demonstrations correspond to a balanced dataset, while those of repetitive generation align with an imbalanced and homogeneous dataset.


\section{Conclusion and Future Work}
In this study, we have taken a novel approach to understanding in-context learning by exploring its relationship with repetitive generation. We have identified a key mechanism, the token reinforcement loop, where any two tokens can form a strong connection through multiple co-occurrences. 
% We have shown that this mechanism is strongly related to both repetitive generation and in-context learning.
Our findings demonstrate that token reinforcement plays a crucial role in shaping the output space and following patterns in in-context learning. Furthermore, we have illustrated through various studies how token reinforcement leads to spurious connections in in-context learning. 
Our work highlights the role of in-context learning as a double-edged sword. 
% By revealing potential biases that can arise from spurious connections, we also highlight important considerations for designing and using in-context learning demonstrations. 
We believe that our findings not only contribute to the current understanding of in-context learning but also pave the way for future explorations in this exciting field.

Based on the insights gained from this study, there are multiple interesting questions to be answered in future work. 
First, future work could delve deeper into the implications of these spurious connections and how they might be mitigated, which could improve the robustness and effectiveness of in-context learning. 
Second, the reason why token co-occurrence reinforcement exists is unclear, as the cross-entropy objective introduces no such inductive bias. 
Last but not least, our findings suggest the quality of connections reinforced from demonstrations profoundly influences in-context learning, thus emphasizing the necessity of measurement and visualization of the connections.

% It would also be interesting to investigate how the principles of token reinforcement could be applied to other areas of machine learning, particularly in the realm of deep learning algorithms.

% In conclusion, this paper illuminates a significant facet of in-context learning through the lens of repetitive generation. The discovery of the token reinforcement loop and its implications for the direction of output space and pattern recognition in in-context learning shed new light on the inner workings of these algorithms. By revealing the potential biases that can arise from spurious connections, we also highlight important considerations for the design and use of in-context learning systems. 




% ICLR requires electronic submissions, processed by
% \url{https://openreview.net/}. See ICLR's website for more instructions.

% If your paper is ultimately accepted, the statement {\tt
%   {\textbackslash}iclrfinalcopy} should be inserted to adjust the
% format to the camera ready requirements.

% The format for the submissions is a variant of the NeurIPS format.
% Please read carefully the instructions below, and follow them
% faithfully.


\bibliography{iclr2024_conference}
\bibliographystyle{iclr2024_conference}

\clearpage
\appendix
\input{appendix}


% \subsection{Experimental Setting of The Undesired}


% \section{}

% You may include other additional sections here.

\end{document}
