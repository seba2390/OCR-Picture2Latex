\section{Applications}
\label{sec:applications}

There are a wealth of examples of $V$-uniformly geometrically ergodic Markov chains satisfying \Cref{assG:kernelP_q} and \Cref{assG:kernelP_q_smallset};  for example \citep[Chapter~15]{meyn:tweedie}, \citep{roberts:rosenthal:2004}, and \citep[Chapters~2,15]{douc:moulines:priouret:soulier:2018}. Using for example \citep{roberts:rosenthal:2004}, we can write Bernstein inequalities for additive functions of Markov Chains Monte Carlo methods in $\rset^d$. Such examples are classic and come close to those given in \cite{adamczak2015exponential}, so we prefer to focus on examples that demonstrate the results of \Cref{sec:geom-ergod-mark}. Our first example is an application to an Monte Carlo algorithm in Hilbert space. Our second example is an analysis of averaging methods for a stochastic approximation algorithm. We verify for each of these examples that the assumptions we consider in \Cref{sec:geom-ergod-mark} are satisfied and therefore the corresponding results can be applied.

\paragraph{The pre-conditioned Crank-Nicolson (pCN) algorithm}
pCN introduced in \cite{beskos:roberts:stuart:voss:2008,cotter:roberts:stuart:white:2013} is a Markov chain Monte Carlo (MCMC) method which aims at sampling
from a target distribution $\pi$ defined on a Hilbert space $\msh$ with norm $\normH{\cdot}$ and
its Borel $\sigma$-field $\mch$. This method has been applied for Bayesian inference in function
spaces and other infinite-dimensional models,
\cite{stuart:2010,cotter:roberts:stuart:white:2013,buitanh:ghattas:2014,eberle:2014,agapiou:roberts:vollmer:2018};
see also \cite{beskos:pinski:sanzserna:stuart:2011,ottobre:et:al:2016,rudolf:sprungk:2018,hosseini2018spectral} for generalizations and extensions.

Let $\muH$ be Gaussian measure $\muH$ on
$(\msh,\mch)$ with mean zero. Assume that the target distribution
$\pi$ has a density with respect to $\muH$  of the form $\rmd \pi / \rmd \muH \propto \exp(-\potU )$, for some potential function $\potU:\msh \to \rset$.  pCN then consists in defining the Markov chain $\sequence{X}[k][\nset]$ by the following recurrence:
\begin{equation}
  \label{eq:def_pCN}
  X_{k+1} = X_k \indiacc{U_{k+1} > \alphaH(X_k,Z_{k+1}) }+  \defEns{\rhoH X_k + (1-\rhoH^2)^{1/2} Z_{k+1}} \indiacc{U_{k+1} \leq \alphaH(X_k,Z_{k+1})}\eqsp.
\end{equation}
Here $\rhoH \in\ooint{0,1}$, $\sequence{Z}[k][\nset]$ and $\sequence{U}[k][\nset]$ are independent sequences of \iid~random variables with distribution $\muH$ and uniform on $\ccint{0,1}$ respectively, defined on the probability space $(\RandSpace,\Filtr,\PP)$ and for any $x,z \in \msh$,
\begin{equation}
\label{eq:accept_pcn}
\alphaH(x,z) = 1\wedge \exp\parenthese{-\potU(\rhoH x+ (1-\rhoH^2)^{1/2} z) + \potU(x)} \eqsp.
\end{equation}

It has been established in \cite{beskos:roberts:stuart:voss:2008,cotter:roberts:stuart:white:2013} that the Markov kernel associated with $\sequence{X}[k][\nset]$ is reversible with respect to $\pi$. Further,  \cite{hairer:stuart:vollmer:2012} provides the following conditions (see \cite[Assumptions~2.10-2.11]{hairer:stuart:vollmer:2012}) on $\potU$ implying that this kernel is geometrically ergodic with respect to some Wasserstein semi-metric. Denote $\ballH{x}{R} = \{z \in \msh \,: \, \normH{x-z} \leq R\}$ for any $x \in\msh$ and $R\geq 0$
%Consider the following assumptions :
\begin{assumptionpCN}
\label{assum:d-small-set-pCN}
There exist  $\alphalpCN >-\infty$, $\rpCNconst >0$, and $a \in \ooint{1/2,1}$
such that for all $x \in \msh$, $\normH{x} \geq \RpCN = \bigl(2 \rpCNconst/ (1 - \rhoH)\bigr)^{1/(1-a)}$,
$\inf_{z \in \ballH{\rhoH x}{\rpCNconst \normH{x}^a}} \alphaH(x, z) \geq \exp \left(\alphalpCN\right)$.
\end{assumptionpCN}
\begin{assumptionpCN}
\label{assum:potU-lipshitz}
$\potU$ is a Lipschitz function with Lipschitz constant $\Lippcn$, and $\exp(- \potU)$ is $\muH$-integrable.
\end{assumptionpCN}
\begin{proposition}
\label{prop:pCN}
Assume \Cref{assum:d-small-set-pCN}  and  \Cref{assum:potU-lipshitz}. Then:
\begin{itemize}
\item \Cref{assG:kernelP_q} is satisfied with $V(x)= \exp(\normH{x})$ and $\lambda$, $b$ given in \eqref{eq:drift_constants_pcn};
\item \Cref{ass:cost_fun} is satisfied with $\cost(x,x') = 1 \wedge [\normH{x-x'}/\varepsilonH]$ and $\pcost=1$, where $\varepsilonH$ is defined in \eqref{eq:varepsilon_H_def};
\item \Cref{assG:kernelP_q_contractingset_m} is satisfied  with $\CKset = \ballH{0}{\Rassumapcn} \times \ballH{0}{\Rassumapcn}$  with $\Rassumapcn = \log\bigl\{4b/(1-\lambda) - 1 \bigr\}$, $m = \lceil \log(\varepsilonH/(4\Rassumapcn))/\log\rhoH \rceil$, $\boundmetric= 1$, and $\minorwas$ defined in \eqref{eq:epsilon_pcn_def}.
\end{itemize}
\end{proposition}
\begin{proof}
The proof is postponed to \Cref{subsec:prop:pCN}.
\end{proof}

We may therefore apply our results to obtain Bernstein-type inequality for sample average $S_n(g) = n^{-1} \sum_{\ell = 0}^{n-1} g(X_\ell)$, where $g \in  \Lclass_{\beta, W}$ for some $\beta > 0$.

 \paragraph{Stochastic gradient descent (SGD) for strongly convex objective function}
 As a second example, we consider now SGD with fixed stepsize  applied to minimize a smooth objective function $\objf : \rset^{\dims} \to \rset$.  We suppose that there
 exists a measurable space $(\Yset,\Ysigma)$ endowed with a probability
 measure $\muY$ and a measurable function $\fieldH: \rset^{\dims} \times \Yset \to \rset^{\dims}$, such that
 $\nabla \objf(\theta) = \int_{\Yset} \fieldH_{\theta}(y) \rmd \muY(y)$
 for any $\theta \in\rset^{\dims}$. Based on \iid~samples $(\YSGD_k)_{k \in\nset}$ from $\muY$, the iterates of SGD with fixed stepsize $\gamma >0$ define a Markov chain given by the recursion
 \begin{equation}
   \label{eq:def_SGD}
   \theta_{k+1} = \theta_k - \gamma  \fieldH_{\theta_k}(\YSGD_{k+1}) \eqsp.
 \end{equation}
Denote by $\MKSGD_{\gamma}$ the Markov kernel associated with this recursion.
Consider the following assumption. % Denote for any $y\in\Yset$ and $\theta\in\rset^d$, $\bar{\fieldH}_{\theta}(y) = \fieldH_{\theta}(y) - \nabla \objf(\theta)$.
\begin{assumptionSGD}
\label{ass:sgd_field}
\begin{enumerate}[wide, labelwidth=0pt, labelindent=0pt, itemsep=0mm, label=(\roman*)]
\item $\objf$ is $\muf$-strongly convex and twice continuously differentiable  with $\nabla \objf$ $\Lf$-Lipschitz: for any $\theta,\theta' \in\rset^{\dims}$, it holds $\ps{\nabla \objf(\theta) - \nabla\objf(\theta')}{\theta-\theta'} \geq \muf \norm{\theta-\theta'}^2$ and $\sup_{\theta \in \rset^{\dims}} \norm{\nabla^2 \objf(\theta)} < \Lf$, where $\norm{\nabla^2 \objf(\theta)}$ denotes the operator norm of the Hessian of $\objf$ at $\theta\in \rset^{\dims}$.
\item For $\muY$-almost every $y\in\Yset$, $\theta \mapsto \fieldH_{\theta}(y)$ is co-coercive, \ie~there exists $\Ccoco >0$ such that $\ps{\fieldH_{\theta}(y) - \fieldH_{\theta'}(y)}{\theta-\theta'} \geq \Ccoco \norm{\fieldH_{\theta}(y) - \fieldH_{\theta'}(y)}^2$ for any $\theta,\theta' \in \rset^{\dims}$
\end{enumerate}
\end{assumptionSGD}
Note that under \Cref{ass:sgd_field}, $\objf$ admits a unique minimizer denoted  $\thetas$. In the sequel we consider the following classical light-tail condition on the gradient noise; see \cite{hsu2012tail,harvey2019tight} and for equivalence between definitions.
\begin{assumptionSGD}
\label{ass:sgd_noise_exp_mom}
The gradient noise is uniformly norm sub-gaussian with variance factor $\sgvarfac < \infty$: for all $\theta \in \rset^{\dims}$
 and $t \in \rset_+$,
\[
\PP\left( \normLigne{\fieldH_{\theta}(\YSGD) - \nabla \objf(\theta)} \geq t \right) \leq  2 \exp(-t^2 /(2 \sgvarfac)) \eqsp.
\]
\end{assumptionSGD}
Denote by $\kapf= \muf \Lf/ (\muf+\Lf)$ the condition number of the function $\objf$.
\begin{proposition}
\label{theo:SGD}
Assume \Cref{ass:sgd_field} and \Cref{ass:sgd_noise_exp_mom}. Pick $\gamma \in \ocint{0,\gamma_{\objf}}$ where $\gamma_{\objf}= 1/2 \wedge \kapf/2 \wedge (\muf+\Lf)^{-1}$. Then the following statements hold:
\begin{enumerate}
\item \label{item:theo:SGD-1} \Cref{ass:cost_fun} is satisfied with $\cost(\theta,\theta') = 1 \wedge \norm{\theta-\theta'}^2$ and $\pcost=2$;
\item \label{item:theo:SGD-2} \Cref{assG:kernelP_q} is satisfied with drift function $V(\theta)= \exp(1 + \norm[2]{\theta-\theta^*}/\tsgvarfac)$, constants
\begin{equation*}
\lambda = \rme^{-\gamma\kapf/(2\tsgvarfac)}, b = \gamma\bigl(\kapf^{-1} + 2 \gamma + \kapf/(2\tsgvarfac)\bigr)\exp\left(2 + (2\tsgvarfac)^{-1} + (2\gamma \kapf+1)\kapf^{-2}\right)\eqsp,
\end{equation*}
where $\tsgvarfac = 2\sgvarfac(\rme+1)/(\rme-1)$;
\item \label{item:theo:SGD-3} \Cref{assG:kernelP_q_contractingset_m} is satisfied with $\CKset= \ball{0}{R} \times \ball{0}{\Rsgd}$, $\Rsgd = \log\bigl\{4b/(1-\lambda) - 1 \bigr\}$, $\boundmetric= 1$, $\varepsilon= 2\muf \gamma (1-\gamma \Lf/2)$, and $m = \lceil \log(4\Rsgd^2)/\log(1/(1-\minorwas)) + 1 \rceil$.
\end{enumerate}
\end{proposition}
\begin{proof}
The proof is postponed to \Cref{sec:proof_drift_sgd}.
\end{proof}
We apply our results to the Polyak-Ruppert averaged estimator $\hat{\theta}_n = n^{-1} \sum_{k=0}^{n-1} \theta_k$ of $\thetas$  \cite{ruppert1988efficient,polyak1992acceleration}.
It follows from \Cref{theo:SGD} that under \Cref{ass:sgd_field} and \Cref{ass:sgd_noise_exp_mom}, for any $\gamma \in \ocint{0,\gamma_\objf}$, $\MKSGD_{\gamma}$ has a unique invariant distribution $\pi_\gamma$
It is well-known that in general $\bar{\theta}_{\gamma} = \int_{\rset^d} \theta \rmd \pi_{\gamma}(\theta) \neq  \thetas$. However, under the same lines as \cite[Theorem~4]{dieuleveut2020bridging}, we obtain a non-asymptotic bound on  $\norm{\bar{\theta}_{\gamma}- \theta^*}$.

%   $\MKSGD_{\gamma}$ admits a unique stationary distribution $\pi_{\gamma}$ satisfying $\int_{\rset^d} \normLigne{\theta-\thetas}^2 \rmd \pi_{\gamma}(\theta) \leq \gamma/[\muf(1-\gamma \Lf)]$.
%
\begin{lemma}
  \label{propo:bias}
  Assume \Cref{ass:sgd_field} and \Cref{ass:sgd_noise_exp_mom}. In addition, suppose that the Hessian of $\objf$ is $\LipHessianf$-Lipschitz, \ie~for any $\theta,\theta' \in \rset^d$,
$\norm{\nabla^2\objf(\theta)-\nabla^2 \objf(\theta')} \leq \LipHessianf\norm{\theta-\theta'}$.
  Then, for any $\gamma \in \ooint{0,1/\Lf}$, it holds $    \norm{\thetas - \bar{\theta}_{\gamma}} \leq \gamma \sgvarfac /[\muf^2(1-\gamma \Lf)]$.
  % \begin{equation*}
  %   \norm{\thetas - \bar{\theta}_{\gamma}} \leq \gamma \sgvarfac /[\muf^2(1-\gamma \Lf)] \eqsp.
  % \end{equation*}
\end{lemma}
\begin{proof}
The proof is postponed to \Cref{sec:proof-crefpropo:bias}.
\end{proof}
\Cref{propo:bias} shows that it is enough to obtain high probability bounds on $\normLigne{\hat{\theta}_n -   \bar{\theta}_{\gamma}}$ in order to obtain non-asymptotic convergence guarantees and high probability bounds on $\normLigne{\hat{\theta}_n - \thetas}$.






\begin{comment}
\begin{proposition}
\label{prop:sgd_concentration_fin}
Assume \Cref{ass:sgd_field} and \Cref{ass:sgd_noise_exp_mom} and let $\gamma \in \ocint{0, (1 \wedge \kapf)/2 \wedge (\muf+\Lf)^{-1} \wedge 2/\Lf}$. Then, for any probability measure $\xi$ on $(\rset^d,\mathcal{B}(\rset^d))$ satisfying $\xi(V_{1/(2\tsgvarfac)}) < \infty$, any $\vectpr \in S^{d-1}$ and $t \geq 0$ it holds
\begin{align*}
&\PP_{\xi}(|\vectpr^\top(\hat{\theta}_n -   \bar{\theta}_{\gamma})| \geq t) \leq
2\exp\biggl\{-\frac{(nt)^2/8}{n^2\PVar[\pi]\{\vectpr^\top(\hat{\theta}_n -   \bar{\theta}_{\gamma})\} + \ConstJW[n,\operatorname{PR}]^{2/7} t^{12/7}}\biggr\} \\
&\quad   +  \exp\parenthese{\log(\ratewas) \constprfirst  t^{2/3}}\defEns{1+(-\log(\ratewas)/4)\frac{[  \vartconstwas \{\pi(V_{1/(2\tsgvarfac)}) + \xi(V_{1/(2\tsgvarfac)})\}]^{1/2}}{\ratewas^{1/4}(1-\ratewas^{1/4})}} \\
&\quad   +\exp\parenthese{-\constprfirst t^{2/3}/2}\defEns{1+ \sup_{b \geq \rme} \{b^{1/4}\log(b)\} \frac{\vartconstwas \{\pi(V_{1/(2\tsgvarfac)}^{1/2}) + \xi(V_{1/(2\tsgvarfac)}^{1/2})\}}{1-\ratewas}}\eqsp,
\end{align*}
where $\ratewas$ and $\vartconstwas$ are defined in \eqref{eq:def:rho}, $\tsgvarfac$ is defined in
\begin{equation*}
\constprfirst  = \frac{3}{2^{14/3}\constprsecond^{2/3}}, \quad \constprsecond = \bigl(2\diam(\CKset_{d}) \vee 1\bigr)\tsgstdfac\eqsp,
\end{equation*}
and
\begin{equation*}
\ConstJW[n,\operatorname{PR}] = 2^{10}\biggl(\frac{\constprsecond^2}{n\PVar[\pi]\{\vectpr^\top(\hat{\theta}_n -   \bar{\theta}_{\gamma})\}} \vee 1\biggr)\bigl(\ratewas/\constprsecond\bigr) \{(2/ \log(1/\ratewas)) \vee 1\}^{2} \{\vartconstwas^2 \pi(V)\}^{3/2}\eqsp.
\end{equation*}
\end{proposition}
\begin{proof}
The proof is postponed to \Cref{sec:proof_sgd_result}.
\end{proof}
\end{comment} 