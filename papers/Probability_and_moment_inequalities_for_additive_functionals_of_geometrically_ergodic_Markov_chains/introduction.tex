In this paper, we derive concentration inequalities for linear statistics of Markov chains under a geometric Foster-Lyapunov drift condition and either a minorization condition (which implies $V$-equivalent geometric ergodicity) or a local Wasserstein contraction (which implies convergence in weighted Wasserstein distance). For sums of  independent variables or martingale difference sequences, a wealth of inequalities have been developed, see, e.g.,  \cite{bercu2015concentration}. At the same time, less attention has been paid to the Markov case. In particular, most existing results are either not quantitative in terms of the dependence on the initial condition or the mixing rate of the Markov chain (see \Cref{sec:related-works} for an overview), or they can only be applied to restricted functions. In this paper, the Bernstein and Rosenthal type inequalities for Markov chains are obtained using the cumulant techniques (\cite{leonov:sirjaev:1959,saulis:statulevicius:1991}) with explicit dependence on the initial distribution of the Markov chain and its mixing rate. In the following, we briefly discuss the Bernstein and Rosenthal type inequalities for sums of independent random variables.
\par 
Let $(Y_\ell)_{\ell=1}^n$ be a sequence of independent centered random variables and set $S_n= \sum_{\ell=1}^n Y_{\ell}$. Under \emph{Bernstein's condition}, that is,
\begin{equation}
  \label{eq:Bernstein_condition}
  |\PE[Y_{\ell}^k]|  \leq (k!/2) \PVar(Y_{\ell}) c^{k-2}\eqsp, \text{ for all } \ell \in\{1, \ldots, n\} \text{ and integers } k \geq 3  \eqsp,
\end{equation}
the following two-sided Bernstein inequality holds: for any $t > 0$ and $n \in \nset$,
\begin{equation}
\label{eq:Bernstein-Rio}
\PP( |S_n| \geq t) \leq 2 \exp\left\{ - \frac{t^2/2}{\PVar(S_n) + c t} \right\} \eqsp.
\end{equation}
The condition \eqref{eq:Bernstein_condition} can be further relaxed, see e.g. \cite[Chapter~2, Theorem~2.9]{bercu2015concentration}. Bernstein's condition and inequality can be further generalized by cumulant expansion, as suggested by \cite{bentkus:1980}. Recall that the $k$-th cumulant of a random variable $Y$ is defined as
\[
\Gamma_k(Y) = \frac{1}{\rmi^k} \left.\frac{\rmd^k}{\rmd t^k}\bigl(\log \PE[\rme^{\rmi t Y}]\bigr)\right\vert_{t=0} \eqsp.
\]
It is shown in \cite{bentkus:1980} that if there exist $\upgamma \geq 1$ and $B \geq 0$ such that
\begin{equation}
\label{eq:bernstein-condition-cumulant}
|\Gamma_k(S_n)| \le (k!/2)^\upgamma \PVar(S_n) B^{k-2}  \quad \text{for all integers  $k \geq 2$} \eqsp,
\end{equation}
then for all $t \geq 0$, the following \emph{Bernstein-type bound} holds
\begin{equation}
\label{eq: Bernstein type bound}
 \PP(|S_n| \geq t) \le 2 \exp \bigg \{ -\frac{t^2/2}{\PVar(S_n) + B^{1/\upgamma} t^{(2\upgamma - 1)/\upgamma}} \bigg \} \eqsp.
\end{equation}
In \cite[Theorem 3.1]{saulis:statulevicius:1991} it is shown that the condition \eqref{eq:bernstein-condition-cumulant} is satisfied if, for example, the following generalization of \eqref{eq:Bernstein_condition} holds: there exists $K > 0 $, so that
\begin{equation}
\label{eq:bernstein_bound_generalized}
|\PE[Y_{\ell}^k]|  \leq (k!/2)^{\upgamma} \PVar(Y_\ell) K^{k-2}\eqsp, \text{ for all } \ell \in\{ 1, \ldots, n\} \text{ and  integers } k \geq 2\eqsp.
\end{equation}
Moreover, it can be shown that \eqref{eq:bernstein_bound_generalized} also holds for subexponential random variables. More precisely, define the Orlicz norm $\| Y_{\ell} \|_{\psi_{1/\upgamma}} = \inf\{t > 0: \PE [\psi_\alpha(|Y_{\ell}|/t)] \le 1\}$, where for $\alpha > 0$ we set $\psi_\alpha(x) = \rme^{x^\alpha} - 1$.
Following \cite{Adamczak2008} and \cite{Lecue2012}, we can show that if $\|\max_{\ell \in \{1,\dots,n\}} Y_\ell\|_{\psi_{1/\upgamma}} < \infty$ and $\upgamma \geq 1$, then there are some universal constants $a, b > 0$ (depending on $\upgamma$ but not on the distribution of $Y_{\ell}$) such that for any $t > 0$ and $n \in \nset$,
\begin{equation}
\label{eq:gene_Bernstein type bound}
\PP( |S_n| \geq t) \le 2 \exp \bigg \{-\frac{t^2}{a \PVar(S_n) + b  \| \max_{\ell \in \{1,\dots,n\}} Y_\ell \|_{\psi_{1/\upgamma}}^{1/\upgamma} t^{(2\upgamma -1)/\upgamma} } \bigg \}.
\end{equation}
A simple example in \cite{Adamczak2008} shows that in general $\| \max_{\ell \in \{1,\dots,n\}} Y_\ell \|_{\psi_{1/\upgamma}}$ cannot be replaced by $\max_{ \ell \in \{1,\dots,n\}} \|Y_\ell \|_{\psi_{1/\upgamma}}$ without introducing an additional factor $\log(n)$.
\par 
Moment inequalities also play an important role in studying the properties of sums of random variables. For sums of martingale difference sequences, \cite{pinelis_1994} shows the following version of Rosenthal's inequality (see \cite{Rosenthal1970}): for $q \geq 2$,
\begin{equation}
\label{eq:ros_independent_pinelis}
    \PE[|S_n|^{q}] \le C^q \bigl\{ q^{q/2} \PVar(S_n)^{q/2} +  q^q \PE[|\max_{\ell \in \{1,\dots,n\}} Y_{\ell}|^q]\bigr\} \eqsp,
\end{equation}
% \cite{Rosenthal1970} proved that for $q \geq 2$ there exists a constant $B(q)$ such that, for all $n \in \nset$,
% \begin{equation}
% \label{eq: Ros independent}
%     \PE[|S_n|^{q}] \le B(q) \biggl\{\PVar(S_n)^{q/2} +  \sum_{\ell=1}^n \PE[|Y_\ell|^q]\biggr\} \eqsp,
% \end{equation}
% provided that $\PE[|Y_\ell|^q] < \infty$, $\ell \in \{1,\dots,n\}$; see  \cite{pinelis1985estimates,johnson1985best,ibragimov1998exact} for the best estimates of the constant $B(q)$.
% Inequality \eqref{eq: Ros independent} was improved in \cite{pinelis_1994}, where it was shown that
% \begin{equation}
% \label{eq: Ros independent Pinelis}
%     \PE[|S_n|^{q}] \le K^q \bigl\{ q^{q/2} \PVar(S_n)^{q/2} +  q^q \PE[|\max_{\ell \in \{1,\dots,n\}} Y_{\ell}|^q]\bigr\}
% \end{equation}
where $C$ is a universal constant. An important property of \eqref{eq:ros_independent_pinelis} is that if it is satisfied for every $q \geq 2$ and $ \| \max_{\ell \in \{1,\dots,n\}} Y_\ell \|_{\psi_{1}}  < \plusinfty$, then $S_n$ satisfies an Bernstein' inequality of the form \eqref{eq:gene_Bernstein type bound} with $\upgamma =1$ can be established.
Assuming that $|Y_\ell|  \le M$, using Markov's inequality one easily obtains, for example
\begin{equation}
    \PP(|S_n| \geq t) \le \rme^2 \exp \bigg\{ -  \bigg(\frac{t}{C \rme \PVar^{1/2}(S_n)}\bigg)^2 \wedge \frac{t}{C \rme M} \bigg\} \eqsp.
\end{equation}
\par 
In this paper we are concerned with the derivation of bounds of the form \eqref{eq: Bernstein type bound} and \eqref{eq:ros_independent_pinelis} for $S_n$ as an additive functional of a Markov chain. More precisely, we consider a Markov kernel $\MK$ on $(\Xset,\Xsigma)$ for which we assume that there is a unique stationary distribution $\pi$. Let $g:\Xset \mapsto \rset$ be a measurable function satisfying $\pi(\absLigne{g}) < \infty$. We set up Rosenthal- and Bernstein-type inequalities for the sums
\begin{equation}
\label{eq:def_S_n_g}
S_n = \sum_{\ell=0}^{n-1}  \{g(X_\ell) - \pi(g)\} \eqsp,
\end{equation}
where $\sequence{X}[n][\nset]$ is a Markov chain with Markov kernel $\MK$ and initial distribution $\xi$. We focus on Markov kernels that converge geometrically to the stationary distribution, either in the $V$-total variation or in the weighted Wasserstein distance. Although we do not use regeneration techniques, the results we obtain have in common with those presented in \cite{clemenccon2001moment,Adamczak2008,adamczak2015exponential,bertail2018new,lemanczyk2020general}
the goal of obtaining bounds with explicit and computable constants. This requirement is crucial to obtain, for example, deviation estimates for Markov Chain Monte Carlo or finite-time bounds for stochastic approximation algorithms with Markovian noise. The proof method we use is based on the \cite{bentkus:1980} (see also \cite{saulis:statulevicius:1991}) outlined cumulant expansion techniques and was further developed in \cite{doukhan2007probability} for weakly dependent processes. In the stationary case (when the initial condition $\xi$ is equal to $\pi$), one of the main steps of the proof is to bound centered moments associated with $\{g(X_{\ell})\}_{\ell=0}^{n-1}$ (see \Cref{sec:cumul-centr-moments} for the corresponding definitions). This connection follows from the Leonov-Shiryaev formula, see \cite{leonov:sirjaev:1959}. As far as we know, this is the first application of the Leonov-Shiryaev formula to Markov chains. 
\par 
Unlike \cite{doukhan2007probability}, which considers the case of weakly dependent sequences, sharper estimates can be obtained using the Markov property. Finally, we also treat the case of an arbitrary initial distribution $\xi$. We derive results for the non-stationary case using coupling methods (distributional in the $V$-uniform geometrically ergodic case and a Markov coupling in the weighted Wasserstein case).
\par 
The paper is organized as follows. The main results are presented in \Cref{sec:main-results}. We divide it into two parts corresponding to two sets of conditions. In \Cref{sec:geom-v-ergod} we consider $V$-geometrically ergodic Markov chains, while in \Cref{sec:geom-ergod-mark} we consider Markov chains that converge geometrically in weighted Wasserstein distances. We discuss and compare our results with the literature in \Cref{sec:related-works}. The proofs are postponed to \Cref{sec:proofs}.



\paragraph{Notations}
Let $(\Xset,\Xsigma)$ be a measurable space and $\MK$ a Markov kernel on $(\Xset,\Xsigma)$. For a measurable function $V: \Xset \to \coint{1,\infty}$, define $\mrl_V$ as a set of all measurable functions $g: \Xset \to \rset$, which $\Vnorm[V]{g}= \sup_{x \in \Xset} \{| g(x) | / V(x) \} < \infty$. The
$V$-norm of a signed measure $\xi$ on $(\Xset,\Xsigma)$ is defined by
$\Vnorm[V]{\xi} = \int_{\Xset} V(x) \rmd \abs{\xi}(x)$, for
$V : \rset \to \coint{1,\plusinfty}$, where $\abs{\xi}$ is the absolute value of $\xi$. In the case $V \equiv 1$, the $V$-norm is the total variation norm and is denoted by $\tvnorm{\cdot}$. Equivalently (see \cite[Theorem D.3.2]{douc:moulines:priouret:soulier:2018} for details), $\Vnorm[V]{\xi}$ can be defined as $\Vnorm[V]{\xi} = \sup\{ \xi(g) \, :\, \Vnorm[V]{g}\leq 1\}$.

For each probability measure $\xi$ on $(\Xset,\Xsigma)$ we denote by
$\PP_{\xi}$ (resp. $\PE_{\xi}$) the probability (resp. the expected value) on the canonical space $(\Xset^\nset,\Xsigma^{\otimes \nset})$ such that the canonical process is
$\sequence{X}[n][\nset]$ is a Markov chain with initial probability $\xi$ and Markov kernel $\MK$. By convention, we set
$\PE_{x} = \PE_{\delta_x}$ for all $x \in \Xset$.

Denote for any $q \in \coint{1,\infty}$ the $2q$-th moment of the
standard Gaussian distribution on $\rset$ by  $\momentGq[q] = (2q)! / (q! 2^{q}) = 2^{q}\Gammabf((2q+1)/2)/\uppi^{1/2}$, where $\Gammabf$ is the Gamma function.