\section{Introduction}

We consider the problem of estimating an unknown vector $x_0 \in \RR^n$ from noisy observations
\begin{equation}\label{eq:ip}
  y = \Phi x_0 + w \in \RR^q ,
\end{equation}
where $\Phi$ is a linear operator from $\RR^n$ to $\RR^q$ and $w$ is the realization of a noise.
This linear model is widely used in imaging for degradation such that entry-wise masking, convolution, etc, or in statistics under the name of linear regression.
Typically, the inverse problem associated to~\eqref{eq:ip} is ill-posed, and one should add additional information in order to recover at least an approximation of $x_0$.

During the last decade, sparse regularization in orthogonal basis has become a classical tool in the analysis of such inverse problem, in particular in imaging~\cite{chen1999atomi,mallat2009a-wav} or in statistics and machine learning~\cite{tibshirani1996regre}.
The sparsity of some coefficients $x \in \RR^n$ is measured using the counting function, or abusively $\ell^0$ norm, which reads
\begin{equation*}
  \norm{x}_0 = \Card (\supp(x)) \qwhereq \supp(x) = \enscond{i \in \ens{1,\dots,n}}{x_i \neq 0} ,
\end{equation*}
where $\supp(x)$ is coined the support of the vector $x$.
The associated regularization
\begin{equation*}
  \uArgmin{x \in \RR^n} \frac{1}{2} \norm{y - \Phi x}_2^2 + \lambda \norm{x}_0
\end{equation*}
is however known to be NP-hard~\cite{natarajan1995sparse}.
A first way to alievate this issue is to consider greedy methods, such as the Matching Pursuit~\cite{mallat1993matching} or derivation from it as the OMP~\cite{pati1993orthogonal}, CoSAMP~\cite{needel2009cosamp}, etc.
This will not be the concern of this paper which focus on one of its most popular convex relaxation through the $\lun$-norm.
More precisely, we consider the Lasso optimization problem~\cite{tibshirani1996regre} which reads
\begin{equation}
  \label{eq:lasso}
  \uArgmin{x \in \RR^n} \frac{1}{2} \norm{y - \Phi x}_2^2 + \lambda \normu{x} ,
\end{equation}
where the $\lun$-norm is defined as $\normu{x} = \sum_{i=1}^n \abs{x_i}$.

In this work, we consider a more general framework, known as the sparse analysis prior, cosparse prior or generalized Lasso.
The idea is to not measure the sparsity of the coefficients in an orthogonal basis, but in any \emph{dictionary}.
Formally, a dictionary $D$ is a linear operator from $\RR^p$ to $\RR^n$ which is defined through $p$ $n$-dimensional \emph{atoms} $d_i$ which may be redundant.
Using this dictionary, one can build an analysis regularization which reads $\norm{D^* \cdot}_1$ associated to the variational framework defined as
\begin{equation}
  \label{eq:p}
  \Xl = \uArgmin{x \in \RR^n}h(x)= \frac{1}{2} \norm{y - \Phi x}_2^2 + \lambda \normu{D^* x} .
\end{equation}
This framework is known in the signal processing community as sparse analysis regularization~\cite{elad2007analysis,vaiter2011robust} or cosparse regularization~\cite{nam2012cosparse}.
Probably the most popular example of analysis sparsity-inducing regularizer is the Total Variation which was introduced in~\cite{rudin1992nonlinear} in a continuous setting for denoising.
In the discrete setting, it corresponds to take $D^*$ as a discretization of a derivative operator.
In the context of one-dimensional signals, a popular choice is to take a forward finite difference.
Other popular choices of dictionary includes translation invariant wavelets (which can be viewed as a higher order total variation following~\cite{steidl2004equivalence}) or the concatenation of a derivative operator with the identity, known under the name of Fused Lasso~\cite{tibshirani2005sparsity} in statistics.

When there is no noise, i.e. $y = \Phi x_0$, it is common to use a constrained version of~\eqref{eq:p} which reads
\begin{equation}
  \label{eq:p0}
  \Xo = \uArgmin{x \in \RR^n} \normu{D^* x} \qsubjq \Phi x = y .
\end{equation}
It has been first introduced in~\cite{chen1999atomi} under the name Basis
Pursuit for $D = \Id$, and one can easily see that~\eqref{eq:p0} can be recasted as linear program (LP).

It is important to keep in mind that $\Xl$, nor $\Xo$ is typically not a singleton.
Most of previous works in this area is concerned with the case where the solution set is a singleton, or to derive guarantees to enforce uniqueness.
Necessary and sufficient conditions has been derived in~\cite{zhang2015necessary,zhang2013one} and also in~\cite{gilbert2015solution} for the constrained case.
In this paper, we tackle the case where $\Xl$ is not a singleton, and we want to better understand the structure of the solution set in this case.
Some insights are given in~\cite{tibshirani2013uniqueness}, but the results are limited to the case where $D=\Id$.
In this work, the authors give a bound on the size of the support, and prove that the LARS algorithm converges to a solution with a maximal support.
To our knowledge, our work is the first to consider the analysis case.