% ==================================================
\section{Solving the Pentomino Problem}
% -----------------------------------
% -----------------------------------

\begin{figure*}[!htb]
\minipage[t]{0.468\textwidth}
  \includegraphics[width=\linewidth]{images/pentomino}
  \endminipage\hfill
\minipage[t]{0.532\textwidth}
  \includegraphics[width=\linewidth]{images/km_solved.png}
\endminipage\hfill
\caption{\emph{Left}: The Pentomino sprites and two examples illustrating the $true$ and $false$ classes.
  \emph{Right}: Test accuracy (median and inner quartiles, 10 trials) on the Pentomino problem with and without modern training advances. Note, log-scale of x-axis.}
\label{pento_sprites}
\end{figure*}

{\em Knowledge Matters} \cite{gulccehre2016knowledge} explores the extent to
which neural networks are able to learn problems given minimal
supervised information. Their formulation has a fully defined loss
function; however, the gradient of the loss with respect to the
parameters provides no direct information about potentially useful
subtasks such as segmentation, object classification, or counting. They concluded that the networks and training methods they tested converged to a
local minima.

The {\em Knowledge Matters} demonstration utilized the Pentomino dataset, which
is formed from a set of sprites \cite{gulcehre_2015}
shown in Figure \ref{pento_sprites}. The
dataset is generated by placing three sprites onto a canvas $C \in
\mathbb{R}^{64\TMS64}$. Each sprite undergoes a random rotation 
(\ang{0}, \ang{90}, \ang{180}, or \ang{270}) and integer scaling ($1\times$ or $2\times$).  The goal of the
neural network is to predict a 1 if the rotated and scaled sprites in an
image are the same and 0 otherwise. One possible solution to the
Pentomino problem is to learn to segment, classify, and count the number
of underlying objects in the image. The challenge (claimed impossible in \cite{gulcehre_2015})
is to find a solution using a generic network given only the binary
label for each image.

G\"{u}l\c{c}ehre et. al \cite{gulccehre2016knowledge} observed that
``black-box machine  learning  algorithms  could  not  perform  better 
than  chance on [the Pentomino problem].'' Decomposing
the problem into two stages however, made the task easily solvable. The
first stage in the decomposition was a classification
step, where extra label information was provided to the model. Given the
predicted classes, the second stage projected this output to the Bernouili
log-likelihood objective. Using some of the recent advances in 
DNN training, we are able to completely solve the original
problem demonstrated in {\em Knowledge Matters}; we do so without the
requirement of an intermediary model or the addition of extra
information. We also experimented with a reproduction of the model 
proposed in the paper and found that given enough time (over 1000
epochs) the model does make progress on the Pentomino problem, as shown
in Figure \ref{pento_sprites} in gray. This observation is in line with
recent insights of \cite{hoffer2017train} that discuss the effects of
training duration and batch size.

The fully-connected (fc) model presented in \cite{gulccehre2016knowledge} was composed of layer sizes
[2050, 11, 1024] and trained with ADADelta \cite{zeiler2012adadelta} and weight regularization. The 11-unit layer
served as a bottleneck to bring structural information into the
network.  We leverage four recent advances to solve the Pentomino problem: Batch
Normalization (BN) \cite{ioffe2015batch}, Exponential Linear Units
\cite{clevert2015fast}, the Adam optimizer \cite{kingma2014adam}, and
Xavier initializations \cite{glorot2010understanding}. In constrast to
the large model employed in \cite{gulccehre2016knowledge}, we use a
fully-connected network with layer sizing of
$[32, 64, 12, 32, 8]$; this translates to a 98.5\% reduction of the
total number of model parameters. Comparable in size to the largest
training sets used in \cite{gulccehre2016knowledge}, 486k
samples were used for training and 54k samples were held out for testing.

G\"{u}l\c{c}ehre et. al \cite{gulccehre2016knowledge} were only able to
train black-box (generic), fully-connected models to achieve 50\% accuracy on the
Pentomino dataset. Their best model, after significant
hyper-parameter search, resulted in a 5.3\% training and 6.7\% test error
on the 80k Pentomino training dataset. This performance was achieved 
via a two-stage network that induced structural information
into the neural network. On the same training set, we achieved a 1\% error using a black-box neural network with the
5-layer network described above.  Figure
\ref{pento_sprites} shows the training accuracy for the original
{\em Knowledge Matters} network (gray), our modification (blue), and our TypeNet model (green, see Section \ref{typenetdetails}) on
the Pentomino problem (note the log scale on the x-axis).
