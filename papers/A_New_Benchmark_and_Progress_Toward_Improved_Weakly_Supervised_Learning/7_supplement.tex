\newpage
\section{Supplement}

\subsection{TypeNet Configuration}
Figure \ref{typenet_fig_sup} shows the data flow for our model as configured for the All-Pairs problem.  The Tables \ref{global_config}, \ref{detail_config_1}, and \ref{detail_config_2} present the detailed network configuration (also found in the sample code distributed with the dataset generator):

\begin{figure*}[!htb]
  \scalebox{0.9}{
\begin{tikzpicture}
\begin{scope}[
  thick,decoration={
        markings,
        %mark=at position 0.5 with {\arrow{>}}
      }
    ]
\node[text={rgb:red,4;green,2;yellow,1}] (image) at (0,0) {image};
\node[text=blue] (conv) at (1.5, 0) {conv};
\draw[postaction={decorate}] (image.east) -- (conv.west);

\node[text=blue] (a11) at (3, -0.5) {$1\TMS1_2$};
\node[text=blue] (b11) at (3, 0.5) {$1\TMS1_1$};
\draw[postaction={decorate}] (conv.east) -- (a11.west);
\draw[postaction={decorate}] (conv.east) -- (b11.west);

\node (sm_3) at (5.0, -0.5) {\textsc{identity}};
\node (sm_4) at (5.0,  0.5) {\textsc{identity}};
\draw[postaction={decorate}] (a11.east) -- (sm_3.west);
\draw[postaction={decorate}] (b11.east) -- (sm_4.west);


\node (p_2) at (7,  0) {$+$};
\draw[postaction={decorate}] (sm_3.east) -- (p_2.west);
\draw[postaction={decorate}] (sm_4.east) -- (p_2.west);

\node (mp3) at (9.5, 1) {\textsc{MaxPool(3)}};
\node (mp5) at (9.5, -1) {\textsc{MaxPool(5)}};
\draw[postaction={decorate}] (p_2.east) -- (mp3.west);
\draw[postaction={decorate}] (p_2.east) -- (mp5.west);

\node (h_1) at (11.5,  0)  [label={[xshift=0.0cm, yshift=-0.7cm]$\sum\limits_{w,h}$}] {\ \ \ \ };
\node (h_2) at (11.5,  -1)  [label={[xshift=0.0cm, yshift=-0.7cm]$\sum\limits_{w,h}$}] {\ \ \ \ };
\node (h_3) at (11.5,  1)  [label={[xshift=0.0cm, yshift=-0.7cm]$\sum\limits_{w,h}$}] {\ \ \ \ };
%\node (h_1) at (11.5,  0) {$\sum\limits_{w,h}$};
%\node (h_2) at (11.5, -1) {$\sum\limits_{w,h}$};
%\node (h_3) at (11.5,  1) {$\sum\limits_{w,h}$};

\node (num) at (12.7,  0.85) {$[n]$};

\draw[postaction={decorate}] (p_2.east) -- (h_1.west);
\draw[postaction={decorate}] (mp3.east) -- (h_3.west);
\draw[postaction={decorate}] (mp5.east) -- (h_2.west);

\node[text=blue] (fc) at (13.5, 0) {$fc$};
\draw[postaction={decorate}] (h_1.east) -- (fc.west);
\draw[postaction={decorate}]  (h_2.east) -- ([yshift=-4pt] fc.west);
\draw[postaction={decorate}]  (h_3.east) -- ([yshift=4pt] fc.west);

%\path (a11) -- (b11) node [midway, sloped] {$\dots$};
%\path (mp3.south) -- (9.5, 0.10) node [midway, sloped] {$\dots$};

\end{scope}
\end{tikzpicture}
}
\caption{Model data-flow used for All-Pairs.}
\label{typenet_fig_sup}
\end{figure*}

\begin{table}[!htb]
  \centering%
    \begin{tabular}{ l l }
    parameter & value \\
    \hline
    $N_c$ & 4  \\
    $N_f$ & 4  \\
    $N_t$ & 2  \\
    $N_s$ & 3  \\
    $n$ & 64  \\
    \textsc{Ac} & \textsc{Elu}  \\
    \textsc{A} & \textsc{Identity}  \\
    \textsc{Spatial} & \{\textsc{Identity}, \textsc{MaxPool3x3}, \textsc{MaxPool5x5}\}  \\
    \textsc{Afc} & \textsc{Elu}  \\
    \end{tabular}
  \caption{Model level parameters for TypeNet as used to solve All-Pairs.}\label{global_config}
\end{table}

\noindent
For the activation, \textsc{A}$_i$, the most useful activations were found to be \textsc{Identity}, \textsc{Selu}, and
\textsc{SoftMax}.  \textsc{SoftMax} is in the feature, rather than spatial
dimension.  Via architecture search, \textsc{Identity} is the most generally useful activation, though \textsc{SoftMax} tended to reduce training times (probably because it forms a strong approximately-sparse bottleneck).  

The convolution block used \textsc{Elu} activation, no bias, batch norm (post-activation),
and padding to align the convolution filters with the image edges.  It's layer-wise characteristics are detailed below.  For larger images, a stride of 2 was used in \textsc{Conv}$_3$.
\begin{table}[!htb]
  \centering%
    \begin{tabular}{ l l }
    parameter & features, size, stride  \\
    \hline
    \textsc{Conv}$_1$ & 128, 3, 1  \\ 
    \textsc{Conv}$_2$ & 128, 5, 2  \\
    \textsc{Conv}$_3$ & 128, 5, 1  \\
    \textsc{Conv}$_4$ & 128, 3, 1  \\
    \end{tabular}
  \caption{Convolution block parameters for TypeNet as used to solve All-Pairs.}\label{detail_config_1}
\end{table}

\noindent
The fully-connected layers have input of size $m=N_s \TMS n$ and their configuration is detailed below:
\begin{table}[!htb]
  \centering%
    {\renewcommand{\arraystretch}{1.15}
    \begin{tabular}{ l l }
    parameter & value  \\
    \hline
    $fc_1$ & $m$-\textsc{Elu}-bnorm  \\
    $fc_2$ & $\floor*{\frac{m}{2}}$-\textsc{Elu}-bnorm  \\
    $fc_3$ & $\floor*{\frac{m}{4}}$-\textsc{Elu}-bnorm  \\
    $fc_4$ & $2$-\textsc{Identity}  \\
    \end{tabular}
    }
  \caption{Fully-connected parameters for TypeNet as used to solve All-Pairs.}\label{detail_config_2}
\end{table}

\subsection{TypeNet Architecture Search} \label{allpairsresult_sup}

The main hyper-parameters and architecture-variations explored are the
feature activation, number of branches ($k$), and number of features
($n$).  First, we explored the choice of activation with $n=64$ and
$k=2$.  All activation combinations drawn from the following options
were explored and the top results are presented in Figure
\ref{arch_search_fig} : \textsc{Elu} ($\mathsf{E}$), \textsc{Identity} ($\mathsf{I}$), \textsc{Relu} ($\mathsf{R}$), \textsc{Selu} ($\mathsf{Se}$),
\textsc{Sigmoid} ($\mathsf{S}$), \textsc{SoftMax} ($\mathsf{Sm}$), \textsc{SoftPlus} ($\mathsf{Sp}$), and \textsc{Tanh} ($\mathsf{T}$).
In each figure, architectures are labeled with $n$ when $n\neq64$, and the
above abbreviations of the $k$ activations are used.  If a ``$\dashw$'' is appended,
the architecture had a wider convolution receptive field (the stride of the
third \textbf{conv} layer was 2).
%\small
\begin{equation} \label{archnotation_sup:1}
%\mathtt{[n\#-] activation_1 [activation_2 ... activation_k]}.
\mathsf{[\ndash] Activation_1 [... Activation_k]}[\dashw].
\end{equation}
%\normalsize

All of the runs represented in Figure \ref{arch_search_fig} had
higher accuracy than any of the baselines.  The main conclusion from
these trials is that \textsc{SoftMax} and \textsc{Selu} are the most useful
activations.  We most frequently used \textsc{SoftMax} as the activation in
exploring the other hyper-parameters because of its low training
variance.

We studied how the number of branches, $k$, affects training; those
results are shown below with the number of training samples needed to
fully solve the 4-4 All-Pairs problem.  All trials reached 100\% accuracy,
save for one three-branch trial which got stuck at a test accuracy of
99.948\% after 30M training examples.  Based on the number of samples
needed to reach maximum test accuracy, we conclude that $k=2$ is best
for this problem.
\begin{center}
\begin{tabular}{ l l l }
branches ($k$) & accuracy  & training samples \\
\hline
 1 [$\times$9] & $1.0 \pm 0.0$                     & $57.1M \pm 3.8M$ \\
 2 [$\times$10] & $1.0 \pm 0.0$                   & $47.7M \pm 4.7M$    \\
 3 [$\times$20] & $1.0 \pm 10^{-4}$     & $49.4M \pm 8.9M$   \\
%  3 [$\times$20] & $0.99997 \pm 10^{-4}$     & $49.4M \pm 8.9M$   \\
\end{tabular}\label{branches}
\end{center}
The \textsc{SoftMax} activated network with two branches was found to train
faster for more features as summarized in the following table:  \\
\begin{center}
\begin{tabular}{ l l l }
features ($n$) & accuracy  & training samples \\
\hline
 48 [$\times$9] & $1.0 \pm 0.0$       & $57.0M \pm 8.9M$ \\
 64 [$\times$10] & $1.0 \pm 0.0$     & $47.7M \pm 4.7M$    \\
 96 [$\times$20] & $1.0 \pm 0.0$     & $40.5M \pm 7.7M$.   \\
\end{tabular} \\
\end{center}
All options consistently achieved 100\% test accuracy, so this trade-off
for the 4-4 problem can be made to optimize training time or inference
time.

\begin{figure*}[!htb]
\centering
\minipage{0.74\textwidth}
  \includegraphics[width=\linewidth]{images/arch_test_4_4}
\endminipage\hfill
\caption{4-4 All-Pairs for different activation functions, \textsc{A}$_i$.}
\label{arch_search_fig}
\end{figure*}

\subsection{More Details on the Harder All-Pairs Problems}

\begin{figure*}[!htb]
\minipage{0.23\textwidth}
  %\includegraphics[width=\linewidth]{images/missed_examples_77}
  %\includegraphics[width=\linewidth]{images/missed_examples_77_highlight}
  \includegraphics[width=\linewidth]{images/missed_examples_77_highlight_red}
  \endminipage\hfill
\minipage{0.74\textwidth}
  \includegraphics[width=\linewidth]{images/even_harder}
\endminipage\hfill
\caption{\emph{Left}: Examples of incorrect test samples from TypeNet $\mathsf{96{\text -}II{\text -}w}$ trained on 7-7 All-Pairs for 200M samples.  White symbols can be paired, leaving the red symbols unpaired.
  \emph{Right}: Test results of applying TypeNet to more difficult All-Pairs problems.  Wider \textbf{conv} receptive fields are notated with ``-$\mathsf{w}$'', see text for details. }
\label{missed_sup}
\end{figure*}

% ------------------------

The TypeNet approach cannot easily be made to solve every All-Pairs
problem; Figure \ref{missed_sup} shows results for the 5-5, 6-6, and 7-7 All-Pairs problem.
The \textsc{Identity} activation was the only activation to reach 100\% accuracy on the
5-5 and 6-6 problem, in 100\% (Fig\ref{missed_sup}-a) and 20\% (Fig\ref{missed_sup}-d) of trials respectively.  The \textsc{Selu} and \textsc{SoftMax} activation
were not successful on any of these problems in any trail within the 100M
training sample limit.

For these problems, the image size was increased
from $76\TMS76$ to $96\TMS96$ to make room for all the symbols.
This image size increase required decreasing the batch size from 600 to
400; all other training settings remained unchanged.  The large image size led us to expand the receptive field of the \textbf{conv} as notated with ``-$\mathsf{w}$'' and detailed in Section \ref{allpairsresult_sup}.  The most enlightening observations from these experiments are as follows:
\vspace{-4mm}
\begin{tightitemize}
\item The \textsc{Selu} activation (Fig\ref{missed_sup}-c) had lower accuracy
than expected from its effectiveness on the 4-4 problem.
\item On these harder problems, the \textsc{SoftMax} activation continued to
show lower variance across trials in both accuracy and training samples.
\item The $\mathsf{SmSm}$ model (Fig\ref{missed_sup}-b) consistently got stuck
at 94.6\% accuracy on the 5-5 problem, perhaps because the \textsc{SoftMax} activations are prone
to local minima.
\item The number of branches was increased to 3 and number of features to 128,
  independently and together, for the best case activations from smaller models.
  The $\mathsf{128{\text -}III}$ (Fig\ref{missed_sup}-f) model had the best test accuracy, but did worst than the simpler $\mathsf{II}$ model
  (Fig\ref{missed_sup}-e) even when trained to 200M training examples.
\item The 7-7 All-Pairs problem (Fig\ref{missed_sup}-g,h) is clearly
harder. The wider $\mathsf{96{\text -}II{\text -}w}$ (Fig\ref{missed_sup}-g) model was the best.
\item As shown in Figure \ref{missed_sup}-Left, the test samples missed by
one of the $\mathsf{96{\text -}II{\text -}w}$ models on the 7-7 problem are semantically similar: the
model incorrectly labels some samples as $true$ that have either an unpaired
\textbf{cross} and \textbf{3-star}, or an unpaired \textbf{theta} and \textbf{phi}.  For this model and trial, all of its errors fall into these two classes, though it correctly classifies some of those examples (achieving a 95\% accuracy when those two classes account for 9.5\% of the test set).  Different trials show different types of semantic errors.
\item Many variations, including mixtures of activations, more features, more branches, even wider \textbf{conv} receptive fields, and combinations of these choices, were tried to solve the 7-7 problem without success.  In the highest test accuracy observed (98\%), the misclassified images are still easy for a human to classify.
\end{tightitemize}

\subsection{Comparison to a Simple CNN}
\label{comptocnn}
\noindent
Are Lines 5 and 6 of Algorithm 1 generally useful, and do they improve the algorithm?
The table below compares (3 trials for each) the test accuracy and model size of TypeNet with a with a simple convolutional net (ConvNet) created by altering TypeNet as follows:
\begin{itemize}
\item Replace Lines 5 and 6 of Algorithm 1 with \textsc{Flatten} (passing the convolution output directly to the fully-connected layers).
\item As with larger All-Pairs images, use a stride of 2 in \textsc{Conv}$_3$.
\end{itemize}

%\begin{center}
%\begin{tabular}{ r | c c | c c  }
%              & \multicolumn{2}{c|}{ConvNet}                        & \multicolumn{2}{c}{TypeNet}  \\
%dataset  & accuracy & \# parameters & accuracy & \# parameters \\
%\hline
%MNIST               & 0.995167 $\pm$ 0.0004    & 1.39M & \textbf{0.9971 $\pm$ 0.0006} & \textbf{1M}  \\
%Fashion-MNIST & \textbf{0.938133 $\pm$ 0.0008}    & 1.39M & 0.9346 $\pm$ 0.0011 & \textbf{1M} \\
%CIFAR10           & 0.767500 $\pm$ 0.0028     & 1.39M & \textbf{0.8820 $\pm$ 0.0080} & \textbf{1M} \\
%4-4 All-Pairs      & 0.754600 $\pm$ 0.0000     & 3.45M & \textbf{1.0000 $\pm$ 0.0000} & \textbf{1M} \\
%\end{tabular}
%\end{center}

\begin{center}
\begin{tabular}{ r | c c | c c  }
              & \multicolumn{2}{c|}{ConvNet}                        & \multicolumn{2}{c}{TypeNet}  \\
dataset  & accuracy & \# parameters & accuracy & \# parameters \\
\hline
MNIST               & 0.9953 $\pm$ 0.0002    & 2M & \textbf{0.9971 $\pm$ 0.0006} & \textbf{1M}  \\
Fashion-MNIST & \textbf{0.9409 $\pm$ 0.0005}    & 2M & 0.9346 $\pm$ 0.0011 & \textbf{1M} \\
CIFAR10           & 0.7773 $\pm$ 0.0013     & 2.5M & \textbf{0.8820 $\pm$ 0.0080} & \textbf{1M} \\
4-4 All-Pairs      & 0.8080 $\pm$ 0.0925     & 9.9M & \textbf{1.0000 $\pm$ 0.0000} & \textbf{1M} \\
\end{tabular}
\end{center}

\noindent
From this comparison, TypeNet is seen to have fewer parameters and shows significant improvements in accuracy for the hardest two datasets (CIFAR10 and 4-4 All-Pairs).  The number of parameters in TypeNet is not dependent on the input size because of the spatial summation in Line 6 of the algorithm.  We anticipate the spatial, learned histogram of TypeNet to be a useful tool in the construction of other DNN architectures.

%\raggedbottom
%\vfill

