% ==================================================
\section{Introduction}
% -----------------------------------
Deep neural networks are powerful functional approximators, allowing for
the learning of complex tasks that were not solvable by traditional
machine learning methods. Recently, \cite{gulccehre2016knowledge}
suggested that there exist problems that neural networks would not be
able to solve without the guidance of human insight; they define and
study the Pentomino problem as an example of this class of problems. 
For the Pentomino problem, we demonstrate that extra
knowledge is not necessary by solving the problem with a small, deep neural network (DNN).
Having found a solution to Pentomino, we introduce a new, scalable problem and present progress toward its solution.

% -----------------------------------
To understand the limits of weakly supervised learning applied to generic models, we divide the task of solving a problem into the application of known
techniques and the engineering of the system (the model plus training data and proceedure).  The palette of known
techniques is constantly improving and is what enables solving the
Pentomino problem with current techniques.  The incorporation of explicit
knowledge into an engineered solution can be estimated by how many problem
specifics can be inferred/discovered by an inspection of the model architecture and the training
procedure.  Common deep-learning techniques are a codification of
knowledge into reusable components which require minimal insight to
select.  For instance, batch norm \cite{ioffe2015batch} speeds
convergence and reduces hyper-parameter sensitivity, jump connections
\cite{srivastava2015training, he2016deep} enable deeper models,
convolutions \cite{lecun1998gradient,krizhevsky2012imagenet} are useful
in spatially-invariant vision problems, and sparse activations
\cite{srivastava2014dropout, wan2013regularization} reduce overfitting
by restricting the flow of information.  Even simple observations, like these,
allow the practitioner to select components suitable for the problem. 
These techniques are all excellent examples of knowledge refined into
heuristically selectable, generic techniques.

% -----------------------------------
We lack ready-to-apply techniques for some problems and much of the
research in the field moves us toward more turn-key application of
learning algorithms; for example, the latest AlphaGo
\cite{silver2017mastering} is trained without expert human examples. 
Some examples of the work done to engineer problems with human knowledge
are engineering model sub-components to include problem details and 
adding sub-goal labels or objective functions.  Successful engineering of a 
solution for a particular problem can lead to either a specific solution only 
applicable to the problem studied or, more usefully, to broadly reusable 
techniques or insights.  The later outcome is our goal in presenting the following 
contributions:
\begin{enumerate}
  \item demonstration of solving the Pentomino problem from {\em Knowledge Matters} \cite{gulccehre2016knowledge} with conventional techniques (both model and training)
  \item new, scalable challenge problem, All-Pairs, with (effectively) infinite data \cite{allpairs2018}
  \item sampling of existing techniques' performance on All-Pairs as baselines
  \item new, generic model, TypeNet \cite{allpairs2018}, which out-performs the baselines on All-Pairs.
\end{enumerate}
The All-Pairs dataset generator and TypeNet reference code are available at \href{https://github.com/apple/ml-all-pairs}{https://github.com/apple/ml-all-pairs}.
