% ==================================================
\section{Toward an All-Pairs Solution}
% -----------------------------------

\begin{algorithm}
\label{tn_algo_block}
\caption{TypeNet algorithm}
\SetAlgoNoEnd
\SetKwComment{tcp}{\# }{}
    %\KwIn{An image, \textsc{Image}}
    %\KwOut{Model prediction}
    \KwData{ 
    	\begin{itemize}
		\renewcommand\labelitemi{--}
		\setlength\itemsep{-0.2em}
		\item Number of layers, $N_c$ and $N_f$.
		\item Number of type branches, $N_t$, and spatial branches, $N_s$.
		\item Activations, \textsc{Ac}, and convolutions, \textsc{Conv}, for feature extraction layers.
		\item Activations, \textsc{A}, and $n$ 1x1 convolutions, \textsc{Conv1$\TMS$1}, for type matching.
		\item Spatial diversity operations, \textsc{Spatial}.
		\item Activations, \textsc{Afc}, weights, $W$, and biases, $B$, for fully-connected layers.
	\end{itemize}
    }
    %\underline{function TypeNet(\textsc{Image})} \\

$C = \textsc{Image}$ \tcp*[f]{convolution block} \\
\For{$i=[1 \to N_c)$}{
	$C = \textsc{Ac}_i(\textsc{Conv}_i(C))$ \\
	$C = \textsc{BatchNorm}(C)$
}
\BlankLine
T = $\sum_{i=0}^{N_t}{\textsc{A}_i(\textsc{Conv1$\TMS$1}_i(C))}$
\BlankLine
Y = \textsc{Concatenate}\Big(\big[ $\sum_{w,h}{\textsc{Spatial}_i(T)}$ for $i = [0 \to N_s)$ \big]\Big)

\BlankLine
\For(\tcp*[f]{fully-connected layers}){$i=[0 \to N_f)$} { 
	$Y = \textsc{Afc}_i(W_i Y + B_i)$ \\
	$Y = \textsc{BatchNorm}(Y)$
}

\KwRet{$\textsc{SoftMax}(Y)$}
\end{algorithm}

%\tikzstyle{vertex}=[auto=left,circle,fill=black!25,minimum size=20pt,inner sep=0pt]

%\begin{tikzpicture}
%  \node(n4)  at (3,4)  {$n_4$};
%  \node (n5)  at (3,6)  {$n_5$};
%
%
%  \path (n4) -- (n5) node [red, font=\Huge, midway, sloped] {$\dots$};
%
%\end{tikzpicture}

\subsection{Type-Net Model} \label{typenetdetails}

After verifying that a fully-connected model can easily solve the 4-4 All-Pairs
problem from the histogram of symbols in each
image, we designed and tested a generic model capable of
learning a similar, whole-image statistic.  The resulting model was created
using insights derived from the All-Pairs problem, but does not make use of
explicit problem details or enhanced training data.

We refer to the resulting network as a TypeNet because it estimates the affinity
of each receptive field to $n$ ideal types (via a dot-product) and then
aggregates those type-affinities over the spatial extent.  This spatial
summation is global for solving the All-Pairs problem, but could
be spatially restricted to produced learned features similar to
histogram of gradients (HOG) found in \cite{mcconnell1986method}.  A learned attention mask could also generalize the summation to salient areas of each image.

Model details can be found in the supplementary material and in the online sample code found at \cite{allpairs2018}.  The general algorithm for TypeNet is presented in Algorithm \ref{tn_algo_block}.  The algorithm begins and ends conventionally with a convolution stack and fully-connected layers, respectively.  Lines 5 and 6 show the key steps for the algorithm:
\begin{itemize}
\item line 5, the $1\TMS$1 convolution implements a dot-product similarity with a learned kernel, these are the ``types'' of TypeNet.
\item line 5, the activation, \textsc{A}$_i$, applied was experimentally studied:
	\begin{itemize}
	\item \textsc{A}$_i$ = \textsc{SoftMax} in the feature dimension, gives a soft $N_t$-hot representation here which was seen to reduce variance in training times).
	\item \textsc{A}$_i$ = \textsc{Identity} was the most versatile activation and can be seen as creating a ``type'' difference operator
	\end{itemize}
\item line 5, superposition (via summation) of learned template matching
\item line 6, diversify spatially with non-linear operators such as \textsc{MaxPool}.
\end{itemize}

The goal of introducing TypeNet is to expand the palette of
techniques available to solve similar types of problems and decrease
the problem specific reasoning required in similar
domains (such as parity, counting, holistic scene understanding, and
visual query answer), which can be solved from a histogram-like summary of
local statistics.

\subsection{Contrast to Relational Methods}

Relational neural learning generally accomplishes it's goal by
learning a functional over $(i,j)$ tuples in a latent feature space $f$. In
Relational Networks \cite{santoro2017simple} for example, the
model learns two functionals $[h, g]$ (parameterized by
deep-neural networks) that \textbf{exhaustively} operate over \textbf{all} $(i,j)$ pairs
in the latent feature space of a deep-convolutional network as shown
in the table below.
Memory Networks \cite{weston2014memory} on the other hand learn a probabilistic relationship
between the input query (embedded into a feature representation) $f_i$ and an
associated set of memory vectors $M = \{m_1, ... m_i, m_N \}$, followed by a smoothed weighting against an embedded query
vector $c_i$.
% $p_i = \text{softmax}(f_i, m_i) ;  o_i = \sum_i p_i c_i$.
% \vskip -0.2in
% \begin{align}
%   p_i = \text{softmax}(f_i, m_i) \hspace{1in}    o_i = \sum_i p_i c_i
    %   \end{align}
\vskip -0.2in
{\renewcommand{\arraystretch}{1.15}
\begin{center}
\begin{tabular}{ c | c } \hline
  Relational Networks & Memory Networks \\ \hline
  $g(\sum_i \sum_j h(f_i, f_j))$ & $p_i = \text{softmax}(f_i^T,
                                         m_i) \hspace{0.2in}    o_i =
                                         \sum_i p_i c_i$
\end{tabular}\label{rel_mem_net}
\end{center}
}                                     


Relational Networks \cite{santoro2017simple} have high computational
complexity when the dimensionality of the feature-space
$f$ is large. Memory-networks on the other hand scale proportionate
to the number of embedded memories dim($M$). Our objective with TypeNet
is twofold: relax computational constraints compared to these relational models and incorporate the probabilistic smoothing of Memory Networks \cite{weston2014memory}.

We reduce the computational complexity by forcing the model to divide
the input representation through a set of $N_t$ branches. This
division allows the model to learn a disparate feature
representation per branch. Rather than learning over every $(i,j)$ as in Relational Networks \cite{santoro2017simple} we
approximate this with a spatial sum after our branch-divide
strategy.

During our branching strategy we do a sum across an activated feature
space; this can be interpreted as a probabilistic weighting of the
features of each individual branch against each other. Training TypeNet to convergence is \textbf{8-10 times faster} than training a comparable Relational Network and produces \textbf{more accurate results} in the
weak-supervised learning scenario of All-Pairs.

\subsection{All-Pairs Result} \label{allpairsresult}

\begin{figure*}[!htb]
  \includegraphics[width=\linewidth]{images/unified}
\caption{Training results, showing validation accuracy and total number of training sample for TypeNet on increasingly difficult versions of All-Pairs, from 4-4 to 7-7.  Shading shows the distribution over 10 trials.  Note, conventional DNN models cannot solve the 4-4 problem.}
\label{solving_all_pairs}
\end{figure*}

As described, the TypeNet for the All-Pairs problem has 1.04M trainable parameters
(many times smaller than the baseline models).
Unless otherwise noted, we used the following training setup for the
TypeNet results on the All-Pairs problem: 4 GPUs, batch size 600, Adam
with learning rate = 0.001 and no weight decay, cross-entropy loss, test
results reported every 50k training samples, and 100M total training samples.
A 100M sample training run typically takes 20 hours on 4$\TMS$P100 GPUs.

The main hyper-parameters and architecture-variations explored are the
feature activation, number of branches ($k$), and number of features
($n$).  Details of those studies can be found in the supplementary material.  We concluded that $k = 2$ and $n = 64$ performed well on the All-Pairs problem.  Increasing the number of features to 96 results in slightly lower training times, at a cost of a larger model.  All options explored reached 100\% accuracy.

The TypeNet approach cannot easily solve every All-Pairs
problem; Figure \ref{solving_all_pairs} shows results for the 4-4 to 7-7 All-Pairs problem.
We see an increase in the magnitude and variance of the number of samples needed for convergence.  The plot shows the results of 10 training runs for each difficulty level; TypeNet can solve the first 3 of these challenges to 100\% validation accuracy.  No model and training methodology has been found that solves the 7-7 problem to 100\% accuracy.  An inspection of the errors made by the best 7-7 solution shows that they are systematic, unambiguous errors.


\begin{figure*}[!htb]
\centering
\minipage{0.6\textwidth}
  \includegraphics[width=\linewidth]{images/train_sizedata}
\endminipage\hfill
\caption{Training the TypeNet on 4-4 All Pairs with 100M samples drawn from a fixed-size training set.}
\label{data_setsize_fig}
\end{figure*}


% ------------------------
\subsection{Training Set Size}

In Figure \ref{data_setsize_fig}, we show the effect of reducing the
cardinality of the training data from effectively infinite to sizes
smaller than the total number of training samples presented.  A training
set cardinality of 100k is minimal for 90\% test accuracy and 500k is
minimal for some trials to reach 100\% test accuracy.  The increased variance in both train and test accuracy at
cardinality 30k is interesting.  We hypothesize this is due to sampling
noise for these small sizes leading to significantly different train and
test distributions.  For larger cardinality, both sets consistently
represent the same distribution; for smaller sets, learning is
limited enough that distribution differences are not apparent.

To avoid the overhead of datasets on disk, varying the training set
cardinality is accomplished using an array of seeds for the data
generator.  Each seed is used to generate 1k samples.  When each seed in
the list has been used once, the list is shuffled and the process
starts back at the beginning of the list.

%\raggedbottom
%\vfill
\subsection{Other Applications}

TypeNet was evaluated on other datasets to determine its applicability
to common classification problems.  The following table presents results for the test
accuracy from four training runs.  For training, each dataset was augmented by random original-size
crops (padding of 4), random rotations from \ang{0} to \ang{4}, and normalized by subtracting 0.5.
CIFAR10 and Fashion MNIST \cite{fashionref}
were also augmented with random horizontal flips.  A detailed discussion and comparison with a simple convolutional net can be found in Supplement \ref{comptocnn}.

\begin{center}
\begin{tabular}{ r | c c | c c  }
              & \multicolumn{2}{c|}{ConvNet}                        & \multicolumn{2}{c}{TypeNet}  \\
dataset  & accuracy & \# parameters & accuracy & \# parameters \\
\hline
MNIST               & 0.9953 $\pm$ 0.0002    & 2M & \textbf{0.9971 $\pm$ 0.0006} & \textbf{1M}  \\
Fashion-MNIST & \textbf{0.9409 $\pm$ 0.0005}    & 2M & 0.9346 $\pm$ 0.0011 & \textbf{1M} \\
CIFAR10           & 0.7773 $\pm$ 0.0013     & 2.5M & \textbf{0.8820 $\pm$ 0.0080} & \textbf{1M} \\
4-4 All-Pairs      & 0.8080 $\pm$ 0.0925     & 9.9M & \textbf{1.0000 $\pm$ 0.0000} & \textbf{1M} \\
\end{tabular}
\end{center}

%\begin{center}
%\begin{tabular}{ l c c l }
%dataset  & accuracy & stdev \\
%\hline
%MNIST  & 0.9971 & 0.0006  \\
%Fashion-MNIST & 0.9346 & 0.0011 \\
%CIFAR10 & 0.882 & 0.008 \\
%\end{tabular}
%\end{center}

For these classification
tasks, adding more spatial information via two parallel pathways branching
from the \textbf{similarity} step (algorithm line 5) and joining at the \textbf{concatenation} step (line 6)
was useful.  One of these extra pathways has a \textsc{MaxPool3x3} and the other has \textsc{AvgPool3x3} after
the \textbf{similarity} step.  This enhanced model also solves the All-Pairs problem and has 10\% more parameters than the simpler TypeNet presented as a minimal version for All-Pairs.
