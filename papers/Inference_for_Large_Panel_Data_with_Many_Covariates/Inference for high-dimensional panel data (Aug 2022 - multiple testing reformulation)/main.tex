\documentclass[11pt]{article}

\usepackage[margin=1in]{geometry}


\usepackage{amsmath}
\usepackage{lipsum}
\usepackage{graphicx,psfrag,epsf}
\usepackage{enumerate}
\usepackage{rotating}
%\usepackage{natbib}
\usepackage{multirow,booktabs,threeparttable,color}
\usepackage{url} % not crucial - just used below for the URL 
\usepackage{amsfonts, amssymb, amsmath,bm}
\usepackage{url}
\usepackage{hyperref}
\usepackage{fancyhdr}
\usepackage{multicol}
\usepackage{courier} % For inline code
\usepackage{float}
\usepackage{subcaption}
\usepackage{titlesec} % Allow customized subsections with no new line
\usepackage{tikz}
\usetikzlibrary{automata,arrows,positioning,calc}
\usepackage{listings}
\usepackage{indentfirst} % Indent first sentence of a new section.
\usepackage{tablefootnote}
\usepackage[english]{babel}
\usepackage{bbm}
\usepackage[longnamesfirst]{natbib}
\usepackage{enumitem}
\usepackage{pdfpages}

\usepackage[margin=10pt,labelfont=bf]{caption}
\captionsetup[table]{labelsep=none}
\usepackage{subcaption}
\usepackage{float}
\usepackage{setspace} % for \onehalfspacing
\usepackage{amsmath,amsthm,amssymb,amsfonts,graphicx,dsfont}
\usepackage[maxfloats=100]{morefloats}
\usepackage[utf8]{inputenc}
\usepackage{booktabs}
\usepackage{epsfig}
\usepackage{url}
\usepackage{bm}
\usepackage{listings}
\usepackage{verbatim}
\usepackage{xcolor}
\usepackage[title]{appendix}
\usepackage{multirow}
\usepackage{longtable}
\usepackage{adjustbox}
%\usepackage[longnamesfirst]{natbib}



\newcommand{\cp}{^c}
\newcommand{\q}{\text{\quad}}
\newcommand{\wh}{\text{where }}
\newcommand{\df}{\mathrm{d}}
\newcommand{\dm}{\mathcal{D}}
\newcommand{\fc}[1]{\sigma^2(#1)}
\newcommand{\id}[1]{\mathbb{I}\{#1\}}
\newcommand{\prob}{\mathbb{P}}
\newcommand{\pa}{\partial}
\newcommand{\II}{\mathbf{I}}
\newcommand{\oo}{\bm{1}}
\newcommand{\LIM}{\lim\limits_{n\to\infty}}
\newcommand{\NN}{\mathbb{N}}
\newcommand{\PP}{\mathbb{P}}
\newcommand{\Pp}{\bm{P}}
\newcommand{\Aa}{\{a_n\}}
\newcommand{\Bb}{\{b_n\}}
\newcommand{\Ca}{\mathcal{C}}
\newcommand{\Da}{\mathcal{D}}
\newcommand{\CL}{\textbf{Claim: }}
\newcommand{\PF}{\textbf{Proof: }}
\newcommand{\QUE}{\textbf{Question: }}
\newcommand{\SOL}{\textbf{Solution: }}
\newcommand{\M}{\mu}
\newcommand{\sgn}{\textrm{sign}}
\newcommand{\SE}{\sum_{n=1}^{\infty}}
\newcommand{\PS}{\sum_{k=1}^{n}}
\newcommand{\FF}{\mathbb{F}}
\newcommand{\Ball}{B_\epsilon(x)}
\newcommand{\CC}{\mathbb{C}}
\newcommand{\BD}{\partial X}
\newcommand{\rr}{\mathbf{r}}
\newcommand{\QQ}{\mathbb{Q}}
\newcommand{\RR}{\mathbb{R}}
\newcommand{\XX}{\mathbf{X}}
\newcommand{\xx}{\mathbf{x}}
\newcommand{\IN}{^{(i)}}
\newcommand{\TN}{^{(t)}}
\newcommand{\JN}{^{(j)}}
\newcommand{\YY}{\mathbf{Y}}
\newcommand{\yy}{\mathbf{y}}
\newcommand{\YT}{\tilde{\mathbf{Y}}}
\newcommand{\ZZ}{\mathbb{Z}}
\newcommand{\WW}{\mathbf{W}}
\newcommand{\LL}{\mathcal{L}}
\newcommand{\Hh}{\mathbf{h}}
\newcommand{\Gg}{\mathbf{g}}
\newcommand{\ee}{\mathbf{e}}
\newcommand{\Uu}{\mathbf{u}}
\newcommand{\Yy}{\mathbf{y}}
\newcommand{\Mo}{\bm{\mu_1}}
\newcommand{\Mt}{\bm{\mu_2}}
\newcommand{\UM}{\overrightarrow{\rightarrow}}
\newcommand{\st}{\text{s.t.}}
\newcommand{\tr}{\mathrm{tr}}
% Probability statement
\newcommand{\CD}{\stackrel{D}{\rightarrow}}
\newcommand{\CP}{\stackrel{p}{\rightarrow}}
\newcommand{\AS}{\stackrel{a.s.}{\rightarrow}}
\newcommand{\iid}{\stackrel{iid}{\sim}}
\newcommand{\io}{\text{ i.o.}}
\newcommand{\XI}{X_\infty}
\newcommand{\EE}{\mathbb{E}}
\newcommand{\FC}{\mathcal{F}}
\newcommand{\BC}{\mathcal{B}}
\newcommand{\NL}{\\[.25cm]}
\newcommand{\floor}[1]{\left\lfloor #1 \right\rfloor}
\newcommand{\indep}{\rotatebox[origin=c]{90}{$\models$}}
\newcommand{\notindep}{\not\!\perp\!\!\!\perp }

\newcommand{\FD}{\text{FDR}}
\newcommand{\PO}{\text{Power}}
\newcommand{\Un}{\text{Unif[0,1]}}
\newcommand{\tT}{\mathbf{t}}
\newcommand{\erf}{\text{erf}}
\newcommand{\erfc}{\text{erfc}}
\newcommand{\erfinv}{\text{erf}^{-1}}
\newcommand{\erfcinv}{\text{erfc}^{-1}}
\usepackage{rotating} % <-- HERE

\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

\newtheorem{definition}{DEFINITION}
\newtheorem{example}{EXAMPLE}
\newtheorem{assumption}{ASSUMPTION}
%\numberwithin{assumption}{section}
\newtheorem{theorem}{THEOREM}
\newtheorem{app-theorem}{APPENDIX THEOREM}
\newtheorem{proposal}{PROPOSAL}
\newtheorem{lemma}{LEMMA}
\newtheorem{corollary}{COROLLARY}
\newtheorem{procedure}{PROCEDURE}


\def\cmt#1{{\textcolor{red}{(#1)}}}
% \usepackage{ulem}

\newcommand\tcaptab[1]{\captionsetup{position=top, font=normalsize, labelfont=bf, textfont=normalfont, justification=centering, margin=0mm, aboveskip=1mm, belowskip=0mm, labelsep=colon, singlelinecheck=false}\caption{#1}}
\newcommand\bnotetab[1]{\captionsetup{position=bottom, font=footnotesize,  textfont=normalfont, margin=1mm, skip=2mm, justification=justified, singlelinecheck=false}\caption*{#1}}
\newcommand\tcapfig[1]{\captionsetup{position=top, font=normalsize, labelfont=bf, textfont=normalfont, justification=centering, margin=0mm, aboveskip=2mm, belowskip=0mm, labelsep=colon, singlelinecheck=false}\caption{#1}}
\newcommand\bnotefig[1]{\captionsetup{position=bottom, font=footnotesize,  textfont=normalfont, margin=1mm, skip=2mm, justification=justified, singlelinecheck=false}\caption*{#1}}
\newcommand\subcap[1]{\captionsetup{position=bottom, font=small, labelfont=bf, textfont=normalfont, justification=RaggedRight, margin=0mm, aboveskip=-5mm, belowskip=0mm, labelsep=space, singlelinecheck=false}\caption{#1}}
\newcommand\subcaptab[1]{\captionsetup{position=bottom, font=small, labelfont=bf, textfont=normalfont, justification=centering, margin=0mm, aboveskip=4mm, belowskip=0mm, labelsep=space, singlelinecheck=false}\caption{#1}}






\begin{document}
	
	\title{Inference for Large Panel Data with Many Covariates\thanks{\scriptsize We thank Siddhartha Chib
			and Ruoxuan Xiong, as well as conference and seminar participants at Stanford, the California Econometric conference and the NBER-NSF SBIES conference for helpful comments. Jiacheng Zou gratefully acknowledges the generous support by the MS\&E Departmental Fellowship, and Charles \& Katherine Lin Fellowship.}}
	
	\date{December 31, 2022}
	\author{Markus Pelger\thanks{\scriptsize Stanford University, Department of Management Science \& Engineering, Email: mpelger@stanford.edu.}
		\and
		Jiacheng Zou\thanks{\scriptsize Stanford University, Department of Management Science \& Engineering, Email: jiachengzou@stanford.edu}
	}
	
	
	\onehalfspacing
	
	\begin{titlepage}
		\maketitle
		\thispagestyle{empty}
		
		\begin{abstract}
			
			
			%This paper proposes a new method for covariate selection in large dimensional panels. We develop the inferential theory for large dimensional panel data with many covariates by combining post-selection inference with a new multiple testing method specifically designed for panel data. 
			%Our procedure Panel-PoSI first provides valid p-values for post-selection LASSO estimation, which are then cross-sectionally combined with a data-driven adjustment for panel multiple testing. Our novel data-driven hypotheses are conditional on the sparse covariate selection and valid for any regularized estimator. %The multiple testing adjustment uses an adaptive simultaneity count through panel localization. 
			%Based on panel localization, our method controls for family-wise error rates for the covariate discovery and allows us to test unordered and nested families of hypotheses for large cross-sections. Empirically,  
			%we select a small number of asset pricing factors that outperforms the benchmarks in explaining out-of-sample a large cross-section of investment strategies.
			
			%This paper proposes a new method for covariate selection in large dimensional panels. We develop the inferential theory for large dimensional panel data with many covariates by combining post-selection inference with a new multiple testing method specifically designed for panel data. 
			%Our novel data-driven hypotheses are conditional on sparse covariate selections and valid for any regularized estimator. Based on our panel localization procedure, we control for family-wise error rates for the covariate discovery and can test unordered and nested families of hypotheses for large cross-sections. As an easy-to-use and practically relevant procedure, we propose Panel-PoSI, which combines the data-driven adjustment for panel multiple testing with valid post-selection p-values of a generalized LASSO, that allows \cmt{us} to incorporate priors. In an empirical study, we select a small number of asset pricing factors that explain a large cross-section of investment strategies. Our method dominates the benchmarks out-of-sample due to its better control of false rejections\cmt{ delete: and detections}.   
			This paper proposes a novel testing procedure for selecting a sparse set of covariates that explains a large dimensional panel. Our selection method provides correct false detection control while having higher power than existing approaches. We develop the inferential theory for large panels with many covariates by combining post-selection inference with a novel multiple testing adjustment. Our data-driven hypotheses are conditional on the sparse covariate selection. We control for family-wise error rates for covariate discovery for large cross-sections. As an easy-to-use and practically relevant procedure, we propose Panel-PoSI, which combines the data-driven adjustment for panel multiple testing with valid post-selection p-values of a generalized LASSO, that allows us to incorporate priors. In an empirical study, we select a small number of asset pricing factors that explain a large cross-section of investment strategies. Our method dominates the benchmarks out-of-sample due to its better size and power.
			
			
			\vspace{1cm}
			
			\noindent\textbf{Keywords:}  panel data, high-dimensional data, LASSO, number of covariates, post-selection inference, multiple testing, adaptive hypothesis, step-down procedures, factor model
			
			
			\noindent\textbf{JEL classification:} C33, C38, C52, C55, G12
		\end{abstract}
	\end{titlepage}
	
	
	\onehalfspacing 
	
	
	
	
	
	\section{Introduction}
	
	
	
	%1 Motivational paragraph
	
	Our goal is the selection of a parsimonious sparse model from a large set of candidate covariates that explains a large dimensional panel. This problem is common in many social science applications, where a large number of potential covariates are available to explain the time-series of a large cross-section of units or individuals. An example is empirical asset pricing, where the literature has produced a “factor zoo” of potential risk factors to explain the large cross-section of stock returns. This problem requires a large panel, as a successful asset pricing model should explain the many available investment strategies, resulting in a large panel of test assets. At the same time, there is no consensus about what are the appropriate risk factors, which leads to a statistical selection problem from a large set of candidate covariates. So far, the literature has only provided solutions to one of the two subproblems, while keeping the dimensionality of the other problem small. Our paper closes this gap.
	
	
	%2. Challenges
	
	The inferential theory on a large panel with many covariates is a challenging problem. As a first step, we have to select a sparse set of covariates from a large pool of candidates with a regularized estimator. The challenge is to provide valid $p$-values from this estimation that account for the post-selection inference. Furthermore, researchers might want to impose economic priors on which variables should be more likely to be selected. The second challenge is that the panel cross-section results in a large number of $p$-values. Hence, some of them are inadvertently very small, which if left unaddressed leads to ``$p$-hacking''.
	The multiple testing adjustment conditional on the selected subset of covariates from the first step is a novel problem, and requires to redesign what hypotheses should be tested jointly. A naive counting of all tests is overly conservative, and the test design and simultaneity counts should to be conditional on the covariate selection. %\cmt{ Since we are focusing on selection from large set of observed explanatory variables, rather than constructing a lean set of latent factors, this is an unavoidable challenge.}
	
	%3. Contribution of this paper
	
	This paper proposes a new method for covariate selection in large dimensional panels, tackling all of the above challenges. We develop the inferential theory for large dimensional panel data with many covariates by combining post-selection inference with a new multiple testing method specifically designed for panel data. Our novel data-driven hypotheses are conditional on sparse covariate selections and valid for any regularized estimator. Based on our panel localization procedure, we control for family-wise error rates for the covariate discovery and can test unordered and nested families of hypotheses for large cross-sections. As an easy-to-use and practically relevant procedure, we propose Panel-PoSI, which combines the data-driven adjustment for panel multiple testing with valid post-selection $p$-values of a generalized LASSO, that allows to incorporate priors. In simulations and an empirical study we show that our selection method provides correct false detection control but has substantially higher power than existing approaches.
	
	Our paper proposes the novel conceptual idea of data-driven hypotheses families for panels. This allows us to put forward a unifying framework of valid post-selection inference and multiple testing. Leveraging our data-driven hypotheses family, we adjust for multiple testing with a localized simultaneity count, which increases the power, while maintaining false discovery rate control. An essential step for a formal statistical test is to formulate the hypothesis. This turns out to be non-trivial for a large panel with a first stage selection step for the covariates. It is a fundamental insight of our paper, that the hypothesis of our test has to be conditional on the selected set of active covariates of the first stage. Once we have defined the appropriate hypothesis, we can deal with the multiple testing adjustment, which by construction is also conditional on the selection step.  
	
	Our method is a disciplined approach based on formal statistical theory to construct and interpret a parsimonious model. It goes beyond the selection of a sparse set of covariates as it also provides the inferential theory. This is important as it allows us to rank the covariates based on their statistical significance and can also be applied for relatively short time horizons, where cross-validation for tuning a regularization parameter might not be reliable. We answer the question which covariates are needed to explain the full panel jointly, and can also accommodate ``weak'' covariates or factors that only affect a small subset of the cross-sectional units. %\cmt{In the simulations, we demonstrate how our method can pick up the weak covariates}. 
	
	%What can we do:
	%significance
	%disciplined selection
	%ranking of variables
	%number of covariates needed to explain panel jointly
	%weak factors
	%selection for short panel
	%cross-validation based on shrinkage noisy
	
	
	Our data-driven hypothesis perspective exploits the geometric structure implied by the first stage selection step. Given valid post-selection $p$-values of a regularized sparse estimator from time-series regressions, we collect them across the large cross-section into a ``matrix'' of $p$-values.  Only active coefficients, that are selected in the first stage, contribute $p$-value entries, whereas covariates that were non-active lead to ``holes'' in this matrix. We leverage the non-trivial shape of this matrix to form our adaptive hypotheses. This allows us to make valid multiple testing adjusted inference statements, for which we design a panel modified Bonferroni-type procedure that can control for the family-wiser error rate (FWER) in the discovery of the covariates. As one loosens the FWER requirements, the inferential thresholds admits more and more explanatory variables. Hence, the number of admitted covariates and the FWER control level form an ``false-discovery control frontier''. We provide a method that allows us to traverse the inferential results and determine the least number of covariates that have to be included given a user-specified FWER level. In other words, we can make a statement on the number of covariates or factors needed to explain a panel based on a statistical significance requirement. %In other words, we provide a statistical significance test for the number of covariates or factors in a panel.
	
	
	
	%5. Lasso Contribution
	
	We propose the novel procedure Panel-PoSI, which combines the data-driven adjustment for panel multiple testing with valid post-selection $p$-values of a generalized LASSO. While our multiple testing procedure is valid for any sparsity constrained model, Panel-PoSI is an easy-to-use and practically relevant special case. We propose Weighted-LASSO for the first stage selection regression and provide valid $p$-values through post-selection inference (PoSI), which yields a truncated-Gaussian distribution for an adjusted LASSO estimator. This geometric perspective is less common in the LASSO literature, but has the advantage that it avoids the use of infeasible quantities, in particular the second moment of the large set of potential covariates. The Weighted-LASSO generalizes LASSO by allowing to put weights onto prior belief sets. For example, a researcher might have economic knowledge that she wants to include in her statistical selection method, and impose an infinite prior weight to include specific covariates in the sparse selection model. Our Weighted-LASSO makes several contributions. First, the expression for the truncated conditional distribution with weights become much more complex than for the special case of the conventional LASSO. Second, we provide a simple, easy-to-use and asymptotically valid conditional distribution in the case of an estimated noise variance. 
	
	%6. Simulations and 7. Empirics
	
	We demonstrate in simulations and empirically that our inferential theory allows us to select better models. We compare different estimation approaches to select covariates and show that our approach better trades off false discovery and correct selections and hence results in a better out-of-sample performance. Our empirical analysis studies the fundamental problem in asset pricing of selecting a parsimonious factor model from a large set of candidate factors that can jointly explain the asset prices of a large cross-section of investment strategies. We consider a standard data set of 114 candidate asset pricing factors to explain 243 double sorted anomaly portfolios. We show that Panel PoSI selects 3 factors which form the best model to explain out-of-sample the expected returns and the variations of the test assets. The selected factors are economically meaningful and include the size and value factors of the Fama-French model. Hence, our statistical selection procedure confirms two of the most widely used asset pricing factors. Our findings contribute to the discussion about the number of asset pricing factors. We confirm that independent of the rotation of the covariates we select 3 factors for a 5\% FWER control.  
	
	%Panel PoSI selects parsimonious factor models with the best out-of-sample performance among the benchmarks. 
	%The Panel PoSI models also explain the average returns the best.
	%The Panel PoSI models select economically meaningful factors.
	%In addition to their significance, we can infer the relative importance of factors.
	%Our method contributes to the discussion about the number of asset pricing factors. Many popular asset pricing models suggest between three and six factors. Our approach allows a disciplined estimate for the number of factors based on inferential theory. The level of sparsity of a linear model also depends on the rotation of the covariates. 
	
	%8. Structure
	
	The rest of the paper is organized as follows. Section \ref{sec:lit} relates our work to the literature. Section \ref{sec:model} introduces the model and the Weighted-LASSO. Section \ref{sec_hypothesis} discusses the appropriate hypotheses to be considered for inference on the entire panel. Section \ref{sec_multiple_testing} proposes a joint unordered test for the panel using multiple testing adjustment so that we can maintain FWER control, and shows how to traverse this procedure to acquire the least covariate count associated with each FWER target. In section \ref{sec:ordered}, we consider the case of nested hypotheses, where the covariates follow a fixed ordering, which is of independent interest, and we propose a step-down procedure for this setting that maintains false discovery control. Section \ref{sec:simulation} provides the results of our simulation and Section \ref{sec:empirics} discusses our empirical analysis on a large asset pricing panel data set. Section \ref{sec6} concludes. The Appendix collects a detailed discussion about post-selection LASSO. The proofs and more technical details are available in the Online Appendix.
	
	
	
	\label{sec:intro}
	
	\subsection{Related Literature}
	\label{sec:lit}
	
	The problem of multiple testing is an active area of research with a long history. The statistical inference community has studied the problem of controlling the classical FWER since \cite{Bonf}, and controlling for false-discover rate (FDR) going back to \cite{BH95} and \cite{BY01}. \cite{Bonf} allows for arbitrary correlations in the test statistics because its validity comes from a simple union bound argument, and is in fact the optimal test when statistics are ``close to independent'' under true sparse non-nulls. FDR control on the other hand requires a discussion about the estimated covariance in the test statistics. Recent developments include a stream of papers led by \cite{15-AOS1337} and \cite{rssb.12265}, which constructs a generative model to produce fake covariates and control for FDR. \cite{fithian2022conditional} is a more recent work that iteratively adjusts the threshold for each hypothesis in the family to seek finite sample exact FDR control and dominates \cite{BH95} and \cite{BY01} in terms of power. Another notion on temporal false discovery control has been revived more recently by \cite{doi:10.1287/opre.2021.2135}, who consider the industry practice of constantly checking $p$-values and provide an early stopping in line with \cite{SiegmundSeq} that adjusts for a bias from sequentially picking favorable evidence, whereas we consider a static panel that is not an on-going experiment. 
	
	
	There are cases where the covariates warrant a natural order such that the hypothesis family possesses a special testing logic. A hierarchical structure in covariates arises when the inclusion of the next covariate only make sense if the previous covariates is included. An example is the use of principal component (PC) factors, where PCs are included sequentially from the dominating one to the least dominating one. We distinguish this from putting weights and assigning importance on features because this variant of family of hypotheses warrants a new definition of FWER. We propose a step-down procedure that can be considered as a panel extension of \cite{rssb.12122}, relying on an approximation of the R\'enyi representation of $p$-values. The step-down control for nested FWER is based on \cite{2336545}, which along with \cite{Bonf} can be seen as comparing sorted $p$-values against linear growth. Our framework contributes to estimating the number of principal component factors in a panel. There are have been many studies that provide consistent estimators for the number of PCs based on the divergence in eigenvalues of the covariance matrix, which include \cite{1468-0262.00273}, \cite{40985808}, \cite{ECTA8968} and \cite{PELGER201923}. Another direction uses sequential testing procedures that presume correct nested family of hypotheses, which include \cite{jbes.2009.07239} and \cite{16-AOS1536}. In contrast, we characterize the least number of covariates (which can also be based on principal components), which should be expected when a FWER rate is provided. The nested version of our procedure is close in nature to a panel version of ``when-to-stop'' problem of a multiple testing procedure.
	
	
	
	
	
	The problem of post-LASSO statistical testing for small dimensional cross-sections is studied in a stream of papers including \cite{009053606000000281}, \cite{rssb.12026}, \cite{14-AOS1221} and \cite{10.1214/17-AOS1630}, which consider inference statements by debiasing the LASSO estimator. An alternative stream of post-selection or post-machine learning inference literature includes \cite{annurev-economics-012315-015826}, \cite{kuchibhotla2018valid} and \cite{zrnic2020postselection}, who provide non-parametric post-selection or post-regularization valid confidence intervals and $p$-values. These papers do not make conditional statements and presume that the researcher sets the hypotheses before seeing the data, which we will refer to as data agnostic hypothesis family. We follow a different train of thought that treats LASSO, among a family of conic maximum likelihood estimator, as a polyhedral constraint on the support of the response variable. This geometric perspective that provides inferential theory post-LASSO is pioneered by the work of \cite{lee2016exact} and followed up by \cite{fithian2017optimal} and \cite{tian2018selective}, assuming Gaussian linear models. \cite{markovic2018unifying} extend the results to LASSO with cross-validation, \cite{tian2017selective} discuss a square-root LASSO variant that takes an unknown covariance into consideration. \cite{taylortibshirani2016inference} and \cite{tian2017asymptotics} study asymptotic results that allow to relax the assumptions of Gaussian errors or generalized linear models. This body of literature is often referred to as PoSI, and traverses the Karush-Kuhn-Tucker (KKT) condition of a LASSO optimization problem to show that the LASSO fit can be expressed as a polyhedral constraint on the support of the response variable. We extend this work by allowing to put weights onto prior belief sets, and by bringing it to the panel setting with multiple testing adjustment. 
	
	
	
	
	
	%The problem of factor selection in panels has long been an active topic of research in econometrics. The case of recovery a low-dimensional principal compenent model with large $N,T$ was considered in \cite{1468-0262.00392}, where $\sqrt{T}$-consistent Gaussian estimates of factor loadings were established for panels of $\sqrt{T}/N\to0$. However, we are interested in inference on a sparse selection problem from a large pool of candidate covariates, rather than the construction of a sparse factors set from the data.  
	%
	%Our setting allows for the outcome variables to have unit-specific loadings on a pool of dynamic covariates, and we do not consider those loadings to be time-varying. A more general formulation of the panel model that allows for both unit and time heterogeneity is often referred to as generalized two-way fixed effects model includes \cite{LU2022}, which provides inference with triple bootstraps. However, their machineary is computation-heavy when there are many covariate candidates to consider, and assumes cross-sectional independence of loadings. Our setting is more constrained in that we do not allow for time heterogeneity, but more general in that we allow for arbitrary cross-sectional dependency structure of the loadings.
	%
	%\cite{https://doi.org/10.1111/jofi.12854} and \cite{doi:10.1080/07350015.2019.1573684} propose a Bayesian marginal likelihood method that scans the combinatorial set of models constructed with candidate factors, if a parsimonious model is assumed. By contrast, we are interested in selecting useful covariates that can explain a panel of outcomes that are specified for us. Thus, our approach is a set of conditional inference statements, conditional on observing the data tuple $(\bm{Y},\bm{X})$ of interest, instead of scanning model space of $\bm{X}$.
	%
	%On the issue of discovering new factors on top of consensus or prior belief factors, there has been the recent work of \cite{jofi.12883} where two LASSOs were sequentially run to discover new factors. This is asking a question of similar nature to the unobserved factors discussed in \cite{01621459.2016.1195743}. We take the stance that it is more transparent to enable researchers to express their level of prior belief in the small subset of consensus factors via a LASSO weighting scheme. By changing the weights in $\RR_+$, we leave the flexibility to the user of the inferential framework, and this weight notion has the Bayesian interpretation of prior variance. On top of transparency and interpretability, our inferential procedures have less tuning than \cite{jofi.12883} and make fewer structural assumptions than \cite{01621459.2016.1195743}.
	%
	%There can also be exogenous reasoning on the features' importance, which means the hypothesis testing problem could become ordered, structured or nested. Many existing methods treat such a structure as a hard rule, meaning that they embed the logic into the order by which hypotheses associated with features are examined. For instance, \cite{15-AOS1337} can be thought of as using the covariate information to create a scoring system to impose ordering on a family of hypotheses. In many settings such as the factor discovery of asset pricing risk factor studies, it is unclear why one factor would be strictly always be considered over another. Thus, we consider a weaker sense of prior belief incorporated by researcher-specified LASSO heterogeneous penalties, which can be construed as a soft ordering. From a Bayesian perspective such as \cite{016214508000000337}, we take in the prior beliefs by putting different priors such that more important variables have larger Laplacian variances.
	%
	%
	%% The problem of post-LASSO statistical testing is studied in a stream of papers including \cite{009053606000000281}, \cite{rssb.12026}, \cite{14-AOS1221} and \cite{10.1214/17-AOS1630}, which consider inference statements by ``debiasing'' the LASSO estimator. This body of paper presumes the researchers set the hypotheses before they see the data, which we will refer to as $H_A$ -- the data agnostic hypothesis family -- in Section \ref{sec5}. Then, for this type of hypotheses, the debiasing literature provides a Gaussian limiting distribution on an adjusted estimator. Although examining different inference objects, our proposed method relies on a key constructed statistic ``one-step estimator'', which is also featured prominently in the debiasing literature. We will examine the ties later, but here we highlight the differences: not only does this stream of debiasing methods usually involve estimation of a high-dimensional covariance or precision matrix, they also imply a philosophy that the researchers make queries before looking at the data, and that their initial queries are fixed and not adapted to what is painted by the data. Considering many discussions on ``replication crisis'' such as \cite{Baker2016} and \cite{LuckyFactors}, we take the philosophical standpoint that in high-dimensional setting, we should expect the hypotheses be posed in a data-driven manner, meaning that researchers actually form their questions after running some model. For instance, among a bank of 200 new asset pricing factors, we think it is more appropriate to pose a data-driven (driven by LASSO output) low-dimensional hypothesis over either a 200-dimensional hypothesis or a handpicked speculative low-dimensional hypothesis. Moreover, it is our view that we should formulate the hypotheses avoiding the non-active LASSO coefficients, since it is hard to distinguish whether they are inactive due to idiosyncratic noises or small loading that were not picked up by LASSO when the actual data set has finite amount of observations.
	%
	%
	%% Specifically, we follow a different train of thought that treats LASSO, among a family of conic maximum likelihood estimator, as a polyhedral constraint on the support of the response variable. This geometric perspective that provides inferential theory post-LASSO is pioneered by the work of \cite{lee2016exact} and followed up by \cite{fithian2017optimal} and \cite{tian2018selective}, assuming Gaussian linear model. \cite{markovic2018unifying} extends the results to LASSO with cross-validation (CV henceforth), \cite{tian2017selective} discusses a square-root LASSO variant that takes unknown covariance into consideration and \cite{tian2017asymptotics} considers the asymptotic results when removing the Gaussianity assumption. This body literature is often referred to as PoSI, and traverses the Karush-Kuhn-Tucker (KKT) condition of a LASSO optimization problem to discover that the LASSO fit is a random event equivalent with a $\{A Y\leq b\}$ polyhedral event on the response variable $Y$. We extend this work by applying it in a multiple testing and panel setting. We also note that there is an alternative stream of post-selection or post-machine learning inference literature such as \cite{annurev-economics-012315-015826}, \cite{kuchibhotla2018valid} and more recently \cite{zrnic2020postselection} that provides a non-parametric post-selection or post-regularization valid confidence intervals and $p$-values. But to construct uniformly valid confidence interval, this stream of papers is not making conditional statements but seeks to marginalize over the selection event. We take the stance in this paper that a common starting point of inference in high-dimensional panel is after selection event because the data-driven hypotheses $H_{\mathcal{M}}$ is written conditional on outputs of regularized model's fit. Moreover, we limit our scope in this paper to applications where it is appropriate to pose a linear model, so that the results yielded by polyhedral constraints on Gaussians would be more powerful compared to very general settings considered in such as non-parametric data-generating process of \cite{annurev-economics-012315-015826}.
	%The problem of factor selection in panels has long been an active topic of research in econometrics. The case of recovery a low-dimensional principal compenent model with large $N,T$ was considered in \cite{1468-0262.00392}, where $\sqrt{T}$-consistent Gaussian estimates of factor loadings were established for panels of $\sqrt{T}/N\to0$. However, we are interested in inference on a sparse selection problem from a large pool of candidate covariates, rather than the construction of a sparse factors set from the data.  
	%
	%Our setting allows for the outcome variables to have unit-specific loadings on a pool of dynamic covariates, and we do not consider those loadings to be time-varying. A more general formulation of the panel model that allows for both unit and time heterogeneity is often referred to as generalized two-way fixed effects model includes \cite{LU2022}, which provides inference with triple bootstraps. However, their machineary is computation-heavy when there are many covariate candidates to consider, and assumes cross-sectional independence of loadings. Our setting is more constrained in that we do not allow for time heterogeneity, but more general in that we allow for arbitrary cross-sectional dependency structure of the loadings.
	%
	%\cite{https://doi.org/10.1111/jofi.12854} and \cite{doi:10.1080/07350015.2019.1573684} propose a Bayesian marginal likelihood method that scans the combinatorial set of models constructed with candidate factors, if a parsimonious model is assumed. By contrast, we are interested in selecting useful covariates that can explain a panel of outcomes that are specified for us. Thus, our approach is a set of conditional inference statements, conditional on observing the data tuple $(\bm{Y},\bm{X})$ of interest, instead of scanning model space of $\bm{X}$.
	%
	%
	%On the problem of multiple testing, the statistical inference community has studied the problem of control for the classical FWER, since \cite{Bonf}, and control for false-discover rate (FDR) going back to \cite{BH95} and \cite{BY01}. \cite{Bonf} allows for arbitrary correlation in the test statistics because its validity comes from a simple union bound argument, and is in fact the optimal test when statistics are ``close to independence'' when the true non-nulls are sparse. FDR control on the other hand requires discussions associated with particular estimated covariance in the test statistics or $p$-values. Recent developments include a stream of papers led by \cite{15-AOS1337} and \cite{rssb.12265} which constructs a generative model to produce fake data and control for FDR. \cite{fithian2022conditional} is a more recent work that iteratively adjusts the threshold for each hypothesis in the family to seek finite sample exact FDR control and dominates \cite{BH95} and \cite{BY01} on power. Another notion on temporal false discovery control has been revived more recently by \cite{doi:10.1287/opre.2021.2135} which considers the industry practice of constantly checking $p$-values and provides an early stopping in line with \cite{SiegmundSeq} that adjusts for bias from sequentially picking favorable evidence, whereas we consider a static panel that is not an on-going experiment. Another aspect of multiple testing is that the exogenous reasoning might compel us to test features in a fixed order. We distinguish this from putting weights and assigning importance on features because this variant of family of hypotheses warrants a new definition of FWER. We propose a step-down procedure that can be considered as a panel extension of \cite{rssb.12122}, relying on an approximation of R\'enyi representation of $p$-values. The step-down control for nested FWER is due to \cite{2336545}, which along with \cite{Bonf} can be seen as comparing sorted $p$-values against linear growth.
	%
	%Our framework contributes to estimating the number of principal component factors. There have been many studies into selecting the amount of factors or principal components such as the consistent estimators using theories that show divergence in eigenvalues of the covariance matrix, which include \cite{10.2307/2329038}, \cite{1468-0262.00273}, \cite{40985808}, \cite{ECTA8968} and \cite{PELGER201923}; and sequential testing procedures that presume correct nested family of hypotheses, which include \cite{jbes.2009.07239} and \cite{16-AOS1536}. In contrast, we characterize the least amount of factors (which can also be based on principal components), which should be expected when a FWER rate is provided. This is not a statement on the exact amount of factors in true model. The nested version of our procedure is more close in nature to a panel version of ``when-to-stop'' problem of a multiple testing procedure.
	%
	%On the issue of discovering new factors on top of consensus or prior belief factors, there has been the recent work of \cite{jofi.12883} where two LASSOs were sequentially run to discover new factors. This is asking a question of similar nature to the unobserved factors discussed in \cite{01621459.2016.1195743}. We take the stance that it is more transparent to enable researchers to express their level of prior belief in the small subset of consensus factors via a LASSO weighting scheme. By changing the weights in $\RR_+$, we leave the flexibility to the user of the inferential framework, and this weight notion has the Bayesian interpretation of prior variance. On top of transparency and interpretability, our inferential procedures have less tuning than \cite{jofi.12883} and make fewer structural assumptions than \cite{01621459.2016.1195743}.
	%
	%There can also be exogenous reasoning on the features' importance, which means the hypothesis testing problem could become ordered, structured or nested. Many existing methods treat such a structure as a hard rule, meaning that they embed the logic into the order by which hypotheses associated with features are examined. For instance, \cite{15-AOS1337} can be thought of as using the covariate information to create a scoring system to impose ordering on a family of hypotheses. In many settings such as the factor discovery of asset pricing risk factor studies, it is unclear why one factor would be strictly always be considered over another. Thus, we consider a weaker sense of prior belief incorporated by researcher-specified LASSO heterogeneous penalties, which can be construed as a soft ordering. From a Bayesian perspective such as \cite{016214508000000337}, we take in the prior beliefs by putting different priors such that more important variables have larger Laplacian variances.
	
	
	
	\section{Sparse linear models}\label{sec:model}
	
	%\begin{itemize}
	%\item Change to lambda (replace c)
	%\item Use debiased instead of one-step and cite relevant literature
	%\item Separate between the two elements debiasing and correcting the distribution
	%\item Explain the benefit of a geometric approach, alternative approaches require infeasible variance correction. Tradeoff between stronger finite sample assumptions, but full distribution theory.
	%\item Clarify contribution of weighting, covariate specific penalty, essentially a selection prior. Theory allows infinite weights. This can be interpreted as a prior to include a covariate in the selection. This does not imply that the covariate will have significant coefficients.
	%\item Emphasize that our results do not depend on these specific lasso p-values, but the results would hold for any selection method with valid p-values.
	%\item Refer to Appendix
	%\item Better notation, with explicit indices.
	%\item Assumption: Selection results in a finite dimensional set of covariates, and this selection includes all relevant covariates. 
	%\item Sparse model should not be a definition. Maybe an assumption.
	%\item Appendix combine Assumption 2 and 3 into one assumption. Label Assumptions as Appendix Assumptions.
	%\item Change notation of indices to t and n, Maybe J for the number of covariates and j as the running index?
	%\end{itemize}
	
	
	We consider a large dimensional panel data set $\bm{Y} \in \RR^{T\times N}$ which we want to explain with a large number of potential covariates $\bm{X} \in \RR^{T\times J}$. The panel data and explanatory variables are both observed over $T$ time periods.\footnote{Our setting and multiple testing results can be readily extended to the case of unbalanced panel, although we focus on the balanced panel case for now to highlight the core multiple testing insight of our method. We will further discuss on this once we introduce our main procedure in Section \ref{sec_multiple_testing}} The size of the cross-section $N$ and the dimension of the covariate candidate set $J$ are both large in our problem. %\cmt{Throughout this paper, we use superscript $\cdot^{(n)}$ to denote variables corresponding to $n$th unit, subscript $\cdot_j$ to denote variables corresponding to $j$th covariate candidate, and subscript $\cdot_t$ to denote variables corresponding to $t$th time period.}
	
	We assume a linear relationship between $\bm{Y}$ and $\bm{X}$:
	\begin{align*}
		Y_{t}^{(n)} = \sum_{j=1}^J X_{t,j} \beta_{j}^{(n)} + \epsilon_{t}^{(n)} \qquad \text{for $n=1,...,N$},
	\end{align*} 
	
	which reads in matrix notation as 
	\begin{equation}\label{1}
		\bm{Y}=\bm{X}\bm{\beta}+\bm{\epsilon}.
	\end{equation}
	We refer to the coefficients $\bm{\beta}$ as loading matrix, where the $n$th column $\beta^{(n)}\in\RR^{J}$ corresponds to the $n$th unit and $\beta^{(n)}_{j}$ denotes the loading of the $n$th unit on the $j$-th covariate. The remainder term $\bm{\epsilon}$ is unexplained noise. Throughout this paper, we use the superscript $\cdot^{(n)}$ to denote cross-sectional variables corresponding to the $n$th unit, subscript $\cdot_j$ for variables corresponding to the $j$th covariate, and subscript $\cdot_t$ for time-series corresponding to the $t$th time period.
	
	
	We assume that a sparse linear model can explain jointly the full panel. Formally, a sparse linear model with $s$ active covariates is 
	\begin{equation}\label{2}
		\bm{Y}=\bm{X}_S\bm{\beta}_S+\bm{\epsilon}
	\end{equation}
	where  $s=|S|$ is the cardinality of the set of active covariates $S=\{j:\exists  \beta^{(n)}_{j}\neq 0, n \in \{1,...,N\}$, that is, the set of covariates with non-zero loadings. $\bm{X}_S$ is the subset of covariates that belong to $S$. Our goal is to estimate this low dimensional model, that can explain the full panel, from a large number of candidate covariates, and provide a valid inferential theory.
	
	Note that our sparse model formulation allows for two important properties. First, different units can be explained by different covariates with different loadings. This means that $\beta^{(n)}\neq \beta^{(m)}$ for $n \neq m$ is allowed. For example, a subset of the cross-sectional units might be modeled by different covariates than the remaining part of the panel. Second, we can accommodate ``weak'' covariates. A covariate is included in $S$ if it is required by at least one cross-sectional unit as explanatory variable. In other words, a sparse model can include covariates in $\bm{X}_S$ that explain only a very small subset of the panel $\bm{Y}$.
	
	
	
	The first step is to estimate the sparse models over the time-series for each unit separately due to the heterogeneity in the loadings. In a second step, we provide the valid inferential theory for the loadings on the full panel. The time-series estimation requires an appropriate regularization to select a small subset of covariates that contains all the relevant covariates for each unit. We allow for prior belief weights $\omega_j\in (0,+\infty]$ on the $J$ candidate covariates, so that different $\bm{X}$ can have different relative penalizations, and a global $\lambda \in\RR_+$ scalar penalty parameter. For the $n$th unit, we denote its $\beta^{(n)}$ regularized estimate as $\hat{\beta}^{(n)}$ and the active set $M^{(n)}=\{j:\hat{\beta}^{(n)}_j\neq 0\}$ as the set of $j$'s with non-zero loadings $\hat{\beta}^{(n)}_j$. A general regularized linear estimator solves the following optimization problem 
	\begin{equation}\label{4}
		\hat{\beta}^{(n)}(\lambda,\omega)=\argmin_{\beta} \frac{1}{2T}\|Y^{(n)}-\bm{X}\beta\|_2^2+\lambda\cdot f(\beta,\omega)
	\end{equation}
	for a penalty function $f$ and appropriate weights, where $Y^{(n)}$ is the vector of response variables of $n$th unit. In this paper, we consider the weighted-LASSO estimator with the regularization function 
	\begin{equation}
		f(\beta,\omega)=\sum_{j=1}^J f_j(\beta_j,\omega_j)
		\quad\wh
		f_j(\beta_j,\omega_j)=\begin{cases}
			\frac{|\beta_j|}{\omega_j}& \omega_j<\infty\\
			0& o.w.
		\end{cases}
	\end{equation}
	and weights $\omega_j>0$ for all $j\in \{1,...,J\}$ and $\sum_{j=1}^J\omega_j^{-1}=J$. We consider the penalty $\lambda$ as exogenously provided such that the set $\|\hat{\beta}^{(n)}\|_0=|M^{(n)}|$ is low dimensional.\footnote{In Appendix \ref{lab:appendixA2} we discuss the case where variances of $\bm{\epsilon}$ are unknown and need to be estimated. We provide a specific discussion on $\lambda$'s rate conditions in terms of $J$ and $T$, as is typically required to ensure consistency in the LASSO literature.} Importantly, we do not need to assume that the selected set contains all ``true'' active covariates. Our goal is to provide a valid inferential theory conditional on the selected set. Our estimator generalizes the conventional LASSO with the $l_1$ regularization function of \cite{2346178} by allowing for different relative weighting in the penalty. Importantly, we also allow for an infinite weight, which can be interpreted as a prior on a set of covariates. This allows researchers to take advantage of prior information and for example ensure that a specific set of covariates will always be included. The weighted-LASSO will be particularly relevant in our empirical study, where we can answer the question which risk factors should be added to a given set of economically motivated risk factors. Our weighted-LASSO formulation can also be interpreted as a Bayesian estimator with the canonical Laplacian prior.  
	
	
	Conventional regression theory will not provide correct inferential statements on the weighted-LASSO estimates. We face two challenges. First, regularized estimation results in a bias, which needs to be corrected. Second and more challenging, post-selection inference changes the distribution of the estimators. When we observe an active $\hat{\beta}^{(n)}_j$ from (\ref{4}), it would be incorrect to simply calculate its $p$-value from a conventional Student $t$-distribution. This invalidity stems from the fact that conditional on observing a LASSO output, ${\beta}^{(n)}_j$ must be large enough in magnitude for its $\hat{\beta}^{(n)}_j$ to be active. In other words, the probability distribution of the estimators is truncated. 
	
	%The correct inference has to be conditional on the covariates being selected by the LASSO estimator. Hence, valid $p$-values have to be the tail probability conditional on being in the selection set. The key to quantify such styles of inference is to recognize that a sparsity constrained estimator is typically the result of solving Karush-Kuhn-Tucker (KKT) conditions, which can in turn be geometrically characterized as polyhedral constraints on the support of response variables. This is first established in \cite{lee2016exact}, who provide the stylized results that Post-Selection Inference (PoSI) of debiased non-weighted LASSO estimators can be calculated as polyhedral truncation on $\bm{Y}$. This line of research is also referred to as Selective Inference in the literature, such as in \cite{taylor2015statistical}.  We extend this line of literature to allow for the Weighted-LASSO. \cmt{Our extension is built upon the debiased parallel $\bar{\beta}_j^{(n)}$ rather than the solved regularized estimate $\hat{\beta}_j^{(n)}$, following similar literature suc as \cite{lee2016exact}. We derive these results with regularity conditions common in the PoSI LASSO literature, formally detailed in Assumptions \ref{asu1}, \ref{asu_known}. Here, we demonstrate a succinct narration of the result. For the complete exhibition of the assumptions, the debiasing lemmas, and explicit formulas of the results below, please refer to Appendices \ref{lab:appendixA1} and \ref{lab:appendixA2}.}
	
	
	The correct inference has to be conditional on the covariates being selected by the LASSO estimator. Hence, valid $p$-values have to be the tail probability conditional on being in the selection set. The key to quantify such styles of inference is to recognize that a sparsity constrained estimator is typically the result of solving Karush-Kuhn-Tucker (KKT) conditions, which can in turn be geometrically characterized as polyhedral constraints on the support of response variables. This is first established in \cite{lee2016exact}, who provide the stylized results that Post-Selection Inference (PoSI) of debiased non-weighted LASSO estimators can be calculated as polyhedral truncation on $\bm{Y}$. This line of research is also referred to as Selective Inference, for example in \cite{taylor2015statistical}.  We extend this line of literature to allow for the Weighted-LASSO. We derive these results with assumptions common in the PoSI LASSO literature, detailed in Appendix \ref{lab:appendix}, and referred to as conventional regularity conditions for the ease of exhibition.
	
	Theorem \ref{thm1_main} shows how we calculate $p$-values from the post-selection distribution of the debiased estimate $\bar{\beta}_j^{(n)}$. The regularized estimate $\hat{\beta}_j^{(n)}$ has a well-known bias. We debiase the LASSO estimate by a shifting argument. While we use a geometric argument to remove the bias, the bias adjustment takes the usual form in the LASSO literature as for example in \cite{10.3150/11-BEJ410}. The debiased LASSO estimator simply equals a standard OLS estimation on the subset $M^{(n)}$ selected by the Weighted-LASSO.
	
	
	% \begin{theorem}{\bf Truncated Gaussian Distribution of Weighted-LASSO}\label{thm1_main}\\
		% \cmt{Under the conventional regularity conditions stated in Assumptions \ref{asu1} and \ref{asu_known} for the finite sample case, or Assumptions \ref{asu1} and \ref{asu_consist} for the $T\rightarrow \infty$ case, the debiased estimate $\bar{\beta}_j^{(n)}$ for the $j$-th Weighted-LASSO active covariate of the $n$th unit is conditionally distributed as}
		% %Under the conventional regularity conditions stated in Assumptions \ref{asu1}, \ref{asu_known} and \ref{asu_consist} in the Appendix and for $T\rightarrow \infty$, the debiased estimate $\bar{\beta}_j^{(n)}$ for the $j$-th Weighted-LASSO active covariate of the $n$th unit is conditionally distributed as
		% 	\begin{equation}\label{eq:thm1_formula}
			% 		\bar{\beta}_{j}^{(n)}|\textrm{Weighted-LASSO}			\sim \mathcal{TN}_{\{\eta^\top Y^{(n)}:AY^{(n)}\leq b(\omega)\}},
			% 	\end{equation}
		%  where $\mathcal{TN}_{\mathcal{A}}$ is truncated-Gaussian with truncation $\mathcal{A}$, and the weights $\omega$ only appear in $b(\omega)$. Under the \cmt{post-selection }null-hypothesis $H_D$, that the active covariates of unit $n$ have zero coefficients, and conditional on the selection events and the weights, the post-selection $p$-values of the active coefficients follow a uniform distribution, that is, 
		%  $$p^{(n)}_j \stackrel{H_D|\mathcal{M},\omega}{\sim} \textrm{Unif }[0,1].$$ 
		% \end{theorem}
	
	
	\begin{theorem}{\bf Truncated Gaussian Distribution of Feasible Weighted-LASSO}\label{thm1_main}\\
		Under the conventional regularity conditions stated in Assumptions \ref{asu1} and \ref{asu_known} in the Appendix, the debiased estimate $\bar{\beta}_j^{(n)}$ for the $j$-th Weighted-LASSO active covariate of the $n$th unit is conditionally distributed as
		\begin{equation}\label{eq:thm1_formula}
			\bar{\beta}_{j}^{(n)}|\textrm{Weighted-LASSO}			\sim \mathcal{TN}_{\{\eta^\top Y^{(n)}:AY^{(n)}\leq b(Y^{(n)},\omega)\}},
		\end{equation}
		where $\mathcal{TN}_{\mathcal{A}}$ is truncated Gaussian with truncation $\mathcal{A}$, and the weights $\omega$ only appear in $b(Y^{(n)},\omega)$. 
  Under the null hypothesis $H_D$, that the active covariates of unit $n$ have zero coefficients, and conditional on the selection events and the weights, the post-selection $p$-values of the active coefficients follow a uniform distribution, that is, 
		$$p^{(n)}_j \stackrel{H_D|\mathcal{M},\omega}{\sim} \textrm{Unif }[0,1].$$
  Under conventional asymptotic conditions stated in \ref{asu_consist} and \ref{asu_gram} in the Appendix, and for $T\rightarrow \infty$, the same truncated Gaussian distribution holds for feasible $p$-values with estimated noise variance. 
  	\end{theorem}
	
	
	
	
	
	%\cmt{From Theorem \ref{thm1_main}, we can construct a post-selection valid $p$-value that satisfies Assumption \ref{assu:valid_p} that would be used throughout the inference procedures in Section \ref{sec_multiple_testing}. We can obtain these $p$-values from cumulative distribution function of the truncated Gaussian distribution.} Crucially, all results for multiple testing adjustment in panels that we study in the following sections neither require us to use a Weighted-LASSO estimator nor to use the $p$-values implied by Theorem \ref{thm1_main}. We only require to have a set of valid $p$-values for sparsity constrained models. These can be obtained with any suitable regularized estimator and post-selection inference. The key element is the selection of a low dimensional subset with $p$-values conditional on this selection. We propose the Weighted-LASSO conditional inference results as an example of the type of sparsity constraint models we are interested in, and demonstrate a machinery with which we can obtain valid $p$-values for sparsity constrained models. In our empirical studies, we use Weighted-LASSO as our sparsity constrained model since we want to specify strong prior beliefs on a few covariates and it is common practice to use LASSO in the context of our empirical studies. Nonetheless, the testing methods in the next sections accommodate any sparse estimator, and can be detached from inference for Weighted-LASSO. 
	
	Theorem \ref{thm1_main} has two key elements. 
	%First, it debiases the LASSO estimate by a shifting argument. While we use a geometric argument to remove the bias, the bias adjustment takes the usual form in the LASSO literature as for example in \cite{10.3150/11-BEJ410}. The debiased LASSO estimator simply equals a standard OLS estimation on the subset $M_n$ selected by the Weighted-Lasso. 
	First, the distribution of the linear coefficients is not a usual Gaussian distribution, but it is truncated due to studying post-selection coefficients. This geometric perspective is less common in the LASSO literature, but provides several advantages. One advantage of the geometric approach is that it avoids the use of infeasible quantities, in particular the second moment of the large set of potential covariates. Second, conditional on the selection and under the null hypothesis that the coefficients of the active covariates for unit $n$ are zero, the post-selection $p$-values of the active coefficients follow a uniform distribution. This is important as it implies that we obtain valid post-selection $p$-values, which provide the correct Type-I error control for individual regressions. These individually valid $p$-values are the key for deriving the multiple testing adjustment in large panels.
	
	Appendix \ref{lab:appendixA1} provides the detailed information on constructing $\bar{\beta}$ and the definitions of $\eta, A, b(\omega)$ along with lemmas that lead up to this result. It also shows that, under the assumption of a known noise variance, the distribution result is not asymptotic in $T$, but also valid in finite samples. We can obtain these results because we make the stronger assumption that the noise is normally distributed. When using a consistent sample estimate of the noise variance, we need to require additionally that $T \rightarrow \infty$. Appendix \ref{lab:appendixA1} clarifies the implications of different assumptions in the three Theorems \ref{thm1}, \ref{thm:appendix} and \ref{col2}. Theorem \ref{col2} with estimated noise variance represents the explicit form of Theorem \ref{thm1_main}, which we use for our empirical analysis. 
	%Our empirical analysis is based on the explicit form of Theorem \ref{thm1_main} formulated in Theorem \ref{col2} with estimated variance. 
	It is possible to relax the normality assumption of the noise and instead use a pivot convergence similarly to \cite{tian2017asymptotics} to obtain asymptotically a truncated Gaussian distribution. However, this would not change the nature of our statement.    
	
	
	
	Our Weighted-LASSO results make several contributions. First, the expression for the truncated conditional distribution with weights become more complex than for the special case of the conventional LASSO. Second, we provide a simple, easy-to-use and asymptotically valid conditional distribution in the case of an estimated noise variance. Last but not least, we show the formal connection with alternative debiased LASSO estimators by showing that debiasing can be interpreted as one step in a Newton-Ralphson method of solving a constrained optimization.
	%Last but not least, we show that \cmt{Weighted-LASSO retains the interpretation as one step in a Newton-Ralphson method of solving LASSO's constrained optimization problem, which is described in more details in Lemma \ref{lem1}}. 
	
	
	
	Theorem \ref{thm1_main} allows us to obtain valid post-selection $p$-values for Weighted-LASSO coefficients. We obtain these $p$ values from the simulated cumulative distribution function of the truncated Gaussian distribution. Crucially, all results for multiple testing adjustment in panels that we study in the following sections neither require us to use a weighted Lasso estimator nor to use the $p$-values implied by Theorem \ref{thm1_main}. We only require to have a set of valid post-selection $p$-values for sparsity constrained models. These can be obtained with any suitable regularized estimator and post-selection inference. The key element is the selection of a low dimensional subset with $p$-values conditional on this selection. We propose the weighted LASSO conditional inference results as an example of the type of sparsity constraint models we are interested in, and demonstrate a machinery with which we can obtain valid $p$-values for sparsity constrained models. In our empirical studies, we use Weighted-LASSO as our sparsity constrained model since we want to specify strong prior beliefs on a few covariates and it is common practice to use LASSO in the context of our empirical studies. Nonetheless, the testing methods in the next sections accommodate any sparse estimator, and can be detached from inference for Weighted-LASSO.      
	
	
	%\begin{itemize}
	%\item Why inferential theory and not CV?
	%\item Short data, not sufficient data for CV, more data for OOS
	%\item p-values are more informative than simply good CV results, ranking of variables and confidence in estimation results
	%\item Run lasso with lambda to get smaller set of covariates, use inferential theory to further trim down variables
	%\end{itemize}
	
	\section{Data-Driven Hypotheses}\label{sec_hypothesis}
	
	%\begin{itemize}
	%\item Clarify that we only need the p-values to start with. No specific model for p-values is assumed. These results are valid even if the model is non-linear. We use the sparse linear model as it is the horse race model for many problems in economics and finance. P-values can be from any non-linear regularized model.
	%\item We do not neglect covariates that only explain a smaller subset of the cross-section
	%\item What is a data-driven hypothesis?
	%\item What is the goal of this section?
	%\item Remove references to epidemiology!
	%\item Update notation n and j and i
	%\item Separate between two problems: hypothesis design and multiple testing adjustment
	%\item Start with example and then define formally the different hypothesis. Figure 1 and beta=0 statement first, before any general definitions
	%\item Double accounting: post-selection inference accounts already for selection. Hypothesis should take this into account.
	%\item Sequential treatment only later. No need to mention right now.
	%\end{itemize}
	%
	%Multiple testing adjustment
	%\begin{itemize}
	%\item control Type I error. Standard approach to define confidence intervals for estimators. 
	%\item combine test and localization subsection
	%\item no discussion about hat p. footnote about power discussion
	%\item Definition Panel-PoSI, clarify the choice of lambda for lasso penalty
	%\item Discussion about why we want inferential theory
	%\item Remove all remarks
	%\item Remove reference to empirics
	%\item Do not count all formulas, use proper bracket size
	%\item What is the number of factors, relate to factor literature
	%\item Table 1: compare data-driven hypothesis with simultaneity count with data-agnostic hypothesis and bonferoni
	%\item Table 1: use it to illustrate advantages: ordering in terms of importance, finding sparse model, understanding effect of covariates, discuss weak factors
	%\end{itemize}
	%
	%Ordered Testing on nested family
	%\begin{itemize}
	%\item drop credit risk!
	%\item hierarchical structure in covariates (inclusion of second covariate only makes sense if the first covariate is included). 
	%\item Usual setup for PCs
	%\item Notation: N with upper script hierachical
	%\item Figure 3: Subfigures to explain what we count.
	%\item Procedure needs to be redefined
	%\end{itemize}
	
	
	Our goal is to provide formal statistical tests that allow us to establish a joint model across a large cross-section with potentially weak covariates. This requires us to provide a form of statistical significance test with multiple testing adjustment that properly accounts for covariates that only explain a small subset of the cross-sectional units. This is important as in many problems in economic and finance, there is substantial cross-sectional variation in the explanatory power of covariates, and a model that simply minimizes an average error metric might neglect weaker covariates.
	
	An essential step for a formal statistical test is to formulate the hypothesis. This turns out to be non-trivial for a large panel with a first stage selection step for the covariates. It is a fundamental insight of our paper, that the hypothesis of our test has to be conditional on the selected set of active covariates of the first stage. Once we have defined the appropriate hypothesis, we can deal with the multiple testing adjustment, which by construction is also conditional on the selection step. 
	
	Our hypothesis formulation and test construction only requires valid post-selection $p$-values from a first stage selection estimator as formalized in Section \ref{sec_multiple_testing}.
	%The formal description of what we refer to as post-selection valid $p$-value is described in Assumption \ref{assu:valid_p}}. 
The results of the next two sections do not depend on a specific model for obtaining these $p$-values and the active set. The results are valid for any model including non-linear ones. The input to the analysis is a $N \times J$ matrix, which specifies which covariates are active for each unit and the corresponding post-selection $p$-values. The Weighted-LASSO is only one possible model, but it can be replaced by any regularized model. We have introduced the sparse linear model as it is the horse race model for many problems in economics and finance, and therefore of practical relevance.

\begin{figure}[t!]
	\tcaptab{Illustrative example of data-driven selection}\label{fig:demo_running}
	\begin{center}\begin{subfigure}[t]{.35\textwidth}
			\includegraphics[width=0.75\linewidth]{plots/sec3_running_example1}
			\caption{Matrix $\bar{\bm{\beta}}$}
		\end{subfigure}\hspace{2.5cm}
		\begin{subfigure}[t]{.35\textwidth}
			\includegraphics[width=0.75\linewidth]{plots/sec3_running_example2}
			\caption{Matrix $\bm{P}$ of $p$-values}
	\end{subfigure}\end{center}
	\bnotetab{This figure illustrates in a simple example the data-driven selection of a linear sparse model. In a first stage, we have estimated a regularized sparse linear model for each of the $N=6$ units with $J=4$ covariates. Each row represents the selected covariates with their estimated coefficients and $p$-values. The columns represent the $J=4$ different covariates. The grey shaded boxes represent the active set, while white boxes indicate the inactive covariates. The numbers are purely for demonstrative purposes.}
\end{figure}

We illustrate the concept of a data-driven hypothesis with a simple example, which we will use throughout this section. For simplicity we assume that we have $J=4$ covariates and want to explain $N=6$ cross-sectional units. In the first stage, we have estimated a sparse model and have obtained the post-selection valid $p$-values for each of the $N$ units. We collect the fitted sparse estimator $\bar{\beta}^{(n)}$ for the $n$th unit in the matrix $\bar{\bm\beta}$.  Note, that this matrix has ``holes'' due to the sparsity for each $\bar{\beta}^{(n)}$.  Figure \ref{fig:demo_running}(a) illustrates $\bar{\bm\beta}$ for this example.

Similarly, we collect the corresponding $p$-values in the matrix $\bm{P}$. For the $n$th unit, we only have $p$-values for those covariates that are active in the $n$th linear sparse model. Thus, Figure \ref{fig:demo_running}(b) also has white boxes showing the same pattern of unavailable $p$-values due to the conditioning on the output of the linear sparse model. These holes can appear at different positions for each unit, which makes this problem non-trivial. This non-trivial shape of either subplot (a) or (b) is completely data-driven and a consequence of linear sparse model selection. We show that the hypothesis should be formed around these non trivial shapes as well, which is why we name it the data-driven hypothesis family.  




We want to test which covariates are jointly insignificant in the full panel. A data-agnostic approach would simply test if all covariates are jointly insignificant, independent of the data-driven selection step in the first stage. A data-agnostic hypothesis is unconditional as it does not depend on any model output. However, as we will show, this perspective is problematic for the high-dimensional panel setting with many covariates as it ignores the dimension reduction from the selection step. Therefore, an unconditional multiple testing adjustment accounts for ``too many'' tests, which severely reduces the power.


We propose to form the hypothesis conditional on the first stage selection step. The data-driven hypothesis only tests the significance of the covariates that were included in the selection, and hence can drastically reduce the number of hypothesis. However, given the non-trivial shape of the active set, the multiple testing adjustment for the data-driven hypothesis is more challenging. 

Before formally defining the families of hypothesis, we illustrate them in our running example. The data-agnostic hypothesis $H_A$ for explaining the full panel takes the following form: 
\begin{equation}\small
	\begin{split}
		H_A=\{
		H_{A_{0,1}}&,H_{A_{0,2}},H_{A_{0,3}},H_{A_{0,4}}
		\}\\=\{
		\beta^{(1)}_1=&\beta^{(2)}_1=\beta^{(3)}_1=\beta^{(4)}_1=\beta^{(5)}_1=\beta^{(6)}_1=0,\\
		\beta^{(1)}_2=&\beta^{(2)}_2=\beta^{(3)}_2=\beta^{(4)}_2=\beta^{(5)}_2=\beta^{(6)}_2=0,\\
		\beta^{(1)}_3=&\beta^{(2)}_3=\beta^{(3)}_3=\beta^{(4)}_3=\beta^{(5)}_3=\beta^{(6)}_3=0,\\
		\beta^{(1)}_4=&\beta^{(2)}_4=\beta^{(3)}_4=\beta^{(4)}_4=\beta^{(5)}_4=\beta^{(6)}_4=0\}
	\end{split}
\end{equation}
The data-driven hypothesis $H_D$ only includes the active set and hence equals
\begin{equation}\small
	\begin{split}
		H_D
		=\{
		\beta^{(2)}_1=&0,\\
		\beta^{(1)}_2=&\beta^{(3)}_2=\beta^{(5)}_2=\beta^{(6)}_2=0,\\
		\beta^{(1)}_3=&\beta^{(2)}_3=\beta^{(3)}_3=\beta^{(4)}_3=\beta^{(5)}_3=\beta^{(6)}_3=0,\\
		\beta^{(2)}_4=&\beta^{(4)}_4=\beta^{(5)}_4=0\}
	\end{split}
\end{equation}
Clearly, $H_A$ has a larger cardinality of $|H_A|=24>|H_D|=14$. This holds in general, unless the first stage selects all covariates for each unit, in which case the two hypotheses coincide. 


Formally, the data-agnostic family of hypothesis is defined as follows:
\begin{definition}{\bf Data-agnostic family}\\
	The data-agnostic family of hypotheses is
	\begin{equation}
		\begin{split}
			H_A&=	\{
			H_{A_{0,j}}|j\in[J]
			\}\\
			\q \wh H_{A_{0,i}}&=\bigcap_{n\in[N]}H_{A_{0,i}}^{(n)}\text{ and }H_{A_{0,j}}^{(n)}:\beta^{(n)}_j=0.
		\end{split}
	\end{equation}
\end{definition}
It is evident that $H_A$ does not need any model output or exploratory analysis, so it is indeed data-agnostic.



As soon as we use a sparsity constrained model that has censoring capabilities, we no longer observe $(\bm{Y},\bm{X})$ from its data generating process. Consequently, unless our hypotheses depend on how we built the model, or equivalently on how the data was censored, the data-agnostic hypotheses forgo power without any benefit in false discovery control. Therefore, we formulate the hypothesis on the $j$th covariate $H_{0,j}^{(n)}$ only if $j\in M^{(n)}$, that is, it is in the active set of the $n$th unit. Conditional on observing the model output, there is no inference statement to be made about $H_{0,j}^{(n)}$ if $j\notin M^{(n)}$, because its estimator is censored by the model. 

We denote as $\mathcal{K}_j$ the set of units for which the $j$th covariate is active. We define the cross-sectional hypothesis for the $j$th covariate as:
\begin{equation}
	H_{0,j}=\bigcap_{n\in\mathcal{K}_i}H_{0,j}^{(n)}\bigg\rvert \mathcal{M},\q \forall j:\mathcal{K}_j\neq\emptyset .
\end{equation}
By combining all covariates $\{j:\mathcal{K}_j\neq \emptyset\}$ that show up at least once in one of the active sets of our sparse linear estimators, we arrive at a data-driven hypothesis associated with our panel. This is defined as follows:
\begin{definition}{\bf Data-driven family}\\\label{def5}
	The data-driven family of hypotheses conditional on $\mathcal{M}$ is 
	\begin{equation}\label{23}
		H_D=	\{
		H_{0,j}|j:\mathcal{K}_j\neq\emptyset 
		\}.
	\end{equation}
\end{definition}
This demonstrates the non-trivial nature of writing down a hypothesis in high-dimensional panel: we can only collect $\mathcal{K}_j$ - the set of units for which the $j$th covariate is active - after seeing the sparse selection estimation result.

% To facilitate our comparison, we introduce the last piece of defintion in this subsection
% \begin{definition}[Sharpness of families]\label{def:sharp}
	% Let there be a hypothesis family $H(\Theta)$  associated with testing some of parameters $\Theta$ being a constant $b$. The cardinality, denoted as $|H(\Theta)|$, is the count $|\{i:\Theta(i)=b\}|$ if $H(\Theta)$ is true. We say $H_1(\Theta)$ is sharper than $H_2(\Theta)$ if $|H_1(\Theta)|>|H_2(\Theta)|$.
	% \end{definition}
% This notion of cardinality is not only natural because it accumulates the pointwise atomic null implied by a family, but also useful to us because both of our faimlies so far are testing parts of the loading matrix $\bm{\beta}$ being constant 0.  Additionally, this is also how a Bonferroni adjustment count would be calculated were we to use it in its original form, so it reflects the level of difficulty in testing $H(\Theta)$, which goes to show the ``size'' of a family.\NL
% To make this observation general, we can apply the definitions and see that the only possibility for the data-agnostic family to coincide with the data-driven family is when all sparse linear estimator activations are the full set of covariates:
% \begin{equation}\label{25}
	% H_D=H_A\q\text{ if and only if }\q
	% \bigcap_{i\in [d]}\left\{\mathcal{K}_i=[N]\right\}
	% \end{equation}
% In large $T$, we expect (\ref{25}) to not happen if we think there is a low-rank true model. When $T$ is finite and the observed dataset is noisy and high-dimensional in $d$ and $N$, the stringent implication of (\ref{25}) makes the data-driven $H_D$ family of hypotheses always less sharp and more suitable for the high-dimensionality of the data. 






\section{Multiple Testing Adjustment for Data-Driven Hypothesis}\label{sec_multiple_testing}

\subsection{Simultaneity Counts through Panel Localization}

We show how to adjust for multiple testing of data-driven hypotheses. Given the the first stage selection of active covariates in $\mathcal{P}$, we form the data-driven hypothesis $H_D$. The only assumption that we require for the multiple testing adjustment is that we have valid post-selection $p$-values $p_j^{(n)}$ for covariate $j\in M^{(n)}$ and unit $n \in \mathcal{K}_j$. This is formalized in the following assumption: 



\begin{assumption}{\bf Valid post-selection $p$-values}\label{assu:valid_p}\\
	We assume that we have valid individual post-selection $p$-values $p_j^{(n)}$ for each unit $n$ and covariate $j$ in the active set in $\bm{P}$. Valid post-selection p-values are defined such that their Type-I error control satisfies
	$$\PP_{H_D|\mathcal{M},\omega}(p^{(n)}_j \leq x)\leq x,\quad \forall x\geq 0.$$
	conditional on the selection and prior weights, and under the null hypothesis that the active covariates are zero. 
\end{assumption}

Valid post-selection $p$-values abstract away from model-specific conditions on how the $p$-values are obtained. Post-selection $p$ values are trivially valid, if they follow a uniform distribution under the null hypothesis conditional on the selection event. This special case can be interpreted as exact valid post-selection $p$-values, since $p^{(n)}_j \stackrel{H_D|\mathcal{M},\omega}{\sim} \textrm{Unif }[0,1]$ implies $\PP_{H_D|\mathcal{M},\omega}(p^{(n)}_j \leq x)= x$. The result, that valid post-selection p-values conditional on the selection event and weights are uniform under the null hypothesis, is first introduced in \cite{tibshirani2016exact} for sparse regressions. In Theorem \ref{thm1_main}, we extend this result to generic sparse regressions with weights. Hence, the post-selection $p$-values of our Weighted-LASSO satisfy Assumption \ref{assu:valid_p}.

Our definition of valid post-selection $p$-values is natural as it simply states that we have correct conditional Type-I error control for each cross-sectional unit and each active covariate. This definition is in line with \cite{tibshirani2016exact} and \cite{heard2018choosing}, but more general. It also allows for more conservative $p$-values, that would have smaller size and worse power. The valid post-selection $p$-values provide the correct Type-I error control for each unit individually, and do not take the multiple testing issue into account. Hence, the $p$-values of Assumption \ref{assu:valid_p} are essentially the results of post-selection inference that is applied separately to each cross-sectional unit $n$. Given those values we show how to correct them to adjust for multiple testing. 



Our derivations for the multiple testing adjustment only take advantage of the conditional valid distribution of the individual post-selection $p$-values, and hence we impose this property as the fundamental underlying assumption. Note that that our results do not require a linear model, but hold for any set of valid post-selection $p$-values.\footnote{This notion of valid $p$-values deviates from what is commonly used in regression analysis as its entire statement is based on a conditional distribution, highlighting its ``post-selection'' nature. In other words, the Assumption \ref{assu:valid_p} holds under the null $H_D$ and conditional on the selection event $\mathcal{M}$ and weights $\omega$, as opposed to the classical regression analysis that does not depend on the selection.} The assumptions on the data generating process and asymptotic regime are implicitly included in Assumption \ref{assu:valid_p}. Theorem \ref{thm1_main} is a specific example that imposes Gaussian errors and $T \rightarrow \infty$. Hence, as long as a researcher has a selection estimation approach that provides valid post-selection $p$-values, our multiple testing results are applicable. 






%\cmt{The intuition for our assumption on valid $p$-value is as follows: as a building block, we require that a $p$-value that enables statistical test with Type I error rate control is available for each individual unit and active covariate. Then, we would be able to proceed to use these building blocks to construct panel joint multiple testing}. Our result, that valid post-selection $p$-values conditional on the selection event and weights are \cmt{correctly controlling for Type I error rate under the null hypothesis, is in line with \cite{tibshirani2016exact} for sparse regressions as valid pivotal statistics.} In Theorem \ref{thm1_main}, we extend this result to generic sparse regressions with weights. \cmt{$p$-values that satisfy $p\stackrel{H_D|\mathcal{M}}{\sim}$Unif$[0,1]$, such as the exact $p$-values from Theorem \ref{thm1_main}'s exact post-selection distribution, would meet this assumption. At the same time, a more conservative $p$-value would have smaller size and worse power, but would still be considered valid. Consequently, the $p$-values considered in Assumption \ref{assu:valid_p} do not necessarily have to be Unif$[0,1]$ under the $H_D|\mathcal{M}$, and are more relaxed than cases considered in \cite{heard2018choosing}. } Our derivations for the multiple testing adjustment only take advantage of the conditional valid distribution of the post-selection $p$-values, and hence we impose this property as the fundamental underlying assumption. Note that that our results do not require a linear model, but hold for any set of post-selection \cmt{valid} $p$-values.\footnote{This notion of valid $p$-values deviates from what is commonly used in regression analysis as its entire statement is based on a conditional distribution, highlighting its ``post-selection'' nature. In other words, the uniform distribution holds under the null $H_D$ and conditional on selection event $\mathcal{M}$ and weights $\omega$, as opposed to the classical regression analysis that does not depend on fitted regression results.} 

%The assumptions on the data generating process and asymptotic regime are implicitly included in Assumption \ref{assu:valid_p}. Theorem \ref{thm1_main} is a specific example that imposes Gaussian errors and $T \rightarrow \infty$. Hence, as long as a researcher has a selection estimation approach that provides valid post-selection $p$-values, our multiple testing results are applicable. 



%\begin{assumption}{\bf Valid post-selection $p$-values}\label{assu:valid_p}\\
%We have individual valid post-selection $p$-values $p_j^{(n)}$ for the active set in $\bm{P}$. Valid post-selection $p$-values  under the null hypothesis, conditional on the selection and prior weights,\cmt{ have individual Type I error control
	%$$\PP_{H_D|\mathcal{M},\omega}(p^{(n)}_j \leq x)\leq x,\quad \forall x\geq 0.$$
	%}
%\end{assumption}

%Valid post-selection $p$-values abstract away from model-specific conditions on how the $p$-values are obtained. \cmt{The intuition for our assumption on valid $p$-value is as follows: as a building block, we require that a $p$-value that enables statistical test with Type I error rate control is available for each individual unit and active covariate. Then, we would be able to proceed to use these building blocks to construct panel joint multiple testing}. Our result, that valid post-selection $p$-values conditional on the selection event and weights are \cmt{correctly controlling for Type I error rate under the null hypothesis, is in line with \cite{tibshirani2016exact} for sparse regressions as valid pivotal statistics.} In Theorem \ref{thm1_main}, we extend this result to generic sparse regressions with weights. \cmt{$p$-values that satisfy $p\stackrel{H_D|\mathcal{M}}{\sim}$Unif$[0,1]$, such as the exact $p$-values from Theorem \ref{thm1_main}'s exact post-selection distribution, would meet this assumption. At the same time, a more conservative $p$-value would have smaller size and worse power, but would still be considered valid. Consequently, the $p$-values considered in Assumption \ref{assu:valid_p} do not necessarily have to be Unif$[0,1]$ under the $H_D|\mathcal{M}$, and are more relaxed than cases considered in \cite{heard2018choosing}. } Our derivations for the multiple testing adjustment only take advantage of the conditional valid distribution of the post-selection $p$-values, and hence we impose this property as the fundamental underlying assumption. Note that that our results do not require a linear model, but hold for any set of post-selection \cmt{valid} $p$-values.\footnote{This notion of valid $p$-values deviates from what is commonly used in regression analysis as its entire statement is based on a conditional distribution, highlighting its ``post-selection'' nature. In other words, the uniform distribution holds under the null $H_D$ and conditional on selection event $\mathcal{M}$ and weights $\omega$, as opposed to the classical regression analysis that does not depend on fitted regression results.} 

%The assumptions on the data generating process and asymptotic regime are implicitly included in Assumption \ref{assu:valid_p}. Theorem \ref{thm1_main} is a specific example that imposes Gaussian errors and $T \rightarrow \infty$. Hence, as long as a researcher has a selection estimation approach that provides valid post-selection $p$-values, our multiple testing results are applicable. 



Our goal is to reject members of $H_D$ while controlling the Type I error, and the common way to measure such an error is the family-wise error rate. This is the same underlying logic that is used to define confidence intervals and determine significance of covariates in a conventional setup. The crucial difference is that we need to account for multiple testing given the large number of cross-sectional units. The family-wise error rate (FWER) is defined as follows:

\begin{definition}{\bf Family-wise error rate}\label{def6}\\
	Let $V$ denote the number of rejections of $H_{0,j}^{(n)}|\mathcal{M}^{(n)}$ when the null hypothesis is true. The family-wise error rate (FWER) is $\PP_{H_D|\mathcal{M},\omega}(V\geq 1)$.
\end{definition}

Similar to the conventional definition, we simply count the number of Type I false rejections $V$, and define FWER as the probability of making at least one false rejection. Importantly, the FWER accounts for the fact that we might repeatedly test a specific covariate for multiple cross-sectional units rather than just for one unit. Our contribution to FWER control in the panel setting is thus to take into consideration both the multiplicities in units and covariates when we deal with the ``matrix'' of $p$-values $\bm{P}$. To achieve this goal, we propose a new simultaneity account for the $j$th covariate, calculated as
\begin{equation}\label{eq28:simcount}
	N_j=\sum_{n\in \mathcal{K}_j}|M_n|
\end{equation}

Figure \ref{fig:demo1} illustrates the simultaneity counting for our running example with $N=6$ units and $J=4$ covariates. The blue boxes represent the active set for a specific covariate. The yellow boxes indicate the ``co-active'' covariates, which have to be accounted for in a multiple testing adjustment. In the case of the first covariate $j=1$, only the second unit $n=2$ has selected this covariate. This second unit has also selected covariate $j=3$ and $j=4$, which are jointly tested with the first covariates. Hence, they are ``co-active'', and the simultaneity count equals $N_1=3$. Intuitively, $N_j$ represents all relevant comparisons for the $j$th covariate because it counts how many covariates are active with the $j$th covariate in the regressions. Hence, $N_j$ quantifies the number of ``multiple tests'' for each covariate.



In subplot \ref{fig:demo1}(a), we see that $\mathcal{K}_1=\{2\}$ for the 1st covariate, indicated by the blue box, because it is only active in the second unit's regression. The multiple testing adjustment needs to consider all yellow boxes, and $N_1=3$ is thus the total count of 1 blue and 2 yellow boxes. Similarly, for the second covariate, $\mathcal{K}_2=\{1,3,5,6\}$, so we shade boxes yellow for the 2nd, 3rd and 5th units and obtain $N_2=9$. We can already see that our design of simultaneity counts takes all relevant pairwise comparisons into considerations, but avoids counting the white boxes - which would cause overcounting and result in over-conservatism. 

Our multiplicity counting is a generalization of the classical Bonferroni adjustment for multiple testing. A conventional Bonferroni method for the data-agnostic hypothesis $H_A$ has a simultaneity count of $|H_A|=N\cdot J=24$ for testing each covariate. A direct application of a vanilla Bonferroni method to the panel of all selected units and the data-driven hypothesis $H_D$, would use a simultaneity count of $|H_D|=14$ for testing each covariate. Our proposed multiplicity counting is a refinement that leverages the structure of the problem, and takes the heterogeneity of the active sets for each covariate into account. Our count has only $N_1=3$, $N_2=9$ and $N_4=8$ for the covariates $j=1,2$ and $4$. Only for covariate $j=3$ is the simultaneity count the same as a vanilla Bonferroni count applied to $H_D$, i.e. $N_3=14$. 

\begin{figure}[t!]
	\tcaptab{Simultaneity counts $N_i$ in the illustrative example}\label{fig:demo1}
	\begin{center}
		\begin{subfigure}[t]{.24\textwidth}
			\includegraphics[width=1\linewidth]{plots/sec3_running_example3}
			\caption{$N_1=3$}
		\end{subfigure}
		\begin{subfigure}[t]{.24\textwidth}
			\includegraphics[width=1\linewidth]{plots/sec3_running_example4}
			\caption{$N_2=9$}
		\end{subfigure}
		\begin{subfigure}[t]{.24\textwidth}
			\includegraphics[width=1\linewidth]{plots/sec3_running_example5}
			\caption{$N_3=14$}
		\end{subfigure}
		\begin{subfigure}[t]{.24\textwidth}
			\includegraphics[width=1\linewidth]{plots/sec3_running_example6}
			\caption{$N_4=8$}
		\end{subfigure}
	\end{center}
	\bnotetab{This figure shows the simultaneity counts $N_i$ in the illustrative example. The subplots represent the simultaneity counts for the $J=4$ covariates. The blue boxes indicate the active set $\mathcal{K}_j$ of the $j$ covariates, while yellow boxes indicate the ``co-active'' covariates of the $j$th covariate. The simultaneity counts are the sum of yellow and blue boxes.}   
\end{figure}


In addition to the simultaneity count of each covariate, we need an additional ``global'' metric for our testing procedure. We define a panel cohesion coefficient $\rho$ as a scalar that measures how sparse or de-centralized the proposed hypotheses family is:
\begin{equation}
	\rho = \left(\sum_{1\leq j\leq J:\mathcal{K}_j\neq\emptyset}\frac{|\mathcal{K}_j|}{N_j}   \right)^{-1}
\end{equation}
The panel cohesion coefficient $\rho$ is conditional on the data-driven selection of the overall panel. It is straightforward to compute once we observe the sparse selection of the panel. This coefficient takes values between $J^{-1}$ and 1,\footnote{We prove this bound in the Online Appendix, without leveraging sparsity of first-stage models but rather as an algebraic result with intuitive interpretation.} where larger values of $\rho$ imply that the active set is more dependent in the cross-section. This can be interpreted as that the panel $Y$ has a stronger dependency due to the covariates $X$. Intuitively, in the extreme case when $\rho=J^{-1}$, the panel can be separated into $J$ smaller problems, each containing a subset of response units explained by only one covariate. Thus the panel would be very incohesive, and could be studied with $J$ separate tests. In the other extreme, if $\rho $ approaches 1, the first-stage models include the same active covariates for all units. We consider this as a very cohesive panel. If $\rho$ is between theses bounds, the panel is cohesive in a non-trivial way such that some units can be explained by some covariates and there is no clear separation of the panel into independent subproblems. 

Figure \ref{fig:cohe} illustrates the panel cohesion coefficient with examples. The subplots show four active sets that are different from our running example. The left subplot \ref{fig:cohe}(a) shows the extreme case of $\rho=J^{-1}$, where the panel is the least cohesive. The right subplot \ref{fig:cohe}(d) illustrates the other extreme for $\rho=1$, where the panel is the most cohesive. The middle subplots \ref{fig:cohe}(b) and (c) correspond to the complex cases of a medium cohesion coefficient. 


\begin{figure}[t!]
	\tcaptab{Illustration of the cohesion coefficient}\label{fig:cohe}
	\begin{center}
		\begin{subfigure}[t]{.24\textwidth}
			\includegraphics[width=1\linewidth]{plots/rho_case1.png}
			\caption{$\rho=J^{-1}=0.25$}
		\end{subfigure}
		\begin{subfigure}[t]{.24\textwidth}
			\includegraphics[width=1\linewidth]{plots/rho_case3.png}
			\caption{$\rho=0.44$}
		\end{subfigure}
		\begin{subfigure}[t]{.24\textwidth}
			\includegraphics[width=1\linewidth]{plots/rho_case2.png}
			\caption{$\rho=0.76$}
		\end{subfigure}
		%	\begin{subfigure}[t]{.19\textwidth}
			%\includegraphics[width=1\linewidth]{plots/rho_case4.png}
			%  \caption{$\rho=1$}
			%\end{subfigure}
			\begin{subfigure}[t]{.24\textwidth}
				\includegraphics[width=1\linewidth]{plots/rho_case5.png}
				\caption{$\rho=1$}
			\end{subfigure}
		\end{center}
		\bnotetab{This figure illustrate the cohesion coefficient $\rho$ in four examples. It shows the smallest, largest and in-between cases of $\rho$. The columns represent the $J=4$ different covariates.The blue boxes indicate the active sets for each panel.}   
	\end{figure}
	
	
	
	Our novel simultaneity count and cohesiveness measure are the basis for modifying a Bonferroni test for FWER-controlled inference. Theorem \ref{thm_MT} formally states the FWER control. The proof is in Appendix \ref{app:proof}. 
	
	\begin{theorem}{\bf FWER control}\label{thm_MT}\\
		Under Assumption \ref{assu:valid_p}, 
		%$p^{(n)}_j$ are valid $p$-values for each univariate unit $n$ for all covariates $j$ in the family $H_D$. Then 
		the following rejection rule has FWER$\leq \gamma$ on $H_D$:  
		\begin{equation}
			\min_{n\in \mathcal{K}_j} \left \{p^{(n)}_j \right\}\leq \rho\frac{\gamma}{ N_j}\Rightarrow\text{ Reject $H_{0,j}$},
		\end{equation}
		where $p^{(n)}_j$ are valid post-selection $p$-values for the active covariates $j$ of unit $n$, and $\rho $ is the panel cohesion coefficient.
	\end{theorem}
	
	Theorem \ref{thm_MT} is based on an algebraic union bound argument, that leverages the structure of the panel hypotheses $H_D$. The assumptions on the data generating process and asymptotic distribution are implicitly included in the valid post-selection $p$-values.
	
	
	This completes the joint testing procedure. First, we calculate $p$-values after running a sparse linear time-series regression. Second, we use the output of the sparse linear estimation to write down a hypothesis and, third, we provide a FWER control inference procedure by combining the $p$-values across the cross-section and test the hypothesis.
	
	The difference between a naive Bonferroni and our FWER control is particularly pronounced for weak covariates that affect only a subset of the cross-sectional units. Given a FWER control level of $\gamma$, the rejection threshold for a naive Bonferroni test is $\frac{\gamma}{J N}$ for every covariate. The rejection threshold for our FWER control is always higher, and differs in particular when $N_j$ is small and $\rho$ is large. This is the case for weak covariates in a cohesive panel. 
	
	As it is common in statistical inference, we focus on Type I error control. Type II error rates require the specification of alternatives. While we do not provide formal theoretical results for the power of our inference approach, we show comprehensively in the simulation and empirical part, that our approach has substantially higher power than conventional approaches. 
	
	
	We point out that the validity of our procedure holds for unbalanced panels as well. This is because even when there are different number of observations for the $n$th and $m$th units, i.e. $T_n\neq T_{m}$ for $n \neq m$, they can still be estimated separately in the first stage of the regularized regression. The hypothesis testing and selection of a parsimonious model only requires the matrix $\bm{P}$ of valid $p$-values, which can be based on samples of different sizes. 
	
	
	
	
	%We currently are not performing such analysis on the power or Type II error rate, either asymptotically as $N,d\to\infty$ or in finite sample, of the procedure in (\ref{thm_MT}), not only because of the technical challenges, but also because power studies require specification of alternatives. Given the appropriate alternative hypothesis can be highly contextual, we leave this open for work in the future.
	
	%\textit{Remark 5: }Note that the above theory is controlling for FWER when the $p$-values are exact. When using approximated versions $\hat{p}^{(j)}(i)$, which are asymptotically valid, there can be pointwise diminishing stochastic residual $\forall i,j:\hat{p}^{(j)}(i)-{p}^{(j)}(i)=o_p(T^{-\kappa})$. We have not yet analyzed how large $\kappa$ needs to be when we aggregate over the data-driven family. Careful analysis of this would reveal up to what rate can we allow for $N$ to grow with respect to $T$, when using asymptotic $p$-values. In terms of power of the procedure when $N$ is large, existing arguments such as \cite{10.1214/10-AOS869} has showed that for sparse truth the Bonferroni method is Bayesian optimal when the number of hypotheses being tested is growing. 
	
	% Our ASSUMPTION \ref{asu1} satisfy the first type sparsity in \cite{10.1214/10-AOS869}, but our null distribution families are truncated Gaussian whereas most discussions on this topic considered full Gaussian. In simulations later (in Figure \ref{fig:olspvalboxplots} and Table \ref{tab3}), we find our modified Bonferroni method works well when the asymptotic $p$-values come from truncated Gaussian at least when $N$ grow linearly as $T$, even though we do not have theoretical reasoning for this situation. 
	
	
	
	
	
	
	
	
	
	%
	%Multiple testing adjustment
	%\begin{itemize}
	%\item control Type I error. Standard approach to define confidence intervals for estimators. 
	%\item combine test and localization subsection
	%\item no discussion about hat p. footnote about power discussion
	%\item Definition Panel-PoSI, clarify the choice of lambda for lasso penalty
	%\item Discussion about why we want inferential theory
	%\item Remove all remarks
	%\item Remove reference to empirics
	%\item Do not count all formulas, use proper bracket size
	%\item What is the number of factors, relate to factor literature
	%\item Table 1: compare data-driven hypothesis with simultaneity count with data-agnostic hypothesis and bonferoni
	%\item Table 1: use it to illustrate advantages: ordering in terms of importance, finding sparse model, understanding effect of covariates, discuss weak factors
	%\end{itemize}
	%
	%
	%Maybe we should define Panel PoSI after introduced the traversing the threshold? This is better as we can define the benefits better.
	%
	%
	
	
	
	\subsection{Least Number of Covariates: Traversing the Threshold}
	
	
	The typical logic of statistical inference is to determine which covariates we should admit from $X_M$, given a significance level $\gamma$. We use $K$ to denote the number of selected covariates. When $\gamma$ is specified as a lower quantity, we expect $K$ to decrease as well, that is,  the rejection becomes harsher.
	
	As the number of admitted covariates of our procedure is monotonically increasing in $\gamma$, we want to ask the following converse question: How do we need to set $\gamma$ such that we reject $K$ covariates? Concretely, we want to find:
	
	\begin{equation}\label{24}
		\gamma^*(K)=\sup \left\{\gamma | K=\sum_{j=1}^{J} \oo \left(\min_{n\in \mathcal{K}_j} \left \{p_j^{(n)} \right \}\leq \rho \frac{\gamma}{N_j} \right) \right\} .
	\end{equation}
	
	
	Let $p_j=\min_{n\in \mathcal{K}_j}\{p_j^{(n)}\}$ be the $1$st order statistic for $j=1,...,J$. Then (\ref{24}) is simply the $K$-th order statistics of $N_j p_j / \rho$:
	\begin{equation}
		\gamma^*(K)=\min\{N_ip_i/\rho |
		\exists j_1,j_2,...,j_K\in \{1,...,J \}: N_ip_i\geq N_{j_k}p_{j_k}
		\}.
	\end{equation}
	
	Since this minimization scan is monotone, we can determine how many covariates at least should be admitted, given a control level, which is similar to the ``SimpleStop'' procedure described in \cite{16-AOS1536}. The following corollary formalizes this inversion method that finds the least number of covariates to admit:
	
	\begin{corollary}{\bf Least number of covariates}\label{col3}\\
		Under Assumption \ref{assu:valid_p}, given the FWER level $\gamma$, there exists a unique number $K^*(\gamma)$ such that
		\begin{equation}
			K^*(\gamma)=
			\begin{cases}
				\argmax_{0\leq K\leq J}\gamma^*(K)\leq\gamma & \exists K:\gamma^*(K)\leq\gamma\\
				d & o.w.\\
			\end{cases}
		\end{equation}
	\end{corollary}
	
	The statement simply states that the simplest linear model should have at least $K^*(\gamma)$ covariates for a given $\gamma$. Note that it is possible that, for example, $\gamma^*(5)$ and $\gamma^*(6)$ are both equal to $0.05$, while $\gamma^*(7)>0.05$. In this case the minimum number of covariates is $K^*(0.05)=6$ because it does not hurt FWER-wise to include 6 covariates in the model. Hence, we are making a slightly different statement than that there would be exactly $K^*(\gamma)$ covariates in the true linear model. The number of covariates is obviously conditional on the set of candidate covariates $\bm{X}$, and we can only make statements for this given set. 
	
	In our empirical study we consider candidate asset pricing factors $\bm{X}$ to explain the investment strategies $\bm{Y}$. More generally, the linear model that we consider is often referred to as a factor model. Therefore, we will also refer to the selected covariates as factors, and use these two expressions as synonyms moving forward. This directly links our procedure to the literature on estimating the number of factors to explain a panel. A common approach in this literature is to use statistics based on the eigenvalues of either $\bm{Y}$ or $\bm{X}$ to make statements about the underlying factor structure. Our approach is different, as it provides significance levels for the selected factors and FWER control for the number of factors.
	
	%One simple reason $K^*(\gamma)$ could be different from $s$ is that the observed factor set might be mis-specified. Other than non-linearity, the common case is that there are unobserved features were missing from our $\bm{X}$ but indeed has non-zero loading the truth, then Corollary \ref{col3} cannot hope to recover the correct factor count. One potential avenue for remedying this is to consider further structural assumptions such as those in \cite{1468-0262.00273}, \cite{40985808} and \cite{ECTA8968} along with a generalization of $\bm{X}$ being not exogenous. This would lead to similar non-linear truncation sets as discussed in Section 2.2 and Online Appendix Section 2, which we do not pursue within the scope of the current paper. 
	
	
	Table \ref{tab:toy} illustrates the estimation of the number of factors and their ranking with our running example introduced in Figure \ref{fig:demo_running}. We calculate the simultaneity counts $N_j$'s as given in (\ref{eq28:simcount}) and demonstrated in Figure \ref{fig:demo1}, and $p_j$ as the smallest $p$-values associated with the $j$th covariate. Then, the rejection rule in Theorem \ref{thm_MT} is based on whether a pre-specified level $\gamma$ satisfies $p_j<\frac{\rho \gamma}{N_j}$, which is equivalent to $\rho ^{-1}\cdot N_j\cdot p_j <\gamma$. 
	
	\begin{table}[t!]
		\tcaptab{Sorted $p$-values for the running example}\label{tab:toy}
		\centering
		\begin{tabular}{cc|cc|cc}
			\toprule		Factor ($j$) & $p_j$ &\multicolumn{2}{c|}{Simultaneity count for $H_D$} & \multicolumn{2}{c}{Conventional Bonferroni for $H_A$}\\ 
			&    & $\rho^{-1} \cdot N_j$    &$\rho^{-1} \cdot N_j \cdot p_j$ & $J\cdot N$&$J\cdot N\cdot p_j$\\ \midrule
			3 &$<0.001$ & 22.1&$<0.001$&24& 0.002\\
			4   &$<0.001$& 11.1& 0.001&24 & 0.003\\
			1   & 0.005 & 4.7& 0.024&24&0.120 \\
			2   & 0.002& 14.3 & 0.028&24&0.051\\ \bottomrule
		\end{tabular}
		\bnotetab{This table constructs ``significance'' levels for the running example introduce in Figure \ref{fig:demo_running}. We compare the simultaneity count for the data-driven hypotheses $H_D$ and a onventional Bonferroni count for data-agnostic hypotheses $H_A$. The products $N_j \cdot p_j$, respectively $J\cdot N\cdot p_j$, can be interpreted as the significance levels for the corresponding approach. Given a FWER control $\gamma$ all factors with $\rho^{-1}\cdot N_j \cdot p_j$ (respectively $J\cdot N\cdot p_j$) below this threshold are selected.}
	\end{table}
	
	Thus, the natural ranking of the covariates is to sort all covariates in descending order of the $\rho ^{-1}\cdot N_j\cdot p_j $ values as shown in Table \ref{tab:toy}. It is then trivial to determine $K^*(\gamma)$ for any choice of $\gamma$. For example, for $\gamma=1\%$, we would select factors 3 and 4, but not 1 or 2. On the other hand, for $\gamma>2\%$, we would include all four factors. Hence, the ranking of $\rho ^{-1}\cdot N_j\cdot p_j $ directly maps into $K^*(\gamma)$. Moreover, the ordered list of $\rho ^{-1}\cdot N_j\cdot p_j $ provides an importance ranking of the factors. Furthermore, the number $N_j$ reveals if significant factors are ``weak''. In our case, factor 1 has $N_1=3$, which indicates that it affects only a small number of hypothesis. Its $p$-value $p_1$ is sufficiently small to still imply significance in terms of FWER control.
	
	For comparison, Table \ref{tab:toy} also includes the corresponding analysis for the data-agnostic hypothesis and a conventional Bonferroni correction. The Bonferroni analysis uses the same $p$-values but a different multiple testing adjustment. In our case, the $p$ values would be multiplied by $J \cdot N=24$ as this corresponds to the total number of hypothesis tests. This will obviously make the inference substantially more conservative. Indeed, even for a FWER control of $\gamma=4\%$, we would only select factors 3 and 4. We would need to raise the FWER control to $\gamma=12\%$ to include factor 1. Hence, weak factors, like factor 1, are more likely to be discarded by the data-agnostic hypothesis with conventional multiple testing adjustment. 
	
	We emphasize that a data-agnostic hypotheses with conventional Bonferroni correction does provide correct FWER control, but it is overly conservative, and does not sufficiently leverage information already observable in LASSO estimation results. By construction, the data-agnostic Bonferroni approach will test a larger number of hypothesis, which means that the corresponding ``significance levels'' will always be lower or equal to our data-driven simultaneity count. Second, the data-agnostic Bonferroni approach does not differentiate the ``strength'' of the factors, while our approach provides a selection-based heterogeneous adjustment of the $p$-values. This is essential for detecting weak factors.
	
	
	
	
	
	
	
	
	Having introduced all building blocks of our novel method to detect covariates, we put the entire procedure together as ``Panel-PoSI'':
	
	\begin{procedure}{\bf Panel-PoSI}\\
		The Panel-PoSI procedure consists of the following steps:
		\begin{enumerate}
			\item For each unit $n=1,...,N$ unit, we fit a linear sparse model $\hat{\beta}^{(n)}$ given $(\bm{X},\bm{Y},\lambda,\omega)$. We suggest cross-validation to select the LASSO penalty $\lambda$. We construct the sparse estimators $\bar{\beta}^{(n)}$ and the corresponding $p$-values for the active covariates for each unit, and collect them in the ``matrix'' of $p$-values $\bm{P}$. 
			\item We collect the panel-level sparse model selection event $\mathcal{M}$ and construct the data-driven hypothesis $H_D$.
			\item  Given the FWER control level $\gamma$ and based on the the simultaneity counts $N_j$'s of active covariates, we make inference decision for the sparse model. We can rank covariates in terms of their significance and select a parsimonious model that explains the full panel.
		\end{enumerate}
	\end{procedure}
	
	
	As we have now all results in place, we can summarize the advantages of our procedure. First, we want to clarify that our goals and results are different from just some form of optimal shrinkage selection. Selecting a shrinkage parameter with some form of cross-validation in a regularized estimator like LASSO does not provide the same insights and model that we do. A shrinkage estimator can either be applied to each unit separately, as we do it in our first step, or to the full panel in a LASSO panel regression. The separate covariate selection for each cross-sectional unit does not answer the question which covariates are needed to explain the full panel jointly. A shrinkage selection on the full panel for some form of panel LASSO can neglect weaker factors, as those receive a low weight in the cross-validation objective function. Second, tuning parameter selection with cross-validation requires a sufficiently large amount of data. Our approach is attractive as we can do the complete analysis on the same data. That means, an initial LASSO is used to first reduce the number of covariates, but this set is then further trimmed down using inferential theory. Hence, we can construct a parsimonious model even for data with a relatively short time horizon, but large cross-sectional dimension. Third, the statements that we can make are much richer than a simple variable selection. We can formally assess the relative importance of factors in terms of their significance. The model selection is directly linked to a form of significance level, which allows us to assess the relevance of including more factors. Last but not least, we can also make statements about the strength of factors. In summary, Panel-PoSI is a disciplined approach based on formal statistical theory to construct and interpret a parsimonious model.
	
	
	
	
	%Ordered Testing on nested family
	%\begin{itemize}
	%\item drop credit risk!
	%\item hierarchical structure in covariates (inclusion of second covariate only makes sense if the first covariate is included). 
	%\item Usual setup for PCs
	%\item Notation: N with upper script hierachical
	%\item Figure 3: Subfigures to explain what we count.
	%\item Procedure needs to be redefined
	%\end{itemize}
	
	
	
	\section{Ordered Multiple Testing on Nested Hypothesis Family}\label{sec:ordered}
	
	
	So far, our hypothesis family $H_D$ has no hierarchy and consequently, we have not imposed a sequential structures on the admission order of covariates of $\bm{X}$. However, there are cases where the covariates or factors warrant a natural order such that the family possesses a special testing logic. A hierarchical structure in covariates arises when the inclusion of the next covariate only make sense if the previous covariates is included. One example would be if the next covariates refines a property of the previous covariate. Another case is the use of principal component (PC) factors. The conventional logic is to include PCs sequentially from the dominating one to the least dominating one. This is similar to the motivation for \cite{16-AOS1536}, but different from them, we treat the PCs as exogenous without taking the estimation of PCs explicitly into account. In this section, we will use exogenous PCs as hierarchical covariates, as this is the main example in our empirical study. However, all the results hold for any set of exogenous hierarchical covariates.
	
	
	
	Without loss of generality, we presume $\bm{X}$ has the $k$th column as the $k$th nested factor. A $k$-order nested model is of the following form
	\begin{equation}
		k-\text{order nested model}: \bm{Y}=\bm{X}_{[k]}\bm{\beta}_{[k]}
	\end{equation}
	where $[k]=\{1,...,k\}$ is the set that includes indices up to $k$. For example, a hierarchical 3-order model corresponds to the case where variables $\bm{X}_{\{1,2,3\}}$ are included, but not for the rest of the covariates in $\bm{X}$. When formulating our hypothesis family, we must represent the sequential testing structure, as reflected in our definition of nested families of hypotheses:
	\begin{definition}{\bf Data-driven nested family}\label{def9}\\
		The data-driven nested family of hypotheses conditional on $\mathcal{M}$ is
		\begin{equation}
			H_{N}=\{H_{N,k}:k=0,1,...,J\},\q
			H_{N,k}=	\bigcap_{j\in\mathcal{K}_k}
			H_{N,k}^{(n)}\bigg\rvert \mathcal{M},\q
			H_{N,k}^{(n)}:\{k':\beta_{k'}^{(n)}\neq 0,k'\leq k\}.
		\end{equation}
	\end{definition}
	
	$H_{N,0}$ completes the case when no rejection on any factor is made. Whenever $H_{N,k}$ is true, then $H_{N,k'}$ is also true for $k<k'\leq J$. Moreover, in the cases where $\mathcal{K}_k=\emptyset$ but $\mathcal{K}_{k'}\neq\emptyset$ with $k<k'$, the notation ensures that the hypothesis $H_{N,k}$ is included in $H_N$ simply because $\mathcal{K}_{k'}$ is present. In other words, if a less dominating hypothesis $H_{N,k'}$ is suggested by data (that is, its active set is non-empty $\mathcal{K}_{k'}\neq\emptyset$), $H_N$ would automatically include all $H_{N,k}$ for $k\leq k'$.
	
	The FWER control property needs to be adapted to the nested nature of this family. \cite{16-AOS1536} argue that the proper measurement is to control for ordered factor count over-estimation with level $\gamma$, as follows:
	
	\begin{definition}{\bf FWER for nested family}\label{def10}\\
		For a test that rejects $H_{N,k}$ for $k=1,2,...,\hat{k}$ of $H_{N}$, the FWER control at the level $\gamma$ satisfies $\PP(\hat{k}\geq s)\leq \gamma$, where $s$ is the true factor count.
	\end{definition}
	
	Given the hierarchical logic embedded in the model, we need  the following assumption, which is more restrictive than Assumption \ref{assu:valid_p}:
	
	\begin{assumption}{\bf Tail $p$-values}\label{asu8}\\
		Under $	H_{N,k}^{(n)}$, it holds that $p^{(n)}_{k'}\iid\text{Unif }[0,1]$ for all $k'>k$.
	\end{assumption}
	
	Assumption \ref{asu8} only needs to hold for the tail hierarchical covariates, but requires them to be independent, whereas Assumption \ref{assu:valid_p} for the unordered tests does not require independence. In the case of PCs, it only applies to the lower order tail PC factors that should not be included for a given null hypothesis. For example, if the true model is $H_{N,5}$, we only need $p^{(n)}_{k'}\iid \text{Unif}[0,1]$ for $k'>5$, which is a usual type of assumption in this literature such as in \cite{rssb.12122}. Moreover, because the nested nature guarantees that the higher-order PCs are more likely to be null, a step-down procedure is expected to increase the power relative to a step-up procedure. 
	
	As our focus is to control for false discoveries, we also need to propose new simultaneity counts in the nested family setting. Concretely, we consider first taking a union to obtain the active unit set ${\mathcal{K}}_k^{\text{order}}$ and then calculate conservative simultaneity counts $N^{\text{order}}_k$, with $|M_j|$ as the number of units that the $j$th variable is active in:
	\begin{equation}
		{\mathcal{K}}^{\text{order}}_k=
		\bigcup_{k'\in\{k,k+1,...,J\}} \mathcal{K}_{k'},\q
		{N}_k^{\text{order}}=\sum_{j\in {\mathcal{K}}_k^{\text{order}}}|M_j| .
	\end{equation}
	It is possible for some $|M_k|$ to be 0 (for instance, the $k$th PC could be inactive for all units), but its ${N}_k^{\text{order}}$ would be 0 if and only if higher-order PCs all have $|M_{k'}|=0$ for $k'>k$. Note, that we are not assuming that $|M_j|$ is decreasing in $j$. Hence, our data-driven approach imposes only a nested structure in the hypotheses, but not a nested structure for the number of active covariates.
	
	% We fully adopt the data-driven approach because we are not putting assumption on whether $|M_j|$ is decreasing in $j$. In other words, we do not want to restrict ourselves only to cases where such monotonicity of LASSO active counts occur. Rather, our design accommodates the possibility that the data does not present a strict nested structure of covariates being less as less active in the cross-section.
	
	Figure \ref{fig:demo2} illustrates the process of our step-down simultaneity count. From the left, we start with factor $k=4$ and move step-wise down to factor $k=1$ on the right. The dark blue columns present the active factors, while the light blue columns capture factors of higher-order. %The sub-plots from left-to-right represent our calculation order from the highest-order PC factor to the 1st PC factor.
	In the left-most sub-figure, we only need to account for the 4th PC, implying ${N}_4^{\text{order}}=3$, whereas in the mid-left sub-figure, the 3rd PC has ${N}_3^{\text{order}}=2+3=5$. Eventually, in the right-most sub-figure, we have swept through the entire panel and the 1st PC has a simultaneity count of ${N}_1^{\text{order}}=12$.
	
	
	\begin{figure}[t!]
		\centering
		\tcaptab{Example of hierarchical simultaneity counts ${N}_k^{\text{order}}$ for $H_{N}$} \label{fig:demo2}
		\begin{center}
			\begin{subfigure}[t]{.24\textwidth}
				\includegraphics[width=1\linewidth]{plots/demoPC4}
				\caption{$N_4^{\text{order}}=3$}
			\end{subfigure}
			\begin{subfigure}[t]{.24\textwidth}
				\includegraphics[width=1\linewidth]{plots/demoPC3}
				\caption{$N_3^{\text{order}}=5$}
			\end{subfigure}
			\begin{subfigure}[t]{.24\textwidth}
				\includegraphics[width=1\linewidth]{plots/demoPC2}
				\caption{$N_2^{\text{order}}=8$}
			\end{subfigure}
			\begin{subfigure}[t]{.24\textwidth}
				\includegraphics[width=1\linewidth]{plots/demoPC1}
				\caption{$N_1^{\text{order}}=12$}
			\end{subfigure}
		\end{center}
		\bnotetab{This figure shows the simultaneity counts $N_i^{\text{order}}$ in an illustrative example. The subplots represent the simultaneity counts for the $J=4$ covariates and $N=6$ units. The dark blue columns present the active factors, while the light blue columns capture factors of higher-order. The sub-plots from left-to-right represent our calculation order from the highest-order factor to the 1st factor.}
	\end{figure}
	
	
	
	
	Now we can introduce a step-down procedure adapted to the nested structure of $H_{N}$:
	
	\begin{procedure}{\bf Step-down rejection of nested ordered family $H_{N}$} \label{procedure:nested}\\
		The step-down rejection procedure consists of the following steps:
		\begin{enumerate}
			\item For each $k \in \{1,...,J\}$ calculate the ordered simultaneity count ${N}_k^{\text{order}}$.
			\item For each $k \in \{1,...,J\}$ calculate the approximated R\'enyi representation ${Z}_k^{\text{order}}$ and its transformed reversed order statistics ${q}_k^{\text{order}}$:
			\begin{equation}
				{Z}_k^{\text{order}}=\sum_{i=k}^J\sum_{n\in\mathcal{K}_i}
				\frac{ \ln(p^{(n)}_k) }{{N}_1^{\text{order}}-{N}^{\text{order}}_{i+1}\oo\{i\neq J\}}
				,\q {q}^{\text{order}}_k=\exp(-{Z}^{\text{order}}_k)
			\end{equation}
			\item Reject hypothesis $1,2,...,\hat{k}$, where $\hat{k}=\max\{k:{q}^{\text{order}}_k\leq\frac{\gamma {N}^{\text{order}}_k}{JN}\}$.
			%\item[Step 1.] For each $k\in[d]$: calculate $\check{Y}_k^{(n)}=-\ln(p^{(n)}(k))$ for $j\in\mathcal{K}_i$ and $i\geq k$;
			%\item[Step 2.] For each $k\in[d]$: calculate the conservative simultaneity count $\check{N}_k$;
			%\item[Step 3.] For each $k\in[d]$: calculate the approximated R\'enyi representation $\check{Z}_k$ and its transformed reversed order statistics $\check{q}_k$:
			%\begin{equation}
			%    \check{Z}_k=\sum_{i=k}^d\sum_{j\in\mathcal{K}_i}
			%    \frac{\check{Y}^{(n)}_i}{\check{N}_1-\check{N}_{i+1}\oo\{i\neq d\}}
			%    ,\q \check{q}_k=\exp(-\check{Z}_k)
			%\end{equation}
			%\item[Step 4.] Reject hypothesis $1,2,...,\hat{k}$ where $\hat{k}=\max\{k:\check{q}_k\leq\frac{\gamma \check{N}_k}{dN}\}$.
		\end{enumerate}
	\end{procedure}
	This procedure will have FWER control at level $\gamma$ as stated in the following theorem:
	
	\begin{theorem}{\bf FWER control for ordered hypothesis}\label{thm4}\\
		Under Assumption \ref{asu8}, Procedure \ref{procedure:nested} has FWER control of $\gamma$ for the ordered hypothesis $H_{N}$.
	\end{theorem}
	
	%	\cmt{We fully adopt the data-driven approach because we are not putting assumption on whether $|M_j|$ is decreasing in $j$. In other words, we do not want to restrict ourselves only to cases where such monotonicity of LASSO active counts occur. Rather, our design accommodates the possibility that the data does not present a strict nested structure of covariates being less as less active in the cross-section.}
	
	The proof is deferred to the Online Appendix. This design extends Procedure 2 from \cite{rssb.12122} and ``Rank Estimation'' from \cite{16-AOS1536}, both of which focus on a single sequence of $p$-values rather than the panel setting.
	
	In Step 2, we use Assumption \ref{asu8} to transform $p$-values into $\ln(p^{(n)}_k)$, which are i.i.d. standard exponential random variables. Since the family $H_{N}$ has $J$ members, we need to modify our simultaneity count and in a sense condense the panel into a sequence of statistics associated with the ordered covariates. We built a staircase sequence of conservative simultaneity counts ${N}_k^{\text{order}}$ in Step 1 to accumulate the number of $p$-values we use up to the $k$th ordered covariate, starting from the end. By the R\'enyi representation of \cite{Rnyi1953OnTT}, the ${Z}_k^{\text{order}}$ of Step 2 approximate exponential order statistics and the ${q}_k^{\text{order}}$ approximate uniform order statistics. The nature of these approximations is to create a more conservative rejection, the technical details of which are examined in the proof in our Online Appendix. Finally, we run the order statistics through a step-down procedure proposed by \cite{2336545} so that we find the $\hat{k}$ largest number of ordered covariates rejected by the data with FWER control. Also note that even if the global null, i.e. $H_{N,0}$, is true, and every linear sparse model active set is empty, that is ${N}^{\text{order}}_1=0$, the procedure in Step 3 is still valid because we do not reject $H_{N,1}$.
	
	
	\section{Simulation}\label{sec:simulation}
	
	
	%example: Table 1, where does 0.002 come from?
	%
	%One large table with both FWER control
	%Use multicolumn to discuss separate cases
	%Remove RMSE
	%
	%Comparison of methods: 
	%Describe table
	%explain the choices that we can make
	%
	%lasso uses 5-fold cross-validation on 50\% in-sample data
	%
	%
	%Simulation setup:
	%linear model, simulation of different components
	%Selection of active set
	%Number of simulations
	%Variance of noise
	%Theorem for PosI assume homogeneous noise
	%dependent noise violates assumptions but still works
	%
	%
	%Description of results in table
	%Panel posy much have wrong selections
	%Important is that the control of false rejections
	%Overconservative results for bonferroni
	%correction of lasso p-values necessary
	%naive ols selects too many
	%
	%Better selection results in better OOS performance
	%OOS R2 the best for Panel PoSI
	%
	%Definition of R2
	%How exactly did you calculate the OOS R2 in Table 3 and 4? I thought that this R2 is essentially the explained variation normalized by the total variation, i.e. simply a normalized form of RMSE. This means that there is no intercept involved. Is this what you did?
	%%
	%
	%
	%paragraphs:
	%
	%1. Methods
	%2. Simulation
	%3. Selection
	%4. Metrics
	%5. Results independent noise
	%6 Results dependent noise
	%
	%
	
	
	
	
	
	
	We demonstrate in simulations that our inferential theory allows us to select better models. We compare different estimation approaches to select covariates and show that our approach better trades off false discovery and correct selections and hence results in a better out-of-sample performance.
	
	Table \ref{tab:summary} summarizes the benchmark models. Our framework contributes among three dimensions: the selection step for the sparse model, the construction of the hypothesis and the multiple testing adjustment. We consider variations for these three dimensions which yields in total six estimation methods. By varying the different elements of the estimators, we can understand the benefit of each component.
	
	
	
	\begin{table}[H]
		\tcaptab{Summary of estimation methods} \label{tab:summary}
		{\small
			\resizebox{\textwidth}{!}{
				\renewcommand{\arraystretch}{1.2}
				%   \setlength{\tabcolsep}{3.2pt}
				\centering
				\begin{tabular}{@{}llllll@{}}
					\toprule
					Name & Abbreviation & Selection & Hypothesis & Multiple Testing & Rejection rule \\ \midrule
					Naive OLS  &  N-OLS  &OLS without LASSO     & Agnostic $H_A$     & No adjustment           &  $ p^{\text{OLS}}< \gamma $      \\
					Bonferroni OLS  &  B-OLS &OLS without LASSO     & Agnostic $H_A$     & No adjustment           &  $ p^{\text{OLS}}< \frac{\gamma}{J N} $      \\
					Naive LASSO  &  N-LASSO &LASSO without PoSI     & Agnostic $H_A$     & No adjustment           &  $ p^{\text{LASSO}}< \gamma $      \\
					Bonferroni Naive LASSO  &  B-LASSO &LASSO without PoSI    & Agnostic $H_A$     & Bonferroni         &  $ p^{\text{LASSO}}< \frac{\gamma}{J N} $      \\
					Bonferroni PoSI  &  B-PoSI & LASSO with PoSI    & Agnostic $H_A$     & Bonferroni         &  $ p^{\text{PoSI}}< \frac{\gamma}{J N} $      \\
					Panel PoSI  &  P-PoSI & LASSO with PoSI    & Data-driven $H_D$     & Simultaneity count         &  $ p^{\text{PoSI}}< \frac{\rho \gamma}{N_i} $      \\
					\bottomrule
				\end{tabular}
		}}
		\bnotetab{This table compares the different methods to estimate a set of covariates from a large dimensional panel. For each method, we list the name and abbreviation. The selection refers to the regression approach for each univariate time-series. The hypothesis is either agnostic or data-driven given the selected subset of covariates. The multiple testing adjustment includes no adjustment, a conventional Bonferroni adjustment and our novel simultaneity count for a data-driven hypothesis. The rejection rules combine the valid p-values and multiple testing adjustment. $p^{\text{OLS}}$ is the p-value for a conventional t-statistics of an OLS estimator. $p^{\text{LASSO}}$ is the p-value without removing the lasso bias or adjusting for post-selection inference, that is, it is simply the OLS p-values using the selected subset of regressors. $p^{\text{PoSI}}$ is the debiased post-selection adjusted p-value based on Theorem \ref{thm1_main}.}
	\end{table}
	
	
	Our baseline model is Panel PoSI, which uses post-selection inference LASSO, and a simultaneity count for a data driven hypothesis. The first component that we modify is the selection of the sparse model. A simple OLS regression without shrinkage does not produce a sparse model. This gives us the methods Naive OLS and Bonferroni OLS. A conventional LASSO results in a sparse selection, but the p-values are not adjusted for the post-selection inference and the bias adjustment. The corresponding models are the Naive LASSO and the Bonferroni Naive LASSO. The second component is the hypothesis, which is agnostic for methods besides Panel PoSI. For the comparison models, we either consider no multiple testing adjustment or the conventional Bonferroni adjustment. Under the multiple testing adjustment we obtain the Bonferroni OLS, the Bonferroni Naive LASSO and the Bonferroni PoSI. The outcome of all the estimations are adjusted p-values for the covariates, which we use to select our model for a given target threshold. For a given value of $\gamma$ we include a covariate if its adjusted p-value is below the critical values summarized in the last column of Table \ref{tab:summary}.
	
	We simulate a simple and transparent model. Our panel follows the linear model
	\begin{align*}
		Y_{t}^{(n)}=\sum_{j=1}^J X_{t,j}\beta_{j}^{(n)}+\epsilon_{t}^{(n)}\qquad \text{for $t=1,...,T$, $n=1,...,N$ and $j=1,..,J$.}
	\end{align*}
	The covariates and errors are sampled independently as normally distributed random variables:
	\begin{align*}
		X_{t,j}\iid N(0,1), \qquad \epsilon_{t}\iid N(0,\Sigma) .
	\end{align*}
	The noise is either generated as independent noise with covariance matrix $\Sigma= \sigma^2 I$ or as cross-sectionally dependent noise with non-zero off-diagonal elements $\Sigma_{ij}=\kappa$ and diagonal elements $\Sigma_{ii}=\sigma^2$. Note that our theorems for PoSI assume homogeneous noise, while dependent noise violates our assumptions. Hence, the dependent noise allows us to test how robust our method is to misspecification. We set $\sigma^2=2$ and $\kappa=1$, but the results are robust to many choices. The covariance $\Sigma$ as treated as unknown and hence has to be estimated.
	
	
	\begin{figure}[t!]
		\centering
		\tcaptab{Design of loadings $\bm{\beta}$} \label{fig:simulation-design}
		\begin{center}
			\includegraphics[width=0.4\linewidth]{plots/simulation_design_staircase.png}
		\end{center}
		\bnotetab{This figure demonstrates the setting of our simulations with $10$ factors, where loadings are shaded based on whether they are active. In this staircase setting, the first factor affects all units, the 2nd factor affects $90\%$, and so on, and lastly the 10th factor affects $10\%$ of all units.}
	\end{figure}
	
	
	We construct the active set based on the staircase structure depicted in Figure \ref{fig:simulation-design}. Of the $J$ covariates in $X$, we have $K=10$ active independent factors. Figure \ref{fig:simulation-design} demonstrates the setting for the $10$ factors, where loadings are shaded based on whether they are active. The first factor affects all units, the 2nd factor affects $90\%$, and so on, and lastly the 10th factor affects $10\%$ of all units. This setting is relevant, and also challenging from a multiple testing perspective. It results in a large cohesion coefficient $\rho$, which makes the correct FWER control even more important. The loadings are sampled from a uniform distribution, if they are in the active set:
	\begin{align*}
		\beta_{j}^{(n)}\iid \textrm{Unif} \left[-\frac{1}{2},\frac{1}{2} \right] \quad \text{for $j$ in the active set}, \qquad \qquad \beta_{j}^{(n)}=0 \quad \text{for $j$ outside the active set.}
	\end{align*}
	
	
	\begin{table}[t!]
		\tcaptab{Simulation Comparison between Selection Methods}\label{tab:sim_results}
		\centering
		%\resizebox{\textwidth}{!}{
			\renewcommand{\arraystretch}{1.0}
			{\small
				\begin{tabular}{@{}llllr@{}}
					\toprule \midrule
					\multicolumn{5}{c}{Independent noise}   \\ \midrule \midrule
					Method & \# Selections & \# False Selections &  \# Correct Selections &  OOS $R^2$ \\ \midrule 
					\multicolumn{5}{c}{FWER $\gamma=5\%$}   \\ \midrule
					Panel PoSI             & 10.8 & 2.8  & 7.9  & 10.0\%    \\
					Bonferroni PoSI        & 4.7  & 0.0  & 4.7  & 8.0\%     \\
					Bonferroni Naive LASSO & 0.0  & 0.0  & 0.0  & 0.0\%     \\
					Naive LASSO            & 0.2  & 0.0  & 0.2  & 0.4\%    \\
					Bonferroni OLS         & 1.0  & 0.0  & 1.0  & 1.7\%    \\
					Naive OLS              & 99.2 & 89.2 & 10.0 & -144.2\%  \\      \midrule
					\multicolumn{5}{c}{FWER $\gamma=1\%$}   \\ \midrule
					Panel PoSI             & 8.6  & 1.1  & 7.5 & 10.6\%  \\
					Bonferroni PoSI        & 2.7  & 0.0  & 2.7 & 5.2\%   \\
					Bonferroni Naive LASSO & 0.0  & 0.0  & 0.0 & 0.0\%   \\
					Naive LASSO            & 0.1  & 0.0  & 0.1 & 0.3\%    \\
					Bonferroni OLS         & 0.2  & 0.0  & 0.2 & 0.5\%    \\
					Naive OLS              & 46.4 & 36.5 & 9.9 & -19.3\% \\     \midrule \midrule
					\multicolumn{5}{c}{Cross-sectionally dependent noise}   \\ \midrule \midrule
					Method & \# Selections & \# False Selections &  \# Correct Selections &  OOS $R^2$ \\ \midrule 
					\multicolumn{5}{c}{FWER $\gamma=5\%$}   \\ \midrule
					Panel PoSI             & 10.1 & 2.2  & 7.9  & 8.0\%   \\
					Bonferroni PoSI        & 4.4  & 0.0  & 4.4  & 7.2\%    \\
					Bonferroni Naive LASSO & 0.0  & 0.0  & 0.0  & 0.0\%   \\
					Naive LASSO            & 0.4  & 0.0  & 0.4  & 0.5\%    \\
					Bonferroni OLS         & 0.9  & 0.0  & 0.9  & 1.3\%    \\
					Naive OLS              & 83.7 & 73.7 & 10.0 & -83.8\%  \\      \midrule
					\multicolumn{5}{c}{FWER $\gamma=1\%$}   \\ \midrule
					Panel PoSI             & 7.9  & 0.6  & 7.3 & 10.3\% \\
					Bonferroni PoSI        & 2.4  & 0.0  & 2.4 & 3.9\%   \\
					Bonferroni Naive LASSO & 0.0  & 0.0  & 0.0 & 0.0\%   \\
					Naive LASSO            & 0.0  & 0.0  & 0.0 & 0.0\%   \\
					Bonferroni OLS         & 0.3  & 0.0  & 0.3 & 0.4\%   \\
					Naive OLS              & 31.0 & 21.2 & 9.8 & -6.8\% \\      
					\bottomrule
				\end{tabular}
			}
			%}
		\bnotetab{This table compares the selection results for different methods in a simulation. For each method we report the number of selected covariates, the number of falsely selected covariates and the number of correctly selected covariates. We also report the out-of-sample $R^2$ of the models that estimated with the selected covariates on the out-of-sample data. All results are averages of 100 simulations. The rejection FWER is set to $\gamma=5\%$ or $\gamma=1\%$. We simulate a panel of dimension $N=120$, $J=100$, $T=300$. The first half of time-series observations is used for the in-sample estimation and selection, while the second half serves for the out-of-sample analysis. The panel is generated by 10 independent factors. The active set of the factors follows the staircase structure of Figure \ref{fig:simulation-design}. The first factor affects all units, the second 90\%, and lastly the 10th factor affects 10\%. The unknown error variance is estimated based as a homogenous sample variance. The noise is either generated as independent noise with covariance matrix $\Sigma= \sigma^2 I$ or as cross-sectionally dependent noise with $\Sigma_{ij}=\kappa$ and $\Sigma_{ii}=\sigma^2$ for $\sigma^2=2$ and $\kappa=1$.}
	\end{table}
	
	We simulate a panel of dimension $N=120$, $J=100$ and $T=300$ with $K=10$ active factors. The first half of the time-series observations is used for the in-sample estimation and selection, while the second half serves for the out-of-sample analysis. All results are averages of 100 simulations. We use the covariates selected on the in-sample data for regressions out-of-sample. Our focus is on the inferential theory, and not on the bias correction for shrinkage. Hence, we first use the inferential theory on the in-sample data to select our set of covariates. Second, we use the selected subset of covariates in an OLS regression on the in-sample data to obtain the loadings. Last but not least, we apply the estimated loadings of the selected subset to the out-of-sample data to obtain the model fit. Note that this procedure helps a Naive LASSO, which in contrast to PoSI LASSO does not have a bias correction. The out-of-sample explained variation is measured by $R^2$, which is the sum of explained variation normalized by the total variation. The rejection FWER is set to $\gamma=5\%$ or $\gamma=1\%$. The LASSO shrinkage penalty $\lambda$ is selected by 5-fold cross-validation on the in-sample data. 
	
	
	
	
	Table \ref{tab:sim_results} compares the selection results for the different methods. For each method we report the number of selected covariates, the number of falsely selected covariates and the number of correctly selected covariates. We also report the out-of-sample $R^2$. The upper panel shows the results for independent noise, while the lower panel collects the results for cross-sectionally dependent noise.
	
	PanelPoSI clearly dominates all models. It provides the best trade-off between correct and false selection, which results in the best out-of-sample performance. In the case of $\gamma=5\%$ and independent noise, Panel PoSI selects 10.8 factors in a model generated by 10 factors. 7.9 of these factors are correct. A simple Bonferroni correction is overly conservative. The Bonferroni PoSI selects only 4.7 correct factors. While this overly conservative selection protects against false discovery, it omits over half of the relevant factors which lowers the out-of-sample performance. Using post-selection inference is important, as a naive lasso provides wrong p-values which makes the overly conservative selection even worse. The other extreme is to have neither shrinkage nor multiple testing adjustment. As expected the naive OLS has an extreme number of false selections with a correspondingly terrible out-of-sample performance. 
	
	As expected, tightening the FWER control to 1\% lowers the number of false rejections, but also the number of correct selections. It reveals again that Panel PoSI provides the best inferential theory among the benchmark models. Panel PoSI selects 7.5 correct covariates, while it controls the false rejections at 1.1. The overly conservative Bonferroni methods select even fewer correct covariates, which further deteriorates the out-of-sample performance. The gap in OOS $R^2$ between Panel PoSI and Bonferroni PoSI widens to 5.4\%. All the other approaches cannot be used for a meaningful selection.
	
	Panel PosI performs well, even when some of the underlying assumptions are not satisfied. The lower panel of Table \ref{tab:sim_results} shows the results for dependent noise. As the dependence in the noise is relatively strong, it can be interpreted as omitting a relevant factor in the set of candidate covariates $X$. Even thought the PoSI theory is developed for homogeneous noise, Panel PoSI continues to perform very well. In contrast, the comparison methods perform even worse, and the Bonferroni approaches select even fewer correct covariates.
	
	
	
	
	
	
	
	
	%
	%1. Methods
	%Reference to table
	%Three variations: selection, hypothesis testing, multiple testing adjustment
	%Contributions to all three elements
	%6 different variations
	%Understand the benefit of each component
	%Explain each method
	%
	%
	%
	%2. Simulation
	%Simulation setup:
	%linear model, simulation of different components
	%Selection of active set
	%Number of simulations
	%Variance of noise
	%Theorem for PosI assume homogeneous noise
	%dependent noise violates assumptions but still works
	%
	%3. Selection and Metrics
	%
	%
	%4. Results:
	%Description of results in table
	%Panel posy much have wrong selections
	%Important is that the control of false rejections
	%Overconservative results for bonferroni
	%correction of lasso p-values necessary
	%naive ols selects too many
	%Results independent noise
	%Results dependent noise
	
	
	
	%We simulate a panel of ($\bm{Y}$,$\bm{X}$) using the following DGP:
	%\begin{itemize}
	%    \item $\epsilon_{t}\iid N(0,\Sigma)$;
	%    \item $\beta_{j}^{(n)}\iid \textrm{Unif}[-\frac{1}{2},\frac{1}{2}]$ if $j$ is a real factor for $n$th unit;
	%    \item $\beta_{j}^{(n)}=0$ if $j$ is null for $n$th unit;
	%    \item $X_{t,j}\iid N(0,1)$;
	%    \item $Y_{t,n}=\sum_{j=1}^J X_{t,j}\beta_{j}^{(n)}+\epsilon_{t,n}$.
	%\end{itemize}
	
	
	
	
	% \begin{figure}[t!]
		% 	\centering
		% 		\tcaptab{Design of loadings $\bm{\beta}$} \label{fig:simulation-design}
		% 			\begin{center}
			% 	\begin{subfigure}[t]{.4\textwidth}
				% 	\includegraphics[width=1\linewidth]{plots/simulation_design-strongweak.png}
				% 	  \caption{Strong-Weak}
				% 	\end{subfigure}\hspace*{2cm}
			% 		\begin{subfigure}[t]{.4\textwidth}
				% 	\includegraphics[width=1\linewidth]{plots/simulation_design-staircase.png}
				% 	  \caption{Staircase}
				% 	\end{subfigure}
			% 	\end{center}
		% 	\bnotetab{This figure demonstrates the setting of our simulations with $10$ factors, where loadings are shaded based on whether they were active in our simulation. In Strong-Weak setting of subfigure (a), the dark blue regions indicate the $5$ strong factors that affect all units and the light blue regions indicate $5$ weak factors that each affects $20\%$ of units. In Staircase setting of subfigure (b), we have 1st factor that affects all units, 2nd factor that affects $90\%$ of all units, ..., and last factor that affects $10\%$ of all units.}
		% \end{figure}
	
	
	
	
	% 
	%\begin{table}[t!]
	%\tcaptab{Simulation comparison between selection methods (Staircase Setting)}
	%\centering
	%%\resizebox{\textwidth}{!}{
		%\renewcommand{\arraystretch}{1.0}
		%{\small
			%\begin{tabular}{@{}llllr@{}}
			%\toprule
			%Method & \# Selections & \# False Selections &  \# selected factors &  OOS $R^2$ \\ \midrule \midrule
			%\multicolumn{5}{c}{Independent noise}   \\ \midrule \midrule
			%\multicolumn{5}{c}{FWER $\gamma=5\%$}   \\ \midrule
			%Panel PoSI             & 10.8 & 2.8  & 7.9  & 10.0\%    \\
			%Bonferroni PoSI        & 4.7  & 0.0  & 4.7  & 8.0\%     \\
			%Bonferroni Naive LASSO & 0.0  & 0.0  & 0.0  & 0.0\%     \\
			%Naive LASSO            & 0.2  & 0.0  & 0.2  & 0.4\%    \\
			%Bonferroni OLS         & 1.0  & 0.0  & 1.0  & 1.7\%    \\
			%Naive OLS              & 99.2 & 89.2 & 10.0 & -144.2\%  \\      \midrule
			%\multicolumn{5}{c}{FWER $\gamma=1\%$}   \\ \midrule
			%Panel PoSI             & 8.6  & 1.1  & 7.5 & 10.6\%  \\
			%Bonferroni PoSI        & 2.7  & 0.0  & 2.7 & 5.2\%   \\
			%Bonferroni Naive LASSO & 0.0  & 0.0  & 0.0 & 0.0\%   \\
			%Naive LASSO            & 0.1  & 0.0  & 0.1 & 0.3\%    \\
			%Bonferroni OLS         & 0.2  & 0.0  & 0.2 & 0.5\%    \\
			%Naive OLS              & 46.4 & 36.5 & 9.9 & -19.3\% \\     \midrule \midrule
			%\multicolumn{5}{c}{Cross-sectionally dependent noise}   \\ \midrule \midrule
			%\multicolumn{5}{c}{FWER $\gamma=5\%$}   \\ \midrule
			%Panel PoSI             & 10.1 & 2.2  & 7.9  & 8.0\%   \\
			%Bonferroni PoSI        & 4.4  & 0.0  & 4.4  & 7.2\%    \\
			%Bonferroni Naive LASSO & 0.0  & 0.0  & 0.0  & 0.0\%   \\
			%Naive LASSO            & 0.4  & 0.0  & 0.4  & 0.5\%    \\
			%Bonferroni OLS         & 0.9  & 0.0  & 0.9  & 1.3\%    \\
			%Naive OLS              & 83.7 & 73.7 & 10.0 & -83.8\%  \\      \midrule
			%\multicolumn{5}{c}{FWER $\gamma=1\%$}   \\ \midrule
			%Panel PoSI             & 7.9  & 0.6  & 7.3 & 10.3\% \\
			%Bonferroni PoSI        & 2.4  & 0.0  & 2.4 & 3.9\%   \\
			%Bonferroni Naive LASSO & 0.0  & 0.0  & 0.0 & 0.0\%   \\
			%Naive LASSO            & 0.0  & 0.0  & 0.0 & 0.0\%   \\
			%Bonferroni OLS         & 0.3  & 0.0  & 0.3 & 0.4\%   \\
			%Naive OLS              & 31.0 & 21.2 & 9.8 & -6.8\% \\      
			%    \bottomrule
			%    \end{tabular}
		%}
	%%}
%\bnotetab{This table compares the selection results for different methods in a simulation. For each method we report the number of selected covariates, the number of falsely selected covariates and the number of correctly selected covariates. We also report the out-of-sample $R^2$ of the estimated models. All results are averages of 100 simulations. The rejection FWER is set to $\gamma=5\%$. We simulate a panel of dimension $N=120$, $J=100$, $T=300$. The first half of time-series observations is used for the in-sample estimation and selection, while the second half serves for the out-of-sample analysis. The panel is generated by 10 independent factors. The noise $\sigma=2$, and cross-sectional $\kappa=1$.\\
	%\textbf{Staircase}: First factor affects all units, 2nd factor affects only 90\% of the units, ..., and 10th factor affects only 10\% of units. Variances are presumed unknown.}
%\end{table}
%
%
%
%\begin{table}[H]
%\tcaptab{Simulation comparison between selection methods (Staircase Setting)}     \centering
%{Independent noise $(\Sigma=\sigma^2I)$}\\
%{\small
	%\resizebox{\textwidth}{!}{
		%\renewcommand{\arraystretch}{1.2}
		%     %  \setlength{\tabcolsep}{3.2pt}
		%\begin{tabular}{@{}llllll@{}}
		%\toprule
		%Method & \# Selections & \# False Selections &  \# selected factors &  OOS $R^2$ & OOS RMS\\ \midrule
		%Panel PoSI             & 10.8 & 2.8  & 7.9  & 10.0\%   & 1.485 \\
		%Bonferroni PoSI        & 4.7  & 0.0  & 4.7  & 8.0\%    & 1.502 \\
		%Bonferroni Naive LASSO & 0.0  & 0.0  & 0.0  & 0.0\%    & 1.566 \\
		%Naive LASSO            & 0.2  & 0.0  & 0.2  & 0.4\%    & 1.563 \\
		%Bonferroni OLS         & 1.0  & 0.0  & 1.0  & 1.7\%    & 1.553 \\
		%Naive OLS              & 99.2 & 89.2 & 10.0 & -144.2\% & 2.447 \\       
		%    \bottomrule
		%\end{tabular}
		%}}\\
%{Cross-sectionally dependent noise $(\Sigma_{ij}=\kappa$, $\Sigma_{ii}=\sigma^2)$}\\
%{\small
	%\resizebox{\textwidth}{!}{
		%\renewcommand{\arraystretch}{1.2}
		%     %  \setlength{\tabcolsep}{3.2pt}
		%\begin{tabular}{@{}llllll@{}}
		%\toprule
		%Method & \# Selections & \# False Selections &  \# selected factors &  OOS $R^2$ & OOS RMS\\ \midrule
		%Panel PoSI             & 10.1 & 2.2  & 7.9  & 8.0\%   & 1.512 \\
		%Bonferroni PoSI        & 4.4  & 0.0  & 4.4  & 7.2\%   & 1.520 \\
		%Bonferroni Naive LASSO & 0.0  & 0.0  & 0.0  & 0.0\%   & 1.581 \\
		%Naive LASSO            & 0.4  & 0.0  & 0.4  & 0.5\%   & 1.576 \\
		%Bonferroni OLS         & 0.9  & 0.0  & 0.9  & 1.3\%   & 1.568 \\
		%Naive OLS              & 83.7 & 73.7 & 10.0 & -83.8\% & 2.135 \\       
		%    \bottomrule
		%\end{tabular}
		%}}
%\bnotetab{This table compares the selection results for different methods in a simulation. For each method we report the number of selected covariates, the number of falsely selected covariates and the number of correctly selected covariates. We also report the out-of-sample $R^2$ of the estimated models. All results are averages of 100 simulations. The rejection FWER is set to $\gamma=5\%$. We simulate a panel of dimension $N=120$, $J=100$, $T=300$. The first half of time-series observations is used for the in-sample estimation and selection, while the second half serves for the out-of-sample analysis. The panel is generated by 10 independent factors. The noise $\sigma=2$, and cross-sectional $\kappa=1$.\\
	%\textbf{Staircase}: First factor affects all units, 2nd factor affects only 90\% of the units, ..., and 10th factor affects only 10\% of units. Variances are presumed unknown.}
%\end{table}
%
%
%\begin{table}[H]
%\tcaptab{Simulation comparison between selection methods (Staircase Setting)}     \centering
%{Independent noise $(\Sigma=\sigma^2I)$}\\
%{\small
	%\resizebox{\textwidth}{!}{
		%\renewcommand{\arraystretch}{1.2}
		%     %  \setlength{\tabcolsep}{3.2pt}
		%\begin{tabular}{@{}llllll@{}}
		%\toprule
		%Method & \# Selections & \# False Selections &  \# selected factors &  OOS $R^2$ & OOS RMS\\ \midrule
		%Panel PoSI             & 8.6  & 1.1  & 7.5 & 10.6\%  & 1.483 \\
		%Bonferroni PoSI        & 2.7  & 0.0  & 2.7 & 5.2\%   & 1.527 \\
		%Bonferroni Naive LASSO & 0.0  & 0.0  & 0.0 & 0.0\%   & 1.569 \\
		%Naive LASSO            & 0.1  & 0.0  & 0.1 & 0.3\%   & 1.566 \\
		%Bonferroni OLS         & 0.2  & 0.0  & 0.2 & 0.5\%   & 1.564 \\
		%Naive OLS              & 46.4 & 36.5 & 9.9 & -19.3\% & 1.713\\       
		%    \bottomrule
		%\end{tabular}
		%}}\\
%{Cross-sectionally dependent noise $(\Sigma_{ij}=\kappa$, $\Sigma_{ii}=\sigma^2)$}\\
%{\small
	%\resizebox{\textwidth}{!}{
		%\renewcommand{\arraystretch}{1.2}
		%     %  \setlength{\tabcolsep}{3.2pt}
		%\begin{tabular}{@{}llllll@{}}
		%\toprule
		%Method & \# Selections & \# False Selections &  \# selected factors &  OOS $R^2$ & OOS RMS\\ \midrule
		%Panel PoSI             & 7.9  & 0.6  & 7.3 & 10.3\% & 1.488 \\
		%Bonferroni PoSI        & 2.4  & 0.0  & 2.4 & 3.9\%  & 1.541 \\
		%Bonferroni Naive LASSO & 0.0  & 0.0  & 0.0 & 0.0\%  & 1.577 \\
		%Naive LASSO            & 0.0  & 0.0  & 0.0 & 0.0\%  & 1.577 \\
		%Bonferroni OLS         & 0.3  & 0.0  & 0.3 & 0.4\%  & 1.572 \\
		%Naive OLS              & 31.0 & 21.2 & 9.8 & -6.8\% & 1.624 \\       
		%    \bottomrule
		%\end{tabular}
		%}}
%\bnotetab{This table compares the selection results for different methods in a simulation. For each method we report the number of selected covariates, the number of falsely selected covariates and the number of correctly selected covariates. We also report the out-of-sample $R^2$ of the estimated models. All results are averages of 100 simulations. The rejection FWER is set to $\gamma=1\%$. We simulate a panel of dimension $N=120$, $J=100$, $T=300$. The first half of time-series observations is used for the in-sample estimation and selection, while the second half serves for the out-of-sample analysis. The panel is generated by 10 independent factors. The noise $\sigma=2$, and cross-sectional $\kappa=1$.\\
	%\textbf{Staircase}: First factor affects all units, 2nd factor affects only 90\% of the units, ..., and 10th factor affects only 10\% of units. Variances are presumed unknown.}
%\end{table}

% \begin{table}[H]
	% \tcaptab{Simulation comparison between selection methods (Strong-Weak Setting)}     \centering
	% {Independent noise $(\Sigma=\sigma^2I)$}\\
	% {\small
		% \resizebox{\textwidth}{!}{
			% \renewcommand{\arraystretch}{1.2}
			%      %  \setlength{\tabcolsep}{3.2pt}
			% \begin{tabular}{@{}lllllll@{}}
				% \toprule
				% Method & \# Selections & \# False Selections &  \# selected strong factors & \# selected weak factors & OOS $R^2$ & OOS RMS\\ \midrule
				% Panel PoSI             & 10.5 & 1.5  & 4.9 & 4.1 & 13.1\%  & 1.471 \\
				% Bonferroni PoSI        & 6.5  & 0.1  & 4.3 & 2.2 & 11.9\%  & 1.482 \\
				% Bonferroni Naive LASSO & 0.9  & 0.0  & 0.7 & 0.1 & 2.1\%   & 1.562 \\
				% Naive LASSO            & 12.2 & 3.1  & 5.0 & 4.1 & 12.0\%  & 1.481 \\
				% Bonferroni OLS         & 6.8  & 0.0  & 4.5 & 2.3 & 12.4\%  & 1.477 \\
				% Naive OLS              & 82.5 & 72.5 & 5.0 & 5.0 & -88.6\% & 2.154\\   
				%     \bottomrule
				% \end{tabular}
			% }}
	% {Cross-sectionally dependent noise $(\Sigma_{ij}=\kappa$, $\Sigma_{ii}=\sigma^2)$}\\
	% {\small
		% \resizebox{\textwidth}{!}{
			% \renewcommand{\arraystretch}{1.2}
			%      %  \setlength{\tabcolsep}{3.2pt}
			% \begin{tabular}{@{}lllllll@{}}
				% \toprule
				% Method & \# Selections & \# False Selections &  \# selected strong factors & \# selected weak factors & OOS $R^2$ & OOS RMS\\ \midrule
				% Panel PoSI             & 9.3  & 0.8  & 4.7 & 3.8 & 11.7\%  & 1.481 \\
				% Bonferroni PoSI        & 5.8  & 0.1  & 3.8 & 1.9 & 9.8\%   & 1.497 \\
				% Bonferroni Naive LASSO & 0.7  & 0.0  & 0.6 & 0.1 & 1.3\%   & 1.567 \\
				% Naive LASSO            & 10.6 & 1.9  & 4.9 & 3.9 & 10.4\%  & 1.492 \\
				% Bonferroni OLS         & 5.7  & 0.0  & 3.9 & 1.8 & 10.6\%  & 1.491 \\
				% Naive OLS              & 67.2 & 57.2 & 5.0 & 5.0 & -64.1\% & 1.996\\   
				%     \bottomrule
				% \end{tabular}
			% }}
	% \bnotetab{This table compares the selection results for different methods in a simulation. For each method we report the number of selected covariates, the number of falsely selected covariates, the number of correctly selected strong covariates and the number of correctly selected weak factors. We also report the out-of-sample $R^2$ of the estimated models. All results are averages of 100 simulations. The rejection FWER is set to $\gamma=5\%$. We simulate a panel of dimension $N=100$, $J=100$, $T=300$. The first half of time-series observations is used for the in-sample estimation and selection, while the second half serves for the out-of-sample analysis. The panel is generated by 10 independent factors. The noise $\sigma=2$, and cross-sectional $\kappa=1$. \\
		% \textbf{Strong-Weak}: First 5 are strong factors that affect all units. The other 5 are weak factors, each of which affects different 20\% of units. Variances are presumed unknown.}
	% \end{table}

\section{Empirical Analysis}\label{sec:empirics}


%
%Subsections:
%1 Data
%2 Asset pricing
%3. Number of factors
%
%
%Empirics:
%
%1. Data description
%2. Benchmark setup description
%3. Asset pricing description
%4. Number of factors for HL factors
%5. Results RMSE
%6. Results alphas
%7. Results selected factors
%8. Number of factors motivation
%9. Number of factors results

\subsection{Data and Problem}

Our empirical analysis studies a fundamental problem in asset pricing. We select a parsimonious factor model from a large set of candidate factors that can jointly explain the asset prices of a large cross-section of investment strategies. Our data is standard and obtained from the data libraries of Kenneth French and \cite{HouEtAl}. 


We consider monthly excess returns from January 1967 to December 2021, which results in a time dimension of $T=660$. Our test assets are the $N = 243$ double-sorted portfolios of Kenneth French's data library summarized in Table \ref{DSSource} in the Appendix. The candidate factors are $J=114$ univariate long-short factors based on the data of \cite{HouEtAl}. We include all univariate portfolio sorts from their data library that are available for our time period, and construct top minus bottom decile factor portfolios. In addition, we include the five Fama-French factors of \cite{FAMA20151} from Kenneth French's data library.



Our analysis projects out the excess return of the market factor. We are interested in the question which factors explain the component that is orthogonal to market movements. Hence, we regress out the market factor from the test assets and use the residuals as test assets. We also do not include a market factor in the set of long-short candidate factors. The original test assets have a market component as they are long only portfolios. Our results are essentially the same when we include the market component in the test assets, with the only difference that we would need to include the market factor as an additional factor in our parsimonious models. The market factor would always be selected by all models as significant, but this by itself is neither a novel nor interesting result.  

We present in-sample and out-of-sample results. The in-sample analysis uses the first 330 observations (January, 1967 to June, 1994), while the out-of-sample results are based on the second 330 observations (July, 1994 to December, 2021). As in the simulation, we first use the inferential theory on the in-sample data to select our set of covariates. Second, we use the selected subset of covariates in an OLS regression on the in-sample data to obtain the loadings. Last but not least, we use the estimated loadings on the selected subset of factors for the out-of-sample model. The LASSO penalty $\lambda$ is selected via 5-fold cross-validation on the in-sample data.\footnote{Our cross-validation follows the one-standard-deviation rule for selecting parsimonious models, that is, the largest choice of $\lambda$ within 1 standard error of minimizing the squared errors. This is the default setting of popular implementations like \texttt{glmnet} and argued for in \S3.4 of \cite{hastie2009elements}. We select $\lambda$ from the grid $\exp(a)\cdot \log J/\sqrt{T}$ with $a=-8,...,8$. This grid choice satisfies the Assumptions in \cite{chatterjee2014assumptionless} and hence Assumption A.4.} Hence, LASSO represents a first-stage dimension reduction tool, and we need the inferential theory to select our final sparse model.



We allow our selection to impose a prior on two of the most widely used asset pricing models. More specifically, we estimate models without a prior, and two specific priors that impose an infinite weight on the Fama-French 3 factors (FF3) and the Fama-French 5 factors (FF5). This prior as part of PoSI LASSO enforces that the FF3 and FF5 factors are included in the active set. Note that because we work with data orthogonal to the market return, we do not include the market factor in the prior, but only the size and value factors for FF3 and in addition the investment and profitability factor for FF5. We denote these weights by $\omega_{\text{FF3}}$ and $\omega_{\text{FF5}}$. This is an example where the researcher has economic knowledge that she wants to include in her statistical selection method. 


We evaluate the models with standard metrics. The root-mean-squared error (RMSE) is based on the squared residuals relative to the estimated factor models. Hence, in-sample the models are estimated to minimize the RMSE. The pricing error is the economic quantity of interest. It is the time-series mean of the residual component of the factor model, and corresponds to the mean return that is not explained by the risk premia and exposure to the factors. In summary, we obtain the residuals as $\hat \epsilon =Y_{t,n}- X_S \hat \beta_S$ for the selected factors, where the loadings are estimated on the in-sample data. The metrics are the RMSE and mean absolute pricing error (MAPE):
\begin{align*}
	\text{RMSE}= \sqrt{ \frac{1}{N \, T}\sum_{i=1}^N \sum_{t=1}^T \hat \epsilon ^2}, \qquad \text{MAPE} = \frac{1}{N } \sum_{i=1}^N \left | \frac{1}{T} \sum_{t=1}^T \hat \epsilon  \right |.
\end{align*}

In addition to Panel PoSI without and with the FF3 and FF5 priors, we consider the benchmark methods of Table \ref{tab:summary}. We compare Panel PoSI (P-PoSI), Panel PoSI with infinite priors on FF3 and FF5 (P-PoSI $\omega_{\text{FF3}}$ respectively $\omega_{\text{FF5}}$), Bonferroni Naive LASSO (B-LASSO), Naive LASSO (N-LASSO), Bonferroni OLS (B-OLS) and Naive OLS (N-OLS). Our main analysis sets the FWER control to the usual $\gamma=5\%$.




%empirics:
%
%start with benchmark case: Explain DS with HL
%List number of estimated factors for FWER 5\% 
%No need for table. This can be done in text
%
%Show out-of-sample results in-sample and out-of-sample
%Need to explain metrics and how regressions are run
%
%Interpret factors
%
%New section: Number of factors
%Different set of candidates
%For all variation, we end up with parsimoneous model
%Bonferroni PoSI: too few factors
%Naive methods: too many factors
%




\subsection{Asset Pricing Results}


Panel PoSI selects parsimonious factor models with the best out-of-sample performance among the benchmarks. For the FWER rate of $\gamma=5\%$ the number of factors differs substantially among the different methods. Panel PoSI selects 3 factors. Imposing infinite priors on FF3 or FF5 results in 4 and 5 factors for P-PoSI $\omega_{\text{FF3}}$ respectively $\omega_{\text{FF5}}$. In contrast, the alternative approaches select too many factors. Bonferroni Naive LASSO includes 10, Naive Lasso 70, Bonferroni OLS 107 and Naive OLS 114. These over-parametrized models lead to overfitting of the in-sample data.



\begin{figure}[t!]
	\tcaptab{RMSE across cross-sections}
	\label{fig:RMSE}
	\begin{subfigure}[t]{.5\textwidth}\centering
		\includegraphics[width=.9\linewidth]{plots/Avg_RMSE_HL_DS_insample.png}
		\caption{In-sample}
	\end{subfigure}
	\begin{subfigure}[t]{.5\textwidth}\centering
		\includegraphics[width=.9\linewidth]{plots/Avg_RMSE_HL_DS_outsample.png}
		\caption{Out-of-sample}
	\end{subfigure}
	\bnotetab{
		This figure shows the in-sample and out-of-sample root-mean-squared errors (RMSE) for each cross-section of test assets for different factor models. The test assets are the $N=243$ double-sorted portfolios, and we show the RMSE for each set of double-sorts. The rejection FWER is set to $\gamma=5\%$ The candidate factors are the 114 univariate factor portfolios. The time dimension is $T=660$. We use the first half for the in-sample estimation and selection, while the second half serves for the out-of-sample analysis. We compare Panel PoSI (P-PoSI), Panel PoSI with infinite priors on FF3 and FF5 (P-PoSI $\omega_{\text{FF3}}$ respectively $\omega_{\text{FF5}}$), Bonferroni LASSO (B-LASSO), Naive LASSO (N-LASSO), Bonferroni OLS (B-OLS) and Naive OLS (N-OLS).          
	}		
\end{figure}

\begin{figure}[t!]
	\tcaptab{MAPE across cross-sections}
	\label{fig:MAPE}
	\begin{subfigure}[t]{.5\textwidth}\centering
		\includegraphics[width=.9\linewidth]{plots/Avg_alpha_HL_DS_insample.png}
		\caption{In-sample}
	\end{subfigure}
	\begin{subfigure}[t]{.5\textwidth}\centering
		\includegraphics[width=.9\linewidth]{plots/Avg_alpha_HL_DS_outsample.png}
		\caption{Out-of-sample}
	\end{subfigure}
	\bnotetab{
		This figure shows the mean absolute pricing errors (MAPE) for each cross-section of test assets for different factor models. The test assets are the $N=243$ double-sorted portfolios, and we show the average $|\alpha|$ for each set of double sorts. The rejection FWER is set to $\gamma=5\%$ The candidate factors are the 114 univariate factor portfolios. The time dimension is $T=660$. We use the first half for the in-sample estimation and selection, while the second half serves for the out-of-sample analysis. We compare Panel PoSI (P-PoSI), Panel PoSI with infinite priors on FF3 and FF5 (P-PoSI $\omega_{\text{FF3}}$ respectively $\omega_{\text{FF5}}$), Bonferroni LASSO (B-LASSO), Naive LASSO (N-LASSO), Bonferroni OLS (B-OLS) and Naive OLS (N-OLS).          
	}		
\end{figure}

Figure \ref{fig:RMSE} shows in-sample and out-of-sample RMSE for each set of double-sorts. The composition of the double sorts is summarized in Table \ref{DSSource} in the Appendix. The in-sample performance in the left subfigure has the expected result that more factors mechanically decrease the RMSE. The important findings are in the right subfigure with the out-of-sample RMSE. The uniformly best performing model is Panel PoSI without any priors. In fact, imposing a prior on the Fama-French factors increases the out-of-sample RMSE. The conventional LASSO and OLS estimates have substantially higher RMSE, which can be more than twice as large.

The Panel PoSI models also explain the average returns the best. In Figure \ref{fig:MAPE}, we compare the mean absolute pricing errors among the benchmarks for each set of double sorts. Importantly, the pricing errors are not used as in objective function of the estimation, and hence the fact that the models with the smallest RMSE explain expected returns is an economic finding supporting arbitrage pricing theory. Our Panel PoSI has the smallest out-of-sample pricing errors, which can be up to six times smaller compared to the OLS estimates. Including the Fama-French factors as a prior does not improve the models, except for the profitability and investment double sort, which uses the same information as two of the Fama-French factors. 




\begin{table}[t!]
	\tcaptab{Selected factors with Panel PoSI}
	\label{DSHL}
	\centering 
	\begin{tabular}{l|ccc|c}
		\toprule
		Factor  & $N_j$    & $p_j$    & $\rho^{-1} N_jp_j$ & Order \\ \toprule
		\multicolumn{5}{c}{No prior } \\\midrule
		Size (SMB)                      & 1824 & \textless{}0.00001 & \textless{}0.0001    & 1 \\
		Dollar Trading Volume (dtv\_12) & 2099 & \textless{}0.00001 & \textless{}0.0001    & 2 \\
		Value (HML)                     & 1191 & \textless{}0.00001 & 0.0280               & 3 \\
		Short-Term Reversal (srev)      & 1050 & 0.00001            & 0.0974               & 4 \\
		Forecast Revisions (rev\_1)     & 242  & 0.00018            & 0.2782               & 5 \\
		Investment (CMA)                & 998  & 0.00112            & \textgreater{}0.9999 & 6 \\
		Profitability (RMW)             & 797  & 0.00123            & \textgreater{}0.9999 & 7\\\midrule
		\multicolumn{5}{c}{FF3 prior ($\omega_{\text{FF3}}$) } \\\midrule
		Size (SMB)                      & 2802 & \textless{}0.00001 & \textless{}0.0001    & 1 \\
		Value (HML)                     & 2802 & \textless{}0.00001 & \textless{}0.0001    & 2 \\
		Dollar Trading Volume (dtv\_12) & 779  & \textless{}0.00001 & 0.0017               & 3 \\
		Short-Term Reversal (srev)      & 1106 & \textless{}0.00001 & 0.0049               & 4 \\
		Profitability (RMW)             & 819  & 0.00006            & 0.2527               & 5 \\
		Investment (CMA)                & 874  & 0.00087            & \textgreater{}0.9999 & 6\\ \midrule 
		\multicolumn{5}{c}{FF5 prior ($\omega_{\text{FF5}}$) }  \\\midrule
		Size (SMB)                      & 2911 & \textless{}0.00001 & \textless{}0.0001 & 1 \\
		Value (HML)                     & 2911 & \textless{}0.00001 & \textless{}0.0001 & 2 \\
		Forecast Revisions (rev\_1)     & 230  & \textless{}0.00001 & 0.0005            & 3 \\
		Short-Term Reversal (srev)      & 1140 & \textless{}0.00001 & 0.0052            & 4 \\
		Dollar Trading Volume (dtv\_12) & 661  & \textless{}0.00001 & 0.0072            & 5 \\
		Profitability (RMW)             & 2911 & 0.00001            & 0.1937            & 6 \\
		Investment (CMA)                & 2911 & 0.00001            & 0.1996            & 7 \\
		Gross profits-to-assets (gpa)   & 1151 & 0.00013            & 0.8382            & 8\\ 
		\bottomrule
	\end{tabular}
	\bnotetab{This table reports ranking of factors based on their FWER bound for no prior, and infinite weight priors on the Fama-French 3 and 5 factors. The test assets are the $N=243$ double-sorted portfolios and the candidate factors are $J=114$ univariate long-short factors. The rows are ordered based on sorted ascending $\rho^{-1} N_jp_j$, which corresponds to the FWER bound.}
\end{table}





The Panel PoSI models select economically meaningful factors. Table \ref{DSHL} reports the ranking of factors based on their FWER bound without prior and infinite prior weights on the Fama-French 3 and 5 factors. The rows are ordered based on sorted ascending $\rho^{-1} N_jp_j$, which corresponds to the FWER bound. It allows us to infer the number of factors for different levels of FWER control values. Setting $\gamma=5\%$ leads to 3, 4 and respectively 5 factors, while a $\gamma=1\%$ results in 2, 4 and 5 factors, respectively. 

In addition to their significance, we can infer the relative importance of factors. The baseline PoSI with $\gamma=5\%$ selects a size, dollar trading volume and value factor. The size and value factors are among the most widely used asset pricing factors. Their selection is in line with their economic importance and confirms the Fama-French 3 factor model. The dollar trading volume factor is less conventional, but is correlated with many assets in our cross-sections. The size factor is the most important as measured by the FWER bound, that is, the product of the number of relevant assets and its minimum p-value are the smallest. The short term reversal factor is less important and would require a FWER control of 10\% to be included.

Imposing a prior affects the p-values of PoSI and the simultaneity count. For example, the cohesiveness coefficient increases from $\rho=0.16$ for no priors to $\rho=0.18$ in the case of the two priors. Hence, the FWER bounds of all factors can change when we impose a prior. The FF3 prior increases the significance of the short-term reversal factor, which is widely used in asset pricing. Interestingly, even for a FF5 prior, the profitability and investment factors remain insignificant. 




\subsection{Number of Factors}

Our method contributes to the discussion about the number of asset pricing factors. Many popular asset pricing models suggest between three and six factors. Our approach allows a disciplined estimate for the number of factors based on inferential theory. The level of sparsity of a linear model also depends on the rotation of the covariates. Therefore, we also study the principal components (PCs) of the covariates $X$ as candidate factors. In this case, we use the step-down procedure, which we refer to as ``Ordered PoSI'' or O-POSI for short.

\begin{figure}[t!]
	\centering	\tcaptab{Number of selected factors for different FWER}	
	\label{fig:number}\centering
	\begin{subfigure}[t]{.48\textwidth}\centering
		\includegraphics[width=0.8\linewidth]{plots/"DS_HL_Rejection_count_by_alpha"}
		\caption{Univariate factors with priors (P-POSI)}
	\end{subfigure}
	\begin{subfigure}[t]{.48\textwidth}\centering
		\includegraphics[width=0.8\linewidth]{plots/"DS_PC_Rejection_count_by_alpha"}\\
		\caption{PCA rotated factors (O-POSI)}
	\end{subfigure}\\
	\bnotefig{This figure shows the number of selected factors to explain the test assets of double-sorted portfolios for different FWER rates $\gamma$. The factor count is obtained by traversing $K^*(\gamma)$ for $\gamma$ ranging from 0.01 to 0.1. The left subfigure uses univariate high-minus-low factors as candidate factors. We consider the case of no prior, and the cases of an infinite weight on the Fama-French 3 factor model ($\omega_{\text{FF3}}$) and an infinite weight on the Fama-French 5 factor model ($\omega_{\text{FF5}}$). The right subfigure uses the PCA rotation as candidate factors with the step-down procedure Ordered PoSI (O-POSI).}
\end{figure}


\begin{table}[t!]
	\tcaptab{Number of selected factors for different methods}
	\label{tab:number}
	\centering
	{\small
		%\resizebox{\textwidth}{!}{
			\renewcommand{\arraystretch}{1.2}
			%  \setlength{\tabcolsep}{3.2pt}
			\begin{tabular}{@{}llll@{}}
				\toprule
				& HL & PCs   & HL + PCs  \\ \midrule
				Panel PoSI             & 3   & 3    & 4 \\
				Bonferroni PoSI        & 2   & 3    & 2 \\
				Bonferroni Naive LASSO & 10  & 29   & 10 \\
				Naive LASSO            & 70  & 50  &76 \\
				Bonferroni OLS         & 107 & 13 & 117 \\
				Naive OLS              & 114 & 50  & 164\\                  
				\bottomrule
			\end{tabular}
		}
		\bnotetab{
			This figure shows the number of selected factors to explain the test assets of double-sorted portfolios for different methods and different sets of candidate factors. The rejection FWER is set to $\gamma=5\%$. The factor count is obtained by traversing $K^*(\gamma)$ for $\gamma$. The number of factors is selected on the in-sample data. For PCs, we use the step-down method for the nested hypothesis. 
		}
	\end{table}
	
	
	
	Figure \ref{fig:number} shows the number of factors for different FWER rates $\gamma$. The factor count is obtained by traversing $K^*(\gamma)$ equal to 0.01, 0.02, 0.05 and 0.1. Panel PoSI without priors selects 2 factors for $\gamma=0.01$ and 3 for $\gamma=0.05$. Once, we impose an infinite weight on the Fama-French 3 factors, we select 4 factors for all FWER levels, while the prior on the Fama-French 5 factors results in a 5 factor model for all FWER levels. The Ordered PoSI with PCA rotated factors selects 3 factors for all FWER levels. In summary, our results confirm that depending on the desired significance, the number of asset pricing factors for a good model seems to be between 2 and 4. Note that our analysis is orthogonal to the market factor, which would also be added to the final model. Thus, the final model would have between 3 and 5 factors.
	
	
	
	Table \ref{tab:number} further confirms our findings about the number of asset pricing factors. We compare the number of factors for $\gamma=5\%$ selected either from the univariate high-minus-low factors (HL), their PCA rotation or the combination of the high-minus-low factors and their PCs. Panel PoSI selects consistently 3 factors from the long-short factors and their PCs. When combined, PoSI selects 4 factors, which is plausible as the optimal sparse model can be different for this larger set of candidate factors. The Bonferroni PoSI is overly conservative and selects only 2 HL factors. The models based on Naive LASSO or OLS select excessively many factors independent of the rotation. Overall, the findings support that parsimonious asset pricing models can be described by three to four factors. Of course, any discussion about the number of asset pricing factors is always subject to the choice of test assets and candidate factors. 
	
	
	
	
	
	
	
	
	
	
	
	
	%
	%
	%\section{Empirical study}\label{sec5}
	%
	%We study the classical problem of factor modeling in empirical asset pricing dating back to \cite{FAMA19933}. By using a data set of $N=243$ excess returns and $d=121$ high-minus-low constructed factors from previously proposed market anomalies, we investigate what factors does our inferential procedures discover. Also, we designed a simulation to compare our method against a number of common practises to show that our proposed inferential procedure is the most reasonable choice in high-dimensional panels by controlling for false discovery and maintaining parsimony. We discuss how we collected and constructed the data set and application of the two-pass model in \S\ref{data_subsec}, the simulation study on comparisons against other methods in \S\ref{empiric:simulation}, the factor findings results in \S\ref{empiric:factor_discovery} and the investigation on the factor count problem in \S\ref{empiric:factor_count}.
	%
	%
	%\subsection{Data and Fama-MacBeth two-pass model}\label{data_subsec}
	%Our test asset returns come from \href{https://mba.tuck.dartmouth.edu/pages/faculty/ken.french/Data_Library/f-f_5_factors_2x3.html}{French's website}. There are two scenarios that we consider:
	%\begin{enumerate}
	%	\item Doubly Sorted portfolios (DS henceforth): these are 243 monthly value-weighted sorted portfolios, as presented in Table \ref{DSSource}. All were constructed using two characteristics.
	%	$$\textbf{[Place Table \ref{DSSource} here]}$$
	%	\item Size \& Value portfolios (SV henceforth): these are the 25 monthly value-weighted portfolios constructed using ME, BE characteristics. In other words, SV is strictly a subset of DS.
	%\end{enumerate}
	%Let $R^{(n)}$ be one of the returns, either of DS or SV and $r_f$ be the risk-free asset returns from \href{https://mba.tuck.dartmouth.edu/pages/faculty/ken.french/Data_Library/f-f_5_factors_2x3.html}{French's website}. Our actual response variable is the excess return:
	%\begin{equation}
	%	Y^{(n)}=R^{(n)}-r_{f}
	%\end{equation}
	%So our $N=25$ for SV test asset pool and $N=243$ for the DS test asset pool.\NL
	%Our factor sets come from the following two sources:
	%\begin{enumerate}
	%	\item Fama-French 5 factors (FF henceforth): these are the SMB, HML, RMW, CMA and Rm-Rf monthly factors from \href{https://mba.tuck.dartmouth.edu/pages/faculty/ken.french/Data_Library/f-f_5_factors_2x3.html}{French's website} as proposed in \cite{FAMA20151}. We also refer to the market excess return factors ,the SMB and HML factors as proposed in \cite{FAMA19933} as the Fama-French 3 factors (FF3 henceforth);
	%	\item \cite{HouEtAl} anomalies (HL henceforth): there are 187 sets of one-way decile portfolios provided from \href{http://global-q.org/testingportfolios.html}{Hou's website}. 
	%\end{enumerate}
	%We collect the monthly observations from the two sources above. Since our response variables SV and DS are observed from January 1967 to December 2020, in order to create a balanced panel, we had to rid of 69 of the 187 anomalies-associated returns due to their missing values. Moreover, the original \cite{HouEtAl} method duplicates 3 of the 5 FF factors but with decile single-sorted portfolios, so we removed them as well and include all of the 5 FF factors into HL. In other words, we end up with $187-69-3+5=121$ HL factors. Now we are ready to construct the 5 unweighted scenarios for our factor candidate set:
	%\begin{enumerate}
	%	\item FF: these are the 5 Fama-French 5 factors stated above. When we investigate this factor set in the following text, we do not put FF through the PoSI selection process but rather directly set $X_{PS}$ to be all 5 FF factors. This is to manifest FF as a control set of factors and examine how other factor sets can improve our capability in explaining excess returns;
	%	\item HL: we construct 121 high-minus-low factors from \cite{HouEtAl} anomalies. For each set of decile portfolios, we use subtract the returns of the lowest decile from the returns of the highest decile to reach a high-minus-low anomaly return.;
	%	\item Principle components of anomalies (HL PC henceforth): we perform PCA on HL, and normalize the principle components based on the standard deviation of HL factors within $T_{selection}$\footnote{The normalization is to make sure that the true coefficients for factors are comparable among factors. And we normalize using $T_{selection}$ period because the selection phase only uses and distinguishes coefficients of this period.}. We use the eigenvector rotation to gain top 50 out of the 121 PCs;
	%	\item HL + HL PC: we concatenate the HL factor set with the HL PC factor set to have $121+50=171$ factors;
	%	\item Principle components of excess returns (DS PC henceforth): we perform PCA on DS, and normalize the principle components based on the standard deviation of DS excess returns within $T_{selection}$. We use the eigenvector rotation to gain top 50 out of the 243 PCs;
	%\end{enumerate}
	%Let there be a weight list $\mathcal{W}=[2,4,100,+\infty]$ that represents researchers who are increasingly confident in the prior belief relative to other covariates. We investigate the following 8 weighted scenarios for our factor candidate set:
	%\begin{itemize}
	%    \item HL-$\omega_1\sim$ HL-$\omega_4$: prior-belief set chosen to be FF3, with increasing confidence in the FF3 prior belief. Recall the weights are of $\RR^d$ and specifically, in weighted scenario HL-$\omega_l$, we assign weight $\mathcal{W}_t$ for factors $\in $ FF3 and 1 otherwise, then normalize to $d=121$. In other words, we assign the weights by
	%    \begin{equation}\label{37}
		%    \omega_l=\eta^{-1}d\|\eta\|_1\q \wh\q 
		%        \eta_i=\oo\{i\in\text{FF3}\} \mathcal{W}_l +\oo\{i\notin\text{FF3}\}
		%    \end{equation}
	%    which guarantees the normaliziation $\|\omega_l^{-1}\|=d$ as required in Definition \ref{def0};
	%    \item HL-$\omega_5\sim$ HL-$\omega_8$: prior-belief set chosen to be FF, with increasing confidence in the 5 FF factors as prior belief. Specifically, in weighted scenario HL-$\omega_{l+4}$, we assign weights $\mathcal{W}_{l}$ to factors $\in $ FF and 1 otherwise, then normalize to $\|\omega\|_1=d=121$. In other words, in HL-$\omega_{l+4}$ of $l=1,2,3$, the weight vector $\omega$ can also be expressed with (\ref{37}) with the substituion of $\eta_i=\oo\{i\in\text{FF}\} \mathcal{W}_l +\oo\{i\notin\text{FF}\} $;
	%\end{itemize}
	%To summarize, we have the following 6 factor candidate pools as presented in Table \ref{candiFactors}.
	%$$\textbf{[Place Table \ref{candiFactors} here]}$$
	%Given a factor set $\bm {X}$ and test asset excess return set $\bm{Y}$, we use our inferential results established in Theorem\ref{thm_MT} to make inference. Specifically, we followed the steps laid out in PROCEDURE 1. Once we run PROCEDURE 1, we would have selected a low-dimensional set of factors. To move forward, we describe the approach we fit the classical low-dimension consistent model. Given a set of factors that were selected using the first $|T_{selection}|$ observations, we can run Fama-Macbeth two-pass regression to acquire estimated model. We use $X_{PS}\in\RR^{T\times |M|}$ (subscript ``PS'': post-selection) to denote the observed selected factors and $X_{PS_t}\in\RR^{|M|}$ to denote the observation of selected factors at time $t$. In particular, we carry out the following procedures:
	%\begin{enumerate}
	%	\item First pass: estimate the factor loading $\bm{\beta}\in\RR^{N\times |M|}$ using time-series OLS on the selection period's data. This is assuming that the loadings hold constant throughout $T=T_{Selection}\cup T_{Exclusion}$ and time-varying risk premia will reflect the temporal changes in excess returns. For each asset $j\in[N]$, there is a loading vector $\beta^{(j)}\in\RR^{|M|}$, and we estimate it via
	%	\begin{equation}\label{19}
		%		\hat{\beta}_{OLS}^{(j)}:=\argmin_\beta \sum_{t\in T_{Selection}}(Y^{(j)}_t-(X_{PS_t})^\top \beta )^2
		%	\end{equation}
	%Collect all $N$ of $\hat{\beta}_{OLS}^{(j)}$ vectors and we would have the loading matrix $\hat{\bm{\beta}}_{OLS}$ estimate;
	%	\item Second pass: estimate the time-varying risk premia $\Lambda\in\RR^{T\times |M|}$ using cross-sectional OLS. For each $t\in [T]=T_{selection}\cup T_{exclusion}$, there is a risk premia vector $\Lambda^{(t)}\in\RR^{|M|}$ which can be estimated by:
	%	\begin{equation}\label{18}
		%		\hat{\lambda}_{OLS}^{(t)}:=\argmin_\lambda \sum_{j\in [N]}(Y^{(j)}_t-\lambda^\top \hat{\beta}_{OLS}^{(j)} )^2
		%	\end{equation}
	%	Collecting all $\hat{\Lambda}_{OLS}^{(t)}$ and we can estimate the out-of-sample $\hat{Y}^{(j)}_t=\hat{\Lambda}_{OLS}^{(t)\top}\hat{\beta}_{OLS}^{(j)}$;
	%\end{enumerate}
	%Using (\ref{18}) and (\ref{19}), we can estimate $\hat{\alpha}$ of $T_a$ by:
	%\begin{equation}
	%	\hat{\alpha}_{T_a}=	\EE_{T_a}[\bm{Y}-\hat{\bm{Y}}]
	%\end{equation}
	%
	%
	%\subsection{Factor discovery problem}
	%Let us begin by considering no prior belief set. Firstly, we use a simulation to show why the naive OLS selection or $t$-statistics directly derived from LASSO estimator would not be a good idea. Then we dig deep into the factor discoveries made by Panel-PoSI procedures.
	%
	%\subsubsection{Benchmark studies: false discoveries on simulated factors and goodness-of-fit comparisons}\label{empiric:simulation}
	%In PROCEDURE 3 described below, we designed a simulation study. In Step 1, we first creates an artificial covariate set $\bm{X}_{sim}$ whose coefficients are known to be zero, and later we will run variants of regressions of $\bm{X}_{sim}$ on $\bm{Y}$. This simulation design of $\bm{X}_{sim}$  to create true null covariates is very similar in style to a knock-off set proposed by \cite{15-AOS1337}. To achieve meaningful contrasts, throughout the cases below we fix type I error control level of $\gamma=0.05$, the response set being the DS 243 excess returns, the factor set being the HL 121 factors and on time period of $T_{selection}$ the same as stated before, i.e. first half of our observation period (Jan, 1967 to Dec, 1993).\\
	%\noindent{\rule{\linewidth}{.4pt}}
	%\textbf{PROCEDURE 3 Simulation and Benchmark Naive Inference}
	%\begin{enumerate}
	%\item[Step 1.] Given $\bm{X}_{T_{selection}}$, sample $\bm{\tilde{X}}_{T_{selection}}$ from $\mathcal{N}(\EE_{T_{selection}}[\bm{X}],cov_{T_{selection}}[\bm{X}])$. Combine the two into  $\bm{X}_{sim,T_{selection}}=[\bm{X}_{T_{selection}},\bm{\tilde{X}}_{T_{selection}}]\in\RR^{|T_{selection}|\times 2d}$;
	%\item[Step 2.] For $j\in[N]$, and $i\in[2d]$, regress $\bm{X}_{sim,T_{selection}(i)}$ on $Y^{(j)}_{T_{selection}}$ to get univariate OLS fit $\tilde{\beta}_i^{(j)}$ and calculate 
	%$$\tilde{\sigma}_i^{(j)2}=\frac{\|Y^{(j)}_{T_{selection}}-\bm{X}_{sim,T_{selection}(i)}\tilde{\beta_i}^{(j)}\|_2^2}{|T_{selection}|-1}\frac{1}{\|\bm{X}_{sim,T_{selection}(i)}\|_2^2}$$
	%Then the naive-OLS $\tilde{p}_i^{(j)}$ associated with $\tilde{\beta}_i^{(j)}$ OLS fit is
	%\begin{equation}\label{naive-ols-t-method}
	%    \tilde{p}_i^{(j)}=F_{T_{|T_{selection}|-1}}(
	%    -|\frac{\tilde{\beta_i}^{(j)}}{\tilde{\sigma}_i^{(j)}}|)+F^C_{T_{|T_{selection}|-1}}(|\frac{\tilde{\beta_i}^{(j)}}{\tilde{\sigma}_i^{(j)}}|)
	%\end{equation}
	%\item[Step 3.] For $j\in[N]$, and $i\in M_j$, use the LASSO fit $\hat{\beta}^{(j)}_i$ from (\ref{4}) and $\hat{\sigma}_i^{(j)}$ from (\ref{eq21:sd.est}) to calculate a naive-LASSO $\acute{p}_i^{(j)}$ by
	%\begin{equation}
	%    \acute{p}_i^{(j)}=F_{T_{|T_{selection}|-|M_j|}}(-|\frac{\hat{\beta}^{(j)}_i}{\hat{\sigma}_i^{(j)}}|)+F^C_{T_{|T_{selection}|-|M_j|}}(|\frac{\hat{\beta}^{(j)}_i}{\hat{\sigma}_i^{(j)}}|)
	%\end{equation}
	%\item [Step 4.] Naive-OLS rejection rule: select $i$th factor if $\exists j:\tilde{p}_i^{(j)}<\gamma$;
	%\item [Step 5.] Bonferroni-OLS rejection rule: select $i$th factor if $\exists j:\tilde{p}_i^{(j)}<\frac{\gamma}{dN}$;
	%\item [Step 6.] Naive-NaiveLASSO rejection rule: select $i$th factor if $\exists j:\acute{p}_i^{(j)}<\gamma$;
	%\item [Step 7.] Bonferroni-NaiveLASSO rejection rule: select $i$th factor if $\exists j:\acute{p}_i^{(j)}<\frac{\gamma}{dN}$;
	%\item [Step 8.] DataDriven-NaiveLASSO rejection rule: select $i$th factor if $\exists j:\acute{p}_i^{(j)}<\frac{\gamma}{N_i}$;
	%\end{enumerate}
	%\noindent{\rule{\linewidth}{.4pt}}
	%Let us dig deeper into the candidate methods. Other than $p$-values computed from naive OLS $t$-statistics in Step 2, the next slightly more advanced method is to compute the $p$-values by pretending the LASSO output were OLS output and inverting a $t$-statistic, which is described in Step 3 of PROCEDURE 3. In other words, even though LASSO regressions were used, neither PoSI calculation of $p$-values nor any form of simultaneity count were used. Based on this, we also consider alternative methods where the researchers are increasingly aware of the multiple testing implications of having a large amount of factors across a large cross-section: the baseline case where no adjustment were made in ``Naive-NaiveLASSO'', a global Bonferroni adjustment in ``Bonferroni-NaiveLASSO'' and a data-driven adjustment that uses our proposed data-driven simultaneity count $N_i$ of \ref{eq28:simcount} in ``DataDriven-NaiveLASSO". These are described in Steps 6, 7 and 8 of PROCEDURE 3. In parallel, we also run our Panel-PoSI procedure with FWER control goal set as $\gamma$ on pseudo-data generated from Step 1 of PROCEDURE 3.\NL
	%In Figure \ref{fig:olspvalboxplots}, we investigate whether there is any pattern in the $p$-values across the cross-section of the simulated factors. We do observe that the OLS naive $p$-values stretch out much wider than the LASSO alternatives, showing the necessicity of using dimension reduction. Moreover, by comparison it is clear the naive LASSO $\acute{p}$'s are not uniformly distributed. In Table \ref{tab3}, we report the amount of factors rejected by the naive-OLS rejection rule, Bonferroni-OLS rejection rule, the 3 NaiveLASSO rejection rules and our Panel-PoSI inference procedure:
	%$$\textbf{[Place Table \ref{tab3} here]}$$
	%The naive-OLS rejection rules clearly fall into the trap of ``$p$-hacking" because it picked up many simulated factors, whose statistical significance is just a by-product of us observing $121\times 243=29,403$ $\tilde{p}_i^{(j)}$'s that should follow $\text{Unif}[0,1]$ where $d+1\leq i\leq 2d$.
	%$$\textbf{[Place Figure \ref{fig:olspvalboxplots} here]}$$
	%When we use the naive LASSO $\acute{p}$'s, it is obvious from Table \ref{tab3} that it yields different results than our PoSI $p$-values. Pictorially showed in Figure \ref{fig:olspvalboxplots}'s sub-figure (a), these values calculated as if normalized LASSO fit are simple $t$-statistics clearly not do not follow uniform distributions under the null and thus, they are not valid $p$-values. In contrast, our PoSI adjusted boxplots in sub-figure (b) are the unbiased and thus valid $p$-values.\NL
	%Without multiple testing correction, we see that the Naive-NaiveLASSO admist too many factors, with one of them known to be incorrect. Both Bonferroni-NaiveLASSO and DataDriven-NaiveLASSO do not admit simulated factors, but the data-driven multiple testing correction admits 6 more factors than its global Bonferroni counterparts, which is to be expected because the global simultaneity count is harsher. However, compared to Panel-PoSI, both of them admit non-parsimonious models.\NL
	%But exactly how would picking a parsimonious model help us? One common motivation for using a smaller set of selected factor is to prevent overfitting. We first use two benchmark methods from PROCEDURE 3 and our Panel-PoSI procedure to select factors from the normal rather than augmented factor sets, and summarize the selected factor count at level $\gamma=5\%$ in Table \ref{tab:comparison-factor-count}. We can clearly see that the same pattern using simulated factors still hold, which is that our Panel-PoSI method consistently selects the most parsimonious set of explanatory variables.
	%$$\textbf{[Place Table \ref{tab:comparison-factor-count} here]}$$
	%Then we conduct a goodness-of-fit investigation to see whether our model is on par with the much larger alternatives, the results of which are reported in Table \ref{tab:comparison-against-naive}.
	%$$\textbf{[Place Table \ref{tab:comparison-against-naive} here]}$$
	%In Panels (I) and (II) we examine the averaged unexplained means and variances, calculated from $\hat{\alpha}$ and averaged across the cross-section, in which we do see that even though the performances our selected factors are comparable to those selected by the Benchmark method, but due to the higher factor count, our method was the only one to be able to prevent a significant drop of performance in the held-out set. In Panel (III) which shows the negative log-likelihood (NLL henceforth) calculated from independent Gaussian law on $\hat{\alpha}$'s, we can see that our Panel-PoSI selected variables that in-sample seem to do relatively worse (recall that lower NLL is better), but become on par with models that have 11 times more variables out-of-sample in terms of NLL. In conclusion, our method indeed produce a better model than the benchmarked inferential procedures that we have considered, especially in producing a lean model that has comparable performance to larger model whilst avoids overfitting. This can also be examined in more details in Figure \ref{fig:heatmap.of.alpha}, in which we clearly see that the naive methods overfit on the BEME-OP and BEME-INVE doubly sorted portfolios, as the models selected by Panel-PoSI have significanlty worse in-sample performance but do better out-of-sample.
	%$$\textbf{[Place Table \ref{fig:heatmap.of.alpha} here]}$$
	%\subsubsection{Discoveries by Panel-PoSI}\label{empiric:factor_discovery}
	%Now we are ready to present the detailed empirical findings when applying Panel-PoSI to our data described in Section \ref{data_subsec}. In Table \ref{DSHL}, we report a subset of the factors based on $N_ip_i$. If we set our FWER control level at $\gamma=0.05$, we would reject 5 factors. The definitions of the non-Fama selected factors are listed in the end notes. We observe that the non-FF selected factors are mostly friction factors. Moreover, the non-FF factors share lower $N_i$'s, which is an indication that they show up in many fewer time-series LASSO regressions than their FF counterparts. This is an indication that we have found evidence of existence and succeeded in picking up ``weak'' factors in that they might not be useful in explaining every excess return in the cross-section, but helpful in a subset of them.
	%$$\textbf{[Place Tables \ref{DSHL}, \ref{tab:DSom4} and \ref{tab:DSom8} here]}$$
	%If we consider the two weighting scheme HL $\omega_4$ and HL $\omega_8$ where hard prior belief was put on FF3 and FF respectively, we observe the selected factors in Table \ref{tab:DSom4} and \ref{tab:DSom8}. There are many interesting observations one could make by cross-examining Tables \ref{DSHL}, \ref{tab:DSom4} and \ref{tab:DSom8}:
	%\begin{itemize}
	%\item When setting $i$th coordinate of $\omega_l$ to $\infty$, the occurrence of $i$th factor in LASSO is guaranteed as we see the $N_i$'s in Tables \ref{tab:DSom4} and \ref{tab:DSom8} are the largest among all factors, but they might still be not selected by Panel-PoSI procedure if there is a weak factor with smaller $N_i$ (fewer occurrences in LASSOs) but smaller $N_ip_i$ (stronger multiple testing signals);
	%\item From all three occurrences, we see the selected factor sets contain srev and dtv\_12 if we set $\gamma=0.05$, suggesting that indeed these two friction factors are insightful in explaining excess returns on top of FF factors;
	%\item When increasingly strong priors are expressed, the weak factors such as srev do not necessarily become less important as represented by their $\gamma^*(K)$. For instance, in \ref{tab:DSom8}, we still see srev being the 4th factor to reject if we want to admit a 4-factor model even though $+\infty$ weights have been put on all 5 of FF factors in time-series weighted-LASSO. This is indication that srev is still significant controlling for FWER and FF factors in explaining at least some parts of the cross-section;
	%\item The ordering $N_ip_i$ adjust for the fact that some factors might be active frequently but their $p$-values might suggest they are not very helpful in explaining the response variables. In other words, we do not put our full faith in LASSO's screening capability, but rather we use it as a rough dimensional reduction tool. This occurred in Table \ref{tab:DSom8} where RMW and CMA both have large $N_i$'s but they are rejected in a later position than srev, dtv\_12 and rev\_1 based on $N_ip_i$ ranking;
	%    \item Throughout the tables, we see that our procedure succeeds in selecting a very parsimonious factor set. \end{itemize}
%Also, we note there is minuscule insight to gain from making comparisons between HL-$\omega_2$, HL-$\omega_3$ vs HL-$\omega_1$, HL-$\omega_4$ and between HL-$\omega_6$, HL-$\omega_7$ vs HL-$\omega_5$, HL-$\omega_8$ because the factors that were selected actually are the same to their neighbouring weighting regimes. Thus, in the following text, we focus our attention on examinations of weighting regimes HL-$\omega_l$ with $l\in\{1,4,5,8\}$.\NL
%How well does our discovered factors explain the returns? We now turn to consider the goodness-of-fit statistics, which investigates whether the following goodness-of-fit hypothesis $H_0'$ is true:
%\begin{equation}\label{21}
%	H_0':\bigcap_{j\in[N]}\{\EE_T[\alpha]=0\}
%\end{equation}
%The simplest way one can think of investigating (\ref{21}) would be to calculate the $z$-score of $\hat{\alpha}_{T_a}$'s by flattening into a single vector of $\RR^{|T_a|\cdot N}$ and $p(z)$:
%\begin{equation}\label{46}
%    z(\hat{\alpha}_{T_a})=avg(\hat{\alpha}_{T_a})/\hat{\sigma}(\hat{\alpha}_{T_a}),
%    \q
%    p(z)=2\Phi(-|z|)
%\end{equation}
%We perform entire Panel-PoSI to select a linear model to explain DS, and then run the Fama-Macbeth regressions to collect residuals, which gives us the following Table \ref{zscoreDS}.
%$$\textbf{[Place Table \ref{zscoreDS} here]}$$
%Since most $p(z)$'s are near 1, it is clearly an indication that $z$-scores are poor at measuring and comparing our different models. Most of the models have in-sample and out-of-sample near-zero $z$-scores when the $\hat{\alpha}$'s are flattened into a vector, motivating us to consider better way to capture cross-sectional performances.\NL
%To measure cross-sectional performances in a panel model, \cite{PelgerChen} proposed a finite-sample estimator for joint deviation as a $F$-statistic
%\begin{equation}\label{22}
%\small
%	\frac{|T_a|-N-K}{s}\EE_{T_a}[\hat{\alpha}]^\top \hat{\Sigma}^{-1}\EE_{T_a}[\hat{\alpha}]\sim F_{N,|T_a|-N-K},\quad
%	s=\frac{|T_a|}{|T_a|-\EE_{T_a}[X_{PS}]^\top \EE_{T_a}[(X_{PS})^\top X_{PS}]^{-1}\EE_{T_a}[X_{PS}]}
%\end{equation}
%where $\EE_{T_a}[X_{PS}]\in\RR^{K}$ is the $T_a$-period sample average of the factors, $\hat{\Omega}$ the sample covariance of the factors, $\hat{\Sigma}$ the sample covariance of $\hat{\alpha}$ and $s$ the Shanken correction term. Empirically, we observe the $F$ statistics in Table \ref{F-statDS} for DS.
%$$\textbf{[Place Table \ref{F-statDS} here]}$$
%In the DS Test asset set, clearly, we achieved joint $F$-test results using HL PC that much better than FF 5 factors, and HL-$\omega_8$ performs much better than both HL and FF. Moreover, we can compare the second and last rows to note that by setting HL-$\omega_8$, we achieve the model that finds the 3 more factors given $+\infty$-weights on the 5 FF and these 3 new factors do indeed help with explaining the panel of excess returns, both in-sample and out-of-sample.\NL
%In addition, we can examine the root-mean-squared $\hat{\alpha}$'s and unexplained variances:
%\begin{equation}\label{49}
%\small
%	\begin{split}
	%		\text{XS}_{T_a}:=\sqrt{\frac{1}{K}\sum_{j\in[N]}\EE_{T_a}[\hat{\alpha}^{(j)}]^2}\q
	%		\text{UV}_{T_a}:=
	%		\sqrt{\frac{1}{K}\bm{1}^\top [diag(Cov_{T_a}(\hat{\alpha}))./diag(Cov_{T_a}(Y))]}
	%	\end{split}
%\end{equation}
%Presented in Table \ref{XSUV}, we see that for DS, indeed HL PC outperforms FF by a little bit. At the same time, we note out of the 171 HL + HL PC factors, $K=6$ were selected which was no greater than either HL or HL PC. This suggests that simply throwing more factors into a high-dimensional panel is not a good idea. Indeed, on the performance metrics, we note HL + HL PC actually has  deteriorated performance compared to HL PC alone by the $\frac{XS}{\sqrt{UV}}$ out-of-sample measurements. This coincides with the intuition that when useful signals are duplicated to increase the $d$-dimensionality, in order to control for FWER, our model had to select fewer useful signals.
%$$\textbf{[Place Table \ref{XSUV} here]}$$
%Another direct way of examining cross-sectional performances is to average over time by assets and see how much our average fitted returns can explain the empirical average returns. To do so, we consider a ``mini-regression'' that uses time-averaged fitted return to predict time-averaged actual returns:
%\begin{equation}\label{eq:mini_reg}
%\texttt{lm}( \underbrace{\EE_{T_a}[Y]}_{\RR^{N}}\sim\underbrace{\EE_{T_a}[\hat{Y}]}_{\RR^{N}}),\quad
%T_a\in\{T_{selection},T_{exclusion} \}
%\end{equation}
%And in Table \ref{r2-statDS} we examine the $R^2$, log-likelihood $LL$, and RMSE of different candidate sets after running the mini-regressions, and in Figure \ref{fig:ds-perf-by-w} we present the them in side-by-side bar plots. We can also visually compare these performances in Figure \ref{fig:ds-perf-by-w}'s side-by-side bar plots, and by the scatter-plots in Figure \ref{fig:DS_scatterplots}.
%$$\textbf{[Place Table \ref{r2-statDS} and Figures \ref{fig:ds-perf-by-w}, \ref{fig:DS_scatterplots} here]}$$
%As depicted in Figure \ref{fig:ds-perf-by-w}, HL PC evidently performs the best with its out-of-sample RMSE bar the lowest in panel (a) and its $LL$ bar the highest in panel (b). Moreover, the weighted regimes indeed make the comparison against FF more reasonable since their performances are better than that of unweighted HL factors. In addition, by looking at panel (b) we find that using the factor set HL + HL PC run the risk of over-fitting, for its in-sample $LL$ metric is the highest whilst deteriorates out-of-sample. From Figure \ref{fig:DS_scatterplots}(a), it appears that HL PC does better in the DS in-sample and out-of-sample cases. In Figure \ref{fig:DS_scatterplots}(b) that corresponds to SV, we know for fact that FF contains the sorting characteristics that were used to create the 25 two-way quitile portfolios, but our selected HL PC factors still perform better. 
%
%\subsection{Factor count problem}\label{empiric:factor_count}
%How many factors would we select given a $\gamma$ level? For instance, by examining the Table \ref{tab:DSom8} we can see that if we believe that there are $K=5$ factors among HL factors that are actually useful in explaining DS while admitting weighting regime of HL-$\omega_8$, we can allow for $\gamma$ up to 0.01, and the five factors up to rev\_1 would be included the selected model while excluding the factors dtv\_12, CMA, RMW etc. On the flip side, if one were to accommodate FWER of up to 0.05, there would be 8 factors included in the selected model in weighted HL-$\omega_8$ setting.\NL
%Let us further explore this by cross-examining different weighted-LASSO scenarios. The following plots explore how different $\gamma$ levels would lead to different amount of rejected factors when the response variables are DS and SV respectively and the weights are the hard prior beliefs (i.e. HL-$\omega_4$ and HL-$\omega_8$):
%
%$$\textbf{[Place Figure \ref{fig:rejection-hldscount-by-alpha} here]}$$
%
%In Figure \ref{fig:rejection-hldscount-by-alpha}, we contrast the traversing results of three factor scenarios: HL, HL-$\omega_4$ and HL-$\omega_8$ when regressing on DS asset returns. Since the prior beliefs are pushing the FF3 or FF to be always active, we expect the purple bars corresponding to HL-$\omega_8$ and the orange bars corresponding to HL-$\omega_4$ to be always taller than the navy blue bars corresponding to unweighted regime HL. And indeed, this pattern is what we observe in Figure \ref{fig:rejection-hldscount-by-alpha}. 
%
%In Figure \ref{fig:rejection-hldscount-by-alpha}, when regressing on SV asset returns, we know that the FF3 factors can fully explain SV due to its construction mechanism, so we should expect a good inference to not select much more than the 3 true factors even if we are in the HL-$\omega_8$ regime. This is confirmed by  Figure \ref{fig:rejection-hldscount-by-alpha} where the purple bars corresponding to HL-$\omega_8$ actually stably select 3 factors across different $\gamma$'s, with the only exception of at $\gamma$ set to 0.1, the Panel-PoSI procedure admits the dtv\_12 factor rather than either of the two factors, namely RMW or CMA, that are part of the 5 FF factors. Finally, we examine the PC count problem by creating the following figure:
%
%$$\textbf{[Place Figure \ref{fig:rejection-count-by-alpha} here]}$$
%
%It does confirm our oracle intuition that DS PC are much better candidates than HL PC, not only because they are estimated from $\bm{Y}$ but also that our inferential framework selected more PCs across $\gamma$'s as depicted in Figure \ref{fig:rejection-count-by-alpha}.


%\section{Discussion} \label{sec6}
%We propose a conditional inference framework that proposes hypothesis on the basis of LASSO findings and then make valid statement with FWER control of $\gamma$ for a large amount of cross-sectional units and high-dimensional covariates. Our method allows researchers to specify weights for a prior belief set of covariates and provides interpretation on the weights' impact on inference. We also provide the traversing procedure that can draw the curve of least amount of factors as the $\gamma$ changes.\NL
%Nonetheless, there are many opening questions that we encountered when devising our framework and we plan on pursuing. First of all,  the asymptotic statistical power of our procedure still remains to be formally discussed, and there might be potential modifications to improve the power. One potential avenue is to make more assumptions on the structure of $\EE[\epsilon\epsilon^\top]$ so that we use many recent results such as \cite{rssb.12265} that control for FDR instead and gain better power. In addition, we think our results could contribute to inference on two-way fixed effects model in the causal inference literature because when $N$ and $d$ are similarly large, researchers can run LASSO either way to yield valid conditional $p$-values for the time-fixed effect or the unit-fixed effect. Our procedure stresses the possibility of weak factors, which will be helpful in identifying heterogeneous treatment effects by testing for $\bar{\beta}^{(1)}_i=\cdots=\bar{\beta}^{(N)}_i$ if $i$th covariate were picked up by our inferential method. Thirdly, we can use the distributional results to make inference on the residuals, which should be a linear combination of truncated Gaussians. Following this train of thought, there would be room for more empirical investigation into the existence of $\alpha$'s or out-performing fund mangers. Beyond making inference, we think that result might be useful for Thompson sampling method in the high-dimensional contextual bandit problem by treating rewards and contextual covariates as our $(\bm{Y},\bm{X})$ panel and updating posteriors of arms fixed effects using our distributional results.


\section{Conclusion} \label{sec6}

%We propose a conditional inference framework that proposes hypothesis on the basis of sparsity constrained models and then make valid statement with FWER control of $\gamma$ for a large amount of cross-sectional units and high-dimensional covariates. Our method allows researchers to specify weights for a prior belief set of covariates and provides interpretation on the weights' impact on inference. We also provide the traversing procedure that can draw the curve of least amount of factors as the $\gamma$ changes.\NL
%Nonetheless, there are many opening questions that we encountered when devising our framework and we plan on pursuing. The asymptotic statistical power of our procedure still remains to be formally discussed, and there might be potential modifications to improve the power. Another potential avenue is to use the distributional results to make inference on the residuals of sparse models, which should be a linear combination of truncated Gaussians. Following this train of thought, in the context of asset pricing, there would be room for more empirical investigation into the testing for $\alpha$'s or out-performing fund mangers. 


This paper proposes a new method for covariate selection in large dimensional panels. We develop the conditional inferential theory for large dimensional panel data with many covariates by combining post-selection inference with a new multiple testing method specifically designed for panel data. Our novel data-driven hypotheses are conditional on sparse covariate selections and valid for any regularized estimator. Based on our panel localization procedure, we control for family-wise error rates for the covariate discovery and can test unordered and nested families of hypotheses for large cross-sections. We provide a method that allows us to traverse the inferential results and determine the least number of covariates that have to be included given a user-specified FWER level. 

As an easy-to-use and practically relevant procedure, we propose Panel-PoSI, which combines the data-driven adjustment for panel multiple testing with valid post-selection p-values of a generalized LASSO, that allows to incorporate weights for priors. In an empirical study, we select a small number of asset pricing factors that explain a large cross-section of investment strategies. Our method dominates the benchmarks out-of-sample due to its better control of false rejections and detections.   



%\newpage

\appendix


\renewcommand{\thesubsection}{\Alph{section}.\arabic{subsection}}
\setcounter{table}{0}
\setcounter{figure}{0}
\renewcommand{\thetable}{A.\arabic{table}}
\renewcommand{\thefigure}{A.\arabic{figure}}


\newtheorem{thm}{Theorem}[section]
\newtheorem{lem}{Theorem}[section]
\newtheorem{defin}{Theorem}[section]
\newtheorem{ass}{Theorem}[section]


\newtheorem{theorem_app}[thm]{Theorem}
\newtheorem{lemma_app}[lem]{Lemma}
\newtheorem{assumption_app}[ass]{Assumption}
\newtheorem{def_app}[defin]{Definition}



\section{Post-selection Inference with Weighted-LASSO}\label{lab:appendix}

\subsection{Weighted-LASSO: Linear Truncation Results}\label{lab:appendixA1}

This appendix collects the assumptions and formal statements underlying Theorem \ref{thm1_main}. We present the results for the Weighted-LASSO, which includes the conventional LASSO as a special case. In order to ensure uniqueness of the LASSO solution, we impose the following condition, which is standard in the LASSO literature:

\begin{def_app}{\bf General position}\label{def1}\\
	The matrix $\bm{X} \in\RR^{T\times J}$ has columns in general position if the affine span of any $J_0+1$ points $(\sigma_1X_{i_1},...,\sigma_{J_0+1}X_{i_{J_0+1}})$ in $\RR^T$ for arbitrary $\sigma_1,...\sigma_{d_0+1}\in\{\pm1\}$ does not contain any element of $\{\pm X_i:i\notin\{ i_1,...,i_{J_0+1}\} \}$, where $J_0<J$%\footnote{The original condition needs to hold for $J_0<\min\{T,J\}$ but in the scope of our study, we consider $T>J$.} 
    and $X_i$ denotes $i$th column of $\bm{X}$.
\end{def_app}



This position notion will help us to avoid ambiguity in the LASSO solution. Note that this condition is a much weaker requirement than the full-rank of $\bm{X}$, and states that if one constructs a $J_0$-dimensional subspace, it must contain at most $J_0+1$ entries of $\{\pm X_1,...,\pm X_J \}$. Even though this appears to be a complicated and mechanical condition, by a union argument it turns out that with probability 1, if the entries of $\bm{X}\in\RR^{T\times J}$ are drawn from a continuous probability distribution on $\RR^{T \times J}$ then $\bm{X}$ is in general position.\footnote{See \cite{cpa.20132} and \S2.2 of \cite{10.1214/13-EJS815} for more discussions on uniqueness and general position.} Then, we will be able to discuss the LASSO solution for general design with relative ease, thanks to Lemma 3 of \cite{10.1214/13-EJS815}. It shows that if $\bm{X}$ lie in general position,  it is sufficient to have a unique LASSO solution regardless of the penalty scalar $\lambda$. This condition will later be used in establishing our Lemma \ref{lem2}.

We can now state the formal assumptions:
\begin{assumption_app}\label{asu1} {\bf Unique low dimensional model}
	\vspace{-0.1cm}
	\begin{enumerate}[label=(\alph*)]
		\item {\bf Low dimensional truth}:\\
		The data satisfies $Y=X_S\beta_S+\epsilon$ where $|S|=O(1)$.
		\item {\bf General position design}:\\
		The covariates $\bm{X}$ have columns in general position as given by Definition \ref{def1}.
	\item {\bf Sufficient observation}:\\
The covariate dimension $J$ is smaller than the number of time-series observations $T$.
		
	\end{enumerate}
\end{assumption_app}

We start our analysis with the simpler model of known error variance, and later extend it to the case of estimated unknown variance.

\begin{assumption_app}\label{asu_known} {\bf Gaussian residual with known variance}\\
	The residuals are distributed as $\epsilon\sim \mathcal{N}(0,\Sigma)$ where $\Sigma$ is known.
\end{assumption_app}


%We refer to a setting where ASSUMPTIONs \ref{asu1}, \ref{asu5} and \ref{asu6} are met as \textbf{General Design, Known Variance} (GDKV henceforth).\NL

Before formalizing the inferential theory, we need to clarify the quantity for which we want to make inference statements. As stated before, we only test the hypothesis on a covariate if its LASSO estimate turns out active. This is exactly the approach how researchers in practice conduct explorations in high-dimensional datasets. In other words, we focus on $\hat{\beta}_M$ and quantities associated with it, where $M$ denotes the active set of selected covariates. 

We study the inferential theory of the ``debiased estimator", which is a shifted version of the LASSO fit as defined below. We show that this debiased estimator is unbiased, consistent and follows a truncated Gaussian distribution, with profound connections to the debiased LASSO literature such as \cite{10.1214/17-AOS1630}, but has different properties by a subtle different descent direction. More concretely, given $\mathcal{M}$, clearly $\hat{Y}=X_M\hat{\beta}_M$ is the fitted value since $\hat{\beta}_{-M}=0$, where $-M$ is the complement of the set $M$. We let $\hat{\epsilon}_M:=Y-X_M\hat{\beta}_M$ be the residual from the LASSO estimator. By considering only the partial LASSO loss of $\ell(Y,X_M,\lambda,\beta)$ and given we are currently at the LASSO estimator $\hat{\beta}$, the Newton step is $X_M^+\hat{\epsilon}$ following \cite[\S~9.5.2]{boyd2004convex}, where we denote $X_M^+=(X_M^\top X_M)^{-1}X_M^\top $ as the pseudo-inverse of the active submatrix of $X$. The invertibility of $X_M^\top X_M$ either is observed when we are in the fixed design regime or happens almost surely when we are dealing with continuous quantities, as a consequence of Assumption \ref{asu1}(b) as argued in \cite{10.1214/13-EJS815} and \cite{lee2016exact}. Now we can formally define the main object of our inferential theories:


\begin{def_app}{\bf Debiased Estimator}\label{def2}\\
	The debiased Weighted-LASSO estimator $\bar{\beta}_M$ given $\mathcal{M}$ is given by
	\begin{equation}\label{11}
		\bar{\beta}_{M}=\hat{\beta}_{M}+X_M^+\hat{\epsilon}_M
	\end{equation}
\end{def_app}


It is now evident why some of the literature refers to the debiased estimator also as the one-step estimator: given that $\hat{\beta}_M$ solves the Karush-Kuhn-Tucker (KKT) condition and reaches the optimal sub-gradient for the full loss $\ell(Y,X,c,\beta)$, our debiased estimator $\bar{\beta}_M$ is the result of moving {one} more Newton-Ralphson method {step} after $\hat{\beta}_M$, but only taking $X_M$ rather than $X$ as a whole into the likelihood loss function. Hence, the update step is actually only a partial update from the LASSO solution point. Intuitively, $\bar{\beta}_M$ should still be close to solving the KKT conditions, and would exactly solve the KKT conditions if $X_M$ happen to be the true covariates (i.e. $X_M=X_S$). 

If we were to take a Newton's method step with gradient and Hessian calculated with the entirety of data $X$, or equivalently taking a full update from the stationary point, we will recover the $\hat{\beta}_{M}^d$ proposed in \cite{10.1214/17-AOS1630}. The material difference is that the full-update would require the $J\times J$ precision matrix $\Omega=\Gamma^{-1}$, where $\Gamma=X^\top X$ if $X$ assumed fixed or $\Gamma=\EE[X^\top X]$ if $X$ assumed to be generated from a stationary process. Using $\ell(Y,X_M,\lambda,\beta)$ instead of $\ell(Y,X,\lambda,\beta)$, our debiased estimator would not need the full Hessian, which is leveraging LASSO's screening property and uses $(X_M^\top X_M)^{-1}X_M^\top $ (i.e. $X_M^+$) as a much lower-dimensional alternative of $\Omega X^\top $.


Without loss of generality, we assume that the covariate indexed $i\leq |M|$ is part of $M$, and we can always rearrange the columns of $X$ to have the first $|M|$ covariates as active. Let $\eta=(X_M^+)^\top e_j\in\RR^T$ be a vector where $e_j\in\RR^{|M|}$ is a vector with 1 at $j$th coordinate and 0 otherwise.
Hence, the $\eta$ vector is the linear mapping from $Y$ to the $j$th coordinate of an OLS estimator. In particular, the debiased estimator and the response satisfy the following relationship:

\begin{lemma_app}\label{lem1}
	{\bf Debiased Estimator is OLS-post-LASSO}\\
	The debiased estimator is a linear mapping of $Y$. Specifically, given $\eta=(X_M^+)^\top e_j$:
	\begin{equation}\label{16}
		\bar{\beta}_j=\eta^\top Y
	\end{equation}
	Moreover, $\bar{\beta}_{M}$ is the OLS estimate of regressing $X_M$ on $Y$:
	\begin{equation}
		\bar{\beta}_{M}=\argmin_\beta\frac{1}{2T}\|Y-X_M\beta\|_2^2.
	\end{equation}
\end{lemma_app}

The proof of Lemma \ref{lem1} is deferred to the Online Appendix. Although its proof is simple, this lemma reveals that our debiased estimator is the same as the least-square after LASSO estimator proposed in \cite{10.3150/11-BEJ410}. 


Our strategy to obtain a rigorous statistical inferential theory with $p$-values is as follows. First we perform an algebraic manipulation to transform $\hat{\beta}_M$ into $\bar{\beta}_M$ in the linear form of (\ref{16}). Then, we follow the strategy in \cite{lee2016exact} to traverse the KKT subgradient optimal equations for general $\bm{X}$ by writing it equivalently into a truncation in the form of $\{ AY\leq b\}$, as we will do in Lemma \ref{lem2}. Finally we will circle back to $\hat{\beta}_M$ by the linear mapping between $\bar{\beta}_M$ and $Y$ and the distributional results induced by the fact that $Y$ is truncated by $\{A Y\leq b\}$.

For our Weighted-LASSO, the KKT sub-gradient equations are 
\begin{equation}
	X^\top (X\hat{\beta}-Y)+\lambda\begin{bmatrix}s\\v\end{bmatrix}\odot\omega^{-1}=0\q \wh 
	\begin{cases}
		{s_j}=\sgn(\hat{\beta}_j)&\text{ if }\hat{\beta}_j\neq 0,\omega_j<\infty\\
		v_j\in[-1,1]&\text{ if }\hat{\beta}_j=0,\omega_j<\infty
	\end{cases}
\end{equation}

In other words, when $\omega$ is specified, the KKT conditions can be identified using the tuple of $\{M,s\}$, where $M$ is the active covariates set and $s$ is the signs of LASSO fit. This is a consequence of how LASSO KKT condition can separate the slacks into $s$ for active variables and $v$ for inactive variables. Let $\mathcal{J}$ be the set of $j$'s corresponding to $\omega_j$'s that are infinite. When $\mathcal{J}\neq \emptyset$, we would simply need $s_j<\infty$ for $i\in\mathcal{J}$ because $\lambda{s}_j/\omega_j=0$ is guaranteed. We rigorously characterize the KKT sub-gradient conditions as a combinations of signs and infinity norm bounds conditions by the following lemma, which parallels Lemma 4.1 of \cite{lee2016exact}:

\begin{lemma_app}\label{lem2}
	{\bf Selection in norm equivalency}\\
	Consider the following random variables
	\begin{equation}
		\begin{split}
			w(M,s,\omega)&=(X_M^\top X_M)^{-1}(X_M^\top Y-\lambda s\odot\omega^{-1}_M)\\
			u(M,s,\omega)&=\omega_{-M}\odot
			\left(
			X_{-M}^\top(X_M^+)^\top s\odot\omega^{-1}_M
			+\frac{1}{\lambda}X_{-M}^\top (I-P_M)Y\right)
		\end{split}
	\end{equation}
	where $P_M=X_MX_M^+$ is the projection matrix. The Weighted-LASSO selection can be written equivalently as
	\begin{equation}
		\{M,s\}=\{\sgn(w(M,s,\omega))=s,\|u(M,s,\omega)\|_\infty<1 \}
	\end{equation}
\end{lemma_app}


Using this characterization, we are then able to provide the distributional results for the debiased estimators. Consider $\xi=\Sigma\eta(\eta^\top \Sigma\eta)^{-1}\in\RR^{T}$ as a covariance-scaled version of our $\eta$, and a mapping of $\bm{Y}$ using residual projection matrix: $ z=(I-\xi\eta^\top )\bm{Y}$. Note that $z$ can be calculated once we observe $(\bm{X},\bm{Y})$, so it can be conditioned on were we to do so. We will soon see that the truncation set will depend on the variable $z$, but this does not cause any issues thanks to the following lemma, the proof of which is deferred to the Online Appendix:

\begin{lemma_app}{\bf Ancillarity in truncation}\label{lem3}\\
	The projected $z$ and the debiased estimator $\bar{\beta}_j$ are independently distributed.
\end{lemma_app}

As a result of Lemma \ref{lem3}, when describing the distribution of $\bar{\beta}_j$, we can use $z$ in its truncation conditions as long as we condition on $z$ as well. To simplify notation, we can collect all quantities we need to condition on into $\tilde{\mathcal{M}}:=((M,s),z,\omega, X)$. Now we can combine Lemmas \ref{lem1}, \ref{lem2}, \ref{lem3} to arrive at the truncated Gaussian statements for the debiased estimator similar to \cite{lee2016exact}, but for weighted-LASSO: 


\begin{theorem_app}{\bf Truncated Gaussian}\label{thm1}\\
	Under Assumptions \ref{asu1} and \ref{asu_known} for $j\in M$,  $\bar{\beta}_{j}$ is conditionally distributed as:
	\begin{equation}
		\bar{\beta}_{j}|\tilde{\mathcal{M}}			\sim \mathcal{TN}(\beta_{j},\eta^\top \Sigma\eta;[V^{-}(z),V^{+}(z)])
	\end{equation}
	where $\mathcal{TN}$ is a truncated Gaussian with mean $\beta_{j}$, variance $\eta^\top \Sigma\eta$ and truncation set $[V^{-}(z),V^{+}(z)]$. $\beta_{j}$ denotes the $i$th entry of the true $\beta$. The vector of signs is $s=\sgn(\hat{\beta}_M)\in\RR^{|M|}$ and the truncation set depends on
	\begin{align*}
		&A=\begin{bmatrix}
			\lambda^{-1}X_{-M}^\top (I-P_M)\\
			-\lambda^{-1}X_{-M}^\top (I-P_M)\\
			-\text{diag}(s)X_M^+
		\end{bmatrix}\in\RR^{(2J-|M|)\times T},  
		\;\;\;  b=\begin{bmatrix}
			\omega_{-M}^{-1}-X_{-M}^\top(X_M^+)^\top s\odot\omega^{-1}_M\\
			\omega_{-M}^{-1}+X_{-M}^\top(X_M^+)^\top s\odot\omega^{-1}_M\\
			-\lambda\cdot \text{diag}(s)(X_M^\top X_M)^{-1}s\odot\omega^{-1}_M
		\end{bmatrix}\in\RR^{2J-|M|} \\
		&V^{-}(z)=\max_{j:(A\xi)_j<0}\frac{b_j-(Az)_j}{(A\xi)_j}, \qquad
		V^{+}(z)=\min_{j:(A\xi)_j>0}\frac{b_j-(Az)_j}{(A\xi)_j}.
	\end{align*}	
\end{theorem_app}
Assumption \ref{asu1} of Gaussian errors can be relaxed and replaced by an appropriate asymptotic normal distribution based on a central limit theorem. The arguments for this extension are discussed in \cite{tian2017asymptotics}. The general structure of the statement would remain unchanged, and the coefficients would asymptotically follow the same truncated Gaussian distribution. We adopt Assumption \ref{asu1} to simplify our discussion, and point out where the weights $\omega$ appear in the post-selection inference.      

%\cmt{Asymptotic normality of debiased LASSO estimator has been studied in \cite{tian2017asymptotics} and \cite{zhao2021defense}, where the former is a post-selection inference result with more general considerations such as generalized linear model, and the latter is not in the post-selection inference framework. Our asymptotic distribution is a consequence of Theorem \ref{thm:appendix} which in Equation (\ref{eq:asymp_appendix_thm}), give us the post-selection distribution of the $\bar{\beta}_j/\|\eta\|\hat{\sigma}(Y)$ under the conditional null. Constructing post-selection $p$-values of the active coefficients then parallels the exercise in classical OLS inference.}

Notice that Theorem \ref{thm1} is decoupled across $M$, which is to say we are able to deal with $1$-dimensional statistics. We arrive at this form because the construction of $(V^-,V^+)$ over the extreme points of the linear inequality system (or vertices of the polyhedral) has decomposed the dimensionality of the truncation. This decoupling is of significant practical value, in that it would be otherwise a non-trivial task to calculate a statistic of multivariate (in our case $|M|$-dimensional) truncated Gaussian and then marginalize over $|M|-1$ dimensions.


\subsection{Weighted-LASSO Quasi-Linear Truncation with Estimated Variance}\label{lab:appendixA2}

This section generalizes the distribution results to the practically relevant case when the noise variance is unknown and has to be estimated. This becomes a challenging problem for post-selection inference. We replace Assumption \ref{asu_known} by the following assumption:

\begin{assumption_app}{\bf Gaussian residual with simple unknown variance}\label{asu_unknown}\\
	The residuals are distributed as $\epsilon_j\iid \mathcal{N}(0,\sigma^2)$ where $\sigma^2$ is unknown.
\end{assumption_app}


The simple structure of unknown variance of Assumption \ref{asu_known} is common in the post-selection inference literature as for example in \cite{lee2016exact} and \cite{tian2017selective}. A feasible conditional distribution replaces $\sigma^2$ with an estimate. Under Assumption \ref{asu_known}, we can estimate the variance using LASSO residuals and then reiterate the previous truncation arguments. The most common standard variance estimator is
\begin{equation}\label{eq21:sd.est}
	\hat{\sigma}^2(Y)=\|Y-X_M\bar{\beta}_M\|_2^2/(T-|M|).
\end{equation}


In classical regression analysis, the normally distributed estimated coefficient divided by an estimated standard deviation follows a $t$-statistic. Hence, we would expect that a truncated normal debiased estimator divided by a sample standard deviation might yield a truncated $t$-distribution. However, the arguments are substantially more involved. Simply using $\hat{\sigma}(Y)$ of (\ref{eq21:sd.est}) in the expression $\eta^\top\Sigma\eta$ of Theorem \ref{thm1} changes the truncation. Specifically, $Y$ having truncated support means $\hat{\sigma}(Y)^2$ is not $\chi^2$-distributed supported on the entire $\RR_+$, which makes the support of $\bar{\beta}/\hat{\sigma}(Y)$ non-trivial. Therefore, in order to correctly assess the truncation of the studentized quantity,  we have to disentangle how much truncation is implied in $\hat{\sigma}(Y)^{-1}$ and $\bar{\beta}$ simultaneously. Geometrically, as $\hat{\sigma}(Y)$ is a non-linear function of $Y$ and $\bar{\beta}$, the truncation on $Y$ is in fact no longer of the simple linear form $\{AY\leq b\}$ such as in Theorem \ref{thm1}.

Instead of a polyhedral induced by affine constraints, we have a ``quasi-affine constraints'' form of $\{C Y\leq \hat{\sigma}(Y) b\}$ because LASSO KKT conditions preserve the estimated variance throughout the arguments. Thus, both sides of the inequality $CY\leq \hat{\sigma}(Y)b$ have $Y$, and in right-hand-side the $\hat{\sigma}(Y)$ is non-linear in $Y$. A significantly more complex set of arguments are needed compute the exact truncation, which is equivalent to solve for a $|M|$-system of non-linear inequalities rather than linear inequalities that constrain the support of $Y$ for inference on each $\bar{\beta}_j$. Theorem \ref{thm:appendix} shows the appropriate truncation based on those arguments:

\begin{theorem_app}{\bf Truncated $t$-distribution for estimated variance}\label{thm:appendix} \\
	Under Assumptions \ref{asu1} and \ref{asu_unknown}, and the null hypothesis that $\beta_j=0$, the conditional distribution of the studentized statistic follows
	\begin{equation}
			\frac{\bar{\beta}_j}{\|\eta\|\hat{\sigma}(Y)}\sim TT_{d;\Omega},
	\end{equation}
	where $TT$ is a truncated t-distribution with $d$ degrees of freedom and truncation set $\Omega$. The truncation set $\Omega=\bigcap_{j\in M}\{t:t\sqrt{W}\nu_j+\xi_j\sqrt{d+t^2}\leq -\theta_j\sqrt{W}\}$ is an $|M|$-intersection of simple inequality-induced intervals based on the following quantities, where the active signs are denoted as $s=\sgn(\hat{\beta}_M)\in\RR^{|M|}$: 
	\begin{align*}
		\theta_j &=
		(\lambda s_j/\hat{\sigma}(Y) )\cdot  e_j^\top \left((X_M^\top X_M)^{-1}s\odot \omega^{-1}\right) \;\; \text{ for }j\in M,\\
		C&=-diag(s) X_M^+\in\RR^{|M|\times T}, \qquad \nu=C\eta/\|\eta\|_2\in\RR^{|M|}, \qquad \xi=C (P_M-\eta\eta^\top /\|\eta\|_2^2 )Y\in\RR^{|M|}, \\
		d&=\tr(I_T-P_M), \qquad W=\|(I-P_M)Y\|_2^2+\|\eta^\top Y\|_2^2/\|\eta\|_2^2.
	\end{align*}

	%So the following $p$ is the conditionally valid $p$-value of the null $H_{0,i}:\beta_j|\tilde{\mathcal{M}}$
	%	\begin{equation}
		%		\begin{split}
			%			p&=F_{TT_{d;\Omega}}(-
			%			|\bar{\beta}_j|/\|\eta\|\hat{\sigma}(Y))+
			%			F^C_{TT_{d;\Omega}}(|\bar{\beta}_j|/\|\eta\|\hat{\sigma}(Y))
			%		\end{split}
		%	\end{equation}
\end{theorem_app}



The quantities $\theta$ and $C$ describe the quasi-linear constraints, whereas $\nu$ and $\xi$ transform them into the form of $\Omega$. Note that the $\Omega$ set is obtained from solving a low-dimensional set of quadratic inequalities that do not necessarily yield a single interval after intersection. We provide a proof of this result in the Online Appendix.

Using Theorem \ref{thm:appendix} in practice poses several challenges. First, the computations are much more involved, especially as each $\beta_j$ requires calculation of $\Omega$ which includes $|M|$ actual constraints, each of which involves solving a simple but still non-linear inequality. It is non-trivial to ensure that the numerical stability holds at every step of the calculations. Second, since $\Omega$ is not necessarily an interval, it is harder to interpret the truncation and also calculate the cumulative density function through Monte-Carlo simulations when there is a non-trivial truncation structure. Third, in fact, the authors in \cite{tian2017selective} recommend a regularized likelihood minimizing variance estimator that deviates from the simple $\hat{\sigma}(Y)$, which would in turn involves more numerical integration and optimization steps. Last but not least, this result was proposed initially for studying scale-LASSO. Our goal is to provide a set of tools that can be useful for a wide range of applications including the LASSO with $l_2$ squared norm loss rather than un-squared norm loss. These implementation difficulties are also discussed in more detail in the Online Appendix, which provides the accompanying proofs and the exact forms of the truncations.

We provide a practical solution based on an asymptotic normal argument. We impose the following two standard assumptions. First, we assume that we have a consistent estimator of the residual variance:
\begin{assumption_app}{\bf Consistent estimator $\hat{\sigma}^2(Y)$} \label{asu_consist}\\
The residual variance estimator is consistent $\hat{\sigma}^2(Y)\CP\sigma^2$ as $T\to\infty$ given the selection of covariates.
\end{assumption_app}
This general assumption includes many common scenarios such as the results specified in Corollary 6.1 of \cite{BG}, or in Theorem 2 of \cite{chatterjee2014assumptionless}. 

Second, we need to impose standard assumptions on the asymptotic moments of the covariates and asymptotic rates. Assuming full rank for the second moment of the selected set of covariates ensures ensures that we can run a regression with the active set. This standard assumption is also referred to as the Irrepresentable Condition (\cite{zhao2006model}) in the LASSO literature. It simply states that the linear regression with the selected covariates is well-defined in the limit. Moreover, we assume that the number of covariates $J$ grows linearly with the number of time-series observations $T$, which is the largest possible rate compatible with Assumption \ref{asu1}(c). Last but not least, we assume that $\lambda$ shrinks sufficiently fast. Our rate assumption on $\lambda$ follows the usual choice in this literature as for example in Corollary 3 of \cite{10.1214/12-STS400} and Corollary 6.1 of \cite{BG}. This $\lambda$ rate is also adopted in \cite{tian2017asymptotics}, and \cite{10.3150/11-BEJ410} refer to this as ``typically used in the literature''. In addition, $\lambda$ shrinking at this rate leads to diminishing $\lambda\sqrt{\log(J)/T}\to 0$ as $J,T$ grow and consistency of $\hat{\sigma}^2(Y)$ of (\ref{eq21:sd.est}) in the setting of \cite{chatterjee2014assumptionless}




%\cmt{More assumptions are needed related to regulate asymptotic behavior of the estimating equations.} One assumption is a full rank assumption on the selected set of covariates that ensures that we can run a regression with the active set. This assumption is also standard and referred to as the Irrepresentable Condition (\cite{zhao2006model}) in the LASSO literature. It simply states that the linear regression with the selected covariates is well-defined in the limit. \cmt{Moreover, we consider our number of covariates to be growing linearly as the number of observations, which is the max rate possible compatible with Assumption \ref{asu1}(c). Finally, we assume that $\lambda$ is shrinking fast enough, compared to $\sqrt{T}$. Our specific rate follows a long line of studies on choice of $\lambda$ to ensure LASSO performance such as Corollary 3 of \cite{10.1214/12-STS400} and Corollary 6.1 of \cite{BG}. This $\lambda$ rate choice is also adopted in \cite{tian2017asymptotics}, and it is referred to as ``typically used in the literature'' and used for comparison by \cite{10.3150/11-BEJ410}. In addition, $\lambda$ shrinking at this rate leads to diminishing $\lambda\sqrt{\log(J)/T}\to 0$ as $J,T$ grow and consistency of $\hat{\sigma}^2(Y)$ of (\ref{eq21:sd.est}) in the setting of \cite{chatterjee2014assumptionless}. }


%For the Weighted-LASSO the full rank assumption has to hold for the weighted active covariates as formalized in the following assumption. It states that the linear regression with the selected weighted covariates is well-defined. 

%\cmt{Another assumption commonly used in LASSO literature is Irrepresentable Condition (\cite{zhao2006model}) on smallest eigvenvalue of the gram matrix of active covariates, which makes sure that the matrix inversions are well-behaved. In Weighted-LASSO setting, we require a weighted parallel of this condition as follows:}
\begin{assumption_app}\label{asu_gram} {\bf Asymptotic behavior}
	\vspace{-0.1cm}
	\begin{enumerate}[label=(\alph*)]
		\item {\bf  Full-rank second moment}:\\
	 Given the selected set of covariates $M$, the sample second moment matrix $\frac{1}{T} X_M^\top X_M $ of the active covariates $X_M$ converges to a full-rank matrix as $T \rightarrow \infty$.
		\item {\bf Growth rates}:\\
The rates satisfy $J=O(T)$ and $\lambda=O \left (\sqrt{\frac{\log J}{T}} \right)$ as $T\to\infty$.
	\end{enumerate}
\end{assumption_app}
%\begin{assumption_app}{\bf Bounded eigenvalue for weighted covariates} \label{asu_gram}\\
%	 Given the selected set of covariates $M$, the sample second moment matrix $\frac{1}{T} X_M^\top X_M $ of the active covariates $X_M$ converges to an invertible matrix as $T \rightarrow \infty$.
% Given the selected set of covariates $M$, the smallest eigenvalue of $(w_M^\top X_M^\top X_M w_M)$ is \cmt{uniformly bounded away from zero for all $T$ at $c_0>0$}, where $w_M$ are the weights for the active covariates $X_M$.
	%Given selected variable set $M$, let $v_{min}$ denote the smallest eigenvalue of gram matrix of weighted covariates $X_Mw_M$, i.e. $v_{min}=\text{Smallest eigenvalue of }(w_M^\top X_M^\top X_M w_M)$. We assume $v_{min}\geq c_0>0$.
%\end{assumption_app}
Under these additional assumptions, the next theorem shows that the truncated Gaussian distribution results hold for the feasible Weighted-LASSO with estimated noise variance.


%Then, it would be immediately appealing to consider the following proposal:

\begin{theorem_app}{\bf Asymptotic truncated normal distribution as approximation}\label{col2}\\
	Suppose Assumptions \ref{asu1}, \ref{asu_unknown}, \ref{asu_consist} and \ref{asu_gram} hold. For $T \rightarrow \infty$ and under the null hypothesis that $\beta_j=0$ and conditional on the selection events and the weights, the studentized quantity $\bar{\beta}_j/\|\eta\|\hat{\sigma}(Y)$ converges to a truncated Gaussian distribution. Moreover, the conditional $p$-values calculated from $TN_{\Omega}(\bar{\beta}_j/\|\eta\|\hat{\sigma}(Y))$ with an estimated noise variance satisfy Assumption \ref{assu:valid_p}, where $TN_{\Omega}$ is a standard normal distribution truncated to  $\Omega=[{V}^-(z)/\|\eta\|_2\hat{\sigma}(Y);{V}^+(z)/\|\eta\|_2\hat{\sigma}(Y)]$, and ${V}^{-}(z)$ and ${V}^{+}(z)$ are the same as in Theorem \ref{thm1}.
	% \begin{equation}\label{eq:asymp_appendix_thm}
	% 	\bar{\beta}_j/\|\eta\|\hat{\sigma}(Y)\sim TN_{\Omega},
	% \end{equation}
	\end{theorem_app}


% \begin{theorem_app}{\bf Asymptotic truncated normal distribution}\label{col2}\\
	%  Suppose Assumptions \ref{asu1}, \ref{asu_unknown} and \ref{asu_consist} hold. \cmt{Consider $T\to\infty$ and under the null hypothesis that $\beta_j=0$ and conditional on the selection events and the weights. The studentized quantity $\bar{\beta}_j/\|\eta\|\hat{\sigma}(Y)$ converges to
		% \begin{equation}\label{eq:asymp_appendix_thm}
			%     \bar{\beta}_j/\|\eta\|\hat{\sigma}(Y)\sim TN_{\Omega},
			% \end{equation}
		% where $TN$ is a standard normal distribution truncated to  $\Omega=[{V}^-(z)/\|\eta\|_2\hat{\sigma}(Y);{V}^+(z)/\|\eta\|_2\hat{\sigma}(Y)]$, where ${V}^{-}(z)$ and ${V}^{+}(z)$ are the same as in Theorem \ref{thm1}.  Moreover, under the null hypothesis that $\beta_j=0$ and conditional on the selection events and the weights, $\Phi^{\Omega}(\bar{\beta}_j/\|\eta\|\hat{\sigma}(Y))$ converges to \textrm{Unif }$[0,1]$, where $\Phi^{\Omega}$ is the cdf of standard normal distribution truncated to $\Omega$.}
	% \end{theorem_app}


%\cmt{Asymptotic normality of debiased LASSO estimator has been studied in \cite{tian2017asymptotics} and \cite{zhao2021defense}, where the former is a post-selection inference result with more general considerations such as generalized linear model, and the latter is not in the post-selection inference framework. Our asymptotic distribution is a consequence of Theorem \ref{thm:appendix} which in Equation (\ref{eq:asymp_appendix_thm}), give us the post-selection distribution of the $\bar{\beta}_j/\|\eta\|\hat{\sigma}(Y)$ under the conditional null. Constructing post-selection $p$-values of the active coefficients then parallels the exercise in classical OLS inference.}

The asymptotic result has several advantages. First, it is intuitive since it parallels the classical OLS inference with a $t$-statistic converging to Gaussianity. Secondly, it is computationally more tractable than results of Appendix Theorem \ref{thm:appendix}. With this result, one can obtain asymptotically valid post-selection $p$-values. The uniform distribution of the post-selection p-values conditional on the selection and under the null hypothesis $H_D$, are a direct consequence of the distribution result.\footnote{\cite{tian2017selective} propose an alternative estimator for the noise variance that also leads to a truncated Gaussian distribution. Their estimator for $\sigma^2$ is obtained from solving a more complicated regularized pseudolikelihood problem, instead of the conventional sample variance estimator in equation (\ref{eq21:sd.est}).} Our asymptotic distribution is derived conditional on the selection that is obtained from the finite sample. This is the same perspective as taken in \cite{tian2017asymptotics}, and the resulting $p$-value statements echo their Lemma 4. It is a challenging open question to derive the asymptotic distribution when the selection event changes as $T$ grows.    


%\cmt{We note that as $T$ grows, the selection event might change. We study the inference of the debiased LASSO estimator from a specific selection event using the observed data, and provide an instrument for $p$-value calculation with unknown variance. The LASSO is viewed similarly to ``selection procedure'', and the resulting $p$-value statements echo Lemma 4 of \cite{tian2017asymptotics}.}

%an asymptotic truncated Gaussian approximation when variance is unknown that uses an alternative estimate of $\sigma^2$ solved from a regularized pseudolikelihood problem, rather than the one in (\ref{eq21:sd.est}). We adhere to using the variance estimator $\hat{\sigma}^2(Y)$ in (\ref{eq21:sd.est}) since it is commonly used, and extract $p$-values that satisfy Assumption \ref{assu:valid_p} and are valid for our panel testing procedures. The asymptotic results are stated in the following theorem.


%\textit{Remark 4:} Note that one could argue if an alternative covariance estimator of rank higher than 1 is available, there might be a setting between GDKV and GDUV where APPENDIX THEOREM \ref{col2} is generalizeable by relaxation of ASSUMPTION \ref{asu7}. However, as it pertains to the theoretical results in this section, we follow the similar setting of \cite{tian2017selective} which also examines rank-1 estimated variances. It is unclear to us what would be the appropriate weaker assumption: would the consistency of an estimate of residuals of response projected onto the orthogonal complement subspace spanned by the active covariates (i.e. $Proj^\perp_{X_M}(Y)$) be sufficient? We leave it as an open question for future research.
%The truncation of  $\{C Y\leq \hat{\sigma}(Y) b\}$ involves inverting this system of inequalities, as discussed in \cite{tian2017selective}, and we provide the solved detailed form, denoted as $\Omega$, in our corollary below. Its distributional result a finite-sample result that directly follows from Gaussian linearity assumption.

%\newpage


\section{Proof of Theorem \ref{thm_MT}}\label{app:proof}

\textbf{Proof:} We show how to count false discoveries under the data-driven null hypothesis conditional on the selection and then evaluate the probability of at least one false discovery under $H_D$. By design of $H_D$, we only need to consider false selections for the covariates that are active for some units. For $\mathcal{K}_j\neq \emptyset$, we denote by $j\in H_M$ that the $j$th covariate is tested in $H_M$. 

First, we separate the data-driven hypothesis into individual components. Within $H_D$, we consider all the hypotheses associated with the $j$th covariate denoted as $H_{D,j}$. The hypothesis $H_{D,j}$ represents an intermediate level of hypotheses between the panel-level $H_D$ and unit-covariate individual null $H_{0,j}^{(n)}|\mathcal{M}^{(n)}$. When written as a set intersection, $H_{D,j}$ equals $\bigcap_{n\in\mathcal{K}_j}H_{0,j}^{(n)}|\bigcap_{n\in \mathcal{K}_j}\mathcal{M}^{(n)}$, that is, $H_{D,j}$ is joint over all the units where the $j$th covariate is active. 

Second, we count the number of false discoveries. Under the null hypothesis $H_D$, we denote the number of false discovery in $H_{D,j}$ as $V_j$. Hence, the total number of false discoveries is the sum over the false discoveries of all covariates given by
\begin{equation}\label{eq:breakupV}
	\begin{split}
		\PP_{H_D} \left(V\geq 1|\mathcal{M} \right)		=	\PP_{H_D} \left(\sum_{j\in H_M} V_j\geq 1|\bigcap_{n\in \mathcal{K}_j}\mathcal{M}^{(n)} \right).
	\end{split}
\end{equation}
Each $V_j$ can be further broken down into the false discovery against the unit-covariate null hypotheses. Each individual potential false discovery is a random event, and the sum of false discoveries greater or equal to 1 corresponds to the union of these random events. The union of these random events has the conditional distribution
\begin{align}
	\PP_{H_D} \left(\sum_{j\in H_M} V_j\geq 1|\bigcap_{n\in \mathcal{K}_j}\mathcal{M}^{(n)} \right)
	= & \PP_{H_D}\left(\bigcup_{j\in H_M} \left\{ \bigcup_{n\in\mathcal{K}_i} \left\{\text{Rejection made based on } p_{j}^{(n)}|\bigcap_{n\in \mathcal{K}_j}\mathcal{M}^{(n)} \right\} \right\} \right) \nonumber\\
	= & \PP_{H_D} \left( \bigcup_{j\in H_M} \left \{ \bigcup_{n\in\mathcal{K}_i} \left\{ p_{j}^{(n)}\leq \rho\frac{\gamma}{ N_j}|\bigcap_{n\in \mathcal{K}_j}\mathcal{M}^{(n)} \right \} \right \} \right ).
\end{align}
The second line simply follows from the design of our rejection procedure. Boole's inequality implies the following union bound:
\begin{equation}\label{eq:boole}
	\begin{split}
		\PP_{H_D} \left (\bigcup_{j\in H_M} \left\{ \bigcup_{n\in\mathcal{K}_i} \left\{ p_{j}^{(n)}\leq \rho\frac{\gamma}{ N_j}|\bigcap_{n\in \mathcal{K}_j}\mathcal{M}^{(n)} \right \} \right \} \right)
		\leq & \sum_{j\in H_M}\sum_{n\in\mathcal{K}_j}\PP \left(p_{j}^{(n)}\leq \rho\frac{\gamma}{ N_j}|\bigcap_{n\in \mathcal{K}_j}\mathcal{M}^{(n)} \right).
	\end{split}
\end{equation}
Third, we take advantage of Assumption \ref{assu:valid_p}. Under Assumption 1, it holds that $\PP_{H_D}(p_{j}^{(n)}\leq \rho\frac{\gamma}{N_j})\leq  \rho\frac{\gamma}{N_j} $, which appears in  the right-hand side of (\ref{eq:boole}). Thus, combining equations (\ref{eq:breakupV})$\sim$(\ref{eq:boole}) yields:
\begin{align}
	\PP_{H_D}(V\geq 1|\mathcal{M})	
	\leq    \sum_{j\in H_M}\sum_{n\in\mathcal{K}_j}\rho\frac{\gamma}{ N_j} =  \gamma\cdot \rho\cdot \sum_{j\in H_M}\frac{1}{N_j}\sum_{n\in\mathcal{K}_j}1
	=  \gamma\cdot \rho\cdot \sum_{j\in H_M}\frac{|\mathcal{K}_j|}{N_j}
	=\gamma.
\end{align}
% \begin{equation}
	% 	\begin{split}
		% \PP_{H_D}(V\geq 1|\mathcal{M})	
		% 	\leq  &  \sum_{j\in H_M}\sum_{n\in\mathcal{K}_j}\rho\frac{\gamma}{ N_j}\\
		% 	= & \gamma\cdot \rho\cdot \sum_{j\in H_M}\frac{1}{N_j}\sum_{n\in\mathcal{K}_j}1\\
		% 	= & \gamma\cdot \rho\cdot \sum_{j\in H_M}\frac{|\mathcal{K}_j|}{N_j}
		% 	=\gamma
		% \end{split}
	% \end{equation}
The second-to-last equation uses and also explains the definition of $\rho$. This completes the proof.\hfill\textbf{[QED]}

\section{Appendix: Empirics}

%\footnotesize
%
%\begin{figure}[H]
%\centering	\tcaptab{Performances of Different Factor Sets on DS}	\label{fig:ds-perf-by-w}
%\begin{subfigure}[t]{.48\textwidth}
%	\includegraphics[width=1\linewidth]{plots/"DS MSE by W"}
%	  \caption{RMSE}
%	\end{subfigure}
%	\begin{subfigure}[t]{.48\textwidth}
%	\includegraphics[width=1\linewidth]{plots/"DS LL by W"}
%	  \caption{$LL$}
%	\end{subfigure}
%	\bnotefig{These figures provide cross-sectional goodness-of-fit measurements in $T_{selection}$ (bars in navy blue) vs $T_{exclusion}$ (bars in orange) with factor sets FF, HL, HL PC, HL + HL PC, HL-$\omega_4$ and HL-$\omega_8$; and with FWER control $\gamma=0.05$.}
%\end{figure}
%
%
%\begin{figure}[H]
%    \tcaptab{Boxplots for $p$-values of simulated factors}\label{fig:olspvalboxplots}\centering
%    	\begin{subfigure}[t]{.9\textwidth}\centering
%		\includegraphics[width=.9\linewidth]{plots/NaiveT_pval_boxplots}
%	  \caption{$\acute{p}_j^{(j)}$ from $t$-stat of LASSO}
%	\end{subfigure}
%	\begin{subfigure}[t]{.9\textwidth}\centering
%		\includegraphics[width=.9\linewidth]{plots/PoSI_pval_boxplots}
%	  \caption{$p_j^{(j)}$ from APPENDIX Theorem\ref{col2} of PoSI}
%	\end{subfigure}\\
%	{\scriptsize This figure is of the boxplots of $p$-values of 121 simulated factors across 243 DS returns. Each boxplot corresponds to the $p$-values of 1 simulated factors across all DS returns. Red horizontal lines are at $5\%$.}
%	\end{figure}
%
%\begin{sidewaysfigure}
%\centering
% 	 \begin{figure}[H]
%	\tcaptab{Scatterplots for actual avg returns vs predicted}
%	\label{fig:DS_scatterplots}\centering
%	\begin{subfigure}[t]{0.9\textwidth}
%	\includegraphics[width=0.8\linewidth]{plots/"DS_scatterplots"}
%		\caption{DS as test assets}
%	\end{subfigure}
%	\begin{subfigure}[t]{0.9\textwidth}
%	\includegraphics[width=0.8\linewidth]{plots/"SV_scatterplots"}
%		\caption{SV as test assets}
%	\end{subfigure}\\
%	{This plot shows the mini regression results by scatterplots in the two separate time periods. Within each figure: first row corresponds to $T_{selection}$; second row corresponds to $T_{exclusion}$. \\
%	The red diagonal line is $y=x$.	\texttt{RMSE}'s are root-mean-squared errors of the cross-sectional regression $\texttt{lm}( \EE_T[Y]\sim\EE_T[\hat{\Lambda}\hat{\bm{\beta}}^\top ])$}
%		\end{figure}
%\end{sidewaysfigure}

%\newpage


\begin{table}[H]
\tcaptab{Compositions of DS portfolios}\label{DSSource}
\centering\scriptsize	
\begin{tabular}{c|c||c|c||c|c||c|c}
\toprule
Sorted by     & \# portfolios &Sorted by     & \# portfolios&Sorted by     & \# portfolios&Sorted by     & \# portfolios\\ \midrule
BEME, INV     & 25& ME, CFP       & 6&   ME, INV       & 25& ME, Prior1  & 25 \\ 
BEME, OP      & 25& ME, DP        & 6&   ME, OP        & 25& ME, Prior12 & 25\\ 
ME, BE        & 25& ME, EP        & 6&   OP, INV       & 25& ME, Prior60 & 25\\\bottomrule
\end{tabular}
\bnotetab{This table lists the composition of double sorted portfolios that we use as test assets in our empirical study. All the double sorted portfolios are from Kenneth French's data library.}
\end{table}
%	
%	
%\begin{table}[H]
%	\tcaptab{Candidate factor sets}\label{candiFactors}
%\centering\small
%	\begin{tabular}{c||c|c|c|l}
%		\toprule
%		Factor set name & Source         & Dimension & Inference & \multicolumn{1}{c}{Remark}    \\ \midrule
%		FF              & \href{https://mba.tuck.dartmouth.edu/pages/faculty/ken.french/Data_Library/f-f_5_factors_2x3.html}{French's website} & 5    & \texttt{NA}     & No normalization or selection             \\
%		HL              & \href{http://global-q.org/testingportfolios.html}{Hou's website}    & 121  & Panel-PoSI     & Normalized by $cov_{T_{selection}}(\text{HL})$\\
%		HL PC           & HL             & 50  & Step-down       & Normalized by $cov_{T_{selection}}(\text{HL PC})$\\
%		HL + HL PC      & HL, HL PC      & 171 & Panel-PoSI        & Direct concatenation          \\
%		DS PC           & DS             & 50     & Step-down  & Normalized by $cov_{T_{selection}}(\text{DS PC})$               \\
%		HL-$\omega_{\text{FF3}}$              & HL & 121  & Panel-PoSI       & $+\infty$ prior belief on FF3\\
%		HL-$\omega_{\text{FF5}}$              & HL & 121   & Panel-PoSI      & $+\infty$ prior belief on FF5\\ \bottomrule
%	\end{tabular}
%	\bnotetab{This table describes the factor candidate sets in our consideration. In total, we explored 5 unweighted and 8 weighted factor candidate pools. Each has 660 monthly observations (January, 1967 to December, 2021). The $T_{selection}$ is defined as the first 330 observations (January, 1967 to June, 1994) which was used in the Panel-PoSI inferential pipeline that selects the parsimonious model. The ``Inference'' column details which inferential method was used: either none was necessary (for FF's case), or the factors are unordered and Panel-PoSI was used, or the factors have intrinsic sequential ordering and the step-down procedure was used.}
%\end{table}

%
%\begin{table}[H]
%	\tcaptab{Number of selected actual factors vs simulated factors}\label{tab3}
%	\centering\small
%	\begin{tabular}{c||c|c|c}
%		\toprule
%		Inference method                     & Selected total & Selected HL factors & Selected simulated factors \\ \midrule
%		Naive-OLS & 164             & 120                 & 44                    \\ 
%		Bonferroni-OLS       & 105             & 105                 & 0                     \\ 
%		Naive-NaiveLASSO & 81 & 80 & 1 \\ 
%		Bonferroni-NaiveLASSO & 14 & 14 & 0 \\ 
%		DataDriven-NaiveLASSO & 20 & 20                 & 0                     \\ 
%		Panel-PoSI            & 5  & 5                   & 0                     \\ \bottomrule
%	\end{tabular}
%	\bnotetab{This table summarizes the inference result on simulated factors across different benchmark inference methods. We used HL factors $\bm{X}_{T_{selection}}\in\RR^{324\times 121}$ and simulated factors $\bm{\tilde{X}}_{T_{selection}}\in\RR^{324\times 121}$; DS excess returns $\bm{Y}_{T_{selection}}\in\RR^{324\times 243}$. The simulated factors and benchmarked inferential procedures were provided in PROCEDURE 3.}
%\end{table}
%
%
%
%
%\begin{table}[H]
%\tcaptab{Comparisons of cross-sectionally averaged performances among\\ factors selected by benchmarked methods of PROCEDURE 3}\label{tab:comparison-against-naive}
%	\centering\small
%\begin{tabular}{@{}l|ccc|ccc|c@{}}
%\toprule\toprule
%\multirow{2}{*}{Factor   set} & \multicolumn{3}{c|}{$T_{Selection}$}                                                            & \multicolumn{3}{c|}{$T_{Exclusion}$}                                                            & \multirow{2}{*}{Panel} \\ \cmidrule(lr){2-7}
%                              & \multicolumn{1}{c}{TNB} & \multicolumn{1}{c}{TB} & \multicolumn{1}{c|}{Panel-PoSI} & \multicolumn{1}{c}{TNB} & \multicolumn{1}{c}{TB} & \multicolumn{1}{c|}{Panel-PoSI} &                        \\ \midrule
%DS PC                         & 0.72                    & 0.72                   & 2.60                            & 5.12                    & 5.12                   & 4.82                            & \multirow{4}{*}{(I)}     \\
%HL                            & 1.01                    & 1.60                   & 3.51                            & 5.32                    & 5.46                   & 4.89                            &                        \\
%HL PC                         & 2.38                    & 4.12                   & 7.37                            & 6.32                    & 7.86                   & 10.74                           &                        \\
%HL + HL PC                    & 0.97                    & 1.87                   & 2.40                            & 5.25                    & 5.20                   & 5.06                            &                        \\ \midrule
%DS PC                         & 0.75                    & 0.75                   & 2.82                            & 64.11                   & 64.11                  & 62.03                           & \multirow{4}{*}{(II)}    \\
%HL                            & 1.54                    & 2.23                   & 2.98                            & 63.39                   & 62.82                  & 61.93                           &                        \\
%HL PC                         & 3.07                    & 5.03                   & 12.95                           & 61.46                   & 60.64                  & 51.04                           &                        \\
%HL + HL PC                    & 1.52                    & 2.31                   & 2.87                            & 63.39                   & 62.74                  & 61.99                           &                        \\ \midrule
%DS PC                         & 395                     & 395                    & 594                             & 1126                    & 1126                   & 1121                            & \multirow{4}{*}{(III)}   \\
%HL                            & 495                     & 557                    & 605                             & 1124                    & 1123                   & 1120                            &                        \\
%HL PC                         & 630                     & 717                    & 873                             & 1119                    & 1117                   & 1086                            &                        \\
%HL + HL PC                    & 494                     & 563                    & 598                             & 1124                    & 1123                   & 1121                            &                        \\ \bottomrule\bottomrule
%\end{tabular}\bnotetab{This table compares the cross-sectionally averaged performances in- and out-of-sample using HL factors selected by the two benchmark methods vs the Panel-PoSI procedure. All results are from regressions with test asset set being the DS. The $T_{selection}$ (in-sample) and $T_{Exclusion}$ (out-of-sample) periods follow from previous bifurcations.
%\begin{itemize}
%    \item [Panel (I)]: unexplained means measured by $\sqrt{\sum_{j\in[N]}\hat{\alpha}^{(j)2}}$, where each $\hat{\alpha}^{(j)}$ comes from an average of a single time-series regression's residuals;
%    \item [Panel (II)]: unexplained variances measured by averages of $\textrm{Var}_{T_a}(Y^{(j)}-X_{PS}\hat{\beta}^{(j)})$ across the $j\in[N]$;
%\item [Panel (III)]: averages of each time-series regression's negative log-likelihood from $\hat{\alpha}^{(j)}$ by Gaussian law.
%\end{itemize}
%}
%\end{table}
%
%
%\begin{table}[H]
%	\centering
%		\tcaptab{$z$-score of Flattened Residuals}	\label{zscoreDS}
%	\begin{tabular}{ccc|cc|cc}
%	\toprule\toprule
%	 &  &   &\multicolumn{2}{c}{$T_{Selection}$} & \multicolumn{2}{|c}{$T_{Exclusion}$}\\ \cmidrule(l){4-7}
%	Factor set &$d$     &       $K$&	$z$           & $p(z)$      & $z$             & $p(z)$      \\ \midrule
%	DS PC      & 50  & 4 & -0.002 & 0.999 & 0.001 & 1.000 \\
%FF         & 5   & 5 & -0.001 & 1.000 & 0.001 & 0.999 \\\midrule
%HL         & 121 & 5 & 0.003  & 0.998 & 0.003 & 0.997 \\
%HL PC      & 50  & 7 & -0.001 & 0.999 & 0.005 & 0.996 \\
%HL + HL PC & 171 & 5 & 0.003  & 0.997 & 0.004 & 0.997 \\\midrule
%HL-$\omega_1$      & 121 & 5 & 0.003  & 0.997 & 0.004 & 0.997 \\
%HL-$\omega_4$      & 121 & 5 & 0.003  & 0.997 & 0.004 & 0.997 \\
%HL-$\omega_5$      & 121 & 6 & 0.003  & 0.998 & 0.003 & 0.997 \\
%HL-$\omega_8$      & 121 & 8 & 0.000  & 1.000 & 0.001 & 0.999\\ \bottomrule\bottomrule
%	\end{tabular}
%	\bnotetab{This table exhibits $z(\hat{\alpha}_{T_a})$ and $p(z)$ given in (\ref{46}). $d$ is dimension of the covariates and $K$ is dimension of selected model. At the very top, we have the DS PC set used as a proxy for oracle factors and the baseline model of all 5 of FF factors. In the 3rd, 4th and 5th rows, different constructed factor set of HL were explored. In the 6th$\sim$9th rows, different weighted regimes of HL were explored. The response consists of the DS test assets, with $N=243$ as detailed in Table \ref{DSSource}. The selection was made with FWER control of $\gamma=5\%$ with $|T_{Selection}|=|T_{Exclusion}|=324$.}
%\end{table}
%\begin{table}[H]
%	\centering
%		\tcaptab{Chen-Pelger $F$-statistic of Residuals}\label{F-statDS}
%	\begin{tabular}{ccc|cc|cc}
%	\toprule\toprule
%	 &  &   &\multicolumn{2}{c}{$T_{Selection}$} & \multicolumn{2}{|c}{$T_{Exclusion}$}\\ \cmidrule(l){4-7}
%	Factor set &$d$     &       $K$&	$F$           & $\ln(pval(F))$      & $F$             & $\ln(pval(F))$      \\ \midrule
%DS PC      & 50  & 4 & 399 & -190 & 386 & -188 \\
%FF         & 5   & 5 & 389 & -186 & 328 & -180 \\\midrule
%HL         & 121 & 5 & 480 & -194 & 414 & -189 \\
%HL PC      & 50  & 7 & 295 & -171 & 257 & -166 \\
%HL + HL PC & 171 & 5 & 480 & -194 & 409 & -188 \\\midrule
%HL-$\omega_1$      & 121 & 5 & 480 & -194 & 409 & -188 \\
%HL-$\omega_4$      & 121 & 5 & 480 & -194 & 409 & -188 \\
%HL-$\omega_5$      & 121 & 6 & 471 & -191 & 402 & -185 \\
%HL-$\omega_8$      & 121 & 8 & 355 & -176 & 283 & -168\\ \bottomrule\bottomrule
%	\end{tabular}
%	\bnotetab{This table exhibits $F$ and $\ln(pval(F))$ given in (\ref{22}). The response set, time periods, table layout and inferential setting are the same as in Table \ref{zscoreDS}.}
%\end{table}
%\begin{table}[H]
%	\centering
%	\tcaptab{Cross-Sectional First- and Second-Moments of $\hat{\alpha}$}	\label{XSUV}
%	\begin{tabular}{ccc|ccc|ccc}
%	\toprule\toprule
%	 &  &   &\multicolumn{3}{c}{$T_{Selection}$} & \multicolumn{3}{|c}{$T_{Exclusion}$}\\ \cmidrule(l){4-9}
%	Factor set &$d$     &       $K$&	XS& UV& $\frac{XS}{\sqrt{UV}}$  & XS       & UV      & $\frac{XS}{\sqrt{UV}}$      \\ \midrule
%DS PC      & 50  & 4 & 0.61 & 0.30 & 1.11 & 0.28 & 0.35 & 0.47 \\
%FF         & 5   & 5 & 0.28 & 0.29 & 0.51 & 0.43 & 0.35 & 0.72 \\\midrule
%HL         & 121 & 5 & 0.79 & 0.30 & 1.46 & 1.14 & 0.35 & 1.93 \\
%HL PC      & 50  & 7 & 0.17 & 0.29 & 0.31 & 0.44 & 0.34 & 0.76 \\
%HL + HL PC & 171 & 5 & 1.02 & 0.30 & 1.87 & 1.28 & 0.35 & 2.16 \\\midrule
%HL-$\omega_1$      & 121 & 5 & 1.02 & 0.30 & 1.87 & 1.28 & 0.35 & 2.16 \\
%HL-$\omega_4$      & 121 & 5 & 1.02 & 0.30 & 1.87 & 1.28 & 0.35 & 2.16 \\
%HL-$\omega_5$      & 121 & 6 & 0.89 & 0.29 & 1.65 & 1.20 & 0.34 & 2.05 \\
%HL-$\omega_8$      & 121 & 8 & 0.25 & 0.28 & 0.48 & 0.44 & 0.33 & 0.76 \\ \bottomrule\bottomrule
%	\end{tabular}
%	\bnotetab{This table compares root-mean-squared $\hat{\alpha}$ (XS) and unexplained variance (UV) defined in (\ref{49}) when using different factor sets. The response set, table layout and inferential setting are the same as in Table \ref{zscoreDS}.}
%\end{table}
%\begin{table}[H]
%	\centering
%	\tcaptab{Cross-Sectional Mini-Regression Performances}\label{r2-statDS}
%	\begin{tabular}{ccc|ccc|ccc}
%	\toprule\toprule
%	 &  &   &\multicolumn{3}{c}{$T_{Selection}$} & \multicolumn{3}{|c}{$T_{Exclusion}$}\\ \cmidrule(l){4-9}
%	Factor set &$d$ &$K$    &      $R^2$    & $LL$       & RMSE      & $R^2$         & $LL$  & RMSE   \\ \midrule
%	DS PC      & 50  & 4 & 0.62 & 75  & 0.18  & 0.34 & 138 & 0.14 \\
%FF         & 5   & 5 & 0.54 & 52  & 0.20 & 0.37 & 143 & 0.13 \\\midrule
%HL         & 121 & 5 & 0.43 & 26  & 0.22 & 0.14 & 105 & 0.16 \\
%HL PC      & 50  & 7 & 0.73 & 116 & 0.15 & 0.48 & 166 & 0.12 \\
%HL + HL PC & 171 & 5 & 0.43 & 26  & 0.22 & 0.14 & 105 & 0.16 \\\midrule
%HL-$\omega_1$      & 121 & 5 & 0.43 & 26  & 0.22 & 0.14 & 105 & 0.16 \\
%HL-$\omega_4$      & 121 & 5 & 0.43 & 26  & 0.22 & 0.14 & 105 & 0.16 \\
%HL-$\omega_5$      & 121 & 6 & 0.44 & 28  & 0.22 & 0.15 & 106 & 0.16 \\
%HL-$\omega_8$      & 121 & 8 & 0.58 & 64  & 0.19 & 0.38 & 144 & 0.13\\ \bottomrule\bottomrule
%	\end{tabular}
%	\bnotetab{This table compares cross-sectional $R^2$ (no need for adjusted $R^2$ since the mini-regressions are univariate), log-likelihood $LL$ (assuming normal residuals of \ref{eq:mini_reg}) and RMSE of a fitted-returns on actual returns simple regression, when using different factor sets. The response set, table layout and inferential setting are the same as in Table \ref{zscoreDS}. }
%\end{table}
%
%
%
%%\theendnotes





\singlespacing
\bibliographystyle{econometrica}
{\small
\bibliography{main}
}
\onehalfspacing



%\newpage
%\bibliographystyle{abbrvnat}
%{\small
%\setlength{\bibsep}{4pt}
%	\bibliography{reference}}
%	
%	


\includepdf[pages={1-20}]{Internet_Appendix_Inference_for_High-Dimensional_Panel_Data_with_Many_Covariates.pdf}


\end{document}
