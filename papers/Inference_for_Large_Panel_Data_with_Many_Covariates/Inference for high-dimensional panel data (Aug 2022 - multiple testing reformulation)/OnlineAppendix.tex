\documentclass[11pt]{article}
\usepackage{amsmath}
\usepackage{graphicx,psfrag,epsf}
\usepackage{enumerate}
\usepackage{natbib}
\usepackage{multirow}
\usepackage{url} % not crucial - just used below for the URL 
\usepackage{endnotes}


%\pdfminorversion=4
% NOTE: To produce blinded version, replace "0" with "1" below.
\newcommand{\blind}{0}

% DON'T change margins - should be 1 inch all around.
\addtolength{\oddsidemargin}{-.5in}%
\addtolength{\evensidemargin}{-.5in}%
\addtolength{\textwidth}{1in}%
\addtolength{\textheight}{1.3in}%
\addtolength{\topmargin}{-.8in}%

\usepackage{bbm}

\usepackage{graphicx}
\usepackage{amsfonts, amssymb, amsmath,bm}
\def\cmt#1{{\textcolor{red}{(#1)}}}
\newcommand{\cp}{^c}
\newcommand{\q}{\text{\quad}}
\newcommand{\wh}{\text{where }}
\newcommand{\df}{\mathrm{d}}
\newcommand{\dm}{\mathcal{D}}
\newcommand{\fc}[1]{\sigma^2(#1)}
\newcommand{\id}[1]{\mathbb{I}\{#1\}}
\newcommand{\prob}{\mathbb{P}}
\newcommand{\pa}{\partial}
\newcommand{\II}{\mathbf{I}}
\newcommand{\oo}{\bm{1}}
\newcommand{\LIM}{\lim\limits_{n\to\infty}}
\newcommand{\NN}{\mathbb{N}}
\newcommand{\PP}{\mathbb{P}}
\newcommand{\Pp}{\bm{P}}
\newcommand{\Aa}{\{a_n\}}
\newcommand{\Bb}{\{b_n\}}
\newcommand{\Ca}{\mathcal{C}}
\newcommand{\Da}{\mathcal{D}}
\newcommand{\CL}{\textbf{Claim: }}
\newcommand{\PF}{\textbf{Proof: }}
\newcommand{\QUE}{\textbf{Question: }}
\newcommand{\SOL}{\textbf{Solution: }}
\newcommand{\M}{\mu}
\newcommand{\sgn}{\textrm{sign}}
\newcommand{\SE}{\sum_{n=1}^{\infty}}
\newcommand{\PS}{\sum_{k=1}^{n}}
\newcommand{\FF}{\mathbb{F}}
\newcommand{\Ball}{B_\epsilon(x)}
\newcommand{\CC}{\mathbb{C}}
\newcommand{\BD}{\partial X}
\newcommand{\rr}{\mathbf{r}}
\newcommand{\QQ}{\mathbb{Q}}
\newcommand{\RR}{\mathbb{R}}
\newcommand{\XX}{\mathbf{X}}
\newcommand{\xx}{\mathbf{x}}
\newcommand{\IN}{^{(i)}}
\newcommand{\TN}{^{(t)}}
\newcommand{\JN}{^{(j)}}
\newcommand{\YY}{\mathbf{Y}}
\newcommand{\yy}{\mathbf{y}}
\newcommand{\YT}{\tilde{\mathbf{Y}}}
\newcommand{\ZZ}{\mathbb{Z}}
\newcommand{\WW}{\mathbf{W}}
\newcommand{\LL}{\mathcal{L}}
\newcommand{\Hh}{\mathbf{h}}
\newcommand{\Gg}{\mathbf{g}}
\newcommand{\ee}{\mathbf{e}}
\newcommand{\Uu}{\mathbf{u}}
\newcommand{\Yy}{\mathbf{y}}
\newcommand{\Mo}{\bm{\mu_1}}
\newcommand{\Mt}{\bm{\mu_2}}
\newcommand{\UM}{\overrightarrow{\rightarrow}}
\newcommand{\st}{\text{s.t.}}
\newcommand{\tr}{\mathrm{tr}}
% Probability statement
\newcommand{\CD}{\stackrel{D}{\rightarrow}}
\newcommand{\CP}{\stackrel{p}{\rightarrow}}
\newcommand{\AS}{\stackrel{a.s.}{\rightarrow}}
\newcommand{\iid}{\stackrel{iid}{\sim}}
\newcommand{\io}{\text{ i.o.}}
\newcommand{\XI}{X_\infty}
\newcommand{\EE}{\mathbb{E}}
\newcommand{\FC}{\mathcal{F}}
\newcommand{\BC}{\mathcal{B}}
\newcommand{\NL}{\\[.4cm]}
\newcommand{\floor}[1]{\left\lfloor #1 \right\rfloor}
\newcommand{\indep}{\rotatebox[origin=c]{90}{$\models$}}
\newcommand{\notindep}{\not\!\perp\!\!\!\perp }

\newcommand{\FD}{\text{FDR}}
\newcommand{\PO}{\text{Power}}
\newcommand{\Un}{\text{Unif[0,1]}}
\newcommand{\tT}{\mathbf{t}}
\newcommand{\erf}{\text{erf}}
\newcommand{\erfc}{\text{erfc}}
\newcommand{\erfinv}{\text{erf}^{-1}}
\newcommand{\erfcinv}{\text{erfc}^{-1}}
\usepackage{rotating} % <-- HERE

\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

\usepackage{url}
\usepackage{xr-hyper,hyperref}
\makeatletter
\newcommand*{\addFileDependency}[1]{% argument=file name and extension
	\typeout{(#1)}
	\@addtofilelist{#1}
	\IfFileExists{#1}{}{\typeout{No file #1.}}
}
\makeatother

\newcommand*{\myexternaldocument}[1]{%
	\externaldocument{#1}%
	\addFileDependency{#1.tex}%
	\addFileDependency{#1.aux}%
}
\myexternaldocument{main}    % Not main.tex

\myexternaldocument{ch123}    % Not main.tex
\myexternaldocument{ch456}    % Not main.tex

\usepackage{fancyhdr}
\usepackage{multicol}
\usepackage{courier} % For inline code
\usepackage{float}
\usepackage{subcaption}

\usepackage{titlesec} % Allow customized subsections with no new line
\usepackage{tikz}

\usetikzlibrary{automata,arrows,positioning,calc}
%\titleformat{\subsection}[runin]{}{}{}{}[]
\usepackage{listings}

\usepackage[linesnumbered,ruled]{algorithm2e}
\usepackage{tablefootnote}
\usepackage[english]{babel}
\newtheorem{Assumption}{IA.Assumption}

\newtheorem{theorem}{IA.Theorem}
\begin{document}
	
	%\bibliographystyle{natbib}
	
	\def\spacingset#1{\renewcommand{\baselinestretch}%
		{#1}\small\normalsize} \spacingset{1}
	
	
	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	
	\if0\blind
	{
		\title{\bf Internet Appendix to \\ \textit{Inference for High-Dimensional Panel Data with Many Covariates}}
		\author{Markus Pelger\thanks{Department of MS\&E, Stanford University. Email: mpelger@stanford.edu. }\hspace{.6cm}
			Jiacheng Zou\thanks{Department of MS\&E, Stanford University. Email: jiachengzou@stanford.edu. }\\
		}
		\date{December 31, 2022}
		\maketitle
	} \fi
	
	\if1\blind
	{
		\bigskip
		\bigskip
		\bigskip
		\begin{center}
			{\LARGE\bf Online Appendix of Conditional inference for\\high-dimensional panel data}
		\end{center}
		\author{Markus Pelger\hspace{.2cm}
			Department of MS\&E, Stanford University}
		and 
		{Jiacheng Zou\hspace{.2cm}
			Department of MS\&E, Stanford University}
		\maketitle
		\medskip
	} \fi
	
	\tableofcontents
	
	\newpage
	
	
	%\begin{center}
	%    \large \textit{Guide to this Online Appendix}
	%\end{center}
	\section*{Overview of this Internet Appendix}
	
	
	The Internet Appendix collects the proofs and additional details for the statements of the main text. It is separated into two parts. 
	\begin{enumerate}
		\item The first part collects the proofs for the multiple testing adjustment for data-driven hypotheses. It contains the proof of Corollary \ref{col3}, Theorem \ref{thm4} and the bounds of the cohesion coefficient.
		\item The second part collects all the proofs for the post-selection inference with Weighted-LASSO. It starts with an illustrative discussion of how adding factor priors changes inference, when the covariates are orthogonal. We then prove the supportive lemmas and Theorems A.1 to A.3 of the main appendix. We conclude with suggestions for best practices in implementing the method.
	\end{enumerate}
	
	
	
	\section{Multiple Testing Adjustment for Data-Driven Hypothesis}
	%
	%\subsection{Theorem \ref{thm_MT}}
	%
	%First we write the FWER definition conditional on $\mathcal{M}$:
	%\begin{equation}
	%	\begin{split}
		%		&\PP(V\geq 1|\mathcal{M})\\
		%		=	&\PP(\sum_{j\in H_M} V_j\geq 1|\bigcap_{n\in \mathcal{K}_j}\mathcal{M}^{(n)})\\
		%  = & \PP(\bigcup_{j\in H_M}\{ \bigcup_{n\in\mathcal{K}_i}\{p_{j}^{(n)}\leq \rho\frac{\gamma}{ N_j}|\bigcap_{n\in \mathcal{K}_j}\mathcal{M}^{(n)}\}\})\\
		%  \leq & \sum_{j\in H_M}\sum_{n\in\mathcal{K}_j}\PP(p_{j}^{(n)}\leq \rho\frac{\gamma}{ N_j}|\bigcap_{n\in \mathcal{K}_j}\mathcal{M}^{(n)}\}\})\\
		%  	\end{split}
	%\end{equation}
	%In first 3 lines, we use the definition of $\mathcal{M}$, $V$ and our test's design. From the 3nd line to the 4th line, we use the Boole's inequality.
	%
	%When $H_D$ is true, under the Assumption 1, since $ \rho\frac{\gamma}{N_j}\geq 0$, we have $\PP(p_{j}^{(n)}\leq \rho\frac{\gamma}{N_j})\leq  \rho\frac{\gamma}{N_j} $. Plugging it back gives us:
	%\begin{equation}
	%	\begin{split}
		%		\PP(V\geq 1|\mathcal{M})  \leq &  \sum_{j\in H_M}\sum_{n\in\mathcal{K}_j}\rho\frac{\gamma}{ N_j}\\
		%  = & \gamma\cdot \rho\cdot \sum_{j\in H_M}\frac{1}{N_j}\sum_{n\in\mathcal{K}_j}1\\
		%  = & \gamma\cdot \rho\cdot \sum_{j\in H_M}\frac{|\mathcal{K}_j|}{N_j}
		%  =\gamma
		%  	\end{split}
	%\end{equation}
	%The last line is invoking the definition of $\rho$.
	
	
	
	
	
	\subsection{Proof of Corollary \ref{col3}}
	
	\textbf{Proof:} The case of $\forall K:\gamma^*(K)>\gamma$ leads to a unique $K^*(\gamma)$ as we define it to be $J$.
	
	In the case where $\exists K:\gamma^*(K)\leq \gamma$, we show that the mapping $g:\gamma\mapsto K^*(\gamma)$ is a monotonically increasing function. The argmax of this mapping is not empty because $\exists K:\gamma^*(K)\leq \gamma$. Next, we show that it returns only one value, as $g$ is monotonically increasing in $\gamma$. Consider $\gamma\leq \gamma'$, then the $K^*(\gamma')$ satisfies
	\begin{equation}
		K^*(\gamma')=	\argmax_{0\leq K\leq J}\{\gamma^*(K)\leq\gamma'\}
		\geq 	\argmax_{0\leq K\leq J}\{\gamma^*(K)\leq\gamma\}=K^*(\gamma).
	\end{equation}
	This concludes our argument.
	
	
	\subsection{Proof of Theorem \ref{thm4}}
	
	\textbf{Proof:} The following proof draws parallels to Theorem 3 of \cite{rssb.12122}, with the key distinction that our approximated R\'enyi representations are conservative approximations rather than exact R\'enyi representations. 
	
	First, we define some notations that further break down the quantities introduced in the procedure of the step-down rejection of nested ordered family $H_{N}$ in the main text:
	\begin{itemize}
		\item ${N}^{\text{order}}_{k},{q}^{\text{order}}_k,{Z}^{\text{order}}_k$ are denoted for short as $\check{N}_k,\check{q}_k,\check{Z}_k$.
		\item The transformed p-values are denoted as $\check{Y}_k^{(n)}=-\ln(p^{(n)}_k)$.
		\item The active unit set for $k$th nested variable $\mathcal{K}_k$ is the collection of units that $k$th variable is active in. The test-related active unit set for $k$th nested variable is $\check{\mathcal{K}}_k=
		\bigcup_{k'\in\{k,k+1,...,J\}} \mathcal{K}_{k'}$, and the simultaneity count for $k$th nested variable is $ \check{N}_k=\sum_{j\in \check{\mathcal{K}}_k}|M_j| $.
		\item The approximated R\'enyi representation equals $\check{Z}_k=\sum_{j=k}^J\sum_{n\in\mathcal{K}_i}
		\frac{ \check{Y}_k^{(n)} }{\check{N}_1-\check{N}_{i+1}\oo\{i\neq J\}}$ and its transformed reversed order statistics is $\check{q}_k=\exp(-\check{Z}_k)$.
	\end{itemize}
	
	We show the FWER control for two cases:
	\subsubsection*{Case I: Global null is true $(s=0)$}\label{thm4p1}
	
	We start with the case when the global null is true, i.e. $\bm{H}_{N,0}$ is true and $s$=0. Then by Assumption 2 in the main text, the random variables $\check{Y}_k^{(n)}$ independently and identically follow the standard exponential distribution.
	
	The nested model implies that $k=1$ should be active for more units than $k=2$, and similar for higher order values. Leveraging this hierarchical structure, we construct a triangle of our $\check{Y}_k^{(n)}$. More specifically, we put the same $k$th value underneath the corresponding value of the other rows, but allow for arbitrary indexing schemes within each row:
	\begin{equation}
		\begin{split}
			\underbrace{\check{Y}_1^{\mathcal{K}_1(1)},\check{Y}_1^{\mathcal{K}_1(2)},...\q...\q...\q,\check{Y}_1^{\mathcal{K}_1(\check{N}_{1}-\check{N}_2)}}_{\mathcal{K}_1(1),...,\mathcal{K}_1(\check{N}_{1}-\check{N}_2)\in\mathcal{K}_1};\\
			\underbrace{\check{Y}_2^{\mathcal{K}_2(1)},\check{Y}_2^{\mathcal{K}_2(2)},...\q...\q,\check{Y}_2^{\mathcal{K}_J(\check{N}_{2}-\check{N}_3)}}_{\mathcal{K}_2(1),...,\mathcal{K}_2(\check{N}_{2}-\check{N}_3)\in\mathcal{K}_2};\\
			&\vdots\\
			\underbrace{\check{Y}_J^{\mathcal{K}_J(1)},\check{Y}_J^{\mathcal{K}_J(2)},...,\check{Y}_J^{(\mathcal{K}_J(\check{N}_J))}}_{\mathcal{K}_J(1),...,\mathcal{K}_J(\check{N}_J)\in\mathcal{K}_J}
		\end{split}
	\end{equation}
	
	Our procedure does not depend on the triangle shape of this construction. This means that we do not limit ourselves to the case where sequences within each row of this triangle are longer on the top than those on the bottom.
	
	If we count from the bottom row to top row and from right to left, then there are $\check{N}_J$ elements when we arrive at the left-most element of the last row $\check{Y}_J^{\mathcal{K}_J(1)}$, and there are $\check{N}_1$ elements when arrive at the left-most element of the top row $\check{Y}_1^{\mathcal{K}_1(1)}$. 
	
	By Assumption 2, under the null $\bm{H}_{N}$ we can invoke \cite{Rnyi1953OnTT} to show that
	\begin{equation}
		\begin{split}
			\bigg(
			\frac{\check{Y}_1^{\mathcal{K}_1(1)}}{1},...,
			\frac{\check{Y}_1^{\mathcal{K}_1(\check{N}_{1}-\check{N}_2)}}{\check{N}_{1}-\check{N}_2}
			,
			\frac{\check{Y}_2^{\mathcal{K}_2(1)}}{\check{N}_{1}-\check{N}_2+1}
			,...,
			\frac{\check{Y}_2^{\mathcal{K}_J(\check{N}_{2}-\check{N}_3)}}{\check{N}_{1}-\check{N}_3}
			,...,
			\frac{\check{Y}_J^{\mathcal{K}_J(1)}}{\check{N}_1-\check{N}_J+1}
			,,...,
			\frac{\check{Y}_J^{(\mathcal{K}_J(\check{N}_J))}}{\check{N}_1}
			\bigg)\\
			\stackrel{d}{=}
			\bigg(
			E_{\check{N}_1,\check{N}_1},...,E_{1,\check{N}_1}
			\bigg).
		\end{split}
	\end{equation}
	where $E_{j,m}$ is the $j$th-order statistic of $m$ standard exponential random variables. As a result, $\tilde{\check{Z}}_k$ defined below is an exact R\'enyi representation:
	\begin{equation}\label{38}
		\tilde{\check{Z}}_k=\sum_{i=k}^J(
		\frac{\check{Y}_i^{\mathcal{K}_i(1)}}{\check{N}_1-\check{N}_i+1}+
		...+
		\frac{\check{Y}_i^{(\mathcal{K}_i(\check{N}_i-\check{N}_{i+1}))}}{\check{N}_1-\check{N}_{i+1}\oo\{i\neq J\}}
		).
	\end{equation}
	Let $\tilde{\check{q}}_k=\exp(-\tilde{\check{Z}}_k)$. Our Assumption 2 in the main text satisfies the independent $p$-value assumption in \cite{2336545}, so the following rejection rule recovers the method of \cite{2336545} that has FWER control at level $\gamma$:
	\begin{equation}\label{renyi_oned}
		\tilde{\hat{k}}=\max\{k:    \tilde{\check{q}}_k\leq\frac{\gamma k}{J}\}.
	\end{equation}
	Our proof is based on the following steps:
	\begin{enumerate}
		\item [Step 1:] $\frac{ k}{J}\geq\frac{\check{N}_k}{JN}$ for each $k$.\NL
		This holds because some of the LASSO coefficients might not be active. By definition of the $\check{N}_k$'s, we have
		\begin{equation}
			\frac{k}{J}=\frac{kJ}{JN}\geq\frac{\sum_{i=k}^J|M_i|}{JN}=\frac{\check{N}_k}{JN} .
		\end{equation}
		It is worth pointing out that if $|M_j|$ is decreasing in $j$, we can further show that $\frac{k}{J}\geq \frac{\check{N}_k}{\check{N}_1} $. This can further increase the power, but we do not want to restrict ourselves to cases where this monotonicity of LASSO active count holds. Our setup enables us to accommodate the possibility that the data does not present a strict nested structure of covariates.% being less as less active in the cross-section.
		\item [Step 2:] $\tilde{\check{Z}}_k\geq {\check{Z}}_k$ for each $k$.\NL
		By construction, our ${\check{Z}}_k$ only differs from $\tilde{\check{Z}}_k$ of (\ref{38}) in terms of the denominator in the sum, that is, each denominator in the sum is $\check{N}_1-\check{N}_{i+1}$, which is the largest possible entry of $\{\check{N}_1-\check{N}_i+1,...,\check{N}_1-\check{N}_{i+1}\}$.
		\item[Step 3:] $\tilde{\check{q}}_k$ and ${\check{q}}_k$ are both monotonically increasing in $k$.\NL
		Since $p^{(n)}_k$ is a $p$-value, it has support on $[0,1]$, so $\check{Y}^{(n)}_k=-\ln (p^{(n)}_k)\geq0$, and the monotonicity follows. 
		\item[Step 4:] For sequences $\{a_k\}$ and $\{b_k\}$, it holds that $\max\{k: a_k\leq c\}\geq \max\{k: b_k\leq c\}\geq\max\{k: b_k\leq d\} $, if $a_k\leq b_k$  for each $k$, both sequences are monotonically increasing, and $c\geq d$.\NL
		The first leg holds as $b_k\leq c$ implies $a_k\leq c$, but not vice versa. The second leg holds because $b_k\leq d$ implies $b_k\leq c$, but not vice versa.
	\end{enumerate}
	Step 2 and the fact that $x\mapsto \exp(-x)$ is monotonically decreasing yield that $\tilde{\check{q}}_k\leq {\check{q}}_k$ for each $k$. Then, we use the first leg of Step 4 to obtain the following:
	\begin{equation}\label{40}
		\tilde{\hat{k}}\geq\max\{k:    {\check{q}}_k\leq\frac{\gamma k}{J}\}.
	\end{equation}
	Combining Step 1, Step 3 and second leg of Step 4, we have:
	\begin{equation}\label{41}
		\max\{k:    {\check{q}}_k\leq\frac{\gamma k}{J}\}\geq \max\{k:    {\check{q}}_k\leq\frac{\gamma\check{N}_k}{JN}\}=\hat{k}.
	\end{equation}
	Combining (\ref{renyi_oned}), (\ref{40}), and (\ref{41}), we derive that the number of discoveries $\hat{k}$ in our rejection rule satisfies the following chain of inequalities:
	\begin{equation}
		\underbrace{\tilde{\hat{k}}=\max\{k:    \tilde{\check{q}}_k\leq\frac{\gamma k}{J}\}}_{\text{\cite{2336545}}}\geq\max\{k:    {\check{q}}_k\leq\frac{\gamma k}{J}\}\geq \underbrace{\max\{k:    {\check{q}}_k\leq\frac{\gamma\check{N}_k}{JN}\}=\hat{k}}_{\text{Our rejection}}.
	\end{equation}
	In the case of the global null $(s=0)$, any rejection is a false discovery. Since \cite{2336545}' method has false discovery FWER control of $\gamma$, our procedure has fewer rejections and has FWER control of $\gamma$ as well. This concludes the proof for Case I.
	
	
	\subsubsection*{Case II: Global null is false $(s\geq 1)$}\label{thm4p2}
	
	For the case when the global null is not true, there are two possibilities:
	\begin{enumerate}
		\item The first possibility is that our rejection is $\hat{k}\leq s$. Due to the nested nature of $\bm{H}_N$, this guarantees that $V=0$, that is, there is no false discovery, so the FWER control holds.
		\item The other possibility is that we have $\hat{k}=k>s$. Since our procedure is a step-down procedure, $\check{q}_k$ depends on $p^{(n)}_{k'}$ only for the $k'\geq k$ and on the $\check{N}_{k'}$'s, which are included in the LASSO selection event that we are conditioning on. So, there is no difference for the random event $\{\check{q}_k\leq\frac{\gamma\check{N}_k}{JN}\}$ between the $s\neq 0$ and $s=0$ ground truth. In other words, we can write:
		\begin{equation}\label{42}
			\PP_{H_D}(V\geq 1|s>0,\hat{k}>s)=\PP_{H_D}(V\geq 1|s=0,\hat{k}>s).
		\end{equation}
		The right-hand side of (\ref{42}) is smaller than $\gamma$ as shown in Case I.
	\end{enumerate}
	Combining Cases I and II, we conclude the proof.
	
	\iffalse
	For $X\sim \mathcal{N}(0,1)$, given $V\sim\chi^2_d$ and $V\indep X$, we would have $\tau=X/\sqrt{V/d}\sim t_{d}$ per \cite[Chapter~5.2]{lehmann2005testing}. Now consider $Z:=X\oo\{X\in [a/\|\eta \|\sigma,b/\|\eta \|\sigma]\}$ and let us investigate the new quotient random variable
	\begin{equation}
		\tilde{\tau}=\frac{Z}{\sqrt{V/ d}}=
		\frac{X}{\sqrt{V/d}}
		\oo\{X\in [a/\|\eta \|\sigma,b/\|\eta \|\sigma]\}
	\end{equation}
	Thus $\tilde{\tau}\sim \mathcal{TT}_{d;[a/\|\eta \|\sigma,b/\|\eta \|\sigma]}$ and it fits our construction of $\bar{\beta}_i/\hat{\sigma}$:
	\begin{equation}
		\frac{\bar{\beta}_i}{
			\|\eta \|\hat{\sigma}}\stackrel{D}{=}\frac{\bar{\beta}_i}{
			\|\eta \|\sigma\sqrt{V/d}}\stackrel{D}{=}\frac{Z}{\sqrt{V/d}}
	\end{equation}
	\fi
	
	
	
	\subsection{Proof of Bounds for Cohesion Coefficient $\rho$}\label{app-sec:proof_bonf_global}
	
	Recall the definition of the cohesion coefficient $\rho =\left(\sum_{1\leq j\leq J:\mathcal{K}_j\neq\emptyset}\frac{|\mathcal{K}_j|}{N_j}   \right)^{-1}$. We want to show that $J^{-1}\leq \rho\leq 1$.
	
	\textbf{Lower bound: }The inverse of the summands in the definition of $\rho$ can be written as
	\begin{equation}\label{75.0}
		\frac{N_j}{|\mathcal{K}_j|}=\frac{|\mathcal{K}_j|+\sum_{j'\textrm{ co-active with }j}|\mathcal{K}_{j'}|}{|\mathcal{K}_j|}
		=1+\frac{\sum_{j'\textrm{ co-active with }j}|\mathcal{K}_{j'}|}{|\mathcal{K}_j|}.
	\end{equation}
	If there are no co-active covariates $j'$ with covariate $j$, then we have $\frac{\sum_{j'\textrm{ co-active with }j}|\mathcal{K}_{j'}|}{|\mathcal{K}_j|}=0$, which implies (\ref{75.0})$\geq 1$. Thus, the sum is bounded by
	\begin{equation}\label{eq:11}
		\begin{split}
			\sum_{1\leq j\leq J:\mathcal{K}_j\neq\emptyset}\frac{|\mathcal{K}_j|}{N_j}\leq 
			\sum_{1\leq j\leq J}1    \iff
			\rho^{-1}\leq 
			J \iff
			\rho\geq  J^{-1}.
		\end{split}
	\end{equation}
	
	\textbf{Upper bound: }For fixed $H_M$, we now show by induction that $\rho\leq 1$ or $\rho^{-1}\geq 1$. 
	First, we re-arrange the indices $j\in H_M$ based on $|\mathcal{K}_j|$ such that
	$$
	|\mathcal{K}_1|\geq |\mathcal{K}_2|\geq \cdots |\mathcal{K}_{|H_M|}| .
	$$
	Let $H_t$ denote the family of hypotheses that is constrained to only include $\{ j\in H_M: j\leq t\}$. We write $\rho^{(t)}$ as 
	\begin{equation}
		(\rho^{(t)})^{-1}:=\sum_{j\leq t}\frac{|\mathcal{K}_j|}{N_j^{(t)}}.
	\end{equation}
	%where we point out that as induction marches on, the numerator in summand $\frac{|\mathcal{K}_j|}{N_j^{(t)}}$. 
	
	We want to show that $(\rho^{|H_M|})^{-1}\geq 1$. It is sufficient to use induction for $t=1,2,...,|H_M|$. In other words, we show that $\rho^{-1}\geq 1$ by first looking at the family of hypotheses truncated up to the covariate with most active units, then the second most active units, etc.\\
	\begin{enumerate}
		\item[(i)] \textbf{Base case:} When $t=1$, clearly $|\mathcal{K}_1|=N_1$ so $\sum_{j\leq 1}\frac{|\mathcal{K}_j|}{N_j}= 1$.
		\item[(ii)] \textbf{Inductive case:} Suppose the claim is true for $t>1$. This implies
		\begin{equation}\label{78.1}
			(\rho^{(t)})^{-1}=\sum_{j\leq t}\frac{|\mathcal{K}_j|}{N_j^{(t)}}\geq 1.
		\end{equation}
		We study the next induction step:
		\begin{equation}\label{eqn:indiction}
			\begin{split}
				(\rho^{(t+1)})^{-1}
				&=\sum_{j\leq t+1}\frac{|\mathcal{K}_j|}{N_j^{(t+1)}}\\
				&=\sum_{j\leq t}\frac{|\mathcal{K}_j|}{N_j^{(t+1)}}+
				\frac{|\mathcal{K}_{t+1}|}{N_{t+1}^{(t+1)}}.
			\end{split}    
		\end{equation}
		Note that $1-\sum_{j\leq t}\frac{|\mathcal{K}_j|}{N_j^{(t)}}\leq 0$ by equation (\ref{78.1}), and thus it holds that
		\begin{equation}
			\begin{split}
				(\rho^{(t+1)})^{-1}
				&\geq 1-\sum_{j\leq t}\frac{|\mathcal{K}_j|}{N_j^{(t)}}+\sum_{j\leq t}\frac{|\mathcal{K}_j|}{N_j^{(t+1)}}+
				\frac{|\mathcal{K}_{t+1}|}{N_{t+1}^{(t+1)}}\\
				&=1+\sum_{j\leq t}|\mathcal{K}_j|(\frac{1}{N_j^{(t+1)}}-\frac{1}{N_j^{(t)}})+
				\frac{|\mathcal{K}_{t+1}|}{N_{t+1}^{(t+1)}}\\
				&=1+\sum_{j\leq t}|\mathcal{K}_j|\frac{-(N_j^{(t+1)}-N_j^{(t)})}{N_j^{(t+1)}N_j^{(t)}}+
				\frac{|\mathcal{K}_{t+1}|}{N_{t+1}^{(t+1)}}.
			\end{split}    
		\end{equation}
		We define $x_j:=N_j^{(t+1)}-N_j^{(t)}$ for $j$th covariate, which represents how many new co-active counts are added with $(t+1)$-th covariates. This is $|\mathcal{K}_{t+1}|+1$ if $j$ is co-active with $(t+1)$-th covariate, and 0 otherwise. Next, we define
		\begin{equation}
			f_j(x_j):=\frac{-(N_j^{(t+1)}-N_j^{(t)})}{N_j^{(t+1)}N_j^{(t)}}
			=\frac{-x_j}{(N_j^{(t)}+x_j)N_{j}^{t}}.
		\end{equation}
		It is straightforward to show that $\forall f_j$ we have $\frac{\partial f_j}{\partial x_j}\leq 0$, i.e. the sum $\sum_{j\leq t}|\mathcal{K}_j|f_j(x_j)$ is monotonically decreasing in $x_j$.
		
		The second part of equation \ref{eqn:indiction} can be written as
		
		\begin{equation}
			g(x)=\frac{|\mathcal{K}_{t+1}|}{N_{t+1}^{(t+1)}}
			=\frac{|\mathcal{K}_{t+1}|}{|\mathcal{K}_{t+1}|+\sum_{j\textrm{ co-active with }(t+1)}|\mathcal{K}_{j}|}.
		\end{equation}
		For non-zero values of $x$, the denominator of $g(x)$ is increasing, and hence $g(x)$ is decreasing in $x$.
		Combing these two arguments yields that $(\rho^{(t+1)})^{-1}$ is decreasing in $x$, and thus the inequality has a valid lower bound if we set $x=0$, which gives us $    (\rho^{(t+1)})^{-1}\geq 1$.
	\end{enumerate}
	
	Combining (i) and (ii), we conclude that $\rho^{-1}\geq 1$, and together with equation (\ref{eq:11}) we obtain the bounds
	\begin{equation}
		\rho\in[J^{-1},1].
	\end{equation}
	
	
	\section{Post-selection Inference with Weighted-LASSO}
	
	
	
	
	\subsection{Adding Priors for Post-Selections Inference}\label{sec2}
	
	We discuss how to make valid inference for $\hat{\beta}^{(n)}(\lambda,\omega) \in \mathbbm R^J$ given $\mathcal{M}^{(n)}$ when $n$ is fixed, which allows us to deal with high-dimensional covariate dimensions (large $J$). The multiple testing adjustment extends the analysis to a large number of cross-sectional units $N$. %It is well-known that LASSO is a biased estimator, since it has an $\ell_1$ regularization, and hence requires a bias correction.
	As we only consider the conditional inference for a single cross-sectional unit, we drop the superscript $n$, that indexes units, and refer to the response variable as $Y\in\RR^T$, the active set as $M$, the LASSO selection event as $\mathcal{M}$, and the active sub-matrix of $X\in\RR^{T\times d}$ as $X_M$. The weights $\omega$ are fixed. \footnote{The PoSI literature, for examples \cite{markovic2018unifying}, has also studied the case where $\lambda$ is determined via cross-validation (CV). The resulting theory depends on the setup for CV and is asymptotic in the pre-selection data-generating process and a user-augmented randomizer. Its implementation requires an MCMC approach. The goal is different from our paper, as we provide easy-to-understand and generic post-selection inference $p$-values that focus on asymptotics for unknown variances.} We use $\hat{\beta}\in\RR^{|M|}$ to denote the LASSO fit in this section. Thus, the hypothesis can be written as
	\begin{equation}\label{10}
		H_{0,j}:\beta_j=0|\mathcal{M}.
	\end{equation}
	If we can calculate the valid $p$-value for a consistent statistic of $\beta_i$ conditioned on $\mathcal{M}$, we are able to reject $H_{0,i}$ of (\ref{10}) with the desired Type I error, which completes inference problem for a single unit.
	
	
	
	%Since we only consider ourselves with single unit's conditional inference in this section, we drop the superscript $j$ indexing of unit and refer to the response variable as $Y\in\RR^T$, the active set as $M$, the LASSO selection event as $\mathcal{M}$, and the active sub-matrix of $X\in\RR^{T\times d}$ as $X_M$. $\omega$ is also fixed once exogenously specified, and we do not consider parameter tuning for $\lambda$ in this study\footnote{There has been studies in the PoSI literature such as \cite{markovic2018unifying} that consider s$\lambda$ being searched via CV tuning. Its theory depends on the setup for CV and is asymptotic in pre-selection data-generating process and a user-augmented randomizer; its implementation requires an MCMC. In this section of our paper, we seek to produce easy-to-understand and generic post-selection inference $p$-values that focus on asymptotics for unknown variances in the pre-selection data, so we leave the extension from fixed $\lambda$ to CV-searched $\lambda$ for future discussions.},
	
	
	
	\subsubsection*{Illustrative case: orthogonal design}\label{sec:2.1}
	
	To provide some intuition, we consider the following simplified case, where an orthogonal collection of features and i.i.d. errors with known variances are available. As expected for orthogonal covariates, this leads to a simple closed-form solution.
	
	\begin{Assumption}[Low dimensional truth]\label{asu1}
		The data satisfies $Y=X_S\beta_S+\epsilon$ where $|S|=O(1)$ is much smaller than $J$ or $T$.
	\end{Assumption}
	\begin{Assumption}[Orthogonal design]\label{asu2}
		The features satisfy $\frac{X^\top X}{T}=diag(\kappa_1^2,...,\kappa_J^2)$.
	\end{Assumption}
	\begin{Assumption}[Gaussian residual with simple known variance]\label{asu3}
		$\epsilon_t\iid \mathcal{N}(0,\sigma^2)$, where $\epsilon_t$ is $t$-th element of $\epsilon$.
	\end{Assumption}
	
	We use the term \textbf{Orthogonal Design} (OD henceforth) to refer to the case where IA.Assumptions \ref{asu1}, \ref{asu2} and \ref{asu3} are met. We can write out the weighted LASSO optimization as
	\begin{equation}
		\begin{split}
			\hat{\beta}=\argmin_\beta		\ell(Y,X,\lambda,\beta,\omega)
			&=\frac{1}{2T}Y^\top Y+\frac{1}{2T}\beta^\top X^\top X\beta-\frac{1}{T}Y^\top X\beta+\lambda\sum_{i=1}^df_j(\beta_j,\omega_j)\\
			&=\frac{1}{2T}Y^\top Y+
			\underbrace{\sum_{i=1}^{d}(\frac{1}{2}\kappa_j^2\beta_j^2-\frac{1}{T}Y^\top X_j\beta_j+\lambda\sum_{i=1}^d f_j(\beta_j,\omega_j)}_{\bar{\ell}(Y,X,\lambda,\beta|\omega)} .
			\\
		\end{split}
	\end{equation}
	Note that only $\bar{\ell}(Y,X,\lambda,\beta|\omega)$ matters within $\ell(Y,X,\lambda,\beta|\omega)$, and $\bar{\ell}$ decouples across $i$'s,
	so that we can take sub-differentials of $\bar{\ell}$ with respect to $\beta$:
	\begin{equation}\label{4.0}
		\pa (\beta\mapsto \bar{\ell}(\beta))=\begin{cases}
			\kappa_j^2\beta_j-\frac{1}{T}Y^\top X_j& \omega_j=\infty\\
			\kappa_j^2\beta_j-\frac{1}{T}Y^\top X_j+\lambda\sgn(\beta_j)/\omega_j & \beta_j\neq 0,\omega_j<\infty\\
			\kappa_j^2\beta_j-\frac{1}{T}Y^\top X_j+\lambda v_j/\omega_j& \beta_j= 0,\omega_j<\infty
		\end{cases}.
	\end{equation}
	where the slacks satisfy $v_j\in[-1,1]$. The solution to this minimization problem equals
	\begin{equation}\label{5}
		\hat{\beta}_j=\begin{cases}
			\frac{1}{\kappa_j^2}(\frac{1}{T}Y^\top X_j-\frac{c}{\omega_j})&\text{if } \frac{1}{T}Y^\top X_j-\frac{\lambda}{\omega_j}>0\\
			\frac{1}{\kappa_j^2}(\frac{1}{T}Y^\top X_j+\frac{c}{\omega_j})&\text{if } \frac{1}{T}Y^\top X_j+\frac{\lambda}{\omega_j}<0\\
			0&o.w.,
		\end{cases}
	\end{equation}
	where the case of $\omega_j=\infty$ is included in the first or the second case of (\ref{5}). Since $\sgn(\frac{1}{T}Y^\top X_j-\frac{\lambda}{\omega_j})=\sgn(\hat{\beta}_j)$, the sign of $\hat{\beta}_j$ tells us in which of the three cases in (\ref{5}) we are, given the exogenous values of  $X$ and $\omega$. In other words, the $\sgn(\hat{\beta})$ values and set of indices of active variable $M$ identify the LASSO selection event, because we can recover $\hat{\beta}$ once we know them. Additional knowledge of $v$ is not needed for identifying $\hat{\beta}$. Hence, we denote by $\mathcal{M}=(M,s)$ the LASSO event that we condition on. Moreover, by observing (\ref{5}), it is intuitive to consider an ``adjusted'' estimator defined as
	\begin{equation}\label{14}
		\forall i\in[d]:\q\bar{\beta}_j=\hat{\beta}_j+\sgn(\hat{\beta_j})\cdot \frac{\lambda}{\kappa_j^2\omega_j}.
	\end{equation}
	We refer to $\bar{\beta}$ as ``one-step estimator'' and explain the motivation for this name below. Note that $\hat{\beta}_j$ is active\footnote{We refer to a LASSO estimated coefficient as active if it is not 0.} if and only if $\bar{\beta}_j$ is active, and for active $\bar{\beta}_j$, it contains an additive component of $\frac{1}{T}\cdot\frac{\epsilon_j^\top X_j}{\kappa_j^2}$ as shown in the proof of IA.Theorem \ref{thm0}. The distribution of $\bar{\beta}_j$ is truncated Gaussian. On the one hand it contains a linear combination of Gaussian random variables, which lead to Gaussian distribution. On the other hand, this Gaussian distribution has to be truncated based on the condition in (\ref{5}). Following this thread, we obtain the marginal distribution for $\bar{\beta}_j$:
	\begin{theorem}[Truncated Gaussian of OD]\label{thm0}
		With Assumptions  \ref{asu1}, \ref{asu2}, \ref{asu3} and conditional on $\mathcal{M}$, we have the following distribution associated with $\bar{\beta}_j$:
		\begin{equation}
			\begin{cases}
				\bar{\beta}_j\sim\mathcal{TN}(\beta_j,\frac{1}{T}\frac{\sigma^2}{\kappa_j^2};[V^-_j,V^+_j]) & j\in M\\
				\bar{\beta}_j=0 & otherwise
			\end{cases}.
		\end{equation}	
		Where the truncation intervals are 
		\begin{equation}
			[V^-_j,V^+_j]=\begin{cases}
				(-\infty,-\frac{\lambda}{\kappa_j^2\omega_j}]& \text{ if }\sgn(\hat{\beta_j})=-1\\
				[\frac{\lambda}{\kappa_j^2\omega_j},+\infty)& \text{ if }\sgn(\hat{\beta_j})=1\\
			\end{cases}.
		\end{equation}	
	\end{theorem}
	Note that the $\sgn(\hat{\beta_j})$ in the truncation are known quantities, because we condition on the LASSO output. This again reflects the perspective of our conditional inference framework: We do not presume that a hypothesis is written without seeing the data. Only after we have seen the LASSO output we can form our hypothesis, and the hypothesis will be about covariates that are $j\in M$. This makes the so the statement of IA.Theorem \ref{thm0} conditional on $\mathcal{M}$. IA.Theorem \ref{thm0} shows that by conditioning on the KKT sub-gradient equations we induce a set of $[V_j^-,V_j^+]$ truncations on the support of the parameters that would otherwise have a Gaussian distribution over entire Euclidean space.\NL
	
	\textit{Remark: }The results of IA.Theorem \ref{thm0} also highlights many known properties of the LASSO estimator. 
	\begin{itemize}
		\item First, we observe that $\bar{\beta}_j$ is a shifted version of the LASSO $\hat{\beta}_j$, which is commonly referred to as ``de-biasing'' of LASSO as suggested by \cite{10.1214/17-AOS1630}. With this shifted one-step estimator $\bar{\beta}_j$, Online Appendix Theorem \ref{thm0} also implies that $\bar{\beta}_j\stackrel{p}{\to}\beta_j$ as $T\to\infty$, and hence establishes consistency.
		
		\item Second, we note that the proper LASSO penalty scalar $\lambda$ needs to scale proportional to $\kappa_j^2$ to maintain the same fit. If we have a scaled version of $X$ as $\tilde{X}=bX$ and $\omega$ fixed, it is necessary to use $\tilde{\lambda}= b^2 \lambda $ to maintain the same LASSO estimate.
		
		
		\item Lastly, we see that the LASSO estimator can miss weak signals when $\lambda$ is mis-specified. For instance, when the true coefficient equals $\beta_j=0.5\lambda/\kappa_j^2$, the truncation would not admit this covariate and its LASSO estimate would be $\hat{\beta}_j=0$. Thus, we recommend to use a moderately sized $\lambda$ for the purpose of selecting a parsimonious model of potentially weak covariates that the explain time series, even if the larger value of $\lambda$ might directly yield a sparse model. In other words, the LASSO time series regression step should serve as a pre-screening tool that conducts a first dimensional reduction, and the inferential framework based on $p$-values that we develop in this paper provides the tool to fine-tune the model.
	\end{itemize} 
	
	
	
	
	% 	\subsection{Proof for Theorem 1}\label{proof_thm0}
	% 	Let the probability space of our Orthogonal Design be $(\Omega,\mathcal{F},P)$. In particular, our OD has $\epsilon\sim\mathcal{N}(0,\sigma^2 I_T)$ so $\Omega=\RR^T$, $\mathcal{F}$ is Borel algebra, and $P$ is law of $\epsilon$.\NL
	% 	First of all, when $i\notin M$, by our definition of $\bar{\beta}$, there is directly:
	% 	\begin{equation}
		% 		\forall i\notin M:\bar{\beta}_i=0+\sgn(0)\cdot\frac{c}{\kappa_i^2\omega_i}=0
		% 	\end{equation}
	% 	When $j\in M$, we discuss two cases based on the sign of $\hat{\beta}_i$, each corresponding to having observed a different random LASSO selection event, which we denote as $\mathcal{M}$ per the notations in our main text.
	% 	\begin{enumerate}
		% 		\item The case of $\hat{\beta}_i<0$. This by our construction would imply $s_i<0$ and we would only observe if and only if $\{\frac{1}{T}Y^\top X_i+c<0\}$ by Equation 13 of the main text. We can rewrite $\mathcal{M}$ into the following sequence of equivalent random events:
		% 		\begin{equation}\label{2}
			% 			\begin{split}
				% 				\mathcal{M}				\iff&
				% 				\{\frac{1}{T}Y^\top X_i+c<0\}\\
				% 				\iff&
				% 				\{\frac{1}{T}(\beta_S^\top X_S^\top +\epsilon^\top ) X_i+c<0\}\q\text{(by Assumption \ref{asu1})}\\
				% 				\iff&
				% 				\{\beta_i\kappa_i^2+\frac{1}{T}\epsilon^\top X_i+c<0\}\q\text{(by Assumption \ref{asu2})}\\
				% 				\iff&
				% 				\{\frac{1}{\sqrt{T}}\epsilon^\top X_i<-\sqrt{T}(c+\beta_i\kappa_i^2)\}
				% 			\end{split}
			% 		\end{equation}
		% 	Let $Z_i:=\frac{1}{\sqrt{T}}\epsilon^\top X_i$. By Assumptions \ref{asu2} and \ref{asu3}, $Z_i\sim \mathcal{N}(0,\kappa_i^2\sigma^2)$ unconditional on $\mathcal{M}$. By (\ref{2}), we have
		% 	\begin{equation}\label{3}
			% Z_i|\mathcal{M}\sim \mathcal{TN}(0,\kappa_i^2\sigma^2;(-\infty,-\sqrt{T}(c+\beta_i\kappa_i^2)])
			% 	\end{equation}
		% 		For the LASSO estimate, we can similarly use OD to write:
		% 		\begin{equation}
			% 			\begin{split}
				% 				\hat{\beta}_i&=\frac{1}{\kappa_i^2}(\frac{1}{T}(\beta_S^\top X_S^\top +\epsilon^\top )X_i+c)\q\text{(by Assumption \ref{asu1})}	\\
				% 				&=\frac{1}{\kappa_i^2}(\beta_i\kappa_i^2+\frac{1}{T}\epsilon^\top X_i+c)	\q\text{(by Assumption \ref{asu2})}\\
				% 				&=\beta_i+\frac{ Z_i}{\sqrt{T}\kappa_i^2}+\frac{c}{\kappa_i^2}\\
				% 			\end{split}
			% 		\end{equation}
		% 		By definition of one-step estimator, we have
		% 		\begin{equation}\label{5}
			% 			\bar{\beta}_i=\hat{\beta}_i+(-1)\cdot \frac{c}{\kappa_i^2}=\beta_i+\frac{ Z_i}{\sqrt{T}\kappa_i^2}
			% 		\end{equation}
		% And	we can use (\ref{3}) to write 
		% \begin{equation}\label{6}
			% 						\frac{ Z_i}{\sqrt{T}\kappa_i^2}|\mathcal{M}\sim \mathcal{TN}(0,\frac{1}{T}\frac{\sigma^2}{\kappa_i^2};(-\infty,-\frac{c+\beta_i\kappa_i^2}{\kappa_i^2}])
			% \end{equation}
		% Combining (\ref{5}) and (\ref{6}) we have
		% \begin{equation}
			% \bar{\beta}_i|\mathcal{M}\sim 
			% \mathcal{TN}(\beta_i,\frac{1}{T}\kappa_i^2\sigma^2;(-\infty,-\frac{c}{\kappa_i^2}])
			% \end{equation}
		% \item The case of $\hat{\beta}_i>0$. This by our construction would imply $s_i>0$ and we would only observe if and only if $\{\frac{1}{T}Y^\top X_i-c>0\}$ by Equation 13 of the main text. We can rewrite $\mathcal{M}$ into the following sequence of equivalent random events:
		% \begin{equation}\label{7}
			% 	\begin{split}
				% 		\mathcal{M}				\iff&
				% 		\{\beta_i\kappa_i^2+\frac{1}{T}\epsilon^\top X_i-c>0\}\q\text{(by Assumptions \ref{asu1}, \ref{asu2})}\\
				% 		\iff&
				% 		\{\frac{1}{\sqrt{T}}\epsilon^\top X_i>\sqrt{T}(c-\beta_i\kappa_i^2)\}
				% 	\end{split}
			% \end{equation}
		% Again let $Z_i:=\frac{1}{\sqrt{T}}\epsilon^\top X_i$ and by Assumptions \ref{asu2} and \ref{asu3}, $Z_i\sim \mathcal{N}(0,\kappa_i^2\sigma^2)$ unconditional on $\mathcal{M}$. By (\ref{2}), we have
		% \begin{equation}\label{9}
			% 	Z_i|\mathcal{M}\sim \mathcal{TN}(0,\kappa_i^2\sigma^2;[\sqrt{T}(c-\beta_i\kappa_i^2),+\infty))
			% \end{equation}
		% For the LASSO estimate, we can similarly use OD to write:
		% \begin{equation}
			% 	\begin{split}
				% 		\hat{\beta}_i
				% 		&=\frac{1}{\kappa_i^2}(\beta_i\kappa_i^2+\frac{1}{T}\epsilon^\top X_i-c)	\q\text{(by Assumptions \ref{asu1}, \ref{asu2})}\\
				% 		&=\beta_i+\frac{ Z_i}{\sqrt{T}\kappa_i^2}-\frac{c}{\kappa_i^2}\\
				% 	\end{split}
			% \end{equation}
		% By definition of one-step estimator, we have
		% \begin{equation}\label{11}
			% 	\bar{\beta}_i=\hat{\beta}_i+(+1)\cdot \frac{c}{\kappa_i^2}=\beta_i+\frac{ Z_i}{\sqrt{T}\kappa_i^2}
			% \end{equation}
		% And	we can use (\ref{9}) to write 
		% \begin{equation}\label{12}
			% 	\frac{ Z_i}{\sqrt{T}\kappa_i^2}|\mathcal{M}\sim \mathcal{TN}(0,\frac{1}{T}\frac{\sigma^2}{\kappa_i^2};[\frac{c-\beta_i\kappa_i^2 }{\kappa_i^2},+\infty))
			% \end{equation}
		% Combining (\ref{5}) and (\ref{6}) we have
		% \begin{equation}
			% 	\bar{\beta}_i|\mathcal{M}\sim 
			% 	\mathcal{TN}(\beta_i,\frac{1}{T}\frac{\sigma^2}{\kappa_i^2};[\frac{c }{\kappa_i^2},+\infty))
			% \end{equation}
		% \end{enumerate}
	% Putting the two cases together, we conclude Theorem 1.\hfill$\blacksquare$
	
	\subsection{Proof of Lemma A.1}
	
	This result follows from the definition and some simple algebraic manipulations. By the definition of $\bar{\beta}_M$ we can write: 
	\begin{equation}\label{14}
		\begin{split}
			\bar{\beta}_M
			&=\hat{\beta}_M+X_M^+\hat{\epsilon}_M\\
			&=\hat{\beta}_M+(X_M^\top X_M)^{-1}X_M^\top (Y-X_M\hat{\beta}_M)\\
			&=\hat{\beta}_M+(X_M^\top X_M)^{-1}X_M^\top Y-(X_M^\top X_M)^{-1}X_M^\top X_M\hat{\beta}_M\\
			&=(X_M^\top X_M)^{-1}X_M^\top Y\\
			&=X_M^+Y.
		\end{split}
	\end{equation}
	Recall that $X_M^+Y=\argmin_\beta\frac{1}{2T}\|Y-X_M\beta\|_2^2$ equals the OLS estimator. Hence, equation (\ref{14}) concludes the second half of Lemma A.1.\NL
	The first half of Lemma A.1 simply uses $e_j$ to map to $j$th coordinate. Given $\eta=(X_M^+)^\top e_j$ we have
	\begin{equation}
		\bar{\beta}_{j}=e_j^\top \bar{\beta}_M=e_j^\top X_M^+Y=((X_M^+)^\top e_j)^\top Y=\eta^\top Y.
	\end{equation}
	Thus, we have shown Lemma A.1.
	
	\subsection{Proof of Lemma A.2}
	We start by calculating the sub-differential of the regularization function evaluated at $\hat{\beta}$:
	\begin{equation}
		\partial(\beta\mapsto f(\beta,\omega))|_{\beta=\hat{\beta}}
		=\hat{r}\odot\omega^{-1}.
	\end{equation}
	where $\hat{r}$ is the sub-differential $\pa(\beta\mapsto\|\beta\|_1)$ evaluated at $\hat{\beta}$, $\odot$ is the element-wise multiplication of two vectors, $\omega^{-1}$ is the element-wise reciprocal of vector $\omega$, i.e. $\omega^{-1}=[\omega_1^{-1},...,\omega_J^{-1}]$ and $1/\infty=0$. Thus, we have the LASSO sub-gradient optimal condition:
	\begin{equation}
		X^\top(X\hat{\beta}-Y)+\lambda \hat{r}\odot\omega^{-1}=0.
	\end{equation}
	Without loss of generality we can assume that the active covariates are the first $|M|$ of the $d$ factors. Then, we can expand the KKT conditions based on $X=[X_M,X_{-M}]$, $\hat{\beta}=[\hat{\beta}_M;\bm{0}]$, $\hat{r}=[\hat{r}_M;\hat{r}_{-M}]$, and $\omega=[\omega_M;\omega_{-M}]$:
	\begin{equation}
		\begin{split}
			X_M^\top(X_M\hat{\beta}_M-Y)+\lambda\hat{r}_{M}\odot\omega_M^{-1}=0\\
			X_{-M}^\top(X_M\hat{\beta}_M-Y)+\lambda\hat{r}_{-M}\odot\omega_{-M}^{-1}=0\\
			\sgn(\hat{\beta}_M)=\hat{r}_M\\
			\|\hat{r}_{-M}\|_\infty<1
		\end{split}
	\end{equation}
	The last two lines are based on the sub-differentials of $\pa(\beta\mapsto\|\beta\|_1)$ evaluated at $\hat{\beta}_M$ and $\hat{\beta}_{-M}$, respectively. Recall that $s=\sgn(\hat{\beta}_M)\in\RR^{|M|}$ is the vector of signs. Since the KKT conditions are sufficient and necessary for a solution, we obtain that the KKT conditions are equivalent to the set of vectors $w\in\RR^{|M|}$ and $u\in\RR^{J-|M|}$ that satisfy 
	\begin{equation}\label{18}
		\begin{split}
			X_M^\top(X_Mw-Y)+\lambda s\odot\omega_M^{-1}=0\\
			X_{-M}^\top(X_Mw-Y)+\lambda u\odot\omega_{-M}^{-1}=0\\
			\sgn(w)=s\\
			\|u\|_\infty<1
		\end{split}
	\end{equation}
	Using only the first line of (\ref{18}), we solve for
	\begin{equation}
		w=(X_M^\top X_M)^{-1}(X_M^\top Y-\lambda s\odot\omega^{-1}_M).
	\end{equation}
	Recall $\mathcal{J}$ is the set of $j$'s corresponding to $\omega_j$'s that are infinite. For $\mathcal{J}\neq \emptyset$, the notation above is still valid and the segment corresponding to ${\mathcal{J}}$ simply becomes the usual OLS coefficients:
	\begin{equation}
		w_{\mathcal{J}}=(X_{\mathcal{J}}^\top X_{\mathcal{J}})^{-1}X_{\mathcal{J}}^\top Y.
	\end{equation}
	This implies that
	\begin{equation}
		\begin{split}
			X_Mw-Y
			=&X_M(X_M^\top X_M)^{-1}(X_M^\top Y-\lambda s\odot\omega^{-1}_M)-Y\\
			=&X_MX_M^+ Y-\lambda (X_M^+)^\top s\odot\omega^{-1}_M-Y\\
			=& -(I-P_M)Y-\lambda (X_M^+)^\top s\odot\omega^{-1}_M.
		\end{split}
	\end{equation}
	Plugging this back into the second line of (\ref{18}), we solve for
	\begin{equation}\label{20}
		u=\omega_{-M}\odot
		\bigg(
		X_{-M}^\top(X_M^+)^\top s\odot\omega^{-1}_M
		+\frac{1}{\lambda }X_{-M}^\top (I-P_M)Y\bigg).
	\end{equation}
	Note that (\ref{20}) does not lead to an ambiguity issue for $\infty\cdot (\infty)^{-1}$ in the case of infinity prior weights, because infinitely weighed covariates are guaranteed to be active, i.e. $\mathcal{J}\subseteq M$. To see this explicitly, we can without loss of generality assume that $\mathcal{J}$ is located at the top of $M$. Then the notation in (\ref{20}) is still valid and we can write the part with infinity prior weights as:
	\begin{equation}
		u=\omega_{-M}\odot
		\begin{bmatrix}
			\frac{1}{\lambda }X_{-M}^\top (I-P_{\mathcal{J}})Y\\
			X_{-M}^\top(X_{M-\mathcal{J}}^+)^\top s_{M-\mathcal{J}}\odot\omega^{-1}_{M-\mathcal{J}}
			+\frac{1}{\lambda }X_{-M}^\top (I-P_{M-\mathcal{J}})Y
		\end{bmatrix}.
	\end{equation}
	The remaining conditions in (\ref{18}) are the third and fourth lines, which are exactly $\sgn(w)=s$ and $\|u\|_\infty<1 $, respectively. This concludes the proof of Lemma A.2.
	
	
	\subsection{Proof of Lemma A.3}
	
	The quantity of interest is given by $\eta=(X_M^+)^\top e_j$, $\bar{\beta}_{j}=\eta^\top Y$ per our Lemma A.1. By Assumption \ref{asu_known}, $\Sigma$ is known, so indeed both $
	\xi=\Sigma\eta(\eta^\top \Sigma\eta)^{-1}$ and $z=(I-\xi\eta^\top )Y$ can be calculated once we observe $(X,Y)$.\NL
	We now show that $z$ is uncorrelated with $\eta^\top Y$. Suppose $I-\xi\eta^\top =\Gamma$, then we have:
	\begin{equation}\label{27}
		\begin{split}
			cov(z,\eta^\top Y)
			=&cov(\Gamma Y,\eta^\top Y)\\
			=&\Gamma cov(Y)\eta\\
			=&\Gamma\Sigma\eta\q\text{(by Assumption \ref{asu_known})}\\
			=&(I-\xi\eta^\top)\Sigma\eta\\
			=&(I-
			\Sigma\eta(\eta^\top \Sigma\eta)^{-1}\eta^\top)\Sigma\eta\q\text{(by defn. of $\xi$)}\\
			=&\Sigma\eta-\Sigma\eta\\
			=&0.
		\end{split}
	\end{equation}
	Since $z$ and $\eta^\top Y$ are both linear mappings of $Y$, they both have a Gaussian distribution by Assumptions \ref{asu_known}. Using (\ref{27}), we conclude that they are independent.
	
	
	\subsection{Proof of Theorem A.1}
	
	The proof below follows similar arguments as in \cite{tian2018selective}. Given our Lemma A.2, we can first rewrite the active constraints $\sgn(w(M,s,\omega))=s$ as the following linear system of inequalities:
	\begin{equation}\label{22}
		\begin{split}
			\{\sgn(w)=s\}
			&=\{diag(s)w>0 \}\\
			&=\{diag(s)(X_M^\top X_M)^{-1}(X_M^\top Y-\lambda s\odot\omega^{-1}_M)>0 \}\\
			&=\{diag(s)X_M^+Y>\lambda \cdot diag(s)(X_M^\top X_M)^{-1}s\odot\omega^{-1}_M\}\\
			&=\{A_1(M,s,\omega)Y<b_1(M,s,\omega)\},
		\end{split}
	\end{equation}
	where
	\begin{equation}
		A_1(M,s,\omega)=-diag(s)X_M^+,\q b_1(M,s,\omega)=-\lambda \cdot diag(s)(X_M^\top X_M)^{-1}s\odot\omega^{-1}_M.
	\end{equation}
	We use the results of Lemma A.2 and the $u$ defined therein. The inactive constraints $\|u\|_\infty<1 $ can also be reformulated into a linear system of inequalities. Since $u$ is of dimension $(J-|M|)$, we expand it with respect to the 1-vector $\bm{1}_{J-|M|}$ of the same dimension. This yields
	\begin{equation}\label{23}
		\begin{split}
			&\{\|u\|_\infty<1 \}\\
			=&\{-\bm{1}_{J-|M|}<\omega_{-M}\odot
			\bigg(
			X_{-M}^\top(X_M^+)^\top s\odot\omega^{-1}_M
			+\frac{1}{\lambda}X_{-M}^\top (I-P_M)Y\bigg)\}\\
			&\cap\{\omega_{-M}\odot
			\bigg(
			X_{-M}^\top(X_M^+)^\top s\odot\omega^{-1}_M
			+\frac{1}{\lambda}X_{-M}^\top (I-P_M)Y\bigg)<\bm{1}_{J-|M|}\}\q\text{(by definition of $\|\cdot\|_\infty$)}\\
			=&\{-\omega_{-M}^{-1}<
			X_{-M}^\top(X_M^+)^\top s\odot\omega^{-1}_M
			+\frac{1}{\lambda}X_{-M}^\top (I-P_M)Y<\omega_{-M}^{-1}\}\q\text{(by design $\omega>0$)}\\
			=&\{-\omega_{-M}^{-1}-X_{-M}^\top(X_M^+)^\top s\odot\omega^{-1}_M<
			\frac{1}{\lambda}X_{-M}^\top (I-P_M)Y<\omega_{-M}^{-1}-X_{-M}^\top(X_M^+)^\top s\odot\omega^{-1}_M\}\\
			=&\{A_2(M,s,\omega)Y<b_2(M,s,\omega)\},
		\end{split}
	\end{equation}
	where
	\begin{equation}\label{29}
		A_2(M,s,\omega)=\begin{bmatrix}
			\frac{1}{\lambda}X_{-M}^\top (I-P_M)\\
			-\frac{1}{\lambda}X_{-M}^\top (I-P_M)
		\end{bmatrix},\q
		b_2(M,s,\omega)=\begin{bmatrix}
			\omega_{-M}^{-1}-X_{-M}^\top(X_M^+)^\top s\odot\omega^{-1}_M\\
			\omega_{-M}^{-1}+X_{-M}^\top(X_M^+)^\top s\odot\omega^{-1}_M
		\end{bmatrix}.
	\end{equation}
	Note that (\ref{29}) is valid when $\mathcal{J}\neq\emptyset$, because $\mathcal{J}\subseteq M$. Now combining (\ref{22}) and (\ref{23}), we have written the KKT conditions $\{\sgn(w)=s,\|u\|_\infty<1\}$ into the form $\{AY\leq b\}$, where $A=[A_1;A_2]$ and $b=[b_1;b_2]$. 
	
	Given the Gaussian Assumption \ref{asu1}, we can directly invoke Lemma 5.1 of \cite{lee2016exact} to construct
	\begin{equation}
		\{AY\leq b\}=\{V^-(z)\leq \eta^\top Y\leq V^+(z),V^0(z)\geq 0 \},
	\end{equation}
	where 
	\begin{equation}
		\begin{split}
			V^{-}(z)=\max_{j:(A\xi)_j<0}\frac{b_j-(Az)_j}{(A\xi)_j}\\
			V^{+}(z)=\min_{j:(A\xi)_j>0}\frac{b_j-(Az)_j}{(A\xi)_j}\\
			V^0(z)=\min_{j:(A\xi)_j=0}b_j-(Az)_j.
		\end{split}
	\end{equation}
	Moreover, $(V^{-}(z),V^{+}(z),V^{0}(z))$ are functions of $z$, and we have argued that $z\indep \eta^\top Y$ in Lemma A.3. To make the final inferential statements about $\eta^\top Y$, we simply drop the $V^0$ conditions given that we have conditioned on $\tilde{\mathcal{M}}$ which contains $z$.\NL
	Our quantity of interest $\bar{\beta}_{j}=\eta^\top Y$ is distributed as
	\begin{equation}
		\begin{split}
			\bar{\beta}_{j}|\tilde{\mathcal{M}}&\stackrel{\mathcal{D}}{=}[\bar{\beta}_{M_{(l)}}|(\{AY\leq b\},z)]\\
			&\stackrel{\mathcal{D}}{=}[\eta^\top Y|(\{V^-(z)\leq \eta^\top Y\leq V^+(z),V^0(z)\geq 0 \},z)]\\
			&\stackrel{\mathcal{D}}{=}[\eta^\top Y|(\{V^-(z)\leq \eta^\top Y\leq V^+(z)\},z)]\\
			&\sim\mathcal{TN}(\beta_{j},\eta^\top \Sigma\eta;[V^{-}(z),V^{+}(z)] )
		\end{split}
	\end{equation}
	This concludes the theorem.
	
	
	\subsection{Proof of Theorem A.2}\label{pf:app-thm1}
	
The studentized quantity requires different arguments from Theorem A.1, because its denominator includes $\hat{\sigma}(Y)$, which depends on $Y$. Our strategy is to first show the conversion from the Weighted-LASSO with penalty $\lambda$ to the Square-Root LASSO with penalty $\tilde{\lambda}$. Then, we show that the truncation is of the form $C Y\leq\hat{\sigma}(Y)$. Lastly, we show the result of a truncated $t$-distribution by following similar arguments as for the Square-Root LASSO in Theorem 1 of \cite{tian2017selective} and by solving a set of non-linear inequalities. 
	
	First, we note equation (9) and Lemma 2 of \cite{tian2017selective} show that the conversion from LASSO to Square-Root LASSO depends on the slack variables from first-order conditions. We calculate the first-order conditions for the active variables from Square-Root LASSO, written in the same form of equation (9) of \cite{tian2017selective}:
	\begin{equation}\label{divi_form}
		\frac{X_M^\top (Y-X_M\hat{\beta}_M)}{\|Y-X_M\hat{\beta}_M\|_2}= \tilde{\lambda}\cdot s\odot\omega^{-1}_M,\q\wh\q s=\sgn(\hat{\beta}_M).
	\end{equation}

By Assumption A.1(b), the pseudo-inverse of $X_M$ : $X_M^+=(X_M^\top X_M)^{-1}X_M^\top$ is well-defined. Equation (\ref{divi_form}) allows us to invoke Lemma 2 of \cite{tian2017selective} to establish the mapping from Square-Root penalty $\tilde{\lambda}$ to LASSO penalty $\lambda$:
	\begin{equation}\label{frac_equation}
		\lambda = \tilde{\lambda}\hat{\sigma}(Y)\sqrt{\frac{T-|M|}{1-\tilde{\lambda}^2\left((X_M^+)^\top s\odot\omega^{-1}_M\right)^2}}.
	\end{equation}

	We solve for $\tilde{\lambda}$ from the fractional equation of \ref{frac_equation}, and complete the conversion to $\tilde{\lambda}$ from $\lambda$:
	\begin{equation}
		\tilde{\lambda}^2=\frac{\lambda^2}{\hat{\sigma}^2(Y)\cdot (T-|M|)+\|(X_M^+)^\top s\odot\omega^{-1}_M\|_2^2\lambda^2}.
	\end{equation}

	
	Next, we consider the distribution of the active coefficients $\hat{\beta}_M$. Lemma 2 of \cite{tian2017selective} states that the distribution is a truncated $t$-distribtion, but the truncation still needs to be calculated. The key is to characterize the condition $\sgn(\hat{\beta}_M)=s$ from (\ref{divi_form}), and we need to isolate $\hat{\beta}_M$. 
	
	First we use (\ref{divi_form}) to calculate the residuals from the projection with $\hat{\beta}_M$ as
	\begin{equation}
\|Y-X_M\hat{\beta}_M\|_2^2=\frac{\|(I-P_M)Y\|_2^2}{1-\tilde{\lambda}^2\left((X_M^+)^\top s\odot\omega^{-1}_M\right)^2}.
	\end{equation}

Note that $\|(I-P_M)Y\|_2^2=(T-|M|)\hat{\sigma}^2(Y)$, and substituting $\hat{\sigma}^2(Y)$ in (\ref{divi_form}) yields
	\begin{equation}
		\hat{\beta}_M=(X_M^\top X_M)^{-1}\left(X_M^\top Y-
		\hat{\sigma}(Y)\sqrt{\frac{T-|M|}{1-\tilde{\lambda}^2\left((X_M^+)^\top s\odot\omega^{-1}_M\right)^2}}		
		 \tilde{\lambda}\cdot s\odot\omega^{-1}_M\right).
	\end{equation}
	Using the same argument that we have applied before, we rewrite the equality condition $\sgn(\hat{\beta}_M)=s$ into an inequality equivalency that states $e_j^\top\hat{\beta}_Ms_j\geq 0$ for $j\in M$ and for one-hot vector $e_j$:
	\begin{equation}\label{36.sigY}
		\begin{split}
			&e_j^\top (X_M^\top X_M)^{-1}\left(X_M^\top Y-
			\hat{\sigma}(Y)\sqrt{\frac{T-|M|}{1-\tilde{\lambda}^2\left((X_M^+)^\top s\odot\omega^{-1}_M\right)^2}}		
			\tilde{\lambda}\cdot s\odot\omega^{-1}_M\right)\cdot  s_j\geq 0\\
			\iff& -e_j^\top X_M^+ Y\cdot s_j\leq- (\tilde{\lambda} s_j )\cdot  e_j^\top \left((X_M^\top X_M)^{-1}s\odot \omega^{-1}\right)\cdot\sqrt{\frac{T-|M|}{1-\tilde{\lambda}^2\left((X_M^+)^\top s\odot\omega^{-1}_M\right)^2}}\cdot	\hat{\sigma}(Y).
		\end{split}
	\end{equation}
	
	The second line in (\ref{36.sigY}) is an inequality with $Y$ on left-hand side and $\hat{\sigma}(Y)$ on right-hand side. We simplify it by referring to the intermediary quantities:
	\begin{equation}\label{eq_truncb_defn}
		\begin{split}
			b_j=- (\tilde{\lambda} s_j )\cdot  e_j^\top \left((X_M^\top X_M)^{-1}s\odot \omega^{-1}\right)\cdot\sqrt{\frac{T-|M|}{1-\tilde{\lambda}^2\left((X_M^+)^\top s\odot\omega^{-1}_M\right)^2}}.
		\end{split}
	\end{equation}

Alternatively, we can also write it in terms of $\lambda$:
	\begin{equation}\label{trunc_b_normal_lambda}
	\begin{split}
		b_j=- (\lambda s_j/\hat{\sigma}(Y) )\cdot  e_j^\top \left((X_M^\top X_M)^{-1}s\odot \omega^{-1}\right).
	\end{split}
\end{equation}

	Then, the inequality in (\ref{36.sigY}) is equivalent to the following quasi-linear inequality:
	\begin{equation}
		\begin{split}
			-s_j\cdot  e_j^\top X_M^+ Y\leq\hat{\sigma}(Y)\cdot b_j.
		\end{split}
	\end{equation}
	Note that $e_j^\top X_M^+\in\RR^{1\times T}$ is the $j$th row of $X_M^+$, so we stack $s_j\cdot  e_j^\top X_M^+$ into a $\RR^{|M|\times T}$ matrix across all $j\in M$, which is $diag(s)X_M^+$. Similarly, we stack $b_j$'s into a vector $b$. 
	
This enables us to rewrite (\ref{36.sigY}) in matrix form across $j\in M$:
	\begin{equation}\label{39}
		C Y\leq \hat{\sigma}(Y)\cdot b,\q\wh\q
			C=-diag(s) X_M^+.
	\end{equation}

Now, it remains for us to construct the specific form of the truncation by solving the quasi-linear inequalities (\ref{39}). By definition, $\eta=(X_M^+)^\top e_j$ so $\eta^\top \eta =((X_M^\top X_M )^{-1})_{jj}$. 

Let $d=\tr(I_T-P_M)$. We note that \cite{tian2017selective} uses another variance estimator $\hat{\sigma}_P^2(Y)=\frac{\|(I-P_M)Y\|_2^2}{d}$. We begin by showing
\begin{equation}
	\tr P_M
	=\tr( X_MX_M^+)
	=\tr \left(X_M(X_M^\top X_M)^{-1}X_M^\top\right)
	=\tr \left( (X_M^\top X_M)^{-1}X_M^\top X_M\right) = |M|.
\end{equation}

Thus, $d=T-|M|$, and we continue to use $\hat{\sigma}^2(Y)=\frac{\|(I-P_M)Y\|_2^2}{T-|M|}$, and our inequalities of (\ref{39})  match those of equation (15) of \cite{tian2017selective}.

Let $\eta'=\frac{\eta}{\|\eta\|_2}$. Clearly $\|\eta'\|=1$ and $\eta'(\eta')^\top=\eta\eta^\top/\|\eta\|_2^2$. In addition, we see that
\begin{equation}
P_M\eta=X_MX_M^+\eta=X_M\underbrace{(X_M^\top X_M)^{-1}X_M^\top}_{X_M^+}
\underbrace{((X_M^\top X_M)^{-1}X_M^\top)^\top e_j}_{\eta}=X_M(X_M^\top X_M)^{-1} e_j=\eta.
\end{equation}
\begin{equation}
CP_M=\underbrace{-  diag(s) X_M^+}_{C}
\underbrace{X_MX_M^+}_{P_M}=- diag(s) X_M^+=C.
\end{equation}
So we obtain $P_M\eta'=\eta'$ and $CP_M=C$, and we satisfy the requirements in equation (16) of \cite{tian2017selective} as well. 

Given Assumption A.1 and since our hypothesis is $\beta_M=0$, we obtain the post-selection law $\mathbb{M}_{(C,b,P)}$ that is the same as the premise of Theorem 1 in \cite{tian2017selective}. In particular, our $b,C,\eta', P_M$ would correspond to their quantities $b,C,\eta,P$ in \cite{tian2017selective}. Then, we construct $\nu=C\eta'$ and $\xi=C (P_M-\eta\eta^\top /\|\eta\|_2^2 )Y$, as well as the $W=\|(I-P_M)Y\|_2^2+\|\eta^\top Y\|_2^2/\|\eta\|_2^2$.

This allows us to conclude that
\begin{equation}
	\frac{\eta^TY-0}{\|\eta\|\hat{\sigma}(Y)}
	| (P_M-\eta\eta^T /\|\eta\|_2^2)Y,\|Y\|_2^2\stackrel{D}{=}\mathcal{TT}_{d;\Omega},\q\wh \Omega=
		\bigcap_{j\in M}\{t:t\sqrt{W}\nu_j+\xi_j\sqrt{d+t^2}\leq b_j\sqrt{rW}\}
	\end{equation}

	By our Lemma A.1, it holds that $\eta^\top Y=\bar{\beta}_j$. Since $(P_M-\eta\eta^T)Y$ and $\|Y\|_2^2$ are both measurable with respect to $\tilde{\mathcal{M}}$, we conclude that
	\begin{equation}\label{40.0}
		\frac{\bar{\beta}_j}{\|\eta\|\hat{\sigma}(Y)}| \tilde{\mathcal{M}}\stackrel{D}{=}\mathcal{TT}_{d;\Omega}.
	\end{equation}
	The $p$-value follows as consequence of (\ref{40.0}).
	
	
	
	\subsection{Proof of Theorem A.3}\label{proof_COL2}
	Denote the studentized coefficient $	\frac{\bar{\beta}_j}{\|\eta\|\hat{\sigma}(Y)}$ estimated with data up to time $T$ as $S_T$. Our proof strategy is as follows. First, we obtain the result of Theorem A.1 when $\Sigma=\sigma^2I$. Second, we show the distribution of $S_T$ converges in distribution to a truncated Gaussian as $T\to\infty$. Finally, we show that the truncation on $Y$ described by Theorem A.2 is contained in the truncation given in Theorem A.1, which means that the $p$-value calculated from the Gaussian with the truncation of Theorem A.1 satisfies Assumption 1 of the main text. %This follows as the denominator is taken care of by invoking the Slutsky's lemma to be swapped by its constant limit. Let us now begin going through these steps.\\
	In more detail, the steps are as follows:
	\begin{enumerate}
		\item[(i)] First, $\bar{\beta}_j/\sqrt{\eta^T\Sigma\eta}\sim \mathcal{TN}(0,1;[V^{-}(z)/\sqrt{\eta^T\Sigma\eta},V^{+}(z)/\sqrt{\eta^T\Sigma\eta}])$ by Theorem A.1. In the case of $\Sigma=\sigma^2 I$, this becomes $\bar{\beta}_j/\|\eta\|_2\sigma \sim \mathcal{TN}(0,1;[V^{-}(z)/\|\eta\|_2\sigma,V^{+}(z)/\|\eta\|_2\sigma])$.
		
		\item[(ii)] Second, for $T\to\infty$, we obtain the following asymptotic results. It holds that $d=\tr(I-P_M)=T-|M|\to\infty$ as $T\to\infty$ and under Assumption A.3 we have $\hat{\sigma}^2(Y)\CP\sigma^2$ as $T\to\infty$. On the other hand, $d=\tr(I-P_M)=T-|M|\to\infty$ as $T\to\infty$. By Lemma 13 of \cite{10.1214/13-EJS815}, the distribution in (\ref{40.0}), that is not conditioned on the selection $\tilde{\mathcal{M}}$, satisfies 
		\begin{equation}\label{56}
			S_T\stackrel{D}{=} t_d\CD\mathcal{N}_{0,1},\q \text{ as }T\to\infty
		\end{equation}		
		\item[(iii)] Lastly, we establish the following claim about the truncation:\\		
		\textbf{Claim:} The truncation of $Y$ in Theorem A.2 is asymptotically same as the truncation of $Y$ in Theorem A.1, and the truncation of $Y$ in Theorem A.1 exists as $T\to\infty$.\\
		\textbf{Proof of the claim:} We consider the cases of active and non-active covariates separately.
		%Let us discuss the truncations in two parts:
		
		\textbf{Case 1: Truncation associated with active covariates}\\
		We start by looking at the truncation of $Y$ in equation (\ref{39}) due to active covariates, which according to Theorem A.2's (\ref{trunc_b_normal_lambda}) satisfies
		\begin{equation}\label{62}
			\begin{split}
				C Y&\leq \hat{\sigma}(Y)\cdot  \tilde{b}\\\wh&
				\begin{cases}
					C=-diag(s) X_M^+\\
										\tilde{b}_j=- (\lambda s_j/\hat{\sigma}(Y) )\cdot  e_j^\top \left((X_M^\top X_M)^{-1}s\odot \omega^{-1}\right)
				\end{cases}.
			\end{split}
		\end{equation}
		In Theorem A.1 we show the truncation corresponding to active covariates is
		\begin{equation}\label{63}
			\begin{split}
				A_1
				Y\leq b_1,\q\wh\q\begin{cases}
					A_1=-diag(s)X_M^+
					\\
					b_1=-\lambda \cdot diag(s)(X_M^\top X_M)^{-1}s\odot\omega^{-1}_M.
				\end{cases}
			\end{split}
		\end{equation}

		As $C=A_1$ and $b_1=\hat{\sigma}(Y)\tilde{b}$, the inequalites of (\ref{62}) and (\ref{63}) are equivalent for all $T$.
  
  It remains to check that the quantities in inequalities (\ref{63}) have limits as $T\to\infty$.
		 To begin with, we note that
		\begin{equation}
CY=A_1Y=-diag(s)X_M^+Y=-diag(s)\bar{\beta}_M.
		\end{equation}
The $j$th coordinate of $diag(s)\bar{\beta}_M$ is simply $s_j\bar{\beta}_j$ for $j\in M$, so left-hand side of the inequalities in (\ref{63}) exists as $T\to\infty$ as long as $\bar{\beta}_j$ exists. 
		
		On the other hand, the $j$th coordinate of $b_1$ is
\begin{equation}
b_1(j)
	= -\lambda s_je_j^\top(X_M^\top X_M)^{-1}s\odot\omega^{-1}_M
	= -\frac{1}{T}\lambda s_je_j^\top(\frac{1}{T}X_M^\top X_M)^{-1}s\odot\omega^{-1}_M
\end{equation}
		Since $\|s\odot\omega^{-1}_M\|_2\leq \sqrt{J}$, by Cauchy-Schwartz we have
		\begin{equation}\label{66}
			|b_1(j)|
			\leq  \frac{1}{T}\lambda \| (\frac{1}{T}X_M^\top X_M)^{-1}e_j\|_2\|s\odot\omega^{-1}_M\|_2
    \leq   \frac{\sqrt{J}}{T}\lambda \| (\frac{1}{T}X_M^\top X_M)^{-1}e_j\|_2
		\end{equation}
	By Assumption A.5(a), there exists a full-rank $G$ such that $\lim\limits_{T\to\infty}\frac{1}{T}X_M^\top X_M=G$, so
	\begin{equation}\label{exist_inv_G}
\lim\limits_{T\to\infty}(\frac{1}{T}X_M^\top X_M)^{-1}= G^{-1}.
	\end{equation}
Thus, $ (\frac{1}{T}X_M^\top X_M)^{-1}e_j$ converges to the $j$th row of $G^{-1}$, and $\| (\frac{1}{T}X_M^\top X_M)^{-1}e_j\|_2$ is bounded by $\sqrt{|M|}$ times the largest value of the $j$th row of $G^{-1}$, and hence finite.

 By Assumption A.5(b), $\lambda \sqrt{J}/T\to 0$.  Thus, (\ref{66}) gives us $b_1(j)\to 0$ as $T\to\infty$, and the right-hand side of the inequalities in (\ref{63}) exists as $T\to\infty$, In conclusion, the truncation associated with the activate covariates converges to the ones corresponding to the active covariates in Theorem A.1.
	
		\textbf{Case 2: Truncation associated with inactive covariates}\\
		We use equation (12) in Lemma 3 of \cite{tian2017selective} to write down the constraints from inactive-covariates. Let $\kappa= 
		(X_M^+)^\top s\odot\omega^{-1}_{M}$, then:
		\begin{equation}\label{eq-truncation-import}
			-\omega_{-M}^{-1}-X_{-M}^\top (X_M^+)^\top s\odot \omega_{M}^{-1}<\sqrt{\frac{1-\tilde{\lambda}^2\kappa^2_2}{\tilde{\lambda}^2}}\frac{X_{-M}(I-P_M)Y}{\|(I-P_M)Y\|_2}
			\leq 	\omega_{-M}^{-1}-X_{-M}^\top (X_M^+)^\top s\odot \omega_{M}^{-1}.
		\end{equation}
		Converting $\tilde{\lambda}$ to $\lambda$, we can rewrite the middle term as
		\begin{equation}
			\begin{split}
				&	\sqrt{\frac{1-\tilde{\lambda}^2\kappa^2}{\tilde{\lambda}^2}}\frac{X_{-M}(I-P_M)Y}{\|(I-P_M)Y\|_2}\\
				=	&\sqrt{\frac{1-	\frac{\lambda^2}{\hat{\sigma}^2(Y)\cdot (T-|M|)+\kappa^2\lambda^2}\kappa^2}{	\frac{\lambda^2}{\hat{\sigma}^2(Y)\cdot (T-|M|)+\kappa^2\lambda^2}}}\frac{X_{-M}(I-P_M)Y}{\|(I-P_M)Y\|_2}\q\text{(convert to $\lambda$)}\\
				=	&\sqrt{\frac{\|(I-P_M)Y\|_2^2+\kappa^2\lambda^2-\lambda^2}{\lambda^2}}\frac{X_{-M}(I-P_M)Y}{\|(I-P_M)Y\|_2}\\
				=	&\sqrt{\frac{\|(I-P_M)Y\|_2^2+\kappa^2\lambda^2-\lambda^2}{\|(I-P_M)Y\|_2^2\lambda^2}}\cdot X_{-M}(I-P_M)Y\q\text{(move into square-root)}\\
				=		&\sqrt{\lambda^{-2}+\frac{\kappa^2}{\|(I-P_M)Y\|_2^2}-\|(I-P_M)Y\|_2^{-2}}\cdot X_{-M}(I-P_M)Y.
			\end{split}
		\end{equation}
		By Assumption A.4, it holds that $\hat{\sigma}^2(Y)=\|(I-P_M)Y\|_2^2/(T-|M|)\stackrel{p}{\to}\sigma^2$, and hence $\|(I-P_M)Y\|_2^{-2}$ is $O_p(T^{-2})$. By Assumption 5(a) and (b), $\lambda^{-2}$ is $O(\frac{T}{\log T})$, dominating $\|(I-P_M)Y\|_2^{-2}$.
	
		Moreover, let $u=s\odot\omega^{-1}_{M}/\|s\odot\omega^{-1}_{M}\|_2\in\RR^{|M|}$, and note that $\|s\odot\omega^{-1}_{M}\|_2\leq \sqrt{J}$. Then, we have:
		\begin{equation}\label{70}
			\begin{split}
\frac{\kappa^2}{\|(I-P_M)Y\|_2^2}
&=
\frac{(s\odot\omega^{-1}_{M})^\top (X_M^+) (X_M^+)^\top s\odot\omega^{-1}_{M}
}{\hat{\sigma}^2(Y)(T-|M|)}
\\
&=
\frac{(s\odot\omega^{-1}_{M})^\top (X_M^\top X_M)^{-1} s\odot\omega^{-1}_{M}
}{\hat{\sigma}^2(Y)(T-|M|)}\\
&\leq
\frac{J}{T} \frac{u^\top (X_M^\top X_M)^{-1} u}{\hat{\sigma}^2(Y)(T-|M|)} .	\end{split}
		\end{equation}
			By Assumption A.5(a), we have $\lim\limits_{T\to\infty}(\frac{1}{T}X_M^\top X_M)^{-1}=G^{-1}$ and $u^\top G^{-1} u$ is bounded from above. By Assumption A.5(b), it holds that $J=O(T)$, and by Assumption A.4 $\hat{\sigma}^2(Y)\stackrel{p}{\to}\sigma^2$. Thus, the quantity in last line of (\ref{70}) satisfies:
\begin{equation}
	\frac{J}{T} \frac{u^\top (X_M^\top X_M)^{-1} u}{\hat{\sigma}^2(Y)(T-|M|)} =
\underbrace{\frac{J}{T(T-|M|)}}_{O(1/T)}
\underbrace{u^\top (\frac{1}{T}X_M^\top X_M)^{-1} u }_{\stackrel{T\to\infty}{\to} u^\top G^{-1} u, }
\underbrace{\frac{1}{\hat{\sigma}^2(Y)}}_{\stackrel{T\to\infty}{\to}\frac{1}{\sigma^2}}
  = o_p(T).\\
\end{equation}		
Therefore, $\lambda^{-2}$ dominates $\frac{\kappa^2}{\|(I-P_M)Y\|_2^2}
$ as well, when as $T\to\infty$, and the first term of the product converges to
			 \begin{equation}
\sqrt{\lambda^{-2}+\frac{\kappa^2}{\|(I-P_M)Y\|_2^2}-\|(I-P_M)Y\|_2^{-2}}\to\lambda^{-1}.
			 \end{equation}
	
		On the other hand, in Theorem A.1 the truncation corresponding to active covariates equals
		\begin{equation}
			-\omega_{-M}^{-1}-X_{-M}^\top (X_M^+)^\top s\odot \omega_{M}^{-1}<\lambda^{-1}{X_{-M}(I-P_M)Y}
			\leq \omega_{-M}^{-1}-X_{-M}^\top (X_M^+)^\top s\odot \omega_{M}^{-1}.
		\end{equation}
		
		This shows that the truncation for inactive covariates (\ref{eq-truncation-import}) converges to the one corresponding to inactive covariates in Theorem A.1
	\end{enumerate}
	
	We now combine the two steps. For $T\to\infty$, (iii) concludes that the truncation converges to the one in Theorem A.1 when $T\to\infty$. The truncation is a finite intersection of convex sets with an interior, given we observe a fitted LASSO from solving the KKT conditions. Combining it with (i) and (ii), we conclude the distribution result
	\begin{equation}\label{eq:converge_t}
		\frac{\bar{\beta}_j}{\|\eta\|\hat{\sigma}(Y)}| \tilde{\mathcal{M}}\CD\mathcal{TN}_{0,1;V^{-}(z)/\|\eta\|_2\sigma,V^{+}(z)/\|\eta\|_2\sigma}.
	\end{equation}
	For any random variable $X$ and its corresponding cumulative distribution function (cdf) $F(\cdot)$, it holds that $F(X)\sim \text{Unif}[0,1]$. Thus, by (\ref{eq:converge_t}), under the null and as $T\to\infty$, it holds that $\Phi^{\Omega}( \frac{\bar{\beta}_j}{\|\eta\|\hat{\sigma}(Y)})\CD \textrm{Unif}[0,1]$, where $\Omega=[V^{-}(z)/\|\eta\|_2\sigma,V^{+}(z)/\|\eta\|_2\sigma]$ and $\Phi^{\Omega}$ is the cdf of a standard normal distribution truncated with $\Omega$.
	
	
	% 	{\footnotesize  Cross-sectional goodness-of-fit measurements in $T_{selection}$ (navy) vs $T_{exclusion}$ (orange) with factor sets FF, HL, HL PC, HL + HL PC, HL-$\omega_4$ and HL-$\omega_8$; and with FWER control $\gamma=0.05$}
	% \end{figure}
% \newpage


\subsection{Implementation of Weighted-LASSO}

\subsubsection*{Calculation of $p$-values using Theorem A.3}


Numerical stability is a practical concern when applying estimation methods. As we calculate potentially the far tail of a truncated Gaussian CDF, we propose a set of best practices for fast and stable calculations with our methods:
% is worth discussing our recommended best practise in using \texttt{R} to carry out the calculation.
\begin{enumerate}
	\item Use the logarithmic transformation of $p$-values instead of $p$-values.\\
	This is motivated by three reasons:
	\begin{enumerate}
		\item Given finite machine accuracy (such as 16-bit float numeric storage) we are susceptible to arithmetic underflow when our $\bar{\beta}$'s are in the far tail and the $p$-values are very close to 0. The logarithmic transformation addresses this issue.
		\item We need to distinguish between small $p$-values because eventually our procedures need to compare $N_jp_j$ for different covariates $j$.
		\item We can save time and preserve numerical accuracy by avoiding an additional log transform step when calculating R\'enyi statistics.
	\end{enumerate}
	\item Use an approximation when the two tails are vastly different.\\
	Without loss of generality, we consider the case of $\bar{\beta}_{M(i)}>0$ and we label the two building blocks of our proposed $p$-value as
	\begin{equation}
		\begin{split}
			LeftTail=\ln\left(F_{TN}-(
			\frac{\bar{\beta}_{M(i)}}{\|\eta\|_2\hat{\sigma}})\right)\\
			RightTail=\ln\left(
			F^C_{TN}(\frac{\bar{\beta}_{M(i)}}{\|\eta\|_2\hat{\sigma}})\right).
		\end{split}
	\end{equation}
	The correct log $p$-value are
	\begin{equation}
		\ln( p)=\ln\left(\exp(LeftTail)+\exp(RightTail)\right).
	\end{equation}
	We propose to consider the following accurate approximation, which speeds up the calculations:
	\begin{equation}
		\ln( p)=
		\begin{cases}
			\ln\left(\exp(LeftTail)+\exp(RightTail)\right) &|LeftTail-RightTail|<\zeta\\
			\max\{LeftTail,RightTail\} &o.w.\\
		\end{cases}
	\end{equation}
	In our empirical study, we use $\zeta=10$ as the largest $N_j\ll10000$ in our data, and $\log \left(\max_j(N_j)p_j\right)\leq 10+\log p_j$.
\end{enumerate}

\subsubsection*{Additional Implementation Details}

There are additional implementational details for LASSO. We summarize what we consider good practice below:
\begin{enumerate}
	\item Search range for the $\ell_1$ penalty scalar $\lambda$: \\We propose candidates by a log-linear sequence of numbers multiplied by $
	\frac{\log J}{\sqrt{|T_{selection}|}}$, inspired by \cite{10.1214/17-AOS1630}.
	
	\item Cross-validation (CV henceforth): \\We use 5-fold random splits of the training data to cross-validate across candidate set of $\lambda$'s.
	
	\item Criteria for CV: \\Our cross-validation follows the one-standard-deviation rule for selecting parsimonious models, that is, the largest choice of $\lambda$ within one standard error of minimizing the squared errors. This is the default setting of popular implementations like \texttt{glmnet} and argued for in \S3.4 of \cite{hastie2009elements}.
	
	% \item No intercept:  we explicitly forbade intercept by \texttt{intercept = F}. There are two reasons: this is motivated by APT model in which excess returns should lie in subspace spanned by factor risk premia; our PoSI theory was developed without the intercept;
	\item Refit after CV: \\We refit the sparse model using the entire training data after selecting the best $\lambda$.
\end{enumerate}




\bibliographystyle{econometrica}
{\small
	\bibliography{main}
}

\end{document}
