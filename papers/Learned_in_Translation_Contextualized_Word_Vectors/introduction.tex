\section{Introduction}

Significant gains have been made through transfer and multi-task learning between synergistic tasks.
In many cases, these synergies can be exploited by architectures that rely on similar components. 
In computer vision, convolutional neural networks (CNNs) pretrained on ImageNet~\citep{krizhevsky2012imagenet, Deng2009ImageNetAL} have become the \textit{de facto} initialization for more complex and deeper models. 
This initialization improves accuracy on other related tasks such as visual question answering~\citep{xiong2016dynamic} or image captioning~\citep{lu2016knowing, Socher2014TACL}.

In NLP, distributed representations pretrained with models like Word2Vec~\citep{Mikolov2013b} and GloVe~\citep{Pennington2014} have become common initializations for the word vectors of deep learning models. 
Transferring information from large amounts of unlabeled training data in the form of word vectors has shown to improve performance over random word vector initialization on a variety of downstream tasks, 
e.g. part-of-speech tagging~\citep{Collobert2011}, named entity recognition~\citep{Pennington2014}, and question answering~\citep{Xiong2017};
however, words rarely appear in isolation.
The ability to share a common representation of words in the context of sentences that include them could further improve transfer learning in NLP.

Inspired by the successful transfer of CNNs trained on ImageNet to other tasks in computer vision,
we focus on training an encoder for a large NLP task and transferring that encoder to other tasks in NLP.
Machine translation (MT) requires a model to encode words in context so as to decode them into another language,
and attentional sequence-to-sequence models for MT often contain an LSTM-based encoder, 
which is a common component in other NLP models.
We hypothesize that MT data in general holds potential
comparable to that of ImageNet 
as a cornerstone for reusable models.
This makes an MT-LSTM pairing in NLP a natural candidate for mirroring the ImageNet-CNN pairing of computer vision.

As depicted in Figure~\ref{fig1}, 
we begin by training LSTM encoders on several machine translation datasets, 
and we show that these encoders can be used to improve performance of models trained for other tasks in NLP.
In order to test the transferability of these encoders, 
we develop a common architecture for a variety of classification tasks,
and we modify the Dynamic Coattention Network for question answering~\citep{Xiong2017}.
We append the outputs of the MT-LSTMs, 
which we call context vectors (CoVe), 
to the word vectors typically used as inputs to these models.
This approach improved the performance of models for downstream tasks over that of baseline models using pretrained word vectors alone.
For the Stanford Sentiment Treebank (SST) and the Stanford Natural Language Inference Corpus (SNLI),
CoVe pushes performance of our baseline model to the state of the art.

Experiments reveal that the quantity of training data used to train the MT-LSTM is positively correlated with performance on downstream tasks. 
This is yet another advantage of relying on MT, 
as data for MT is more abundant than for most other supervised NLP tasks, 
and it suggests that higher quality MT-LSTMs carry over more useful information.
This reinforces the idea that machine translation is a good candidate task for further research into models that possess a stronger sense of natural language understanding.

\begin{figure}
  \centering
 \includegraphics[width=\textwidth]{figure1.pdf}
  \caption{We a) train a two-layer, bidirectional LSTM as the encoder of an attentional sequence-to-sequence model for machine translation and b) use it to provide context for other NLP models.
  }\label{fig1}
  \vspace{-0.3cm}
\end{figure}