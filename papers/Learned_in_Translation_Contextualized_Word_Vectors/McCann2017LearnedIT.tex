\documentclass{article}

\usepackage[final]{nips_2017}
\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{xcolor}         % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{multirow}
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage[pdftex]{graphicx}
\usepackage{amsmath}    
\usepackage{wrapfig}
\usepackage{subcaption}
\usepackage{array}
\usepackage[hidelinks]{hyperref}

\definecolor{LinkColor}{rgb}{0,0,0}    
\hypersetup{colorlinks=true,
    linkcolor=LinkColor,
    citecolor=LinkColor,
    filecolor=LinkColor,
    menucolor=LinkColor,
    urlcolor=LinkColor}

\newcommand{\forward}{\rightarrow}
\newcommand{\backward}{\leftarrow}
\newcommand{\lstm}[2]{{\rm LSTM} \left( #1 , #2 \right)}
\newcommand{\bilstm}[1]{{\rm biLSTM} \left( #1 \right)}
\newcommand{\sql}{^{\rm sql}}
\newcommand{\enc}{^{\rm enc}}
\newcommand{\dec}{^{\rm dec}}
\newcommand{\softmax}[1]{{\rm softmax} \left( #1 \right)}
\newcommand{\argmax}[1]{{\rm argmax} \left( #1 \right)}
\newcommand{\embin}[1]{L \left[ #1 \right]}
\newcommand{\embout}[1]{O \left[ #1 \right]}
\newcommand{\ftanh}[1]{{\rm tanh} \left( #1 \right)}

%\setlength{\textfloatsep}{0.15cm}
\setlength{\floatsep}{0.3cm}
\captionsetup[table]{skip=5pt}

\title{Learned in Translation: Contextualized Word Vectors}

\author{
Bryan McCann \\
  \texttt{bmccann@salesforce.com}
  \And James Bradbury  \\
  \texttt{james.bradbury@salesforce.com}
  \And Caiming Xiong \\
  \texttt{cxiong@salesforce.com} 
  \And Richard Socher \\
  \texttt{rsocher@salesforce.com}
}

\begin{document}
\maketitle

\input{abstract}
\input{introduction}
\input{background}
\input{translation}
\input{transfer}
\input{downstream}
\input{data}
\input{experiments}

\section{Conclusion}
We introduce an approach for transferring knowledge from an encoder pretrained on machine translation to a variety of downstream NLP tasks. 
In all cases, models that used CoVe from our best, pretrained MT-LSTM performed better than baselines that used random word vector initialization, baselines that used pretrained word vectors from a GloVe model, and baselines that used word vectors from a GloVe model together with character n-gram embeddings.
We hope this is a step towards the goal of building unified NLP models that rely on increasingly more general reusable weights.

The PyTorch code at \url{https://github.com/salesforce/cove} includes an example of how to generate CoVe from the MT-LSTM we used in all of our best models. We hope that making our best MT-LSTM available will encourage further research into shared representations for NLP models.

\small
\bibliography{references}
\bibliographystyle{abbrvnat}



\end{document}