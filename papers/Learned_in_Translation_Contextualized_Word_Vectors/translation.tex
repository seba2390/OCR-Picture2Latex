\section{Machine Translation Model}

We begin by training an attentional sequence-to-sequence model for English-to-German translation based on~\citet{2017opennmt} with the goal of transferring the encoder to other tasks.

For training, 
we are given a sequence of words in the source language 
$w^{x}=[w^{x}_1, \ldots, w^{x}_n]$ 
and a sequence of words in the target language
$w^{z}=[w^{z}_1, \ldots, w^{z}_m]$. 
Let GloVe($w^x$)
be a sequence of GloVe vectors corresponding to the words in $w^x$,
and let $z$ be a sequence of randomly initialized word vectors corresponding to the words in $w^z$.

We feed GloVe($w^x$) to a standard, two-layer, bidirectional, long short-term memory network
\footnote{
Since there are several biLSTM variants, we define ours as follows. Let $h=[h_1, \ldots, h_n]=\bilstm{x}$ represent the output sequence of our biLSTM operating on an input sequence $x$. Then a forward LSTM computes $h^{\forward}_t = \lstm{x_t}{h^{\forward}_{t-1}}$ 
for each time step, and a backward LSTM computes 
$h^{\backward}_t = \lstm{x_t}{h^{\backward}_{t+1}}$.
The final outputs of the biLSTM for each time step are 
$h_t = \left[ h^{\forward}_t; h^{\backward}_t \right]$.
}~\citep{graves2005framewise} 
that we refer to as an MT-LSTM 
to indicate that it is this same two-layer BiLSTM that we later transfer as a pretrained encoder.
The MT-LSTM  is used to compute a sequence of hidden states
\begin{equation}
h = \text{\rm MT-LSTM}(\text{GloVe}(w^x)).
\label{eq:mtlstm}
\end{equation}

For machine translation, 
the MT-LSTM supplies the context for an attentional decoder 
that produces a distribution over output words 
$p(\hat{w}^z_t|H,w^z_1,\ldots,w^z_{t-1})$ at each time-step.

At time-step $t$, 
the decoder first uses a two-layer, 
unidirectional LSTM to produce a hidden state $h\dec_t$ 
based on the previous target embedding $z_{t-1}$ 
and a context-adjusted hidden state $\tilde h_{t-1}$:
\begin{equation}
h\dec_{t} = \lstm{ [ z_{t-1}; \tilde h_{t-1} ] }{h\dec_{t-1}}.
\end{equation}

The decoder then computes a vector of attention weights $\alpha$ 
representing the relevance of each encoding time-step to the current decoder state.
\begin{equation}
\alpha_{t} = \softmax{H (W_1 h\dec_{t} + b_1)}
\end{equation}

where $H$ refers to the elements of $h$ stacked along the time dimension.

The decoder then uses these weights as coefficients in an attentional sum
that is concatenated with the decoder state 
and passed through a $\rm tanh$ layer
to form the context-adjusted hidden state $\tilde{h}$:
\begin{equation}
\tilde h_{t} = \ftanh{W_2 \left[ H^\top \alpha_t ; h\dec_{t} \right ] + b_2}
\end{equation}

The distribution over output words is generated by a final transformation of the context-adjusted hidden state:
$
p(\hat{w}^z_t|X,w^z_1,\ldots,w^z_{t-1}) = \softmax{W_{\rm out} \tilde h_t + b_{\rm out}}.
$