\section{Experiments}
\subsection{Machine Translation}
The MT-LSTM trained on MT-Small obtains an uncased,
tokenized BLEU score of $38.5$ on the Multi30k test set from 2016.
The model trained on MT-Medium obtains an uncased, 
tokenized BLEU score of $25.54$ on the IWSLT test set from 2014.
The MT-LSTM trained on MT-Large obtains an uncased, 
tokenized BLEU score of $28.96$ on the WMT 2016 test set.
These results represent strong baseline machine translation models for their respective datasets. 
Note that, while the smallest dataset has the highest BLEU score, 
it is also a much simpler dataset with a restricted domain.

\textbf{Training Details. } 
When training an MT-LSTM,
we used fixed 300-dimensional word vectors.
We used the CommonCrawl-840B GloVe model for English word vectors,
which were completely fixed during training,
so that the MT-LSTM had to learn how to use the pretrained vectors for translation.
The hidden size of the LSTMs in all MT-LSTMs is 300. 
Because all MT-LSTMs are bidirectional,
they output 600-dimensional vectors.
The model was trained with stochastic gradient descent with a learning rate that began at $1$ and decayed by half each epoch after the validation perplexity increased for the first time.
Dropout with ratio $0.2$ was applied to the inputs and outputs of all layers of the encoder and decoder.

\begin{figure}
\begin{subfigure}{0.48\textwidth}
\includegraphics[width=\linewidth]{cove_glove.pdf}\vspace{-0.2cm}
\caption{CoVe and GloVe} \label{fig:coveGloVe}
\end{subfigure}
\hspace*{\fill} % separation between the subfigures
\begin{subfigure}{0.48\textwidth}
\includegraphics[width=\linewidth]{cove_char.pdf}\vspace{-0.2cm}
\caption{CoVe and Characters} \label{fig:coveChar}
\end{subfigure}
\caption{The Benefits of CoVe} \label{fig:bars}
\end{figure}


\begin{table}
\captionsetup{width=.92\textwidth}
  \centering
\begin{tabular}{lccccccc}
\toprule
        &        &       & \multicolumn{5}{c}{GloVe+} \\
\cmidrule(lr){4-8}
Dataset & Random & GloVe & Char & CoVe-S & CoVe-M & CoVe-L & Char+CoVe-L \\
\midrule
SST-2   & 84.2 & 88.4 & 90.1 & 89.0 & 90.9 & 91.1 & \textbf{91.2}\\ 
SST-5   & 48.6 & 53.5 & 52.2 & 54.0 & 54.7 & 54.5 & \textbf{55.2}\\
IMDb    & 88.4 & 91.1 & 91.3 & 90.6 & 91.6 & 91.7 & \textbf{92.1}\\
TREC-6  & 88.9 & 94.9 & 94.7 & 94.7 & 95.1 & 95.8 & \textbf{95.8}\\ 
TREC-50 & 81.9 & 89.2 & 89.8 & 89.6 & 89.6 & 90.5 & \textbf{91.2}\\
SNLI    & 82.3 & 87.7 & 87.7 & 87.3 & 87.5 & 87.9 & \textbf{88.1}\\
SQuAD   & 65.4 & 76.0 & 78.1 & 76.5 & 77.1 & 79.5 & \textbf{79.9}\\
\bottomrule
  \end{tabular}
      \caption{CoVe improves validation performance. CoVe has an advantage over character n-gram embeddings, but using both improves performance further. Models benefit most by using an MT-LSTM trained with MT-Large (CoVe-L). Accuracy is reported for classification tasks, and F1 is reported for SQuAD.}
  \label{validationPerformance}
  \vspace{-0.8cm}
\end{table}
 
\subsection{Classification and Question Answering}
For classification and question answering, 
we explore how varying the input representations affects final performance.
Table~\ref{validationPerformance} contains validation performances for experiments comparing the use of GloVe, character n-grams, CoVe, and combinations of the three.

\textbf{Training Details.}
Unsupervised vectors and MT-LSTMs remain fixed in this set of experiments.
LSTMs have hidden size 300.
Models were trained using Adam with $\alpha=0.001$. 
Dropout was applied before all feedforward layers with dropout ratio $0.1$, $0.2$, or $0.3$.
Maxout networks pool over $4$ channels, reduce dimensionality by $2$, $4$, or $8$, reduce again by $2$, and project to the output dimension.

\textbf{The Benefits of CoVe.}
Figure~\ref{fig:coveGloVe} shows that models that use CoVe alongside GloVe achieve higher validation performance than models that use only GloVe.
Figure~\ref{fig:coveChar} shows that using CoVe in Eq.~\ref{eq:concatVectors} brings larger improvements than using character n-gram embeddings~\citep{Hashimoto2016AJM}. It also shows that altering Eq.~\ref{eq:concatVectors} by additionally appending character n-gram embeddings can boost performance even further for some tasks.
This suggests that the information provided by CoVe is complementary to both the word-level information provided by GloVe as well as the character-level information provided by character n-gram embeddings.


\begin{wrapfigure}[16]{r}{0.62\textwidth}
\vspace{-5.5mm}
  \centering
  \includegraphics[width=\linewidth]{cove_comp.pdf}
  \caption{The Effects of MT Training Data
  }
\label{fig:coveComp}
\end{wrapfigure}

\textbf{The Effects of MT Training Data.}
We experimented with different training datasets for the MT-LSTMs to see how varying the MT training data affects the benefits of using CoVe in downstream tasks.
Figure~\ref{fig:coveComp} shows an important trend we can extract from Table~\ref{validationPerformance}. 
There appears to be a positive correlation between the larger MT datasets,
which contain more complex, varied language, 
and the improvement that using CoVe brings to downstream tasks.
This is evidence for our hypothesis that MT data has potential as a large resource for transfer learning in NLP.

\textbf{Test Performance.} 
Table~\ref{testPerf} shows the final test accuracies of our best classification models, 
each of which achieved the highest validation accuracy on its task using GloVe, CoVe, and character n-gram embeddings.
Final test performances on SST-5 and SNLI reached a new state of the art.

\begin{wraptable}{r}{8cm}
    \vspace{-3.5mm}
  \centering
  \begin{tabular}{lcc}
    \toprule
Model & EM  & F1 \\
\midrule
LR~\citep{rajpurkar2016squad} & 40.0 & 51.0 \\
DCR~\citep{yu2017end} & 62.5 & 72.1  \\
hM-LSTM+AP~\citep{wang2017machine} & 64.1 & 73.9 \\
DCN+Char~\citep{Xiong2017} & 65.4 & 75.6 \\
BiDAF~\citep{Seo2017BidirectionalAF} & 68.0 & 77.3 \\
R-NET~\citep{Wang2017} & 71.1 & 79.5 \\
{\it \textbf{DCN+Char+CoVe} } [{\it \textbf{Ours}}] &{\it \textbf{71.3} }&{\it \textbf{79.9} }\\
\bottomrule
  \end{tabular}
      \caption{Exact match and F1 validation scores for single-model question answering.
      }
  \label{squadComps}
    \vspace{-8mm}
\end{wraptable}

Table~\ref{squadComps} shows how the validation exact match and F1 scores of our best SQuAD model compare to the scores of the most recent top models in the literature. 
We did not submit the SQuAD model for testing, 
but the addition of CoVe was enough to push the validation performance of the original DCN,
which already used character n-gram embeddings,
above the validation performance of the published version of the R-NET. 
Test performances are tracked by the SQuAD leaderboard~\footnote{\url{https://rajpurkar.github.io/SQuAD-explorer/}}.

\begin{table}
  \centering
  \setlength\tabcolsep{3.65pt}
\begin{tabular}{llcllc}
    \toprule
 & Model & Test &  & Model & Test\\
\midrule
 \parbox[t]{2mm}{\multirow{6}{*}{\rotatebox[origin=c]{90}{SST-2}}} & P-LSTM~\citep{Wieting2015TowardsUP} & 89.2             & \parbox[t]{2mm}{\multirow{6}{*}{\rotatebox[origin=c]{90}{TREC-6}}} & SVM~\citep{Silva2011FromST} & 95.0           \\
                        & CT-LSTM~\citep{Looks2017DeepLW} & 89.4             &                         & SVM~\citep{Vantu2016QC}  & 95.2              \\
                        & TE-LSTM~\citep{Huang2017EncodingSK} & 89.6             &                         & DSCNN-P~\citep{Zhang2016DependencySC} & 95.6             \\
                        & NSE~\citep{Munkhdalai2016NeuralSE} & 89.7             &                         & {\it BCN+Char+CoVe [Ours] }& {\it95.8 }          \\
                        & {\it BCN+Char+CoVe [Ours]           }          & {\it90.3}          &                         &  TBCNN~\citep{Mou2015DiscriminativeNS}& 96.0             \\
                        & \textbf{bmLSTM~\citep{Radford2017LearningTG}} & \textbf{91.8}             &                         & \textbf{LSTM-CNN~\citep{Zhou2016TextCI}}            & \textbf{96.1} \\
\midrule
 \parbox[t]{2mm}{\multirow{6}{*}{\rotatebox[origin=c]{90}{SST-5}}} & MVN~\citep{Guo2017EndtoEndMN} & 51.5          & \parbox[t]{2mm}{\multirow{6}{*}{\rotatebox[origin=c]{90}{TREC-50}}}& SVM~\citep{Loni2011QuestionCB}&89.0           \\
                        & DMN~\citep{Kumar2016} & 52.1             &                         & SNoW~\citep{Li2006LearningQC} & 89.3             \\
                        & LSTM-CNN~\citep{Zhou2016TextCI} & 52.4             &                         & {\it BCN+Char+CoVe [Ours]        }           & {\it90.2}             \\
                        & TE-LSTM~\citep{Huang2017EncodingSK} & 52.6             &                         & RulesUHC~\citep{Silva2011FromST}       & 90.8             \\
                        & NTI~\citep{Munkhdalai2016NeuralTI} & 53.1             &                         &  SVM~\citep{Vantu2016QC}                    &     91.6      \\
                        & {\it \textbf{BCN+Char+CoVe [Ours]} }           & {\it \textbf{53.7}} &                         &  \textbf{Rules~\citep{Madabushi2016HighAR}}                   & \textbf{97.2}             \\
\midrule
 \parbox[t]{2mm}{\multirow{6}{*}{\rotatebox[origin=c]{90}{IMDb}}}  & {\it BCN+Char+CoVe [Ours]}                     &{\it91.8}          & \parbox[t]{2mm}{\multirow{6}{*}{\rotatebox[origin=c]{90}{SNLI}}}   &DecAtt+Intra~\citep{Parikh2016ADA} &86.8             \\
                        & SA-LSTM~\citep{Dai2015SemisupervisedSL} &92.8             &                         & NTI~\citep{Munkhdalai2016NeuralTI} & 87.3             \\
                        & bmLSTM~\citep{Radford2017LearningTG} &92.9             &                        & re-read LSTM~\citep{Sha2016ReadingAT} & 87.5             \\
                        & TRNN~\citep{Dieng2016TopicRNNAR} &93.8             &                        & btree-LSTM~\citep{Paria2016ANA} & 87.6            \\
               & oh-LSTM~\citep{Johnson2016SupervisedAS} &94.1             &                         & 600D ESIM~\citep{Chen2016EnhancingAC} &  88.0             \\
                        & {\bf Virtual~\citep{Miyato2017AdversarialTM}} &{\bf94.1}            &                         & {\it \textbf{BCN+Char+CoVe [Ours]}}            & {\it\textbf{88.1}} \\
\bottomrule
  \end{tabular}
      \caption{Single model test accuracies for classification tasks.}
  \label{testPerf}
  \vspace{-4mm}
\end{table}

\begin{wraptable}{r}{6cm}
  \vspace{-3.5mm}
\captionsetup{width=5.6cm}
  \centering
\begin{tabular}{lcc}
\toprule
        & \multicolumn{2}{c}{GloVe+Char+} \\
\cmidrule(lr){2-3}
Dataset & Skip-Thought & CoVe-L \\
\midrule
SST-2   & 88.7 & \textbf{91.2}\\ 
SST-5   & 52.1 & \textbf{55.2}\\
TREC-6  & 94.2 & \textbf{95.8}\\ 
TREC-50 & 89.6 & \textbf{91.2}\\
SNLI    & 86.0 &  \textbf{88.1}\\
\bottomrule
  \end{tabular}
      \caption{Classification validation accuracies with skip-thought and CoVe.}
  \label{SkipTable}
\end{wraptable}

\textbf{Comparison to Skip-Thought Vectors.}
~\citet{Kiros2015SkipThoughtV} show how to encode a sentence into a single skip-thought vector that transfers well to a variety of tasks.
Both skip-thought and CoVe pretrain encoders to capture information at a higher level than words. However, skip-thought encoders are trained with an unsupervised method that relies on the final output of the encoder. MT-LSTMs are trained with a supervised method that instead relies on intermediate outputs associated with each input word. Additionally, the $4800$ dimensional skip-thought vectors make training more unstable than using the $600$ dimensional CoVe. Table~\ref{SkipTable} shows that these differences make CoVe more suitable for transfer learning in our classification experiments.