\section{Context Vectors (CoVe)}

We transfer what is learned by the MT-LSTM to downstream tasks by treating the outputs of the MT-LSTM as context vectors. 
If $w$ is a sequence of words and GloVe$(w)$ the corresponding sequence of word vectors produced by the GloVe model,
then
\begin{equation}
\text{CoVe}(w) = \text{MT-LSTM}(\text{GloVe}(w))
\label{eq:contextVectors}
\end{equation}
is the sequence of context vectors produced by the MT-LSTM. 
For classification and question answering, 
for an input sequence $w$, 
we concatenate each vector in GloVe($w$) with its corresponding vector in CoVe($w$)
\begin{equation}
\tilde w = [\text{GloVe}(w); \text{CoVe}(w)]
\label{eq:concatVectors}
\end{equation}
as depicted in Figure~\ref{fig1}b.