\section{Classification with CoVe}

We now describe a general biattentive classification network (BCN) we use to test how well CoVe transfer to other tasks.
This model, 
shown in Figure~\ref{classarch},
is designed to handle both single-sentence and two-sentence classification tasks.
In the case of single-sentence tasks, 
the input sequence is duplicated to form two sequences,
so we will assume two input sequences for the rest of this section.

\begin{wrapfigure}[30]{r}{0.45\textwidth}
  \centering
  \includegraphics[width=0.4\textwidth]{classification.pdf}
  \caption{Our BCN uses a feedforward network with ReLU activation and biLSTM encoder to create task-specific representations of each input sequence. Biattention conditions each representation on the other, a biLSTM integrates the conditional information, and a maxout network uses pooled features to compute a distribution over possible classes.
  }
  \label{classarch}
\end{wrapfigure}

Input sequences $w^x$ and $w^y$ are converted to sequences of vectors, $\tilde w^x$ and $\tilde w^y$, as described in  Eq.~\ref{eq:concatVectors} before being fed to the task-specific portion of the model (Figure~\ref{fig1}b).

A function $f$ applies a feedforward network with ReLU activation~\citep{Nair10} 
to each element of  $\tilde w^x$ and $\tilde w^y$, 
and a bidirectional LSTM processes
the resulting sequences to obtain task specific representations, 
\begin{equation}
x = \bilstm{f(\tilde w^x)}
\label{eq:taskVectorsX}
\end{equation}
\begin{equation}
y = \bilstm{f(\tilde w^y)}
\label{eq:taskVectorsY}
\end{equation}
These sequences are each stacked along the time axis to get matrices $X$ and $Y$.

In order to compute representations that are interdependent, 
we use a biattention mechanism~\citep{Seo2017BidirectionalAF,Xiong2017}.
The biattention first computes an affinity matrix $A=XY^\top$. 
It then extracts attention weights with column-wise normalization:
\begin{equation}
A_x=\softmax{A}
\qquad
A_y=\softmax{A^\top}
\end{equation}
which amounts to a novel form of self-attention when $x=y$. 
Next, 
it uses context summaries 
\begin{equation}
C_x=A^\top_x X
\qquad 
C_y=A_y^\top Y
\end{equation}
to condition each sequence on the other.\bigskip

We integrate the conditioning information into our representations
for each sequence with two separate one-layer, 
bidirectional LSTMs that operate on the concatenation of
the original representations
(to ensure no information is lost in conditioning), 
their differences from the context summaries
(to explicitly capture the difference from the original signals),
and the element-wise products between originals and context summaries
(to amplify or dampen the original signals).
\begin{align}
X_{|y} &= \bilstm{ \left[X; X - C_y; X \odot C_y \right]}
\\
Y_{|x} &= \bilstm{ \left[Y; Y - C_x; Y \odot C_x \right] }
\end{align}

The outputs of the bidirectional LSTMs are aggregated by pooling along the time dimension.
Max and mean pooling have been used in other models to extract features, 
but we have found that adding both min pooling 
and self-attentive pooling can aid in some tasks.
Each captures a different perspective on the conditioned sequences.

The self-attentive pooling computes weights for each time step of the sequence 
\begin{equation}
\beta_{x} = \softmax{X_{|y}v_1 + d_1}
\qquad
\beta_{y} = \softmax{Y_{|x}v_2 + d_2}
\end{equation}

and uses these weights to get weighted summations of each sequence:
\begin{equation}
x_{\rm self} = X_{|y}^\top \beta_{x}
\qquad
y_{\rm self} = Y_{|x}^\top \beta_{y}
\end{equation}

The pooled representations are combined to get one joined representation for all inputs. 
\begin{align}
x_{\rm pool} &= \left [ \max(X_{|y}); {\rm mean}(X_{|y}); \min(X_{|y}); x_{\rm self} \right ]
\\
y_{\rm pool} &= \left [ \max(Y_{|x}); {\rm mean}(Y_{|x}); \min(Y_{|x});  y_{\rm self} \right ]
\end{align}

We feed this joined representation through a three-layer, 
batch-normalized~\citep{Ioffe2015BatchNA} maxout network~\citep{Goodfellow2013MaxoutN} 
to produce a probability distribution over possible classes.

\section{Question Answering with CoVe}

For question answering, 
we obtain sequences $x$ and $y$ just as we do in Eq.~\ref{eq:taskVectorsX} and Eq.~\ref{eq:taskVectorsY} for classification, except that the function $f$ is replaced with a function $g$ that uses a tanh activation instead of a ReLU activation.
In this case, one of the sequences is the document and the other the question in the question-document pair.
These sequences are then fed through the coattention and dynamic decoder implemented as in the original Dynamic Coattention Network (DCN) ~\citep{xiong2016dynamic}. 