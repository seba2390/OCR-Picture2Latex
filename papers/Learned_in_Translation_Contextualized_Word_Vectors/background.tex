\section{Related Work}

\textbf{Transfer Learning.}
Transfer learning, or domain adaptation, has been applied in a variety of areas where researchers identified synergistic relationships between independently collected datasets.~\citet{Saenko2010AdaptingVC} 
adapt object recognition models developed for one visual domain to new imaging conditions by learning a transformation that minimizes domain-induced changes in the feature distribution.~\citet{Zhu2011HeterogeneousTL}
use matrix factorization to incorporate textual information into tagged images to enhance image classification.
In natural language processing (NLP),~\citet{Collobert2011} leverage representations learned from unsupervised learning to improve performance on supervised tasks like named entity recognition, part-of-speech tagging, and chunking.
Recent work in NLP has continued in this direction by using pretrained word representations to improve models for
entailment~\citep{Bowman2014},
sentiment analysis~\citep{Socher2013EMNLP},
summarization~\citep{Nallapati2016AbstractiveTS},
and question answering~\citep{Seo2017BidirectionalAF,Xiong2017}.~\citet{Ramachandran2016UnsupervisedPF} propose initializing sequence-to-sequence models with pretrained language models and fine-tuning for a specific task.~\citet{Kiros2015SkipThoughtV} propose an unsupervised method for training an encoder that outputs sentence vectors that are predictive of surrounding sentences.
We also propose a method of transferring higher-level representations than word vectors, but we use a supervised method to train our sentence encoder and show that it improves models for text classification and question answering without fine-tuning.

\textbf{Neural Machine Translation.} 
Our source domain of transfer learning is machine translation, 
a task that has seen marked improvements in recent years with the advance of neural machine translation (NMT) models.~\citet{Sutskever2014} investigate sequence-to-sequence models that consist of a neural network encoder and decoder for machine translation.~\citet{Bahdanau2015} propose the augmenting sequence to sequence models with an attention mechanism that gives the decoder access to the encoder representations of the input sequence at each step of sequence generation.~\citet{Luong2015EffectiveAT} further study the effectiveness of various attention mechanisms with respect to machine translation.
Attention mechanisms have also been successfully applied to NLP tasks like
entailment~\citep{conneau2017supervised},
summarization~\citep{Nallapati2016AbstractiveTS},
question answering~\citep{Seo2017BidirectionalAF,Xiong2017,min2017question},
and semantic parsing~\citep{Dong2016LanguageTL}.
We show that attentional encoders trained for NMT transfer well to other NLP tasks.

\textbf{Transfer Learning and Machine Translation.}
Machine translation is a suitable source domain for transfer learning because the task,
by nature, 
requires the model to faithfully reproduce a sentence in the target language without losing information in the source language sentence.
Moreover, there is an abundance of machine translation data that can be used for transfer learning.~\citet{Hill2016LearningDR} study the effect of transferring from a variety of source domains to the semantic similarity tasks in~\citet{Agirre2014SemEval2014T1}.~\citet{Hill2017} further demonstrate that fixed-length representations obtained from NMT encoders outperform those obtained from monolingual (e.g. language modeling) encoders on semantic similarity tasks.
Unlike previous work, 
we do not transfer from fixed length representations produced by NMT encoders. 
Instead, we transfer representations for each token in the input sequence.
Our approach makes the transfer of the trained encoder more directly compatible with subsequent LSTMs, attention mechanisms, and, in general, layers that expect input sequences.
This additionally facilitates the transfer of sequential dependencies between encoder states.

\textbf{Transfer Learning in Computer Vision.} 
Since the success of CNNs on the ImageNet challenge, 
a number of approaches to computer vision tasks have relied on pretrained CNNs as off-the-shelf feature extractors.~\citet{girshick2014rich} show that using a pretrained CNN to extract features from region proposals improves object detection and semantic segmentation models.~\citet{qi2016hedged} propose a CNN-based object tracking framework, 
which uses hierarchical features from a pretrained CNN (VGG-19 by~\citet{simonyan2014very}).
For image captioning,~\citet{lu2016knowing} train a visual sentinel with a pretrained CNN and fine-tune the model with a smaller learning rate.
For VQA,~\citet{fukui2016multimodal} propose to combine text representations with visual representations extracted by a pretrained residual network~\citep{he2016deep}.
Although model transfer has seen widespread success in computer vision, 
transfer learning beyond pretrained word vectors is far less pervasive in NLP.
