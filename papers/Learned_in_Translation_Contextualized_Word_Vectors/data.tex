\section{Datasets}

\textbf{Machine Translation.} 
We use three different English-German machine translation datasets to train three separate MT-LSTMs. 
Each is tokenized using the Moses Toolkit~\citep{Koehn2007}.

Our smallest MT dataset comes from the WMT 2016 multi-modal translation shared task~\citep{Specia2016}.
The training set consists of 30,000 sentence pairs that briefly describe Flickr captions and is often referred to as Multi30k.
Due to the nature of image captions, this dataset contains sentences that are, on average, shorter and simpler than those from larger counterparts.

Our medium-sized MT dataset is the 2016 version of the machine translation task prepared for the International Workshop on Spoken Language Translation~\citep{cettolo2015iwslt}. 
The training set consists of 209,772 sentence pairs from transcribed TED presentations that cover a wide variety of topics with more conversational language than in the other two machine translation datasets.

Our largest MT dataset comes from the news translation shared task from WMT 2017.
The training set consists of roughly 7 million sentence pairs that comes from web crawl data, a news and commentary corpus, European Parliament proceedings, and European Union press releases.

We refer to the three MT datasets as MT-Small, MT-Medium, and MT-Large, respectively,
and we refer to context vectors from encoders trained on each in turn as CoVe-S, CoVe-M, and CoVe-L.
\bigskip

\begin{table}
  \centering
\begin{tabular}{llll}
    \toprule
Dataset & Task & Details & Examples\\
\midrule
 SST-2   &Sentiment Classification & 2 classes, single sentences        & 56.4k\\
 SST-5   & Sentiment Classification & 5 classes, single sentences           &94.2k \\
 IMDb    & Sentiment Classification & 2 classes, multiple sentences          &22.5k   \\
TREC-6  & Question Classification& 6 classes & 5k  \\
  TREC-50 & Question Classification& 50 classes &5k  \\
SNLI    &Entailment Classification &  2 classes        &  550k \\
SQuAD & Question Answering & open-ended (answer-spans)  & 87.6k  \\
\bottomrule
  \end{tabular}
      \caption{Datasets, tasks, details, and number of training examples.}
      \vspace{-0.6cm}
  \label{tasks}
\end{table}

\textbf{Sentiment Analysis.} 
We train our model separately on two sentiment analysis datasets: the Stanford Sentiment Treebank (SST)~\citep{Socher2013EMNLP} and the IMDb dataset~\citep{maas-EtAl:2011:ACL-HLT2011}.
Both of these datasets comprise movie reviews and their sentiment.
We use the binary version of each dataset as well as the five-class version of SST.
For training on SST, we use all sub-trees with length greater than $3$.
SST-2 contains roughly $56,400$ reviews after removing ``neutral'' examples. 
SST-5 contains roughly $94,200$ reviews and does include ``neutral'' examples.
IMDb contains $25,000$ multi-sentence reviews, which we truncate to the first $200$ words. $2,500$ reviews are held out for validation. 

\textbf{Question Classification.} 
For question classification, 
we use the small TREC dataset~\citep{voorhees1999trec} dataset of open-domain, fact-based questions divided into broad semantic categories.
We experiment with both the six-class and fifty-class versions of TREC,
which which refer to as TREC-6 and TREC-50, respectively.
We hold out $452$ examples for validation and leave $5,000$ for training.

\textbf{Entailment.}
For entailment, 
we use the Stanford Natural Language Inference Corpus (SNLI)~\citep{bowman2015snli}, 
which has 550,152 training, 
10,000 validation,
and 10,000 testing examples.
Each example consists of a premise, 
a hypothesis, 
and a label specifying whether the premise entails, contradicts, 
or is neutral with respect to the hypothesis.

\textbf{Question Answering.}
The Stanford Question Answering Dataset
(SQuAD)~\citep{rajpurkar2016squad} 
is a large-scale question answering dataset 
with 87,599 training examples,
10,570 development examples,
and a test set that is not released to the public.
Examples consist of question-answer pairs associated with a paragraph from the English Wikipedia.
SQuAD examples assume that the question is answerable and that the answer is contained verbatim somewhere in the paragraph.
