\begin{figure*}[ht]
    \centering
    \includegraphics[width=0.95\textwidth]{images/framework.pdf}
    \caption{\textbf{Overview of framework.} Given a query sample $\mathbf{x}$, a latent representation $\mathbf{z}$ is derived by sampling from the variational posterior, the parameters of which are $\mu$ and $\sigma$. This embedding is further enhanced via a modulation process to get $\hat{\mathbf{z}}$. The latent embedding is used for classifying the sample into one of the few-shot classes. The decoder reconstructs the exemplar, $\mathbf{t}$ associated with the sample's class. The exemplar reconstruction error, the modulated embedding, and the classification scores themselves are fed to an MLP-based out-of-distribution detector to predict the probability $p_{\eta}$ of whether $\mathbf{x}$ is in/out-of-distribution with respect to the few-shot classes.}
    \vskip -0.1in
    \label{fig:framework}
\end{figure*}

\section{Methodology}
In this section, we present our framework for open-set few-shot classification. We first provide a  definition of the problem, followed by a brief overall methodology, and then present a detailed description of our framework.

\subsubsection*{Problem setting} 
Consider the standard few-shot learning setting, where we have access to a support set of labeled examples $\mathbb{S}=\left\{\mathrm{S}_1,\dots,\mathrm{S}_N\right\}$. Each $\mathrm{S}_c=\{\mathbf{x}_i\}_{i=1}^K$ denotes a set of $K$ examples belonging to the class $y_c$, for $N$-way $K$-shot recognition.  We make two changes to this setup. First, we assume the existence of class-specific exemplar images $\mathbf{t}_c$ for each $\mathbb{S}_c$; in case exemplars are not present, we estimate a class-specific exemplar for all the categories.  Second, the query set $\mathbb{Q}$ is comprised of both in-distribution and out-of-distribution samples w.r.t $\mathbb{S}$, i.e., $\mathbb{Q} = \mathbb{Q}_{in}\cup\mathbb{Q}_{out}$. In-distribution samples belong to classes seen in the support set while out-of-distribution samples belong to unseen classes. The goal is to detect the samples in $\mathbb{Q}_{out}$ as out-of-distribution, while correctly classifying the samples in $\mathbb{Q}_{in}$.

In order to develop a strong prior for few-shot learning, we utilize a base training set of labeled samples $\mathcal{B}=\{(\mathcal{X}_c,\mathcal{Y}_c)\}_{c=1}^M$ to meta-train our framework, where $M$ is the set of all classes in $\mathcal{B}$, $\mathcal{X}_c=\{\mathbf{x}_1,...,\mathbf{x}_{|c|} \}$ is the set of images in class $c$ and $\mathcal{Y}_c$ denotes the $c^{th}$ class label. If class-specific exemplars $\mathbf{t}_c$ are provided, we add them to $\mathcal{B}$ to obtain $\mathcal{\hat{B}}=\{(\mathcal{X}_c,\mathcal{Y}_c,\mathbf{t}_c)\}_{c=1}^M$, otherwise we first estimate $\mathbf{t}_c$ from $\mathcal{B}$ as shown in section 3.5 and then add them to $\mathcal{B}$ to get $\mathcal{\hat{B}}$ in a similar fashion. Like prior work \cite{proto_net,match_net} a set of training tasks or episodes $\{\mathcal{T}_i,\mathcal{T}_2,... \}$ are sampled from $\mathcal{\hat{B}}$ to conduct meta-training of the model. 
For an $N$ way $K$ shot problem, each episode $\mathcal{T}$ is constructed by first randomly sampling $N$ classes from $B$ and then constructing a support set $\mathbb{S}$ comprised of $K$ randomly chosen samples from each of the $N$ classes. Along with $\mathbb{S}$, a query set $\mathbb{Q}$ is constructed in a similar fashion. In order to simulate the presence of out-of-distribution samples, we follow the strategy outlined in \cite{peeler} and augment the query set with samples from classes \textit{absent} in the support.

\subsubsection*{Overall framework}
 A pictorial description of our framework is shown in Fig. \ref{fig:framework}. Given a sample $\mathbf{x}$, we first use variational inference to reconstruct the possible exemplar $\mathbf{t}$ associated with the ground-truth class of $\mathbf{x}$ and simultaneously obtain a latent representation of the same sample. This embedding is used to obtain a classification score, similar to \cite{chen2020new,match_net}, while the reconstructed exemplar is used as a proxy for the out-of-distribution detection task. Specifically, if $\mathbf{x} \in \mathbb{Q}_{out}$, we hypothesize that the reconstruction will fail for all the exemplars of the support classes. Based on this hypothesis, the latent representation, classification scores, and reconstruction errors (with respect to support exemplars) are subsequently fed into a binary classifier to predict the probability of the sample $\mathbf{x}$ being out-of-distribution. 
 
\subsection{Exemplar reconstruction from real images} \label{rec_mod}
% The reconstruction \sayak{unit} has a two-fold objective: given an input sample $\textbf{x}$, learn a latent representation $\textbf{z}$ which will be used to classify the sample and, secondly, utilize the same representation to reconstruct its corresponding class-specific exemplar image $\mathbf{t}$ via the decoder. 
Since the class-specific exemplars $\mathbf{t}$ form a compact representation of the real world images belonging to that class, we hypothesize that the ability to reconstruct any exemplar belonging to in-distribution classes correlates positively with the sample being an in-distribution one. Inspired by \cite{kim2019variational}, we use a Variational Auto-Encoder (VAE) \cite{Kingma2014} for the purpose of such reconstruction. The choice of VAE is motivated by its robustness to outliers and better generalization to unseen data, as shown in \cite{dai2018connections}. This is ideal for the image-to-exemplar translation task where the exemplars lie on a canonical domain devoid of the perturbations in real images. 

Given an input sample $\mathbf{x}$, its exemplar reconstruction of $\mathbf{t}$ is carried out by maximizing the variational lower bound of the likelihood $p({\mathbf{t}})$ \cite{kim2019variational} as follows,
\begin{equation}\label{elbo}
     \log p(\mathbf{t}) 
     \geq \mathbb{E}_{q_{\phi}(\mathbf{z}|\mathbf{x})} [\log   p_{\theta}(\mathbf{t}|\mathbf{z})] - D_{KL}[q_{\phi}(\mathbf{z}|\mathbf{x})||p(\mathbf{z})]
\end{equation}
where $D_{KL}[.]$ is the Kullback-Leibler (KL) divergence and $q_{\phi}(\mathbf{z}|{\mathbf{x}})$ denotes the variational distribution introduced to approximate the intractable posterior. Note that this is different from a vanilla VAE \cite{Kingma2014}, which is derived by maximizing the log-likelihood of the input data $\mathbf{x}$. A differentiable version of the lower bound is derived by assuming the latent variable $\mathbf{z}$ to be Gaussian in nature, sampled from the prior $q_{\phi}(\mathbf{z}|{\mathbf{x}})$. The empirical loss to be minimized is given as follows, 
\begin{equation}\label{recon_loss}
    \mathcal{L}_{VAE} = \frac{1}{K_e}\sum_{i=1}^{K_e}-\text{log}\ p_{\theta}({\mathbf{t}}_i|\mathbf{z}_i)+D_{KL}[q_{\phi}(\mathbf{z}|{\mathbf{x}})||p(\mathbf{z})]
\end{equation}
where $K_e$ is the number of samples in one episode, i.e., $K_e=|\mathbb{S}\cup\mathbb{Q}_{in}|$. Since $\mathbf{z} \sim q_{\phi}(\mathbf{z}|{\mathbf{x}})$ is non-differentiable, the re-parameterization trick is applied via the decoder network \cite{Kingma2014} such that $\mathbf{z} = \mathbf{\mu} + 
\mathbf{\sigma}\odot\mathbf{\epsilon}$, where $\mathbf{\epsilon} \sim \mathcal{N}(0,\textbf{I})$ and $\odot$ is the Hadamard product. 

The first term in Eq. \ref{recon_loss} is the reconstruction loss which affects the mapping of the real images to their class-specific exemplars, while the second term acts as a distribution regularization, enforcing the latent variable $\mathbf{z}$ to follow the chosen prior. Although binary cross entropy (BCE) is the common choice for the reconstruction loss in VAE, other losses such as $\ell_1$ or $\ell_2$ norm can also be used.

\subsection{Few-shot classification} \label{clf_mod}
The latent representation of a query sample $\mathbf{z}_q$, obtained from the encoder in the previous section, is used for computing the classification scores. We use the cosine metric to compute a relation score between the query sample and the support set $\mathbb{S}$. Specifically, the relation score is obtained by computing the cosine similarity between $\mathbf{z}_q$ and the set of prototypes or centroids, $\{\mathbf{\Omega}_{c}\}_{c=1}^{N}$ for each class $c \in \mathbb{S}$ ( Fig. \ref{fig:framework}). The classification score for $\mathbf{x}_q$ is obtained by performing a softmax operation on these relation scores. In contrast to prior works \cite{proto_net,peeler}, the class specific prototype $\mathbf{\Omega}_{c}$ is obtained by a weighted mean of the support samples instead of a simple mean. Note that these prototypes are different from the class specific exemplars $\mathbf{t}$.

\subsubsection*{Prototype computation} Given the support set, the prototypes for each class $c$ are calculated as follows,
\begin{equation}
    \mathbf{\Omega}_{c} = \sum\limits_{k=1}^{K}\omega_k \cdot \mathbf{z}_k^{c}
\end{equation}
$\mathbf{z}_k^{c}$ denotes the latent representation of $\mathbf{x}^k \in \mathbb{S}_c$, while
$\omega_k$ is the weight assigned to $\mathbf{z}_k^{c}$ based on how close it is from the embedding of the exemplar belonging to the $c^{th}$ class, $\mathbf{z}_t^{c}$,
\begin{equation}\label{proto_compute}
    \omega_k = \frac{e^{\cos(\mathbf{z}_k^{c},\mathbf{z}_t^{c})}}{\sum\limits_{k=1}^{K}e^{\cos(\mathbf{z}_k^{c},\mathbf{z}_t^{c})}}
\end{equation}
We use these weights in an effort to control the phenomena of \textit{intra-class bias} \cite{proto_rectify}, i.e.,  the difference between the true expected prototype and the Monte-Carlo estimated value. As explained earlier, each of the exemplars are an effective ideogram which provide a good abstraction of their respective classes. Thus the image-to-exemplar translation learned by the VAE leads to a feature space where the embedding of the real images cluster around that of their corresponding exemplars. This makes the exemplars good approximation of the true prototype and we leverage this via the weights $\omega_k$ to alleviate the intra-class bias.

\subsubsection*{Classification} After computing the prototypes, we predict the classification scores for the query sample $x_q$ as follows,
\begin{equation}
    p_{\phi}(y=c|\mathbf{x}_q) = \frac{e^{\tau \cdot cos(\mathbf{z}_q,\mathbf{\Omega}_c)}}{\sum\limits_{c^{'}}e^{\tau \cdot
    cos(\mathbf{z}_q,\mathbf{\Omega}_{c^{'}})}}
    \label{eq:classifier_prob}
\end{equation}
where $\tau$ is a learnable temperature parameter to scale the logits computed by cosine similarity  \cite{chen2020new,gidaris2018dynamic}. Learning proceeds by minimizing a cross-entropy loss over the in-distribution classes as follows:
\vspace{1pt}
\begin{equation}
    \mathcal{L}_{CE} =-\frac{1}{|\mathbb{Q}_{in}|}\sum_{i=1}^{|\mathbb{Q}_{in}|} \sum_{c=1}^{N} \mathbbm{1}{\{y_i=c\}}\log \ p_{\phi}(y=c|\mathbf{x}_{q,i})
    \label{eq:ce}
\end{equation}
where $y_i$ represents the true class of the query sample.

\subsection{Out of distribution detection}
Unlike prior work, we do not rely solely on the classification score to detect out-of-distribution query samples. Instead, ReFOCS flags query samples by leveraging the output of a multi-layer perceptron (MLP) classifier. This binary classifier takes into account three sources of information for scoring the openness of a query sample. These sources are (i) the class probability $\mathbf{p}_{\phi}$ as predicted in Eq. \ref{eq:classifier_prob}, (ii) a modulated version of the latent representation $\mathbf{z}$ (described below), and (iii) the set of reconstruction errors with respect to the support set exemplars, $\mathbf{D} =\left[||\hat{\mathbf{t}} -  \mathbf{t}_1||_F^2,...,||\hat{\mathbf{t}} -  \mathbf{t}_N||_F^2\right]$, where $\mathbf{D}$ indicates how far the reconstructed exemplar $\hat{\mathbf{t}}$ deviates from the actual exemplar $\mathbf{t} \in \mathbb{S}$. Intuitively, for out-of-distribution queries, all the entries of $\mathbf{D}$ will be very high, while for in-distribution samples, at least one of them will be very small. 

\subsubsection*{Embedding Modulation} At a fine-grained level, samples from many of the out-of-distribution classes can have very similar visual features to some of the in-distribution classes (Fig \ref{fig:softmax_issues}. This issue becomes more relevant for the few-shot setting since the model does not have access to large amounts of samples from the in-distribution classes for generalization. Therefore, given only a handful of samples from the in-distribution classes, it is of paramount importance to obtain an embedding that is discriminative enough to provide good segregation between in-distribution and out-of-distribution classes. This, in turn, will help the MLP in better detecting the out-of-distribution samples. While the latent embedding obtained from the VAE does have good discriminative properties, we introduce an additional modulation step that can enhance it even further. The enhanced embedding, $\mathbf{\hat{z}_q}$, is obtained by scaling the embedding of a query sample $\mathbf{z}_q$ with a scalar $\kappa > 0$ as shown  below,
\begin{equation}
    \mathbf{\hat{z}_q}=\frac{\mathbf{z}_q}{\kappa}, \ \  \text{where} \ \kappa = \min_{c \ \in \  \mathbb{S}}||\mathbf{z}_q - \mathbf{\Omega}_c||_1
\end{equation}
%\begin{equation}
%    \text{where} \ \ \ \gamma = \min_{c \ \in \  %\mathbb{S}}||\mathbf{z}_q - \mathbf{z}_{\mathbf{t}}^{c}||_1
%\end{equation}
where $c \in \{1,...N\}$ represents the classes in the support and $\mathbf{\Omega}_c$ is the prototype corresponding to the $c^{th}$ class. $\kappa$ is a modulation factor measuring how close a query sample is to any of the in-distribution classes in the embedding space \cite{Savinov2019_EC}. out-of-distribution samples will tend to have higher values for $\kappa$ compared to in-distribution samples. Therefore this form of modulation will amplify the embeddings of in-distribution samples while scaling them down for out-of-distribution queries.

% Since the samples belonging to a specific class tend to cluster around the embedding of its exemplar and therefore the prototype, out-of-distribution samples will have a higher value for $\kappa$ compared to in-distribution samples, thereby, amplifying the embedding value for the in-distribution queries while scaling it down for out-of-distribution queries. 

Therefore, the final input to the out-of-distribution detecting MLP is the concatenated vector $[\boldsymbol{\mathrm{p}}_{\phi}, \boldsymbol{\hat{\mathrm{z}}}_q, \mathbf{D}]$. This MLP classifier outputs a sigmoidal probability value $p_{\eta}$, indicating the openness of a query sample. Training proceeds by minimizing binary cross-entropy loss as follows,
 \begin{equation}
     \mathcal{L}_{BCE} = -\frac{1}{|\mathbb{Q}|}\sum\limits_{i=1}^{|\mathbb{Q}|}y_{\eta,i}\log \ p_{\eta,i}+(1-y_{\eta,i})\log (1-p_{\eta,i})
     \label{eq:bce}
 \end{equation}
where $y_{\eta}$ is equal to $0$ or $1$ depending on whether $x_q \in \mathbb{Q}_{in}$ or $x_q \in \mathbb{Q}_{out}$ respectively. 

\subsection{Training} \label{sec:training}
The parameters of the Encoder ($\phi$), Decoder ($\theta$) and the out-of-distribution detector ($\eta$) are jointly meta-trained by optimizing over the aggregate loss $\mathcal{L}$,
\vspace{1pt}
\begin{equation}\label{loss_func}
    \mathcal{L} = \lambda_1\mathcal{L}_{VAE}+\lambda_2\mathcal{L}_{CE}+\lambda_3\mathcal{L}_{BCE}
\end{equation}
where $\lambda_1$, $\lambda_2$ and $\lambda_3$ are hyper-parameters choices of which is discussed in Section \ref{Experiments}.

\subsection{Estimation of exemplars}
While it is easy to obtain well-defined exemplar images for images of graphical symbols, such class-specific exemplars may not always be provided for all kinds of natural images. We perform a simple exemplar estimation for categories that have missing exemplar images. First, we perform non-episodic training of the VAE encoder, $f_{\phi}$ on the entire base set $\mathcal{B}$. Second, the category-wise samples in $\mathcal{B}$ are passed through the pre-trained encoder to obtain corresponding feature representations, $f_{\phi}(\mathbf{x}_c)$ from the penultimate layer of the encoder. Finally, the exemplar image is defined via a nearest neighbor scheme in the feature space as follows,
\begin{equation}
    \mathbf{t}_{c} = \arg\min_{\mathbf{x} \in \mathcal{X}_c} ||f_{\phi}(\mathbf{x}) - \Psi_{c}||_2.
\end{equation}
Here, $\Psi_c = \frac{1}{|\mathcal{X}_c|}\sum\limits_{\mathbf{x} \in \mathcal{X}}{f_{\phi}(\mathbf{x})}$ is the centroid of the $c^{th}$ training class in the feature space.

For test episodes, we calculate the exemplar in a similar fashion by selecting the support sample closest to its centroid representation in the feature space.
% For images of graphical symbols there exists well-defined exemplars which provide a good abstraction of it's corresponding class. However, such class-specific exemplars need not be always provided for all natural images. In that case, we first perform non-episodic training of the VAE encoder, $f_{\phi}$ on the entire base set $\mathcal{B}$. Following this, the category wise samples in $\mathcal{B}$ are passed through the pre-trained encoder and their corresponding feature representations, $f_{\phi}(\mathbf{x}_c)$ are extracted from the penultimate layer of the encoder. The class-specific exemplar is then computed as follows,
% \begin{equation}
%     \mathbf{t}_{c} = \min_{c \in \mathcal{B}} ||f_{\phi}(\mathbf{x}_{c}) - \Psi_{c}||_2
% \end{equation}
% where, $\Psi_c = \frac{1}{|\mathcal{B}_c|}\sum\limits_{c \in \mathcal{B}}{f_{\phi}(\mathbf{x}_c)}$ is the centroid of the $c^{th}$ training class in the feature space. 

% For test episodes we simply compute a centroid for the support samples using the representation from the meta-trained encoder and choose the support sample closest to this centroid as the exemplar for that particular test episode.

% \noindent \textbf{Training without given exemplars.} The class-specific exemplars used in our framework need not be always provided as images. In that case, we pass the category wise samples through a ResNet-18 model pre-trained on ImageNet \cite{imagenet} and extract the features from the penultimate fully-connected layer. We then compute the centroid of each class in the feature-space and choose the image whose representation is closest to it as the exemplar of that class.

% \noindent \textbf{Testing without given exemplars.} In case we do not have exemplars in the test set, we estimate it directly from the support set. We pass the support examples through the encoder to obtain their latent space embedding and compute their mean embedding.  We then choose the support sample closest to this centroid as the exemplar of that class and the rest of the process remains the same. In case of $1$-shot recognition we use the support sample itself as the exemplar.

