\input{tables/table1_sampling}
\section{Experiments}\label{Experiments}
In this section, we provide comprehensive experiments over several data sets to prove the efficacy of our framework, ReFOCS, for few-shot open-set classification. We compare the performance of ReFOCS with existing state-of-the-art methods that rely on thresholding approaches and softmax re-calibration for the detection of out-of-distribution samples. The overall results show that our method outperforms or achieves comparable performance to existing methods, without relying on the need for carefully tuned thresholds.

% The goal of our evaluation is to answer the following question: \emph{can the proposed framework detect out-of-distribution samples in the few-shot setting in conjunction to correctly classifying the in-distribution samples?} We answer this question in the affirmative through comprehensive experiments over several data sets, which establish our method as the clear state-of-the-art. 

\subsubsection*{Datasets} 
We use six datasets to set up three different types of few-shot open-set recognition experiments: (i) \emph{traffic sign recognition}, for which we use the GTSRB \cite{gtsrb} and TT100K \cite{tt100k} datasets, (ii) \emph{brand logo recognition}, for which we use BelgaLogos \cite{belga1,belga2}, FlickrLogos-32 \cite{flickr} and TopLogo-10 \cite{toplogo}, and (iii) \emph{natural image classification} on the benchmark \textit{mini}ImageNet dataset \cite{match_net}. Different base training and meta-testing sets  are configured from these datasets to obtain five few-shot scenarios as shown in Tables \ref{tab:traffic}, \ref{tab:logo} and \ref{tab:miniimagenet}. Some of these scenarios involve cross-dataset experimentation, which is more challenging compared to using splits from the same dataset and better mimics real-world scenarios where training and test data can have significant domain shifts \cite{tseng2020cross}. It must be noted that all the traffic sign and logo datasets are provided with well-defined canonical exemplars for each of the classes. For these datasets the canonical exemplars can be directly used for the reconstruction task. To show the efficacy of our framework, we show results with both estimated and canonical exemplars for the traffic sign and logo datasets. The estimated exemplars are obtained following the strategy in section 3.5. Since \textit{mini}ImageNet is not provided with any well-defined exemplars, all of its class-specific exemplars are estimated. For the traffic sign and natural image datasets, we evaluate our model on both the $5$-way $5$-shot and the $5$-way $1$-shot tasks, while for the logo classification task, we show results only on the $5$-way $1$-shot scenario. This is due to BelgaLogos and Toplogos having multiple classes with less than $5$ samples. More details on these datasets and splits can be found in the supplementary material. 



 % \subsubsection*{Sampling Strategies}
 %    The episodic sampling strategies for all the few-shot experiments are shown in Table \ref{tab:sampling}. For an $\text{N}$-way $\text{K}$ shot problem each episode $\mathcal{T}$ has $\mathbf{K}_{\mathbb{Q}_{in}}^c$ number of in-distribution queries sampled from each support class $c \in \{1,..,N\}$. Therefore, the total number in-distribution query samples per episode  $\mathbf{K}_{\mathbb{Q}_{in}} = \sum\limits_{c=1}^N\mathbf{K}_{\mathbb{Q}_{in}}^c$. Along with the in-distribution samples, the query set of each episode is also augmented with $\mathbf{K}_{\mathbb{Q}_{out}}$ samples which do not belong to any of the support classes i.e. out-of-distribution. Hence, the total number of samples $\mathbf{K}_{\mathbf{e}}$, in each episode is given as, $\mathbf{K}_{\mathbf{e}}=\mathbf{K}+\mathbf{K}_{\mathbb{Q}_{in}}+\mathbf{K}_{\mathbb{Q}_{out}}$ where, $\mathbf{K} = |\mathbb{S}|$.  The model is trained on $\mathbf{E}_{train}$ number of episodes each epoch and meta-tested on $\mathbf{E}_{test}$ number of test episodes. 



\subsubsection*{Implementation} 
The episodic sampling strategies for all the few-shot experiments are shown in Table \ref{tab:sampling}.
For the traffic sign and logo classification experiments, the VAE architecture is adapted from \cite{kim2019variational}. For the natural image classification task, we experiment with different resnet architectures \cite{resnet18} for the VAE encoder. In each case, the decoder is designed as an inverted version of the encoder architecture, for e.g., if Resnet18 \cite{resnet18} is used as the encoder, the decoder is designed as an inverted Resnet18. For all experiments, the MLP-based out-of-distribution detector is designed with two hidden layers, each containing $200$ and $100$ nodes, respectively. For a fair comparison, the encoder network is kept the same for all competing methods. The Adam optimizer \cite{kingma2017adam} is used for all the experiments. For all the traffic sign and brand logo recognition experiments, the values of the loss function hyperparameters, $\lambda_1,\lambda_2$ and $\lambda_3$, are set to $10^{-4},10$ and $10$ respectively. For the natural image classification task on \textit{mini}ImageNet $\lambda_1,\lambda_2$ and $\lambda_3$, are set to $10^{-4},1$ and $10$ respectively. The initial learning rate is set to $10^{-4}$ for both the traffic sign and brand logo experiments and to $10^{-3}$ for the natural image classification task. When using canonical exemplars for   the traffic sign and brand logo experiments the standard BCE criterion is used as the VAE reconstruction loss. On the other hand, for all experiments involving the estimated exemplars, we observed  using the $\ell_2$ norm as the reconstruction loss results in more improved performance. Additional implementation details are provided in the supplementary.

% Similar to \cite{kim2019variational}, the embedding size is set to $300$ for the traffic sign and logo datasets, while the ResNet-18 encoder is used to obtain a $512$ dimensional feature representation for the natural image classification task. 

\input{tables/table2_traffic_results}
\input{tables/table3_logorecog}


\subsubsection*{Baselines} We compare ReFOCS against existing FSOSR methods such as PEELER \cite{peeler} and SNATCHER \cite{snatcher}. Both these methods are built on top of the Prototypical Networks (ProtoNet) \cite{proto_net} and rely on thresholding to reject out-of-distribution samples. Therefore, we also compare the performance of ReFOCS to that of ProtoNet, which although not designed for open-set recognition, provides an approximate lower bound on FSOSR performance. We also compare against OpenMax \cite{open_max}, which was originally proposed for open-set recognition in the fully supervised setting. OpenMax fits a Weibull distribution on the classification logits and utilizes its parameters to recalibrate the final softmax classification score. In order to adapt it for the few-shot setting, we implement OpenMax over the standard ProtoNet. We henceforth denote this baseline as Proto$+$OM. For both ProtoNet and Proto$+$OM, the out-of-distribution samples are rejected by using a the same thresholding principle as PEELER.  



\subsubsection*{Evaluation Metrics} To quantify the closed-set classification performance, we compute the accuracy over in-distribution queries and utilize the Area Under the Receiver Operating Characteristic curve (AUROC) to quantify the model's performance in detecting the out-of-distribution samples. For all our evaluations, we split the query set equally among in-distribution and out-of-distribution samples.



\subsection{Traffic Sign Recognition}
The performance of ReFOCS on the two traffic sign recognition tasks, GTSRB$\rightarrow$GTSRB and GTSRB$\rightarrow$TT100K, is shown in Table \ref{tab:traffic}. GTSRB and TT100K have a total of $43$ and $36$ classes, respectively. For GTSRB$\rightarrow$GTSRB, $22$ classes are used for meta training, and the remaining $21$ are used for meta testing. For GTSRB$\rightarrow$TT100K, all classes of GTSRB are used for training, and testing is done on all the classes of TT100K. In both experiments, all images are resized to $64 \times 64$. As shown in Table \ref{tab:traffic}, ReFOCS outperforms all baselines using both estimated and canonical exemplars. Specifically for the task of detecting out-of-distribution samples, on average, ReFOCS achieves $\mathbf{7.4}$ percentage points higher AUROC compared to PEELER while using the estimated exemplars. On the other hand, using the well-defined canonical exemplars yields even greater performance gains up to nearly $\mathbf{10.4}$ percentage points compared to PEELER. The problem of just thresholding softmax classification scores to indicate the openness of samples can be clearly seen in the lower AUROC values of ProtoNet, Proto$+$OM, and PEELER. The issue is more pronounced for ProtoNet, since, unlike PEELER and Proto$+$OM, ProtoNet does not explicitly regularize the probability scores of out-of-distribution samples resulting in even more high-confidence false predictions. Utilizing our proposed prototype computation (Eq. \ref{proto_compute}) also results in higher classification accuracy on most of the FSOSR tasks. Since the canonical exemplars lie in a simpler domain they are devoid of many background and lighting perturbations seen in the estimated exemplars. As a result, the image-to-exemplar translation is much simpler for canonical exemplars compared to the estimated ones, which results in higher reconstruction errors for the latter, in turn affecting the out-of-distribution detection. This factor is even more amplified if there is a domain shift between the base training and meta-testing sets as in the case of GTSRB$\rightarrow$TT100K, which is why is comparison to GTSRB$\rightarrow$GTSRB, using the estimated exemplars over the canonical ones leads to a larger drop in AUROC for GTSRB$\rightarrow$TT100K. Nevertheless, the VAE-based reconstruction and our entire learning setup enable ReFOCs to better compensate for such domain shifts in comparison to existing FSOSR methods leading to significant increases in AUROC even while using the estimated exemplars.  Some sample exemplar reconstructions for the canonical exemplar case are provided in Fig \ref{fig:exemp_recon}.

\input{images/reconstruction_qualitative_gtsrb}
\input{tables/table_miniimagenet}
\input{tables/table4_ablations}

\subsection{Brand Logo Recognition}
The brand logo datasets consist of everyday images of commercial brand logos. In both few-shot tasks, Belga$\rightarrow$Flickr32 and Belga$\rightarrow$Toplogos, the Belga dataset consisting of $37$ classes, is used for meta-training. Flickr32 and Toplogos each have $32$ and $11$ classes, respectively. Since some of the classes have as low as $2$ samples, we restrict our experiments to the $5$-way $1$-shot scenario. In both cases, all images are resized to $64 \times 64$. Similar to the traffic sign recognition experiments, ReFOCS outperforms the existing baselines as evident from the results shown in  Table \ref{tab:logo}. Specifically, while using estimated exemplars, on average, ReFOCS achieves nearly $\mathbf{1.89}$ percentage points higher AUROC compared to PEELER and with the canonical exemplars, the average AUROC gains are $\mathbf{4.28}$ percentage points. The overall accuracy is also significantly higher compared to the  baselines. Specifically, when using the canonical exemplars we observe an average increase of $\mathbf{3.05}$ percentage points in accuracy over PEELER. These results establish the superiority of ReFOCS for FSOSR. Similar to the GTSRB$\rightarrow$TT100K, the domain shift between BelgaLogos and the other two brand logo datasets increases the difficulty of the task, especially when using the estimated exemplars. However, the overall results clearly show the superiority of ReFOCS over existing FSOSR methods in compensating for such domain shifts. 

% From the results shown in Table \ref{tab:logo}, it is evident that ReFOCS achieves significant gains in both accuracy and AUROC, outperforming PEELER \cite{peeler} by an average of $\mathbf{4.28}\%$ in AUROC and $\mathbf{3.05}\%$ in classification accuracy. This clearly highlights the superiority of our approach for few-shot open-set recognition. The in-distribution classification accuracy, although better than the competing baselines, is lower in comparison to the traffic sign datasets, which is due to the lower quality of the training set, as pointed out in \cite{kim2019variational}. In addition, there is a significant domain gap between the images and exemplars of Belga and that of Toplogos, which makes it even more difficult to achieve good classification accuracy.


\subsection{Natural Image Classification}
The \textit{mini}ImageNet dataset, introduced in \cite{match_net}, is a benchmark dataset used for evaluating the performance of models for few-shot natural image classification. This dataset has a total of $100$ classes, and as per the splits introduced in \cite{match_net}, $64$ classes are used for training, $16$ for validation, and the remaining $20$ for testing. The images are resized to the standard resolution of $84 \times 84$ \cite{match_net}. As mentioned previously, this dataset does not have any categorical exemplars, and therefore, for all its experiments, we use the estimated exemplars. As shown in Table \ref{tab:miniimagenet}, for this dataset, we show results with different encoder or backbone architectures. Specifically for the Resnet12 encoder, it is first pre-trained for all competing methods using the same setup as \cite{snatcher}. Following this, meta-training starts using the pre-trained weights of the encoder. From the overall results, we can observe that ReFOCS outperforms or achieves comparable performance to existing thresholding-based FSOSR methods. Since the images in \textit{mini}ImageNet have much more variations, methods like Proto$+$OM fail to achieve a decent AUROC score. This is because the Weibull distribution used by OpenMax \cite{open_max} under-fits to the handful of support samples for each of the novel test categories occurring in the meta-testing phase, which in turn renders it ineffective in recalibrating the softmax classification scores of the underlying Proto-Net. Since ReFOCS does not rely solely on the softmax scores for out-of-distribution detection it is able to achieve significantly higher AUROC scores. Using the pre-trained Resnet12 encoder for the meta-training phase boosts ReFOCS's performance even further. Pre-training also helps improve the performance of PEELER, although with the Resnet18 backbone, it fails to achieve its reported performance when the results are generated by using its official implementation  \footnote{\label{url}\url{https://github.com/BoLiu-SVCL/meta-open/}}.

In comparison to the variants of SNATCHER \cite{snatcher}, our framework is able to achieve comparable performance, even outperforming them in some cases. Since the authors of SNATCHER did not release their code we are able to compare with it for \textit{mini}ImageNet benchmark only, and its results shown in Table \ref{tab:miniimagenet} are taken directly from their paper \cite{snatcher}. Although SNATCHER is able to effectively use complex architectures like transformers \cite{vaswani2017attention} and carefully crafted normalization schemes to improve performance over PEELER, being a thresholding type FSOSR method, it is highly sensitive to the choice of an appropriate threshold. In contrast, ReFOCS eliminates the need for a hand-tuned threshold by utilizing the exemplar reconstruction setup to induce self-awareness in the model, making it more versatile than existing FSOSR baselines.
% For both the $5$-way $5$-shot and $5$-way $1$-shot scenarios, ReFOCS is meta-trained with $50000$ episodes. It must be noted that the estimated exemplars are not on a canonical domain unlike those of the traffic sign and brand logo datasets. This makes the image-to-image translation for exemplar reconstruction even more challenging, specifically due to the presence of varying lighting and color contrasts in the real domain. Consequently, the standard BCE reconstruction loss is not able to compensate for these perturbations, and thus, we use the $\ell2$ norm as the reconstruction criterion, which leads to a marked improvement in the results. 
 % From Table \ref{tab:miniimagenet}, we can clearly see that ReFOCS outperforms the baselines especially for the open sample detection as evidenced by the higher AUROC scores. Many of the \textit{mini}ImageNet objects have fine-grained visual differences (eg: cats and dogs) - this makes ProtoNet, which relies on the raw softmax score, prone to misclassification of the unseen categories as one of the seen ones. This is reflected in the performance of ProtoNet as it is barely able to achieve better than random AUROC (Table \ref{tab:traffic}). Unlike the prior experiments, Proto$+$OM fails to achieve a decent AUROC score. This is predominantly due to the failure of OpenMax to fit a Weibull distribution with just a handful of samples for each of the novel test categories during the meta-testing phase. As a result, OpenMax fails to recalibrate the ProtoNet's logits with such a small number of support samples. Note that PEELER fails to achieve the reported performance on the \textit{mini}ImageNet experiments \cite{peeler} when results are generated by using the official implementation  \footnote{\label{url}\url{https://github.com/BoLiu-SVCL/meta-open/}}. 


\begin{figure*}[ht]
\centering
\includegraphics[width=0.9\textwidth]{images/tsne.pdf}
\caption{\textbf{t-SNE visualization.} We project the latent space learned via (a) ProtoNet (b) PEELER (c) ReFOCS, on $5$ classes of GTSRB$\rightarrow$TT100K, on a 2D space using t-SNE. Out-of-distribution queries are in yellow}
% \vskip -0.1in
\label{fig:tsne}
\end{figure*}

\subsection{Ablation Studies}
In this section, we use two cross-dataset scenarios to perform ablation studies of different components of our framework to understand their contribution toward the final performance.

\subsubsection*{Impact of using variational encoding.} The reconstruction module of ReFOCS is designed using a VAE \cite{kim2019variational} due to its better generalization ability \cite{dai2018connections}. We highlight this by replacing the VAE and experimenting with a standard auto-encoder (AE). As shown in Table \ref{tab:ablation}, both the classification and out-of-distribution detection performance drops significantly. This shows that the improved generalization capability of a VAE is necessary for handling FSOSR scenarios with domain shifts between the training and testing sets.

\subsubsection*{Impact of weighted prototype computation.}
By comparing the second and last row of Table \ref{tab:ablation} we can see that when our weighted prototype is replaced with a simple one as \cite{proto_net}, there is a considerable drop in the classification performance. This shows that utilizing the exemplar information in computing the weighted prototype results in more unbiased prototypical representations of each class, in turn facilitating improved metric-based classification of the in-distribution query samples.

\subsubsection*{Impact of embedding modulation.} The importance of the embedding modulation can be clearly seen in Table \ref{tab:ablation}. The modulated embedding results in more distinct feature representations with adequate segregation between the seen and unseen classes, thereby making it easier for the novelty module to flag unseen categories. Removing it results in a significant drop in AUROC, and on the other hand, its presence also amplifies the classification performance. This suggests that the improvement of the open class detection is complemented by improvement in seen class categorization.

\subsubsection*{Input to the Out-of-distribution detector.} We feed a concatenation of the modulated embedding, classification score, and $\ell_2$ reconstruction errors to the out-of-distribution detector. Empirically, the combination of all three gives the best results for out-of-distribution detection. We study the impact of removing the classification score or the embedding from the input. As seen from the results in Table \ref{tab:ablation}, in both cases, there is a drop in both the classification accuracy and the AUROC score. Additionally, we also examine the scenario when we do not use any exemplar for reconstruction and, consequently, feed in just the classification score and the raw embedding as input to the novelty module. We call this variant of ReFOCS, ProtoC$+$ND, and as seen from Table \ref{tab:ablation} the removal of the exemplar reconstruction errors results in the biggest drop in performance - with AUROC dropping to $50\%$ in one case (random prediction) - which again  consolidates the impact of the exemplars in out-of-distribution detection.

\subsubsection*{Choice of distance metric.} Computing the logits for classification requires a distance metric to measure the similarity between the prototypes and the sample in the latent space. We experiment with both the cosine and euclidean metrics and choose the cosine distance as it leads to more discriminative embeddings \cite{gidaris2018dynamic,proto_rectify} which is particularly important for segregating the open class samples from the support class ones. This is validated by the results shown in Table \ref{tab:metric}, where we can see that choosing the euclidean distance metric leads to a significant drop in AUROC. 


\input{tables/table5_metric}
% \sout{Calculating the logits for classification requires a distance metric to measure the similarity between the prototypes and the sample in the latent space. We experiment with both the cosine and euclidean metrics and choose the cosine distance as our metric as it leads to more a discriminative embedding space \cite{gidaris2018dynamic,proto_rectify}. The results, shown in Table \ref{tab:metric}, depict the significant gains in classification accuracy on using the cosine metric instead of the euclidean distance.}

\input{tables/table6_openness.tex}

\subsubsection*{Out-of-distribution performance with varying Openness}
Openness as defined in \cite{sun2020conditional} is shown below,
\begin{equation}
    openness = 1 - \sqrt{\frac{2N_{train}}{N_{test} + N_{target}}}
\end{equation}
where $N_{train}$ is the number of known classes seen during training, $N_{test}$ is the number of classes that will be observed during testing, and $N_{target}$ is the number of open classes to be recognized during testing \cite{sun2020conditional}. For an $N$-way $K$-shot problem, the support and the training set are the same, and therefore, $N_{train} = N_{test} = N$. $N_{target}$ refers to the number of open classes we sample $\mathbb{Q}_{out}$. We vary $N_{target}$ from $5$ to $15$ and observe the open-set recognition performance in terms of averaged F1 score \cite{sun2020conditional} for both ReFOCS and PEELER. As validated by Table \ref{tab:openness} ReFOCS consistently outperforms the thresholding-based PEELER on all the levels of openness.

\subsubsection*{Embedding Visualization.} In Fig. \ref{fig:tsne}, we compare the t-SNE \cite{tsne} visualization of the embedding spaces induced by all the competing methods. We can see from Fig. \ref{fig:tsne} that, in general, ReFOCS achieves more distinct class clusters in comparison to both PEELER \cite{peeler} and ProtoNet \cite{proto_net}, with adequate segregation between seen and unseen classes. 








% \begin{figure}[t]
%     \centering
%     \includegraphics[scale=0.5]{images/tsne.pdf}
%     \caption{\textbf{t-SNE visualization.} We project the latent space learned via (a) ProtoNet (b) PEELER (c) ReFOCS, on $5$ classes of GTSRB$\rightarrow$TT100K, on a 2D space using t-SNE. Out-of-distribution queries are in yellow}
%     \vskip -0.1in
%     \label{fig:tsne}
% \end{figure}









