\documentclass[10pt,journal,compsoc]{IEEEtran}

\usepackage{url}
\usepackage{microtype}
% *** CITATION PACKAGES ***
%
\ifCLASSOPTIONcompsoc
  % IEEE Computer Society needs nocompress option
  % requires cite.sty v4.0 or later (November 2003)
  \usepackage[nocompress]{cite}
\else
  % normal IEEE
  \usepackage{cite}
\fi

% \usepackage{IEEtrans}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb,amsfonts}
\usepackage{enumitem}
\usepackage{microtype}
\usepackage{bbm}
\usepackage{multicol}
\usepackage{subfig}
\usepackage[
singlelinecheck=false % <-- important
]{caption}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage[normalem]{ulem}
\usepackage[dvipsnames]{xcolor}
% Include other packages here, before hyperref.

% % If you comment hyperref and then uncomment it, you should delete
% % egpaper.aux before re-running latex.  (Or just hit 'q' on the first latex
% % run, let it finish, and you should be clear).
% % \usepackage[pagebackref=true,breaklinks=true,letterpaper=true,colorlinks,bookmarks=false]{hyperref}

% % \iccvfinalcopy % *** Uncomment this line for the final submission

% % \def\iccvPaperID{7547} % *** Enter the ICCV Paper ID here
% % \def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}

% % % Pages are numbered in submission mode, and unnumbered in camera-ready
% % \ificcvfinal\pagestyle{empty}\fi
% % \urlstyle{same}

\onecolumn
\begin{document}
    \title{Reconstruction guided Meta-learning for Few Shot Open Set Recognition (Supplementary material)}
    \maketitle
    \section{Datasets}
    In this section we go over the details of each dataset used in our experiments. A summary of the statistics of each dataset is shown in Table \ref{tab:data_size}.
    
    \subsection{Traffic Sign Datasets.}
    
    \noindent \textbf{GTSRB.} This dataset \cite{gtsrb} is one of the largest traffic-sign dataset. It is comprised of 43 classes broadly categorized under prohibitory, danger and mandatory categories. For GTSRB$\rightarrow$GTSRB, the training set contains a total of 39,209 images and the test set has a total of 12,630 images. These images have variations in illumination, shading as well as, resolution \cite{kim2019variational}.  
   
    \noindent \textbf{TT100K.} The Tsinghua-Tencent 100K (TT100K) \cite{tt100k} is a Chinese traffic sign detection dataset. This dataset has more than 200 categories, out of which we used the ones with valid annotation and a corresponding exemplar. Similar to \cite{kim2019variational} this amounts to a total of $36$ valid classes to work with.
    
    \subsection{Brand Logo Datasets}
    \noindent \textbf{BelgaLogos.} BelgaLogos \cite{belga1,belga2} is comprised of $10,000$ images of everyday brand logos commonly found in almost every aspect of daily life. The images have significant perturbations such as blurring, lighting and contrast variations as well as, occlusions. The dataset is also riddled with significant class imbalance as it contains classes with as little as $2$ samples making it suitable for only $1$-shot learning problems. Similar to \cite{kim2019variational} we collect $9,475$ images from BelgaLogos categorised among $37$ logo classes to construct our logo recognition dataset. The images of this dataset have a lot more variations compared to the traffic sign datasets, especially blurring and occlusion, which in-turn makes learning harder. 
    
    \noindent \textbf{FlickrLogos-32.} This dataset \cite{flickr} is comprised of a collection of images corresponding to the $32$ brand logos of Flickr.  We use the splits introduced in \cite{kim2019variational}, which is comprised of a total of $3,372$ cropped logo images. 
    
    \noindent \textbf{TopLogo-10.} This dataset \cite{toplogo} contains logo images from 10 brands related to popular cloth, shoes, and accessory brands. The logo images are obtained from their respective products, and similar to \cite{kim2019variational} the collected images were cropped from bounding box annotations and divided among $11$ classes where the ‘Adidas’ class is divided into ‘Adidas-logo’ and the ‘Adidas-text’. 
    
    \subsection{Natural Image Dataset}
    
    \noindent \textbf{\textit{mini}ImageNet.} Originally proposed by Vinyals et. al. \cite{match_net}, \textit{mini}ImageNet has become a benchmark dataset for few-shot classification \cite{proto_net,peeler}. It is derived from the larger ILSVRC-12 dataset \cite{imagenet} and is comprised of $100$ classes each having $600$ colored images, which are popularly resized to $84 \times 84$ \cite{proto_net,match_net}. We use the same splits as \cite{match_net}, which comprises a training set with $64$ classes, a validation set with $16$ classes, and a test set with $20$ classes.

    % \section{Sampling Strategies}
    % The episodic sampling strategies for all the experiments performed is shown in Table \ref{tab:sampling}. For an $\text{N}$-way $\text{K}$ shot problem each episode has $\mathbf{K}_{\mathbb{Q}_{in}}^c$ number of in-distribution queries sampled from each support class $c \in \{1,..,N\}$. This means the total number in-distribution query samples per episode  $\mathbf{K}_{\mathbb{Q}_{in}} = \sum\limits_{c=1}^N\mathbf{K}_{\mathbb{Q}_{in}}^c$. Along with the in-distribution samples, the query set of each episode is also augmented with $\mathbf{K}_{\mathbb{Q}_{out}}$ samples which do not belong to any of the support classes i.e. out-of-distribution. Hence, the total number samples $\mathbf{K}_{\mathbf{e}}$, in each episode is given as, $\mathbf{K}_{\mathbf{e}}=\mathbf{K}+\mathbf{K}_{\mathbb{Q}_{in}}+\mathbf{K}_{\mathbb{Q}_{out}}$ where, $\mathbf{K} = |\mathbb{S}|$.  The model encounters $\mathbf{E}_{train}$ number of episodes each epoch during the meta-training phase which means for $T$ epochs the model is trained over a total of $T \times \mathbf{E}_{train}$ number of episodes. Finally, the model is tested over $\mathbf{E}_{test}$ number of test episodes and the average performance is recorded. 
    % \input{tables/table1_sampling}

\input{tables/table_dataset_size}

    \section{Additional implementation details}
    \subsection{Setup details}
    For the traffic sign and brand logo recognition, the learning rate remains constant throughout the meta-training stage. For experiments on \textit{mini}ImageNet we drop the learning rate by a factor of 10 after every 20000 episodes.
    \subsection{Training details}
    For the traffic sign and brand logo classification, all modules of ReFOCS are meta-trained end-to-end. However, for \textit{mini}ImageNet training the entire framework end-to-end results in over-powering of the reconstruction module resulting in decreased classification accuracy. We suspect this problem is due to the difficult image-to-exemplar translation task for \textit{mini}ImageNet which is brought about by the fact that the estimated exemplars of \textit{mini}ImageNet tend to have much more variability compared to those of the traffic sign and brand logo datasets. In order to rectify this, we first meta-train the VAE encoder with just the cross-entropy classification loss (Eq. 6), following which the weights of the trained encoder are fixed and utilized to jointly train the decoder and the novelty detection module using a combination of the reconstruction loss (Eq. 2) and the novelty detection loss (Eq. 8). 

    For \textit{mini}ImageNet we also explore starting the meta-training phase by using the pre-trained encoder (also called backbone) when using Resnet-12. For both backbones the pre-trained encoder is directly used for the exemplar estimation process explained in section 3.5. For the Resnet12 case, the pre-training is done following \cite{snatcher} and during meta-training, the encoder's weights are initialized with those of the pre-trained encoder. 
    
    % This two-step training methodology also allows us to explore pre-training of the encoder, which involves normal supervised training of the encoder on the entire base training set. The pre-trained encoder is then meta-trained with the classification loss as explained before. We find that pre-training in general does not give any performance boost for the $5$-shot scenario but results in approximately $5 \%$ increase in AUROC for the $1$-shot case.
    
    % \section{Implementation Details}
    % \noindent \textbf{Traffic Sign Recognition.} For all the traffic sign experiments the iamges were resized to $64 \times 64$ and a learning rate (\textit{lr}) of $10^{-4}$ is used. For the GTSRB$\rightarrow$GTSRB $5$-shot and GTSRB$\rightarrow$TT100K $1$-shot case, we observe that dropping the \textit{lr} by a factor of $10$ after $13000$ and $18000$ episodes result in a slight performance increase. The loss function hyperparameters $\lambda_1, \lambda_2$ and $\lambda_3$, for all the traffic sign experiments, are set to $10^{-4}, 10$ and $10$ respectively. The temperature parameter $\tau$ was initialized with a value of $10$ for all the traffic sign experiments.
    
    % \noindent \textbf{Brand Logo Recognition.} In case of all the $1$-shot brand logo experiments the \textit{lr} is fixed at $10^{-4}$. The brand logo images are also resized to $64\times64$. For these experiments too, $\tau$ is initialized to $10$, and the values of $\lambda_1, \lambda_2$ and $\lambda_3$ are set to $10^{-4}, 10$ and $10$ respectively. 
    
    % \noindent \textbf{Natural Image Classification.} For \textit{mini}ImageNet the popular $84\times84$ image size is used, and for both the $1$-shot and $5$-shot experiments the \textit{lr} is initially set to $10^{-3}$ and then dropped by a factor of $10$ after every $20000$ episodes. Since, in this case, the encoder is separately meta-trained, the hyperparameter corresponding to the classification loss $\lambda_2$ is set to $1$ and $\lambda_1$ and $\lambda_3$ are set to $10^{-4}$ and $10$ respectively. For the $5$-shot scenario $\tau$ is initialized to a value of $1000$, and for the $1$-shot case it is initialized to $100$.
 



  \section{More Ablations}

\input{tables/table_exemplar_importance.tex}
\input{tables/table_why_not_cosine.tex}
\subsection{Why exemplar reconstruction is important for FSOSR?}
As we have mentioned before, in few-shot learning, naively applying self-reconstruction (reconstructing the input query sample itself) will not enable the learning of an effective task-irrelevant representation. Therefore, it is necessary to anchor the samples of each class to a high-quality abstraction of that class, which we achieve using class-specific exemplars. To verify this, we compare self-reconstruction with exemplar reconstruction for the FSOSR tasks of GTSRB$\rightarrow$TT100K and Belga$\rightarrow$Flickr32. As observed from Table \ref{tab:exemplar_importance}, it can be seen that naively applying self-reconstruction leads to a catastrophic failure to generalize to FSOSR tasks as evident from the lower than random AUROC values as well as the significantly lower classification accuracies.



\subsection{Choice of metric for exemplar estimation and modulation factor $\kappa$}

For the nearest neighbor based exemplar estimation (Eq 10) and the modulation factor $\kappa$ (Eq 7) we choose the $\ell_2$ norm (euclidean distance) and the $\ell_1$ norm (manhattan distance), respectively. One might ask why a cosine similarity-based distance such as the following,
\begin{equation}
    Distance_{cosine}(\mathbf{x},\mathbf{y}) = 1 - \frac{\mathbf{x}^T \mathbf{y}}{||\mathbf{x} ||_2 || \mathbf{y}||_2}
\end{equation}
 was not used for these two.  This is because, for the exemplar estimation (Eq 10), we observe that the simple euclidean distance suffices to get the best exemplar. However, for the embedding modulation factor $\kappa$, it is necessary to use the $\ell_1$ norm. This is because if a cosine similarity-based distance is used for $\kappa$, it is constrained between 0 and 1, whereas utilizing the $\ell_1$ norm allows $\kappa$ to have $\geq 1$ values enabling greater scaling down of the embedding of out-of-distribution samples. To study this, we experiment by considering the following three cases,
\begin{itemize}
    \item  \textbf{Case 1}: In Eq 10 we use $Distance_{cosine}()$ as shown above and keep $\kappa$ the same as Eq 7.

    \item \textbf{Case 2}: For $\kappa$ we use $Distance_{cosine}()$ and keep the exemplar estimation same as Eq 10.

    \item \textbf{Case 3}: For both $\kappa$ and exemplar estimation we use $Distance_{cosine}()$.
\end{itemize}
We compare the results obtained from these three cases with the current setup of Eq 7 and Eq 10. From the observations tabulated in Table \ref{tab:why_not_cosine}, we can see that estimating the exemplars using the cosine distance, as opposed to the euclidean distance, does not significantly impact the Accuracy or AUROC. Therefore, choosing the euclidean distance suffices. On the other hand, if $Distance_{cosine}()$  is used to compute $\kappa$ (Case 2 and Case 3), we observe a significant drop in AUROC since the scaling of the query embedding becomes limited.




\begin{figure}[t]
\centering
\captionsetup[subfigure]{justification=centering}
\subfloat[ ]{
	\label{subfig:lmbda_1}
	\includegraphics[width=0.47\textwidth]{images/lmbda_1.pdf} } 
\hfill
\subfloat[ ]{
	\label{subfig:lmbda_3}
	\includegraphics[width=0.47\textwidth]{images/lmbda_3.pdf} } 
\caption{\textbf{Hyperparameter Analysis}. (a) $\lambda_2$ \& $\lambda_3$ are fixed at $5$ and $10$,  changing only the reconstruction loss term. (b) $\lambda_1$ and $\lambda_2$ are fixed at $10^{-4}$ and $10$ and $\lambda_3$ is varied.}
\label{fig:hyper_params}
\end{figure}




  \subsection{Hyperparameter variation}
  Fig. \ref{fig:hyper_params} shows how the open-set AUROC is affected by the hyperparameters $\lambda_1$ or $\lambda_3$ for the GTSRB$\rightarrow$TT100K task. In both cases, the classification term $\lambda_2$ is fixed at $10$. In Fig. \ref{subfig:lmbda_1}, when $\lambda_1$ is very low, the open-set detection is hampered due to poor quality of the reconstruction and when it is increased beyond $10^{-4}$ reconstruction becomes the sole objective of the model, thus the open-set detection again degrades. From Fig. \ref{subfig:lmbda_3} we can see that for both the $5$-shot and $1$-shot cases increasing $\lambda_3$ causes the AUROC to improve the knee point of $\lambda_3=10$, after which it starts to degrade.

% \input{tables/table_supp_abl_gtsrb}
% \input{tables/table_supp_abl_belga}
\section{More Qualitative Visualizations}

Some exemplar reconstructions for the \textit{mini}ImageNet $\rightarrow$ \textit{mini}ImageNet experiment are shown in Fig \ref{fig:exemp_recon_mini}. Since the exemplars are also natural images, the high variation in lighting and backgrounds makes the reconstruction much harder. However, ReFOCS performs the way it's intended to, and as such out-of-distribution samples fail to reconstruct the in-distribution exemplars.

\input{images/reconstruction_mini}


\bibliographystyle{IEEEtran}
\bibliography{IEEEabrv,ref}
\end{document}
