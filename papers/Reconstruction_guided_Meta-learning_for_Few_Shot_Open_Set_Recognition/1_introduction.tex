\section{Introduction}
Deep neural networks have achieved excellent performance on a wide variety of visual tasks \cite{he2016deep,lin2017feature,long2015fully}. However, the majority of this success has been realized under the closed-set scenario, where it is assumed that all classes that appear during inference are present in the training set. In real-world applications, it is often difficult to obtain samples that exhaustively cover all possible semantic categories \cite{openlongtailrecognition}. This inherent open-ended nature of the visual world restricts the wide-scale applicability of deep models and machine learning models in general. Thus, it is more realistic to consider an \textit{open-set} scenario \cite{geng2020recent}, where the predictive model is expected to not only recognize samples from the seen classes, but also to recognize when it encounters an \textit{out-of-distribution} sample and reject it, rather than making a prediction for it.

\begin{figure}
    \centering
    \includegraphics[width=0.48\textwidth]{images/setup.pdf}
    \caption{\textbf{Problem setup.} We formulate few-shot open-set recognition as a meta-learning problem where training proceeds in an episodic manner. In contrast to the usual setup, where the support and query sets share the same categories, we consider the more challenging open-set scenario where the query set can contain samples from classes not seen in the support (highlighted in red).}
    % \vskip -0.1in
    \label{fig:prob_setup}
\end{figure}

Notable approaches for open-set recognition involve adversarial training \cite{Lu_2017_ICCV} to reject adversarial samples which are too hard to classify, inducing self-awareness in CNNs \cite{hendrycks17baseline} to reject out-of-distribution samples, and using extreme value statistics to re-calibrate classification scores of samples from novel classes \cite{open_max}. All of these approaches require large amounts of labeled data per category for the seen classes. On the contrary, humans can easily grasp new concepts with very limited supervision and simultaneously perceive the occurrence of unforeseen abnormalities. Aiming to emulate this, we seek to perform open-set recognition in the \textit{few-shot learning} scenario, which has been largely ignored in the literature. A visual description of the problem setting is shown in Fig. \ref{fig:prob_setup}. 

The challenge in few-shot open-set recognition (FSOSR) stems from the limited availability of samples for in-distribution classes. This complicates learning a good abstraction of the provided categories to comprehensively distinguish between low-likelihood in-distribution samples and actual out-of-distribution samples. Following the success of meta-learning approaches \cite{maml,match_net,proto_net} in closed-set few-shot recognition, recent works \cite{peeler,snatcher} attempt FSOSR by building on top of the popular Prototypical Network framework \cite{proto_net}. In \cite{peeler}, the authors employ entropy maximization to enforce uniform predictive distribution of out-of-distribution samples to maximize model confusion for unseen categories. Out-of-distribution samples are identified by thresholding the maximum probability of the classification logits. In contrast, \cite{snatcher} leverages a transformer \cite{vaswani2017attention} and carefully crafted normalization techniques \cite{snatcher} to enforce improved representation consistency of in-distribution samples, enabling the rejection of out-of-distribution samples via simple distance thresholding in the feature space. Despite their effectiveness, these approaches rely heavily on carefully tuned threshold values. Furthermore, assuming near uniform predictive distribution for out-of-distribution samples does not generally hold in practice due to the sensitivity of deep CNNs to minor perturbations in the input \cite{goodfellow_adv} along with the tendency of the softmax operator to produce high confidence false predictions for out-of-distribution samples \cite{hendrycks17baseline}. This is further explained in Fig. \ref{fig:softmax_issues}.

% \textcolor{blue}{ Recent works \cite{peeler,snatcher} have attempted to extend the prototypical networks \cite{proto_net} for FSOSR by addressing it as a thresholding problem. Both works employ the meta-learning setup \cite{maml,match_net,proto_net}, and while \cite{peeler} employs entropy maximization to enforce uniform predictive distribution of out-of-distribution samples and  maximize model confusion for unseen categories, \cite{snatcher} leverages a transformer \cite{vaswani2017attention} to enforce improved representation consistency of in-distribution samples. Therefore, \cite{peeler} rejects out-of-distribution samples by thresholding the maximum probability of the classification logits. On the other hand, improving the consistency of in-distribution samples \cite{snatcher} enables the rejection of out-of-distribution data via simple distance thresholding in the feature space. Although effective, these approaches rely on carefully chosen threshold values. Additionally, the assumption of \cite{peeler} that out-of-distribution samples will have near uniform predictive distribution does not hold in practice due to the sensitivity of deep CNNs to minor perturbations in the input \cite{goodfellow_adv} along with the tendency of the softmax operator to produce high confidence false predictions for out-of-distribution samples \cite{hendrycks17baseline}. This is further explained in Fig. \ref{fig:softmax_issues}. }

% Recently, Liu et. al. \cite{peeler} attempt this task using Prototypical Networks in a meta-learning setup \cite{maml,match_net,proto_net}. Their approach maximizes the entropy over the predicted softmax scores of samples from open classes in an attempt to maximize model confusion for such unseen categories. Thus, out-of-distribution samples are desired to have a uniform predictive distribution. Unfortunately, this assumption does not hold in practice: the sensitivity of deep CNNs to minor perturbations in the input \cite{goodfellow_adv}, in addition to the softmax function being a smooth variant of the indicator function, make them prone to producing high confidence false predictions for out-of-distribution samples \cite{hendrycks17baseline}. This is further explained in Fig. \ref{fig:softmax_issues}. 

In order to overcome these drawbacks, we propose a different approach to detecting out-of-distribution samples in the few-shot setting, which utilizes \emph{reconstruction} as an auxiliary task to induce self-awareness in a few-shot classifier. This self-awareness would allow the classifier to better detect when it is presented with an out-of-distribution sample, as it would be able to recognize when it is unable to accurately reconstruct the input. However, naively applying reconstruction fails in the few-shot setting due to overfitting. Inspired by \cite{kim2019variational}, we propose using reconstruction of \textit{class-specific exemplars}, instead of self-reconstruction, to flag out-of-distribution samples. Such exemplars act as ideograms to effectively encode the semantic information of the class it belongs to. Consequently, they serve to anchor the representations of in-distribution classes when access to a large number of samples is restricted. Many real-world graphical symbols, such as traffic signs and brand logos, have well-defined exemplars that lie on a simpler or canonical domain. Some examples are shown in Fig. \ref{fig:exemplar}. However, exemplars may not be available for all datasets, and for such cases, we provide a simple scheme to estimate them from the few-shot data in the \emph{embedding space}, without any changes to the algorithmic formulation.

% Examples of exemplar images corresponding to real-world graphical symbols, such as, traffic signs and brand logos are shown in Fig. \ref{fig:exemplar}. In cases where exemplars are not readily available, we provide a simple scheme to estimate them from the few-shot data in the \emph{embedding space}, without any changes to the algorithmic formulation.

\begin{figure}
    \centering
    \includegraphics[width=0.48\textwidth]{images/intro.pdf}
    \caption{\textbf{Assuming uniform predictive distribution for outliers.} Out-of-distribution samples can be correlated to in-distribution classes by varying amounts. In this figure, we train a classifier for three in-distribution classes: dog, bird and truck (corresponding samples highlighted in green). The out-of-distribution sample - a cat - shares highly similar visual characteristics to a dog, as evidenced by the prediction. This suggests that desiring outliers to have a uniform predictive distribution, as suggested in \cite{peeler}, is often inaccurate.} 
    % \vskip -0.1in
    \label{fig:softmax_issues}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=0.2\textwidth]{images/exemplars.pdf}
    \caption{\textbf{Canonical Exemplars of real images.} Examples of real-world images of symbolic data such as traffic signs and brand logos (left column) along with their corresponding class-specific exemplar images (right column). These exemplars are readily available for such symbols and lie on a canonical domain devoid of perturbations of the real images.}
    % \vskip -0.1in
    \label{fig:exemplar}
\end{figure}

Building on this idea of reconstruction, we propose a new meta-learning strategy to tackle the few-shot open-set recognition task. In the meta-training phase, we use episodes sampled from a base set, with each episode simulating a token few-shot open-set task. Specifically, each episode is created by randomly selecting a set of classes and populating a support set with limited samples belonging to those classes. A query set is created in a similar fashion but contains samples from classes both seen in the support set and beyond (see Fig. \ref{fig:prob_setup}). These episodes are subsequently used to train our framework: Reconstructing Exemplar-based Few-shot Open-set ClaSsifer (\textbf{ReFOCS}). Given an episode, ReFOCS projects these samples to a low-dimensional embedding space to perform a metric-based classification over the support classes. Simultaneously, a variational model is used to reconstruct the class exemplars to compute the reconstruction error. The class scores, in addition to the reconstruction error, are then utilized to recognize the probability of the sample being out-of-distribution.



\subsubsection*{Main Contributions}
Our primary contributions are summarized below:
\begin{itemize}
    \item[$\bullet$] We develop a new reconstruction based meta-learning framework which utilizes class-specific exemplars to jointly perform few-shot classification and out-of-distribution detection.
    \item By utilizing reconstruction as an auxiliary task in our meta-learning setup we induce self-awareness in our learning framework, enabling it to self-reject out-of-distribution samples without relying on carefully tuned thresholds.
    \item[$\bullet$] We introduce a novel embedding modulation scheme to make the learned representations more robust and discriminative in the presence of scarce samples. A weighted strategy for prototype computation is also introduced for reducing intra-class bias.
    \item Our framework outperforms or achieves comparable performance to the current state-of-the-art on a wide array of FSOSR experiments, thereby establishing it as a new baseline for FSOSR.
    % \item[$\bullet$]  In comparison to the current state-of-the art we obtain a median increase of approximately $\mathbf{6\%}$ in AUROC for $5$-shot experiments and around $\mathbf{7\%}$ for $1$-shot experiments with a maximum increase of over $ \mathbf{12}\%$. The increased AUROC is complemented with greater or comparable classification accuracy of in-distribution samples which altogether clearly establishes our proposed approach as the new state-of-the art in few-shot open-set recognition.
\end{itemize}


