\section{Experiments}
\begin{figure*}[t!]
    \centering
    \includegraphics[width=.78\textwidth]{figures/randomwalk_tableau.png}
    \caption{Interface generated from a log of random OLAP queries over the On-Time database. The widgets were created by \sys, we edited the layout and titles manually. Users choose dimensions and measures by dragging and dropping in the leftmost boxes. They create filters with the dropdown lists and text boxes at the bottom of the screen.}
    \label{fig:ui-randomwalk}
\end{figure*}
\begin{figure}[t!]
    \centering
    \includegraphics[width=\columnwidth]{figures/randomwalk_simple}
    \caption{Interfaces generated for the random OLAP queries. Values simplicity.}
    \label{fig:ui-simple}
\end{figure}
\begin{figure}[t!]
    \centering
    \includegraphics[width=\columnwidth]{figures/randomwalk_noeffort.png}
    \caption{Interfaces generated for the random OLAP queries. Values directness.}
    \label{fig:ui-direct}
\end{figure}
\begin{figure*}[t!]
    \centering
    \begin{subfigure}[b]{0.25\textwidth}
        \includegraphics[height=1.8in]{figures/sub_UI1.png}
        \caption{First interface.}
        \label{fig:ui-manual1}
    \end{subfigure}
    ~
  \begin{subfigure}[b]{0.25\textwidth}
        \includegraphics[height=1.8in]{figures/sub_UI2.png}
        \caption{Second interface.}
        \label{fig:ui-manual2}
    \end{subfigure}
    ~
  \begin{subfigure}[b]{0.25\textwidth}
        \includegraphics[height=1.8in]{figures/sub_UI3.png}
        \caption{Third interface.}
        \label{fig:ui-manual3}
    \end{subfigure}
    \caption{\small Top 3 interfaces generated by \sys{} from a set of queries over the On-Time database written by students. The first window lets users compose simple \texttt{group by} queries, by selecting lines in the list boxes. The second one presents aggregates for each combination of origin and desintation airport. The last interface shows statistics for each carrier. Collectively, those interfaces cover 59\% of all the queries that we collected.}
      \label{fig:ui-manual}
\end{figure*}
We evaluate \sys using 5 query logs---4 SQL and 1 SPARQL log.  We used simulated query logs, logs from existing data systems, and logs generated through manual visual exploration, in order to study the system on clean, real-world, and ad-hoc types of logs types, respectively.   We seek to answer four questions: 1) can \sys{}'s interfaces express query logs? 2) is the runtime acceptable? 3) can the system support multiple languages?  and 4) how do users prefer the generated interfaces as compared with existing and user-designed interfaces?

Parsing and interaction mining are implemented in Java, and the widget mapping and rendering are implemented in Python, which generates HTML+Javascript interfaces.  We defined 12 widget types (e.g., dropdown, checkbox list, slider, range slider, textbox, multi-select) and manually created $exec()$ and $render()$ functions for SQL and SPARQL.   After generating the interfaces, we named and positioned the widgets for presentation purposes. We used a MacBook Pro with Intel Core i7 2.5 GHz CPU and 8GB RAM.

\subsection{Expressing Query Logs}
\label{sec:case_stud}

We first showcase the generated interface designs and their ability to express queries from two logs---synthetically generated OLAP queries, and those generated through manual exploration---over the On-Time Database\footnote{\small 521,000 rows, 91 cols. \url{https://www.transtats.bts.gov}}.


\stitle{Synthetic OLAP queries:} The aim of this experiment is to show that \sys{} can generate a simple Tableau-like interface from a standard OLAP query workload.
To simulate the exploration process, our generator explores the OLAP space by starting with a group by-aggregate query, and iteratively modifying a random clause (GROUPBY, WHERE, SELECT) at each step. To seed the process, we generate a random group by-aggregate query that follows the following format:
\begin{verse}
\texttt{SELECT dim1, \ldots, dimM,}\\
        \texttt{~~~~~~~agg1(meas1), \ldots, aggN(measN)}\\
\texttt{FROM Ontime}\\
\texttt{WHERE var1=val1 AND \ldots AND varP=valP}\\
\texttt{GROUP BY dim1, \ldots, dimM}\\
\end{verse}
The number of dimension, filters and measures is a sampled from a uniform distribution. We then perform random edits, one for each step. We present the possible edits in Table~\ref{tab:changes}. We wrote 7 PILang statements that correspond to structural and value changes that our query generator expressed.

\begin{table}
\caption{Possible modifications from the query generator.}
\begin{tabular}{ c l }
\hline
  Type &  Actions\\
  \hline
  Dimensions &  Add, Remove, Change\\
  Measures   & Add, Remove, Change col., Change agg.\\
  Filters    &  Add, Remove, Change col., Change val.\\
  \hline
\end{tabular}
\label{tab:changes}
\end{table}


Figure~\ref{fig:ui-randomwalk} presents the generated interface. The two drag-n-drop boxes on the left let users choose measures and dimensions to visualize. The bottom section of the interface provides three filters, each consisting of a drop-down list to select a column and a text field to specify a value. This interface can express 100\% of the queries in the log (i.e., its closure contains all the queries in the interaction graph). As Tableau, it lets users produce OLAP queries by dragging columns onto ``shelves'', however further work is needed to generate complex logic such as small multiples.
% To differentiate those between two types of changes, we exploited the fact that dimensions are present in both the \texttt{GROUP BY} and the \texttt{SELECT} statement, while the measures are only present in the \texttt{SELECT} clause. Thanks to \lang{}, we could specify that those are two different types of changes, and have them represented by two different widgets.

Figure~\ref{fig:ui-simple} presents an alternative UI, obtained from the same set of queries but with different parameters. In this case, we tuned the weight associated to the cost functions to obtain the most simple interface possible. We assigned a high cost to visual complexity and a low cost to user effort. As a result, the UI contains only three text boxes---one for the dimensions, one for the measures and one for the filters. The user must type the queries manually.

To generate Figure~\ref{fig:ui-direct}, we reversed the weighing scheme. We assigned a high cost to user efforts (e.g., number of keyboard interactions and clicks) and ignored visual complexity. In the resulting window, all the options are explicit: there is one tick box for each possible dimension, measure of filter column, and a dropdown lists for the filter values.


\stitle{Manual log:} We created an ad-hoc exploration query log by asking 12 students to perform 3 random (out of 12) tasks using the On Time dataset (e.g., ``how delayed are the flights to from AA?''), answer one free form question (``tell us something you found surprising'') and report their findings.   We logged all queries that were executed.  There are a total of $298$ statements, $148$ unique. We did no clean the log (e.g., dead-end analyses, erroneous queries) and simply report the top interfaces.  We used $15$ \lang statements writtend using the tool described in Section~\ref{sec:tool}.

\begin{figure}[h!]
    \centering
    \includegraphics[width=.7\columnwidth]{figures/UI_coverage}
  \caption{Coverage as more interfaces are added to $\mathbb{I}^*$ for manual log. Shows long tail of queries.}
    \label{fig:ui-manual-coverage}
\end{figure}
This query log contains far more variability than the synthetic dataset, and we consider it a ``hard'' case. Figure~\ref{fig:ui-manual-coverage} shows the total number of queries covered as the number of interfaces in the output $\mathbb{I}^*$ increases. We observe that the first interface covers $166$ ($55\%$) queries, and subsequent interfaces cover $<10$ queries each. This suggests that the interaction graph is sparse, which is reflected in our post-hoc analysis of the logs. %\sys{} would require many more \lang{} statements, and therefore many more widgets, to cover it entirely.


Figure~\ref{fig:ui-manual} shows the top three interfaces. The left interface is the primary one that resembles a simplified Tableau: most students incrementally vary the select, where, groupby and orderby clauses.  The middle interface covers $10$ ($3\%$) and computes aggregate statistics for each flight origin; the right interface is representative of the long tail (covers $1-3$ queries). Although those three interfaces do not cover the whole log, they express the primary exploration structure using only 6 interaction components. %This shows that the students' exploration process is mostly structured and incremental and that \sys{} can detect this this structure.







\subsection{Performance and Languages}
\label{sec:atscale}

In this experiment, we evaluate \sys{}'s language support and scalability. To evaluate the first aspect, we run the pipeline on logs written in two different query languages. To
test the second, we measure its runtime for different optimizations. We show that \sys{} spends more than 90\% of its time in the interaction mining stage, and therefore we focus on this step.

We use two programs logs. The first one is the {\it SDSS} log~\cite{sdss}, which contains $125,603$ SQL queries ($112,847$ unique).  The second is a sample from the {\it British Museum's} Semantic Web Collection~\cite{britishmuseum}, which contains $110,677$ SPARQL queries ($38,933$ unique). We respectively used $16$ and $4$ \lang statements for SQL and SPARQL, which describe the more frequent transformations, detected both by manual inspection and by using the tool described in Section~\ref{sec:tool}. We compare four settings: no optimization, the clique-based optimization of Section~\ref{transcliques} (Clique), the program templates of Section~\ref{s:templates} (Template), and both optimizations. By default, the latter setting is enabled. Our main finding is that using both optimizations allows \sys to scale to logs that are two orders of magnitude larger than without any optimization, and thus it can process logs with 10,000s of queries in minutes.


\begin{figure}[h!]
    \centering
    \includegraphics[width=.8\columnwidth]{figures/breakdown}
    \caption{Breakdown of execution time for each dataset.}
    \label{fig:breakdown}
\end{figure}
\stitle{Cost Breakdown:} Figure~\ref{fig:breakdown} shows the overall cost breakdown. The Interaction Mining phase is by far the most time consuming. Because this cost largely dominates \sys{}'s runtime, the rest of this section focuses on it.

\begin{figure}[h!]
    \centering
    \includegraphics[width=\columnwidth]{figures/both_vary_queries}
    \caption{Runtime (secs) of interaction mining vs log size (log scale). The points for Template and Both overlap. We cap maximum runtime to $1$ hour.}
    \label{fig:time-both}
\end{figure}
\stitle{Scalability with the size of the program logs:} Figure~\ref{fig:time-both} shows that the runtime increase quadratically for both logs, even when the optimizations are enabled. This comes from the fact that \sys{} must align and compare $\mathcal{O}(N^2)$ pairs of programs to build the interaction graph, where $N$ is the number of programs. We ran a micro-benchmark and found that the cost of the comparisons is almost constant---they take in average $3.4\pm0.08ms$ for the SDSS log and $1.20\pm0.01$ms for British Museum log ($\pm$ are 95\% CIs). The cost comes from the high number of comparisons.

The optimizations do not reduce the $\mathcal{O}(N^2)$ worst-case complexity of the algorithm, but they allow the system to skip comparisons. In particular, the Template optimization incurs a runtime improvement of about two orders of magnitude  compared to No Optimization---up to 347x for the SDSS data set and 71x for the British Museum data.

\begin{figure}[h!]
    \centering
    \includegraphics[width=\columnwidth]{figures/both_vary_pilang}
    \caption{Runtime of interaction mining vs. number of PILang statements, for samples of each dataset. In both cases, the plots for Cliques and Both overlap.}
    \label{fig:pl-sdss}
\end{figure}
\stitle{Scalability with the number of \lang{} statements:} Figure~\ref{fig:pl-sdss} presents how the interaction miner's runtime changes when we vary the number of \lang statements. The number of queries is fixed; we used small sample sizes to enforce that all the versions of the algorithm reach completion within one hour. In both cases, we find that increasing the number of statements linearly increases the runtime. Here again, the Template optimization yields improvements of about two order of magnitudes.

\begin{figure}[h!]
    \centering
    \includegraphics[width=\columnwidth]{figures/preprocessing}
    \caption{Templated clique extraction costs for each log.}
    \label{fig:prepro}
\end{figure}
\stitle{Preprocessing: } Figure~\ref{fig:prepro} shows the time required to extract the query templates (Section~\ref{s:templates}). The runtime increases quadratically with respect to the log size, but it runs within $30$s for both program logs. %  In all cases, we observe that operation runs in less than 30 seconds, which indicates that the overhead induced by the optimization is minimal.



\begin{figure}[h!]
    \centering
    \includegraphics[width=.7\columnwidth]{figures/vary_templates}
    \caption{Runtime of interaction mining for a log of 50,000 queries, varying the number of templates.}
    \label{fig:ui-template}
\end{figure}
\stitle{Impact of the Structure on the Optimizations: }
In this set of experiments, we show how the optimizations' efficiency varies with the structure of the programs in the log. We generate random queries using templates; we fix the number of queries, vary the number of templates and measure how \sys{}'s runtime varies. We expect that the runtime decreases as the variability of the queries (i.e. the number of templates) decreases.

Figure~\ref{fig:ui-template} presents the results of the experiments. We observe that the runtime of interaction mining varies linearly with the number of templates, for a fixed number of queries. The effect is similar for both Clique and Template, though Clique is two orders magnitude slower than Template. This illustrates that the optimizations successfully exploit the structure in the log. The more structure the log contains, the faster interaction mining runs.






\begin{figure*}[t!]
    \centering
  \begin{subfigure}[b]{0.28\textwidth}
	    \includegraphics[width=\textwidth]{figures/SDSS.png}
	    \caption{Original SDSS Interface.}
	    \label{fig:ui-1}
	\end{subfigure}
	~
  \begin{subfigure}[b]{0.18\textwidth}
	    \includegraphics[width=\textwidth]{figures/Vizgened.jpg}
	    \caption{Precision Interface}
	    \label{fig:ui-2}
    \end{subfigure}
	~
  \begin{subfigure}[b]{0.18\textwidth}
	    \includegraphics[width=\textwidth]{figures/M1.jpg}
	    \caption{Custom Interface 1}
	    \label{fig:ui-3}
  \end{subfigure}
    ~
  \begin{subfigure}[b]{0.28\textwidth}
	    \includegraphics[width=\textwidth]{figures/M2.jpg}
	    \caption{Custom Interface 2}
	    \label{fig:ui-4}
  \end{subfigure}
  \caption{The original SDSS interface, the interface generated by \sys, and two manually designed interfaces.}
  \label{fig:allinterfaces}
\end{figure*}

\subsection{User Study}
We conducted users studie based on the SDSS query log, using the original Sky Server interface\footnote{\url{http://skyserver.sdss.org/dr14/en/tools/search/form/searchform.aspx}} for reference. We studied whether 1) the generated interfaces reduce the reponse time and analysis accuracy as compared to the existing interface and 2) the generated interfaces are competitive with the original one and handcrafted alternatives in terms of user preference. We recruited 40 CS graduate students for the study.

\stitle{Comparison With Existing Interface}
 Users were given 5 minutes to read the manual\footnote{\url{skyserver.sdss.org/dr9/en/tools/search/}} describing the 4 tasks supported by the existing SDSS interface (Figure~\ref{fig:ui-1}), and interact with the interface.  To avoid learning effects, we randomly split the users into two groups which were asked to complete the 4 tasks using different interfaces. The first group used the existing interface, while the second used \sys (Figure~\ref{fig:ui-2}). We recorded the analysis time and result accuracy for each task.

\begin{figure}[h!]
    \centering
    \includegraphics[width=.9\columnwidth]{figures/user_both.png}
    \caption{Comparison of response time and accuracy using the original SDSS interface and the automatically generated interface.}
    \label{fig:userstudy}
\end{figure}
Figure ~\ref{fig:userstudy} depicts the average accuracy and time needed for each task for both groups of users. For reference, the average time users needed to perform task 2 (filter objects using the sky coordinates RA (Right Ascension) and dec (Declination) is 34 seconds while it takes only 12 seconds using our generated interface. We explain this by the fact that the original interface does not have default widgets for this task, and users have to choose a combination of options for the widgets to appear and then filter using the widgets, which involves multiple interactions. On the other hand, performing this analysis with our interface requires a single interaction. The case for task 4 (filter using spectrum and redshift) is similar and therefore generated similar results. The response time and accuracy differ the most for task 1 (filtering objects by id) as the original interface does not have a widget for this task and users have to write their own query. Our generated interface led to faster and more accurate analysis for all tasks except task 3 (filter objects by their colors), where both interfaces provided straightforward widgets. Overall, the generated interface created higher quality widgets than the original SDSS interface, which led to an increase in accuracy and a decrease in response time.

\stitle{Interface Preferences}
After users performed the above tasks, we presented them with four interfaces---the SDSS original, \sys, and two manually crafted interfaces---and asked them to choose their preferred interface based on their design.  The aim is to understand the extent to which \sys's interactions are congruent with user expectations.  The two manual interfaces (Figures~\ref{fig:ui-3},~\ref{fig:ui-4}) were implemented by two software engineers that read the SDSS task manual and implemented applications to support the described analyses.

% The second study asked users to We performed a second user study using the same set of users to show that the generated interface is almost as good as manually crafted interfaces by software engineers after reading the interface manual. We start out having two graduate students read the SDSS manual, which describes the underlying data and the kinds of analysis users are interested in. They then manually coded the interface to support the analysis described in the SDSS manual. After the first user study, we presented the original SDSS interface, the generated interface and the two manually created interfaces to each of the users and let them choose their favorite interface based on their own visual preferences.
\begin{figure}[h!]
    \centering
    \includegraphics[width=.6\columnwidth]{figures/user_prefs.png}
    \caption{Percentage of user who prefers each interface.}
    \label{fig:preference}
\end{figure}

Figure ~\ref{fig:preference} shows that both \sys and the manually crafted interfaces are preferable to the original. Over 70 percent of the users chose the forth interface while over 20 percent of the users chose \sys.   Out of 40 users, only 1 user chose the first manually designed interface while none chose the original SDSS interface. These results suggest that manually crafted interfaces can vary considerably in perceived quality, and that \sys can generate interfaces competitive with manual implementations.  It also suggests that \sys, without domain-specific knowledge---of the SDSS manuals, the tasks, or the underlaying data---can summarize the salient analysis operations in an interactive interface from query logs that can be simpler to obtain than expert developers.

\subsection{Experiments with Tableau}
\begin{figure*}[t!]
    \centering
    \includegraphics[width=.9\textwidth]{figures/UI_tableau.png}
    \caption{Interface generated for the Tableau log.}
    \label{fig:ui-tableau}
\end{figure*}

In this experiments, we run \sys{} on queries generated directly by Tableau, and check if (1)~our system can detect the underlying interactions and (2)~can generate a simpler, Tableau-like interface. We asked 7 students to use Tableau on the Ontime dataset, using the same setup as that discussed in Section~\ref{sec:case_stud}. We logged the queries generated by Tableau, and obtained $506$ SQL statements ($459$ unique).

 We present the resulting interface in Figure~\ref{fig:ui-tableau}. This UI can express 100\% of the queries in the log, with only 4 components. The select clause widget lets users select the set of attributes and expressions to return (they can drag a column from the bottom box and drop it in the top one). The where clause widget specifies the four predicates that were used; the order by widget shows the three attributes students combined to sort the output. The having clause widget simply adds a no-op expression to the query --- it is a side-effect of how Tableau automatically generates queries, which would likely be removed by the user.

 This use case is ``easy'' for \sys{} because Tableau generates highly structured transformations. In fact, more than 99\% of the edges in the interaction graph express changes of columns in the \texttt{GROUP BY} and the \texttt{SELECT} clause. Those actions correspond to drag and drops in the leftmost component in our interface.