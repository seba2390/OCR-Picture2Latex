
Semidefinite programs (SDP) are an important class of optimization problems \citep{vandenberghe1996semidefinite}, and are critical to several learning-related tasks, e.g., clustering \citep{shi2000normalized,abbe2017community}, matrix completion and regression \citep{recht2010guaranteed,candes2009exact}, kernel learning \citep{lanckriet2004learning}, sum-of-squares relaxations \citep{barak2015dictionary}, etc. 

However, solving SDPs in practice is a challenging task. Consider the following  canonical SDP: 
\begin{align}
\underset{X \in \R^{n \times n}}{\minimize} \quad & \ip{C}{X} \nonumber \\
\text{subject to} \quad & \ip{A_i}{X} =b_i,\quad i =1,\cdots, m, \text{ and } X \succeq 0,
\label{eq:sdp}
\end{align}
where $C, A_1, \ldots, A_m \in \R^{n\times n}$ are symmetric matrices, $\ip{A}{B} = \trace{A^T B}$, and $X$ is positive semidefinite. Such problems are convex and can be solved in polynomial time using classical iterative algorithms such as ellipsoid and interior-point methods~\citep{nesterov1994interior}.  However, these algorithms have super-linear complexity (in input size) and tend to scale poorly in practice, and are not well suited for typical learning tasks where both $m$ and $n$ can be fairly large. The two key challenges for these algorithms are: (a) a search space of high dimension on the order of $n^2$; and (b) the need to maintain positive semidefiniteness of the variable matrix $X$ throughout the iterations. 

%
%\TODO{However, even the best known algorithms have time-complexity of at least $O(mZ+m^3+mn^\omega)$ \citep{lee2015faster},  where $n\times n$ is the number of variables, $m$ is the number of constraints,  $Z = \nnz(C)+\sum_{i=1}^m \nnz(A_i)$ is the sparsity of the problem, and $O(n^{\omega})$ is the time required to multiply two arbitrary $n\times n$ matrices.---NB: my opinion is that we should remove this sentence entirely. It is complicated, appears very early in the paper, and is besides the point. The only thing we want to mention at this stage is the (well known) fact that IPM's take poly time, but that they're slow in practice. It doesn't help the reader to have these extra details here, especially not since we're not going to do any better.}
%
%Unfortunately, such techniques

In response to these challenges, \citet{burer2003nonlinear, burer2005local} suggested solving~\eqref{eq:sdp} by constraining the search space to matrices of rank at most $k$, using a parameterization of the form $X = UU^T$ where $U \in \Rnk$. This reduces the number of variables from $O(n^2)$ to $O(nk)$, and mechanically enforces positive semidefiniteness:
\begin{align}
	\underset{U \in \Rnk}{\minimize} \quad & \ip{C}{UU^T} \nonumber \\
	\text{subject to} \quad & \ip{A_i}{UU^T} = b_i,\quad i = 1,\cdots, m.
	\label{eq:factored}
\end{align}
This is equivalent to~\eqref{eq:sdp} with the additional constraint $\rank(X) \leq k$. This rank constraint is fairly natural, as several SDPs of interest are themselves relaxations of rank-constrained problems. Moreover, \citet{barvinok1995problems, pataki1998rank} showed that for every compact SDP with a solution, there exists a rank $\Omega(\sqrt{m})$ solution that is also globally optimal. While this ensures that the global optimum of the factored SDP problem (with $k=\Omega(\sqrt{m})$) is a global optimum of the original SDP problem, it is not immediately clear how to solve the factorized problem. 

In fact, the factorized problem is a non-convex quadratically constrained quadratic program which in general can be NP-hard. The challenge in solving the problem arises due to the non-convexity as well as due to constraints. In this work, we propose a simple penalty method that gets rid of the constraints and replaces them via a quadratic penalty in the objective function. The penalty formulation allows us to study first-order and second-order stationary points of the problem and lends itself to efficient algorithms, as we detail in this paper.


The proposed penalty formulation is given by: 
\begin{align}
	\underset{X \succeq 0}{\minimize} ~\quad \Fm(X) =  \ip{C}{X} + \mu \sum_{i=1}^m \left(\ip{A_i}{X} -b_i\right)^2,
	\label{eq:penalty_sdp}
\end{align}
where $\mu$ is generally a large positive constant. Notice that this is a convex problem. Intuitively, for increasingly large $\mu$, solutions of~\eqref{eq:penalty_sdp} converge to solutions of~\eqref{eq:sdp}.

Combining the formulation with the Burer--Monteiro factorization we get:
\begin{align}
	\underset{U \in \Rnk}{\minimize} \quad  L_{\mu}(U) =  \ip{C}{UU^T} + \mu \sum_{i=1}^m (\ip{A_i}{UU^T} -b_i)^2.
	\label{eq:penalty_factored}
\end{align}
The cost function $L_{\mu}$ is non-convex, and generic optimization algorithms can only guarantee computation of an approximate second-order stationary points (SOSP) \citep{cartis2012complexity,ge2015escaping}. That is, such algorithms converge to a point $U$ where the gradient of $L_\mu$ is small and the Hessian of $L_\mu$ is almost positive semidefinite. Such second-order stationary points need not be close to optimal in general.

We construct an explicit SDP where a suboptimal SOSP exists even for $k$ as large as $n-1$. However, we show that there are only measure zero of such bad SDPs. Hence, we show that if the cost matrix has a small amount of randomness then {\em any} SOSP of $L_\mu$ is a global optimum, as long as at least one SOSP exists. That is, for almost all cost matrices $C$, an SOSP of \eqref{eq:penalty_factored} corresponds to a global optimum. We would like to stress here that for certain non-compact SDPs existence of an SOSP itself is not guaranteed. However, as shown in Section~\ref{sec:applications}, SOSPs exists for several important SDPs. 

We next address the question of approximate optimality for approximate SOSPs, as optimization algorithms can only recover approximate SOSPs in polynomial time. Since there is a measure zero set of SDPs with bad SOSPs, there can be a non-zero (but small) measure set of SDPs with bad approximate SOSPs. We use {\em smoothed analysis} to avoid these bad SDPs, by perturbing the objective matrix. We show that for $k=\tilde \Omega(\sqrt{m})$, any approximate SOSP of  $L_{\mu}$  with a perturbed objective and bounded residues is approximately optimal to the penalty objective~\eqref{eq:penalty_sdp}. We further discuss settings under which all SOSPs of the penalty objective have bounded solutions (residues).

Since our results are about approximate SOSPs, and not any particular algorithm, it is an interesting question to see if we can adapt classical techniques such as interior point or cutting plane to optimize over the low dimensional, factored space. We provide results for gradient descent convergence in Section \ref{sec:gd}.

Finally, even though finding the smallest rank solution satisfying a set of linear equations is NP hard \citep{natarajan1995sparse}, our result shows how increasing the number of parameters (rank) makes the optimization of this non-convex problem easier. While the extreme case of rank $n$ makes the constraint trivial, our results show optimality for a non-trivial rank ($\tilde{\Omega}(\sqrt{m})$), and it is an interesting question to understand this trade-off in more detail.

\subsection{Main results}
The main contributions of this work are: 
\begin{itemize}
	\item We propose a simple penalty version of the factored SDP~\eqref{eq:factored} and show that, for almost all cost matrices $C$, any exact SOSP of the rank-constrained formulation~\eqref{eq:penalty_factored} is a global optimum for rank ${\Omega}(\sqrt{m})$---see Corollary~\ref{cor:exactpenaltyfactorized}. This result removes the smooth manifold requirement of~\citep{boumal2016non}, though it applies to~\eqref{eq:penalty_sdp}, not~\eqref{eq:sdp}.
	\item We show that there indeed exists a compact, feasible SDP with a worst-case $C$ for which the penalized, factorized problem admits a suboptimal SOSP (see Theorem~\ref{thm:bad_sdp}), even for rank almost as big as the dimension.
	\item We show that by perturbing the objective function slightly and by performing a smoothed analysis on the resulting problem, we can guarantee every approximate SOSP of the perturbed problem is an approximate global optimum of the perturbed and penalized SDP. Hence, we can use standard techniques~\citep{cartis2012complexity,ge2015escaping} to find approximate SOSPs and guarantee global optimality---see Theorem~\ref{thm:optimal_approx_compact}.
\end{itemize}

In summary, we show that for a class of SDPs with bounded solutions, we can find a low-rank solution that is close to the global optimum of the penalty objective. We believe that the factorization technique can be leveraged to design faster SDP solvers, and any looseness in the current bounds is an artifact of our proof, which hopefully can be tightened in future works. 

\subsection{Prior work}

Fast solvers for SDPs have garnered interest in the optimization and in the theoretical computer science communities for a long time. Most of the existing results for SDP solvers can be categorized into direct (convex) methods and factorization methods. \\

\noindent \textbf{Convex methods:}
Classical techniques such as interior point methods~\citep{ nesterov1989self, nesterov1988polynomial, alizadeh1995interior} and cutting plane methods~\citep{anstreicher2000volumetric,krishnan2003properties} enjoy geometric convergence, but their computational complexity per iteration is high. As a result, it is hard to scale these methods to SDPs with a large number of variables.

With the goal of speeding up the computation, many works have considered: i) a specific and important class of SDPs, namely, SDPs with a trace constraint ($\trace{X}=1$), and ii) methods with sub-linear convergence. For these SDPs, \citet{arora2005fast} proposed a multiplicative weights method which provides faster techniques for some graph problems, with running time depending on $O(\frac{1}{\eps^2})$ and the \textit{width} of the problem.  \citet{hazan2008sparse} proposed a Frank--Wolfe-type algorithm with a complexity of $\tilde{O}( \frac{Z}{\eps^{3.5}})$ where $Z$ is the sparsity of $C$ and the $A_i$'s.  \citet{garber2016sublinear, garber2016faster}  proposed faster methods that either remove the dependence on $Z$ (sub-linear time), or improve the dependence on $\eps$. While these methods improve the per iteration complexity, they still need significant memory as the rank of solutions for these methods is not bounded, and scales at least at the rate of $O(\frac{1}{\eps})$.  An exception to this is the work by \citet{yurtsever2017sketchy}, which uses sketching techniques in combination with conditional gradient method to maintain a low rank representation. However this method is guaranteed to find a low rank optimum only if the conditional gradient method converges to a low rank solution. \\

\noindent \textbf{Factorization methods:}
\citet{burer2003nonlinear, burer2005local} proposed a different approach to speed up computations, namely by searching for solutions with smaller rank.  Even though all feasible compact SDPs have at least one solution of rank $O(\sqrt{m})$ \citep{barvinok1995problems, pataki1998rank}, it is not an easy task to optimize directly on the rank-constrained space because of non-convexity. However,  \citet{burer2003nonlinear, burer2005local} showed that any \emph{rank-deficient local minimum} is optimal for the SDP; \citet{journee2010low} extended this result to any rank-deficient SOSP under restrictive conditions on the SDP. However, these results cannot guarantee that SOSPs are rank deficient, or at least that rank-deficient SOSPs can be computed efficiently (or even exist). \citet{boumal2016non} address this issue by showing that for a particular class of SDPs satisfying some regularity conditions, and for almost all cost matrices $C$, any SOSP of the rank-constrained problem with $k = \Omega(\sqrt{m})$ is a global optimum. Later, \citet{pmlr-v65-mei17a} showed that for SDPs with elliptic constraints (similar to the Max-Cut SDP), any rank-$k$ SOSP gives a $(1-\frac{1}{k-1})$ approximation to the optimum value. Both these results are specific to particular classes of SDPs and do not extend to general problems.
 
In a related setup, \citet{keshavan2010matrix, jain2013low} have showed that rank-constrained matrix completion problems can be solved using smart initialization strategies followed by local search methods. Following this, many works have identified interesting statistical conditions under which certain rank-constrained matrix problems have no spurious local minima \citep{ sun2016geometric, bandeira2016low, ge2016matrix, bhojanapalli2016global, park2017non, ge2017no, zhu2017global, ge2017optimization}.  These results are again for specific problems and do not extend to general SDPs.

In contrast, our result holds for a large class of SDPs in penalty form, without strong assumptions on the constraint matrices $A_i$ and for a large class of cost matrices $C$. We avoid degenerate SDPs with spurious local minima by perturbing the problem and then using a smoothed analysis, which is one of the main contribution of the work. 


%\TODO{overstatement} In contrast, our result holds for {\em any} SDP of form~\eqref{eq:sdp} (with feasible and bounded solutions), i.e., it holds for any set of constraint matrices $A_i$ and almost all cost matrices $C$. We avoid the issue of spurious local minima by perturbing the problem and then using a smoothed analysis, which is one of the main contribution of the work. 

\subsection*{Notation}

For a smooth function $f(X)$, we refer to first-order stationary points $X$ as FOSPs. Such points satisfy $\nabla f(X) = 0$ (zero gradient). We refer to second-order stationary points at SOSPs. Such points are FOSPs and furthermore satisfy $\nabla^2 f(X) \succeq 0$, i.e., the Hessian is positive semidefinite. The set of symmetric matrices of size $n$ is $\Snn$. $\sigma_i()$  and $\lambda_i()$ denote the $i$th singular- and eigenvalues respectively, in decreasing order.
