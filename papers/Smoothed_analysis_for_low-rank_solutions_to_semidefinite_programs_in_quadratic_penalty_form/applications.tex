\section{Applications}\label{sec:applications}

In this section, we present applications of our results to two SDPs: Max-Cut and matrix completion, both of which are important problems in the learning domain and have been studied extensively. Interest has grown to develop efficient solvers for these SDPs~\citep{arora2007combinatorial, pmlr-v65-mei17a, hardt2013understanding, bandeira2016low}.
%General SDP solvers such as interior point or ellipsoid method can lead to slow algorithms for these problems. Instead, over the years, specialized efficient algorithms have been developed for these problems \citep{arora2007combinatorial, hardt2013understanding}. 

This work differs from previous efforts in at least two ways. First, we aim to demonstrate that Burer--Monteiro-style approaches, which are often used in practice, can indeed lead to provably efficient algorithms for general SDPs. We believe that building upon this work, it should be possible to improve the time-complexity guarantees of such factorization-based algorithms. Second, we note that several problems formulated as SDPs in fact necessitate low-rank solutions, for example because of memory concerns (as is the case in matrix completion),  and factorization approaches provide a natural means to control rank. % For such problem, existing methods do not provide low rank solutions while our method is guaranteed to give a low-rank solution. 

%We note that both of these problems have been extensively studied and for both of them there exist highly specialized algorithms that are highly efficient~\cite{arora2007combinatorial,hardt2013understanding}. Our results here do not beat them; rather our goal here is to demonstrate that the Burer-Monteiro approach can successfully solve these SDPs in polynomial time. In practice, this approach is much faster than other generic SDP solvers such as interior point method and ellipsoid method, and in addition returns low rank solutions.
\subsection{Max-Cut}

We first consider the popular Max-Cut problem which finds applications in clustering related problems. In a seminal paper, \cite{goemans1995improved} defined the following SDP to solve the Max-Cut problem: $\min_{X\in \Rnn} \ip{C}{X}, \mbox{s.t. } X_{ii} = 1 \; \forall \; 1 \leq i \leq n, X \succeq 0 $, where $n$ is the number of vertices in the given graph and $C$ is its adjacency matrix. Since the constraint set also satisfies $\trace{X}=n$, we consider the following penalized, non-convex version of the problem.
% \begin{align*}	& \min_{X\in \Rnn} \ip{C}{X} \\
%	& \mbox{s.t. } X_{ii} = 1 \; \forall \; 1 \leq i \leq n \\
%	& \qquad X \succeq 0,
%\end{align*}
\begin{align}
	\widehat{L}_{\mu}(U) \defeq \ip{C+G}{U\trans{U}} + \mu\left(\left(\ip{I}{U\trans{U}}-n\right)^2 + \sum_{i=1}^{n}\left(\ip{e_i \trans{e_i}}{U\trans{U}}-1\right)^2\right),\label{eqn:maxcut}
\end{align}
where $G$ is a random symmetric Gaussian matrix.  Let $\widehat F_{\mu}(UU^T) = \widehat L_{\mu} (U)$. After some simplifying computations, we have the following corollary of Theorem~\ref{thm:optimal_approx_compact}.
\begin{corollary}\label{cor:maxcut}
There exists an absolute numerical constant $c_1$ such that the following holds. With probability greater than $1-\delta$,
every $(\eps, \gamma)$-SOSP $U$ of the perturbed Max-Cut problem $\widehat{L}_{\mu}(U)$~\eqref{eqn:maxcut} with:
\begin{align*}\epsilon \leq \frac{1}{c_1} \left(\frac{\gamma \sigma_G^2}{\mu n}\right)^{2/3},~~ \text{ and } ~~  k = \tilde{\Omega} \left( \sqrt{n \log\left(\frac{\mu^2 \sqrt{n}}{\sigma_G}\right)}\right),
\end{align*}
satisfies $	\widehat{F}_{\mu}(UU^T) - \widehat{F}_{\mu}(X^*) \leq \gamma \sqrt{\epsilon} \trace{X^*} +\frac{1}{2} \epsilon \frob{U}$, where $X^*$ is a global optimum of $\widehat{F}_{\mu}(X)$.
%\begin{align*}
%	\widehat{L}_{\mu}(U) - \widehat{L}_{\mu}(U^*) \leq \gamma \sqrt{\epsilon} \trace{U^* \trans{U^*}} + \epsilon \frob{U}.
%\end{align*}
\end{corollary}
The above result states that for the penalized version of the perturbed Max-Cut SDP, the Burer--Monteiro approach finds an approximate global optimum as soon as the factorization rank $k = \tilde{\Omega}(\sqrt{n})$. Existing results for Max-Cut using this approach either only handle exact SOSPs~\citep{boumal2016non}, or require $k=n+1$~\citep{boumal2016globalrates}, or require $k$ that is dependent on $\frac{1}{\eps}$~\citep{pmlr-v65-mei17a}. Moreover, complexity per iteration scales only linearly with the number of edges in the graph. %However, current analysis is required to set $\epsilon$ to be fairly small which can lead to a super-linear algorithm; we leave further tightening of dependence on $\epsilon$ for future work.


\subsection{Matrix Completion}
In this section we specialize our results for the matrix completion problem \cite{candes2009exact}. The goal of a matrix completion problem is to find a low-rank matrix $M$ using only a small number of its entries, with applications in recommender systems. To ensure that the computed matrix is low-rank and generalizes well, one typically imposes nuclear-norm regularization which leads to the following SDP: 

\begin{minipage}{0.2\linewidth}
	\begin{align*}
	\min &\quad \trace{W_1} + \trace{W_2}\\ \text{s. t. }&\quad X_{ij} =M_{ij}, (i,j) \in \calS \\  &\quad \begin{bmatrix}W_1 & X \\ X^T & W_2\end{bmatrix} \succeq 0.
	\end{align*}
\end{minipage}
\begin{minipage}{0.05\linewidth}
	\begin{align*}
		\equiv \\
	\end{align*} \break
\end{minipage}
\begin{minipage}{0.6\linewidth}
	\begin{align*}
	\min & \quad \ip{I}{Z} \nonumber \\ \text{s. t. }&\quad \frac{1}{2}\ip{e_{i+n}e_{j+n}^T + e_{j+n} e_{i+n}^T}{Z} = M_{ij}, (i,j) \in \calS \nonumber \\  &\quad Z \succeq 0.
	\end{align*}
\end{minipage}
%\begin{align*} \min &\quad \trace{W_1} + \trace{W_2}\\ \text{s. t. }&\quad X_{ij} =M_{ij}, (i,j) \in \Omega \\  &\quad \begin{bmatrix}W_1 & X \\ X^T & W_2\end{bmatrix} \succeq 0.
%\end{align*} 
\noindent Here $\calS$ is the set of observed indices of $M$ and $Z\defeq \begin{bmatrix}W_1 & X \\ X^T & W_2\end{bmatrix}$. % by $Z$, we can rewrite the above SDP as \begin{align} \min & \quad \ip{Z}{I} \nonumber \\ \text{s. t. }&\quad \frac{1}{2}\ip{e_{i+n}e_{j+n}^T + e_{j+n} e_{i+n}^T}{Z} = M_{ij}, (i,j) \in \Omega \nonumber \\  &\quad Z \su,cceq 0. \label{eq:mc_sdp}\end{align}
Let
\begin{align}
	\widehat{L}_{\mu}(U) = \ip{I+G}{UU^T} + \mu \sum_{i=1}^m \left(\frac{1}{2}\ip{e_{i+n}e_{j+n}^T + e_{j+n} e_{i+n}^T}{UU^T} - M_{ij} \right)^2  \label{eq:matcomp}
\end{align}
be the corresponding penalty objective.  Let $\widehat F_{\mu}(UU^T) = \widehat L_{\mu} (U)$. The objective is positive definite with $\lambda_1(C)=\lambda_n(C)=1$. Also, since $\calA$ is a sub-sampling operator, $\|\calA\| \leq 1$. Finally, for $\eps^2 \leq \frac{\mu}{2}\sqrt{\sum_{(i,j) \in \calS} M_{ij}^2}$, the residues are bounded by: \begin{align*} B&=\|\calA\| \max \left \{ \left( \frac{2\eps} {\lambda_n(C)}\right)^2, \frac{2\mu}{\lambda_n(C)} \|\vb\|_2^2 \right \}+\|\vb\|_2 \leq \max  3\mu \sqrt{\sum_{(i,j) \in \calS} M_{ij}^2}. \end{align*}


\noindent Applying Theorem~\ref{thm:optimal_approx} for this setting gives the following corollary.
\begin{corollary}\label{cor:mc_optimal}There exists an absolute numerical constant $c_2$ such that the following holds. With probability greater than $1-\delta$,
every $(\eps, \gamma)$-SOSP $U$ of the perturbed matrix completion problem $\widehat{L}_{\mu}(U)$~\eqref{eq:matcomp} with:
\begin{align*}\sG \leq \frac{1}{4\sqrt{n \log(n/ \delta)}},~~ \eps \leq \frac{1}{c_2}\left(\frac{\gamma \abs{\calS} \sigma_G^2 }{ n  \mu }\right)^{\sfrac{2}{3}}, ~\text{ and }~ k = \tilde{\Omega} \left( \sqrt{ \abs{\calS}   \log\left(\frac{\mu^2 \sqrt{n} \sqrt{\sum_{(i,j) \in \calS} M_{ij}^2}}{\sigma_G}\right) } \right),\end{align*} satisfies $\widehat{F}_{\mu}(UU^T)  - \widehat F_{\mu}(X^*)  \leq \gamma \sqrt{\epsilon} \trace{X^*} + \frac{1}{2} \eps \|U\|_F$, where $X^*$ is a global optimum of $\widehat{F}_{\mu}(X)$.
\end{corollary}
\noindent This result shows that for the  matrix completion problem with $m$ observations, for rank $\tilde{\Omega}(\sqrt{m})$, any approximate local minimum of the factorized and penalized problem is an approximate global minimum. 

Most of the existing results on matrix completion either require strong distribution assumptions on $\calS$ and incoherence assumptions on $M$ to recover a low-rank solution \citep{candes2009exact, jain2013low}. The standard nuclear norm minimization algorithms are not guaranteed to converge to low-rank solutions without these assumptions,  which implies that the entire matrix would need to be stored for prediction which is infeasible in practice. Similarly,  generalization error bounds \citep{foygel2011concentration} as well as differential privacy guarantees  depend on recovery of a low-rank solution.

Our result guarantees finding a rank -$\tilde{\Omega}(\sqrt{m})$ solution without any statistical assumptions on the sampling or the matrix. The tradeoff is our results do not guarantee finding a lower (potentially a constant) rank solution, even if one exists for a given problem. 


%\subsection{Normalized Cut}
%
%In this section we will consider the problem of computing the Normalized cut of a given graph $\calG$ \citep{shi2000normalized}. Given a graph $G$ with $n$ vertices and $e$ edges, the normalized cut is the partition of vertices into two sets $S$ and $S^c$ that minimizes, $$\frac{cut(S,S^c)}{D(S)}+ \frac{cut(S,S^c)}{D(S^c)}. $$ Here $cut(S,S^c)$ is the number of edges between $S$ and $S^c$. $D(S)$ is the sum of degree of vertices in $S$. This problem is NP-hard in the worst case. 
%
%Let $\widehat{X}$ be a $n+1 \times n+1$ matrix, with the following structure, $\widehat{X} = \begin{bmatrix}  X & 0_{n \times 1} \\ 0_{1 \times n} & x \end{bmatrix}$. \citet{bie2006fast} proposed the following SDP relaxation to find the normalized cut.
%
% \begin{align*} \underset{\widehat{X}}{\minimize} &\quad \ip{\widehat{X}}{L}\\ \text{subject to } &\quad \widehat{X} \succeq 0 \\
%&\quad \ip{\widehat{X}}{A_i} =0, i \in [n]\\
%&\quad \ip{\widehat{X}}{A_{n+1}}=-1 \\
%\end{align*}
%Here $L$ is the graph Laplacian divided by twice the number of edges, $2e$. $A_i$ ($i \in [n]$) is a matrix with $1$ at $(i,i)$ and $-1$ at $(n+1,n+1)$ entry, with rest of the entries begin $0$s. $\displaystyle A_{n+1} =\begin{bmatrix} dd^T/(4e^2) & 0_{n \times 1} \\0_{1 \times n} & -1 \end{bmatrix}$, where $d$ is the vector of degrees of the vertices.
%
%
%
