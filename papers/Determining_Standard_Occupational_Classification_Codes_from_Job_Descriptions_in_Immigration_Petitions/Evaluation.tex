\section{Evaluation}
\label{sec:Evaluation}

\subsection{Dataset}

Our dataset consists of 46,999 labeled instances, where each instance corresponds to a visa petition. For every instance, the relevant attributes include \textit{job title}, \textit{job description}, \textit{company name}, \textit{SOC code (normalized)} (which we will refer to as simply \textit{SOC code}), and \textit{SOC occupation} which is a moniker of the SOC code. We exclude company name from the model since we have found it to be irrelevant to the predictive task; moreover, the predictive model should be able to generalize to all companies. We exclude job title from the model as well because we have found many instances of the same job title being associated with different SOC codes in this dataset, suggesting that job title does not consistently map to SOC code. Therefore, we use job description as the only input to our models. Since SOC occupation is simply a moniker of SOC code, we use SOC code as the only output of our models.

It is worth noting that the distribution of SOC codes in this dataset is uneven. While the dataset includes abundant examples of the most common categories, less frequent codes may not have sufficient instances. To build a predictive model that is accurate for a majority of use cases, we focus on the 5 most frequent codes, which results in a dataset with 32,262 instances.

\subsection{Experimental Setup}

Our experiments are implemented using Python 3 as the programming language, in interactive notebooks hosted on the Databricks\footnote{\url{https://databricks.com/}} platform. Other standard libraries used include Scikit-learn \cite{scikit-learn} for sparse vector representations and training classifiers, Gensim for doc2vec  \cite{rehurek_lrec}, Numpy \cite{numpy} for numerical computations, Pandas \cite{pandas} for tabular data processing, and Matplotlib \cite{matplotlib} for plotting. We use Managed MLflow\footnote{https://databricks.com/product/managed-mlflow} for deployment.

We have implemented and benchmarked 14 classifiers, 7 of which are based on TF-IDF $n$-gram representation, while the rest are based on doc2vec representation. These are compared in terms of training time, accuracy, precision, recall, and f1 score \cite{scikit-learn-metrics}.  The values of these metrics are averaged over 10-fold cross validation and reported.

\subsection{Hyperparameters}

All hyperparameters used in this evaluation are manually tuned. Automatic parameter tuning is outside the scope of this paper and left as future work.

\subsubsection{Vector Representation}

For TF-IDF $n$-gram representation, we use $ 1 \leq n \leq 10$. However, $n$-grams that occur in fewer than 10\% of the instances or greater than 90\% of the instances are ignored. The resulting sparse vectors have a dimensionality of 858. For doc2vec, we use a dimensionality of 100.

\subsubsection{Predictive Modeling}

For $k$-nearest neighbor classifiers, we use $k = 3$. For random forest classifiers, we use an ensemble of 100 estimators.

\subsection{Experimental Results}

\subsubsection{Accuracy}

We measure accuracy as the fraction of predictions that are correct. Figure \ref{fig:Accuracy} compares the accuracies of the models being evaluated.
\begin{figure}[h!]
    \centering
    \includegraphics[scale = 0.50]{figures/accuracy.jpg}
    \caption{Accuracy scores of SOC Code predictors.}
    \label{fig:Accuracy}
\end{figure}

\subsubsection{Precision}

In a binary classification problem, precision is defined as the fraction of all positive predictions that are correct. Since our problem involves more than two classes, we report the macro average, i.e., the average of precision scores measured with respect to each SOC code in the dataset \cite{scikit-learn-precision}. Figure \ref{fig:Precision} shows the macro average precision scores of the models.
\begin{figure}[h!]
    \centering
    \includegraphics[scale = 0.5]{figures/precision.jpg}
    \caption{Precision (macro average) of SOC Code predictors.}
    \label{fig:Precision}
\end{figure}

\subsubsection{Recall}

In a binary classification problem, recall is defined as the fraction of all positive instances that are correctly predicted as positive. Since our problem involves more than two classes, we report the macro average, i.e., the average of recall scores measured with respect to each SOC code in the dataset \cite{scikit-learn-recall}. Figure \ref{fig:Recall} shows the macro average recall scores of the models.
\begin{figure}[h!]
    \centering
    \includegraphics[scale = 0.5]{figures/recall.jpg}
    \caption{Recall (macro average) of SOC Code predictors.}
    \label{fig:Recall}
\end{figure}

\subsection{F1 Score}

In a binary classification problem, f1 score is defined as the harmonic mean of precision and recall. Since our problem involves more than two classes, we report the macro average, i.e., the average of f1 scores measured with respect to each SOC code in the dataset \cite{scikit-learn-f1}. Figure \ref{fig:f1} shows the macro average f1 scores of the models.
\begin{figure}[h!]
    \centering
    \includegraphics[scale = 0.5]{figures/f1.jpg}
    \caption{F1 score (macro average) of SOC Code predictors.}
    \label{fig:f1}
\end{figure}

\subsection{Training Time}

Finally, the time taken (in seconds) to train each model (averaged over 10-fold cross validation as will all the other metrics) is shown in Figure \ref{fig:TrainingTime}.
\begin{figure}[h!]
    \centering
    \includegraphics[scale = 0.5]{figures/training_time.jpg}
    \caption{Training time of SOC Code predictors.}
    \label{fig:TrainingTime}
\end{figure}

In the next section, we interpret these results.
