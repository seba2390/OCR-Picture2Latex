\section{Methodology}
\label{sec:Methodology}

We begin by formally defining our problem.  
\subsection{Problem Statement}
\label{subsec:ProblemStatement}

\noindent \textbf{Input:}
\begin{enumerate}
    \item $\Sigma$: a finite alphabet. $\Sigma^+$ denotes the set of all non-empty strings over $\Sigma$. In this paper, we focus on strings that are job descriptions expressed as free form text.
    \item $\mathcal{Y}$: a finite set of labels. In this paper, SOC codes are treated as labels.
    \item $\mathcal{D} = \{(x_i, y_i): 1 \leq i \leq n \}$: a labeled dataset of size $n \in \mathbb{N}$, where $x_i \in \Sigma^+$ is a job description, and $y_i \in \mathcal{Y}$ is its corresponding SOC code.
\end{enumerate}

\noindent \textbf{Output:}
A function $f: \Sigma^+ \rightarrow \mathcal{Y}$ which maps a job description $x$ to an SOC code $y = f(x)$ such that $f$ minimizes the expected error with respect to some loss function.

From a pragmatic standpoint, we want such a function $f$ to be available as a web service (i.e., web API) which accepts a request containing description $x$ to produce a response containing the predicted SOC code $y = f(x)$.

\subsection{Approach}
\label{subsec:Approach}

Our approach may be described as a sequence of steps as follows.

\subsubsection{Text Vectorization}
Since a majority of machine learning algorithms assume inputs to be real valued vectors, predictive modeling based on text often requires vectorizing the text, i.e., computing real valued vector representation of text. We consider two different vectorization techniques, which are as follows.
\paragraph{TF-IDF $n$-grams} An $n$-gram ($n \in \mathbb{N}$) is a sequence of $n$ tokens. Given $n_{\mathrm{min}}, n_{\mathrm{max}} \in \mathbb{N}$ ($n_{\mathrm{min}} \leq n_{\mathrm{max}}$), a corpus of text in $\Sigma^+$ can be used to compute the vocabulary of all $n$-grams where $n_{\mathrm{min}} \leq n \leq  n_{\mathrm{max}}$. Subsequently, any string $x \in \Sigma^+$ may be represented as a vector of counts, i.e., term frequencies (TF) of $n$-grams present in $x$. Such a vector representation of a string is typically sparse, i.e., most of its components are zero, since most $n$-grams in the vocabulary are typically absent in it. To offset the effect of highly frequent $n$-grams with little semantic value, the vectors are weighted by inverse document frequencies (IDF), resulting in TF-IDF $n$-gram representations.
While TF-IDF representations have been found to achieve high accuracy in text categorization \cite{DBLP:conf/ecml/Joachims98}, the high dimensionality of the sparse vectors generally entails high computational costs for training predictive models.
\paragraph{Doc2vec} An alternative approach that addresses the issue of dimensionality consists of using neural architectures for vectorizing words \cite{mikolov2013efficient} and strings \cite{DBLP:conf/icml/LeM14}, using contextual similarity to predict semantic similarity. The resulting representations are known as word embeddings and document embeddings, respectively, and the above neural architectures are referred to as word2vec and doc2vec, respectively. Embeddings computed by word2vec and doc2vec are typically of lower dimensionality compared to TF-IDF $n$-gram representations. Therefore, such embeddings are considered dense vector representations. Since job descriptions are strings of arbitrary length, we use doc2vec to compute dense vector representations of such descriptions.

\subsubsection{Predictive Modeling}
For each type of vectorization, we train a set of standard classifiers for predicting SOC code, namely, $k$-nearest neighbors (KNN), Gaussian na\"ive Bayes (GNB), logistic regression (LR), linear support vector machine (LinearSVC), support vector machine with radial basis function (SVC-RBF), decision tree (DT), and random forest (RF).

\subsubsection{Evaluation and Model Selection}
To evaluate the models, we use $n$-fold cross validation. The dataset is first divided into $n$ slices (or folds) of (roughly) equal size. In each round of cross validation, a different slice is held out for testing while the remaining $n - 1$ slices are used for training. Several metrics are recorded in each round. At the end of $n$ rounds of training and testing, these metrics are averaged and reported. These scores help identify models best suited to the problem.

\subsubsection{Deployment}
Once a model has been selected, we deploy it as a web service which can accept a \texttt{POST} request whose body contains a job description in free form text and produce a response containing the predicted SOC code.

The next section presents our empirical evaluation.

