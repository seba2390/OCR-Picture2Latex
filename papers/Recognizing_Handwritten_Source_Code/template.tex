% $Id: template.tex 11 2007-04-03 22:25:53Z jpeltier $
\documentclass{vgtc}                          % final (conference style)
%\documentclass[review]{vgtc}                 % review
%\documentclass[widereview]{vgtc}             % wide-spaced review
%\documentclass[preprint]{vgtc}               % preprint
%\documentclass[electronic]{vgtc}             % electronic version

%% Uncomment one of the lines above depending on where your paper is
%% in the conference process. ``review'' and ``widereview'' are for review
%% submission, ``preprint'' is for pre-publication, and the final version
%% doesn't use a specific qualifier. Further, ``electronic'' includes
%% hyperreferences for more convenient online viewing.

%% Please use one of the ``review'' options in combination with the
%% assigned online id (see below) ONLY if your paper uses a double blind
%% review process. Some conferences, like IEEE Vis and InfoVis, have NOT
%% in the past.

%% Figures should be in CMYK or Grey scale format, otherwise, colour 
%% shifting may occur during the printing process.

%% These few lines make a distinction between latex and pdflatex calls and they
%% bring in essential packages for graphics and font handling.
%% Note that due to the \DeclareGraphicsExtensions{} call it is no longer necessary
%% to provide the the path and extension of a graphics file:
%% \includegraphics{diamondrule} is completely sufficient.
%%
\ifpdf%                                % if we use pdflatex
  \pdfoutput=1\relax                   % create PDFs from pdfLaTeX
  \pdfcompresslevel=9                  % PDF Compression
  \pdfoptionpdfminorversion=7          % create PDF 1.7
  \ExecuteOptions{pdftex}
  \usepackage{graphicx}                % allow us to embed graphics files
  \DeclareGraphicsExtensions{.pdf,.png,.jpg,.jpeg} % for pdflatex we expect .pdf, .png, or .jpg files
\else%                                 % else we use pure latex
  \ExecuteOptions{dvips}
  \usepackage{graphicx}                % allow us to embed graphics files
  \DeclareGraphicsExtensions{.eps}     % for pure latex we expect eps files
\fi%

%% it is recomended to use ``\autoref{sec:bla}'' instead of ``Fig.~\ref{sec:bla}''
\graphicspath{{figures/}{pictures/}{images/}{./}} % where to search for the images
\usepackage{microtype}                 % use micro-typography (slightly more compact, better to read)
\PassOptionsToPackage{warn}{textcomp}  % to address font issues with \textrightarrow
\usepackage{textcomp}                  % use better special symbols
\usepackage{mathptmx}                  % use matching math font
\usepackage{times}                     % we use Times as the main font
\renewcommand*\ttdefault{txtt}         % a nicer typewriter font
\usepackage{cite}                      % needed to automatically sort the references
\usepackage{tabu}                      % only used for the table example
\usepackage{booktabs}   % only used for the table example
\usepackage{caption} 
\captionsetup[table]{skip=10pt}
\usepackage{color}


%%% 
%%% Meta notes
%%%
\newlength{\dummylen}
\newcommand{\NOTE}[1]{\setlength{\dummylen}{\fboxrule}\setlength{\fboxrule}{2pt}%
            \vspace{1ex}\noindent\hfill%
            \fbox{\begin{minipage}{.96\columnwidth}#1\end{minipage}}%
            \setlength{\fboxrule}{\dummylen}\hfill{}\vspace{1ex}}

%\renewcommand{\NOTE}[1]{\ignorespaces}



%% We encourage the use of mathptmx for consistent usage of times font
%% throughout the proceedings. However, if you encounter conflicts
%% with other math-related packages, you may want to disable it.


%% If you are submitting a paper to a conference for review with a double
%% blind reviewing process, please replace the value ``0'' below with your
%% OnlineID. Otherwise, you may safely leave it at ``0''.

\onlineid{0}

%% declare the category of your paper, only shown in review mode
\vgtccategory{Research}

%% allow for this line if you want the electronic option to work properly
\vgtcinsertpkg

%% In preprint mode you may define your own headline.
%\preprinttext{To appear in an IEEE VGTC sponsored conference.}

%% Paper title.

\title{Recognizing Handwritten Source Code}
%\title{Writing Source Code (literally!)
%% This is how authors are specified in the conference style

%% Author and Affiliation (single author).
%%\author{Roy G. Biv\thanks{e-mail: roy.g.biv@aol.com}}
%%\affiliation{\scriptsize Allied Widgets Research}

%Author and Affiliation (multiple authors with single affiliations).
\author{Qiyu Zhi\thanks{e-mail: qzhi@nd.edu} %
\and Ronald Metoyer\thanks{e-mail:rmetoyer@nd.edu}}
\affiliation{\scriptsize University of Notre Dame}

%% Author and Affiliation (multiple authors with multiple affiliations)
% \author{Roy G. Biv\thanks{e-mail: roy.g.biv@aol.com}\\ %
%         \scriptsize Starbucks Research %
% \and Ed Grimley\thanks{e-mail:ed.grimley@aol.com}\\ %
%      \scriptsize Grimley Widgets, Inc. %
% \and Martha Stewart\thanks{e-mail:martha.stewart@marthastewart.com}\\ %
%      \parbox{1.4in}{\scriptsize \centering Martha Stewart Enterprises \\ Microsoft Research}}

%% A teaser figure can be included as follows, but is not recommended since
%% the space is now taken up by a full width abstract.
% \teaser{
%  \includegraphics[width=1.5in]{sample.eps}
%  \caption{Lookit! Lookit!}
% }

%% Abstract section.
\abstract{
%Programming requires textual input and typically requires typing which is not always the %optimal input method for all users - especialy on touchscreen devices. 
Supporting programming on touchscreen devices requires effective text input and editing methods.  Unfortunately, the virtual keyboard can be inefficient and uses valuable screen space on already small devices.  Recent advances in stylus input make handwriting a potentially viable text input solution for programming on touchscreen devices.  
The primary barrier, however, is that handwriting
recognition systems are built to take advantage of the rules of natural language, not those of a programming language. In this paper, we explore this particular problem of handwriting recognition for source code.  
We collect and make publicly available a dataset of handwritten \textit{Python} code samples from 15 participants and we characterize the typical recognition errors for this handwritten \textit{Python} source code when using a state-of-the-art handwriting recognition tool.  We present an approach to improve the recognition accuracy by augmenting a handwriting recognizer with the programming language grammar rules. Our experiment on the collected dataset shows an 8.6\% word error rate and a 3.6\% character error rate which outperforms standard handwriting recognition systems and compares favorably to typing source code on virtual keyboards.
} % end of abstract

%% ACM Computing Classification System (CCS). 
%% See <http://www.acm.org/class/1998/> for details.
%% The ``\CCScat'' command takes four arguments.


\keywords{programming, handwriting recognition, touch screen, source code, python.}
\CCScatlist{ 
  \CCScat{H.1.2.}{User/Machine Systems}{Human information processing}{};
  \CCScat{H.5.2.}{User Interfaces}{Input devices and strategies (e.g., mouse, touchscreen)}{}
}


%% Copyright space is enabled by default as required by guidelines.
%% It is disabled by the 'review' option or via the following command:
% \nocopyrightspace

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%% START OF THE PAPER %%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{document}

%% The ``\maketitle'' command must be the first command after the
%% ``\begin{document}'' command. It prepares and prints the title block.

%% the only exception to this rule is the \firstsection command
%\firstsection{Introduction}

\maketitle

\section{Introduction} %for journal use above \firstsection{..} instead
%Supporting programming on mobile devices is not a novel idea. 
With the rapid technology shift in current computing devices, high-quality low-cost mobile devices such as tablets and smartphones are being increasingly used in everyday activities. Many tasks that previously required a PC are now feasible on mobile devices. For example, tablets are typically equipped with powerful batteries, advanced graphic processors, high-resolution screens and fast processors, making writing and compiling code on them completely plausible. TouchDevelop, for example, is a novel programming environment, language and code editor for mobile devices  \cite{tillmann2011touchdevelop}. Furthermore, Tillmann et al. predict that programming on mobile devices will be widely used for teaching programming \cite{tillmann2012future}.  However, mobile devices are also inherently restricted by their limitations such as small screens and the clumsy virtual keyboard.  Entering and editing large amounts of text for programming tasks can quickly become difficult and time consuming with these virtual keyboards because they are notoriously difficult to use when compared to a physical keyboard and they consume valuable screen space\cite{raab2013refactorpad}.

%that is difficult to use and that takes up valuable screen space - posing challenges for a programmer who wishes to enter or edit source code.


% comparing handwriting and typing in different scenarios to motivate the work. 
While keyboards have been the primary input device for entering computer programs since the computer was invented\cite{gordon2013improving}, this predominant mechanism is not ideal for all programming situations. For example, software developers that suffer from repetitive strain injuries (RSI) and related disabilities may find typing on a keyboard difficult or impossible \cite{begelprogramming}.
%people suffering from repetitive strain injuries (RSI), carpal tunnel syndrome, or other motor impairments may experience tremendous difficulty using a keyboard and mouse \cite{arnold2000programming}.
Instead, handwriting with a stylus may be a preferred input mechanism for some of these users \cite{mankoff1998cirrin}.
%In addition, entering and editing large amounts of text for programming tasks can quickly become difficult and time consuming with
%\emph{virtual} keyboards, for any user, because they are notoriously difficult to use when compared to a physical keyboard and they consume valuable screen space\cite{raab2013refactorpad}.
%People with disabilities  (e.g. limb loss) or who do not have full mobility may also find typing on a keyboard  (physical or virtual) is difficult or impossible \cite{begelprogramming}. 
In addition, some physical configurations (e.g. seated on a plane) may simply be more suited to the writing posture than a typing posture for many users.

Handwriting has also been shown to have potential cognitive benefits\cite{alonso2015metacognition}.  In particular, Mueller and Oppenheimer found that students who took longhand notes performed better on conceptual questions than those that typed notes on a laptop \cite{mueller2014pen}.  Given these findings, and that fact that many programmers write pseudocode by hand before typing, it is reasonable to consider that handwriting may provide cognitive benefits for programming, especially on mobile devices. 
Furthermore, recent advances in pen-based input and handwriting recognition technology are quickly making handwriting a viable alternative to typing.
%Entering and editing large amounts of text for
%programming tasks can quickly become difficult and %time consuming
%with \emph{virtual} keyboards that are notoriously %difficult to use when compared to a physical %keyboard and consume valuable screen %space\cite{raab2013refactorpad}. 
%It is, therefore, important that we explore handwriting as an input option to support these users and their needs.
%keyboards are not conveniently available on many of today's touchscreen devices or come in the form of \emph{virtual} keyboards that are notoriously difficult to use and require the use of valuable screen space \cite{}.  


% One alternative to typing is to use speech (via dictation) to create the text of a program. Desilets et al. \cite{desilets2006voicecode} propose VoiceCode, which translates the pronounced syntax into native syntax in the current programming language to support programming by speech. Gordon \cite{gordon2013improving} employs language design and incorporates dynamic context for this purpose. Speech, however, is not always an appropriate option given social conventions and privacy issues.
% %While many alternatives have been explored, few have thoroughly examined the use of handwriting as a means for textual input, especially when considering source code. 
% Given the recent advances in pen-based input, however, handwriting is a potentially viable alternative to typing and speech.
%other input forms, such as speech, which are not always appropriate.

%Given recent advances in pen-based input, handwriting code is becoming a potentially viable alternative to typing.


In this paper, we explore the use of handwriting as a means for source code text input.  There are two ways to approach this problem.  One alternative is to develop or modify a handwriting recognition engine to take source code directly into account.  Given that source code often includes English language words, another alternative is to leverage the capabilities of an existing English language handwriting recognition engine. We explore this latter option.  First, we collect and present a publicly available dataset of handwritten \textit{Python} source code for use in handwriting recognition research.  Second, we explore the use of the state-of-the-art recognition system, MyScript \cite{myscript} for recognizing \textit{Python} source code.  We characterize the errors made by the MyScript engine and present a method for post-processing the engine's results to improve recognition performance on handwritten \textit{Python} source code.   

After presenting related work and necessary background information, we describe our data collection process and the resulting publicly available dataset. We then describe the performance of MyScript on recognizing the handwritten source code and present our algorithm for leveraging the MyScript engine to produce improved results.  We discuss those results in Section \ref{results} and conclude with avenues of future work.


%leveraging programming language grammar and lexicon information into a current handwriting recognition system, MyScript \cite{myscript}. Our primary contributions include a handwritten source code dataset, a characterization of errors made when using a general handwriting recognition system on the source code, and an algorithm to leverage general handwriting recognition systems to produce improved results for handwritten source code.


% overview of our work

%Another possible situation is when keyboards are not available, which is becoming a common scenario because touchscreen devices such as the advanced iPad and Android-based tablet is sufficient for daily use.

\section{Background and Related Work}
% Handwriting source code
% For each paper, what did they do  (few sentences), and what did they find, how are we going to be different.

% Handwriting recognition


Many alternatives to typed source code have been considered, typically in the context of making programming more accessible.   Most of those appoaches fall in the realm of speech-based programming \cite{desilets2006voicecode, gordon2013improving}, however speech is not always an acceptable solution, especially in quiet environments or with applications that require privacy.
%One approach is to use human speech (via dictation) to create the text of a program. Desilets et al. \cite{desilets2006voicecode} propose VoiceCode, which translates the pronounced syntax into native syntax in the current programming language to support programming by speech. Gordon \cite{gordon2013improving} employs language design and incorporates
%dynamic context for this purpose.   
Given the recent advances in pen-based input, handwriting is a potentially viable alternative.  The pendragon supports people who are unable to use
a keyboard and seeks to find new interaction techniques for input which may improve communication speed \cite{Pendragon}. Mankoff et al.also suggest that word prediction, sentence completion and the syntax of programming languages could be used for handwriting source code \cite{mankoff1998cirrin}. 
Most closely related to our work is a programming IDE integrated with a handwriting area in which the handwritten code is recognized by an enhanced handwriting recognition system \cite{frye2008pdp}.  This work, however, does not present an evaluation of the recognition engine or guidance for how to improve general handwriting recognition engines for application to source code recognition.
%However, they failed to present a clear evaluation for the handwritten source code recognition system.
In this paper, we focus on the recognition step of handwritten source code as the most important step for developing an effective handwriting interface for source code input and editing.

Research in handwriting recognition has a long history dating back to the 1960s~\cite{tappert1990state}. Hidden Markov Model (HMM) based handwriting recognition~\cite{hu1996hmm,kundu1988recognition,nag1986script} is one of the 
most widely used approaches while neural networks are gaining in popularity~\cite{jaeger2001online}. Some approaches also leverage additional constraints for recognizing handwriting in specific domains such as postal addresses \cite{srihari1993recognition, srihari1993interpretation} and banking checks \cite{gorski1999a2ia, agarwal1997bank}. These handwriting recognition systems are developed to take advantage of the English language \cite{van2003using}, which is intrinsically different from source code. For instance, variable names are often created from
concatenated words (e.g. camelCase or underscore naming), which poses a problem for the traditional handwriting
recognition system as it expects spaces to appear between words
contained within its dictionary.   We do not aim to contribute to the extensive literature in handwriting recognition, but rather, we intend to examine how we can leverage this existing work for application to handwritten source code recognition interfaces.  




\section{Data Collection}

Our first contribution in this paper is a data collection study designed to generate a sample set of handwritten source code for research purposes.  The first question to consider is what programming language to study.  We decided to collect handwritten \textit{Python} source code because of the current popularity of \textit{Python}\footnote{\url{http://www.tiobe.com/tiobe-index/}} and its projected growth rate 
%at some high-profile
%organizations, such as Google, Disney and the US
%Government 
\cite{radenski2006python}. 
We chose a ``copying task'', where three code samples are provided for every participant to copy on the tablet using the stylus. While we understand that a ``copying task'' may be cognitively quite different from other writing tasks that require synthesis, we sought to eliminate sources of cognitive load that could impact timing as well as writing quality for the purposes of this data collection task. The three shared code samples allow for comparison across participants. To broaden our dataset of unique handwritten source code samples, we also randomly selected a fourth source code sample function (per participant) that was unique to that participant. In this section, we describe our data collection process and the resulting database of handwritten \textit{Python} samples.


%its current popularity as a programming language and it's growth trajectory.  



%In order to test the recognition system, we collected a dataset of handwritten \textit{Python} code samples written by students from the [anonymized for review] campus. We chose \textit{Python} because of its current popularity as a programming language. 

\subsection{Participants}
We recruited 15 participants (9 females) from the University of Notre Dame for our study. Thirteen of the participants were computer science majors and all participants had at least two semesters of programming experience. Their ages ranged from 19 to 29 (mean = 22.3). Two participants were left-handed. Eight participants had used a pen/stylus for handwriting on a touchscreen device and only one participant had used a tablet for inputting source code (via the virtual keyboard). All participants were compensated \$5 for the study which took approximately 30 minutes each.

\subsection{Apparatus and Software}

\begin{figure}[tb]
 \centering % avoid the use of \begin{center}...\end{center} and use \centering instead (more compact)
 \includegraphics[width=\columnwidth]{datacollection}
 \caption{Screen shot of our data collection web application.  Participants entered their name and code sample number into the boxes in the upper left.  They then entered the code sample using the Apple Pencil and selected `Save' when finished.}
 \label{fig:webpage}
\end{figure}


We used a 12.9 inch iPad Pro with a 2732-by-2048 screen resolution at 264 pixels per inch (PPI) and fingerprint-resistant oleophobic coating. Participants used the Apple Pencil as the stylus device. We implemented a web application with a writing area to record user input. This application was responsible for converting touch points of the stylus into handwriting strokes and saving strokes to a JSON file. Each stroke consists of the coordinates of the sampled points and time-stamp information for each coordinate. The writing area in this application measured 795 * 805 pixels with subtle lines on the background to provide guides for the participants (See \autoref{fig:webpage}). We also implemented functions to undo or redo the previous stroke as well as clear the writing area of all strokes. 

\subsection{Representative Source Code Material}
Our goal was to create a database of representative samples of handwritten \textit{Python} source code for use in evaluating the performance of a handwriting recognition system. Because different \textit{Python} samples contain different language elements, there is no single representative corpus\cite{almusaly2015syntax}. Ideally, representative code samples should contain a variety of language constructs and not be restricted to a single project. 

Our process for choosing source code samples is based on that used by McMillan et al. \cite{mcmillan2012exemplar, rodeghero2014improving}. First, we selected six popular \textit{Python} projects on Github. \autoref{table:project} summarizes the details of these projects. We then extracted all functions from the project source code and eliminated comments in order to focus solely on the source code of the samples.  Next, to obtain functions that were sufficiently long to collect a substantial amount of handwriting, but not so long as to require multiple pages of handwriting, we filtered the functions to those with between 9 and 18 lines of source code and those with no lines greater than 60 characters (to eliminate long, wrapping lines).  We also manually filtered out highly repetitive functions, such as a function that includes only assignment statements for variables. The result was 1324 eligible functions. We randomly selected the three shared test code samples from this set for use by all participants and one additional unique code sample to be entered by each participant in the study. Although \textit{Python} syntax considers whitespace, we decided to ignore indentation for the purposes of focusing purely on handwriting recognition. 
%We are currently in the process of augmenting the collected samples to produce a second identical dataset with indentation inserted.


%We collected this unique code sample for each participant to broaden the coverage of code samples and characterize the common recognition errors from the common recognition system as well.


\subsection{Procedure}

For every participant, we began our data collection with an informed consent process.  Each participant then filled out a pre-study questionnaire about demographics and experience using touchscreen devices and a stylus. Participants were given a practice task to familiarize them with the process.  For each of the four tasks, participants were given a sheet of paper with the sample typed \textit{Python} source code.   Participants entered their name and code sample number into the web application and then entered the code sample using the Apple Pencil and selected `Save' when finished.  After completing all four input tasks, participants were compensated and the session ended.


\subsection{Data Collection Results}
The final dataset includes stroke data for four code samples for each of 15 participants resulting in a total of 60 handwritten source code samples. So, for each of 3 given source code input samples, we have 15 copies of handwritten source code (for a total of 45 handwritten source code samples). The remaining 15 are handwritten samples of unique input source code examples from each participant. 
The handwritten source code data can be downloaded at \newline \url{http://www.purl.org/recognizinghandwrittencode/data}.
 
\begin{table}
  \centering
  \begin{tabular}{l r r r}
    % \toprule
    {Project}
    & {Lines}
      & {Fuctions}
    & {Eligible Functions} \\
    \midrule
    AlphaGo & 1,963 & 151 & 1 \\
    Bittorrent & 7,164 &  570 & 39 \\
    Blender & 265,684  & 12,774 & 1,126 \\
    Instagram & 1,265 & 145 & 8 \\
    Requests & 14,009  & 862 & 84 \\
    Webpy & 10,199 & 1,029 & 66 \\
    % \bottomrule
  \end{tabular}
  \caption{\textit{Python} projects used for selecting code samples}~\label{table:project}
\end{table}



\section{Source Code Recognition Errors}

Current commercial handwriting recognition systems are built to take advantage of the rules of the English language as opposed to that of a programming language, therefore it is not surprising that these systems might perform poorly on source code recognition \cite{frye2008pdp}. There is, however, no previous research that evaluates how well existing state-of-the-art handwriting recognition systems perform on handwritten source code. Here we describe the state-of-the-art handwriting recognition system we employed and characterize the errors based on the dataset we collected.

\subsection{State-of-the-art: Myscript}
Automatic recognition of handwriting is now a mature discipline that has found many commercial uses\cite{plamondon2000online}. MyScript\cite{myscript} is an online handwriting recognition engine that supports more than 80 languages and achieved the best recognition rate in the International Conference on Document Analysis and Recognition competition\cite{el2011line}.
Here we use the MyScript engine as our baseline for comparison and study the typical recognition errors produced when applied to handwritten source code to better understand the complexities introduced by \textit{Python} source code and source code in general.


\subsection{Data Pre-Processing}

In order to use MyScript efficiently and to make a fair comparison between its performance and our algorithm, we apply two simple pre-processing steps to the data. First, we provide MyScript with \textit{Python} specific context through the Subset Knowledge (SK) facility and a custom lexicon. SK is a MyScript feature for telling the recognizer that we only want it to enable recognition of certain characters. For example, for a phone number field, we may want only digits to be recognized.   We created an SK resource in MyScript to allow only characters that can legally appear in \textit{Python} source code.  We also provide the legal \textit{Python} keywords through a user-defined lexicon.

%We used two main MyScript resources to process the data we collected: 
%The MyScript Cloud Development Kit (CDK) and Subset knowledge(SK).
The MyScript Cloud Development Kit (CDK) is an HTTP-based set of services that take handwritten strokes as input and produce potential recognition results as output. To use the CDK in experiments, we must send strokes to the recognizer at some level of granularity (e.g. single character, whole word, whole line, etc).
%The data processing step is to process the data for submission to the MyScript Cloud Development Kit (CDK). Pre-processing includes character filtration and sentence separation. 
%Character filtration is to restrict the recognition result to certain characters. We used the Subset knowledge(SK) feature by creating an SK resource in MyScript to allow only characters that can legally appear in \textit{Python} source code. In sentence separation, 
We chose to break the stroke data into lines assuming developers might write a statement at a time on a single line. To do so, we analyze stroke coordinates and create a new line each time the user moves to a new vertical position. Each line is then sent one at a time to the MyScript CDK.  This simulates a developer writing one programming statement (one line) at a time, pausing at the end of each line.  Alternative pre-processing is possible given the raw stroke data and timestamp information (e.g. sending incomplete lines when a participant pauses).

\subsection{Characterizing errors}
\label{sec:characterizing}
We processed all of the handwritten data as described above to collect baseline recognition results for all handwritten samples in our dataset.  We then set out to understand the types of recognition errors that were present in the final recognized text. We identified three major types of recognition errors: word errors, symbol errors, and space errors. 

Word errors occur when MyScript simply incorrectly recognizes a written word. This is typically due to poor writing and can occur for keywords as well as non-keywords.
For example, when the handwritten word `self' is recognized as `silt', we characterize this as a word error. 
Symbol errors represent incorrect recognition of symbols or non alpha-numeric characters.
For example, an `\_' (underscore) is often recognized as a `-' (dash). 
Finally, a space error results when the system inserts an unexpected space.
For example, when `ConflictError' is recognized as `Conflict Error', we characterize it as a space error. 

Most of the word errors and symbol errors can be attributed to poor writing or cursive writing (characters are written joined together in a flowing manner) which is inherently more difficult for MyScript to recognize than block writing (characters are written separately). Space errors, on the other hand, appear to depend on the language model of the recognizer, which most likely does not include training on CamelCase\footnote{\url{https://en.wikipedia.org/wiki/Camel\_case}} or proper English words separated by dot notation (e.g. student.name).  The result is that MyScript inserts space at these word and dot notation separators.

In summary, from the statistical results for each type of error presented in \autoref{fig:errors}, space errors, mainly caused by the internal mechanism of English handwriting recognition system, represent the most prevalent recognition error. In addition, poor writing and the tendency to return an English word for a non-English word in the source code lead to word errors, which also represents a significant portion of all errors. Symbol errors are also a prevalent error type. This makes sense given that MyScript is designed to recognize general words, however, symbols, dot notation, and combinations of symbols and words are typically not present in general text, especially in the way that they are used in source code. For example, the most problematic symbols includes underscore `\_', parentheses `( )' and equal `='.


\begin{figure}[t!]
\centering
\includegraphics[width=3.16in, height = 2.2in]{errors}
\caption{Average error numbers of all participants for each code sample from MyScript general handwriting recognition engine}
\label{fig:errors}
\end{figure}


\section{Handwritten Source Code Recognition Pipeline}



A programming language is governed by grammar rules, which stipulate the positions of keywords and symbols. For example, in \textit{Python}, a \textit{def} sentence must end with a `:'. However, handwritten symbols are often problematic.  For example, colons `:' are sometimes recognized as semicolons `;'. In addition to grammar rules, programming languages are highly repetitive with predictable properties\cite{hindle2012naturalness}. 
Function names and variable names are the most common repetitive words in a single source code project. If a function name appears more than once in the same handwritten code sample, however, it is impossible for users to hand write the \textit{exact} same strokes for this function name, which makes different recognition results of the same handwritten function name a possibility that we must account for.

In this section, we present an approach to improve the recognition rate for handwritten source code by addressing these issues as well as those common errors characterized in Section \ref{sec:characterizing}. We leverage what we know about the predictability and structure of source code to improve recognition results beyond that of the state-of-the-art recognizer. 


The general premise of our approach is that state-of-the-art engines can produce excellent results given good writing and the absence of symbols and programming practices like camelCase.  Our framework, illustrated in \autoref{fig:overview}, is therefore aimed at analyzing and post-processing the recognition results produced from MyScript to utilize its recognition capabilities but correct for those common errors. This framework can be divided into four parts: statement classification, statement parsing, token processing, and statement concatenation. 
%The various parts are presented in the following sub-sections.
The source code for this post-processing algorithm can be found at \url{http://www.purl.org/recognizinghandwrittencode/code}.

\begin{figure}[h!]
\centering
\includegraphics[width=0.5\textwidth]{overview}
\caption{Framework for augmenting MyScript to correct for common recognition errors in handwritten source code.}
\label{fig:overview}
\end{figure}

\begin{figure*}[h]
 \center
  \includegraphics[width=2.2in, height = 1.32in]{picture1}
  \includegraphics[width=2.2in, height = 1.32in]{picture2}
  \includegraphics[width=2.2in, height = 1.32in]{picture3}
  \caption{Average recognition error rate of MyScript and our augmented MyScript system for three test code samples}
  \label{result}
\end{figure*}



\subsection{Statement Classification}
As we mentioned before, we process the handwritten source code data considering each statement as a unit. According to the \textit{Python} grammar specification, we can restrict \textit{Python} source code statements into a limited number of classes, each of which has specified structure rules\footnote{\url{https://docs.python.org/2/reference/grammar.html}}. Here we use the first token in the statement as the symbol for classification. For example, a `def' statement starts with `def' and its structure is defined as `def' + `function name' + `(parameters0, parameters1 ...):'. We define 14 classes for \textit{Python} code statements, including an `assignment' statement, which means the first word in this statement is not a keyword but rather a variable name. In \autoref{fig:overview}, the recognition result is classified as an `if' statement. \autoref{table:statement} presents statistics for the various statement classes in the three code samples.



\begin{table}
  \centering
  \begin{tabular}{l r r r}
    % \toprule
    {Class}
    & {Frequency}
      & {Class}
    & {Frequency} \\
    \midrule
    def & 3 & except & 1  \\
    if & 7 &  while & 1 \\
    for & 3  & try & 1 \\
    raise & 2 & break & 1 \\
    return & 2  & else & 1 \\
    yield & 2 & assignment & 13 \\
    % \bottomrule
  \end{tabular}
  \caption{Frequency for each statement class in three test code samples}~\label{table:statement}
\end{table}



\subsection{Statement Parsing}
After classifying the statement, we need to break it down into independent parts according to the grammar rules.
Similar to a recursive-descent parser \cite{van1993recursive}, our system consists of a series of functions, each of which is responsible for one class of statement. Each function includes a set of mutually recursive procedures where each such procedure implements one of the productions of the grammar as a regular expression. We implement a top-down LL parser to parse the input from left to right and perform a leftmost derivation \cite{fernau1998regulated} of the statement. As a result, a statement is parsed into a list of single tokens and/or characters. For example, the statement in \autoref{fig:overview} is parsed into five individual tokens. Specifically, `if' is a keyword token; `Cookie. name' is a variable token; `==' is a symbol token; `naue' is a variable token; `;' is the last symbol token.



\subsection{Token Processing}
The previous stage results in a list of single tokens and/or characters that make up the statement.
%In this step, we process the list of the single words or characters received from the last step. 
We assume all non-keywords are properly recognized and add them to the lexicon assuming they are \emph{variable} names.
%We build a non-keyword lexicon to save all non-keywords in the first sentence. 
Then for all non-keywords in each statement that follows, we first compare the token to all the words in the non-keyword lexicon. If a `similar' token already exists in the lexicon, we replace it with the `similar' token in the lexicon. For example, in \autoref{fig:overview}, `naue' is very similar to `name', which is already in the lexicon, so we just replace the token `naue' with `name'. If there is no `similar' token in the lexicon, we accept this token as it is and add it to the lexicon. We calculate similarity using the Levenshtein distance \cite{levenshtein1966binary} with a threshold of 0.7, determined empirically.

\subsection{Statement concatenation}
After processing all tokens, we remove all extra spaces in any single token, then concatenate each token with a single space between them to reconstruct the final statement.
Additionally, we ensure that the last recognized character of a statement is a ':'.
For example, in \autoref{fig:overview}, we first remove the space in `cookie. name' and then replace the last character `;' with `:'.





\section{Evaluation}


%define WER and CER
To assess the performance of our system, we measure the Character Error Rate (CER) and Word Error Rate (WER). WER and CER are percentages obtained from the Levenshtein distance between the recognized sequence and the corresponding ground truth. They are calculated as
\[ \frac{D+I+S}{L} \times 100\% \]
where D is the number of deleted units, I is the number of inserted
units, S is the number of substituted units, and L is the total number of
units in the ground truth transcriptions. A unit is a word for WER or a
character for CER.


We evaluate our recognition approach by applying our framework to the 45 code samples in our database. In the following section, we compare the results of our enhanced recognizer to the results of using MyScript alone.

\section{Results} \label{results}

\begin{table}
  \centering
  \begin{tabular}{l r r}
    % \toprule
    {}
    & {t-test score ($t_{14}$)}
      & {P-value} \\
    \midrule
    WER on sample 1 & -9.02 & $P < 0.00001$ \\
    WER on sample 2 & -8.29 & $P < 0.00001$ \\
    WER on sample 3 & -6.57 &  $P < 0.00001$  \\
    CER on sample 1 & -3.88  & $P < 0.001$  \\
    CER on sample 2 & -5.45 & $P < 0.0001$ \\
    CER on sample 3 & -6.13 & $P < 0.0001$ \\
    % \bottomrule
  \end{tabular}
  \caption{Statistical evidence (T-test and P-value) for WER and CER on three code samples}~\label{table:significance}
\end{table}


%result
As shown in \autoref{result}, 
our augmented recognition approach results in an 8.6\% word error rate and 3.6\% character error rate, on average, over the three code samples, which outperforms the original MyScript recognizer with 31.31\%  and 9.24\% in word and character error rate respectively. We also find statistical evidence for an effect of our augmented recognition approach on both WER and CER (See \autoref{table:significance}). 

%on WER on sample \#1 ($t_{14} = -9.02, P < 0.00001$), on CER on sample #1 ($t_{14} = -3.88, P < 0.001$), WER on sample 2 ($t_{14} = -8.29, P < 0.00001$), CER on sample 2 ($t_{14} = -5.45, P < 0.0001$), WER on sample 3 ($t_{14} = -6.57, P < 0.00001$), and CER on sample3 ($t_{14} = -6.13, P < 0.0001$).


%compare to English handwriting recognition
%%Hi Ron, these four systems has lowest WER and CER, the 97% and 95% I said before is not English recognition, it's for arabic. 
\begin{table}
  \centering
  \begin{tabular}{l r r}
    % \toprule
    {System}
    & {WER(\%)}
      & {CER(\%)} \\
    \midrule
    \textcolor{red}{Augmented MyScript}  & \textcolor{red}{8.6} & \textcolor{red}{3.6} \\
    Kozielski et al. \cite{doetsch2013improvements} & 9.5 & 2.7 \\
    Keysers et al. \cite{keysers2016multi} & 10.4 &  4.3  \\
    Zamora et al. \cite{zamora2014neural} & 16.1  & 7.6  \\
    Poznanski et al. \cite{poznanski2016cnn} & 6.45 & 3.44 \\
    % \bottomrule
  \end{tabular}
  \caption{Performance of our system compared to handwritten English recognition systems on the IAM dataset}~\label{table:iam}
\end{table}

%To examine how our source code recognition rates compare
%to acceptable recognition rates in general handwriting recognizers,
%we present the recognition rate comparison between our augmented
%MyScript recognition system (on source code) to four other state-ofthe-art
%general handwritten English recognition systems

Since there is no existing handwriting source code recognizer for comparison, we compare the recognition rate of our our augmented MyScript recognition system (on source code) to that of four state-of-the-art general handwritten English recognition systems (on general text). 
%Since there is no existing handwriting source code recognizer for comparison, we compare our recognition rates on source code to acceptable recognition rates of general handwriting recognizers on general text.  Specifically, we compare the recognition rate of our our augmented MyScript recognition system (on source code) to that of four state-of-the-art general handwritten English recognition systems on general text. 
The IAM handwriting database \cite{marti2002iam} consists of 9,285 lines of general handwritten text
written by approximately 400 writers with no restrictions on style or writing tool. This database has been widely used to evaluate English handwriting recognition systems. The four systems in \autoref{table:iam} were tested based on this IAM handwriting database. \autoref{table:significance} shows that the WER and CER of our augmented source code recognition system are comparable with other state-of-the-art handwritten English recognition systems on general handwritten text.

%Finally, we acknowledge the importance of indentation in \textit{Python} source code.  We chose to ignore indentation for the purposes of this project to focus solely on recognition and would argue that recognition results will not be affected by indentation considerations.  We are in the process of introducing indentation into the collected data to create an identical set of source code samples with appropriate indentation.

%Because typing on a virtual keyboard is the standard input method on touchscreen devices, it's useful to examine how virtual keyboard typing error rates compare to those of handwritten source code recognition.
% %Regular typing is also compared with handwriting source code on a virtual keyboard. 
% For example, Almusaly et al. report a 7.81\% total error rate (TER) for typing java programs on a standard virtual keyboard as measured from 32 participants \cite{almusaly2015syntax}. TER, similar to CER, is a measure of the total number of errors (i.e., omissions, substitutions, and insertions) and corrections that are made in the resulting typed text. 


%explain how our system fix errors in characterization and existing errors

\section{Discussion}


%The improvement of recognition rate embodied in fixing all the three error types. 


Our approach achieved an 8.6\% word error rate and a 3.6\% character error rate on the collected dataset by taking the language grammar rules into account. Overall, improvement of our recognition pipeline over the baseline MyScript recognition engine can be attributed to addressing the three main error types identified in Section \ref{sec:characterizing}.  After statement concatenation, all unnecessary space errors in a single token are removed. Ensuring the last character of a statement eliminates 32\% of the symbol errors. Token processing fixes around 78\% of the word errors. 

Recognition results, however, are still not 100\% accurate. Initial inspection indicates that this is mainly due to the illegible or cursive handwriting of the participants and the incorrect recognition of symbols. Also, since one of our lexicons is dependent on the non-keywords already recognized in the code, incorrectly recognized words will also be added to the lexicon, thereby corrupting the lexicon and preventing it from enhancing the recognition of the following words. Additionally, it is difficult to identify incorrectly recognized symbols; for example, if `(' appearing in the middle of the text is recognized as `l', it becomes impossible to rectify it using our approach. 
%errors still existing, develop widget for errors, what will improve
%Our approach achieved an 8.6\% word error rate and a 3.6\% character error rate on the collected dataset by taking the language grammar rules into account. 
%It should be noted that there are still identified errors remaining to be corrected. 
Errors like unmatched `(' and `)' in a statement can be detected, but not reliably corrected. For example, `(name' can be recognized as `cname', but we have no evidence to correct `cname' to `(name'. Two methods can be employed to resolve remaining errors such as this. The first is to develop a widget in the handwriting interface to highlight all errors that are identified but can't be corrected and let users correct them manually. Another option is to train a language model to identify words that do not exist \cite{zamora2014neural}. 


Because typing on a virtual keyboard is the standard input method on touchscreen devices, it is useful to examine how virtual keyboard typing error rates compare to those of handwritten source code recognition.
% %Regular typing is also compared with handwriting source code on a virtual keyboard. 
Almusaly et al. report a 7.81\% total error rate (TER) for typing \textit{Java} programs on a standard virtual keyboard as measured from 32 participants \cite{almusaly2015syntax}. TER, similar to CER, is a measure of the total number of errors (i.e., omissions, substitutions, and insertions) and corrections that are made in the resulting typed text.  Our handwriting results are comparable.

%generalize to other languages
This approach can also be generalized to other programming languages with strict grammar rules. For instance, one can define statement classes for \textit{Java} according to the first word in the statement and then replace the regular expressions with productions of \textit{Java} grammar rules.  Algorithms for searching and replacing similar words can be kept unchanged. Other heuristic steps like concatenating tokens are also trivial to implement for new languages. 
%Due to the uncertainty of the program structure, however, our approach cannot easily be generalized to non-strict programming languages.

%\section{Discussion and Implication for HCI}
\section{Conclusion and Future Work}
%%mobile programming is widely used, and its limitation.
%Supporting programming on mobile devices is not a novel idea. With the rapid technology shift in current computing devices, high-quality low-cost mobile devices such as tablets and smartphones are being increasingly used in everyday activities. Many tasks that previously required a PC are now feasible on mobile devices. For example, tablets are typically equipped with powerful batteries, advanced graphic processors, high-resolution screens and fast processors, making writing and compiling code on them completely plausible. TouchDevelop \cite{tillmann2011touchdevelop}, for example, is a novel programming environment, language and code editor for mobile devices and Tilman et al. \cite{tillmann2012future} predict that programming on mobile devices will be widely used for teaching programming.  However, mobile devices are also inherently restricted by their limintations such as small screens and the clumsy virtual keyboard that is difficult to use and that takes up valuable screen space - posing challenges for a programmer who wishes to enter or edit source code.
%For this to happen, however, input must be made more intuitive.
%Alternatives to typing have been considered for a long time. One is to use speech (via dictation) to create the text of a program. Desilets et al. \cite{desilets2006voicecode} propose VoiceCode, which translates the pronounced syntax into native syntax in the current programming language to support programming by speech. Gordon \cite{gordon2013improving} employs language design and incorporates dynamic context for this purpose. Speech, however, is not always an appropriate option given social conventions and privacy issues. Given the advances of pen-based input technology, we chose to explore handwriting input in this paper.  

 
%conclusion
The keyboard is not an ideal input mechanism for every person and situation.
Alternatives to typing, such as speech, have been considered in the past \cite{desilets2006voicecode, gordon2013improving}. However, speech is not always an appropriate option given social conventions and privacy issues.
Given advances in pen-based technology that provides an opportunity for users to engage with devices in a potentially more `natural' way than that supported by a virtual keyboard, handwriting input is a viable alternative to virtual keyboard input. In this paper, we have explored handwriting recognition specifically for source code with the ultimate goal of supporting handwriting as a means for programming.
%Handwriting is a viable alternative to a keyboard given recent technological advances in pen-based technology.
%for programming can be used for people with disabilities and people suffering RSI. %In this paper, we focus on supporting handwriting recognition for the particular domain of source code text input. 
We collect and present a small database of publicly available handwritten source code samples and we propose an approach to recognize handwritten source code by leveraging a commercial handwriting recognition system. Experiments on the data collected from 15 participants show our framework has an average 8.6\% word error rate and 3.6\% character error rate which outperforms the baseline recognition system and produces rates comparable to the recognition of general handwritten English text.  We are encouraged by these initial results but believe there are several avenues of future work.


%Given the advances of pen-based input technology, we chose to explore handwriting input in this paper. %%writing is new viable, stylus technique
%Input via stylus is becoming more precise and the familiar writing action makes handwriting a viable alternative to typing code on mobile devices. 
%Given advances in pen-based technology that provides an opportunity for users to engage with devices in a potentially more `natural' way than that supported by a virtual keyboard, we have explored handwriting recognition for source code with the ultimate goal of supporting handwriting as a means for programming.
%Additionally, research suggests that handwriting can lead to cognitive, memory, and creativity enhancements \cite{alonso2015metacognition}. Alternative input mechanisms are also important for people with RSI or other motor impairments who find typing difficult, and in general, for those who may wish to carry out simple source code editing and entry tasks in mobile situations. 
%While handwriting without recognition produces `digital ink' that is appropriate for applications like annotation and graphic design, 
%For handwriting to be used in scenarios like programming, however, applications must be equipped with recognition technology to support translation to searchable and editable digital text. To effectively incorporate the handwriting experience for source code entry and editing, we must first address the source-code recognition problem.




%importance


  

% Summarize what we've done
%We have presented an initial study to collect data on handwritten source code and explored the use of a state-of-the-art recognition system for recognizing handwritten source code. 
%In this section, we talk about why it is important to HCI and how HCI community could benefit from our work.


%%writing is new viable, stylus technique
%Input via stylus is becoming more precise and the writing action is very similar to writing on paper. Handwriting is therefore a viable alternative to typing code on mobile devices. Moreover, handwriting is an acceptable input method for people with RSI or other motor impairments who find typing difficult. 
%Thus studying handwriting input method will be a benefit for a large group of users. 
%While handwriting without recognition produces `digital ink' that is appropriate for applications like annotation and graphic design, for handwriting to be used in scenarios like programming, it must be equipped with recognition technology to support translation to searchable and editable digital text.

%%implication and importance for HCI
From the view-point of human-computer interaction, usability and user satisfaction is critical. For handwriting text input, users expect recognition technology with a low error rate and responsive recognition speed. LaLomia et al. \cite{lalomia1994user} reported that users are willing to accept a recognition error rate of only 3\% (a 97\% recognition rate), although Frankish et al. \cite{frankish1995recognition} concluded that users will accept higher error rates depending on the text-editing task. It would not be surprising, therefore, if higher error rates were acceptable for source code entry and editing which is inherently difficult due primarily to the use of symbols. Input speed is another concern with respect to handwriting. Modest touch typing speeds on a virtual keyboard in the range of 20 to 40 words per minute (wpm) are achievable.
Handwriting speeds are commonly in the 15 to 25 wpm range \cite{card1983psychology,devoe1967alternatives,dunlop2009pickup}. We suspect that this decrease in speed, however, will be acceptable to the particular groups for whom handwriting is the most viable input option. Additionally, in professional programming, most of the code that developers
write involves reuse of existing example code and libraries \cite{bellon2007comparison}. This `reuse' typically amounts to editing existing code to suit a
new context or problem and generally provides benefits to developers in terms of time and error reduction \cite{ko2011state}.
%is able to save time and
%avoid the risk of writing erroneous new code \cite{ko2011state}.
For these reasons, we envision our system as being particularly useful in the code editing domain as opposed to writing extensive source code from scratch.  Studying how the algorithms perform in editing tasks is left as future work.

%a recognizer used primarily for code edits as opposed to being used to write an entire program from scratch.


% How does the HCI community benefit from this work
%% Dataset for testing

While databases exist for research in general handwritten text recognition \cite{marti2002iam, grosicki1rimes}, there is no such dataset for handwritten source code.  This paper represents the first such contribution of a handwritten source code dataset consisting of 555 lines of  \textit{Python} code written by 15 participants. While we recognize that using the same three code samples for all users and employing a ``copy task'' may lessen the generality of the dataset, we sought to eliminate all effects of cognitive complexity (e.g. actually solving programming problems) to focus solely on the handwritten source code quality.  Collecting data for other programming languages and for actual programming tasks is left as future work.

%A standard database is needed to facilitate research in handwritten source code recognition. For general handwritten text recognition, the IAM database \cite{marti2002iam} and the RIMES database \cite{grosicki1rimes} are widely used for research purposes. Based on Lancaster-Oslo/Bergen (LOB) corpus, the IAM database \cite{marti2002iam} consists of 9,285 lines of handwriting text from 400 writers. The RIMES Database comes from the ICDAR 2011 block-recognition competition and consists of 1,500 paragraphs of the handwritten French text.
%Unfortunately, there is no such dataset for handwritten source code - this paper represents the first such contribution.  We collected only a small handwritten source code dataset consisting of 555 lines written by 15 participants. While we recognize that using the same three code samples for all users and employing a ``copy task'' may lessen the generality of the dataset, we sought to eliminate all effects of cognitive complexity (e.g. actually solving programming problems) to focus solely on the handwritten source code quality.  Collecting data from actual programming tasks and for additional programming languages is left as future work.
%and affect the writing organization.   but cognitive load and possible errors are avoided. In addition, t
%It is expected that the database would be particularly useful for further handwritten source code recognition research using \textit{Python} as the language of choice.  More data on additional languages will be necessary to further investigate handwriting as a viable input mechanism for source code.



%% Approach based on state-of-the-art recognizer
%Our handwritten source code recognition framework is implemented by leveraging the programming language grammar information to augment an existing handwriting recognition system. By replacing the grammar rules in the framework, the system can be generalized to other programming languages. It should be noted, however, that using an existing handwriting recognition system designed for natural language is not tackling the problem at its source. Rebuilding the core of a recognition system based on properties of source code is thus an alternative approach that should be explored.  We leave this for future work.






%conclusion
%The keyboard is not an ideal input mechanism for everyone.
%Handwriting as an alternative to a keyboard for programming can be used for people with disabilities and people suffering RSI. In this paper, we focus on supporting the recognition aspect of handwriting for source code text input. We collect and present a small database of publicly available handwritten source code samples and we propose an approach to recognize handwritten source code by leveraging a commercial handwriting recognition system. Experiments on the data collected from 15 participants shows our framework has an average 8.6\% word error rate and 3.6\% character error rate which outperforms the baseline recognition system and produces rates comparable to recognition of general handwritten English text.

%future work

%%from scratch
%%IDE
%Clearly, the current work is limited in both scope and depth and 
%We are encouraged by these initial results but believe there are several avenue of future work. 

The next most obvious area of future work is to develop a handwritten source code recognition system from scratch instead of augmenting the results produced by an existing system.  We suspect this approach would lead to comparable and most likely improved recognition rates. Building a universal handwritten source code reading system could employ deep learning techniques such as Concurrent Neural Networks \cite{poznanski2016cnn} or neural network language models \cite{zamora2014neural} trained purely on the source code. 

Additionally, there are several opportunities to explore the integration of handwriting recognition into source code IDEs \cite{frye2008pdp}.  For example, how do we now integrate source code completion into a handwriting-based interaction?   Can we integrate elements such as syntax insertion and highlighting?  Exploring the affordances of handwriting in the context of an IDE is an exciting area of future work that is enabled by these initial findings.

%Similar to an IDE for handwriting C\# code \cite{frye2008pdp}, integrating handwriting source code to current programming IDE or building a programming IDE solely with handwriting as an input method is also valuable to explore.

%%speed, auto completion
%Furthermore, to address concerns pertain to handwriting speed, it may be possible to add auto-completion feature into handwriting source code interface. Next word or character suggestion is also helpful to facilitate inputting. In addition, combing voice input and handwriting input may also improve the inputting speed.

Multimodal methods present another area of future work.  Perhaps the combination of handwriting and speech input or handwriting and occasional keyboard input \cite{mueller2014pen} begin to produce interaction experiences that rival those of typed source code input.  

Finally, we will never reach a perfect recognition rate for handwritten text (general or source code).  How do we effectively support efficient editing of the recognized text so that users can quickly correct mistakes? Natural and effective text entry and editing is an interesting topic for future studies.

%%texue of future work.t editing
%, our research projeng avenct aims to develop a programming interface to support handwriting, editing and recognizing source code. We also intend to explore the potential of adding editing techniques such as selecting, deleting, copying, and pasting.






%% if specified like this the section will be committed in review mode
\acknowledgments{
The authors wish to thank all the study participants as well as Poorna Talkad Sukumar, Jason Liu, and Suwen Lin for their valuable discussions and input.}

%\bibliographystyle{abbrv}
\bibliographystyle{abbrv-doi}
%\bibliographystyle{abbrv-doi-narrow}
%\bibliographystyle{abbrv-doi-hyperref}
%\bibliographystyle{abbrv-doi-hyperref-narrow}

\bibliography{template}
\end{document}
