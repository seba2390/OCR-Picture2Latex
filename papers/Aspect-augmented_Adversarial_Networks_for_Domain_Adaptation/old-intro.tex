One common deep learning approach for domain adaptation is to simply train a single autoencoder on texts from both the source and the target domains. A label predictor (e.g. SVM) is then trained on hidden representations induced by the autoencoder in a separate step \cite{glorot2011domain,chen2012marginalized,chopra2013dlid}. We argue that there are two main disadvantages of this autoencoder approach. First, this approach still provides no effective means to encourage the emergence of domain-invariant features.  Second, many classification tasks require attention to a particular aspect in texts while prior methods only predict the category based on an \emph{overall} representation of the input. Figure \ref{fig:pathology} illustrates a breast pathology report along with diagnosis results for two aspects related to types of breast disease. If the model aims to predict the label for lymphatic vessel invasion (LVI), it should learn to extract features only from the second sentence (in blue). Moreover, an autoencoder only learns task-independent feature representations which may not be optimal for the targeting classification task.
%Indeed, distilling domain-invariant properties have been shown to be the key to improving domain adaptation performance in the context of shallow learning \cite{}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

The accuracy of deep learning methods depends on the access to large amounts of in-domain training data. While for many applications, such data is not available, it is often possible to find related data. In this paper, we propose a transfer method for neural models that can utilize annotations developed for related domains and related tasks. 
% Yuan: saying "in the same domain" will cause confusion later
While cross-domain adoptation has been commonly considered in the literature, we are particularly interested in applying this method to transfer across aspects within the same domain. For example, when learning to classify pathology reports (shown in Figure \ref{fig:pathology}) for the presence of lymph invasion, we would like to utilize training data available for identifying carcinoma in the same report. %Figure \ref{fig:pathology} illustrates a breast %pathology report along with diagnosis results for two aspects %related to types of breast disease. If the model aims to %predict the label for lymphatic vessel invasion (LVI), it %should learn to extract features only from the second sentence %(in blue).

Existing domain adaptation approaches cannot be effectively applied for this task. For instance, one commonly used approach employs autoencoders to learn cross-domain representation so that a classifier trained on this representation can handle both source and target domains~\cite{glorot2011domain,chen2012marginalized,chopra2013dlid}. However, in the case of aspect transfer, both aspects have to predicted from the same document, so both classifiers operate over the same representation from the very beginning. Moreover, since auotoencoders learn the representation in the task agnostic scenario, it is unlikely to be useful in this scenario. 

\begin{figure}[t]
\centering
\includegraphics[width=0.5\textwidth]{figures/pathology.pdf}
\caption{A snippet of a breast pathology report with diagnosis results for two types of disease. Evidence for both results is in red and blue, respectively. }\label{fig:pathology}
\end{figure}

In this paper, we propose a novel approach that aims to find a balance between learning an invariant representation that supports generalization across tasks and domains, while at the same time is tailored to a specific target task. We implement our model as an aspect-augmented adversarial network, building on successful use of these networks in computer vision~\cite{ganin2014unsupervised}. Our model is composed of aspect-specific label predictors, a document encoder that extracts hidden features, and a domain classifier trained to predict the domain of the input document based on learned representation. The performance of this classifier naturally provides an effective measurement on cross-aspect dissimilarities. In other words, the domain classifier will perform poorly if features are aspect-invariant. We therefore jointly optimize the encoder as an adversary to promote features that fail the classifier, while also
aiming to minimize the loss of the label predictor. 

We are interested to employ this method for scenarios where the annotated data is only available for the source aspect. As an alternative source of weak supervision, we assume a small set of keywords associated with a target aspect, such as terms representative of a specific hormonal marker in a pathology report. Such annotations can be easily provided by domain experts, or extracted from medical literature such as codex rules in pathology. Our model utilizes this keywords, by augmenting the document encoder with a sentence-level aspect-relevance scorer similar to the attention mechanism. These scores allow the model to selectively consider information from relevant fragments while ignore the rest.

Another departure of our method relates to the use of reconstruction loss to stabilize adversarial 
training. Our initial experiments with the adversarial training revealed that the encoder tends to induce sparse and low-variance representations regardless of inputs, which in turn degrades prediction performance. To address this issue, we revisit the idea of autoencoders and introduce a word-level reconstruction loss that is jointly optimized with other components of the model. We empirically demonstrate that this new objective yields richer and more diversified feature representation, as reflected in improved adaptation performance.

We evaluate out model on a pathology reports dataset and a review dataset. On the pathology dataset, we explore the task of cross-aspect adaptation across different types of breast disease. Specifically, we test on six adaptation tasks, and empirical results show that our method consistently outperforms all other baselines. On average, our full model achieves 22.5\% absolute improvement over the counterpart without adversarial training. Moreover, our unsupervised adaptation result is only 2.6\% behind the accuracy of a supervised model. On the review dataset, we test on adaptation from the hotel domain to the restaurant domain. Our model achieves the best performance on four out of five adaptation tasks, and outperforms the no-adversarial-training baseline by 3.2\%. Finally, we analyze the behavior of adversarial training and assess the impact of individual components to the overall model performance.