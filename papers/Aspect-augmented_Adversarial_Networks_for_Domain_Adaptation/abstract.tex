%In this paper we introduce a neural method for aspect transfer between two classification tasks operating over the same domain. We do not assume access to target labels. Instead, as a source of weak supervision we utilize a small number of keywords pertaining to both source and target aspects, which indicate sentence relevance for a particular aspect. Using relevance information, we construct differential aspect-driven encodings of the same document. We employ a shared end classifier between the tasks, which is trained on the source class labels and ultimately applied  to the target encodings. We further jointly optimize this process with an adversarial objective to distill invariant representation that facilitates generalization. 

We introduce a neural method for transfer learning between two (source and target) classification tasks or aspects over the same domain. Rather than training on target labels, we use a few keywords pertaining to source and target aspects indicating sentence relevance instead of document class labels. Documents are encoded by learning to embed and softly select relevant sentences in an aspect-dependent manner. A shared classifier is trained on the source encoded documents and labels, and applied to target encoded documents. We ensure transfer through aspect-adversarial training so that encoded documents are, as sets, aspect-invariant.
Experimental results demonstrate that our approach outperforms different baselines and model variants on two datasets, yielding an improvement of 27\% on a pathology dataset and 5\% on a review dataset.\footnote{The code is available at \url{https://github.com/yuanzh/aspect_adversarial}.}
%Experiments on a pathology report dataset demonstrate that both adversarial training and aspect-driven encoding play a crucial role in adaptation performance, yielding a maximum of 24\% improvement over the counterpart baseline.
%We also successfully illustrate the effectiveness of our method on a traditional cross-domain sentiment classification task, outperforming the marginalized autoencoder~\cite{chen2012marginalized} baseline by 5\%.\footnote{The code is available at XXX.}
