\section{Introduction}

The accuracy of deep learning methods depends on the access to large amounts of in-domain training data. While for many applications, such data is not available, it is often possible to find labeled data in another related domain. In this paper, we propose a transfer method for neural models that can utilize annotations developed for related domains and related tasks to improve performance in the target domain. 
% Yuan: saying "in the same domain" will cause confusion later, perhaps "in the same (set of) document(s)"
While cross-domain adaptation has been commonly considered in the literature, we are particularly interested in applying this method to transfer across aspects within the same domain. For example, when learning to classify pathology reports (shown in Figure \ref{fig:pathology}) for the presence of lymph invasion, we would like to utilize training data available for identifying carcinoma in the same report. %Figure \ref{fig:pathology} illustrates a breast %pathology report along with diagnosis results for two aspects %related to types of breast disease. If the model aims to %predict the label for lymphatic vessel invasion (LVI), it %should learn to extract features only from the second sentence %(in blue).

Existing domain adaptation approaches cannot be effectively applied for this task. For instance, one commonly used approach employs autoencoders to learn cross-domain representation so that a classifier trained on this representation can handle both source and target domains~\cite{glorot2011domain,chen2012marginalized,chopra2013dlid}. These methods learn representation without the awareness of different aspects in a document. In the case of aspect transfer, however, both aspects have to be predicted from the same document. As a result, both classifiers operate over the same representation from the very beginning, and will always predict the same label for both aspects. Moreover, since auotoencoders learn the representation in the task agnostic scenario, it is unlikely to be useful in this scenario. 

\begin{figure}[t]
\centering
\includegraphics[width=0.5\textwidth]{figures/pathology.pdf}
\caption{A snippet of a breast pathology report with diagnosis results for two types of disease. Evidence for both results is in red and blue, respectively. }\label{fig:pathology}
\end{figure}

In this paper, we propose a novel approach that aims to find a balance between (1) learning an invariant representation that supports generalization across tasks and domains (2) tailoring the representation to a specific target task at the same time. We implement our model as an aspect-augmented adversarial network, building on successful use of these networks in computer vision~\cite{ganin2014unsupervised}. Our model is composed of an aspect-specific label predictor, a document encoder that extracts hidden features, and a domain classifier trained to predict the domain of the input document based on learned representation. The performance of this classifier naturally provides an effective measurement on cross-aspect dissimilarities. In other words, the domain classifier will perform poorly if features are aspect-invariant. We therefore jointly optimize the encoder as an adversary to promote features that fail the classifier, while also aiming to minimize the loss of the label predictor. 

% Yuan: the original paragraph is confusing because the keywords sound like the supervision for the label prediction rather than for the relevance scorer. I put the old version in old-intro.tex

%We are interested in employing this method for scenarios where label annotations are only available for the source aspect. However, to enable aspect transfer, we do require extra supervision to distinguish different aspects in the same document. As an alternative source of weak supervision, we assume a small set of keywords associated with each aspect, such as terms representative of a specific hormonal marker in a pathology report. Such annotations can be easily provided by domain experts, or extracted from medical literature such as codex rules in pathology. Our model utilizes these keywords by augmenting the document encoder with a sentence-level aspect-relevance scorer similar to the attention mechanism. These scores allow the model to selectively consider information from relevant fragments while ignoring the rest.

To enable aspect-specific transfer and classification, we augment the document encoder with a sentence-level aspect-relevance scorer similar to the attention mechanism. These scores allow the model to selectively consider information from relevant fragments while ignoring the rest. While we are interested in scenarios where annotations are only available for the source aspect, we do require supervision for training the relevance scorer to distinguish different aspects. As an alternative source of weak supervision, we assume a small set of keywords associated with each aspect, such as terms representative of a specific hormonal marker in a pathology report. Such annotations can be easily provided by domain experts, or extracted from medical literature such as codex rules in pathology.  

Another departure of our method from prior work relates to the use of reconstruction loss to stabilize adversarial training. Our initial experiments with the adversarial training revealed that the encoder tends to induce sparse and low-variance representations regardless of inputs, which in turn degrades prediction performance. To address this issue, we revisit the idea of autoencoders and introduce a word-level reconstruction loss that is jointly optimized with other components of the model. We empirically demonstrate that this new objective yields richer and more diversified feature representation, as reflected in improved adaptation performance.

We evaluate our model on a pathology report dataset and a review dataset. On the pathology dataset, we explore the task of cross-aspect adaptation across different types of breast disease. Specifically, we test on six adaptation tasks, and empirical results show that our method consistently outperforms all other baselines. We demonstrate that both adversarial training and aspect-relevance scoring play a crucial role in this task, yielding a 13\% and a 24\% absolute gain over the counterpart baseline respectively. Moreover, our unsupervised adaptation result is only 2.8\% behind the accuracy of a supervised model. On the review dataset, we test on adaptation from the hotel domain to the restaurant domain. Our model outperforms the no-adversarial-training baseline by 2.5\%. Finally, we analyze the behavior of adversarial training and assess the impact of individual components to the overall model performance.

