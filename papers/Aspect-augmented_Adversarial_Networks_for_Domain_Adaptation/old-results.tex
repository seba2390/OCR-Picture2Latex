%\newcommand\Tstrut{\rule{0pt}{2.6ex}}         % = `top' strut
%\newcommand\Bstrut{\rule[-0.9ex]{0pt}{0pt}}   % = `bottom' strut

\begin{table*}[t]
    \centering
    \begin{tabular}{llcccccc}
	\toprule
	%\hline
	\multicolumn{2}{c}{\textsc{Domain}} & \multirow{2}{*}{~~~SVM~~~} & \multirow{2}{*}{~~~mSDA~~~} & ~Source~ & \multirow{2}{*}{~Ours-NA} & \multirow{2}{*}{Ours-Full} & \multirow{2}{*}{In-Domain}\\
    \textsc{Source} & \textsc{Target} & & & Only & & \\
    \midrule
    Value & \multirow{5}{*}{\begin{tabular}[x]{@{}c@{}}Restaurant\\Overall\end{tabular}} & 82.2 & 84.7 & 87.4 & 89.9 & \tb{90.7} & \multirow{5}{*}{93.4} \\
    Room &  & 75.6 & 80.3 & 79.3 & 80.3 & \tb{88.0} &  \\
    Checkin &  & 77.8 & 81.0 & 83.0 & 80.7 & \tb{85.9} & \\
    Service &  & 82.2 & 83.8 & 88.0 & 88.9 & \tb{89.1} &  \\
    Cleanliness &  & 77.9 & 78.4 & \tb{83.2} & 79.7 & 81.8 &  \\
    \midrule
    \midrule
    %\hline
    \multicolumn{2}{c}{\textsc{Average}}  & 79.1 & 81.6 & 84.2 & 83.9 & \tb{87.1} & 93.4 \\
	\bottomrule
    %\hline
    \end{tabular}
    \caption{\tb{Review:} Classification accuracy (\%) of different approaches on the reviews dataset. The hotel reviews from TripAdvisor (source domain) consist of five different aspects while the restaurant reviews from Yelp (target domain) has labels only for a single \emph{overall} aspect. Boldface numbers indicate the best accuracy for each testing scenario.}\label{tb:review}
\end{table*}

\section{Results}\label{sec:result}

In this section, we first show the main comparisons of the prediction accuracy between our model and other baselines. In Section \ref{sec:analysis}, we provide a more detailed analysis on each component of our model and demonstrate their impact to the model performance.

\subsection{Main Results}\label{sec:main}

Table \ref{tb:pathology} summarizes the classification accuracy of different methods on the pathology dataset, including the results of six domain adaptation tasks. Our full model (Ours-Full) consistently achieves the best performance on each task compared to other baselines and model variations. It is not surprising that SVM and mSDA perform poorly on this dataset because they only predict labels based  on an overall feature representation of the input. Thus they are unable to predict labels for a particular aspect in the report. Compared to Ours-NA, our model achieves a significant improvement of 22.5\% (94.2\% vs 71.7\%) on average. Note that Ours-NA uses the same set of labeled and unlabeled data as our model, and the only difference is that Ours-NA has no adversarial training. As a reference, we also provide a performance upper bound by training our model on the full labeled set in the target domain, denoted as In-Domain in the last column of Table \ref{tb:pathology}. On average, the accuracy of our model is only 2.6\% behind this upper bound.

Table \ref{tb:review} shows the results of domain adaptation from each aspect in the hotel reviews to the overall ratings of restaurant reviews. Our full model achieves the best performance on four out of five adaptation tasks. On average, our full model outperforms Ours-NA by 3.2\% (87.1\% vs. 83.9\%). The gains are more prominent when using room quality and checkin service aspects as the source domain, reaching 7.7\% and 5.2\%, respectively. The last column shows the in-domain supervised accuracy (93.4\%) using 100k restaurant reviews. 

\begin{figure}[t]
\centering
\includegraphics[width=0.45\textwidth]{figures/recon_heat.pdf}
%\vspace{-0.11in}
\caption{Heat map of $150\times 150$ matrices. Each row corresponds to the vector representation of a document that comes from either the source domain (top half) or the target domain (bottom half). The left matrix is generated by our model without the reconstruction loss function and the right one is generated by our full model.}\label{fig:heat}
\end{figure}


\subsection{Analysis}\label{sec:analysis}

\begin{figure*}[t]
\centering
\includegraphics[width=0.95\textwidth]{figures/neighbor.pdf}
\caption{Examples of restaurant reviews and their nearest neighboring hotel reviews induced by different models (column 2 and 3). The distance between reviews is measure by the cosine similarity between their vector representations induced by the model. For hotel reviews, the room quality is the focal aspect and we show the sentences that have high relevance score. The sentiment phrases of each review are in blue, and some reviews are also shortened for space.}
\label{fig:neighbor}
\end{figure*}

\begin{table}[t]
    \centering
    \begin{tabular}{lcccc}
	\toprule
    \multirow{2}{*}{\textsc{Dataset}} & \multicolumn{2}{c}{Ours-Full} &  \multicolumn{2}{c}{Ours-NA}\\
    \cmidrule(lr){2-3} \cmidrule(l){4-5}
     & -REC. & +REC. & -REC. & +REC.\\
    \midrule
    \textsc{Pathology} & 90.4 & 94.2 & 68.8 & 71.7 \\
    \textsc{Review} & 80.4 & 87.1 & 85.4 & 83.9 \\
    \bottomrule
    \end{tabular}
    \caption{Impact of adding the reconstruction component in the model, measured by the average accuracy on each dataset. +REC. and -REC. denote the presence and absence of the reconstruction loss, respectively. }\label{tb:recon}
\end{table}

\paragraph{Impact of the reconstruction loss} We first analyze the impact of adding embedding reconstruction loss in the learning objective. Figure \ref{fig:heat} shows the heat maps of the learned document representations from either the source domain (top half) or the target domain (bottom half) on the review dataset when room quality is the focal aspect. There is no significant distribution difference between the top and the bottom half of the matrices, indicating domain-invariance of learned representations in both cases. However, the left matrix (w/o reconstruction) is much more sparse than the right one (with reconstruction). Almost 85\% entries of the left matrix have small values ($<10^{-6}$) while the sparsity is only about 30\% for the right one. Moreover, the standard deviation of the right matrix is also ten times higher than the left one. These comparisons demonstrate that the reconstruction loss function improves both the richness and diversity of the learned representations. 

Table \ref{tb:recon} summarizes the impact of the reconstruction loss on model performance. For our full model (Ours-Full), adding the reconstruction loss yields an average of 3.8\% gain on the pathology dataset and 6.7\% on the review dataset. However, in the case of no adversarial training (Ours-NA), adding the reconstruction component has no clear effect. This is expected because the main motivation of adding this component is to achieve a more robust adversarial training.

\begin{table}[t]
    \centering
    \begin{tabular}{lcc}
	\toprule
     & \textsc{Pathology} & \textsc{Review} \\
     \midrule
     w/o Relevance Score & 69.5 & 87.0 \\
     w/ Relevance Score & 94.2 & 87.1 \\
    \bottomrule
    \end{tabular}
    \caption{Comparisons of average accuracy on each dataset if removing the relevance scoring component from the model. }\label{tb:relevance}
\end{table}

\paragraph{Impact of relevance scoring} Next we analyze the impact of the aspect-relevance scorer on model performance. As shown in Table \ref{tb:relevance}, the relevance scoring component has more significant impact on the pathology reports dataset (+24.7\%) than on the review dataset (+0.1\%). This is because on the pathology dataset our task is to adapt the model from one aspect to another within the \emph{same} set of reports. Therefore, it is essential for the model to have the capacity of distinguishing between different aspects in this case. In contrast, on the review dataset, different aspects in hotel reviews are highly correlated, and their labels are often consistent to each other.

\begin{table}[t]
    \centering
    \begin{tabular}{lccc}
	\toprule
    \textsc{Dataset} & $\lambda^t=0$ & $0<\lambda^t<\infty$ & $\lambda^t=\infty$ \\
    \midrule
    \textsc{Pathology} & 86.9 & 94.2 & 79.0 \\
    \textsc{Review} & 80.1 & 87.1 & 85.7 \\
    \bottomrule
    \end{tabular}
    \caption{The effect of regularization of the transformation layer on the performance. $\lambda^t=0$ indicates no regularization on transformation and $0<\lambda^t<\infty$ corresponds to our model. $\lambda^t=\infty$ corresponds to the removal of the transformation layer because the transformation is always identity in this case. }\label{tb:transformation}
\end{table}


\paragraph{Regularization on the transformation layer} Table \ref{tb:transformation} shows the model accuracy with different regularization weights $\lambda^{t}$ in Equation \ref{eq:transformation}. We change  $\lambda^{t}$ to reflect different model variations. First, our model performs better than $\lambda^{t}=\infty$ on both datasets, indicating the importance of adding the transformation layer. Second, no regularization $\lambda^{t}=0$ also consistently results in inferior performance. Actually, we note that the results of $\lambda^{t}=0$ are close to the accuracy of our model without the reconstruction loss, as shown in the first column of Table \ref{tb:recon}. This is not a coincidence because a transformation without regularization has too much flexibility and will dilute the effect from reconstruction. As a result, the adversarial training will fail in a similar way to the case of no reconstruction loss.

\paragraph{Examples of neighboring reviews} Finally, we illustrate in Figure \ref{fig:neighbor} a case study on the properties of learned document representations by different models. The first column shows two example restaurant reviews that have similar representations by either Ours-Full or Ours-NA. Sentiment phrases in these examples are in blue color and are mostly food-specific, such as ``undercooked'' and ``closer to bland''. In the other two columns, we show example hotel reviews that are nearest neighbors to the restaurant reviews, measured by cosine similarity between their representations. In column 2, many sentiment phrases are specific for room quality, such as ``old'' and ``rained water from above''. Our full model successfully learns to eliminate domain-specific information and to map those domain-specific words into similar domain-invariant representations. In column 3, however, most sentiment phrases are common (e.g. old) or about food (e.g. food poison). This demonstrates that Ours-NA only captures domain-invariant features from phrases that commonly present in both domains. 

\com{
\begin{table}[t]
    \centering
    \begin{tabular}{lcccc}
	\toprule
    \textsc{~Method~} & \textsc{Syn1} & \textsc{Syn2} & \textsc{Syn3} & \textsc{Syn4} \\
    \midrule
    ~Ours-NA & 75.4 & 52.8 & 100.0 & 49.2 \\
    ~Ours-Full & 99.8 & 100.0 & 100.0 & 100.0 \\
    \bottomrule
    \end{tabular}
    \caption{Classification accuracy on four synthetic datasets that represent different challenges in domain adaptation.}\label{tb:synthetic}
\end{table}

\paragraph{When adversarial training is useful?} As shown in Table \ref{tb:pathology} and \ref{tb:review}, the gains from using adversarial training vary significantly over different adaptation scenarios. To better understand when adversarial training is useful, we further test on four synthetic datasets that represent different challenges in domain adaptation. Specifically, each synthetic document consists of several randomly generated sentences. Each sentence is always associated with a random aspect and contains a special token as the aspect name (e.g. \texttt{ASPECT0\_NAME0}) and another special token as the aspect polarity (e.g. \texttt{ASPECT0\_POSITIVE0}).\footnote{Aspects, names and polarity tokens each have about ten options.} Document labels are either positive or negative, indicated by the polarity tokens of the focal aspect, except for the first dataset (see below). The adaptation task is to transfer the model from one aspect to another. The characteristics of each dataset are as follows.

\begin{itemize}[leftmargin=12pt,parsep=-2pt,topsep=2pt]
\item \textsc{Syn1:} Positive if the name of the focal aspect  (e.g. \texttt{ASP0\_NAME0}) occurs, otherwise negative. 

\item \textsc{Syn2:} Positive polarity tokens have 20\% overlap across domains (the focal aspects). Negative polarity tokens have no overlap.

\item \textsc{Syn3:} Both positive and negative polarity tokens have 20\% overlap across domains.

\item \textsc{Syn4:} Both positive and negative polarity tokens have 5\% overlap across domains.
\end{itemize}
Table \ref{tb:synthetic}
}

