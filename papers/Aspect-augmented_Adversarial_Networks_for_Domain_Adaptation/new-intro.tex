%!TEX root = paper.tex
\section{Introduction}

Many NLP problems are naturally multitask classification problems. For instance, values extracted for different fields from the same document are often dependent as they share the same context. Existing systems rely on this dependence (transfer across fields) to improve accuracy. In this paper, we consider a version of this problem where there is a clear dependence between two tasks but annotations are available only for the source task. For example, the target goal may be to classify pathology reports (shown in Figure \ref{fig:pathology}) for the presence of lymph invasion but training data are available only for carcinoma in the same reports. We call this problem \emph{aspect transfer} as the objective is to learn to classify examples differently, focusing on different aspects, without access to target aspect labels. Clearly, such transfer learning is possible only with auxiliary information relating the tasks together. 

\iffalse
In this paper, we consider multitask classification from a different angle. Specifically, we assume that annotations are provided for a single task but not available for other tasks. The goal is to utilize these available source annotations to build a classifier for a target task.  We call this problem ``aspect transfer'' as the two classification tasks can be thought to pertain to different aspects of the same document. For example, the target goal may be to classify pathology reports (shown in Figure \ref{fig:pathology}) for the presence of lymph invasion but the available training data involve only annotations for carcinoma in the same reports. This problem is a special case of transfer learning where both source and target classifiers operate over the same document -- in the example above, pathology reports.
\fi

% \begin{figure}[t]
% \centering
%  \includegraphics[width=0.46\textwidth]{figures/pathology.pdf}
% \caption{A snippet of a breast pathology report with diagnosis results for two types of disease. Evidence for both results is in red and blue, respectively. }\label{fig:pathology}
% \vspace{-0.1in}
% \end{figure}

\begin{figure}[t]
%\vspace{-0.2in}
\centering
 \includegraphics[width=0.48\textwidth]{figures/task_new.pdf}
\caption{A snippet of a breast pathology report with diagnosis results for two types of disease (aspects): carcinoma (IDC) and lymph invasion (LVI). Note how the same phrase indicating positive results (e.g. \emph{identified}) is applicable to both aspects.
A transfer model learns to map other key phrases (e.g. \emph{Grade 3}) to such shared indicators.
%Relevant fragments for each aspect are marked by solid line for IDC and dash line for LVI. 
}\label{fig:pathology}
%\vspace{-0.1in}
\end{figure}

The key challenge is to articulate and incorporate commonalities across the tasks. For instance, in classifying reviews of different products, sentiment words (referred to as \emph{pivots}) can be shared across the products. This commonality enables one to align  feature spaces across multiple products, enabling useful transfer~\cite{blitzer2006domain}. Similar properties hold in other contexts and beyond sentiment analysis. Figure~\ref{fig:pathology} shows that certain words and phrases like ``identified'', which indicates the presence of a histological property, are applicable to both carcinoma and lymph invasion. Our method learns and relies on such shared indicators, and utilizes them for effective transfer. 
%\add{One remaining challenge 
%The key challenge is to learn to project aspect-specific information to this invariant space.
%}

The unique feature of our transfer problem is that both the source and the target classifiers operate over the same domain, i.e., the same examples. In this setting, traditional transfer methods will always predict the same label for both aspects and thus leading to failure. Instead of supplying the target classifier with direct training labels, our approach builds on a secondary relationship between the tasks using aspect-relevance annotations of sentences. These relevance annotations indicate a possibility that the answer could be found in a sentence, not what the answer is. One can often write simple keyword rules that identify sentence relevance to a particular aspect through representative terms, e.g., specific hormonal markers in the context of pathology reports. Annotations of this kind can be readily provided by domain experts, or extracted from medical literature such as codex rules in pathology~\cite{pantanowitz2008informatics}. We assume a small number of relevance annotations (rules) pertaining to both source and target aspects as a form of weak supervision. We use this sentence-level aspect relevance to learn how to encode the examples (e.g., pathology reports) from the point of view of the desired aspect. In our approach, we construct different aspect-dependent encodings of the same document by softly selecting sentences relevant to the aspect of interest. The key to effective transfer is how these encodings are aligned. 

This encoding mechanism brings the problem closer to the realm of standard domain adaptation, where the derived aspect-specific representations are considered as different domains. Given these representations, our method learns a label classifier shared between the two domains. To ensure that it can be adjusted only based on the source class labels, and that it also reasonably applies to the target encodings, we must align the two sets of encoded examples.\footnote{This alignment or invariance is enforced on the level of sets, not individual reports; aspect-driven encoding of any specific report should remain substantially different for the two tasks since the encoded examples are passed on to the same classifier.}  Learning this alignment is possible because, as discussed above, some keywords are directly transferable and can serve as anchors for constructing this invariant space. To learn this invariant representation, we introduce an adversarial domain classifier analogous to the recent successful use of adversarial training in computer vision~\cite{ganin2014unsupervised}. The role of the domain classifier (adversary) is to learn to distinguish between the two types of encodings. During training we update the encoder with an adversarial objective to cause the classifier to fail.  
The encoder therefore learns to eliminate aspect-specific information so that encodings look invariant (as sets) to the classifier, thus establishing aspect-invariance encodings and enabling transfer.
%The role of the domain classifier (adversary) is to learn to distinguish between the two types of encodings, establishing invariance (as sets) when it fails. The encoder therefore learns to eliminate aspect-specific information, establishing aspect-invariance encodings.
All three components in our approach, 1) aspect-driven encoding, 2) classification of source labels, and 3) domain adversary, are trained jointly (concurrently) to complement and balance each other.

Adversarial training of domain and label classifiers can be challenging to stabilize. In our setting, sentences are encoded with a convolutional model.
%weighted by predicted aspect relevance, and then combined into aspect-driven document representations. 
Feedback from adversarial training can be an unstable guide for how the sentences should be encoded.
%in the first place. 
To address this issue, we incorporate an additional word-level auto-encoder reconstruction loss to ground the convolutional processing of sentences. We empirically demonstrate that this additional objective yields richer and more diversified feature representations, improving transfer. 

We evaluate our approach on pathology reports (aspect transfer) as well as on a more standard review dataset (domain adaptation). On the pathology dataset, we explore cross-aspect transfer across different types of breast disease. Specifically, we test on six adaptation tasks, consistently outperforming all other baselines. Overall, our full model achieves 27\% and 20.2\% absolute improvement arising from aspect-driven encoding and adversarial training respectively. Moreover, our unsupervised adaptation method is only 5.7\% behind the accuracy of a supervised target model. On the review dataset, we test adaptations from hotel to restaurant reviews.
Our model outperforms the marginalized denoising autoencoder~\cite{chen2012marginalized} by 5\%.
Finally, we examine and illustrate the impact of individual components on the resulting performance. 