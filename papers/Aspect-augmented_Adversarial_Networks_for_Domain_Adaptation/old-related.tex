% domain adaptation, SCL etc. deep learning approach
% technically similar
% semeval shared task
% rationale augmented CNN
% In both approaches, the actual classifier/predictor is learned in a separate step using the feature representation learned by autoencoder(s)

\section{Related Work}

\paragraph{Domain Adaptation for Text Classification}
A large number of domain adaptation methods have been proposed in the context of shallow learning, ranging from data reweighing \cite{huang2006correcting,gong2013connecting} to learning correspondence between features \cite{blitzer2006domain,daume2009frustratingly,yang2015unsupervised}. The key idea is to learn to distill the domain-invariant properties from explicitly defined features, such as word n-grams. However, this idea is not directly applicable to deep learning methods because the learned mapping from inputs to representations lacks interpretability in this case.

Recently, there has been an increasing amount of deep learning work on domain adaptation for NLP tasks. A number of earlier researches propose to train an autoencoder on both domains \cite{glorot2011domain,chen2012marginalized,chopra2013dlid}. Later, \newcite{zhou2016bi} proposes to minimize the distribution shift over domains in a linear data combination manner. Other methods on inducing abstract transferable representations include transduction learning \cite{sener2016learning} and residual transfer networks \cite{long2016unsupervised}. In contrast, we use adversarial training to encourage learning domain-invariant features in a more explicit way. Our approach also offers another two advantages over prior work. First, we jointly optimize features with the final classification task while previous work only learns task-independent features using autoencoders. Second, our model can handle classification regarding a particular aspect in texts, while previous methods only operate on an overall representation of an input.

Our approach closely relates to the idea of domain-adversarial training. Adversarial networks have originally been developed for image generation \cite{goodfellow2014generative,makhzani2015adversarial}, and later applied to domain adaption in computer vision \cite{ganin2014unsupervised,ganin2015domain,domain_separation_nets16} and speech recognition \cite{shinohara2016adversarial}. While \newcite{ganin2015domain} also apply this idea to sentiment analysis, their practical gains have remained limited. We departure from this work from at least two aspects: 1) our model is aspect-augmented and 2) we provide detailed analysis and achieve a more robust adversarial training through the use of reconstruction loss. As our experiments demonstrate these advancements result in improved performance.


\paragraph{Aspect-based Text Classification} Our aspect-augmented model also relates to previous work on rationale/aspect-based classification \cite{zaidan2007using,marshall2015robotreviewer,zhang2016rationale,brun-perez-roux:2016:SemEval}. These methods typically rely on human-provided rationales to improve prediction. In contrast, our model only assumes access to a small set of keywords as a source of weak supervision. Attention-based models, on the other hand, offer an unsupervised mechanism to induce \emph{soft} attention over each unit in texts \cite{bahdanau2014neural,rush2015neural,chen2015abc,cheng2016long,xu2015show,xu2015ask,yang2015stacked,martins2016softmax}. However, such attention is learned with a large amount of labeled data while our work assumes no annotations in the target domain. Most recently, \newcite{lei2016rationalizing} introduce a stochastic attention mechanism that selects \emph{hard} rationales in an unsupervised manner. However, similarly
to attention models, this rationale selections is also learned using labeled data. Moreover, all prior approaches focus on in-domain classification. In this paper, however, we study aspect-based classification in the context of domain adaptation. 

% Domain-Adversarial Training of Neural Networks
% Adversarial Multi-Task Learning of Deep Neural Networks for Robust Speech Recognition
% Revisiting Batch Normalization For Practical Domain Adaptation
% Bi-Transferring Deep Neural Networks for Domain Adaptation, a linear data reconstruction manner
% Deep learning for domain adaptation by interpolating between domains
%Deep Learning for Aspect-Based Sentiment Analysis, assume gold relevance score. 1
% brun-perez-roux:2016:SemEval
% Show, attend and tell: Neural image caption generation with visual attention
% Rationale-Augmented Convolutional Neural Networks for Text Classification

