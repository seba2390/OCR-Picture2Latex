This document encoding mechanism brings the problem closer to the realm of standard domain adaption. Such aspect-dependent encodings can be considered as features extracted from source or target domains, which are then passed to a single label classifier that are shared between the two domains. To ensure that the classifier can be adjusted only based on the source class labels and also reasonably applies to the target encodings, we must align the two sets of encoded examples.\footnote{This alignment or invariance is enforced on the level of sets, not individual examples or reports; aspect-drive encoding of any specific report should remain substantially different for the two tasks since the encoded examples are passed on to the same classifier.}  Learning this alignment is possible because, as discussed above, some keywords are directly transferable and can serve as anchors for constructing this invariant space. To learn this invariant representation, we employ domain adversarial training analogously to recent success in computer vision~\cite{ganin2014unsupervised}. The core idea is to jointly learn a domain classifier (adversary) whose role is to distinguish between the two types of encodings. During training we update the encoder with an adversarial objective to fail the classifier. The encoder therefore learns to eliminate aspect-specific information so that encodings look invariant (as sets) to the classifier, thus enabling transfer. All the three components in our approach, 1) aspect-driven encoding, 2) classification of source labels, and 3) domain adversary, are trained jointly (concurrently) to complement and balance each other.

Adversarial training of domain and label classifiers can be challenging to stabilize. In our setting, feedback from adversarial training can be an unstable guide for how documents should be encoded. To address this issue, we incorporate an additional word-level autoencoder reconstruction loss to ground the convolutional processing of sentences. We empirically demonstrate that this additional objective yields richer and more diversified feature representations, improving transfer. 
