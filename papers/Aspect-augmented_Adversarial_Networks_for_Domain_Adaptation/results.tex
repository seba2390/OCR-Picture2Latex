
\begin{table*}[t]
%\small
%\vspace{-0.05in}
    \centering
    \begin{tabular}{l@{~}@{~}lcccc@{~~}@{~~}c@{~~}@{~~}c@{~~}@{~~}c}
	\toprule
	%\hline
	\multicolumn{2}{c}{\textsc{Domain}} & \multirow{2}{*}{~SVM~} & Source & \multirow{2}{*}{~mSDA~} & \multirow{2}{*}{AAN-NA} & \multirow{2}{*}{AAN-NR} & \multirow{2}{*}{AAN-Full} & \multirow{2}{*}{In-Domain}\\
    \textsc{Source} & \textsc{Target} & & Only & & & & \\
    \midrule
    Value & \multirow{5}{*}{\specialcell{c}{Restaurant\\Overall}} & 82.2 & 87.4 & 84.7 & 87.1 & \tb{91.1} & 89.6 & \multirow{5}{*}{93.4} \\
    Room &  & 75.6 & 79.3 & 80.3 & 79.7 & 86.1 & \tb{86.6} &  \\
    Checkin &  & 77.8 & 83.0 & 81.0 & 80.9 & \tb{87.2} & 85.4 & \\
    Service &  & 82.2 & 88.0 & 83.8 & 88.8 & 87.9 & \tb{89.1} &  \\
    Cleanliness &  & 77.9 & 83.2 & 78.4 & 83.1 & \tb{84.5} & 81.4 &  \\
    \midrule
    %\midrule
    %\hline
    \multicolumn{2}{c}{\textsc{Average}}  & 79.1 & 84.2 & 81.6 & 83.9 & \tb{87.3} & 86.4 & 93.4 \\
	\bottomrule
    %\hline
    \end{tabular}
    \caption{\tb{Review:} Classification accuracy (\%) of different approaches on the reviews dataset. 
%The hotel reviews from TripAdvisor (source domain) consist of five different aspects while the restaurant reviews from Yelp (target domain) has labels only for a single \emph{overall} aspect. 
Columns have the same meaning as in Table \ref{tb:pathology}. Boldface numbers indicate the best accuracy for each testing scenario.}\label{tb:review}
\end{table*}

%\section{Results}\label{sec:result}

%Yuan: if u need space, you can cut this part out
%In this section, we first present classification accuracy of different approaches on the pathology and the review datasets. In Section \ref{sec:analysis}, we provide detailed analysis on different components of our model and demonstrate their impact on the overall model performance.


\begin{figure*}[t]
\centering
\includegraphics[width=0.9\textwidth]{figures/recon_heat_new.pdf}
%\vspace{-0.11in}
\caption{Heat map of $150\times 150$ matrices. Each row corresponds to the vector representation of a document that comes from either the source domain (top half) or the target domain (bottom half). Models are trained on the review dataset when room quality is the source aspect. 
%The left matrix is generated by Ours-NA (no adversarial training) without adding reconstruction loss. The middle/right matrix is generated by Ours-Full in the absence/presence of reconstruction loss respectively.
}\label{fig:heat}
\end{figure*}



%\subsection{Main Results}\label{sec:main}
\section{Main Results}\label{sec:main}

%Yuan: maybe it is worth to add another baseline for pathology that uses rules only
%The accuracy of rule-based models can be as high as 90%. I prefer not to add them.
Table \ref{tb:pathology} summarizes the classification accuracy of different methods on the pathology dataset, including the results of twelve adaptation tasks. Our full model (AAN-Full) consistently achieves the best performance on each task compared with other baselines and model variants. It is not surprising that SVM and mSDA perform poorly on this dataset because they only predict labels based on an overall feature representation of the input, and do not utilize weak supervision provided by aspect-specific keywords.  As a reference, we also provide a performance upper bound by training our model on the full labeled set in the target domain, denoted as In-Domain in the last column of Table \ref{tb:pathology}. On average, the accuracy of our model (AAN-Full) is only 5.7\% behind this upper bound.
% The key to this success is that our model learns to map representations of different aspect names (e.g. DCIS and LCIS) to the same one.

Table \ref{tb:review} shows the adaptation results from each aspect in the hotel reviews to the overall ratings of restaurant reviews. AAN-Full and AAN-NR are the two best performing systems on this review dataset, attaining around 5\% improvement over the mSDA baseline. Below, we summarize our findings when comparing the full model with the two model variants AAN-NA and AAN-NR.
%The last column shows the in-domain supervised accuracy (93.4\%) using 100k restaurant reviews. 

\paragraph{Impact of adversarial training} We first focus on comparisons between AAN-Full and AAN-NA. The only difference between the two models is that AAN-NA has no adversarial training. On the pathology dataset, our model significantly outperforms AAN-NA, yielding a 20.2\% absolute average gain (see Table \ref{tb:pathology}). On the review dataset, our model obtains 2.5\% average improvement over AAN-NA. As shown in Table \ref{tb:review}, the gains are more significant when training on \emph{room} and \emph{checkin} aspects, reaching 6.9\% and 4.5\%, respectively. 
%Yuan: do you have a clue why these aspects are more promising
%no, I guess other aspects use sentiment words that are more commonly appear in the restaurant domain, so models without adversarial training also works well. It's hard to verify that.

\paragraph{Impact of relevance scoring} As shown in Table \ref{tb:pathology}, the relevance scoring component plays a crucial role in classification on the pathology dataset. Our model achieves more than 27\% improvement over AAN-NR.
%Yuan: have you actually computed the correlation. Based on what I know they do have some weak correlation
%Yes, I computed the correlation between LCIS, DCIS, IDC and ALH. for most pairs, the correlation is in the range [-0.1, 0.1]. The only exception is DCIS & IDC, their correlation is 0.4
This is because in general aspects have zero correlations to each other in pathology reports. Therefore, it is essential for the model to have the capacity of distinguishing across different aspects in order to succeed in this task. 

On the review dataset, however, we observe that relevance scoring has no significant impact on performance. On average, AAN-NR actually outperforms AAN-Full by 0.9\%. This observation can be explained by the fact that different aspects in hotel reviews are highly correlated to each other. For example, the correlation between room quality and cleanliness is 0.81, much higher than aspect correlations in the pathology dataset. In other words, the sentiment is typically consistent across all sentences in a review, so that selecting aspect-specific sentences becomes unnecessary. Moreover, our supervision for the relevance scorer is weak and noisy because the aspect keywords are obtained in a semi-automatic way. Therefore, it is not surprising that AAN-NR sometimes delivers a better classification accuracy than AAN-Full.


% \begin{figure}[t]
% \centering
% \includegraphics[width=0.45\textwidth]{figures/recon_heat.pdf}
% %\vspace{-0.11in}
% \caption{Heat map of $150\times 150$ matrices. Each row corresponds to the vector representation of a document that comes from either the source domain (top half) or the target domain (bottom half). Models are trained on the review dataset when room quality is the source aspect. The left matrix is generated by our model without the reconstruction loss function and the right one is generated by our full model.}\label{fig:heat}
% \end{figure}


%\subsection{Analysis}\label{sec:analysis}
\section{Analysis}\label{sec:analysis}

\begin{figure*}[t]
%\vspace{-0.1in}
\centering
\includegraphics[width=0.95\textwidth]{figures/neighbor.pdf}
\caption{Examples of restaurant reviews and their nearest neighboring hotel reviews induced by different models (column 2 and 3). 
%The distance between reviews is measure by the cosine similarity between their vector representations induced by the model. 
We use room quality as the source aspect.
%and we show the sentences that have high relevance score.
The sentiment phrases of each review are in blue, and some reviews are also shortened for space.}
\label{fig:neighbor}
\end{figure*}

\begin{table}[t]
    \centering
    \begin{tabular}{lcccc}
	\toprule
    \multirow{2}{*}{\textsc{Dataset}} & \multicolumn{2}{c}{AAN-Full} &  \multicolumn{2}{c}{AAN-NA}\\
    \cmidrule(lr){2-3} \cmidrule(l){4-5}
     & -REC. & +REC. & -REC. & +REC.\\
    \midrule
    \textsc{Pathology} & 86.2 & 91.2 & 68.6 & 72.0 \\
    \textsc{Review} & 80.8 & 86.4 & 85.0 & 83.9 \\
    \bottomrule
    \end{tabular}
    \caption{Impact of adding the reconstruction component in the model, measured by the average accuracy on each dataset. +REC. and -REC. denote the presence and absence of the reconstruction loss, respectively. }\label{tb:recon}
%\vspace{-0.1in}
\end{table}


\begin{table}[t]
    \centering
    \begin{tabular}{lccc}
	\toprule
    \textsc{Dataset} & $\lambda^t=0$ & $0<\lambda^t<\infty$ & $\lambda^t=\infty$ \\
    \midrule
    \textsc{Pathology} & 77.4 & 91.2 & 81.4 \\
    \textsc{Review} & 80.9 & 86.4 & 84.3 \\
    \bottomrule
    \end{tabular}
    \caption{The effect of regularization of the transformation layer $\lambda^t$ on the performance. 
    %$\lambda^t=0$ indicates no regularization on transformation and $0<\lambda^t<\infty$ corresponds to our model. $\lambda^t=\infty$ corresponds to the removal of the transformation layer because the transformation is always identity in this case. 
    }\label{tb:transformation}
\end{table}

\paragraph{Impact of the reconstruction loss} Table \ref{tb:recon} summarizes the impact of the reconstruction loss on the model performance. For our full model (AAN-Full), adding the reconstruction loss yields an average of 5.0\% gain on the pathology dataset and 5.6\% on the review dataset.

To analyze the reasons behind this difference, consider 
Figure \ref{fig:heat} that shows the heat maps of the learned document representations on the review dataset.
%YUan: put this sentence about room uqality aspect into the captions
%Done
%when room quality is the source aspect. 
The top half of the matrices corresponds to input documents from the source domain and the bottom half corresponds to the target domain. Unlike the first matrix, the other two matrices have no significant difference between the two halves, indicating that adversarial training helps learning of domain-invariant representations. However, adversarial training also removes a lot of information from representations, as the second matrix is much more sparse than the first one. 
The third matrix shows that adding reconstruction loss effectively addresses this sparsity issue. Almost 85\% entries of the second matrix have small values ($<10^{-6}$) while the sparsity is only about 30\% for the third one. Moreover, the standard deviation of the third matrix is also ten times higher than the second one. These comparisons demonstrate that the reconstruction loss function improves both the richness and diversity of the learned representations. 
%Yuan: how the matrix would look without adversarial training. The value of showing it is to make it clear for the reader that adversarial training removes a lot of information
%see the new figure. it has (1) difference between domains (2) less sparsity. 
Note that in the case of no adversarial training (AAN-NA), adding the reconstruction component has no clear effect. This is expected because the main motivation of adding this component is to achieve a more robust adversarial training.

%There is no significant difference between the two halves, indicating that learned representations are domain-invariant in both cases. However, the left matrix (without reconstruction) is much more sparse than the right one (with reconstruction). Almost 85\% entries of the left matrix have small values ($<10^{-6}$) while the sparsity is only about 30\% for the right one. Moreover, the standard deviation of the right matrix is also ten times higher than the left one. These comparisons demonstrate that the reconstruction loss function improves both the richness and diversity of the learned representations. 




\paragraph{Regularization on the transformation layer} 
Table \ref{tb:transformation} shows the averaged accuracy with different regularization weights $\lambda^{t}$ in Equation \ref{eq:transformation}. We change  $\lambda^{t}$ to reflect different model variants. 
%We can change the regularization weight of the transformation layer $\lambda^{t}$ in Equation \ref{eq:transformation} to reflect different variants of our model. 
First, $\lambda^{t}=\infty$ corresponds to the removal of the transformation layer because the transformation is always identity in this case. Our model performs better than this variant on both datasets, yielding an average improvement of 9.8\% on the pathology dataset and 2.1\% on the review dataset. This result indicates the importance of adding the transformation layer. Second, using zero regularization ($\lambda^{t}=0$) also consistently results in inferior performance, such as 13.8\% loss on the pathology dataset. We hypothesize that zero regularization will dilute the effect from reconstruction because there is too much flexibility in transformation. As a result, the transformed representation will become sparse due to the adversarial training, leading to a performance loss.
%Table \ref{tb:transformation} shows the averaged accuracy with different regularization weights $\lambda^{t}$ in Equation \ref{eq:transformation}. We change  $\lambda^{t}$ to reflect different model variants. First, $\lambda^{t}=\infty$ corresponds to the removal of the transformation layer because the transformation is always identity. Our model performs better than this variant on both datasets, indicating the importance of adding the transformation layer. Second, using zero regularization ($\lambda^{t}=0$) also consistently results in inferior performance. We hypothesize that zero regularization will dilute the effect from reconstruction because of too much flexibility in transformation. As a result, the transformed representation will become sparse due to the adversarial training, leading to the performance loss.

%Actually, we note that the results of $\lambda^{t}=0$ are close to the accuracy of our model without the reconstruction loss, as shown in the first column of Table \ref{tb:recon}. This is not a coincidence because

\paragraph{Examples of neighboring reviews} Finally, we illustrate in Figure \ref{fig:neighbor} a case study on the characteristics of learned abstract representations by different models. The first column shows an example restaurant review. Sentiment phrases in this example are mostly food-specific, such as ``undercooked'' and ``tasted weird''. In the other two columns, we show example hotel reviews that are nearest neighbors to the restaurant reviews, measured by cosine similarity between their representations. In column 2, many sentiment phrases are specific for room quality, such as ``old'' and ``rained water from above''. In column 3, however, most sentiment phrases are either common sentiment expressions (e.g. dirty) or food-related (e.g. food poison), even though the focus of the reviews is based on the room quality of hotels. This observation indicates that adversarial training (AAN-Full) successfully learns to eliminate domain-specific information and to map those domain-specific words into similar domain-invariant representations. In contrast, AAN-NA only captures domain-invariant features from phrases that commonly present in both domains.

\begin{figure}[t]
\centering
\includegraphics[width=0.46\textwidth]{figures/keyword_new.jpg}
%\vspace{-0.11in}
\caption{Classification accuracy (y-axis) on two transfer scenarios (one on review and one on pathology dataset) with a varied number of  keyword rules for learning sentence relevance (x-axis).
}\label{fig:keyword}
\end{figure}

\paragraph{Impact of keyword rules} Finally, Figure \ref{fig:keyword} shows the accuracy of our full model (y-axis) when trained with various amount of keyword rules for relevance learning (x-axis). As expected, the transfer accuracy drops significantly when using fewer rules on the pathology dataset (LCIS as source and ALH as target). In contrary, the accuracy on the review dataset (hotel service as source and restaurant as target) is not sensitive to the amount of used relevance rules. This can be explained by the observation from Table \ref{tb:review} that the model without relevance scoring performs equally well as the full model due to the tight dependence in aspect labels. 


% \begin{table}[t]
%     \centering
%     \begin{tabular}{lcc}
% 	\toprule
%      & \textsc{Pathology} & \textsc{Review} \\
%      \midrule
%      w/o Relevance Score & 69.5 & 87.0 \\
%      w/ Relevance Score & 94.2 & 87.1 \\
%     \bottomrule
%     \end{tabular}
%     \caption{Comparisons of average accuracy on each dataset if removing the relevance scoring component from the model. }\label{tb:relevance}
% \end{table}

