\begin{figure*}[t]
\vspace{-0.1in}
\centering
\includegraphics[width=0.95\textwidth]{figures/model_new.pdf}
%\vspace{-0.11in}
\caption{Aspect-augmented adversarial network for domain adaptation. The model is composed of (a) an aspect-driven document encoder, (b) a label predictor and (c) a domain classifier.
%Each document is encoded in a relevance weighted, aspect-dependent manner and passed on to both the primary label classifier and the domain classifier as the adversary to ensure invariance. Parameters of all the components are learned jointly during training.
}
\label{fig:model}
\end{figure*}

\section{Experimental Setup}\label{sec:setup}

\begin{table}[t]
\begin{small}
    \centering
    \begin{tabular}{llcc}
	\toprule
    \multicolumn{2}{l}{\textsc{Dataset}} & \#Labeled & \#Unlabeled \\
    \midrule
    \multirow{4}{*}{\textsc{Pathology}} & DCIS & 23.8k & \multirow{4}{*}{96.6k} \\
     & LCIS & 10.7k &  \\
     & IDC & 22.9k &  \\
     & ALH & 9.2k &  \\
     \midrule
    \multirow{2}{*}{\textsc{Review}} & Hotel & 100k & 100k \\
     & Restaurant & - & 200k \\
    \bottomrule
    \end{tabular}
\end{small}
    \caption{Statistics of the pathology reports dataset and the reviews dataset that we use for training. The same set of unlabeled reports is used for all different aspects in the pathology reports dataset.}\label{tb:data}
\end{table}


\paragraph{Pathology dataset} 
We first use the breast pathology reports dataset that are collected from three partner hospitals. This dataset contains a total of 96.6 thousand pathology reports. The pathology reports typically include diagnosis results for more than 20 different aspects that are related to breast cancer disease. In our experiments, we focus on four aspects: Ductal Carcinoma In-Situ (DCIS), Lobular Carcinoma In-Situ (LCIS), Invasive Ductal Carcinoma (IDC) and Atypical Lobular Hyperplasia (ALH). 

We explore the domain adaptation problem from one aspect to the other. For example, we want to train a model on annotations of DCIS and apply it on LCIS. Each aspect has two possible class labels, positive or negative. For annotations, we use the combination of the data used in \cite{buckley2012feasibility} and the data from Breast Cancer Database (BCD). Table \ref{tb:data} summarizes the statistics of labeled data for each aspect. We use a held out of 500 reports as the development set (source domain) or the testing set (target domain). We also use the common names of each aspect as a source of weak supervision for learning the relevance scorer. On average, each aspect has about two to three different name variations and they can be easily provided by a doctor. 
%All reports were automatically anonymized for privacy, and we further use NLTK 3.0 for sentence split and tokenization. 


\paragraph{Review dataset}
For our second application, we use a review dataset. As the source domain, we use the hotel reviews dataset that is used in previous work \cite{wang2010latent,wang2011latent}. For the target domain, we use the restaurant reviews dataset from Yelp.\footnote{\url{https://www.yelp.com/dataset_challenge}} Both datasets have ratings on a scale of 1 to 5 stars. We label reviews with ratings $>3$ as positive and those with ratings $<3$ as negative, and we discard the rest. The hotel dataset includes a total of around 200k reviews collected from TripAdvisor,\footnote{\url{https://www.tripadvisor.com/}} so we split 100k as labeled and the other 100k as unlabeled data. We randomly select 200k restaurant reviews as the unlabeled data in the target domain. We use a held out of 2k reports as the development set (source domain) or the testing set (target domain). Table \ref{tb:data} summarizes the statistics of the review dataset.

The hotel reviews naturally have ratings for six aspects, including \emph{value}, \emph{rooms} quality, \emph{checkin} service, room \emph{service}, \emph{cleanliness} and \emph{location}. We use the first five aspects because the sixth aspect \emph{location} has positive values for over 95\% of the reviews and thus the trained model will suffer from the lack of negative examples. The restaurant reviews, however, only have single ratings for an \emph{overall} impression. Therefore, we explore the task of adaptation  from each of the five hotel aspects to the restaurant domain. The hotel reviews dataset also provides a total of 290 keywords for different aspects that are generated by the bootstrapping method used in \cite{wang2010latent}. We use those keywords as supervision for learning the relevance scorer.

\begin{table}[t]
    \centering
    \begin{tabular}{@{~}lc@{~~~}cc@{~~~}c@{~}}
	\toprule
    \multirow{2}{*}{Method} & \multicolumn{2}{c}{Source} & \multicolumn{2}{c}{Target} \\
    \cmidrule(lr){2-3} \cmidrule(l){4-5}
     & Label & Unlabel & Label & Unlabel \\
    \midrule
    SVM & \Yes & \No & \No & \No \\
    mSDA & \Yes & \Yes & \No & \Yes \\
    SourceOnly & \Yes & \Yes & \No & \No \\
    Ours-NA & \Yes & \Yes & \No & \Yes \\
    Ours-Full & \Yes & \Yes & \No & \Yes \\
    In-Domain & \No & \No & \Yes & \No \\
    \bottomrule
    \end{tabular}
    \caption{Usage of labeled and unlabeled data in each domain by our model and other baseline methods. }\label{tb:usage}
\end{table}


\begin{table*}[t]
    \centering
    \begin{tabular}{llcccccc}
	\toprule
	%\hline
	\multicolumn{2}{c}{\textsc{Domain}} & \multirow{2}{*}{~~~SVM~~~} & \multirow{2}{*}{~~~mSDA~~~} & ~Source~ & \multirow{2}{*}{~Ours-NA} & \multirow{2}{*}{Ours-Full} & \multirow{2}{*}{In-Domain}\\
    \textsc{~Source} & \textsc{Target~} & & & Only & & \\
	\midrule
    %\hline
    ~IDC & ALH & 64.4 & 65.4 & 66.4 & 52.6 & \tb{93.8} & 96.8 \\
    ~ALH & IDC & 37.0 & 36.2 & 26.8 & 27.6 & \tb{92.0} & 96.8 \\
    \midrule
    %\hline
    ~DCIS & IDC & 94.0 & 94.0 & 77.4 & 92.4 & \tb{94.6} & 96.8 \\
    ~IDC & DCIS & 71.8 & 73.0 & 62.4 & 87.6 & \tb{95.0} & 96.2\\
    \midrule
    ~LCIS & DCIS & 45.8 & 45.0 & 25.2 & 81.2 & \tb{93.8} & 96.2 \\
    ~DCIS & LCIS & 73.8 & 76.2 & 75.4 & 89.0 & \tb{96.0} & 97.8 \\
    \midrule
    \midrule
    %\hline
    \multicolumn{2}{c}{\textsc{Average}}  & 64.5 & 65.0 & 55.6 & 71.7 & \tb{94.2} & 96.8 \\
	\bottomrule
    %\hline
    \end{tabular}
    \caption{\tb{Pathology:} Classification accuracy (\%) of different approaches on the pathology reports dataset, including the results of six adaptation scenarios from four different aspects (IDC, ALH, DCIS and LCIS) in breast cancer pathology reports. ``mSDA'' indicates the marginalized denoising autoencoder in \protect\cite{chen2012marginalized} and ``Ours-NA'' corresponds to our model without the adversarial training component. We also include in the last column the in-domain supervised training results of our model as the performance upper bound. Boldface numbers indicate the best accuracy for each testing scenario.}\label{tb:pathology}
\end{table*}

\paragraph{Baselines} We compare against different baselines and variants of our model
% Ours-NA, SVM, mSDA, Source Only, data usage table
\begin{itemize}[leftmargin=10pt,parsep=-4pt,topsep=0pt]

\item \textbf{SVM}: the linear SVM baseline trained on the raw bag-of-words representation of labeled data on source and test it on target.

\item \textbf{mSDA}: marginalized Stacked Denoising Autoencoders \cite{chen2012marginalized}, a domain adaptation algorithm that outperforms both prior deep learning and shallow learning approaches.\footnote{We use the publicly available implementation provided by the authors at \url{http://www.cse.wustl.edu/~mchen/code/mSDA.tar}}

\item \textbf{SourceOnly}: our model trained with only labeled and unlabeled data in the source domain. No target domain data is used.

\item \textbf{Ours-NA}: our model without the adversarial component. To implement this model we simply set the strength of adversarial training to zero.

%\item \textbf{In-Domain}: supervised model trained on in-domain annotation as the performance upper bound.

\end{itemize}
Table \ref{tb:usage} summarizes the usage of labeled and unlabeled data in each domain by our model and different baselines. Note that our model assumes the same set of data as Ours-NA and mSDA methods.


\paragraph{Implementation details}
Following prior work \cite{ganin2014unsupervised}, we gradually increase the adversarial strength $\rho$ and decay the learning rate during training. We also apply batch normalization \cite{ioffe2015batch} on the sentence encoder and dropout with ratio 0.2 on word embeddings and each hidden layer activation. We set the hidden layer size to 150 and pick the transformation regularization weight $\lambda^{t}=0.1$ for the pathology dataset and $\lambda^{t}=10.0$ for the review dataset. 
%We use Adam \cite{kingma2015method} as the optimization method with the default setting suggested by the authors. 