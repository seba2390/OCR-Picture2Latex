%\newcommand\Tstrut{\rule{0pt}{2.6ex}}         % = `top' strut
%\newcommand\Bstrut{\rule[-0.9ex]{0pt}{0pt}}   % = `bottom' strut

\section{Experimental Setup}\label{sec:setup}

\begin{table}[t]
\begin{small}
    \centering
    \begin{tabular}{llcc}
	\toprule
    \multicolumn{2}{l}{\textsc{Dataset}} & \#Labeled & \#Unlabeled \\
    \midrule
    \multirow{4}{*}{\textsc{Pathology}} & DCIS & 23.8k & \multirow{4}{*}{96.6k} \\
     & LCIS & 10.7k &  \\
     & IDC & 22.9k &  \\
     & ALH & 9.2k &  \\
     \midrule
    \multirow{2}{*}{\textsc{Review}} & Hotel & 100k & 100k \\
     & Restaurant & - & 200k \\
    \bottomrule
    \end{tabular}
\end{small}
    \caption{Statistics of the pathology reports dataset and the reviews dataset that we use for training. Our model utilizes both labeled and unlabeled data.
    %The same set of unlabeled reports is used for all different aspects in the pathology reports dataset.
    }\label{tb:data}
%    \vspace{-0.05in}
\end{table}


\paragraph{Pathology dataset}
This dataset contains 96.6k breast pathology reports collected from three hospitals~\cite{Yala:2016a}. A portion of this dataset is manually annotated with 20 categorical values, representing various aspects of breast disease. In our experiments, we focus on four aspects related to carcinomas and atypias:  Ductal Carcinoma In-Situ (DCIS), Lobular Carcinoma In-Situ (LCIS), Invasive Ductal Carcinoma (IDC) and Atypical Lobular Hyperplasia (ALH). Each aspect is annotated using binary labels. We use 500 held out reports as our test set and use the rest of the labeled data as our training set: 23.8k reports for DCIS, 10.7k for LCIS, 22.9k for IDC, and 9.2k for ALH. Table \ref{tb:data} summarizes statistics of the dataset.
%\textbf{YUAN: You have to be more specific here}
%Our training set contains XX, and our test set consists of 500 reports.

\begin{table}[t]
    \centering
    \begin{tabular}{ll}
	\toprule
    \textsc{Aspect} & \textsc{Keywords} \\
%    \midrule
%    DCIS & \specialcell{l}{DCIS, Ductal Carcinoma In-Situ\\Ductal Carcinoma In Situ} \\
%    \midrule
%    LCIS & \specialcell{l}{LCIS, Lobular Carcinoma In-Situ\\Lobular Carcinoma In Situ} \\
    \midrule
    IDC & IDC, Invasive Ductal Carcinoma \\
    %\midrule
    ALH & ALH, Atypical Lobular Hyperplasia \\
    \bottomrule
    \end{tabular}
    \caption{Examples of aspects and their corresponding keywords (case insensitive) in the pathology dataset.}\label{tb:keywords}
    %\vspace{-0.05in}
\end{table}


We explore the adaptation problem from one aspect to another. For example, we want to train a model on annotations of DCIS and apply it on LCIS. 
%Yuan: I don't understand what it means that the same set was used as test for the target aspect and development for the source one.
%Ans: Consider the transfer scenario from DCIS to LCIS. For DCIS, it is used as the development. For LCIS, it is used as the test set. In the other scenario from LCIS to DCIS, DCIS is used as the test and LCIS is used as the development. 
%We use a held out of 500 reports as the %development set (source domain) or the %testing set (target domain).
%Yuan: if the place permits, please add these words in a table. 
%Done.
For each aspect, we use up to three 
common names as a source of supervision for learning the relevance scorer, as illustrated in Table \ref{tb:keywords}. Note that the provided list is by no means  exhaustive. In fact~\newcite{buckley2012feasibility} provide example of 60 different verbalizations of LCIS, not counting negations.

\paragraph{Review dataset} Our second experiment is based on a domain transfer of sentiment classification. For the source domain, we use the hotel review dataset introduced in previous work \cite{wang2010latent,wang2011latent}, and for the target domain, we use the restaurant review dataset from Yelp.\footnote{The restaurant portion of \url{https://www.yelp.com/dataset_challenge}.} Both datasets have ratings on a scale of 1 to 5 stars. 
%Yuan: was this binarization done in prior work?
%Yes, I add a reference.
Following previous work~\cite{blitzer2007biographies}, we label reviews with ratings $>3$ as positive and those with ratings $<3$ as negative, discarding the rest. The hotel dataset includes a total of around 200k reviews collected from TripAdvisor,\footnote{\url{https://www.tripadvisor.com/}} so we split 100k as labeled and the other 100k as unlabeled data. We randomly select 200k restaurant reviews as the unlabeled data in the target domain. Our test set consists of 2k reviews. Table \ref{tb:data} summarizes the statistics of the review dataset.

The hotel reviews naturally have ratings for six aspects, including \emph{value}, \emph{room} quality, \emph{checkin} service, room \emph{service}, \emph{cleanliness} and \emph{location}. We use the first five aspects because the sixth aspect \emph{location} has positive labels for over 95\% of the reviews and thus the trained model will suffer from the lack of negative examples. The restaurant reviews, however, only have single ratings for an \emph{overall} impression. Therefore, we explore the task of adaptation  from each of the five hotel aspects to the restaurant domain. The hotel reviews dataset also provides a total of 280 keywords for different aspects that are generated by the bootstrapping method used in \newcite{wang2010latent}. We use those keywords as supervision for learning the relevance scorer.

\begin{table}[t]
    \centering
    \begin{tabular}{@{~~}lc@{~~}cc@{~~}cc@{~~}}
	\toprule
    \multirow{2}{*}{\textsc{Method}} & \multicolumn{2}{c}{\textsc{Source}} & \multicolumn{2}{c}{\textsc{Target}} & \multirow{2}{*}{\specialcell{c}{Key-\\word}}\\
    \cmidrule(lr){2-3} \cmidrule(l){4-5}
     & Lab. & Unlab. & Lab. & Unlab. \\
    \midrule
    SVM & \Yes & \No & \No & \No & \No\\
    SourceOnly & \Yes & \Yes & \No & \No & \Yes\\
    mSDA & \Yes & \Yes & \No & \Yes & \No\\
    AAN-NA & \Yes & \Yes & \No & \Yes & \Yes\\
    AAN-NR & \Yes & \Yes & \No & \Yes & \No\\
    In-Domain & \No & \No & \Yes & \No & \Yes\\
    AAN-Full & \Yes & \Yes & \No & \Yes & \Yes\\
    \bottomrule
    \end{tabular}
    \caption{Usage of labeled (Lab.), unlabeled (Unlab.) data and keyword rules in each domain by our model and other baseline methods. 
	AAN-* denote our model and its variants.}
	\label{tb:usage}
    %\vspace{-0.05in}
\end{table}


% \begin{table*}[t]
% %\small
%     %\vspace{-0.05in}
%     \centering
%     \begin{tabular}{llcccc@{~~}@{~~}c@{~~}@{~~}c@{~~}@{~~}c}
% 	\toprule
% 	%\hline
% 	\multicolumn{2}{c}{\textsc{Domain}} & \multirow{2}{*}{~SVM~} & ~Source & \multirow{2}{*}{~mSDA~} & \multirow{2}{*}{Ours-NA} & \multirow{2}{*}{~Ours-NR} & \multirow{2}{*}{Ours-Full} & \multirow{2}{*}{In-Domain}\\
%     \textsc{~Source} & \textsc{Target~} & & Only & & & & \\
%     \midrule
%     ~LCIS & DCIS & 45.8 & 25.2 & 45.0 & 81.2 & 50.0 & \tb{93.0} & 96.2 \\
%     ~DCIS & LCIS & 73.8 & 75.4 & 76.2 & 89.0 & 81.2 & \tb{95.2} & 97.8 \\
%     \midrule
%     %\hline
%     ~DCIS & IDC & 94.0 & 77.4 & 94.0 & 92.4 & 93.8 & \tb{95.4} & 96.8 \\
%     ~IDC & DCIS & 71.8 & 62.4 & 73.0 & 87.6 & 81.4 & \tb{94.8} & 96.2\\
% 	\midrule
%     %\hline
%     ~ALH & LCIS & 54.4 & 46.4 & 54.2 & 84.8 & 52.4 & \tb{93.2} & 97.8 \\
%     ~LCIS & ALH & 59.0 & 51.6 & 60.4 & 52.6 & 60.0 & \tb{92.8} & 96.8 \\
%     %\midrule
%     \midrule
%     %\hline
%     \multicolumn{2}{c}{\textsc{Average}}  & 66.5 & 56.4 & 67.1 & 81.3 & 69.8 & \tb{94.1} & 96.9 \\
% 	\bottomrule
%     %\hline
%     \end{tabular}
%     \caption{\tb{Pathology:} Classification accuracy (\%) of different approaches on the pathology reports dataset, including the results of six adaptation scenarios from four different aspects (IDC, ALH, DCIS and LCIS) in breast cancer pathology reports. ``mSDA'' indicates the marginalized denoising autoencoder in \protect\cite{chen2012marginalized}. ``Ours-NA'' and ``Ours-NR'' corresponds to our model without the adversarial training and the aspect-relevance scoring component, respectively. We also include in the last column the in-domain supervised training results of our model as the performance upper bound. Boldface numbers indicate the best accuracy for each testing scenario.}\label{tb:pathology}
% \end{table*}

\begin{table*}[t]
%\small
    %\vspace{-0.05in}
    \centering
    \begin{tabular}{llcccc@{~~}@{~~}c@{~~}@{~~}c@{~~}@{~~}c}
	\toprule
	%\hline
	\multicolumn{2}{c}{\textsc{Domain}} & \multirow{2}{*}{~SVM~} & ~Source & \multirow{2}{*}{~mSDA~} & \multirow{2}{*}{AAN-NA} & \multirow{2}{*}{~AAN-NR} & \multirow{2}{*}{AAN-Full} & \multirow{2}{*}{In-Domain}\\
    \textsc{~Source} & \textsc{Target~} & & Only & & & & \\
    \midrule
    ~LCIS & \multirow{3}{*}{DCIS} & 45.8 & 25.2 & 45.0 & 81.2 & 50.0 & \tb{93.0} & \multirow{3}{*}{96.2} \\
    ~IDC &  & 71.8 & 62.4 & 73.0 & 87.6 & 81.4 & \tb{94.8} & \\
    ~ALH &  & 37.2 & 20.6 & 39.0 & 49.2 & 48.0 & \tb{84.6} & \\
    \midrule
    ~DCIS & \multirow{3}{*}{LCIS} & 73.8 & 75.4 & 76.2 & 89.0 & 81.2 & \tb{95.2} & \multirow{3}{*}{97.8} \\
    ~IDC &  & 71.4 & 66.4 & 71.6 & 84.8 & 52.0 & \tb{85.0} & \\
    ~ALH &  & 54.4 & 46.4 & 54.2 & 84.8 & 52.4 & \tb{93.2} & \\
    \midrule
    %\hline
    ~DCIS & \multirow{3}{*}{IDC} & 94.0 & 77.4 & 94.0 & 92.4 & 93.8 & \tb{95.4} & \multirow{3}{*}{96.8} \\
    ~LCIS &  & 51.6 & 29.5 & 53.2 & 89.6 & 51.2 & \tb{93.8} & \\
    ~ALH &  & 41.0 & 26.8 & 39.2 & 68.0 & 31.6 & \tb{89.6} & \\
	\midrule
    %\hline
    ~DCIS & \multirow{3}{*}{ALH} & 74.6 & 75.0 & 75.0 & 52.6 & 74.2 & \tb{90.4} & \multirow{3}{*}{96.8} \\
    ~LCIS &  & 59.0 & 51.6 & 60.4 & 52.6 & 60.0 & \tb{92.8} &  \\
    ~IDC &  & 67.6 & 66.4 & 68.8 & 52.6 & 69.2 & \tb{87.0} &  \\
    %\midrule
    \midrule
    %\hline
    \multicolumn{2}{c}{\textsc{Average}}  & 61.9 & 51.9 & 62.5 & 71.0 & 64.2 & \tb{91.2} & 96.9 \\
	\bottomrule
    %\hline
    \end{tabular}
    \caption{\tb{Pathology:} Classification accuracy (\%) of different approaches on the pathology reports dataset, including the results of twelve adaptation scenarios from four different aspects (IDC, ALH, DCIS and LCIS) in breast cancer pathology reports. ``mSDA'' indicates the marginalized denoising autoencoder in \protect\cite{chen2012marginalized}. ``AAN-NA'' and ``AAN-NR'' corresponds to our model without the adversarial training and the aspect-relevance scoring component, respectively. We also include in the last column the in-domain supervised training results of our model as the performance upper bound. Boldface numbers indicate the best accuracy for each testing scenario.}\label{tb:pathology}
\end{table*}


%Yuan move the rows in the table based on the order in the paper
%Add in Domain to the list of the described baselines
% Done
%We compare against different baselines and variants of our model. 
\paragraph{Baselines and our method} We first compare against a linear \textbf{SVM} trained on the raw bag-of-words representation of labeled data in source. Second, we compare against our \textbf{SourceOnly} model that assumes no target domain data or keywords. It thus has no adversarial training or target aspect-relevance scoring. Next we compare with marginalized Stacked Denoising Autoencoders (\textbf{mSDA})~\cite{chen2012marginalized}, a domain adaptation algorithm that outperforms both prior deep learning and shallow learning approaches.\footnote{We use the publicly available implementation provided by the authors at \url{http://www.cse.wustl.edu/~mchen/code/mSDA.tar}. We use the hyper-parameters from the authors and their models have more parameters than ours.} 

In the rest part of the paper, we name our method and its variants as \textbf{AAN} (\textbf{A}spect-augmented \textbf{A}dversarial \textbf{N}etworks).
We compare against \textbf{AAN-NA} and \textbf{AAN-NR} that are our model variants without adversarial training and without aspect-relevance scoring respectively. Finally we include supervised models trained on the full set of \textbf{In-Domain} annotations as the performance upper bound.
Table \ref{tb:usage} summarizes the usage of labeled and unlabeled data in each domain as well as keyword rules by our model (\textbf{AAN-Full}) and different baselines. Note that our model assumes the same set of data as the AAN-NA, AAN-NR and mSDA methods.

% \begin{itemize}[leftmargin=10pt,parsep=-4pt,topsep=0pt]

% \item \textbf{SVM}: a linear SVM trained on the raw bag-of-words representation of labeled data on source and test it on target.

% \item \textbf{SourceOnly}: our model trained with only labeled and unlabeled data in the source domain. No target domain data is used.

% \item \textbf{mSDA}: marginalized Stacked Denoising Autoencoders \cite{chen2012marginalized}, a domain adaptation algorithm that outperforms both prior deep learning and shallow learning approaches.\footnote{We utilize the publicly available implementation provided by the authors at \url{http://www.cse.wustl.edu/~mchen/code/mSDA.tar}}



% \item \textbf{Ours-NA}: our model without the adversarial component. To implement this model we simply set the strength of adversarial training to zero.

% \item \textbf{Ours-NR}: our model without the aspect-relevance scorer. We set the relevance score $\tilde{r}$ to a constant 1.0 for every sentence in this model. 

% \item \textbf{In-Domain}: supervised model trained on the full set of in-domain annotations as the performance upper bound.

% \end{itemize}
%Table \ref{tb:usage} summarizes the usage of labeled and unlabeled data in each domain by our model and different baselines. Note that our model assumes the same set of data as Ours-NA, Ours-NR and mSDA methods.


\paragraph{Implementation details}
Following prior work \cite{ganin2014unsupervised}, we gradually increase the adversarial strength $\rho$ and decay the learning rate during training. We also apply batch normalization \cite{ioffe2015batch} on the sentence encoder and apply dropout with ratio 0.2 on word embeddings and each hidden layer activation. We set the hidden layer size to 150 and pick the transformation regularization weight $\lambda^{t}=0.1$ for the pathology dataset and $\lambda^{t}=10.0$ for the review dataset. 
%We use Adam \cite{kingma2015method} as the optimization method with the default setting suggested by the authors. 