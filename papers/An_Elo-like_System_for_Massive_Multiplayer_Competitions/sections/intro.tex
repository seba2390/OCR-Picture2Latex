
\section{Introduction}

% \begin{comment}
% Notes to self:
% - Should we do all-pairs ELO/Glicko per contest as a comparison?
%     - Performances where you do really well is amplified (\#1 is treated as O(n) wins where n is the number of competitors)
% - Can we prove the "aligned incentives" property? I.e. is a participant ever incentivized to lose?
%     - Can show that edits to past performance is monotonic w.r.t. to the edits

% Important points in introduction:
% - Rating systems are ad-hoc, preferences for algorithms based mostly on word-of-mouth and first movers.
% - Different domains use different systems (chess, football, etc --> elo based. team-based online games --> trueskill based or glicko based. coding contests --> ad-hoc, codeforces)
% - In general, the more complicated the rating system, the less adoptation.
% - Players overwhelmingly prefer the ability to understand and predict their own performance.
% - Rating systems are even used outside of sports: "Web-scale bayesian clickthrough
% rate prediction for sponsored search advertising in microsoftâ€™s bing search
% engine" pits ads against each other in a TrueSkill-like fashion.
% - The goal of a rating system is two-fold: firstly to predict the results between players, but also equally as important is to give the players an easily interpretable proxy for their skill.

% Brief literature review:
% - In programming contests the actual scores do not matter much, the ranks give the most discriminative information
% - Talk about area of pairwise comparisons
%     - Paired comparisons assume static "ability" throughout time.
%     - First to look at time-varying paired comparisons was the glicko rating system
% - Glicko and Glicko 2:
%     - Improvement upon Elo in that the deviation was also explicitly modelled
%     - A drift parameter was also added to increase rating volatility over time
%     - This in turn increases the rating uncertainty since the uncertainty is updated from the volatility
%     - Flaws famously exploited in pokemon go, as rating gain is proportional to rating uncertainty
% - MOV Elo techniques:
%     - These not only take in ranking as input, but also "margin of victory". This is appropriate for more fine-grained contests where the same type of contest is played over and over again (e.g. tennis). For programming contests this is not as useful since the contest changes over and over again. Methods not directly applicable in our setting.
%     - Streamlines interface between rating system and contest type and for contest authors. More applicable to other domains. Goodhart's Law.
% - IRT rating system:
%     - Tasks of a contest are modelled explicitly. Allows for more fine grained predictions (such as the number of problems solved during a contest)
% - WHR (Who-Rating-History): a bayesian approach to ratings that takes into account time-varying strength
%     - Three types of rating sys: Static, Incremental, Decayed-History
%     - Our rating system is Incremental. Inferior in the following sense: suppose that there are two players (A & B) that only play against each other. Then A plays against established players. This gives information to B's rating, but incremental systems leave B's rating unchanged.
%     - This rating system doesnt seem to necessarily have a monotonicity property: the addition of a game could decrease the ratings of some of the players in the system who do not directly participate in the game? Players are not typically happy about this; good motivation for our second algorithm property
%     - Limit of large # of players guarantee a connectivity property
% - TrueSkill and TrueSkill 2: 
%     - Models games where medium sized teams of players play against each other
%     - Each user has some skill drawn from a distribution
%     - The performance on any given day is drawn from a distribution centred at the skill
%     - The performance of a team is the sum of team skills
%     - A parameter eps is defined so that performances with difference less than eps is counted as a tie.
%     - Trained through message passing
%     - Used in massive scale in Halo 2
%     - Convergence properties unclear, hard to proof stuff about it
%     - Normal vs logistics support.
%     - TrueSkill 2 takes into account further application-specific statistics
% - Trueskill Through Time:
%     - Similar to WHR in that a new game affects whole history. Does this suffer from non-monotonicity?
% - TrueSkill StPb:
%     - Improved on trueskill by adapting factor graph to handle ties in a principled way
%     - Nodes are added to the factor graph grouping teams that are tied
%     - Instead of summing team performances, non-linear team performance functions are explored
% - TeamSkill and TeamSkill Evolved:
%     - An ensemble learner that leverages TrueSkill, Elo, and Glicko to better predict ratings in the presence of "team chemistry" and non-equal team sizes.
%     - However only works when teams play together semi-consistently
% - Codeforces & leetcode:
%     - Generalized Elo system
%     - Heuristics to fight against inflation
%     - O(n^2) time.
%     - Used in leetcode as well, somewhat of an industry standard
%     - total website use at least 3 million users
% - TopCoder:
%     - More similar to glicko in the sense that each user has a rating deviation
%     - Successfully attacked by Forisek

% The Elo rating system assigns a quantitative measure of skill to each player. For example, an average player may be rated 1500, while a top player may exceed 2500. The scale is arbitrary, but can be used to rank players relative to one another. The odds of any one player winning against another may be estimated from the difference in their ratings. The famous Elo system, and variants such as Glicko \cite{G99}, provide useful formulas for updating ratings of players who compete 1v1 against one another, resulting in a winner and a loser. These algorithms have some nice properties: they are fairly simple, fast, and only modify the ratings of the two participating players.

% Now let's consider a general setting in which a typical contest has much more than two participants. An arbitrary number of players compete simultaneously at a task. Rather than producing just a winner and a loser, the contest ranks its participants 1st place, 2nd, 3rd, and so on. This description fits popular programming competition websites such as Codeforces \cite{Codeforces} and TopCoder \cite{TopCoder}, each of which has tens of thousands of rated members from across the globe. Each publishes its own rating system, but without much theoretical justification to accompany the derivations.

% We build Elo-MMR upon a more rigorous probabilistic model, mirroring the Bayesian development of the Glicko system. In doing so, we inherit its nice properties in practice, resolving known issues with the Codeforces and TopCoder systems. Compared with these systems, it achieves faster convergence and robustness against unusual performances. Issues specific to Codeforces than we improve upon are the overall spread of ratings, inter-division boundary artifacts, and inflation (TODO: cite, and quantify this with tests). An issue specific to TopCoder that we eliminate completely is non-monotonicity: simply put, there are cases in which improving a member's past performance would actually decrease their TopCoder rating, and vice-versa \cite{forivsektheoretical}.

% Furthermore, Elo-MMR retains simplicity and efficiency on par with the other systems. To demonstrate this, I provide a very efficient parallel implementation that can process the entire history of rated competitions hosted by Codeforces on a modest quad-core laptop within 30 minutes. It is implemented entirely within the safe subset of Rust using the Rayon crate; hence, the Rust compiler verifies that it contains no data races.

% This paper is organized as follows: in section 2, we develop a Bayesian model for the competitions. Rating updates are phrased as a latent skill estimation problem, which is naturally divided into two phases. Sections 3 and 4 describe each of these phases in turn, and supplement the derivations with some intuitive interpretations. Then in section 5, we discuss some ways to model uncertainty, in a manner analogous to the Glicko system. In section 6, we discuss some properties of the Elo-MMR system in comparison with the Codeforces and TopCoder systems. Finally, section 7 presents the conclusions of this work.
% \end{comment}

Competitions, in the form of sports, games, and examinations, have been with us since antiquity. Many competitions grade performances along a numerical scale, such as a score on a test or a completion time in a race. In the case of a college admissions exam or a track race, scores are standardized so that a given score on two different occasions carries the same meaning. However, in events that feature novelty, subjectivity, or close interaction, standardization is difficult. The Spartan Races, completed by millions of runners, feature a variety of obstacles placed on hiking trails around the world~\cite{Spartan}. Rock climbing, a sport to be added to the 2020 Olympics, likewise has routes set specifically for each competition. DanceSport, gymnastics, and figure skating competitions have a panel of judges who rank contestants against one another; these subjective scores are known to be noisy~\cite{DanceSport}. Most board games feature considerable inter-player interaction.
In all these cases, scores can only be used to compare and rank participants at the same event. Players, spectators, and contest organizers who are interested in comparing players' skill levels across different competitions will need to aggregate the entire history of such rankings. A strong player, then, is one who consistently wins against weaker players. To quantify skill, we need a \emph{rating system}.
\looseness=-1

Good rating systems are difficult to create, as they must balance several mutually constraining objectives. First and foremost, the rating system must be accurate, in that ratings provide useful predictors of contest outcomes. Second, the ratings must be efficient to compute: in video game applications, rating systems are predominantly used for matchmaking in massively multiplayer online games (such as Halo, CounterStrike, League of Legends, etc.)~\cite{HMG06, MCZ18, Y14}. These games have hundreds of millions of players playing tens of millions of games per day, necessitating certain latency and memory requirements for the rating system~\cite{AL09}. Third, the rating system must align incentives. That is, players should not modify their performance to ``game'' the rating system. Rating systems that can be gamed often create disastrous consequences to player-base, more often than not leading to the loss of players from the game~\cite{pokemongo}. Finally, the ratings provided by the system must be human-interpretable: ratings are typically represented to players as a single number encapsulating their overall skill, and many players want to understand and predict how their performances affect their rating~\cite{G95}.

% Should we consider something about robustness, and about ratings not changing when a player doesn't compete? There are two things that these systems have in common. First, all of the these systems propagate changes forward in time, never backward. This is a strict requirement of the system in most cases, as users prefer their historical ratings to state fixed (even if it's possible to infer more accurate historical ratings from future matches). Such a property will be a requirement of our system as well.

Classically, rating systems were designed for two-player games. The famous Elo system~\cite{E61}, as well as its Bayesian successors Glicko and Glicko-2, have been widely applied to games such as Chess and Go~\cite{G95, G99, G12}. Both Glicko versions model each player's skill as a real random variable that evolves with time according to Brownian motion. Inference is done by entering these variables into the Bradley-Terry model, which predicts probabilities of game outcomes. Glicko-2 refines the Glicko system by adding a rating volatility parameter. Unfortunately, Glicko-2 is known to be flawed in practice, potentially incentivising players to lose. This was most notably exploited in the popular game of Pokemon Go~\cite{pokemongo}; see \Cref{sec:mono} for a discussion of this issue.

The family of Elo-like methods just described only utilize the binary outcome of a match. In settings where a scoring system provides a more fine-grained measure of match performance, Kovalchik~\cite{K20} has shown variants of Elo that are able to take advantage of score information. For competitions consisting of several set tasks, such as academic olympiads, Fori{\v{s}}ek~\cite{forivsektheoretical} developed a model in which each task gives a different ``response'' to the player: the total response then predicts match outcomes. However, such systems are often highly application-dependent and hard to calibrate.
\looseness=-1
%In terms of predictive power, the main disadvantage of Elo-like methods is that they are ``history-less'' and ``static''. That is, previous competitions of an individual are forgotten and summarized solely through a few per-user parameters. Consider an example where two groups of users compete within their own groups and never outside of their group. To compare the two groups, we may choose a representative user from each group to compete against each other. Given the results of this match, it should be possible to calibrate the ratings of all the users in the two groups accordingly. However, Elo-like methods only update the ratings of the two users participating in the match. Coulom~\cite{C08} exploit this observation by retaining the entire match history of a user, allowing for more accurate predictions. 

Though Elo-like systems are widely used in two-player contests, one needn't look far to find competitions that involve much more than two players. Aside from the aforementioned sporting examples, there are video games such as CounterStrike and Halo, as well as academic olympiads: notably, programming contest platforms such as Codeforces, TopCoder, and Kaggle~\cite{Codeforces, TopCoder, Kaggle}. In these applications, the number of contestants can easily reach into the thousands. Some more recent works present interesting methods to deal with competitions between two teams \cite{HLW06, CJ16, LCFHH18, GFYLWTFC20}, but they do not present efficient extensions for settings in which players are sorted into more than two, let alone thousands, of distinct places.%For these cases, a rating system must take in a final ranking of the competitors, and produce a list of rating updates. 

In a many-player ranked competition, it is important to note that the pairwise match outcomes are not independent, as they would be in a series of 1v1 matches. Thus, TrueSkill~\cite{HMG06} and its variants~\cite{NS10, DHMG07, MCZ18} model a player's performance during each contest as a single random variable. The overall rankings are assumed to reveal the total order among these hidden performance variables, with various strategies used to model ties and teams. These TrueSkill algorithms are efficient in practice, successfully rating userbases that number well into the millions (the Halo series, for example, has over 60 million sales since 2001~\cite{Halo}).

The main disadvantage of TrueSkill is its complexity: originally developed by Microsoft for the popular Halo video game, TrueSkill performs approximate belief propagation on a factor graph, which is iterated until convergence~\cite{HMG06}. Aside from being less human-interpretable, this complexity means that, to our knowledge, there are no proofs of key properties such as runtime and incentive alignment. Even when these properties are discussed~\cite{MCZ18}, no rigorous justification is provided. In addition, we are not aware of any work that extends TrueSkill to non-Gaussian performance models, which might be desirable to limit the influence of outlier performances (see \Cref{sec:robust}).

It might be for these reasons that platforms such as Codeforces and TopCoder opted for their own custom rating systems. These systems are not published in academia and do not come with Bayesian justifications. However, they retain the formulaic simplicity of Elo and Glicko, extending them to settings with much more than two players. The Codeforces system includes ad hoc heuristics to distinguish top players, while curbing rampant inflation. TopCoder's formulas are more principled from a statistical perspective; however, it has a volatility parameter similar to Glicko-2, and hence suffers from similar exploits~\cite{forivsektheoretical}. Despite their flaws, these systems have been in place for over a decade, and have more recently gained adoption by additional platforms such as CodeChef and LeetCode~\cite{LeetCode, CodeChef}.

\paragraph{Our contributions} 
In this paper, we describe the Elo-MMR rating system, obtained by a principled approximation of a Bayesian model very similar to TrueSkill. It is fast, embarrassingly parallel, and makes accurate predictions. Most interesting of all, its simplicity allows us to rigorously analyze its properties: the ``MMR'' in the name stands for ``Massive'', ``Monotonic'', and ``Robust''. ``Massive'' means that it supports any number of players with a runtime that scales linearly; ``monotonic'' means that it \emph{aligns incentives} so that a rating-maximizing player always wants to perform well; ``robust'' means that rating changes are bounded, with the bound being smaller for more consistent players than for volatile players. Robustness turns out to be a natural byproduct of accurately modeling performances with heavy-tailed distributions, such as the logistic. TrueSkill is believed to satisfy the first two properties, albeit without proof, but fails robustness. Codeforces only satisfies aligned incentives, and TopCoder only satisfies robustness.

Experimentally, we show that Elo-MMR achieves state-of-the-art performance in terms of both prediction accuracy and runtime. In particular, we process the entire Codeforces database of over 300K rated users and 1000 contests in well under a minute, beating the existing Codeforces system by an order of magnitude while improving upon its accuracy. A difficulty we faced was the scarcity of efficient open-source rating system implementations. In an effort to aid researchers and practitioners alike, we provide open-source implementations of all rating systems, datasets, and additional processing used in our experiments at \url{https://github.com/EbTech/EloR/}.

\paragraph{Organization}
%Since our Bayesian system is constructed from several components, it's not strictly necessary to understand each section in sequential order. 
In \Cref{sec:bayes_model}, we formalize the details of our Bayesian model. We then show how to estimate player skill under this model in \Cref{sec:main-alg}, and develop some intuitions of the resulting formulas. As a further refinement, \Cref{sec:skill-drift} models skill evolutions from players training or atrophying between competitions. This modeling is quite tricky as we choose to retain players' momentum while ensuring it cannot be exploited for incentive-misaligned rating gains. \Cref{sec:properties} proves that the system as a whole satisfies several salient properties, the most critical of which is aligned incentives. Finally, we present experimental evaluations in \Cref{sec:experiments}.