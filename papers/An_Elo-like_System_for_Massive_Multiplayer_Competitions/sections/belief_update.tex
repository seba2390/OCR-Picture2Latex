\subsection{Belief update}
\label{sec:belief}

Having estimated $P_{i,t}$ in the first phase, the second phase is rather simple. Ignoring normalizing constants, \Cref{eq:new-obj} tells us that the pdf of the skill posterior can be obtained as the pointwise product of the pdfs of the skill prior and the performance model. When both factors are differentiable and log-concave, then so is their product. Its maximum is the new rating $\mu_{i,t}$; let's see how to compute it for the same two specializations of our model.

\paragraph{Gaussian skill prior and performance model}
When the skill prior and performance model are Gaussian with known means and variances, multiplying their pdfs yields another known Gaussian. Hence, the posterior is compactly represented by its mean $\mu_{i,t}$, which coincides with the MAP and rating; and its variance $\sigma_{i,t}^2$, which is our \textbf{uncertainty} regarding the player's skill.

\paragraph{Logistic performance model}
When the performance model is non-Gaussian, the multiplication does not simplify so easily. By \Cref{eq:new-obj}, each round contributes an additional factor to the belief distribution. In general, we allow it to consist of a collection of simple log-concave factors, one for each round in which player $i$ has participated. Denote the participation history by
\[\cH_{i,t} := \{k\in\{1,\ldots,t\}:i\in\mathcal P_k\}.\]

Since each player can be considered in isolation, we'll omit the subscript $i$. Specializing to the logistic setting, each $k\in\cH_t$ contributes a logistic factor to the posterior, with mean $p_k$ and variance $\beta_k^2$. We still use a Gaussian initial prior, with mean and variance denoted by $p_0$ and $\beta_0^2$, respectively. Postponing the discussion of skill evolution to \Cref{sec:skill-drift}, for the moment we assume that $S_k=S_0$ for all $k$. The posterior pdf, up to normalization, is then
\begin{align}
&\pi_0(s) \prod_{k\in\cH_t} \Pr(P_k=p_k \mid S_k=s) \nonumber
\\&\propto \exp\left( -\frac{(s-p_0)^2}{2\beta_0^2} \right) \label{eq:posterior}
\prod_{k\in\cH_t} \sech^{2}\left( \frac\pi{\sqrt{12}} \frac{s-p_k} {\beta_k} \right).
\end{align}

Maximizing the posterior density amounts to minimizing its negative logarithm. Up to a constant offset, this is given by
\begin{align*}
L(s) &:= L_2\left(\frac{s-p_0}{\beta_0}\right)
+ \sum_{k\in\cH_t} L_R\left(\frac{s-p_k}{\beta_k}\right),
\\\text{where }L_2(x) &:= \frac 12 x^2\text{ and }
L_R(x) := 2\ln\left(\cosh \frac{\pi x}{\sqrt{12}}\right).
\end{align*}
\begin{equation}
\label{eq:loss}
\text{Thus, }L'(s) = \frac{s-p_0}{\beta_0^2} + \sum_{k\in\cH_t} \frac{\pi}{\beta_k\sqrt{3}} \tanh \frac{(s-p_k)\pi}{\beta_k\sqrt{12}}.
\end{equation}

$L'$ is continuous and strictly increasing in $s$, so its zero is unique: it is the MAP $\mu_t$. Similar to what we did in the first phase, we can solve for $\mu_t$ with either binary search or Newton's method.

We pause to make an important observation. From \Cref{eq:loss}, the rating carries a rather intuitive interpretation: Gaussian factors in $L$ become $L_2$ penalty terms, whereas logistic factors take on a more interesting form as $L_R$ terms. From \Cref{fig:l2-lr-plot}, we see that the $L_R$ term behaves quadratically near the origin, but linearly at the extremities, effectively interpolating between $L_2$ and $L_1$ over a scale of magnitude $\beta_k$ 
%\aram{cite literature to justify this claim, and the next one? It would take more space to derive it ourselves}.

It is well-known that minimizing a sum of $L_2$ terms pushes the argument towards a weighted mean, while minimizing a sum of $L_1$ terms pushes the argument towards a weighted median. With $L_R$ terms, the net effect is that $\mu_t$ acts like a robust average of the historical performances $p_k$. Specifically, one can check that
\[\mu_t = \frac{\sum_k w_k p_k}{\sum_k w_k}, \text{ where } w_0 := \frac{1}{\beta_0^2} \text{ and }\]
\begin{equation}
\label{eq:average}
w_k := \frac{\pi}{(\mu_t-p_k)\beta_k\sqrt{3}}\tanh\frac{(\mu_t-p_k)\pi}{\beta_k\sqrt{12}} \text{ for }k\in\cH_t.
\end{equation}

$w_k$ is close to $1/\beta_k^2$ for typical performances, but can be up to $\pi^2/6$ times more as $|\mu_t-p_k| \rightarrow 0$, or vanish as $|\mu_t-p_k| \rightarrow\infty$. This feature is due to the thicker tails of the logistic distribution, as compared to the Gaussian, resulting in an algorithm that resists drastic rating changes in the presence of a few unusually good or bad performances. We'll formally state this \emph{robustness} property in \Cref{thm:robust}.

%Empirically, contest performances have indeed been seen to have thick tails, more like the logistic than the Gaussian (TODO citation).

\paragraph{Estimating skill uncertainty} While there is no easy way to compute the variance of a posterior in the form of \Cref{eq:posterior}, it will be useful to have some estimate $\sigma_t^2$ of uncertainty. There is a simple formula in the case where all factors are Gaussian. Since moment-matched logistic and normal distributions are relatively close (c.f. \Cref{fig:l2-lr-plot}), we apply the same formula:
\begin{equation}
\label{eq:variance}
\frac{1}{\sigma_t^2} := \sum_{k\in\{0\}\cup\cH_t}\frac{1}{\beta_k^2}.
\end{equation}