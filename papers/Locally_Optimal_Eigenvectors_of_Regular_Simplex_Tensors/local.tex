 \documentclass{elsarticle}
\usepackage{geometry}
%  \usepackage{lineno,hyperref}
%\geometry{a4paper,left=3cm,right=3cm,top=3cm,bottom=3cm}
\usepackage{bm}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amssymb}
\usepackage{tikz}
\usepackage{mathtools}

%  \usepackage{cite}  %  自带cite包？

\usepackage{float}
\usepackage{tabularx}

\usepackage{caption,subcaption}
\usepackage{color}

\usepackage{tikz}
\newtheorem{theorem}{\textbf{ Theorem} }
\newtheorem{lemma}{\textbf{Lemma } }
\newtheorem{definition}{\textbf{Definition} }
\newtheorem{corollary}{\textbf{Corollary} }
\newtheorem{remark}{\textbf{Remark} }

\newtheorem{property}{\textbf{Property} }
\newtheorem{conjecture}{\textbf{Conjecture} }
\begin{document}
	
	\title{
		Locally  Optimal   Eigenvectors of  Regular  Simplex Tensors
		\tnoteref{mytitlenote}
	}
	\tnotetext[mytitlenote]{
		This work was partially supported by National Natural Science Fund of China (62271090), Chongqing Natural Science Fund (cstc2021jcyj-jqX0023), National Key R\&D Program of China (2021YFB3100800), CCF Hikvision Open Fund (CCF-HIKVISION OF 20210002), CAAI-Huawei MindSpore Open Fund, and Beijing Academy of Artificial Intelligence (BAAI).
	}
	
	\author[add1]{Lei  Wang\corref{coraut}}
		\cortext[coraut]{Corresponding author}
	\ead{wanglei179@mails.ucas.ac.cn}
	\author[add2,add3,add4]{Xiurui Geng}
		\author[add1]{Lei  Zhang}

	
	

		\address[add1]{	the School of Microelectronics and Communication
	Engineering, Chongqing University, Chongqing 400044, China}
	\address[add2]{Aerospace Information Research Institute, Chinese Academy of Sciences, Beijing 100094, China}
	\address[add3]{University of the Chinese Academy of Sciences, Beijing 100049, China} 
	\address[add4]{Key Laboratory of  Technology in Geo-Spatial Information Processing and Application System, Chinese Academy of Science, Beijing 100190, China}


%
%\input{deshared}
%
%
%\ifpdf
%\hypersetup{
%	pdftitle={	Locally  Optimal   Eigenvectors of  Regular  Simplex Tensors},
%	pdfauthor={ 
%		Lei Wang, Xiurui Geng, Lei Zhang
%	}
%}
%\fi


%\begin{document}
	%\maketitle
	
	
\begin{abstract}
%The  concept of   tensor  eigenpairs  is  a higher-order  generalization  for  the matrix  one,  which  has  received  more attentions  in  past  decades. 
Identifying  locally     optimal 
solutions 
is an important issue given an   optimization  model.
In this paper, 
we focus on  a  special class of  symmetric tensors  termed  regular simplex tensors,  which is  
   a newly-emerging concept, 
%   and  is constructed by  
%equiangular tight frame  of $n$ vectors in 
%$n-1$-dimensional space, 
and 
 investigate    its  local optimality  of the related  constrained  nonconvex  optimization model.
 This   is proceeded by  checking the first-order and  second-order necessary condition sequentially.  
Some  interesting directions concerning the regular simplex  tensors, 
including  the  
robust eigenpairs checking   and other  potential issues, 
are  discussed in the end   for    future work.

% and    the  robustness of eigenpairs was investigated. 
%In  the end of the literature,  a  conjecture  was    claimed  that the robust eigenvectors of a regular simplex tensor  are precisely
%the
%vectors in the frame.
%Later,  
%the case of $n=3$ was analyzed. 
%In this paper, 
%we  proceed further   
%and demonstrate a weaker conclusion  of the conjecture
%---
%the
%vectors in the frame
%are indeed the robust ones,  which 
%can be  attributed to checking the   second-order  local optimality  of the related  constrained  optimization model. 
%Some  interesting directions, 
%including  the  unanswered  uniqueness  concerning 
%whether  
%the
%vectors in the frame
%are only   robust  ones  and other  potential issues, 
%are  discussed in the end   for    future work.

% and    the  robustness of eigenpairs was investigated. 
%In  the end of the literature,  a  conjecture  was    claimed  that the robust eigenvectors of a regular simplex tensor  are precisely
% the
%vectors in the frame.
%Later,  
%the case of $n=3$ was analyzed. 
%In this paper, 
%we  proceed further   
%and demonstrate a weaker conclusion  of the conjecture
%---
% the
%vectors in the frame
%are indeed the robust ones,  which 
%can be  attributed to checking the   second-order  local optimality  of the related  constrained  optimization model. 
%Some  interesting directions, 
%including  the  unanswered  uniqueness  concerning 
%whether  
% the
%vectors in the frame
%are only   robust  ones  and other  potential issues, 
%are  discussed in the end   for    future work.

\end{abstract}

\begin{keyword}
	Regular simplex tensor \sep  eigenpairs \sep   
	 second-order   necessary condition \sep    constrained optimization.
\end{keyword}

\maketitle
\textbf{AMS subject classifications. 15A69,	90C26}
% REQUIRED
%MSC：
%	15A69,	90C26




%Among this field,  the tensor power method  is  one  of  the widely   used  ones  to  obtain  eigenpairs,    and  one  key   problem  is  to  determine whether the  obtained  eigenpairs  are robust  or  not. 
%Previous  works have  focused on the  orthogonal  decomposable  tensors,  and  analyze  its  robustness. 
%Recently,  some  researchers  further  generalize   the case  for   the  robustness  of  equiangular   tensor,  which  is  constructed by  
% equiangular set of $r$ vectors in $n$-dimensional space. 

%In this material,    the  detailed  proof  for  the  presented  Theorem   1  in  the  manuscript  are    provided.
\section{Introduction }
Matrix  analysis and   applications  have been researched for more than 
150 years,
and with very  fruitful  achievements in  both  theoretical  results and  practical applications.
As  the high-order  generalization of  matrix,  tensor   analysis and  applications have  been  given   more   attentions in  recent  years \cite{kolda,2007Numerical,hosvd,tensorrank_qi,TensorPCA,RobustTensorCompletion,TensorDiagonalization,TensorApproximation}. 


 Naturally, 
 the concept of  eigen-analysis for  second-order matrix  
has also    been    extended  into    higher-order  tensors.
Different  from  the matrix  case,    there are several  definitions for eigenpairs (a pair refers to the eigenvalue and it   corresponding eigenvector)  of symmetric tensors, such as   D-eigenpairs \cite{Deigen}, H-eigenpairs  \cite{qi},  E-eigenpairs  \cite{lim,qi},   etc.  
In this paper,  we mainly  focus  on  one of them ---  E-eigenpairs, whose eigenvector is  subject to    unit length.  
Without causing  ambiguities, 
in the whole context, 
we use  the  term eigenpair  to refer to the focused  E-eigenpairs  for simplicity. 
Such a concept has been widely exploited  in  theory \cite{SHOPM,ASHOPM,NCM,OTD,Cuicf,hm,Zglobal}  and also  
made 
numerous  applications   in  many disciplines, such as latent  variable mode \cite{Hsu}, hyperspectral  image processing \cite{PSA, NPSA,MSDP}, 
signal processing \cite{tensor_bss1},  hypergraph theory \cite{hypergraph,hypergraph3} and so on. 


%Throughout this paper,   without  causing   ambiguities,   eigenpairs   will   always  refer  to   E-eigenpairs   by  default.

However,  it  has  also  been   shown  that  most of tensor  problems  are NP hard  \cite{NP-hard},  including computing all  eigenpairs of tensors.
To our best knowledge,  
so far,  there are  only  two  algorithms that  can  obtain  all  eigenpairs of tensor \cite{Cuicf,hm}.
However, 
both of them  still  suffer from high   computational  complexity for large-scale tensor. 
Therefore, most of  previous works aim
 to obtain the maximized or minimized one eigenpairs, and different optimization algorithms were developed, such as \cite{SHOPM,ASHOPM,NCM}.


In  this sense, previous works have 
turned to  paying  attentions to some   special  classes  of  symmetric  tensors,  
and analyzed the  eigen-problem. 
One   widely researched   type   with fruitful results 
is termed 
  orthogonally  decomposable  (odeco) tensors \cite{odst,OTD,Hsu,sr1,cunmu,GloballyConvergent},  
  which 
  is  
  an natural generalization of orthogonal matrix decomposition.
It has been shown that
concerning the odeco tensors, 
the number of 
its 
all  eigenpairs can reach at the upper bound\cite{odst},
and the locally maximized eigenpairs correspond to the 
orthogonal basis. 



  
  
%We   first  prove  that   the   eigenpairs of  such  a  type of  tensors   can  be  enumerated  in  a linear-combination   way     using     several  basic  eigenvectors,  and  the  number  of eigenpairs  can   be  uniquely   determined  by  the  order  and  rank  of   the  symmetric  tensor.
%In  addition,     the   
%local  optimality  of   each  eigenpair   is   also   analyzed  by checking  the  second-order  necessary   condition.

However, 
unfortunately, 
most of symmetric tensors cannot be 
orthogonally  decomposable. 
In  this sense, 
recently, 
some researchers further  extended 
the case of  odeco tensors into 
a  more generalized one, where the  symmetric  tensor  is  generated  by 
the set of some  equiangular  set (ES) or equiangular tight frame (ETF), which contains 
 of $r$ vectors in 
$n-1$-dimensional space \cite{RobustEigen}. 
For  example, 
the case of  $r=n-1$  serves as a  special one of tight frame,  which  
forms a  standard orthonormal  basis and  corresponds to 
the  odeco tensors.
When $r=n$,  the 
frame is termed the regular simplex one, 
and the  generated tensor is thus called regular simplex tensor. 
In  \cite{RobustEigen}, 
they  discussed that under what  condition 
the eigenvectors will be a robust one.
Later, 
several researchers 
investigated 
the real eigen-structure 
of  all eigenpairs
by  analyzing the first-order 
necessary condition \cite{teneigenstructure}.

However, 
the  local 
optimality of regular simplex tensor,
which is determined by the second-order  necessary condition, 
has not been answered. 
%Compared to  the task of analyzing the eigen-structure of all eigenpairs,
%determining the 
%locally  
%optimal  
%eigenpairs is relatively 
In this paper, 
different from 
the previous works that focused on the original  optimization model, 
we further 
focused on this issue 
and 
equivalently  developed 
a  new reformulated model  to analyze 
its 
local optimality
by checking the first-order and second-order necessary condition 
sequentially. 
 

%However, 
%they only provided the robustness proof for the case of  $n=3$,
%and the experimental results for  the case of  $n=4$.
%For higher-dimensional cases, it still remains  an open problem. 
%In this paper, in the basis of the only two works \cite{RobustEigen,teneigenstructure},
%we proceed further concerning this  conjecture 
%and 
%the main contributions of this paper are 
%concluded as  follows:
%
%1. 
%Concerning  the  conjecture, we prove a weaker conclusion,  
%which demonstrate that 
% the
%vectors in the frame
%are indeed the robust ones. 
%Compared  to the previous work \cite{teneigenstructure}
%that  only provided the robustness proof for the case of  $n=3$, 
%this  work proceeds one  step further
%towards the conjecture.
%
%
%2. To complete   our proof, 
%the original  optimization  model   concerning  the eigenpairs of   regular  simplex  tensor  is firstly 
%reformulated as  an equivalent one with  separable structure.
%Compared with the original one, 
%such a new model 
%fully utilizes the property of regular simplex 
%frame and enables us to 
%intuitively 
%derive the  structure  of the solutions of the optimization model. 
%
%
%3. 
%The original  criterion for checking  the  robustness of eigenpair is  a  little 
%complicated  for analysis. 
%In  this  paper,  
%we also develop a new   criterion, serving 
%as a   sufficient  (but still unnecessary) 
%condition  
%to examine 
%the robustness of 
%eigenpairs,
%which 
%can be  attributed to checking the   second-order  local optimality  of the related  constrained  optimization model. 
%Combining with  the contribution 2  will 
%serve as an efficient tool  for our aim of robust eigenpairs identification.




The  rest of the paper
is organized as  follows.
In Section  \ref{pre},
Some preliminaries related to the subject are provided,  including 
the  definition  for  
tensor  eigenpairs,  and  the  focused regular simplex tensor. 
In  Section  \ref{reformulated},
the 
optimization model  for   regular simplex  tensor eigenpairs
 is  
 reformulated as a equivalent   one with better separable  structure, which is beneficial 
 for  analyzing the eigen-structure of tensor eigenpairs.
 Then, in Section  \ref{station} and \ref{locallymaximized},
 the structure for all  stationary 
 and  
 the locally optimal  solutions  of 
 the newly reformulated model 
 is investigated 
 by checking the first-order and second-order necessary condition   sequentially.
 Some future works are discussed in Section  \ref{futurework}.
 

\section{Background}\label{pre}
In this part, 
some 
background  preliminaries   related to the subject are provided,  including 
the used notations, 
the  definition  for  
tensor  eigenpairs,  and  the  focused regular simplex tensor. 
The  details are as  follows.

\subsection{Preliminaries}\label{notation}
We    introduce  some  necessary   notations, definitions   and  lemmas used  in   this  article. 
In  this  material,  as adopted in  many tensor-related works \cite{kolda,TensorPCA,9521829},  high-order tensors are denoted
in
boldface Euler script letters, e.g.,
$\mathcal  A $.
Matrices are denoted  in   boldface capital letters, e.g., $\mathbf  A $; vectors are denoted in  
boldface lowercase letters, e.g.,  $\mathbf  a $.
Sets and subsets are denoted  in blackboard bold  capital letters, e.g.,  $\mathbb  A $.




A  $d$th-order tensor is denoted 
$\mathcal A \in \mathbb {R}^{I_1 \times I_2  \times \dots \times I_{d} }$,
where 
$d$
is the order   of
$\mathcal A $, and 
$ I_j $ ($  j \in \{ 1,2,\dots,d \}$)  is  the  dimension  of  
$j$th-mode.
The element of $\mathcal A$,    which  is  indexed  by integer tuples $(i_1,i_2,\dots,i_d) $, is denoted 
($a_{i_1,i_2,\dots,i_d})_
{1 \le i_1 \le I_1, 
	%1 \le i_2 \le I_2,
	\dots, 
	1 \le i_d \le I_d}
$. 
%A   tensor is called    symmetric if its elements remain invariant under any permutation of 
%$(i_1,i_2,\dots,i_d) $.\cite{kolda}.  
%A   symmetric  tensor of order
%$ m $
%and dimension
%$ n$
%is denoted 
%$\mathcal S \in \mathbb R^{n \times n  \times \dots \times n } $,
%whose element is
%($s_{i_1,i_2,\dots,i_m})_
%{1 \le i_1 \le n, 
%	1 \le i_2 \le n,
%	\dots, 
%	1 \le i_m \le n}
%$.
%Given   a   vector  
%$  \mathbf u
%=(u_{1}, u_2, \dots, u_{n})^{\mathrm T} \in  \mathbb  C^{n \times 1} $, 
%and  an $n \times n$ matrix $\mathbf A$,
%the bilinear form
%$ \mathbf u^{\mathrm T}\mathbf A\mathbf u $
%can be calculated by
%$  \mathbf u^{\mathrm T}\mathbf A\mathbf u
%=
%\sum_{i,j=1}^{n} 
%a_{ij}u_{i} u_{j}.$
%Naturally, by extending  it into the multilinear form,  
A   tensor is called    symmetric if its elements remain invariant under any permutation of 
the  indices\cite{kolda}. 
Let    $  T^{m}(\mathbb R^{n}) $ denotes   the  space  of  all  such  real  symmetric    tensors.
Given a $m$th-order  $n$-dimensional symmetric  tensor  $\mathcal S $ and  a  vector $ \mathbf u \in \mathbb {R}^{n \times 1}$, we have
$ 
\mathcal S \mathbf u^{m} =
\sum\limits_{i_1,i_2,\dots,i_m=1}^{n} 
s_{i_1,i_2,\dots,i_m}  u_{i_1} \dots   u_{i_{m}}
$, 
and  $   \mathcal S \mathbf u^{m-1}   $   denotes   a    $n$-dimensional
column   
vector,  whose  $j$th  element   is    
$
(\mathcal S \mathbf u^{m-1})_{j} =
\sum\limits_{i_2,\dots,i_m =1}^{n} 
s_{j,i_2,\dots,i_m}  u_{i_2} \dots   u_{i_{m}}
$\cite{Cuicf}.
Furthermore,   $   \mathcal S \mathbf u^{m-2}   $   is  an    $n  \times  n $  matrix,    whose  $(i, j )$th  element   is  
$
(\mathcal S \mathbf u^{m-2})_{i, j} =
\sum\limits_{i_3,\dots,i_m =1}^{n} 
s_{i,j,i_3,\dots,i_m}  u_{i_3} \dots   u_{i_{m}}.
$

$\mathbf  1_{n}$   denotes   a  $ n  \times  1$ column vector,  and  $\mathbf  I_{n}$   denotes   a  $ n  \times  n$  identity  matrix.
$\mathbf  J_{m \times p}$    and  $\mathbf  J_{m}$    is   a   $ m  \times  p$  matrix  and   a  $ m  \times  m$    square    matrix,     with all  elements equal to 1,  respectively. 
It holds   that\cite{zhang2017matrix}
\begin{equation}\label{Jproperty}
\mathbf  J_{m \times p} = \mathbf  1_{m}  \mathbf  1_{p}^{\mathrm {T}}. 
\end{equation}
Similarly,  $\mathbf  0_{n}$,  $\mathbf  O_{m \times p}$    and  $\mathbf  O_{m}$   are   matrices   with all  elements equal to 0.

$\triangledown$ is  the  gradient  operator.
$ \rm Null( \mathbf A) $ 
denotes the null space  of $\mathbf A$. 
$ \mathbf  P_{\mathbf  A }^{\bot} $ is  the  orthogonal   cpmplement  operator  of  $\mathbf A$.
%The  spectral  radius  of    a  matrix  $\mathbf A$  is  the   maximum  value of  the  absolute  value  of  the   eigenvalue of 
%$\mathbf A$,  denoted  by 
%$ \rho (\mathbf A) = max  \vert  \sigma(\mathbf A) \vert$.
$``diag(\mathbf u)" $ is an operator which maps the vector  $  \mathbf u  \in  \mathbb R^{n \times 1} $   to 
a $ n \times n $ 
diagonal matrix with its  diagonal  elements to be that of $  \mathbf u$.
%In this article, we adopt  
$``$ $\circledast$ $"$ is an  elementwise   multiplication  operation, where $   (\mathbf  {A} \circledast  \mathbf  {B})_{ij} =
\mathbf  A_{ij}\mathbf  B_{ij}$.
  For simplicity,  we use
$ \mathbf A ^{\circledast^{p}}$
and
$ \mathbf a ^{\circledast^{p}}$
to denote the $p$-times  elementwise   multiplication  of the matrix
$ \mathbf A  $ and the vector $ \mathbf a $,  respectively.


%\begin{definition}[\textbf{Whiten operator}]\label{whitenoperator}
%	Given the data that 	$\mathbf R=[\mathbf r_1,\mathbf r_2,\dots, \mathbf r_{N}] \in \mathbb {R}^{L \times N}$, 
%	denote   
%	$ \mathbf m = \frac  {1}{N}  \mathbf R   \mathbf   1_{N}  $   
%	is the mean vector of  
%	$  \mathbf R$.
%	$    \mathbf K  =   \frac {1} {N}  [  \sum_{i=1}^{N}  (\mathbf r_{i} -   \mathbf  m)  (\mathbf r_{i} -   \mathbf  m)^{\mathrm {T}}    ]   $      is  the  covariance  matrix of
%	$ \mathbf  R$, whose  eigen-decomposition  is  
%	$  \mathbf K =\mathbf E^{\mathrm {T}}  \mathbf D  \mathbf E $, where   $\mathbf E $ represents the eigenvector matrix of  covariance matrix of  $  \mathbf R$,  and $ \mathbf D $ is the corresponding eigenvalue diagonal matrix.   
%	The  whitening  operator  for the data $\mathbf R$ can be   constructed by 
%	$ \mathbf F=\mathbf E\mathbf D^{-\frac {1} {2}  }  $.
%\end{definition}
\begin{definition}[\textbf{outer product}]
	\label{outerprod}
	Given   $m$  vectors 
	$ \mathbf a^{(i) } \in \mathbb {R}^{I_i \times 1}$ 
	($i=1,2, \dots, m$),
	their   outer  product   
	$ \mathbf a^{(1) }
	\circ
	\mathbf a^{(2) }
	\circ  \dots
	\circ
	\mathbf a^{(m) } 
	$  
	is   a    $ m$th-order  tensor denoted $   \mathcal A$,  with  a size  of  
	$ I_1 \times I_2  \times \dots \times I_m  $.  
	And  its    element is    the  product  of    the  corresponding  vectors'   elements, i.e., 
	$ 
	a_{i_1,i_2,\dots,i_m}
	= 
	\mathbf a^{(1) }_{i_1}
	\mathbf a^{(2) }_{i_2}
	\dots
	\mathbf a^{(d) }_{i_m} 
	.
	$
	When 
	$  \mathbf a^{(1) }
	=  
	\mathbf a^{(2) }
	=   \dots
	= 
	\mathbf a^{(m) }
	=\mathbf a $ $(I_1 =  I_2  = \dots = I_m =I)$,  we use  the  notation 
	$ \mathcal A =  \mathbf a^{\circ m}$  for  simplicity,  where 
	$   \mathcal A $ is  a  symmetric  tensor  of  order  $m$  and  dimension  $ I$.  
\end{definition}

\begin{definition}[\textbf{direct  sum}]
	The  direct  sum  of   a   $ n \times n$  matrix  $\mathbf  {A}  $ and  a   $m \times m$ matrix   $\mathbf  {B} $  is a  matrix  with a  size  of  $(n+m) \times(n+m)$,  denoted    $\mathbf  A \oplus \mathbf  B$, which  follows: 
	
	\begin{equation}
	\mathbf  {A} \oplus \mathbf  {B}=
	\left[\begin{array}{cc}
	\mathbf {A} & \mathbf {O}_{n \times m} \\
	\mathbf {O}_{m \times n} & \mathbf {B}
	\end{array}\right] .
	\end{equation}
\end{definition}



\input{HOS_tri}
\input{Even3}

%\begin{lemma}\label{Theorem_structure_altitude}
%	if  the direction of $\mathbf   w  $ is the same as that of  the  altitude of the simplex  from the first vertex $\mathbf r_{i}$ ($i=1,2, \dots, n$)  to the  sub-simplex constructed by the left $n-1$ ones, 
%	%it can be concluded by (\ref{propto_relation})  that 
%	$  \mathbf u  $ is with the form of  
%	\begin{equation}\label{altitudeform}
%	\mathbf   u   = 
%	[\gamma ,\gamma,\dots, 
%	\gamma,
%	\underbrace{ \eta }_{i}, 
%	\gamma, \dots, \gamma]^{\mathrm  T},
%	\end{equation}
%	where except for the $i$-th  element, the left $n-1$ ones are the same.
%	and permutation matrix
%\end{lemma}
%
%\begin{proof}
%	(\ref{udenote}) can be  further rewritten as 
%	\begin{align}\label{uandwlink}
%	\mathbf   u 
%	\nonumber 
%	&= \frac  {1}{  \sqrt {n}  }   \tilde {\mathbf R}^{\mathrm T} \tilde {\mathbf v}
%	=
%	\frac  {1}{  \sqrt {n}  }  \mathbf R^{\mathrm {T}} \mathbf {F} \tilde {\mathbf v} 
%	\\  \nonumber 
%	&	=
%	\frac  {1}{  \Vert  \mathbf   v  \Vert_{2} \sqrt {n}  }  \mathbf R^{\mathrm {T}} \mathbf {F} \mathbf v 
%	=
%	\frac  {1}{  \Vert  \mathbf   v  \Vert_{2} \sqrt {n}  }  \mathbf R^{\mathrm {T}}  \mathbf w	
%	\\   
%	&=
%	\frac  {1}{  \Vert  \mathbf   v  \Vert_{2} \sqrt {n}  }  \mathbf R^{\mathrm {T}}\mathbf w  .
%	\end{align}
%	%\begin{align}\label{uandwlink}
%	%\mathbf   u 
%	%\nonumber 
%	%&= \frac  {1}{  \sqrt {n}  }   \tilde {\mathbf R}^{\mathrm T} \tilde {\mathbf v}
%	%=
%	%\frac  {1}{  \sqrt {n}  }  (\mathbf R-\mathbf m)^{\mathrm {T}} \mathbf {F} \tilde {\mathbf v} 
%	%\\  \nonumber 
%	%&	=
%	%\frac  {1}{  \Vert  \mathbf   v  \Vert_{2} \sqrt {n}  }  (\mathbf R-\mathbf m)^{\mathrm {T}} \mathbf {F} \mathbf v 
%	%	=
%	%\frac  {1}{  \Vert  \mathbf   v  \Vert_{2} \sqrt {n}  }  (\mathbf R-\mathbf m)^{\mathrm {T}}  \mathbf w	
%	%\\   
%	%&=
%	%\frac  {1}{  \Vert  \mathbf   v  \Vert_{2} \sqrt {n}  }  (\mathbf R^{\mathrm {T}}\mathbf w  -\mathbf m^{\mathrm {T}}  \mathbf w ) .
%	%\end{align}
%	
%	
%	It can be seen from  (\ref{uandwlink}) that the direction of $\mathbf   u$ is parallel to that of $\mathbf R^{\mathrm {T}}\mathbf w$. 
%	Thus, the conclusion in  Lemma \ref{Theorem_structure_altitude}  can be deduced. This completes the proof. 
%	%$\blacksquare$
%\end{proof}


\section{Future Work}\label{futurework}
The regular simplex tensor
is a  newly-emerging 
concept and has received  some  attentions since its proposal.   
In  this part, we  will  discuss  some  interesting issues that are worth studying in the future.



1. 
One widely researched issue concerning the tensor eigenpair concept
is 
to identify whether the  eigenpairs  obtained by the tensor power method  is  robust or not. 
The detailed definition for robust eigenpairs can refer to Theorem 3.2 of Ref  \cite{RobustEigen}.
Concerning this subject, it has been discussed in  Ref  \cite{RobustEigen}
but is not comprehensively 
addressed,
which was claimed as the following conjecture:
	\begin{conjecture}[Conjecture 4.7 in  Ref  \cite{RobustEigen} ] \label{conjecturesimplex}
	The robust eigenvectors of a regular simplex tensor   are precisely the
	vectors in the frame.
\end{conjecture}
A rigorous and  complete   theoretical proof  is  one of the  following  tasks to be paid attentions. 
%For this purpose, 
%considering the difficulty of calculating the Jacobian matrix as  
%discussed in the above context, 
%more efficient strategy  for  checking the  robustness of eigenpairs 
%needs to  be developed.
%The  main difficulty  lies in the calculation of the Jacobian matrix. 

%2. Regardless of  the  robustness of   eigenpairs, 
%the  issue of  identifying the  local  optimality of each eigenpair   itself 
%is also an interesting  problem  in  the  field of  optimization  theory.
%In  this  paper,  we  actually  only  analyze the case for the vectors in  the frame.
%What  is  the  local  optimality of the other eigenpairs  as  listed  in  Lemma \ref{Theorem_structureofall}?
%We have  deduced the  eigen-distribution  of  the  corresponding  matrix 
%$\mathbf M$  for the odd $m$ case 
%and the  first case   (\ref{u_classifyeven1}) of the even $m$ case.
%However, 
% for  the  second  case   (\ref{u_classifyeven2}) of the even $m$ case, 
% analyzing    eigenvalues  of  the  corresponding  matrix 
% $\mathbf M$ 
% is relatively  tough,  
%since  there is no   explicit expression for the variables  $c,d,e$
% in (\ref{u_classifyeven2}).
%It will be paid attentions in the future.

2. Consider  the  following  more  generalized   construction of   regular simplex tensor
	\begin{equation}%\label{simplextensor}
\mathcal{S}:=\sum_{i=1}^{n} 
\lambda_{k} \mathbf{w}_{i}^{\circ m}
\end{equation}
where the  weight  factors $\lambda_{k}$  is   included  in  each  term,  which we 
would like to term as the weighted regular simplex tensor.
In this  paper,  
we only consider  one  special case where all  $\lambda_{k} $  are  equal to 1, as defined in 
(\ref{simplextensor}).
The  above weighted version could be  more generalized.
What is the eigen-structure of  its  eigenpairs?
Whether the   conclusion  in conjecture  \ref{conjecturesimplex}  
can be  extended into the generalized  weighted regular simplex tensor?
These  remain    open problems.

3. The other conjectures  claimed in the original reference \cite{RobustEigen}
are also worth investigating  in the next  stage. 
One can refer to Conjecture 4.8 and 5.2 for details. 


\section{Appendix}

In this  appendix part,   
we will provide some important theorems   and lemmas 
 used in our proof.
 In addition,  
the detailed  explanations for the second-order necessary  condition is also presented.

\begin{lemma}(Ref \cite{zhang2017matrix})\label{direct_eig}
	Suppose  that 
	$ (\lambda_{i},  \mathbf u_{i})$  
	($i=1,2,\dots,n$),
	$  (\sigma_{j},  \mathbf v_{j})$  
	($j=1,2,\dots,m$)  
	are  eigenpairs of  
	$ \mathbf A$ and $ \mathbf B$, respectively. 
	The ($n+m$)  eigenpairs   of  
	$ \mathbf  {A} \oplus \mathbf  {B}$  
	is  
	$ (\lambda_{i},  
	\begin{bmatrix}
	\mathbf u_{i}   \\
	\mathbf 0_{m}    
	\end{bmatrix}	)$, 
	$ (\sigma_{j},  
	\begin{bmatrix}
	\mathbf 0_{n}   \\
	\mathbf v_{j}
	\end{bmatrix}	)$ 	($i=1,2,\dots,n$, $j=1,2,\dots,m$).
\end{lemma}










%\setcounter{theorem}{2}
\begin{lemma}(Ref \cite{zhang2017matrix})\label{detblocklemma}
	Assume that   a  matrix  with a  size  of  $(n+m) \times(n+m)$,   
	can be  further  divided  into      $ n \times n$  and     $m \times m$   two blocks,
	its  determinant  can be calculated by 
	\begin{equation}\label{detblock}
	det \left[\begin{array}{ll}
	\mathbf {A} & \mathbf {B} \\
	\mathbf {C} & \mathbf {D}
	\end{array}\right]
	=det  (\mathbf {D})
	det\left(\mathbf {A}-\mathbf {B} \mathbf {D}^{-1} \mathbf {C}\right)
	\end{equation}
	when  the  matrix  $\mathbf D$ is 
	assumed to be an  inverse one.
\end{lemma}




\begin{lemma}[\textbf{Weyl Theorem}]\label{weyltheo}
Assume  that 
$\mathbf A$, $\mathbf B \in \mathbb R^{n \times n}$ are Hermitian  matrix,  and  the  eigenvalues are  sorted  in  a  ascending  order. 
\begin{equation}
\begin{aligned}
\lambda_{1}(\mathbf{A}) & \le \lambda_{2}(\mathbf{A}) \le \cdots \le \lambda_{n}(\mathbf{A}) \\
\lambda_{1}(\mathbf{B}) & \le \lambda_{2}(\mathbf{B}) \le \cdots \le \lambda_{n}(\mathbf{B}) \\
\lambda_{1}(\mathbf{A}+\mathbf{B}) & \le \lambda_{2}(\mathbf{A}+\mathbf{B}) \le \cdots \le \lambda_{n}(\mathbf{A}+\mathbf{B})
\end{aligned}
\end{equation}

Then,  the  following  inequalities   hold:
\begin{equation}
\lambda_{i}(\mathbf{A}+\mathbf{B}) \geqslant\left\{\begin{array}{c}
\lambda_{i}(\mathbf{A})+\lambda_{1}(\mathbf{B}) \\
\lambda_{i-1}(\mathbf{A})+\lambda_{2}(\mathbf{B}) \\
\vdots \\
\lambda_{1}(\mathbf{A})+\lambda_{i}(\mathbf{B})
\end{array}\right.
\end{equation}
and 
\begin{equation}
\lambda_{i}(\mathbf A+\mathbf B) \le\left\{\begin{array}{c}
\lambda_{i}(\mathbf{A})+\lambda_{n}(\mathbf B) \\
\lambda_{i+1}(\mathbf{A})+\lambda_{n-1}(\mathbf{B}) \\
\vdots \\
\lambda_{n}(\mathbf{A})+\lambda_{i}(\mathbf{B})
\end{array}\right.
\end{equation}
for  $i=1,2,\dots, n$.
\end{lemma}

\begin{lemma}(Ref \cite{zhang2017matrix})\label{AB_BA_eig}
Given  
the    $ m \times n$    matrix  
$\mathbf A$
and   the   $n \times m$   matrix  
$\mathbf B$ ( $m < n $),
assuming  that 
the $m$ eigenvalues of 
$ 	\mathbf A
\mathbf B $
are  denoted  by 
$
\lambda_{1}, \lambda_{2},  \dots, \lambda_{m}$,
then  the $n$  eigenvalues  of  
$ 	\mathbf  B
\mathbf A $
are  given  by 
$
\lambda_{1}, \lambda_{2},  \dots, \lambda_{m}, 
0, 0$.
\end{lemma}


The  following  theorem is  well  established     for   the   constrained  optimization  problem   to  identify  the  locally  optimal   solutions
(Page 332 in    \cite{Numerical}): 

\begin{theorem}[\textbf{Second-order necessary condition}]\label{second_order_necessary}\cite{Numerical}
	Suppose that
	for any  vector $ \mathbf w \in \mathbb V $,
	if 
	\begin{equation}\label{second_order}
	\mathbf w^{\mathrm T}
	\mathbf H (\mathbf v) 
	\mathbf w  
	\le 0   
	\end{equation}
	holds, then
	$\mathbf v $
	is a local maximum solution of (\ref{opti_ori}).
	And for  (\ref{opti_ori}), the set
	$\mathbb V $
	is defined as
	\begin{equation}\label{vdefine}
	\mathbb V=\{
	\mathbf w \in \mathbb R^{n}
	\vert   (\triangledown  g)^{\mathrm T} \mathbf w   =0
	\}=
	\rm Null [(\triangledown  g)^{\mathrm T} ]
	=
		\rm Null [\mathbf A ],
	\end{equation}
	where   $ \rm Null( \mathbf A) $ 
	denotes the null space  of $\mathbf A$  and $  \triangledown  g =\mathbf v $  denotes the gradient of the constraint: $ g (\mathbf v) = 
	\mathbf v^{\mathrm T}
	\mathbf v 
	-1
	=0 $.  
	
	If a stronger condition, i.e.,
	$ \mathbf w^{\mathrm T}
	\mathbf H (\mathbf v) 
	\mathbf w  <   0 $,   %\prec
	is satisfied, then,  
	$\mathbf v$
	is a   strict     local maximum  solution of (\ref{opti_ori}).
\end{theorem}

%In  addition, due to  the  related  optimization  model  is  with  constraints, it   can be  analyzed 
%that  except  for  the  second-order   Hessian  matrix,  another  vector  $\mathbf  w$  is  incorporated,  which  is  related to  and   deduced  by  the  constraint  condition
%$ g (\mathbf u) 
%=0 $.  
In  practice, instead of  directly utilizing  Theorem 
\ref{second_order_necessary},  
it  is  more  preferred to  identify the  local  extremum  by  checking  the  positive or negative  definiteness of  the  projected  Hessian  matrix (denoted 
$\mathbf P 
%\in \mathbb R^{(n-1) \times  (n-1)} 
$).  
It  is  calculated  by  
\begin{equation}\label{Pmatrix}
\mathbf P = 
\mathbf Q_{2}^{\mathrm T}
\mathbf H(\mathbf  v) 
\mathbf Q_{2},  
\end{equation}
where 
$  \mathbf Q_{2} $ is  obtained by
QR  factorization of   $     \triangledown  g $:
\begin{equation}\label{QR_factor}
\begin{split}
\triangledown  g
&=\mathbf Q
\begin{bmatrix}
\mathbf R   \\
\mathbf 0
\end{bmatrix} =
\begin{bmatrix}
\mathbf Q_{1}  &   \mathbf Q_{2}
\end{bmatrix}
\begin{bmatrix}
\mathbf R  \\
\mathbf 0
\end{bmatrix}
=\mathbf Q_{1}
\mathbf R
\end{split},
\end{equation}
where 
$  \mathbf Q $ 
is   an     
orthogonal  matrix, and $ \mathbf R$ is a square upper triangular matrix. 
In this  case, 
$ \mathbf R$ is  reduced  to  a  scalar  since  
$ \triangledown  g $  is  a  column  vector  and  
$\mathbf 0$ is  an    vector  with  all  elements  equal to 0.  
%$  \mathbf Q_{1} $  and   
%$  \mathbf Q_{2} $
%are   
%$ n \times 1$,   $ n \times (n-1) $ matrix, respectively.
See more  details for  this  part in  Page 337 of  \cite{Numerical}. 

Furthermore,  based on  (\ref{vdefine}), we  can  conclude  that  $   \mathbf w$  must  also  lie  in  the  orthogonal complement space of $ \mathbf  v$. 
Therefore,  $   \mathbf w$ can be linearly expressed  by the column vectors of  $\mathbf  P_{\mathbf  v } ^{\bot}$.
Assume that  $ \mathbf z \in \mathbb R^{(n-1) \times 1} $ is the coefficient vectors, it holds that  
$   \mathbf w   =  \mathbf  P_{\mathbf  v } ^{\bot}  \mathbf  z$.  
Then it can be  verified   that 
$  \mathbf v^{\mathrm T} \mathbf w 
=   \mathbf v^{\mathrm T}   \mathbf  P_{\mathbf  v } ^{\bot}  \mathbf  z
=  
\mathbf v^{\mathrm T} (\mathbf  I_{n} -
\mathbf  v (\mathbf  v^{\mathrm T}\mathbf  v)^{-1} \mathbf  v^{\mathrm T} )
\mathbf  z
= \mathbf  0
$  holds.  
Therefore,   (\ref{second_order}) can be  rewritten as 
$ 	\mathbf w^{\mathrm T}\mathbf H (\mathbf v) \mathbf w  
=
\mathbf  z^{\mathrm T}    (\mathbf  P_{\mathbf  v } ^{\bot})^{\mathrm T}    \mathbf H (\mathbf v)  \mathbf  P_{\mathbf  v }^{\bot}  \mathbf  z     \le 0 $.
Since  $ \mathbf  z $ can be   arbitrary  vectors,  checking  (\ref{second_order}) 
is equivalent to  checking  negative semi-definiteness    of  the  matrix, denoted  
$\mathbf  M $, with the  form of 
\begin{equation}%\label{matrix_M} 
\notag
\mathbf {M} =
(\mathbf  P_{\mathbf  v } ^{\bot})^{\mathrm T}    \mathbf H (\mathbf v)  \mathbf  P_{\mathbf  v }^{\bot}
%=
%\mathbf  P_{\mathbf  A } ^{\bot}   \mathbf H (\mathbf u)  \mathbf  P_{\mathbf  A }^{\bot}
=
(\mathbf  P_{\mathbf  v } ^{\bot})    \mathbf H (\mathbf v)  \mathbf  P_{\mathbf  v }^{\bot}
%=
%\mathbf  P_{\mathbf  A } ^{\bot}   \mathbf H (\mathbf u)  \mathbf  P_{\mathbf  A }^{\bot}
,
\end{equation}  
which  
corresponds to 
	(\ref{Mhess}).

Note that 
there are only one constraint for model (\ref{opti_ori}).
When more constraints are included, it can be similarly analyzed. 
For example. 
for    model  (\ref{optmodel}) with two constraints,  $\mathbf A$  should be  defined  as  
\begin{equation}
%\label{aaAform}
\notag
\mathbf A =  [\triangledown  g_{1}(\mathbf u) , \triangledown  g_{2}(\mathbf u) ] =
[\mathbf u, \mathbf 1_{n}]  \in   \mathbb R^{n  \times 2 } .
\end{equation}
And 
the 
locally    optimal   solutions of   (\ref{optmodel})
can  be  identified  by
checking the  negative 
definiteness of the   matrix  as  defined  in (\ref{Mhess2}).
%\input{Diri}
%\input{ES}


\bibliographystyle{siamplain}
\bibliography{simplexref}




\end{document}