%!TEX root = ../main.tex
\section{Experiments}\label{sec:experiments}
We evaluate \system having the following questions in mind:
(i)~{\em What is the runtime benefit of using \system compared to the state-of-the-art inverted index~\cite{abedjan2015dataxformer} when dealing with n-ary joins?}
(ii)~{\em How do parameters, such as the cardinality of tables, the choice of $k$ , $|Q|$, and the hash size affect the runtime?}
(iii)~{\em What is the filtering power of \hash compared to other hash functions?}
\subsection{Experimental Setup} \label{subsec:experimental_setup}
We probed different query tables against two different table corpora: the openly available Dresden Web Table Corpus (DWTC) and the German Open Data repository(\url{https://www.govdata.de/}). These corpora are selected because they yield tables with different sizes and dimensionalities.
The DWTC corpus contains more than $145M$ tables, $870M$ columns, $1.45B$ rows, and $660M$ unique values. 
The German open data also contains $17k$ tables, $440k$ columns, $62M$ rows, and $20M$ unique values. 
As query tables, we leverage sets of random inputs.
Similar to the state-of-the-art~\cite{zhu2019josie}, we selected $900$ query tables as six groups ($150$ tables in each category) from open data and webtable corpora. Each group is randomly selected from different cardinality distributions and we make sure that no duplicate tables are included inside those groups.The query columns are selected randomly.
For open data, the groups reflect query tables with cardinality smaller than $100$, $1000$, and $10k$, respectively. As WTDC tends to have smaller cardinalities, the groups contain tables where the cardinalities are smaller than $10$, $100$, and $1000$. In the experiments, we label the input query table collections as \textit{Corpus (size)}, e.g., \textit{WT (10)} and \textit{OD (100)}.
% The query sets on average, have the cardinality of $33$, $394$, $4933$, $3$, $17$, and $169$ and lead to joinability score $40$, $1434$, $8187$, $4$, $52$, and $99$ for OD (100), OD (1k), OD (10k), WT (10), WT (100), and WT (1k), respectively.
In addition, we also leverage \textit{School} corpus, generated in a previous work~\cite{chepurko13arda}. This corpus contains $335$ tables. The tables are comprised of over $27$ and $30,000$ columns and rows on average respectively. This corpus enables us to evaluate the systems in the existence of larger tables. In this set of experiments, we extend the tables containing ``Program Type'' and ``School Name'' columns with joinable tables.
We also selected machine learning datasets that require enrichment from Kaggle. This query set has more general content, therefore, they run against the webtable corpus. These query tables lead to almost $7M$ joinable candidates on average based on their first query column. Table~\ref{tab:queries} shows the query table sets used in this paper including statistics about the number of tables, average cardinality, and the average joinability score.
\input{charts_and_tables/query_tables}
In all our experiments, unless specified otherwise, we leverage the $128$-bit hash space and we look for top-$10$ joinable tables.
We implemented \system in \textit{Python 3.7} and ran it on a server with 128 CPU cores, 500 GBs of RAM, and 2 TBs of SSD drive. 
We used Vertica \textit{v10.1.1-0}~\cite{lamb2012vertica} to store the index.
Our code and datasets are publicly available~\footnote{\url{https://github.com/LUH-DBS/MATE}}.

\noindent{\textbf{Index generation.}} 
All of the hash-based solutions including \system+\hash require a prior hash generation. 
The original webtable and open data corpus sizes are approximately $250$ and $12$ GB each with the inverted index SCR.
\system requires additional space to store the super keys. As we defined the super keys per cell value (See Section~\ref{sec:desiderata}), \system needs $8.3B \times 128b = 123.6$ and $62M \times 128b = 11.9$ Gigabyte for webtables and open data, respectively.
Leveraging more efficient structure, where the super keys are stored per row, one can reduce the index size to $1.45B \times 128b = 21.6$ and $62M \times 128b = 0.92$ Gigabyte, respectively. This space efficient structure might lead to integration overhead, where the super keys are joined with the PLs.
% As super keys are defined per row, using $128$ bits hash space, \system needs $1.45B \times 128b = 21.6$ and $62M \times 128b = 0.92$ Gigabyte for webtables and open data, respectively. 
%For $256$- and $512$-bit hash sizes the additional required space doubles and quadruples, respectively.
% To avoid the overhead of finding the corresponding row per value, we store each super key for each row value. This increases the index size for webtables and open data to GB and GB, respectively. 
Josie on the other hand, requires $293$ and $20$ Gigabytes additional storage for webtables and open data, respectively. Note that the index for Josie is not sufficient for multi-column join discovery as it does not store the rows information of the cell values. Therefore, Josie-based approaches require an inverted index like SCR. 
The offline index generation for \system takes $35$ and $2$ hours for webtable and open data corpora, respectively. For Josie, the index generation time is $336$ and $50$ hours, respectively.

\subsubsection{Systems}\label{subsec:baselines_systems}
To our best knowledge, there is no a multi-attribute join discovery system. We compare \system and \hash, to multiple possible adaptations of single-column systems. 

\noindent{\textbf{Single-Column Retrieval (SCR).}} 
We, adapted a single column index SCR for n-ary join discovery. It uses all the optimizations in Algorithm~\ref{alg:application} including the table filtering strategies and initial column selection.
Yet, SCR cannot utilize the super key:
It fetches the candidate joinable rows and validates them through exact value comparisons in memory. %We have tested two versions of SCR: when the data fits into memory as has presented in~\cite{zhu2019josie} and the data does not fit into memory following the implementation of DataXFormer~\cite{abedjan2015dataxformer}. The two versions only differ in the initial fetching time for the tables that are joinable in the initial column.

\noindent{\textbf{SCR Josie.} Josie is the a state-of-the-art single-column join discovery algorithm introduced in literature. Josie leverages an index that maps values to columns, where as the columns are represented as sets. To infer the joinable rows we fall back on the SCR index.}

\noindent{\textbf{Multi-Column Retrieval (MCR)~\cite{zhu2019josie}.} In this approach, we fetch the PL items for each query column and intersect the results to discover the top-k joinable tables.}

\noindent{\textbf{MCR Josie~\cite{zhu2019josie}.} One can adapt Josie to discover multi-column joins by repeating it on every key column in Q, finding the most joinable tables per column, and then intersecting them and evaluating the tables that appear in all joinable results. 
% Due to the fact that there is no guarantee that the top-k joinable tables based on the composite key appears in the top-k of all joinable tables for single-columns this approach can lead to false negatives. In the simplest scenario $k = 1$. The goal is to find top-1 joinable table based on ``F. Name'', ``L. Name'', and ``Country''. One cannot make sure that this top-1 joinable table is the same with the top-1 joinable table based on ``Country''. In this baseline, to prevent FNs, we set $k = 500$ for the initial query column and a large $k = 10k$ for the remaining columns. 2) The offline index generation is very time consuming specially for large corpora such as, DWTC.
% 2) Repeating Josie per column is more time consuming specially if the number of key columns is large. 
}

\subsubsection{Hash Functions} \label{subsec_baseline_hash}
We compare the filtering performance of \system using different hash functions.

\noindent{\textbf{Bloom filter (BF).}} This is a variation of \system that uses bloom filters (instead of \hash) with a fixed number of hash functions to generate the super key~\footnote{\url{https://gist.github.com/malaikannan/22401e67b332f52651dcef8ea5039498}}.
The BF implementation calculates the number of hash functions based on the average number of columns in the corpus tables. We use Murmur3 hash family as the base function in the BF implementation. 
In BF, the FP ratio can be calculate as: $FP = (1-e^{-\frac{V\cdot H}{|a|}})^H$, where $V$ is the number of values to be inserted and $H$ is the number of hash functions~\cite{fan2000summary}. To calculate the optimum number of hash functions, we set FP to $0$. Doing so, the number of hashes can be calculated as: $H = \frac{|a|}{V} log2$.
We set $V=5$ for webtables and $V=26$ for OD, using the average number of columns in the corresponding corpus.

\noindent{\textbf{Less Hashing Bloom Filter~\cite{DBLP:conf/esa/KirschM06} (LHBF).} This approach is an optimized version of the standard bloom filter that uses only two hash functions per value. We use the LessHash-BloomFilter python library, which is publicly available.}~\footnote{\url{https://github.com/garawalid/LH-BloomFilter}}

\noindent{\textbf{Hash table (HT).}} 
It is the same as BF but uses one hash function. 

\noindent{\textbf{Other hash functions.}} 
We also compare \system to hash functions that are not followed by any post-processing of the hash values. 
We considered the following state-of-the-art hash functions: SimHash~\cite{charikar2002similarity, sadowski2007simhash}, MD5~\cite{rivest1992md5}, Google's CityHash~\cite{pike2017cityhash}, and Murmur.
%These functions are fast and lead to low collision rates. %Therefore, they are suitable for membership lookup tasks.
We excluded hash functions based on SHA as they turned out to be similar but slower than the aforementioned hash functions.

\subsection{\system VS. Baseline Systems}
\input{charts_and_tables/mate_vs_dxf_runtime_random_tables}
%Here, we only report the runtime for the 512-bit version of \system.
%Later in this section, we also evaluate the system performance for hash arrays with varied sizes.
Figure~\ref{fig:dxf_vs_mate_runtime} shows the runtime comparison \system against SCR, MCR, and the corresponding JOSIE implementations in log scale.
Without the row filtering optimization of \system, the runtime for joinability calculation dominates the overall runtime.
The depicted runtime results for baselines include the time required to discover the multi-column joins in-memory on top of the state-of-the-art single-attribute joinable table discovery systems. 
Note that there is an additional fetching cost that is negligible, i.e.,~in the order of milliseconds when we use the in-memory implementation like state-of-the-art~\cite{zhu2019josie}. However, the fetching cost can vary between $1$ and $40$ seconds when the data and the index has to be retrieved from disk. This is the case for the DWTC, which cannot fit into memory. The depicted runtime does not contain the fetching time as it is the same for both approaches.
We observe that \system outperforms baseline systems in almost all experiments: it is up to $61x$, $13x$, $9x$, $22x$ faster than MCR, SCR, MCR Josie, and SCR Josie respectively.
%As a result, it can drop most of the non-joinable table rows before any further computation.
This experiment also shows that no other baseline constantly performs better than the other approaches. For instance, SCR-based approaches are slower than their corresponding MCR-based systems for open data queries but on the large webtable corpus they underperform. Note that MCR Josie for webtables did not terminate in $7$ days. The slower performance of the MCR approaches on webtable queries is because accessing the relevant PLs does not scale for the size of the webtable corpus. While MCR Josie performs similar to MCR in OD (100) its performance drops as the cardinality of the open data queries grows.
The size of the input dataset correlates with the runtime. This is expected because join columns with higher cardinality initially match more PL items in the index which again need to be fetched and filtered. 

In addition to the input table size, FPs play an important role in the number of comparisons and ultimately the runtime.
For instance, in case of SCR, although OD (1k) and WT (1k) have similar query table sizes, the OD (1k) queries lead to $35\%$ higher runtime than WT (1k). This increase in runtime is because of the large number of FPs for OD (1k) tables, i.e., $3M$ more FP rows compared to WT (1k).
%Moreover, For some query datasets, such as OD (10,000), the performance difference between \system and baselines is smaller, because the initial column filters already many of the FPs.
%On the other hand, for some datasets, such as WT (10), this performance gap is substantial because SCR retrieves over one orders of magnitude more FPs than \system. % 400 times more
\newline
\textbf{Summary.}
(i) \system is up to $100x$ faster than unary join discovery systems in discovering n-ary joins.
(ii) Performance gain of \system over SCR-based approaches depends on the number of FP rows.


\subsection{\hash VS. Baseline Hash Functions} \label{subsubsec:xash_vs_hashes}
Table~\ref{tab:dataset_runtime} depicts the runtime of \system with different hash functions, including the proposed \hash.
Note that all the competing hash functions benefit from all of \system's optimizations and only differ in the applied hash function during row filtering.

\input{charts_and_tables/dataset_runtime_table_random_tables}
\input{charts_and_tables/precision_random_tables}
In Table~\ref{tab:dataset_runtime}, we highlight the best performing approach of each row and hash size in \textbf{bold} font.
We observe that \system with \hash outperforms all the other baselines on all of the queries.
%\hash leverages simple syntactic information of the cell values to generate distinguishable index elements. 
\system + \hash can be up to $10x$ faster than \system + BF, which is the second most efficient baseline on average.
The cells showing this speedup are highlighted with \colorbox[HTML]{FFABA8}{red}.
BF's under-performance is rooted in its higher collision rate compared to \hash. Besides, by checking the length attribute that we use in \hash to generate the hash results, we can drop many of the non-joinable rows without evaluating all the bits in the super keys. This feature gives \system a superiority over BF in runtime even in the cases with similar or lower FP rates.
Another interesting insight is that \system + BF is faster than \system + HT in most of the cases and this is because BF is able to leverage more bits in the hash space to encode the join keys than a hash table that leverages only a single bit to hash each key value.%A single bit can only generate a limited number of unique hash values, e.g., 128 different hash values in 128-bit hash space, therefore, in the best case, $\frac{1}{128}$ of the single hashes will lead to collision, i.e., the same hash value. 
Table~\ref{tab:dataset_runtime} shows that \hash performs also better than LHBF in all of the cases. LHBF leverages fewer hash functions in comparison to BF with statistically similar FP probability. However, similar to HT, fewer hashing does not necessarily lead to a better performance in discovering the joinable tables. In our experiments LHBF only performs better on larger hash sizes for webtable queries, but in the remaining cases, LHBF is less efficient than BF.

The results also show that although super keys based on the standard hash functions, MD5, CityHash, SimHash, and Murmur, lead to overall performance gains over the naive SCR approach, all of them clearly fall behind \hash.
This is because they are not optimized to identify subset relationships that we encode via bitwise OR masking. They generate too many 1 bits (on average $50\%$) in the hash results, which leads to a higher number of FPs.
Thus, if a table contains six columns the aggregation of six hash results will on average turn $98\%$ of the super key to 1s, which would make the super key highly ineffective in masking.

%MD5, CityHash, SimHash, and Murmur lead to a very similar performance to SCR specially in open data and school experiments. This performance drop is because the row filtering overhead is almost as high as the runtime benefit from the row filtering.

Further, the experiments show that in most of the cases, a larger hash size (512 bits) leads to a faster join discovery than the smaller versions. However, in cases such as WT (100) and Simhash, the larger hash size results in slower discovery. The experiments that lead to slower runtime compared to their smaller hash size are marked as \colorbox[HTML]{A2EDFF}{blue} cells. 
When the FP rate is similar for two hash sizes, the approach with the larger hash size will result in similar or higher runtime. 
This is because the bit-wise OR operation is more expensive for larger hash values.
For instance, for WT (100) query tables and SimHash, the difference between the FP rate is almost zero, therefore, the runtime overhead of 512-bit hash causes worse performance.
In the more common case that a larger hash size reduces the number of FPs the resulting runtime is also lower.
For example, for the OD (100) datasets, the larger hash size for \hash results in $85\%$ fewer FPs on average, leading to $48\%$ speed-up.

Finally, we also observed that \system can indeed find interesting joinable tables based on composite keys. 
%Similarly, we observe that using composite keys we can find interesting tables for almost any dataset.  
% As another example, the most joinable table to the \textit{Page View} dataset~\cite{base_pageview} with query columns <``Name'', ``Country''>, which contains the information about Wikipedia pages of people, reveals the birth/death date of the people in the given countries. 
Searching for the top joinable tables to Kaggle Movie dataset based on a single-column join and ``Movie Title'' key columns, none of the top-10 tables contains more than one additional column, containing float numbers. However, using our multi-column join discovery approach and query columns <``Director name'', ``Movie Title''>, we obtain a table with 8 columns worth of information, including the plot of the movie, actor names, etc.
For the \textit{Kaggle Airline} dataset with join keys ``Airline name'' and ``Country'' \system obtains a table representing the airports in which the airlines operate flights in the given countries. 

\noindent\textbf{Summary.}
(i)~The syntactic feature extraction in \hash leads to a faster multi-attribute join discovery compared to the baselines;
(ii)~the standard hash functions are not ideal for n-ary joins due to their uniform distribution property that results in too many 1 bits; and
(iii)~ larger hash sizes result in higher calculation overhead.

\subsection{FP Rate}\label{subsubsection:fp}
%We now delve into more details regarding the FP rates.
There is a direct relationship between the table discovery runtime and the FP rate of the hash function used.
We measure the precision as the ratio of true positives (TP) over both TPs and FPs: $precision = \frac{TP}{TP + FP}$. Precision normalizes the TP-rate across datasets.

Table~\ref{tab:dataset_precision} shows the results of this experiment. Due to space reasons in this experiment, we only show the results for $128$- and $512$-bit hash sizes. Note that we observe the same trends as the other hash sizes.
Like runtime, precision increases with larger hash sizes in most of the cases. This is due to the fact that hash functions can scatter the values in a larger range and this leads to lower collision rate between super keys.
On average, \system + \hash achieves the highest precision compared to all the other approaches both in 128- and 512-bit hash spaces. \hash achieves up to $25\%$ higher precision than BF. 
% \hash maps the character and length features of the cell values to hash bits and rotates them based on the length. This removes the random overlap between the hashes. 
However, in five cases, BF outperforms \hash. In these experiments the difference between BF and \hash is only $4\%$.
\system's runtime superiority regardless of its precision is because of the design of \hash: the length feature of the cell value is located on the left-most segment of the array. Thus, the bit-wise OR operation can skip a table row before even getting to the character features of the cell values ensuring that this happens in the very first bit-segment of the hash. If the length of the key value has never appeared in the candidate row, \system moves to the next row.
In baseline approaches, such as BF, the 0-bits are randomly scattered. Therefore, step-wise filtering might take longer to find the contradicting bits.
% For query tables, such as WT (1k), \hash leads to distinctively high precision with 128 bits. 
%\hash achieves up to $25\%$ higher precision than BF. 
\hash displays the smallest standard deviation showing its robustness in achieving high precision.
In general, the precision achieved by LHBF is consistent with its runtime performance compared to the BF approach. LHBF achieves higher precision and ultimately, lower runtime for webtable queries with $512$-bit hash sizes but in the remaining experiments BF outperforms LHBF.
%As observed before, the SimHash function performs better than the other remaining hash functions. %CityHash performs very similarly to Murmur and MD5 except on WT (100), OD (100), and OD (10k), where CityHash gains up to $2\%$ more precision.
% As the precision experiments show, BF performs better than \hash in a single case of school query tables and $512$-bits hash size. However, due to the special hash design in \hash and the segmentation of the hash array, the evaluation process in \hash is much faster than BF. In \hash, a large portion of the FPs are pruned by only checking the length bits. This optimization prevents from checking all the remaining bits and leads to faster join discovery.

\noindent\textbf{Summary.}
(i) The precision of an approach correlates with its runtime.
(iii) The segmentation of bits in \hash leads to faster join discovery even in cases where precision rates are similar.

\subsection{\system In Depth}
\subsubsection{Top-$k$.}
% \input{charts_and_tables/topk_experiments_random_tables}
The number of top-$k$ joinable tables changes the stopping criteria of the system (see table filtering step in Section~\ref{sec:index_applicaiton}).
Larger $k$ requires more tables to be evaluated.
In this experiment, we measure the precision of \system with different hash functions and vary $k$ from $2$ to $20$.
We ran the experiment with the WT (100) datasets as the input dataset against the DWTC corpus.
Other variables, such as key columns, are fixed in this experiment.
%We limit this experiment to the best performing hash functions.
In this expeirment, \system + \hash achieves the highest precision in comparison to other approaches for all $k$ values.
Increasing $k$, the precision for \hash increases $4\%$, while the precision remains the same for BF. The other hash functions lead to a slight precision cut encountering new tables with lower candidate joinable rows. This shows that \hash has higher ability to filter non-related rows compared to the given baselines. According to our observations on the candidate tables, this variation in precision occurs when the candidate tables contain more columns than average.
%Because of the reason that \hash is able to encode the cell values depending on their syntactical differences, the values from different domains map to different hash functions and these achieves fewer FPs.

% Figure~\ref{fig:topk} shows the results. \system + \hash achieves the highest precision in comparison to other approaches for all $k$ values.
% As expected, increasing $k$, increases the number of validated PL items because the first rule in the table filtering step starts later. Therefore, the system evaluates more candidate tables to discover the top-$k$ joinable tables. 
% According to the experiment, increasing $k$, the precision for \system increases, where, generally, the other hash functions lead to a slight precision cut encountering new tables with lower candidate joinable rows. This shows that \hash has higher ability to filter non-related rows compared to the given baselines. According to our observations on the candidate tables, this variation in precision occurs when the candidate tables contain more columns than average.
% Because of the reason that \hash minimizes the number of used bits in the candidate rows, the super key does not lose its ability in filtering irrelevant rows even in the existence of high dimensional tables. 
% This is not the case for BF. Because of the higher FP rate, more tables and in particular tables with large dimensions are passed to BF, which BF fails to filter.

\subsubsection{\hash components}
\input{charts_and_tables/mate_components_random_tables}
We now evaluate the impact of each feature used in \hash on the average precision and the FP rate of \system.
We use the WT (100) datasets in this experiment with the same setup as in the previous experiments.
According to the results in Figure~\ref{fig:components}, encoding the \textit{character and its location} has higher filtering power than the \textit{length} feature.
The difference between \hash and \textit{character + length + location} is the rotation operation.
In particular, we observe that the rotation filters $20\%$ of the remaining FPs undiscovered in \textit{character + length + location}.
This shows that the rotation plays an important role in pruning FPs.

\subsubsection{Join-Key Size}
\input{charts_and_tables/composite_key_size_experiment}
Here, we evaluate the scalability of \system in the existence of different join-key sizes, i.e.,~the number of columns in the composite key of the input table. 
Due to the limited number of tables with high dimensional composite key, we ran this experiment with a random dataset from the German Open Data corpus with up to 10 columns that can form a composite key (out of 33 columns).
Figure~\ref{fig:keysize} (a) and \ref{fig:keysize} (b) depict the runtime and precision results for different key sizes.
Increasing the number of columns, we observe that the runtime for \system constantly reduces because the FP rate constantly decreases. However, this does not imply a constant increase in precision because increasing the number of key columns changes the ratio of joinable and non-joinable rows. In our experiment, moving from a composite key with two columns to a three-column key, $97\%$ of the joinable rows become non-joinable due to the newly introduced key column. Thus, the filter is confronted with a larger number of FPs, some of which come through.
From key size $4$ upwards, precision increases again as expected.
The runtime gain for larger key sizes is because of two reasons:
\textit{(1)}  With more columns in the query join key, there will be more 1-bits in the query super key, which makes it harder to mask. 
\textit{(2)} Increasing the size of the composite key leads to fewer joinable table rows. Therefore, it is more likely that the second table filtering rule drops the candidate tables without evaluating the remaining rows.
%Again, \hash outperforms other baselines in runtime and precision.   

\subsubsection{Initial column selection.}
%\input{charts_and_tables/ICS}
We evaluate our heuristic to select the initial query column by comparing the number of retrieved PL items in \system to four other baselines:
\textit{(i)} The column order. In this approach, the initial column will be the first query column according to the column order inside the table. %The idea behind this strategy is that usually, the identifying columns come earlier in the table.
\textit{(ii)} The longest string (TLS). In this approach, the system selects the column that contains the longest cell value as the initial column. %The heuristic here is that the longer the text, the more specific is the column. 
\textit{(iii)} The worst-case scenario. A hypothetical approach that always picks the worst column that returns the larger number of PL items.
\textit{(iv)} Best, i.e. ground truth, which chooses the column that filters most.
The best- and the worst-case scenarios provide lower and upper bounds for our experiment.
This experiment is done using the OD (10k) tables.
In this experiment our cardinality-based heuristic outperfmed the other heuristics. It retrieved on average only 179 PLs compared to Column Order, TLS, and the worst-case scenario with 202, 248, and 728 Pls, respectively. The optimal number of PLs based on ground truth was 83. The heuristic used in \system performs better because of the fact that the number of PL items per cell value follows the power-law distribution. There is a small set of values that have a large number of PL items but most of the values lead to a similar number of PL items (The average number is $12$).
%Table~\ref{tab:ICS} shows that the cardinality-based heuristic used in \system, performs best and is very close to the ground truth approach. The heuristic used in \system performs better because of the fact that the number of PL items per cell value follows the power-law distribution. There is a small set of values that have a large number of PL items but most of the values lead to a similar number of PL items (The average number is $12$).% Thus, there is a direct relationship between the cardinality and the number of retrieved PL items.




