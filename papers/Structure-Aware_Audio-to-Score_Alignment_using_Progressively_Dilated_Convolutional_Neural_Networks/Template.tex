% Template for ICASSP-2020 paper; to be used with:
%          spconf.sty  - ICASSP/ICIP LaTeX style file, and
%          IEEEbib.bst - IEEE bibliography style file.
% --------------------------------------------------------------------------
%style file copy of icassp 2020 draft (test) (bkp) (Copy)
\documentclass{article}
\usepackage{spconf,amsmath,graphicx}
\usepackage{booktabs} 
\usepackage{graphics}
\usepackage{adjustbox}
\usepackage{graphicx}
\usepackage{tabularx}
\usepackage{comment}
\usepackage{subfig}
\usepackage[justification=centering]{caption}
\usepackage{amsmath}
\usepackage{cite}
\usepackage{tikz}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{booktabs} 
\usepackage{multirow}
\usepackage{svg}
\usepackage{multicol}
\usepackage{comment} 
\captionsetup[table]{skip=5pt}
\usepackage[utf8]{inputenc} 
\usepackage{textcomp}
\usepackage{xcolor}
%\usepackage{subfigure}
% Example definitions.
% --------------------
\def\x{{\mathbf x}}
\def\L{{\cal L}}

% Title.
% ------
\title{Structure-aware Audio-to-Score Alignment using progressively dilated Convolutional Neural Networks}
%
% Single address.
% ---------------

\name{Ruchit Agrawal $^{\dagger}$ \qquad Daniel Wolff $^{\star}$ \qquad Simon Dixon $^{\dagger}$ 
      \thanks{This research is supported by the European Union’s Horizon 2020 research and innovation programme under the Marie Skłodowska-Curie grant agreement number 765068.}}
  \address{$^{\dagger}$ Centre for Digital Music, Queen Mary University of London, UK \\
      $^{\star}$ Institute for Research and Coordination in Acoustics/Music, Paris, France}
%
% For example:
% ------------
%\address{School\\
%	Department\\
%	Address}
%
% Two addresses (uncomment and modify for two-address case).
% ----------------------------------------------------------
%\twoauthors
%  {A. Ruchit, B. Author-two\sthanks{Thanks to XYZ agency for funding.}}
%	{School A-B\\
%	Department A-B\\
%	Address A-B}
%  {C. Author-three, D. Author-four\sthanks{The fourth author performed the work
%	while at ...}}
%	{School C-D\\
%	Department C-D\\
%	Address C-D}
%
\begin{document}
\ninept
%
\maketitle
  \tikz [remember picture, overlay] %
     \node [shift={(50mm,22mm)}] at (current page.south west) %
     [anchor=south west] %
     {\includegraphics[width=6mm]{flag_yellow_high.jpg}};
%
\vspace{-0.5cm}
\begin{abstract}
The identification of structural differences between a music performance and the score is a challenging yet integral step of audio-to-score alignment,  an important subtask of music information retrieval. We present a novel method to detect such differences between the score and performance for a given piece of music using progressively dilated convolutional neural networks. Our method incorporates varying dilation rates at different layers to capture both short-term and long-term context, and can be employed successfully in the presence of limited annotated data. We conduct experiments on audio recordings of real performances that differ structurally from the score, and our results demonstrate that our models outperform standard methods for structure-aware audio-to-score alignment.
\end{abstract}
\begin{keywords}
Music Information Retrieval, Audio-to-Score Alignment, Music Alignment, Music Structure Analysis
 %For a list of suggested keywords, send a blank e-mail to keywords@ieee.org or visit \url{http://www.ieee.org/organizations/pubs/ani_prod/keywrd98.txt}
\end{keywords}
\vspace{-0.2cm}
\section{Introduction}
\vspace{-0.2cm}
The analysis of %expressive
music performance is a challenging area of Music Information Retrieval (MIR), owing to multiple factors such as suboptimal recording conditions, structural differences from the score, and subjective interpretations. 
%The analysis of music performance is a challenging area of Music Information Retrieval, owing to multiple factors such as x, y, and z. Among these, one particular caveat of traditional performance tracking systems is the inability to capture a, b and c. 
%One such issue is the handling of repeats and jumps. 
Deviations from the structure and/or tempo prescribed by the score are %some methods through which music performers add expressiveness to their music. Such deviations are 
common in several genres of music, particularly classical music and jazz \cite{widmer2016getting}.
%,\cite{paulus2010state}.
Identification of such structural differences from the score is a challenging yet important step of performance analysis, especially for the audio-to-score alignment task. 
The alignment task aims at generating an accurate mapping between a performance audio and the score for a given piece of music.
%Structural changes like repeats pose a major challenge to music alignment, since they can be of different types and can have multiple endings.  
%One such example from the famous piece Für Elise is shown in Figure \ref{repEx}.
The particular path through the score that the musician is going to take is difficult to predict, and although there have been several methods proposed in the recent past to handle these impromptu changes;  it still remains a problem that is not fully solved \cite{arzt2016flexible}.
\vspace{0.1cm}
 \par The majority of approaches for audio-to-score alignment are based on Dynamic Time Warping (DTW) \cite{dixon2005line} or Hidden Markov Models (HMM) \cite{muller2015fundamentals}. These methods typically assume that the musician follows the score from the beginning to the end without inducing any structural modifications, which is often not the case in real world scenarios. The alignments computed using DTW-based methods are constrained to progress monotonically through the piece, and are thereby unable to model structural deviations such as repeats and jumps. Previous methods for handling structural changes during alignment are either reliant on robust Optical Music Recognition (OMR) to detect repeat and jump directives \cite{Fremerey2010handling}; or on frameworks fundamentally different from DTW, such as the Needleman Wunsch Time Warping method \cite{grachten2013automatic}. The former method, called \begin{math}\textit{JumpDTW}\end{math}, requires manually annotated frame positions for block boundaries, initially provided by an OMR system, and is unable to model deviations that are not foreseeable from the score. The latter is unable to align repeated segments, since it does not introduce backward jumps, and is based on a waiting mechanism upon mismatch of the two streams. 
 Since structural changes like repeats and jumps can be arbitrarily added during a performance, especially during rehearsals; these are challenging to model using rule-based approaches, and machine learning methods offer promise at effectively addressing these challenges. 
 %Mention how previous methods for structural changes use such and such method. While we use neural networks for this task. 
 \par This paper is aimed at handling structural changes between the performance and the score for the offline audio-to-score alignment task. We propose a custom Convolutional Neural Network (CNN) based architecture for modeling these differences, coupled with a flexible DTW framework to generate the fine alignments. 
 %Using dilation allows us to capture jumps and skips effectively, using much fewer parameters than a sequential model using RNNs.
 Our method does not require a large corpus of hand-annotated data and can also be trained exclusively using synthetic data if hand-annotated data is unavailable.
 %Chapter \ref{ch:literature} covers the relevant work in this direction. We recap the limitations of some of the main approaches  below:
%One such issue is the handling of repeats and jumps. 
 %elaborate
%\par We present a method that can detect structural deviations in audio, without the need for manual annotations, using Dilated Convolutional Neural Networks.
Our architecture employs progressively dilated convolutions in addition to standard convolutions, with varying amounts of dilation applied at different layers of the network. 
The primary motivation behind our architecture is that it allows us to effectively capture both short-term and long-term context, %long-term jumps in addition to repeated sections,
using much fewer parameters than sequential models such as recurrent neural networks, and without facing the vanishing gradient problem. We conduct experiments on piano music and compare our method with three major alignment approaches; namely \begin{math}\textit{JumpDTW}\end{math} \cite{Fremerey2010handling}, \begin{math}\textit{NWTW}\end{math} \cite{grachten2013automatic}, and \begin{math}\textit{MATCH}\end{math} \cite{dixon2005line}. 
We demonstrate results on two different test sets containing real performances. Our method yields higher performance than previous methods proposed for handling structural deviations, without requiring manually annotated data; and noticeably outperforms these methods given a limited amount of annotated data.  To the authors' knowledge, this is the first method to employ dilated convolutional neural networks for structure-aware audio-to-score alignment. 
%To the authors' knowledge, this is the first method to employ neural networks for structure-aware audio-to-score alignment, and the first the incorporate dilated convolution for this task.
%\par The rest of the paper is organized as follows: We describe prior work in this direction in Section \ref{related}. Section \ref{method} details our proposed method and the general architecture of our models. The experimentation conducted and results obtained using our method are described in Section \ref{setup}. We present our results and highlight possible directions  for future work in Section \ref{results}.
\begin{figure*}[th]
  \vspace{-2.6cm}
  \centering
  \includegraphics[width=6.5in]{images/atrous.pdf}
  \vspace{-0.7cm}
  \caption{Schematic diagram illustrating the general architecture of our models.\\ \textit{d: Dilation rate, FC: Fully connected layer}}
  \vspace{-0.4cm}
  \label{fig:pipeline}
\end{figure*}
\vspace{-0.3cm}
\section{Related Work}\label{related}
\vspace{-0.2cm}
%We begin with discussing relevant research aimed at flexible music alignment and then move on to broadly discussing more general research conducted in structure-aware MIR.
%in this section and then specifically describe methods aimed at audio-to-score alignment.
Early work on structure-aware MIR focuses on structural segmentation of musical audio by constrained clustering \cite{levy2008structural} and music repetition detection using histogram matching \cite{tian2009histogram}. %They propose a new feature called chroma histogram, which facilitates finding out repetitive segments from popular songs accurately and quickly. 
Arzt and Widmer \cite{arzt2010towards} propose a multilevel matching and tracking algorithm to deal with issues in score following due to deviations from the score in live performance. 
%a works on addressing issues dealt in score following by proposing a multilevel matching and tracking algorithm which continually updates and evaluates multiple high-level hypotheses to deal with deviations of the live performer from the score. 
A challenge faced by this approach appears when complex piano music is played with a lot of expressive freedom in terms of tempo changes. Hence, they propose methods to estimate the current tempo of a performance, which could then be used to improve online alignment  \cite{arzt2010simple}. This is similar to the work proposed by Müller et al. \cite{muller2009towards}, wherein they develop a method for automatic extraction of tempo curves from music recordings
by comparing performances with neutral reference representations.
%They incorporate tempo models which illustrate different possible performance strategies. This helps making the tracking algorithm more robust to on-the-fly structural changes.

%For a different task but using relevant features, \cite{li2010automatic} propose Convolutional Neural Networks for  automatic musical pattern feature extraction for genre classification, trained on the GTZAN dataset. Theirs is an early work that showsthis is a promising direction, however their model is not robust to unseen data. 
\par Work specifically on structure-aware audio-to-score alignment includes \begin{math}\textit{JumpDTW}\end{math} \cite{Fremerey2010handling} and Needleman-Wunsch Time Warping (\begin{math}\textit{NWTW}\end{math}) \cite{grachten2013automatic}  among others. 
Fremerey et al. \cite{Fremerey2010handling} focus on tackling structural differences induced specifically by repeats and jumps, using a novel DTW variation called \begin{math}\textit{JumpDTW}\end{math}. This method identifies the ``block sequence" taken by a performer along the score, however it requires manually annotated block boundaries to yield robust performance, which are generally not readily available at test time for real world applications. Additionally, it cannot align deviations that are not foreseeable from the score. \begin{math}\textit{NWTW}\end{math} \cite{grachten2013automatic}, on the other hand, is a pure dynamic programming method to align
music recordings that contain structural differences. This method is an extension of the classic Needleman-Wunsch sequence alignment algorithm  \cite{likic2008needleman}, with added capabilities to deal with the time warping aspects of aligning music performances. A caveat of this method is that %it cannot skip certain parts of the score, thereby being unable to effectively model forward jumps. Additionally, 
it does not successfully align repeated segments owing to its waiting mechanism, which skips unmatchable parts of either sequence, and makes a clean jump when the two streams match again. %the two streams to match again, rather than introduce jumps, like \begin{math}\textit{JumpDTW}\end{math}.
\vspace{-0.1cm}
 \par Apart from \begin{math}\textit{JumpDTW}\end{math} and \begin{math}\textit{NWTW}\end{math}, which focus on offline alignment, work on online score following \cite{nakamura2015real} has demonstrated the effectiveness of HMMs for modeling variations from the score. While this method focuses on real-time score following for monophonic music, our work deals with offline audio-to-score alignment for polyphonic music performance. 
%\par Nakamura et al \cite{nakamura2015real} propose an HMM-based method for score following, a task related to audio-to-score alignment. Their method is able to deal with arbitrary repeats and skips in monophonic music, however it struggles with polyphonic music performance. While they focus on real-time score following, our work deals with offline audio-to-score alignment. 
Another work related to ours is that proposed by Jiang et al.\cite{jiang2019offline}, which  focuses on offline score alignment for the practice scenario. Similar to Nakamura et al.\cite{nakamura2015real}, their approach is also based on HMMs; but they propose using pitch trees and beam search to model skips. However, their method struggles with pieces containing both backward and forward jumps, which is an important challenge we tackle using our progressively dilated convolutional models.
%Another work related to ours is that proposed by Jiang et al\cite{jiang2019offline}, which focuses on offline score alignment for the practice scenario; and is based on Hidden Markov Models and they propose using Pitch Trees and Beam Search for the score alignment with skips problem. However, their method struggles with pieces with both backward and forward jumps, which is an important challenge we tackle using our models.
Recent work on audio-to-score alignment has demonstrated the efficacy of multimodal embeddings \cite{dorfer2018learning}, reinforcement learning \cite{dorfer2018learning2}, \cite{henkel2019score} and learnt frame similarities \cite{agrawal2021learning}, albeit these are not structure-aware methods. Very recently, Shan et al. propose Hierarchical-DTW \cite{shan2020improved} to automatically generate piano score following videos given an audio and a raw image of sheet music. Their method is reliant on an automatic music transcription system \cite{hawthorne2017onsets} and a pre-trained model to extract bootleg score representations \cite{tanprasert2020midi}. It struggles when this representation is not accurate, and also on short pieces containing jumps. While they work with raw images of sheet music and generate score following videos; our method works with symbolic scores, is not reliant on other pre-trained models, and performs well on both short and long pieces. 
\begin{comment}
\par Apart from alignment-specific research, work on analyzing music structure in MIR is moving towards machine learning based methods \cite{serra2014unsupervised}, \cite{ullrich2014boundary}, \cite{mcfee2014analyzing}, and \cite{grill2015music}.
%These studies deviate from previous methods and employ machine learning techniques for structure analysis. 
While these methods focus on analyzing the content of a single piece or a performance, with tasks such as boundary detection  \cite{ullrich2014boundary}, \cite{grill2015music} and music structure annotation \cite{mcfee2014analyzing}, our research inherently differs from them in that it is primarily concerned with the alignment of a performance to the score for a given piece of music. 
\end{comment}
\begin{table*}[th]
%\caption{Results of our models}
  % \centering
  \vspace*{-0.6cm}
  \begin{adjustbox}{max width=\textwidth}
\begin{tabular}{ccccccccccccc}
\toprule
\hline 
\multirow{2}{*}{\textbf{Model}} & 
\multicolumn{4}{c}{\textit{On Mazurka dataset}}  
& \multicolumn{4}{c}{\textit{With structural differences (Tido)}} & \multicolumn{4}{c}{\textit{Without structural differences (Tido)}}
\tabularnewline
%\cline{2-7} 
%\cline{3-7} \cline{4-7} \cline{5-7} \cline{6-7} \cline{7-7} 
  & \textbf{$<$25ms}& \textbf{$<$50ms} & \textbf{$<$100ms} & \textbf{$<$200ms} & \textbf{$<$25ms} &\textbf{$<$50ms} & \textbf{$<$100ms} & \textbf{$<$200ms}
  &
  \textbf{$<$25ms} &
  \textbf{$<$50ms} & \textbf{$<$100ms} & \textbf{$<$200ms}
  \\
\midrule 
 \begin{math}\textit{MATCH}\end{math}\cite{dixon2005line} & 64.8 & 72.1 & 77.6 & 83.7 & 61.5 & 70.4 & 74.6 & 80.7 & 70.2 & 78.4 & 84.7 & 90.3  \\
\midrule
 %$DTW_{Chroma}$ & - & - & - & -  & 62.9 & 70.5  & 76.3 & 82.4 & 64.8 & 72.1 & 77.6 & 83.7\\
%\midrule 
% \begin{math}\mathit{CCA}\end{math} & 68.2 & 75.3 & \textbf{81.4} & \textbf{87.8} & 70.3 & 76.7  & 82.1 & 88.4 & 64.8 & 72.1 & 77.6 & 83.7 \\
%\midrule 
 \begin{math}\textit{JumpDTW}\end{math} \cite{Fremerey2010handling} & 65.8 & 75.2 & 79.8 & 85.7 & 69.1 & 77.2 & 82.0 & 88.4 & 68.7 & 77.5 & 82.1 & 88.9  \\
\midrule 
 \begin{math}\textit{NWTW}\end{math} \cite{grachten2013automatic} & 67.6 & 75.5 & 80.1 & 86.2 & 68.6 & 75.8 & 80.7 & 87.5 & 68.4 & 77.1 & 82.8 & 89.4  \\
\midrule 
     \emph{CNN}${}_{1+1}$ & 68.2 & 75.7 & 80.5 & 87.1   & 70.4 & 78.3 & 83.4 & 90.1 & 69.3 & 78.0 & 84.1 & 89.3 \\
\midrule 
      \emph{DCNN}${}_{2+2}$ & \textbf{69.9} & 76.4 & 81.6 & 88.9 & 72.7 & 80.1  & 84.5 & 91.4 & \textbf{71.4} & 79.5 & 85.3 & 90.5 \\
\midrule 
    \emph{DCNN}${}_{2+3}$ & 69.7 & \textbf{77.2} & \textbf{82.4} & \textbf{89.8} & \textbf{73.9} & \textbf{81.3} & \textbf{85.6} & \textbf{92.8} & 71.0 & \textbf{80.3} & \textbf{85.8} & \textbf{91.8} \\
\midrule 
     \emph{DCNN}${}_{3+3}$ & 69.2 & 76.1 & 81.2 & 88.7  & 72.3 & 79.5 & 84.2 & 90.4  & 70.6 & 78.8 & 84.9 & 91.2\\
  \midrule
  \emph{DCNNsyn}${}_{2+3}$ & 68.1 & 75.9 & 80.7 & 87.5  & 70.5 & 78.6 & 83.8 & 90.5 & 69.2 & 78.3 & 84.6 & 89.8  \\
\midrule 
\bottomrule
\end{tabular}
\end{adjustbox}
\vspace{0.1cm}
\caption{Alignment accuracy in \% on  the Mazurka and Tido test sets \\ \emph{DCNN}${}_{m+n}$: Dilated CNN model with dilation rates of $m$ and $n$ at the second and third layer respectively \\
%\begin{math}\mathit{DCNNsyn_{2+3}}\end{math}: Dilated CNN model trained exclusively on synthetic data, with dilation rates 2 and 3
}
\vspace{-0.5cm}
\label{results2}
\end{table*}
\vspace{-1.0cm}
\section{Proposed
Method}\label{method}
\vspace{-0.2cm}
We present a novel method to detect structural differences for audio-to-score alignment using a custom convolutional architecture.  The closest to our method is the work proposed by  Ullrich et al.\cite{ullrich2014boundary}, an adaptation of the onset detection method proposed by Schl{\"u}ter et al.\cite{schluter2014improved}.
While Ullrich et al.\cite{ullrich2014boundary} train a CNN as a binary classifier on spectrogram excerpts for popular music (using hand annotated data) to detect boundaries in a song, we focus on structure-aware alignment of a performance to the score, with the cross-similarity matrices as inputs. Additionally, we incorporate dilation in our CNN models to incorporate multi-scale context. 
\par Our architecture combines standard convolution and dilated convolution \cite{yu2015multi}, with varying amounts of dilation applied at different layers of the network. 
We approach the sturctural deviation identification problem as a multi-label prediction task, and train our models to detect synchronous subpaths between the score and the performance. These subpaths are detected by means of inflection points, which encode the positions of structural differences between the two streams. 
%We approach the deviation identification problem as a multi-label prediction task and train our model to detect the inflection positions in the two input streams corresponding to the performance and the score respectively.
Figure \ref{fig:pipeline} illustrates the general architecture of our models. Our networks operate on the cross-similarity matrix between the score and performance and predict the ($x$, $y$) co-ordinates of the inflection points as the output.
% line about cross similarity matrix
% output
We employ dilated convolution at the second and the third layer, and standard convolution at the first layer.
%The motivation behind using varying amounts of dilation is to incorporate both short-term and long-term context, thereby effectively modeling different types of structural deviations like repeated sections, skipped measures, and arbitrary jumps. 
The dilated convolution operation \cite{yu2015multi} of a discrete function $F$ with a discrete filter $f$ on an element $\textbf{p}$ is defined as follows:
\vspace{-0.1cm}
\begin{comment}
\begin{equation}
(F*_df)(\textbf{p}) = \sum\limits_{s+dt=\textbf{p}}F(\textbf{s})f(\textbf{t})
\end{equation}
\end{comment}
\begin{equation}
(F*_df)(\textbf{p}) = \sum\limits_{\textbf{t}}F(\textbf{p}-d\textbf{t})f(\textbf{t})
\end{equation}
where $d$ is the factor by which the kernel is inflated, referred to as the dilation rate. Inflating the kernel using dilation allows us to incorporate larger context without increasing the number of parameters, which is essential for modeling structure. The receptive field is exponentially increased, and for a dilation rate $d$, a kernel of size $m$ effectively works as a kernel of size $m'$  as follows:
\vspace{-0.1cm}
%due to the increase in kernel size from $k$ to 
\begin{equation} m' = m + (d-1)*(m-1) 
\end{equation} This facilitates the incorporation of context better than standard convolutions, which can only offer linear growth of the effective receptive field as we move deeper into the network. We conduct experiments using varying dilation rates at different layers of the network to determine the optimal amount of dilation at each layer. The motivation behind using varying amounts of dilation is to incorporate both short-term and long-term context to model structure, 
%thereby effectively modeling different types of structural deviations like repeated sections, skipped measures, and arbitrary jumps. The incorporation of multi-scale contextual information
which has proven to be useful in computer vision as well as natural language processing tasks \cite{lee2017going, agrawal2018contextual}.
%The motivation behind using varying amounts of dilation is to incorporate both short-term and long-term context, thereby effectively modeling different types of structural deviations like repeated sections, skipped measures, and arbitrary jumps. 
We observe that progressively increasing dilation as we move deeper into the network produces the best results for detecting the structural differences. We compare the results of our networks with previous methods proposed for handling structural changes during alignment (\cite{Fremerey2010handling}, \cite{grachten2013automatic}) as well as a baseline CNN model trained without any dilation.  
\begin{comment}
\par We train our models to detect synchronous subpaths from the cross-similarity matrix between the score and  the performance. These subpaths are detected by means of ``inflection points", which encode the positions of mismatch between the two audio streams. 
\end{comment}
%The inflection points predicted by our models can then be fed to our flexible DTW framework to generate the fine alignments. 
% DTW framework that allows for jumps using the inflection points detected by our network
\begin{comment}
We conduct experiments using different dilation rates at different layers, to determine the optimal amount of dilation at each layer. We observe that progressively increasing dilation produces the best results for detecting the structural differences. We compare the results of our approach with previous methods proposed for handling structural changes during alignment \cite{Fremerey2010handling}, \cite{grachten2013automatic} as well as a baseline CNN model trained without any dilation. 
\end{comment}
%and compare the results with \begin{math} \textit{JumpDTW}\end{math} \cite{Fremerey2010handling}, \begin{math}\textit{NWTW}\end{math} \cite{grachten2013automatic}, \begin{math}\textit{MATCH}\end{math} \cite{dixon2005line} and a vanilla CNN model without dilation \begin{math}\mathit{CNN_{1+1}}\end{math}.
%We also compare our results with \textit{MATCH} \cite{dixon2005line} , a popular alignment technique.
\vspace{0.1cm}
\begin{comment}
\par The closest to our method is the work proposed by  Ullrich et al.\cite{ullrich2014boundary}, an adaptation of an onset detection method proposed by Schl{\"u}ter et al.\cite{schluter2014improved}.
While Ullrich et al.\cite{ullrich2014boundary} train a CNN as a binary classifier on spectrogram excerpts for popular music (using hand annotated data) to detect boundaries in a song, we focus on structure-aware alignment and model it as a multi-label prediction task, with the cross-similarity matrices as inputs. Additionally, we incorporate dilation in our CNN models to incorporate multi-scale context. 
\end{comment}
% put line about synthetic data
% reference for dilated CNNs / RNNs :
%\cite{chang2017dilated} - dilated RNN
%\cite{mayer2018makes} -- what makes good synthetic data
\begin{comment}
\par We train our dilated CNN models to detect the inflection points where the performance audio starts to mismatch with the score audio. Our goal is to detect all the synchronous segments between the two audio recordings, thereby yielding inflection points corresponding to jumps and repeats.
These can then be used either to generate an unrolled representation of the score, or as annotations for the JumpDTW algorithm, thereby eschewing the need for OMR and manually corrected block boundaries, and enhancing the possibilities for jumps.
\end{comment}
%The inflection points predicted by our models can then be fed to our flexible DTW framework to generate the fine alignments. 
% DTW framework that allows for jumps using the inflection points detected by our network
 \par In order to generate the fine alignments, the inflection points predicted by our dilated convolutional models are employed as potential jump positions to assist a DTW-based alignment algorithm. We implement such an extended DTW framework, inspired by \begin{math}\textit{JumpDTW}\end{math}  \cite{Fremerey2010handling}, to allow for jumps between the synchronous subpaths.  
 %We utilize our model predictions as the inflection points, as opposed to using manually annotated block boundaries to signify jump and repeat directives, as \begin{math}\textit{JumpDTW}\end{math} does.
We assume $X$= $(x_1, x_2,..., x_p)$ to be the feature sequence corresponding to the performance and $Y$ = $(y_1, y_2,..., y_q)$ to be the feature sequence corresponding to the score. Furthermore, let ($a_i$, $b_i$) denote the ($x$, $y$) co-ordinates of the $i_{th}$ inflection point, and $N$ denote the total number of inflection points. The odd numbered inflection points correspond to the end of the synchronous subpaths and the even numbered points correspond to the beginning of the subpaths. We modify the classical DTW framework to extend the set of possible predecessor cells for the cell $(a_i, b_i)$ for all \begin{math} i \in \{2, 4, 6, .., N\} \end{math}, as follows:
\vspace{-0.2cm}
\begin{equation}
D(m, n)  = e(m, n) + min\begin{cases}
D(m, n-1) \\ D(m-1, n) \\  D(m-1,  n-1) \\
D(a_{i-1}, b_{i-1}) \hspace{0.1cm} \forall (m, n) = (a_i, b_i),\\ 
\hspace{2.2cm} i \in \{2, 4, ..., N\}
\end{cases}
\end{equation}
\begin{comment}
\begin{adjustbox}{max width=\columnwidth}
\begin{equation}
$D(m, n)  = e(m, n) + min$\begin{cases}
D(m, n-1) \\ D(m-1, n) \\  D(m-1,  n-1) \\
D(a_{i-1}, b_{i-1}) \hspace{0.1cm} \forall m=a_i, i \in \{2, 4, ..., N\}
\end{cases}
\end{equation}
\end{adjustbox}
\end{comment}
\begin{comment}
 We assume $A$ and $B$ to be the sets comprising the $x$ co-ordinates and $y$ co-ordinates of the inflection points predicted by our dilated CNN models, wherein the performance and score progress in the $X$ and $Y$ direction respectively. Formally, this can be expressed as follows: \begin{equation*} A=\{a_i \hspace{0.1cm} | \hspace{0.1cm} i \in [1 :N]\}  \hspace{0.5cm} and \hspace{0.5cm} 
B=\{b_i \hspace{0.1cm} | \hspace{0.1cm} i \in [1 :N]\}\end{equation*} 
%wherein $a_i$ and $b_i$ correspond to the beginning index and the end index of each synchronous subpath detected by our model, and $N$ is the total number of inflection points.
wherein ($a_i$, $b_i$) denotes the ($x$, $y$) co-ordinates of the $i_{th}$ inflection point, and $N$ is the total number of inflection points.
We extend the set of possible predecessor cells for the cell $(a_i, b_i)$, for all \begin{math} i \in \{2, 4, 6, .., N\} \end{math}; as follows: \\

%\begin{math}
{P^{ext}}_{a_i,b_i} = P_{a_i,b_i} \cup (\{a_{i-1}, b_{i-1}) | b \in B\} \cap P)
%\end{math}
\begin{equation}
%\begin{math}
{P^{ext}}_{a_i,b_i} = P_{a_i,b_i} \cup \{(a_{i-1}, b_{i-1}) \hspace{0.1cm} | \hspace{0.1cm} i \in \{2, 4, 6, .., N\}
%\end{math}
\end{equation}
where \begin{math}P_{a_i, b_i}\end{math} is the set of possible predecessor cells for the cell \begin{math}(a_i, b_i)\end{math} generated using classical DTW.
\end{comment}
where $e(m, n)$ is the Euclidean distance between points $x_m$ and $y_n$, and $D(m, n)$ is the total cost to be minimized for the path until the cell $(m, n)$. The path which yields the minimum value for $D(p, q)$ is taken to be the optimal alignment path between the performance and score sequences.
%We thus compute the alignment using the extended DTW framework allowing jumps from ${P^{ext}}_{a_i, b_i}$ to the cell $(a_i, b_i)$, for all $i \in \{2, 4, 6, .., N\}$

\begin{figure*}%
    \centering
   \vspace{-1.0cm}
   % was -0.8
  {{\includegraphics[width=2.9cm, height=2.7cm]{images/b_dist.png} }}%
    \qquad
    %\subfloat[label 2]
    {{\includegraphics[width=2.9cm, height=2.7cm]{images/b_path_jump2.png} }}%
    \qquad
    %\subfloat[label 3]
    {{\includegraphics[width=2.9cm, height=2.7cm]{images/b_path_nwtw.png} }}%
    \qquad
    %\subfloat[label 4]
    {{\includegraphics[width=2.9cm, height=2.7cm]{images/b_path_ACNN.png} }}%
    \qquad
    %\subfloat[label 4]
    {{\includegraphics[width=2.9cm, height=2.7cm]{images/b_path_GT.png} }}%
    \qquad
    
    %---
   % \subfloat[Input]
    {{\includegraphics[width=2.9cm, height=2.6cm]{images/8_dist.png} }}%
    \qquad
%    \subfloat[JumpDTW]
    {{\includegraphics[width=2.9cm, height=2.6cm]{images/8_path_jump1.png} }}%
    \qquad
 %   \subfloat[NWTW]
    {{\includegraphics[width=2.9cm, height=2.6cm]{images/8_path_nwtw.png} }}%
    \qquad
 %   \subfloat[DCNN]
    {{\includegraphics[width=2.9cm, height=2.6cm]{images/8_path_ACNN_1.png} }}%
    \qquad
 %   \subfloat[Ground Truth]
    {{\includegraphics[width=2.9cm, height=2.6cm]{images/8_path_GT.png} }}%
    \qquad
    \subfloat[Input]
    {{\includegraphics[width=2.9cm, height=2.8cm]{images/a_dist.png} }}%
    \qquad
    \subfloat[\begin{math}\textit{JumpDTW}\end{math}]
    {{\includegraphics[width=2.9cm, height=2.8cm]{images/a_path_jump3.png} }}%
    \qquad
    \subfloat[\begin{math}\textit{NWTW}\end{math}]
    {{\includegraphics[width=2.9cm, height=2.8cm]{images/a_path_nwtw.png} }}%
    \qquad
    \subfloat[\emph{DCNN}${}_{2+3}$]
    {{\includegraphics[width=2.9cm, height=2.8cm]{images/a_path_ACNN.png} }}%
    \qquad
    \subfloat[Ground Truth]
    {{\includegraphics[width=2.9cm, height=2.8cm]{images/a_path_GT.png} }}%
    \qquad
    
    \vspace{-0.1cm}
    \caption{Comparison of our alignment path with standard methods. \\Input: Cross-similarity matrix between score and performance, 
    X-axis: Frame index (performance), Y-axis: Frame index (score)}%
    \vspace{-0.5cm}
    \label{fig:comparison}%
\end{figure*}
\vspace{-0.3cm}
\section{Experiments and Results}\label{experiments}
\vspace{-0.2cm}
\subsection{Experimental Setup}\label{setup}
\vspace{-0.1cm}
%Our model combines standard as well as dilated convolution. 
We model the task of detecting the synchronous subpaths as a multi-label prediction task using progressively dilated CNNs, with each output label encoding a deviation in the performance from the score. 
A key challenge in modelling structural changes for alignment is the lack of hand annotated data, marked for repeats and jumps accurately at the frame level. This is also one of the caveats of \begin{math}\textit{JumpDTW}\end{math}, which is reliant on the accuracy of the OMR system to detect jump and repeat directives in the absense of manually annotated boundaries.
To overcome the lack of annotated training data, we generated synthetic samples containing jumps and repeats using the audio from the MSMD \cite{dorfer2018learning} dataset. 
%We then concatenated real samples from the Tido dataset to generate our complete training set.
Our training dataset contains 2625 pairs of audio recordings, corresponding to the MIDI score and performance respectively. 2475 of these are obtained from the MSMD dataset, with each piece utilized 4 times for varying number of repetitions (generated synthetically), and once without any repetition. In addition to synthetic data, we employed a small amount of hand annotated data from a private dataset procured from Tido UK Ltd., referred to as the Tido dataset further in the paper. The training set taken from the Tido dataset comprises audio pairs for 150 pieces, 80 of which contain structural differences. In addition to our models trained with different dilation rates on the entire training set, we also demonstrate the results obtained by our progressively dilated CNN model trained exclusively on synthetic data (\emph{DCNNsyn}${}_{2+3}$).
%\vspace{0.2cm}
\par We compute the cross-similarity matrix for each performance-score pair using the Euclidean distance between the chromagrams corresponding to the score and performance respectively. We employ librosa\cite{mcfee2015librosa} to compute the chromagrams as well as the cross-similarity.
%different features, namely chroma-based features, CQT representation, multimodal embeddings \cite{balke2019learning} and pitch-based features. 
%%%Our model is compatible with any feature representation, and our choice is motivated by the ability to compare with previous methods. 
%and the one we employed for our experiments is the chroma-based feature representation \cite{muller2006efficient}, in order to facilitate comparison with \textit{JumpDTW} and \textit{NWTW}. 
Our models consist of three convolutional and subsampling layers, with standard convolutions at the first layer and dilated convolutions with varying dilation rates at the second and third layer respectively. The output of the third convolutional and subsampling layer is sent through a flatten layer, following which it is passed through two fully connected layers of size 4096 and 1024 respectively to predict the (x, y) co-ordinates of the inflection points. The output of the final layer is a one-dimensional tensor of size 64, signifying that the model can predict up to 32 inflection points, with their $(x, y)$ co-ordinates in chronological order. The output is compared with the ground truth using the L2 regression loss, since we want to capture the distance of the predicted inflection points from the ground truth inflection points in time. The outputs of each layer are passed through rectified linear units to add non-linearity, followed by batch normalization before being passed as inputs to the next layer. We employ a dropout of 0.5 for the fully connected layers to avoid overfitting. Our batch size is 64 and the models are trained for 40 epochs, with early stopping. Our dilated CNN models are denoted as  \emph{DCNN}${}_{m+n}$, where $m$ and $n$ correspond to the dilation rates at the second and third layer respectively. 
%\vspace{0.2cm}
\par We test the performance of our models on two different datasets, both containing recordings of real performances. We demonstrate the results of our models on the publicly available Mazurka dataset \cite{sapp2007comparative}. In order to analyze specific improvements for structurally different pieces, we also demonstrate results on subsets of the Tido dataset \emph{with} and \emph{without} structural differences. Both the subsets contain 75 pieces each. We compare the results obtained by our models with \begin{math} \textit{JumpDTW}\end{math} \cite{Fremerey2010handling}, \begin{math}\textit{NWTW}\end{math} \cite{grachten2013automatic}, \begin{math}\textit{MATCH}\end{math} \cite{dixon2005line} and a vanilla CNN model without dilation     \emph{CNN}${}_{1+1}$. The number of parameters of  our \emph{DCNN}${}_{m+n}$ networks is comparable with that of the baseline \emph{CNN}${}_{1+1}$ network.
For comparison with \begin{math}\textit{JumpDTW}\end{math}, we employ the SharpEye OMR engine to extract frame predictions for block boundaries from the  sheet images \cite{Fremerey2010handling}. These are then passed on to an implementation of \begin{math}\textit{JumpDTW}\end{math} to generate the alignment path. 
Similarly, we compare our models with an implementation of the \begin{math}\textit{NWTW}\end{math} method, and estimate the optimal gap penalty parameter $\gamma$  \cite{grachten2013automatic} on our data.
\vspace{-0.4cm}
\subsection{Results and Discussion}\label{results}
\vspace{-0.2cm}
%We conduct experiments using different number of convolutional layers, as well as different dilation rates for our fully convolutional models. For our CRNN models, we conduct experiments using bi-LSTM and LSTM layers. 
The results obtained by our models are given in Table \ref{results2}. We report alignment accuracy in \%, %on the subsets of the Tido dataset, for performance-score pairs \textit{with} and \textit{without} structural differences. Both the subsets contain 75 pieces each. We also report results on the publicly available Mazurka dataset \cite{sapp2007comparative}.
where each value denotes the percentage of beats aligned correctly within the corresponding time durations of 25, 50, 100 and 200 ms respectively.
Our models show an increase of 2-5\% in alignment accuracy over \begin{math}\textit{JumpDTW}\end{math} and \begin{math}\textit{NWTW}\end{math} on the test subset containing structural differences and an increase of 1-3\% on the test subset not containing structural differences. Compared with \begin{math}\textit{MATCH}\end{math}, our models show an increase of 9-10\% on the subset with structural differences, and an increase of 1-2\% on the subset without structural differences. Overall accuracy on the Mazurka dataset suggests that our models perform better than \begin{math}\textit{MATCH}\end{math} by 4-6\% as well as the \begin{math}\textit{JumpDTW}\end{math} and \begin{math}\textit{NWTW}\end{math} frameworks by 1-4\% (Table 1, columns 1-4).
%Overall accuracy on the Mazurka dataset suggests that our models outperform \begin{math}\textit{MATCH}\end{math}  as well as the \begin{math}\textit{JumpDTW}\end{math} and \begin{math}\textit{NWTW}\end{math} frameworks.
Our model trained exclusively on synthetic data (\emph{DCNNsyn}${}_{2+3}$) yields better alignment accuracy than \begin{math}\textit{JumpDTW}\end{math}, which requires manually labelled block boundaries to handle repeats and jumps \cite{Fremerey2010handling}. This emphasizes the applicability of our method in real-world scenarios with scarce availability of hand-annotated data. Our models noticeably outperform all methods when a limited amount of real data is added to the synthetic data during training (Table 1, rows 5-7).
\vspace{0.1cm}
\par The experimentation with different dilation rates reveals that progressively increasing dilation as we move deeper (\emph{DCNN}${}_{2+3}$) yields better results than models trained using equal amounts of dilation (\emph{DCNN}${}_{2+2}$, \emph{DCNN}${}_{3+3}$). Models trained with dilation at the first layer and those trained using dilation rates of 4 and higher did not yield improvement over the vanilla CNN model \emph{CNN}${}_{1+1}$ and hence are not reported. We speculate that progressively increasing dilation helps the model learn higher level features better further down the network. Manual inspection of the alignments confirmed that long-term context was better captured by the progressively dilated CNNs than other models, and they could detect larger deviations in addition to short ones. We demonstrate the alignment paths generated by the comparative methods for three performance-score pairs in Figure \ref{fig:comparison} to facilitate qualitative understanding of our results. %It can be seen that \begin{math}\mathit{DCNN_{2+3}}\end{math} generates alignments closest to the ground truth.
While \begin{math}\textit{JumpDTW}\end{math} struggles with deviations from the score, \begin{math}\textit{NWTW}\end{math} struggles with cases containing both forward and backward jumps (Fig. \ref{fig:comparison}, example 1). This could be attributed to the waiting mechanism of \begin{math}NWTW\end{math}, which makes backward jumps especially challenging. Our model \emph{DCNN}${}_{2+3}$ struggles with cases where there are multiple deviations within a short time span (Example 2). We speculate that this is due to the larger receptive fields of the dilated convolutions, which, while capturing greater context, are sometimes unable to capture multiple inflection points within a small context. Our model is specifically able to model forward jumps better than both the methods  (Example 1), while also handling deviations not foreseeable from the score (Example 3).
\vspace{-0.1cm}
%\section{Conclusion}
\par We demonstrate that progressively dilated convolutional neural networks are effective at detecting structural differences between the score and the performance for structure aware audio-to-score alignment. While we used chroma-based features for score-performance audio pairs, our method can also be used with raw or scanned images of sheet music using learnt features, for instance, using multimodal embeddings trained on audio and sheet image snippets \cite{dorfer2018learning}, \cite{balke2019learning}. Additionally, our method can also be utilized by frameworks other than DTW to generate the alignments. For instance, an unrolled score representation could be achieved via the inflection points predicted by our model, which could further be employed by score following approaches based on reinforcement learning \cite{dorfer2018learning2}, \cite{henkel2019score} for structure-aware tracking.
The advantage of our method is that it does not require manually labelled block boundaries, and can effectively deal with deviations from the structure given in the score, in both the forward and backward directions.
%We employ measure boundaries estimated by the OMR system proposed by \cite{waloschek2019identification}.
 In the future, we would like to experiment with parallel  dilation using different dilation rates and merging the learnt features. A current limitation of our method is the handling of trills and cadenzas, and we would also like to address these issues in future research. 
 %We would also like to incorporate score information in our model, to give preference to jumps to and from  musically relevant positions.\
 %Additionally, we would like to explore the generation of synthetic data using deep models like generative adversarial networks or variational autoencoders.
\bibliographystyle{IEEEbib}
\bibliography{refs}

\end{document}
