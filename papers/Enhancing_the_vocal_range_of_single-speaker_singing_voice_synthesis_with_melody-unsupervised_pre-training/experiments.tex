\section{Experimental Setup}
\label{sec:expt-setup}

\subsection{Datasets}
% 实验部分分为
% 3.1 数据集介绍
% 在这篇文章中
%  a. opencpop 数据集介绍
%  b. opensinger数据集介绍 
% 3.2 系统设置介绍
%  a.  Baseline ： 介绍Baseline的模型，基于VISinger搭建起来的端到端歌声合成模型。In VISinger, the embedding dimension of the input features is 192. Each note pitch is converted into a pitch ID following the MIDI stan- dard [17], while the note duration is converted into frame count by the sampling rate, window length and hop length. Specifi- cally, given note duration dur, sampling rate sr, window length wl and hop length hl, the frame count frame is calculated as: xxx。 Text embedding xxx
%  b.  Proposed
%  The proposed method in this paper. 预训练阶段所有特征都是frame-level，finetune阶段通过duration predictor 将phoneme-level特征扩展为frame-level特征。预训练阶段我们将
%  c.  训练设置
% 3.3 实验结果
%    3.3.1 MOS + 客观指标
%      
%    3.3.2 消融实验
%    3.3.3 案例学习
The experiments are conducted on two open-sourced Mandarin singing corpus, named OpenSinger \cite{huang2021multi} and Opencpop \cite{wang2022opencpop}. The OpenSinger dataset is used for pre-training, it contains 50 hours of pop songs, including 30 hours from 41 females and 20 hours from 25 males. The singing data is saved in the WAV format, sampled at 24 kHz, and quantized by 16 bits.
The Opencpop dataset is used for fine-tuning and testing the system. 
It consists of 100 popular Mandarin songs sampled at an audio rate of 44.1 kHz and recorded by a female professional singer. The recordings have been annotated with phoneme, note pitch, note duration and phonetic timing information. All audio is down-sampled to 24 kHz with 16-bit quantization for experiments. The whole dataset consists of 3,756 audio clips, with a total duration of around 5.2 hours. 3550 clips of them are selected for training while the other 206 utterances are for testing.

\begin{table*}[th]
\normalsize
\renewcommand{\arraystretch}{1.2}
  \caption{Experimental results in terms of subjective mean opinion score (MOS) with 95\% confidence intervals and four objective metrics.}
  \setlength{\tabcolsep}{2mm}{
  \label{tab:mos}
  \centering
  \begin{tabular}{l|cccccc} %l@{}l  r r
    \toprule
    \textbf{Model} &\textbf{Sound quality}&\textbf{Naturalness}&\textbf{F0 MAE $\downarrow$} & \textbf{F0 Correlation (Hz) $\uparrow$}& \textbf{Duratin MAE $\downarrow$} & \textbf{Speaker Similarity $\uparrow$}\\
    \midrule
    Baseline & $3.350\pm0.103$& $3.317\pm0.104$& $6.207~(11.270)$ &$0.984~(0.972)$&$2.262$&$0.862$  \\
    Proposed & \bm{$3.708\pm0.099$}& \bm{$3.667\pm0.102$}& \bm{$6.051~(8.118)$}&\bm{$0.986~(0.984)$}&\bm{$2.086$}&\bm{$0.884$}  \\
    \midrule
    Recording & $4.854\pm0.058$ & $4.833\pm0.057$ & $-$ & $-$ & $-$ & $-$\\
    \bottomrule
  \end{tabular}}
\end{table*}

\begin{figure*}[htbp]
	\centering
	\subfigure[Ground truth] {\includegraphics[width=.28\textwidth]{img/gt-2093003449.png}}
	\subfigure[Proposed] {\includegraphics[width=.28\textwidth]{img/proposed-2093003449.png}}
	\subfigure[Baseline] {\includegraphics[width=.28\textwidth]{img/baseline-2093003449.png}}
	\caption{Visualizations of spectrograms and pitch contours in three systems: Ground truth, Proposed, and Baseline. The blue line represents the pitch contour. The numbers on the right represent the range of pitches (Hz) and the pitch value at the red line.}
	\label{case}
\end{figure*}
\subsection{System Configuration}
Two systems are constructed for evaluating the effectiveness of the proposed method.

\textbf{Baseline:}
The baseline model follows the structure of the VISinger system \cite{zhang2021visinger} and also adopts the improvements proposed by our previous work \cite{zhou22f_interspeech}.
The dimension of pitch, phoneme, and duration embedding is 192.
Each note pitch is converted into a pitch ID following the MIDI standard\cite{midi}, while the note duration is converted into the number of frames. 
The dimension of text embedding derived from BERT is 768 and converted into 192 by an additional linear layer in the text encoder. 
% Specifically, given note duration $dur$, sampling rate $sr$, window length $wl$ and hop length $hl$, the frame count $frame$ is calculated as:
% \begin{align}
%     frame &= [(dur * sr) - wl]/hl + 1
% \end{align}
% All the embeddings of the input features are summed and fed into the model. 
The note encoder contains 6 FFT blocks, and the duration predictor consists of 3 one-dimensional convolutional networks.
Moreover, the pitch and energy predictors are designed to predict the pitch and energy of each frame, respectively. They are configured as \cite{zhou22f_interspeech} and output 192-dimensional embedding vectors.

\textbf{Proposed:}
The proposed method adopts all the contributions aforementioned.
% All features in the pre-training stage are frame-level, and the finetune stage extends phoneme-level features to frame-level features by duration predictor. 
% Our proposed model is first pre-trained on the Opensinger dataset before being fine-tuned on the Opencpop dataset.
The phoneme vocabulary size is 61.
We pre-processed the phonemes in OpenSinger since they differ from those in the Opencpop dataset.
Specifically, we re-extract the phonemes from the transcripts provided by the dataset using the ``pypinyin'' algorithm.\footnote{Source-code: \href{https://github.com/mozillazg/python-pinyin}{https://github.com/mozillazg/python-pinyin}}
The other hyperparameter settings are consistent with those in the baseline.

The proposed method is firstly pre-trained on OpenSinger with 100k steps, then fine-tuned by 200k steps on Opencpop. The baseline model is trained up to 300k steps on Opencpop. 
We adopt the AdamW\cite{loshchilov2017decoupled} optimizer with $\beta_1$ = 0.8, $\beta_2$ = 0.99, $\epsilon$ = $10^{-9}$. The initial learning rate is set to $1e^{-4}$, with a learning decay of $0.999875$.
All the models are trained on 2 Nvidia Tesla A100 devices, and the batch size on each GPU is 8.




\section{Experiment Results}
\label{sec:expt-rst}

\subsection{Performance Comparison}
The mean opinion score (MOS) test is conducted to evaluate the quality and naturalness of the synthesized singing voices.
For each system, we randomly select 10 audio samples, with each sample within about 10 seconds. 
24 Chinese native speakers are recruited to rate the testing samples by a score from 1 to 5 (the higher, the better), with a 1-point interval. 
% Samples that are identical to the ground-truth samples will receive a score of 5.
As shown in Table.\ref{tab:mos}, our method achieves a better MOS of 3.708 in sound quality and 3.667 in naturalness, exceeding baseline by 0.358 and 0.350.
It demonstrates that the proposed method can improve the quality and naturalness of the synthesized singing voice effectively.

We further calculate some objective metrics including F0 mean absolute error (F0 MAE), duration mean absolute error (Duration MAE), and F0 correlation\cite{lu2020xiaoicesing}.
These three metrics represent the accuracy of pitch and duration prediction.
% For the duration, we compute the MAE between the predicted duration and ground-truth duration.
As shown in Table.\ref{tab:mos}, our proposed method outperforms the baseline in all three metrics mentioned above. 
This demonstrates that the use of the melody-unsupervision pre-training method to expand the vocal range and the use of differentiable up-sampling layer can lead to more accurate predictions of pitch and duration, respectively
% It demonstrates that using the melody-unsupervision method to extend the vocal range and indicates that our proposed method can restore more accurate characteristics, such as pitch and duration.
% indicate that the proposed method can synthesize singing voice with more precise pitch, which indicates the effectiveness of the pre-train method in extending the vocal range of SVS model.
% Besides, the proposed method achieves better results in duration MAE compared to the baseline model, showing the differentiable up-sampling layer can mitigate the rhyme problem effectively.

Moreover, we also calculate the speaker similarity between synthesized and ground-truth waveforms.
We first apply the pre-trained speaker recognition model released in\cite{jia2018transfer} to extract the speaker embedding of the waveform.
Then the cosine similarity is performed between the speaker embeddings of the synthesized singing voice and the ground-truth waveform, which reflects the speaker similarity. 
As can be seen in Table.\ref{tab:mos}, the proposed method achieves better speaker similarity compared with the baseline, showing that using speaker encoder to model timbre information can improve the timbre similarity of synthesized singing voice.

To further verify the effectiveness in extending the pitch range, we selected audio segments from the testing set that contained pitches below F3 or above D5 and recalculated the F0 metrics.
As shown in the bracketed data of `F0 MAE' and `F0 Correlation' in Table.\ref{tab:mos}, the proposed method has a much larger gap over the baseline method when synthesizing high- or low-pitched singing voice, verifying the validity of our contribution.


\subsection{Ablation Study}
To demonstrate the effectiveness of different contributions in the proposed method, we carry out three ablation studies.
The comparison mean opinion score (CMOS) is used to compare the synthesized singing voices in terms of both naturalness and quality.
The naturalness and quality are evaluated by a single CMOS score to reflect the overall impact of each proposed contribution and also save labor efforts.
% We considered naturalness and quality in order to evaluate the whole effect of the proposed method on the synthesized singing voice.
As shown in Table.\ref{tab:cmos}, the absence of pre-training, bi-direction flow and differentiable duration regulator results in -0.857 CMOS, -0.335 CMOS and -0.143 CMOS, respectively.
This demonstrates the validity of the above three contributions in improving the quality and naturalness of synthesized singing voice.
The audio used for comparison in the ablation study can be heard on our demo page.
% the absence of pre-training results in -0.857 CMOS, demonstrating that the pre-training strategy enhances the vocal range of the SVS model thus improving the naturalness and quality of synthesized voice. 
% Removing of the bi-directional flow model results in -0.335 CMOS, showing that the proposed module can effectively alleviate the training and inference mismatch problem.
% Furthermore, removing the differentiable up-sampling layer resulted in -0.143 CMOS.
% This indicates the effectiveness of mitigating the rhythm problem.



\begin{table}[th]
\renewcommand{\arraystretch}{1.1}
  \caption{Ablation study results.}
  \begin{center}

  \setlength{\tabcolsep}{4mm}{
  \label{tab:cmos}
  \centering
  \begin{tabular}{l|c} %l@{}l  r r
    \toprule
    \textbf{Model} &\textbf{CMOS} \\
    \midrule
    Proposed & $0.000$ ~~~\\
     - pretrain & - $0.857$ ~~~  \\
     - bi-direction flow & - $0.335$ ~~~  \\
     - differentiable duration regulator& - $0.143$ ~~~  \\
    \bottomrule
  \end{tabular}}
        
  \end{center}
\end{table}

\subsection{Case Study}
To demonstrate the impact of the aforementioned contributions, a case study is conducted to synthesize a testing sample that contains pitch values of limited training data.
% we further conduct a case study to synthesize a singing voice segment from the test set with low pitch. 
We compare the ground-truth, the proposed method and the baseline.
As shown in \ref{case}, the pitch is marked with blue lines and the pitch value at the red line is shown on the right. 
This sample ends with a slightly low pitch that is associated with few training data. 
It is observed that the proposed method synthesizes this pitch accurately, but the baseline method tends to incorrectly use a higher pitch to replace this one, proving that the proposed pre-training strategy is effective in enhancing the vocal range.
