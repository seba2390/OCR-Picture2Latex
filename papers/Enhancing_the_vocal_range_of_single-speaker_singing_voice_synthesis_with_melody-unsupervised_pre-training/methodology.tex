
\section{The Proposed Method}
\label{sec:Method}
% 开头阐述模型图
% 如图所示，模型的训练分为两阶段，大数据集预训练以及小数据集finetune。在预训练阶段，posterior encoder 以 linear spectrogram作为输入，输出隐变量Z，隐变量Z送入Phoneme predictor。Phoneme predictor 输出Phoneme probability，与Phoneme Look Up Table 相乘得到Phoneme embedding。该embeddding作为歌声合成输入的一部分。
% 2.1 阐述预训练框架
%     标注精良的单歌手歌声数据集例如Opencpop，往往很难有较大的规模，因为标注十分耗费人力。并且由于单歌手的音域固定，所训练出的歌声合成模型很难拥有较广的音域，从而丧失了和真人演唱相比，可能具有的音域优势。为了能够利用大的歌声合成数据集以提高歌声合成系统的音域表现，受xxx文章的启发，我们基于proposed方法的框架，采用了melody-unsupervision在大的歌声合成数据集上做预训练，但又与该文章有着不同。由于我们预训练阶段使用的歌声合成数据集没有详细的标注，仅仅只有text、phonemes，但并没有wav在时间上的详细标注，因此我们希望能够借助ASR的训练方式，通过Phoneme Predictor预测出每一个frame的音素注意力向量 。该音素注意力向量与Phonemes lookup table相乘，最终得到了frame-level的Phonemes embedding。此外，我们从wav中提取出连续的音高，并且将其量化为乐谱音高，经过embedding layer后得到frame-level的pitch embedding。同时由于Opensinger是一个多歌手的数据集，我们使用了基于ECAPA-TDNN的speaker encoder去建模不同歌手的音色信息。
% 2.1.1 Phonemes predictor
% 由于数据集没有Phonemes 的time alignments信息，因此我们采用ASR的训练方法，在pronunciation 层面上使用CTC Loss对Phonemes predictor进行训练。具体而言，Phonemes predictor包含两层FFT Blocks ，一层线性层。线性层将hidden channels 映射到Phonemes 总数对应的类别数。对于线性层的输出，我们取softmax后得到每一个phonemes的概率 p，作为注意力向量与phonemes lookup table相乘得到phonemes embedding。同时我们对p取log，与ground truth标签计算CTC Loss。值得注意的是，与xxx工作不同，由于预训练时没有duration信息，我们的phonemes predictor输出的注意力向量是frame-level的。
% 2.1.2 阐述fine-tune架构
%     finetune架构的搭建整体基于proposed 方法，在proposed 方法的基础上，提出了 d-durator 和bi-flow，提升了我们模型的性能。
% 具体来说，在finetune阶段，模型读取预训练模型的checkpoints，并且在OpenCpop上进行finetune。由于事先在大规模数据集进行了预训练，模型的可合成音域得到了提升。
% 2.2 阐述可微分的上采样模块
% 以前的歌声合成模型，大多采用简单的复制操作将phoneme-level信息转变为frame-level信息，导致模型存在韵律问题。受xxx工作启发，我们提出了可学习的时长预测器，包含一个时长预测器以及可学习的上采样层。时长预测器输出每一个phonemes占发声总时长的比例，该比例与note duration相乘，送入可微分的上采样层。该上采样层 takes a phoneme hidden sequence as input, and outputs a sequence of prior distribution at the frame level。 Compared to simply repeating each phoneme hidden sequence with the predicted duration in a hard way, the differentiable upsampling layer enables more flexible duration adjustment for each phoneme. Also, the differentiable upsampling layer makes the phoneme to frame expansion differentiable, and thus can be jointly optimized with other modules in the TTS system.
% 2.3 阐述双向flow层
% 原先的工作中，flow层在训练阶段将复杂的后验分布映射为简单的先验分布，并且在推理阶段将简单的先验分布转换为复杂的后验分布。但这里存在着training和inference时候的mismatch，也就是 train in backward direction but infer in forward direction。因此我们在训练的时候提出了一个双向的flow模型，对posterior encoder预测的后验分布以及prior encoder预测的先验分布进行转换，并计算loss。值得一提的是，我们发现将flow model两边的kl loss
The training stage of the proposed model consists of two steps: the multi-singer pre-training step and the single-singer fine-tuning step. The architecture of the proposed model is illustrated in Fig.\ref{fig: architecture}, which consists of a prior encoder, a posterior encoder, and a decoder together with a discriminator.
The proposed model is designed from our previous work \cite{zhou22f_interspeech} with the following modifications.
The posterior encoder utilizes a phoneme predictor to predict frame-level phoneme probabilities in the pre-training step. 
The prior encoder adds a speaker encoder to model the timbre variations, replaces the length regulator with a differentiable duration regulator to improve the rhythm naturalness, and upgrades the flow module to be bi-directional to improve the sound quality.

% As illustrated in Fig.\ref{fig: architecture}, the training of the proposed model consists of two stages: the pre-training stage and the fine-tuning stage. 

% In the pre-training stage, the posterior encoder takes the linear spectrogram as input and predicts the latent representation $z$. 
% The phoneme predictor estimates the frame-level phoneme probability $p$ given the latent representation $z$. 
% We multiply $p$ with the phoneme lookup table to get the phoneme embedding. The pitch is extracted from the waveform and we quantified it into note pitch. 

% we reload the checkpoint of the pre-train model and resume training in the OpenCpop\cite{wang2022opencpop} datasets. 


\subsection{The Melody-Unsupervised Multi-Singer Pre-Training Step}
Since the multi-singer training data has no phonemic timing information, in the pre-training step, this work utilizes the automatic speech recognition (ASR) training strategy to train a phoneme predictor in the posterior encoder and predict the frame-level phoneme probabilities $p$.
The probability vectors are multiplied with the phoneme look-up table to obtain the frame-level phoneme embeddings.
In addition, the continuous pitch $f_{0}$ is estimated from the audio and quantized into the note pitch.
The note pitch is passed through the embedding layer to obtain frame-level pitch embeddings.
Moreover, we apply a speaker encoder to extract frame-level speaker embeddings to model the timbre variations of different singers.
Since the pre-training step directly deals with estimated pitch values and focuses on enhancing the vocal range, the pitch predictor, the energy predictor and the duration-related modules are dropped during the pre-training step.


\subsubsection{Phoneme predictor}
We train the phoneme predictor using the connectionist temporal classification (CTC) \cite{graves2006connectionist} loss. 
It contains two layers of FFT blocks and one linear layer.
The linear layer maps the hidden channels to the number of phoneme categories. 
We obtain the probability vector $p$ after taking the softmax operation on the linear layer's output, then multiply it with the phoneme look-up table to get frame-level phoneme embeddings.
Meanwhile, we take the log function of $p$ to compute the CTC loss with the ground truth phoneme sequences. 

\subsubsection{Speaker encoder}
This work adopts one of the state-of-the-art speaker recognition models, i.e. ECAPA-TDNN \cite{desplanques2020ecapa}, as the speaker encoder. Its advanced network architecture and attentive statistics pooling layer have shown great effectiveness in both speaker recognition \cite{desplanques2020ecapa} and voice conversion \cite{guo2022improving,li2022hierarchical}. The speaker encoder is configured as the one with 512 channels in Table 1 of \cite{desplanques2020ecapa}, and it extracts 192-dimensional frame-level speaker embeddings from the audio's Mel-Spectrograms. These embeddings are given as the speaker condition in the multi-singer pre-training step. 

\subsection{The Single-Singer Fine-Tuning Step}
In the fine-tuning step, it loads the pre-trained model parameters, then utilizes the single-singer Opencpop dataset to fine-tune model parameters.
It uses the phoneme and note-pitch annotations provided by the dataset to derive the phoneme and pitch embeddings, instead of using the phoneme probability vectors and quantized f0 values in the pre-training step.
Note that the phoneme and note-pitch annotations are at the phoneme level, rather than the frame level, such that a duration regulator after the note encoder is necessary to up-sample the embedding vectors into the frame level.
As for the speaker embedding, we use the pre-trained speaker encoder to extract an averaged speaker embedding over the Opencpop dataset, then utilize it as a fixed speaker condition during the fine-tuning step.
Moreover, the energy predictor and the pitch predictor join the fine-tuning process to enhance the expressiveness and pitch accurateness of the synthesized samples, following our previous work\cite{zhou22f_interspeech}.
% The fine-tuning architecture is built based on the proposed method \cite{zhou22f_interspeech}, and on top of the proposed method, a differentiable duration predictor and bi-directional flow model are proposed to improve the performance of the synthesized singing voice.
% Specifically, in the fine-tuning phase, the model reloads the checkpoints of the pre-trained model and performs inference on OpenCpop.
The synthesizable vocal range of the model is enhanced due to the multi-singer pre-training on a large-scale dataset.

\subsection{Differentiable Duration Regulator}
Most previous SVS systems simply replicate each phoneme hidden representation with the predicted duration in a hard way, which may degrade the rhythm naturalness.
Inspired by \cite{tan2022naturalspeech}, we leverage a differentiable duration regulator, which contains a duration predictor and a differentiable up-sampling layer. 
The duration predictor outputs the ratio of each phoneme to the corresponding note duration, then the ratio is multiplied by the note duration and fed to the differentiable up-sampling layer.
The differentiable up-sampling layer leverages the predicted duration to learn a projection matrix to extend the phoneme hidden sequence from the phoneme level to the frame level.
It makes the phoneme-to-frame expansion differentiable and thus can be jointly optimized with other modules in the system.

\subsection{Bi-directional Flow}
In the previous work \cite{zhou22f_interspeech}, the flow model maps the complex posterior distribution to the simple prior distribution in the training stage while operating reversely in the inference stage.
This process suffers from the mismatch problem between the training and inference stages.
Therefore, we leverage a bi-directional flow module\cite{tan2022naturalspeech} during training, which bridges the complex posterior distribution and the simple prior distribution bi-directionally to alleviate the mismatch issue in the inference stage.
% not only maps the complex posterior distribution to the simple prior distribution during training, but also maps the simple prior distribution to the complex posterior distribution, in order to improve the quality of the synthesized singing voice.
It is worth noting that we observed that the system can easily fail to train and encounter gradient explosion when the KL losses on both sides of the flow contribute equally, so we define the reverse KL loss weight as 0.5.




