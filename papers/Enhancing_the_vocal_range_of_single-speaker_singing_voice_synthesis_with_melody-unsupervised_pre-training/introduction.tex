\section{Introduction}
\label{sec:intro}
% 第一段歌声合成的定义，应用场景、重要性、难点在哪里

% 第二段
% 1. 开头先引用几篇文章，阐述目前的歌声合成模型的基本方法，基于什么在做
% 2. 引出音域问题、阐述目前针对音域所做的一些工作
%    1. 引出音域问题，阐述音域对于歌声合成模型的重要性。（音域是虚拟歌手能够超越真人演唱的因素之一、更广的音域意味着合成歌声具有更丰富的表现力）
%    2. 目前的歌声合成模型大部分基于单歌手的数据集，受限于歌手的音域范围，很难正常合成歌手音域范围外的歌声。同时由于数据集中音高的分布大部分集中在中间某一段音域，某些较高音和较低音数据量较少，模型也难以正常合成这些声音。
% 3. 引出一些做pitch augmentation的工作，介绍他们的方法，优缺点在哪（音质、音色相似度），效果如何

% 第三段：
% 1. 我们针对音域问题，受xxx工作的启发，提出了基于预训练的方法。该方法基于xxx工作的melody-unsupervision策略，对模型在多歌手的大型歌声数据集上进行预训练。
% 2. 但是从结构上与xxx工作完全不同，首先模型结构不同，其次特征维度不同，并且由于采用了多歌手的歌声数据集，使用了speaker encoder去建模不同歌手的音色信息
% 3. 其次还根据韵律问题以及音质问题，提出了differentiable up-sampling layer以及bi-directional flow model

% 第四段：
% 总结我们的贡献点：1。。2。。3。。4。。
Singing voice synthesis (SVS) aims at synthesizing human-like singing voices given musical scores.
Unlike text-to-speech (TTS), which only considers text as input, SVS needs to generate voice with accurate pronunciation, pitch, and tempo according to the musical scores \cite{hono2018recent,gu2021bytesing}. As a subtopic of speech synthesis, SVS has been paid more and more attention due to its potential applications in virtual singers, entertainment, etc.

% pronunciation while taking pitch and tempo information from the musical score into account.
% Nowadays, singing voice synthesis, as part of speech synthesis technology, is increasingly used in technologies such as virtual avatars, smartphone assistance, and so on.
% Because they are both produced by the same human voice production system, singing voice and speech share several similarities.
% However, when compared to speech, singing voices have a broader range of pitch variation, making it more challenging to precisely synthesize pitch in a singing voice synthesis model.

With recent success in deep learning techniques, singing voice synthesis has made great progress in improving sound quality \cite{lu2020xiaoicesing,nishimura2016singing,chen2020hifisinger}.
Ren et al. \cite{ren2020deepsinger} developed a pipeline to synthesize singing voices with data mined from the web. Chen et al. \cite{huang2022singgan} proposed SingGAN which is specifically designed for singing voice vocoding. Other attempts \cite{zhang2021visinger,zhou22f_interspeech} were proposed to train the SVS system in an end-to-end manner to alleviate accumulated errors.
% Deep learning methods have achieved significant success in singing voice synthesis.
% In recent years, the two-stage SVS models \cite{ren2020deepsinger,zhang2020dursc,blaauw2020sequence,hono2018recent,gu2021bytesing,chen2020hifisinger} are the mainstream architecture of SVS system, and 
% % , while it suffers from a mismatch problem due to separate training of neural acoustic model and neural vocoder
% some SVS models\cite{zhang2021visinger,zhou22f_interspeech} are proposed to train in an end-to-end manner.
%  % in order to alleviate this problem
Human singing voices are usually limited by the singer's vocal range because they cannot produce pitch values out of their vocal ranges.
Similarly, the single-singer SVS system has the same problem because the training data is limited to a specific vocal range as well.
Moreover, the single-singer SVS system also fails at pitch values that are associated with limited training samples, even if they are within the singer's vocal range.
To extend the vocal range of the SVS model, \cite{zhang2022wesinger} tried pitch shifting as a data augmentation method. 
The pitch of each song is either increased or decreased by one semitone to enlarge the vocal range of the training data.
However, they claimed that this method could lead to somewhat perceptible changes in timbre.

To explore the potential ability to develop virtual singers with wide vocal ranges, we extend our previous work \cite{zhou22f_interspeech} with a melody-unsupervised pre-training method conducted on a large-scale multi-singer dataset to enhance the vocal range of a single-speaker SVS system, while not degrading the timbre similarity.
This pre-training method can be deployed to datasets that only contain audio-and-lyrics pairs without phonemic timing information and pitch annotation.
% 首先讲我们的melody-unsupervision method是如何提供phonemes信息和音高信息的
% 首先对于Phonemes 信息，我们使用ASR的训练方法，利用数据集提供的phonemes 标签，训练了一个phonemes predictor，该phonemes predictor以posterior encoder输出的隐特征作为输入，输出frame-level的注意力向量，该注意力向量与phonemes look-up table相乘，得到我们的phonemes embedding。总而言之，我们训练了phonemes predictor用于提供phonemes的temporal-alignments 信息。 
% 其次对于音高信息，我们直接从wav中提取音高，并且量化为乐谱音高。
% 由于预训练数据集是一个多歌手的数据集，我们加入了speaker encoder
% 在这个基础上，我们也对合成歌声的自然度和韵律做改进，受naturalspeech的启发，我们加入了differentiable up-sampling layer以及 bi-directional flow model。据我们所知，我们的工作是第一个引入这两个模块到歌声合成工作中的。（这里可能会被argue，differentiable up-sampling layer在interspeech2020的一篇歌声合成工作中，有看见类似的上采样模块）
% 在先前的工作中，我们发现模型输出的声音存在一定的韵律问题。具体表现为：我们的模型输出的声音比应该发声的位置提前一点或者延迟一些，导致歌声听起来较为不自然。
Inspired by \cite{choi2022melody}, we design a phoneme predictor to produce frame-level phoneme probabilities as phonemic timing information and directly estimate the frame-level f0 values from the audio to provide the pitch information.
Besides, we make some further modifications in order to deploy it to our previous framework: 1) the input features for the phoneme predictor are latent features predicted by the posterior encoder, rather than the Mel-spectrograms in \cite{choi2022melody}; 2) we introduce an additional speaker encoder in the prior encoder to model the timbre variations of different singers, while \cite{choi2022melody} conducted experiments on the single-singer dataset.
It is also worth noting that the role of the melody-unsupervised training in this work is as a pre-training strategy to enhance the single speaker's vocal range, while \cite{choi2022melody} merely aims at exploring the potential feasibility of melody-unsupervised SVS training.
% Specifically, to obtain the phoneme temporal alignment information required for SVS, we adopted the ASR training method to train a phonemes predictor utilizing the phonemes labels provided by the dataset.
% The phonemes predictor takes the output of the posterior encoder and predicts the frame-level attention vector.
% The phoneme embedding for the SVS model is derived by multiplying the attention vector by the phoneme look-up table.
% For the pitch information required for SVS, we extracted f0 from the ground-truth waveform and quantified it into note pitch.
% The frame-level note pitch is then converted into pitch embedding using a phonemes look-up table.
% It is worth noting that we made some adjustments to this melody-unsupervision method based on our framework.
% First, rather than the mel-spectrogram, the input to the phoneme predictor is the frame-level hidden features predicted by the posterior encoder.
% Second, because the pre-training dataset lacks duration information, we eliminate the downsampling network to ensure that the feature dimension is always frame-level.
% Last but not the least, we adopt one of the state-of-the-art speaker recognition models, i.e. ECAPA-TDNN, as the speaker encoder to model the timbre variations in the multi-singer dataset.

Moreover, this work also contributes to improving the sound quality and rhythm naturalness of the synthesized voice. It is observed in our previous work \cite{zhou22f_interspeech} that some phonemes are pronounced slightly earlier or later than they should be, resulting in an unnatural-sounding rhythm. Motivated by \cite{tan2022naturalspeech}, we leverage a differentiable duration regulator and a bi-directional flow model to enhance the sound quality and rhythm naturalness. To the best of our knowledge, this work is the first to adopt the differentiable duration regulator and the bi-directional flow in the SVS systems. 
% Moreover, we found that the output voice of the model has rhythmic issues in previous work.
% Sometimes some pronunciations are slightly earlier or later than they should be, resulting in an unnatural-sounding tune.
% Furthermore, we also found that the flow model has a training-inference mismatch problem, that is, train in the
% back-ward direction but infer in the forward direction. 
% Motivated by \cite{tan2022naturalspeech}, we leverage a differentiable duration predictor and a bi-directional flow model to enhance the rhythm and naturalness of synthesized singing voice.
% To the best of our knowledge, this paper is the first work to adopt the differentiable duration predictor and bi-directional flow model in SVS.

% We applied this melody-unsupervision strategy to our previous framework\cite{zhou22f_interspeech} and made a few adjustments.
% First, rather than the mel-spectrogram, the input to our phonemes predictor is the frame-level hidden feature predicted by the posterior encoder.
% Second, because the pre-training dataset lacks duration information, we eliminate the downsampling network to ensure that the feature dimension is always frame-level.
% Last but not the least, we adopt one of the state-of-the-art speaker recognition models, i.e. ECAPA-TDNN, as the speaker encoder to model the different timbre information of the multi-singer dataset.
% Besides, we found that previous work has a rhyme problem. 
% To solve this problem, we propose the differentiable up-sampling layer and replace the length-regulator with it.
% Furthermore, we found that the previous flow model has the problem of mismatch between training and inference, i.e., the mapping of the model in the opposite direction during training and inference.
% We proposed a bi-directional flow model to alleviate this problem.

\begin{figure*}[ht]
  \centering
  \subfigure[\label{subfig:a}The multi-singer pre-training step]{\includegraphics[width=0.50\textwidth]{img/fig_a.pdf}}
  \quad % Add some horizontal space between the two subfigures
  \subfigure[\label{subfig:b}The single-singer fine-tuning step]{\includegraphics[width=0.40\textwidth]{img/fig_b.pdf}}
  \caption{The structure of the proposed SVS system, where (a) represents the pre-training stage, while (b) represents the fine-tuning stage. Modules with blue boxes and highlighted backgrounds only exist in the fine-tuning step.}
  \label{fig: architecture}
\end{figure*}



%\begin{figure*}[t]
%	\includegraphics[width=0.98\textwidth]{icassp2023-zhoushaohuan/img/FrameWork.pdf}
%	\caption{The structure of the proposed SVS system. Modules with blue boxes and highlighted backgrounds only exist in the fine-tuning step.}
%	\label{fig: architecture}
%\end{figure*}

The contributions of this work are summarized as follows: 1) Proposing a melody-unsupervised multi-speaker pre-training strategy to enhance the vocal range of the single-singer SVS system; 2) Exploring the potential feasibility of developing virtual singers with wide vocal ranges; 3) Introducing a differentiable up-sampling layer and a bi-directional flow model to improve the sound quality and rhythm naturalness. Some audio samples are provided for listening\footnote{Sample: \href{https://thuhcsi.github.io/melody-unsupervised-pretraining-svs/}{https://thuhcsi.github.io/melody-unsupervised-pretraining-svs}}.

The rest of this paper is organized as follows: Section~\ref{sec:Method} illustrates the proposed system. Experimental setup and experiment results are demonstrated in Section~\ref{sec:expt-setup} and \ref{sec:expt-rst}, respectively. We conclude this work in Section~\ref{sec:conclusion}.

% The contributions of this work can be summarized as follows: 1) We propose a pre-training method base on the main architecture of out previous work to improve the vocal range of SVS model. Our method is highly transferable and can be used to enhance the vocal range of all SVS models trained with a single speaker dateset. 2) Since the pre-train dataset is a large-scale multi-singer dataset, we use a speaker encoder to  model the speaker timbre information and improve the speaker similarity of the synthesized singing voice. 3) To alleviate the rhyme problem, a differentiable up-sampling layer is used in place of the simple copy operation used by length-regulator to extend phoneme-level features into frame-level. 4) A bi-directional flow model to alleviate the training-inference mismatch problems to improve the naturalness of synthesized singing voice. Here are some audio samples\footnote{Samples:\href{https://thuhcsi.github.io/melody-unsupervised-pretraining-svs/}{https://thuhcsi.github.io/melody-unsupervised-pretraining-svs}}.
