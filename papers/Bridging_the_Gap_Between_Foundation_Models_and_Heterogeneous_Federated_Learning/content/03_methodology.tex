\section{Methodology}
\input{content/f01_overview}
This section offers an in-depth overview of the primary components of the proposed Resource-Aware Federated Foundation Model (RaFFM): foundation model scaling and resource-aware federated learning.

\subsection{Foundation Model Scaling}
RaFFM employs a foundation model scaling technique, inspired by~\citep{bootstrapnas_aaai}, primarily designed to compress pre-trained FMs while ensuring adherence to the heterogeneous resource constraints in edge-FL systems.  The overarching objective is to enhance resource utilization both during the training and inference phases in FL.
As highlighted in Figure~\ref{fig:overview}, foundation model scaling incorporates two key components: salient parameter prioritization and high-performance sub-model extraction.



\subsubsection{Salient Parameter Prioritization}
Recent advancements in model compression, such as network pruning~\citep{he2018amc,BlalockOFG20_pruning,yu2022gnnrl,yu2021agmc} and neural architecture search \citep{nas1000, munoz2022automated}, underscore that deep neural networks, particularly pre-trained FMs, often exhibit over-parameterization. Only a subset of parameters critically influence the model's performance.
Identifying these impactful parameters is of paramount importance in resource-limited FL settings. 
Within RaFFM, we leverage salient parameter prioritization to identify salient parameters in FMs, and extract high-performance sub-models with salient parameters that are uniquely tailored for individual clients' resources. Additionally, salient parameter prioritization can ease the resource-aware sub-model extractions and model fusion in FL global updates using parallel matrix slicing.



 \textbf{Parameter Salience Score.}
 Inspired by magnitude-based pruning techniques, salient parameters are recognized by ranking model weights using a salience evaluation metric (examples include the L1 and L2 norms~\citep{li2016l1pruning,kumar2021pruningl1}). Our experimental analysis preferred the L1 norm, thus adopted for our context. The L1 norm salience for a channel \(c\) within weight matrix \(W\) is illustrated by:


\begin{equation}
\text{Salience}(c) = \sum_{i=1}^{n} |W_{c,i}|
\label{eq:l1norm}
\end{equation}
Equation \ref{eq:l1norm} captures the L1 norm by aggregating the absolute weight values in a channel, reflecting the channel's cumulative significance.

\textbf{Salient Parameter Prioritization.} 
We rank the original model's weight channels based on their salience scores. Prioritization ensures that channels with the highest salience are forefronted in the weight matrix.
\begin{equation}
c_{ranked} = \text{argsort}(\text{Salience}(c), \text{descending})
\label{eq:channel_ranking}
\end{equation}
\begin{equation}
W_{ranked} = W[c_{ranked}:]
\label{eq:weight_ranking}
\end{equation}
Equation \ref{eq:channel_ranking} and Equation \ref{eq:weight_ranking} delineate the procedure for ranking channels based on salience scores.


 \subsubsection{Parameter Prioritization in Transformer}
While existing magnitude-based compression methods predominantly target convolutional neural networks (CNNs), applying salient parameter prioritization to multi-head attention transformers architectures~\citep{vaswani2017transformer} requires additional deliberation. We introduce a specialized salient parameter prioritization strategy tailored for transformers, which ensures the preservation of the inherent attention information present in the original FM.

\begin{equation}
\label{eq:multihead}
\text{Attention}(W^q,W^k,W^v,x) = \text{softmax}\left(\frac{xW^q (xW^k)^T}{\sqrt{d_k}}\right) xW^v
\end{equation}
Equation~\ref{eq:multihead} characterizes the attention-head. Here, the input sequence $x\in \mathbb{R}^{l\times d}$ has a sequence length of $l$ and an embedding size of $d$. The matrices $W^q \in \mathbb{R}^{d\times d_k}$ and $W^k \in \mathbb{R}^{d\times d_k}$ represent the query and key weights, respectively, while $W^v \in \mathbb{R}^{d\times d_v}$ stands for the value weights.

\textbf{Theorem 1}: Given matrices $W^q$ and $W^k$ of dimensions $d \times d_k$ and an input $x$ of size $l \times d$, if we uniformly apply a permutation $\pi$ to the columns of both $W^q$ and $W^k$ to obtain $W^{q'}$ and $W^{k'}$ respectively, the subsequent dot-product attention scores determined using $W^{q'}$ and $W^{k'}$ will match those derived using the original $W^q$ and $W^k$. (A detailed proof is provided in Appendix \ref{sec:proof}).



\begin{equation}
\text{Salience}(c) = \frac{\text{Salience}(w^q_c) + \text{Salience}(w^k_c) }{2}
\label{eq:avg_norm}
\end{equation}

To retain the inherent attention characteristics of FMs, we prioritize $W^q$ and $W^k$ within an attention head by employing the same ranked permutation. The salience score, defined in Equation~\ref{eq:avg_norm}, calculates the average norm of the query and key matrices for each channel index $c$. A consistent salience-based rank is concurrently imposed on both $W^q$ and $W^k$. As validated by \textbf{Theorem 1}, this ensures that post-prioritization, the derived dot-product attention scores will remain identical with the original attention head of the FMs.









\subsubsection{High-performance Sub-Model extraction}
Given a resource constraint, denoted as \( \tau \), and a FM \( \mathcal{F(W)} \) with weights \( \mathcal{W} \), the objective of high-performance sub-model extraction is to derive a sub-model from \( \mathcal{F(W)} \) that comprises salient parameters and adheres to constraints \( \tau \).

% Leveraging the salient parameter prioritization, we can easily extract high-performance sub-networks using weight matrix slicing. A sub-model, \( \mathcal{F}(\mathcal{W}_c) \), is defined by a network configuration \( c \) from \( \mathbb{R}^n \). In this representation, \( c \) enumerates the channel width for each hidden layer within the FM. An essential procedure involves selecting a sub-model in compliance with the resource constraints \( \mathcal{R} \). The potential sampling space is symbolized by \( \mathcal{S} = [s_1, ... , s_n] \), where each \( s_i \) demarcates the width (i.e., channel size) of the \( i^{th} \) hidden layer.

Leveraging the salient parameter prioritization, we can easily extract high-performance sub-networks using weight matrix slicing.  We represent a sub-model, \( \mathcal{F}(\mathcal{W}_{c_\tau}) \), as a network configuration \( c_\tau  \in \mathbb{R}^n \) satisfying constraint $\tau$. The \( c_\tau\) is a list specifying the layer width for each hidden layer of the FM.
To sample the target $c_\tau$, the sampling space is denoted by \( \mathcal{S} = [s_1, ... , s_n] \) where \( s_i \) denotes the width (number of channels) of the \( i^{th} \) hidden layer.

For a sampled network configuration \( c_\tau \), we evaluate the size of the sub-model \( \mathcal{F}(\mathcal{W}_{c_\tau}) \) in terms of measurable metrics (e.g., number of parameters). If it aligns the constraints \( \tau \), we extract \( \mathcal{F}(\mathcal{W}_{c_\tau}) \) using Equation \ref{eq:subnetwork_size}:

% For any selected network configuration \( c \), the sub-model's size, \( \mathcal{F}(\mathcal{W}_c) \), is assessed based on specific metrics (for instance, the total parameter count). If it aligns with constraints \( \mathcal{R} \), \( \mathcal{F}(\mathcal{W}_c) \) is culled as per Equation \ref{eq:subnetwork_size}:

\begin{equation}
\mathcal{W}_{c_\tau} =\mathcal{W}[:c_\tau] = \{w_i[:c_i] | w_i \in \mathcal{W}, c_i \in c_\tau, \text{and } i = 1,...,n \}
\label{eq:subnetwork_size}
\end{equation}

In the above equation, \( w_i \) represents the weights of the \( i^{th} \) hidden layer, and \( c_i \) stands for the number of channels sampled from the \( i^{th} \) hidden layer.  The salient parameter prioritization component ensures the extracted sub-model, \( \mathcal{F}(\mathcal{W}_{c_\tau}) \), retains the most salient weights from the original FM.



\subsection{Resource-aware Federated Foundation Models}
RaFFM can seamlessly integrate with mainstream FL model fusion algorithms, such as FedAvg~\citep{mcmahan2017fedavg} and Fedprox~\citep{li2020fedprox}. This is because all resource-aware local models in RaFFM are sub-networks derived from the forefront channels of the given FM. As a result, heterogeneous federated learning model fusion can be easily executed using matrix slicing, as demonstrated in Equations~\ref{eq:subnetwork_size} and~\ref{eq:aggregate}. In this paper, we use FedAvg as our backend algorithm.

\begin{equation}
\mathcal{W}^t = \sum_{c_\tau \in C}(\mathcal{W}^{t-1}[:c_\tau] + \eta_{c_\tau}  \nabla \mathcal{W}_{c_\tau}^t)
\label{eq:aggregate}
\end{equation}
Equation~\ref{eq:aggregate} aggregate resource-aware local models, $c_\tau$ represent the local model configuration satisfying constraint $\tau$, and  $\eta_{c_\tau}$ signifies the learning step for the client. 

The RaFFM procedure in an FL communication round can be described as follows:
\begin{enumerate}
    \item In the $t^{th}$ communication round, RaFFM first perform salient parameters prioritization for the global FM $ \mathcal{F}(\mathcal{W}^{t}) $.
    \item RaFFM samples a set of sub-network configurations, $C=\{c_{\tau1},..,c_{\tau k}\}$, in accordance with the resource constraints of participating clients, then generate sub-models.
    \item These sampled sub-models, represented as $\{\mathcal{W}^{t}_{c_\tau} |{c_\tau} \in C\}$, are dispatched to the local participating clients for further training.
    \item Once local training finished, the clients relay their model updates back to the server.
    \item The server then undertakes model fusion using Equation~\ref{eq:aggregate}.
\end{enumerate}
The entire process is iteratively executed until federated learning is complete (Refer Appendix~\ref{algo:raffm}).


