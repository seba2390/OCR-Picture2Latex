\section{Conclusion and discussion}

In this paper, we introduced the concept of Federated Foundation Models (FFMs), which integrate Federated Learning (FL) into the lifespan of Foundation Models (FMs). We defined three FFM tasks: FFM pre-training, FFM fine-tuning, and federated prompt engineering. Through our experiments, we demonstrated that FFM can significantly improve the performance of centralized FM optimization by leveraging local private data, and the flexible and adaptive optimization process between centralized training and FL makes it more stable and robust than FL-only optimization.

It is important to note that the advancement of computation at edge users is crucial for the widespread adoption of FFMs, and we believe that such advancements will be realized in the near future. As the field of FFM continues to grow, we anticipate the emergence of numerous related research areas, including improved privacy-preserving techniques, the integration of FFM with emerging technologies like IoT and edge computing, and the exploration of FFM in various application domains such as healthcare, finance, and manufacturing.
Additionally, we foresee advancements in adaptive model compression methods for FFM local institutions, communication efficiency research, specialized FL algorithms for efficient updates and aggregation of FFM models, and security attack research. Overall, FFM represents a promising research area in the age of Foundation Models, with the potential to address various challenges in privacy, scalability, and robustness across diverse domains.






