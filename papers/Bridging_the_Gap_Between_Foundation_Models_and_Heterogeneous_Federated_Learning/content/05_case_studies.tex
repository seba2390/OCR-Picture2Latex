\section{Experiments}
\label{sec:exp}
\subsection{Experiment setup}
\label{sec:exp_setup}
\textbf{Federated Learning Settings.} 
RaFFM is designed to address the challenges posed by resource heterogeneity in Federated Learning (FL) scenarios, especially deploying resource-hungry FMs. Our experiments were conducted in a standard cross-silo FL setup. This environment comprises 100 local clients coordinated by a central server. In each communication round, 10\% of the clients are randomly selected to participate in local updates. We employ FedAvg~\citep{mcmahan2017fedavg} as the underlying FL algorithm for model aggregation.



\textbf{Model and Datasets.}
Our evaluations span a variety of pre-trained models:
\begin{itemize}
    \item NLP models: DistilBert~\citep{sanh2019distilbert}, RoBERTa~\citep{liu2019roberta}, BERT~\citep{kenton2019bert}, and FLAN-T5~\citep{chung2022flan-t5}.
    \item Large language model: LLaMA2~\citep{touvron2023llama2}.
    \item Computer Vision model: ViT~\citep{dosovitskiy2020vit}.
\end{itemize}
For our experiments, we employ diverse datasets, including the GLUE benchmark~\citep{wang2018glue}, question-answering benchmarks like SQuAD and SQuAD-v2~\citep{Rajpurkar2016squad}, SVAMP~\citep{SVAMP}, CIFAR-10 and CIFAR-100~\citep{krizhevsky2009cifar}, and the Flower-102 dataset~\citep{Nilsback08flower102}.




\input{content/t01_glue}

\subsection{Learning Efficiency}
Our primary objective with RaFFM is to achieve efficient FL by leveraging fewer computational resources without significantly compromising model performance. To this end, we compare the performance of resource-aware sub-models deployed through RaFFM against the conventional full-size FL model implementations.

\subsubsection{Results on GLUE Benchmark}
We assessed RaFFM's efficacy using the NLP General Language Understanding Evaluation (GLUE) benchmark~\citep{wang2018glue}, which comprises eight language datasets: QQP, QNLI, SST-2, CoLA, STS-B, MRPC, RTE, and MNLI.

In experiments, baseline FL optimizes full-size FMs at local clients. In contrast, RaFFM deployed resource-aware sub-models tailored to individual clients. Table~\ref{tab:res_glue} summarizes the results. Performance metrics were computed for the global model post-training on validation datasets. The column titled \#Param represents the average count of local model parameters across all clients for the eight datasets. Training acceleration estimates the relative GPU hours. We also present an average score (AVG) as an aggregate performance indicator.

Notably, RaFFM required fewer computational resources (evidenced by the reduced parameter count) and demonstrated faster training times than the conventional full-model FL. Impressively, despite the reduced model size at the client side, RaFFM-optimized models not only competitive but occasionally surpassed the performance of baseline full-size FL models. This superiority was particularly pronounced in datasets such as SST-2, MRPC, and MNLI.



\subsubsection{Results on Question Answering Benchmark}
\input{content/t02_SQUAD}
Further, we evaluate RaFFM on the Stanford Question Answering Dataset (SQuAD and SQuAD-V2)~\citep{Rajpurkar2016squad}. 

Table \ref{tab:squad} presents the results across various models. Impressively, RaFFM consistently speeds up the training process on both SQuAD tasks. Taking BERT-Large as an example, RaFFM accelerates FL training (measured in GPU hours) by a factor of 6.59. Remarkably, despite deploying substantially trimmed models at edge clients, RaFFM's efficiency advantage does not come at the cost of model performance. Indeed, in cases like FLAN-T5 base and BERT-Large, RaFFM's performance even eclipses that of full-size FL deployments.

We further evaluate the parameter efficiency—especially vital in resource-tight FL settings. Figure~\ref{fig:squad_param_efficiency} (a) and (b) visually depict RaFFM's supremacy in parameter efficiency (higher values are preferable). As an illustrative point, in BERT-Large, RaFFM, with an average model size of 95M parameters, posts an Exact Match score of 83.34 and an F1 score of 90.87 on SQuAD, which stands in stark contrast to full-size FM deployment in FL, and the efficiency translates to a speed-up of 3.52 times.

\subsection{Results on Computer Vision Tasks}
\label{subsec:vision_results}

In addition to our evaluations on NLP benchmarks, we extended our assessment to ascertain the efficacy of RaFFM on computer vision (CV) tasks, specifically leveraging the Vision Transformer (ViT)~\citep{dosovitskiy2020vit}.
As shown in Table~\ref{tab:vit}, we fine-tune ViT with 12-shot learning. The results corroborate RaFFM's consistent performance metrics on CV tasks. Specifically, in line with our findings from NLP benchmarks, RaFFM demonstrated a marked superiority in the training acceleration. Additionally, the reduced communication overhead and negligible compromise in model performance further underline its efficiency and robustness.


\input{content/f03_squad_param_efficiency}
\input{content/t03_cvtasks}

Conclusively, RaFFM offers an excellent balance between performance and computational efficiency. With fewer parameters and higher speed-up factors, it provides a viable alternative to more computationally intensive models without perceptible degradation in performance. 


\subsection{Communication Efficiency}
A key challenge in FL is the substantial communication cost, often due to frequent model weights or gradients sharing between edge devices and the central server. 
Figure~\ref{fig:squad_param_efficiency} (c) illustrates the average communication footprint of a client in each round under various FMs. RaFFM substantially reduces communication costs at the client level. The rationale is straightforward: its resource-aware model deployment leads to more compact models at the edge. Consequently, there is less information to relay between the edge devices and the server, reducing communication burdens.
% In our experiments, we tailored the target FMs to meet a specific performance benchmark, specifically the average median convergence accuracy. During this process, we monitored the network traffic generated by the training communications. As highlighted in Table~\ref{tab:com_cost}, the results unequivocally demonstrate RaFFM's capability to reduce communication costs across all our experimental scenarios.

Additionally, we optimized FMs to meet specific performance benchmarks (set to the average median convergence accuracy). We monitored the network traffic induced by the training process during communication. As highlighted in Table~\ref{tab:com_cost}, it consistently underlines RaFFM's superiority in minimizing communication costs across all experimental setups.
Furthermore, Figure~\ref{fig:squad_param_efficiency} (d) showcases the network traffic needed to achieve the average median convergence performance on the GLUE benchmark. RaFFM also consistently outperforms full-size model deployment, evidencing significantly decreased average communication cost across different FMs.



\input{content/t05_com_target}


\subsection{Resource Efficiency}
\input{content/f05_resource_efficiency}
\subsubsection{System Resource Efficiency}
Traditional FL approaches often suffer inefficient system resource utilization. When identical models are dispatched to both high-end and resource-constrained devices, the latter often dictates the constraints, compelling the entire system to conform to its limitations. This results in inflated system requirements and often leaves high-end devices underutilized.
Figure~\ref{fig:resoruce_efficiency} (a) elucidates this challenge, showcasing the minimum system resource prerequisites for various FMs within a 100-client FL system to achieve the median F1 scores as detailed in Table~\ref{tab:squad}. The baseline FL has lofty uniform resource demands, whereas RaFFM, illustrated through box plots, showcases a range of resource allocations wherein it attains the target performance. Evidently, RaFFM exhibits enhanced system resource efficiency, paving the way for a more cost-effective FL setup in terms of hardware requirements.


Further clarity is provided in Figure~\ref{fig:resoruce_efficiency} (b), which shows the system requirements of RoBERTa deployments for different performance levels on the SST-2 dataset. The red dashed line represents full-size FM deployment in traditional FL, which indicates high baseline system requirements, irrespective of the performance target. Conversely, RaFFM, depicted by the box plot, offers a flexible range of system requirements to attain similar performance outcomes. This adaptability of RaFFM enables dynamic system budget adjustments based on performance goals, maximizing resource efficiency and preventing unnecessary expenditures.


In Figure~\ref{fig:resoruce_efficiency} (c) and (d), we further assess RaFFM's robustness amidst a variety of system resource constraints, spanning Xlarge to XSmall budget categories. The findings underscore RaFFM's consistency: even as resources swell or dwindle, its performance remains steadfast, signifying its adaptability in heterogeneous resource environments.



\subsubsection{Edge Resource Efficiency}
\input{content/f06_local_performances}
In FL, not only do system-wide resource requirements matter, but individual client resources also play a pivotal role. Given the varied resource capacities of individual clients, ensuring stable performance of resource-aware models on these clients becomes crucial. 

Figure~\ref{fig:sys_heto} (a) and (b) depict two distinct resource budget FL systems. The distribution heat map of clients' resources for these two setups is showcased in (a). Figure~\ref{fig:sys_heto} (b) depicts local clients' performances, demonstrating the stability of local clients' performance: even with notable variations in local resources, every client's performance aligns closely with its peers. This inherent stability is further emphasized when comparing the two different FL systems; despite the system disparities, the local model performance across the two systems remains stable.
This consistency is underscored by Figure~\ref{fig:sys_heto} (c), which plots the relationship between local model size and its achieved performance. Interestingly, even as the model sizes differ, each maintains commendable local performance.
To sum up, RaFFM showcases its prowess in adeptly navigating resource heterogeneity, ensuring that performance remains stable at the client level.

\subsubsection{Edge Inference Efficiency}
\label{subsec:edge_inference_efficiency}
The resource consumption during the training phase is usually at least seven times that of the inference phase. 
This disparity means that FL often faces resource-hungry during model training while leaving resources underutilized during inference.

The mentioned observation is depicted in Figure~\ref{fig:sys_heto} (d), which delineates the resource utilization efficiency during both training and inference phases for edge clients. 
RaFFM, with its inherent FM scaling components, 
enables the post-training deployment of relatively larger models at the edge during the inference stage, hence increasing the model performance and resource utilization.



\subsection{Enhancing RaFFM with Efficient Fine-tuning: A Case with LoRA}
\label{sec:raffm_lora}

Incorporating parameter efficient fine-tuning (PEFT) methods like LoRA~\citep{hu2021lora} into RaFFM holds potential for optimized performance, especially when dealing with large language models (LLMs) in an FL setting. Though PEFTs such as LoRA can markedly trim the trainable parameters of LLMs—often to less than 1\% of the total parameters—it's essential to note that the full-size weights and activations are still retained during training. This results in substantial memory overheads.

To investigate the benefits of this synergy, we paired RaFFM with LoRA and fine-tuned the LLaMA2 model~\citep{touvron2023llama2} using the preprocessed instruction math question answering dataset, SVAMP~\citep{hu2023llmadapter,SVAMP}. This dataset was partitioned among 10 FL clients for the experiments.
Table~\ref{tab:llama} highlights the strengths of the RaFFM-LoRA combination. Specifically, compared to a full-size model paired with LoRA in an FL context, RaFFM coupled with LoRA demonstrates enhanced communication efficiency and a marked acceleration in inference.



\input{content/t06_llama2}
% \subsection{Comparison with Efficient FL}
% ?? computational cost figure, communication figure, performance figure.

% \subsection{Ablation study}
% RaFFM instroduce 

% \subsection{Comparison with Efficient Finetuning}

% \input{content/t04_method_compare}




%add table for prompt engineerings