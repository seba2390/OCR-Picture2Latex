\appendix
\section{Appendix}
You may include other additional sections here.

\textbf{Salient Parameter Prioritization.} 
We ranking the original model weights channels based on the salience score for better subnetwork extraction and model fusion in FL global updates. Prioritization ensures that the most impactful channels are at forefront of the weight matrix:
    \begin{equation}
    c_{ranked} = \text{argsort}(\text{Salience}(c), \text{descending})
    \label{eq:channel_order}
    \end{equation}
    \begin{equation}
    W_{ranked} = W[c_{ranked}:]
    \label{eq:channel_order}
    \end{equation}
    Equation \ref{eq:channel_order} describes the process of ordering channels based on their computed salient score.

 \subsubsection{Parameter Prioritization in Transformer}
Existing magnitude-based compression methods mainly focus on convolution neural networks (CNNs), transformer~\cite{}, with their multi-head attention mechanisms, need further consideration. We proposed specialized salient parameter prioritization for multi-head transformers.

\begin{equation}
\label{eq:multihead}
\text{Attention}(W^q,W^k,W^v,x) = \text{softmax}\left(\frac{xW^q (xW^k)^T}{\sqrt{d_k}}\right) xW^v
\end{equation}

We can simplify the computation as Equation~\ref{eq:multihead}, where the input sequence $x\in \mathcal{R}^{n\times d}$ with sequence length $n$ and embedding size $d$, $W^q \in \mathcal{R}^{d\times d_k}$ and $W^k \in \mathcal{R}^{d\times d_k}$ are the query and key weights, and $W^v \in \mathcal{R}^{d\times d_v}$ is the value weights.

To satisfy the Rule~\ref{itm:rule2}, in transformer architecture, we only prioritize the $W^q$ and $W^k$.  Specifically, we compute the average norm for the query (q) and key (k) matrices in each channel. 
We decompose $W^q$ and $W^k$ as a set of vectors:
$W^q = [w^q_1, ...,w^q_c,..., w^q_{d_k}]$ and $W^k = [w^k_1, ...,w^k_c,..., w^k_{d_k}]$, where $w^q_c \text{,  } w^k_c \in\mathcal{R}^{d\times1}$, and $c \in [1,d_k]$.
% $$ W^k = {w^k_c} \text{,    \space} c \in [1,d_k] $$

Importantly, an identical ranked order is applied to $W^q$ and $W^k$ together. Formally, in attention layers, for a given channel $c$ in $W^q$ and $W^k$ we calculate the average L1 norm as salient parameter score as Equation~\ref{eq:avg_norm}.
     % \text{AvgNorm}(c) = \frac{\text{Norm}(q_c) + \text{Norm}(k_c) + \text{Norm}(v_c)}{3}
    \begin{equation}
    \text{Norm}(c) = \frac{\text{Norm}(w^q_c) + \text{Norm}(w^k_c) }{2}
    \label{eq:avg_norm}
    \end{equation}
The average norm provides a unified measure for the attention mechanism, then we ensure that all components $(W^q, W^k)$ are consistently ranked with the same permutation $\pi$ in Equation~\ref{eq:argsort}~\ref{eq:wq'}~\ref{eq:wk'}.

\begin{equation}
 \pi = \text{argsort}(\text{Norm}(1),\ldots, \text{Norm}(c), \ldots, \text{Norm}({d_k})) 
 \label{eq:argsort}
\end{equation}

\begin{equation}
W^{q'} = [w^q_{\pi(1)}, ..., w^q_{\pi(d_k)}] 
\label{eq:wq'}
\end{equation}


\begin{equation}
W^{k'} = [w^k_{\pi(1)}, ..., w^k_{\pi(d_k)}]
\label{eq:wk'}
\end{equation}


 % Consider a simple FM with three channels, where the L1 norms of the weights for these channels are 10, 15, and 5, respectively. Using our method, the channels would be ranked in the order: 2, 1, 3, indicating the importance of each channel based on its weight magnitude.



\textbf{Theorem}: Given matrices $W^q$ and $W^k$ of size $d \times d_k$ and an input $x$ of size $n \times d$, if we apply a consistent permutation $\pi$ to the columns of both $W^q$ and $W^k$ to obtain $W^{q'}$ and $W^{k'}$, respectively, then the resulting dot-product attention scores computed using $W^{q'}$ and $W^{k'}$ will be identical to those computed using the original $W^q$ and $W^k$.

\textbf{Proof}:

Let $Q = xW^q$ and $K = xW^k$ be the original query and key matrices. Their sizes are $n \times d_k$.

Let $Q' = xW^{q'}$ and $K' = xW^{k'}$ be the query and key matrices after applying the permutation $\pi$. 

By our definition of the permutation, for each column $c$ in $W^q$ and $W^k$, the new position in the permuted matrices is $\pi(c)$. So, the elements of $Q'$ and $K'$ are given by:
\begin{equation}
    Q'_{ij} = \sum_{c=1}^{d} x_{ic} W^{q'}_{cj} = \sum_{c=1}^{d} x_{ic} W^{q}_{c\pi(j)} = x_i \cdot \Vec{w}^q_{\pi(j)}
    \label{eq:Q'}
\end{equation}
\begin{equation}
    K'_{ij} =  \sum_{c=1}^{d} x_{ic} W^{k'}_{cj} = \sum_{c=1}^{d} x_{ic} W^{k}_{c\pi(j)} = x_i \cdot \Vec{w}^k_{\pi(j)}
    \label{eq:K'}
\end{equation}


Now, consider the dot-product of rows from $Q$ and $K$:
\begin{equation}
    (QK^T)_{ij} = \sum_{c=1}^{d_k} Q_{ic} K_{jc} 
\end{equation}


Similarly, for $Q'$ and $K'$:
\begin{equation}
    (Q'K'^T)_{ij} = \sum_{c=1}^{d_k} Q'_{ic} K'_{jc}
\end{equation}


Using the previous definitions of $Q'$ and $K'$, and plugging them into the above equation, we see:
\begin{equation}
\begin{split}
\label{eq:Q'K'}
(Q'K'^T)_{ij} & = \sum_{p=1}^{d_k} \left( \sum_{c=1}^{d} x_{ic} W^{q'}_{cp} \right) \left( \sum_{r=1}^{d} x_{jr} W^{k'}_{rp} \right) 
\end{split}
\end{equation}

We take the Equation~\ref{eq:Q'}~\ref{eq:K'} into Equation~\ref{eq:Q'K'} and we have:
\begin{equation}\label{eq:Q'K'=QK}
\begin{split}
       (Q'K'^T)_{ij}  &= \sum_{p=1}^{d_k} \left( x_i \cdot \Vec{w}^q_{\pi(p)} \right) \left( x_j \cdot \Vec{w}^k_{\pi(p)} \right) 
       \\ &= \sum_{p=1}^{d_k} \left( x_i \cdot \Vec{w}^q_{p} \right) \left( x_j \cdot \Vec{w}^k_{p} \right) 
\end{split}
\end{equation}

Because the permutation $\pi$ is applied consistently to both $W^q$ and $W^k$, and the dot product operation is commutative, each term of the form $x_{i} \cdot W^{q'}_{\pi(p)}$ will correspond to a term of the form $x_{j} \cdot W^{k'}_{\pi(p)}$, yielding identical products as in the original $QK^T$ computation.

Therefore, we can conclude that:
\[ QK^T = Q'K'^T \]

Hence, the attention scores remain unchanged after the permutation.
