\appendix
\section{Appendix}

\subsection{Proof for Theorem 1}
\label{sec:proof}


\textbf{Notations:} To better illustrate the proof, we decompose $W^q$ and $W^k$ as a set of vectors:
$$W^q = [w^q_1, ...,w^q_c,..., w^q_{d_k}]$$ and 
$$W^k = [w^k_1, ...,w^k_c,..., w^k_{d_k}]$$
, where $w^q_c \text{,  } w^k_c \in\mathcal{R}^{d\times1}$, and $c \in [1,d_k]$.
Similarly, $W^{q'}$ and $W^{k'}$  can be decomposed as:

\begin{equation}
W^{q'} = [w^q_{\pi(1)}, ..., w^q_{\pi(d_k)}] 
\label{eq:wq'}
\end{equation}


\begin{equation}
W^{k'} = [w^k_{\pi(1)}, ..., w^k_{\pi(d_k)}]
\label{eq:wk'}
\end{equation}


 % Consider a simple FM with three channels, where the L1 norms of the weights for these channels are 10, 15, and 5, respectively. Using our method, the channels would be ranked in the order: 2, 1, 3, indicating the importance of each channel based on its weight magnitude.



\textbf{Theorem}: Given matrices $W^q$ and $W^k$ of size $d \times d_k$ and an input $x$ of size $n \times d$, if we apply a consistent permutation $\pi$ to the columns of both $W^q$ and $W^k$ to obtain $W^{q'}$ and $W^{k'}$, respectively, then the resulting dot-product attention scores computed using $W^{q'}$ and $W^{k'}$ will be identical to those computed using the original $W^q$ and $W^k$.

\textbf{Proof}:

Let $Q = xW^q$ and $K = xW^k$ be the original query and key matrices. Their sizes are $n \times d_k$.

Let $Q' = xW^{q'}$ and $K' = xW^{k'}$ be the query and key matrices after applying the permutation $\pi$. 

By our definition of the permutation, for each column $c$ in $W^q$ and $W^k$, the new position in the permuted matrices is $\pi(c)$. So, the elements of $Q'$ and $K'$ are given by:
\begin{equation}
    Q'_{ij} = \sum_{c=1}^{d} x_{ic} W^{q'}_{cj} = \sum_{c=1}^{d} x_{ic} W^{q}_{c\pi(j)} = x_i \cdot \Vec{w}^q_{\pi(j)}
    \label{eq:Q'}
\end{equation}
\begin{equation}
    K'_{ij} =  \sum_{c=1}^{d} x_{ic} W^{k'}_{cj} = \sum_{c=1}^{d} x_{ic} W^{k}_{c\pi(j)} = x_i \cdot \Vec{w}^k_{\pi(j)}
    \label{eq:K'}
\end{equation}


Now, consider the dot-product of rows from $Q$ and $K$:
\begin{equation}
    (QK^T)_{ij} = \sum_{c=1}^{d_k} Q_{ic} K_{jc} 
\end{equation}


Similarly, for $Q'$ and $K'$:
\begin{equation}
    (Q'K'^T)_{ij} = \sum_{c=1}^{d_k} Q'_{ic} K'_{jc}
\end{equation}


Using the previous definitions of $Q'$ and $K'$, and plugging them into the above equation, we see:
\begin{equation}
\begin{split}
\label{eq:Q'K'}
(Q'K'^T)_{ij} & = \sum_{p=1}^{d_k} \left( \sum_{c=1}^{d} x_{ic} W^{q'}_{cp} \right) \left( \sum_{r=1}^{d} x_{jr} W^{k'}_{rp} \right) 
\end{split}
\end{equation}

We take the Equation~\ref{eq:Q'}~\ref{eq:K'} into Equation~\ref{eq:Q'K'} and we have:
\begin{equation}\label{eq:Q'K'=QK}
\begin{split}
       (Q'K'^T)_{ij}  &= \sum_{p=1}^{d_k} \left( x_i \cdot \Vec{w}^q_{\pi(p)} \right) \left( x_j \cdot \Vec{w}^k_{\pi(p)} \right) 
       \\ &= \sum_{p=1}^{d_k} \left( x_i \cdot \Vec{w}^q_{p} \right) \left( x_j \cdot \Vec{w}^k_{p} \right) 
\end{split}
\end{equation}

Because the permutation $\pi$ is applied consistently to both $W^q$ and $W^k$, and the dot product operation is commutative, each term of the form $x_{i} \cdot W^{q'}_{\pi(p)}$ will correspond to a term of the form $x_{j} \cdot W^{k'}_{\pi(p)}$, yielding identical products as in the original $QK^T$ computation.

Therefore, we can conclude that:
\[ QK^T = Q'K'^T \]

Hence, the attention scores remain unchanged after the permutation.

% \subsection{Experiment Setting}

% \subsection{Supplementary Experiment Data}

% \input{content/t05a.com_target}


\input{content/f0x_ablation}
\subsection{Ablation Study}
RaFFM leverages the salient parameter prioritization (SPP) component to identify impactful weights in FMs. Hence, it's critical to evaluate the effect of salient parameter prioritization. The experiment is conducted on BERT-Large with SST-2~\citep{wang2018glue}
Figure~\ref{fig:ablaton} shows the comparison of SPP and without SPP. Figure~\ref{fig:ablaton} (a) box plot shows the local sub-model performance range for randomly sampled 100 clients from supernet with the same resource constraints. It's evident that SPP produces subnetworks with higher performance.
Figure~\ref{fig:ablaton} (b) shows the communication rounds vs global model (super-network) accuracy. RaFFM equipped with SPP, produced higher model performance and a more stable learning trend.

\label{sec:ad_algo}
\input{content/algorithm/fedavg}
