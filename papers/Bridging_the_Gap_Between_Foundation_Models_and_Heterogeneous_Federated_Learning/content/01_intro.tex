\section{Introduction}
Federated learning (FL)~\citep{mcmahan2017fedavg} represents a significant advancement in machine learning, emphasizing decentralized training while preserving data privacy. FL enhances data privacy and collaboration compared to traditional machine learning by enabling model training across multitudes of decentralized devices without direct data sharing. However, challenges like non-identical independent distribution (non-IID) data and heterogeneous computational resources among devices present potential training failures.


Concurrently, transformer-based foundation models (FMs) \citep{Bommasani2021FoundationModels}, typified by GPT~\citep{radford2019gpt2,brown2020gpt3,openai2023gpt4}, BERT~\citep{kenton2019bert}, and ViT~\citep{dosovitskiy2020vit}, pre-trained on large-scale datasets, have revolutionized AI research. 
FMs leverage their inherent pre-trained knowledge, achieving exceptional performance across multiple domains in downstream tasks even with limited fine-tuning data.


Given the superior strengths of FMs in few-shot transfer learning, they appear well-suited for non-IID FL environments~\citep{yu2023ffm,zhuang2023foundationfl}. However, seamlessly integrating FMs into FL presents significant challenges. The substantial size and intensive resource demands of FMs make their deployment on resource-constrained FL edge devices problematic. Furthermore, the uneven distribution of computational resources within FL increases the difficulty of existing challenges. A resource-limited device must first satisfy the resource requirements for FM optimization, despite the presence of more capable devices within the same FL network, leading to high system requirements overall.
Additionally, fine-tuning FMs typically requires approximately seven times the resources compared to inference. This disparity means that FL often faces resource-hungry during model training while leaving resources underutilized during inference.

We propose a framework, Resource-aware Federated Foundation Models (RaFFM), to address the resource utilization challenges in FL. RaFFM uses specialized transformer-based FM compression algorithms tailored for FL-edge environments, and dynamically deploys resource-aware scaled FMs to local clients. 
Since FMs are over-parameterized, a subset of salient parameters is more impactful to model performance. RaFFM first identifies and prioritizes the salient parameters in the given FM before FL communication starts. The salient parameter prioritization can ease the resource-aware sub-model extractions from FMs and model fusion in FL global updates via parallel matrix operations.
Then, RaFFM generates resource-aware sub-models with salient weights so clients can proceed with the FL fine-tuning cycle.
Post-training, given that model inference is more resource-friendly than training, RaFFM can deploy larger models, ensuring optimized performance and consistent resource allocation based on the client's capabilities.

In essence, RaFFM brings forth the following key contributions:
\begin{itemize}
    \item Designed specialized FM compression algorithms tailored for edge FL.
    \item Proposed specialized salient parameter prioritization strategy for transformer-based FMs.
    \item Enhanced resource utilization throughout the training and deployment stages of FL.
    \item Significant reduction in communication overhead between edge devices and central servers.
\end{itemize}

% \begin{itemize}
%     \item Introducing specialized FM compression algorithms ideal for edge FL environments.
%     \item Proposing a unique salient parameter prioritization strategy tailored for transformer-based FMs.
%     \item Optimizing resource utilization across FL's training and deployment phases.
%     \item Achieving a substantial reduction in communication overhead between edge devices and central servers.
% \end{itemize}




