
\section{Distance Estimation}\label{sec:dist}
We now prove~\cref{thm:distances_ub}.
To this end, we augment the basic sketch from Section~\ref{sec:sketch} with additional information, relying on the following distance sketches due to~\cite{achlioptas2001database} (following~\cite{johnson1984extensions}) and~\cite{kushilevitz2000efficient}.

\begin{lemma}[\cite{achlioptas2001database}]\label{lmm:binaryjl}
Let $\epsilon,\delta'>0$.
Let $d'=c\epsilon^{-2}\log(1/\delta')$ for a sufficiently large constant $c>0$.
Let $M$ be a random $d'\times d$ matrix in which every entry is chosen independently uniformly at random from $\{-1/\sqrt{d'},1/\sqrt{d'}\}$.
Then for every $x,y\in\R^d$, with probability $1-\delta'$, $\norm{Mx-My}=(1\pm\epsilon)\norm{x-y}$.
\end{lemma}

\begin{lemma}[\cite{kushilevitz2000efficient}]\label{lmm:kor}
Let $R>0$ be fixed and let $\epsilon,\delta'>0$. There is a randomized map $\mathrm{sk}_R$ of vectors in $\R^d$ into $O(\epsilon^{-2}\log(1/\delta'))$ bits, with the following guarantee.
For every $x,y\in\R^d$, given $\mathrm{sk}_R(x)$ and $\mathrm{sk}_R(y)$, one can output the following with probability $1-\delta'$:
\begin{itemize}
  \item If $R\leq\norm{x-y}\leq 2R$, output a $(1+\epsilon)$-estimate of $\norm{x-y}$.
  \item If $\norm{x-y}\leq (1-\epsilon)R$, output ``Small''.
  \item If $\norm{x-y}\geq (1+\epsilon)R$, output ``Large''.  
\end{itemize} 
\end{lemma}

%\begin{lemma}\label{lmm:kor_onescale}
%Let $R>0$ be fixed. There is a randomized map $\mathrm{sk}_R$ of vectors in $\R^d$ into $O(\epsilon^{-2}\log(1/\delta))$ bits, and a boolean predicate $P_R(\cdot,\cdot)$, such that for every $x,y\in\R^d$, with probability $1-\delta$,
%\begin{itemize}
%  \item If $\norm{x-y}<(1-\epsilon)R$ then $P_R(\mathrm{sk}_R(x),\mathrm{sk}_R(y))$ is true.
%  \item If $\norm{x-y}>R$ then $P_R(\mathrm{sk}_R(x),\mathrm{sk}_R(y))$ is false.
%\end{itemize}
%\end{lemma}
%
%\begin{lemma}\label{lmm:kor_multiscale}
%There is a randomized map $\mathrm{sk}^*$ of vectors in $\R^d$ into $O(\epsilon^{-2}\log(1/\delta)\log\Phi)$ bits, and a map $D^*(\cdot,\cdot)$, such that for every $x,y\in\R^d$, with probability $1-\delta$,
%$D^*(\mathrm{sk}^*(x),\mathrm{sk}^*(y))=(1\pm\epsilon)\norm{x-y}$.
%\end{lemma}


We augment the basic sketch from Section~\ref{sec:sketch} as follows.
We sample a matrix $M$ from Lemma~\ref{lmm:binaryjl}, with $\delta'=\delta/q$.
In addition, for every level $\ell$ in the tree $T$, we sample a map $\mathrm{sk}_{2^\ell}$ from Lemma~\ref{lmm:kor}, with $\delta'=\delta/(q\log(2\sqrt{d}\Phi))$.
For every subtree root $r$ in $T$, we store $Mx_{c(r)}$ and $\mathrm{sk}_{2^{\ell(r)}}(x_{c(r)})$ in the sketch.
Let us calculate the added size to the sketch:

\begin{itemize}
  \item Since $x_{c(r)}$ has $d$ coordinates of magnitude $O(\Phi)$ each, $Mx_{c(r)}$ has $d'$ coordinates of magnitude $O(d\Phi)$ each. Since there are $O(n)$ subtree roots (cf.~Lemma~\ref{lmm:sketch_size}), storing $Mx_{c(r)}$ for every $r$ adds $O(nd'd\Phi)=O(\epsilon^{-2}n\log(q/\delta)\log(d\Phi))$ bits to the sketch. In addition we store the matrix $M$, which takes $O(d'd)$ bits to store, which is dominated by the previous term.
  \item By Lemma~\ref{lmm:kor}, each $\mathrm{sk}_{2^{\ell(r)}}(x_{c(r)})$ adds $O(\epsilon^{-2}\log(q\log(2\sqrt{d}\Phi)/\delta))$ bits to the sketch, and as above there are $O(n)$ of these. In addition we store the map $\mathrm{sk}_{2^{\ell(r)}}$ for every $\ell$. Each map takes $\mathrm{poly}(d,\log\Phi,\log(q/\delta),1/\epsilon)$ bits to store.
\end{itemize}
In total, we get the sketch size stated in~\Cref{thm:distances_ub}.
Next we show how to compute all distances from a new query point $y$.

\paragraph{Query algorithm.}
Given the sketch, an index $k\in[n]$ of a point in $X$, and a new query point $y$, the algorithm needs to estimate $\norm{y-x_k}$ up to $1\pm O(\epsilon)$ distortion. It proceeds as follows.
\begin{enumerate}
  \item Perform the approximate nearest neighbor query algorithm from Section~\ref{sec:ann}. Let $r_0,r_1,\ldots$ be the downward sequence of subtree roots traversed by it.
  \item For each $r_j$, estimate from the sketch whether $\norm{y-x_{c(r_j)}}\leq 2^{\ell(r_j)}$.
  This can be done by Lemma~\ref{lmm:kor}, since the sketch stores $\mathrm{sk}_{2^{\ell(r_j)}}(x_{c(r_j)})$ and also the map $\mathrm{sk}_{2^{\ell(r_j)}}$, with which we can compute $\mathrm{sk}_{2^{\ell(r_j)}}(y)$.
  
   \item Let $t$ be the smallest $j$ that satisfies $\norm{y-x_{c(r_j)}} > 2^{\ell(r_j)}$ according the estimates of Lemma~\ref{lmm:kor}.
  (This attempts to recover from the sketch the same $t$ as defined in the analysis in Section~\ref{sec:ann}.)
  \item Let $t_k\in\{0,\ldots,t\}$ be the maximal such that $x_k\in C(r_{t_k})$.
  
  (In words, $r_{t_k}$ is the root of the subtree in which $x_k$ and $y$ ``part ways''.)
  \item If $t_k=t$, return $\norm{My-Mx_{c(r_t)}}$. Note that $M$ and $Mx_{c(r_t)}$ are stored in the sketch.
  \item If $t_k<t$, let $v_k$ be the bottom node on the downward path from $r_{t_k}$ to $\mathrm{leaf}(x_k)$ that does not traverse a long edge. Return $\norm{y-s^*(v_k)}$.
\end{enumerate}

\paragraph{Analysis.}
Fix a query point $y$.
Define the ``good event'' $\mathcal A(y)$ as the intersection of the following:
%We specify a ``good event'' $\mathcal A(y)$ as the intersection of the following events:
% for $y$. It is the intsersection of following events:
\begin{enumerate}
  \item For every subtree root $r_j$ traversed by the query algorithm above, the invocation of Lemma~\ref{lmm:kor} on $\mathrm{sk}_{2^{\ell(r_j)}}(x_{c(r_j)})$ and $\mathrm{sk}_{2^{\ell(r_j)}}(y)$ succeeds in deciding whether $\norm{y-x_{c(r_j)}}\leq 2^{\ell(r_j)}$.
  Specifically, this ensures that $\norm{y-x_{c(r_j)}}\leq 2^{\ell(r_j)}$ for every $j<t$, and $\norm{y-x_{c(r_t)}}\geq (1-\epsilon)2^{\ell(r_t)}$.
  Recalling that we invoked the lemma with $\delta'=\delta/(q\log(2\sqrt{d}\Phi))$, we can take a union bound and succeed in all levels simultaneously with probability $1-\delta/q$.

  \item $\norm{My-Mx_{c(r_t)}}=(1\pm\epsilon)\norm{y-x_{c(r_t)}}$. By Lemma~\ref{lmm:binaryjl} this holds with probability $1-\delta/q$.
\end{enumerate}
Altogether, $\mathcal A(y)$ occurs with probability $1-O(\delta/q)$.

\begin{lemma}
Conditioned on $\mathcal A(y)$ occuring, with probabiliy $1-\delta/q$, Lemma~\ref{lmm:hashes} holds. Namely, the query algorithm correctly recovers all surrogrates in the subtrees rooted by $r_j$ for $j=0,1,\ldots,t-1$.
\end{lemma}
\begin{proof}
The proof of Lemma~\ref{lmm:hashes} in Section~\ref{sec:ann} relied on having $\norm{y-x_{c(r_j)}}\leq2^{\ell(r_j)}$ for every $j<t$. Conditioning on $\mathcal A(y)$ ensures this holds.
\end{proof}

\paragraph{Proof of~\Cref{thm:distances_ub}.}
Let $\mathcal A^*(y)$ denote the event in which both $\mathcal A(y)$ occurs and the conclusion of Lemma~\ref{lmm:hashes} occurs. By the above lemma, $\mathcal A^*(y)$ happens with probability $1-O(\delta/q)$.
From now on we will assume that $\mathcal A^*(y)$ occurs, and conditioned on this, we will show that the distance from $y$ to any data point can be deterministically estimated correctly.
%
To this end, fix $k\in[n]$ and suppose our goal is to estimate $\norm{y-x_k}$. Let $t_k$ and $v_k$ be as defined by the distance query algorithm above.
We handle the two cases of the algorithm separately.

\textbf{Case I:} $t_k=t$. This means $x_k\in C(r_t)$. By Lemma~\ref{lmm:subtree_root} we have $\norm{x_k-x_{c(r_t)}}\leq2^{\ell(r_t)}\epsilon$. By the occurance of $\mathcal A^*(y)$ we have $\norm{y-x_{c(r_t)}}>(1-\epsilon)2^{\ell(r_t)}$. Together,
%\[
  $\norm{y-x_k} =
  \norm{y-x_{c(r_t)}} \pm \norm{x_k-x_{c(r_t)}} =
  (1\pm2\epsilon)\norm{y-x_{c(r_t)}}$.
%\]
This means that $\norm{y-x_{c(r_t)}}$ is a good estimate for $\norm{y-x_k}$. Since $\mathcal A^*(y)$ occurs, it holds that $\norm{My-Mx_{c(r_t)}}=(1\pm\epsilon)\norm{y-x_{c(r_t)}}$, hence $\norm{My-Mx_{c(r_t)}}$ is also a good estimate for $\norm{y-x_k}$, and this is what the algorithm returns.

\textbf{Case II:} $t_k<t$. Let $T_{t_k}$ be the subtree rooted by $r_{t_k}$. 
By the occurance of $\mathcal A^*(y)$, all surrogates in $T_{t_k}$ are recovered correctly, and in particular $s^*(v_k)$ is recovered correctly.
By Lemma~\ref{lmm:surrogates} we have $\norm{x_{c(v_k)}-s^*(v_k)}ֿ\leq2^{\ell(v_k)}\epsilon$,
and by Lemma~\ref{lmm:subtree_leaf} (noting that $x_k\in C(v_k)$ by choice of $v_k$) we have $\norm{x_k-x_{c(v_k)}}\leq2^{\ell(v_k)}\epsilon$. Together,
$\norm{x_k-s^*(v_k)}ֿ\leq2\cdot2^{\ell(v_k)}\epsilon$.
%\begin{equation}\label{eq:xk_vk}
%\norm{x_k-s^*(v_k)}ֿ\leq2\cdot2^{\ell(v_k)}\epsilon.
%\end{equation}

Let $v$ be the leaf in $T_{t_k}$ that minimizes $\norm{y-s^*(v)}$ (over all leaves of $T_{t_k}$). Equivalently, $v$ is the top node of the long edge whose bottom node is $r_{t_k+1}$.
Let $\ell:=\max\{\ell(v),\ell(v_k)\}$.
By choice of $t_k$ we have $v\neq v_k$, hence the centers of these two leaves are separated already at level $\ell$, hence $\norm{x_{c(v_k)}-x_{c(v)}}\geq2^{\ell}$ by Lemma~\ref{lmm:separation}.
By two applications of Lemma~\ref{lmm:surrogates} we have $\norm{x_{c(v_k)}-s^*(v_k)}ֿ\leq2^{\ell}\epsilon$ and $\norm{x_{c(v)}-s^*(v)}ֿ\leq2^{\ell}\epsilon$.
Together, $\norm{s^*(v_k)-s^*(v)}\geq(1-2\epsilon)\cdot2^{\ell}$.
Since $y$ is closer to $s^*(v)$ than to $s^*(v_k)$ (by choice of $v$), we have
$\norm{y-s^*(v_k)} \geq \frac12\cdot\norm{s^*(v_k)-s^*(v)} \geq \left(\frac12-\epsilon\right)\cdot2^{\ell}$.
%\begin{equation}\label{eq:y_vk}
%\norm{y-s^*(v_k)} \geq \frac12\cdot\norm{s^*(v_k)-s^*(v)} \geq \left(\frac12-\epsilon\right)\cdot2^{\ell} .
%\end{equation}
%Combining~\cref{eq:y_vk} and~\cref{eq:xk_vk} yields
Combining this with $\norm{x_k-s^*(v_k)}ֿ\leq2\cdot2^{\ell(v_k)}\epsilon$, which was shown above, yields
$\norm{x_k-s^*(v_k)}ֿ\leq\epsilon\cdot\frac{1}{1/2-\epsilon}\cdot\norm{y-s^*(v_k)}=O(\epsilon)\cdot\norm{y-s^*(v_k)}$.
Therefore,
$\norm{y-x_k} = \norm{y-s^*(v_k)} \pm \norm{x_k-s^*(v_k)} = (1\pm O(\epsilon))\cdot\norm{y-s^*(v_k)}$,
%\[
%  \norm{y-x_k} =
%  \norm{y-s^*(v_k)} \pm \norm{x_k-s^*(v_k)} =
%  (1\pm O(\epsilon))\cdot\norm{y-s^*(v_k)},
%\]
which means $\norm{y-s^*(v_k)}$ is a good estimate for $\norm{y-x_k}$, and this is what the algorithm returns.

\textbf{Conclusion:} Combining both cases, we have shown that for any query point $y$, all distances from $y$ to $X$ can be estimated correctly with probability $1-O(\delta/q)$. Taking a union bound over $q$ queries, and scaling $\delta$ and $\epsilon$ appropriately by a constant, yields the theorem. \qed