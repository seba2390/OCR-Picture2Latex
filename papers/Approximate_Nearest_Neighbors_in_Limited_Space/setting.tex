\section{Formal Problem Statements}
\label{s:formal}
We formalize the problems in terms of one-way communication complexity.
The setting is as follows.
Alice has $n$ data points, $X=\{x_1,\ldots,x_n\} \subset \{-\Phi \ldots \Phi\}^d$, while
Bob has $q$ query points, $Y=\{y_1,\ldots,y_q\}  \subset \{-\Phi \ldots \Phi\}^d$, where $1\leq q\leq n$.
Distances are Euclidean, and we can assume w.l.o.g.~that $d\leq n$.\footnote{Any $N$-point Euclidean metric can be embedded into $N-1$ dimensions.}
% and Bob has $q$ points $Y=\{y_1,\ldots,y_q\}\subset\R^d$. 
%We assume that $x_1=\mathbf0$ (the origin).
% and denote the aspect ratio of $X\cup Y$ by $\Phi$.
Let $\epsilon,\delta\in(0,1)$ be given parameters.
In the one-way communication model, Alice computes a compact representation (called a~\emph{sketch}) of her data points and sends it to Bob, who then needs to report the output.
We define two problems in this model (with private randomness), each parameterized by $n,q,d,\Phi,\epsilon,\delta$:\footnote{Throughout we use $[m]$ to denote $\{1,\ldots,m\}$, for an integer $m>0$.}

\paragraph{Problem 1 -- All-nearest-neighbors:}
Bob needs to report a $(1+\epsilon)$-approximate nearest neighbor in $X$ for all his points simultaneously, with probability $1-\delta$.
That is, for every $j\in[q]$, Bob reports an index $i_j\in[n]$ such that 
\[ \Pr\left[ \forall j\in[q], \;\; \norm{y_j-x_{i_j}}\leq(1+\epsilon)\min_{i\in[n]}\norm{y_j-x_i} \right] \geq 1-\delta . \]

Our upper bound for this problem is stated in~\Cref{thm:ann_ub}.
%For this problem we give the following upper bound.

\paragraph{Problem 2 -- All-cross-distancess:}
Bob needs to estimate all distances $\norm{x_i-y_j}$ up to distortion $(1\pm\epsilon)$ simultaneously, with probability $1-\delta$.
That is, for every $i\in[n]$ and $j\in[q]$, Bob reports an estimate $E(i,j)$ such that
\[ \Pr\Big[ \forall i\in[n],j\in[q], \;\; (1-\epsilon)\norm{x_i-y_j} \leq E(i,j) \leq (1+\epsilon)\norm{x_i-y_j} \Big] \geq 1-\delta . \]
Our upper and lower bounds for this problem are stated in~\Cref{thm:distances_ub,thm:distances_lb}.

%For this problem we give the following upper bound.

%We also show a nearly tight lower bound, by adapting techniques from~\cite{molinaro2013beating} and~\cite{indyk2017near}.


%\subsection{Prior Results}
%Let us state the previously known upper bounds for the foregoing problems.
%We will also require them for our proofs.
%
%\paragraph{Dimension redcution (\cite{johnson1984extensions}}
%We state a variant due to~\cite{achlioptas2001database}.
%\begin{theorem}
%Let $m=c\cdot\epsilon^{-2}\log(1/\delta)$ for a sufficianly large constant $c>0$.
%Let $M$ be a $m\times d$ matrix with entries picked independently and uniformly at random from $\{-\frac1{\sqrt{m}},\frac1{\sqrt{m}}\}$.
%Then for every $x,y\in\R^d$,
%\[ \Pr\Big[ (1-\epsilon)\norm{x-y} \leq \norm{Mx-My} \leq (1+\epsilon)\norm{x-y} \Big] \geq 1-\delta. \]
%\end{theorem}
%To solve either of the one-way communication problems above, Alice and Bob can choose $M$ using shared randomness. Alice sends the sketch $\{Mx_i:i\in[n]\}$. Bob can estimate the distance $\norm{x_i-y_j}$, for any $i\in[n],j\in[q]$, as $\norm{Mx_i-My_j}$. Setting $\delta'=\delta/(nq)$ and taking a union bound over all pairs, ensures that with probability $1-\delta$, all estimates are correct up to distortion $(1\pm\epsilon)$.
%Since each $Mx_i$ has coordinates in $\{-\frac{1}{\sqrt m}d\Phi \dots \frac{1}{\sqrt m}d\Phi\}$, it takes $O(m\log(d\Phi))$ bits to store. Noting that $m=O(\epsilon^{-2}\log(nq/\delta))$ and $q,d=O(n)$, the total sketch size is $O(\epsilon^{-2}n\log(n)\log(d\Phi)$).
%
%\paragraph{Distance sketches (\cite{kushilevitz2000efficient})}
%\begin{theorem}\
%\end{theorem}
%\textcolor{red}{(ASK PIOTR)}\\
%
%\textcolor{red}{We remark that both of the above are data oblivious whereas our are data-dependent.}
