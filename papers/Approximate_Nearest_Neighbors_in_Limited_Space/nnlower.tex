\section{Approximate Nearest Neighbor Sketching Lower Bound}\label{sec:nnlower}

\begin{theorem}\label{thm:ann_lb}
Suppose that $d\geq\Omega(\epsilon^{-2}\log n)$, $\Phi\geq1/\epsilon$, and $1/n^{0.5-\beta}\leq\epsilon\leq\epsilon_0$ for a constant $\beta>0$ and a sufficiently small constant $\epsilon_0$.
Suppose also that $\delta<1/n^2$.
Then, for the all-nearest-neighbors problem, Alice must use a sketch of at least $\Omega(\beta\epsilon^{-2}n\log n)$ bits.
\end{theorem}
\begin{proof}
We start with dimension $d=n+1+\log n$; it can then be reduced by standard dimension reduction.
Fix $k=1/\epsilon^2$ and assume w.l.o.g.~that $k$ is a square integer (by taking $\epsilon$ to be appropriately small). Note that since $\epsilon>1/\sqrt{n}$ we have $k\leq n$, and that since $\Phi\geq1/\epsilon$ we have $\sqrt{k}\leq\Phi$. 

The data set will consist of $2n$ points, $x_1,\ldots,x_n$ and $z_1,\ldots,z_n$.
Let $i\in[n]$. We choose the first $n$ coordinates of $x_i$ to be an arbitrary $k$-sparse vector, in which each nonzero coordinate equals $1/\sqrt{k}$. Note that the norm of this part is $1$. The $(n+1)$th coordinate of $x_i$ is set to $0$. The remaining $\log n$ coordinates encode the binary encoding of $i$, with each coordinate multiplied by $10$.

Next we define $z_i$. The first $n$ coordinates are $0$. The $(n+1)$th coordinate equals $\sqrt{1-\epsilon}$. The remaining $\log n$ coordinates encode $i$ similarly to $x_i$.

The number of different choices for $\{x_1,\ldots,x_n\}$ is ${n\choose k}^n$.
Therefore if we show that one can fully recover $x_1,\ldots,x_n$ from a given all-nearest-neighbor sketch of the dataset, we would get the desired lower bound
\[
  \log\left({n\choose k}^n\right) \geq
   nk\log(n/k) = \epsilon^{-2}n\log(\epsilon^2n) =\epsilon^{-2}n\log(n^{2\beta}) = 2\beta\epsilon^{-2}n\log n .
\]

Suppose we have such a sketch.
For given $i,j\in[n]$ we now show how to recover the $j$th coordinate of $x_i$, denoted $x_i(j)$, with a single approximate nearest neighbor query.
Let $y_{ij}$ be the following vector in $\R^d$: The first $n+1$ coordinates are all zeros, except for the $j$th coordinate which is set to $1$. The last $\log n$ coordinates encode $i$ similary to $x_i$ and $z_i$.

Consider the distances from $y_{ij}$ to all data points. We start with $x_i$. It is identical to $y_i$ in the last $\log n+1$ coordinates, so we will restrict both to the first $n$ coordinates and denote the restricted vectors by $x_i^{:n}$ and $y_{ij}^{:n}$. $x_i^{:n}$ is a $k$-sparse vector with nonzero entries equal to $1/\sqrt{k}$, hence $\norm{x_i^{:n}}=1$. $y_{ij}^{:n}$ is just the standard basis vector $e_j$ in $\R^n$. Hence,
\[
  \norm{x_i-y_{ij}}^2 =
  \norm{x_i^{:n}-y_{ij}^{:n}}^2 =
  \norm{x_i^{:n}}^2 + \norm{y_{ij}^{:n}}^2 - 2(x_i^{:n})^\top y_{ij}^{:n} =
  2-2x_i(j).
\]
This equals $2$ if $x_i(j)=0$ and $2-2/\sqrt k=2-2\epsilon$ if $x_i(j)=1/\sqrt k$.

Next consider $z_i$. It is identical to $y_{ij}$ in all except the $j$th coordinate, which is $0$ in $z_i$ and $1$ in $y_{ij}$, and the $(n+1)$th coordinate, which is $0$ for $y_{ij}$ and $\sqrt{1-\epsilon}$ for $z_i$. Therefore, $\norm{z_i-y_{ij}}^2=2-\epsilon$.

Finally, for every $i'\neq i$, both $x_{i'}$ and $z_{i'}$ are at distance at least $10$ from $y_{ij}$ due to the encoding of $i$ (as binary multiplied by $10$) in the last $\log n$ coordinates.

In summation we have established the following:
\begin{itemize}
  \item If $x_i(j)\neq0$, then the closest point to $y_{ij}$ in the dataset is $x_i$ at distance $\sqrt{2-2\epsilon}$, and the next closest point is $z_i$ at distance $\sqrt{2-\epsilon}$.
  \item If $x_i(j)=0$, then the closest point to $y_{ij}$ in the dataset is $z_i$ at distance $\sqrt{2-\epsilon}$, and the next closest point is $x_i$ at distance $2$.
\end{itemize}
Therefore, if the sketch supports $(1+\frac{1}{8}\epsilon)$-approximate nearest neighbors, we can recover the true nearest neighbor of $y_{ij}$ and thus recover $x_i(j)$. By hypothesis, the query succeeds with probability $\delta<1/n^2$.
By a union bound over all $i,j\in[n]$ we can recover all of $x_1,\ldots,x_n$ simultaneously, and the theorem follows.
\end{proof}