
\section{Basic Sketch}\label{sec:sketch}

In this section we describe the basic data structure (generated by Alice) used for all of our results.
% Below we describe Alice's sketching algorithm. Assume w.l.o.g.~that the minimum distance within $X\cup Y$ is $1$ and the diameter is $\Phi$. 
The data structure augments the representation from~\cite{indyk2017near}, which we will now reproduce.
For the sake of readability, the notions from the latter paper (tree construction via hierarchical clustering, centers, ingresses and surrogates) are interleaved with the new ideas introduced in this paper (top-out compression, grid quantization and surrogate hashing).
Proofs in this section are deferred to Appendix~\ref{sec:sketch_proofs}.

%\textcolor{blue}{Changes from the SODA construction are colored in blue.}

\subsection{Hierarchical Clustering Tree}
The sketch consists of an annotated hierarchical clustering tree, which we now describe with our modified ``top-out compression'' step.

\paragraph{Tree construction}
We construct the inter-link hierarchical clustering tree of $X$: In the bottom level (numbered $0$) every point is a singleton cluster, and level $\ell>0$ is formed from level $\ell-1$ by recursively merging any two clusters whose distance is at most $2^\ell$, until no two such clusters are present.
We repeat this until level $\lceil\log(2\sqrt d\Phi)\rceil$, even if all points in $X$ are already joined in one cluster at a lower level.
%Note that every level in the tree induces a partition of $X$ into the clusters associated with the nodes at that level.
The following observation is immediate.
\begin{lemma}\label{lmm:separation}
If $x,x'\in X$ are in different clusters at level $\ell$, then $\norm{x-x'}\geq2^\ell$.
\end{lemma}

\paragraph{Notation}
Let $T^*$ denote the tree.
For every tree node $v$, we denote its level by $\ell(v)$, its associated cluster by $C(v)\subset X$, and its cluster diameter by $\Delta(v)$. For a point $x_i\in X$, let $\mathrm{leaf}(x_i)$ denote the tree leaf whose associated cluster is $\{x_i\}$.


\paragraph{Top-out compression}
The~\emph{degree} of a node in $T^*$ is its number of children.
A~\emph{$1$-path with $k$ edges} in  $T^*$ is a downward path $u_0,u_1,\ldots,u_k$, such that (i) each of the nodes $u_0,\ldots,u_{k-1}$ has degree $1$, (ii) $u_k$ has degree either $0$ or more than $1$, (iii) if $u_0$ is not the root of $T^*$, then its ancestor has degree more than $1$.

For every node $v$ denote $\Lambda(v) := \log(\Delta(v)/(2^{\ell(v)}\epsilon))$. 
If $v$ is the bottom of a $1$-path with more than $\Lambda(v)$ edges, we replace all but the bottom $\Lambda(v)$ edges with a~\emph{long edge}, and annotate it by the length of the path it represents.
More precisely, if the downward $1$-path is $u_0,\ldots,u_k=v$ and $k>\Lambda(v)$, then we connect $u_0$ directly to $u_{k-\Lambda(v)}$ by the long edge, and the nodes $u_1,\ldots,u_{k-\Lambda(v)-1}$ are removed from the tree, and the long edge is annotated with length $k-\Lambda(v)$.

\begin{lemma}\label{lmm:tree_size}
The compressed tree has $O(n\log(1/\epsilon))$ nodes.
\end{lemma}


We henceforth refer only to the compressed tree, and denote it by $T$.
However, for every node $v$ in $T$, $\ell(v)$ continues to denote its level before compression (i.e., the level where the long edges are counted according to their lengths).
We partition $T$ into~\emph{subtrees} by removing the long edges.
Let $\mathcal F(T)$ denote the set of subtrees.

\begin{lemma}\label{lmm:subtree_root}
Let $v$ be the bottom node of a long edge, and $x,x'\in C(v)$. Then $\norm{x-x'}\leq2^{\ell(v)}\epsilon$.
\end{lemma}

\begin{lemma}\label{lmm:subtree_leaf}
Let $u$ be a leaf of a subtree in $\mathcal F(T)$, and $x,x'\in C(u)$. Then $\norm{x-x'}\leq2^{\ell(u)}\epsilon$.
\end{lemma}


\subsection{Surrogates}
The purpose of annotating the tree is to be able to recover a list of~\emph{surrogates} for every point in $X$.
A surrogate is a point whose location approximates $x$.
Since we will need to compare $x$ to a new query point, which is unknown during sketching, we define the surrogates to encompass a certain amount information about the absolute point location, by hashing a coarsened grid quantization of a representative point in each subtree.

\paragraph{Centers}
With every tree node $v$ we associate an index $c(v)\in[n]$ such that $x_{c(v)}\in C(v)$, and we call $x_{c(v)}$ the~\emph{center} of $C(v)$.
The centers are chosen bottom-up in $T$ as follows.
For a leaf $v$, $C(v)$ contains a single point $x_i\in X$, and we set $c(v)=i$. 
For a non-leaf $v$ with children $u_1,\ldots,u_k$, we set $c(v)=\min\{c(u_i):i\in[k]\}$.

\paragraph{Ingresses}
Fix a subtree $T'\in\mathcal F(T)$.
To every node $u$ in $T'$, except the root, we will now assign an~\emph{ingress} node, denoted $\mathrm{in(u)}$.
Intuitively this is a node in the same subtree whose center is close to $u$, and the purpose is to store the location of $u$ by its quantized displacement from that center (whose location will have been already stored, by induction).

We will now assign ingresses to all children of a given node $v$. (Doing this for every $v$ in $T'$ defines ingresses for all nodes in $T'$ except its root.) Let $u_1,\ldots,u_k$ be the children of $v$, and w.l.o.g.~$c(v)=c(u_1)$. Consider the graph $H_v$ whose nodes are $u_1,\ldots,u_k$, and $u_i,u_j$ are neighbors if there are points $x\in C(u_i)$ and $x'\in C(u_j)$ such that $\norm{x-x'}\leq2^{\ell(v)}$. By the tree construction, $H_v$ is connected. We fix an arbitrary spanning tree $\tau(v)$ of $H_v$ which is rooted at $u_1$.

For $u_1$ we set $\mathrm{in}(u_1):=v$. For $u_i$ with $i>1$, let $u_j$ be its (unique) direct ancestor in the tree $\tau(v)$. Let $x\in C(u_j)$ be the closest point to $C(u_i)$ in $C(u_j)$. Note that in $T$ there is a downward path from $u_j$ to $\mathrm{leaf}(x)$. Let $u_x$ be the bottom node in that path that belongs to $T'$. (Equivalently, $u_x$ is the bottom node on that downward path that is reachable from $u$ without traversing a long edge.) We set $\mathrm{in}(u_i):=u_x$.

\paragraph{Grid net quantization}
Assume w.l.o.g.~that $\Phi$ is a power of $2$.
We define a hierarchy of grids aligned with $\{-\Phi \ldots \Phi\}^d$ as follows.
We begin with the single hypercube whose corners are $(\pm\Phi, \ldots, \pm\Phi)^d$.
We generate the next grid by halving along each dimension, and so on.
For every $\gamma>0$, let $\mathcal{N}_\gamma$ be the coarsest grid generated, whose cell side is at most $\gamma/\sqrt{d}$. Note that every cell in $\mathcal{N}_\gamma$ has diameter at most $\gamma$.
For a point $x\in\R^d$, we denote by $\mathcal{N}_\gamma[x]$ the closest corner of the grid cell containing it.

We will rely on the following fact about the intersection size of a grid and a ball; see, for example, \cite{har2012approximate}.

\begin{claim}\label{clm:gridball}
For every $\gamma>0$, the number of points  in $\mathcal N_\gamma$ at distance at most $2\gamma$ from any given point, is at most $O(1)^d$.
\end{claim}


\paragraph{Surrogates}
Fix a subtree $T'\in\mathcal F(T)$. With every node $v$ in $T'$ we will now associate a~\emph{surrogate} $s^*(v)\in\R^d$.
Define the following for every node $v$ in $T'$:
\[
  \gamma(v) = 
  \begin{cases}
  \left(5 + \lceil\frac{\Delta(v)}{2^{\ell(v)}}\rceil\right)^{-1}\cdot\epsilon & \text{if $v$ is a leaf in $T'$,}\\
  \left(5 + \lceil\frac{\Delta(v)}{2^{\ell(v)}}\rceil\right)^{-1} & \text{otherwise.}
  \end{cases}
\]
The surrogates are defined by induction on the ingresses.
%Note that we have defined an ingress node for every node in $T'$ except its root.

Induction base: For the root $v$ of $T'$ we set $s^*(v) := \mathcal N_{2^{\ell(v)}}[x_{c(v)}]$.

Induction step: For a non-root $v$ we denote the quantized displacement of $c(v)$ from its ingress by $\eta(v)=\mathcal N_{\gamma(v)}\left[\frac{\gamma(v)}{2^{\ell(v)}}(x_{c(v)}-s^*(in(v)))\right]$, and set
$s^*(v) := s^*(in(v)) + \frac{2^{\ell(v)}}{\gamma(v)}\cdot\eta(v)$.

\begin{lemma}\label{lmm:surrogates}
For every node $v$, $\norm{x_{c(v)}-s^*(v)}\leq2^{\ell(v)}$.
Furthermore if $v$ is a leaf of a subtree in $\mathcal F(T)$, then $\norm{x_{c(v)}-s^*(v)}\leq2^{\ell(v)}\epsilon$.
\end{lemma}
%\begin{proof}
%By induction on the ingresses.
%In the base case we use that $\norm{x_{c(v)}-s^*(v)}\leq2^{\ell(v)}$ by the choice of grid net.
%The induction step is identical to~\cite{indyk2017near}.
%\end{proof}

\paragraph{Hash functions}
For every level $\ell$ in the tree, we pick a hash function $H_\ell:\mathcal N_{2^\ell}\rightarrow[m]$, from a universal family (\cite{carter1979universal}), where $m=O(1)^d\cdot\log(2\sqrt d\Phi)\cdot q/\delta$.
The $O(1)$ term is the same constant from~\Cref{clm:gridball} above.
For every subtree root $v$, we store its hashed surrogate $H_{\ell(v)}(\mathcal N_{2^{\ell(v)}}[x_{c(v)}])$.
We also store the description of each hash function $H_\ell$ for every level $\ell$.

\subsection{Sketch Size}
The sketch contains the tree $T$, with each node $v$ annotated by its center $c(v)$, ingress $\mathrm{in(u)}$, precision $\gamma(v)$ and quantized displacement $\eta(v)$ (if applicable). For subtree roots we store their hashed surrogate, and for long edges we store their length. We also store the hash functions $\{H_\ell\}$.
\begin{lemma}\label{lmm:sketch_size}
The total sketch size is
\[
  O \left( n\left((d+\log n)\log(1/\epsilon) + \log\log\Phi + \log\frac{q}{\delta}\right) + d\log\Phi \right)
  \;\; \text{bits.}
\]
\end{lemma}
%\begin{proof}
%The sketch of~\cite{indyk2017near} stores the compressed tree $T'$, with each node annotated by its center $c(v)$, ingress $\mathrm{in}(v)$, precision $\gamma(v)$ and quantized displacement $\eta(v)$.
%Every long edge is annotated by its length.
%They show this takes $O\left( n\left((d+\log n)\log(1/\epsilon) + \log\log\Phi\right)\right)$ bits;
%note that by Lemma~\ref{lmm:tree_size}, top-out compression did not effect this bound.
%
%We additionally store the hashed surrogates of subtree roots.
%There are $O(n)$ subtrees,\footnote{By construction, the tree of subtrees in $T$ has no degree-$1$ nodes. Since $T$ has $n$ leaves, there are at most $2n-1$ subtrees.} and each hash takes $\log m$ bits to store, which adds $O(n(d+\log\log\Phi+\log(q/\delta)))$ bits to the above.
%Finally, we store the hash functions $H_\ell$ for every $\ell$.
%The domain of each $H_\ell$ is $N_{2^\ell}$, which is a subset of $\{-\Phi \ldots \Phi\}^d$, and hence $H_\ell$ can be specified by $O(\log(\Phi^d))$ random bits (\cite{carter1979universal}).
%Since we do not require independence between hash functions of different levels, we can use the same random bits for all hash functions, adding a total of $O(d\log\Phi)$ bits to the sketch.
%\end{proof}

As a preprocessing step, Alice can reduce the dimension of her points to $O(\epsilon^{-2}\log(qn/\delta))$ by a Johnson-Lindenstrauss projection.
She then augments the sketch with the projection, in order for Bob to be able to project his points as well.
By~\cite{kane2011almost}, the projection can be stored with $O(\log d + \log(q/\delta)\cdot\log\log((q/\delta)/\epsilon))$ bits.
This yields the sketch size stated in Theorem~\ref{thm:ann_ub}.


\paragraph{Remark} Both the hash functions and the projection map can be sampled using public randomness.
If one is only interested in the communication complexity, one can use the general reduction from public to private randomness due to~\cite{newman1991private}, which replaces the public coins by augmenting $O(\log(nd\Phi))$ bits to the sketch (since Alice's input has size $O(nd\Phi)$ bits).
The bound in~\Cref{thm:ann_ub} then improves to $O\left( n\left(\frac{\log n\cdot\log(1/\epsilon)}{\epsilon^2} + \log\log\Phi + \log\left(\frac{q}{\delta}\right)\right) + \log\Phi \right)$ bits, and the bound in~\Cref{thm:distances_ub} improves to $O\left(\frac{n}{\epsilon^2}\left(\log n\cdot\log(1/\epsilon) + \log(d\Phi)\log\left(\frac{q}{\delta}\right)\right) \right)$ bits.
However, that reduction is non-constructive; we state our bounds so as to describe explicit sketches.
