
\section{Lower Bound for Distance Estimation}\label{sec:dist_lb}
In this section we prove~\Cref{thm:distances_lb}.
We handle the two terms in the lower bound separately.

\subsection{First Lower Bound Term}
\begin{lemma}\label{lmm:lb1}
Suppose that $d\geq\Omega(\epsilon^{-2}\log n)$; $\Phi\geq1/\epsilon$; $\epsilon$ is at most a sufficiently small constant; and $\epsilon\geq1/n^{0.5-\rho'}$ for a constant $\rho'>0$.
Then, for the all-cross-distances problem, Alice must use a sketch of at least $\Omega(\rho'\epsilon^{-2}n\log n)$ bits.
\end{lemma}
\begin{proof}
Consider the following problem: Given a dataset $X\subset\{-\Phi \ldots \Phi\}^d$ consisting of $n$ points, we need to produce a sketch, from which we can recover (deterministically) all distances $\{\norm{x-x'}:x,x'\in X\}$ up to distortion $1\pm\epsilon$.
This is the problem considered in~\cite{indyk2017near}, and they show a lower bound of $\Omega(\rho'\epsilon^{-2}n\log n)$ under the asssumptions of the current lemma. We will obtain the same lower bound by reducing this problem to all-cross-distances.

To this end, suppose we have a given sketching procedure for the all-cross-distances problem that uses $s=s(n,d,\Phi,1,\epsilon,\delta)$ amortized bits per point.
We invoke it on $X$ and denote the resulting sketch by $S_0$. For every point $y\in\{-\Phi \ldots \Phi\}^d$, with probabiliy $1-\delta$, all distances $\{\norm{x-y}:x\in X\}$ can be recovered from $S_0$. In particular, this holds in expectation for $(1-\delta)n$ of the points in $X$.
By Markov's inequality, this holds for $\frac12(1-\delta)n>\frac14n$ of the points in $X$ with probability at least $1/2$.
We proceed by recursion on the remaining $\frac34n$ points in $X$.
The sketch produced in the $i$th step of the recursion is denoted by $S_i$ and has total size $(\frac34)^ins$ bits.
After $t=O(\log n)$ steps, with nonzero probability $1/n^{O(1)}$, we have produced a sequence of sketches $S_0,\ldots,S_t$ from which every distance in $\{\norm{x-x'}:x,x'\in X\}$ can be recovered, with a total size of $O(\sum_{i=0}^t(\frac34)^ins)=O(ns)$ bits. This yields the desired lower bound.
\end{proof}

%%% DIRECT PROOF that needs to assume $\delta<1/n$
%\begin{proof}
%%Since $q$ does not appear in the claimed lower bound, we may take $q=n$.
%The proof is essentially identical to the lower bound proof in~\cite{indyk2017near}.
%Let $k=1/\epsilon^2$ and assume w.l.o.g.~that $k$ is a square integer (by taking $\epsilon$ to be appropriately small). Note that since $\epsilon>1/\sqrt{n}$ we have $k\leq n$, and that since $\Phi\geq1/\epsilon$ we have $\sqrt{k}\leq\Phi$. 
%Let $X$ be an arbitrary set of $k$-sparse vectors in $\R^n$ (with repetitions), in which each non-zero coordinate equals $1/\sqrt{k}$. For $i\in[n]$, denote by $x(i)$ the $i$th coordinate of $x\in X$.
%Let $Y=\{e_1,\ldots,e_n\}$ be the set of standard basis vectors in $\R^n$.
%These defines the hard family of inputs $X,Y$ for Alice and Bob in the all-cross-distances problem.
%
%Let $x\in X$ and $e_i\in Y$. Note that $\norm{x}=\norm{e_i}=1$ and hence $\norm{x-e_i}=2-2x^\top e_i=2-2x(i)$.
%This equals $2$ if $x(i)=0$ and $2-2/\sqrt{k}=2-2\epsilon$ if $x(i)=1/\sqrt{k}$.
%Therefore approximating the distance $\norm{x-e_i}$ up to distortion $1\pm\epsilon/2$ is sufficient to recover $x(i)$.
%If Bob can estimate all cross distances between $X$ and $Y$, he can thus recover all vectors in $X$.
%Therefore, the number of bits needed to specify $X$ is a lower bound on Alice's sketch size.
%The number of choices for $X$ is ${n\choose k}^n$, and hence the resulting lower bound is
%\[
%  \log\left({n\choose k}^n\right) \geq
%   nk\log(n/k) = \epsilon^{-2}n\log(\epsilon^2n) =\epsilon^{-2}n\log(n^{2\rho'}) = 2\rho'\epsilon^{-2}n\log n,
%\]
%where we have used the hypothesis $\epsilon\geq1/n^{0.5-\rho'}$.
%
%Finally, note that we proved the lower bound in the setting $d=n$.
%However, we can embed the hard inputs into dimension as low as $d\geq\Omega((\beta\epsilon)^{-2}\log n)$ by the Johnson-Lindenstrauss theorem, and distort the distances by only $(1\pm\beta\epsilon)$, which for a sufficiently small constant $\beta$ would not effect the above arguments.
%\end{proof}

\subsection{Second Lower Bound Term}

\begin{lemma}\label{lmm:lb2}
Suppose that $d^{1-\rho}\geq\epsilon^{-2}\log(q/\delta)$ for a constant $\rho>0$, and $\epsilon$ is at most a sufficiently small constant.
Then, for the all-cross-distances problem, Alice must use a sketch of at least $\Omega(\epsilon^{-2}n\log(d\Phi)\log(q/\delta))$ bits.
\end{lemma}

The proof is by adapting the framework of~\cite{molinaro2013beating}, who proved (among other results) this statement for the case $q=n$.
We describe the adaption and refer to~\cite{molinaro2013beating} for missing details that remain similar.

\subsubsection{Preliminaries}
We follow the approach of~\cite{jayram2013optimal}, of proving one-way communication lower bounds by reduction to variants of the augmented indexing problem, defined next.
\begin{definition}[Augmented Indexing]\label{def:augind}
In the Augmented Indexing problem $AugInd(k,\delta)$, Alice gets a vector $A$ with $k$ entries, whose elements are entries of a universe of size $20/\delta$.
Bob gets an index $i\in[k]$, an element $e$, and the elements $A(i')$ for every $i'<i$.
Bob needs to decide whether $e=A(i)$, and succeed with probability $1-\delta$.
\end{definition}

\cite{jayram2013optimal} give a one-way communication lower bound of $\Omega(k\log(1/\delta))$ for this problem.
The main component in~\cite{molinaro2013beating} is a modified one-way communication model, in which the protocol is allowed to abort with a substantially larger (constant) probability than it is allowed to err.
We will refer to it simply as the~\emph{abortion model} and refer to~\cite{molinaro2013beating} for the exact definition (which we will not require).
They prove the same lower bound for Augmented Indexing.
\begin{lemma}[informal]\label{lmm:augind}
In the abortion model, the one-way communication complexity of $AugInd(k,\delta)$ is $\Omega(k\log(1/\delta))$.
\end{lemma}

\subsubsection{Variants of Augmented Indexing}

We start by defining a variant of augmented indexing that will be suitable for out purpose.

\begin{definition}
In the Matrix Augmented Indexing problem $MatAugInd(k,m,\delta)$, Alice gets a matrix $A$ of order $k\times m$, whose entries are elements of a universe of size $1/\delta$.
Bob gets indices $i\in[k]$ and $j\in[m]$, an element $e$, and the elements $A(i,j')$ for every $j<j'$.
Bob needs to decide whether $e=A(i,j)$, and succeed with probability $1-\delta$.
\end{definition}

This problem is clearly at least as difficult as $AugInd(km,\delta)$ from Definition~\ref{def:augind}, since in the latter Bob gets more information (namely, if we arrange the vector $A$ in $AugInd(km,\delta)$ as a $k\times m$ matrix, then 
Bob gets all entries of $A$ which lexicographically precede $A(i,j)$).
We get the following immediate corollary from Lemma~\ref{lmm:augind}.

\begin{corollary}\label{cor:mataugind}
In the abortion model,
the one-way communication complexity of $MatAugInd(k,m,\delta)$ is $\Omega(km\log(1/\delta))$.
\end{corollary}

\cite{molinaro2013beating} reformulate Augmented Indexing so that Alice's input is a set instead of vector.
Similary, we reformulate Matrix Augmented Indexing as follows.

\begin{definition}
Let $m>0$ and $k>0$ be integers, and $\delta\in(0,1)$.
Partition the interval $[m/\delta]$ into $m$ intervals $I_1,\ldots,I_m$ of size $1/\delta$ each.

In the Augmented Set List problem $AugSetList(k,m,\delta)$, Alice gets a list of subsets $S_1,\ldots,S_k\subset [m/\delta]$, such that each $S_i$ has size exactly $m$ and contains exactly one element from each interval $I_1,\ldots,I_m$.
Bob gets an index $i\in[k]$, an element $e\in[m/\delta]$ and a subset $T$ of $S_i$ that contains exactly the elements of $S_i$ that are smaller than $e$.
Bob needs to decide whether $e\in S_i$, and succeed with probability at least $1-\delta$.
\end{definition}

The equivalence to Matrix Augmented Indexing is not hard to show; the details are similar to~\cite{molinaro2013beating} and we omit them here. By the equivalence, we get the following corollary from Corollary~\ref{cor:mataugind}.
\begin{corollary}\label{cor:setlist}
In the abortion model,
the one-way communication complexity of $AugSetList(k,m,\delta)$ is $\Omega(km\log(1/\delta))$.
\end{corollary}

Next we define the $q$-fold version of the same problem.
\begin{definition}\label{def:qaugsetlist}
In the problem $q$-$AugSetList(k,m,\delta)$, Alice and Bob get $q$ instances of AugSetList$(k,m,\delta/q)$, and Bob needs to answer correctly on all of them simoultaneously with probability at least $1-\delta$.
\end{definition}

The main tehcnical result of~\cite{molinaro2013beating} is, loosely speaking, a direct-sum theorem which lifts a lower bound in the abortion model to a $q$-fold lower bound in the usual model.
Applying their theorem to Corollary~\ref{cor:setlist}, we obtain the following.
\begin{corollary}\label{cor:augsetlist}
The one-way communication complexity of $q$-$AugSetList(k,m,\delta)$ is $\Omega(qkm\log(q/\delta))$.
\end{corollary}

Finally, we construct a ``generalized augmented indexing'' problem over $r$ copies of the above problem.
\begin{definition}
In the problem $r$-$Ind(q$-$AugSetList(k,m,\delta))$,
Alice gets $r$ instances $A_1,\ldots,A_r$ of $q$-$AugSetList(k,m,\delta)$.
Bob gets an index $j\in[r]$, his part $B_j$ of instance $j$, and Alice's instances $A_1,\ldots,A_{j-1}$.
Bob needs to solve instance $j$ with success probability at least $1-\delta$.
\end{definition}

By standard direct sum results in communication complexity (reproduced in~\cite{molinaro2013beating}) we obtain from Corollary~\ref{cor:augsetlist} the final lower bound we need.
\begin{proposition}\label{prp:lb}
The one-way communication complexity of $r$-$Ind(q$-$AugSetList(k,m,\delta))$ is $\Omega(rqkm\log(q/\delta))$.
\end{proposition}

\subsubsection{Reductions to All-Cross-Distances}

We now prove Lemma~\ref{lmm:lb2}, by reducing $r$-$Ind(q$-$AugSetList(k,m,\delta))$ to the all-cross-distances problem.
We will use two reductions, to get a lower bound once in terms of $d$ and once in terms of $\Phi$.
Specifically, in the first reduction
we will set $m=1/\epsilon^2$, $k=n/q$ and $r=\rho\log d$ (where $\rho$ is the constant from the statement of Lemma~\ref{lmm:lb2}). Then the lower bound we would get by Proposition~\ref{prp:lb} is $\Omega\left(\epsilon^{-2}n\log d\log(q/\delta)\right)$.
In the second reduction we will set $r=\rho\log\Phi$, yielding the lower bound $\Omega\left(\epsilon^{-2}n\log\Phi\log(q/\delta)\right)$.
Together they lead to Lemma~\ref{lmm:lb2}.

In both settings, recall we are reducing to the following problem: For dimension $d=\Omega(\epsilon^{-2}\log(q/\delta))$ and aspect ratio $\Phi$, Alice gets $n$ points, Bob gets $q$ points, and Bob needs to estimate all cross-distances up to distortion $1\pm\epsilon$.

Consider an instance of $r$-$Ind(q$-$AugSetList(k,m,\delta))$.
It can be visualized as follows: Alice gets a matrix $S$ with $n=qk$ rows and $r$ columns, where each entry contains a set of size $m$.
Bob gets an index $j\in[r]$, indices $i_1,\ldots,i_q\in[k]$, elements $e_1,\ldots,e_q$, subsets $T_1\subset S(i_1,j),\ldots, T_q\subset S(i_q,j)$, and the first $j-1$ columns of the matrix $S$.

We now use the encoding scheme of~\cite{jayram2013optimal}, in the set formulation which was given in~\cite{molinaro2013beating}. We restate the result.
\begin{lemma}[\cite{jayram2013optimal}]\label{lmm:jw}
Let $m=1/\epsilon^2$ and $0<\eta<1$. Suppose we have the following setting:
\begin{itemize}
  \item Alice has subsets $S_1,\ldots,S_r$ of $[m/\eta]$.
  \item Bob has an index $j\in[r]$, an element $e\in[m/\eta]$, the subset $T\subset S_j$ of elements smaller than $e$, and the sets $S_1,\ldots,S_{j-1}$.
\end{itemize}
There is a shared-randomness mapping of their inputs into points $v_A,v_B$ and a scale $\Psi>0$ (the scale is known to both), such that
\begin{enumerate}
  \item $v_A,v_B\in\B^D$ for $D=O(\epsilon^{-2}\log(\frac1\eta)\exp(r))$.
  \item If $e\in S_j$ (YES instance) then w.p.~$1-\eta$, $\norm{v_A-v_B}^2 \leq (1-2\epsilon)\Psi$.
  \item If $e\notin S_j$ (NO instance) then w.p.~$1-\eta$, $\norm{v_A-v_B}^2 \geq (1-\epsilon)\Psi$
\end{enumerate}
\end{lemma}

\subsubsection{Lower Bound in terms of $d$}
We start with the first reduction that yields a lower bound in terms of $d$.
\begin{lemma}\label{lmm:lb2a}
Under the assumptions of Lemma~\ref{lmm:lb2}, for the all-cross-distances problem, Alice must use a sketch of at least $\Omega(\epsilon^{-2}n\log(d)\log(q/\delta))$ bits.
\end{lemma}

\begin{proof}
We invoke Lemma~\ref{lmm:jw} with $r=\rho\log d$ and $\eta=\delta/q$.
Note that the latter is the desired success probability in each instance of $q$-$AugSetList(k,m,\delta)$ (cf.~Definition~\ref{def:qaugsetlist}).
Alice encodes each row of the matrix, $(S(i,1),\ldots,S(i,r))$, into a point $x_i$, thus $n$ points $x_1,\ldots,x_n$.
Bob encodes $(S(i,1),\ldots,S(i_z,j-1),T_i,j,e_z)$ for each $z\in[q]$ into a point $y_z$, thus $q$ points $y_1,\ldots,y_z$.
For every $z\in[q]$, the problem represented by row $i_z$ in the matrix $S$ is reduced by Lemma~\ref{lmm:jw} to estimating the distance $\norm{x_{i_z}-y_z}$.
By Item 1 of Lemma~\ref{lmm:jw}, the points $\{x_i\}_{i\in[n]}$, $\{y_z\}_{z\in[q]}$ have binary coordinates and dimension $D=O(\epsilon^{-2}\log(q/\delta)d^\rho)$. 
By the hypothesis $d^{1-\rho}\geq\epsilon^{-2}\log(q/\delta)$ of Lemma~\ref{lmm:lb2}, $D=O(d)$.
Therefore Alice and Bob can now feed them into a given black-box solution of the all-cross-distances problem, which estimates all the required distances and solves $r$-$Ind(q$-$AugSetList(k,m,\delta))$.

Let us establish the success probability of the reduction.
Since we set $\eta=\delta/q$ in Lemma~\ref{lmm:jw}, it preserves each distance $\norm{x_{i_z}-y_z}$ for $z\in[q]$ with probability $1-\delta/q$. By a union bound, it preserves all of them simultaneously with probability $1-\delta$.
The success probability of the all-cross-distances problem, simultaneously on all query points $\{\tilde y_z:z\in[q]\}$, is again $1-\delta$. Altogether, the reduction succeeds with probability $1-O(\delta)$. As a result, the all-cross-distances problem solves the given instance of  $r$-$Ind(q$-$AugSetList(k,m,\delta))$, and Lemma~\ref{lmm:lb2a} follows.
\end{proof}

\subsubsection{Lower Bound in terms of $\Phi$}
We proceed to the second reduction that would yield a lower bound in terms of $\Phi$.
\begin{lemma}\label{lmm:lb2b}
Under the assumptions of Lemma~\ref{lmm:lb2}, for the all-cross-distances problem, Alice must use a sketch of at least $\Omega(\epsilon^{-2}n\log(\Phi)\log(q/\delta))$ bits.
\end{lemma}
\begin{proof}
We may assume that $\Phi\geq d$ since otherwise Lemma~\ref{lmm:lb2b} already follows from Lemma~\ref{lmm:lb2a}. Therefore $\Phi^{1-\rho}\geq\epsilon^{-2}\log(q/\delta)$.

The reduction is very similar to the one in Lemma~\ref{lmm:lb2a}.
Again we evoke Lemma~\ref{lmm:jw} with $\eta=\delta/q$, but this time we set $r=\rho\log\Phi$.
Again we denote Alice's encoded points by $x_1,\ldots ,x_n$, and Bob's by $y_1,\ldots,y_q$.
By Item 1 of Lemma~\ref{lmm:jw}, the points have binary coordinates and dimension $D=O(\epsilon^{-2}\log(q/\delta)\Phi^\rho)$. The difference from Lemma~\ref{lmm:lb2a} is that since it is possible that $\Phi\gg d$, the dimension $D$ is too large for the given black-box solution of the all-cross-distances problem (which is limited to dimension $O(d)$).

To solve this, Alice and Bob project their points into dimension $D'=O(\epsilon^{-2}\log(q/\delta))$ by a Johnson-Lindenstrauss transform, using shared randomness.
Let $\tilde x_1,\ldots,\tilde x_n$ and $\tilde y_1,\ldots,\tilde y_z$ denote the projected points.
After the projection each coordinate has magnitude at most $O(\epsilon^{-2}\log(q/\delta)\Phi^\rho)$.
By our assumption $\Phi^{1-\rho}\geq\Omega(\epsilon^{-2}\log(q/\delta))$, this is at most $O(\Phi)$.
Since the dimension $D'$ is $O(d)$, Alice and Bob can now feed $\tilde x_1,\ldots,\tilde x_n$ and $\tilde y_1,\ldots,\tilde y_z$ into a given black-box solution of the all-cross-distances problem with dimension $O(d)$ and aspect ratio $O(\Phi)$.

Let us establish the success probability of the reduction.
As before, Lemma~\ref{lmm:jw} preserves all the required distances, $\norm{x_{i_z}-y_z}$ for $z\in[q]$, with probability $1-\delta$.
The Johnson-Lindenstrauss transform into dimension $D'$ preserves each distance as $\norm{\tilde x_{i_z}-\tilde y_z}$ with probability at least $1-\delta$, since we picked the dimension to be $D'=O(\epsilon^{-2}\log(\frac{q}{\delta}))$.
The success probability of the all-cross-distances problem simultaneously is again $1-\delta$. Altogether, the reduction succeeds with probability $1-O(\delta)$. As a result, the all-cross-distances problem solves the given instance of  $r$-$Ind(q$-$AugSetList(k,m,\delta))$, and Lemma~\ref{lmm:lb2b} follows.
\end{proof}

\subsubsection{Conclusion}
Lemmas~\ref{lmm:lb2a} and~\ref{lmm:lb2b} together imply Lemma~\ref{lmm:lb2}.
The latter, together with Lemma~\ref{lmm:lb1}, implies~\Cref{thm:distances_lb}. \qed
