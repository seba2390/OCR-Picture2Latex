\section{Background}
\label{back}

\subsection{Convection Calculations}

The computational grid for the atmosphere is divided into latitudes, longitudes along the x and y direction and vertical columns along the z direction. The vertical columns extend from the surface up through the atmosphere. The columns are further divided into layers. The characteristic feature of the physics routines is that every column can be computed independent of the other columns giving rise to data parallelism that can be exploited on multi and many core architectures. For the purpose of load balancing among the different CPU processes, the columns are grouped into {\em chunks}. A chunk is a unit of execution containing a configurable number of columns. A chunk may or may not contain contiguous set of vertical columns i.e. columns from neighboring grid points may not be in a single chunk. These chunks are distributed among the MPI processes and the OpenMP threads. Every physics routine is called for each chunk. The pseudo-code for physics calculations is shown below.

\begin{algorithm}
\begin{small}
\SetKwInOut{Input}{input}
\SetKwInOut{Output}{output}
\SetKwFunction{stratiform}{stratiform}
\SetKwFunction{deepconvection}{deep\_convection}
\SetKwFunction{shallowconvection}{shallow\_convection}
\SetKwFunction{radiation}{radiation}

  \ForEach{time step}{
    \ForEach{chunk}{
      \stratiform() \;
      \deepconvection() \;
      \shallowconvection() \;
      ... \BlankLine
      \radiation() \;
      ...
    }
  }
\caption{Pseudocode for Physics Calculations in CAM}
\label{physics_pseudo}
\end{small}
\end{algorithm}

Convections are of two kinds: deep and shallow convections. \\
{\bf Deep Convection:} In tropics tall clouds occur on scales of 1-10 km much smaller than that of the resolved grid (typically about 50 km in climate models).  These clouds could be within thunderstorms or similar unsettled weather conditions.  While they cannot be resolved explicitly by the model, their effect is very important and needs to be incorporated. Hence they are parameterized. The parameterization in most climate models works on the basis of moist static instability of the atmospheric column. Formation of clouds removes the moist static instability and cause precipitation (which could be rain or snow). There are various schemes to parameterize these tall clouds. In CAM5 they are parameterized using the method of Zhang and McFarlane \cite{zhang-sensitivity-atmocn1995}. \\
{\bf Shallow Convection:} If the moist static instability is not present over the entire atmospheric column but over only part of it, clouds will form only in this part. Such a situation is handled by shallow convection routines. Shallow convection may or may not cause precipitation. In CAM5, shallow convection is handled using the University of Washington scheme \cite{park-UWshallowconvect-jc2009}.

Along  with  other  processes  like radiation  and  stratiform,  deep  and  shallow  convection  are  two  physical  processes that  are  applied  on  all  grid  points  before  coupling  to  land,  sea  and  ice  models.  Computations in these routines are related to local conditions and it is difficult to a-priori predict their occurrence. It is also difficult to determine a pattern in their occurrence -- though deep convection could be more prominent in the tropics than in extra-tropics. Such computations are therefore not conducted at every grid point at a particular time-step. The stencil of these computations would change with time and hence would be difficult to develop load-balancing techniques for these computations. Hence it is essential that these routines be accelerated.  Shallow and deep convection computations are parallel across chunks and the loops  within  the  shallow  and  deep  convection  routines  that  represent  the  core computations  iterate  over  columns  within  a  chunk. The chunk computations within an MPI process are parallelized across OpenMP threads.

The  core computations in deep convection consists of two loops - entrainment and precipitation loops, as shown in Figure \ref{deep_convect}.
Entrainment is a process in which dry air from outside a cloud mixes with the moist (almost saturated) air inside the cloud. This process reduces the relative humidity of the air and could result in shorter clouds of smaller heights. Precipitation is the process of conversion of water vapor inside the cloud converting into liquid droplets that coalesces and form rain. The entrainment and precipitation loops iterate over columns in a given chunk and the computations are independent across different columns/iterations. 
In this work, we exploit the fine-level parallelism of the column computations on Xeon Phi by adopting column-level parallelism for the convection routines and the default chunk-level parallelism for the other routines.

\begin{algorithm}
\begin{small}
\SetKwInOut{Input}{input}
\SetKwInOut{Output}{output}
\SetKwFunction{ientropy}{ientropy}
\SetKwFor{Do}{do}{}{end}

  \tcc{Entrainment Loop}
  \Do{i=1,numColumnsinChunk}{
    \Do{k=1,numVerticalLevels}{
      ... \BlankLine
      ... \BlankLine
      \ientropy() \;
      ...
    }
  }
  \BlankLine

  \tcc{Precipitation Loop}
  \Do{i=1,numColumnsinChunk}{
    \Do{k=1,numVerticalLevels}{
      ... \BlankLine
      ... \BlankLine
      \ientropy() \;
      ...
    }
  }

\caption{Deep Convection}
\label{deep_convect}
\end{small}
\end{algorithm}

\subsection{Intel Xeon Phi Architecture}

Intel's first generation Many Integrated Core (MIC) architecture codenamed Knights Corner, is an x86 based system that contains up to 61 in-order processing cores, and offers peak double precision performance of nearly 1.2 TFLOPS. Each of these cores supports up to 4-way hardware multithreading and features a vector unit which uses wide 512 bit registers. Each core features fully coherent L1 and L2 caches, with a bidirectional ring that provides fast access to L2 caches of other cores.

The Xeon Phi cards used in our experiments offers a peak memory bandwidth of nearly 352 GBPS, but the L1 and L2 caches offer much higher memory bandwidth than the memory, and using them effectively is the key to approaching peak performance. Apart from thread parallelism, it is crucial to properly utilize the wide vector unit; more so than on the Xeon which has only 256 bit wide registers. Although the Xeon Phi offers gather and scatter operations for vectors, vectorization is significantly more effective if data is contiguous in memory with unit stride accesses. Data alignment will provide even better vector performance. The Xeon Phi also features low precision hardware support for certain math functions like power, logarithm etc apart from FMA (Fused Multiply and Add) that provide additional speedup for workloads that have these computations.

Programming the Xeon Phi is similar to programming any other x86 machine - the same programming model is applicable with a host of popular libraries like MPI and OpenMP that are supported. The same optimizations that apply on the Xeon also apply without change on the Xeon Phi. In general, development time of a parallel application on the Xeon Phi is short.  The Xeon Phi offers three modes of operation - native, offload and symmetric. We use the offload model of execution in our study, where the host offloads a portion of computation to the Xeon Phi either synchronously or asynchronously with simultaneous CPU executions.
