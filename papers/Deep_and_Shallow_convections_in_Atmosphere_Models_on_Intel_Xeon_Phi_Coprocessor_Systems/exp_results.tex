\section{Experiments and Results}
\label{exp_res}

\subsection{Experimental Setup}

In our experiments, we used CESM v 1.2.2 with compset FC5 comprising active atmosphere component with CAM5 physics, active land and sea-ice components, stub land-ice, and data model for ocean. We used three different resolutions in our experiments. Table \ref{resolutions} shows the parameters for the resolutions used in the atmosphere model. In all cases, 30 vertical levels were used.  The performance-related experiments were conducted with 5-day simulation runs.

The experiments were conducted on a cluster containing 8 nodes of 16-core (dual octo-core) Intel Xeon E5-2670 CPU with a speed of 2.6 GHz. Each node is equipped with two Intel Xeon Phi 7120 PX card, each with 61 cores.  In all cases, -O3 compiler optimization was used. For performance-related experiments, we used single-node with 16 threads on the CPU for the default R1 resolution due to the small size involved. For the higher R2 and R3 resolutions, we used all the 128 CPU cores of the cluster with 8 MPI tasks and 16 OpenMP threads per MPI task. We used the Xeon Phi cards depending on the experiment. For experiments involving offloading to the Xeon Phi, 180 threads were used for R1 resolution, and 240 threads for R2 and R3 resolutions. All calculations including those on Xeon-Phi were performed using double precision.

Each result shown was obtained as a mean of five runs. The timings were found to be consistent across the runs with the overall CESM times varying between 1 to 5\%. For CESM times, we report the total time excluding the initialization time.

\begin{table}
 \small
 \centering
 \begin{tabular}{||p{0.65in}|p{0.5in}|p{0.9in}|p{0.4in}||}
  \hline\hline
Pseudonym & Resolution & lat x lon (degrees) & Columns \\
  \hline\hline
R1 (default) & f19\_g16 & 1.9 x 2.5 & 13824 \\ \hline
R2 & f05\_g16 & 0.47 x 0.63 & 221184 \\ \hline
R3 & f02\_g16 & 0.23 x 0.31 & 884736 \\
  \hline\hline
 \end{tabular}
\caption{Details of Resolutions}
\label{resolutions}
\end{table}

\subsection{Results on Correctness}

We first demonstrate the correctness of our code modifications due to various optimizations. We verified the accuracy of the results by finding the error growth of the temperature values produced in the code over the simulations \cite{rosinski-roundingerrors-siamjscicomp1997}. The error growth curves compare the RMS difference between the results of the original code and the results of the modified code due to our optimizations, and the RMS difference between the results from the original code and the results obtained by perturbing the inputs by the least significant bit. We refer to these perturbations to the least significant bit as {\em induced perturbations}. In general, for an optimization or modification to be acceptable, the error growth curve due to the modification should be smaller than the error growth curve due to the induced perturbations. Our error growth curves were obtained for a 2-day simulation run with the default R1 resolution.

We first show the error growths with using the advanced compiler flag of ``$-fp-model fast=2$'', which is expected to give fast but less accurate results, over the default flag of ``$-fp-model source$'', which uses source precision. Figure \ref{error_fast2_xeon} shows the error results on Xeon. We find that the use of the ``$-fp-model fast=2$'' optimization did not alter the accuracy significantly. Hence the advanced compiler flags can be safely used to potentially obtain high performance without compromizing on accuracy.

\begin {figure}
\centering
\includegraphics[scale=0.35]{images/error_fast2_xeon.png}
\caption{Error Growth on Xeon with ``$-fp-model fast=2$'' Compiler Flag}
\label{error_fast2_xeon}
\end{figure}

Figure \ref{error_xeonphi} shows the error growth for our modified optimized code executing on Xeon Phi compared with the original code on Xeon. We find that our offloading to Xeon Phi did not alter the error, and the error growth matches well with the induced perturbations.

\begin {figure}
\centering
\includegraphics[scale=0.35]{images/error_xeonphi.png}
\caption{Error Growth with Execution on Xeon Phi with ``$-fp-model source$'' Compiler Flag}
\label{error_xeonphi}
\end{figure}

\subsection{Performance Results}

For most of the performance-related results, we used the medium-range R2 resolution run using 8 MPI tasks on 8 nodes with 16 threads per MPI task for a total of 128 threads. For the Xeon Phi related experiments, we used one Xeon Phi in each node for a total of 8 Xeon Phi cards.

\subsubsection{Performance Benefits of Individual Optimizations}

We first show the performance benefits due to the individual optimizations.

%-----------------------------------------

Table \ref{firstprivate_reduction} shows the timings of the loop before and after reducing first private for scalars on both Intel and Xeon Phi. The experiments correspond to 10-day CESM run with $f16\_g19$ resolution. Using columns 2-4, we find that the use of firstprivate reduction related optimization reduced both the OpenMP overheads for memcpy by about 4.5\% in both Xeon and Xeon Phi, and also reduced the resulting overall looptimes by about 4.8\% in Xeon and about 6.0\% in Xeon Phi. While the OpenMP loop overhead on Xeon considerably reduced to only 2.3\% on Xeon, it is still a significant 20\% in Xeon Phi. The last column of the table shows that the primary reason for this large overhead in Xeon Phi is OpenMP dynamic scheduling. The use of static scheduling reduced the overhead significantly to 7.7\%.
% Thus, improvements in OpenMP runtime for dynamic scheduling will have to be strongly considered in the future versions of Intel Xeon Phi architecture.
While static scheduling reduced the OpenMP loop overhead, we find that the overall loop runtime with dynamic scheduling is about 40\% less than with static scheduling.

\begin{table*}
 \small
 \centering
 \begin{tabular}{||p{1.5in}|p{0.75in}|p{0.75in}|p{0.75in}|p{0.75in}|p{1.5in}||}
\hline\hline
Runtime Component & \multicolumn{2}{|p{1.5in}|}{Intel Xeon with 16 threads, dynamic scheduling} & \multicolumn{2}{|p{1.5in}|}{Intel Xeon Phi with 180 threads, dynamic scheduling} & Intel Xeon Phi with 180 threads, static scheduling \\ \hline
  & Original & After firstprivate reduction & Original & After firstprivate reduction & After firstprivate reduction \\ \hline\hline
Loop runtime (msecs) & 16.26 & 15.47 & 49.16 & 46.20 & 66.88 \\ \hline
Avg. thread runtime (msecs) & 15.15 & 15.11 & 36.9 & 36.7 & 61.70 \\ \hline
\% Overhead & 6.8\% & 2.3\% & 25.0\% & 20.5\% & 7.7\% \\ \hline\hline
  \end{tabular}
\caption{Firstprivate Reduction and Dynamic Scheduling in Shallow Convection}
\label{firstprivate_reduction}
\end{table*}

Our optimization due to elimination of false sharing resulted in 7\% improvement in single-thread performance, 18\% improvement in performance with 16-threads on Xeon, and 30-40\% improvement with 180 threads in Xeon Phi. The code rearrangement for elimination of false sharing also resulted in increase in scalability of the loops from about 37\% to 64-79\% with 16 threads on Xeon.

The combination of strength reduction and data reuse resulted in improvements of 21.5\% on Xeon and 3-10\% on Xeon Phi for single and multi-thread executions.

Tables \ref{all_fpmodel_xeon}-\ref{all_fast2_xeonphi} show and summarize the benefits of all our optimizations for a model run corresponding to $f19\_g16$ resolution, single-node run with 16 OpenMP threads for 16-core Xeon CPU and 240 threads for Xeon Phi, with 13824 columns across the entire grid divided into 16 chunks of 864 columns each, and using -O3 compiler optimization. The results for a row correspond to the cumulative optimizations from the previous rows and the optimization for the current row. The non-convections parts were threaded on the Xeon CPU at the chunk level, and the convection routines threaded at the fine-grained loop level either on the Xeon CPU or on Xeon Phi. The CESM model was executed with a 5-day run for collecting timings and a 2-day run for perturbation. The first two tables show results with compiler flag ``$-fp-model source$'' and the last table shows results with compiler flag ``fast=2''.

\begin{table*}
 \small
 \centering
 \begin{tabular}{||p{1.5in}|p{0.75in}|p{0.75in}|p{0.75in}|p{0.75in}|p{0.75in}||}
\hline\hline
Optimization &	Shallow Convection loop runtime	& Deep Convection loop runtime	& Convection runtime & Atmosphere runtime & CESM runtime \\ \hline\hline
Baseline & 120.6 & 19.1	& 284.3	& 830.1	& 1030.8 \\ \hline
Firstprivate overhead optimization in shallow convection & 78.3	& 19.1 & 238.4 & 786.9 & 984.4 \\ \hline
Dynamic scheduling for load balance & 66.5 & 17.1 & 226.2 & 776.4 & 972.1 \\ \hline
Eliminating false sharing in shallow convection loop & 54.3 & 17.1 & 223.1 & 773.0 & 970.0 \\ \hline
Strength Reduction & 42.6 & 17.1 & 211.6 & 762.8 & 960.0  \\\hline\hline
  \end{tabular}
\caption{All optimizations - ``-fp-model source'' on Xeon. All times are in seconds}
\label{all_fpmodel_xeon}
\end{table*}


\begin{table*}
 \small
 \centering
 \begin{tabular}{||p{1.5in}|p{0.75in}|p{0.75in}|p{0.75in}|p{0.75in}|p{0.75in}||}
\hline\hline
Optimization &	Shallow Convection loop runtime	& Deep Convection loop runtime	& Convection runtime & Atmosphere runtime & CESM runtime \\ \hline\hline
Baseline & 631.2 & 62.1 & 1410.8 & 1936.3 & 2136.9 \\ \hline
Firstprivate overhead optimization in shallow convection & 469.4 & 62.1	& 1256.6 & 1782.8 & 1984.8 \\ \hline
Dynamic scheduling for load balance & 409.8 & 49.0 & 1190.8 & 1718.9 & 1915.7 \\ \hline
Eliminating false sharing in shallow convection loop & 288.3 & 49.0 & 1073.5 & 1615.4 & 1811.1 \\ \hline
Strength Reduction & 258.1 & 49.0 & 1053.4 & 1590.7 & 1787.1  \\ \hline\hline
  \end{tabular}
\caption{All optimizations - ``-fp-model source'' on Xeon Phi. All times are in seconds}
\label{all_fpmodel_xeonphi}
\end{table*}


\begin{table*}
 \small
 \centering
 \begin{tabular}{||p{1.5in}|p{0.75in}|p{0.75in}|p{0.75in}|p{0.75in}|p{0.75in}||}
\hline\hline
Optimization &	Shallow Convection loop runtime	& Deep Convection loop runtime	& Convection runtime & Atmosphere runtime & CESM runtime \\ \hline\hline
Baseline & 520.0 &43.7 & 1291.5 & 1774.9 & 1973.4 \\ \hline
Firstprivate overhead optimization in shallow convection & 363.8 & 43.1 & 1195.2 & 1828.6 & 2094.1 \\ \hline
Dynamic scheduling for load balance & 325.7 & 35.5 & 1095.0 & 1581.7 & 1775.1 \\ \hline
Eliminating false sharing in shallow convection loop & 198.6 & 35.5 & 961.5 & 1453.3 & 1649.4 \\ \hline
Strength Reduction & 192.3 & 35.5 & 955.3 & 1446.0 & 1640.0 \\ \hline\hline
  \end{tabular}
\caption{All optimizations - ``-fp-model fast=2 -fimf-precision=high'' on Xeon Phi. All times are in seconds}
\label{all_fast2_xeonphi}
\end{table*}

Comparing the first and last lines in Tables \ref{all_fpmodel_xeon} and \ref{all_fpmodel_xeonphi}, we find that on Xeon CPU with ``$-fp-model source$'' our optimizations result in performance improvements of about 65\% for shallow convection loop, 10\% for deep convection loop, 25\% for the entire convection, 8\% for the entire atmosphere model, and 7\% improvement in the overall CESM model. On Xeon Phi with ``$-fp-model source$'' our optimizations result in performance improvements of about 59\% for shallow convection loop, 21\% for deep convection loop, 25\% for the entire convection, 18\% for the entire atmosphere model, and 16\% improvement in the overall CESM model. Thus, we find that our optimization provide higher returns for the atmosphere model and the entire CESM run on the Xeon Phi co-processor. Comparing the last lines of the Tables \ref{all_fpmodel_xeonphi} and \ref{all_fast2_xeonphi}, we find that the use of the fast compiler flags provided further performance improvements of about 25\% for shallow convection, 28\% for deep convection, 9\% for total convection, 9\% for the entire atmosphere model, and 8\% for the overall CESM run.

%---------------------------------------


We first show the performance benefits due to the individual optimizations.
For the results in Figures \ref{opt_deepshallow_perf_xeonphi} and \ref{opt_conatmcesm_perf_xeonphi}, the optimizations are cumulatively applied in the order shown.

 Figure \ref{opt_deepshallow_perf_xeon} shows the improvements in shallow and deep convections times over the baseline model on Xeon for the different optimizations for the R2 resolution. Note that only the dynamic scheduling optimization is applicable to deep convection, and it results in 20\% performance improvement for this convection routine. For shallow convection, the performance improvements are 34\% with firstprivate reduction, an additional 5\% with dynamic scheduling, an additional 25\% with false sharing elimination, and an additional 5\% with strength reduction. Thus we find firstprivate reduction and false sharing elimination as the most important optimizations in our work. Note that these optimizations relate to data management among multiple threads.

\begin {figure}
\centering
\includegraphics[scale=0.35]{images/opt_deepshallow_perf_xeon.pdf}
\caption{Effect of Optimizations on Deep and Shallow Convection Execution Times on Xeon for ``fast=2'' flag for R2 resolution}
\label{opt_deepshallow_perf_xeon}
\end{figure}
%
Figure \ref{opt_deepshallow_perf_xeonphi} shows the improvements in shallow and deep convections times over the baseline model on Xeon Phi for the different optimizations for the R2 resolution. Note that only the dynamic scheduling optimization is applicable to deep convection, and it results in 17\% performance improvement for this convection routine.
 Dynamic scheduling results in 17\% improvement in the deep convection.
For shallow convection, the performance improvements are 30\% with firstprivate reduction, an additional 24\% with dynamic scheduling, an additional 16\% with false sharing elimination, and an additional 2\% with strength reduction.
Interestingly, we find that unlike in Xeon,
dynamic scheduling plays a major role in addition to the firstprivate reduction and false sharing elimination in Xeon Phi. Thus, load balancing using dynamic scheduling is important in Xeon Phi due to the use of a large number of threads. Note that the firstprivate reduction and false sharing elimination relate to data management among multiple threads.

\begin {figure}
\centering
\includegraphics[scale=0.35]{images/opt_deepshallow_perf_xeonphi.pdf}
\caption{Effect of Optimizations on Deep and Shallow Convection Execution Times on Xeon Phi for ``fast=2'' flag for R2 resolution}
\label{opt_deepshallow_perf_xeonphi}
\end{figure}

We next show the effects of the individual optimizations on the overall convection and atmosphere execution times.  Figures \ref{opt_conatmcesm_perf_xeon} and \ref{opt_conatmcesm_perf_xeonphi} show the improvements in the total convection and atmosphere times over the baseline model on Xeon and Xeon Phi, respectively, for the different optimizations for the R2 resolution. We notice that the individual optimizations in the deep and shallow convection show significant and visible individual improvements even in the higher-level computations in the call trace, namely, overall convections and complete atmosphere model executions.  For example in Xeon Phi, the firstprivate reduction and dynamic scheduling optimizations result in performance improvements of 7\% each in the atmosphere model timings.

\begin {figure}
\centering
\includegraphics[scale=0.35]{images/opt_conatm_perf_xeon.pdf}
\caption{Effect of Optimizations on Convection, Atmosphere Execution Times on Xeon for ``fast=2'' flag for R2 resolution}
\label{opt_conatmcesm_perf_xeon}
\end{figure}

\begin {figure}
\centering
\includegraphics[scale=0.35]{images/opt_conatm_perf_xeonphi.pdf}
\caption{Effect of Optimizations on Convection, Atmosphere Execution Times on Xeon Phi for ``fast=2'' flag for R2 resolution}
\label{opt_conatmcesm_perf_xeonphi}
\end{figure}

Our proportional partitioning method takes into account the variable performance ratios of Xeon and Xeon Phi to offload the column computations of shallow convections to Xeon Phi. Table \ref{pp_results} compares the time taken using proportional partitioning with the times taken for Xeon-only and Xeon-Phi only computations for the R2 and R3 resolutions. The results correspond to 2-day simulation runs. The last two columns of the table also show the times spent on Xeon and Xeon Phi in the proportional partitioning approach. The difference between the total time (4th column) and the maximum of the last two columns gives the overheads including data transfers.

\begin{table}
\centering
\begin{tabular}{|p{0.4in}|p{0.43in}|p{0.65in}|p{0.43in}|p{0.45in}|p{0.6in}|}
\hline\hline
{\bf Resolution} & {\bf Xeon-only} & {\bf Xeon-Phi Only} & \multicolumn{3}{|c|}{{\bf Proportional Partitioning}} \\
\hline
 & & & Total Time & Xeon Time & Xeon Phi Time \\
\hline\hline
R2 & 38.0 & 83.6 & 88.0 & 46.4 & 27.2 \\
\hline
R3 & 292.5 & 744.3 & 421.6 & 262.7 & 164.0 \\
\hline\hline
\end{tabular}
\caption{Results of Proportional Partitioning. All times are in seconds}
\label{pp_results}
\end{table}

First, we find that the Xeon Phi shows 2X slowdown when compared to the Xeon CPU. This is primarily due to the poor single thread performance on Xeon Phi, and the lack of vectorization opportunities in the critical loops of the shallow and deep convection routines, in which convergence is tested for termination. We expect the performance to improve in the next generation Intel Xeon Phi Knoghts Landing processors that have superior single thread performance.

As for the proportional partitioning approach, it gives equivalent (for R2) or 1.77x performance improvement over the Xeon-Phi only computations for both the resolutions. This is due to some of the work that is shared by the Xeon CPU. When compared to the Xeon-only approach, the proportional partitioning shows slowdowns primarily due to the data transfer overheads for the fine grained offloads on the PCIe link between the host and the coprocessor. Our study involves fine-grained parallelism in Xeon Phi in which the computations are offloaded at the column level and not at the chunk level. For every chunk assigned to a node, offloading is performed once. In all our experiments, 16 chunks are assigned to a Xeon node to optimize the computations of the other physics routines. This results in 16 offloads per time step and the corresponding large data transfer overheads. In future, we plan to explore chunk-level offloading in which only one offloading will be performed for all the chunks assigned to a node. Interestingly, we find that this slowdown in proportional partitioning when compared to the Xeon-only result decreases as we increase the resolution or problem size: 132\% for R2 and 44\% for R3. Thus, the proportional partitioning approach has very good promise as the climate modeling community plans to explore large and very large resolutions in the future.

\begin {figure}
\centering
\includegraphics[scale=0.35]{images/proportional_part_perf.pdf}
\caption{Benefits of Proportional Partitioning with ``$-fp-model source$'' and ``fast=2'' flags and for R1 and R2 resolutions}
\label{proportional_part_perf}
\end{figure}

\subsubsection{Overall Optimization Benefits}

Figure \ref{flags_xeon_perf} shows the performance improvement in the complete atmosphere calculations of the modified code containing our optimizations over the baseline code on Xeon CPU for the two compiler flags namely, ``$-fp-model source$'' and ``$-fp-model fast=2$'' flags. The modified code contains all optimizations except proportional partitioning which involves Xeon Phi. We find that the performance improvement is about 5\% with both the compiler flags.

\begin {figure}
\centering
\includegraphics[scale=0.35]{images/flags_xeon_perf.pdf}
\caption{Execution Times for Atmosphere on Xeon for different compiler flags for R2 resolution}
\label{flags_xeon_perf}
\end{figure}

We show the effect of all the optimizations put together for the various modules. We first show the impact of compiler flags on our optimizations.
Figure \ref{flags_xeonphi_perf} shows the performance improvements in the different components due to our optimizations over the baseline code on Xeon Phi for the ``$-fp-model source$'' and ``$-fp-model fast=2$'' flags. We find that the performance improvements with the ``source'' flag are 19\%, 2.5x, 30\%, 15\%, 16\% for the deep convection, shallow convection, total convection, total atmosphere and the entire CESM, respectively. With the ``fast=2'' flag, the performance improvements are  17\%, 4x, 37\%, 17\%, 16\% for the deep convection, shallow convection, total convection, total atmosphere and the entire CESM, respectively. We find that the use of ``fast=2'' flag results in higher performance benefits due to our optimizations than the use of ``$-fp-model source$'' flag in the shallow convection calculations (2.5x vs 4x). This is due to our larger number of vectorization-related optimizations in the shallow convection routines, whose benefits are higher with the fast flags. The use of ``fast=2'' flags generates hardware instructions for transcendental math functions on Xeon Phi which improves performance.
Comparing this with the previous figure, we also find that
 the performance benefits with our optimizations are much higher in Xeon Phi than in Xeon. This is due to the wider vectorization units in the Xeon that can harness our vectorization related optimizations more. Significantly, we find that our work on optimizations in the convection routine gives about 15\% improvement even in the overall CESM executions.

\begin {figure}
\centering
\includegraphics[scale=0.35]{images/flags_xeonphi_perf.pdf}
\caption{Execution Times on Xeon Phi for different compiler flags for R2 resolution}
\label{flags_xeonphi_perf}
\end{figure}

We also evaluate the benefits of all our optimizations for different resolutions. Figure \ref{res_perf} compares the execution times of the entire convection and atmosphere calculations with the baseline model on Xeon Phi for the three resolutions, R1, R2 and R3. We find that as the resolution is increased, the performance gains due to our optimizations increase for convection calculations. Specifically, the performance gains in the convection calculations are 26\%, 35\% and 42\% for R1, R2, and R3 resolutions, respectively. Thus, overall, we find that optimizations will play bigger roles in the future when the climate models will explore larger resolutions and many-core systems will be built with wider vectorization units with more advanced compiler flags.

\begin {figure}
\centering
\includegraphics[scale=0.35]{images/res_perf.pdf}
\caption{Execution Times for Different Resolutions on Xeon Phi for ``fast=2'' flag. R1 was executed on a single node, 16 cores. R2 and R3 were executed on 8 nodes, 128 cores.}
\label{res_perf}
\end{figure}

\subsection{Savings for Multi-Century Runs}

Multi-century simulation runs are typically of interest in climate models to study the long-term effects on the climate due to various factors including CO$_{2}$ levels. We extrapolated the performance gains obtained due to our optimized convection computations for a multi-century simulation run using our runs for limited number of simulation days. Specifically, we obtained the performance gains in seconds on both Xeon and Xeon Phi over their baseline executions for a 5-day simulation run, and extrapolated the gains in terms of days for a 1000-year simulation run. Table \ref{multi_century_savings} shows savings in terms of the number of days for execution for different resolutions.

\begin{table}
\small
\centering
\begin{tabular}{|p{0.8in}|p{1.0in}|p{1.0in}|p{1.0in}|}
\hline
Resolution & Savings on Xeon (days) & Savings on Xeon Phi (days) \\
\hline
R1 & 61.4 & 284.0 \\
R2 & 181.0 & 555.10 \\
R3 &  1349.70 & 5467.40 \\
\hline
\end{tabular}
\caption{Savings in Execution Days for Multi-Century Runs with``$-fp model fast=2 -fimf-precision=high$'' Compiler Flag}
\label{multi_century_savings}
\end{table}

We find that the use of our optimizations results in highly significant savings in execution days.
As shown above, the performance and hence the savings increase with increase in resolutions. We also find that the savings on Xeon Phi are about 4X the savings on Xeon. This is due to our optimizations harnessing the wider vector units of the Xeon Phi.
Considering one of the results, for example on Xeon, our optimized executions results in savings of up to 181 days or half a year in execution for the R2 resolution. These are highly significant savings and implies not only improved performance, but also savings in power consumption, electricity and maintenance costs.
