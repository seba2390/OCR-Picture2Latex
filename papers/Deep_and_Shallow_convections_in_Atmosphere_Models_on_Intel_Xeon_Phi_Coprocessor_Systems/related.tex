\section{Related Work}
\label{related}

There have been a number of efforts in using GPUs for climate and weather models.  Michalakes and Vachharajani \cite{michalakes-gpuweather-ipdps2008} used GPUs to improve the performance of the Weather Research and Forecast (WRF) model.  Their work resulted in  in 5-20x speed-up for the computationally intensive routine WSM5. 
In the work by Govett et al. \cite{govett-nim-ccgrid2010}, the non-hydrostatic icosahedral (NIM) model was ported to the GPU \cite{govett-nim-ccgrid2010}.  The dynamics portion which is the most expensive part of the NIM model was accelerated using GPU and the speed-up achieved was about 34 times on Tesla - GTX-280 when compared to a CPU.
The Oak Ridge National Laboratory (ORNL) ported the spectral element dynamical core of CESM, HOMME, to the GPU \cite{carpenter-homme-ijhpca2013}.
A very high resolution of (1/8) th degree was used as a target problem.
Using asynchronous data transfer, the most expensive routine performed three times faster on the GPU than the CPU. This execution model was shown to be highly scalable.  The climate model ASUCA \cite{shimokawabe-asuca-sc2010} is a production weather code developed by the Japan Meteorological Agency. By porting their model fully onto the GPU they were able to achieve 15 TFlops in single precision using 528 GPUs. The TSUBAME 1.2 supercomputer in Tokyo Institute of Technology was used to run the model. The CPU is used only for initializing the models and all the computations are done on the GPU. There are different kernels for the different computational components.

In the work by Schalkwijk et al. \cite{schalkwijk-eddygpu-ams2015}, the authors have utilized the GPU's for Large Eddy Simulation (LES) models, which has allowed them to provide turbulence-resolving numerical weather forecasts over a region the size of the Netherlands, at 100m resolution. Garcia et al. \cite{garcia-cloud-iccs2012} accelerate a Cloud Resolving Model (CRM) by implementing the MPDATA algorithm on GPU using CUDA.  They  perform optimizations like data reuse on GPU for saving transfer time, coalesced memory accesses on GPU's, and utilizing the texture memory and shared memory on the GPU. Fuhrer et al. \cite{fuhrer-portableweatherclimate-sci2014} optimize the atmospheric model COSMO by rewriting the dynamical core using STELLA DSEL and porting the remaining parts of the Fortran code to the GPU's using OpenACC compiler directives.

Intel Xeon Phi is one of the important accelerator architectures and have been becoming prevalent in supercomputer sites that adopt heterogeneous systems. Hence, porting climate models to Intel Xeon Phi accelerators is essential.
Intel Xeon Phi processors have been used to provide high performance for different scientific domains \cite{liu-swaphils-cluster2014,heybrock-latticeqcd-sc2014,luo-mica-bmc2015}.
There have been recent efforts in porting weather and climate models on Intel Xeon Phi accelerators. Mielikainen et al. have a number of efforts on optimizing Weather Research Forecast Model (WRF) on Intel Xeon Phi architecture. In \cite{mielikainen-optwrf-spie2014, mielikainen-revisitingCloud-spie2015}, the authors have  optimized  the  Thomspson  cloud  microphysics scheme,  a sophisticated cloud microphysics scheme.  They have used optimization techniques such as modifying the tile size processed by each core, using SIMD, data alignment, memory footprint reduction, etc.  to achieve a speedup of 1.8x over the original code on Intel Xeon Phi 7120P.
 and 1.8x over the original code on dual socket configuration of eight core Intel Xeon E5-2670. In  another work \cite{mielikainen-longwave-spie2015}, the authors optimize the longwave radiative transfer scheme of the Goddard microphysics scheme of the WRF model, for Intel MIC architecture.  Their optimization yields a speedup of 2.2x over the original code on Xeon Phi 7120P. They also optimize the updated Goddard shortwave radiation of the WRF model for Intel Xeon Phi \cite{mielikainen-shortwave-spie2015}.  They observe a speedup of 1.3x over the original code on Xeon Phi 7120P. 

Betro et al. \cite{betro-performancemetrics-cug2013} highlight experiences and knowledge gained from porting such codes as ENZO,  H3D, GYRO, a  BGK Boltzmann solver,  HOMME-CAM, PSC, AWP-ODC, TRANSIMS, and ASCAPE to the Intel Xeon Phi architecture running on a Cray CS300- AC Cluster Supercomputer named Beacon.
Most of these were ported by compiling with the flag -mmic.  They conclude that accelerator based systems are the wave of the future based both on their power consumption and variety of programming paradigms to  fit the needs of all application developers. 

Michalakes et. al \cite{michalakes-optimizingweathermodel-hmcw2014} optimize a standalone kernel implementation of Rapid Radiative Transfer Model of the NOAA Nonhydrostatic Multiscale Model (NMM- B). They apply methods such as dynamic load balancing, lowering inner loops, avoiding vector remainders, trading computation for data movement, prefetching etc.  and obtain a speedup of 1.3x over the original code on Xeon Sandybridge and 3x over the original code on Intel Xeon Phi.

To our knowledge, ours is the first effort on accelerating convection calculations on Intel Xeon Phi clusters. While existing efforts on accelerators and co-processors focused on data management and vectorization, in addition to these optimizations, our work proposes novel asynchronous execution model for simultaneous executions on both CPU and coprocessor cores.
