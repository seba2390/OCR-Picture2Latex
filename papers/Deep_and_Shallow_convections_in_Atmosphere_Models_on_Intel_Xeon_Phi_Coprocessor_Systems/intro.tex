\section{Introduction}
\label{intro}

The importance of climate to energy usage and agriculture has made it a prominent field of study. Climate study is based on the mathematical models of physical processes such as radiation, circulation, precipitation and their interaction with chemical and biological processes. The equations of conservation of mass, momentum, energy and species are used to represent various components of the climate model such as atmosphere, land, sea and ice. Numerical methods are extensively used to solve these equations and as a result, climate models have many computationally intensive routines.

One  such  climate  model is the Community Earth System Model \cite{hurrell-cesm-bams2013}, developed and maintained by the National Center for Atmospheric Research (NCAR). CESM consists of several component models, eg. physical climate, chemistry, land ice, whole atmosphere, etc, that can be coupled in different configurations. In all cases, geophysical fluxes across the components are exchanged via a central coupler module. A large number of simulations with CESM have been conducted, some of which are available for community analysis \cite{hurrell-cesm-bams2013}.

The atmosphere is the most time-consuming model in CESM, as shown in Figure \ref{cesm_profile}.  We obtained such execution profiles using Intel VTune Amplifier and HPCToolkit profiler\cite{hpctoolkit}. The execution profile is obtained by running CESM v 1.2.2 with $f02\_g16$, the highest resolution\footnote{see Table \ref{resolutions} for resolutions.} and B compset (fully-coupled run) on eight-node 128-core Intel Xeon processors with 8 MPI processes and 16 OpenMP threads per process for a total of 128 threads. The model used for atmosphere is the Community Atmosphere Model (CAM5)\cite{collins-cam-jc2006,neale-meanclimate-jc2013}. CAM consists of two computational phases, namely, dynamics and physics. The dynamics advances the evolutionary equations for the flow of atmosphere and the physics approximates sub-grid phenomena including clouds, long and short wave radiations, precipitation processes and turbulent mixing. The default core for the dynamics is a finite volume method \cite{neale-meanclimate-jc2013} that uses a longitude $\times$  latitude $\times$ vertical level computational grid over the sphere. The physics in CAM is based upon vertical columns whose computations are independent from each other. The parallel implementation of the physics is based on the assignment of columns to MPI processes and using OpenMP threads within a process to compute the columns assigned to a process.

\begin {figure}
\centering
\includegraphics[scale=0.2]{images/cesm_profile.png}
\caption{Execution Profile of CESM}
\label{cesm_profile}
\end{figure}
\begin {figure}
\centering
\includegraphics[scale=0.2]{images/phy_profile.pdf}
\caption{Execution Profile of the CAM5 Physics model}
\label{phy_profile}
\end{figure}

Most of the work to accelerate climate science routines have concentrated on the dynamics. In this work, we chose to accelerate the physics routines as they were observed to consume significant times, typically about 40\% of the entire CESM run. Figure~\ref{phy_profile} illustrates the execution profile of CAM5 physics. This figure shows that the the deep and shallow convection routines (dark blue and orange) are the most time consuming routines, taking about 41\% of the total time spent in the CAM5 physics. Deep convections consider parameterization of clouds on the basis of moist static instability over the entire atmosphere column. Shallow convections consider clouds when the moist static instability is present only over parts of the atmosphere column.  These calculations also present significant load imbalances due to varying cloud covers over different regions of the grid. Convection is at the crux of modeling clouds which play an important role in atmospheric circulation. More complex methods such as super parameterization, while being more accurate could be about two orders of magnitude more time-consuming than present day approaches. Thus, acceleration of convection calculations can result in use of more sophisticated methods.

Accelerators and co-processors are widely prevalent and have been used to provide high performance for many scientific applications. Intel{\textregistered} Xeon Phi{\texttrademark} coprocessors have been gaining ground to provide speedups for advanced scientific applications \cite{liu-swaphils-cluster2014,heybrock-latticeqcd-sc2014,luo-mica-bmc2015}. However, the use and demonstration of these coprocessors for climate modeling are limited. In this work, we accelerate the deep and shallow convection calculations on Intel{\textregistered} Xeon Phi{\texttrademark} Coprocessor Systems. By employing dynamic scheduling in OpenMP, we demonstrate large reductions in load imbalance and about 10\% increase in speedups. By careful categorization of data as private, firstprivate and shared, we minimize data copying overheads for the coprocessors. We identify regions of false sharing among threads and eliminate them by loop rearrangements. We also employ proportional partitioning of independent column computations across both the CPU and coprocessor cores based on the performance ratio of the computations on the heterogeneous resources. These techniques along with various vectorization strategies resulted in about 30\% improvement in convection calculations.

In Section \ref{back}, we give the overall structure of the atmosphere model and the convection calculations.  Section \ref{related} covers related work in the area of high performance of climate and weather models on accelerators and coprocessors. In Section \ref{opt}, we describe our different optimization techniques including avoiding false sharing, and proportional partitioning of the computations among the CPU and Xeon Phi cores. Section \ref{exp_res} presents experiments and results. In Section \ref{discussion}, we derive general principles from our optimizations on the climate modeling application. Section \ref{con_fut} gives conclusions and presents scope for future work.
