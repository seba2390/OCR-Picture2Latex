\documentclass{article} 
  

% if you need to pass options to natbib, use, e.g.:
%     \PassOptionsToPackage{numbers, compress}{natbib}
% before loading neurips_2023
 

% ready for submission 
%\usepackage{neurips_2023}


% to compile a preprint version, e.g., for submission to arXiv, add add the
% [preprint] option:
%     \usepackage[preprint]{neurips_2023}
 

% to compile a camera-ready version, add the [final] option, e.g.: 
     \usepackage[final, nonatbib]{neurips_2023}


% to avoid loading the natbib package, add option nonatbib:
% \usepackage[nonatbib]{neurips_2023}



\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{wrapfig}
\usepackage{xcolor}         % colors
\usepackage{graphicx} 
\usepackage{caption} 
\usepackage{subcaption}  
%\usepackage{hyperref}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}
\usepackage[textsize=tiny]{todonotes} 
% if you use cleveref..
\usepackage[capitalize,noabbrev]{cleveref}

\usepackage{algorithm}
\usepackage{algorithmic}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark} 
\usepackage{enumitem}
\setlist{leftmargin=7mm}

\title{StableFDG: Style and Attention Based  Learning for Federated Domain Generalization}


% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to LaTeX to determine where to break the
% lines. Using \AND forces a line break at that point. So, if LaTeX puts 3 of 4
% authors names on the first line, and the last on the second line, try using
% \AND instead of \And before the third author name.


\author{%
  Jungwuk Park\thanks{Authors contributed equally to this work.} \\
  KAIST\\
  \texttt{savertm@kaist.ac.kr} \\ 
  \And
  Dong-Jun Han$^{*}$ \\
  Purdue University\\
  \texttt{han762@prudue.edu} \\
  \And
  Jinho Kim \\
  SK Hynix\\
  \texttt{jinho123.kim@sk.com} \\
  \And
Shiqiang Wang \\
  IBM Research\\
  \texttt{wangshiq@us.ibm.com} \\
  \And
  Christopher G. Brinton \\
  Purdue University\\
  \texttt{cgb@purdue.edu} \\
  \And
  Jaekyun Moon \\
  KAIST \\
  \texttt{ jmoon@kaist.edu \ } \\
  % examples of more authors
  % \And
  % Coauthor \\
  % Affiliation \\
  % Address \\
  % \texttt{email} \\
  % \AND
  % Coauthor \\
  % Affiliation \\
  % Address \\
  % \texttt{email} \\
  % \And
  % Coauthor \\
  % Affiliation \\
  % Address \\
  % \texttt{email} \\
  % \And
  % Coauthor \\
  % Affiliation \\
  % Address \\
  % \texttt{email} \\
}


\begin{document}


\maketitle


\begin{abstract}
Traditional federated learning (FL) algorithms operate under the assumption that the data distributions at training (source domains) and testing (target domain) are the same. The fact that domain shifts often occur in practice necessitates equipping FL methods with a domain generalization (DG) capability. However, existing DG algorithms face fundamental challenges in FL setups due to the lack of samples/domains in each clientâ€™s  local dataset. In this paper, we propose StableFDG, a \textit{style and attention based learning strategy} for accomplishing federated domain generalization, introducing two key contributions. The first is style-based learning, which enables each client to explore novel styles beyond the original source domains in its local dataset, improving domain diversity based on the proposed style sharing, shifting, and exploration strategies. Our second contribution is an attention-based feature highlighter, which captures the similarities between the features of data samples in the same class, and emphasizes the important/common characteristics to better learn the domain-invariant characteristics of each class in data-poor FL scenarios. Experimental results show that StableFDG outperforms existing baselines on various DG benchmark datasets, demonstrating its efficacy.






%Traditional federated learning (FL) algorithms operate under the assumption that the data distribution at training (source domains) and testing (target domain) are the same. To overcome domain shifts, FL methods have to be equipped with a  domain generalization (DG) capability,  but existing DG algorithms face fundamental challenges in FL setups due to the lack of samples/domains in each client's   local dataset.  In this paper, we propose  StableFDG, a   \textit{style and attention based learning strategy} for accomplishing federated domain generalization based on two key ideas. Our first one is style-based learning, which  enables each client to explore novel styles beyond the original source domains in its local dataset to improve domain diversity,   based on the proposed  style sharing, shifting, and exploration strategies.  Another key component of our algorithm is attention-based weighted averaging, which captures the similarities between the features of data samples in the same class, and emphasizes the important/common characteristics to better learn the domain-invariant characteristics of each class in data-poor FL scenarios. Experimental results show that StableFDG outperforms existing baselines on various DG benchmark datasets, demonstrating its efficacy. 

 %injects the style information of each client into other clients to train local models on diverse domains without  direct data exchange. Moreover, based on style exploration module based on feature-level oversampling, it enables each client to explore novel styles beyond the source domains of each client. 
%Our attention-based learning strategy enables the clients to capture the essential characteristics of each class   in data-poor FL scenarios. Experimental results show that StableFDG outperforms existing baselines on DG benchmark datasets, demonstrating its efficacy.  
%Specifically, we first propose a style sharing/shifting strategy which injects the style information of each client into other clients to train local models on diverse domains without  direct data exchange.  Secondly, we propose a style exploration module based on feature-level oversampling  to   explore novel styles beyond the source domains of each client. Finally, attention-based weighted averaging is proposed to capture the essential characteristics of each class   in data-poor FL scenarios. Experimental results show that StableFDG outperforms existing baselines on DG benchmark datasets, demonstrating its efficacy. 

\end{abstract}

%\vspace{-2.1mm}

\section{Introduction}
%\vspace{-1.5mm}

Federated learning (FL) has now become a key paradigm for training a machine learning model using local data of distributed clients \cite{mcmahan2017communication, li2020federated_survey, kairouz2021advances}. Without directly sharing each client's data to the third party, FL enables the clients to construct  a global  model  via collaboration. However, although FL has achieved remarkable success, the underlying assumption of previous works is that the data distributions during training and testing   are the same. This assumption is not valid in various  scenarios with domain shifts; for example, although the FL clients only have data samples that belong to the source domains (e.g., images on sunny days and rainy days), the trained  global model should  be also able to make reliable predictions for the unseen target domain (e.g., images on snowy days). Therefore,  in practice, FL methods have to be equipped with a domain generalization (DG) capability.    
%Targeting the distributions shift between training and testing, various domain generalization (DG) schemes have been studied in a centralized setup [xx], [xx].

Given the source domains in the training phase, DG aims to construct a model that has a generalization capability to predict well on an unseen target domain.
Various DG methods have been proposed in a centralized setup \cite{zhou2021domain,li2022uncertainty,zhang2022exact,zhou2020learning,li2019episodic,li2018learning,li2018deep,li2018domaincc}.  
However, directly applying centralized DG schemes to FL can potentially restrict the model performance since each client has  limited numbers of data samples and styles in its local dataset. The local models are unable to capture the domain-invariant characteristics due to lack of data/styles in individual clients.%due to the limited number of  styles in local data, each client model  achieves  restricted generalization capability. Moreover, the local models are unable to capture the domain-invariant characteristics due to lack of data in individual clients.   
  
 \vspace{-0.2mm}

Although several researchers have recently focused on DG  for  FL  \cite{liu2021feddg, chen2023federated, wu2021collaborative, nguyen2022fedsr},  they still do not directly handle the fundamental issues that arise from the lack of data and styles (which  represent domains) in individual FL clients. These performance limitations become especially prevalent when federating complex DG datasets having large style shifts between domains or having backgrounds unrelated to the prediction of class, as we will see in Sec. \ref{sec:experiments}. Despite the practical significance of federated DG, this field is still in an early stage of research and remains a great challenge.
%the performance of these schemes are limited in  FL scenarios, 
%\textcolor{red}{Although several researchers have recently focused on DG  for  FL  \cite{liu2021feddg, chen2023federated, wu2021collaborative, nguyen2022fedsr},  it is still unclear how to handle the above issues caused by lack of data samples and styles in FL clients (see Sec. \ref{sec:related}). Despite the practical significance of federated DG, this field is still in the early stage of research and potentially has a considerable room for improvement.} %\textcolor{red}{OK!!} %there are a few papers on federated dg, this field is unexplored.   
  
 
 
 \textbf{Contributions.} In this paper, we propose StableFDG, a \textit{style and attention based learning strategy} tailored to federated domain generalization.    StableFDG  tackles the fundamental challenges in federated DG that arise due to the lack of data/styles in each FL client, with two novel characteristics:%(i) it  enables the  model  of each FL client to get exposed to novel styles beyond the source domains within its local dataset, and (ii) it  enables each client model to  capture common characteristics of data samples in the same class (regardless of the domain) via attention, even in data-poor FL scenarios.   Specifically, our main technical contributions are two-folds:
 \begin{itemize}
\vspace{-1mm}
\item  We first propose \textit{style-based learning},  which exposes the  model  of each FL client to various styles beyond the source domains  in its local dataset. %, to handle the issue on the lack of styles.
 Specifically, we (i) design a style-sharing method that can compensate for the missing styles in each client by sharing the style statistics with other clients; (ii) propose a style-shifting strategy that can select the best styles to be shifted to the new style to balance between the original and new styles; and (iii) develop style-exploration to further expose the model to a wider variety of styles by extrapolating the current styles. Based on these unique characteristics, our style-based learning handles the issue of the lack of styles in each FL client, significantly improving   generalization capability. %Based on these unique characteristics, our   style-based learning significantly improves the generalization capability to the unseen domain, even when only ..... 


%, which strategically performs style sharing, shifting and exploration in the style space based on the feature-level style statistics of data samples in each client.  By compensating the missing styles in each client and exploring wider style spaces via extrapolation, our style-based learning strategy significantly improves the generalization capability to the unseen domain.%the model robustness against domain shift at testing. 
\item   We also propose an \textit{attention-based feature highlighter}, which  enables  the  model to focus  only on the important/essential parts of the features when making the prediction. Our key contribution here is to utilize an attention module to  capture the similarities between the features of data samples in the same class (regardless of the domain), and emphasize the important/common characteristics to better learn the domain-invariant features.  Especially in data-poor FL scenarios where models are prone to overfitting to small local datasets,  our new strategy  provides  advantages   for   complicated DG  tasks by removing background noises that are unrelated to class prediction and focusing on the important parts.  
%This strategy provides significant advantages especially in complicated DG tasks having background noises that are unrelated to the prediction of classes, even when each FL client has limited number of samples or styles/domains.
\end{itemize}
\vspace{-1mm}

%Specifically, we propose style sharing, shifting, and exploration strategies that operate in the style space based on the feature-level style statistics of data samples.  By compensating the missing styles in each client and exploring wider style spaces, these style-based learning methods improve the model robustness against domain shift at testing. We also propose  attention-based weighted averaging that computes the similarities between the features of data samples in the same class, and emphasizes the important/common characteristics to better learn the domain-invariant features in FL scenarios.  This strategy provides significant advantages especially in complicated DG tasks where capturing the key characteristics of each class is of paramount importance.%datasets having backgrounds unrelated to the prediction of class or having large style shifts between  domains.  %Fig. \ref{fig:overall_algorithm} provides a brief overview of our problem setup and our StableFDG algorithm. 

The two suggested schemes  work in a complementary fashion, each providing one necessary component for federated DG: our style-based learning
improves domain diversity, while the attention-based feature highlighter   learns domain-invariant characteristics of each class.  Experiments on various  FL setups using  DG benchmarks confirm the advantage of  StableFDG over   (i) the  baselines that directly apply DG methods to FL and (ii)  the    baselines that are specifically designed for federated DG. 

%the advantage of our method in various FL scenarios with different data distributions setups.  We show that StableFDG performs better than (i) the  baselines that directly apply DG methods to FL and (ii) other existing baselines that are specifically designed tailored to federated domain generalization.   
%Experimental results on DG benchmarks confirm the advantage of our method in various FL scenarios with different data distributions setups.  We show that StableFDG performs better than (i) the  baselines that directly apply DG methods to FL and (ii) other existing baselines that are specifically designed tailored to federated domain generalization.   




\vspace{-1mm}

\section{Related Works}\label{sec:related}
\vspace{-1mm}
\textbf{Federated learning.} FL  enables multiple clients to train a shared global model or personalized models without directly sharing each client's data with the server or other clients.  FedAvg  \cite{mcmahan2017communication} is a well-known early work that sparked interest in FL in the machine learning community. Since then, various  FL strategies have been proposed    to handle  the communication burden issue  \cite{reisizadeh2020fedpaq,hamer2020fedboost}, data heterogeneity issue  \cite{li2020federated, karimireddy2020scaffold},  adversarial attack issue \cite{wang2020attack, park2021sageflow, fang2020local}, and  personalization issue  \cite{deng2020adaptive, li2021ditto, fallah2020personalized}. However,   existing   FL methods do not have generalization capabilities to predict well on  an arbitrary unseen domain. In other words,  most prior FL methods are not  able to  handle the DG problem.

 

\textbf{Domain generalization.}  DG is one of the emerging fields in the AI community due to its significance in practical applications.  Existing DG strategies based on domain alignment  \cite{li2018domain, li2018deep, li2018domaincc},  meta-learning \cite{li2019episodic, du2020learning, li2018learning, zhao2021learning}  and style/data-augmentation  \cite{zhou2021domain,li2022uncertainty,zhang2022exact, zhou2020learning} have shown great success in a centralized setup where the whole dataset is accessible during training.   Recently, style-augmentation methods \cite{zhou2021domain,li2022uncertainty,zhang2022exact} including MixStyle \cite{zhou2021domain} and DSU \cite{li2022uncertainty} are receiving considerable attention  due to their high compatibility with various tasks and  model architectures.  However, although existing DG solutions work well in a centralized setup, they face challenges in FL scenarios; % client has  limited amount of data that cannot be shared with other clients;  
in data-poor FL setups, prior works achieve relatively low performance due to the lack of  data samples or domains in each client, resulting in compromised generalization capabilities.   %Moreover, when  the styles of data samples   are not well separated in   style space, style-augmentation  schemes do not provide meaningful performance gains.  
The applications of meta-learning or domain alignment methods could be also limited when domain labels are not accessible in each client. Compared to these prior works focusing on  a centralized DG setup, we develop a style and attention based DG strategy tailored to  FL.  The advantages of our methodology against these baselines are   confirmed via experiments in Sec.~\ref{sec:experiments}.
%Domain alignment or meta-learning based methods: limited performance in a FL setup due to limited domain in each client.           Style-based methods: good performance and has high compatibility with various models. However, limits the performance in a FL setup due to limited styles in each client. Moreover, when the styles are not separated well in the style-space, does not provide meaningful gains. All existing DG methods have been studied in a centralized setup. It is unclear how to train a good DG model in a federated setup with distributed data. 


 
\textbf{Federated domain generalization.} Only a few recent works   \cite{liu2021feddg, chen2023federated, wu2021collaborative, nguyen2022fedsr}  have focused on the intersection of FL and DG.   
 Based on the training set distributed across  clients, the goal of these works  is to construct a  global model that is generalizable to an   unseen target domain.  In \cite{chen2023federated}, the  authors proposed to share the   style statistics  across different clients that could be utilized during  local updates.    However,  this method does not utilize the  styles beyond the clients' domains and   shows limited performance in specific datasets where the data samples are not well separated in the style space. Moreover, it increases the computation and memory costs for generating new images in each client using a pretrained style transfer. In our work, we handle these issues via style exploration and the attention-based feature highlighter to train the model with novel styles while capturing  the  important knowledge of each class.  In \cite{liu2021feddg}, the authors proposed to exchange  distribution information among clients based on Fourier transform,  especially targeting image segmentation tasks for medical data.  The authors of \cite{wu2021collaborative} proposed a strategy for federated DG based on the domain-invariant feature extractor and an ensemble of domain-specific classifiers. Two regularization losses  are developed in \cite{nguyen2022fedsr}   aiming to learn a simple representation of   data during client-side local updates.   
 
 
 Although  these prior works \cite{liu2021feddg,   wu2021collaborative, nguyen2022fedsr} improve DG performance, the authors  do not directly handle the issues that arise from limited styles and data in each client. Compared to these works, we take an orthogonal approach based on  style  and attention based learning  to effectively learn style-invariant features while capturing common knowledge of classes. Experimental results in Sec.~\ref{sec:experiments} reveals that our scheme outperforms existing ideas tackling federated DG in practical data distribution setups.  
  
 \vspace{-1mm}

\section{Proposed StableFDG Algorithm}
\vspace{-1mm}

\textbf{Problem formulation.} We consider a FL   setup with $N$ clients distributed over the network. Let  $\mathcal{S}_n = \{(x_i^n, y_i^n)\}_{i=1}^{\rho_n}$
 be the local dataset  of the $n$-th client, which
   consists of $\rho_n$ pairs of data sample $x$ and the corresponding label $y$.  Here, each client $n$ can have data samples from either a single  or multiple source domains in its local dataset $\mathcal{S}_n$.  Previous works on  FL focus on constructing a global model that predicts well on the overall dataset   $\mathcal{S}=\{\mathcal{S}_1, \mathcal{S}_2, \dots, \mathcal{S}_N\}$ or personalized models that work well on individual local datasets $\mathcal{S}_n$. In contrast to the conventional FL setup,  given the overall dataset (or source domains) $\mathcal{S}$, the goal of this work is to construct a shared global model $\mathbf{w}$ that has  a generalization capability to predict well on any unseen target domain $\mathcal{T}$. 
 
 
 \textbf{Background.} Let $s\in\mathbb{R}^{C\times H\times W}$ be the feature of a  sample which is obtained at a specific layer of the neural network. Here, $C$, $H$, $W$ denote the dimensions of channel, height, width, respectively. Given the feature $s$ of a specific data sample, the channel-wise mean  $\mu(s)\in \mathbb{R}^C$ and the channel-wise standard deviation $\sigma(s)\in \mathbb{R}^C$ can be written as 
%\vspace{-1mm}
\begin{align}\label{eq:style_statistics}
\mu(s)_c = \frac{1}{HW}\sum_{h=1}^H\sum_{w=1}^Ws_{c,h,w}, \ \ 
\sigma^2(s)_c = \frac{1}{HW}\sum_{h=1}^H\sum_{w=1}^W(s_{c,h,w} - \mu(s)_c)^2,
\end{align}
respectively. These  variables are known as   \textit{style statistics} as they contain style information of an image in CNNs \cite{huang2017arbitrary}.   Based on these style statistics, various style-augmentation schemes such as MixStyle \cite{zhou2021domain} or DSU \cite{li2022uncertainty}  have been proposed in the literature targeting a centralized setup.



%  \begin{figure}[t]
%\centering
%    \centerline{\includegraphics[width=78mm]{Fig_overall.pdf}}   
%  \caption{Overview of proposed StableFDG for federated domain generalization. Our scheme consists of three components with different roles: (1-1) style sharing  and (1-2) shifting, (2) style exploration, and (3) attention-based weighted averaging.}  
%  \label{fig:overall_algorithm}
%\end{figure}  
 \begin{wrapfigure}{r}{0.48\textwidth}
   \vspace{-5mm}
  \centering
  \includegraphics[width=0.48\textwidth]{Fig_overall_new.pdf}
  \vspace{-5.5mm}
  \caption{Overview of our problem setup and algorithm for federated domain generalization. Each client can have  a single source domain  as described in Fig. \ref{fig:overall_algorithm}, or even multiple source domains in its local dataset. }%Our scheme consists of three components with different roles: (1-1) style sharing  and (1-2) shifting, (2) style exploration, and (3) attention-based weighted averaging.}
  \label{fig:overall_algorithm}
  \vspace{-3mm}
\end{wrapfigure}
\textbf{Overview of approach.} Fig. \ref{fig:overall_algorithm} provides an overview of the problem setup and our StableFDG algorithm. As in conventional FL, the training process  consists of multiple global rounds, which we index  $t=1,2,\dots, T$.  In the beginning of    round $t$,  a selected set of clients   download the current global model $\mathbf{w}_t$ from the server. %and the style information of  another client  $n'$ (i.e., $\Phi_{n'}=[\mu_{n'}, \sigma_{n'}, \Sigma_{n'}(\mu), \Sigma_{n'}(\sigma)]$) from the server, which will be clarified soon.  
Before local training begins, each client $n$ computes its own style information  $\Phi_{n}=[\mu_{n}, \sigma_{n}, \Sigma_{n}(\mu), \Sigma_{n}(\sigma)]$ using its local dataset according to (\ref{eq:mu}), which will be clarified soon.  This  information is sent to the server, and    the server  shares these   information with other clients to compensate for the lack of styles or domains in each client.  
%Our first step is to let the clients to compute the style information   using their local data, and transmit them to the server.    The server then shares these style information to other clients to compensate with the lack of styles or domains in each client.  
 During the  local update process, each client selectively shifts the styles of the original data in the mini-batch to the new style (received from the server) via adaptive instance normalization (AdaIN) \cite{huang2017arbitrary}, to improve domain diversity (inner box in Fig. \ref{fig:bbbbbbbbbb}).  After this style sharing and shifting process, each client performs style exploration via feature-level oversampling to further expose the model to novel styles beyond the current source domains of each client (outer box in Fig. \ref{fig:bbbbbbbbbb}). %After this style sharing and shifting process, each client performs class-balanced oversampling and style exploration  to further expose the model to various styles during training while handling the class-imbalance issue  (Fig. \ref{fig:algorithm_step2}).  
Finally, at the output of the feature extractor,  we apply our   attention-based feature highlighter to extract common/important feature information within each class and emphasize them for better generalization   (Fig. \ref{fig:attention}). When local updates are finished, the server aggregates the client models  and proceeds to the next round.





 In the following, we first describe our  style-based learning    in  Sec. \ref{subsec:31}, which determines how the style information is shared and utilized in each FL client, and how  style exploration is conducted, to overcome   the lack of  styles in each client. In Sec.  \ref{subsec:33}, we  show how attention mechanism is utilized  to capture   the essential characteristics  of each class for better generalization in data-poor FL setups.  


%\begin{figure}[ht]
%\centering
%  \subfigure[Style sharing and shifting]{\includegraphics[width=0.38\textwidth]{Step_1_style_sharing_injection.pdf}\label{fig:algorithm_step1}}   \vspace{-2mm}
%  \subfigure[Style exploration]{\includegraphics[width=0.42\textwidth]{Step_2_style_exploration.pdf}\label{fig:algorithm_step2}}          \vspace{-2mm}
%  \subfigure[Attention-based information extraction]{\includegraphics[width=0.45\textwidth]{Step_3_attention.pdf} \label{fig:algorithm_step3}}
%  \vspace{-2mm}
%\caption{Description of each of our component. These three components  have their own roles and work in a complementary fashion.}
%  \label{fig:Main_algorithm}
%\end{figure}






\vspace{-1mm}

\subsection{Style-based Learning for Federated DG} \label{subsec:31}
\vspace{-1mm}
  
 The main   components of our style-based learning are style-sharing, selective style-shifting, and style-exploration, each having its own role to handle the style limitation problem at each   client. As illustrated in Fig. \ref{fig:Main1_Style}, this fundamental challenge of federated DG is not directly resolvable using existing style-augmentation DG methods that can only explore   limited areas in the style-space. %relying on interpolation or shifting near the original style.}

\textbf{Step 1: Style information sharing.} Based on the  data samples in its local dataset $\mathcal{S}_n$, at a specific layer of the model, each client $n$ computes   the  average of channel-wise means, $\mu_n$,  and the variance of the channel-wise means, $\Sigma_n(\mu)$, as follows: 
 \begin{equation}\label{eq:mu}
\mu_n = \frac{1}{\rho_n}\sum_{i=1}^{\rho_n}\mu(s_i^n), \  \ \Sigma_n(\mu) = \frac{1}{\rho_n}\sum_{i=1}^{\rho_n}(\mu_n - \mu(s_i^n))^2,
\end{equation}
where $s_i^n\in\mathbb{R}^{C\times H\times W}$ is the feature of the $i$-th  sample in the $n$-th client's local dataset, and  the square operation $(\cdot)^2$  works in an element-wise manner. Similarly, the average and  the variance of channel-wise standard deviations are computed as $\sigma_n = \frac{1}{\rho_n}\sum_{i=1}^{\rho_n}\sigma(s_i^n)$, $\Sigma_n(\sigma) = \frac{1}{\rho_n}\sum_{i=1}^{\rho_n}(\sigma_n - \sigma(s_i^n))^2$,
%  \vspace{-1.2mm}
%\begin{equation}\label{eq:sigma}
% \sigma_n = \frac{1}{\rho_n}\sum_{i=1}^{\rho_n}\sigma(s_i^n),
% \  \Sigma_n(\sigma) = \frac{1}{\rho_n}\sum_{i=1}^{\rho_n}(\sigma_n - \sigma(s_i^n))^2,
%\end{equation}
respectively.  Here, $\mu_n$ and $\sigma_n$ represent the center of the style statistics of client $n$, while $\Sigma_n(\mu)$ and $\Sigma_n(\sigma)$ show how  these style statistics of client $n$ are distributed around the center. Now we define  
%\vspace{-1mm} 
\begin{equation}
\Phi_n=[\mu_n, \sigma_n, \Sigma_n(\mu), \Sigma_n(\sigma)]
\end{equation}
as  the \textit{style information} representing the domain identity of the $n$-th client  in the style-space.  %\textcolor{red}{OK!!}  
 Each client $n$ sends   $\Phi_n$  to the server, and the server randomly shares the style information   to other clients in a one-to-one manner;   client  $n$ receives  $\Phi_{n'}$ that belongs to   client  $n'$  ($n\neq n'$) without overlapping with other clients.   By compensating for the lack of styles  in each  FL client,  this style-sharing process is the first step that   provides an opportunity for each model to get exposed  to new styles (blue region in Fig. \ref{fig:bbbbbbbbbb}) beyond the client's original  styles  (orange region in Fig. \ref{fig:bbbbbbbbbb}). %We report the  ablation studies related to style sharing  in  Supplementary Material.     %Downloading  style information of multiple clients is also possible, with the cost of increased communication. The related ablation studies regarding this style sharing process are reported  in  Supplementary Material.    
 
 
 %Sharing multiple %client $n_1$ and $n_2$ receive   $\Phi_{m_3}$ and $\Phi_{m_4}$, where $$%and the server collects the style information of each device. To compensate for the lack of styles  in each client, the server shares this style information and the current global model to other clients in the beginning of each global round. Specifically, each client randomly receives one of the style inforation the server has.}

%\textcolor{red}{Specifically, each client $n$ receives a $\Phi_{n'}$ that belongs to a specific client $n'$  ($n\neq n'$) from the server, and all clients receive different style information without  overlapping.  } %\textcolor{red}{Note that....describe privacy issue.}

 
  
%
%\begin{figure}[t]
%     \centering
%     \begin{subfigure}[b]{0.4\textwidth}
%         \centering
%         \includegraphics[width=\textwidth]{Step_1_style_sharing_injection.pdf}
%         \caption{Step 1: Style sharing and shifting}
%\label{fig:algorithm_step1}    
% \end{subfigure}
%     \begin{subfigure}[b]{0.4\textwidth}
%         \centering
%         \includegraphics[width=\textwidth]{Step_2_style_exploration.pdf}
%         \caption{Step 2: Style exploration}
%\label{fig:algorithm_step2}    
%     \end{subfigure}
%     \begin{subfigure}[b]{0.4\textwidth}
%         \centering
%         \includegraphics[width=\textwidth]{Step_3_attention_new.pdf}
%         \caption{Step 3: Attention-based weighted averaging}
%\label{fig:algorithm_step3}    
%     \end{subfigure}
%          
%  \caption{Three  components of StableFDG. Each component  has  its own role and works in a complementary fashion with others. }
 %         \label{fig:three graphs} 
           
%  \end{figure}
     
\begin{figure}[t]
       \centering
     \begin{subfigure}[b]{0.38\textwidth}
         \centering
         \includegraphics[width=\textwidth]{Fig_MixStyle_DSU.pdf}
         \caption{Existing style-based DG baselines}
\label{fig:aaaaaaaaa}    
 \end{subfigure}
     \begin{subfigure}[b]{0.6\textwidth}
         \centering
         \includegraphics[width=\textwidth]{Step_12_new.pdf}
         \caption{Proposed style-based learning for federated DG}
\label{fig:bbbbbbbbbb}    
     \end{subfigure}
 \caption{\textbf{Proposed style-based learning strategy (Sec. \ref{subsec:31}):} Compared to existing style-based DG methods that rely on interpolation or style shift near the original style of each sample, our  style-based learning  (i) effectively utilizes other FL clients' styles based on style sharing and shifting, and (ii) enables the model to further  explore a wider style space   via feature-level oversampling and extrapolation, handling the issue  of domain-limited FL settings.}
        \label{fig:Main1_Style} 
        \vspace{-2mm}
 \end{figure}
     
%\begin{figure}[t]
%\vspace{-0.8mm}
%     \centering
%     \begin{subfigure}[b]{0.48\textwidth}
%         \centering
%         \includegraphics[width=\textwidth]{Step_12.pdf}
%         \caption{Style-based learning}
%\label{fig:algorithm_step1 & 2}    
% \end{subfigure}
%     \begin{subfigure}[b]{0.49\textwidth}
%         \centering
%         \includegraphics[width=\textwidth]{Step_3_attention_new.pdf}
%         \caption{Attention-based weighted averaging}
%\label{fig:algorithm_step3}    
%     \end{subfigure}
%     \vspace{-1mm}
%\caption{\small Two main  components of StableFDG. Style-based learning improves  domain diversity during training while attention-based weighted averaging   captures the key characteristics of each class for better generalization.}
%        \label{fig:three graphs} 
%                \vspace{-3mm}
%\end{figure}
%     
     


\textbf{Step 2: Selective style shifting.} Suppose client $n$ received $\Phi_{n'}$ from the server. Now the question is, how should each client   utilize this   additional style information    during training   to improve domain/style diversity? Our idea  is to selectively shift the styles of the  samples from the original style  to the new style to effectively balance between the original/new source domains.  To this end, given a  mini-batch with size  $B$, each client runs $k$-means++  with $k=B/2$ in the style-space for one iteration and selects $B/2$ cluster centers. This enables each client to    choose the  $B/2$ styles that are similar  to the remaining $B/2$ styles, so that the model can get exposed to new styles while not losing the performance on the original styles.
%we utilize k-means++ to strategically select the most different B/2 styles compared to the remaining B/2 styles in the mini-batch, to effectively balance between the original source domain and the new source domain for better generalization. 
  The selected $B/2$ samples keep their original styles, while for the remaining $B/2$ samples, we shift the style  of their feature $s$ to the new style via AdaIN \cite{huang2017arbitrary} as  
%\vspace{-1.2mm} 
%\begin{equation}
$f(s)=(\sigma_{n'} + \epsilon_{\sigma}\Sigma_{n'}(\sigma))\Big(\frac{s-\mu(s)}{\sigma(s)}\Big)+(\mu_{n'} + \epsilon_{\mu}\Sigma_{n'}(\mu))$,
%\end{equation}
where $f(s)$ is the new feature shifted from $s$ and $\epsilon_{\mu}\sim \mathcal{N}(0,\Sigma_n(\mu))$, $\epsilon_{\sigma}\sim \mathcal{N}(0,\Sigma_n(\sigma))$ are   the values sampled from   normal distributions.    Then, the mini-batch applied with new styles in  $\Phi_{n'}$ is forwarded to the next layer.  The inner box in Fig. \ref{fig:bbbbbbbbbb}    shows  how style shifting  is performed in client $n$ based on the  new style information  $\Phi_{n'}$.         

%Note that the above process is technically different from MixStyle  \cite{zhou2021domain} and	DSU  \cite{li2022uncertainty} where the new styles are generated via style-interpolation or    shifting near the original style. Compared to these methods, our style sharing in Step 1 and strategic style shifting in Step 2 enable each client (having limited styles) to get exposed to not only the original styles in its local but also  other clients' plausible styles for better generalization, making StableFDG novel beyond existing methods. 



%These are the novelties of StableFDG compared to the existing works.   %These are also confirmed via experiments in Sec.~\ref{sec:experiments}.

%\vspace{1mm} 
%\begin{remark}
 
 
%\subsection{Style Exploration} \label{subsec:32}
 Overall, based on the shared style statistics in Step 1, our style shifting in Step  2 balances between the original source domain and the new source domain via $k$-means++ for better generalization, which cannot be achieved by previous methods in Fig.  \ref{fig:aaaaaaaaa} that rely on interpolation or style-shift near the original style. %Although new styles  are injected to each client   in the above procedures, the  model may still need  to explore more diverse styles since the target domain may be significantly far from the current styles;  for example, Sketch domain in PACS dataset is significantly far from other domains in the style space \cite{zhou2021domain}.  To this end, 
 In the following, we  describe our  style exploration that can further resolve the style-limitation problem  based on feature-level oversampling and extrapolation. 


%\textcolor{blue}{After  the style sharing/shifting process  in  Sec. \ref{subsec:31}, we  perform class-balanced oversampling in the feature space  to mitigate the class-imbalance issue that naturally arises in FL scenarios. At the same time, the styles of oversampled features are shifted  outside the original source domains  to (i) prevent overfitting caused by oversampling and (ii) expose the model to novel styles beyond the styles of each client.  } 

\textbf{Step 3: Feature-level oversampling.}
 Let $s^n\in \mathbb{R}^{B \times C \times H \times W}$ be a mini-batch of features in client $n$ at a specific layer,  obtained after   Steps 1 and 2 above.     Here, we oversample the features  by the mini-batch size $B$ in a class-balanced manner;  the samples belonging to minority classes are compensated  as balanced as possible up to size $B$.  For example, suppose  that the number of samples for classes 
$a$, $b$, $c$ are 3, 2, 1, respectively in the mini-batch. Using oversampling size of $6$, we oversample by 1, 2, 3 data points for classes $a$, $b$, $c$, respectively, to balance the mini-batch in terms of classes.   %If the oversampling size is 1, we oversample the data point in class $c$. If the oversampling size is 3, we oversample one data point from class  $b$ and two data points from class $c$. 
Based on this, we obtain a new oversampled mini-batch $\tilde{s}^n$ with size $B$, and concatenate it with the original mini-batch as follows: $\hat{s}^n =  [s^n, \tilde{s}^n ]$.   This   not only mitigates the class-imbalance issue in each client but also provides a good platform for better style exploration; the  oversampled features are utilized to explore a wider style-space  beyond the current source domains, as we will  describe  in Step 4.  The  oversampling size can be adjusted depending on the clients' computation/memory constraints. %Ablation studies on feature-level oversampling   can be found in Sec. \ref{sec:experiments} and Supplementary Material.

%as balanced as possible among the samples in the batch


\textbf{Step 4: Style exploration.} In order to further  expose the model to a wider variety of styles, we transfer the styles of tensors in  $\tilde{s}^n$ (i.e., the set of oversampled features) to novel styles beyond the style of each client.  Let $\tilde{s}_i^n$ be the feature of $i$-th sample in the oversampled mini-batch $\tilde{s}^n$. We obtain the new styles by extrapolating the original styles in $\tilde{s}^n$ around the average of channel-wise mean $\mu_n(\hat{s}^n)$  and the average of standard deviations $\sigma_n(\hat{s}^n)$ computed on the   concatenated mini-batch $\hat{s}^n =  [s^n, \tilde{s}^n ]$ as 
\begin{align}\label{alpha12} 
\mu_{new}(\tilde{s}_i^n)= \mu(\tilde{s}_i^n) + \alpha \cdot (\mu(\tilde{s}_i^n) - \mu_n( s^n)),\\  \sigma_{new}(\tilde{s}_i^n)= \sigma(\tilde{s}_i^n) + \alpha \cdot (\sigma(\tilde{s}_i^n) - \sigma_n(s^n)),
\end{align}
%\begin{align}\label{alpha12} 
%\mu_{new}(\tilde{s}_i^n)= \mu(\tilde{s}_i^n) + \alpha \cdot (\mu(\tilde{s}_i^n) - \mu_n( s^n)), \ \ 
%\sigma_{new}(\tilde{s}_i^n)= \sigma(\tilde{s}_i^n) + \alpha \cdot (\sigma(\tilde{s}_i^n) - \sigma_n(s^n)),
%\end{align}
where $\alpha$ is the \textit{exploration level}.  We perform AdaIN to shift the style   of $\tilde{s}_i^n$ to the  new style statistics $\mu_{new}(\tilde{s}_i^n)$ and  $\sigma_{new}(\tilde{s}_i^n)$. If $\alpha=0$, the styles remain unchanged, and as   $\alpha$ increases, the styles are shifted farther from the center.    The outer box in Fig. \ref{fig:bbbbbbbbbb}      describes the concept of our style exploration.  



% The above procedures in Steps 3 and 4 handle  the issue of the limited styles, which is one of the unique contributions of StableFDG. 
 
 
%\textcolor{red}{By enabling the model to explore a wider region in the style space  using a controllable parameter  $\alpha$,   the above Step 4 is especially beneficial when  the target domain is significantly far from the source domains, which again makes  existing DG methods (e.g., MixStyle, DSU) to be not enough (Fig. \ref{fig:aaaaaaaaa} vs. Fig. \ref{fig:bbbbbbbbbb}).   It turns out that our scheme has significant performance improvements over these baselines even with an arbitrarily  chosen $\alpha$, as we will see in Sec. \ref{subsec:ablation}. } 

%\textcolor{red}{The above procedures in Steps 3 and 4 handle  the  issue on the limited styles, which is one of the unique contributions of StableFDG. Also, regardless of datasets, we performed all experiments with  $\alpha=3$ in this paper and the results (Sec. \ref{subsec:experiment_main}, \ref{subsec:ablation}) demonstrates that our scheme is not sensitive to $\alpha$.}


% it turns out that our scheme is not sensitive to the exploration level $\alpha$. We also provide a detailed analysis regarding $\alpha$ in Sec. \ref{subsec:ablation}. 




\textbf{Step 5: Style augmentation.} After   style exploration, we can apply existing style-augmentation methods during training.   In this work, we mix the style statistics of the entire samples in $\hat{s}^n$ to generate diverse domains as in \cite{zhou2021domain} as
$\mu_{new}(\hat{s}_i^n)= \lambda \cdot \mu(\hat{s}_i^n) + (1 - \lambda) \cdot \mu(\hat{s}_j^n)$ and 
$\sigma_{new}(\hat{s}_i^n)= \lambda \cdot\sigma(\hat{s}_i^n) + (1 - \lambda) \cdot\sigma(\hat{s}_j^n)$, 
where $\hat{s}_i^n$ and $\hat{s}_j^n$ are arbitrary two samples in $\hat{s}^n$ and $\lambda$ is a mixing parameter sampled from the beta distribution.  Below, we  wish to highlight two important points.%  \textcolor{red}{OK!!}%\textcolor{red}{OK!} %Note that the proposed style exploration module is probabilistically operated in multiple layers and once activated at a specific layer, only style mixing is performed without oversampling in the subsequent layers. 

\textbf{Remark 1 (Privacy).}  It is already well-known that   there is an inherent clustering of samples based on their domains in the style-space, regardless of their labels  \cite{zhou2021domain}. This indicates  that label information is not contained in the style statistics, resolving   privacy issues. Note that some prior works  on federated DG \cite{liu2021feddg, chen2023federated}   also adopt sharing style information between clients, but in different ways (see Sec. \ref{sec:related}). 


 \textbf{Remark 2.} By enabling the model to explore a wider region in the style space  based on the exploration level  $\alpha$,   our style exploration in  Step 4 is especially beneficial when  the target domain is significantly far from the source domains (e.g., Sketch domain in PACS dataset). Existing style-based DG methods (e.g., MixStyle, DSU) are ineffective in this case as they can explore only some limited areas near the original styles in each client (Fig. \ref{fig:aaaaaaaaa} vs. Fig. \ref{fig:bbbbbbbbbb}),  which leads to performance degradation especially in data-poor FL scenarios.   It turns out that our scheme has significant performance improvements over these baselines even with a rather   arbitrarily  chosen $\alpha$, as we will see in Sec. \ref{subsec:ablation}. 

% In   Sec. \ref{sec:experiments}, we  confirm the advantage of our StableFDG  compared to these prior works.%, while experimental results comparing with these baselines can be found in Sec. \ref{sec:experiments}. % In   Sec. \ref{sec:experiments}, we compare our StableFDG with these prior works on federated DG.%; \cite{liu2021feddg} operates in the frequency space using Fourier transform while ours works in the style space based on AdaIN. \cite{chen2023federated} utilizes a pretrained  style transfer model for generating new images in each client, which   increases the computational and memory costs. In   Sec. \ref{sec:experiments}, we compare our StableFDG with these prior works on federated DG. %\end{remark}

%As described in Sec. \ref{subsec:31} and later in Sec. \ref{subsec:32}, StableFDG trains the model to learn style-invariant features by shifting the style of each sample to various other styles. Moreover, it is already well-known that   there is an inherent clustering of samples based on their domains in the style-space, regardless of their labels  \cite{zhou2021domain}. These indicate  that label information is not contained in the style statistics, resolving   privacy issues. Note that some prior works   \cite{liu2021feddg, chen2023federated}   also adopt sharing style information between clients and utilize them, but in different ways; \cite{liu2021feddg} operates in the frequency space using Fourier transform while ours works in the style space based on AdaIN. \cite{chen2023federated} utilizes a pretrained  style transfer model for generating new images in each client, which   increases the computational and memory costs. In   Sec. \ref{sec:experiments}, we compare our StableFDG with these prior works on federated DG. %\end{remark}



%\textbf{Remark 2 (Comparison with existing style-augmentation methods for DG).} Existing style-augmentation methods for DG in Fig. \ref{fig:aaaaaaaaa}    %As mentioned, only applying %As mentioned, existing  style-based methods achieve limited performance in data-poor FL scenarios with limited styles/domains. The conceptual difference compared to  these methods are  illustrated  in Fig. \ref{fig:Main1_Style}, while the advantage of StableFDG compared to these baselines are confirmed via experiments in Sec. \ref{sec:experiments}. %while performance advantage of StableFDG is confirmed via  experiments in  Sec.~\ref{sec:experiments}.% Draw a figure that compares MixStyle vs. DSU vs. Ours.
 %  \vspace{-1.5mm}
  \vspace{-1mm}

\subsection{Attention-based Feature Highlighter}  \label{subsec:33}
  \vspace{-1mm}


%Recall that the proposed schemes in Sec. \ref{subsec:31} operate at the early layer  of the model where the style information is preserved. 
 In this subsection, we describe the second component of our solution,  the attention-based feature highlighter, which operates at the output of the feature extractor to tackle  the remaining challenge of federated DG: limited generalization in each client due to the lack of data.  %Achieving a desired DG performance is especially challenging in FL scenarios due to the limited amount of data   in each client. 
To handle this issue, we start from our key  intuition that the images  from the same class have inherently common characteristics  regardless of domains to which they belong  (e.g., Fig. \ref{fig:attention}). Based on this insight,   we take advantage of attention to find the essential characteristics of images in the same class and emphasize them, which turns out to be very effective in data-poor FL scenarios.   
%Recall that the proposed schemes in Sec. \ref{subsec:31} operate at the early layer  of the model where the style information is preserved. In this subsection, we describe the second component of our solution  termed  attention-based weighted averaging, which operates at the output of the feature extractor  to further improve the DG performance in  data-poor FL scenarios. Our key motivation is that the images from the same class have inherently common characteristics  regardless of domains to which they belong  (e.g., Fig. \ref{fig:attention}). Based on this motivation,      we take advantage of attention to find the essential characteristics of images in the same class and emphasize them for better prediction performance.   

\textbf{Attention score.} Consider the $i$-th data sample of client $n$. Given the feature $z_i^n \in \mathbb{R}^{C \times H \times W}$  obtained from  the output of the feature extractor, we flatten it to  a two-dimensional tensor $X_i\in \mathbb{R}^{C\times HW}$ with  a size of $(C, HW)$, where we omit the client index  $n$ for notational simplicity. Now consider another $X_j$ ($j\neq i$) in a mini-batch  that belongs to the same class as $X_i$, where the domains of $X_i$ and $X_j$ can be either   same or different. Inspired by the  attention mechanism \cite{vaswani2017attention},   we measure the spatial similarity $S    \in \mathbb{R}^{HW \times HW}$ between   $X_i$ and $X_j$ as follows: 
%\vspace{-0.3mm}
\begin{align}\label{eq:sim}
S := \mathrm{Sim}(X_i, X_j) =  (\theta_q X_j)^T (\theta_k X_i),
\end{align}
where $\theta_q \in \mathbb{R}^{d \times C}$,  $\theta_k \in \mathbb{R}^{d \times C}$ are the learnable parameters  trained to extract important  information
 in each class from the given samples and $d$ is the embedding size of queries $Q=\theta_q X_j$ and 
 \begin{wrapfigure}{r}{0.52\textwidth}
   \vspace{-3mm}
  \centering
  \includegraphics[width=0.52\textwidth]{Step_3_attention_new_new.pdf}
  \vspace{-4.5mm}
  \caption{\textbf{Proposed attention-based feature highlighter (Sec. \ref{subsec:33}):} Our  attention-based learning  captures   important characteristics within each class (regardless of the domain) for better generalization.}%Style-based strategies in Sec. \ref{subsec:31} are applied prior to this attention  process.}%Our scheme consists of three components with different roles: (1-1) style sharing  and (1-2) shifting, (2) style exploration, and (3) attention-based weighted averaging.}
  \label{fig:attention}
  \vspace{-3mm}
\end{wrapfigure}
keys $K=\theta_k X_i$.    Then, we reshape  $S\in \mathbb{R}^{HW \times HW}$ to $S_{r} \in \mathbb{R}^{HW \times H \times W}$; the $(m,n)$-th spatial feature ($\in \mathbb{R}^{HW \times 1 \times 1}$)  of $S_{r}$ represents the similarity between the $(m, n)$-th spatial feature of the key feature $X_i$ and the overall spatial feature of the query feature $X_j$. Then by taking the mean of $S_{r}$ along the first dimension, we obtain the \textit{attention score} $a_s$, which represents how the spatial feature at each location of   key $X_i$ is similar to the overall features of   query $X_j$:  $a_s= \mathrm{mean}(S_{r}) \in \mathbb{R}^{H \times W}$. A higher score $a_s$ indicates a higher similarity. %The more similar parts to the query feature, the higher the values of $a_s$. 
Finally, we normalize the attention score  through   softmax function so that the total sum is one: $a_s\leftarrow \mathrm{softmax}(a_s) \in \mathbb{R}^{H \times W}$. \\


\vspace{-3mm}


%Then, we reshape the $Sim$ to have the size of $(HW, H, W)$ i.e., $Sim \rightarrow Sim^r \in \mathbb{R}^{HW \times H \times W}$. The $(m,n)$-th spatial feature ($\in \mathbb{R}^{HW \times 1 \times 1}$)  of $Sim^r$ means the similarity between the $(m, n)$-th spatial feature of the key feature ($X^i$) and the overall spatial feature of the query image ($X^j$). Then, by taking the mean of $Sim^r$ along the first dimension, we calculate \textit{attention score $(a^s)$}, which indicates how similar the spatial feature at each location of the key is to all features of the query: $a^s= Mean(Sim^{r}) \in \mathbb{R}^{H \times W}$. The more similar parts to the query feature, the higher the values of $a^s$. Finally, we obtain an \textit{attention weight} $(a^w)$ by normalizing the attention score through softmax function so that the total sum is 1: $a^w=Softmax(a^s) \in \mathbb{R}^{H \times W}$.


\textbf{Attention-based weighted averaging.} Based on the attention score, we take the weighted average of $z_i\in \mathbb{R}^{C\times H \times W}$ to generate an attention feature $A(z_i) \in \mathbb{R}^{C}$ in which the important parts of the \textit{key image},  having  common characteristics with the \textit{query image} (in the same class), are emphasized:
% \vspace{-0.4mm}
 \begin{align}
A(z_i)_c = \sum_{h=1}^H\sum_{w=1}^W (a_s)_{h,w} (z_i)_{c,h,w}.
\end{align} 
As shown in Fig. \ref{fig:attention},  this attention feature is concatenated with the last embedding feature (e.g., after the  global average pooling in ResNet) before the classifier, and it goes through the classifier to compute the loss during training. 
One important thing to note is that $\theta_q$ and $\theta_k$ are trained so that the common features of query and key images become close in the embedding space while unimportant factors   such as  backgrounds  are effectively distanced (i.e., less emphasized). 

 
When implementing attention-based weighted averaging in practice,  instead of directly adopting equation  \eqref{eq:sim}, we modify the similarity function using the query of its own  as
 %query is averaged with this own query to avoid cases where there is little in common between the key image and the query image as follows:
% \vspace{-0.3mm}
\begin{align}\label{eq:mix}
\mathrm{Sim_{mix}}(X_i, X_j) =  \bigg( \frac{\theta_q X_j + \theta_q X_i }{2} \bigg)^T (\theta_k X_i),
\end{align} 
to avoid performance degradation  when there is little commonality between key and query images. In other words, StableFDG takes advantage of both cross-attention and self-attention, enabling the model to extract and learn important characteristics across images (via cross-attention), and within the image (via self-attention).  A more detailed analysis on  \eqref{eq:mix} can be found in Appendix. %The attention module requires additional model parameters. In     Sec. \ref{sec:experiments}, we show that the proposed idea is effective with only  0.44\% of additional model parameters for the attention module.%Although this attention module requires additional model parameters, the size of the attention module is only 0.44\% of the backbone network that we use in Sec. .
 
\textbf{Inference.} During testing, given a new test sample, we compute the spatial similarity of the test sample itself as  $\mathrm{Sim}(X_i, X_i)$ based on  self-attention,   take the weighted average, and concatenate the features to make a prediction.   %As a result, based on the characteristics the model make predictions about the test sample.

%these also find and emphasize the key characteristics of each class from the test sample and thus, based on the characteristics the model make predictions about the test sample.






 \textbf{Remark 3.}  It is interesting to note that  we can adopt \textit{self-attention} during testing. This is because         $\theta_q$ and $\theta_k$ are trained to capture the essential characteristics of each class, which enables the attention module to  effectively  find   the important parts  of  individual   samples   via self-attention (e.g.,  see heat map visualization in Fig.  \ref{fig:attention}).  Moreover, with only 0.44\% of additional  model  parameters for the attention module, this   strategy turns out to be very effective    in learning domain-invariant characteristics of classes   in data-poor FL setups, handling the key challenge of federated DG. %handling the issue on the lack of data in  each   FL  client. }     %This attention strategy turns out to be very effective in learning domain-invariant characteristics of each class in data-poor FL scenarios, as we will see in Sec.   \ref{sec:experiments}. %It turns out in Sec. \ref{subsec:33}
%Overall, by utilizing our scheme in Sec. \ref{subsec:33}, each local model can learn the essential characteristics of each class that are important for prediction even in data-poor FL scenarios. }





%he attention module requires additional model parameters. In     Sec. \ref{sec:experiments}, we show that the proposed idea is effective with only  0.44\% of additional model parameters for the attention module
 \vspace{-1mm}

\subsection{StableFDG}
\vspace{-1mm}
 Finally, we put together StableFDG. In  each  FL round, the clients first download the global model from the server  and perform style sharing, shifting, and exploration according to \ref{subsec:31}, which are done  in the early layers of CNNs where the style information is preserved. Then, at the output of the feature extractor, attention-based weighted averaging is applied  according to Sec. \ref{subsec:33}.  These two components have their own roles and work in a complementary fashion to handle the challenging DG problem in FL; our style-based strategy is effective in  improving the domain diversity, while our attention-based method can directly  capture the domain-invariant characteristics of each class.  After the local update process,   the server aggregates the client models and proceeds to the next round.  %The pseudo code   algorithm of StableFDG is also provided in  Supplementary Material. 
  
  
%Finally, we put together StableFDG. In the beginning of each round, the clients download the global model from the server, compute style information, and share them with other clients according to Sec. \ref{subsec:31}. Then each client performs selective style shifting and style exploration as in Sec. \ref{subsec:31}, which are operated in the early layers of CNNs where the style information is preserved. At the output of the feature extractor, attention-based weighted averaging is applied  according to Sec. \ref{subsec:33} to compute the training loss. After the local update process,   the server aggregates the client models to update the global model.  The pseudo code   algorithm of StableFDG is also provided in  Supplementary Material. %The overall procedure Sec. \ref{subsec:31}..... Sec. \ref{subsec:32}..... Sec. \ref{subsec:33}.... 

 
\textbf{Remark 4 (Computational complexity).} The computational complexity of StableFDG depends on the oversampling size in Sec.  \ref{subsec:31} and the attention module size in Sec. \ref{subsec:33}, %which are the costs for achieving better generalization, and 
which could be controlled depending on  the resource constraints of  clients. We show in Sec. \ref{sec:experiments} that StableFDG   achieves  the state-of-the-art performance with (i) minimal oversampling size and (ii) negligible cost of attention module. A more detailed discussion on the computational complexity is   in Appendix. % \textcolor{red}{To be updated!!}




\vspace{-1.5mm}

\section{Experimental Results}\label{sec:experiments} 

\vspace{-1.5mm}



\subsection{Experimental Setup}
\vspace{-1.5mm}
 \textbf{Datasets.} We consider   five datasets commonly adopted in DG literature: PACS \cite{li2017deeper}, VLCS \cite{fang2013unbiased}, Digits-DG \cite{zhou2020learning}, Office-Home \cite{venkateswara2017deep}, and DomainNet  \cite{peng2019moment}. PACS consists of 7 classes from 4 different domains, while VLCS contains 4 domains  with 5 classes. Digits-DG is composed of 4 different digit datasets, MNIST \cite{lecun1998gradient}, MNIST-M \cite{ganin2015unsupervised}, SVHN \cite{netzer2011reading}, SYN \cite{ganin2015unsupervised},  each  corresponding to a single domain. Office-Home consists of 65 classes from 4 domains, while DomainNet has  345 classes   from 6   domains. The DomainNet results are reported in Appendix.
 %\textbf{Datasets.} We consider the following four datasets commonly adopted in DG literature: PACS \cite{li2017deeper}, VLCS \cite{fang2013unbiased}, Digits-DG \cite{zhou2020learning}, Office-Home \cite{venkateswara2017deep}, and DomainNet  \cite{peng2019moment}. PACS consists of 7 classes from 4 different domains (Art, Cartoon, Photo, Sketch). VLCS contains 4 domains (Caltech, LabelMe, Pascal, Sun) with 5 classes. Digits-DG is composed of 4 different digit datasets, MNIST \cite{lecun1998gradient}, MNIST-M \cite{ganin2015unsupervised}, SVHN \cite{netzer2011reading}, SYN \cite{ganin2015unsupervised}, with distinct font style and background. Each dataset corresponds to a single domain. Office-Home consists of 65 classes from 4 domains (Artistic, Clipart, Product, Real-World), while DomainNet has  345 classes   from 6 different domains (xxxxx ). 
 
 
 
 %PACS [xx] (9,991 images, 4 domains and 7 classes), VLCS [xx] (10,729 images, 4 domains and 5 classes), Digits-DG [xx] (), Office-Home [xx] (15,588, images, 4 domains and 65 classes). 
 
   

\textbf{Data partitioning for FL.} When evaluating the  model performance, we follow the conventional leave-one-domain-out protocol where one domain is selected as a target    and the remaining domains are utilized as sources.  Compared to the centralized setup, in FL, the source domains are distributed across the  clients. We consider a setup with $N=30$ clients and distribute the training set into two different ways: \textit{single-domain data distribution} and \textit{multi-domain data distribution} scenarios. In a single-domain   setup, we let each client to have training data that belong to a single source domain. Since there are three different source domains during training (except DomainNet), the training data of each domain is distributed across 10 clients uniformly at random. In a multi-domain distribution setup, each client can have  multiple domains, but the domain distribution within each client is heterogeneous. For each domain, we sample the heterogeneous proportion from  Dirichlet distribution with dimension $N=30$ and  parameter of 0.5,  and distribute the train  samples of each domain to individual clients according to the sampled proportion.  In Appendix, we also report the results  with $N=3$ following the settings of prior  works in federated DG \cite{chen2023federated, nguyen2022fedsr}.

%\textcolor{red}{Prior federated DG  works \cite{chen2023federated, nguyen2022fedsr} only consider a  total of three clients, allocating a whole source domain to a single client. However, it does not reflect well the practical FL scenario with a large number of clients. To simulate more practical scenarios, we consider a larger number of clients with $N=30$} and distribute the training set into two different ways: \textit{single-domain data distribution} and \textit{multi-domain data distribution} scenarios. In a single-domain   setup, we let each client to have training data that belong to a single source domain. Since there are three different source domains during training (except DomainNet), the training data of each domain is distributed across 10 clients uniformly at random. In a multi-domain distribution setup, each client can have  multiple domains, but the domain distribution within each client is heterogeneous. For each domain, we sample the heterogeneous proportion from  Dirichlet distribution with dimension $N=30$ and  parameter of 0.5,  and distribute the train  samples of each domain to individual clients according to the sampled proportion.  
 

\textbf{Implementation.} Following \cite{zhou2021domain}, we utilize ResNet-18 pretrained on ImageNet as a backbone while the results on ResNet-50 are reported in Sec. \ref{subsec:ablation}.  The exploration level $\alpha$ is set to 3 for all experiments regardless of datasets. For our attention module, we set the embedding size $d$ of queries $Q$ and keys $K$ to 30, where   $Q$ and $K$ matrices are extracted from the output of the last residual block using $1\times1$ convolution such that the channel size of each output is 30. When the attention module is applied, the input dimension of the classifier becomes twice as large since we concatenate the attention feature with the last embedding feature before the classifier. The number of additional model parameters for the attention module is only 0.44\% of the entire model.  FL  is performed for 50 global rounds   and we trained the local model for 5 epochs with a mini-batch size of 32. Among a total of $N=30$ clients, 10 clients participate in each global round. All reported results are averaged over three random seeds. 


 \textbf{Where to apply style-based modules.} Inspired by \cite{zhou2021domain, li2022uncertainty}, we apply our style-based modules with a  probability of 0.5 at specific layers. In particular, style sharing and shifting are executed at the first residual block of ResNet  with a probability 0.5, while the style exploration module is performed at the first or second or third residual blocks independently with  probability 0.5 after   style sharing/shifting. %When the style exploration module works at multiple layers, class oversampling is performed at the first layer and only style mixing is conducted at subsequent layers. 




 \begin{table*}[!t]
%\vspace{-1mm}
 \centering
\begin{subtable}[!t]{1\linewidth}
	\scriptsize
	\centering
	\begin{tabular}{c||cccc|c||cccc|c}
		\toprule  
		&  \multicolumn{5}{c}{\textbf{PACS}} & \multicolumn{5}{c}{\textbf{VLCS}} \\
		\cmidrule{2-11}
	 Methods   & Art & Cartoon	& Photo & Sketch & Avg. & Caltech & LabelMe & Pascal & Sun & Avg.  \\
		\midrule
		FedAvg  \cite{mcmahan2017communication}  &73.67	&70.87	&90.27	&55.70&	72.63& 93.75	&59.30	&70.05	&69.90	&73.25\\	
				FedBN \cite{lifedbn}  & 78.42 &	70.9	 &90.96 &	54.07 &	73.59 & 94.81	&58.59&	72.06	&70.36&	73.96  \\	

		MixStyle  \cite{zhou2021domain} & 79.10	&76.30	&90.10	&60.63	&76.53& 95.20&	60.40	&72.10&	69.93	&74.41 \\
		DSU  \cite{li2022uncertainty}  & 80.43	&75.70	&92.60&	69.87&	79.65 & 96.13&	58.77&	71.80&	71.87&	74.64\\
	      CCST \cite{chen2023federated}  &  71.35&	72.40&	88.65	&64.10&	74.13 &92.50&	61.20	&68.20&	66.50&	72.10 \\
	      FedDG  \cite{liu2021feddg}     & 71.20&	71.40	&90.70	&59.20	&73.13& 95.3&	57.5	&72.8	&69.8	&73.85\\
	      FedSR  \cite{nguyen2022fedsr}   & 76.40	&71.25	&93.25&	60.55	&75.36 &92.10&	60.50&	70.75&	71.65&	73.75\\
	      \textbf{StableFDG} (ours) & 84.10	&78.57&	95.40&	72.73&	\textbf{\underline{82.70}} &98.13&	59.20&	73.60&	70.27&	\textbf{\underline{75.30}} \\
		\bottomrule
	\end{tabular}
		\caption{PACS and VLCS datasets.}
\end{subtable}
\hfill
\newline
\begin{subtable}[!t]{1\linewidth}
 	\scriptsize
 	\centering
	\begin{tabular}{c||cccc|c||cccc|c}
		\toprule  
		&  \multicolumn{5}{c}{\textbf{Office-Home}} & \multicolumn{5}{c}{\textbf{Digits-DG}} \\
		\cmidrule{2-11}
	 Methods     & Art & Clipart	& Product & Real & Avg. & MNIST& MNIST-M & SVHN & SYN & Avg.  \\
		\midrule
		FedAvg  \cite{mcmahan2017communication}&  57.27	&48.23	&72.77&	74.60&	63.22& 98.05&	70.95	&68.95&	86.40	&81.09\\	
	FedBN \cite{lifedbn}   & 57.56&	48.13&	72.65	&74.57&	63.23   & 97.33	&72.68&	71.77	&85.36	&81.79  \\	
		MixStyle  \cite{zhou2021domain} & 56.05	&51.55 	&70.95&	73.25&	62.95& 97.75&	74.25	&70.85&	85.50	&82.09 \\
		DSU  \cite{li2022uncertainty}  & 58.55&	52.60&	71.60	&73.15	&63.98 &98.10&	75.60	&70.47	&85.80	&82.49\\
	      CCST \cite{chen2023federated}  &  51.3&	51.75	&70.2	&70.3&	60.89  &95.10	&62.80	&56.60	&74.90&	72.35 \\
	      FedDG  \cite{liu2021feddg}    & 57.6	&48.1	&72.55&	74.33	&63.15 & 97.97&	72.13	&71.03&	87.87	&82.25\\
	      FedSR  \cite{nguyen2022fedsr}    & 57.8&	48.1&	72.1	&74.2&	63.05& 98.00&	73.00&	68.50	&86.70	&81.55\\ 
	   \textbf{StableFDG} (ours)   & 57.57&	54.30	&72.33	&74.97	&\textbf{\underline{64.79}}  & 97.23	&74.53	&72.95	&85.85	& \textbf{\underline{82.64}}\\
		\bottomrule
	\end{tabular}
	\caption{Office-Home and Digits-DG datasets.}
\end{subtable}
\vspace{-1mm}
\caption{\textbf{Main result 1 (single-domain data distribution):} Each client has one source domain in its local data. The proposed StableFDG achieves the best generalization, underscoring its effectiveness.}	\label{table:main1}
 \vspace{-2mm}
\end{table*}

\textbf{Baselines.} %We consider  the baselines from   different categories. 
\textbf{(i) FL baselines:}  First, we consider   FedAvg \cite{mcmahan2017communication}  and FedBN \cite{lifedbn}, which are the basic FL baselines not specific  to DG. \textbf{(ii) DG baselines applied to each FL client:}  We apply MixStyle \cite{zhou2021domain} during the local update process of each client and aggregate the model via FedAvg. Similarly, we also apply   DSU \cite{li2022uncertainty}  at each client and then perform FedAvg to compare with our work.  \textbf{(iii) Federated DG baselines:}   Among  DG schemes tailored to  FL, we implement the recently proposed CCST \cite{chen2023federated}, FedDG \cite{liu2021feddg}, FedSR \cite{nguyen2022fedsr} and evaluate the performance. For a fair comparison, we reproduced all the baselines in accordance with our experimental setup. The \textit{augmentation level} in CCST, a hyperparameter that controls the amount of augmented images, is set to 3 as in the original paper \cite{chen2023federated}. The hyperparameters in FedSR,  that control  the regularization losses, are tuned to achieve the best performance in our setup. Except for these, we adopted the  parameter values in  the original papers.  




%First, we consider the pure FedAvg \cite{mcmahan2017communication} as the simplest baseline. Secondly, we consider applying MixStyle \cite{zhou2021domain} during the local update process of each client and aggregate the model via FedAvg. Similarly, we also apply   DSU \cite{li2022uncertainty}  at each client and then perform FedAvg to compare with our work.  Among  DG schemes tailored to  FL, we implement the recently proposed CCST \cite{chen2023federated}, FedDG \cite{liu2021feddg}, FedSR \cite{nguyen2022fedsr} and evaluate the performance. For a fair comparison, we reproduced all the baselines in accordance with our experimental setup. The \textit{augmentation level} in CCST, a hyperparameter that controls the amount of augmented images, is set to 3 as in the original paper \cite{chen2023federated}. The hyperparameters in FedSR,  that controls the regularization losses, are tuned to achieve the best performance in our setup. Except for these, we adopted the  parameter values in  the original papers.  
%\textbf{Baselines.} We compare our method with the following baselines. First, we consider the pure FedAvg \cite{mcmahan2017communication} as the simplest baseline. Secondly, we consider applying MixStyle \cite{zhou2021domain} during the local update process of each client and aggregate the model via FedAvg. Similarly, we also apply   DSU \cite{li2022uncertainty}  at each client and then perform FedAvg to compare with our work.  Among  DG schemes tailored to  FL, we implement the recently proposed CCST \cite{chen2023federated}, FedDG \cite{liu2021feddg}, FedSR \cite{nguyen2022fedsr} and evaluate the performance. For a fair comparison, we reproduced all the baselines in accordance with our experimental setup. The \textit{augmentation level} in CCST, a hyperparameter that controls the amount of augmented images, is set to 3 as in the original paper \cite{chen2023federated}. The hyperparameters in FedSR,  that controls the regularization losses, are tuned to achieve the best performance in our setup. Except for these, we adopted the  parameter values in  the original papers.  



%\textbf{Implementation details.} \textcolor{red}{(To be updated)}  30 clients. 10 clients selected in each round. Describe data distribution scenarios. Describe models.  Describe other parameters.  
 \vspace{-1.5mm}

\subsection{Main Experimental Results} \label{subsec:experiment_main}
 \vspace{-1mm}
\textbf{Single-domain data distribution.} Table  \ref{table:main1}  shows our  results in a single-domain data distribution setup. We have the following observations. Compared to the previous results provided in the centralized DG works \cite{zhou2021domain, li2022uncertainty}, the performance of each method is generally lower. This is due to the limited numbers of styles and data samples in each FL client,   which restricts the generalization performance of individual client models.  It can be seen that most of the baselines perform better than FedAvg and FedBN that do not tackle the DG problem.  
 The proposed StableFDG achieves the best average accuracy for all benchmark datasets, where the gain is especially large in PACS   having large  shifts between domains.  In contrast  to our scheme, the prior works \cite{chen2023federated, liu2021feddg, nguyen2022fedsr}  targeting federated DG show marginal performance gains relative to FedAvg in our practical experimental  setup  with (i) more clients (which results in less data in each client) and (ii) partial client participations. 

 
 
 
 
 
\textbf{Multi-domain data distribution.} In Table  \ref{table:main2}, we report the results in a multi-domain data distribution scenario. Compared to the results in Table  \ref{table:main1},  most of the schemes achieve improved performance in Table \ref{table:main2}. %, especially on PACS dataset having a large discrepancy between domains. 
This is because each client has multiple source domains, and thus providing a better platform for each client model to gain generalization ability. The proposed StableFDG still performs the best, demonstrating the effectiveness of our style and attention based learning strategy for federated DG.



 
\begin{table*}[!t]
%\vspace{-2mm}
 \centering
\begin{subtable}[!t]{1\linewidth}
	\scriptsize
	\centering
	\begin{tabular}{c||cccc|c||cccc|c}
		\toprule  
		&   \multicolumn{5}{c}{\textbf{Office-Home}} & \multicolumn{5}{c}{\textbf{VLCS}} \\
		\cmidrule{2-11}
	 Methods    & Art & Clipart	& Product & Real & Avg. & Caltech & LabelMe & Pascal & Sun & Avg.  \\
		\midrule
		FedAvg  \cite{mcmahan2017communication}&  57.70&	48.30	&72.87	&75.33	&63.55 &  93.65&	61.10	&72.55&	65.40&	73.18 \\		
		FedBN \cite{lifedbn}&  57.07	&48.32	&72.31&	74.57	&63.07 &  94.34	&62.61&	69.89&	69.04	&73.97 \\	
		MixStyle  \cite{zhou2021domain}&   55.87&	52.03	&71.10&	74.20&	63.30 &   95.20	&60.77	&73.90&	66.73&	74.15 \\
DSU  \cite{li2022uncertainty} &    58.60	&52.80&	71.63	&74.00&	64.26 & 96.87	&60.23	&72.97	&68.97&	74.76  \\
 CCST \cite{chen2023federated}  & 52.2	&52.2&	70.6&	72.3&	61.83 & 96.70	&60.40	&71.40&	65.00	&73.38 \\ 
 FedDG  \cite{liu2021feddg}     & 57.9&	48.6	&73.2&	75.0	&63.68  &96.2&	60.7	&72.4&	67.3&	74.15  \\
	      FedSR  \cite{nguyen2022fedsr}   & 58.1&	48.2&	72.5&	75.4&	63.55   & 92.60&	60.80&	72.15&	68.30&	73.46\\
	      \textbf{StableFDG}   (ours)   & 57.87&	54.20&	73.10&	75.00&	\textbf{\underline{65.04}}     & 98.50&	60.07&	74.40	&69.43	&\textbf{\underline{75.60}}\\
		\bottomrule
	\end{tabular}
		\caption{Office-Home  and VLCS datasets.}
\end{subtable}
\hfill
\newline
\begin{subtable}[t]{1\linewidth} 
	\scriptsize
\centering
	\begin{tabular}{c||    cccc | c}
		\toprule      &Art  & Cartoon & Photo & Sketch & Avg. \\ 
		\midrule
		FedAvg  \cite{mcmahan2017communication}&   74.87	&74.53&	95.30&	63.37&	77.02\\
		FedBN \cite{lifedbn} & 77.00	&76.83	&95.45	&67.01&	79.07 \\

		MixStyle  \cite{zhou2021domain} & 80.07&	77.53&	96.23&	67.40&	80.31\\
		
		DSU  \cite{li2022uncertainty}   &  80.53&	76.30&	95.37&	70.93&	80.78\\
	CCST \cite{chen2023federated}   & 75.53	&75.80&	93.53&	71.13	&79.00\\
	    FedDG  \cite{liu2021feddg}    & 75.35	&75.85&	95.65&	61.05	&76.98 \\
FedSR  \cite{nguyen2022fedsr}     &73.83	&74.83	&95.53	&66.03&	77.56\\
	   \textbf{StableFDG}  (ours)  & 83.97	&79.10	&96.27&	75.67	&\textbf{\underline{83.75}} \\
		\bottomrule
	\end{tabular}
\caption{PACS dataset.}
\end{subtable}
\vspace{-1mm}
\caption{\textbf{Main result 2 (multi-domain data distribution):} Each client has multiple source domains in its local dataset. The results are consistent with the single-domain scenario in Table \ref{table:main1}.}
\label{table:main2}
\vspace{-4mm}
\end{table*}


 

%Tables \ref{table:main1} and \ref{table:main2} show our main experimental results, which compare StableFDG with other baselines.
  
%\begin{table}[t]
%\scriptsize
%	\caption{Results on PACS.}
%	\centering
%	\label{tab:pacs}
%	\begin{tabular}{l|    cccc | c}
%		\toprule  
%		Methods   &Art  & Cartoon & Photo & Sketch & Avg. \\ 
%		\midrule
%		FedAvg &   \\
%		
%		MixStyle &  \\
%		
%		DSU &  \\
%	      CCST &  \\
%	      FedSR &  \\
%								\midrule
%	Ours & \\
%		Ours & \\
%	Ours & \\
%		\bottomrule
%	\end{tabular}
%\end{table}
%
%
%\begin{table}[t]
%	\caption{Results on Office Home.}
%	\centering
%	\label{tab:pacs}
%	\begin{tabular}{l|    cccc | c}
%		\toprule  
%		Methods   &Art  & Cartoon & Photo & Sketch & Avg. \\ 
%		\midrule
%		FedAvg  &   \\
%		
%		MixStyle &  \\
%		
%		DSU &  \\
%	      CCST &  \\
%	      FedSR &  \\
%								\midrule
%	Ours & \\
%		Ours & \\
%	Ours & \\
%		\bottomrule
%	\end{tabular}
%\end{table}
% 
%
%\begin{table}[t]
%	\caption{Results on VLCS.}
%	\centering
%	\label{tab:pacs}
%	\begin{tabular}{l|    cccc | c}
%		\toprule  
%		Methods   &Art  & Cartoon & Photo & Sketch & Avg. \\ 
%		\midrule
%		FedAvg &   \\
%		
%		MixStyle &  \\
%		
%		DSU &  \\
%	      CCST &  \\
%	      FedSR &  \\
%								\midrule 
%	Ours & \\
%		Ours & \\
%	Ours & \\
%		\bottomrule 
%	\end{tabular}
%\end{table}
 
 
 
%  \vspace{-4mm}
\vspace{-1mm}

\subsection{Ablation Studies and Discussions } \label{subsec:ablation}


\vspace{-1mm}

\begin{wraptable}{r}{7.7cm}
\vspace{-3.5mm}
\tiny
\centering
	\begin{tabular}{c|cccc|c}
		\toprule  
	 Methods    & Art & Cartoon	& Photo & Sketch & Avg.  \\
		\midrule
		MixStyle  \cite{zhou2021domain}  &80.07&	77.53&96.23&67.40&80.31\\
		DSU  \cite{li2022uncertainty}    &  80.53&	76.30&	95.37&	70.93&	80.78\\		
		StableFDG (only style)  & 82.62 &79.01& 95.57 &74.47& 82.92\\
		StableFDG (only attention)  & 79.98& 78.58& 95.75& 71.35& 81.41\\
		StableFDG (both)  & 83.97& 79.10& 96.27& 75.67& \textbf{\underline{83.75}}\\
		\bottomrule
	\end{tabular}
			\vspace{-0.5mm}
 		\caption{Effect of each component of StableFDG.}
		\vspace{-3mm}
\label{table:ablation_each_comp}
\end{wraptable}  


\textbf{Effect of each component.} To see the effect of each component of StableFDG, in Table \ref{table:ablation_each_comp}, we apply our style-based learning and attention-based learning   one-by-one in a multi-domain data distribution setup using PACS. We compare our results with style-augmentation  DG baselines, MixStyle \cite{zhou2021domain} and DSU \cite{li2022uncertainty}. By applying only our style-based learning, StableFDG already outperforms prior style-augmentation methods.
 Furthermore, by adopting only one of the proposed components, our scheme   performs better than all the baselines in Table   \ref{table:main2}.  Additional   ablation studies using other datasets are reported in Appendix.


%\begin{table}[!t]
%\scriptsize
%\centering
%	\begin{tabular}{c||cccc|c||cccc|c}
%		\toprule  
%		& \multicolumn{5}{c}{\textbf{PACS}} & \multicolumn{5}{c}{\textbf{VLCS}} \\
%		\cmidrule{2-11}
%	 Methods    & Art & Cartoon	& Photo & Sketch & Avg. & Caltech & LabelMe & Pascal & Sun & Avg.  \\
%		\midrule
%		Mistyle  \cite{zhou2021domain}  &80.07&	77.53&96.23&67.40&80.31&  95.20&	60.77&73.90&	66.73&	74.15\\
%		StableFDG (1)  & 82.62 &79.01& 95.57 &74.47& 82.92& 98.82& 58.47& 75.12& 68.32& 75.18\\
%		StableFDG (2)  & 79.98& 78.58& 95.75& 71.35& 81.41& 95.28& 63.11& 72.26& 63.86& 73.63\\
%		StableFDG (1+2)  & 83.97& 79.10& 96.27& 75.67& 83.75& 98.50& 60.07& 74.40& 69.43& 75.61\\
%		\bottomrule
%	\end{tabular}
%		 \vspace{3mm}
%		\caption{\small \textbf{Ablation study 1 (Effect of each component):} Effect of each component of StableFDG in a multi-domain distribution scenario. By comparing the results of Tables \ref{table:main2} and \ref{table:ablation_each_comp}, it can be seen that StableFDG already performs better than existing baselines with  only one or two of the proposed components. }
%\label{table:ablation_each_comp}
%
%\end{table}  
	




%\begin{table}[!t]
%\scriptsize
%\centering
%\begin{subtable}[!t]{1\linewidth}
%\centering
%	\begin{tabular}{l|    cccc | c}
%		\toprule  
%		Methods   &Art  & Cartoon & Photo & Sketch & Avg. \\ 
%		\midrule
% 	MixStyle  \cite{zhou2021domain}& 80.07&	77.53&	96.23&	67.40&	80.31\\
%		
%  StableFDG (2)&  83.07	&77.97&	96.03&	74.10	&82.79 \\
%	   StableFDG  (2+3)&   83.53&	78.53&	96.30&	75.27&	83.41\\
%		    StableFDG  (2+3+1)&  83.97	&79.10	&96.27&	75.67	&\textbf{\underline{83.7}}  \\
%
%		\bottomrule
%	\end{tabular}
%\caption{PACS}
%\end{subtable}
%\hfill
%\newline
%%\caption{Adversarial Test Accuracy against Max-Average Attack for Anytime Prediction Setting. All methods in this table are built upon adversarial training with max-average attack.}
%\begin{subtable}[!t]{1\linewidth} 
%\centering
%	\begin{tabular}{l|    cccc | c}
%		\toprule  
%		Methods    & Art & Clipart	& Product & Real  & Avg. \\ 
%		\midrule
%		
%		MixStyle \cite{zhou2021domain}& 55.87&	52.03	&71.10&	74.20&	63.30\\
%		
%	  StableFDG (2)&  57.63	&53.00	&72.10&	74.60&	64.33 \\
%	  	  StableFDG  (2+3)&  58.30&	54.20&	72.70&	75.27&	\textbf{\underline{65.12}} \\
%		  	  	  StableFDG  (2+3+1)&    57.87&	54.20&	73.10&	75.00&65.04 \\
%
%
%		\bottomrule
%	\end{tabular}
%\caption{Office-Home}
%\end{subtable}
%\begin{subtable}[!t]{1\linewidth} 
%\centering
%	\begin{tabular}{l|    cccc | c}
%		\toprule  
%		Methods   &Caltech & LabelMe & Pascal & Sun & Avg. \\ 
%		\midrule
%		MixStyle  \cite{zhou2021domain}&  95.20	&60.77	&73.90&	66.73&	74.15\\
%		
%	  StableFDG (2)&  98.33	&58.00	&74.43	&68.73	&74.88 \\
%	  	  StableFDG  (2+3)& 98.03&	59.57&	74.27&	68.10	&74.99  \\
%		  	    StableFDG (2+3+1)& 98.50&	60.07&	74.40	&69.43	&\textbf{\underline{75.60}}\\
%		\bottomrule
%	\end{tabular}
%\caption{VLCS}
%\end{subtable}
% \vspace{-3mm}
%\caption{\small \textbf{Ablation study 1 (Effect of each component):} Effect of each component of StableFDG in a multi-domain distribution scenario. By comparing the results of Tables \ref{table:main2} and \ref{table:ablation_each_comp}, it can be seen that StableFDG already performs better than existing baselines with  only one or two of the proposed components. }
%\label{table:ablation_each_comp}
%\end{table}  
 

 

\begin{wrapfigure}{r}{0.51\textwidth}
   \vspace{-4mm}
  \centering
  \includegraphics[width=0.25\textwidth]{Single_domain_various_alpha-eps-converted-to.pdf}
\includegraphics[width=0.25\textwidth]{Single_domain_oversampling_size-eps-converted-to.pdf}
  \vspace{-4mm}
  \caption{ Effects of exploration level $\alpha$ (left) and oversampling size (right) in StableFDG.}
  \label{fig:ablation2}
  \vspace{-3mm}
\end{wrapfigure}


% \vspace{-0.4mm}
\textbf{Effect of hyperparameters.}  In DG setups, it is generally impractical to tune the hyperparameter using the target domain, because there is no information on the target domain during training.  Hence, we used a fixed exploration level $\alpha=3$     throughout all experiments without tuning.   In  Fig. \ref{fig:ablation2},  we observe how the hyperparameters affect the target domain performance on PACS. In the first plot of Fig. \ref{fig:ablation2}, if $\alpha$ is too small, the performance is relatively low   since
  the model is not able to explore novel styles
 beyond the clientâ€™s source domains.  If $\alpha$ is too large, the performance could be slightly degraded because the model would explore too many redundant styles.  The overall results show that StableFDG still performs better than the baselines with an arbitrarily chosen  $\alpha$, which is a significant advantage of our scheme in the DG setup where hyperparameter tuning is challenging. The second plot of Fig. \ref{fig:ablation2} shows how the oversampling size (introduced in Step 3 of Sec. \ref{subsec:31}) affects the DG performance. %It can be seen that the the accuracy improves as oversampling size increases, by providing better opportunity for the model to explore various novel styles. 
 StableFDG still outperforms the baseline  with minimal oversampling size,
 indicating that other components of our solution (style sharing/shifting and attention-based components) are already strong enough. The size of oversampling can be determined depending on the clients' computation/memory constraints, with the cost of improved generalization.    

%\textbf{Effect of exploration level $\alpha$.} In the first plot of Fig. \ref{fig:ablation2}, we study the effect of exploration level $\alpha$ introduced in   \eqref{alpha12}, which was set to 3 for all experiments. We vary   $\alpha$ from 0 to 7 in a single-domain setup using PACS.   If $\alpha$ is too small, the performance is relatively low since the model is not able to explore novel styles beyond the client's source domains. It is also important to note that if $\alpha$ is too large,   the performance could be slightly degraded because the model would explore too redundant/unnecessary styles. The overall results show that the performance of StableFDG is not sensitive to $\alpha$ if they are chosen in an appropriate range.   Moreover, StableFDG still performs better than the baselines with a coarsely chosen hyperparameter $\alpha$.  

 

%  \textbf{Effect of   oversampling size.} In Sec. \ref{subsec:31}, we proposed feature-level oversampling and applied novel styles to those oversampled features.  In the second plot of Fig. \ref{fig:ablation2}, we observe how the oversampling size affects the DG performance. It can be seen that the the accuracy improves as oversampling size increases, by providing better opportunity for the model to explore various novel styles. More importantly, the proposed method still outperforms existing baselines with minimal oversampling size, indicating that other components of our solution (style sharing/shifting and attention-based weighted averaging) are already strong enough. The size of oversampling can be determined depending on the computation and memory constraints of each client with the cost for improved generalization.    Additional results on computational cost analysis are provided in Supplementary  Material.



%  \begin{wrapfigure}{r}
%   \begin{subfigure}[b]{0.235\textwidth} 
%  \centering
%           \includegraphics[width=\textwidth]{Single_domain_various_alpha.eps}
%  \caption{Effect of exploration level $\alpha$}
%\label{fig:effect_alpha}
%  \end{subfigure}
%  \hfill
%  \begin{subfigure}[b]{0.235\textwidth} 
%  \centering
%           \includegraphics[width=\textwidth]{Single_domain_oversampling_size.eps}
%  \caption{Effect of oversampling size}
%\label{fig:effect_oversample}
%  \end{subfigure}        
%  \vspace{-3mm}
%  \caption{ \small \textbf{Ablation study 2 (Varying parameters):} Effect of exploration level $\alpha$ and oversampling size in a single-domain data distribution scenario. StableFDG is competitive even with coarsely chosen $\alpha$ and minimal oversampling size.}
%    \label{fig:ablation2}
%    \vspace{-1mm}
%\end{wrapfigure}
 
 
 
 
\begin{table*}[ht]
%\vspace{9mm}
	\begin{subtable}[c]{0.5\linewidth}
	\tiny
	%\centering
	\begin{tabular}{l|    cccc | c}
		\toprule  
		Methods   &Art  & Cartoon & Photo & Sketch & Avg. \\ 
		\midrule		
		MixStyle \cite{zhou2021domain}& 81.61&78.83&96.77&72.29&82.38 \\
		DSU  \cite{li2022uncertainty}&78.84&79.56&95.21&79.39&83.25   \\
	   \textbf{StableFDG}  (ours)&85.02&79.65&96.38&78.35& \textbf{\underline{84.85}}   \\
		\bottomrule
	\end{tabular}
		 %\vspace{-2mm}
		\caption{Performance in a centralized DG setup.}
		%\vspace{-3mm}
\label{table:ablation_centralized}
\end{subtable}  
\begin{subtable}[c]{0.5\linewidth}
%\vspace{-3.5mm}
\tiny
%\centering
	\begin{tabular}{l|    cccc | c}
		\toprule  
		Methods   &Art  & Cartoon & Photo & Sketch & Avg. \\ 
		\midrule		
		MixStyle \cite{zhou2021domain}& 87.51&81.12&97.48&68.39&83.63  \\
		
		DSU  \cite{li2022uncertainty}&86.48&81.22&97.21&73.99 &84.73  \\
	   \textbf{StableFDG}  (ours)&90.01&83.29&98.02&79.47& \textbf{\underline{87.70}}   \\
		\bottomrule
	\end{tabular}
			% \vspace{-1mm}
\caption{Performance using ResNet-50. } 
\label{table:resnet}
		%\vspace{-3mm}
\end{subtable}
\caption{The applicability of StableFDG in a centralized DG setup (Table \ref{table:ablation_centralized}) and performance using a larger model (Table \ref{table:resnet}) on the PACS dataset.}
 \vspace{-1mm}
\end{table*} 
 
\textbf{Performance   in a centralized setup.}  Although our scheme is tailored to  federated DG, the ideas of StableFDG can be also utilized in a centralized   setup. In Table \ref{table:ablation_centralized}, we study the effects of our style and attention based strategies   in a centralized DG setting using PACS,   while the other settings are the same as in the FL setup. The results demonstrate that the proposed ideas are not  only specific to data-poor FL scenarios but also have potentials to be utilized in centralized DG settings.%..........\textcolor{blue}{To be updated - DJ}
%   
%




\textbf{Performance with ResNet-50.} In Table \ref{table:resnet}, we also conduct experiments using ResNet-50 on PACS  dataset in the  multi-domain data distribution scenario. Other settings are exactly the same as in Table \ref{table:main2}. The results further confirm the advantage of StableFDG with larger models.
 \vspace{-0.4mm}

\begin{wrapfigure}{r}{0.47\textwidth}
 \vspace{-3.5mm}
  \centering
  \includegraphics[width=0.11\textwidth]{attentionvis/6originalimage.jpg}
\includegraphics[width=0.11\textwidth]{attentionvis/6attentionmap.jpg}
  \includegraphics[width=0.11\textwidth]{attentionvis/14ori_image.jpg}
\includegraphics[width=0.11\textwidth]{attentionvis/14attentionmap.jpg}
  \vspace{-1mm}
  \caption{Visualization of    attention score maps of each input image (left: bed, right: radio).}
  \label{fig:attenscore}
  \vspace{-4mm}
\end{wrapfigure}
 \textbf{Attention score visualization.} To gain an intuitive understanding of the effect of our attention-based learning, in Fig. \ref{fig:attenscore}, we visualize the   score maps obtained via our attention module at testing. The   score maps are interpolated so that it has the same size as the original image. A warmer color indicates a higher value. It can be seen  that our attention module highlights important parts of each class even in the presence of  unrelated backgrounds.

 
\textbf{Additional results.} Other implementation details, comprehensive ablation studies for each component,  discussions on complexity, and results on DomainNet dataset are in Appendix.  




%\begin{table}[!t]
%\scriptsize
%\centering
%\begin{subtable}[!t]{1\linewidth}
%\centering
%	\begin{tabular}{l|    cccc | c}
%		\toprule  
%		Methods   &Art  & Cartoon & Photo & Sketch & Avg. \\ 
%		\midrule		
%		MixStyle \cite{zhou2021domain}& 81.61&78.83&96.77&72.29&82.38 \\
%		
%		DSU  \cite{li2022uncertainty}&78.84&79.56&95.21&79.39&83.25   \\
%	   \textbf{StableFDG}  (ours)&85.02&79.65&96.38&78.35& \textbf{\underline{84.85}}   \\
%		\bottomrule
%	\end{tabular}
%	\vspace{-0.5mm}
%\caption{PACS}
%\end{subtable}
%\hfill
%\newline
%\begin{subtable}[!t]{1\linewidth} 
%\centering
%	\begin{tabular}{l|    cccc | c}
%		\toprule  
%		Methods    & Art & Clipart	& Product & Real  & Avg. \\ 
%		\midrule
%		
%		MixStyle \cite{zhou2021domain}&58.61&53.05&73.22&75.77 &65.15 \\  
%		
%		DSU  \cite{li2022uncertainty}&58.52&55.08&73.41&74.29&65.33 \\
% 
%	   \textbf{StableFDG}  (ours)&57.78&56.62&74.01&76.19& \textbf{\underline{66.15}}   \\
%		\bottomrule
%	\end{tabular}
%		\vspace{-0.5mm}
%\caption{Office-Home}
%\end{subtable}
%\vspace{-3mm} 
%\caption{\textbf{Ablation study 3:} Performance comparison in a centralized setup. The style and attention based ideas of StableFDG are also effective in a centralized DG setup. } 
%\label{table:ablation_centralized}
%\end{table} 
%
%\begin{table}[!t]
%\scriptsize
%\centering
%	\begin{tabular}{l|    cccc | c}
%		\toprule  
%		Methods   &Art  & Cartoon & Photo & Sketch & Avg. \\ 
%		\midrule		
%		MixStyle \cite{zhou2021domain}& 87.51&81.12&97.48&68.39&83.63  \\
%		
%		DSU  \cite{li2022uncertainty}&86.48&81.22&97.21&73.99 &84.73  \\
%	   \textbf{StableFDG}  (ours)&90.01&83.29&98.02&79.47& \textbf{\underline{87.70}}   \\
%		\bottomrule
%	\end{tabular}
%\caption{\textbf{Ablation study 4:} Performance using ResNet-50 on PACS. Our StableFDG also achieves high performance. } 
%\label{table:resnet}
%\end{table} 
\vspace{-1mm}
\section{Conclusion}
 \vspace{-1mm}
 
 
Despite the practical significance, the field of federated domain generalization is still in the early stage of research. In this paper, we proposed StableFDG, a new training strategy tailored to this unexplored area. Our style-based strategy enables the model to get exposed to various novel styles beyond each client's source domains, while our attention-based method captures and emphasizes the important/common characteristics of each class. Extensive experimental results confirmed the advantage of our StableFDG for federated domain generalization with data-poor FL clients.

  \vspace{-0.5mm}

 
 \textbf{Limitations and future works.}  StableFDG requires 0.45 \% more communication load compared to FedAvg for sharing the attention module and style statistics, which is the cost for  a better DG   performance. 
 %As can be seen in Supplementary Material, albeit small, StableFDG requires more computation   compared to the baselines, which is the cost for achieving a better DG   performance. 
 Further developing our idea to tailor to centralized DG and extending our attention
strategy  to segmentation/detection DG tasks are also interesting directions for future research.%Further developing our attention-based strategy to be tailored to semantic segmentation and object detection DG tasks are also interesting   directions, which we leave for a future work.%Further developing our idea to be tailored to centralized DG and extending our attention  strategy to segmentation/detection DG tasks are also interesting directions for future research.%, which we leave for   future works.
 
\section*{Acknowledgments}
This work was supported by the National Research Foundation of Korea (NRF) grant funded by the Korea government (MSIT) (No. NRF-2019R1I1A2A02061135), by IITP funds from MSIT of Korea (No. 2020-0-00626),  by the National Science Foundation (NSF) under grants CNS-2146171 and CPS-2313109, and by the Defense Advanced Research Projects Agency (DARPA) under grant D22AP00168-00.








%This work was supported by the National Research Foundation of Korea (NRF) grant funded by the Korea government (MSIT) (No. NRF-2019R1I1A2A02061135), IITP funds from MSIT of Korea (No. 2020-0-00626), National Science Foundation (NSF) CNS-2146171, CPS-2313109, and Defense Advanced Research Projects Agency (DARPA)  D22AP00168-00.
 
%Despite the practical significance, the field of federated domain generalization is still in the early stage of research. In this paper, we proposed StableFDG, a new training strategy tailored to this unexplored area. Our style-based strategy enables the model to get exposed to various novel styles beyond each client's source domains, while our attention-based method captures and emphasizes the important/common characteristics of each class. Extensive experimental results confirmed the advantage of our StableFDG for federated domain generalization with data-poor FL clients.  %\textcolor{red}{We also showed the potential of the proposed ideas in a centralized setup, providing  new directions to both DG and FL communities. Further developing our idea to be tailored to centralized DG is an interesting direction, which we leave for a future work.}%We  leave further developing our idea to a  centralized DG setup as a future work.% \textcolor{blue}{One interesting and important future research direction is to develop our ideas more specific to the centralized DG setup,  }

%\nocite{langley00}

%\bibliography{FedDG_references_neurips.bib}


\bibliographystyle{plain}
\bibliography{FedDG_references_neurips.bib}


\newpage

\appendix

\section{Results on DomainNet Dataset}
To demonstrate the effectiveness of our StableFDG on a larger dataset, we performed experiments on DomainNet dataset in a single-domain data distribution scenario. To this end, we utilize ResNet-50 pretrained on ImageNet. The number of global rounds and mini-batch size are set to 60 and 64, respectively. The remaining settings are the same as those in our main paper. The results in Table \ref{table:domainnet} show that our StableFDG consistently outperforms not only the centralized DG works but also the prior works on federated DG even with a more complex dataset.

\begin{table}[!h]
\small
\centering
	\begin{tabular}{l|    cccccc | c}
		\toprule  
		Methods   &Clipart  & Inforgraph & Painting & Quickdraw &Real &Sketch & Avg. \\ 
		\midrule	
		FedAvg  \cite{mcmahan2017communication}  &61.52 &24.75 &50.83&12.08&60.00 &49.92& 43.18\\
		FedBN \cite{lifedbn} &60.23& 24.40& 50.43& 11.99& 59.57& 49.66& 42.71  \\	
		MixStyle  \cite{zhou2021domain}  & 61.39& 24.33& 51.41& 13.07& 57.90& 51.40& 43.25 \\
		DSU  \cite{li2022uncertainty}   & 62.43& 24.30& 51.87& 13.65& 58.75& 52.40& 43.90  \\
		FedDG  \cite{liu2021feddg}    &62.49& 23.71& 48.42& 12.45& 61.44& 49.11& 42.94  \\
		FedSR  \cite{nguyen2022fedsr}  & 61.91& 25.37& 50.54& 11.59& 62.03& 50.13& 43.60  \\
		\textbf{StableFDG} & 62.58& 24.12& 52.23& 14.87& 60.60& 52.50& \textbf{\underline{44.48}}   \\
		\bottomrule
	\end{tabular}
\vspace{+3mm}
\caption{Results on DomainNet dataset in a single-domain data distribution scenario. } 
\label{table:domainnet}
\end{table} 



\section{Discussion on Computational and Communication Costs}
Table \ref{table:complexity} compares the communication, computation, and average accuracy  of different schemes on PACS in a multi-domain data distribution scenario. ResNet-18 is adopted as in our main manuscript. We first compare the uplink communication load of each client in a specific global round. Compared to FedAvg that only transmits the model in each round, our scheme requires additional communication burden for transmitting the style statistics and the attention module, which are negligible. We also compare the computation time by measuring the time required for local update at each client using an GTX 1080 Ti GPU. CCST \cite{chen2023federated} and FedDG \cite{liu2021feddg} require large  computation due to the increased amounts of data samples or multiple backpropagations for meta training. Our scheme requires additional computation caused by style exploration, attention module update, etc., which are the costs for better generalization to the unseen domain.


\begin{table}[h]
\small
\centering
	\begin{tabular}{l|    ccc c}
		\toprule  
		Methods   &Communication load &Computation time &Achievable average accuracy \\ 
		\midrule	
		FedAvg \cite{mcmahan2017communication} & 44.98 MB & 4.57 sec & 77.02 \% \\ 
		CCST \cite{chen2023federated} & 44.98 MB & 9.03 sec & 79.00 \% \\
		FedDG  \cite{liu2021feddg}  &44.98 MB & 22.44 sec &  76.98 \%   \\
	   	FedSR  \cite{nguyen2022fedsr} & 44.98 MB & 4.59 sec & 77.56 \% \\
		StableFDG &45.00 MB & 7.39 sec & \textbf{\underline{83.75}} \% \\
		\bottomrule
	\end{tabular}
\vspace{+3mm}
\caption{Computation and communication cost comparison. } 
\label{table:complexity}
\end{table} 




\section{Experiments with Three Clients}
Different from the prior works \cite{chen2023federated, nguyen2022fedsr} for federated DG adopting the usual setting where the number of clients equals the number of source domains, in our main paper, we introduce a more practical experimental setting for federated DG where the source data is distributed to more clients than the number of source domains. For a comparison with them in the same setting, we also provide additional experimental results in the setup with \textit{number of clients = number of source domains}. Table \ref{table:threeclients} shows the results on PACS and Office-Home datasets with three clients in a single-domain data distribution scenario. It is confirmed from the results that our StableFDG also achieves better performance compared to the existing works \cite{chen2023federated, nguyen2022fedsr} in this simple setting.


\begin{table*}[!h]
 \centering
\begin{subtable}[!h]{1\linewidth}
	\small
	\centering
	\begin{tabular}{c||    cccc | c}
		\toprule  
		Methods      &Art  & Cartoon & Photo & Sketch & Avg. \\ 
		\midrule
	CCST \cite{chen2023federated} & 81.25 &73.34 &	95.21 &	80.27 &82.52 \\
FedSR  \cite{nguyen2022fedsr}    &83.20 &76.00&93.80 &81.90 &	83.70\\
	   \textbf{StableFDG}  & 83.01&79.31 &94.85 &	79.76 &\textbf{\underline{84.23}} \\
		\bottomrule
	\end{tabular}
		\caption{Results on PACS dataset.}
\end{subtable}
\hfill
\newline
\begin{subtable}[h]{1\linewidth} 
	\small
\centering
	\begin{tabular}{c||    cccc | c}
		\toprule  
		Methods      &Art  & Clipart & Product & Real& Avg. \\ 
		\midrule
	CCST \cite{chen2023federated} & 59.05 &50.06 &	72.97 &	71.67 &63.56\\
	   FedSR  \cite{nguyen2022fedsr}   & 57.93 &50.45 &	73.33&	75.51 &64.31\\
	   \textbf{StableFDG}  & 57.19	&57.94 &72.76 &72.16&\textbf{\underline{65.01}} \\
		\bottomrule
	\end{tabular}
\caption{Results on Office-Home dataset.}
\end{subtable}
\caption{Results in a single-domain data distribution scenario with three clients.}
\label{table:threeclients}
\end{table*}



\section{Effect of Each Component of StableFDG}
As mentioned in the main manuscript, we provide further ablation studies on the effect of each component in StableFDG using VLCS dataset. Table \ref{table:eachcomp} shows that each component individually brings performance gain compared to FedAvg. Using both strategies achieves greater performance gains, confirming that the two proposed schemes work in a complementary fashion.

\begin{table}[h]
\small
\centering
	\begin{tabular}{l|    cccc | c}
		\toprule  
		Methods   &Caltech  & Labelme & Pascal & Sun & Avg. \\ 
		\midrule	
		FedAvg \cite{mcmahan2017communication}   & 93.65 $\pm$ 1.59 & 61.10 $\pm$  2.08 & 72.55 $\pm$ 0.90 & 65.40 $\pm$ 0.28  & 73.18 $\pm$  0.27 \\ 
		%Mixstyle \cite{zhou2021domain}  &95.20 &60.77 &73.90 &66.73&74.15\\	
		%DSU \cite{li2022uncertainty} &96.87 &60.23&72.97 &68.97&74.76\\	
		StableFDG (only style) & 98.19 $\pm$  0.81 &58.93 $\pm$ 1.16 &75.19 $\pm$  0.67 &68.69 $\pm$  0.73 & 75.25 $\pm$  0.20 \\
		StableFDG (only attetnion) & 94.10 $\pm$  1.22 &62.02 $\pm$  1.55 &72.26 $\pm$  0.78 &66.16 $\pm$  2.58 &73.64 $\pm$  0.89 \\
		StableFDG (both) & 98.50 $\pm$ 0.17 &60.07 $\pm$  0.79 &74.40 $\pm$  1.88 &69.43 $\pm$  1.11 & \textbf{\underline{75.61}} $\pm$  0.71  \\
		\bottomrule
	\end{tabular}
\vspace{+3mm}
\caption{Effect of each component on VLCS dataset in a multi-domain data distribution scenario. The reported results indicate (mean $\pm$ 95\% confidence interval) over 3 random trials.} 
\label{table:eachcomp}
\end{table} 



\section{Ablation Studies on Style-Based Learning}

\subsection{Randomness in style sharing}
In our style based learning, style sharing among clients is performed at random. However, one can think of the strategy where client $n$  receives the style information $\Phi_{n'}$  that has the largest distance with its own style information $\Phi_{n'}$ in the style space.  Table \ref{table:largedistance} shows the corresponding result using PACS dataset in a multi-domain data distribution setup. Interestingly, it can be seen that the random selection adopted in this paper performs better, since most of the users generally tend to receive the same style statistics when using the \textit{largest distance} strategy.


\begin{table}[!h]
\scriptsize
\centering
	\begin{tabular}{l|    cccc | c}
		\toprule  
		Methods   &Art  & Cartoon & Photo & Sketch & Avg. \\ 
		\midrule	
		Random sharing (current manuscript)  &83.97 $\pm$ 1.25 & 79.10 $\pm$ 0.45 & 96.27 $\pm$ 0.36 & 75.67 $\pm$ 0.58 & 83.75 $\pm$ 0.23 \\	
		Large distance & 83.01 $\pm$ 1.43 & 78.33 $\pm$ 0.51 & 96.19 $\pm$ 0.76 & 74.63 $\pm$ 0.65 & 83.04 $\pm$ 0.30  \\
		\bottomrule
	\end{tabular}
\vspace{+3mm}
\caption{Random sharing vs receiving the style that has the largest distance in style space. The reported results indicate (mean $\pm$ 95\% confidence interval) over 3 random trials.} 
\label{table:largedistance}
\end{table}  

\subsection{Effect of the number of shared styles}
We also compare the effect of number of styles received at each client in Table \ref{table:numsharestyles} using PACS dataset in a multi-domain data distribution setup. The performance increases as the number of received styles increases, with small additional communication load (the vector length of the style information is 128, which is negligible compared to the number of model parameters, which is 11,180,103).

\begin{table}[!h]
\scriptsize
\centering
	\begin{tabular}{l|    cccc | c}
		\toprule  
		Methods   &Art  & Cartoon & Photo & Sketch & Avg. \\ 
		\midrule	
		No style sharing &83.16 $\pm$ 0.18 & 78.54 $\pm$ 1.11 & 95.56 $\pm$ 0.92 & 74.64 $\pm$ 1.06 & 82.97 $\pm$ 0.36 \\	
		Receive 1 style (current manuscript) &83.97 $\pm$ 1.25 & 79.10 $\pm$ 0.45 & 96.27 $\pm$ 0.36 & 75.67 $\pm$ 0.58 & 83.75 $\pm$ 0.23 \\
		Receive 3 styles & 84.55 $\pm$ 1.20 & 78.67 $\pm$ 0.59 & 95.75 $\pm$ 0.58 & 76.77 $\pm$  0.27 & 83.94 $\pm$  0.07 \\
		\bottomrule
	\end{tabular}
\vspace{+3mm}
\caption{Effect of the number of shared styles. The reported results indicate (mean $\pm$ 95\% confidence interval) over 3 random trials. } 
\label{table:numsharestyles}
\end{table}

\subsection{Effect of $k$-means++ in style shifting}
In Section 3.1, we utilized the $k$-means++ as a tool for facilitating our key idea in style shifting, which is to effectively balance between the original source domain and the new source domain for better generalization; k-means++ plays a role to select the $B/2$ styles that are similar to the remaining $B/2$ styles in the mini-batch. By doing so, the model can explore new styles while not losing the performance on the original styles. To see this effect, we compare $k$-means++ vs. random sampling when selecting B/2 samples to be shifted, in Table \ref{table:effectkmeans}. The results show that strategically selecting the B/2 samples to be shifted achieves better performance especially in the Sketch domain (1.89\% gain) that has a large style gap with other domains. We believe that these results motivate and justify our design choice.

\begin{table}[!h]
\small
\centering
	\begin{tabular}{l|    cccc | c}
		\toprule  
		Methods   &Art  & Cartoon & Photo & Sketch & Avg. \\ 
		\midrule	
		Shifting $B/2$ random styles &83.69& 79.61& 95.99& 73.78& 83.27\\	
		Shifting $B/2$ styles via $k$-means++ &83.97& 79.10& 96.27& 75.67& \textbf{\underline{83.75}}  \\
		\bottomrule
	\end{tabular}
\vspace{+3mm}
\caption{Effect of $k$-means++ in style shifting  on  PACS dataset. } 
\label{table:effectkmeans}
\end{table} 



\subsection{Effect of class-balanced oversampling}
In our main paper, we performed the class-balanced oversampling in the feature space to alleviate the class-imbalance issue   during style exploration. To confirm the effectiveness of the class-balanced oversampling, we compare it with random oversampling under the same condition where only the style exploration is applied without other components. Table \ref{table:sm2} shows the results on Office-Home dataset, where the class distribution is highly imbalanced in a multi-domain data distribution scenario. It can be seen that our class-balanced oversampling achieves higher performance over the simple random sampling, which validates the efficacy of mitigating the class imbalance problem in FL clients.

\begin{table}[!h]
\small
\centering
	\begin{tabular}{l|    cccc | c}
		\toprule  
		Methods   &Art  & Cartoon & Photo & Sketch & Avg. \\ 
		\midrule	
		Random oversampling  &56.21&54.43&69.37&72.29 &63.08  \\	
	 Class-balanced oversampling & 57.20&53.15&71.2&74.23& \textbf{\underline{64.10}}  \\
		\bottomrule
	\end{tabular}
\vspace{+3mm}
\caption{Effect of the class-balanced oversampling on Office-Home dataset in a multi-domain data distribution scenario. } 
\label{table:sm2}
\end{table}  

\subsection{Effect of the operation probability   in  style-based learning}
We conduct additional ablation studies on the probability value (defined as $p$ here) utilized to control the operation of the style sharing/shifting and style exploration modules. Larger  $p$ means that our scheme is more likely  to be activated.  In Table \ref{table:sm0}, we provide results on various probability values in a single-domain data distribution scenario using PACS dataset. From the results, it is confirm that for all $p$ values, the proposed StableFDG outperforms   existing baselines, demonstrating that our scheme can work well with an arbitrarily chosen probability $p$. In detail, when $p$ ranges from 0.3 to 0.7, the high performance is maintained while the performance decreases at both extreme probabilities ($p=0.1$ and 0.9). Therefore, it is recommended for practitioners to select the $p$ in an appropriate range, avoiding extreme cases.

\begin{table}[!h]
\small
\centering
	\begin{tabular}{l|    cccc | c}
		\toprule  
		Methods   &Art  & Cartoon & Photo & Sketch & Avg. \\ 
		\midrule		
		MixStyle \cite{zhou2021domain}   & 79.10&76.30&90.10&60.63&76.53  \\
		DSU  \cite{li2022uncertainty} & 80.43	&75.70	&92.60&	69.87&	79.65 \\
		\textbf{StableFDG} ($p=0.1$)  &82.70&78.30&95.30&75.30&82.90  \\
	   	\textbf{StableFDG} ($p=0.3$)  & 84.50&79.50&96.00&75.70&  83.93   \\
		\textbf{StableFDG} ($p=0.5$)  & 83.10&79.50&96.40&76.00&83.75  \\
		\textbf{StableFDG} ($p=0.7$)  & 82.40&78.10&95.70&76.30&83.13  \\
		\textbf{StableFDG} ($p=0.9$)  & 81.40&78.80&95.90&73.60&82.70 \\
		\bottomrule
	\end{tabular}
\vspace{+3mm}
\caption{Effect of the probability value in our style-based learning on PACS in a single-domain data distribution scenario.} 
\label{table:sm0}
\end{table}

\subsection{Where to apply the style module}

For implementation, style-based learning is applied only in the 1st, 2nd, 3rd blocks among 4 residual blocks in ResNet-18. Note that at the output of the 4th block, label information is dominant rather than style information, which results in degraded performance when style-based schemes are applied. This is confirmed by our new experiments in the table below. It can be seen from the results that if we consider the 4th residual block to apply our style-based learning, the performance gets degraded. This result confirms the intuition that style-based learning should be conducted at the earlier layers where style information is preserved.


\begin{table}[!h]
\small
\centering
	\begin{tabular}{l|    cccc | c}
		\toprule  
		Methods   &Art  & Cartoon & Photo & Sketch & Avg. \\ 
		\midrule		
		Style exploration at 1st, 2nd, 3rd layers (main manuscript)  &84.10 &78.57 &95.40&72.73&82.70\\
	   	Style exploration at 1st, 2nd, 3rd, 4th layers  & 82.99 &78.54 &94.13 &73.35&  82.25\\
		\bottomrule
	\end{tabular}
\vspace{+3mm}
\caption{Ablation experiments on applying style-based learning at different layers (PACS dataset).} 
\label{table:ablationdifl}
\end{table}






\section{Ablation Studies on Attention-based Feature Highlighter}

\subsection{Effect of adopting both cross-attention and self-attention}
In our main paper, the similarity metric in equation \eqref{eq:sim} adopts cross-attention, while the metric in equation \eqref{eq:mix} combines cross-attention and self-attention. When applying only the cross-attention-based metric in equation \eqref{eq:sim}, we found that the similarity value could become low even when the two samples belong to the same class, in special cases. We handled this issue by adding the self-attention component as in equation  \eqref{eq:mix}. Intuitively, by doing this, the attention module is learning to extract the important features across images (via cross-attention), and within the image (via self-attention). Table \ref{table:crossself}  compares the performance of our StableFDG when using (i) self-attention alone, (ii) cross-attention alone (equation \eqref{eq:sim}),  and (iii) both self and cross attentions at the same time (equation  \eqref{eq:mix}),  confirming the advantage of using self-attention and cross-attention together. 


\begin{table}[h]
\scriptsize
\centering
	\begin{tabular}{l|    cccccc | c}
		\toprule  
		Methods   &Clipart & Infograph& Painting &Quickdraw & Real & Sketch & Avg. \\ 
		\midrule	
				StableFDG (with self-attention alone) &61.77	&24.88&	48.28	&14.15&	59.78&	52.41&	43.55 \\
				StableFDG (with cross-attention alone) &61.24& 24.97& 50.55& 14.67& 61.12 &50.44 & 43.83 \\
		StableFDG (with self + cross) & 62.58& 24.12& 52.23& 14.87 & 60.60 & 52.50 & \textbf{\underline{44.48}}  \\
		\bottomrule
	\end{tabular}
\vspace{+3mm}
\caption{Effects of similarity metrics using DomainNet dataset in a multi-domain data distribution scenario.} 
\label{table:crossself}
\end{table} 




%\begin{table}[!h]
%\small
%\centering
%	\begin{tabular}{l|    cccc | c}
%		\toprule  
%		Methods   &Art  & Cartoon & Photo & Sketch & Avg. \\ 
%		\midrule		
%		StableFDG (similarity metric in equation \eqref{eq:sim})  &83.09&78.60&95.87&74.71&83.07\\
%	   	StableFDG (similarity metric in equation  \eqref{eq:mix})  & 83.97&79.10&96.27&75.67&  \textbf{\underline{83.75}} \\
%		\bottomrule
%	\end{tabular}
%\vspace{+3mm}
%\caption{Effect of simlarity metrics using PACS dataset in a multi-domain data distribution scenario.} 
%\label{table:simmetric}
%\end{table}

%\subsection{\textcolor{red}{Comparison of cross + self-attention and self-attention in the attention module }}
%
%Then, the next key question is: Can the cross-attention strategy effectively capture eseential common information across two different images from the same class?  To answer this question, we compare our attention strategy (cross + self-attention) with the naive self-attention strategy when performing the similarity function (equation (6)) of our attention module during training. It can be seen from the table below that considering cross attention together improves the performance compared to the scheme that uses self-attention alone. This indicates that different samples in the same class indeed share some common characteristics regardless of the domain, confirming the advantage of adopting cross-attention to learn domain-invariant features. By taking advantage of both cross and self attention, StableFDG enables the model to extract and learn important characteristics across images (via cross-attention), and within the image (via self-attention).
%
%\begin{table}[h]
%\scriptsize
%\centering
%	\begin{tabular}{l|    cccccc | c}
%		\toprule  
%		Methods   &Clipart & Infograph& Painting &Quickdraw & Real & Sketch & Avg. \\ 
%		\midrule	
%		StableFDG (with self-attention alone) &61.77& 24.88& 48.28& 14.15& 59.78 &52.41 & 43.55 \\
%		StableFDG (with self + cross) & 62.58& 24.12& 52.23& 14.87 & 60.60 & 52.50 & \textbf{\underline{44.48}}  \\
%		
%		\bottomrule
%	\end{tabular}
%\vspace{+3mm}
%\caption{Using cross + self attention together vs. using self attention alone (DomainNet dataset). } 
%\label{table:crossself}
%\end{table} 



\subsection{Comparison using the same model size}
Our attention module requires 0.44\% of additional model parameters to perform the attention-based learning. For a fair comparison to see the effect of our attention-based learning, we consider a different baseline with the same model size but without attention-based learning. Specifically, the baseline computes the attention score map using only additional convolutional operations and take the weighted average of the feature $z_i$ based on the attention score map. Table \ref{table:ablationattention} shows the results using PACS dataset in a single-domain data distribution scenario. The results demonstrate that our attention-based learning achieves performance improvements on all four domains while playing a key role in capturing essential parts of the features.


\begin{table}[h]
\small
\centering
	\begin{tabular}{l|    cccc | c}
		\toprule  
		Methods   &Art  & Cartoon & Photo & Sketch & Avg. \\ 
		\midrule	
		Baseline (same model size) &78.88& 69.03& 91.74& 60.77& 75.11  \\
		Attention-based learning & 79.54& 72.48& 92.51& 64.71& \textbf{\underline{77.31}}  \\
		
		\bottomrule
	\end{tabular}
\vspace{+3mm}
\caption{Ablation study on attention-based learning using PACS dataset in a single-domain data distribution scenario. } 
\label{table:ablationattention}
\end{table} 


\subsection{Effect of attention in a centralized setup}

%In FL, each device typically has a limited number of samples, and thus models are prone to overfitting to each local dataset during local updates. Our attention strategy effectively learns domain-invariant features in such data-poor FL setups while mitigating the risk of overfitting to irrelevant feature information unrelated to each class. This is achieved by extracting common characteristics of samples in the same class and emphasizing them, while removing unimportant parts (e.g. background noises) that cause overfitting. Table 3 in the main paper shows that applying our attention scheme alone already outperforms baselines.

Now we provide answer to the following question: Instead of the FL setup we focused on, can attention provide benefits in the centralized DG setup? Table \ref{table:cenattn}  shows the results with/without attention module in a centralized setup using PACS dataset. The results show that attention still provides performance improvements in the centralized setup by learning domain-invariant features, although the gain is slightly lower than the gain in the FL setup as shown in Table 3 of the main manuscript. These results indicate that the proposed attention-based learning indeed captures the domain-invariant characteristics of samples, while the scheme provides more benefits in the FL setup where each client is prone to overfitting due to lack of data.


\begin{table}[h]
\small
\centering
	\begin{tabular}{l|    cccc | c}
		\toprule  
		Methods   &Art  & Cartoon & Photo & Sketch & Avg. \\ 
		\midrule	
		StableFDG (centralized setup, without attention)	 &84.15 & 79.45 & 96.21 & 77.09& 84.23\\
		StableFDG (centralized setup, with attention)	& 85.02 & 79.65 & 96.38 & 78.45& 84.88  \\
		
		\bottomrule
	\end{tabular}
\vspace{+3mm}
\caption{Effect of proposed attention-based feature highlighter in a centralized DG setup using PACS dataset. } 
\label{table:cenattn}
\end{table} 






\section{Other Implementation Details}
Our code is built upon the official code of \cite{zhou2021domain} and \cite{bello2019attention}. During the local update process, we use SGD as an optimizer  with a momentum of  0.9 and a weight decay of $5e^{-4}$. For PACS, Office-Home and VLCS, the learning rate is set to 0.001 and the cosine annealing is used as a scheduler. For Digits-DG, we set the learning rate to 0.02 and the learning rate is decayed by 0.1 every 20 steps.  For our attention-based feature highlighter, at least two samples are required to be in the mini-batch of every client. When this condition is not met, additional samples are extracted from the corresponding client's local dataset to facilitate cross-attention.


\textbf{More detailed description on oversampling:} Let $s^n\in \mathbb{R}^{B \times C \times H \times W}$ be a mini-batch of features in client $n$ at a specific layer,  obtained after   Steps 1 and 2 in the main manuscript.     Now given a fixed oversampling size, we oversample the features in the mini-batch to obtain  $\tilde{s}^n$, so that the concatenated mini-batch $\hat{s}^n =  [s^n, \tilde{s}^n ]$ becomes class-balanced as much as possible. Consider a toy example where the number of samples for classes  $a$, $b$, $c$ are 3, 2, 1, respectively in the mini-batch $s^n$. In this example, if the oversampling size is 3, we randomly choose   one data point from class $b$
 and two data points from class  $c$ (in this case, the same data point is selected for two times with duplication) to obtain $ \tilde{s}^n$, so that the concatenated mini-batch $\hat{s}^n =  [s^n, \tilde{s}^n ]$ becomes class-balanced. If the oversampling size is 1, we oversample one  data point in class $c$ to make the concatenated mini-batch to be balanced as much as possible. If the oversampling size is 6, we oversample 1, 2, 3 samples from classes  $a$, $b$, $c$, respectively to construct $\tilde{s}^n$.  The concatenated mini-batch $\hat{s}^n =  [s^n, \tilde{s}^n ]$ is utilized for    style-based learning and updating the model.  This  process not only mitigates the class-imbalance issue in each client but also provides a good platform for  style exploration by oversampling the  features. In our work, we reported the results with oversampling size of $B$ (which is  equal to the mini-batch size), while  the effect of the oversampling size is also reported in Fig. 4 of the main manuscript: A larger oversampling size leads to a better performance, and more importantly, our StableFDG outperforms the baseline even without any oversampling.

\section{Visualization of Attention Score Maps}
Finally in Fig. \ref{fig:attenscoresss}, we visualize the attention score maps of different input images during testing. The results confirm the effectiveness of our attention-based feature highlighter to focus on the important parts of each image from the unseen domain.


\begin{figure}[h]
   \centering
\includegraphics[width=0.11\textwidth]{attentionvis/2originalimage.jpg}
\includegraphics[width=0.11\textwidth]{attentionvis/2attentionmap.jpg}
\ \ \ \ \ \ 
  \includegraphics[width=0.11\textwidth]{attentionvis/12ori_image.jpg}
\includegraphics[width=0.11\textwidth]{attentionvis/12attentionmap.jpg}
\ \ \ \ \ \ 
\includegraphics[width=0.11\textwidth]{attentionvis/13ori_image.jpg}
\includegraphics[width=0.11\textwidth]{attentionvis/13attentionmap.jpg}
\ \ \ \ \ \   

\includegraphics[width=0.11\textwidth]{attentionvis/15ori_image.jpg}
\includegraphics[width=0.11\textwidth]{attentionvis/15attentionmap.jpg}
\ \ \ \ \ \ 
\includegraphics[width=0.11\textwidth]{attentionvis/18ori_image.jpg}
\includegraphics[width=0.11\textwidth]{attentionvis/18attentionmap.jpg}
\ \ \ \ \ \ 
\includegraphics[width=0.11\textwidth]{attentionvis/19ori_image.jpg}
\includegraphics[width=0.11\textwidth]{attentionvis/19attentionmap.jpg}
\ \ \ \ \ \ 
  \vspace{-1mm}
  \caption{Visualization of    attention score maps of each input image.}
  \label{fig:attenscoresss}
  \vspace{-2.5mm}
\end{figure}



\section{Pseudo Code Algorithm of Stable FDG}
Algorithm \ref{alg} summarizes the overall process of our StableFDG.
\begin{algorithm*}\caption{StableFDG }\label{alg}
\textbf{Input:} Initialized model $\mathbf{w}_0$, \ \textbf{Output:} Global model $\mathbf{w}_T$
%\textbf{Process at the Server} 
	\begin{algorithmic}[1]
		\FOR{each global round $t=0,1,..., T-1$ }
		\STATE
		\STATE \textbf{Stage 1.} Model download and style sharing
		\STATE The server samples a set of participating clients $M_t$ and sends the global model  $\mathbf{w}_t$ to the clients in $M_t$  
 		\FOR{each device $n \in M_t$}
			\STATE Compute   style information $\Phi_n$ according to the Step 1 in Sec. 3.1
			\STATE Transmit    $\Phi_n$ to the server
		\ENDFOR
		 	%	\STATE The server shares $\Phi_{n'}$ from $\{\Phi_{n}\}_{n \in M}$ to the clients in $M$ 
			\STATE The server shares $\{\Phi_{n}\}_{n \in M}$ to the clients in $M_t$ according to the Step 1 in Sec. 3.1
			%\STATE The server randomly samples $\Phi_{n'}$ from $\{\Phi_{n}\}_{n \in M}$  and sends it to each 
 		%\STATE $\#$ Style sharing at the server
		%\STATE Randomly sample a style information $\Phi_{n'}$ from $\{\Phi_{n}\}_{n \in M}$ and send this to each local device in $M$ 
		\STATE
		\STATE \textbf{Stage 2.} Local  updates and model aggregation		\FOR{each device $n \in M_t$}
			\FOR{local epoch = $1, 2...,E$}
			\STATE  (i) Style shifting according to the Step 2 in Sec.  3.1, 
				\STATE (ii) Style exploration  according to the Step 3 and 4 in Sec.  3.1
				\STATE  (iii) Attention based weighted averaging according to 	Sec. 3.2
			\STATE   Loss computation and model update
			\ENDFOR
			\STATE The server updates the global model by aggregating the client models 
			\ENDFOR
		
		\STATE
	\ENDFOR
	\end{algorithmic}
 %At each global round $t$, every device $k\in S_t$ receives $(\mathbf{w}_t, t)$ from the server and performs $E$ local updates to obtain $\mathbf{w}_t(k)$. Then each device $k$ sends $(\mathbf{w}_t(k), t)$ to the server.
%	\begin{algorithmic}[1]
%	\STATE At global round $t$, each device in $S_t$ receives $(\mathbf{w}_t, t)$ from the server and perform $E$ local updates of SGD to obtain $\mathbf{w}_t(k)$. Then each device transmits $(\mathbf{w}_t(k), t)$ back to the server
%\end{algorithmic}
\end{algorithm*}






%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\end{document}