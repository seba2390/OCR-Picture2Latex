%File: anonymous-submission-latex-2024.tex
\documentclass[letterpaper]{article} % DO NOT CHANGE THIS
\usepackage[]{aaai24}  % DO NOT CHANGE THIS
\usepackage{times}  % DO NOT CHANGE THIS
\usepackage{helvet}  % DO NOT CHANGE THIS
\usepackage{courier}  % DO NOT CHANGE THIS
\usepackage[hyphens]{url}  % DO NOT CHANGE THIS
\usepackage{graphicx} % DO NOT CHANGE THIS
\urlstyle{rm} % DO NOT CHANGE THIS
\def\UrlFont{\rm}  % DO NOT CHANGE THIS
\usepackage{natbib}  % DO NOT CHANGE THIS AND DO NOT ADD ANY OPTIONS TO IT
\usepackage{caption} % DO NOT CHANGE THIS AND DO NOT ADD ANY OPTIONS TO IT
\frenchspacing  % DO NOT CHANGE THIS
\setlength{\pdfpagewidth}{8.5in} % DO NOT CHANGE THIS
\setlength{\pdfpageheight}{11in} % DO NOT CHANGE THIS
%
% These are recommended to typeset algorithms but not required. See the subsubsection on algorithms. Remove them if you don't have algorithms in your paper.
\usepackage{algorithm}
\usepackage{algorithmic}

%
% These are are recommended to typeset listings but not required. See the subsubsection on listing. Remove this block if you don't have listings in your paper.
\usepackage{newfloat}
\usepackage{listings}
\DeclareCaptionStyle{ruled}{labelfont=normalfont,labelsep=colon,strut=off} % DO NOT CHANGE THIS
\lstset{%
	basicstyle={\footnotesize\ttfamily},% footnotesize acceptable for monospace
	numbers=left,numberstyle=\footnotesize,xleftmargin=2em,% show line numbers, remove this entire line if you don't want the numbers.
	aboveskip=0pt,belowskip=0pt,%
	showstringspaces=false,tabsize=2,breaklines=true}
\floatstyle{ruled}
\newfloat{listing}{tb}{lst}{}
\floatname{listing}{Listing}
%
% Keep the \pdfinfo as shown here. There's no need
% for you to add the /Title and /Author tags.
\pdfinfo{
/TemplateVersion (2024.1)
}

% DISALLOWED PACKAGES
% \usepackage{authblk} -- This package is specifically forbidden
% \usepackage{balance} -- This package is specifically forbidden
% \usepackage{color (if used in text)
% \usepackage{CJK} -- This package is specifically forbidden
% \usepackage{float} -- This package is specifically forbidden
% \usepackage{flushend} -- This package is specifically forbidden
% \usepackage{fontenc} -- This package is specifically forbidden
% \usepackage{fullpage} -- This package is specifically forbidden
% \usepackage{geometry} -- This package is specifically forbidden
% \usepackage{grffile} -- This package is specifically forbidden
% \usepackage{hyperref} -- This package is specifically forbidden
% \usepackage{navigator} -- This package is specifically forbidden
% (or any other package that embeds links such as navigator or hyperref)
% \indentfirst} -- This package is specifically forbidden
% \layout} -- This package is specifically forbidden
% \multicol} -- This package is specifically forbidden
% \nameref} -- This package is specifically forbidden
% \usepackage{savetrees} -- This package is specifically forbidden
% \usepackage{setspace} -- This package is specifically forbidden
% \usepackage{stfloats} -- This package is specifically forbidden
% \usepackage{tabu} -- This package is specifically forbidden
% \usepackage{titlesec} -- This package is specifically forbidden
% \usepackage{tocbibind} -- This package is specifically forbidden
% \usepackage{ulem} -- This package is specifically forbidden
% \usepackage{wrapfig} -- This package is specifically forbidden
% DISALLOWED COMMANDS
% \nocopyright -- Your paper will not be published if you use this command
% \addtolength -- This command may not be used
% \balance -- This command may not be used
% \baselinestretch -- Your paper will not be published if you use this command
% \clearpage -- No page breaks of any kind may be used for the final version of your paper
% \columnsep -- This command may not be used
% \newpage -- No page breaks of any kind may be used for the final version of your paper
% \pagebreak -- No page breaks of any kind may be used for the final version of your paperr
% \pagestyle -- This command may not be used
% \tiny -- This is not an acceptable font size.
% \vspace{- -- No negative value may be used in proximity of a caption, figure, table, section, subsection, subsubsection, or reference
% \vskip{- -- No negative value may be used to alter spacing above or below a caption, figure, table, section, subsection, subsubsection, or reference

\usepackage{float}
\usepackage{multirow}
\usepackage{amsmath} 
\usepackage{amssymb}
\usepackage{subcaption}

\setcounter{secnumdepth}{2} %May be changed to 1 or 2 if section numbers are desired.

% The file aaai24.sty is the style file for AAAI Press
% proceedings, working notes, and technical reports.
%

\title{Treat Different Negatives Differently: Enriching Loss Functions with Domain and Range Constraints for Link Prediction}
\author {
    % Authors
    Nicolas Hubert\textsuperscript{\rm 12}\thanks{Corresponding author: nicolas.hubert@univ-lorraine.fr},
    Pierre Monnin\textsuperscript{\rm 3},
    Armelle Brun\textsuperscript{\rm 1},
    Davy Monticolo\textsuperscript{\rm 2}
}
\affiliations {
    % Affiliations
    \textsuperscript{\rm 1}Université de Lorraine, CNRS, LORIA, France\\
    \textsuperscript{\rm 2}Université de Lorraine, ERPI, France\\
    \textsuperscript{\rm 3}Université Côte d’Azur, Inria, CNRS, I3S, France\\
}

% REMOVE THIS: bibentry
% This is only needed to show inline citations in the guidelines document. You should not need it and can safely delete it.
% \usepackage{bibentry}
% END REMOVE bibentry

\begin{document}

\maketitle

\begin{abstract}
Knowledge graph embedding models (KGEMs) are used for various tasks related to knowledge graphs (KGs), including link prediction. 
They are trained with loss functions that are computed considering a batch of scored triples and their corresponding labels. 
Traditional approaches consider the label of a triple to be either true or false. However, recent works suggest that all negative triples should not be valued equally. 
In line with this recent assumption, we posit that negative triples that are semantically valid w.r.t. domain and range constraints might be high-quality negative triples. As such, loss functions should treat them differently from semantically invalid negative ones. To this aim, we propose semantic-driven versions for the three main loss functions for link prediction.
In an extensive and controlled experimental setting, we show that the proposed loss functions systematically provide satisfying results on three public benchmark KGs underpinned with different schemas, which demonstrates both the generality and superiority of our proposed approach. 
In fact, the proposed loss functions do (1) lead to better MRR and Hits@$K$ values, (2) drive KGEMs towards better semantic awareness as measured by the Sem@$K$ metric. 
This highlights that semantic information globally improves KGEMs, and thus should be incorporated into loss functions. Domains and ranges of relations being largely available in schema-defined KGs, this makes our approach both beneficial and widely usable in practice.
\end{abstract}

\section{Introduction}\label{introduction}
A knowledge graph (KG) is a collection of triples $(h,r,t)$ where $h$ (head) and $t$ (tail) are two entities of the graph, and $r$ is a predicate that qualifies the nature of the relation holding between them.
KGs are inherently incomplete, incorrect, or overlapping and thus major refinement tasks include entity matching and link prediction~\cite{wang2017}. The latter is the focus of this paper.
Link prediction (LP) aims at completing KGs by leveraging existing facts to infer missing ones.
In the LP task, one is provided with a set of incomplete triples, where the missing head (resp. tail) needs to be predicted. This amounts to holding a set of triples where, for each triple, either the head $h$ or the tail $t$ is missing. 

The LP task is often addressed using knowledge graph embedding models (KGEMs) that represent entities and relations as low-dimensional vectors in a latent embedding space. A plethora of KGEMs has been proposed in the literature. They usually differ w.r.t. their scoring function, \textit{i.e.} how they model interactions between entities. Entities and relations embeddings are learned throughout several epochs, in an optimization process relying on loss functions. Although the choice and parametrization of these loss functions is key to driving KGEM to better performance~\cite{ali2022}, their individual influence on the learning process is rarely studied~\cite{mohamed2019}. 
 
In parallel, the underpinning semantics of KGs has been considered as additional information to improve results w.r.t. the LP task. Over the past few years, semantic information has been incorporated in various parts, \textit{e.g.} the model itself~\cite{tarp,transc,autoeter,transet,tkrl}, the negative sampling procedure~\cite{jain_iswc,krompas}, or the loss function~\cite{cao2022,damato2021,guo2015,minervini2017}. Existing works proposing to include semantic information into loss functions showcase promising results, but are restricted to the use of specific loss functions~\cite{damato2021,guo2015} or require ontological axioms~\cite{damato2021,minervini2017} that do not include domain and range axioms which are widely available in KGs. Hence, our first research question:
\begin{description}
    \item[RQ1] how main loss functions used in LP can incorporate
    domain and range constraints that are widely available in most KGs? 
\end{description}

Besides, precedent work pointed out the performance gain of incorporating ontological information as measured by rank-based metrics such as MRR and Hits@$K$~\cite{cao2022,damato2021,guo2015,minervini2017}. 
However, while such approaches include semantic information as KGEM inputs, the semantic capabilities of the resulting KGEM are left unassessed, even though this would provide a fuller picture of its performance~\cite{dl4kg,hubert2023}. Hence, our second research question:
\begin{description}
    \item[RQ2] what is the impact of semantic-driven loss functions on the overall KGEM performance?
\end{description}

To address both questions, we propose semantic-driven loss functions, \textit{i.e.} loss functions containing terms that depend on some background knowledge (BK) about types of entities and domains and ranges of relations. 
To broaden the impact of our approach, our work is concerned with the three most encountered loss functions in the literature: the pairwise hinge loss (PHL)~\cite{transe}, the 1-N binary cross-entropy loss (BCEL)~\cite{conve}, and the pointwise logistic loss (PLL)~\cite{complex} (further detailed in Section~\ref{lossfunc}).
For each of them, we propose a tailored semantic version.
The considered BK is available in many schema-defined KGs~\cite{ding2018}, which makes these newly introduced loss functions widely usable in practice.
 Furthermore, the impact of loss functions is evaluated using both rank-based metrics and Sem@$K$~\cite{dl4kg,hubert2023} -- a metric that measures the semantic awareness of KGEMs for the LP task.

To summarize, the main contributions of this work are: 
\begin{itemize}
    \item We propose semantic-driven versions for the three mostly used loss functions for the LP task, leveraging BK about relation domains and ranges.
    \item We evaluate our approach not only in terms of traditional rank-based metrics, but also w.r.t. Sem@$K$, which gives more insight into the benefits of our proposal.
    \item We show that the designed semantic-driven loss functions provide, in most cases, better performance w.r.t. both rank-based metrics and Sem@$K$. Consequently, our findings strongly indicate that semantic information should be systematically incorporated into loss functions.
\end{itemize}
The remainder of the paper is structured as follows. 
Related work is presented in Section~\ref{related-work}. 
In Section~\ref{lossfunc}, we detail the semantic-driven loss functions proposed in this work. 
Dataset descriptions and experimental settings are provided in Section~\ref{experimental-setting}. Key findings are presented in Section~\ref{results} and are further discussed in Section~\ref{discussion}.
Lastly, Section~\ref{conclusion} sums up the main findings and outlines future research directions.

\section{Related Work}\label{related-work}
This section firstly relates to former contributions that make use of semantic information to enhance model results regarding the LP task. Emphasis is placed on how semantic information can be incorporated in the loss functions~(Section~\ref{related-work:semantic}) -- a research avenue which remains relatively unexplored compared to incorporating semantic information in other parts of the learning process, \textit{e.g.} in the negative sampling or in the interaction function. Secondly, a brief background on the mainstream loss functions is provided~(Section~\ref{related-work:lossfunc}). This is to help position our contributions w.r.t. the vanilla loss functions used in practice.
\subsection{Semantic-Enhanced Approaches}\label{related-work:semantic}
A significant body of the literature proposes approaches that incorporate semantic information for performing LP with KGEMs, with the purpose of improving KGEM performance w.r.t. traditional rank-based metrics. 

The most straightforward way to do so is to embed semantic information in the model itself. For instance, AutoETER~\cite{autoeter} is an automated type representation learning mechanism that can be used with any KGEM and that learns the latent type embedding of each entity. In TaRP~\cite{tarp}, type information and instance-level information are simultaneously considered and encoded as prior probabilities and likelihoods of relations, respectively.
TKRL~\cite{tkrl} bridges type information with hierarchical information: while type information is utilized as relation-specific constraints, hierarchical types are encoded as projection matrices for entities. TKRL allows entities to have different representations in different types. Similarly, in~\cite{transet}, the proposed KGEM allows entities to have different vector representations depending on their respective types. TransC~\cite{transc} encodes each concept of a KG as a sphere and each instance as a vector in the same semantic space. The relations between concepts and instances (\texttt{rdf:type}), and the relations between concepts and sub-concepts (\texttt{rdfs:subClassOf}) are based on the relative distance within this shared semantic space.

A few works incorporate semantic information to constrain the negative sampling (NS) procedure and generate meaningful negative triples~\cite{krompas,jain_iswc,weyns}. For instance, type-constrained negative sampling (TCNS)~\cite{krompas} replaces the head or the tail of a triple with a random entity belonging to the same type (\texttt{rdf:type}) as the ground-truth entity. Jain \textit{et al.}~\cite{jain_iswc} go a step further and use ontological reasoning to iteratively improve KGEM performance by retraining the model on inconsistent predictions.

A few work actually propose to include semantic information in the learning and optimization process.
In~\cite{guo2015}, entities embeddings of the same semantic category are enforced to lie in a close neighborhood of the embedding space. However, their approach only fits single-type KGs. In addition, the only mainstream model benchmarked in this work is TransE~\cite{transe}, and only the pairwise hinge loss is used. Likewise, d'Amato \textit{et al.}~\cite{damato2021} solely consider the pairwise hinge loss, and their approach is benchmarked w.r.t. to translational models only. Moreover, BK is injected in the form of \texttt{equivalentClass}, \texttt{equivalentProperty}, \texttt{inverseOf}, and \texttt{subClassOf} axioms, similarly to~\cite{minervini2017} who incorporate \texttt{equivalentProperty} and \texttt{inverseOf} axioms as regularization terms in the loss function. However, the aforementioned axioms are rarely provided in KGs~\cite{ding2018}. Cao \textit{et al.}~\cite{cao2022} propose a new regularizer called Equivariance Regularizer, which limits overfitting by using semantic information. However, their approach is data-driven and does not rely on a schema. In contrast, the approach presented in Section~\ref{lossfunc} leverages domain and range constraints which are available in most schema-defined KGs.

Finally, all the aforementioned semantic-driven approaches are only evaluated w.r.t.. rank-based metrics. However, semantic-driven approaches would benefit from a semantic-oriented evaluation. To the best of our knowledge, the work around Sem@$K$~\cite{dl4kg,ekaw,hubert2023} is the only one to provide appropriate tools for measuring KGEM semantic awareness. Hence, our experiments will also be evaluated with this metric.

\subsection{Loss Functions for the Link Prediction Task}\label{related-work:lossfunc}
Few works revolve around the influence of loss functions on KGEM performance~\cite{ali2022,mohamed2019,mohamed2021}. Mohamed~\textit{et al.}~\cite{mohamed2019} point out the lack of consideration regarding the impact of loss functions on KGEM performance.
Experimental results provided in~\cite{ali2022} indicate that no loss function consistently provides the best results, and that it is rather the combination between the scoring and loss functions that impacts KGEM performance. In particular, some scoring functions better match with specific loss functions. For instance, Ali~\textit{et al.} show that TransE can outperform state-of-the-art KGEMs when configured with an appropriate loss function. Likewise, Mohamed~\textit{et al.}~\cite{mohamed2019} show that the choice of the loss function significantly influence KGEM performance. Consequently, they provide an extensive benchmark study of the main loss functions used in the literature. Namely, their analysis relies on a commonly accepted categorization between pointwise and pairwise loss functions. 
The main difference between pointwise and pairwise loss functions lies in the way the scoring function, the triples, and their respective labels are considered all together. Under the pointwise approach, the loss function relies on the predicted scores for triples and their actual label values, which is usually $1$ for positive triple and $0$ (or $-1$) for negative triples. In contrast, pairwise loss functions are defined in terms of differences between the predicted score of a true triple and the score of a negative counterpart. 

In our approach (Section~\ref{lossfunc}), we consider the three most commonly used loss functions for performing LP~\cite{rossi}: the pairwise hinge loss (PHL)~\cite{transe}, the 1-N binary cross-entropy loss (BCEL)~\cite{conve}, and the pointwise logistic loss (PLL)~\cite{complex}. Their vanilla formulas are recalled in Equations~\eqref{eq:phl-vanilla}, \eqref{eq:bcel-vanilla}, and \eqref{eq:pll-vanilla}.

\begin{equation}
\mathcal{L}_{PHL} = \sum_{t\in\mathcal{T^{+}}} \sum_{t'\in\mathcal{T^{-}}}  
\left[ \gamma + f(t') - f(t) \right]_{+}
\label{eq:phl-vanilla}
\end{equation}
where $\mathcal{T}$, $f$, and $[x]_{+}$ denote a batch of triples, the scoring function, and the positive part of $x$, respectively. $\mathcal{T}$ is further split into a batch of positive triples $\mathcal{T^{+}}$ and a batch of negative triples $\mathcal{T^{-}}$. $\gamma$ is a configurable margin hyperparameter specifying how much the scores of positive triples should be separated from the scores of corresponding negative triples.

\begin{equation}
\mathcal{L}_{BCEL} = -\frac{1}{|\mathcal{E}|}\sum_{t\in\mathcal{T}}
\ell(t)\log(f(t)) + (1-\ell(t))\log(1-f(t))
\label{eq:bcel-vanilla}
\end{equation}
where $\ell(t) \in \{0,1\}$ denotes the true label of $t$ and $\mathcal{T}$ is a batch with all possible $(h,r,*)$. $|\mathcal{E}|$ is the number of entities in the KG.

\begin{equation}
\mathcal{L}_{PLL} = \sum_{t\in\mathcal{T}}
\log(1+\exp^{-\ell(t) \cdot f(t)})
\label{eq:pll-vanilla}
\end{equation}
where $\ell(t) \in \{-1,1\}$ denotes the true label of $t$.

\section{Semantic-driven Loss Functions}\label{lossfunc}
Building on the limits of previous work (Section~\ref{related-work:semantic}), we propose semantic-driven loss functions that extend the three most frequently used loss functions~\cite{rossi}. 
We focus on BK about domains and ranges of relations, which are provided in many KGs used in the literature.

The purpose of the proposed loss functions is to distinguish \emph{semantically valid negatives} from \emph{semantically invalid ones}. The former are defined as triples $(h,r,t)$ respecting both the domain and range of the relation $r$ -- \textit{i.e.} $\operatorname{type}(h) \cap \operatorname{domain}(r) \neq \emptyset \land \operatorname{type}(t) \cap \operatorname{range}(r) \neq \emptyset$ -- whereas the latter violate at least one of the constraints -- \textit{i.e.} $\operatorname{type}(h) \cap \operatorname{domain}(r) = \emptyset \lor \operatorname{type}(t) \cap \operatorname{range}(r) = \emptyset$. $\operatorname{domain}(r)$ (resp. $\operatorname{range}(r)$) is defined as the expected type as head (resp. tail) for the relation $r$. For example, the relation \texttt{presidentOf} expects a \texttt{Person} as head and a \texttt{Country} as tail. Starting from a positive triple (\texttt{EmmanuelMacron}, \texttt{presidentOf}, \texttt{France}) which represents a true fact, (\texttt{BarackObama}, \texttt{presidentOf}, \texttt{France}) and (\texttt{EmmanuelMacron}, \texttt{presidentOf}, \texttt{Germany}) are examples of semantically valid negative triples, whereas (\texttt{Adidas}, \texttt{presidentOf}, \texttt{France}) and (\texttt{EmmanuelMacron}, \texttt{presidentOf}, \texttt{Christmas}) are examples of semantically invalid negative triples. In this work, entities are multi-typed. Therefore, $\operatorname{type}(e)$ returns the set of types (a.k.a. classes) that the entity $e$ belongs to.

We introduce a loss-independent $\epsilon$ factor, which is dubbed as the \textit{semantic factor} and aims at bringing the scores of semantically valid negative triples closer to the positive ones. This common semantic factor fitting into different loss functions shows the generality of our approach that can possibly be extended to other loss functions.
Interestingly, this factor also allows to take into account to some extent the Open World Assumption (OWA) under which KGs are represented. 
Under the OWA, triples that are not represented in a KG are either false or missing positive triples. 
In traditional training procedures, these triples are indiscriminately considered negative, which corresponds to the Closed World Assumption.
On the contrary, our proposal considers semantically invalid triples as true negative while semantically valid triples (and possibly positive or false negative under the OWA) are closer to true positive triples.
This assumes that entity types are complete and correct. 
	
$\mathbf{\mathcal{L}_{PHL}}$ defined in Equation~\eqref{eq:phl-vanilla} relies on the margin hyperparameter $\gamma$. Increasing (resp. descreasing) the value of $\gamma$ will increase (resp. descrease) the margin that will be set between the scores of positive and negative triples.
However, this unique $\gamma$ treats all negative triples indifferently: the same margin will separate the scores of semantically valid and semantically invalid negative triples from the score of the positive triple they both originate from. With the aim of improving the semantic validity of KGEM predictions -- and hopefully its performance w.r.t. rank-based metrics at the same time --, the scores of these two kinds of negative triples should be treated differently. Hence our approach which redefines $\mathcal{L}_{PHL}$ as follows (Equation~\eqref{eq:phl-sem}):
\begin{equation}
\begin{split}
\mathcal{L}^{S}_{PHL} & = \sum_{t\in\mathcal{T^{+}}} \sum_{t'\in\mathcal{T^{-}}}
\left[ \gamma \cdot \ell(t') + f(t') - f(t) \right]_{+}  \\ \\
\text{where } & \ell(t') = \begin{cases}
    1 \text{ if $t'$ is semantically invalid} \\
    \epsilon \text{ otherwise}
\end{cases}
\label{eq:phl-sem}
\end{split}
\end{equation}
The loss function in Equation~\eqref{eq:phl-sem} now has a superscripted \textit{S} to make it clear this is the semantic-driven version of the vanilla $\mathcal{L}_{PHL}$ as defined in Equation~\eqref{eq:phl-vanilla}. 
A choice of $\epsilon < 1$ leads the KGEM to apply a higher margin between scores of positive and semantically invalid triples than between positive and semantically valid ones. 
For a given positive triple, this allows to keep the scores of its semantically valid negative counterparts relatively closer compared to the scores of its semantically invalid counterparts. Intuitively, when the KGEM outputs wrong predictions, more of them are still expected to meet the domain and range constraints imposed by relations. Thus, wrong predictions are assumed to be more meaningful, and, in a sense, semantically closer to the ground-truth triple. 
	
$\mathbf{\mathcal{L}_{BCEL}}$ defined in Equation~\eqref{eq:bcel-vanilla} is adapted to $\mathcal{L}^{S}_{BCEL}$ by redefining the labelling function $\ell$. In particular, when dealing with a KG featuring typed entities and providing information about domains and ranges of relations, the labelling function $\ell$ is no longer binary. Instead, the labels of semantically valid negative triples can be fixed to some intermediate value between the label value of positive triples and of semantically invalid negative triples, which leads to the labelling function $\ell$ defined in Equation~\eqref{eq:label-bcel}:

\begin{equation}
\label{eq:label-bcel}
\begin{aligned}
\ell(t') = \left\{
    \begin{array}{lll}
1 & \mbox{if } t' \in  \mathcal{T^{+}}\\
 \epsilon & \mbox{if } t' \in \mathcal{T^{-}} \text{ and $t'$ is semantically valid} \\ 
 0 & \mbox{if } t' \in \mathcal{T^{-}} \text{ and $t'$ is semantically invalid}
    \end{array}
\right.
\end{aligned}
\end{equation}

where the semantic factor $\epsilon$ is a tunable hyperparameter denoting the label value of semantically valid negative triples. The intuition underlying the refinement of the labelling function $\ell$ is to voluntarily cause some confusion between semantically valid negative triples and positive triples.
By bridging their respective label values, it is expected that the KGEM will somehow consider the former as ``less negative triples`` and assign them a higher score compared to positive triples.

$\mathbf{\mathcal{L}_{PLL}}$ defined in Equation~\eqref{eq:pll-vanilla} could be adapted to $\mathcal{L}^{S}_{PLL}$ similarly to $\mathcal{L}^{S}_{BCEL}$. In other words, the labelling function $\ell$ could also output an intermediate label value $\epsilon$ for semantically valid negative triples. Although this approach provides very good results in terms of Sem@$K$ values, it does not provide consistently good results across datasets.
Furthermore, obtained results in terms of MRR and Hits@$K$ can be far below the ones obtained with the vanilla model (see Supplementary files for further details).
This raises the question whether a huge improvement w.r.t. Sem@$K$ is still desirable when this causes a significant drop in terms of rank-based metrics. Here, to treat semantically valid and invalid negative triples differently, instead of modifying the labelling function $\ell$, the semantic factor $\epsilon$ for $\mathcal{L}^{S}_{PLL}$ defines the probability with which semantically valid negative triples are considered as positive triples and therefore are labelled the same way. For example, with $\epsilon = 0.05$, at each training epoch and for each batch, the semantically valid negative triples of the given training batch will be considered positive with a probability of $5\%$.

It is worth mentioning that our approach does not focus on negative sampling or complex negative sample generators such as KBGAN~\cite{kbgan}, NSCaching~\cite{nscaching}, and self-adversarial negative sampling~\cite{rotate}. Although these works are related to ours, they constrain negative sampling upstream.
On the contrary, our approach does not constrain the sampling of negative triples but rather dynamically distributes the negative triples into different parts of the loss functions, based on their semantic validity.

\section{Experimental Setting}\label{experimental-setting}
\subsection{Datasets and Models}\label{datasets}

It is noteworthy that our approach can be applied in practice to KGs with or without types, domains and ranges.
Indeed, in the absence of such background knowledge, our semantic-driven loss functions reduce to their respective vanilla counterparts.
However, we evaluate our proposal in an ideal experimental setting to precisely qualify the interest of considering semantics.
Firstly, all relations appearing in the training set have a defined domain and range. Secondly, both  the head $h$ and tail $t$ of train triples have another semantically valid counterpart for negative sampling. These two conditions guarantee that each positive train triple can be paired with at least one semantically valid triple. Finally, validation and test sets contain triples whose relation have a well-defined domain (resp. range), as well as more than 10 possible candidates as head (resp. tail). This ensures Sem@$K$ is not unduly penalized and can be calculated on the same set of entities as Hits@$K$ and MRR until $K=10$.
The datasets used in the experiments derive from FB15K237-ET, DB93K, and YAGO4-19K~\cite{hubert2023}, that are all filtered so that they meet the aforementioned criteria.
In the following, their filtered versions are referred to as FB15k187, DBpedia77k, and Yago14k, respectively.
Table~\ref{tab:datasets} provides statistics for these datasets and reflects the diversity of their characteristics. In this work, several KGEMs are considered and summarized in Table~\ref{tab:kgems}.

\begin{table}
\small
\centering
\caption{Datasets used in the experiments. These are filtered versions of the standard FB15k237, DBpedia100k, and YAGO4-19k.}\label{tab:datasets}
\begin{tabular}{lrrrrr}
\hline 
Dataset & \multicolumn{1}{c}{$|\mathcal{E}|$} & \multicolumn{1}{c}{$|\mathcal{R}|$} & \multicolumn{1}{c}{$|\mathcal{T}_{train}|$} & \multicolumn{1}{c}{$|\mathcal{T}_{valid}|$} & \multicolumn{1}{c}{$|\mathcal{T}_{test}|$}\\
\hline 
FB15k187 & $14,305$ & $187$ & $245,350$ & $15,256$ & $17,830$ \\
DBpedia77k  & $76,651$ & $150$ & $140,760$ & $16,334$ & $32,934$ \\
Yago14k & $14,178$ & $37$ & $18,263$ & $472$ & $448$  \\
\hline 
\end{tabular}
\end{table}

\subsection{Implementation Details}\label{implementation}
For the sake of comparison, MRR, Hits@$K$ and Sem@$K$ are computed after training models from scratch.
KGEMs used in the experiments were implemented in PyTorch. After training KGEMs for a large number of epochs, we noticed the best achieved results were found around epoch $400$ or below. Consequently, a maximum of $400$ epochs of training was set, as in LibKGE\footnote{\url{https://github.com/uma-pi1/kge}} (except RGCN which is trained during 4,000 epochs due to lower convergence to the best achieved results).
Except when using the BCEL which does not require negative sampling, Uniform Random Negative Sampling~\cite{transe} was used to pair each train triple with two corresponding negative triples: one which is semantically invalid, and one which is semantically valid. 
Regarding BCEL, each positive triple is scored against negative triples formed with all other entities in the graph. 
Hence, negative triples comprise both semantically valid and invalid triples.
In order to ensure fair comparisons between models, embeddings are initialized with the same seed and each model is fed with exactly the same set of negative triples at each epoch.
Grid-search based on predefined hyperparameters was performed. The full hyperparameter search space is provided in Table~\ref{tab:searchspace}. The chosen optimal hyperparameters are reported in supplementary materials. Datasets and source code are publicly available\footnote{\url{https://github.com/nicolas-hbt/semantic-lossfunc}}.

\subsection{Evaluation Metrics}
In Section~\ref{results}, KGEM performance is assessed w.r.t. MRR, Hits@$K$ and Sem@$K$, with $K=10$.

\begin{description}
\item [Mean Reciprocal Rank (MRR)] corresponds to the arithmetic mean over the reciprocals of ranks of the ground-truth triples.
MRR is bounded in the $\left[0,1\right]$ interval, where the higher the better.
\item [Hits@$K$] accounts for the proportion of ground-truth triples appearing in the first $K$ top-scored triples.
This metric is bounded in the $ \left[0,1\right]$ interval and its values increases with $K$, where the higher the better.
\item [Sem@$K$~\cite{dl4kg,hubert2023}] accounts for the proportion of
triples that are semantically valid in the first $K$ top-scored triples, \textit{i.e.} triples whose predicted head (resp. tail) belongs to the domain (resp. range) of the relation. This metric is bounded in the $ \left[0,1\right]$ interval. Higher Sem@$K$ values indicate better KGEM semantic capabilities.
\end{description}

\section{Results}\label{results}
\subsection{Global Performance}\label{global-perf}
\begin{table*}[h]
        \caption{Rank-based and semantic-based results on the schema-defined knowledge graphs. Bold fonts indicate which model performs best w.r.t. a given metric. Suffixes V and S indicate whether the model is trained under the vanilla or semantic-driven version of the loss function, respectively. Hits@$10$ and Sem@$10$ are abbreviated to H@$10$ and S@$10$. Underlined cells indicate results that are more specifically referred to in Section~\ref{global-perf}.}
	\label{tab:schema-defined-results-compiled}
    \small
	\centering
                \setlength{\tabcolsep}{0.15cm}
			\begin{tabular}{lccccccccc}
                    \hline
				&\multicolumn{3}{c}{FB15k187} & \multicolumn{3}{c}{DBpedia77k} & \multicolumn{3}{c}{Yago14k}
                \\
                    & MRR & H@10 & S@10 &
                    MRR & H@10 & S@10 &
                    MRR & H@10 & S@10 \\
				\hline
				TransE-V &
    $.260$&$.446$&$.842$& $.274$&$.438$&$.936$& $.868$&$\mathbf{.945}$&$.795$\\
                TransE-S &
    $\mathbf{.315}$&$\mathbf{.497}$&$\mathbf{.973}$& $\mathbf{.275}$&$\mathbf{.440}$&$\mathbf{.985}$& $\mathbf{.876}$&$.944$&$\mathbf{.968}$ \\
    \hline
                TransH-V &
    $.266$&$.450$&$.855$& $.270$&$.437$&$.907$& $.836$&$.944$&$.581$ \\
                TransH-S &
    $\mathbf{.319}$&$\mathbf{.501}$&$\mathbf{.973}$& $\mathbf{.274}$&$\mathbf{.442}$&$\mathbf{.980}$& $\mathbf{.857}$&$\mathbf{.945}$&$\underline{\mathbf{.831}}$ \\
    \hline
                DistMult-V &
    $.291$&$.457$&$.824$& $.295$&$.405$&$.784$& $.904$&$\mathbf{.930}$&$.409$ \\
			    DistMult-S &
    $\mathbf{.332}$&$\mathbf{.504}$&$\mathbf{.971}$& $\mathbf{.300}$&$\mathbf{.416}$&$\mathbf{.901}$& $\mathbf{.912}$&$.929$&$\mathbf{.449}$ \\
    \hline
    		ComplEx-V &
    $.280$&$.416$&$.472$& $\mathbf{.309}$&$\mathbf{.415}$&$.769$& $\mathbf{.925}$&$\mathbf{.932}$&$.333$ \\
    		ComplEx-S &
    $\mathbf{.316}$&$\mathbf{.476}$&$\underline{\mathbf{.796}}$& $.297$&$.409$&$\mathbf{.897}$& $.923$&$.931$&$\underline{\mathbf{.667}}$ \\
    \hline
                SimplE-V &
    $.261$&$.387$&$.462$& $\mathbf{.259}$&$\mathbf{.346}$&$\mathbf{.883}$& $\mathbf{.926}$&$\mathbf{.931}$&$.355$ \\
                SimplE-S &
    $\mathbf{.268}$&$\mathbf{.409}$&$\underline{\mathbf{.759}}$& $.230$&$.302$&$.850$& $.924$&$.927$&$\underline{\mathbf{.769}}$ \\
    \hline
				ConvE-V &
    $.273$&$.470$&$.973$& $.273$&$.382$&$.935$& $\mathbf{.934}$&$\mathbf{.942}$&$.904$ \\
				ConvE-S &
    $\mathbf{.283}$&$\mathbf{.476}$&$\mathbf{.996}$& $\mathbf{.283}$&$\mathbf{.405}$&$\mathbf{.985}$& $.933$&$.940$&$\mathbf{.997}$  \\
    \hline
    			TuckER-V &
    $.316$&$.516$&$.985$& $.311$&$.410$&$.912$& $.923$&$.927$&$.781$ \\
                TuckER-S &
    $\mathbf{.320}$&$\mathbf{.522}$&$\mathbf{.996}$& $\mathbf{.312}$&$\mathbf{.421}$&$\mathbf{.969}$& $\mathbf{.931}$&$\mathbf{.943}$&$\mathbf{.929}$ \\
    \hline
                RGCN-V &
    $.241$&$.386$&$.775$& $.194$&$.297$&$.872$& $.911$&$.923$&$.349$ \\
                RGCN-S &
    $\mathbf{.260}$&$\mathbf{.415}$&$\mathbf{.860}$& $\mathbf{.197}$&$\mathbf{.320}$&$\mathbf{.957}$& $\mathbf{.927}$&$\mathbf{.934}$&$\underline{\mathbf{.828}}$ \\
    \hline
    % \vspace{0.5mm}
			\end{tabular}
 \end{table*}

Table~\ref{tab:schema-defined-results-compiled} displays KGEM performance, datasets and evaluation metrics of interest. It clearly shows that, with the sole exception of SimplE on DBpedia77k, the semantic-driven loss functions $\mathcal{L}^{S}_{PHL}$, $\mathcal{L}^{S}_{BCEL}$, and $\mathcal{L}^{S}_{PLL}$ all lead to significant improvement in terms of Sem@$10$. Importantly, in some cases the relative gain in Sem@$10$ compared to the corresponding vanilla loss function is huge (underlined in Table~\ref{tab:schema-defined-results-compiled}): $+137\%$, $+117\%$, and $+100\%$ for RGCN, SimplE, and ComplEx on the smaller Yago14k dataset, respectively, and $+69\%$ and $+64\%$ for ComplEx and SimplE on FB15k187, respectively. These gains in terms of Sem@$10$ are observed regardless of the loss function at hand, which demonstrates that our designed semantic-driven loss functions can all drive KGEM semantic awareness towards more satisfying results. It is worth noting that, in most cases, they also lead to better KGEM performance as measured by MRR and Hits@$10$. We observe that in $19$ out of $24$ ($\approx79\%$) one-to-one comparisons between the same KGEM trained with vanilla vs. semantic-driven loss functions, better MRR values are reported for the model trained under the semantic-driven loss function. The remaining comparisons ($\approx21\%$) highlight minimal losses in terms of MRR, often for the benefit of significantly better Sem@$10$ values. This observation raises the question whether better MRR and Hits@$K$ should be pursued at any cost, or whether a small drop w.r.t. to these metrics is acceptable if this leads to a significantly better KGEM semantic awareness. Plus, these promising results imply that even if the intended purpose is to only maximize MRR and Hits@$K$ values, taking the available semantic information into consideration is strongly advised: this does not only improve KGEM semantic awareness, but also provide performance gains in terms of MRR and Hits@$K$. 

In the following, we provide a finer-grained results' analysis, which focuses on the different loss functions and datasets used in the experiments.
Although we previously showed the effectiveness of our approach, the benefits brought from considering BK in the form of relation domains and ranges differ across loss functions. In particular, the gains achieved using $\mathcal{L}^{S}_{PHL}$ and $\mathcal{L}^{S}_{BCEL}$ are substantial. With these loss functions, we observe a systematic improvement w.r.t. Sem@$10$. Gains are also reported w.r.t. rank-based metrics in the vast majority of cases, with the only exception being ConvE on Yago14k. However, the difference in terms of MRR and Hits@$10$ values between ConvE-V and ConvE-S is negligible. Therefore, incorporating BK about relation domains and ranges into $\mathcal{L}^{S}_{PHL}$ and $\mathcal{L}^{S}_{BCEL}$ is a viable approach that provides consistent gains both in terms of MRR, Hits@$10$ and Sem@$10$.
Regarding the benefits from doing so under $\mathcal{L}^{S}_{PLL}$, we can notice a slight decline in MRR and Hits@$10$ values in some cases. However, the other side of the coin is that achieved Sem@$10$ values are substantially higher: except for SimplE on DBpedia77k, Sem@$10$ values increase in a range from $+17\%$ to $+117\%$ for the remaining one-to-one comparisons. As such, our approach using $\mathcal{L}^{S}_{PLL}$ also provides satisfactory results, as long as a slight drop in rank-based metrics is acceptable if it comes with the benefit of significantly better KGEM semantic awareness. Besides, it is worth noting the following points: our hyperparameter tuning strategy relied on the choice of the best $\epsilon$ value on Yago14k -- for computational limitations. The $\epsilon$ value which was found to perform the best on Yago14k was subsequently used in all the remaining scenarios. A more thorough tuning of $\epsilon$ on the other datasets would have potentially provided even more satisfying results under the $\mathcal{L}^{S}_{PLL}$, thus strengthening the value of our approach.

\subsection{Ablation Study}
In this section, KGEMs are tested on three buckets of relations that feature narrow (B1), intermediate (B2), and large (B3) sets of semantically valid heads or tails, respectively. The cut-offs have been manually defined and are provided in supplementary materials. The analysis of B1 allows us to gauge the impact of semantic-driven loss functions on relations for which it is harder to predict semantically valid entities. Results reported in Table~\ref{tab:bucket1} for B1 clearly demonstrate that the impact of injecting BK into loss functions is exacerbated for them, thus supporting the value of our approach in a sparse and difficult setting. One might think that the better MRR values achieved with $\mathcal{L}^{S}_{PHL}$, $\mathcal{L}^{S}_{BCEL}$, and $\mathcal{L}^{S}_{PLL}$ are highly correlated to the better Sem@$10$ values. This is partially true, as for a relation in B1, placing all semantically valid candidates at the top of the ranking list is likely to uplift the rank of the ground-truth itself. However, we can see that RGCN-V and RGCN-S have almost equal MRR values on Yago14k, while Sem@$10$ values of RGCN-S are much higher than RGCN-V ($+254\%$). Similar findings hold for ComplEx on Yago14k, ConvE on DBpedia77k, TransE on DBpedia77k and Yago14k. This shows that in a number of cases, semantic-driven loss functions improve the semantic awareness of KGEM for small relations while leaving its performance untouched in terms of rank-based metrics. 
Results on B2 and B3 are provided in supplementary materials. In particular, it can be noted that the relative benefit of our approach w.r.t. MRR and Hits@$10$ is more limited on such buckets. Regarding Sem@$K$, results achieved with the vanilla loss functions are already high, hence the relatively lower gain brought by injecting ontological BK. 
These already high Sem@$K$ values may be explained by a higher number of semantically valid candidates. 

\begin{table*}
        \caption{Rank-based and semantic-based results on the bucket of relations that feature a narrow set of semantically valid heads or tails (B1). The cut-offs have been manually defined and are provided in supplementary materials. Bold fonts indicate which model performs best w.r.t. a given metric. Suffixes V and S indicate whether the model is trained under the vanilla or semantic-driven version of the loss function, respectively. Hits@$10$ and Sem@$10$ are abbreviated to H@$10$ and S@$10$.}
	\label{tab:bucket1}
    \small
	\centering
                \setlength{\tabcolsep}{0.15cm}
			\begin{tabular}{lccccccccc}
                    \hline
				&\multicolumn{3}{c}{FB15k187} & \multicolumn{3}{c}{DBpedia77k} & \multicolumn{3}{c}{Yago14k} 
                \\
                    & MRR & H@10 & S@10 &
                    MRR & H@10 & S@10 &
                    MRR & H@10 & S@10 \\
				\hline
				TransE-V &
    $.535$&$.727$&$.647$& $.498$&$.578$&$.460$& $.914$&$\mathbf{.979}$&$.620$\\
                TransE-S &
    $\mathbf{.646}$&$\mathbf{.805}$&$\mathbf{.937}$& 
    $\mathbf{.539}$&$\mathbf{.626}$&$\mathbf{.910}$& 
    $\mathbf{.922}$&$.975$&$\mathbf{.924}$ \\
    \hline
                TransH-V &
    $.541$&$.734$&$.661$& $.475$&$.555$&$.425$& $.897$&$.972$&$.436$ \\
                TransH-S &
    $\mathbf{.655}$&$\mathbf{.814}$&$\mathbf{.936}$& 
    $\mathbf{.541}$&$\mathbf{.643}$&$\mathbf{.828}$& 
    $\mathbf{.923}$&$\mathbf{.981}$&$\mathbf{.684}$ \\
    \hline
                DistMult-V &
    $.589$&$.735$&$.628$& $.466$&$.462$&$.244$& $.949$&$\mathbf{.959}$&$.304$ \\
			  DistMult-S &
    $\mathbf{.667}$&$\mathbf{.802}$&$\mathbf{.929}$& 
    $\mathbf{.498}$&$\mathbf{.547}$&$\mathbf{.451}$& 
    $\mathbf{.965}$&$.958$&$\mathbf{.372}$ \\
    \hline
    		  ComplEx-V &
    $.530$&$.567$&$.116$& $\mathbf{.424}$&$\mathbf{.425}$&$.161$& $.955$&$.956$&$.133$ \\
    			ComplEx-S &
    $\mathbf{.637}$&$\mathbf{.723}$&$\mathbf{.537}$& 
    $.421$&$.399$&$\mathbf{.198}$& 
    $\mathbf{.961}$&$.956$&$\mathbf{.423}$ \\
    \hline
                SimplE-V &
    $.507$&$.553$&$.136$& $\mathbf{.396}$&$\mathbf{.370}$&$\mathbf{.273}$& $\mathbf{.959}$&$.882$&$\mathbf{.932}$ \\
                SimplE-S &
    $\mathbf{.576}$&$\mathbf{.671}$&$\mathbf{.505}$& 
    $.324$&$.259$&$.206$& 
    $.958$&$\mathbf{.883}$&$.930$ \\
    \hline
				ConvE-V &
    $.549$&$.779$&$.973$& $.518$&$\mathbf{.569}$&$.789$& $.969$&$\mathbf{.972}$&$.915$ \\
				ConvE-S &
    $\mathbf{.562}$&$\mathbf{.783}$&$\mathbf{.986}$& 
    $.518$&$.566$&$\mathbf{.927}$& 
    $.969$&$.965$&$\mathbf{.960}$  \\
    \hline
    			TuckER-V &
    $.597$&$.811$&$.969$& $.519$&$.568$&$.740$& $.949$&$.970$&$.846$ \\
                TuckER-S &
    $\mathbf{.598}$&$\mathbf{.815}$&$\mathbf{.973}$& 
    $\mathbf{.526}$&$\mathbf{.582}$&$\mathbf{.797}$& 
    $\mathbf{.964}$&$.970$&$\mathbf{.892}$ \\
    \hline
                RGCN-V &
    $.510$&$.629$&$.468$& $.386$&$.387$&$.254$& $.963$&$.959$&$.141$ \\
                RGCN-S &
    $\mathbf{.549}$&$\mathbf{.705}$&$\mathbf{.682}$& 
    $\mathbf{.396}$&$\mathbf{.415}$&$\mathbf{.398}$& 
    $\mathbf{.966}$&$\mathbf{.967}$&$\mathbf{.499}$ \\
    \hline
			\end{tabular}
 \end{table*}

\section{Discussion}\label{discussion}
Two major research questions have been formulated in Section~\ref{introduction}. Based on the analysis presented in Section~\ref{results}, we address each of them:
\begin{description}
    \item[RQ1] how main loss functions used in LP can incorporate ontological information widely available in most KGs? 
\end{description}
The design of $\mathcal{L}^{S}_{PHL}$, $\mathcal{L}^{S}_{BCEL}$, and $\mathcal{L}^{S}_{PLL}$ is detailed in Section~\ref{lossfunc}. These loss functions provide adequate training objective for KGEMs, as evidenced in Table~\ref{tab:schema-defined-results-compiled}. Most importantly, in Section~\ref{lossfunc} we clearly show how the inclusion of ontological information into $\mathcal{L}^{S}_{PHL}$, $\mathcal{L}^{S}_{BCEL}$, and $\mathcal{L}^{S}_{PLL}$ can be brought under one roof thanks to a commonly defined semantic factor. Although this semantic factor operates at different levels depending on the loss function, its common purpose is to differentiate how semantically valid and semantically invalid negative triples should be considered compared to positive triples, whereas traditional approaches treat all negative triples indifferently.
Besides, our approach which consists in transforming vanilla loss functions into semantic-driven ones can also work for other loss functions such as the pointwise hinge loss or the pairwise logistic loss, as presented in~\cite{mohamed2019}. Such losses can include a semantic factor $\epsilon$ as well. The tailoring of these losses and the experiments are left for future work.

\begin{description}
    \item[RQ2] what is the impact of semantic-driven loss functions on the overall KGEM performance?
\end{description}
Mohamed \textit{et al.}~\cite{mohamed2021} investigate the effects of specific choices of loss functions on the scalability and performance of KGEMs w.r.t. rank-based metrics. In this present work, we also assess the semantic capabilities of such models. Based on the results provided and analyzed in Section~\ref{lossfunc}, incorporating BK about relation domains and ranges into the loss functions clearly contribute to a better KGEM semantic awareness, as evidenced by the huge increase frequently observed w.r.t. Sem@$K$. It should also be noted that this increase is not homogeneously distributed across relations: relations with a smaller set of semantically valid entities as heads or tails are more challenging w.r.t. Sem@$K$ (see Table~\ref{tab:bucket1} compared to results on B2 and B3 provided in supplementary materials). For such relations, the relative gain from semantic-driven loss functions is more acute (\textit{e.g.} Sem@$10$ results on B1). In addition, semantic-driven loss functions also drive most of the KGEMs towards better MRR and Hits@$K$ values. As shown in Table~\ref{tab:schema-defined-results-compiled}, this is particularly the case when using the $\mathcal{L}^{S}_{PHL}$ and $\mathcal{L}^{S}_{BCEL}$. This result is particularly interesting, as it suggests that when semantic information about entities and relations is available, there are benefits in using them, even if the intended goal remains to enhance KGEM performance w.r.t. rank-based metrics only.
In addition, it has been noted that including ontological information in loss functions has the highest impact on small relations (B1) (Table~\ref{tab:bucket1}), both in terms of rank-based metrics (MRR, Hits@$10$) and semantic awareness (Sem@$10$). For relations featuring a larger pool of semantically valid entities, the impact is still positive w.r.t. Sem@$10$, but it sometimes comes at the expense of a small drop in terms of rank-based metrics. If the latter metrics are the sole optimization objective, it would thus be reasonable to design an adaptive training strategy in which vanilla and semantic-driven loss functions are alternatively used depending on the current relation and the number of semantically valid candidates as head or tail.

Finally, recall that compared to complex negative sampler such as KGBAN~\cite{kbgan}, NSCaching~\cite{nscaching}, and self-adversarial NS~\cite{rotate}, we do not introduce any potential overhead due to the need for maintaining a cache~\cite{nscaching} or training an intermediate adversarial learning framework~\cite{kbgan, rotate} for generating high-quality negatives. Instead, negative triples dynamically enter a different part of the loss function depending on their semantic validity. In future work, we will compare the algorithmic complexity of our approach, thereby highlighting the potential cost savings in computational resources and execution time compared to sophisticated NS. Besides, our approach can be applied even in the absence of BK. In this case, semantic-driven loss functions reduce to their vanilla version.
Our approach is also agnostic to the underlying NS procedure, and can work with simple uniform random NS~\cite{transe} as well as more complex procedures~\cite{kbgan, nscaching, rotate}.

\section{Conclusion}\label{conclusion}
In this work, we focus on the main loss functions used for link prediction task in knowledge graphs. 
Building on the assumption that negative triples are not all equally good for learning better embeddings, we propose to differentiate them based on their semantic validity w.r.t. the domain and range of relations. 
This is achieved by including ontological information into loss functions. 
Semantically valid negative triples are considered as negative examples of higher quality. Consequently, their scores are closer to the score of the ground-truth triple, while the scores of semantically invalid ones are relatively farther away. 
A wide range of KGEMs are subsequently trained under both the vanilla and semantic-driven loss functions. In our experiments on three public KGs with different characteristics, the proposed semantic-driven loss functions lead to promising results: in most cases, they do not only lead to better MRR and Hits@$10$ values, but also drive KGEMs towards better semantic awareness as measured with Sem@$10$. This clearly demonstrates that semantic information should be incorporated into loss functions whenever such information is available.

In future work, we will study how the proposed loss functions can accommodate other types of ontological constraints. We will also explore the possibility of designing a unique loss function that would provide systematically good results regardless of the KGEM and KG at hand.

\clearpage
\appendix\label{appendix}

\section{Model Details}\label{appendix:models}
Details about the scoring (a.k.a interaction) and loss functions of the KGEMs used in this paper are shown in Table~\ref{tab:kgems}.

\begin{table*}[h]
\small
\centering
\caption{Characteristics of the KGEMs used in the experiments}\label{tab:kgems}
\begin{tabular}{ccc}
\hline
Model&\multicolumn{1}{c}{Scoring Function}&
\multicolumn{1}{c}{Loss Function}\\
\hline
TransE~\cite{transe}     &$\left\|\mathbf{e}_h+\mathbf{e}_r-\mathbf{e}_t\right\|_p$ & $\mathcal{L}_{PHL}$\\
TransH~\cite{transh}      &$\left\|\mathbf{e}_{h_{\perp}}+\mathbf{d}_{r}-\mathbf{e}_{t_{\perp}}\right\|_p$ & $\mathcal{L}_{PHL}$\\
DistMult~\cite{distmult}     &$\left\langle\mathbf{e}_{h}, \mathbf{W}_{r}, \mathbf{e}_{t}\right\rangle$ & $\mathcal{L}_{PHL}$\\ [7pt]
ComplEx~\cite{complex}      &$\operatorname{Re}\left(\mathbf{e}_{h} \odot \mathbf{e}_{r} \odot \overline{\mathbf{e}}_{t}\right)$ & $\mathcal{L}_{PLL}$\\
SimplE~\cite{simple}      &$\frac{1}{2}{\left(\left\langle\mathbf{e}_{h}^{h}, \mathbf{e}_{r}, \mathbf{e}_{t}^{t}\right\rangle + \left\langle\mathbf{e}_{h}^{t}, \mathbf{e}_{r}^{-1}, \mathbf{e}_{t}^{h}\right\rangle\right)}$ & $\mathcal{L}_{PLL}$\\[7pt]
ConvE~\cite{conve}     &${g\left(\operatorname{vec}\left(g\left({\operatorname{concat}}\left(\widehat{\mathbf{e}}_h, \widehat{\mathbf{e}}_r\right) * \mathbf{\omega}\right)\right) \mathbf{W}\right) \cdot \mathbf{e}_t}$ & $\mathcal{L}_{BCEL}$\\
TuckER~\cite{tucker}     &$\mathcal{W} \times_1 \mathbf{e}_h \times_2 \mathbf{w}_r \times_3 \mathbf{e}_t$ & $\mathcal{L}_{BCEL}$\\
RGCN~\cite{rgcn}      &DistMult decoder & $\mathcal{L}_{BCEL}$\\
\hline
\end{tabular}
\end{table*}

\section{Hyperparameters}\label{appendix:hyperparam}

Full hyperparameter search space is reported in Table~\ref{tab:searchspace}. Those hyperparameters leading to the best MRR results under the original loss formulation for each KGEM were obtained with grid-search over a defined hyperparameter space. These optimal hyperparameters are reported in Table~\ref{tab:hyperparams}. Likewise, grid-search was performed for finding the best hyperparameters for $\mathcal{L}^{S'}_{BCEL}$ and $\mathcal{L}^{S'}_{PLL}$, which are reported in Table~\ref{tab:hyperparams-alternative}.

\begin{table*}[h]
\small
\centering
\caption{Hyperparameter search space}\label{tab:searchspace}
\begin{tabular}{ll}
\hline 
Hyperparameters & Range \\
\hline 
Batch Size & $\{128,256,512,1024,2048\}$ \\
Embedding Dimension & $\{50,100,150,200\}$ \\
Regularizer Type & $\{$None, $L1$, $L2$$\}$ \\
Regularizer Weight $(\lambda)$ & $\{1\mathrm{e}^{-2}, 1\mathrm{e}^{-3}, 1\mathrm{e}^{-4}, 1\mathrm{e}^{-5}\}$ \\
Learning Rate $(l r)$ & $\{1\mathrm{e}^{-2},5\mathrm{e}^{-3},1\mathrm{e}^{-3},5\mathrm{e}^{-4},1\mathrm{e}^{-4}\}$ \\
Margin $\gamma~(\mathcal{L}_{PHL})$ & $\{1,2,3,5,10,20\}$\\
\hline
Semantic Factor $\epsilon~(\mathcal{L}^{S}_{PHL})$ & $\{0.01,0.1,0.25,0.5,0.75\}$\\
Semantic Factor $\epsilon~(\mathcal{L}^{S}_{PLL})$ & $\{0.05,0.10,0.15,0.25\}$\\
Semantic Factor $\epsilon~(\mathcal{L}^{S}_{BCEL})$ & $\{1\mathrm{e}^{-1}, 1\mathrm{e}^{-2},1\mathrm{e}^{-3}, 1\mathrm{e}^{-4},1\mathrm{e}^{-5}\}$\\
\hline
\end{tabular}
\end{table*}

\begin{table*}[h]
\caption{Chosen hyperparameters for the KGEMs as trained with their original loss function, both in vanilla and semantic-driven versions. $|\mathcal{B}|$, $d$, $lr$, $\lambda$, and $\epsilon$ denote the batch size, embedding dimension, learning rate, regularization weight, and semantic factor, respectively. 
We experimentally found that $L2$ regularizer systematically worked the best. 
We therefore decide not to refer to it in the table.}\label{tab:hyperparams}
\centering
\small
\renewcommand{\arraystretch}{1.0}
\setlength{\tabcolsep}{7.5pt}
\begin{tabular}{lrrrr}
\hline 
 \multicolumn{1}{c}{Model} & \multicolumn{1}{c}{Hyperparameters} & \multicolumn{1}{c}{DBpedia77k} & \multicolumn{1}{c}{FB15k187} & \multicolumn{1}{c}{Yago14k} \\
\hline
\multirow{5}*{TransE} & $|\mathcal{B}|$ & $2048$&$2048$&$1024$\\
& $d$ & $200$&$200$&$100$\\
& $lr$ &$1\mathrm{e}^{-3}$&$1\mathrm{e}^{-3}$&$1\mathrm{e}^{-3}$\\
& $\lambda$ & $1\mathrm{e}^{-3}$&$1\mathrm{e}^{-3}$&$1\mathrm{e}^{-3}$\\
& $\epsilon$ & $5\mathrm{e}^{-1}$&$0.25$&$0.25$\\
\hline
\multirow{5}*{TransH}& $|\mathcal{B}|$ & $2048$&$2048$&$1024$\\
& $d$ & $200$&$200$&$100$\\
& $lr$ & $1\mathrm{e}^{-3}$&$1\mathrm{e}^{-3}$&$1\mathrm{e}^{-3}$\\
& $\lambda$ &$1\mathrm{e}^{-5}$&$1\mathrm{e}^{-5}$&$1\mathrm{e}^{-5}$\\
& $\epsilon$ & $5\mathrm{e}^{-1}$&$0.25$&$0.25$\\
\hline
\multirow{5}*{DistMult} & $|\mathcal{B}|$ & $2048$&$2048$&$1024$\\
& $d$ & $200$&$200$&$100$\\
& $lr$ & $1\mathrm{e}^{-1}$&$1\mathrm{e}^{1}$&$1\mathrm{e}^{-4}$\\
& $\lambda$ & $1\mathrm{e}^{-5}$&$1\mathrm{e}^{-5}$&$1\mathrm{e}^{-5}$\\
& $\epsilon$ & $5\mathrm{e}^{-1}$&$0.25$&$0.25$\\
\hline
\multirow{5}*{ComplEx} & $|\mathcal{B}|$ & $2048$&$2048$&$1024$\\
& $d$ & $200$&$200$&$100$\\
& $lr$ &$1\mathrm{e}^{-3}$&$1\mathrm{e}^{-3}$&$1\mathrm{e}^{-2}$\\
& $\lambda$ & $1\mathrm{e}^{-1}$&$1\mathrm{e}^{-1}$&$1\mathrm{e}^{-1}$\\
& $\epsilon$ & $0.15$&$0.15$&$.015$\\
\hline
\multirow{5}*{SimplE} & $|\mathcal{B}|$ & $2048$&$2048$&$1024$\\
& $d$ & $200$&$200$&$100$\\
& $lr$ &$1\mathrm{e}^{-1}$&$1\mathrm{e}^{-1}$&$1\mathrm{e}^{-1}$\\
& $\lambda$ & $1\mathrm{e}^{-2}$&$1\mathrm{e}^{-1}$&$1\mathrm{e}^{-5}$\\
& $\epsilon$ & $.15$&$.15$&$.15$\\
\hline
\multirow{5}*{ConvE} & $|\mathcal{B}|$ & $512$&$128$&$512$\\
& $d$ & $200$&$200$&$200$\\
& $lr$ &$1\mathrm{e}^{-3}$&$1\mathrm{e}^{-3}$&$1\mathrm{e}^{-3}$\\
& $\lambda$ & $0$&$0$&$0$\\
& $\epsilon$ &$1\mathrm{e}^{-4}$&$1\mathrm{e}^{-3}$&$1\mathrm{e}^{-3}$\\
\hline
\multirow{5}*{TuckER} & $|\mathcal{B}|$ & $128$&$128$&$128$\\
& $d$ & $200$&$200$&$100$\\
& $lr$ &$1\mathrm{e}^{-3}$&$5\mathrm{e}^{-4}$&$1\mathrm{e}^{-3}$\\
& $\lambda$ & $0$&$0$&$0$\\
& $\epsilon$ &$1\mathrm{e}^{-5}$&$1\mathrm{e}^{-4}$&$1\mathrm{e}^{-4}$\\
\hline
\multirow{4}*{RGCN} & $d$ & $500$&$500$&$500$\\
& $lr$ &$1\mathrm{e}^{-2}$&$1\mathrm{e}^{-2}$&$1\mathrm{e}^{-2}$\\
& $\lambda$ & $1\mathrm{e}^{-2}$&$1\mathrm{e}^{-2}$&$1\mathrm{e}^{-2}$\\
& $\epsilon$ &$1\mathrm{e}^{-1}$&$1\mathrm{e}^{-1}$&$1\mathrm{e}^{-1}$\\
\hline
\end{tabular}
\end{table*}

\begin{table*}[h]
\caption{Chosen hyperparameters for the KGEMs trained with the alternative semantic-driven loss function as detailed in Section~\ref{modified-versions}.
$|\mathcal{B}|$, $d$, $lr$, $\lambda$, and $\epsilon$ denote the batch size, embedding dimension, learning rate, regularization weight, and semantic factor, respectively. 
We experimentally found that $L2$ regularizer systematically worked the best. 
We therefore decide not to refer to it in the table.}\label{tab:hyperparams-alternative}
\centering
\small
\renewcommand{\arraystretch}{1.0}
\setlength{\tabcolsep}{7.5pt}
\begin{tabular}{lrrrr}
\hline 
 \multicolumn{1}{c}{Model} & \multicolumn{1}{c}{Hyperparameters} & \multicolumn{1}{c}{DBpedia77k} & \multicolumn{1}{c}{FB15k187} & \multicolumn{1}{c}{Yago14k} \\
\hline
\multirow{5}*{ComplEx} & $|\mathcal{B}|$ & $2048$&$2048$&$1024$\\
& $d$ & $200$&$200$&$100$\\
& $lr$ &$1\mathrm{e}^{-4}$&$1\mathrm{e}^{-4}$&$1\mathrm{e}^{-3}$\\
& $\lambda$ & $1\mathrm{e}^{-1}$&$1\mathrm{e}^{-1}$&$1\mathrm{e}^{-1}$\\
& $\epsilon$ & $-1\mathrm{e}^{-1}$&$-1\mathrm{e}^{-1}$&$1\mathrm{e}^{-2}$\\
\hline
\multirow{5}*{SimplE} & $|\mathcal{B}|$ & $2048$&$2048$&$1024$\\
& $d$ & $200$&$200$&$100$\\
& $lr$ &$1\mathrm{e}^{-3}$&$1\mathrm{e}^{-4}$&$1\mathrm{e}^{-3}$\\
& $\lambda$ & $1\mathrm{e}^{-1}$&$1\mathrm{e}^{-1}$&$1\mathrm{e}^{-1}$\\
& $\epsilon$ & $-1\mathrm{e}^{-1}$&$1\mathrm{e}^{-2}$&$1\mathrm{e}^{-2}$\\
\hline
\multirow{5}*{ConvE} & $|\mathcal{B}|$ & $512$&$128$&$512$\\
& $d$ & $200$&$200$&$200$\\
& $lr$ &$1\mathrm{e}^{-3}$&$1\mathrm{e}^{-3}$&$1\mathrm{e}^{-3}$\\
& $\lambda$ & $0$&$0$&$0$\\
& $\epsilon$ &$1\mathrm{e}^{-6}$&$1\mathrm{e}^{-5}$&$1\mathrm{e}^{-4}$\\
\hline
\multirow{5}*{TuckER} & $|\mathcal{B}|$ & $128$&$128$&$128$\\
& $d$ & $200$&$200$&$100$\\
& $lr$ &$1\mathrm{e}^{-3}$&$5\mathrm{e}^{-4}$&$1\mathrm{e}^{-3}$\\
& $\lambda$ & $0$&$0$&$0$\\
& $\epsilon$ &$1\mathrm{e}^{-6}$&$1\mathrm{e}^{-5}$&$1\mathrm{e}^{-5}$\\
\hline
\multirow{4}*{RGCN} & $d$ & $500$&$500$&$500$\\
& $lr$ &$1\mathrm{e}^{-2}$&$1\mathrm{e}^{-2}$&$1\mathrm{e}^{-2}$\\
& $\lambda$ & $1\mathrm{e}^{-2}$&$1\mathrm{e}^{-2}$&$1\mathrm{e}^{-2}$\\
& $\epsilon$ &$1\mathrm{e}^{-4}$&$1\mathrm{e}^{-5}$&$1\mathrm{e}^{-4}$\\
\hline
\end{tabular}
\end{table*}

\section{Modified Versions of $\mathcal{L}^{S}_{BCEL}$ and $\mathcal{L}^{S}_{PLL}$}
\label{modified-versions}
In Table~\ref{tab:epsilon-alpha}, we report results achieved with KGEMs trained under $\mathcal{L}^{S'}_{BCEL}$ and $\mathcal{L}^{S'}_{PLL}$, where the superscript $S'$ denotes a different way to include semantic information. In fact, $\mathcal{L}^{S'}_{PLL}$ makes use of the modified labelling function as used in $\mathcal{L}^{S}_{BCEL}$ and defined in Equation (5) of the paper. Conversely, $\mathcal{L}^{S'}_{BCEL}$ uses the binary (unmodified) labelling function $\ell$ but adopts the same procedure as $\mathcal{L}^{S}_{PLL}$: semantically valid negative triples are considered as positive with probability $\epsilon~\%$.
Hyperparameters for $\mathcal{L}^{S}_{BCEL}$ and $\mathcal{L}^{S}_{PLL}$ are reported in Table~\ref{tab:hyperparams}, while hyperparameters for $\mathcal{L}^{S'}_{BCEL}$ and $\mathcal{L}^{S'}_{PLL}$ are reported in Table~\ref{tab:hyperparams-alternative}.
Results achieved with all the aforementioned loss functions are provided in Table~\ref{tab:epsilon-alpha}. It shows that the semantic-driven loss functions presented in the paper are the most performing ones.

\begin{table*}[h]
        \caption{Rank-based and semantic-based results on FB15k187, DBpedia77k, and Yago14k. Bold fonts indicate which model performs best w.r.t. a given metric. Suffixes S and S' indicate whether the model is trained under the best (as presented in the paper) or the worst (as presented here) semantic-driven version of the loss function, respectively.}
	\label{tab:epsilon-alpha}
	\small
        \centering
                \setlength{\tabcolsep}{0.25cm}
			\begin{tabular}{lccccccccc}
                    \hline
				&\multicolumn{3}{c}{FB15k187} & \multicolumn{3}{c}{DBpedia77k} & \multicolumn{3}{c}{Yago14k} 
                \\\hline
                    & MRR & H@10 & S@10 &
                    MRR & H@10 & S@10 &
                    MRR & H@10 & S@10 \\
				\hline
    		  ComplEx-S &
    $\mathbf{.316}$&$\mathbf{.476}$&$\mathbf{.796}$& $\mathbf{.297}$&$\mathbf{.409}$&$\mathbf{.897}$& $\mathbf{.923}$&$\mathbf{.931}$&$\mathbf{.667}$ \\
    ComplEx-S' &
    $.227$&$.384$&$.777$& $.252$&$.350$&$.918$& $.907$&$.930$&$.603$ \\
    \hline
                SimplE-S &
    $\mathbf{.268}$&$\mathbf{.409}$&$.759$& $.230$&$\mathbf{.302}$&$\mathbf{.850}$& $\mathbf{.924}$&$\mathbf{.927}$&$\mathbf{.769}$ \\
    SimplE-S' &
    $.169$&$.288$&$\mathbf{.827}$& $.230$&$.297$&$.583$& $.885$&$.915$&$.290$ \\
    \hline
			ConvE-S &
    $\mathbf{.283}$&$\mathbf{.476}$&$\mathbf{.996}$& $\mathbf{.283}$&$\mathbf{.405}$&$\mathbf{.985}$& $.933$&$.940$&$\mathbf{.997}$  \\
    ConvE-S' &
    $.271$&$.472$&$.975$& $.273$&$.383$&$.935$& $.933$&$\mathbf{.941}$&$.894$  \\
    \hline
    		TuckER-S &
    $\mathbf{.320}$&$\mathbf{.522}$&$\mathbf{.996}$& $\mathbf{.312}$&$\mathbf{.421}$&$\mathbf{.969}$& $\mathbf{.931}$&$\mathbf{.943}$&$\mathbf{.929}$ \\
    TuckER-S' &
    $.316$&$.517$&$.983$& $.311$&$.412$&$.912$& $.918$&$.938$&$.867$ \\
    \hline
                RGCN-S &
    $\mathbf{.260}$&$\mathbf{.415}$&$\mathbf{.860}$& $\mathbf{.197}$&$\mathbf{.320}$&$\mathbf{.957}$& $\mathbf{.927}$&$\mathbf{.934}$&$\mathbf{.828}$ \\
    RGCN-S' &
    $.243$&$.391$&$.780$& $.146$&$.246$&$.862$& $.912$&$.922$&$.385$ \\
    \hline
			\end{tabular}
 \end{table*}

\section{Bucket Analysis}\label{appendix:bucket}
Relations are separated into three non-intersecting buckets : relations that feature narrow (B1), intermediate (B2), and large (B3) sets of semantically valid heads or tails, respectively. 
Cut-offs are manually defined for placing a given relation in its corresponding bucket. Such buckets are reported in Table~\ref{tab:cutoff}.
Results achieved on B1 are reported in the paper, while results for buckets B2 and B3 are shown for DBpedia77k, FB15k187, and Yago14k in Table~\ref{tab:db}, Table~\ref{tab:fb}, and Table~\ref{tab:yago}, respectively.

\begin{table*}[h]
\centering
\caption{Cut-offs for FB15k187, DBpedia77k, and Yago14k. B1, B2, and B3 denote the buckets of relations with narrow, intermediate, and large sets of semantically valid heads or tails, respectively. $|\mathcal{R}|$ denotes the number of unique relations in a given bucket and $|\text{Sem. Val}|$ indicates the interval of the number of semantically valid entities for the bucket relations. To illustrate, $|\text{Sem. Val}|$ = [11, 216] for the head side means that relations in the bucket have at least $11$ and at most $216$ semantically valid heads.}\label{tab:cutoff}
\setlength{\tabcolsep}{0.25cm}
\begin{tabular}{llcccccc}
\hline
Bucket&Side&\multicolumn{2}{c}{FB15k187} & \multicolumn{2}{c}{DBpedia77k} & \multicolumn{2}{c}{Yago14k}
\\
&&$|\text{Sem. Val}|$&$|\mathcal{R}|$ & $|\text{Sem. Val}|$&$|\mathcal{R}|$ & $|\text{Sem. Val}|$&$|\mathcal{R}|$ \\
\hline
\multirow{2}*{B1} &Head& $[11, 216]$ & $69$ & $[12, 930]$ & $62$ & $[93, 811]$ & $10$ \\
 &Tail& $[12, 244]$ & $80$ & $[19, 801]$ & $44$ & $[35, 678]$ & $13$ \\
\multirow{2}*{B2} &Head& $[278, 1391]$ & $55$ & $[1295, 11586]$ & $58$ & $[2102, 3624]$ & $15$ \\
 &Tail& $[278, 1391]$ & $49$ & $[1419, 11586]$ & $55$ & $[2102, 3624]$ & $16$ \\
\multirow{2}*{B3} &Head& $[1473, 4500]$ & $63$ & $[22252, 57242]$ & $25$ & $\{5730\}$ & $12$ \\
 &Tail& $[1473, 4500]$ & $58$ & $\{57242\}$ & $50$ & $\{5730\}$ & $8$ \\
\hline
\end{tabular}
\end{table*}

\begin{table*}[h]
    \caption{Rank-based and semantic-based results on DBpedia77k for buckets of relations that feature an intermediate (B2) and large (B3) set of semantically valid heads or tails. Bold fonts indicate which model performs best w.r.t. a given metric. Suffixes V and S indicate whether the model is trained under the vanilla or semantic-driven version of the loss function, respectively. Hits@$10$ and Sem@$10$ are abbreviated to H@$10$ and S@$10$.}
    \setlength{\tabcolsep}{0.175cm}
    \centering
    \vspace{3mm}
    \begin{tabular}{lcccccc}
    \hline
    &\multicolumn{3}{c}{B2} & \multicolumn{3}{c}{B3}
                    \\\hline
                    & MRR & H@10 & S@10 &
                    MRR & H@10 & S@10 \\
\hline
				TransE-V&$\mathbf{.450}$&$\mathbf{.607}$&$.838$&$\mathbf{.317}$&$\mathbf{.429}$&$.995$\\
                    TransE-S &$.404$&$.556$&$\mathbf{.987}$&$.300$&$.407$&$\mathbf{1}$ \\
    \hline
                    TransH-V &$\mathbf{.449}$&$\mathbf{.610}$&$.729$&$\mathbf{.311}$&$\mathbf{.425}$&$.971$ \\
                    TransH-S &$.423$&$.592$&$\mathbf{.981}$&$.296$&$.413$&$\mathbf{1}$ \\
    \hline
                    DistMult-V &$.446$&$.553$&$.669$&$.505$&$.413$&$.742$ \\
				DistMult-S &$\mathbf{.450}$&$\mathbf{.566}$&$\mathbf{.790}$&$\mathbf{.506}$&$\mathbf{.422}$&$\mathbf{.920}$ \\
    \hline
    ComplEx-V &$.442$&$.538$&$.551$&$\mathbf{.582}$&$\mathbf{.453}$&$.787$ \\
    			ComplEx-S &$\mathbf{.448}$&$\mathbf{.545}$&$\mathbf{.707}$&$.505$&$.426$&$\mathbf{.975}$ \\
    \hline
                    SimplE-V &$\mathbf{.381}$&$\mathbf{.461}$&$\mathbf{.716}$&$\mathbf{.485}$&$\mathbf{.357}$&$.954$ \\
                    SimplE-S &$.350$&$.404$&$.649$&$.386$&$.276$&$\mathbf{.960}$ \\
    \hline
				ConvE-V &$.388$&$.535$&$.890$&$\mathbf{.489}$&$.371$&$.960$ \\
				ConvE-S&$\mathbf{.429}$&$\mathbf{.559}$&$\mathbf{.977}$&$.450$&$\mathbf{.399}$&$\mathbf{.999}$  \\
    \hline
    			TuckER-V &$.438$&$.547$&$.874$&$\mathbf{.591}$&$.436$&$.898$ \\
                   TuckER-S &$\mathbf{.444}$&$\mathbf{.568}$&$\mathbf{.923}$&$.564$&$\mathbf{.444}$&$\mathbf{.983}$ \\
    \hline
                    RGCN-V &$\mathbf{.282}$&$.413$&$.670$&$\mathbf{.367}$&$.322$&$.971$ \\
                    RGCN-S &$.275$&$\mathbf{.423}$&$\mathbf{.861}$&$.362$&$\mathbf{.357}$&$\mathbf{.999}$ \\
    \hline
    \end{tabular}
    \label{tab:db}
\end{table*}

\begin{table*}[h]
    \caption{Rank-based and semantic-based results on FB15k187 for the buckets of relations that feature an intermediate (B2) and large (B3) set of semantically valid heads or tails. Bold fonts indicate which model performs best w.r.t. a given metric. Suffixes V and S indicate whether the model is trained under the vanilla or semantic-driven version of the loss function, respectively. Hits@$10$ and Sem@$10$ are abbreviated to H@$10$ and S@$10$.}
    \setlength{\tabcolsep}{.25cm}
    \centering
    \small
    \begin{tabular}{lcccccc}    
    \hline
    &\multicolumn{3}{c}{B2} & \multicolumn{3}{c}{B3}
                    \\\hline
                    & MRR & H@10 & S@10 &
                    MRR & H@10 & S@10 \\
\hline
				TransE-V&$.330$&$.526$&$.934$&$.141$&$.255$&$.953$\\
                    TransE-S &$\mathbf{.385}$&$\mathbf{.588}$&$\mathbf{.972}$&$\mathbf{.169}$&$\mathbf{.290}$&$\mathbf{.993}$ \\
    \hline
                    TransH-V &$.330$&$.517$&$.846$&$.161$&$.262$&$.963$ \\
                    TransH-S &$\mathbf{.380}$&$\mathbf{.590}$&$\mathbf{.967}$&$\mathbf{.171}$&$\mathbf{.291}$&$\mathbf{.993}$ \\
    \hline
                    DistMult-V &$.336$&$.527$&$.780$&$.177$&$.274$&$.946$ \\
				DistMult-S &$\mathbf{.388}$&$\mathbf{.579}$&$\mathbf{.962}$&$\mathbf{.187}$&$\mathbf{.309}$&$\mathbf{.995}$ \\
    \hline
    ComplEx-V &$.327$&$.476$&$.318$&$\mathbf{.197}$&$.306$&$.717$ \\
    			ComplEx-S &$\mathbf{.351}$&$\mathbf{.537}$&$\mathbf{.769}$&$.191$&$\mathbf{.310}$&$\mathbf{.942}$ \\
    \hline
                    SimplE-V &$.283$&$.432$&$.331$&$\mathbf{.179}$&$\mathbf{.274}$&$.694$ \\
                    SimplE-S &$.283$&$\mathbf{.448}$&$\mathbf{.671}$&$.159$&$.243$&$\mathbf{.923}$ \\
    \hline
				ConvE-V &$.347$&$.529$&$.974$&$.172$&$.277$&$.977$ \\
				ConvE-S&$\mathbf{.354}$&$\mathbf{.543}$&$\mathbf{.998}$&$\mathbf{.188}$&$\mathbf{.283}$&$\mathbf{.999}$  \\
    \hline
    			TuckER-V &$.387$&$.574$&$.987$&$.215$&$.330$&$.994$ \\
                   TuckER-S &$\mathbf{.396}$&$\mathbf{.585}$&$\mathbf{.991}$&$\mathbf{.222}$&$\mathbf{.337}$&$\mathbf{.997}$ \\
    \hline
                    RGCN-V &$.294$&$.448$&$.749$&$.140$&$.222$&$.954$ \\
                    RGCN-S &$\mathbf{.303}$&$\mathbf{.474}$&$\mathbf{.869}$&$\mathbf{.157}$&$\mathbf{.229}$&$\mathbf{.974}$ \\
    \hline
    \end{tabular}
    \label{tab:fb}
\end{table*}

\begin{table*}[h]
    \caption{Rank-based and semantic-based results on Yago14k for the buckets of relations that feature an intermediate (B2) and large (B3) set of semantically valid heads or tails. Bold fonts indicate which model performs best w.r.t. a given metric. Suffixes V and S indicate whether the model is trained under the vanilla or semantic-driven version of the loss function, respectively. Hits@$10$ and Sem@$10$ are abbreviated to H@$10$ and S@$10$.}
    \centering
    \small
    \setlength{\tabcolsep}{.25cm}
    \begin{tabular}{lcccccc}
    \hline
    &\multicolumn{3}{c}{B2} & \multicolumn{3}{c}{B3}
                    \\\hline
                    & MRR & H@10 & S@10 &
                    MRR & H@10 & S@10 \\
\hline
				TransE-V&$\mathbf{.879}$&$\mathbf{.928}$&$.892$&$.841$&$\mathbf{.923}$&$.974$\\
                    TransE-S &$.861$&$.922$&$\mathbf{.997}$&$\mathbf{.854}$&$.917$&$\mathbf{1}$ \\
    \hline
                    TransH-V &$.854$&$\mathbf{.922}$&$.567$&$\mathbf{.788}$&.92&$.803$ \\
                    TransH-S &$\mathbf{.865}$&$.921$&$\mathbf{.876}$&$.778$&$\mathbf{.926}$&$\mathbf{.996}$ \\
    \hline
                    DistMult-V &$.852$&$\mathbf{.915}$&$\mathbf{.443}$&$.941$&$.911$&$.536$ \\
				DistMult-S &$\mathbf{.862}$&$.911$&$.441$&$.941$&$.911$&$\mathbf{.584}$ \\
    \hline
    ComplEx-V &$\mathbf{.883}$&$\mathbf{.921}$&$.352$&$\mathbf{.932}$&$\mathbf{.914}$&$.619$ \\
    			ComplEx-S &$.881$&$.918$&$\mathbf{.738}$&$.922$&$.914$&$\mathbf{.964}$ \\
    \hline
                    SimplE-V &$.882$&$.915$&$.378$&$\mathbf{.932}$&$\mathbf{.914}$&$.656$ \\
                    SimplE-S &$\mathbf{.883}$&$\mathbf{.918}$&$\mathbf{.841}$&$.930$&$.905$&$\mathbf{.991}$ \\
    \hline
				ConvE-V &$\mathbf{.893}$&$\mathbf{.928}$&$.858$&$\mathbf{.941}$&$.917$&$.904$ \\
				ConvE-S&$.892$&$.925$&$\mathbf{.931}$&$.939$&$\mathbf{.923}$&$\mathbf{.956}$  \\
    \hline
    			TuckER-V &$.884$&$.928$&$.791$&$.941$&$.917$&$.915$ \\
                   TuckER-S &$\mathbf{.894}$&$\mathbf{.935}$&$\mathbf{.930}$&$\mathbf{.942}$&$.917$&$\mathbf{.983}$ \\
    \hline
                    RGCN-V &$.880$&$.894$&$.380$&$.930$&$.898$&$.670$ \\
                    RGCN-S &$\mathbf{.892}$&$\mathbf{.925}$&$\mathbf{.953}$&$\mathbf{.942}$&$\mathbf{.923}$&$\mathbf{.996}$ \\
    \hline
    \end{tabular}
    \label{tab:yago}
\end{table*}


\clearpage

\bibliography{aaai24}
%\bibliographystyle{aaai24}

\end{document}
