\section{Preliminaries}
\label{sec:preliminaries}

\subsection{Preferences}

We denote by $C = \set{\enum{c}}$ a set of \e{items} or \e{candidates} (used interchangeably.) For any $a, b, c \in C$, preference is a binary relation $\succ$ that is transitive ($a \succ b$ and $b \succ c$ imply $a \succ c$), irreflexive ($a \not\succ a$), and asymmetric ($a \succ b$ implies $b \not\succ a$).
A \e{preference pair} is an instance of this relation.

A (strict) \e{partial order} or \e{poset} is a set of preference pairs that corresponds to a directed acyclic graph with edges being the preference pairs.
For example, $\partialOrder_0 = \set{c_1 \succ c_2, c_1 \succ c_3}$ states that candidate $c_1$ is preferred to both $c_2$ and $c_3$.

A \e{ranking} or \e{permutation} is a list $\ranking = \angs{\enum{\tau}}$ such that $\forall i < j, \tau_i \succ \tau_j$.
It defines a bijection between the candidates and the ranks: $\ranking(i) = \tau_i$ and $\ranking^{-1}(\tau_i) = i$.

The \e{linear extensions} of a partial order $\partialOrder$, denoted by $\Omega(\partialOrder)$, is a set of rankings consistent with $\partialOrder$.
For example, $C_0 = \set{c_1, c_2, c_3}$ and $\partialOrder_0 = \set{c_1 \succ c_2, c_1 \succ c_3}$, then $\Omega(\partialOrder_0) = \set{\angs{c_1, c_2, c_3}, \angs{c_1, c_3, c_2}}$.

\subsection{Voting and Winners}
\label{sec:preliminaries:vote}

We denote by $V = \set{\enum[n]{v}}$ a set of voters and by $\completeVP = (\enum[n]{\ranking})$ a \e{(complete) voting profile} where $\ranking_i$ is a ranking over $C$ by voter $v_i$.

Among various voting rules, the \e{positional scoring rules} are arguably the most thoroughly studied.
Let $\vr_m = (\vr_m(1), ..., \vr_m(m))$ denote a positional scoring rule where $\forall 1 \leq i < j \leq m, \vr_m(i) \geq \vr_m(j)$ and $\vr_m(1) > \vr_m(m)$.
It assigns a score $\vr_m(i)$ to the candidate at rank $i$.  The performance of a candidate $c$ is the sum of her scores across the voting profile $\completeVP$:
$\score(c, \completeVP) = \sum_{\ranking \in \completeVP}{\score(c, \ranking)}$ where $\score(c, \ranking) = \vr_m(\ranking^{-1}(c))$.
Candidate $w$ is a (co-)winner if her score is not less than that of any other candidate.
We use $\winners(\vr_m, \completeVP)$ to denote the set of co-winners.

\begin{table}[tb!]
  \caption{Examples of positional scoring rules.}
  \begin{tabular}{@{}ll@{}}
    \toprule
    Voting rule          & $\vr_m$                    \\ \midrule
    \textit{plurality}   & $(1, 0, \ldots, 0, 0)$     \\
    \textit{veto}        & $(1, 1, \ldots, 1, 0)$     \\
    \textit{2-approval } & $(1, 1, 0, \ldots, 0, 0)$  \\
    \textit{Borda}       & $(m-1, m-2, \ldots, 1, 0)$ \\ \bottomrule
  \end{tabular}
  \centering
  \label{tab:voting_rules}
\end{table}

\begin{example}
  \rev{Given the voting profile in Table~\ref{tab:complete_profile}, under the Borda rule, \txt{Biden} obtains 3 points from \txt{Ann} and \txt{Dave}, and 0 point from \txt{Bob}, which makes him the winner with a total score of 6.}
\end{example}

\begin{table}[b!]
  \centering
  \caption{\rev{A voting profile of 3 voters over 4 candidates.}}
  \begin{tabular}{@{}ll@{}}
    \toprule
    voter      & ranking                                       \\ \midrule
    \txt{Ann}  & $\angs{\txt{Biden, Sanders, Weld, Trump}}$ \\
    \txt{Bob}  & $\angs{\txt{Trump, Weld, Sanders, Biden}}$ \\
    \txt{Dave} & $\angs{\txt{Biden, Sanders, Weld, Trump}}$ \\ \bottomrule
  \end{tabular}
  \label{tab:complete_profile}
\end{table}

\paragraph{\mPw.}

Voters may only partially express their preferences.
Let $\partialVP = (\enum[n]{\partialOrder})$ denote a \e{partial voting profile} of $n$ partial orders.
Recall that $\Omega(\partialOrder)$ is the set of \e{linear extensions} of $\partialOrder$, which are also called the \e{completions} of $\partialOrder$.
A complete voting profile $\completeVP = (\enum[n]{\ranking})$ is a \e{completion} or a \e{possible world} of a partial voting profile $\partialVP = (\enum[n]{\partialOrder})$ if $\forall \ranking_i \in \completeVP, \ranking_i \in \Omega(\partialOrder_i)$.
Let $\Omega(\partialVP) = \set{\enum[\numPW]{\completeVP}}$ denote the set of completions of $\partialVP$.
Assume that $\partialVP$ represents a uniform distribution of its possible worlds.
The \mPw (\mpw)~\cite{DBLP:conf/aaai/BachrachBF10,DBLP:journals/ai/HazonAKW12} is defined as follows.

\begin{definition} [\mpw] \label{def:mpw}
  Given a partial voting profile $\partialVP$ and a positional scoring rule $\vr_m$, candidate $w \in C$ is the \mPw (\mpw), \ifff, $\Pr(w \mid \partialVP) = \max_{c \in C} \Pr(c \mid \partialVP)$ where $\Pr(c \mid \partialVP) = \sum_{\completeVP \in \Omega(\partialVP)} \mathds{1}(c \in \winners(\vr_m, \completeVP)) \cdot \Pr(\completeVP \mid \partialVP)$.
\end{definition}

A candidate is the \mpw if her chance of winning in a random possible world is no less than any other candidate.

\begin{example}\label{eg:nw_pw}
  \rev{Given the partial voting profile in Table~\ref{tab:partial_profile}, we first construct its completions listed in Table~\ref{tab:completions_of_partial_profile}. Under the plurality rule, \txt{Biden} is the only winner in completion No.1, while completion No.2 has winners \txt{Biden}, \txt{Sanders} and \txt{Trump}. Both completions have the probability 0.5. Thus, Biden is the \mpw with winning probability 1, while the winning probability for \txt{Sanders} and \txt{Trump} is 0.5.}
\end{example}

\begin{table}[t!]
  \centering
  \caption{\rev{A partial voting profile of 3 voters over 4 candidates. \txt{Ann} gives a partial order, while \txt{Bob} and \txt{Dave} give rankings.}}
  \begin{tabular}{@{}ll@{}}
    \toprule
    voter      & partial order                                 \\ \midrule
    \txt{Ann}  & $\set{\txt{Biden} \succ \txt{Weld}, \txt{Sanders} \succ \txt{Weld}, \txt{Weld} \succ \txt{Trump}}$  \\
    \txt{Bob}  & $\angs{\txt{Trump, Weld, Sanders, Biden}}$ \\
    \txt{Dave} & $\angs{\txt{Biden, Sanders, Weld, Trump}}$ \\ \bottomrule
  \end{tabular}
  \label{tab:partial_profile}
\end{table}

\begin{table}[t!]
  \centering
  \caption{\rev{The two completions of the voting profile in Table~\ref{tab:partial_profile}.}}
  \begin{tabular}{@{}lll}
    \toprule
    completion & voter      & ranking                                       \\ \midrule
    & \txt{Ann}  & $\angs{\txt{Biden, Sanders, Weld, Trump}}$ \\
    No. 1      & \txt{Bob}  & $\angs{\txt{Trump, Weld, Sanders, Biden}}$ \\
    & \txt{Dave} & $\angs{\txt{Biden, Sanders, Weld, Trump}}$ \\ \midrule
    & \txt{Ann}  & $\angs{\txt{Sanders, Biden, Weld, Trump}}$ \\
    No. 2      & \txt{Bob}  & $\angs{\txt{Trump, Weld, Sanders, Biden}}$ \\
    & \txt{Dave} & $\angs{\txt{Biden, Sanders, Weld, Trump}}$ \\ \bottomrule
    &            &
  \end{tabular}
  \label{tab:completions_of_partial_profile}
\end{table}

\subsection{Preference Models}
\label{sec:preliminaries:pref}

\paragraph{Repeated Insertion Model (RIM)}

Doignon \etal \cite{Doignon2004} proposed a generative model $\RIM(\bsigma, \Pi)$ that defines a probability distribution over permutations.  It is parameterized by a reference ranking $\bsigma=\angs{\enum{\sigma}}$ and a probability function $\Pi$ where $\Pi(i, j)$ is the probability of inserting $\sigma_i$ at position $j$.
Algorithm~\ref{alg:rim} presents the RIM generative procedure.
It starts with an empty ranking, inserts items in the order of $\bsigma$, and places $\sigma_i$ at the $j^{th}$ position of the incomplete ranking $\ranking$ with probability $\Pi(i, j)$.

\begin{algorithm}[tb!]
\small 
  \raggedright
  \caption{Repeated Insertion Model}
  \label{alg:rim}
  \textbf{Input}: $\RIM(\bsigma, \Pi)$ where $|\bsigma| = m$ \\
  \textbf{Output}:  Ranking $\ranking$
  \begin{algorithmic}[1]
    \STATE Initialize an empty ranking $\ranking = \angs{}$.
    \FOR {$i = 1, \ldots, m$}
      \STATE Insert $\bsigma(i)$ into $\ranking$ at $j \in [1, i]$ with probability $\Pi(i, j)$.
    \ENDFOR
    \RETURN  $\ranking$
  \end{algorithmic}
\end{algorithm}

\begin{example}
    $\RIM(\angs{a, b, c}, \Pi)$ generates $\ranking' {=} \angs{c, a, b}$ as follows.
    Initialize $\ranking_0 {=} \angs{}$. 
    When $i=1$, $\ranking_1 {=} \angs{a}$ by inserting $a$ into $\ranking_0$ with probability $\Pi(1,1)$. 
    When $i=2$, $\ranking_2 {=} \angs{a, b}$ by inserting $b$ into $\ranking_1$ at position 2 with probability $\Pi(2,2)$.
    When $i=3$, $\ranking' {=} \angs{c, a, b}$ by inserting $c$ into $\ranking_2$ at position 1 with probability $\Pi(3,1)$. 
    Overall, $\Pr(\ranking' \mid \angs{a,b,c}, \Pi) {=} \Pi(1,1) \cdot \Pi(2,2) \cdot \Pi(3,1)$.
\end{example}

\paragraph{RIM Inference and Cover Width.}

Given $\RIM(\bsigma, \Pi)$ and a partial order $\partialOrder$, RIM inference is used to calculate the marginal probability of $\partialOrder$ over $\RIM(\bsigma, \Pi)$, or, equivalently, the probability that a sample from $\RIM(\bsigma, \Pi)$ is a linear extension of $\partialOrder$.

RIMDP~\cite{DBLP:conf/aaai/KenigIPKS18} is a RIM inference solver that uses Dynamic Programming (DP).
It runs the RIM insertion procedure (Algorithm~\ref{alg:rim}) to generate all possible rankings that satisfy the partial order. Given $m$ items, there are potentially $m!$ permutations to generate.
RIMDP prunes the space based on an insight that it is only necessary to track the positions of items directly related to some not-yet-inserted item in the partial order.
Two items $\set{a, b}$ are \e{directly related} and $a$ is a \e{cover} of $b$, \ifff $a \succ b$ and $\nexists c \in C, a \succ c \succ b$.
As a DP approach, the states of RIMDP are mappings from tracked items to their positions.
The maximum number of tracked items during RIM procedure is called the \e{cover width}, denoted $\text{cw}(\bsigma, \partialOrder)$.
The complexity of RIMDP is $O(m^{\text{cw}(\bsigma, \partialOrder)+2})$. 
RIMDP runs in polynomial time for partial orders whose cover widths are bounded.

\begin{example}
    Let $\bsigma=\angs{\enum[9]{\sigma}}$ and $\partialOrder=\set{\sigma_3 \succ \sigma_5, \sigma_5 \succ \sigma_8}$.
    RIMDP does not track the positions of $\sigma_1$ and $\sigma_2$ since they are not directly related to any item in $\partialOrder$.
    The insertion of $\sigma_3$ generates 3 states: $\set{\sigma_3 \rightarrow 1}$, $\set{\sigma_3 \rightarrow 2}$, and $\set{\sigma_3 \rightarrow 3}$.
    The position of $\sigma_3$ is the boundary for $\sigma_5$.
    After inserting $\sigma_5$, RIMDP will stop tracking $\sigma_3$ by merging all states sharing the same position of $\sigma_5$, \eg merging $\set{\sigma_3 \rightarrow 1, \sigma_5 \rightarrow 3}$ and $\set{\sigma_3 \rightarrow 2, \sigma_5 \rightarrow 3}$ into $\set{\sigma_5 \rightarrow 3}$.
    RIMDP will keep tracking $\sigma_5$ until $\sigma_8$ is inserted.
    The cover width in this example is 1, since RIMDP tracks at most 1 item for each insertion.
\end{example}

\begin{algorithm}[b!]
	\small 
	\raggedright
	\caption{Repeated Selection Model}
	\label{alg:rsm}
	\textbf{Input}: $\mathsf{RSM}(\bsigma, \Pi, p)$ where $|\bsigma|=m$ \\
	\textbf{Output}:  Partial order $\partialOrder$
	\begin{algorithmic}[1]
		\STATE Initialize an empty partial order $\partialOrder = \set{}$.
		\FOR {$i = 1, \ldots, m-1$}
		\STATE Sample $j \in [1, m-i+1]$ with probability $\Pi(i, j)$.
		\STATE Obtain item $\sigma = \bsigma(j)$ and remove it from $\bsigma$.
		\FOR {$k = 1, \ldots, m-i$}
		\STATE Add $\sigma \succ \bsigma(k)$ into $\partialOrder$ with probability $p(i)$.
		\ENDFOR
		\ENDFOR
		\RETURN  $\partialOrder$
	\end{algorithmic}
\end{algorithm}

\paragraph{Repeated Selection Model (RSM)}

Algorithm~\ref{alg:rsm} presents a generative model for partial orders called $\mathsf{RSM}(\bsigma, \Pi, p)$~\cite{DBLP:journals/tdasci/ChakrabortyDKKR21}.
The model is parameterized by a reference ranking $\bsigma$, a probability function $\Pi$ where $\Pi(i, j)$ is the probability of the $j^{th}$ item selected at step $i$, and a probability function $p:\set{1,...,m-1} \rightarrow [0,1]$ determining whether the selected item is preferred to each remaining item.
In contrast to RIM that randomizes insertion positions, RSM randomizes the insertion order.
%
In this paper, we will use RSM with $p \equiv 1$, such that it only outputs rankings, and will denote it  $\RSM(\bsigma, \Pi)$.


\paragraph{Mallows Model.}

Denoted by $\mallows(\bsigma, \phi)$, where $0 \leq \phi \leq 1$, the Mallows model~\cite{Mallows1957} defines a probability distribution over rankings: reference ranking $\bsigma$ at the center and other rankings closer to $\bsigma$ having higher probabilities.
For a given ranking $\ranking$, $\Pr(\ranking|\bsigma, \phi) \propto \phi^{D(\bsigma, \ranking)}$, where $D(\bsigma, \ranking) = |{(a, a') | a \succ_{\bsigma} a', a' \succ_{\ranking} a}|$ is the Kendall-tau distance counting the number of disagreed preference pairs.
If $\phi=1$, the Mallows becomes a uniform distribution.
The $\mallows(\bsigma, \phi)$ is a special case for both $\RIM(\bsigma, \Pi)$ by $\Pi(i, j) = \frac{\phi^{i-j}}{1+\phi+...+\phi^{i-1}}$ and $\RSM(\bsigma, \Pi)$ by $\Pi(i, j) = \frac{\phi^{j-1}}{1+\phi+...+\phi^{m-i}}$.
