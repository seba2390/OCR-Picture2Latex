\appendix

\section{Additional proofs}
\label{sec:appendix:proofs}

\begin{reptheorem}{\ref{theorem:least_expected_regret_winner}}
    \theoremLeastExpectedRegretWinner
\end{reptheorem}

\begin{proof}
    Let $\probaVP$ denote a general voting profile with $\Omega(\probaVP)=\set{\enum[\numPW]{\VP}}$.
    The expected regret of a candidate $w$ can be rewritten as follows.
    
    \begin{align*}
        & \mathds{E}(\txt{Regret}(w, \probaVP)) \\
        =& \sum_{i=1}^z \txt{Regret}(c, \completeVP_i) \cdot \Pr(\completeVP_i \mid \probaVP) \\
        =& \sum_{i=1}^z \bigg( \max_{c \in C} \score(c, \completeVP_i) - \score(w, \completeVP_i) \bigg) \cdot \Pr(\completeVP_i \mid \probaVP) \\
        =& \sum_{i=1}^z \max_{c \in C} \score(c, \completeVP_i) \cdot \Pr(\completeVP_i \mid \probaVP)  - \mathds{E}(\score(w, \probaVP))\\
    \end{align*}
    
    The first term $\sum_{i=1}^z \max_{c \in C} \score(c, \completeVP_i) \cdot \Pr(\completeVP_i \mid \probaVP)$ is a constant value, when $\probaVP$ and the voting rule are fixed.
    Thus, $\mathds{E}(\txt{Regret}(w, \probaVP))$ is minimized by maximizing $\mathds{E}(\score(w, \probaVP))$, the expected score of the candidate $w$.
\end{proof}

\begin{reptheorem}{\ref{theorem:gigantic_election_winner}}
    \theoremGiganticElectionWinner
\end{reptheorem}

\begin{proof}
    Let $\probaVP$ denote a general voting profile with $\Omega(\probaVP)=\set{\enum[\numPW]{\VP}}$, and $\completeVP_{meta} = (\completeVP_1, \ldots, \completeVP_\numPW)$ denote the large meta profile where rankings in $\completeVP_i$ are weighted by $\Pr(\completeVP_i \mid \probaVP)$.
    According to the definition of the Meta-Election Winner, $\score(w, \completeVP_{meta}) = \max_{c \in C} \score(c, \completeVP_{meta})$.
    As a result, for any candidate $c$,
    \[
    \mathds{E}(\score(c, \probaVP)) = \sum_{\completeVP \in \Omega(\probaVP)} \score(c, \completeVP) \cdot \Pr(\completeVP \mid \probaVP) = \score(c, \completeVP_{meta})
    \]
    Her expected score in $\probaVP$ is precisely her score in $\completeVP_{meta}$.
    The two winner definitions are optimizing the same metric.
\end{proof}

\begin{reptheorem}{\ref{theorem:FCP_shaPcomplete_over_partialOrders}}
    \theoremFCPhardnessOverPartialOrders
\end{reptheorem}

\begin{proof}
    First, we prove its membership in \#P.
    The FCP is the counting version of the following decision problem: given a partial order $\partialOrder$, an item $c$, and an integer $j$, determine whether $\partialOrder$ has a linear extension $\ranking \in \Omega(\partialOrder)$ where $c$ is ranked at $j$.
    This decision problem is obviously in NP, meaning that the FCP is in \#P.
    
    Then, we prove that the FCP is \#P-hard by reduction.
    Recall that counting $|\Omega(\partialOrder)|$, the number of linear extensions of a partial order $\partialOrder$, is \#P-complete~\cite{DBLP:conf/stoc/BrightwellW91}.
    This problem can be reduced to the FCP by $|\Omega(\partialOrder)| = \sum_{j=1}^{m}{N(c@j \mid \partialOrder)}$.
    
    In conclusion, the FCP is \#P-complete.
\end{proof}

\begin{replemma}{\ref{lemma:REPt_shaPcomplete_over_partialOrders}}
    \lemmaREPtHardnessOverPartialOrders
\end{replemma}

\begin{proof}
    First, we prove that the REP-t is in FP$^{\#P}$.
    Recall that $\Omega(\partialOrder)$ is the linear extensions of a partial order $\partialOrder$, and $N(c@1 {\mid} \partialOrder)$ is the number of linear extensions in $\Omega(\partialOrder)$ where candidate $c$ is at rank 1.
    Then $\Pr(c@1 {\mid} \partialOrder) = N(c@1 {\mid} \partialOrder) / |\Omega(\partialOrder)|$.
    Consider that counting $N(c@1 \mid \partialOrder)$ is in \#P (Theorem~\ref{theorem:FCP_shaPcomplete_over_partialOrders}) and counting $|\Omega(\partialOrder)|$ is \#P-complete~\cite{DBLP:conf/stoc/BrightwellW91}, so $\Pr(c@1 {\mid} \partialOrder)$ is in FP$^{\#P}$.
    
    In the rest of this proof, we prove that the REP-t is \#P-hard by reduction from the \#P-complete problem of counting $|\Omega(\partialOrder)|$.
    
    Let $c^*$ denote an item that has no parent in $\partialOrder$.
    Let $\partialOrder_{-c^*}$ denote the partial order of $\partialOrder$ with item $c^*$ removed.
    If we are interested in the probability that $c^*$ is placed at rank 1, we can write $\Pr(c^*@1 {\mid} \partialOrder) = N(c^*@1 \mid \partialOrder) / |\Omega(\partialOrder)|$.
    The item $c^*$ has been fixed at rank 1, so any placement of the rest items will definitely satisfy any relative order involving $c^*$.
    That is to say, the placement of the rest items just needs to satisfy $\partialOrder_{-c^*}$, which leads to $N(c^*@1 \mid \partialOrder) = |\Omega(\partialOrder_{-c^*})|$.
    
    For example, let $\partialOrder' = \set{c_1 \succ c_4, c_2 \succ c_4, c_3 \succ c_4}$.   Then $N(c_1@1 \mid \partialOrder') = |\Omega(\partialOrder'_{-c_1})| = |\Omega(\set{c_2 \succ c_4, c_3 \succ c_4})|$.
    Then, we re-write $\Pr(c^*@1 {\mid} \partialOrder) = N(c^*@1 {\mid} \partialOrder) / |\Omega(\partialOrder)| = |\Omega(\partialOrder_{-c^*})| / |\Omega(\partialOrder)|$.
    The oracle for $\Pr(c^*@1 {\mid} \partialOrder)$ manages to reduce the size of the counting problem from $|\Omega(\partialOrder)|$ to $|\Omega(\partialOrder_{-c^*})|$.
    This oracle should be as hard as counting $|\Omega(\partialOrder)|$.
    Thus calculating $\Pr(c^*@1 {\mid} \partialOrder)$ is FP$^{\#P}$-hard.
    
    In conclusion, the REP-t is FP$^{\#P}$-complete.
\end{proof}

\begin{replemma}{\ref{lemma:REPb_shaPcomplete_over_partialOrders}}
    \lemmaREPbHardnessOverPartialOrders
\end{replemma}

\begin{proof}
    This proof adopts the same approach as the proof of Lemma~\ref{lemma:REPt_shaPcomplete_over_partialOrders}.
    
    Let $m$ be the number of items in the ranking model $\model$.
    For the membership proof that the REP-b is in FP$^{\#P}$, let $N(c@m {\mid} \partialOrder)$ denote the number of linear extensions in $\Omega(\partialOrder)$ where candidate $c$ is at the bottom rank $m$.
    Then $\Pr(c@m {\mid} \partialOrder) = N(c@m {\mid} \partialOrder) / |\Omega(\partialOrder)|$.
    Consider that counting $N(c@m {\mid} \partialOrder)$ is in \#P (Theorem~\ref{theorem:FCP_shaPcomplete_over_partialOrders}) and counting $|\Omega(\partialOrder)|$ is \#P-complete~\cite{DBLP:conf/stoc/BrightwellW91}, so $\Pr(c@m \mid \partialOrder)$ is in FP$^{\#P}$.
    
    In the proof of Lemma~\ref{lemma:REPt_shaPcomplete_over_partialOrders}, item $c^*$ is an item with no parent in the partial order $\partialOrder$.
    In the current proof, item $c^*$ is set to be an item with no child in $\partialOrder$.
    The $\partialOrder_{-c^*}$ still denotes the partial order of $\partialOrder$ but with item $c^*$ removed.
    Then the probability that item $c^*$ at the bottom rank $m$ is $\Pr(c^*@m \mid \partialOrder) = N(c^*@m \mid \partialOrder) / |\Omega(\partialOrder)| = |\Omega(\partialOrder_{-c^*})| / |\Omega(\partialOrder)|$.
    The oracle for $\Pr(c^*@m \mid \partialOrder)$ manages to reduce the size of the counting problem again from $|\Omega(\partialOrder)|$ to $|\Omega(\partialOrder_{-c^*})|$.
    Thus, this oracle is \#P-hard, and calculating $\Pr(c^*@m \mid \partialOrder)$ is FP$^{\#P}$-hard.
    
    In conclusion, the REP-b is FP$^{\#P}$-complete.
\end{proof}

\begin{reptheorem}{\ref{theorem:REP_shaPcomplete_over_partialOrders}}
    \theoremRepHardnessOverPartialOrders
\end{reptheorem}

\begin{proof}
    First, we prove that the REP is in FP$^{\#P}$.
    Recall that $\Omega(\partialOrder)$ is the linear extensions of a partial order $\partialOrder$, and $N(c@j {\mid} \partialOrder)$ is the number of linear extensions in $\Omega(\partialOrder)$ where candidate $c$ is at rank $j$.
    Then $\Pr(c@j {\mid} \partialOrder) = N(c@j {\mid} \partialOrder) / |\Omega(\partialOrder)|$.
    Consider that counting $N(c@j {\mid} \partialOrder)$ is \#P-complete (Theorem~\ref{theorem:FCP_shaPcomplete_over_partialOrders}) and counting $|\Omega(\partialOrder)|$ is \#P-complete~\cite{DBLP:conf/stoc/BrightwellW91} as well.
    So $\Pr(c@j {\mid} \partialOrder)$ is in FP$^{\#P}$.
    
    Lemma~\ref{lemma:REPt_shaPcomplete_over_partialOrders} demonstrates that REP-t, a special case of REP, is FP$^{\#P}$-hard.
    Thus REP is \#P-hard as well.
    
    In conclusion, REP is FP$^{\#P}$-complete.
\end{proof}

\begin{reptheorem}{\ref{theorem:reduction}}
    \theoremReductionEscToRep
\end{reptheorem}

\begin{proof}
    Recall that the \mew $w$ maximizes the expected score, \ie
    \[
    \score(w, \probaVP) = \max_{c \in C}\mathds{E}(\score(c, \probaVP))
    \]
    The voting profile $\probaVP$ contains $n$ ranking distributions $\set{\enum[n]{\model}}$, so
    \[
    \mathds{E}(\score(c, \probaVP)) = \sum_{i=1}^{n} {\mathds{E}(\score(c, \model_i))}
    \]
    where $\mathds{E}(\score(c, \model_i))$ is the expected score of $c$ from voter $v_i$.
    \[
    \mathds{E}(\score(c, \model_i)) = \sum_{j=1}^{m} Pr(c@j \mid \model_i) \cdot \vr_m(j)
    \]
    where $c@j$ denotes candidate $c$ at rank $j$, and $\vr_m(j)$ is the score of rank $j$.
    
    Let $\mathds{T}$ denote the complexity of calculating $Pr(c@j {\mid} \model_i)$.
    The original \mew problem can be solved by calculating $Pr(c@j {\mid} \model_i)$ for all $m$ candidates, $m$ ranks and $n$ voters, which leads to the complexity of $O(n\cdot m^2 \cdot \mathds{T})$.
\end{proof}

\begin{reptheorem}{\ref{theorem:REP_MEW_equivalent_under_k_approval}}
    \theoremRepMewEquivalentUnderKapproval
\end{reptheorem}

\begin{proof}
    The \esc problem has been reduced to the REP (Theorem~\ref{theorem:reduction}).
    This proof will focus on the other direction, \ie reducing the REP to the \esc problem.
    
    Let $\Pr(c@j \mid \model)$ denote the probability of placing candidate $c$ at rank $j$ over a ranking distribution $\model$.
    Let $\VP^{\model}$ denote a single-voter profile consisting of only this ranking distribution $\model$.
    
    When $k=1$, the REP can be reduced to solving the \esc problem under plurality or $1$-approval rule.
    \[
    \Pr(c@1 \mid \model) = \mathds{E}(\score(c \mid \VP^{\model}, 1\text{-approval}))
    \]
    
    When $k=m$, the REP can be reduced to solving the \esc problem under veto or $(m {-} 1)$-approval rule.
    \[
    \Pr(c@m \mid \model) = 1 - \mathds{E}(\score(c \mid \VP^{\model}, (m {-} 1)\text{-approval}))
    \]
    
    When $2 \leq k \leq m$, the REP can be reduced to solving the \esc problem twice under $k$-approval and $(k-1)$-approval rules.
    \begin{align*}
        \Pr(c@k \mid \model) 
        &= \mathds{E}(\score(c \mid \VP^{\model}, k\text{-approval})) \\
        &- \mathds{E}(\score(c \mid \VP^{\model}, (k-1)\text{-approval}))
    \end{align*}
\end{proof}

\begin{reptheorem}{\ref{theorem:shaPcomplete_for_expected_score_given_partialOrder_and_plurality}}
    \theoremHardnessForEscGivenPartialOrderAndPlurality
\end{reptheorem}

\begin{proof}
    Firstly, we prove the membership of the \esc problem as an FP$^{\#P}$ problem.
    Consider that the REP is FP$^{\#P}$-complete over partial orders (Theorem~\ref{theorem:REP_shaPcomplete_over_partialOrders}), and the \esc problem can be reduced to the REP (Theorem~\ref{theorem:reduction})
    So the \esc problem is in FP$^{\#P}$ for partial voting profiles.
    
    Secondly, we prove that the \esc problem is FP$^{\#P}$-hard, even for plurality rule, by  reduction from the REP-t that is FP$^{\#P}$-hard (Lemma~\ref{lemma:REPt_shaPcomplete_over_partialOrders}).
    
    Let $\partialOrder$ denote the partial order of the REP-t problem.
    Recall that the REP-t problem aims to calculate $\Pr(c@1 \mid \partialOrder)$ for a given item $c$.
    Let $\VP^{\partialOrder}$ denote a voting profile consisting of just this partial order $\partialOrder$.
    The answer to the REP-t problem is the same as the answer to the corresponding \esc problem, \ie $\Pr(c@1 \mid \partialOrder) = \mathds{E}(\score(c \mid \VP^{\partialOrder}, \text{plurality}))$.
    So the \esc problem is FP$^{\#P}$-hard, even for plurality voting rule.
    
    In conclusion, the \esc problem is FP$^{\#P}$-complete, under plurality rule.
\end{proof}

\begin{reptheorem}{\ref{theorem:shaPcomplete_for_expected_score_given_partialOrder_and_veto}}
    \theoremHardnessForEscGivenPartialOrderAndVeto
\end{reptheorem}

\begin{proof}
    This proof adopts the same approach as the proof of Theorem~\ref{theorem:shaPcomplete_for_expected_score_given_partialOrder_and_plurality}.
    
    Firstly, the membership proof that the ESC is in FP$^{\#P}$ is based on the conclusions that the REP is FP$^{\#P}$-complete over partial orders (Theorem~\ref{theorem:REP_shaPcomplete_over_partialOrders}), and that the ESC can be reduced to the REP (Theorem~\ref{theorem:reduction})
    So the ESC is in FP$^{\#P}$ for partial voting profiles.
    
    Secondly, we prove that the ESC is FP$^{\#P}$-hard, under veto voting rule, by reduction from the REP-b that is FP$^{\#P}$-hard (Lemma~\ref{lemma:REPb_shaPcomplete_over_partialOrders}).
    
    Let $\partialOrder$ denote the partial order of the REP-b problem.
    Recall that the REP-b problem aims to calculate $\Pr(c@m \mid \partialOrder)$ for a given item $c$.
    Let $\VP^{\partialOrder}$ denote a voting profile consisting of just this partial order $\partialOrder$.
    The answer to the ESC indirectly solves the REP-b, \ie $\Pr(c@m \mid \partialOrder) = 1 - \mathds{E}(\score(c \mid \VP^{\partialOrder}, \text{veto}))$.
    So the ESC problem is FP$^{\#P}$-hard under veto rule.
    
    In conclusion, the ESC is FP$^{\#P}$-complete, under veto rule.
\end{proof}

\begin{reptheorem}{\ref{theorem:shaPcomplete_for_expected_score_given_partialOrder_and_k_approval}}
    \theoremHardnessForExpectedScoreGivenPartialOrderAndKApproval
\end{reptheorem}

\begin{proof}
    Firstly, the proof that the Expected Score Computation (ESC) is in FP$^{\#P}$ is the same as the proof of Theorem~\ref{theorem:shaPcomplete_for_expected_score_given_partialOrder_and_plurality}.
    Now we prove that the ESC problem is FP$^{\#P}$-hard, under $k$-approval rule $\vr_m$, by reduction from the REP-t problem that is FP$^{\#P}$-hard (Lemma~\ref{lemma:REPt_shaPcomplete_over_partialOrders}).
    
    Let $\partialOrder$ denote the partial order of the REP-t problem.
    Recall that the REP-t problem aims to calculate $\Pr(c@1 \mid \partialOrder)$ for a given item $c$.
    Let $\partialOrder_+$ denote a new partial order by inserting $(k - 1)$ ordered items $d_1 \succ \ldots \succ d_{k-1}$ into $\partialOrder$ such that item $d_{k-1}$ is preferred to every item in $\partialOrder$.
    Such placement of items $\set{\enum[k-1]{d}}$ is to guarantee that all linear extensions of $\partialOrder_+$ start with $d_1 \succ \ldots \succ d_{k-1}$ and these linear extensions will be precisely the linear extensions of $\partialOrder$ after removing $\set{\enum[k-1]{d}}$.
    
    Let $\VP^{\partialOrder_+}$ denote a voting profile consisting of just this partial order $\partialOrder_+$.
    The answer to the ESC problem for item $c$ is $\mathds{E}(\score(c \mid \VP^{\partialOrder_+}, k\text{-approval}))$.
    Since there is only one partial order $\partialOrder_+$ in the voting profile, $\mathds{E}(\score(c \mid \VP^{\partialOrder_+}, k\text{-approval})) = \sum_{j=1}^{k} \Pr(c@j \mid \partialOrder_+)$.
    Recall that any linear extension of $\partialOrder_+$ always starts with $d_1 \succ \ldots \succ d_{k-1}$, so $\forall 1 \leq j \leq (k-1), \Pr(c@j \mid \partialOrder_+) = 0$, which leads to $\mathds{E}(\score(c \mid \VP^{\partialOrder_+}, k\text{-approval})) = \Pr(c@k \mid \partialOrder_+)$.
    Since $\partialOrder_+$ is constructed by inserting $(k-1)$ items before items in $\partialOrder$, $\Pr(c@k \mid \partialOrder_+) = \Pr(c@1 \mid \partialOrder)$.
    So $\mathds{E}(\score(c \mid \VP^{\partialOrder_+}, k\text{-approval})) = \Pr(c@1 \mid \partialOrder)$.
    The answer to the REP-t problem has been reduced to the ESC problem.
    So the ESC problem is FP$^{\#P}$-hard, under $k$-approval rule.
    
    In conclusion, the ESC problem is FP$^{\#P}$-complete, under the $k$-approval rule.
\end{proof}


\begin{reptheorem}{\ref{theorem:tractability_of_fullparVP}}
  \theoremTractabilityOfFP
\end{reptheorem}

\begin{proof}
    Any $\fullpar \in \fullparVP$ defines a set of consecutive ranks in the linear extensions of $\fullpar$ for each of its partitions of candidates.
    Any candidate is equally likely to be positioned at these ranks.
    So the REP can be solved in $O(1)$ for any candidate.
    Thus, the \mew problem can be solved in $O(nm^2)$ by calculating the expected scores of all candidates.
\end{proof}

\begin{reptheorem}{\ref{theorem:tractability_of_pchainVP}}
  \theoremTractabilityOfPC
\end{reptheorem}

\begin{proof}
  For any $\pchain \in \pchainVP$ and any candidate $c$, the $\Pr(c {\rightarrow} j \mid \pchain)$ is proportional to the degree of freedom to place the rest of the candidates, after fixing $c$ at rank $j$.
  \begin{itemize}
     \item If $c \not\in \pchain$, this is a trivial case where $c$ is equally likely to be placed at any rank, thus $\forall 1 \leq j \leq m, \Pr(c {\rightarrow} j \mid \pchain) = 1/m$.
    \item If $c \in \pchain$, let $K_l = |\set{c' \mid c' \succ_{\pchain} c}|$ be the number of items preferred to $c$ by $\pchain$ and $K_r = |\set{c' \mid c \succ_{\pchain} c'}|$ be the number of items less preferred to $c$ by $\pchain$, then $\Pr(c {\rightarrow} j \mid \pchain) \propto \binom{j-1}{K_l} \cdot \binom{m - j}{K_r}$ where .
  \end{itemize}

  It takes $O(nm^2)$ to obtain the expected scores of all candidates and to determine whether $w$ is a \mew.
\end{proof}

\begin{reptheorem}{\ref{theorem:tractability_of_parparVP}}
  \theoremTractabilityOfPP
\end{reptheorem}

\begin{proof}
  For any $\parpar \in \parparVP$ and any candidate $c$, the $\Pr(c {\rightarrow} j \mid \parpar)$ is proportional to the degree of freedom to place the rest of the candidates, after fixing $c$ at rank $j$.

 \begin{itemize}
     \item If $c \not\in \parpar$, this is a trivial case where $c$ is equally likely to be placed at any rank, thus $\forall 1 \leq j \leq m, \Pr(c {\rightarrow} j \mid \pchain) = 1/m$.
     \item If $c \in \parpar$, let $K_l = |\set{c' \mid c' \succ_{\parpar} c}|$ be the number of items preferred to $c$ by $\parpar$, $K_r = |\set{c' \mid c \succ_{\parpar} c'}|$ be the number of items less preferred to $c$ by $\parpar$, and $K_c$ be the number of items in the partition of $c$, then $\Pr(c {\rightarrow} j \mid \pchain) \propto \sum_{x=0}^{K_c - 1} \binom{j-1}{K_l + x} \cdot \binom{m - j}{K_r + K_c - 1 - x}$ where $x$ is the number of items from the same partition as $c$ and placed to the left of $c$.
   \end{itemize}

  It takes $O(nm^2)$ to obtain the expected scores of all candidates and to determine whether $w$ is a \mew.
\end{proof}

\begin{reptheorem}{\ref{theorem:tractability_of_rimVP}}
  \theoremTractabilityOfRIM
\end{reptheorem}

\begin{proof}
    Given any $\RIM \in \rimVP$ and any candidate $c$, the $Pr(c {\rightarrow} j \mid \RIM)$ for $j=1,\ldots,m$ can be calculated by Algorithm~\ref{alg:rim_rank_estimation} in $O(m^3)$.
    Algorithm~\ref{alg:rim_rank_estimation} is a variant of RIMDP~\cite{DBLP:conf/aaai/KenigIPKS18}.
    RIMDP calculates the marginal probability of a partial order over RIM via Dynamic Programming (DP).
    Algorithm~\ref{alg:rim_rank_estimation} is simplified RIMDP in the sense that Algorithm~\ref{alg:rim_rank_estimation} only tracks a particular item $c$, while RIMDP tracks multiple items to calculate the insertion ranges of items that satisfy the partial order.
    Note that Algorithm~\ref{alg:rim_rank_estimation} calculates all $m$ different values of $j$ simultaneously.
    So it takes $O(nm \cdot m^3) = O(nm^4)$ to obtain the expected scores of $m$ candidates over $n$ RIMs to determine \mew.
\end{proof}

\begin{reptheorem} {\ref{theorem:tractability_of_rimTrunVP}}
  \theoremTractabilityOfRimTrun
\end{reptheorem}

\begin{proof}
    Given any $(\RIM, \ranking^{(t, b)}) \in \rimTrunVP$, candidate $c$, and rank $j$, if $c$ is in the top or bottom part of $\ranking^{(t, b)}$, its rank has been fixed, which is a trivial case;
    If $c$ is in the middle part of $\ranking^{(t, b)}$, we just need to slightly modify Algorithm~\ref{alg:rim_rank_estimation} to calculate $\Pr(c {\rightarrow} j \mid \RIM, \ranking^{(t, b)})$.
    Line~\ref{alg:rim_rank_estimation:j} in Algorithm~\ref{alg:rim_rank_estimation} enumerates values for $j$ from 1 to $i$.
    The constraints made by $\ranking^{(t, b)}$ limits this insertion range of item $\bsigma(i)$.
    If $\bsigma(i)$ is in the top or bottom part of $\ranking^{(t, b)}$, its insertion position has been fixed by $\ranking^{(t, b)}$ and the inserted items of the top and bottom parts of $\ranking^{(t, b)}$ should be recorded as well by the state $\delta'$;
    If $\bsigma(i)$ is in the middle part of $\ranking^{(t, b)}$, $\bsigma(i)$ can be inserted into any position between the inserted top and bottom items.

    Theoretically, the algorithm needs to track as many as $(t+b+1)$ items.
    But $(t+b)$ items are fixed, which makes $c$ the only item leading to multiple DP states.
    The complexity of calculating $\Pr(c {\rightarrow} j \mid \RIM, \ranking^{(t, b)})$ for all $j$ values is $O(m^3)$.
    It takes $O(nm^4)$ to calculate the expected scores of all candidates across all voters to determine the \mew.
\end{proof}

\begin{reptheorem}{\ref{theorem:tractability_of_mallowsPartitionVP}}
  \theoremTractabilityOfMallowsFP
\end{reptheorem}

\begin{proof}
    Given any $(\mallows(\bsigma, \phi), \fullpar) \in \mallowsPartitionVP$, candidate $c$, and rank $j$, consider calculating $Pr(c {\rightarrow} j \mid \bsigma, \phi, \fullpar)$.
    Let $C_P$ denote the set of candidates in the same partition with $c$ in $\fullpar$.
    The relative orders between $c$ and items out of $C_P$ are already determined by $\fullpar$.
    That is to say, for a non-trivial $j$ value, $Pr(c {\rightarrow} j \mid \bsigma, \phi, \fullpar)$ is proportional to the exponential of the number of disagreed pairs within $C_P$.
    So we can construct a new Mallows model $\mallows'(\bsigma', \phi)$ over $C_P$.
    It has the same $\phi$ as $\mallows$ and its reference ranking $\bsigma'$ is shorter than but consistent with $\bsigma$.
    The $Pr(c {\rightarrow} j \mid \mallows', \fullpar)$ for all non-trivial $j$ values can be calculated in $O(|C_P|^3) < O(m^3)$ by Algorithm~\ref{alg:rim_rank_estimation}.

    The \mew problem can be solved in $O(nm^4)$ by calculating the expected scores of all candidates across all voters to determine whether $w$ is a \mew.
\end{proof}

\section{Tractability over RSM profiles}
\label{sec:appendix:rsm_profile}

RSM~\cite{DBLP:journals/tdasci/ChakrabortyDKKR21} denoted by $\mathsf{RSM}(\bsigma, \Pi, p)$ is another generalization of the Mallows.
It is parameterized by a reference ranking $\bsigma$, a probability function $\Pi$ where $\Pi(i, j)$ is the probability of the $j^{th}$ item selected at step $i$, and a probability function $p:\set{1,...,m-1} \rightarrow [0,1]$ where $p(i)$ is the probability that the $i^{th}$ selected item preferred to the remaining items.
In contrast to the RIM that randomizes the item insertion position, the RSM randomized the item insertion order.
In this paper, we use RSM as a ranking model, \ie $p \equiv 1$ such that it only outputs rankings.
This ranking version is named rRSM and denoted by $\RSM(\bsigma, \Pi)$.

\begin{example}
    $\RSM(\bsigma, \Pi)$ with $\bsigma=\angs{a, b, c}$ generates $\ranking {=} \angs{c, a, b}$ as follows.
    Initialize $\ranking_0 {=} \angs{}$. 
    When $i=1$, $\ranking_1 {=} \angs{c}$ by selecting $c$ with probability $\Pi(1,3)$, making the remaining $\bsigma = \angs{a, b}$.
    When $i=2$, $\ranking_2 {=} \angs{c, a}$ by selecting $a$ with probability $\Pi(2,1)$, making the remaining $\bsigma = \angs{b}$.
    When $i=3$, $\ranking {=} \angs{c, a, b}$ by selecting $b$ with probability $\Pi(3,1)$. 
    Overall, $\Pr(\ranking \mid \bsigma, \Pi) {=} \Pi(1,3) \cdot \Pi(2,1) \cdot \Pi(3,1)$.
\end{example}

\begin{theorem} \label{theorem:tractability_of_rsmVP}
    Given a positional scoring rule $\vr_m$, an RSM voting profile $\rsmVP = (\enum[n]{\RSM})$, and candidate $w$, determining $w \in \mew(\vr_m, \rsmVP)$ is in $O(nm^4)$.
\end{theorem}

\begin{proof}
    Given any $\RSM \in \rsmVP$, candidate $c$, and rank $j$, the $Pr(c@j \mid \RSM)$ is computed by Algorithm~\ref{alg:rsm_rank_estimation} in a fashion that is similar to Algorithm~\ref{alg:rim_rank_estimation}.
    This is also a Dynamic Programming (DP) approach.
    The states are in the form of $\angs{\alpha, \beta}$, where $\alpha$ is the number of items before $c$, and $\beta$ is that after $c$ in the remaining $\bsigma$.
    For state $\angs{\alpha, \beta}$, there are $(\alpha + 1 + \beta)$ items in the remaining $\bsigma$.
    Algorithm~\ref{alg:rsm_rank_estimation} only runs up to $i=(k-1)$ (in line~\ref{alg:rsm_rank_estimation:step}), since item $c$ must be selected at step $k$ and the rest steps do not change the rank of $c$ anymore.
    Each step $i$ generates at most $(i + 1)$ states, corresponding to $[0, \ldots, i]$ items are selected from items before $c$ in the original $\bsigma$.
    The complexity of Algorithm~\ref{alg:rsm_rank_estimation} is bounded by $O(m^2)$.
    It takes $O(nm^4)$ to obtain the expected scores of all candidates and to determine the \mew.
    
\end{proof}

\begin{algorithm}[tb!]
    \caption{REP solver for rRSM}
    \label{alg:rsm_rank_estimation}
    \textbf{Input}: Item $c$, rank $k$, $\RSM(\bsigma, \Pi)$ % TODO
    \textbf{Output}: $\Pr(c@k \mid \bsigma, \Pi)$
    \begin{algorithmic}[1] %[1] enables line numbers
        \STATE $\alpha_0 \defeq |\set{\sigma_i | \sigma_i \succ_{\bsigma} c}|$, $\beta_0 \defeq |\set{\sigma_i \mid c \succ_{\bsigma} \sigma_i}|$
        \STATE $\mathcal{P}_0 \defeq \set{\angs{\alpha_0, \beta_0}}$ and $q_0(\angs{\alpha_0, \beta_0}) \defeq 1$
        \FOR {$i=1, \ldots, (k - 1)$} \label{alg:rsm_rank_estimation:step}
        \STATE $\mathcal{P}_i \defeq \set{}$
        \FOR {$\angs{\alpha, \beta} \in \mathcal{P}_{i-1}$}
        \IF {$\alpha > 0$}
        \STATE Generate a new state $\angs{\alpha', \beta'} = \angs{\alpha - 1, \beta}$.
        \IF {$\angs{\alpha', \beta'} \notin \mathcal{P}_i$}
        \STATE $\mathcal{P}_i.add(\angs{\alpha', \beta'})$
        \STATE $q_i(\angs{\alpha', \beta'}) \defeq 0$
        \ENDIF
        \STATE $q_i(\angs{\alpha', \beta'}) \pluseq q_{i-1}(\angs{\alpha, \beta}) \cdot \sum_{j=1}^{\alpha} {\Pi(i, j)}$
        \ENDIF
        \IF {$\beta > 0$}
        \STATE Generate a new state $\angs{\alpha', \beta'} = \angs{\alpha, \beta - 1}$.
        \IF {$\angs{\alpha', \beta'} \notin \mathcal{P}_i$}
        \STATE $\mathcal{P}_i.add(\angs{\alpha', \beta'})$
        \STATE $q_i(\angs{\alpha', \beta'}) \defeq 0$
        \ENDIF
        \STATE $q_i(\angs{\alpha', \beta'}) \pluseq q_{i-1}(\angs{\alpha, \beta}) \cdot \sum_{j=\alpha + 2}^{\alpha + 1 + \beta} {\Pi(i, j)}$
        \ENDIF
        \ENDFOR
        \ENDFOR
        \RETURN  $\sum_{\angs{\alpha, \beta} \in \mathcal{P}_{k-1}} {q_{k - 1}(\angs{\alpha, \beta}) \cdot \Pi(k, \alpha + 1)}$
    \end{algorithmic}
\end{algorithm}

\begin{example}
Let $\RSM(\bsigma, \Pi)$ denote a RSM where $\bsigma=\angs{\sigma_1, \sigma_2, \sigma_3, \sigma_4}$, and $\Pi = [[0.1, 0.3, 0.4, 0.2],$ $[0.2, 0.5, 0.3], [0.3, 0.7],[1]]$.
Assume we are interested in $\Pr(\sigma_2 @ 3 \mid \bsigma, \Pi)$, the probability of item $\sigma_2$ placed at rank $3$.
\begin{itemize}
    \itemsep -0.1em
    \item Before running RSM, there is $\alpha_0 = 1$ item before $\sigma_2$ and $\beta_0 = 2$ items after $\sigma_2$ in $\bsigma$. So the initial state is $\angs{\alpha_0, \beta_0} = \angs{1, 2}$, and $q_0(\angs{1, 2}) = 1$.
    \item At step $i = 1$, the selected item can be either from $\set{\sigma_1}$ or $\set{\sigma_3, \sigma_4}$. So two new states are generated here.
    \begin{itemize}
        \item The $\sigma_1$ is selected with probability $\Pi(1, 1) = 0.1$, which generates a new state $\angs{0, 2}$, and $q_1(\angs{0,2}) = q_0(\angs{1,2}) \cdot \Pi(1, 1) = 0.1$.
        \item An item $\sigma \in \set{\sigma_3, \sigma_4}$ is selected with probability $\Pi(1, 3) + \Pi(1, 4) = 0.6$, which generates a new state $\angs{1, 1}$, and $q_1(\angs{1, 1}) = q_0(\angs{1,2}) \cdot 0.6 = 0.6$.
    \end{itemize}
    So $\mathcal{P}_1 {=} \set{\angs{0, 2}, \angs{1, 1}}$, $q_1 {=} \set{\angs{0, 2} {\mapsto} 0.1, \angs{1, 1} {\mapsto} 0.6}$.
    \item At step $i = 2$, iterate states in $\mathcal{P}_1$.
    \begin{itemize}
        \item For state $\angs{0, 2}$, the selected item must be from the last two items in the remaining reference ranking. A new state $\angs{0, 1}$ is generated with probability $\Pi(2, 2) + \Pi(2, 3) = 0.8$.
        \item For state $\angs{1, 1}$, the selected item is either the first or last item in remaining reference ranking. A new state $\angs{0, 1}$ is generated with probability $\Pi(2, 1) = 0.1$, and another state $\angs{1, 0}$ is generated with probability $\Pi(2, 3) = 0.3$.
    \end{itemize}
    So $\mathcal{P}_2 = \set{\angs{0, 1}, \angs{1, 0}}$ and
    \begin{itemize}
        \item[$\square$] $q_2(\angs{0, 1}) = q_1(\angs{0, 2}) \cdot 0.8 + q_1(\angs{1, 1}) \cdot 0.1 = 0.1 \cdot 0.8 + 0.6 \cdot 0.1 = 0.14$
        \item[$\square$] $q_2(\angs{1, 0}) = q_1(\angs{1, 1}) \cdot 0.3 = 0.6 \cdot 0.3 = 0.18$
    \end{itemize}
    \item At step $i = 3$, item $\sigma_2$ must be selected to meet the requirement. For each state $\angs{\alpha, \beta} \in \mathcal{P}_2$, the rank of $\sigma_2$ is $(\alpha + 1)$ in the corresponding remaining ranking. So $\Pr(\sigma_2 @ 3 {\mid} \bsigma, \Pi) = q_2(\angs{0, 1}) \cdot \Pi(3, 1) + q_2(\angs{1, 0}) \cdot \Pi(3, 2) = 0.14 \cdot 0.3 + 0.18 \cdot 0.7 = 0.168$.
\end{itemize}
\end{example}

\section{Additional experiments}
\label{sec:appendix:experiments}

\revv{Figure~\ref{fig:mpw} in Section~\ref{sec:mew_vs_mpw} has demonstrated that \mew is much more scalable than \mpw under the plurality rule. Figure~\ref{fig:mpw_borda} presents results of a similar experiment under the Borda rule, with up to $6$ candidates and up to $15$ voters. We first fixed the number of voters to 5 and varied the number of candidates from 3 to 6, then fixed the number of candidates to 5 and varied the number of voters from 1 to 15. In this experiment, \mew is still much more scalable than \mpw.}

\begin{figure}[tb!]
  \centering
  \subfloat[\revv{5 voters}, varying \#candidates]{
    \label{fig:mpw_borda:candidates}
    \includegraphics[width=0.3\linewidth]{figs/synthetic_mpw_borda_5_voters__time_vs_m}
  }\hspace{5em}
  \subfloat[\revv{5 candidates}, varying \#voters]{
    \label{fig:mpw_borda:voters}
    \includegraphics[width=0.3\linewidth]{figs/synthetic_mpw_borda_5_candidates__time_vs_n}
  }
  \caption{\revv{Average time of parallel MPW and MEW, using 48 worker processes, under Borda, over partial voting profiles, fixing $\phi = 0.5$ and $p_{max}=0.1$. \mew scales much better than \mpw,  with both \#candidates and \#voters.}}
  \label{fig:mpw_borda}
\end{figure}