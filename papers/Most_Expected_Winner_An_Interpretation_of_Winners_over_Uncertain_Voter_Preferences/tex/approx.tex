\section{Approximate solvers}
\label{sec:approx}

Since the \esc problem over partial voting profiles are proved to be intractable in Section~\ref{sec:complexity}, we now propose a sampling-based estimator for these voting profiles, and more generally, the ones of Mallows models combined with partial orders.
This estimator relies on Theorem~\ref{theorem:reduction} and aims to solve the REP over a Mallows model $\mallows(\bsigma, \phi)$ combined with a partial order $\partialOrder$.
The partial voting profiles are a special case where the Mallows models are uniform distributions with the spread parameter $\phi=1$.

\paragraph{Problem formalization.}
Given a Mallows model $\mallows(\bsigma, \phi)$ of $m$ items, a partial order $\partialOrder$, a rank $1 \leq k \leq m$ and a candidate $c$, we are interested in $\Pr(c \rightarrow k \mid \bsigma, \phi, \partialOrder)$, the probability that candidate $c$ is placed at rank $k$ by a ranking drawn from the posterior distribution of $\mallows(\bsigma, \phi)$ conditioned on $\partialOrder$.

The difficult part of this problem origins from the fact that there is no efficient exact sampling method for the Mallows posteriors conditioned on the partial orders. We can easily come up with a rejection sampler (RS) to draw rankings from $\mallows(\bsigma, \phi)$ and only accept the samples that are the completions of the partial order $\partialOrder$. But this sampler is inept when the posterior probability of the partial order is low enough that samples are almost always rejected.  Importance Sampling (IS) can tackle this issue; it effectively estimates rare events by obtaining samples from an proposal distribution then re-weight the samples for unbiased estimation~\cite{kahn1950random1, kahn1950random2}.

Assume that IS draws $N$ samples $\set{x_1, \ldots, x_N}$ from a proposal distribution $\propDistr$ to estimate the expected value of a function $f(x)$ in a probability space $\oriDistr$.

\begin{equation}\label{eq:importance_sampling}
\small 
\begin{split}
  \mathds{E}(f(x)) 
  =& \sum_{x \in \oriDistr} {f(x) \cdot \Pr(x \mid \oriDistr)} 
  = \sum_{x \in \propDistr} {f(x) \cdot \frac{\Pr(x \mid \oriDistr)}{\Pr(x \mid \propDistr)} \cdot \Pr(x \mid \propDistr)} \\
  =& \mathds{E}\left( f(x) \cdot \frac{\Pr(x \mid \oriDistr)}{\Pr(x \mid \propDistr)} \right)
  \approx \frac{1}{N} \sum_{i=1}^{N} {f(x_i) \cdot \frac{\Pr(x_i \mid \oriDistr)}{\Pr(x_i \mid \propDistr)}}
\end{split}
\end{equation}

IS re-weights the contribution of $x_i$ by an \e{importance factor} $\frac{\Pr(x_i \mid \oriDistr)}{\Pr(x_i \mid \propDistr)}$.
An ideal $\propDistr$ should be easy to draw samples from and be similar to the real posterior distribution.
When applying IS, we adopt AMP, the state-of-the-art Mallows posterior sampler~\cite{DBLP:journals/jmlr/LuB14}, and $f(x_i) = \mathds{1}(c {\rightarrow} k \mid x_i)$, \ie whether candidate $c$ is placed at rank $k$ by sample ranking $x_i$.
