\documentclass{article}
\usepackage[final]{nips_2017}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage[dvipsnames,table]{xcolor}
\usepackage[colorlinks=true,citecolor=Blue,urlcolor=Blue,linkcolor=Blue]{hyperref}
\usepackage{url}
\usepackage{booktabs}
\usepackage{amsfonts}
\usepackage{nicefrac}
\usepackage{microtype}
\usepackage{enumitem}

\usepackage{bbm}
\usepackage{Definitions}

\usepackage{tikz}
\usetikzlibrary{shapes,backgrounds}

\usepackage{relsize}

\usepackage{graphicx}
\usepackage{epsfig}
\usepackage{subcaption}
\usepackage{wrapfig}
\usepackage{lipsum}

\newcommand\nonumfootnote[1]{
  \begingroup
  \renewcommand\thefootnote{}\footnote{#1}
  \addtocounter{footnote}{-1}
  \endgroup
}




\newcommand{\todo}[1]{\textcolor{red}{TODO: #1}}
\newcommand{\xhdr}[1]{\vspace{0mm}\noindent{{\bf #1.}}}
\newcommand{\new}[1]{\textcolor{blue}{#1}}
\newcommand{\bz}[1]{\textcolor{cyan}{\bf [Bilal: #1]}}
\newcommand{\manuel}[1]{\textcolor{red}{\bf [Manuel: #1]}}
\newcommand{\denselist}{\itemsep -3pt\topsep-20pt\partopsep-20pt \leftmargin 3mm}
\newcommand{\emphb}[1]{\textbf{\emph{#1}}}

\newcommand{\thetaUnf}{\thetab^{\text{un}}}
\newcommand{\thetaEqfS}{\thetab^{\text{eq}}}

\newcommand{\thetaEqf}[1]{\thetab^{\text{eq}}_{#1}}
\newcommand{\thetaPrf}[1]{\thetab^{\text{pr}}_{#1}}
\newcommand{\thetaEnf}[1]{\thetab^{\text{en}}_{#1}}

\newcommand{\cUnf}{\mathcal{C}^{\text{un}}}
\newcommand{\cEqf}{\mathcal{C}^{\text{eq}}}

\newcommand{\cPrf}{\mathcal{C}^{\text{pr}}}
\newcommand{\cEnf}{\mathcal{C}^{\text{en}}}

\newcommand{\data}{\mathcal{D}}
\newcommand{\pop}{\mathcal{P}}
\newcommand{\ben}{\mathcal{B}}
\newcommand{\xytup}{\xb,y}



\title{
From Parity to Preference-based Notions \\of Fairness in Classification
  }



\author{
  Muhammad Bilal Zafar \\
  MPI-SWS \\
  \texttt{mzafar@mpi-sws.org}
  \And
  Isabel Valera \\
  MPI-IS\\
  \texttt{isabel.valera@tue.mpg.de}
  \And
  Manuel Gomez Rodriguez\\
  MPI-SWS\\
  \texttt{manuelgr@mpi-sws.org}
  \And
  Krishna P. Gummadi \\
  MPI-SWS \\
  \texttt{gummadi@mpi-sws.org}
  \And
  Adrian Weller\\
  University of Cambridge \& Alan Turing Institute\\
  \texttt{aw665@cam.ac.uk}
}

\begin{document}
\maketitle

\nonumfootnote
{
An open-source code implementation of our scheme is available at: \href{http://fate-computing.mpi-sws.org/}{http://fate-computing.mpi-sws.org/}
}

\begin{abstract}
The adoption of automated, data-driven decision making in an ever
expanding range of applications has raised concerns about its
potential unfairness towards certain social groups. In this context, a
number of recent studies have focused on defining, detecting, and
removing unfairness from data-driven decision systems. However, the
existing notions of fairness, based on \emph{parity} (equality) in
treatment or outcomes for different social groups, tend to be
quite
stringent, limiting the overall decision making accuracy.
In this paper, we draw inspiration from the fair-division and
envy-freeness literature in economics and game theory and propose
\emph{preference}-based notions of fairness---given the choice between
various sets of decision treatments or outcomes, any group of users
would collectively prefer its treatment or outcomes, regardless of the
(dis)parity as compared to the other groups.
Then, we introduce tractable proxies to design
margin-based classifiers that satisfy these preference-based notions of fairness.
Finally, we experiment with a variety of synthetic and real-world
datasets and show that preference-based fairness allows for
greater decision accuracy than parity-based fairness.

\end{abstract}


\section {Introduction}\label{sec:intro}
As machine learning is increasingly being used to automate decision making in domains that affect human lives (\eg, credit ratings,
housing allocation, recidivism risk prediction), there are growing concerns about the potential for {\it unfairness} in such algorithmic
decisions~\cite{propublica_compas, bigdatawhitehouse2016}. A flurry of recent research on fair learning has focused on defining
appropriate notions of fairness and then designing mechanisms to ensure fairness in automated decision making
~\cite{Dwork2012,feldman_kdd15, hardt_nips16, joseph_bandits, kamiran_sampling, kamishima_regularizer,pedreschi_discrimination, zafar_dmt, zafar_fairness, icml2013_zemel13}.




Existing notions of fairness in the machine learning literature are largely inspired by the concept of {\bf discrimination} in social sciences
and law. These notions call for {\bf parity} (\ie, equality) in {\bf treatment}, in {\bf impact}, or both.
To ensure parity in treatment (or treatment parity), decision making systems need to avoid using users'{} sensitive attribute information, \ie, avoid using the
membership information in socially salient groups (\eg, gender, race), which are protected by anti-discrimination laws~\cite{barocas_2016, civil_rights_act}. As a result, the use of group-conditional
decision making systems is often prohibited.
To ensure parity in impact (or impact parity), decision making systems need to avoid disparity in the fraction of users belonging to different sensitive
attribute groups (\eg, men, women) that receive {\it beneficial} decision outcomes.
A number of learning mechanisms have been proposed to achieve parity in treatment~\cite{salvatore_knn}, parity
in impact~\cite{cadlers_naivebayes, hardt_nips16, kamishima_regularizer} or both~\cite{Dwork2012,feldman_kdd15,goh_nips2016,kamiran_sampling,zafar_dmt,zafar_fairness,icml2013_zemel13}.
However, these mechanisms pay a significant cost in terms of the accuracy (or utility) of their predictions. In fact, there exist
some inherent tradeoffs (both theoretical and empirical) between achieving high prediction accuracy and satisfying treatment and / or
impact parity~\cite{dimpact_fpr,goel_cost_fairness, friedler_impossibility, kleinberg_itcs17}.







In this work, we introduce, formalize and evaluate new notions of fairness that are inspired by the concepts of {\bf fair division} and {\bf envy-freeness} in economics
and game theory~\cite{berliant_envy,nash1950bargaining,varian_envy}.  Our work is motivated by the observation that, in certain decision making scenarios, the existing parity-based fairness notions may
be too stringent, precluding more accurate decisions, which may also be desired by every sensitive attribute group.
To relax these parity-based notions, we introduce the concept of a user {\bf group'{}s  preference} for being assigned one set of decision outcomes over another. Given
the choice between various sets of decision outcomes, any group of users would collectively {\it prefer} the set that contains {\it the largest fraction} (or the greatest number)
of beneficial decision outcomes for that group.
\footnote{\scriptsize{Although it is quite possible that certain \textit{individuals} from the group may not prefer the set that maximizes the benefit for the \textit{group as a whole}.}}
More specifically, our new preference-based notions of fairness, which we formally define in the next section, use the concept of user group'{}s preference as follows:

\noindent
{\hspace{0.3mm}--- \bf From Parity Treatment to Preferred Treatment:} To offer preferred treatment, a decision making system should ensure that every sensitive attribute group
(\eg, men and women) {\it prefers} the set of decisions they receive over the set of decisions they would have received had they collectively presented themselves
to the system as members of a different sensitive group.





\begin{figure}[t]
    \centering
        \includegraphics[trim={2cm 5cm 6cm 7cm},clip, width=0.9\columnwidth]{decision_boundaries.pdf}
        \caption{
        \small{
        A fictitious decision making scenario involving two groups: men (M) and women (W). Feature $f_{1}$ (x-axis) is highly predictive for women whereas $f_{2}$ (y-axis) is highly predictive for men. Green (red) quadrants denote the positive (negative) class. Within each quadrant, the points are distributed uniformly and the numbers in parenthesis denote the number of subjects in that quadrant.  The \textbf{left panel} shows the optimal classifier satisfying parity in treatment. This classifier leads to all the men getting classified as negative. The \textbf{middle panel} shows the optimal classifier satisfying parity in impact (in addition to parity in treatment). This classifier achieves impact parity by misclassifying women from positive class into negative class, and in the process, incurs a significant cost in terms of accuracy. The \textbf{right panel} shows a classifier consisting of group-conditional classifiers for men (purple) and women (blue). Both the classifiers satisfy the preferred treatment criterion since for each group, adopting the other group's classifier would lead to a smaller fraction of beneficial outcomes. Additionally, this group-conditional classifier is also a preferred impact classifier since both groups get more benefit as compared to the impact parity classifier. The overall accuracy is better than the parity classifiers.
        }
        }
        \label{fig:example}
\end{figure}

The preferred treatment criterion represents a relaxation of treatment parity. That is, every decision making system that achieves treatment parity also satisfies the preferred treatment
condition, which implies (in theory) that the optimal decision accuracy that can be achieved under the preferred treatment condition is at least as high as the one achieved under treatment parity.
Additionally, preferred treatment allows group-conditional decision making (not allowed by treatment parity), which is necessary to achieve high decision
accuracy in scenarios when the predictive power of features varies greatly between different sensitive user groups~\cite{dwork2017decoupled}, as shown
in Figure~\ref{fig:example}.

While preferred treatment is a looser notion of fairness than treatment parity, it retains a core fairness property embodied in treatment parity, namely, {\it envy-freeness at
the level of user groups}. Under preferred treatment, no group of users (\eg, men or women, blacks or whites) would feel that they would be collectively better off by switching their
group membership (\eg, gender, race). Thus, preferred treatment decision making, despite allowing group-conditional decision making, is not vulnerable to being characterized
as ``reverse discrimination'' against, or "affirmative action'' for certain groups.

\noindent
{\hspace{0.3mm}--- \bf From Parity Impact to Preferred Impact:} To offer preferred impact, a decision making system needs to ensure that every sensitive attribute group (\eg, men and
women) {\it prefers} the set of decisions they receive over the set of decisions they would have received under the criterion of impact parity.

The preferred impact criterion represents a relaxation of impact parity. That is, every decision making system that achieves impact parity also satisfies the preferred
impact condition,
which implies (in theory) that the optimal decision accuracy that can be achieved under the preferred impact condition is at least as high as the one achieved under impact parity.
Additionally, preferred impact allows disparity in benefits received by different groups, which may be justified in scenarios where insisting on
impact parity would only lead to a reduction in the beneficial outcomes received by one or more groups, without necessarily improving them for any
other group.
In such scenarios, insisting on impact parity can additionally lead to a reduction in the decision accuracy, creating a case of tragedy of impact parity with a
worse decision making all round, as shown in Figure~\ref{fig:example}.

While preferred impact is a looser notion of fairness compared to impact parity, by guaranteeing that every group receives {\it at least} as many beneficial
outcomes as they would would have received under impact parity, it retains the core fairness gains in beneficial outcomes that discriminated groups would
have achieved under the fairness criterion of impact parity.


Finally, we note that our preference-based fairness notions, while having many attractive properties, are not the most suitable notions of fairness in \textit{all} scenarios. In certain cases, parity fairness may well be the eventual goal~\cite{plato-disc} and the more desirable notion.

In the remainder of this paper,
we  formalize our preference-based fairness notions in the context of binary classification (Section~\ref{classifying_discrimination}),
propose tractable and efficient proxies to include these notions in the formulations of convex margin-based classifiers in the form of convex-concave constraints (Section~\ref{sec:classifiers}), and show on several real world datasets that our preference-based fairness notions can provide significant gains in overall decision making accuracy as compared to parity-based fairness (Section~\ref{sec:eval}).


\section{Defining preference-based fairness for classification}\label{classifying_discrimination}
In this section, we will first introduce two useful quality metrics---\emph{utility} and \emph{group benefit}---in the context of fairness in classification, then revisit parity-based
fairness definitions in the light of these quality metrics, and finally formalize the two preference-based notions of fairness introduced in Section~\ref{sec:intro} from the perspective
of the above metrics.
For simplicity, we consider binary classification tasks, however, the definitions can be easily extended to m-ary classification.

\xhdr{Quality metrics in fair classification}
In a fair (binary) classification task, one needs to find a mapping between the user feature vectors $\xb \in \RR^{d}$
and class labels $y \in \{-1, 1\}$, where $(\xb, y)$ are drawn from an (unknown) distribution $f(\xb, y)$.
This is often achieved by finding a mapping function $\thetab: \RR^d \to \RR$ such that given a feature vector $\xb$ with an unknown label $y$, the corresponding classifier predicts $\hat{y} = \sgn(\thetab(\xb))$.
However, this mapping function also needs to be \emph{fair} with respect to the values of a user sensitive attribute $z \in \Zcal \subseteq \ZZ_{\geq 0}$ (\eg,
sex, race), which are drawn from an (unknown) distribution $f(z)$ and may be dependent of the feature vectors and class
labels, \ie, $f(\xb, y, z) = f(\xb, y | z) f(z) \neq f(\xb, y) f(z)$.

Given the above problem setting, we introduce the following quality metrics, which we will use to define and compare different fairness notions:


\vspace{-2mm}
\begin{itemize}[leftmargin=6mm]
\item[I.] \textbf{Utility ($\Ucal$)}:
overall profit obtained by the decision maker using the classifier. For example, in a loan approval scenario, the decision maker is the bank that gives the loan and the utility can be
the overall accuracy of the classifier, \ie:
\begin{equation}
\Ucal(\thetab)
=
\EE_{\xytup} [\II\{\sgn(\thetab(\xb)) = y\}], \nonumber
\end{equation}
where $\II(\cdot)$ denotes the indicator function and the expectation is taken over the distribution $f(\xytup)$.
It is in the decision maker'{}s interest to use classifiers that maximize utility. Moreover, depending on the scenario, one can attribute different profit to true positives and true negatives---or conversely,
different cost to false negatives and false positives---while computing utility. For example, in the loan approval scenario, marking an eventual defaulter as non-defaulter may have a higher cost
than marking a non-defaulter as defaulter. For simplicity, in the remainder of the paper, we will assume that the profit (cost) for true (false) positives and negatives is the same.

\item[II.] \textbf{Group benefit ($\mathcal{B}_z$)}:
the fraction of beneficial outcomes received by users sharing a certain value of the sensitive attribute $z$ (\eg, blacks, hispanics, whites). For example, in a loan approval scenario, the beneficial outcome for a user may be receiving the loan and the group benefit for each value of $z$ can be defined as:
\begin{align}
\Bcal_z(\thetab)
=
\EE_{\xb | z} [\II\{\sgn(\thetab(\xb)) = 1\}], \nonumber
\end{align}
where the expectation is taken over the conditional distribution $f(\xb | z)$ and the bank offers a loan to a user if $\sgn(\thetab(\xb)) = 1$.
Moreover, as suggested by some recent studies in fairness-aware learning~\cite{hardt_nips16,kleinberg_itcs17,zafar_dmt}, the group benefits can also be defined as the fraction of beneficial outcomes conditional on the true label of the user. For example,
in a recidivism prediction scenario, the group benefits can be defined as the fraction of eventually non-offending defendants sharing a certain sensitive attribute value getting bail, that is:
\begin{align}
\Bcal_z(\thetab)
=
\EE_{\xb | z, y=1} [\II\{\sgn(\thetab(\xb)) = 1\}], \nonumber
\end{align}
where the expectation is taken over the conditional distribution $f(\xb | z, y=1)$, $y=1$ indicates that the defendant does not re-offend, and bail is granted
if $\sgn(\thetab(\xb)) = 1$.
\end{itemize}




\xhdr{Parity-based fairness}
A number of recent studies~\cite{cadlers_naivebayes, feldman_kdd15, hardt_nips16, kamishima_regularizer,zafar_dmt, zafar_fairness, icml2013_zemel13} have considered a classifier to be fair if
it satisfies the impact parity criterion. That is,
it ensures that the group benefits for all the sensitive attribute values are equal, \ie:
\begin{equation}
\mathcal{B}_{z}(\thetab) = \mathcal{B}_{z'{}}(\thetab) \quad \mbox{for all } z, z'{} \in \Zcal.
\end{equation}
In this context, different (or often same) definitions of group benefit (or beneficial outcome) have lead to different terminology, \eg, disparate impact~\cite{feldman_kdd15,zafar_fairness},
indirect discrimination~\cite{feldman_kdd15,kamishima_regularizer}, redlining~\cite{cadlers_naivebayes}, statistical parity~\cite{Dwork2012,goel_cost_fairness,kleinberg_itcs17,icml2013_zemel13},
disparate mistreatment~\cite{zafar_dmt}, or equality of opportunity~\cite{hardt_nips16}.
However, all of these group benefit definitions invariably focus on achieving impact parity. We point interested readers to Feldman et al.~\cite{feldman_kdd15} and Zafar et al.~\cite{zafar_dmt}
regarding the discussion on this terminology.

Although not always explicitly sought, most of the above studies propose classifiers that also satisfy treatment parity
in addition to impact parity, \ie, they do not use the sensitive attribute $z$ in the decision making process. However,
some of them~\cite{cadlers_naivebayes, hardt_nips16, kamishima_regularizer} do not satisfy treatment parity since they resort to group-conditional classifiers, \ie, $\thetab = \{ \thetab_z \}_{z \in \Zcal}$.
In such case, we can rewrite the above parity condition as:
\begin{equation}
\mathcal{B}_{z}(\thetab_z) = \mathcal{B}_{z'{}}(\thetab_{z'{}}) \quad \mbox{for all } z, z'{} \in \Zcal.
\end{equation}









\xhdr{Fairness beyond parity}
Given the above quality metrics, we can now formalize the two preference-based fairness notions introduced in Section~\ref{sec:intro}.

\vspace{-2mm}
\begin{itemize}[leftmargin=6mm]

\item[---] \textbf{Preferred treatment}: if a classifier $\thetab$ resorts to group-conditional classifiers, \ie, $\thetab = \{ \thetab_z \}_{z \in \Zcal}$, it is a preferred treatment classifier if each group sharing a sensitive attribute
value $z$ benefits more from its corresponding group-conditional classifier $\thetab_z$ than it would benefit if it would be classified by any of the other group-conditional classifiers
$\thetab_{z'}$, \ie,
\begin{equation}\label{eq:enf}
\mathcal{B}_{z}(\thetab_z) \geq \mathcal{B}_{z}(\thetab_{z'{}}) \quad \mbox{for all } z, z'{} \in \Zcal.
\end{equation}
Note that, if a classifier $\thetab$ does not resort to group-conditional classifiers, \ie, $\thetab_z = \thetab$ for all $z \in \Zcal$, it will be always be a preferred treatment classifier. If, in addition, such
classifier ensures impact parity, it is easy to show that its utility cannot be larger than a preferred treatment classifier consisting of group-conditional classifiers.

\item[---] \textbf{Preferred impact}: a classifier $\thetab$ offers preferred impact over a classifier $\thetab'{}$ ensuring impact parity if it achieves higher group
benefit for each sensitive attribute value group, \ie,
\begin{equation}
\mathcal{B}_{z}(\thetab) \geq \mathcal{B}_{z}(\thetab') \quad \mbox{for all } z \in \Zcal.
\end{equation}
One can also rewrite the above condition for group-conditional classifiers,
\ie, $\thetab = \{ \thetab_z \}_{z \in \Zcal}$ and
$\thetab'{} = \{\thetab_z'{} \}_{z \in \Zcal}$, as follows:
\begin{equation} \label{eq:pref-gc}
\mathcal{B}_{z}(\thetab_z) \geq \mathcal{B}_{z}(\thetab_z'{}) \quad \mbox{for all } z \in \Zcal.
\end{equation}
Note that, given any classifier $\thetab'{}$ ensuring impact parity, it is easy to show that there will always exist a preferred impact classifier $\thetab$ with equal or higher utility.


\end{itemize}






\xhdr{Connection to the fair division literature}
Our notion of preferred treatment is inspired by the concept of
envy-freeness~\cite{berliant_envy,varian_envy} in the fair division
literature. Intuitively, an envy-free resource division ensures that
no user would \emph{prefer the resources allocated} to another user
over their own allocation. Similarly, our notion of preferred
treatment ensures envy-free decision making at the level of sensitive
attribute groups. Specifically, with preferred treatment classification,
no sensitive attribute group would \emph{prefer the outcomes from the
  classifier} of another group.


Our notion of preferred impact draws inspiration from the two-person
bargaining problem~\cite{nash1950bargaining} in the fair division
literature. In a bargaining scenario, given a base resource allocation
(also called the disagreement point), two parties try to divide some
additional resources between themselves. If the parties cannot agree
on a division, no party gets the additional resources, and both would
only get the allocation specified by the disagreement point. Taking
the resources to be the beneficial outcomes, and the disagreement
point to be the allocation specified by the impact parity classifier,
a preferred impact classifier offers enhanced benefits to all the
sensitive attribute groups. Put differently, the group benefits provided
by the preferred impact classifier Pareto-dominate the benefits
provided by the impact parity classifier.


\xhdr{On individual-level preferences}
Notice that preferred treatment and preferred impact notions are defined based on the group preferences, \ie, whether a  \textit{group as a whole} prefers (or, gets more beneficial outcomes from) a given set of outcomes over another set. It is quite possible that a set of outcomes preferred by the group collectively is not preferred by certain \textit{individuals} in the group.
Consequently, one can extend our proposed notions to account for individual preferences as well, \ie, a set of outcomes is preferred over another if \emph{all} the individuals in the group prefer it. In the remainder of the paper, we focus on preferred treatment and preferred impact in the context of group preferences, and leave the case of individual preferences and its implications on the cost of achieving fairness for future work.


\section{Training preferred classifiers} \label{sec:classifiers}
In this section, our goal is training preferred treatment and preferred impact group-conditional classifiers, \ie, $\thetab = \{ \thetab_z \}_{z \in \Zcal}$, that maximize utility
given a training set $\Dcal = \{(\xb_i, y_i, z_i)\}_{i=1}^{N}$, where $(\xb_i, y_i, z_i) \sim f(\xb, y, z)$.
In both cases, we will assume that:
\footnote{\scriptsize Exploring the relaxations of these assumptions is a very interesting avenue for future work.}
\vspace{-1mm}
\begin{itemize}[leftmargin=6mm]
\item[I.] Each group-conditional classifier is a
convex boundary-based classifier. For ease of exposition, in this section, we additionally assume these classifiers to be linear, \ie, $\thetab_z(\xb) = \thetab_z^{T} \xb$, where
$\thetab_z$ is a parameter that defines the decision boundary in the feature space.
We relax the linearity assumption in Appendix~\ref{app:non-lin-svm} and extend our methodology to a non-linear SVM classifier.

\item[II.] The utility function $\Ucal$ is defined as the overall accuracy of the group-conditional classifiers, \ie,
\begin{equation}
\Ucal(\thetab) = \EE_{\xytup} [\II\{\sgn(\thetab(\xb)) = y\}] = \sum_{z \in \Zcal} \EE_{\xb, y | z} [\II\{\sgn(\thetab_z^{T} \xb) = y\}] f(z). \label{eq:utility}
\end{equation}
\vspace{-4mm}
\item[III.] The group benefit $\Bcal_z$ for users sharing the sensitive attribute value $z$ is defined as their average probability of being classified into the
positive class, \ie,
\begin{equation}
\Bcal_z(\thetab) = \EE_{\xb | z} [\II\{\sgn(\thetab(\xb)) = 1\}] =  \EE_{\xb | z} [\II\{\sgn(\thetab_z^{T} \xb) = 1\}]. \label{eq:group-benefit}
\end{equation}
\vspace{-4mm}
\end{itemize}

\xhdr{Preferred impact classifiers}
Given a  impact parity classifier with decision boundary parameters $\{ \thetab'_z \}_{z \in \Zcal}$, one could think of finding the decision boundary parameters $\{ \thetab_z \}_{z \in \Zcal}$
of a preferred impact classifier that maximizes utility by solving the following optimization problem:
\begin{equation}\label{eq:cons_preferred_im_intractable}
	\begin{array}{ll}
	\underset{\{\thetab_z\}}{\mbox{minimize}} & -\frac{1}{N} \sum_{(\xb, y, z) \in \Dcal} \II\{\sgn(\thetab_z^{T} \xb) = y\} \\
		\mbox{subject to} & \sum_{\xb \in \Dcal_z} \II\{\sgn(\thetab_z^{T} \xb) = 1\} \geq \sum_{\xb \in \Dcal_z} \II\{\sgn({\thetab'_z}^{T} \xb) = 1\} \quad \mbox{for all} \, z \in \Zcal,
	\end{array}
\end{equation}
where $\Dcal_z = \{(\xb_i, y_i, z_i) \in \Dcal \,|\, z_i = z\}$ denotes the set of users in the training set sharing sensitive attribute value $z$, the objective uses
an empirical estimate of the utility, defined by Eq.~\ref{eq:utility}, and the preferred impact constraints, defined by Eq.~\ref{eq:pref-gc}, use empirical estimates
of the group benefits, defined by Eq.~\ref{eq:group-benefit}.
Here, note that the right hand side of the inequalities does not contain any variables and can be precomputed, \ie, the  impact parity classifiers $\{ \thetab'_z \}_{z \in \Zcal}$ are given.

Unfortunately, it is very challenging to solve the above optimization problem since
both the objective and constraints are nonconvex.
To overcome this difficulty, we minimize instead a convex loss function $\ell_{\thetab}(\xb, y)$, which is classifier dependent~\cite{bishop2006pattern}, and approximate
the group benefits using a ramp (convex) function $r(x) = \max(0, x)$, \ie,
\begin{equation}\label{eq:cons_preferred_im_tractable}
	\begin{array}{ll}
	\underset{\{\thetab_z\}}{\mbox{minimize}} & -\frac{1}{N} \sum_{(\xb, y, z) \in \Dcal} \ell_{\thetab_{z}}(\xb, y) + \sum_{z \in \Zcal} \lambda_z \Omega(\thetab_z) \\
		\mbox{subject to} & \sum_{\xb \in \Dcal_z} \max(0, \thetab_z^{T} \xb) \geq \sum_{\xb \in \Dcal_z} \max(0, {\thetab'_z}^{T} \xb) \quad \mbox{for all} \, z \in \Zcal,
	\end{array}
\end{equation}
which, for any convex regularizer $\Omega(\cdot)$, is a disciplined convex-concave program (DCCP) and thus can be efficiently solved using well-known heuristics~\cite{boyd_concave_convex}.
For example, if we particularize the above formulation to group-conditional (standard) logistic regression classifiers $\thetab'_z$ and $\thetab_z$ and $L_2$-norm regularizer,
then, Eq.~\ref{eq:cons_preferred_im_tractable} adopts the following form:
\begin{equation}\label{eq:cons_preferred_tractable-lr}
	\begin{array}{ll}
	\underset{\{\thetab_z\}}{\mbox{minimize}} & -\frac{1}{N} \sum_{(\xb, y, z) \in \Dcal} \log p(y | \xb, \thetab_{z}) + \sum_{z \in \Zcal} \lambda_z ||\thetab_z||^2 \\
		\mbox{subject to} & \sum_{\xb \in \Dcal_z} \max(0, \thetab_z^{T} \xb) \geq \sum_{\xb \in \Dcal_z} \max(0, \thetab{'_{z}}^{T} \xb) \quad \mbox{for all} \, z \in \Zcal.
	\end{array}
\end{equation}
where $p(y = 1 | \xb, \thetab_z) = \frac{1}{1+e^{-\thetab_z^T \xb}}$.

The constraints can similarly be added to other convex boundary-based classifiers like linear SVM. We further expand on particularizing the constraints for non-linear SVM in Appendix~\ref{app:non-lin-svm}.


\xhdr{Preferred treatment classifiers}
Similarly as in the case of preferred impact classifiers, one could think of finding the decision boundary parameters $\{ \thetab_z \}_{z \in \Zcal}$ of a preferred treatment classifier that maximizes
utility by solving the following optimization problem:
\begin{equation}\label{eq:cons_preferred_tr_intractable}
	\begin{array}{ll}
	\underset{\{\thetab_z\}}{\mbox{minimize}} & -\frac{1}{N} \sum_{(\xb, y, z) \in \Dcal} \II\{\sgn(\thetab_{z}^{T} \xb) = y\} \\
		\mbox{subject to} & \sum_{\xb \in \Dcal_z} \II\{\sgn(\thetab_{z}^{T} \xb) = 1\} \geq \sum_{\xb \in \Dcal_z} \II\{\sgn(\thetab_{z'}^{T} \xb) = 1\} \quad \mbox{for all} \, z, z'{} \in \Zcal,
	\end{array}
\end{equation}
where $\Dcal_z = \{(\xb_i, y_i, z_i) \in \Dcal \,|\, z_i = z\}$ denotes the set of users in the training set sharing sensitive attribute value $z$, the objective uses
an empirical estimate of the utility, defined by Eq.~\ref{eq:utility}, and the preferred treatment constraints, defined by Eq.~\ref{eq:enf}, use empirical estimates
of the group benefits, defined by Eq.~\ref{eq:group-benefit}.
Here, note that both the left and right hand side of the inequalities contain optimization variables.

However, the objective and constraints in the above problem are also nonconvex and thus we adopt a similar strategy as in the case of preferred impact classifiers. More specifically, we
solve instead the following tractable problem:
\begin{equation}\label{eq:cons_preferred_tr_tractable}
	\begin{array}{ll}
	\underset{\{\thetab_z\}}{\mbox{minimize}} & -\frac{1}{N} \sum_{(\xb, y, z) \in \Dcal} \ell_{\thetab_{z}}(\xb, y) + \sum_{z \in \Zcal} \lambda_z \Omega(\thetab_z) \\
		\mbox{subject to} & \sum_{\xb \in \Dcal_z} \max(0, \thetab_z^{T} \xb) \geq \sum_{\xb \in \Dcal_z} \max(0, \thetab_{z'}^{T} \xb) \quad \mbox{for all} \, z, z' \in \Zcal,
	\end{array}
\end{equation}
which, for any convex regularizer $\Omega(\cdot)$, is also a disciplined convex-concave program (DCCP) and can be efficiently solved.


\vspace{-2mm}
\section{Evaluation}\label{sec:eval}
\vspace{-2mm}

In this section, we compare the performance of preferred treatment and preferred impact classifiers against unconstrained, treatment parity and impact parity classifiers on
a variety of synthetic and real-world datasets. More specifically, we consider the following classifiers, which we train to maximize utility subject to the corresponding
constraints:
\vspace{-2mm}
\begin{itemize}[leftmargin=6mm]
\item[---] \emphb{Uncons}: an unconstrained classifier that resorts to group-conditional classifiers. It violates treatment parity---it trains a separate classifier per sensitive attribute value
group---and potentially violates impact parity---it may lead to different benefits for different groups.
\item[---] \emphb{Parity}: a parity classifier that does not use the sensitive attribute group information in the decision making, but only during the training phase, and is constrained to satisfy both treatment parity---its
decisions do not change based on the users' sensitive attribute value as it does not resort to group-conditional classifiers---and impact parity---it ensures that the benefits for all groups are the same. We train
this classifier using the methodology proposed by Zafar et al.~\cite{zafar_fairness}.
\item[---] \emphb{Preferred treatment}: a classifier that resorts to group-conditional classifiers and  is constrained to satisfy preferred treatment---each group gets the highest benefit with its own classifier than any
other group'{}s classifier.
\item[---] \emphb{Preferred impact}: a classifier that resorts to group-conditional classifiers and is constrained to be preferred over the \emph{Parity} classifier.
\item[---] \emphb{Preferred both}: a classifier that resort to group-conditional classifiers and  is constrained to satisfy both \emph{preferred treatment} and \emph{preferred impact}.
\end{itemize}
\vspace{-1mm}

For the experiments in this section, we use logistic regression classifiers with $L_2$-norm regularization. We randomly split the corresponding dataset into $70\%$-$30\%$ train-test folds 5 times, and report the average
accuracy and group benefits in the test folds. Appendix~\ref{app:exp_details} describes the details for selecting the optimal $L_2$-norm regularization parameters.
Here, we compute utility ($\Ucal$) as the overall accuracy of a classifier and group benefits ($\Bcal_z$) as the fraction of users sharing
sensitive attribute $z$ that are classified into the positive class.
Moreover, the sensitive attribute is always binary, \ie, $z \in \{0, 1\}$.



\begin{figure*}[t]
    \centering
    \begin{subfigure}[b]{0.27\columnwidth}
        \includegraphics[trim={2.5cm 1cm 2cm 0.6cm},clip, width=.9\columnwidth]{{syn_dtype_1_n_samples_10000_loss_logreg_val_size_0.30_boundaries_Uncons}.pdf}
        \caption{Uncons} \label{syn:uncons}
    \end{subfigure}
        \hspace*{-7mm}
        \begin{subfigure}[b]{0.27\columnwidth}
        \includegraphics[trim={2.5cm 1cm 2cm 0.6cm},clip, width=.9\columnwidth]{{syn_dtype_1_n_samples_10000_loss_logreg_val_size_0.30_boundaries_Parity}.pdf}
        \caption{Parity}\label{syn:parity}
    \end{subfigure}
        \hspace*{-7mm}
    \begin{subfigure}[b]{0.27\columnwidth}
       \includegraphics[trim={2.5cm 1cm 2cm 0.6cm},clip, width=.9\columnwidth]{{syn_dtype_1_n_samples_10000_loss_logreg_val_size_0.30_boundaries_Prf_Imp}.pdf}
        \caption{{Preferred impact}}\label{syn:preferred}
    \end{subfigure}
        \hspace*{-7mm}
    \begin{subfigure}[b]{0.27\columnwidth}
        \includegraphics[trim={2.5cm 1cm 2cm 0.6cm},clip, width=.9\columnwidth]{{syn_dtype_1_n_samples_10000_loss_logreg_val_size_0.30_boundaries_Prf_both}.pdf}
        \caption{Preferred both} \label{syn:both}
    \end{subfigure}
    \caption{
    [Synthetic data]
    Crosses denote group-0 (points with $z=0$) and circles denote group-1. Green points belong to the positive class in the training data whereas red points belong to the negative class.
    Each panel shows the accuracy of the decision making scenario along with group benefits ($\Bcal_0$ and $\Bcal_1$) provided by each of the classifiers involved.
    For group-conditional classifiers, cyan (blue) line denotes the decision boundary for the classifier of group-0 (group-1). Parity case (panel (b)) consists of just one classifier for both groups in order to meet the  treatment parity criterion.
    }\label{fig:syn}
\end{figure*}


\vspace{-2mm}
\subsection{Experiments on synthetic data}
\vspace{-2mm}
\xhdr{Experimental setup} Following Zafar et al.~\cite{zafar_fairness}, we generate a synthetic dataset in which the unconstrained classifier (\emph{Uncons}) offers different benefits to each sensitive attribute group. In particular,
we generate $20{,}000$ binary class labels $y \in \{-1,1 \}$ uniformly at random along with their corresponding two-dimensional feature vectors sampled from the following Gaussian distributions:
$p(\xb | y=1) = \Ncal([2;2], [5,1;1,5])$ and $p(\xb | y=-1) = \Ncal([-2;-2], [10,1;1,3])$.
Then, we generate each sensitive attribute from the Bernoulli distribution $p(z=1)=p(\xb'{} | y=1)/(p(\xb'{} | y=1)+p(\xb'{} | y=-1))$, where $\xb'{}$ is a rotated version
of $\xb$, \ie, $\xb'{} = [\cos(\pi/8), -\sin(\pi/8); \sin(\pi/8),\cos(\pi/8)]$.
Finally, we train the five classifiers described above and compute their overall (test) accuracy and (test) group benefits.

\xhdr{Results}
Figure~\ref{fig:syn} shows the trained classifiers, along with their overall accuracy and group benefits. We can make several interesting observations:

The \emphb{Uncons} classifier leads to an accuracy of $0.87$, however, the group-conditional boundaries and high disparity in
treatment for the two groups ($0.16$ vs. $0.85$) mean that it satisfies neither treatment parity nor impact parity.
Moreover, it leads to only a small violation of preferred treatment---benefits for group-0 would increase slightly from 0.16 to 0.20 by adopting the classifier of group-1. However, this will not always be the
case, as we will later show in the experiments on real data.


The \emphb{Parity} classifier satisfies both treatment and impact parity, however, it does so at a large cost in terms of accuracy, which drops from $0.87$ for \emph{Uncons} to $0.57$
for \emph{Parity}.

The \emphb{Preferred treatment} classifier (not shown in the figure), leads to a minor change in decision boundaries as compared to the \emph{Uncons} classifier to achieve preferred treatment.
Benefits for group-0 (group-1) with its own classifier are $0.20$ ($0.84$) as compared to $0.17$ ($0.83$) while using the classifier of group-1 (group-0).
The accuracy of this classifier is $0.87$.

The \emphb{Preferred impact} classifier, by making use of a looser notion of fairness compared to impact parity, provides higher benefits for both groups at a much smaller
cost in terms of accuracy than the \emph{Parity} classifier ($0.76$ vs. $0.57$).
Note that, while the \emph{Parity} classifier achieved equality in benefits by misclassifying \textit{negative examples from group-0} into the positive class and misclassifying \textit{positive
examples from group-1} into the negative class, the \emph{Preferred impact} classifier only incurs the former type of misclassifications.
However, the outcomes of the \emph{Preferred impact} classifier do not satisfy the preferred treatment criterion: group-1 would attain higher benefit if it used the classifier of group-0 ($0.96$
as compared to $0.86$).

Finally, the classifier that satisfies preferred treatment and preferred impact (\emphb{Preferred both}) achieves an accuracy and benefits at par with the \emph{Preferred impact} classifier.


\begin{figure*}[t]
 \centering
    \includegraphics[bb=352 50 554 1058, trim={1.5cm 2cm 1.5cm 1.5cm},clip, width=1\columnwidth]{all_real_data.pdf}
    \caption{
    The figure shows the accuracy and benefits received by the two groups for various decision making scenarios. `Prf-treat.', `Prf-imp.', and `Prf-both' respectively correspond to the classifiers satisfying preferred treatment, preferred impact, and both preferred treatment and impact criteria.
    Sensitive attribute values $0$ and $1$ denote blacks and whites in ProPublica COMPAS dataset and NYPD SQF datasets, and women and men in the Adult dataset.
    $\Bcal_{i}(\thetab_j)$ denotes the benefits obtained by group $i$ when using the classifier of group $j$.
    For the \emph{Parity} case, we train just one classifier for both the groups, so the benefits do not change by adopting other group's classifier.
    }
    \vspace{-5mm}
    \label{fig:real}
\end{figure*}


We present the results of applying our fairness constraints on a non linearly-separable dataset with a SVM classifier with a radial basis function (RBF) kernel in Appendix~\ref{app:exp_non_lin_svm}.

\vspace{-3mm}
\subsection{Experiments on real data}
\vspace{-2mm}
\xhdr{Dataset description and experimental setup} We experiment with three real-world datasets: the COMPAS recidivism prediction dataset compiled by ProPublica~\cite{propublica_compas}, the
Adult income dataset from UCI machine learning repository~\cite{adult_dataset}, and the New York Police Department (NYPD) Stop-question-and-frisk (SQF) dataset made publicly available by
NYPD~\cite{sqf_dataset}. These datasets have been used by a number of prior studies in the fairness-aware machine learning literature~\cite{feldman_kdd15,goel_frisk,zafar_dmt,icml2013_zemel13,zafar_fairness}.

In the COMPAS dataset, the classification task is to predict whether a criminal defendant would recidivate within two years (negative class) or not (positive class);
in the Adult dataset, the task is to predict whether a person earns more than $50$K USD per year (positive class) or not; and, in the SQF dataset, the task is to predict
whether a pedestrian should be stopped on the suspicion of having an illegal weapon or not (positive class). In all datasets, we assume being classified as positive
to be the beneficial outcome.
Additionally, we divide the subjects in each dataset into two sensitive attribute value groups: women (group-$0$) and men (group-$1$) in the Adult dataset and
blacks (group-0) and whites (group-1) in the COMPAS and SQF datasets.
The supplementary material (Appendix~\ref{app:data}) contains more information on the sensitive and the non-sensitive features as well as the class distributions.
\footnote{\scriptsize Since the SQF dataset is highly skewed in terms of class distribution ($\sim$$97\%$ points in the positive class) resulting in a trained classifier predicting all points as positive
(yet having $97\%$ accuracy), we subsample the dataset to have equal class distribution. Another option would be using penalties proportional to the size of the class, but we observe
that an unconstrained classifier with class penalties gives similar predictions as compared to a balanced dataset. We decided to experiment with the balanced dataset since
the accuracy drops in this dataset are easier to interpret.}



\xhdr{Results}
Figure~\ref{fig:real} shows the accuracy achieved by the five classifiers described above along with the benefits they provide for the three datasets.
We can draw several interesting observations:\footnote{\scriptsize The unfairness in the SQF dataset is different from what one would expect~\cite{nycly_sqf}---an unconstrained classifier gives more benefits to blacks as compared to whites. This is due to the fact that a larger fraction of stopped whites were found to be in possession on an illegal weapon (Tables~\ref{table:sqf-stats-imba} and~\ref{table:sqf-stats} in Appendix~\ref{app:data}).}

In all cases, the \emphb{Uncons} classifier, in addition to violating treatment parity (a separate classifier for each group) and impact parity (high disparity in group benefits), also violates the preferred
treatment criterion (in all cases, at least one of group-0 or group-1 would benefit more by adopting the other group'{}s classifier).
On the other hand, the \emphb{Parity} classifier satisfies the treatment parity and impact parity but it does so at a large cost in terms of accuracy.

The \emphb{Preferred treatment} classifier provides a much higher accuracy than the \emph{Parity} classifier---its accuracy is at par with that of the \emph{Uncons} classifier---while satisfying the
preferred treatment criterion. However, it does not meet the preferred impact criterion.
The \emphb{Preferred impact} classifier meets the preferred impact criterion but does not always satisfy preferred treatment. Moreover, it also leads to a better accuracy then \emph{Parity} classifier in
all cases. However, the gain in accuracy is more substantial for the SQF datasets as compared to the COMPAS and Adult dataset.


The classifier satisfying preferred treatment and preferred impact (\emphb{Preferred both}) has a somewhat underwhelming performance in terms of accuracy for the Adult dataset. While the performance of this classifier is better than the
\emph{Parity} classifier in the COMPAS dataset and NYPD SQF dataset, it is slightly worse for the Adult dataset.


In summary, the above results show that ensuring either preferred treatment or preferred impact is less costly in terms of accuracy loss than ensuring parity-based fairness, however,
ensuring both preferred treatment and preferred impact can lead to comparatively larger accuracy loss in certain datasets. We hypothesize that this loss in accuracy may be partly due to splitting the number of available
samples into groups during training---each group-conditional classifier use only samples from the corresponding sensitive attribute group---hence decreasing the effectiveness of empirical risk minimization.








\vspace{-3mm}
\section{Conclusion}
\vspace{-3mm}

In this paper, we introduced two preference-based notions of fairness---preferred treatment and preferred impact---establishing a previously unexplored
connection between fairness-aware machine learning and the economics and game theoretic concepts of envy-freeness and bargaining.
Then, we proposed tractable proxies to design boundary-based classifiers satisfying these fairness notions and experimented with a variety
of synthetic and real-world datasets, showing that preference-based fairness often allows for greater decision accuracy than existing parity-based
fairness notions.

Our work opens many promising venues for future work. For example, our methodology is limited to convex boundary-based classifiers. A natural follow up would be to
extend our methodology to other types of classifiers, \eg, neural networks and decision trees.
In this work, we defined preferred treatment and preferred impact in the context of group preferences, however, it would be worth revisiting the proposed definitions in the
context of individual preferences.
The fair division literature establishes a variety of fairness axioms~\cite{nash1950bargaining} such as Pareto-optimality and scale invariance. It would be interesting
to study such axioms in the context of fairness-aware machine learning.

Finally, we note that while moving from parity to preference-based fairness offers many attractive properties, we acknowledge it may not always be the most
appropriate notion, \eg, in some scenarios, parity-based fairness may very well present the eventual goal and be more desirable~\cite{plato-disc}.










\subsection*{Acknowledgments}
AW acknowledges support by the Alan Turing Institute under EPSRC grant EP/N510129/1, and by the Leverhulme Trust via the CFI.

\small \bibliographystyle{abbrv}
\begin{thebibliography}{10}

\bibitem{sqf_dataset}
{Stop, Question and Frisk Data}.
\newblock
  \href{http://www1.nyc.gov/site/nypd/stats/reports-analysis/stopfrisk.page}{http://www1.nyc.gov/site/nypd/stats/reports-analysis/stopfrisk.page},
  2017.

\bibitem{adult_dataset}
{Adult data}.
\newblock
  \href{https://archive.ics.uci.edu/ml/datasets/adult}{https://archive.ics.uci.edu/ml/datasets/adult},
  1996.

\bibitem{plato-disc}
A.~Altman.
\newblock Discrimination.
\newblock In {\em The Stanford Encyclopedia of Philosophy}. Metaphysics
  Research Lab, Stanford University, 2016.
\newblock
  \href{https://plato.stanford.edu/archives/win2016/entries/discrimination/}{https://plato.stanford.edu/archives/win2016/entries/discrimination/}.

\bibitem{barocas_2016}
S.~Barocas and A.~D. Selbst.
\newblock {Big Data's Disparate Impact}.
\newblock {\em {California Law Review}}, 2016.

\bibitem{berliant_envy}
M.~Berliant and W.~Thomson.
\newblock {On the Fair Division of a Heterogeneous Commodity}.
\newblock {\em {Journal of Mathematics Economics }}, 1992.

\bibitem{bishop2006pattern}
C.~M. Bishop.
\newblock {\em {Pattern Recognition and Machine Learning}}.
\newblock Springer, 2006.

\bibitem{cadlers_naivebayes}
T.~Calders and S.~Verwer.
\newblock {Three Naive Bayes Approaches for Discrimination-Free
  Classification}.
\newblock {\em {Data Mining and Knowledge Discovery}}, 2010.

\bibitem{Chapelle:2007:TSV:1246422.1246423}
O.~Chapelle.
\newblock {Training a Support Vector Machine in the Primal}.
\newblock {\em Neural Computation}, 2007.

\bibitem{dimpact_fpr}
A.~Chouldechova.
\newblock {Fair Prediction with Disparate Impact:A Study of Bias in Recidivism
  Prediction Instruments}.
\newblock {\em {arXiv preprint, arXiv:1610.07524}}, 2016.

\bibitem{civil_rights_act}
{Civil Rights Act}.
\newblock Civil Rights Act of 1964, Title VII, Equal Employment Opportunities,
  1964.

\bibitem{goel_cost_fairness}
S.~Corbett-Davies, E.~Pierson, A.~Feller, S.~Goel, and A.~Huq.
\newblock {Algorithmic Decision Making and the Cost of Fairness}.
\newblock In {\em {KDD}}, 2017.

\bibitem{Dwork2012}
C.~Dwork, M.~Hardt, T.~Pitassi, and O.~Reingold.
\newblock Fairness {T}hrough {A}wareness.
\newblock In {\em ITCSC}, 2012.

\bibitem{dwork2017decoupled}
C.~Dwork, N.~Immorlica, A.~T. Kalai, and M.~Leiserson.
\newblock {Decoupled Classifiers for Fair and Efficient Machine Learning}.
\newblock {\em arXiv preprint arXiv:1707.06613}, 2017.

\bibitem{feldman_kdd15}
M.~Feldman, S.~A. Friedler, J.~Moeller, C.~Scheidegger, and
  S.~Venkatasubramanian.
\newblock {Certifying and Removing Disparate Impact}.
\newblock In {\em KDD}, 2015.

\bibitem{friedler_impossibility}
S.~A. Friedler, C.~Scheidegger, and S.~Venkatasubramanian.
\newblock {On the (im)possibility of Fairness}.
\newblock {\em {arXiv preprint arXiv:1609.07236}}, 2016.

\bibitem{gentle2012handbook}
J.~E. Gentle, W.~K. H{\"a}rdle, and Y.~Mori.
\newblock {\em {Handbook of Computational Statistics: Concepts and Methods}}.
\newblock Springer Science \& Business Media, 2012.

\bibitem{goh_nips2016}
G.~Goh, A.~Cotter, M.~Gupta, and M.~Friedlander.
\newblock {Satisfying Real-world Goals with Dataset Constraints}.
\newblock In {\em {NIPS}}, 2016.

\bibitem{hardt_nips16}
M.~Hardt, E.~Price, and N.~Srebro.
\newblock {Equality of Opportunity in Supervised Learning}.
\newblock In {\em {NIPS}}, 2016.

\bibitem{joseph_bandits}
M.~Joseph, M.~Kearns, J.~Morgenstern, and A.~Roth.
\newblock {Fairness in Learning: Classic and Contextual Bandits}.
\newblock In {\em NIPS}, 2016.

\bibitem{kamiran_sampling}
F.~Kamiran and T.~Calders.
\newblock {Classification with No Discrimination by Preferential Sampling}.
\newblock In {\em {BENELEARN}}, 2010.

\bibitem{kamishima_regularizer}
T.~Kamishima, S.~Akaho, H.~Asoh, and J.~Sakuma.
\newblock {Fairness-aware Classifier with Prejudice Remover Regularizer}.
\newblock In {\em {PADM}}, 2011.

\bibitem{kleinberg_itcs17}
J.~Kleinberg, S.~Mullainathan, and M.~Raghavan.
\newblock {Inherent Trade-Offs in the Fair Determination of Risk Scores}.
\newblock In {\em ITCS}, 2017.

\bibitem{propublica_compas}
J.~Larson, S.~Mattu, L.~Kirchner, and J.~Angwin.
\newblock
  \href{https://github.com/propublica/compas-analysis}{https://github.com/propublica/compas-analysis},
  2016.

\bibitem{salvatore_knn}
B.~T. Luong, S.~Ruggieri, and F.~Turini.
\newblock {kNN as an Implementation of Situation Testing for Discrimination
  Discovery and Prevention}.
\newblock In {\em {KDD}}, 2011.

\bibitem{bigdatawhitehouse2016}
C.~Mu{\~n}oz, M.~Smith, and D.~Patil.
\newblock {Big Data: A Report on Algorithmic Systems, Opportunity, and Civil
  Rights}.
\newblock {\em Executive Office of the President. The White House.}, 2016.

\bibitem{nash1950bargaining}
J.~F. Nash~Jr.
\newblock {The Bargaining Problem}.
\newblock {\em Econometrica: Journal of the Econometric Society}, 1950.

\bibitem{nycly_sqf}
NYCLU.
\newblock {Stop-and-Frisk Data}.
\newblock
  \href{https://www.nyclu.org/en/stop-and-frisk-data}{https://www.nyclu.org/en/stop-and-frisk-data},
  2017.

\bibitem{pedreschi_discrimination}
D.~Pedreschi, S.~Ruggieri, and F.~Turini.
\newblock {Discrimination-aware Data Mining}.
\newblock In {\em {KDD}}, 2008.

\bibitem{goel_frisk}
R.~S. Sharad~Goel, Justin M.~Rao.
\newblock {Precinct or Prejudice? Understanding Racial Disparities in New York
  City's Stop-and-Frisk Policy}.
\newblock {\em {Annals of Applied Statistics}}, 2015.

\bibitem{boyd_concave_convex}
X.~Shen, S.~Diamond, Y.~Gu, and S.~Boyd.
\newblock {Disciplined Convex-Concave Programming}.
\newblock {\em {arXiv:1604.02639}}, 2016.

\bibitem{varian_envy}
H.~R. Varian.
\newblock {Equity, Envy, and Efficiency}.
\newblock {\em {Journal of Economic Theory}}, 1974.

\bibitem{zafar_dmt}
M.~B. Zafar, I.~Valera, M.~G. Rodriguez, and K.~P. Gummadi.
\newblock {Fairness Beyond Disparate Treatment \& Disparate Impact: Learning
  Classification without Disparate Mistreatment}.
\newblock In {\em {WWW}}, 2017.

\bibitem{zafar_fairness}
M.~B. Zafar, I.~Valera, M.~G. Rodriguez, and K.~P. Gummadi.
\newblock {Fairness Constraints: Mechanisms for Fair Classification}.
\newblock In {\em {AISTATS}}, 2017.

\bibitem{icml2013_zemel13}
R.~Zemel, Y.~Wu, K.~Swersky, T.~Pitassi, and C.~Dwork.
\newblock Learning {F}air {R}epresentations.
\newblock In {\em ICML}, 2013.

\end{thebibliography}


\newpage
\begin{appendix}

\onecolumn
\clearpage
\normalsize



\section{Particularizing fairness constraints for non-linear SVM} \label{app:non-lin-svm}

For a non-linear SVM, given a training dataset $\Dcal = \{(\xb_i, y_i, z_i)\}_{i=1}^{N}$, one typically finds the optimal decision boundary parameters $\alphab$ by
solving the dual of the corresponding optimization problem~\cite{gentle2012handbook}, which takes the following form:
\begin{align}
       \begin{array}{ll} \nonumber
		\underset{\alphab}{\mbox{minimize}} &    \frac{1}{2} \alphab^{T}\mathbf{G}\alphab - \mathbf{1}^T \alphab  \\
        \mbox{subject to} & 0 \leq \alphab \leq C, \\
        &  \yb^T \alphab = 0
    \end{array}
\end{align}
where $\alphab = [\alpha_1, \alpha_2, \ldots, \alpha_N]^T$ are the optimization variables specifying the decision boundary,
$\yb = [y_1, y_2, \ldots, y_N]^T$ are the class labels,
and $\Gb$ is the $N \times N$ Gram matrix with
$\Gb_{i,j} = y_iy_jk(\xb_i, \xb_j)$. Here, the kernel function, $k(\xb_i, \xb_j) = \phi(\xb_i) \cdot \phi(\xb_j)$ denotes the inner product between a pair of transformed feature vectors.
Then, given an unknown data point $\xb$, one computes $\hat{y} = \sgn(\alphab(\xb))$  where $\alphab(\xb) = \sum_{i=1}^{N} \alpha_i y_i k(\xb, \xb_i)$ where $\alphab(\xb)$ can still be interpreted as the signed distance from the decision boundary.

Given, this specification, one can particularize Eq.~\ref{eq:cons_preferred_im_tractable} for training group-conditional preferred impact non-linear SVMs as:
\begin{align}
       \begin{array}{ll} \nonumber
		\underset{\{\alphab_z\}}{\mbox{minimize}} &  \sum_{z \in \Zcal}  \frac{1}{2} \alphab_{z}^{T}\mathbf{G}_z\alphab_z  - \mathbf{1}^T \alphab_z  \\
        \mbox{subject to} & 0 \leq \alphab_z \leq C_z \quad \mbox{for all } z \in \Zcal, \\
        & \yb_{z}^{T} \alphab_z   = 0 \quad \mbox{for all } z \in \Zcal, \\
        & \sum_{\xb \in \Dcal_z} \max(0, \alphab_z(\xb)) \geq \sum_{\xb \in \Dcal_z} \max(0, \alphab_{z}^{'}(\xb)) \quad \mbox{for all } z \in \Zcal, \\
    \end{array}
\end{align}
where $\{\alphab_{z}^{'}\}_{z \in \Zcal}$ are the given parity impact classifiers and $\mathbf{G}_z$ and $\yb_z$ denote the Gram matrix and class label vector for the sensitive attribute group $z$.

One can similarly particularize Eq.~\ref{eq:cons_preferred_tr_tractable} for training group-conditional preferred treatment non-linear SVMs as:
\begin{align}
       \begin{array}{ll} \nonumber
		\underset{\{\alphab_z\}}{\mbox{minimize}} &  \sum_{z \in \Zcal}  \frac{1}{2} \alphab_{z}^{T}\mathbf{G}_z\alphab_z  -  \mathbf{1}^T \alphab_z \\
        \mbox{subject to} & 0 \leq \alphab_z \leq C_z \quad \mbox{for all } z \in \Zcal, \\
        & \yb_{z}^{T} \alphab_z   = 0 \quad \mbox{for all } z \in \Zcal, \\
        & \sum_{\xb \in \Dcal_z} \max(0, \alphab_z(\xb)) \geq \sum_{\xb \in \Dcal_z} \max(0, \alphab_{z'}(\xb)) \quad \mbox{for all } z,z' \in \Zcal. \\
    \end{array}
\end{align}

One can similarly add the constraints to the non-linear SVM in the primal form~\cite{Chapelle:2007:TSV:1246422.1246423}.


\section{Experimental details} \label{app:exp_details}

In this section, we provide details for selecting the  optimal $L_2$-norm regularization parameters ($\lambda$) for the experiments performed in Section~\ref{sec:eval}. For performing the validation procedure below, we first split the training dataset ($\Dcal_{train}$) further into a $70\%$-$30\%$ train set ($\Dcal_{tr}$) and a validation set ($\Dcal_{val}$). Then, for a given range $L = \{\lambda_1, \lambda_2, \ldots, \lambda_k\}$ of candidate values, we select the optimal ones as follows.

\xhdr{Unconstrained and parity classifiers}
These cases consist of \emph{training one classifier at a time}. For the unconstrained classifier, we train one classifier for each sensitive attribute group separately. For the parity classifier, we train one classifier for all groups.

For each value of $\lambda \in L$, we train the classifier on $\Dcal_{tr}$, and choose the one that provides best accuracy on the validation set $\Dcal_{val}$. We call it $\lambda^{opt}$. We then train the classifier on the whole training dataset $\Dcal_{train}$ with  $\lambda^{opt}$.


\xhdr{Preferentially fair classifiers}
Training preferentially fair classifiers in Eq.~\ref{eq:cons_preferred_im_tractable} and Eq.~\ref{eq:cons_preferred_tr_tractable} consists of jointly minimizing the objective function for both groups while satisfying the fairness constraints. For training these classifiers for two groups (say group-0 and group-1), we take all combinations of $\lambda_0, \lambda_1 \in L$, and choose the combination that provides best accuracy on $\Dcal_{val}$ while satisfying the constraints.
For real-world datasets, we specify the following tolerance level for the constraints: for a given pair of $\lambda_0, \lambda_1 \in L$, we consider the constraints to be satisfied if the observed value of group benefits $\Bcal_z$ in the validation set $\Dcal_{val}$ and the desired value are at least within $90\%$ of each other, and additionally, the difference between them is no more that $0.03$. We notice that setting hard thresholds with no tolerance on real-world datasets sometimes leads to divergent solutions in terms of group benefits.
We hypothesize that this effect may be due to the underlying variance between $D_{tr}$ and $D_{val}$.

\section{Experiments with non-linear SVM} \label{app:exp_non_lin_svm}





\begin{figure*}[t]
    \centering
    \begin{subfigure}[b]{0.27\columnwidth}
        \includegraphics[trim={1cm 1cm 1cm 0cm},clip, width=.9\columnwidth]{{syn_dtype_2_n_samples_1000_loss_svm_non_lin_val_size_0.30_boundaries_Uncons_group_0}.pdf}
        \includegraphics[trim={1cm 1cm 1cm 0cm},clip, width=.9\columnwidth]{{syn_dtype_2_n_samples_1000_loss_svm_non_lin_val_size_0.30_boundaries_Uncons_group_1}.pdf}
        \caption{Uncons} \label{syn:uncons}
    \end{subfigure}
    \hspace*{-7mm}
   \begin{subfigure}[b]{0.27\columnwidth}
        \includegraphics[trim={1cm 1cm 1cm 0cm},clip, width=.9\columnwidth]{{syn_dtype_2_n_samples_1000_loss_svm_non_lin_val_size_0.30_boundaries_Parity}.pdf}
        \includegraphics[trim={1cm 1cm 1cm 0cm},clip, width=.9\columnwidth]{{syn_dtype_2_n_samples_1000_loss_svm_non_lin_val_size_0.30_boundaries_Parity}.pdf}
        \caption{Parity} \label{syn:uncons}
    \end{subfigure}
    \hspace*{-7mm}
   \begin{subfigure}[b]{0.27\columnwidth}
        \includegraphics[trim={1cm 1cm 1cm 0cm},clip, width=.9\columnwidth]{{syn_dtype_2_n_samples_1000_loss_svm_non_lin_val_size_0.30_boundaries_Prf-Treat_group_0}.pdf}
        \includegraphics[trim={1cm 1cm 1cm 0cm},clip, width=.9\columnwidth]{{syn_dtype_2_n_samples_1000_loss_svm_non_lin_val_size_0.30_boundaries_Prf-Treat_group_1}.pdf}
        \caption{Preferred treatment} \label{syn:uncons}
    \end{subfigure}
    \hspace*{-7mm}
   \begin{subfigure}[b]{0.27\columnwidth}
        \includegraphics[trim={1cm 1cm 1cm 0cm},clip, width=.9\columnwidth]{{syn_dtype_2_n_samples_1000_loss_svm_non_lin_val_size_0.30_boundaries_Prf-Imp_group_0}.pdf}
        \includegraphics[trim={1cm 1cm 1cm 0cm},clip, width=.9\columnwidth]{{syn_dtype_2_n_samples_1000_loss_svm_non_lin_val_size_0.30_boundaries_Prf-Imp_group_1}.pdf}
        \caption{Preferred impact} \label{syn:uncons}
    \end{subfigure}
    \caption{
    [Non linearly separable synthetic data]
    Crosses denote group-0 (points with $z=0$) and circles denote group-1. Green points belong to the positive class in the training data whereas red points belong to the negative class.
    Each panel shows the classifiers with top row containing the classifiers for group-0 and the bottom for group-1, along with the overall accuracy as well as the group benefits ($\Bcal_0$ and $\Bcal_1$) provided by each of the classifiers involved.
    For parity classifier, no group-conditional classifiers are allowed, so both top and bottom row contain the same classifier.
    }\label{fig:syn_non_lin}
\end{figure*}







In this section, we demonstrate the effectiveness of our constraints in ensuring fairness on a non linearly-separable dataset with a SVM  classifier using radial basis function (RBF) kernel.

Following the setup of Zafar et al.~\cite{zafar_fairness},  we  generated a synthetic dataset consisting of $4$,$000$ user binary class labels uniformly at random. We then assign a 2-dimensional user feature vector to each label by drawing samples from the following distributions:
\begin{align*}
p(\xb| y=1, \beta) &= \beta N([2; 2], [5\,\, 1; 1\,\, 5]) +  (1-\beta) N([-2; -2], [10\,\, 1; 1\,\, 3]) \\
p(\xb | y=-1, \beta ) &= \beta N([4; -4], [4\,\, 4; 2\,\, 5]) +  (1-\beta) N([-4; 6], [6\,\, 2; 2\,\, 3])
\end{align*}
where $\beta \in\{0,1\}$ is sampled from $\mbox{Bernoulli}(0.5)$.
We then generate the corresponding user sensitive attributes $z$ by applying the same rotation as detailed in Section~\ref{sec:eval}.

We then train the various classifiers described in Section~\ref{sec:eval}. The results are shown in Figure~\ref{fig:syn_non_lin}. Top row in the figure shows the group-conditional classifiers for group-0, whereas, the bottom row shows the ones for group-1. For the case of parity classifier, due to treatment parity condition, both groups use the same classifier.

The \emphb{Uncons} classifier leads to an accuracy of $0.96$, however, the group-conditional classifiers lead to high disparity in beneficial outcomes for both groups ($0.07$ vs. $0.87$).
The classifier also leads to a violation of preferred treatment---the benefits for group-0 would increase from $0.07$ with its own classifier to $0.17$ with the classifier of group-1.

The \emphb{Parity} classifier satisfies both treatment and impact parity, however, it does so at a large cost in terms of accuracy, which drops from $0.96$ for \emph{Uncons} to $0.61$
for \emph{Parity}.


The \emphb{Preferred treatment} classifier, adjusts the decision boundary for group-0 to remove envy and does so at a small cost in accuracy (from $0.96$ to $0.93$).

The \emphb{Preferred impact} classifier, by making use of the relaxed parity-fairness conditions, provides higher or equal benefits for both groups at a much smaller cost in terms of accuracy than
the \emph{Parity} classifier ($0.84$ vs. $0.61$). The preferred impact classifier in this case also satisfies the preferred treatment criterion.



\section{Dataset statistics} \label{app:data}



For the ProPublica COMPAS dataset, we use the same non-sensitive features as used by Zafar et al.~\cite{zafar_dmt}. The non-sensitive features include number of prior offenses, the degree of the arrest charge (misdemeanor or felony), \etc~The class and sensitive attribute distribution in the dataset is  in Table~\ref{table:compas-stats}.

\begin{table}[h]
\centering
\caption{Recidivism rates in ProPublica COMPAS data for both races.} \label{table:compas-stats}
\begin{tabular}{cccc}
\hline
Race  &  Yes (-ve)       & No (+ve)       & Total \\ \hline
Black &  $ 1,661 (52\%)  $ & $ 1,514 (48\%)$ &  $ 3,175$ \\
White &  $ 8,22  (39\%) $  & $1,281  (61\%)$ &  $ 2,103$ \\ \hline
Total &  $ 2,483 (47\%) $  & $2,795  (53\%)$ &  $ 5,278$ \\ \hline
\end{tabular}
\end{table}



For Adult dataset~\cite{adult_dataset}, we use the same non-sensitive features as a number of prior studies~\cite{feldman_kdd15,zafar_fairness,icml2013_zemel13} on fairness-aware  learning. The non-sensitive features include educational level of the person, number of working hours per week, etc.
The class and sensitive attribute distribution in the dataset is as follows in Table~\ref{table:adult-stats}.

\begin{table}[h]
\centering
\caption{High income ($\geq 50$K USD) in Adult data for both genders.} \label{table:adult-stats}
\begin{tabular}{cccc}
\hline
Gender  & Yes (+ve) & No (-ve) & Total \\ \hline
Males &  $ 9{,}539 (31\%)$&  $ 20{,}988 (69\%)$   &  $ 30{,}527$\\
Females & $1{,}669 (11\%)$&  $ 13{,}026 (89\%)$   &  $14,695 $\\ \hline
Total & $34{,}014 (75\%)$ &  $11{,}208 (25\%)$
&  $ 45{,}222$ \\\hline
\end{tabular}
\end{table}



For the NYPD SQF dataset~\cite{sqf_dataset}, we use the same prediction task and non-sensitive features as used by Goel et al.~\cite{goel_frisk}. We only use the stops made in 2012.
The prediction task is, whether a pedestrian stopped on the suspicion of having a weapon actually possesses a weapon or not.
The non-sensitive features include proximity to a crime scene, age/build of a person, and so on.
Finally, as explained in Section~\ref{sec:eval}, since the original dataset (Table~\ref{table:sqf-stats-imba}) is highly skewed towards the positive class
we subsample the majority class (positive) to match the size of the minority (negative) class.




\begin{table}[h]
\centering
\caption{Persons found to be in possession of a weapon in 2012 NYPD SQF dataset (original).} \label{table:sqf-stats-imba}
\begin{tabular}{cccc}
\hline
Race  & Yes (-ve) & No (+ve) & Total \\ \hline
Black &  $ 2,113 (3\%) $ &  $ 77,337 (97\%) $  &  $ 79,450 $\\
White &  $ 803 (15\%) $ & $ 4,616 (85\%)$  &  $ 5,419 $\\ \hline
Total &  $ 2,916 (3\%)$ &  $  81,953 (97\%)$  &  $84,869$ \\ \hline
\end{tabular}
\end{table}


\begin{table}[!h]
\centering
\caption{Persons found to be in possession of a weapon in 2012 NYPD SQF dataset (class-balanced).} \label{table:sqf-stats}
\begin{tabular}{cccc}
\hline
Race  & Yes (-ve) & No (+ve) & Total \\ \hline
Black &  $ 2,113 (43\%) $ &  $ 2,756 (57\%) $  &  $ 4,869$\\
White &  $ 803 (83\%) $ & $ 160 (17\%) $  &  $963 $\\ \hline
Total &  $ 2,916 (50\%)$ &  $ 2,916 (50\%) $  &  $5,832$ \\ \hline
\end{tabular}
\end{table}








\makeatletter
\setlength{\@fptop}{0pt}
\makeatother

\end{appendix}



\end{document}


