\section{The Consistency-Checking Network}

We introduce the task of evidence-assisted veracity assessment of \textbf{\textcolor{myOrange}{image}}-\textbf{\textcolor{myblue}{caption}} pairing. As shown in~\autoref{fig:teaser}, we perform the \textbf{`multi-modal cycle-consistency check'} by comparing the \textbf{\textcolor{myblue}{textual evidence}} against the query \textbf{\textcolor{myblue}{caption}}, and the \textbf{\textcolor{myOrange}{visual evidence}} against the query \textbf{\textcolor{myOrange}{image}}.

\textbf{Challenges.} The task is significantly more complex than the merely one-to-one matching of the query against the evidence. First, many search results may be  unrelated to the query (neither falsify nor support) and act as noise. Second, comparing the query against the evidence requires further comprehension and reasoning. For pristine examples, the \textbf{\textcolor{myblue}{textual evidence}} might range from being paraphrases of the query \textbf{\textcolor{myblue}{caption}} to distantly related but supporting. For falsified examples, they might range from having different named entities to having the same ones but in a different context, such as the example in~\autoref{fig:teaser}. Similarly, comparing the \textbf{\textcolor{myOrange}{visual evidence}} against the query \textbf{\textcolor{myOrange}{image}} requires visual and scene understanding or regions comparison.
\begin{figure}[!t]
\centering
\includegraphics[width=0.8\linewidth]{figs/method_small.pdf}
\caption{Overview of our Consistency-Checking Network, \model{}.} 
\label{fig:method_small}
\vspace{-3mm}
\end{figure}

We propose a novel architecture, the Consistency-Checking Network (\model{}), to meet these challenges. We show an overview of the method in~\autoref{fig:method_small}. At the core of our approach is the memory networks architecture~\cite{sukhbaatar2015end,kumar2016ask,mohtarami2018automatic,chunseong2017attend}, which selectively compares the claim to the relevant items of the possibly large list of evidence. In addition, the attention mechanism allows inspecting which evidence items were most relevant to the decision. The model consists of a \textbf{\textcolor{myOrange}{visual reasoning}} component, a \textbf{\textcolor{myblue}{textual reasoning}} component, and a `CLIP' component. 
\begin{figure}[!b]
\vspace{-2mm}
\centering
\includegraphics[width=0.85\linewidth]{figs/vis_mem.pdf}
\caption{Visual evidence reasoning component.} 
\label{fig:vis_mem}
\end{figure}

\subsection{Visual Reasoning}
\autoref{fig:vis_mem} outlines the visual reasoning component that inspects the consistency between the query \textbf{\textcolor{myOrange}{image}} and the \textbf{\textcolor{myOrange}{visual evidence}}. First, we represent the images (query and evidence) using ResNet152~\cite{he2016deep}, pretrained on the ImageNet dataset. Each image is represented as: $I^q/I^e\in\mathbb{R}^{2048}$, where $q$ denotes the query representation and $e$ denotes evidence. 
Moreover, to reason over the overlap of regions and objects in the query image vs. evidence images, we used the label detection Google API~\cite{google2} to get a list of labels for each image. Then, for each evidence image, we compute the number of \textit{overlapping labels} between it and the query. We use this number as an additional feature, and we concatenate it with the evidence images' representations. 

The memory holds the evidence images. Each input to the memory is embedded into input and output memory representations~\cite{sukhbaatar2015end}, denoted by $a$ and $c$, respectively. The image memory vectors $m_i\in\mathbb{R}^{1024}$ are represented by:
\begin{equation}
m_i^a = \text{ReLU}(W_i^aI^e+b_i^a),
\end{equation}
\begin{equation}
m_i^c = \text{ReLU}(W_i^cI^e+b_i^c)
\end{equation}
The learned parameters are $W_i^a$ and $W_i^c$ $\in\mathbb{R}^{2048\times1024}$, and $b_i^a$ and $b_i^c$ $\in\mathbb{R}^{1024}$. The query image $I^q$ is also projected into a $1024$-dimension vector ($\hat{I^q}$) by another linear layer for modelling convenience. The matching between $\hat{I^q}$ and the memory vectors $m_i^a$ are then computed by: 
\begin{equation} \label{eqn:softmax}
{p_i}_j = \text{Softmax}(\hat{I^q}^T{m_i^a}_j),
\end{equation}
where $i$ denotes the image memory, $j$ is a counter for the memory items, and $p_i$ is a probability vector over the items. The output of the memory is the sum of the query and the average of the output representations $m_i^c$, weighted by $p_i$: 
\begin{equation} \label{eqn:output}
o_i = \sum_{j}{p_i}_j{m_i^c}_j + \hat{I^q}
\end{equation}
In addition, for some mismatched examples, there could be context discrepancies based on the place. To make the model aware of scenes and places similarity, we also represent the images using a ResNet50 trained on the Places365 dataset~\cite{zhou2017places}. We form a separate memory for the scene representations to allow more flexibility. Similar to the previous formulation, each image is represented as: $P^q/P^e\in\mathbb{R}^{2048}$, and the scenes memory vectors $m_p\in\mathbb{R}^{1024}$ are represented by:
\begin{equation}
m_p^{a/c} = \text{ReLU}(W_p^{a/c}P^e+b_p^{a/c})
\end{equation}
Similar to Eqn.~\ref{eqn:softmax} and Eqn.~\ref{eqn:output}, we get the output of the scenes (places) memory $o_p$.
\subsection{Textual Reasoning}
The second component of our model evaluates the consistency between the query \textbf{\textcolor{myblue}{caption}} and the \textbf{\textcolor{myblue}{textual evidence}}. As shown in~\autoref{fig:teaser}, we have two types of textual evidence: sentences (captions or pages' titles), and entities. As they have different granularities and might differ in importance, we form a separate memory for each. 

As shown in~\autoref{fig:text_mem}, we represent the query caption and each evidence item using a sentence embedding model. 
We experiment with state-of-the-art inference models that were trained on large corpuses such as Wikipedia and were shown to implicitly store world knowledge~\cite{petroni2019language,lee2020language,roberts2020much}, making them suitable for our task. We evaluate two methods in our experiments: 1) a pre-trained sentence transformer model~\cite{reimers-2019-sentence-bert} that is trained for sentence similarity, 2) using BERT~\cite{devlin2019bert} to get strong contextualized embeddings, in addition to an LSTM to encode the sequence. In the second method, we use the second-to-last BERT layer~\cite{zellers2019recognition} as the tokens' embeddings. We concatenate the last time-step LSTM output and the average of all time-steps' outputs.
\begin{figure}[!t]
\centering
\includegraphics[width=0.9\linewidth]{figs/text_mem.pdf}
\caption{Textual evidence reasoning component.} 
\label{fig:text_mem}
\vspace{-2mm}
\end{figure}

In addition, to help the model be entity-aware, we utilize a binary indicator feature to denote if there is a named entity overlap between the query caption and the evidence item. We used the spaCy NER~\cite{honnibal2017spacy} to extract the entities and concatenated the binary feature with the evidence (both captions and entities) representations. 

Using either of these previously mentioned methods, we get embeddings for the query caption $C^q$, the evidence entities $E$, and the evidence captions/sentences $S$. The entities input and output memory representations are given by: 
\begin{equation}
m_e^{a/c} = \text{ReLU}(W_e^{a/c}E+b_e^{a/c}),
\end{equation}
similarly, the captions/sentences input and output memory representations are given by: 
\begin{equation}
m_s^{a/c} = \text{ReLU}(W_s^{a/c}S+b_s^{a/c}),
\end{equation}
where $W_e^{a/c},W_s^{a/c}\in\mathbb{R}^{d\times d}$ and $b_e^{a/c},b_s^{a/c}\in\mathbb{R}^{d}$ are trainable weights, and $d$ is the dimension of the sentence embedding model (768 in the case of the pre-trained model, and 512 in the case of using BERT+LSTM).

As per Eqn.~\ref{eqn:softmax} and Eqn.~\ref{eqn:output}, we compute the output of the entities and sentences memories as $o_e$ and $o_s$, respectively. 
%\vspace{-3mm}

\textbf{Encoding the evidence's domain.} 
Features of websites, e.g., how frequently they appear and the types of news they feature, could help to prioritize evidence items. 
Thus, we learn an embedding of the evidence's domain names. We represent the domains as one-hot vectors and project them into a 20-dimensional space. We consider the domains that appeared at least three times, resulting in 17148 unique domains, the rest are set to $\textit{UNK}$. The domain embeddings are then concatenated with the evidence representations (both visual and textual, excluding entities).

\subsection{CLIP}
In addition to reasoning over evidence, we leverage CLIP~\cite{radford2021learning}, used in~\cite{luo2021newsclippings}, to integrate the query \textbf{\textcolor{myOrange}{image}}-\textbf{\textcolor{myblue}{text}} consistency into the decision. We first fine-tune CLIP ViT/B-32 on the task of classifying image-caption pairs into pristine or falsified, without considering the evidence. 

During fine-tuning, we pass the image and text through the CLIP encoders and normalize their embeddings. We produce a joint embedding that is a dot product of the image and text ones, and we add a linear classifier on top. The model is trained to classify the pair into pristine or falsified. Then, we freeze the fined-tuned CLIP and integrate the joint CLIP embeddings ($J_\text{clip}$) into the final classifier of \model{}.  

\subsection{Classifier}
Now that we individually evaluated the \textbf{\textcolor{myblue}{text}}-\textbf{\textcolor{myblue}{text}}, \textbf{\textcolor{myOrange}{image}}-\textbf{\textcolor{myOrange}{image}}, and \textbf{\textcolor{myOrange}{image}}-\textbf{\textcolor{myblue}{text}} consistency, we aggregate these observations in order to reach a unified decision. 

We found it helpful during training to apply a batch normalization layer~\cite{ioffe2015batch} to the output of each component. We then concatenate all previous components in one feature vector $o_t$ as follows:
\begin{equation} \label{eqn:bn}
    o_t = \text{BN}(o_i)\oplus\text{BN}(o_p)\oplus\text{BN}(o_e)\oplus\text{BN}(o_s)\oplus\text{BN}(J_\text{clip}), 
\end{equation}
where $\text{BN}$ denotes the batch normalization. $o_t$ is then fed to a simple classifier that has two fully connected layers with ReLU and batch normalization after the first one (dimension: 1024), and Sigmoid after the second one that outputs a final falsified probability ($p_f$). The model is trained, with freezing the backbone embedding networks, to binary classify the examples using the binary cross-entropy loss: 
\begin{equation}
L = -y_\text{true}\log(p_f) - (1-y_\text{true})\log(1-p_f)
\end{equation}
More implementation details can be found in Supp.~\ref{sec:implem}.