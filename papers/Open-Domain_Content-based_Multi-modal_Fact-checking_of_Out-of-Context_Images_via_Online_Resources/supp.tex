\setcounter{section}{0} 

\section*{Supplementary Material} In the supplementary material, we first discuss more implementation details in Section~\ref{sec:implem} and present additional experiments in Section~\ref{sec:add_exp}. Then we present some examples that were selected as `hard to verify' in the user study in Section~\ref{sec:user_study_ex}. We present other qualitative examples in Section~\ref{sec:qual_analysis2}. Finally, we discuss societal aspects and potential risks in Section~\ref{sec:risks}.


\section{Implementation Details} \label{sec:implem}
We elaborate on some implementation details of our framework. 
\begin{itemize}

\item \textbf{Sentence representation.}
We preprocessed the crawled captions to remove some artefacts (e.g., HTML tags). When using BERT+LSTM, we used the pre-trained `bert-base-uncased' model, whose dimension is 768. We set a maximum length of 150 tokens for the captions. Items (i.e., query captions, evidence captions, and entities) are padded to the maximum sequence length in this item's batch. When using the sentence transformer model, we used the `paraphrase-mpnet-base-v2' model\footnote{https://huggingface.co/sentence-transformers/paraphrase-mpnet-base-v2}. For both, we used the Hugging Face library\footnote{https://huggingface.co/}. We used the PyTorch framework\footnote{https://pytorch.org/} for all our experiments.

\item \textbf{Memory.}
The items in each memory (images, entities, and captions) are padded to the maximum number of evidence items in this memory's batch.

\item \textbf{CLIP.}
We used the pre-trained ViT-B/32 CLIP model\footnote{https://github.com/openai/CLIP}, where the text length is truncated at 77 tokens.

\item \textbf{Training details.}
When fine-tuning CLIP, we follow the implementation details in~\cite{luo2021newsclippings}, we used a learning rate of 5e-5 for
the linear classifier and 5e-7 for other layers of the CLIP model itself, in addition to using the Adam optimizer~\cite{kingma2014adam}. We used a batch size of 64 and trained the model for 100 epochs. For training \model{}, we used a batch size of 32, the Adam optimizer, and a cyclical learning rate~\cite{smith2017cyclical} with a maximum value of 6e-5. We trained the model for 30 epochs. We used a dropout~\cite{srivastava2014dropout} value of 0.05 to the input representations, 0.25 to domain embeddings, and 0.25 to the memory representations. Experiments were done on one NVIDIA A100 GPU. With precomputing the representations, the training takes roughly 5 hours. When training using BERT without precomputing, training takes roughly 30 hours.

\end{itemize}
\section{Additional Experiments} \label{sec:add_exp}

\paragraph{Evidence-only classification.} We examine whether claims (and consequently, the evidence) are having different characteristics (and thus, unwanted biases or naive give-aways) between pristine and falsified classes. The NewsCLIPpings dataset avoided linguistic biases in creating falsified examples by using real news \textbf{\textcolor{myblue}{captions}} mismatched with real news \textbf{\textcolor{myOrange}{images}}, instead of introducing manipulations in the captions. Also, to avoid text bias, each \textbf{\textcolor{myblue}{caption}} (and consequently, its \textbf{\textcolor{myOrange}{visual evidence}} in our dataset) appears twice (within the same split), once as pristine and once as falsified. Therefore, we hypothesize that the evidence websites for both classes are similar. To confirm, we ran an \textit{evidence-only} model, which achieved 53.4\% (\textit{basically chance level}), showing that \textit{reasoning against the query} is the distinguishing factor.

\begin{table}[!b]
\begin{center}
\vspace{-1mm}
\resizebox{0.65\linewidth}{!}{
\begin{tabular}{cccc}
\toprule
Conc. & Avg-pool & Max-pool & Multiply \\ \midrule
\textbf{83.9} &  82.46 & 82.48 & 77.1 \\ \bottomrule
\end{tabular}}
\end{center}
\vspace{-6mm}
\caption{Accuracy (\%) vs. aggregation strategies.}
\vspace{-3mm}
\label{rebuttal_tab:ablation1}
\end{table}

\paragraph{Additional ablation studies.} We include further experiments related to the fusion of the different components in our model (visual reasoning, textual reasoning, and CLIP). We tried a late fusion by having a separate classifier on top of each branch and aggregating the decision, however, this performed worse than the current intermediate fusion we employ. We also tried other strategies (\autoref{rebuttal_tab:ablation1}) to combine visual and textual memories before concatenating with CLIP, where we found that concatenation had the highest performance. 

Finally, we found that changing the dimension of the penultimate layer had a relatively small effect; e.g., increasing the dimension to 2048 increased the accuracy by 0.3 percentage points.

\section{User Study: `Hard to Verify' Examples} \label{sec:user_study_ex}
In Figure~\ref{tbl:qual_study_appendix}, we show some examples that were selected as `hard to verify' in the user study. This is possibly due to: 1) the \textbf{\textcolor{myblue}{captions}} could contain specific context information (e.g., locations such as \textit{`Denver'} or \textit{`Massachusetts'}) that is hard to verify with the \textbf{\textcolor{myOrange}{image}} alone, 2) the lack of \textbf{\textcolor{myblue}{textual}} evidence returned by the search \vcenteredinclude{figs/icon3.pdf}; the \textbf{\textcolor{myOrange}{images}} were not found by the inverse image search, so there are no \textbf{\textcolor{myblue}{captions/titles}} found. Moreover, the \textbf{\textcolor{myblue}{entities}} are generic descriptions of the \textbf{\textcolor{myOrange}{image}}, or not at all related (the first example). The performance of the model on these examples is possibly dependent on how similar the \textbf{\textcolor{myOrange}{visual}} evidence is to the query \textbf{\textcolor{myOrange}{image}} \vcenteredinclude{figs/icon4.pdf}. 

Another possible reason is having falsified pairs that are highly similar in context to the original ones (and, therefore, to the evidence as well). For instance, the last example shows a `hard to verify' falsified example (that was also misclassified by our model); the \textbf{\textcolor{myOrange}{image}} shows the same people mentioned in the \textbf{\textcolor{myblue}{caption}}, and thus, they also appeared in the \textbf{\textcolor{myOrange}{visual}} evidence. Additionally, the \textbf{\textcolor{myblue}{caption}} mentions the band name \textit{`One Direction'} that is also mentioned in the \textbf{\textcolor{myblue}{textual}} evidence, without strong contradictions. Meanwhile, the actual \textbf{\textcolor{myOrange}{image}} of this \textbf{\textcolor{myblue}{caption}} showed the band performing on a stage, however, this was not clearly emphasized by the \textbf{\textcolor{myblue}{caption}}; that is possibly why the \textbf{\textcolor{myOrange}{visual}} evidence is generic.

\section{Qualitative Examples} \label{sec:qual_analysis2}
In Figure~\ref{tbl:qual_appendix}, we show more qualitative examples. \model{} predicted many examples correctly despite not having a one-to-one matching with the evidence in the case of pristine examples and having close similarity to the evidence in the case of falsified examples. 

For instance, in the first three examples (pristine), we observed that the model highly attended to supporting evidence such as persons' and countries' names, topics, and events. Additionally, in the third example, we observed that the model prioritized the \textbf{\textcolor{myOrange}{image}} that is from the same scene and the evidence \textbf{\textcolor{myblue}{caption}} that contains a subset from the query \textbf{\textcolor{myblue}{caption}} (\textit{`soon to be a Trump International Hotel'}). 

The fourth and fifth examples (falsified) suggest that the model does not simply rely on having any similarity or overlap between the query and evidence in order to identify pristine examples. Despite having the same persons in the evidence, they were correctly predicted as falsified, possibly as they have contradicting location information and different scene details (e.g., lighting, stage setup, or colours), indicating a different context or event. The last falsified example also indicates that both \textbf{\textcolor{myblue}{textual}} and \textbf{\textcolor{myOrange}{visual}} evidence is helpful, as the evidence \textbf{\textcolor{myOrange}{images}} are clearly different from the falsified one (showing a different building and place). 

%As for the last example, it highlights one of the `hard to detect' examples in the dataset, even with the presence of evidence, as the falsified image is also showing a similar object \textit{`iPhone'} without a strongly different context. 
%\clearpage

\section{Limitations and Societal Aspects}  \label{sec:risks}
%Automating fact-checking can be beneficial to fight the spread of misinformation. 
Nowadays, with the spread and reliance on social media to digest and get updated with news, misinformation (e.g., on Twitter) can reach hundreds of millions of users~\cite{vo2020facts}. This crucially motivates the need to fact-check and verify the credibility of online content, especially during critical times such as a pandemic or political instabilities. On the other hand, manual fact-checking is usually time-consuming, needing from less than one hour to many days to verify a claim~\cite{thorne2018automated}. Therefore, automating fact-checking can be extremely beneficial to alleviate the burden upon fact-checkers and journalists. 

However, completely or overly relying on automated tools might give an unwanted sense of security and could have many dangerous consequences. These include the dangers of flagging many true examples as falsified due to the real-life class imbalance, and missing out challenging falsified examples that require more fine-grained and complex reasoning. In addition, a currently active and much-needed research direction in the textual domain shows that fact-verification models might be partially relying on dataset biases without in-depth understanding and reasoning~\cite{schuster2019towards}. They might also be brittle to complex claims that require multi-hop reasoning~\cite{hidey2020deseption}. Additionally, as facts are continuously evolving, we face the danger of relying on old retrieved evidence~\cite{schuster2021get} or even possibly outdated world knowledge that is implicitly stored in pre-trained language models during training~\cite{schuster2019towards}.

In addition to their inherent limitations in reasoning and interpretation, several works have shown that textual verifications models are also vulnerable to adversarial attacks~\cite{thorne2019evaluating}, such as inserting trigger words~\cite{atanasova2020generating}, introducing lexical variations~\cite{hidey2020deseption}, or paraphrasing\cite{thorne2019evaluating}. As we have a multi-modal task, our model might also be vulnerable to image-based adversarial attacks~\cite{goodfellow2014explaining}. %Beyond manipulating claims via adversarial attacks, adversaries can also poison the evidence and introduce items that lead to the required entailment. 
Another potential misuse scenario is using the fact-checking model as an adversarial filter in order to curate hard examples that might be misclassified by fact-checking models in general. 

As a conclusion, we believe that automating fact-checking is strongly beneficial and that there have been many encouraging advancements to improve and harden it in the textual domain and the multi-modal domain, as we propose. However, due to their limitations and vulnerabilities to active attacks and manipulation, they should be used to assist humans and speed up the process, while still keeping them in the loop to avoid such dangers and consequences. In this regard, in our framework, we show that the model can filter and select the most important evidence, which would enable quicker inspection of the evidence items. 


\input{examples}
\input{examples2}
