% CVPR 2022 Paper Template
% based on the CVPR template provided by Ming-Ming Cheng (https://github.com/MCG-NKU/CVPR_Template)
% modified and extended by Stefan Roth (stefan.roth@NOSPAMtu-darmstadt.de)

\documentclass[10pt,twocolumn,letterpaper]{article}

%%%%%%%%% PAPER TYPE  - PLEASE UPDATE FOR FINAL VERSION
\usepackage{cvpr}      % To produce the REVIEW version
%\usepackage{cvpr}              % To produce the CAMERA-READY version
%\usepackage[pagenumbers]{cvpr} % To force page numbers, e.g. for an arXiv version

% Include other packages here, before hyperref.
\makeatletter
\@namedef{ver@everyshi.sty}{}
\makeatother
\usepackage{tikz}

\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{booktabs}
\usepackage{mdframed}
\usepackage{xcolor,soul}
\usepackage{enumitem}
\usepackage{pifont}% 
\usepackage{makecell}
\usepackage{varwidth}
\usepackage{multirow}
\usepackage[super]{nth}
\pdfimageresolution=300

%\documentclass[twocolumn]{article}

%\usepackage{tcolorbox}



% It is strongly recommended to use hyperref, especially for the review version.
% hyperref with option pagebackref eases the reviewers' job.
% Please disable hyperref *only* if you encounter grave issues, e.g. with the
% file validation for the camera-ready version.
%
% If you comment hyperref and then uncomment it, you should delete
% ReviewTempalte.aux before re-running LaTeX.
% (Or just hit 'q' on the first LaTeX run, let it finish, and you
%  should be clear).
\usepackage[pagebackref,breaklinks,colorlinks,hypertexnames=false]{hyperref}


% Support for easy cross-referencing
\usepackage[capitalize]{cleveref}
\crefname{section}{Sec.}{Secs.}
\Crefname{section}{Section}{Sections}
\Crefname{table}{Table}{Tables}
\crefname{table}{Tab.}{Tabs.}



%%%%%%%%% PAPER ID  - PLEASE UPDATE
\def\cvprPaperID{7457} % *** Enter the CVPR Paper ID here
\def\confName{CVPR}
\def\confYear{2022}


\begin{document}
%\definecolor{myOrange}{rgb}{1.0, 0.49, 0.0}
\definecolor{myOrange}{rgb}{1.00, 0.60, 0}
%{1.0, 0.55, 0.0}
\definecolor{myblue}{rgb}{0.0,0.40,0.80}
\newcommand{\model}{\textit{CCN}}
\pdfoutput=1
%{0.216, 0.494, 0.722}
%{0.19, 0.55, 0.91}



\newcommand{\cmark}{\ding{51}}%
\newcommand{\xmark}{\ding{55}}%

\newcommand{\vcenteredinclude}[1]{\begingroup
\setbox0=\hbox{\includegraphics[scale=0.17]{#1}}%
\parbox{\wd0}{\box0}\endgroup}

\newcommand{\mycircle}[2][black,fill=black]{\tikz[baseline=-0.5ex]\draw[#1,radius=#2] (0,0) circle ;}%

\definecolor{applegreen}{rgb}{0.55, 0.71, 0.0}
\definecolor{harlequin}{rgb}{0.25, 1.0, 0.0}
\def\boxit#1{%
  \smash{\color{harlequin}\fboxrule=1pt\relax\fboxsep=4pt\relax%
  \llap{\rlap{\fbox{\vphantom{0}\makebox[#1]{}}}~}}\ignorespaces
}

%%%%%%%%% TITLE - PLEASE UPDATE
\title{Open-Domain, Content-based, Multi-modal Fact-checking of\\Out-of-Context Images via Online Resources}

\author{Sahar Abdelnabi, Rakibul Hasan, and Mario Fritz\\
CISPA Helmholtz Center for Information Security\\
{\tt\small \{sahar.abdelnabi,rakibul.hasan,fritz\}@cispa.de}}
% For a paper whose authors are all at the same institution,
% omit the following lines up until the closing ``}''.
% Additional authors and addresses can be added with ``\and'',
% just like the second author.
% To save space, use either the email address or home page, not both
%\and
%Second Author\\
%Institution2\\
%First line of institution2 address\\
%{\tt\small secondauthor@i2.org}
%}
%\maketitle
\twocolumn[{%
\renewcommand\twocolumn[1][]{#1}%
\maketitle
\begin{center}
    \centering
    \captionsetup{type=figure}
    \includegraphics[width=0.90\textwidth]{figs/teaser.pdf}
    \captionof{figure}{To evaluate the veracity of \textbf{\textcolor{myOrange}{image}}-\textbf{\textcolor{myblue}{caption}} pairings, we leverage \textbf{\textcolor{myOrange}{visual}} and \textbf{\textcolor{myblue}{textual}} evidence gathered by querying the Web. We propose a novel framework to detect the consistency of the claim-evidence (\textbf{\textcolor{myblue}{text}}-\textbf{\textcolor{myblue}{text}} and \textbf{\textcolor{myOrange}{image}}-\textbf{\textcolor{myOrange}{image}}), in addition to the \textbf{\textcolor{myOrange}{image}}-\textbf{\textcolor{myblue}{caption}} pairing. Highlighted evidence represents the model's highest attention, showing a difference in location compared to the query \textbf{\textcolor{myblue}{caption}}.} \label{fig:teaser}
\end{center}%
}]


%%%%%%%%% ABSTRACT

\begin{abstract}
Misinformation is now a major problem due to its potential high risks to our core democratic and societal values and orders. \textbf{Out-of-context} misinformation is one of the easiest and effective ways used by adversaries to spread viral false stories. In this threat, a real image is \textbf{re-purposed} to support other narratives by misrepresenting its context and/or elements. 
The internet is being used as the go-to way to verify information using different sources and modalities. Our goal is an inspectable method that automates this time-consuming and reasoning-intensive process by fact-checking the \textbf{\textcolor{myOrange}{image}}-\textbf{\textcolor{myblue}{caption}} pairing using Web evidence. 
To integrate evidence and cues from both modalities, we introduce the concept of \textbf{`multi-modal cycle-consistency check'} \vcenteredinclude{figs/icon3.pdf} / \vcenteredinclude{figs/icon4.pdf}; starting from the \textbf{\textcolor{myOrange}{image}}/\textbf{\textcolor{myblue}{caption}}, we gather \textbf{\textcolor{myblue}{textual}}/\textbf{\textcolor{myOrange}{visual}} evidence, which will be compared against the other paired \textbf{\textcolor{myblue}{caption}}/\textbf{\textcolor{myOrange}{image}}, respectively. 
%
Moreover, we propose a novel architecture, \textbf{Consistency-Checking Network (CCN)}, that mimics the layered human reasoning across the same and different modalities: the \textbf{\textcolor{myblue}{caption}} vs. \textbf{\textcolor{myblue}{textual evidence}}, the \textbf{\textcolor{myOrange}{image}} vs. \textbf{\textcolor{myOrange}{visual evidence}}, and the \textbf{\textcolor{myOrange}{image}} vs. \textbf{\textcolor{myblue}{caption}}. Our work offers the first step and benchmark for \textbf{open-domain, content-based, multi-modal fact-checking}, and significantly outperforms previous baselines that did not leverage external evidence\footnote{For code, checkpoints, and dataset, check: \url{https://s-abdelnabi.github.io/OoC-multi-modal-fc/}}.


\end{abstract}

%%%%%%%%% BODY TEXT
\vspace{-5mm}
\input{intro}
\input{related_work}
\input{dataset}
\input{task}
\input{method}
\input{results}
\input{conclusion}


%-------------------------------------------------------------------------



%%%%%%%%% REFERENCES
{\small
\bibliographystyle{ieee_fullname}
\bibliography{egbib}
}
\clearpage
\input{supp}

\end{document}
