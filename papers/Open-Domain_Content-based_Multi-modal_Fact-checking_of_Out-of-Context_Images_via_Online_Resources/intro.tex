\section{Introduction}

Recently, there has been a growing and widespread concern about `fake news' and its harmful societal, personal, and political consequences~\cite{agarwal2019protecting,huh2018fighting,lazer2018science}, including people's own health during the pandemic\cite{nightingale2020examining,press_bbc_covid,press_bbc_covid2}. Misusing generative AI technologies  
to create deepfakes~\cite{goodfellow2014generative,karras2020analyzing,radford2019language} further fuelled these concerns~\cite{press1,press2}. 
However, \textbf{\textit{image-repurposing}}--- where a real image is misrepresented and used \textit{out-of-context} with another false or unrelated narrative to create more credible stories and mislead the audience---is still one of the easiest and most effective ways to create realistically-looking misinformation. Image-repurposing does not require profound technical knowledge or experience~\cite{luo2021newsclippings,aneja2021catching}, which potentially amplifies its risks. Images usually accompany real news~\cite{tan2020detecting}; thus, adversaries may augment their stories with images as `supporting evidence' to capture readers' attention~\cite{luo2021newsclippings,hameleers2020picture,wang2021understanding}.

\textbf{Image re-purposing datasets and threats.} Gathering large-scale labelled out-of-context datasets is hard due to the scarcity and substantial manual efforts. Thus, previous work attempted to construct synthetic out-of-context datasets~\cite{jaiswal2017multimedia,sabir2018deep}.  
A recent work~\cite{luo2021newsclippings} proposed to automatically, yet non-trivially, match images accompanying real news with other real news captions. The authors used trained language and vision models to retrieve a close and convincing image given a caption. While this work contributes to misinformation detection research by automatically creating datasets, it also highlights the threat that \textbf{\textit{machine-assisted}} procedures may ease creating misinformation at scale. Furthermore, the authors reported that both defense models and humans struggled to detect the out-of-context images. 
In this paper, we use this dataset as a challenging benchmark; we leverage external evidence to push forward the automatic detection. 

\textbf{Fact-checking.} To fight misinformation, huge fact-checking efforts are done by different organizations~\cite{fc1,fc2}. However, they require substantial manual efforts~\cite{press_vice}. Researchers have proposed several automated methods and benchmarks to automate fact-checking and verification~\cite{thorne2018fever,popat2018declare}. However, most of these works focus on textual claims. Fact-checking multi-modal claims has been under-explored.
%\vspace{-3mm}

\textbf{Our approach.} People frequently use the Internet to verify information. We aggregate evidence from images, articles, different sources, and we measure their consensus and consistency. Our goal is to design an inspectable framework that automates this multi-modal fact-checking process and assists users, fact-checkers, and content moderators. 

More specifically, we propose to gather and reason over evidence to judge the veracity of the \textbf{\textcolor{myOrange}{image}}-\textbf{\textcolor{myblue}{caption}} pair. First \vcenteredinclude{figs/icon3.pdf}, we use the \textbf{\textcolor{myOrange}{image}} to find its other occurrences on the internet, from which, we crawl \textbf{\textcolor{myblue}{textual evidence}} (e.g., captions), which we compare against the paired \textbf{\textcolor{myblue}{caption}}. Similarly \vcenteredinclude{figs/icon4.pdf}, we use the \textbf{\textcolor{myblue}{caption}} to find other images as \textbf{\textcolor{myOrange}{visual evidence}} to compare against the paired \textbf{\textcolor{myOrange}{image}}. We call this process: \textbf{`multi-modal cycle-consistency check'}. Importantly, we retrieve evidence in a fully automated and flexible \textbf{open-domain} manner~\cite{chen2017reading}; no `golden evidence' is pre-identified or curated 
and given to the model. 

To evaluate the claim's veracity, we propose a novel architecture, the \textbf{Consistency-Checking Network (\model{})}, that consists of 1)~memory networks components to evaluate the consistency of the claim against the evidence (described above), 2)~a CLIP~\cite{radford2021learning} component to evaluate the consistency of the \textbf{\textcolor{myOrange}{image}} and \textbf{\textcolor{myblue}{caption}} pair themselves. As the task requires machine comprehension and visual understanding, we perform different evaluations to design the memory components and the evidence representations. Moreover, we conduct two user studies to 1)~measure the human performance on the detection task and, 2)~understand if the collected evidence and the model's attention over the evidence help people distinguish true from falsified pairs.
~\autoref{fig:teaser} depicts our framework, showing a falsified example from the dataset along with the retrieved evidence.


\textbf{Contributions.} We summarize our contributions as follows: 1) we formalize a new task of multi-modal fact-checking. 2) We propose the \textbf{`multi-modal cycle-consistency check'} to gather evidence about the multi-modal claim from both modalities. 3) We propose a new inspectable framework, \textbf{\model{}}, to mimic the aggregation of observations from the claims and world knowledge. 4) We perform numerous evaluations, ablations, and user studies and show that our evidence-augmented method significantly improves the detection over baselines. 
