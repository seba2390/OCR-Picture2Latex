\section{Experimental Results}
In this section, we show the quantitative analysis of different variants of the model and baselines. We then present our user studies, qualitative analysis, and discussion. 

\newcommand{\hlc}[2][yellow]{{\sethlcolor{#1}\hl{#2}}}
\definecolor{light_red}{rgb}{0.96, 0.76, 0.76}
\definecolor{light_green}{rgb}{0.56, 0.93, 0.56}
\definecolor{light_yellow}{rgb}{0.99, 0.97, 0.37}

\newcommand*\rot{\rotatebox{90}}
	
\begin{table} [!b]
%\vspace{-1mm}
\centering
\resizebox{0.88\linewidth}{!}{%
\begin{tabular}{c c c c c c | c c c |c c c ||c}
\toprule
\# & \rot{Evidence type} & \rot{Separate mem.} & \rot{BN} & \rot{Dataset filter} & \rot{CLIP} & \rot{ResNet (ImageNet)} & \rot{ResNet (Scenes)} & \rot{Labels} & \rot{Sent. transformer} & \rot{BERT+LSTM}  &  \rot{NER} & \textbf{\rot{Accuracy}} \\ \hline
1 & all & \cmark & \xmark & \xmark & \xmark & \cmark & \xmark & \xmark &  \cmark & \xmark & \xmark & 73.5\% \\  [0.1cm]

2 & \textbf{\underline{\hlc[light_yellow]{all w/o Images}}} & \cmark  & \xmark & \xmark & \xmark  & - & - & - &  \cmark & \xmark & \xmark & 62.5\% \\    [0.1cm]

3 & \textbf{\underline{\hlc[light_yellow]{all w/o Captions}}} & \cmark & \xmark & \xmark & \xmark  & \cmark  & \xmark & \xmark &  \cmark & \xmark & \xmark & 57.4\% \\   [0.1cm]
4 & \textbf{\underline{\hlc[light_yellow]{all w/o Entities}}} & \cmark & \xmark & \xmark & \xmark  & \cmark  & \xmark & \xmark &  \cmark & \xmark & \xmark & 71.8\% \\  [0.1cm]

5 & all & \cmark & \textbf{\underline{\hlc[light_yellow]{\cmark}}} & \xmark & \xmark & \cmark  & \xmark & \xmark &  \cmark & \xmark & \xmark & 84.2\% \\  [0.1cm]
6 & all & \textbf{\underline{\hlc[light_yellow]{\xmark}}} & \cmark & \xmark & \xmark  & \cmark  & \xmark & \xmark &  \cmark & \xmark & \xmark & 81.7\% \\ [0.1cm]

7 & all & \cmark & \cmark & \textbf{\underline{\hlc[light_yellow]{\cmark}}} & \xmark  & \cmark  & \xmark & \xmark &  \cmark & \xmark & \xmark & 80.3\% \\  [0.1cm]

8 & all & \cmark &  \cmark & \cmark & \xmark  & \cmark  & \xmark & \xmark &  \cmark & \xmark & \textbf{\underline{\hlc[light_yellow]{\cmark}}} & 81.2\% \\  [0.1cm]

9 & all & \cmark &  \cmark & \cmark & \textbf{\underline{\hlc[light_yellow]{\cmark}}}  & \cmark  & \xmark & \xmark &  \cmark & \xmark & \cmark & 82.6\% \\  [0.1cm]


10 & all & \cmark &  \cmark & \cmark & \cmark  & \cmark  & \textbf{\underline{\hlc[light_yellow]{\cmark}}} & \xmark &  \cmark & \xmark & \cmark & 83.4\% \\  [0.1cm]

11 & all & \cmark &  \cmark & \cmark & \cmark   & \cmark & \cmark & \textbf{\underline{\hlc[light_yellow]{\cmark}}} &  \cmark & \xmark & \cmark & 83.9\% \\  [0.1cm]

12 & \boxit{3.8in} all & \cmark &  \cmark & \cmark & \cmark  &  \cmark & \cmark & \cmark &  \xmark & \textbf{\underline{\hlc[light_yellow]{\cmark}}} & \cmark & \textbf{84.7\%} \\  [0.1cm]

13 & \textbf{\underline{\hlc[light_yellow]{all w/o domains}}} &\cmark &  \cmark & \cmark & \cmark  &  \cmark & \cmark & \cmark &  \xmark & \cmark & \cmark & 83.9\% \\  [0.1cm]
\bottomrule
\end{tabular}}
\caption{Classification performance on the test set for different variants of the model. Highlighted cells represent the changed factor in that experiment. The green box represents the best model.} \label{table:ablation}
\end{table}
\subsection{Quantitive Analysis}
We evaluated our model and other variants of it in order to understand the effect of each component.~\autoref{table:ablation} shows our experiments. We summarize different aspects and highlight the most interesting observations in what follows.

\textbf{Evidence types.} We first show the effect of each evidence type in the first four rows. Removing the evidence \textbf{\textcolor{myOrange}{images}} or the evidence \textbf{\textcolor{myblue}{captions}} dropped the performance significantly; these results indicate the importance of integrating both modalities for verification. Removing the \textbf{\textcolor{myblue}{Entities}} had less effect. This might be due to having some redundant information with the evidence captions already, or because of sometimes having generic named entities that are not helpful to verify the caption claim. 

\textbf{Memory design.} Adding a batch normalization layer after each component, as in Eqn.~\ref{eqn:bn}, improved the training and increased the accuracy by nearly 11 percentage points. Another variant we studied had a unified memory containing images, captions, and entities. The query here was a concatenation of the image and caption pairs. As shown in row 6, this was less successful than the separate memory setup, suggesting that the explicit \textbf{\textcolor{myblue}{text}}-\textbf{\textcolor{myblue}{text}} and \textbf{\textcolor{myOrange}{image}}-\textbf{\textcolor{myOrange}{image}} consistency comparison aids the learning. 

\textbf{Evidence filtering.} As the dataset is constructed from real news articles, the Google search may return the exact news as the query search (i.e., exact news with the exact webpage). While this is needed in a real fact-checking setup, it might bias the training; the model might use it/or its absence as a shortcut to predict pristine/falsified pairs, respectively, without stronger reasoning. Therefore, we filtered the evidence as follows: for pristine examples, we discard an evidence item if it \textit{matches} the query and comes from the \textit{same website} as the query. To detect matching, we use perceptual hashing for \textbf{\textcolor{myOrange}{images}}. For \textbf{\textcolor{myblue}{captions}}, we remove punctuations and lower-case all the sentences and then check if they are an exact match. We then trained and evaluated with this filtered dataset. As shown in row 7, this did not significantly reduce the accuracy, suggesting that the model reasons about consistency beyond exact matches.

\textbf{Other improvements.} We show that our other enhancements, including adding CLIP and improving visual and textual representations, recovered the performance drop due to the evidence filtering. CLIP had relatively the largest effect, with around a 1.5 percentage points increase. Training the LSTM with BERT embeddings performed better than using a pre-trained sentence transformer model. This might be because it allowed the model to learn on the token level and focus on the consistency in, e.g., named entities, location, etc., which are more specific cues in our use-case than general sentence entailment tasks. Finally, the last row shows that including the evidence's domain 
helps to some extent, as it might help the model to attend to and prioritize evidence items. Additional experiments are in Supp.~\ref{sec:add_exp}.

\textbf{Baselines.}
We compare our evidence-assisted detection against the CLIP-only baseline used in~\cite{luo2021newsclippings} in~\autoref{table:baseline}. We fine-tuned CLIP~\cite{radford2021learning}, reaching a higher accuracy than originally reported in~\cite{luo2021newsclippings} on this dataset subset. As the dataset pairing is not trivial, this baseline achieved a relatively low performance. 
In contrast, we achieve a significant improvement of a nearly 19 percentage points increase, indicating that leveraging evidence is important to solve the task. 
\begin{table} [!t]
\centering
\resizebox{0.75\linewidth}{!}{%
\begin{tabular}{c| c c| c c c} 
\toprule
\textbf{Method} & \textbf{Evidence} & \textbf{Pair} & \textbf{All} & \textbf{Falsified} & \textbf{Pristine}  \\ \midrule
CLIP & \xmark & \cmark & 66.1\% & 68.1\% & 64.2\% \\
Averaged & \cmark & \xmark & 70.6\% & 72.4\% & 68.9\% \\ 
\textbf{\model{}} & \cmark & \cmark & \textbf{84.7\%} & \textbf{84.8\%} & \textbf{84.5\%} \\ 
\bottomrule
\end{tabular}}
\caption{Classification performance on the test set for our model in comparison with baselines.
} \label{table:baseline}
\vspace{-2mm}
\end{table}

As there are no previous baselines for evidence-assisted out-of-context detection, we design a baseline that uses evidence. We use the pretrained image and text representations of ResNet-152 and sentence transformer in the same setup of \textbf{\textcolor{myblue}{text}}-\textbf{\textcolor{myblue}{text}} and \textbf{\textcolor{myOrange}{image}}-\textbf{\textcolor{myOrange}{image}} similarity. We compute the matching between the query and the evidence via dot product. Then, we use an average pooling layer across all evidence items, which will be used for classification. As shown in~\autoref{table:baseline}, this baseline outperforms the CLIP-only. However, our proposed model with the other improvements achieves a $\sim$14 percentage points increase.
\subsection{User Studies}
%\textbf{Motivation.} 
We conducted user studies to estimate the human performance on the dataset and evaluate the usefulness of the evidence in detection, as well as the relevancy of the evidence items that the model highly attends to.
\vspace{-2mm}
\subsubsection{Study 1: Human Performance Baseline}
We aim to establish a human baseline as an upper bound estimate of the out-of-context images detection accuracy. Due to the automatic open-world evidence retrieval, we do not have a labelled dataset to indicate if an evidence item is relevant to the claim. Furthermore, some examples might not have any relevant evidence retrieved. Also, the falsified examples could be very close to the original context, making them hard to verify even with the presence of evidence.

\textbf{Setup.} We randomly selected 100 examples (48 pristine, 52 falsified) from the test dataset. Along with the \textbf{\textcolor{myOrange}{image}}-\textbf{\textcolor{myblue}{caption}} pairs, we presented the gathered evidence (\textbf{\textcolor{myOrange}{images}}, \textbf{\textcolor{myblue}{captions}}, and \textbf{\textcolor{myblue}{entities}}). For each pair, first, we asked users if the \textbf{\textcolor{myblue}{caption}} matches the \textbf{\textcolor{myOrange}{image}}, considering any of: inconsistency cues between them, the evidence presented, or their prior knowledge about the subject. Then, they answered which source(s) of information helped them label the pair, or indicated `None' if it was hard to verify. 
We instructed them \textit{not to} search for other evidence, so that both our model and humans have access to the same evidence, and to evaluate the usefulness of the evidence gathered by our framework. We recruited 8 experienced native English-speaking crowd workers through Amazon Mechanical Turk.  

\textbf{Results.}
\autoref{table:user_study} shows the average performance across all workers and the results of the best worker.   
Compared to the findings reported in~\cite{luo2021newsclippings}, human performance significantly increased when presented with evidence (average detection was 65.6\%, with only 35\% falsified detection rate). Additionally, \model{} achieved 80\% accuracy on these 100 examples, which is lower than the best worker but on a par with the average worker. 

\begin{table} [!t]

\centering
\resizebox{0.8\linewidth}{!}{%
\begin{tabular}{c|cccc} \toprule
& \textbf{Study} & \textbf{All} & \textbf{Falsified} & \textbf{Pristine}   \\ \midrule
\multirow{3}{*}{\textbf{Average}} & \nth{1} & 81.0\%$\pm$4.71 & 79.5\%$\pm$8.31 & 82.3\%$\pm$9.31 \\ \cline{2-5}
 & \nth{2}, Highest & 86.2\%$\pm$4.9 & 84.5\%$\pm$9.3 & 88.0\%$\pm$7.2 \\ 
 & \nth{2}, Lowest  & 77.7\%$\pm$6.0 &  76.0\%$\pm$9.0 &  79.5\%$\pm$7.5 \\
 \midrule 
\multirow{3}{*}{\textbf{Best worker}} & \nth{1} & 89.0\%  & 92.0\% & 93.7\% \\ \cline{2-5}
&\nth{2}, Highest & 94.0\%  & 98.0\% & 98.0\% \\ 
& \nth{2}, Lowest & 88.0\%  & 90.0\% & 86.0\% \\ \bottomrule
\end{tabular}}
\caption{The results of our two user studies. The first is to label 100 examples, selected randomly. 
The second is to label another 100 examples (that have enough evidence retrieved) 
using 1) the highest-attention, and 2) the lowest-attention evidence.
} \label{table:user_study}
\vspace{-2mm}
\end{table}
\begin{figure}[!b]
    \centering
    \includegraphics[width=0.71\linewidth]{figs/study_helpful_cues.pdf}
    \caption{ 
    Workers indicated the factors that helped their decision. %(prior knowledge, the image-caption pair, different types of evidence, or none). 
    `Any evid.' means that any evidence type was helpful. `Evid. only' means that only the evidence was helpful.}
    \label{fig:study}

\end{figure}
~\autoref{fig:study} shows which information helped workers to label the \textbf{\textcolor{myOrange}{image}}-\textbf{\textcolor{myblue}{caption}} pairs during the study. We highlight the following observations: 1) In 77.2\% of the examples, on average, the evidence contributed to the workers' decision, in comparison with 59.3\% only for the \textbf{\textcolor{myOrange}{image}}-\textbf{\textcolor{myblue}{caption}} pair. In 28.3\%, the evidence was the only helpful cue. 2) Among the evidence types, the \textbf{\textcolor{myOrange}{images}} were the most helpful (64\%), possibly because it is easier to grasp different images at a glance. 3) 12.3\% of the examples were hard to verify. When checking some of them (Supp.~\ref{sec:user_study_ex}), we observed that they do not have obvious cues (e.g., generic scenes with event-specific captions, an image for the same person with a similar context). Also, they sometimes had poor retrieval (the inverse search did not find the \textbf{\textcolor{myOrange}{image}}, so there are no evidence \textbf{\textcolor{myblue}{captions}}, and the evidence \textbf{\textcolor{myOrange}{images}} are unrelated or not conclusive). Our model struggled in detecting these examples as well. Augmenting with looser retrieval (e.g., searching with keywords of the caption, finding captions of other similar images) might help in these cases.
\vspace{-2mm}
\subsubsection{Study 2: Evaluating the Attention}
One of our main goals is to have an automated fact-checking tool while also allowing humans to be in the loop, if needed. We hypothesize that the attention weights given by the model can be used to retrieve the most relevant and useful evidence, which enables a quick inspection.

We design a second study to evaluate this hypothesis. We randomly selected 100 examples (50 each) that at least have 8 evidence items in each type\footnote{In this first study, some examples might not have enough evidence. However, we keep them to have a representative set of the dataset.}. We designed two variants using the same 100 pairs; in the first, we display the highest-attention 4 items from each evidence type, in the second, we display the lowest-attention 4 ones. The two variants are labelled by non-overlapping groups (8 workers each). We follow the rest of the first study's setup and instructions.

\textbf{Results.}~\autoref{table:user_study} and~\autoref{fig:study} show that the highest-attention evidence had higher performance and generally better ratings as `helpful' compared to the lowest-attention evidence. These findings suggest that the model learned to prioritize the most relevant items, as intended, and can potentially be beneficial for 1) inspectability and, 2) assistive fact-checking; as workers had a higher performance with only a subset of evidence. 

\input{qual}
\subsection{Qualitative Analysis}
We show some successful predictions of our model in Figure~\ref{tbl:qual}. When inspecting the attention in the case of \textit{pristine} examples, we found that the highest attention is on items that are most relevant to the query (e.g., a similar \textbf{\textcolor{myOrange}{image}} in the first example, named \textbf{\textcolor{myblue}{entities}} that are present in or similar to the query \textbf{\textcolor{myblue}{caption}} such as cities' names, and semantically similar \textbf{\textcolor{myblue}{captions}}). The model also predicted the second example correctly, despite not having an \textbf{\textcolor{myOrange}{image}} of the same scene. For \textit{falsified} examples, we observe that the third one is predicted correctly despite having a similar falsified topic (\textit{`Affordable health care'} and \textit{`Lawsuits'}). Moreover, the fourth one shows the highest attention on contradicting locations in \textbf{\textcolor{myblue}{entities}}, and on the most syntactically similar \textbf{\textcolor{myblue}{caption}}. This was predicted correctly, despite having similar-style evidence to the query. Similarly, the falsified example in~\autoref{fig:teaser} was similar in the persons' names and images (\textit{`David Cameron'}), but different in context and scene details. Finally, the last example shows a \textit{pristine} example that was misclassified as \textit{falsified}. When inspecting the \textbf{\textcolor{myblue}{textual evidence}}, we observed that although it is revolving around the same topic, there is little connection to the context of the query \textbf{\textcolor{myblue}{caption}}, in addition to having a diverse set of \textbf{\textcolor{myOrange}{visual evidence}} that is not similar to the query \textbf{\textcolor{myOrange}{image}}. Other examples are in Supp.~\ref{sec:qual_analysis2}.

\subsection{Discussion and Limitations}
We propose a multi-modal fact-checking framework  
that significantly outperforms baselines and is comparable to human performance. However, the task has yet many challenges, and fully relying on automated tools might have dangerous consequences. 
Therefore, humans should still be in the loop. Thus, we offer an inspectable and assistive tool that helps to reduce the load of the otherwise fully manual process~\cite{press_vice}. We further discuss potential risks in Supp.~\ref{sec:risks}.

Moreover, our approach relies on the retrieval results of the search engine. However, as we show in our analysis (Tables~\ref{table:ablation} and~\ref{table:baseline}), naively considering the evidence is not adequate, and a careful design of the model is needed to meet the challenges of the task, including the noisy open-domain setup with no relevancy supervision, and the high resemblance of evidence across pristine and falsified examples.

Finally, in some situations, some evidence items might contradict others, e.g., due to the websites' opposing political orientations, or misinformation on the Web. We did not observe such scenarios with the used dataset; identifying and studying them might require poisoning the search results, or carefully curating claims that lead to contradicting results, which is beyond the scope of this work.