\section{Related Work}
\textbf{Multi-modal Misinformation.}
Previous work has studied multi-modal misinformation~\cite{wang2018eann,khattar2019mvae,nakamura2020fakeddit}. For instance, Khattar et al.~\cite{khattar2019mvae} studied multi-modal fake news on Twitter by learning representations of images and captions which were used in classification. The images in the dataset could be edited. 
In contrast, we focus on out-of-context real news images and verifying them using evidence. 

Moreover, Zlatkova et al.~\cite{zlatkova2019fact} studied the factuality of the image-claim pairs using information from the Web. They collected features about the claim image, such as its URL. The actual content of the claim image is not considered against evidence. Our work is different in how we collect both \textbf{\textcolor{myOrange}{visual}} and \textbf{\textcolor{myblue}{textual}} evidence to perform the \textbf{cycle-consistency} check. In addition, they only calculate features from the claim text such as TF-IDF, while we use memory networks with learned representations. 

Related to the out-of-context threat, Aneja et al.~\cite{aneja2021catching} constructed a large, yet unlabelled, dataset of different contexts of the same image. They propose a self-supervised approach to detect whether two captions (given an image) are having the same context. However, unlike our work, they do not judge the veracity of a single image-caption claim. Also, the unlabelled dataset collected in this work does not allow the veracity detection training and evaluation. 

In order to produce labelled out-of-context images, previous work created synthetic datasets by changing the captions, either by naive swapping or named entities manipulations~\cite{jaiswal2017multimedia,sabir2018deep}, however, the falsified examples were either too naive 
or contained linguistic biases that are easy to detect even by language-only models~\cite{luo2021newsclippings}. 

Therefore, Luo et al.~\cite{luo2021newsclippings} proposed to create falsified examples by matching real images with real captions~\cite{liu2020visualnews}. They created the large-scale NewsCLIPpings dataset that contains both \textit{pristine} and convincing \textit{falsified} examples. The matching was done automatically using trained language and vision models (such as SBERT-WK~\cite{wang2020sbert}, CLIP~\cite{radford2021learning}, or scene embeddings~\cite{zhou2017places}). The falsified examples could misrepresent the context, the place, or people in the image, with inconsistent entities or semantic context. The authors show that both machine and human detection are limited, indicating that the task is indeed challenging. 
Thus, to improve the detection, we propose to use external Web evidence to verify the \textbf{\textcolor{myOrange}{image}}-\textbf{\textcolor{myblue}{caption}} claim.

\textbf{Open-domain QA and Fact-verification.}
Our work is similar to textual work in open-domain QA~\cite{chen2017reading} and fact-verification~\cite{thorne2018fever} (from Wikipedia) in having a large-scale and open-domain task that involves automatic retrieval and comprehension. We do not assume that the input to the model is already labelled and identified as relevant, simulating real-life fact-checking. Moreover, we do not restrict the evidence to be from a specific curated source only, such as fact-checking websites, in contrast to~\cite{vo2020facts}. 

Similar to our work, Popat et al.~\cite{popat2018declare} built a credibility assessment end-to-end method of textual claims using external evidence. However, to the best of our knowledge, no previous work attempted to verify multi-modal claims using both modalities. Also, their model is designed to predict the per-source credibility of claims, while we learn the aggregated consistency from multiple sources. 

