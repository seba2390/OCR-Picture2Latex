\section{Dataset and Evidence Collection} \label{sec:dataset}
\textbf{Dataset.} We use the NewsCLIPpings~\cite{luo2021newsclippings} that contains both pristine and falsified (`out-of-context') images. It is built on the VisualNews~\cite{liu2020visualnews} corpus that contains news pieces from 4 news outlets: The Guardian, BBC, USA Today, and The Washington Post. The NewsCLIPpings dataset contains different subsets depending on the method used to match the images with captions (e.g., text-text similarity, image-image similarity, etc.). We use the `balanced' subset that has representatives of all matching methods and consists of 71,072 train, 7,024 validation, and 7,264 test examples. 
To kick-start our \textit{evidence-assisted detection}, we use the \textbf{\textcolor{myOrange}{image}}-\textbf{\textcolor{myblue}{caption}} pairs as queries to perform Web search, as depicted in~\autoref{fig:teaser}. 
%\vspace{-3mm}

\textbf{\textcolor{myblue}{Textual evidence.}} We use the query \textbf{\textcolor{myOrange}{image}} in an inverse search mode using Google Vision APIs~\cite{google3} to retrieve \textbf{\textcolor{myblue}{textual evidence}} \vcenteredinclude{figs/icon3.pdf}. The API returns a list of \textbf{entities} that are associated with that image, which we collect as part of the textual evidence. They might describe the content of the image and, further, the contexts of where these images appeared, such as the entities' list in~\autoref{fig:teaser}.

In addition, the API returns the images' URLs and the containing pages' URLs. In contrast to previous work~\cite{zlatkova2019fact} that only considered the containing pages' titles, we also collect the images' captions. We designed a Web crawler that visits the page, searches for the image's tag using its URL or by image content matching (using perceptual hashing), then retrieves the \textbf{captions} if found. We scrape the \textless$\textit{figcaption}$\textgreater\ tag, as well as the \textless\textit{img}\textgreater\ tag's textual attributes such as \textit{alt}, \textit{image-alt}, \textit{caption}, \textit{data-caption}, and \textit{title}. In addition, we observed the returned pages for a few hundreds of the API calls and implemented other strategies to scrape the captions based on them. We also save the \textbf{titles} of the pages. From each page, we collect all the non-redundant text snippets that we found. The API returns up to 20 search results. We discard a page if the detected language of the title is not English, using the fastText library~\cite{fasttext} for language identification. We collect the \textbf{domains} of each evidence item as metadata.
%

\textbf{\textcolor{myOrange}{Visual evidence.}} Second, we use the \textbf{\textcolor{myblue}{caption}} as textual queries to search for \textbf{\textcolor{myOrange}{images}} \vcenteredinclude{figs/icon4.pdf}. We use the Google custom search API~\cite{google} to perform the image search. We retrieve up to 10 results, while also saving their \textbf{domains}. It is important to note that, unlike the inverse image search, the search results here are not always corresponding to the exact match of the textual query. Therefore, the \textbf{\textcolor{myOrange}{visual evidence}} might be more loosely related to the query \textbf{\textcolor{myOrange}{image}}. However, even if it is not exactly related to the event, it works as a useful baseline of the type of images that could be associated with that topic. 
