\section{Nonlinear programming for pMDPs}\label{sec:nonlinear}
In this section we formally state a general pMDP parameter synthesis problem and describe how
it can be formulated using nonlinear programming.

\subsection{Formal problem statement}\label{sec:problem} 
%We are given a pMDP $\pMdpInit$, a set of specifications $\varphi_1,\ldots,\varphi_n$ that are either probabilistic reachability properties or expected cost properties, and an objective function $f\colon\Paramvar\rightarrow\R$ over the variables $V$.
%We define the \emph{pMDP synthesis problem} as computing a \emph{well-defined valuation} $u$ for $\mdp$, and a (randomized) scheduler $\sched\in\Sched^\mdp$, such that for the MC $\mdp^\sched[u]$ induced by $\sched$ and instantiated by $u$ it holds that $\mdp^\sched[u]\models\varphi_1,\ldots,\varphi_n$ and $f[u]$ is extremal; the obtained pair $(u,\sched)$ is an \emph{optimal solution} to the pMDP synthesis problem.
%Intuitively, we want to compute a valuation of parameters and a scheduler such that all specifications are satisfied and the value of the objective is extremal.
%We refer to a pair $(u,\sched)$ that only guarantees satisfaction of the specifications, but does not necessarily extremize the objective $f$, as a \emph{feasible solution}.
\fbox{
%\begin{minipage}{\textwidth}
\begin{minipage}{\dimexpr\textwidth-4\fboxsep}
\begin{problem}\label{prob:pmdpsyn}
Given a pMDP $\pMdpInit$, specifications
$\varphi_1,\ldots,\varphi_q$ that are either probabilistic reachability
properties or expected cost properties, and an objective function
$f\colon\Paramvar\rightarrow\R$ over the variables $V$, compute a well-defined
valuation $u\in\valuations^V$ for $\mdp$, and a (randomized) scheduler $\sched\in\Sched^\mdp$
such that the following conditions hold:
\begin{enumerate}[(a)]
	\item \label{prob:pmdpsyn_a} \emph{Feasibility}:
		the Markov chain $\mdp^\sched[u]$ induced by scheduler $\sched$ and
		instantiated by valuation $u$ satisfies the specifications, \ie, $\mdp^\sched[u]\models\varphi_1 \wedge \ldots \wedge \varphi_q$.
	
	\item \label{prob:pmdpsyn_b} \emph{Optimality}:
		the objective $f$ is minimized.
\end{enumerate}
\end{problem}
\end{minipage}
}

\noindent Intuitively, we wish to compute a parameter valuation and a scheduler such
that all specifications are satisfied, and the objective is globally minimized.
%We minimize the objective $f$, but depending on the
%application, we might wish to maximize it instead.\sj{I don't like this sentence.. It remains unclear whether we can also maximize or not. Can't we just always minimize in the formal problem statement}
We refer to a valuation--scheduler pair $(u,\sched)$ that satisfies
condition~(\ref{prob:pmdpsyn_a}), \ie, only guarantees satisfaction of the
specifications but does not necessarily minimize the objective $f$, as a
\emph{feasible} solution to the pMDP synthesis problem. If both
(\ref{prob:pmdpsyn_a}) and (\ref{prob:pmdpsyn_b}) are satisfied, the pair
is an \emph{optimal} solution to the pMDP synthesis problem.

%\emph{Strategy variables} $\sigma^{s,\act}\in[0,1]$ for each state $s\in S$ and action $\act\in\Act$ define a strategy $\sigma$ by $\sigma(s,\act)=\sigma^{s,\act}$ if additional side constraints ensure well-definedness. 
%Analogously, \emph{perturbation variables} $\delta^{s,\act}\in[-1,1]$ define a perturbation via $\delta(s,\act)=\delta^{s,\act}$ for all $s\in S$ and $\act\in\Act$.	

%\subsection{Nonlinear programming}\label{sec:nonlinear_approach}
%functions over $\Var$. An NLP is an optimization problem of the form
%As mentioned, we will provide a nonlinear program (NLP) that encodes
%Problem~\ref{prob:pmdpsyn}. 
% by providing a valuation
%$u^\star$ such that $f[u^\star]\leq f[u]$ for all
%$u\in\valuations^\Var$ \sj{while adhering to the constraints $g_i$ and $h_j$}.
%The usefulness of the NLP encoding with respect to the original problem is
%captured by the concepts of \emph{soundness} and \emph{completeness}.  An NLP
%encoding is \emph{sound} if a feasible (optimal) solution of the NLP is a
%feasible (optimal) solution of the problem encoded by the NLP.  The encoding is
%\emph{complete} if every feasible (optimal) solution of the original problem is
%a feasible (optimal) solution of the encoding.


%We refer to soundness in the sense that each variable assignment that satisfies
%the constraints induces a scheduler and a valuation of parameters such that a
%feasible solution of the problem is induced. Moreover, any optimal solution to
%the NLP induces an optimal solution of the problem. 
%
%Completeness means that all possible solutions of the problem can be encoded by this NLP; while
%unsatisfiability means that no such solution exists, making the problem



\subsection{Nonlinear encoding}
We now provide an NLP encoding of Problem~\ref{prob:pmdpsyn}. A general NLP over %valuations $u \in \valuations^\Var$ of 
a set of real-valued variables $\Var$ can be written as
\begin{align}
	\text{minimize} 		&\quad f\label{eq:nl_obj} \\
	\text{subject to} 		&\notag\\
	\forall i.\, 1\leq i\leq m 	&\quad g_i \leq 0,\label{eq:nl_ineq}\\
	\forall j.\, 1\leq i\leq p 	&\quad h_j = 0,\label{eq:nl_eq}
\end{align}
where $f$, $g_i$, and $h_j$ are arbitrary functions over $\Var$, and $m$ and $p$ are the 
number of inequality and equality constraints of the program respectively. Tools like
\tool{IPOPT}~\cite{ipopt} solve small instances of such problems.



Consider a
pMDP $\pMdpInit$ with specifications $\varphi_1=\reachProplT$ and $\varphi_2=\expRewProp{\kappa}{G}$. We will discuss how additional specifications of either type can be encoded.
The set $\Var = \Paramvar \cup W$ of variables of the NLP consists of
the variables $\Paramvar$ that occur in the pMDP as well as a set $W$ of additional variables:
\begin{itemize}
	\item $\{ \sched^{s,\alpha} \mid s \in S, \act\in\Act(s) \}$,
		which define the randomized scheduler $\sched$ by $\sched(s)(\act)=\sched^{s,\act}$.
		% Note that these scheduler variables may be assigned zero.
	\item $\{ p_s \mid s \in S \}$, 
		where $p_s$ is the probability of reaching the target set 
		$T\subseteq S$ from state $s$ under scheduler $\sched$, and
	\item $\{ c_s \mid s \in S \}$, where $c_s$ is the expected cost to reach $G\subseteq S$ from $s$ under $\sched$.
\end{itemize}
%We then lift the given objective function $f$ from the domain $\Paramvar$ to $\Var$,
%yielding $f\colon \Var\rightarrow\R$.
 A valuation over $\Var$ consists of a valuation $u\in\valuations^V$ over the
pMDP variables and a valuation $w\in\valuations^W$ over the additional variables.
\begin{align}
	\mbox{minimize } &\quad f \label{eq:min_rand}\\
	\mbox{subject to}\notag \\
					 &\quad p_{\sinit}\leq \lambda,				\label{eq:lambda}\\
					 &\quad c_{\sinit}\leq \kappa,				\label{eq:kappa}\\
	\forall s\in S.	&\quad \sum_{\act\in\Act(s)}\sched^{s,\act}=1, \label{eq:well-defined_sched_rand}\\
	\forall s\in S\,~\forall\act\in\Act(s). &\quad 0 \leq \sched^{s,\act} \leq 1,				\label{eq:sched_is_dist}\\
	\forall s\in S\,~\forall\act\in\Act(s).	 &\quad \sum_{s'\in S}\probmdp(s,\act,s')=1,	\label{eq:well-defined_probs_rand}\\
	\forall s, s'\in S\,~\forall\act\in\Act(s).	 &\quad 0 \leq \probmdp(s,\act,s') \leq 1,	\label{eq:probs_is_prob}\\
	\forall s\in T.	&\quad p_s=1,															\label{eq:targetprob_rand}\\
	\forall s\in S\setminus T. &\quad p_s=\sum_{\act\in\Act(s)}\sigma^{s,\act}\cdot\sum_{s'\in S}	\probmdp(s,\act,s')\cdot p_{s'}, \label{eq:probcomputation_rand}\\
	\forall s\in G.	 &\quad c_s=0,															\label{eq:targetrew}\\
	\forall s\in S\setminus G.	&\quad c_s= \sum_{\act\in\Act(s)} \sigma^{s,\act} \cdot \Bigl(c(s,\act) + \sum_{s'\in S}	\probmdp (s,\act,s') \cdot c_{s'}\Bigr). \label{eq:rewcomputation}
\end{align}%
The NLP~\eqref{eq:min_rand}--\eqref{eq:rewcomputation} encodes Problem~\ref{prob:pmdpsyn} in the following way.
The objective function $f$ in~\eqref{eq:min_rand} is any real-valued function over the variables $\Var$. 
The constraints~\eqref{eq:lambda} and~\eqref{eq:kappa} encode the
specifications $\varphi_1$ and $\varphi_2$, respectively.
The constraints~\eqref{eq:well-defined_sched_rand}--\eqref{eq:sched_is_dist}
ensure that the scheduler obtained is well-defined by requiring that the
scheduler variables at each state sum to unity. 
Similarly, the constraints
\eqref{eq:well-defined_probs_rand}--\eqref{eq:probs_is_prob} ensure that
for all states, parameters from $\Paramvar$ are instantiated such that
probabilities sum up to one. 
(These constraints are included if not all probabilities at a state are constant.)
The probability of reaching the target for all states in the target set is
set to one using~\eqref{eq:targetprob_rand}.
The reachability probabilities in each state 
depend on the reachability of the successor states and the transition
probabilities to those states through~\eqref{eq:probcomputation_rand}.
Analogously to the reachability probabilities, the cost for each goal state $G\subseteq S$
must be zero, thereby precluding the collection of infinite cost at
absorbing states, as enforced by~\eqref{eq:targetrew}.
Finally, the expected cost for all states except target states is given by
the equation~\eqref{eq:rewcomputation}, where according to the
strategy $\sched$ the cost of each action is added to the expected cost of
the successors. 
%Note that the probability of reaching states from $G$ needs to be one in order
%for the expected cost to be defined. 
	%The NLP works as follows.
	%First, the value of the objective function is minimized~\eqref{eq:min_rand}. The probability assigned to the initial state $\sinit\in S$ has to be smaller than or equal to $\lambda$ to satisfy $\varphi=\reachProplT$~\eqref{eq:lambda}. This is analogous for the expected cost assigned to the initial state~\eqref{eq:kappa}. To have a well-defined randomized scheduler $\sched$, we ensure that the assigned values of the corresponding strategy variables at each state sum up to one~\eqref{eq:well-defined_sched_rand}. 

	%For all target states $T\subseteq S$, the probability of the corresponding probability variables is assigned one~\eqref{eq:targetprob_rand}; analogously, for each goal state $G\subseteq S$ the expected cost of the cost variables is assigned zero, otherwise infinite cost is collected at absorbing states. The probability to reach $T\subseteq S$ from each $s\in S$ is computed in~\eqref{eq:probcomputation_rand}, defining a nonlinear equation system, where action probabilities, given by the induced strategy $\sched$, are multiplied by probability variables for all possible successors. 
	
We can readily extend the NLP to include more specifications. If
another reachability property $\varphi'=\reachProp{\lambda'}{T'}$ is given, we add the set of probability variables $\{ p'_s \mid
s \in S\}$ to $W$, and duplicate the 
constraints~\eqref{eq:targetprob_rand}--\eqref{eq:probcomputation_rand} accordingly.
%is extended to all
%states $s\in T\cup T'$, and~\eqref{eq:probcomputation_rand} is copied for all
%$p'_s$, thereby computing the probability of reaching $T'$ under the same
%scheduler as for reaching $T$. 
To ensure satisfaction of $\varphi'$, we also add the constraint
$p'_{\sinit}\leq \lambda'$.
The procedure is similar for additional expected cost properties. 
By construction, we have the following result relating the NLP encoding and Problem~\ref{prob:pmdpsyn}.
\begin{theorem}
	\label{thm:soundcomplete}
	The NLP~\eqref{eq:min_rand}--\eqref{eq:rewcomputation} is sound and complete with respect to Problem~\ref{prob:pmdpsyn}.
\end{theorem}
%
We refer to soundness in the sense that each variable assignment that satisfies
the constraints induces a scheduler and a valuation of parameters such that a
feasible solution of the problem is induced. Moreover, any optimal solution to
the NLP induces an optimal solution of the problem. Completeness means that all
possible solutions of the problem can be encoded by this NLP; while
unsatisfiability means that no such solution exists, making the problem
\emph{infeasible}.
%We refer to soundness and completeness as in Section~\ref{sec:nonlinear_approach}. 

\paragraph{Signomial programs.} By Def.~\ref{def:posy} and~\ref{def:pmdp}, all constraints in the NLP consist of signomial functions.
A special class of NLPs known as \emph{signomial programs} (SGPs) is of the form~\eqref{eq:nl_obj}--\eqref{eq:nl_eq} where $f$, $g_i$ and $h_j$ are signomials over $\Var$, see Def.~\ref{def:posy}. Therefore, we observe that the NLP~\eqref{eq:min_rand}--\eqref{eq:rewcomputation} is an SGP. We will refer to the NLP as an SGP in what follows.

SGPs with equality constraints consisting of functions that are \emph{not affine} are not \emph{convex} in general. 
In particular, the SGP~\eqref{eq:min_rand}--\eqref{eq:rewcomputation} is not necessarily convex. Consider a simple pMC only having transition probabilities of the form $p$ and $1-p$, as in Example~\ref{ex:die}. The function in the equality
constraint~\eqref{eq:probcomputation_rand} of the corresponding SGP encoding is not affine in
parameter $p$ and the probability variable $p_s$ for some state $s\in S$.
More generally, the equality constraints
\eqref{eq:well-defined_probs_rand},
\eqref{eq:probcomputation_rand}, and
\eqref{eq:rewcomputation}
involving $\probmdp$ are not necessarily affine, and thus the SGP may not be a convex program~\cite{boyd_convex_optimization}.
%If the transition probability functions are polynomials in the parameters, then the constraints~\eqref{eq:probcomputation_rand} or not even bilinear.
%For more complicated problems involving higher degree transition probability functions and nondeterminism, the nonconvexity of the constraint~\eqref{eq:probcomputation_rand} is recursively worse. 
Whereas for convex programs \emph{global optimal solutions} can be
found efficiently~\cite{boyd_convex_optimization}, such guarantees are
not given for SGPs. 
However, we can efficiently obtain local optimal solutions for SGPs in our setting, as shown in the following sections.




