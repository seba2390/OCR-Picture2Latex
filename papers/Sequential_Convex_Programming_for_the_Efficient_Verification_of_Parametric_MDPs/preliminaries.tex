\section{Preliminaries}
\label{sec:preliminaries}
%\subsection{Distributions and polynomials}
%

%and
% $\supp(\distFunc)=\{x\in\distDom \mid \distFunc(x)>0\}$ is the \emph{support} of $\distFunc \in \Distr(\distDom)$. 
%If $\mu(x)=1$ for $x\in\distDom$ and $\mu(y)=0$ for all $y\in\distDom\setminus\{x\}$, $\mu$ is called a \emph{Dirac distribution}.
%%
%$\pol$ over $\Paramvar$ satisfying $\pol = \sum_{i \le m} a_i \cdot \prod_{x \in \Paramvar_i} x$ for suitable $m \in \N$, $a_i \in \Q$, and $\Paramvar_i \subseteq \Paramvar$ (for $i \le m$). 
%$\functions[\Var]$ does not contain polynomials where a variable has a degree greater than $1$, \eg, $x\cdot y \in \functions[\Var]$ but $x^2  \notin \functions[\Var]$.
%. 

%For an arbitrary function $f\colon X\rightarrow Y$ we denote the \emph{domain of $f$} by $\dom(f)=X$. Let $V$ denote a set of parameters. The domain of a parameter $x\in V$ is denoted by $\dom(x)$. The domain of the whole set of parameters is denoted by $\dom(V)=\bigcup_{x\in V}\dom(x)$.
% 
%\subsection{Distributions and functions} 
A \emph{probability distribution} over a finite or countably infinite set $\distDom$ is a function $\distFunc\colon\distDom\rightarrow\Ireal$ with $\sum_{\distDomElem\in\distDom}\distFunc(\distDomElem)=1$. 
%In this paper, all probabilities are taken from $\Q$.
The set of all distributions on $\distDom$ is denoted by $\Distr(\distDom)$.

% \hp{Define variable and domain?}
% A \emph{variable} is a symbol with an unspecified value, with the restriction that the value belongs to a set known as the domain of the variable. An assignment of a unique element of the domain to the variable is known as a valuation of the variable. 
% \hp{end definition}
% A symbol $x$ and set $X$ together define a \emph{set}-valued \emph{variable} which can have values in the set $X$, known as the domain of $x$. For example, $x$ is a real-valued variable is the possible values of $x$ belong to the real number line. A
%
%Now we introduce the notions of monomials and posynomials and discuss their differences with standard polynomials.
\begin{definition}[Monomial, Posynomial, Signomial]\label{def:posy}
  Let $V=\{x_1,\ldots,x_n\}$ be a finite set of strictly positive real-valued \emph{variables}.
  A \emph{monomial} over $V$ is an expression of the form
  \begin{align*}
      \mono=c\cdot x_{1}^{a_{1}}\cdots x_{n}^{a_{n}}\ ,
  \end{align*}
  where $c\in \R_{>0} $ is a positive coefficient, and $a_i\in\R$ are exponents for $1\leq i\leq n$. 
  A \emph{posynomial} over $V$ is a sum of one or more monomials:
  \begin{align}
      \posy=\sum_{k=1}^K c_k\cdot x_1^{a_{1k}}\cdots x_n^{a_{nk}} \ .\label{eq:signomial}
  \end{align}
  If $c_k$ is allowed to be a negative real number for any $1\leq k\leq K$, then the expression~\eqref{eq:signomial} is a \emph{signomial}. The sets of all monomials, posynomials, and signomials over $V$ are denoted by $\monos[V]$, $\posys[V]$, and $\signomis[V]$, respectively.
 \end{definition}
%
This definition of monomials differs from the standard algebraic definition where exponents are positive integers with no restriction on the coefficient sign. A sum of monomials is then called a \emph{polynomial}. Our definitions are consistent with~\cite{boyd2007tutorial}.
%, whereas a fraction of polynomials is called a \emph{rational function}.
% Let $\poly[\Paramvar]$ denote the set of  \emph{polynomials} and $\functions[\Paramvar]$ the set of \emph{rational functions} over $\Paramvar$.
%\nj{introduce signomials?}
\begin{definition}[Valuation]
  \label{def:valuation}
  For a set of real-valued variables $V$, a \emph{valuation $u$ over $V$} is a function $u\colon V \rightarrow \R$.
  The set of all valuations over $V$ is $\valuations^V$.
\end{definition}
 Applying valuation $u$ to monomial $\mono$ over $V$ yields a real number $\mono[u]\in\R$ by replacing each occurrence of variables $x\in V$ in $\mono$ by $u(x)$; the procedure is analogous for posynomials and signomials using standard arithmetic operations.

%\subsection{Probabilistic models}\label{sec:probmodels}

%We introduce parametric probabilistic models and specifications for such models.
 %
%
\begin{definition}[pMDP and pMC]\label{def:pmdp}
A \emph{parametric Markov decision process (pMDP)} is a tuple $\pMdpInit$ with a finite set $S$ of states, an initial state $\sinit \in S$, a finite set $\Act$ of actions, a finite set of real-valued variables $\Paramvar$, and a transition function $\probmdp \colon S \times \Act \times S \rightarrow \signomis[\Paramvar]$ satisfying for all $s\in S\colon
\Act(s) \neq \emptyset$,  where $\Act(s) = \{\act \in \Act \mid \exists s'\in S.\,\probmdp(s,\,\act,\,s') \neq 0\}$.
If for all $s\in S$ it holds that $|\Act(s)| = 1$, $\mdp$ is called a \emph{parametric discrete-time Markov chain (pMC)}%, denoted by $\dtmc$
.
\end{definition}
%
% 
%At each state, an action is chosen \emph{nondeterministically} and successor states are determined \emph{probabilistically} according to the transition function.
%
$\Act(s)$ is the set of \emph{enabled} actions at state $s$; as $\Act(s) \neq \emptyset$, there are no deadlock states.
% For pMCs there is only one single action per state and we write the transition probability function as $\probmdp\colon S\times S\rightarrow\poly[\Paramvar]$, omitting that action.
%
%\emph{Rewards} are defined using a \emph{reward function} $\rewFunction \colon S \rightarrow \R$ which assigns rewards to states of the model.
%Intuitively, the reward $\rewFunction(s)$ is earned upon \emph{leaving} the state $s$. 
\emph{Costs} are defined using a state--action \emph{cost function} $\rewFunction \colon S \times \Act \rightarrow \R_{\geq 0}$.
%Intuitively, the cost $\rewFunction(s)$ is incurred upon \emph{leaving} the state $s$. 
\begin{remark}
%\sj{Alternative: the transition probabilities are given as rational functions?}
Largely due to algorithmic reasons, the transition probabilities in the literature~\cite{param_sttt,dehnert-et-al-cav-2015,quatmann-et-al-atva-2016} are polynomials or rational functions, \ie, fractions of polynomials.  
 Our restriction to signomials is realistic; \emph{all} benchmarks from the \tool{PARAM}--webpage~\cite{param_website} contain only signomial transition probabilities. 
\end{remark}

%\begin{definition}[MDP]
A pMDP $\mdp$ is a \emph{Markov decision process (MDP)} if the transition function is a valid probability distribution, \ie, $\probmdp \colon S \times \Act \times S \rightarrow [0,1]$ and $\sum_{s'\in S}\probmdp(s,\act,s') = 1$ for all $s \in S \mbox{ s.t.\ } \act \in \Act(s)$. 
%\end{definition}
%%
Analogously, a Markov chain (MC) is a special class of a pMC; a model is \emph{parameter-free} if all  probabilities are constant. Applying a \emph{valuation} $u$ to a pMDP, denoted $\mdp[u]$, replaces each signomial $f$ in $\mdp$ by $f[u]$; we call $\mdp[u]$ the \emph{instantiation} of $\mdp$ at $u$.
The application of $u$ is to replace the transition function $f$ by the probability $f[u]$. 
A valuation $u$ is \emph{well-defined} for $\mdp$ if the replacement yields \emph{probability distributions} at all states; the resulting model $\mdp[u]$ is an MDP or an MC.

\begin{example}[pMC]\label{ex:die}
Consider a variant of the Knuth--Yao model of a die~\cite{KY76}, where a six-sided die is simulated by successive coin flips. 
We alternate flipping two biased coins, which result in \emph{heads} with probabilities defined by the monomials $p$ and $q$, respectively. Consequently, the probability for \emph{tails} is given by the signomials $1-p$ and $1-q$, respectively. The corresponding pMC is depicted in Fig.~\ref{fig:pkydie}; and the \emph{instantiated} MC for $p = 0.4$ and $q = 0.7$ is given in Fig.~\ref{fig:pkydiei}. Note that we omit actions, as the model is deterministic.
\end{example}
%
\begin{figure}[t]
\centering
	\subfigure[pMC model]{
		\scalebox{0.75}{
			\input{pics/pkydie}
	\label{fig:pkydie}
		}
	}
	\subfigure[Instantiation using $p{=}0.4$ and $q{=}0.7$]{
		\scalebox{0.75}{
			\input{pics/pkydiei}
	\label{fig:pkydiei}
		}
	}
%	\vspace{-0.3cm}
\caption{A variant of the Knuth--Yao die for unfair coins.}
\vspace{-0.5cm}
\end{figure}
%
%
%\paragraph{Schedulers.}
In order to define a probability measure and expected cost on MDPs, nondeterministic choices are resolved by so-called \emph{schedulers}.
%\footnote{Also referred to as strategies or policies.}. 
For practical reasons we restrict ourselves to \emph{memoryless} schedulers; details can be found in~\cite{BK08}.
%\nj{footnote about how memoryless schedulers are not sufficient for multi-objective props}
%

\begin{definition}[Scheduler]\label{def:scheduler}
	A (randomized) \emph{scheduler} for an MDP $\mdp$ is a function $\sched\colon S\rightarrow\Distr(\Act)$ such that $\sigma(s)(\alpha) > 0$ implies $\alpha \in \Act(s)$.
%	Schedulers using only Dirac distributions are called \emph{deterministic}. 
	The set of all schedulers over $\mdp$ is denoted by $\Sched^\mdp$.
\end{definition}
%
%Deterministic schedulers are functions of the form $\sched\colon S\rightarrow\Act$ with $\sigma(s) \in \Act(s)$. 
Applying a scheduler to an MDP yields a so-called \emph{induced Markov chain}.
%, as all nondeterminism is resolved. 

\begin{definition}[Induced MC]\label{def:induced_dtmc}
	Let MDP $\MdpInit$ and scheduler $\sched\in\Sched^\mdp$. The \emph{MC induced by $\mdp$ and $\sched$} is  $\mdp^\sched=(S,\sinit,\Act,\pmdp^\sched)$ where for all $s,s'\in S$,
	\begin{align*}
		 \pmdp^\sched(s,s')=\sum_{\alpha\in\Act(s)} \sched(s)(\act)\cdot\pmdp(s,\alpha,s').
	\end{align*} 
\end{definition}
%Intuitively, the transition probabilities in $\mdp^\sched$ are obtained with respect to the random choices of action of the scheduler. 
%
%\paragraph{Specifications.}
We consider \emph{reachability properties} and \emph{expected cost properties}.
For MC $\dtmc$ with states $S$, let $\reachPrs{\dtmc}{s}{T}$ denote the probability of reaching a set of \emph{target states} $T \subseteq S$ from  state $s\in S$; simply $\reachPrT[\dtmc]$ denotes the probability for initial state $\sinit$.
We use the standard probability measure as in~\cite[Ch.\ 10]{BK08}.
%
For threshold $\lambda\in [0,1]$, the \emph{reachability property} asserting that a target state is to be reached with probability at most $\lambda$ is denoted $\reachPropSymbol = \reachProplT$.
The property is satisfied by $\dtmc$, written $\dtmc \models \reachPropSymbol$, iff $\reachPrT[\dtmc]\leq\lambda$.
%(Comparisons like $<$, $>$, and $\geq$ are treated in a similar way.)

The cost of a path through MC $\dtmc$ until a set of \emph{goal states} $G\subseteq S$ is the sum of action costs visited along the path. The expected cost of a finite path is the product of its probability and its cost.
For $\reachPr{\dtmc}{G}  = 1$, the expected cost of reaching $G$ is the sum of expected costs of all paths leading to $G$.
%In case $\reachPrT[\dtmc] < 1$, we set $\expRewT[\dtmc] = \infty$.\sj{As we do not go into technical details, this sentence seems superfluous}
An expected cost property $\expRewProp{\kappa}{G}$ is satisfied if the expected cost of reaching $T$ is bounded by a threshold $\kappa \in \R$.
Formal definitions are given in e.g.,~\cite{BK08}.

If multiple specifications $\varphi_1,\ldots,\varphi_q$ are given, which are either reachability properties or expected cost properties of the aforementioned forms, we write the satisfaction of all specifications $\varphi_1,\ldots,\varphi_q$ for an MC $\dtmc$ as $\dtmc\models\varphi_1\land\ldots\land\varphi_q$. 

An MDP $\mdp$ satisfies the specifications $\varphi_1,\ldots,\varphi_q$, iff \emph{for all} schedulers $\sched\in\Sched^\mdp$ it holds that $\mdp^\sched\models\varphi_1\wedge\ldots\wedge\varphi_q$. The verification of multiple specifications is also referred to as \emph{multi-objective model checking}~\cite{DBLP:journals/lmcs/EtessamiKVY08,DBLP:conf/atva/ForejtKP12}.
We are also interested in the so-called scheduler \emph{synthesis problem}, where the aim is to find a scheduler $\sched$ such that the specifications are satisfied (although other schedulers may not satisfy the specifications).





%When reasProbabilities and expected cost on MDPs are computed based on specific schedulers on the induced MC.
%
%\begin{remark}\label{rem:det_schedulers}
%\emph{Deterministic schedulers} pick just one action at each state and the associated probability distribution determines the probabilities. In this case we write for all states $s\in S$ and $a\in\Act$ with $\sched(s)(a)=1$:
%\begin{align*}
%	\pmdp^\sched(s,s')=\pmdp(s,a)(s')\ .
%\end{align*}
%\end{remark}


%
%If $\probmdp \colon S \times \Act \times S \rightarrow \Ireal$ and $\sum_{s'\in S}\probmdp(s,\act,s') = 1$ for all $s \in S \mbox{ and } \act \in \Act(s)$, $\mdp$ is called an MDP.




%\paragraph{Properties.}
%
%For our purpose we consider \emph{conditional reachability properties} and \emph{conditional expected reward properties} in MCs.
%For more detailed definitions we refer to~\cite[Ch.\ 10]{BK08}.
%Given an MC $\dtmc$ with state space $S$ and initial state $\sinit$, let $\pr^{\dtmc}(\neg \finally U)$ denote the probability \emph{not} to reach a set of undesired states $U$ from the initial state $\sinit$ within $\dtmc$.
%Furthermore, let $\creachPr{\dtmc}{T}{U}$ denote the conditional probability to reach a set of target states $T \subseteq S$ from the initial state $\sinit$ within $\dtmc$, given that no state in the set $U$ is reached.
%We use the standard probability measure on infinite paths through an MC.
%For threshold $\lambda\in \Ireal$, the reachability property, asserting that a target state is to be reached with conditional probability at most $\lambda$, is denoted $\reachPropSymbol = \creachProplT$.
%The property is satisfied by $\dtmc$, written $\dtmc \models \reachPropSymbol$, iff $\creachPrT[\dtmc]\leq\lambda$.
%This is analogous for comparisons like $<$, $>$, and $\geq$.
%
%The reward of a path through an MC $\dtmc$ until $T$ is the sum of the rewards of the states visited along on the path before reaching $T$.
%The expected reward of a finite path is given by its probability times its reward.
%Given $\reachPrT[\dtmc] = 1$, the conditional expected reward of reaching $T \subseteq S$, given that no state in set $U \subseteq S$ is reached, denoted $\cexpRewT[\dtmc]$, is the expected reward of all paths accumulated until hitting $T$ while not visiting a state in $U$ in between divided by the probability of not reaching a state in $U$ (\ie, divided by $\pr^{\dtmc}(\neg \finally U)$).
%An expected reward property is given by $\ereachPropSymbol = \cexpRewPropkT$ with threshold $\kappa \in \R_{\geq 0}$.
%The property is satisfied by $\dtmc$, written $\dtmc \models \ereachPropSymbol$, iff $\cexpRewT[\dtmc]\leq\kappa$.
%Again, this is analogous for comparisons like $<$, $>$, and $\geq$.
%%We again refer to, \eg,~\cite[Ch.\ 10]{BK08} for formal definitions.
%%
%For details about conditional probabilities and expected rewards see~\cite{DBLP:conf/tacas/BaierKKM14}. 
%
%Reachability probabilities and expected rewards for MDPs are defined on induced MCs for specific schedulers. 
%We take here the conservative view that a property for an MDP has to hold for \emph{all possible schedulers}. 
%

%\paragraph{Parameter Synthesis.}
%For pMCs, one is interested in \emph{synthesizing} well-defined valuations that induce satisfaction or violation of the given specifications~\cite{dehnert-et-al-cav-2015}. In detail, for a pMC $\dtmc$, a rational function $\pol\in\functions[\Paramvar]$ is computed which---when instantiated by a well-defined valuation $u$ for $\dtmc$---evaluates to the actual reachability probability or expected reward for $\dtmc$, \ie, $\pol[u]=\reachPr{\dtmc[u]}{T}$ or $\pol[u]=\expRewT[{\dtmc[u]}]$. For pMDPs, schedulers inducing \emph{maximal} or \emph{minimal} probability or expected reward have to be considered~\cite{quatmann-et-al-techreport-2016}.
%\nj{adapt to our setting}
%\cd{1. I don't really understand the last sentence (other than that we should cite the paper). 2. We only presented conditional probabilities and conditional expected rewards before and in this paragraph this changes. We should probably present it in a general way and then later say that we can only handle the condition 'true' for MDPs, because we can in fact treat conditional stuff on pMCs.\nj{I somehow don't get what you mean...}}
