\section{Parameter/strategy synthesis for probabilistic systems using nonlinear optimization}
\nj{restrict rational functions to the ones we can handle.}
We are given a parametric Markov chain (pMC) $\dtmcInit$ with $P\colon S\times S\rightarrow\functions[\Var]$ for $\functions[\Var]$ being the set of \emph{rational functions} over a finite set of parameters $\Var$. 
Adding nondeterminism, assume a parametric Markov decision process pMDP $\MdpInit$ with $\probmdp \colon S \times \Act \times S \rightarrow \functions[\Var]$. For both $\dtmc$ and $\mdp$ we are given a set of target states $T\subseteq S$ and a unique initial state $\sinit\in S$.

There are several measures of interest: 
\begin{enumerate}
	\item For a pMC $\dtmc$, find a \emph{parameter valuation} for $\Var$ that induces the minimal or maximal probability or expected cost to reach $T$ 
	\item For a pMDP $\mdp$, find a parameter valuation and a \emph{scheduler} inducing again minimal or maximal probability or expected cost to reach $T$.
	\item For a given valuation $u$ of parameters and either a pMC $\dtmc$ or a pMDP $\mdp$, find a new valuation $u'$ \emph{minimally deviating} from $u$ such that a certain property for $\dtmc$ or $\mdp$ is satisfied which was violated before. (This is referred to as model repair~\cite{bartocci2011model}).
	\item For an MDP and multiple objectives, find a \emph{randomized scheduler} that induces satisfaction of all objectives. This problem was studied in~\cite{DBLP:journals/lmcs/EtessamiKVY08,DBLP:conf/atva/ForejtKP12}. To distinguish ourselves from these results, we investigate scenarios where it makes sense to impose restrictions on schedulers, \ie dependencies of randomized decisions at different states.
	\nj{I need to check how optimality is defined for multiple objectives.}
\end{enumerate}


%\emph{Strategy variables} $\sigma^{s,\act}\in[0,1]$ for each state $s\in S$ and action $\act\in\Act$ define a strategy $\sigma$ by $\sigma(s,\act)=\sigma^{s,\act}$ if additional side constraints ensure well-definedness. 
%Analogously, \emph{perturbation variables} $\delta^{s,\act}\in[-1,1]$ define a perturbation via $\delta(s,\act)=\delta^{s,\act}$ for all $s\in S$ and $\act\in\Act$.	
%

		
\subsection{Nonlinear optimization/feasibility problems}
In the following we list the problems for MCs and MDPs, restricting ourselves to reachability probabilities.
\subsubsection{A. Markov chains.}
Let $\dtmc$ be a pMC. We assume that all transition probabilities are either constant probabilities or are of the form $p$ for $p\in\Var$. If a transition probability is, \eg, of the form $1-p$, we substitute it by a fresh parameter $\overline p$ which we add to $\Var$. In that case we have to ensure that assignments of parameters induce that at each state the probabilities sum up to one.

In addition to the parameters from $\Var$, we introduce the following \emph{variables}:
		\begin{itemize}
			\item $p_s\in[0,1]$ for each $s\in S$ will be assigned the probability of reaching $T\subseteq S$.
		\end{itemize}
	 	The NLP for maximizing the probability of reaching $T$ reads as follows:
		\begin{align}
			\text{maximize } &\quad p_{\sinit}\label{eq:min}\\
			\text{such that}\\
%							 &\quad p_{\sinit}\leq \lambda\label{eq:strategyah:lambda}\\
			\forall s\in T.	 &\quad p_s=1\label{eq:targetprob}\\
			\forall s\in S\setminus T.	&\quad p_s = \sum_{s'\in S}	P(s,s')\cdot p_{s'}\label{eq:probcomputation}\\
			\forall s\in S.	 &\quad \sum_{s'\in S}P(s,s')=1\label{eq:well-defined_probs}
		\end{align}
		Constraint~\ref{eq:well-defined_probs} enforces that parameters are assigned values in a way such that well-defined probability distributions result. 
		
\subsubsection{B. Markov decision processes---deterministic schedulers.}
Let $\mdp$ be an MDP. We again use the following \emph{variables}:
		\begin{itemize}
			\item $p_s\in[0,1]$ for each $s\in S$ will be assigned the probability of reaching $T\subseteq S$.
		\end{itemize}
	 	The NLP reads as follows:
		\begin{align}
			\text{minimize } &\quad p_{\sinit}\label{eq:min_mdp}\\
			\text{such that}\\
%							 &\quad p_{\sinit}\leq \lambda\label{eq:strategyah:lambda}\\
			\forall s\in T.	 &\quad p_s=1\label{eq:targetprob_mdp}\\
			\forall s\in S\setminus T.\,\forall \act\in\Act.	&\quad p_s \geq \sum_{s'\in S}	\probmdp(s,\act,s')\cdot p_{s'}\label{eq:probcomputation_mdp}\\
			\forall s\in S.\, \forall\act\in\Act.	 &\quad \sum_{s'\in S}\probmdp(s,\act,s')=1\label{eq:well-defined_probs_mdp}
		\end{align}
Note that this optimization problem computes the \emph{maximal probability} of reaching T under a deterministic scheduler. The reachability probabilities for each choice of action are put as a lower bound to the probability variables $p_s$ for each $s\in S$. In order for the assignment to satisfy the highest lower bound with equality, $p_{\sinit}$ is minimized. If the minimal probability is to be found, the probabilities need to be put as upper bound while $p_{\sinit}$ is maximized.

\subsubsection{C. Markov decision processes---randomized schedulers.}
For some properties, in particular when having multiple objectives, randomized schedulers are strictly more powerful than deterministic ones. For the NLP, we introduce the following specific \emph{variables}:
		\begin{itemize}
			\item $\sched^{s,\alpha}\in[0,1]$ for each $s\in S$ and $\act\in\Act$ define the randomized scheduler $\sched$. Note that these scheduler variables may be assigned zero.
			\item $p_s\in[0,1]$ for each $s\in S$ will be assigned the probability of reaching $T\subseteq S$ from state $s$ under strategy $\sched$.
		\end{itemize}
	 	The NLP reads as follows:
		\begin{align}
			\text{minimize } &\quad p_{\sinit}\label{eq:min_rand}\\
			\text{such that}\\
%							 &\quad p_{\sinit}\leq \lambda\label{eq:strategyah:lambda}\\
			\forall s\in T.	 &\quad p_s=1\label{eq:targetprob_rand}\\
			\forall s\in S.	&\quad \sum_{\act\in\Act}\sched^{s,\act}=1\label{eq:welldefined_sched_rand}\\
			\forall s\in S\setminus T.	&\quad p_s=\sum_{\act\in\Act}\sigma^{s,\act}\cdot\sum_{s'\in S}	\probmdp(s,\act,s')\cdot p_{s'}\label{eq:probcomputation_rand}\\
			\forall s\in S.\, \forall\act\in\Act.	 &\quad \sum_{s'\in S}P(s,\act,s')=1\label{eq:well-defined_probs_rand}
		\end{align}
	\paragraph{Further specifications.}
We now explain how the NLP can be extended for further specifications. Assume  another reachability probability for target states $T'$ with $T'\neq T$ is of interest. We add another set of probability variables $p'_s$ for each state $s\in S$; Equation~\eqref{eq:targetprob_rand} is defined for all states $s\in T\cup T'$ and~\eqref{eq:probcomputation_rand} is copied for all $p'_s$, thereby computing the probability of reaching $T'$ under the same scheduler as for reaching $T$. It mostly makes sense, to optimize the probability of only one objective; therefore we assume for any further reachability objective to have a reachability specification to be satisfied, \eg, $\reachProp{\lambda}{T'}$.

	To handle an \emph{expected cost property} $\expRewProp{\kappa}{G}$ for $G\subseteq S$, we use variables $r_s$ being assigned the expected cost for reaching $G$ for all $s\in S$. We add the following equations:
%	
	\begin{align}
							 &\quad r_{\sinit}\leq \kappa\label{eq:strategyahexp:kappa}\\
			\forall s\in G.	 &\quad r_s=0\label{eq:strategyahexp:targetrew}\\
			\forall s\in S.	&\quad r_s= \sum_{\act\in\Act} \Bigl(\sigma_{ha}^{s,\act} \cdot \rew(s,\act) +    \sum_{s'\in S}	\probmdp (s,\act)(s') \cdot r_{s'}\Bigr)\label{eq:strategyahexp:rewcomputation}	
		\end{align}
		First, the expected cost of reaching $G$ is smaller than or equal to $\kappa$ at $\sinit$~\eqref{eq:strategyahexp:kappa}. Goal state are assigned cost zero~\eqref{eq:strategyahexp:targetrew}, otherwise infinite cost is collected at absorbing states. Finally, the expected cost for all other states is computed by~\eqref{eq:strategyahexp:rewcomputation} where according to the blended strategy $\sched_{ha}$ the cost of each action is added to the expected cost of the successors. 
	An important insight is that if all specifications are expected reward properties, the program is \emph{no longer nonlinear} but a linear program (LP), as there is no multiplication of variables.
%

\medskip

\subsubsection{Feasibility problems.} In case one is not interested in the exact minimal (or maximal) probability, we can also impose upper or lower bounds $\lambda\in\R$ on the reachability probability:
		\begin{align}
		p_{\sinit}\leq\lambda\\
		p_{\sinit}\geq\lambda
		\end{align}
		If we want to determine feasibility within certain \emph{parameter regions}, we can also add constraints restricting the valuations of parameters, \eg, 
	\begin{align}
		\forall x\in \Var.	 &\quad a_x\leq x\leq b_x\label{eq:region}
	\end{align}
	for $a_x,b_x\in[0,1]$ for all $x\in\Var$.
		Together with such restrictions on parameter valuations, this can help to find out if inside such a region there exists one valuation satisfying/violating the given reachability specification. This is a nonlinear feasibility problem and can be solved by SMT solvers like \tool{Z3}~\cite{demoura_nlsat}.
		
		

\section{Utilizing convex optimization}

\paragraph{Geometric programming}
In~\cite{boyd_convex_optimization}, it is described how a nonlinear program of a special form, called \emph{geometric program}, can be transformed into a convex optimization problem. We want to make use of this transformation.
Let $x=\{x_1,\ldots,x_n\}$ be a set of variables with $x_i\in\R_{>0}$ for $1\leq i\leq n$. Let $h(x)$ denote a \emph{monomial} over $x$ and $f(x)$ denote a \emph{posynomial} over $x$, as in~\cite{boyd_convex_optimization}. \nj{Add details.}

A \emph{geometric program} is of the form
\begin{align}
	\text{minimize} 		&\quad f_0(x)\\
	\text{such that} 		&\\
	\forall i.\, 1\leq i\leq m 	&\quad f_i(x)\leq 1\\
	\forall i.\, 1\leq i\leq p 	&\quad h_i(x)= 1
\end{align}
Summarized, only real-valued, positive variables are allowed. All posynomials are upper bounded by one, all monomials are equal to one, and the objective function is a posynomial.



\paragraph{Thoughts on the transformation.}
First, recall that in order for a nonlinear program to be geometric, all variables need to be positive.
For \emph{probability variables} $p_s$, we have to ensure that the computed strategy ensures a positive reachability probability for target states \emph{for all states}. We consider the following cases for pMCs:
\begin{itemize}
	\item \textit{Prob0}-states: There exists on path from these states to the target states $T$, \ie for $s\in\textit{Prob0}$ the probability to reach $T$ is $0$ and $p_s=0$ needs to hold. These states are removed from the pMC. Note that this might leave substochastic distributions at the predecessor states.
	\item \textit{Prob1}-states: The probability to reach $T$ is $1$. As a preprocessing, these states can be added to $T$ and made absorbing.
\end{itemize}
For pMDPs, we have the following cases:
\begin{itemize}
	\item \textit{Prob0A}-states: For all schedulers, the probability of reaching $T$ is $0$. As before, these states can be removed.
	\item \textit{Prob0E}-states: For these states, there exists a scheduler that induces probability $0$ of reaching $T$. In~\cite{wimmer-et-al-tcs-2014}, we called these states \emph{problematic} and offered an SMT/MILP encoding to avoid that the problematic schedulers are assigned. Note that in case of maximizing the reachability probability of target states, this is not a problem as problematic schedulers will never be chosen. For feasibility problems, however, we might need to handle this problem. 
	\nj{Are constraints enforcing reachability of target states possible in geometric program?}
\end{itemize}
All these sets of states can be computed by simple graph analysis of the pMC/pMDP.


%Algorithmically, this can be achieved by a preprocessing in the following way: Compute the set of \emph{problematic states} for $\mdp$, \ie, the set of states where a strategy exists that induces probability zero of reaching target states. For these states, inside the NLP encoding, reachability has to be ensured, see~\cite{wimmer-et-al-tcs-2014}.

In addition to the probability variables, also for \emph{strategy variables} $\sched^{s,\act}$ it is now no longer allowed to have probability zero for an action at state $s$, \ie, $\sched^{s,\act}=0$. One idea could be to ensure a \emph{minimal probability} $p_\epsilon$ to be assigned for all actions at each state. Getting exactly this minimal probability as a result might hint to the fact that actually $0$ induces the optimal solution.

\subsubsection{Equality constraints}
The foremost problem in our setting is, that we have a lot of equality constraints in the original NLP encodings. Recall the problem of computing maximal probabilities for an MDP with deterministic strategies. Basically, we need to relax Equation~\eqref{eq:well-defined_probs_mdp}, resulting in the following geometric program:
\begin{align}
			\text{minimize } &\quad p_{\sinit}\label{geo:eq:min_mdp}\\
			\text{such that}\\
%							 &\quad p_{\sinit}\leq \lambda\label{eq:strategyah:lambda}\\
			\forall s\in T.	 &\quad p_s=1\label{geo:eq:targetprob_mdp}\\
			\forall s\in S\setminus T.\,\forall \act\in\Act.	&\quad \sum_{s'\in S}\frac{\probmdp(s,\act,s')\cdot p_{s'}}{p_s}\leq 1 \left(\Leftrightarrow p_s \geq \sum_{s'\in S} P(s,s')\cdot p_{s'}\right)\label{geo:eq:probcomputation_mdp}\\
			\forall s\in S.\, \forall\act\in\Act.	 &\quad \sum_{s'\in S}\probmdp(s,\act,s')\leq 1\label{geo:eq:well-defined_probs_mdp}
		\end{align}
In fact, the optimal solution to this problem is also an optimal solution to the original problem iff 
Equation~\ref{geo:eq:well-defined_probs_mdp} is satisfied with equality.
\nj{relate to boyd tutorial}
Note that constraints given by \eqref{geo:eq:well-defined_probs_mdp} are only generated if $\probmdp{s,\act,s'}$ is not constant. Assume now that we have $\probmdp(s,\act,s'_1)=p$ and $\probmdp(s,\act,s'_2)=1-p$. We substitute each occurrence of $1-p$ by the fresh parameter $\bar p$ and add the constraint $p + \bar p \leq 1$ which we need to be satisfied with equality. We pick $p$ which needs to have the following properties: 
\begin{enumerate}
	\item $p$ does not appear in any of the monomial constraints given by \eqref{geo:eq:targetprob_mdp}
	\item Both the objective function \eqref{geo:eq:min_mdp} and the (other) inequality constraints given by \eqref{geo:eq:probcomputation_mdp} need to be \emph{monotonically decreasing} in $p$. 
	\item The constraint generated from \eqref{geo:eq:well-defined_probs_mdp} for which $p$ was chosen needs to be \emph{monotone strictly increasing} in $p$. 
\end{enumerate}
The first and the third property are satisfied. Regarding the second property, the objective function is trivially monotonically decreasing in $p$ as the derivative of $p_{\sinit}$ with respect to $p$ is zero. The problem is \eqref{geo:eq:probcomputation_mdp}, as there will be a constraint where $p$ occurs in the numerator, rendering it monotonically increasing in $p$. Our solution is as follows: Introduce a fresh variable $p'$ which substitutes $p$ in each constraint generated by \eqref{geo:eq:probcomputation_mdp}, making the constraints (trivially) monotonically decreasing in $p$.
	Now we need to set $p$ and $p'$ into relation. Consider two programs generated either adding the constraint $p\leq p'$ or $p'\leq p$. Depending on wether $p_s$ is monoton in $p$, we can use these constraints to judge if the computed optimal probability is an upper or a lower bound on the original optimal solution. 

\subsection{Markov chains---geometric program.}
Let $\dtmc$ be a pMC. 
 We introduce the following \emph{variables}:
		\begin{itemize}
			\item $p_s\in[0,1]$ for each $s\in S$ will be assigned the probability of reaching $T\subseteq S$.
		\end{itemize}
	 	The direct adaption of the NLP in Equations~\ref{eq:min}--\ref{eq:well-defined_probs} as a geometric program reads as follows:
		\begin{align}
			\text{ minimize } &\quad \nicefrac{1}{p_{\sinit}}\\
			\text{such that}\\
%							 &\quad p_{\sinit}\leq \lambda\label{eq:strategyah:lambda}\\
			\forall s\in T.	 &\quad p_s=1\label{eq:targetprob_geo}\\
			\forall s\in S\setminus T.	&\quad  \sum_{s'\in S}	\frac{P(s,s')\cdot p_{s'}} {p_s} \leq 1\quad \left(\Leftrightarrow p_s \geq \sum_{s'\in S} P(s,s')\cdot p_{s'}\right) \label{eq:probcomputation_geo}\\
			\forall s\in S.	 &\quad \sum_{s'\in S}P(s,s')\leq 1\label{eq:well-defined_probs_geo}
		\end{align}
%
First, observe two relaxations we had to allow. First, Equation~\ref{eq:probcomputation_geo} states that $p_s \geq \sum_{s'\in S} P(s,s')\cdot p_{s'}$ for all $s\in S$, \ie, the assignment of $p_s$ is \emph{lower-bounded} by the actual probability of reaching the target states. Minimizing $p_{\sinit}$ in the objective shall push the assignment to this lower bound in order to satisfy the inequation with equality. 

Secondly, in Equation~\ref{eq:well-defined_probs_geo}, the sum of all probabilities at each state is now only upper bounded by one instead of being exactly one. This is problematic, because it might be beneficial for the objective to assign parameter values that induce sub-stochastic distributions. The first approach to remedy this is to adapt the objective function towards also maximizing the sum of all occurring parameters:
\begin{align}
			\text{ minimize } &\quad \frac{1}{p_{\sinit}} + \sum_{p\in\Var}\frac{1}{p}\label{eq:min_geo_relax1}
\end{align}

Consider the following example pMC with parameters $p$ and $\overline p = 1-p$:
\begin{figure*}
	\centering\input{pics/nonmonotonic_dtmc}	
\end{figure*}
%

We know that the probability of reaching $s_3$ from $s_0$ is given by the polynomial $f_{03}(p)=p^2-p^3$. The maximal value is at $p=\nicefrac{2}{3}$ inducing a probability of $0.1481$. State $s_4$ has probability $0$ to reach $s_3$ (\textit{Prob0}-state). It is therefore excluded, indicated by the gray outlines.

The geometric program determining this maximal reachability probability has variables $p,\overline p, p_0,p_1,p_2,p_3$ and reads as follows:
\begin{align}
			\text{ minimize } &\quad \nicefrac{1}{p_{0}} + \nicefrac{1}{p} + \nicefrac{1}{\overline p}\\
			\text{such that}\\
			&\quad p_{s_3}=1\\
			&\quad  \nicefrac{p\cdot p_{s_1}} {p_0} \leq 1\\
			&\quad  \nicefrac{\overline p\cdot p_{s_2}} {p_1} \leq 1\\
			&\quad  \nicefrac{p\cdot p_{s_3}} {p_2} \leq 1\\
			&\quad p+\overline p\leq 1\label{eq:well-defined_probs_geo}
		\end{align}

%\nj{now comes a summarizing discussion on the outcomes of this example.}

Encoding this in \tool{CVXOPT} yields an assignment $\nu$ of variables that induces sub-stochastic distributions, \ie values $\nu(p)$ and $\nu(\overline p)$ such that $\nu(p)+\nu(\overline p)<1$. Simply scaling them such that they sum up to one, \ie $\nu'(p)=\nicefrac{\nu(p)}{\nu(p)+\nu(\overline p)}$ and $\nu'(\overline p)=\nicefrac{\nu(\overline p)}{\nu(p)+\nu(\overline p)}$ induces a strictly smaller objective value. 

\nj{We can't explain this effect yet. I think we need a sellable argument in order to motivate why any iterative method on improving the objective value is needed.}

\paragraph{Possible Remedies.}
Adding scaling factors to the summands in the objective function helps improving the objective value:
\begin{align}
			\text{ minimize } &\quad \nicefrac{1}{p_{\sinit}} + \epsilon_1\cdot\sum_{p\in\Var}\frac{\epsilon_p}{p}\label{eq:min_geo_relax1}
\end{align}
for $\epsilon_1\in\R$ and $\epsilon_p\in\R$ for all $p\in\Var$. The factor $\epsilon$ favors maximizing the sum of parameters. This help to a certain extend getting stochastic solutions, \ie, satisfying Constraints~\ref{eq:well-defined_probs_geo} with equality. The individual factors $\epsilon_p$ for each parameter $p$ help in getting the probability $p_{\sinit}$ closer to the actual optimum.

Our experiments with the example above indicated, that there is a minimal value for $\epsilon_1$ which still induces stochastic distributions and has the highest value for $p_{\sinit}$. Then, favoring certain parameters by increasing the $\epsilon_p$-values drives the solution even closer to the optimum. 

For the example, we have:
\begin{align}
			\text{ minimize } &\quad \nicefrac{1}{p_{0}} + \epsilon_1\cdot \nicefrac{\epsilon_p}{p} + \nicefrac{1}{\overline p}
\end{align}
for an $\epsilon_1$ that induces stochastic distributions and $\epsilon_p=1$. The corresponding assignment is called $\nu^*$ and we observe that $\nu^*(p)>\nu^*(\overline p)$ which means that $p$ is ``favored'' over $\overline p$. We take the value $\nu^*(p)$ and set $\epsilon_2=\nicefrac{\nu^*(p)}{1-\nu^*(p)}$ and reiterate this procedure. In general that means that we strengthen the favoring of certain parameters. In our example we were able to drive the solution closer to the optimum using this iterative heuristic. Note that at some point of the iteration, a smaller value for $\epsilon_1$ still induced stochastic distributions, indicating that a two stage iteration is possible here.


\section{Computing local maximum using geometric programming}

Note: This explanation is written for Case A and C. As the objective for Case B involves minimizing the probability, it is tricky to find a local maximum point for this case.

\lmc{I guees we need to figure out a way to compute the local maximum for case B.}

For case A, the NLP problem given by Equations~\ref{eq:min}-\ref{eq:well-defined_probs} is non-convex due to constraints in Equation~\ref{eq:well-defined_probs}. To find a local solution for these cases we compute the best local monomial approximation for the posynomial term in Equation~\ref{eq:well-defined_probs} near the initial guess $x(k)$ with following, where $x(k)$ is the vector that has the variable assignments from initial guess for previous iteration:

\begin{align*}
f(w)\approx f(x(k))\prod_{i=1}^{m}\Bigg(\dfrac{w_{i}}{x(k)_{i}}\Bigg)^{a\textsl{i}},
\end{align*}

\noindent where

\begin{align*}
a_{i}=\dfrac{x(k)_{i}}{f(x(k)}\dfrac{\partial f}{\partial x_{i}}
\end{align*}

which is evaluated at $x(k)$. The approximation would be valid for $w \approx x$ and the right-hand side is the best local monomial approximation of $f$ near $x$.

After computing $f(w)$, we solve the following problem:


		\begin{align}
			\text{maximize } &\quad p_{\sinit}\\
			\text{such that}\\
%							 &\quad p_{\sinit}\leq \lambda\label{eq:strategyah:lambda}\\
			\forall s\in T.	 &\quad p_s=1\\
			\forall s\in S\setminus T.	&\quad p_s = \sum_{s'\in S}	P(s,s')\cdot p_{s'}\\
			\forall s\in S.	 &\quad f(w_{s})=1\\
			&\quad (1/k)x(k)_{i}\leq x_{i} \leq kx(k)_{i}, i=1,2,...,n
		\end{align}
		
\noindent where $k > 1$ is the value that determines the trust region of the approximation, $f(w_{s})$ is the best local monomial approximation of $\sum_{s'\in S}P(s,s')$ in state $s$ and $n$ is the number of variables. Generally, we set $k \approx 1$ to make sure that the error of the approximation is small.

As this problem is a GP, we could solve this problem efficiently and the solution of this problem would be taken for the next iteration and we repeat this procedure until convergence to a local maximum. The procedure would be similar in Case C, as we would compute the local momomial approximation of the posynomials in Equations~\ref{eq:welldefined_sched_rand} and \ref{eq:well-defined_probs_rand}. Then, we would iteratively compute the next guess until we converge to a local maximum.

