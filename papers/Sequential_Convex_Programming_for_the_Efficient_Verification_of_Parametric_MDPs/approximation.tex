\section{Sequential Geometric Programming}\label{sec:approximation}

We showed how to efficiently obtain a feasible solution
% of the parameter and scheduler variables 
 for Problem~\ref{prob:pmdpsyn} by solving GP~\eqref{eq:min_rand_geofull}--\eqref{eq:rewcomputation_geofull}. %In some applications, finding a feasible solution for a pMDP synthesis problem itself is useful. 
%In some applications, this is itself a useful result which enables the solution of problems involving thousands of states and parameters. %
%The ability to efficiently obtain such a feasible solution is useful by itself. %, due to the good scaling with respect to the number of parameters in the pMDP. 
%In particular, the good scaling with the number of parameters in $V$ cannot be understated. (For example, existing parameter synthesis tools can only handle thousands of states with tens of parameters. With the GP~\eqref{eq:min_rand_geofull}--\eqref{eq:rewcomputation_geofull}, we can handle thousands of states with thousands of parameters.)
%However, we are further interested in obtaining optimal solutions to Problem~\ref{prob:pmdpsyn}. In general, an SGP cannot be solved for the optimal solution efficiently. Moreover, the GP~\eqref{eq:min_rand_geofull}--\eqref{eq:rewcomputation_geofull} obtained by convexifying the SGP cannot be used to compute an optimal solution.
%Solving an SGP to obtain the global optimal solution is not efficient in general, however we can 
%However, an optimal solution to the GP~\eqref{eq:min_rand_geofull}--\eqref{eq:rewcomputation_geofull} does not necessarily optimize the original objective $f$ in Problem~\ref{prob:pmdpsyn}. 
%While the general SGP cannot be efficiently solved, we can still do well with respect to the original objective $f$. 
%
%However, this solution does not necessarily optimize the original objective $f$ in Problem~\ref{prob:pmdpsyn}. 
We propose a \emph{sequential convex programming} trust-region method to compute a local optimum of the SGP~\eqref{eq:min_rand}--\eqref{eq:rewcomputation}, following~\cite[\S{}9.1]{boyd2007tutorial}, solving a sequence of GPs. We obtain each GP by replacing signomial functions in equality constraints of the SGP~\eqref{eq:min_rand}--\eqref{eq:rewcomputation} with \emph{monomial approximations} of the functions. 



%The local optimum of the SGP provides a local optimal solution to Problem~\ref{prob:pmdpsyn}. Note that in some applications, a feasible solution to Problem~\ref{prob:pmdpsyn} is itself useful.
%
%In this section we observe that our original problem actually forms  a \emph{signomial program} (SGP), for which one can compute \emph{locally optimal solutions} efficiently~\cite{boyd2007tutorial}. 
%
%
%\subsection{Signomial programming}
%
%We identify a subclass of NLPs called \emph{signomial programs} and explain certain useful properties. Then we show how by a simple insight the NLP \eqref{eq:min_rand}--~\eqref{eq:well-defined_probs_rand} is transformed to such a program.
%Note: This explanation is written for Case A and C. As the objective for Case B involves minimizing the probability, it is tricky to find a local maximum point for this case.
%We return to the SGP~\eqref{eq:min_rand}--\eqref{eq:rewcomputation}. To find a locally optimal solution, we compute the best local monomial approximation for each non-monomial equality constraint. We enforce the approximation over a trust region. We then replace each non-monomial equality constraint with its monomial approximation, and solve a local trust-region GP.
%The monomial approximations are defined using a valuation of the variables of the SGP, and the approximation is only valid for valuations which belong to a region known as the trust-region. We describe the process of 
%
%
%The solution of each geometric program in the sequence is used to obtain a feasible solution to the SGP, which in turn is used to obtain the next geometric program in the sequence. The sequence of solutions of the geometric programs so obtained yields a sequence of feasible solutions to Problem~\ref{prob:pmdpsyn}, and the latter sequence converges to a locally optimal solution of Problem~\ref{prob:pmdpsyn}.
%
%
%that in turn yield a sequence of decreasing values of the objective $f$ in \eqref{eq:min_rand}. The solution sequence will eventually converge to a locally optimal solution of Problem~\ref{prob:pmdpsyn}. 
%Each successive member of the sequence, obtained by solving a geometric program, yields a feasible solution to Problem~\ref{prob:pmdpsyn} with a smaller value of the objective $f$ in \eqref{eq:min_rand}. 
%A valuation of the SGP variables Solving the geometric program 
%The monomial approximations are defined using a valuation of the variables of the SGP, and the approximation is only valid for valuations which belong to a region known as the trust-region. Each valuation obtained as a solution to the GP serves as  
%
\begin{definition}[Monomial approximation]
Given a posynomial $f \in \signomis[\Var]$, variables $\Var = \{x_1,\dots,x_n\}$, and a valuation $u \in \valuations^\Var$, a \emph{monomial approximation $\hat f\in\monos[\Var]$ for $f$ near $u$} is 
\begin{align*}
	\forall i. 1 \leq i \leq n
	\quad
	\hat f = f[u]
	\prod_{i=1}^{n}\Bigg(\dfrac{x_{i}}{u(x_i)}\Bigg)^{a_{i}},
	\quad
	\text{where }
	a_{i}=\dfrac{u(x_i)}{f[u]}\dfrac{\partial f}{\partial x_{i}}[u].
\end{align*}	
\end{definition}
Intuitively, we compute a \emph{linearization} $\hat f$ of $f\in\signomis[\Var]$ around a fixed valuation $u$.
%
%
%\paragraph{Trust region.}
We enforce the fidelity of monomial approximation $\hat{f}$ of $f \in \signomis[\Var]$ by restricting valuations to remain within a set known as \emph{trust region}. We define the following constraints on the variables $\Var$ with $t > 1$ determining the size of the trust region:
\begin{align}
	\label{eq:trust_region}
	\forall i. 1 \leq i \leq n
	\quad (\nicefrac{1}{t})\cdot u(x_i) \leq x_i \leq t\cdot u(x_i)
\end{align}
% 
%
%
%%Generally, we want $t$ to have a value of the form $1 + \epsilon$, where $0 < \epsilon \ll 1$, to make sure that the error of
%%the approximation is small.
%%$f(w_{s})$ is the best local monomial approximation of $\sum_{s'\in S}P(s,s')$ in state $s$ and $n$ is the number of variables. 
%
%
%
For a given valuation $u$, we approximate the SGP~\eqref{eq:min_rand}--\eqref{eq:rewcomputation} to obtain a \emph{local GP} as follows. %First, we replace the objective function of the SGP by its monomial approximation near $u$.
First, we apply a \emph{lifting} procedure (Section~\ref{sec:lifting}) to the SGP ensuring that all constraints consist of posynomial functions. The thus obtained posynomial inequality constraints are included in the local GP.
After replacing posynomials in every equality constraint by their monomial approximations near $u$, the resulting monomial equality constraints are also included.
Finally, we add trust region constraints~\eqref{eq:trust_region} for scheduler and parameter variables. The objective function is the same as for the SGP.
The optimal solution of the local GP is not necessarily a feasible solution to the SGP. 
Therefore, we first normalize the scheduler and parameter values to obtain well-defined probability distributions. These normalized values are used to compute precise probabilities and expected cost using \tool{PRISM}. The steps above provide a feasible solution of the SGP.

We use such approximations to obtain a sequence of feasible solutions to the SGP approaching a local optimum of the SGP. 
First, we compute a feasible solution $u^{(0)}$ for Problem~\ref{prob:pmdpsyn} (Section \ref{sec:geometric}), 
%These values form a feasible solution $u^{(0)}$ to SGP~\eqref{eq:min_rand}--\eqref{eq:rewcomputation} which is the 
%
forming the initial point of a sequence of solutions $u^{(0)},\ldots, u^{(N)}, N \in \N$. 
The solution $u^{(k)}$ for $0\leq k\leq N$ is obtained from a local GP defined using $u^{(k-1)}$ as explained above.

%Using solution $u^{(k)}, we obtain a local GP by locally approximating SGP~\eqref{eq:min_rand}--\eqref{eq:rewcomputation} near $u^{(k)}$. 

%The optimal solution of this GP does not necessarily yield a feasible solution to the SGP. 
%We normalize the scheduler and parameter values for the optimal solution to obtain well-defined probability distributions. These normalized values are then used to compute reachability probabilities and expected cost values as was done for $u^{(1)}$. In this way, we obtain $u^{(k+1)}$ ( which is a feasible solution of SGP~\eqref{eq:min_rand}--\eqref{eq:rewcomputation}) from $w^{(k)}$. %
%Once gain, \tool{PRISM} is used to obtain $u^{(k+1)}$ from $w^{(k)}$.

The parameter $t$ for each iteration $k$ is determined based on its value for the previous iteration, and the ratio of $ f\left[u^{(k-1)}\right] $ to $f\left[u^{(k-2)}\right]$, where $f$ is the objective function in \eqref{eq:min_rand}. The iterations are stopped when $\left| f\left[u^{(k)}\right] - f\left[u^{(k-1)}\right] \right| <  \epsilon$. Intuitively, $\epsilon$ defines the required improvement on the objective value for each iteration; once there is not enough improvement the process terminates.
%%The trust region must get smaller with the iteration count $k$, because otherwise there is no guarantee that the local method will converge without oscillations. In practice, any rule for which $t\to 1^+$ as $k\to\infty$ will work. We define $t$ at iteration $k$ as $t = 1 + 1 / k$. 
%When $?$ holds, the sequential optimization is ended and $?$ is taken as the locally optimal solution. 
%\hp{Formal statement that the sequence reaches a locally optimal solution.}

%As we will see in the next section, this local method allows us to improve the objective value from the initial valuation and scheduler by a factor of $100$ or more.
%\ip{check this number}
%We denote the valuation of the variables $\Var$ obtained as a feasible valuation  of the SGP~\eqref{eq:min_rand}--\eqref{eq:rewcomputation} at iteration $k$ by $u^{(k)} \in \valuations^\Var$. The valuation $u^{(k)}$ contains scheduler variable assignments $(\sigma^{s,\act})^{(k)}$, transition probabilities $\probmdp(s,\act,s')^{(k)}$, reachability probabilities $p_s^{(k)}$, and costs $c_s^{(k)}$. \hp{The valuation should contain a `sub-valuation' of the variables \dots}

%We locally approximate the SGP~\eqref{eq:min_rand}--\eqref{eq:rewcomputation} near a valuation $u^{(k)}$ at iteration $k$ to obtain a trust-region GP denoted by $\text{GP}(k)$ over the variables $\Var$. The procedure to obtain $\text{GP}(k)$ consists of replacing every equality constraint of the SGP~\eqref{eq:min_rand}--\eqref{eq:rewcomputation} by its monomial approximation around $u^{(k)}$. The objective function of the SGP is also replaced by its monomial approximation near $u^{(k)}$. %With this approximation in place, the SGP~\eqref{eq:min_rand}--\eqref{eq:rewcomputation} becomes $\text{GP}(k)$. 
%We solve $\text{GP}(k)$ to obtain a valuation denoted by $w^{(k)}$.
%Note that the trust-region GPs obtained here are different from the GP~\eqref{eq:min_rand_geofull}--\eqref{eq:rewcomputation_geofull}. The trust-region GPs provide (a sequence of) local solutions to the SGP~\eqref{eq:min_rand}--\eqref{eq:rewcomputation} which converge to a locally optimal solution of Problem~\ref{prob:pmdpsyn}. In contrast, GP~\eqref{eq:min_rand_geofull}--\eqref{eq:rewcomputation_geofull} provides feasible solutions to Problem~\ref{prob:pmdpsyn}.% In fact, the main use of GP~\eqref{eq:min_rand_geofull}--\eqref{eq:rewcomputation_geofull} is to provide an initial point for the sequence 

% \paragraph{Sequential convex optimization.}
% We then repeat the process.
% The trust region must get smaller with the iteration count $k$, because
% otherwise there is no guarantee that the local method will converge without
% oscillations. In practice, any rule for which $t\to 1^+$ as $k\to\infty$ will
% work. 
% 
% As we will see in the next section, this local method allows us to
% improve the objective value from the initial valuation and scheduler by a
% factor of $100$ or more.
% \ip{check this number}


%for the posynomial term in Equation~\ref{eq:well-defined_probs} near the initial guess $x(k)$ with following, where $x(k)$ is the vector that has the variable assignments from initial guess for previous iteration:

%After computing $f(w)$, we solve the following problem:
%\begin{align}
%	\text{maximize } &\quad p_{\sinit}\\
%	\text{such that}\\
%%		 &\quad p_{\sinit}\leq \lambda\label{eq:strategyah:lambda}\\
%	\forall s\in T.	 &\quad p_s=1\\
%	\forall s\in S\setminus T.	&\quad p_s = \sum_{s'\in S}	P(s,s')\cdot p_{s'}\\
%	\forall s\in S.	 &\quad f(w_{s})=1\\
%	&\quad (1/k)x(k)_{i}\leq x_{i} \leq kx(k)_{i}, i=1,2,...,n
%\end{align}
%		

% \begin{tabbing}
%     \rule{\linewidth}{0.4pt}\\
%     \textbf{algorithm:} sequential convex programming\\
%     \textbf{given}: pMPD Problem~\ref{prob:pmdpsyn} \\
%     \textbf{output}: better valuation $u$ and scheduler $\sched$.\\
%     \textbf{begin} \\
%         \quad
% 		\= 1. \= Solve GP~\eqref{eq:min_rand_geofull}--\eqref{eq:rewcomputation_geofull}
% 				to obtain initial valuation and scheduler\\
% 		\> 2. set $k:=0$\\
%         \> 3. \> \textbf{repeat}\\
%         \> \> \quad \= approximate around
% 			$((\sigma^{s,\act})^{(k)}, \probmdp(s,\act,s')^{(k)}, p_s^{(k)}, c_s^{(k)})$\\
% 		\> \> \> solve $\text{GP}(k)$\\
% 		\> \> \> perform updates\\
%         \> \> \textbf{until} convergence criterion\\
%     \textbf{end}\\
%     \rule{\linewidth}{0.4pt}
% \end{tabbing}


%As this problem is a GP, we could solve this problem efficiently and the solution of this problem would be taken for the next iteration and we repeat this procedure until convergence to a local maximum. The procedure would be similar in Case C, as we would compute the local momomial approximation of the posynomials in Equations~\ref{eq:welldefined_sched_rand} and \ref{eq:well-defined_probs_rand}. Then, we would iteratively compute the next guess until we converge to a local maximum.
%
