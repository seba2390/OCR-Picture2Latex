% This must be in the first 5 lines to tell arXiv to use pdfLaTeX, which is strongly recommended.
\pdfoutput=1
% In particular, the hyperref package requires pdfLaTeX in order to break URLs across lines.

\documentclass[11pt]{article}

% Remove the "review" option to generate the final version.
\usepackage{EMNLP2022}
\usepackage{subcaption}
\usepackage{float}
% Standard package includes
\usepackage{times}
\usepackage{latexsym}
\usepackage{arabtex}
\usepackage{utf8}
\setcode{utf8}
\usepackage{makecell}
\usepackage{comment}
\usepackage{graphicx}

% For proper rendering and hyphenation of words containing Latin characters (including in bib files)
\usepackage[T1]{fontenc}
% For Vietnamese characters
% \usepackage[T5]{fontenc}
% See https://www.latex-project.org/help/documentation/encguide.pdf for other character sets

% This assumes your files are encoded as UTF8
\usepackage[utf8]{inputenc}

% This is not strictly necessary, and may be commented out.
% However, it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}

% This is also not strictly necessary, and may be commented out.
% However, it will improve the aesthetics of text in
% the typewriter font.
\usepackage{inconsolata}
\usepackage{arabtex}  % caption must be loaded before arabtex


% If the title and author information does not fit in the area allocated, uncomment the following
%
%\setlength\titlebox{<dim>}
%
% and set <dim> to something 5cm or larger.

\title{Detecting Suicidality in Arabic Tweets Using Machine Learning Techniques}






% Author information can be set in various styles:
% For several authors from the same institution:
% \author{Author 1 \and ... \and Author n \\
%         Address line \\ ... \\ Address line}
% if the names do not fit well on one line use
%         Author 1 \\ {\bf Author 2} \\ ... \\ {\bf Author n} \\
% For authors from different institutions:
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \And  ... \And
%         Author n \\ Address line \\ ... \\ Address line}
% To start a seperate ``row'' of authors use \AND, as in
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \AND
%         Author 2 \\ Address line \\ ... \\ Address line \And
%         Author 3 \\ Address line \\ ... \\ Address line}

%\author{Asma Abdulsalam \\
%  Department of Computer Science\\King AbdulAziz University \\Jeddah,Saudi Arabia \\
  %Affiliation / Address line 2 \\
  %Affiliation / Address line 3 \\
%  \texttt{aabdulsalam0012@stu.kau.edu.sa} \\\And
%  Areej Alhothali \\
% Department of Computer Science\\King AbdulAziz University \\Jeddah,Saudi Arabia \\
  %Affiliation / Address line 2 \\
  %Affiliation / Address line 3 \\
%  \texttt{aalhothali@kau.edu.sa} \\}
  
 % Saleh Yahya  Al-Ghamdi
%درجة الدكتوراة من علم النفس, التربية, جامعة ام القرى, مكة المكرمة, المملكة العربية السعودية
%syalghamdi@kau.edu.sa
%https://www.kau.edu.sa/CVEn.aspx?Site_ID=0016592&Lng=AR
\begin{document}
\maketitle

\begin{abstract}
Social media platforms have transformed traditional communication methods by allowing users worldwide to communicate instantly, openly, and frequently. People use social media to express their opinion and share their personal stories and struggles. Negative feelings that express hardship, thoughts of death, and self-harm are widespread in social media, especially among young generations. Therefore, using social media to detect and identify suicidal ideation will help provide proper intervention that will eventually dissuade others from self-harming and committing suicide and prevent the spread of suicidal ideations on social media. In this research,  AraBert (a deep-learning model) is used to automatically detect suicidal thoughts based on textual features. The model was trained and tested on a newly collected and annotated dataset of of Arabic tweets. The proposed model is compared with the most popular machine learning, e.g., Naïve Bayes, Support vector machine, K-Nearest Neighbor, Random forest, and  XGBoost algorithms, along with various word representations. 
The model aims to accurately detect suicide ideation on Twitter and identify it from neutral posts. To the best of our knowledge, this is the first research focuses on detecting suicidality in Arabic tweets.
\end{abstract}
%\begin{IEEEkeywords}
%Suicidal, twitter, supervised learning,machine learning
%\end{IEEEkeywords}



\section{Introduction}
Millions of individuals regularly use social media such as chat rooms, blogging websites, and social networking platforms, with $3.96$ billion people actively utilizing the internet~\cite{1astoveza2018suicidal}. Facebook, Twitter, Snapchat, and other social media networking sites allow users to share information and interact with others. Twitter is a free social media broadcast site that allows registered users to communicate with others through a 140 characters message called "tweets." This popular platform allows users to say or express whatever they want, whether positive or negative. Many users prefer to utilize social media networks to share their thoughts, emotions, their daily experiences, problems, and issues~\cite{15n10.1145/2858036.2858207}. Suicidal ideation, death, and self-harming thoughts are among the most widely discussed topics on social media.

Suicide is described as a person's deliberate attempt to take their own life~\cite{2nock2008suicide}. Suicide is a multifaceted occurrence that results from a complex interaction of biological, psychological, social, cultural, and spiritual variables~\cite{3beck1979assessment}. Suicide is a manifestation of underlying suffering caused by a mix of events, including underlying mental diseases that generate psychological pain~\cite{4liu2020suicidal}. Suicide ideation, suicide planning, and suicide attempts are three types of suicidal behavior~\cite{2nock2008suicide,3beck1979assessment,4liu2020suicidal}. Suicide ideation refers to a person's ideas or intentions to end their life without actually trying to do so. In contrast, a suicide plan is a specific technique a person can use to end their life, and a suicide attempt is an act of self-harm that results in death with the intended purpose being to die~\cite{2nock2008suicide,3beck1979assessment,4liu2020suicidal}.

Suicide has ramifications for people, families, communities, and even countries~\cite{4liu2020suicidal}. Suicide is the second largest cause of mortality among young people, killing more people than diabetes, liver disease, stroke, or infection~\cite{5weber2017psychiatric}. More than 40\% of individuals who seek primary care are reluctant to address their depressive symptoms because of the stigma associated with mental disorders. Suicidal thoughts and acts necessitate quick intervention, and there is no reliable approach for managing, assessing, or preventing suicide~\cite{5weber2017psychiatric}. 
Traditional suicide ideation detection approaches rely on the knowledge of psychologists and self-reported questionnaires~\cite{4liu2020suicidal}. Patient Health Questionaire-9 (PHQ-9) and Columbia Suicide Severity Rating Scale (C-SSRS) are two examples of public forum questionnaires that can screen for suicide and identify depressive symptoms~\cite{5weber2017psychiatric}. These approaches are effective and quick, but they are subject to false negatives due to participant concealment. They are also difficult to carry out over a lengthy period or on a very large scale~\cite{5weber2017psychiatric}.

The task of identifying suicidality has attracted researchers in different fields to investigate linguistic and psychological signs and other factors that aid in diagnosing and identifying individuals with suicidal thoughts~\cite{4liu2020suicidal}.
Social media posts provide a valuable source of information about individuals' lives and their emotional and psychological states.
For various reasons, many individuals are unable to share their personal stories and express their emotions in real life and instead choose to write blogs about their feelings or suicide plans. Unfortunately, these suicide posts are often either overlooked or ignored. This information can help to perform screening of suicidality on a wide scale.


%%%%%%7/29
Social media has been used as a powerful tool to determine the psychological states of its users~\cite{15n10.1145/2858036.2858207}. A huge number of Twitter content is written in the Arabic language, and millions of its users are from Arabic countries. A total of 15 million Twitter users are from Saudi Arabia alone, as reported by Statista in 2020~\cite{16nhttps://doi.org/10.48550/arxiv.2004.04315}. The Arabic language is the predominant language of 422 million people in over 27 nations~\cite{17nBOUDAD20182479}.

The  Arabic language has many vocabularies, and it has different varieties: Classical Arabic, Modern Standard Arabic (MSA), and dialects or colloquial~\cite{17nBOUDAD20182479,18n10.1145/1644879.1644881}. The language of the Quran refers to it as classical Arabic. MSA is mostly used in formal speech and writing, while the dialectical or colloquial often varies from one country to another and used between the public for communication~\cite{17nBOUDAD20182479}. Arabic language variety may have some properties in common, but each has its own lexicon, grammar, and morphology~\cite{18n10.1145/1644879.1644881}. Morphology and orthography of Arabic language text are the main difficulties facing NLP researchers~\cite{17nBOUDAD20182479,19n7945623}. Most MSA text lack orthographic representation for short vowel letters as it is replaced with diacritical marks. MSA is written without diacritical marks, which affect words’ meaning~\cite{17nBOUDAD20182479,18n10.1145/1644879.1644881}. Morphology plays an important role in the Arabic language because it is a derivational language and highly structured~\cite{18n10.1145/1644879.1644881}. The Arabic word may differ based on the morphological such as root, stem, part of speech (POS), and affix\cite{17nBOUDAD20182479}. Even letters in the Arabic language have different shapes based on their position in the word\cite{18n10.1145/1644879.1644881}. 
%%%%%%

%%%%%%%%%%%this paper
This paper proposes an Arabic suicide ideation detection framework to analyze tweets with various features and explore the possibility of monitoring suicidal behaviors. We have developed the first Arabic suicidality detection dataset that consisted of 5719 Arabic tweets. 47292 tweets were gathered in the interval between 23 August 2021 to 21 April 2022. 5720 tweets left after deleting duplicated tweets.%Different Arabic dialect were use. %e.g.,~\<عايز اموت , ابى اموت , بدي موت>Therefore, 
The data were gathered from different Arabic countries using different Arabic dialects. 1429 were labeled as Suicidal tweets and 4290 were Non-Suicidal. The labeling process was done by a physiologist expert in the field.
There is a lack of formal suicidal research on Arabic social media. Therefore, developing machine learning techniques to extract suitable information about suicide ideation in Arabic languages will help provide an accurate and effective mechanism to conduct a large screening through social media platforms with an aim to prevent suicidal behaviors. To the best of our knowledge, this is the first Arabic research done on Arabic suicidal dataset. In this study, a deep learning based approach was developed to detect suicidal content.

 %more emphasis on the contribution of the paper .. add summary of the contribution... we have developed the first Arabic suicidality detection dataset that consisted of .. the dataset was annotated by physiologist  expert in the field .. a deep learning based approach was developed in this study to detect suicidal content.

\section{Related Works}
~\label{Sec:related works} 

Today with the large and growing numbers of active users on social media, many users use their social media accounts to share and write on a daily basis. This trend of using social media as a modern-day diary can help reveal and show part of users’ personalities and mental states. This section presents a summary of the latest research on suicidal detection with the main focus on text classification.

%%%%%%%%%%%%%%%%%% Machine learing approaches 
%1
Supervised learning machine learning model was used by several researchers in the field for suicidal ideation detection. \cite {9ji2018supervised} used different classifiers from both traditional and
deep learning perspectives to detect online suicidal users through their online content such as support vector
machine (SVM), Random Forest (RRF), gradient boost classification tree (GBDT), XGBoost, multilayer feed forward neural net (MLFFNN), and long short-term memory (LSTM). Several sets of features were extracted, including statistical, linguistic, syntactic, topic features, and word embedding. Combining different type of features such as Statistics, topic, TF-IDF, Part of Speach (POS) and LIWC increases models’ performance. \cite{11o2015detecting} examined the possibility of determining the level of concern from Twitter post content. They developed two text classifiers using machine learning algorithms, which are logistic regression(LR) and SVM. The "freq, tfidf, and filter" variations of the feature space were used to test these techniques. TF-IDF with filter means removing any terms that appeared more than a certain number of times in the document. While "freq" means no filtering. The results show that SVMs with TFIDF no-filter had the highest accuracy. In \cite{15moulahi2017dare}, a probabilistic framework based on Conditional Random Fields (CRF) was developed to track suicidal ideation. It has been noticed that the CRF model outperformed other machine learning methods, such as  SVM, NB, J48, RF, and multilayer perceptron. \citet{13burnap2015machine} incorporated SVM and NB as an ensemble approach known as Rotation Forest (RF). They tested the RF approach with three classifiers SVM, NB, J48 and with only SVM and NB. The experiment showed a better performance when SVM and NB were integrated. \cite{31new7307052} used a real-time system to detect suicidal ideation using machine learning and a psychological lexicon dictionary. They identified 53 users who posted suicidal content on Weibo before their deaths. Different classifiers were used, including SVM, naïve Bayes, logistic regression, J48 classifier, random forest, and Sequential minimal optimization(SMO) with three N-gram features and psychological lexicon dictionary. The SVM classifier had the best performance compared to the other classifiers. %, with an F1-measure of 68.3%, a precision of 78.9%, a recall of 60.3%, and an accuracy of over 94%. 
This encouraging result shows the possibility of creating a suicide detection system that can help psychologists with suicidal detection and will be a great tool for suicide prevention. 

Other studies have utilized lexicon or dictionary based features to identify suicidal content.
\citet{26new10.1007/978-3-030-37429-7_17} proposed an approach to identify suicidal content in the Sina Weibo microblogging platform. Three extracted linguistic feature sets, namely, automated machine learning dictionary, Chinese suicide dictionary, and Simplified Chinese Micro-Blog Word Count(SCMBWC), were used to detect suicide ideation. These three feature sets were employed separately with the SVM, DT, and LR algorithms to get six different detection results. % using a logistic regression model. %More than 65,352 messages were crawled from Sina Weibo, only 8,548 blogs were labeled as suicide blogs. Two machine learning algorithms were used (SVM, DT) to build a classification model with three features sets (automated machine learning dictionary, Chinese suicide dictionary, and Simplified Chinese Micro-Blog Word Count(SCMBWC)).  
%Each feature set was used with SVM, DT, and logistic regression algorithms with the three features sets separately to generate six detection results.% Those were input to a logistic regression model. 
It has been found that SVM with a feature set extracted using an automated machine learning dictionary from real blogs data-driven by N-gram gave the highest accuracy. 

Machine classifiers were built to classify suicide-related text on Twitter~\cite{32n10.1145/2700171.2791023}. Twitter text was classified into suicidal intent or other suicide-related topics e.g., campaigning and support, memorial or reporting of suicide. The dataset was collected from twitter after exacting keywords from four known websites identified as a discussion website for suicidal support and prevention. Four million posts were collected using suicidal keywords, in addition to different tweets gathered using the names of reported suicide cases. Structural, lexical, psychological, and emotive used were the most popular classifiers in classifying suicidal topics (SVM, DT, and NB). Also, they incorporated SVM and NB as an ensemble approach known as Rotation Forest (RF). They tested RF approach with the three classifiers and another one with only SVM and NB. The experiment showed a better performance when SVM and NB were integrated.
%The dataset was randomly sampled from both datasets with 800 suicidal tweets, and 200 undirect suicidal ideation tweets.
%Therefore, the DT was dropped from the ensemble approach. RF gained 0.690 for F1-measure, precision performance of 0.644, and recall of 0.744.

Another study~\cite{33n8527039} used four machine classifiers which are DT, NB, RF, and SVM, on the same dataset~\cite{32n10.1145/2700171.2791023}. The data was divided into two datasets, one set for binary classification (Suicide and Flippant reference to suicide) and another for multi-class classification (Suicide, Flippant, and Non-Suicide) classes. Then, Part of Speech (POS), Bag of Words (BOW) and Inverse Document Frequency (IDF) were used. The results show that DT gave the highest accuracy for the multi-class.
%DT had the best F1-measure of 0. 879 and 0.790 accuracies for multi-class dataset.
A third study~\cite{34n8527032} made on the same dataset~\cite{32n10.1145/2700171.2791023} with the same pre-processing techniques. The aim was to investigate the performance of the Prism algorithm against the standard machine learning algorithms (SVM, DT, NB, and RF) with BOW feature. The results show that Prism algorithm %had 0.84 precision, recall, and F1-measures, which 
gave the best performance in comparison with other classifiers in all measures.
%A total of 2,000 tweets or instances were collected, however, after rigorous pre-processing only 1060 instances were left for the experiment.

%7
%%%%%%%%%%%%%%% lexicon based approches
% move it to the lexicon dictionary based methods
Vader sentiment analysis was used by Rajesh Kumar et al. to give a sentiment score for each word with different classifiers such as NB, RF, XGBoost, and logistic regression. Vader sentiment analysis helped separate the sentence to distinguish the sentences into positive, or neutral. Many features, such as statistical features, tokenization stemming, bag of words (BOW), and word frequency, were employed after preprocessing and data cleaning. They achieved the highest accuracy using the RF method by considering all set of features~\cite{22rajesh2020suicidal}.


%%%%%%%%%%%%%%%%%%5
Supervised Deep learning has been used by several researchers to identify suicidal content. \citet{25na13010007} proposed a model that detection suicidality in Reddit platform. Two model were combined Long Short-Term Memory (LSTM) and Convolutional Neural Network (CNN) to classify Reddit content in one of multiple classes. TF-IDF, BOW and Statistical Features were used with four machine learning (SVM, NB, RF and GXBoosting) and Word2vec was used with two deep learning models (LSTM, CNN). The proposed LSTM-CNN hybrid model outperform other deep and machine learning algorithm used in this study. %improved the overall performance. % with $93.8\%$, F1-score $93.4\%$, recall $94.1\%$ and Precision $93.2\%$.


 

\citet{36n8269767} studied online users’ textual and behavior features were detected and then passing these features to a martingale framework detect the change in the data streams.
%%%%%new LR
~\cite{info:doi/10.2196/34705} investigated the link between soical media content consumption and suicidal behavior. 3202 English tweets were manually annotated using a new classification system that divides tweets about suicide into 12 groups. These included personal stories of either suicidal ideation,  coping and recovery, spread awareness, information related to prevention , reporting of suicide cases, and other irrelevant tweets to these categories. Several machine learning models based on these categories was developed to perform multi-class and binary classification tasks. Majority classifier used (TF-IDF with a linear SVM), and two state-of-the-art deep learning models (BERT, XLNet). Based on prior research, the first task categorized postings into six key content categories that are particularly pertinent for suicide prevention. The second classification challenge used binary classification to distinguish between posts in the 12 categories that refer to actual suicide and those in the off-topic category that utilize terms connected to suicide in a different context or meaning. The two deep learning models performed quite similarly in both tasks and outperformed SVM with TFIDF. With the exception of the suicidal ideas and attempts category ERT and XLNet achieved highest accuracy. Bert model had the greatest overall scores in binary classification.

%(F1 = 0.55), BERT and XLNet achieved accuracy scores above 0.73 on average across the six major categories in the test set. BERT, the model with the greatest overall scores, accurately identified 0.88 of tweets in binary classification, with fairly identical performance on the validation and test sets. The test set's F1-scores for off-topic and tweets about suicide were 0.73 and 0.92, respectively.

More than 50,000 tweets were used by~\cite{technologies10030057} to detect suicidal ideation using ML and DL classifiers.% The annotation is done in two steps. First, using python tool TextBlob and VADER to annotate tweets by extracting the sentiment polarity (positive, negative, or neutral). Second, review all tweets manually into two classes (suicidal, Non-suicidal). 
Several DL classifiers were used such as LSTM, Bi-directional LSTM (BiLSTM), Gated Recurrent Unit (GRU), Bi-directional GRU (BiGRU), and combined model of CNN and LSTM (C-LSTM). These DL modules was compared with ML approaches such as Random Forrest (RF), Support Vector classifier (SVC), Stochastic Gradient Descent classifier (SGD), Logistic Regression (LR), and Multinomial Naive Bayes (MNB) classifier. Results show that RF model %, with an accuracy of 0.93 and an F1 score of 0.92, 
reached the highest classification score among machine learning methods. However, word embedding training improves the performance of ML models, with the BiLSTM model achieving higher  precision, F1-score.%, recall and AUC.

~\citet{9753295} developed a DL approach that aims at analyzing online Twitter posts and identifying any characteristics that could indicate individuals' suicidal ideation thoughts. To dveloped the model, a total of 188704 tweets were extracted from 1169 users and manually annotated into two classes, suicide, and non-suicidal posts. A variety of features were extracted, including sentiment analysis, emoticons, TF-IDF, statistics, topic-based features, and temporal features. LDA + Trigram + TF-IDF + Tweet Statistics + Temporal + Emoticons + Sentiment Analysis feature has the best performance for detecting suicidal ideation in comparison with the other feature combinations.% with an accuracy of 0.87, precision of 0.83 and 0.81 F1-score using Logistic Regression classifier.

~\cite{9528252} developed a machine learning to predict whether or not a tweet has suicide ideation. A dataset with 5121 non-suicidal tweets and 3998 tweets was collected and labeled "suicidal" and "non-suicidal." In this research, various machine learning models were used, including SVM, DT, LR, NB, K-NN, and different ensemble models, namely AdaBoost, Gradient Boost, Bagging, CatBoost, XGBoost, and Voting Classifier(VC) were also implemented. The results indicate that the VC has given the best accuracy. It was used as an estimator with three classifiers LR, SVM, and DT, that gave the best result of all other combinations.% with 0.90,0.91, 0.89, 0.90 for accuracy, precision, recall and F1-score respectively.

A dataset of individuals who expressed suicidal thoughts on Twitter was developed by~\cite{9388638}. The dataset consists of 1897 tweets that were gathered using keywords extracted from previous study~\cite{COLOMBO2016291} and various web forums. The annotation was done by a human annotator and a psychiatric expert. Several machine learning techniques were used, including SVM, LR, MNB, BNB, and DT. Three ensemble learning techniques, including Voting Ensemble, AdaBoost, and RF were also used. The results indicate that LR outperforms other models.% with LR outperforms other models with 0.79 accuracy, 0.48 precision and 0.12 recall. 

~\cite{9678419} aimed to improve the machine learning model that uses grey wolves optimizer (GWO) to identify a person contemplating suicide. GWO was combined with a machine learning algorithm to predict Twitter suicidal users. Tweets that mention depression, self-harm, and anxiety were gathered, and only tweets with signs associated with suicide were collected. A total of 193,720 tweets were collected and annotated. Different machine learning models were implemented that include NB, LR, SVM, and DT. The results show that LR-GWO trained on unigram gave the best accuracy in comparison with the other machine learning models.   


~\cite{9966481} created an Arabic Suicide dataset from an English dataset called ASuiglish. ASuiglish was created by combing several English data sources (Chadha uses Twitter provided by~\cite{10.1093/comjnl/bxab060}, Suicide Notes from Kaggle dataset~\cite{mashaly_2020}, Victoria Suicide Data~\cite{mashaly_2020} and Suicidal Phrases from Kaggle dataset~\cite{sonu_2020}). The final dataset consists of 1960 with 980 passages per class. The dataset was then pre-processed and cleaned, and abbreviations from the dataset were deleted. After that, the dataset was first translated into Arabic using Google and Microsoft Translate APIs. Then, each entry was examined manually to ensure accuracy and correctness. The dataset was then vectorized using TF-IDF. Then, fed into the following algorithms: RF, MNB, SVM, and LR. SVM provided a higher accuracy in comparison with the other models. %0.93 accuracy, 0.93 Precision, and 0.92 F1-score. %The resulting dataset was balanced, but due to the heterogeneous sources (Reddit and Twitter) there were lengthier phrases than others. To harmonize segments, manual monitoring was used.

\section{Methodology}
Classifying suicidal-related posts or blogs aims to determine whether the user has a suicidal tendency or not. Thus, suicidal ideation detection in social media content is a typical classification problem of supervised learning. Different machine learning and deep learning techniques have been previously applied to detect suicidality in English and other languages. In this study, we collect, create and annotate an Arabic suicidality dataset. We employ a deep learning model (AraBert) to identify suicidal content and compare the results with standard machine learning approaches.
Figure~\ref{fig:steps} shows the procedure used in this study, which flows most studies discussed in ~\ref{Sec:related works} section~\cite{9ji2018supervised,1astoveza2018suicidal,11o2015detecting}.
Our methodology starts with data collection from Twitter using suicidal keywords extracted from previous research. Then, annotating step involves labeling the datasets after removing duplicated tweets. The fourth step is feature extraction which is applied before employing machine and deep learning models. The fifth step is employing machine and deep learning models. The last step is performance analysis.

%This is a primer work further work is in progress.


%The third step, is feature extraction, is applied before employing machine and deep learning models.


\begin{figure}[h]
   \includegraphics[width=\linewidth]
   {Nsteps3.JPG}
    \caption{Architecture of Suicide Detection Methodology}
  \label{fig:steps}
\end{figure}

\subsection{Data collection}
The first part of this study consists of the data collection process performed using Tweepy. Tweepy is an open-source Python library used to access Twitter API provided by the Twitter developer. An Arabic tweet corpus has been developed from Arabic tweets written in different Arabic dialects e.g.,~\<عايز اموت , ابى اموت , بدي موت>. Between 23 August 2021 and 21 April 2022, 47292 tweets were gathered using Arabic suicidal keywords conducted from previous English research shown in Table~\ref{table1}. As a result, 5720 tweets were obtained after removing duplicated tweet.
%The data collection process is performed using Tweepy. Tweepy is an open-source python library used to access Twitter API provided by the Twitter developer. The collected data were gathered based on suicidal Keywords extracted from previous English research shown in table ~\ref{table1}, and then translated to Arabic language. In the interval between 23 August 2021 to 21 April 2022, 47292 tweets were gathered. After deleting duplicated tweets 5720 tweets were left . Different dialect were use e.g.,~\<عايز اموت , ابى اموت , بدي موت> . Therefore, the data were gathered from different Arabic countries.
%20 Feb to 21 Apr

\begin{figure*}[h!]
    \centering
    \begin{subfigure}[t]{0.45\textwidth}
        \centering
        \includegraphics[width=7cm,height=6cm]{STraninglength.png}
        \caption{\label{WL1} Average Sentence Lengths for Suicidal Tweets}
    \end{subfigure}%
    \begin{subfigure}[t]{0.45\textwidth}
        \centering
        \includegraphics[width=7cm,height=6cm]{NTraninglength.png}
        \caption{Average Sentence Lengths for Non-Suicidal Tweets}
    \end{subfigure}
    \caption{ \label{WL2}Average Sentence Lengths for  Suicidal and Non-Suicidal Tweets}
\end{figure*}




%\begin{figure}\centering
 %  \includegraphics[width=8cm,height=6cm]
  % {STraninglength.png}
   %\caption{\label{WL1}
   %Average Sentence Lengths for Suicidal Tweets}
 %\end{figure}

%\begin{figure}\centering
%   \includegraphics[width=8cm,height=6cm]
%   {NTraninglength.png}
%   \caption{\label{WL2}
 %  Average Sentence Lengths for Non-Suicidal Tweets}
% \end{figure}


\begin{table}[t!]
\centering
\resizebox{\columnwidth}{!}{%
\begin{tabular}{c c}
\hline
\textbf{Reference} & \textbf{Keywords}\\
\hline
\cite{word1FAHEY2020112960}  & \makecell{I want to kill myself\\ I want to die\\I want to disappear%ابى ابغى اموت اقتل نفسي اختفي
}\\\hline

\cite{word2ODEA2015183} &  \makecell{suicidal; suicide;\\
kill myself;end my life;
\\ never wake up; sleep forever; 
\\want to die; be dead;
\\better off dead; \\ tired of living; \\don't want to be here; \\die alone; 
} \\
\hline

\cite{word3valeriano2020detection}& \makecell{Just want to sleep forever 
\\Kill myself
\\Life is so meaningless
\\Tired of being alone 
\\Don’t want to exist
\\Life is worthless 
\\Don’t want to live 
\\My life is pointless
\\My life is this miserable 
\\My life isn’t worth 
\\Want to be dead 
\\Hate my life 
\\Want to disappear 
\\Hate myself 
\\Suicidal / Suicide Suicida
\\Isn’t worth living
}\\\hline
\cite{word4ji2018supervised} &\makecell{I want to end
my life
\\I’m feeling so bad
\\I’m going to kill myself
}\\\hline
\end{tabular}}
\caption{\label{table1}
Suicidal keywords
}
\end{table}

\subsection{Data Annotation}
All Tweets collected were labeled manually by psychology doctor to determine the level of concern of each tweet. Annotator were asked to read the tweets’ text and check the level of concern of each tweet. There will be two-level of concerns: ‘Suicidal’ tweets labeled with '1', and '0' for ‘Non-Suicidal’ tweets~\cite{11o2015detecting}.

%Associate Professor of Psychology at the Graduate School of Education
\begin{itemize}
\item Suicidal: tweet shows serious suicidal ideation; the person expresses a deep and personal intention to commit suicide; e.g., \<أقتل نفسي , نفسي اموت , افكر انتحر>. In contrast, a  suicide risk will not be considered if connected to some type of conditional event e.g., \<حقتل نفسي لو ما فاز الاتحاد اليوم \\، اتمنى اموت اول ما يقول الدكتور كويز \\، افكر انتحر لو مانزلوا جزء ثالث> unless this event was a serious suicidal risk factor e.g., abuse, drug use, or bullying e.g., \<كلامها القذر تجاوز قوتي ابى اموت>.
%; the occurrence of suicide appears to be imminent e.g.,  versus
%\item Possibly concerning: the default category.
\item Non-Suicidal: no fair evidence to indicate that there is a possibility of suicide; 
e.g., \<وين كان عقلي يوم سجلت  ليدر؟ 
افكر انتحر>
\end{itemize} %cite {Suicidal Behavior Detection on Twitter Using Neural Network}
%وين كان عقلي يوم سجلت رقمي ليدر بكل المواد؟افكر انتحر
%2 undeterment: ابي طريقه اموت فيها وادخل الجنه دايركت
%عندي تقارير وعرضين لازم اسلمهم رمضان تحسون اسويهم اقوم اذبح نفسي وافتك

\begin{figure*}[t!]
    \centering
    \begin{subfigure}[t]{0.45\textwidth}
        \centering
        \includegraphics[width=7cm,height=6cm]{wordcloudhSD.png}
        \caption{\label{figW1} Word cloud for Suicidal tweets}
    \end{subfigure}%
    \begin{subfigure}[t]{0.45\textwidth}
        \centering
        \includegraphics[width=7cm,height=6cm]{wordcloudhND.png}
        \caption{\label{figW2} Word cloud for Non-Suicidal tweets}
    \end{subfigure}
    \caption{ Word Cloud for Suicidal and Non-Suicidal Tweets}
\end{figure*}


%\textwidth
%\begin{figure}\centering
%   \includegraphics[width=7.5cm,height=5cm]
%   {wordcloudhSD.png}
%   \caption{\label{figW1}
%   Word cloud for Suicidal tweets}
%\end{figure}

%\begin{figure}\centering
%   \includegraphics[width=7.5cm,height=5cm]
%   {wordcloudhND.png}
%   \caption{\label{figW2}
%   Word cloud for non-Suicidal tweets}
%\end{figure}

%\begin{figure}\centering
%   \includegraphics[width=7.5cm,height=5cm]
%   {wordcloudh1not0predrop.png}
%   \caption{\label{figW2}
%   Word cloud for Dataset}
%\end{figure}
\begin{figure*}\centering
   \includegraphics[width=13cm,height=10cm]
   {S10mot.png
   %plot4.png   
   }
   \caption{\label{Common}
   Common Words Found in Suicidal Tweets (Without Stop Words)
   }
\end{figure*}

\begin{figure*}\centering
   \includegraphics[width=13cm,height=10cm]
   {Not10mot.png
   %plot4.png   
   }
   \caption{\label{Common}
   Common Words Found in Non-Suicidal Tweets (Without Stop Words)
   }
\end{figure*}

After annotating each tweet, 1426 tweets were labeled as Suicidal tweets and 4293 as non-Suicidal tweets. Table~\ref{table2} shows dataset classes and their weights. The first class is suicide ideation tweets (Suicidal), and the second class indicates that there is no potential suicidal intention (Non-Suicidal). It has been noticed that most suicidal tweets are short, it ranges from 2 to 20 words, as shown in Figure~\ref{WL1}. While the Non-Suicidal tweets vary from 2 to 80 words, as shown in Figure~\ref{WL2}. The most frequent words used in each class are shown in word-cloud representation in Figure~\ref{figW1} and ~\ref{figW2}, and as shown in Figure~\ref{Common}, the most frequent word used in a Suicidal tweet is \<"اموت">,  it's been used and repeated more than 800 times. As shown in Figure~\ref{hours}, the peaks of the dangerous tweets show up after ten o’clock in the night and continue to increase, and then there is a clear decrease at five o’clock in the afternoon.
%As its shown in ~\ref{WL1} most of suicidal tweets are short 

\begin{table}[h]
\centering
\begin{tabular}{ll}
\hline
\textbf{Class} & \textbf{\%of Datasets}\\
\hline
Suicidal  & \makecell{25\% }\\
%Non- suicidal & No evidence of possible suicidal intent & 75\% \\
Non-Suicidal &  \makecell{75\%} \\
\hline
\end{tabular}
\caption{\label{table2}
Labelled tweets and their weights
}
\end{table}


\begin{figure*}\centering
   \includegraphics[width=\textwidth,height=8cm]
   {maxhourSall-.png
   %plot4.png   
   }
   \caption{\label{hours}
   Trends of active hours for Suicidal tweets.
   %Frequency of tweets during day hours
   }
\end{figure*}


\begin{figure*}\centering
   \includegraphics[width=\textwidth,height=8cm]
   {maxhourUnSall.png
   %plot4.png   
   }
   \caption{\label{hours}
   Trends of active hours for Non-Suicidal tweets.
   %Frequency of tweets during day hours
   }
\end{figure*}


%\subsection{Data Pre-processing}
\subsection{Feature Extraction}
Transform features into numbers is needed to execute features in a machine learning classifier. We used the text feature as an efficient feature
set to distinguish Suicidal tweets from Non-Suicidal tweets. Several features were used in this research that, includes the bag of the word and TF-IDF (term frequency-inverse document frequency) models.
\begin{itemize}

%CountVectorizer
 \item Bag of Word: a basic vectorizer which converts text document into numbers (each token in the data is converted into a feature). Each word or token in the dataset is set to a value, and each value is set to the number of times the word appears. The classifier can then receive this multiset of words as input. This process that represent a dataset as a bag of words vector is known as tokenization. Bag of Word is one of the most useful ways to convert text data into feature but it is also known as a sparse dataset because each file contains numerous zeros for each text that does not appear in the dataset which is better to avoid\cite{deepa2019sentiment}.
%As a whole it transforms into array of features and create matrix. It count the repetition of each word in the document


\item TF-IDF: vectorizes words by taking into account how frequently a word appears in a document. TF-IDF is used to improve the performance of the frequent words that occurred in the document using the formula described below.

\begin{itemize}
\item TF = number of times the term occurs in
the text divided by the total number of words in the text,
\item IDF = Total number of documents divided by the number of documents with the word in it.
\item TF-IDF can be calculated by,\newline
TF-IDF = TF*IDF \cite{deepa2019sentiment}
\end{itemize}
All ngrams features are normalized by “TF-IDF” value to determine their relative importance in the corpus.

\begin{itemize}

\item Unigram: The basic type of textual feature, which consists of individual word TF-IDF value.
\item Word-ngrams: A combination of word bigram and trigram.
\item Char-ngrams: 
%The Arabic alphabet consist of 28 letters in addition to the space character.
This technique is particularly important for morphologically rich languages such as Arabic, as it can identify the morphological components of words. It is also useful for identifying word misspellings and alternative spellings that are common in online communications.\cite{alsafari2020hate}
\end{itemize}
\item Word embedding:
Word embedding achieves extremely significant advancement in numerous areas of natural language processing (NLP), such as sentiment analysis, topic segmentation, text mining and recommendation~\cite{xu2018incorporating,Li_Shah_Liu_Nourbakhsh_2017, naili2017comparative}. Here, two common word embedding models (Word2vec and Fasttext) were used. A study by~\citet{kaibi2019comparative} showed that Fasttext, followed by Word2vec, gave the best techniques for Twitter Sentiment Analysis~\cite{kaibi2019comparative}. 

\begin{itemize}
    \item Word2vec:
Word2vec is one of the most popular word embedding models created by Mikolov et al.~\cite{mikolov2013efficient}. It is based on two neural network model architectures (Skip-gram and CBOW). Skip-gram uses the word as input and predicts the surrounding words as output, while CBOW uses the context words to predict the word. Arabic Word2vec models, have been shown to be effective in capturing semantic and syntactic relationships between Arabic words and have achieved state-of-the-art results in various Arabic natural language processing tasks~\cite{}
doi:10.1137/1.9781611974010.66
%such as AraVec and AWE, 

 \item FastText:
FastText is an extension of the Skip-gram model.%~\cite{grave2018learning}
Each word in FastText is represented by a sum of n-gram vectors~\cite{athiwaratkun2018probabilistic}.  
\end{itemize}

\end{itemize}
%Figure ~\ref{figW1} illustrates the top most frequent word used in Suicidal tweet, while figure ~\ref{figW2} illustrates the top most frequent word used in non-Suicidal tweet



\subsection{Training and Performance Evaluation}%or \subsection{Classification}
In this experiment, the performance of AraBert was compared with five popular classifiers (Naïve Bayes (NB), Support vector machine(SVM with gamma=1 and c =10), K-Nearest Neighbor(KNN with Kneighbors=30), Random forest(RF) and Extreme Gradient Boosting(XGBoost) with extreme gradient boosting=6) to classify Arabic suicidal tweets. AraBert is a pre-trained Bidirectional Encoder Representation from Transformer (BERT) model developed by Google for the Arabic language.\cite{berthttps://doi.org/10.48550/arxiv.1810.04805}

% BERT uses a masked language model (MLM) objective, which allows the left and right fuse context. Unlike other language representation models that uses only left to right language model pretraining [29]. BERT model is utilized in wide range of NLP tasks, such as question answering, sentiment classification and provide the state of art accuracy [29, 30].  BERT enhanced the ability of word embedding model generalization. It can fully express the character level, word level, sentence level, and sentence relationship [6]. state-of-the-art models for a wide range of tasks, such as question answering and language inference

In order to evaluate the performance of our system and assess the capacity of the models used in this experiment, we considered the measurements of precision, recall, F1 score (F1) and accuracy defined as follows:



\begin{itemize}
    \item TP = true positives,the model correctly predicts the positive class.
    \item FP = false positives,the model incorrectly predicts the positive class.
    \item TN = true negatives,the model correctly predicts the negative class.
    \item FN = false negatives,the model incorrectly predicts the negative class.
\end{itemize}

Using the formula:
\begin{itemize}
    \item Precision =$\frac{TP}{TP+FP}$% TP/ (TP + FP)
    
    \item Recall = $\frac{TP}{TP+FN}$%TP/(TP + FN)
    
    \item F1 = $\frac{2*(precision * recall)}{precision + recall}$%2*(precision * recall)/(precision + recall)

    \item Recall = $\frac{TP+TN}{TP+TN+FP+FN}$%TP+TN/(TP+TN+FP+FN)
\end{itemize}

%cite Analyzing Tweets For Predicting Mental Health States Using Data Mining And Machine Learning Algorithms

\section{Result analysis}
Bag of Word, TF-IDF with different levels (Unigram, bigram and trigram, Character level), and neural word embedding were used for feature representation. %As it's shown in Table 3, TF-IDF CharLevel gives better performance
Six machine learning models were examined and compared against Arabert which are, Naïve Bayes, Support vector machine, K-Nearest Neighbors, Random forest, XGBoost, and AraBert. Accuracy, Precision, Recall, and F1-Score were used for evaluation. Table~\ref{table3} shows the comparison of classifiers. The results show that character level TF-IDF gave the best performance (accuracy, precision, recall, and F1-score ) in all the machine learning models in comparison with the Bag of word model and word level Unigram, Bigram, Trigram. The highest accuracy is 86\% in SVM and RF using TF-IDF CharLevel. Figure~\ref{ROC} presents the ROC curve (receiver operating characteristic curve) obtained for the five algorithms (Naïve Bayes, SVM, Random Forest and Xgboost). 
%Figure \ref{ROC} depicts the performance  obtained for the five algorithms (Naïve Bayes, SVM, Random Forest and Xgboost).
%
AraBert algorithm gave the best performance in comparison with all other machine learning algorithms with 88\% precision, 86\% recall, 87\% f1-score, and 90\% accuracy in classifying suicidal tweets. The confusion matrix of Arabert is shown in Figure \ref{conarabert}.



%plot2Classes+Copy of gridsearchCV.ipynb
\begin{table*}[H]
%\label{bigtable}
\centering
\begin{tabular}{llllll}
\hline
\textbf{Classifier} & \textbf{Feature} & \textbf{Precision} & \textbf{Recall}& \textbf{F1-score} & \textbf{Accuracy} \\
\hline
NB & \makecell{Bag of word\\ Unigram\\ N-Gram\\CharLevel* \\ Word2Vec \\ Fasttext} & 
\makecell{73\% \\75\%\\ 73\%\\ \textbf{81\%} \\ 77\% \\76\%}& 
\makecell{73\% \\ 72\%\\ \textbf{75\%}\\ 72\%\\ 54\% \\ 62\%}&
\makecell{73\% \\ 74\%\\  74\%\\\textbf{75\%}\\ 51\% \\ 63\%}&
\makecell{81\% \\ 82\%\\   80\%\\\textbf{84\%}\\ 76\% \\ 78\%} \\
% without gridsearch
%accuracy   0.59      
%macro avg       0.65      0.62      0.58   
%Fasttext without gridsearch: accuracy= 0.71 macro avg       0.72      0.67      0.68      1144
%GaussianNB(var_smoothing=1.0)

\hline
%0.73      0.73      0.73  0.81  
%MultinomialNB(alpha=1e-07)
% 0.75      0.72      0.74  0.82  MultinomialNB(alpha=1e-07)
%0.73      0.75      0.74   0.80 MultinomialNB(alpha=1e-06)
%0.81      0.72      0.75   0.84    MultinomialNB(alpha=0.001)

SVM & \makecell{Bag of word\\  Unigram\\ N-Gram\\CharLevel* \\ word2vec \\Fasttext} & 
\makecell{82\% \\84\%\\ \textbf{86\%}\\ 85\%\\ 78\% \\77\%}&
\makecell{\textbf{78\%} \\ 76\%\\ 72\%\\ 76\%\\ 66\% \\72\%}&
\makecell{\textbf{79\%} \\ \textbf{79\%}\\76\%\\\textbf{ 79\%}\\ 68\% \\74\%}&
\makecell{\textbf{86\%} \\ 86\%\\85\%\\ \textbf{86\%}\\  80\% \\81\%}\\
%word2vec {'C': 10, 'class_weight': 'balanced', 'gamma': 1, 'kernel': 'rbf', 'probability': True}SVC(C=10, class_weight='balanced', gamma=1, probability=True)
%fastext without: accuracy=0.81  macro avg       0.69      0.77      0.71 
%SVC(C=10, class_weight='balanced', gamma=0.1, probability=True)
\hline
%C and gamma
%0.82      0.78      0.79  0.86 Grid
%SVC(C=10, class_weight='balanced', gamma=0.1, probability=True)
%0.84      0.76      0.79  0.86  Grid
%SVC(C=10, class_weight='balanced', gamma=1, probability=True)
%0.88      0.57      0.57  0.79
%0.86      0.72      0.76  0.85 grid
%SVC(C=1, class_weight='balanced', gamma=1, kernel='linear', probability=True)
%0.85      0.76      0.79  0.86
%{'C': 10, 'class_weight': 'balanced', 'gamma': 1, 'kernel': 'rbf', 'probability': True} SVC(C=10, class_weight='balanced', gamma=1, probability=True)SVC(C=10, class_weight='balanced', gamma=1, probability=True)
KNN & \makecell{Bag of word\\  Unigram\\ N-Gram\\CharLevel*\\ Word2Vec \\ Fasttext} & 
\makecell{63\% \\75\%\\ 75\%\\ 78\%\\ 64\% \\71\%}&
\makecell{68\% \\ 76\%\\ 69\%\\ 75\% \\ 71\%\\ 77\%}&
\makecell{60\% \\ 75\%\\  71\%\\ 76\% \\ 65\% \\73\%}&
\makecell{63\% \\ 81\%\\81\%\\ 84\% \\77\% \\81\%}\\
%word2vec reported result wihout grid
%FastText accuracy=0.80, macro avg       0.77      0.67      0.69    
\hline
%0.63      0.      0.60    0.63   KNeighborsClassifier(n_neighbors=2)
% 0.75      0.76      0.75   0.81  KNeighborsClassifier(n_neighbors=30)
%0.75      0.69      0.71   0.81   KNeighborsClassifier(n_neighbors=3)
%0.78      0.75      0.76   0.84  KNeighborsClassifier(n_neighbors=30)
RF & \makecell{Bag of word\\  Unigram\\ N-Gram\\CharLevel*\\ Word2Vec \\Fasttext} & 
\makecell{77\%\\ 83\%\\ 77\%\\ 85\% \\ 81\% \\69\%}&
\makecell{76\% \\ 72\%\\ 73\%\\74\% \\ 65\% \\81\%}&
\makecell{77\% \\ 75\%\\ 75\%\\77\% \\ 67\% \\ 72\%}&
\makecell{83\% \\ 85\%\\ 85\%\\86\% \\ 81\% \\82\%}
%RandomForestClassifier(criterion='entropy', max_features=2, min_samples_split=6)
%FAsttext repotered result without gridsearch RandomForestClassifier(criterion='entropy', max_features=3) accuracy= 0.81  macro avg       0.81      0.67      0.70      1144
\\\hline
%0.77      0.76      0.77    0.83
%0.83      0.72      0.75    0.85
%0.77      0.73      0.75    0.83
%0.85      0.74      0.77   0.86
XGBoost & \makecell{Bag of word\\  Unigram\\ N-Gram\\CharLevel*\\ Word2Vec \\ Fasttext} & 
\makecell{79\% \\83\%\\ 79\%\\ 82\% \\ 64\%\\ 81\%}&
\makecell{67\% \\ 69\%\\ 60\%\\75\% \\ 78\% \\ 68\%}&
\makecell{69\% \\ 72\%\\62\%\\78\% \\ 66\% \\ 80\%}&
\makecell{82\% \\ 84\%\\80\%\\85\%\\ 79\% \\70 \%}\\
\hline
%fasttext with gridsearch XGBClassifier(colsample_bytree=0.7, learning_rate=0.5, max_depth=6, min_child_weight=11, missing=-999, n_estimators=5, seed=1337,  silent=1, subsample=0.8) accuracy        =0.80 macro avg       0.75      0.68      0.70      1144
%0.79      0.67      0.69   0.82
%0.83      0.69      0.72   0.84
%0.79      0.60      0.62   0.80
%0.82      0.75      0.78   0.85
%CNN & \makecell{glove} %\makecell{Count Vectors\\ TF-IDF\\ N-Gram} &    \makecell{50\%}& \makecell{38\%}& \makecell{43\%}&\makecell{76\%} \\\hline
%RNN & \makecell{glove} %\makecell{Count Vectors\\ TF-IDF\\ N-Gram} &    \makecell{50\%}& \makecell{38\%}& \makecell{43\%}&\makecell{76\%} \\\hline
%LSTM & \makecell{glove} %\makecell{Count Vectors\\ TF-IDF\\ N-Gram} &    \makecell{50\%}& \makecell{38\%}& \makecell{43\%}&\makecell{76\%} \\\hline
AraBert & \makecell{-} %\makecell{Count Vectors\\ TF-IDF\\ N-Gram} 
&    \makecell{88\%}& \makecell{86\%}& \makecell{87\%}&\makecell{90\%} \\
\hline
\end{tabular}
\caption{\label{table3}
 Performance analysis of different classifiers in classifying Suicidal Tweets.* indicates best performing feature extraction technique.
}
\end{table*}

\begin{figure*}\centering
   \includegraphics[width=\textwidth,height=20cm]
   {6plotwithoutP.png
   %plot4.png   
   }
   \caption{\label{ROC}
   ROC Curves depicts the performance  obtained for Machine Learning algorithm}
\end{figure*}

\begin{figure*}\centering
   \includegraphics[width=\textwidth,height=6cm]
   {Bert.png}
   \caption{\label{conarabert}
   Confusion Matrix for Best performing Algorithm (AraBert)}
\end{figure*}
%\begin{figure*}\centering
   %\includegraphics[width=\textwidth,height=10cm]
   %{DeepAUC.png}
   %\caption{\label{ROC}
   %ROC Curves depicts the performance  obtained for Deep leaning algorithm}
%\end{figure*}

\section{Conclusion}
%In this paper, four popular machine learning algorithm  were compered in classifying Arabic Suicide ideation tweets.
This paper investigates the potential of machine learning algorithms for detecting suicidal ideation in social media by applying a variety of machine learning algorithms aimed to enable the community to detect suicidal ideation at an early stage. NB, SVM, Knn, RF, XGBoost, and Arabert were compared in classifying Arabic Suicide ideation tweets. Experiments showed that Arabert was the best performing algorithm.% with F-scour, precision, and accuracy.
Future work will concentrate on increasing the size of the dataset. Also, examining and comparing the performance of other machinBERT uses a masked language modele and deep learning techniques with different feature selection approaches.% with the result of this experiment.

 %%%%
 %Summary of related research applied models and accuracy.
 %Reference/ Dataset Size
%(Number of Tweets) / Classifier/ %Feature Extraction/ Accuracy

%\subsection{References}

% Entries for the entire Anthology, followed by custom entries
\newpage
\bibliography{anthology,custom}
\bibliographystyle{acl_natbib}



\end{document}
