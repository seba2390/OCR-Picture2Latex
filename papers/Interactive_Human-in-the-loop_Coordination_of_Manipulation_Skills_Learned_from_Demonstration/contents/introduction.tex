\section{Introduction}\label{sec:introduction}
Robots have been making their ways out of the closed fences in industrial factories. 
Collaborative robots (\emph{cobots}) are intended for direct human interactions within a shared space.
This workspace is much more dynamic than the precisely-arranged structures, e.g., assembly lines.
Moreover, the designated tasks are often low in repetition and high in variance.
Namely, the robots should adapt to different tasks and different scenarios. 
For instance, 
the same service robot might be used for cleaning, packing and transportation. 
Two characteristics are essential for such use cases:
(i) the ability to \emph{quickly} programme robot for different tasks;
(ii) the learned task policy should \emph{adapt} automatically to unforeseen situations.
Most motion planners however require precise modeling of the scene and the robot~\cite{lavalle2006planning}, thus difficult to configure for non-technical end-users.
Instead, learning from demonstrations (LfD) provides an intuitive and efficient method to program robot skills even for layman.


%========================================
\begin{figure}[t!]
    \centering
    \includegraphics[width=0.98\linewidth]{figures/intro.png}
    %--------------------
    \caption{Diagram of the proposed human-in-the-loop skill coordination scheme. 
Once the skill primitives are demonstrated offline, 
the task network and the associated policy are learned online given real-time human instructions and state observations.}
    \label{fig:framework}
    %--------------------
    \vspace{-0.15cm}
\end{figure}
%========================================

Moreover, instead of a single motion, 
complex manipulation tasks often contain multiple branches of skill sequences that share some common skills.
The planning process should generate the right sequence of skills and their parameters under different scenarios. 
For instance, as discussed in later the experiment, 
the bin-picking task involves to pick the object differently from the box, clear it from the corners if needed, re-orient it to reveal the barcode, and show the barcode to the scanner. 
To choose the correct skill sequence is essential for flexible robotic systems across various applications.
Such transitions among the skills and the associated conditions are often difficult and tedious to specify manually.
In fact, both self-adaptation and autonomous decision-making are important design principles of Industry 4.0 systems~\cite{hermann2016design}.

Lastly, besides kinesthetic demonstrations, human operators can also provide \emph{interactive} guidance during online task execution. 
They are often more efficient and effective than offline simulated data,
see~\cite{xin2018accelerating, holzinger2016interactive}.
However, the amount of such inputs should not be excessive and only required under appropriate indications, e.g., when the confidence of a decision process is low. 
In other words, transparency during the interaction is crucial for a human-in-the-loop system.

As shown in Fig.~\ref{fig:framework}, in this work we address these issues by proposing a human-in-the-loop coordination framework for skills learned from demonstrations. 
In particular, the backbone of the coordination framework is a geometric task network (GTN) which consists of the primitive skills as nodes and their transition relations as edges.  
Given a manipulation task, the learned task network can decide not only which skill to execute given the current system state, 
but also the associated parameters. 
The proposed algorithm first constructs the network structure, 
then learns the underlying hierarchical selectors based on the geometric constraints among the robots and the objects.
Human instructions are only required at the first few executions to improve the task network on-the-fly.
Several industrial applications are studied on hardware to validate the performance.


Main contribution of this work is threefold:
(i) a more general task-parameterized model for skills with multiple branches;
(ii) a novel structure as geometric task networks (GTN) for coordinating LfD skills, which is 
fully-explainable regarding the underlying decisions;
(iii) an online, human-in-the-loop and interactive learning algorithm, which is {extremely} data-efficient and intuitive regarding the required human instructions.


%% The remainder of this paper is organized as follows.
%% Sec.~\ref{sec:related} reviews the state of the art in motion primitives and skills sequencing. 
%% Sec.~\ref{sec:preliminary} presents some preliminaries.
%% Sec.~\ref{sec:problem} details the considered problem.
%% Given the current limitations described in Sec.~\ref{sec:limitation}, 
%% Sec.~\ref{sec:solution} contains the main components of the proposed solution.
%% Experimental results are presented in Sec.~\ref{sec:experiments}.
%% Sec.~\ref{sec:conclusion} concludes the work.
