% ==============================
\section{Related Work}\label{sec:related}

\subsection{Learning from Demonstration}\label{sec:lfd-review}
% lfd
Compared with traditional motion planning from~\cite{lavalle2006planning}, learning from demonstration (LfD) is an intuitive way to transfer human skills to robots, see~\cite{calinon2016tutorial, Osa2018Imitation, pathak2018zero}.
Various teaching methods can be used such as kinesthetic teaching in~\cite{calinon2016tutorial}, tele-operation in~\cite{abbeel2004apprenticeship}, and visual demonstration in~\cite{pathak2018zero}. 
Different skill models are proposed to abstract these demonstrations: 
full trajectory of robot end-effector in~\cite{Osa2018Imitation}, 
dynamic movement primitives (DMPs) in~\cite{paraschos2013probabilistic, rozo2016learning},
task-parameterized Gaussian mixture models (TP-GMMs) in~\cite{calinon2016tutorial, Zeestraten17riemannian} which extend GMMs by incorporating observations from different perspectives (so called task parameters),
task-parametrized hidden semi-Markov models (TP-HSMMs) in~\cite{Schwenkel2019Optimizing, rozo2020learning,le2021learning},
and deep neural networks~\cite{pathak2018zero}.
In this work, we adopt the TP-HSMM representation to extract both temporal and spatial features from few human teachings, while allowing generalization over multiple task parameters.


\subsection{Task and Motion Planning}\label{sec:tamp-review}
% classic TAMP
Task planning focuses on constructing a discrete high-level plan via abstract decision-making (e.g., via logic-reasoning from~\cite{fox2003pddl2}), 
while motion planning addresses the low-level sensing and control problem of a dynamic system, see~\cite{lavalle2006planning}.
The area of task and motion planning (TAMP) attempts to improve the synergies between them.
The planning process over the state graph consists of searching over both discrete skill sequences and the continuous parameters, see~\cite{kaelbling2010hierarchical, konidaris2018skills, Toussaint18diff}.
A conjugate view of the state graph is the so-called skill graph, where instead the nodes are primitive skills and edges are implicit state abstractions, see~\cite{huang2019neural,   hayes2016autonomously,  konidaris2012robot, frazzoli2005maneuver}. 
The work in \cite{hayes2016autonomously} extends the hierarchical task networks (HTN) to conjugate task graph (CTG) without any parameterization on the skill primitives.   
Moreover, \cite{frazzoli2005maneuver} calls such graph as maneuver automaton, which however is \emph{manually} designed instead of learned, whereas~\cite{huang2019neural} require similar structural supervision during training.
%% The works in~\cite{niekum2015learning, konidaris2012robot} learn such task-level graph from complete demonstrations of the \emph{whole} manipulation task and with manual specification of action sequences during training. 
The method in~\cite{konidaris2012robot} relies on ``change point'' detection to segment these task demonstrations with simple 2D models,
while~\cite{niekum2015learning} assumes each skill primitive is parameterized to only {one} object frame.
In this work, we adopt this conjugate perspective, but with an embedded hierarchical structure of selectors. 
Moreover, 
both the graph structure and the associated geometric constraints are learned online, without manual specifications. 
%we only require demonstrations of primitive skills \emph{locally} without any specific global task. 
%The main advantage is that such local skills can often be shared across different tasks.


\subsection{Human-in-the-loop Learning}\label{sec:hil-review}
% HIL
Human operators can also provide interactive instructions during online task execution, 
which are often more efficient and effective than offline simulated data,
see~\cite{xin2018accelerating, holzinger2016interactive}.
Human inputs can be of different formats such as qualitative preferences~\cite{holzinger2016interactive}, directly modifying the underlying algorithms~\cite{xin2018accelerating}, additional actions~\cite{mandel2017add}.
In this work, the human operators are only queried when the decision confidence is low, 
and the expected inputs are simple instructions such as the next desired skill or branch name. 
