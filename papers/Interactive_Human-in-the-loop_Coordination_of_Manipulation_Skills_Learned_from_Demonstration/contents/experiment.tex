%========================================
%========================================
\section{Experiments} \label{sec:experiments}
This section presents the experimental validation on a 7-DoF Franka Emika robot arm 
for two different industrial applications: 
first one as a part of an assembly task, and second as a bin-sorting task. 
The arm is extended by a Zivid RGBD sensor for perception 
and a parallel (or suction) gripper. 
Additionally, kinaesthetic teaching can be done directly by guiding the end effector manually. 
The proposed framework is implemented in Python3 under the Robot Operating System (ROS). 
All benchmarks are run on a desktop with an 8-core Intel Xeon CPU. 
Experiment videos can be found in the supplementary file. 


%========================================
\subsection{Workspace and Task Description} \label{subsec:task-description}
The assembly task was introduced in our previous work~\cite{rozo2020learning}.
For brevity, we omit the detailed description here and refer the readers there.
As shown in Fig.~\ref{fig:assembly-gtn},  a metallic cap is fed onto the inspection platform 
and depending on the result, the robot arm should either attach the cap to the top of a peg 
or drop it into a pallet.
Given different initial states of the cap (lying-flat of standing), 
additional manipulation skills are needed to re-orient and translate the cap before the normal pick and drop skills. 
Note that in our earlier work~\cite{Schwenkel2019Optimizing, rozo2020learning}, 
these sequences are specified \emph{manually} before every execution.
%========================================
\begin{figure}[t!]
    \centering
    \includegraphics[height=0.24\linewidth]{figures/cap_ws.png}
    \includegraphics[height=0.24\linewidth]{figures/assembly_gtn.png}
    %--------------------
    \caption{\textbf{Left}: workspace for the assembly task; 
             \textbf{Right}: the learned final GTN with one example plan (in blue).}
    \label{fig:assembly-gtn}
    %--------------------
    \vspace{-0.15cm}
\end{figure}
%========================================

%========================================
\begin{figure}[t!]
    \centering
    \includegraphics[height=0.24\linewidth]{figures/bin_ws.PNG}
    \includegraphics[height=0.24\linewidth]{figures/bin_gtn.png}
    %--------------------
    \caption{\textbf{Left}: workspace for the bin-sorting task; 
             \textbf{Right}: the learned final GTN with one example plan (in blue).}
    \label{fig:bin-gtn}
    %--------------------
    \vspace{-0.15cm}
\end{figure}
%========================================

Another application is the well-known bin picking and sorting task. 
As shown in Fig.~\ref{fig:bin-gtn}, the goal is to pick unknown objects out of the bin, scan them for product info, and then sort them accordingly. 
Instead of emphasizing performance such as ``pick per hour'', 
we are interested in addressing several corner cases:
(i) the object should be picked with different orientations when close to the bin boundaries;
(ii) the object should be cleared out of the corners before picking;
(iii) the object should be flipped when the barcode overlaps with the suction area;
and (iv) the objects are placed differently given the scanning results.


%========================================
\subsection{Results} \label{subsec:results}

%========================================
\subsubsection{Learned Skill Model}\label{subsubsec:skill-model-results}
For the assembly application, in total $5$ primitive skills are taught, i.e., 
\texttt{pick} to pick the cap from the platform with three branches;
\texttt{re\_orient} to re-orient the cap from lying flat to standing with two branches;
\texttt{translate} to translate the cap to the platform boundary while standing;
\texttt{attach} to attach the cap to the peg;
\texttt{drop} to drop the cap with two branches.
Similarly, for the bin sorting application, in total $6$ primitive skills are taught, i.e.,
\texttt{pick\_bin} to pick any object from bin with five branches. 
\texttt{scan} to scan the object; 
\texttt{press\_shift} to press and shift any object out of the corners with four branches;
\texttt{flip} to flip the object;
\texttt{drop\_bin} to drop the picked object into another bin;
\texttt{sort} to arrange the picked object in rows.



Due to the new branch selector proposed in Sec.~\ref{subsec:learn-primitive}, 
the number of demos needed is much reduced compared with our previous work~\cite{Schwenkel2019Optimizing, rozo2020learning}.
In average $6$ demos are performed for each skill and the associated skill model with the branch selector is learned in $0.2s$.
Afterwards, the model of each skill is verified independently under different scenes.
For instance, the \texttt{pick} skill are demonstrated $2$ times for each branch,
and the feature vector $\mathbf{v}$ has dimension $7$.

%========================================
\begin{figure}[t!]
    \centering
    \includegraphics[width=0.98\linewidth]{figures/corners.png}
    %--------------------
    \caption{Evolution of the edge selector for node $\texttt{STR}$ in the GTN for bin-sorting in Fig.~\ref{fig:bin-gtn}.
    There are two modes projected onto the $x-y$ plane: 
    \texttt{pick\_bin} skill (in red) and \texttt{press\_shift} skill (in blue). 
    Human instructions are requested at the samples marked in circles, 
    while autonomous predictions are marked in diamonds.}
    \label{fig:change-edge-selector}
    %--------------------
    \vspace{-0.15cm}
\end{figure}
%========================================

%========================================
\subsubsection{Learned Task Network}\label{subsubsec:task-network-results}
The proposed GTN is learned by following Alg.~\ref{alg:hil-coordination} for each application.
%% Both the initial state $\boldsymbol{s}_0$ and goal state $\boldsymbol{s}_G$ are specified by the end effector pose and the object pose, thus of dimension $14$.
Various problem instances of $(\boldsymbol{s}_0, \boldsymbol{s}_G)$ are defined in the learning process.
Human instructions are requested for the next desired  skill and branch.
For instance, as shown in Fig.~\ref{fig:assembly-gtn},
if the cap is standing and the goal is to drop it into the pallet, 
the skill sequence is \texttt{pick}, \texttt{translate}, \texttt{pick} again, and finally \texttt{drop}.
As shown in Fig.~\ref{fig:bin-gtn}, 
if the object is close to one corner with the barcode on the top, 
the sequence is \texttt{press\_shift},  \texttt{pick\_bin}, \texttt{flip}, \texttt{scan}, and  \texttt{drop} into another bin.

%========================================
\begin{figure}[t!]
    \centering
    \includegraphics[width=0.98\linewidth]{figures/tikz/human_inputs.pdf}
    %--------------------
    \caption{$x$-axis: number of task executions. Left-$y$-axis: number of human instructions requested for GTN ($\mathcal{G}$) and branch selector ($\mathsf{B}$), for assembly task \texttt{Asm} and bin-sorting task \texttt{Bin}.
    Right-$y$-axis: evolution of the lowest confidence in the GTN execution for both tasks.}
    \label{fig:human-input-change}
    %--------------------
    \vspace{-0.15cm}
\end{figure}
%========================================

Each time a human instruction is given, either the GTN or the branch selector is updated via Line~$6-7$ in Alg.~\ref{alg:hil-coordination}.
Fig.~\ref{fig:change-edge-selector} shows an example how the edge selector of node GTN evolves within the execution of the first five executions.
Moreover, Fig.~\ref{fig:human-input-change} records how many human inputs are required for the GTN $\mathcal{G}$, and the branch selectors $\{\mathcal{C}^{\texttt{B}}\}$ are updated during the whole run.
Notice that the topology of $\mathcal{G}$ is quickly learned while the edge and branch selectors are improved whenever a new scene is experienced with a low confidence score. 
Fig.~\ref{fig:human-input-change} also shows how the lowest confidences for selecting edges and branches increase with time as the GTN is improved, 
where both lower bounds $\underline{\rho}^\texttt{B}$ and $\underline{\rho}^\texttt{E}$ are set to $0.8$.
In the end, the success rate is close to $100\%$ with full autonomy for both applications, where most failures are caused by execution and perception errors. 
The final learned GTNs for both are shown in Fig.~\ref{fig:assembly-gtn} and~\ref{fig:bin-gtn}.



%========================================
\subsection{Comparison} \label{subsec:comparison}
The proposed scheme (\textbf{GTN} for short) is compared to the following baselines:
(i) the vanilla TP-HSMM scheme as stated in~\cite{calinon2016tutorial, rozo2020learning} (\textbf{TPH} for short), i.e., in combination with proposed GTN but without the proposed branch selector;
(ii) the \emph{full} system state (\textbf{FUL} for short) is used as the feature vector for the branch selector in~\eqref{eq:skill-feature} and the edge selector in~\eqref{eq:edge-feature}, 
i.e., instead of the relative frames;
(iii) a task and motion planner (\textbf{TMP} for short) that searches in simulation over the system state space for each new task $(\boldsymbol{s}_0, \boldsymbol{s}_G)$;
(iv) a completely manual design of the branch and edge selection (\textbf{MAN} for short), 
i.e., by specifying the rules for each case.

As summarized in Table~\ref{table:compare}, 
\textbf{TMP} does not learn from past solutions and requires the longest solution time.
For certain problems where the sequence has more than $4$ skills, it can not solve them in reasonable time ($10$ min).
Moreover, \textbf{TPH} performs relatively well for predicting the skill sequence, however fails at executing the skill due to choosing the wrong branch, especially when the scenes are different from the demos.
Notably, \textbf{FUL} learns not only slower but also performs worse in predicting both the edges and the branches, compared with \textbf{GTN}.
The plausible explanation is that the \emph{relative transformation} in~\eqref{eq:skill-feature} and~\eqref{eq:edge-feature} is difficult to capture with linear or even nonlinear kernels such as RBF~\cite{bishop2006pattern}.
Last but not least, the manual rules in \textbf{MAN} are much harder to design than the string inputs required by our scheme. 
Particularly, for both applications, boundaries on orientations of \emph{different} objects are 
often transformed to Euler angles, which however are often ill-posed;
In addition, such rules are hard to cover the complete state space and thus some corner cases are not defined. 
%========================================
\begin{table}[t]
\begin{center}
%--------------------
\begin{adjustbox}{height=0.15\linewidth}
\begin{tabular}{ccccc}
\toprule
Methods & L-time[$s$] & S-time[$s$] & R-skill/task & H-design[min]\\
\midrule
\textbf{GTN} & 1.5 & 0.2 & 0.95/0.9 & 2 \\
{TPH} & 0.1 & 0.8 & 0.8/0.6 & N/A \\
{FUL} & 10 & 0.5 & 0.3/0.2 & 10 \\
{TMP} & N/A & $>300$ & 0.6/0.5 & N/A \\
{MAN} & N/A & 0.1 & 0.9/0.9 & 20 \\
\bottomrule
\end{tabular}
\end{adjustbox}
%--------------------
\caption{Comparison: learning time, solution time, success rate for each skill and the complete task, and the total time to design human inputs, of all baselines for the assembly task.}
\label{table:compare}
\end{center}
\vspace{-0.35cm}
\end{table}
%========================================
 
