%========================================
\begin{figure}[t!]
    \centering
    \includegraphics[width=0.45\linewidth]{figures/branch_gaussian_2p_box.png}
    \includegraphics[width=0.45\linewidth]{figures/branch_svm_2p_box.png}
    %--------------------
    \caption{
Comparison between branch selection based on the Gaussian preconditions (left) 
and the proposed selector (right), 
    for the bin-picking skill with 5 branches (four sides and the center). 
    Note that different colors indicate the predicted branches at that sample point (in dots),
 while the \emph{projected} training data are indicated as diamonds (left).}
    \label{fig:branch_compare}
    %--------------------
    \vspace{-0.15cm}
\end{figure}
%========================================

\section{Proposed Solution}\label{sec:solution}
In this section, we present the main components of the proposed solution:
first, we introduce an extension to the current skill model learning;
then, we show how to construct a Geometric Task Network (GTN) for a given task; 
lastly, we describe how both the skill model and the task network can be improved during online execution with human inputs.

%==============================
\subsection{Primitive Skills Learning}\label{subsec:learn-primitive}

As illustrated in Fig.~\ref{fig:framework}, 
there are often multiple ways of executing the same skill under different scenarios (called \emph{branches}).
For instance, there are five different ways of picking objects from a bin, i.e., approaching with different angles depending on the distances to each boundary.  
Our earlier work~\cite{rozo2020learning} proposed a learning algorithm for TP-HSMM with multiple branches, 
and moreover a precondition model that chooses the best branch based on the first GMMs of all branches.
However, this model requires a large number of demos to cover the area of interest 
and does not generalizes well to new scenarios.
This is mainly due to the usage of Gaussian clustering over few samples in high dimensions. 
Fig.~\ref{fig:branch_compare} shows one example of the bin-picking skill.
%It can be seen that the precondition model from~\cite{rozo2020learning} can yield bad choices even close to the demonstrated scenes. 

%% In order to overcome the two limitations mentioned in Sec.~\ref{subsec:limit-few} and~\ref{subsec:limit-branch}, 
%% we propose the following extensions to the skill model~$\boldsymbol{\theta}_{\mathsf{a}}$ in Sec.~\ref{subsec:tp-hsmm}. 


%========================================
\begin{figure}[t!]
    \centering
    \includegraphics[width=0.7\linewidth]{figures/frames.png}
    %--------------------
    \caption{Illustration of the computation of feature vectors $\mathbf{v}_m$ for the edge selector and $\mathbf{h}_\ell$ for the branch selector, given skill frames $\mathbf{F}_p$.}
    \label{fig:frames}
    %--------------------
    \vspace{-0.15cm}
\end{figure}
%========================================

%% As mentioned in Sec~\ref{subsec:limit-few}, 
%% the reproduced trajectory under the current method does not satisfy the constraints on the via-poses nor the goal pose anymore. 
%% Thus, we propose the following modification to the learned local GMMs:
%% \begin{equation}\label{eq:modify-gmms}
%% \boldsymbol{\Sigma}_{k}^{p} = 
%% \begin{cases}
%% \boldsymbol{\Sigma}_{k}^{p},\; \text{if}\; k=k^{p};\\[10pt]
%% \exp\left(c \sum_{\ell=k}^{k^{p}}d_\ell\right)\, \boldsymbol{\Sigma}_{k}^{p},\; \forall k \neq k^{p};
%% \end{cases}
%% \end{equation}
%% where $\boldsymbol{\Sigma}_k^{p}$ is the covariance of the local $k$-th GMM within frame $p$;
%% $k^{p}$ is the component index of the via-point associated with frame $p$; 
%% $d_\ell$ is the average duration of component $\ell$ 
%% thus $\sum_{\ell=k}^{k^{p}}d_\ell$ is the total elapsed time from component $k$ to $k^{p}$ (or from $k^{p}$ to $k$, depending on the order);
%% $c>0$ is a design parameter
%% Namely, the further away a component $k$ is w.r.t component $k^{p}$, 
%% the more the covariance of its local GMM w.r.t frame $p$ is inflated. 

%% Consequently, given a set of new frames $\{\boldsymbol{b}^{p},\boldsymbol{A}^{p}\}_{p=1}^P$, 
%% the learned TP-GMMs are converted into one single GMM with parameters $\{\pi_k, (\boldsymbol{\hat{\mu}}_{k}, \boldsymbol{\hat{\Sigma}}_{k})\}_{k=1}^K$, by multiplying the affine-transformed Gaussians across different frames:
%% \begin{equation}\label{eq:gaussian-product}
%% \boldsymbol{\hat{\Sigma}}^{-1}_{k}= \sum_{p=1}^P\left(\boldsymbol{\hat{\Sigma}}_{k}^{p}\right)^{-1}, \,\boldsymbol{\hat{\mu}}_{k}=\boldsymbol{\hat{\Sigma}}_{k} \sum_{p=1}^P\left(\boldsymbol{\hat{\Sigma}}_{k}^{p}\right)^{-1}\boldsymbol{\hat{\mu}}_{k}^{p},
%% \end{equation}
%% where the updated Gaussian at each frame~$p$ are computed as $\boldsymbol{\hat{\mu}}_{k}^{p} = \boldsymbol{A}^{p} \boldsymbol{\mu}_k^{p} + \boldsymbol{b}^{p}$ and ${\boldsymbol{\hat{\Sigma}}_{k}^{p} = \boldsymbol{A}^{p} \boldsymbol{\Sigma}_k^{p} \boldsymbol{A}^{p^\mathsf{T}}}$. 
%% As a result, the global components are more influenced by the respective frames associated with each key point.



%% %========================================
%% \begin{figure}[t!]
%%     \centering
%%     \includegraphics[width=0.48\linewidth]{figures/branch_svm_2p_box.png}
%%     \includegraphics[width=0.48\linewidth]{figures/branch_svm_2p_cap.png}
%%     %--------------------
%%     \caption{Choice of branches based on the proposed branch selector. 
%%     Captions follow Fig.~\ref{fig:branch_gaussian}.
%%     The separation of different branches are correct and intuitive.}
%%     \label{fig:branch_svm}
%%     %--------------------
%%     \vspace{-0.15cm}
%% \end{figure}
%% %========================================

%==========
%\subsubsection{Branch Selector}\label{subsubsec:branch-class}


To overcome this limitation, 
we propose a \emph{branch selector} as an extension to the original TP-HSMM model~$\boldsymbol{\theta}_{\mathsf{a}}$ in Sec.~\ref{subsec:tp-hsmm}. 
Consider a skill primitive $\mathsf{a}$ with $M$ demonstrations and $B$ different branches. 
Furthermore, each execution trajectory or demonstration of the skill is denoted by 
$\mathsf{J}_m=\big[\mathbf{s}_t\big]_{t=1}^{T_m},$
which is associated with \emph{exactly} one branch $b_m\in B_{\mathsf{a}}=\{1,\cdots, B\}$. 
Denote by~$\boldsymbol{J}_{\mathsf{a}}$ the set of such trajectories,
initialized to be the set of demos~$\boldsymbol{D}_{\mathsf{a}}$.
As mentioned in Sec.~\ref{subsec:tp-hsmm}, 
the frames associated with~$\mathsf{J}_m$ are computed from the initial state $\mathbf{s}_0$, 
by abstracting the coordinates of the robot and the objects, denoted by:
\begin{equation}\label{eq:frames}
(\mathbf{F}_0,\,\mathbf{F}_1,\cdots,\mathbf{F}_P),
\end{equation}
where $\mathbf{F}_p=(\mathbf{b}^{p},\mathbf{A}^{p})$ is the coordinate of frame $p\in \{1,\cdots,P\}$;
their order can be freely chosen but fixed afterwards.
Then, we can derive the \emph{feature} vector:
\begin{equation}\label{eq:skill-feature}
\mathbf{v}_m = \left( \mathbf{F}_{01},\,\mathbf{F}_{12},\cdots,\mathbf{F}_{(P-1)P}\right),
\end{equation}
where $\mathbf{F}_{ij}=(\mathbf{b}_{ij},\,\boldsymbol{\alpha}_{ij})\in \mathbb{R}^6$ is the relative transformation from frame $\mathbf{F}_i$ to frame $\mathbf{F}_j$:
$\mathbf{b}_{ij}\in \mathbb{R}^3$ is the relative pose and $\boldsymbol{\alpha}_{ij}\in \mathbb{S}^3$ is the relative orientation.
Thus, given $\boldsymbol{J}_{\mathsf{a}}$, we can construct the \emph{training data} for the branch selector:
\begin{equation}\label{eq:branch-data}
\boldsymbol{\tau}_{\mathsf{a}}^{\texttt{B}}=\left\{ (\mathbf{v}_m,\,b_m),\,\forall \mathsf{J}_m\in \boldsymbol{J}_{\mathsf{a}}\right\},
\end{equation}
where $b_m$ is the branch label of trajectory $\mathsf{J}_m$;
$\mathbf{v}_m$ is the associated feature vector.
Then the branch selector, denoted by $\mathcal{C}_{\mathsf{a}}^{\texttt{B}}$, can be learned via any multi-nomial classification algorithms.
As described in Sec.~\ref{subsec:classification}, logistic regression under the ``one-vs-rest'' strategy yields an effective selector from few training samples.
More comparisons are given in Sec.~\ref{sec:experiments}.
Afterwards, given a new scenario with state $\mathbf{s}_t$,
the probability of choosing branch $b$ is given by:
\begin{equation}\label{eq:branch-prob}
\rho_{b} = \mathcal{C}_{\mathsf{a}}^{\texttt{B}}(\mathbf{s}_t,\, b), \;\forall b \in B_{\mathsf{a}},
\end{equation}
where $\rho_{b}\in [0,\,1]$.
Since most skills contain two or three frames, 
the feature vector $\mathbf{v}_m$ in~\eqref{eq:skill-feature} normally has dimension $6$ or $12$.
Fig.~\ref{fig:branch_compare} illustrates much better classification results compared with the Gaussian preconditions~\cite{rozo2020learning}.


%==============================
\subsection{Task Network Construction}\label{subsec:task-network}

As mentioned in Sec.~\ref{sec:introduction}, complex manipulation tasks often contain various sequences of skills to account for different scenarios. 
A high-level abstraction of such relations is referred as task networks~\cite{hayes2016autonomously}.
A valid plan evolves by transition from one skill to another until the task is solved. 
%% Different sequences can share some skills and one skill may appear multiple times in the same sequence. 
Even though the graph structure can be sketched easily, 
the \emph{conditions} on these transitions are particularly difficult and tedious to specify manually. 
%Often it is required to modify these conditions whenever the workspace or the goal specification is changed. 
To overcome this limitation,
we propose a novel coordination structure as geometric task networks (GTNs)~\cite{guo2021geometric}, 
where the conditions are learned from the task execution results.


%========================================
\begin{figure}[t!]
    \centering
    \includegraphics[width=0.98\linewidth]{figures/GTN.png}
    %--------------------
    \caption{The proposed network structure over primitive skills, 
      where the conditions for selecting edges and branches are critical for the execution.}
    \label{fig:gtn}
    %--------------------
    \vspace{-0.15cm}
\end{figure}
%========================================

\subsubsection{Network Structure}\label{subsubsec:gtn-structure}
As illustrated in Fig.~\ref{fig:gtn}, a GTN has a simple structure defined by the triple $\mathcal{G}=(V,\, E,\, f)$.
The set of nodes~$V$ is a subset of the primitive skills $\mathsf{A}$;
the set of edges $E \subseteq V \times V$ contains the allowed transitions from one skill to another;
the function $f: v \rightarrow \mathcal{C}$ maps each node to a edge selector w.r.t. all of its outgoing edges.
Intuitively, $(V,\, E)$ specifies how skills can be executed consecutively for the given task,
while function $f(v)$ models the different geometric constraints among the objects and the robot,
for the outgoing edges of node $v$.
Note that $f(\cdot)$ is \emph{explicitly} conditioned on both the current system state and the goal state. 
Its detailed model is given in the sequel.

%% As mentioned in Sec.~\ref{sec:tamp-review}, this conjugate structure of task network is already used in numerous previous work such as~\cite{huang2019neural, hayes2016autonomously, konidaris2012robot}.
%% However, the representation of function $f(\cdot)$ and its learning process proposed here differ significantly from these aforementioned work.


\subsubsection{Learning from Complete Plans}\label{subsubsec:gtn-learn}
Without loss of generality, a complete plan of the considered problem in Problem~\ref{main-problem} is given by the following sequence:
\begin{equation}\label{eq:complete-plan}
\boldsymbol{\xi} =\underline{\mathsf{a}} \mathbf{s}_0 \,\mathsf{a}_0\,\mathbf{s}_1 \,\mathsf{a}_1\, \mathbf{s}_2 \cdots \mathbf{s}_G \overline{\mathsf{a}}, 
\end{equation}
where~$\underline{\mathsf{a}}$ and $\overline{\mathsf{a}}$ are virtual ``start'' and ``stop'' skills, respectively.
For different initial and goal states (i.e., problems) of the same task, the resulting plans can be different. 
Denoted by $\boldsymbol{\Xi}=\{\boldsymbol{\xi}\}$ the set of complete plans for a set of given problems.
Then, for each ``action-state-action'' triple $(\mathsf{a}_n, \mathbf{s}_{n+1}, \mathsf{a}_{n+1})$ within $\boldsymbol{\xi}$,
first, the pair $(\mathsf{a}_{n},  \mathsf{a}_{n+1})$ is added to the edge set $\widehat{E}$ if not already present; 
second, for each \emph{unique} skill transition $(\mathsf{a}_{n},\mathsf{a}_{n+1})$, 
a set of \emph{augmented} states is collected, denoted by~$\widehat{\mathbf{s}}_{\mathsf{a}_{n}\mathsf{a}_{n+1}}=\{\widehat{\mathbf{s}}\}$, 
where $\widehat{\mathbf{s}}= (\mathbf{s}_{n+1},\mathbf{s}_G)$.
Furthermore, for each augmented state $\widehat{\mathbf{s}}_\ell=(\mathbf{s}_t,\mathbf{s}_G)\in \widehat{\mathbf{s}}_{\mathsf{a}_{n}\mathsf{a}_{n+1}}$, 
we derive the following \emph{feature} vector:
\begin{equation}\label{eq:edge-feature}
\mathbf{h}_\ell = (\mathbf{h}_{tG},\, \mathbf{v}_G),\,
\end{equation}
where $\mathbf{h}_{tG}=(\boldsymbol{H}_{\mathsf{r}},\,\boldsymbol{H}_{\mathsf{o}_1},\cdots,\boldsymbol{H}_{\mathsf{o}_H})$, 
where $\boldsymbol{H}_{\mathsf{o}}=(\mathbf{b}_{\mathsf{o}},\,\boldsymbol{\alpha}_{\mathsf{o}})\in \mathbb{R}^6$ is the relative translation and rotation of robot $\mathsf{r}$ and objects $\mathsf{o}_1,\mathsf{o}_2,\cdots, \mathsf{o}_H \in \mathsf{O}_{\mathsf{a}_n}$, 
{from} the current system state $\mathbf{s}_t$ {to} the goal state $\mathbf{s}_G$;
$\mathbf{v}_G$ is the feature vector defined in~\eqref{eq:skill-feature} associated with the goal state $\mathbf{s}_G$.
Note that $\mathbf{h}_\ell$ encapsulates features from both the relative transformation to the goal, and the goal state itself. 
Its dimension is linear to the number of objects relevant to skill $\mathsf{a}_{n}$, as shown in Fig.~\ref{fig:frames}.



Once all plans within $\boldsymbol{\Xi}$ are processed, the GTN $\mathcal{G}$ can be constructed as follows.
First, its nodes and edges are directly derived from $\widehat{E}$.
Then, for each node $\mathsf{a}$, the set of its outgoing edges in $\widehat{E}$ is given by $\widehat{E}_{\mathsf{a}}=\{(\mathsf{a}, \mathsf{a}_\ell) \in \widehat{E}\}$.
Thus the function $f(\mathsf{a})$ returns the edge selector $\mathcal{C}_{\mathsf{a}}^{\texttt{E}}$ over $\widehat{E}_{\mathsf{a}}$.
To compute this selector, we first construct the following training data:
\begin{equation}\label{eq:node-data}
\boldsymbol{\tau}^{\texttt{E}}_{\mathsf{a}} = \left\{ (\mathbf{h}_\ell,\, e),\, \forall \widehat{s}_\ell \in  \widehat{\mathbf{s}}_e,\, \forall e\in \widehat{E}_{\mathsf{a}} \right\},
\end{equation}
where $e$ is the label for an edge $e = (\mathsf{a},\,\mathsf{a}_\ell)\in \widehat{E}_{\mathsf{a}}$;
$\mathbf{h}_\ell$ is the feature vector derived from~\eqref{eq:edge-feature}.
Then the edge selector $\mathcal{C}_{\mathsf{a}}^{\texttt{E}}$ can be learned via the multi-nomial classification algorithms presented in Sec.~\ref{subsec:classification}.
Similar to~\eqref{eq:branch-prob}, 
given a new scenario with state $\mathbf{s}_t$ and the specified goal state $\mathbf{s}_G$,
the probability of choosing edge $e$ is given by:
\begin{equation}\label{eq:edge-prob}
\rho_{e} = \mathcal{C}_{\mathsf{a}}^{\texttt{E}}\left((\mathbf{s}_t,\,\mathbf{s}_G),\, e\right), \;\forall e \in \widehat{E}_{\mathsf{a}},
\end{equation}
where $\rho_{e}\in [0,\,1]$.
Note that $\rho_{e}$ is trivial for skills with only one outgoing edge.


%==============================
\subsection{Human-in-the-loop Policy Learning and Execution}\label{subsec:learn-policy}
The previous sections present the methods to learn the extended skill model and the task network.
The required training data are execution trajectories of the skill in~\eqref{eq:frames} and complete plans of the task in~\eqref{eq:complete-plan}.
In this section, we show how human instructions during run time can be used to generate such data, 
and thus to learn or improve both the skill model and the task network \emph{on-the-fly}. 


%% %========================================
%% \begin{algorithm}[t]
%%    \caption{Execute and Update GTN: $\texttt{ExUpGtn}(\cdot)$} \label{alg:update-GTN}
%%    \LinesNumbered
%%    \DontPrintSemicolon
%%    \KwIn{$\mathcal{G}$, $\mathsf{a}_n$, $(\mathbf{s}_n, \mathbf{s}_G)$, human input $a_{n+1}^\star$ if required.}
%%    \KwOut{Updated $\mathcal{G}$, next skill $\mathsf{a}_{n+1}^\star$.}
%%         \If{$\{\rho_e(\mathbf{s}_n, \mathbf{s}_G)>\underline{\rho}^{\texttt{E}},\forall e\in \widehat{E}_{\mathsf{a}_n}\}=\emptyset$}{
%%         Request human input as $\mathsf{a}^\star_{n+1}$.\;
%%         Add edge $(\mathsf{a}_n,\,\mathsf{a}^\star_{n+1})$ and update $\mathcal{G}$.\;
%%         Extend $\boldsymbol{\tau}_{\mathsf{a}_n}^{\texttt{E}}$ by~\eqref{eq:human-input-1} and update $\mathcal{C}^{\texttt{E}}_{\mathsf{a}_n}$.\;
%%         }
%%         Choose the next skill $\mathsf{a}_{n+1}^\star$ by~\eqref{eq:next-skill}.\;
%% \end{algorithm}
%% %========================================


%========================================
\subsubsection{Execute and Update GTN}\label{subsubsec:update-gtn}
The GTN $\mathcal{G}$ is initialized as empty.
Consider a problem instance of the task, namely~$(\mathbf{s}_0, \mathbf{s}_G)$.
The system starts from state $\mathbf{s}_n$ whereas the GTN starts from the virtual start node $\mathsf{a}_n=\underline{\mathsf{a}}$ for $n=0$. 
Then the associated edge selector $\mathcal{C}_{\mathsf{a}_n}^{\texttt{E}}$ is used to compute the probability $\rho_e$ of each outgoing edge $e\in \widehat{E}_{\mathsf{a}_n}$ based on~\eqref{eq:edge-prob}.
Then, the next skill to execute is chosen as:
\begin{equation}\label{eq:next-skill}
\mathsf{a}^\star_{n+1}=
\underset{e\in \widehat{E}_{\mathsf{a}_n}}{\mathrm{argmax}}
\;\left\{\rho_e(\mathbf{s}_n,\,\mathbf{s}_G),\, \text{where}\; \rho_e>\underline{\rho}^{\texttt{E}}\right\},\;
\end{equation}
where $\underline{\rho}^{\texttt{E}}>0$ is a design parameter as the lower confidence bound.
Note that if the current set of outgoing edges is empty, i.e., $\widehat{E}_{\mathsf{a}_n}=\emptyset$, 
{or} the maximal probability of all edges is less than $\underline{\rho}^{\texttt{E}}$, 
the human operator is asked to \emph{input} the preferred next skill $\mathsf{a}^\star_{n+1}$. 
Consequently, an \emph{additional} data point is added to the training data $\boldsymbol{\tau}_{\mathsf{a}_n}^{\texttt{E}}$, i.e., 
\begin{equation}\label{eq:human-input-1}
\boldsymbol{\tau}_{\mathsf{a}_n}^{\texttt{E}} \leftarrow \left( \mathbf{h}(\mathbf{s}_n,\,\mathbf{s}_G),\,(\mathsf{a}_n,\, \mathsf{a}^\star_{n+1}) \right),
\end{equation}
where the feature vector $\mathbf{h}$ is computed based on~\eqref{eq:edge-feature}.
Thus, a new edge $(\mathsf{a}_n,\, \mathsf{a}^\star_{n+1})$ is added to the graph topology $(V, E)$ if not present, 
and the embedded function $f(\cdot)$ is updated by re-learning the edge selectors $\mathcal{C}_{\mathsf{a}_n}^{\texttt{E}}$ given this new data point. 


%% %========================================
%% \begin{algorithm}[t]
%%    \caption{Execute and Update Branch Selector: $\texttt{ExUpBrs}(\cdot)$} \label{alg:update-branch-selector}
%%    \LinesNumbered
%%    \DontPrintSemicolon
%%    \KwIn{$\mathcal{C}^{\texttt{B}}_{\mathsf{a}_{n+1}}$, $\mathbf{s}_n$, human input $b_{n+1}^\star$ if required.}
%%    \KwOut{Updated $\mathcal{C}^{\texttt{B}}_{\mathsf{a}_{n+1}}$, optimal branch $b^\star_{n+1}$.}
%%          \If{$\{\rho_{b}(\mathbf{s}_n) > \underline{\rho}^{\texttt{B}},\forall b\in B_{\mathsf{a}_{n+1}}\}= \emptyset$}{
%%          Request human input as $b_{n+1}^\star$.\;
%%          Extend $\boldsymbol{\tau}_{\mathsf{a}_{n+1}}^{\texttt{B}}$ by~\eqref{eq:human-input-2} and update $\mathcal{C}_{\mathsf{a}_{n+1}}^{\texttt{B}}$.\;
%%          }
%%          Choose the optimal branch $b_{n+1}^\star$ by~\eqref{eq:next-branch}.\;
%% \end{algorithm}
%% %========================================
%% %
%========================================
\begin{algorithm}[t]
   \caption{HIL Coordination of LfD Skills} \label{alg:hil-coordination}
   \LinesNumbered
   \DontPrintSemicolon
   \KwIn{$\{\boldsymbol{D}_{\mathsf{a}}, \, \forall \mathsf{a} \in \mathsf{A}\}$, human inputs $\{\mathsf{a}_n^\star, b_n^\star\}$.}
   \KwOut{$\mathcal{G}$, $\{\mathcal{C}^{\texttt{B}}_{\mathsf{a}}\}$, $\boldsymbol{u}^\star$.}
   \tcc{offline learning}
   Learn $\boldsymbol{\theta}_{\mathsf{a}}$ and $\{\mathcal{C}^{\texttt{B}}_{\mathsf{a}}\}$, $\forall \mathsf{a} \in \mathsf{A}$.\;
   Initialize or load existing $\mathcal{G}$.\;
   \tcc{online execution and learning}
   \While{new task $(\mathbf{s}_0, \mathbf{s}_G)$ given}{
       Set $\mathsf{a}_n \leftarrow\underline{\mathsf{a}}$ and $\mathbf{s}_n \leftarrow\mathbf{s}_0$.\;
       \While{$\mathbf{s}_n \neq \mathbf{s}_G$}{
             $\mathcal{G},\, \mathsf{a}_{n+1}=\texttt{ExUpGtn}(\mathcal{G},\, \mathsf{a}_{n},\, (\mathbf{s}_n,\, \mathbf{s}_G),\, \mathsf{a}^\star_{n+1})$.\;
         $ \mathcal{C}^{\texttt{B}}_{\mathsf{a}_{n+1}},\,b_{n+1}=\texttt{ExUpBrs}(\mathcal{C}^{\texttt{B}}_{\mathsf{a}_{n+1}},\, \mathbf{s}_n,\, b^\star_{n+1})$.\;
             Compute $\boldsymbol{u}^\star$ for branch $b_{n+1}$ of skill $\mathsf{a}_{n+1}$.\;
             Obtain new state $\mathbf{s}_{n+1}$. Set $n\leftarrow n+1$.\;
       }
   }
\end{algorithm}
%========================================
%

%========================================
\subsubsection{Execute and Update Branch Selector}\label{subsubsec:update-branch}
%% Before the online execution, the skill model~$\boldsymbol{\theta}_{\mathsf{a}}$ is first learned based on the demonstrations as described in Sec.~\ref{subsec:tp-hsmm} and Sec.~\ref{subsec:learn-primitive}.

Now assume that $\mathsf{a}_{n+1}$ is chosen as the next skill. 
Then the branch selector $\mathcal{C}_{\mathsf{a}_{n+1}}^{\texttt{B}}$ is used to predict the probability of each branch $\rho_{b}$, $\forall b\in B_{\mathsf{a}_{n+1}}$
Then, the most likely branch for $\mathsf{a}_{n+1}$ is chosen by:
\begin{equation}\label{eq:next-branch}
b^\star_{n+1}=
\underset{b \in B_{\mathsf{a}_{n+1}}}{\mathrm{argmax}} \;\left\{\rho_{b}(\mathbf{s}_n),\, \text{where} \, \rho_{b} > \underline{\rho}^{\texttt{B}} \right\},
\end{equation}
where $\underline{\rho}^{\texttt{B}}>0$ is another parameter as the lower confidence bound
for the branch selection.
Note that if no branch is found above, then the human operator is asked to input the preferred branch $b^\star_{n+1}$ for skill $\mathsf{a}_{n+1}$.
Consequently, an \emph{additional} data point is added to the training data $\boldsymbol{\tau}_{\mathsf{a}_{n+1}}^{\texttt{B}}$, i.e., 
\begin{equation}\label{eq:human-input-2}
\boldsymbol{\tau}^{\texttt{B}}_{\mathsf{a}_{n+1}} \leftarrow \left( \mathbf{v}(\mathsf{s}_n),\,b^\star_{n+1} \right),
\end{equation}
where the feature vector $\mathbf{v}$ is computed based on~\eqref{eq:skill-feature}.


%========================================
\subsubsection{Skill Execution}\label{subsubsec:skill-execution}
Once the optimal branch $b^\star$ is chosen for the desired next skill $\mathsf{a}^\star_{n+1}$, 
its trajectory can be retrieved given the skill model~$\boldsymbol{\theta}_{\mathsf{a}_{n+1}}$. 
The retrieval process consists of two steps: First, 
the most-likely sequence of GMM components  within the \emph{desired branch} (denoted by $\boldsymbol{k}^\star_T$) can be computed via the modified Viterbi algorithm proposed in our earlier work~\cite{rozo2020learning}.
Then, a reference trajectory is generated by an optimal controller (e.g., LQG from~\cite{sciavicco2012modelling}) to track this sequence of Gaussians in the task space.
%% , i.e.,
%% \begin{equation}\label{eq:lqr}
%% \begin{split}
%% &\boldsymbol{u}^\star = \;\underset{\boldsymbol{u}}{\text{argmin}}\, \left( \sum_{t=1}^T \big(\boldsymbol{\delta}_t^{\intercal}\hat{\boldsymbol{\Sigma}}^{-1}_{k_t} \boldsymbol{\delta}_t + \boldsymbol{u}_t^{\intercal}\boldsymbol{R}\boldsymbol{u}_t\big)\right),\\
%% & \text{s.t.}\; \boldsymbol{\delta}_{t} = \boldsymbol{x}_t-\hat{\boldsymbol{\mu}}_{k_t},\; \boldsymbol{x}_{t+1} = g(\boldsymbol{x}_t,\, \boldsymbol{u}_t);
%% \end{split}
%% \end{equation}
%% where $(\hat{\boldsymbol{\mu}}_{k_t}, \hat{\boldsymbol{\Sigma}}_{k_t})$ is the $k_t$-th global component from~\eqref{eq:gaussian-product};
%% $\boldsymbol{x}_t$, $\boldsymbol{u}_t$ are the state and control of system $g(\cdot)$, 
%% which can be a virtual second order integrator or the actual dynamics of the manipulator;
%% and $\boldsymbol{R}$ is the control penalty.
Thus this reference trajectory is then sent to the low-level impedance controller to compute the control signal $\boldsymbol{u}^\star$.
More algorithmic details and extension to Riemannian manifolds are given in~\cite{rozo2020learning}. 


Afterwards, the system state is changed to $\mathbf{s}_{n+2}$ with different poses of the robot and the objects, i.e., obtained from  the  state estimation and perception modules. 
Given this new state, the same process is repeated to choose the next skill and its optimal branch, until the goal state is reached.



%========================================
\subsection{Overall Algorithm}\label{subsubsec:overall-algo}
The overall procedure is summarized in Alg.~\ref{alg:hil-coordination}.
Namely, during the online execution for solving new tasks, 
Sec.~\ref{subsubsec:update-gtn} is followed to execute and update the GTN, 
namely the function $\texttt{ExUpGtn}(\cdot)$ in Line 6, 
with possible human input $\mathsf{a}^\star_n$ if required. 
Once the next skill $\mathsf{a}_{n+1}$ is chosen, 
Sec.~\ref{subsubsec:update-branch} is followed to execute and update the branch selector, 
namely the function $\texttt{ExUpBrs}(\cdot)$ in Line 7, 
with possible human input $b^\star_{n+1}$ if required.
Consequently the GTN and branch selectors are updated and improved via~\eqref{eq:human-input-1} and~\eqref{eq:human-input-2} {on-the-fly}.
%% Such a human-in-the-loop framework is particularly useful when the scenarios during execution are different from the demonstrations, 
%% but the same set of skills can still be used. 
Compared with the manual specification of the transition and branching conditions,  
the required human inputs above are more intuitive and easier to specify. 

Lastly, the computation complexity of the logistic regression is $\mathcal{O}(d^2)$, where $d$ is the dimension of the feature vector.
The skill model learning and the LQG control algorithm have polynomial complexity to the robot state dimension. 
