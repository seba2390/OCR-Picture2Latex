\section{Preliminaries}\label{sec:preliminary}



%========================================
\subsection{Multi-nomial Classification}\label{subsec:classification}
Multi-nomial or multi-class classification is the problem of classifying instances into one of several classes, see~\cite{bishop2006pattern}.
More precisely, consider the training data $\{(\boldsymbol{y}_m, k_m)\}$ where $\boldsymbol{y}_m\in \mathbb{R}^N$ is the feature vector and $k_m\in \{1,\cdots,K\}$ is the set of possible classes. 
Our goal is to learn a classifier $f:\mathbb{R}^N \rightarrow \{1,\cdots,K\}$ that $f(\boldsymbol{y})$ predicts the most likely class of a new point $\boldsymbol{y}\in \mathbb{R}^N$.
Various algorithms have been proposed such as support vector machines, logistic regression, $k$-nearest neighbors, naive Bayes and neural networks, see~\cite{bishop2006pattern} for details.
An extremely data-efficient yet effective method is to use logistic regression along with the \emph{one-vs-rest} strategy, by maximizing the likelihood of correctly classifying all training data.
More specifically, the probability of $\boldsymbol{y}_m$ belonging to class $k_m$ is given by
\begin{equation}\label{eq:log}
p(k_m \,|\, \boldsymbol{y}_m) = \sigma(\boldsymbol{\beta}^{\intercal} \boldsymbol{y}_m),
\end{equation}
where $\sigma(t)=1/(1+e^{-t})$ is the logistic function for $t\in \mathbb{R}$;
$\boldsymbol{\beta}\in \mathbb{R}^N$ is the model parameter as the weight vector.

Logistic regression is used here (instead of SVMs and DNNs) as: 
(i) it requires much less training data;
(ii) it offers probabilistic measures that can be used as confidence in a decision;
(iii) the decision model is more interpretable as the weighted combination of the feature variables.


%========================================
\subsection{Skill Model as TP-HSMM}\label{subsec:tp-hsmm}
As proposed in~\cite{calinon2016tutorial, Schwenkel2019Optimizing,rozo2020learning}, 
the skill model~$\boldsymbol{\theta}_{\mathsf{a}}$ of a primitive skill~$\mathsf{a}$ as the TP-HSMM is defined as:
%% can be learned 
%% as the task-parameterized Hidden semi-Markov Model (TP-HSMM) that encapsulates both temporal and spatial property of all human demonstrations. 
%% HSMM extends the standard HMM by embedding temporal information of the underlying stochastic process. 
%% This means that a transition to the next component depends on the current component as well as on the elapsed time since it was entered.
%More specifically, a TP-HSMM is defined as:
\begin{equation}\label{eq:tp-hsmm}
\boldsymbol{\theta}_{\mathsf{a}} = \left\{ \{a_{hk}\}_{h=1}^K,\, (\mu_k^D, \sigma_k^D),\, \boldsymbol{\gamma}_k \right\}_{k=1}^K,
\end{equation}
where $a_{hk}$ is the transition probability from component~$h$ to component~$k$; 
$(\mu_k^D, \sigma_k^D)$ describe the duration Gaussian model of component~$k$; 
and $\boldsymbol{\gamma}_k$ is component~$k$ of a TP-GMM:
%\begin{equation}\label{eq:tp-gmm}
$\boldsymbol{\gamma} = \{\pi_k,\{\boldsymbol{\mu}_k^{p},\boldsymbol{\Sigma}_k^{p}\}_{p=1}^P\}_{k=1}^K$,
%\end{equation}
where $K$ represents the number of Gaussian components in the mixture model, 
$\pi_k$ is the prior probability of each component, 
and $\{\boldsymbol{\mu}_k^{p},\boldsymbol{\Sigma}_k^{p}\}_{p=1}^P$ are the mean and covariance of the $k$-th Gaussian component within frame~$p$. 
Frames provide observations of the \emph{same} data but from different perspectives, the instantiation of which is called a {task parameter}.
%Differently from standard GMM used in~\cite{niekum2015learning}, the mixture model above can not be learned independently for each frame. 
%Indeed, the mixing coefficients $\pi_k$ are shared by all frames and the \mbox{$k$-th} component in frame $p$ must map to the corresponding \mbox{$k$-th} component in the global frame. 
An Expectation-Maximization (EM) method from~\cite{dempster77em} is used to optimize this model.
