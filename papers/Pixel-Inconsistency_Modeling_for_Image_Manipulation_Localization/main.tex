
%\documentclass[10pt,journal,compsoc,draftcls,onecolumn]{IEEEtran}
\documentclass[10pt,journal,compsoc]{IEEEtran}

\usepackage{hyperref}       % hyperlinks
\usepackage{flushend}       % balances columns at the end
\usepackage{todonotes}      % todo annotations 
\usepackage{setspace}       % allows doublespacing in reviewing
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{graphicx}
\usepackage{multirow}
\usepackage{graphicx}
\usepackage{caption}
\usepackage{amssymb}
\usepackage{pifont}
\usepackage{color}
\usepackage{bm}
\usepackage{float}
%\doublespacing

\ifCLASSOPTIONcompsoc
  \usepackage[nocompress]{cite}
\else
  % normal IEEE
  \usepackage{cite}
\fi
\ifCLASSINFOpdf
\else
\fi

\hyphenation{op-tical net-works semi-conduc-tor}

 
\begin{document}
\title{Pixel-Inconsistency Modeling for Image Manipulation Localization}
\author{Chenqi~Kong,~\IEEEmembership{Member,~IEEE,}
        Anwei~Luo,       Shiqi~Wang,~\IEEEmembership{Senior~Member,~IEEE,}
        Haoliang~Li,~\IEEEmembership{Member,~IEEE,}
        Anderson~Rocha,~\IEEEmembership{Senior~Member,~IEEE,} and~Alex~C.~Kot,~\IEEEmembership{Life~Fellow,~IEEE}% <-this % stops a space

\IEEEcompsocitemizethanks{\IEEEcompsocthanksitem {C. Kong and A. Kot are with the Rapid-Rich Object Search (ROSE) Lab, School of Electrical and Electronic Engineering, Nanyang Technology University, Singapore, 639798. \protect\\ 
E-mail: chenqi.kong@ntu.edu.sg, eackot@ntu.edu.sg.}
\IEEEcompsocthanksitem {A. Luo is with the School of Computer Science and Engineering, Sun Yat-sen University, Guangzhou, China. He is also with the Rapid-Rich Object Search (ROSE) Lab, School of Electrical and Electronic Engineering, Nanyang Technology University, Singapore, 639798. \protect\\ 
E-mail: luoanw@mail2.sysu.edu.cn.}
\IEEEcompsocthanksitem {S. Wang is with the Department of Computer Science, City University of Hong Kong, Hong Kong. \protect\\ 
E-mail: shiqwang@cityu.edu.hk.}
\IEEEcompsocthanksitem {H. Li is with the Department of Electrical Engineering, City University of Hong Kong, Hong Kong. \protect\\ 
E-mail: haoliang.li@cityu.edu.hk.}
\IEEEcompsocthanksitem {A. Rocha is with the Artificial Intelligence Laboratory (Recod.ai), Institute of Computing, University of Campinas, Campinas 13084-851, Brazil \protect\\ E-mail: arrocha@unicamp.br}
\IEEEcompsocthanksitem {Corresponding author: Haoliang Li.}
}}
% \thanks{ Manuscript received ... }
% \IEEEcompsocitemizethanks{\IEEEcompsocthanksitem M. O. Gani, T. Fayezeen,and S. I. Ahamed are with the Department of Mathematics, Statistics, and Computer Science, Marquette University, Milwaukee, WI, USA 53233.\protect\\
% E-mail: md.gani@mu.edu, taskina.fayezeen@mu.edu, sheikh.ahamed@mu.edu}
% \IEEEcompsocitemizethanks{\IEEEcompsocthanksitem R. J. Povinelli is with the Department of Electrical and Computer Engineering, Marquette University, Milwaukee, WI, USA 53233.\protect\\
% E-mail: richard.povinelli@mu.edu}


\markboth{Journal of \LaTeX\ Class Files,~Vol.~14, No.~8, August~2015}%
{Shell \MakeLowercase{\textit{et al.}}: Bare Demo of IEEEtran.cls for Computer Society Journals}


\IEEEtitleabstractindextext{%
\begin{abstract}
Digital image forensics plays a crucial role in image authentication and manipulation localization. Despite the  progress powered by deep neural networks, existing forgery localization methodologies exhibit limitations when deployed to unseen datasets and perturbed images (i.e., lack of generalization and robustness to real-world applications). To circumvent these problems and aid image integrity, this paper presents a  generalized and robust manipulation localization model through the analysis of pixel inconsistency artifacts. The rationale is grounded on the observation that most image signal processors (ISP) involve the demosaicing process, which introduces pixel correlations in pristine images. Moreover, manipulating operations, including splicing, copy-move, and inpainting, directly affect such pixel regularity. We, therefore, first split the input image into several blocks and design masked self-attention mechanisms to model the global pixel dependency in input images. Simultaneously, we optimize another local pixel dependency stream to mine local manipulation clues within input forgery images. In addition, we design novel Learning-to-Weight Modules (LWM) to combine features from the two streams, thereby enhancing the final forgery localization performance. To improve the training process, we propose a novel Pixel-Inconsistency Data Augmentation (PIDA) strategy, driving the model to focus on capturing inherent pixel-level artifacts instead of mining semantic forgery traces. This work establishes a comprehensive benchmark integrating 15 representative detection models across 12 datasets. Extensive experiments show that our method successfully extracts inherent pixel-inconsistency forgery fingerprints and achieve state-of-the-art generalization and robustness performances in image manipulation localization.
\end{abstract}

% Note that keywords are not normally used for peerreview papers.
\begin{IEEEkeywords}
Image forensics, image manipulation localization, image manipulation detection, generalization, robustness.
\end{IEEEkeywords}}
\maketitle

\IEEEdisplaynontitleabstractindextext
\IEEEpeerreviewmaketitle

\IEEEraisesectionheading{\section{Introduction}\label{sec:introduction}}


% During in-camera processing, filters and lens are employed. Filters are utilized to eliminate undesired light, while lens focus light onto the sensor. Sequentially, the Color Filter Array (CFA) is applied to extract red-green-blue color components. Following this, a series of software operations will be carried out. Demosaicing, also known as color interpolation, is performed to reconstruct full-color pixels from surrounding single-color pixels. Some internal processing steps, such as color correction, noise reduction, and compression, will be sequentially conducted to produce the final processed RGB image. During out-camera processing, malicious attackers can use image editing tools for image manipulation. These manipulations can destroy the pixel correlation introduced by the demosaicing operation, leaving specific pixel inconsistency artifacts for forensics analyses.


\IEEEPARstart{I}{mage} manipulation has been carried out since photography was born \cite{image_mani}. In recent decades, there has been significant advances in image manipulation techniques, including splicing, copy-move, and inpainting, which are three pervasive but notorious attack types \cite{verdoliva2020media}, as shown in Fig.~\ref{showcase_dist}. These techniques can produce forgery content with a very high level of realism, blurring the boundaries between authentic and forgery images. Manipulation traces are very subtle and can hardly be perceived by the naked eye. With the widespread use of digital images on the internet, it has become much easier for malicious attackers to launch manipulation attacks using off-the-shelf yet powerful image editing tools, such as Photoshop, After Effects Pro, GIMP, and more recently, Firefly. The produced sophisticated content can be used to commit fraud, generate fake news, and blackmail people. Image manipulation certainly undermines the trust in media content. Moreover, the proliferation of fakes has raised pressing security concerns for the public \cite{kong2023digital}. Therefore, designing effective image forgery localization models to address these issues is paramount.

Early attempts at image manipulation localization mainly focused on extracting features based on prior knowledge, such as lens distortions \cite{mayer2018accurate, yerushalmy2011digital, johnson2006exposing, yerushalmy2011digital, gloe2010efficient, fu2012forgery}, Color Filter Array (CFA) artifacts \cite{cao2009accurate, ferrara2012image, popescu2005exposing, gallagher2008image, ho2010inter}, noise patterns \cite{lyu2014exposing, kobayashi2010detecting, popescu2004statistical, mahdian2009using, cozzolino2015splicebuster, fan2013estimating}, compression artifacts \cite{fan2003identification, chen2011detecting, iakovidou2018content, barni2010identification, bianchi2012image, fu2007generalized, pasquini2017statistical}. However, these traditional methods demonstrate limited accuracy and generalizability. In turn, learning-based detectors have been proposed thanks to recent advancements in deep learning and artificial intelligence. These methods exhibit promising performance in image forgery localization under the intra-domain setting. Nonetheless, data-driven methods are typically prone to overfitting the training data, resulting in limited robustness and generalization performance. Namely, they are fragile to image perturbations and vulnerable to unseen image manipulation datasets. 

\begin{figure}[ht]
\centering
\includegraphics[scale=0.32]{ example.png}
\caption{Illustration of manipulation types: splicing, copy-move, and inpainting. The top, middle, and bottom rows show the real, forgery, and ground-truth images.}
\label{showcase_dist}
\end{figure}

Extracting inherent forgery fingerprints for generalized and robust image forgery localization remains a challenging problem. This paper recasts the typical image manipulation pipeline and proposes a new forgery localization framework that captures the pixel inconsistencies in manipulated images. Fig.~\ref{demosaicing} shows the typical forgery image construction chain. The filter and lens eliminate undesired light and focus light onto the sensor. Subsequently, the Color Filter Array (CFA) is applied to extract single-color components. A series of software operations is carried out during the in-camera processing. Demosaicing, also known as color interpolation, is performed to reconstruct full-color pixels from surrounding single-color pixels. Some internal processing steps, such as color correction, noise reduction, and compression, are subsequently conducted to generate the final processed RGB image. In turn, malicious attackers can utilize image editing tools to manipulate pristine images during the out-camera processing. These manipulations can disrupt such pixel correlation (i.e., perturb the periodic patterns) introduced by the demosaicing operation, leaving distinctive pixel inconsistency artifacts for forensics analyses \cite{cao2009accurate, popescu2005exposing, verdoliva2020media}. 

Fig.~\ref{CFA} showcases four typical CFA types: (a). Bayer CFA, (b). RGBE, (c). CMY, and (d). CMYG. Color filtering allows the capture of a specific color at each pixel. Consequently, in the resulting RAW image, only one color is present at each pixel, and the demosaicing process reconstructs the missing color samples. Some existing forensics analysis techniques for forgery fingerprint extraction focus on mathematically modeling different image regularities. For instance, Popescu $et~al.$ \cite{popescu2005exposing} quantifies the specific correlations introduced by CFA interpolation and describes how these correlations can be automatically detected. Ferrara $et~al.$ \cite{ferrara2012image} proposes a novel feature that measures the presence or absence of these image regularities at the smallest 2$\times$2 block level, thus predicting a forgery probability map. In \cite{cao2009accurate2} and \cite{li2016color}, the intra-block fingerprint is modeled using a linear regression approach. Despite the effectiveness of these pixel correlation modeling approaches in forensic analysis, most require knowledge of the CFA type as prior information. Furthermore, these methods cannot sufficiently capture more complex regularities introduced by smart image signal processors (ISPs) in modern AI cameras \cite{AIcamera}. 

% Because image manipulation processes such as splicing, inpainting and copy-move will destroy pixel correlation (e.g., by introducing manipulation artifacts or inconsistencies in image regularities), it is reasonable to model the pixel correlation as the intra-block fingerprint for further image manipulation detection purposes. Hence, we aim to model pixel correlation based on a masked autoencoder, where the input of the masked autoencoder is an image block. Specifically, we will design a novel masking scheme for convolution by leveraging the neighboring pixel samples to predict the distribution of the current pixel. 

% To further model the fingerprint of the image construction pipeline based on Bayer filters [2], which are the most commonly used CFA in digital camera.

% To model the complex image regularities of ISP driven by
% deep learning algorithms, we draw inspiration from the blind-spot network [50], which models
% pixel correlation on the conditional pixel distribution, where each pixel in an image can in
% turn be modeled by conditioning on all the previous pixel values in raster scan order. Although
% some research in manipulation detection (e.g., [21]) has applied the blind-spot network to
% model pixel correlation in a specific sequential manner (i.e., pixel xi only conditioned on the
% previous pixels x<i scanned row by row and pixel by pixel within every row)

\begin{figure}[ht]
\centering
\includegraphics[scale=0.24]{ demosaicing2.png}
\caption{Typical forgery image construction pipeline.}
\label{demosaicing}
\end{figure}

\begin{figure}[ht]
\centering
\includegraphics[scale=0.30]{ CFA.png}
\caption{Typical Color Filter Array (CFA) types. (a). Bayer CFA; (b). RGBE; (c). CMY; (d). CMYG.}
\label{CFA}
\end{figure}

Different from the prior arts, we propose a learning-based method to capture inherent pixel inconsistencies within forged images based on this insight. We design a two-stream pixel-dependency modeling framework for image manipulation localization to achieve this. Drawing inspiration from recent success of autoregressive models (e.g., PixelCNN \cite{van2016pixel, van2016conditional}) in various computer vision tasks, we design a masked self-attention mechanism to model the global pixel dependency within input images. Furthermore, we design a Pixel Difference Convolution (PDC) stream to capture local pixel inconsistency artifacts within local image regions. In addition, we introduce a novel Learning-to-Weight Modules (LWM) to combine global and local pixel-inconsistency features from these two streams.

We design three decoders to predict the potential manipulated regions, forgery boundaries, and reconstructed images. We finally introduce the Pixel-Inconsistency Data Augmentation (PIDA) strategy to explore the pixel-level forgery traces. PIDA is an effective approach that relies upon only real images for data augmentation. It guides the model to focus on capturing pixel-inconsistency artifacts rather than semantic forgery traces. The designed framework is trained end-to-end, jointly supervised by the binary mask and boundary labels.


\begin{figure*}[ht]
\centering
\includegraphics[scale=0.495]{ framework5.png}
\caption{Proposed image manipulation localization framework. The input image is split into several patches, which are simultaneously fed forward to the 
Local Pixel Dependency Encoder and Global Pixel Dependency Encoder. The upper stream comprises four Pixel Difference Convolution (PDC) blocks to capture local pixel inconsistencies in forged images. Meanwhile, the Global Pixel Dependency Encoder, which incorporates four masked self-attention (Masked SA) blocks, focuses on modeling long-range statistics within the input images. Four Learning-to-Weight Modules (LWM) have been devised to combine global and local features extracted by the two encoders. The Forgery Decoder and Boundary Decoder take the aggregated features as inputs and predict the final forgery and boundary maps.  
}
\label{framework}
\end{figure*}

The key contributions of our work are:
\begin{itemize}
    \item We establish a comprehensive benchmark assessing the generalization capabilities of 15 representative image forgery localization methods across 12 datasets. We also extend this benchmark to evaluate the robustness performance across six unseen image perturbation types, each with nine severity levels.
    \item We design a two-stream image manipulation localization framework comprising a local pixel dependency encoder, a global pixel dependency encoder, four feature fusion modules, and three decoders. The proposed model can effectively extract the pixel-inconsistency forgery fingerprints, leading to more generalized and robust manipulation localization performance.  
    \item We introduce a Pixel-Inconsistency Data Augmentation strategy that exclusively utilizes real images to create the generated data. The proposed data augmentation drives the model to focus on capturing the inherent pixel-level artifacts rather than the semantic forgery clues, contributing to a forgery localization performance boost. 
    \item Extensive quantitative and qualitative experimental results demonstrate that our proposed method consistently outperforms state-of-the-art in generalization and robustness evaluations. 
\end{itemize}

Sec. 2 overviews prior work in image forgery localization and pixel dependency modeling. Sec. 3  elaborates on the designed framework. Sec. 4 presents comprehensive evaluation results under diverse experimental settings. Finally, Sec. 5 concludes this paper and discusses current limitations and possible future research directions.


% The major hinderances in developing general and robust models are lacking proper benchmarks and unified evaluation protocols. In this vein, we build a suite of benchmarks for pixel-level and image-level manipulation detection to facilitate developing more powerful detectors. 



\section{Related Work}
In this section, we broadly review existing works on image forgery detection and localization, including both handcrafted and learning-based methodologies. Additionally, we review the studies related to pixel dependency modeling and their applications. 

\subsection{Manipulation detection and localization methods using low-level traces}  Image manipulation detection is no new problem. Early methods focus on detecting low-level artifacts derived from in-camera processing traces. For example, lens distortions \cite{mayer2018accurate, yerushalmy2011digital, johnson2006exposing, yerushalmy2011digital, gloe2010efficient, fu2012forgery}, introduced by the imperfection of complex optical systems, can be regarded as unique fingerprints for forensics purposes. Chromatic aberration is a typical lens distortion cue widely studied for forgery detection \cite{johnson2006exposing, yerushalmy2011digital, mayer2018accurate}. Besides, many methods \cite{ferrara2012image, popescu2005exposing, cao2009accurate, cao2009accurate2} propose to capture color filter array (CFA) artifacts to detect manipulations. These techniques demonstrated that manipulation operations can disrupt periodic patterns introduced by the demosaicing process. Additionally, since photo-response nonuniformity (PRNU) is specific to each camera model, some methods \cite{lyu2014exposing, mahdian2009using, kobayashi2010detecting} extract noise patterns from query images for detecting digital tampering traces. 
Furthermore, extensive research has been dedicated to studying JPEG compression artifacts that persist in the discrete cosine transform (DCT) domain \cite{chen2011detecting, farid2009exposing, fan2003identification, bianchi2012image, pasquini2017statistical} for forgery detection. While these traditional image manipulation detection methods are explainable and computationally efficient, most suffer from poor detection accuracy and limited generalization. 
To achieve an accurate, generalized, and interpretive image forgery localization, we introduce a learning-based framework in this work designed to capture low-level pixel inconsistency artifacts.

\subsection{Learning-based Manipulation detection and localization methods}
Recent years have witnessed significant progress in image forensics, with various learning-based methods proposed to solve the forgery localization problem, which subtanstially improved detection performances. Many of these methods leverage a wide range of prior knowledge, such as noise telltales~\cite{cozzolino2019noiseprint, guillaro2022trufor, zhou2018learning}, CFA artifacts~\cite{bammey2020adaptive}, and JPEG features~\cite{kwon2022learning, rao2022towards, wang2022jpeg} to perform the forgery detection. High-frequency (HF) filters~\cite{zhuo2022self, li2019localization}, such as steganalysis rich
model (SRM) filter~\cite{zhou2018learning, wu2019mantra} and Bayer filter~\cite{dong2022mvss, wu2019mantra} have also been used to capture abundant HF forgery artifacts. Besides, detecting the forgery boundary~\cite{dong2022mvss, salloum2018image} has effectively improved pixel-level forgery detection performance. In turn, some methods~\cite{dong2022mvss, liu2022pscc, gao2022generic, ferreira2016behavior, luo2023beyond} utilize multi-scale learning to extract forgery features from different levels, thereby achieving increased detection accuracy. Thanks to the advent of vision transformer (ViT), ViT-based detectors~\cite{wang2022objectformer, lin2023image, yu2023msvt} take advantage of long-range interaction and no inductive bias, yielding outstanding detection performance in different problems, including forensics. However, these data-driven methods suffer from limited generalization and robustness capability.
This paper argues that pixel inconsistency within forgery images represents a more ubiquitous artifact across different manipulations and datasets. As such, we devise a novel image forgery localization framework that captures pixel inconsistency artifacts to achieve more generalized and robust forgery localization performance.


\subsection{Pixel Dependency Modeling}
Autoregressive (AR) models \cite{larochelle2011neural, germain2015made, van2016conditional, salimans2017pixelcnn++, parmar2018image, chen2018pixelsnail, child2019generating} have achieved remarkable success across various computer vision tasks, including image generation \cite{germain2015made, van2016conditional, kolesnikov2017pixelcnn}, completion \cite{chen2018pixelsnail, jain2020locally, parmar2018image}, and segmentation \cite{ouali2020autoregressive}. These AR methods aim to model the joint probability distribution of each pixel as follows:
% \begin{equation}
%      {\hat{x}_{\pi(i)}} \sim p_{\theta}(x_{\pi(i)}|x_{\pi(1)},...,x_{\pi(i-1)})
% \end{equation}
\begin{equation}
     {\hat{x}_{i}} \sim p_{\theta}(x_{i}|x_{1},...,x_{i-1}).
\end{equation}
These models employ specific mask convolution or mask self-attention strategies, such that the probability distribution of the current pixel depends on all previous pixels in the generation order. Pioneering AR models like PixelCNN \cite{van2016conditional} and PixelRNN \cite{van2016pixel} demonstrate their effectiveness in modeling long-range pixel dependencies for natural images in the context of image generation. Follow-up variations, such as PixelCNN++ \cite{salimans2017pixelcnn++}, have been introduced to enhance image generation performance further. Furthermore, masked self-attention can also aid dependency modeling, such as image transformer \cite{parmar2018image} and sparse transformer \cite{child2019generating}. PixelSNAIL \cite{chen2018pixelsnail} combines causal convolutions with self-attention, improving image generation. Inspired by the success of pixel-dependency modeling in various generative tasks, we seek to extend upon this concept to the domain of forensic analysis. This paper introduces novel pixel-difference convolutions and masked self-attention mechanisms to capture local and global pixel inconsistency artifacts. 
% Our proposed method can greatly enhance generalization and robustness within image manipulation localization.

\begin{figure}[ht]
\centering
\includegraphics[scale=0.4]{ maskatt4.png}
\caption{(a). Illustration of the proposed masked attention mechanism. $\otimes$ indicates the matrix multiplication. \textbf{Q}, \textbf{K}, and \textbf{V} stand for Query, Key and Value. We designed the Raster-scan mask to model the pixel dependency. (b). The mask and corresponding pixel scan ordering. The green squares indicate the value `1' while the red squares indicate the value `0'.}
\label{maskatt}
\end{figure}


\section{Proposed Method}
This section presents the proposed manipulation localization method. We first introduce the overall framework. Subsequently, we delve into the details and underlying rationales of the designed components, including the Global Pixel Dependency Modeling Module, the Local Pixel Dependency Modeling Module, and the Learning-to-Weight Module. Lastly, we introduce the proposed Pixel-Inconsistency Data Augmentation strategy and its advantages. 

\subsection{Overall Framework}
As Fig.~\ref{framework} depicts, this paper designs a two-stream image manipulation localization framework, which draws inspiration from the observation that manipulation processes, such as splicing, copy-move, and inpainting, inevitably disrupt the pixel regularity introduced by the demosaicing operation. The framework relies upon a Local Pixel Dependency Encoder and a Global Pixel Dependency Encoder to explore pixel inconsistency and context for manipulation localization. The input image is firstly split into  patches [$p_{1}$, $p_{2}$, ... $p_{n}$], which are then concurrently processed by the two encoders.
 
To explore long-range interaction and no inductive bias, we adopt transformer architectures as backbones of the two streams. The upper Local Pixel Dependency Encoder comprises four Pixel Difference Convolution (PDC) Blocks designed to capture pixel inconsistencies in local regions. 
In turn, we introduce a Global Pixel Dependency Encoder comprising four novel masked self-attention blocks. The designed masked self-attention mechanism models global pixel dependencies within input images. Additionally, we design four Learning-to-Weight Modules (LWM) to complementarily combine global features [$f_{g1}$, $f_{g2}$, $f_{g3}$, $f_{g4}$] and local features [$f_{l1}$, $f_{l2}$, $f_{l3}$, $f_{l4}$] at multiple levels. The designed framework also incorporates a Boundary Decoder, a Forgery Decoder, and an Image Decoder. 

Notably, pixel inconsistency is most prominent in the boundary region. We, therefore, integrate the boundary auxiliary supervision to enhance the final forgery localization performance. The Forgery Decoder takes the combined features [$f_{1}$, $f_{2}$, $f_{3}$, $f_{4}$] as inputs to predict potential manipulated regions of input images, while the Image Decoder takes [$f_{g1}$, $f_{g2}$, $f_{g3}$, $f_{g4}$] as inputs and aims to reconstruct the original input image.  
Finally, we propose a novel Pixel-Inconsistency Data Augmentation (PIDA) strategy that focuses on pixel inconsistency rather than semantic forgery traces. This strategy further enhances the model's generalization and robustness capabilities. 
 
 % novel Pixel Dependency Encoder to complementarily capture the pixel inconsistency within local patches. This design draws inspiration from the observation that manipulation processes, such as splicing, copy-move, and inpainting, inevitably disrupt pixel regularity introduced by the in-camera demosaicing operation. In this encoder, we design causal attention mechanism by masking the attention matrix to be lower triangular. Causality is widely used in training autoregressive models, enabling the output at a given position depends only on the past information. This feature can be naturally used in our image manipulation localization framework to model the pixel dependency. 
 
 
 


\subsection{Global Pixel Dependency Modeling}
We introduce masked attention blocks, in a style similar to the self attention, into pixel global dependency modeling. Fig.~\ref{framework} depicts the combination of the global pixel dependency encoder and the image decoder that forms an auto-encoder. Fig.~\ref{maskatt} (a) illustrates the details of the proposed masked self-attentions and the corresponding mask design, with \textbf{Q}, \textbf{K}, and \textbf{V} representing Query, Key, and Value, respectively. $z^{i}$ and $z^{i+1}$ indicate the input and output features. $\otimes$ denotes the matrix multiplication operator. The masked self-attention mechanism can be formulated as:  
\begin{equation}
     z^{i+1} = {\textbf{\rm Mask}} [{\rm softmax}(\frac{f_{query}(z^{i})f_{key}(z^{i})^\top} {\sqrt{d}})]f_{value}(z^{i}),
\end{equation}
where $f_{query}(\cdot)$, $f_{key}(\cdot)$, and $f_{value}(\cdot)$ represent the learnable parameters, and ${f_{query}(z^{i})}$, ${f_{key}(z^{i})}$, and ${f_{value}(z^{i})}$ are equivalent to \textbf{Q}, \textbf{K}, and \textbf{V}. As Fig.~\ref{maskatt} (b) shows, we employ a raster-scan mask to model the global pixel dependency, corresponding to the raster-scan sampling ordering for the input image \cite{ouali2020autoregressive}. If we name the feature vectors of the feature map $z^{i}$ as $z^{i}_{1}$, $z^{i}_{2}$,..., $z^{i}_{N}$. Then for $z^{i+1}$=attention($z^{i}$), the output feature $z^{i+1}$ of the proposed masked attention mechanism can be rewritten as:  
\begin{equation}
     z^{i+1}_{m} = \sum_{n<m} p_{mn} f_{value}(z^{i}_{n}),
\end{equation}
where elements $p_{mn}$ in row $m$ can be formulated as:
\begin{footnotesize}
\begin{equation}
     p_{m} = {\rm softmax}[f_{key}(z^{i}_{1})^{\top}f_{query}(z^{i}_{m}), ... ,f_{key}(z^{i}_{m-1})^{\top}f_{query}(z^{i}_{m})],
\end{equation}
\end{footnotesize}
Each conditional can access any pixel within its context through the attention operator, as indicated by the summation over all available context, denoted as $\sum_{n<m}$. This design enables the access of far-away pixels, thereby enhancing the modeling of long-range statistics. As such, the probability distribution of each pixel in features [$f_{g1}$, $f_{g2}$, $f_{g3}$, $f_{g4}$] can be estimated as: 
\begin{equation}
     {\hat{x}_{j}} \sim p_{\theta}(x_{j}|x_{1},...,x_{j-1}).
\end{equation}
Given pixels $x_{1}$,...,$x_{j-1}$ in the input image, we can sample $x_{j}$ using $p_{\theta}(x_{j}|x_{<j})$. The extracted features [$f_{g1}$, $f_{g2}$, $f_{g3}$, $f_{g4}$] can carry abundant global pixel dependency information. Experimental results demonstrate that the captured pixel correlations between real and manipulated images are distinctive for image forgery localization. 


% where $W_{Q}$, $W_{K}$, and $W_{V}$ represent the learnable parameters. As shown in Fig.~\ref{maskatt}, we design two masks to model the pixel dependency, corresponding to two sampling orderings. In this work, we apply raster-scan ordering and Zigzag ordering to the input image \cite{ouali2020autoregressive}. If we denote the sampling ordering as $\pi(\cdot)$, the probability distribution of each pixel in features [$f_{c1}$, $f_{c2}$, $f_{c3}$, $f_{c4}$] can be estimated by: 
% \begin{equation}
%      {\hat{x}_{\pi(i)}} \sim p_{\theta}(x_{\pi(i)}|x_{\pi(1)},...,x_{\pi(i-1)})
% \end{equation}
% By using the proposed causal self-attention mechanism, the extracted features [$f_{c1}$, $f_{c2}$, $f_{c3}$, $f_{c4}$] will carry abundant information of pixel correlations. Experimental results demonstrate that the captured pixel correlations between real and manipulated images are distinctive for detection. 


\subsection{Local Pixel Dependency Modeling}
% In the proposed framework, the upper stream is dedicated to exposing forgery artifacts by modeling patch correlations, while the pixel dependency encoder focuses on capturing pixel inconsistencies in manipulated images. The Feature Fusion Modules (FFM) play a vital role in complementarily fusing two-stream features at different scales, thereby enhancing the overall generalization and robustness of image manipulation localization. In this subsection, we elaborate the architecture details of FFM. As shown in Fig.~\ref{FFM}. (a). The feature $f_{t}$ is firstly fed forward to two pixel difference convolution modules: Central Pixel Difference Convolution (CPDC) and Radial Pixel Difference Convolution (RPDC).

% In the proposed framework, the Feature Fusion Modules (FFM) play a vital role in complementarily fusing two-stream features at multiple scales, thereby enhancing the overall generalization and robustness performance. The architecture details of FFM is shown in Fig.~\ref{FFM} (a). The feature $f_{l}^{'}$ will be firstly fed forward to two pixel difference convolution modules: Central Pixel Difference Convolution (CPDC) and Radial Pixel Difference Convolution (RPDC).

According to the nature of demosaicing algorithms, the pixel correlation regularity of a given pixel largely depends on its neighboring pixels \cite{cao2009accurate, cao2009accurate2}. Moreover, the pixel regularity can be modeled by linear demosaicing formulas \cite{cao2009accurate, li2016color}. However, these traditional methods exhibit limited forgery detection performance. Inspired by \cite{yu2020searching, su2021pixel, liu2012extended}, we propose to model the local pixel dependency by integrating the traditional demosaicing ideas into convolutional operations. In the Local Pixel Dependency Encoder, we place Pixel Difference Convolution (PDC) heads on the top of each transformer block, as they can naturally model the pixel dependency in local image regions in a learning-based fashion. 

\begin{figure}[ht]
\centering
\includegraphics[scale=0.29]{ FFM3.png}
\caption{(a). Pixel Difference Convolution (PDC) head; (b). Details of Central Pixel Difference Convolution (CPDC) and Radial Pixel Difference Convolution (RPDC); (c). Details of Learning-to-Weight Module (LWM).}
\label{FFM}
\end{figure}

Fig.~\ref{FFM} (a). depicts the architecture of the designed Pixel Difference Convolution (PDC) head. The feature $f_{l}^{'}$ is firstly fed forward to two 
pixel difference convolution modules: Central Pixel Difference Convolution (CPDC) and Radial Pixel Difference Convolution (RPDC). By exploiting CPDC and RPDC, the local pixel dependencies can be effectively modeled, enhancing the final forgery localization performance. Fig.~\ref{FFM}. (b) presents the details of CPDC and RPDC. We first calculate the pixel difference within local feature map regions for a given input feature map. Then, we respectively convolve the two pixel-difference feature maps with the corresponding convolutional weights, resulting in CPDC and RPDC feature maps. The CPDC operation can be formulated as:
\begin{equation}
     f_{l}^{C} = \sum_{(x_{i},x_{c})\in {\Omega}} {w_{i}(x_{i}-x_{c})}.
\end{equation}
Here, $x_{c}$ represents the center pixel in the local region $\Omega$, and $x_{i}$ represents the corresponding surrounding pixels. The $w_{i}$ values represent learnable convolutional weights. Similarly, the RPDC operation can be expressed as:
\begin{equation}
     f_{l}^{R} = \sum_{(x_{i},x_{i}^{'})\in {\Omega}} {w_{i}(x_{i}-x_{i}^{'})},
\end{equation}
where $x_{i}$ and $x_{i}^{'}$ are pixel pairs in region ${\Omega}$, as illustrated in Fig.~\ref{FFM} (b). 

We complementarily combine CPDC features $f_{l}^{C}$ and RPDC features $f_{l}^{R}$ using a Learning-to-Weight Module (LWM), which shall be elaborated in Sec. 3.4.
The proposed model aims at extracting pixel-difference features, thereby exposing more pixel inconsistency artifacts and boosting the final image forgery localization performance.  

\begin{figure*}[ht]
\centering
\includegraphics[scale=0.345]{ Blending3.png}
\caption{(a). Pixel-Inconsistency Data Augmentation pipeline. \ding{192} For a given real pristine image $I_{p}$, we firstly apply common image perturbations to obtain the perturbed image $I_{c}$; \ding{193} We use built-in OpenCV function to extract the foreground mask $M$ of $I_{p}$; The Blending Module takes $I_{p}$, $I_{c}$, and $M$ as inputs and outputs the self-blended forge image $I_{b}$. \ding{194} The boundary label $B$ of the manipulated image can be obtained from $M$. (b). Details of the blending module in (a). The output blended image is the combination of the donor image's foreground and the target image's background.}
\label{Selfblending}
\end{figure*}

\subsection{Learning-to-Weight Module}
As Fig.~\ref{FFM} (a) shows, the two features $f_{l}^{C}$ and $f_{l}^{R}$ generated by CPDC and RPDC are combined and sequentially delivered to the Learning-to-Weight Module (LWM). As Fig.~\ref{framework} depicts, the local pixel-dependency features $f_{l}$ are combined with the global pixel-dependency features $f_{g}$ by using another LWM. The designed Learning-to-Weight Module (LWM) aims to fuse two input features with the learned weights, enabling a more effective feature fusion. Fig.~\ref{FFM} (c) illustrates the details of LWM, where \textbf{FC} and \textbf{EltMul} represent the fully-connected layer and element-wise multiplication. The concatenated feature goes through one average pooling layer and one \textbf{FC} layer. The learned weights $A_{1}\oplus A_{2}$ are then sequentially applied to the concatenated feature $f_{1}\oplus f_{2}$ via element-wise multiplication. Finally, the fused feature $f_{F}$ is derived by adding the concatenated feature to the weighted feature, which is  used by the boundary and forgery decoder for boundary and forgery map prediction.  



\subsection{Pixel-Inconsistency Data Augmentation}
Previous methods \cite{wang2022objectformer} mainly focus on discovering semantic-level (or object-level) inconsistencies in forgery images. Some methods \cite{dong2022mvss, zhuang2021image} also propose randomly pasting objects to pristine real images to perform data augmentation. However, as image manipulation techniques advance, forgery content's sophistication grows in tandem. Consequently, the methods designed to capture semantic-level inconsistencies struggle to generalize well to the advanced manipulations. We introduce a Pixel-Inconsistency Data Augmentation (PIDA) strategy to capture pixel-level inconsistencies instead of semantic forgery traces. Fig.~\ref{Selfblending} (a) illustrates the proposed PIDA pipeline. \ding{192} For a given real pristine image $I_{p}$, we apply image perturbations (e.g., compression, noise, and blurriness) to generate the perturbed image $I_{c}$; \ding{193} We can readily use built-in OpenCV function to extract the foreground mask $M$ of $I_{p}$. The Blending Module takes $I_{p}$, $I_{c}$, and $M$ as inputs and produces the self-blended forge image $I_{b}$; \ding{194} The boundary label $B$ of the manipulated image can be easily derived from $M$. Fig.~\ref{Selfblending} (b) details the blending module. we combine the donor image's foreground with the target image's background to generate the self-blended forgery sample.

The proposed PIDA method bears the following advantages: (1) It exclusively utilizes pristine images to generate examples of forgeries. Real data is considerably more accessible than image forgeries, facilitating training data-hungry detectors; (2) As the generated forgery samples maintain semantic consistency, the PIDA strategy directs the model's attention toward capturing pixel inconsistencies, enhancing detection performance (3) The generated forgery samples can be regarded as harder samples, effectively increasing the difficulty of the training set.

% \subsection{Feature Fusion Module}
% In the proposed framework, the upper stream is dedicated to exposing forgery artifacts by modeling patch correlations, while the pixel dependency encoder focuses on capturing pixel inconsistencies in manipulated images. The Feature Fusion Modules (FFM) play a vital role in complementarily fusing two-stream features at different scales, thereby enhancing the overall generalization and robustness of image manipulation localization. In this subsection, we elaborate the architecture details of FFM. As shown in Fig.~\ref{FFM}. (a). The feature $f_{t}$ is firstly fed forward to two pixel difference convolution modules: Central Pixel Difference Convolution (CPDC) and Radial Pixel Difference Convolution (RPDC). The two generated features $f_{t}^{C}$ and $f_{t}^{R}$ are concatenated and sequentially passed to the Learning-to-Weight Module (LWM). Finally, we employ an LWM to fuse the feature $f_{t}^{'}$ and $f_{c}$, producing the fused feature $f$. 
 
% \noindent \textbf{Pixel Difference Convolution} Inspired by \cite{yu2020searching, su2021pixel, liu2012extended}, we propose to exploit pixel difference convolutions to model the pixel difference within a local region, thereby exposing pixel inconsistencies within forgery images. Fig.~\ref{FFM}. (b) presents the details of Central Pixel Difference Convolution (CPDC) and Radial Pixel Difference Convolution (RPDC). For a given input feature map, we firstly calculate the pixel difference within local feature map regions. Then, we perform the convolve the pixel-difference feature maps with the convoluntional weights, resulting in CPDC and RPDC feature maps. The CPDC operation can be formularized as follows:
% \begin{equation}
%      f_{t}^{C} = \sum_{(x_{i},x_{c})\in {\Omega}} {w_{i}(x_{i}-x_{c})}
% \end{equation}
% Here, $x_{c}$ represents the center pixel in the local region $\Omega$, and $x_{i}$ represents the corresponding surrounding pixels. The $w_{i}$ values represent learnable convolutional weights. Similarly, the RPDC operation can be expressed as:
% \begin{equation}
%      f_{t}^{R} = \sum_{(x_{i},x_{i}^{'})\in {\Omega}} {w_{i}(x_{i}-x_{i}^{'})}
% \end{equation}
% where $x_{i}$ and $x_{i}^{'}$ are pixel pairs in region ${\Omega}$, as illustrated in Fig.~\ref{FFM} (b). 

% By complementarily combining CPDC feature $f_{t}^{C}$ and RPDC feature $f_{t}^{R}$, the proposed model can effectively extract abundant pixel-difference features, thereby exposing more pixel inconsistency artifacts and boosting the final image manipulation detection performance.  

% \noindent \textbf{Learning-to-Weight Module}
% Learning-to-Weight Module (LWM) aims to fuse two input features using the learned weights, enabling a more effective feature fusion. Fig.~\ref{FFM} (c) illustrates the details of LWM, where \textbf{FC} and \textbf{EltMul} represent the fully connected layer and element multiplication, respectively. The concatenated feature firstly goes through one average pooling layer and one \textbf{FC} layer. The learned weights are then sequentially applied to the concatenated feature via multiplication. Finally, the fused feature $f_{F}$ is derived by adding the concatenated feature to the weighted feature. 

\subsection{Objective Function}
The whole framework is trained in an end-to-end manner, and the overall objective function consists of the following four components: mask prediction loss $L_{M}$, boundary prediction loss $L_{B}$, compactness loss $L_{C}$, and image reconstruction loss $L_{R}$:
\begin{equation}
     L = L_{M} + {\lambda_{B}} L_{B} + {\lambda_{C}} L_{C} + {\lambda_{R}} L_{R},
\end{equation}
where $L_{M}$ and $L_{B}$ are cross-entropy losses between predicted results and the corresponding labels. $L_{B}$ can be considered as an auxiliary supervision for better forgery localization performance. Based on the observation that most manipulated regions are rather compact, we apply compactness constraint to predicted masks:
\begin{equation}
     L_{C} = \frac{1}{N}\sum_{{i=1}}^{N}\frac{P^{2}}{4\pi A} = \frac{1}{N}\sum_{{i=1}}^{N}\frac{\sum_{{j\in \hat{B}}}|\hat{B_{j}}^2|}{4\pi (\sum_{{k\in \hat{M}}}|\hat{M_{k}}|+\epsilon)}.
\end{equation}
In this equation, $P$ and $A$ denote the perimeter and area of the predicted forgery region, while $\hat{B}$ and $\hat{M}$ stand for the predicted boundaries and masks. Using $L_{C}$ can improve the manipulation localization performance.

The image reconstruction loss $L_{R}$ calculates the $\rm L_{1}-norm$ of the difference between the reconstructed images $\hat{I_{i}}$ and the corresponding input images $I_{i}$:
\begin{equation}
     L_{R}= \frac{1}{N}\sum_{{i=1}}^{N}
     ||I_{i}-\hat{I_{i}}||_{1}.
\end{equation}
By using $L_{R}$, the global pixel dependency can be modeled in [$f_{g1}$, $f_{g2}$, $f_{g3}$, $f_{g4}$], which is used in the LWMs for the forgery map and boundary map prediction. 

% \begin{small}
% \begin{equation}
%      L_{M}= -\frac{1}{N}\sum_{{i=1}}^{N}\sum_{{j}}\frac{1}{N_{M}}\sum_{{x,y}} 
%      (M^{x,y}_{i,j}\log\hat{M}^{x,y}_{i,j}+(1-M^{x,y}_{i,j})\log(1-\hat{M}^{x,y}_{i,j})),
% \end{equation}
% \end{small}

% \begin{small}
% \begin{equation}
%      L_{B}= -\frac{1}{N}\sum_{{i=1}}^{N}\sum_{{j}}\frac{1}{N_{B}}\sum_{{x,y}} 
%      (B^{x,y}_{i,j}\log\hat{B}^{x,y}_{i,j}+(1-B^{x,y}_{i,j})\log(1-\hat{B}^{x,y}_{i,j})),
% \end{equation}
% \end{small}

% \begin{equation}
%      L_{I}= -\frac{1}{N}\sum_{{i=1}}^{N}
%      (c_{i}\log\hat{c}_{i}+(1-c_{i})\log(1-\hat{c}_{i}))
% \end{equation}

\section{Experiments and Results}
Herein, we first introduce the datasets, evaluation metrics, as well as baseline models involved in this work. Subsequently, we evaluate our model in terms of generalization and robustness under different experimental settings. We also visualize the forgery localization results to illustrate the superiority of our method. Finally, we conduct ablation studies to demonstrate the effectiveness of the designed components.

\subsection{Datasets}  
This paper adopts 12 image manipulation datasets with varying properties, images resolutions and quality. We summarize these datasets in Table~\ref{dataset}, where CM, SP, and IP denote three common image manipulation types: copy-move, splicing, and inpainting. Consistent with previous research \cite{dong2022mvss, zhou2018learning, salloum2018image}, we utilize the CASIAv2 \cite{Dong2013} dataset as the training set due to its extensive collection of over 12,000 images with diverse contents. Furthermore, we employ the DEF-12k-val \cite{mahfoudi2019defacto} as the validation set, consisting of 6,000 challenging fake images with three forgery types and 6,000 real images collected from the MS-COCO \cite{lin2014microsoft} dataset. For the testing phase, we select 11 challenging datasets, including Columbia \cite{hsu2006columbia}, IFC \cite{IFC}, CASIAv1+\footnote{CASIAv1+ and the training set CASIAv2 share 782 identical real images. To prevent data leakage, CASIAv1+ relaces these real images with the equal number of images from COREL \cite{wang2001simplicity}.} \cite{dong2013casia}, WildWeb \cite{zampoglou2015detecting}, COVER \cite{wen2016coverage}, NIST2016 \cite{guan2019mfc}, Carvalho \cite{carvalho2015illuminant}, Korus \cite{korus2016evaluation}, In-the-wild \cite{huh2018fighting}, DEF-12k-test \cite{mahfoudi2019defacto}, and IMD2020 \cite{novozamsky2020imd2020}, sorted by released dates. In all  datasets, we uniformly label forgery regions as `1' and authentic regions as `0'.


\subsection{Evaluation metrics}
This paper evaluates state-of-the-art models' pixel-level forgery detection performances using four metrics: F1, MCC, IoU, and AUC. 

% Additionally, we utilize AUC and ACC to evaluate the image-level detection performance. 

% This paper comprehensively evaluates the pixel-level forgery detection performances of state-of-the-art models using four metrics: F1, MCC, IoU, and AUC. Additionally, we utilize AUC and ACC to evaluate the image-level detection performance. The metric details are summarized below:

\noindent\textbf{F1 Score} is a pervasive metric in binary classification, employed in image forgery detection and localization. It calculates the harmonic mean of precision and recall:   
\begin{equation}
    \label{F1}
    % \small
 F1 = 2 \cdot \frac{Precision \times Recall} {Precision + Recall} = \frac{2 \times TP} {2 \times TP + FP + FN},
\end{equation}
where $TP$, $TN$, $FP$, and $FN$ represent True Positives, True Negatives, False Positives, and False Negatives.  

\noindent\textbf{Matthews Correlation Coefficient (MCC)} measures the correlation between the predicted and true values. MCC value falls within -1 and 1, where a higher MCC indicates better  performance. The calculation of MCC is derived from the formula below:  

\begin{equation}
    \label{MCC}
    % \small
 MCC = \frac{TP \times TN - FP \times FN} {\sqrt{(TP+FP)(TP+FN)(TN+FP)(TN+FN)}}.
\end{equation}

\noindent\textbf{Intersection over Union (IoU)} is a widely used metric in semantic segmentation. 
The numerator of the IoU metric measures the area of intersection between prediction $P$ and ground-truth $G$, while the denominator calculates the area of the union between $P$ and $G$: 
\begin{equation}
    \label{IoU}
     IoU = \frac{P \cap G}{P \cup G}.
\end{equation}
 
% \noindent\textbf{Accuracy (ACC)} is a simple and straightforward metric used to evaluate classification models. It measures the proportion of correctly classified samples out of the total number of samples:  
% \begin{equation}
%     \label{acc}
%     % \small
%  ACC = \frac{TP + TN} {TP + FP + TN + FN}, 
% \end{equation}

\noindent\textbf{Area Under Curve (AUC)} measures the area under the Receiver Operating Characteristic (ROC) curve. Unlike the other metrics, the AUC does not require threshold selection. It quantifies the overall performance of the model across all possible thresholds. 

\begin{table}
  \centering
\captionof{table}{Summary of image manipulation datasets involved in this paper. CM, SP, and IP indicate three common image manipulation types: copy-move, splicing, and inpainting.}
\scalebox{0.71}{
\begin{tabular}{cccccccc}
    \toprule
    Dataset & Year & Venue & \#Real & \#Fake & \#CM & \#SP & \#IP \\
    \midrule
    CASIAv2\cite{Dong2013} & 2013 & ChinaSIP & 7,491 & 5,123 & 3,295 & 1,828 & 0 \\
    \midrule[1.5pt]
    DEF-12k-val\cite{mahfoudi2019defacto} & 2019 & EUSIPCO & 6,000  & 6,000 & 2,000 & 2,000 & 2,000 \\
    \midrule[1.5pt]
    Columbia\cite{hsu2006columbia} & 2006 & ICME & 183 & 180 & 0 & 180 & 0 \\
    \midrule
    IFC\cite{IFC} & 2013 & IFC-TC & 1050 & 450 & - & - & - \\
    \midrule
    CASIAv1+\cite{dong2013casia} & 2013 & ChinaSIP & 800 & 920 & 459 & 461 & 0 \\
    \midrule
    WildWeb\cite{zampoglou2015detecting} & 2015 & ICMEW & 99 & 9,657 & 0 & 9,657 & 0\\
    \midrule
    COVER\cite{wen2016coverage} & 2016 & ICIP & 100 & 100 & 100 & 0 & 0 \\
    \midrule
    NIST2016\cite{guan2019mfc} & 2016 & OpenMFC & 0 & 564 & 68 & 288 & 208 \\
    \midrule
    Carvalho\cite{carvalho2015illuminant} & 2016 & IEEE TIFS & 100 & 100 & 0 & 100 & 0 \\
    \midrule
    Korus\cite{korus2016evaluation} & 2016 & WIFS & 220 & 220 & - & - & - \\
    \midrule
    In-the-wild\cite{huh2018fighting} & 2018 & ECCV & 0 & 201 & 0 & 201 & 0 \\
    \midrule
    DEF-12k-test\cite{mahfoudi2019defacto} & 2019 & EUSIPCO & 6,000  & 6,000 & 2,000 & 2,000 & 2,000 \\
    \midrule
    IMD2020\cite{novozamsky2020imd2020} & 2020 & WACVW & 404 & 2010 & - & - & - \\
    \bottomrule
  \end{tabular}}
\label{dataset}
\end{table}

\begin{table*}
  \centering
  \caption{Image manipulation localization performance (\textbf{F1 score} with fixed threshold: 0.5).}
  \scalebox{0.76}{\begin{tabular}{lcccccccccccccc}
    \toprule
    Method & Venue & NIST & Columbia & CASIAv1+ & COVER & DEF-12k & IMD & Carvalho & IFC & In-the-Wild & Korus & WildWeb & AVG \\
    \midrule
    FCN \cite{long2015fully} & CVPR15 & .167 & .223 & .441 & .199 & .130 & .210 & .068 & .079 & .192 & .122 & .110 & .176 \\
    \midrule 	
    U-Net \cite{ronneberger2015u} & MICCAI15 & .173 & .152 & .249 & .107 & .045 & .148 & .124 & .070 & .175 & .117 & .056 & .129\\
    \midrule
    DeepLabv3 \cite{chen2017deeplab} & TPAMI18 & .237 & .442 & .429 & .151 & .068 & .216 & .164 & .081 & .220 & {.120} & .098 & .202\\ 
    \midrule
    MFCN \cite{salloum2018image} & JVCIP18 & .243 & .184 & .346 & .148 & .067 & .170 & .150 & .098 & .161 & .118 & .102 & .162 \\ 
    \midrule 	
    RRU-Net \cite{bi2019rru} & CVPRW19 & .200 & .264 & .291 & .078 & .033 & .159 & .084 & .052 & .178 & .097 & .092 & .139  \\ 
    \midrule
    MantraNet \cite{wu2019mantra} & CVPR19 & .158 & .452 & .187 & .236 & .067 & .164 & .255 & \underline{.117} & \underline{.314} & .110 & \underline{.224} & .208\\ 
    \midrule
    HPFCN \cite{li2019localization} & ICCV19 & .172 & .115 & .173 & .104 & .038 & .111 & .082 & .065 & .125 & .097 & .075 & .105\\ 
    \midrule
    H-LSTM \cite{bappy2019hybrid} & TIP19 & \textbf{.357} & .149 & .156 & .163 & .059 & .202 & .142 & .074 & .173 & .143 & .141 & .160\\ 
    \midrule
    SPAN \cite{hu2020span} & ECCV20 & .211 & .503 & .143 & .144 & .036 & .145 & .082 & .056 & .196 & .086 & .024 & .148 \\
    \midrule
    ViT-B \cite{dosovitskiy2020vit} & ICLR21 & .254 & .217 & .282 & .142 & .062 & .154 & .169 & .071 & .208 & \underline{.176} & .117 & .168\\ 
    \midrule
    Swin-ViT \cite{liu2021swin} & ICCV21 & .220 & .365 & .390 & .168 & {.157} & \underline{.300} & .183 & .102 & .265 & .134 & .040 & .211 \\
    \midrule
    % DenseFCN \cite{zhuang2021image} & TIFS21 & .113 & .344 & .145 & .174 & .050 & .128 & .175 & .068 & .239 & .056 & .183 & .152\\		
    % \midrule
    PSCC \cite{liu2022pscc} & TCSVT22 & .173 & .503 & .335 & .220 & .072 & .197 & \textbf{.295} & .114 & .303 & .114 & .112 & .222 \\
    \midrule
    MVSS-Net++ \cite{dong2022mvss} & TPAMI22 & \underline{.304} & \underline{.660} & \underline{.513} & \textbf{.482} & .095 & .270 & \underline{.271} & .080 & .295 & .102 & .047 & \underline{.284} \\	
    \midrule		
    CAT-NET \cite{kwon2022learning} & IJCV22 & .102 & .206 & .237 & .210 & \textbf{.206} & .257 & .175 & .099 & .217 & .085 & .170 & .179\\
    \midrule
    EVP \cite{liu2023evp} & CVPR23 & .210 & .277 & .483 & .114 & .090 & .233 & .060 & .081 & .231 & .113 & .099 & .181 \\
    \midrule[1.5pt]
    Ours & - & .280 & \textbf{.680} & \textbf{.566} & \underline{.251} & \underline{.167} & \textbf{.419} & .253 & \textbf{.155} & \textbf{.418} & \textbf{.234} & \textbf{.236} & \textbf{.333}\\
    \bottomrule
  \end{tabular}}
  \label{f1_table}
\end{table*}


\begin{table*}
  \centering
  \caption{Image manipulation localization performance (\textbf{IoU score} with fixed threshold: 0.5).}
  \scalebox{0.76}{\begin{tabular}{lcccccccccccccc}
    \toprule
    Method & Venue & NIST & Columbia & CASIAv1+ & COVER & DEF-12k & IMD & Carvalho & IFC & In-the-Wild & Korus & WildWeb & AVG \\
    \midrule
    FCN \cite{long2015fully} & CVPR15 & .114 & .177 & .367 & .117 & .089 & .158 & .043 & .058 & .140 & .089 & .084 & .131\\ 		
    \midrule 
    U-Net \cite{ronneberger2015u} & MICCAI15 & .128 & .097 & .204 & .072 & .031 & .105 & .082 & .048 & .121 & .082 & .044 & .092 \\ 		
    \midrule 
    DeepLabv3 \cite{chen2017deeplab} & TPAMI18 & .191 & .353 & .361 & .106 & .050 & .159 & .112 & .058 & .162 & .084 & .073 & .155\\ 			
    \midrule
    MFCN \cite{salloum2018image} & JVCIP18 & .193 & .123 & .291 & .100 & .050 & .124 & .103 & .074 & .112 & .083 & .080 & .121 \\ 
    \midrule 		
    RRU-Net \cite{bi2019rru} & CVPRW19 & .156 & .196 & .244 & .057 & .024 & .119 & .057 & .039 & .131 & .068 & .080 &  .106 \\ 
    \midrule 											
    MantraNet \cite{wu2019mantra} & CVPR19 & .098 & .301 & .111 & .139 & .039 & .098 & .153 & .068 & .201 & .061 & \underline{.146} & .129\\  
    \midrule 
    HPFCN \cite{li2019localization} & ICCV19 & .126 & .076 & .137 & .070 & .026 & .076 & .054 & .045 & .084 & .064 & .057 & .074\\	
    \midrule	
    H-LSTM \cite{bappy2019hybrid} & TIP19 & \textbf{.276} & .090 & .101 & .108 & .037 & .131 & .084 & .047 & .106 & .094 & .095 & .106\\ 	
    \midrule
    SPAN \cite{hu2020span} & ECCV20 & .156 & .390 & .112 & .105 & .024 & .100 & .049 & .037 & .132 & .055 & .015 & .107\\ 	
    \midrule
    ViT-B \cite{dosovitskiy2020vit} & ICLR21 & .197 & .164 & .232 & .101 & .045 & .192 & .121 & .051 & .152 & \underline{.130} & .094 & .134\\
    \midrule	
    Swin-ViT \cite{liu2021swin} & ICCV21&  .167 & .297 & .356 & .124 & {.129} & \underline{.243} & .132 & \underline{.078} & .214 & .103 & .033 & .171\\
    \midrule
    % DenseFCN \cite{zhuang2021image} & TIFS21 & .068 & .229 & .087 & .102 & .029 & .077 & .104 & .039 & .151 & .031 & .127 & .095\\ 										
    % \midrule
    PSCC \cite{liu2022pscc} & TCSVT22 & .108 & .360 & .232 & .130 & .042 & .120 & {.185} & .067 & .193 & .066 & .070 & .143\\  								
    \midrule										
    MVSS-Net++ \cite{dong2022mvss} & TPAMI22 & \underline{.239} & \underline{.573} & {.397} & \textbf{.384} & .076 & .200 & \underline{.188} & .055 & \underline{.219} & .075 & .034 & \underline{.222}\\  
    \midrule	
    CAT-NET \cite{kwon2022learning} & IJCV22 & .062 & .140 & .165 & .141 & \textbf{.152} & .183 & .110 & .062 & .144 & .049 & .107 & .120\\		
    \midrule 											
    EVP \cite{liu2023evp} & CVPR23 & .160 & .213 & \underline{.421} & .083 & .070 & .183 & .043 & .062 & .182 & .084 & .071 & .143 \\				
    \midrule[1.5pt]
    Ours & - & .225 & \textbf{.604} & \textbf{.512} & \underline{.188} & \underline{.133} & \textbf{.340} & \textbf{.194} & \textbf{.119} & \textbf{.338} & \textbf{.182} & \textbf{.193} & \textbf{.275} \\
    \bottomrule	
  \end{tabular}}
  \label{iou_table}
\end{table*}

\subsection{Baseline Models} 
This paper incorporates 15 representative baseline detectors from top journals and conferences, including five data-driven architectures and ten state-of-the-art image forgery detectors. The goal is to evaluate the detection performance of different network architectures and facilitate a head-to-head comparison. The baselines include three pervasive CNN architectures (FCN \cite{long2015fully}, U-Net \cite{ronneberger2015u}, and DeepLabv3 \cite{chen2017deeplab}) and two vision transformers (ViT-B \cite{dosovitskiy2020vit} and Swin-ViT \cite{liu2021swin}). Furthermore, this benchmark incorporates ten state-of-the-art image forgery detection models: 

\noindent\textbf{MFCN} \cite{salloum2018image} casts the image splicing localization as a multi-task problem. It exploits the two-branch FCN VGG-16 network to predict the forgery map and boundary map simultaneously.

\noindent\textbf{RRU-Net} \cite{bi2019rru} is an end-to-end ringed residual U-Net architecture specifically designed for image splicing detection. It leverages residual propagation to address the issue of gradient perturbation in deep networks effectively. By incorporating this mechanism, RRU-Net strengthens the learning process of forgery clues.

\noindent\textbf{MantraNet} \cite{wu2019mantra} is an end-to-end image forgery detection and localization framework trained on a dataset consisting of 385 manipulation types. To achieve robust image manipulation detection, MantraNet introduces a novel long short-term memory solution specifically designed to detect local anomalies.

\noindent\textbf{HPFCN} \cite{li2019localization} ensembles the ResNet blocks and a learnable high-pass filter to perform the pixel-wise inpainting localization.  

\noindent\textbf{H-LSTM} \cite{bappy2019hybrid} is a forgery detection model that integrates both a CNN encoder and LSTM networks. This combination enables the model to capture and analyze spatial and frequency domain artifacts in forgery images.

\noindent\textbf{SPAN} \cite{hu2020span} is a framework that constructs a pyramid attention network to capture the interdependencies between image patches across multiple scales. It builds upon the foundation of the pre-trained MantraNet and offers the flexibility to fine-tune its parameters on specific training sets.

\noindent\textbf{PSCC} \cite{liu2022pscc} is a progressive spatial-channel correlation network, which extracts local and global features at multiple scales with dense cross-connections. The progressive learning mechanism enables the model to predict the forgery mask in a coarse-to-fine manner, thereby empowering the final detection performance. 

\noindent\textbf{MVSS-Net++} \cite{dong2022mvss} designs a two-stream network to capture boundary and noise artifacts using multi-scale features. Incorporating two streams effectively analyzes different aspects of the image to detect manipulations at both pixel and image levels.  

\noindent\textbf{CAT-NET} \cite{kwon2022learning} is a CNN-based model that leverages discrete cosine transform (DCT) coefficients to capture JPEG compression artifacts in manipulated images. 

\noindent\textbf{EVP} \cite{liu2023evp} presents a unified low-level structure detection framework for images. ViT Adaptors and visual promptings enable the EVP model to achieve outstanding forgery localization accuracy. 

The selected manipulation methods encompass a wide variety of forgery fingerprints, such as boundary artifacts (MFCN \cite{salloum2018image}, MVSS-Net++ \cite{dong2022mvss}), multi-scale features (PSCC \cite{liu2022pscc}, MVSS-Net++ \cite{dong2022mvss}), high-frequency artifacts (HPFCN \cite{li2019localization}, MVSS-Net++ \cite{dong2022mvss}, MantraNet \cite{wu2019mantra}), and compression artifacts (CAT-NET \cite{kwon2022learning}). 

% We establish comprehensive benchmarks on the baseline methods, which offer insights into the effectiveness of different forgery features and learning mechanisms. Furthermore, it can also facilitate a head-to-head comparison between our method and previous arts.

\subsection{Implementation Details} 
Our models are implemented in PyTorch \cite{paszke2019pytorch} and trained on two Quadro RTX 8000 GPUs. The input image size is 512 $\times$ 512. We use Adam optimizer \cite{kingma2014adam} with $\beta_{1}$=0.9 and $\beta_{2}$=0.999 to train the designed model with batch size 28. The learning rate and weight decay are 6e-5 and 1e-5, respectively. The model is trained for 20 epochs and validated every 1,600 global steps. Following the experimental setting of \cite{dong2022mvss}, we train our model on CASIAv2 \cite{Dong2013} dataset and validate it on DEF-12k-val \cite{mahfoudi2019defacto} dataset. Besides the proposed Pixel-Inconsistency Data Augmentation, we follow \cite{dong2022mvss} to use common data augmentation for training, including flipping, blurriness, compression, noise, pasting, and inpainting. 


\begin{table*}
  \centering
  \caption{Image-level manipulation detection performance (F1 score with fixed threshold: 0.5).}
  \scalebox{0.76}{\begin{tabular}{lcccccccccccccc}
    \toprule
    Method & Venue &  NIST & Columbia & CASIAv1+ & COVER & DEF-12k & IMD & Carvalho & IFC & In-the-Wild & Korus & WildWeb & AVG \\
    \midrule
    FCN \cite{long2015fully} & CVPR15 & .897 & .702 & .713 & .653 & .607 & .827 & .566 & .441 & .908 & .627 & .769 & .701 \\ \midrule 	
    U-Net \cite{ronneberger2015u} & MICCAI15 & .945 & .692 & .673 & \underline{.660} & .633 & .878 & .662 & \underline{.466} & .972 & .637 & .715 & .721 \\ \midrule 		
    DeepLabv3 \cite{chen2017deeplab} & TPAMI18 & .939 & \underline{.724} & .746 &  \underline{.660} & .626 & .867 & .646 & .441 & .974 & .610 & .827 & .733 \\ \midrule												
    RRU-Net \cite{bi2019rru} & CVPRW19 & .871 & .678 & .661 & .553 & .564 & .798 & .646 & .387 & .877 & .587 & .602 & .657 \\ \midrule	
    HPFCN \cite{li2019localization} & ICCV19 & .893 & .664 & .580 & .624 & .615 & .824 & .636 & .446 & .902 & .632 & .715 & .685 \\ \midrule 	
    ViT-B \cite{dosovitskiy2020vit} & ICLR21 & .969 & .707 & .653 & \textbf{.671} & \underline{.646}  & .870  & .664  & .448 & .972 & .644 & .829 & \underline{.734} \\ \midrule 	
    PSCC \cite{liu2022pscc} & TCSVT22 & .953 & .698  & .577  & \underline{.660}  & \underline{.646}  & .866& \textbf{.674} & .463  & .972 & .649 & .812 & .725 \\ \midrule	
    MVSS-Net++ \cite{dong2022mvss} & TPAMI22 & .831 & \textbf{.735} & \underline{.758} & .659 & \underline{.646} & .863 & .613 & \textbf{.472} & .953 & .613 & .540 & .698 \\	
    \midrule				
    CAT-NET \cite{kwon2022learning} & IJCV22 & \textbf{.982} & .687 & .548 & .641 & .642 & \underline{.885} & .662 & .464  & \textbf{.992} & \textbf{.668} & .685 & .714 \\	\midrule 							
    EVP \cite{liu2023evp} & CVPR23 & .878 & .623 & .746  & .569 & .563 & .813 & .554 &  .418 & .828  & .573 & \underline{.888} & .678 \\	
    \midrule[1.5pt] 				
    Ours & - & \underline{.973}  & .702 & \textbf{.779}  & .655  & \textbf{.651} & \textbf{.896}  & \underline{.669}  & .458  & \underline{.977}  & \underline{.657} & \textbf{.932} & \textbf{.759} \\
    \bottomrule				
  \end{tabular}}
  \label{f1_img}
\end{table*}

\begin{figure*}[ht]
\centering
\includegraphics[scale=0.34]{ threshold.png}
\caption{Image forgery localization performance with varying thresholds. (a). F1; (b). MCC; (c). IoU.}
\label{threshold}
\end{figure*}

\subsection{Generalization Evaluation} 
\noindent\textbf{Pixel-level evaluation.} Localizing manipulated regions in forgery images is crucial as it provides evidence regarding the regions that have been manipulated. Predicted forgery regions can unveil the potential intents of attackers \cite{kong2022detect}. However, most detectors suffer from poor localization performance in cross-dataset evaluations due to substantial domain gaps between the training and testing sets. Herein, we evaluate the generalization capability of different detectors in terms of pixel-level forgery detection (i.e., manipulation localization).
In line with the cross-dataset evaluation protocols in \cite{dong2022mvss}, we train our model on CASIAv2 \cite{Dong2013} dataset and validate it on DEF-12k-val \cite{mahfoudi2019defacto} dataset. To facilitate a comprehensive interpretation of the results, we report two key metrics, namely F1 and IoU, in Table~\ref{f1_table} and Table~\ref{iou_table}, which have been widely used in image forgery localization. We further provide the AUC and MCC results in the Appendix. We highlight the best localization results in bold and underline the second-best results. Unlike in \cite{dong2022mvss} where optimal thresholds are determined individually for each model and dataset, we set the default decision threshold of F1, MCC, and IoU as 0.5 for the following two reasons: (1). In real-world application scenarios, it is unlikely to predefine different optimal threshold values for each testing data sample, and (2). Unifying the decision threshold as 0.5 enables us to compare all baseline models fairly. 


% \begin{table*}
%   \centering
%   \caption{Image-level manipulation detection performance (F1 score with fixed threshold: .5).}
%   \scalebox{0.76}{\begin{tabular}{lcccccccccccccc}
%     \toprule
%     Method & Venue &  NIST & Columbia & CASIAv1+ & COVER & DEF-12k & IMD & Carvalho & IFC & In-the-Wild & Korus & WildWeb & AVG \\
%     \midrule
%     FCN \cite{long2015fully} & CVPR15 & .897 \textcolor{blue}{(7)} & .702 \textcolor{blue}{(4)} & .713 \textcolor{blue}{(5)} & .653 \textcolor{blue}{(7)} & .607 \textcolor{blue}{(9)} & .827 \textcolor{blue}{(8)} & .566 \textcolor{blue}{(10)} & .441 \textcolor{blue}{(8)} & .908 \textcolor{blue}{(8)} & .627 \textcolor{blue}{(7)} & .769 \textcolor{blue}{(6)} & .701 \textcolor{blue}{(7)}\\ \midrule 	
%     U-Net \cite{ronneberger2015u} & MICCAI15 & .945 \textcolor{blue}{(5)} & .692 \textcolor{blue}{(7)} & .673 \textcolor{blue}{(6)} & .660 \textcolor{red}{(2)} & .633 \textcolor{blue}{(6)} & .878 \textcolor{red}{(3)} & .662 \textcolor{blue}{(4)} & .466 \textcolor{red}{(2)} & .972 \textcolor{blue}{(4)} & .637 \textcolor{blue}{(5)} & .715 \textcolor{blue}{(7)} & .721 \textcolor{blue}{(5)}\\ \midrule 		
%     DeepLabv3 \cite{chen2017deeplab} & TPAMI18 & .939 \textcolor{blue}{(6)} & .724 \textcolor{red}{(2)} & .746 \textcolor{red}{(3)} &  .660 \textcolor{red}{(2)} & .626 \textcolor{blue}{(7)} & .867 \textcolor{blue}{(5)} & .646 \textcolor{blue}{(6)} & .441 \textcolor{blue}{(8)} & .974 \textcolor{red}{(3)} & .610 \textcolor{blue}{(9)} & .827 \textcolor{blue}{(4)} & .733 \textcolor{red}{(3)}\\ \midrule																
%     RRU-Net \cite{bi2019rru} & CVPRW19 & .871 \textcolor{blue}{(10)} & .678 \textcolor{blue}{(9)} & .661 \textcolor{blue}{(7)} & .553 \textcolor{blue}{(11)} & .564 \textcolor{blue}{(10)} & .798 \textcolor{blue}{(11)} & .646 \textcolor{blue}{(6)} & .387 \textcolor{blue}{(11)} & .877 \textcolor{blue}{(10)} & .587 \textcolor{blue}{(10)} & .602 \textcolor{blue}{(10)} & .657 \textcolor{blue}{(11)} \\ \midrule	
%     HPFCN \cite{li2019localization} & ICCV19 & .893 \textcolor{blue}{(8)} & .664 \textcolor{blue}{(10)} & .580 \textcolor{blue}{(9)} & .624 \textcolor{blue}{(9)} & .615 \textcolor{blue}{(8)} & .824 \textcolor{blue}{(9)} & .636 \textcolor{blue}{(8)} & .446 \textcolor{blue}{(7)} & .902 \textcolor{blue}{(9)} & .632 \textcolor{blue}{(6)} & .715 \textcolor{blue}{(7)} & .685 \textcolor{blue}{(9)}\\ \midrule 	
%     ViT-B \cite{dosovitskiy2020vit} & ICLR21 & .969 \textcolor{red}{(3)} & .707 \textcolor{red}{(3)} & .653 \textcolor{blue}{(8)} & \textbf{.671} \textbf{\textcolor{red}{(1)}} & .646 \textcolor{red}{(2)} & .870 \textcolor{blue}{(4)} & .664 \textcolor{red}{(3)} & .448 \textcolor{blue}{(6)} & .972 \textcolor{blue}{(4)} & .644 \textcolor{blue}{(4)} & .829 \textcolor{red}{(3)} & .734 \textcolor{red}{(2)}\\ \midrule 	
%     PSCC \cite{liu2022pscc} & TCSVT22 & .953 \textcolor{blue}{(4)} & .698 \textcolor{blue}{(6)} & .577 \textcolor{blue}{(10)} & .660 \textcolor{red}{(2)} & .646 \textcolor{red}{(2)} & .866 \textcolor{blue}{(6)} & \textbf{.674} \textbf{\textcolor{red}{(1)}} & .463 \textcolor{blue}{(4)} & .972 \textcolor{blue}{(4)} & .649 \textcolor{red}{(3)} & .812 \textcolor{blue}{(5)} & .725 \textcolor{blue}{(4)} \\ \midrule	
%     MVSS-Net++ \cite{dong2022mvss} & TPAMI22 & .831 \textcolor{blue}{(11)} & \textbf{.735} \textbf{\textcolor{red}{(1)}} & .758 \textcolor{red}{(2)} & .659 \textcolor{blue}{(5)} & .646 \textcolor{red}{(2)} & .863 \textcolor{blue}{(7)} & .613 \textcolor{blue}{(9)} & \textbf{.472} \textbf{\textcolor{red}{(1)}} & .953 \textcolor{blue}{(7)} & .613 \textcolor{blue}{(8)} & .540 \textcolor{blue}{(11)} & .698 \textcolor{blue}{(8)}\\	
%     \midrule				
%     CAT-NET \cite{kwon2022learning} & IJCV22 & \textbf{.982} \textbf{\textcolor{red}{(1)}} & .687 \textcolor{blue}{(8)} & .548 \textcolor{blue}{(11)} & .641 \textcolor{blue}{(8)} & .642 \textcolor{blue}{(5)} & .885 \textcolor{red}{(2)} & .662 \textcolor{blue}{(4)} & .464 \textcolor{red}{(3)} & \textbf{.992} \textbf{\textcolor{red}{(1)}} & \textbf{.668} \textbf{\textcolor{red}{(1)}} & .685 \textcolor{blue}{(9)} & .714 \textcolor{blue}{(6)}\\	\midrule 							
%     EVP \cite{liu2023evp} & CVPR23 & .878 \textcolor{blue}{(9)} & .623 \textcolor{blue}{(11)} & .746 \textcolor{red}{(3)} & .569 \textcolor{blue}{(10)} & .563 \textcolor{blue}{(11)} & .813 \textcolor{blue}{(10)} & .554 \textcolor{blue}{(11)} &  .418 \textcolor{blue}{(10)} & .828 \textcolor{blue}{(11)} & .573 \textcolor{blue}{(11)} & .888 \textcolor{red}{(2)} & .678 \textcolor{blue}{(10)}\\	
%     \midrule[1.5pt] 				
%     Ours & - & .973 \textcolor{red}{(2)} & .702 \textcolor{blue}{(4)} & \textbf{.779} \textbf{\textcolor{red}{(1)}} & .655 \textcolor{blue}{(6)} & \textbf{.651} \textbf{\textcolor{red}{(1)}} & \textbf{.896} \textbf{\textcolor{red}{(1)}} & .669 \textcolor{red}{(2)} & .458 \textcolor{blue}{(5)} & .977 \textcolor{red}{(2)} & .657 \textcolor{red}{(2)} & \textbf{.932} \textbf{\textcolor{red}{(1)}} & \textbf{.759} \textbf{\textcolor{red}{(1)}} \\
%     \bottomrule				
%   \end{tabular}}
%   \label{f1_img}
% \end{table*}





F1-score is the most popular metric in this field \cite{ying2023learning, qi2022principled, rao2022towards, dong2013casia}. In Table~\ref{f1_table}, our method achieves the best detection F1-score on seven datasets and the second-best performance on two datasets. In comparison to MVSS-Net++ \cite{dong2022mvss}, the proposed method achieves close to 5$\%$ average F1-score improvement, going from 28.4$\%$ to 33.3$\%$.  % average AUC score and once again achieves the best average performance, demonstrating its superior generalization performance from another perspective. MCC and IoU are two key metrics that can directly reflect the localization accuracy of different models. 
 In Table~\ref{iou_table}, our method consistently achieves the best or second-best detection performance on unseen testing datasets. Even though the 11 unseen datasets exhibit diverse distributions, our method's average IoU score outperform all previous approaches by a significant margin. The superiority of the proposed method can be attributed to its ability to capture pixel inconsistency artifacts and neighboring context, which serve as a common fingerprint across different forgery datasets. 

\noindent\textbf{Pixel-level evaluation at different thresholds.} The determination of threshold values is crucial for the final localization performance \cite{dong2022mvss}. We assess the effectiveness of our model's forgery localization across a range of threshold values from 0.1 to 0.9. We classify a pixel as a forgery if its predicted probability exceeds the specified threshold. Fig.~\ref{threshold} presents the average localization performance on the 11 unseen datasets using F1, MCC, and IoU metrics. Namely, we plot the average results under the cross-dataset setting with varying thresholds. Our proposed method consistently outperforms existing models across all thresholds, underscoring its superiority regardless of the threshold selection.


\begin{figure*}[ht]
\centering
\includegraphics[scale=0.44]{ showcase3.png}
\caption{Showcases of (a). raw real image, raw forgery image, and ground-truth mask. The corresponding six image perturbation types of the raw forgery image: (b). Brightness; (c). Contrast; (d). Darkening; (e). Dithering; (f). Pink noise; (g). JPEG2000 compression. The top, middle, and bottom rows show Severity `1', `5', and `9' for all perturbation types.}
\label{perturbation}
\end{figure*}


We observe that most detectors' performance continuously decreases with higher threshold values. This phenomenon may be attributed to subtle artifacts in challenging forgery regions, where detectors struggle to make confident decisions, resulting in reduced true positives (TP) at higher thresholds. This finding indicates the importance of selecting a lower threshold when deploying a forgery detector in real-world scenarios.

\noindent\textbf{Image-level evaluation.}
In this subsection, we further evaluate the image-level forgery detection under cross-dataset evaluation. Ideally, the tampering probability map should all be zero for a pristine real image. To this end, we employ maximum pooling on the tampering probability map and utilize the resulting output score as the overall prediction for the input image \cite{rao2022towards}. We present the key metric F1 score in Table \ref{f1_img}. We highlight the best results in bold and underline the second-best results. Notably, our method achieves the top-2 image-level detection performance on eight datasets: NIST, CASIAv1+, DEF-12k, IMD, Carvalho, In-the-Wild, Korus, and WildWeb. Even in cases where our method ranks 6th on the COVER dataset and 5th on the IFC datasets, it closely approaches the best detection results (\textbf{COVER}: \underline{Ours: .655} v.s. \underline{Best: .671}; \textbf{IFC}: \underline{Ours: .458} v.s. \underline{Best: .472}). Our method achieves the best average results, demonstrating its outstanding forgery detection generalization performance. 
% We can conclude that the pixel inconsistency artifact is a common and generalized forgery fingerprint in the image-level evaluation. 

% We can observe variations in F1 scores across different datasets, indicating significant distribution differences and the dataset-dependent performance of the methods. For instance, MVSS-Net++ \cite{dong2022mvss} achieves the highest F1 scores on the Columbia \cite{hsu2006columbia}, CASIAv1+ \cite{dong2013casia}, and COVER \cite{wen2016coverage} datasets, while SPAN \cite{hu2020span} achieves the highest F1 score on the IMD dataset. Furthermore, the AUC scores provide insights into the difficulty levels of different datasets. The IFC \cite{IFC} and WildWeb \cite{zampoglou2015detecting} datasets again exhibit to be the most challenging in terms of image-level detection. Additionally, the average F1 score enables a comparison of the overall performance across different detection methods. Based on the average scores, MVSS-Net++ \cite{dong2022mvss} demonstrates the highest detection performance, mainly benefiting from its multi-scale and multi-view learning scheme. However, the image-level F1 scores of all methods are limited, ranging from XXX to XXX, primarily due to the distribution gap among different datasets. In future research, it may be beneficial for researchers to design a dedicated image-level classifier to improve the detection performance. 


\subsection{Robustness Evaluation Results} 
Due to uncontrollable variables in real-world applications (e.g., black-box compression via social media platforms), detectors may encounter unseen image perturbations, resulting in significant performance drops. Although regular data augmentations have been considered during the training process, it is challenging to foresee all perturbation types under the deployment circumstance.

This study introduced six common image perturbations, brightness, contrast, darkening, dithering, pink noise, and JPEG2000 compression, on the CASIAv1+ \cite{dong2013casia} dataset, which was unknown during the training process. We further set nine severity levels for each perturbation type to accommodate various environmental variations.
Fig.~\ref{perturbation} showcases examples of raw images and the corresponding perturbed versions. The pixel-level AUC detection scores are illustrated in Fig.~\ref{Robustness_AUC}. The $x$ dimension indicates the severity levels, where Severity `0' indicates no perturbation applied. We can observe that all detection models suffer certain performance drops due to these unforeseen perturbation types. The proposed method consistently achieves the best AUC across different perturbation levels on all unseen perturbation types, demonstrating the robustness of our method. As most image perturbations encountered in real-world scenarios are uniformly applied to images, the pixel dependencies within unaltered images and the pixel inconsistencies within manipulated images remain consistent. Therefore, our proposed method continues to exhibit the best forgery localization performance in such robustness evaluations.  




% Generally, the detection performance consistently decreases as the distortion severity increases. 

% However, we found that no single model is robust enough against all six distortion types. For example, as shown in Fig.~\ref{Robustness_AUC}, despite ViT-B \cite{dosovitskiy2020vit} consistently achieving the best detection performance on JPEG2000 compression across nine severity levels, it still struggles with dark images. This observation suggests that different models tend to learn specific knowledge from the training data, leading to discrepancies in detection results on different distortion testing sets.
% From Fig.~\ref{Robustness_AUC}, we can observe that transformer-based models (e.g., ViT-B~\cite{dosovitskiy2020vit}, EVP~\cite{liu2023evp}) demonstrate better stability as the perturbation severity increases. Compared with CNN-based methods, vision transformers take advantage of long-term interactions and no inductive bias, thereby exhibiting a more expressive feature representation and more robust forgery localization performances. 
% We provide the MCC and IoU detection results in the supplemental materials.  
% Similar conclusions can be drawn from Fig.~\ref{f1_fig}, where the F1 score decreases dramatically even with slightly distorted data (at Severity `1'). 

\begin{figure*}[ht]
\centering
\includegraphics[scale=0.393]{ Robustness_AUC2.png}
\caption{Robustness evaluation results (AUC) on six unseen perturbation types: (a). Brightness, (b). Contrast, (c). Darkening, (d). Dithering, (e). Pink noise, (f). JPEG2000. The x-axis indicates the perturbation severity level. 
}
\label{Robustness_AUC}
\end{figure*}


\begin{table*}
  \caption{Ablation study for image manipulation localization. }
  \label{ablation}
  \centering
  \renewcommand\arraystretch{1.15}
  \scalebox{0.92}{\begin{tabular}{cccccccccccc}
    \hline
    Setting & ViT & BDD & RDA & PIDA & CPDC & RPDC & LWM & $L_{C}$ & GPDM & AVG. F1 & AVG. IoU \\
    \hline
    \hline
    1 & \checkmark & - & - & - & - & - & - & - & - & .211 & .171 \\
    2 & \checkmark & \checkmark & - & - & - & - & - & - & - & .220 & .178 \\
    % \checkmark & - & \checkmark & - & .171 & .172 & .140 & .688\\
    3 & \checkmark & \checkmark & \checkmark & - & - & - & - & - & - & .233 & .190 \\
    4 & \checkmark & \checkmark & - & \checkmark & - & - & - & - & - & .260 & .209 \\
    5 & \checkmark & \checkmark & \checkmark & \checkmark & - & - & - & - & - & .283 & .237 \\
    6 & \checkmark & \checkmark & \checkmark & \checkmark & \checkmark & - & - & - & - & .304 &  .252 \\
    7 & \checkmark & \checkmark & \checkmark & \checkmark & - & \checkmark & - & - & - & .308 & .258 \\
    8 & \checkmark & \checkmark & \checkmark & \checkmark & \checkmark & \checkmark & - & - & - & .312 & .262 \\
    9 & \checkmark & \checkmark & \checkmark & \checkmark & \checkmark & \checkmark & \checkmark & - & - & .317 & {.271}  \\
    10 & \checkmark & \checkmark & \checkmark & \checkmark & \checkmark & \checkmark & \checkmark & \checkmark & - & {.323} & .269  \\
    11 & \checkmark & \checkmark & \checkmark & \checkmark & \checkmark & \checkmark & \checkmark & \checkmark & \checkmark & \textbf{.333} & \textbf{.275} \\
    \hline
\end{tabular}}
\end{table*}

\begin{figure*}[ht]
\centering
\includegraphics[scale=0.51]{ visualization.png}
\caption{Forgery localization results on the 11 unseen test sets. The three left columns show the input images, corresponding ground-truth, and the localization results of our method. The right 10 columns present the results of SOTA methods.
}
\label{visualization}
\end{figure*}

\begin{figure*}[ht]
\centering
\includegraphics[scale=0.36]{ shuffle2.png}
\caption{Visualization results on shuffled images. (a) Input unshuffled images. (b) Forgery localization labels. (c) Forgery boundary labels. (d) Our forgery localization results. (e) Our boundary prediction results. (f) MVSS-Net++ forgery localization results. (g) Input shuffled images. (h) Shuffled forgery localization labels. (i) Shuffled forgery boundary labels.  (j) Our forgery localization results on shuffled images. (k) Our boundary prediction results on shuffled images. (l) MVSS-Net++ forgery localization results on shuffled images. 
}
\label{shuffle}
\end{figure*}

\subsection{Qualitative Experimental Results} 
In Fig.~\ref{visualization}, we qualitatively evaluate the image manipulation localization performance across 11 unseen test sets, where the leftmost three columns show the input images, the corresponding ground-truth masks, and the predicted results of our method. Besides, we show the forgery localization results of SOTA methods in the right ten columns. Our method can accurately localize the manipulated regions for forgery images with diverse image quality, scenes, occlusions, and illumination conditions. Our localization results are superior to previous methods, regardless of whether the forgery regions are relatively substantial (e.g., Columbia and WildWeb) or subtle (e.g., DEF-12k and Korus) in fake images.

The boundaries of predicted results are much sharper for the proposed method than in previous arts. This can be attributed to the global pixel dependency modeling module and local pixel difference convolution module that can highlight pixel inconsistency in forgery boundary regions. As the predicted results in CASIAv1+ and In-the-Wild show, the proposed method can successfully localize extremely subtle forgery details. This can be attributed to the local pixel difference convolution module, which allows the model to capture local pixel inconsistency artifacts. Our method maintains accurate localization performance for more challenging images, such as the one in the IMD row that contains multiple tiny forgery regions. Finally, the proposed method results in fewer false alarms, as evidenced in the predictions of COVER and NIST. This characteristic can ensure a more dependable forgery detection for real-world deployment. We provide more visualization results in the Appendix.

The qualitative experimental results demonstrate that the proposed formulation effectively deals with various challenging forgery situations. This is primarily attributed to the dedicated module designs to extract inherent pixel-level forgery fingerprints. 
% Our method's localization accuracy surpasses all previous methods, exhibiting its superiority from another point of view.  




% \begin{table*}
%   \caption{Ablation study for image manipulation localization. }
%   \label{ablation}
%   \centering
%   \renewcommand\arraystretch{1.15}
%   \scalebox{0.92}{\begin{tabular}{cccccccccccccc}
%     \hline
%     Setting & ViT & BDD & RDA & PIDA & CPDC & RPDC & LWM & $L_{C}$ & GPDM & AVG. F1 & AVG. MCC & AVG. IoU & AVG. AUC\\
%     \hline
%     \hline
%     1 & \checkmark & - & - & - & - & - & - & - & - & .211 & .204 & .171 & .704 \\
%     2 & \checkmark & \checkmark & - & - & - & - & - & - & - & .220 & .206 & .178 & .726 \\
%     % \checkmark & - & \checkmark & - & .171 & .172 & .140 & .688\\
%     3 & \checkmark & \checkmark & \checkmark & - & - & - & - & - & - & .233 & .223 & .190 & .692 \\
%     4 & \checkmark & \checkmark & - & \checkmark & - & - & - & - & - & .260 & .249 & .209 & .743 \\
%     5 & \checkmark & \checkmark & \checkmark & \checkmark & - & - & - & - & - & .283 & .274 & .237 & .762 \\
%     6 & \checkmark & \checkmark & \checkmark & \checkmark & \checkmark & - & - & - & - & .304 & .288 & .252 & .770 \\
%     7 & \checkmark & \checkmark & \checkmark & \checkmark & - & \checkmark & - & - & - & .308 & .293 & .258 & .784 \\
%     8 & \checkmark & \checkmark & \checkmark & \checkmark & \checkmark & \checkmark & - & - & - & .312 & .302 & .262 & \textbf{.797} \\
%     9 & \checkmark & \checkmark & \checkmark & \checkmark & \checkmark & \checkmark & \checkmark & - & - & .317 & .304 & {.271} & .793 \\
%     10 & \checkmark & \checkmark & \checkmark & \checkmark & \checkmark & \checkmark & \checkmark & \checkmark & - & {.323} & {.306} & .269 & .767 \\
%     11 & \checkmark & \checkmark & \checkmark & \checkmark & \checkmark & \checkmark & \checkmark & \checkmark & \checkmark & \textbf{.333} & \textbf{.318} & \textbf{.275} & .782 \\
%     \hline
% \end{tabular}}
% \end{table*}
\subsection{Visualization Results on Shuffled Images}
To demonstrate the effectiveness of the proposed model in capturing pixel inconsistency artifacts for forgery localization, we split the input image into 3$\times$3 patches and shuffle them randomly. This random shuffling effectively suppresses the semantic information within the input images and allows us to assess whether our model can still accurately localize the forgery regions. We present results for unshuffled and shuffled images in Fig.~\ref{shuffle}, denoted as (a)-(f) and (g)-(l), respectively. Columns (a)-(c) show the original input images, their respective mask, and boundary labels. 
Columns (d)-(f) present the forgery localization maps, boundary predictions, and localization results of MVSS-Net++. In this evaluation, we select the MVSS-Net++ model as the baseline as it is the SOTA method according to our experiments in Sec. 4.5. We observe that both our method and MVSS-Net++ can successfully predict manipulated regions in the unshuffled images. 

Next, we present  prediction results on shuffled images in Fig.~\ref{shuffle} (g)-(l). These randomly shuffled images inherently contain limited semantic information. In column (j), the proposed method effectively localizes the forgery regions within each patch. Column (k) showcases the predicted sharp boundaries of forgery patches. In contrast, forgery prediction results of MVSS-Net++ in column (l) reveal struggling performance, marked by numerous false alarms and undetected forgery regions. The localization results of shuffled images further demonstrate the superiority of our method. Therefore, we conclude the proposed method focuses more on pixel-level artifacts than semantic-level forgery traces.



 % On the other hand, we present the prediction results on shuffled images in Fig.~\ref{shuffle} (g)-(l). It can be observed that the randomly shuffled images can only deliver limited semantic information. Columns (i) shows that the proposed method can successfully localize the forgery region within each patch. Columns (j) presents our predicted sharp boundaries of forgery patches. We can therefore conclude that our method focuses more on pixel-level artifacts instead of semantic-level forgery traces. We further show the forgery localization results of MVSS-Net++ in column (l) for reference. Compared with our method, MVSS-Net++ shows struggling performance in forgery localization, exhibiting many false alarms and undetected forgery regions. This further demonstrates the superiority of our method.   


 % We can observe that our method can more accurately localize the manipulated regions. In addition, the localization boundary predicted by our method is more sharp than previous methods, benefiting from the designed modules that captures the pixel inconsistency in forgery images.   




\subsection{Ablation Experiments} 
In this subsection, we present comprehensive ablation studies to evaluate the effectiveness of the components designed in our framework. Table.~\ref{ablation} shows the average forgery localization performance in the cross-dataset evaluations, where `$\checkmark$' denotes the used component. 

ViT indicates the ensemble of the transformer backbone and the mask decoder. BDD denotes the utilization of the boundary decoder. RDA and PIDA represent the regular data augmentation and the proposed Pixel-Inconsistency Data Augmentation. CPDC, RPDC, and LTW stand for central pixel difference convolution, radial pixel difference convolution, and the learning to weight module, respectively. $L_{C}$ indicates the usage of the compactness loss. Lastly, GPDM represents the global pixel dependency modeling stream.

From Table.~\ref{ablation}, we can observe that using a boundary decoder can boost the forgery localization performance. A comparison between setting 3 and 4 highlights the superiority of the proposed PIDA over RDA, suggesting that PIDA encourages the detector to focus on more general artifacts. Intuitively, the combination of RDA and PIDA in setting 5 is expected to enhance pixel-level forgery detection performance, primarily because the model has been fed more data.
The CPDC and RPDC modules (settings 6-8) effectively capture local pixel difference features, contributing to enhanced localization results. Furthermore, setting 9 demonstrates the effectiveness of the LWM, which learns the weights more smartly and performs a better feature fusion. Using the compactness loss $L_C$ in setting 10 produces more compact outputs, improving the final performance. Finally, the GPDM successfully models global pixel dependency in input images while revealing pixel inconsistency artifacts in forgery images. This contributes significantly to the overall localization performance. 

In summary, the ablation studies exhibit the critical role of the designed components in our framework. The ensemble of these components jointly enhances the forgery localization performance.




% \begin{table*}
%   \centering
%   \caption{Image-level manipulation detection performance (AUC score).}
%   \scalebox{0.92}{\begin{tabular}{lcccccccccccc}
%     \toprule
%     Method & Venue & Columbia & CASIAv1+ & COVER & DEF-12k & IMD & Carvalho & IFC & Korus & WildWeb & AVG \\
%     \midrule
%     FCN \cite{long2015fully} & CVPR15 & .703 & .771 & .541 & .555 & .595 & .539 & .547 & .565 & .471 & .587\\ \midrule
%      U-Net \cite{ronneberger2015u} & MICCAI15 & .670 & .757 & .494 & .497 & .561 & .533 & .558 & .556 & .439 & .563 \\ \midrule
%     DeepLabv3 \cite{chen2017deeplab} & TPAMI18 & .832 & .807 & .515 & .503 & .568 & .627 & .522 & .548 & .498 & .602\\ \midrule						
%     MFCN \cite{salloum2018image} & JVCIP18 & .687 & .793 & .523 & .504 & .562 & .569 & .521 & \textbf{.595} & .390 & .572\\ \midrule 			
%     RRU-Net \cite{bi2019rru} & CVPRW19 & .566 & .759 & .489 & .513 & .543 & .580 & .480 & .554 & .379 & .540\\ \midrule	
%     MantraNet \cite{wu2019mantra} & CVPR19 & .847 & .457 & .548 & .537 & .557 & .542 & .536 & .550 & .494 & .563\\ \midrule		
%     HPFCN \cite{li2019localization} & ICCV19 & .572 & .705 & .516 & .507 & .556 & .569 & .544 & .561 & .440 & .552 \\ \midrule 	
%     SPAN \cite{hu2020span} & ECCV20 & .946 & .660 & .580 & .546 & .546 & \textbf{.630} & .537 & .522 & .395 & .596\\ \midrule  
%     H-LSTM \cite{bappy2019hybrid} & TIP19 & .611 & .431 & .490 & .485 & .516 & .536 & .526 & .547 & .516 & .518\\ 
%     \midrule 								
%     ViT-B \cite{dosovitskiy2020vit} & ICLR21 & .705 & .622 & .551 & .506 & .531 & .556 & .497 & .562 & .441 & .552\\ \midrule 	
%     Swin-ViT \cite{liu2021swin} & ICCV21 & .804 & .900 & .556 & .553 & \textbf{.689} & .608 & .513 & .555 & .368 & .616\\\midrule
%     % DenseFCN \cite{zhuang2021image} & TIFS21 & .667 & .520 & .509 & .482 & .524 & .452 & .506 & .506 & .631 & .533\\
%     % \midrule 					
%     PSCC \cite{liu2022pscc} & TCSVT22 & .604 & .457 & .491 & .506 & .571 & .522 & .504 & .513 & .516 & .520\\ \midrule	
%     MVSS-Net++ \cite{dong2022mvss} & TPAMI22 & \textbf{.980} & \textbf{.927} & \textbf{.731} & \textbf{.563} & .668 & .454 & .571 & .557 & .355 & \textbf{.645}\\ 
%     \midrule				
%     CAT-NET \cite{kwon2022learning} & IJCV22 & .438 & .867 & .516 & .502 & .575 & .609 & \textbf{.613} & .537 & .376 & .559\\	
%     \midrule 										
%     EVP \cite{liu2023evp} & CVPR23 & .653 & .804 & .518 & .508 & .578 & .571 & .514 & .554 & \textbf{.582} & .587 \\	
%     \midrule[1.5pt]
%     Ours & - & .904 & .845 & .564 & .551 & .668 & .594 & .519 & .573 & .507 & .636 \\
%     \bottomrule  				
%   \end{tabular}}
%   \label{auc_img}
% \end{table*}

% \begin{table*}
%   \centering
%   \caption{Image-level manipulation detection performance (ACC score with fixed threshold: .5).}
%   \scalebox{0.87}{\begin{tabular}{lcccccccccccccc}
%     \toprule
%     Method & Venue &  NIST & Columbia & CASIAv1+ & COVER & DEF-12k & IMD & Carvalho & IFC & In-the-Wild & Korus & WildWeb & AVG \\
%     \midrule
%     FCN \cite{long2015fully} & CVPR15 & .814 & .587 & .687 & .500 & .533 & .719 & .525 & .445 & .831 & .530 & .625 & .618 \\ \midrule 	
%     U-Net \cite{ronneberger2015u} & MICCAI15 & .895 & .567 & .701 & .505 & .496 & .788 & .505 & .407 & .945 & .516 & .558 & .626 \\ \midrule 
%     DeepLabv3 \cite{chen2017deeplab} & TPAMI18 & .885 & .639 & .701 & .520 & .502 & .769 & .545 & .389 & .950 & .511 & .706 & .647 \\ \midrule							
%     MFCN \cite{salloum2018image} & JVCIP18 & .762 & .628 & .718 & .505 & .502 & .713 & .480 & .424 & .836 & .557 & .392 & .592 \\ \midrule 					
%     RRU-Net \cite{bi2019rru} & CVPRW19 & .771 & .565 & .671 & .490 & .510 & .681 & .550 & .421 & .781 & .511 & .434 & .580 \\ \midrule	
%     MantraNet \cite{wu2019mantra} & CVPR19 & 1.000 & .496 & .535 & .500 & .500 & .829 & .500 & .300 & 1.000 & .500 & .990 & .650 \\ \midrule				
%     HPFCN \cite{li2019localization} & ICCV19 & .807 & .562 & .625 & .505 & .503 & .713 & .530 & .435 & .821 & .532 &  .558 & .599 \\ \midrule 		
%     SPAN \cite{hu2020span} & ECCV20 & .950 & .606 & .606 & .510 & .513 & .797 & .525 & .383 & .985 & .498 & .568 & .631 \\ \midrule  
%     H-LSTM \cite{bappy2019hybrid} & TIP19 & .993 & .501 & .531 & .500 & .500 & .829 & .500 & .303 & 1.000 & .502 & .986 & .650 \\ 
%     \midrule 					
%     ViT-B \cite{dosovitskiy2020vit} & ICLR21 & .940 & .612 & .594 & .520 & .503 & .775 & .545 & .360 & .945 & .507 & .708 & .637 \\ \midrule 		
%     Swin-ViT \cite{liu2021swin} & ICCV21 & .787 & .708 & .741 & .525 & .532 & .723 & .540 & .454 & .746 & .539 & .290 & .599 \\\midrule 											
%     % DenseFCN \cite{zhuang2021image} & TIFS21 & .667 & .520 & .509 & .482 & .524 & .452 & .506 & .506 & .631 & .533\\
%     % \midrule 					
%     PSCC \cite{liu2022pscc} & TCSVT22 & .910 & .576 & .444 & .505 & .508 & .771 & .530 & .371 & .945 & .516 & .684 & .615 \\ \midrule		
%     MVSS-Net++ \cite{dong2022mvss} & TPAMI22 & .711 & .645 & .790 & .535 & .533 & .769 & .470 & .425 & .910 & .550 & .372 & .610\\ 											
%     \midrule				
%     CAT-NET \cite{kwon2022learning} & IJCV22 & .965 & .554 & .635 & .490 & .494 & .799 & .495 & .337 & .985 & .507 & .522 & .617 \\	
%     \midrule 							
%     EVP \cite{liu2023evp} & CVPR23 & .782 & .540 & .686 & .500 & .504 & .701 & .525 & .491 & .706 & .536 & .800 & .616 \\	
%     \midrule[1.5pt] 											
%     Ours & - & .947 & .579 & .741 & .495 & .506 & .813 & .525 & .370 & .955 & .516 & .873 & .665\\
%     \bottomrule					
%   \end{tabular}}
%   \label{acc_img}
% \end{table*}

% \begin{table*}
%   \centering
%   \caption{Image-level manipulation detection performance (ACC score with fixed threshold: .5).}
%   \scalebox{0.8}{\begin{tabular}{lcccccccccccccc}
%     \toprule
%     Method & Venue &  NIST & Columbia & CASIAv1+ & COVER & DEF-12k & IMD & Carvalho & IFC & In-the-Wild & Korus & WildWeb & AVG \\
%     \midrule
%     FCN \cite{long2015fully} & CVPR15 & .814 & .587 & .687 & .500 & .533 & .719 & .525 & .445 & .831 & .530 & .625 & .618 \\ \midrule 	
%     U-Net \cite{ronneberger2015u} & MICCAI15 & .895 & .567 & .701 & .505 & .496 & .788 & .505 & .407 & .945 & .516 & .558 & .626 \\ \midrule 
%     DeepLabv3 \cite{chen2017deeplab} & TPAMI18 & .885 & .639 & .701 & .520 & .502 & .769 & .545 & .389 & .950 & .511 & .706 & .647 \\ \midrule							
%     % MFCN \cite{salloum2018image} & JVCIP18 & .762 & .628 & .718 & .505 & .502 & .713 & .480 & .424 & .836 & .557 & .392 & .592 \\ \midrule 					
%     RRU-Net \cite{bi2019rru} & CVPRW19 & .771 & .565 & .671 & .490 & .510 & .681 & .550 & .421 & .781 & .511 & .434 & .580 \\ \midrule	
%     % MantraNet \cite{wu2019mantra} & CVPR19 & 1.000 & .496 & .535 & .500 & .500 & .829 & .500 & .300 & 1.000 & .500 & .990 & .650 \\ \midrule				
%     HPFCN \cite{li2019localization} & ICCV19 & .807 & .562 & .625 & .505 & .503 & .713 & .530 & .435 & .821 & .532 &  .558 & .599 \\ \midrule 		
%     % SPAN \cite{hu2020span} & ECCV20 & .950 & .606 & .606 & .510 & .513 & .797 & .525 & .383 & .985 & .498 & .568 & .631 \\ \midrule  
%     % H-LSTM \cite{bappy2019hybrid} & TIP19 & .993 & .501 & .531 & .500 & .500 & .829 & .500 & .303 & 1.000 & .502 & .986 & .650 \\ 
%     % \midrule 					
%     ViT-B \cite{dosovitskiy2020vit} & ICLR21 & .940 & .612 & .594 & .520 & .503 & .775 & .545 & .360 & .945 & .507 & .708 & .637 \\ \midrule 		
%     % Swin-ViT \cite{liu2021swin} & ICCV21 & .787 & .708 & .741 & .525 & .532 & .723 & .540 & .454 & .746 & .539 & .290 & .599 \\\midrule 											
%     % DenseFCN \cite{zhuang2021image} & TIFS21 & .667 & .520 & .509 & .482 & .524 & .452 & .506 & .506 & .631 & .533\\
%     % \midrule 					
%     PSCC \cite{liu2022pscc} & TCSVT22 & .910 & .576 & .444 & .505 & .508 & .771 & .530 & .371 & .945 & .516 & .684 & .615 \\ \midrule		
%     MVSS-Net++ \cite{dong2022mvss} & TPAMI22 & .711 & .645 & .790 & .535 & .533 & .769 & .470 & .425 & .910 & .550 & .372 & .610\\											
%     \midrule				
%     CAT-NET \cite{kwon2022learning} & IJCV22 & .965 & .554 & .635 & .490 & .494 & .799 & .495 & .337 & .985 & .507 & .522 & .617 \\	
%     \midrule 							
%     EVP \cite{liu2023evp} & CVPR23 & .782 & .540 & .686 & .500 & .504 & .701 & .525 & .491 & .706 & .536 & .800 & .616 \\	
%     \midrule[1.5pt] 											
%     Ours & - & .947 \textcolor{red}{(2)} & .579 \textcolor{blue}{(5)} & .741 \textcolor{red}{(2)} & .495 \textcolor{blue}{(9)} & .506 \textcolor{blue}{(5)} & \textbf{.813}l \textbf{\textcolor{red}{(1)}} & .525 \textcolor{blue}{(6)} & .370 \textcolor{blue}{(9)} & .955 \textcolor{red}{(2)}  & .516 \textcolor{blue}{(5)} & \textbf{.873} \textbf{\textcolor{red}{(1)}} & \textbf{.665} \textbf{\textcolor{red}{(1)}}\\
%     \bottomrule					
%   \end{tabular}}
%   \label{acc_img}
% \end{table*}



\section{Conclusions and Future Work}
This paper presented a generalized and robust image manipulation localization model by capturing pixel inconsistency in forgery images. The method is underpinned by a two-stream pixel dependency modeling framework for image forgery localization. It incorporates a novel masked self-attention mechanism to model the global pixel dependencies within input images effectively. Additionally, two customized convolutional modules, the Central Pixel Difference Convolution (CPDC) and the Radial Pixel Difference Convolution (RPDC), better capture pixel inconsistency artifacts within local regions. We find that modeling pixel interrelations can effectively mine intrinsic forgery clues. To enhance the overall performance, Learning-to-Weight Modules (LTW) complementarily combines global and local features. The usage of the dynamic weighting scheme can lead to a better feature fusion, contributing to a more robust and generalized image forgery localization.

Furthermore, a novel Pixel-Inconsistency Data Augmentation (PIDA) that exclusively employs pristine images to generate augmented forgery samples, guides the focus on pixel-level artifacts. The proposed PIDA strategy can shed light on improving the generalization for future forensics research. Extensive experimental results demonstrated the state-of-the-art performance of the proposed framework in image manipulation detection and localization, both in generalization and robustness evaluations. The ablation studies further validated the effectiveness of the designed components. 

% comprising a patch correlation encoder, a pixel dependency encoder, and four Feature Fusion Modules (FFM) can effectively combine global and local features at different levels, contributing to more general and robust localization performance. The usage of Central Pixel Difference Convolution (CPDC) and Radial Pixel Difference Convolution (RPDC) enables the model to capture the abundant pixel difference features within the local regions. In addition, the designed causal self-attention mechanism facilitates the modeling of pixel dependencies for input images, thereby exposing pixel inconsistency artifacts for forgery detection.


While our method is robust against unseen image perturbations, it remains susceptible to recapturing attacks. This vulnerability stems from the framework's primary objective: to identify pixel inconsistency artifacts resulting from the disruption of CFA regularity during the manipulation process. Recapturing operations reintroduce the pixel dependencies initially constructed during the demosaicing process, concealing the pixel inconsistency artifacts and leading to failed forgery detection. In future research, developing an effective recapturing detection module becomes a crucial research direction to ensure more secure manipulation detection.

% use section* for acknowledgment
% \ifCLASSOPTIONcompsoc
%   % The Computer Society usually uses the plural form
%   \section*{Acknowledgments}
% \else
%   % regular IEEE prefers the singular form
%   \section*{Acknowledgment}
% \fi


% The authors would like to thank...

% \ifCLASSOPTIONcaptionsoff
%   \newpage
% \fi


{\small
\bibliographystyle{plain}
\bibliography{main}
}



% \begin{IEEEbiography}[{\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{photos/Chenqi.jpg}}]{Chenqi Kong} received the B.S. and M.S. degrees in the College of Science and the College of Electrical Engineering and Automation Harbin Institute of Technology, Harbin, China, in 2017 and 2019, respectively. He is currently pursing the Ph.D. degree in the Department of Computer Science, City University of Hong Kong, Hong Kong, China (Hong Kong SAR). His research interests include computer vision and multimedia forensics.
% \end{IEEEbiography}

% \begin{IEEEbiography}[{\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{photos/Chenqi.jpg}}]{Chenqi Kong} received the B.S. and M.S. degrees in the College of Science and the College of Electrical Engineering and Automation, Harbin Institute of Technology, Harbin, China, in 2017 and 2019, respectively. He received the Ph.D. degree in the Department of Computer Science, City University of Hong Kong, Hong Kong, China (Hong Kong SAR) in 2023. He is currently a research fellow in the School of Electrical and Electronic Engineering, Nanyang Technological University, Singapore. He is a recipient of National Scholarship and  Research Tuition Scholarship. His research interests include AI security and multimedia forensics.
% \end{IEEEbiography}

% \begin{IEEEbiography}[{\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{photos/Anwei.jpg}}]{Anwei Luo} received the B.S. degree from Jilin University, Changchun, China, in 2013. He is currently pursuing the Ph. D. degree from Sun Yat-sen University, Guangzhou, China. His current research interests include digital multimedia forensics, watermarking and security.
% \end{IEEEbiography}

% \begin{IEEEbiography}[{\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{photos/ShiqiWang.jpg}}]{Shiqi Wang} received the B.S. degree in computer science from the Harbin Institute of Technology in 2008 and the Ph.D. degree in computer application technology from Peking University in 2014. From 2014 to 2016, he was a Post-Doctoral Fellow with the Department of Electrical and Computer Engineering, University of
% Waterloo, Waterloo, ON, Canada. From 2016 to
% 2017, he was a Research Fellow with the Rapid-Rich
% Object Search Laboratory, Nanyang Technological
% University, Singapore. He is currently an Associate Professor with the Department of Computer Science, City University of Hong Kong. He has proposed over 40 technical proposals to ISO/MPEG, ITU-T, and AVS standards, and authored/coauthored more than 200 refereed journal articles/conference papers. He received the Best Paper Award from IEEE VCIP 2019, ICME 2019, IEEE Multimedia 2018, and PCM 2017 and is the coauthor of an article that received the Best Student Paper Award in the IEEE ICIP 2018. His research interests include video compression, image/video quality assessment, and image/video search and analysis. 
% \end{IEEEbiography}

% \begin{IEEEbiography}[{\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{photos/HaoliangLi.jpg}}]{Haoliang Li} received the B.S. degree in communication engineering from University of Electronic Science and Technology of China (UESTC) in 2013, and his Ph.D. degree from Nanyang Technological University (NTU), Singapore in 2018. He is currently an assistant professor in Department of Electrical Engineering, City University of Hong Kong. His research mainly focuses on AI security, multimedia forensics and transfer learning. His research works appear in international journals/conferences such as TPAMI, IJCV, TIFS, NeurIPS, CVPR and AAAI. He received the Wallenberg-NTU presidential postdoc fellowship in 2019, doctoral innovation award in 2019, and VCIP best paper award in 2020.  
% \end{IEEEbiography}

% \begin{IEEEbiography}[{\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{photos/anderson_rocha.jpg}}]{Anderson Rocha} received his Ph.D. degree in computer science. He is a full professor of artificial intelligence and digital forensics at the Institute of Computing, University of Campinas, Campinas 13083-852, Brazil, where he is the coordinator of the Artificial Intelligence Lab. A Microsoft and Google Faculty Fellow, he is a former chair of the IEEE Information Forensics and Security Technical Committee (20192020) and an affiliated member of the Brazilian Academy of Sciences and the Brazilian Academy of Forensics Sciences. His research interests include artificial intelligence, digital forensics, and reasoning for complex data. He is a Senior Member of IEEE.
% \end{IEEEbiography}
% \vspace{-12cm}
% \begin{IEEEbiography}[{\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{photos/Alex.jpg}}]
% {Prof. Alex Kot} has been with the Nanyang Technological University, Singapore since 1991. He was Head of the Division of Information Engineering and Vice Dean Research at the School of Electrical and Electronic Engineering. Subsequently, he served as Associate Dean for College of Engineering for eight years. He is currently Professor and Director of Rapid-Rich Object SEarch (ROSE) Lab and NTU-PKU Joint Research Institute. He has published extensively in the areas of signal processing, biometrics, image forensics and security, and computer vision and machine learning.
% Prof. Kot served as Associate Editor for more than ten journals, mostly for IEEE transactions. He served the IEEE SP Society in various capacities such as the General Co-Chair for the 2004 IEEE International Conference on Image Processing and the Vice-President for the IEEE Signal Processing Society. He received the Best Teacher of the Year Award and is a co-author for several Best Paper Awards including ICPR, IEEE WIFS and IWDW, CVPR Precognition Workshop and VCIP. He was elected as the IEEE Distinguished Lecturer for the Signal Processing Society and the Circuits and Systems Society. He is a Fellow of IEEE, and a Fellow of Academy of Engineering, Singapore 
% \end{IEEEbiography}

\begin{table*}
  \centering
  \caption{Image manipulation localization performance (\textbf{AUC score}).}
  \scalebox{0.76}{\begin{tabular}{lcccccccccccccc}
    \toprule
    Method & Venue & NIST & Columbia & CASIAv1+ & COVER & DEF-12k & IMD & Carvalho & IFC & In-the-Wild & Korus & WildWeb & AVG \\
    \midrule
    FCN \cite{long2015fully} & CVPR15 & .675 & .696 & .819 & .694 & .628 & .748 & .686 & .605 & .690 & .644 & .651 & .685\\ 
    \midrule
    U-Net \cite{ronneberger2015u} & MICCAI15 & .668 & .645 & .759 & .622 & .587 & .703 & .653 & .598 & .654 & .626 & .591 & .646\\ 										
    \midrule 	
    DeepLabv3 \cite{chen2017deeplab} & TPAMI18 & .720 & .853 & \underline{.861} & .763 & .667 & .815 & \textbf{.807} & .631 & {.752} & .675 & \underline{.709} & .750\\ 											
    \midrule
    MFCN \cite{salloum2018image} & JVCIP18 & .691 & .634 & .740 & .614 & .576 & .664 & .631 & .591 & .621 & .621 & .575 & .633 \\ 
    \midrule 	
    RRU-Net \cite{bi2019rru} & CVPRW19 & .715 & .749 & .800 & .676 & .593 & .754 & .661 & .586 & .704 & .669 & .633 & .685  \\ 
    \midrule 											
    MantraNet \cite{wu2019mantra} & CVPR19 & .734 & .734 & .733 & .722 & .696 & .760 & .644 & .592 &  .719 & .646 & .626 & .691\\			
    \midrule
    HPFCN \cite{li2019localization} & ICCV19 & .688 & .607 & .725 & .591 & .583 & .683 & .583 & .564 & .642 & .607 & .626 & .627\\		
    \midrule 
    H-LSTM \cite{bappy2019hybrid} & TIP19 & .696 & .571 & .634 & .634 & .581 & .656 & .586 & .553 & .611 & .588 & .630 & .613\\ 
    \midrule 
    SPAN \cite{hu2020span} & ECCV20 & .751 & \underline{.855} & .756 & .777 & .641 & .763 & .671 & .602 & .749 & .649 & .582 & .709\\ 
    \midrule							
    ViT-B \cite{dosovitskiy2020vit} & ICLR21 & .705 & .689 & .763 & .665 & .602 & .693 & .674 & .580 & .692 & .653 & .605 & .666 \\   
    \midrule 											
    Swin-ViT \cite{liu2021swin} & ICCV21 & .723 & .750 & .777 & .740 & .669 & .793 & .668 & .641 & .710 & {.701} & .572 & .704 \\  								
    \midrule
    % DenseFCN \cite{zhuang2021image} & TIFS21 & .532 & .540 & .526 & .534 & .526 & .529 & .467 & .487 & .537 & .499 & .533 & .519\\ 										
    % \midrule
    PSCC \cite{liu2022pscc} & TCSVT22 & .676 & .731 & .822 & .660 & .600 & .762 & .700 & .589 & .696 & .646 & .558 & .676\\ 								
    \midrule									
    MVSS-Net++ \cite{dong2022mvss} & TPAMI22 & \textbf{.791} & .818 & .845 & \textbf{.871} & .683 & \underline{.817} & .731 & .635 & \underline{.794} & .659 & .646 & \underline{.754}\\	
    \midrule	
    CAT-NET \cite{kwon2022learning} & IJCV22 & .522 & .524 & .668 & .662 & \textbf{.818} & .588 & .603 & .442 & .504 & .531 & .536 & .582\\	
    \midrule 									
    EVP \cite{liu2023evp} & CVPR23 & \underline{.775} & .791 & .855 & .716 & \underline{.697} & .811 & .688 & \underline{.648} & .748 & \underline{.715} & .695 & .740 \\	
    \midrule[1.5pt]
    Ours & - & {.752} & \textbf{.884} & \textbf{.889} & \underline{.809} & .687 & \textbf{.870} & \underline{.760} & \textbf{.669} & \textbf{.831} & \textbf{.725} & \textbf{.725} & \textbf{.782} \\
    \bottomrule
  \end{tabular}}
  \label{auc_table}
\end{table*} 


\begin{table*}
  \centering
  \caption{Image manipulation localization performance (\textbf{MCC score} with fixed threshold: 0.5).}
  \scalebox{0.76}{\begin{tabular}{lcccccccccccccc}
    \toprule
    Method & Venue & NIST & Columbia & CASIAv1+ & COVER & DEF-12k & IMD & Carvalho & IFC & In-the-Wild & Korus & WildWeb & AVG \\
    \midrule
    FCN \cite{long2015fully} & CVPR15 & .151 & .194 & .425 & .154 & .113 & .212 & .083 & .078 & .192 & .126 & .162 & .172 \\
    \midrule 
    U-Net \cite{ronneberger2015u} & MICCAI15 & .155 & .119 & .263 & .073 & .036 & .137 & .098 & .058 & .140 & .105 & .053 & .112 \\
    \midrule 	
    DeepLabv3 \cite{chen2017deeplab} & TPAMI18 & .226 & .404 & .428 & .132 & .065 & .214 & .173 & .071 & .203 & .119 & .091 & .193\\
    \midrule
    MFCN \cite{salloum2018image} & JVCIP18 & .230 & .172 & .351 & .118 & .062 & .165 & .145 & .090 & .152 & .119 & \underline{.102} & .155 \\ 
    \midrule 		
    RRU-Net \cite{bi2019rru} & CVPRW19 & .190 & .228 & .292 & .068 & .028 & .154 & .054 & .041 & .155 & .094 & .087 &  .126 \\ 
    \midrule						
    MantraNet \cite{wu2019mantra} & CVPR19 & .107 & .156 & .120 & .134 & .061 & .118 & .090 & .020 & .157 & .038 & .087 & .099\\ 	
    \midrule 
    HPFCN \cite{li2019localization} & ICCV19 & .155 & .074 & .180 & .069 & .028 & .094 & .052 & .047 & .093 & .081 & .068 & .086\\ 
    \midrule
    H-LSTM \cite{bappy2019hybrid} & TIP19 & \textbf{.354} & .140 & .140 & .130 & .044 & .187 & .114 & .053 & .155 & .131 & .133 & .144\\ 
    \midrule 								
    SPAN \cite{hu2020span} & ECCV20 & .195 & .454 & .153 & .142 & .031 & .141 & .077 & .046 & .166 & .075 & .023 & .137\\
    \midrule 	
    ViT-B \cite{dosovitskiy2020vit} & ICLR21 & .242 & .193 & .285 & .114 & .052 & .196 & .151 & .053 & .185 & \underline{.163} & .099 & .158\\	
    \midrule	
    Swin-ViT \cite{liu2021swin} & ICCV21 & .208 & .321 & .392 & .159 & .158 & \underline{.303} & .175 & \underline{.098} & .260 & .136 & .039 & .204\\	
    \midrule
    % DenseFCN \cite{zhuang2021image} & TIFS21 & .043 & .096 & .042 & .033 & .017 & .033 & -0.056 & -0.017 & .053 & -0.016 & .056 & .026 \\ 		
    % \midrule
    PSCC \cite{liu2022pscc} & TCSVT22 & .131 & .338 & .319 & .110 & .056 & .166 & \underline{.184} & .035 & .156 & .085 & .046 & .148 \\ 
    \midrule 
    MVSS-Net++ \cite{dong2022mvss} & TPAMI22 & \underline{.289} & \underline{.545} & \underline{.503} & \textbf{.464} & {.097} & .265 & .170 & .068 & \underline{.265} & .105 & .063 & \underline{.258}\\	
    \midrule	
    CAT-NET \cite{kwon2022learning} & IJCV22 & .023 & .055 & .147 & .135 & \textbf{.216} & .208 & .125 & .043 & .109 & .040 & .042 & .104\\
    \midrule 											
    EVP \cite{liu2023evp} & CVPR23 & .205 & .266 & .478 & .103 & .090 & .236 & .055 & .082 & .228 & .118 & .096 & .178 \\	
    \midrule[1.5pt]
    Ours & - & .264 & \textbf{.630} & \textbf{.565} & \underline{.230} & \underline{.162} & \textbf{.415} & \textbf{.229} & \textbf{.142} & \textbf{.396} & \textbf{.228} & \textbf{.212} & \textbf{.318} \\
    \bottomrule
  \end{tabular}}
  \label{mcc_table}
\end{table*}

\newpage
\appendix
\noindent \textbf{Additional evaluations.} In Table~\ref{auc_table}, we report the cross-dataset forgery localization performance using the threshold-free metric AUC. Notably, our method achieves an outstanding 78.2\% AUC performance. Compared with MVSS-Net++ \cite{dong2022mvss}, the proposed method achieves a 2.8$\%$ average AUC-score improvement, increasing  from 75.4$\%$ to 78.2$\%$. In Table~\ref{mcc_table}, our method consistently achieves the best or second-best detection performance on unseen testing datasets. And our average MCC performance outperforms SOTA methods by a clear margin. 

\noindent \textbf{Additional visualization results.}
Fig.~\ref{visualization2} shows additional forgery localization results under the cross-dataset experimental setting. Our method accurately identifies the manipulated regions. In comparison with state-of-the-art (SOTA) methods, the proposed method demonstrates a superior forgery localization performance. 

\begin{figure*}[ht]
\centering
\includegraphics[scale=0.56]{ visualization2.png}
\caption{Additional forgery localization results on the 11 unseen test sets. The three left columns show the input images, corresponding ground-truth, and the localization results of our method. The right 10 columns present the results of SOTA methods.
}
\label{visualization2}
\end{figure*}






\end{document}


