\section{Background}

\subsection{Alloy}

Alloy is a formal specification language, with a simple syntax and a relational semantics. The syntax of the language is rather small, with only a few constructs. Moreover, this syntax is compatible with an intuitive reading of specifications, or \emph{models}, as they are typically called in the context of Alloy \cite{Jackson2006} (we will use \emph{specification} and \emph{model} interchangeably in this paper), resembling object-oriented notions that are familiar to developers. The basic syntactic elements of Alloy specifications are: \emph{signatures}, which declare data domains; \emph{signature fields} (akin to class attributes), that give structure to specifications and declare \emph{relations} between signatures; \emph{predicates}, parameterized formulas that can be used to state properties, represent operations, etc.; \emph{functions}, parameterized expressions that are useful to better organize specifications; \emph{facts}, formulas that constrain the specifications and represent assumptions; and \emph{assertions}, formulas that capture \emph{intended} properties of the specification, i.e., properties that the user would like to verify. 

Formulas in Alloy are expressed in \emph{relational logic}, a first-order logic extended with relational operators such as relational transpose, union, difference and intersection. Alloy also has other important relational operators: \emph{relational join}, a generalization of  composition to $n$-ary relations, which can be used to express \emph{navigations} as in object orientation; and \emph{transitive closure}, which can be applied only to binary relations, and extends the expressiveness of Alloy beyond that of first-order logic. As an example, consider the (buggy) Alloy model in Figure~\ref{alloy-model}, a modified version of an Alloy specification that is part of the benchmark used in \cite{Wang+2019}. First, notice that signature \texttt{Boolean} is declared as \emph{abstract}, and two singleton signatures, namely \texttt{True} and \texttt{False}, are its extending signatures. This defines constants as singleton relations in a model, as well as the enumerated set \texttt{Boolean}. Then, signatures \texttt{Node} and \texttt{List} declare data domains for a linked list model; nodes have a link (a set of nodes), and an element (an integer); lists have a header (a set of nodes). A \emph{fact} constrains the cardinalities of these signature fields: lists have at most one header, and nodes have at most one successor node, and exactly one element. Notice the additional fact, which is there for analysis purposes: it states that exactly one \texttt{List} is going to be considered in each instance of the model, and that all nodes present in an instance will be those in the list (no unreachable ``heap'' objects). As explained above, \emph{predicates} can be used to express properties, as well as \emph{operations}. For instance, predicate \texttt{Loop} captures lists with a loop in its last node, saying that a list satisfies the predicate if it either has no header, or for exactly one of its nodes, the elements reachable in one or more steps from \texttt{link} are exactly the same reachable in zero o more steps through \texttt{link}. Predicate \texttt{Contains}, on the other hand, is used to model an \emph{operation} on lists, namely, the operation for querying membership of an integer as an element of a node of a list. The result of the operation is captured by an additional Boolean parameter. This predicate is \emph{buggy}, it does not correctly model the intended operation (there are other bugs in the specification). 

\begin{figure}[ht!]
%{\small
%\begin{verbatim}
\begin{lstlisting} []
abstract sig Boolean { }
one sig True, False extends Boolean { }

sig Node {
 link: set Node,
 elem: set Int
}

sig List {
 header: set Node
}

fact CardinalityConstraints {
 all l : List | lone l.header
 all n : Node | lone n.link
 all n : Node | one n.elem
}

fact IGNORE {
 one List && List.header.*link = Node
}

pred Loop[This: List] {
 no This.header || 
 one n : This.header.*link | n.^link = n.*link 
}

pred Sorted[This: List] { // buggy
 all n: This.header.*link | n.elem <= n.link.elem 
}

pred RepOk[This: List] {
 Loop[This] && Sorted[This]
}

// buggy
pred Contains[This: List, x: Int, res: Boolean]{ 
 RepOk[This] &&
 ((x !in This.header.*link.elem => res=False ) || 
 res = True) 
}

pred Count[This: List, x: Int, res: Int] {
 RepOk[This] &&
 res = #{ n:This.header.*link | n.elem = x }
}

assert ContainsCorrect {
 all l : List, i, j : Int | 
   (Count[l, i, j] && j > 0) iff Contains[l, i, True]
}

check ContainsCorrect for 10
\end{lstlisting}
\caption{A (faulty) sample Alloy specification.}
\label{alloy-model}
\end{figure}

Alloy specifications can be automatically analyzed, by an analysis mechanism that resorts to SAT solving, and is implemented in a tool called \emph{Alloy Analyzer} \cite{Jackson2006}. Two kinds of analysis are possible: \emph{running} a predicate, and \emph{checking} an assertion. Both are analyzed in \emph{bounded} scenarios. Running a predicate searches for instances (scenarios) that satisfy all the constraints (cardinalities, facts, etc.), including the predicate being run. Assertion checking looks for \emph{counterexamples} of the asserted properties. Analysis is performed up to a bound $k$ (typically referred to as the \emph{scope} of the analysis), meaning, e.g., that assertion checking will either find a counterexample within the given scope, or guarantee the validity of the formula within the bound (similarly, a predicate will be found to be satisfiable within the provided scope, or not to have a satisfying instance within the scope). This \emph{bounded exhaustive analysis}, of course, does not necessarily mean that the formula is valid (resp., satisfiable), as counterexamples (resp., instances) of greater size may exist if larger scopes are considered.

\subsection{Constructing Alloy Models}

Alloy is an expressive language to describe software abstractions. The language is the vehicle for defining abstract software models in a lightweight and incremental way, with immediate feedback via automated analysis \cite{Jackson2006}. Typically, the process of constructing an Alloy model starts very much in the same way one would proceed while eliciting requirements, or sketching an abstract software design: basic domains of the model are identified (signatures of the model), over which more structured components are organized (signatures equipped with fields). How these domains and components are constituted, the inherent constraints of the problem domain and the operations that represent the software model capacities, are all incrementally created, via a constant interaction with the Alloy Analyzer. This process eventually involves the use of \emph{assertions} and \emph{predicates}, that capture intended properties of the model, and that serve essentially as the \emph{oracle} of the specification, i.e., the properties that would convey the acceptance of the model. Sometimes these properties can help find surprising counterexamples, that lead to refinements of the properties themselves, but more often they help one in ``debugging'' the core of the model, i.e., in getting the model ``right'', adapting it until the intended properties result as expected. 

While the intended properties are subject to defects too, they are typically significantly shorter and clearer than the ``core'' of the specification. They capture high level properties of the model, so they are expected to be simpler to write and get right. So, once the intended properties are set, the user performs the corresponding analyses and uses the results as an acceptance criterion for the specification, and the corresponding design it conveys. That is, a model will be considered incorrect if any of the analyses of the intended properties fails, i.e., has a result that contradicts the user expectations. In Figure~\ref{alloy-model}, for instance, the user may consider the assertion \texttt{ContainsCorrect} and the auxiliary predicate \texttt{Count} as the oracle of the specification, meaning that when this intended property is found to be invalid, the user would start modifying the remainder of the specification, as an attempt to fix the error. \technique\ as well as other model repair techniques aim at reducing human intervention along this overall modeling process, by automatically fixing errors in incorrect models.

Notice that in this specification and analysis situations, the concept of \emph{test}, as typically found in programming contexts (and the ARepair work \cite{Sullivan+2018}), is not present, at least not in its standard form. Satisfying scenarios do participate in the modeling process, but as a result of analyzing \emph{properties}. That is, tests are not a natural explicitly described part of Alloy specifications (despite the recent proposals incorporating unit tests for Alloy \cite{Sullivan+2018}). This is important to remark, as most automated repair techniques are for the programming context, and make substantial use of tests as (partial) specifications. Our observation here is that Alloy model characteristics make it not straightforward to apply test-based automated repair techniques in the context of Alloy, as they require the provision of tests, that are not commonly found as part of Alloy specifications.   

\subsection{Automated Repair}

Automated repair refers to a family of techniques for automatically repairing faulty software artifacts, usually through the application of transformations that modify the artifact's notation. It has been most often used for programs, where the artifacts are software source code, and transformations are (guided and/or localized) mutations to the code. In its general form, automated repair can be described as a not necessarily exhaustive search that, given a faulty artifact, a fixed set of syntactic ``mutation'' operators, and an acceptance criterion for a fix:
\begin{itemize}

\item takes the faulty program or program specification to be repaired as the initial repair candidate;

\item if $p$ is a repair candidate, and $q$ is the result of applying a mutation operator on $p$, then $q$ is also a repair candidate; and

\item a candidate $s$ is \emph{successful} if it satisfies the provided acceptance criterion.

\end{itemize}

\noindent
From this problem statement, it is clear that two aspects make the space of repair candidates grow: the number $b$ of mutation operators to be considered (the branching factor), and the maximum number $d$ of successive mutations considered to generate the candidates (the depth of the solution). A geometric sum explains a search space consisting of $\frac{b^{d+1}-1}{b-1}$ candidates ($O(b^d)$ candidates). Even for small faulty software artifacts and a modest number of mutation operators, the number of fix candidates can grow to an extent that deems the whole process infeasible. In the context of automated program repair, existing tools tame this explosion in different ways. Techniques to reduce the explosion include approaches that bring down the branching factor by using a single mutation (e.g., \cite{DBLP:conf/tacas/GopinathMK11}) or considering a very small set of mutators (e.g., based on patterns of human-written fixes \cite{Kim+2013}), or considering coarse grained mutations (e.g., no intra-statement program modifications \cite{LeGoues+2012}), and resorting to non-exhaustive heuristic search, e.g., based on evolutionary computation \cite{LeGoues+2012}. Fault localization, i.e., the process of identifying the parts of a software artifact that are more likely to contain bugs, is crucial for taming the space of fix candidates too. In effect, by successfully localizing faults, one can restrict the generated mutations to only those suspicious statements or blocks, thus significantly reducing the number of candidates. Fault localization is an important problem on its own, and different techniques based on mutations, and most generally, on spectra (running tests and classifying program statements based on their participation in passing/failing executions) have been proposed. Some automated repair techniques are coupled with their own fault localization techniques (e.g., \cite{LeGoues+2012}, which uses its own form of weighted frequency based fault localization). 

As mentioned, automated repair requires a criterion for fix acceptance, i.e., for considering a fix candidate a correct repair of the faulty program (or specification). In the context of automated program repair, most existing approaches use \emph{tests as specifications} (e.g., \cite{Kim+2013,LeGoues+2012}), considering a program correct if it passes all tests in a provided suite, and faulty otherwise. The use of test cases as the acceptance criterion for fixes leads to the so-called \emph{overfitting} problem, the problem that arises when a candidate passes all tests, but is not a true repair, i.e., there are situations, not captured by the tests, in which the software artifact fails to comply with the intended behavior. This, as usual, is strongly related to the quality of the provided test suite.

