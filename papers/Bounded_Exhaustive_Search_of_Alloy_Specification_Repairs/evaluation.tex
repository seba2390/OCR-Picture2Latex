% -*- root: repairing-alloy-models.tex-*-
\section{Evaluation}

We now assess our technique for automated repair of Alloy specifications. Our evaluation is based on two benchmarks of real faulty Alloy specifications, one taken from \cite{Nelson+2017} and used in the evaluation of ARepair \cite{Wang+2018}, and the other originated in the Alloy4Fun project \cite{Macedo+2020}, which includes 6 new models, with a total of 1936 faulty variants (considering different specification assignments resolved by different students). All the presented experiments were run on a 3.6GHz Intel Core i7 processor with 16 GB RAM, running GNU/Linux. We used a 1 hour timeout for each repair analysis instance. 

Our evaluation considers the following research questions:
\begin{itemize}

    \item \textbf{RQ1} What is the impact of the pruning strategies in the performance of our technique?

    \item \textbf{RQ2} How does our technique compare to previous work on automated repair for Alloy specifications?

\end{itemize}
For \textbf{RQ1}, notice that the pruning strategies only apply to specifications with multiple faulty locations. We then evaluate our technique, with pruning enabled vs. pruning disabled, over the following cases:
\begin{itemize}

    \item From ARepair's benchmark (we will refer in this way to the benchmark used in the original evaluation of \cite{Wang+2018}), we consider 18 specifications out of the 38 that are part of the benchmark. We disregard cases that have exactly one bug (20 in total in the benchmark), as these will not make pruning checks, nor trigger the pruning. %, nor cause  our technique does not handle (specifications with only one bug, repairs that must arise from the empty expression and repairs that require fully removing an expression, that are not supported by our mutation operators, and modifications to fields and signatures that are out of the scope of our technique). 

    \item From Alloy4Fun, we consider a total of 273 faulty specifications. To build these specifications, we tracked the models with multiple assignments, and identified the cases in which a given model was submitted with more than one bug by the same student. (While the student id is not reported as part of the Alloy4Fun dataset, submissions are organized as chains of interaction ids, that correspond to a same student session. We use this information to organize submissions based on student sessions.) 

\end{itemize}
%Since for these models we have ``golden versions'' of the to-be-repaired specifications, the experiments were run using the golden versions as oracles for fix acceptance. This prevents the tool from stopping with an early overfitted fix, and allows us to effectively measure the cost of finding the first true fix both with and without pruning. 
The results are summarized in Table~\ref{pruning-vs-no-pruning-experiments}. This table shows, for each of the benchmarks, the number of cases, how many were repaired with pruning enabled and disabled (recall the 1 hour timeout), and the average time for those cases that were repaired within the timeout (time is in milliseconds). We also report the increased repairability, and improved efficiency, obtained by pruning. We considered the cases that were not repaired with pruning disabled, but were repaired with pruning enabled, as if they were repaired in 1 hour. So, the increased efficiency is actually a lower bound of the actual improvement. For reference, we also report the range of efficiency improvement along all cases in each benchmark. %In the experiments, we manually checked that the fixes that were found to be equivalent to the corresponding golden versions were indeed true fixes, as fixes may be correct within the analysis scope, and still have equivalence counterexamples for larger scopes. 

% \begin{table*}[ht]
% \caption{Impact of pruning in repairability.}
% \begin{center} 
% \begin{small}
% \begin{tabular}{|l|r|r|r|r|r|r|r|}
% \hline
% \rowcolor{black!25}
% \cellcolor[gray]{.75}\textbf{Benchmark} & \cellcolor[gray]{.75}\textbf{Total} & \multicolumn{2}{c|}{\cellcolor[gray]{.75}\textbf{Pruning Disabled}} & \multicolumn{2}{c|}{ \cellcolor[gray]{.75}\textbf{Pruning Enabled}} & \multicolumn{2}{c|}{\cellcolor[gray]{.75}\textbf{Improved repairability/efficiency}}\\
% %
% \cellcolor[gray]{.75}{} & \cellcolor[gray]{.75}{\textbf{cases}} & \cellcolor[gray]{.75}{Repaired Cases} &\cellcolor[gray]{.75}\textit{Avg. Time} &\cellcolor[gray]{.75}{Repaired Cases} &\cellcolor[gray]{.75}\textit{Avg. Time} &\cellcolor[gray]{.75}{Repaired Cases} & \cellcolor[gray]{.75}\textit{Avg. Time [Range min - max]} \\
%     \hline
%     ARepair  &\textit{18} &  3 & \textit{146066} & 8 & \textit{62442} & \textbf{2.7X} & \textbf{38.3X \emph{[1X - 133X]}}\\
%     \hline
%     Alloy4Fun  &\textit{273} & 27 & \textit{210101} & 64 & \textit{25889} & \textbf{2.4X} & \textbf{85.5X \emph{[1X - 433X]}}\\
%     \hline
% \end{tabular}
% \end{small}
% \end{center}
% \label{pruning-vs-no-pruning-experiments}
% \end{table*}

\begin{table*}[ht]
\caption{Impact of pruning in repairability.}
\begin{center} 
\begin{small}
\begin{tabular}{|l|r|r|r|r|r|r|c|}
\hline
\rowcolor{black!25}
\cellcolor[gray]{.75}\textbf{Benchmark} & \cellcolor[gray]{.75}\textbf{Total} & \multicolumn{2}{c|}{\cellcolor[gray]{.75}\textbf{Pruning Disabled}} & \multicolumn{2}{c|}{ \cellcolor[gray]{.75}\textbf{Pruning Enabled}} & \multicolumn{2}{c|}{\cellcolor[gray]{.75}\textbf{Improved repairability/efficiency}}\\
%
\cellcolor[gray]{.75}{} & \cellcolor[gray]{.75}{\textbf{cases}} & \cellcolor[gray]{.75}{Repaired Cases} &\cellcolor[gray]{.75}\textit{Avg. Time} &\cellcolor[gray]{.75}{Repaired Cases} &\cellcolor[gray]{.75}\textit{Avg. Time} &\cellcolor[gray]{.75}{Repaired Cases} & \cellcolor[gray]{.75}\textit{Avg. Time \scriptsize{[Range min - max]}} \\
%======
\hline
\multicolumn{8}{|c|}{\cellcolor[gray]{.90}\textit{\scriptsize{ARepair's benchmarks}}} \\
%----
balancedBST & 2 & 0 & \textit{} & 0 & \textit{} & - & - \\ \hline
cd & 1 & 1 & \textit{1765} & 1 & \textit{540} & 1.00\scriptsize{\emph{X}} & 3\scriptsize{\emph{X}} [3\scriptsize{\emph{X}} - 3\scriptsize{\emph{X}}] \\ \hline
dll & 3 & 2 & \textit{290366} & 2 & \textit{2756} & 1.00\scriptsize{\emph{X}} & 80\scriptsize{\emph{X}} [26\scriptsize{\emph{X}} - 133\scriptsize{\emph{X}}] \\ \hline
farmer & 1 & 0 & \textit{} & 0 & \textit{} & - & - \\ \hline
fsm & 1 & 0 & \textit{} & 0 & \textit{} & - & - \\ \hline
student & 10 & 0 & \textit{} & 5 & \textit{184030} & 5.00\scriptsize{\emph{X}} & 26\scriptsize{\emph{X}} [1\scriptsize{\emph{X}} - 81\scriptsize{\emph{X}}] \\ \hline
%----
\rowcolor{black!5}\textbf{Total:}  & 18 & 3 & \textit{146066} & 8 & \textit{62442} & 2.66\scriptsize{\emph{X}} & 37\scriptsize{\emph{X}} [1\scriptsize{\emph{X}} - 133\scriptsize{\emph{X}}] \\ \hline
%======
\multicolumn{8}{|c|}{\cellcolor[gray]{.90}\textit{\scriptsize{Alloy4Fun's benchmarks}}} \\
%----
Graphs & 22 & 6 & \textit{409667} & 16 & \textit{6821} & 2.66\scriptsize{\emph{X}} & 123\scriptsize{\emph{X}} [9\scriptsize{\emph{X}} - 387\scriptsize{\emph{X}}] \\ \hline
LTS & 33 & 0 & \textit{} & 1 & \textit{1983} & 1.00\scriptsize{\emph{X}} & 181\scriptsize{\emph{X}} [181\scriptsize{\emph{X}} - 181\scriptsize{\emph{X}}] \\ \hline
Trash & 23 & 7 & \textit{94960} & 15 & \textit{8084} & 2.14\scriptsize{\emph{X}} & 46\scriptsize{\emph{X}} [2\scriptsize{\emph{X}} - 107\scriptsize{\emph{X}}] \\ \hline
Production & 2 & 0 & \textit{} & 0 & \textit{} & - & - \\ \hline
Classroom & 169 & 14 & \textit{755978} & 32 & \textit{138447} & 2.28\scriptsize{\emph{X}} & 82\scriptsize{\emph{X}} [1\scriptsize{\emph{X}} - 433\scriptsize{\emph{X}}] \\ \hline
CV & 24 & 0 & \textit{} & 0 & \textit{} & - & - \\ \hline
%----
\rowcolor{black!5}\textbf{Total:}  & 273 & 27 & \textit{420201} & 64 & \textit{38833} & 2.36\scriptsize{\emph{X}} & 85\scriptsize{\emph{X}} [1\scriptsize{\emph{X}} - 433\scriptsize{\emph{X}}] \\ \hline
\end{tabular}
\end{small}
\end{center}
\label{pruning-vs-no-pruning-experiments}
\end{table*}

For \textbf{RQ2}, we compare our technique with the only other approach for repairing Alloy models, namely ARepair \cite{Wang+2018}. We analyze both tools in their corresponding abilities to repair specifications in our considered benchmarks. For ARepair's benchmark, we used the models' corresponding assertions as oracles for \technique, and automatically generated test suites, using AUnit \cite{Sullivan+2018}, for ARepair. Recall that ARepair requires tests as oracles for the repair process; we actually follow the procedure suggested in \cite{Wang+2018}, as test cases are not commonly found accompanying Alloy specifications. Notice then that the results reported in \cite{Wang+2018} do not coincide with those reported here for ARepair's benchmark, as we use the same models with different test suites. The test suites used in \cite{Wang+2018} include manually designed cases, to help ARepair in overcoming overfitting. In our evaluation, we favored a comparison in which only the original assertions are available, and thus we generated test cases automatically, with AUnit (using the best performing criterion, predicate coverage \cite{Sullivan+2018}). 

From the Alloy4Fun dataset, we generated a benchmark consisting of: \emph{(i)} every faulty submission of the dataset as a single specification (these correspond to every intermediate specification submitted for analysis check in Alloy4Fun); and \emph{(ii)} the specifications combining all modifications within a single user session, that we used for \textbf{RQ1}. The total number of faulty specifications in this benchmark is 2209 (1936 faulty submissions, plus 273 sessions combining submissions of the same user). For \technique, we used the models' corresponding assertions as oracles. Since we do not have tests for these specifications, and ARepair inherently requires tests as repair oracles, we generated tests automatically using AUnit \cite{Sullivan+2018} (with predicate coverage as a target criterion), using the specification assertions, and employed these generated test suites for running ARepair. 

In all of the above cases, we contrasted the obtained repairs against correct versions of the corresponding specifications, using Alloy Analyzer, to account for overfitting. The results for ARepair and Alloy4Fun benchmarks are summarized in Tables~\ref{arepair-experiments} and \ref{alloy4fun-experiments}, respectively. For each model, we report the number of cases, and for each tool, the number of fixes found (percentage also reported), and how many of these are correct and incorrect (the latter, due to overfitting) patches. We also report the percentage of correct and incorrect patches, with respect to the total number of cases, and the average repair time in milliseconds, for each tool (these are the averages only for the repaired cases). 


%The results for the ARepair and Alloy4Fun benchmarks are summarized in Table~\ref{single-bug-experiments}. As it can be seen from this table, in the case of the ARepair benchmark, out of the 20 faulty models, ARepair is able to fix 18 (our of which 3 are spurious), whereas our technique repairs 16 (no spurious). For Alloy4Fun's benchmark, out of the 1936 cases, ARepair finds fixes for 1577 specifications (81\%). However, only 193 are correct fixes (9.97\% of the total number of cases). In other words, 87.76\% of the fixes are spurious, i.e., overfitted cases due to the fact that ARepair requires tests as oracles. \technique, on the other hand, is able to find fixes for 978 cases (50,52\%), all of which are \emph{actual fixes} (due to the correct predicate being used as an oracle). 

%Notice that even when having ``stronger'' oracles available, ARepair is not able to profit from these, as tests are the only oracles acceptable for the repair process. One can however check these stronger oracles \emph{a posteriori}, once a fix has been found, to check overfitting. But the result of this process cannot be easily integrated into the repair process (other than getting a new test that prevents the overfitted case being produced as a repair candidate). 

%For specifications containing multiple bugs, we have a total of 18 in ARepair's benchmark, and 273 in Alloy4Fun's benchmark, the same specifications used for RQ1. The specification oracles for both tools are exactly the same as for the 1-bug experiments. The results are summarized in Table~\ref{multi-bug-experiments}.

 
\begin{table*}[ht]
    \caption{Experiments taken from ARepair's benchmarks.} 
\begin{center} 
\begin{small}
\begin{tabular}{|l|r||r|r|r|r||r|r|r|r|}
\hline
\rowcolor{black!25}
\cellcolor[gray]{.75}\textbf{} & \cellcolor[gray]{.75}\textbf{Total} &\multicolumn{4}{|c||}{\textbf{ARepair }} &  \multicolumn{4}{|c|}{\textbf{ \technique } } \\
    \cellcolor[gray]{.75}\textbf{Model}& \cellcolor[gray]{.75}\textbf{Cases }& \multirow{ 2}{*}{\emph{Repaired} \scriptsize{(\%)}} &{\emph{Avg.}} & \multirow{ 2}{*}{\emph{Correct} \scriptsize{(\%)}}& \multirow{ 2}{*}{\emph{Incorrect} \scriptsize{(\%)}}& \multirow{ 2}{*}{\emph{Repaired} \scriptsize{(\%)}} &{\emph{Avg.}} & \multirow{ 2}{*}{\emph{Correct} \scriptsize{(\%)}}& \multirow{ 2}{*}{\emph{Incorrect} \scriptsize{(\%)}}\\
 \cellcolor[gray]{.75}\textbf{}& \cellcolor[gray]{.75}\textbf{}& \emph{} & \emph{time} & \emph{}& \emph{}& \emph{} & \emph{time} & \emph{}& \emph{} \\\hline\hline
%----
addr & 1 & 1 \emph{\scriptsize(100\%)} & \textit{9010} & 1 \emph{\scriptsize(100\%)} & 0 \emph{\scriptsize(0\%)} & 1 \emph{\scriptsize(100\%)} & \textit{351} & 1 \emph{\scriptsize(100\%)} & 0 \emph{\scriptsize(0\%)} \\ \hline
arr & 2 & 2 \emph{\scriptsize(100\%)} & \textit{7651} & 2 \emph{\scriptsize(100\%)} & 0 \emph{\scriptsize(0\%)} & 2 \emph{\scriptsize(100\%)} & \textit{2394} & 2 \emph{\scriptsize(100\%)} & 0 \emph{\scriptsize(0\%)} \\ \hline
balancedBST & 3 & 2 \emph{\scriptsize(67\%)} & \textit{120276} & 1 \emph{\scriptsize(33\%)} & 1 \emph{\scriptsize(33\%)} & 1 \emph{\scriptsize(33\%)} & \textit{358} & 1 \emph{\scriptsize(33\%)} & 0 \emph{\scriptsize(0\%)} \\ \hline
bempl & 1 & 0 \emph{\scriptsize(0\%)} & \textit{} & 0 \emph{\scriptsize(0\%)} & 0 \emph{\scriptsize(0\%)} & 0 \emph{\scriptsize(0\%)} & \textit{} & 0 \emph{\scriptsize(0\%)} & 0 \emph{\scriptsize(0\%)} \\ \hline
cd & 2 & 2 \emph{\scriptsize(100\%)} & \textit{3302} & 0 \emph{\scriptsize(0\%)} & 2 \emph{\scriptsize(100\%)} & 2 \emph{\scriptsize(100\%)} & \textit{742} & 2 \emph{\scriptsize(100\%)} & 0 \emph{\scriptsize(0\%)} \\ \hline
ctree & 1 & 1 \emph{\scriptsize(100\%)} & \textit{6774} & 1 \emph{\scriptsize(100\%)} & 0 \emph{\scriptsize(0\%)} & 0 \emph{\scriptsize(0\%)} & \textit{} & 0 \emph{\scriptsize(0\%)} & 0 \emph{\scriptsize(0\%)} \\ \hline
dll & 4 & 3 \emph{\scriptsize(75\%)} & \textit{22585} & 0 \emph{\scriptsize(0\%)} & 3 \emph{\scriptsize(75\%)} & 3 \emph{\scriptsize(75\%)} & \textit{2624} & 3 \emph{\scriptsize(75\%)} & 0 \emph{\scriptsize(0\%)} \\ \hline
farmer & 1 & 0 \emph{\scriptsize(0\%)} & \textit{} & 0 \emph{\scriptsize(0\%)} & 0 \emph{\scriptsize(0\%)} & 0 \emph{\scriptsize(0\%)} & \textit{} & 0 \emph{\scriptsize(0\%)} & 0 \emph{\scriptsize(0\%)} \\ \hline
fsm & 2 & 2 \emph{\scriptsize(100\%)} & \textit{6068} & 2 \emph{\scriptsize(100\%)} & 0 \emph{\scriptsize(0\%)} & 1 \emph{\scriptsize(50\%)} & \textit{313} & 1 \emph{\scriptsize(50\%)} & 0 \emph{\scriptsize(0\%)} \\ \hline
grade & 1 & 1 \emph{\scriptsize(100\%)} & \textit{124797} & 0 \emph{\scriptsize(0\%)} & 1 \emph{\scriptsize(100\%)} & 0 \emph{\scriptsize(0\%)} & \textit{} & 0 \emph{\scriptsize(0\%)} & 0 \emph{\scriptsize(0\%)} \\ \hline
other & 1 & 0 \emph{\scriptsize(0\%)} & \textit{} & 0 \emph{\scriptsize(0\%)} & 0 \emph{\scriptsize(0\%)} & 1 \emph{\scriptsize(100\%)} & \textit{3120} & 1 \emph{\scriptsize(100\%)} & 0 \emph{\scriptsize(0\%)} \\ \hline
student & 19 & 12 \emph{\scriptsize(63\%)} & \textit{76120} & 9 \emph{\scriptsize(47\%)} & 3 \emph{\scriptsize(16\%)} & 13 \emph{\scriptsize(68\%)} & \textit{71197} & 13 \emph{\scriptsize(68\%)} & 0 \emph{\scriptsize(0\%)} \\ \hline
\rowcolor{black!5}\textbf{Total:}    & 38 & 26 \emph{\scriptsize(68\%)} & \textit{41843} & 16 \emph{\scriptsize(42\%)} & 10 \emph{\scriptsize(26\%)} & 24 \emph{\scriptsize(63\%)} & \textit{10137} & 24 \emph{\scriptsize(63\%)} & 0 \emph{\scriptsize(0\%)} \\ \hline
\end{tabular}
\end{small}
\end{center}
\label{arepair-experiments}
\end{table*}

 
\begin{table*}[ht]
    \caption{Experiments taken from Alloy4Fun's benchmarks.} 
\begin{center} 
\begin{small}
\begin{tabular}{|l|r||r|r|r|r||r|r|r|r|}
\hline
\rowcolor{black!25}
\cellcolor[gray]{.75}\textbf{} & \cellcolor[gray]{.75}\textbf{Total} &\multicolumn{4}{|c||}{\textbf{ARepair }} &  \multicolumn{4}{|c|}{\textbf{ \technique } } \\
    \cellcolor[gray]{.75}\textbf{Model}& \cellcolor[gray]{.75}\textbf{Cases }& \multirow{ 2}{*}{\emph{Repaired} \scriptsize{(\%)}} &{\emph{Avg.}} & \multirow{ 2}{*}{\emph{Correct} \scriptsize{(\%)}}& \multirow{ 2}{*}{\emph{Incorrect} \scriptsize{(\%)}}& \multirow{ 2}{*}{\emph{Repaired} \scriptsize{(\%)}} &{\emph{Avg.}} & \multirow{ 2}{*}{\emph{Correct} \scriptsize{(\%)}}& \multirow{ 2}{*}{\emph{Incorrect} \scriptsize{(\%)}}\\
 \cellcolor[gray]{.75}\textbf{}& \cellcolor[gray]{.75}\textbf{}& \emph{} & \emph{time} & \emph{}& \emph{}& \emph{} & \emph{time} & \emph{}& \emph{} \\\hline\hline
 %----
Graphs & 305 & 276 \emph{\scriptsize(90\%)} & \textit{2625} & 18 \emph{\scriptsize(6\%)} & 258 \emph{\scriptsize(85\%)} & 248 \emph{\scriptsize(81\%)} & \textit{6734} & 248 \emph{\scriptsize(81\%)} & 0 \emph{\scriptsize(0\%)} \\ \hline
LTS & 282 & 165 \emph{\scriptsize(59\%)} & \textit{8729} & 7 \emph{\scriptsize(2\%)} & 158 \emph{\scriptsize(56\%)} & 42 \emph{\scriptsize(15\%)} & \textit{5999} & 42 \emph{\scriptsize(15\%)} & 0 \emph{\scriptsize(0\%)} \\ \hline
Trash & 229 & 220 \emph{\scriptsize(96\%)} & \textit{4077} & 68 \emph{\scriptsize(30\%)} & 152 \emph{\scriptsize(66\%)} & 199 \emph{\scriptsize(87\%)} & \textit{4915} & 199 \emph{\scriptsize(87\%)} & 0 \emph{\scriptsize(0\%)} \\ \hline
Production & 63 & 47 \emph{\scriptsize(75\%)} & \textit{6232} & 8 \emph{\scriptsize(13\%)} & 39 \emph{\scriptsize(62\%)} & 56 \emph{\scriptsize(89\%)} & \textit{4124} & 56 \emph{\scriptsize(89\%)} & 0 \emph{\scriptsize(0\%)} \\ \hline
Classroom & 1168 & 911 \emph{\scriptsize(78\%)} & \textit{95717} & 92 \emph{\scriptsize(8\%)} & 819 \emph{\scriptsize(70\%)} & 418 \emph{\scriptsize(36\%)} & \textit{82856} & 418 \emph{\scriptsize(36\%)} & 0 \emph{\scriptsize(0\%)} \\ \hline
CV & 162 & 132 \emph{\scriptsize(81\%)} & \textit{4966} & 4 \emph{\scriptsize(2\%)} & 128 \emph{\scriptsize(79\%)} & 82 \emph{\scriptsize(51\%)} & \textit{2805} & 82 \emph{\scriptsize(51\%)} & 0 \emph{\scriptsize(0\%)} \\ \hline
\rowcolor{black!5}\textbf{Total:}      & 2209 & 1751 \emph{\scriptsize(79\%)} & \textit{20391} & 197 \emph{\scriptsize(9\%)} & 1554 \emph{\scriptsize(70\%)} & 1045 \emph{\scriptsize(47\%)} & \textit{17905} & 1045 \emph{\scriptsize(47\%)} & 0 \emph{\scriptsize(0\%)} \\ \hline
\end{tabular}
\end{small}
\end{center}
\label{alloy4fun-experiments}
\end{table*}
 

\subsection{Discussion}

Let us discuss the evaluation results. For \textbf{RQ1}, the results are conclusive: the impact of pruning is significant. Let us remark that the efficiency speed up is better than the increase in repairability (38X to 85X speed up, as opposed to roughly 2.5X increase in repairability). This may be explained by the timeout that we have set: 1 hour may be a small timeout for specification repair using \technique: increasing it may show a repairability increase closer to the speed up. Another important issue about these results is that the semantic check that we need to perform for pruning using variabilization, does in fact pay off. In other words, the variabilization checks, that require additional calls to the SAT solver, implied a time saving thanks to pruning that improved the overall analysis time. This, of course, is relative to the considered case studies. We did not observe any case where the overhead caused by pruning made the tool to actually take longer to repair a faulty specification, which may in fact happen for a specification, if most feasibility checks succeed, consuming time and leading to no pruning. The benchmarks were taken from other authors' work; we did not purposely look for specifications that may favor or harm the pruning strategies. We plan to design synthetic specifications, and extend the set of case studies, to further assess the effect of pruning.

Regarding \textbf{RQ2}, the comparison between \technique\ and ARepair can be analyzed along various dimensions. Let us first consider the evaluation over ARepair's benchmark. For this benchmark, the test suites used for running ARepair are solely composed of automatically generated tests, using AUnit with predicate coverage. As a result, the number of correct specification fixes differ from the experiments in \cite{Wang+2018}, where manually designed test cases helped the tool from overfitting. In our current experiments, ARepair is affected by overfitting: 16 out of the 26 produced fixes are correct fixes. \technique\ outperforms ARepair in terms of the number of repaired models: 16 models repaired by ARepair, against 24 repaired by \technique\ (a 21\% difference in the number of repaired models, over the size of the benchmark). It is worth remarking that the two techniques complement each other in terms of the repaired models: ARepair is able to repair models that \technique\ does not repair (see for instance \texttt{ctree} and \texttt{fsm}), and \technique\ repairs models that ARepair is not able to repair (see for instance \texttt{student} and \texttt{other}). In terms of efficiency, both tools show comparable running times. The average time to produce a repair is however just a reference, since the tools perform different kinds of tasks. \technique\ does not include fault localization, so the times here account for absolute repair times, given that the faults have been localized offline. ARepair, on the other hand, includes both the time to localize faults and perform the repair. Let us remark however that, in ARepair, on average 62\% of the time corresponds to repair and 38\% to fault localization. Unlike ARepair, that alternates between patching and calling fault localization, \technique\ calls fault localization only once, before triggering repair. As such, the proportion of time devoted to fault localization is much less. In our experiments, when we consider the combination of fault localization and {\technique}, on average 4\% is devoted to fault localization (in the worst case, Student6, the fault localization time was 13\% of the total time). Further details can be found in the tool's site (see below).  

Now let us consider the Alloy4Fun benchmark. For this benchmark, we did not have any choice but to automatically generate test cases, as these were not available for these models. We generated test cases automatically, using AUnit \cite{Sullivan+2018} (again, using the best performing generation criterion, as reported in \cite{Sullivan+2018}). ARepair is able to repair a significant number of models: 1751 out of 2209. However, only 197 were correct fixes; the remaining 1554 were overfitting cases, that passed the automatically generated tests, but were not correct fixes for the corresponding specifications. \technique, on the other hand, produced a smaller number of fixes: 1042 out of the 2209. But since it uses Alloy assertions as repair oracles, instead of test cases, it showed no overfitting issues for these specifications. As a result, \technique\ shows a better effectiveness in repair: 47\% of correctly repaired models by \technique, against 9\% of correctly repaired models by ARepair. Regarding the cases themselves, again, the tools complement each other: there are cases correctly repaired by one tool that were not repaired by the other, and vice versa. 

The observed overfitting is an important difference between the two tools and their approaches, and confirms our intuition and motivation regarding the use of stronger repair oracles, that naturally come in specifications. Clearly, one may argue that ARepair's performance, in terms of overfitting, can be improved by feeding the tool with different/stronger test suites. We fully agree, and in fact, this is confirmed with ARepair's benchmark: if the test suites used in \cite{Wang+2018} are fed to ARepair (which, as we mentioned, include manually crafted tests), then 26 out of 38 models are repaired, compared to the 14 out of 38 repaired models obtained with just automatically generated tests (effectiveness is increased from 42\% to 68\%). Writing the \emph{right} set of test cases for specification repair is a time consuming task, that would require a manual design of a test suite for each of the models, to improve the tool's results. The overfitting problem is an inherent problem of using tests as specifications, and thus it is expected of tools such as ARepair. 

It is important to remark that we do not claim that our technique leads to no overfitting, since this will depend on the oracle being used, and how faithfully it captures the developer's intentions. In the case of our controlled experiments, where we had the ground truths as oracles (which would not be the general case in formal specification), we had no overfitting, although overfitting may still have been observed due to the bounded nature of the analysis. In any case, being forced to use test cases as opposed to more general properties makes it more prone to overfitting. %In relation to this issue, the difference in effectiveness between ARepair and \technique\ suggests a line of future work: to find mechanisms that can automatically produce additional tests to feed ARepair, in an ``on demand'' fashion and from available assertions, to overcome overfitting. 

Other attributes of the generated patches may be considered. One of these is readability. We can remark that candidate patches are built out of mutations of the faulty expressions, and the space of faulty expressions is visited in breadth-first. Therefore, simpler/shorter fix candidates are considered first. While we did not evaluate readability in a systematic fashion, {\technique}'s patches can be simpler and clearer than manual, human-written ones. For instance, for Production.Inv4 in the Alloy4Fun benchmark, the faulty expression:
\begin{lstlisting} []
all c: Component | 
    (c.parts).position in (c.position).^~next
\end{lstlisting} 
is manually fixed by a student with the following expression: 
\begin{lstlisting} []
all c: Component | 
    ((c.^parts) & Component).position not in 
    (c.position).^next or no (c.^parts & Component)
\end{lstlisting}
\technique\, on the other hand, produces the following:
\begin{lstlisting} []
all c : one Component | 
    c.parts.position in c.position.~*next
\end{lstlisting}

Another dimension to consider is efficiency of our technique, compared with manual repairs. In Alloy4Fun we can measure the effort of human patches, by considering the time of the sessions of a same student, from defect introduction to its fixing. On average, it takes a student about 10 minutes to fix a defect, once it is introduced. On the other hand, the average time to repair in the case of \technique\ is about 10 seconds. For instance, for the above faulty specification, it took the student a total of 49 minutes to get it right. \technique\ repaired it in 3 seconds. Due to space reasons, we do not present here a more detailed comparison. The benchmarks, the tool's output with further statistical information, and the tool itself, can be found in the tool's site (see below).  

