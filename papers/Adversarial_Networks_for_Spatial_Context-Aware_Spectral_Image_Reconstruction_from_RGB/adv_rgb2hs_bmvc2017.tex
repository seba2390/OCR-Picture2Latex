%%%%%%%%% PREAMBLE
\documentclass{bmvc2k}

% aitor's packages, macros etc.
% Andy's minisection macro.
\usepackage{amsfonts}
\usepackage{mathtools}  % e.g. for \DeclarePairedDelimiter :\abs{} and \abs*{}
\DeclarePairedDelimiter\abs{\lvert}{\rvert}%
\DeclarePairedDelimiter\norm{\lVert}{\rVert}%
\DeclareMathOperator*{\argmin}{\arg\!\min}
%https://tex.stackexchange.com/questions/43008/absolute-value-symbols

\usepackage{gensymb}  % degree

% https://tex.stackexchange.com/questions/56163/subfigure-error-missing-number-treated-as-zero
% https://tex.stackexchange.com/questions/64858/how-to-create-subfloat-figures-two-in-first-row-and-one-below
% http://latex.org/forum/viewtopic.php?f=45&t=9186   <------------------
% subfigure is outdated and new one is subfig which introduces subfloat command. You may consider using subfig instead of subfigure.
\usepackage{graphicx,subfigure}

\newcommand{\minisection}[1]{\vspace{0.04in} \noindent {\bf #1}\ \ }

\usepackage[normalem]{ulem}  % \sout command for text strikeout

%/ aitor's packages, macros etc.

%% Enter your paper number here for the review copy
%\bmvcreviewcopy{716}

\title{Adversarial networks for spatial context-aware spectral image reconstruction from RGB}

% Enter the paper's authors in order
% \addauthor{Name}{email/homepage}{INSTITUTION_CODE}
\addauthor{Aitor Alvarez-Gila}{aitor.alvarez@tecnalia.com}{12}
\addauthor{Estibaliz Garrote}{estibaliz.garrote@tecnalia.com}{1}
\addauthor{Joost van de Weijer}{joost@cvc.uab.es}{2}

% Enter the institutions
% \addinstitution{Name\\Address}
\addinstitution{
 Tecnalia\\
 Derio, Spain
}

\addinstitution{
 Computer Vision Center,\\
 Universitat Aut\`onoma de Barcelona,\\
 Barcelona, Spain
}

\runninghead{Alvarez-Gila, et al.}{Adversarial spectral image reconstruction}

% Any macro definitions you would like to include
% These are not defined in the style file, because they don't begin
% with \bmva, so they might conflict with the user's own macros.
% The \bmvaOneDot macro adds a full stop unless there is one in the
% text already.
\def\eg{\emph{e.g}\bmvaOneDot}
\def\Eg{\emph{E.g}\bmvaOneDot}
\def\etal{\emph{et al}\bmvaOneDot}

%-------------------------------------------------------------------------
% Document starts here
\begin{document}

\maketitle

\begin{abstract}

Hyperspectral signal reconstruction aims at recovering the original spectral input that produced a certain trichromatic (RGB) response from a capturing device or observer.
Given the heavily underconstrained, non-linear nature of the problem, traditional techniques leverage different statistical properties of the spectral signal in order to build informative priors from real world object reflectances for constructing such RGB to spectral signal mapping.
However, they treat each sample independently, and thus do not benefit from the contextual information that the spatial dimensions can provide. 
We pose hyperspectral natural image reconstruction as an image to image mapping learning problem, and apply a conditional generative adversarial framework to help capture spatial semantics. 
This is the first time Convolutional Neural Networks -and, particularly, Generative Adversarial Networks- are used to solve this task. Quantitative evaluation shows a Relative Root Mean Squared Error drop of \textbf{XXXXXXX} on the ICVL natural hyperspectral image dataset.

\end{abstract}


%%%%%%%%% BODY TEXT
%###############################################################################
\section{Introduction}
\label{sec:intro}

Hyperspectral (HS) imaging has gained relevance over the last couple of years in the applied vision community. 
Remote sensing, UAV-based imaging, precision agriculture or autonomous driving are only some of the fields that are already benefiting from the use of imaging devices that provide a response that spans the spectral dimension with narrow-band channels to produce an image with higher spectral resolution than the standard RGB trichromatic one.

While the evolution of HS imaging devices has undergone major breakthroughs, it is also true that there is still a trade-off inherent to the fact that we are ultimately capturing three dimensional information with a two dimensional sensor, which limits the quality or resolution of the acquired signal in either of those dimensions: spatial, spectral or temporal. 
On top of that, the cost of such devices is orders of magnitude above that of conventional RGB cameras.

In this context, HS signal reconstruction from broadband or limited acquisition channels (typically, from RGB sensors) arises as a natural computational alternative, either to compete against native HS systems or to be included as part of their signal post-processing backends. 
The spectral reconstruction problem is a severely underconstrained, highly non-linear one, and the algorithms trying to solve this mapping should exploit the low dimensionality of the natural HS images~\cite{chakrabarti2011statistics} and learn informative priors of diverse forms from real world object reflectances, to be leveraged in the reconstruction phase.
Note, however, that most of the existing solutions handle each pixel individually.
By doing so, they are not taking advantage of the latent contextual information available in the spatially local neighborhood~\cite{chakrabarti2011statistics}.

Generative adversarial Networks (GAN) is a class of neural networks which have shown to be able to successfully generate samples from the complex manifold of real images.
In this work, we use this class of algorithms to learn a generative model of the joint spectro-spatial distribution of the data manifold of natural HS images use it to optimally exploit spatial context information.
To our knowledge, this is the first time Convolutional Neural Networks (CNN) are used in the task of spectral reconstruction of natural images.
We quantitatively evaluate our approach on the largest HS natural image dataset available to date, i.e. ICVL, by comparing against~\cite{arad_sparse_2016}, and show an error drop of \textbf{XXXXXXXX} over their state of the art results.

%###############################################################################
\subsection{Related work}
\label{sec:related}

A number of previous works are relevant to the proposed approach.
This task was first addressed by isolating its spatial component and focusing on the reconstruction of homogeneous, well-established reflectances of real world surfaces such as Munsell color chips, either from multispectral, or RGB components~\cite{heikkinen_evaluation_2008} or from the tristimulus values~\cite{ayala_use_2006}.

Initial attempts on the spectral reconstruction of natural images from full size RGB input required additional constrains or multiple input forms to help in their task:~\cite{kawakami_high-resolution_2011} and~\cite{cao_high_2011} use the aid of a low resolution HS measurement in addition to the RGB input,~\cite{lopez-alvarez_using_2008} restricts to the skylight samples domain, and~\cite{park_multispectral_2007,parmar_spatio-spectral_2008,goel_hypercam:_2015} rely on the aid of computational photography-like multiplexed narrow band lighting. The latter does, however, use spatial information for learning, as does~\cite{chakrabarti2011statistics}, which focuses on the statistics for this class of images and defines a representation basis and computation method for the associated coefficients, but does not tackle reconstruction.

Solutions relying on a single RGB image input at test time are scarce, and none of them leverage the spatial context:~\cite{nguyen_training-based_2014} uses a Radial Basis Function network and produces an estimate of scene reflectance and global illuminant, but assumes a known camera color matching function, and directly depends on the performance of a white balancing stage as part of the workflow.
\cite{arad_sparse_2016} learns a sparse dictionary of HS signatures as bases for the reconstruction.
By treating each pixel independently, the ability to use the surround information is lost e.g. for producing distinct spectral outputs for metameric RGB pairs dependent on the context. 

Finally, there exist a certain relation between the HS reconstruction and the image colorization~\cite{Cheng_2015_ICCV} tasks, which has been previously addressed in a similar fashion~\cite{zhang_colorful_2016,pix2pix2016}, but under different evaluation requirements.
We can think of the former being a generalization of the latter for an arbitrary number of input/output channels. 

None of these methods would have been possible without the existence of publicly available HS natural image datasets. Until recently, the amount of images per set was the limiting factor for the development of HS reconstruction algorithms that learn on the basis of images or image patches
\cite{foster_frequency_2006,yasuma_generalized_2010,chakrabarti2011statistics,nguyen_training-based_2014,eckhard_outdoor_2015,foster_time-lapse_2016}.~\cite{arad_sparse_2016} changed this releasing a set of $201$ high resolution images that we prove is enough for the successful training of deep neural networks.   

%###############################################################################
\section{Adversarial spectral image reconstruction from RGB}
\label{sec:adv_rgb2hs}

This section describes the core functioning of our method, along with some of the mathematical developments that derived into the proposed models.
%------------------------------------------------------------------------------------------
\subsection{Adversarial learning}

\minisection{Generative Adversarial Networks (GANs)} 
GAN-s \cite{goodfellow_generative_2014} are generative statistical models that learn to produce realistic samples $y$ that lay in the data manifold by relying on a setup consisting on two competing agents: the generator $G$ takes  noise $z$ as input as a source of randomness, and creates \emph{fake} data samples $G(z)$.
It is trained to make the generated samples as realistic as possible.
On the other end, the aim of the discriminator, $D$, which randomly takes as input both samples from the training data set and those generated by $G$, is to learn to tell if the received input samples are real or fake.
Typically, both $G$ and $D$ are neural nets, and they are trained iteratively to progressively become better in their respective tasks.
The objective function associated to such a setting is:
\begin{equation} \label{eq:l_gan}
\mathcal{L}_{GAN}(G,D)= \mathop{\mathbb{E}_{y\sim p_{data}(y)}} [\log D(y)] + \mathop{\mathbb{E}_{z\sim p_{noise}(z)}} [\log(1-D(G(z)))] 
\end{equation}
where $G$ tries to minimize this loss and $D$ attempts to maximize it, yielding  the objective function:
\begin{equation} \label{eq:minmax_lgan}
G^* = \argmin_{G} \max_{D} \mathcal{L}_{GAN}
\end{equation}

This adversarial framework has successfully been applied to the unsupervised generation of data of different modalities, including natural images~\cite{denton_deep_2015}, and empirical architecture guidelines for $G$ and $D$ have been derived~\cite{radford_unsupervised_2015} for such cases, along with common tricks to stabilize the training process~\cite{salimans_improved_2016}.

\minisection{Conditional Generative Adversarial Networks (cGANs)} 
cGANs~\cite{mirza2014conditional} extend this framework by feeding both $G$ and $D$ with additional information $x$ to be used to condition on the output of the generator. 
Such conditioning input could adopt different modalities, and range from simple categorical labels~\cite{mirza2014conditional} to more sophisticated content, such as text~\cite{reed_generative_2016}  or images~\cite{Li2016}, either alone or as a combination of multiple input  modalities~\cite{reed_learning_2016,zhu2016generative}). 
This has been proved useful for a number of tasks and output types~\cite{wang_generative_2016, mathieu2015deep}.
Eq.~\ref{eq:l_cgan} shows the updated loss function for conditional GANs. In this case, $G$ attempts to generate images that look realistic given the additional provided input $x$ (be it the class of $y$, a descriptive text, or an additional image), and $D$ tries to determine whether the given $(x,y)$ pair makes sense or not as a mapping.   
\begin{equation} \label{eq:l_cgan}
\mathcal{L}_{cGAN}(G,D) = \mathop{\mathbb{E}_{x,y\sim p_{data}(x,y)}} [\log D(x,y)] + \mathop{\mathbb{E}_{x\sim p_{data}(x),z\sim p_{noise}(z)}} [\log(1-D(x, G(x,z)))]
\end{equation}
As a result, cGANs open the door to using generative statistical modeling for our HS reconstruction problem by conditioning the generation of an HS outcome on a given input RGB image.  

\minisection{Adversarial image to image mapping}
Many modern computer vision tasks can better be regarded under the common reference framework of image to image mapping learning, in which a generator model $G$ is learned that translates an input image $x$ into the most probable representation $y$ of such image in the output domain.
This is the case \eg for semantic segmentation~\cite{shelhamer_fully_2016}, instance segmentation~\cite{Dai_2016_CVPR}, or depth and surface normal estimation from single image~\cite{bansal_pixelnet:_2017}, among others. 
Most of these tasks have been recently addressed making use of Convolutional Neural Networks (CNNs) that yield deterministic results as generators, and which are specifically tailored, in terms of architecture design, objective function or other specific training details, for their respective tasks. 

There are, in addition, some tasks for which this mapping is not unique, and one same input image could have multiple equally correct representations in the output domain.
 Realistic image rendering from semantically labeled images (inverse of the semantic segmentation problem) or from hand-drawn sketches, or image colorization~\cite{Cheng_2015_ICCV}, are just a few examples of this. 
The choice of the objective functions to use in each of these cases is a particularly challenging design aspect; applying an otherwise useful $\ell_2$ loss to $x,y$ image pairs is known to be problematic and yield blurry results~\cite{larsen_autoencoding_2015}, as the generator tends to average over the space of valid image representations.  

For all of the above,~\cite{pix2pix2016} proposes a common image to image mapping learning framework based on the cGAN adversarial setting, which, provided that one can feed it with co-registered image pairs of input and output domains, is able to learn the most suitable loss function for each of the tackled tasks in a data-driven approach. This is done implicitly using the adversarial objective from eq.~\ref{eq:l_cgan}, enforced by the discriminator trying to identify the fake images and, this way, encouraging the generator to become better at trying to deceive it. 

By doing this,~\cite{pix2pix2016} manages to get rid of the blur inherent to $\ell_2$ distance-based models and produce sharp results.
Nevertheless, it has been previously shown~\cite{Pathak_2016_CVPR, shrivastava_learning_2016} that combining one of the traditional loss functions with the adversarial objective $\mathcal{L}_{cGAN}$ can help produce more spatially consistent results and make the generator less prone to artifacts inherent to the adversarial scheme.
They thus place an additional $\ell_1$ term (eq.\ref{eq:ll1}) on the generator, which is known to yield less blur:

\begin{equation} \label{eq:ll1}
\mathcal{L}_{\ell_1}(G) = \mathop{\mathbb{E}_{x,y\sim p_{data}(x,y),z\sim p_{noise}(z)}} [\|y-G(x,z)\|_1]
\end{equation}
and produce the following combined objective function:
\begin{equation} \label{eq:minmax_lgan_lone}
G^* = \argmin_{G} \max_{D} \mathcal{L}_{cGAN}(G,D) + \lambda\mathcal{L}_{\ell_1}(G)
\end{equation}
where $\lambda$ is a weighting factor for the $\ell_1$ term, which is set to 100 in~\cite{pix2pix2016}. 
In essence, $\mathcal{L}_{cGAN}(G,D)$ would be in charge of producing sharp, realistic looking results, while $\ell_1$ takes care of the global image structure.

Interestingly, the stochastic output pursued by the noise input to cGAN-like models does not manifest itself under this design (see details in section \ref{sec:architecture}), and the resulting mapping is a fundamentally deterministic one.
A probable interpretation is the generator learning to ignore its effect. 
As a consequence, ~\cite{pix2pix2016} gets rid of the noise input and leaves train and test-time dropout as unique source of randomness.

\minisection{Adversarial spectral reconstruction networks} The forward correspondence learning between the RGB and hyperspectral signals is a heavily under-constrained one, which could benefit from an approach that aims at exploiting the underlying priors present in both the spectral and spatial dimensions and learn a model that specifically produces realistic outcomes as a target. 
It not only requires mapping a 3-dimensional image to a  much higher dimensional one (typically 31 spectral channels and the two spatial dimensions), but such mapping can be context-dependent as well, as is in the case of metameric colors.
The inverse mapping, however, i.e. the rendition of RGB images from their spectral counterparts, is well defined, and deterministic under the only assumption of the color matching functions defining the observer, or the spectral sensitivity functions that characterize specific sensors.
This makes it immediate to generate perfectly aligned (RGB, hyperspectral) image pairs (see section \ref{sec:experimental_evaluation} for details) to be used under the described solution.

Hyperspectral image reconstruction from RGB can then be posed as one of the aforementioned image to image mapping learning problems and thus be solved under the conditional adversarial network-based image to image translation framework proposed by~\cite{pix2pix2016}.

The resulting adversarial and combined objectives would then become:
\begin{equation} \label{eq:l_hscgan}
\mathcal{L}_{adv} = \mathop{\mathbb{E}_{I_{rgb},I_{hs}\sim p_{data}(I_{rgb},I_{hs})}} [\log D(I_{rgb}, I_{hs})] + \mathop{\mathbb{E}_{I_{rgb}\sim p_{data}(I_{rgb})}} [\log(1-D(I_{rgb},G(I_{rgb})))]
\end{equation}
\begin{equation} \label{eq:ltot}
\mathcal{L}_{rgb2hs}(G,D) = \mathcal{L}_{adv} + \lambda\mathop{\mathbb{E}_{I_{rgb},I_{hs}\sim p_{data}(I_{rgb},I_{hs})}} [\|I_{hs}-G(I_{rgb})\|_1]
\end{equation}
where $I_{hs}$ represents the original hyperspectral image and $I_{rgb}$ is the corresponding input image in the RGB domain. Note that we have explicitly removed any reference to the input noise, and the RGB image remains as the only input to the generator. 

Figure \ref{fig:overview} shows an overview of the whole adversarial spatial context-aware spectral image reconstruction process. We depart from a database of perfectly aligned RGB and hyperspectral image pairs, which are extracted one pair at a time. In a first iteration, a first pair of real images of size $H\times W$ is taken: $\{I_{RGB}, I_{HS}\}$. The generator $G$ takes $I_{RGB}$ as input, and yields the corresponding hyperspectral reconstruction of size $H\times W$, $\hat{I}_{HS}$. The discriminator $D$ is now fed with two pairs of images, $\{I_{RGB}, I_{HS}\}$ and $\{I_{RGB}, \hat{I}_{HS}\}$ and uses the associated labels indicating if they are real or fake $\{1,0\}$ to compute the adversarial loss and update its gradients. $G$'s weights are also updated, and they continue to become better at their respective tasks iteratively.

%_________________________________________
\begin{figure*}
	\begin{center}
        %\fbox{\includegraphics[width=0.9\linewidth]{images/fig1_overview.pdf}}
        \includegraphics[width=1.0\linewidth]{images/fig1_overview.pdf}
	\end{center}
	\caption{Adversarial spatial context-aware spectral image reconstruction model}
	\label{fig:overview}
\end{figure*}
%_________________________________________

%------------------------------------------------------------------------------------------
\subsection{Architecture design and training}
\label{sec:architecture}

As for the specific implementation of the models, since $G$ needs to yield full-size detailed images, a \emph{U-Net}-like architecture~\cite{ronneberger_u-net:_2015} is used.
Regular autoencoder networks~\cite{kingma_auto-encoding_2013} exhibit a progressively reduced representation size until a bottleneck layer and there is no way for the last layers of accessing the original data, which negatively affects the results when we aim at detailed outcomes. 
Unlike these, the \emph{U-Net} incorporates skip connections between layers of equal representation size, and concatenates local activations from the upscaling phase with those coming from the downscaling stages, which has shown to achieve superior performance on tasks were the details are relevant.
It was first proposed with semantic segmentation tasks in mind, but original spectral signal reconstruction falls within the kind of tasks that can clearly benefit from accessing the original input  levels at each sample (i.e. pixel).

The discriminator $D$, defined as \emph{PatchGAN}, is simpler in terms of convolutional layer count, and is focused solely on modeling high-frequency structure. Each of the $M\times M$ output neurons is restricted to see only a limited $N\times N$ receptive field from the input image, which can be significantly smaller than the input image size.
Consequently, only the adversarial loss term is placed over $D$ (eq \ref{eq:minmax_lgan_lone}).

The use of this design solution for $D$ is consistent with our initial hypothesis that local spatial context can help better reconstruct the spectral signal. 
% Specifically, if we consider the dichromatic reflection model introduced by \cite{shafer_using_1985}:
% \begin{equation} \label{eq:drm}
% \mathbf{f}=e(m^b\mathbf{c}^b + m^i\mathbf{c}^i)
% \end{equation}
% where $\mathbf{c}^i$ represents the illuminant, $\mathbf{f}$ is the trichromatic response vector and $\mathbf{c}^b$ is the vector representing the object;
Specifically, we argue that the proposed approach can help disentangling the illuminant and object body reflectance components of a pixel's trichromatic response, as defined by the dichromatic reflection model \cite{shafer_using_1985}.
The design of $D$, with its attached $\ell_1$ objective, helps capture the high frequencies that characterize the textures in the image.
These are, together with the body color component, one of the main features characteristic of the different materials which, ultimately, produce distinct spectral responses. Therefore, convolutionally integrating the trichromatic response of adjacent pixels should yield a better estimate of the central spectral response. 
To this respect, the \emph{PatchGAN} design isolates $D$'s response associated to pixels separated by more than one input patch.
For small enough patch sizes, this effectively implies that the discriminator is learning a loss function tailored for texture or material recognition, making sure that the reconstructed spectra falling within the patch are not only plausible in the spectral domain, but also spatially consistent in the close proximities. 

The illuminant-specific component of ~\cite{shafer_using_1985}, on the other hand, is typically largely constant or slowly varying across big portions of the image (especially in terms of chromaticity and conversely, spectral shape), and the $\ell_1$ norm does a good job taking care of its global image-wide consistency, along with that of the the low and mid frequency spatial structures. 

\minisection{Avoiding Batch Normalization}
Given the intrinsically exact nature of our task (some of the described design choices help leverage spatial structure consistency for our task, but we ultimately want the reconstructed spectra to be accurate), we choose to remove all the Batch Normalization~\cite{ioffe_batch_2015} layers present in the generator architectures proposed in~\cite{pix2pix2016}.
While this technique has proven to be useful to help accelerate and regularize the training process for a wide variety of tasks by reducing the internal covariate shift, the fact that it makes the signal loose track of its original value, along with the deterministic nature of the desired output, makes it non-advisable for reconstruction tasks. 
We experimentally found that including Batch Normalization produced inferior results.

%------------------------------------------------------------------------------------------
\subsection{Implementation details}
\label{sec:implementation}
We now provide some details on the configurations used for our implementation.

We use Keras with Theano backend and take the implementation of~\cite{pix2pix2016} made by~\cite{costa_towards_2017} as starting point, conveniently modifying it for our purposes.
We use Adam optimizer~\cite{kingma_adam:_2015} for both $G$ and $D$, with a learning rate of $2\cdot10^{-4}$ and $\beta_1=0.5$. We use a minibatch size of 1 in order to benefit from the regularization provided by the gradient estimation noise~\cite{keskar_large-batch_2017}, and following common practice~\cite{pix2pix2016}. The training is performed iteratively and alternates between the two models. Concretely, at each step, the discriminator is first trained for $50$ iterations and then the generator gets trained for $25$ more minibatches. 
Further network architecture details are provided in section $1$ of the Appendix. 

We crop the original $1392\times 1300$ images during the training phase by extracting one random crop of size $256\times 256$ (the $H,W$ values from section \ref{sec:adv_rgb2hs}) per image and epoch.
The models are fed with these crops during training, while, for the testing phase, each full size RGB image is divided in tiles of $256x256$ with no overlap.
Each tile gets processed by the generator independently and we reconstruct the full image back before evaluating it.

%###############################################################################
\section{Experimental evaluation}
\label{sec:experimental_evaluation}

This section contains an overview of the experiments performed in an attempt to quantitatively assess our algorithm's performance as compared to previous methods.

%------------------------------------------------------------------------------------------
\subsection{Dataset}
Given the amount of images, diversity and resolution, we evaluate our approach on the dataset presented in~\cite{arad_sparse_2016}. 
At the time of writing, it comprised $201$ hyperspectral images (see fig.~\ref{fig:srgb_samples} for RGB renditions of a few random samples) of $1392\times1300$ spatial resolution and $519$ spectral bands in the $400nm-1000nm$ range, with a spectral resolution of 1.25nm. 
As for the acquisition, a \emph{Specim PS Kappa DX4} hyperspectral camera was used, together with a rotary stage for spatial scanning.
This aspect is noticeable in some of the samples, in which common objects such as cars exhibit aspect ratios that do not match those we find in real life.
There is also a spectrally downsampled version of $31$ bands in the $400nm-700nm$ range. Following practice from~\cite{arad_sparse_2016}, we use the latter for our reconstruction experiments.

%------------------------------------------------------------------------------------------
\subsection{Preparation}
\label{ssec:preparation}
%_________________________________________
% http://latex.org/forum/viewtopic.php?f=45&t=9186
\begin{figure}
\centering     %%% not \center
\subfigure{\label{fig:srgb_samples1}\includegraphics[width=0.15\textwidth]{images/1.jpg}}
\subfigure{\label{fig:srgb_samples2}\includegraphics[width=0.15\textwidth]{images/2.jpg}}
\subfigure{\label{fig:srgb_samples3}\includegraphics[width=0.15\textwidth]{images/3.jpg}}
\subfigure{\label{fig:srgb_samples4}\includegraphics[width=0.15\textwidth]{images/4.jpg}}
\subfigure{\label{fig:srgb_samples5}\includegraphics[width=0.15\textwidth]{images/5.jpg}}
\subfigure{\label{fig:srgb_samples6}\includegraphics[width=0.15\textwidth]{images/6.jpg}}\\
\subfigure{\label{fig:srgb_samples7}\includegraphics[width=0.15\textwidth]{images/7.jpg}}
\subfigure{\label{fig:srgb_samples8}\includegraphics[width=0.15\textwidth]{images/8.jpg}}
\subfigure{\label{fig:srgb_samples9}\includegraphics[width=0.15\textwidth]{images/9.jpg}}
\subfigure{\label{fig:srgb_samples10}\includegraphics[width=0.15\textwidth]{images/10.jpg}}
\subfigure{\label{fig:srgb_samples11}\includegraphics[width=0.15\textwidth]{images/11.jpg}}
\subfigure{\label{fig:srgb_samples12}\includegraphics[width=0.15\textwidth]{images/12.jpg}}
% \subfigure{\label{fig:srgb_samples1}\includegraphics[width=0.2\textwidth]{images/srgb_sample1.png}}
% \subfigure{\label{fig:srgb_samples2}\includegraphics[width=0.2\textwidth]{images/srgb_sample2}}
% \subfigure{\label{fig:srgb_samples3}\includegraphics[width=0.2\textwidth]{images/srgb_sample3}}
% \subfigure{\label{fig:srgb_samples4}\includegraphics[width=0.2\textwidth]{images/srgb_sample4}}\\
% \subfigure{\label{fig:srgb_samples5}\includegraphics[width=0.2\textwidth]{images/srgb_sample5.png}}
% \subfigure{\label{fig:srgb_samples6}\includegraphics[width=0.2\textwidth]{images/srgb_sample6}}
% \subfigure{\label{fig:srgb_samples7}\includegraphics[width=0.2\textwidth]{images/srgb_sample7}}
% \subfigure{\label{fig:srgb_samples8}\includegraphics[width=0.2\textwidth]{images/srgb_sample8}}
%\subfigure[Figure B]{\label{fig:srgb_samples4}\includegraphics[width=0.24\textwidth]{images/srgb_sample4}}
\caption{Samples from the ICVL dataset~\cite{arad_sparse_2016} rendered as sRGB.}
\label{fig:srgb_samples}
\end{figure}
%_________________________________________
In order to get the aligned image pairs dataset required by our method, and given the deterministic correspondence between spectral and RGB samples once the observer (or sensor sensitivity functions) and the output color space are specified, we render wide band trichromatic RGB versions of the spectral images in the sRGB color space as follows: we first obtain the CIE $XYZ$ tristimulus values for each spectral image pixel location $x$, making use of the color matching functions corresponding to the CIE 1964 $10\degree$ standard observer:
\begin{equation} \label{eq:trix}
\mathbf{X}(x)=K\sum_{\lambda=400nm}^{700nm}S(\lambda, x)\mathbf{\bar{x}(\lambda)}\Delta\lambda
\end{equation}
%\begin{equation} \label{eq:triy}
%Y(x)=K\sum_{\lambda=400nm}^{700nm}S(\lambda, x)\bar{y}(\lambda)\Delta\lambda
%\end{equation}
%\begin{equation} \label{eq:triz}
%Z(x)=K\sum_{\lambda=400nm}^{700nm}S(\lambda, x)\bar{z}(\lambda)\Delta\lambda
%\end{equation}
where $S(\lambda, x)$ is the relative spectral power distribution of pixel $x$, $\mathbf{X}=\{X,Y,Z\}$, $\mathbf{\bar{x}(\lambda)}=\{\bar{x}(\lambda), \bar{y}(\lambda), \bar{z}(\lambda)\}$ are the color matching functions, $\Delta\lambda=10$ and $K(x)$ is the normalization factor, defined, for illuminant $L(\lambda, x)$, as:
\begin{equation} \label{eq:trik}
K(x)=\frac{100}{\sum_{\lambda=400nm}^{700nm}L(\lambda, x)\bar{y}(\lambda)\Delta\lambda}
\end{equation}
Note that, before going through this computation, the original spectral power distribution captured by the camera for each image $S'(\lambda, x)$ is preprocessed with min value subtraction and max value scaling.
The final step is producing the sRGB renders. We do so by applying the associated $3x3$ transformation matrix and unlinearizing (i.e. \emph{gamma-correcting}) the result with a $1/2.4$ power law gamma with a linear segment in low luminance values.
    %_________________________________________
\begin{figure*}
	\begin{center}
		%\fbox{\rule{0pt}{2in} \rule{.9\linewidth}{0pt}}
        %\fbox{\includegraphics[width=3.8cm]{images/eg1_largeprint.png}}
        \includegraphics[width=0.49\linewidth]{images/_00}
        \includegraphics[width=0.49\linewidth]{images/_01}
        \includegraphics[width=0.49\linewidth]{images/_10}
        \includegraphics[width=0.49\linewidth]{images/_22}
        \includegraphics[width=0.49\linewidth]{images/_20}
        \includegraphics[width=0.49\linewidth]{images/_21}
	\end{center}
	\caption{Sample results for our method. Left, center: sRGB rendition of original and reconstructed hyperspectral signals. Right: Original and reconstructed spectra of eight random pixels identified by the colored dots}
	\label{fig:results}
\end{figure*}
%_________________________________________

While not suffering from the same lack of an adequate performance evaluation method that affects typical generative modeling tasks~\cite{theis_note_2015}, spectral signal reconstruction algorithms assessment is an active research field that lacks consensus on what is the most adequate metric to measure spectral match of signals~\cite{imai_comparative_2002}.
When the signals comprise the visual spectrum, the task can be tackled from a variety of perspectives, ranging from the pure signal processing point of view of spectral curve difference metrics, to a full spectrum of metric families that place different levels of perceptual load on their computation: metameric indexes, CIE $\Delta E$ color difference equations, or weighted spectral metrics.

If we widen the scope onto full reference image difference metrics, little work has been done on the spectral extension of these families~\cite{moan_image-difference_2014}. We here focus on four of the most widely used formulas, namely RMSE (Root Mean Squared Error, computed across the spectral dimension for each pixel and then averaging for whatever number of pixels present in the image or the dataset), RMSERel (i.e. RMSE relative to the value of the real signal), GFC (Goodness of Fit Coefficient~\cite{romero1997linear}) and~\Delta E_{00} (CIEDE2000~\cite{cie_cie_2001}).

%------------------------------------------------------------------------------------------
\subsection{Experiments and discussion}
We compare our method with the only other one having reported on the ICVL dataset, i.e.~\cite{arad_sparse_2016}. In their general experiment over the whole set (we do not have enough samples in each of the domain-specific ones they define to be able to learn), they perform a leave-one out-like procedure, and learn from pixels sampled along the whole set except for the unique image being tested at a time. We choose to split the dataset in two equal partitions, training on one and reporting on the other. Table~\ref{tab:results} compares the obtained RMSE and RMSERel values over the testing set,\textbf{COMMENT ON THIS} while fig.\ref{fig:results} shows the sRGB rendition of original and reconstructed hyperspectral images for some randomly chosen image samples. In addition, for each image, we show the original and estimated spectra for eight randomly selected pixels from the image. Note also that, even though the full size image is built from tiles reconstructed independently, the appearance is globally consistent, and the junctions go unnoticed.

\begin{table}%
	\begin{center}
		\begin{tabular}{|l|ccc|}
			\hline
			Method & RMSE $[0-255]$ & RMSERel & GFC \\
			\hline\hline
			Arad \etal\cite{arad_sparse_2016} & 2.633 & 0.0756 & N/A\\
            \hline
%            val set average results: {'deltae00': 0.0, 'rmse': 2.1735099999999994, 'nb_files': 100, 'rmserel_sum': 9884807.1831273101, 'nb_fin_px': 161373440, 'rmserel': 0.06125423851116584, 'gfc': 0.0}
			\sout{Ours w.blk.bug (avg) $\mathcal{L}_{adv} + 100\mathcal{L}_{\ell_1}$ no BN} & (smart avg) & (smart avg) & N/A\\
   			\sout{Ours w.blk.bug (fold 0)$\mathcal{L}_{adv} + 100\mathcal{L}_{\ell_1}$ no BN} & 2.29209 & 0.0644 & N/A\\          
			\sout{Ours w.blk.bug (fold 1)$\mathcal{L}_{adv} + 100\mathcal{L}_{\ell_1}$ no BN} & 2.17351 & 0.061254 & N/A\\
			\hline
            Ours no.blk.bug (avg) $\mathcal{L}_{adv} + 100\mathcal{L}_{\ell_1}$ no BN & (smart avg) & (smart avg) & N/A\\
             %fold 0 without blacks val set average results: {'deltae00': 0.0, 'rmse': 2.2905200000000003, 'nb_files': 100, 'rmserel_sum': 10384431.340359218, 'rmserel': 0.06435031279223656, 'gfc': 0.99900766332451596, 'nb_nonblack_px_in_img': 161373440}
			Ours no.blk.bug (fold 0)$\mathcal{L}_{adv} + 100\mathcal{L}_{\ell_1}$ no BN & 2.29052 & 0.06435 & N/A\\
            %val set average results: {'deltae00': 0.0, 'rmse': 2.1752800000000008, 'nb_files': 100, 'rmserel_sum': 9886201.3825589269, 'rmserel': 0.06126287809542219, 'gfc': 0.99916040393975214, 'nb_nonblack_px_in_img': 161373440}
   			Ours no.blk.bug (fold 1)$\mathcal{L}_{adv} + 100\mathcal{L}_{\ell_1}$ no BN & 2.17528 & 0.061263 & N/A\\
            \hline
		\end{tabular}
	\end{center}
	\caption{Summary results of the conducted experiments over ICVL dataset. *Infinite results in computation of RMSERel are discarded (and an effective pixel count added), but the black pixels from the hs images are being used for RMSE/GFC estimation as well, and they should not. See issue \#98. Also, repeat this with overlapping tiles so that overlapped areas are averaged (will this correct the 2-pixel width distortions?}
    \label{tab:results}
\end{table}

%..........................................................................................
\subsubsection{Does the spatial information really help?}
In an attempt to empirically prove our main hypothesis of contextual spatial information on a local neighborhood being relevant for the correct spectral reconstruction of any given central pixel, we set the spectra of all the pixels of each $256\times256$ tile to be equal to that of the central pixel of such tile. We do this at test time, after training the model as in the previous section. We then obtain the sRGB version of the resulting image, and proceed as usual. Note that, by doing this, we are effectively reducing the number of samples upon which we evaluate the model to $5\times5=25$ samples per image. Table~\ref{tab:results_spatial_info} shows a $432.27\%$ and a $413.67\%$ increase of RMSE and RMSERel values, respectively (XXXXXXXXX THIS IS FOR FOLD 1), when constant patches are considered.

\begin{table}%
	\begin{center}
		\begin{tabular}{|l|ccc|}
			\hline
			Method & RMSE $[0-255]$ & RMSERel & GFC \\
			\hline\hline
			\sout{Ours w.blk.bug(avg) $\mathcal{L}_{adv} + 100\mathcal{L}_{\ell_1}$ no BN} & (smart avg) & (smart avg) & N/A\\
			% val set average results fold 1: {'deltae00': 0.0, 'rmse': 2.1735099999999994, 'nb_files': 100, 'rmserel_sum': 9884807.1831273101, 'nb_fin_px': 161373440, 'rmserel': 0.06125423851116584, 'gfc': 0.0}
   			\sout{Ours w.blk.bug (fold 0)$\mathcal{L}_{adv} + 100\mathcal{L}_{\ell_1}$ no BN} & 2.29209 & 0.0644 & N/A\\          
			\sout{Ours w.blk.bug (fold 1)$\mathcal{L}_{adv} + 100\mathcal{L}_{\ell_1}$ no BN} & 2.17351 & 0.061254 & N/A\\
			\hline
            \sout{$256\times256$ Cons.patch w.blk.bug (avg) $\mathcal{L}_{adv} + 100\mathcal{L}_{\ell_1}$ no BN} & (smart avg) & (smart avg) & N/A\\
            %val set average results fold 0: {'deltae00': 0.0, 'rmse': 2.0342099999999994, 'nb_files': 100, 'rmserel_sum': 9279517.1895151529, 'nb_fin_px': 161218560, 'rmserel': 0.05755861601490023, 'gfc': 0.99907287014254242}
			\sout{$256\times256$ Cons.patch w.blk.bug (fold 0)$\mathcal{L}_{adv} + 100\mathcal{L}_{\ell_1}$ no BN} & 2.03421 & 0.057559 & N/A\\
			% val set average results fold 1: results: {'deltae00': 0.0, 'rmse': 1.9391000000000003, 'nb_files': 100, 'rmserel_sum': 9033600.0474907979, 'nb_fin_px': 161218560, 'rmserel': 0.05603325105676913, 'gfc': 0.99922669847182743}
			\sout{$256\times256$ Cons.patch w.blk.bug (fold 1)$\mathcal{L}_{adv} + 100\mathcal{L}_{\ell_1}$ no BN} & 1.9391 & 0.056033 & N/A\\ 					\hline       
            \hline            
            Ours no.blk.bug (avg) $\mathcal{L}_{adv} + 100\mathcal{L}_{\ell_1}$ no BN & (smart avg) & (smart avg) & N/A\\
			Ours no.blk.bug (fold 0)$\mathcal{L}_{adv} + 100\mathcal{L}_{\ell_1}$ no BN & 2.29052 & 0.06435 & N/A\\
   			Ours no.blk.bug (fold 1)$\mathcal{L}_{adv} + 100\mathcal{L}_{\ell_1}$ no BN & 2.17528 & 0.061263 & N/A\\
            \hline
            $256\times256$ Cons.patch no.blk.bug (avg) $\mathcal{L}_{adv} + 100\mathcal{L}_{\ell_1}$ no BN & (smart avg) & (smart avg) & N/A\\
            % fold 0 val set average results: {'deltae00': 0.0, 'rmse': 2.030029999999999, 'nb_files': 100, 'rmserel_sum': 9279517.1895151529, 'rmserel': 0.05755861601490023, 'gfc': 0.99907287014254242, 'nb_nonblack_px_in_img': 161218560}
			$256\times256$ Cons.patch no.blk.bug (fold 0)$\mathcal{L}_{adv} + 100\mathcal{L}_{\ell_1}$ no BN & 2.03003 & 0.057559 & N/A\\
            % fold 1 val set average results: {'deltae00': 0.0, 'rmse': 1.9383000000000001, 'nb_files': 100, 'rmserel_sum': 9033600.0474907979, 'rmserel': 0.05603325105676913, 'gfc': 0.99922669847182743, 'nb_nonblack_px_in_img': 161218560}
			$256\times256$ Cons.patch no.blk.bug (fold 1)$\mathcal{L}_{adv} + 100\mathcal{L}_{\ell_1}$ no BN & 1.93830 & 0.056033 & N/A\\ 				\hline       
            % ours evaluating only on central
			Ours central no.blk.bug (avg) $\mathcal{L}_{adv} + 100\mathcal{L}_{\ell_1}$ no BN & (smart avg) & (smart avg) & N/A\\
            % fold 0 val set average results: {'deltae00': 0.0, 'rmse': 2.5107800000000005, 'nb_files': 100, 'rmserel_sum': 170.73544054182918, 'rmserel': 0.06829417621673167, 'gfc': 0.99900766332451596, 'nb_nonblack_px_in_img': 2500}
			Ours central no.blk.bug (fold 0)$\mathcal{L}_{adv} + 100\mathcal{L}_{\ell_1}$ no BN & 2.510780 & 0.0682942 & N/A\\
   			% fold 1 val set average results: {'deltae00': 0.0, 'rmse': 2.39032, 'nb_files': 100, 'rmserel_sum': 163.00179072606639, 'rmserel': 0.06520071629042655, 'gfc': 0.99916040393975214, 'nb_nonblack_px_in_img': 2500}         
   			Ours central no.blk.bug (fold 1)$\mathcal{L}_{adv} + 100\mathcal{L}_{\ell_1}$ no BN & 2.39032 & 0.0652007 & N/A\\
            \hline
		\end{tabular}
	\end{center}
	\caption{\textbf{This is done for 256x256 patches with non-overlapping areas. Train on normal, test on constant patches} *Infinite results in computation of RMSERel are discarded (and an effective pixel count added), but the black pixels from the hs images are being used for RMSE/GFC estimation as well, and they should not. See issue \#98.}
    \label{tab:results_spatial_info}
\end{table}


\textbf{
We now repeat experiment from Table~\ref{tab:results} after not considering the black pixels for the compuation of the RMSE (RMSErel should be already well computed).
}


we now do a \textbf{pruning experiment}, as part of issue \#107, in order to show which levels are helping. Results are in Table~\ref{tab:results_pruning}.
\begin{table}%
	\begin{center}
		\begin{tabular}{|lcccc|ccc|}
			\hline
			Name & \#Skips in U-net & 1st skip & Last skip & fold & RMSE $[0-255]$ & RMSERel & GFC \\
			\hline\hline
            g256\_2 & 7 & 128x128 & 2x2 & 1 & 2.17528 & 0.061263 & Todo\\
			\hline
            g256\_3 & 8 & 256x256 & 2x2 & 0 & 1.89984 & 0.052485 & Todo\\
			\hline
			g256\_3 & 8 & 256x256 & 2x2 & 1 & 1.43158 & 0.037314 & Todo\\
			\hline
            g2prun7lev & 6 & 128x128 & 4x4 & 1 & Todo & Todo & Todo\\
			\hline           
            g2prun6lev & 5 & 128x128 & 8x8 & 1 & Todo & Todo & Todo\\
			\hline
            g2prun5lev & 4 & 128x128 & 16x16 & 1 & Todo & Todo & Todo\\
			\hline           
            g2prun4lev & 3 & 128x128 & 32x32 & 1 & Todo & Todo & Todo\\
            \hline            
            g2prun3lev &2 & 128x128 & 64x64 & 1 &  1.86433 & 0.049939 & 0.999218\\
            \hline            
            g2prun2lev & 1 (seq) & 128x128 & 128x128 & 1 & 2.67301 & 0.0803488 & 0.998678\\
			\hline
		\end{tabular}
	\end{center}
	\caption{\textbf{This is done for 256x256 patches with non-overlapping areas. Progressively prune the bottom layers of the U, training and testing on normal, non-constant tiles. These are done after fixing issue \#98 for the general numbers, but not for the maps}. All the results are for our approach with no.blk.bug and $\mathcal{L}_{adv} + 100\mathcal{L}_{\ell_1}$ no BN }
    \label{tab:results_pruning}
\end{table}

%..........................................................................................
\subsubsection{Ablation experiments with the losses}

%###############################################################################
\section{Conclusion}
\label{sec:conclusion}

We propose a convolutional neural network architecture that successfully learns an end-to-end mapping between pairs of input RGB images and their hyperspectral counterparts.
We adopt an adversarial framework-based generative model that shows effective in capturing the the structure of the data manifold, and takes into account the spatial contextual information present in RGB images for the spectral reconstruction process. 
State of the art results in the ICVL dataset suggest that individual pixel-based approaches suffer from the fundamental limitation of not being able to effectively exploit the local context when applied to spectral image data in their attempt to build informative priors. 
The observed performance in terms of both reconstruction error and speed open the door to a full range of potential higher level applications in sectors of increasing demand for spectral footage at a lower cost.


%%%%%%%%% BIB
%###############################################################################
\bibliography{adv_rgb2hs_better_bibtex_ascii_noforcekeys}
% Do auto export of collection with bibtex and ascii encoding without forcing ascii keys. Otherwise, accents will fail. Had the correct some keys.

\end{document}