\section{Examples}
\label{sec:prob}

%To illustrate the asymmetric game, we can start from the most basic constant function family $\mathcal{G}_C$. Then for each $x_t$, $g_{x_t}$ is just the mean of $\{f(x_{t-\epsilon}),\cdots,f(x_{t+\epsilon})\}$, where the gradient should be disconnected from $g_x$ when updating $f$ due to game-theoretic setting. Similarly, for linear function family $\mathcal{G}_\text{Linear}$, we can also use a linear family by fitting a linear regression model based on data $\{(x_{t-\epsilon}, f(x_{t-\epsilon})),\cdots,(x_{t+\epsilon}, f(x_{t+\epsilon}))\}$

%\subsection{Conditional Generative Model}
{\bf Conditional Generative Model.} The basic idea of sequence modeling can be generalized to conditional sequence generation.
For example, given historical data of a stock's price, how will the price evolve over time?
Such mechanism allows us to inspect the temporal dynamics inside the problem of interest to assist in long-term decisions, while a conditional generation allows us to control the progression of generation with different settings of interest.

Formally, given an observation sequence $x_{1},\dots,$ $x_{t}\in\mathbb{R}^N$, the goal is to estimate the probability $p(x_{t+1},\dots,x_{T} | x_{1},\dots,x_{t})$ of future events $x_{t+1},\dots,$ $x_{T}\in \mathbb{R}^N$. For notational simplicity, we will use ${x}_{1:t}$ to denote the sequence of variables $x_1,\dots,x_t$.
A popular approach to estimate this conditional probability is to train a conditional model by maximum likelihood (ML)~\cite{van2016wavenet}. 
If we model the conditional distribution of $x_{i+1}$ given ${x}_{1:i}$ as a multivariate Gaussian distribution with mean $\mu(\cdot)$ and covariance $\Sigma(\cdot)$, we can define the asymmetric game on $\mu(\cdot)$ by minimizing
\begin{align}
\vspace{-2mm}
%\min_{\mu, \Sigma} 
\sum_{i=t}^{T-1} \bigg[& -\log \mathcal{N}(x_{i+1}; \mu({x}_{1:i}), \Sigma({x}_{1:i})) \nonumber \\
& + \lambda \; \|\mu({x}_{1:i}) - g_{{x}_{1:i}}({x}_{1:i})\|_2^2 \bigg],
\label{eq:implicit}
\end{align}
with respect to $\mu(\cdot)$ and $\Sigma(\cdot)$, both parametrized as recurrent neural networks.
For the explainer model $g_{{x}_{1:i}}(\cdot)$, we use the neighborhood data $\{({x}_{1:i-\epsilon}, \mu({x}_{1:i-\epsilon})),$ $\cdots,({x}_{1:i+\epsilon}, \mu({x}_{1:i+\epsilon}))\}$ to fit a $K$-order Markov autoregressive (AR) model: 
\begin{equation}
g_{x_{1:i}}(x_{1:i}) = \sum_{k=0}^{K-1} \theta_{k+1} \cdot x_{i-k} + \theta_0,
\label{eq:arma}
\end{equation}
where $\theta_k \in \mathbb{R}^{N \times N}, k=1,\dots,K$ and $\theta_0 \in \mathbb{R}^{N}$.
AR model is a generalization of linear model to temporal modeling and thus admits a similar analytical solution. The choice of Markov horizon $K$ makes this model flexible and should be informed by the application at hand.
% The past history horizon $K$ allows flexibility of Markov order enables explanation over customized period
%By manipulate the Markov order, we can tailor the interpretation toward desirable period of events.


%\subsection{Explicit Interpretability Game}
\label{sec:meta}
{\bf Explicit Interpretability Game.}
In some cases, we wish to articulate interpretable parts explicitly in the predictor $f$. For example, if we view the predictor as approximately locally linear, we could explicitly parameterize $f$ in a manner that highlights these linear coefficients. To this end, in the temporal domain, we can explicate the locally linear assumption already in the parametrization of $\mu$:
\begin{align}
\mu(x_{1:i}) & = \sum_{k=0}^{K-1} \hat{\theta}(x_{1:i})_{k+1} \cdot x_{i-k} + \hat{\theta}_0(x_{1:i}),\label{eq:explicit}
\end{align}
which we can write as $\hat{\theta}_\text{AR}(x_{1:i}) + \hat{\theta}_0(x_{1:i})$, where $\hat{\theta}$ and $\hat{\theta}_0$ are learned as recurrent networks. However, this explicit parameterization is relevant only if we further encourage them to act their part, i.e., that the locally linear part of $\mu(x_{1:i})$ is really expressed by $\hat{\theta}(x_{1:i})_{k}$, $k=1,\ldots,K$.

%However, in order to effectively establish the interpretability of $\hat{\theta}$ and $\hat{\theta}_0$ we must address some subtle points. We note that if $\hat{\theta}_0$ is parametrized by non-parametric function\todo{seemingly contradictory terms! parametrized, non-parametric}, then all the $\hat{\theta}$ can be ignored, then there is no valid interpretability in the form of AR parameters in (\ref{eq:explicit}).

To this end, we formulate a refined game that defines the discrepancy measure for the explainer in a coefficient specific manner, separately for $\hat{\theta}_\text{AR}$ and $\hat{\theta}_0$ so as to locally mimic the AR and constant family, respectively. % \emph{without offset} $\mathcal{G_\text{AR}}$ and constant family $\mathcal{G_C}$.
The objective of local explainer at $x_{1:i}$ with respect to $\hat{\theta}_\text{AR}$ and $\hat{\theta}_0$ then becomes
\begin{align}
& \min_{g_{x_{1:i}} \in \mathcal{G_\text{AR}}} 
\frac{1}{2\epsilon+1} 
\sum_{i'=i-\epsilon}^{i+\epsilon} 
%d(\sum_{k=0}^{K-1} \hat{\theta}(x_{1:i'})_{k+1} \cdot x_{i'-k}, g_{x_{1:i}}(x_{1:i'}))\nonumber\\
\|\hat{\theta}_\text{AR}(x_{1:i'}), g_{x_{1:i}}(x_{1:i'})\|_2^2\nonumber\\
& + \min_{\bar{g}_{x_{1:i}} \in \mathcal{G_C}} 
\frac{1}{2\epsilon+1} 
\sum_{i'=i-\epsilon}^{i+\epsilon} 
\|\hat{\theta}_{0}(x_{1:i'}), \bar{g}_{x_{1:i}}\|_2^2,
\label{eq:factor_local_deviation}
\end{align}
where $\mathcal{G}_\text{AR}$ is the family of AR models (which does not include any offset/bias, consistent with $\hat{\theta}_\text{AR }$) and $\mathcal{G_C}$ is simply the set of constant vectors. The objective for $f$ is defined analogously, symmetrically or asymmetrically. For simplicity, our notation doesn't include end-point boundary cases with respect to the neighborhoods. 
