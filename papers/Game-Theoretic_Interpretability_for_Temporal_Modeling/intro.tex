\section{Introduction}
\label{sec:intro}
State-of-the-art predictive models tend to be complex and involve a very large number of parameters. While the added complexity brings modeling flexibility, it comes at the cost of transparency or interpretability. This is particularly problematic when predictions feed into decision-critical applications such as medicine where understanding of the underlying phenomenon being modeled may be just as important as raw predictive power.

Previous approaches to interpretability have focused mostly on fixed-size data, such as scalar-feature datasets~\cite{lakkaraju2016interpretable} or image prediction tasks~\cite{selvaraju2016grad}. Recent methods do address the more challenging setting of sequential data \cite{Lei2016Rationalizing,arras2017relevant} in NLP tasks where the input is discrete. Interpretability for continuous temporal data has remained mostly unexplored~\cite{wu2018tree}. 

% David's version
%In this work, we propose a novel approach to model interpretability which naturally lends itself to (though is not limited to) time-series data. Our approach departs from other frameworks in the way it enforces interpretability. As opposed to classic interpretable models (\eg, classification trees), our framework operates on a wide range of prediction models, \ie, it does not restrict their architecture. On the other hand, differing from post-hoc \textit{extrinsic} explanation approaches, such as LIME \cite{Ribeiro2016Why} and related perturbation-based methods~\cite{alvarez2017causal}, the explanations are \textit{intrinsic} and trained \textit{in coordination} with the predictive model to be explained. Compared to existing intrinsic intepretable methods~\cite{al2017contextual}, where interpretation is estimated in a point-wise basis, our interpretation is intrinsically local.

% semi-intrinsic version
In this work, we propose a novel approach to model interpretability that is naturally tailored (though not limited to) time-series data. 
Our approach differs from interpretable models such as interpretation generators~\cite{al2017contextual,Lei2016Rationalizing} where the architecture or the function class is itself explicitly constrained towards interpretability, e.g., taking it to be the set of linear functions.
%constrained to give rise to interpretable predictions. 
We also differ from post-hoc explanations of black-box methods through local perturbations~\cite{Ribeiro2016Why, alvarez2017causal}. In contrast, we establish an intermediate regime, game-theoretic interpretability, where the predictor remains functionally complex but is encouraged during training to follow a locally interpretable form. 

\iffalse
 new notion of \emph{semi-intrinsic interpretability} that explanations are intrinsic to model training but extrinsic to model itself.
By such nature, the derived approach can operate on an unconstrained set of predictive/interpretable models, \ie, it does not restrict their architectures. 
Meanwhile, the explanations are \textit{intrinsic} in the objective and trained \textit{in coordination} with the predictive model to be explained. 
\fi

At the core of our approach is a game-theoretic characterization of interpretability. This is set up as a two-player co-operative game between \emph{predictor} and \emph{explainer}. The predictor remains a complex model whereas the explainer is chosen from a simple interpretable family. The players minimize asymmetric objectives that combine the prediction error and the discrepancy between the players. The resulting predictor is biased towards agreeing with a co-operative explainer. The co-operative game equilibrium is stable in contrast to  GANs~\cite{goodfellow2014generative}.

% Instead of generating post-hoc extrinsic explanations, we propose a game-theoretic characterization of interpretability, in which the 

% Things that have to be mentioned:
% \begin{itemize}
% \item Emphasize novelty of interpretability in sequences
% \item Related work:
% \item Importance of applications of interpretable temporal-modeling: medical, weather, etc.
% \begin{itemize}
% 	\item Tao's rationales \cite{Lei2016Rationalizing}
%     \item LRP for discrete sequences \cite{arras2017relevant}
%     \item Murdoch et al
%     \item Check if there's a paper for \url{github.com/emanuel-metzenthin/Lime-For-Time}
% \end{itemize}
% \item Non-sequential stuff but probably also need to me mentioned:
% \begin{itemize}
% 	\item LIME \cite{Ribeiro2016Why}
%     \item 
% \end{itemize}
% \end{itemize}
The main contributions of this work are as follows:
\begin{itemize}
	\vspace{-2mm}
	\itemsep0em
	\item A novel game-theoretic interpretability framework that can take a wide range of prediction models without architectural modifications.
    \item Accurate yet explainable predictors where the explainer is trained coordinately with the predictor to actively balance interpretability and accuracy.  
    % semi-intrinsic version
%    \item We establish a novel notion of semi-intrinsic interpretability that incorporates advantage from both intrinsic and extrinsic methods.
    \item Interpretable temporal models, validated through quantitative and qualitative experiments, including stock price prediction and a physical component modeling. 
\end{itemize}

\iffalse
Our main contribution is to establish a game-theoretic characterization of interpretability. 
The formulation only assumes the target model and the axiomatic set of interpretable models are trainable.
With the progression of the cooperative game, the functional behavior the target function is biased toward interpretable models.
The characterization also admits quantitative evaluation of predictive models. 
\fi