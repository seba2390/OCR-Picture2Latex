\section{Methodology}
\label{sec:method}

In this work, we learn a (complex) predictive target function $f\in \mathcal{F}: \mathcal{X} \to \mathcal{Y}$ together with a simpler function $g \in \mathcal{G}:\mathcal{X}\to\mathcal{Y}$ defined over an axiomatic class of interpretable models $\mathcal{G}$. We refer to functions $f$ and $g$ as the predictor and explainer, respectively, throughout the paper. Note that we need not make any assumptions on the function class $\mathcal{F}$, instead allowing a flexible class of predictors. In contrast,
the family of explainers $\mathcal{G}$ is explicitly and intentionally constrained such as the set of linear functions. As any $g\in \mathcal{G}$ is assumed to be interpretable, the family $\mathcal{G}$ does not typically itself suffice to capture the regularities in the data. We can therefore hope to encourage the predictor to remain close to such interpretable models only locally. 


For expository purposes, we will develop the framework in a discrete time setting where the predictor maps  $x_t \in \mathcal{X}$ to $f(x_t) \in \mathcal{Y}$ for $t=\{1, 2, \dots\}$. 
The data are denoted as $\mathcal{D}=\{(x_1,y_1), (x_2,y_2), \dots\}$. 
%We then instantiate the predictor with deep sequence generative models and explainers with linear models.
We then instantiate the predictor with deep sequence generative models, and the explainers with linear models.

\subsection{Game-Theoretic Interpretability}

\begin{figure}	
	\begin{subfigure}{0.225\textwidth}
		\centering
		\includegraphics[width=1.15\linewidth]{data.png}
		\caption{Neighborhood $\mathcal{B}_{\epsilon}(x_t=1)$}\label{fig:data}
	\end{subfigure}
	~
	\begin{subfigure}{0.225\textwidth}
		\centering
		\includegraphics[width=1.15\linewidth]{deep.png}
		\caption{Piece-wise linear $f$}\label{fig:deep}
	\end{subfigure}


	\begin{subfigure}{0.225\textwidth}
		\centering
		\includegraphics[width=1.15\linewidth]{linear.png}
		\caption{$g_{x_t} \in \mathcal{G}_{\text{Linear}}$}\label{fig:linear}
	\end{subfigure}
	~
	\begin{subfigure}{0.225\textwidth}
		\centering
		\includegraphics[width=1.15\linewidth]{stump.png}
		\caption{$g_{x_t} \in \mathcal{G}_{\text{Decision Stump}}$}\label{fig:stump}
	\end{subfigure}

	\caption{Examples of fitting a neighborhood $\mathcal{B}_{\epsilon}(x_t=1)$~(\ref{fig:data}) with piece-wise linear predictor~(\ref{fig:deep}). When playing with different families of explainers~(Figure \ref{fig:linear} and \ref{fig:stump}; dashed lines), the resulting predictor (in solid green) behaves differently although they admit the same prediction error (mean squared error = 1.026).}\label{fig:synthetic}
    \vspace{-1mm}
\end{figure}

There are many ways to use explainer functions $g \in \mathcal{G}$ to guide the predictor $f$ by means of discrepancy measures. However, since the explainer functions are inherently weak such as linear functions, we cannot expect that a reasonable predictor would be nearly linear globally. Instead, we can enforce this property only locally. To this end, we define \emph{local} interpretability by measuring how close $f$ is to a family $\mathcal{G}$ over a local neighborhood $\mathcal{B}_\epsilon(x_t)$ around an observed point $x_t$. One straightforward instantiation of such a neighborhood $\mathcal{B}_\epsilon({x_t})$ in temporal modeling will be simply a local window of points $\{x_{t-\epsilon},\dots, x_{t+\epsilon}\}$. Our resulting local discrepancy measure is
\begin{align}
\vspace{1mm}
& \min_{g \in \mathcal{G}} d_{x_t}(f, g)  = \min_{g \in \mathcal{G}} \frac{\sum_{x' \in \mathcal{B}_\epsilon(x_t)} d(f(x'), g(x'))}{|\mathcal{B}_\epsilon({x_t})|},
\label{eq:local_deviation}
\vspace{1mm}
\end{align}
where $d(\cdot, \cdot)$ is a deviation measurement.
The minimizing explainer function, $\hat g_{x_t}$, is indexed by the point $x_t$ around which it is estimated. Indeed, depending on the function $f$, the minimizing explainer can change from one neighborhood to another. If we view the minimization problem game-theoretically, $\hat g_{x_t}$ is the \emph{best response strategy} of the local explainer around $x_t$. 
  
The local discrepancy measure can be subsequently incorporated into an overall regularization problem for the predictor either symmetrically (shared objective) or asymmetrically (game-theoretic) where the goals differ between the predictor and the explainer.  

{\bf Symmetric criterion.} Assume that we are given a primal loss $\mathcal{L}(\cdot, \cdot)$ that is to be minimized for the problem of interest. The goal of the predictor is then to find $f$ that offers the best balance between the primal loss and local interpretability. 
Due to the symmetry between the two players, the full game can be collapsed into a single objective
%With the above characterization of interpretability, we can establish the complete game between predictor and local explainers to collaborate toward minimizing (\ref{eq:local_deviation}) on observed data $\mathcal{D}$. 
%Then the game is established as follows:
\begin{align}
%\min&_{f, g_{x_t}:(x_t, y_t)\in \mathcal{D}}  
\sum_{(x_t, y_t) \in \mathcal{D}}
\bigg[& \mathcal{L}(f(x_t), y_t) \nonumber\\
& + \frac{\lambda}{|\mathcal{B}_\epsilon(x_t)|} \sum_{x' \in \mathcal{B}_\epsilon(x_t)} d(f(x'), g_{x_t}(x'))\bigg]
%\label{eq:coop_game}
\end{align}
to be minimized with respect to both $f$ and $g_{x_t}$. Here, $\lambda$ is a hyper-parameter that we have to set.

To illustrate the above idea, we generate a synthetic dataset to show a \emph{neighborhood} in \figref{fig:data} with a perfect piece-wise linear predictor $f\in \mathcal{F_{\text{Piece-wise Linear}}}$ in \figref{fig:deep}. Clearly, $f$ does not agree with linear explainers within the neighborhood, despite its piece-wise linear nature. However, when we establish a game between $f$ and a linear explainer $g_{x_t=1} \in \mathcal{G}_{\text{Linear}}$ in \figref{fig:linear}, it admits lower functional deviation (and thus stronger linear interpretability). We also show in \figref{fig:stump} that different explainer family would induce different outcomes of the game.

%In (\ref{eq:coop_game}), the game between predictor function $f$ and explainer functions $g_{x_t}$ can be collapsed into a single objective, which makes it simple to understand the structure of cooperative game. 
%The symmetric structure in (\ref{eq:coop_game}) makes it simple to understand the objective of cooperative game, but there are some subtleties in the formulation to be addressed. 

{\bf Asymmetric Game.}
The symmetric criterion makes it simple to understand the overall co-operative objective, but solving it is inefficient computationally. Different possible sizes of the neighborhood $\mathcal{B}_\epsilon(x)$ (e.g., end-point boundary cases) makes it hard to parallelize optimization for $f$ (note that this does not hold for $g_{x_t}$), which is problematic when we require parallel training for neural networks. Also, since $f$ is reused many times across neighborhoods in the discrepancy measures, the value of $f$ at each $x_t$ may be subject to different functional regularization across the neighborhoods, which is undesirable. 

In principle, we would like to impose a uniform functional regularization for every $x_t$, where the regularizer is established on a local region $\mathcal{B}_\epsilon(x_t)$ basis. This new modeling framework leads to an asymmetric co-operative game, where the information sets are asymmetric between predictor $f$ and local explainers $g_{x_t}$. Accordingly, each local best response explainer $\hat g_{x_t}$ is minimized for local interpretability (\ref{eq:local_deviation}) within $\mathcal{B}_\epsilon(x_t)$, thus relying on $f$ values within this region. In contrast, the predictor $f$ only receives feedback in terms of resulting deviation at $x_t$ and thus only sees $\hat g_{x_t}(x_t)$. From the point of view of the predictor, the best response strategy is obtained by %minimizing 
\begin{align}
\vspace{-8mm}
\min_{f\in \mathcal{F}}\sum_{(x_t, y_t) \in \mathcal{D}} \mathcal{L}(f(x_t), y_t) + \lambda \cdot d(f(x_t), \hat g_{x_t}(x_t)).
\label{eq:asym_coop_game}
\vspace{-8mm}
\end{align}
%with respect to $f$.

{\bf Discussion.}
We can analyze the scenario when the loss and deviation are measured in squared error, the explainer is in constant family, and the predictor is non-parametric.
Both games induce a predictor that is equal to recursive convolutional average of $y_t$, where the decay rate in each recursion is the same $\frac{\lambda}{1+\lambda}$ for both games, but the convolutional kernel evolves twice faster in the symmetric game than in the asymmetric game. %, which indicates symmetric game would induce a smoother predictor.

The formulation involves a key trade-off between the size of the region where explanation should be simple and the overall accuracy achieved with the predictor. When the neighborhood is too small, local explainers become perfect, inducing no regularization on $f$. Thus the size of the region is a key parameter in our framework. Another subtlety is that solving (\ref{eq:local_deviation}) requires optimization over explainer family $\mathcal{G}$, where specific deviation and family choices matter for efficiency. For example, $L_2$ and affine family lead to linear regression with closed-form local explainers. Finally, a natural extension to solving (\ref{eq:local_deviation}) is to add regularizations. 

We remark that the setup of $f$ and $\mathcal{G}$ leaves considerable flexibility in tailoring the predictive model and the explanations to the application at hand --- which is not limited to temporal modeling. Indeed, the derivation is for temporal models but extends naturally to others as well. 
%The derivation is based on temporal modeling, with proper adjustment to the neighborhood, the method is generalizable to typical predictive models. 
%Regardless of the application, the game-theoretic framework is the first work to establish and quantify (by deviation) \emph{semi-intrinsic} interpretability for any class of model that is trainable.

% old content below

% and $k$-NN family can have $O(\log k)$ binary search optimization if the neighborhood is cached and sorted.
%For the cooperative game, it will degenerate to normal predictor once $\lambda \to 0$. However, when $\lambda \to \infty$, it degenerates to any function $g_{x_t}\in\mathcal{G}$, which may not be the one that minimizes $\mathcal{L}(g_{x_t}(x_t), y_t)$. 
%Hence, we recommend users to try $\lambda$ from a small value and gradually increase it.

\if 0
  Theoretically, the symmetric game and asymmetric game have the same set of equilibrium as revealed by the following theorem:
  \begin{theorem}
  \label{popular}
  Assume the target function $f$ is non-parametric, then $\{f, g_x| (x, y) \in \mathcal{D}\}$ is an equilibrium in the symmetric game iff it is an equilibrium in the asymmetric game.
  \end{theorem}

  \begin{proof}
  $\{f, g_x| (x, y) \in \mathcal{D}\}$ is an equilibrium iff $\forall (x, y) \in \mathcal{D}$, $f(x')=g_{x}(x'), \forall x' \in \mathcal{B}_\epsilon(x)$. The optimality condition applied to both symmetric and asymmetric game.
  \end{proof}

  We note that investigating the composite property with loss $\mathcal{L}(\cdot, \cdot)$ involves knowledge to the exact shape of specific loss, which is left to future work. We would also show that introducing asymmetry does not lead to noticeable artifact. 
\fi

%We note that in some cases when $f$ can explicate the parametrization form of $\mathcal{G}$, then we can still use $f$ as interpretation without a post-hoc model.
%The idea will be elaborated in detail in Section \ref{sec:meta}.



%\todo[inline]{DAM. This section needs a strong, concluding paragraph that highlights the novel paradigm of interpretability proposed here. If there is no related section after all, then this might be a good place to distinguish this approach from: i) extrinsic post-hoc interpretability methods like LRP, LIME, etc and ii) intrinsic methods like Tao's work. Since the latter also has an game theoretic interpretation, it is necessary to contrast here (e.g. advantages of cooperative vs adversarial}
