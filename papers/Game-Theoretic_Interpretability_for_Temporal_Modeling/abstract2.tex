\begin{abstract}

% Tommi's version
Interpretability has arisen as a key desideratum of machine learning models alongside performance. Approaches so far have been primarily concerned with fixed dimensional inputs emphasizing feature relevance or selection. In contrast, we focus on temporal modeling and the problem of tailoring the {predictor}, functionally, towards an interpretable family. To this end, we propose a co-operative game between the \emph{predictor} and an \emph{explainer} without any a priori restrictions on the functional class of the predictor. The goal of the explainer is to highlight, locally, how well the predictor conforms to the chosen interpretable family of temporal models. Our co-operative game is setup asymmetrically in terms of information sets for efficiency reasons. We develop and illustrate the framework in the context of temporal sequence models with examples. 

\iffalse
% semi-intrinsic version
Balancing interpretability with predictive performance is at the cornerstone of modern machine learning. 
Current interpretability methods usually focus on fixed-dimensional inputs, while temporal data has garnered considerably less attention. 
Here, we propose a novel game-theoretic formulation of interpretability that is particularly well suited for this setting. 
In contrast to either intrinsic interpretable methods that are restricted to a particular class of model or extrinsic methods that are explanation-agnostic in model training, we establish a new notion of semi-intrinsic interpretability: explanations are intrinsic in model training, but extrinsic to the model.
Accordingly, the resulting framework is model-agnostic, and produces explanations that are enforced, by means of a cooperative game between \textit{predictor} and \textit{explainer}, to be faithful local approximators of the true behavior of the predictor model. 
We instantiate our framework in the context of sequence generative models, and demonstrate compatibility of the framework with meta-learning.
\todo[inline]{Too verbose, needs compressing}
\fi

\iffalse
% David's version
Balancing interpretability with predictive performance is at the cornerstone of modern machine learning. Current interpretability methods usually focus on fixed-dimensional inputs, while temporal data, such as time series, has garnered considerably less attention. Here, we propose a novel game-theoretic formulation of interpretability that is particularly well suited for this setting. In contrast to most popular interpretability approaches, this framework allows for a wide class of predictor functions, is not limited to linear explanations, and produces \textit{intrinsic} explanations that are enforced, by means of a cooperative game between \textit{predictor} and \textit{explainer}, to be faithful local approximators of the true behavior of the predictor model. We instantiate our framework in the context of sequence generative models, and demonstrate compatibility of the framework with meta-learning.
\fi

\iffalse
% Previous^2 version
In this paper, we propose a game theoretic formulation to enforce interpretability of a target machine learning model, which can belong to arbitrary family of models. 
We axiomatize the notion of interpretability as being captured by another arbitrary family of models.
We only assume that the two arbitrary families of models are trainable.
To retain model complexity of the target model, we establish such interpretability locally.
A cooperative game between the target and interpretable models is accordingly established to minimize their functional deviation locally.
We instantiate the framework with sequence generative model, and also demonstrate compatibility of the framework with meta-learning.
\fi

\end{abstract}