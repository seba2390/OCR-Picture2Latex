\section{Experiments}
\label{sec:experiment}

\begin{table}[t]
%\vspace{2mm}
\centering
\begin{tabular}{ l  c  c  c }  
\hline
\bf Dataset & \bf\small \# of seq. & \small \bf input len. & \small \bf output len. \\
\hline\hline
Stock & 15,912 & 30 & 7\\
Bearing & 200,736 & 80 & 20\\
\hline
\end{tabular}
\vspace{-2mm}
\caption{Dataset statistics}
\vspace{-2mm}
\label{tab:stat}
\end{table}	
\begin{table}[t]
%\vspace{2mm}
\centering
\begin{tabular}{ l  c  c  c }  
\hline
\bf Stock & \bf\small Error & \small \bf Deviation & \small \bf TV\\
\hline\hline
AR & 1.557 & 0.000 & 0.000\\
\hline
game-implicit & 1.478 & 0.427 & 0.000\\
deep-implicit & 1.472 & 0.571 & 0.000\\
\hline
game-explicit & 1.479 & 0.531 & 73.745\\
deep-explicit & 1.475 & 0.754 & 91.664\\
\hline
\hline
\bf Bearing & \bf\small Error & \small \bf Deviation & \small \bf TV\\
\hline\hline
AR & 9.832 & 0.000 & 0.000\\
\hline
game-implicit & 8.309 & 3.431 & 5.706\\
deep-implicit & 8.136 & 4.197 & 7.341\\
\hline
game-explicit & 8.307 & 4.177 & 27.533\\
deep-explicit & 8.151 & 6.134 & 29.756\\
\hline
\end{tabular}
\vspace{-2mm}
\caption{Performance. All units are in $10^{-2}$.}
\vspace{-6mm}
\label{tab:peref}
\end{table}	

\iffalse
\begin{table}[t]
%\vspace{2mm}
\centering
\begin{tabular}{ l  c  c  c }  
\hline
\bf Stock & \bf\small Error & \small \bf Deviation & \small \bf TV\\
\hline\hline
AR & 0.01557 & 0.00000 & 0.00000\\
\hline
game-implicit & 0.01478 & 0.00427 & 0.00000\\
deep-implicit & 0.01472 & 0.00571 & 0.00000\\
\hline
game-explicit & 0.01479 & 0.00531 & 0.73745\\
deep-explicit & 0.01475 & 0.00754 & 0.91664\\
\hline
\hline
\bf Bearing & \bf\small Error & \small \bf Deviation & \small \bf TV\\
\hline\hline
AR & 0.09832 & 0.00000 & 0.00000\\
\hline
game-implicit & 0.08309 & 0.03431 & 0.05706\\
deep-implicit & 0.08136 & 0.04197 & 0.07341\\
\hline
game-explicit & 0.08307 & 0.04177 & 0.27533\\
deep-explicit & 0.08151 & 0.06134 & 0.29756\\
\hline
\end{tabular}
\vspace{-2mm}
\caption{Performance}
\vspace{-6mm}
\label{tab:peref}
\end{table}	
\fi

\begin{table}[t]
%\vspace{2mm}
\centering
\begin{tabular}{ l c  c  c  c c}  
\hline
{$\times 10^{-2}$} & \bf\small $\lambda=0.$ & \bf\small $\lambda=0.1$ & \small \bf $\lambda=1$ & \small \bf $\lambda=10$ & AR\\
\hline\hline
Error & 8.136 & 8.057 & 8.309 & 9.284 & 9.832\\
Deviation & 4.197 & 4.178 & 3.431 & 1.127 & 0.000\\
TV & 7.341 & 7.197 & 5.706 & 1.177 & 0.000\\
\hline
\end{tabular}
\vspace{-2mm}
\caption{Implicit game on bearing dataset when $\lambda$ varies.}
\vspace{-4.25mm}
\label{tab:compare}
\end{table}	

\iffalse
  \begin{table}[t]
  %\vspace{2mm}
  \centering
  \begin{tabular}{ l c  c  c  c }  
  \hline
  \bf {} & \bf\small $\lambda=0.$ & \bf\small $\lambda=0.1$ & \small \bf $\lambda=1$ & \small \bf $\lambda=10$\\
  \hline\hline
  Error & 0.08136 & 0.08057 & 0.08309 & 0.09284\\
  Deviation & 0.04197 & 0.04178 & 0.03431 & 0.01127\\
  TV & 0.07341 & 0.07197 & 0.05706 & 0.01177 \\
  \hline
  \end{tabular}
  \vspace{-2mm}
  \caption{Implicit game on bearing dataset when $\lambda$ varies.}
  \vspace{-4.mm}
  \label{tab:compare}
  \end{table}	
\fi

\begin{figure}[t]
\centering
  \includegraphics[width=1.1\linewidth]{fixfont_bearing_weight_visual.png}
  \vspace{-8mm}
  \caption{Visualization of weight vectors for predicting the first channel along each autoregressive timestamp in bearing dataset. The $y$-axis from $0$ to $8$ denotes $(\theta_0)_1$, $(\theta_1)_{1, 1:4}$ and $(\theta_2)_{1, 1:4}$}.
  \label{fig:visual}
  \vspace{-9mm}
\end{figure}


We validate our approach on two real-world applications: a bearing dataset from NASA~\cite{bearing_dataset} and a stock market dataset consisting of historical data from the S\&P 500 
%index\footnote{\url{www.kaggle.com/camnugent/sandp500/data}}. 
index\footnote{\url{www.kaggle.com/camnugent/sandp500/data}}. 
The bearing dataset records 4-channel acceleration data on 4 co-located bearings, and stock dataset records daily prices (4 channels in open, high, low, and close).
Due to the diverse range of stock prices, we transform the data to daily percentage difference.
We divide the sequence into disjoint subsequences and train the sequence generative model on them.
The input and output length are decided based on the nature of dataset.
The bearing dataset has a high frequency period of 5 points and low frequency period of 20 points. On the stock dataset, we used 1 month to predict the next week's prices. 
The statistics of the processed dataset is shown in \tabref{tab:stat}.
We randomly sample $85\%$, $5\%$, and $10\%$ of the data for training, validation, and testing.

We set neighborhood $\epsilon$ and Markov order $K$ to be $6$ and $7$ to impose sequence-level coherence in stock dataset; and $9$ and $2$ for smooth variation in bearing dataset.
We parametrize $\mu(\cdot)$ and $\Sigma(\cdot)$ jointly by stacking $1$ layer of CNN, LSTM, and $2$ fully connected layers. % implemented in \texttt{Tensorflow}~\cite{abadi2016tensorflow}. 
We use Ridge regression\footnote{Ridge is used to alleviate degenerate cases of linear regression.} with default parameter in \texttt{scikit-learn}~\cite{scikit-learn} to implement the AR model. For efficiency, $10\%$ of the sequences are sampled for regularization in each batch.


We compare our asymmetric game-theoretic approach (`game') against the same model class without an explainer (`deep'). We use `implicit' label to distinguish predictors from those `explicitly' written in an AR-like form. Evaluation involves three different types of errors: `Error' is the root mean squared error (RMSE) between greedy autoregressive generation and the ground truth; `Deviation' is RMSE between the model prediction $\mu(x_{1:i})$ and the explainer $g_{x_{1:i}}(x_{1:i})$, estimated also for `deep' that is not guided by the explainer; and `TV' is the average total variation of $[\hat{\theta}, \hat{\theta}_0]$ over any two consecutive time points. For testing, the explainer is estimated based on a greedy autoregressive generative trajectory. TV for implicit models is based on the parameters of the explainer $g_{x_t}$. For explicit models, deviation RMSE is the sum of AR and constant deviations as in Eq. (\ref{eq:factor_local_deviation}), thus not directly comparable to the implicit `Deviation' based only on output differences.

The results are shown in \tabref{tab:peref}.
TV is not meaningful for implicit formulation in stock dataset due to the size of the neighborhood.
The proposed game reduces the gap between deep learning model and AR model on deviation and TV, while retaining promising prediction accuracy.

We also show results of implicit setting on bearing dataset for different $\lambda$ in \tabref{tab:compare}. The trends in increasing error and decreasing deviation and TV are quite monotonic with $\lambda$. When $\lambda = 0.1$, the `game' model is even more accurate than `deep' model due to regularization effect of the game.

We visualize the explanations from implicit setting ($[\theta_0, \theta]$ in explainer model) over autoregressive generative trajectories in \figref{fig:visual}. The explanation from the `game' model is more stable. 
Compared to the ground truth, different temporal pattern after the $9^\text{th}$ point is captured by the explanations.
%\vspace{-1mm}