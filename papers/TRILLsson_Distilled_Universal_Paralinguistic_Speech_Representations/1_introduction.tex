Self-supervised learning for audio has improved the quality of speech representations, resulting in huge performance gains on downstream tasks~\cite{bigssl, hubert, baevski2020wav2vec}. 
%This work examines paralinguistic speech tasks which, unlike lexical ones, focus on aspects of speech other than the textual meaning. 
However, a few obstacles prevent these representations from being widely adopted on devices, particularly for paralinguistic speech tasks which focus on aspects of speech other than the textual meaning.
First, recent self-supervised models are often extremely large, making them challenging to use in resource-constrained environments like mobile phones (e.g. HuggingFace's Wav2Vec 2.0~\cite{baevski2020wav2vec} at 360 MB).
Second, due to the private nature of most speech data, high-performance models are often not publicly released (e.g. CAP12~\cite{cap12}). In this work, we overcome both constraints using public data and knowledge distillation~\cite{distillation}. Specifically, we knowledge distill a recent state-of-the-art paralinguistic speech representation ``Conformer Applied to Paralinguistics" (CAP12~\cite{cap12}) into a series of models, which we call \trillsson.

Our approach primarily relies on ``knowledge distillation"~\cite{distillation}. We train small student models on several fixed-length input architectures, including ResNets, EfficientNets, and Transformers, to match the arbitrary-length input CAP12 (teacher) Transformer embeddings. Our architectures explore the model size versus performance tradeoff. To our knowledge, this is the first successful cross-architecture distillation from a Transformer to non-Transformer model using a different dataset. 

We use nearly 58K hours of publicly available speech data from Libri-light~\cite{librilight} and Audioset~\cite{hershey2017cnn} for distillation. %and Youtube (YT-U)~\cite{bigssl}
For evaluation, we use the ``Non-Semantic Speech Benchmark" (NOSS)~\cite{trill}. NOSS includes 7 tasks such as emotion recognition and speaker identification that require slow-time features.
Additionally, we demonstrate the superior performance of \trillsson{} models by comparing them with existing publicly available representations such as TRILL~\cite{trill} and \wavvec~\cite{baevski2020wav2vec}.
Our contributions are:
\begin{enumerate}\itemsep0em
   \item Create generally-useful paralinguistic models that are small enough to run on-device.
   \item Demonstrate successful cross-architecture knowledge distillation from Transformers to fixed-context convolutional networks.
    \item Publicly release models at different points of the model size and performance trade-off curve.
    \item Identify the best paralinguistic representation in the public Wav2Vec2.0 model and demonstrate that our models outperform it.
 \end{enumerate}

%Our main approach to create a smaller, faster version of CAP12 is via ``knowledge distillation"~\cite{distillation}. In this paradigm, we train small models (students) to match the 1024-dimensional CAP12 embedding (teacher) on publicly available data. We then evaluate the student representations and release the best. Other works have distilled speech representations~\cite{frill}, but to our knowledge this work is the first successful speech embedding distillation from arbitrary-length input transformers to fixed-length input models.

%There have been a few attempts to establish benchmarks for evaluating speech representations on downstream tasks~\cite{trill, yang21c_interspeech}. In this paper, we use the ``Non-Semantic Speech Benchmark" (NOSS)~\cite{trill}, which focuses on tasks that require slow-time features that are necessary to capture for paralinguistic tasks, instead of benchmarks that measure both slow and fast-time feature tasks. The student architectures are smaller and faster, but also different architectures than CAP12, making this one of the first successful cross-architecture distillations we are aware of.

%The main contributions of this paper are:
 %\begin{enumerate}
 %  \item Publicly release a collection of generally-useful paralinguistic models that are outperform previous publicly-released state-of-the-art
%   \item Create and release generally-useful paralinguistic models that are small enough to run on-device
%   \item Demonstrate successful cross-architecture knowledge distillation from transformers to fixed-context size convolutional networks
   %\item Quantify the importance of certain factors such as dataset, context window size, and target generation method on the quality of the paralinguistic representation
% \end{enumerate}

