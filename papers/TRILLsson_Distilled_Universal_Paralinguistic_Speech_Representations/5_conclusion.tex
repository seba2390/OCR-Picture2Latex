In this work, we demonstrate that it's possible to distill huge models trained on large datasets to obtain much smaller models that perform well on paralinguistic speech tasks.
The distillation uses only \textbf{7\% of the training data} and is entirely from public sources. The models we obtain are between 22MB and 314MB, and achieve between \textbf{90\% and 96\% of the larger CAP12 accuracy on 6 of 7 tasks}. These models are between \textbf{1\% and 15\% the size} of the original model. We release the model to allow the research community to benefit from the practical applications of self-supervised representations for paralinguistic speech.