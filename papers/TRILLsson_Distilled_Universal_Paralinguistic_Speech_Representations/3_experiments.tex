\section{Approach}
\label{sec:experiments}

%%%%%%%%%%%%%%%%%%%%%%%%%%%

\input{table_train_datasets}

%%%%%%%%%%%%%%%%%%%%%%%%%%%
\input{table_eval_datasets}

\begin{figure}[t]
{\centering
  \includegraphics[width=0.9\columnwidth]{images/eval_flow.png}\hspace{0.2cm}
  \caption{Depiction of the evaluation process.}\label{fig:eval_flow}}
  \vspace{-3mm}
\end{figure}

%%%%%%%%%%%%%%%%%%%%%%%%%%%

%\begin{figure}[t]
%{\centering
%  \includegraphics[width=0.7\columnwidth]{images/training.png}\hspace{0.2cm}
%    \caption{Two paradigms for generating target embeddings from a teacher. In both cases, fixed context-window students are trained to match the target. Targets can be generated by either computing teacher embeddings over the entire audio clip (global matching) or over just the fixed-length window that the student sees (local matching). In this work, we use local matching.}\label{fig:training}}
  %\caption{Diagram of two different approaches for generating targets for fixed context window student models from transformer teacher models.}\label{fig:training}}
%  \vspace{-3mm}
%\end{figure}
%%%%%%%%%%%%%%%%%%%%%%%%%%%

Shor et. al. \cite{cap12} demonstrated that the 1024 dimension representation of the 12th layer of the CAP Conformer model (referred to as ``CAP12'') achieved at or near state-of-the-art performance across all tasks in the paralinguistic Non-Semantic Speech Benchmark (NOSS)~\cite{trill}. CAP12 is a 606M parameter (2.2GB) Conformer model trained via a modified \wavvec\  self-supervised training loss on a 900M+ hour speech dataset derived from YouTube (YT-U~\cite{bigssl}, Tab.~\ref{tab:train_ds}). %\cite{cap12} showed that the 1024 dimension representation of the 12th layer of the model (referred to as ``CAP12'') has impressive performance on a wide range of paralinguistic tasks.  
In this work, we use CAP12 as the teacher and distill this model to several ``lite'' architectures based on the teacher-student distillation approach~\cite{distillation}.

\subsection{Student architectures.}
%\noindent\textbf{Student architectures.}
\label{sec:kd}
We explore 3 different student architectures of varying sizes.
\begin{enumerate}\itemsep0em
    \item \textbf{Audio Spectrogram Transformer (AST)}~\cite{ast} is a Transformer-based model for audio classification. We train student models with different depths and widths.
    
    \item \textbf{EfficientNetv2}~\cite{efficientnetv2} was designed by neural architecture search on image classification. The architecture is mobile friendly. Different versions of this architecture vary in terms of depths and filters.
    
    \item \textbf{Resnetish}~\cite{hershey2017cnn} are modified ResNet-50 architectures designed to take audio spectral features as input. Different versions of this architecture include different depths and different number of filters per layer.
\end{enumerate}

%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Floating fig.
\input{table_results}

%\begin{figure}[t]
%{\centering
%  \includegraphics[width=0.9\columnwidth]{images/final_model_performance.png}\hspace{0.2cm}
%  \caption{Test set accuracies for final TRILLsson models.}\label{fig:final_model_performance}}
%  \vspace{-3mm}
%\end{figure}

%%%%%%%%%%%%%%%%%%%%%%%%%%%

Based on the Figure 1 lower in \cite{cap12} and the fact that some benchmark datasets have audio that's mostly less than 3 seconds (Tab.~\ref{tab:eval})  we focus on student architectures that process 2 seconds of audio at a time. When operating on audio less than 2 seconds, we symmetrically pad the audio around the end.

For the frontend, we use window length of 25ms and a hop length of 10ms. We use log-magnitude mel-frequency spectrograms with 80 bins ranging from 125 Hz to 7500 Hz. The frame width for all models is 2 seconds of audio, and we treat the frame advance between successive model patches as a hyperparameter. Since the model sees audio of exactly 2 seconds during training (see Sec~\ref{subsec:matching}), the frame advance hyperparameter can be explored after the model is fully trained.

\subsection{Distillation targets: global vs local matching}
%\noindent\textbf{Distillation paradigms.}
\label{subsec:matching}
There are two paradigms for generating targets from a teacher that borrow ideas from distilling large vision models~\cite{caron2021emerging}. In both cases the audio is first chunked into context windows which the student processes. Then the student is trained to match a target embedding generated using the teacher model. In ``global matching", the target is the average teacher embedding from the entire un-chunked audio clip. In the ``local matching" paradigm, the target is generated by averaging the teacher's output on the same context window that the student sees.  In this work, we focus on ``local matching."

\subsection{Training datasets}
We perform distillation on two open source datasets which together contain about 58K hours of speech data (Tab. \ref{tab:train_ds}). \textbf{Audioset}~\cite{audioset} clips are collection from YouTube, so it represents a variety of settings and acoustic environments. We use the speech subset of this data, which yields approximately 5K hours. \textbf{Libri-light}~\cite{librilight} contains 60k hours of audio derived from open-source audio books in the LibriVox project. It is the largest publicly available, unlabeled semi-supervised dataset to date. We note that the CAP12 teacher model~\cite{cap12} is trained on \textbf{YT-U}~\cite{bigssl}, which is a 900K hour dataset derived from YouTube.

%To evaluate the quality of our student networks, we use a modified version of NOSS datasets and the evaluation procedure. We describe the datasets and evaluation protocol in Section \ref{subsec:eval}. Our main aggregate comparison metric is ``Equivalent D-Prime," which we describe and motivate in Section~\ref{subsubsec:dprime}. We then describe our baseline comparisons in Section~\ref{subsubsec:baselines}. We describe our knowledge distillation approach in Sec.~\ref{sec:kd}. For student networks, we select 3 types of ``lite'' architectures, and train many variants of each. We describe these hyperparameters in the remaining sections.
%, and summarize them in Tab.~\ref{tab:train_hp}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure*}[t]
{\centering
  \includegraphics[width=2.0\columnwidth]{images/all_performance.pdf}\hspace{0.2cm}
  \caption{Comparison of ``average $d'$" vs. ``model size" for various student model architectures and sizes. Performance in this figure is across test sets, although only dev-set performance is used to select "best" models.}\label{fig:performance}}
%  \caption{`aggregate embedding performance" vs ``model size" for various student model architectures and sizes}\label{fig:performance}}
\end{figure*}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\vspace{-3mm}
\subsection{Representation Evaluation}
\label{subsec:eval}

We compare and evaluate the distilled representations on several tasks including detection of speaker, language, command, synthetic speech, dysarthria, and emotion. Table~\ref{tab:eval} provides an overview of the 7 datasets these tasks come from ~\cite{trill, cap12, asremb}.
%Table~\ref{tab:eval} describes the 7 datasets from ~\cite{trill, cap12, asremb} that we use to compare and evaluate the distilled representations. The tasks include detection of speaker, language, command, synthetic speech, dysarthria, and emotion. %5 are from the NOSS~\cite{trill} benchmark and 2 additional tasks described in \cite{cap12} and \cite{asremb}
%In order to evaluate distilled representations, we compare their performances on 7 tasks, 5 in the ``Non-Semantic Speech Benchmark (NOSS)" and 2 additional tasks described in \cite{cap12} and \cite{asremb}. %We follow the evaluation criteria described in \cite{cap12}, and summarize here for completeness (see Fig~\ref{fig:eval_flow} and Tab~\ref{tab:eval} for an overview).
As depicted in Fig~\ref{fig:eval_flow}, for each (model, eval dataset) pair, we first generate the candidate embeddings for the train, dev, and test splits. We then train three types of linear models on the train set embeddings. We take the model that performs best on the dev set, and report the model's performance on the test set as the score for that (model, eval dataset) pair. %When a single overall score is required (e.g. to compare embeddings), we aggregate by averaging d-prime scores (Sec.~\ref{subsubsec:dprime}) across tasks.

% Floating fig.
%\input{table_model_hparams}
\vspace{-2mm}
\subsubsection{Aggregation metric: Equivalent D-Prime ($d'$)}
\label{subsubsec:dprime}
To fairly compare with previous results, we report on the typical performance metric for each dataset (accuracy or equal error rate). However, to aggregate performance into a single scalar, for the purpose of comparing embedding quality, we average the ``equivalent d-prime" ($d'$) metric, defined as follows: 
\begin{equation}
    d' = \sqrt{2}Z(AUC)
\end{equation}
where ``AUC" is the ``Area Under the Receiver Operating Characteristic Curve" (ROC AUC), and ``$Z()$" is the inverse Cumulative Distribution Function for the standard normal distribution.

$d'$ is better suited to aggregate performance on multiple datasets for two reasons. First, ROC AUC and $d'$, unlike accuracy, take into account performance at various levels along the accuracy/recall curve. This makes it more reflective of the overall performance of a particular representation. Second, unlike ROC AUC, $d'$ doesn't saturate in the highly performant regime. Thus, 1 unit of $d'$ is in some sense more equivalent, so averaging $d'$ values across datasets is more natural than averaging AUC values. We use the average $d'$ scores to sort and select models.

\subsection{Models for comparison}
\label{subsubsec:baselines}
To contextualize performance on the NOSS benchmark, we compare our models to 1) previous state-of-the-art (SoTA) results, 2) publicly available speech representation models, and 3) CAP12 with different input sizes. Previous SoTA models are been mostly domain-specific. The public models we compare to are:

\begin{enumerate}\itemsep0em
    \item \textbf{Hugging Face's Wav2Vec \text{2.0}}~\cite{baevski2020wav2vec,transformers_lib}: This model was trained on the approximately 1K hour Librispeech~\cite{librispeech}. We use the TensorFlow model from the Hugging Face library. We compute the performances for each layer, and find that \textbf{layer 6 of 11 performs the best overall}.
    \item \textbf{TRILL}~\cite{trill}: A Resnet triplet-loss model trained on AudioSet. We access this model from TensorFlow Hub.
    \item \textbf{YAMNet}~\cite{hershey2017cnn}: This is a supervised audio classification network, also trained on AudioSet. We use layer 19 as in \cite{trill}. We access this model through TensorFlow Hub.
\end{enumerate}

%We make three comparisons to the teacher CAP12 model. As in \cite{bigssl}, we report NOSS numbers on CAP12 when it is provided the entire audio clip. We also report results when embeddings are computed on 2 and 3 second chunks, and then averaged over the clip. These last two comparisons explore the importance of limited context window on generating high-quality paralinguistic representations.

%\subsection{Knowledge Distillation}
%\label{sec:kd}

%To create small TRILLsson models, we perform  student training by matching CAP12~\cite{cap12} features on a collection of open source datasets (Tab. \ref{tab:train_ds}).
%We experiment with different context window sizes and two methods for generating targets (Sec.~\ref{subsec:matching}), and we explore a variety of student architectures (Sec.~\ref{subsubsec:students}).
%The list of hyperameters can be found in Tab.~\ref{tab:train_hp}


%\subsubsection{Distillation teacher: CAP12}

%The CAP12 model serves as the teacher. It is a 606M parameter conformer model. CAP's training objective is a modified Wav2Vec 2.0 self-supervised training loss on a 900M+ hour speech dataset derived from YouTube (YT-U~\cite{bigssl}, Tab.~\ref{tab:train_ds}). One intermediate layer, layer 12, shows particularly impressive performance on a wide range of paralinguistic tasks~\cite{cap12}. We refer to this 1024 dimension representation of speech as ``CAP12."

%\subsubsection{Distillation targets: global vs local matching}
%\label{subsec:matching}

%We explore two training paradigms that borrow ideas from distilling large vision models~\cite{caron2021emerging}. In the first method, which we call \textbf{``local matching"}, the student is tasked with matching the teacher on small chunks of audio e.g. 2 seconds. The training audio is chunking into the minimum size that the student processes as once, and the target is generated by averaging the teacher's output on that clip. In \textbf{``global matching"}, the student takes minimally chunked audio in the same way, but the target is the average teacher embedding from the entire, pre-chunking clip. This process is compared in Fig~\ref{fig:training}.

%We note that ``local matching" is a harder problem, in the sense that perfectly solving it implies that a network has also learned to do global matching. However, we also note that ``local matching" is harder than necessary; perfectly solving the ``global matching" problem would allow a student to reproduce a teacher's performance on our downstream evaluation tasks.

%\subsubsection{Distillation student architectures}
%\label{subsubsec:students}

%e experiment with three types of student architectures, and different hyperparameters for each. For all models, we experiment with 2 sizes of input (2 seconds and 3 seconds) and two types of targets (global matching and local matching).

%\begin{enumerate}
%    \item \textbf{AST}~\cite{ast}: The Audio Spectrogram Transformer (AST) is a convolution-free, transformer-based model for audio classification. We train student models with different depths and widths.
    
%    \item \textbf{EfficientNet/EfficientNetv2}: EfficientNetV2~\cite{efficientnet,efficientnetv2} is the result of neural architecture search on image classification. The architecture is designed to be high performing and mobile friendly. Different versions of this architecture involve different model sizes, which vary in terms of depths and filters.
    
%    \item \textbf{Resnetish}~\cite{hershey2017cnn}: This architecture is a Resnet architecture designed to take audio spectral features as input. Different versions of this architecture include different depths and a different number of filters per layer.
    
%\end{enumerate}

%\subsubsection{Training datasets}
%To create the distilled TRILLsson models, we perform  student training by matching teacher features on a collection of open source datasets (Tab. \ref{tab:train_ds}).

%\begin{enumerate}

%\item \textbf{Audioset}~\cite{audioset}: An audio event dataset. These clips are collected from YouTube, so it represents a variety of settings and acoustic environments. We use the speech subset of this data, which yields approximately 5K hours.

%\item \textbf{Libri-Light}~\cite{librilight}: Contains 60k hours of audio derived from open-source audio books in the LibriVox project. It is the largest publicly available, unlabeled semi-supervised dataset to date.

%\item \textbf{YT-U}~\cite{bigssl} (teacher network only): A 900k hour dataset derived from YouTube. YT-U is built by first randomly collecting 3 million
%hours of audio from ``speech-heavy" YouTube videos. The results are then segmented, and the non-speech segments are removed to yield approximately 900k hours of unlabeled audio data.

%\end{enumerate}