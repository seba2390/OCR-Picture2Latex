

Table~\ref{tab:results} shows the sizes and performances of 5 \trillsson{} models as compared to CAP12 and other publicly available models. We see that the two largest \trillsson{} models outperform all publicly available models on all datasets except Wav2Vec2 layer 6 on speech commands, and that even \trillsson{}1, the smallest model, outperforms previously available embeddings on emotion recognition tasks. In particular, \trillsson{}2 outperforms the best Wav2Vec 2.0 representation on 5 of 6 public tasks despite being less than 12\% the size.

Compared to CAP12, the distilled models maintain performance on language identification and dysarthria detection. \trillsson{} models suffer minor degradation on speech emotion recognition tasks, and major degradation on speaker identification, speech commands, and fake speech detection.

Fig~\ref{fig:performance} shows the relationship between size and performance across different architectures. We see that Resnetish models perform best at the low end, EfficientNets perform best at sizes between 40-230MB, and AST performs best when larger then 230MB.

\textbf{Training on AudioSet improves performance on emotion recognition tasks:} Using a dependent t-test on paired samples between models trained on Libri-light and AudioSet versus just Libri-light, we observed statistically significant improvements in dev and test set accuracies on the speech emotion recognition tasks (CREMA-D and IEMOCAP, $n=86$, $p<0.03$). Interestingly, training on the combined dataset hurt performance on ASVSpoof2019 test set compared to either dataset on its own ($n=86$, $p<10^{-3}$), but the paired t-test did not reject the null hypothesis on the ASVSpoof2019 dev set.

\textbf{Best model curve robust to removing a single benchmark task:} Removing any single task keeps the size versus performance curve relatively stable. Either \trillsson{}5 or 4 are the best models with any single task removed. \trillsson{}3 and 2 appear in every optimal curve. \trillsson{}1 appears in every optimal curve unless 'voxforge' is removed.

\input{table_kendaltau}

\textbf{$d'$ orderings}: Table~\ref{tab:kendeltau} justifies our choice of $d'$ on the dev set as our ordering criteria. The Kendall rank coefficient, a correlation statistic for different orderings, shows that average $d'$ on the dev sets more closely correlates to average accuracy and average $d'$ than average accuracy on the dev set.

%\subsection{Choice of dprime}

%The pearson coefficient between average dprime on dev/test and average accuracy on dev/test is 0.73/0.79. Whereas 

%\begin{figure}[t]
%{\centering
%  \includegraphics[width=0.9\columnwidth]{images/correlation.png}\hspace{0.2cm}
%  \caption{Relationship between dprime on dev/test and average accuracy on dev/test.}\label{fig:corr}}
%  \vspace{-3mm}
%\end{figure}


