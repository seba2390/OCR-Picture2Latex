\section{\large Method}
\label{sec_method}
Our method aims to address the issue of ambiguity and distortion in images by providing additional knowledge about the sensor used to capture the image. This additional information is encoded in a deflection metric, which can be interpreted or processed by a CNN.

First, we define the deflection metric, which encodes the projection properties of a sensor, such as the sensor's field of view and distortion. This is done by creating a spherical projection of the sensor data, and transforming it into an image representation (see sections \autoref{sec:spherical projection} and \ref{sec:deflection}).

Second, we show how the deflection metric is provided as input to a state-of-the-art object detection and instance segmentation architecture (see \autoref{sec:backbone}), which enables the CNN to better understand the image, reducing the ambiguities in scale and distortions.


\subsection{Spherical Projection}
\label{sec:spherical projection}
There has been a growing interest in using spherical projections for LiDAR instance segmentation in recent years. Spherical projections offer several advantages, such as being able to capture the full 360-degree field of view and being able to maintain consistent point density across the whole projection.

\begin{figure}[!h] 
    \centering
    \includegraphics[width=\columnwidth]{images/spinning_lidar.pdf}
    \caption{Coordinate system of a spinning LiDAR. $\phi$ as the azimuth angle and $\theta$ as an inclination angle from the $xy$-plane.}
    % \renewcommand{\the}{}
    \label{fig:spinning_lidar}
\end{figure}

The process of projecting point clouds from a spinning LiDAR sensor into a spherical image involves converting the Cartesian coordinates of the measurement points into spherical coordinates. Specifically, the process converts each point in the point cloud represented by its Cartesian coordinates $[x,y,z]^T$, into spherical coordinates represented by $[phi, theta, r]^T$. This conversion is illustrated in Fig. \ref{fig:spinning_lidar}. The phi coordinate corresponds to the angle of the point in the XY plane, theta is the angle from the positive Z-axis, and r is the distance from the origin.
This spherical projection is a way to capture the geometry of the sensor in a single image. Subsequently, we use the following projection model to generate a spherical range image:
\begin{equation}
\underbrace{\begin{bmatrix}
 u\\
 v\\
 1\\
\end{bmatrix} }_{\vec{u}}
=
\underbrace{\begin{bmatrix}
\frac{1}{\triangle \phi} &  0 &  c_{\phi}\\
 0 &  \frac{1}{\triangle \theta} &  c_{\theta}\\
 0 & 0 & 1\\
\end{bmatrix}}_{\mathbb{K}} \cdot \underbrace{\begin{bmatrix}
 \phi\\
 \theta\\
 1\\
\end{bmatrix}}_{\vec{x}}  
\end{equation}
Analogous to the projection model of pinhole cameras, the projection matrix $\mathbb{K}$ describes a discretization $\triangle \phi, \triangle \theta$ along the angles $\phi, \theta$ and a shift of the center coordinates $c_{\phi}, c_{\theta}$ defined by the height and width of the resulting image. Since the discretization can cause several points to be projected onto one pixel, we only use the points with the smallest Euclidean distance $r$ to the sensor. For a conventional spinning LiDAR sensor, the image height $h$ and width $w$ will be equivalent to the number of layers and azimuth increments, respectively, as displayed in Fig.~\ref{fig:spherical}.
\begin{figure}[!h] 
    \centering
    \includegraphics[width=\columnwidth]{images/projection_img.pdf}
    \caption{Spherical image $I$ with principal point $P$, height $h$, width $w$.}
    % \renewcommand{\the}{}
    \label{fig:spherical}
\end{figure}

With the spherical projection, an image representation $I$ can be constructed. Points from a 3D point cloud and auxiliary data can be projected to this ordered image representation. They result in several images for a LiDAR scan.  E.g. $I_{r}$ for the Euclidean distance.

\subsection{Deflection Metric}
\label{sec:deflection}
\begin{figure}[!h] 
    \centering
    \includegraphics[width=\columnwidth]{images/Projection.pdf}
    \caption{Deflection metric $\alpha$: is calculated as an angle between the optical axis, defined by a sensor's origin $C$ and a principal point $P$, and a pixel $\vec{u}=[u,v]^T$.}
    % \renewcommand{\the}{}
    \label{fig:sens_dat_abs}
\end{figure}
With the deflection metric, as shown in \autoref{fig:sens_dat_abs}, we propose a method to encode the geometric characteristics of a sensor alongside its data. We do this using a deflection image $I_{\alpha}$, which encodes an inclination angle $\alpha$ in every pixel as a one channel image. $\alpha$ provides a consistent relation between a projected 3D point and the position of this 3D point in relation to the sensor and is, therefore, compatible among sensors with different characteristics. The deflection metric $\alpha$ is defined as an angle between the optical axis, defined by a sensor's origin $C$ and a principal point $P$, and a pixel $\vec{u}=[u,v]^T$. $\alpha$ essentially describes half the field of view at a pixel position $\vec{u}$. 

The projection can be modeled by a sensor model and parameterized by linear or nonlinear sensor intrinsics. This intrinsics can be calculated based on the constituents of the projection system, e.g., the position and properties of lenses and apertures, measured in an optical setup or estimated by calibration. 

Based on a parameterized sensor model, the deflection metric  $\alpha$ for a pixel position $\vec{u}$ can be determined by the sensor's intrinsic matrix $\mathbb{K}$. Based on the image coordinates, $\alpha$ of a pixel $\vec{u}$ with respect to the projection center $P$ can be determined:
\begin{equation}
\alpha(\vec{u}) = \sqrt{\left[\mathbb{K}^{-1}\vec{u}\right]^T\left[\mathbb{K}^{-1}\vec{u}\right] - 1}
\label{equ:deflection}
\end{equation}
All the same values of the deflection metric $\alpha$ are arranged on a circle or ellipse $e$ (see dashed circle in \autoref{fig:sens_dat_abs}) in image data. 
From $\alpha(\vec{u})$ a normed distance $d(\vec{u})$, such that the distance between sensor center $C$ and principal point $P$ is one, can be calculated by $d(\vec{u})= tan(\alpha(\vec{u}))$. We would like to emphasize that due to the change of coordinates to spherical coordinates, \autoref{equ:deflection} does not calculate the $\alpha(\vec{u})$ when using a camera intrinsic matrix $\mathbb{K}$, but the normalized distance $d(\vec{u})$. To obtain $\alpha(\vec{u})$ for a camera sensor, the inverse trigonometric function $\alpha(\vec{u})= arctan(d(\vec{u}))$ has to be used.

The deflection image $I_{\alpha}$ is a one-channel image, in which every pixel $\vec{u}$ is aligned with the data image $I$. This increases the information content of each pixel by the geometric sensor properties. The deflection image $I_{\alpha}$ can be processed together with the sensor data by convolutional layers of a CNN. Unlike image data, the deflection image is not invariant to translation, rotation, and scale. However, since the convolutional layers of CNNs are learned, a CNN can decide whether to use this additional information in the learning process. 
The range image $I_{r}$ and the deflection image $I_{\alpha}$ are components of a 3D coordinate system and designed to be processable with convolutional filters. 
% From $I_{\alpha}$ and the curvature of $\alpha$ in its pixel neighborhood $I_{\beta}$ and $\beta$ can be derived by building the gradient direction, as shown in \autoref{fig:beta}. This can be done utilizing simple convolution operations, i.~e. Sobel, Prewitt, or Scharr. The Scharr filter might be beneficial due to numerical precision and rotational symmetry. Since these are convolutional filters, a CNN can learn the calculation of $I_{\beta}$ and $\beta$ implicitly, if needed. $\alpha$ and $\beta$ together uniquely define a point on an image plane $I$. Together with a range measure $r$, a unique 3D point can be defined. We would like to emphasize that this is completely differentiable and thus can be applied in CNN architectures.
% \begin{figure}[!h] 
%     \centering
%     \includegraphics[width=0.7\columnwidth]{images/beta.pdf}
%     \caption{Definition of a rotation angle $\beta$ form $I_{\alpha}$. }
%     % \renewcommand{\the}{}
%     \label{fig:beta}
% \end{figure}

\subsection{Top-Down Injection}
\label{sec:backbone}

\begin{figure}[!h] 
    \centering
    \includegraphics[width=\columnwidth]{images/projFPN.pdf}
    \caption{Backbone Architecture. The deflection metric is injected before every pyramid stage. $\downarrow2$ denotes a down-sample operation, $\uparrow2$ denotes an up-sample operation, $\bigoplus$ denotes a channel wise concatenation.}
    % \renewcommand{\the}{}
    \label{fig:architecture}
\end{figure}
An existing backbone meta-architecture is modified to inject the deflection metric into the model at the input and selected locations. Based on the findings in \cite{wang2021solo}\cite{wang2020solov2} we decided to use a ResNet50-FPN. The modification to the ResNet-FPN is shown in \autoref{fig:architecture}. At the input stage, the deflection image $I_{\alpha}$ is concatenated with a three-channel image $I$, resulting in an input shape of $h\times w \times 4$. The image input is processed top-down in five down-sampling stages. Each stage halves the height $h$ and width $w$. This is done for the image data using strided convolutions, followed by a residual block ($C1$ to $C5$). The deflection image $I_{\alpha}$ is down-sampled in parallel and concatenated to the features of the stages $C1$ to $C5$. This ensures that the feature map can be used in every stage. After the injection, a $1\times1$-convolution is used to fuse the features of the stage with the deflection metric. This allows the network to keep or discard the deflection metric for a particular stage. The feature maps are up-sampled from the bottom up prior to a fusion with the pristine feature map from the respective stages. The fusion is performed by a channel-wise concatenation of the feature maps and a subsequent $3\times3$-convolution for anti-aliasing, as with common FPN architectures. This results in the pyramid stages $P$ with the respective shapes $(h/2^{i})\times( w/2^{i})\times256$ ($i$ denotes the stage index). The pyramid stages are then fed into a semantic segmentation head, as described in \cite{LinFPN}.

\subsection{Data Augmentation}
This section discusses the use of image augmentation in the training process of the proposed method. Image augmentation is a technique of altering the existing data to create more data for the model, which is especially important when the training data comes from a single sensor source, but the model should work sensor equivariant.

The proposed method uses geometric image-based operations to augment both the range image and the deflection image. The operations used are resize and center-crop. The resize operation changes the resolution of the sensor, and the center-crop operation changes the field of view. The combination of both allows the simulation of various sensors during training. It's worth noting that with this kind of augmentation, only sensors with an equal or smaller field of view and resolution can be simulated.

In the context of LiDAR semantic segmentation, the augmentation process modifies the vertical and azimuthal resolution of the sensor, as well as the vertical field of view. This helps the model learn to generalize to new sensor configurations.

%\begin{equation}
%\mathbb{K}^{*} =\begin{bmatrix}
%s_u &  0 &  0\\
% 0 &  s_v &  0\\
% 0 & 0 & 1\\
%\end{bmatrix}\cdot\mathbb{K}
%\end{equation}
