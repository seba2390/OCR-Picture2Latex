\section{\large Experiment}
\label{sec:expirement}

\subsection{Dataset}

\textbf{Data Generation}
We used the CARLA simulator for data generation. In CARLA we simulated an ultra-high resolution LiDAR with full dome coverage, mounted to a carrier vehicle, resulting in dense point clouds with semantic segmentations, instance segmentations, cuboids, and meta information. 
For the instance segmentation, we consider three classes of road user $C \in \{\textit{pedestrian},\textit{vehicle},\textit{two-wheeler}\}$. Our simulated high-resolution LiDAR is beyond current commercially available sensors in terms of resolution and FoV. However, we can generate LiDAR sensors with more reasonable parameters regarding resolution and field of view from the high-resolution LiDAR, while maintaining initial conditions like semantic context, giving us a controlled environment for our experiment. 
We generate our simulated LiDAR frames at a frame rate of 1 Hz so that two consecutive frames do not look too similar. We also chunked our dataset into scenes of 100 frames. For each scene, we altered the simulation location (i. e. map), the initial parameters for traffic simulation (e.g. amount, type, and behavior of other road users), the carrier vehicle, and its trajectory. This totals 24 sequences, from which we use 15 as training data and 9 for testing. In addition, we ensured that the carrier vehicle and the simulation location differed in the test and train sequences.
The high-resolution LiDAR scans consist of a vertical FoV of $f_v = 180^\circ$, a horizontal FoV of $f_h = 360^\circ$, the number of azimuth increments $w=2048$, and the number of horizontal layers $h=1024$. This results in a spherical projected range image $I_r$ (as described in \autoref{sec:spherical projection}) with a resolution $1024\times2048$.


\begin{figure}[h]
     \centering

    \begin{minipage}{\columnwidth}
        \centering
        \includegraphics[width=0.8\linewidth]{images/pc_1170.jpg}
        \subcaption{Point Cloud.}
    \end{minipage}
    \hfill
    \begin{minipage}{\columnwidth}
        \centering
        \includegraphics[width=0.8\linewidth]{images/1170.jpg}
        \subcaption{Spherical Projection.}
    \end{minipage}
    \caption{Example frame with instance labels of the simulated high-resolution LiDAR.}
    \label{fig:dataset}
\end{figure}

\subsection{Setup}
\textbf{Implementation details}: We implemented our method in Detectron2 \cite{wu2019detectron2} and used their reference implementations. For training, we used a single NVIDIA 3090 TI GPU. For the instance segmentation, we use  MaskRCNN \cite{maskRCNN} and PointRend \cite{kirillov2019pointrend} as application head. For the backbone, we modified a ResNet-50 FPN as described in \autoref{sec:backbone}. All models were trained until convergence. 
\\

\textbf{Evaluation and Metrics}:
We use mean average recall (mAR) as defined in COCO. We prefer mAR over mean average precision (mAP) since it indicates how many objects are correctly detected. A higher recall is preferable for downstream tasks like object tracking since removing false positive detection over time is easier than adding missed detections. 
\\

\textbf{Experimental Setup}: \label{sec:experiment_B}
In the following, we describe our experimental design to test our method's generalizability to sensors with other characteristics (e.g. FoV and resolution). We projected the high resolution LiDAR scans and instance labels to sensors with different characteristics. We mainly changed the number of azimuth increments, the number of horizontal layers, and the vertical FoV to $w \in \{512,1024,2048\}$, $h \in \{32,64,128\}$, and  $f_v \in \{22.5^\circ, 45^\circ, 90^\circ\}$ respectively. We use the spherical projection described in \autoref{sec:spherical projection}  to project the point clouds to the image representation $I$. We then resize the projected image $I$ accordingly to $h$ and $w$. 
In this way, we reproduce sensors that are currently on the market. Currently, spinning LiDAR with a vertical resolution of 128 and a horizontal resolution of 2048 from manufacturers such as Ouster or Velodyne represent the commercial state of the art. However, these are currently still expensive, and if this aspect and also the required runtime are taken into account, sensors with lower resolution are also relevant for research and industrial applications. However, sensors with a lower resolution can be augmented from these high-resolution sensors at any time. This is not possible with FoV. For real world data acquisition, we recommend to use high resolution sensors if possible.

\subsection{Ablation Study}

\begin{figure*}[h]
    \centering
    \includegraphics[clip,width=0.8\textwidth]{images/boxplot_AR_eval.pdf}
    \caption{Distribution over Mean Average Recall.}
    \label{fig:mAR}
\end{figure*}

In ablation, we study the impact of the deflection metric and data augmentation, focusing on modifications in FoV. 
As our baselines, we use ResNet-50 with FPN, MaskRCNN, and PointRend as application head exclusively trained on a single vertical FoV $f_v \in \{22.5^\circ, 45^\circ, 90^\circ\}$ , which we further denote as $R_{22.5^\circ}$, $R_{45^\circ}$, and $R_{90^\circ}$ respectively. We also train this model with augmentation, further denoted as $RA$. Our model, as described in \autoref{sec_method} is further denoted as $DRA$ and also trained with augmentation. For the augmentation of $RA$ and $DRA$, we random change the $w \in \{512,1024,2048\}$, $h \in \{32,64,128\}$, and  $f_v \in \{22.5^\circ, 45^\circ, 90^\circ\}$ during training. For $R_{22.5^\circ}$, $R_{45^\circ}$, and $R_{90^\circ}$ we only augment $h$ and $w$. We refrain from training a model for each resolution to limit the scope of this work. For testing, we used the sequences from the test set and generated a small test set for each sequence at each resolution and field of view from $w$, $h$, and $f_v$, totaling 243 small test sets. We calculate the mAR for every subset. In \autoref{fig:mAR} the distribution of mAR over those small test sets for the considered models can be seen as box plots. We display the mAR over the complete dataset and exclusively for the subsets with $22.5^\circ$, $45^\circ$, and  $90^\circ$ FoV, respectively.
While there is some noticeable spread over mAR over the complete dataset for each model, we can see that the median mAR for the models $R_{22.5^\circ}$, $R_{45^\circ}$, and $R_{90^\circ}$ (marked as an indent in the box plots) is low, the median mAR for $RA$ is noticeably better, and the median mAR of $DRA$ is even better. This indicates a limitation of $R_{22.5^\circ}$, $R_{45^\circ}$, and $R_{90^\circ}$ to transfer to data captured from sensors with different FoVs. Furthermore, in the experiments in which we exclusively test the performance on a single FoV we can see that there are general differences in the mAR concerning the FoV.
We assume this is due to the size of the objects in pixel space. Objects appear smaller at a larger FoV, and the smaller an object is, the harder it is to detect it. 
The models that are exclusively trained on the evaluated FoV tend to work well, while those trained on other FoVs have a significantly lower mAR. It is noticeable that $RA$ seems to generalize but cannot catch up with the mAR of the models exclusively trained on the evaluated FoV. On the other hand, DRA achieves a slightly better mAR for each FoV under consideration compared to the models trained exclusively on the FoV under consideration.
\\

\textbf{Discussion}:

We showed in our experiment that using data from a single LiDAR sensor source for training might bias an instance segmentation model when applying the model to data from novel sensors with different FoVs. 
Using data from various sensors as simulated by the augmentation in $RA$ can increase the capability of a model to generalize. However, it is not possible to use the full potential of data from multiple sensors. The experiment shows that our method $DRA$ can help to generalize over various sensors without sacrificing quality compared to the sensor specific models.

We neglect some sensor effects by using simulated and idealized data. Nevertheless, we interpret our results so that our method can contribute to achieving sensor equivariance. Beyond our method, annotated data from multiple sensors and suitable augmentation strategies are needed for real world applications. 



