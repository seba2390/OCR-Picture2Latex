\section{\large Introduction}
\label{sec_introduction}


This work aims to address the problem of generalizing a machine learning model, specifically instance segmentation, trained on data from a single sensor or a collection of sensors, to new sensors with different characteristics, such as field of view and resolution. The motivation behind this research is the fact that a wide variety of sensors with different properties are integrated into many products, and the functionalities and capabilities of these products heavily depend on the sensors used. This means that new sensors will be continuously introduced as they become available.

One important application for this research is in the field of autonomous vehicles, specifically in creating a scene understanding from sensor data using segmentation, which assigns a class and instance label to each data point, such as a 3D point of a LiDAR scan or a pixel of a camera image as shown in \autoref{fig:visual_abstract}. Autonomous driving perception modules usually consist of data-driven models based on sensor data, but these models may be biased toward the sensor used for data acquisition, which can seriously impair the transferability of the perception models to new sensor setups.

For LiDAR sensors, in particular, numerous manufacturers have emerged in recent years, adding new technologies and sensors to the market. Various previous approaches have projected a LiDAR 3D point cloud onto a 2D spherical range image using efficient 2D convolutional operations and architectures for image segmentation. We propose using deflection metric to solve the problem of transferring machine learning model from a single sensor to new sensors.


\subsection{Problem Statement and Intuition} \label{sec:problem}
\begin{figure}
     \centering
     \begin{minipage}{0.49\columnwidth}
        \includegraphics[width=\linewidth, height=4.5cm]{images/abstract_pcl.pdf}
        \subcaption{LiDAR Scan.}
    \end{minipage}
    \hfill
    \begin{minipage}{0.49\columnwidth}
        \includegraphics[width=\linewidth, height=4.5cm]{images/abstract_projection.pdf}
        \subcaption{Spherical Projection.}
    \end{minipage}
    \caption{Simulated LiDAR scans of a pedestrian at different fields of view and vertical resolutions at the distances 10 m, 20 m, and 40 m, in yellow, green, and blue, respectively (left). Spherical projections of those scans (right).}
    \label{fig:problem}
\end{figure}
LiDAR point clouds are unstructured and sparse. As the distance between the sensor and the object increases, the number of measurement points on the object decreases, making it difficult for machine learning models to accurately detect and classify the object. Mainly because it's hard to find connected components in an unstructured point cloud at different levels of sparsity. The level of sparsity is further compounded by the sensor's Field of View (FoV), vertical resolution, and azimuth resolution. In \autoref{fig:problem} this is shown for a person distanced 10 m, 20 m, and 40 m from sensors with differences in resolution and field of view. 

With spherical projections of LiDAR scans are considered. Instead of becoming sparse, the size of the object changes in pixel space \autoref{fig:problem}. Thus, relaxing the issue of dealing with lack of structure and sparsity to the issue of dealing with different scales in pixel space. 

\subsection{Related Work} \label{sec_sota}

For image data, there is some preliminary work, addressing the problems stated in \autoref{sec:problem}:
Liu et al. introduce CoordConvs \cite{CoordConvs}, as a solution for the coordinate transform problem.
The convolution itself is spatial invariant. However, for some tasks, spatial variance might be needed. In object detection, for example, the coordinate transform problem arises from processing features in pixel space and output bounding boxes in cartesian space. 
A CoordConv layer is a simple extension of the standard convolutional layer. It has the same functional signature as a convolutional layer, but utilizes extra channels for the incoming representation. These channels contain hard-coded coordinates, the most basic version of which is one channel for the $u$ coordinate and one for the $v$ coordinate. The authors claim that the CoordConv layer keeps the properties of few parameters and efficient computation from convolutions, but allows the network to learn if spatial variance or invariance is needed for learning the task. This is useful for coordinating transform-based tasks where regular convolutions can fail. However, the authors proved their concept on simplified tasks.

Wang et al. \cite{wang2021solo}\cite{wang2020solov2} utilize the CoordConv concept as a component in an instance segmentation framework. The authors argue that spatially variant convolutions are necessary for instance segmentation, which is related to semantic segmentation. Furthermore, they concluded that few CoordConv layers within the backbone are enough to achieve this.

Facil et al. teach camera-aware multi-scale convolutions for depth estimation from image data \cite{CamConv} supplied to a neural network. The method comprises pre-computing pixel-wise coordinates and horizontal and vertical field-of-view maps, fed with input features to a convolution operation. These maps are supplied to the neural network with different resolutions and on different layers to allow the network to learn and predict depth patterns that depend on the camera calibration. The authors conclude that the neural network supplied with the respective maps can generalize over camera intrinsics and allow depth prediction networks to be camera-independent.
This work can be seen as the work being most closely related to our approach. Even if this method can be adapted for image data in general, it utilizes four additional channels exclusively designed for camera sensors.

In order to process LiDAR data with such a method, the data must first be converted into an image representation.
Many works exist that convert LiDAR point clouds into images through a spherical projection. The SemanticKITTI benchmark \cite{behley2019iccv} and RangeNet \cite{8967762} have contributed significantly to this development. The elegance of spherical projection is that LiDAR data can be processed similarly to camera data with a Convolutional Neural Network (CNN). This also includes a significant improvement in runtime when processing LiDAR data. Methods such as Lite-HDSeg \cite{litehdseg}, SalsaNext \cite{SalsaNet}, or SqueezeSeg \cite{SqueezeSegV2} all consider spherical projections and propose different architectures for processing.  

In \cite{LiDARNet} the authors present a boundary-aware domain adaptation
model for LiDAR scan full-scene semantic segmentation. Their method considers boundary information while learning to predict full-scene semantic segmentation labels. They also use spherical projection of LiDAR data and demonstrate the adaption of their model to different sensors. They mainly use sensors with similar resolutions. In this respect, they do not address the problems we state in \autoref{sec:problem}. 

\subsection{Main Contributions} \label{sec_contrib}
This paper presents a novel approach for encoding the projection properties of a sensor in an image representation called the deflection metric. The deflection metric is a one-channel image that can resolve ambiguities in the projection and homogenize data from various sensors.

To demonstrate the effectiveness of the deflection metric, we propose a backbone architecture based on Faster-RCNN and a spherical projection model for spinning LiDAR sensors, adapted from the pinhole camera model. Combined with range information the deflection metric can define a 3D coordinate system suitable for processing with CNNs.

We conducted experiments using a high-resolution LiDAR dataset generated with the CARLA \cite{Dosovitskiy17} simulator. To evaluate the transfer ability of our method and conduct an ablation study to confirm its usability on sensors with different fields of view.

For transparency and reproducibility, we also included the source code for the data generation process, training and evaluation of our models, and a collection of pre-trained models.
