\section{Proposed single- and two-stage CNN approaches}
\begin{figure}[h]
	\begin{center}
		\includegraphics[width=.8\columnwidth]{workflow2.png}
	\end{center}
	\caption{
		Workflow of the proposed algorithm. First a CNN is employed to estimate
		a coarse pupil position based on subregions from a downscaled version of
		the input image. This position is upsampled to the
		full resolution of the input image (\emph{upsampled position} in the
		workflow diagram). This position is then refined using subregions around
		the coarse estimation in the original input image by a second CNN.
	}
	\label{fig:workflow}
\end{figure}
The overall workflow for the proposed algorithm is shown in
\figref{fig:workflow}.
In the first stage, the image is downscaled and divided into overlapping
subregions. These subregions are evaluated by the first CNN, and the center of
the subregion that evokes the highest CNN response is used as a coarse pupil
position estimate. Afterwards, this initial estimate is fed into the second pipeline stage. In this
stage, subregions surrounding the initial estimate of the pupil position in the
original input image are evaluated using a second CNN. The center of the
subregion that evokes the highest CNN response is chosen as the final pupil
center location. This two-step approach has the advantage that the first step (i.e., coarse
positioning) has to handle less noise because of the bicubic downscaling of the
image and, consequently, involves less computational costs than detecting the
pupil on the complete upscaled image. In the following subsections, we delineate these pipeline stages and their CNN structures in detail, followed by the training procedure employed for each CNN.


\subsection{Overview of all CNNs}
\begin{table}[h]
	\caption{Overview of all evaluated CNN configurations. In row CNN the
	assigned names can be seen. \textbf{C} stands for convolution filter size,
	\textbf{K} is the amount of kernels (or filters), \textbf{D} stands for the
	pooling layer where D comes from down sampling and \textbf{P} stands for the
	amount of perceptron weights in the fully connected layer.}
	\begin{center}
		\begin{tabular}{lllccc|ccc|c|}
			&& & \multicolumn{3}{c}{Layer 1} & \multicolumn{3}{c}{Layer 2} & \multicolumn{1}{c}{}\\ \cline{4-10}
			& & \multicolumn{1}{l|}{\textbf{CNN}} & \textbf{C} & \textbf{K} & \textbf{D}
			& \textbf{C} & \textbf{K} & \textbf{D} & \textbf{P}\\ \cline{3-10}
			&\multirow{3}{*}{Coarse}&\multicolumn{1}{|l|}{\ckp{8}{8}}  & 5 & 8 & 4 & 5 & 8 & - & 8\\
			& &\multicolumn{1}{|l|}{\ckp{8}{16}}  & 5 & 8 & 4 & 5 & 16 & - & 16\\
			& &\multicolumn{1}{|l|}{\ckp{16}{32}} & 5 & 16 & 4 & 5 & 32 & - & 32\\ \cline{3-10}
			& \multirow{1}{*}{Fine}&\multicolumn{1}{|l|}{\cfin}  & 20 & 8 & 5 & 14 & 8 & - &  8\\ \cline{2-10}
			&\multirow{1}{*}{Direct}&\multicolumn{1}{|l|}{\csin{}} & 6 & 8 & 4 & 5 & 8 & - &  8\\
			&\multirow{1}{*}{Fine}&\multicolumn{1}{|l|}{$F_{SK_8P_8}$} & 20 & 8 & 5 & 14 & 8 & - &  8\\ \cline{2-10}
		\end{tabular}
	\end{center}
	\label{tbl:overview}
\end{table}

Table~\ref{tbl:overview} shows an overview of all CNN configurations with their
assigned names. All coarse CNNs follow the core architecture presented
in~\secref{subsec:coarsestage}, and each candidate has a specific number of
filters in the convolution layer as well as perceptron weights in the fully connected
layer. Their names ($CK_{X}P_{Y}$) are prefixed with \underline{C}
(\underline{C}oarse) using X \underline{K}ernels in the first layer and Y
connections to the final \underline{P}erceptron in the fully connected layer.
The second stage CNN (see Figure~\ref{fig:workflow}) is named \underline{F}ine
CNN. The name ($F_{CK_{X}P_{Y}}$) specifies also the assigned coarse positioning
CNN. This CNN is further described in \secref{subsec:finestage}. The last CNN is the
direct pupil center estimation approach \csin{}, where only one
\underline{S}ingle stage is used based on the downsampled image. Those are
described in \secref{subsec:trainingdirect}. However, we evaluated \csin{} also
with the two step approach ($F_{SK_8P_8}$).  

\subsubsection{Coarse positioning CNN (\ckp{8}{8}, \ckp{8}{16}, \ckp{16}{32})}
\label{subsec:coarsestage}
\begin{figure}[h]
	\begin{center}
		\hfill
		\subfloat[][\label{a}]{
			\includegraphics[width=.4\columnwidth]{splitt.png}
			\label{fig:splitt}
		}
		\hfill
		\subfloat[][\label{b}]{
			\includegraphics[width=.4\columnwidth]{cnnauswertung2.png}
			\label{fig:cnncoarse}
		}
		\hfill{}
		\caption{
			The downscaled image is divided in subregions of size $24\times24$
			pixels with a stride of one pixel (a), which are then rated by the
			first stage CNN (b).
		}
	\end{center}
\end{figure}
The grayscale input images generated by the mobile eye tracker used in this work
are sized $384\times288$ pixels.
Directly employing CNNs on images of this size would demand a large amount of
resources and, thus, would be computationally expensive, impeding their usage in
state-of-the-art mobile eye trackers. Thus, one of the purposes of the first
stage is to reduce computational costs by providing a coarse estimate that can
in turn be used to reduce the search space of the exact pupil location.
However, the main reason for this step is to reduce noise, which can be induced
by different camera distances, changing sensory systems between head-mounted eye
trackers~\citet{boie1992analysis,dussault2004noise,reibel2003ccd}, movement of the
camera itself, or the usage of uncalibrated cameras (e.g., out of focus,
unbalanced white levels).
To achieve this goal, first the input image is downscaled using a bicubic
interpolation, which employs a third order polynomial in a two dimensional space
to evaluate the resulting values.
In our implementation, we employ a downscaling factor of four times, resulting
in images of $96\times72$ pixels.
Given that these images contain the entire eye, we chose a CNN input size of
$24\times24$ pixels to guarantee that the pupil is fully contained within a
subregion of the downscaled images.
Subregions of the downscaled image are extracted by shifting a $24\times24$
pixels window with a stride of one pixel (see \figref{fig:splitt}) and evaluated
by the CNN, resulting in a rating within the interval [0,1]
(see~\figref{fig:cnncoarse}).

These ratings represent the confidence of the CNN that the pupil center is
within the subregion. Thus, the center of the highest rated subregion is chosen
as the coarse pupil location estimation.
The core architecture of the first stage CNN is summarized in Table~\ref{tbl:overview}.
The first layer is a convolutional layer with filter size $5\times5$ pixels, one
pixel stride, and no padding.
The convolution layer is followed by an average pooling layer with window size
$4\times4$ pixels and four pixels stride. The subsequent stage is an additional
convolution layer with filter size $5\times5$, reducing the size of the feature
map to $1\times1\times8$, which is fed into the last fully connected layer with
depth one.
The last layer can be seen as a single perceptron responsible for yielding
the final rating within the interval [0,1]. The size of the filter in combination
with the pooling size is a trade-off between the information the CNN can hold
and its computational costs. Many small convolution layers would increase the
processing time of the net; in contrast, higher pooling would reduce the
information held by the CNN.


We have evaluated this architecture for different amounts of filters in the
convolutional layers as well as varying the quantity of perceptrons in the fully
connected layer; these values are reported in \secref{sec:eval}.
The main idea behind the selected architecture is that the convolutional layer
learns basic features, such as edges, approximating the pupil structure.
The average pooling layer makes the CNN robust to small translations and
blurring of these features (e.g., due to the initial downscaling of the input
image). The second convolution layer incorporates deeper knowledge on how to combine the
learned features for the coarse detection of the pupil position. The final
perceptron learns a weighting to produce the final rating.

\subsubsection{Fine positioning CNN (\cfin and $F_{SK_8P_8}$)}
\label{subsec:finestage}
Although the first stage yields an accurate pupil position estimate, it lacks
precision due to the inherent error introduced by the downscaling step.
Therefore, it is necessary to refine this estimate.
This refinement could be attempted by applying methods similar to those
described in \secref{sec:related} to a small window around the coarse pupil
position estimate. However, since most of the previously mentioned challenges are not alleviated by
using this small window, we chose to use a second CNN that evaluates subregions
surrounding the coarse estimate in the original image.

The second stage CNN employs the same architecture pattern as the first stage
(i.e., convolution $\Rightarrow$ average pooling $\Rightarrow$ convolution $\Rightarrow$ fully connected) since their motivations are analogous.
Nevertheless, this CNN operates on a larger input resolution to increase
accuracy and precision.
Intuitively, the input image for this CNN would be $96\times96$ pixels: the input
size of the first CNN input ($24\times24$) multiplied by the downscaling factor
($4$).
However, the resulting memory requirement for this size was larger than
available on our test device; as a result, we utilized the closest working size
possible: $89\times89$ pixels.
The size of the other layers were adapted accordingly.
The convolution filters in the first layer were enlarged to $20\time20$ pixels to
compensate for increased noise and motion blur.
The dimension of the pooling window was increased by one pixel,
leading to a decreased input size on the second convolution layer and reduced runtime.

This CNN uses eight convolution filters in the first stage and eight perceptron weights due to the
increased size of the convolution filter and the input region size.
Subregions surrounding the coarse pupil position are extracted based on a window
of size $89\times89$ pixels centered around the coarse estimate, which is
shifted in a radius of $10$ pixels (with a one pixel stride) horizontally and vertically.
Analogously to the first stage, the center of the region with the highest CNN
rating is selected as fine pupil position estimate.
Despite higher computational costs in the second stage, our approach is highly
efficient and can be run on today's conventional mobile eye-tracking systems.


\subsubsection{Direct coarse to fine positioning CNN (\csin{})}
\label{subsec:trainingdirect}

Unfortunately, the fine positioning CNN requires computational capabilities that
are not always found in state-of-the-art embedded systems.
To address this issue, we have developed one additional fine positioning method
for this evaluation that employs a CNN similar to the ones used in the coarse
positioning stage.
However, this CNN uses an input size of $25\times25$ pixels to obtain an even
center.
As a consequence, the first convolution layer was increased to $6 \times 6$ filters.
This method is used as an inexpensive single stage approach (\csin{}) as well as
in combination with the fine positioning CNN ($F_{SK_8P_8}$).

\subsection{CNN training methodology}
\label{subsec:training}
Both CNNs were trained using supervised batch gradient
descent~\citet{lecun2012efficient} with a dynamic learning rate from $10^{-1}$ to $10^{-6}$. The learning rate was dropped after each ten epochs by $10^{-1}$. In the first round we trained for $50$ epochs and selected the best performing CNN on the validation set. This was repeated four times and in each new round the starting learning rate was decreased by a factor of $10^{-1}$. After the last round we did fine tuning by inspecting each iteration additionally. For each round we generated a new training set. The batch size for one iteration was $100$ and all CNNs' weights were initialized using a Gaussian with standard deviation of $0.01$.
While stochastic gradient descent searches for minima in the error plane more
effectively than batch learning when given valid
examples~\citet{heskes1993line,orr1995dynamics}, it is vulnerable to disastrous
hops if given inadequate examples (e.g., due to poor performance of the
traditional algorithm). On the contrary, batch training dilutes this error which is why we have opted for this method.

\subsubsection{Coarse positioning CNN (\ckp{8}{8}, \ckp{8}{16}, \ckp{16}{32})}
\begin{figure}[h]
	\begin{center}
		\includegraphics[width=0.8\textwidth]{traindata.png}
	\end{center}
	\caption{
		Nine valid (top right) and 32 invalid (bottom) training samples for the coarse
		position CNN extracted from a downscaled input image (top left).
	}
	\label{fig:traindata}
\end{figure}
The coarse position CNN was trained on subregions extracted from the downscaled
input images that fall into two different data classes: containing a valid
($label=1$) or invalid ($label=0$) pupil center.  Training subregions were
extracted by collecting all subregions with center distant up to twelve pixels
from the hand-labeled pupil center. In the first round of training we only used
half of the distance to reduce the amount of invalid examples.  Subregions with
center distant up to one pixel were labeled as valid examples while the
remaining subregions were labeled as invalid examples. As exemplified
by~\figref{fig:traindata}, this procedure results in an unbalanced set of valid
and invalid examples therefore we only used samples on the diagonal (top left to
bottom right) where every second was discarded for the invalid samples. This
reduces the amount of samples per frame. Due to the huge size difference of the
data sets we reduced the amount of samples per set to 20,000 for the first round
and 40,000 for the others. Therefore we picked randomly two thousand images per
data set, created the samples and dropped the overflow. If the data set was to
small we copied the samples to reach the 20,000 or 40,000.


\subsubsection{Fine positioning CNN (\cfin and $F_{SK_8P_8}$)}
The fine positioning CNN (responsible for detecting the exact pupil position) is
trained similarly to the coarse positioning one.
However, we extract only valid subregion up to a distance of three pixels from the hand-labeled
pupil center and selected samples up to a distance of twenty four pixels with a step size of three.  Afterwards the valid examples where again copied to balance the amount of valid and invalid examples. This reduced amount of samples per hand-labeled data ad is to constrain learning time, as well as main memory and storage consumption.

\subsubsection{Direct coarse to fine positioning CNN (\csin{})}
\label{subsec:trainingdirect}

For these CNNs, training and evaluation were performed in an analogous fashion
to the previous ones, with the exception that training samples were generated
from both diagonals (top left to bottom right and top right to bottom left).

\subsection{Fast fine accuracy improvement}
\label{sec:ffai}
The main idea here is to use the response of the CNN surrounding the maximum value to refine the pupil center estimation.
\begin{figure}[h]
	\begin{center}
		\includegraphics[width=0.7\textwidth]{ACCweight.png}
	\end{center}
	\caption{
		The work flow of the accuracy improvement. On the top left the input image is shown, on the top right is the output of the CNN. For accuracy improvement the surrounding area of the maximum position is converted to a distribution and a shift vector is computed. This distribution is shown in the green box on the bottom right. On the bottom left the maximum position (red dot) and the shifted position (green dot) are shown.
	}
	\label{fig:accimp}
\end{figure}
For a fast accuracy improvement of all CNNs the response surrounding the maximum position is converted into a probability distribution. Such a response of a CNN is shown in figure~\ref{fig:accimp} on the top left. The converted area is surrounded by a green square. In our implementation we used an $7 \times 7$ ($N \times M$) square centered at the maximum position. The resulting distribution is shown in Figure~\ref{fig:accimp} on the bottom right. To convert the response into a distribution each value is divided by the sum of all values in the square (equation~(\ref{eq:distriacc})).
\begin{equation}
D(x,y)=\frac{R(x,y)}{\sum_{i=0}^{N}\sum_{j=0}^{M}R(i,j)}
\label{eq:distriacc}
\end{equation}\\
In equation~(\ref{eq:distriacc}) $D(x,y)$ is the distribution value at location $x,y$ and $R(x,y)$ is the CNN response at location $x,y$. Each value in this distribution is weighted by the displacement vector to the maximum position. The calculation is shown in equation~(\ref{eq:forcevec}).
\begin{equation}
\overrightarrow{SV}=\sum_{i=-\frac{N}{2}}^{\frac{N}{2}}\sum_{j=-\frac{M}{2}}^{\frac{M}{2}}D(\frac{N}{2}+i,\frac{M}{2}+j)*\Spvek{i;j}
\label{eq:forcevec}
\end{equation}\\
In equation~(\ref{eq:forcevec}) $\overrightarrow{SV}$ is the vector shifting the initial maximum position (red dot in figure~\ref{fig:accimp} on the bottom left) to the new more accurate position (green dot in figure~\ref{fig:accimp} on the bottom left). $D(i,j)$ is the result of equation~(\ref{eq:distriacc}) at location $i,j$ and $\Spvek{i;j}$ is the displacement vector to the center.






