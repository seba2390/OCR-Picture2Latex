With a CNN natural image classifier $M$, an input image $I$, and a target class $c$, our goal is to provide saliency maps to highlight which image portions lead to the model's prediction of $c$ from a causal perspective.
First, we recall the definition of \textit{causal mechanism} as follows.

\noindent\textbf{Causal mechanism~\cite{falleti2009context}.}
A portable concept explaining how and why the hypothesized causes, in a given context $u$, contribute to a particular outcome. 

Next, we define the outcome and causes in our problem, then demonstrate how we establish the causal mechanism and formulate our problem based on it.

\noindent\textbf{Outcome: }
As a class-discriminative explanation, we directly measure the model's prediction probability w.r.t.\;the target class and set it as the outcome.

\noindent\textbf{Causes: }
Given that existing CNN causal explanations regard input features or model filters as causes, we allow the flexibility of setting either of them as causes.
Specifically, a single cause is (1) a region of the image identified by a feature map; or (2) a model filter in a convolutional layer.

When considering input features as causes, the context $u$ is an unknown process determining the combination of image features.
Regarding model filters as causes, the context $u$ is the image region on which we apply the model filters.
After establishing the causal mechanism, we can quantify the effect of causes by counterfactually intervening on the hypothesized causes and measuring the outcome, whose results can be used as weights for feature maps' linear combination.
Finally, we can produce saliency maps as a causal explanation for the CNN classifier.
