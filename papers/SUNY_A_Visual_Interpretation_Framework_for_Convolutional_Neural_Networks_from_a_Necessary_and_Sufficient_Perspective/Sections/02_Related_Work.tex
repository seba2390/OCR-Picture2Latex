\noindent\textbf{Saliency Map Explanations for CNNs.}
The class-discri-minative visual explanation is a key tool for CNN interpretation by highlighting important regions corresponding to a model's particular prediction.
Two widely-adopted types are perturbation-based methods~\cite{zeiler2014visualizing,ribeiro2016should,petsiuk2018rise} and CAMs~\cite{zhou2016learning,selvaraju2017grad,chattopadhay2018grad,smilkov2017smoothgrad,wang2019score,zhang2021group}.
Perturbation-based methods such as RISE~\cite{petsiuk2018rise} are inspired by a human-understandable concept of measuring the output when the inputs are perturbed, which is in line with the \textit{N} causal semantics.
Somehow enumerating and examining all possible input perturbations is computationally challenging~\cite{ivanovs2021perturbation}, making such methods infeasible for applications demanding fast explanations.

CAMs compute a weighted sum of feature maps to generate explanations, which are often more computationally efficient.
CAM~\cite{zhou2016learning} is the pioneering method in this type, which requires a global average pooling layer in the CNN architecture.
GradCAM~\cite{selvaraju2017grad} and a series of its variations, such as Grad-CAM++~\cite{chattopadhay2018grad} and SmoothGrad~\cite{smilkov2017smoothgrad}, address this structural restriction by using gradients as weights, hence applicable to a broader variety of CNN families.
However, gradients knowingly suffer from vanishing issues and introduce noise to the saliency map~\cite{ghorbani2019interpretation}.
To fix the gradient issues, Score-CAM~\cite{wang2019score} and Group-CAM~\cite{zhang2021group} weight feature maps by their corresponding input features' contribution to the output, which matches the \textit{S} perspective of causal semantics and is implicitly consistent with the ideas of perturbation-based methods. 

\noindent\textbf{Causal Importance Evaluations.}
\label{subsec:current_res}
Based on the HP causality~\cite{pearl2009causality, halpern2020causes} introduced by Halpern and Pearl, recent research for causal importance quantification involves two main genres, \textit{causal effect} and \textit{responsibility}.
The average \textit{causal effect}~\cite{glymour2016causal} measures the expected outcome difference when the values of selected causes are changed, which regards the set of causes as a whole.
An individual cause can thus affect the outcome very differently, i.e., receive completely different effect quantification, when being analyzed with different sets of interacting causes~\cite{yao2021survey}.
The second genre, the degree of \textit{responsibility}~\cite{chockler2004responsibility}, quantifies the minimal set of causes whose removal leads to a satisfactory outcome change, which overlooks the actual value of the outcome~\cite{lagnado2013causal,baier2021verification}.
Besides, current \textit{causal effect} and \textit{responsibility} both focus on the \textit{N} perspective, i.e., the change of causes, and the other \textit{S} perspective needs to be fulfilled for a thorough explanation~\cite{lipton1990contrastive,woodward2006sensitive,grinfeld2020causal,watson2021local}.
More recent works such as \cite{watson2021local} and \cite{kommiya2021towards} address this deficiency by measuring the causal importance from both \textit{N} and \textit{S}.
However, their proposed quantification focuses on the probability of the outcome change when causes are changed (\textit{N}) or unchanged (\textit{S}), which also misses the actual outcome value.

\noindent\textbf{Causal Explanations for CNNs.}
Given recent works on CNN causal explanations, two main groups exist according to the type of the hypothesized causes -- the model's input features or inner filters.
As for the first group, \cite{chattopadhyay2019neural} and \cite{harradon2018causal} estimate the average \textit{causal effect} of input features.
The Shapley value-inspired methods~\cite{vstrumbelj2014explaining,datta2016algorithmic,lundberg2017unified,sundararajan2020many,aas2021explaining}  also implicitly analyze the causality of input features from the \textit{S} perspective. However, these methods are too computationally intensive or only provide a hazy outline of salient regions.
Two CAM-based methods, Score-CAM~\cite{wang2019score} and Group-CAM~\cite{zhang2021group}, as discussed before, can be regarded as partially causal-grounded by analyzing the \textit{sufficiency} of input features.
On the other side, the perturbation-based methods ~\cite{zeiler2014visualizing,ribeiro2016should,petsiuk2018rise} analyze input features' \textit{necessity}.

The second group of methods interprets CNN through the model filter intervention.
For example, \cite{narendra2018explaining} explores CNN's inner workings by ranking filters from a layer according to their counterfactual (CF) influence.
However, without any visualizations, this causal explanation of CNN is less intuitive than saliency maps.
CexCNN~\cite{debbi2021causal} provides visualizations with the help of CAMs and uses \textit{responsibility} to quantify the CNN's filter importance, which means only the \textit{N} aspect has been analyzed.
