\subsection{Experimental Setup}
\begin{figure*}[htbp]
  \centering
   \includegraphics[width=1\linewidth]{Figures/CausalSemanticV4.png}
\caption{
Semantic evaluation of \name{} explanations for a VGG16 trained on CUB for bird species classification.
The bird images in the first row are from four bird species belonging to two families and the correct/incorrect predictions are marked by~\textcolor{LimeGreen}{\faCheck} and~\textcolor{red}{\faTimes}, respectively.
For the two images marked by~\textcolor{red}{\faTimes}, the model mistakes the actual species with the other species under the same family.
Each column corresponds to one image; the second and third rows: \textit{sufficiency} and \textit{necessity} heatmaps.
The small image in the bottom corner of each heatmap presents the highlighted image portion.}
\label{fig:Bird Causal}
\end{figure*}

\noindent \textbf{Baseline Methods.}
We compare \name{} with seven popular class-discriminative visual explanation methods proposed between 2017 and 2021: Grad-CAM~\cite{selvaraju2017grad}, Grad-CAM++~\cite{chattopadhay2018grad}, SmoothGrad~\cite{smilkov2017smoothgrad}, RISE~\cite{petsiuk2018rise}, Score-CAM~\cite{wang2019score}, CexCNN~\cite{debbi2021causal}, and Group-CAM~\cite{zhang2021group}.

\noindent \textbf{Datasets and CNN Models.}
The experiments involve two commonly-used public image datasets, ILSVRC2012 (ILSVRC)~\cite{ILSVRC15} with $1000$ classes and $50k$ images in the validation set and CUB-200-2011 (CUB)~\cite{welinder2010caltech} with $200$ classes and $5794$ images in the validation set.
We use the baseline methods and \name{} to explain two CNN models with different architectures, including VGG16~\cite{simonyan2014very} and Inception-v3~\cite{szegedy2016rethinking}.
For ILSVRC, trained models are from TorchVision~\cite{marcel2010torchvision}.
For CUB, we apply transfer-learning with a training batch size of $128$ and standard data augmentations, including the random resized crop and random horizontal flip.

\noindent \textbf{Implementation Details.}
We implement \name{} and the seven aforementioned visual explanation methods in Python using PyTorch framework~\cite{paszke2017automatic}.
Specifically, we run the official or publicly available code of other methods as the results at the same data scale are unavailable.
The platform is equipped with two NVIDIA RTX 3090 GPUs. 
Unless explicitly stated, the comparisons discussed in this section are conducted following the same settings, including --

\noindent(1) Images are converted to the RGB format, resized to $224\times224$ (VGG16) or $299\times299$ (Inception-v3), transformed to tensors, and normalized to the range of $[0,1]$.

\noindent(2) All visual explanation methods' results are generated for both models in the validation sets of both datasets, where we use all images predicted correctly by the models.

\noindent (3) Explanations are applied to the last convolutional layer for explaining the predicted class. We bilinearly interpolate the results to the required size of each experiment.

\subsection{Semantic Evaluation}

In Fig.~\ref{fig:Compare all}, we visually compare \name{} with other saliency map explanations and observe two advantages: 
(1) Saliency maps provided by \name{} contain fewer noises.
(2) \name{} uniquely provides both \textit{necessary} and \textit{sufficient} information to support interpretation. For example, for the first image, \name{} tells that the bottom wing is \textit{necessary} and the head is \textit{sufficient} for {\fontfamily{qcr}\selectfont Gull} prediction. For the second image, \name{} shows that the sled is \textit{necessary} and the dog is \textit{sufficient} for the prediction of {\fontfamily{qcr}\selectfont Dog-sled}.
However, other explanations overlook one side of \textit{necessity} or \textit{sufficiency}, and none provide such distinction.
More examples can be found in the Supplementary Materials.

To demonstrate how \textit{necessity} and \textit{sufficiency} support interpretation, we use \name{}-feature to explain a VGG16 trained on CUB for bird species classification.
As shown in Fig.~\ref{fig:Bird Causal}, we provide images from CUB corresponding to four bird species in two families\footnote{Family is a higher-level taxonomic category than species~\cite{enwiki:1116635228}}.
By inspecting the \textit{sufficiency} heatmaps in the second row, we can observe that the highlighted regions are similar for those belonging to the same bird family. -- All {\fontfamily{qcr}\selectfont kingfisher} images share similar heads and belted necks; all {\fontfamily{qcr}\selectfont red tanager} images share similar heads.
This explains why the model correctly identifies the bird family for every image.
From the third row, we find the \textit{necessity} heatmaps provide further explanations through the following observations.
For the {\fontfamily{qcr}\selectfont kingfisher} family, the belted bellies are highlighted in images predicted as {\fontfamily{qcr}\selectfont belted kingfisher}, while the red bellies are highlighted in the images predicted as {\fontfamily{qcr}\selectfont ringed kingfisher}.
This explains why the third image is mistaken -- the red belly is easily observable through this view and is identical to a {\fontfamily{qcr}\selectfont ringed kingfisher}.
For the {\fontfamily{qcr}\selectfont red tanager} family, the black wings are highlighted in images predicted as {\fontfamily{qcr}\selectfont Scarlet tanager}, while the red wings or heads are highlighted in the images predicted as {\fontfamily{qcr}\selectfont summer tanager}. For the {\fontfamily{qcr}\selectfont Scarlet tanager} image being misclassified, the black wing feature is not observable from this view, and the head and red body are captured by the model, which are characteristics belonging to the other species.
Therefore, Fig.~\ref{fig:Bird Causal} shows that \textit{sufficiency} and \textit{necessity} provides semantically-complementary explanations to better support model behavior interpretation.
More examples are provided in the Supplementary Materials.

\begin{table*}[ht]
\centering
\scalebox{0.78}{
% \resizebox{\textwidth}{!}{
\begin{tabular}{p{2cm}<{\centering}| p{3.5cm}<{\centering}| p{1.8cm}<{\centering} p{1.8cm}<{\centering} p{1.8cm}<{\centering}| p{1.8cm}<{\centering} p{1.8cm}<{\centering} p{1.8cm}<{\centering}}
\toprule[0.8pt]
                          &                           & \multicolumn{3}{c|}{VGG 16}    & \multicolumn{3}{c}{Inception\_v3} \\
\multirow{-2}{*}{Dataset} & \multirow{-2}{*}{Methods} & Deletion $\downarrow$ & Insertion $\uparrow$ & Overall $\uparrow$ & Deletion $\downarrow$ & Insertion $\uparrow$  & Overall $\uparrow$  \\ \hline
                          &Grad-CAM\cite{selvaraju2017grad}                 &  0.1098          & 0.6112          &  0.5015        & 0.1276          &0.6567           & 0.5291         \\
                          &Grad-CAM++\cite{chattopadhay2018grad}               & 0.1155         & 0.6033           & 0.4878        & 0.1309         & 0.6476          & 0.5167           \\
                          & SmoothGrad \cite{smilkov2017smoothgrad}              & 0.1136         &0.6023           &0.4887        & 0.1317         &  0.6465         & 0.5148          \\
                          & RISE \cite{petsiuk2018rise}                       &0.1185           & 0.6188        & 0.5003         & 0.1404          &0.6444     &  0.5040          \\
                          & Score-CAM \cite{wang2019score}              
                          & 0.1070         &  0.6382         & 0.5312        
                          & 0.1309         & 0.6528          & 0.5219           \\
                          & CexCNN \cite{debbi2021causal}              
                          & 0.1161         & 0.6025          &0.4864         
                          & 0.1355         & 0.6543          & 0.5188           \\
                          & Group-CAM \cite{zhang2021group}              
                          &0.1138          & 0.6218          & 0.5080        
                          &0.1292          & 0.6545          & 0.5253          \\
 &
  \cellcolor[HTML]{EFEFEF}\name{}-filter (Ours) & 
  \cellcolor[HTML]{EFEFEF}{\textcolor{blue}{0.1019}} &
  \cellcolor[HTML]{EFEFEF}{\textcolor{blue}{0.6389}} &
  \cellcolor[HTML]{EFEFEF}{\textcolor{blue}{0.5370}} &
  \cellcolor[HTML]{EFEFEF}{\textcolor{blue}{0.1258}}  &
  \cellcolor[HTML]{EFEFEF}{\textcolor{blue}{0.6570}}  &
  \cellcolor[HTML]{EFEFEF}{\textcolor{blue}{0.5312}} \\
\multirow{-9}{*}{ILSVRC} &
  \cellcolor[HTML]{EFEFEF}\name{}-feature (Ours) &
  \cellcolor[HTML]{EFEFEF}{\textcolor{Green}{0.1005}} &
  \cellcolor[HTML]{EFEFEF}{\textcolor{Green}{0.6468}} &
  \cellcolor[HTML]{EFEFEF}{\textcolor{Green}{0.5462}} &
  \cellcolor[HTML]{EFEFEF}{\textcolor{Green}{0.1215}} &
  \cellcolor[HTML]{EFEFEF}{\textcolor{Green}{0.6603}} &
  \cellcolor[HTML]{EFEFEF}{\textcolor{Green}{0.5388}} \\ \hline
                          & Grad-CAM\cite{selvaraju2017grad}                 &  0.0558        &  \textcolor{Green}{0.7617}         &  \textcolor{blue}{0.7059}       & 0.0963         & 0.7323          & 0.6360           \\
                          & Grad-CAM++\cite{chattopadhay2018grad}               &  0.0589        & 0.7541           & 0.6951        & 0.0950         &  0.7281          & 0.6331          \\
                          & SmoothGrad \cite{smilkov2017smoothgrad}              & 0.0594         & 0.7489          & 0.6895         & 0.0977         & 0.7244          & 0.6266          \\
                          & RISE \cite{petsiuk2018rise}              &0.0560          & 0.7583          &  0.7023       & 0.0855         & 0.7168          & 0.6314         \\
                          & Score-CAM\cite{wang2019score}               &  \textcolor{blue}{0.0542}        & 0.7575          &   0.7033      &0.0901          & 0.7326          &  0.6424          \\
                        & CexCNN \cite{debbi2021causal}              & 0.0630         & 0.7389          & 0.6760        & 0.1017         & 0.7283          &  0.6267          \\
                          & Group-CAM \cite{zhang2021group}              & 0.0606         & 0.7521          & 0.6915        & 0.0971         & 0.7290          & 0.6318         \\
 &
  \cellcolor[HTML]{EFEFEF}\name{}-filter (Ours) &
  \cellcolor[HTML]{EFEFEF} 0.0544&
  \cellcolor[HTML]{EFEFEF} 0.7556&
  \cellcolor[HTML]{EFEFEF} 0.7012&
  \cellcolor[HTML]{EFEFEF} {\textcolor{blue}{0.0853}}&
  \cellcolor[HTML]{EFEFEF} {\textcolor{blue}{0.7333}}&
  \cellcolor[HTML]{EFEFEF} {\textcolor{blue}{0.6480}}\\
\multirow{-9}{*}{CUB} &
  \cellcolor[HTML]{EFEFEF}\name{}-feature (Ours) &
  \cellcolor[HTML]{EFEFEF}{\textcolor{Green}{0.0518}} &
  \cellcolor[HTML]{EFEFEF}{\textcolor{blue}{0.7591}} &
  \cellcolor[HTML]{EFEFEF}{\textcolor{Green}{0.7073}} &
  \cellcolor[HTML]{EFEFEF}{\textcolor{Green}{0.0842}}&
  \cellcolor[HTML]{EFEFEF}{\textcolor{Green}{0.7361}} &
  \cellcolor[HTML]{EFEFEF}{\textcolor{Green}{0.6519}} \\ \bottomrule[0.8pt]
\end{tabular}}
\caption{Comparative evaluation w.r.t.\;the \textit{deletion}, \textit{insertion}, and \textit{overall} AUC on the validation sets of ILSVRC and CUB with VGG16 and Inception\_v3,
with lower \textit{deletion}, higher \textit{insertion}, and higher \textit{overall} indicative of a better explanation. 
The \textcolor{Green} {first} and \textcolor{blue} {second} best performances are marked in \textcolor{Green} {green} and \textcolor{blue} {blue}, respectively.}
\label{tab:AUC}
\end{table*}

\subsection{Necessity and Sufficiency Evaluation}
\label{subsec:NS_eval}

\noindent \textbf{Deletion and Insertion.}
% \subsubsection{Deletion and Insertion}
To evaluate the saliency regions' effects on the model's prediction, we conduct the \textit{deletion and insertion} experiments proposed in~\cite{petsiuk2018rise}.
According to the saliency map, the \textit{deletion} metric quantify the model's predicted probability upon the removal of more and more important image pixels, while the complementary \textit{insertion} metric measure the same variable as the image pixels adds back from the most important to unimportant ones.
Consistent with~\cite{petsiuk2018rise, zhang2021group}, we utilize the Area Under the probability Curve (AUC) to quantify the results, with lower \textit{deletion}, higher \textit{insertion}, and higher \textit{overall(insertion-deletion)} indicative of a better explanation.
We follow the same experimental settings as~\cite{zhang2021group}, including the blurring method and the step size to remove/add pixels. 
The comparison results are presented in Table~\ref{tab:AUC},
where we can observe that except for VGG16 on CUB, both \name{}-feature and \name{}-filter outperform other approaches in terms of \textit{deletion}, \textit{insertion} and \textit{overall} AUC scores. 
In the case of VGG16 on CUB, \name{}-feature still outperforms most of the other explanations, and \name{}-filter achieves competitive results.

\noindent \textbf{N-S Quantification.}
We introduce the \textit{N-S Quantification} metric to quantify the average \textit{N} and \textit{S} degree of a visual explanation method.
The \textit{N-S scores} correspond to one input image $I$ are given by $N_{score}=\frac{p_{c}(I)-p_{c}(I\bigodot(1 - map))}{p_{c}(I)\times Size_{map}}$, $S_{score}=\frac{p_{c}(I\bigodot(map))}{p_{c}(I)\times Size_{map}}$, respectively,
where $map$ is the saliency map provided by the explanation method, $p_{c}(\bigcdot)$ is the model's prediction probability w.r.t.\;the target class $c$, and $Size_{map}=\frac{\sum_{i, j}(1_{true}map[i, j] \neq 0)}{map.h\times map.w}$.
\begin{figure}[bhtp]
  \centering
   \includegraphics[width=1\linewidth]{Figures/NSCUB.png}
   \caption{Comparison of \name{} with seven visual explanation methods in terms of the \textit{N-S Quantification} metric.}
   \label{fig:NSCUB}
\end{figure}
As the highlighted image regions being removed or kept, $N_{score}$ and $S_{score}$ measure how \textit{necessary} or \textit{sufficient} these regions are for the model's decision, with larger scores indicative of higher \textit{N/S}.
For every visual explanation method, we conduct the \textit{N-S Quantification} with VGG16 on the correct-predicted images in the CUB validation set, and calculate the average $N_{score}$ and $S_{score}$ as its \textit{necessary} and \textit{sufficient} level.
The result is visualized in Fig.~\ref{fig:NSCUB}, which highlights that \name{} outperforms most of the seven methods in the ability to identify the most \textit{necessary} and \textit{sufficient} image regions corresponding to the model's class prediction. 



\subsection{Saliency Attack}
Researchers have proposed a series of local adversarial attack approaches~\cite{dong2020robust, wang2022attention, xiang2021local, dai2022saliency} guided by saliency maps such as CAM~\cite{dong2020robust,wang2022attention}, Grad-CAM~\cite{xiang2021local}, and others~\cite{dai2022saliency}, which is to fool CNN models by perturbing a small image region highlighted by saliency maps.
These methods require the saliency maps to be ``minimal and essential''~\cite{dai2022saliency}, i.e., be capable of capturing image portions that most significantly impact the model output.
Inspired by current saliency attack approaches, we propose an evaluation metric, $Attack_{score}=\frac{FlipRate}{AvgAttackSize}$, to validate whether \name{} explanations can detect the most important regions w.r.t.\;the model's decision.

Specifically, we add random Gaussian noise to the saliency regions $I'=(I + noise \bigodot map)$, and then check whether this operation can change the model's decision, i.e., $Flip = 1$ if ${argmax(p(I))}\neq{argmax(p(I'))}$, where $p(\bigcdot)$ is the model's prediction probability.
The $FlipRate$ represents the ``essential'' level of the saliency regions.
To validate whether the region is ``minimal'', we include $AvgAttackSize$, which is the average size of all saliency maps. 
For a single saliency map, the calculation of $Size_{map}$ is the same as shown in Sec.~\ref{subsec:NS_eval}.
Finally, we can obtain the $Attack_{score}$ and the comparison is reported in Table~\ref{tab:Attack}, which proves that bi-directional explanations provided by \name{} are better at highlighting the most important image region corresponding to the model's decision.

\begin{table}[ht]
\centering
\scalebox{0.78}{
% \resizebox{\columnwidth}{!}{
\begin{tabular}{p{2cm}<{\centering}|p{3cm}<{\centering}|p{1.8cm}<{\centering} p{1.8cm}<{\centering}}
\toprule[0.8pt]
                          &                                            & \multicolumn{2}{c}{$Attack_{score}$ $\uparrow$}                          \\
\multirow{-2}{*}{Dataset} & \multirow{-2}{*}{Methods}                  & VGG16                    & Inception\_v3            \\ \hline
                           & Grad-CAM\cite{selvaraju2017grad}                  & 0.9615        &   1.0435              \\
                          & Grad-CAM++\cite{chattopadhay2018grad}                & 0.9991        & 0.9821              \\
                          & SmoothGrad\cite{smilkov2017smoothgrad}               &  1.0449       & 0.9675               \\
                          & RISE\cite{petsiuk2018rise}               
                          & 0.9928        &  0.7353              \\
                          & Score-CAM\cite{wang2019score}               & 0.5326        & 0.9673               \\
                          & CexCNN\cite{debbi2021causal}                       & 1.6341         & 1.0653     \\
                          & Group-CAM\cite{zhang2021group}                      & 1.1556          &  1.0200   \\
                          & \cellcolor[HTML]{EFEFEF}\name{}-filter (Ours) & \cellcolor[HTML]{EFEFEF}{\textcolor{blue}{1.6602}} & \cellcolor[HTML]{EFEFEF}{\textcolor{blue}{1.5725}}  \\
\multirow{-9}{*}{ILSVRC} & \cellcolor[HTML]{EFEFEF}\name{}-feature (Ours) & \cellcolor[HTML]{EFEFEF}{\textcolor{Green}{2.0452}} & \cellcolor[HTML]{EFEFEF}{\textcolor{Green}{1.9874}} \\ \hline
                          & GradCam\cite{selvaraju2017grad}                 & 0.5969        & 0.5985               \\
                          & GradCam++\cite{chattopadhay2018grad}                & 0.6670        &  0.5950              \\
                          & SmoothGrad\cite{smilkov2017smoothgrad}                & 0.7783        & 0.5929               \\
                          & RISE \cite{petsiuk2018rise}              
                          & 0.5063        & 0.3860               \\
                          & Score-CAM\cite{wang2019score}               
                          & 1.2215        &  0.5989              \\
                          & CexCNN\cite{debbi2021causal}               
                          &  1.2673       & 0.5898               \\
                          & Group-CAM\cite{zhang2021group}                
                          & 0.6742        & 0.5951               \\
                          & \cellcolor[HTML]{EFEFEF}\name{}-filter (Ours) & \cellcolor[HTML]{EFEFEF}{\textcolor{blue}{1.3691}} & \cellcolor[HTML]{EFEFEF}{\textcolor{blue}{0.8867}} \\
\multirow{-9}{*}{CUB}    & \cellcolor[HTML]{EFEFEF}\name{}-feature (Ours) & \cellcolor[HTML]{EFEFEF}{\textcolor{Green}{2.8111}} & \cellcolor[HTML]{EFEFEF}{\textcolor{Green}{1.0475}}  \\ \bottomrule[0.8pt]
\end{tabular}
}
\caption{
Comparative evaluation w.r.t.\;the saliency attack score (higher is better) on the validation sets of ILSVRC and CUB with VGG16 and Inception\_v3. The \textcolor{Green} {first} and \textcolor{blue} {second} best performances are marked in \textcolor{Green} {green} and \textcolor{blue} {blue}, respectively.
}
\label{tab:Attack}
\end{table}

\begin{table}[ht]
\centering
\scalebox{0.78}{
% \resizebox{\columnwidth}{!}{
\begin{tabular}{p{2cm}<{\centering}|p{3cm}<{\centering}|p{1.8cm}<{\centering} p{1.8cm}<{\centering}}
\toprule[0.8pt]
                          &                                            & \multicolumn{2}{c}{Proption ($\%$) $\uparrow$}                          \\
\multirow{-2}{*}{Dataset} & \multirow{-2}{*}{Methods}                  & VGG16                    & Inception\_v3            \\ \hline
                           &Grad-CAM\cite{selvaraju2017grad}                  &  57.68  &  66.35              \\
                          & Grad-CAM++\cite{chattopadhay2018grad}               &  61.31       &  65.93             \\
                          & SmoothGrad\cite{smilkov2017smoothgrad}               &  62.18       &  65.78              \\
                          & RISE\cite{petsiuk2018rise}              
                          & 58.93        &  59.26              \\
                          & Score-CAM\cite{wang2019score}               &  64.25      &  65.94              \\
                          & CexCNN\cite{debbi2021causal}               
                          &  65.24        & 66.33         \\
                          & Group-CAM\cite{zhang2021group}              
                          & 62.70         & 66.17         \\
                          & \cellcolor[HTML]{EFEFEF}\name{}-filter (Ours) & \cellcolor[HTML]{EFEFEF}{\textcolor{Green}{65.63}} & \cellcolor[HTML]{EFEFEF}{\textcolor{Green}{67.02}}  \\
\multirow{-9}{*}{ILSVRC} & \cellcolor[HTML]{EFEFEF}\name{}-feature (Ours) & \cellcolor[HTML]{EFEFEF}{\textcolor{blue}{65.61}} & \cellcolor[HTML]{EFEFEF}{\textcolor{blue}{66.71}} \\ \hline
                          & Grad-CAM\cite{selvaraju2017grad}                 & 43.06        & 40.05               \\
                          & Grad-CAM++\cite{chattopadhay2018grad}                & 45.45        & 40.45               \\
                          & SmoothGrad\cite{smilkov2017smoothgrad}               & 47.12        & 40.34               \\
                          & RISE\cite{petsiuk2018rise}                       & 37.28           & 34.74   \\
                          & Score-CAM\cite{wang2019score}               
                          &  49.68       &  40.67             \\
                          & CexCNN\cite{debbi2021causal}               
                          &  37.13        &   41.38      \\
                          & Group-CAM\cite{zhang2021group}              
                          & 43.53         &  41.08        \\
                          & \cellcolor[HTML]{EFEFEF}\name{}-filter (Ours) & \cellcolor[HTML]{EFEFEF}{\textcolor{Green}{51.74}} & \cellcolor[HTML]{EFEFEF}{\textcolor{Green}{42.03}} \\
\multirow{-9}{*}{CUB}    & \cellcolor[HTML]{EFEFEF}\name{}-feature (Ours) & \cellcolor[HTML]{EFEFEF}{\textcolor{blue}{49.97}} & \cellcolor[HTML]{EFEFEF}{\textcolor{blue}{41.96}}  \\ \bottomrule[0.8pt]
\end{tabular}
}
\caption{
Comparative evaluation w.r.t.\;the proportion in energy-pointing games (higher is better) on the validation sets of ILSVRC and CUB with VGG16 and Inception\_v3. The \textcolor{Green} {first} and \textcolor{blue} {second} best performances are marked in \textcolor{Green} {green} and \textcolor{blue} {blue}, respectively.
}
\label{tab:Loc}
\end{table}

\subsection{Localization Evaluation}
In this section, we conduct the localization evaluation with the energy-based pointing game~\cite{wang2019score}, a variant of the pointing game~\cite{petsiuk2018rise}.
The goal is to measure the localization ability of saliency maps using the ground-truth bounding box of the target class, $bbox$.
The input image is binarized with $bbox$ by assigning the inside and outside regions with $1$ and $0$, respectively. 
Then, we apply the Hadamard product between the binarized input and the saliency map, the summary of which can quantify how much ``energy'' falls into $bbox$.
The performance is measured by $Proportion=\frac{\sum map[i,j]_{(i,j)\in bbox}}{\sum map[i,j]}$.
For convenience, we consider all correctly-predicted images with single $bbox$ in the validation set of ILSVRC, CUB, respectively, and report the average proportion value for every method in Table~\ref{tab:Loc}. 
The results indicate that both \name{}-filter and \name{}-feature outperform other methods in terms of localization ability by ranking first and second in the energy-based pointing game.



\begin{figure}[htbp]
  \centering
   \includegraphics[width=1\linewidth]{Figures/sanityV2.png}
   \caption{Sanity check of \name{} explanations for VGG16 by cascading randomization with an example image (shown on the top left) from ILSVRC.
   Results of \name{}-filter and \name{}-feature are represented in the first and second row, respectively.
   The first column is the original saliency maps, and the following columns show results after randomizing specific layers.}
   \label{fig:sanity}
\end{figure}


\subsection{Sanity Check}
The sanity check~\cite{adebayo2018sanity} is to validate whether a visual explanation method can be considered as a reliable explanation to reflect the modelâ€™s behavior.
We conduct cascading randomization to the model's weights from the top to the bottom layer successively and generate explanations every time after the randomization.
% The weights of VGG16 are reinitialized randomly from the top layers to the bottom.
If the saliency maps remain similar for the resulting model with widely differing parameters, it means the method fails the sanity check.
Fig.~\ref{fig:sanity} presents the sanity check of \name{}-filter and \name{}-feature, which indicates significant changes for both explanation results after the model parameter randomization.
Therefore, the explanations provided by \name{} pass the sanity check.
