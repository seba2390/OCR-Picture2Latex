In computer vision, an important research direction is to give Convolutional Neural Network (CNN) more transparency to extend its deployment on a broader basis.
This paper addresses the eXplainable Artificial Intelligence (XAI)~\cite{gunning2017explainable} problem corresponding to CNN for natural image classification, i.e., reasoning why a classifier makes particular decisions.
Specifically, we study a leading family of techniques called Class-Activation-Maps (CAMs), which present heatmaps highlighting image portions associated with a model's class prediction.
\begin{figure}[thbp]
  \centering
   \includegraphics[width=1\linewidth]{Figures/IntroCompare.png}
   \caption{An example of \name{} explanations. \name{} highlights sufficient and necessary input regions w.r.t.\;the model's prediction towards the target class. The 2D saliency map is the first-of-its-kind visual explanation design to the best of our knowledge.}
   \label{fig:Intro Compare}
\end{figure}
The pioneering method in the family is CAM~\cite{zhou2016learning}, which produces saliency maps by linearly combining the convolutional feature maps with weights generated by a global average pooling layer.
CAM's structural restriction on the global average pooling layer prohibits its use for general CNNs, and a series of gradient-weighted CAMs~\cite{selvaraju2017grad,chattopadhay2018grad,omeiza2019smooth} have been proposed to overcome the restriction.
However, such CAMs suffer from gradients' saturation and vanishing issues, which cause the resulting explanations to be noisy~\cite{ghorbani2019interpretation}.
To bypass the shortcomings of gradients, Score-CAM~\cite{wang2019score} and Group-CAM~\cite{zhang2021group} weight the feature maps by contribution scores, referring to the corresponding input features' importance to the model output.

The design of Score-CAM and Group-CAM to measure the model's class prediction (outcome) when only keeping specific input features (cause) matches the idea of causal \textit{sufficiency (S)}, while the other aspect, \textit{necessity (N)}, is missing.
\textit{N} refers to changing the hypothetical causes and measuring the outcome differences, and \textit{S} investigates the outcome stability when specific causes are unchanged.
Both are desirable and essential perspectives for a successful explanation~\cite{lipton1990contrastive,woodward2006sensitive,grinfeld2020causal,watson2021local}.
Other CAMs, on the other hand, lack both causal semantics in their design, which makes it challenging to answer the core ``why'' questions and match human instincts.

Producing a saliency map through causal explanations requires calculating the importance of each cause (e.g., a region of input image or a model filter) when interacting with other causes.
Techniques for quantifying the importance of causes consist of two main genres, \textit{causal effect}~\cite{glymour2016causal} and \textit{responsibility}~\cite{chockler2004responsibility}.
\textit{Causal effect} measures the outcome differences when intervening on the causes; \textit{responsibility} measures the minimal set of causes needed to change the outcome.
In \textit{causal effect} analysis, the effect of a group of causes is a single value, which cannot be easily attributed to distinguish the importance of each individual cause.
\textit{Responsibility} also fails to provide a decent importance score for a single cause because it measures the number of causes rather than the actual outcome differences.
That is, these two causal quantification genres cannot be directly taken to produce a sound saliency map.
Furthermore, both \textit{causal effect} and \textit{responsibility} typically focus on $N$ and overlook $S$.
There are a few works addressing CNN causal explanations by adopting either of these two methods, such as CexCNN~\cite{debbi2021causal} and~\cite{harradon2018causal}.
However, the aforementioned limitations have not yet been carefully addressed.
To summarize, current CNN explanations, to some extent, fail in (1) human interpretability (not designed from a causal perspective, like GradCAM), (2) precision (failing to generate decent importance for individual cause, given their basis of \textit{causal effect} or \textit{responsibility}), or (3) completeness (missing either \textit{N} or \textit{S} perspective, like CexCNN).
Therefore, it calls for more research efforts in this demanding field of CNN explanations to fulfill the deficiencies.

Considering the issues mentioned above, we propose an explanation framework called \textbf{SU}fficiency and \textbf{N}ecessit\textbf{Y} causal explanation (\name{}), which interprets the CNN classifier by analyzing how the cooperation of a single cause and other causes affect the model's decision.
\name{} regards either model filters or input features as the hypothesized causes and quantifies each cause's importance towards the class prediction from angles of both \textit{N} and \textit{S}.
Specifically, we draw on the strength of both \textit{causal effect} and \textit{responsibility} to propose \textit{N-S responsibility}, which solves the three issues above by (1) being designed from causality theory that is in line with instinctive human habits; (2) providing importance for each individual cause when interacting with other causes by quantizing the effect on top of responsibility; (3) conducting a bi-directional quantification from both \textit{N} and \textit{S} perspectives.
Based on the qualitative evaluation, including the semantic evaluation and the sanity check, we demonstrate that \name{} provides a more faithful and interpretable visual explanation for CNN models.
Comprehensive experiment evaluations on benchmark datasets, including ILSVRC2012~\cite{ILSVRC15} and CUB-200-2011~\cite{welinder2010caltech}, validate that \name{} outperforms other popular saliency map-based visual explanation methods. 
Our key contributions are summarized as follows: 
\begin{itemize}
\setlength{\itemsep}{1pt}
\setlength{\parsep}{1pt}
\setlength{\parskip}{1pt}
\item  We propose a novel definition, \textbf{N-S responsibility}, for a bi-directional causal importance quantification for CNNs from \textit{N} and \textit{S} perspectives.
\item  We introduce a flexible framework, \name{}, which can provide valid CNN visual explanations by analyzing either model filters or input features.
\item We provide 2D saliency maps for a more informative visual explanation, which is the first 2D visual explanation design of CNN to the best of our knowledge.
\item Extensive quantitative and qualitative experiments highlight our explanations' effectiveness and uncover the potential of \name{} in real-world problem-solving.
\end{itemize}
