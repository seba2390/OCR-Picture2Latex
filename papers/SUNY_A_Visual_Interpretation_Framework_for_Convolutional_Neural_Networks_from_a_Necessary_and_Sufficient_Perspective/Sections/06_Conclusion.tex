We design a causality-driven framework, \name{}, for bi-directional CNN interpretation from a \textit{necessary} and \textit{sufficient} perspective.
By establishing a causal mechanism for the CNN classification process, we conduct causal analysis by regarding the CNN model's prediction probability of a target class as the outcome and the model filters or input features as the hypothesized causes, respectively.
Furthermore, our explanations, \name{}-filter and \name{}-feature, are provided by 2D saliency maps, a unique visual explanation design to support a more informative model interpretation.
Our semantic evaluation results show that \name{} can provide more insightful visualizations and demonstrate how model interpretations can benefit from \textit{necessity} and \textit{sufficiency} information.
Moreover, \name{} explanations also pass the sanity check and quantitatively outperform seven other visual explanation methods in necessity and sufficiency evaluation, saliency attack, and localization evaluations using two different large-scale public datasets for two CNN models.

In the future, we plan to exploit the generality of \name{} framework in the scenario of interpreting image segmentation and vision-language tasks. 
In addition, we plan to apply \name{} in data and model validation.