\section{Introduction}
\label{sec:introduction}

\begin{table*}
    \centering
    \resizebox{\linewidth}{!}{
    \begin{tabular}{ll}
    \hline
         \bm{\mathrm{Input - Sentence}}: & They use \bm{\mathrm{CNN}} in the proposed model.\\
\bm{\mathrm{Input - Dictionary}}: & CNN: 1. Convolutional Neural Network, 2. Cable News Network, 3. Condensed Nearest Neighbor\\
\hline
\bm{\mathrm{Output}}: & Convolutional Neural Network\\
\hline
    \end{tabular}
    }
    \caption{A toy sample of acronym disambiguation.}
    \label{tab:example}
\end{table*}

In recent years, it has witnessed the vigorous development of deep learning.
Among the most successful scenarios, natural language processing (NLP) is advancing steadily.
However, natural language is frequently ambiguous, so many words and phrases can be interpreted in many ways depending on the context in which they appear~\cite{navigli2009word}.
Specifically, an obstacle to scientific document understanding (SDU) is the widespread use of acronyms, which are shortened forms of long technical phrases~\cite{veyseh-et-al-2020-what,Beltagy2019SciBERT}.
In order to understand the document correctly, the SDU system should be able to identify acronyms and their correct meanings.
The goal of acronym disambiguation (AD) is to determine the correct long form of an ambiguous acronym in a given text~\cite{veyseh2020acronym}.
%~\footnote{We won second place in the acronym disambiguation competition. \url{https://sites.google.com/view/sdu-aaai21/shared-task}}.
It is usually formulated as a sequence classification problem in general~\cite{veyseh-et-al-2020-what}.
For instance, a toy sample of this task is shown in Table~\ref{tab:example}.
In this example, 
the ``\textit{CNN}'' might be an acronym for ``\textit{Convolutional} \textit{Neural} \textit{Network}'', ``\textit{Cable} \textit{News} \textit{Network}'' or ``\textit{Condensed} \textit{Nearest} \textit{Neighbor}''.
Given a sentence ``\textit{They} \textit{use} \textit{\textbf{CNN}} \textit{in} \textit{the} \textit{proposed} \textit{model}.'' and a dictionary with possible expansions (i.e., long forms) of the acronym ``\textit{CNN}'', the expected prediction for its correct meaning is ``\textit{Convolutional} \textit{Neural} \textit{Network}''.
Recent efforts attempted to incorporate hand crafted features~\cite{li2018guess}, word embeddings~\cite{charbonnier2018using,ciosici2019unsupervised}, graph structures~\cite{prokofyev2013ontology,veyseh-et-al-2020-what}, and deep learning architectures~\cite{jin2019deep,blevins2020moving} and achieved significant effects in this task.


In this paper, we pay more attention to the scenario of scientific acronym disambiguation.
Some observations are still worthy of further investigation.
Generally, large-scale training data for natural language processing tasks in general domains is often possible to obtain through crowd-sourcing, emerging a variety of domain-independent fine-grained pretrained models.
However, these models based on domain agnostic knowledge might achieve insufficient performance when applied to the specific domain~\cite{Beltagy2019SciBERT}.
Furthermore, obtaining large-scale annotated data in the scientific domain is challenging and expensive~\cite{Beltagy2019SciBERT}, which leads to the shortage of high-level semantic expression to some extent.

To remedy these challenges, we fully consider both the domain agnostic and specific knowledge, and propose a \underline{H}ierarchical \underline{D}ual-path \underline{BERT} method coined \textbf{hdBERT} to fusion the general fine-grained and high-level specific representations for acronym disambiguation.
The overall architecture is illustrated in Figure~\ref{fig:structure}.
We pinpoint that hdBERT is a BERT-based supervised method adopting the now ubiquitous transformer architecture~\cite{vaswani2017attention}.
First, RoBERTa~\cite{liu2019roberta} and SciBERT~\cite{Beltagy2019SciBERT} modules are elaborately involved to distill representations from inputs consist of sentence and candidate long forms.
Specifically, we utilize RoBERTa, a robustly optimized method trained on general domain corpora via byte-level Byte-Pair-Encoding~\cite{sennrich2016neural}, to capture domain agnostic and fine-grained semantic information.
Moreover, SciBERT which is also a pretrained language model based on BERT~\cite{devlin2018bert} is exploited to model the high-level scientific domain representation.
Since it leverages unsupervised pretraining on a large multi-domain corpus of scientific publications using WordPiece~\cite{wu2016google} tokenization strategy.
Second, we integrate these dual-path representations from RoBERTa and SciBERT simultaneously via multiple layer perceptron and output the prediction. %Finally, the model output the prediction result.
The main contributions of this work are summarized as follows:
\begin{itemize}
	\item We are the very first attempt to resolve the acronym disambiguation problem simultaneously leveraging domain agnostic and specific knowledge.
	\item We propose a novel hierarchical dual-path BERT method coined hdBERT to capture both general fine-grained and high-level specific representations. It is mainly implemented based on the well-known transformer architecture, which can train the overall model more effectively.
	\item Experiments on real-world datasets demonstrate the effectiveness of the proposed approach. It achieves competitive performance and outperforms state-of-the-art methods.
\end{itemize}



% The remainder of this paper is organized as follows. Section~\ref{sec:relatedwork} surveys the related researches in the literature. Section~\ref{sec:methodology} introduces problem statement and details the proposed hdBERT method. Section~\ref{sec:experiments} illustrate the implementation details and demonstrate the experimental results. Section~\ref{sec:conclusion} concludes the paper.
%\redfont{
%In order to correctly process a document, an SDU system should be able to  identify  acronyms  and  their  correct  meanings.  As acronyms  might  be  defined  either  locally  in  the  same document or globally in an external dictionary with multiple meanings, a successful document understanding model should  both capture local  definitions  and disambiguate acronyms which are not defined in documents.
%Generally, it is relatively simple for human to recognize the specific meaning of a word assumed in the context, but the machine needs to process unstructured text information and convert it into a data structure to determine the potential meaning.
% analysis. The computational recognition of the meaning of a word in the context is called word sense disambiguation~(WSD)~\cite{navigli2009word}.
% In order to correctly process a document, an SDU system should be able to  identify  acronyms  and  their  correct  meanings.  As acronyms  might  be  defined  either  locally  in  the  same document or globally in an external dictionary with multiple meanings, a successful document understanding model should  both capture local  definitions  and disambiguate acronyms which are not defined in documents. To push forward the research on acronym understanding, we propose two shared tasks at SDU@AAAI-21. 
%On the other hand, acronyms  are  short  forms  of  phrases  that  help  to  convey lengthy sentences in documents. And as one of the subjects of writing. Because of their importance, please identify abbreviations and it is very important to understand the text to find the correct meaning of each phrase and find the initial abbreviation (namely, the ambiguity elimination of the initial abbreviation~(AD)).
%For example, the CNN can be used as an abbreviation for Convolutional Neural Network or Cable News Network.
%Obviously, in different contexts, abbreviations have completely different meanings. This brings greater challenges to natural language processing.
%Because of its importance, identifying acronyms and corresponding phrases and finding the correct meaning of each acronym is essential for understanding the text~\cite{veyseh-et-al-2020-what}.
%Despite the remarkable success of existing researches on specific datasets are mostly limited to
%the medical domain~\cite{prokofyev2013ontology}, scientific domain~\cite{veyseh-et-al-2020-what}, and ignore other domains. 
%In this paper, we pay more attention to the field of scientific phrase abbreviations.
%Some observations are still worth further investigations. 
%When we do an AD task in a specific domain, we often ignore the information that can be input in other domain. Then the results may not be covered by general fine-grained and combined domain high-level information.
%So this problem can be regarded as a problem of multi-level information fusion. 
%On the other hand, the proposed method in previous papers may rely on the time-consuming long short-term memory neural network~\cite{veyseh-et-al-2020-what}, which makes it difficult to train tasks and refined the models, and does not have domain extension. 

%To remedy these challenges, we propose a novel approach
%coined~\textbf{DPHBERT}~(\underline{\textbf{D}}ual-\underline{\textbf{p}}ath \underline{\textbf{H}}ierarchical \underline{\textbf{BERT}}) in this paper. 
%We pinpoint that the DPHBERT is a Bert-based supervised method for ambiguity elimination of the initial abbreviation to resolve the special challenges in scientific domain. 
%The motivation of the DPHBERT method is that 
%the network architecture of the BERT uses the multi-layer transformer structure~\cite{vaswani2017attention}. Its biggest feature is that it discards the traditional Recurrent Neural Network and Convolutional Neural Network, and effectively solve the thorny long-term dependency problem in NLP. The structure of Transformer has been widely used in the field of NLP. The SciBERT~\cite{Beltagy2019SciBERT} follows the same architecture as BERT but is instead pre-trained on scientific text. Since we are exploring AD tasks in the scientific domain, SciBERT is the most suitable pre-training model in all variant models of BERT.
%Secondly, abbreviations are all byte-level, so more fine-grained byte-level representation is also crucial.  Therefore, RoBERT~\cite{liu2019roberta} which is mainly trained by byte-level text encoding based on the structure of BERT, has also been applied in our model.
%In this paper, we propose the dual-path hierarchical model framework to solve the above problems and give an implementation solution.

%First, we get the general representation of each word through the SciBERT in high level. Secondly, we obtain a more fine-grained byte-level representation via the RoBERT.
%Through this method, the mixed multi-level dual-path information will provide rich representations for the downstream networks. Since the above network structure is mainly based on the BERT framework, the training and inference speeds are relatively fast. In this way, we will provide efficient training for our target tasks.
%The main contributions of this work are summarized as follows.

%}

