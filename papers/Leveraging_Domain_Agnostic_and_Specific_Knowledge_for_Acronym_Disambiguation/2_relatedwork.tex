\section{Related Work}
\label{sec:relatedwork}
In this section, we review the related researches on word sense disambiguation especially acronym disambiguation as well as BERT and its two representative variants.

\subsection{Word Sense Disambiguation}
Word sense disambiguation (WSD) is an open problem concerned with identifying which sense of a word is used in a text~\cite{navigli2009word}.
It is a core and difficulty in natural language processing tasks, which affects the performance of almost all downstream tasks.
The methods to solve word sense disambiguation are usually divided into two categories: knowledge-based and supervised~\cite{wang2020word,barba2020mulan}.

Knowledge-based methods usually rely on amounts of statistical information and can be easily extended to other low-resource languages~\cite{agirre2014random,scarlini2020sensembert}.
For example, SensEmBERT~\citep{scarlini2020sensembert}, a knowledge- and BERT-based method that combines the expressive power of language modeling with the vast amount of knowledge contained in the semantic network, produces high-quality latent semantic representations of the meanings of the word in different languages.
And it can achieve competitive results attained by most of the supervised neural approaches on the WSD tasks.
On the other hand, supervised methods require lots of labeled data to learn word representations~\cite{bevilacqua2020breaking,wang2020word}.
Of course, this defect can be alleviated through semi-supervised methods~\cite{barba2020mulan} by jointly leveraging contextualized word embedding and the multilingual information to project some sense labels.

Furthermore, acronym disambiguation is more challenging since we need to identify the acronym first and then to understand the text to determine the correct meaning of acronyms.
Recently, an effective solution is to extract acronym definitions from unstructured texts by computing the Levenshtein string edit distance between any pair of long forms~\cite{ciosici2019unsupervised}, which is an entirely unsupervised acronym disambiguation method. And researches also attempt to incorporate hand crafted features~\cite{li2018guess}, word embeddings~\cite{charbonnier2018using,ciosici2019unsupervised}, graph structures~\cite{prokofyev2013ontology,veyseh-et-al-2020-what}, and deep learning architectures~\cite{jin2019deep,blevins2020moving}, and have achieved significant effects in this task.
Specifically, a supervised method named GAD~\cite{veyseh-et-al-2020-what}, which utilizes the syntactic structure of sentences to extend ambiguous acronyms in sentences by combining Bidirectional
Long Short-Term Memory~(BiLSTM) with Graph Convolutional Networks~(GCN), provides a strong baseline on acronym disambiguation tasks in the scientific domain.
\begin{figure*}
    \centering
    \includegraphics[width=\linewidth]{structure3.png}
    \caption{Illustration of the proposed hdBERT model.}
    \label{fig:structure}
\end{figure*}
%The computational recognition of the meaning of a word in the context is called word sense disambiguation~(WSD)~\cite{navigli2009word}.
%WSD is a core and difficult point in natural language processing tasks, which affects the performance of almost all tasks.
%But it's relatively difficult for the machine to process unstructured text information and convert it into a data structure to determine the potential meaning.
\subsection{BERT-based Methods}
Bidirectional Encoder Representations from Transformers (BERT) \cite{devlin2018bert} is a self-supervised learning method that trains based on a large number of corpora to express better features for word embedding. And its network architecture utilizes the multi-layer transformer structure~\cite{vaswani2017attention}.
The feature representation of BERT could be directly adopted as word embedding features for downstream tasks.
Besides, BERT provides a model for transfer learning of other tasks. It can be fine-tuned or fixed according to tasks and then treated as a feature extractor.
BERT was significantly undertrained, and there have been many fine-grained improvements or specific domain variants of it~\cite{Beltagy2019SciBERT,liu2019roberta,scarlini2020sensembert,lee2020biobert}.
\subsubsection{RoBERTa.}
RoBERTa~\cite{liu2019roberta} is mainly trained on general domain corpora via byte-level Byte-Pair-Encoding \cite{sennrich2016neural} based on the structure of BERT and can supply more fine-grained representation.
This encoding scheme can process amounts of words that are common in natural language corpora and is more conducive to the translation of acronyms.

\subsubsection{SciBERT.}

SciBERT~\cite{Beltagy2019SciBERT} is a specific pretrained language model for scientific domain texts. This model follows the same architecture as BERT to solve the lack of high-quality, large-scale labeled scientific data. It significantly outperforms previous BERT-based methods and achieves new state-of-the-art results on some scientific NLP tasks.

