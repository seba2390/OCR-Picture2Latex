\section{Methodology}
\label{sec:methodology}
In this section, we first introduce the problem statement of acronym disambiguation and then describe the overall architecture and details of our proposed hdBERT model.
\subsection{Problem Statement}
Acronym disambiguation is formulated as a sequence classification problem in general~\cite{veyseh-et-al-2020-what}.
Formally, given an input sentence $\bm{s}=w_1, w_2, ..., w_n$ and the position of the acronym, i.e., $p$, the goal is to disambiguate the acronym $w_p$, that is, predicting the true long form $\bm{l}$ from all candidate long forms of $w_p$.
Specifically, in this paper, we simplify it into a binary classification problem. That is, given an input sample consists of the sentence $\bm{s}$ with acronym $w_p$ and the candidate long form $\bm{l}$, i.e., $\bm{x}=(\bm{s}; \bm{l})$, our purpose is to predict the probability of $\bm{l}$ being the right long form of $w_p$. 
We assign a label $y \in \{0,1\}$ on each sample in training dataset to indicate whether $\bm{l}$ is a true long form of $w_p$ in sentence $\bm{s}$ or not.
In the testing phase, the long form with the highest prediction probability among the candidate long form set of a sentence would be chosen as its final result.

\subsection{Overview}
Figure~\ref{fig:structure} exhibits the schematic illustration of the proposed hdBERT model.
As mentioned previously, we design a hierarchical integration model comprising three major components, each plays a different role in final prediction.
The first two context-based components, i.e., RoBERTa~\cite{liu2019roberta} and SciBERT~\cite{Beltagy2019SciBERT} modules, distill representations of the sentence and the candidate long forms.
Specifically, as a robustly optimized method trained on vast amounts of general domain corpora, we use RoBERTa to capture the general and fine-grained semantic information via byte-level Byte-Pair-Encoding~\cite{sennrich2016neural}.
Moreover, SciBERT, which leverages unsupervised pretraining on a large scientific corpus by WordPiece~\cite{wu2016google} tokenization strategy, is exploited to represent the high-level scientific domain information.
Finally, a multiple layer perceptron network is devised to fusion these two kinds of representations. In the following, we present detail of each major component.

\subsection{Information Distillation}
\subsubsection{General and Fine-grained Information.}
We involve RoBERTa to capture domain agnostic and fine-grained information of the sentence and its candidate long form.
RoBERTa uses the now ubiquitous transformer architecture~\cite{vaswani2017attention} via byte-level Byte-Pair-Encoding (BPE), which is a hybrid between character- and word-level representations that allow handling large vocabularies common in natural language corpora. Instead of full words, BPE relies on subwords units, which are extracted by performing statistical analysis of the training corpus.
The size of the original vocabulary released with RoBERTa is about 50K, which is 20K more than BERT's.

We define the encoding of a sample $\bm{x}=(\bm{s},\bm{l})$ after the BPE strategy as $\bm{e}_{\mathrm{BPE}}$ and the output representation throughout the RoBERTa model as $\bm{h}_{\mathrm{RoBERTa}}$.
\begin{equation}
    \bm{e}_{\mathrm{BPE}} = \bm{\mathrm{BPE}}(\bm{x})
\end{equation}
\begin{equation}
    \bm{h}_{\mathrm{RoBERTa}} = \bm{\mathrm{RoBERTa}}(\bm{e}_{\mathrm{BPE}})
\end{equation}

\subsubsection{High-level Scientific Domain Information.}
To handle the high-level scientific domain information, SciBERT is chosen elaborately. SciBERT follows the same architecture as BERT but is instead pretrained on the scientific texts.
% illustrating a substantial difference in frequently used words between scientific and general domain texts.
It constructed a new WordPiece vocabulary on scientific corpus using the SentencePiece
%\footnote{\url{https://github.com/google/sentencepiece}} 
library and trained on a random sample of 1.14M papers from Semantic Scholar~\cite{ammar2018construction}. Its corpus consists of 18\% papers from the computer science domain and 82\% from the broad biomedical domain.
The size of the original vocabulary released with SciBERT is about 30K, which is 20K less than RoBERTa. The resulting token overlap between SciBERT and BERT is 42\%,
which illustrates the significant difference in common terms between scientific and general domain texts.

We define the encoding of a sample $\bm{x}=(\bm{s},\bm{l})$ after SciBERT's encoding strategy (noted as WPE) as $\bm{e}_{\mathrm{WPE}}$ and the output representation throughout the SciBERT model as $\bm{h}_{\mathrm{SciBERT}}$.
\begin{equation}
    \bm{e}_{\mathrm{WPE}} = \bm{\mathrm{WPE}}(\bm{x})
\end{equation}
\begin{equation}
    \bm{h}_{\mathrm{SciBERT}} = \bm{\mathrm{SciBERT}}(\bm{e}_{\mathrm{WPE}})
\end{equation}

\subsection{Integration}
After modeling the two complex representations above, the obtained concatenation $\bm{h}$ is fed into multiple layer perceptron network and followed by a regression layer with $\mathrm{sigmoid}$ unit, as follows:
\begin{equation}
    \bm{h}=[\bm{h}_{\mathrm{RoBERTa}};\bm{h}_{\mathrm{SciBERT}}]
\end{equation}
\begin{equation}
p = \mathrm{sigmoid}(\bm{W}^\mathrm{T} \bm{\mathrm{MLP}}(\bm{h}) + b)
\end{equation}
where $\bm{W}$ is the weight vector, $b$ is the bias, and $\bm{\mathrm{MLP}}(\cdot)$ represents the operation of multiple layer perceptron shown in Figure~\ref{fig:structure}. Here $p$ is the predicted probability.

Finally, our model is trained with cross entropy loss with regularization. The loss function is defined as
\begin{equation}
\label{eq:loss}
\mathcal{\bm{L}}(\theta) = -\sum_{\mathcal{D}}{\left(y\log(p)+(1-y)\log(1-p)\right)} + \lambda{\|\bm{\theta}\|}_2^2
\end{equation}
where $y$ is the ground truth, $\bm{\theta}$ is the parameter set of the proposed model, $\lambda$ is the regularizer parameter, and $\mathcal{D}$ is the training dataset.