\section{Experiments}
\label{sec:experiments}
\begin{table}
    \centering
    \begin{tabular}{l|c}
    \hline
        \bm{\mathrm{Statistical Information}} & \bm{\mathrm{SciAD}} \\
         \hline
        number of acronyms & 732 \\
        average number of long form per acronym & 3.1 \\
        overlap between sentence and long forms & 0.32 \\
        average sentence length & 30.7 \\
         \hline
         number of training & 50,034\\
         number of development & 6,189\\
         number of test & 6,218\\
         \hline
    \end{tabular}
    \caption{The statistical information of original SciAD dataset. Note that the third row shows the ratio of sentences that have at least one word in common with the long forms of the acronyms appearing in the sentence.}
    \label{tab:statinfo}
\end{table}

In this section, we first illustrate the datasets, evaluation metrics, and implementation details, then demonstrate the experimental results and further studies.


\subsection{Datasets}
The SciAD~\footnote{We won second place in the acronym disambiguation competition. \url{https://sites.google.com/view/sdu-aaai21/shared-task}} dataset created from 6,786 English scientific papers
aims to find the correct meaning of an ambiguous acronym in a given sentence~\cite{veyseh-et-al-2020-what}. It contains 62,441 sentences and a dictionary of 732 ambiguous acronyms.
More statistical information is shown in Table~\ref{tab:statinfo}.
Besides, a toy sample of the SciAD dataset is shown in Table~\ref{tab:example}. The input is a sentence with an ambiguous acronym and a dictionary with possible expansions (i.e., long forms) of the acronym. In this example, the ambiguous acronym ``\textit{CNN}'' in the input sentence is shown in boldface and the expected prediction for its correct meaning is ``\textit{Convolutional} \textit{Neural} \textit{Network}''.
In addition, Figures~\ref{fig:disofacr} and~\ref{fig:disofsample} demonstrate more statistics of SciAD dataset~\cite{veyseh-et-al-2020-what}. More specifically, Figure~\ref{fig:disofacr} shows the distribution of the number of acronyms based on the number of long forms per acronym, and the distribution of the number of samples based on the number of long form per acronym is shown in Figure~\ref{fig:disofsample}.

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{disofarc2.png}
    \caption{Distribution of acronyms based on number of long form per acronym.}
    \label{fig:disofacr}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{disofsample2.png}
    \caption{Distribution of samples based on number of long form per acronym.}
    \label{fig:disofsample}
\end{figure}



\begin{table*}
    \centering
    \resizebox{\linewidth}{!}{
    \begin{threeparttable}
    \begin{tabular}{l|ccc}
    \hline
         \textbf{Parameter} & \textbf{BERT} & \textbf{RoBERTa} & \textbf{SciBERT} \\
         \hline
         pretrained model & bert-large-ucased\tnote{a} & roberta-large\tnote{b} & allenai/scibert\_scivocab\_uncased\tnote{c} \\
         architecture & sequence classification & sequence classification & sequence classification \\
  attention\_probs\_dropout\_prob & 0.1 & 0.1 & 0.1\\
  hidden\_act &gelu & gelu & gelu\\
  hidden\_dropout\_prob & 0.1 & 0.1 & 0.1\\
  hidden\_size & 1024 & 1024 & 768\\
  initializer\_range & 0.02 & 0.02 & 0.02\\
  intermediate\_size & 4096 & 4096 & 3072\\
  layer\_norm\_eps & 1e-12 & 1e-05 & 1e-12\\
  max\_position\_embeddings & 512 & 514 & 512\\
  model\_type & bert & roberta & bert\\
  num\_attention\_heads & 16 & 16 & 12\\
  num\_hidden\_layers & 24 & 24 & 12\\
  position\_embedding\_type & absolute & absolute & absolute\\
  vocab\_size & 30522 & 50265 & 31090\\
  learning\_rate & 2e-5& 2e-5& 2e-5\\
  epoch & 5 & 5 & 5\\
  \hline
    \end{tabular}
    \begin{tablenotes}
\item[a] \url{https://huggingface.co/bert-large-uncased}
\item[b] \url{https://huggingface.co/roberta-large}
\item[c] \url{https://github.com/allenai/scibert}
\end{tablenotes}
\end{threeparttable}
}
    \caption{Architecture and hyper parameters information. Our proposed hdBERT model ensembles RoBERTa and SciBERT via three MLP layers.}
    \label{tab:impts}
\end{table*}

\begin{table}
    \centering
    \begin{tabular}{l|c}
    \hline
        \bm{\mathrm{Statistical Information}} & \bm{\mathrm{SciAD}}$_{\mathrm{BI}}$ \\
         \hline
         number of training & 352,366\\
         number of development & 28,286\\
         number of test & 28,364\\
         \hline
    \end{tabular}
    \caption{The statistical information of SciAD$_{\mathrm{BI}}$ dataset.}
    \label{tab:newdataset}
\end{table}
As mentioned previously, we convert the original SciAD dataset into a binary classification dataset named SciAD$_{\mathrm{BI}}$ during modeling. 
For a sentence $\bm{s}$ with acronym $w_p$, $y=1$ if a long form $\bm{l}$ is true for $w_p$, while $y=0$ for other false candidate long forms of $w_p$. Specifically, to alleviate the imbalance problem during training, we upsample each positive sample to equal the number of candidate long forms of its acronym. More statistics of SciAD$_{\mathrm{BI}}$ is shown in Table~\ref{tab:newdataset}. We finally evaluate performances on SciAD's test dataset.



\subsection{Compared Methods}
We compare with several state-of-the-art and representative methods including Non-deep learning methods and Deep learning methods to verify the effectiveness of our proposed method.

\noindent \textbf{Non-deep learning methods}.
\begin{itemize}
    \item \textbf{MF}: most frequent which takes the long form with the highest frequency among all possible meanings of an acronym as the expanded form of the acronym.
    \item \textbf{ADE}~\cite{li2018guess}: a feature-based model that employs hand crafted features from the context of the acronyms to train a disambiguation classifier.
\end{itemize}
\noindent \textbf{Deep learning methods}.
\begin{itemize}
    \item \textbf{NOA}~\cite{charbonnier2018using} and  \textbf{UAD}~\cite{ciosici2019unsupervised}: language-model-based baselines that train the word embeddings using the training corpus.
    \item \textbf{DECBAE}~\cite{jin2019deep} and \textbf{BEM}~\cite{blevins2020moving}: models employing deep architectures (e.g., LSTM).
    \item \textbf{GAD}~\cite{veyseh-et-al-2020-what}: supervised method which utilizes syntactic structure of sentences to extend ambiguous acronyms in sentences by combining BiLSTM with GCN.
    \item \textbf{BERT}~\cite{devlin2018bert},  \textbf{RoBERTa}~\cite{liu2019roberta} and \textbf{SciBERT}~\cite{Beltagy2019SciBERT}: pretrained models use the now ubiquitous transformer architecture.
\end{itemize}

\subsection{Evaluation Metrics}
To evaluate the performance of different methods, three popular metrics are adopted, namely \textbf{Macro Precision}, \textbf{Macro Recall} and \textbf{Macro F1}. The definitions are as follows:
\begin{equation}
    \bm{\mathrm{Precision}}_\mathrm{MACRO} = \frac{\sum_{i=1}^{n}\mathrm{Precision}_i}{n}
\end{equation}
\begin{equation}
    \bm{\mathrm{Recall}}_\mathrm{MACRO} = \frac{\sum_{i=1}^{n}\mathrm{Recall}_i}{n}
\end{equation}
\begin{equation}
    \bm{\mathrm{F1}}_\mathrm{MACRO} = \frac{2\times\mathrm{Precision}_\mathrm{MACRO}\times\mathrm{Recall}_\mathrm{MACRO}}{\mathrm{Precision}_\mathrm{MACRO} + \mathrm{Recall}_\mathrm{MACRO}}
\end{equation}
where $n$ is the number of total classes, $\mathrm{Precision}_i$ and $\mathrm{Recall}_i$ represent the precision and recall of class $i$ respectively. The higher $\mathrm{Precision}_\mathrm{MACRO}$, $\mathrm{Recall}_\mathrm{MACRO}$ and $\mathrm{F1}_\mathrm{MACRO}$ indicate the higher performance of approaches.

\begin{table*}%[!t]
    \centering
    \begin{tabular}{l|ccc}
    \hline
         \textbf{Methodology} & \textbf{Macro Precision}(\%) & \textbf{Macro Recall}(\%) & \textbf{Macro F1}(\%) \\
         \hline
         \textbf{MF} & 89.03 & 42.20 & 57.26 \\
         \textbf{ADE}~\cite{li2018guess} & 86.74  & 43.25  & 57.72 \\
         \textbf{NOA}~\cite{charbonnier2018using} & 78.14  & 35.06  & 48.40 \\
         \textbf{UAD}~\cite{ciosici2019unsupervised} & 89.01  & 70.08  & 78.37 \\
         \textbf{BEM}~\cite{blevins2020moving} & 86.75  & 35.94  & 50.82 \\
         \textbf{DECBAE}~\cite{jin2019deep} & 88.67  & 74.32  & 80.86 \\
         \textbf{GAD}~\cite{veyseh-et-al-2020-what} & 89.27  & 76.66  & 81.90 \\
         \textbf{Human Performance}~\cite{veyseh-et-al-2020-what} & 97.82  & 94.45  & 96.10 \\
         \hline
         \textbf{MF} & 89.00 & 46.36 & 60.97 \\
         \textbf{BERT}~\cite{devlin2018bert} & 95.26& 86.92& 90.90\\
         \textbf{RoBERTa}~\cite{liu2019roberta} & 95.96 & 88.36 & 92.00\\
         \textbf{SciBERT}~\cite{Beltagy2019SciBERT} & 96.36 & 89.77 & 92.95\\
         \textbf{hdBERT}~(ours) & \textbf{96.94} & \textbf{90.73} & \textbf{93.73}\\
         \hline
    \end{tabular}
    \caption{Performance of models in acronym disambiguation.}
    \label{tab:results}
\end{table*}

\subsection{Implementation Details}
For models ADE, NOA, UAD, DECBAE, BEM, and GAD, please refer to~\citeauthor{veyseh-et-al-2020-what} for more implementation information.
We implement the proposed model based on Pytorch~\cite{paszke2019pytorch} and Transformers~\cite{wolf-etal-2020-transformers}.
For models BERT, RoBERTa, and SciBERT, we fine-tune them on dataset based on their popular pretrained models. The implementation details of these models are shown in Table~\ref{tab:impts}. Moreover, the information distillation components of our model are the same as model RoBERTa and SciBERT respectively. And we simply adopt three MLP layers for integration simultaneously. As mentioned previously, in the testing phase, the long form with the highest prediction probability in the candidate long form set of a sentence would be chosen as its final result. In addition, we use two V100 GPUs with 12 cores to complete all these experiments.



\subsection{Performance Comparison}

Table~\ref{tab:results} demonstrates the main results of all compared methods~\footnote{We assume that both~\citeauthor{veyseh-et-al-2020-what} and this task have the same distribution of dataset due to the randomly dividing by the same ratio, making all these methods comparable.} on the dataset.
The major findings from the experimental results can be summarized as follows:

First, GAD achieves a better result than methods such as ADE, NOA, UAD, BEM, and DECBAE, showing the importance of syntactic structure for the acronym disambiguation task. But it still far worse than pretraining-based models like BERT and RoBERTa.
Second, between the two general-domain models, RoBERTa gets better performance than BERT, indicating the advantage of more fine-grained encoding. Moreover, SciBERT is more advanced than the domain agnostic methods, i.e., BERT and RoBERTa, with about 2.26\% and 1.03\% increased macro F1 respectively, showing the importance of the scientific domain pretraining for this task.
Furthermore, we can clearly observe that our hdBERT model outperforms all the baselines by a large margin. 
Its macro F1, with the reported value of 93.73\%, is about 1.88\% and 0.84\% higher than state-of-the-art RoBERTa and SciBERT respectively.
And its loss curve falls faster and converges lower than the two pretrained methods on the development dataset, as shown in Figure~\ref{fig:loss}.
These observations demonstrate that it is effective to model both fine-grained domain agnostic and high-level domain specific knowledge simultaneously.

However, despite the significant improvements among these approaches, performances of all models are still not as effective as humans on the dataset, especially on macro recall and macro F1, thus providing many further research opportunities for this scenario.

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{loss2.png}
    \caption{Loss curve on development dataset.}
    \label{fig:loss}
\end{figure}


\subsection{Case Study}
\begin{table*}
    \centering
    	\resizebox{\linewidth}{!}{
    \begin{tabular}{l|l}
    \hline
        \textbf{Sentence} & \textbf{Conflicted Annotation} \\
        \hline
        Just like \textbf{RF}, QRF is a set of binary regression trees. & TR-43200: Regression Forest \\
        & TR-49535: Regression Function \\
        \hline
        Extensions of the \textbf{SBM} regarding the type of graph are reviewed in Section. & TR-17276: Sequential Monte Carlo\\
        &TR-47761: Stochastic Block Model\\
        \hline
        The obfuscated term is the term for which the \textbf{MACS} score is the lowest. & TR-15480: Mean Average Conceptual Similarity\\
        &TR-27970: Minimum Average Conceptual Similarity \\
        \hline
    \end{tabular}
    }
    \caption{Examples of noise data of SciAD dataset.}
    \label{tab:noise}
\end{table*}

We further focus on studying both success and failure cases of pretraining-based models to provide more insight into acronym disambiguation. 
Specifically, for success case of our model in which RoBERTa and SciBERT fail,
e.g., ``\textit{Each} \textit{SP} \textit{within} \textit{an} \textit{\textbf{SM}} \textit{shares} \textit{an} \textit{instruction} \textit{unit,} \textit{dedicated} \textit{to} \textit{the} \textit{management} \textit{of} \textit{the} \textit{instruction} \textit{flow} \textit{of} \textit{the} \textit{threads.}'' (DEV-6156), the true long form of ``\textit{SM}'' is
``\textit{Streaming} \textit{Multiprocessors}''. While both RoBERTa and SciBERT output ``\textit{Shared} \textit{Memory}'', which may often appear in deep learning publications. 
It might benefit from the additional integration modeling of two different information from RoBERTa and SciBERT.
However, all the three models fail in this example: ``\textit{In} \textit{the} \textit{first} \textit{stage,} \textit{we} \textit{train} \textit{the} \textit{SPM,} \textit{and} \textit{extract} \textit{the} \textit{\textbf{FL}} \textit{and} \textit{FR.}'' (DEV-4604) with the wrong prediction ``\textit{Federated} \textit{Learning}'' for ``\textit{FL}''. The true long form of ``\textit{FL}'' is ``\textit{Fixated} \textit{Locations}''. 
We guess that all models pay too much attention to ``\textit{Federated} \textit{Learning}'', a hot phrase nowadays, and ignore the subtle information among the sentence and its different candidate long forms.
It also indicates the necessity of more advanced models for this task.

\subsection{Further Discussion}
As mentioned previously and shown in Table~\ref{tab:results}, all the current models are still less effective than humans in this scenario. There are still many samples that all models fail in. Some further research opportunities on this dataset are discussed in this section. First, as shown in Table~\ref{tab:noise}, there are some noise data, i.e., conflicted annotation, in the SciAD dataset.
For example, the acronym ``\textit{RF}'' in boldface in sentence ``\textit{Just} \textit{like} \textit{\textbf{RF},} \textit{QRF} \textit{is} \textit{a} \textit{set} \textit{of} \textit{binary} \textit{regression} \textit{trees.}'' gets two different long form ``\textit{Regression} \textit{Forest}'' (TR-43200) and ``\textit{Regression} \textit{Function}'' (TR-49535) respectively. 
It will be some negative impacts on modeling to some extent.
Furthermore, to a certain extent, samples constructed from the same sentence with different long forms are independent during our training stage.
It might lose more subtle information among them.
Therefore, recent methods such as self-training~\cite{peng2019trainable,chi2020learning}, adversarial learning~\cite{fgsm,fgm,danqing2020}, and contrastive learning~\cite{hadsell2006dimensionality} are worth studying to further improve the performance.