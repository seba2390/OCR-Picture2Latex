\documentclass[a4paper,twoside]{article}

\usepackage{epsfig}
\usepackage{subfigure}
\usepackage{calc}
\usepackage{amssymb}
\usepackage{amstext}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{comment}
\usepackage{bm}
\usepackage{multirow}
\usepackage{cite}
\usepackage{multicol}
\usepackage{pslatex}
\DeclareMathOperator*{\argmax}{argmax}
\usepackage{apalike}
\usepackage{SCITEPRESS}     % Please add other packages that you may need BEFORE the SCITEPRESS.sty package.

\subfigtopskip=0pt
\subfigcapskip=0pt
\subfigbottomskip=0pt

\begin{document}

\title{Efficient and Effective Single-Document Summarizations and A Word-Embedding Measurement of Quality}


\author{\authorname{Liqun Shao, Hao Zhang, Ming Jia and Jie Wang}
\affiliation{Department of Computer Science, University of Massachusetts, Lowell, MA, USA}
\email{\{Liqun\_Shao, Hao\_Zhang, Ming\_Jia\}@student.uml.edu, Jie\_Wang@uml.edu}
}

\keywords{Single-Document Summarizations, Keyword Ranking, Topic Clustering, Word Embedding, SoftPlus Function, Semantic Similarity, Summarization Evaluation, Realtime.}

\abstract{
Our task is to generate an effective summary for a given document with specific realtime requirements. %In our applications there is no labeled data available for training a model.
%We study how to extract efficiently a summary from a single document within a given length boundary to capture the main topics of the document and be human readable.
%Following the common approach to extract multiple key sentences as coherent as possible and cover the major topics of the original document
%as diverse as possible,
We use the softplus function %$\ln(1+e^x)$
to enhance keyword rankings to favor important sentences,
%that are more important.
based on which we present a number of summarization algorithms using various
keyword extraction and topic clustering methods. We show %called ERAKE and ERAKE-TT % using RAKE keyword extractions and topic clusterings
%two algorithms called RAKEN and RAKETT %using word-pair co-occurrences to identify keyword phrases and TextTiling to promote coverage of diverse topics,
that our algorithms meet the realtime requirements and yield the best ROUGE recall scores on DUC-02 over all previously-known
algorithms. % are better than the previously-known best algorithm.
%For instance, our algorithm ET3Rank generates the scores of (49.2, 25.6, 27.5) for (ROUGE-1, ROUGE-2, ROUGE-SU4),
%
%
%the (ROUGE-1, ROUGE-2, ROUGE-SU4) scores over DUC-02 of our algorithm ET3Rank are (49.2, 25.6, 27.5),
%while the previously-reported best results are (49.0, 24.7, 25.8).
%
%In particular, both RANEN and RAKETT
%have higher ROUGE scores over DUC-02 and meet the realtime requirement, with RAKETT slightly better on ROUGE scores
%and RAKEN faster in runtime.
%The ROUGE measure requires summaries written by human experts as benchmarks for comparisons by NLP algorithms.
%This presents an obstacle when dealing with a large number of documents of various topics and lengths, for it is infeasible to produce a sufficient number %of benchmarks.
%To overcome this barrier,
To evaluate the quality of summaries %against the original documents
without human-generated benchmarks, % of particular topics,
we define a measure called WESM based on word-embedding using %a word-embedding method to measure the quality of a summary. In particular, we
%to use
Word Mover's Distance.
% over paragraphs, denoted by WESM,
%to measure similarities between a summary and its original document.
%and
We show that
%the best and the second best algorithms remain the same under both average ROUGE and WESM over different datasets, and
the orderings of the ROUGE and WESM scores of our algorithms are highly comparable, %, with the average $L_1$-norm equal to 0.4. %Moreover, we show that
%the orderings over DUC-02 and NewsIR-16 are also comparable, with the average $L_1$-norm equal to 2.
%where
%over DUC-02 have a similar ordering of the WEP scores as that of the ROUGE scores. %We argue that WEP may serve as a viable alternative
%for evaluating the quality of a summary.
%We show that under WEP, these algorithms have almost the same orderings over the NewsIR-16 and DUC-02 datasets.
%that ofsimilar toand
%Moreover, the best algorithm  also ranks the highest under WEP over both NewsIR-16 and DUC-02.
suggesting that WESM may serve as a viable alternative for measuring the quality of a summary.
%over the NewsIR-16 dataset,
% is similar to the order of ROGUE rankings produced by these algorithms over the DUC 2012 dataset.
%in place of ROGUE when human-generated benchmarks are not available for comparisons.
%Our WMD method obtains competitive performance and our summarization system outperforms prior work on ROUGE.
}

\onecolumn \maketitle \normalsize \vfill

\section{\uppercase{Introduction}}
\label{sec:introduction}
\noindent Text summarization algorithms have been studied intensively and extensively.
%For practical purposes,
An effective summary must
be human-readable and %is considered effective if it
convey the central meanings of the original document within
a given length boundary. % and is human readable.
% over the last several decades.
%A number of unsupervised and supervised algorithms have been devised to achieve these goals.
%To capture the key points of a given document and achieve coherence,
The common approach of unsupervised summarization algorithms
extracts sentences based on importance rankings (e.g., see \cite{DUC02,Mihalcea04,Rose:10,lin:11,ParveenR015}),
where a keyword may also
be a phrase. %Sentence extraction in the original order of the document also produces reasonable coherence. % is a sequence of one or more words that provides a compact representation of the content in the document.
A sentence with a larger number of keywords of higher ranking scores is considered more important %a candidate
for extraction.
Supervised algorithms include
%Attempts were also made to use
CNN and RNN models % in recent years
for generating extractive and abstractive summaries (e.g., see \cite{RushCW15,NallapatiZSGX16,ChengL16a}). %List the 3 papers published in 2016
%These algorithms
%depend on large sets of labeled data to train models, and their ROUGE scores over DUC data are yet to catch up with unsupervised algorithms.
%Significant advancements were made in recent years in supervised and unsupervised summarization algorithms
%over a single document, multiple documents, and queries, including CNNs \cite{}
%
%An adequate summary for a given document should succinctly convey the central meanings for the document. Summarization systems generate a concise document as %output and take a long document as input. Several summarization variants
%

%In a project on
We were asked to construct a general-purpose text-automation tool to produce, among other things, an effective
summary for a given document with the following realtime requirements: Generate a summary instantly for a document of
up to 2,000 words, under 1 second for
a document of slightly over 5,000 words, and under 3 seconds for a very long document of around 10,000 words.
Moreover, we need to deal with documents of arbitrary topics without knowing what the topics are in advance.
%and we would like to measure the quality of a summary directly against the original document without
After investigating all existing summarization algorithms,
we conclude that unsupervised single-document summarization algorithms would be the best approach to meeting
our requirements. %We show that by modifying the common approach of sentence extractions, we can achieve both efficiency and effectiveness.
%learning is our only choice.

%Methods can be characterized as supervised and unsupervised, and summaries may be obtained over a single document, multiple documents, and based on queries.
%can be query based, multi document, and single document.
%For the above different summarization types, the basic requirements remain the same, which contain salient information and let readers not miss anything from the original document. Readers are not %interested in redundant information, so summaries should cover many diverse topics. Finally, summaries should represent main information, be concise and cover different topics.





\begin{comment}
Recent methods used sentence compression to convert a sentence into a shorter sentence or phrase, while trying to maintain syntactic correctness. For example, Turner and Charniak ~\shortcite{Turner:05} used a language model to trim sentences. Vandegehinste and
Pan ~\shortcite{Vandeghinste:04} used context-free grammar (CFG) trees to compress a sentence. However, CFG is ambiguous and constructing CFG trees' complexity is high. Dependency trees are widely developed because they offer better syntactic representations of sentences ~\cite{kahane:12}. To increase the expressive capacity of our model, we apply compression of the selected individual sentence from a syntactic aspect with the dependency tree compression model ~\cite{Shao:16}. This model defines the set of empirical rules to specify what can and cannot be trimmed. These rules were formed based on experiences from working with a large number of text documents. It showed that this model can be used to generate titles which have higher F1 scores than those generated by the previous methods.
\end{comment}

We use topic clusterings to obtain a good topic coverage in the summary when extracting key sentences.
%we may simply treat each paragraph as a separate topic. For a long document, however, a topic may spread out among several paragraphs and so a topic clustering method would be needed. %are often not %independent with each other.
%Based on
%We achieve this using topic clusterings to
In particular, we first determine which topic a sentence belongs to,
and then %by computing the probability.
%We then
extract key sentences to cover as many topics as possible within the given length boundary.
%In particular, we choose sentences of higher scores so that they belong to as many topics as possible %from each topics
%until the summary covers all the topics or reaches the size bound.

Human judgement is the best evaluation of the quality of a summarization algorithm. %For example, %either abstractive or extractive,
It is a standard practice to run an algorithm over DUC data and
compute the ROUGE recall scores with a set of DUC benchmarks,
%The ROUGE measure requires summaries
which are human-generated summaries for articles of a moderate size.
DUC-02 \cite{DUC02}, in particular, is a small set of benchmarks for single-document summarizations.
% as benchmarks for comparisons by NLP algorithms.
When dealing with a large number of documents of unknown topics and various sizes, human judgement may be
impractical, and so
we would like to have an alternative mechanism of measurement %suitable for evaluating the quality of a summary
without human involvement.
%this approach presents a barrier, for it infeasible to produce a sufficient number of benchmarks.
%To solve this issue
%we would like a new measure to compute how similar.
Ideally, this mechanism should preserve the same ordering as %able to distinguish better summaries and are comparable with
ROUGE over DUC data; namely,
if $S_1$ and $S_2$ are two summaries of the same DUC document %in DUC
produced by two algorithms,
and the ROUGE score of $S_1$ is higher than that of $S_2$, then it should also be the case under the new measure.
%The new measure is then referred to as ROUGE-compatible.
%To overcome the obstacle when no benchmarks are available to evaluate summaries for documents of particular topics,
%we define a measure based on a word-embedding method to measure the quality of a summary. In particular, we
%to use

%For this purpose, we devise WESM (Word-Embedding Similarity Measure)
%based on Word Mover's Distance (WMD) \cite{wmd}
%to measure word-embedding similarity of the summary and the original document.
Louis and Nenkova \cite{Louis:2009} devised an unsupervised method to
evaluate summarization without human models using
common similarity measures: Kullback-Leibler divergence, Jensen-Shannon divergence, and  cosine similarity.
%to measure similarities between a summary and its original document.
%
%The reason we choose WMD as a baseline similarity measurement is its capability to deal with abstractive summaries.
%
%that human-generated benchmarks
%are abstractive, and
These measures, %similarity measurements such as
%cosine similarity and
as well as the information-theoretic similarity measure \cite{Aslam03},
are meant to measure lexical similarities, which are unsuitable for measuring semantic similarities.

Word embeddings such as Word2Vec can
be used to fill this void %measure similarities from semantic aspects
and
we devise WESM (Word-Embedding Similarity Measure)
based on Word Mover's Distance (WMD) \cite{wmd}
to measure word-embedding similarity of the summary and the original document. WESM is meant to evaluate summaries for new datasets when no human-generated benchmarks are available. WESM has an advantage that it can measure the semantic similarity of documents. We show that WESM correlates well with ROUGE on DUC-02. Thus, WESM may be used as an alternative summarization evaluation method when benchmarks are unavailable.
%we choose WMD to measure semantic similarities. %as a summarization evaluation method.
%are not able to capture abstraction the similarity between an abstractive summary and the original document.
%This means that we would need to consider semantic similarity and context similarity, in addition to lexical similarity.
%WMD is the first successful attempt in this direction.

%that ofsimilar toand
%Moreover, RAKETT also ranks the highest under WEP over both DUC-02 and NewsIR-16.
%over the NewsIR-16 dataset,
% is similar to the order of ROGUE rankings produced by these algorithms over the DUC 2012 dataset.

%in place of ROGUE when human-generated benchmarks are not available for comparisons.
%Our WMD method obtains competitive performance and our summarization system outperforms prior work on ROUGE.

The major contributions of this paper are summarized below:

\vspace*{-3pt}
\begin{enumerate}
\item We present a number of summarization algorithms using topic clustering methods and enhanced keyword rankings by the softplus function,
%over keyword rankings by various
%keyword extraction algorithms and topic clustering methods,
and show that they meet the realtime requirements and outperform all the previously-known summarization algorithms under
the ROUGE measures over DUC-02.
%For instance, our algorithm ET3Rank generates the scores of (49.2, 25.6, 27.5) for (ROUGE-1, ROUGE-2, ROUGE-SU4),
%
%
%the (ROUGE-1, ROUGE-2, ROUGE-SU4) scores over DUC-02 of our algorithm ET3Rank are (49.2, 25.6, 27.5),
%while the previously-reported best results are (49.0, 24.7, 25.8) \cite{parveen2016}.

%over the DUC-02 dataset under ROUGE and they meet the realtime requirement.
%For instance, under the common measures of ROUGE-1, ROUGE-2, and ROUGE-SU4, our ET3Rank algorithm has scores of (49.2, 25.6, 27.5) over DUC-02, while
%the previously-known best scores are (48.1, 24.3, 24.2), generated by Tgraph \cite{ParveenR015}.
%ERAKE (Enhanced Rapid Automatic Keyword Extraction) and
%ERAKE-TT (ERAKE-TextTiling) %an algorithm using word-pair co-occurrences to identify keyword phrases and TextTiling to promote coverage of diverse topics,
%is the state of the art for single-document summarizations.  % superior to an extensive list of summarization algorithms.
%In particular, we show %evaluate the quality and runtime of an extensive list of summarization algorithms and show
%that both ERAKE and ERAKE-TT
%with ERAKE-TT better on ROUGE scores and ERAKE faster on runtime. For example,
%For a long document of 5,000 words, ET3Rank generates a summary of 30\% length in
%ess than 1 second on an average computer.

%combine the above approaches to generate a summary from two aspects, which are central meaning representation and topic diversity coverage with sentence %compression using the dependency tree compression model.
\vspace*{-3pt}
\item We propose a new mechanism WESM as
%
%show that these summarization algorithms
%over DUC-02 have a similar ordering of the WEP scores as that of the ROUGE scores.
%In particular, we show that under WEP, these algorithms have almost the same orderings over the DUC-02 and NewsIR-16 \cite{Signal1M2016} datasets.
%Thus, WEP may serve as
%we may use WEP as a viable
an alternative measurement of summary quality when human-generated benchmarks are unavailable.
%
%\item We de
%
%We test our unsupervised summary approach on datasets from NewsIR-16 ~\cite{Signal1M2016} and Document Understanding Evaluations 2002 ~\cite{DUC:02}. %Experimental results show that our approach outperforms the state-of-the-art results on DUC 2002 with ROUGE scores ~\cite{rouge}.
%\item We first use a novel word embedding method by Word Mover's Distance (WMD) ~\cite{wmd} to evaluate the quality of summaries without reference summaries. %Our experiments show that our approach performs comparable to ROUGE scores.
\end{enumerate}

The rest of the paper is organized as follows:
We survey in Section \ref{sect:work} unsupervised single-document summarization algorithms.
We present in Section \ref{sect:smethod} the details of our summarization algorithms and describe WESM in Section \ref{sect:emethod}. % the  automatic summarization evaluation methods.
We report the results of extensive experiments in Section \ref{sec:experiments} and conclude the paper in
%reports the experimental results of our approaches. Conclusions are given in
Section \ref{sect:conclusion}.

\section{\uppercase{Early Work}}
\label{sect:work}

\noindent Early work on single-topic summarizations can be described in the following three categories: keyword extractions, coverage and diversity optimizations,
and topic clusterings.

\subsection{Keyword extractions}

To identify keywords in a document over a corpus of documents, the measure of term-frequency-inverse-document-frequency (TF-IDF) \cite{salton87} is often used.
%  over
%which is %takes advantage of The preassumption for using TF-IDF is that
%a reasonably sized corpus of documents.
%However, TF-IDF has a shortcoming.
%Otherwise the TF-IDF value of a keyword may just be the term frequency of the keyword in the document.
%To identify keywords in a single document without using
When document corpora are unavailable,
the measure of word co-occurrences (WCO) can produce a %identify keywords with %\cite{Matsuo:03}. %which applies to a single document without a corpus.
comparable performance to TF-IDF over a large corpus of documents \cite{Matsuo:03}.
The methods of TextRank \cite{Mihalcea04} and RAKE (Rapid Automatic Keyword Extraction) \cite{Rose:10}
further refine the WCO method from different perspectives, %. We note that both TextRank and RAKE
which are also sufficiently fast to become candidates %for keyword extractions
for meeting the realtime requirements. %In particular,
%are unsupervised keyword extraction methods with weights.

TextRank %is a graph-based ranking model %for text processing and it can obtain competitive results with state-of-the-art systems developed in these areas.
%Graph-based ranking algorithms are a way of
%for deciding
computes the rank of a word in an undirected, weighted word-graph using a slightly modified PageRank algorithm \cite{Brin:98}.
To construct a word-graph for a given document, first remove stop words and represent each remaining word
as a node, then link two words if they both appear in a sliding window of a small size. Finally,
assign the number of co-occurrences of the endpoints of an edge as a weight to the edge.
%which are based on global information recursively drawn from the entire graph.
%This graph-based ranking model is that of ��voting�� or ��recommendation��. The higher the number of votes for a vertex, the higher the importance of the vertex.

RAKE %assigns a ranking to each word using word pair co-occurrences: It
first removes stop words using a stoplist, and then generates words (including phrases) using a set of word delimiters and %generates
%keyword phrases using
a set of phrase delimiters.
For each remaining word $w$, the degree of $w$ is the frequency of $w$ plus the number of co-occurrences of consecutive word pairs $ww'$ and $w''w$ in the document, where $w'$ and $w''$ are
remaining words.
The score of $w$ is the degree of $w$ divided by the frequency of $w$. We note that
the quality of RAKE also depends on a properly-chosen stoplist, which is language dependent.
%may be easier to determine for some languages and
%harder for other languages.
%The score of a keyword phrase is the summation of
%the scores of the words in the phrase.
%
%. RAKE is superior to WCO because it is simpler and achieves a higher precision rate. We use TextRank and RAKE as our keyword extraction methods for sentence ranking.
%RAKE is a linear-time algorithm, while the runtime of TextRank depends on
%the speed of convergence. Experimental results


\subsection{Coverage and diversity optimization}
%Our work is inspired by the concept of a class of submodular functions for document summarization ~\cite{lin:11}.
%Compared to all the other known extractive summarization algorithms known at the time it was published, Tgraph offered the best ROUGE scores on DUC-02.

 %, Lin and Bilmes \cite{lin:11}
The general framework of selecting sentences gives rise to optimization problems with objective functions
being monotone submodular \cite{lin:11} to promote coverage and diversity.
Among them is an objective function in the form of $L(S) + \lambda R(S)$ with
%
%that is the sum of
%two components.
%$S$ is
a summary $S$ and a coefficient $\lambda \geq 0$, where
$L(S)$ measures the coverage of the summary and $R(S)$ rewards diversity.
We use SubmodularF to denote the algorithm computing this objective function.
%and the algorithm computing it to be described later as SubmodularF.
SubmodularF uses TF-IDF values of words in sentences to compute the cosine similarity of two sentences.
% on the generalized TF-IDF values of the words contained in the respective sentence,
%where the TF value of a word is at the sentence level while the IDF value is still at the document level over a corpus.
%and the IDF value is for the entire corpus.
%The ranking of a sentence $s$ in a document
%is the similarity of $s$ and the document.
%$L(S)$ is measured by the total ranking of the sentences in $S$ with a parameterized upper bound different for each sentence.
%$R(S)$ is measured by first partitioning the document (often using its paragraphs as partitions)
%and then summing up, for each partition, the square root of the total average ranking for each sentence in the partition that is also in $S$.
While
it is NP-hard to maximize a submodular objective function subject to a summary length constraint, the submodularity allows
a greedy approximation with a proven approximation ratio of $1-1/\sqrt{e}$.
%
%Given a document $D$ in a corpus, let $S$ be a set of sentences extracted from
%$${\cal F}(S) = {\cal L}(S) + \lambda{\calR}(S),$$
%where ${\cal L}(S)$
%
%that
%This method
%encourages the summary to be representative of the corpus and positively rewards diversity. They used monotone nondecreasing and submodular functions which is %an efficient scalable greedy optimization scheme.
%However, they

SubmodularF needs labeled data to train the parameters in the objective function to achieve a better summary and it is intended to work on multiple-document summarizations.
While it is possible to work on a single document without a corpus, we note that the greedy algorithm has at least a quadratic-time complexity and it produces a summary with
low ROUGE scores over DUC-02 (see Section \ref{sec:other}),
and so it would not be a good candidate to meet our needs. This also applies to a generalized objective function
%
%
%its ROUGE scores over DUC-02 are much lower than those of RAKETT (see Section \ref{sec:experiments}).
%SubmodularF also incurs much longer time that does not meet the realtime requirement.
%
%The submodularity approach was generalized to include dispersion where the objective
%function
consisting of a submodular component and a non-submodular component \cite{DasguptaKR13}.
%While this generalization provides a slightly better ROUGE-1 score over DUC-04 than SubmodularF, it incurs
%a higher time complexity.

\subsection{Topic clusterings}

Two unsupervised approaches to topic clusterings for a given document have been investigated.
One is TextTiling \cite{hearst:97} and the other is LDA (Latent Dirichlet Allocation) \cite{blei:03}.
TextTiling %is a lighter-weight method,
%Another method we use for topic coverage is TextTiling ~\cite{hearst:97}.
%TextTiling is a technique for
%which
represents a topic as a set of consecutive paragraphs in the document.
It merges adjacent paragraphs that belong to the same topic. TextTiling identifies major topic-shifts based on patterns of lexical co-occurrences and distributions.
LDA computes for each word a distribution under a pre-determined number of topics. %is a heavy-weight method, which
%and represents a topic % would be better
%to identify the topics contained in the document,
%where each topic is represented
%as a set of keywords with larger probabilities. It needs to predetermine the number of topics contained in a document corpus.
LDA is a computation-heavy algorithm
that incurs a runtime too high to meet our realtime requirements.
TextTiling has a time complexity of almost linear, which meets the requirements of efficiency. % and runs much faster than LDA.
%Sentences from the same subtopic won't be selected unless all the subtopics have been covered to ensure the topic coverage.
%We note that LDA incurs a time complexity too high to meet the requirement of realtime summarizations.

\subsection{Other algorithms}
\label{sec:other}

Following the general framework of selecting sentences to meet the requirements of topic coverage and diversity,
a number of unsupervised single-document summarization algorithms have been devised.
The most notable is $CP_3$ \cite{parveen2016}, which produces the best ROUGE-1 (R-1), ROUGE-2 (R-2), and ROUGE-SU4 (R-SU4) scores on DUC-02
among all early algorithms,
including
%is an extractive single-document summarization algorithm devised recently.
%is the most recent algorithm
%, which
%offers the better ROUGE scores over DUC-02 compared to an extensive list of algorithms,
%including
Lead \cite{ParveenR015}, DUC-02 Best, TextRank, LREG \cite{ChengL16a},
Mead \cite{radev2004}, $ILP_{phrase}$ \cite{woodsend2010}, URANK \cite{wan2010}, UniformLink \cite{WanX10}, Egraph + Coherence \cite{Parveen015},
Tgraph + Coherence (Topical Coherence for Graph-based Extractive Summarization) \cite{ParveenR015},
NN-SE \cite{ChengL16a}, and SubmodularF.

$CP_3$ maximizes importance, non-redundancy, and pattern-based coherence of sentences to generate a coherent summary using ILP. 
It computes %considers
the ranks of selected sentences for the summary by the Hubs and Authorities algorithm (HITS) \cite{kleinberg1999}, %(will add a cite).
%Non-redundancy represents if the summary
and ensures that each selected sentence has unique information. % in every sentence.
It then uses mined patterns to extract sentences if the connectivity among nodes in the projection graph matches the connectivity among nodes in a coherence pattern. Because of space limitation, we omit the descriptions of the other algorithms.

Table \ref{CP3} shows the comparison results, where the results for SubmodularF
is obtained using
%To compare with SubmodularF,
%we use
the best parameters trained on DUC-03 \cite{lin:11}.
%%%table to be inserted here
%Our computation show that the ROUGE-1, ROUGE-2, and ROUGE-Su4 scores for SubmodularF are (39.6, 16.9, 17.8), which are
%much lower than those of $CP_3$.
%$CP_3$ \cite{parveen2016} is the most recent algorithm that has the best results reported over DUC-02 so far.
Thus, to demonstrate the effectiveness of our algorithms, we will compare our algorithms with only $CP_3$ %under these common ROUGE measures
over DUC-02.
\begin{table}[h]
\begin{center}
\caption{\label{CP3} ROUGE scores (\%) on DUC-02 data.}
\begin{tabular}{l|c|c|c}
\hline
\bf Methods & \bf R-1 & \bf R-2 & \bf R-SU4 \\
\hline
Lead & 45.9 & 18.0 & 20.1 \\
DUC 2002 Best & 48.0 & 22.8 & \\
TextRank & 47.0 & 19.5 & 21.7 \\
LREG & 43.8 & 20.7 & \\
Mead & 44.5 & 20.0 & 21.0 \\
$ILP_{phrase}$ & 45.4 & 21.3 & \\
URANK & 48.5 & 21.5 & \\
UniformLink & 47.1 & 20.1 &   \\
Egraph + Coh. & 48.5 & 23.0 & 25.3  \\
Tgraph + Coh. & 48.1 & 24.3 & 24.2 \\
NN-SE & 47.4 & 23.0 & \\
SubmodularF & 39.6 & 16.9 & 17.8 \\
$\bm{CP_3}$& \bf 49.0 & \bf 24.7 & \bf 25.8  \\
\hline
\end{tabular}

\end{center}
\end{table}

Solving ILP, however, is
time consuming even on documents of a moderate size, for ILP is NP-hard.
%
%to compute the optimization problem, and ILP is NP-hard.
Thus, $CP_3$ does not meet the requirements of time efficiency. We will need to investigate new methods.

%Important function
 %It uses coherence patterns with 3 nodes.
%
\begin{comment}
Tgraph is based on a weighted graphical representation of documents using LDA topic modeling. It %Tgraph + Coherence
uses ILP to optimize the objectives of sentence importance, coherence, and non-redundancy simultaneously. $CP_3$ optimizes Tgraph % Tgraph has considered simultaneously
using Mixed Integer Programming.
\end{comment}
%Because $CP_3$ offers %and Tgraph are the algorithms with
%the best ROUGE scores and the space is limited,
%
%mentioned above are omitted because of space limitation.
%We will be only
%and it suffices to compare our algorithms with only $CP_3$ under the common ROUGE measures over DUC-02 to demonstrate the effectiveness of our algorithms. %with $CP_3$. % and Tgraph.
%Tgraph, however, incurs much higher time complexity because of LDA (see Section \ref{sec:experiments}).
%, which does not need the realtime requirement.
%Moreover, we will show in Section \ref{sec:experiments} that its ROUGE scores on DUC-02 is not as good as RAKETT.
%Our method is superior %resulting in higher ROUGE scores and our summary evaluation methods leading to higher WMD scores on single documents without development sets.

%Another approach for extractive single-document summarization is called

%
%compared ROUGE scores with state-of-the-art results on DUC-02 data. Our experiments show that our method has higher ROUGE scores than Tgraph and other state-of-the-art methods such as TextRank,
%are other and so on using the same data. We also obtain comparable results from our summary evaluation methods by WMD.

\begin{comment}
ROUGE has been widely accepted for the evaluation of summaries. It includes measures to automatically determine the quality of a summary by comparing it to ideal summaries created by humans. These measures count the number of overlapping units between summary to be evaluated and the ideal summaries created by humans. However, the performance of the ROUGE method fully depends on the quality of reference summaries made by humans. Thus, it cannot ensure similarity between the original document and the summary to be evaluated. ROUGE does not take topic diversity coverage into account. WMD is a novel distance function between text documents. This work is based on word embeddings that learn semantically meaningful representations for words from local co-occurrences in setences. We used WMD to measure the similarity to the original document and take topic diversity coverage into consideration by comparing different paragraphs.
\end{comment}


\section{\uppercase{Our Methods}}

\label{sect:smethod}

\begin{comment}
\subsection{Sentence Ranking}
\label{ssect:sr}
The summary should contain only important sentences. We find all the keywords with positive numerical scores made from RAKE ~\cite{Rose:10} and TextRank  ~\cite{Mihalcea:04} and give a score to every sentence based the number of keywords it contains. Following Shao and Wang ~\shortcite{Shao:16}, we use their central sentence extraction algorithm for ranking sentences by importance.
Assume that a sentence contains $n$ keywords $w_1, \cdots, w_n$, and $w_i$ has a positive score $s_i$.

We use the power of 2 to amplify the differences of the rankings and the ranking score of the sentence is as follows:
\begin{equation} \label{rank1}
\mbox{Rank} = \sum_{i=1}^{n}2^{s_{i}}
\end{equation}

After we calculate the ranking scores of all the sentences, we can order the sentences by their scores. We try to pick as many sentences with high scores as possible. From high to low scoring sentences, we put each sentence into the summary based on different topic diversity coverage constraints in Section \ref{ssect:tdc}.


\subsection{Compression Constrains}
\label{ssect:cc}
\begin{figure}[h]
\includegraphics[width=3in]{fig1_1.png}
\includegraphics[width=3in]{fig1_2.png}
%\DeclareGraphicsExtensions.
\caption{The grammatical relations and trimmed dependency tree using DTCM for sentence ``Market concerns about the deficit has hit the greenback''}%: $\Lambda$ restricts the $\theta$ to the labels a document belongs to}
\label{fig:fig1}
\end{figure}

Our goal is to be able to compress sentences so we can pack more information into a summary. We use the Dependency tree compression model (DTCM) in Shao and Wang ~\shortcite{Shao:16} to do compressions. A dependency tree can provide relations between each word in the sentence. We use the Standord Dependency Parser (SDP) \footnote{\tt http://nlp.stanford.edu/software/\\ \-\hspace{.75cm} stanford-dependencies.shtml} to obtain grammatical relations between words in a sentence. This model follows a set of empirical rules to specify what can or cannot be trimmed. DTCM can delete all the unnecessary branches from sematic aspects. Figure \ref{fig:fig1}shows one example sentence ``Market concerns about the deficit has hit the greenback'' . The compressed sentence is ``Market concerns about deficit hit greenback''.
\end{comment}

%\subsection{Topic Diversity Coverage Constrains}
%\label{ssect:tdc}
\noindent We use TextRank and RAKE to obtain initial ranking scores of keywords, and
%In general, TextRank ranking scores are small while RAKE ranking scores are much larger.
use the softplus function \cite{softplus}
\begin{equation}\label{eq1}
sp(x) = \ln (1+e^x)
\end{equation}
to
enhance keyword rankings to favor sentences that are more important. % that are more important.
%enhance the rankings of sentences
%that are more important.

\subsection{Softplus ranking}

%as compared to the direct summation of the ranking scores of the keywords contained in the sentence.
Assume that after filtering, a sentence $s$ consists of $k$ keywords $w_1, \cdots, w_k$, and $w_i$ has a ranking score $r_i$
produced by TextRank or RAKE. 
Following Shao and Wang ~\cite{Shao:16}, we use their central sentence extraction algorithm for ranking sentences by importance as $\mbox{Rank}(s)$.
We can rank $s$ using one of the following two methods:
\begin{equation}\label{eq2}
{Rank}(s) = \sum_{i=1}^{k} r_i
\end{equation}
\begin{equation}\label{eq3}
{Rank}_{sp}(s) = \sum_{i=1}^{k} sp(r_i)
\end{equation}
\begin{comment}
\[
\mbox{Rank}(s) = \sum_{i=1}^{k} r_i;~~
\mbox{Rank}_{sp}(s) = \sum_{i=1}^{k} sp(r_i).
\]
\end{comment}
%A straightforward ranking of $s$ is to sum up the scores of the keywords contained in it; that is,
%$$\mbox{Rank}(s) = \sum_{i=1}^{k} r_i.$$
%We use the softplus function to enhance the ranking of $s$ as follows:
%$$\mbox{Rank}_{sp}(s) = \sum_{i=1}^{k} sp(r_i).$$
%We use DTRank (Direct TextRank) and DRAKE (Direct RAKE)
%to refer to this method of computing sentence ranking.
Let DTRank (Direct TextRank) and ETRank (Enhanced TextRank) denote
the methods of ranking sentences using, respectively,
$\mbox{Rank}(s)$ and $\mbox{Rank}_{sp}(s)$ over TextRank keyword rankings,
and
%
%and DRAKE (Direct RAKE) denote
%the method of ranking sentences using $\mbox{Rank}(s)$
%over TextRank and RAKE keyword rankings, respectively;
%and
DRAKE (Direct RAKE) and ERAKE (Enhanced RAKE)
%ETRank (Enhanced TextRank) and ERAKE (Enhanced RAKE)
to denote the methods of ranking sentences
using, respectively, $\mbox{Rank}(s)$ and $\mbox{Rank}_{sp}(s)$ over RAKE keyword rankings.

%To enhance the ranking of a sentence that is more important,
%We use the softplus function to enhance the ranking of $s$ as follows:
%$$\mbox{Rank}_{sp}(s) = \sum_{i=1}^{k} sp(r_i).$$
%
%
%If $r_i$ is produced by TextRank, and respectively by RAKE, then the ranking score of $s$ is denoted by
%Then the softplus ranking of $s$ is computed by % $\mbox{Rank}(s)$ and $\mbox{Rank}_R(s)$, which are computed by
%\begin{eqnarray*}
%$$\mbox{Rank}(s) = \sum_{i=1}^{k} \ln(1+e^{r_i}).$$
%$\mbox{Rank}_R(s) = \sum_{i=1}^{k} 2^{r_i}$.
%\end{eqnarray*}
%We refer to TextRank and RAKE using this measure of sentence ranking as ETRank (Enhanced TextRank) and ERAKE (Enhanced RAKE).
%We use ETRank (Enhanced TextRank) and ERAKE (Enhanced RAKE) to denote this method of computing sentence ranking based
%on TextRank and RAKE keyword rankings, respectively.

The softplus function is helpful because %enhance the ranking of a more important sentence,
%let $sp(x) = \ln (1+e^x)$. We note that
%we note that
when $x$ is a small positive number, $sp(x)$ increases the value of $x$ significantly (see Figure \ref{fig:softplus})
and when $x$ is large, $sp(x) \approx x$.
\begin{figure}[h]
\centering
\includegraphics[width=2.5in]{softplus1.png}
%\DeclareGraphicsExtensions.
\caption{Softplus function $\ln(1+e^x)$.} %: $\Lambda$ restricts the $\theta$ to the labels a document belongs to}
\label{fig:softplus}
\end{figure}
In particular,
given two sentences $s_1$ and $s_2$, suppose that $s_1$ has a few keywords with high rankings and the rest of the keywords with low rankings,
while $s_2$ has medium rankings for almost all the keywords. In this case, we would consider $s_1$ more important than $s_2$. However,
%because the keywords in $s_1$ gave low ranking scores,
we may end up with $\mbox{Rank}(s_1) < \mbox{Rank}(s_2)$.
To illustrate this using a numerical example, assume that $s_1$ and $s_2$ each consists of 5 keywords, with
original scores (sc) and softplus scores (sp) given in the following table \ref{example}.

\medskip

%The ranking of $s_1$ can be enhanced after using softplus. For example, the following two sentences are extracted from a news article using TextRank to rank keywords, with
%$s_1$ being more important than $s_2$ because $s_1$ specifies the name, strength, and direction of the hurricane, as well as the name of the place it is expected to hit.
%\begin{itemize}
%\item[$s_1$:] {\sf Hurricane Gilbert is heading toward Jamaica with 100 mph winds.}
%\item[$s_2$:] {\sf A hurricane warning has been issued for the island.}
%\end{itemize}
%The ranking score of each keyword is depicted in Table \ref{table:example}, from which we
%can see that $s_2$ is selected without using softplus. After using softplus, $s_1$ is selected as it should be.

\noindent
\begin{table}[h]
\begin{center}
\caption{\label{example} Numerical examples with given sc and sp scores.}
\begin{tabular}{l||c|c|c|c|c||c}
\hline
$s_1$ & $w_{11}$ & $w_{12}$ & $w_{13}$ & $w_{14}$ & $w_{15}$&   Rank \\
\hline
sc & 2.6 & 2.2 & 2.1 & 0.3 & 0.2 & 7.4\\
sp & 2.67 & 2.31 & 2.22 & 0.85& 0.80 & \bf 8.84\\
\hline
$s_2$ & $w_{21}$ & $w_{22}$ & $w_{23}$ & $w_{24}$ & $w_{25}$ & \\
\hline
sc & 1.6 & 1.5 & 1.5 & 1.5 & 1.4  & \bf 7.5  \\
sp & 1.78& 1.70& 1.70& 1.70& 1.62&  8.51 \\
\hline
\end{tabular}
\end{center}
\end{table}

\medskip
Sentence $s_1$ is more important than $s_2$
because it contains three keywords of much higher ranking scores than those of $s_2$.
However, $s_2$ will be selected without using softplus. After using softplus, $s_1$ is selected as it should be.

\begin{comment}
For example, Hurricane Gilbert is heading toward Jamaica with 100 mph winds.
����1��(0.7, 0.6, 0.01, 0.02, 0.03)
softplus:(1.10318604889, 1.03748795049, 0.698159680508, 0.703197179727, 0.708259676341)
softplusֵ�ĺͣ�4.25029053595
Ȩ�غͣ�1.36


A hurricane warning has been issued for the island.
����2��(0.2, 0.2, 0.3, 0.3, 0.4)
softplus:(0.798138869382, 0.798138869382, 0.854355244469, 0.854355244469, 0.9130152524)
softplusֵ�ĺͣ�4.2180034801
Ȩ�غͣ�1.4
\end{comment}

%\medskip
For a real-life example, consider the following two sentences from an article in DUC-02:
\vspace*{-1pt}
\begin{itemize}
\item[$s_1$:] {\small\sf Hurricane Gilbert swept toward Jamaica yesterday with 100-mile-an-hour winds, and officials issued warnings to residents on the southern coasts of the Dominican Republic, Haiti and Cuba.}
\vspace*{-1pt}\item[$s_2$:] {\small\sf Forecasters said the hurricane was gaining strength as it passed over the ocean and would dump heavy rain on the Dominican Republic and Haiti as it moved south of Hispaniola, the Caribbean island they share, and headed west.}
\end{itemize}
We consider $s_1$ more important as it
specifies the name, strength, and direction of the hurricane, the places affected, and the official warnings.
Using TextRank to compute
keyword scores, we have $\mbox{Rank}(s_1) = 1.538 < \mbox{Rank}(s_2) = 1.603$, which returns a less important sentence $s_2$. After computing
softplus,
we have $\mbox{Rank}_{sp}(s_1) = 8.430 > \mbox{Rank}_{sp}(s_2) = 7.773$; the more important sentence $s_1$ is selected.

Note that not any exponential function would do the trick. What we want is a function to return roughly the same value as the input when the input is large, and a significantly larger value than the input when the input is much less than 1. The softplus function meets this requirement.

\begin{comment}
(u'Forecasters said the hurricane was gaining strength as it passed over the ocean and would dump heavy rain on the Dominican Republic and Haiti as it moved south of Hispaniola, the Caribbean island they share, and headed west.', [1.6033724386659647, u'hurricane#caribbean island#south#republic#forecasters#haiti#west#headed#heavy rain#dominican#', '0.376361942128#0.222770119935#0.152622310017#0.144567553334#0.140516484532#0.140516484532#0.117560427986#0.117560427986#0.11182141385#0.0790752743651#'])

(u' Hurricane Gilbert swept toward Jamaica yesterday with 100-mile-an-hour winds, and officials issued warnings to residents on the southern coasts of the Dominican Republic, Haiti and Cuba.', [1.538394147633235, u'hurricane#south#coast#republic#jamaica yesterday#haiti#southern coasts#mile#winds#dominican#warnings#', '0.376361942128#0.152622310017#0.152622310017#0.144567553334#0.142542018933#0.140516484532#0.117560427986#0.0824985459553#0.0790752743651#0.0790752743651#0.0709520059994#'])
assume that sentence
$s$ consists of five keywords: $(w_1, w_2, w_3, w_4, w_5)$ whose corresponding ranking scores
are
%How we select sentences is critical. In general,
We would want to select sentences with
ranking scores as high as possible, while avoiding selections of multiple sentences from one topic
and no sentence at all from another topic.
\end{comment}


\subsection{Topic clustering schemes}

We consider four topic clustering schemes: TCS, TCP, TCTT, and TCLDA.
%The first one is called naive TCDS, denoted by
\begin{enumerate}
\item TCS %(TC based on sentences)
selects sentences without checking topics.
\item TCP %(TC based on paragraphs)
treats each paragraph as a separate topic.
%sentences based on their scores and also ensure the selected sentences distribute different paragraphs.
%For example, we first select a sentence with the highest ranking score (break ties arbitrarily) and place the paragraph number that contains this
%sentence into KTS. We then select the second highest score sentence. If the second sentence is not in the KTS, we put the second one into the summary and the %paragraph number into the KTS. Otherwise, we skip it to the next sentence. The following parts are the same as the basic procedure of TDCC.
\item TCTT %(TC based on TextTiling)
%The third TDCC is called TextTiling TDCC (t-TDCC). TextTiling ~\cite{hearst:97} is used to
partitions a document into a set of multi-paragraph %topical
segments using TextTiling.
%It may merge or divide paragraphs into several topics. In t-TDCC, we try not to pick sentences from the same topic. The procedure is similar to the basic TDCC.
\item TCLDA %(TC based on LDA)
%The last TDCC is called LDA TDCC (l-TDCC). LDA ~\cite{blei:03} is a model for topic modeling where topic probabilities are assigned words in documents.
computes a topic distribution for each word using LDA. %To use LDA we must first fix the number of topics for a document corpus.
%The probabilities can be sued to measure the semantic relatedness between words. The number of topics in a document should be set by us.
We set the number of topics from 5 to 8 depending on the length of the document.
Assume that a document contains $K$ topics ($5 \leq K \leq 8$) and the topic $j$ consists of $k_j$ words $w_{1j}, \cdots, w_{k_j,j}$, where $1 \leq j \leq K$
and
$w_{ij}$ has a probability $p_{ij} > 0$. For a document with $n$ sentences $s_1, \cdots, s_n$,
%if $s_z$ does not contain word $w_{ij}$, then the probability $p_{ij}$ in the topic $j$ is set to $1$, otherwise is $p_{ij}$.
we use the following maximization to determine which topic $t_z$ the sentence $s_z$ belongs to ($1 \leq t \leq K$):
\begin{equation} \label{eq4}
t_z = \argmax_{1 \leq j\leq k}\bigg(\prod_{i:w_{ij} \in s_z} {p_{ij}}\bigg)
\end{equation}
\end{enumerate}




\begin{comment}
Figure \ref{fig:fig3} shows the bipartite topical graph of TTCD, where $\bm{w}_j = (w_{1i}, \cdots, w_{k_j,j})$ is the vector words under topic $j$
($1 \leq j \leq m$), $s_z$ ($1 \leq z \leq n$) is a sentence in a document, $p_j$ ($1 \leq j \leq m$) is the product of possibilities in $wi$. $tz$ is the topic for the sentence $sz$ with the highest possibility. From equation \ref{topic}, we know that each sentence belongs to certain topic. Then we can use the basic TDCC to ensure the summary cover topics as many as possible.

 \begin{figure}[h]
\includegraphics[width=3in]{fig3.png}
%\DeclareGraphicsExtensions.
\caption{Bipartite topical graph of TTCD}%: $\Lambda$ restricts the $\theta$ to the labels a document belongs to}
\label{fig:fig3}
\end{figure}
\end{comment}

\subsection{Summarization algorithms}
\label{ssect:sa}

%The goal of topic diversity coverage constraints (TDCC) is to avoid sentences coming from the same topic, which means to make the summary cover as many topics %as possible.
The length of a summary may be specified by users,
either as a number of words or as a percentage of the number of characters of the original document.
By a ``30\% summary'' we mean that the number of characters of the summary does not exceed 30\% of that of the original document.

Let $L$ be the summary length (the total number of characters) specified by the user and $S$ a summary.
If $S$ consist of $m$ sentences
%Assume that a document contains $m$ topics and $n$ sentences
$s_1, \cdots, s_m$, and the number of characters of $s_i$ is $\ell_i$, then the following inequality must hold:
$\sum_{i=1}^m \ell_i \leq L.$
%The summary must satisfy additional length requirement:
%\begin{equation} \label{length}
%\mbox{Len(summary)} \geq \sum_{i=1}^{n}{l_{i}}
%\end{equation}

Depending on which sentence-ranking algorithm and which topic-clustering scheme to use, we have eight combinations
using ETRank and ERAKE, and eight combinations using DTRank and DRAKE, shown in Table \ref{algorithm_names}.
%
%ESTRank, EPTRank, ET3Rank, ELDATRank when using ETRank to compute sentence rankings;
%ESRAKE, EPRAKE, ET2RAKE, and ELDARAKE when using ERAKE to compute sentence rankings. %extract keywords.
For example, ET3Rank (Enhanced TextTiling TRank) means to use $\mbox{Rank}_{sp}(s)$ to rank sentences and
TextTiling to compute topic clusterings, and
%
%Likewise, using DTRank and DRAKE we have
%the following combinations: STRank, PTRank, T3Rank, LDATRank,
%SRAKE, PRAKE, T2RAKE, LDARAKE. For example,
T2RAKE (TextTiling RAKE)
means to use $\mbox{Rank}(s)$ rank sentences over RAKE keywords and TextTiling
to compute topic clusterings. %The description of all the algorithms are shown in Table \ref{algorithm_names}.

\begin{table}[h]
\begin{center}
\caption{\label{algorithm_names} Description of all the Algorithms with different sentence-ranking (S-R) and topic-clustering (T-C) schemes.}
\begin{tabular}{l|c|c}
\hline
\bf Methods & \bf S-R & \bf T-C \\
\hline
ESTRank & ETRank & TCS \\
EPTRank & ETRank & TCP \\
ET3Rank & ETRank & TCTT \\
ELDATRank & ETRank & TCLDA \\
\hline
ESRAKE & ERAKE & TCS \\
EPRAKE & ERAKE & TCP \\
ET2RAKE & ERAKE & TCTT \\
ELDARAKE & ERAKE & TCLDA \\
\hline
STRank & DTRank & TCS \\
PTRank & DTRank & TCP \\
T3Rank & DTRank & TCTT \\
LDATRank & DTRank & TCLDA \\
\hline
SRAKE & DRAKE & TCS \\
PRAKE & DRAKE & TCP \\
T2RAKE & DRAKE & TCTT \\
LDARAKE & DRAKE & TCLDA \\
\hline
\end{tabular}
\end{center}
\end{table}

All algorithms follow the following procedure for selecting sentences:

\vspace*{-2pt}
\begin{enumerate}
\item Preprocessing phase
\begin{enumerate}
\vspace*{-1pt}\item Identify keywords and compute the ranking of each keyword.
\vspace*{-1pt}\item Compute the ranking of each sentence.
\end{enumerate}
\vspace*{-2mm}
\item Sentence selection phase
\begin{enumerate}
\vspace*{-1pt}\item Sort the sentences in descending order of their ranking scores. % according to the underlying keyword extraction algorithm.

\vspace*{-1pt}\item Select sentences one at a time with a higher score to a lower score.
Check if the selected sentence $s$ belongs to the known-topic set (KTS) according to the underlying
topic clustering scheme, where KTS is a set of topics from sentences placed in the summary so far. If $s$ is in KTS, then discard it; otherwise, place $s$ into the summary and its topic into KTS.
\vspace*{-1pt}\item Continue this procedure until the summary reaches its length constraint.

\vspace*{-1pt}\item If the number of topics contained in the KTS is equal to the number of topics in the document,
% (assume a document contains $m$ topics), we
empty KTS and repeat the procedure from Step 1. %to empty and select the unpicked sentences from the start.
\end{enumerate}
\end{enumerate}

%Given a keyword extraction algorithm and a topic clustering scheme, it is straightforward to replace them in the algorithm. For example,
%ET3Rank use ETank to compute the ranking of each sentence and use TextTiling for topic clustering.

Figure \ref{fig:fig5} shows an example of 30\% summary generated by ET3Rank on an article in NewsIR-16.
\begin{figure*}[t]
\includegraphics[width=6in]{fig5.png}
%\DeclareGraphicsExtensions.
\caption{An example of 30\% summary of an article in NewsIR-16 by ET3Rank, where the
original document is on the left and the summary is on the right.}
\label{fig:fig5}
\end{figure*}


\begin{comment}
Figure \ref{fig:fig2} depicts this procedure.

\begin{figure}[h]
\includegraphics[width=3in]{fig2.png}
%\DeclareGraphicsExtensions.
\caption{The general procedure of extracting sentences}%: $\Lambda$ restricts the $\theta$ to the labels a document belongs to}
\label{fig:fig2}
\end{figure}

Our basic procedure of selecting sentences is as follows: (1) Sort the sentences in descending order of their ranking scores. (2) Select sentences one at a time with a higher score to a lower score. (3) Check if the selected sentence $s$ belongs to the known topic set (KTS), which is a set of topics from sentences placed in the summary so far. If $s$ is in KTS, then discard it; otherwise, place $s$ into the summary and its topic into KTS. (4) Continue this procedure until the summary reaches its length constraint. (5) If the number of topics contained in the KTS is equal to the number of topics in the document,
% (assume a document contains $m$ topics), we
empty KTS and repeat the procedure from Step 1. %to empty and select the unpicked sentences from the start.
Figure \ref{fig:fig2} depicts this procedure. % basic procedure of TDCC.


In this section, we introduce our four different summarization algorithms based on the above TDCC. We name summarization methods as n-SA, p-SA, t-SA and l-SA based on n-TDCC, p-TDCC, t-TDCC and l-TDCC. The only difference between these four algorithms is used different TDCC and other parts are the same. For utilizing TextRank or RAKE as the sentence ranking method, we can divide four summarization algorithms into eight, which are TextRankN, RAKEN, TextRankP, RAKEP, t-SA-T, RAKETT, TextRankLDA and RAKELDA. We describe our algorithm in general as follows:
\begin{itemize}
\item Get keywords set with weights from TextRank or RAKE.
\item Split the original text into sentences.
\item Find all the keywords in each sentence and use sentence ranking algorithm in Section \ref{ssect:sr} to get the sentence score.
\item Sort the sentences by their scores.
\item Iterate the sentences from high to low score and select the sentences based on TDCC in Section \ref{ssect:tdc} until it reaches the limitation of the summary.
\item Compress the selected sentences by algorithm in Section \ref{ssect:cc}.
\item Order the selected trimmed sentences by their order in the original text to generate the final summary.
\end{itemize}
\end{comment}
\section{\uppercase{A Word-Embedding Measurement of Quality}}
\label{sect:emethod}

%\subsection{Word2Vec Embedding}
%\label{ssec:Word2Vec}
\noindent Word2vec \cite{mikolov13,mikolov2013}
%introduced a novel word-embedding procedure Word2Vec. Their model
is an NN model that learns a vector representation for each word contained in a corpus of documents.
%In addition, they used the skip-gram model with neural network architecture.
The model consists of an input layer, a projection layer, and an output layer to predict nearby words in the context. In particular,
a sequence of $T$ words $w_1, \cdots, w_T$ are used to train a Word2Vec model for maximizing the probability of neighboring words:
\begin{equation} \label{eq5}
{\frac{1}{T}\sum_{t=1}^{T}{\sum_{j\in b(t)}{\log p(w_j|w_t)}}}
\end{equation}
where $b(t) = [t-c, t + c]$ is the set of center word $w_t$'s neighboring words, $c$ is the size of the training context, and $p(w_j|w_t)$ is defined by the softmax function.
%(See more details in ~\cite{mikolov2013}.)
Word2Vec can learn complex word relationships if it trains on a very large data set.
%A commonly cited example is that vec(king) + vec(man) - vec(woman) $\approx$ vec(queen).

\subsection{Word Mover's Distance}
\label{ssec:wmd}
Word Mover's Distance (WMD) \cite{wmd} uses Word2Vec as a word embedding representation method.
It measures the dissimilarity between two documents and calculates the minimum cumulative distance to ``travel'' from the embedded words of one document to the other. Although two documents may not share any words in common, WMD can still measure the semantical similarity by considering their word embeddings, while other bag-of-words or TF-IDF methods only measure the similarity by the appearance of words. A smaller value of WMD indicates that the two sentences are more similar.

\begin{comment}
 \begin{figure}[h]
\includegraphics[width=3in]{fig4.png}
%\DeclareGraphicsExtensions.
\caption{The WMD metric on two sentences S1, S2 compared with the query sentence Q \cite{wmd}}%: $\Lambda$ restricts the $\theta$ to the labels a document belongs to}
\label{fig:fig4}
\end{figure}
Figure \ref{fig:fig4}, extracted from \cite{wmd}, shows an example of the WMD metric on two sentences $S_1$, $S_2$ compared with the query sentence $Q$.
The arrows represent flow between two words, which are labeled with their distance costs.
For each sentence, stop words are removed. Comparing the main components one by one and adding their distance contributions together, we obtain the WMD of two sentences. The lower the value of WMD, the more similar two sentences are. Intuitively, traveling from $Illinois$ to $Chicago$ is much closer than is $Japan$ to $Chicago$. Also, Word2Vec embedding generates the vector vec ($Illinois$) closer vec ($Chicago$) than vec ($Japan$). Thus, sentence $S_1$ to query $Q$ is more similar than sentence $S_2$ to query $Q$.
\end{comment}

\subsection{A word-embedding similarity measure}

Based on WMD's ability of measuring the semantic similarity of documents, %we design innovative methods to evaluate the quality of the summary.
we propose a summarization evaluation measure WESM (Word-Embedding Similarity Measure). %One is based on the original document, denoted by WED and the other is based on paragraphs, denoted by WEP.
Given two documents $D_1$ and $D_2$, let $\mbox{WMD}(D_1,D_2)$ denote
the distance of $D_1$ and $D_2$.
Given a document $D$, assume that it consists of $\ell$ paragraphs $P_1, \cdots, P_\ell$.
Let $S$ be a summary of $D$.
We compare the word-embedding similarity of a summary $S$ with $D$ using WESM$(S,D)$ as follows:
\begin{equation} \label{eq6}
\mbox{WESM}(S,D) = \frac{1}{\ell}\sum_{i=1}^{\ell} \frac{1}{1 + \mbox{WMD}(S,P_i)}
\end{equation}
%$s$ is system summary and $t$ is original text. $wmd(s, t)$ is the WMD distance between the system summary and the original text, and the result we can obtain from Section \ref{ssec:wmd}.
The value of WESM$(S,D)$ %(Word-Embedding Document comparison) %we can learn that the range of WMD-o valu
is between 0 and 1. Under this measure, the higher the WESM$(S,D)$ value, the more similar $S$ is to $D$. %the original text.

%\paragraph{WEP similarities}

%WED takes semantic aspects into account.
 %We calculate the sum of WED scores with each paragraph of the original text compared with the system summary and then take the average of the sum.
%Assume that a document $D$ consists of $\ell$ paragraphs $P_1, \cdots, P_\ell$. %$t$ is the original text.
%Define WEP$(S,D)$ as follows:
% \begin{equation*} \label{wmd-p}
%\mbox{WEP}(S,D) = \frac{\sum_{i=1}^{\ell}\mbox{WED}(S,P_i)}{\ell}.
%
%{\frac{1}{1+\mbox{WMD}(S,P_{i})}}}{n}
%\end{equation*}
%From the above formula, we can conclude that the range of the value of WEP is from 0 to 1.
%The value of WEP (Word-Embedding Paragraph comparison) %we can learn that the range of WMD-o valu
%is between 0 and 1. The higher the WEP value, the more similar the summary is with the original text.
%We experimented on WMD-o and WMD-p with Word2Vec using different training sets and use them to evaluate eight summarization methods we mentioned in Section %\ref{ssect:sa} compared with ROUGE score %in Section \ref{sec:experiments}.
%Note that WEP measures %we also think of
%the influence of topic diversity to a summary.

\begin{comment}
\subsection{Absolute vs. relative measurements}
We note that different training sets may cause WMD to produce different ranges of values, and
so unless we fix a standard dataset to train Word2Vec,
we do not have a standard base for evaluating the absolute qualities of summaries using WESM. %by different training sets.
However, we can use it to evaluate relative qualities by
the score orderings.
%compare various summarizations over the same training set by looking at the ordering of their values.
%because the relative rank of summarization evaluated by WED does not change.

%On the other hand, we note that  trained on different datasets preserve similar orderings (see Sections 5.2 and 5.3), which provides
%a level of assurance of robustness.
%
% we may train WED on one dataset and use it to measure summaries produced by different algorithms on a different dataset;
%regardless what the training dataset is used, the ordering of WED scores remains relatively the same (see analysis in Section \ref{}).
\end{comment}

\section{\uppercase{Numerical Analysis}}
\label{sec:experiments}

%We describe the datasets in our experiments in the next section. %and report empirical results in this section.
%
%\subsection{Datasets}

\noindent We evaluate the qualities of summarizations using the DUC-02 dataset \cite{DUC02}
and the NewsIR-16 dataset \cite{Signal1M2016}.
DUC-02 consists of 60 reference sets, each of which consists of a number of documents, single-document summary benchmarks, and multi-document abstracts/extracts.
The common ROUGE recall measures of ROUGE-1, ROUGE-2, and ROUGE-SU4 are used to compare the quality of summarization algorithms over DUC data.
%By using DUC-02 data, we can evaluate our methods with other state-of-the-art summarization methods using ROUGE.
%DUC-02 contains single-document summary benchmarks.
NewsIR-16 consists of 1 million articles %that are mainly
from English news media sites and blogs.
%The average length of an article is 405 words.

We use various software packages to implement TextRank (with window size = 2) \cite{TRurl}, RAKE \cite{RAurl}, TexTiling \cite{TTurl}, LDA and Word2Vec \cite{gen}.
% from \url{https://github.com/summanlp/textrank},
%\url{https://github.com/aneesha/RAKE},
%url{https://pypi.python.org/pypi/nltk},
%and \url{https://pypi.python.org/pypi/gensim}.
%
%LDA��Word2Vec: �õ�gensim python������
%https://pypi.python.org/pypi/gensim
%TextTiling�õ���nltk������
%https://pypi.python.org/pypi/nltk
%
%We generate summaries by our summarization algorithms on NewsIR-16 dataset
%and evaluate
%evaluated by WED and WEP to find the summarization method with the best performance.

We use the existing Word2Vec model trained on English Wikipedia \cite{wiki},
%and the other on the GoogleNews-vectors model\footnote{\tt https://github.com/mmihaltz/Word2Vec-\\ \-\hspace{.75cm} GoogleNews-vectors} by genism.
%The English Wikipedia dataset
which consists of 3.75 million articles formatted in XML. The reason to choose this dataset is
for its large size and the diverse topics it covers.
%and the GoogleNews-vectors is the pre-trained Google News corpus that contains 3 million 300-dimension English word vectors.

%To use the WED and WEP measures, we need to train Word2Vec models or use pre-trained Word2Vec models.
% on different English training datasets.
%to present that
%We note that although the ranges of WMD values may differ on various datasets,
%we can still compare the quality of summaries
%for they present the
%
%. % and acquire comparable results with ROUGE.
%In other words, WED and WEP can evaluate the quality of summarization methods using the same dataset and different datasets can still obtain similar results.
%In particular, we trained our Word2Vec models over English Wikipedia\footnote{\tt https://dumps.wikimedia.org/enwiki/latest\\ \-\hspace{.75cm} /enwiki-latest-pages-articles.xml.bz2}
%and used GoogleNews-vectors model\footnote{\tt https://github.com/mmihaltz/Word2Vec-\\ \-\hspace{.75cm} GoogleNews-vectors} for Word2Vec by genism. English Wikipedia dataset contains 3 million %articles formated in XML. GoogleNews-vectors is the pre-trained Google News corpus (3 billion running words) word vector model, which is 3 million 300-dimension English word vectors.

\subsection{ROUGE evaluations over DUC-02}

%We compare the ROUGE-1, ROUGE-2, and ROUGE-SU4 recall scores for single-document summarizations produced by all algorithms over DUC-02.
As mentioned before, we use %Tgraph and
$CP_3$ to cover all previously known algorithms for the purpose of comparing qualities of summaries, as
%Tgraph produces the better results over these algorithms and
$CP_3$ produces the best results among them. %Since the $CP_3$ paper \cite{Parveen2016} does not cover SubmodularF,
%To compare with SubmodularF,
%we use the best parameters trained on DUC-03 \cite{lin:11}.
%Our computation show that the ROUGE-1, ROUGE-2, and ROUGE-Su4 scores for SubmodularF are (39.6, 16.9, 17.8), which are
%much lower than those of $CP_3$.

Among all the algorithms we devise, we only present those with at least one ROUGE recall score better than or equal to the corresponding score of $CP_3$, %Tgraph,
identified in bold %, where R-1, R-2, and R-SU4 stand for ROUGE-1,
%ROUGE-2, and ROUGE-SU4, respectively
(see Table \ref{duc}).
%The results are shown in %. In particular,
%we compare the ROUGE-1, ROUGE-2, and ROUGE-SU4 scores
Also shown in the table is the average of the three ROUGE scores (R-AVG). We can see that
ET3Rank is the winner, followed by T2RAKE; both are superior to $CP_3$.
%We can also see that all our algorithms
%in Table \ref{duc}
%perform better than Tgraph under ROUGE-1 and ROUGE-SU4.
Moreover, ET2RAKE offers the highest
ROUGE-1 score of 49.3.
%where the results for Lead, DUC-02 Best, TextRank, UniformLink, Egraph + Coh., and Tgraph +
%Coh. are copied from \cite{.}.
%
%with ROUGE to check against the state-of-the-art in Table \ref{duc}.
%\textit{Lead} selects the top five ranking sentences.
%DUC-02 Best is best result reported at DUC-02.
%We also compare with other popular summarization methods \textit{TextRank}, \textit{Submodular function},
%\textit{UniformLink (k=10)}, \textit{Egraph} and \textit{Tgraph (n=2000)}.
%We can see that RAKEN and RAKETT all outperform the rest of the algorithms on
%all of the ROUGE-1, ROUGE-2 and ROUGE-SU4 measures, and RAKETT is better than RAKEN on all of these measures. Moreover,
%RAKELAD also outperforms Tgraph on ROUGE-1 and ROUGE-SU4.
%
%perform better than the well known best systems on DUC-02. It shows that our sentence ranking system using RAKE can produce more informative summaries.
\begin{table}[h]
\begin{center}
\caption{\label{duc} ROUGE scores (\%) on DUC-02 data.}
\begin{tabular}{l|c|c|c||c}
\hline
\bf Methods & \bf R-1 & \bf R-2 & \bf R-SU4 & \bf R-AVG \\
\hline
$CP_3$& 49.0 & 24.7 & 25.8 & 33.17 \\
\hline
\bf ET3Rank & \bf 49.2 & \bf 25.6 & \bf 27.5 & \bf 34.10 \\
ESRAKE & \bf 49.0 & 23.6 & \bf 26.1 &  32.90 \\
%EPRAKE & \bf 48.9 & 22.8 & \bf 25.7  & \bf 32.46\\
ET2RAKE & \bf 49.3 & 21.4 & 24.5  & 31.73\\
%ELDARAKE & \bf 48.3 & 21.8 & \bf 24.5  & 31.53\\
PRAKE & \bf 49.0 & 24.5 & 25.3 & 32.93\\
\bf T2RAKE & \bf 49.1 & \bf 25.4 & \bf 25.8 & \bf 33.43 \\
%LDARAKE & \bf 48.3 & 22.5 & \bf 25.3 & 32.03\\
%SubmodularF & 39.6 & 16.9 & 17.8  \\
%Lead & 45.9 & 18.0 & 20.1  \\
%DUC-02 Best & 48.0 & 22.8 &   \\
%TextRank & 47.0 & 19.5 & 21.7  \\
%UniformLink & 47.1 & 20.1 &   \\
%Egraph + Coh. & 47.9 & 23.8 & 23.0  \\
\hline
%Tgraph + Coh. & 48.1 & 24.3 & 24.2 & 32.20 \\
%$CP_3$& 49.0 & 24.7 & 25.8 & 33.17 \\
%\hline
\end{tabular}
\end{center}
\end{table}

\subsection{WESM evaluations over DUC-02 and NewsIR-16}

%In this section, we generated summaries with 30\% and 50\% of the NewsIR-16 dataset using TextRankN, RAKEN, TextRankP, RAKEP, TextRankTT, RAKETT, TextRankLDA and RAKELDA and evaluated them by WMD-o %and WMD-p.
%We evaluate our algorithms listed
%in Table \ref{duc} using WESM over the Word2Vec model trained on English Wikipedia
%and GoogleNews.
%
Table \ref{results1} shows the evaluation results on DUC-02 and NewsIR-16 using WESM based on the Word2Vec model trained on English Wikipedia.
%and GoogleNews, respectively.
%Under each measure of WED and WEP,
The first number in the third row is the average score on all benchmark summaries in DUC-02.
For the rest of the rows, each number is the average
score of summaries produced by the corresponding algorithm for all documents in DUC-02 and NewsIR-16.
The size constraint of a summary on DUC-02 for each document is the same as that of the corresponding DUC-02 summary benchmark.

For NewsIR-16, we select at random 1,000 documents from NewsIR-16 and remove the title, references, and other unrelated content from each article.
%We then merge several articles at random to generate a new article of different sizes up to 10,000 words in an article.
Based on an observation that a 30\% summary allows for a good summary,
we compute 30\% summaries of these articles using each algorithm.
%and compute the average WED and WEP scores. The results are given in Table \ref{NewsIR-16}.
%
%for each algorithm, the
%average score of summaries for all documents in DUC-02 such that
%the size of the summary for each document is the same as that of the corresponding DUC-02 summary benchmark.
%The right column-hand is the average score

%WMD-o and WMD-p are trained on English Wikipedia.
%As shown in Table \ref{results}, RAKETT has the best WMD-o and WMD-p scores with 30\% and the best WMD-p score with 50\%. The summarization methods based on LDA which are TextRankLDA and RAKELDA %have the worst performance. The results are consistent with human judges. We can also conclude that WMD-p can better evaluate summaries than WMD-o, because RAKETT has a lower WMD-o score but a %higher WMD-p score. From WMD-p, we learn that summaries generated by RAKE based methods can produce better performance.

\begin{table}[h]
\begin{center}
\caption{\label{results1} Scores (\%) over DUC-02  and NewsIR-16 under WESM trained on English-Wikipedia.}
\begin{tabular}{l|c|c}
\hline
%\multirow{2}{*}{\bf Methods} &
%\bf Methods &
%\bf Measures & \multicolumn{2}{c}{\bf WESM EW}  & \multicolumn{2}{|c}{\bf WESM GN} \\
%\cline{2-5}
%Methods & WED & WEP & WED & WEP \\
%\hline
\bf Datasets & \bf DUC-02 & \bf NewsIR-16 \\ % & \bf D02 & \bf NIR \\
%{\bf Methods} & \bf WED & \bf WEP \\
\hline
%\bf Methods &
%\multicolumn{2}{c}{\bf WED}  & \multicolumn{2}{|c}{\bf WEP} \\
%\cline{2-3}
%\bf Methods &
Benchmarks & 3.021  & \\% &67.96 & \\
\hline









ET3Rank &\bf 3.382 &\bf 2.002 \\ % &\bf 69.45      &\bf 17.29 \\
ESRAKE      &3.175 & 1.956  \\ %&69.20      &17.15     \\
%EPRAKE      &3.154 &1.964   \\ %   &69.21     &17.04      \\
ET2RAKE     &3.148 &1.923   \\ %   &69.03      &17.01      \\
%ELDARAKE    &3.149 &1.960   \\ %   & 68.92  &17.02     \\
PRAKE       &3.150 &1.970   \\ %  &68.85      &17.21     \\
T2RAKE      &3.247 &1.990   \\ %  &69.17      &17.19    \\
%LDARAKE     &3.157 &1.914   \\ %   &68.45      &17.19    \\
\hline
\end{tabular}
\end{center}
\end{table}
It is expected that scores of our algorithms are better than the score for benchmarks under each measure, for the benchmarks often use different words not in the original documents, and hence would
have smaller similarities.

%We also note that WESM trained
%on
%To evaluate our algorithms over NewsIR-16 under WED and WEP, we select at random 1,000 documents from NewsIR-16 and remove the title, references, and other unrelated content from each article.
%We then merge several articles at random to generate a new article of different sizes up to 10,000 words in an article.
%Based on an observation that a 30\% summary allows for a good summary,
%we compute 30\% summaries of these articles using each algorithm
%and compute the average WED and WEP scores. The results are given in Table \ref{NewsIR-16}.

%summaries and average the scores
%
%The results shown in Tables \ref{results} and \ref{results1} are the average scores of the scores for each article's summary.

\begin{comment}
\begin{table}[h]
\begin{center}
\begin{tabular}{l|c|c|c|c}
\hline
\multirow{2}{*}{\bf Methods} &
%\bf Methods &
\multicolumn{2}{c}{\bf E-Wikipedia}  & \multicolumn{2}{|c}{\bf GoogleNews} \\
\cline{2-5}
%Methods & WED & WEP & WED & WEP \\
& \bf WED & \bf WEP & \bf WED & \bf WEP \\
\hline
ET3Rank &\bf 5.695  &\bf 2.002  &\bf 23.25  &\bf 11.32 \\
ESRAKE  &5.398      &1.956          &23.12  &11.18 \\
EPRAKE  &5.377      &1.964          &22.86  &11.21 \\
ET2RAKE &5.585      &1.922          &22.85  &11.17 \\
ELDARAKE &5.359     &1.960          &22.96  &11.08 \\
PRAKE   &5.377      &1.970          &23.14  &11.28 \\
T2RAKE &5.688       &1.990          &23.17  &11.20 \\
LDARAKE &5.554      &1.914          &23.18  &11.20 \\
\hline
\end{tabular}
\caption{\label{NewsIR-16} Scores (\%) over NewsIR-16 under WED and WEP trained on English Wikipedia and GoogleNews}
\end{center}
\end{table}
%Figure \ref{fig:fig5} shows a 30\% summary example of an article in the NewsIR-16 dataset by the RAKETT method. In Figure \ref{fig:fig5}, the left side is the content of the article and the right %side is the summary generated by the RAKETT method. The RAKETT method can merge sentences from the same subtopic, let the summary cover as many subtopics as possible and trim sentences from semantic %aspects.
\end{comment}

\subsection{Normalized $L_1$-norm}

We would like to determine if WESM is a viable measure. From our experiments, we know that the all-around best algorithm ET3Rank, the second best
algorithm T2RAKE, and ET2RAKE remain the same positions under R-AVG over DUC-02 and under WESM over both DUC-02 and NewsIR-16 (see Table \ref{ordering}),
ESRAKE and PRAKE remain the same positions under R-AVG over DUC-02 and under WESM over NewsIR-16, while ESRAKE and PRAKE only differ by one place under R-AVG and WESM over DUC-02.
\begin{table}[h]
\begin{center}
\caption{\label{ordering} Orderings of R-AVG scores over DUC-02 and
WESM scores over DUC-02 and NewsIR-16.}
\begin{tabular}{l|c|c|c}
\hline
\multirow{2}{*}{\bf Methods} &
%\bf Methods &
%\bf Measures &
\bf R-AVG  & \multicolumn{2}{|c}{\bf WESM} \\
\cline{2-4}
%Methods
& DUC-02 & DUC-02 & NewsIR-16 \\
\hline
%\bf Datasets & \bf DUC-02 & \bf NewsIR-16 \\ % & \bf D02 & \bf NIR \\
%{\bf Methods} & \bf WED & \bf WEP \\
%\hline
%\bf Methods &
%\multicolumn{2}{c}{\bf WED}  & \multicolumn{2}{|c}{\bf WEP} \\
%\cline{2-3}
%\bf Methods &
%Benchmarks & 4.22  & \\% &67.96 & \\
%\hline
\bf ET3Rank     &\bf 1  &\bf 1 &\bf 1  \\ % &\bf 69.45      &\bf 17.29 \\
ESRAKE          &4  &3 &4  \\ %&69.20      &17.15     \\
%\bf EPRAKE      &\bf 5  &\bf 5 &\bf 4  \\ %   &69.21     &17.04      \\
\bf ET2RAKE     &\bf 5  &\bf 5 &\bf 5  \\ %   &69.03      &17.01      \\
%ELDARAKE      &8  &7 &5   \\ %   & 68.92  &17.02     \\
PRAKE          &3  &4 &3  \\ %  &68.85      &17.21     \\
\bf T2RAKE      &\bf 2  &\bf 2 &\bf 2  \\ %  &69.17      &17.19    \\
%LDARAKE     &6  &4 &8  \\ %   &68.45      &17.19    \\
\hline
& ${\bm O}_1$ & ${\bm O}_2$ & ${\bm O}_3$ \\
\hline
\end{tabular}
\end{center}
\end{table}

Next, we compare the ordering of the R-AVG scores and the WESM scores over DUC-02. For this purpose, we use the normalized $L_1$-norm to compare the distance of two orderings. Let ${\bm X} = (x_1, x_2, \cdots, x_k)$ be a sequence of $k$ objects, where
each $x_i$ has two values $a_i$ and $b_i$ such that
$a_1, a_2, \ldots, a_k$ and $b_1,b_2,\ldots, b_k$ are, respectively, permutations of $1,2,\ldots,k$.
Let
\[
D_k= \sum_{i=1}^k |(k-i+1)-i|,
\]
which is the maximum distance two permutations can possibly have. Then
the normalized $L_1$-norm of ${\bm A} = (a_1, a_2, \cdots, b_k)$ and ${\bm B} = (b_1, b_2, \cdots, b_k)$ is defined by
$$||{\bm A}, {\bm B}||_1 = \frac{1}{D_k}\sum_{i=1}^k |a_i - b_i|.$$
Table \ref{ordering} shows the orderings of the R-AVG scores over DUC-02 and WESM scores over DUC-02 and NewsIR-16 (from Tables \ref{duc} and \ref{results1}).

It is straightforward to see that $D_5 = 12$,
$||{\bm O}_1, {\bm O}_2||_1 = ||{\bm O}_2, {\bm O}_3||_1 = 2/12 = 1/6$ and $||{\bf O}_1,{\bf O}_3||_1 = 0$.
%Note that
%the average $L_1$-norm on $(1,2,\cdots, 5)$ and the revers order is 2.4.
This indicates that WESM and ROUGE are highly comparable over DUC-02 and NewsIR-16,
and the orderings of WESM on different datasets, while with larger spread, are
still similar.

\begin{comment}
\subsection{WED and WEP evaluations on a different model}

We now apply a different model of Word2Vec trained on the GoogleNews dataset to evaluate the algorithms listed in Tabel \ref{duc}
over DUC-02 and NewsIR-16.
The results are shown in Table \ref{other}.

%Results on DUC-02 data are shown in Table \ref{other}. We used WMD-o and WMD-p trained by GoogleNews datasets to evaluate our summarization methods. o-s is the score of WMD-o comparing our system %summary with the DUC document. p-r is the score of WMD-p comparing the DUC reference summary with the DUC document. p-s is the score of WMD-p comparing our system summary with the DUC document. o-r %is the score of WMD-o comparing the DUC reference summary with the DUC document. From Table \ref{other}, although the absolute values of WMD-o and WMD-p are larger than the results in Table %\ref{results} because of using different training sets, we can still conclude the similar results with Table \ref{results} that RAKETT has the best performance and our methods are better than the %submodular function. The scores of WMD-o and WMD-p with our system summaries are higher than with DUC reference summaries.
\begin{table}[h]
\begin{center}
\begin{tabular}{l|c|c|c|c}
\hline
\multirow{2}{*}{\bf Methods} &
%\bf Methods &
\multicolumn{2}{c}{\bf WED}  & \multicolumn{2}{|c}{\bf WEP} \\
\cline{2-5}
%Methods & WED & WEP & WED & WEP \\
& 30\% & 50\% & 30\% & 50\% \\
\hline
\bf ET3Rank &82.32      &\bf 81.38&\bf 55.21&\bf 54.53 \\
ESRAKE      &83.38      &\bf 81.38&54.92    &\bf 54.53 \\
EPRAKE       &83.69     &\bf 81.38&55.03    &\bf 54.53  \\
ET2RAKE     &83.12      &\bf 81.38&54.94    &\bf 54.53  \\
ELDARAKE    &\bf 83.71  &\bf 81.38&55.11    &\bf 54.53 \\
PRAKE       &82.46      &\bf 81.38&54.81    &\bf 54.53 \\
T2RAKE      &82.10      &\bf 81.38&54.77    &\bf 54.53\\
LDARAKE     &82.02      &\bf 81.38&54.87    &\bf 54.53\\
\hline
\end{tabular}
\caption{\label{other1} Scores (\%) of GoogleNews-based WED and WEP on 30\% and 50\% summaries over DUC-02}
\end{center}
\end{table}

\begin{table}[h]
\begin{center}
\begin{tabular}{l|c|c|c|c}
\hline
\multirow{2}{*}{\bf Methods} &
%\bf Methods &
\multicolumn{2}{c}{\bf WED}  & \multicolumn{2}{|c}{\bf WEP} \\
\cline{2-5}
%Methods & WED & WEP & WED & WEP \\
& 30\% & 50\% & 30\% &50\% \\
\hline
\bf ET3Rank &\bf 23.35 &\bf 23.55   &\bf 11.36 &11.38 \\
ESRAKE      &23.22      &23.08      &11.31      &11.34 \\
EPRAKE      &23.06      &23.25      &11.21      &11.31  \\
ET2RAKE     &23.08      &23.38      &11.24      &  11.33\\
ELDARAKE    &23.25      &\bf 23.55  &11.26      &\bf 11.42  \\
PRAKE       &23.06      &23.25      &11.21      &11.31 \\
T2RAKE      &23.18      &23.18      &11.23      &11.34 \\
LDARAKE     &23.25      &23.28      &11.32      &11.33\\
\hline
\end{tabular}
\caption{\label{other} Scores (\%) of GoogleNews-based WED and WEP on 30\% and 50\% summaries over NewsIR-16}
\end{center}
\end{table}
\end{comment}

\subsection{Runtime analysis}

We carried out runtime analysis through experiments on a computer with a 3.5 GHz Intel Xeon CPU E5-1620 v3. %2.7 GHz dual-core Intel Core i5 CPU and 8 GB memory.
We used a Python implementation of our summarization algorithms. %methods on the NewsIR-16 dataset.
Since DUC-02 are short, all but LDA-based algorithms run in about the same time.
To obtain a finer distinction, we ran our experiments on NewsIR-16. Since the average size of NewsIR-16 articles is 405 words,
we selected at random a number of articles from NewsIR-16 and merged them to generate a new article.
For each size from around 500 to around 10,000 words, with increments of 500 words, we selected at random 100 articles and
computed the average runtime of different algorithms to produce 30\% summary (see Figure \ref{fig:runtime}).
We note that the time complexity of each of our algorithms incurs mainly in
the preprocessing phase; %that is, the runtime incurs in the sentence selection phase
%is minor; namely,
the size of summaries in the sentence selection phase only incur minor fluctuations of computation time, and
so it suffice to compare the runtime for producing 30\% summaries.

%and use summary rate with 30\%, 50\% and 70\%.
\begin{figure}[h]
\centering
\includegraphics[width=3.3in]{runtime-color.png}
\caption{Runtime analysis, where the unit on the x-axis is 100 words and the unit of the y-axis is seconds.} \label{fig:runtime}
\end{figure}
We can see from Figure \ref{fig:runtime} that ESRAKE and PRAKE incur about the same linear time and they are extremely fast.
Also, ET3RANK, ET2RAKE, and T2RAKE incur about the same time. While the time is higher because of the use of TextTiling and
is closed to being linear, it meets the realtime requirements. For example, for a document of up to 3,000 words, over
3,000 but less than 5,500 words, and 10,000 words, respectively,
the runtime of ET3Rank is under 0.5, 1, and 2.75 seconds.
%,
%
%for a document less than 5,500 words,
%the runtime is
%less than 1 second, % and for a very long document of 10,000 words, the runtime is
%and less than 2.75 seconds.

The runtime of SubmodularF is acceptable for documents of moderate sizes (not shown in the paper); but for a document of about 10,000 words, the runtime is close to 4 seconds.
LDA-based algorithms is much higher. For example, LDARAKE incurs about 16 seconds for a document of
about 2,000 words, about 41 seconds for a document of about 5,000 words, and about 79 seconds for a document of about 10,000 words.

%\begin{table}
%\begin{tabular}
%
%\end{tabular}
%\end{table}




\begin{comment}
\begin{table}[h]
\begin{center}
\begin{tabular}{l|c|c|c|c}
\hline
\bf Methods & \bf 10\% & \bf 30\% & \bf 50\% & \bf 70\% \\
\hline
\bf ET3Rank & &&& \\
ESRAKE & &&& \\
EPRAKE & &&&  \\
ET2RAKE & &&&  \\
ELDARAKE & &&&  \\
PRAKE & &&& \\
T2RAKE & &&& \\
LDARAKE & &&&\\
\hline
\end{tabular}
\caption{\label{runtime} Runtime (seconds) of different summary rate on the NewsIR-16 dataset.}
\end{center}
\end{table}

The Runtime of different summary rate on the NewsIR-16 dataset is given in Table \ref{runtime}. We can find that RAKE based summarization methods are much faster than TextRank based ones, which is bacause TextRank needs to calculate iterations until it converges while RAKE only calculates word co-occurrence. The runtime of generating summaries with different rate is similar and LDA based methods take more time.
\end{comment}

\section{\uppercase{Conclusions}}
\label{sect:conclusion}
\noindent We presented a number of unsupervised single-document summarization algorithms for generating effective summaries in realtime and
a new measure based on word-embedding similarities to evaluate the quality of a summary. We showed that ET3Rank is the best all-around algorithm. A web-based summarization tool using ET3Rank and T2RAKE will be made available to the public.

To further obtain better topic clusterings efficiently, we plan to extend TextTiling over non-consecutive paragraphs.
To obtain a better understanding of word-embedding similarity measures, we plan to compare WESM with human evaluation and
other unsupervised methods including those devised by Louis and Nenkova \cite{Louis:2009}.
We also plan to
explore new ways to measure summary qualities
without human-generated benchmarks.


\section*{\uppercase{Acknowledgements}}

\noindent
We thank the members of the Text Automation Lab at UMass Lowell for their support and fruitful discussions.



%the major topics of the original document as diverse as possible. We also

%We take sentence ranking and topic diversity coverage into account. We also present summarization evaluation methods based on Word2Vec without reference summaries. We experimented our system with several news datasets. Our summarization methods improve substantially over state-of-the-art systems on ROUGE while still maintaining good linguistic quality and the runtime is competitive. Our summarization evaluation methods can produce comparable results with ROUGE. We plan to explore sentence compression model with pronoun anaphora and trim sentences from phrases instead of words.

% \section*{\uppercase{Acknowledgements}}


\vfill
\bibliographystyle{apalike}
{\small
\bibliography{example}}


% \section*{\uppercase{Appendix}}

\vfill
\end{document}

