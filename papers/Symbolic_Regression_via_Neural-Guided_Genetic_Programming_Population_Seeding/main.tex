\documentclass{article}

\usepackage[final]{neurips_2021}
\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{xcolor}         % colors

% Custom packages
\usepackage{colortbl}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{nccmath, mathtools}
\usepackage{wrapfig,lipsum,booktabs}
\usepackage{verbatim}
\usepackage{bm}

\def\baselinestretch{0.97}

\newcommand{\placeholder}{\mathord{\color{black!33}\bullet}}

\newcommand{\base}{\mathcal{L}_0}
\newcommand{\NguyenX}{$\base$}
\newcommand{\NguyenXY}{$\base \cup \{ y \}$}
\newcommand{\NguyenXC}{$\base \cup \{ \textrm{const} \}$}
\newcommand{\NguyenXYC}{$\base \cup \{ y, \textrm{const} \}$}
\newcommand{\NguyenXOne}{$\base \cup \{ 1 \}$}
\newcommand{\Jin}{$\base - \{ \log \} \cup \{\placeholder^2, \placeholder^3, y, \textrm{const} \}$}
\newcommand{\GrammarVAE}{$\base - \{ -, \cos, \log \} \cup \{ 1, 2, 3 \}$}
\newcommand{\Keijzer}{$\{ +, \times, \div, \placeholder^{-1}, -\placeholder, \sqrt{\placeholder}, x \}$}
\newcommand{\Korns}{$\base \cup \{ \tan, \tanh, \placeholder^2, \placeholder^3, \sqrt{\placeholder}, y \}$}
\newcommand{\VladislavlevaB}{$\{ +, -, \times, \div, \exp, e^{-\placeholder}, \placeholder^2, x, y \}$}

\newcommand\RCOMMENT[1]{\hfill\(\triangleright\) #1}
\newcommand\LCOMMENT[1]{\(\triangleright\) #1 }
\newcommand{\RETURN}[1]{\STATE \textbf{return} #1}
\newcommand{\DEFINE}[1]{\STATE \textbf{define} #1 }

\newcommand{\eps}{\varepsilon}
\newcommand{\Jstd}{J_\textrm{std}(\theta)}
\newcommand{\Jrisk}{J_\textrm{risk}(\theta; \eps)}
\newcommand{\JCVaR}{J_\textrm{CVaR}(\theta)}
\newcommand{\Jpqt}{J_\textrm{PQT}(\theta; k)}

\newcommand{\alternativeToRL}{RNN training }

\usepackage{soul}

\newcounter{subroutine}
\makeatletter
\newenvironment{subroutine}[1][htb]{
  \let\c@algorithm\c@subroutine
  \renewcommand{\ALG@name}{Subroutine}
  \begin{algorithm}[#1]
  }{\end{algorithm}
}
\makeatother

\setboolean{ALC@noend}{true}

\title{Symbolic Regression via Neural-Guided Genetic Programming Population Seeding}

\author{
    T. Nathan Mundhenk\\
    \texttt{mundhenk1@llnl.gov} \\
    \And
    Mikel Landajuela \\
    \texttt{landajuelala1@llnl.gov} \\
    \And
    Ruben Glatt \\
    \texttt{glatt1@llnl.gov} \\
    \And
    Claudio P. Santiago \\
    \texttt{prata@llnl.gov} \\
    \And
    Daniel M. Faissol \\
    \texttt{faissol1@llnl.gov} \\
    \And
    Brenden K. Petersen \\
    \texttt{bp@llnl.gov} \\
    \and
    Computational Engineering Division\\
    Lawrence Livermore National Laboratory\\
    Livermore, CA 94550 \\
}

\definecolor{bgreen}{rgb}{0,0.4,0}
\newcommand{\nate}[1]{{\color{red} Nate-o: #1}}
\newcommand{\bp}[1]{{\color{bgreen} B: #1}}
\newcommand{\ruben}[1]{{\color{purple} R: #1}}
\newcommand{\ml}[1]{{\color{blue} M: #1}}
\newcommand{\cs}[1]{{\color{magenta} CS: #1}}
\newcommand{\df}[1]{{\color{orange} DF: #1}}

\begin{document}

\maketitle

\begin{abstract}
Symbolic regression is the process of identifying mathematical expressions that fit observed output from a black-box process.
It is a discrete optimization problem generally believed to be NP-hard.
Prior approaches to solving the problem include neural-guided search (e.g. using reinforcement learning) and genetic programming.
In this work, we introduce a hybrid neural-guided/genetic programming approach to symbolic regression and other combinatorial optimization problems.
We propose a neural-guided component used to seed the starting population of a random restart genetic programming component, gradually learning better starting populations.
On a number of common benchmark tasks to recover underlying expressions from a dataset, our method recovers 65\% more expressions than a recently published top-performing model using the same experimental setup.
We demonstrate that running many genetic programming generations without interdependence on the neural-guided component performs better for symbolic regression than alternative formulations where the two are more strongly coupled.
Finally, we introduce a new set of 22 symbolic regression benchmark problems with increased difficulty over existing benchmarks.
Source code is provided at \url{www.github.com/brendenpetersen/deep-symbolic-optimization}.
\end{abstract}

\section{Introduction}
Symbolic regression involves searching the space of mathematical expressions to fit a dataset using equations which are potentially easier to interpret than, for example, neural networks.
A key difference compared to polynomial or neural network-based regression is that we seek to illuminate the true underlying process that generated the data.
Thus, the process of symbolic regression is analogous to how a physicist may derive a set of fundamental expressions to describe a natural process.
For example, Tycho Brahe meticulously mapped the motion of planets through the sky, but it was Johannes Kepler who later created the expressions for the laws that described their motion.
Given a dataset $(X, y)$, where each point has inputs $X_i \in \mathbb{R}^n$ and response $y_i \in \mathbb{R}$, symbolic regression aims to identify a function $f : \mathbb{R}^n \rightarrow \mathbb{R}$ that best fits the dataset, where the functional form of $f$ is a short closed-form mathematical expression.

The space of mathematical expressions is structurally discrete (in functional form) but continuous in parameter space (e.g. floating-point constants).
The search space grows exponentially with the length of the expression, rendering symbolic regression a challenging machine learning problem.
It is generally believed to be NP-hard \citep{lu2016using}; however, no formal proof exists.
Given the large, combinatorial search space, traditional approaches to symbolic regression commonly utilize evolutionary algorithms, especially \emph{genetic programming} (GP) \citep{koza1992genetic, schmidt2009distilling, deap2012, back2018evolutionary}.
GP-based symbolic regression operates by maintaining a population of mathematical expression ``individuals'' that are ``evolved'' using evolutionary operations like selection, crossover, and mutation.
A fitness function acts to improve the population over many generations.

Neural networks can also be leveraged for symbolic regression \citep{kusner2017grammar, sahoo2018learning, udrescu2020ai}.
Recently, it has been proposed to solve symbolic regression using \emph{neural-guided search} \citep{petersen2019deep, landajuela2021discovering}.
This approach works by using a recurrent neural network (RNN) to stochastically emit batches of expressions as a sequence of mathematical operators or ``tokens.''
Expressions are evaluated for goodness of fit and a training strategy is used to improve the quality of generated formulas.
\citet{petersen2019deep} propose a \emph{risk-seeking policy gradient} strategy, which filters out the lesser performers and returns an ``elite set'' of expressions to the RNN each step of training. 
Constraints can be employed to prune the search space, preventing nonsensical or extraneous statements in generated expressions.
For instance, inversions (e.g. $\log\left(e^x\right)$) and nested trigonometric functions (e.g. $\sin(1 + \cos(x)$) can be avoided.
The performance attained using neural-guided search outperformed GP-based approaches, including commonly used commercial software.

\begin{figure}
  \centering
  \includegraphics[trim={0 0.4cm 0 0}, clip, width=\linewidth]{gp-meld.png}
  \caption{\textbf{Method overview.}
  A parameterized sequence generator (e.g. RNN) generates $N$ samples, i.e. expressions for symbolic regression.
  These samples are used as the starting population for a GP component.
  GP then runs $S$ generations.
  The top $M$ samples from GP are extracted, combined with the $N$ samples from the RNN, and used to train the RNN, e.g. using VPG, RSPG, or PQT.
  Since GP is stateless, it runs in a random restart-like fashion each time the RNN is sampled.
  }
  \label{fig:gp_meld}
\end{figure}

Genetic programming and neural-guided search are mechanistically dissimilar, yet both have proven to be effective solutions to symbolic regression.
Might it be possible to combine the two approaches to leverage each of their strengths?
The population of individual solutions in GP is structurally the same as a batch of samples emitted by the RNN in neural-guided search.
In principle, they can formally be interfaced by allowing GP individuals to flow to the RNN training and vice versa.

For reinforcement learning-based training objectives like the one used in \citet{petersen2019deep},
this way of coupling creates an out-of-distribution problem on the RNN side.
Standard off-policy methods, like importance sampling \citep{glynn1989importance}, do not apply here since the GP distribution is intractable.
Note, however, that the mechanistic interpretation of the policy gradient as maximizing the log likelihood of individuals proportional to their fitness is still valid.
On the other hand, for training alternatives based on maximum likelihood estimation over a selected subset of samples,
like the cross-entropy method \citep{de2005tutorial} or priority queue training \citep{abolafia2018neural},
there are no assumptions about samples being ``on-policy,'' and thus they can be applied seamlessly. 
In this work, we explore three different ways of training the RNN: two reinforcement learning-based training methods (without off-policy correction) and the priority queue training method.

\section{Related Work}

\textbf{Deep learning for symbolic regression.}
Several recent approaches leverage deep learning for symbolic regression.
AI Feynman \citep{udrescu2020ai} proposes a problem-simplification tool for symbolic regression.
They use neural networks to identify simplifying properties in a dataset (e.g. multiplicative separability, translational symmetry), which they exploit to recursively define simplified sub-problems that can then be tackled using any symbolic regression algorithm.
In GrammarVAE, \citet{kusner2017grammar} develop a generative model for discrete objects that adhere to a pre-specified grammar, then optimize them in latent space.
They demonstrate this can be used for symbolic regression; however, the method struggles to exactly recover expressions, and the generated expressions are not always syntactically valid.
\citet{sahoo2018learning} develop a symbolic regression framework using neural networks whose activation functions are symbolic operators.
While this approach enables an end-to-end differentiable system, back-propagation through activation functions like division or logarithm requires the authors to make several simplifications to the search space, ultimately precluding learning certain simple classes of expressions like $\sqrt{x}$ or $\sin(x/y)$. 

\textbf{Genetic programming/policy gradient hybrids.}
The overall idea of combining GP with gradient-based methods in general predates the deep learning era \citep{igel1999using,topchy2001gpgd,zhang2004gpgb, montastruc2004gpgb, wierstra2008gpgb} and dissatisfaction with deep reinforcement learning (DRL) is not entirely new \citep{kurenkov2018reinforcementflaw}.
Recently, several approaches have combined GP and reinforcement learning (RL).
In these works, RL is used to alter or augment the GP process, e.g. by adjusting the probabilities of performing different genetic operations.
Alternatively, GP is used to augment the creation or operation of a neural network \citep{petronski2018gprl,chang2018gprl,gangwani2018gprl,chen2019gprl, stanley2019gprl,real2019gprl,miikkulainen2019gprl,sehgal2019gprl,tian2020gprl,chen2020gprl_a,chen2020gprl_b}.

\textbf{Sample population interchanging methods.}
We identify a broad class of existing methods that can be characterized by using samples from a \emph{sequence generator} and interchanging those samples with a GP population.
The sequence generator may be any discrete distribution or generative process that creates a sequence of tokens, e.g. a recurrent neural network or transformer.
Samples from the sequence generator are then treated as interchangeable with a population generated by GP.
Thus, sequence generator samples can be inserted into the population of GP, and GP samples can be used to update the sequence generator.
In some cases the sequence generator may not have learnable parameters. 

Within this class of methods, \citet{pourchot2019gprl} and \citet{khadka2018gprl} solve physical continuous control problems like the inverted pendulum or moon rover with a fungible neural network controller.
Their approaches are similar to neuro-evolution \citep{stanley2002neuroev, floreano2008neuroev, luders2017neruoev, risi2017neuroev}, as both techniques are based on using deep deterministic policy gradients (DDPG) \citep{lillicrap2015continuous} and utilize a synchronous shared pool of actors between GP and RL components.
\citet{khadka2018gprl} introduced the ERL method that pools RL and GP actor samples into a cyclic replay buffer.
This single buffer is drawn to either seed a GP step or help update the actor-critic model.
Both GP and RL work synchronously, and the population is interchangeable.
\citet{pourchot2019gprl} introduce CEM-RL which solves the same family of problems in a similar way.
The most notable difference is that the shared population is not fed directly into a GP, but rather it is used to update a distribution from which a population is drawn.

Most closely related to our work, \citet{ahn2020gprl} develop genetic expert-guided learning (GEGL).
In GEGL, samples are generated using a stochastic RNN-based sequence generator and added to a maximum reward priority queue (MRPQ).
A GP component then applies mutation and/or crossover on each item in the MRPQ.
Notably, only one generation of evolutionary operators is applied to each sample in the MRPQ.
As a consequence, there is no notion of a selection operator.
In contrast, we demonstrate that the selection operator and performing multiple generations of evolution is crucial to maximize the benefits of GP.
After performing one generation of GP, the resulting population is then added to a second MRPQ.
Samples from the union of the GP-based MRPQ and RNN-based MRPQ are then used to train the RNN.
In contrast, we find that priority queues are not necessary and may prevent sufficient exploration.
As with ERL and CEM-RL, GEGL's evolutionary and training steps are one-to-one, meaning that each training step is followed by exactly one GP generation.
Additionally, GP pulls from a persistent memory of sample populations, which it shares with the neural network component.
This strong coupling makes the evolutionary component much more interdependent on the neural network.

\section{Methods}

Our overall algorithm comprises two main components: (1) a sequence generator (with learnable parameters) and (2) a genetic programming component.
In the sections below, we first discuss preliminaries, then describe each component individually, and finally describe how the two components interact.

\textbf{Preliminaries.}
Any mathematical expression $f$ can be represented by an algebraic expression tree, where internal nodes are operators (e.g. $\times, \sin$) and terminal nodes are input variables (e.g. $x$) or constants \citep{petersen2019deep}.
We refer to $\tau = [\tau_1, \dots, \tau_{|\tau|}]$ as the \textit{pre-order traversal} of such an expression tree.
Notably, there is a one-to-one correspondence between an expression tree and its pre-order traversal (see \citet{petersen2019deep} for details).
Each $\tau_i$ is an operator, input variable, or constant selected from a library of possible tokens, e.g. $[+, -, \times, \div, \sin, \cos, \exp, \log, x]$.
A pre-order traversal $\tau$ can be instantiated into a corresponding mathematical expression $f$ and evaluated based on its fitness to a dataset.
Specifically, we consider the metric normalized root-mean-square error (NRMSE), defined as follows.
Given a pre-order traversal $\tau$ and a dataset of $(X, y)$ pairs of size $N$, with $X\in\mathbb{R}^n$ and $y\in\mathbb{R}$, we define $\textrm{NRMSE}(\tau) = \frac{1}{\sigma_y} \sqrt{\frac{1}{N}\sum_{i=1}^N \left(y_i - f\left(X_i\right)\right)^2}$, where $f : \mathbb{R}^n \rightarrow \mathbb{R}$ is the instantiated mathematical expression represented by $\tau$, and $\sigma_y$ is the standard deviation of $y$.
Finally, the fitness or reward function is given by $R(\tau) = 1/(1 + \textrm{NRMSE}(\tau))$.
We use the terms ``fitness'' and ``reward'' interchangeably, with the former being more typical in the context of GP and the latter more common in the context of RL.

\textbf{Sequence generator.}
The sequence generator is a parameterized distribution over mathematical expressions, $p(\tau|\theta)$.
Typically, a model is chosen such that the likelihood of an expression is tractable with respect to parameters $\theta$, allowing backpropagation of a differentiable loss function.
A common choice of model is an autoregressive RNN, in which the likelihood of the $i$th token (denoted $\tau_i$) is conditionally independent given the preceding tokens $\tau_1, \dots, \tau_{(i-1)}$.
That is, $p(\tau_i|\tau_{j\neq i},\theta) = p(\tau_i|\tau_{j<i}, \theta)$.
We follow the sequence generator used in \citet{petersen2019deep}: an autoregressive RNN comprising a single-layer LSTM with 32 hidden nodes.
For notational simplicity, hereafter we assume the use of an RNN as the sequence generator.

The sequence generator is typically trained using reinforcement learning or a related approach.
Under this perspective, the sequence generator can be viewed as a reinforcement learning \textit{policy}, which we seek to optimize by sampling a batch of $N$ expressions $\mathcal{T}$, evaluating each expression under a reward function $R(\tau)$, and performing gradient descent on a loss function.
In this work, we explore three methods for training the RNN:
\begin{itemize}
    \item \textbf{Vanilla policy gradient (VPG)}: Using the well-known REINFORCE rule \citep{williams1992simple}, training is performed over the batch $\mathcal{T}$, yielding the loss function: $\mathcal{L}(\theta) = \frac{1}{|\mathcal{T}|}\sum_{\tau\in\mathcal{T}} (R(\tau)-b) \nabla_\theta \log p(\tau|\theta)$, where $b$ is a baseline term or control variate, e.g. an exponentially-weighted moving average (EWMA) of rewards.
    \item \textbf{Risk-seeking policy gradient (RSPG)}: \citet{petersen2019deep} develop an alternative to VPG intended to optimize for best-case instead of average reward: $\mathcal{L}(\theta) = \frac{1}{\varepsilon|\mathcal{T}|}\sum_{\tau\in\mathcal{T}} (R(\tau)-\tilde{R}_\varepsilon) \nabla_\theta \log p(\tau|\theta)\mathbf{1}_{R(\tau)>\tilde{R}_\varepsilon}$, where $\varepsilon$ is a hyperparameter controlling the degree of risk-seeking and $\tilde{R}_\epsilon$ is the empirical $(1-\varepsilon)$ quantile of the rewards of $\mathcal{T}$.
    \item \textbf{Priority queue training (PQT)}: \citet{abolafia2018neural} introduce a non-RL approach also intended to focus on optimizing best-case performance. Samples from each batch are added to a persistent maximum reward priority queue (MRPQ), and training is performed over samples in the MRPQ using a supervised learning objective: $\mathcal{L}(\theta) = \frac{1}{k}\sum_{\tau\in\textrm{MRPQ}}\nabla_\theta \log p(\tau|\theta)$, where $k$ is the size of the MRPQ.
\end{itemize}
Note that our method is agnostic to the training procedure; additional procedures may be considered, for example the cross-entropy method \citep{de2005tutorial}, which is closely related to PQT. In our formulation, we include, as is common, an additional term in the loss function proportional to the entropy of the distribution at each position along the sequence \citep{bello2016neural,abolafia2018neural,petersen2019deep,landajuela2021discovering}.

\textbf{Genetic programming.}
Genetic programming (GP) begins with a set (``population'') of expression trees (``individuals''), denoted $\mathcal{T}_\textrm{GP}$.
In standard GP, these individuals are randomly generated; however, we discuss in the subsequent section how this differs in our algorithm.
A single iteration or ``generation'' of a GP algorithm consists of several ``evolutionary operations'' that directly alter the current population.
A \textit{mutation} operator introduces random variations to an individual, for example by replacing one subtree with another randomly generated subtree.
A \textit{crossover} operator involves exchanging content between two individuals, for example by swapping one random subtree of one individual with another random subtree of another individual.
A \textit{selection} operator is used to select which individuals from the current population persist onto the next population.
A common selection operator is tournament selection \citep{koza1992genetic}, in which a set of $k$ candidate individuals are randomly sampled from the population, and the individual with the highest fitness is selected.
In each generation of GP, each individual has a probability of undergoing mutation and a probability of undergoing crossover; selection is performed until the new generation's population has the same size as the current generation's population.
GP does not have an explicit objective function as does the sequence generator, but it does tend to move the population toward higher fitness \citep{petronski2018gprl}.

For the GP component, we begin with a standard formulation from DEAP \citep{deap2012} and introduce several key changes:
\begin{enumerate}
    \item Typically only \emph{uniform} mutation is used.
    Instead, we select among uniform, node replacement, insertion, or shrink mutation with equal probability.
    \item We incorporate constraints from \citet{petersen2019deep} (for example, constraining nested trigonometric functions, e.g. $\sin(1 + \cos(x))$).
    If a genetic operation would result in an individual that violates any constraint, that genetic operation is instead reverted.
    That is, the child expression instead becomes a copy of the parent expression.
    This procedure ensures that all individuals satisfy all constraints for all generations.
    \item The sample population is never initialized randomly.
    Rather, the initial population is always seeded by samples from the RNN.
    (See next section for details.)
\end{enumerate}

\begin{algorithm*}[t]
\caption{Neural-guided genetic programming population seeding}
\label{alg:algorithm}
\begin{algorithmic}[1]
\INPUT batch/population size $N$; number of GP generations per RNN training update $S$; constraints $\Omega$; crossover probability $P_c$; mutation  probability $P_m$; tournament size $k$; GP sample selection size $M$ (must be $\leq N$)
\INPUT Loss function $\mathcal{L}(\theta)$ for training the RNN, including corresponding hyperparameters (e.g. EWMA coefficient for VPG, risk factor for RSPG, priority queue size for PQT)

\OUTPUT Best sample $\tau^\star$
\STATE Initialize reward function $R : \tau \rightarrow \mathbb{R}$
\STATE {Initialize RNN distribution over expressions, $p(\cdot | \theta, \Omega)$}
\STATE {Initialize GP operation, $\textrm{GP}(P_c, P_m, k, R) = \Gamma : \mathcal{T} \rightarrow \mathcal{T}$}

\STATE $\tau^\star \leftarrow \textrm{null}$
\WHILE{total samples below budget}
    \STATE $\mathcal{T}_\textrm{RNN} \leftarrow \{\tau^{(i)} \sim p(\cdot | \theta, \Omega)\}_{i=1}^N$ \RCOMMENT{Sample batch of size $N$}
    \STATE $\mathcal{T}_\textrm{GP}^{(0)} \leftarrow \mathcal{T}_\textrm{RNN}$ \RCOMMENT{Seed GP starting population with RNN batch}
    
    \FOR {$s = 1,\ldots,S$} 
        \STATE $\mathcal{T}_\textrm{GP}^{(s)} \leftarrow \Gamma\left(\mathcal{T}_\textrm{GP}^{(s-1)}\right)$ \RCOMMENT{Apply GP operations}
    \ENDFOR

    \STATE $\mathcal{T}_\textrm{train} \leftarrow \textrm{Top-}M\left(\mathcal{T}_\textrm{GP}^{(0)} \cup \mathcal{T}_\textrm{GP}^{(1)} \cup \cdots \cup \mathcal{T}_\textrm{GP}^{(S)}\right)$ \RCOMMENT{Filter best $M$ samples from GP}
    
    \STATE $\mathcal{T}_\textrm{train} \leftarrow \mathcal{T}_\textrm{train} \cup \mathcal{T}_\textrm{RNN}$ \RCOMMENT{Join RNN and best GP samples}
    
    \STATE $\mathcal{R} \leftarrow \{R(\tau) \ \forall \ \tau\in\mathcal{T}_\textrm{train} \}$ \RCOMMENT{Compute rewards}
    \STATE $\theta \leftarrow \theta + \nabla_\theta \mathcal{L}(\theta)$ \RCOMMENT{Train the RNN (e.g. using PQT)}
    
    \SHORTIF{$\max\mathcal{R} > R(\tau^\star)$} {$\tau^\star \leftarrow \tau^{(\arg\max \mathcal{R})}$} \RCOMMENT{Update best sample}
\ENDWHILE
\RETURN $\tau^\star$
\end{algorithmic}
\end{algorithm*}


\textbf{Neural-guided genetic programming population seeding.}
While both a fully RNN-based approach (i.e. deep symbolic regression \citep{petersen2019deep}) and a fully GP-based approach (i.e. GP-based symbolic regression \citep{koza1992genetic}) involve generating ``batches'' or ``populations'' of expressions, they arrive at their expressions very differently.
Namely, we observe that the RNN can generate expressions ``from scratch'' given only parameters $\theta$; in contrast, GP requires an extant population to operate on.
More specifically, given parameters $\theta$, the RNN can be used to sample a batch of expressions, $\mathcal{T}_\textrm{RNN}$; in contrast, generation $i$ of GP begins with a population of expressions $\mathcal{T}_\textrm{GP}^{(i)}$ and application of one generation of GP produces a new, augmented population $\mathcal{T}_\textrm{GP}^{(i+1)}$.

Thus, we propose that a natural point at which to interface neural-guided search and GP is the starting population of GP, $\mathcal{T}_\textrm{GP}^{(0)}$.
Specifically, we propose to use the most recent batch of samples from the RNN directly as the starting population for GP: $\mathcal{T}_\textrm{GP}^{(0)} = \mathcal{T}_\textrm{RNN}$.
From there, we can perform $S$ generations of GP, resulting in a final GP population, $\mathcal{T}_\textrm{GP}^{(S)}$.
Finally, we can sub-select an ``elite set'' of the top-performing GP samples, and include these samples in the gradient update for neural guided search (e.g. VPG, RSPG, PQT).
This process constitutes one step of our algorithm, and is repeated until a maximum number of total expression evaluations is reached.
Thus, GP acts as an inner optimization loop, within an outer gradient-based loop of neural-guided search.

Note that the GP process is \textit{restarted} for each new batch of samples from the RNN; thus, the process is similar to GP with random restarts; the key difference for our proposed algorithm is that the GP starting population upon each restart \textit{changes} as the RNN learns via an objective function.
Thus, from the perspective of GP, the RNN provides increasingly better starting populations.
Empirically, we found that this hybrid approach also allows for larger learning rates relative to pure neural-guided search.

We hypothesize that this hybrid approach of neural-guided search and GP will leverage the strengths of each individual approach.
Whereas GP is stateless and there is no learning step, neural-guided search is stateful (via RNN parameters $\theta$) and \textit{learns} from data via a well-defined, differentiable loss function.
Whereas neural-guided search is known to easily get stuck in local optima, GP can produce large variations in the population, resulting in ``fresh'' samples that are essentially out-of-distribution from the RNN-induced distribution.
We discuss this further in Discussion.

We provide pseudocode for our algorithm in Algorithm \ref{alg:algorithm} and illustrate it in Figure \ref{fig:gp_meld}.
While this work focuses on the task of symbolic regression, our approach applies to any symbolic optimization task with a black-box reward function.

\section{Results}

\begin{table*}[t]
    \centering
    \caption{Recovery rate of several algorithms on the Nguyen benchmark problem set across 100 independent training runs. Results of our algorithm are obtained using PQT; slightly lower recovery rates were obtained using VPG and RSPG training (see Table \ref{tab:ablations} for comparisons).
    }
    \begin{tabular}{cccccccc}
    & & \multicolumn{6}{c}{Recovery rate (\%)} \\
    Benchmark & Expression & Ours & DSR & PQT & VPG & GP & Eureqa \\
    \midrule
    Nguyen-1 & $x^3+x^2+x$ & 100 & 100 & 100 & 96 & 100 & 100 \\
    Nguyen-2 & $x^4+x^3+x^2+x$ & 100 & 100 & 99 & 47 & 97 & 100 \\
    Nguyen-3 & $x^5+x^4+x^3+x^2+x$ & 100 & 100 & 86 & 4 & 100 & 95 \\
    Nguyen-4 & $x^6+x^5+x^4+x^3+x^2+x$ & 100 & 100 & 93 & 1 & 100 & 70 \\
    Nguyen-5 & $\sin(x^2)\cos(x)-1$ & 100 & 72 & 73 & 5 & 45 & 73 \\
    Nguyen-6 & $\sin(x)+\sin(x+x^2)$ & 100 & 100 & 98 & 100 & 91 & 100 \\
    Nguyen-7 & $\log(x+1)+\log(x^2+1)$ & 97 & 35 & 41 & 3 & 0 & 85 \\
    Nguyen-8 & $\sqrt{x}$ & 100 & 96 & 21 & 5 & 5 & 0 \\
    Nguyen-9 & $\sin(x)+\sin(y^2)$ & 100 & 100 & 100 & 100 & 100 & 100 \\
    Nguyen-10 & $2\sin(x)\cos(y)$ & 100 & 100 & 91 & 99 & 76 & 64 \\
    Nguyen-11 & $x^y$ & 100 & 100 & 100 & 100 & 7 & 100 \\
    Nguyen-12 & $x^4-x^3+\frac{1}{2}y^2-y$ & 0 & 0 & 0 & 0 & 0 & 0 \\
    \cmidrule{3-8}
    & \multicolumn{1}{r}{Average} & \textbf{91.4} & 83.6 & 75.2 & 46.7 & 60.1 & 73.9 \\
    \end{tabular}
    \label{tab:results}
\end{table*}

We used two popular benchmark problem sets to compare our technique to other methods: Nguyen \citep{nguyen2014bench} and the R rationals \citep{krawiec2012geometric}.
A symbolic regression benchmark problem is defined by a ground truth expression, a set of sampled points from the expression, and a set of allowable tokens.
Additionally, we introduce a new benchmark problem set with this work, which we call Livermore.
The impetus for introducing a new benchmark problem set was that our algorithm achieves nearly perfect scores on Nguyen, so we designed a benchmark problem set with a large range of problem difficulty.
Finally, we include variants Nguyen-12$^\star$, R-1$^\star$, R-2$^\star$, and R-3$^\star$, which use the same expression and set of tokens, but increase the domain over which sampled points are taken.
All benchmarks are described in Appendix Table \ref{tab:benchmarks}.
Hyperparameters are shown in Appendix Table \ref{tab:hyperparameters}.
For all algorithms, we tuned hyperparameters using Nguyen-7 and R-3$^\star$.
None of the Livermore benchmarks were used for hyperparameter tuning. 

We follow the experimental procedure of \citet{petersen2019deep} unless otherwise noted.
For all benchmark problems, we run each algorithm multiple times using a different random number seed.
Experiments were conducted on 36 core, 2.1 GHz, Intel Xeon E5-2695 workstations.
We run each benchmark on a single core, which take an average of 4.4 minutes per run on the Nguyen benchmarks.
Runtimes on individual Nguyen benchmarks are shown in Appendix Table \ref{tab:runtime}.

Our primary empirical performance metric is ``recovery rate,'' defined as the fraction of independent training runs in which an algorithm discovers an expression that is \textit{symbolically equivalent} to the ground truth expression within a maximum of 2 million candidate expressions.
The ground truth expression is used to determine whether the best found candidate was correctly recovered.
Table \ref{tab:results} shows recovery rates on each of the Nguyen benchmark problems compared with DSR, PQT, VPG, GP, and Eureqa \citep{schmidt2009distilling}.
We note that DSR stands as the prior top performer on this set \citep{petersen2019deep}.
As we can see, the recovery rate for our algorithm is 9.3\% higher than the previous leader, DSR.

\begin{table*}[t]
  \centering
  \caption{Recovery rate of several algorithms on the Nguyen, R, and Livermore benchmark problem sets across 25 independent training runs. 95\% confidence intervals are obtained from the standard error between mean recovery on 37 unique benchmark problems. Recovery rates on individual benchmark problems are shown in Appendix Table \ref{tab:all_results}.
  }
    \begin{tabular}{lcccc}
    & \multicolumn{4}{c}{Recovery rate (\%)} \\
    & All & Nguyen & R & Livermore \\
    \cmidrule{2-5}
    Ours & \textbf{74.92} & \textbf{92.33} & \textbf{33.33} & \textbf{71.09} \\
    GEGL \citep{ahn2020gprl} & 64.11 & 86.00 & \textbf{33.33} & 56.36 \\
    Random Restart GP (i.e. GP only) & 63.57 & 88.67 & 2.67 & 58.18 \\
    DSR (i.e. RNN only) \citep{petersen2019deep} & 45.19 & 83.58 & 0.00 & 30.41 \\
    \midrule
    95\% confidence interval & $\pm$1.54 & $\pm$1.76 & $\pm$2.81 & $\pm$1.32 \\
    \end{tabular}
  \label{tab:long_results}
\end{table*}

We next compare against a recent method called GEGL \citep{ahn2020gprl} using the Nguyen, R, and Livermore benchmark problem sets.
GEGL is a hybrid RNN/GP algorithm originally demonstrated for designing small molecules that fit a set of desired parameters.
Many details of the open-source implementation are tied to the particular problem of molecular design (e.g. the mutation operators are specific to molecules), so we adapted their method to symbolic regression (i.e. using the same genetic operators as our method).
In addition to GEGL, we compare against a ``GP only'' and ``RNN only'' version of our method, which are the most critical ablations.
``GP only'' does not use an RNN, and each GP population is seeded using randomly generated expressions; this is essentially GP with random restarts.
``RNN only'' does not use GP, essentially reducing to DSR \citep{petersen2019deep}.
For these experiments, we use the variations Nguyen-12$^\star$, R-1$^\star$, R-2$^\star$, and R-3$^\star$.
Table \ref{tab:long_results} shows that our method ties with GEGL on the R benchmark set and outperforms the other three methods overall. It  also outperforms all other approaches on the other benchmark sets.
It is interesting to note how well random restart GP does on its own, outperforming DSR and on par with GEGL.
Results for individual benchmarks are shown in Appendix Table \ref{tab:all_results}.

Results on additional benchmark problem sets are found in Appendix Tables \ref{tab:jin} and \ref{tab:neat}.
Finally, in Appendix Table \ref{tab:slp_her} we see an additional improvement in recovery rates by repeating our experiments for the Nguyen, R, and Livermore benchmarks using the soft length prior and hierarchical entropy regularizer from \citet{larma2021improving}, which were developed concurrently with this work.

\section{Discussion}

\textbf{Intuition.}
We provide a two-sided perspective as to why we believe our hybrid method outperforms its constituent components so strongly.
First, from the RNN perspective, we believe GP helps by providing "fresh" new samples that help escape local optima. NN-based discrete distributions often concentrate their probability mass on a relatively small portion of the search space, resulting in premature convergence to locally optimal solutions. In contrast, the evolutionary operations of GP generate new individuals that may fall well outside the RNN's concentrated region of the search space.
Second, from the perspective of GP, the RNN helps by learning good starting populations. With a good starting set, GP can easily identify excellent solutions within a few generations. Further, the RNN may act to keep GP from moving too quickly in a suboptimal direction and may have an effect analogous to using a trust region. Each time GP restarts, it falls back to the current probability mass of the training RNN.  

\begin{figure}
  \centering
  \includegraphics[height=4.25 cm]{gp_steps.png}
  \caption{Recovery rate of our method from 25 independent runs on the Nguyen, R, and Livermore benchmark problem sets as a function of number of GP generations per RNN training step.
  Error bars represent the 95\% confidence interval.
  The zero generations case reduces to DSR.}
  \label{fig:gp_steps}
\end{figure}

\textbf{GP generations per RNN training step.}
An important hyperparameter in our method is $S$, the number of GP generations to perform per RNN training step.
We hypothesized that $S$ be proportional to expression length.
That is, the larger an expression is, the more evolutionary operations should be required to transform any arbitrary seed individual into the target expression.
Our initial estimate was that 25 GP steps would be needed per each \alternativeToRL  step.
Figure \ref{fig:gp_steps} shows a post-hoc analysis of how performance varies depending on how many GP steps we do between each \alternativeToRL  step.
The optimal number of steps is between 10 and 25.
We note that the hard limit set by our implementation is 30 tokens.
However, each GP step can make many changes at a time, so we expect the number of needed GP steps to be less than the maximum possible hamming distance.
As such, we can see the similarity between the optimal number of GP steps and the length of expressions.

\begin{table}[t]
  \centering
  \caption{Recovery rates for various ablations of our algorithm (sorted by overall performance) from 25 independent runs on the Nguyen, R, and Livermore benchmark problem sets.}
    \begin{tabular}{lcccc}
    & \multicolumn{4}{c}{Recovery rate (\%)} \\
    & All & Nguyen & R & Livermore \\
    \cmidrule{2-5}
    Trainer = PQT & \textbf{74.92} & 92.33 & \textbf{33.33} & \textbf{71.09} \\
    No RL samples to train RNN (all off-policy) & 74.81 & 93.00 & 29.33 & \textbf{71.09} \\
    Trainer = VPG & 74.27 & \textbf{93.67} & 28.00 & 70.00 \\
    Entropy weight = 0 & 73.95 & 92.00 & 30.67 & 70.00 \\
    Trainer = RSPG & 73.95 & 92.67 & 39.33 & 69.82 \\
    (A): Uniform mutation only & 72.65 & 91.67 & 25.33 & 68.73 \\
    (B): No inv/trig constraint & 71.35 & 91.33 & 32.00 & 65.82 \\
    (A) and (B) & 68.11 & 91.33 & 21.33 & 61.82 \\
    No GP samples to train RNN (all on-policy) & 66.27 & 89.00 & 24.00 & 59.64 \\
    (C): Trainer learning rate = 0 & 65.95 & 90.67 & 17.33 & 59.09 \\
    Random Restart (GP only) & 63.57 & 88.67 & 2.67  & 58.18 \\
    (D): No RL seeds to GP & 63.24 & 89.00 & 2.67  & 57.45 \\
    (C) and (D) & 63.14 & 88.67 & 2.67  & 57.45 \\
    DSR (RL only) \citep{petersen2019deep} & 45.19 & 83.58 & 0.00  & 30.41 \\
    \midrule
    95\% Confidence Interval & $\pm$1.54 & $\pm$1.76 & $\pm$2.81 & $\pm$1.32 \\
    \end{tabular}
  \label{tab:ablations}
\end{table}

\textbf{Ablations.}
We ran several post-hoc ablations to determine the contribution of our various design choices to our method's performance.
Table \ref{tab:ablations} shows the recovery rate for several ablations.
We first note that the choice of RNN training procedure (i.e. VPG, RSPG, or PQT) does not make a large difference; while PQT outperforms VPG and RSPG, the difference falls within the 95\% confidence interval.
Interestingly, removing the entropy regularizer by setting the weight to zero made a small difference in performance.
Other works have found the entropy regularizer to be extremely important \citep{abolafia2018neural, petersen2019deep, larma2021improving}.
We hypothesize that entropy is less important in our hybrid approach because the GP-produced samples provide the RNN with sufficient exploration.
Using different types of mutation in the GP component with equal probability slightly improves performance; we hypothesize that this increases sample diversity.
Including the constraints proposed by \citet{petersen2019deep} also improves performance.

\begin{table}[t]
  \centering
  \caption{Minimum normalized mean-square error (NMSE) from 25 independent runs on three challenging benchmark problems.}
    \begin{tabular}{lcc}
    & \multicolumn{2}{c}{Minimum NMSE} \\
    & Ours & DSR \\
    \cmidrule{2-3}
    Nguyen-12 & $\bm{1.41 \times 10^{-4}}$ & $2.01 \times 10^{-2}$ \\
    R-1 & $\bm{3.55 \times 10^{-5}}$ & $2.74 \times 10^{-3}$ \\
    R-2 & $\bm{2.83 \times 10^{-4}}$ & $5.31 \times 10^{-3}$ \\
\end{tabular}
  \label{tab:hard-cases}
\end{table}

We performed several ablations designed to determine the relative importance of the RNN versus GP components.
Completely removing the RNN leads to a standalone GP algorithm running with random restarts.
This actually performs quite well compared to GP without random restarts. This is not surprising since random-restarts has been shown to help GP by others \citep{houck1996restart, ghannadian1996restart, fukunaga2010restart}.
Similarly, we can set the RNN learning rate to zero.
This ablation essentially reduces to a version of GP with random restarts, in which the starting population is based on a randomly initialized RNN; this performs similarly to GP with random restarts.

We also explore a fully ``off-policy'' ablation, in which only GP samples are considered in the training step (RNN samples are excluded).
Notably, this does not significantly affect performance, as the GP samples (having undergone many generations of GP refinement) are generally superior to the RNN samples.
Similarly, we explore a fully ``on-policy'' version, in which only RNN samples are considered in the training step (GP samples are excluded).
In this case, there is no feedback between GP and the RNN; the only role of GP is to provide strong candidates that may be selected as the best final expression.
This ablation results in a large drop in performance, suggesting that it is important that the RNN be trained on samples from the GP.

Finally, in Appendix Table \ref{tab:extra_ablations} we show additional results when various hyperparameters from \citet{petersen2019deep} or \citet{deap2012} are restored to their original values.

\textbf{Limitations.}
The primary limiting factor to our approach is that there are still some expressions which cannot be completely recovered. 
While it may be a matter of finding the right set of hyperparameters, we have not as yet recovered benchmarks Nguyen-12, R-1, or R-2.
While we have not yet solved three benchmarks, Table \ref{tab:hard-cases} shows that we still significantly outperform DSR in terms of NMSE.
Other challenging benchmarks include R-3, which we recover about 1 in 25 runs, and Livermore-7 and Livermore-8, which we recover about once in every few hundred runs.

\section{Conclusion and Future Work}
We introduce a hybrid approach to solve symbolic regression and other symbolic optimization problems using neural-guided search to generate starting populations for a generic programming component running in a random restart-like fashion.
The results are state-of-the-art on a series of benchmark symbolic regression tasks.

Our future research will focus on methods to mitigate or resolve the off-policy issue of policy gradient-based training methods, by either correcting the weights on the gradient computation or considering alternative optimization objectives.
We will also further examine why Nguyen-12, R-1, and R-2 seem intractable and see if we can come to a solution.
We also plan on doing in-depth analysis of our algorithm with noisy data. 

\section*{Acknowledgements}
We thank the LLNL Hypothesis Testing team, Kate Svyatets, and Miles the cat for their helpful comments, feedback, and support.
This work was performed under the auspices of the U.S. Department of Energy by Lawrence Livermore National Laboratory under contract DE-AC52-07NA27344. Lawrence Livermore National Security, LLC. LLNL-CONF-820015 and was supported by the LLNL-LDRD Program under Projects 19-DR-003 and 21-SI-001.
The authors have no competing interests to report. 

\bibliography{main}
\bibliographystyle{plainnat}
\clearpage
\appendix

% Put all appendix floats (tables,figure) at the top of the page
\makeatletter
\setlength{\@fptop}{0pt}
\setlength{\@fpbot}{0pt plus 1fil}
\makeatother

\section*{Appendix}

\begin{table}[htbp]
  \centering
  \caption{Recovery rates for individual benchmark problems.}
    \begin{tabular}{lcccc}
    & \multicolumn{4}{c}{Recovery rate (\%)} \\
    & DSR & Random Restart GP & GEGL & Ours \\
    \cmidrule{2-5}
    Nguyen-1 & 100 & 100 & 100 & 100 \\
    Nguyen-2 & 100 & 100 & 100 & 100 \\
    Nguyen-3 & 100 & 100 & 100 & 100 \\
    Nguyen-4 & 100 & 100 & 100 & 100 \\
    Nguyen-5 & 72  & 100& 92  & 100 \\
    Nguyen-6 & 100 & 100 & 100 & 100 \\
    Nguyen-7 & 35  & 64  & 48  & 96 \\
    Nguyen-8 & 96  & 100 & 100 & 100 \\
    Nguyen-9 & 100 & 100 & 100 & 100 \\
    Nguyen-10 & 100 & 100 & 92  & 100 \\
    Nguyen-11 & 100 & 100 & 100 & 100 \\
    Nguyen-12$^\star$ & 0   & 0   & 0   & 12 \\
    \midrule
    \textbf{Nguyen average} & 83.58 & 88.67 & 86.00 & \textbf{92.33} \\
    \midrule
    R-1$^\star$   & 0   & 4   & 0   & 4 \\
    R-2$^\star$   & 0   & 0   & 0   & 4 \\
    R-3$^\star$   & 0   & 4   & 100 & 92 \\
    \midrule
    \textbf{R average} & 0.00 & 2.67 & \textbf{33.33} & \textbf{33.33} \\
    \midrule
    Livermore-1 & 3   & 100 & 100 & 100 \\
    Livermore-2 & 87  & 92  & 44  & 100 \\
    Livermore-3 & 66  & 100 & 100 & 100 \\
    Livermore-4 & 76  & 100 & 100 & 100 \\
    Livermore-5 & 0   & 0   & 0   & 4 \\
    Livermore-6 & 97  & 4   & 64  & 88 \\
    Livermore-7 & 0   & 0   & 0   & 0 \\
    Livermore-8 & 0   & 0   & 0   & 0 \\
    Livermore-9 & 0   & 0   & 12  & 24 \\
    Livermore-10 & 0   & 0   & 0   & 24 \\
    Livermore-11 & 17  & 100 & 92  & 100 \\
    Livermore-12 & 61  & 100 & 100 & 100 \\
    Livermore-13 & 55  & 96  & 84  & 100 \\
    Livermore-14 & 0   & 96  & 100 & 100 \\
    Livermore-15 & 0   & 92  & 96  & 100 \\
    Livermore-16 & 4   & 92  & 12  & 92 \\
    Livermore-17 & 0   & 64  & 4   & 68 \\
    Livermore-18 & 0   & 32  & 0   & 56 \\
    Livermore-19 & 100 & 100 & 100 & 100 \\
    Livermore-20 & 98  & 100 & 100 & 100 \\
    Livermore-21 & 2   & 12  & 64  & 24 \\
    Livermore-22 & 3   & 0   & 68  & 84 \\
    \midrule
    \textbf{Livermore average} & 30.41 & 58.18 & 56.36 & \textbf{71.09} \\
    \midrule
    \textbf{All average} & 45.19 & 63.57 & 64.11 & \textbf{74.92} \\
    \midrule
    \end{tabular}
  \label{tab:all_results}
\end{table}

\begin{table}[htbp]
  \centering
  \caption{Single-core runtimes of our algorithm on the Nguyen benchmark problem set.}
    \begin{tabular}{lc}
          Benchmark & Runtime (sec) \\
    \midrule
    Nguyen-1 & 27.05 \\
    Nguyen-2 & 59.79 \\
    Nguyen-3 & 151.06 \\
    Nguyen-4 & 268.88 \\
    Nguyen-5 & 501.65 \\
    Nguyen-6 & 43.96 \\
    Nguyen-7 & 752.32 \\
    Nguyen-8 & 123.21 \\
    Nguyen-9 & 31.17 \\
    Nguyen-10 & 103.72 \\
    Nguyen-11 & 66.50 \\
    Nguyen-12$^\star$ & 1057.11 \\
    \midrule
    Average & 265.54 \\
    \end{tabular}
  \label{tab:runtime}
\end{table}

\begin{table}[htbp]
  \centering
  \caption{Comparison of mean root-mean-square error (RMSE) for our method to literature-reported values from DSR \citep{petersen2019deep} and Bayesian symbolic regression (BSR) \citep{jin2019bayesian} on the Jin benchmark problem set.
  Note that these benchmarks include floating-point constant values that are optimized as part of the reward function computation, as in \citet{petersen2019deep}. Table \ref{tab:benchmarks-const} shows the formulas for these benchmarks. }
    \begin{tabular}{lcccc}
    & \multicolumn{4}{c}{Mean RMSE} \\
    & Ours & DSR & BSR & Recovered by Ours \\
    \cmidrule{2-5}
    Jin-1 & \textbf{0} & 0.46 & 2.04 & Yes \\
    Jin-2 & \textbf{0} & \textbf{0} & 6.84 & Yes \\
    Jin-3 & \textbf{0} & 0.00052 & 0.21 & Yes \\
    Jin-4 & \textbf{0} & 0.00014 & 0.16 & Yes \\
    Jin-5 & \textbf{0} & \textbf{0} & 0.66 & Yes \\
    Jin-6 & \textbf{0} & 2.23 & 4.63 & Yes \\
    \midrule
    Average & \textbf{0} & 0.45 & 2.42 & \\
    \end{tabular}
  \label{tab:jin}
\end{table}

\begin{table}[htbp]
  \centering
  \caption{Comparison of median root-mean-square error (RMSE) for our method to literature-reported values from DSR \citep{petersen2019deep} and \textit{neat} genetic programming (Neat-GP) \citep{trujillo2016neat} on the Neat benchmark problem set.
  Note that Neat-6, Neat-7, and Neat-8 are not fully recoverable given the function set prescribed by the benchmark. Table \ref{tab:benchmarks-const} shows the formulas for these benchmarks.}
    \begin{tabular}{lcccc}
    & \multicolumn{4}{c}{Median RMSE} \\
    & Ours & DSR & Neat-GP & Recovered by Ours \\
    \cmidrule{2-5}    
    Neat-1 & \textbf{0} & \textbf{0} & 0.0779 & Yes \\
    Neat-2 & \textbf{0} & \textbf{0} & 0.0576 & Yes \\
    Neat-3 & \textbf{0} & 0.0041 & 0.0065 & Yes \\
    Neat-4 & \textbf{0} & 0.0189 & 0.0253 & Yes \\
    Neat-5 & \textbf{0} & \textbf{0} & 0.0023 & Yes \\
    Neat-6 & $\bm{6.1 \times 10^{-6}}$ & 0.2378 & 0.2855 & --- \\
    Neat-7 & \textbf{1.0028} & 1.0606 & 1.0541 & --- \\
    Neat-8 & \textbf{0.0228} & 0.1076 & 0.1498 & --- \\
    Neat-9 & \textbf{0} & 0.1511 & 0.1202 & Yes \\
    \midrule
    Average & \textbf{0.1139} & 0.1756 & 0.1977 & \\
    \end{tabular}
  \label{tab:neat}
\end{table}

\begin{table}[htbp]
    \centering
    \caption{Benchmark symbolic regression problem specifications.
Input variables are denoted by $x$ and/or $y$.
$U\left(a, b, c\right)$ denotes $c$ random points uniformly sampled between $a$ and $b$ for each input variable; training and test datasets use different random seeds.
$E\left(a, b, c\right)$ denotes $c$ points evenly spaced between $a$ and $b$ for each input variable; training and test datasets use the same points.
All benchmark problems use the following set of allowable tokens: $\{ +, -, \times, \div, \sin, \cos, \exp, \log, x, y \}$ ($y$ is excluded for single-dimensional datasets).}
    \begin{tabular}{lll}
    Name  & Expression & Dataset \\
    \midrule
    Nguyen-1 & $x^3+x^2+x$ & $U\left(-1, 1, 20\right)$ \\
    Nguyen-2 & $x^4+x^3+x^2+x$ & $U\left(-1, 1, 20\right)$ \\
    Nguyen-3 & $x^5+x^4+x^3+x^2+x$ & $U\left(-1, 1, 20\right)$ \\
    Nguyen-4 & $x^6+x^5+x^4+x^3+x^2+x$ & $U\left(-1, 1, 20\right)$ \\
    Nguyen-5 & $\sin(x^2)\cos(x)-1$ & $U\left(-1, 1, 20\right)$ \\
    Nguyen-6 & $\sin(x)+\sin(x+x^2)$ & $U\left(-1, 1, 20\right)$ \\
    Nguyen-7 & $\log(x+1)+\log(x^2+1)$ & $U\left(0, 2, 20\right)$ \\
    Nguyen-8 & $\sqrt{x}$ & $U\left(0, 4, 20\right)$ \\
    Nguyen-9 & $\sin(x)+\sin(y^2)$ & $U\left(0, 1, 20\right)$ \\
    Nguyen-10 & $2\sin(x)\cos(y)$ & $U\left(0, 1, 20\right)$ \\
    Nguyen-11 & $x^y$ & $U\left(0, 1, 20\right)$ \\
    Nguyen-12 & $x^4-x^3+\frac{1}{2}y^2-y$ & $U\left(0, 1, 20\right)$ \\
    Nguyen-12$^\star$ & $x^4-x^3+\frac{1}{2}y^2-y$ & $U\left(0, 10, 20\right)$ \\
    \midrule
    R-1    & $\frac{{\left(x+1\right)}^3}{{x}^2-x+1}$ & $E\left(-1,1,20\right)$ \\
    R-2    & $\frac{{x}^5-3{x}^3+1}{{x}^2+1}$ & $E\left(-1,1,20\right)$ \\
    R-3    & $\frac{{x}^6+{x}^5}{{x}^4+{x}^3+{x}^2+x+1}$ & $E\left(-1,1,20\right)$ \\
    R-1$^\star$   & $\frac{{\left(x+1\right)}^3}{{x}^2-x+1}$ & $E\left(-10,10,20\right)$ \\
    R-2$^\star$   & $\frac{{x}^5-3{x}^3+1}{{x}^2+1}$ & $E\left(-10,10,20\right)$ \\
    R-3$^\star$   & $\frac{{x}^6+{x}^5}{{x}^4+{x}^3+{x}^2+x+1}$ & $E\left(-10,10,20\right)$ \\
    \midrule
    Livermore-1 & $\frac{1}{3}+x+\sin\left({x}^2\right)$ & $U\left(-10,10,1000\right)$ \\
    Livermore-2 & $\sin\left({x}^2\right) \cos\left(x\right)-2$ & $U\left(-1,1,20\right)$ \\
    Livermore-3 & $\sin\left({x}^3\right) \cos\left({x}^2\right)-1$ & $U\left(-1,1,20\right)$ \\
    Livermore-4 & $\log(x+1)+\log({x}^2+1)+\log(x)$ & $U\left(0,2,20\right)$ \\
    Livermore-5 & ${x}^4-{x}^3+{x}^2-y$ & $U\left(0,1,20\right)$ \\
    Livermore-6 & $4{x}^4+3{x}^3+2{x}^2+x$ & $U\left(-1,1,20\right)$ \\
    Livermore-7 & $\sinh(x)$ & $U\left(-1,1,20\right)$ \\
    Livermore-8 & $\cosh(x)$ & $U\left(-1,1,20\right)$ \\
    Livermore-9 & ${x}^9+{x}^8+{x}^7+{x}^6+{x}^5+{x}^4+{x}^3+{x}^2+x$ & $U\left(-1,1,20\right)$ \\
    Livermore-10 & $6\sin\left(x\right) \cos\left(y\right)$ & $U\left(0,1,20\right)$ \\
    Livermore-11 & $\frac{{x}^2 {x}^2}{x+y}$ & $U\left(-1,1,50\right)$ \\
    Livermore-12 & $\frac{{x}^5}{{y}^3}$ & $U\left(-1,1,50\right)$ \\
    Livermore-13 & ${x}^{\frac{1}{3}}$ & $U\left(0,4,20\right)$ \\
    Livermore-14 & ${x}^3+{x}^2+x+\sin\left(x\right)+\sin\left({x}^2\right)$ & $U\left(-1,1,20\right)$ \\
    Livermore-15 & ${x}^{\frac{1}{5}}$ & $U\left(0,4,20\right)$ \\
    Livermore-16 & ${x}^{\frac{2}{5}}$ & $U\left(0,4,20\right)$ \\
    Livermore-17 & $4\sin\left(x\right) \cos\left(y\right)$ & $U\left(0,1,20\right)$ \\
    Livermore-18 & $\sin\left({x}^2\right) \cos\left(x\right)-5$ & $U\left(-1,1,20\right)$ \\
    Livermore-19 & ${x}^5+{x}^4+{x}^2+x$ & $U\left(-1,1,20\right)$ \\
    Livermore-20 & $\operatorname{exp}\left({-x}^2\right)$ & $U\left(-1,1,20\right)$ \\
    Livermore-21 & ${x}^8+{x}^7+{x}^6+{x}^5+{x}^4+{x}^3+{x}^2+x$ & $U\left(-1,1,20\right)$ \\
    Livermore-22 & $\operatorname{exp}\left(-0.5{x}^2\right)$ & $U\left(-1,1,20\right)$ \\
    \end{tabular}
  \label{tab:benchmarks}
\end{table}

\begin{table*}[h]
\centering
\caption{
Benchmark symbolic regression problems that include unknown constants.
Input variables are denoted by $x$ and/or $y$.
U$(a, b, c)$ denotes $c$ random points uniformly sampled between $a$ and $b$ for each input variable; training and test datasets use different random seeds.
E$(a, b, c)$ denotes $c$ points evenly spaced between $a$ and $b$ for each input variable; training and test datasets use the same points (except Neat-6, which uses E$(1, 120, 120)$ as test data, and the Jin tests, which use U$(-3, 3, 30)$ as test data).
To simplify notation, libraries are defined relative to a ``base'' library $\base = \{ +, -, \times, \div, \sin, \cos, \exp, \log, x \}$.
Placeholder operands are denoted by $\placeholder$, e.g. $\placeholder^2$ corresponds to the square operator.}
\begin{small}
\begin{tabular}{ccccc}
Name & Expression & Dataset & Library \\  
\midrule
Jin-1 & $2.5 x^4-1.3 x^3 +0.5 y^2 - 1.7y$ & U$(-3, 3, 100)$ & \Jin \\
Jin-2 & $8.0 x^2 + 8.0 y^3 - 15.0$ & U$(-3, 3, 100)$ & \Jin \\
Jin-3 & $0.2 x^3 + 0.5 y^3 - 1.2 y - 0.5 x$ & U$(-3, 3, 100)$ & \Jin \\
Jin-4 & $1.5 \exp(x) + 5.0 \cos(y)$ & U$(-3, 3, 100)$ & \Jin \\
Jin-5 & $6.0 \sin(x) \cos(y)$ & U$(-3, 3, 100)$ & \Jin \\
Jin-6 & $1.35 x y + 5.5 \sin((x - 1.0)(y - 1.0))$ & U$(-3, 3, 100)$ & \Jin \\
\midrule
Neat-1 & $x^4+x^3+x^2+x$ & U$(-1, 1, 20)$ & \NguyenXOne \\
Neat-2 & $x^5+x^4+x^3+x^2+x$ & U$(-1, 1, 20)$ & \NguyenXOne \\
Neat-3 & $\sin(x^2)\cos(x)-1$ & U$(-1, 1, 20)$ & \NguyenXOne \\
Neat-4 & $\log(x+1)+\log(x^2+1)$ & U$(0, 2, 20)$ & \NguyenXOne \\
Neat-5 & $2\sin(x)\cos(y)$ & U$(-1, 1, 100)$ & \NguyenXY \\
Neat-6 & $\sum_{k=1}^x \frac{1}{k} $ & E$(1, 50, 50)$ & \Keijzer \\
Neat-7 & $2 - 2.1\cos(9.8x)\sin(1.3y)$ & E$(-50, 50, 10^5)$ & \Korns \\
Neat-8 & $\frac{e^{-(x-1)^2}}{1.2 + (y-2.5)^2}$ & U$(0.3, 4, 100)$ & \VladislavlevaB \\
Neat-9 & $\frac{1}{1+x^{-4}} + \frac{1}{1+y^{-4}}$ & E$(-5, 5, 21)$ & \NguyenXY \\
\newline
\end{tabular}
\end{small}
\label{tab:benchmarks-const}
\end{table*}

\begin{table}[htbp]
  \centering
  \caption{Hyperparameter values for all experiments, unless otherwise noted. If an applicable hyperparameter value differs from one of the baseline methods, it is noted.}
    \begin{tabular}{lccl}
    \multicolumn{1}{c}{\textbf{Hyperparameter}} & \textbf{Symbol} & \textbf{Value} & \multicolumn{1}{c}{\textbf{Comment}} \\
    \midrule
    \textbf{Shared Parameters} & & & \\
    \midrule
    Batch/population size & $N$     & 500 & \citet{petersen2019deep}: 1000\\
    & & & \citet{deap2012}: 300\\
    Minimum expression length  & --    & 4 & --\\
    Maximum expression length & --    & 30 & --\\
    Maximum expressions & --    & 2,000,000 & --\\
    Maximum constants & --    & $\infty$ & \citet{petersen2019deep}: 3 \\
    & & & (only used for Jin benchmarks) \\
    Reward/fitness function & $R$ & Inverse NRMSE & --\\
    \midrule
    \textbf{RNN Parameters} &       &  & \\
    \midrule
    Optimizer & --    & Adam & --\\
    RNN cell type & -- & LSTM & --\\
    RNN cell layers & -- & 1 & --\\
    RNN cell size & -- & 32 & --\\
    Training method & --    & PQT & --\\
    PQT queue size & --    & 10 & --\\
    Sample selection size & $M$     & 1 & --\\
    Learning rate & $\alpha$ & 0.0025 & \citet{petersen2019deep}: 0.0005\\
    Entropy weight & -- & 0.005 & --\\
    \midrule
    \textbf{GP Parameters} & & &\\
    \midrule
    Generations per iteration & $S$     & 25 & -- \\
    Crossover operator & -- & One Point & -- \\
    Crossover probability & $P_c$   & 0.5 & -- \\
    Mutation operator & -- & Multiple & \citet{deap2012}: Uniform\\
    Mutation probability & $P_m$   & 0.5 & \citet{deap2012}: 0.1\\
    Selection operator & -- & Tournament & -- \\
    Tournament size & $k$     & 5 & \citet{deap2012}: 3 \\
    Mutate tree maximum & -- & 3 & \citet{deap2012}: 2 \\
    \end{tabular}
  \label{tab:hyperparameters}
\end{table}

\begin{table}[htbp]
  \centering
  \caption{Additional ablations when using original hyperparameter values from \citet{petersen2019deep} and/or \citet{deap2012} rather than the values in Table \ref{tab:hyperparameters}.
  }
    \begin{tabular}{lcccc}
    & \multicolumn{4}{c}{Recovery rate (\%)} \\
    & All & Nguyen & R & Livermore \\
    \cmidrule{2-5}
    Mutation probability $P_m = 0.1$ & 75.24 & 90.33 & 32.00 & 72.91 \\
    Baseline (Table \ref{tab:hyperparameters} values) & 74.92 & 92.33 & 33.33 & 71.09 \\
    Tournament size $k = 3$ & 73.62 & 92.00 & 29.33 & 69.64 \\
    Batch size $N = 1000$ & 72.11 & 93.33 & 25.33 & 66.91 \\
    RNN learning rate $\alpha = 0.0005$ & 71.03 & 91.67 & 22.67 & 66.36 \\
    \midrule
    95\% confidence interval & $\pm$1.54 & $\pm$1.76 & $\pm$2.81 & $\pm$1.32 \\
    \end{tabular}
  \label{tab:extra_ablations}
\end{table}

\begin{table}[htbp]
  \centering
  \caption{
  Recovery rates when using the soft length prior (SLP) and hierarchical entropy regularizer (HER) introduced in \citet{larma2021improving}2021a], and increasing maximum length from 30 to 100.
  These results are post-hoc, as \citet{larma2021improving} was performed concurrently.}
    \begin{tabular}{lcc}
          & \multicolumn{2}{c}{Recovery rate (\%)}  \\
          & Ours & Ours + SLP/HER \\
\cmidrule{2-3}    Nguyen-1 & 100 & 100 \\
    Nguyen-2 & 100 & 100 \\
    Nguyen-3 & 100 & 100 \\
    Nguyen-4 & 100 & 100 \\
    Nguyen-5 & 100 & 100 \\
    Nguyen-6 & 100 & 100 \\
    Nguyen-7 & 96 & 100 \\
    Nguyen-8 & 100 & 100 \\
    Nguyen-9 & 100 & 100 \\
    Nguyen-10 & 100 & 100 \\
    Nguyen-11 & 100 & 100 \\
    Nguyen-12$^\star$ & 12 & 4 \\
    \midrule
    \textbf{Nguyen average} & \textbf{92.33} & 92.00 \\
    \midrule
    R-1$^\star$  & 4.00  & 100.00 \\
    R-2$^\star$  & 4.00  & 100.00 \\
    R-3$^\star$  & 92.00 & 96.00 \\
    \midrule
    \textbf{R average} & 33.33 & \textbf{98.67} \\
    \midrule
    Livermore-1 & 100 & 100 \\
    Livermore-2 & 100 & 100 \\
    Livermore-3 & 100 & 100 \\
    Livermore-4 & 100 & 100 \\
    Livermore-5 & 4  & 40 \\
    Livermore-6 & 88 & 100 \\
    Livermore-7 & 0  & 4 \\
    Livermore-8 & 0  & 0 \\
    Livermore-9 & 24 & 88 \\
    Livermore-10 & 24 & 8 \\
    Livermore-11 & 100 & 100 \\
    Livermore-12 & 100 & 100 \\
    Livermore-13 & 100 & 100 \\
    Livermore-14 & 100 & 100 \\
    Livermore-15 & 100 & 100 \\
    Livermore-16 & 92 & 100 \\
    Livermore-17 & 68 & 36 \\
    Livermore-18 & 56 & 48 \\
    Livermore-19 & 100 & 100 \\
    Livermore-20 & 100 & 100 \\
    Livermore-21 & 24 & 88 \\
    Livermore-22 & 84 & 92 \\
    \midrule
    \textbf{Livermore average} & 71.09 & \textbf{77.45} \\
    \midrule
    \textbf{All average} & 74.92 & \textbf{83.89} \\
    \midrule
    \end{tabular}%
  \label{tab:slp_her}%
\end{table}%
\end{document}