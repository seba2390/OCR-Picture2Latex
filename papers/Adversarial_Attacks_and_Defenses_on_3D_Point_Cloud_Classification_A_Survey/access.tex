\documentclass{ieeeaccess}
% \usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage[hidelinks]{hyperref}
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}

\def\etal{\textit{et al.}}
%%
\usepackage{multirow}
\usepackage{comment}
\usepackage{subcaption}
\usepackage[numbers,sort]{natbib}
\usepackage{multirow}
% \usepackage{lscape} 
\usepackage{array, booktabs, tabularx,makecell}
\usepackage{soul}
% \usepackage{xcolor}
% \usepackage{cite}
% \sethlcolor{yellow}
%%%
\newcommand{\sign}{\text{sign}}


    
\begin{document}
\history{Date of publication xxxx 00, 0000, date of current version xxxx 00, 0000.}
\doi{10.1109/ACCESS.2017.DOI}


\title{Adversarial Attacks and Defenses on 3D Point Cloud Classification: A Survey}
\author{\uppercase{Hanieh Naderi}\authorrefmark{1}, \IEEEmembership{Student Member,~IEEE},
\\
\uppercase{Ivan V. Baji\'{c}\authorrefmark{2}
\IEEEmembership{Senior Member,~IEEE},
}}
% , and Third C. Author,
% Jr}.\authorrefmark{3},
% \IEEEmembership{Senior~Member,~IEEE}}
\address[1]{Department of Computer Engineering, Sharif University of Technology Tehran (e-mail: hanieh.naderii@gmail.com)}
\address[2]{School of Engineering
Science, Simon Fraser University, Burnaby, BC, Canada (e-mail: ibajic@ensc.sfu.ca)}
% \address[3]{Electrical Engineering Department, University of Colorado, Boulder, CO 
% 80309 USA}



\begin{abstract}

Deep learning has successfully solved a wide range of tasks in 2D vision as a dominant AI technique. Recently, deep learning on 3D point clouds has become increasingly popular for addressing various tasks in this field. Despite remarkable achievements, deep learning algorithms are vulnerable to adversarial attacks. These attacks are imperceptible to the human eye, but can easily fool deep neural networks in the testing and deployment stage.
To encourage future research, this survey summarizes the current progress on adversarial attack and defense techniques on point-cloud classification. This paper first introduces the principles and characteristics of adversarial attacks and summarizes and analyzes adversarial example generation methods in recent years. Additionally, it provides an overview of defense strategies, organized into data-focused and model-focused methods.  Finally, it presents several current challenges and potential future research directions in this domain.

\end{abstract}

\begin{keywords}
3D deep learning, deep neural network, adversarial examples, adversarial defense, machine learning security, 3D point clouds.
\end{keywords}

\titlepgskip=-15pt

\maketitle
\begingroup\renewcommand\thefootnote{\textsection}

\endgroup
%-----------------------Introduction---------------------------
\section{Introduction}
\label{sec:introduction}

\PARstart{D}{eep} learning (DL) \cite{lecun2015deep} is a subset of machine learning (ML) and artificial intelligence (AI) that analyzes large amounts of data %with
using a structure roughly similar to the human brain. Deep learning is characterized by the use of multiple layers of neural networks, which process and analyze large amounts of data. These neural networks are trained on large datasets, which allows them to learn patterns and make decisions on their own. DL has achieved impressive results in the fields of image recognition \cite{li2022research,krizhevsky2017imagenet,naderi2020scale}, semantic analysis \cite{silberman2011indoor,mo2022review}, speech recognition \cite{nassif2019speech,taher2021deep} and natural language processing \cite{chowdhary2020natural} in recent years. 



Despite the tremendous success of DL, in 2013 Szegedy \etal~\cite{szegedy2014intriguing} found that deep models are vulnerable to adversarial examples in image classification tasks. Adversarial examples are inputs to a deep learning model that have been modified in a way that is intended to mislead the model. In the context of image classification, for example, an adversarial example might be a picture of a panda that has been slightly modified in a way that is imperceptible to the human eye but that causes a deep learning model to classify the image as a gibbon. Adversarial examples can be created in two or three dimensions. In the case of 2D adversarial examples, the input is an image, and the modification is applied to the pixels of the image. These modifications can be small perturbations added to the image pixels~\cite{moosavi2016deepfool} or they can be more significant changes to the structure of the image~\cite{eykholt2018robust}.

Thanks to the rapid development of 3D acquisition technologies, various types of 3D scanners, LiDARs, and RGB-D cameras have become increasingly affordable. 3D data is often used as an input for Deep Neural Networks (DNNs) in %many scenarios, including 
healthcare~\cite{mozaffari2014systematic}, self-driving cars~\cite{badue2021self}, drones~\cite{hassanalian2017classifications}, robotics~\cite{pierson2017deep}, and many other applications. These 3D data, compared to 2D counterparts, %(a compressed form of 3D data), 
capture more information from the environment, thereby allowing more sophisticated analysis. %Therefore can cause more accurate results as an output. 
There are different representations of 3D data, like voxels~\cite{liu2019point}, meshes~\cite{ladicky2017point}, and point clouds~\cite{qi2017pointnet}. Since point clouds can be received directly from scanners, they can precisely capture shape details.
Therefore, it is the preferred representation for many safety-critical applications. Due to this, in the case of 3D adversarial examples, the input is a point cloud, and the modification is applied to the points in the cloud. These examples can be created by adding, dropping, and shifting some points in the input point clouds, or by generating entirely new point clouds with predefined target labels using methods such as Generative Adversarial Networks (GANs) or other transformation techniques.
It is typically easier to create adversarial examples in 2D space than in 3D space because the input space is smaller and there are fewer dimensions to perturb. 
In general, adversarial examples exploit the vulnerabilities or weaknesses in the model's prediction process, and they can be very difficult to detect because they are often indistinguishable from normal examples to the human eye. As a result, adversarial examples can pose a serious threat to the security and reliability of DL %deep learning 
models. Therefore, it is important to have effective methods for defending against adversarial examples in order to ensure the robustness and reliability of DL %deep learning 
models.

Adversarial defense in the 2D image and the 3D point clouds both seek to protect DL %deep learning 
models from being fooled by adversarial examples. However, there are some key differences between the approaches used to defend against adversarial images and adversarial point clouds. Some of the main differences include the following:

\begin{itemize}
    \item Input data: Adversarial images are 2D data representations, while adversarial point clouds are 3D data representations. This means that the approaches used to defend against adversarial images and point clouds may need to take into account the different dimensions and characteristics of the input data.
    \item Adversarial perturbations: Adversarial images may be modified using small perturbations added to the image pixels, while adversarial point clouds may be modified using perturbations applied to individual points or groups of points in the point cloud. This means that the approaches used to defend against adversarial images and point clouds may need to be tailored to the specific types of adversarial perturbations that are being used.
    \item Complexity: Adversarial point clouds may be more complex to defend against than adversarial images, as the perturbations applied to point clouds may be more difficult to identify and remove. This may require the use of more sophisticated defenses, such as methods that are able to detect and remove adversarial perturbations from the input point cloud.
\end{itemize}

On the whole, adversarial point clouds can be challenging to identify and defend against, as they may not be easily recognizable in the 3D point cloud data. Adversarial point clouds may be more harmful and harder to defend against, because their changes may be less obvious to humans due to the lack of familiarity compared to images. %and the key adversarial characterization of being imperceptible compared to adversarial images. 
As a result, it is important to conduct a thorough survey of adversarial attacks and defenses on 3D point clouds in order to identify the challenges and limitations of current approaches and to identify opportunities for future research in this area.
% In 2019, Xiang \etal~\cite{xiang2019generating} generated the first adversarial attack on 3D point clouds. With the introduction of adversarial attacks on point clouds, researchers have begun to generate different types of attacks and defenses in this field and have achieved breakthrough results. Therefore, attacks and defenses on 3D point clouds draw great attention. 
There are a number of published surveys that review adversarial attacks and defenses in general, including in the context of computer vision, ML, and AI systems. %These surveys provide an overview of the various types of attacks and defenses that have been proposed, as well as their strengths and limitations.
For example, Akhtar \etal~\cite{akhtar2018threat} focus on adversarial attacks in computer vision, with a particular emphasis on image and video recognition systems. Yuan \etal~\cite{yuan2019adversarial} delve into both adversarial attacks and defense mechanisms within the domain of images. Qiu \etal~\cite{qiu2019review} provide a comprehensive review of adversarial attacks and defenses in various AI domains, including image, video, and text. Wei \etal~\cite{wei2022physical} survey both attacks and defenses against physical 2D objects. Zhai \etal~\cite{zhai2023state} explore adversarial attacks and defenses within the context of graph-based data. Bountakas \etal~\cite{bountakas2023defense} review domain-agnostic defense strategies across multiple domains, including audio, cybersecurity, natural language processing (NLP), and computer vision. Pavlitska \etal~\cite{pavlitska2023adversarial} focus on adversarial attacks within the specific domain of traffic sign recognition, which is relevant to autonomous vehicles and road safety. These and several other surveys of adversarial attacks and defenses in various domains have been summarized in Table~\ref{surveys}. As seen in the table, there is a lack of surveys focused specifically on 3D point cloud attacks and defenses. Some published surveys do mention 3D attacks and defenses briefly, for example~\cite{akhtar2021advances}, but there is a need for more comprehensive surveys that delve deeper into this topic. 


%Table~\ref{surveys} summarizes} published surveys of adversarial attacks and defenses in different application domains. It provides an overview of research papers, specifying the area of focus for each survey, such as computer vision, image, AI, physical 2D objects, graphs, audio, cybersecurity, and traffic sign recognition. The table also highlights whether the survey primarily concentrates on adversarial attacks, defenses, or both, and includes the publication year of each survey for reference.} 

% Table~\ref{surveys} refers to a summary or overview of published surveys of adversarial attacks and defenses. Some of these surveys focus on specific domains, such as computer vision~\cite{akhtar2018threat, akhtar2021advances, wei2022physical}, text~\cite{qiu2022adversarial}, and images~\cite{khamaiseh2022adversarial,yuan2019adversarial,chakraborty2021survey, michel2022survey} while others provide a more general overview of adversarial attacks and defenses in the field of artificial intelligence~\cite{qiu2019review,gupta2022adversarial}.


While our survey is focused on adversarial attacks and defenses on 3D point cloud classification, it is important to mention that there are existing general surveys on point cloud analysis and processing, which are not focused on adversarial attacks and defenses. For example, 
Guo \etal~\cite{guo2020deep} provide a comprehensive overview of deep learning methods for point cloud analysis, including classification, detection, and segmentation. Xiao \etal~\cite{xiao2023unsupervised} concentrate on unsupervised point cloud analysis. Nguyen \etal~\cite{xie2020linking} and Xie \etal~\cite{xie2020linking} specifically address point cloud segmentation tasks. Zhang \etal~\cite{zhang2023deep} focus on point cloud classification. Fernandes \etal~\cite{fernandes2021point} discuss point cloud processing in specialized tasks like self-driving, while Krawczyk \etal~\cite{krawczyk2023segmentation} tackle full human body geometry segmentation. Cao \etal~\cite{cao20193d} explore compression methods for 3D point clouds, essential for handling large data volumes. Although the focus of our survey is on 3D point cloud attacks and defenses, there is an intersection with some of the aforementioned surveys, especially in terms of models and datasets used for point cloud classification. We review the models and datasets that are relevant to the area of adversarial attacks and defenses, which can also be valuable resources for the broader community working on point cloud analysis and processing.


% %--------------------Table4------------------------------------
\begin{table*}
\begin{center}
\caption{A review of published surveys of adversarial attacks and defenses.}
\label{surveys} 
    \centering
    
    %\begin{tabular*}{\textwidth}{c|c|c c c c c c}
    \begin{tabular}{ c  c  c  c }
    %\hline
    \toprule
    
    %\multicolumn{1}{|c|}{Defenses}  & \multicolumn{7}{c|}{Attacks}\\
    \bf Survey  &\bf Application Domain  &\bf Focus on &\bf Year  \\
    %\hline
    \midrule

    
    Akhtar \etal~\cite{akhtar2018threat} & Computer Vision (Image \& Video) & Attack & 2018  \\
    
    Yuan \etal~\cite{yuan2019adversarial} & Image & Attack \& defense &  2018 \\

    Qiu \etal~\cite{qiu2019review} & AI (Image \& Video \& Text) & Attack \& defense & 2019\\

    Wiyatno \etal~\cite{wiyatno2019adversarial} & ML (Image) & Attack & 2019\\

    Xu \etal~\cite{xu2020adversarial} & Image \& Graph \& Text & Attack \& defense & 2020 \\
    
    Martins \etal~\cite{martins2020adversarial} & Cybersecurity & Attack & 2020\\
        
    Chakraborty \etal~\cite{chakraborty2021survey} & Image \& Video & Attack \& defense & 2021 \\
    
    Rosenberg \etal~\cite{rosenberg2021adversarial} & Cybersecurity & Attack \& defense & 2021 \\    
    
    Akhtar \etal~\cite{akhtar2021advances} & Computer Vision (Image \& Video) & Attack \& defense  & 2021 \\
    
    Michel \etal~\cite{michel2022survey} & Image & Attack \& defense & 2022 \\
    
    Tan \etal~\cite{tan2022adversarial} & Audio & Attack \& defense & 2022 \\
    
    Qiu \etal~\cite{qiu2022adversarial} & Text & Attack \& defense & 2022 \\
    
    Liang \etal~\cite{liang2022adversarial} & Image & Attack \& defense & 2022 \\
    
    Li \etal~\cite{li2022review} & Image & Attack \& defense & 2022\\
    
    Gupta \etal~\cite{gupta2022adversarial} & AI (All) & Attack \& defense & 2022 \\
    
    
    Wei \etal~\cite{wei2022physically} & Physical 2D object & Attack \& defense & 2022 \\
    
      Wei \etal~\cite{wei2022physical} & Physical 2D object & Attack & 2022 \\
    
    Mi \etal~\cite{mi2022adversarial} & Object & Attack & 2022 \\

    
    Khamaiseh \etal~\cite{khamaiseh2022adversarial} & Image & Attack \& defense & 2022 \\


    Pavlitska \etal~\cite{pavlitska2023adversarial} & Image (Traffic Sign Recognition) & Attack & 2023 \\


    Kotyan \etal~\cite{kotyan2023reading} & ML & Attack & 2023 \\


    Zhai \etal~\cite{zhai2023state} & Graph & Attack \& defense & 2023 \\


   Baniecki \etal~\cite{baniecki2023adversarial} & AI (Image) & Attack \& defense & 2023 \\

    Han \etal~\cite{han2023interpreting} & Image & Attack & 2023 \\

    Bountakas \etal~\cite{bountakas2023defense} & Audio, cyber-security, NLP, \& computer vision & Defense & 2023 \\



    
    %\hline
    \bottomrule
    \end{tabular}

\end{center}
\end{table*}
% %--------------------Table4------------------------------------


% In this paper, we provide a comprehensive review of recent findings on adversarial attack and defense on point clouds for deep models, summarize the methods for generating and defending adversarial point clouds, and propose a taxonomy of these methods. Also, the current challenges adversarial point clouds face, and possible solutions, are discussed. 



Our key contributions are as follows:


\begin{itemize}
\item  A review of the different types of adversarial attacks on point clouds that have been proposed, including their methodologies and attributes, with specific examples from the literature. %and the methods that have been used to generate them, and proposing a taxonomy of these methods.
\item A review of the various methods that have been proposed for defending against adversarial attacks, organized into data-focused and model-focused methods, with examples from the literature. %including data optimization, input transformation methods, and deep model modification.
\item A summary of the most important datasets and models used by researchers in this field.
\item An overview %assessment 
of the challenges and limitations of the current approaches to adversarial attacks and defenses on 3D point clouds, and identification of opportunities for future research in this area.
\end{itemize}

%-------------------figure1-------------------------------------------
\begin{figure*}[t]
    \centering
    %\includegraphics[width=0.9\textwidth]{overall.png}
    \includegraphics[width=0.8\textwidth]{3DPC_adversarial.png}
    \caption{%An Overview of 
    Categorization of adversarial attack and defense approaches on 3D point clouds.}
    \label{fig_overview}
\end{figure*}
%-------------------figure1-------------------------------------------


An overview of the categorization of adversarial attack and defense approaches on 3D point clouds is shown in 
Fig.~\ref{fig_overview}. The rest of this paper is organized as follows. Section~\ref{sec:Backgrounds} introduces a list of notations, terms and measurements used in the paper. We discuss adversarial attacks on deep models for 3D point cloud classification %s raised in deep models 
in Section~\ref{sec:Adversarial attacks}. Section~\ref{sec:Defense} provides a detailed review of the existing adversarial defense methods. In Section~\ref{sec:Taxonomy}, we summarize commonly used datasets for point cloud classification and present an overview %a taxonomy 
of datasets and victim models used in %recent studies. 
the area of adversarial attacks and defenses on point clouds. We discuss current challenges and potential future directions in Section~\ref{sec:Challenges}. Finally, Section~\ref{sec:Conclusion} concludes the survey.

% ~\cite{mozaffari2014systematic}
%-----------------------Introduction---------------------------


\section{Background}
\label{sec:Backgrounds}
In this section, we provide the necessary background in terms of notation, terminology, and point cloud distance measures used in the field of 3D adversarial attacks. By establishing clear definitions, researchers can more accurately compare the effectiveness of different approaches and identify trends or patterns in the methods. 

A list of symbols used in the paper is given in Table~\ref{table:Symbols}, along with their explanations. These symbols are used to represent various quantities related to point cloud adversarial attacks. The table provides a brief description of each symbol to help readers understand and follow the discussions and %calculations presented 
equations in the paper. Next, we briefly introduce the terminology and distance measures used in the field of adversarial attacks and defenses on 3D point clouds. 



% Table~\ref{table:Symbols} is a list of notations and symbols used in the paper. These symbols are used to represent various quantities discussed in the paper. The table provides brief descriptions of the symbols used to help readers understand and follow the discussions and calculations presented in the paper.

% %--------------------Table1------------------------------------
\begin{table*}
\caption{
{Symbols and their explanations.}}
\centering
\label{table:Symbols}
%\begin{tabular*}{\textwidth}{c|c|c c c c c c}
\begin{tabular}{| c  | l |  }
\hline

\multicolumn{1}{|c|}{\bf Symbol}  & \multicolumn{1}{c|}{\bf Description}\\
 % \multicolumn{5}{c}{\bf Datasets }\\
\hline
% $x$ &  Original input image \\

% $x^{adv}$ &  Adversarial example (image) \\

$\mathcal{P}$ &  An instance of an original (input) point cloud \\

$\mathcal{P}^{adv}$ &  An instance of an adversarial point cloud \\


% $x_i$ & $i^{th}$ pixel in original input image \\

% $x^{adv}_i$ &  $i^{th}$ pixel in adversarial example (image) \\

$p_i$ &  $i$-th point in the original (input) point cloud \\

$p^{adv}_i$ &  $i$-th point in the adversarial point cloud \\

$\eta$ &  Perturbation vector (difference between the original and adversarial point cloud) \\

$\epsilon$ & Perturbation threshold %that $\eta$ should not exceed
\\

$\alpha$ &   Scale parameter %used to scale $\eta$ (learning rate 
%\textcolor{red}{(wasn't $\eta$ defined as perturbation?)}) \textcolor{blue}{no need for this sentence.  In the text, $\eta$ is always used for perturbation}%and $\sigma$ (standard deviation)
\\


$n$ &  Total number of points in a point cloud \\
% $m$ &  Total number of pixels in an image \\

 % $P$ to achieve $\ell_P$-norm
$Y$ & ground-truth label associated with original input \\
$Y^{\prime}$ &  Wrong label associated with an adversarial example that deep model predicts \\
$T$ & Target attack label\\

$f(\cdot)$ & Mapping from the input point cloud to the output label implemented by the deep model  \\

$\theta$ & Parameters of model $f$  \\

$J(\cdot,\cdot)$ & Loss function used for model $f$  \\ 

$\nabla$ & Gradient \\

$\sign(\cdot)$ & Sign function \\

$P$ &  Parameter of the $\ell_P$-norm; typical values of $P$ are $1,2$ and $\infty$. \\

%$\lambda$  & is a hyperparameter that determines the importance or weight given to the second term. It controls the trade-off between the two terms in the objective function.
% $p$ &  One point in original point cloud \\
$\lambda$ &  Controls the trade-off between the two terms in the objective function \\

% $p^{adv}$ &  One adversarial point in adversarial point cloud \\

$D_{\ell_P}$ & $\ell_P$-norm distance \\
$D_{H}$ & Hausdorff distance \\

$D_{C}$ & Chamfer distance \\

 $k$ & Number of nearest neighbors of a point\\

 $\kappa$ & Confidence constant %represents a constant that controls confidence 
 \\

$z$ & Latent space of a point autoencoder\\
$g(\cdot)$ & Penalty function \\
$S(\cdot)$ & Statistical Outlier Removal (SOR) defense \\
$t$ & Number of iterations %in PGD attack 
\\
$\mu$ & Mean of $k$ nearest neighbor distance of all points in a point cloud \\
$\sigma$ & Standard deviation of $k$ nearest neighbor distance of all points in a point cloud\\



\hline
\end{tabular}

\end{table*}
% %--------------------Table1------------------------------------


% This section reviews the notations used in the survey ,  3D dataset that exists and defenition of terms and and measurments 
% \subsection{Datasets}
% \label{sec:Datasets}

\subsection{Definition of terms}
\label{sec:Definition}

It is crucial to define the technical terms %typically 
used in the literature in order to provide %an obvious 
a consistent discussion of the various methods and approaches. The definitions of these terms appear below. The rest of the paper follows the same definitions %of these concepts 
throughout.
%In order to make reading easier, we have placed these definitions first.



\begin{itemize}

\item \textbf{3D point cloud} is a set of %irregular 
points in 3D space, typically representing a 3D shape or scene.
\item \textbf{Adversarial point cloud} is a 3D point cloud that has been intentionally modified in order to mislead a DL model that analyzes 3D point clouds. We focus on geometric modifications, rather than attribute (e.g., color) modifications since these are predominant in the literature on adversarial point clouds.
\item \textbf{Adversarial attack} is a technique that intentionally introduces perturbations or noise to an input point cloud in order to fool a DL model, causing it to make incorrect predictions or decisions.
\item \textbf{Black-box attacks} are a type of adversarial attack in which the attacker only has access to the model's input and output and has no knowledge of the structure of the DL model being attacked.
\item \textbf{White-box attacks} are a type of adversarial attack in which the attacker knows all the details about the DL model’s architecture and parameters.
\item \textbf{Gray-box attacks} cover the spectrum between the extremes of black- and white-box attacks. Here, the attacker knows partial details about the DL model’s architecture and parameters in addition to having access to its input and output.
\item \textbf{Targeted attacks} involve manipulating the input point cloud in a way that causes the model to output %classify the input to 
a specific target label when presented with the modified input.
\item \textbf{Non-targeted attacks} involve manipulating the input point cloud in a way that causes the model to %classify the input to a class, other than the original class.
output a wrong label, regardless of what that label is.
\item \textbf{Point addition attacks} involve adding %a few 
points to the point cloud %and increase the point numbers in the original point cloud 
to fool the DL model.
\item \textbf{Point shift attacks} involve shifting points of the point cloud to fool the DL model, while the number of points remains the same as in the original point cloud.
\item \textbf{Point drop attacks} involve dropping %a few 
points %and reduces the point numbers in 
from the %original 
point cloud to fool the DL model.
\item \textbf{Optimization-based attacks} are a type of attack in which %first, the initial estimate of adversarial perturbation is considered an optimization problem, then it is solved using some optimizers.
the creation of an adversarial point cloud is formulated and solved as an optimization problem.
\item \textbf{Gradient-based attacks} are a type of attack in which %first 
the gradients of the loss function corresponding to each input point are %acquired. They are then used to acquire an adversarial point cloud such that the proposed attack has a more 
used to generate an adversarial point cloud with a higher tendency toward being misclassified. 
\item \textbf{On-surface perturbation attacks} are a type of %3D adversarial 
attack that involves modifying points along the object's surface in the point cloud.
\item \textbf{Off-surface perturbation attacks} are a type of %3D adversarial 
attack that involves modifying points outside the object surface in the point cloud. %; such as noise and outliers.
\item \textbf{Transferability} refers to the ability of adversarial examples generated for one DL model to be successful in causing misclassification for another DL model. %\textcolor{red}{(Doesn't it mean transferring an attack generated for one DL model to another DL model?)} \textcolor{blue}{Yes, you are correct. Transferability refers to the ability of an adversarial example generated from one deep learning model to be effective in attacking another deep learning model.}
\item \textbf{Adversarial defense} is a set of techniques that aim to mitigate the impact of adversarial attacks and improve the robustness of the DL model against them.
\item \textbf{Attack success rate} refers to the percentage of times that an adversarial attack on a DL model is successful.
\end{itemize}


\subsection{Distance measures}
\label{sec:Measurement}

The objective of adversarial attacks is to modify the points of $\mathcal{P}$, creating an adversarial point cloud $\mathcal{P}^{adv}$, which could fool a DL model to produce wrong results. 
Geometric 3D adversarial attacks can be achieved by adding, dropping, or shifting points in $\mathcal{P}$. %the original point cloud. 
If the adversarial point cloud is generated by shifting points, $\boldsymbol{\ell_P}$\textbf{-norms} can be used to measure the distance between $\mathcal{P}$ and $\mathcal{P}^{adv}$, as the two %original and adversarial 
point clouds have the same %are identical in the 
number of points. In this case, we can talk about the vector difference (perturbation) $\eta = \mathcal{P}-\mathcal{P}^{adv}$, and consider $\|\eta\|_P$ as the distance between $\mathcal{P}$ and $\mathcal{P}^{adv}$. The typical choices for $P$ are $P \in \{0, 2, \infty\}$, and the equation is:
%Usually, these perturbations, the distance between the original input and the adversarial one, are measured by three typical criteria of $\ell_P$-norm named $\ell_0$-norm, $\ell_2$-norm, and $\ell_\infty$-norm. The formula for the $\ell_P$-norm is as follows
% The $\ell_P$-norm is used as a similarity criterion when the adversarial example points equal the original input points. In this type of attacks, An adversarial attack aims to find a small perturbation $\eta = \mathcal{P}^{adv} - X$ such that generates the adversarial input $X^{adv}$, which looks similar to $X$ to humans but will be misclassified by the victim model. Usually, these perturbations, the distance between the original input and the adversarial one, are measured by three typical criteria of $\ell_P$-norm named $\ell_0$-norm, $\ell_2$-norm, and $\ell_\infty$-norm. The formula for the $\ell_P$-norm is as follows
% An adversarial attack is a method to generate adversarial examples based on a given original input and the victim model. 
% Suppose $\mathcal{P}$ denotes the original input, and $Y$ denotes the label associated with $\mathcal{P}$ that the DNN model can correctly predict its label. An adversarial attack aims to find a small perturbation $\eta = \mathcal{P}^{adv} - X$ such that generates the adversarial input $X^{adv}$, which looks similar to $X$ to humans but will be misclassified by the victim model. Usually, these perturbations, the distance between the original input and the adversarial one, are measured by three typical criteria of $\ell_P$-norm named $\ell_0$-norm, $\ell_2$-norm, and $\ell_\infty$-norm. The formula for the $\ell_P$-norm is as follows
% \begin{equation}
% \|\eta\|_p = D_{\ell_P} (X , X^{adv}) = (\sum_{i=1} ^{m} |x_i - x^{adv}_i|^P)^{1/P}
% % L_P (x , x^{adv}) = (|X_1 - X^{adv}_1|^P + |X_2 - X^{adv}_2|^P + .. + |X_m - X^{adv}_m|^P)
% \label{eq:0}
% \end{equation}
% where $x_i$ and $x^{adv}_i$ are the $i^{th}$ pixel in original and adversarial image, and $m$ shows the number of pixels. A value of 0 or 2 or $\infty$ is usually set for $P$ to achieve $\ell_P$-norm with the following concepts
% where $m$ shows the number of pixels, $x_{i}\,|\, i=1,2, ..., m$ and $x^{adv}_{i}\,|\, i=1,2, ..., m$ refer to pixels in the original image and adversarial one, respectively. 
% \begin{itemize}
% \item \textbf{$\ell_0$-norm} or $ \|\eta\|_0 $ indicates how many pixels in the adversarial samples have changed from the original one.
% \item \textbf{$\ell_2$-norm} or $ \|\eta\|_2 $ is the Euclidean distance between all pixels in the adversarial and the original image (this will be small if only a few pixels change.)
% \item\textbf{$\ell_\infty$-norm} or $ \|\eta\|_\infty $ represents the maximum difference between all the pixels of the adversarial and the original image.
% \end{itemize}
% Typically, the $\ell_P$-norm is a widely used criterion for measuring adversarial perturbation of fixed-shape data. However, it is also used for unfixed-shape data like point clouds as follows
\begin{equation}
 D_{\ell_P}  (\mathcal{P} , \mathcal{P}^{adv}) = \|\eta\|_P =  \left(\sum_{i=1}^n\|p_i - p^{adv}_i\|_P^P\right)^{1/P}
% L_P (x , x^{adv}) = (|x_1 - x^{adv}_1|^P + |x_2 - x^{adv}_2|^P + .. + |x_m - x^{adv}_m|^P)
\label{eq:12}
\end{equation}
where $\mathcal{P} \in \mathbb{R}^{n\times3}$ is the original point cloud consisting of $n$ points in 3D space, $\mathcal{P}=\left\{p_{i}\,|\, i=1,2, ..., n\right\}$ and the $i^{th}$ point, $p_{i} = (x_i,y_i,z_i)$, is a 3D vector of coordinates. %represented by a 3-tuple ($x_p$, $y_p$, $z_P$) coordinate. %\textcolor{red}{(If $p_i$ is a 3D vector, then what does $|p_i - p^{adv}_i|$ mean? You can't take an absolute value of a vector. So either equation~(\ref{eq:12}) needs to be rewritten in terms of x, y, z coordinates of $p_i$, or it has to be rewritten in terms of norms of vector difference $p_i-p^{adv}_i$.)} \textcolor{blue}{Corrected.}
$\mathcal{P}^{adv}$ is the adversarial point cloud formed by adding the adversarial perturbation $\eta = (\eta_1,\eta_2, ..., \eta_n), \eta_i\in \mathbb{R}^3$, to $\mathcal{P}$. % $\eta$~=~$\left\{\eta_i \in \mathbb{R}^{n\times3}\,|\, i=1,2, ..., n\right\}$ to the original point cloud $\mathcal{P}$. 
%Similarly to the 2D image, a 3D point cloud also typically follows three common $\ell_P$-norm criteria ($\ell_0$-norm, $\ell_2$-norm, and $\ell_\infty$-norm). A value of 0 or 2 or $\infty$ is usually set for $P$ to achieve $\ell_P$-norm with the following concepts
The three common $\ell_P$ norms have the following interpretations:

\begin{itemize}
\item $\boldsymbol{\ell_0}$\textbf{-norm} or $ \|\eta\|_0 $ counts the number of non-zero elements in $\eta$, so it indicates how many points in $\mathcal{P}^{adv}$ have changed compared to $\mathcal{P}$.
\item $\boldsymbol{\ell_2}$\textbf{-norm} or $ \|\eta\|_2 $ is the Euclidean distance between $\mathcal{P}^{adv}$ and $\mathcal{P}$.
\item $\boldsymbol{\ell_\infty}$\textbf{-norm} or $ \|\eta\|_\infty $ is the maximum difference between the points in $\mathcal{P}^{adv}$ and $\mathcal{P}$.
\end{itemize}

% Typically, the $\ell_P$-norm is a widely used criterion for measuring adversarial perturbation of fixed-shape data. However, it is also used for unfixed-shape data like point clouds as follows



As mentioned above, $\ell_P$-norm distance criteria require that $\mathcal{P}^{adv}$ and $\mathcal{P}$ have the same number of points. Hence, these distance measures cannot be %applied to all types of 3D adversarial 
used for attacks
% 3D adversarial attacks can be achieved by adding, dropping, or shifting points in the original point cloud. If adversarial point clouds are generated by shifting some points, $\ell_P$-norm can be used for measuring adversarial perturbation, as two original and adversarial point clouds are identical in the number of points.
%A one-to-one correspondence between original and adversarial point clouds cannot be achieved in the case of 
that involve adding or dropping points. To quantify the dissimilarity between two point clouds that don't have the same number of points, \textbf{Hausdorff distance} $D_H$ and \textbf{Chamfer distance} $D_C$ are commonly used. %used because there is no requirement for one-to-one correspondence in these criteria. 
%\begin{itemize}
%\item \textbf{Hausdorff Distance (HD).} 
Hausdorff distance is defined as follows: %for an original point cloud $\mathcal{P}$ and its adversarial counterpart $\mathcal{P}^{adv}$
\begin{equation}
D_{H} (\mathcal{P} , \mathcal{P}^{adv}) = \max\limits_{p \in \mathcal{P}} \min\limits_{p^{adv} \in \mathcal{P}^{adv}} \|p - p^{adv}\|_2^2
\label{eq:2}
\end{equation}
It locates the nearest original point $p$ for each adversarial point $p^{adv}$ and then finds the maximum squared Euclidean distance between all such nearest point pairs. Chamfer distance is similar to Hausdorff distance, except that it %takes the average of all 
sums the distances among all pairs of closest points, instead of taking the maximum:
\begin{equation}
%D_{C} (\mathcal{P} , \mathcal{P}^{adv}) = \sum_{p^{adv} \epsilon \mathcal{P}^{adv}} \frac{1}{\|\mathcal{P}^{adv} \|_0} \min\limits_{p \epsilon \mathcal{P}} \|p - p^{adv}\|_2^2
\begin{split}
D_{C} (\mathcal{P} , \mathcal{P}^{adv}) =& \sum_{p^{adv} \in \mathcal{P}^{adv}} \min\limits_{p \in \mathcal{P}}\|p - p^{adv}\|_2^2 \\
& + \sum_{p \in \mathcal{P}} \min\limits_{p^{adv} \in \mathcal{P}^{adv}}\|p - p^{adv}\|_2^2    
\end{split}
\label{eq:3}
\end{equation}
Optionally, Chamfer distance can be averaged with respect to the number of points in the two point clouds. 

In addition to the distance measures mentioned above, there are other distance measures for point clouds, such as the point-to-plane distance~\cite{PC_distortion_ICIP2017}, which are used in point cloud compression. However, these are not commonly encountered in the literature on 3D adversarial attacks, so we do not review them here. 


\section{Adversarial attacks}
\label{sec:Adversarial attacks}


Various techniques have been proposed to generate adversarial attacks on 3D point cloud models. This section presents a classification of these attacks based on several criteria, illustrated in Fig.~\ref{fig_overview}. While different classifications are possible, ours is based on attack methodologies, such as gradient-based point cloud modification, etc., and attack attributes such as attack location (on-/off-surface), adversarial knowledge (white-box, gray-box, or black-box) and target type (targeted or non-targeted). %generation strategies (optimization-based, gradient-based, or generative-based attacks), perturbation location (On-surface perturbation and Off-surface perturbation attacks) and perturbation Techniques (shift, add, drop and transform attacks). Perturbation location and perturbation Techniques which involve where and how the perturbations occur, are specifically appropriate for attacks in the context of attacks on 3D point clouds. 
In the following, we first present various methodologies, each with specific examples of the attack methods from that category. The discussion of various attack attributes is provided later in the section.
 % This section describes the seven most common approaches for generating adversarial point clouds. Our discussion covers the technicalities of these seven widely used methods and also briefly touches upon similar approaches related to these seven attacks. Some of the approaches~\cite{yang2019adversarial,xiang2019generating} described in this section are extended versions of adversarial examples for 2D data, adapted for use with 3D point clouds. When extended from 2D to 3D, these approaches may face new challenges due to the additional dimension of the data. Other approaches~\cite{naderi2022model} are specifically designed for 3D data and may be more effective at generating adversarial point clouds than methods that are simply adapted from 2D data. These ``made-for-3D'' approaches may consider the unique characteristics of 3D point clouds and the deep models that process them. 
% Overall, the goal of these approaches is to better understand how adversarial point clouds could affect current deep 3D models. 
The most popular attack approaches are also summarized in Table~\ref{table:Categories} for quick reference. %and we explain how adversarial attacks and attack categories relate in the context of adversarial examples for point-cloud classification tasks.

%------------------% % %--------------------Table4------------------------------------

% For tables use
\begin{table*}
\centering
% table caption is above the table
\caption{Popular adversarial attacks.}
\label{table:Categories}       % Give a unique label
% For LaTeX tables use
\begin{tabular}{c c|ccccccccccc}
%hline
\toprule
&  & \bf Targeted / & \bf Shift / Add / & \bf On- / & \bf Optimized / & \bf Black- / Gray- /\\ 

\bf Reference &  \bf Attack Name & \bf Non-targeted &  \bf Drop / Transform & \bf Off-surface &  \bf Gradient & \bf White-box \\
   
% \multirow{3}{*}{\cite{xiang2019generating}} & - & - & - & - & - & -\\ & - & - & - & - & - & - \\  & - & - & - & - & - & - \\
% \hline

% \multirow{1}{*}{\cite{tsai2020robust}} & - & - & - & - & - & -\\

%\hline
\midrule

\multirow{4}{*}{Xiang \etal~\cite{xiang2019generating}} & Perturbation  & Targeted & Shift & Off & Optimized & White\\& Independent points &Targeted & Add & Off & Optimized & White \\ & Clusters & Targeted & Add & Off &Optimized & White  \\ & Objects & Targeted & Add & Off &Optimized & White  \\
\hline
\multirow{2}{*}{Zheng \etal~\cite{zheng2019pointcloud}} & Drop100  & Non-Targeted & Drop & On& Gradient & White\\  & Drop200 & Non-Targeted & Drop & On& Gradient & White\\
\hline
\multirow{1}{*}{Hamdi \etal~\cite{hamdi2020advpc}} & Advpc & Targeted & Transform & On & Optimized & White\\
\hline
\multirow{1}{*}{Lee \etal~\cite{lee2020shapeadv}} & ShapeAdv & Targeted & Shift & On & Optimized & White\\
\hline
\multirow{1}{*}{Zhou \etal~\cite{zhou2020lg}} & LG-GAN & Targeted & Transform & On & - & White\\
\hline
\multirow{1}{*}{Wen \etal~\cite{wen2020geometry}} & GeoA$^3$ & Targeted & Shift & On & Optimized & White\\
\hline
\multirow{1}{*}{Tsai \etal~\cite{tsai2020robust}} & KNN & Targeted & Shift & On & Optimized & White\\
\hline
\multirow{1}{*}{Liu \etal~\cite{liu2019extending}} & Extended FGSM & Non-Targeted & Shift & Off & Gradient & White\\
\hline
\multirow{1}{*}{Arya \etal~\cite{arya2021adversarial}} & VSA & Non-Targeted & Add & On & Optimized & White\\
\hline
\multirow{4}{*}{Liu \etal~\cite{liu2020adversarial}} & Distributional attack & Non-Targeted & Shift & On & Gradient & White\\ & Perturbation resampling & Non-Targeted & Add & Off & Gradient & White\\ & Adversarial sticks & Non-Targeted & Add & Off & Gradient & White\\ & Adversarial sinks & Non-Targeted & Add & Off & Gradient & White\\
\hline
\multirow{2}{*}{Kim \etal~\cite{kim2021minimal}} & Minimal & Non-Targeted & Shift & Off & Optimized & White\\ & Minimal & Non-Targeted & Add & Off & Optimized & White\\
\hline
\multirow{1}{*}{Ma \etal~\cite{ma2020efficient}} & JGBA & Targeted & Shift & On & Optimized & White\\
\hline
\multirow{1}{*}{Liu \etal~\cite{liu2022imperceptible}} & ITA & Targeted & Shift & On & Optimized & Black\\
\hline
\multirow{4}{*}{Liu \etal~\cite{yang2019adversarial}} & FGSM & Non-Targeted & Shift & Off & Gradient & White\\& MPG  & Non-Targeted & Shift & Off & Gradient & White\\ & Point-attachment & Non-Targeted & Add & Off & Gradient & White\\& Point-detachment & Non-Targeted & Drop & On & Gradient & White\\
\hline
\multirow{1}{*}{Wicker \etal~\cite{wicker2019robustness}} & --- & Both & Drop & On & Optimized & Both\\

\hline
\multirow{1}{*}{He \etal~\cite{he2023generating}} & --- & Non-Targeted & Shift & On & Optimized & White\\



%\hline
\bottomrule

\end{tabular}
\end{table*}
%--------------% % %--------------------Table4------------------------------------


%%%%%%%------------------------------
\subsection{Point cloud modification strategies}
\label{subsubsec: generation Strategies}
%%%%%%%------------------------------


\subsubsection{Gradient-based strategies}
\label{subsubsec:Gradient-based}

DNNs are typically trained using the gradient descent method to minimize a specified loss function. Attackers targeting such models often take advantage of the fact that they can achieve their goals by maximizing this loss function along the gradient ascent direction. Specifically, attackers can create adversarial perturbations utilizing the gradient information of the model and iteratively adjusting the input to maximize the loss function. 
Fast Gradient Sign Method (FGSM) and Projected Gradient Descent (PGD) are the most commonly used gradient-based techniques for this purpose. We review each of them below.

\paragraph{3D Fast Gradient Sign Method (3D FGSM)}
\label{subsubsubsec:FGSM}

The inception of adversarial attacks on 3D data occurred in 2019 using gradient-based techniques. During this period, Liu \etal~\cite{liu2019extending} and Yang \etal~\cite{yang2019adversarial} extended the Fast Gradient Sign Method (FGSM), originally proposed by Goodfellow \etal~\cite{goodfellow2015explaining}, to 3D data. The 3D version of FGSM adds an adversarial perturbation $\eta$ to each point in the given point cloud $\mathcal{P}$ to create an adversarial point cloud  $\mathcal{P}^{adv} = \mathcal{P}+\eta$. Perturbations are generated according to the direction of the sign of the gradient at each point. %which is usually constrained to be small via the maximum norm ($\ell_{\infty}$).
The perturbation can be expressed as
\begin{equation}
\eta = \epsilon \cdot \sign\left[\nabla_\mathcal{P}J(f(\mathcal{P};\theta),Y)\right]
\label{eq:11}
\end{equation}
where $f$ is the deep model that is parameterized by $\theta$ and takes an input point cloud $\mathcal{P}$, and $Y$ denotes the label associated with $\mathcal{P}$. $J$ is the loss function,  $\nabla_\mathcal{P}J$ is its gradient with respect to $\mathcal{P}$ and $\sign(\cdot)$ denotes the sign function. The $\epsilon$ value is an adjustable hyperparameter that determines the $\ell_P$-norm of the difference between the original and adversarial inputs.% The perturbation is limited by $\epsilon$.

% Using linearization of the cost function $J$, the FGSM attack finds the perturbation that maximizes the cost function under the $\ell_\infty$-norm constraint. The attack requires no iterative procedure to compute adversarial examples and is, therefore, considerably faster than other approaches.



% \begin{equation}
% \begin{split}
% \max\limits_\eta  \;\;\;    J(f(\mathcal{P}:\theta),Y)  \\
% s.t.  \;\;\;\;\;\; ||\eta||_{\infty} \leq \epsilon
%  \end{split}
% \label{eq:40}
% \end{equation}



Liu \etal~\cite{liu2019extending} introduced three different ways  to define $\epsilon$ as a constraint for $\eta$ as follows
% the adversarial perturbations on points after adding to point cloud $\mathcal{P}$ create an adversarial point cloud $\mathcal{P}^{adv}$.

% \begin{enumerate}
%     \item $\ell_{\infty}$ norm of perturbation for each coordinate of each point should be smaller than $\epsilon$. ($||\eta^{x-y-z}||_{\infty} \le \epsilon$) where $x-y-z$ denotes $x$ or $y$ or $z$.
%     \item $\ell_2$-norm of the perturbation for each point should be smaller than $\epsilon$. ($||\eta||_{2} \le \epsilon$)
%     \item $\ell_2$-norm of the perturbation for the entire point cloud should be smaller than $\epsilon$. ($||\eta||_{2} \le \epsilon$)
% \end{enumerate}

\begin{enumerate}
    \item Constraining the $\ell_2$-norm between each dimension of points in $\mathcal{P}$ and $\mathcal{P}^{adv}$.
    \item Constraining the $\ell_2$-norm between each point in $\mathcal{P}$ and its perturbed version in $\mathcal{P}^{adv}$.
    \item Constraining the $\ell_2$-norm between %all points 
    the entire $\mathcal{P}$ and $\mathcal{P}^{adv}$.
\end{enumerate}
Due to the fact that the first method severely limits the movement of points, the authors suggest the second and third methods. However, all three methods have shown little difference in attack success rates.

Yang \etal~\cite{yang2019adversarial}  used the Chamfer distance (instead of the $\ell_2$-norm) between the original point cloud and the adversarial counterpart to extend the FGSM to 3D. There is a trade-off between the Chamfer distance and the attack success rate because, as the Chamfer distance decreases, it may become more difficult for an adversarial attack to achieve a high attack success rate. However, if the Chamfer distance is set too high, the model may be more vulnerable to adversarial attacks. Finding the right balance between these two factors can be challenging, and it may depend on the specific characteristics of the point cloud model and the type of adversarial attack being used. Figure~\ref{fig_FGSM} illustrates an example of an FGSM adversarial point cloud with Chamfer distances varying from 0.01 to 0.05 between the two point clouds. The authors in~\cite{yang2019adversarial} set it to 0.02. %as an "appreciate distance". 

%-------------------figure1-------------------------------------------
\begin{figure*}
    \centering
    \includegraphics[width=\textwidth]{FGSM.png}
    \caption{An example of original point cloud and 3D FGSM adversarial counterpart~\cite{yang2019adversarial} with Chamfer distances $D_c$ varying from 0.01 to 0.05. (Image source:~\cite{yang2019adversarial}; use permitted under the Creative Commons Attribution License CC BY 4.0.)}
    \label{fig_FGSM}
\end{figure*}
%-------------------figure1-------------------------------------------
Apart from the FGSM attack, Yang \etal~\cite{yang2019adversarial} introduced another attack called Momentum-Enhanced Pointwise Gradient (\textbf{MPG}). The (3D) MPG attack, similar to its 2D version~\cite{dong2018boosting}, integrates momentum into the iterative FGSM. The MPG attack produces more transferable adversarial examples because the integration of momentum into the iterative FGSM process enhances its ability to escape local minima and generate effective perturbations.



%--------------------------------------
\paragraph{3D Projected Gradient Decent (3D PGD)}
\label{subsubsubsec:PGD}

One of the most potent attacks on 3D data is the Projected Gradient Descent (PGD), whose foundation is the pioneering work by Madry \etal~\cite{madry2019deep}. The iterative FGSM is considered a basis for PGD.
% One of the most potent attacks on 2D data is the Projected Gradient Descent (PGD), which has its roots in the pioneering paper of Madry \etal~\cite{madry2019deep}. The iterative FGSM is considered a basis for PGD.
Taking the iterative FGSM method, we can generate the adversarial point cloud as
% The attack apply the same step as FGSM multiple times with a small step size $\alpha$ and clip the pixel values of intermediate results after each step to ensure that they are in an $\epsilon$-neighbourhood of the original input. The adversarial example can be demonstrated as
\begin{equation}
\begin{split}
    &\mathcal{P}^{adv}_{0} = \mathcal{P}, \\ &\mathcal{P}^{adv}_{t+1} = \text{clip}_{\mathcal{P},\epsilon}\left[\mathcal{P}^{adv}_t+\alpha \cdot \sign(\nabla_\mathcal{P}J(f(\mathcal{P};\theta),Y)\right],
\end{split}
\label{eq:10}
\end{equation}
% \begin{equation}
% \begin{split}
% \min\limits_\eta  \;\;\;    D (\mathcal{P} , \mathcal{P}^{adv}) + c . g(\mathcal{P} + \eta)  \\
% s.t.  \;\;\;\;\;\; f(\mathcal{P}^{adv})=T
%  \end{split}
% \label{eq:30}
% \end{equation}
where $t$ is the iteration number and $\text{clip}_{\mathcal{P},\epsilon}[\cdot]$ limits the change of the generated adversarial input to be within $\epsilon$ distance of $\mathcal{P}$, according to a chosen distance measure. %\textcolor{red}{(to be within $\epsilon$ distance of $\mathcal{P}$? \textcolor{blue}{Thanks. I've fixed it.} According to which distance measure? \textcolor{blue}{Any distance measure can be used} Also, what is $x$? Is $x=\mathcal{P}$? \textcolor{blue}{That's right. I've fixed it.)}}

% In each iteration, they clipped pixel values to avoid large change on each pixel:

% In every iteration, the PGD attack projects gradients to the $\epsilon$-ball around the original input. In fact, a PGD attack considers the gradient of the loss w.r.t. to the input as a perturbation for input and gradually increases its magnitude until the input is misclassified.

% This solution is motivated by linearizing the cost function and solving for the perturbation that maximizes the cost subject to an L∞ constraint.

% Using linearization of the cost function $J$, the FGSM attack finds the perturbation that maximizes the cost function under the $\ell_\infty$-norm constraint. The attack requires no iterative procedure to compute adversarial examples and is, therefore, considerably faster than other approaches.

 % They suggest applying the same step as FGSM multiple times with a small step size and clip the pixel values of intermediate results after each step to ensure that they are in an ε-neighbourhood of the original image.

%Both the methods described till now have focused on simply trying to increase the cost of the correct class, without specifying which of the incorrect classes the model should select. A technique for this has been shown in the paper. Since this blog focuses on a basic overview of gradient based adversarial attacks, we are skipping explicit details from the paper. Kindly refer to the paper for knowledge about the same.

The PGD attack is based on increasing the cost of the correct class $Y$, without specifying which of the incorrect classes the model should select. To do this, the PGD attack finds the perturbation $\eta$ that maximizes the loss function under the perturbation constraint controlled by $\epsilon$. The optimization problem can be formulated as:


\begin{equation}
\begin{split}
&\max\limits_\eta  \;\;\;    J(f(\mathcal{P}+\eta;\theta),Y)  \\
&\text{such that} \;\;\; D (\mathcal{P} , \mathcal{P}+\eta) \leq \epsilon
 \end{split}
\label{eq:50}
\end{equation}
where $J$ is the loss function and $\epsilon$ controls how far the adversarial point cloud can be from the original one according to the chosen distance measure $D$. %It is a hypersphere centered at $\mathcal{P}$ with a radius of $\epsilon$. The perturbation $\eta$ is constrained to lie within this hypersphere, ensuring that the perturbed data $\mathcal{P}+\eta$ remains close to the original point cloud $\mathcal{P}$ and does not deviate too far.

%The 3D PGD attack is similar to the 2D version but uses distance measures relevant to 3D point clouds. In particular, 


Liu \etal~\cite{liu2020adversarial} proposed the following four flavors of the PGD attack.%, which are also illustrated in Figure~\ref{fig_Daniel}.

\begin{enumerate}
\item \textbf{Perturbation resampling} This attack resamples a certain number of points with the lowest gradients by farthest point sampling to ensure that all points are distributed approximately uniformly. The algorithm is iterated to generate an adversarial point cloud that deceives the model. Hausdorff distance is used to maintain the similarity between $\mathcal{P}$ and $\mathcal{P}^{adv}$.
% by using the Hausdorff distance between the $\mathcal{P}$ and $\mathcal{P}^{adv}$. 
% The goal of this attack is to make the density of added adversarial points as uniform as possible in $\mathcal{P}$.

\item \textbf{Adding adversarial sticks} During this attack, the algorithm adds four sticks to the point cloud, such that one end is attached to the point cloud while the other end is a small distance away. The algorithm optimizes the two ends of the sticks so that the label of the point cloud is changed. %Finally, it adds a few points between the two ends to make them look like sticks.

\item \textbf{Adding adversarial sinks} In this case, critical points (the points remaining after max pooling in PointNet) are selected as ''sink'' points, which pull the other points towards them until the point cloud label is changed. The goal is to minimize global changes to non-critical points. The distance measure used to maintain the similarity between $\mathcal{P}$ and $\mathcal{P}^{adv}$ is $\ell_2$-norm.

\item \textbf{Distributional attack} This attack uses the Hausdorff distance between the adversarial point cloud and the triangular mesh fitted over the original point cloud, to push adversarial points towards the triangular mesh. This method is less sensitive to the density of points in $\mathcal{P}$ because it uses a mesh instead of the point cloud itself to measure the perturbation. 
Figure~\ref{fig_PGD} shows two examples of adversarial point clouds generated by the distributional attack.
\end{enumerate}
  % $\mathcal{L}$


% Liu \etal~\cite{liu2020adversarial} proposed a PGD attack named \textbf{Distributional attack} that uses the Hausdorff distance between the adversarial point cloud and the triangular mesh fitted over the original point cloud, to push adversarial points towards the triangular mesh. This method is less sensitive to the density of points in $\mathcal{P}$ because it uses a mesh instead of the point cloud itself to measure the perturbation. %When the distance between adversarial points to the nearest triangular mesh is more than a certain threshold, the adversarial point projects on the triangular mesh. 


 
%-------------------figure3-------------------------------------------
\begin{figure}
    \centering
    \includegraphics[width=0.5\textwidth]{pgd.png}
    \caption{Two examples of the original point clouds (left) and adversarial point clouds generated by the distributional attack (right).~\cite{liu2020adversarial} (Image source:~\cite{liu2020adversarial}; use permitted under the Creative Commons Attribution License CC BY 4.0.).}
    \label{fig_PGD}
\end{figure}
%-------------------figure3-------------------------------------------

% %-------------------figure4-------------------------------------------
% \begin{figure*}
%     \centering
%     \includegraphics[width=0.9\textwidth]{Daniel.png}
%     \caption{Left to right: original point cloud and the adversarial examples produced by the shape attacks proposed in~\cite{liu2020adversarial} (Image source:~\cite{liu2020adversarial}; use permitted under the Creative Commons Attribution License CC BY 4.0).} %\textcolor{red}{(The text in this figure is blurry. Is this figure copy-pasted from another paper?)}\textcolor{blue}{The quality was low. fixed}
%     \label{fig_Daniel}
% \end{figure*}
% %-------------------figure4-------------------------------------------

Ma \etal~\cite{ma2020efficient} proposed the Joint Gradient Based Attack (\textbf{JGBA}). They added an extra term to the objective function of the PGD attack~(\ref{eq:50}) to defeat statistical outlier removal (SOR), a common defense against attacks. 
%The term finds the perturbation $\eta$ that maximizes the loss function of model w.r.t to points in $\mathcal{P}$ after removing outliers when the first term (term in ~\ref{eq:50}) finds the perturbation $\eta$ that maximizes the loss function of model w.r.t to all points in $\mathcal{P}$. These two terms are combined to solve the optimization problem. The JGBA attack takes $\ell_2$-norm as the distance measure to constraint shifting of points. \textcolor{red}{((Let's write out the optimization problem of JGBA explicitly.))} \textcolor{blue}{Added.}
Specifically, their optimization problem is:
\begin{equation}
\begin{split}
&\max\limits_\eta  \;\;\;    J(f(\mathcal{P}+\eta;\theta),Y) + \lambda \cdot J(f(S( \mathcal{P}+\eta);\theta),Y)   \\
&\text{such that} \;\;\; D_{\ell_2} (\mathcal{P} , \mathcal{P}+\eta) \leq \epsilon
 \end{split}
\label{eq:5000}
\end{equation}where $S(\cdot)$ denotes SOR and $\lambda$ is a hyperparameter that controls the trade-off between the two terms in the objective function. This way, the adversarial point cloud becomes more resistant against the SOR defense.

Kim~\etal~\cite{kim2021minimal} proposed a so-called \textbf{minimal attack} that aims to manipulate a minimal number of points in a point cloud. This can be thought of as minimizing $D_{\ell_0}  (\mathcal{P} , \mathcal{P}^{adv})$. To find an adversarial point cloud, Kim~\etal~ modify the loss function of the PGD attack~(\ref{eq:30}) by adding a term that tries to keep the number of changed points to a minimum. Furthermore, they used Hausdorff and Chamfer distances to preserve the similarity between $\mathcal{P}$ and $\mathcal{P}^{adv}$. Figure~\ref{fig_min} illustrates examples of minimal adversarial attack, where the altered points are indicated in red.

%-------------------figure6-------------------------------------------
\begin{figure}
    \centering
    \includegraphics[width=0.4\textwidth]{min.png}
    \caption{Two examples of the original point cloud and the corresponding minimal adversarial attack, where the altered points are shown in red~\cite{kim2021minimal} (© 2021 IEEE. Reprinted, with permission, from~\cite{kim2021minimal}).}
    \label{fig_min}
\end{figure}
%-------------------figure6-------------------------------------------

%------------------------------


\subsubsection{Optimization-based strategies}
\label{subsubsec:optimization-based}

While gradient-based strategies rely on model gradients, optimization-based methods instead utilize model output logits to create attacks. These methods usually aim to keep perturbations minimal, to reduce the chance of detecting the attack, while deceiving the model into making a wrong decision. Hence, these are often formulated as constrained optimization problems or multi-objective problems. %These concepts are employed in the design of optimization models. These models are designed with the specific aim of minimizing the loss linked to these dual objectives. 
The Carlini and Wagner (C\&W) attack is founded on these ideas, as explained below.

\paragraph{3D Carlini and Wagner attack (3D C\&W)}
\label{subsubsubsec:C&W}

%The C\&W attack was developed by Carlini and Wagner~\cite{carlini2017towards} for 2D data.
The 3D version the C\&W attack was developed by Xiang \etal~\cite{xiang2019generating} as an extension of the original work by Carlini and Wagner~\cite{carlini2017towards}. The method can be described as an optimization problem of finding the minimum perturbation $\eta$ such that the output of the deep model to the adversarial input $\mathcal{P}^{adv}=\mathcal{P} + \eta$ is changed to the target label $T$. The problem can be formulated as 
\begin{equation}
%\begin{split}
\min\limits_\eta  \;\;\;    D (\mathcal{P} , \mathcal{P} + \eta) + c \cdot g(\mathcal{P} + \eta)  %\\
%&\text{such that}  \;\;\; f(\mathcal{P} + \eta)=T
%\end{split}
\label{eq:30}
\end{equation}
where $D(\cdot,\cdot)$ is the distance measure, %(it can be defined using different distance measures like $\ell_P$-norm, Chamfer or Hausdorff distance), 
%$f(\cdot)$ represents the deep model, 
$c$ is a Lagrange multiplier and $g(\cdot)$ is a penalty function such that $g(\mathcal{P}^{adv})\leq$ 0 if and only if the output of the deep model is $f(\mathcal{P}^{adv})=T$. %By doing so, the distance and penalty term can be optimized more effectively. 
%In addition, they set another condition that the pixels must be within the allowed range (between 0,1) after perturbation.
Seven choices for $g$ were listed in~\cite{carlini2017towards}. One of the functions evaluated in their experiments, which became popular in the subsequent literature, is as follows:
\begin{equation}
    g(\mathcal{P}^{adv}) = \max\left[\max\limits_{i=t}(Z(\mathcal{P}^{adv})_i)-Z(\mathcal{P}^{adv})_t , -\kappa\right]
\label{eq:4}
\end{equation}
where $Z$ denotes the Softmax function, and $\kappa$ represents a constant that controls confidence.
Compared to the FGSM attack, the C\&W attack does not constrain the perturbation; instead, it searches for the minimal perturbation that would produce the target label.

%-------------------figure2-------------------------------------------
\begin{figure*}
    \centering
    \includegraphics[width=1\textwidth]{cw.png}
    \caption{Left to right: original point cloud and the adversarial examples produced by the attacks proposed in~\cite{xiang2019generating} (© 2019 IEEE. Reprinted, with permission, from~\cite{xiang2019generating}).}
    \label{fig_CW}
\end{figure*}
%-------------------figure2-------------------------------------------

% A 3D version of the C\&W attack was developed by Xiang \etal~\cite{xiang2019generating}, where four types of attacks were proposed: 

Xiang \etal~\cite{xiang2019generating} developed four versions of the 3D C\&W attack, featuring various %attacks based on different 
distance measures:

\begin{enumerate}

    \item \textbf{Adversarial perturbation} to shift the points toward the point cloud's surface, using the $\ell_2$-norm between all points of $\mathcal{P}$ and $\mathcal{P}^{adv}$ as the distance measure.
    
    \item \textbf{Adding adversarial independent points} by using two distance measures -- Hausdorff distance and Chamfer distance -- between $\mathcal{P}$ and $\mathcal{P}^{adv}$, to push independent points toward the point cloud's surface.

      
    \item \textbf{Adding adversarial clusters} based on three principles. %by the combination of three different distance measures. 
    (1) Chamfer distance between the original point cloud and the adversarial cluster is used to push clusters toward the point cloud's surface.
    (2) Only a small number of clusters is added, specifically one to three. %The number of clusters added. Using this measure, only 1 to 3 clusters are added, so there is only a small number of clusters added.
    (3) %Minimize the farthest distance. In this measure, 
    The distance between the two most distant points in each cluster is minimized to constrain the added points clustered to be within small regions.
    
    \item \textbf{Adding adversarial objects} based on three principles.
    (1) Chamfer distance between the original point cloud and the adversarial object is used to push adversarial objects toward the point cloud's surface.
    (2) Only a small number of objects is added, specifically one to three. %The number of objects added. Using this measure, only 1 to 3 objects are added, so there is only a small number of objects added.
    (3) The $\ell_2$-norm between a real-world object and an adversarial object is used to generate shapes similar to those in the real world.
\end{enumerate}


% Tsai \etal~\cite{tsai2020robust} developed a shifting point attack called K-Nearest Neighbor (\textbf{KNN}) attack that limits distances between adjacent points by adding another loss term to~(\ref{eq:30}). This additional loss term is based on the K-Nearest Neighbor distance for each point, while their main distance term in~(\ref{eq:30}) is the Chamfer distance. 
Wen \etal~\cite{wen2020geometry} considered a new distance measure named \textit{consistency of local curvatures} to guide perturbed points towards object surfaces. Adopting the C\&W attack framework, the authors use a combination of the Chamfer distance, Hausdorff distance, and local curvature consistency as the distance measure to create a geometry-aware adversarial attack (\textbf{GeoA$^\mathbf{3}$}). The  GeoA$^3$ attack enforces the smoothness of the adversarial point cloud to make the difference between it and the original point cloud imperceptible to the human eye.
Finally, Zhang~\etal~\cite{zhang20233d} introduced a \textbf{Mesh Attack} designed to perturb 3D object meshes while minimizing perceptible changes. The Mesh Attack employs two key components in its loss function: a C\&W loss, encouraging misclassification of adversarial point clouds, and a set of mesh losses, including Chamfer, Laplacian, and Edge Length losses, to maintain the smoothness and geometric fidelity of the adversarial meshes relative to the original input mesh.

%%----------------

\subsubsection{Transform attacks}
\label{subsubsec:Transform attacks}

Transform attacks are crafted in the transform domain rather than the input domain. Usually, the 3D point cloud is transformed into another domain (e.g., 3D frequency domain), then modified, and then transferred back to the original input domain to be fed to the classifier. Liu~\etal~\cite{liu2022boosting} have suggested an adversarial attack based on the frequency domain, which aims to improve the transferability of generated adversarial examples to other classifiers.  They transformed the point cloud into the frequency domain using the graph Fourier transform (GFT)~\cite{GSP2013SPM}, then divided it into low- and high-frequency components, and applied perturbations to the low-frequency components to create an adversarial point cloud. 
Liu \etal~\cite{liu2022point} investigated the geometric structure of point clouds by perturbing, in turn, low-, mid-, and high-frequency components. They found that perturbing low-frequency components significantly changed their shape. To preserve the shape, they created an adversarial point cloud with constraints on the low-frequency perturbations and instead guided perturbations to the high-frequency components. Hu \etal~\cite{hu2022exploring} suggest that by analyzing the eigenvalues and eigenvectors of the graph Laplacian matrix~\cite{GSP2013SPM} of a point cloud, one can determine which areas of the cloud are particularly sensitive to perturbations. By focusing on these areas, the attack can be crafted more effectively.

A related attack, though not exactly in the frequency domain, was proposed by Huang \etal~\cite{huang2022shape}. This attack is based on applying reversible coordinate transformations to points in the original point cloud, which reduces one degree of freedom and limits their movement to the tangent plane. The best direction is calculated based on the gradients of the transformed point clouds. After that, all points are assigned a score to construct the sensitivity map. Finally, top-scoring points are %selected to fool deep models.
moved to generate the adversarial point cloud.

In another attack called Variable Step-size Attack (\textbf{VSA})~\cite{arya2021adversarial}, a hard constraint on the number of modified points is incorporated into the optimization function of a PGD attack~(\ref{eq:30}) to try to preserve the point cloud's appearance. Specifically, points with the highest gradient norms, which are thought to have the greatest impact on classification, are selected initially. The selected points are then subject to adversarial perturbations. The goal is to shift these points in a way that maintains their original appearance while maximizing the loss function, thus causing the model to misclassify the input. By controlling the step size, VSA adjusts the magnitude of perturbations applied to the selected points. It starts with a larger step size to allow for rapid exploration of the optimization landscape. As the process advances, the step size is progressively reduced to guide the optimization toward more precise modifications. 


\subsubsection{Point shift attacks}
\label{subsubsec:Point shift attacks}

Point shift attacks involve shifting the points of the original 3D point cloud to fool the deep model, while the number of points remains the same. Tsai \etal~\cite{tsai2020robust} developed a shifting point attack called K-Nearest Neighbor (\textbf{KNN}) attack that limits distances between adjacent points by adding another loss term to~(\ref{eq:30}). This additional loss term is based on the K-Nearest Neighbor distance for each point, while their main distance term in~(\ref{eq:30}) is the Chamfer distance. 
Miao \etal~\cite{zhao2020isometry} proposed an adversarial point cloud based on rotation by applying an isometry matrix to the original point cloud. To find an appropriate isometry matrix, the authors used the Thompson Sampling method \cite{russo2018tutorial}, which can quickly find a suitable isometry matrix with a high attack rate. %During this attack, the number of points doesn't change.} 

Liu~\etal~\cite{liu2022imperceptible} proposed an Imperceptible Transfer Attack (\textbf{ITA}) that enhances the imperceptibility of adversarial point clouds by shifting each point in the direction of its normal vector. Although some of the attacks described earlier also involve shifting points, the main difference here is that ITA aims to make the attack imperceptible, whereas earlier attacks may cause noticeable changes to the shape. Along the same lines, Tang~\etal~\cite{tang2022normalattack} presented a method called \textbf{NormalAttack} for generating imperceptible point cloud attacks. Their method deforms objects along their normals by considering the object's curvature to make the modification less noticeable.

Zhao \etal~\cite{zhao2020nudge} proposed a class of point cloud perturbation attacks called Nudge attacks that try to minimize point perturbation while changing the classifier's decision. They generated adversarial point clouds using gradient-based and genetic algorithms with perturbations of up to 150 points to deceive the classifier. In some cases, the attack can fool the classifier by changing a single point when the point has a large distance from the surface of the objects.
Analogously to the one-pixel attack for images~\cite{su2019one}, Tan \etal~\cite{tan2023explainability} proposed an attack called \textbf{One point attack} in which only a single point in the point cloud needs to be shifted in order to fool the deep model. The authors also present a method to identify the most important points in the point cloud based on a saliency map, which could be used as candidates for the attack.


\subsubsection{Point add attacks}
\label{subsubsec:Point add attacks}

Point add attacks involve the addition of points to the point cloud with the aim of misleading deep models, while remaining plausible. Obviously, the number of points in the point cloud increases after this attack.
Yang~\etal~\cite{yang2019adversarial} provided a point-attachment attack by attaching a few points to the point cloud. Chamfer distance is used to keep the distance between the newly added points and the original point cloud small. Hard constraints limit the number of points added in the point cloud, making the adversarial point cloud preserve the appearance of the original one. 

Shape Prior Guided Attack~\cite{Shi2022Shape} is a method that adds points by using a shape prior, or prior knowledge of the structure of the object, to guide the generation of the perturbations. This method introduces Spatial Feature Aggregation (SPGA), which divides a point cloud into sub-groups and introduces structure sparsity to generate adversarial point sets. It employs a distortion function comprising Target Loss and Logical Structure Loss to guide the attack. The Shape Prior Guided Attack is optimized using the Fast Optimization for Attacking (FOFA) algorithm, which efficiently finds spatially sparse adversarial points. The goal of this method is to create adversarial point clouds that have minimal perturbations while still being able to fool the target classification model.

Note that some of the attack approaches described earlier also involve addition of points and can be considered to be point add attacks. For example, Liu \etal~\cite{liu2020adversarial} present several attacks such as \textbf{Perturbation resampling}, \textbf{Adding adversarial sticks} and \textbf{Adding adversarial sinks}, which can be considered point add attacks. These attacks were explained in more detail in Section~\ref{subsubsubsec:PGD}.

\subsubsection{Point drop attacks}
\label{subsubsec:Point drop attacks}

Attacks described in the previous sections %mostly revolved around 
involved adding or shifting points in the point domain or transform (latent) domain.  %or transforming points (transforming points into another space and making changes there). 
This section reviews attacks that instead remove (drop) points from the point cloud to generate the adversarial point cloud. Obviously, the number of points in a point cloud reduces after a point drop attack. The points that are selected to be dropped are often referred to as ``critical'' points, in the sense that they are expected to be critical for a classifier to make the correct decision. Various methods have been developed to identify critical points in a point cloud.  %Depending on how points are dropped, these attacks can be made. The authors have provided various algorithms for removing critical points effectively. 

For example, Zheng \etal~\cite{zheng2019pointcloud} developed a method that uses a saliency map~\cite{zhou2016learning} to find critical points %that are important in model decision-making 
and drop them. As an illustration, critical points identified %The points dropped 
by high saliency values~\cite{zhou2016learning} are illustrated in red in Figure~\ref{fig_saliency}. The figure also shows what happens when these points are dropped. A version of this attack exists where, instead of dropping high-saliency points, they are shifted towards the point cloud center, thereby altering the shape of the point cloud in a manner similar to dropping points. %According to this method, every point is assigned a saliency score that reflects its contribution to the deep model recognition. By shifting high-saliency points towards the point cloud center, these points will not affect the surfaces much and practically operate in the same way as drop points. Consequently, the model can be deceived by shifting high-scoring points in a point cloud, resulting in adversarial point clouds. 
Two versions of this attack have become popular in the literature, %This method was proposed in two popular dropped attacks, 
\textbf{Drop100} and \textbf{Drop200}, which drop 100 and 200 points, respectively.


%-------------------figure7-------------------------------------------
\begin{figure*}
    \centering
    \includegraphics[width=0.9\textwidth]{Saliency.png}
    \caption{Original point clouds with labels (left), dropped points in red associated with highest scores (middle), and adversarial point clouds with new labels (right)~\cite{zheng2019pointcloud} (© 2019 IEEE. Reprinted, with permission, from~\cite{zheng2019pointcloud}).}
    \label{fig_saliency}
\end{figure*}
%-------------------figure7-------------------------------------------


An attack described in~\cite{naderi2022model} identifies ``adversarial drop points'' in a 3D point cloud that, when dropped, significantly reduce a classifier's accuracy. These points are specified by analyzing and combining fourteen-point cloud features, independently of a target classifier that is to be fooled. In this way, the attack becomes more transferable to different classifier models. %the model and determining which features play key roles in the model's decision-making.

In~\cite{wicker2019robustness}, the critical points are randomly selected and checked for dropping one by one. If dropping a point increases the probability of changing the ground-truth label  $f(\mathcal{P}) = Y$, such point is considered a critical point and will be dropped. Otherwise, it will not be dropped. This procedure continues iteratively until %the minimum critical points are dropped
the classifier's decision is changed. The process can be described %according to 
as the following optimization problem
\begin{equation}
\begin{split}
&\min\limits_{\mathcal{P}^{adv} \subseteq \mathcal{P}}   \;\;\;    (|\mathcal{P}| - |\mathcal{P}^{adv}|) \\
&\text{such that}  \;\;\;  f(\mathcal{P}^{adv}) \neq f(\mathcal{P})  
 \end{split}
\label{eq:60}
\end{equation}
where $|\mathcal{P}|$ and $|\mathcal{P}^{adv}|$ are the number of points in the original and adversarial point cloud, respectively. %The adversarial examples are generated by dropping critical points that optimize formula~\ref{eq:60}.

In order to determine the level of %effectiveness 
influence of a given point in PointNet decision-making, Yang \etal~\cite{yang2019adversarial} introduced a \textbf{Point-detachment attack} that assigned a \textit{class-dependent importance} to each point. A greedy strategy was employed to generate an adversarial point cloud, in which the most important points dependent on the ground-truth label are dropped iteratively. The class-dependent importance associated with a given point was determined by multiplying two terms. The first term used the PointNet feature matrix before max-pooling aggregation %(In this matrix, each row represents a point in the point cloud and each column represents a special feature). 
and the second term used the gradients relative to the ground-truth label output. The combination of these terms helped determine which points had the highest impact on the PointNet decision. %from gradient the feature matrix w.r.t. the ground-truth label output, which is a sparse matrix with non-zero only at the critical points. If a given point has the largest value in some columns, the first term sums the difference between the first and second largest values in these columns. A bigger difference means more significance for the largest value. This means that a given point that corresponds to the largest value is more effective in the model decision. The second term sums up all values for a given point at a row level in the sparse matrix.

\subsubsection{Generative strategies}
\label{subsubsec:generative-based}

% Since there are gradient back propagations when training the generative model or crafting adversarial examples, these methods are usually combined with other techniques, such as C\&W
Generative approaches utilize models such as Generative Adversarial Networks (GANs) and variational autoencoder models to create adversarial point clouds. Most of these attacks~\cite{zhou2020lg,lee2020shapeadv,tang2023deep,dai2021generating} attempt to change the shape of the point cloud in order to fool the deep model. The concept of these attacks can be related to what is called unrestricted attacks in 2D images~\cite{brown2018unrestricted,naderi2021generating,song2018constructing}. When such attacks occur, the input data might change significantly while remaining plausible. %without changing the semantics. T
These attacks can fool the classifier without making humans confused. In this regard, Lee \etal~\cite{lee2020shapeadv} proposed shape-aware adversarial attacks called \textbf{ShapeAdv} that are based on injecting an adversarial perturbation $\eta$ into the latent space $z$ of a point cloud autoencoder. Specifically, the original point cloud is processed using an autoencoder to generate an adversarial point cloud, then the adversarial point cloud is fed to the classifier. Lee \etal~\cite{lee2020shapeadv} proposed three attacks with varying distance measures, which are used as a term in the C\&W loss to maintain similarity between the original and adversarial point clouds. All three attacks calculate the gradient of the C\&W loss with respect to the adversarial perturbation of the latent representation $z$. The three attacks are as follows: 
\begin{enumerate}
\item \textbf{Shape-aware attack in the latent space.} Here, the goal is to minimize the $\ell_2$-distance between the latent representation $z$ and the perturbed representation $z+\eta$. Using this approach, the original and adversarial point clouds are close in the latent space, but they could be highly dissimilar in the point space. %generated adversarial point cloud is highly dissimilar from the original counterpart in terms of appearance.

\item \textbf{Shape-aware attack in the point space.} In this case, Chamfer distance is used to encourage the similarity of the original and adversarial point cloud in the point space. This is an attempt %is being made 
to resolve the issues with the previous attack, where the original and adversarial point cloud could be very different in the point space. %In order to maintain similarity between the original point cloud and the adversarial one, the distance measure is replaced by minimizing the Chamfer distance between the two.

\item \textbf{Shape-aware attack with auxiliary point clouds.} This attack minimizes the Chamfer distance between the adversarial point cloud and an auxiliary point cloud, which is created as the average of $k$ nearest neighbors sampled from the same class as the original point cloud. The goal is to avoid large adversarial perturbations in any direction in the latent space. To guide %the direction in the latent space, it employs auxiliary 
this process, point clouds sampled from the class of the original point cloud are used.
\end{enumerate} 


%\subsubsection{Shape attacks via autoencoders and generative models}
\label{subsubsec:GAN}
% This type of attack can be considered a subset of shape attacks because all attacks discussed (Advpc is not!) in this section morph the point cloud’s shape.
%Besides the above attacks, another group of shape attacks was developed based on generative models. 
Hamdi \etal~\cite{hamdi2020advpc} proposed an attack called \textbf{Advpc} by using an autoencoder that could be transferred between classification networks. The autoencoder was trained using a combination of two loss functions: the C\&W loss when the adversarial point cloud is fed directly to the classifier, and the C\&W loss when the point cloud is first fed to the autoencoder to project a perturbed point cloud onto the natural input manifold, then reconstructed, and then fed to the classifier. This strategy improved the transferability of the attack to different classification networks.


Tang \etal~\cite{tang2023deep} proposed a deep \textbf{manifold attack} that deforms the intrinsic 2-manifold structures of 3D point clouds. The attack strategy comprises two steps. In the first step, an autoencoder is used to establish a representation of the mapping between a 2D parameter plane and the underlying 2-manifold surface of the point cloud. This representation serves as a basis for subsequent transformations. The second step involves learning stretching operations within the 2D parameter plane. This stretching produces a 3D point cloud that can fool a pretrained classifier, all while keeping geometric distortion minimal.
%This was achieved by introducing a new loss function and pipeline. Minimizing two losses was the goal of the Loss function. The first loss is C\&W loss when adversarial point clouds are fed into deep models, and  the second loss is C\&W loss when adversarial point clouds are fed into deep models after reconstruction with a point cloud autoencoder. Using an autoencoder to generate an adversarial point cloud makes perturbations more meaningful. Consequently, their transferability from one network to another will be more promising.
%Lee \etal~\cite{lee2020shapeadv} also proposed Shape-aware attacks by injecting adversarial perturbation $\eta$ in the latent space $z$ of a point cloud autoencoder. In section~\ref{subsubsec:Shape}, this attack was described in detail.

%-------------------figure5-------------------------------------------
\begin{figure}
    \centering
    \includegraphics[width=0.4\textwidth]{lggan.png}
    \caption{An example of original point cloud and LG-GAN attack were proposed in~\cite{zhou2020lg} (© 2020 IEEE. Reprinted, with permission, from~\cite{zhou2020lg}).}
    \label{fig_lggan}
\end{figure}
%-------------------figure5-------------------------------------------


\textbf{LG-GAN} attack~\cite{zhou2020lg} generates an adversarial point cloud based on a Generative Adversarial Network (GAN). The GAN is trained using the original point clouds and target labels to learn how to generate adversarial point clouds to fool a classifier. It extracts hierarchical features from original point clouds, %using one multi-branch adversarial network, 
then integrates the specified label information into multiple intermediate features using the label encoder. The encoded features are fed into a reconstruction decoder to generate the adversarial point cloud. Once the GAN is trained, the attack is very fast because it only takes one forward pass to generate the adversarial point cloud. Figure~\ref{fig_lggan} shows an instance of the LG-GAN attack.


Dai \etal~\cite{dai2021generating} proposed another GAN-based attack, where the input to the GAN is noise, %new type of attack based on GAN, which is created from noise 
rather than the original point cloud. The noise vector and the target label are fed into a graph convolutional GAN, which outputs the generated adversarial point cloud. The GAN is trained using a %loss function containing 
four-part loss function including the objective loss, the discriminative loss, the outlier loss, and the uniform loss. %to achieve a realistic adversarial attack that fools the victim network. 
The objective loss encourages the victim network to assign the (incorrect) target label to the adversarial point cloud while the discriminative loss encourages an auxiliary network to classify the adversarial point cloud correctly. The outlier loss and the uniform loss %by removing outliers and generating a more uniform point cloud force 
encourage the generator to preserve the point cloud shape.
% the discriminator distinguishes between original and generated point clouds and classifies them using the auxiliary classifier.
Besides these GAN-based attacks, Lang \etal~\cite{lang2021geometric} proposed an attack that alters the reconstructed geometry of a 3D point cloud %rather than just the predicted label, 
using an autoencoder trained on semantic shape classes, while Mariani \etal~\cite{mariani2020generating} proposed a method for creating adversarial attacks on surfaces embedded in 3D space, under weak smoothness assumptions on the perceptibility of the attack. 

% Fengmei\etal~\cite{he2023point} designed a generator to produce point-to-point perturbations based on the original point cloud and its classification label. they apply the GAN structure to encourage distribution learning to boost the effectiveness of the attacker. they also design a dynamic outlier removal step to constrain the number of perturbed points by canceling excessive perturbation.

%%%%%%%------------------------------
%\subsection{Perturbation Techniques}
%\label{subsubsec:shift}
%%%%%%%------------------------------

%In the context of 3D point clouds, perturbation techniques can be classified into four primary categories. Among these, three techniques operate in the spatial point domain, encompassing point addition, point shift, and point drop attacks. %The remaining technique occurs in the transform domain, where perturbations are introduced through methods like frequency domain transformations. 
%Point addition attacks involve the addition of points to the point cloud with the aim of misleading deep models. %Point shift attacks, on the other hand, involve shifting points of the point cloud to fool the deep model, while the number of points remains the same as in the original point cloud. 
%Point drop attacks involve the intentional removal of points from the point cloud to deceive the deep model. Furthermore, in the subsequent sections, we will provide an in-depth exploration of specific methodologies for 3D attacks, shedding light on each of these four perturbation techniques.}

%%%%%%%------------------------------
\subsection{Attack location}
\label{subsubsec:perturbation location}
%%%%%%%------------------------------

The location of perturbations plays a crucial role in changing the shape and distribution of points within a point cloud. This can result in points being shifted either off the object's surface, introducing noise, or along the surface, thereby altering the distribution of points.
Hence, in terms of the location of the perturbations, attacks can be categorized into two groups: on-surface and off-surface.

\textbf{On-surface perturbation attacks} are those attacks in which the points of the adversarial cloud $\mathcal{P}^{adv}$ are located along the object's original surface. Notably, drop attacks~\cite{zheng2019pointcloud,wicker2019robustness,yang2019adversarial} are an example of such attacks, since drop attacks involve solely the removal of points from the point cloud, so the remaining points stay on the original surface. While other attack methods like point shift or transform would normally tend to move the points off the object's surface, various approaches can be employed to keep the points at or near the original surface. For example, Hamdi~\etal~\cite{hamdi2020advpc} employ an autoencoder that projects off-surface perturbations onto the natural input manifold, thereby minimizing the movements of points off the surface. Another example is provided by Tsai~\etal~\cite{tsai2020robust}, who developed the KNN attack. This approach introduces constraints on the distances between adjacent points by adding an extra loss term based on the K-Nearest Neighbor distances for each point, with the goal of keeping the perturbations on the original surface. 
In the VSA attack~\cite{arya2021adversarial}, the magnitude of perturbations applied to adversarial points is adjusted by controlling the step size, which again could be used to keep points on the surface. The ``distributional attack''~\cite{liu2020adversarial} employs the Hausdorff distance between the adversarial point cloud and the triangular mesh fitted over the original point cloud. In this way, perturbed points can be guided toward the triangular mesh, effectively keeping the perturbed points at or near the object's original surface.

\textbf{Off-surface perturbation attacks} produce adversarial point clouds $\mathcal{P}^{adv}$ that include points off the original object's surface. As noted earlier, many strategies for generating adversarial point clouds include a distance term $D(\mathcal{P},\mathcal{P}^{adv})$ between the original and adversarial point cloud. This distance term is either involved in a hard constraint (upper bounded by an explicit value) or included as a loss term in the overall loss function. However, if its hard constraint is too high or if its scaling factor in the loss function is too low compared to other terms, this can result in off-surface points. For example, Yang \etal~\cite{yang2019adversarial} set the upper bound on the Chamfer distance to $0.2$, which is somewhat high and could result in off-surface perturbations. In other cases, off-surface perturbations are intentionally created. For example, Liu~\etal~\cite{liu2020adversarial}, in their adversarial sticks attack, add four sticks to the point cloud. These sticks are attached at one end to the point cloud and extend a small distance away, thereby creating an off-surface attack. Similarly, Xiang \etal~\cite{xiang2019generating} introduce clusters, individual points, or small objects off the surface of the object to craft their attacks. It should be noted that off-surface attacks might be easier to detect since the perturbed cloud's appearance starts deviating more obviously from the original one.



%%%%%%%------------------------------
\subsection{Adversarial knowledge}
\label{subsubsec:Adversarial Knowledge}
%%%%%%%------------------------------

In the context of adversarial knowledge, attacks can be categorized into three classes: white-box, black-box, and gray-box attacks. This classification is based on the extent of the attacker's knowledge about the target model. White-box and black-box scenarios represent extremes, whereas the gray-box scenario covers a wide range of possibilities between these extremes.

\textbf{White-box attacks} are those in which the attacker has complete information about the DL model under attack. This includes knowledge of the model's architecture, parameters, loss function, training details, and input/output training data. In the literature on adversarial attacks on 3D point cloud models, white-box attacks are quite common. Examples in this category include various gradient-based attack methods, such as those by Zhang \etal~\cite{zheng2019pointcloud} and Liu \etal~\cite{liu2020adversarial}. These approaches make use of the gradients of the loss function, propagated back through the model, to construct a variety of attacks such as point shifting, addition, and dropping. Other examples include attacks developed by Xiang \etal~\cite{xiang2019generating} and Liu \etal~\cite{liu2019extending,liu2020adversarial}, among others.
%\cite{xiang2019generating,liu2019extending,arya2021adversarial,liu2020adversarial,hamdi2020advpc,lee2020shapeadv,zhou2020lg,yang2021adversarial,ma2020efficient,tsai2020robust,kim2021minimal,zheng2019pointcloud}

\textbf{Black-box attacks} are those in which the attacker has limited information -- sometimes none -- about the target model being attacked. In this case, at most, the attacker has access to the target model as a ``black box,'' meaning that it can generate the output of the model for a given input but lacks knowledge of the model's internal structure, training details, etc. Black-box attacks align more closely with real-world attack scenarios, but they are more difficult to construct. 

Black-box attacks are less common in the literature on adversarial attacks on 3D point cloud models. One example of a black-box attack is the ``model-free'' approach of Naderi \etal~\cite{naderi2022model}, which does not require any knowledge of the target model and instead focuses on identifying critical points within point clouds. This method takes advantage of the inherent properties of point clouds, bypassing the need for knowledge about the target model, and is therefore applicable to any model. Huang \etal~\cite{huang2022shape} proposed two versions of their attack, one white-box and the other black-box. The black-box attack relies on queries and saliency maps generated from a separate white-box surrogate model to craft adversarial perturbations that fool the target model.

Wicker and Kwiatkowska in~\cite{wicker2019robustness} randomly select and test critical points, dropping them if it increases the likelihood of changing the label. This iterative process continues until the model's decision changes. Therefore, the approach requires the ability to input a point cloud into the target model and access the output, but not the internal details of the target model, making it a black-box attack. ITA, by Liu \etal~\cite{liu2022imperceptible}, is another approach that could be classified as a black-box attack. It is based on subtly shifting each point in the point cloud along its normal vector, for which the knowledge of the internal architecture of the target model is not needed.  

The term \textbf{gray-box attack} has appeared in the literature more recently~\cite{Vivek_2018_ECCV}. It is intended to capture various scenarios between the extremes of white-box and black-box attacks. It should be noted that the boundaries of what are considered white- or black-box attacks are not crisp, and some variations in their interpretations do exist. For example, in a white-box scenario, the attacker may have access to all the internal parameters of the target model, but might not use all of them in constructing the attack. Therefore, such an attack can also be classified as a gray-box attack. That being said, so far very few approaches in the literature on 3D point cloud adversarial attacks have been declared gray-box, with notable exceptions in~\cite{dong2020self,Lang_3DV_2021}.


%%%%%%%------------------------------
\subsection{Target type}
\label{subsubsec:Target type }
%%%%%%%------------------------------

Some adversarial attacks attempt to guide the DL model towards a specific wrong label, while others simply want the model to produce any wrong label. The choice between these depends on the objectives of the attacker. Depending on the type of the label target, attacks can be classified as targeted or non-targeted.

\textbf{Targeted attacks} are those in which the goal is to make the DL model's output be a specific target label. There are two common approaches for choosing the target label in a targeted attack: 
\begin{enumerate}
    \item Most likely wrong label: Here, the target label is selected to be the one with the highest probability (confidence) other than the ground-truth label. The underlying idea is that this may be the easiest wrong label to lead the model towards.  
    \item Random label: Here, the target label is chosen randomly among the wrong labels. Although it might be harder to lead the model towards a randomly chosen label, such attack may be more impactful, especially in cases where the most likely wrong label is semantically close to the ground truth label (e.g., motorcycle vs. bicycle).
\end{enumerate}
The approach for target selection will depend on the specific objectives in a given scenario. For example, Wu \etal~\cite{wu2020if} and Naderi \etal~\cite{naderi2023lpf} have utilized the latter approach, while Ma \etal~\cite{ma2020efficient} have explored both strategies in their work.
 
\textbf{Non-targeted attacks} are those in which the goal is simply to make the DL model misclassify the input, regardless of which wrong label it eventually predicts. Examples of such attacks are those in~\cite{zheng2019pointcloud} and \cite{yang2019adversarial}, which operate by dropping points from the original point cloud until a label change occurs. Further examples of non-targeted attacks include ~\cite{hamdi2020advpc,lee2020shapeadv,zhou2020lg,wen2020geometry}. Interestingly, some studies present attacks that encompass both targeted and non-targeted types, for example~\cite{wicker2019robustness}. Here, a flexible attack framework is presented where, by encoding appropriate conditions and objectives through a Boolean function, both targeted and non-targeted attacks can be produced.

%In Table} \ref{table:Categories}, you can find a list of popular attacks categorized by targeted or non-targeted types.}







% \subsection{Attacks with minimal number of points%Minimal level of point manipulations for attacking
% }
% \label{subsubsec:Minimal}
% % 
% A special type of adversarial attack on 2D data focuses on perturbing the minimum number of pixels~\cite{su2019one,papernot2016limitations,carlini2017towards,modas2019sparsefool,croce2019sparse,narodytska2016simple,schott2018towards}. For instance, the one-pixel attack \cite{su2019one}, which changes only a single pixel in an image to generate an adversarial example, is a famous attack of this type. Taking inspiration from 2D attacks, Kim~\etal~\cite{kim2021minimal} proposed a so-called \textbf{minimal attack} that aims to manipulate a minimal number of points in a point cloud. This can be thought of as minimizing $D_{\ell_0}  (\mathcal{P} , \mathcal{P}^{adv})$. To find an adversarial point cloud, Kim~\etal~modify the loss function of the PGD attack~(\ref{eq:30}) by adding a term that tries to keep the number of changed points to a minimum. Furthermore, they used Hausdorff and Chamfer distances to preserve the similarity between $\mathcal{P}$ and $\mathcal{P}^{adv}$. Figure~\ref{fig_min} illustrates examples of minimal adversarial attack, where the altered points are indicated in red.

% %-------------------figure6-------------------------------------------
% \begin{figure}
%     \centering
%     \includegraphics[width=0.4\textwidth]{min.png}
%     \caption{Two examples of the original point cloud and the corresponding minimal adversarial attack, where the altered points are shown in red~\cite{kim2021minimal} (© 2021 IEEE. Reprinted, with permission, from~\cite{kim2021minimal}).}
%     \label{fig_min}
% \end{figure}
% %-------------------figure6-------------------------------------------



% In another attack called Variable Step-size Attack (\textbf{VSA})~\cite{arya2021adversarial}, a hard constraint on the number of modified points is incorporated into the optimization function of a PGD attack~(\ref{eq:30}) to try to preserve the point cloud's appearance. Specifically, points with the highest gradient norms, which are thought to have the greatest impact on classification, are selected initially. The selected points are then subject to adversarial perturbations. The goal is to shift these points in a way that maintains their original appearance while maximizing the loss function, thus causing the model to misclassify the input. By controlling the step size, VSA adjusts the magnitude of perturbations applied to the selected points. It starts with a larger step size to allow for rapid exploration of the optimization landscape. As the process advances, the step size is progressively reduced to guide the optimization toward more precise modifications. 

% Zhao \etal~\cite{zhao2020nudge} proposed a class of point cloud perturbation attacks called Nudge attacks that try to minimize point perturbation while changing the classifier's decision. They generated adversarial point clouds using gradient-based and genetic algorithms with perturbations of up to 150 points to deceive the classifier. In some cases, the attack can fool the classifier by changing a single point when the point has a large distance from the surface of the objects.

% Yang~\etal~\cite{yang2019adversarial} provided a point-attachment attack by attaching a few points to the point cloud. Chamfer distance is used to keep the distance between the newly added points and the original point cloud small. Hard constraints limit the number of points added in the point cloud, making the adversarial point cloud preserve the appearance of the original one. 


% Analogously to the one-pixel attack for images~\cite{su2019one}, Tan \etal~\cite{tan2023explainability} proposed an attack called \textbf{One point attack} in which only a single point in the point cloud needs to be shifted in order to fool the deep model. The authors also present a method to identify the most important points in the point cloud based on a saliency map, which could be used as candidates for the attack.

% Shape Prior Guided Attack~\cite{Shi2022Shape} is a method that uses a shape prior, or prior knowledge of the structure of the object, to guide the generation of the perturbations. This method introduces Spatial Feature Aggregation (SPGA), which divides a point cloud into sub-groups and introduces structure sparsity to generate adversarial point sets. It employs a distortion function comprising Target Loss and Logical Structure Loss to guide the attack. The Shape Prior Guided Attack is optimized using the Fast Optimization for Attacking (FOFA) algorithm, which efficiently finds spatially sparse adversarial points.} The goal of this method is to create adversarial point clouds that have minimal perturbations while still being able to fool the target classification model.


% \subsection{Miscellaneous attacks}
% \label{subsubsec:Other}


% Several other attacks that do not fall neatly into the categories defined above have been developed. Miao \etal~\cite{zhao2020isometry} proposed an adversarial point cloud based on rotation by applying an isometry matrix to the original point cloud. To find an appropriate isometry matrix, the authors used the Thompson Sampling method \cite{russo2018tutorial}, which can quickly find a suitable isometry matrix with a high attack rate.

% Liu~\etal~\cite{liu2022imperceptible} proposed an Imperceptible Transfer Attack (\textbf{ITA}) that enhances the imperceptibility of adversarial point clouds by shifting each point in the direction of its normal vector. Although some of the attacks described earlier also involve shifting points, the main difference here is that ITA aims to make the attack imperceptible, whereas earlier attacks may cause noticeable changes to the shape. 
% Along the same lines, Tang~\etal~\cite{tang2022normalattack} presented a method called \textbf{NormalAttack} for generating imperceptible point cloud attacks. Their method deforms objects along their normals by considering the object's curvature to make the modification less noticeable.
% Finally, Zhang~\etal~\cite{zhang20233d} proposed a \textbf{Mesh Attack} that directly perturbs the mesh of a 3D object, while trying to keep the modifications unnoticeable by using three losses including the C\&W loss that use chamfer distance,the laplacian loss and the edge length loss. . 

%--------------------------Defenses-------------------------------------------
\section{Defenses against adversarial attacks}
\label{sec:Defense}

Adversarial defense methods for 3D point clouds can be data-focused or model-focused, as indicated in Fig.~\ref{fig_overview}. Data-focused strategies involve modifying the data on which the model is trained, or at inference time, in order to defend against attacks. Model-focused strategies may involve changing the model's architecture and/or retraining it to increase its robustness against attacks. Of course, combinations of these strategies are also possible. %generally be divided into three categories: input transformation, data optimization, and deep model modification. 
The following sections discuss defense methods under each of these categories. Moreover, we have provided an overview of the most prevalent defense strategies in Table~\ref{table:Categories_defenses} to simplify navigation and provide quick reference.



%------------------% % %--------------------Table5------------------------------------

% For tables use
\begin{table*}
\centering
% table caption is above the table
\caption{Categorization of defenses against adversarial attacks.}
\label{table:Categories_defenses}       % Give a unique label
% For LaTeX tables use
\begin{tabular}{c c | c c}
%hline
\toprule

\bf Reference &  \bf Defense Name & \bf Data- / Model-focused & \bf Type   \\
   
% \multirow{1}{*}{\cite{xiang2019generating}} & - \\ 
% \hline

% \multirow{1}{*}{\cite{tsai2020robust}} & - \\ 

%\hline
\midrule

\multirow{1}{*}{Yang \etal~\cite{yang2019adversarial}} & SRS & Data & Input transformation \\ 
\hline

\multirow{1}{*}{Zhou \etal~\cite{zhou2019dup}} & SOR & Data & Input transformation \\ 
\hline

\multirow{1}{*}{Liu \etal~\cite{liu2019extending}} & SRP & Data & Input transformation \\ 
\hline

\multirow{1}{*}{Zhou \etal~\cite{zhou2019dup}} & DUP-Net & Data & Input transformation \\ 
\hline

\multirow{1}{*}{Wu \etal~\cite{wu2020if}} & If-Defense & Data & Input transformation \\ 
\hline

\multirow{1}{*}{Liu \etal~\cite{liu2019extending}} & FGSM & Data & Adversarial training \\ 
\hline

\multirow{1}{*}{Liu \etal~\cite{liu2022imperceptible}} & ITA & Data & Adversarial training \\ 
\hline

\multirow{1}{*}{Liang \etal~\cite{liang2022pagn}} & PAGN & Data & Adversarial training \\ 
\hline

\multirow{1}{*}{Sun \etal~\cite{sun2021adversarially}} & --- & Data & Adversarial training \\ 
\hline

\multirow{1}{*}{Zhang \etal~\cite{zhang2022comprehensive}} & --- & Data & Data augmentation \\ 
\hline

\multirow{1}{*}{Yang \etal~\cite{yang2019adversarial}} & --- & Data & Data augmentation \\ 
\hline

\multirow{1}{*}{Zhang \etal~\cite{zhang2022pointcutmix}} & PointCutMix & Data & Data augmentation \\ 
\hline

\multirow{1}{*}{Naderi \etal~\cite{naderi2023lpf}} & LPF-Defense & Data & Data augmentation \\ 
\hline


\multirow{1}{*}{Zhang \etal~\cite{zhang2019defense}} & Defense-PointNet & Model & Deep model modification \\ 
\hline


\multirow{1}{*}{Zhang \etal~\cite{zhang2019defense}} & CCN & Model & Deep model modification \\ 
\hline


\multirow{1}{*}{Li \etal~\cite{li2022robust}} & LPC & Model & Deep model modification \\ 
\hline

\multirow{1}{*}{Sun \etal~\cite{sun2020adversarial}} & DeepSym & Combined & Deep model modification \& adversarial training \\ 

\bottomrule

\end{tabular}
\end{table*}
%--------------% % %--------------------Table5------------------------------------

%---------------------------------------------------------------------

\subsection{Data-focused strategies}

\subsubsection{Input transformation}
\label{sec:Input transformation}

An input transformation is a preprocessing %approach 
step that involves applying some transformation(s) to the input point cloud before it is fed into the deep model. These transformations could be designed to reduce the sensitivity of the deep model to adversarial attacks or to make it more difficult for an attacker to craft an adversarial point cloud. Input transformation methods are listed below.



\paragraph{Simple Random Sampling (SRS)}
\label{sec:SRS}
Simple random sampling (\textbf{SRS})~\cite{xiang2019generating} is a statistical technique that randomly drops a certain number of points (usually 500) from an input point cloud, with equal probability. It is crude but very fast. Many attacks involve shifting or adding points to a point cloud to cause a deep model to make an error. Random removal of points may remove some of these deliberately altered/inserted points and thereby make it less likely for the model to make an error. %\textcolor{red}{(Can you briefly describe why it works? It is a bit counter-intuitive, because it looks like a drop attack.)}
% Despite its simplicity, this defense is a powerful one. A variety of attack methods have been proposed to defeat it.
%\textcolor{blue}{ SRS creates a representative subset of points from the original data, helping to reduce the sensitivity of deep models to adversarial attacks by introducing variability in the input data while preserving data integrity through random, unbiased selection, but Drop attack target specific points to manipulate the classification decision.}
% 

\paragraph{Statistical Outlier Removal (SOR)}
\label{sec:SOR}
Adversarial attacks that involve adding or shifting points usually result in outliers. %Since there exist outliers in most adversarial attacks, 
Based on this observation, Zhou~\etal~\cite{zhou2019dup} proposed a defense based on statistical outlier removal (\textbf{SOR}). Specifically, their method  removed %that trimmed the points 
a point in an adversarial point cloud if the average distance of the point to its $k$ nearest neighbors was larger than %falls outside the 
$\mu + \sigma\cdot\alpha$, where $\mu$ is the mean and $\sigma$ is the standard deviation of the distance of the $k$ nearest neighbors to %distance of all 
other points in the %original 
point cloud. Scaling factor $\alpha$ depends on $k$ and in \cite{zhou2019dup}, the authors used %Depending on the size of the analyzed neighborhood, $\alpha$ will be determined. (In \cite{zhou2019dup} 
$\alpha = 1.1$ and $k=2$. %are considered). 
A similar defense method was proposed in~\cite{zhou2018deflecting}. The Euclidean distance between each point and its $k$-nearest neighbors was used to detect outliers, and points with high average distances were discarded as outliers.

\paragraph{Salient Point Removal (SPR)}
\label{sec:Salient points removal}
Conceptually, salient point removal (\textbf{SPR}) is related to SOR, except that the outliers here are identified differently. %geometric rather than statistical. 
For example, Liu \etal~\cite{liu2019extending} assumed that the adversarial points have fairly large gradient values. Based on this assumption, %Taking this as true, 
this method calculates the saliency of each point using the gradient of the output class of the model %$f$ w.r.t. 
with respect to each point, and then removes the points with high saliency scores. %were discarded.

\paragraph{Denoiser and Upsampler Network (DUP-Net)}
\label{sec:DUP-Net}
The \textbf{DUP-Net} defense approach consists of two steps. The first is a ``denoising'' step using SOR to remove outliers. %To remove outliers, it uses SOR as a denoiser in the first step. 
This results in a point cloud with fewer points than the input cloud. The second step is upsampling using % the output of the first step is given to 
an upsampler network~\cite{yu2018pu} to produce a denser point cloud. These two steps are meant to undo typical attacks that generate outliers (either by shifting or adding points) in order to fool the deep model. By removing outliers and then bringing back the density, DUP-Net is meant to approximate the original point cloud. %It is generally found that adversarial perturbations are missing critical points from original point clouds, so this defense uses a denser point cloud tracking the underlying surface of the point cloud with uniform distribution to recover these critical points.


\paragraph{IF-Defense}
\label{sec:IF-Defense}
\textbf{IF-Defense}~\cite{wu2020if} is a preprocessing technique %on the input point cloud. It first employs 
whose first step is SOR to remove outliers from the input point cloud.
In the next step, two losses are used to optimize the coordinates of the remaining points under geometry and distribution constraints.
The geometry-aware loss tries to push points towards the surface to improve smoothness. To estimate the surfaces of objects, the authors train a separate implicit function network~\cite{peng2020convolutional,mescheder2019occupancy}. %on original point clouds.
Because the outputs of implicit functions are continuous, the predicted surface is locally smooth. This reduces the impact of the remaining outliers.
The second, distribution-aware loss, encourages the points to have a uniform distribution by maximizing the distance between each point and its $k$-nearest neighbors. Accordingly, IF-Defense produces smooth, uniformly sampled point clouds. %the input point clouds are captured in a clean shape using If-Defense.

Figure~\ref{fig_ifdefense} shows the results of three defense methods -- SOR, DUP-Net, and If-Defense -- against a Drop200 attack. As seen in the figure, SOR results in a relatively sparse point cloud, while DUP-Net produces a much denser cloud. IF-Defense produces a smooth, approximately uniformly sampled point cloud. 


%-------------------figure8-------------------------------------------
\begin{figure*}
    \centering
    \includegraphics[width=0.7\textwidth]{if-defense.png}
    \caption{Results of three different defense methods applied on the Drop200 attack. Figure taken from~\cite{wu2020if} (Image source:~\cite{wu2020if}; use permitted under the Creative Commons Attribution License CC BY 4.0).}
    \label{fig_ifdefense}
\end{figure*}
%-------------------figure8-------------------------------------------


\paragraph{Miscellaneous defenses}
\label{sec:other Defenses}
Besides the above defenses, a few other approaches have been proposed to counter adversarial attacks through input transformation. Dong~\etal~\cite{dong2020self} proposed Gather-vector Guidance (\textbf{GvG}), which is sensitive to the change of local features. In case the adversarial perturbation changes the local features, the gather-vector will also change, thereby providing a way to detect the attack. %This method learns to ignore noisy local features.
Zhang \etal~\cite{zhang2022ada3diff} proposed \textbf{Ada3Diff}, which uses adaptive diffusion to smooth out perturbations in the point cloud. In doing so, it acts similarly to outlier removal, since the points perturbed during the attack often reduce local smoothness in order to fool the classifier. %  is a method for defending against adversarial attacks on 3D point cloud models. It uses an adaptive diffusion process to smooth out perturbations in the point cloud, effectively reducing the impact of the adversarial attack.


Liu~\etal~\cite{liu2021pointguard} developed an ensembling method called \textbf{PointGuard}. Here, a number of random subsets of the point cloud are taken and each is separately classified. Then % points in the original point cloud, then predicts the label of the original point cloud based on 
the majority vote among the labels of these random subsets is taken as the final prediction. Similarly to SRS, the idea is that a random subset has fewer adversarially-perturbed points than the input point cloud, which may make it more likely to be classified correctly. An ensemble of such decisions makes the final prediction more robust.


%---------------------------------------------------------------------
\subsubsection{Training data optimization}
\label{sec:Data optimization}

Another group of defenses involves optimizing training data in order to make the trained model more robust against adversarial attacks. Various modifications to the training data have been proposed, as described below. %Another category is data optimization for training, which involves optimizing the training data to improve the robustness of the deep model to adversarial attacks. This could involve techniques such as data augmentation, which involves generating additional training examples by applying transformations to the existing training data, or adversarial training, which involves intentionally introducing adversarial examples into the training data in order to improve the model's robustness to such attacks. The following methods can be used to optimize data.


\paragraph{Adversarial training}
\label{sec:Adversarial Training}
One way to make the model more robust against adversarial attacks is to expose it to adversarial examples during training, which is termed %In terms of modified training sets, 
\textbf{adversarial training}~\cite{goodfellow2015explaining}. %is an effective defense method, which augments the training set with adversarial examples to increase the model’s robustness against attacks.  To be precise, in standard training, the model is trained using only the original point clouds, while 
In adversarial training, both the original and adversarial point clouds are used. The use of adversarial training as a defense for point cloud models was first described in~\cite{liu2019extending}.  The authors of~\cite{liu2019extending} and~\cite{liu2022imperceptible} trained a deep model by augmenting the training data using adversarial examples generated by FGSM and ITA attacks. 
As a way to improve adversarial training, the authors of~\cite{liang2022pagn} employed adaptive attacks. Using this new adversarial training, different types of attacks are added to the deep model by embedding a perturbation-injection module. This module is utilized to generate the perturbed features for adversarial training. Sun~\etal~\cite{sun2021adversarially} applied self-supervised learning to adversarial training on point clouds.
 
\paragraph{PointCutMix}
\label{sec:PointCutMix}
Zhang~\etal~\cite{zhang2022pointcutmix} proposed the \textbf{PointCutMix} technique to generate a new training set by swapping points between two optimally aligned original point clouds and training a model on this new training set. 
PointCutMix provides two strategies for point swapping: randomly replacing all points or replacing the k nearest neighbors of a randomly chosen point. Additionally, the method uses a saliency map to guide point selection, enhancing its effectiveness. Augmented sample labels in PointCutMix are formed by blending the labels of the source point clouds. The augmented point clouds, along with their associated labels, are integrated into the training set, thereby creating a novel collection of training samples that capture variations from both original point clouds. Overall, PointCutMix proves valuable for augmenting point cloud data in tasks such as classification and defense against adversarial attacks.

\paragraph{Low Pass Frequency-Defense (LPF-Defense)}
\label{sec:LPF-Defense}
In LPF-Defense~\cite{naderi2023lpf}, deep models are trained with the low-frequency version of the original point clouds.  More specifically, using the Spherical Harmonic Transform (SHT)~\cite{cohen2018spherical}, original point clouds are transformed from the spatial to the frequency domain. Then the high-frequency components are removed and the low-frequency version of the point cloud is %then retrieved back into 
recovered in the spatial domain. %by filtering the high-frequency input data components.  This method is based on the assumption that 3D deep models are overly dependent on features with unnecessary information in the training sets, making them vulnerable to adversarial point clouds. Therefore it discards the unnecessary information from the training data by suppressing the high-frequency contents in the training phase.
The idea is that adversarial attacks, through point shifting, insertion, or deletion, often introduce high frequencies into the point cloud. When a deep model is trained on the low-frequency versions of the point clouds, it learns to associate the label with low frequencies and thereby implicitly ignores high frequencies which may have been introduced during an attack.


%---------------------------------------------------------------------
\subsection{Model-focused strategies}

\subsubsection{Deep model modification}
\label{sec:Deep model modification}

Another class of defenses involves %category is deep model modifications, which refer to 
modifying the architecture of the deep model itself and may involve retraining in order to improve its robustness to adversarial attacks. %This could be achieved by making changes to the original deep neural network architecture during training. 
Examples of this type of defense are given below.


\paragraph{Defense-PointNet}
\label{sec:Defense-PointNet}
Zhang~\etal~\cite{zhang2019defense} proposed a defense method that involves splitting the PointNet model into two parts. The first part is the feature extractor, with a discriminator attached to its last layer. %enabling it to learn more powerful features. 
The second part is the remainder of the PointNet model, which acts as a classifier. %which is trained to classify each input correctly. 
The feature extractor is fed with %feeds 
a mini-batch consisting of the original point clouds and adversarial examples generated by the FGSM attack. %as input to extract features and also fool the discriminator. 
The discriminator attempts to classify whether the features come from the original or adversarial point cloud.
Model parameters are optimized using three different loss functions: one for the classifier, one for the discriminator, and one for the feature extractor. While discriminator loss encourages the model to distinguish the original point cloud from the adversarial one, the feature extractor loss tries to mislead the discriminator to label every feature %original/adversarial 
vector as the original. Therefore, the feature extractor acts as an adversary to the discriminator. Finally, the classifier loss encourages the classifier to give correct predictions for each input.


\paragraph{Context-Consistency dynamic graph Network (CCN)}
\label{sec:Context-Consistency dynamic graph Network}
Li \etal~\cite{li2022improving} proposed two methodologies to improve the adversarial robustness of 3D point cloud classification models. 
The first one involves a novel point cloud architecture named the Context-Consistency dynamic graph Network (CCN). This architecture is predominantly constructed upon the Dynamic Graph CNN (DGCNN) model~\cite{wang2019dynamic}, but it incorporates a lightweight Context-Consistency Module (CCM) into various layers of DGCNN. This module aims to reduce feature gaps between clean and noisy samples. 
The second one is a new data augmentation technique. In each training epoch, the method generates three types of batches from each sample: adversarial examples created by dropping points, adversarial examples created by shifting points, and clean samples. Subsequently, it dynamically identifies the most appropriate samples based on their accuracy to train the model, thereby adaptively balancing the model's accuracy and robustness to attacks.
To provide a more robust model against adversarial point clouds, the authors integrate the two methodologies.


\paragraph{Lattice Point Classifier (LPC)}
\label{sec:Lattice Point Classifier (LPC)}
Li~\etal~\cite{li2022robust} proposed embedding a declarative node into the networks to transform adversarial point clouds such that they may be classified more easily. %examples to the clean manifold. 
Specifically, structured sparse coding in the permutohedral lattice~\cite{kiefel2015permutohedral} is used to construct a %  The authors proposed an effective instantiation, the 
Lattice Point Classifier (LPC). The LPC projects each point cloud onto a lattice and generates a 2D image, which is then input to a 2D CNN for classification. %(Structured sparse coding in the permutohedral lattice is defined as the declarative node in LPC.). The declarative nodes defend the adversarial attacks through implicit gradients by leading them to wrong updating directions for inputs.
Projection onto a lattice may %be considered as a form of vector quantization, which may 
remove some of the noise and/or outliers introduced during an adversarial attack.

\subsubsection{Model retraining}
Model (re)training strategies include adversarial (re)training, discussed in Section~\ref{sec:Adversarial Training}, but also other strategies intended to make the model more robust without explicitly using adversarial examples. Such strategies may involve various data augmentation methods, additional regularization terms to encourage robustness and generalization, as well as contrastive learning to robustify class boundaries. The authors of~\cite{zhang2022comprehensive,yang2019adversarial} augmented the training data by noise to make the resulting model more robust against attacks. One noise model employed was additive Gaussian noise, which was meant to improve robustness against point shifts in an attack. Another type of noise used was quantization noise, which involved converting point cloud coordinates to low precision during training. Quantization noise is often modeled as uniform noise~\cite{Widrow2008}, so this augmentation was meant to improve robustness against small point movements in a limited range.

\subsubsection{Combined strategies}
Some of the adversarial defense methods combine various strategies described above. 
For example, Sun \etal~\cite{sun2020adversarial} studied the role of pooling operations in enhancing model robustness during adversarial training. They found that fixed operations like max-pooling weaken the effectiveness of adversarial training, while sorting-based parametric pooling operations improve the model's robustness. As a result, they proposed \textbf{DeepSym}, a symmetric pooling operation that increases model's robustness to attacks.




\section{Datasets and victim models}
\label{sec:Taxonomy}

A variety of 3D point cloud datasets have been collected to train and evaluate deep models on point cloud classification. These include ModelNet~\cite{wu20153d}, ShapeNet~\cite{chang2015shapenet}, ScanObjectNN~\cite{uy2019revisiting}, McGill Benchmark~\cite{siddiqi2008retrieving}, ScanNet~\cite{dai2017scannet}, Sydney Urban Objects~\cite{de2013unsupervised}. 
A summary of these datasets and their unique characteristics is presented in Table~\ref{table:datasets}.

These 3D point cloud datasets can be broadly categorized into two groups: synthetic and real. ShapeNet and ModelNet are well-known datasets that contain synthetic data. These datasets are often used for model training and evaluation in controlled settings, because objects in synthetic datasets are typically complete, without occlusions, ``holes,'' and free of noise. For instance, ModelNet10 and ModelNet40 consist of 3D models of various objects, categorized into 10 and 40 classes, respectively, and are widely used in point cloud research. ShapeNet is a larger dataset with a larger number of classes, making it suitable for more challenging classification tasks. Virtual KITTI~\cite{gaidon2016virtual} is an example of a synthetic dataset built for autonomous driving.  

In contrast, datasets such as ScanNet and ScanObjectNN contain real data collected from real-world measurements, reflecting the complexity and variability of actual environments. ScanObjectNN is a real-world dataset suitable for evaluating 3D object classification in real-world scenarios. ScanNet is another real-world dataset that includes 3D scans of indoor environments.  KITTI~\cite{geiger2012we} is a real-world dataset featuring 3D scenes related to autonomous driving. Real 3D point-cloud scans are often subject to occlusion and may contain noise, which may necessitate ``hole filling''~\cite{dinesh2018inpainting} and/or denoising~\cite{dinesh2020point} before further use.
Among the datasets discussed above,  ModelNet10~\cite{wu20153d}, ModelNet40~\cite{wu20153d}, ShapeNet~\cite{chang2015shapenet} and ScanObjectNN~\cite{uy2019revisiting} have been very popular in the literature on point cloud adversarial attacks and defenses. 

Table~\ref{tab_Bechmark} presents an overview of prominent victim models that researchers commonly employ to assess adversarial attacks and defense strategies in the context of point cloud classification. PointNet, PointNet++, and DGCNN are the models that are the most frequently targeted for adversarial assessment.
Each of these models employs distinct mechanisms for processing point clouds. PointNet employs multi-layer perceptrons (MLPs) to extract pointwise features and aggregate them using max-pooling. PointNet++ builds upon PointNet, incorporating three key layers: the sampling layer, the grouping layer, and the PointNet-based learning layer. This architecture is repeated to capture fine geometric structures in point clouds.
DGCNN, another widely used model, leverages local geometric structures by constructing a local neighborhood graph and applying convolution-like operations on the edges connecting neighboring points.

Beyond these popular models, there are other notable architectures like PointConv, which extends the Monte Carlo approximation of 3D continuous convolution operators. It employs MLPs to approximate weight functions for each convolutional filter and applies density scaling to re-weight these learned functions.
The Relation-Shape Convolutional Neural Network (RS-CNN) extends regular grid CNNs to handle irregular point-cloud configurations. It achieves this by emphasizing the importance of learning geometric relations among points, forcing the convolutional weights to capture these relations based on predefined geometric priors.
VoxNet, on the other hand, is an architecture that combines a volumetric grid and a 3D CNN to improve object recognition using point cloud data from sensors like LiDAR and RGBD cameras. VoxNet predicts object class labels directly from the volumetric occupancy information. 

SpiderCNN is specifically designed for extracting geometric features from point clouds. It achieves this by using a family of convolutional filters parametrized as a combination of a step function, capturing local geodesic information, and a Taylor polynomial to enhance expressiveness.
PointASNL is capable of handling noisy point clouds effectively. Its core feature is the adaptive sampling module, which re-weights and adjusts sampled points to improve feature learning and mitigate the impact of outliers. It also includes a local-nonlocal module to capture local and global dependencies.
CurveNet addresses the limitations of existing local feature aggregation approaches by grouping sequences of connected points (curves) through guided walks in point clouds and then integrating these curve features with point-wise features.
Lastly, AtlasNet introduces a novel approach to 3D shape generation that does not rely on voxelized or point-cloud representations. Instead, it directly learns surface representations by deforming a set of learnable parameterizations.
%Also, there is a taxonomy of datasets and victim models used in recent studies in Table~\ref{tab_Bechmark}.

% %--------------------Table4------------------------------------
\begin{table*}
\caption{
{ Summary of the datasets commonly used 3D point cloud classification.
}}
\centering
%\begin{tabular*}{\textwidth}{c|c|c c c c c c}
\begin{tabular}{ c  c  c  c  c }
%\hline
\toprule
%\multicolumn{1}{|c|}{Defenses}  & \multicolumn{7}{c|}{Attacks}\\
% \multicolumn{5}{c}{\bf Datasets }\\
%\hline
\bf Dataset & \bf Year & \bf Type & \bf Classes & \bf Samples (Training / Test) \\
%\hline
\midrule
ModelNet10~\cite{wu20153d} & 2015 & Synthetic & 10 & 4899 (3991 / 605)\\

ModelNet40~\cite{wu20153d} & 2015 & Synthetic & 40 & 12311 (9843 / 2468)\\

ShapeNet~\cite{chang2015shapenet} & 2015 & Synthetic & 55 & 51190 ( / ) \\

ScanObjectNN~\cite{uy2019revisiting} & 2019 & Real & 15 & 2902 (2321 / 581)\\

KITTI~\cite{geiger2012we} & 2012 & Real & 8 & 7058 (6347 / 711) \\

Virtual KITTI~\cite{gaidon2016virtual} & 2016 & Synthetic & 8 & 21260 ( / ) \\

ScanNet~\cite{dai2017scannet} &2017 & Real & 17 & 12283 (9677 / 2606) \\

3DMNIST~\cite{3Dminst} & 2019 & Synthetic & 10 & 12000 (10000 / 2000) \\

McGill Benchmark~\cite{siddiqi2008retrieving} & 2008 & Synthetic & 19 & 456 (304 / 152) \\

Sydney Urban Objects~\cite{de2013unsupervised} & 2013 & Real & 14 & 588 ( / ) \\
%\hline
\bottomrule
\end{tabular}
\label{table:datasets}
\end{table*}
% %--------------------Table1------------------------------------





% %--------------------Table5------------------------------------
% \begin{landscape}
\begin{table*}
\begin{center}
\caption{Summary of datasets and victim models used in attacks and defenses on 3D point clouds.}
\label{tab_Bechmark} 
\begin{tabular}{ c|c|c} 
\hline

% ,\cite{he2023point}
\multirow{9}{*}{Datasets}
& \multirow{1}{*}{ModelNet10~\cite{wu20153d}} & \cite{wicker2019robustness}, \cite{sun2020adversarial}, \cite{sun2021improving}, \cite{zhao2020nudge}\\
\cline{2-3}
& \multirow{3}{*}{ModelNet40~\cite{wu20153d}} & \cite{liu2019extending}, \cite{wicker2019robustness}, \cite{liu2022imperceptible}, \cite{kim2021minimal}, \cite{liu2020adversarial}, \cite{arya2021adversarial}, \cite{tsai2020robust}, \cite{zhou2020lg}, \cite{wen2020geometry}, \cite{zheng2019pointcloud}\\ && \cite{xiang2019generating}, \cite{wu2020if}, \cite{huang2022shape}, \cite{tang2022rethinking}, \cite{liang2022pagn}, \cite{denipitiyage2021provable}, \cite{sun2020adversarial}, \cite{sun2021local}, \cite{sun2021improving}, \cite{zhang20233d}\\ && \cite{lee2020shapeadv}, \cite{dong2020self}, \cite{zhao2020nudge}, \cite{zhao2020isometry}, \cite{ma2020efficient}, \cite{zhou2019dup}, \cite{ma2021towards}, \cite{li2022robust}, \cite{miao2022isometric}, \cite{liu2022point}, \cite{Shi2022Shape}, \cite{he2023point}\\
\cline{2-3}
&\multirow{1}{*}{ShapeNet~\cite{chang2015shapenet}} & \cite{arya2021adversarial}, \cite{zhou2020lg}, \cite{hamdi2020advpc}, \cite{wu2020if}, \cite{tang2022rethinking}, \cite{zhao2020isometry}, \cite{lang2021geometric}, \cite{zhang2019defense}\\
\cline{2-3}
& \multirow{1}{*}{ScanObjectNN~\cite{uy2019revisiting}} & \cite{kim2021minimal}, \cite{denipitiyage2021provable}, \cite{liu2021pointguard}, \cite{sun2020adversarial}, \cite{sun2021improving} \\
\cline{2-3}

& \multirow{1}{*}{KITTI~\cite{geiger2012we}} & \cite{wicker2019robustness}, \cite{cheng2021universal} \\
\cline{2-3}

& \multirow{1}{*}{ScanNet~\cite{qi2017pointnet}} & \cite{liu2021pointguard}, \cite{li2022robust}  \\
\cline{2-3}


% & MNIST\cite{} & \cite{}\\
& \multirow{1}{*}{3D-MNIST~\cite{3Dminst}} & \cite{zheng2019pointcloud}, \cite{Shi2022Shape}\\
\hline


\multirow{18}{*}{Victim models}
% & P\cite{} & \cite{} , \cite{}  \\


& \multirow{3}{*}{PointNet~\cite{qi2017pointnet}} & \cite{liu2019extending}, \cite{wicker2019robustness}, \cite{yang2019adversarial}, \cite{liu2022imperceptible}, \cite{kim2021minimal}, \cite{liu2020adversarial}, \cite{arya2021adversarial}, \cite{zhou2020lg}, \cite{wen2020geometry}, \cite{hamdi2020advpc}\\
&& \cite{zheng2019pointcloud}, \cite{xiang2019generating}, \cite{wu2020if}, \cite{liang2022pagn}, \cite{liu2022boosting}, \cite{denipitiyage2021provable}, \cite{liu2021pointguard}, \cite{sun2020adversarial}, \cite{sun2021local}, \cite{sun2021improving}\\ && \cite{dai2021generating}, \cite{zhang20233d}, \cite{lee2020shapeadv}, \cite{dong2020self}, \cite{zhao2020nudge}, \cite{zhao2020isometry}, \cite{lang2021geometric}, \cite{zhang2019defense}, \cite{ma2020efficient}, \cite{zhou2019dup}\\ && \cite{cheng2021universal}, \cite{ma2021towards}, \cite{li2022robust}, \cite{miao2022isometric}, \cite{liu2022point}, \cite{Shi2022Shape}, \cite{he2023point}\\
\cline{2-3}

& \multirow{3}{*}{PointNet++~\cite{qi2017pointnet1}} & \cite{liu2019extending}, \cite{yang2019adversarial}, \cite{liu2022imperceptible}, \cite{kim2021minimal}, \cite{liu2020adversarial}, \cite{arya2021adversarial}, \cite{tsai2020robust}, \cite{zhou2020lg}, \cite{wen2020geometry}, \cite{hamdi2020advpc}\\ && \cite{zheng2019pointcloud}, \cite{xiang2019generating}, \cite{wu2020if}, \cite{huang2022shape}, \cite{tang2022rethinking}, \cite{liang2022pagn}, \cite{liu2022boosting}, \cite{sun2021local}, \cite{dai2021generating}, \cite{zhang20233d}\\ && \cite{lee2020shapeadv}, \cite{dong2020self}, \cite{zhao2020isometry}, \cite{ma2020efficient}, \cite{zhou2019dup}, \cite{cheng2021universal}, \cite{miao2022isometric}, \cite{liu2022point}, \cite{Shi2022Shape}, \cite{he2023point}\\
\cline{2-3}
& \multirow{3}{*}{DGCNN~\cite{phan2018dgcnn}} & \cite{yang2019adversarial}, \cite{liu2022imperceptible}, \cite{kim2021minimal}, \cite{liu2020adversarial}, \cite{arya2021adversarial}, \cite{zhou2020lg}, \cite{wen2020geometry}, \cite{hamdi2020advpc}, \cite{zheng2019pointcloud}, \cite{xiang2019generating}\\ && \cite{wu2020if}, \cite{huang2022shape}, \cite{tang2022rethinking}, \cite{liang2022pagn}, \cite{liu2022boosting}, \cite{liu2021pointguard}, \cite{sun2021improving}, \cite{dai2021generating}, \cite{zhang20233d}, \cite{lee2020shapeadv}\\ && \cite{zhao2020nudge}, \cite{zhao2020isometry}, \cite{ma2020efficient}, \cite{ma2021towards}, \cite{miao2022isometric}, \cite{liu2022point}, \cite{Shi2022Shape}, \cite{he2023point}\\
\cline{2-3}
&PointConv~\cite{wu2019pointconv} & \cite{wu2020if}, \cite{tang2022rethinking}, \cite{liu2022boosting}\\
\cline{2-3}
&RS-CNN~\cite{liu2019relation} & \cite{wu2020if}, \cite{he2023point}\\
\cline{2-3}
&VoxNet~\cite{maturana2015voxnet} & \cite{wicker2019robustness}   \\
\cline{2-3}
&SpiderCNN~\cite{xu2018spidercnn} & \cite{kim2021minimal}\\
\cline{2-3}
&PointASNL~\cite{yan2020pointasnl} & \cite{kim2021minimal}\\
\cline{2-3}
&CurveNet~\cite{xiang2021walk} & \cite{huang2022shape}\\
\cline{2-3}
&AtlasNet~\cite{groueix2018papier} & \cite{lang2021geometric}\\
\cline{2-3}
&PointTrans~\cite{zhao2021point} & \cite{liu2022point}\\
\cline{2-3}
&PointMLP~\cite{ma2022rethinking} & \cite{liu2022point}\\

\hline

\end{tabular}
\end{center}
\end{table*}
% \end{landscape}
% %--------------------Table5------------------------------------



\section{Challenges and future directions}
\label{sec:Challenges}

In this section, we explore the current challenges within the domain of adversarial attacks and defenses on 3D point clouds. We also present several promising directions for future research in this area. %We have provided intriguing insights and have highlighted several crucial issues, each offering the potential to shed light on the development of more effective attack and defense strategies.


\subsection{Current challenges}

\subsubsection{Crafting real-world attacks}
%The irregular structure of 3D point clouds makes the construction of attacks challenging. For instance, many attacks introduce high frequencies into the point cloud through methods like point shifting or adding. But if the original point cloud already contains high frequencies, they may mask the attack and therefore make defenses less effective.
As mentioned earlier, majority of attacks on 3D point clouds reported in the literature are white-box attacks. However, in practice, the white-box scenario is much less likely compared to the black-box and gray-box scenarios. Existing results suggest that black-box attacks are much less effective than white-box attacks. Hence, one of the current challenges is developing attack strategies that do not rely on complete knowledge of the target model and whose effectiveness could approach that of white-box attacks. 

%\textbf{Developing practical attacks in the real world.} Theoretical progress in adversarial point clouds has been achieved, but applying these advancements into practical real-world applications, such as autonomous driving, remains a challenge. While white-box attacks are often impractical, the widespread reliance on them underscores the significance of transfer-based attack methods.  However, the current transfer-based attacks in the physical environment are still relatively weak. Addressing this gap between theoretical developments and practical effectiveness is a crucial challenge.}

\subsubsection{Understanding the role of frequency} 

Points in a point cloud are irregularly placed in the 3D space. This makes understanding the frequency content of point clouds more challenging than that in the case of images or other regularly-sampled signals. Tools from graph signal processing~\cite{GSP2013SPM} or spherical harmonic analysis~\cite{cohen2018spherical} are useful in this context, but the fact remains that even the basic notion of frequency and its role in attacks and defenses is harder to analyze in the case of point clouds.
Many attacks introduce high frequencies into the point cloud through methods like point shifting or adding. But if the original point cloud already contains high frequencies, they may mask the attack and therefore make defenses less effective.

A better understanding of the role of frequency may help explain the reasons behind the vulnerability of 3D deep models to adversarial attacks. %More research is necessary to uncover these vulnerabilities in the context of 3D deep learning. Currently, as far as we know, only one paper
For example,~\cite{naderi2023lpf} has tackled this problem, suggesting that 3D deep models may rely too heavily on high-frequency details within 3D point clouds, and removing these details could potentially lead to models that are more robust against attacks. Such deeper understanding may be useful in the context of adversarial attacks and defenses in other areas, not just 3D point clouds. %Hence, it is imperative to pursue further research to delve deeper into the causes of vulnerability in 3D DNNs and discover effective solutions.



%\textbf{Vulnerabilities in defenses against drop attacks.} 
%Drop attacks cannot be effectively countered by the existing defense mechanisms. Researchers have been working towards this goal to develop an effective defense against drop attacks. As of yet, no existing work meets this necessity. 

\subsubsection{Training for robustness}

As mentioned earlier in Section~\ref{sec:Defense}, model training plays a key role in achieving robustness against adversarial attacks. %However, it is a challenging process due to the irregular structure of point clouds. 
The issue of distinguishing the appearance of point clouds from one class versus another class may present significant challenges. For example, a ``flower pot'' looks similar to a ``cup'' (see Figure~\ref{fig_saliency}) due to its conical shape, so it does not take much to make a deep model misclassify one for another. From this point of view, models should be trained to be very strong at distinguishing classes whose appearance is similar. This would help improve not only robustness against attacks but also the overall accuracy and generalizability.

A basic premise in statistical ML~\cite{abu-mostafa2012LFD} is that simpler models generalize better, although they may not be as accurate as more complex models. Since adversarial attacks often involve perturbations of the original point cloud, this would seem to imply that simpler models are less likely to be fooled by them. From this point of view, the choice of a model is a trade-off between accuracy, which generally requires higher complexity, and robustness (to attacks, as well as unseen data), which seems to favor not-too-high complexity. Since accuracy is the predominant factor of usefulness of a model, the trend has been towards more complex models, but with additional regularization~\cite{abu-mostafa2012LFD} and more sophisticated learning strategies to strengthen the robustness.
%It is important to keep complexity at a reasonable level. We typically have lower robustness if we have an actual lower complexity because they have simpler decision boundaries, which are more easily attacked.





\subsection{Future directions}

\subsubsection{Transferability}
In the context of adversarial attacks, the term \textbf{transferability} refers to the ability of an attack against a given target model to be effective against a different, potentially unknown model. Transferable attacks are not tied the specifics of any one model, but target more fundamental issues, and are therefore also useful in broadening the understanding of adversarial attack and defense principles. Currently, there is a limited amount of research on transferable attacks on 3D point clouds ~\cite{hamdi2020advpc,liu2022boosting,he2023generating,liu2022imperceptible}, so this is one potentially fruitful direction for future research. %has been done on transferability in this field, and the transferability of 3D point cloud adversarial attacks remains notably underdeveloped. Hence, it is crucial to conduct an in-depth examination of this aspect.


\subsubsection{New tasks} 
Presently, most adversarial attack research is focused on the classification task. This was in part influenced by the wide availability of datasets and related classification models. However, in practice, the role of adversarial attacks is to disrupt a complex system, which may involve other tasks such as detection, segmentation, tracking, etc. It is important to study adversarial attacks and defenses in these more general settings, %that relies on the used of  with only minimal attention directed toward alternative applications. It is imperative to assess the efficacy of these 3D attacks within different contexts, such as point cloud segmentation, 
in order to gain a comprehensive understanding of their performance in diverse applications.


\subsubsection{Point cloud attributes} 
The vast majority of adversarial attacks and defenses related to point clouds have focused on point-cloud geometry. However, point clouds may also have attributes such as color~\cite{8i2017color}. Changing the color of points in a point cloud may disrupt classification, segmentation, and other analysis tasks, hence attributes are a potential target for attacks. Since the color attributes of a point cloud play a similar role to the pixel colors in an image, 2D attacks and defenses may provide useful guidelines for initiating the work in this area. Moreover, this would open up possibilities for creating attacks and defenses that simultaneously consider geometry and attributes, a previously unexplored topic.


% \subsection{Factors affecting the success of adversarial attacks}
% \label{sec:What factors affect the success of adversarial attacks on 3D point clouds?}

% General factors affecting the success of adversarial attacks on point cloud models include model complexity and the characteristics of the point cloud itself. A basic premise in statistical machine learning~\cite{abu-mostafa2012LFD} is that simpler models generalize better, although they may not be as accurate as more complex models. Since adversarial attacks often involve perturbations of the original point cloud, this would seem to imply that simpler models are less likely to be fooled by them. From this point of view, the choice of a model is a trade-off between accuracy, which generally requires higher complexity, and robustness to attacks, which seems to favor lower complexity. 

% %There are some general factors that be more important for adversarial attacks on 3D point clouds including:
% %The complexity and robustness of the model being attacked: When a deep model is less complex and less robust, it may be less immune to adversarial attacks and require a less sophisticated or weaker attack to fool it.
% %The structure of the 3D point cloud: The distribution of points in the point cloud and the presence of outliers can potentially affect the success of most types of adversarial point clouds.

% The other aspect is the structure of the point cloud itself. It was mentioned earlier that many attacks introduce high frequencies into the point cloud through point shifting or insertion. But if the original point cloud already contains high frequencies, they may mask the attack and therefore make defenses less effective. Another aspect is how distinct the appearance of the point clouds from one class versus another class. For example, a ``flower pot'' looks similar to a ``cup'' (see Figure~\ref{fig_saliency}) due to its conical shape, so it does not take much to make a deep model misclassify one for another. From this point of view, models should be trained to be very strong at distinguishing classes whose appearance is similar.   


% \subsection{Comparison of Different Defense Methods}
% \label{sec:Comparison of Different Defense Methods}

% The distribution of points in a 3D point cloud, as well as potential outliers, can significantly impact the effectiveness of defense methods against adversarial attacks.
% For example, input transformation techniques are designed to make it more difficult for an attacker to craft adversarial point clouds. These techniques may rely on modifying the location and/or distribution of points in the point cloud, or pre-emptively removing outliers. By doing this, the structure of the original point cloud is changed in a way that makes it harder for the attacker to make successful modifications.

% Adversarial training has first been proposed for 2D data (images).
% The paper~\cite{madry2019deep} proves that adversarial training maximizes the classifier loss by finding a worst-case example inside a constrained search space. This procedure can change decision boundaries so that the model gets more robust to different types of attacks. The proof relies on the regular structure of 2D data.   
% However, a point cloud is a set of 3D data points that are placed irregularly in space. Furthermore, typical point clouds do not have uniform density, which is another difference from typical 2D data. 

% These structural differences result in different defense behaviors in the adversarial training phase. Therefore, training the model with the worst-case example inside a constrained search space can not guarantee robustness against other attacks. In other words, due to the irregular structure of 3D point clouds, it is very challenging to model adversarial points to eliminate their impact on defense. This is one of the reasons why adversarial training has not been as successful in the case of 3D point clouds as it was in the case of 2D images. 

% \subsection{Comparison of 3D point clouds and 2D image data in terms of attacks and defenses}
% \label{sec:Comparison of 3D point clouds and image data in terms of attacks and defenses}
% % Why are 3D point clouds more challenging compared to image data?
% % Comparison of Diﬀerent Defense Methods

% Initial ideas about adversarial attacks and defenses have been developed in the context of 2D image data. Many of the approaches to attacks and defenses have been extended from image to point cloud data. However, as noted above, there are some fundamental differences between 2D image data and 3D point clouds that can make transplanted 2D methods less effective in 3D, and thus necessitate developing new, made-for-3D approaches. 

% First, points in a point cloud are irregularly placed in the 3D space, in contrast to regularly sampled pixels in an image. This makes understanding the frequency content of point clouds more challenging than that in the case of images. Tools from graph signal processing~\cite{GSP2013SPM} or spherical harmonic analysis~\cite{cohen2018spherical} are useful in this context, but the fact remains that even the basic notion of frequency and its role in attacks and defenses is harder to analyze in the case of point clouds. 

% Second, attacks on images have generally used small perturbations of pixel values, leaving the number of pixels the same as in the original image. While there are attacks on point clouds with similar characteristics, many successful attacks deviate from this philosophy and involve substantial modifications to the point locations, as well as insertion or removal of points. 

% Finally, the models used to analyze images and point clouds are different. Since the goal of an adversarial attack is to fool the target victim model(s), this necessitates crafting adversarial attacks for this purpose and thereby deviating from the 2D version of the attack that may have served as an inspiration. For this reason, defenses also need to be tailored to the models used to analyze point clouds, even if the inspiration came from 2D defenses. 

% %On the other hand, attacks on 3D point cloud 
% %There are several differences between 3D point clouds and images in terms of adversarial attacks and defenses:
% %An adversarial attack on 3D point clouds can be more complex. Typically, an adversarial attack on an image data involves adding small perturbations to the pixel values. In contrast, adversarial attacks on 3D point clouds can involve more complex modifications, such as adding or dropping points, or changing the connectivity of the points in the point cloud. In fact, the structure of 3D point clouds is different from that of images. Images are typically represented as 2D arrays of pixel values, while 3D point clouds are represented as sets of 3D points. This difference in structure can make it more challenging to apply defense methods that were developed for image data to 3D point clouds. On the other hand, 3D point clouds can be more sensitive to perturbations. Because 3D point clouds are used to represent physical objects in the real world, even small perturbations to the point cloud can result in significant changes to the shape or appearance of the represented object. This sensitivity can make it more difficult to develop robust defense methods for 3D point clouds.




\section{Conclusion}
\label{sec:Conclusion}

Adversarial attacks on 3D point cloud classification have become a significant concern in recent years. These attacks are able to manipulate 3D point clouds in a way that leads the victim model(s) to make incorrect decisions with potentially harmful consequences. Adversarial attacks on 3D point clouds can be categorized according to the methodologies employed to modify the point cloud, and may have additional attributes in terms of the location of the attack, target type, and adversarial knowledge. We have reviewed a variety of attack methodologies, with examples from the existing literature, highlighting their main characteristics and their relationships. %into several types, including point removal attacks, point insertion attacks, point shift attacks, and transform attacks.  

To defend against these attacks, researchers have proposed two main categories of approaches: data-focused and model-focused. Data-focused techniques attempt to undo adversarial modifications on the point cloud in order to increase the chance of correct decision, while model-focused approaches attempt to make the model(s) more resilient to adversarial attacks. %input transformation and adversarial training. Input transformation methods aim to preprocess the input data to make it more robust to adversarial perturbations, while adversarial training involves augmenting the training data with adversarial examples to improve the model's robustness against attacks. 
For stronger protection against %adversarial 
attacks, data-focused and model-focused %input transformation 
techniques can be combined. %with adversarial training.

In addition to reviewing the main attack and defense approaches related to 3D point cloud classification, we also presented the main datasets used in this field, as well as the most widely used victim models. Finally, we summarized the main challenges and outlined possible directions for future research in this field. We hope the article will be helpful to those entering the field of adversarial attacks on 3D point clouds and serve the current research community as a quick reference.


%Some potential future directions for research on adversarial attacks on 3D point clouds include optimizing attack methods by targeting only a subset of points in the point cloud and focusing on the local rather than global structure of the point cloud, as well as exploring the robustness of 3D point cloud classifiers to attacks that are specifically designed for 3D data rather than adapted from methods developed for 2D images.



%\section{Future Directions}
%\label{sec:Future_Directions}


%\textcolor{green}{Add point clouds with attributes, GSP, ...}


% % %--------------------Table1------------------------------------
% \begin{table*}
%     \caption{
%     {\bf Relationship between adversarial attacks and attack categories.}}
    
%     \centering
    
%     %\begin{tabular*}{\textwidth}{c|c|c c c c c c}
%     \begin{tabular}{ c  c  c  c  c c c}
%     \hline
    
%     %\multicolumn{1}{|c|}{Defenses}  & \multicolumn{7}{c|}{Attacks}\\
%     \bf Ref & \bf Attack Name & \multicolumn{4}{|c}{\bf Categories}\\
%     \hline
%     & Targeted/Non-targeted & Shift/Add/Drop/Transform & On-/Out-surface & Optimized/Gradient & Black-/White-box\\
    
    
%    \cite{xiang2019generating}  & Targeted & Shift & Out & Optimized & White\\
    
%     Add-CD \cite{xiang2019generating} &Targeted & Add & Out & Optimized & White \\
    
%     Add-HD \cite{xiang2019generating} & Targeted& Add & Out &Optimized & White\\
    
%     Shift-KNN \cite{tsai2020robust} & Targeted& Shift & On & Optimized\\
    
%     Drop-100 \cite{zheng2019pointcloud} & Non-Targeted & Drop & On& Gradient\\
    
%     Drop-200 \cite{zheng2019pointcloud} & Non-Targeted & Drop & On& Gradient\\

%     D \cite{hamdi2020advpc} & Targeted &  &  &  \\
%     D \cite{lee2020shapeadv} & Targeted &  &  & \\
%     D \cite{zhou2020lg} & Targeted &  &  & \\
%     D \cite{tsai2020robust} & Targeted &  &  & \\
%     D \cite{wen2020geometry} & Targeted &  &  & \\
%     D \cite{liu2019extending} & Non-Targeted &  &  & \\
%     D \cite{arya2021adversarial} & Non-Targeted &  &  & \\
%     D \cite{liu2020adversarial} & Non-Targeted &  &  &\\
%     D \cite{yang2019adversarial} & Non-Targeted &  &  & \\
%     D \cite{kim2021minimal} & Non-Targeted &  &  & \\
%     D \cite{zheng2019pointcloud} & Non-Targeted &  &  & \\
%     D \cite{} & Non-Targeted &  &  & \\
    
    
%     \hline
%     \end{tabular}
% \label{table:Categories}
% \end{table*}





%------------------
% % For tables use
% \begin{table*}
% % table caption is above the table
% \caption{Bechmark}
% \label{tab_Bechmarks}       % Give a unique label
% \centering
% \begin{tabular}{|c|c|c|}

% \hline\noalign{} 
% \multirow{}{}{Victim models}
% & PointNet\cite{qi2017pointnet} & \cite{wu20153d} ,\cite{hhope}  \\
% & PointNet++\cite{qi2017pointnet1}&\cite{wu20153d} ,\cite{hhope}  \\
% \hline\noalign{} 

% \multirow{}{}{Datasets}
% & ScanNet\cite{qi2017pointnet} & \cite{wu20153d} ,\cite{hhope}  \\
% & ModelNet++\cite{qi2017pointnet1}&\cite{wu20153d} ,\cite{hhope}  \\
% \hline\noalign{} 


% \end{tabular}
% \end{table*}
%------------------
% %--------------------Table1------------------------------------


\bibliographystyle{IEEEtranN}
\bibliography{References}




\EOD
\end{document}
