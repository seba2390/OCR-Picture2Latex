\section{Mathematical Model}
\label{sec:Model}
\vspace{-0.2em} %EXTRASPACE
Let us introduce some notation. Let $x_i$ denote the $i^{\mathrm{th}}$ entry of the vector $x\in\mathbb{R}^n$. Moreover, $x(i:q:(k-1)q+i)$ denotes the sub-vector in $\mathbb{R}^k$ formed by the entries of $x$ starting at index $i + qr$, where $r = 0, 1, \ldots, k-1$ and $i \in [q]$. In addition, $A(i:q:(k-1)q,l)$ represents the sub-vector constructed by selecting the $l^{\mathrm{th}}$ column of matrix $A$ and choosing the entries of the selected column as above. 


In~\eqref{quan_obs}, we assume that the matrix $A$ can be factorized as $A=DB$ where $D\in\R^{p\times n}$ is obtained by stacking $k'$ diagonal matrices with size $q\times q$. Moreover, $B\in\R^{q\times n}$ is a matrix which depends on the prior used for $x$. For instance, if $x$ is assumed to be a sparse vector in some orthobasis, then $B$ can be any matrix that supports sparse recovery; for instance, $B$ can satisfy the restricted isometry property (RIP)~\cite{CandesRIP}, or the null-space property (NSP)~\cite{foucart2013}. 

Also, in~\eqref{quan_obs} we assume that $C$ is another block diagonal matrix of size $m\times p$ formed by $k$ diagonal matrices with size $p\times p$ such that the diagonal entries in each of these blocks are specially chosen (see Section~\ref{sec:harmonic} below for more details). We assume that $p$ and $m$ are multiples of $q$ and $p$, respectively. 

For brevity, let us denote $\mod(\cdot,R)$ as $\mod(\cdot)$. Then, the model~\eqref{quan_obs} can be expanded as:
\begin{align}\label{Mainmodel}
y = Q\left(
\begin{bmatrix}
C^0 \\
C^1 \\
\vdots  \\
C^{k-1}
\end{bmatrix}
\mod\left(
\begin{bmatrix}
D^1 \\
D^2 \\
 \vdots  \\
D^{k'}
\end{bmatrix}
Bx
\right)\right) .
\end{align}

We define the following intermediate variables:
\begin{align}\label{mainabr}
u = \mod(DBx)\in\R^p, \ \ z = Bx\in\R^q,
\end{align}
As discussed in~\cite{SoltaniHegde_ICASSP16}, the block diagonal structure of $D$ and $C$ allows the signal reconstruction problem to be reduced to a sequence of \emph{decoupled} scalar estimation problems; such a decoupling enables the estimation of each entry of $z$ and $u$ independently of all other entries.

We also assume that the analog values of the input signal lies within a range known \emph{a priori}. 
We define $\Delta$ as the reference point for the quantization, and assume that the inputs to the quantizer are bounded within the region $[0,2\Delta]$. 
The function under consideration is the 1-bit quantization function, 
defined as follows:
\begin{align}\label{eq:quantfunc}
Q(u_i)= 
\begin{cases}
0,& \text{if } u_i\leq {\Delta}\\
1,              & \text{if } {\Delta} < u_i \leq {2\Delta} 
\end{cases}
\end{align}

For every element of the input to the quantizer, $u_i$, we measure multiple outputs $y_{i,0},y_{i,1}, \cdots ,y_{i,k-1}$, given by:
\begin{align}\label{eq:hmshort}
y_{i,j} = Q(c_{i,j}u_i) , ~~~~ j = 0,1,2,...,k-1.
\end{align}
with $c_0 =1$, and each subsequent $c_{i,j}$ is defined as :
\begin{align}\label{eq:hm}
c_{i,j} = 
\begin{cases}
\frac{k}{k-j},& \text{if } y_{i,0} = 0,\\
\frac{k}{k+j},              & \text{if } y_{i,0} = 1,
\end{cases} ~~~~ j = 1,2,...,k-1.
\end{align}
The underlying idea is to increase or decrease the value of $c_{i,j}u_i$ gradually and to detect the index $j^*$ for which $y_{i,j}$ changes its value for the first time. Using $j^*$, Interval containing $u_i$ can be determined. Here, the reason for choosing the values of $c_{i,j}$ in harmonic progression is to ensure that all such intervals corresponding to the different values of $j^*$ have equal widths. This fact is made clearer in Section~\ref{sec:harmonic}.

As the multipliers $c_{i,j}$ form a harmonic progression for both the cases, we call the proposed measurement scheme the \emph{harmonic multipliers method}. In order to express it as a linear transformation, these multipliers can be arranged into a block diagonal matrix $C$ of size $(kp) \times p$, for which the diagonal entry in $i^{th}$ row in block $C^j$ will be $c_{i,j}$, the multiplier corresponding to the input signal element $u_i$. 
%Now, the final measurement matrix $y$ is $(kp) \times 1$, containing $k$ blocks of size $p\times 1$, each representing the measurement vector $y^j$. 
The forward model can be written in the form of following equation:
\begin{align}
y = Q(Cu),
\end{align}
where $u$ is defined in~\eqref{mainabr}.


\section{Reconstruction Procedure}
%\vspace{-0.5em}%EXTRASPACE
To solve the inverse problem in~\eqref{Mainmodel}, we propose a three stage procedure that we call \textit{reconstruction from de-quantized modulo observations}, or \textit{RQM} for short. In the first stage, RQM estimates $u= mod(Ax)$ from vector $y$. Next, it uses the estimate, $\widehat{u}$ to produce an estimate $z$ in the second stage (say $\widehat{z}$). Finally, this estimate $\widehat{z}$ is used for recovery of the original $x$. The pseudocode of RQM is given as Alg.~\ref{alg:DMF}.  We now describe each of these stages in detail. 

\begin{algorithm}[t]
\caption{\textsc{RQM}}
\label{alg:DMF}
\begin{algorithmic}
\State\textbf{Inputs:} $y$, $D$, $B$, $C$, $k$, $k'$, $\Omega$, $s$
\State\textbf{Output:}  $\widehat{x}$
%\State $ k \leftarrow m/q$
\State \textbf{Stage 1: Harmonic dequantization}
\State $\widehat{u}\leftarrow \textsc{HMDequantization}(y,C,k)$
\State \textbf{Stage 2: Modulo recovery}
\State $\theta\leftarrow  \exp(i \widehat{u})$
\For {$l =1:q$}
\State $t \leftarrow D(l:q:(k'-1)q+l,l)$
\State $\phi \leftarrow \theta(l:q:(k'-1)q+l)$
%\State estimate $z_l$ from $u = \exp(jtz_l)$ using 
\State $\widehat{z_l} = \argmax_{\omega\in\Omega}|\langle y,\psi_{\omega}\rangle|$
\EndFor
\State $\widehat{z} \leftarrow [\widehat{z_1},\widehat{z_2}\ldots,\widehat{z_q}]^T$
\State \textbf{Stage 3: Sparse recovery}
%\State $\varepsilon \leftarrow \mathcal{O}(\frac{1}{|T|})$
\State $\widehat{x} \leftarrow \textsc{CoSaMP}(\widehat{z},B,s)$
\end{algorithmic}
\end{algorithm}

\subsection{Harmonic dequantization}
\label{sec:harmonic}
We first attempt to recover $u$. Based on the value of $Q(u_i)$, we can know the interval in which $u_i$ lies, which can either be $[0,\Delta]$ or $[\Delta,2\Delta]$. For $u_i \in [0,\Delta]$, the multipliers are designed in such a way that with each multiplication, we gradually increase the value of $c_{i,j}u_i$ until it becomes greater than $\Delta$ at $j=j^*$ to give $Q(c_{i,j^*}u_i)=1$. 

Using $j^*$, we can decide the interval of $u_i$ as follows. %For $u_i$ such that $y_{i,0}=0$, if $y_{i,j}$ changes from $0$ to $1$ at $j = j^*$ for the first time, 
Equation \eqref{eq:hmshort} gives:
\[
y_{i,{j^*-1}} = Q(c_{i,j^*-1} u_i) = \left \lfloor{\dfrac{k u_i}{(k-j^*+1) \Delta}}\right \rfloor = 0.
\]
%$$\implies u < \Delta \dfrac{(k-j^*+1)}{k}$$
Similarly, 
\[
y_{i,{j^*}} = Q(c_{i,j^*} u_i) = \left \lfloor{\dfrac{k u_i}{(k-j^*) \Delta}}\right \rfloor = 1.
\]
%$$\implies u \geq \Delta \dfrac{(k-j^*)}{k}$$
Combining the above relations, we infer that:
\begin{align}
\label{eq:hminter1}
\Delta \dfrac{(k-j^*)}{k} \leq u_i < \Delta \dfrac{(k-j^*+1)}{k}.
\end{align}
Similarly, for $u_i$ with $y_{i,0}=1$, through each multiplication with $c_{i,j}$, the value of $c_{i,j} u_i$ decreases gradually and becomes less than $\Delta$ for $j=j^*$ for the first time. 
\begin{align}
\label{eq:hminter2}
\Delta \dfrac{(k+j^*-1)}{k} \leq u_i < \Delta \dfrac{(k+j^*)}{k}.
\end{align}
\begin{algorithm}[t]
	\caption{\textsc{HMDequantization}}
	\label{alg:HM}
	\begin{algorithmic}
		\State\textbf{Inputs:} $y$, $C$, $k$
		\State\textbf{Output:}  $\widehat{u}$\\
		$n \leftarrow length(y)/k$
		%\State $ k \leftarrow m/q$
		\For {$l =1:n$}
		\If {$y_l = 0$}
		\State $t \leftarrow y(l+n:n:(k-1)n+l,1)$
		\State $j^* \leftarrow \min_{j \in \{1,2,...,k-1\}} \text{such that } t_j = 1$
		\State $\widehat{u}_{l} \leftarrow v \sim U[\Delta\frac{k-j^*}{k},\Delta
		\frac{k-j^*+1}{k}]$
		\ElsIf {$y_l = 1$}
		\State $t \leftarrow y(l+n:n:(k-1)n+l,1)$
		\State $j^* \leftarrow \min_{j \in \{1,2,...,k-1\}} \text{such that } t_j = 0$
		\State $\widehat{u}_{l} \leftarrow v \sim U[\Delta\frac{k+j^*-1}{k},\Delta\frac{k+j^*}{k}]$
		\EndIf
		\EndFor
		
	\end{algorithmic}
\end{algorithm}
Equations \eqref{eq:hminter1} and \eqref{eq:hminter2} provide us the interval on the real line that contains $u_i$. To remove bias, a random real number is chosen from this interval as the final estimate $\widehat{u_i}$. The width of this interval is $\delta = \frac{\Delta}{k}$, which is same for every interval corresponding to different values of $j^*$ owing to harmonic design of the multipliers. The value of $\delta$ (and consequently, the estimation error) can be made sufficiently small by increasing the value of $k$. For the estimate $\widehat{u_i}$ to lie within an $\epsilon \Delta$ neighborhood of $u_i$, the minimum value required for $k$ can be calculated as 
%\begin{align}
$k_{\text{req}} = \left \lceil{\frac{1}{\epsilon}}\right \rceil$.
%\end{align}
%As the quantized measurements occupy only 1 bit of storage for each pixel, increasing number of measurements doesn't affect the sample complexity by large.
%For the case when the range of the input is $[\Delta - \alpha, \Delta + \beta]$, %the number of measurements required is different for the points lying on the either side of the reference point $\Delta$. 
%an analogous procedure can be developed by defining two separate $k$ values $k_{\alpha}, k_{\beta}$, one for each interval. 


In this paper, we assume that the first multiplier $c_0 =1$, and it is possible to decide the appropriate $c_j$'s by looking at the value of the first measurement $y_{i,0}$ in Eq\ \eqref{eq:hm}. 
In case purely non-adaptive measurements are desired, a similar approach can be followed by acquiring $2k-1$ measurements with each possible value of $c_j$ specified by both cases of~\eqref{eq:hm}. The pseudocode of this stage is given as Alg~\ref{alg:HM}.

\vspace{-1.2em}%EXTRASPACE
\subsection{Modulo recovery}\label{ToneEst}
	\vspace{-0.5em}%EXTRASPACE
The output of \textsc{HMDequantization} acts as input for the modulo recovery stage. The goal of this stage is to find an estimate for the vector $z$. There are several different ways of doing this, including the multi-shot UHDR method of~\cite{ICCP15_Zhao}. Here, we describe a novel approach, based on the MF-Sparse algorithm of~\cite{SoltaniHegde_ICASSP16}. We assume that the entries of $z$ belong to some bounded set $\Omega \in R$. Fix $l \in [q]$ and form $\theta =  \exp(i \widehat{u})$. Let {$t = D(l:q:(k'-1)q+l,l)$ and $\phi = \theta(l:q:(k'-1)q+l)$}, 
%h = e(l:q:(k-1)q+l)$} 
which are vectors in $\mathbb{R}^{k'}$. Thus, we have the following model:
\begin{align}\label{tonmodel}
\phi = \exp(i z_l t) \, .
\end{align}
In the above model, $\phi$ can be interpreted as a set of time samples of a complex-valued signal with frequencies $z_l \in \Omega$, measured at time locations $t$. As a result, we can independently recover $z_l$ for $l=1, \ldots, q$ by solving a least-squares problem~\cite{eftekhari2013matched}:
\begin{align}
\label{OptmExp}
\widehat{z_l} = \underset{v \in \Omega}{\argmin}~\|\phi - \exp(i \, vt )\|_2^2 = \underset{v \in \Omega}{\argmax}~\left|\langle \phi,\psi_{v}\rangle\right|,
 \end{align}
for all $l=1,\ldots,q$, where  $\psi_{v}\in\mathbb{R}^{k'}$ denotes a \emph{template vector} given by $\psi_{v} = \exp(j t v)$ for any $v \in \Omega$. The solution of this optimization problem is equivalent to performing a \emph{matched filter} from irregularly spaced samples. Numerically, the optimization problem in~\eqref{OptmExp} can be solved using a grid search over the set $\Omega$, and the resolution of this grid search controls the running time of the algorithm. For fine enough resolutions, the estimation of $z_l$ is more accurate, at the expense of increased running time. This issue is also discussed in~\cite{eftekhari2013matched} and~\cite{eldarxampling,tangcsoffgrid,chioffgrid} have proposed more sophisticated spectral estimation techniques. %After obtaining all the estimates $\widehat{z_l}$'s, we stack them in a vector $\widehat{z}$. 

In~\eqref{tonmodel}, the vector $\theta$ is modeled in terms of complex exponentials. As discussed in~\cite{SoltaniHegde_ICASSP16}, we can equivalently use a real-valued sine function. That is, the vector $\phi$ can be defined as:
\begin{align}
\label{eq:realsine_obs}
\phi = \sin(i z_l t),
\end{align}
Similar to the complex case, we estimate $z$ by solving~\cite{SoltaniHegde_ICASSP16}:
\begin{align*}
\label{OptmSin}
\widehat{z_l} &= \underset{v \in\Omega}{\argmin}~\|\phi - \sin(v t)\|_2^2 = \underset{v \in \Omega}{\argmax}~\left( 2\left|\langle \phi,\psi_{v}\rangle\right| - \|\psi_{v}\|_2^2\right),
\end{align*}
for $l=1,\ldots,q$ and $\phi$ as defined above and $\psi_v =\sin(tv)$. 
%{Also, $\psi_{v} = \sin(tv)$ for any $v \in \Omega$}. Now it remain to estimate the original signal $x$ from $\widehat{z}$ obtained in the second stage. This is done by the final stage, Sparse recovery.
	\vspace{-1.2em}%EXTRASPACE
\subsection{Sparse recovery}
	\vspace{-0.5em}%EXTRASPACE
Finally, we estimate the original signal $x$ from $\widehat{z}$ obtained as the output of the second stage.
Note that the use of sparse recovery here is generic, and we could in principle use any other prior model of relevance to the specific imaging application. Since we assume that matrix $B$ in~\eqref{Mainmodel} supports stable sparse recovery and the underlying signal $x$ is $s$-sparse, we can use any generic sparse recovery algorithm to estimate $x$. In our experiments, we chose to use the CoSaMP algorithm \cite{cosamp} due to its ease and speed. Hence, we take $\widehat{z}$ from previous stage and run CoSaMP to obtain the final estimation, $\widehat{x}$.



