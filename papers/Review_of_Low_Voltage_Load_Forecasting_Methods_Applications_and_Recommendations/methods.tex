\section{Low-Voltage Load Forecasting Methods}
\label{sec_forecasting_method_main}

This section will focus on the forecast models and techniques including the inputs and explanatory variables used in LV level forecasting methods found in the literature. It starts with a focus on the explanatory variables that have been used within the surveyed approaches. The remainder of this section structures the studied approaches by the type of method used. Table~\ref{tab:methodoverview} gives an overview of the surveyed methods of this section for reference by the aggregation level and the forecasting horizon. For the individual level, we distinguish between residential and commercial/industrial customers (or mixed if both are included or not specifically distinguished). For aggregate level we split these into those directly applied to substation data, or those which are the aggregation of individual consumers. We classify forecasting horizons up to a few hours as \emph{very short-term}, day-ahead and up to a few days as \emph{short-term}, \emph{medium term} from weeks to months and \emph{long-term} from a year ahead and up.

\begin{table}[]
	\scriptsize
	\caption{Overview of reviewed LV Load Forecasting Approaches by Aggregation Level and Forecasting Horizon.} \label{tab:methodoverview}
	\begin{tabular}{p{.06\linewidth}p{.08\linewidth}p{.08\linewidth}p{.46\linewidth}p{.29\linewidth}}
		\toprule
		\textbf{Level} & \textbf{Category} & \textbf{Horizon} & \textbf{Method} & \textbf{Reference} \\ 
		\midrule
		\multirow[t]{32}{*}{Individual} & \multirow[t]{11}{*}{Residential} & \multirow[t]{1}{*}{Very} & Statistical and time series Approaches & \cite{Ghofrani2011smb} \\
		&  & \multirow[t]{4}{*}{Short-term} & Machine Learning and other Artificial   Intelligence Approaches & \cite{Singh2018bdm, mehdipour2020slf, liu2019tsh, estebsari2020srl, kong2017str, Elvers2019spl, voss2018residential, voss2018adjusted} \\
		&  &  & Comparing Methods & \cite{Cordova2019cet, Cerquitelli2019esm, nawaz2019aaf, Humeau2013elf} \\
		&  &  & Probabilistic Forecasting & \cite{wang2019pil, yang2020bdl, wang2019pil} \\
		&  &  & Hybrid, Combination and Ensemble   Approaches & \cite{dong2016ahm, ai2019hpd} \\
		&  & \multirow[t]{5}{*}{Short-term} & Statistical and time series Approaches & \cite{kipping2016mad, Litjens2018aof, Arora2016fes, Chaouch2014cio, dinesh2020rpf, Nugraha2018ldp} \\
		&  &  & Machine Learning and other Artificial   Intelligence Approaches & \cite{Singh2018bdm, sousa2012atr, bessani2020mhv, shah2020stm, Elvers2019spl, Shi2017dlf, voss2018residential, Haben2014ane, voss2018adjusted} \\
		&  &  & Comparing Methods & \cite{Cordova2019cet, nawaz2019aaf, gajowniczek2014ste, Humeau2013elf} \\
		&  &  & Probabilistic Forecasting & \cite{Arora2016fes, Arora2016fes, pinto2017mpf, gerossier2018rda} \\
		&  &  & Hybrid, Combination and Ensemble   Approaches & \cite{dong2016ahm, Kiguchi2019pil} \\
		&  & Long-term & Machine Learning and other Artificial Intelligence Approaches & \cite{Singh2018bdm} \\
		\cmidrule(lr){2-5}
		& \multirow[t]{11}{*}{Mixed} & \multirow[t]{1}{*}{Very} & Machine Learning and other Artificial Intelligence Approaches & \cite{Hosein2017lfu, desilva2011ipc} \\
		&  & \multirow[t]{2}{*}{Short-term} & Comparing Methods & \cite{mirowski2014dfi} \\
		&  &  & Probabilistic Forecasting & \cite{Wang2019cpl, Yang2019del} \\
		&  & \multirow[t]{5}{*}{Short-term} & Statistical and time series Approaches & \cite{aprillia2019oda, Bhattacharyya2020sgd} \\
		&  &  & Machine Learning and other Artificial   Intelligence Approaches & \cite{moon2019aca, pati2020mfc, khan2020tee} \\
		&  &  & Comparing Methods & \cite{mirowski2014dfi} \\ 
		&  &  & Probabilistic Forecasting & \cite{Taieb2020hpf, chaouch2015rcq, Yang2019del, taieb2016fui} \\
		&  &  & Hybrid, Combination and Ensemble   Approaches & \cite{Amato2021fhr, taieb2016fui} \\
		&  & Med.-term & Hybrid, Combination and Ensemble Approaches & \cite{massidda2019smf} \\
		&  & \multirow[t]{2}{*}{Long-term} & Machine Learning and other Artificial Intelligence Approaches & \cite{fiot2018edf} \\
		&  &  & Hybrid, Combination and Ensemble   Approaches & \cite{massidda2019smf} \\
		\cmidrule(lr){2-5}
		& \multirow[t]{1}{*}{{Commercial/}} & \multirow[t]{1}{*}{Very} & Statistical and time series Approaches & \cite{ullah2018apm} \\
		& \multirow[t]{9}{*}{{Industrial}} & \multirow[t]{3}{*}{Short-term} & Machine Learning and other Artificial   Intelligence Approaches & \cite{Feng2020rda} \\
		&  &  & Comparing Methods & \cite{grolinger2016eff} \\
		&  &  & Other methods & \cite{xu2020ats} \\
		&  & \multirow[t]{4}{*}{Short-term} & Statistical and time series Approaches & \cite{Ding2015nms, ullah2018apm, Lee2013asm} \\
		&  &  & Machine Learning and other Artificial   Intelligence Approaches & \cite{ribeiro2018tls, petrosanu2019ddv, Jurado2015hme, jurado2017fir} \\
		&  &  & Hybrid, Combination and Ensemble   Approaches & \cite{ruiz-abellon2018lff, Jung2020bem} \\
		&  &  & Other methods & \cite{xu2020ats} \\
		&  & Med.-term & Machine Learning and other Artificial Intelligence Approaches & \cite{gao2020dla} \\
		&  & Long-term & Statistical and time series Approaches & \cite{tsekouras2006anl} \\
		\midrule
		\multirow[t]{21}{*}{Aggregate} & \multirow[t]{12}{*}{Substation} & \multirow[t]{1}{*}{Very} & Statistical and time series Approaches & \cite{Ghofrani2011smb, Bracale2013ABB} \\
		&  & \multirow[t]{3}{*}{Short-term} & Machine Learning and other Artificial   Intelligence Approaches & \cite{zhao2020onp, lourenco2012stl, wang2013acv} \\
		&  &  & Comparing Methods & \cite{idowu2016aml, mirowski2014dfi} \\
		&  &  & Hybrid, Combination and Ensemble   Approaches & \cite{Cao2020hed, sulandari2016fel, tajer2017lfv} \\
		&  & \multirow[t]{6}{*}{Short-term} & Statistical and time series Approaches & \cite{Haben2019stl, aprillia2019oda, vincenzo2020cml, borges2020etm, Goude2014lsa, Hayes2015acs, Haben2019stl, espinoza2005stl} \\
		&  &  & Machine Learning and other Artificial   Intelligence Approaches & \cite{bersani2006mol, bersani2006mol, sousa2012atr, ahmad2019dlf, ding2016nnb, stephen2020ngr, fidalgo2005lfp, naeem2020stl, zufferey2020psf, Abreu2018mlf, chen2018dfl} \\
		&  &  & Comparing Methods & \cite{idowu2016aml, mirowski2014dfi, Yunusov2018sss} \\
		&  &  & Probabilistic Forecasting & \cite{Haben2019stl, bikcora2018dfo, kodaira2020oes, zufferey2020psf, bikcora2018dfo} \\
		&  &  & Hybrid, Combination and Ensemble   Approaches & \cite{bogomolov2016ecp, hashim2019anl, Bennett2014flv, sun2015aea, grmanova2016iel, nespoli2020hdf, kodaira2020oes} \\
		&  &  & Other methods & \cite{mcqueen2004mcs} \\
		&  & \multirow[t]{2}{*}{Long-term} & Statistical and time series Approaches & \cite{Friedrich2014mfu, Goude2014lsa} \\
		&  &  & Machine Learning and other Artificial   Intelligence Approaches & \cite{Ye2019adb} \\
		\cmidrule(lr){2-5}
		& \multirow[t]{9}{*}{Aggregation} & \multirow[t]{1}{*}{Very} & Statistical and time series Approaches & \cite{perfumo2014mbe} \\
		&  & \multirow[t]{2}{*}{Short-term} & Machine Learning and other Artificial   Intelligence Approaches & \cite{lin2020ecp, Elvers2019spl, voss2018residential, voss2018adjusted} \\
		&  &  & Comparing Methods & \cite{Humeau2013elf} \\
		&  & \multirow{6}{*}{Short-term} & Statistical and time series Approaches & \cite{macmackin2019mad, dinesh2020rpf, Marinescu2013RED} \\
		&  &  & Machine Learning and other Artificial   Intelligence Approaches & \cite{moon2019aca, chen2019daa, sun2019ubd, wu2020ast, Elvers2019spl, voss2018residential, sousa2014slf, voss2018adjusted, konjic2005fis} \\
		&  &  & Comparing Methods & \cite{garulli2015mat, Humeau2013elf} \\
		&  &  & Probabilistic Forecasting & \cite{Taieb2020hpf, taieb2016fui} \\
		&  &  & Hybrid, Combination and Ensemble   Approaches & \cite{Laurinec2019due, Amato2021fhr, taieb2016fui} \\
		&  &  & Other methods & \cite{zhou2019pse} \\ \bottomrule
	\end{tabular}
\end{table}


\subsection{Explanatory Variables}
\label{subsec:expl_var}
Selecting the correct inputs (predictors/features) is almost as important as choosing the most appropriate models when developing a forecasting method. The selection of the most relevant set of explanatory variables is imperative for both the descriptive and predictive performance of a model. The literature review confirmed many of the most common choices of explanatory variables used in forecast models, as well as some novel features. While the impact of weather on the load at the national level is well studied, there is a need to understand the influence of weather predictions at the LV level, for both short- and medium-term forecasts. In this section, we review some of the most commonly used explanatory variables used for forecasting at the LV level.

The majority of models use some form of historical or lagged observations in the forecasts \cite{sousa2012atr, gajowniczek2014ste, Yunusov2018sss, jurado2017fir}. LV level demand is similar to higher voltage demand in that there is often daily, weekly and annual seasonalities that needs to be accommodated to generate forecasts. This also includes using calendar variables, \cite{Cerquitelli2019esm,ahmad2019dlf,moon2019aca, sousa2012atr}. In addition to lagged load values, Louren{\c{c}}o and Santos \cite{lourenco2012stl} used derivative terms (differences between the adjacent values), which was not found to be common in the literature review. 

\subsubsection{Meteorological variables}

By far the most common explanatory inputs are meteorological variables \cite{Jung2020bem}, in particular temperature \cite{hoverstad2015stl, kipping2016mad}. Temperature is also combined with other variables such as  humidity \cite{Marinescu2013RED}, solar irradiance \cite{dong2016ahm}, solar irradiance and wind speed \cite{larsen2017dre}, or dry-bulb temperature, dew point, precipitation rate, wind chill and humidity index \cite{lusis2017str}. One paper Zhou et al. \cite{zhou2019pse} combines temperature, precipitation, pressure and wind speed. Additionally, the weather variables can be combined to produce joint variables as in \cite{ding2016nnb} which combines lagged, calendar and temperature and the cross product of these features for capturing non-linearities.
If the raw meteorological variables or their lagged counterparts are not used as inputs then derived features are used instead, for example cooling degree days (CDD) and heating degree days (HDD), which measure temperature exceedances from a given threshold value. Commonly an exponentially smoothed version of the temperature is used as in \cite{larsen2017dre}. This can help take into account the delayed effect of temperature on demand.

The interaction between explanatory variables or their effect (whether positive or negative) in forecast models has only been touched upon in a few of the papers considered in this review. Lusis et al. \cite{lusis2017str} find that calendar effects have less predictive power when used with the weather, daily and weekly seasonality. Fidalgo and Lopez \cite{fidalgo2005lfp} state that experiments with temperature features did not strongly affect the forecast accuracy and were discarded. Furthermore, Haben et al. \cite{Haben2019stl} consider several models with and without temperature data (both forecast and actuals) and they found no, or negative effect on the short term forecasts accuracy for both point and probabilistic forecasts. In contrast, Bennett et al. \cite{Bennett2014flv} incorporate temperature and relative humidity (RH) for generating point day-ahead LV transformer level forecasts, and report that temperature accounted for about half the variation in the load. Transformed values of the temperature (squared temperature) and interaction effects (RH with temperature) were also considered.


This highlights a common feature of the models which use meteorological explanatory variables. Ideally, in practice, the predicted load should only rely on using the predictions of weather variables as explanatory variables (so-called \textit{ex-ante forecasts}), which are preferably obtained from several weather stations in a geographic neighbourhood. However, a vast majority of the reviewed papers employ the actual weather observations (\textit{ex-post forecasts}). It can be thus expected that the corresponding forecast accuracy is over-optimistic. To the best of our knowledge, other than Haben et al. \cite{Haben2019stl} there are only two other papers found in this review using weather forecast inputs \cite{prakash2018rbe, nespoli2020hdf}.  In \cite{nespoli2020hdf}, the authors considered a large number of different variables including temperature, global horizontal and normal irradiance, relative humidity, pressure and wind speed and direction. In \cite{prakash2018rbe} the authors developed a Gaussian Process regression-based 10-minute ahead forecast for the aggregated consumption of ten buildings using both actual and predicted temperature. In this case, using predicted temperature produces smaller errors than using actual for the longer-term forecasts. Although only a handful of papers include weather predictions as model inputs for LV load forecasting \cite{Haben2019stl, prakash2018rbe, nespoli2020hdf}, it is worth noting that these papers only employ point estimates and not weather ensemble predictions, thereby ignoring the underlying uncertainty.

Since these papers consider LV level, they require more local but unknown weather inputs. The Irish CER dataset~\cite{Commission2012csm} is a common dataset used despite the locations of the households being unknown. Hence many authors use an average over the entire country or use the weather of a major city such as Dublin as representative\cite{yang2020bdl}.

\subsubsection{Econometric variables}

Econometric variables or information about the types and number of consumers on an LV network are very common inputs \cite{Bunnoon2013mcc}. These variables are typically used for longer-term forecast models, for example, \cite{tsekouras2006anl} uses a whole host of information about manufacturing, number of consumers and gross national product to produce mid-term length forecasts. Additional features from surveys, such as the number of residents, social class and electricity use for heating and cooling are used in \cite{bessani2020mhv}. In a similar way, a range of different variables were considered in \cite{Kiguchi2019pil,kipping2016mad}, including gender, age group, social class, and the number of other residents.  The number of customers on an LV substation and the monthly energy consumption is used in \cite{konjic2005fis}. In \cite{idowu2016aml}, the authors used the substation internal state in addition to lagged load, temperature and temporal features, which proved to be significant in improving the forecast.

Inputs from other monitored customers are also common. For forecasting (and imputing) the load of one substation~\cite{borges2020etm} uses neighbouring substation's data to improve forecasts.  Ziekow et al.~\cite{ziekow2013tpo} analyze how the availability of submetered data impacts very short term forecasts (15-minute and 1-hour-ahead). They generally find improvements varied among three households from 5\% to above 30\%. They also find that higher resolution produces greater accuracy improvements in very short term forecasts (15 min ahead).  

\subsubsection{Novel variables}
\label{subsec:novel_var}
Besides weather, calendar, and econometric explanatory variables, other explanatory variables are also employed for modelling LV load. The following is a list of some further examples of more novel explanatory variables which were identified in the literature review:
\begin{itemize}
	\item For forecasting the change in demand side response, in~\cite{garulli2015mat}, active  demand (requested change in demand) is used as an explanatory variable.
	\item For sites with photovoltaics (PV), a net load forecast in~\cite{chu2017nlf} was produced using the red-blue ratios in sky images to derive features for inputs to the model. 
	\item In \cite{Cordova2019cet} electricity data is combined with transport data.
	\item To produce forecasts for an event-organizing venue, event type, schedule, day-of-the-year, and seating configurations are used as inputs in~\cite{grolinger2016eff}.
	\item In~\cite{bogomolov2016ecp} electricity data is combined with mobile usage data. Interestingly, the authors do not use any energy data as predictors. Also, although daily and weekly seasonalities were present in the data and coded into the feature space, these features are not present in the final feature list. Finally, the three different areas that have different demand patterns are not coded into the features either. 
	\item  Forecasting in terms of the customer baseline load (CBL), i.e., the consumption level that a customer would have have consumed in the absence of any demand response program is considered in ~\cite{pati2020mfc} using the CER Irish smart meter data.
\end{itemize}


\subsection{Statistical and Time Series Approaches}  
\label{secStatandTS}
Statistical and time series approaches are one of the most commonly used models for short term load forecasting. The majority of them are linear in their parameters and include multiple linear regression (MLR), exponential smoothing and traditional time series approaches such as autoregressive and moving averages models (ARIMA, AR, etc.). However, many nonlinear approaches such as nonlinear regression, and generalised additive models have become more popular in recent years. 

\subsubsection{Linear Regression}
Regression estimates the relationship between one or more predictors (explanatory variables)  and the variable one wants to predict (dependent variable), in our case, the load at LV level. Multiple linear regression (MLR) assumes that the relationship between explanatory and dependent variables can be adequately modelled using a linear modelling approach (i.e., no deviations from the Gauss-Markov assumptions), whereby the model parameters are typically estimated based on the minimization of residual sum of squared errors. Some of the simplest linear models are applied by \cite{Litjens2018aof}, including seasonal persistence models and simple averages. Although simple in their formulation, such models can serve as sophisticated benchmarks during the out-of-sample validation of more sophisticated models \cite{Haben2019stl}.

Linear models are popular because they are easy to interpret and solve, and despite the constraints on linear coefficients, they can model a wide diversity of behaviours, including non-smooth and nonlinear relationships. In \cite{macmackin2019mad}, an MLR model is used for forecasting aggregate smart meter data from a utility in Canada. Dummy variables are included for types-of-day, and change points define heating and cooling degree days/hours. Haben et al.~\cite{Haben2019stl} also consider dummy variables for modelling types of the hour and day, and include Fourier components to model annual seasonalities. In \cite{Ding2015nms}  day-ahead forecasts are generated for a medium voltage/low voltage (MV/LV) substation, using a regression-based model with Fourier components, however, the results are compared with only a na\"{\i}ve benchmark based on a random walk model. The authors in \cite{aprillia2019oda} introduce an MLR method integrated with discrete wavelets transform, to produce a day ahead hourly load forecast at both the system and end-users levels. This model uses both the weather and lagged features and is benchmarked against multiple methods including standard MLR, Artificial Neural Networks (ANN), Autoregressive moving average model with external variable (ARMAX), and Support vector regression (SVR). 

Linear models are often coupled with meteorological data. Borges et al.~\cite{borges2020etm} use linear models with different subsets of features for short-term forecasting and missing data imputation for substation data. Their model uses historic values, meteorological data and neighbouring substation data. Bhattacharyya and Bhattacharyya (2020) \cite{Bhattacharyya2020sgd} forecast hourly load profiles for demand-side management using an MLR model with temperature, heating and cooling degree days as inputs. 
Haben et al.~\cite{Haben2019stl} incorporated temperature observations and forecasts into their linear models via a third-degree polynomial. In \cite{kipping2016mad} two MLR load forecasting models are developed, based on daily and hourly mean values of outdoor temperature, for a household in Norway. 
The authors report that outdoor temperature, dwelling group, floor space, and number of residents are the most important variables required for modeling hourly electricity consumption in dwellings with electric heating. Also, the model with HDD achieve slightly higher accuracy. 

In \cite{vincenzo2020cml}, the authors study a micro-grid that consists of a block of three offices, PV generation, a storage system and three smart charging electric-vehicles (EV) stations. The idea is to find the optimal operation of the integrated energy systems in order to reduce the peak. 

To forecast the building consumption, an MLR is used and feature selection is performed using a genetic algorithm 
on lagged load values, e.g. 1-3 days before, and weather variables.

Although mostly solved by least-squares estimation the linear state model (linear regression) can also be solved using a Kalman filter as in \cite{Ghofrani2011smb} where the household demand is split into a deterministic part (modelled by a 10-degree polynomial) and the remaining random part modelled as a linear Gaussian-Markov process. Another use of a Kalman filter was presented in  \cite{perfumo2014mbe} to generate probabilistic hour ahead forecasts of the aggregated demand of 70 households. 




\subsubsection{Time Series Models}
\label{sec:tsmodel}
Time series methods, including autoregressive (AR), moving averages (MA), autoregressive integrated moving average (ARIMA), and those with exogenous inputs (ARX, ARIMAX etc.) are commonly used for demand forecasting. However, despite the daily and weekly seasonality of demand time series the seasonal version of these models (e.g. SARIMA) appear to be used less.

In \cite{Marinescu2013RED} a few models are compared, including ANN, fuzzy logic and wavelet neural networks, with the ARIMA model performing the best. In \cite{Nugraha2018ldp}, an ARIMA model is used for short-term (one hour-ahead and one day-ahead) forecasting of smart electricity meter data for building energy management systems. For the shorter-term forecasts, they use an online ARIMA. An ARIMA model is also used in \cite{Lee2013asm} for forecasting electricity for public school buildings, using historical load and temperature, to assist building management systems. They use variable base degree day/hour regression models combined with ARIMA to access energy efficiency, predict energy consumption, and detect energy usage anomaly.

While ARIMA models are often used to forecast the time series, in \cite{Bracale2013ABB} an AR(1) process is deployed to forecast the mean parameter of a Gaussian distribution of a Bayesian model. The standard deviation is similarly updated but with a Gamma distribution assumed.


Espinoza et al. \cite{espinoza2005stl} use a unified modelling framework based on periodic autoregressive (PAR) models for forecasting and clustering load profiles using data from 245 HV and LV substations from the Belgian National Grid Operator Elia.


\subsubsection{Nonlinear regression, exponential smoothing, and other models}

Although standard linear regression and time series models have been successfully applied to demand forecasts at all levels of the LV network, nonlinear regression models are also considered. Nonlinear models are very versatile but may be more prone to overfitting. This can be mitigated with cross-validation and other specific methods, such as model penalties in generalised additive models to tune smoothness.   

Hayes et al. \cite{Hayes2015acs} use a nonlinear autoregressive exogenous (NARX) model to forecast smart meter loads, which are shown to outperform both traditional ARX models and a neural network model. A nonlinear multivariate regression is used by Tsekouras et al.  \cite{tsekouras2006anl}, they select models based on testing multiple combinations of nonlinear functions to produce a medium-term forecast. The authors in \cite{Friedrich2014mfu} propose linear and multiplicative, nonlinear trend regression models to generate medium-term load forecasts (up to a year ahead), using substation-level data and weather data. The novelty of this work is regarding an estimate of the fraction of electricity load that can be attributed to cooling, which is a major consumption factor in the United Arab Emirates. A feature of this model is the use of dummy variables to differentiate Fridays/Holidays, Saturdays and Ramadan days from workdays.

Nonparametric and semi-parametric approaches can be beneficial in that they make fewer assumptions regarding the underlying data generating process than the fully parametric models. Chaouch \cite{Chaouch2014cio} uses a functional wavelet-kernel approach with clustering to forecast 2000 households from the Irish smart meter trial~\cite{Commission2012csm}. Goude et al. \cite{Goude2014lsa} generate short- and medium-term load forecasts for 2200 distribution network substations in France using a semi-parametric additive model. Although additive models have been quite successful at higher voltages (e.g. winning the GEFCOM 2014 challenge) to the authors' knowledge this is the first example of them being applied to low voltage demand. The authors state that modelling the middle term trend is quite challenging, and they did not have access to relevant commercial or sociological variables for modelling the trend in this study.

Although relatively simple linear models, exponential smoothing methods, which put more weight on recent past loads than older data, have been shown to be quite powerful. The double seasonal exponential smoothing method, also know as Holt-Winters-Taylor (HWT),  was one of the best performing in generating both point and probabilistic estimates against a multitude of methods as shown in \cite{Haben2019stl} and \cite{Arora2016fes}. 

In \cite{dinesh2020rpf} the authors use a bottom-up approach to predict load at the household and micro-grid level from individual appliances. The novelty of this paper is that statistical relationships between appliances are modelled (time-of-day probabilities and state duration probabilities). Ullah et al. \cite{ullah2018apm} also use a statistical approach by employing  Hidden Markov Models (HMM) to predict the consumption of residential buildings in Korea. The energy data for each floor is transformed into a floor occupancy sequence which are the observed values of the HMMs.


\subsection{Machine Learning and other Artificial Intelligence Approaches} % GG, MV
\label{sec:machine-learning}

The advances in the machine learning domain are also reflected in the LV load forecasting domain. ANN were introduced in the 90s and have been a popular method ever since. However, other methods based on nearest neighbours, regression trees, and kernels have also been popular over the past decades. With the recent advances of deep learning in other areas, many deep approaches have been proposed for LV load forecasting. 

\subsubsection{Nearest Neighbors}

The k-nearest neighbours (kNN) algorithm can be used for nonparametric regression. It is a simple method that often functions as a robust benchmark and is therefore included in many comparative studies (see Section~\ref{sec:comparing_methods}). Voss et al.~\cite{voss2018adjusted} show that at the household-level where load profiles are somewhat intermittent, that kNN can be adjusted to minimise the adjusted error measures proposed in ~\cite{Haben2014ane} (see Section~\ref{sec:application_specific}). As shown in Section~\ref{section_prob}, it can also be used for probabilistic forecasting by optimally weighting the k nearest neighbors~\cite{zufferey2020psf}. De Silva et al.~\cite{desilva2011ipc} propose an online learning algorithm, called Incremental Pattern Characterization Learning, based on a data stream mining algorithm and is related to nearest-neighbour methods. Considering the load forecasting problem as an incremental learning problem, it mitigates some weaknesses of static machine learning (ML) approaches by better handling concept shifts in the load data (e.g. expanding grid, changing customers or infrastructure, etc.).

\subsubsection{Regression Trees} \label{sec:trees_and_forests}

Regression trees (RT) are a form of decision trees where the target variable takes a continuous set of values. Compared to other machine learning models, regression trees can still be interpretable if they maintain a reasonable size. Chen et al.~\cite{chen2018dfl} investigate day-ahead peak load forecasting at the feeder-level using a parametric version of RT, Bayesian additive regression trees (BART), and report that BART generates more accurate point forecasts compared with linear regression, SVMs, and composite kernel methods based on Gaussian process regression.


\subsubsection{Kernel-based Learning Approaches}

A Gaussian process (GP) is a stochastic process based on lazy (just-in-time) learning and a kernel function that can also be used for probabilistic estimates based on a Gaussian distribution. Lourenco et al.~\cite{lourenco2012stl} use GP regression for an hour ahead load forecasting of three substations using the variance for estimating forecast confidence intervals. In \cite{Ye2019adb} a hierarchical approach for spatial load forecasting is proposed, to assist medium and low-voltage planning. They employ load, geographic, meteorological, economic and spatial data explanatory variables in their modelling. Specifically, they use non-parametric kernel density estimation (KDE) and adaptive k-means to generate aggregate spatial load densities, while stacked auto-encoders are used for forecasting the spatial load densities. Fiot and Dinuzzo \cite{fiot2018edf} generate forecasts of load at the smart meter level using kernel-based multi-task learning that considers the relationships between multiple demand profiles using the Irish CER data~\cite{Commission2012csm}. They demonstrate that kernels with multiplicative structure result in better forecast accuracy compared to additive structure kernels.


Support Vector Machines (SVM) is a robust, non-parametric method that is also based on kernel functions and has been popular, especially for smaller machine learning problems. Support Vector Regression (SVR) is the application of SVM to regression problems. Sousa et al.~\cite{sousa2014slf} propose an approach based on SVR for day-head residential load forecasting using simulated annealing for hyper-parameter search. The SVR model takes a representative load profile for a cluster of similar load profiles as input. The SVR with automated hyper-parameters can outperform a manually tuned ANN. Wang et al.~\cite{wang2013acv} propose a three-stage process using SVR at the feeder level, finding SVR outperforms ANN.

\subsubsection{Artificial Neural Networks}

Artificial Neural Networks (ANN) have attracted much attention in machine learning and they have been very popular in load forecasting and time series forecasting in general. The most basic form is a feed-forward neural network that has one or multiple hidden layers and is trained using the back-propagation algorithm and (stochastic) gradient descent. 

\paragraph{Feed-forward ANN} This basic ANN may be referred to as vanilla ANN or multi-layer perceptron (MLP) and is a popular benchmark for other approaches (see Section~\ref{sec:comparing_methods}) and the basis for more complex neural network architecture (see Section~\ref{sec:deep_learning}). Such a feed-forward ANN has, for instance, been used in \cite{sousa2012atr} for day-ahead forecasting of LV customers and substations in Portugal. They use a single hidden layer and an output layer with 24 nodes for each value of the hourly day-ahead profile. The authors compare different architectures, numbers of hidden nodes and activation functions. Similarly, Moon et al.~\cite{moon2019aca} explore day-ahead forecasting in buildings and groups of buildings in South Korea. They find that Scaled Exponential Linear Units outperforms other activation functions like the popular ReLU and that five hidden layers exhibited the best average performance. Pati et al.~\cite{pati2020mfc} compare ANN with simple moving average and exponential moving average methods for the Customer Baseline load (CBL) problem. 

A simple statistical benchmark method MidXofY (excluding Z lowest and Z highest load days to predict CBL)  results in the highest prediction accuracy for most of the consumers, followed by exponential moving average and an ANN method. In \cite{fidalgo2005lfp}, the authors develop one-hour to one-week load forecasts of 800 primary substation feeders and 200 transformers in Portugal. Their methodology is split into two parts. The first part is forecasting power, reactive power and current for a number of substations and feeders using ANN. The second part is about identifying anomalous events, such as historical data bugs, holidays, consumption price modifications, and re-training the ANN models. They find that retraining the ANN once the performance for a group of feeders is degrading for a period of four weeks, can make the forecasts more robust. Besides forecasting the load, ANN can also be used for imputing missing values of electricity consumption at the LV level.


\paragraph{Feature Transformations} While many authors use features directly related to load demand, others explore more advanced feature transformations. 
For instance, Bersani et al.~
\cite{bersani2006mol} explore Wavelet transforms as input for an ANN for day-ahead and intraday scheduling of LV transformers. Ding et al.~\cite{ding2016nnb} use ANNs for short-term forecasting of two MV/LV substations on the French distribution network. They split the load profile into two parts (i) daily average power and (ii) intraday power variation. Each component is forecast separately with different features via an ANN with a single hidden layer, but different model complexities (i.e. number of hidden neurons). ANN outperforms a naive and a linear regression model. 


\paragraph{Beyond Back-Propagation} Besides regular back-propagation, also variants like Broyden–Fletcher–Goldfarb–Shannon algorithm, Quasi-Newton back-propagation and a One-step secant for gradient descent in back-propagation neural networks are explored, for instance at the district level (cf. \cite{ahmad2019dlf}).
Shah et al.~\cite{shah2020stm} propose an ANN that is optimised using particle swarm optimization (PSO) instead of back-propagation. They compare it to regular ANN and also long short-term memory (LSTM) and find that the PSO ANN substantially outperforms the other two. Extreme Learning Machines (ELM), introduced initially as randomized neural networks~\cite{schmidt1992ffn}, are a variant of ANN that use the Moore-Penrose generalized inverse to determine the weights instead of gradient-based back-propagation. Stephen et al.~\cite{stephen2020ngr} compare ELM with a standard multivariate Gaussian and a Gaussian Copula for probabilistic forecasting of LV feeders, showing that ELM outperforms the benchmark. Similarly, Zhao et al.~\cite{zhao2020onp} use ELM to produce probabilistic forecasts, focusing on prediction intervals. 


\paragraph{Bayesian neural networks}
Bayesian neural network (BNN) are extending standard ANN with posterior inference. Bessani et al~\cite{bessani2020mhv} use a BNN for day-ahead forecasting. They train a global model, i.e., one that uses all available data of multiple households instead of fitting one local model per household. This approach outperforms a regular ANN benchmark as well as a HMM. Bayesian Networks are also used in \cite{Singh2018bdm} to predict energy consumption at the household level from individual appliances.


\subsubsection{Deep Neural Networks} \label{sec:deep_learning}

With the advancements in the area of deep learning, deep methods are also gaining relevance in the LV forecasting domain. Initially, recurrent approaches using long short-term memory (LSTM) cells were most popular. Compared to vanilla neurons, LSTM cells contain several gates, each with several trainable parameters, increasing the overall number of parameters to estimate from the training data. 

\paragraph{Long Short-term Memory} Kong et al.\cite{kong2017str} compare LSTM to regular ANN, ELM, kNN, and a na{\"i}ve model and find that it outperforms the other approaches in one-step-ahead forecasting (30 minutes) on 69 Australian households at the individual level. In contrast, at the aggregate level, LSTM perform similarly to regular ANN. Similarly, Mehdipour et al.~\cite{mehdipour2020slf} compare LSTM to SVR, Gradient Boosted Regression Trees (GBRT), and ANN at the household-level and also find that LSTM can consistently outperform the other models. Petroșanu et al.~\cite{petrosanu2019ddv} find that the combination of an LSTM with an NARX Feedback Neural Network can produce improvements over either alone. In \cite{lin2020ecp} an LSTM with an attention mechanism is proposed for short-term load forecasting in the China Southern Power Grid and four types of aggregated load; ‘Residential’, ‘Large Industrial electricity’, ‘Business’ and ‘Agricultural’. Particularly, the attention mechanism is used to assign weight coefficients to the input sequence data so that the specific features can be accurately extracted. The authors compare the proposed method to five benchmarks, an LSTM without attention, Holt-Winters, ARIMA, SVM and ANN and find that it outperforms each of the state-of-the-art methods.

\paragraph{Addressing Overfitting} Several studies find that due to the comparatively large number of trainable parameters, LSTM's trained for specific time series (e.g., a household or feeder) may be prone to overfitting. Mehdipour et al.~\cite{mehdipour2020slf} compare how different machine learning models generalise by assessing one-hour ahead forecasts on household data they have not trained on. For that, they try including 11, 22, 33, and 45 households in the training set and find that LSTM's improve as they have access to more data, while the other models' performances remain similar or deteriorate. The approach by Shi et al.~\cite{Shi2017dlf} uses this property of LSTM and finds that training a model on a pool of similar households improves over only training a local model. They find that this allows for deeper architectures and more accurate forecasts. Sun et al.~\cite{sun2019ubd} propose another approach based on incorporating deep Bayesian learning (DBL) into LSTM to avoid overfitting. DBL avoids overfitting by imposing a prior on hidden units or neural network parameters. Another direction is taken by Wu et al.~\cite{wu2020ast}. They use a cell related to the LSTM cell, the gated-recurrent unit (GRU). It has fewer parameters than the LSTM and can hence also avoid issues with overfitting. Further, they employ early-stopping as regularisation as a way to prevent overfitting.

\paragraph{Convolutional Neural Networks} In addition to approaches based on recurrent models like LSTM and GRU, methods based on convolutional neural network (CNN) layers have been proposed. Voss et al.~\cite{voss2018residential} apply a time series specific CNN, the WaveNET architecture based on causal and dilated convolutions at the household-level and show that it can outperform more straightforward approaches like MLR and ANN. Besides time series specific convolutions, two- and three-dimensional encodings of time series into images have also been applied. Estebsari and Rajabi~\cite{estebsari2020srl} compare three different image encodings based on recurrence plots, the Gramian angular field and the Markov transition field, finding that recurrence plots work well for encoding the time series as input for the convolutional layers. Elvers et al.~\cite{Elvers2019spl} utilise the daily seasonality and arrange the time series by aligning the daily load profiles row-wise in a 2D input matrix for regular CNN layers. The proposed model minimises the pinball loss for quantile regression and outperforms a quantile regression approach in intraday and day-ahead forecasting at the household-level and different aggregations.


\paragraph{Auto-encoders for Feature Representation} The previous applications used either purely LSTM or CNN layers directly to produce a forecast. Literature has investigated the combination of these different layers. Chen et al.\cite{chen2019daa} propose a multi-step load forecasting approach to aggregate load in the LV grid. After clustering similar customers, they use an auto-encoder (AE) based on convolutional layers for dimensional reduction. LSTM are then used to create a forecast per cluster in the encoded latent space, which is then decoded for the final forecast. They find that three neurons in the latent space are enough to represent the 48 dimensions of the load profile for the best forecasting results. Similarly, Khan et al.~\cite{khan2020tee} use a convolutional layer to extract spatial features, which are fed into an LSTM-AE to generate encoded sequences. A final dense layer is used for energy prediction. Here, the auto-encoder is used to tackle the issue that LSTM fails to learn temporal dependencies from one sequence to another. The method is applied for forecasting Korean residential and commercial building load data. Liu et al.~\cite{liu2019tsh} also find that a sparse encoding network can improve the forecast for an LSTM at the household-level. Naeem et al.~\cite{naeem2020stl} develop a day-ahead load forecast of an Australian network-grid using an Ensemble Empirical Mode Decomposition (EEMD) to decompose the signal into Intrinsic Mode Functions (IMF) and residuals. These modes and residuals are passed onto a Denoising Auto Encoder (DAE) for feature extraction. The output from the DAE is passed on to a CNN. The proposed method outperforms a deep belief network and an empirical mode decomposition mixed kernel-based ELM benchmark. % 

\paragraph{Comparing Deep Models} Hosein and Hosein~\cite{Hosein2017lfu} compare different neural architectures (regular ANN, ANN with fully connected AE, recurrent neural network (RNN) and LSTM). They find that an ANN combined with an AE is the best deep architecture and outperforms other traditional models such as weighted moving average, linear regression, regression trees, SVR, and MLP.


\paragraph{Transfer Learning} As mentioned above, deep architectures can benefit from using data not only from the local level but also from other (similar) buildings or households to improve accuracy. This property of deep learning models can be used in the cold start setting when not much data is yet available for a new building or a building where metering infrastructure is just deployed. For that transfer learning can be utilised. For instance, Gao et al.~\cite{gao2020dla} apply transfer learning from buildings with similar (i) environmental variables (close proximity) and (ii) functional type, e.g., office buildings. 
Their approach makes further use of pre-training and data augmentation. They find that their transfer approach can increase accuracy by around 20\% in settings with few available data. 
Similarly, Ribeiro et al.~\cite{ribeiro2018tls} introduce their transfer learning approach Hephaestus, a cross-building energy prediction method based on transfer learning with seasonal and trend adjustment. It is an inductive transfer learning method that is independent of the specific data-driven algorithm. 

\subsubsection{Other Artificial Intelligence Methods}
The aforementioned machine learning models are based on supervised machine learning, sometimes in combination with unsupervised approaches (see Section~\ref{sec:clustering}). Another class of machine learning algorithms is reinforcement learning. Feng et al.~\cite{ Feng2020rda} use Q-learning for model selection out of a pool of candidate ML models (ANN, SVR, GBM, RF) and for probabilistic forecasting from candidate distributions (Gaussian, Gamma, T, Laplace). Q-learning is used to choose the best model in each time-step, based on performance in a sliding window of data. They find that their ensemble approach outperforms the best individual models. 

By far, the most popular and successful artificial intelligence (AI)-based approaches, as introduced in the former section, are based on only a sub-section of artificial intelligence, i.e., machine learning, the sub-field that deals with algorithms that learn from data. Some approaches from fuzzy logic, another subfield of AI, are proposed or combined with ML approaches. Konjic et al.~\cite{konjic2005fis} use a so-called Takagi-Sugeno Fuzzy inference system for forecasting different classes of LV substations. Jurado et al.~\cite{Jurado2015hme} propose a forecasting method based on Fuzzy Inductive Reasoning (FIR) at the building level. They show based on three buildings that it outperforms ANN, RF and ARIMA. In \cite{jurado2017fir}, the authors introduce an improved version of the FIR algorithm, called flexible FIR, for studying missing values occurring in the training and test sets of load data. The method is applied to eight buildings in Catalonia (Spain), and it is demonstrated that increasing the percentage of missing values increases the modelling error. However, further details regarding hyperparameter tuning could have been provided. 

The work of Abreu et al.~\cite{Abreu2018mlf} combines ML with symbolic approaches by employing an ANN from fuzzy-adaptive resonance theory (which they refer to as fuzzy ARTMAP neural networks) for load forecasting, while considering different hierarchies of the distribution system. Their modelling comprises two components: global load forecasting (considering the sum of all loads on the system), and multimodal load forecasting (focusing on substations, transformers, and feeders).


\subsection{Comparing Methods} 
\label{sec:comparing_methods}

Many authors perform comparative studies where the focus is not on a single method, but on several different statistical and machine learning methods. Humeau et al.~\cite{Humeau2013elf} compare MLR, SVR and ANN at different aggregations of households and find that MLR works best at the household level and SVR performs best at the aggregate level (after 32 households). Given that both ANN and SVR are popular, others have also compared them. In \cite{gajowniczek2014ste}, the authors develop two day-ahead load forecasts at hourly resolution at the household level in Warsaw, using ANN and SVR. Particularly, the authors train a single ANN with 24 output nodes and 24 SVR models one for each hour of the horizon. However, the paper does not mention any comparison of the proposed method to any benchmarks, neither does it discuss hyper-parameter training nor the feature selection process.

ANN and SVR based forecasts are also compared in \cite{garulli2015mat} and in \cite{grolinger2016eff}. While Garulli et al.~\cite{garulli2015mat} find that SVM perform better than ANN, \cite{grolinger2016eff} finds the opposite. 

Tree-based approaches are also popular (see Section~\ref{sec:trees_and_forests}). Yunusov et al.~\cite{Yunusov2018sss} develop three two-day ahead forecasts based on a seasonal regression model, RF and SVR for nine feeders in the UK network and the forecasts are used in storage system control algorithms. A seasonal linear regression model outperforms the other two approaches in terms of forecast error and also the objective of the storage control. Nawaz et al.~\cite{nawaz2019aaf} uses Recursive Feature Elimination to select features and then compare various methods, i.e., kNN, RT, RF and SVR. They show that at different forecast horizons different methods perform best. In contrast, Cerquitelli et al.~\cite{Cerquitelli2019esm} compare ridge regression, ANN, and RF for hour-ahead building-level load forecasts and find that ANN generates the most accurate forecasts overall.

In their rather innovative work, Cordova et al.~\cite{Cordova2019cet} model residential load for the city of Tallahassee combining smart electricity meter data with transport network data and use information theory and causality models for the simultaneous study of the two datasets. They compare several statistical and machine learning algorithms, such as ARIMA, MLR, absolute shrinkage and selection operator (LASSO) and ridge regression, deep neural networks (DNN) and SVR.

In addition to ANN and SVR methods, Idowu et al.~\cite{idowu2016aml} also train, evaluate and compare MLP and Regression Tree for load forecasting in district \textit{heating} substations in Sweden. 
The authors show that SVM is the best performing method on their dataset with ANN and MLP following closely. Also, it is shown that the substations' internal state variables are important for high accuracy.
As shown,  linear regression models can also be quite accurate, but the choice of predictors is usually largely manual and ad-hoc. Thus, it would be beneficial to see studies with more automated feature selection, including LASSO methods which have been shown to be powerful at higher voltages (see e.g. \cite{Ziel14}).

Mirowski et al.~\cite{mirowski2014dfi} presents results from a uniquely large hierarchical data set of 32000 end-users over two years. They compare different approaches ranging from simple linear regression, to time series-based models (ARIMA and Holt-WinterS), a state-space model and machine learning (weighted kernal regression, kernel ridge regression and SVR) for forecasting of smart meter data at the customer, feeder, and system level. They find that SVR and the Holt-Winter model perform best in one-step ahead and day-ahead forecasting and for all aggregations. Simple combinations, like the mean of the best models, can improve the results (see also next Section).


\subsection{Hybrid, Combination and Ensemble Approaches}
\label{hybridSection}
The majority of the papers reviewed in this paper considers only individual models but combinations of models can produce much more accurate forecasts. The most popular and well-known methods for combining models are the ensemble approaches such as random forest, boosting and bagging, which combine weaker models to produce a strong predictor. Jung et al. \cite{Jung2020bem} propose a bagging ensemble of ANN for three building clusters in Seoul, South Korea, demonstrating that their methodology delivers better performance than other imputation techniques. In \cite{Laurinec2019due} bagging is applied for forecasting clusters of households which are then clustered with unsupervised ensemble learning methods to produce the final day-ahead aggregate load forecast.

Random forests (RF) are an ensemble learning method based on regression trees (Section \ref{sec:trees_and_forests}). In the regression case, RF constructs an ensemble of simple RT and make a final prediction based on the average of the individual trees. Compared to RT, they are typically more accurate but sacrifice interpretability. However, RF can handle a large number of features and can be used to analyse feature importance. For instance, Bogomolov et al.~\cite{bogomolov2016ecp} develop a novel RF regression model for predicting average and peak energy demand over the next seven days at the aggregate level, in Trentino (Italy), from energy data and aggregate mobile data usage. The authors extract several characteristics (e.g. mean, median, std, kurtosis, entropy) from the telecommunication data resulting in initially 3,000 features. After feature selection based on the total decrease in node impurities, the features space is reduced to 32 features with the number of customers per grid power-line having the highest feature importance. Kiguchi et al.~\cite{Kiguchi2019pil} use RFs to model residential load under time-of-use tariffs, considering different variables such as gender, age group, social class, and the number of other residents. They found RF to be more accurate than linear regression and ANNs and used RF to provide feature rankings based on the Gini coefficient.

Ben Taieb et al. \cite{taieb2016fui} consider an additive quantile regression model with boosting for probabilistic forecasts at an individual smart meter and aggregated level (all 3639 smart meters from the Irish CER dataset~\cite{Commission2012csm}). They compare this method to a standard additive model assuming normal distribution of errors. At an individual household level the proposed methods is most accurate, but at the aggregated the normal errors model is best. This is one of few demonstrations of the central limit theorem and shows this assumption of normality breaks down at the LV level. Nespoli et al. \cite{nespoli2020hdf} consider an XGBoost algorithm to perform hierarchical forecasting (just two levels LV to MV) and different ways to reconcile the forecasts. Ruiz-Abellón et al. \cite{ruiz-abellon2018lff} consider regression trees, and compare this with other ensemble approaches including bagging, random forest, conditional forest, and boosting, with random forest outperforming others, and XGBoost also showing promise. 

The ensemble approaches above use the same baseline models to create the final forecast. Another approach is to simply combine different models, with the aim to create the combined model that outperforms the individual ones. This is considered by Grmanová et al. \cite{grmanova2016iel} in which several machine learning and statistical baseline models, including SVR, MLR, AR, Holt-Winters, and a feed-forward ANN, are all averaged together and shown to perform statistically better than any individual model. An average of models is also considered by Dong et al. \cite{dong2016ahm}. The interesting feature of this paper is that five traditional machine learning models (ANN, SVR, least-squares SVM, Gaussian Process regression and Gaussian mixture model) are combined with a physics-driven model to forecast the air-conditioning (AC) component of the household electricity consumption. 

Not all papers use a simple average, Kodaira et al. \cite{kodaira2020oes} use a weighted average (learnt using a particle swarm optimisation) to combine a k-means and an ANN method. In \cite{tajer2017lfv} a learning-based load framework is proposed for an hour ahead forecast where the aggregated load prediction is combined from several weighted predictors for local sub-networks using kernels that are most suited to their load characteristics. Investigation of weightings in more detail would be an interesting avenue of further research. 

A slight modification to the simple averaging of models is to use a hybrid approach, where different features/components of the forecasts are generated using different models. These are by far the most common form of multiple-model based forecasts. Massidda and Marrocu~\cite{massidda2019smf} split the forecast into long term effects (trend and seasonality) and short term effects which are modelled by a random forest and multiple linear regression, respectively. In \cite{hashim2019anl}, the authors apply a hybrid neuro-fuzzy method with three stages to remove 
load shedding effects in load forecasting at the feeder level applied to two LV feeders from two substations in Najaf city (Iraq). % 
Amato et al. \cite{Amato2021fhr} propose a hybrid model based on a partially linear additive model to generate point forecasts for the Irish CER dataset~\cite{Commission2012csm}. They focused on forecasting for a small aggregation of households. 
The authors demonstrate that their modelling approach based on wavelets and spline decomposition 
can accommodate both the smooth  (temperature effects) and irregular (peak) patterns at the low aggregation levels. 

ARIMA and ANN type models are common components in model combinations. The authors in \cite{Bennett2014flv} propose a hybrid model using ARIMA and ANN to generate point forecasts for day-ahead total energy and peak load at the LV transformer level (serving 128 residential consumers). The authors report that ANN accounted more for small variations, while ARIMAX was more suited for modelling large spikes in load. This formed the justification for the authors to propose a hybrid model whereby load was first forecasted using ANNs, and if the forecasted demand was higher than a threshold, ARIMAX was used as a final forecasting model. Sulandari et al. \cite{sulandari2016fel} generate a hybrid model where the outputs from a TBATS (exponential smoothing state-space model with Box-Cox Transformation, ARMA error, trend and seasonal components) model are used as inputs to an ANN.  TBATS is used to decompose load time series data into a level, trend, seasonal and irregular components and ANN is applied to capture nonlinearity in the data. The hybrid model outperforms the individual TBATS and ANN models.   

Cao et al. combine the hybrid approach of ensemble techniques \cite{Cao2020hed}. They employ one bagging and five boosting techniques to improve the forecast accuracy of a deep belief network (DBN) and propose a hybrid modelling approach that adaptively combines six base ensemble technique, whereby the combination weights were computed via a k-nearest neighbour. The authors consider two sources of prediction uncertainty: (i) model misspecification, and (ii) data noise, both of which are assumed as being independent and as confirming to a Gaussian distribution. This could potentially be a rather restrictive assumption if this methodology were to be scaled. To satisfy the stationarity requirements of bagging and boosting, the data was differenced, and assumptions checked using the Augmented Dickey-Fuller test.

Ai et al.~\cite{ai2019hpd} propose a computational ensemble approach based on evolution strategies to select and configure candidate forecast models and combine their outputs. The forecast is used for very short-term forecasting and outperforms the persistence forecast and the predictions of the individual candidate models.

Sun et al.\cite{sun2015aea} propose a forecasting method at the feeder-level that consists of a two-step method that selects the best model based on the characteristics of the load. If the node is classified as regular, it is forecasted using a simple seasonal moving-average model. Irregular nodes are forecasted using a wavelet neural network approach.


\subsection{Probabilistic Forecasting} 
\label{section_prob} 
LV demand typically exhibits higher volatility and less seasonality, as compared to the high voltage, or system demand see \cite{Haben2019stl} and Fig.~\ref{lvplots}. For these reasons, probabilistic forecasts are often more appropriate than point forecasts due to their representation of the underlying uncertainty. A probabilistic forecast encapsulates the uncertainty associated with the forecasts, often in the form of a full probability distribution, which allows for more informed decision-making, as opposed to the case when either a point forecast or forecasts for a discrete set of predefined quantiles are considered. Probabilistic forecasts at the LV level can have a range of applications including, efficient operations and control of the distribution grid, anomaly detection, early warning system for peak consumption, designing time-of-use tariffs, energy trading, and online power management of the micro-grid, to name a few.

Uncertainty in forecasts can be conveyed in a variety of forms, they can be simple prediction intervals \cite{kodaira2020oes}, quantile estimates \cite{gerossier2018rda}, full continuous distributions \cite{pinto2017mpf} or ensembles, and can be univariate or multivariate. The best choice often depends on the purpose of the forecast. 

Evaluation of probabilistic forecasts requires the use of proper scoring functions, such as the continuous ranked probability score (CRPS) \cite{taieb2016fui} or the pinball loss score \cite{wang2019pil}. For probabilistic forecasts, such proper scoring functions reward a forecast if it is sharp (has a narrower distribution), subject to calibration (statistical consistency between forecast distribution and observations). Probabilistic scoring rules have also been used to estimate the model parameters, to try and ensure that both model estimation and evaluation take into account the uncertainty associated with load forecasts \cite{Arora2016fes}. There are also some less common methods for evaluating probabilistic forecasts. In addition to using a normalised quantile score, Gerossier et al. \cite{gerossier2018rda} also consider the count of observations between successive quantiles, in other words, similar to the validation via the probability integral transform. Yang et al. \cite{yang2020bdl} also consider the infrequently used Winkler score, which measures the sharpness and unconditional coverage of prediction intervals. 
\subsubsection{Prediction intervals}
Prediction intervals are the least descriptive form of probabilistic estimates, since they only provide the range of a particular distribution of points, e.g. from 5\% to 95\%, but are very easy to interpret and describe to non-experts. Chaouch and  Khardani \cite{chaouch2015rcq} uses kernel regression (with Nadaraya-Watson weights) to generate a full continuous density function forecast for peak demand of a smart meter, but only use it to generate a prediction interval whose accuracy is assessed using MAPE.  Kodaira et al.\cite{kodaira2020oes} forecasts prediction intervals by fitting various models to the residuals, including an assumed Gaussian model, a Chebyshev inequality-based method and a sample-based method.

\subsubsection{Quantile regression and similar methods}
One of the most common approaches to generate a probabilistic forecast is quantile regression. They are typically less computationally expensive than generating the full distribution but can be arbitrarily descriptive by choosing sufficiently large numbers of quantile cut points. Wang et al. \cite{wang2019pil} produce a quantile regression by fitting an LSTM to a pinball loss function, and compares it to other quantile methods, including a quantile regression neural network, a quantile gradient boosting regression tree and a benchmark that assumes Gaussian errors. Assuming Gaussian errors is a common simple model used since it is entirely defined by two parameters. Similarly, Elvers et al.~\cite{Elvers2019spl} propose a pinball loss guided CNN and show that it outperforms linear quantile regression, needing only to train one model, instead of one model per quantile and step in the horizon. The authors in \cite{yang2020bdl} produce a quantile forecast based on an LSTM model with a clustering-based pooling method to prevent over-fitting.  Zufferey et al.~\cite{zufferey2020psf} compare a pinball loss guided ANN with a probabilistic kNN for net power load forecasting. Within the probabilistic kNN, the forecasts of the k nearest neighbours are optimally weighted to minimize the pinball loss. A Bayesian neural network is also used to put a prior and update the parameters on the LSTM layers. The method is compared to, and improves upon, several benchmarks including quantile regression forests, and gradient boosting quantile regression. 

Ben Taieb et al. \cite{taieb2016fui} use an additive quantile regression using boosting, where base learners for each variable are added at each iteration of the algorithm. This is tested on both the 3639 individual smart meters from the Irish smart meter trial~\cite{Commission2012csm}, as well as their aggregation. Gerossier et al. \cite{gerossier2018rda} produce several simple quantile forecasts models including empirical quantiles conditional on the period of the week, and an additive model with temperature forecast and historical load as the main inputs. 

The paper by Bikcora et al. \cite{bikcora2018dfo} is slightly more unusual in that, in addition to consider a quantile model, it also considers an expectile regression which is rarely seen in the load forecasting literature. This is essentially a quantile regression using squared differences rather than absolute differences between the estimate and the observations\footnote{Expectiles are to the mean, as quantiles are to the median}. Yang and Hong, \cite{Yang2019del} propose a deep learning ensemble framework for probabilistic load forecasting. The authors use the Irish CER smart electricity meter data~\cite{Commission2012csm} and generate forecasts for one-hour and one-day ahead. The authors employ DNN for forecasting and use LASSO-based quantile combination strategy to refine the ensemble forecasts, whereby the pinball loss and Winkler score are used for model evaluation. As benchmarks, they use different ML approaches including quantile regression forest, quantile regression gradient boosting, and quantile LSTM. 

\subsubsection{Density estimation}

Less common than quantile estimates are full density estimates. These give the most information for the distribution but unless they assume Gaussian errors (as considered in e.g. \cite{wang2019pil}) they are typically much more expensive to estimate. The aforementioned paper by Bikcora et al. \cite{bikcora2018dfo} also considers a density estimate using an ARMA-GARCH model. This is one of the few examples of a traditional econometric model being used in load forecasting but has the advantage of requiring relatively few easy-to-train parameters, since it models both a mean and variance model.

Kernel density estimates are more common for density forecasts and are the summation of kernel functions with specific bandwidths which must be trained for each conditional variable \cite{pinto2017mpf}, \cite{Arora2016fes}. The drawback to these methods is that each new parameter is very expensive to train and therefore only a few conditional variables can be included. Arora et al. \cite{Arora2016fes} compare several conditional KDE models to generate household level density forecasts using the Irish CER dataset~\cite{Commission2012csm}. They condition consumption on time-of-day information and include decay parameters to reduce the influence of older data. Interestingly, they estimate model parameters by minimizing the in-sample continuous ranked probability score (CRPS) and evaluate the models using a range of performance scores to evaluate point, quantile, and probabilistic forecasts.

\subsubsection{Comparison of different methods}
Haben et al. \cite{Haben2019stl} present the first large-scale study of real LV feeder data by investigating the effect of temperature (real and predicted) on the accuracy of probabilistic load forecasts. They employed a range of forecasting models including quantile regression, GARCH-type models and conditional kernel density estimation among others. Interestingly, it was demonstrated that the power-law relationship between feeder size and forecast accuracy (Figure \ref{lvplots}) does not hold true for some feeders, such as those with large numbers of overnight storage heating, large amounts of lighting etc.

\subsubsection{Combined and hierarchical modelling}
Despite the importance of probabilistic forecasts for LV level demand, they are relatively few in number and are far outnumbered with point-based methods. Of the 221 papers reviewed, 44 included some probabilistic forecast element, and as seen in this section many of the models are relatively simple.  However, recent research is beginning to investigate some important aspects of probabilistic forecasts such as model combination and hierarchical modelling. Wang et al. \cite{Wang2019cpl} considers a constrained quantile regression averaging (CQRA) which is a weighted average across several quantile regression models with weights found by solving a linear programming problem that minimises the pinball loss function. They consider a quantile regression neural network, quantile regression random forests, and quantile regression gradient boosting and compare nine different forecast combination schemes, with the CQRA performing the best for reducing forecast errors. 

Another important topic is hierarchical forecasting which aims to ensure forecasts at the lowest level aggregate coherently with the higher level. This is becoming increasingly important with the development of smart connected grids. Although producing coherent point forecasts is relatively simple it is much more complicated for probabilistic forecasts. Ben Taieb et al. \cite{Taieb2020hpf} produce hierarchical probabilistic forecasts by first generating an empirical copula method for each series in the hierarchy and then using a hybrid model to revise them to be coherent. The copulas also ensure that inter-dependencies are preserved. It is worth noting here that although aggregate forecasts can be generated by simply summing up the corresponding forecasts at the lower level, it has been shown that this bottom-up approach produces poor results \cite{Hyndman2011ocf}. 
Hierarchical forecasting has also been a common topic in the Global Energy Forecasting Competitions (GEFCom). In particular, GEFCom2014 considered probabilistic load forecasts at two levels with the loads mostly at 4kV and 12kV \cite{Gefcom2014}. 


\subsection{Other methods} 

This section considers some methods which don't fall into the traditional categorisation of forecasts described above. This includes the utilisation of new data sources, different frameworks and techniques. 

The authors in \cite{mcqueen2004mcs} focus on modelling the maximum demand on distribution networks using Monte Carlo simulations. Particularly, their goal is to create representative load profiles for use in the simulations. The daily and intraday demand is sampled from parametrized distributions. The method is applied to a single LV network and an entire network consisting of 557 transformers in Dunedin, New Zealand.

Zhou et al. \cite{zhou2019pse} use a graph approach where edges are correlation between variables and nodes are sectors (industrial and commercial firms in Shanghai). Nodes of the graph represent different sectors and their sizes correspond to the scale of the sectors, measured by average daily demand. A Granger causality test is used to indicate if there is a relationship between sectors and Akaike information criterion is used to choose lags. Pruning of the network is done using a minimum spanning tree to retain the most important information in the graph. Selected nodes are used as inputs to linear regression models.

Finally, the authors in \cite{xu2020ats} produce a day-ahead forecast of a twenty-floor office building in Shanghai using a gray theory model (numerically solved ordinary differential equations) using historic and current load. 
These methods demonstrate that there is a wide scope for applying less-traditional machine learning or time series methods for forecasting. These can only be expected to expand, especially as more granular or diverse datasets become available. 
