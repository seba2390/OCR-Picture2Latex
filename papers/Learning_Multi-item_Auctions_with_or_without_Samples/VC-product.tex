\section{Uniform Convergence under Product Measures}\label{sec:uniform convergence under product measure}

In this section, we develop machinery for obtaining uniform convergence bounds for hypotheses over product measures. Our goal is to save on the sample complexity implied by VC dimension bounds, as summarized in Table~\ref{tab:productVC}. Indeed, we obtain low sample complexity  bounds for indicators over \emph{single-intersecting} sets (see Definition~\ref{def:single-intersecting}), which play a key role in proving our results for learning approximately revenue-optimal auctions. Our main results of this section are Theorem~\ref{thm:uniform convergence for product measure PARTITION} for general functions, and Corollary~\ref{cor:VC for product measure} for sets.

We first define what type of uniform convergence bounds we seek to prove.

\begin{definition}[$(\epsilon,\delta)$-uniform convergence with respect to proxy measure]
	A hypothesis class $\HH$ of functions mapping domain set $\XX$ to $\mathbb{R}$ has {\em $(\epsilon,\delta)$-uniform convergence with sample complexity $s(\epsilon,\delta)$} iff, for all $\epsilon,\delta > 0$, there exists a {\em processing} ${\cal P}:{\cal X}^{s(\epsilon,\delta)} \rightarrow \Delta(\XX)$ such that for any distribution $\DD \in \Delta(\XX)$ when $k=s(\epsilon,\delta)$: 
	\begin{align}
	\Pr_{z_1,\cdots, z_k \sim \DD}\left[\sup_{g\in \HH}\left|\E_{z\sim {\cal P}(z_1,\cdots, z_k)}[g(z)] - \E_{z\sim \DD}[g(z)]\right|\leq \epsilon\right]\geq 1-\delta. \notag
	\end{align}
%$$\Pr\left[\sup_{g\in \HH}\left|\frac{1}{k}\cdot \sum_{j=1}^{k} g(z_j) - \E_{z\sim \DD}[g(z)]\right|\leq \epsilon\right]\geq 1-\delta.$$

When ${\cal X}$ is the Cartesian product of a collection of sets ${\cal X}_1,\ldots,{\cal X}_k$, i.e.~${\cal X} =\times_i {\cal X}_i$, we say that a hypothesis class $\HH$ as above has {\em $(\epsilon,\delta)$-p.m.~uniform convergence with sample complexity $s(\epsilon,\delta)$} if the above holds for all ${\cal D}$ that are product measures over ${\cal X}$.	
\end{definition}

Next we provide a simple lemma, which leads to a simple version of our main result stated as Theorem~\ref{thm:uniform convergence for product measure}. Our main result, stated as Theorem~\ref{thm:uniform convergence for product measure PARTITION}, follows.

%In particular, we show that if each of the projected hypothesis class uniformly converges with polynomial many samples, the original hypothesis class also uniformly converges with a polynomial number of samples. 
%
%In the end of this section, we apply our uniform convergence results to a particular hypothesis class called 

\begin{lemma} \label{lem:approx product dist}
	Let $\XX_1,\ldots, \XX_d$ be $d$ domain sets and $\HH$ be a hypothesis class with functions mapping from the product space $\times_{i=1}^{d} \XX_i$ to $\mathbb{R}$. For all $i\in[d]$, let $\HH_i$ be the projected hypothesis class of $\HH$ on $\XX_i$, that is, $\HH_i=\left\{g\ |\ \exists f\in \HH\ \exists\ a_{-i}\in \times_{j\neq i} \XX_j \ \forall\ x_i\in \XX_i, \ g(x_i) = f(x_i,a_{-i})\right\}$. For every $i\in[d]$, let $\DD_i$ and $\hat{\DD}_i$ be two distributions supported on $\XX_i$. Suppose for all $i\in[d]$, $$\sup_{g\in\HH_i}\left|\E_{x\sim \DD_i}[g(x)]-\E_{x\sim \hat{\DD}_i}[g(x)]\right|\leq \epsilon,$$ then $$\sup_{f\in\HH} \left|\E_{\boldsymbol{x}\sim \times_{i=1}^d \DD_i}\left[f(\boldsymbol{x})\right]-\E_{\boldsymbol{x}\sim \times_{i=1}^d \hat{\DD}_i}\left[f(\boldsymbol{x})\right]\right|\leq d\cdot \epsilon.$$
\end{lemma} 

\begin{proof}
	Let $\FF_i$ and $\hat{\FF}_i$ be the probability measure function for $\DD_i$ and $\hat{\DD}_i$ respectively. We will prove the statement using a hybrid argument. We create a sequence of product distributions $\{\DD^{(j)}\}_{j\leq d}$, where $\DD^{(j)}=\hat{\DD}_1\times\cdots\times\hat{\DD}_j\times \DD_{j+1}\times\cdots\times \DD_{d},$ and $\DD^{(0)}=\DD$, $\DD^{(d)}=\hat{\DD}$. To prove our claim, it suffices to show that for any integer $j\in[d]$, $$\left|\E_{\boldsymbol{x}\sim \DD^{(j-1)}}\left[f(\boldsymbol{x})\right]-\E_{\boldsymbol{x}\sim \DD^{(j)}}\left[f(\boldsymbol{x})\right]\right|\leq \epsilon.$$
	%Let us first fix some notations. Let $\EE_{-j}=\left\{x_{-j}\ |\ \exists x_j\in \mathbb{R},\ (x_j,x_{-j})\in \EE\right\}$, $\EE_j(a_{-j})= \left\{x_{j} \in \mathbb{R} \ |\ (x_j,a_{-j})\in \EE\right\}$ for all $j\in[d]$. As $\EE$ is single-intersecting, $\EE_j(x_{-j})$ is an interval for all $j$ and $x_{-j}$. Moreover, since $||\DD_j-\hat{\DD}_j||_K\leq \xi$, we have $|\Pr_{\DD_j}[x_j\in \EE_j(x_{-j})]-\Pr_{\hat{\DD}_j}[x_j\in \EE_j(x_{-j})]|\leq 2\xi$ for all $j$ and $x_{-j}$. Next, we bound the difference for the probability of event $\EE$ under $\DD^{(j-1)}$ and $\DD^{(j)}$.
	
	Next, we show how to derive this inequality.
	\begin{align*}
		&\left|\E_{\boldsymbol{x}\sim \DD^{(j-1)}}\left[f(\boldsymbol{x})\right]-\E_{\boldsymbol{x}\sim \DD^{(j)}}\left[f(\boldsymbol{x})\right]\right|\\
		=&\Bigg{|}\int_{\times_{i\neq j} \XX_i}\left(\int_{\XX_j}f(x_{j},x_{-j}) d \FF_j(x_j)\right) d\hat{\FF}_1(x_1)\cdots d\hat{\FF}_{j-1}(x_{j-1}) d\FF_{j+1}(x_{j+1})\cdots d\FF_{d}(x_{d})\\
		 & ~~~~~~~~~~~~~~~~~~~~ - \int_{\times_{i\neq j} \XX_i}\left(\int_{\XX_j}f(x_{j},x_{-j}) d \hat{\FF}_j(x_j)\right) d\hat{\FF}_1(x_1)\cdots d\hat{\FF}_{j-1}(x_{j-1}) d\FF_{j+1}(x_{j+1})\cdots d\FF_{d}(x_{d})\Bigg{|}\\
		 = & \left|\int_{\times_{i\neq j} \XX_i} \left(\E_{x_j\sim \DD_j}\left[f(x_j,x_{-j})\right]-\E_{x_j\sim \hat{\DD}_j}[f(x_j,x_{-j})]\right)
		  d\hat{\FF}_1(x_1)\cdots d\hat{\FF}_{j-1}(x_{j-1}) d\FF_{j+1}(x_{j+1})\cdots d\FF_{d}(x_{d})\right|\\
		  \leq & \epsilon \cdot\int_{\times_{i\neq j} \XX_i} d\hat{\FF}_1(x_1)\cdots d\hat{\FF}_{j-1}(x_{j-1}) d\FF_{j+1}(x_{j+1})\cdots d\FF_{d}(x_{d})\\
		   = & \epsilon
	\end{align*}
\end{proof}



\begin{theorem}\label{thm:uniform convergence for product measure}
Let $\XX_1,\ldots, \XX_d$ be $d$ domain sets and $\HH$  a hypothesis class of functions mapping from the product space $\times_{i=1}^{d} \XX_i$ to $\mathbb{R}$. For all $i\in[d]$, let $\HH_i$ be the projected hypothesis class of $\HH$ on $\XX_i$, that is $\HH_i=\left\{g\ |\ \exists f\in \HH\ \exists\ a_{-i}\in \times_{j\neq i} \XX_j \ \forall\ x_i\in \XX_i, \ g(x_i) = f(x_i,a_{-i})\right\}$. 

Suppose that, for all $i\in [d]$, $\HH_i$ has $(\epsilon,\delta)$-uniform convergence with sample complexity $s_i(\epsilon,\delta)$.  Then $\HH$ has $(\epsilon,\delta)$-p.m.~uniform convergence with sample complexity $s(\epsilon,\delta) = \max_{i\in[d]} s_i(\epsilon/d,\delta/d)$.

In particular, let $\boldsymbol{z}^{(1)},\ldots, \boldsymbol{z}^{(\ell)}$ be a sample of size $\ell = s(\epsilon,\delta)$ from a product measure $\times_{i\in[d]} \DD_i$. Define $\hat{\DD}_i = {\cal P}_i({z}^{(1)}_i,\ldots,z^{(\ell)}_i)$, for all $i\in[d]$, where $z^{(j)}_i$ is the $i$-th entry of sample $\boldsymbol{z}^{(j)}$ and ${\cal P}_i$ is the processing corresponding to $\HH_i$'s uniform convergence. Then 
$$\Pr_{\boldsymbol{z}^{(1)},\ldots, \boldsymbol{z}^{(\ell)}}\left[\sup_{f\in \HH}\left|\E_{\boldsymbol{z}\sim \times_{i\in[d]}\hat{\DD}_i}\left[f(\boldsymbol{z})\right] - \E_{\boldsymbol{z}\sim \times_{i\in[d]}{\DD}_i}\left[f(\boldsymbol{z})\right]\right|\leq \epsilon\right]\geq 1-\delta.$$
\end{theorem}

\begin{prevproof}{Theorem}{thm:uniform convergence for product measure}
	Since $\ell\geq s_i(\epsilon/d,\delta/d)$, $\Pr\left[\sup_{g\in \HH_i}\left|\E_{z\sim \hat{\DD}_i}[g(z)]- \E_{z\sim \DD_i}[g(z)]\right|\leq \epsilon/d\right]\geq 1-\delta/d$ for all $i\in[d]$. By the union bound, with probability at least $1- \delta$, $\sup_{g\in \HH_i}\left|\E_{z\sim \hat{\DD}_i}[g(z)]- \E_{z\sim \DD_i}[g(z)]\right|\leq \epsilon/d$ for all $i\in[d]$. According to Lemma~\ref{lem:approx product dist}, $\sup_{f\in \HH}\left|\E_{\boldsymbol{z}\sim \times_{i\in[d]}\hat{\DD}_i}\left[f(\boldsymbol{z})\right] - \E_{\boldsymbol{z}\sim \times_{i\in[d]}{\DD}_i}\left[f(\boldsymbol{z})\right]\right|\leq \epsilon$ with probability at least $1- \delta$.
\end{prevproof}

\begin{theorem}\label{thm:uniform convergence for product measure PARTITION}
Let $\XX_1,\ldots, \XX_d$ be $d$ domain sets and $\HH$ a hypothesis class of functions mapping from the product space $\times_{i=1}^{d} \XX_i$ to $\mathbb{R}$. For all $T\subseteq [d]$, let $\HH_T$ be the projected hypothesis class of $\HH$ on $\XX_T \equiv \times_{i \in T}\XX_i$, that is, $\HH_T=\left\{g\ |\ \exists f\in \HH\ \exists\ a_{-T}\in \times_{j\notin T} \XX_j \ \forall\ x_T\in \XX_T, \ g(x_T) = f(x_T,a_{-T})\right\}$. Suppose that, for all $T \subseteq [d]$, $\HH_T$ has $(\epsilon,\delta)$-p.m.~uniform convergence with sample complexity $s_T(\epsilon,\delta)$, and define 
\begin{align}s(\epsilon,\delta) = \min_{\begin{minipage}[h]{3.5cm}\centering $k$,~partitions\\$T_1 \sqcup T_2 \sqcup \ldots \sqcup T_k = [d]$\end{minipage}} \max_{i=1,\ldots,k} s_{T_i}(\epsilon/k,\delta/k). \label{eq:yang}
\end{align}
Then $\HH$ has $(\epsilon,\delta)$-p.m.~uniform convergence with sample complexity $s(\epsilon,\delta)$.

In particular, let $\boldsymbol{z}^{(1)},\ldots, \boldsymbol{z}^{(\ell)}$ be a sample of size $\ell = s(\epsilon,\delta)$ from a product measure $\times_{i\in[d]} \DD_i$. Suppose that the optimum of~\eqref{eq:yang} is attained at $k=\tilde{k}$ for partition $\tilde{T}_1 \sqcup \tilde{T}_2 \sqcup \ldots \sqcup \tilde{T}_{\tilde{k}} = [d]$. Define $\hat{\DD}_{\tilde{T}_i} = {\cal P}_{\tilde{T}_i}({\boldsymbol{z}}^{(1)}_{\tilde{T}_i},\ldots,{\boldsymbol{z}}^{(\ell)}_{\tilde{T}_i})$, for all $i\in[d]$,  where ${\boldsymbol{z}}^{(j)}_{\tilde{T}_i}$ contains the entries of sample $\boldsymbol{z}^{(j)}$ in coordinates $\tilde{T}_i$ and ${\cal P}_{\tilde{T}_i}$ is the processing corresponding to $\HH_{\tilde{T}_i}$'s uniform convergence. Then 
$$\Pr_{\boldsymbol{z}^{(1)},\ldots, \boldsymbol{z}^{(\ell)}}\left[\sup_{f\in \HH}\left|\E_{\boldsymbol{z}\sim \times_{i\in[\tilde{k}]}\hat{\DD}_{\tilde{T}_i}}\left[f(\boldsymbol{z})\right] - \E_{\boldsymbol{z}\sim \times_{i\in[d]}{\DD}_i}\left[f(\boldsymbol{z})\right]\right|\leq \epsilon\right]\geq 1-\delta.$$


%Then $\HH$ has $(\epsilon,\delta)$-uniform convergence with sample complexity $s(\epsilon,\delta)$.
\end{theorem}
\begin{prevproof}{Theorem}{thm:uniform convergence for product measure PARTITION}
For every possible partition use Theorem~\ref{thm:uniform convergence for product measure}.
\end{prevproof}


Nest, we specialize Theorem~\ref{thm:uniform convergence for product measure PARTITION} to indicator functions over sets.

%In the next Corollary, we show that if all functions in $\HH$ have $0$ or $1$ values and each $\HH_i$ has VC dimension $V_i$, 
\begin{corollary}\label{cor:VC for product measure}
	We use the same notation as in Theorem~\ref{thm:uniform convergence for product measure PARTITION}. Suppose that all functions in $\HH$ map $\times_{i=1}^{d} \XX_i$ to $\{0,1\}$, i.e.~they are indicators over sets. Suppose also that the VC dimension of $\HH_T$ (viewed as a collection of sets) is $V_T$. Define 
	\begin{align}V_{\max}=\min_{\begin{minipage}[h]{3.5cm}\centering $k$,~partitions\\$T_1 \sqcup T_2 \sqcup \ldots \sqcup T_k = [d]$\end{minipage}} \left\{ k^2 \cdot\max_{i=1,\ldots,k} V_{T_i}\right\}. \label{eq:costas}
	\end{align}
	Assume that the optimum of~\eqref{eq:costas} is attained at $k=\tilde{k}$ for partition $\tilde{T}_1 \sqcup \tilde{T}_2 \sqcup \ldots \sqcup \tilde{T}_{\tilde{k}} = [d]$. 
	
	Then
	%
	%Theorem~\ref{thm:uniform convergence for product measure PARTITION} implies that 
	$\ell = O\left(\frac{V_{\max}}{\epsilon^2}\cdot \ln \frac{\tilde{k}}{\epsilon}+\frac{\tilde{k}^2}{\epsilon^2}\cdot \ln \frac{\tilde{k}}{\delta} \right)$ samples from $\times_{i\in[d]}\DD_i$ suffice to obtain $(\epsilon,\delta)$-p.m. uniform convergence for $\HH$. 
	Formally,
	$$\Pr_{\boldsymbol{z}^{(1)},\ldots, \boldsymbol{z}^{(\ell)}}\left[\sup_{f\in \HH}\left|\E_{\boldsymbol{z}\sim \times_{i\in[\tilde{k}]}\hat{\DD}_{\tilde{T}_i}}\left[f(\boldsymbol{z})\right] - \E_{\boldsymbol{z}\sim \times_{i\in[d]}{\DD}_i}\left[f(\boldsymbol{z})\right]\right|\leq \epsilon\right]\geq 1-\delta,$$
	where for a given sample $\boldsymbol{z}^{(1)},\ldots, \boldsymbol{z}^{(\ell)}$ from a product distribution $\times_{i\in[d]} \DD_i$ the distributions $\hat{\DD}_{\tilde{T}_i}$ are defined to be uniform over ${\boldsymbol{z}}^{(1)}_{\tilde{T}_i},\ldots,{\boldsymbol{z}}^{(\ell)}_{\tilde{T}_i}$, where ${\boldsymbol{z}}^{(j)}_{\tilde{T}_i}$ contains the entries of sample $\boldsymbol{z}^{(j)}$ in coordinates $\tilde{T}_i$.
\end{corollary}

Table~\ref{tab:productVC} compares the sample complexity for uniform convergence implied by Theorem~\ref{thm:uniform convergence for product measure PARTITION} and Corollary~\ref{cor:VC for product measure} to that implied by VC theory, when the underlying measures are product. Suppose $\HH$ contains the indicator functions of all convex sets in $\mathbb{R}^d$. VC theory does not provide any finite sample  bound for  uniform convergence, as the VC dimension of $\HH$ is $\infty$. Do our results provide a finite bound? Notice that, for all $i$, $\HH_i$  simply contains all intervals in $\mathbb{R}$. Hence, $V_i=2$ and Corollary~\ref{cor:VC for product measure} implies that $\ell = O(\frac{d^2}{\epsilon^2}\cdot \left(\log \frac{d}{\delta}+\log \frac{d}{\epsilon}\right) )$ samples  suffice to obtain $(\epsilon,\delta)$-p.m.~uniform convergence for $\HH$ . In fact, our sample complexity bound can be improved to $O\left(\frac{d^2}{\epsilon^2}\cdot \log \frac{d}{\delta}\right)$, as $O\left(\frac{\log \frac{1}{\delta}}{\epsilon^2}\right)$ samples suffice to guarantee $(\epsilon,\delta)$-uniform convergence for all intervals in $\mathbb{R}$ due to the DKW inequality~\cite{DvoretzkyKW56}. 

%\todo{Expand the discussion here.}

In the next a few sections, we apply our uniform convergence results to learn a mechanism with approximately optimal revenue. A type of events called \emph{single-intersecting} (see Definition~\ref{def:single-intersecting})   plays a key role in our analysis. These events are defined based on the geometric shape of the corresponding sets. For example, balls, rectangles and all convex sets are single-intersecting, but this definition includes some non-convex sets as well, for example, ``cross-shaped'' sets. It turns out that being able to handle these non-convex sets is crucial for our results, as many events we care about are not convex but nonetheless are single-intersecting. 

\begin{definition}[Single-intersecting Events]\label{def:single-intersecting}
For any event $\EE$ in $\mathbb{R}^{\ell}$, $\EE$ is \textbf{single-intersecting} if the intersection of $\EE$ and any line that is parallel to one of the axes is an interval. More formally, for any $i\in[\ell]$ and any line $L_i=\left\{x \in \mathbb{R}^{\ell}\ |\ x_{-i}=a_{-i} \right \}$, where $a_{-i} \in \mathbb{R}^{\ell-1}$, the intersection of $L_i$ and $\EE$ is of the form $\left\{x \in \mathbb{R}^{\ell}\ |\ x_{-i}=a_{-i}, x_i\in[\ubar{a},\bar{a}] \right\}$ where $\ubar{a}\leq \bar{a}$. In particular, we allow $\ubar{a}$ to be $-\infty$ and $\bar{a}$ to be $+\infty$.
\end{definition}


\noindent We establish a uniform convergence bound for single-intersecting events by combing the DKW inequality and Theorem~\ref{thm:uniform convergence for product measure}.

\begin{lemma}\label{lem:uniform convergence for single-intersecting}
	For any integer $\ell$, let $\HH$ be the hypothesis class that contains all indicator functions for single-intersecting events in $\mathbb{R}^\ell$. Then $\HH$ has $(\epsilon,\delta)$-p.m. uniform convergence with sample complexity $O\left(\frac{\ell^2}{\epsilon^2}\cdot \log \frac{\ell}{\delta}\right)$.
\end{lemma}
\begin{proof}
	As the projected hypothesis class for the $i$-th coordinate simply contains all intervals in $\R$, the sample complexity for $(\epsilon,\delta)$-uniform convergence is $O({1 \over \epsilon^2}\cdot \log \frac{1}{\delta})$ due to the DKW inequality. The claim follows from Theorem~\ref{thm:uniform convergence for product measure}.
\end{proof}

Next, we show a slightly stronger statement, which is a type of uniform convergence bound when access to approximate distributions is given. More specifically, we argue that for any single-intersecting event, the difference in the probability of this event under two product distributions $\DD=\times_{i\in[\ell]}\DD_i$ and $\hat{\DD}=\times_{i\in[\ell]}\hat{\DD}_{i}$ is at most $2\xi\cdot \ell$, if $||\DD_i-\hat{\DD}_i||_K\leq \xi$ for all $i$. It is not hard to see that Lemma~\ref{lem:Kolmogorov stable for sc} and the DKW inequality imply Lemma~\ref{lem:uniform convergence for single-intersecting}.

\begin{lemma}\label{lem:Kolmogorov stable for sc}
For any integer $\ell$, let $\DD=\times_{i=1}^\ell \DD_i$ and $\hat{\DD}=\times_{i=1}^\ell \hat{\DD}_i$, where $\DD_i$ and $\hat{\DD}_i$ are both supported on $\mathbb{R}$ for any $i\in[\ell]$. If $||\DD_i-\hat{\DD}_i||_K\leq \xi$, $\left|\Pr_\DD[\EE]-\Pr_{\hat{\DD}}[\EE]\right|\leq 2\xi\cdot \ell$ for any single-intersecting event $\EE$.
\end{lemma}
\begin{proof}
	Let $\HH=\left\{\ind_{x\in \EE} :\EE\text{ is \emph{single-intersecting}}\right\}$. By the definition of single-intersecting events, $\HH_i$ is the set of the indicator functions of all intervals in $\mathbb{R}$ for any $i\in [\ell]$. Since $||\DD_i-\hat{\DD}_i||_K\leq \xi$, $$\sup_{g\in\HH_i}\left|\E_{x\sim \DD_i}[g(x)]-\E_{x\sim \hat{\DD}_i}[g(x)]\right|\leq 2\xi.$$ By Lemma~\ref{lem:approx product dist}, $$\sup_{f\in\HH} \left|\E_{\boldsymbol{x}\sim \DD}\left[f(\boldsymbol{x})\right]-\E_{\boldsymbol{x}\sim \hat{\DD}}\left[f(\boldsymbol{x})\right]\right|\leq 2\xi\cdot \ell.$$
\end{proof}

The following table (Table~\ref{tab:productVC}) summarizes some uniform convergence bounds implied by our results in this section.


\begin{table}[h]
	\centering
		\begin{tabular}{c || c | c}
			
			\hline\hline
			Hypotheses Class & \begin{minipage}[h]{4cm}\centering VC Bound\end{minipage} & \begin{minipage}[h]{7cm}\centering  Bounds from Theorem~\ref{thm:uniform convergence for product measure PARTITION} and Corollary~\ref{cor:VC for product measure}\end{minipage}\\
			\hline
			& &\\
			 axis-aligned rectangles in $\mathbb{R}^d$ &  $\tilde{O}(d /\epsilon^2)$ & $\tilde{O}(d /\epsilon^2)$	\\		
			polytopes with $k$ facets in $\mathbb{R}^d$ &  $\tilde{O}(d k /\epsilon^2)$ & $\tilde{O}(d \cdot \min\{d,k\} /\epsilon^2)$\\
			arbitrary convex sets in $\mathbb{R}^d$ & $\infty$ & $\tilde{O}(d^2 /\epsilon^2)$\\
			single-intersecting sets in $\mathbb{R}^d$ & $\infty$ & $\tilde{O}(d^2 /\epsilon^2)$
		\end{tabular}
	\caption{Number of samples required for $(\epsilon,\Theta(1))$-p.m. uniform convergence for different ${\cal H}$'s.}
	\label{tab:productVC}
\end{table}