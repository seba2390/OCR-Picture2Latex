\centering
% \Rotatebox{90}{%
\captionof{table}{\textbf{Summary of GAN Papers}}
\begin{adjustbox}{max width = 9cm}
\label{tab:2}
\begin{tabular}{|l|l|l|l|}
\hline 
 \textbf{Paper Name }& \textbf{Problems Raised} & \textbf{Novel approach} & \textbf{Comment} \\\hline
On Distinguish-ability Criteria & \textcolor{purple}{ The expected gradient of generator}  & \textcolor{purple}{In gans G being the primary model} &  \\
   For Estimating& \textcolor{purple}{does not  match that of MLE.}&\textcolor{purple}{causes it to differ from MLE.}& \\
Generative Models \cite{2014Goodfellow} & Non convergence of independent SGD& No close relation between NCE and GAN.& \\
&results in under-fitting in gans& &\\
&&& \\
June & Improved Techniques  & \textcolor{red}{Overtraining of the discriminator}  & \textcolor{red}{Feature Matching}  \\
2016 &for GANs \cite{goodfellow16} &\textcolor{purple}{Mode collapse of Generator} & \textcolor{purple}{Mini-batch Discrimination} \\
&& \textcolor{orange}{Gradient descent may not converge} & \textcolor{orange}{Historical Averaging (Fictitious play)}\\
& & \textcolor{magenta}{Vulnerable to adversarial examples.} & \textcolor{magenta}{Label-smoothing} \\
&&\textcolor{brown}{GAN outputs depend on the inputs} & \textcolor{brown}{Virtual Batch normalization} \\
&& \textcolor{brown}{within a same batch due to BN} &\\
& & & \\
Jan& Towards Principled Methods & \textcolor{purple}{Perfect Discriminator resulting in} & \textcolor{purple}{Softer Metrics- Adding Gaussian}\\
2017 &for Training GANs \cite{arjovsky01} & \textcolor{purple}{zero grads when distributions are  in} & \textcolor{purple}{Noise (Contrastive Divergence)} \\
&&\textcolor{purple}{dimensional manifolds}& \textcolor{purple}{Earth-Mover Distance}\\
&&The $-\log D$ alternative causes-  & \\
&&unstable updates& \\
& & & \\
Mar & WGAN \cite{arjovskyWGAN}& Learning distributions supported& Define EM Distance (KR Duality) with well  \\
2017 && in lower dimension manifolds.& defined gradients and avoids balancing \\
&& JSD maxes and $KL\rightarrow\infty$& the critic and generator. Consistent decrease\\
&& Mode Collapse &in loss with training. Weakest metric\\
&&& \\
May & Improved Techniques &Lipschitz enforced by weight clipping. & Introduced a Gradient Penalty term. \\
2017 &on WGAN  \cite{gularajani} & Capacity underuse (critic has high & wrt sample as following was observed.\\
& & bias), Exploding and&Optimal critic has unit-grad \\
&& vanishing gradients. & norm between $\mathbb{P}_{r}$ and $\mathbb{P}_{g}$\\
& & & \\

Mar& Loss-Sensitive GAN on & \textcolor{purple}{Over-pessimistic D as fake samples} & \textcolor{purple}{Loss having a data-dependent margin.} \\
2017& Lipschitz Densities\cite{ls-gan}& \textcolor{purple}{closer to data are still negative} & \\
&&\textcolor{magenta}{Assuming infinite capacity which leads}& \textcolor{magenta}{Limited to the space of}\\
&&\textcolor{magenta}{to mode collapse and vanishing grads}&\textcolor{magenta}{Lipschitz-continuous functions}\\
&&\textcolor{orange}{WGAN is unbounded from above as} & \textcolor{orange}{Pairwise comparisons as against WGAN}\\
&& \textcolor{orange}{critic is minimized over gen samples}&\textcolor{orange}{which decomposes the loss}\\
&&&\textcolor{orange}{into two first order moments}\\
&&&\\
& & & \\
ICLR & EBGAN \cite{eb-gan} & With binary logistic loss only two & Auto-encoder Discriminator to evade    \\
2017 & &targets are possible, so gradients in & the need of negative samples \\
&&a mini batch are far from orthogonal.& Repelling regularizer \\
&&Mode Collapse.& to prevent mode collapse\\
& & & \\

Feb & Least Squares GAN \cite{lsgan}& Vanishing gradients with sigmoid & Pearson $\chi^2$ Divergence. The following  \\
2017&&cross-entropy loss when updating & constants , a- encoding for real data\\
&&the gen using fake samples&b - encoding for fake data \\
&&that are on the correct side & c- encoding for value that G wants\\
&&of decision boundary.& D to believe is fake.\\
&&&\\
June & f-GAN\cite{fgan}&   & General $f$-divergence \\
2016&&\textcolor{red}{No convergence to saddle point} & variational estimation of $f-$ divergence \\
&&\textcolor{red}{when using single step GD} & \textcolor{red}{Variational Divergence Minimization}\\
&&& \\
Nov & Stabilizing Training of\cite{fgan_2} & Empirical estimation& Convolving with noise \\
2017 &  GANs through Regularization & Density misspecification& Noise induced regularization \\
&&Dimensional misspecification &\\
&&Addition of high dim noise introduces &\\
&&huge variance in parameter estimation&\\
&&&\\
NIPS& The Numerics of GANs \cite{geiger06} & Failure of simultaneous  &
Consensus Optimization: Alternative method  \\
 2017 & &gradient descent as the eigenvalues &  for finding the Nash Equilibrium\\
 &&gradients have zero real part &Introduced norm of the gradients\\
 &&and large imaginary part&w.r.t parameters in the loss.\\
 
% \hline 
% \end{tabular}
% \end{adjustbox}
% \newpage

% \begin{adjustbox}{max width = 9cm}
% \begin{tabular}{|l|l|l|l|}
% \hline
Nov&Unrolled GANs \cite{unrolled} & Mode collapse, Oscillations& Training the D to optimality\\
2016&&of G and D. G tries to move much mass to single & is expensive.A surrogative loss \\
&&point, D tracks it and assigns lower probability& which in limit equals optimal D \\
&&The cycle continues forever.& The G's updates consider future\\
&&&steps of the discriminator.\\
&&&\\
Aug& Generalization and Equilibrium &\textcolor{purple}{Generalization is not guaranteed i.e.} &\textcolor{purple}{NN Distance which generalizes} \\
2017 &in GANs \cite{Arora03}&\textcolor{purple}{$d(\mathcal{D}_{real}, \mathcal{D}) = 0$ but  $d(\mathcal{\hat{D}}_{real}, \mathcal{\hat{D}}) \neq 0$}&\textcolor{purple}{as it assumes finite capacity D} \\
&& \textcolor{purple}{where $\hat{D}$ is the empirical distribution} & \textcolor{purple}{At the cost of diversity of samples.}\\
&&\textcolor{red}{Non Existence of equilibrium in GAN} &\textcolor{red}{Mixed Strategy Nash using}  \\
&&&\textcolor{red}{mixture of Generators which} \\
&&& \textcolor{red}{on folding results in pure NE.}\\
&&&\\
May & On Convergence and Stability& Gradient descent is unstable and & Regret Minimization converges to  \\
 2017 & in GANs \cite{DRAGAN} &results in mode collapse &$\epsilon$-equilibrium in non-convex case.  \\
& &Divergence minimization hypothesis doesn't  & Sharp gradients results in  \\ 
&& explain gan learning swiss roll distribution & mode collapse hence a penalty\\
&&WGAN and LS-GAN regularize the discriminator's& term is introduced.\\
&&gradients in domain space.&\\
&&&\\
Nov & Gradient Descent GAN & It is assumed that updates occur  & local asymptotic stability of GAN  \\
 2017 &  Optimization is locally stable & in functional space, with models having & 
(not WGAN)  optimization under   \\
&  &infinite capacity. Discriminator is assumed  & 
proper conditions, gradient-based  \\
&  &to be fully optimized between gen updates.  & 
 regularizer(GAN + WGAN)  \\
&&&\\
\hline 
\end{tabular}
\end{adjustbox}