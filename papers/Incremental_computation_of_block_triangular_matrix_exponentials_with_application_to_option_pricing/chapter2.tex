\section{Incremental scaling and squaring}
\label{sec:scaling_and_squaring}

Since the set of conformally partitioned block triangular matrices
forms an algebra, and $\exp(G_n)$ is a polynomial in $G_n$, the matrix
$\exp(G_n)$ has the same block upper triangular structure as $G_n$, that is,
\begin{equation*}
    \exp(G_n) =
    \begin{bmatrix}
        \exp(G_{0,0}) & \ast          & \cdots & \ast \\
                      & \exp(G_{1,1}) & \ddots & \vdots \\
                      &               & \ddots & \ast  \\
                      &               &        & \exp(G_{n,n}) \\
    \end{bmatrix}
    \in \R^{d_n \times d_n}.
\end{equation*}
As outlined in the introduction, we aim at computing $\exp(G_n)$ block
column by block column, from left to right.  Our algorithm is based on
the scaling and squaring methodology, which we briefly summarize
next.

\subsection{Summary of the scaling and squaring method}
\label{sec:scaling_squaring}

The scaling and squaring method uses a rational function to
approximate the exponential function, and typically involves three
steps. Denote by $r_{k,m}(z) = \frac{p_{k,m}(z)}{q_{k,m}(z)}$ the
$(k,m)$-Pad\'e approximant to the exponential function, meaning that
the numerator is a polynomial of degree $k$, and the denominator is a
polynomial of degree $m$. These Pad\'e approximants are very accurate close
to the origin, and in a first step the input matrix $G$ is therefore scaled by a
power of two, so that $\norm{2^{-s}G}$ is small enough to guarantee an
accurate approximation $r_{k,m}(2^{-s}G) \approx \exp(2^{-s} G)$.

The second step consists of evaluating the rational approximation
$r_{k,m}(2^{-s}G)$, and, finally, an approximation
to $\exp(G)$ is obtained in a third step by repeatedly squaring the
result, i.e.,
\begin{equation*}
    \exp(G) \approx r_{k,m}(2^{-s}G)^{2^s}.
\end{equation*}

Different choices of the scaling parameter $s$, and of the
approximation degrees $k$ and $m$ yield methods of different
characteristics. The choice of these parameters is critical for
the approximation quality, and for the computational efficiency,
see~\cite[chapter~10]{Higham2008}.

In what follows we describe techniques that allow for an incremental
evaluation of the matrix exponential of the block triangular
matrix~\eqref{eq:G_n}, using scaling and squaring.  These techniques
can be used with any choice for the actual underlying scaling and
squaring method, defined through the parameters $s$, $k$, and $m$.


% The idea developed in our algorithm can be employed for every type of
% scaling and squaring method. In fact, our algorithm is mainly based on
% the modification of the basic operations performed in the scaling and
% squaring method. For instance, the computational effort of the
% algorithm consists in building the powers of the scaled input matrix
% $2^{-s}G$ (when we compute $p_{k,m}(2^{-s}G)$ and $q_{k,m}(2^{-s}G)$);
% in evaluating the inversion $p_{k,m}(2^{-s}G)^{-1}q_{k,m}(2^{-s}G)$
% and in the squaring phase. In each of these operations the structure
% of the input matrix is exploited and past stored quantities are
% reused.

\subsection{Tools for the incremental computation of exponentials}
\label{sec:tools}
Before explaining the algorithm, we first introduce some notation that
is used throughout.  The matrix $G_n$ from~\eqref{eq:G_n} can be written as
\begin{equation}
    \label{eq:G_twobytwo}
    G_n =
\left[
    \begin{array}{ccc|c}
        G_{0,0} &  \cdots &G_{0,n-1}& G_{0,n} \\
                &  \ddots & \vdots & \vdots  \\ 
                &   & G_{n-1,n-1}& G_{n-1,n} \\ \hline
                &         & & G_{n,n} \\
    \end{array}
\right]
    \bydef
    \begin{bmatrix}
        G_{n-1} & g_n \\
        0       & G_{n,n}
    \end{bmatrix}
\end{equation}
where $G_{n-1} \in \mathbb{R}^{d_{n-1} \times d_{n-1}}$, $G_{n,n}\in
\mathbb{R}^{b_n \times b_n}$, so that $g_n \in \R^{d_{n-1} \times
b_n}$.  Let $s$ be the scaling parameter, and $r = \frac{p}{q}$ the
rational function used in the approximation (for simplicity we will often omit the indices $k$ and $m$).  We denote the scaled
matrix by $\tilde{G}_n \defby 2^{-s} G_n$ and we partition it as
in~\eqref{eq:G_twobytwo}.\

The starting point of the algorithm consists in computing the Pad\'e approximant of the exponential $\exp(G_0)= \exp(G_{0,0})$, using a scaling and squaring method. Then, the sequence of matrix exponentials~\eqref{eq:exp_sequence} is incrementally computed by reusing at each step previously obtained quantities. So more generally, assume that $\exp(G_{n-1})$ has been approximated by using a scaling and squaring
method.  The three main computational steps for obtaining the Pad\'e approximant of $\exp(G_n)$ are
\begin{inparaenum}[(i)]
\item evaluating the polynomials $p(\tilde{G}_n)$, $q(\tilde{G}_n)$,
\item evaluating $p(\tilde{G}_n)^{-1} q(\tilde{G}_n)$, and
\item repeatedly squaring it.
\end{inparaenum}
We now discuss each of these steps separately, noting the quantities to keep at every iteration.

\subsubsection{Evaluating $p(\tilde{G}_n)$, $q(\tilde{G}_n)$ from $p(\tilde{G}_{n-1})$, $q(\tilde{G}_{n-1})$}
Similarly to \eqref{eq:G_twobytwo}, we start by writing $P_n\defby p(\tilde{G}_n)$ and $Q_n\defby q(\tilde{G}_n)$ as
\begin{equation*}
    P_n = 
    \begin{bmatrix}
        P_{n-1} & p_n \\
        0       & P_{n,n}
    \end{bmatrix},
    \quad
    Q_n = 
    \begin{bmatrix}
        Q_{n-1} & q_n \\
        0       & Q_{n,n}
    \end{bmatrix}.
\end{equation*}
In order to evaluate $P_{n}$, we first need to compute monomials of $\tilde{G}_{n}$, which for $l = 1, \dotsc, k$, can be written as
\begin{equation*}
    \tilde{G}_{n}^l =
    \begin{bmatrix}
        \tilde{G}_{n-1}^l & \sum_{j=0}^{l-1}
            \tilde{G}_{n-1}^j \tilde{g}_{n} \tilde{G}_{n,n}^{l-j-1} \\
        &                   \tilde{G}_{n,n}^l
    \end{bmatrix}.
\end{equation*}
Denote by $X_l \defby \sum_{j=0}^{l-1} \tilde{G}_{n-1}^j
\tilde{g}_{n} \tilde{G}_{n,n}^{l-j-1}$ the upper off diagonal block of
$\tilde{G}_n^l$, then we have the relation
\begin{equation*}
    X_{l} = \tilde{G}_{n-1}X_{l-1} + \tilde{g}_n \tilde{G}_{n,n}^{l-1}, \quad \text{for } l = 2, \cdots, k,
\end{equation*}
with $X_{1} \defby \tilde{g}_n$, so that all the monomials $\tilde{G}_n^l$, $l = 1, \dotsc, k$, can be
computed in $\bigO(b_n^3 + d_{n-1} b_n^2 + d_{n-1}^2 b_n)$.  Let $p(z)
= \sum_{l=0}^k \alpha_l z^l$ be the numerator polynomial of $r$, then
we have that
\begin{equation}
    \label{eq:P_n}
    P_n = 
    \begin{bmatrix}
        P_{n-1} & \sum_{l=0}^{k} \alpha_l X_l\\
                & p(\tilde{G}_{n,n})
    \end{bmatrix},
\end{equation}
which can be assembled in $\bigO(b_n^2 + d_{n-1} b_n)$, since only the
last block column needs to be computed. The complete evaluation of $P_n$ is summarized in Algorithm~\ref{alg:P_n}.
\begin{algorithm}[ht]
    \caption{Evaluation of $P_n$, using $P_{n-1}$
    \label{alg:P_n}}
    \begin{algorithmic}[1]
        \REQUIRE $G_{n-1}, G_{n,n}, g_n, P_{n-1}$, Pad\'e coefficients $\alpha_l, l = 0, \cdots, k$.
        \ENSURE $P_n$.
        \STATE $\tilde{g}_n \leftarrow 2^{-s} g_n$,
            $\tilde{G}_{n,n} \leftarrow 2^{-s} G_{n,n}$,
	 $\tilde{G}_{n-1} \leftarrow 2^{-s} G_{n-1}$
        \STATE $X_1 \leftarrow \tilde{g}_n$
        \FOR {$l=2, 3,  \cdots, k$}
	\STATE Compute $\tilde{G}_{n,n}^l$
         	\STATE $X_l = \tilde{G}_{n-1} X_{l-1} + \tilde{g}_n \tilde{G}_{n,n}^{l-1}$
        \ENDFOR
        \STATE $X_0 \leftarrow \mathbf{0}_{d_{n-1} \times b_n}$
        \STATE Compute off diagonal block of $P_n$:  $\sum_{l=0}^{k} \alpha_l X_l$.
        \STATE Compute $p(\tilde{G}_{n,n}) = \sum_{l=0}^{k} \alpha_l \tilde{G}_{n,n}^l$
        \STATE Assemble $P_n$ as in $\eqref{eq:P_n}$
    \end{algorithmic}
\end{algorithm}

Similarly, one computes $Q_n$ from $Q_{n-1}$, using again the matrices
$X_l$.
\subsubsection{Evaluating $Q_n^{-1} P_n$}

With the matrices $P_n$, $Q_n$ at hand, we now need to compute the
rational approximation $Q_n^{-1} P_n$. 
We assume that $Q_n$ is well
conditioned, in particular non-singular, which is ensured by the choice of the 
scaling parameter and of the Pad\'e approximation, see, e.g.,~\cite{Higham2009}.
We focus on the computational cost.
For simplicity, we introduce the notation
\begin{equation*}
    \tilde{F}_n = 
    \begin{bmatrix}
        \tilde{F}_{0,0} &  \cdots & \tilde{F}_{0,n} \\
                        &  \ddots & \vdots  \\
                        &         & \tilde{F}_{n,n} \\
    \end{bmatrix}
    \defby Q_n^{-1} P_n,
    \quad
    F_n = 
    \begin{bmatrix}
        F_{0,0} &  \cdots & F_{0,n} \\
                &  \ddots & \vdots  \\
                &         & F_{n,n} \\
    \end{bmatrix}
    \defby \tilde{F}_n^{2^s},
\end{equation*}
and we see that
\begin{equation}
    \label{eq:F_n_tilde}
    \begin{split}
    \tilde{F}_n & = Q_n^{-1} P_n =
    \begin{bmatrix}
        Q_{n-1}^{-1} & - Q_{n-1}^{-1} q_n Q_{n,n}^{-1} \\
        0            & Q_{n,n}^{-1}
    \end{bmatrix}
    \begin{bmatrix}
        P_{n-1} & p_n \\
        0       & P_{n,n}
    \end{bmatrix}\\
    & =
    \begin{bmatrix}
        \tilde{F}_{n-1} & Q_{n-1}^{-1} ( p_n - q_n Q_{n,n}^{-1} P_{n,n} ) \\
        0               & Q_{n,n}^{-1} P_{n,n}
    \end{bmatrix}.
    \end{split}
\end{equation}

To solve the linear system $Q_{n,n}^{-1} P_{n,n}$ we compute an LU
decomposition with partial pivoting for
$Q_{n,n}$, requiring $\bigO(b_n^3)$ operations.
This LU decomposition is saved for future use, and hence we may assume
that we have available the LU decompositions for all diagonal
blocks from previous computations:
\begin{equation}
    \label{eq:store_lu}
    \Pi_l Q_{l,l}= L_l U_l, \quad l=0, \dotsc, n-1.
\end{equation}
Here, $\Pi_l \in \mathbb{R}^{b_l \times b_l},$ $l=0, \dotsc, n-1$ are permutation matrices; 
$L_l\in \mathbb{R}^{b_l \times b_l},$ $l=0, \dotsc, n-1$ are lower triangular matrices and $U_l\in \mathbb{R}^{b_l \times b_l},$ $l=0, \dotsc, n-1$ are upper triangular matrices.

Set $Y_n \defby p_n - q_n Q_{n,n}^{-1} P_{n,n} \in \R^{d_{n-1} \times
b_n}$, and partition it as
\begin{equation*}
    Y_n = 
    \begin{bmatrix}
      Y_{0,n} \\
      \vdots  \\
      Y_{n-1,n} \\
    \end{bmatrix}.
\end{equation*}
Then we compute $Q_{n-1}^{-1} Y_n$ by block backward
substitution, using the decompositions of the diagonal blocks.  The
total number of operations for this computation is hence $\bigO(d_{n-1}^2 b_n + d_{n-1} b_n^2)$, so that the number of
operations for computing $\tilde{F}_n$ is $\bigO(b_n^3 + d_{n-1}^2
b_n + d_{n-1} b_n^2)$.
Algorithm~\ref{alg:tilde_F_n} describes the complete procedure to compute $\tilde{F}_n$.

\begin{algorithm}[ht]
    \caption{Evaluation of $\tilde{F}_n = Q_n^{-1} P_n$}
    \label{alg:tilde_F_n}
    \begin{algorithmic}[1]
        \REQUIRE $Q_n, P_n$ and quantities \eqref{eq:store_lu}
        \ENSURE $\tilde{F}_n = Q_n^{-1} P_n$ and LU decomposition of $Q_{n,n}$.
        \STATE Compute $\Pi_n Q_{n,n}= L_n U_n$ and keep it for future use \eqref{eq:store_lu}
        \STATE Compute  $\tilde{F}_{n,n} \defby Q_{n,n}^{-1} P_{n,n}$
        \STATE $Y_n = p_n - q_n Q_{n,n}^{-1} P_{n,n}$
        \STATE $\tilde{F}_{n-1,n} = U_{n-1}^{-1} L_{n-1}^{-1} \Pi_{n-1} Y_{n-1,n} $
        \FOR {$l = n - 2, n - 3,  \cdots, 0$}
        \STATE $\tilde{F}_{l, n} = U_{l}^{-1}L_{l}^{-1} \Pi_{l}(Y_{l, n}-\sum_{j=l+1}^{n-1} Q_{l,j}\tilde{F}_{j, n}) $
        \ENDFOR
        \STATE Assemble $\tilde{F}_n$ as in \eqref{eq:F_n_tilde}
    \end{algorithmic}
\end{algorithm}
\subsubsection{The squaring phase}

Having computed $\tilde{F}_n$, which we write as
\begin{equation*}
    \tilde{F}_n =
    \begin{bmatrix}
        \tilde{F}_{n-1} & \tilde{f}_n \\
                        & \tilde{F}_{n,n}
    \end{bmatrix},
\end{equation*}
we now need to compute $s$ repeated squares of that matrix, i.e.,
\begin{equation}
    \label{eq:squares}
    \tilde{F}_{n}^{2^l} =
    \begin{bmatrix}
        \tilde{F}_{n-1}^{2^l} & \sum_{j=0}^{l-1}
            \tilde{F}_{n-1}^{2^{l - 1 + j}} \tilde{f}_n \tilde{F}_{n,n}^{2^j}\\
                              & \tilde{F}_{n,n}^{2^l}
   \end{bmatrix}, \quad l = 1, \dotsc, s,
\end{equation}
so that $F_n = \tilde{F}_{n}^{2^s}$.  Setting $Z_l \defby
\sum_{j=0}^{l-1} \tilde{F}_{n-1}^{2^{l - 1 + j}} \tilde{f}_j
\tilde{F}_{n,n}^{2^j}$, we have the recurrence
\begin{equation*}
    Z_l = \tilde{F}_{n-1}^{2^{l-1}} Z_{l-1} + Z_{l-1} \tilde{F}_{n,n}^{2^{l-1}},
\end{equation*}
with $Z_0 \defby \tilde{f}_n$.  Hence, if we have stored the
intermediate squares from the computation of $F_{n-1}$, i.e.,
\begin{equation}
    \label{eq:store_squares}
    \tilde{F}_{n-1}^{2^l},  \quad l=1, \dotsc, s
\end{equation}
we can compute all the quantities $Z_l$, $l=1, \dotsc, s$ in
$\bigO(d_{n-1}^2 b_n + d_{n-1} b_n^2)$, so that the total cost for
computing $F_n$ (and the intermediate squares of $\tilde{F}_n$) is
$\bigO(d_{n-1}^2 b_n + d_{n-1} b_n^2 + b_n^3)$. Again, we summarize the squaring phase in the following algorithm.

\begin{algorithm}[ht]
    \caption{Evaluation of $F_n = \tilde{F}_n^{2^s}$
    \label{alg:F_n}}
    \begin{algorithmic}[1]
        \REQUIRE $\tilde{F}_{n-1}, \tilde{f}_n, \tilde{F}_{n,n}$, quantities \eqref{eq:store_squares}.
        \ENSURE $F_n$ and updated intermediates.
        \STATE $Z_0 \leftarrow \tilde{f}_n$
        \FOR {$l=1, 2,  \cdots, s$}
          \STATE Compute $\tilde{F}_{n,n}^{2^l}$
          \STATE $Z_l = \tilde{F}_{n-1}^{2^{l-1}} Z_{l-1} + Z_{l-1} \tilde{F}_{n,n}^{2^{l-1}}$
          \STATE Assemble $\tilde{F}_n^{2^l}$ as in \eqref{eq:squares} and save it
        \ENDFOR
          \STATE $F_n \leftarrow \tilde{F}_{n}^{2^s}$
    \end{algorithmic}
\end{algorithm}
\subsection{Overall Algorithm}
\label{sec:overall}

Using the techniques from the previous section, we now give a concise
description of the overall algorithm.  We assume that the quantities
listed in equations~\eqref{eq:store_lu}
and~\eqref{eq:store_squares} are stored in memory, with a space requirement of $\bigO(d_{n-1}^2)$. 

In view of this, we assume that $F_{n-1}$ and the aforementioned intermediate
quantities have been computed.  Algorithm~\ref{alg:step} describes the
overall procedure to compute $F_n$, and to update the intermediates;
we continue to use the notation introduced in~\eqref{eq:G_twobytwo}.

\begin{algorithm}[ht]
    \caption{Computation of $F_n \approx \exp(G_n)$, using $F_{n-1}$
    \label{alg:step}}
    \begin{algorithmic}[1]
        \REQUIRE Block column $g_n$, diagonal block $G_{n,n}$, quantities \eqref{eq:store_lu},
        and~\eqref{eq:store_squares}.
        \ENSURE $F_n$, and updated intermediates.
        \STATE Extend $P_{n-1}$ to $P_n$ using Algorithm~\ref{alg:P_n},
        and form analogously $Q_n$
        \STATE Compute $\tilde{F}_n$ using Algorithm~\ref{alg:tilde_F_n}
        \STATE Evaluate $F_n = \tilde{F}_n^{2^s}$
        using Algorithm~\ref{alg:F_n}
    \end{algorithmic}
\end{algorithm}

As explained in the previous section, the number of operations for
each step in Algorithm~\ref{alg:step} is $\bigO(d_{n-1}^2 b_n +
d_{n-1} b_n^2 + b_n^3)$, using the notation at the beginning of
section~\ref{sec:tools}.  If $F_n$ were simply computed from scratch, without
the use of the intermediates, the number of operations for scaling and
squaring would be $\bigO((d_{n-1} + b_n)^3)$.  In the typical
situation where $d_{n-1} \gg b_n$, the dominant term in the latter
complexity bound is $d_{n-1}^3$, which is absent from the complexity
bound of Algorithm~\ref{alg:step}.

In order to solve our original problem, the computation of the
sequence $\exp(G_0)$, $\exp(G_1)$, $\exp(G_2)$, $\dotsc$, we use
Algorithm~\ref{alg:step} repeatedly; the resulting procedure is shown
in Algorithm~\ref{alg:full}.

\begin{algorithm}[ht]
    \caption{Approximation of $\exp(G_0), \exp(G_1), \dotsc$
    \label{alg:full}}
    \begin{algorithmic}[1]
        \REQUIRE Pad\'e approximation parameters $k$, $m$, and $s$
        \ENSURE $F_0 \approx \exp(G_0)$, $F_1 \approx \exp(G_1), \dotsc$
        \STATE Compute $F_0$ using scaling and squaring, store
            intermediates for Algorithm~\ref{alg:step}
        \FOR{$n=1,2,\dotsc$}
            \STATE Compute $F_{n}$ from $F_{n-1}$ using Algorithm~\ref{alg:step}
            \IF{termination criterion is satisfied} \RETURN
            \ENDIF
        \ENDFOR
    \end{algorithmic}
\end{algorithm}

We now derive a complexity bound for the number of operations
spent in Algorithm~\ref{alg:full}.  For simplicity of notation we
consider the case where all diagonal blocks are of equal size, i.e.,
$b_k \equiv b \in \N$, so that $d_k = (k+1)b$.  At iteration $k$ the
number of operations spent within Algorithm~\ref{alg:step} is thus 
$\bigO(k^2 b^3)$.  Assume that the termination criterion used in
Algorithm~\ref{alg:full} effects to stop the procedure after the
computation of $F_n$.  The overall complexity bound for the number of
operations until termination is $\bigO(\sum_{k=0}^n k^2 b^3 ) =
\bigO(n^3 b^3)$, which matches the complexity bound of applying
scaling and squaring only to $G_n \in \R^{(n+1)b \times (n+1)b}$,
which is also $\bigO( (nb)^3 )$.

In summary the number of operations needed to compute $F_n$ by
Algorithm~\ref{alg:full} is asymptotically the same as applying the
same scaling and squaring setting \emph{only} to compute $\exp(G_n)$,
while Algorithm~\ref{alg:full} incrementally reveals \emph{all}
exponentials $\exp(G_0)$, $\dotsc$, $\exp(G_n)$ in the course of the
iteration, satisfying our requirements outlined in the introduction.

\subsection{Adaptive scaling}

In Algorithms~\ref{alg:step} and~\ref{alg:full} we have assumed
that the scaling power $s$ is given as input parameter, and that it is
fixed throughout the computation of $\exp(G_0), \dotsc, \exp(G_n)$.
This is in contrast to what is usually intented in the scaling and squaring method, see Section~\ref{sec:scaling_squaring}.  On the one hand $s$
must be sufficiently large so that $r_{k,m}(2^{-s} G_l) \approx \exp(2^{-s}
G_l)$, for $0 \le l \le n$.  If, on the other hand, $s$ is chosen
\emph{too large}, then the evaluation of $r_{k,m}(2^{-s}G_l)$ may become
inaccurate, due to \emph{overscaling}.
So if $s$ is fixed, and the norms $\norm{G_l}$ grow with
increasing $l$, as one would normally expect, an accurate approximation cannot
be guaranteed for all $l$.

Most scaling and squaring designs hence choose $s$ in dependence of the
norm of the input matrix~\cite{Moler2003,Guttel2016,Higham2009}.  For
example, in the algorithm of Higham described in~\cite{Higham2009}, it
is the smallest integer satisfying
\begin{equation}
    \label{eqn:theta}
    \norm{2^{-s} G_l}_1 \le \theta \approx 5.37... .
\end{equation}
In order to combine our incremental evaluation techniques with this
scaling and squaring design, the scaling power $s$ must thus be chosen
dynamically in the course of the evaluation.  Assume that $s$
satisfies the criterion~\eqref{eqn:theta} at step $l-1$, but not at step
$l$.  We then simply discard all accumulated data structures from
Algorithm~\ref{alg:step}, increase $s$ to match the
bound~\eqref{eqn:theta} for $G_{l}$, and start
Algorithm~\ref{alg:full} anew with the \emph{repartitioned} input matrix
\begin{equation}
    \label{eqn:repart}
G_n = \left[
    \begin{array}{ccc|c|c|c}
        G_{0,0} &  \cdots &G_{0,l}  & G_{0,l+1}  & \cdots & G_{0,n}\\
                &  \ddots & \vdots  & \vdots     &        & \vdots\\ 
                &         & G_{l,l} & G_{l,l+1}  & \cdots & G_{l,n}\\ \hline
                &         &         & G_{l+1,l+1}& \cdots & G_{l+1,n}\\
                &         &         &            & \ddots & \vdots   \\
                &         &         &            &        & G_{n,n}  \\
    \end{array}
\right]
    =
    \underbrace{
\left[
    \begin{array}{c|c|c|c}
        \hat{G}_{0,0} & \hat{G}_{0,1}    & \cdots & \hat{G}_{0,n-l}\\ \hline
                      & \hat{G}_{1,1}    & \cdots & \hat{G}_{1,n-l}\\
                      &                  & \ddots & \vdots   \\
                      &                  &        & \hat{G}_{n-l,n-l}  \\
    \end{array}
\right]}_{\bydef \hat{G}_{n-l}}.
\end{equation}
The procedure is summarized in Algorithm~\ref{alg:adaptive}.

\begin{algorithm}[t]
    \caption{Approximation of $\exp(G_0), \exp(G_1), \dotsc$ with
    adaptive scaling
    \label{alg:adaptive}}
    \begin{algorithmic}[1]
        \REQUIRE Pad\'e approximation parameters $k$, $m$, norm
        bound $\theta$.
        \ENSURE $F_0 \approx \exp(G_0)$, $F_1 \approx \exp(G_1), \dotsc$
        \STATE $s \leftarrow \max\{0, \log(\norm{G_0}_1)\}$
        \STATE Compute $F_0$ using scaling and squaring, store
            intermediates for Algorithm~\ref{alg:step}
        \FOR{$l=1,2,\dotsc$}
            \IF{ $\norm{G_l}_1 > \theta$ }
                \STATE Repartition $G_n = \hat{G}_{n-l}$ as in~\eqref{eqn:repart}
                \STATE Restart algorithm with $\hat{G}_{n-l}$.
            \ENDIF
            \STATE Compute $F_{l}$ from $F_{l-1}$ using Algorithm~\ref{alg:step}
            \IF{termination criterion is satisfied} \RETURN
            \ENDIF
        \ENDFOR
    \end{algorithmic}
\end{algorithm}

It turns out that the computational overhead induced by this
restarting procedure is quite modest.  In the notation introduced for
the complexity discussion in Section~\ref{sec:overall}, the number of
operations for computing $\exp(G_n)$ by Higham's scaling and squaring
method is $\bigO(\log(\norm{G_n}_1) (nb)^3)$.  Since there are at
most $\log(\norm{G_n}_1)$ restarts in Algorithm~\ref{alg:adaptive},
the total number of operations for incrementally computing all
exponentials $\exp(G_0), \dotsc, \exp(G_n)$ can be bounded by a
function in $\bigO(\log(\norm{G_n}_1)^2 (nb)^3)$.  We assess the
actual performance of Algorithm~\ref{alg:adaptive} in
Section~\ref{sec:numerical_experiments}.

In our application from option pricing it turns out that the norms of
the matrices $G_l$ do not grow dramatically (see
Sections~\ref{sec:jacobix} and~\ref{sec:hestonx}) and quite accurate
approximations to all the matrix exponentials can be computed even if
the scaling factor is fixed (see Section~\ref{sec:exp_jacobi}).
