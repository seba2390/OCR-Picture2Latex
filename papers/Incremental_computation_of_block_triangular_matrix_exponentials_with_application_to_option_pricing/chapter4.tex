\section{Numerical experiments}
\label{sec:numerical_experiments}

We have implemented the algorithms described in this paper in {\sc
Matlab} and compare them with Higham's scaling and squaring method
from~\cite{Higham2009}, which typically employs a diagonal Pad\'e
approximation of degree $13$ and is referred to as ``\texttt{expm}''
in the following. The implementation of our algorithms for block
triangular matrices, Algorithm~\ref{alg:full} (fixed scaling
parameter), and Algorithm~\ref{alg:adaptive} (adaptive scaling
parameter), is based on the same scaling and squaring design and are
referred to as ``\texttt{incexpm}' in the following.  All experiments
were run on a standard laptop (Intel Core i5, 2 cores, 256kB/4MB L2/L3
cache) using a single computational thread.

\subsection{Random block triangular matrices}
\label{sec:random}

We first assess run time and accuracy on a randomly generated block
upper triangular matrix $G_n \in \R^{2491\times 2491}$.  There are $46$ diagonal
blocks, of size varying between $20$ and $80$.  The matrix is
generated to have a spectrum contained in the interval $[-80, -0.5]$,
and a well conditioned eigenbasis $X$ ($\kappa_2(X) \approx 100)$.

\begin{figure}[t]
\centering
    \includegraphics[width=0.49\textwidth]{Figures/exp_rand_time}
    \hfill
    \includegraphics[width=0.49\textwidth]{Figures/exp_rand_err}
    \caption{Comparison of \texttt{incexpm} and \texttt{expm} for a
    random block triangular matrix.  \emph{Left:} Cumulative run time
    for computing the leading portions. \emph{Right:} Relative error
    of \texttt{incexpm} w.r.t.~\texttt{expm}.\label{fig:exp_random}}
\end{figure}

Figure~\ref{fig:exp_random} (left) shows the wall clock time for the
incremental computation of all the leading exponentials. 
Specifically, given $0 \le l \le n$, each data point shows the time vs.~$d_l = b_0 + \cdots + b_l$ needed for computing the $l+1$ matrix exponentials $\exp(G_0), \exp(G_1), \dotsc, \exp(G_l)$ when using
\begin{itemize}
 \item \texttt{expm} (by simply applying it to each matrix separately);
 \item \texttt{incexpm} with the adaptive scaling strategy from
Algorithm~\ref{alg:adaptive};
\item \texttt{incexpm} with fixed scaling power $6$ (scaling used by \texttt{expm} for $G_0$);
\item \texttt{incexpm} with fixed scaling power $12$ (scaling used by \texttt{expm} for $G_n$).
\end{itemize}
As expected, \texttt{incexpm} is much faster than naively applying
\texttt{expm} to each matrix separately; the total times for $l = n$
are also displayed in Table~\ref{tab:exp_random}.  For reference we
remark that the run time of \textsc{Matlab}'s \texttt{expm} applied
only the final matrix $G_n$ is $13.65$s, which is very close to the
run time of \texttt{incexpm} with scaling parameter set to $12$ (see
Section~\ref{sec:overall} for a discussion of the asymptotic
complexity).  Indeed, a closer look at the runtime profile of \texttt{incexpm}
reveals that the computational overhead induced by the more
complicated data structures is largely compensated in the squaring
phase by taking advantage of the block triangular matrix structure,
from which \textsc{Matlab}'s \texttt{expm} does not profit
automatically.  It is also interesting to note that the run time of
the adaptive scaling strategy is roughly only twice the run time for
running the algorithm with a fixed scaling parameter $6$, despite its
worse asymptotic complexity.
\begin{table}[t]
\centering
    \caption{Run time and relative error attained by
    \texttt{expm} and \texttt{incexpm} on a random block triangular
    matrix of size $2491$.\label{tab:exp_random}}
    \begin{tabular}{l|r|r}
        Algorithm                  & Time (s) & Rel.~error\\  \hline
        \texttt{expm}              & 163.60   &\\
        \texttt{incexpm} (adaptive)&  20.01   & 3.27e-15\\
        \texttt{incexpm} ($s=6$)   &   9.85   & 2.48e-13\\
        \texttt{incexpm} ($s=12$)  &  13.70   & 6.17e-14\\
    \end{tabular}
\end{table}

The accuracy of the approximations obtained by \texttt{incexpm} is
shown on the right in Figure~\ref{fig:exp_random}.  We assume
\texttt{expm} as a reference, and measure the relative distance
between these two approximations, i.e.,
\begin{equation*}
    \frac{\norm{\texttt{expm}(G_l) - \texttt{incexpm}(G_l)}_F}
        {\norm{\texttt{expm}(G_l)}_F},
\end{equation*}
at each iteration $l$ (quantities smaller than the machine precision
are set to $u$ in Figure~\ref{fig:exp_random}, for plotting purpose).  One notes that the
approximations
of the adaptive strategy remain close to \texttt{expm} throughout the
sequence of computations.  An observed drop of the error down to $u$ for this strategy
corresponds to a restart in Algorithm~\ref{alg:adaptive}; the
approximation at this step is \emph{exactly} the same as the one of
\texttt{expm}.  Even for the fixed scaling parameters $6$ and $12$, the
obtained approximations are quite accurate.


\subsection{Application to option pricing}
\label{sec:exp_jacobi}

We now show results for computing option prices using
Algorithm~\ref{alg:jacobi} for the set of parameters 
\begin{align*}
    &v_0=0.04, \quad  x_0=0,\quad \sigma_w=0.5,\quad \mu_w=0,\quad \kappa=0.5,\quad \theta=0.04,\quad \sigma=0.15, 
    \\ &\rho=-0.5,\quad v_\text{min}=0.01,\quad v_\text{max}=1,\quad r=0,\quad \tau=1/4, \quad k=\log(1.1).
\end{align*}
We use the tolerance $\epsilon = 10^{-3}$ for stopping Algorithm~\ref{alg:jacobi}.

We explore the use of different algorithms for the computation of the matrix
exponentials in line~\ref{line:jacobiexp} of
Algorithm~\ref{alg:jacobi}: \texttt{incexpm} with adaptive scaling,
\texttt{incexpm} with fixed scaling parameter $s=7$ (corresponding to
the upper bound from Lemma~\ref{lemmanormJ} for $n = 60$), and
\texttt{expm}. Similar to Figure~\ref{fig:exp_random}, the observed
cumulative run times and errors are shown in
Figure~\ref{fig:exp_jacobi}. Again, \texttt{incexpm} is observed to be
significantly faster than \texttt{expm} (except for small matrix
sizes) while delivering the same level of accuracy.  Both
\texttt{incexpm} run times are also close to the run time of
\textsc{Matlab}'s \texttt{expm} applied only to the final matrix $\tau G_n$
($4.64$s).

\begin{figure}[t]
    \centering
    \includegraphics[width=0.49\textwidth]{Figures/exp_jacobi_time}
    \hfill
    \includegraphics[width=0.49\textwidth]{Figures/exp_jacobi_err}
    \caption{Comparison of \texttt{incexpm} and \texttt{expm} for the block upper triangular matrices arising in the context of the Jacobi model in Algorithm~\ref{alg:jacobi}.
    \emph{Left:} Cumulative run time for computing the leading
    portions. \emph{Right:} Relative error of \texttt{incexpm}
    w.r.t.~\texttt{expm}. \label{fig:exp_jacobi}}
\end{figure}

\begin{table}[t]
    \centering
    \caption{Total run time and option price errors for the
    Jacobi model for $n = 61$. \label{table_jacobi} }
    \begin{tabular}{l|r|r}
        Algorithm & Time (s) & Rel.~price error\\  \hline
        \texttt{expm}               &42.97 & 1.840e-03\\
        \texttt{incexpm} (adaptive) & 5.84 & 1.840e-03\\
        \texttt{incexpm} ($s=7$)    & 5.60 & 1.840e-03\\
    \end{tabular}
\end{table}

Table~\ref{table_jacobi} displays the impact of the different algorithm on the overall Algorithm~\ref{alg:jacobi}, in terms of execution time and accuracy. Concerning accuracy, we computed the relative error with respect to a reference option price computed by considering a truncation order $n = 100$.  It can be observed that there is no difference in accuracy 
for the three algorithms.

\begin{remark}
    \begin{rm}
    The block triangular matrices $G_n$ arising from the generator in
    the Jacobi model actually exhibit additional structure. They are quite sparse and the diagonal blocks
    are in fact permuted triangular matrices (this does not hold for
    polynomial diffusion models in general, though). For example, for $n=2$ the matrix $G_2$ in the Jacobi model is explicitly given by
\begin{equation*}
    G_2 = \left[
    \begin{array}{c|cc|ccc}
        0& r     &\kappa \theta &0&-\frac{\rho \sigma v_\text{max}v_\text{min}}{S}&-\frac{\sigma^2 v_\text{max}v_\text{min}}{S}\\ \hline
        & 0    & 0 &2r&\kappa \theta&0\\
        &  -\frac{1}{2} & -\kappa&1&r+\frac{\rho \sigma (v_\text{max}+v_\text{min})}{S}& 2\kappa\theta +\frac{\sigma^2( v_\text{max}+v_\text{min})}{S}\\ \hline
       &     &&0&0&0\\
        &     & &-1&-\kappa&0\\
        &    & &0&-\frac{1}{2}-\frac{\rho \sigma }{S}&-2\kappa-\frac{\sigma^2}{S}\\
    \end{array}
    \right],
\end{equation*}
    for $S\defby (\sqrt{v_{\max}}-\sqrt{v_{\min}})^2$.
    
    While the
    particular structure of the diagonal blocks is taken into account automatically by \texttt{expm} and
    \texttt{incexpm} when computing the LU decompositions of the diagonal blocks, it is not so easy to benefit from the sparsity.
    Starting from sparse matrix arithmetic, the matrix quickly becomes 
    denser during the evaluation of the initial rational
    approximation, and in particular during the squaring phase.  In
    all our numerical experiments we used a dense matrix
    representation throughout.
    \end{rm}
\end{remark}

We repeated the experiments above for the Heston instead of the Jacobi model, that is, we investigated the impact of using our algorithms for computing the matrix exponentials in Algorithm~\ref{algoSDP}. We found that the results for computing the matrix exponentials themselves look very similar to those for the Jacobi model (Figure~\ref{fig:exp_jacobi}), both
in terms of run time and accuracy, so we refrain from giving further
details here. There is, however, a notable difference. The
evaluation of the stopping criterion requires the solution of two SDPs, which quickly becomes a
computational challenge, eventually completely dominating the time needed for
the computation of the matrix exponentials. 
