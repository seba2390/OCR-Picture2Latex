\renewcommand{\arraystretch}{1.20}
\begin{table}[tb]
\caption{Number of images in BP and no-BP classes before data augmentation.}
\label{table:Nr_images}
\centering
\begin{tabular}{c|c c}
\hline
\textit{} & \textit{Non-filtered} & \textit{Filtered}\\ \hline
BP & 2322 & 1454 \\ 
No-BP & 3313 & 2452 \\ \hline
\end{tabular}
\end{table}

\begin{figure}[tb] 
    \centering
    \subfloat[Ultrasound Image of the neck\label{subfig:ExampleImage1}]{\includegraphics[width=0.45\linewidth]{figures/1_14.png}} \hspace{0.05\linewidth}
    \subfloat[Annotated Mask\label{subfig:ExampleMask1}]{\includegraphics[width=0.45\linewidth]{figures/1_14_mask.png}} \\
    \subfloat[Similar Ultrasound Image\label{subfig:ExampleImage2}]{\includegraphics[width=0.45\linewidth]{figures/1_13.png}}\hspace{0.05\linewidth}
    \subfloat[Incoherent Mask\label{subfig:ExampleMask2}]{\includegraphics[width=0.45\linewidth]{figures/1_13_mask.png}}
    \caption{(a,b) An example of an ultrasound image from the neck containing the brachial plexus nerve region with the corresponding annotated ground truth highlighting this region. (c,d) An example of similar ultrasound images with a complete different annotated brachial plexus nerve region.}
    \label{fig:example_images} 
\end{figure}


\subsection{Data}\label{section:data}
We use the data presented in \cite{KaggleDataset}. This dataset consisted of US images of the neck with their corresponding masks, as can be seen in Figure \ref{fig:example_images}. The images originate from $47$ different subjects, with 119-120 images per subject. All images had a resolution of 420x580 pixels. Trained volunteers annotated the binary masks indicating the BP. 

The dataset was found to contain contradictory annotations of close matching images. An example of this is shown in Figure \ref{fig:example_images}. A second dataset, here we call it filtered data, was constructed, in which the close matching samples without an annotated BP were removed.
Both the filtered and non-filtered data will be tested in the experiments. The dataset is unbalanced, with the no-BP class over-represented. The number of images in each class for both datasets are shown in Table \ref{table:Nr_images}. 

The data is then downsampled (128x128 in the CNN and 96x96 in the segmentation models). The images were mean-centered with a standard deviation of 1.  
In all experiments, 5-fold cross validation was used for the model evaluation. For each fold, the train data was randomly divided into a train and validation set by a 80\%-20\% split. To increase the number of samples used for training the CNN, we augmented training data by 2500 extra samples using affine transformations (scaling, shearing, rotating and reflecting) on randomly selected samples.

\renewcommand{\arraystretch}{1.2}
\begin{table*}[bth]
\caption{Experiment results (DSC) on filtered and non-filtered data.}
\label{table:DSC_Results}
\centering
\begin{tabular}{m{3cm} | m{1.5cm}  m{1.5cm} | m{1.5cm}  m{1.5cm} }
\hline
& \multicolumn{2}{c|}{\textbf{Non-filtered data}} & \multicolumn{2}{c}{\textbf{Filtered data}} \\
& \textit{U-net} & \textit{M-net} &  \textit{U-net} & \textit{M-net}\\
\hline
{No classification}      & 0.53 \textpm\ 0.06 & 0.59 \textpm\ 0.07 & 0.60 \textpm\ 0.08 & 0.65 \textpm\ 0.08 \\
{Hybrid model}           & 0.72 \textpm\ 0.01 & 0.67 \textpm\ 0.05 & 0.83 \textpm\ 0.02 & 0.79 \textpm\ 0.05 \\
{Perfect classification} & 0.92 \textpm\ 0.00 & 0.83 \textpm\ 0.10 & 0.93 \textpm\ 0.00 & 0.86 \textpm\ 0.09 \\
\hline
\end{tabular}
\end{table*}

\subsection{Experimental setup}

To investigate the effect of employing the hybrid model on the performance of segmenting BP region, three experiments were conducted:
\subsubsection*{Experiment 1: No classification} 
In the first experiment, the segmentation models were trained on the data without any prior classification model. In this experiment, training data contains US images with and without BP regions.
\subsubsection*{Experiment 2: hybrid model}
In the second experiment, the CNN architecture was used to identify images with BP. Then, the segmentation models (U-net and M-net) were trained on positive images only, and tested on the images that were classified by the CNN as BP.

\subsubsection*{Experiment 3: perfect classification model}
In the final experiment, a perfect classification model was mimicked by manually discarding all images without an annotated mask from data, and the segmentation models were trained and tested to these sets. This experiment was done to be able to discuss the potential of a hybrid model with a better performing classification model.



\subsection{Evaluation}
For evaluation of the classification results, two metrics were used in this research: the F1-score and the accuracy metric (see Equations \ref{eq:Acc} and \ref{eq:F1}). The accuracy metric is more suited for balanced datasets, whereas the F1-score is more suited for imbalanced datasets \cite{Tharwat2020}. Since the dataset was somewhat imbalanced, as discussed in Section \ref{section:data}, both metrics were evaluated.

For evaluation of the segmentation results, the dice similarity coefficient (DSC)  was used (see Equation \ref{eq:DSC}). DSC makes a pixel-wise comparison between the  ground truth and the corresponding prediction. The DSC ranges between 0 and 1, where 0 indicates there is no overlap, and 1 represents a perfect overlap.

All acquired data was tested for normality with the Shapiro-Wilk test~\cite{shapiro1965analysis}. The models were then tested for different means with a two sided T-test. 


\begin{equation}\label{eq:Acc}
    {Accuracy} = \frac{{TP} + {TN}}{{TP} + {FP} +{TN} + {FN}}
\end{equation}
\begin{equation}\label{eq:F1}
    F1 = \frac{2 {TP}}{2{TP}+{FP}+{FN}}
\end{equation}
\begin{equation}\label{eq:DSC}
    DSC = \frac{2 \cdot |M\cap GT|}{|M| + |GT|}
\end{equation}