\section{NNLIF with learning rules}\label{sec:lr}
In this Section, we consider the NNLIF model with a learning rule which is an extension of the Fokker-Planck equation \eqref{eq:problem1}, involving synaptic weights and the Hebbian learning rule. This is a novel and intriguing model and the dynamics of the membrane potential $v$ and the synaptic weight $w$ are on different time scales, making numerical simulation far more challenging. In order to better understand this model and verify the generality of the method proposed in Section \ref{sec:scheme}, we further explore this model from a numerical perspective. 

\subsection{Model introduction}


Compared with the simplest form of NNLIF model, the NNLIF model with learning rules introduces a new variable, the synaptic weight $w$, which is also the connectivity of the network $b$ mentioned in \eqref{eq:ha}. Furthermore, an external input function $I(w,t)$ is added to the drift coefficient $h$
\begin{equation}
    h(w,N(t))=-v+I(w,t)+w\sigma(N(t)).
\end{equation}
The function $\sigma(\cdot)$ represents the response of the network to the total activity, usually taking $\sigma(N)=N$. Then the Fokker-Planck equation without learning rules can be written as 
\begin{equation}
    \label{eq:problem3}
    \begin{cases}
        \partial_{t}p+\partial_{v}((-v+I(w,t)+w\sigma(\bar{N}(t)))p)-a\partial_{v v}p=0,\qquad v\in(-\infty,V_F]/\{V_R\},\\
        p(v,w,0)=p^0(v,w),\qquad p(-\infty,w,t)=p(V_F,w,t)=0,\\
        p(V^-_R,w,t)=p(V^+_R,w,t),\quad \partial _vp(V^-_R,w,t)=\partial _vp(V^+_R,w,t)+\frac{N(w,t)}{a},\\
    \end{cases}
\end{equation}
where, $p(v,w,t)$ describes the probability of finding a neuron at voltage $v$, synaptic weight $w$ and given time $t$. The diffusion coefficient $a$ is the same as in \eqref{eq:ha}. The subnetwork activity $N(w, t)$ and total activity
$\bar{N}(t)$ are defined as
\begin{equation}
    N(w,t)=-a\frac{\partial p}{\partial v}(V_F,w,t)\geq 0,\quad \bar{N}(t)=\int_{-\infty}^{+\infty}N(w,t)dw.
\end{equation}
Then we define the probability density of finding a neuron at synaptic weight $w$ and given time $t$ by 
\begin{equation}
    H(w, t)=\int_{-\infty}^{V_{F}} p(v, w, t) d v, \quad \int_{-\infty}^{\infty} H(w, t) d w=1 .
\end{equation}


In this case of no learning rule, the function $H(w,t)$ is time-independent because the distribution of synaptic weights in \eqref{eq:problem3} is fixed. The input signal $I(w,t)$ can be reflected by an output signal related to network activity $N(w,t)$. Next, we employ the learning rules of \cite{perthame2017distributed} to modulate the distribution of synaptic weights $H$, enabling the network to discriminate specific input signals $I$ by choosing an apposite synaptic weight distribution $H$ that is adapted to the signal $I$.


In \cite{perthame2017distributed}, the authors choose learning rules inspired by the seminal Hebbian rule and assume synaptic weights described with a single parameter $w$ and the subnetworks interact only via the total rate $\bar{N}$. They elucidate that all subnetworks parameterized by $w$ can vary their intrinsic synaptic weights $w$ according to a function $\Phi$ that is based on the intrinsic activity $N(w)$ of the network and the total activity of the network $\bar{N}$. Then, they give the generalization choice of Hebbian rule
\begin{equation}
    \Phi(N(w), \bar{N})=\bar{N} N(w) K(w),
\end{equation}
where $K(\cdot)$ represents the learning strength of the subnetwork with synaptic weight $w$. Adding the above choice of learning rule, the Fokker-Planck equation with learning rules is given by
\begin{equation}
    \label{eq:problem40}
    \frac{\partial p}{\partial t}+\frac{\partial}{\partial v}[(-v+I(w,t)+w \sigma(\bar{N}(t))) p]+\varepsilon \frac{\partial}{\partial w}[(\Phi-w) p]-a \frac{\partial^{2} p}{\partial v^{2}}=N(w, t) \delta\left(v-V_{R}\right).
\end{equation}
In order to better apply the numerical scheme and study the learning behavior of the model, we consider the equation \eqref{eq:problem40} for time rescaling $t \rightarrow t / \varepsilon$ and convert $\delta$-function to dynamic boundary condition such as:
\begin{equation}
    \label{eq:problem4}
    \begin{cases}
        \displaystyle
        \frac{\partial p}{\partial t}+\frac{\partial}{\partial w}[(\bar{N}(t)N(w,t)K(w)-w)p]
        =\frac{1}{\varepsilon}\left\{a\frac{\partial^2p}{\partial v^2}-\frac{\partial}{\partial v}[(-v+I(w,t)+w\sigma(\bar{N}(t)))p]\right\},\\
        p(v,w,0)=p^0(v,w),p(V_F,w,t)=p(-\infty,w,t)=p(v,\pm \infty,t)=0,\\
        p(V_R^-,w,t)=p(V_R^+,w,t),\qquad \frac{\partial}{\partial v}p(V^-_R,w,t)=\frac{\partial}{\partial v}p(V^+_R,w,t)+\frac{N(w,t)}{a}.
    \end{cases}
\end{equation}

Here, $p^0(v,w)$ is initial condition and the probability density
function p(v, t) should satisfy the condition of conservation of mass
\begin{equation}
    \int_{-\infty}^{\infty} \int_{-\infty}^{V_{F}} p(v, w, t) d v d w=\int_{-\infty}^{\infty} \int_{-\infty}^{V_{F}} p^{0}(v, w) d v d w=1.
\end{equation}



Despite some research on model \eqref{eq:problem4} as indicated by the theoretical properties presented in \cite{perthame2017distributed} and the numerical analysis and experiments in \cite{he2022structure}, it is still a relatively new model with limited established knowledge. In this paper, the numerical method proposed in Section \ref{sec:scheme} is used to further investigate the learning behaviors of this model numerically.

\subsection{Numerical scheme}
Now, we describe the numerical scheme for \eqref{eq:problem4}. We choose the calculation interval as $[V_{\text{min}},V_F]\times [W_{\text{min}},W_{\text{max}}]\times [0,T_{\text{max}}]$ and suppose the density function is practically negligible out of this region.
We use spectral methods for v-wise discretization and Differential method for w-wise and t-wise discretization. So we divide the interval $[W_{\text{min}},W_{\text{max}}],[0,T_{\text{max}}]$ into $n_w,n_t$ equal sub-intervals with size
\begin{equation}
    \Delta w=\frac{W_{\text{max}}-W_{\text{min}}}{n_w},\Delta t=\frac{T_{\text{max}}}{n_t}.
\end{equation}
Then the grid points can be represented as follows
\begin{equation}
    \begin{aligned}
        &w_{j}=W_{\text{min} }+j \Delta w, & j=0,1,2, \cdots, n_{w} \\
        &t^{n}=n \Delta t, & n=0,1,2, \cdots, n_{t}
    \end{aligned}
\end{equation}
For the v-direction discretization, we take the same scheme as in Section \ref{sec:fully_discrete_scheme}. The approximate solution is expended as 
\begin{equation}
    \label{eq:approximate_solution4}
    p_N(v,w,t)=\sum_{k=1}^{2N+3}\hat{u}_k(w,t)\psi_k(v).
\end{equation}
The initial condition for the expansion coefficients $\{\hat{u}_{k}(w,0)\}_{k=0}^{2N+3}$ can be obtained by the least square approximation,
\begin{equation}
    \label{eq:initial_vector3}
    \int_{V_{\min}}^{V_F} \sum_{k=1}^{2N+3}\hat{u}_k(w_j,0)\psi_k(v) \phi_i dv=\int_{V_{\min}}^{V_F} p^0(w_j,v)\phi_i dv, \quad j=0,1,2, \cdots, n_{w} \quad \forall \phi_i \in \mathrm{V}_N.
\end{equation}
From the properties of the basis functions \eqref{eq:lambda}, subnetwork activity $N(w,t)$ can be expressed as
\begin{equation}
    N^n_j=N(w_j,t^n)=-a\hat{u}_{2N+3}(w_j,t^n).
\end{equation}
And we apply the simplest rectangular numerical integration rule to discretize the total activity $\bar{N}(t)$
\begin{equation}
    \bar{N}^n=\Delta w \sum_{j=0}^{n_w}N^n_j.
\end{equation}
For the w-direction discretization, we inherit the idea form \cite{he2022structure} which takes the following explicit flux construction adapted from Godunov's Method 
 \begin{equation}
    \Phi_{i, j+\frac{1}{2}}^{n}=
        \begin{cases}
            \begin{cases}
                \min \left\{\Phi_{i, j}^{n}, \Phi_{i, j+1}^{n}\right\} \qquad &\hat{P}_{i, j}^{n} \leq \hat{P}_{i, j+1}^{n} \\
                \max \left\{\Phi_{i, j}^{n}, \Phi_{i, j+1}^{n}\right\}  &\hat{P}_{i, j}^{n}>\hat{P}_{i, j+1}^{n} \\
            \end{cases}&j=0, \cdots, n_{w}-1\\
            0  &j=-1, n_{w}
        \end{cases}
    \end{equation}
where
\begin{equation}
    \Phi_{i, j}^{n}=\left(\bar{N}^{n} N_{j}^{n} K\left(w_{j}\right)-w_{j}\right) \hat{P}_{i, j}^{n} \quad \text { for } \quad j=0, \cdots, n_{w}.
\end{equation}
$\hat{P}_{i,j}^n$ is the coefficients of the basis functions in \eqref{eq:approximate_solution4}
\begin{equation}
    \hat{P}_{i,j}^n=\hat{u}_i(w_j,t^n).
\end{equation}
Define 
\begin{equation}
\begin{aligned}
    &p_{N,j}^{n}=\sum_{k=1}^{2N+3}\hat{u}_k(w_j,t^n)\psi_k(v),\\
    &q_{N,j+\frac{1}{2}}^n=\sum_{k=1}^{2N+3} \Phi_{k,j+\frac{1}{2}}^{n}\psi_k(v).
\end{aligned}
\end{equation}
After using a semi-implicit method for time discretization, we obtain the fully discrete scheme as follows:
\begin{equation}
    \frac{p_{N,j}^{n+1}-p_{N,j}^{n}}{\Delta t}+\frac{q_{N,j+\frac{1}{2}}^n-q_{N,j-\frac{1}{2}}^n}{\Delta w}=\frac{1}{\varepsilon}\left\{a\frac{\partial^2p_{N,j}^{n+1}}{\partial v^2}-\frac{\partial}{\partial v}\left[(-v+I(w_j)+w_j\sigma(\bar{N}(t^n)))p_{N,j}^{n+1}\right]\right\}.
\end{equation}
When the test function space $\mathrm{V}_N$ is given, the coefficients of the approximate solution \eqref{eq:approximate_solution4} for each $t$ and $w$ step can be obtained by the following linear system
\begin{equation}
    \begin{aligned}
        &\frac{\hat{S}(\hat{\mathbf{P}}^{n+1}_{j}-\hat{\mathbf{P}}^n_{j})}{\Delta t}+\frac{\hat{S}(\mathbf{\Phi}_{j+\frac{1}{2}}^n-\mathbf{\Phi}_{j-\frac{1}{2}}^n)}{\Delta w}\\
        +&\frac{1}{\varepsilon}\left\{-\hat{A}\hat{\mathbf{P}}^{n+1}_{j}+\left(I(w_{j},t^n)+w_{j}\sigma(\bar{N}(t^n))\right)\hat{B}\hat{\mathbf{P}}^{n+1}_{j}-a\hat{C}\hat{\mathbf{P}}^{n+1}_{j}\right\}=0,
    \end{aligned}
\end{equation}
where 
\begin{equation}
    \begin{aligned}
    &\hat{\mathbf{P}}^n_j=\left(\hat{u}_1(w_j,t^n),\hat{u}_2(w_j,t^n),...,\hat{u}_{2N+3}(w_j,t^n)\right)^T,\\
        &\mathbf{\Phi}_{j+\frac{1}{2}}^n=(\Phi_{1, j+\frac{1}{2}}^{n},\Phi_{2, j+\frac{1}{2}}^{n},...,\Phi_{2N+3, j+\frac{1}{2}}^{n})^T,
    \end{aligned}
\end{equation}
 and the matrix $\hat{S},\hat{A},\hat{B},\hat{C}$ are defined in \eqref{eq:Matrix2}.

 
This numerical scheme is conserved naturally in the $w$ direction, however, strict conservation of mass in the $v$ direction is not achieved when the test function space is selected based on Section \ref{sec:stability}. When $\varepsilon$ is small enough, the asymptotic preserving properties of the model can only be verified through the use of MPGM.

 