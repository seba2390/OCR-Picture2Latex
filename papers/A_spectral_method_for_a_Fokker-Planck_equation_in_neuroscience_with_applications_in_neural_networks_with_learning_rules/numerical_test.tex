\section{Numerical test} \label{sec:numerical_test}

In this section, we give  numerical tests to verify the properties of the proposed schemes and demonstrate some explorations of the model. Numerical solutions for the initial three subsections are obtained by LGM; results for the MPGM approach are similar except for Section \ref{sec:Convergence}, which are thus omitted, and numerical solutions for Section \ref{sec:Learning_testing} are obtained by MPGM, as variations in the time scale require the scheme to be asymptotic preserving.

The tests are structured as follows. In Section \ref{sec:Convergence}, the convergence order of the method is tested in both the NNLIF model and the NNLIF model with learning rules. In Section \ref{sec:time_saving}, we validate the efficiency of the spectral method by comparing it to existing methods. In Section \ref{sec:blow_up}, we test a few properties of the NNLIF model. In Section \ref{sec:Learning_testing}, we test the learning and discrimination abilities of NNLIF model with learning rules for the periodic input function.


\subsection{Order of accuracy}\label{sec:Convergence}
In this part, we test the order of accuracy of the proposed scheme based on the NNLIF model and the NNLIF model with learning rules. Since the exact solution is unavailable, we choose the numerical solution $p_e$ of the finite difference method \cite{hu2021structure} with sufficient accuracy to replace the exact solution.



For NNLIF model \eqref{eq:problem2}, we choose $V_F=2,V_R=1,V_{\text{min}}=-4,a=1, b=3$ and the Gaussian distribution
\begin{equation}
    p_G(v)=\frac{1}{\sqrt{2 \pi} \sigma_{0} M_0} e^{-\frac{\left(v-v_{0}\right)^{2}}{2 \sigma_{0}^{2}}},
\end{equation}
as the initial condition with $v_0=-1$ and $\sigma_0^2=0.5$, $M_0$ is a normalization factor such that
\begin{equation}
    \int_{V_{\text{min}}}^{V_F} p_G(v) dv=1.
\end{equation}
 The numerical solution is computed till time $t=0.2$. Errors in both $L^{\infty}$ and $L^2$ norm are examined with fixed $N=12$ and different $\Delta t$ in Table \ref{convergence1}. It should be noted that the number of basis functions is not $N$, but rather $2N+3$, as shown in equation \eqref{eq:approximate_solution3}. 
\begin{table}[!htb]
	\centering
	\begin{tabularx}{10cm}{ccccc}
	\toprule
	$\Delta t$ & $\left\| p_N-p_e \right\|_{L^{\infty}}$& {$O_{\tau,L^{\infty}}$}& $\left\| p_N-p_e \right\|_{L^2}$& {$O_{\tau,L^2}$}\\ 
	\midrule
	0.04 & 3.880E-03 &0.9520 & 1.868E-03 &0.9508 \\
	0.02 & 2.005E-03 & 0.9792 & 9.662E-04 &0.9617 \\
	0.01 & 1.017E-03 & 0.9926 &4.961E-06&0.9287 \\
    0.005 & 5.111E-04 & - &2.606E-04& - \\
	\bottomrule
    \end{tabularx}
    \caption{Error and order of accuracy of the proposed numerical scheme for NNLIF model with different temporal sizes. The parameter $N$ is fixed as $N=12$.}
    \label{convergence1}
\end{table}

For the order of accuracy in the $v$ direction, we choose the time step size $\Delta t=10^{-5}$. Errors in the $L^2$ norm are examined with different $N$. The logarithm of the error versus $N$ is plotted in Figure \ref{convergence2}. We remark that when testing the order of spatial convergence, the results present a zig-zag decreasing profile as $N$ increases, which is a common phenomenon for spectral methods. We thus plot the errors for odd and even numbers of $N$, respectively. For each scenario, we clearly observe the spectral convergence as the number of spatial basis functions increases.


\begin{figure}[!htb]
    \centering
        \begin{minipage}[c]{0.49\textwidth}
            \centering
            \includegraphics[width=7cm]{O_N1.eps}
        \end{minipage}
        \begin{minipage}[c]{0.49\textwidth}
            \centering
            \includegraphics[width=7cm]{O_N2.eps}
        \end{minipage}
		\caption{Logarithm of the error of the proposed numerical scheme for NNLIF model with learning rules with different $N$. The temporal size is fixed as $\Delta t=10^{-5}$. Left: $N$ is odd; Right: $N$ is even.}
  \label{convergence2}
\end{figure}


For NNLIF with learning rules model \eqref{eq:problem4}, we choose $V_F=2,V_R=1,V_{\text{min}}=-4,a=1,\varepsilon=0.5,W_{\text{min}}=-1.1,W_{\text{max}}=0.1,\sigma(\bar{N})=\bar{N}, I(w)=0$ and initial condition
\begin{equation}
    p_{\text{init}}=\begin{cases}
        \frac{1}{\sqrt{2 \pi} \sigma_{0} } e^{-\frac{\left(v-v_{0}\right)^{2}}{2 \sigma_{0}^{2}}}\text{sin}^2(\pi w) \qquad &-1<w<0,\\
        0 &\text{otherwise},
    \end{cases}
\end{equation}
with $v_0=-1$ and $\sigma_0^2=0.5$.

The numerical solution is computed till time $t=0.1$. For $t$ direction and $w$ direction, we fix $\frac{\Delta w}{\Delta t}=1, N=16$. Considering that both the $t$ direction and the $w$ direction are theoretically first-order accurate, as well as the stability factor, it is reasonable to jointly test the order of accuracy. Errors in both $L^{1}$ and $L^2$ norm are examined with different $\Delta t$ and $\Delta w$ in Table \ref{convergence3}. For $v$ direction, we fix ${\Delta w}={\Delta t}=10^{-5}$. Errors in the $L^2$ norm are examined with different $N$. The logarithm of the error versus $N$ is plotted in Figure \ref{convergence4}.

\begin{table}[!htb]
	\centering
	\begin{tabularx}{10cm}{cccccc}
	\toprule
	$\Delta t$ &$\Delta w$ & $\left\| p_N-p_e \right\|_{L^{1}}$& $O_{\tau,L^{1}}$& $\left\| p_N-p_e \right\|_{L^2}$& {$O_{\tau,L^2}$}\\ 
	\midrule
	0.02 &0.02 & 1.599E-03 &1.04 & 3.234E-03 &1.07 \\
	0.01 &0.01 & 7.755E-04 & 0.99 & 1.536E-03 &0.97 \\
	0.005 &0.005 &3.893E-04 & 1.01 &7.812E-04&1.05 \\
    0.0025 & 0.0025 & 1.926E-04 & - &3.757E-04& - \\
	\bottomrule
    \end{tabularx}
    \caption{Error and order of accuracy of the proposed numerical scheme for NNLIF model with learning rules with different ${\Delta w}$ and ${\Delta t}$. The parameter $N$ is fixed as $N=16$.}
    \label{convergence3}
\end{table}
\begin{figure}[!htb]
    \centering
        \begin{minipage}[c]{0.49\textwidth}
            \centering
            \includegraphics[width=7cm]{O2_N1.eps}
        \end{minipage}
        \begin{minipage}[c]{0.49\textwidth}
            \centering
            \includegraphics[width=7cm]{O2_N2.eps}
        \end{minipage}
		\caption{Logarithm of the error of the proposed numerical scheme for NNLIF model with learning rules with different $N$. The temporal size is fixed as $\Delta t=10^{-5}$. Left: $N$ is odd; Right: $N$ is even}
  \label{convergence4}
\end{figure}

The results indicate that the scheme shows first-order accuracy in time and exponential convergence in space for the NNLIF model; first-order accuracy in the $w$, $t$ direction and exponential convergence in the v direction for the NNLIF model with learning rules.






\subsection{Simulation time comparison}\label{sec:time_saving}
In this part, we compare the CPU time between the proposed spectral method and the finite difference method \cite{hu2021structure}, to show that our scheme has a significant computational time advantage with the same level of accuracy.


We choose NNLIF model with parameters $a=1,b=0.5,\Delta t=5\times 10^{-4}$ and the Gaussian initial condition with $v_0=0,\sigma_0^2=0.25$. The numerical solution is computed till time $t=0.5$. The results of the spectral method and the finite difference method are shown in Table \ref{error1} and Table \ref{error2}.
\begin{table}[!htb]
	\centering
	\begin{tabularx}{8cm}{cccc}
	\toprule
	$N$& $\left\| \cdot \right\|_{\infty}$&$\left\| \cdot \right\|_{1}$& CPU Time (s) \\ 
	\midrule
	5 & 5.15e-02 & 1.58e-02& 0.026 \\
    10 & 3.33e-03 & 2.85e-04 & 0.030  \\
    15 & 9.71e-05 & 2.51e-05& 0.053 \\
    20 & 1.30e-06 & 3.76e-07 & 0.071  \\
	\bottomrule
    \end{tabularx}
    \caption{Errors using the spectral method with different numbers of basis functions.}
    \label{error1}
\end{table}

\begin{table}[!htb]
	\centering
	\begin{tabularx}{8cm}{cccc}
	\toprule
	$h$& $\left\| \cdot \right\|_{\infty}$&$\left\| \cdot \right\|_{1}$& CPU Time (s) \\ 
	\midrule
    ${1/4}$ & 3.01e-03 & 7.09e-04& 0.031 \\
    ${1/8}$ & 9.69e-04 & 2.18e-04& 0.073 \\
    ${1/16}$ & 2.79e-04 & 6.18e-05 & 0.157  \\
    ${1/32}$ & 7.54e-05 & 1.64e-05& 0.348 \\
    ${1/64}$ & 1.97e-05 & 4.21e-06 & 2.801  \\
    ${1/128}$ & 4.41e-06 & 1.12e-06 & 11.971  \\
	\bottomrule
    \end{tabularx}
    \caption{Errors using the finite difference method  with different spatial grid sizes}
    \label{error2}
\end{table}
These tables clearly indicate that to achieve the same level of accuracy, the spectral method is more efficient in terms of the simulation time, and the advantage is more noticeable when the accuracy level is higher.



\subsection{Global solution and blow-up in NNLIF model}\label{sec:blow_up}
\subsubsection{Blow up}


In \cite{caceres2011analysis}, the authors find the solution may blow up in finite time with the suitable initial conditions for the excitatory network. They show that whenever the value of $b>0$ is, if the initial data is concentrated enough around $v=V_F$, then the defined weak solution in Definition 2.1 of \cite{caceres2011analysis} does not exist for all times. Figure \ref{fig:blowup1} and Figure \ref{fig:blowup2} show this phenomenon. It can be seen that when the blow-up phenomenon is about to occur, the density function $p(v,t)$ is increasingly concentrated and sharp at reset point $V_R$ and the firing rate $N(t)$ is growing rapidly. 
\begin{figure}[!htb]
    \centering
        \begin{minipage}[c]{0.49\textwidth}
            \centering
            \includegraphics[width=7cm]{blowupNt1.eps}
        \end{minipage}
        \begin{minipage}[c]{0.49\textwidth}
            \centering
            \includegraphics[width=7cm]{blowup1.eps}
        \end{minipage}
        \caption{Equation parameters $a=1, b=3$ with Gaussian initial condition $v_0=-1, \sigma_0^2=0.5$. Left: evolution of firing rate $N(t)$. Right: density function $p(v, t)$ at $t=2.95,3.15,3.35$.}
        \label{fig:blowup1}
\end{figure}
\begin{figure}[!htb]
        \begin{minipage}[c]{0.49\textwidth}
            \centering
            \includegraphics[width=7cm]{blowupNt2.eps}
        \end{minipage}
        \begin{minipage}[c]{0.49\textwidth}
            \centering
            \includegraphics[width=7cm]{blowup2.eps}
        \end{minipage}
        \caption{Equation parameters $a=1, b=1.5$ with Gaussian initial condition $v_0=1.5, \sigma_0^2=0.005$.Left: evolution of firing rate $N(t)$. Right: density function $p(v, t)$ at $t=0.0325,0.0365,0.0405$.}
        \label{fig:blowup2}
\end{figure}


For spectral methods, the approximate solution of the density function is dependent on the coefficients of the basis functions. We aim to further investigate how the coefficients change when the blow-up phenomenon is about to occur. We choose $a=1, b=1.5$ in equation and $N=20,\Delta t=10^{-5}$.
\begin{figure}[!htb]
    \centering
        \begin{minipage}[c]{0.49\textwidth}
            \centering
            \includegraphics[width=7cm]{coeff.eps}
        \end{minipage}
        \begin{minipage}[c]{0.49\textwidth}
            \centering
            \includegraphics[width=7cm]{coeff2.eps}
        \end{minipage}
    \caption{Changes of the coefficients of the first few terms in the expansion formula \eqref{eq:approximate_solution3} during blow up. Left: evolution of the coefficients $\{p_k,f_k\}_{k=0}^2$. Right:evolution of the coefficients $\{\lambda_k\}_{k=1}^3$.  }
    \label{fig:coeff}
\end{figure}


Recall that
\begin{equation}
    \lambda_3(t)=\partial_vp(V_F,t)=-\frac{N(t)}{a},\qquad \partial_vp(V_R^+,t)=\lambda_2(t)+\lambda_3(t).
\end{equation}
Therefore, $\lambda_2$ and $\lambda_3$ are directly influenced by the firing rate. Due to the use of global basis functions, as the firing rate $N(t)$ increases, all the basis functions are affected. In response to the change of $\lambda_3$, $\lambda_2$ and the coefficients of the basis functions in $\mathrm{W}_2$ change accordingly, respectively controlling the derivative value on both sides of point $V_R$ and the function value in the interval. Figure \ref{fig:coeff} show the change of coefficients $\{p_k,f_k\}_{k=0}^2$, $\{\lambda_k\}_{k=1}^3$ in \eqref{eq:approximate_solution3} as time involves. It can be seen from the figure that the changes in $\lambda_2$ and $\lambda_3$ are most obvious, while the coefficients of all basis functions in $\mathrm{W}_2$ space are affected but the changes are relatively small.



\subsubsection{Relative entropy}

As we have mentioned, since little is known about the properties of the solutions of the Fokker-Planck equation \eqref{eq:problem1}, there  is a lack of complete understanding of the long-time asymptotic behavior in the continuous case. In \cite{caceres2011analysis}, they studied relative entropy theory for linear problem $a_1=b=0$, which implies exponential convergence to equilibrium. The relative entropy is given by
\begin{equation}
    I_e=\int_{-\infty}^{V_{F}} G\left(\frac{p(v, t)}{p^{\infty}(v)}\right) p_{\infty}(v) d v,
\end{equation}
which can be shown to be decreasing in time, where $G(\cdot)$ is a smooth convex function and $p^{\infty}(v)$ represents the stationary solution. In this part, we numerically verify the relative entropy theory. The numerical relative entropy is given by
\begin{equation}
    S(t)=\int_{V_L}^{V_{F}} G\left(\frac{p_N(v, t)}{p^{\infty}(v)}\right) p_{\infty}(v) d v.
\end{equation}

We consider nonlinear cases with $a_0=1,a_1=0,b=-0.5$ and $a_0=1,a_1=0.1,b=0$. We choose the numerical solution of a sufficiently long time as the stationary solution $p^{\infty}(v)$ and the Gaussian initial condition $v_0=-1, \sigma_0^2=0.5$. Figure \ref{fig:relative_entropy2} \ref{fig:relative_entropy3} show the time evolution of the firing rate and the numerical relative entropy for these cases.
\begin{figure}[!htb]
    \centering
    \begin{minipage}[c]{0.49\textwidth}
        \centering
        \includegraphics[width=1\textwidth]{Nt2.eps}
    \end{minipage}
    \begin{minipage}[c]{0.49\textwidth}
        \centering
        \includegraphics[width=1\textwidth]{relative2.eps}
    \end{minipage}
    \caption{Equation parameters $a=1,b=-0.5$ with Gaussian initial condition $v_0=-1, \sigma_0^2=0.5$. Left: evolution of firing rate $N(t)$. Right: evolution of relative entropy $S(t)$ with $G(x)=\frac{(x-1)^2}{2}$.}
     \label{fig:relative_entropy2}
\end{figure}
\begin{figure}[!htb]
    \centering
    \begin{minipage}[c]{0.49\textwidth}
        \centering
        \includegraphics[width=1\textwidth]{Nt3.eps}
    \end{minipage}
    \begin{minipage}[c]{0.49\textwidth}
        \centering
        \includegraphics[width=1\textwidth]{relative3.eps}
    \end{minipage}
    \caption{Equation parameters $a_0=1,a_1=0.1,b=0$ with Gaussian initial condition $v_0=-1, \sigma_0^2=0.5$. Left: evolution of firing rate $N(t)$. Right: evolution of relative entropy $S(t)$ with $G(x)=\frac{(x-1)^2}{2}$.}
     \label{fig:relative_entropy3}
\end{figure}


As shown in \cite{caceres2011analysis}, there may be two stationary solutions for the system of $b>0$. For example, when $ a(N(t)) = 1$ and $b =1.5$, there are two different steady states whose firing rates are $N^{\infty}=2.319$ and $N^{\infty}=0.1924$. Given the firing rate $N^{\infty}$, the expression of $p^{\infty}(v)$is given by
\begin{equation}
    p^{\infty}(v)=\frac{N^{\infty}}{a\left(N^{\infty}\right)} e^{-\frac{h\left(v, N^{\infty}\right)^{2}}{2 a\left(N^{\infty}\right)}} \int_{\max \left\{v, V_{R}\right\}}^{V_{F}} e^{\frac{h\left(\omega, N^{\infty}\right)^{2}}{2 a\left(N^{\infty}\right)}} d \omega ,
\end{equation}
which is the stationary solution when we calculate the relative entropy for multiple steady-state problems. The results are shown in Figure \ref{fig:relative_entropy4}, where the steady state with a larger firing rate $N^{\infty}=2.319$ is unstable while the stationary solution with a lower firing rate $N^{\infty}=0.1915$ is stable. We see that the relative entropy decreases with time for the stable state, while the other one does not.
\begin{figure}[!htb]
    \centering
    \begin{minipage}[c]{0.49\textwidth}
        \centering
        \includegraphics[width=1\textwidth]{st1.eps}
    \end{minipage}
    \begin{minipage}[c]{0.49\textwidth}
        \centering
        \includegraphics[width=1\textwidth]{st2.eps}
    \end{minipage}
    \caption{Equation parameters $a=1,b=1.5$  with Gaussian initial condition $v_0=-1, \sigma_0^2=0.5$. In this case, the model has two stationary states with firing rates $N^{\infty}=0.1924$ and $N^{\infty}=2.319$. Left: evolution of relative entropy $S(t)$ with $G(x)=\frac{(x-1)^2}{2}$ for stable state with $N^{\infty}=0.1924$. Right: evolution of relative entropy $S(t)$ with $G(x)=\frac{(x-1)^2}{2}$ for unstable state with $N^{\infty}=2.319$ }
     \label{fig:relative_entropy4}
\end{figure}



\subsection{Learning and testing in NNLIF model with learning rules}\label{sec:Learning_testing}
In this part, we consider the learning and discrimination abilities in NNLIF model with learning rules. In \cite{perthame2017distributed}, the authors proposed a two-phase test to illustrate the discrimination property:\medskip


\textbf{Learning phase}
\smallskip

1. An heterogeneous input $I(w)$ is presented to the system, when the learning process is active. The initial data is supported on inhibitory weights and the learning rule is determined for the present weights by $-N(w)\bar{N}$ by taking $K(w) = -1$ if $w \leq 0$.


2. After some time, the synaptic weight distribution $H(w, t)$ converges to an equilibrium distribution $H^*_
I(w)$, which depends on $I$.\medskip



\textbf{Testing phase}
\smallskip

1. The learning process is now switched off, i.e. there is no w-direction convection, and a new input $J(w)$ is presented to the system.


2. After some time, the solution $p_J (v,w, t)$ reaches an equilibrium $p^*_J (v,w)$, which is characterized  by the output signal $N^*_J(w)$ which is the neural activity distribution across the heterogeneous populations.\medskip


Some numerical explorations of the learning behavior and discriminative properties of the model have been done in \cite{perthame2017distributed}\cite{he2022structure}. When the learning phase is over, in addition to the synaptic weight distribution $H(w, t)$, the equilibrium state $N_I(w)$ of the sub-network activity $N(w,t)$ can also be obtained, which we call the \textbf{prediction signal}. In the previous work on the time-independent input function $I(w)$ for the learning phase \cite{perthame2017distributed}\cite{he2022structure}, the prediction signal $N_I(w)$ is like a triangle depending on the input function $I(w)$ of the learning phase.  After the testing phase when the learning input $I(w)$ and testing input $J(w)$ are the same, the output signal $N_J^*(w)$ is like a triangle that matches the prediction signal $N_I(w)$; but when $I(w)$ and $J(w)$ are different, the output signal is not in a regular shape. 


They explore learning and discriminative power in the model only if the input function is constant in time. In our work, we plan to explore how the model would react to a time-varying input signal through numerical experiments, and there have been studies in the field of neuroscience surrounding time-varying input \cite{isidori1990output}. Especially, we consider input functions that are time-periodic and explore the effect of oscillation periods on the learning ability of the model. To this aim, we have designed $4$ sets of experiments, progressively revealing the nature of its learning behavior.

\paragraph{Test 1. Synchronizing with oscillating inputs.}
We choose the testing input functions
\begin{equation}
    \begin{aligned}
        I_{1}&=\pi^{-\frac{1}{4}} e^{-\frac{1}{2}(10 w+5)^{2}}+2 \\
        I_{2}&=\pi^{-\frac{1}{4}} \sqrt{2}(10 w+5) e^{-\frac{1}{2}(10 w+5)^{2}}+2,
    \end{aligned}
\end{equation}
and the learning input function is periodically switching between those two
\begin{equation}
    \label{input}
    I(w,t)=a(t)I_1(w)+b(t)I_2(w),
\end{equation}
where
\begin{equation}
    \label{eq:input_coff}
    \begin{aligned}
        a(t)&=\frac{1+\cos(\frac{2\pi t}{D})}{2},\\
        b(t)&=1-a(t).
    \end{aligned}
\end{equation}

For other parameters, we choose $V_F=2,V_R=1,V_{\text{min}}=1,a=1,\varepsilon=0.1,W_{\text{min}}=-1.1,W_{\text{max}}=0.1,T_{\text{max}}=4,\sigma(\bar{N})=\bar{N},\Delta t=2.5\times 10^{-4},\Delta w=0.01$ and the initial condition 
\begin{equation}
    p_{\text{init}}=\begin{cases}
        \text{sin}^2(\pi v)\text{sin}^2(\pi w) \qquad &-1<w<0\text{ and }-1<v<1,\\
        0 &\text{otherwise}.
    \end{cases}
\end{equation}


In the learning phase, the input function changes periodically in time; the smaller the period is, the greater the rate of change of the input function is. The total network activity $\bar{N}$ is an intuitive response to the input function, so we first observe the change in the total network activity. First, we choose period $D=1,0.5,0.2$. 
\begin{figure}[!htb]
    \centering
        \begin{minipage}[c]{0.3\textwidth}
            \centering
            \includegraphics[width=1\textwidth]{Nbar1.eps}
        \end{minipage}
        \begin{minipage}[c]{0.3\textwidth}
            \centering
            \includegraphics[width=1\textwidth]{Nbar2.eps}
        \end{minipage}
        \begin{minipage}[c]{0.3\textwidth}
            \centering
            \includegraphics[width=1\textwidth]{Nbar3.eps}
        \end{minipage}
    \caption{Equation parameters $a=1$ and $\varepsilon=0.1$. The evolution of total firing rate $\bar{N}$. Left: the input function period $D=1$. Middle: the input function period $D=0.5$. Right: the input function period $D=0.2$.}
    \label{fig:Nbar}
\end{figure}

Figure \ref{fig:Nbar} shows the evolution of the total firing rate at different periods. As we expected, except for the initial transient evolutionary phase, the total activity of the network changes periodically over time and its period is the same as the input function.



\paragraph{Test 2. Adapting to fast oscillating inputs.}
Since the prediction signal is determined by the learning input function and reflects the model's learning of the learning input function $I(w,t)$, observing the prediction signal in different periods helps us explore the learning behavior of the model. We compare numerical results for different periods $D=4,0.4,0.2,0.1,0.01$.  In this case, the last input function learned by the model is $I(w,t_\text{max})=I_1$.



\begin{figure}[!htb]
    \centering
        \begin{minipage}[c]{0.3\textwidth}
            \centering
            \includegraphics[width=1\textwidth]{D=4.eps}
        \end{minipage}
        \begin{minipage}[c]{0.3\textwidth}
            \centering
            \includegraphics[width=1\textwidth]{D=0.4.eps}
        \end{minipage}
        \begin{minipage}[c]{0.3\textwidth}
            \centering
            \includegraphics[width=1\textwidth]{D=0.2.eps}
        \end{minipage}
        \begin{minipage}[c]{0.3\textwidth}
            \centering
            \includegraphics[width=1\textwidth]{D=0.1.eps}
        \end{minipage}
        \begin{minipage}[c]{0.3\textwidth}
            \centering
            \includegraphics[width=1\textwidth]{D=0.05.eps}
        \end{minipage}
        \begin{minipage}[c]{0.3\textwidth}
            \centering
            \includegraphics[width=1\textwidth]{D=0.01.eps}
        \end{minipage}
    \caption{Equation parameters $a=1$ and $\varepsilon=0.1$. The prediction signal at $t=4$ with different input function periods. Top: the input function period $D=4,0.4,0.2$ from left to right. Bottom: the input function period $0.1,0.05,0.01$ from left to right.}
    \label{fig:prediction}
\end{figure}

Figure \ref{fig:prediction} shows the prediction signal at different periods. When the period is large, the prediction signal is like a triangle. As the period gets smaller, the shape of the prediction signal is getting more and more irregular. However, as the period is getting further smaller, the shape of the prediction signal is becoming triangular again. In previous experiments \cite{perthame2017distributed}\cite{he2022structure}, for the time-independent learning input signal $I(w)$, the test signal always resembles a triangle. So we speculate from Figure \ref{fig:prediction} that for sufficiently large or sufficiently small periods, the predicted signal looks like a triangle, and the model has effectively learned a signal of a certain form.




\paragraph{Test 3. Learning from oscillating inputs.}

In order to verify the above conjecture, we choose a relatively large period with $D=4$ and a small period with $D=0.01$ in \eqref{eq:input_coff}.  In the testing phase, we choose testing input functions $J=I_1$, $J=I_2$, and $J=\frac{I_1+I_2}{2}$.
\begin{figure}[!htb]
    \centering
        \begin{minipage}[c]{0.3\textwidth}
            \centering
            \includegraphics[width=1\textwidth]{D=4_I1test.eps}
        \end{minipage}
        \begin{minipage}[c]{0.3\textwidth}
            \centering
            \includegraphics[width=1\textwidth]{D=4_I2test.eps}
        \end{minipage}
        \begin{minipage}[c]{0.3\textwidth}
            \centering
            \includegraphics[width=1\textwidth]{D=4_I12test.eps}
        \end{minipage}
    \caption{(Output signal for the large period learning input) The
final firing rate $N(w)$ with different testing input $J(w)$. Equation parameters $a=1$ and $\varepsilon=0.1$, and the period of the input function in the learning phase is $D=4$. Left: Output signal with testing input function $J=I_1$. Middle: Output signal with testing input function $J=I_2$. Right: Output signal with testing input function $J=\frac{I_1+I_2}{2}$.}
    \label{fig:largeD}
\end{figure}
\begin{figure}[!htb]
    \centering
        \begin{minipage}[c]{0.3\textwidth}
            \centering
            \includegraphics[width=1\textwidth]{D=0.01_I1test.eps}
        \end{minipage}
        \begin{minipage}[c]{0.3\textwidth}
            \centering
            \includegraphics[width=1\textwidth]{D=0.01_I2test.eps}
        \end{minipage}
        \begin{minipage}[c]{0.3\textwidth}
            \centering
            \includegraphics[width=1\textwidth]{D=0.01_I12test.eps}
        \end{minipage}
    \caption{Output signal for the small period learning input) The
final firing rate $N(w)$ with different testing input $J(w)$. Equation parameters $a=1$ and $\varepsilon=0.1$, and the period of the input function in the learning phase is $D=0.01$. Left: Output signal with testing input function $J=I_1$. Middle: Output signal with testing input function $J=I_2$. Right: Output signal with testing input function $J=\frac{I_1+I_2}{2}$.}
    \label{fig:shortD}
\end{figure}

Figure \ref{fig:largeD} shows the output signal of period $D=4$ with testing input function $J=I_1$, $J=I_2$ and $J=\frac{I_1+I_2}{2}$. When $J=I_1=I(w,t_\text{max})$, the output signal $N_J^*(w)$ is like a triangle. Figure \ref{fig:shortD} shows the output signal of period $D=0.01$ with testing input function $J=I_1$, $J=I_2$ and $J=\frac{I_1+I_2}{2}$. When $J=\frac{I_1+I_2}{2}$, the output signal $N_J^*(w)$ is like a triangle. Numerical results show that when the period is relatively large, the signal learned by the model matches $I_1$, and when the period is relatively small, it matches $\frac{I_1+I_2}{2}$. 

The experimental results can be interpreted as follows. When the period is large, the model has enough time to learn, so the learned signal is the input function at the last moment. And when the period is small, neither $I_1$ nor $I_2$ can be learned well, but the result of learning is the average of the two.  Because when the switching process is too fast, the effect of the model on the learning of either $I_1$ or $I_2$ is poor. Instead, the average signal $\frac{I_1+I_2}{2}$ is captured by the time averaging of the learning process. 


 \paragraph{Test 4. Phase diagram for leaning.} There are multiple typical time scales in this model: the time scale for the voltage activities, the time scale for learning by redistributing the synaptic weights and the time period in the external input. When introducing the model, we perform a time rescaling for \eqref{eq:problem4}, where the parameter $\varepsilon$ reflects the ratio between the time scales of voltage activities and learning. In the next numerical experiment, we choose $\varepsilon=1,0.5,0.25,0.125$ and periods $D=2^2,2^1,\dots,2^{-7}$ to compare the results of the output signal under different parameters. After the testing phase, we choose the total activity $\bar{N}(t)$ to quantify the output signal:
\begin{equation}
    \label{judge_tool}
    E^{\varepsilon,D}_J=\left| \bar{N}^{\varepsilon,D}_J -\bar{N}^{\varepsilon}_J \right|.
\end{equation}
Here, $\bar{N}^{\varepsilon,D}_J$ denotes the total activity when the equation parameter is $\varepsilon$, the learning input function is given by \eqref{input} with period $D$, while the testing input function is $J$. $\bar{N}^{\varepsilon}_J$ represents the total activity where the equation parameter is $\varepsilon$ and both the learning input function and the testing input function are $J$. $E^{\varepsilon,D}_J$ can roughly measure the output signal. The closer the value of $E^{\varepsilon,D}_J$ is to zero, the superior the model's learning efficacy.

\begin{figure}[!htb]
    \centering
        \begin{minipage}[c]{0.49\textwidth}
            \centering
            \includegraphics[width=1\textwidth]{eps_test1.eps}
        \end{minipage}
        \begin{minipage}[c]{0.49\textwidth}
            \centering
            \includegraphics[width=1\textwidth]{eps_test2.eps}
        \end{minipage}
    \caption{Equation parameter $a=1$. Left: The value of $E^{\varepsilon,D}_J$ under different $D$ and $\varepsilon$ with input function $J=I_1$. Right: The value of $E^{\varepsilon,D}_J$ under different $D$ and $\varepsilon$ with input function $J=\frac{I_1+I_2}{2}$.}
    \label{fig:output3}
\end{figure}

 As shown in Figure \ref{fig:output3}, as the period becomes smaller, the testing indicator becomes less significant with respect to the testing input function $J=I_1$, and the test indicator becomes more significant with respect to the testing input function $J=\frac{I_1+I_2}{2}$. Besides, the numerical results also suggest that when epsilon is small, the transition in learning takes place at a smaller time period, whereas such a trend is not prominent. Although the experiments are not fully conclusive yet, they show a lot of promise for using the proposed numerical method to simulate large-scale tests.
