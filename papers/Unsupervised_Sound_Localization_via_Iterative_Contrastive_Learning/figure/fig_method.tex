
\begin{figure*}[t!]
    \centering
	\includegraphics[width=0.9\linewidth]{figure/images/CVPR_method.pdf}
 	\vspace{-3mm}
     \caption{\textbf{Algorithm overview.} Our framework consists of a visual feature extractor, an audio feature extractor, an intra-frame sampling module, and an inter-frame relation module. 
     \textit{(upper-left)} Sound localization $\Tilde{R}$ is obtained by computing the correlation between the visual and audio features.
     %
     \textit{(bottom-left)} 
     Our iterative contrastive learning scheme uses the localization results predicted in the \emph{previous} training epoch as the \emph{pseudo}-labels for the current epoch.
     %
     \textit{(upper-right)}
     The intra-frame sampling module uses the pseudo-labels to extract (non-)sounding regions for enhancing the efficacy of the contrastive learning.
     %3) The intra-frame sampling uses the pseudo-labels to extract the (non-)sounding regions to for the contrastive learning.
     %
     \textit{(lower-right)} 
     The inter-frame relation module determines the correlation of images and audios sampled across videos by observing the relationship in the audio modality.
     %\hy{(a) the four module blocks are not aligned. (b) R in the caption while R tilde in the upper-left figure. (c) for inter-frame relation is the audio feature from previous epoch or current epoch }
    }
    \vspace{\figmargin}
	\label{fig:method}
\end{figure*}
