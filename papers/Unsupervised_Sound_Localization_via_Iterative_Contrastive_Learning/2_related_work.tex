\section{Related Work}
\vspace{\secmargin}
\Paragraph{Self-Supervised Audio-Visual Representation Learning.} 
%
Inherent correlation among different modalities of a video provides supervisory signals for learning a deep neural network model.
%
Information sources used in existing self-supervised audio-visual representation learning methods can be broadly categorized as follows. 
%
First, audio-visual pairs are extracted from a video clip as positive association. 
%
The assumption is that the audio and visual features extracted from the same video clip should be strongly correlated ~\cite{av_iccv17_look,av_eccv18_obj_that_sound,av_nips16_soundnet,av_eccv16_abSound,avt_nips20_VersatileNet,av_nips20_xdc,av_nips20_CrossLabelling,av_iclr21_activeContrastive}. 
%
In addition, these schemes differentiate the features extracted from unpaired video clips.
%
Furthermore, some concurrent methods~\cite{av_cvpr21_agreementAVID,av_cvpr21_RAVID} jointly consider the correlations within each modality or across different modalities (i.e., audio and vision).
%
Different from~\cite{av_cvpr21_agreementAVID,av_cvpr21_RAVID} that learn visual information of an entire image, our method leverages pseudo-annotations to provide training guidance from both sounding and non-sounding regions.
% 
Second, video temporal information~\cite{av_eccv18_Owens,av_nips18_coop} is explored to determine strong or weak correlation. 
%
%MH: check this sentence
%A few methods sample audio and visual features extracted from the same video as strong correlation, and consider those across different time instances as weak correlation for the representation learning~\cite{av_eccv18_Owens,av_nips18_coop}.
Given a video sequence, a few methods sample the audio and visual features from the same time frame as strong correlation and consider those across different frames as weak correlation for the representation.
%
Third, spatial relations among image regions are exploited. 
%
Since the binaural recording techniques (spatial audio) preserve the spatial information of the sound origins, some approaches~\cite{25d,av_nips20_spatial_alignment,360gen,av_cvpr20_tell_left_right,av_eccv20_sep-stereo,my_aaai21_avconsistency,av_cvpr21_withoutBinaural,lu2019self} jointly model the visual and audio information spatially to construct spatial audio generation systems or learn representations for downstream tasks.
%

%\yb{
%While our method based on the methods tackling with video and monaural audio data, we can further leverage the relationship between audio instances and pseudo non-sounding regions to learn regions of interests across instances rather than utilizing such correlation to learn global visual information~\cite{av_cvpr21_agreementAVID,av_cvpr21_RAVID}. 
%}
%\hy{what is "reassembling" and "cost function"}\ybc{revised}

\input{figure/fig_method}
\Paragraph{Sound Source Localization in Visual Scenes.}
This task aims to find corresponding sounding regions in images from audio signals. 
%
We categorize methods addressing this task into three groups.
%
The first group of work~\cite{av_eccv20_objs_vids,av_cvpr18_lls,av_tpami20_lls} leverages the correspondence between audio and visual signals for supervision.
%
These methods assume that the audio and visual features extracted from the same video clip should be more similar than those extracted from different clips.
%
Some sound localization methods~\cite{av_cvpr18_lls,av_tpami20_lls} are formulated in a semi-supervised way to deal with limited annotated data.
%
The second line of work uses the class activation map (CAM)~\cite{cvpr16_CAM} to determine discriminative regions for categorical prediction.
%
Owens~\etal~\cite{av_eccv18_Owens} learn the audio and visual representations by the audio-visual correspondence and perform sound localization using the CAM model. 
%
Similarly, given the event labels, Qian \etal~\cite{av_eccv20_mms_loc} use the CAM model to identify sounding regions and corresponding audio clips.
%
As such, the sound and visual object in the same event can be associated.
%
Finally, some models~\cite{av_cvpr19_deep_cluster,av_arxiv_curricumlum_av_clutser} utilize audio-visual clusters to model audio-visual relationships.
%
These methods cluster different frequencies of an audio signal and visual patches in the images.
%
The centers of the audio and visual clusters extracted from the same video clip are associated during the training stage.

We note that existing sound localization approaches are limited in several aspects. 
%
These methods typically require additional information in other modalities (\eg optical flow~\cite{av_eccv20_objs_vids}), a pre-defined number of sound sources~\cite{av_cvpr19_deep_cluster,av_arxiv_curricumlum_av_clutser}, event labels in both audio and visual modalities~\cite{av_eccv20_mms_loc}, or single-source videos~\cite{av_nips20_loc}. 
%
In this work, we present a sound localization framework that does not rely on any additional annotation or assumption. 
%
Furthermore, the correlation between (non-)sounding objects and audio across pairs is jointly considered to further enhance sound localization.% in our model to localize sound sources accurately.  
%
% \yenyu{Please check the previous sentence.} yb:ok