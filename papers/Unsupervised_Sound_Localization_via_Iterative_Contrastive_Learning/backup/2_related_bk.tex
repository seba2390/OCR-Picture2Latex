\section{Related Work}
\Paragraph{Self-Supervised Audio-Visual Representation Learning.} 
%
%Videos provide natural supervision via inherent correlation among modalities, which serve as natural supervisory signal for training a model.
The inherent correlation among different modalities of a video provides natural supervisory signals for learning a deep neural network model.
%provides vital information to train a machine to understand it without human supervision. 
%
Existing self-supervised audio-visual representation learning methods
can be broadly categorized as follows. 
% simple supervision
First, audio-visual pairs are extracted from a video clip as positive correlation. 
%
The assumption is that the audio and visual features should be strongly correlated if they are extracted from the same video clip~\cite{av_iccv17_look,av_eccv18_obj_that_sound,av_nips16_soundnet,av_eccv16_abSound}. 
%
%When audio and visual features are extracted from the same video clip, the audio and visual representation should be highly correlated. 
%The audio and visual features should be correlated if they are extracted from the same video clip.
%
In addition, these schemes differentiate the features extracted from the unpaired video clips.
%assume the correlation between unpaired audio and videos should be low.
% \james{"respectively" implies you mean correlation between audios from different videos should be low, so do videos. But it should be unpaired videos and audios have low correlation}
Furthermore, some approaches~\cite{av_arxiv_agreement,avt_nips20_VersatileNet} consider the correlation within each modality or across more modalities (i.e., audio, vision, and text) to optimize models.
%\james{can not quite get it what is the correlation in a single modality}
%\ybc{should be "each": correlation in audio/visual modality respectively [16]}
% 
Second, temporal information with a video is considered to determine strong or weak correlation. 
%
%MH: check this sentence
%A few methods sample audio and visual features extracted from the same video as strong correlation, and consider those across different time instances as weak correlation for the representation learning~\cite{av_eccv18_Owens,av_nips18_coop}.
Given a video sequences, a few methods sample the audio and visual features extracted from the same time frame as strong correlation, and consider those across different time instances as weak correlation for the representation.
%
Third, spatial relations among image regions are exploited. 
%jointly modeling spatial audio and vision: 
%
Since the binaural recording techniques (spatial audio) preserve the spatial information of the sound origins, some approaches~\cite{av_nips20_spatial_alignment,av_eccv20_sep-stereo,av_cvpr20_tell_left_right,25d,360gen} jointly model the visual and audio information spatially to construct spatial audio generation systems or learn representations for downstream tasks.
%associate visual and audio information, especially binaural recording to build spatial audio generation systems or deploy on downstream tasks. 
%


%However, we note that, the mainstream media contain only monaural recording (single channel). Therefore, we mainly focus on monaural audio localization in visual scenes.
%In this work, we focus on monaural audio localization in visual scenes.
%
%Even though the aforementioned methods present decent results on video-based applications, only the relationship of audio-visual pairs across videos or time but same videos is considered during learning process.
%
%We propose a framework, which benefit from further exploring the relationship between objects in the same visual scenes and audio signal across pairs, to learn discriminative audio-visual features on sound localization task.
% \james{kinda weird to spend quite a lot of words on spatial audio}
% \james{It's quite unorganized. Maybe I'll organize them this way: Videos provides natural supervision via inherent correlation among modalities. There are three branches. First, audio-visual pair. Second, taking temporal into consideration. Third, taking spatial into consideration.}
\input{figure/fig_method}
\Paragraph{Sound Source Localization in Visual Scenes.}
Sound source localization in visual scenes aims to find sounding regions from audio signals. 
%
We summarize the methods for tackling this problem into three categories.
% Recently, some works~\cite{av_eccv18_obj_that_sound,av_eccv18_Owens,eccv18_avel} either utilize the correspondence between audio and visual patches or class activation map (CAM)~\cite{cvpr16_CAM} to explore sounding regions.
%
%First, the correspondence between audio and visual signal provides guidance to training~\cite{av_cvpr18_lls,av_tpami20_lls,av_eccv20_objs_vids}.
The first group of work~\cite{av_cvpr18_lls,av_tpami20_lls,av_eccv20_objs_vids} leverages the correspondence between audio and visual signals for supervision.
%
The assumption is that the audio and visual features extracted from the same video clip should be similar, while those extracted from different clips are differentiated.
%the aaaudio-visual correspondence learning: by exploiting audio and visual features from video clips, audio /visual features should be similar from the same video clips. 
%
%Otherwise, audio-visual representations from the different video clips are likely to be dissimilar.  
%
%some works~\cite{av_cvpr18_lls,av_tpami20_lls,av_eccv20_objs_vids} are proposed with this kind of guidance. 
%
Some sound localization methods~\cite{av_cvpr18_lls,av_tpami20_lls} are formulated within the semi-supervised framework to deal with limited annotated data.
%
%train the models in a semi-supervised fashion, which utilize sounding regions annotations.
%
%Since it is arduous to collect the annotations of sounding regions due to the labor-intensive
%annotation process, methods without these annotations would be valuable.
%Second, class activation map (CAM)~\cite{cvpr16_CAM}-based methods are proposed without the annotation of sounding regions.
%CAM can interpret the prediction decision made by models and indicate discriminative regions used for categorical prediction.
The second line of work uses the class activation map (CAM)~\cite{cvpr16_CAM} to determine discriminative regions used for categorical prediction.
%
%Since CAM is able to explore salient image regions by classes, 
Owens \etal~\cite{av_eccv18_Owens} learn the audio and visual representations by the audio-visual correspondence and perform sound localization using the CAM model. 
% 
%Similarly, in~\cite{av_eccv20_mms_loc}, given audio/visual event labels, audio signal and regions of corresponding events are extracted by CAM 
Similarly, given the event labels, Qian \etal~\cite{av_eccv20_mms_loc} use the CAM model to extract sounding regions and corresponding audio clips.
%
As such, the sound and visual object in the same event can be associated.
%
Finally, some models~\cite{av_cvpr19_deep_cluster,av_arxiv_curricumlum_av_clutser} utilize audio-visual clusters to model audio-visual relationships.
%
These methods cluster different frequencies of audio signal and visual patches in the images.
%
The centered features of the audio and visual clusters extracted from the same video clip are associated during the training stage.


We note that existing sound localization approaches are limited in several aspects. 
%
These methods typically require additional information in other modality (\eg optical flow~\cite{av_eccv20_objs_vids}), pre-defined number of sound source~\cite{av_cvpr19_deep_cluster,av_arxiv_curricumlum_av_clutser}, event labels in both audio and visual modalities~\cite{av_eccv20_mms_loc}, or single-source videos~\cite{av_nips20_loc}. 
%
In this work, we present a sound localization framework without the requirement of any annotation or assumption. 
%
Furthermore, the correlation between (non-)sounding objects and audio across pairs are jointly considered in our model to localize sound sources accurately. 
% \james{Similar to the previous paragraphs, it's quite unorganized. Do not just list works out like A does X, B does Y, C..... But try to cluster them in high-level concept.}

% \yb{Audio-Visual Separation???} 
% \hyt{have this only if we have the separation results in the exp.}
