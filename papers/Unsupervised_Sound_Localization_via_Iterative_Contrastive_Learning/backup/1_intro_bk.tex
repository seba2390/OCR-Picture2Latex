\section{Introduction}\label{sec:intro}

% \yb{To describe problem and the importance of this topic. (e.g, the sound localization would benefit downstream audio-visual tasks like av separation. Since ...) or refer to other CVPR/ECCV paper.}\\

% 1. the goal of the sound localization tasks
% 2. why does this problem important? describe some down-stream applications
Multisensory signals (\eg vision, touching, and hearing) provide rich information when human beings perceive the environment.
%When human beings perceive the environment, the multisensory signals (\eg vision, touching, and hearing) provide rich information.  
%When human beings perceive multisensory feelings in the world (e.g., vision, touching, and hearing), the data from different modalities provide rich information, which enable human beings to achieve complicated tasks in our daily life. 
%
Moreover, these different modalities are usually intertwined and thus enable human beings to achieve complicated tasks in our daily lives.
%
Take audio and vision as example, we can easily imagine a lightning scene upon hearing thunders, we can associate multiple objects with their sources on a noisy street, and we can easily identify and converse with our friends in a crowded cocktail party. 
%
In this work, we target at the \emph{sound localization} task ~\cite{av_cvpr18_lls,av_tpami20_lls,av_eccv20_mms_loc,av_nips20_loc} which aims to find regions co-occurring with audio events.
% \james{the definition looks weird to me}
%Take audio and visual as example, human beings are able to do tasks including recognizing objects, separating clear from mixed signal, localizing sound, etc. In this work, we will target at \textit{sound localization} ~\cite{av_cvpr18_lls,av_tpami20_lls,av_eccv20_mms_loc,av_nips20_loc}, which requires one to identify objects and determine whether objects make sound or not \ybTodo{(in Fig??)}. 
% 
The sound localization algorithms serve as the cornerstone techniques to various audio-visual applications such as audio-visual source separation~\cite{pix,av_eccv18_sep,co_sep_iccv19,sofm_iccv19,av_iccv19_mpnet,av_cvpr20_sep-gesture} and audio-visual event localization~\cite{eccv18_avel,av_eccv20_avvp}.
%A good sound localization algorithm can serve as a cornerstone to various audio-visual applications such as audio-visual source separation and audio-visual event localization.
% \ybc{need citation or not ?}\hyt{yes}
% \james{One more use case here.}
%Therefore, it would be desirable if one can design a machine to identify the sounding regions of interest in a scene, which is a stepping stone to understand multisensory data in the world. 
%
%The capability of sound localization can benefit audio-visual tasks such as audio-visual source separation~\cite{co_sep_iccv19, pix,sofm_iccv19} because it requires the model to identify sounding objects and separate their sound. 
% \james{kinda weird to narrow down so fast "we can achieve complicated tasks!!"->"btw, sound localization is rly great". Might be better: human can handle multiple sensor joinly->take audio/visual as example, human can do tasks including...->in this work, we'll target at localization, which is ....}



% \yb{To tackle with this problem. some work.... how to deal with this problem.} \hyt{describe the challenges of this task first, then summarize the current solutions and their drawbacks in next paragraph.}\\

% challenges in this tasks:
% 1. lack of audio/image source localization annotation (e.g., bounding box) 
% 2. the sound source may come from partial region of an image
% 3. ...
\input{figure/teaser}
% \hyt{need to revise the flow of this paragraph, see comments in the slack}
Sound localization approaches based on deep learning require a large amount of training data with the annotations of sound-visual association.
% Since associating sound and objects requires supervision from jointly observing both audio and visual data, the annotations of sounding regions is difficult to access.
%
% \james{still, it is not "difficult for a machine", it's difficult for human to label. So, assume we have perfect data, we can train perfect models? If not, we can say: "Data is a big issue cuz label is hard to get->someone propose dataset->however, even with perfect data, the leraning is still not easy because XYZ"}
% \james{I cannot get it why this is an issue}
%
To this end, Senocak~\etal~\cite{av_cvpr18_lls,av_tpami20_lls} propose the first sound localization dataset containing bounding box annotations of sounding regions.
% and a sound localization framework \hyt{not necessary to have "a sound localization framework" as it is never mentioned hereafter"}.
%
These annotations provide the supervised guidance for models to learn from audio/visual information.
%semi-supervised and unsupervised manner.
%\james{why mention semi and unsupervised here???}
%\ybc{20,21 did not present fully supervised results}
% \james{How does this dataset resolve the difficulty above? or they are two different issues (data and partial regions?)}
% 
Nevertheless, the annotated images with sounding regions are not sufficient (only 5k annotations). 
%
We note that, it is arduous to collect a large amount of sounding regions annotations due to the labor-intensive annotation process, which requires annotator to jointly observe visual and audio signals and easily results in noisy data.
% Nevertheless, even with annotations data, by observing audio and visual inputs to localize sound is still a challenging task because complex scenes only provide imprecise sounding region information to associate corresponding sound.
% 
%Several methods~\cite{av_cvpr18_lls,av_tpami20_lls,av_arxiv_curricumlum_av_clutser,av_cvpr19_deep_cluster} tackle with that issue by leveraging the image-audio correspondence information to achieve sound localization in a (semi-) unsupervised manner.
% Several efforts have been made to learn the sound localization task.
% 
%Nevertheless, merely using the audio-visual correspondence is not informative enough since the image may contain non-sounding or ambient regions, which would associate incorrect sounding object and audio signals.

% \yb{However, the drawback of these methods and limitation.}\hyt{summarize current solutions and their drawbacks here}\\
To tackle the task with limited data, several methods are proposed in a weakly supervised manner with audio-visual event labels~\cite{av_eccv20_mms_loc} or a self-supervised manner by leverage image-audio correspondences~\cite{av_cvpr18_lls,av_tpami20_lls} and audiovisual clusters~\cite{av_arxiv_curricumlum_av_clutser,av_cvpr19_deep_cluster}.
%
%methods~\cite{av_cvpr18_lls,av_tpami20_lls,av_arxiv_curricumlum_av_clutser,av_cvpr19_deep_cluster} tackle issue by leveraging the image-audio correspondence information to achieve sound localization in a (semi-) unsupervised manner
%Without any supervision of sounding regions, a number of methods~\cite{av_eccv20_mms_loc,av_arxiv_curricumlum_av_clutser,av_cvpr19_deep_cluster} jointly leverage audio-visual representations have been proposed.
% \hyt{consider "feature" is confusing, just say leverage audio-visual feature representation learning}
% \ybc{w/o location annotation. audio/visual events are weaker supervision than location one}
% \james{What does it mean by "prior knowledge on sounding regions"? labelled audio/visual events seem to be a strong human prior annotation to me}
%
%Audiovisual clusters~\cite{av_cvpr19_deep_cluster,av_arxiv_curricumlum_av_clutser} are built up to associate audio signal and visual regions, and Qian~\etal~\cite{av_eccv20_mms_loc} exploit audio/visual event labels to find out potential sound regions. 
%
However, all these methods rely on certain extent of human prior knowledge such as the pre-defined number of clusters~\cite{av_cvpr19_deep_cluster,av_arxiv_curricumlum_av_clutser} or image-level audio-visual event labels~\cite{av_eccv20_mms_loc}.
%
Furthermore, using the audio-visual correspondence alone is not informative enough since the image may contain non-sounding or ambient regions, which will lead to the association between incorrect sounding object and audio signals.
%
These constraints limit the generalization of sound localization in unconstrained scenarios (e.g., single/multiple sound sources, objects undefined in the annotations).
%\james{still not sure what you mean novel. Is it unseen objects during training?}
%\ybc{Some objects may not be defined in label spaces. }
% \james{"unseen" is ambiguous, it can refer to unseen instances during training, occluded objects in a scene, background sound, etc}
%
% \james{quite wordy, can move most of these to related work. Might be better to state sthg like: There are n branches of work -> But they require either A~\cite{}, B~\cite{}, or C~\cite{}. -> They are bad bad.}
% \yb{To address aforementioned issue. We exploit ... and ... to ...., which can further ??? to benefit av-localization task. (high level) }\\

% In this paper, we propose .....
% Contribution 1: briefly introduction -> address what challenges listed in paragraph #2 (e.g., we propose a iterative contrastive learning algorithm -> address the lack of audio/image source localization issue)
% contribution 2: ....
% contribution 3: ....
% yb: iterative ->  decompose visual objects
%  inter -> associate event w/o supervision and false negative?
%  intra -> exclude non-interest objects w/o labels
%In this work, we propose an iterative contrastive representation learning algorithm without any prior assumption or event label for the sound localization task.
In this work, we propose an iterative contrastive representation learning algorithm which does not require any prior assumption or event labels.
% to address the issue of lacking annotations.
% To accurately associate sounding objects and sound, we propose an iterative contrastive learning algorithm, which includes potential sounding regions and exclude non-interest objects without any prior assumption/event labels.
% \hyt{"includes potential sounding regions and exclude non-interest objects" is what sound localization does. $\rightarrow$ In this work, we propose an iterative contrastive representation learning algorithm without the requirement of prior assumption/event labels for the sound localization task.}
% \james{so...this sentence indicates that the major goal and main features for the proposed method is to "eliminate sielent objects"???}
%
We illustrate the core idea of our method in \figref{teaser}.
%
Starting from the conventional contrastive learning (\textit{baseline} in \figref{teaser})~\cite{av_iccv17_look,av_eccv18_obj_that_sound,av_nips16_soundnet,av_eccv16_abSound} \ybc{put the ref. in audio-visual learning with baseline sampling technique}
% \hyt{(should we have a citation for infoNEC here?)}
, the proposed approach uses the sound localization model obtained in \emph{previous} epoch to estimate sounding and non-sounding regions as the \emph{pseudo}-labels for the current training epoch.
%To be more specific, the core idea is to utilize the sound localization trained in \emph{previous} epoch to sample the (non-)interest regions to be considered in the current training epoch.
% Specifically, sounding regions of (non-)interest from previous(?) model are considered during current model training.
% \hyt{basically you want to describe what "iterative" is for $\rightarrow$ The core idea is to use the sound localization model trained in the \emph{previous} epoch to sample the (non-)interest regions to be considered in the current training epoch.}
%
% Since there are a few objects making sound in visual scenes, audio signal would be a favorable guidance to model the correlation of audio-visual data. \james{cannot understand this}
%
In addition to exploiting the relations between audio and visual signals within an instance, we further leverage the relationships among audio signals across instances, as the \emph{inter-frame relation} shown in \figref{teaser}.
%Furthermore, we leverage the relationship between audio signals to guide the audio-visual learning.
%
\yb{Specifically, the representations of different sounding objects should be similar if the corresponding audio instances are highly correlated, which preventing pulling sounding objects away from related audio signals with naive contrastive learning.
%HYT
%when audio instances are highly correlated, it indicates that the corresponding sounding objects are likely to be similar.
%
It would be a reliable guidance to learn accurate sound localization by exploring relationship between audio instances.}
%\james{Why? Is it: prevent sounding obj not ass with related audio signgla, or prevent audios signal ass with unrelated sounding obj??}
%\ybc{The former one is correct. e.g., if two pairs contain train in it, but original one would repel the distance between these pairs.} % 避免應該要拉近的 卻被推遠
% Furthermore, in order to preserve the relationship between audio and visual data without any supervision,
% we exploit the correlation between all the audio instances to model the relationship in audio-visual data. 
% %
% In other words, the relationship between audio-visual and audio data should be consistent. 
% %
% Such correlation information would guide our model to associate the same event audio signal and objects even lacking of labels supervision. 
% \hyt{see the comment in the slack}
%
We evaluate the proposed method on the SoundNet Flickr dataset~\cite{av_cvpr18_lls,av_tpami20_lls} with the consensus intersection over union (cIoU) and area under curve (AUC) as evaluation metrics.% to support the effectiveness of our framework.
%
Both the qualitative and quantitative results demonstrate the effectiveness of the proposed method. 
% \james{Quite vague, I cannot tell what exactly we propose at the first glance. Might do: We propose an awesome algo-> First, to handle X, we use \textbf{XX}. Second, to handle Y, we use \textbf{YY}}
% \james{Experiments missing. Just need to mention, we use XYZ metrics on ZYX datasets. We beat them all. We good}

% \hyt{[optional] briefly describe the experiments}\\
% \yb{To highlight our key contribution..}\\
%%%%%%%%%%%%%%%%
The contributions of this paper are highlighted below:
\begin{compactitem}
  \item We propose an iterative contrastive learning algorithm for the unsupervised sound localization task without the requirement of data annotations. %HYT
  %We propose a unsupervised training strategy for the sound localization task.
  %
  %The proposed iterative contrastive learning algorithm can associate audio signals to regions of interest.
  %which excludes non-interest regions for robust training, which prevents non-sounding objects associated with audio source.
  %
  \item We propose to leverage not only the regions of interests, but also exploit the non-sounding regions and the relationship across audio instances to jointly learn the audio and visual representations.
  %\ybc{may sounding regions of non-interest be confusing? means non-sounding?}
  %which guides the sound localization process with improved performances.
  %
  \item Qualitative and quantitative experiments on the benchmark dataset demonstrate that the proposed method performs favorably against state-of-the-art \emph{weakly supervised} and \emph{unsupervised} approaches. %with any  and confirm that the learning scheme can be deployed without any human supervision.
%   
  %\james{Btw, isn't our results are way better than the weakly-supervised method? Should stress it somewhere}
\end{compactitem}

