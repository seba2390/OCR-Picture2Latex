\begin{abstract}
%
Human beings perform various tasks by jointly perceiving multisensory signals.
%
However, it is time-consuming and labor-intensive to annotate the correlations of signals from multiple sensors (modalities), thus making it difficult supervise a machine to learn such tasks.
%
In this work, we focus on the sound localization task, which relies on the mutual understanding of visual and audio cues to associate sounds with corresponding objects in the visual scene.
%
We propose an iterative contrastive learning framework that requires no data annotation. 
%
Starting from simple contrastive learning, the proposed training method iteratively takes the localization results predicted in the previous epoch as pseudo-annotations of sounding and non-sounding regions, and learns the correlation between visual and audio signals within instances as well as the correlation between audio signals across instances. 
%
The training method encourages localization of sounding regions and reduces association between non-sounding objects with audio sources. 
%By exploiting the localization results from previous epoch, our training strategy can utilize (non-)sounding regions to learn robust audio-visual features to achieve accurate localization results.
%
%Furthermore, we observe the correlation between audio signals across instances, which prevents non-sounding  objects associated with audio source.
%
Quantitative and qualitative experimental results demonstrate that the proposed self-supervised framework performs favorably against existing unsupervised and weakly-supervised methods for sound localization.
%Experiments on the benchmark dataset validate the effectiveness of our proposed framework in an unsupervised scenarios, with ablation studies and visualization further support the use of our model for sound localization.


% \keywords{Audio spatialization, self-supervised learning, deep learning}


\end{abstract}  