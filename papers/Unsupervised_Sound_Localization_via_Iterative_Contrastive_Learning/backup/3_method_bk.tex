
\section{Methodology}
\label{sec:method}

\subsection{Preliminaries}
Our goal is to learn a sound localization model in an unsupervised fashion.
%
During the testing time, we are given the input image of shape $W\times H\times 3$ and the detected audio (\ie sound).
%
We denote the term $M=WH$.
%
As shown in the top side of \figref{method}, the proposed sound localization model first extracts the corresponding visual representation $\mathbf{V}=[\mathbf{v}_1,\cdots,\mathbf{v}_M]\in\mathbb{R}^{M \times d}$ and audio feature $\mathbf{a}\in\mathbb{R}^{1 \times d}$ from the input image and the short-time Fourier-transformed~\cite{stft} audio, respectively.
%
To localize the sounding source in the image, we use the attention mechanism to compute the response map $\mathbf{R}=[\mathbf{r}_1,\cdots,\mathbf{r}_M]\in\mathbb{R}^{M\times 1}$, where
\begin{equation}
    \mathbf{r}_i = \mathbf{v}_i^\top \cdot \mathbf{a}\hspace{5mm} i = 1,\cdots,M.
\end{equation}
%
Then the sounding region $\mathbf{S}\in\mathbb{R}^{w\times h\times 3}$ is determined by thresholding the response map $\mathbf{R}$.


In Section~\ref{sec:av-learning}, we first introduce the initial audio-visual training scheme with contrastive learning as the paradigm for the following epochs. 
% 
We then further present our iterative learning strategy and proposed sampling techniques to learn discriminative audio-visual features. 
% 
How sound localization task can be achieved by audio-visual response will be demonstrated in section~\ref{sec:results}.


% \hyt{Use the notation to describe the problem to be solved (can refer to the method overview figure)}
% We first define the notations and settings for sound localization in this paper. 
% problem
Our goal is to associate sounding regions $\mathbf{v}\footnote{note that $w,h$ is smaller compared with original image inputs for simplicity} \in \mathbb{R}^{d \times w \times h}$ and corresponding audio signal $\mathbf{a} \in \mathbb{R}^{d \times 1 \times 1}$. 
% 
Note that $d$ indicates the dimension of each channel; $w$ and $h$ denote as the size of visual feature.
%
A batch of visual regions $\mathbf{V} = [\mathbf{v}_1,\mathbf{v}_2,\ldots,\mathbf{v}_k]$ and audio signals $\mathbf{A} = [\mathbf{a}_1,\mathbf{a}_2,\ldots,\mathbf{a}_k]$ with total $k$ pairs are the inputs.
% 
For each visual feature $\mathbf{v} = [\mathbf{v}(1),\mathbf{v}(2),\ldots,\mathbf{v}(w \times h)]$, it contains $w \times h$ patches.
% 
The model would be trained in total $n$ epochs.
%
For example, we illustrate our training framework with a audio-visual pair shown in Fig~\ref{fig:method}. 
% 
The input audio signal is transformed into frequency domain by short-time Fourier transform (STFT)~\cite{stft} and encoded as audio feature $\mathbf{a}$ by an audio network with global average pooling.
% 
As for visual feature $\mathbf{v}$, the input image is encoded by a visual network but without performing average pooling.  
% 

In Section~\ref{sec:av-learning}, we first introduce the initial audio-visual training scheme with contrastive learning as the paradigm for the following epochs. 
% 
We then further present our iterative learning strategy and proposed sampling techniques to learn discriminative audio-visual features. 
% 
How sound localization task can be achieved by audio-visual response will be demonstrated in section~\ref{sec:results}.


\subsection{Audio-Visual Representation Learning}
\label{sec:av-learning}

\Paragraph{Initial Contrastive Learning.} 
% how to train our baseline ... 
% \hyt{illustrate the learning scheme of the baseline.}
Natural audio-visual pairs from videos provide rich information to learn robust representation by optimizing an objective. 
% 
It maximizes correlation between sound and objects from the same pair (called positive pairs) and minimizes one between arbitrary audio and objects from different pairs (called negative pairs). 
% 
Such the property can be utilized as training objective function to learn audio-visual representation.
% 
The objective of contrastive loss can be expressed as follows: 
\begin{equation}
\begin{gathered}
\label{eq:nce_loss}
\mathcal{L}_{\text {contrast }}=-\log \frac{\exp \left(\phi(\overline{\mathbf{v}}_{i})^{\top} \cdot \overline{\mathbf{a}}_{i} / \tau_i\right)}{\sum_{j=0}^{k} \exp \left(\phi(\overline{\mathbf{v}}_{i})^{\top} \cdot \overline{\mathbf{a}}_{j} / \tau_i\right)}, \\
\forall i=\{1, \ldots,k\},
\end{gathered}
\end{equation}
where $\tau_i$ is a temperature parameter controlling the probability distribution from a softmax function. 
% 
$\phi$ and $\exp$ are average pooling function ($\phi(\mathbf{v}) \in \mathbb{R}^{d \times 1 \times 1}$) and exponential function. 
% 
$\overline{\mathbf{v}}$ and $\overline{\mathbf{a}}$ denote as $\ell_{2}$-normalized vectors.
% 
By \eqref{eq:nce_loss}, audio-visual representations are optimized at the initial stage (\eg the first five epochs).


\Paragraph{Iterative Contrastive Learning.}
% \hyt{1) baseline may use the features of incorrect region, 2) region annotation is not available, 3) describe proposed iterative scheme}
Although the audio-visual features can be learned by \eqref{eq:nce_loss}, it would include all visual patches containing either sounding or non-sounding regions. 
% 
The similarity between non-sounding objects and audio signal is maximized under this circumstance. 
% 
If sounding regions are known or accessible before performing contrastive learning, non-sounding regions would not be associated with the corresponding audio signal. 
% 
Nevertheless, there is no prior information about sounding regions in an unsupervised sound localization manner. 
% 


To this end, we consider potential sounding regions from the previous model into contrastive learning instead of directly performing average pooling on all visual patches. 
% 
To include potential sounding regions, we randomly sample visual patches which are highly related audio signals.
%
The stochastic sampling process can benefit representation learning by preventing models from overfitting training audio-visual pairs.
%
The visual features of sounding regions are described as follows:  
\begin{equation}
\begin{aligned}
\label{eq:pos_idx}
\mathbf{I}_{pos} &= idx(\overline{\mathbf{v}}^{\top} \cdot \overline{\mathbf{a}} > \delta_{v}), \\
\mathbf{v}^{+}&= \frac{1}{r}  \sum_{i=0}^{r} \mathbf{v}(\mathbf {I}_{pos}(i)),
\end{aligned}
\end{equation}
where $idx(.)$ returns the indexes of visual patches from given conditions in a random order, and $\delta_{v} \in [0,1]$ is a threshold for excluding non-sounding regions. 
% 
Note that the audio-visual features are extracted from the model from the previous epoch. 
% 
In~\eqref{eq:pos_idx}, $r$ is the number of sampled patches that is less than  $\mathbf{I}_{pos}$.
% 
If no patch is confident enough to be sounding regions, $\mathbf{v}^{+}$ would be replaced with $\phi(\mathbf{v})$.


\Paragraph{Intra-Frame Sampling.}
% \hyt{1) in the proposed iterative approach, we use psudo bbx as the positive example, 2) remaining region is not used, 3) we use it as additional negative example}]
% 
The key idea of contrastive learning is to model the relationship between all instances.
% 
It would be favorable to add additional instances as negative pairs. 
% 
However, directly sampling more negative pairs from other instances cannot benefit model training due to easy negative pairs which limit models to learn discriminative audio-visual features. 
% 
With aforementioned iterative contrastive learning scheme, we exploit the pseudo sounding regions as the positive examples. 
% 
Since some sounding objects are accessible from the paradigm model at previous epoch, the possible non-sounding regions can be further determined as well.
% 
Similarly, non-sounding regions can be obtained by:
\begin{equation}
\begin{aligned}
\label{eq:neg_idx}
\mathbf{I}_{neg} &= idx(\overline{\mathbf{v}}^{\top} \cdot \overline{\mathbf{a}} < \delta_{v}), \\
\mathbf{v}^{-}&= \frac{1}{r}  \sum_{i=0}^{r} \mathbf{v}(\mathbf {I}_{neg}(i)).
\end{aligned}
\end{equation}
Note that if no patch is lower the threshold, $\mathbf{v}^{-}$ would not be sampled.

\Paragraph{Inter-Frame Relation.}
Due to various objects in audio-visual pairs, audio-visual representations can be optimized by observing the correlation between audio and visual instances. %
We note that, the objects across videos may appear similar contents as shown in Fig~\ref{fig:teaser} (\eg trains appear in different pairs).
% 
However, the common contrastive learning schemes in an unsupervised manner would not associate between sounding objects and similar sound but in other pairs. 
% 

To this end, we propose a training strategy to associate sounding regions and audio even across pairs by observing relationship between all sound signals in the batch. 
% 
We can redefine the audio-visual pairs as follows:
\begin{equation}
\begin{aligned}
\label{eq:a_th}
y_{i,j}=\left\{\begin{array}{ll}
0, & \text { if } \overline{\mathbf{a}}_{i}^{\top} \cdot \overline{\mathbf{a}}_{j} < \delta_{a} \\
1, & \text { if } \overline{\mathbf{a}}_{i}^{\top} \cdot \overline{\mathbf{a}}_{j} \geq \delta_{a}\\
\end{array}\right. 
\forall i, j=\{1, \ldots,k\},
\end{aligned}
\end{equation}
where $\delta_{a} \in [0,1]$ is a threshold for adjusting the audio-visual pairs in the batch. With the redefined correlation and aforementioned sampling techniques, the training objective after initial contrastive learning is formulated as:
\begin{equation}
\label{eq:full}
\begin{aligned}
&\mathcal{L}_{\text {av-complex }}= \\ &-\log
\frac{\sum_{i,j=0}^{k} \exp ( (\overline{\mathbf{v}}_{i}^{+})^{\top} \cdot \overline{\mathbf{a}}_{j} / \tau_{itr}) y_{i,j}}
{\sum_{i,j=0}^{k} \exp ((\overline{\mathbf{v}}_{i}^{-})^{\top} \cdot \overline{\mathbf{a}}_{j} / \tau_{itr})
+ \exp ((\overline{\mathbf{v}}_{i}^{+})^{\top} \cdot \overline{\mathbf{a}}_{j} / \tau_{itr})
}.
\end{aligned}
\end{equation}
Note that we optimize our framework by~\eqref{eq:nce_loss} at initial stage. Otherwise, the model is trained with~\eqref{eq:full}.


% \subsection{Sounding Object Localization}
% \label{sec:localization}
 