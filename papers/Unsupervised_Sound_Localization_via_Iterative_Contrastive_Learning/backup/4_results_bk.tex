
% \input{table/stoa.tex}

\section{Experimental Results}
\label{sec:results}

%\subsection{Datasets}
%\label{sec:dataset}
\Paragraph{Datasets.} The \textbf{SoundNet-Flickr}~\cite{av_nips16_soundnet} dataset consists of more than two million video sequences.
% 
We use the $5$-seconds audio clip and corresponding image, \ie video frame extracted from the center of the $5$-seconds interval, to form the input pair for the proposed framework.
% 
Note that we do not rely on any annotation (\eg bounding boxes) for the training.
%
In all experiments, we perform the training using the subset of the SoundNet-Flickr dataset constructed by Qian~\etal~\cite{av_eccv20_mms_loc} that contains $10$k audio-visual pairs.
%
Following the protocol in~\cite{av_eccv20_mms_loc,av_cvpr18_lls,av_tpami20_lls}, we conduct the evaluation using the testing set o the SoundNet-Flickr dataset that consists of $250$ audio-visual pairs with bounding box annotations.
%
%As for quantitative evaluation, Senocak~\etal~\cite{av_cvpr18_lls,av_tpami20_lls} provide bounding boxes of sounding regions for the subset of SoundNet-Flickr, which contains 5k annotations.
% 
%In our experiment, we train with 10k pairs (no supervision) in a random sample subset\footnote{github.com/shvdiwnkozbw/Multi-Source-Sound-Localization} of SoundNet-Flickr, which are the \yb{exact} same pairs as~\cite{av_eccv20_mms_loc}.
% 
%To fairly compare with the methods, the evaluation is performed in test set of SoundNet-Flickr~\cite{av_cvpr18_lls,av_tpami20_lls} containing 250 audio-visual pairs with bounding box annotations. 
% 
%\yb{The aforementioned settings are following the ones in~\cite{av_cvpr18_lls,av_tpami20_lls,av_eccv20_mms_loc}}.

\input{table/stoa}
\Paragraph{Implementation Details.} We implement the proposed method using Pytorch~\cite{pytorch}, and conduct the training as well as evalution on a single NVIDIA GTX 1080 Ti GPU with $12$ GB memory.
%
Specifically, we use the ResNet-18~\cite{resnet} model for both the visual and audio feature extractors.
%
Note that we adopt the strategy in~\cite{av_eccv20_mms_loc} that uses the visual feature extractor pre-trained on the ImageNet~\cite{ImageNet} dataset.
%
As for the data pre-processing, the raw 5-seconds audio clips are re-sampled at $22.05$ kHZ, then transformed into the log-mel spectrograms~(LMS) using $64$ frequency bins and the window of size $40$ ms for every $20$ ms.
%
The images are re-sized to the resolution of $256\times 256$.
%
For fair comparisons, we adopt the batch size of $96$ which is used in~\cite{av_eccv20_mms_loc} for all the experiments since a larger batch size may benefit our contrastive learning approach.
%
More implementation details are presented in the supplementary material.
%
We will release the source code stimulate future research.

%\Paragraph{Sound Localization.}
\ybc{main reason for initial model beats STOAs}
We first compute the response map $\mathbf{R}$ using Eq.~\eqref{eq:cos}.
%
Then we recover the resolution of the response map $\mathbf{R}$ from $w\times h$ to original image resolution $W \times H$ using bilinar up-sampling and min-max normalization on each instance:
\begin{equation}
\label{eq:normalization}
\mathbf{R} = \frac{\mathbf{R}_i -\min(\mathbf{R}_i)}{\max(\mathbf{R}_i) - \min(\mathbf{R}_i)},
\quad i=1\sim k.
\end{equation}
%
Finally, we the re-scale the values in the response map to the be in the range of $[0, 1]$.


%With audio and visual representation, it is possible to recognize sounding regions by calculating the response between visual patches and audio signal by~\eqnref{cos}.
% 
% We calculate the response with cosine similarity:
% \begin{equation}
% \label{eq:cos}
% \mathbf{C} = \overline{\mathbf{v}}^{\top} \cdot \overline{\mathbf{a}},
% \end{equation}
% where $\mathbf{C} \in \mathbb{R}^{1 \times w \times h}$ is the response maps for visual patches. 
%To correspond the size of image input, we perform bilinear up-sampling on response maps 

\Paragraph{Evaluation Metrics.}
\hyt{cannot understand this section, refer the previous paper and make it more clear}
%\Paragraph{From Bounding Box to Score Map.}
Following~\cite{av_cvpr18_lls,av_tpami20_lls,av_eccv20_mms_loc,av_cvpr19_deep_cluster,av_arxiv_curricumlum_av_clutser}, 
The bounding box annotations in coordinate form are transformed into binary maps.
% 
Note that there are multiple $N$ bounding boxes for each image (\eg $\mathbf{b}_1$ is one of the boxes).
% 
To be more specific, the patches of non-sounding/sounding regions are labeled as 0 and 1 respectively. 
% 
The representative score map $\mathbf{g}$ can be described as follows:
\begin{equation}
\label{eq:loc_gt}
\begin{aligned}
\mathbf{g}=\min \left(\frac{\sum_{i=1}^{N} \mathbf{b}_{i}}{\# \text { consensus }}, 1\right),
\end{aligned}
\end{equation}
where \#consensus is a minimum number of agreement. For example, the minimum number of people considering whether a pixel makes sounds or not. 
% 
We set \#consensus $=2$, which is suggested by~\cite{av_cvpr18_lls,av_tpami20_lls}.
% 

With ground truth score map $\mathbf{g}$, 
\textbf{consensus intersection over union (cIoU)} and \textbf{area under curve (AUC)} are utilized for measuring the quality of predicted sound localization results. 

\Paragraph{Baseline methods.}
We compare our method train with the $10$k audio-visual with the following weakly-supervised or unsupervised approaches: \hyt{do not introduce how these methods work, focus on 1) what/how manay training pairs used 2) do they use any assumption or event label}
%To evaluate the quality of our predicted sound localization, we compare our method (10K audio-visual pairs) with the following state-of-the-arts methods in a weakly supervised or unsupervised manner:
\begin{compactitem}
\item\textbf{Attention 10K~\cite{av_cvpr18_lls,av_tpami20_lls}:} The model leverages audio-visual co-attention mechanism during representation learning.

\item \textbf{DMC AudioSet~\cite{av_cvpr19_deep_cluster}:} Audio-visual clusters are utilized to minimize the distance of center audio and visual features. It is trained on AudioSet-Balanced-Train~\cite{Audioset} with 20K samples and test on SoundNet-Flick dataset.

\item \textbf{CAVL AudioSet~\cite{av_arxiv_curricumlum_av_clutser}:} 
The audio-visual cluster based method with a curriculum learning strategy to handle with number of sounding source. The training and testing scenario are the same as DMC~\cite{av_cvpr19_deep_cluster}. 
\item \textbf{MSSL 10K/20K~\cite{av_eccv20_mms_loc}:} As a state-of-the-art method for sound localization, it utilizes audio/visual event labels and CAM~\cite{cvpr16_CAM} to find sounding regions with 10K/20K training samples.

\end{compactitem}


\input{figure/cIoU_curve}
\input{figure/vis.tex}
\subsection{Quantitative Results}
% 
\tabref{stoa} shows the quantitative comparisons on the SoundNet-Flickr dataset.
% 
The proposed method performs favorably against the baseline approaches.
%
We note that different from the proposed method, the DMC, CAVL, MSSL schemes require prior assumption \hyt{be specific, what assumption}.
%
Furthermore, our model trained with $10$k audio-visual pairs show more accurate sound localization results than the MSSL-$20$k approach that uses more (\ie $20$k) audio-visual pairs for the training.
%
In addition to the cIoU\@$0.5$ metric, we present the cIoU scores calculated with various thresholds \figref{curve}.
%
Our method reports favorable cIoU scores under all thresholds, suggesting the effectiveness of the proposed iterative contrastive learning algorithm.


\input{figure/vis_abs.tex}
\input{table/ablation}
\subsection{Qualitative evaluation}
The visualization results are shown in Fig.~\ref{fig:vis} on image-sound pairs selected from the SoundNet-Flicker dataset. 
% 
From this figure, we see that our method presents more accurate sound localization results.
% 
The example in the top row is particularly challenging. Since trains usually run on the rail, models for unsupervised sound localization would jointly associate rails and trains with train sounds. 
% 
As for MSSL, due to the guide of audio/visual event labels, it can reduce influences from the noise background.
% 
It is worth noting that, no prior assumption is available our during model training. Instead, DMC, CAVL and MSSL require pre-defined number of sound source or audio/visual event labels. 
% 
Thus, these results support the use of our training strategy for discovering the sounding objects in a scene.


\input{figure/vis_inter}
\input{figure/vis_itr}
\subsection{Ablation Study}
\Paragraph{Localization and Normalization.}
Since we adopt state-of-the-art model~\cite{av_eccv20_mms_loc} with different localization and visualization technique to localize sound,
the investigation of these techniques are desirable.
% 
The brief summarization of MSSL~\cite{av_eccv20_mms_loc} is illustrated as follows:
% 
MSSL~\cite{av_eccv20_mms_loc} a two-stage sound localization model trained with extra audio-visual event labels.
% 
The event classifiers and audio-visual correspondence are utilized during the first stage.
% 
The additional label-guide contrastive learning is applied at the second stage.
% 
When the model can understanding vision and audio signals,
the sound localization results are achieved by CAM with normalizing the maximum value to one. 
% 
Note that such details in our model are described in~\eqnref{cos} and~\eqnref{normalization}.

In \tabref{ablation}, the top block indicates the results with original MSSL setting. 
% 
We can see that the performance is degraded significantly since CAM requires labels to find discriminative regions.
% 
As for the second top block, audio-visual correlation~\eqnref{cos} is utilized to find sounding regions instead of CAM.
% 
The first stage of MSSL with~\eqnref{cos} can achieve favorable performance compared with the methods, whose localization results are calculated by CAM.
% 
For the third block, we further apply minmax normalization mechanism~\eqnref{normalization}, where the results within an instance are normalized to values from zero to one. 
% 
The minmax normalization~\eqnref{normalization} can strengthen the localization results in cIoU metric at 0.5 threshold, which indicates the improvement by~\eqnref{normalization} only at certain threshold.
% 
For the bottom block, we compares and verifies the contribution of every components in our method, where Itr, Intra, and Inter denote iterative contrastive learning, intra-frame sampling, and inter-frame relation respectively described in~\secref{av-learning}. 
% 
Check mark indicates methods which are utilized.
% 
Note that the initial contrastive learning~\eqnref{nce_loss} is able to achieve favorable performance against MSSL. 
% 

Furthermore, the localization results with each module in our model are presented in~\figref{vis_abs}. 
% 
We can see that our training strategy is able to effectively alleviate the noise of non-sounding regions. 
% 
It is clear that iterative contrastive learning can provide more accurate results than initial one, and the results from our proposed method are correctly localized on sounding regions compared with only iterative contrastive learning (without intra-frame sampling and inter-frame relation).
% 
Note that intra-frame sampling and inter-frame relation are based on iterative contrastive learning.

\Paragraph{Paradigm from Previous Epoch.}
Since our model is based the results from the model at previous epoch as pseudo-annotation for current model learning, the localization results at different epoch play a critical factor of performance.
% 
To this end, we investigate the localization results at different epochs in~\figref{vis_itr}.
% 
In~\figref{vis_itr}, the localization results are gradually accurate to sounding regions, which indicates the model from previous epoch can provide favorable guide for the current model learning.


\Paragraph{From Sound to Associate Objects.}
As discussed earlier, since inter-frame relation is based on the correlation between audio signals within a batch, the visualization can explain the justification of the model design.
% 
That is, given an audio-visual pair, we would like to explore the related sounding objects by audio signals.
% 
In~\figref{vis_inter}, the first and fifth column with reference caption is the anchor images. The corresponding audio signals are utilized to measure the correlation of all testing data.
% 
We find the top three similar sound and present the corresponding images in the following columns.
% 
Our assumption that sounding objects are related if their sounds are similar is verified with the cases in~\figref{vis_inter}, which alleviates typical contrastive learning methods from differentiating the sounding object and related audio signals.
% 









