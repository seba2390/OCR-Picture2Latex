
\section{Methodology}
\label{sec:method}
\vspace{\secmargin}
\subsection{Sound Localization}
\vspace{\subsecmargin}
Our goal is to localize the source of the detected sound in the image.
%
Specifically, given the input image of size $W\times H\times 3$ and the detected audio, \ie sound, we aim to estimate the sounding region $\mathbf{S}$.
%
As shown in lower left panel of \figref{method}, the proposed sound localization model first extracts the corresponding visual representation $\mathbf{V}\in\mathbb{R}^{w\times h\times d}$ from the input image, and the audio feature representation $\mathbf{a}\in\mathbb{R}^{d}$ from the short-time Fourier-transformed~\cite{stft} audio.
%
We then use the attention mechanism to compute the response map $\mathbf{R}\in\mathbb{R}^{w\times h\times 1}$ followed by min-max normalization,
\vspace{\eqmargin}
\begin{equation}
\begin{aligned}
    \label{eq:normalization}
    \mathbf{R} &= \mathbf{V} * \mathbf{a},\\
    \mathbf{R} &= \frac{\mathbf{R} - \min(\mathbf{R})}{\max(\mathbf{R}) - \min(\mathbf{R})},
    \end{aligned}
    \vspace{\eqmargin}
\end{equation}
where the notation $*$ represents the pixel-wise inner-product operation. %, \ie convolution with a $ 1 \times 1$ kernel.
We then determine the potential sounding region by thresholding the response map $\mathbf{R}$:
%
\vspace{\eqmargin}
\begin{equation}
    \label{eq:cos}
    \mathbf{S} = \mathrm{idx}(\mathbf{R} > \delta_{v}),
    \vspace{\eqmargin}
\end{equation}
%
where $\delta_v\in[0,1]$ is a parameter for thresholding. 
%
The function $\mathrm{idx}(\cdot)$ returns the spatial indexes of the sampled patches that match the given condition. 
%\yb{Note that min-max normalization is performed on each instance before thresholding:
%\begin{equation}
%\label{eq:normalization}
%\mathbf{R} = \frac{\mathbf{R} - %\min(\mathbf{R})}{\max(\mathbf{R}) - \min(\mathbf{R})}.
%\end{equation}
%}
%
%\yenyu{$\tilde{R}$ has not been defined before (1).}
%
%Finally, the 
%\begin{equation}
%    S = \mathrm{idx}(\mathbf{\Tilde{R}} > \delta_{v}) 
%\end{equation},
%where the term $\delta_v\in[0,1]$ is a thresholding parameter and the function $\mathrm{idx}(.)$ computes the spatial indexes of the sampled patches that match the given condition,

In the following, we will illustrate how the proposed method learns to localize sound via audio-visual representation learning.
%
The baseline audio-visual contrastive learning is first introduced.
It is used for initializing our model.
%
We then present our iterative training approach and finally discuss how we leverage the relationship given in the audio signals to facilitate the contrastive learning process.



\subsection{Audio-Visual Representation Learning}
\label{sec:av-learning}
\vspace{\subsecmargin}

\Paragraph{Contrastive Learning.} 
As audio-image pairs extracted from videos provide natural implication of the correlation between the two modalities, we use contrastive learning~\cite{infonce} to learn the audio-visual feature representations in an unsupervised manner.
%
The core idea is to maximize the correlation between the audio and visual representations extracted from the same video (\ie positive pairs) while minimizing the correlation between those from different videos (\ie negative pairs).
%
Specifically, during the training stage, our model extracts a set of audio features $\{\mathbf{a}_1,\cdots,\mathbf{a}_k\}$ and a set of visual representations $\{\mathbf{V}_1,\cdots,\mathbf{V}_k\}$ from the input batch consisting of $k$ image-audio pairs sampled from the same videos.
%
Then the model is optimized by the following training objective: 
\vspace{\eqmargin}
{\small
\begin{equation}
    \label{eq:nce_loss}
    \mathcal{L}_\mathrm{contrast} = -\frac{1}{k}\sum_{i=1}^k\Big[\log\frac{\exp(\phi(\mathbf{V}_i)\cdot\mathbf{a}_i/\tau)}{\sum_{j=1}^k\exp(\phi(\mathbf{V}_i)\cdot\mathbf{a}_j/\tau)}\Big],
    \vspace{\eqmargin}
\end{equation}}
where the term $\tau$ is a hyper-parameter controlling the temperature.
%
The notation $\phi$ represents the operations of $L2$ normalization on the feature dimension followed by average pooling on the spatial dimensions.


\Paragraph{Iterative Contrastive Learning.}
Since an image typically contains both sounding and non-sounding regions, the training loss in Eq.~\eqref{eq:nce_loss} is less effective as it takes the whole image into consideration at a time, which may associate non-sounding regions with the audio signals extracted from the same video.
%
Moreover, the annotations of the sounding objects are not available under the unsupervised setting.

To this end, we develop an iterative contrastive learning approach.
%
As illustrated in \figref{method}, starting from using conventional contrastive learning in Eq.~\ref{eq:nce_loss} for initialization, we take the sound localization results predicted in the \emph{previous} training epoch as the \emph{pseudo}-labels for \emph{current} training epoch.
%
Specifically, let $\mathbf{\Tilde{R}}_i= \mathbf{\Tilde{V}}_i * \mathbf{\Tilde{a}}_i$ denote the response map predicted from the model with parameters in the previous training epoch.
%
We randomly sample the visual features from patches, which show high responses on the map $\mathbf{\Tilde{R}}_i$ in the previous epoch, as the sounding feature $\mathbf{v}^+$ \ie
\vspace{\eqmargin}
\begin{equation}
\begin{aligned}
\label{eq:pos_idx}
\mathbf{x}^\mathrm{pos}_i &= \mathrm{idx}(\mathbf{\Tilde{R}}_i > \delta_{v}), \\
\mathbf{v}^{+}_i&= \phi(\mathrm{feats}({\mathbf{V}_i,\mathbf{x}^\mathrm{pos}_i})),\quad i=1\sim k,
\end{aligned}
\vspace{\eqmargin}
\end{equation}
%where the term $\delta_v\in[0,1]$ is a thresholding parameter.
%
%The function $\mathrm{idx}(.)$ computes the spatial indexes of the sampled patches that match the given condition, 
where function $\mathrm{feats}(\cdot)$ returns a set of visual features for the given indexes. 
%
We replace the term $\phi(\mathbf{V}_i)$  in \eqnref{nce_loss} with the sounding feature $\mathbf{v}^+_i$.
%
In this way, the sounding regions are iteratively explored while non-sounding regions are gradually excluded.
%
In practice, we perform min-max normalization for $\mathbf{\Tilde{R}}_i$, same as \eqnref{normalization}, to prevent the threshold~$\delta_{v}$ too high to find confident sounding patches.

\Paragraph{Intra-Frame Sampling.}
We enhance the efficacy of the proposed contrastive learning by incorporating more negative pairs.
%
However, merely sampling more negative pairs by extracting audio and images from different videos is less effective as the model may easily determine the correlation.
%
Consequently, we propose to use the \emph{pseudo-non-sounding} regions predicted in the previous training epoch to form the negative pairs with the audio clips extracted from the same video.
%
We illustrate the process in \figref{teaser} (red line and red dotted circle) and \figref{method} (top right).
%
The correlation of these negative pairs is more challenging to determine as they are sampled from the \emph{same} video sequence, thus helping the sound localization model to learn more discriminative audio-visual representations.
%
We call such a strategy intra-frame sampling, which is formulated as follows:
\vspace{\eqmargin}
\begin{equation}
\begin{aligned}
\label{eq:neg_idx}
\mathbf{x}^\mathrm{neg}_i &= \mathrm{idx}(\mathbf{\Tilde{R}}_i < \delta_{v}), \\
\mathbf{v}^-_i&= \phi(\mathrm{feats}({\mathbf{V}_i, \mathbf{x}^\mathrm{neg}_i})),\quad i=1\sim k.
\end{aligned}
\vspace{\eqmargin}
\end{equation}



\Paragraph{Inter-Frame Relation.}
As the semantically similar contents may appear in different video sequences, contrastive learning can be further improved if it explores the correlation between images and audio signals from different videos.
%
An example is given in \figref{teaser} (black line and green dotted region).
%
Specifically, we leverage the relationship in the audio modality to determine the correlation of the image and audio clip sampled from different videos.
%
The relationship in the audio modality is estimated by using the audio representations $\mathbf{\Tilde{a}}$ computed in the \emph{previous} training epoch.
%
As shown in the bottom-right corner of \figref{method}, we determine the correlation $y_{ij} \in \{0,1\}$ between the $i$-th image and the $j$-th audio within the same mini-batch according to the audio representations, \ie
\vspace{\eqmargin}
\begin{equation}
\begin{aligned}
\label{eq:a_th}
y_{i,j}=\left\{\begin{array}{ll}
1, & \text { if } \left<\mathbf{\Tilde{a}}_{i}, \mathbf{\Tilde{a}}_{j}\right> \geq \delta_{a}, \\
0, & \text { otherwise,}\\ %(\mathbf{\Tilde{a}}_{i}\cdot\mathbf{\Tilde{a}}_{j}) \geq \delta_{a}\\
\end{array}\right. 
\; \forall i, j \in \{1, \ldots,k\},
\end{aligned}
\vspace{\eqmargin}
\end{equation}
where the term $\delta_{a} \in [0,1]$ is a thresholding parameter. 
%
Combining the proposed intra-frame sampling and inter-frame relation strategies, our training objective becomes
\vspace{\eqmargin}
{\small
\begin{equation}
\label{eq:full}
\begin{aligned}
&\mathcal{L}^\mathrm{iterative}_\mathrm{contrast}=\\
&-\frac{1}{k}\sum_{i=1}^k\Big[\log\frac{\sum_{j=1}^{k}y_{i,j}\exp(\textbf{v}_i^+\cdot\mathbf{a}_j/\tau)}{\sum_{j=1}^{k}\exp(\textbf{v}_i^-\cdot\mathbf{a}_j/\tau)+\exp(\textbf{v}_i^+\cdot\mathbf{a}_j/\tau)}\Big].
\end{aligned}
\vspace{\eqmargin}
\end{equation}
}
%
We train our sound localization model using Eq.~\eqref{eq:nce_loss} at the initialization stage, and then iteratively optimize the objective in Eq.~\eqref{eq:full} until the localization results converge.

\input{table/stoa}
\input{figure/cIoU_curve}