\vspace{-4mm}
\section{Introduction}\label{sec:intro}
\vspace{\secmargin}
% --- introduce sound localization --- %
Multisensory signals (\eg vision, hearing, and touching) provide rich information for human beings to perceive the surrounding environments.
%
These cues from different modalities are usually closely related and thus enable human beings to perform complicated tasks in our daily lives.
%
Take vision and audio as an example, one can easily imagine a lightning scene upon hearing thunders, associate multiple objects with their sources on a noisy street, and identify and converse with friends in a crowded cocktail party. 
%
In this work, we target the \emph{sound localization} task~\cite{av_nips20_loc,av_eccv20_mms_loc, av_cvpr18_lls,av_tpami20_lls} that aims to identify the sounding region in the image, as the example shown in \figref{teaser}.
% 
Sound localization is an emerging research topic since it is the nexus of various audio-visual applications such as audio-visual source separation~\cite{av_cvpr20_sep-gesture,av_eccv18_sep,co_sep_iccv19,av_iccv19_mpnet,sofm_iccv19,pix,av_iclr21_AudioScope,av_cvpr21_co_learning,av_cvpr21_gao2021VisualVoice,av_cvpr20_PreviewAudio} and audio-visual event localization/parsing/recognition~\cite{av_eccv20_avvp,eccv18_avel,av_iccv19_DAM,AVSDN,av_cvpr21_av_parsing,my_accv20_av-trans,av_iclr21_lee2021crossattentional}.


% --- current solutions and their drawbacks ---
\input{figure/teaser}
%
Sound localization methods based on supervised learning entail a large amount of training data with the annotated sound-visual associations.
%
%\hyt{YB: check this sentence}
Although Senocak~\etal~\cite{av_cvpr18_lls,av_tpami20_lls} collect $5000$ audio-image pairs from the Flickr-Sound database~\cite{av_nips16_soundnet} with bounding box annotations of the sounding regions, the amount of labeled data is not sufficient to train a deep learning model in a fully-supervised fashion.
%In~\cite{av_cvpr18_lls,av_tpami20_lls}, Senocak~\etal~collect a sound localization dataset consisting of bounding-box annotations of sounding regions in which 5,000 images randomly selected from the Flickr-Sound database~\cite{av_nips16_soundnet} are annotated.
% 
%MH: this sentnece does not make sense. See whether I understand and revise it right or not #YB: Y, ref added
%
%Nevertheless, there are no abundant annotated images in the dataset (\ie only 5k annotated images) for the training deep-learning-based sound localization models. 
%
%Nevertheless, only a subset of images (about 5K) are annotated  from the large Flickr-Sound dataset. 
%
%These annotations provide the supervised guidance for models to learn from audio/visual information.
%Nevertheless, the annotated images with sounding regions are not sufficient (only 5k annotations). 
%
Moreover, it is challenging to scale up the efforts to collect a large labeled dataset since the annotators need to meticulously observe visual and audio signals simultaneously.
% and easily results in noisy data.

%MH: if you want to use self-supervised, why don't you use weakly-supervised (now you have self-supervied and weakly supervised).
%To tackle the issue of the limited labeled data, weakly-supervised methods based on audio-visual event labels have been developed~\cite{av_eccv20_mms_loc}.
%Furthermore, self-supervised approaches are designed to leverage the image-audio correspondences~\cite{av_iccv17_look,av_eccv18_obj_that_sound} or audio-visual clusters~\cite{av_cvpr19_deep_cluster,av_arxiv_curricumlum_av_clutser}.
Semi-supervised~\cite{av_cvpr18_lls,av_tpami20_lls}, weakly-supervised~\cite{av_eccv20_mms_loc}, and self-supervised learning frameworks~\cite{av_cvpr19_deep_cluster,av_arxiv_curricumlum_av_clutser,av_nips20_loc} are proposed to overcome the limited data issue.
%
The weakly-supervised methods~\cite{av_eccv20_mms_loc} require audio-visual event labels, and existing self-supervised methods rely on a pre-defined number of clusters~\cite{av_cvpr19_deep_cluster,av_arxiv_curricumlum_av_clutser} or require videos of single sounding sources~\cite{av_nips20_loc}.
\mycomment{
\yb{
Particularly, self-supervised approaches are designed to leverage audio-visual clusters~\cite{av_cvpr19_deep_cluster,av_arxiv_curricumlum_av_clutser}, or sounding object dictionaries~\cite{av_nips20_loc}.
%
However, such weakly-supervised and self-supervised frameworks rely on prior knowledge such as the pre-defined number of clusters~\cite{av_cvpr19_deep_cluster,av_arxiv_curricumlum_av_clutser}, audio-visual event labels~\cite{av_eccv20_mms_loc}, or single-source videos for building up sounding object dictionaries~\cite{av_nips20_loc}.
}
\hy{why mention 4,5 here? and what is the drawback of 18? and it suggests that we are not self-supervised}
\ybc{revised}
}
%
Furthermore, the semi-supervised methods~\cite{av_cvpr18_lls,av_tpami20_lls} using audio-visual correspondences alone as the supervision is less effective since a scene may contain non-sounding or ambient regions, which leads to the association between the incorrect sounding regions and reference audio signals.
%
These issues hamper the performance of sound localization in unconstrained scenarios where the numbers of sound sources are usually unknown and there may exist objects unseen during training.


% --- our approach ---
In this work, we propose an iterative contrastive representation learning algorithm that does not require any prior assumption or labels for the sound localization task.
%
Starting from conventional contrastive learning~\cite{av_cvpr18_lls,av_tpami20_lls}, we use the sound localization model obtained in the \emph{previous} epoch to estimate the sounding and non-sounding regions as the pseudo-labels for the current epoch.
%
With such pseudo regions, the model is encouraged to disassociate non-sounding or ambient regions from object sounds and thus explores more negative training samples for contrastive learning.
%
In addition to the relationships between the audio and visual signals within an instance, we correlate audio signals \emph{across} instances.
%
For instance, if the audio clips of two different instances are semantically similar, the image and audio \emph{across} the two instances should be positively correlated and can then serve as a positive pair for contrastive learning, and vice versa.
%
We show an example of two train sounds across instances in \emph{inter-frame relation} of \figref{teaser}.
%
Such a strategy alleviates typical contrastive learning methods from differentiating the representations of the related sounding object and audio signals across instances, and provides more reliable guidance to learn a sound localization model.
%\hy{hard to understand. and does not mention issue of typical contrastive learning }\ybc{revised}
%\yb{Specifically, the representations of different sounding objects should be similar if the corresponding audio instances are highly correlated, which preventing pulling sounding objects away from related audio signals with naive contrastive learning.
%It would be a reliable guidance to learn accurate sound localization by exploring relationship between audio instances.}
%

We evaluate the proposed method on the Flickr-Sound ~\cite{av_cvpr18_lls,av_tpami20_lls} and the MUSIC-Synthetic~\cite{av_nips20_loc} datasets using the consensus intersection over union (cIoU) and area under curve (AUC) as evaluation metrics.
%
Both qualitative and quantitative results demonstrate the effectiveness of the proposed method on the sound localization task.
%
The main contributions of this work are summarized as follows:
\begin{compactitem}
  \item We propose an iterative contrastive learning algorithm to tackle the sound localization task without any data annotations.
  %
  \item Our method not only leverages regions of interests, but also exploits non-sounding regions as well as the relationship across audio instances to jointly learn the audio and visual representations.
  %
  \item Qualitative and quantitative experimental results on the benchmark dataset demonstrate that the proposed method performs favorably against the state-of-the-art weakly supervised and unsupervised approaches. %\hyt{weekly supervised or semi supervised? u use semi in the previous paragraph} \ybc{weakly. fixed the term in previous paragraph}
\end{compactitem}

