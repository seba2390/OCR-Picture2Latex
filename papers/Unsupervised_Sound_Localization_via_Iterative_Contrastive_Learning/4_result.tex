
% \input{table/stoa.tex}

\input{figure/vis.tex}
\section{Experimental Results}
\label{sec:results}
\vspace{\secmargin}
%\subsection{Datasets}
%\label{sec:dataset}
\Paragraph{Datasets.} We evaluate all methods on two datasets:\begin{compactitem}[$\bullet$]
    \item\textbf{SoundNet-Flickr}~\cite{av_nips16_soundnet} dataset consists of more than two million video sequences.
% 
We use a $5$-second audio clip and the central frame of the $5$-seconds corresponding video clip, to form an input pair for the proposed framework.
% 
Note that we do not rely on any annotation (\eg bounding boxes) for model training.
%
In all experiments, we perform the training process with the subsets of the SoundNet-Flickr dataset constructed by Qian~\etal~\cite{av_eccv20_mms_loc} that contains $10$k and $20$k audio-visual pairs.
%
Following the protocol in~\cite{av_cvpr18_lls,av_eccv20_mms_loc,av_tpami20_lls}, we conduct the evaluation using the testing set of the SoundNet-Flickr dataset which consists of $250$ audio-visual pairs with bounding box annotations.
%
%As for quantitative evaluation, Senocak~\etal~\cite{av_cvpr18_lls,av_tpami20_lls} provide bounding boxes of sounding regions for the subset of SoundNet-Flickr, which contains 5k annotations.
% 
%In our experiment, we train with 10k pairs (no supervision) in a random sample subset\footnote{github.com/shvdiwnkozbw/Multi-Source-Sound-Localization} of SoundNet-Flickr, which are the \yb{exact} same pairs as~\cite{av_eccv20_mms_loc}.
% 
%To fairly compare with the methods, the evaluation is performed in test set of SoundNet-Flickr~\cite{av_cvpr18_lls,av_tpami20_lls} containing 250 audio-visual pairs with bounding box annotations. 
% 
%\yb{The aforementioned settings are following the ones in~\cite{av_cvpr18_lls,av_tpami20_lls,av_eccv20_mms_loc}}.
 \item{\textbf{MUSIC-Synthetic}}~\cite{av_nips20_loc} is a dataset consisting of synthetic audio-visual pairs.
 %
 Each audio-visual pair is constructed by concatenating four music instrument frames and randomly selecting two out of four corresponding 1-second audios. 
 %
 In other words, for each audio-visual pair, there are two instruments making sound while the other two are silent.
 %
 We follow the protocol~\cite{av_nips20_loc} to train the models with all $25$k audio-visual pairs in the training set and conduct the evaluation on the testing set consisting of $455$ audio-visual pairs with bounding box annotations.

\end{compactitem}


\Paragraph{Implementation Details.} We implement the proposed method using Pytorch~\cite{pytorch}, and conduct the training and evaluation processes on a single NVIDIA GTX 1080 Ti GPU with $11$ GB memory.
%
We use the ResNet-18~\cite{resnet} architecture for both the visual and audio feature extractors.
%
Following the strategy in~\cite{av_eccv20_mms_loc,av_nips20_loc}, the visual feature extractor pre-trained on the ImageNet~\cite{ImageNet} dataset is employed.
%
As for the audio data pre-processing, the raw 5-seconds audio clips are re-sampled at $22.05$ kHz for the SoundNet-Flicker dataset (1-sceond clip at $16$kHz for the MUSIC-Synthetic dataset), and transformed into the log-mel spectrograms~(LMS).
%
% For the MUSIC-Synthetic dataset, the raw 1-seconds audio clips are re-sampled at $16$ kHz and also transformed into LMS with window length of $160$ and a hop length of $80$.
%
Images are re-sized to the resolution of $256\times 256$ on SoundNet-Flicker and $224\times 224$ on MUSIC-Synthetic.
%
For fair comparisons, we adopt the same batch size of $96$ as in~\cite{av_eccv20_mms_loc,av_nips20_loc} for all the experiments. % since a larger batch size may benefit our contrastive learning approach.
%
More implementation details are in the Supplementary.
%
The code and models will be made publicly available.


% and min-max normalization on each instance:
% \begin{equation}
% \label{eq:normalization}
% \mathbf{R} = \frac{\mathbf{R} - \min(\mathbf{R})}{\max(\mathbf{R}) - \min(\mathbf{R})}.
% \end{equation}
%
% As shown in \secref{ablation}, the post-processing scheme improves the performance of both the competing methods and our method.
%\hy{move this to section 3}\ybc{moved and remove as shown.... 4.3}


\Paragraph{Evaluation Metrics.}
%\Paragraph{From Bounding Box to Score Map.}
Following previous work~\cite{av_cvpr19_deep_cluster,av_arxiv_curricumlum_av_clutser,av_eccv20_mms_loc,av_cvpr18_lls,av_tpami20_lls}, we adopt \textit{consensus intersection over union (cIoU)} and \textit{area under curve (AUC)} as the evaluation metrics.
%
Note that the ground-truth sounding region of an image is computed according to the overlapping of the bounding box labels annotated by different people.
%
The response map $\mathbf{R}$ in Eq.~\eqref{eq:normalization} is post-processed to serve as the sound localization results for evaluation.
%
Specifically, we first compute the response map $\mathbf{R}$ using Eq.~\eqref{eq:cos}.
%
Then we recover the resolution of the response map $\mathbf{R}$ from $w\times h$ to original image resolution $W \times H$ using bilinear up-sampling.
\begin{comment}
\yb{
Following~\cite{av_cvpr18_lls,av_tpami20_lls,av_eccv20_mms_loc,av_cvpr19_deep_cluster,av_arxiv_curricumlum_av_clutser}, 
the bounding box annotations of sounding regions in coordinate form are transformed into binary maps.
% 
The binary maps indicate whether each pixel is sounding or not.
% 
The pixels are labels as scores from 0 to 1, which is based on the overlapped of human annotations.
% 
Given ground truth score maps and predicted results, 
\textbf{consensus intersection over union (cIoU)} and \textbf{area under curve (AUC)} are performed for measuring the quality of predicted sound localization results.
}
\end{comment}

\input{figure/vis_abs.tex}

\Paragraph{Competing methods.}
We compare the proposed method to the following weakly- and unsupervised approaches:
%\yb{Our method trained with the same audio-visual pairs is compared with the following weakly-supervised or unsupervised approaches}:
%\ybc{revised}
% \hyt{do not introduce how these methods work, focus on 1) what/how manay training pairs used 2) do they use any assumption or event label}
%To evaluate the quality of our predicted sound localization, we compare our method (10K audio-visual pairs) with the following state-of-the-arts methods in a weakly supervised or unsupervised manner:
\begin{compactitem}
\item\textbf{Attention}~\cite{av_cvpr18_lls,av_tpami20_lls} is trained using the audio-visual co-attention mechanism.

\item \textbf{DMC}~\cite{av_cvpr19_deep_cluster} is an unsupervised approach based on the usage of audio-visual clusters, and  requires \emph{a pre-defined number} of sound sources. We set the number of source to one suggested by ~\cite{av_cvpr19_deep_cluster,av_arxiv_curricumlum_av_clutser} for the SoundNet-Flick dataset and set to two for the MUSIC-Synthetic dataset.

% \item \textbf{CAVL AudioSet}~\cite{av_arxiv_curricumlum_av_clutser} is an unsupervised approach that uses the same training dataset as the DMC approaches. Similarly, it requires {\em a pre-determined number} of sound sources.
%The unsupervised audio-visual cluster-based method with a curriculum learning strategy to handle multiple sounding sources. The training and testing scenario are the same as DMC~\cite{av_cvpr19_deep_cluster}, which also requires a pre-determined number of sound sources.
\item \textbf{MSSL}~\cite{av_eccv20_mms_loc} reports the state-of-the-art performance on the sound localization task. It requires audio/visual event labels obtained from pre-trained classifiers and the CAM~\cite{cvpr16_CAM} predictions to find the sounding regions. %\yb{The event audio/visual labels come from pre-trained classifiers and associated with related to events of datasets.}
%As a state-of-the-art method for sound localization, it utilizes audio/visual event labels and CAM~\cite{cvpr16_CAM} to find sounding regions in a weakly supervised manner with 10K/20K training samples.

\item \textbf{DSOL}~\cite{av_nips20_loc} is a two-stage approach requiring a large amount of single-source videos for the first stage to build up class-based visual dictionaries and train audio and visual encoders. %
For a fair comparison, we only train the network in the second stage.
We use pre-trained audio and visual encoders and use CAM~\cite{cvpr16_CAM} predictions to replace visual dictionaries.


\end{compactitem}




\subsection{Quantitative Results}
\vspace{-3.2mm}
% \vspace{\subsecmargin}
% 
\tabref{stoa} shows the quantitative comparisons on the SoundNet-Flickr and MUSIC-Synthetic datasets.
% 
The proposed method performs favorably against the competing approaches on the sound localization task.
%
We note that different from the proposed method, the competing schemes require a pre-defined number of sounding sources~(\ie DMC) or audio/visual event labels~(\ie MSSL).
%
In contrast, the proposed method does not need any prior knowledge about the source number or data annotations.
%
Furthermore, our model trained with $10$k audio-visual pairs already outperforms MSSL and DSOL approaches which use more (\ie $20$k) audio-visual pairs during training.
%
In addition to the cIoU metric, the cIoU scores calculated with various thresholds are shown in \figref{curve}.
%
Our method reports favorable cIoU scores under all thresholds.
%
The consistent performance advantage suggests the effectiveness and efficacy of our iterative contrastive learning algorithm.


\input{table/ablation}



\vspace{-0.2mm}
\subsection{Qualitative Evaluation}
\vspace{\subsecmargin}
We demonstrate the qualitative comparisons in \figref{vis}.
%The visualization results are shown in Fig.~\ref{fig:vis} on image-sound pairs selected from the SoundNet-Flicker dataset. 
% 
The localization results of the proposed method are more accurate compared to those of the competing approaches.
%From this figure, we see that our method presents more accurate sound localization results.
% 
The example in the 3rd and 4th row is particularly challenging.
%
Since the multiple-sounding and non-sounding instruments appear in the same scene, it is difficult to localize exact-sounding objects.
MSSL and DSOL are both struggling with unrelated background. 
%Since trains usually run on the rail, models for unsupervised sound localization would jointly associate rails and trains with train sounds. 
% 
As for DMC, with the prior defined number of sounding source for the MUSIC-Synthetic dataset, it is more resistant to the unrelated background yet fail to identify the sounding instruments correctly.
%
%it can reduce influences from the noise background.
% 
Compared to these methods, the proposed framework can focus on the sounding objects with better accuracy, while trained without audio-visual event labels or any prior information.

%Our results also focus on the sounding objects only even though the proposed method is trained without the requirement of audio-visual event labels.
%It is worth noting that, no prior assumption is available our during model training. Instead, DMC, CAVL and MSSL require pre-defined number of sound source or audio/visual event labels. 
% 
%These observations validate the effectiveness of the proposed training strategy for the sound localization task.
%Thus, these results support the use of our training strategy for discovering the sounding objects in a scene.




\vspace{-0.2mm}
\subsection{Ablation Study}
\vspace{\subsecmargin}
\label{sec:ablation}
%\Paragraph{Localization and Normalization.}
%We adopt the official implementation of 

We conduct the ablation study to analyze the individual impact of each design component in the proposed method.
%
The results are presented in the fourth block of \tabref{ablation}, where \textbf{Itr} indicates the iterative contrastive training that uses the pseudo-sounding regions inferred from the previous epoch, \textbf{Intra} represents the usage of the pseudo-non-sounding regions, and \textbf{Inter} is the proposed inter-frame relation module.
%
We also demonstrate the qualitative comparisons in \figref{vis_abs}.
%
Particularly, the iterative strategy (\ie \textbf{Itr}) ensures the localization model focus only on the sounding region compared to the conventional contrastive learning approach (\ie \textbf{Initial}).
%
Both the quantitative and qualitative results confirm the efficacy of individual components designed in our approach.

\Paragraph{Comparison with MSSL.}
The proposed method shares similar backbone with the MSSL~\cite{av_eccv20_mms_loc} method.
%
%We adopt the official source code of the MSSL~\cite{av_eccv20_mms_loc} method to implement our method.
%
Therefore, we also conduct the ablation study to show the impact of each modification we made, including replacing CAM with thresholding for sounding region  localization (Eq.~\eqref{eq:cos}), normalization (Eq.~\eqref{eq:normalization}), and conventional contrastive learaning (Eq.~\eqref{eq:nce_loss}).
%
The results are summarized in the first three blocks of \tabref{ablation}.
%
Since the MSSL method uses a two-stage model trained with audio-visual event labels, we study the case of removing the second stage (\textbf{Stage I}) and training without labels (\textbf{w/o Labels}).
%
As the results shown in the first block, training with the first stage and without labels both significantly degrade the performance of the MSSL method.
%
We show in the second and third block that using Eq.~\eqref{eq:cos} and Eq.~\eqref{eq:normalization} can greatly improve the performance.
%
Finally, we obtain our baseline (\textbf{Initial}) by applying Eq.~\eqref{eq:nce_loss} to the MSSL \textbf{Stage I} \textbf{w/o Labels} method with Eq.~\eqref{eq:cos} and Eq.~\eqref{eq:normalization}.
%
To conclude, \tabref{ablation} summarizes the impact of the proposed components and the transition from the original MSSL method to the proposed approach.
%
\mycomment{
Second, different from our methods that uses Eq.~\eqref{eq:cos} to identify the sounding regions, the MSSL method estimates the sounding regions using CAM.
%
Therefore, we report the performance of the MSSL model that uses Eq.~\eqref{eq:cos} in the second block to localize the sound (\ie \textbf{w/ \eqref{eq:cos}}).
%
The results indicate the effectiveness of localization using Eq.~\eqref{eq:cos} \yb{for the first stage}.
%
Finally, we observe that applying the min-max normalization described in Eq.~\eqref{eq:normalization} improves the performance \yb{when Eq.~\eqref{eq:cos} is utilized to perform localization}, as shown in the results reported in the third block (\ie \textbf{w/ \eqref{eq:normalization}}).
%
\yb{With these modification, we train our initial model with Eq.~\eqref{eq:nce_loss}, whose performance has been improved compared with MSSL.}
%
To conclude, \tabref{ablation} summarizes the impact of the proposed components and the modifications we develop for sound localization.
%
The results not only supports the transition from the MSSL to our approach, but also validate the effectiveness of the proposed iterative contrastive learning framework.}
%Since we adopt state-of-the-art model~\cite{av_eccv20_mms_loc} with different localization and visualization technique to localize sound, the investigation of these techniques are desirable.
% 
%The brief summarization of MSSL~\cite{av_eccv20_mms_loc} is illustrated as follows:
% 
%MSSL~\cite{av_eccv20_mms_loc} a two-stage sound localization model trained with extra audio-visual event labels.
% 
%The event classifiers and audio-visual correspondence are utilized during the first stage.
% 
%The additional label-guide contrastive learning is applied at the second stage.
% 
%When the model can understanding vision and audio signals, the sound localization results are achieved by CAM with normalizing the maximum value to one. 
% 
%Note that such details in our model are described in~\eqnref{cos} and~\eqnref{normalization}.
%
%In \tabref{ablation}, the top block indicates the results with original MSSL setting. 
% 
%We can see that the performance is degraded significantly since CAM requires labels to find discriminative regions.
% 
%As for the second top block, audio-visual correlation~\eqnref{cos} is utilized to find sounding regions instead of CAM.
% 
%The first stage of MSSL with~\eqnref{cos} can achieve favorable performance compared with the methods, whose localization results are calculated by CAM.
% 
%For the third block, we further apply minmax normalization mechanism~\eqnref{normalization}, where the results within an instance are normalized to values from zero to one. 
% 
%The minmax normalization~\eqnref{normalization} can strengthen the localization results in cIoU metric at 0.5 threshold, which indicates the improvement by~\eqnref{normalization} only at certain threshold.
% 
%For the bottom block, we compares and verifies the contribution of every components in our method, where Itr, Intra, and Inter denote iterative contrastive learning, intra-frame sampling, and inter-frame relation respectively described in~\secref{av-learning}. 
% 
%Check mark indicates methods which are utilized.
% 
%Note that the initial contrastive learning~\eqnref{nce_loss} is able to achieve favorable performance against MSSL. 
% 

%Furthermore, the localization results with each module in our model are presented in~\figref{vis_abs}. 
% 
%We can see that our training strategy is able to effectively alleviate the noise of non-sounding regions. 
% 
%It is clear that iterative contrastive learning can provide more accurate results than initial one, and the results from our proposed method are correctly localized on sounding regions compared with only iterative contrastive learning (without intra-frame sampling and inter-frame relation).
% 
%Note that intra-frame sampling and inter-frame relation are based on iterative contrastive learning.



\input{figure/vis_itr}
\input{figure/vis_inter}



\Paragraph{Localization results in various epochs.}
Since the proposed iterative method is based on the strategy where the localization results predicted in the previous training epoch serve as the pseudo-label, the iterative localization results are crucial.
%
Therefore, we visualize the localization results at different epochs. 
%Since our model is based on the results from the model at the previous epoch as pseudo-annotation for current model learning, the localization results at different epoch play a critical factor of performance.
% 
%To this end, we investigate the localization results at different epochs in~\figref{vis_itr}.
% 
As shown in~\figref{vis_itr}, the localization results gradually focus on the sounding regions.
The results validate the efficacy of the proposed iterative procedure that takes localization results from the previous epoch as training guidance for the current epoch.
%our motivation that the localization results in the previous epoch provide meaningful guidance for the following training epochs.



\Paragraph{Relationships in audio modality.}
The proposed inter-frame relation illustrated in \secref{av-learning} is based on the assumption that the relationships in the audio modality can be the guidance of the contrasting learning.
%
To verify the assumption, we visualize the retrieval results in the audio modality in \figref{vis_inter}.
%
Specifically, given a reference audio-visual pair, we retrieve the top three audio-visual pairs according to the distances between audio features.
%
We present the images of the reference and retrieved visual-audio pairs in~\figref{vis_inter}.
%we investigate the related pair according to the distance between the corresponding audio features.
%
%In~\figref{vis_inter}, we show the image of the reference audio and the images corresponding to the top three audio clips that are most related to the reference one.
%
As the reference and retrieved images share semantically similar contents, we validate the intuition behind the proposed inter-frame relation design.
%As discussed earlier, since inter-frame relation is based on the correlation between audio signals within a batch, the visualization can explain the justification of the model design.
% 
%That is, given an audio-visual pair, we would like to explore the related sounding objects by audio signals.
% 
%In~\figref{vis_inter}, the first and fifth column with reference caption is the anchor images. The corresponding audio signals are utilized to measure the correlation of all testing data.
% 
%We find the top three similar sound and present the corresponding images in the following columns.
% 
%Our assumption that sounding objects are related if their sounds are similar is verified with the cases in~\figref{vis_inter}, which alleviates typical contrastive learning methods from differentiating the sounding object and related audio signals.
% 











