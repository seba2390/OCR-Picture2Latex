
% --------------------------------------------------------------------------
\documentclass{article}
\usepackage{spconf}
\input{./preamble}

\newif\ifproof
\prooffalse

\title {A Coupled Compressive Sensing Scheme for Unsourced Multiple Access}
%
% Single address.
% ---------------
\name{\begin{tabular}{c}Vamsi K. Amalladinne,
Avinash Vem,
Dileep Kumar Soma, \\
	Krishna R. Narayanan,
Jean-Francois Chamberland \end{tabular}
\thanks{
This material is based upon work supported by the National Science Foundation (NSF) under Grant No.~CCF-1619085.}
}
\address{Department of Electrical and Computer Engineering, Texas A\&M University}
%
% For example:
% ------------
%\address{School\\
%	Department\\
%	Address}
%
% Two addresses (uncomment and modify for two-address case).
% ----------------------------------------------------------
%\twoauthors
%  {A. Author-one, B. Author-two\sthanks{Thanks to XYZ agency for funding.}}
%	{School A-B\\
%	Department A-B\\
%	Address A-B}
%  {C. Author-three, D. Author-four\sthanks{The fourth author performed the work
%	while at ...}}
%	{School C-D\\
%	Department C-D\\
%	Address C-D}
%
\begin{document}

\maketitle

\begin{abstract}
This article introduces a novel paradigm for the unsourced multiple-access communication problem.
This divide-and-conquer approach leverages recent advances in compressive sensing and forward error correction to produce a computationally efficient algorithm.
Within the proposed framework, every active device first partitions its data into several sub-blocks, and subsequently adds redundancy using a systematic linear block code.
Compressive sensing techniques are then employed to recover sub-blocks, and the original messages are obtained by connecting pieces together using a low-complexity tree-based algorithm.
Numerical results suggest that the proposed scheme outperforms other existing practical coding schemes.
Measured performance lies approximately $4.3$~dB away from the Polyanskiy achievability limit, which is obtained in the absence of complexity constraints.
\end{abstract}

\begin{keywords}
Communication, forward error correction, unsourced multiple-access, compressive sensing.
\end{keywords}

\section{Introduction}
\label{sec:intro}

Unsourced multiple access communication (MAC), initially proposed by Polyanskiy~\cite{polyanskiy2017perspective}, is a novel formulation for concurrent uplink data transfers.
It is closely related to uncoordinated multiple access~\cite{paolini2015coded,chen2017capacity}.
In this new paradigm, a system contains a total of $\Ktot$ users; out of this group, $\Ka$ users each wish to transmit a $B$-bit message to the access point at any given time.
The access point is tasked with recovering only the set of messages being transmitted, without regard for the identities of the corresponding sources.
The total number of users $\Ktot$ can be very large, whereas parameters $\Ka$ and $B$ are envisioned to remain small, typically in the hundreds.

For the regime of interest, with its characteristic short message lengths, non-asymptotic information-theoretic results apply.
Along these lines, Polyanskiy~\cite{polyanskiy2017perspective} derives finite block-length, achievability bounds for the unsourced MAC.
The findings reported therein are based on random Gaussian codebooks, and they assume that information is recovered using a maximum likelihood decoder.
Algorithmically, this scheme can be very computationally demanding.
In~\cite{ordentlich2017low}, Ordentlich and Polyanskiy report that many existing multiple access strategies perform poorly in this context, especially when $\Ka$ exceeds $100$.
They also propose the first low-complexity coding scheme tailored to this setting.
In their scheme, a transmission period is divided into sub-blocks, or slots, and the system operates in a synchronous fashion.
That is, all the users are aware of slot boundaries.
Within this framework, every active user transmits a codeword during a randomly chosen slot.
A data block is formed with a concatenated code that is designed for a $T$-user real-addition Gaussian multiple access channel ($T$-GMAC); typical values for $T$ range from 2 to 5.
Although this proposed scheme performs significantly better than existing MAC protocols, there remains an important gap of approximately $20$~dB between its performance and the achievability limit associated with the unsourced MAC~\cite{polyanskiy2017perspective}.
In related work~\cite{vem2017user}, we introduce a low-complexity coding scheme that relies on a similar slotted structure.
Our previous framework consists of an improved, close-to-optimal coding strategy for the $T$-GMAC; coupled to the application of successive interference cancellation across slots to reduce the performance degradation caused by overcrowded slots.
The combination of these two features constitutes a significant improvement over previous results~\cite{ordentlich2017low}, with a performance curve that lies only approximately $6$~dB away from the above mentioned achievability limit.

Both schemes discussed above adopt a channel coding viewpoint wherein the $\Ka$-user GMAC is reduced to multiple smaller $T$-GMAC channel problems.
Contrastingly, in this paper, we develop an alternate compressive sensing (CS) view of the problem.
To begin, we emphasize that a naive CS solution to the unsourced MAC entails resolving a $2^B$-length linear problem with a $\Ka$-sparse solution.
This implies sensing matrices with $2^{100}$ columns, which renders the problem intractable.
A key idea in establishing a pragmatic scheme is to divide the information blocks of the users into smaller sub-blocks such that each sub-block is amenable to a CS recovery.
Before transmission, redundancy is added to individual sub-blocks using a systematic linear block code.
The collection of sub-blocks transmitted within a slot are recovered using a CS algorithm.
Once this is achieved, individual segments of the original messages need to be pieced together.
This is accomplished via a low-complexity tree-based algorithm.
The overall structure of this communication architecture yields better performance compared to other existing algorithms with comparable computational complexity.
% One key advantage with this approach is the lack of the need for slot synchronization

Throughout, we employ $\mathbb{R}_{+}, \mathbb{Z}_{+}$, and $\mathbb{N}$ to denote the non-negative real numbers, non-negative integers, and natural numbers, respectively.
For any $a,b \in  \mathbb{Z}_{+}$ with $a \le b$, we use $[a:b]$ to denote $\{c \in \mathbb{Z}_{+}: a \le c \le b \}$.
We write $X \sim B(n,p)$ if a random variable $X$ possesses a binomial distribution with parameters $n$ and $p$.
We employ $|A|$ for the cardinality of set $A$, and we use $[x]$ to denotes the closest integer to $x$.


\section{SYSTEM MODEL}
\label{sec:systemmodel}

Let $\mathbf{S}_\mathrm{tot}$ represent the collection of devices within a network, and let $\mathbf{S}_\mathrm{a}$ denote the subset of active devices within a communication round, $\mathbf{S}_\mathrm{a} \subset \mathbf{S}_\mathrm{tot}$.
Then, we have $|\mathbf{S}_\mathrm{tot}| = \Ktot$ and $|\mathbf{S}_\mathrm{a}| = \Ka$.
Every active device wishes to communicate $B$ bits of information to a base station through an uncoordinated uplink transmission scheme.
The number of channel uses dedicated to this process is $N$, and $\mathit{W} = \left\{ \vec{\mathit{w}}_k:k \in \mathbf{S}_\mathrm{a} \right\}$ represent the collection of $B$ bit message vectors associated with these active devices.
We assume that devices pick their message vectors independently and uniformly at random from the set of binary sequences $\{0, 1\}^B$.
% $\mathbb{F}^{B}_2$.
% TECHNICALLY, WE ARE NOT IN GF2.

The base station facilitates a slotted structure for multiple access on the uplink through coarse synchronization.
As such, the signal available at the receiver assumes the form
\[
\textstyle
\yv= \sum_{k \in \mathbf{S}_\mathrm{a}} \xv_k + \zv,
\]
where $\xv_k$ is the $N$-dimensional vector sent by device~$k$ and $\zv$ represents additive white Gaussian noise.
The signal sent by every device is power constrained, i.e., $\| \xv_k \|_2^2 \le NP$ for $k \in \mathbf{S}_\mathrm{a}$, a scenario akin to \cite{polyanskiy2017perspective}.
The energy-per-bit is then given by $\frac{E_b}{N_0} \triangleq \frac{NP}{2B}$. The receiver produces an estimate $\widehat{\mathit{W}}(\yv)$ for the list of transmitted binary vectors $\mathit{W}$ with $| \widehat{\mathit{W}}(\yv) | \le K_\text{a}$.
The per-user error probability of the system is defined by
\begin{equation}
P_\mathrm{e}
= \textstyle
\frac{1}{K_\text{a}}\sum_{k \in \mathbf{S}_\mathrm{a}}
\Pr \left( \vec{\mathit{w}}_k \notin \widehat{\mathit{W}}(\yv) \right) .
\end{equation}
We propose an encoding and decoding scheme that achieves $P_\mathrm{e} \le \varepsilon$, where $\varepsilon$ is the target error probability with manageable computational complexity.
% WE COULD EXPLAIN RELATIONSHIP TO CS BETTER


\section{PROPOSED SCHEME}
\label{sec:proposedscheme}
%The key idea to limit complexity consists in dividing the data stream generated by active devices into several sub-blocks.
%These sub-blocks, with their very short packet fragments, are then amenable to computationally efficient compressed sensing (CS) algorithms.
%This process yields a list of likely fragments for every sub-block.
%The original messages are then recovered by piecing together compatible fragments.
%Technically, this latter step is accomplished by preemptively adding redundancy to the codewords $\{ \vec{x}_k \}$, and then leveraging this redundancy while combining sub-blocks via a low-complexity tree-based algorithm.

%The specifics of the coding scheme are detailed below.
%
%\subsection{Encoder}
%\label{encoder}
The transmission strategy outlined above features two parts: a systematic linear block code based on random parity checks, which we refer to as the tree encoder, and a CS encoder.
A notional diagram of the proposed system appears in Fig.~\ref{fig:1}.

\begin{figure}[!ht]
%\hspace*{-6.4cm}
\centering
%\includegraphics[angle=-90, clip, trim={7.5cm 2cm 7cm 2cm},width=0.95\textwidth]{../../Figures/Schematic.pdf}
\resizebox{\columnwidth}{!}{\input{tree_system_diagram}}
\caption{This is a schematic of the proposed scheme.
Original messages are split into sub-blocks, and redundancy is added to individual components.
Transmitted sub-signals are then determined via a CS matrix, and sent over a MAC channel.
A CS decoder recovers lists of sub-blocks, and a tree decoder reconstructs the orginal messages.}
\label{fig:1}
\end{figure}

%\subsection{Encoder}
%\label{encoder}
%The tree encoder adds the redundancy required to identify and combine the sub-blocks corresponding to a parent message from a pool of candidate fragments; whereas the CS encoder transforms the sub-blocks output by the tree encoder into signals suitable for noisy compressive sensing.

%\subsubsection{Tree Encoder Based on Random Parity Checks}
%\label{subsec:TreeEnc}
\textbf{\textit{Tree Encoder:}}
Every $B$-bit binary message vector $\vec{\mathit{w}}$ is encoded into $M$ bits using a systematic linear block code, which has random parity check constraints.  Algorithmically, a message vector is partitioned into $n$ sub-blocks, with the $i^{\text{th}}$ sub-block consisting of $m_i$ message bits, $\sum_{i=0}^{n-1}m_i=B$.
The tree encoder appends $l_i$ parity bits to sub-block~$i$, except for the first block as we choose $l_0 = 0$.
All the coded sub-blocks have the same length, i.e., $m_i+l_i =J\defeq  {M}/{n}$. %,~\forall~i \in [0:n-1]$.
The parity check bits in each sub-block are constructed as follows.
Let $\big( p^{(i)}_{0},p^{(i)}_{1},\dots,p^{(i)}_{l_{i}-1} \big)$ denote the parity bits in sub-block $i$.
These bits are selected to satisfy random parity check constraints for all the message bits preceding their respective sub-block.
To this end, we concatenate the message bits of all the sub-blocks $k \in [0:i]$ and index them with the set $\big[ 0:\sum_{k=0}^{i}m_k-1 \big]$. We then choose $l_{i}$ subsets $\mathcal{A}^{(i)}_j \subseteq \big[ 0:\sum_{k=0}^{i}m_k-1 \big]~\forall ~j \in [0:l_i-1]$ uniformly at random without replacement.
Parity check $p^{(i)}_j$ is chosen as the modulo-2 sum of all the message bits indexed by the set $\mathcal{A}^{(i)}_j$.
In effect, $p^{(i)}_j$ acts as a parity check constraint for some randomly chosen message bits preceding it.
In Section~\ref{parityvector}, we describe an optimization framework for the choice of parity length vector $\vec{l}=(l_0=0,l_1.\dots,l_{n-1})$.

\begin{comment}
\subsubsection{Random Interleaver}
\label{RandIinter}
The $M$ bit binary vector output by the tree encoder for each user is interleaved using a random permutation matrix $\mathbf{\Pi}$ which is common across all the users and known at the receiver. This is done to enforce randomness at sub-block level. Let $\vec{u}_{k}^{(i)}$ denote the $J$ length binary vector that corresponds to the $i^{\text{th}}$ sub-block of the active user $k$ output by the random interleaver. Then, it is reasonable to assume that the set of vectors $\{\vec{u}_{k}^{(i)}: k \in \mathbf{S}_\textbf{a}\}$ behave like vectors selected uniformly at random from the space $\mathbb{F}_2^J$. 
\end{comment}

%\subsubsection{CS Encoder}
%\label{CompSensEnc}
\textbf{\textit{CS Encoder:}} Let $\mathbf{A}=[\av_1,\ldots,\av_{2^J}] \in \{\pm \sqrt{P}\}^{\tilde{N}\times2^J}$, where $\tilde{N}\defeq {N}/{n}$, denote a compressed sensing matrix that is designed to recover any $\Ka$-sparse binary vector in the presence of noise with a low probability of error.  The $J$ bits in a sub-block are encoded  using a bijective function $f:\{0,1\}^J \rightarrow \{\av_j, j \in [1:2^J]\},$ which maps each sub-block to a column in $\mathbf{A}$.
That is, a column of $\mathbf{A}$ is a potentially transmitted sub-block.

%if $\vec{\tilde{\mathit{w}}}_{k} = [\vec{\tilde{\mathit{w}}}_{0}^{(k)} \vec{\tilde{\mathit{w}}}_{1}^{(k)} \dots \vec{\tilde{\mathit{w}}}_{n-1}^{(k)}]$ denotes the binary sequence corresponding to the user $k$ output by the tree encoder, where $\vec{\tilde{\mathit{w}}}_{i}^{(k)}$ denotes the $i^{\text{th}}$ tree encoded sub-block. Then, for each sub-block $\vec{\tilde{\mathit{w}}}_{i}^{(k)}$, the user transmits a column $\av_j$ from the sensing matrix $\mathbf{A}$.
% Thus, one column of $\mathbf{A}$ is transmitted for each sub-block corresponding to a user.

%$\{\vec{a}_j:j \in [0:2^J-1]\}$,
% Let $\av_j$ denote the $j^{\text{th}}$ column of $\mathbf{A}$.
%The CS encoder maps a sub-block of length $J$ bits corresponding to an active user into a vector of length $\tilde{N}$ via the sensing matrix $\mathbf{A}$.

\subsection{Decoder}
\label{decoder}
The decoding scheme consists of two components: the CS decoder operating in each sub-block, and a tree decoder operating across sub-blocks.
% The CS decoder implements a compressed sensing algorithm at the sub-block level, to identify the list of sub-blocks transmitted by the active users. The tree decoder tries to group all the sub-blocks that correspond to a particular user together, by harnessing the redundancy employed during the encoding process. Finally, the results from the tree decoder can be fed back into the CS decoder to improve the estimate of the transmitted list. By doing this, we remove the contribution of the sub-blocks authenticated by the tree decoder from the received signal and this improves the estimate provided by the CS decoder in the previous iteration. This iterative decoding process can be continued till the gains due to iterations become insignificant or stopped at any stage based on the complexity requirements of the system. We describe the two components of the proposed decoder below.

%\subsubsection{CS Decoder}
%\label{csdec}
\textbf{\textit{CS Decoder:}} The signal received during the $i^{\text{th}}$ sub-block can be expressed as $\yv_i = \mathbf{A}\bv_i + \zv_i$,
%\begin{align*}
%\yv_i = \mathbf{A}\bv_i + \zv_i,
%\end{align*}
where $\bv_i \in \{0,1\}^{2^J}$ is a $\Ka$-sparse binary vector that indicates the list of $i^{\text{th}}$ sub-blocks transmitted by the active users.
The task of the CS decoder is to provide an estimate of the sparse vector $\bv_i$ from the received signal $\yv_i$ during the corresponding time slot.
% This problem is similar to the support recovery problem in standard compressed sensing literature because of the fact that the non-zero entries of $\vec{b}_i$ are all 1's.
This is accomplished by first applying a non-negative least squares (NNLS) algorithm to get an estimate $\vec{b}^{(\text{nnls})}_i$ of vector $\vec{b}_i$.
Yet, this does not ensure that the entries of vector $\vec{b}^{(\text{nnls})}_i$ are binary.
The desired binary estimate $\hat{\vec{b}}_i$ is obtained by setting the $K$ largest entries of the vector $\bv^{(\text{nnls})}_i$ to one and the remaining $2^J-K$ entries to zero.
The number $K$ is chosen as $K=\Ka+K_{\delta}$, where $K_{\delta}$ is a small positive integer.
Although the list output by the CS decoder is larger than $\Ka$, the quantity $K_{\delta}$ is carefully chosen such that the erroneously decoded sub-blocks are very unlikely to satisfy the parity check constraints associated with encoding process.

\begin{comment}
\subsubsection{De-interleaver}
\label{deinter}
\end{comment}
%\subsubsection{Tree Decoder}
%\label{Treedec}
\textbf{\textit{Tree Decoder:}} The tree decoder seeks to recover the original messages transmitted by all the users by piecing together valid sequences of elements drawn from the various CS lists.
Towards this end, the access point constructs a decoding tree for \textit{each candidate message} as follows.
We fix a sub-block from the list of all possible first sub-blocks supplied by the CS decoder as the root node for a tree.
Once the first sub-block is determined, there are $K$ possible choices for the second sub-block, and these are the nodes which appear in the first stage of the tree.
Similarly, there are $K$ possible choices for the third sub-block for each choice of the second sub-block and, hence, $K^2$ nodes in the second stage.
This process continues until the $(n-1)^{\text{th}}$ stage is reached; at this point, the tree has $K^{n-1}$ leafs.
Every path connecting the root node to a leaf becomes a possible message.
If there exist a single valid path at the end, the decoder outputs the corresponding message; otherwise, it reports a failure.
%  a valid path  among these with reasonable complexity and tolerable probability of error.

The number of possible paths increases exponentially with the stages of the tree and, hence, a naive search through all the leaf nodes is infeasible.
In practice, invalid paths are pruned iteratively through the parity check constraints.
Specifically, at stage $i \ge 1$, the decoder retains only nodes that satisfy the $l_i$ bit parity constraints on all the message bits preceding that stage.
This iterative procedure continues until the $(n-1)^{\text{th}}$ stage is reached.
The complexity of this decoding scheme depends on the number of nodes surviving each stage, since parity checks have to be enforced only on the children of surviving nodes in the subsequence stages of the tree decoding process.
%A comprehensive analysis of the probability of decoding failure and the decoding complexity is given in the following section.

\begin{remark}[Iterative Extension]
\label{rmk:iterations}
The successful outputs from the tree decoder can be subtracted off from their respective received signals in each sub-block.
This extra step can potentially improve the estimate provided by the CS decoder compared to the previous iteration.
This successive interference cancellation method can be repeated iteratively, leading to significant potential gains in performance, particularly for the first few steps.
% or stopped at any stage based on the complexity requirements of the system.
\end{remark}
% and if only one path survives, the decoder outputs that path as the valid  transmitted string. Decoder declares a failure if more than one path survives the tree decoding process.

\section{Performance Analysis}%{of the Proposed Scheme}
\label{perr}
Suppose that the list output by the CS decoder contains the sub-block $i$ transmitted by user $k$ with a probability $1-p_{cs}$ and, with a probability $p_{cs}$, this block is erroneously replaced by a vector chosen uniformly at random from the set $\{0,1\}^{J}$.
Let $E_k$ denote the event that the transmitted binary message form user~$k$ is not present on the list output by the tree decoder.
Similarly, let $C_k$ be the event that all the sub-blocks corresponding to this user are present on the lists output by the CS decoder.
Probability $P(E_k)$ can be computed as,
\begin{align}
P(E_k) = P(E_k\lvert C_k)P(C_k) + P(E_k\lvert \overline{C_k})P(\overline{C_k}).  \label{Totalprob}
\end{align}
If the CS decoder fails to decode at least one of the sub-blocks that correspond to a user, then the output of the tree decoder would not contain the original message transmitted by that user.
Thus, we have $P(E_k \lvert \overline{C_k})=1$.
The quantity $P(C_k)$ can be computed as $P(C_k)=(1-p_{cs})^n$.
We denote the event that the tree decoder declares a failure because of more than one path surviving the tree decoding process by $E_k\lvert C_k$.
We write the probability of this event as $p_{\mathrm{tree}}$.
When there are no iterations involved in the decoding process, the quantity $P_e$ is the same as $P(E_k)$; they can be computed using \eqref{Totalprob} and the above observations as $P_e = 1-(1-p_{\mathrm{tree}})(1-p_{cs})^n$.
% In the following sub-sections, we provide a closed form expression for computing the quantity $p_{\text{tree}}$ and also analyze the decoding complexity of this system. Further, we provide an optimization framework for the choice of parity lengths.

%\begin{align*}
%P_e = 1-(1-p_{\text{tree}})(1-p_{cs})^n.
%\end{align*}

%\subsection{Computation of $p_{\text{tree}}$}
Let $L_i$ denote the random variable corresponding to the number of erroneous paths that survive stage $i \in[1:n-1]$ of the tree decoding process.
The following results hold.
\begin{lemma} \label{Lemma1}
Expected values for $L_i$ are given by
\begin{align} \label{expLi}
\mathbf{E}[L_{i}] &= \sum_{m=1}^{i} \left( K^{i-m}(K-1)
\textstyle \prod_{j=m}^{i}p_j \right)
\end{align}
where $p_i ={1}/{2^{l_i}}$, $q_i= 1-p_i$, and $i \in [1:n-1]$.
\end{lemma}
\begin{lemma} \label{Lemma2}
The probability of error for the tree decoder $p_{\mathrm{tree}}$ is given by
$p_{\mathrm{tree}} = 1-G_{L_{n-1}}(0)$ where
\begin{align}
G_{L_{n-1}}(z) &= \textstyle \prod_{i=0}^{n-2} f_{n-1-i}^{K-1}(z) \notag \\
f_k(z) &= \begin{cases}
    q_k + p_k f_{k+1}^{K}(z),& ~1 \le k \le n-1 \\
    z^{\frac{1}{K}},              & ~k=n,
\end{cases} \label{ptree}
\end{align}
and $p_i, q_i$ are given in \eqref{expLi}.
\end{lemma}
\begin{comment}
\begin{proof}
\end{proof}
\end{comment}
\ifproof
\begin{proof}
Let $\vec{v}$ denote a vector chosen uniformly at random from the space $\mathbb{F}^m_2$ for some $m \in \mathbb{N}$. Probability that $\vec{v}$ satisfies $l$ random linearly independent parity checks ($l < m$) is given by $p = \frac{1}{2^l}$. For simplicity of notation, let us denote $p_i=\frac{1}{2^{l_i}}$ and $q_i= 1-p_i ~ \forall ~i \in [1:n-1]$. At every stage $i \in [1:n-1]$ of the decoding tree, let $L_i$ denote the random variable for the number of paths that survive this stage minus 1 (The true path survives all the checks deterministically). At stage $1$, probability that any binary random vector of length $2J$ satisfies $l_1$ randomly chosen parity checks is given by $p_1 = \frac{1}{2^{l_1}}$. Hence, probability that $k$ out of the $K-1$ incorrect paths survive during stage $1$ is given by
\begin{align*}
P(L_1 =k) &= {K-1 \choose k}p_1^kq_1^{K-1-k} .
\end{align*}
Hence, $L_1 \sim B(K-1,p_1)$. Also, for all $i \ge 2$ given $L_{i-1}$, $L_i$ is the sum of $L_{i-1}+1$ independent binomial random variables, $L_{i-1}$ of them with parameters $(K,p_i)$ and one of them with parameters $(K-1,p_i)$. Hence $L_i \lvert L_{i-1} \sim B((L_{i-1}+1)K-1,p_i)$. The quantity $p_{\text{tree}}$, which denotes the probability that more than one path survives the last stage of tree  decoding process can be quantified as $P(L_{n-1} \ge 1)$. To compute this probability, we first derive the probability generating function (PGF) $G_{L_{n-1}}(z)$ of the random variable $L_{n-1}$. The quantity $G_{L_{n-1}}(z)$ is defined as,
\begin{align}
G_{L_{n-1}}(z) &= \mathbf{E}[z^{L_{n-1}}] \nonumber \\
&= \sum_{k=0}^{K^{n-1}-1}P(L_{n-1}=k)z^k.   \label{pgfdef}
\end{align}
Using the fact that $L_{n-1} \lvert L_{n-2} \sim B((L_{i-2}+1)K-1,p_{n-1})$, the above expression can be computed as,
\begin{align}
G_{L_{n-1}}(z) &= \mathbf{E}[z^{L_{n-1}}] \nonumber \\
&= \mathbf{E}[\mathbf{E}[z^{L_{n-1}} \lvert L_{n-2}]] \nonumber \\
&= \mathbf{E}[\left(q_{n-1} + p_{n-1}z\right)^{(L_{n-2}+1)K-1}] \label{binom} \\
&= \left(q_{n-1} + p_{n-1}z\right)^{K-1}G_{L_{n-2}}(\left(q_{n-1}+p_{n-1}z\right)^{K}), \nonumber
\end{align}
where (\ref{binom}) is because $\mathbf{E}[z^{L_{n-1}} \lvert L_{n-2}]$ is the PGF of the binomial random variable $L_{n-1} \lvert L_{n-2}$. The above equation can be solved recursively with the initial condition $G_{L_1}(z) = (q_1 + p_1z)^{K-1}$ to yield a closed form solution for the PGF as,
\begin{align*}
G_{L_{n-1}}(z) &= \prod_{i=0}^{n-2} f_{n-1-i}^{K-1}(z), \\
\text{where}~ f_k(z) &= \begin{cases}
    q_k + p_k f_{k+1}^{K}(z),& ~1 \le k \le n-1 \\
    z^{\frac{1}{K}},              & ~k=n.
\end{cases}
\end{align*}
Using (\ref{pgfdef}), the quantity $p_{\text{tree}}$ can be finally computed as,
\begin{align}
p_{\text{tree}}=P(L_{n-1} \ge 1) &= 1 - P(L_{n-1} = 0)  \nonumber \\
&= 1 - G_{L_{n-1}}(0), \label{pgfprob}
\end{align}
where the quantity $G_{L_{n-1}}(0)$ can be computed by evaluating (\ref{pgf}) at $z=0$.
\end{proof}
\fi

\begin{table*}[!tbh]
\centering
 \begin{tabular}{||c|| c| c| c| c| c| c| c| c| c| c| c| c||}
 \hline
$\Ka$ & 25 & 50 & 75 & 100 & 125 & 150 & 175 & 200 & 225 & 250 & 275 & 300 \\[0.4ex]
 \hline
 $J$ & 14 & 14 & 14 & 14 & 14 & 15 & 15 & 15 & 15 & 15 & 15 & 15 \\[0.4ex]
 \hline
$\varepsilon_{\text{tree}}$ &0.0025 & 0.0045 & 0.006 & 0.01 & 0.0125 & 0.0055 & 0.0065 & 0.007 & 0.008 & 0.01 & 0.0125 & 0.0175 \\[0.4ex]
 \hline
\end{tabular}
\caption{Various parameters used in simulations.}
 \label{tableofparams}
\end{table*}

%\subsection{Average Decoding Complexity}
We define the computational complexity $C$ of this decoder as the number of nodes on which parity checks need to be performed.
\begin{lemma}
\label{Lemma3}
A closed-form expression for computing the expected computational complexity $\mathbf{E}[C]$ is given by
\begin{equation*} %\label{expcompucomplexity}
\mathbf{E}[C] = K \left( n-1
+ \sum_{i=1}^{n-2} \sum_{m=1}^{i} \left( K^{i-m}(K-1)
\textstyle \prod_{j=m}^{i} p_j \right) \right)
\end{equation*}
where $p_i, q_i$ are given in \eqref{expLi}.
\end{lemma}
\begin{comment}
\begin{proof}
This proof is based on computing the expected value of number of erroneous paths surviving each stage. More details about this proof can be found in \cite{vamsi2017tree}.
\end{proof}
\end{comment}
The proof of Lemma~\ref{Lemma1} relies on the fact that $L_i \lvert L_{i-1} \sim B((L_{i-1}+1)K-1,p_i)$.
The proof of Lemma~\ref{Lemma2} is based on computing a closed-form expression for the probability generating function (PGF) $G_{L_{n-1}}(z)$ of the random variable $L_{n-1}$.
Lemma~\ref{Lemma3} is a straightforward extension of Lemma~\ref{Lemma1}.
Additional details about these proofs can be found in~\cite{vamsi2017tree}.
\ifproof
\begin{proof}
For each non-leaf node that survives the stage $i,$ parity checks need to be done for all it's $K$ children. Hence, computational complexity and the expected computational complexity can be expressed as,
\begin{align}
C &= K + K\left[\sum_{i=1}^{n-2}L_i + n - 2\right], \nonumber \\
\mathbf{E}[C] &= K + K\left[ \sum_{i=1}^{n-2}\mathbf{E}[L_i] + n - 2\right]. \label{comput}
\end{align}
The quantity $\mathbf{E}[L_i]$ in (\ref{comput}) can be computed similar to the way PGF was computed in last section as given below.
\begin{align}
\mathbf{E}[L_i] &= \mathbf{E}[ \mathbf{E}[L_i \lvert L_{i-1}]] \nonumber \\
&= \mathbf{E}[((L_{i-1}+1)K-1)p_i] \label{exprec} \\
&= p_iK\mathbf{E}[L_{i-1}] + p_i(K-1), \nonumber
\end{align}
where \eqref{exprec} is due to the fact that $\mathbf{E}[L_{i} \lvert L_{i-1}]$ is the mean of the binomial random variable $L_{i} \lvert L_{i-1}$. The above equation can be solved recursively using the initial condition $\mathbf{E}[L_{1} ]= (K-1)p_1$ to get the following closed form expression:
\begin{align}
\mathbf{E}[L_{i}] = \sum_{m=1}^{i} \left[K^{i-m}(K-1) \prod_{j=m}^{i}p_j\right]. \label{expclosedform}
\end{align}
Substituting (\ref{expclosedform}) into (\ref{comput}), the final succinct closed form expression for the expected computational complexity is given as,
\begin{align}
\mathbf{E}[C] = K \left[n-1 + \sum_{i=1}^{n-2}\sum_{m=1}^{i} \left[K^{i-m}(K-1) \prod_{j=m}^{i}p_j\right]\right]. \label{expcompucomplexity}
\end{align}
\end{proof}
\fi


\subsection{Choice of the Parity Length Vector}
\label{parityvector}
%It can be seen from (\ref{ptree}) and (\ref{expcompucomplexity}) that there is a trade-off between probability of decoding failure and the average decoding complexity.
\begin{comment}
For a fixed code rate of the tree encoder (fixed number of total parity bits), allocating more parity bits at the initial stages of the tree would result in a lesser average complexity. However, doing this would result in an order increase in the probability of decoding failure. Similarly, allocating more parity bits at the later stages of the tree would result in lesser error rates, but at the expense of higher computational costs.
\end{comment}
%Hence, parity lengths have to be chosen such that both complexity and the error rate can be handled. Towards this end, 
We formulate the constrained optimization problem of minimizing the expected complexity subject to the probability of decoding failure being less than a carefully chosen threshold $\varepsilon_{\mathrm{tree}}$.
Since the parity lengths are non-negative integers, such a problem would be very difficult to solve.
As such, we relax the problem to $(l_1,l_2,\dots,l_{n-1}) \in \mathbb{R}^{n-1}_+$. Also, we replace the constraint $p_{\mathrm{tree}} \le \varepsilon_{\mathrm{tree}}$ with $ \mathbf{E}\left[L_{n-1}\right] \le \varepsilon_{\mathrm{tree}}$ for the purpose of mathematical tractability.
(By Markov's inequality, the quantity $\mathbf{E}\left[L_{n-1}\right]$ is an upper bound on $p_{\mathrm{tree}}$).
After these modifications, the optimization framework for the choice of parity lengths is given by
\begin{equation}
\begin{aligned}
& \underset{(p_1, p_2 , \dots, p_{n-1})}{\text{minimize}}
& &\mathbf{E}[C] \\
& \text{subject to}
& & \mathbf{E}\left[L_{n-1}\right] \le \varepsilon_{\mathrm{tree}}, \\
&&& \textstyle \sum_{i=1}^{n-1} \log_2\left({1}/{p_i}\right) = M-B, \\
&&& p_i \in \left[{1}/{2^{J}},1\right]~ \forall~i \in [1:n-1]. \label{optprob3}
\end{aligned}
\end{equation}
The above is a geometric program \cite{boyd2004convex}, and it can be solved using standard convex solvers.
We choose the parity check lengths as $\hat{l}_i = \left[ \log_2 \left({1}/{\hat{p}_i}\right) \right]$, for $i \in [1:n-1]$ where $\big( \hat{p}_1, \hat{p}_2, \dots \hat{p}_{n-1} \big)$ is the solution to the optimization problem.

\begin{figure}[!ht]
\centering
\resizebox{0.5\textwidth}{!}{\input{Full_plot.tex}}
%\includegraphics[trim={3 cm 0cm 0cm 2cm},clip,width=0.5\textwidth]{
\caption{Minimum $E_b/N_0$ required to acheive $P_e \le 0.05$ vs.\ number of users for various schemes.
Results for $2$ and $3$ iterations (see Remark~\ref{rmk:iterations}) are represented by `x' and `o'.
Observe that the SNR gains diminish with each iteration.}
\label{fig:sim_results}
\end{figure}

\section{Simulation Results}
\label{sec:simresults}
In this section, we study the performance of the proposed framework and we provide comparisions with existing schemes in literature.
We consider a system with $\Ka \in [25:300]$ active users, each having $B=75$~bits of information to transmit.
We divide these bits into $n=11$ sub-blocks and the quantity $J$, which denotes the length of each sub-block is chosen depending on $\Ka$;
it is given in Table~\ref{tableofparams}.
Similar to \cite{vem2017user}, we use sensing matrices that are constructed based on BCH codes for the compressed sensing problem. Specifically, we select a subset $\mathcal{C}^0$ of codewords of size $|\mathcal{C}^0|=2^J$ from the (2047,23) BCH codebook $\mathcal{C}$ with the following properties: \textbf{(i)} $\vec{c} \in \mathcal{C}^0 \implies \vec{1} \oplus \vec{c} \in \mathcal{C}\setminus \mathcal{C}^0$, where $\vec{1} \oplus \vec{c}$ denotes the one's complement of $\vec{c}$; \textbf{(ii)} $\vec{c}_1, \vec{c}_2 \in \mathcal{C}^0 \implies \vec{c}_1 + \vec{c}_2 \in \mathcal{C}^0$; \textbf{(iii)} $\vec{0} \in \mathcal{C}^0$ where $\vec{0}$ denotes the all zero codeword.
\begin{comment}
\begin{enumerate}[label=(\roman*)]
\item $\vec{c} \in \mathcal{C}^0 \implies \vec{1} \oplus \vec{c} \in \mathcal{C}\setminus \mathcal{C}^0$, where $\vec{1} \oplus \vec{c}$ denotes the one's complement of $\vec{c}$.
\item $\vec{c}_1, \vec{c}_2 \in \mathcal{C}^0 \implies \vec{c}_1 + \vec{c}_2 \in \mathcal{C}^0$.
\item $\vec{0} \in \mathcal{C}^0$, where $\vec{0}$ denotes the all zero codeword.
\end{enumerate}
\end{comment}
We then choose the sensing matrix as $\mathbf{A} = [\vec{a}_0, \vec{a}_1, \cdots, \vec{a}_{2^J-1}]$, with dimension $2047 \times 2^J$ where $\vec{a}_i=\sqrt{P}(2\vec{c}_i-1), \vec{c}_i \in \mathcal{C}^0$ for $i \in [0:2^J-1]$. The total number of channel uses is therefore given by $N=11\times2047=22,517$. The target error probability of the system is fixed at $\varepsilon=0.05$. We set list size $K$ for the NNLS CS problem to $K=\Ka+10$. For each $\Ka \in [25:300]$, we solve the optimization problem \eqref{optprob3} using the CVX solver~\cite{grant2008cvx}, and the resulting solution dictates the choice of parity length vector.
Choice of the quantity $\varepsilon_{\mathrm{tree}}$ for each $\Ka$ is given in Table~\ref{tableofparams}.
The parameters $B$ and $N$ are chosen such that the rate $\frac{B}{N}=\frac{75}{22,517}$ is approximately the same as the rate resulting from the choice of parameters $B=100$ and $N=30,000$ in \cite{ordentlich2017low,vem2017user}.
This enables a fair comparison between these schemes and our proposed scheme.
We emphasize that the choice of $B$ and $N$ for our simulations is motivated by the existence of good compressive sensing matrices based on BCH codes.
When these parameters are proportionally scaled up, performance of the system can only improve, as the finite block length effects are more pronounced for lower values of $B$ and $N$.
%The subset thus generated has good distance properties ($d_{\text{max}}-d_{\text{min}}$ is very small, where $d_{\text{max}}$ and $d_{\text{min}}$ denote the maximum and minimum distance between the codewords respectively) and is more suitable for noisy recovery of binary sparse signals than random Gaussian based constructions. 
In Fig.~\ref{fig:sim_results}, the $E_b/N_0$ required to achieve a target error probability of 0.05 is plotted as a function of $\Ka$ for various schemes.
%The bottom most curve corresponds to Polyanski's acheivability bound\cite{polyanskiy2017perspective} on the performance of a finite block length (FBL) code for this model. The curves labelled $T=2, T=4$ and 4-fold ALOHA which correspond to the performance curves in\cite{avinash2017unsourcedma} and \cite{ordentlich2017low} assume the existance of a code for the $T$-user MAC channel which acheives the bound in \cite{polyanskiy2017perspective}. The curve labelled OP-Exact describes the performance of a practical scheme introduced in \cite{ordentlich2017low}.
%
%\textit{Conclusion}:
It can be seen from Fig.~\ref{fig:sim_results} that our proposed scheme with just one extended round of iteration outperforms existing schemes for $\Ka \in [75:300]$. 
%Moreover, similar to the FBL bound, the slope of the curve corresponding to one iteration is very small for small values of $\Ka$.
% and  we expect all the curves corresponding to various iterations to converge.
%It would be interesting to compare the slope of the limiting curve with that of the FBL bound.

% References should be produced using the bibtex program from suitable
% BiBTeX files (here: strings, refs, manuals). The IEEEbib.bst bibliography
% style file from IEEE produces unsorted bibliography list.
% -------------------------------------------------------------------------
\bibliographystyle{IEEEbib}
\bibliography{IEEEabrv,MACcollision}

\end{document}
