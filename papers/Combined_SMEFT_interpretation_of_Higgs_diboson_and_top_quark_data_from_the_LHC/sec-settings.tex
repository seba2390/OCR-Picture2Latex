\section{Fitting methodology}
\label{sec:fitsettings}

In this section we describe the fitting methodology that
is used in this work to map the EFT parameter space spanned by 
the Higgs, diboson, and top quark data.
%
In addition to results obtained with
the Monte Carlo replica fitting (MCfit) method
presented in Ref.~\cite{Hartland:2019bjb},
now we also determine the posterior probability 
distributions in the parameter space
using the {\tt MultiNest} Nested Sampling (NS) 
algorithm~\cite{Feroz:2013hea,Feroz:2007kg},
a robust sampling procedure that is completely orthogonal to
the MCfit method and that is based on Bayesian inference.

We begin with a brief discussion of the 
log-likelihood function and the treatment of uncertainties that is adopted
in the fit.
%
We then present the individual $\chi^2$ profiles associated to each
EFT coefficient in the quadratic fits
and discuss the eventual presence of (quasi-)degenerate minima.
%
Subsequently, the main features of the NS and MCFit strategies
used in the global fit are summarized, including 
several improvements that have been
implemented in the latter technique. The results obtained
with the two methods are also benchmarked.
%
Finally, we carry out a principal component analysis (PCA)
to determine the linear combinations of parameters that have
the highest and lowest variabilities
given our global dataset and
assess the possible presence of flat directions.

\subsection{Log-likelihood}
\label{sec:generalsettings}

The overall fit quality is quantified by the log-likelihood, 
or $\chi^2$ function, defined as
\begin{equation}
  \chi^2\lp {\boldsymbol c} \rp \equiv \frac{1}{n_{\rm dat}}\sum_{i,j=1}^{n_{\rm dat}}\lp 
  \sigma^{(\rm th)}_i\lp {\boldsymbol c} \rp
  -\sigma^{(\rm exp)}_i\rp ({\rm cov}^{-1})_{ij}
\lp 
  \sigma^{(\rm th)}_j\lp {\boldsymbol c}\rp
  -\sigma^{(\rm exp)}_j\rp
 \label{eq:chi2definition2}
    \; ,
\end{equation}
where $\sigma_i^{\rm (exp)}$ and
$\sigma^{(\rm th)}_i\lp {\boldsymbol c}\rp$ are the
central experimental data and corresponding theoretical
predictions for the $i$-th cross-section, respectively.
%
The total covariance 
matrix, ${\rm cov}_{ij}$, should contain all relevant sources of
experimental and theoretical uncertainties.
%
Assuming the latter are normally
distributed, and that they are uncorrelated
with the experimental uncertainties,
this total covariance matrix can be expressed as
a sum of the separate experimental and theoretical covariance 
matrices~\cite{AbdulKhalek:2019ihb,AbdulKhalek:2019bux},
\be
\label{eq:covmatsplitting}
{\rm cov}_{ij} = {\rm cov}^{(\rm exp)}_{ij} + {\rm cov}^{(\rm th)}_{ij} \, .
\ee
%
As usual, the experimental covariance matrix is constructed from all
sources of statistical and systematic uncertainties that are
made available by the experiments (as discussed
in Sect.~\ref{sec:settings_expdata}).
%
Moreover, the correlated multiplicative uncertainties are treated
via the `$t_0$' prescription~\cite{Ball:2009qv} in the fit, while the standard
experimental definition is used to quote the resulting $\chi^2$ values.

Concerning the theoretical covariance matrix, ${\rm cov}^{(\rm th)}$, its contributions depend
on the specific type of processes considered.
%
In the case of the top quark and LHC diboson production cross-sections,
we compute the SM predictions using the best possible
theoretical accuracy.
%
In doing so, we also evaluate the uncertainty associated
to the input PDFs and their correlations, as discussed in Ref.~\cite{Hartland:2019bjb}.
%
These computations are based on the NNPDF3.1 no-top fit~\cite{Ball:2017nwa}, a global
PDF determination based on a dataset that excludes
all measurements that are used in the present SMEFT analysis.

For the Higgs production
and decay measurements,
we take instead the SM predictions from the experimental publications,
which in turn are collected from the HXSWG reports.
%
In such a case, the total theory uncertainty is available, which includes both
PDF errors and missing higher order uncertainties (MHOUs).
%
The total theory uncertainty for Higgs measurements is therefore
included in the fit covariance matrix by means
of the correlation prescription provided in the corresponding ATLAS and CMS
publications.

\subsection{Individual fits from the $\chi^2$ profiles}
\label{sec:quarticfits}

Individual (one-parameter) fits correspond to varying a single EFT coefficient while keeping
the rest fixed to their SM values.
%
While such fits neglect the correlations between the
different coefficients, they provide a useful 
baseline for the global analysis,
since there the CL intervals will be by construction looser (or at best, similar)
as compared to those of the one-parameters fits.
%
They are also computationally inexpensive, 
as they can be carried out analytically 
from a scan of the $\chi^2$ profile
without resorting to numerical methods.
%
Another benefit is that 
they facilitate the comparison between different
EFT analyses, which 
may adopt different fitting bases but whose individual bounds 
should be similar provided they are based on comparable 
data sets and theoretical calculations.

In the scenario where a single EFT coefficient, $c_j$, is allowed to vary while the rest are set to zero,
the theoretical cross-section (for $\Lambda=1$ TeV) given by 
Eq.~(\ref{eq:quadraticTHform}) simplifies to
\be
\label{eq:quadraticTHform_simplified}
\sigma_m^{\rm (th)}(c_j)= \sigma_m^{\rm (sm)} + c_j\sigma^{(\rm eft)}_{m,j} +
c_j^2 \sigma^{(\rm eft)}_{m,jj} \, ,
\ee
which results in a quartic polynomial form for the $\chi^2$ when inserted into 
Eq.~(\ref{eq:chi2definition2}), namely
\be
\label{eq:quartic-chi2}
\chi^2(c_j) = \sum_{k=0}^4 a_k \lp c_j\rp^k \, .
\ee
Restricting the analysis to the linear order in the EFT expansion further
simplifies Eq.~(\ref{eq:quartic-chi2}) to a parabolic form,
\be
\label{eq:quadratic-chi2}
\chi^2(c_j) = \sum_{k=0}^2 a_k \lp c_j\rp^k = \chi^2_0 + b\lp  c_j-c_{j,0}\rp^2 \, ,
\ee
where $c_{j,0}$ is the value of $c_j$ at the minimum of the parabola,
and in this case  linear error propagation (Gaussian statistics) is applicable.

To determine the values of the quartic polynomial coefficients $ a_k$ in Eq.~(\ref{eq:quartic-chi2}),
it is sufficient to fit this functional form to a scan of the $\chi^2$ profile
obtained by varying the EFT coefficient
$c_j$ when all other coefficients are set to their SM value.
%
The associated
95\% CL interval to the coefficient $c_j$ can then be 
determined by imposing the condition
\be
\label{eq:deltachi2}
\chi^2(c_j)-\chi^2(c_{j,0}) \equiv \Delta \chi^2 \le 
  5.991\, .
\ee
We note that if the size of the quadratic
$\mathcal{O}\lp \Lambda^{-4}\rp$ corrections is sizable,
there will be more than one solution for $c_{j,0}$
and one might end up with  pairwise disjoint CL intervals.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure}[htbp]
  \begin{center}
    \includegraphics[width=0.297\linewidth]{plots_v2/OQQ1-Baseline1fit.pdf}
\includegraphics[width=0.297\linewidth]{plots_v2/OQQ8-Baseline1fit.pdf}
\includegraphics[width=0.297\linewidth]{plots_v2/OQt1-Baseline1fit.pdf}
\includegraphics[width=0.297\linewidth]{plots_v2/OQt8-Baseline1fit.pdf}
\includegraphics[width=0.30\linewidth]{plots_v2/Ott1-Baseline1fit.pdf}
\includegraphics[width=0.297\linewidth]{plots_v2/O11qq-Baseline1fit.pdf}
\includegraphics[width=0.297\linewidth]{plots_v2/O13qq-Baseline1fit.pdf}
\includegraphics[width=0.297\linewidth]{plots_v2/O1dt-Baseline1fit.pdf}
\includegraphics[width=0.297\linewidth]{plots_v2/O1qd-Baseline1fit.pdf}
\includegraphics[width=0.297\linewidth]{plots_v2/O1qt-Baseline1fit.pdf}
\includegraphics[width=0.297\linewidth]{plots_v2/O1qu-Baseline1fit.pdf}
\includegraphics[width=0.297\linewidth]{plots_v2/O1ut-Baseline1fit.pdf}
\includegraphics[width=0.297\linewidth]{plots_v2/O81qq-Baseline1fit.pdf}
\includegraphics[width=0.297\linewidth]{plots_v2/O83qq-Baseline1fit.pdf}
\includegraphics[width=0.297\linewidth]{plots_v2/O8dt-Baseline1fit.pdf}
\includegraphics[width=0.297\linewidth]{plots_v2/O8qd-Baseline1fit.pdf}
\includegraphics[width=0.297\linewidth]{plots_v2/O8qt-Baseline1fit.pdf}
\includegraphics[width=0.297\linewidth]{plots_v2/O8qu-Baseline1fit.pdf}
\caption{\small Results of quartic polynomial fits
  to the $\chi^2$ profiles obtained  in one-parameter scans
  for each EFT coefficient, with all others set to their
  SM values.
  %
  We show the absolute $\chi^2$ for the  $n_{\rm dat}=317$ data points
  of the global dataset calculated with the $t_0$ prescription,
  with the horizontal (vertical) line indicating the
  corresponding 95\% CL ranges (the SM prediction).
     \label{fig:quartic-individual-fits} }
  \end{center}
\end{figure}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure}[htbp]
  \begin{center}
    \includegraphics[width=0.297\linewidth]{plots_v2/O8ut-Baseline1fit.pdf}
\includegraphics[width=0.30\linewidth]{plots_v2/Otp-Baseline1fit.pdf}
\includegraphics[width=0.30\linewidth]{plots_v2/OtG-Baseline1fit.pdf}
\includegraphics[width=0.30\linewidth]{plots_v2/Obp-Baseline1fit.pdf}
\includegraphics[width=0.30\linewidth]{plots_v2/Ocp-Baseline1fit.pdf}
\includegraphics[width=0.30\linewidth]{plots_v2/Otap-Baseline1fit.pdf}
\includegraphics[width=0.30\linewidth]{plots_v2/OtW-Baseline1fit.pdf}
\includegraphics[width=0.30\linewidth]{plots_v2/OtZ-Baseline1fit.pdf}
\includegraphics[width=0.30\linewidth]{plots_v2/O3pQ3-Baseline1fit.pdf}
\includegraphics[width=0.30\linewidth]{plots_v2/OpQM-Baseline1fit.pdf}
\includegraphics[width=0.30\linewidth]{plots_v2/Opt-Baseline1fit.pdf}
\includegraphics[width=0.30\linewidth]{plots_v2/OWWW-Baseline1fit.pdf}
\includegraphics[width=0.30\linewidth]{plots_v2/OpB-Baseline1fit.pdf}
\includegraphics[width=0.30\linewidth]{plots_v2/OpG-Baseline1fit.pdf}
\includegraphics[width=0.30\linewidth]{plots_v2/OpW-Baseline1fit.pdf}
\includegraphics[width=0.30\linewidth]{plots_v2/Opd-Baseline1fit.pdf}
\caption{\small Fig.~\ref{fig:quartic-individual-fits} continued.
     \label{fig:quartic-individual-fits-2} }
  \end{center}
\end{figure}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Figs.~\ref{fig:quartic-individual-fits} and~\ref{fig:quartic-individual-fits-2} display
the results of quartic polynomial fits
  to the $\chi^2$ profiles obtained  in the one-parameter scans
 of each EFT coefficient,  based on the $n_{\rm dat}=317$ data points
 of the global dataset and the baseline theory settings (where higher-order QCD and EFT
 corrections are accounted for).
  %
 Here the absolute $\chi^2$ is evaluated with the $t_0$ prescription,
 and we also display the 
 corresponding 95\% CL ranges (vertical line) and the SM expectation (horizontal line).
 %
 We show the 34 profiles associated to the independent EFT coefficients
 in Table~\ref{tab:operatorbasis} which are not constrained by the EWPOs.
 %
 These profiles are shown in the following order: four-heavy, two-light-two-heavy, two-fermion,
 and purely bosonic coefficients.

From the $\chi^2$ profiles displayed in Figs.~\ref{fig:quartic-individual-fits} 
and~\ref{fig:quartic-individual-fits-2} one can observe, on the one hand,
how for several of the coefficients  the parabolic approximation 
performs reasonably well, indicating the dominance of the linear EFT corrections.
%
On the other hand, other coefficients
deviate from the parabolic behaviour in a striking manner, including
several degrees of freedom that exhibit two quasi-degenerate solutions,
one being ``SM-like'' and the other distinctly non-zero.
%
It is important to identify in particular which coefficients display such degenerate
solutions in the one-parameter fits, since these might lead to a multi-modal posterior distributions
in the case of the global analysis.

From the inspection of these $\chi^2$ profiles,
one can identify three categories of EFT coefficients whose individual profiles
are poorly described by the parabolic approximation.
%
First of all, one has the case of coefficients such as the four-heavy operators, for which a quartic profile
with two quasi-degenerate solutions distributed symmetrically around the SM value is observed.
%
Secondly, there are coefficients such as $c_{tZ}$ which display a second solution
far from the SM-like one but which corresponds to higher values of the $\chi^2$, and hence
does not modify the calculation of the CL intervals (at least within these 1D fits).
%
In both cases,  the resulting CL intervals remain non-disjoint.
%
Thirdly, one finds coefficients that exhibit quasi-degenerate solutions leading to
disjoint CL intervals, where again one solution is SM-like and the other is far from the SM value.
%
Examples of this category are the operators that modify the bottom 
 and tau lepton Yukawa interactions, $c_{\varphi b}$ and $c_{\varphi \tau}$, and
 the purely bosonic operators $c_{\varphi B}$ and $c_{\varphi W}$.
 %
 Such degenerate solutions are likely to propagate to the global fit where all operators
 are simultaneously varied,
 and indeed as will be discussed in Sect.~\ref{sec:results} the presence of these quasi-degenerate
 minima on the one-parameter fits has consequences at the level
 of posterior probability distributions in the global case.
 
Another useful application of the parameter bounds
obtained from these individual fits is to help defining in an automated manner
the suitable initial 
sampling ranges for each EFT coefficient  in the global fits based on the MCfit and NS approaches.
%
With this motivation, the individual bounds
corresponding to a given input dataset and settings of the theoretical calculations
are evaluated by default before each fitting run.
%
Let us also mention that the bounds on the EFT coefficients obtained
with the method presented here for the one-parameter analyses (quartic fits to the $\chi^2$ profiles) are found
to be in agreement with the corresponding results obtained the NS and MCfit approaches.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Nested Sampling}
\label{sec:nestedsampling}

The main approach that is adopted in this work to constrain the EFT parameter
space is Nested Sampling (NS), specifically
the version implemented in the {\tt MultiNest} algorithm~\cite{Feroz:2013hea}.
%
In comparison to MCfit, which is an optimisation problem aimed
to determine the best-fit values for each of the replicas, NS is based on 
sampling the figure of merit $\chi^2$  to determine its dependence
on the Wilson coefficients and locate the region of maximum likelihood.
%
Since NS is completely independent from the MCfit procedure,
its availability makes possible validating the robustness of the resulting bounds in EFT parameter
space via  two orthogonal methods. 

The starting point of NS is Bayes' theorem, which allows one to evaluate the
probability distribution of a set of parameters $\boldsymbol{c}$ 
associated to a model $\mathcal{M}(\boldsymbol{c})$
given a set of experimental measurements $\mathcal{D}$,
\be
\label{eq:bayestheorem}
P\lp\boldsymbol{c}| \mathcal{D},\mathcal{M} \rp = \frac{P\lp\mathcal{D}|\mathcal{M},\boldsymbol{c}
  \rp P\lp \boldsymbol{c}|\mathcal{M}  \rp
}{P(\mathcal{D}|\mathcal{M})} \, .
\ee
Here $P\lp\boldsymbol{c}| \mathcal{D},\mathcal{M} \rp $ represents the posterior
probability of the model parameters given the assumed model and the observed
experimental data,
$P\lp\mathcal{D}|\mathcal{M},\boldsymbol{c}
\rp = \mathcal{L}\lp\boldsymbol{c} \rp$ is the likelihood (conditional
probability) of the experimental measurements
given the model and a specific choice of parameters,
and $P\lp \boldsymbol{c}|\mathcal{M}  \rp = \pi \lp  \boldsymbol{c} \rp $
is the prior distribution for the model parameters.
%
The denominator in Eq.~(\ref{eq:bayestheorem}), $P(\mathcal{D}|\mathcal{M}) = \mathcal{Z}$,
is known as the Bayesian evidence and ensures the normalisation of the posterior
distribution,
\be
\mathcal{Z} = \int \mathcal{L}\lp  \boldsymbol{c} \rp
\pi \lp  \boldsymbol{c} \rp d \boldsymbol{c} \, ,
\ee
where the integration is carried out over the domain of the model parameters $\boldsymbol{c}$.

The key ingredient of Nested Sampling is to utilise the ideas underlying 
Bayesian inference to map the $n$-dimensional integral over the prior density 
in model parameter space $\pi(\boldsymbol{c} )d\boldsymbol{c}$,
where $n$ represents the dimensionality of $\boldsymbol{c}$, into a one-dimensional function
of the form
\be
\label{eq:NS1}
X(\lambda) = \int_{\{ \boldsymbol{c} : \mathcal{L}\lp\boldsymbol{c} \rp > \lambda \}}
\pi(\boldsymbol{c} ) d\boldsymbol{c} \,. 
\ee
In this expression, the prior mass $X(\lambda)$ corresponds to the
(normalised) volume
of the prior density $\pi(\boldsymbol{c} )d\boldsymbol{c}$ associated with values
of the model parameters that lead to a likelihood $\mathcal{L}\lp\boldsymbol{c}\rp $ greater
than the parameter $\lambda$.
%
Note that by construction, the prior mass $X$ decreases monotonically
from the limiting value $X=1$ to $X=0$ as $\lambda$ is increased.
%
The integration of $X(\lambda)$ extends over the regions in the model parameter space contained
within the fixed-likelihood contour defined by the condition $\mathcal{L}\lp \boldsymbol{c} \rp=\lambda$.
%
This property allows the evidence to be expressed as,
\be
\label{eq:bayesianevidence}
\mathcal{Z} = \int_0^1  \mathcal{L}\lp X\rp dX \, ,
\ee
where $  \mathcal{L}\lp X\rp$ is defined as the inverse function of $X(\lambda)$, which
always exists provided the likelihood is a continuous and smooth function
of the model parameters.
%
Therefore, the transformation from $\boldsymbol{c}$
to $X$ in Eq.~(\ref{eq:NS1}) achieves a mapping of the prior distribution into infinitesimal
elements, sorted by their associated likelihood $\mathcal{L}(\boldsymbol{c})$.

The next step of the NS algorithm is to define a decreasing sequence of values in the prior
volume, that is now parameterised by the prior mass $X$.
%
In other words, one slices the prior volume into a large number of
small regions
\be
1 = X_0 > X_1 > \ldots X_{\infty} = 0 \, ,
\ee
and then evaluates the likelihood at each of these values, $\mathcal{L}=\mathcal{L}(X_i)$.
%
This way, all of the $\mathcal{L}_i$ values
can be summed in order to  evaluate the integral 
for the Bayesian evidence, Eq.~(\ref{eq:bayesianevidence}).

Since in general the likelihood $\mathcal{L}({\boldsymbol c})$ exhibits a complex dependence
on the model parameters $\boldsymbol{c}$, the summation
in Eq.~(\ref{eq:bayesianevidence}) must be evaluated
numerically using {\it e.g.} Monte Carlo integration methods.
%
In practice, one draws $N_{\rm live}$ points from the parameter prior
volume $\pi\lp\boldsymbol{c} \rp$, known as {\it live points}, and orders
the likelihood values from smallest to largest, including the 
starting value of the prior mass at $X_0=1$.
%
As samples are drawn from the prior volume, 
the live point with the lowest likelihood $\mathcal{L}_i$
is removed from the set and replaced by another live point drawn from the same prior
distribution but now under the constraint that its likelihood is larger than
$\mathcal{L}_i$.
%
This sampling process is repeated until the entire hyper-volume
$\pi \lp \boldsymbol{c} \rp$ of the prior parameter space has been covered, with
ellipsoids of constrained likelihood being assigned
to the live-points as the prior volume is scanned.

While the end result of the NS procedure is the estimation of the 
Bayesian evidence $\mathcal{Z}$, as a byproduct one also obtains 
a sampling of the posterior distribution
associated to the EFT coefficients expressed as
\be
\{ \boldsymbol{c}^{(k)} \}\, ,\qquad  k=1,\dots,N_{\rm spl}\, ,
\ee
with $N_{\rm spl}$ indicating the number
of samples drawn by the final NS iteration.
%
One can then compute expectation values, variances, and correlations of the model
parameters by evaluating the MC sum over these posterior samples together 
with their associated weights, in the same
manner as averages are carried out
over the $N_{\rm rep}$ replicas in the MCfit method. 

\paragraph{Prior volume.}
%
An important input for NS is the choice of prior volume $\pi \lp \boldsymbol{c} \rp$
in the model parameter space.
%
In this analysis, we adopt flat priors 
defined by ranges in parameter space for the coefficients $\boldsymbol{c}$.
%
A suitable choice of prior volume where the sampling takes place is important
to speed up the NS algorithm: a range too wide will make the optimisation less
efficient, while a range too narrow might bias the results by cutting 
specific regions of the parameter space that are relevant.
%
Furthermore, using a common range for all parameters should be avoided,
since the range of intrinsic variation will be rather different for each
of the EFT coefficients, as illustrated also by the one-parameter fits
reported in Figs.~\ref{fig:quartic-individual-fits} 
and~\ref{fig:quartic-individual-fits-2}.

Taking these considerations into account, we adopt here the following strategy.
%
First, a single model parameter $c_i$ is allowed to vary
while all others are set to their SM value, $c_j=0$ for $j\ne i$.
%
The $\chi^2 \lp c_i \rp$ is then scanned in this
direction to determine the values $c_i^{\rm (min)}$ and  $c_i^{\rm (max)}$
satisfying the condition $\chi^2/n_{\rm dat}=4$.
%
We then repeat this procedure 
for all parameters and end up with a hyper-volume 
defined by pairs of values
\be
\pi \lp \boldsymbol{c} \rp = \lc \lp c_i^{\rm (min)},c_i^{\rm (max)} \rp \, ,\quad
i=1,\ldots, n_{\rm op} \rc \, ,
\ee
which then defines the initial prior volume.
%
At this point, one performs an initial exploratory NS global analysis using this
volume to study the posterior probability distribution
for each EFT coefficient.
%
Our final analysis is then obtained by manually adjusting the 
initial sampling ranges until the full posterior distributions are captured for the chosen
prior volume. 
%
For parameters that are essentially unconstrained in the global fit,
such as the four-heavy operators in the case of linear EFT calculations,
a hard boundary of $\lp -50 , 50 \rp$ is imposed (for $\Lambda=1$ TeV) . 

\paragraph{Performance.}
%
In order to increase the efficiency of the posterior probability estimation by NS, we enable the
``constant efficiency mode'' in MultiNest, which adjusts the
total volume of ellipsoids spanning the live points so that the sampling
efficiency is close to its associated hyperparameter set by the user. 
%
With 24 cpu cores, 
we are able to achieve an accurate posterior for the linear EFT fits
in around 30 minutes using 500 live points, a target efficiency of 0.05, and
an evidence tolerance of 0.5, which results in $N_{\rm spl}\simeq 5000$ posterior samples.
%
To ensure the stability of our final results, we chose 
1000 live points and a target efficiency of 
0.005, which yields $\simeq$$1.5\times 10^4$ samples for the
linear analysis and $\simeq$$10^4$ samples for an analysis that includes also the quadratic EFT
corrections.
%
With these settings, our final global analyses containing the simultaneous
determination of $n_{\rm op}\simeq 36$ coefficients
take $\sim 3.5$ hours running in 24 cpu cores, with a similar performance for
linear and quadratic EFT fits.

The NS method is especially suitable to tackle parameter spaces of moderate dimensionality.
%
Being based purely on sampling, it is not affected by limitations in minimisation
methods such as ending up in local minima.
%
It is also more robust upon the presence of fluctuations, and does not
require specifying certain hyperparameters such as the learning rates which are used in MCfit.
%
The main limitation of NS is that, as in all sampling methods, the execution times
grows exponentially with $n_{\rm op}$, the dimensionality of the model parameter space.
%
For parameter spaces of dimensionality greater than around 50, 
the current NS implementation that we use
becomes unpractically slow and MCfit becomes the most suitable strategy available.

\subsection{The Monte Carlo replica method revisited}
\label{sec:mcfit}

The {\tt SMEFiT} analysis of Ref.~\cite{Hartland:2019bjb} was based
on the Monte Carlo replica approach (MCfit), which in turn was inspired by the
NNPDF analysis of the quark and gluon substructure of protons.
%
The MCfit method aims to construct a sampling of the probability
distribution in the space of the experimental data, which then translates 
into a sampling of the probability distribution in the space of the EFT 
coefficients through an optimisation procedure where the best-fit values
of the coefficients for each replica, $\boldsymbol{c}^{(k)}$,
are determined.

Given an experimental measurement of a hard-scattering
cross-section, denoted by $\sigma_i^{\rm (exp)}$, with
total uncorrelated uncertainty $\delta_{i}^{\rm (stat)}$ and $n_{\rm sys}$ 
correlated systematic uncertainties $\delta^{\rm (sys)}_{i,\alpha}$,
the $N_{\rm rep}$ artificial MC replicas of the 
experimental data are generated as
\be
\label{eq:replicas}
\sigma_{i}^{(\art)(k)}
= 
\sigma_{i}^{\rm (\mrexp)}\lp 1
+
r_{i}^{(k)}\delta_{i}^{\rm (stat)}
+
\sum_{\alpha=1}^{n_{\rm sys}}r_{i,\alpha}^{(k)}\delta^{\rm (sys)}_{i,\alpha}\rp
\ , \quad k=1,\ldots,N_{\rep} \ , 
\ee
%
where the index $i$ runs from 1 to $n_{\rm dat}$ and
$r_{i}^{(k)}$, $r_{i,\alpha}^{(k)}$
are univariate Gaussian random numbers.
%
Correlations between data points induced by systematic uncertainties 
are accounted for by ensuring that $r^{(k)}_{i,\alpha}=r^{(k)}_{i',\alpha}$.
%
It can be show that central values, variances, and covariances evaluated
by averaging over the MC replicas reproduce the corresponding
experimental values.

A fit to the $n_{\rm op}$ degrees of freedom $\boldsymbol{c}/\Lambda$
is then performed for each of the MC replicas generated by Eq.~\eqref{eq:replicas}.
%
These best-fit values are determined from
the minimisation of the cost function
\begin{equation}
  E^{(k)}({\boldsymbol c})\equiv \frac{1}{n_{\rm dat}}\sum_{i,j=1}^{n_{\rm dat}}\lp 
  \sigma^{(\rm th)}_i\lp {\boldsymbol c}^{(k)}\rp-\sigma^{{(\rm art)}(k)}_i\rp ({\rm cov}^{-1})_{ij}
  \lp \sigma^{(\rm th)}_j\lp {\boldsymbol c}^{(k)} \rp-\sigma^{{(\rm art)}(k)}_j\rp
  \label{eq:chi2definition}
    \; ,
\end{equation}
where $\sigma^{(\rm th)}_i( {\boldsymbol c}^{(k)} )$ indicates the theoretical
prediction for the $i$-th cross-section evaluated with the $k$-th set of
EFT coefficients.
%
This process results in a collection of ${\boldsymbol c}^{(k)}$ best-fit 
coefficient values from which estimators such as expectation values, variances,
and correlations are evaluated.
%
The overall fit quality is then evaluated using Eq.~(\ref{eq:chi2definition2}),
where the central experimental values are compared to the mean theoretical
prediction computed by the resulting fit replicas.

As mentioned in Sect.~\ref{sec:generalsettings}, 
various theoretical uncertainties
are also included in the $\chi^2$ definition for some datasets.
%
A consistent treatment
of theoretical uncertainties in the fitting procedure means
that these are not only included in the fit via 
the covariance matrix in Eqs.~(\ref{eq:chi2definition}), 
but also in the corresponding replica generation.
%
In other words, the replicas are sampled according to a multi-Gaussian distribution
defined by the total covariance matrix Eq.~(\ref{eq:covmatsplitting})
which receives contributions both of experimental and of theoretical origin.
%
We therefore account for such errors
in the generation of Monte Carlo replicas~\cite{AbdulKhalek:2019ihb} 
using Eq.~(\ref{eq:replicas}).

There are numerous advantages of using the MCfit method for 
global EFT analyses. 
%
First, it does not require specific 
assumptions about the underlying probability distribution
of the fit parameters, and in particular does not rely
on the Gaussian approximation.
%
Secondly, the computational cost scales in a much milder way 
with the number of operators 
$n_{\rm op}$ included in the fit as compared to NS. 
%
Thirdly, it can be used to assess the impact of new datasets in the fit
{\it a posteriori}
with the Bayesian reweighting formalism.\\[-0.3cm]

In comparison with~\cite{Hartland:2019bjb}, several improvements
have been implemented to increase the efficiency and accuracy of the MCfit procedure 
used in this analysis:

\paragraph{Optimisation.}
%
In the top quark sector analysis of~\cite{Hartland:2019bjb}, the minimisation of 
Eq.~\eqref{eq:chi2definition} was achieved by a gradient descent method which relies 
on local variations of the error function.
%
This choice is advantageous since $E^{(k)}$ is at most a quartic form
of the fit parameters, see Eq.~(\ref{eq:quartic-chi2}) and its generalisation
to multiple operators, and therefore evaluating its gradient is computationally efficient.

Since in the present analysis our parameter space is more complex, the optimiser that we use now to 
determine the best-fit values of the degrees of freedom ${\boldsymbol c}^{(k)}$ within MCfit is a 
trust-region algorithm {\tt trust-constr} available in the {\tt SciPy} package. 
%
An advantage of using {\tt trust-constr} in this context is that it allows 
one to provide the optimiser with any combination of constraints on the 
coefficients, including existing bounds.
%
This is a rather useful feature, since in many cases of interest one would like to restrict
the EFT parameter space based on theoretical considerations, such as when
accounting for the LEP EWPOs or in the top-philic scenario discussed in Sect.~\ref{sec:smefttheory}.

\paragraph{Initial sampling range and bounds.}
%
For each MC replica fit, the initial values of the fit coefficients ${\boldsymbol c}^{(k)}$
are initialised at random within a pre-defined range.
%
This sampling range, as well as the boundaries imposed on the minimisation procedure for the poorly constrained
parameters,
are taken to be the same as those used in the NC procedure.
%
That is, the sampling ranges for the global fits are derived from a one-parameter
$\chi^2$ scanning procedure subsequently
inflated to cover a sufficiently large parameter hyper-volume. 

\paragraph{Cross-validation.}
%
Given the large dimensionality of the considered EFT parameter space, it is conceivable
that the optimiser algorithm ends up fitting the statistical fluctuations 
of the experimental data rather than the underlying physical law.
%
One way to prevent the minimiser from over-fitting the data is to use
look-back cross-validation stopping.
%
In this method, each replica dataset is randomly split with equal probability into two 
disjoint sets, known as the training and validation sets.
%
Only the data points in the training set are then used to compute the figure of 
merit being minimised, Eq.~(\ref{eq:chi2definition}), while the data points in 
the validation set are monitored alongside the fit.
%
The random assignment of the data points to the training or validation sets
is different for each MC replica, and the splitting only occurs for experiments
that contain more than 5 bins in the distribution. 
%
The fit is run for a fixed large number of iterations, and then
the optimal stopping point of 
the fit is then determined as the iteration for which the figure of merit evaluated on the validation set, 
$E^{(k)}_{\rm val}$, exhibits a global minimum.
%
All in all, it is found that the risk of over-fitting is small and that
MCfit results with and without cross-validation applied are reasonably similar.

\paragraph{Quality selection criteria.}
%
One disadvantage of optimisation strategies such as MCfit is that as the parameter space
space is increased, the minimiser might sometimes converge on a local,
rather than on the global, minimum.
%
This is specially problematic in the quadratic EFT fits which often display
quasi-degenerate minima, as illustrated by the $\chi^2$ profiles of
Figs.~\ref{fig:quartic-individual-fits} 
and~\ref{fig:quartic-individual-fits-2}.
%
For this reason, it is important to implement post-fit quality selection criteria
that indicate when a fitted replica should be kept and when it should be discarded.
%
Here, a MC
replica is kept if the total error function of the replica dataset, $E_{\rm tot}^{(k)}$, satisfies
$E_{\rm tot}^{(k)}\le 3$.

\paragraph{Benchmarking.}
%
Fig.~\ref{fig:smefit-mcfit-vs-ns-global} compares
the outcome of global fits obtained with either the NS or MCfit method,
all other settings identical.
%
Specifically, here we show the best-fit values and 95\% CL intervals for global fits
based on linear EFT
calculations.
%
We provide the results corresponding to the 50 coefficients listed in Table~\ref{tab:operatorbasis}
(except for $c_{\ell\ell}$,  which is set to zero by the EWPOs)
of which 36 are independent fit parameters.
%
We will further discuss the physical interpretation of these
results in Sect.~\ref{sec:results},
here we only aim to establish that the two methods indeed lead to equivalent results.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure}[t]
  \begin{center}
    \includegraphics[width=0.86\linewidth]{plots_v2/Coeffs_Central_NS_vs_MCfit.pdf}
    \includegraphics[width=0.86\linewidth]{plots_v2/Coeffs_Bar_NS_vs_MCfit.pdf}
    \vspace{-0.27cm}
    \caption{\small The best-fit values and 95\% CL intervals for a global fit based on linear EFT
      calculations, comparing the outcome of the NS and MCfit methods.
      %
      We display the results corresponding to the 50 coefficients listed in Table~\ref{tab:operatorbasis}
      (except for $c_{\ell\ell}=0$), of which 36 are independent fit parameters.
      %
      {  The bottom panel displays
        the magnitude of the  95\% CL intervals.}
     \label{fig:smefit-mcfit-vs-ns-global} }
  \end{center}
\end{figure}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

The comparison of Fig.~\ref{fig:smefit-mcfit-vs-ns-global} demonstrates that in general
the two methods are in excellent agreement, both in terms of best-fit values
and of the corresponding uncertainties.
%
This said, for specific coefficients one observes small differences, with MCfit in general
tending to provide somewhat looser bounds.
%
The reason for this behaviour is that optimisation-based
methods such as MCfit can be distorted by fitting inefficiencies,
such as when the optimiser finds a local, rather
than global, minimum.
%
This phenomenon is further illustrated in Fig.~\ref{fig:chi2dist_mcfit_vs_ns}, which compares
the $\chi^2$ distributions evaluated over replicas
and posterior samples in the MCfit
and NC methods respectively.
%
We observe that the MCfit distribution exhibits broader tails, implying that the
 bounds obtained this way might
in some cases be slightly over-conservative.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure}[t]
  \begin{center}
    \includegraphics[width=0.80\linewidth]{plots_v2/Chi2_Hist_NS_vs_MCfit.pdf}
    \vspace{-0.3cm}
    \caption{\small Comparison of the $\chi^2$ distributions evaluated over replicas
      and posterior samples in global linear EFT fits based on the MCfit
      and NC methods, respectively.
      %
      The corresponding 95\% CL intervals on the EFT coefficients are
      displayed in Fig.~\ref{fig:smefit-mcfit-vs-ns-global}.
     \label{fig:chi2dist_mcfit_vs_ns} }
  \end{center}
\end{figure}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Fig.~\ref{fig:smefit-mcfit-vs-ns-global}, as well as the corresponding
benchmark comparison for fits based on quadratic EFT calculations, demonstrates
that results obtained with either NS or MCfit are statistically equivalent.
%
In the rest of this work, we will adopt NS as the baseline method,
since its not affected by potential inefficiencies in the minimisation procedure
and, as discussed above, can produce global fits within
a reasonable  execution time.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Principal Component Analysis}
\label{sec:pca}

Principal Component Analysis (PCA) represents a valuable tool to identify the combinations of
degrees of freedom that exhibit the largest and smallest variabilities in a linear algebra problem.
%
This identification has many applications,
for instance, a large gap in variability suggests that the effective dimensionality
of the problem is smaller than the nominal one, and thus dimensional reduction methods
are advantageous to simplify the solution.
%
Furthermore, directions in the parameter space with very small variability are difficult
to constrain from data and are identified with flat directions.
%
Such flat directions might compromise the reliability of the
obtained results i.e. in Hessian EFT fits.\footnote{The PCA method can also be exploited to
  efficiently carry out linear SMEFT fits~\cite{Bodwin:2019ivc}.}

Here we apply the PCA technique combined with
Singular Value Decomposition (SVD) to global fits based on linear
EFT calculations.
%
The goal is to ascertain the presence of possible flat directions,
identify  large gaps in variability
between the principal components, and determine the relation between the physical
fitting basis and these principal components.
%
The starting point is the expression for the cross-section
as a function of the EFT coefficients, Eq.~(\ref{eq:quadraticTHform}), truncated
at the linear order,
\be
\label{eq:linearTHform}
\sigma_m^{\rm (th)}({\boldsymbol c})= \sigma_m^{\rm (sm)} + \sum_{i=1}^{n_{\rm op}}c_i\sigma^{(\rm eft)}_{m,i}\, ,
\qquad m =1\,\ldots, n_{\rm dat} \, ,
\ee
where recall that we have set $\Lambda=1$ TeV.
%
We then define a matrix $K$ of dimensions $n_{\rm dat}\times~n_{\rm op}$ and
(dimensionless) components $K_{mi}=\sigma^{(\rm eft)}_{m,i}/\delta_{{\rm exp},m}$,
where $\delta_{\rm exp,m}$ is the same total experimental error that appears
in the evaluation of the Fisher information matrix Eq.~(\ref{eq:fisherinformation}).
%
By means of SVD, we can decompose this matrix $K$ as
\be
\label{eq:SVD}
K = U W V^\dagger \, ,
\ee
where $U~(V)$ is a $n_{\rm dat} \times n_{\rm dat}$~($n_{\rm op} \times n_{\rm op}$) unitary matrix and $W$ is an $n_{\rm dat}\times n_{\rm op}$ diagonal
matrix with semi-positive real entries, called the singular values, which are ordered by decreasing
magnitude.
%
The larger a singular value, the higher the variability of its principal component
and the higher the likelihood that this component will be well constrained from the fit.

The elements of the symmetric matrix $V$ in Eq.~(\ref{eq:SVD}) contain the principal components associated
to each of the $n_{\rm op}$ singular values.
%
These correspond to a linear superposition of the original coefficients, that is, we have that
\be
\label{eq:PCdef}
{\rm PC}_k = \sum_{i=1}^{n_{\rm op}} a_{ki}c_i \, , \quad k=1,\ldots,n_{\rm op} \, ,\qquad \lp~ \sum_{i=1}^{n_{\rm op}} a_{ki}^2=1\,~\forall k \rp
\ee
where the larger the value of the squared coefficient $a^2_{kl}$, the larger the relative weight
of the associated EFT coefficient in this specific (normalised) principal component.
%
By means of the matrix $V$ (and its inverse), one can rotate between
the original fitting basis and the one defined by the principal components.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure}[t]
  \begin{center}
    \includegraphics[width=0.95\linewidth]{plots_v2/PCA_SVs.pdf}
\caption{\small The distribution of singular values $\lambda_i$ (the elements of the diagonal of the
  matrix $W$), for the principal
  components evaluated for the global fit settings
 summarised in Tables~\ref{tab:operatorbasis} and~\ref{eq:table_dataset_overview}.
  %
  In the linear EFT approximation where the PCA analysis is carried out, there exist three flat
  directions (with vanishing singular values) associated to the four-heavy operators.
  \label{fig:PCA_SVs}
  }
  \end{center}
\end{figure}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Fig.~\ref{fig:PCA_SVs} displays the singular values $\lambda_i$, that is,
the elements of the diagonal
matrix $W$ in the decomposition of Eq.~(\ref{eq:SVD}), for the $n_{\rm op}=36$ principal
components associated to the global fit settings
 summarised in Tables~\ref{tab:operatorbasis} and~\ref{eq:table_dataset_overview}.
%
From the definition of the matrix $K$, a singular value $\lambda_i\simeq 1$
corresponds to a direction in the parameter space where the magnitude of the (linear) EFT corrections
is of the same size as the associated experimental uncertainties.
%
We observe that there are three flat directions (principal components with vanishing singular value),
which as shown below can be associated to linear combinations of the four-heavy operators.
%
Except for these three flat directions, there are no large hierarchies in the distribution
of singular values, indicating that the physical dimensionality of our problem coincides
with that of the chosen fitting basis.
%
The principal component with the highest singular value is dominated by the bosonic
operator $c_{\varphi D}$, which modifies
the Higgs-gauge interactions and  is well constrained by the EWPOs.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure}[t]
  \begin{center}
    \includegraphics[width=0.83\linewidth]{plots_v2/PCA_PCs.pdf}
 \caption{\small Heat map displaying
  the values of the squared coefficients $a_{ki}^2$ that relate the original
  fitting basis to the principal components, Eq.~(\ref{eq:PCdef}), whose
  associated singular values were reported in  Fig.~\ref{fig:PCA_SVs}.
  %
  For the entries with $a_{ki}^2 \ge 0.1$ we also indicate the corresponding numerical value.
  \label{fig:PCA_PCs}
  }
  \end{center}
\end{figure}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Then  Fig.~\ref{fig:PCA_PCs} displays a heat map with
the values of the (squared) coefficients $a_{ki}^2$ that relate the original
fitting basis to the principal components, Eq.~(\ref{eq:PCdef}), and whose
associated eigenvalues are displayed in the upper panel.
%
For those entries with
$a_{ki}^2 \ge 0.1$ we also indicate the corresponding numerical value.
%
Since the principal components are normalised, the sum
of the entries associated to a given row in the heat map adds up to unity.
%
Note also that in this table we have chosen the purely bosonic coefficients
$c_{\varphi W B}$ and $c_{\varphi D}$ to represent the two directions that are left
unconstrained by the EWPOs, see the discussion in Sect.~\ref{sec:operatorbasis}.

From Fig.~\ref{fig:PCA_PCs} one can observe
that some principal components (PC$_k$) are dominated by a single
EFT coefficient from the fitting basis.
%
Examples of this are $c_{\varphi D}$ (for $k=1$), $c_{\varphi G}$ ($k=2$), $c_{\varphi B}$ ($k=3$),
$c_{t G }$ ($k=4$), and $c_{Qq}^{3,1}$ ($k=7$).
%
These PCs have associated reasonably large singular values, $\lambda_k\gsim 10$,
and therefore one expects that the corresponding coefficients will be well constrained
from the fit.
%
Other principal components are instead composed by a superposition
of two or at most three coefficients, for instance $c_{\varphi W B}$ and $c_{\varphi Q}^{(3)}$ are combined
into PC$_{14}$ and PC$_{15}$ with similar weight each.
%
On the other hand, several PCs arise instead from the combination of a large number of EFT coefficients
without any of them dominating.
%
This is the case e.g. for PCs associated to combinations involving
of the two-light-two-heavy operators, such $k=22, 23$ and 25, where
no single squared coefficient $a_{ki}^2$ is larger than 0.4.

The three flat directions (vanishing singular values) observed in
Fig.~\ref{fig:PCA_SVs} can be traced back to linear combinations
for four-heavy operators, specifically to the following combinations:
\bea
    {\rm PC}_{34} &=& 0.91 c_{QQ}^{1} - 0.42 c_{tt}^1 \, , \nonumber \\
    {\rm PC}_{35} &=& 0.62 c_{Qt}^{1} - 0.56 c_{Qt}^{8} + 0.49 c_{QQ}^8 \, , \\
    {\rm PC}_{36} &=& 0.78 c_{Qt}^1 -0.50 c_{QQ}^{8}+0.38 c_{tQ}^8 \, ,\nonumber
    \eea
    where we don't indicate the contributions with $a_{ki}^2<0.1$.
    %
    This implies that, in a linear EFT fit, one can only constrain two directions out of the five
four-heavy operators considered.
%
These flat directions disappear only once we consider the
 quadratic corrections to the $t\bar{t}t\bar{t}$
 and  $t\bar{t}b\bar{b}$ cross-sections.

The results of this PCA indicate that
our choice of fitting basis,
summarised in Table~\ref{tab:operatorbasis}, represents a sensible option
for which well-defined constraints will be obtained from the fit,
up to the previous caveat concerning the four-heavy operators.
%
Therefore, in our case there is no advantage in carrying out
the fit in the rotated basis spanned by the principal components
 Eq.~(\ref{eq:PCdef}) rather than in the original one.
%
Furthermore, the lack of large hierarchies in the distribution of singular values
reflects the fact that the true dimensionality of the problem coincides with that
of the original basis.

While for the global dataset genuine flat directions are either absent or removed by quadratic
corrections, this might not be in general the case if we consider
fits to reduced datasets.
%
In such scenario, one could consider deploying the PCA method to reduce the dimensionality
of the EFT parameter space by removing the directions with singular values below
some threshold before inverting back to the physical basis.
%
We note that a similar strategy has been successfully applied to
construct compressed Hessian PDF sets in~\cite{Carrazza:2015aoa,Carrazza:2016htc}.
%
However, in this work we use PCA as a diagnosis tool to guide
the selection of the fitting basis,
and postpone to future work its application to carry out EFT fits
in the PC rotated basis.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


