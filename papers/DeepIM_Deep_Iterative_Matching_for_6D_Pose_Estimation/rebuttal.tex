% LaTeX rebuttal letter example. 
% 
% Copyright 2019 Friedemann Zenke, fzenke.net
%
% Based on examples by Dirk Eddelbuettel, Fran and others from 
% https://tex.stackexchange.com/questions/2317/latex-style-or-macro-for-detailed-response-to-referee-report
% 
% Licensed under cc by-sa 3.0 with attribution required.

\documentclass[11pt]{article}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{amssymb} % define this before the line numbering.
\usepackage{gensymb}
\usepackage{fullpage}

% import Eq and Section references from the main manuscript where needed
% \usepackage{xr}
% \externaldocument{manuscript}

% package needed for optional arguments
\usepackage{xifthen}
\usepackage{color}
% define counters for reviewers and their points
\newcounter{reviewer}
\setcounter{reviewer}{0}
\newcounter{point}[reviewer]
\setcounter{point}{0}

% This refines the format of how the reviewer/point reference will appear.
\renewcommand{\thepoint}{P\,\thereviewer.\arabic{point}} 

% command declarations for reviewer points and our responses
\newcommand{\reviewersection}{\stepcounter{reviewer} \bigskip \hrule
                  \section*{Reviewer \thereviewer}}

\newenvironment{point}
   {\refstepcounter{point} \bigskip \noindent {\textbf{Reviewer~Point~\thepoint} } ---\ }
   {\par }

\newcommand{\shortpoint}[1]{\refstepcounter{point}  \bigskip \noindent 
    {\textbf{Reviewer~Point~\thepoint} } ---~#1\par }

\newenvironment{reply}
   {\medskip \noindent \textbf{Reply}:\  }
   {\medskip }

\newcommand{\shortreply}[2][]{\medskip \noindent \textbf{Reply}:\  #2
    \ifthenelse{\equal{#1}{}}{}{ \hfill \footnotesize (#1)}%
    \medskip }
\newcommand{\gu}[1]{\textcolor{blue}{Gu:#1}}

\begin{document}

\section*{Response to the reviewers}
% General intro text goes here
We thank the reviewers for their critical assessment of our work. In the following, we address their concerns point by point. 

% Let's start point-by-point with Reviewer 1
\reviewersection

% Point one description 
\begin{point}
    \label{pt:1-1}
The SE (3) transformation should be briefly introduced and explained as for an inexperienced reader is quite difficult to follow and understand.
\end{point}

% Our reply
\begin{reply}
We agree that the term SE (3) transformation is not friendly to readers who are not experts in this field, so we added a brief description in the introduction that they can just treat SE (3) transformation as rotation and translation.
\end{reply}

\begin{point}
    \label{pt:1-2}
    The caption of the figures (particularly Figs. 1, 2, 3) should have a larger caption describing the content of each figure.
\end{point}

\begin{reply}
Thanks to your suggestion and we have added more description in the caption of Fig 1, 2, 3 and 12.
\end{reply}

\begin{point}
    \label{pt:1-3}
    All the mathematical notations and terminology should be properly defined and introduced. Also, they should specify all the domains for each
variable and function.
\end{point}

\begin{reply}
Good idea! We have modified mathematical notations that may cause confusion ($\mathbf{x}_{\text{obs}}$ to $\mathbf{img}_{\text{obs}}$, $(n \degree, n~\text{cm})$ to $(k \degree, k~\text{cm})$ so that every notation is unique and share the same meaning through the paper. Explanation is also added that why we use $n$ to denote the number of points in point matching loss while $m$ in metric.
\end{reply}

% Use the short-hand macros for one-liners.
\shortpoint{ Minor typos : page 6 - line 35 - second col, page 7 - line 50 - second col. }
\shortreply{ Fixed.}

\begin{point}
    \label{pt:1-4}
The authors should include a bigger explanation behind the decision of using another head for the model, the one which the optical flow and
the foreground mask. They should argument
more on why they decided to take this approach, and include an extra experimental setup which enforced this decision.
\end{point}

\begin{reply}
Thanks for your suggestion. We have added a new table comparing the performance (mean and standard deviation) of the design of auxiliary branches predicting mask and flow. We train with each configuration 5 times to remove the impact of noise. Networks are trained on one category (ape) of the LINEMOD dataset. It shows that with these two branches, the network can improve its performance.
\end{reply}

% Begin a new reviewer section
\reviewersection

\begin{point}
    \label{pt:2-1}
    Missing reference: DeepVO [1] and ICSTN [2].
\end{point}

\begin{reply}
Thanks for your noticing. These are now mentioned and discussed in the paper.
\end{reply}

\begin{point}
    \label{pt:2-2}
    The main issue I see with this paper is that the introduction uses textureless objects as a point of motivating the approach, the only time textureless objects is mentioned later in the paper is briefly as a failure case contradicting the motivation. Given such an emphasis on this issue, I was disappointed to not see a more insightful discussion on this aspect. Adding more details on this would strengthen this paper adding more value compared to what is available in the ECCV version.
\end{point}

\begin{reply}
Thanks for your suggestion of spending more pages discussing the performance over textureless objects. In fact, The DeepIM could do well on textureless objects. For example, the objects in LINEMOD dataset have little texture that traditional local feature-based method like those based on SIFT will have difficulty to deal with. DeepIM's success on the LINEMOD dataset proved its ability on textureless objects. On the YCB-video dataset, DeepIM can also perform well on textureless objects like the bowl when the initial poses are not too bad. We now believe that the failure on scissors and large markers are caused by their unnatural shape rather than because they are textureless. 
\end{reply}

\begin{point}
Minor issues:
Closet $\rightarrow$ closest (search for closet)
Variance $\rightarrow$ standard deviation (page 10, L45)
Inconsistent notation, Equation 4 has $\ell_1$ while equation 5-7 use bar norm notation for Euclidean distance.
Introduction uses "textureless" while the rest of the paper uses hyphenation "texture-less
\end{point}

\begin{reply}
Fixed. Thanks for your carefulness.
\end{reply}

[1] Wang et al. ``DeepVO: Towards End-to-End Visual Odometry with Deep Recurrent
Convolutional Neural Networks'' ICRA 2017

[2] Lin CH, Lucey S. ``Inverse compositional spatial transformer networks'' CVPR 2017
\end{document}