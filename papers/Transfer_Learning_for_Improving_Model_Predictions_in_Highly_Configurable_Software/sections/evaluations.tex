%!TEX root = ../paper.tex

\section{Experimental results}
\label{sec:evaluation}

We evaluate the \emph{effectiveness} and \emph{applicability} of our transfer learning approach for learning models for highly-configurable systems, in particular, compared to conventional non-transfer learning. Specifically, we aim to answer the following three research questions:

{\noindent \em \textbf{RQ1}: How much does transfer learning improve the prediction accuracy?} %What is the difference in terms of range of prediction errors we get with our transfer learning approach comparing with no transfer learning?}

{\noindent \em \textbf{RQ2}: What are the trade-offs between learning with different numbers of samples from source and target in terms of prediction accuracy?}

{\noindent \em \textbf{RQ3}: Is our transfer learning approach applicable in the context of self-adaptive systems in terms of model training and evaluation time?}

We will proceed as follows. First, we will return to our motivating example of an autonomous service robot to explore
the benefits and limitations of transfer learning for this single case. Subsequently, we
explore the research questions quantitatively, performing experiments on 5 different configurable systems.
We have implemented our cost-aware transfer learning approach in Matlab 2016b and, depending on each experiment, we have developed different scripts for data collections and automated configuration changes. The source code and data are available in an online appendix~\cite{onlineappendix}.


%We first start the evaluation with a robotics system where the configuration space that we have considered is 2-dimensional. We present this as a case study in which we explore the feasibility and preparation of it. We then systematically approach the evaluation by controlling the sampling from both source and target configurations. We have conducted three systematic experiments to (i) evaluate the prediction error range comparing with the no transfer learning (RQ1) and (ii) to construct a Pareto front to discuss the trade-off between measurement costs and model accuracy (RQ2). In a third experiment, we assess the feasibility of our approach for self-adaptive systems discussing the training and testing efforts for model constructions and model update at runtime (RQ3).



\subsection{Case study: Robotics software}
\label{sec:case-study}
We start by demonstrating our approach with a small case
study of service robots, which we already motivated in Section~\ref{sec:example}.
Our long-term goal is to allow the service robots to adapt
effectively to (possibly unanticipated) changes in the environment, in
its goals, or in other parts of the system, among others, by reconfiguring
parameters. Specifically, we work with the CoBot robotic platform~\cite{veloso2015cobots}.

To enable a more focused analysis and presentation, we focus
on a single but important subsystem and two parameters:
The particle-filter-based autonomous localization component of the CoBot software~\cite{biswas2013localization}
determines the current location of the robot, which is essential for navigation.
Among the many parameters of the component, we analyze two that strongly
influence the accuracy of the localization and required computation effort,
(1)~the number of particle estimates and (2)~the number of gradient descent refinement
steps applied to each of these particles when we receive a new update from sensors.
At runtime, the robot could decide to reconfigure its localization
component to trade off location accuracy with energy consumption,
based on the model we learn.

We have extensively measured the performance of the localization
component with different parameters and with different simulated
environmental conditions in prior work~\cite{kawthekarsensitivity}.
For the two parameters of interest (number of particles and refinements), we used 25 and 27 different values
each, evenly distributed on the log scale within their ranges ($[5-10,000]$ and $[1-10,000]$),
\emph{i.e.}, $|\mathbb{X}|=25\times 27=675$.
Specifically, we have repeatedly executed a specific mission to navigate along
a corridor in the simulator and measured performance in terms of CPU
usage (a proxy for energy consumption), time to completion, and location accuracy.
Each measurement takes about 30 seconds. In Figure~\ref{fig:example}a, we illustrate how CPU usage depends on those parameters
(the white area represents configurations in which the mission fails to complete).

As a transfer scenario, we consider simulation results given
different environment conditions.
As a target, we consider measurements that we have performed in the
default configuration.
As a source, we consider more challenging environment conditions,
in which the odometry sensor of the robot is less reliable
(\emph{e.g.}, due to an unfamiliar or slippery surface); we simulate
that the odometry sensor is both miscalibrated ($30\,\%$) and noisy ($\pm45\,\%$). We assume that we have collected these measurements
in the past or offline in a simulator and that those measurements
were therefore relatively cheap.
As shown in Figure~\ref{fig:example}b, localization becomes
more computationally expensive in the target environment.
Our goal is to use (already available) data from the noisy environment for
learning predictions in the non-noisy target environment with only a few
measurements in that environment.


%We have empirically observed that the cost of taking samples is approximately 40 times higher than simulator samples. Let us now assume for obtaining a source (target) sample the following cost incurred $c_s=0.3,c_t=12$ and we consider an experimental budget of $\mathcal{C}_{max}=250$.


\paragraph*{Observations}
For both source and target environments, we can learn fairly accurate
models if we take large numbers of samples (say $|\mathcal{D}|>80\,\%\times |\mathbb{X}|$) in that environment, but predictions
are poor when we can sample only a few configurations (say $|\mathcal{D}|<1\,\%\times |\mathbb{X}|$).
That is, we need at least a moderate number of measurements to
produce useful models for making self-adaptation decisions at runtime.
Also, using the model learned exclusively from measurements in the
source environment to predict the performance of the robot in the
more noisy environment leads to a significant prediction error
throughout the entire configuration space (cf. Figure \ref{fig:example}c).
However, transfer learning can effectively calibrate a model
learned from cheap measurements in the source environment to the
target environment with only a few additional measurements
from that environment. For example, Figure \ref{fig:example}d shows the predictions provided by a model that has been learned by transfer learning using 18 samples from the source and only 4 samples from the target environment. As a result, the model learned the structure of the response properly in terms of changing of values over the configuration space.

In Figure \ref{fig:pareto-cobot-mean-variance}a, we illustrate how well we can predict the target
environment with a different number of measurements in the source
and the target environment. More specifically, we randomly sampled configurations from both source and target between $[0-100\%]$ and $[0-10\%]$ respectively and we measured the accuracy as the relative error comparing predicted to actual performance in all 675 configurations. Note $|\mathcal{D}_s| = 0$ corresponds to model learning without transfer learning.
%We illustrated average and variance of prediction error over 3 repetitions with random samples in both environments in Figure \ref{fig:pareto-cobot-mean-variance}a,b respectively.
We can see that, in this case, predictions can be similarly accurate when we replace
expensive measurements from the target environment
by more, but cheaper samples.
Moreover, we have noticed that the models learned without transfer learning are highly sensitive to the sample selection (cf. Figure \ref{fig:pareto-cobot-mean-variance}b). The variance becomes smaller when we increase the number of source samples (along with the y-axis) leading to more reliable models.
In this case study, all model learning and evaluation times (both for normal learning and transfer learning) are negligible ($<5s$), cf. Figure \ref{fig:pareto-cobot-mean-variance}c,d.


\begin{figure}[t]
	\begin{center}
		\includegraphics[width=\columnwidth]{figures/tl-example2}
		\caption{Illustration of transfer learning: (a) a source response function from which we can take samples; (b) a target response function which we are interested to learn; (c) a prediction without transfer learning; (d) a prediction with transfer learning.}
		\label{fig:example}
	\end{center}
\end{figure}

\begin{figure}[t]
	\begin{center}
		\includegraphics[width=\columnwidth]{figures/pareto-cobot-rr-all}
		\caption{(a) Prediction error of the trained model, (b) variance of predictions (transfer learning contributes to lower the uncertainty regarding model predictions, therefore, more reliable learning), (c) training and (d) evaluation overhead.}
		\label{fig:pareto-cobot-mean-variance}
	\end{center}
\end{figure}



% CK: as agreed, let's remove learning from multiple sources from the paper, just complicates things without adding much here

% In Section \ref{sec:tl}, we have claimed that we can get samples from more than one source. This usually leads to more accurate predictions, because the model can learn from different parts of the configuration space using different related sources. In order to evaluate this claim, we have used two different sources by getting samples from CoBot simulator under two different values for parameter $odometry\_micalibration=0.3,0.2$)  and we have taken 18 samples from both two sources. Figure \ref{fig:prediction-error} shows the absolute percentage errors for all 675 possible configurations. The results demonstrate that the model trained using two different sources provides more accurate predictions comparing with the one using only one other source.

% \begin{figure}[h]
% 	\begin{center}
% 		\includegraphics[width=0.6\columnwidth]{figures/prediction_error_gp-t2-t3}
% 		\caption{Prediction accuracy of models without and with transfer learning. GP is the regression model without transfer learning correspond to Figure \ref{fig:example}(c); TL(T=2) is the accuracy corresponding to Figure \ref{fig:example}(d); and TL(T=3) is the accuracy of the model we trained by adding yet another source.}
% 		\label{fig:prediction-error}
% 	\end{center}
% \end{figure}


% We have also evaluated the prediction accuracy using different number of samples from the simulator. That is, we vary $\mathcal{D}_s$. The results in Figure \ref{fig:prediction_performance_source_percentage} show that as we increase $|\mathcal{D}_s|$ the model becomes more accurate, leveraging the exploration on the source in order to exploit the gained knowledge to learn the target response better. The high standard deviation around the average with less than 20\% of data is the result of random selection of training data from the source space. This is an important observation as it shows it is important where to sample in the source space. So if the locations are far from locations of samples from the target space, the performance may deteriorate.

% \begin{figure}[t]
% 	\begin{center}
% 		\includegraphics[width=0.6\columnwidth]{figures/predict_perf_ape}
% 		\caption{Prediction accuracy of the model trained with different numbers of source data size.}
% 		\label{fig:prediction_performance_source_percentage}
% 	\end{center}
% \end{figure}

\paragraph*{Relatedness}
In this case study, we can also observe that the relatedness between the source and target matters by changing the environments more or less aggressively.
For that purpose, we simulated six alternative source environments by adding noise with different power levels to the source we considered previously (\emph{i.e.}, odometry miscalibration of $30\,\%$ and noise of $\pm45\,\%$, cf., Figure~\ref{fig:sensitivity_corr_boxplot}). Typically,
the stronger the target environment is changed from the source (default) environment, the larger
the difference in performance is observed for the same configuration between the
two environments. This relationship is visible in correlation measures in
Figure~\ref{fig:sensitivity_corr_boxplot}). The prediction error reported in Figure \ref{fig:sensitivity_corr_boxplot} indicates that transfer learning becomes more effective
(\emph{i.e.}, produces accurate models with a few samples from the target environment)
if measurements in the source and target are strongly correlated. This also demonstrates that even transfer from a response with a small correlation helps to learn a better model comparing with no transfer learning.

Based on this observation, we need to decide (i) from which alternative source and (ii) how many samples we transfer to derive an accurate model. In general, we can: either (a) select the most related source and sample only from that source, or (b) select fewer samples from unrelated sources and more from the more related ones to transfer.
Note that our transfer learning method supports both strategies. For this purpose, we use the concept of relatedness (correlation) between source and target. The former is simpler, but we need to know, a priori, the level of relatedness of different sources. The latter is more sophisticated using a probabilistic interpretation of relatedness to learn from different sources using a concept of distance from the target. Our method supports this through the kernel function in Eq. \ref{eq:mt-kernel} that embeds the knowledge of source correlations and exploits that knowledge in the model learning process. The selection of a specific strategy depends on several factors including: (i) the cost of samples from each specific source (samples from some sources may come for free, \emph{i.e.} $c_s=0$ and we may want to use more samples from this source despite the fact that it may be less relevant), (ii) the bound on runtime overhead (more samples leads to a higher learning overhead), and (iii) domain for which the model is used (some domain are critical and some adversaries may manipulate the input data exploiting specific vulnerabilities of model learning to compromise the whole system security \cite{papernot2016towards}). We leave the investigation of strategy selection as a future work.



\begin{figure}[t]
	\begin{center}
		\includegraphics[width=0.7\columnwidth]{figures/boxplot_APs}
		%\begin{table}{Example Table (floating)}
			%\caption{Sources with different levels of relatedness.}
			\resizebox{\columnwidth}{!}{%
				\begin{threeparttable}
					\begin{tabular}{lccccccc}
						\toprule
						Sources & $s$ & $s_1$ & $s_2$ & $s_3$ & $s_4$ & $s_5$ & $s_6$\\
						\midrule
						noise-level & $0$ & $5$ & $10$ & $15$ & $20$ & $25$ & $30$ \\
						corr. coeff. & $0.98$ & $0.95$ & $0.89$ & $0.75$ & $0.54$ & $0.34$ & $0.19$ \\
						$\mu(pe)$ & $15.34$ & $14.14$ & $17.09$ & $18.71$ & $33.06$ & $40.93$ & $46.75$ \\
						\bottomrule
					\end{tabular}
				\end{threeparttable}}
				\caption{Prediction accuracy of the model learned with samples from different sources of different relatedness to the target. GP is the model without transfer learning.}
				\label{fig:sensitivity_corr_boxplot}
			%\end{table}
	\end{center}
\end{figure}



%\begin{figure}[h]
%	\begin{center}
%		\includegraphics[width=0.8\columnwidth]{figures/pareto-sps-effort-accuracy-annotated1}
%		\caption{Pareto optimal solutions for an stream processing system.}
%		\label{fig:pareto-sps-effort-accuracy}
%	\end{center}
%\end{figure}



% Finally, we have investigated the situation in which we have the choice of selecting samples from alternative sources of different relatedness to the target domain. For instance, we may have access to robotic simulations under different environmental uncertainties (\emph{e.g.} Odometry noise). This is especially useful in robotics domain where the simulators can provide measurement data that impose different cost and comes with different accuracy. In order to evaluate this situation, we have used the original source and have added different noise levels to simulate 6 different sources to control the correlation coefficients as it is listed in Table \ref{tab:sources}. Note that $s$ is the original source and $s_1-s_6$ are the sources that come with different correlations from high, $s_1$, to low, $s_6$. We have investigated the performance of the model trained using these different sources. Figure \ref{fig:sensitivity_corr_predicted_mtgp} shows that as the source becomes less relevant, the respective model, trained using the transfer learning using the samples from that source, becomes less accurate and cannot get the structure in the response surface correctly. The results in Figure \ref{fig:sensitivity_corr_boxplot} and Table \ref{tab:omega-index-synthetic-m} confirm this observation as the response functions becomes unrelated (lower correlation coefficient) the trained model becomes less accurate.


% \begin{figure*}[h]
% 	\begin{center}
% 		\includegraphics[width=\textwidth]{figures/predicted_mtgp}
% 		\caption{Prediction of the target space using source data taken from different response functions from (a) to (f) of decreasing correlation coefficients. As the source becomes less related to the target the prediction power of the model is decreased and the model is not able to get the structure of the target response correctly (cf. Figure \ref{fig:example}(b)).}
% 		\label{fig:sensitivity_corr_predicted_mtgp}
% 	\end{center}
% \end{figure*}




% \begin{table}[h]
% 	\caption{Prediction accuracy of the models trained with different sources.}
% 	\resizebox{\columnwidth}{!}{%
% 		\begin{threeparttable}
% 			\begin{tabular}{lcccccccc}
% 				\toprule
% 				Model & $GP$ & $TL(s)$ & $TL(s_1)$ & $TL(s_2)$ & $TL(s_3)$ & $TL(s_4)$ & $TL(s_5)$ & $TL(s_6)$ \\
% 				\midrule
% 				NMSE & $49.8425$ & $0.1474$ & $0.1545$ & $0.2659$ & $0.3415$ & $1.7242$ & $3.3798$ & $5.7275$ \\
% 				\bottomrule
% 			\end{tabular}
% 		\end{threeparttable}}
% 		\label{tab:omega-index-synthetic-m}
% 	\end{table}




\subsection{Experiment: Prediction accuracy and model reliability}

With our case study, we demonstrated our approach on a specific small example.
To increase both internal and external validity, we perform a more systematic
exploration on multiple different configurable systems.

\paragraph*{Subject systems}
First, we again use the localization component of the CoBot system, but (since we are no longer constrained by plotting results in two dimensions) explore a larger space with 4 parameters.
In addition, we selected highly-configurable systems as subjects that have been explored for
configuration optimization in prior work~\cite{jamshidi2016bo4co}: three stream processing applications on Apache Storm
({\sf \small WordCount, RollingSort, SOL}) and a NoSQL database benchmark system on Apache Cassandra.
{\sf \small WordCount} is a popular benchmark~\cite{ghazal2013bigbench} featuring a three-layer architecture that counts the number of words in the incoming stream and it is essentially a CPU intensive application. {\sf \small RollingSort}  is a memory intensive system that performs rolling counts of incoming messages for identifying trending topics.
{\sf \small SOL} is a network intensive system, where the incoming messages will be routed through a multi-layer network.
%These are standard benchmarks that are widely used in the research community~\cite{ghazal2013bigbench} as well as industry benchmarks~\cite{huang2010hibench}. 
From the practical perspective, these benchmark applications are based on popular big data engines (\emph{e.g.}, Storm) and understanding the performance influence of configuration parameters can have a large impact on the maintenance of modern systems that are based on these highly-configurable engines. The Cassandra measurements was done using scripts that runs YCSB \cite{cooper2010benchmarking} and was originally developed for a prior work \cite{artavc2017dice}.

The notion of source and target set depends on the subject system.
In CoBot, we again simulate the same navigation mission in the default environment (source) and
in a more difficult noisy environment (target).
For the three stream processing applications, source and target
represent different workloads, such that we transfer measurements
from one workload for learning a model for another workload. More specifically, we control the workload using the maximum number of messages which we allow to enter the stream processing architecture.
For the NoSQL application, we analyze two different transfers:
First, we use as a source a query on
a database with 10 million records and as target the same query
on a database with 20 million records, representing a more expensive environment to sample from. Second, we use as a source a query on 20 million records on one cluster and as target a query on the same
dataset run on a different cluster, representing hardware changes.
Overall, our subjects cover different kinds of applications and
different kinds of \emph{transfer scenarios} (changes in the environment,
changes in the workload, changes in the dataset, and changes
in the hardware).

% the changes are along two different directions.
% First, we created a database of two different sizes: in the first, the database was loaded with 10 million records ({\sf cass-10}), while in the second it was loaded with 20 million records ({\sf cass-20}). We have also deployed each of these two versions of the benchmark on two different infrastructure setting leading to a total of 4 different response functions. %Figure~\ref{fig:cass-surf-all} (a,b,c,d) shows the response surfaces for the 4 versions of the NoSQL database where {\sf concurrent\_read} and {\sf concurrent\_write}, as configuration parameters, are varied ({\sf cass-10,20} are of different sizes, while $v_1,v_2$ are different in terms of deployment settings).

\paragraph*{Experimental setup}
As independent variables, we systematically vary the size of the
learning sets from both source and target environment in each
subject system.
We sample between 0 and~100\,\% of all configurations in the source
environment and between 1 and~10\,\% in the
target.

As a dependent variable, we measure learning time and prediction accuracy of the learned model.
For each subject system, we measure a large number of random configurations
as the evaluation set, independently from configurations sampled for learning,
and compare the predictions
of the learned model $\hat{f}$ to the actual measurements of the configurations in the evaluation
set $\mathcal{D}_o$. We compute the \emph{absolute percentage error (APE)}
for each configuration $x$ in the evaluation set $\frac{|\hat{f}(\mathbf{x})-f(\mathbf{x})|}{f(\mathbf{x})}\times 100$
and report the average to characterize the accuracy of the prediction model.
% and the \emph{normalized mean square error (NMSE)} $\frac{\Sigma(\hat{f}(\mathbf{x})-f(\mathbf{x}))^2}{|\mathcal{D}_o|.\sigma(\hat{\mathbf{y}})}$.
% The second accuracy metric is more sensitive to large deviations, i.e., it penalizes models that gives occasional large errors.
Ideally, we would use the whole configuration space as evaluation set ($\mathcal{D}_o=\mathbb{X}$), but the measurement effort would be prohibitively high for most real-world systems~\cite{jamshidi2016bo4co,influence}; hence we use large random samples (cf. size column in Table \ref{tab:configuration-parameters}).

The measured and predicted metric depends on the subject system:
For the CoBot system, we measure average CPU usage during the same mission of navigating along a corridor as in our case study;
we use the average of three simulation runs for each configuration.
For the Storm and NoSQL experiments, we measure average latency over a window of 8 and 10 minutes respectively.
Also, after each sample collection, the experimental testbed was cleaned which required several minutes for the Storm measurements and around 1 hour (for offloading and cleaning the database) for the Cassandra measurements.
We sample a given number of configurations in the source and target
randomly and report average results and standard deviations
of accuracy and learning time over 3 repetitions.





%\begin{figure*}[t]
%	\begin{center}
%		\includegraphics[width=\textwidth]{figures/cass-surfaces-all}
%		\caption{Response functions corresponding to the 2D subspace of {\sf cass-10,20} that are different in terms of \emph{infrastructure}.}
%		\label{fig:cass-surf-all}
%	\end{center}
%\end{figure*}


\begin{table}[t]
	\centering
	\caption{Overview of our experimental datasets. ``Size'' column indicates the number of measurements in the datasets and ``Testbed'' refer to the infrastructure where the measurements are taken and their details are in the appendix.} %Note $pe_{GP},pe_{TL}$ are the average APE resulted by the models trained without and with transfer learning.}
	\label{tab:configuration-parameters}
	\resizebox{0.8\columnwidth}{!}{%
		\begin{threeparttable}
			\begin{tabular}{@{}lllgc@{}}
				\toprule
				&\textbf{Dataset} & \multicolumn{1}{c}{\textbf{Parameters}}                                                                                       & \multicolumn{1}{c}{\textbf{Size}} & \multicolumn{1}{c}{\textbf{Testbed}} %  & \multicolumn{1}{c}{{$pe_{GP}$}} & \multicolumn{1}{c}{{$pe_{TL}$}}
				    \\ \midrule
				1  & {\sf CoBot(4D) }               & \begin{tabular}[c]{@{}l@{}}{\sf 1-odom\_miscalibration,}\\{\sf 2-odom\_noise,}\\{\sf 3-num\_particles,}\\ {\sf 4-num\_refinement}\\ \end{tabular}                      & 56585 & C9  % & 77.12 & 25.74
				\\
				\midrule
				2  & {\sf wc(6D) }               & \begin{tabular}[c]{@{}l@{}}{\sf 1-spouts, 2-max\_spout, }\\ {\sf 3-spout\_wait, 4-splitters,}\\ {\sf 5-counters, 6-netty\_min\_wait} \end{tabular}                      & 2880 & C1   % & 77.45&  0.44
				\\ \midrule
				3  & {\sf sol(6D) }             & \begin{tabular}[c]{@{}l@{}}{\sf 1-spouts, 2-max\_spout,} \\ {\sf 3-top\_level, 4-netty\_min\_wait,} \\ {\sf 5-message\_size, 6-bolts} \end{tabular}                          & 2866 & C2      %   & 89.12 & 0.11
				   \\ \midrule
				4  & {\sf rs(6D) }           & \begin{tabular}[c]{@{}l@{}}{\sf 1-spouts, 2-max\_spout, }\\ {\sf 3-sorters, 4-emit\_freq,}\\ {\sf 5-chunk\_size, 6-message\_size} \end{tabular}                                    & 3840 & C3   %    & 103.12 & 0.21
				  \\ \midrule
				\begin{tabular}[c]{@{}l@{}}5 \\ \\ 6\end{tabular} & \begin{tabular}[c]{@{}l@{}} {\sf cass-10 } \\ \\ {\sf cass-20 }\end{tabular}      & \begin{tabular}[c]{@{}l@{}}{\sf 1-trickle\_fsync, 2-auto\_snapshot,} \\ {\sf 3-con.\_reads, 4-con.\_writes} \\ {\sf 5-file\_cache\_size\_in\_MB}\\ {\sf 6-con.\_compactors} \end{tabular}     & 1024 & C6x,C6y \\ %&56.45 & 0.56  \\
				\bottomrule
			\end{tabular}
		\end{threeparttable}}
	\end{table}




\paragraph*{Results}
We show results of our experiments in Figure~\ref{fig:errors2dallsystems}. The 2D plot shows average errors across all subject systems.
The results in which the set of source samples $\mathcal{D}_s$ is empty represents the baseline case without transfer learning.
%In Figure~\ref{fig:accuracy-all}, we additionally show a specific slice through our accuracy results, in which we only vary the number of samples from the source (and only for 4 subject systems to produce a reasonably clear plot), but keep the number of samples from the target at a constant 1\,\%.
Although the results differ significantly among subject systems (not surprising, given different relatedness of the source and target) the overall trends are consistent.



\begin{figure*}[t]
	\begin{center}
		\includegraphics[width=0.95\textwidth]{figures/aveErr_all}
		\caption{Prediction accuracy for (a) CoBot, (b) {\sf \small WC}, (c) {\sf \small SOL}, (d) {\sf \small RS}, (e) {\sf \small cass} (hardware change), (f) {\sf \small cass} (DB size change).}
		\label{fig:errors2dallsystems}
	\end{center}
\end{figure*}


% \begin{figure}[h]
% 	\begin{center}
% 		\includegraphics[width=0.7\columnwidth]{figures/aveErr_perc_source}
% 		\caption{Average prediction error for the subject systems with a fixed target sample size.}
% 		\label{fig:accuracy-all}
% 	\end{center}
% \end{figure}

First, our results show that transfer learning can achieve high prediction accuracy with only a few samples from the target environment;
that is, transfer learning is clearly beneficial if samples from the source environment are much cheaper than samples from the target environment.
Given the same number of target samples, the prediction accuracy becomes better up to even 3 levels of magnitudes as we include more samples from the source.

Second, using transfer learning with larger samples from the source environment reduces  the variance of the prediction error. That is, models learned with only a few samples from the target environment are more affected by the randomness in sampling from the configuration space. Conversely, transfer learning improves the quality and
reliability of the learned models, by reducing sensitivity to the sample selection.

Third, the model training time increases monotonically when we increase the sample size for target or source (cf. Figure \ref{fig:pareto-cobot-mean-variance}c for the CoBot case study and for the other subject systems in the appendix). At the same time,
even for large samples, it rarely exceeds 100 seconds for our subject systems, which is reasonable for online use in a self-adaptive system. Learning time is negligible compared to measurement times in all our subject systems.

Finally, the time for evaluating the models (\emph{i.e.}, using
the model to make a prediction for a given configuration), does not change
much with models learned from different sample sizes (cf. Figure \ref{fig:pareto-cobot-mean-variance}d) and is negligible overall ($<300$ms).

\begin{shaded}
Summary: we see improved accuracy and reliability
when using transfer learning with additional source samples (RQ1); we
see clear trade-offs among using more source or target samples (RQ2);
and we see that training and evaluation times are acceptable to be applied for self-adaptive systems (RQ3).
\end{shaded}


%\begin{figure}
%	\begin{center}
%		\includegraphics[width=0.7\columnwidth]{figures/training_time_cobot}
%		\caption{Model training time with different source and target samples for the CoBot experiment}
%		\label{fig:training-testing-runtime}
%	\end{center}
%\end{figure}


% \begin{figure}
% 	\begin{center}
% 		\includegraphics[width=\columnwidth]{figures/runtime_all}
% 		\caption{Model learning and evaluation time.}
% 		\label{fig:training-testing-runtime-all}
% 	\end{center}
% \end{figure}






%\begin{figure}
%	\begin{center}
%		\includegraphics[width=0.7\columnwidth]{figures/pareto-sps-annotated1}
%		\caption{Indifference curves for an stream processing system.}
%		\label{fig:pareto-sps}
%	\end{center}
%\end{figure}


%\begin{figure}
%	\begin{center}
%		\includegraphics[width=0.7\columnwidth]{figures/model_reliability}
%		\caption{Variance of prediction error with and without transfer learning and different number of sample sets.}
%		\label{fig:model-reliability}
%	\end{center}
%\end{figure}






\subsection{Discussion: Trade-offs and cost models}
Our experimental results clearly show that we can achieve
accurate models both with a larger number of samples from source
or target environment or both:
In Figure~\ref{fig:errors2dallsystems}, we can see how
many models for different numbers of source and target samples
yield models with similar accuracy.
Those different points with the same accuracy can be interpreted
as indifference curves (a common tool in economics~\cite{binger1988microeconomics}).
We can invest different measurement costs to achieve models with
equivalent utility.
In Figure~\ref{fig:errors2dallsystems}a,
we show those indifference curves more explicitly for the CoBot
system. Using the indifference curves enables us to decide how many samples from either a source or a target we should use for the model learning process that is most beneficial in increasing predictive accuracy over unseen regions of the configuration space while satisfying the budget constraint.
Given concrete costs and budgets, say, samples from the target environment are 3 times
more expensive to gather than samples from the source environment,
we plotted an additional line representing our budget and find the
combination of source and target samples that produces the best
prediction model for our budget (see intersection of the budget
line with the highest indifference curve in Figure~\ref{fig:errors2dallsystems}).
Although we will not know the indifference lines until we run
extensive experiments, we expect that the general trade-offs
are similar across many subject systems and transfer scenarios
and that such a cost model (cf. Eq. \ref{eq:cost-model}) can help to justify decisions
on how many samples to take from each environment.
We leave a more detailed treatment to future work.

In this context, we can fix a cost model (\emph{i.e.}, fixing specific values for $c_s,c_t$) and transform the indifference diagrams into a multi-objective goal and derive the Pareto front solutions as shown in Figure \ref{fig:pareto-cobot-effort-accuracy}. This helps us to locate the feasible Pareto front solutions within the sweet spot.
%In the self-adaptation context, we are usually given specific upper bound for budget and accuracy and we are interested to the solutions that are located below the two thresholds.
%One possible way to exploit this new dimension is to apply a design of experiment method ~\cite{srinivas2009gaussian} to take sample from source and target in a way that we gain more knowledge comparing with the random selection as we used here. We could also take samples in a active learning fashion ~\cite{jamshidi2016bo4co}. We consider these two directions as a future work.

%Using different combinations of source and target samples, we construct a two dimensional indifference curve~\cite{beattie2006law} corresponding to different model accuracy levels. Each point on the indifference curve has the same level of model accuracy but comes with a different cost depending on costs $c_s,c_t$. By exploring the resulting indifference curves, we obtain a set of solutions that satisfy the constrain in Eq. \eqref{eq:cost-constraint}. Figure~\ref{fig:pareto-synthetic} illustrates the indifference curves corresponding to different prediction errors in our synthetic example.
%Using these indifference curves, we are then able to choose a solution that has an expected model accuracy higher than others or has the least prediction error as defined in Eq. \eqref{eq:objective} for different costs $c_s,c_t$, cf. Figure \ref{fig:pareto-synthetic}. In this synthetic experiment, we assume $c_s=3,c_t=10,\mathcal{C}_{max}=30$, the black line indicates budget and a point of maximum utility that satisfies the cost constraint is indicated.
%Using the indifference curves enables us to decide how many samples from either a source or a target we should use for the model learning process that are most beneficial in increasing predictive accuracy over unseen regions of the configuration space while satisfying the budget constraint. For instance, one may wish to spend all the budget by getting 3 samples from the target. The lowest prediction error she will then observe is 40. However, she can use two samples from the target, but transfer 4 samples from the source to reach a much lower error of 10.



% When varying the number of source and target samples systematically
% In Figure~\ref{fig:pareto-cobot}, we show which

% Figure \ref{fig:pareto-cobot} illustrates the indifference curves corresponding to different prediction errors. %Here we assume $c_s=0.3,c_t=12,\mathcal{C}_{max}=250$.
% Different feasible solutions (combinations of source and target samples) can be decided based on the required accuracy and the budget we are willing to spend. We have used the cost model in the CoBot case study and derived the Pareto optimal solutions shown in Figure \ref{fig:pareto-cobot-effort-accuracy}. This helps us to analyze the trade-off among model accuracy and measurement effort. %We then plotted the Pareto optimal solutions in Figure \ref{fig:pareto-cobot} in order to understand where they are located.
% By looking at the indifference curve (Figure \ref{fig:pareto-cobot}), the corresponding Pareto optimal solutions are clustered into two different groups. The first group are those who demand high number of samples from both source and target. The second group are those who have extremely fewer number of samples from target while high number of samples from source. Not surprisingly, the feasible solutions in terms of experimental budget are those who demand less samples from target and more from source. Based on this observation, we can develop a heuristic that leads us to the appropriate selection of source and target samples in order to arrive at an optimal solution. For instance, we can apply a design of experiment method ~\cite{srinivas2009gaussian} to take sample from source and target in a way that we gain more knowledge comparing with the random selection as we used here. We could also take samples in a active learning fashion ~\cite{jamshidi2016bo4co}. We consider these two directions as a future work.

% %To analyze the trade-off among model accuracy and measurement effort (RQ2), we plot the Pareto optimal solutions of the combination source and target sampling sizes in Figure \ref{fig:pareto-cobot-effort-accuracy}.


%\begin{figure}[h]
%	\begin{center}
%		\includegraphics[width=0.7\columnwidth]{figures/pareto-cobot-lr2}
%		\caption{Indifference curves representing combinations of $(|\mathcal{D}_t|,|\mathcal{D}_s|)$ associated with equal levels of prediction errors.} %The axis represent percentage of configuration space.}
%		\label{fig:pareto-cobot}
%	\end{center}
%\end{figure}


 \begin{figure}[t]
 	\begin{center}
 		\includegraphics[width=0.8\columnwidth]{figures/pareto-cobot-effort-accuracy-lr-annotated}
 		\caption{A two-objective optimization goal. We are interested in the solutions in the targeted sweet spot.}
 		\label{fig:pareto-cobot-effort-accuracy}
 	\end{center}
 \end{figure}



\subsection{Threats to validity and limitations}

%{\noindent \em Internal validity.}
\subsubsection{Internal validity}
In order to ensure internal validity, we repeated the execution of the benchmark systems and measure performance for a large number of configurations. %We repeated this at least 3 times for each configuration to avoid measurement bias.
For doing so, we have invested several months for gathering the measurements, which resulted in a substantial dataset. Moreover, we used standard benchmarks so that we are confident in that we have measured a realistic scenario.

\subsubsection{External validity}
%{\noindent \em External validity.}
In order to ensure external validity, we use three classes of systems in our experiments including: (i)~a robotic system, (ii)~3 different stream processing applications, and (iii)~a NoSQL database system. These systems have different numbers of configuration parameters and are from different application domains.

\subsubsection{Limitations}
%{\noindent \em Limitations.}
Our learning approach relies on several assumptions. First, we assume that the target response function is smooth. If a configuration parameter has an unsteady performance behavior, we cannot learn a reliable model, but only approximate its performance close to the observations. Furthermore, we assume that the source and target responses are related. That is they are correlated to a certain extent. The more related, the faster and better we can learn. Also, we need the configurable system to have a deterministic performance behavior. If we replicate the performance measurements for the same system, the observed performance should be similar. %Finally, our approach, because of GP limitations, has its limits regarding the number of configuration parameters and the size of the learning set. For instance, it is infeasible to learn a model with thousands of configuration parameters. However, we could combine our approach with a dimensionality reduction technique to determine and learn only for the relevant parameters, as this have been shown before~\cite{influence}.

\newpage