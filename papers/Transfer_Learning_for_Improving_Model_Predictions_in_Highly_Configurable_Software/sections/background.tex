%!TEX root = ../paper.tex
\section{Model Learning for Performance Reasoning in Highly Configurable Software}
\label{sec:background}

We motivate the model learning problem by reviewing specific challenges in the robotics domain. However, our method can be applied in other configurable systems as well. %Later, for the sake of evaluation, we will demonstrate the results with 3 different classes of system: (i) robotics, (ii) stream processing, and (iii) NoSQL database.

\subsection{A motivating example}
\label{sec:example}

The software embedded in a robot implements algorithms that enable the robot to accomplish the missions assigned to it, such as path planning~\cite{kawthekarsensitivity}. %The quality of the algorithms and their implementation thus determine the performance we observe from the robot.
Often, robotics software exposes many different configuration parameters that can be tuned and typically affect the robot's performance in accomplishing its missions.
For instance, we would like to tune the localization parameters of an autonomous robot while performing a navigation task as \emph{fast} and \emph{safe} as possible while consuming \emph{minimal energy} (corresponding to longer battery life).
%The configuration parameters may be hidden in the source code or in a configuration file. They may be different in terms of granularity, from categorical to real values~\cite{Rabkin:2011}.

Navigation is the most common activity that a robot does in order to get from point A to point B in an environment as efficient as possible without hitting obstacles, walls, or people. Let us concentrate on the \emph{localization} algorithm that enables navigation tasks and previous studies shown that configuration setting is influential to its performance \cite{kawthekarsensitivity}. Localization is the technique that enables the robot to find (i) its current position and (ii) its current orientation at any point in its navigation task~\cite{thrun2005probabilistic}.
Configurations that work well for one robot in one environment may not work well or at all in another environment for multiple reasons: %For example, localization configurations which work well in an outdoors environment may be ineffective indoors. This motivated us to consider changing the configuration parameters relevant to the localization of autonomous robots that typically operate in dynamic environments in order to improve the accuracy of the localization task that leads to improving the efficiency of its navigation.
%There are specific reasons why we need to change the configuration of robots at runtime:
(i) \textbf{The environment in which robots operate may change at runtime}. For instance, the robot may start in a dark environment and then reaches a much brighter area. As a result, the quality of localization may be affected as the range sensors for estimating the distance to the wall provide measurements with a higher error rate and the false readings increase then. %As a result, the robot may require more time to localize in the environment.
(ii) \textbf{The robot itself may move to an environment where the motion of the robot will change}. For example, when a wheeled robot moves to a slippery floor, then the motion dynamics of the robot will change and the robot cannot accelerate the speed with the same power level.
(iii) \textbf{The physics of the environment may change}. For example, imagine the situation where some items are put on a service robot while doing a mission. As a result, the robot has to sustain more weight and consumes more energy for doing the same mission. Therefore, it may be desirable to sacrifice the accuracy by adjusting localization parameters in order to perform the mission before running out of battery.
(iv) \textbf{A new mission may be assigned to the robot}. While a service robot is doing a mission, a new mission may be assigned, for example, a new delivery spot may be defined. In cases where the battery level is too low to finish the new mission, we would change to a configuration that sacrifices the localization accuracy in favor of energy usage.

In all of these situations, it is desirable to change the configuration regarding the localization of the robot, in an automated fashion. For doing so, we need a model to reason about the system performance especially when we face contradicting aspects that require a trade-off at runtime. %For instance, we may want to sacrifice performance for a lower battery usage for the service robot in order to finish the navigation task before running out of battery.
%As a results of these dynamic changes to the environment and the robot itself, we need to find a new configuration parameter that works well for the localization algorithm and as a result the robot will be able to accomplish the task \emph{safely} with \emph{minimum battery usage}.
We can learn a performance model empirically using observations which are taken from the robot performing missions in a real environment under different configurations. But, the observations in real-world are typically costly, error-prone, and sometimes infeasible. There are, however, several simulation platforms that provide a simulated environment, in which we can learn about the performance of the robot in different conditions given a specific configuration (\emph{e.g.}, Gazebo \cite{reckhaus2010overview}). Although the observations from a simulated environment are far less costly than the measurements taken from the real robot and impose no risks when a failure happens, they may not necessarily reflect real behavior, since not every physical aspect can be accurately modeled and simulated. Fortunately, simulations are often highly \emph{correlated} with the real behavior. The key insight behind our approach is to learn the robot performance model by using only a few expensive observations on a real system, but several cheap samples from all sources. %We can then exploit the knowledge we gain from the relationship of simulation samples and the few ones on real system to train a performance model that is more accurate at less costly comparing with the situation where we rely only on observations from real systems.

%The localization algorithm in robotics platforms (\emph{e.g.}, ROS) comprises of hundreds parameters and the configuration space is simply too large to learn a reliable model relying on sampling the real robot platform. There are, however, several simulation platforms that provide a simulated environment in which we can learn about the performance of the robot in different conditions given a specific configuration (\emph{e.g.}, Gazebo). Now the key challenge is how to learn a model mainly from the measurements from simulation (which are far less costly than the measurements on the real robot platform and does not have the risks mentioned earlier).

\subsection{Challenges}

Although we can take relatively cheap samples from simulation, it is impractical and naive to exhaustively run simulation environments for all possible configurations:
\begin{itemize}
\item \textbf{Exponentially growing space}. The configuration space of just 20 parameters with binary options for each comprises of $2^{20}\approx1m$ possible configurations. Even for this small configuration space, if we spend just one minute for collecting each sample, it will take about 2 years to perform an exhaustive sampling. Therefore, we can sample only a subset of this space, and we need to do the sampling purposefully.
\item \textbf{Negative transfer}. Not all samples from a simulator reflect the real behavior and, as a result, they would not be useful for learning a performance model. More specifically, the measurements from simulators typically contain noise and for some configurations, the data may not have any relationship with the data we may observe on the real system. Therefore, if we learn a model based on these data, it becomes inaccurate and far from real system behavior. Also, any reasoning based on the model predictions becomes misleading or ineffective.
\item \textbf{Limited budget}. Often, different sources of data exist (\emph{e.g.}, different simulators, different versions of a system) that we can learn from and each may impose a different cost. Often, we are given a limited budget that we can spend for either source and target sample measurements.
%Moreover, too many training samples cause long training time. Since the learned model may be used in some time constrained environments (\emph{e.g.}, runtime decision making in a feedback loop for robots), it is important to select the sources purposefully.
\end{itemize}


\subsection{Problem formulation: Black-box model learning}
\label{sec:problem}

In order to introduce the concepts in our approach concisely, we define the model learning problem using mathematical notations. Let $X_i$ indicate the $i$-th configuration parameter, which ranges in a finite domain $Dom(X_i)$. In general, $X_i$ may either indicate (i) an integer variable (\emph{e.g.}, the \emph{number of iterative refinements} in a localization algorithm) or (ii) a categorical variable (\emph{e.g.}, \emph{sensor names} or binary options (\emph{e.g.}, \emph{local vs global localization method}). The configuration space is mathematically a Cartesian product of all of the domains of the parameters of interest $\mathbb{X}=Dom(X_1) \times \dots \times Dom(X_d)$. %A configuration $\mathbf{x}$ then resides in the configuration space $\mathbf{x} \in \mathbb{X}$.

A black-box response function $f:\mathbb{X}\rightarrow\mathbb{R}$ is used to build a performance model given some observations of the system performance under different settings. In practice, though, the observation data may contain noise, \emph{i.e.},~$y_i=f(\mathbf{x}_i)+\epsilon_i,\mathbf{x}_i \in \mathbb{X}$ where $\epsilon_i \sim \mathcal{N}(0,\sigma_i)$ and we only partially know the response function through the observations $\mathcal{D}=\{(\mathbf{x}_i,y_i)\}_{i=1}^d, |\mathcal{D}|\ll|\mathbb{X}|$.
In other words, a response function is simply a mapping from the configuration space to a measurable performance metric that produces interval-scaled data (here we assume it produces real numbers). Note that we can learn a model for all measurable attributes (including accuracy, safety if suitably operationalized), but here we mostly train predictive models on performance attributes (\emph{e.g.}, response time, throughput, CPU utilization).

%\PJ{I can simply delete the following paragraph if it is not useful, I only referred back to it in the main section to describe that a classifier may learned to detect these invalid configurations...}
%A configurable system may fail to meet a constraint (\emph{e.g.}, a robot may not be able to find the end spot) under some specific configurations, make the configuration invalid. Constraint violation can be encoded as a constraint function $c: \mathbb{X} \rightarrow L$, where $L$ is a set of possible labels indicating the validity of the configuration. For instance, $l=0$ indicates a valid, while $l=1$ indicates an invalid configuration. %Due to the effect of black-box nature of this, we assume that $c(\mathbf{x})$ is a random process that consists of a number of independent random variables, each following a distribution with an unknown probability density function $p_{\mathbf{x}}(l) = p(c(\mathbf{x}) = l)$.
%The subspace of the configuration space that has a non-zero probability of satisfying all constraints is called the valid region $\mathcal{V} = \{x| x \in \mathbb{X}: p_{\mathbf{x}}(1) = 0\}\subseteq \mathbb{X}$. The invalid region is then defined by $I = \{x| x \in \mathbf{X}: p_{\mathbf{x}}(1) > 0\}\subseteq \mathbb{X}$. These two subsets are disjoint, $I\cap \mathcal{V}=\emptyset$ and together creates the whole configuration space, $\mathbb{X}=I\cup\mathcal{V}$.  An example of an invalid configuration in the case of robotics system happens when the robot tries to localize in an environments in a corridor with many windows leading to high sensory errors and this leads to a very high localization error and the robot cannot find the end spot and the mission cannot be accomplished and therefore invalid.

Our main goal is to learn a reliable regression model, $\hat{f}(\cdot)$, that can predict the performance of the system, $f(\cdot)$, given a limited number of observations $\mathcal{D}$. More specifically, we aim to minimize the \emph{prediction error} over the configuration space:
\begin{equation} \label{eq:objective}
\arg \min _{\mathbf{x}\in\mathbb{X}} pe=|\hat{f}(\mathbf{x})-f(\mathbf{x})|
\end{equation}

In order to solve the problem above, we assume $f(\cdot)$ is reasonably smooth, but otherwise little is known about the response function. Intuitively, we expect that for ``near-by'' input points $\mathbf{x}$ and $\mathbf{x}'$ their corresponding output points $y$ and $y'$ to be ``near-by'' as well. %This allows us to build a model using black-box models that can predict unobserved values.


\subsection{State-of-the-art}
In literature, model learning has been approached from two standpoints: (i) sampling strategies and (ii) learning methods.

\subsubsection{Sampling}
Random sampling has been used to collect unbiased observations in computer-based experiments. However, random sampling may require a large number of samples to build an accurate model \cite{influence}.
More intelligent sampling strategies (such as Box-Behnken and Plackett-Burman) have been developed in the statistics and machine learning communities, under the umbrella of experimental design, to ensure certain statistical properties~\cite{montgomery2008design}. The aim of these different experimental designs is to ensure that we gain a high level of information from sparse sampling (partial design) in high dimensional spaces. Relevant to the software engineering community, several approaches tried different designs for highly configurable software~\cite{guo2013variability} and some even consider cost as an explicit factor to determine optimal sampling \cite{sarkar2015cost}.

For finding optimal configurations, researchers have tried novel ways of sampling with a feedback embedded inside the process where new samples are derived based on information gained from the previous set of samples. A recent solution \cite{bodin2016integrating} uses active learning based on a random forest to find a good design.
Recursive Random Sampling (RRS)~\cite{ye2003recursive} integrates a restarting mechanism into the random sampling to achieve high search efficiency. Smart Hill Climbing (SHC)~\cite{xi2004smart} integrates importance sampling with Latin Hypercube Design (LHD). %SHC estimates the local regression at each potential region, then it searches toward the steepest descent direction. 
An approach based on direct search~\cite{Zheng2007} forms a simplex in the configuration space, and iteratively updates the simplex through a number of operations to guide the sample generation. Quick Optimization via Guessing (QOG)~\cite{osogami2007optimizing} speeds up the optimization process exploiting some heuristics to filter out sub-optimal configurations. Recently, transfer learning has been explored for configuration optimizations by exploiting the dependencies between configurations parameters \cite{chen2009experience} and measurements for previous versions of big data systems using an approach called TL4CO in DevOps \cite{artavc2017dice}.  

\subsubsection{Learning} 
Also, standard machine-learning techniques, such as support-vector machines, decision trees, and evolutionary algorithms have been tried~\cite{jamshidi2016bo4co,yigitbasi2013towards}. These approaches trade simplicity and understandability of the learned models for predictive power. For example, some recent work~\cite{zhang2015performance} exploited a characteristic of the response surface of the configurable software to learn Fourier sparse functions by only a small sample size. Another approach \cite{influence} also exploited this fact, but iteratively construct a regression model representing performance influences in an active learning process.

\subsubsection{Positioning in the self-adaptive community}
Performance reasoning is a key activity for decision making at runtime. Time series techniques~\cite{ehlers2011self} shown to be effective in predicting response time and uncovering performance anomalies ahead of time. FUSION~\cite{esfahani2013learning,elkhodary2010fusion} exploited inter-feature relationships (\emph{e.g.}, feature dependencies) to reduce the dimensions of configuration space, making runtime performance reasoning feasible. Different classification models have been evaluated for the purpose of time series predictions in \cite{anaya2014prediction}.
Performance predictions also have been applied for resource allocations at runtime~\cite{huber2017model,jamshidi2016managing}.
Note that the approaches above are referred to as black-box models. However, another category of models known as white-box is built early in the life cycle, by studying the underlying architecture of the system \cite{gomaa2007model,happe2011facilitating} using Queuing networks, Petri Nets, and Stochastic Process Algebras~\cite{balsamo2004model}.
Performance prediction and reasoning have also been used extensively in other communities such as component-based~\cite{becker2006performance} and control theory~\cite{filieri2015automated}.

\subsubsection{Novelty}
Previous work attempted to improve the prediction power of the model by exploiting the information that has been gained from the target system either by improving the sampling process or by adopting a learning method.
Our approach is orthogonal to both sampling and learning, proposing a new way to enhance model accuracy by exploiting the knowledge we can gain from other \emph{relevant} and possibly \emph{cheaper} sources to accelerate the learning of a performance model through transfer learning.
%Transfer learning attempts to transfer knowledge learned in one or more source tasks and leverage it to improve learning in a related target task \cite{torrey2009transfer}. In this context, source tasks are performance models of the system under different environmental conditions: (i)~different workload, (ii)~different deployment infrastructure or (iii)~different versions of the system. The performance of the system varies when one of these environmental conditions changes, however, as we will show later in our experimental results, the performance responses are correlated. This correlation is an indicator that the source and target are related and there is a potential to learn across the environments.


Transfer learning has been applied previously for regression and classification problems in machine learning \cite{pan2010survey}, in software engineering for defect predictions \cite{krishna2016too,nam2015heterogeneous,nam2013transfer} and effort estimation \cite{kocaguneli2015transfer} and in systems for configuration optimization \cite{chen2009experience,artavc2017dice}. However, in this paper, we enable a generic form of transfer learning for the purpose of sensitivity analysis over the entire configuration space. Our approach can enable (i) performance debugging, (ii) performance tuning, (iii) design-time evolution, or (iv) runtime adaptation in the target environment by exploiting any source of knowledge from the source environment including (i)~different workloads, (ii)~different deployments, (iii)~different versions of the system, or (iv)~different environmental conditions. The performance of the system varies when one or more of these changes happen, however, as we will show later in our experimental results, the performance responses are correlated. This correlation is an indicator that the source and target are related and there is a potential to learn across the environments. To the best of our knowledge, our approach is the first attempt towards learning performance models using cost-aware transfer learning for configurable software.