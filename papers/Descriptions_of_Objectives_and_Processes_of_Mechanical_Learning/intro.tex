Mechanical learning is a computing system that is based on a simple set of fixed rules (so called mechanical), and can modify itself according to incoming data (so called learning). A learning machine $\mathcal{M}$ is a system that realizes mechanical learning. 

In \cite{paper1}, we introduced mechanical learning and discussed some basic aspects of it. Here, we are going to continue the discussion of mechanical learning. As we proposed in \cite{paper1}, there are naturally 2 ways to go: to directly realize one learning machine, or to well describe what mechanical learning is really doing. Here, we do not try to design a specific learning machine, instead, we focus on describing the mechanical learning, specially, the objects and the process of learning, and related properties. Again, the most important assumption is mechanical, i.e., the system must follow {\it a set of simple and fixed rules}. By posting such requirement on learning, we can go deeper and reveal more interesting properties of learning.   

In section 2, we discuss more about learning machine. We show one useful simplification: a $N$-$M$ learning machine can be reduced to $M$ independent $N$-1 learning machines. This simplification could help us a lot. We define level 1 learning machine in section 2. This concept clarifies a lot of confusing.

The driving force of a learning machine $\mathcal{M}$ is its incoming data, and incoming data forms patterns. Thus, we need to understand pattern first. In section 3, we discuss patterns and examples. In the process of understanding pattern, what is objective and what is subjective is naturally raised. In fact, these issues are very crucial to learning machine. Objective patterns and their basic operators are straightforward. In order to understand subjective pattern, we discuss how learning machine to perceive and process pattern. Such discussions lead us to subjective pattern and basic operators on them. We introduce X-form for subjective expressions, which will play central role in our later discussions. We prove that for any objective pattern we can find a proper X-form based upon least base patterns and to express the objective pattern well. 

Learning by teaching, i.e. learning driving by a well designed teaching sequence (a special kind of data sequence), is a much simpler and effective learning. Though learning by teaching is only available in very rare cases, it is very educational to discuss it first. This is what we do in section 4. We show if a learning machine has certain capabilities, we can make teaching sequence so that under driven of such teaching sequence, it learns effectively. So, with these capabilities, we have an {\it universal learning machine}.

From learning by teaching, we get insight that the most crucial part of learning is abstraction from lower to higher. We try to apply such insights to learning without teaching. In section 5, we first defined mechanical learning without teaching. Then we introduce {\it internal representation space}, which is the center of learning machine and best to be expressed by X-forms. Internal representation space is actually where learning is happening. We write down the formulation of learning dynamics, which gives a clear picture about how data drives learning. However, one big issue is how much data are enough to drive the learning to reach the target. With the help of X-form and sub-form, we define data sufficiency: sufficient to support a X-form, and sufficient to bound a X-form. Such sufficiency gives a framework for us to understand data used to drive learning. We then show that by a proper learning strategy, with sufficient data, with certain learning capabilities, a learning machine indeed  can learn. We demonstrate 3 learning strategies: embed into parameter space, squeezing to higher abstraction from inside, and squeezing to higher abstraction from inside and outside. We show that the first learning strategy is actually what deep learning is using (see Appendix for details).  And, we show that by other 2 learning strategies with certain learning capabilities, a learning machine can learn any pattern, i.e. it is an {\it universal learning machine}. Squeezing to higher abstraction and more generalization is one strategy that we invent here. We believe that this strategy would work well for many learning tasks. We need to do more works in this direction.

In Section 6, we put more thoughts about learning machine. We will continue work on these directions. In section 7, we briefly discuss some issues of designing a learning machine. 

In Appendix, we view deep learning (restricted to the stacks of RBMs) from our point of view, i.e. internal representation space. We start discussions from simplest, i.e. 2-1 RBM, then 3-1 RBM, N-1 RBM, N-M RBM, and stacks of RBM, and deep learning. In this way, it is clear that deep learning is using the learning strategy: embed a group of X-forms into parameter space that we discuss in section 5. 

As in \cite{paper1}, for the same reason, here we will restrict to {\it spatial learning}, not consider {it temporal learning}.
