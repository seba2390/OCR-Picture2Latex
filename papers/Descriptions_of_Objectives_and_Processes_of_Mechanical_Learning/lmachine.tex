{\bf IPU -- Information Processing Unit}\\
We have discussed mechanical learning in \cite{paper1}. A learning machine is a concrete realization of mechanical learning. We can briefly recall them here. See the illustration of IPU (Information Processing Unit):

\begin{center}
\begin{picture}(300,150)(0,0)
\put(0,-200){\resizebox{10 cm}{!}{\includegraphics{IPU.png}}}
\end{picture}

{\bf Fig. 1. Illustration of $N$-$M$ IPU (Information Processing Unit)} 
\end{center}

One $N$-$M$ IPU has input space ($N$ bits) and output space ($M$ bits), and it will process input to output. If the processing are adapting according to input and feedback to output, and such adapting is governed by a set of simple and fixed rules, we call such adapting as {\it mechanical learning}, and such IPU as {\it learning machine}. Notice the phrase "a set of simple and fixed rules". This is a strong restriction. Mostly, we use this phrase to rule out human intervention. And, we pointed out this: since the set of adapting rules is fixed, we can reasonablly think the adapting rules are built inside learning machine at the setup. 

We will try to well describe learning machine. First, we can put one simple observation here.


\begin{theorem}
One $N$-$M$ IPU $\mathcal{M}$ is equivalent to $M$ $N$-1 IPU $\mathcal{M}_i,  i=1, \ldots, M$.
\end{theorem}
{\bf Proof:} The output space of $\mathcal{M}$ is $M$-dim, so, we assume it is $(v_1, v_2, \ldots, v_M)$. If we project to first component, i.e. $v_1$, we get a $N$-1 IPU, denote it as: $\mathcal{M}_1$. We can do same for $v_i, i = 2, \ldots, M$, and get $N$-1 IPUs: $\mathcal{M}_2, \ldots, \mathcal{M}_M$. This tells us, if we have one $N$-$M$ IPU $\mathcal{M}$, we can get $M$ $N$-1 IPU $\mathcal{M}_1, \ldots$, so that $\mathcal{M}$ = $(\mathcal{M}_1, \mathcal{M}_2, \ldots  \mathcal{M}_M)$.

On the other side, if we have $M$ $N$-1 IPU, $\mathcal{M}_1, \mathcal{M}_2, \ldots,  \mathcal{M}_M$, we can use them to form a $N$-$M$ IPU in this way:  
$\mathcal{M}$ = $(\mathcal{M}_1, \mathcal{M}_2, \ldots  \mathcal{M}_M)$.

Though this theorem is very simple, it can make our discussion much simpler. For most time, we can only consider $N$-1 IPU, which is much simpler to discuss. However, this is only to consider IPU, i.e. ability to process information. For learning, we need to consider more. See theorem 2. 
\bigskip



{\bf The purpose or target of learning machine:} \\
One learning machine is one IPU, i.e. it will do information processing for each input and
generate output, it maps one input (a $N$-dim binary vector) to a $M$-dim binary vector. This is what a CPU does as well (More abstractly, since we do not restrict the size of $N$ and $M$, any software without temporal effect can be thought as one IPU). 

However, learning machine and CPU have very different goal. One CPU is designed to distinguish a
input $b \in PS^0_N$ from any other, even there is only one bit difference, i.e. bit-wise. Yet, IPU and learning machine are not designed for such purpose. IPU and learning machine are designed to distinguish patterns. It should generate different output for different patterns, but, should generate same output for different inputs of a same pattern. That is to say, the target of a learning machine is to learn to distinguish a group of base patterns and how to process them. Thus, we need to understand patterns. Actually, to understand patterns is the most essential job, which is done in next section. 
\bigskip


{\bf Data } \\
The purpose of a learning machine is to learn, i.e. to modify its information processing. However, we would emphasis that for mechanical learning, learning is driven by data fed into it.


\begin{definition}[\bf Data Sequence]
If we have a sequence $T_i, i=1, 2, \ldots$, and $T_i = (b_i, o_i)$, where $b_i$ is a base pattern, $o_i$ is either $\varnothing$ (empty) or a binary vector in output space, we call this sequence a data sequence. 
\end{definition}
Note, $o_i$ could be empty or a vector in output space. If it is non-empty, it means that at the moment, the vector should be the value of output. If it is empty, it means there is no data for output match up. Learning machine should be able to learn even $o_i$ is empty. Of course, with value of output, the learning is often easier and faster.  

We can easily see that data sequence is the only information source for a learning machine to modify itself. Without information from data sequence, learning machine just has no information about what to modify. Learning machine will adapt itself only based on information from data sequence. 

There are 2 kinds of data sequence. One is very well designed data sequence, i.e. we know the consequence of this data, and we can expect the outcome of learning. This is called teaching sequence. Another kind of data sequence is not teaching sequence. These data sequences are just outside data to drive the learning machine (could be random from outside). We have no much knowledge about them. Clearly, in order to learn certain target, if available, a teaching sequence is much more efficient. However, in most cases, we just do not have teaching sequence. 
\bigskip


{\bf Universal Learning Machine } \\
Naturally, we will ask what a learning machine can learn? Can it learn anything? To address this, we need some careful defintion. Suppose we have a learning machine $\mathcal{M}$. At the beginning, $\mathcal{M}$ has the processing $P_0$, i.e. $P_0$ is one mapping from input space ($N$-dim) to output space ($M$-dim). As the learning going, the processing will changed to $P_1$, which is also one mapping from input space to output space, different one though. This is exactly what a learning machine does: its processing $P$ is adapting. We then have following definition.


\begin{definition}[\bf Universal Learning Machine]
For a learning $\mathcal{M}$, suppose its current processing is $P_0$, and $P_1$ is another processing, if we have one data sequence $T$ (which depends on $P_0$ and $P_1$), so that when we apply $T$ to $\mathcal{M}$, at the end, the processing of $\mathcal{M}$ become $P_1$, we say $\mathcal{M}$ can learn $P_1$ starting from $P_0$. If for any given processing $P_0$ and $P_1$, $\mathcal{M}$ can learn $P_1$ starting from $P_0$, we say $\mathcal{M}$ is an universal learning machine. 
\end{definition}

Simply say, an universal learning machine can learn anything starting from anything. Universal learning machine is desirable. But, clearly, not all learning machine are universal. So, we will discuss what properties can make a learning machine become universal. 



In Theorem 1, we gave the relationship of $N$-$M$ IPU and $N$-1 IPU. In order to discuss the relationship of  $N$-$M$ learning machine and $N$-1 learning machine, we need to introduce one property: standing for zero input. We say a learning machine $\mathcal{M}$ with property of standing for zero input, if $\mathcal{M}$ will do nothing for learning, i.e. doing nothing to modify its internal status, when input is zero vector (i.e. $(0, 0, \ldots, 0)$ ) and output side value is empty. Such a property for a learning machine should be very reasonable and very common. After all, zero input means no stimulation from outside, and it is very reasonable to require that learning machine should do nothing for such input.



\begin{theorem}
If we have one $N$-1 universal learning machine $\mathcal{S}$ with property of standing for zero input, we can use $M$ independent $\mathcal{S}$ to construct a $N$-$M$ universal learning machine $\mathcal{M}$.
\end{theorem}
{\bf Proof:} For simplicity and without loss of generality, we only consider the case of $M = 2$. Now, $\mathcal{S}$ is a $N$-1 universal learning machine. As in theorem 1, we can construct a $N$-2 IPU $\mathcal{M}$ by this way:  $\mathcal{M}$ = $(\mathcal{S}_1, \mathcal{S}_2)$. 

$\mathcal{M}$ is sure a $N$-2 learning machine. We only need to show it is universal learning machine. That is to say, for any given processing $P_0$ and $P_1$, there is one data sequence, and driven by the data sequence, $\mathcal{M}$ can learn $P_1$ from $P_0$. 

Actually, we can design a data sequence as following: $T$ is $T_1$ followed by $T_2$, where, $T_1  = ( (D_1, Z_1)$, and $T_2 = (Z_2, D_2) $, where $D_1$ is the data sequence that drives $\mathcal{S}_1$ to learn $P_1$ from $P_0$, $D_2$ is the data sequence that drives $\mathcal{S}_2$ to learn $P_1$ from $P_0$, and  $Z_1, Z_2$ are the zero inputs. Since $\mathcal{S}_1$ and $\mathcal{S}_2$ are universal learning machine, $D_1, D_2$ indeed exist. We know the data sequence $T_1$ followed by $T_2$ indeed is the data sequence we want. 

Of course, the data sequence $T$ ($T_1$ forllowed by $T_2$) is far from optimal, and not desired in practice. But, here we just show the existence of such data sequence.
 
From theorem 1 and 2, we can see that without loss of generality, in many cases, we can focus on $N$-1 learning machine. From now on, we will mostly discuss $N$-1 learning machine. 
\bigskip



{\bf Different Level of Learning } \\
Learning machine modifies its processing by data sequence. Obviously, there is some mechanism inside learning machine to do the learning. More specifically, this learning mechanism would catch information embedded inside data sequence, and use the information to modify its processing. But, we need to be very careful to distinguish 2 things: 1) the learning mechanism only modify the processing, and the learning mechanism itself is not modified; 2) the learning mechanism itself is also modified. But, how to describe these things well?

If $\mathcal{M}$ is an universal learning machine, so, for any giving 2 processing $P_0$ and $P_1$,  we have one data sequence $\mathcal{T}$ so that, starting from $P_0$, and by applying $\mathcal{T}$ to  $\mathcal{M}$, its processing becomes $P_1$. This is clear. But, consider this, somehow, we apply some other data sequence so that the processing becomes $P_0$ again. Since $\mathcal{M}$ is universal, this is allowed. But, we ask, what about if we apply data sequence $\mathcal{T}$ again? what would happen? Do we still have processing becomes $P_1$? There is no guarantee for this. Actually, for many learning machine, this is not the case. However, if this is true, it indicate this: learning mechanism does not change as the processing is changing. This would be one important property. We use next definition to capture this property. 


\begin{definition}[\bf Level 1 Learning Machine]
$\mathcal{M}$ is an universal learning machine, for any giving pair of processing $P_0$ and $P_1$, by definition, there is at least one data sequence $T$, so that, starting from $P_0$, and by applying $T$ to  $\mathcal{M}$, processing becomes $P_1$. If the teaching sequence $T$ will only depends on $P_0$ and $P_1$, and dose not depend on any history of processing of $\mathcal{M}$, we call $\mathcal{M}$ as one level 1 universal learning machine.
\end{definition}
Note, following this line of thoughts, we also can define level 0 learning machine, which is an IPU that its processing could not be changed. And, we also can define level 2 learning machine, which is a learning machine that its processing could change, and its learning mechanism could change as well, but its learning mechanism of learning mechanism could not be changed. We can actually follow this line, to define level $J$ learning machine, $J = 0,1, 2, \ldots$. But, we do not discuss in this direction. We will mostly consider level 1 learning machine. 
\bigskip







{\bf Some Examples}\\
{\bf Example 2.1 [\bf Perceptron]}
Perhaps, the simplest learning machine is the perceptron. Perceptron $\mathcal{P}$ is a 2-1 IPU, and it is a learning machine. However, it is not universal. As well known, $\mathcal{P}$ does not have AND gate and XOR gate. That is to say, no matter what, $\mathcal{P}$ could not learn these 2 processing . 
\medskip

{\bf Example 2.2 [\bf RBM is learning machine]} 
See \cite{hinton} for RBM. $N$-1 RBM is one $N$-1 IPU. It is a learning machine as well. There could be many ways to make it learn. The most common way is the so-called Gibbs sampling methods. We can see this clearly: Gibbs sampling is a simple set of rules, and the processing is modified as data is fed into. However, as we can see in Appendix, $N$-1 RBM is not universal.

Put $M$ independent $N$-1 RBM together by the way in theorem 1, we get a $N$-$M$ RBM. So, $N$-$M$ RBM is one learning machine, but it is not universal. 
\medskip

{\bf Example 2.3 [\bf Deep learning might be a learning machine]}
Deep learning normally is a stack of RBM, see \cite{hinton}. It is often formed in this way: first use data to train RBM at each layer, then stack different layers together, then use data to do further training. By the restricted sense, the whole deep learning action is not mechanical learning, since it involves a lot of human intervention. But, if we just see the stage after different layers stacked together, and exclude any further human intervention, it is a mechanical learning. So, in this sense, deep learning is a learning machine.
\medskip

{\bf Example 2.4 [\bf Deep learning might not be a learning machine]}
But, these days, deep learning is much more than stacking RBM together then training without human intervention. There are a lot of pruning, change structure, adjusting done by human. Such learning is surely not mechanical learning. However, many properties can still be studied by point of view of mechanical learning. 
\medskip

Generally, we can say, for software to do learning, it often needs people to establish its very complicated structure and initial parameters. This establishment is not simple and fixed. But, once software is established, and is running without human intervention, such software is learning machine.

