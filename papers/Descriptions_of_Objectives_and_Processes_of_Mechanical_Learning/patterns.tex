Incoming data drive learning. But, IPU and learning machine do not treat data bit-wise. They treat data as patterns. So, patterns are very important to learning machine. Everything of a learning machine is around patterns. Yet, pattern is also quite confusing. We can actually view pattern from different angles and get quite different results. We can view patterns objectively, i.e. totally independent from learning machine and learning, and we can view patterns subjectively, i.e. quite dependent on learning machine and its view on pattern. It is very important we clarify the concept here.
\bigskip


{\bf Examples of Patterns}
Before going to more rigorous discussions, we here discuss some examples of patterns, which could help us to clean thoughts. The simplest patterns are 2-dim patterns. 

{\bf Example 3.1 [\bf All 2-dim Base Patterns]}
2-dim patterns is so simple that we can list all of base patterns explicitly below:
\begin{center}
$PS^0_2$ = $\{ (0,0), (0,1), (1,0), (1,1) \}$ 
\end{center} 


All base patterns are here: totally 4 base patterns. For example, (0, 1) is a base pattern. But, besides base patterns, there are more patterns. How about this statement: "incoming pattern is (0,0) or (0,1)"? Very clearly, what this statement describes is not in $PS^0_2$. However, equally clearly, this statement is valid, and specifies an incoming pattern. We have solid reason to believe that the statement represents a new pattern that is not in base pattern space. So, the patterns should be able to include "combination of patterns". We can introduce one way to express this:
\begin{center} 
$p$ = $(0,0) \sqcup (0, 1)$ = \{ one pattern that either (0,0) or (0,1) appears \} 
\end{center} 

In above equation, the symbol $\sqcup$ is called OR (see the similar usage of symbol in \cite{dlog}). The combination operator $\sqcup$ would make a new pattern out of 2 base patterns. Clearly, this new pattern is not in base pattern space. Additional important point: we should note that the new pattern $p$ above is independent from learning machine.  


{\bf Example 3.2 [\bf 2x2 Black-White Images]}
We can consider a little more complicated base patterns: 2x2 black-white images. See below illustrations.

\begin{center}
\begin{picture}(340,120)(0,0)
\put(100,20){\resizebox{3 cm}{!}{\includegraphics{Fig1.png}}}
\end{picture}

{\bf Fig. 1 One base pattern in base pattern space of 2x2 black-white images} 
\end{center}

Although in the above illustrations, the patterns are in 2-dim form, it is easy to see that all these patterns can be represented well in linear vector form (for example, the base pattern in Fig. 1 is (1, 1, 0, 1)). It is simple enough so that we can list them:
\begin{center}
$PS^0_4$ = $\{ (0,0,0,0), (1,1,0,0), (0,0,1,1), (1,0,1,0), (0,1,0,1), \cdots \}$ 
\end{center} 

One pattern could be shown as the vector or as 2x2 image. For example, (1,0,1,0) is in vector form, the equivalent image is a vertical line. Let's see some example of combination operators. We can view (1,1,0,0) as one horizontal line, and (0,1,0,1) as one vertical line. Consider this statement "one pattern that has this horizontal line and also this vertical line". Clearly, this is one new pattern. We try to capture it as below:
\begin{center}
$p$ = $(1,1,0,0) \sqcap (0,1,0,1)$ = \{ one pattern that both (1,1,0,0) and (0,1,0,1) appears together \} 
\end{center}  
The symbol $\sqcap$ is called AND (see the similar usage of symbol in \cite{dlog}). But, what is the new pattern $p$? First impression that it is the base patter: $(1,1,0,1)$ (see it in Fig. 1). It is. This is a new base  pattern out from 2 base patterns. How come? Yet, it could be even more complicated. We will address this later.

Now, we should note that the new pattern $p$ above is surely dependent on learning machine and how it views patterns. Without learning machine and how it views patterns, we could not even talk about "appears together".   

We will see another example of pattern but not base pattern. $(1,1,0,0)$ is a base pattern. How about this statement: "one pattern that (1,1,0,0) not appears"? This is one new pattern as well. We would have:
\begin{center}
$p$ = $\neg (1,1,0,0)$ = \{ one pattern that (1,1,0,0) not appears  \} 
\end{center}  
The symbol $\neg$ is called NOT (see the similar usage of symbol in \cite{dlog}). However, what is the new pattern? Is it a group of base patterns: \{(0,0,1,1), (0,0,0,1), $\cdots$\}? As the last question, this should be addressed later.

Besides the above situations, actually, we can see more interesting things (which could not be seen in $PS^0_2$).


{\bf Example 3.4 [\bf Abstraction and Concretization]}
Let's see this pattern:

\begin{center}
$p_h$ = \{ common feature of (1,1,0,0) and (0,0,1,1) \} 
\end{center}  
Clearly, this common feature is not in $PS^0_4$. But, this common feature is one very important pattern: it represents horizontal line. Actually, we can say this pattern $p_h$ is horizontal line. Similarly, we have:  
\begin{center}
$p_v$ = \{ common feature of (1,0,1,0) and (0,1,0,1) \} 
\end{center}  

This time, $p_v$ is vertical line. Further, we can see:
\begin{center}
$p_l$ = \{ common feature of $p_h$ and $p_v$ \}
\end{center}

This time, $p_l$ is line, vertical or horizontal. From the examples above, we can see clearly that abstracting a common feature out from a group of patterns is one very important operation. Without it, we simply could not see some very crucial patterns (such as line). Thus, we need to develop symbols for such operations. For example:

\begin{center}
$p_l$ = $\alpha (p_h, p_v)$
\end{center}

Here, $\alpha$ is one operation that abstract some common features out from the patterns $p_h$ and $p_v$. Note, $\alpha$ is not one operator, but one operation. That means that for same set of patterns, could have more than one operations, which abstract different features from the set of patterns. As we meet more complicated patterns later, this properties would become very clearer.

Very clearly, the operation $\alpha$ is highly dependent on a learning machine and what the learning machine learned previously. 

Conversely to abstraction operation $\alpha$, we can also have concretization operation $\rho$. See examples below:
\begin{center}
$p = \rho(p_h, (0,0,0,1))$ = \{ one concrete horizontal line related to the pattern (0,0,0,1) \} = (0,0,1,1) 
\end{center}

$\rho$ is one operation that concretize a pattern (which is one abstraction pattern) by related it to some pattern. 
Any concretization of a pattern is a pattern. As above, concretizing a horizontal line would give a real horizontal line. And, since it is related to (0,0,0,1), this horizontal line should be (0,0,1,1). 

Very clearly, the operation $\alpha$ and $\rho$ are highly dependent on a learning machine (such as: what the learning machine learned previously, how it views patterns, etc). 

From above examples, we can see that patterns are much more than base patterns. We can have pattern of patterns (see horizontal lines, vertical lines). We can have pattern of patterns of patterns (see line). We can have operations on the patterns. We have operators of patterns. All results are still patterns. So, patterns are not just one type, it has many types. Or, we can say patterns are typeless. Base patterns are just simplest patterns and fundamental building blocks. 


{\bf Example 3.3 [\bf 4x4 Black-White Images]}
We now consider even more complicated patterns: 4x4 black-white images. See below illustrations.

\begin{center}
\begin{picture}(340,120)(0,0)
\put(100, 20){\resizebox{3 cm}{!}{\includegraphics{Fig2.png}}}
\end{picture}

{\bf Fig. 2  A base pattern in base pattern space of 4x4 black-white images} 
\end{center}

The binary vector space has $2^{16}$ elements. This is a large number. While in theory we can still list all base patterns, it would be very hard.  
\begin{center}
$PS^0_{16}$ = $\{ \cdots, (1,1,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0), \cdots \}$ 
\end{center} 

Since there is a larger dimension, more phenomenon would appears. We can see  some of them here. Clearly, the binary vector shown in the above equation is one horizontal line. So, we can still have:
\begin{center}
$p_h$ = $\alpha$(first 2 horizontal lines)
\end{center}  

Clearly, this pattern $p_h$ is not in $PS^0_{16}$. But, it represents first 2 horizontal lines. Can this pattern $p_h$, which abstracts first 2 horizontal line, represent all horizontal 4 lines? This is one very important question. At this moment, we can not answer it. 

Similarly, we have:  
\begin{center}
$p_v$ = $\alpha$(all vertical lines) 
\end{center}  

And, we ca have: 
\begin{center}
$p_l$ = $\alpha (p_h, p_v)$
\end{center}

But, again, since we are dealing more complicated patterns space now, we can see something that Example 2 could not show. How about: 
\begin{center}
$p_c$ = \{ a point at coordination (3,3) \},  $p_b$ = \{ a point at coordination (0,0) \}
\end{center} 

\begin{center}
$p_0$ = $\rho(p_v, p_b)$
\end{center} 
This is concretization of vertical line related to point (0,0). 

And, more:
\begin{center}
$p$ = $p_0 \sqcap p_c$
\end{center} 
This is one pattern with one vertical line and a point at (3,3). The pattern $p$ is AND of 2 different types of patterns. This is one example that we have to make all operations and operators on patterns typeless.

Let's try to put the above equations together, we then have:
\begin{center}
$p$ = $\rho(\alpha$(all vertical lines), \{ a point at coordination (0,0) \}) $\sqcap$ \{ a point at coordination (3,3) \}
\end{center}

Might be easier to just state: a vertical line pass through (0,0) and a point at (3,3). But, as we can see, the above equation describe the pattern much more precisely and mechanically (i.e. to avoid to use language, either natural language or programming language, just use our simple and mechanical terms: $\alpha$, $\rho$, $\sqcup$, $\sqcap$, $\neg$).  


We examined some simple examples above. Though simple, they are very revealing. From these examples, we can see some important properties of patterns. First, patterns are more than base patterns, much more. Second, some patterns together could generate new pattern. There are many ways to generate new patterns, such as OR, AND, NOT, abstraction, concretization, and more. Third, very crucially, we realize that some patterns are independent from learning machine, while some depend on learning machine heavily. In other words, for a learning machine, some patterns are objective, while some are subjective. 
\bigskip


{\bf Pattern, Objectively } \\
First, we want to discuss pattern that is objective to learning machine. Base pattern is the foundation for all patterns. We defined it before. But, we repeat it again here for easy to cite. 


\begin{definition}[\bf Base Pattern Space]
$N$-dim base pattern space, denote as $PS^0_N$,  is a $N$-dim binary vector space, i.e.  
\end{definition}
\begin{center}
$PS^0_N$ = $\{ all \  p = (p_1, p_2, \ldots, p_N) | \  p_k = 0 \ or \ 1 \}$ 
\end{center}

Each element of $p \in PS^0_N$ is a base pattern. There are totally $2^N$ patterns in $PS^0_N$. When $N$ is not very small, $PS^0_N$ is a huge set. Actually, this hugeness is the source of richness of world and fundamental reason of difficulty of learning.

Base pattern space is just the starting point of our discussion. From above examples, we know that many patterns are not base pattern. But, if a pattern is not base pattern, what is it? We can see in this angle: no matter what a pattern is, what is presented to input space of a learning machine is a base pattern. So, naturally, we have definition below.


\begin{definition}[\bf Pattern as Set of Base Patterns]
A $N$-dim pattern $p$ is a set of base patterns: 
\end{definition}
\begin{center}
$p$ = \{ $b_1, b_2, \cdots | b_i \in  PS^0_N$ \}
\end{center}
We can denote this set as $p_b$, and call is as the base set of $p$ (b stands for base). While we use $p$ as the notation of a pattern, we understand it is a set of base patterns. If we want to emphasis it is a set of base patterns, we use notation $p_b$. We also can write $p = p_b$. Any base pattern in base set is called a base face of $p$ (or just simply face). For example, in above, $b_2$ is one face of $p$. Specially, any base pattern $b$ is one pattern, and it is the (only) base face of itself. 

According to this definition, a pattern is independent from learning machine, which is just a group of base patterns, no matter what a learning machine is. If we want to view pattern objectively, the only way is to define a pattern as a group of base patterns. So, objectively, a pattern is a set of base patterns. 

What objective operators on objective patterns are? Since patterns are set of base patterns, naturally we first examine basic set operations: union, intersection, and complement.


\begin{definition}[\bf Operator OR (set union)]
Based on any 2 patterns $p_1$ and $p_2$ , we have a new pattern $p$: 
\end{definition}
\begin{center}
$p$ = $p_1$ OR $p_2$ = $p_{1_b} \cup p_{2_b}$ 
\end{center}

Here, $\cup$ is the set union. That is to say, this new pattern is such a pattern whose base set is the union set of base sets of 2 old patterns. Or, we can say, $p$ is such a pattern whose face is either a face of $p_1$ or a face of $p_2$. 



\begin{definition}[\bf Operator AND (set intersection)]
For any 2 patterns $p_1$ and $p_2$ , we define a new pattern: 
\end{definition}
\begin{center}
$p$ = $p_1$ AND $p_2$ = $p_{1_b} \cap p_{2_b}$ 
\end{center}
 
Here, $\cap$ is the set intersection. Or we can say, $p$ is such a pattern that its face is both face of $p_1$ and $p_2$. In this sense, we say, $p$ is both $p_1$ and $p_2$. 



\begin{definition}[\bf Operator NOT (set complement) ]
For any patterns $p$ , we define a new pattern: 
\end{definition}
\begin{center}
$q$ = NOT $p$ = $p_{b} ^c$ = $\{ b \in PS^0_N | b \not \in p_{b} \}$  
\end{center}

Here, $A^c$ is complement set of $A$. That is to say, $q$ is such a pattern that its face is not a face of $p$.  

Very clearly, the above 3 operators do not depend on learning machine. So, they are all objective. Consequently, if we apply these 3 operators consecutively any times, we still generate a new pattern that is objective. 
\bigskip



{\bf Pattern, Subjectively} \\
Now we turn attention to subjective pattern, i.e. pattern to be viewed from a particular learning machine.

We need to go back for a while and consider basic. When we say there is an incoming pattern $p$ to a learning machine, what do we mean? If we see this objectively, the meaning is clear: at input space, a binary vector is presented, which is a face of the incoming pattern $p$. This does not depend on learning machine at all. And, this is very clear and no unambiguity.  

However, as our examples demonstrated, we have to consider pattern subjectively. We need to go slowly since there are a lot of confusing here. We have to consider something that is not valid at all objectively. 

{\bf Pattern, 1-significant or 0-significant }\\
First, when we discuss patterns subjectively, we need to know: Is 1 significant? or 0 significant, or both are equally significant?

Does this sound wrong? By definition, a base pattern is a binary vector, so, of course, both 0 and 1 would be equally significant. Why consider 1-significant, or 0-significant? Let's consider one simple example. For 4-dim pattern, $p_1 = (1, 1, 0, 0)$ is one base pattern, and could be viewed as one horizontal line (see example 2 and Fig. 2). $p_2 = (0, 1, 0, 1)$ is also one base pattern, and could be viewed as one vertical line. When we talk about $p_1$ and $p_2$ appears together (or happen together), do we mean this pattern: (1, 1, 0, 1), or (0, 1, 0, 0)? Former one is 1-significant, and latter is 0-significant. So, if we want to use the term such as "2 pattern happen together", it is necessary to distinguish 1-significant and 0-significant. 

So, to distinguish 1-significant pattern or 0-significant pattern indeed makes sense, and is necessary. When we consider a pattern as 1-significant, we often look at its 1 components, not pay much attention to its 0 components, just as we did in the example: "(1, 1, 0, 1) equals (1, 1, 0, 0) and (0, 1, 0, 1) appear together". Contrast, we do not think:  "(0, 1, 0, 0) equals (1, 1, 0, 0) and (0, 1, 0, 1) appear together", since we do not consider 0-significant. 

Perhaps, 1-significant is actually already in our sub-conscious. Just see which sentence is more appealing to us: "(1, 1, 0, 1) equals (1, 1, 0, 0) and (0, 1, 0, 1) appear together", or "(0, 1, 0, 0) equals (1, 1, 0, 0) and (0, 1, 0, 1) appear together". 

Additional to the above consideration, most patterns that people consider for many applications are sparse pattern, i.e. only a few bits in the pattern are 1, most are zero. For sparse patterns, 1-significant is very natural choice. In fact, in sparse pattern, 1-significant is very natural. Just see this example:
\begin{center}
(1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1) =  \\ (1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0)  and (0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1) appear together.
\end{center}
We would accept this statement easily. 
From now on, unless we state explicitly, we will use 1-significant. \\


{\bf Patterns and Learning Machine} \\
From the examples, we know that one pattern $p$ could be perceived very differently by different learning machine. This make us consider this question carefully: from the view of learning machine, what really is a patterns? We have not really addressed this crucial question yet, we just let our intuition play at the background. In Example 2, when we talk about $\sqcap$ operator, and give an equation $p$ = $(1,1,0,0) \sqcap (1,0,1,0)$, we did not really tell what is this pattern $p$. Now we address this more carefully.

Take a look at this: {\it \{ one pattern that both (1,1,0,0) and (1,0,1,0) appears together \} }. In our tuition, this is a right thought. However, if we see things objectively, this is simply wrong: Base patterns (1,1,0,0) and (1,0,1,0) cannot appears together. They are different base patterns. At one time, only one of them can appear. In this sense, "together" cannot happen.

To address this question, we have to going deep to see what a pattern really is. When we talk about base pattern, i.e. binary vector in $PS^0_N$, there is no unambiguity. Everything is very clear. However, just base pattern is not good enough. With only base patterns, we simply cannot handle most things that we want to work with. 

At this point, we should be able to realize that pattern is not only associated with what is presented at input space (surely base pattern), but also associated with how a learning machine perceives incoming pattern. For example, when base pattern (1,1,1,0) is at input, the learning machine could perceive it is just one base pattern, but also could perceive it as two base patterns (1,1,0,0) and (1,0,1,0) appear together, or could perceive much more, much more complicated. 

So, naturally, a question arise: can we define pattern without introducing perception of learning machine? Yes, this can be done. Since no matter what pattern is, when a pattern is sent to learning machine, it is one base pattern at input space. In this way, we can surely define a pattern to be a set of base patterns. So, no matter what learning machine is and how it perceives, pattern is a set of base patterns. This is just objective pattern. For example, we can forcefully define  {\it \{ one pattern that both (1,1,0,0) and (1,0,1,0) appears together \} } as the set of base patter  \{ (1,1,1,0) \}. This is what we did in above section. 

Seems this way resolves unambiguity. However, as all examples indicated, objective way cannot go far and we need to understand patterns subjectively. Pattern cannot be separated from how a learning machine perceives. Pattern defined as a set of base patterns is precise, but how a learning machine perceives patterns is much more important. Without learning machine perceiving, actually, no matter how precise a pattern is, it is not much useful.

Here, it is worth to stop and review our thoughts here. The major point is: learning machine plays an active role, and it must have its own way to see its outside world. More precisely, a learning machine must have the ability to tell what is outside of itself, and what is inside of itself, and what is its view to outside. With or without such ability is very critical. Only with this ability, the learning machine can go further and our later discussions can be conducted. It is very important we realize this. Without such ability, a learning machine is reduced to an ordinary computer program that is very hard to do learning. From now on, our learning machine will have such ability and we will make the ability more clearer. So, patterns would be mainly subjective to a learning machine. 

Thus, we have to address this critical issue: how a learning machine perceives pattern? And we need to see this by considering relationship among patterns. We need to think these issues as well: 1) how to form new pattern from old pattern? 2) how to associate new pattern with prior learned patterns? 3) how to organize learned patterns? 4) how to re-organized learned patterns? In order to do these, we have to see how machine perceives. 
\bigskip




{\bf How Learning Machine Perceives Patterns} \\
How a learning machine perceives pattern is closely related to how it processes information. So we go back to IPU for a while. Consider a $N$-1 IPU $\mathcal{M}$, suppose its processing is $\mathcal{P}$. We define of black set:


\begin{definition}[\bf Black set of $N$-1 IPU]
For a $N$-1 IPU $\mathcal{M}$, if its processing is $\mathcal{P}$, the black set of $\mathcal{M}$ is:
\begin{center}
$B$ = $\{ b \in PS^0_N | \mathcal{P}(b) = 1 \}$ 
\end{center}
\end{definition}
Equivalently, we also call $B$ as the black set of processing $\mathcal{P}$.

For IPU $\mathcal{M}$, suppose $B$ is its black set, this means: if we put one base pattern $b \in B$ to input space, $\mathcal{M}$ will process it to 1, if $b \notin B$, to 0. This reveals one important fact to us: inside $\mathcal{M}$, there must exist a bit $pb$ with such property: if input $b \in B$, $pb = 1$, if $b \notin B$, $pb = 0$. 

We do not know exactly what inside $\mathcal{M}$ is, and we do not know how exactly the processing is done. However, we do know such a bit $pb$ must be there. We do not know where this bit $pb$ is and exists in what form, but we know it exists. Otherwise, how can $\mathcal{M}$ be able to distinguish input from $B$ or not from $B$? Such bit $pb$ reflects how $\mathcal{M}$ process input to output. We can imagine that $\mathcal{M}$ could have more such bits. So, we have definition.


\begin{definition}[\bf Processing Bits]
For IPU $\mathcal{M}$, if it has some internal bit $pb$ has such properties: there is a set $B \subset PS^0_N$ so that for any $b \in B$, $pb = 1$ (light), for any $b \not\in B$, $pb = 0$ (dark), we call bit $pb$ as one processing bit. If $\mathcal{M}$ has more than one such bit, say, $pb_j, j=1, \ldots, L$ are all processing bits, we call such set as set of processing bits of  $\mathcal{M}$, or just simply, processing bits.
\end{definition}


\begin{theorem}
For a IPU $\mathcal{M}$, set of processing bits $\{pb_j \ |\ j=1, \ldots, L\}$ exists and is not empty.
\end{theorem}
{\bf Proof: } We will exclude 2 extreme cases, i.e. $\mathcal{M}$ maps all input to 0 vector $(0, 0, \ldots, 0)$, and $\mathcal{M}$ maps all input to 1 vector $(1, 1, \ldots, 1)$. After excluding the 2 extreme cases, we can say, black set $B$ of $\mathcal{M}$ is a proper subset of $PS^0_N$, so does $B^c$. Thus, as we argued above, there must exist a bit $pb$ inside $\mathcal{M}$ with such property: for $b \in B$, $pb = 1$, for $b \notin B$, $pb = 0$. So, set of processing bits indeed exists and not empty.
 
In proof, we show that set of processing bits are not empty, at least there is bit in it. Such case indeed exists. There are IPU whose set of processing bits has only one elememt. But in most cases, set of processing bits has more than one element. In fact, $L$, the number of processing bits, can reflect the complexity of IPU. Processing bits reflects how processing of IPU is conducted.  



Since a learning machine is also a IPU, it has processing bits as well. But, as we discussed before, how a learning machine perceives pattern is closely related to how it process input. So, for learning machine, we will call these bits as perception bits, instead of processing bits. When one base pattern is put into input, each perception bit will take its value. All these values together, we have perception values. Perception values reflects how a learning machine perceives this particular base pattern. If a learning machine is learning, its perception bits could chang, the number perception bits could increase or decrease, its behavior could change. Even the array of perception bits might not change, the behavior could change. 

Armed with perception bits, we can describe how $\mathcal{M}$ perceives pattern. When a base pattern $b$ is put into input space, perception bits act, some are light and some are dark. These bits reflect how $b$ is perceived, i.e. the perception bits $\{pb_j \ |\ j=1, \ldots, L\}$ are taking values, we have a binary vector $pv = (pv_1, pv_2, \ldots, pv_L)$, where $pv_j$ is value (1 or 0) of $pb_j$ takes. We call them as perception values. Note, the perception values depends on a particular base pattern. The perception values tells how $\mathcal{M}$ perceives a base pattern $b$. 

If $b$ and $b'$ are 2 different base patterns, i.e. they are different bit-wise, but they have same perception values, we know that these 2 base patterns are perceived as same by $\mathcal{M}$, since $\mathcal{M}$ has no way to tell any difference between $b$ and $b'$. If 2 base patterns are possibly perceived different by $\mathcal{M}$, their perception values must be different (at least one perception bit must behaves differently). 


However, reverse is not true. It is possible that 2 base patterns $b$ and $b'$ have different perception values, but $\mathcal{M}$ still could perceives $b$ and $b'$ as same subjectively. That is to say, $\mathcal{M}$ can perceives 2 different base patterns as same even their perception values are different. So we have definition below.


\begin{definition}[\bf Base patterns perceived same by a learning machine subjectively]
Suppose $\mathcal{M}$ is a learning machine, $\{pb_j \ |\ j = 1, \ldots, L\}$ are perception bits, if for 2 base patterns $b_1$ and $b_2$, their perception values are $(pv^1_1, pv^1_2, \ldots, pv^1_L)$ and $(pv^2_1, pv^2_2, \ldots, pv^2_L)$, and at least for one $k$, $pv^1_k = pv^2_k = 1$, we say, at perception bit $pb_k$, $\mathcal{M}$ could subjectively perceives $b_1$ and $b_2$ as same. 
\end{definition}
That is to say, for 2 base patterns, if at any perception bit, their perception values are different, learning machine is not possible to perceive them same. But, if at least at one perception bit, their perception values are both 1, $\mathcal{M}$ could possibly perceive them as same subjectively. Of course, $\mathcal{M}$ could also perceive them as different subjectively. Note, perception value should be 1, not 0. This is related to 1-significant. 


\begin{definition}[\bf Pattern perceived by a learning machine subjectively]
Suppose $\mathcal{M}$ is a learning machine, $\{pb_j \ |\ j = 1, \ldots, L\}$ are perception bits. And suppose $p$ is a group of base pattern, and  at perception bit $pb_k, 1\le k \le L$, perception value for all base patterns in $p$ equals 1, then $\mathcal{M}$ could perceive all base patterns of $p$ as same, and if so, we say $\mathcal{M}$ perceives $p$ as one pattern subjectively at $pb_k$, and $p$ forms a subjective pattern. 
\end{definition}
Note, in definition, it only needs that all base patterns in $p$ behaves same at one perception bit. This is minimal requirement. Of course, this requirement could be increased. For example, to require at all perception bits behaving same. But, all requirements are subjective.



Here we put down the major points about subjective patterns and how a learning machine to perceive them.
\begin{enumerate}
\item  There are perception bits in a learning machine (only exclude 2 extreme cases). Any system that satisfies the definition of Iearning machine must have perception bits. How perception bits are formed and how exactly perception bits are realized inside a learning machine could be different greatly. But we emphasis that perception bits indeed exist. 

\item These bits are very crucial for a learning machine. They reflect how learning machine perceive and process patterns. When a base pattern is put into input space of learning machine, then perception bits act and the learning machine uses these values to perceive pattern subjectively, and process pattern accordingly. 

\item For learning machine, its perception bits are changing with learning. However, even the number of perception bits are not changing, the behavior of perception bits could change (so does the perception of learning machine). 

\item Armed by perception bits, we can well understand subjective pattern. If 2 base patterns behave same at one perception bit, then, the 2 base patterns can be perceived as same at this perception bit subjectively. This can be extended to more than 2 base patterns. For a group of base patterns $p$, and if all base patterns behave same at one perception bit, then $p$ can be perceived as same at this perception bit subjectively. This is the way to connect objective and subjective.
\end{enumerate}

To consider pattern objectively, only need to involve set operation, no need to do any modification on learning machine itself. But, to consider pattern subjectively, set operation could be used. But more importantly, perception bits are needed. And, quite often, to modify perception bits is necessary. For subjective operator of subjective patterns, we need to base our discussion on perception bits.
\bigskip




{\bf Pattern, Subjective Operators} \\
Just as operators for objective patterns, it is naturally to consider subjective operators for subjective patterns. There are 3 very basic operators: NOT, OR, AND. First, consider NOT.

\begin{definition}[\bf Operator NOT for a Pattern, Subjectively]
Suppose $\mathcal{M}$ is a learning machine, $\{pb_j \ |\ j = 1, \ldots, L\}$ are perception bits. For a subjective pattern $p$ perceived at $pb_k$ by $\mathcal{M}$, $q$ is another pattern perceived at $pb_k$ by $\mathcal{M}$ in this way: $q$ are all such base patterns that is perceived by $\mathcal{M}$, and at $pb_k$, the perception value is 0. 
\end{definition}
We can denote this pattern $q$ as $q$ = NOT $p$ or $q = \neg p$. This notation $\neg$ is following \cite{mkrot}. We can also say, pattern $q$ is a pattern that $p$ does not appear. 

Note, this operation NOT is subjective. $q$ consists of base patterns that are perceived by $\mathcal{M}$. So, this is quite different than the objective operation NOT (set complement). Another important point is: in order to do this operator, no need to modify perception bits of $\mathcal{M}$, only perception value is different. 

Now we turn attention to another operator OR. Consider that we have a subjective pattern $p_1$, and the perception values of $p_1$ are $pv^1_1, \ldots$, and subjective pattern $p_2$, and the perception values of $p_2$ are  $pv^2_1, \ldots$. Since $p_1$ and $p_2$ are different pattern, their perception values must be different at some bits. Now, we want to put them together to form a new pattern, i.e. $p = p_1 OR p_2$, which measn either $p_1$ or $p_2$. This action of course changes the perception of $\mathcal{M}$ and must change the perception. If the perception is not changed, there is no way to have OR. So, when we introduce the OR operator, we in fact change $\mathcal{M}$. This is what subjective really means: learning machine changes its perception so that $p_1$ and $p_2$ are treated same, though $p_1$ and $p_2$ indeed have difference, and the difference is ignored. 


\begin{definition}[\bf Operator OR for 2 Patterns, Subjectively]
Suppose $\mathcal{M}$ is a learning machine, $\{pb_j \ |\ j = 1, \ldots, L\}$ are perception bits. For any 2 subjective patterns $p_1$ and $p_2$, $p_1$ perceived at $pb_{k_1}$ by $\mathcal{M}$, and $p_2$ perceived at $pb_{k_2}$ by $\mathcal{M}$, $p$ is another subjective pattern, and perceived by $\mathcal{M}$ in this way: first $\mathcal{M}$ will modify its perception bits if necessary, then $\mathcal{M}$ perceive any base patterns from either $p_1$ or $p_2$ at another perception bit $pb_l$ same. That is to say, if $pb_l$ does not exist, $\mathcal{M}$ will generate this perception bit first.
\end{definition} 
We can also say, new pattern $p$ is either $p_1$ or $p_2$ appears. We can denote this new pattern as $p = p_1 OR p_2$ = $p_1 +_s p_2$. This notation $\neg$ is following \cite{valiant}.

Note, if we want to do operation OR, we might need to modify perception bits of $\mathcal{M}$. This is often done by adding a new perception bit. This is totally different from the objective OR (or set union). On surface, $p_1 + p_2$ indeed is a union (set union) of $p_1$ and $p_2$. But, without modification of perception bits, there is no way to do this union.   




Then consider subjective operator AND. This operator is crucially important. Actually, we spent a lot of time to argue about this operator, i.e. {\it appears together}. 


\begin{definition}[\bf AND Operator for 2 Base Patterns, Subjectively]
Suppose $\mathcal{M}$ is a learning machine, $\{pb_j \ |\ j = 1, \ldots, L\}$ are perception bits. If  $p_1$ is one subjective patterns perceived at $pb_{k_1}$, $p_2$  is one subjective patterns perceived at $pb_{k_2}$, then, all base patterns that $\mathcal{M}$ perceives at both $pb_{k_1}$ and $pb_{k-2}$ at the same time will form another subjective pattern $p$, and $p$ is perceived by $\mathcal{M}$ at $pb_l$. That is to say, if $pb_l$ does not exist, $\mathcal{M}$ will generate this perception bit first.
\end{definition}
We can also say, new pattern $p$ is both $p_1$ and $p_2$ appear together. We can denote this pattern $p$ as $p = b_1 \ AND \ b_2$ = $b_1 \cdot b_2$. This notation $\cdot$ is following \cite{valiant}.

Note, if we want to do AND operator, we have to modify perception bits of $\mathcal{M}$. This is totally different from the objective AND (or set intersection). 
\bigskip





{\bf X-Form} \\
We have setup 3 subjective operators for subjective patterns. If applying the 3 operators consecutively, we will have one algebraic expression. Of course, in order this algebraic expression makes sense, learning machine needs to modify its perception bits. But, we want to know what we can construct from such algebraic expressions? First, we see some examples.

{\bf Example 3.4 [\bf One Simple X-form ]}
Suppose $b_1, b_2, b_3$ are 3 different base patterns. Then,
$$
E(b_1, b_2, b_3) = b_1 + (b_ 2 \cdot b_3)
$$
is one subjective pattern. We can say, this pattern is: either $b_1$ or $b_2$ and $b_3$ happen together. However, the expression has more aspects. Since $E$ is one algebraic expression, we can substitute base patterns into it, and get one value. This is actually what algebraic expression for. That is to say, $E(b) = b_1 + b_3$ is one mapping on $PS_N^0$ to \{0, 1\}, and it behaves like this: for any $b \in PS_N^0$, if $b = b_1$ or $b \in b_2 \cdot b_3$, $E(b) = 1$, otherwise $E(b) = 0$. This matches our intuition well.  


{\bf Example 3.5 [\bf More X-forms ]}
If $g = {b_1, b_2, \ldots, b_K}$ is a group of base patterns, and we have some algebraic expressions, we get more subjective patterns based on $g$. See some example here: 
$$
b_1 + b_3, \ b_2 + (\neg b_1 + b_2 \cdot b_5), \ (b_2 + (b_1 \cdot b_2)) \cdot (b_3 + b_4), \ \ldots
$$
are subjective pattern. But, also these expressions can be used to define a mapping on  $PS_N^0$ to \{0, 1\}, just like above. 


{\bf Example 3.6 [\bf Prohibition ]}
If $e_1$ and $e_2$ are expressions, we want to find an expression for this situation: $e_2$ prohibits $e_1$, i.e. if $e_2$ is light, output has to be dark, otherwise, output equals $e_1$. This expression is:
$$
\neg (e_1 \cdot e_2) + (e_1  \cdot \neg e_2)
$$
are subjective pattern.




Above, each expression has 2 faces: first, it is one algebraic expression, second, it is one subjective pattern perceived by $\mathcal{M}$. In order to make sense for these expressions, $\mathcal{M}$ has to modify its perception bits accordingly. This is crucial. Thus, we have following definition.


\begin{definition}[\bf X-Form for patterns]
If $E$ is one algebraic expression of 3 subjective operators, and $g = \{b_1, b_2, \ldots, b_K\}$ is a group of base patterns, then we call the expression $E(g) = E(b_1, b_2, \ldots, b_K)$ as an X-form upon $g$, or simply X-form. We note, in order to have this expression make sense, quite often, learning machine $\mathcal{M}$ needs to modify its perception bits accordingly. And, if this expression make sense, we then have a subjective pattern $p = E(g) =  E(b_1, b_2, \ldots, b_K)$.
\end{definition}
The name X-form is chosen for reason: these expressions are forms, and we do not know them well, and X means unknown. In \cite{valiant}, there are similar form called conjunction normal form (CNF). Though, our expression are quite different than CNF of Valiant. CNF of Valiant is basically objective, while X-forms are subjective. 

One important aspect of X-form is: it is one algebraic expression, so, we can substitute variable into and calculate to get output value, 0 or 1. See above examples. In this sense, one X-form would be a mapping on $PS_N^0$ to \{0, 1\}. The calculation of this expression is actually same as learning machine doing processing inside itself. This is one wonderful property. This is exactly the reason why we introduce the construction of X-form. In this way, one X-form can be thought as one processing. Thus, we can also think one X-form has a black set, which is exactly equals the subjective pattern of this X-form.   

In order to connect objective patterns, subject patterns, and X-form, we have following theorem.



\begin{theorem}
Suppose $\mathcal{M}$ is a $N$-1 learning machine. For any objective pattern $p_o$ (i.e. a set of base patterns in $PS_N^0$),  we can find some algebraic expression $E$ upon some group of base patterns $g = {b_1, b_2, \ldots, b_K}$ so that $p_o = E(g)$. If so, we say X-form $E(g)$ express $p_o$. In most cases, there are many X-form to express $p_o$. However, among those X-forms, we can find at least one so that it base upon no more than $N$ base patterns, i.e. in $g = \{b_1, b_2, \ldots, b_K\}$ $K \le N$. 
\end{theorem}
{\bf Proof:} Suppose $p_o$ is one objective pattern. It is easy to see there is one algebraic expression $E$ can express $p_o$. Since $p_o$ is a set of base patterns, surely we can write $p_o$ as:
$$
p_o = \{b_1, b_2, \ldots, b_L \}, \ b_i \in PS_N^0
$$ 
where each $b_i$ is a base pattern. The algebraic expression
\[
E_1(b_1, b_2, \ldots, b_L) = b_1 + b_2 + \ldots + b_L   \label{eq:ex1} \tag{1}
\]
can express $p_o$, since we can easily see $p_o = E_1(b_1, b_2, \ldots, b_L)$. If $L$ is not bigger than $N$, we already find such a group of base pattern and such an algebraic expression, and proof is done. 

If $L$ is bigger than $N$, we can do further. For one base pattern $b$, we can find some other base patterns $b'_1, b'_2, \ldots, b'_J$, $J \le N$, and to express $b$ in this way:  $b = b'_1 \cdot  b'_2 \cdot \ldots \cdot b'_J$. Such $b'_1, b'_2, \ldots$ sure can be found. For example, if $b = (1, 1, 0, \ldots, 0)$, we can find $b'_1 = (1, 0, \ldots, 0)$ and $b'_2 = (0, 1, 0, \ldots, 0)$, then $b = b'_1 \cdot b'_2$. 

 
For a group of base patterns, we can do same. That is to say, for $b_1, b_2, \ldots, b_L$, we can find There are at most $N$ base patterns $b'_1, b'_2, \ldots, b'_K$, $K \le N$ so that for each of $b_j, j = 1, 2, \ldots, L$, we can  find some $b'_{j_1}, \ldots, b'_{j_{K_j}}, K_j \le K$, and $b = b'_{j_1} \cdot \ldots \cdot b'_{j_{K_j}}$. We know such a group base patterns indeed exists. For example, $b'_1 = (1, 0, \ldots, 0, 0), \ldots, b'_N = (0, 0, \ldots, 0, 1)$ are such a group.


Now, we can continue: 
\begin{multline}
p_o = E_1(b_1, b_2, \ldots, b_L) = b_1 + b_2 + \ldots + b_L =  \\ 
(b'_{1_1} \cdot \ldots \cdot b'_{1_{K_1}}) + (b'_{2_1} \cdot \ldots \cdot b'_{2_{K_2}})  +  \ldots +  (b'_{L_1} \cdot \ldots \cdot b'_{L_{K_L}}) = \\
E(b'_1, b'_2, \ldots, b'_K)  \label{eq:ex2}  \tag{2}
\end{multline}

This algebraic expression $E$ and a group of base patterns $b'_1, b'_2, \ldots, b'_K$, and $K \le N$, are what we are looking for. We should note, expression \eqref{eq:ex1} is "level 1" expression, while \eqref{eq:ex2} is "level 2" expression. We can do for higher level expressions. 


Of course, the expression in the proof is just used to do the existence proof. It is not best expression. This expression is very "shallow". We can push the expression to higher level. But, here we do not discuss how to do so.


Theorem 4 tells the relationship of objective pattern and subjective pattern. For any objective pattern $p_o$, we can find a good group of base patterns (size of this group is as small as possible, at worst, not greater than $N$), and a good algebraic expression, to express this objective pattern as one subjective pattern. 

Here is the major point. One objective pattern $p_o$ is a set of base patterns. However, when $p_o$ is perceived by a learning machine, learning machine generates a subjective pattern. The major question is: will the subjective one match with objective one? Theorem 4 confirm that, yes, for any objective pattern $p_o$, we always can find X-form to express $p_o$. 

Naturally, next we would ask, how well such expression is? For "how well", we need some criteria. There could have many such criteria. However, this criteria is very important: use as less as possible base patterns, i.e. in $p_o = E(b) = E(b_1, b_2, \ldots, b_K)$, $K$ is as small as possible. There could have other important properties of X-form. To satisfy these properties, we can get a better X-form.

Of course, next question is how to really find or construct such X-form. That is what we do next. 
\bigskip



{\bf Sub-Form} \\
Several X-form could form a new X-form. And, some part of a X-form is also a X-form. Such part could be quite useful. So, we discuss sub-form here. 



\begin{definition}[\bf Sub-Form of a X-form]
Suppose $e$ is a X-form, so, it is one algebraic expression $E$ (of 3 subjective operations) upon a set of base patterns $g = \{b_1, b_2, \ldots, b_K\}$ so that $e = E(g) = E(b_1, b_2, \ldots, b_K)$. A sub-form of $e$ is one algebraic expression $E_s$ upon a subset of $g$, $g_s = \{b_{s_1}, \ldots, b_{s_J}\}$,  $J \le K$, so that $e_s = E_s(g_s) = E_s(b_{s_1}, \ldots, b_{s_J})$, and the objective pattern expressed by $e_s$ is a proper subset of the objective pattern expressed by $e$.
\end{definition}
So, by definition, a sub-form is also a X-form. 

{\bf Example 3.7 [\bf  Sub-Form]}
1. $e = b_1 + b_2$ is one X-form. Both $b_1$ and $b_2$ are sub-form of $e$. \\
2. $e = b_1 + b_2 \cdot b_2$ is one X-form. Both $b_1$ and $b_2 \cdot b_3$ are sub-form of $e$. But, $b_2$ (or $b_3$) is not sub-form of $e$. \\
3. $e = (b_1 + b_2) \cdot (b_1 + b_3)$ is one X-form. We can see that the black set of $e$ is $\{b_1, b_1 \cdot b_2, b_1 \cdot b_3, b_2 \cdot b_3 \}$. So, $b_1$ is sub-form of $e$, but $b_2, b_3$ are not.   



One X-form $e$ could have more than one sub-form. Or one X-form has no sub-form. For a sub-form, since it is one X-form, it could have sub-form for itself. So, we can have sub-forms for sub-forms, and go on. It is easy to see, any sub-form of sub-form is still a sub-form. So, X-form could have many sub-form. We denote all sub-forms as $e_i, i = 1, 2, \ldots L$. These sub-form are play important roles. They are actually fabric of processing. 


