In the discussions of learning machine, we introduced our view on how mechanical learning is conducted. In this Appendix, we would like to use this view to see deep learning. Hopefully, from this special angle, we can see some interesting structure and details of deep learning. 

According to our definition, deep learning is mechanical learning, if without human intervene. Of course, this "if" is a big if. Often, deep learning program is running with a lot of human intervene. Actually, we introduce the term "mechanical learning" is for this purpose: to isolate a learning machine from human intervene so that we can discuss details of a learning machine. 

We would like to restrict our discussion to Hinton's original model \cite{hinton}, i.e., a stack of RBMs. Each level of RBM is clearly a $N$-$M$ learning machine ($N$ and$M$ are dimensions of input and output). Hinton's deep learning model is by stacking RBM together. If without further human intervene, it is a learning machine. This is the original model of deep learning. Other deep learning program can be thought as variations based on this model. Though in the past few years deep learning has leaped forward greatly, stacking RBM together is still the most typical deep learning model.  

Thus, we would expect many things we discussed could be applied to deep learning. The point is, we are viewing it from a quite different angle -- angle of mechanical learning. For example, we can view Hinton's original deep learning program \cite{hinton} as one 258-4 learning machine, and we ask what is the internal representation space of it? We expect such angle and questions would reveal useful things for us.

The central questions indeed are: what is the internal representation space of deep learning? what is the learning dynamics? At first, seems it is quite hard since learning is conducted on a huge parameter space, and learning methods are overwhelmingly a big body of math. However, when we apply the basic thoughts of learning machine to deep learning, starting from simplest RBM, i.e. 2-1 RBM, we start to see much more clearly.
\bigskip


{\bf 2-1 RBM} \\
2-1 RBM is the simplest. However, it is also very useful since we can examine all details, and such details will give us a good guide on more general RBM. 

2-1 RBM is one IPU. We know 2-1 IPU totally has 16 processing ($2^{2^2}$). But, we only consider those processing: $p(0, 0) = 0$, so totally 8 processing, which we denote as $P_j, j=0, \ldots, 7$ (see \cite{paper1}). For 2-1 RBM, any processing $p$ can be written as: for input $(i_1, i_2)$, the output $o$ is:
$$
o = p(i_1, i_2) = \text{Sign}(a i_1 + b  i_2), \text{where} \ (a, b) \in \mathbb{R}^2, \ \text{Sign}(x) =
\left\{
	\begin{array}{ll}
		1  & \mbox{if } x \ge 0 \\
		0  & \mbox{if } x < 0
	\end{array}
\right.
$$

%\begin{comment}

The parameters $(a, b)$ determine what the processing really is. Parameter space $\mathbb{R}^2$ has infinite many choices of parameters. But, there are only 6 processing, thus, for many different parameters, the processing is actually same. We can see all processing in below table:

\begin{center}
%\begin{table}[h3]
\centering
    \begin{tabular}{|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|}
        \hline
        ~     & $P_0$ & $P_1$ & $P_2$ & $P_3$ & $P_4$ & $P_5$ & $P_6$ & $P_7$  \\ \hline
        (0,0) & 0     & 0     & 0     & 0     & 0     & 0     & 0     & 0       \\ 
        (1,0) & 0     & 1     & 0     & 1     & 0     & 1     & 0     & 1       \\ 
        (0,1) & 0     & 0     & 1     & 0     & 1     & 1     & 0     & 1       \\ 
        (1,1) & 0     & 0     & 0     & 1     & 1     & 0     & 1     & 1       \\
        \hline
          Region   & $R_4$ & $R_3$ & $R_5$ & $R_2$ & $R_6$ & None & None & $R_1$   \\	
        \hline
          X-form    & $0$ & $b_1$ & $b_2$ & $b_1 + b_3$ & $b_2 + b_3$ & $b_1 + b_2$ & $b_3$ & $b_1 + b_2 + b_3$   \\	
        \hline
    \end{tabular}
%\end{table}

{\bf Tab. 1.  Table for all processing of 2-1 RBM} 
\end{center}


%\end{comment}


\begin{center}
\begin{picture}(220,250)(0,0)
\put(0, 10){\resizebox{9 cm}{!}{\includegraphics{Fig3-rbm21.pdf}}}
\end{picture}

{\bf Fig. 1. Parameter space and regions of 2-1 RBM} 
\end{center}

In first row of table, $P_i, i = 0, \ldots, 7$ are all processing of 2-1 IPU. Under first row, there is value table for each processing. We point out some quite familiar processing: $P_7$ is actually logical OR gate, $P_6$ is logical AND gate, $P_5$ is logical XOR gate. Note, $P_5, P_6$ are processing for 2-1 IPU, but not in 2-1 RBM. It is well known, 2-1 RBM has no XOR and AND (i.e. no $P_5, P_6$).

$R_j$, $j=1, \ldots, 6$ indicate regions in parameter space $\mathbb{R}^2$, each region for one processing. There are only 6 regions, since 2-1 RBM only has 6 processing. We briefly discuss how we get these regions. See illustration in figure.

Suppose $p$ is processing. Line $a = 0$ cuts $\mathbb{R}^2$ into 2 regions: $a \ge 0$ and $a < 0$. If $(a, b)$ is in first region, then $p(1, 0) = 1$, in second, $p(1, 0) = 0$. Line $b = 0$ is perpendicular to $a=0$, so, it cuts the previous 2 regions into 4 regions: $a \ge 0, b \ge 0$ and $a \ge 0, b < 0$ and  $a < 0, b \ge 0$ and $a < 0, b < 0$. Clearly, if $b \ge 0$, $p(0, 1) = 1$, if $b < 0$, $p(0, 1) = 0$. Line $a + b = 0$ could no longer cuts the previous 4 regions into 8 regions, it could only cut 2 regions (2nd, 4th quadrant) into 4 regions ($R_2, R_3, R_5, R_6$). So, totally, we have 6 regions, and each region is for one processing. This argument about regions is very simple, yet very effective. We can extend this argument to $N$-1 RBM.
 
Each region is for one processing. So, region can be used to represent processing. That is 6th row in the table shown. Yet, a much better expression is by X-form (\cite{paper2}). We explain them here. 
Here $b_0 = (0, 0), b_1 = (1, 0), b_2 = (0, 1), b_3 = (1, 1)$ are base patterns. For 2-dim pattern space, there are only these 4 base patterns. But, $b_i$ can also be used to represent one processing of 2-1 IPU, i.e. $b_i$ is such a processing: when input is $b_i$, output is 1, otherwise output is zero. X-forms are expressions based on all $N$-1 base patterns, operations +, $\cdot$, $\neg$, composition, Cartesian product, and apply them consecutively. Example, $b_1 + b_3$, $b_1 \cdot b_2$, $b_1 + \neg b_2$ are X-forms. 
Any processing of 2-1 IPU can be expressed by at least one X-form \cite{paper2}. For example, if region is $R_3$, processing is $P_1$, X-form is $b_1$. 
Another example, region is $R_1$, processing is $P_7$ (this is OR gate), X-form is $b_1 + b_2 + b_3$. $P_5$ is a processing of 2-1 IPU (XOR gate), but not in 2-1 RBM, its X-form is $b_1 + b_2$.  
The 7th row in the table shows X-forms representing processing. We can say that each processing is represented by a region, and by a X-form as well.


When 2-1 RBM is learning, clearly, parameter $(a, b)$ is adapting. But, only when $(a, b)$ cross region, processing changes. Before crossing, change of parameters is just for preparation for crossing (perhaps many parameter changes are just wasted). Learning is moving from one region to another. Or, equivalently, learning is from one X-form to another. Such view is crucial. Now, we are clear, on surface, learning on 2-1 RBM is a dynamics on parameter space $\mathbb{R}^2$, but real learning dynamics is on 6 regions (or X-forms). Such indirectness causes a lot of problem. 
\bigskip





{\bf 3-1 RBM} \\
Just increase input dimension 1, we have 3-1 RBM. To discuss it, we can gain some insights for general RBM. For 3-1 RBM, still we can write: for any input $(i_1, i_3, i_3) \in \mathbb{B}^3$, output $o \in \mathbb{B}$ is:
$$
o = p(i_1, i_2, i_3) = \text{Sign}(a i_1 + b  i_2 + c i_3), \text{where} \ (a, b, c) \in \mathbb{R}^3, \ \text{Sign}(x) =
\left\{
	\begin{array}{ll}
		1  & \mbox{if } x \ge 0 \\
		0  & \mbox{if } x < 0
	\end{array}
\right.
$$

However, while we can easily write down all possible processing of 2-1 RBM, it would be hard to do so for 3-1 RBM. For 3-1 IPU, we know that the number of all possible processing is $2^{2^3} = 2^8 = 256$. Since only considering such processing $p$: $p(0, 0, 0) = 0$, the number becomes $256/2 = 128$. We expect 3-1 RBM has less processing. But, how many possible processing of 3-1 RBM could have?

Following the guidance that 2-1 RBM gives to us, i.e. to consider the hyperplane generated by nonlinearity that cuts parameter spaces, we examine parameter space $(a, b, c) \in \mathbb{R}^3$, and following planes:
$a = 0, b = 0, c = 0, a+b = 0, a+c = 0, b+c = 0, a+b+c =0$. 
These planes are naturally obtained. For example, if we consider the input $(1, 0, 0)$, it is easy to see plane $a = 0$ is where cut the value of output: 1 or 0. So, the above planes are associated with following inputs:
$(1, 0, 0), (0, 1, 0), (0, 0, 1), (1, 1, 0), (1, 0, 1), (0, 1, 1), (1, 1, 1)$

We can clearly see that in one region that is cut out by above 7 planes, the output values are same. Therefore, one region actually is representing one processing: in the region, processing is same. So, question of how many possible processing becomes how many possible regions cut out by the 7 planes. We do counting for the regions below. 

First, $a = 0$ cuts parameter space into 2 pieces: $R_1^1$, $R_2^1$. 
Second, $b = 0$ perpendicular to $a = 0$, so, it cuts each region  $R_1^1$, $R_2^1$ into 2 pieces, we then have 4 regions: $R_1^2$, $R_2^2$.$R_3^2$, $R_4^2$. 
Then, $c = 0$ perpendicular to $a = 0$ and $b = 0$, so, we have 8 regions: $R_j^3, j=1, \ldots, 8$.
Then, consider $a+b = 0$. This plane no longer could be perpendicular to all $a = 0, b = 0, c = 0$. We will not have $2 * 8 = 16$ regions. We only have $1.5 * 8 = 12$ regions.
Following the same argument, we have: For $a+c = 0$, $1.5 * 12 = 18$ regions. For $b+c = 0$, $1.5 * 18 = 27$ regions. For $a+b+c = 0$, $1.5 * 27 < 41$ regions. 

So, for 3-1 RBM, there are at most 41 possible processing, comparing to 128 possible processing of full 3-1 IPU. However, there are possibility that the number of processing is even less than 41, since among those regions, it is possible that 2 different regions give same processing. We do not consider these details here. 

Since regions can be represented by X-form, each processing 3-1 RBM can be represented by at least one X-form.  $b_1 = (1,0,0), b_2 = (0,1,1), b_3=(0,0,1), \ldots, b_7 = (1,1,1)$ are X-form for all base patterns. Examples, X-form $b_1 + b_2$ is in 3-1 RBM.  But, $b_1 \cdot b_2$ is not in 3-1 RBM. There are a lot of such X-form that is not in 3-1 RBM.

Learning dynamics on 3-1 RBM is also in such way: on surface, it is dynamics on $\mathbb{R}^3$, but real learning dynamics is on 41 regions (or X-forms). 
\bigskip




{\bf $N$-1 RBM} \\
The argument for 3-1 RBM can be extended to $N$-1 RBM (See details in \cite{paper2}). We consider hyperplanes and regions cut off by these hyperplanes. The number of these regions is less than:  $2^N 1.5^{2^N-N-1}$. Compare to the number of all processing of $N$-1 IPU, which is $2^{2^N - 1}$, easy to see, $N$-1 RBM has much less processing. This means that $N$-1 RBM could not express many processing. 

For $N$-1 RBM, still we can write: for any input $(i_1, \ldots, i_N) \in \mathbb{B}^N$, output $o \in \mathbb{B}$ is:
$$
o = p(i_1, \ldots, i_3) = \text{Sign}(a_1 i_1 + a_2  i_2 + \ldots + a_N i_3), \text{where} \ (a_1, \ldots, a_N) \in \mathbb{R}^N, \ \text{Sign}(x) =
\left\{
	\begin{array}{ll}
		1  & \mbox{if } x \ge 0 \\
		0  & \mbox{if } x < 0
	\end{array}
\right.
$$


\begin{equation}
a_1 = 0, \ldots, a_1+a_2 = 0, \ldots, \ a_1+a_2+a_3 =0 \ldots, \ \ldots, \ a_1+a_2+a_3+\ldots = 0, \ldots
\end{equation}
There are $ {N\choose 1} $ hyperplanes such as $a_1 = 0$;  $ {N\choose 2} $ hyperplanes such as $a_1+a_2 = 0$;
\ldots. We also have this: First $N$ hyperplanes will cut parameter space into $2^N$ regions. Then, later each hyperplanes will cut more regions by the rate of multiplying factor $1.5$. Thus, we can see the number of regions are:

$$
2^N * 1.5^{K_2} * 1.5 ^{K_3} * \ldots * 1.5^{K_N}
$$
where ${K_2} = {N\choose 2}$ is the number of hyperplanes such as $a+b = 0$, etc. 

And, we have the equation:

\begin{equation}
2^N = {N\choose 0} +  {N\choose 1} +  {N\choose 2} + \ldots +  {N\choose N}   
\end{equation}

So, 

\begin{equation}
K_2 + K_3 + \ldots + K_N = {N\choose 2} +  {N\choose 3} + \ldots +  {N\choose N} = 2^N - {N\choose 1} -{N\choose 0}    = 2^N - N -1
\end{equation}

Thus, the number of regions are

$$
2^N * 1.5^{2^N-N-1} = 2^N * ({\frac{3}{2}})^{2^N-N-1} 
$$

This is a very big number. Yet, compare to the total possible processing of full IPU, it is quite small. See their quotient:

\[
\frac{2^{2^N}}{2^N * ({\frac{3}{2}})^{2^N-N-1} }   = 2* (4/3)^{2^N - N - 1}
\]
It tells that full IPU has $f = 2* (4/3)^{2^N - N - 1}$ folds more processing comparing to RBM. This is huge difference. Say, just for $N = 10$, $f$ is more than 120 digits, i.e. the number of processing of full IPU would has more 120 digits at the end.




Also, each region can be expressed by at least one X-form. For examples, $b_1 + b_N$, $b_1 + b_3 + b_5$, etc. Learning dynamics on $N$-1 RBM is in such way: on surface, it is dynamics on $\mathbb{R}^N$, but real learning dynamics is on those  $2^N 1.5^{2^N-N-1}$ regions (or X-forms). 
\bigskip



{\bf $N$-$M$ RBM} \\
Suppose $\mathcal{R}_i, i=1, \ldots, M$ are $M$ $N$-1 RBMs, we can form a $N$-$M$ RBM, denote as $\mathcal{R} = (\mathcal{R}_1, \ldots. \mathcal{R}_M)$, whose processing are  $p = (p_1, p_2, \ldots, p_M)$, where $p_i, i = 1, \ldots, M$ are processing of $\mathcal{R}_i$. So, $\mathcal{R}$ is Cartesian product of $\mathcal{R}_i, i=1, \ldots, M$. 

Since all $\mathcal{R}_i$ are cut into regions, and in each region, processing is same, we can see $\mathcal{R}$ is also cut into regions, and each region is a Cartesian product of regions of $\mathcal{R}_i$: $R = R_1 \times R_2 \times \ldots \times R_M$, where $R_i$ is one region from i-th RBM $\mathcal{R}_i$. Thus, the number of all possible regions of $\mathcal{R}$ is $(2^N 1.5^{2^N-N-1})^M = 2^{NM} 1.5^{M(2^N-N-1)}$. This is a much smaller number than $2^{M2^N}$, which is the number of total possible processing for $N$-$M$ IPU. 

X-form for each region of $\mathcal{R}$, are actually Cartesian product of X-form for those regions of $\mathcal{R}_i$. Suppose $R = R_1 \times R_2 \times \ldots \times R_M$, and $f_i$ are X-forms for region $R_i$ in $\mathcal{R}_i$, $i=1, \ldots, M$, then X-form for $R$ is $f = (f_1, \ldots, f_M)$.  For example, $(b_1, b_1+b_3, \ldots, b_2 \cdot b_4)$ is one X-form of $\mathcal{R}$. 

Learning dynamics on $N$-$M$ RBM is in such way: on surface, it is dynamics on parameter space $\mathbb{R}^NM$, but real learning dynamics is on those  $2^{NM} 1.5^{M(2^N-N-1)}$ regions (or X-forms). 
\bigskip




{\bf Stacking RBMs} \\
Consider a $N$-$M$ RBM $\mathcal{R}_1$, and a $M$-$L$ RBM $\mathcal{R}_2$, stacking them together, we get one $N$-$L$ IPU $\mathcal{R}$: A processing $p$ of $\mathcal{R}$ are composition of processing $p_1, p_2$ of $\mathcal{R}_1, \mathcal{R}_2$: $p(i_1, i_2, \ldots, i_N) = p_2 ( p_1(i_1, i_2, \ldots, i_N) )$. And we denote as: $\mathcal{R} = \mathcal{R}_1 \otimes \mathcal{R}$.

The parameter space of $\mathcal{R}$ clearly is $\mathbb{R}^{NM} \times \mathbb{R}^{ML}$. We know $\mathbb{R}^{NM}$ is cut into some regions, in each region processing is same. So does $\mathbb{R}^{ML}$. Thus, $\mathbb{R}^{NM + ML}$ is cut into some regions, in each region processing is same, and these regions are Cartesian product of regions in $\mathbb{R}^{NM}$ and $\mathbb{R}^{ML}$. So, we know number of total possible processing $\mathcal{R}$ equals total possible processing of $\mathcal{R}_1$ times $\mathcal{R}_2$, i.e. 
$2^{NM} 1.5^{M(2^N-N-1)} \times 2^{ML} 1.5^{L(2^M-M-1)} = 2^{NM+ML} 1.5^{M(2^N-N-1) + L(2^M-M-1)}$. 

We can easily see if $M$ is large enough, the above number will become greater than $2^{L2^N}$, which is total possible processing of $\mathcal{R}$. We can see, at least potentially, $\mathcal{R}$ has enough ability to become a $N$-$L$ full IPU. But, we will not consider here. In fact, it is very closely related to the so called Universal Approximation Theorem. Indeed, stacking RBM together is powerful. 

X-form can be expressed as composition as well. For example, consider 3 2-1 RBM $\mathcal{R}_1$, $\mathcal{R}_2$, and $\mathcal{R}_3$. Using $\mathcal{R}_1$ and $\mathcal{R}_2$ to form a 2-2 RBM, and using $\mathcal{R}_3$ to stack on it, we get a 2-1 IPU $\mathcal{R}$: $\mathcal{R} = \mathcal{R}_3 \otimes (\mathcal{R}_1, \mathcal{R}_2)$. If for this case, $\mathcal{R}_1$ has X-form $b_1$, and $\mathcal{R}_2$ has X-form $b_2$, and $\mathcal{R}_3$ has X-form $b_1 + b_2 + b_3$, them, $\mathcal{R}$ has X-form $(b_1 + b_2 + b_3) (b_1, b_2)$. Easy to see this X-form is processing $P_5$ (XOR gate), which is not expressible by one 2-1 RBM. So, putting 3 2-1 RBMs together, more X-form can be expressed.
\bigskip






{\bf Deep Learning and Learning Dynamics} \\
Following idea of "putting RBM together, then more expressible", by stacking more RBMs, deep learning is formed. Suppose $\mathcal{R}_j$, are $N_j$-$N_{j+1}$ RBM, $j = 1, \ldots, J$, where $N_1, \ldots, N_J, N_{J+1}$ are sequence of integers.  So, we can stack these RBM together to form one $N_1$-$N_{J+1}$ IPU, which has processing: $p(i_1, i_2, \ldots, i_{N_1}) = p_J ( \ldots  p_2 ( p_1(i_1, i_2, \ldots, i_{N_1}) )$
where each $p_j$ is processing of $\mathcal{R}_j$. We denote this IPU as $\mathcal{R} =  \mathcal{R}_1 \otimes \ldots \otimes \mathcal{R}_J$. 


Deep learning is conduct on IPU $\mathcal{R}$. How is learning dynamics? As we have seen from simplest RBM to deep learning, we know that the parameter space is cut into regions, and each region is for one processing, and learning is moving from one region to another. In another words, on surface, learning dynamics is going on the huge parameter space $\mathcal{R}^{N_1N_2 + \ldots + N_JN_{J+1}}$, but is actually on those regions. The number of regions are huge: $2^{N_1N_2} 1.5^{N_2(2^{N_1}-N_1-1)} \times \ldots$. 

Thus, as learning, huge number of parameters are adapting. But only when parameters cross region, processing changes. Before crossing, processing remains same, parameter changes at most can be thought as the preparation for crossing (perhaps many parameter changes are just wasted). We can say, learning dynamics is to move from one region to another. We also know each region is represented by one X-form, thus, learning dynamics is to move from one X-form to another X-form. 


Deep learning structure is formed by human intervene. Once it is formed, if no further human intervene, it is mechanical learning. But, once it is formed, the structure, such as how many RBMs, how to stack, how to do Cartesian product, etc, are fixed. Thus, regions are fixed. Thus, available X-forms for learning are fixed. Then learning is conducted on the available X-forms. In other words, learning dynamics is to find a good X-form among those available X-forms. There are quite a variety methods of learning, i.e. how to find the good X-form. But, most essential methods are 2: contrastive divergence (CD), and stochastic gradient descendant (SGD). 


We can see one example. $\mathcal{R}_1$, $\mathcal{R}_2$, $\mathcal{R}_3$ are 3 2-1 RBMs. We put them like this: $\mathcal{R} = \mathcal{R}_3 \otimes (\mathcal{R}_1, \mathcal{R}_2)$. We have 3 parameter space $(a, b), (c, d), (e, f)$. We have 6 regions for each parameter space. Put them together, we have 6x6x6 = 216 regions. $\mathcal{R}$ is one 2-1 IPU. So, $\mathcal{R}$ at most has 8 processing. Thus, among those 216 regions some different regions will have same processing. But, each region will have one X-form. That is to say, for one processing, there could have several X-form associate with it. For example, consider this region: $R^3_1 \times (R^1_3, R^2_5)$. This gives processing $P_5$ (XOR gate). Normally, for this processing, we can use X-form $b_1 + b_2$ for it. But, for the region, naturally, the X-form is: $(b_1 + b_2 + b_3) \otimes (b_1, b_2)$. That is to say, this X-form will generate the same processing as $b_1 + b_2$. Another region: $R^3_2 \times (R^1_2, R^2_6)$ will give the same processing. And, the X-form is: $(b_1 + b_3) \otimes (b_1+b_3, b_2+b_3)$.    


So, for deep learning, we can say that all X-forms form the internal representation space. But, for deep learning, this internal representation space could not be seen directly, it is embedded into huge parameter space   $\mathbb{R}^{N_1 N_2 + \ldots + N_J N_{J+1}}$, each X-form equivalent to one region in this parameter space, which is a Cartesian product of regions of $\mathcal{R}_j, j=1, \ldots, J$. 

So, we can see deep learning is doing: 1) to form a set of available X-forms and embed those X-forms into parameter space, this is done at the time to set up the learning structure and this is done by human; 2) to use some learning methods (e.g. CD+SGD) to form dynamics on parameter space, and use a big amount of data to drive the dynamics, 3) hope to reach a good X-form among the set. This is what we described in section 5, Strategy 1. In fact, the loss function used in SGD is exactly the function in the equation \eqref{eq:embk} in section 5. This is what happens inside a deep learning. 

Now, we have a clear picture on what deep learning is doing and why it is working well: It conducts learning by using Strategy 1 -- Embed X-forms into Parameter Space. Also, the data sufficiency that we defined actually lay down a framework for understanding what kind of data and what amount of data are sufficient to drive a successful learning. This is very important. We will no longer be blind about big data. We can calculate if data is sufficient for one task. This a huge contribution from our descriptions on mechanical learning to deep learning.
\bigskip


{\bf Disadvantages of Deep Learning} \\
Finally, we want to point out: deep learning might not be best mechanical learning. We list some its disadvantages below: 
\begin{enumerate} [topsep=0pt,itemsep=-1ex,partopsep=1ex,parsep=1ex]
\item It is acting on huge parameter space, but, actual learning dynamics is on a fixed set of regions (which is equivalent to a set of X-forms). This indirectness makes every aspects of learning harder, especially, it is near impossible to know what is exactly happening in learning dynamics.
\item As theorem 7 in section 5 shows, successful learning needs data sufficient to support and to bound. This requirement is very costly.
\item The structure of learning is setup by human. Once setup, structure (how many layers, how big a layer is, how layers fitting together, how to do convolution, etc) is not be able to change. This means that learning is restricted on a fixed group of regions, equivalent a fixed group of X-forms. If best X-form is not in this set, deep learning has no way to reach best X-form, no matter how big data are and how hard to try. Consequently, it is not universal learning machine.
\item It is very costly to embed X-forms into a huge parameter space. Perhaps, among all computing spend on learning, only a very small fraction is used on critical part, i.e. on moving X-form, and most are just wasted.
\item Since there is no clear and reachable internal representation (due to the embedding), it will be very hard to do advanced learning, such as to unite all 5 learning methods together (see \cite{pedro}).
\item Since there is no clear internal representation space, it is hard to define initial X-form, which is very essential to efficiency improving and several stages of learning.
\end{enumerate}
Knowing the disadvantages is good, we can find some better way to turn disadvantages to advantages. 
\bigskip


