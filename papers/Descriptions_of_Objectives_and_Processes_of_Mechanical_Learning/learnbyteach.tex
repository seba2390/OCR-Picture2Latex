We now turn attentions to learning. We emphasis again that a learning machine is based on patterns, not bitwise, and the purpose of a learning machine is to process patterns and learn how to process. 

Theorem 1 and Theorem 2 tell us, for simplicity and without loss of generality, we can just consider $N$-1 learning machine. For a $N$-1 learning machine, its processing is actually equivalent to its black set. We can also consider an objective pattern $p$, which is a set of base patterns. Thus, $p$ can be thought as black set of one processing, and vise versa. This tells us, for a $N$-1 learning machine, its processing is equivalent to a objective pattern, called its black pattern. Obviously, black set and black pattern are equivalent. We can switch the 2 terms freely. By this understanding, we can define universal learning machine equivalently below. 


\begin{definition}[\bf  Universal Learning Machine (by Black Set)] 
For a $N$-1 learning $\mathcal{M}$, if its current black set is $B$, and a given objective pattern $p$, $\mathcal{M}$ can start from $B$ to learn and at the end of learning its black set becomes $p$, we call $\mathcal{M}$ can learn from $B$ to $p$. If for any $B$ and $p$,  $\mathcal{M}$ can learn from $B$ to $p$, we call $\mathcal{M}$ a universal $N$-1 learning machine.   
\end{definition}
For a $N$-1 learning $\mathcal{M}$, it is easy to see definition 4.1 and definition 2.2 are equivalent. 

Now, we turn attention to how to make a learning machine learn from $B$ to $p$. It is easy to imagine, there are many possible ways to learn. Here, we discuss learning by teaching, that is to say, we can design a special data sequence $T$ and apply it to the learning machine, then the machine learns  effectively under the driven of $T$. We call $T$ as teaching sequence. Teaching sequence is a specially designed data sequence.


It is easy to imagine, if we know the teaching sequence, learning by teaching is easy. Just feed the teaching sequence into, and learning is done. It is quite close to programming. But, learning by teaching can reveal interesting properties to us, and can guide us for further discussions. 

Consider a teaching sequence $T = \{(b_i, o_i) \ |\  i = 1, 2, \ldots\}$. Here, output feedback $o_i$ could be empty, i.e. there is just no output feedback. Learning machine still could learn without output feedback. Of course, with output feedback, the learning will be more effective and efficient. Teaching sequence is the only information source for the machine. Learning machine will not get any other outside information besides teaching sequence. This is very essential. 





The fundamental question would be: what kind properties of learning machine to make it universal? We will reduce this questions to see some capabilities of learning machine, and with these capabilities, machine is universal.

Note, one special case is: black set of  $\mathcal{M}$ is empty set, we call it as empty state. This is one very useful case. There are some base patterns quite unique: $b_1 = (1, 0, \ldots, 0)$, $b_2 = (0, 1, \ldots, 0)$, $b_N = (0, 0, \ldots, 1)$, i.e. these base patterns only has one component equals 1, and rest equals 0. We call such base patterns as elementary base patterns. 


\begin{definition}[\bf Learning by Teaching - Capability 1]
For a learning machine $\mathcal{M}$, the capability 1 is: for any elementary base pattern $b_j$, $j=1, 2, \ldots, N$, there is one teaching sequence $T_j, j= 1, 2, \ldots, N$, so that starting from empty state, driven by $T_j$, the black pattern become $b_j$.
\end{definition}
The capability 1 means: $\mathcal{M}$ can learn any elementary base pattern  from empty state.   



\begin{definition}[\bf Learning by Teaching - Capability 2]
For a learning machine $\mathcal{M}$, the capability 2 is: for any black pattern $p$, there is at least one teaching sequences $T$, so that starting from $p$, driven by $T$, the black set becomes empty.
\end{definition}
The capability 2 means: to forget current black pattern, can go back to empty state.



\begin{definition}[\bf Learning by Teaching - Capability 3]
For a learning machine $\mathcal{M}$, the capability 3 is: for any 2 objective patterns $p_1$ and $p_2$, there is at least one teaching sequence $T_d$, so that starting from $p_1$, driven by $T_d$, the black pattern becomes $p_1 \cdot p_2$; and there is at least one teaching sequence
$T_p$ so that starting from $p_1$, driven by $T_p$, the black pattern becomes $p_1 + p_2$; and there is at least one teaching sequence
$T_n$ so that starting from $p_1$, driven by $T_n$, the black pattern becomes $\neg p_1$;
\end{definition}
Simply say, capability 3 means: for any 2 objective patterns $p_1, p_2$, learning machine is capable to learn subjective pattern of applying operator "$\cdot$", "+" to $p_1$ and $p_2$, and "$\neg$" to $p_1$. This is the most crucial capability. 

If one learning machine has all 3 capabilities, we expect a strong learning machine. Actually, we have following major theorem.


\begin{theorem}
If a $N$-1 learning machine $\mathcal{M}$ has the above 3 capabilities, it is an universal learning machine. 
\end{theorem}
{\bf Proof: } Since we have capability 2, we only need to consider the case: to start from empty state. That is to say, we only need to prove this: for any objective pattern $p$, we can find a teaching sequence $T$, so that starting from empty state, driven by $T$, the black pattern becomes $p$. 

According to Theorem 4, for any objective pattern $p$, we can find an X-form $E(b)$, where $E$ is one algebraic expression, $b$ is a group of elementary base patterns $b = \{e_i | i = 1, \ldots, K, K \le N\}$, so that $p$ equals this X-form, i.e. $p = E(e_1, e_2, \ldots, e_K)$. 

By $E$, we can construct teaching sequence like this way: \\
1) First we have a teaching sequence $T_1$ so that $\mathcal{M}$ go to empty state. This is using capability 1.\\
2) Then, have a teaching sequence $T_2$ so that $\mathcal{M}$ have black pattern $e_1$. This is using capability 2. \\
3) Since $E$ is formed by finite steps of $\cdot$, $\neg$, and $+$ starting from $e_1$, we can use capability 3 consecutively to construct teaching sequence $T$ for each operator in $E$. Eventually, we will get a teaching sequence over all operators in $E$.\\
Such teaching sequence $T_1 + T_2 + T$ will drive $\mathcal{M}$ to $p$.   

Note: The expression $E$ depends on several things: the complexity of $p$, and to find an X-form $E$ for $p$. In theorem 4, we demonstrated 2 level X-forms. We actually expect to have a much better X-form. The worst case would be: $E = b_1 + b_2 + \ldots$, in which, the pattern $p$ is so complicated that there is no way to find a X-form for higher level, so the only way is to just list all base patterns.  



\begin{corollary}
If we have $N$-1 learning machine $\mathcal{M}$ with the above 3 capabilities, we then can use it to build one universal $N$-$M$ learning machine.
\end{corollary}

This is just following Theorem 5 and Theorem 2. From Theorem 5 and corollary, we reduce the task to find university learning machine to find a $N$-1 learning machine with 3 capabilities. Once we can find a way to construct a $N$-1 learning machine with those 3 capabilities, we have an universal learning machine. 

Also, it is easy to see that an universal learning machine surely has the 3 capabilities. Thus, the necessary and sufficient conditions for a learning machine to become universal are the 3 capabilities. 

But, do we have one learning machine with those 3 capabilities? Well, it is up for us to design a concrete learning machine with the 3 capacities. We will do this in other places. Any way, the 3 capabilities will give us a clear guide on design of effective learning machine: The most essential capabilities for a learning machine is to find a way to move patterns to higher organized patterns. See the quotation at the front, most important step is: "from a lower level to ...... higher". This indeed guides us well.




