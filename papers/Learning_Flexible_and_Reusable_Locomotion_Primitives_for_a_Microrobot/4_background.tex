\subsection{Central Pattern Generators}
\label{sec:bg:cpg}
	Central pattern generators (CPGs) are neural circuits found in nearly all vertebrates, which produce periodic outputs without sensory input~\citep{Junzhi_Yu_2014}.
	CPGs are also a common choice for designing gaits for robot locomotion~\citep{Ijspeert2008}.
    We chose to use CPGs for our controller because they are capable of reproducing a wide variety of different gaits simply by manipulating the relative coupling phase biases between oscillators. 
	This allows us to easily produce a variety of gait patterns without having to manually program those behaviors. 
	In addition, CPGs are not computationally intensive and can have on-chip hardware implementations using VLSI or FPGA. 
	This makes them well suited to be eventually used in our physical microrobot, where the processing power is limited.
	CPGs can be modeled as a network of coupled non-linear oscillators where the dynamics of the network are determined by the set of differential equations
	%
	\begin{align}
	  \dot{\phi_i} &= \omega_{i} + \displaystyle \sum_{j} (\omega_{ij}r_j\sin(\phi_j - \phi_i - \varphi_{ij}))\,,\\
	  \ddot{r_i} &= a_r (\dfrac{a_r}{4} (R_i - r_i) - \dot{r_i})\,,\\
	  \ddot{x_i} &= a_x (\dfrac{a_x}{4} (X_i - x_i) - \dot{x_i})\,,
	\end{align}
	%
	where $\phi_{i}$ is a state variable corresponding to the phase of the oscillations and $\omega_{i}$ is the target frequency for the oscillations. 
    $\omega_{ij}$ and $\varphi_{ij}$ are the coupling weights and phase biases which change how the oscillators influence each other.
    To implement our desired gaits, we only need to modify the phase biases between the oscillators~$\phi_{ij}$.
    $r_{i}$ and $x_{i}$ are state variables for the amplitude and offset of each oscillator, and $R_{i}$ and $X_{i}$ are control parameters for the desired amplitude and offset. 
    The constants $a_{r}$ and $a_{x}$ are constant positive gains and allow us to control how quickly the amplitude and offset variables can be modulated. 
    A more detailed explanation of the network can be found in Crespi's original work~\citep{Crespi_2007}. 
	One of the foremost benefits of using a CPG controller is a drastic reduction in the number of parameters~$\parameters_i$ we need to optimize.
    Overall, the parameters that we consider during the optimization are $\parameters = \left[\omega, R, X_{l}, X_{r} \right]$ where $\omega$ is the frequency of the oscillators and $R$ is the phase difference between each of the vertical-horizontal oscillator pairs. 
    In order to allow for directional control, $X_{l}$ and $X_{r}$ are the amplitudes of the left and right side oscillators respectively.    
    
\subsection{Bayesian Optimization}
\label{sec:bg:BO}
	%
	Even with a complete CPG network, some amount of parameter tuning is necessary to obtain efficient locomotion. 
	To automate the parameter tuning, we use Bayesian optimization (BO), an approach often used for global optimization of black box functions~\citep{Kushner1964,Jones2001,Calandra2015a}.
	We formulate the tuning of the CPG parameters as the optimization
	%
	\begin{align}
		\parameters^* = \maximize_{\parameters}\, \objfunc{\parameters}\,,
		\label{eq:optimization}
	\end{align}
	%
	where $\parameters$ are the CPG parameters to be optimized w.r.t. the objective function of choice~$\objfuncNo$ (\eg, walking speed, which we investigate in \sec{sec:results:soo}).
	At each iteration, BO learns a model $\tilde{\objfuncNo}: \parameters \rightarrow \objfunc{\parameters}$ from the dataset of the previously evaluated parameters and corresponding objective values measured~$\dataset=\{\parameters, \objfunc{\parameters}\}$.
	Subsequently, the learned model $\tilde{\objfuncNo}$ is used to perform a ``virtual'' optimization through the use of an acquisition function which controls the trade-off between exploration and exploitation.
	Once the model is optimized, the resulting set of parameters $\parameters^*$ is finally evaluated on the real system, and is added to the dataset together with the corresponding measurement $\objfunc{\parameters^*}$  before starting a new iteration.
	A common model used in BO for learning the underlying objective, and the one that we consider, is Gaussian processes~\citep{Rasmussen2006}. 
	For more information regarding BO, we refer the readers to~\citep{Jones2001,Shahriari2016}.

	
\subsection{Multi-objective Bayesian Optimization}
	A special case of the optimization task of \eq{eq:optimization} is multi-objective optimization~\citep{Branke2008a}.
	Often times in robotics\footnote{As well as in nature~\citep{Hoyt1981}.}, there are multiple conflicting objectives that need to be optimized simultaneously, resulting in design trade-offs (e.g., walking speed vs energy efficiency which we investigate in \sec{sec:results:moo}).  
	When multiple objectives are taken into consideration, there is no longer necessarily a single optimum solution, but rather the goal of the optimization became to find the set of Pareto optimal solutions~\citep{Pareto1906}, which also takes the name of Pareto front~(PF).
	Formally, the PF is the set of parameters that are not dominated, where a set of parameters~$\parameters_1$ is said to dominate $\parameters_2$ when 
	%
	\begin{align}
		\left\{
		\begin{array}{l l}
		\forall i \in \{1,\dots ,\numbersubobj\}: &\objfuncNo_i(\parameters_1) \leq \objfuncNo_i(\parameters_2)\\
		\exists j \in \{1,\dots , \numbersubobj\}:  &\objfuncNo_j(\parameters_1) < \objfuncNo_j(\parameters_2)
		\end{array} \right.
	\end{align}
	Intuitively, if $\parameters_1 \dom \parameters_2$, then $\parameters_1$ is preferable to $\parameters_2$ as it never performs worse, but at least in one objective function it performs strictly better. 
	However, different dominant variables are equivalent in terms of optimality as they represent different trade-offs.
	      
	Multi-objective optimization can often be difficult to perform as it might require a significant amount of experiments.
	This is especially true with our microrobot where large number of experiments can wear-and-tear the robot.
	As a result, the number of evaluations allowed to find the Pareto set of solutions is limited. 
	Luckily for us, there exist extensions of BO which address multi-objective optimization.
	In particular, the multi-objective Bayesian optimization algorithm that we consider is ParEGO~\citep{Knowles2006}. 
	The main intuition of ParEGO is that at every iteration, the multiple objectives can be randomly scalarized into a single objective (via the augmented Tchebycheff function), which is subsequently optimized as in the standard Bayesian optimization algorithm (by creating a response surface, and then optimizing its acquisition function).
	For more information about multi-objective Bayesian optimization we refer the reader to~\citep{Wagner2010}.
    
\subsection{Contextual Bayesian Optimization}
	%
	Another special case of the optimization task of \eq{eq:optimization}, is contextual optimization.
	In contextual optimization, we assume that there are multiple correlated, but slightly different, tasks which we want to solve, and that they are identified by a context variable~$\context$.
	An example (which we investigate in \sec{sec:results:context1}) might be walking on inclined slopes, where the contextual variable is the angle of the slope.
	The contextual optimization can hence be formalized as 
	%
	\begin{align}
		\parameters^* = \maximize_{\parameters}\, \objfunc{\parameters,\context}\,,
	\end{align}
	%
	where for each context~$\context$, a potentially different set of parameters~$\parameters^*$ exists.
	The main advantage compared to treating each task independently is that, in contextual optimization, we can exploit the correlation between the tasks to generalize, and as a result quickly learn how to solve a new context.
	Specifically, in this paper we consider contextual Bayesian optimization (cBO)~\citep{Metzen2015} which extends the classic BO framework from \sec{sec:bg:BO}.
	Contextual Bayesian Optimization learns a joint model $\tilde{\objfuncNo}: \{\parameters,\context\} \rightarrow \objfunc{\parameters}$, but now, at every iteration the acquisition function is optimized with a constrained optimization where the context $\context$ is provided by the environment. 
	However, because the model jointly model the context-parameter space, experience learned in one context can be generalized to similar contexts. 
	By utilizing cBO, we will show in \sec{sec:results} that our microrobot can learn to walk (and generalize) to different environmental contexts such as walking uphill and curving.

