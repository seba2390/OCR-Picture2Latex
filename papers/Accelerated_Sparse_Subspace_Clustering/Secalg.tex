In this section, we develop a novel self-expressiveness-based algorithm for the subspace clustering problem and analyze its performance.  We propose to find an approximate solution to the problem
\begin{equation}\label{eq:prob-aols}
\begin{aligned}
& \underset{\c_j}{\text{min}}
\quad \|\c_j\|_0
& \text{s.t.}\hspace{0.5cm}  \|\y_j -\Y\c_j\|_2^2 \leq \epsilon, \quad \C_{jj} = 0,
\end{aligned}
\end{equation}
by employing a low-complexity variant of the orthogonal least-squares (OLS) algorithm 
\cite{chen1989orthogonal} so as to find a sparse representation for each data point and 
thus construct $\C$. Note that in \ref{eq:prob-aols}, $\epsilon\geq 0$ is a small predefined parameter that is used as the stopping criterion of the proposed algorithm.

The OLS algorithm, drawn much attention in recent years \cite{hashemi2017sparse,chen1989orthogonal,rebollo2002optimized,hashemi2016sparse,soussen2013joint,herzet2016relaxed}, is a greedy heuristic that iteratively reconstructs sparse signals by identifying one nonzero signal component at a time. The complexity of using classical OLS \cite{chen1989orthogonal} to find a subspace preserving $\C$ -- although lower than that of the BP-based SSC method \cite{elhamifar2009sparse} --  might be prohibitive in applications involving large-scale data. To this end, we propose a fast variant of OLS referred to as accelerated OLS (AOLS) \cite{a2bol2016} that significantly improves both the running time and accuracy of the classical OLS. AOLS replaces the aforementioned single component selection strategy by the procedure where $L\geq 1$ indices are selected at each iteration, leading to significant improvements in both computational cost and accuracy. {\color{black}{To enable significant gains in speed, AOLS efficiently builds a collection of orthogonal vectors $\{\u_{\ell_1},\dots,\u_{\ell_L}\}_{\ell=1}^{T}$ that represent the basis of the subspace that includes the approximation of the sparse signal.}}\footnote{$T < N$ is the maximum number of iterations that depends on the threshold parameter $\epsilon$.}

In order to use AOLS for the SSC problem, consider the task of finding a sparse representation for $\y_j$. Let $A_j\subset[N]\backslash\{j\}$ be the set containing indices of data points with nonzero coefficients in the representation of $\y_j$. That is, for all $l\in A_j$, $\C_{lj}\neq 0$. The proposed algorithm for sparse subspace clustering, referred to as accelerated sparse subspace clustering (ASSC), finds $A_j$ in an iterative fashion (See Algorithm 1). In particular, starting with $A_j=\emptyset$, in the $i\ts{th}$ iteration we identify $L\geq 1$ data points $\{\y_{s_1},\dots,\y_{s_L}\}$ for the representation of $\y_j$. The indices $\{{s_1},\dots,{s_L}\} \subset [N]\backslash(A_j\cup\{j\})$ correspond to the $L$ largest terms $(\y_l^\top \r_{i-1}\slash \y_l^\top \t_l^{(i-1)})^2\|\t_l^{(i-1)}\|_2^2$, where $\r_{i-1} = \P_{A_j}^{\bot}\y_j$ denotes the residual vector in the $i\ts{th}$ iteration with $\r_0 = \y_j$, and 
\begin{equation}
\t_l^{(i)} =\t_l^{(i-1)} -   \sum_{k = 1}^{L}\frac{{\t_l^{(i-1)}}^\top\u_{i_k}}{\|\u_{i_k}\|_2^2}\u_{i_k}
\end{equation}
is the projection of $\y_l$ onto the span of orthogonal vectors 
$\{\u_{\ell_1},\dots,\u_{\ell_L}\}_{\ell=1}^{i}$. Once $\{\y_{s_1},\dots,\y_{s_L}\}$ are selected, we use the assignment
\begin{equation}\label{eq:update}
\u_{i_{k}} = \frac{\y_{s_k}^\top \r_i}{\y_{s_k}^\top \t_{s_k}^{(i)}}\t_{s_k}^{(i)},\quad \r_{i} \leftarrow \r_i - \u_{i_{k}},
\end{equation}
$L$ times to obtain $\r_{i}$ and $\{\u_{\ell_1},\dots,\u_{\ell_L}\}_{\ell=1}^{i}$ that are required for subsequent iterations. This procedure is continued until $\|\r_i\|_2^2< \epsilon$ for some iteration $1\leq i\leq T$, or the algorithm reaches the predefined maximum number of iterations $T$.  Then the vector of coefficients $\c_j$ used for representing $\y_j$ is computed as the least-squares solution $\c_j=\Y_{A_j}^{\dagger}\y_j$. Finally, having found $\c_j$'s, we construct $\W = |\C|+|\C|^\top$ and apply spectral clustering on its normalized Laplacian to obtain the clustering solution.
%=================================== ALGORITHM 1
\renewcommand\algorithmicdo{}	% removes "DO" from for loops
\begin{algorithm}[t]
\caption{Accelerated Sparse Subspace Clustering}
\label{alg:greedy}
\begin{algorithmic}[1]
\STATE \textbf{Input:} $\Y$, $L$, $\epsilon$, $T$\\
\STATE \textbf{Output:} clustering assignment vector $\s$\\
\FOR {$j = 1,\dots,N$}
\STATE Initialize $\r_0 = \y_j$, $i=0$, $A_j = \emptyset$, $\t_l^{0} = \y_l$ for all $l \in [N]\backslash \{j\}$\\
\WHILE  {  $\|\r_i\|_2^2\geq \epsilon$ and $i<T$ }
\STATE Select $\{{s_1},\dots,{s_L}\}$ corresponding to $L$ largest terms
$(\y_l^\top \r_i\slash \y_l^\top \t_l^{(i)})^2\|\t_l^{(i)}\|_2^2$ \\
\STATE $A_j\leftarrow A_j \cup\{{s_1},\dots,{s_L}\}$\\
\STATE $i \leftarrow i+1$\\
\STATE Perform \ref{eq:update} $L$ times to update $\{\u_{\ell_1},\dots,\u_{\ell_L}\}_{\ell=1}^{i}$ and $\r_i$ \\
\STATE $\t_l^{(i)} =\t_l^{(i-1)} -   \sum_{k = 1}^{L}\frac{{\t_l^{(i-1)}}^\top\u_{i_k}}{\|\u_{i_k}\|_2^2}\u_{i_k}$ for all $l \in [N]\backslash \{j\}$\\
\ENDWHILE \\
\STATE $\c_j=\Y_{A_j}^{\dagger}\y_j$\\
\ENDFOR\\
\STATE $\W = |\C|+|\C|^\top$\\
\STATE Apply spectral clustering on the normalized Laplacian of $\W$ to obtain $\s$\\
\end{algorithmic}
\end{algorithm}
%===================================
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\vspace{-0.2cm}
\subsection{Performance Guarantee for ASSC}
In this section, we analyze performance of the ASSC algorithm under the scenario that data points are noiseless and drawn from a union of independent subspaces, as defined next.
%===================================
\begin{definition}
\textit{Let $\{S_i\}_{i=1}^{n}$ be a collection of subspaces with dimensions $\{d_i\}_{i=1}^{n}$. Define $\sum_{i=1}^n{S_i} = \{\sum_{i}\y_{i}: \y_{i} \in S_i\}$. Then, $\{S_i\}_{i=1}^{n}$ is called independent if and only if $\mathrm{dim}(\sum_{i=1}^n{S_i}) = \sum_{i=1}^n{d_i}$.}
\end{definition}
% Note that when $n=2$, subspaces are independent if and only if they are disjoint and only intersect at the origin. However, for $n>2$ pairwise disjoint subspaces need not be independent. In addition, any subset of a collection of independent subspaces is also independent. 
%===================================
Theorem \oldref{thm:indep} states our main theoretical results about the performance of the 
proposed ASSC algorithm.
%===================================
\begin{theorem}\label{thm:indep}
\textit{Let $\{\y_i\}_{i=1}^N$ be a collection of noiseless data points drawn  from a union of independent subspaces $\{S_i\}_{i=1}^{n}$. Then, the representation matrix $\C$ returned by the 
ASSC algorithm is subspace preserving.}
\end{theorem}

The proof of Theorem \oldref{thm:indep}, omitted for brevity, relies on the observation that in order to select new representation points, ASSC finds data points that are highly correlated with the current residual vector. Since the subspaces are independent, if ASSC chooses a point that is drawn from a different subspace, its corresponding coefficient will be zero once ASSC meets a terminating criterion (e.g., $\ell_2$-norm of the residual vector becomes less than $\epsilon$ or $T= N-1$). Hence, only the points that are drawn from the same subspace will have nonzero coefficients in the final sparse representation. 
%===================================

\textit{Remark:} It has been shown in \cite{elhamifar2009sparse,dyer2013greedy,you2015sparse} that if subspaces are independent, SSC-BP and SSC-OMP schemes are also subspace preserving. However, as we illustrate in our simulation results, ASSC is very robust with respect to dependencies among the data points across different subspaces while in those settings SSC-BP and SSC-OMP struggle to produce a subspace preserving matrix $\C$. Further theoretical analysis of this setting is left to future work. 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\subsection{Further Optimization for ASSC}
