%\subsection{Notation}
First, we briefly summarize notation used in the paper and then  formally introduce the SSC problem. 

Bold capital letters denote matrices while 
bold lowercase letters represent vectors. For a matrix $\A$, $\A_{ij}$ denotes the $(i,j)$ entry of
$\A$, and $\a_j$ is the $j\ts{th}$ column of $\A$.
%, and $\A_{-j}$ is the matrix constructed by removing the $j\ts{th}$ column of $\A$. 
Additionally, $\A_S$ is the submatrix of $\A$ that contains the columns of $\A$ indexed by the set $S$. 
${\cal L}_S$ denotes the subspace spanned by the columns of $\A_S$. $\P_S^\bot=\I-\A_S \A_S^\dagger$ is the projection operator 
onto the orthogonal complement of ${\cal L}_S$ where $\A_S^\dagger=\left(\A_S^{\top}\A_S\right)^{-1}\A_S^{\top}$ 
denotes the Moore-Penrose pseudo-inverse of $\A_S$ and $\I$ is the identity matrix. Further, let $[n] = \{1,\dots,n\}$, $\mathbf{1}$ be the vector of all ones, and $\mathcal{U}(0,q)$ denote the uniform distribution on $[0,q]$.

%%%%%%%%%%%%%%%%%%%%%%%Subspace Clustering%%%%%%%%%%%%%%%%%%%%
%\subsection{Sparse Subspace Clustering}\label{sec:ssc}
The SSC problem is detailed next. Let $\{\y\}_{i=1}^N$ be a collection of data points in $\R^D$ and let $\Y = [\y_1,\dots,\y_N] \in \R^{D\times N}$ be the data matrix representing the data points. The data points are drawn from a union of n subspaces $\{S_i\}_{i=1}^n$ with dimensions $\{d_i\}_{i=1}^n$. Without a loss of generality, we assume that the columns of $\Y$, i.e., the data points, are normalized vectors with unit $\ell_2$ norm. The goal of subspace clustering is to partition $\{\y\}_{i=1}^N$ into $n$ groups so that the points that belong to the same subspace are assigned to the same cluster. In the sparse subspace clustering (SSC) framework \cite{elhamifar2009sparse}, one assumes that the data points satisfy the self-expressiveness property formally stated below.
\begin{definition}
\textit{A collection of data points $\{\y\}_{i=1}^N$ satisfies the self-expressiveness property if each data point has a linear representation in terms of the other points in the collection, i.e., there exist a representation matrix $\C$ such that}
\begin{equation}
\Y = \Y \C, \quad \mathrm{diag}(\C) = \mathbf{0}.
\end{equation}
\end{definition}
Notice that since each point in $S_i$ can be
written in terms of at most $d_i$ points in $S_i$, SSC aims to find a sparse subspace preserving 
$\C$ as formalized next.
\begin{definition}
\textit{A representation matrix $\C$ is subspace preserving if for all $j,l \in [N]$ and a 
subspace $S_i$} it holds that
\begin{equation}
\C_{lj} \neq 0 \quad \Longrightarrow \quad \y_j, \y_l \in S_i.
\end{equation}
\end{definition}
The task of finding a subspace preserving $\C$ leads to the optimization problem \cite{elhamifar2009sparse}
\begin{equation}\label{eq:prob}
\begin{aligned}
& \underset{\c_j}{\text{min}}
\quad \|\c_j\|_0
& \text{s.t.}\hspace{0.5cm}  \y_j = \Y\c_j, \quad \C_{jj} = 0,
\end{aligned}
\end{equation}
where $\c_j$ is the $j\ts{th}$ column of $\C$. Given a subspace
preserving solution $\C$, one constructs a similarity matrix $\W = |\C|+|\C|^\top$ for the data points. The graph normalized Laplacian of the similarity matrix $\W$ is then used as an input to a spectral clustering algorithm \cite{ng2001spectral} which in turn produces clustering assignments.