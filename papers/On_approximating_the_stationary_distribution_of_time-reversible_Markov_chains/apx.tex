\section{Appendix}
\label{sec:apx}

\subsection{Probability bounds}
\label{apx:chernoff_bounds}
This appendix provides Chernoff-type probability bounds that are repeatedly used in our analysis; these bounds can be found in e.g.~\cite{Auger&2011}, and can be derived from~\cite{Panconesi&1997}.

Let $X_1,\ldots,X_n$ be binary random variables. We say that $X_1,\ldots,X_n$ are non-positively correlated if for all $I \subseteq \{1,\ldots,n\}$ we have:
\begin{align}
Pr[\forall i \in I: X_i=0] &\leq \prod_{i \in I} Pr[X_i=0] \\
Pr[\forall i \in I: X_i=1] &\leq \prod_{i \in I} Pr[X_i=1]
\end{align}
The following lemma holds:
\begin{lemma}
\label{lem:chernoff}
Let $X_1,\ldots,X_n$ be independent or, more generally, non-positively correlated binary random variables. Let $a_1,\ldots,a_n \in [0,1]$ and $X=\sum_{i=1}^{n}a_i X_i$. Then, for any $\epsilon > 0$, we have:
\begin{align}
Pr[X < (1-\epsilon)\E[X]] &< e^{-\frac{\epsilon^2}{2}\E[X]} \\
Pr[X > (1+\epsilon)\E[X]] &< e^{-\frac{\epsilon^2}{2+\epsilon}\E[X]} 
\end{align}
\end{lemma}
Note that Lemma~\ref{lem:chernoff} applies if $X_1,\ldots,X_n$ are indicator variables of mutually disjoint events, or if they can be partitioned into independent families $\{X_1,\ldots,X_{i_1}\}$, $\{X_{i_1+1},\ldots,X_{i_2}\}$, \ldots of such variables. 

\subsection{A lower bound for non-time-reversible Markov chains}
\label{sub:general_impossible}
\begin{lemma}
For any functions $\tau(n) = \omega(1)$ and $p(n) = o(\frac{1}{n})$ there exists a family of ergodic non-time-reversible Markov chains on $n$ states having mixing time $\tau = \Theta(\tau(n))$, and containing a state $v$ with $\pi(v) = \Theta(p(n))$ such that any algorithm needs $\Omega(\frac{\tau}{\pi(v)})$ calls to \step() to estimate $\pi(v)$ within constant multiplicative factors with constant probability.
\end{lemma}
\begin{proof}
Consider a chain with state space $\{u\} \cup \{u_1,\ldots,u_{n-1}\}$ and
the following transition probabilities (we assume $n$ large enough to set in $[0,1]$ any quantity where needed).
For $u$, set $p_{uu} = 1-\frac{(n-1)p(n)}{\tau(n)}$, and $p_{uu_i} = \frac{p(n)}{\tau(n)}$ for all $i=1,\ldots,n-1$.
For all $i=1,\ldots,n-1$, set $p_{u_i u_i} = 1-\frac{1}{\tau(n)}$ and $p_{u_i u} = \frac{1}{\tau(n)}$.
The chain is clearly ergodic.
Note that $\frac{(n-1)p(n)}{\tau(n)} = o(\frac{1}{\tau(n)})$ and therefore the expected time to leave $u$ is asymptotically larger than the expected time to leave any of the $u_i$.
One can then check that (i) $\pi(u_i) = \Theta(p(n))$, and (ii) the mixing time is $\tau = \Theta(\tau(n))$ (essentially, the expected time to leave the $u_i$).
Pick any $u_i$ as target state $v$.
Suppose now to alter the chain as follows: pick some $u_j \ne v$ and set $p_{u_j v} = 1$.
The new stationary probability of $v$ would then be roughly $2\pi(v)$.
However one cannot distinguish between the two chains with constant probability with less than
$\Omega(\frac{\tau}{\pi(v)})$ \step() calls.
Indeed, to distinguish between them one must at least visit $u_j$ (and then perform e.g.\ \probe($u_j,v$)).
Since $u$ is the only state leading to $u_j$ with positive probability, one must invoke \step($u$) until it returns $u_j$.
But $p_{uu_j}=\frac{p(n)}{\tau(n)}$, hence one needs $\Omega(\frac{\tau(n)}{p(n)}) = \Omega(\frac{\tau}{\pi(v)})$ calls in expectation.
The construction can be adapted to any constant approximation factor by adding more transitions towards $v$.
\end{proof}


\clearpage
\subsection{Pseudocode of \taupiest}
\label{apx:taupiest}
\renewcommand{\thealgorithm}{}
\begin{algorithm}[h!]
\label{alg:taupiest}
\small
\caption{\taupiest($v, \epsilon, \delta$)}
\begin{algorithmic}[1]
\State $S \leftarrow \emptyset$ \Comment{distinct states visited so far}
\State $w_S\leftarrow 0$ \Comment{$\sum_{u \in S} \gamma_u$ for the current $S$}
\State $w\leftarrow 0$ \Comment{will accumulate $w_S$}
\State $r\leftarrow 0$ \Comment{number of repeats witnessed}
\State $\kerr \leftarrow 4\lceil\frac{2+4.4\epsilon}{\epsilon^2}\ln{\!\frac{3}{\delta}}\rceil$ \Comment{halting threshold on the number of repeats}
\vspace*{0.3em}
\While{$r < \kerr$}
\State $w\leftarrow w + w_S$
\State $(u, \gamma_u) \leftarrow $ sample drawn by walking $t$ steps starting from $v$
\If {$u \in S$} \Comment{detect repeat}
\State $r \leftarrow r+1$ 
\Else
\State $S \leftarrow S \cup \{u\}$
\State $w_S \leftarrow w_S + \gamma_u$
\EndIf
\EndWhile
\State \textbf{return} $r/w$ \Comment{estimate of $1/\gamma$, i.e.\ of $\pi(v)$}
\end{algorithmic}
\end{algorithm}


%\clearpage
\subsection{Pseudocode of \taunest}
\label{apx:fma}
\renewcommand{\thealgorithm}{}
\begin{algorithm}[h]
\small
\caption{\taunest($\epsilon, \delta, v$)}
\begin{algorithmic}[1]
\State $S \leftarrow \{v\}$ \Comment{distinct states visited so far}
\State $D \leftarrow \{v:1\}$ \Comment{dictionary mapping $u$ to $\gamma_u$}
\State $w_S\leftarrow 1$ \Comment{$\sum_{u \in S} \gamma_u$ for the current $S$}
\State $w\leftarrow 0$ \Comment{will accumulate $w_S$}
\State $r\leftarrow 0$ \Comment{number of repeats witnessed}
\State $\kerr \leftarrow 4\lceil\frac{2+4.4\epsilon}{\epsilon^2}\ln{\!\frac{3}{\delta}}\rceil$ \Comment{halting threshold on the number of repeats}
\State $u \leftarrow v$ \Comment{current walk state}
\While{$r < \kerr$}
\State $w\leftarrow w + w_S$
\State $N \leftarrow \emptyset$ \Comment{new states visited}
\For{$i = 1$ to $t$} %\Comment{enlarge $S$ with $\le \tau'$ states}
\State $\bar{u} \leftarrow$ \step($u$)
  \If{$\bar{u} \notin D$} \Comment{$\bar{u}$ never visited before}
  \State $D[\bar{u}] = D[u]\,\cdot\,$\probe($u,\bar{u}$)$ / $\probe($\bar{u},u$)
  \State $N \leftarrow N \cup \{\bar{u}\}$
  \EndIf
\State $u \leftarrow \bar{u}$
\EndFor
\If {$u \in S$} \Comment{detect repeat}
\State $r \leftarrow r+1$ 
\EndIf
\State $w_S \leftarrow w_S + \sum_{u \in N} D[u]$
\State $S \leftarrow S \cup N$ 
\EndWhile
\State \textbf{return} $r/w$ \Comment{estimate of $1/\gamma$, i.e.\ of $\pi(v)$}
\end{algorithmic}
\end{algorithm}

\clearpage
\subsection{Experiments}
\label{sub:exp}
We experimentally evaluate \taupiest\ and \taunest\ against the algorithms of Lee et al.~\cite{Lee&2013} and Banerjee et al.~\cite{Lofgren&2015b} (see Section~\ref{sub:rel}).
All algorithms were ran on synthetic time-reversible Markov chains on $1$M states, created as follows.
We start from an undirected torus graph (i.e.\ a grid with periodic boundary) of $n = 1000 \times 1000$ nodes.
We then add $0.01n$ edges between random pairs of nodes, to reduce the mixing time and thus the cost (and running time) of the algorithms.
We add self-loops to all nodes to ensure ergodicity.
Finally, we weight the arcs according to two distributions:
\begin{itemize}
\item $\pi_{U}$ (uniform): each arc has weight $1$.
The norm is $\|\pi_{U}\| = 0.001$, or essentially $1/\sqrt{n}$.
\item $\pi_{S}$ (skewed): each arc is given an independent weight $1/X$ where $X \sim \mathcal{U}(0,1]$.
The norm is $\|\pi_{S}\| \simeq 0.07$.
\end{itemize}
For each weighted graph, we consider the time-reversible chain of the associated random walk.

We picked $v=0$ as the target node, which is equivalent to any other one (and indeed repeating the experiments on other nodes yielded the same results).
For all algorithms we set $\delta=0.1$.
For the algorithm of Banerjee et al.\ we set the minimum detection threshold at $\epsilon \pi(v)$, and for all other algorithms we set $\epsilon=0.25$.
One must then fix the random walk length: $t$ in our algorithms, $\ell$ in Banerjee et al., and $1/\Delta$ in Lee et al.
Setting the lengths to $\simeq \tau \ln(n)$ would make all algorithms satisfy the desired guarantees.
Since we do not know $\tau$, for each algorithm we proceed as follows.
We initially set the length of random walks to $l=10$.
We then perform three independent executions of the algorithm.
If all three executions return an estimate $\hat{\pi}(v)$ within a multiplicative factor $(1 \pm \epsilon)$ of $\pi(v)$, then we stop.
Otherwise, we increase $l$ by a factor $\sqrt{2}$ and repeat.
For each value of $l$ we record the average relative error $\hat{\epsilon} = \frac{|\hat{\pi}(v) - \pi(v)|}{\pi(v)}$ and the average total number of \step() and \probe() calls.
Figure~\ref{fig:1} shows how $\hat{\epsilon}$ decreases as the number of calls increases.

\begin{figure}[h]
\begin{minipage}{0.49\textwidth}
\centering
\includegraphics[width=\textwidth]{conv-1m-sparse-alpha0-inv-crop.pdf}
\end{minipage}
\begin{minipage}{0.02\textwidth}
\hspace{0.01\textwidth}
\end{minipage}
\begin{minipage}{0.49\textwidth}
\centering
\includegraphics[width=\textwidth]{conv-1m-sparse-alpha1-inv-crop.pdf}
\end{minipage}
\caption{cost incurred by the algorithms as their estimates converge towards $\pi(v)$. Left: chain with uniform distribution $\pi_{U}$. Right: chain with skewed distribution $\pi_S$.}
\label{fig:1}
\end{figure}

\taupiest\ and \taunest\ are the fastest candidates in all cases.
In the uniform chain, \taupiest\ is approached by the algorithm of Banerjee et al.\ at high accuracies.
This seems a confirmation of theory: \taupiest\ has complexity $\tilde{O}(\tau n^{-0.5})$ on a chain with uniform distribution, and the algorithm of Banerjee et al.\ has complexity $\tilde{O}(\tau^{1.5} n^{-0.5})$ on the ``typical'' target state with mass $\pi(v) \approx 1/n$.
If $\tau$ is not exceedingly large, the two complexities can translate into close performance in practice.
On the other hand, \taunest\ is neatly more efficient than previous algorithms.
To obtain a fairly accurate estimate of $\pi(v)$, say $\pm 50 \%$, it improves on their performance by two orders of magnitude -- and possibly by more on the skewed chain.
These results suggest that our algorithms are not only of theoretical interest, but also of practical value.
A final observation is that \taunest\ outperforms also \taupiest\ on the uniform chain. The complexity bounds we have are the same for both algorithms, but perhaps \taunest\ takes advantage of some specific structural properties of the chain we have used, which makes its complexity drop further.

