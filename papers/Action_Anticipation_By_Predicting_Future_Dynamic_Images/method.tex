\begin{figure*}
    \centering
  \includegraphics[width=0.90\textwidth]{imgs/overview_v4.png}
    \caption{\textbf{Overview of our approach}. We receive as an input a sequence of RGB video frames \textbf{(a)}. Then we use RGB images with windows size $T$ to compute the Dynamic Images for seen part of the video \textbf{(b)}. The last dynamic image of the seen part is used to feed our dynamic image generator and generate $\hat{D}_{t+1}$ \textbf{(c)}. Next, we feed {\em Dynamic} CNN with observed dynamic images and generated dynamic images and {\em Static} CNN with RGB images \textbf{(d)}. Finally, we fusion all the outputs of our recognition networks \textbf{(e)}.} 
  \label{fig:overview}
\end{figure*}

The objective of our work is to recognise human actions as early as possible from a video sequence depicting human action.
We present a method that hallucinates future motion from a partially observed human action sequence (RGB video clip).
Then we process these hallucinated future motion representations to make future action predictions a.k.a. action anticipation.
Our motion representation is based on dynamic images~\cite{bilen2016dynamic,Bilen2017}.
Dynamic images model dynamic information of a short video clip and summarise motion information to a single frame.
We present a method to hallucinate future dynamic images using a convolutional autoencoder neural network. 
We process generated dynamic images to predict future human actions using a CNN named \emph{dynamic CNN}. 
To improve action recognition performance further, we use observed still image appearance information and process them with a \emph{static CNN}.
Furthermore, we make use of dynamic images created from observed RGB data and use the same dynamic CNN to make predictions.
Therefore, we make use of three kinds of predictions and fuse them to make the final prediction (see Fig.~\ref{fig:overview}).
In the following section, we present some background about dynamic images~\ref{sec.background} and then we present our dynamic image generation model in section~\ref{sec.model}. Then we discuss loss functions in section~\ref{sec.digen} and how to train our model in section~\ref{sec.mtl}.
%
\subsection{Background}
\label{sec.background}
Dynamic images~\cite{bilen2016dynamic,Bilen2017} are a compact motion representation of videos which is useful for human action recognition. 
They summarise the temporal evolution of a short video clip (\eg 10 frames) to a single still RGB image. 
Dynamic images are constructed using the rank pooling~\cite{FernandoGMGT15}. 
Rank pooling represents a video as a parameters of a linear ranking function that is able to chronologically order the elements of a sequence $\left<I_1, ..., I_T\right>$.
Precisely, let $\psi(I_t) \in \mathbb{R}^{d}$ be a feature vector extracted from each individual frame in the video and $V_t = \frac{1}{t} \sum_{\tau=1}^t \psi(I_\tau)$ be the average of these features up to time $t$. 
The ranking function $S(t|\mathbf{d})$ predicts a ranking score for each frame at time $t$ denoted by $S(t|\mathbf{d}) = \langle \mathbf{d}, V_t\rangle$, where $\mathbf{d} \in \mathbb{R}^d$ is the parameter of the linear ranking function~\cite{FernandoGMGT15}.
The parameter set $\mathbf{d}$ is learned so that the score reflect the rank of each frame.
Therefore, the ranking score for later frame at time $q$ ($q>t$) is associated with a larger score, \ie $S(q|\mathbf{d}) > S(t|\mathbf{d})$.
Learning $\mathbf{d}$ is posed as a convex optimisation problem using the RankSVM~\cite{smola2004tutorial} formulation given as equation~\ref{eq:rank}.
%
\begin{equation}
 \begin{multlined}
\mathbf{d}^*  = \rho(I_1, ... , I_t ; \psi) = \underset{d}{\mathrm{argmin}}~ E(\mathbf{d}), \\
E(\mathbf{d}) = \frac{\lambda}{2}||\mathbf{d}||^2 + 
\frac{2}{T(T-1)} \times \displaystyle \sum_{q>t} \max\{0, 1 - S(q|\mathbf{d}) + S(t|\mathbf{d})\}.
\end{multlined}
\label{eq:rank}
\end{equation}
%
Optimising equation~\ref{eq:rank} defines a function $\rho(I_1 , . . . , I_T ; \psi)$ that maps a video sequence of length $T$ to a single vector denoted by $\mathbf{d^âˆ—}$. 
Since this parameter vector contains enough information to rank all frames in the video clip, it aggregates temporal information from all frames. 
Therefore, it  can be used as a video motion descriptor or a temporal descriptor.

When one applies this technique directly on RGB image pixels, the resulting $\mathbf{d^*}$ is known as the \emph{dynamic image}. 
The output $\mathbf{d^*}$ has same dimensions as input images.
Resulting dynamic image $\mathbf{d^*}$ summarises the temporal information of the RGB video sequence. 
Bilen~\etal~\cite{bilen2016dynamic} present an approximation to rank pooling which is faster.
This approximate rank pooling is essential for our method to hallucinate future dynamic images.
%
%\begin{align*}
%\nabla E(\vec{0}) &\propto \displaystyle \sum_{q>t} \nabla  \max\{0, 1 - S(q|\mathbf{d}) + S(t|\mathbf{d})\}|_{d=\vec{0}}.\\
%&= \displaystyle \sum_{q>t} \nabla \langle \mathbf{d}, V_t-V_q \rangle\\
%&= \sum_{q>t} V_t - V_q
%\end{align*}
%
Bilen~\etal~\cite{bilen2016dynamic} proved that $\mathbf{d^*}$ can be expressed by the following equation~\ref{eq:di}.
\begin{equation}
 \mathbf{d^*} = \sum_{t=1}^T \alpha_t I_t.
 \label{eq:di}
\end{equation}
%
The coefficients $\alpha_t$ are given by
%
%\begin{equation}
 $\alpha_t = 2(T - t + 1) - (T+1)(H_{T} - H_{t-1})$
%\end{equation}
%
where $H_t = \sum_{i=1}^t 1/t$  is the $t$-th Harmonic number and $H_0=0$. 
% Hence, the rank pooling operator can be reduced to:
% \begin{equation}
%  \hat{\rho}(I_1,...,I_T;\psi) = \sum_{t=1}^T \alpha_t \psi_t
%  \label{eq:dicomp}
% \end{equation}
We construct dynamic images using approximated rank pooling by taking a weighted sum of input image sequence where weights are given by predefined coefficients $\alpha$.

\subsection{Future motion prediction model}
\label{sec.model}
Given a collection of videos $X$ with corresponding human action class labels $Y$, our aim is to predict the human action label as early as possible.

Each video $X_i \in X$ is a sequence of frames $X_i= \left< I_1, I_2, \cdots, I_n \right> $ of variable length $n$. 
We process each sequence of RGB frames to obtain a sequence of dynamic images using equation~\ref{eq:di}. 
Instead of summarising the entire video with a single dynamic image, we propose to generate multiple dynamic images from a single video sequence using a fixed window size of length $T$. 
Therefore, each dynamic image is created using $T$ consecutive frames. 
We process each training video $X_i$ and obtain a sequence of dynamic images $\left< D_1, D_2, \cdots, D_{n} \right> $. 
%Obviously, the first $T-1$, dynamic images are constructed from less than $T$ static still frames using available frames according to equation~\ref{eq:di}.
%Rest of the dynamic images are created by processing all $T$ static RGB frames. 
Our objective is to train a model that is able to predict the future dynamic image $D_{t+k}$ given the current dynamic images up to time $t$ \ie $\left< D_1, D_2, \cdots, D_t \right>$. 
Therefore, we aim to model the following conditional probability distribution using a parametric model
\begin{equation}
P(D_{t+k} | \left< D_1, D_2, \cdots, D_t \right>; \Theta)
\label{eq.prb.model}
\end{equation} 
where $\Theta$ are the parameters of our generative model ($k\ge1$). 
We simplify this probabilistic model using the Markov assumption, hence now $k=1$ and condition only on the previous dynamic image $D_t$. Then our model simplifies to following equation~\ref{eq.prb.model.simple}.
\begin{equation}
P(D_{t+1} | D_t ; \Theta)
\label{eq.prb.model.simple}
\end{equation} 
The model in equation~\ref{eq.prb.model.simple} simplifies the training process. 
Furthermore, it may be possible to take advantage of different kinds of neural machine to implement the model in equation~\ref{eq.prb.model.simple} such as autoencoders \cite{baldi2012autoencoders}, variational conditional autoencoders \cite{kingma2014semi,sohn2015learning} and conditional generative adversarial networks \cite{mirza2014conditional}.

Now the challenge is to find a good neural technique and loss function to train such a model. 
We use a denoising convolutional autoencoder to hallucinate future dynamic images given the current ones.
Our convolutional autoencoder receives a dynamic image at time $t$ and outputs a dynamic image for next time step $t+1$. 
In practice, dynamic images up to time $t$ is observed, and we recursively generate dynamic images for time $t+1, \cdots, t+k$ using Markov assumption.
Although we use a denoising convolutional autoencoder, our idea can also be implemented with other generative models.
The autoencoder we use has 4 convolution stages. Each convolution has kernels of size $5 \times 5$ with a stride of $2$ and the number of features maps for the convolution layers are set to 64, 128, 256, and 512 respectively. Then the deconvolution is the inverted mirror of the encoding network (see Fig~\ref{fig:overview}), which is inspired by the architecture used in DCGAN \cite{RadfordMC15}.
%
%
Next, we discuss suitable loss functions for training the autoencoder.
%
\subsection{Loss functions for training the autoencoder}
\label{sec.digen}

First, we propose make use of reconstructive loss coined \emph{Dynamic Loss} to reduce the $\mathcal{L}_2$ distance between predicted dynamic image $\hat{D}_{t+1}$ and the ground truth dynamic image obtained from the training data $D_{t+1}$ as shown in equation~\ref{eq:dl}.
%
\begin{equation}
 \mathcal{L}_{DL} = ||\hat{D}_{t+1} - D_{t+1}||_2
 \label{eq:dl}
\end{equation}
%
Even though this loss function helps us to generate expected future dynamic image, it does not guarantee that the generated dynamic image is discriminative for action anticipation. 
Indeed, we would like to generate a dynamic image that contains more action class information.
Therefore, we propose to explore the teacher-student networks~\cite{hinton2015distilling} to teach the autoencoder to produce dynamic images that would be useful for action anticipation.
First, we train a teacher CNN which takes dynamic images as input and produces the action category label. Let us denote this teacher CNN by $f(D_i;\Theta_{cnn})$ where it takes dynamic image $D_i$ and produces the corresponding class label vector $\hat{y_i}$. 
This teacher CNN that takes dynamic images as input and outputs labels is called \emph{Dynamic CNN} (see Fig~\ref{fig:overview}).
This teacher CNN is trained with cross-entropy loss~\cite{szegedy2017inception}.
Let us denote our generator network as $g(D_t;\Theta)\rightarrow D_{t+1}$. We would like to take advantage of the teacher network $f(;\Theta_{cnn})$ to guide the student generator $g(D_t;\Theta)$ to produce future dynamic images that are useful for classification. Given a collection of current and future dynamic images with labels, we train the generator with the cross-entropy loss as follows:
%
\begin{equation}
 \mathcal{L}_{CL} = - \sum_t y_i \log f(g(D_t;\Theta);\Theta_{cnn})
\end{equation}
where we fix the CNN parameter $\Theta_{cnn}$. Obviously, we make the assumption that CNN $f(D_i;\Theta_{cnn})$ is well trained and has good generalisation capacity. 
We call this loss as the \emph{classification loss} which is denoted by $\mathcal{L}_{CL}$. 
In theory, compared to original dynamic images~\cite{bilen2016dynamic,Bilen2017}, our generated dynamic images are class specific and therefore discriminative.

Motion and appearance are related.
Optical flow depends on the appearance of two consecutive frames.
Dynamic images depends on the evolution of appearance of several consecutive frames.
Therefore, it is important verify that generated future motion actually adhere to future expected appearance.
Another advantage of using dynamic images to generate future motion is the ability exploit this property explicitly.
We make use of future expected appearance to guide the generator network to produce accurate dynamic images.
Let us explain what we mean by this. 
When we generate future dynamic image $D_{t+1}$, as demonstrated in equation \ref{eq:rgb}, implicitly we also recover the future RGB frame $I_{t+1}$. 
Using this equation~\ref{eq:rgb}, we propose so-called \emph{static loss} (SL) (equation~\ref{eq:sl}) that consists of computing the $\mathcal{L}2$ loss between the generated RGB image $\hat{I}_{t+1}$ and real expected image $I_{t+1}$.
% 
\begin{align}
\label{eq:rgb}
D_{t+1} &= \displaystyle \sum_{i=1}^{T} \alpha_{i} I_{t+1+i} \\ \nonumber
D_{t+1} &= \alpha_{T}I_{T+t+1} \displaystyle \sum_{i=1}^{T-1} \alpha_{i} I_{t+1+i} \\ \nonumber
I_{T+t+1} &= \frac{D_{t+1} - \displaystyle \sum_{i=1}^{T-1} \alpha_{i} I_{t+1+i}}{\alpha_{T}}
%  D_{t+1} &=  \displaystyle \sum_{t=1}^{\tau} \alpha_t I_{t+1} \\
%  D_{t+1} &=  \alpha_{\tau}I_{t+1} + \displaystyle \sum_{t=1}^{\tau-1} \alpha_t I_{t+1} \\
%  I_{t+1} &= \frac{D_{t+1} - \displaystyle \sum_{t=1}^{\tau-1} \alpha_t I_{t+1}}{\alpha_{\tau}}
 %\label{eq:rgb}
\end{align}
%
The applicability of static loss does not limit only to matching the future expected appearance, but also we guide the autoencoder model $g(;\Theta)$ to use all implicitly generated RGB frames from $\hat{I}_{t+2}$ to $\hat{I}_{T+t+1}$ making future dynamic image better by modeling the evolution of appearance of static images. Indeed, this is a better loss function than simply taking the dynamic loss as in equation~\ref{eq:dl}.
%
\begin{equation}
 \mathcal{L}_{SL} = ||\hat{I}_{T+t+1} - I_{T+t+1}||_2
 \label{eq:sl}
\end{equation}
%
\subsection{Multitask learning}
\label{sec.mtl}
We train our autoencoder with multiple losses, the static loss ($\mathcal{L}_{SL}$), the dynamic loss ($\mathcal{L}_{DL}$) and the classification loss ($\mathcal{L}_{CL}$). By doing so, we aim to generate dynamic images that are good for the classification, as well as representative of future motion. 
With the intention to enforce all these requirements, we propose to train our autoencoder with batch wise multitask manner. 
Overall, one might write down the global loss function $\mathcal{L} =  \lambda_{sl} \mathcal{L}_{SL} + \lambda_{dl} \mathcal{L}_{DL} + \lambda_{cl} \mathcal{L}_{CL}$. 
However, instead of finding good scalar weights $\lambda_{sl}, \lambda_{dl},$ and $\lambda_{cl}$, we propose to divide each batch into three sub-batches, and optimise each loss using only one of those sub batches. Therefore, during each batch, we optimise all losses with different sets of data. 
We found this strategy leads to better generalisation than optimising a linear combination of losses.

\subsection{Inference}
During inference, we receive RGB frames from a video sequence as input. Using those RGB frames, we compute \textit{dynamic images} following equation \ref{eq:di} with a window size length $T=10$. In the case that the amount of frames is less that what is needed to compute the dynamic image i.e. 10\% of the video is observed, we compute the dynamic image with the available frames according to equation~\ref{eq:di}.
%Obviously, the first $T-1$, dynamic images are constructed from less than $T$ static still frames using available frames according to equation~\ref{eq:di}.
We use the last dynamic image ($D_t$) to predict the following dynamic image ($\hat{D}_{t+1}$). 
We repeat this process to generate $k$ number of future dynamic images using Markov assumption.
We process each observed RGB frame, observed dynamic images and generated dynamic images by respective static and dynamic CNNs that are trained to make predictions (see Fig.~\ref{fig:overview}). 
Then, we obtain a score vector for each RGB frame, dynamic image and generated dynamic image.
We sum them together and use temporal average pooling to make the final prediction. 
