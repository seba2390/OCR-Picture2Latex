When interacting with other people, human beings have the ability to anticipate the behaviour of others and act accordingly. This ability comes naturally to us and we make use of it subconsciously. Almost all human interactions rely on this {\em action-anticipation} capability. For example, when we greet each other, we tend to anticipate what is the most likely response and act slightly proactively. When driving a car, an experienced driver can often predict the behaviour of other road users. Tennis players predict the trajectory of the ball by observing the movements of the opponent. The ability to anticipate the action of others is essential for our social life and even survival. It is critical to transfer this ability to computers so that we can build smarter robots in the future, with better social interaction abilities that think and act fast.

In computer vision, this topic is referred to as {\em action anticipation}~\cite{ma2016learning,ryoo2011human,aliakbarian2017encouraging,soomro2016online,soomro2016predicting} or early action prediction~\cite{lan2014hierarchical,yu2012predicting}. 
Although action anticipation is somewhat similar to {\em action recognition}, they differ by the information being exploited.  Action-recognition processes the entire action within a video and generate a category label, whereas action-anticipation aims to recognise the action {\em as early as possible}.  More precisely, action-anticipation needs to predict the future action labels as early as possible by processing fewer image frames (from the incoming video), even if the human action is still in progress. 

Instead of directly predicting action labels~\cite{aliakbarian2017encouraging}, we propose a new method that generates future motion representation from partial observations of human action in a video. We argue that the generation of future motion representation is more intuitive task than generating future appearance, hence easier to achieve. A method that is generating future appearance given the current appearance requires to learn a conditional distribution of factors such as colour, illumination, objects and object parts, therefore, harder to achieve.  In contrast, a method that learns to predict future motion does not need to learn those factors.  Furthermore, motion information is useful for recognising human actions~\cite{Bilen2017,Simonyan2014} and can be presented in various image forms~\cite{Bilen2017,Ahad2012}.

In this paper we propose to predict future motion representation for action anticipation. 
Our method hallucinates what is in the next motion representation of a video sequence given only a fraction of a video depicting a partial human action. 
We make use of a convolutional autoencoder network that receives a motion image as input at time $t$ and outputs a motion image for the future (\eg $t+1$). 
Using Markov assumption, we generate more motion images of the future using already generated motion images (\ie we generate motion images for time $t+1, \cdots, t+k$).
Then we process generated motion images using Convolutional Neural Network (CNN) to make action predictions for the future.
As we are able to generate future motion images, now we are able to predict human actions only observing few frames of a video containing an action.  

We train our action anticipation and motion generation network with several loss functions.
These loss functions are specifically tailored to generate accurate representations of future motion and to make accurate action predictions.
%
\begin{figure}[t]
    \centering
    \includegraphics[width=0.49\textwidth]{imgs/overviewLosses.png}
    \caption{Training of our generation module using multiple loss functions. \textbf{a) \textit{Dynamic Loss}} evaluates the difference in motion information between predicted and ground truth dynamic image using $\mathcal{L}_2$ norm. \textbf{b) \textit{Classification Loss}} takes care of generating dynamic images that are useful for action anticipation. \textbf{c) Static Loss} computes the $\mathcal{L}_2$ norm between predicted and ground truth RGB information at $t+k$ to evaluate the difference in appearance.} 
    \label{fig:losses}
\end{figure}

Clearly, the motion information depends on the appearance and vice versa. 
For example, motion representations such as the optical flow relies on two consecutive RGB frames. 
Similarly, the content of dynamic images~\cite{Bilen2017} relies on the appearance of consecutive frames.
The relationship between static appearance and motion information is somewhat surprising and mysterious~\cite{Carreira2017}. 
Recently, proposed dynamic images has managed to explore this relationship to some degree of success~\cite{Bilen2017}. 
In particular, dynamic images summarise the temporal evolution of appearance of few frames (\eg 10 frames) into a single image.
Therefore, this motion summary image (a.k.a. dynamic image) captures the motion information of those frames.
In this work, we hallucinate dynamic images for the future and use them for the task of action anticipation
\footnote{However, the main concept of this paper is applicable for other types of motion images as well (optical flow, motion history images).}.

We generate dynamic images using both expected appearance and motion of the future.
Specifically, future dynamic images are generated by taking into account both reconstructive loss (coined \emph{dynamic loss}) and future expected appearance which is coined \emph{static loss}.
As motion and appearances should adhere to each other, static loss is designed to satisfy expected future appearance in the generated
dynamic images.
In addition to that our generated dynamic images make use of class information and therefore discriminative.
These loss functions are tailored to generate accurate future dynamic images as is depicted in Fig.~\ref{fig:losses}.
In a summary, we make the following contributions:
\begin{itemize}
 \item Using a simple CNN architecture, we demonstrate the effectiveness of dynamic images for future content prediction.
 \item We design a set of effective loss functions to produce accurate future dynamic images.
 \item We obtain state-of-the-art performance for early activity recognition on standard benchmarks.
\end{itemize}


