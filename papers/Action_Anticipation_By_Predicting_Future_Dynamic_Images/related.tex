Action prediction and anticipation literature can be classified into deep learning and non-deep learning-based methods. 

Human activity prediction is studied using integral histograms of spatial-temporal bag-of-features coined dynamic bag-of-words in the early days~\cite{ryoo2011human}. Yu~\etal~\cite{Yu2012} propose to use spatial-temporal action matching for early action prediction task using spatial-temporal implicit shape models. Li~\etal~\cite{Li2014}, propose to explore sequence mining where a series of actions and object co-occurrences are encoded as symbolic sequences. Kong~\etal~\cite{kong2014discriminative} explore the temporal evolution of human actions to predict the class label as early as possible. This model~\cite{kong2014discriminative} captures the temporal dynamics of human actions by explicitly considering all the history of observed features as well as features in smaller temporal segments. More recently, Soomro~\etal~\cite{soomro2016predicting} propose to use binary SVMs to localise and classify video snippets into sub-action categories and obtain the final class label in an online manner using dynamic programming. Because it is needed to train one classifier per sub-action,~\cite{soomro2016online} extended this approach using a structural SVM formulation. Furthermore, this method introduces a new objective function to encourage the score of the correct action to increase as time progresses~\cite{soomro2016online}.

While all above methods utilise handcrafted features, most recent methods use deep learning approaches for action anticipation~\cite{ma2016learning,aliakbarian2017encouraging,vondrick2016anticipating}. Deep learning-based methods can be primarily categorised into two types; 1. methods that rely on novel loss functions for action anticipation~\cite{ma2016learning,aliakbarian2017encouraging,jain2016recurrent} and 2. methods that try to generate future content by content prediction~\cite{vondrick2016anticipating}.

In this context,~\cite{ma2016learning} propose to use a Long Short-Term Memory (LSTM) with ranking loss to model the activity progression and use that for effective action prediction task. They use Convolutional Neural Network (CNN) features along with a LSTM to model both spatial and temporal information. Similarly, in~\cite{jain2016recurrent}, a new loss function known as the exponentially growing loss is proposed. It tries to penalize errors increasingly over time using a LSTM-based framework. Similarly, in~\cite{aliakbarian2017encouraging}, a novel loss function for action anticipation that aims to encourage correct predictions as early as possible is proposed. The method in~\cite{aliakbarian2017encouraging} tries to overcome ambiguities in early stages of actions by preventing false negatives from the beginning of the sequence. Furthermore, a recently online action localisation method is presented which can also be used for online early action predictions~\cite{Singh2017}. However, this method primarily focuses on online action detection.

Instead of predicting the future class label, in~\cite{vondrick2016anticipating}, the authors propose to predict the future visual representation. However, the main motivation in~\cite{vondrick2016anticipating} is to learn representations using unlabeled videos. Our work is different from ~\cite{vondrick2016anticipating} as we are predicting the future motion using dynamic images. We make use reconstruction loss, class information loss, and expected future appearance as a guide to predict future motion images. As our generated dynamic images are trained for action anticipation, they are class specific and different from original dynamic images~\cite{bilen2016dynamic}. As demonstrated, our generated dynamic images are more effective than original dynamic images for action anticipation task. Gao~\etal~\cite{gao2017red} propose to generate future appearance using LSTM autoencoder to anticipate actions using both regression loss and classification loss. We argue that predicting future appearance representation is a complex task. We believe that action anticipation can benefit from motion prediction more than challenging appearance prediction.

Predicting the future content has been explored on other related problems in other domains of computer vision. Some of the work focuses on predicting (or forecasting) the future trajectories of pedestrians \cite{kitani2012activity} or predicting motion from still images \cite{kitani2012activity,pellegrini2009you}. However, we are the first to show the effectiveness of predicting good motion representations for early action anticipation.