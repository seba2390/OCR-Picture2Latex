\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage{cvpr}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}

% Include other packages here, before hyperref.

% If you comment hyperref and then uncomment it, you should delete
% egpaper.aux before re-running latex.  (Or just hit 'q' on the first latex
% run, let it finish, and you should be clear).
\usepackage[breaklinks=true,bookmarks=false]{hyperref}

\cvprfinalcopy % *** Uncomment this line for the final submission

\def\cvprPaperID{****} % *** Enter the CVPR Paper ID here
\def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}

% Pages are numbered in submission mode, and unnumbered in camera-ready
%\ifcvprfinal\pagestyle{empty}\fi
\setcounter{page}{1}
\begin{document}

%%%%%%%%% TITLE
\title{Action Anticipation Through Generating The Future Video Representantion.}

\author{Cristian Rodriguez \qquad Basura Fernando \qquad Hongdong Li \\
% For a paper whose authors are all at the same institution,
% omit the following lines up until the closing ``}''.
% Additional authors and addresses can be added with ``\and'',
% just like the second author.
% To save space, use either the email address or home page, not both
ARC Centre if Excellence for Robotic Vision (ACRV)\\
Research School of Engineering, Australian National University\\
{\tt\small firstname.lastname@anu.edu.au}
}

\maketitle
%\thispagestyle{empty}


\begingroup
\let\clearpage\relax
% \include{content}
\begin{abstract}
In this work we propose a method to generate image video representation that will be used for early activity recognition and video forecasting.
\end{abstract}

\section{Introduction}
% present the problem and explain why action anticipation is important 1 paragraph?
Almost all human interaction is based in certain way of predicting what will be the next action of others humans. For examples, when we are driving cars we tend to predict if some pedestrian is going to cross in a share street to avoid any accident. When we we are playing sports, like tennis, we tend to predict where the ball will go depending on the human pose. When we greet another people, we try to predict what is the way that the other person want to salute and we perform depending on that, i.e. handshake, high five, hug or kiss. This particular ability, that is very important for humans, has to be transfer to computers if we want to create safer robots with better sport and social abilities.

In the intention to understand actions from a video, computer vision community had defined two different task. Action recognition, which aims to recognize what is the action that has been perform in a complete video sequence, with great progress CITES. Action anticipation, which become popular recently and tries to recognize the action of a video sequence as early as it is possible, thus, using only a few frames of the video sequence \cite{aliakbarian2017encouraging, ma2016learning, ryoo2011human, lan2014hierarchical, soomro2016online, soomro2016predicting, yu2012predicting}. 

In this paper, we introduce a method to generate future video representation that can be used for action anticipation and hallucinate what is the next frame in the sequence. In specific we are using an autoencoder network that receive as an input image video representation (dynamic image) in time $t$ and  the output is the dynamic image in time $t+1$.

Moreover, we present different loss functions that has been used in a multitask learning scheme to generate better dynamic images.

In summary, we make the following contributions:
\begin{itemize}
 \item We hallucinate future image video representations. 
 \item We propose several loss to train our generative module using multitask learning scheme.
 \item We generate a future appearance image using the generated future image video representation .
 \item Activity recognition using the generated future appearance image and video representation.
 \item We archieve state-of-the-art performance for early activity recognition.
\end{itemize}

The rest of the paper is organized as follows: Section 2 provides an overview of related work in image video representation, early action recognition and video forecasting. Section 3 describe our method to generate future video representations. Section 4 presents our experiments over JHMDB-21 and UCF101 . Section 5 summarizes our findings and discusses future directions.

\section{Related Work}
\section{Method}
\subsection{Dynamic Image}

Dynamic image has been proposed by Bilen et al. in 2016 \cite{bilen2016dynamic} are still RGB image that summarize the appearance and dynamics of a whole video sequence, as can be seen in the Figure \ref{fig:dis}. It has been used the rank pooling proposed by Fernando et al \cite{FernandoGMGT15} to create such still image. Rank pooling consist on represent a video as a ranking function for its frames $I_1, ..., I_n$. In more details let ψ(It ) $\in$ Rd be a representation or feature vector extracted from each individual frame It in the video. Let Vt = τ =1 ψ(Iτ ) be
ttime average of these features up to time t. The ranking
function associates to each time t a score S(t|d) = hd, Vt i,
where d ∈ Rd is a vector of parameters. The function pa-
rameters d are learned so that the scores reflect the rank of
the frames in the video. Therefore, later times are associ-
ated with larger scores, i.e. q > t =⇒ S(q|d) > S(t|d).
Learning d is posed as a convex optimization problem using
the RankSVM [26] formulation:

d∗ = ρ(I1, . . . , IT ; ψ) = argmin E(d),
d
λ
E(d) = kdk2 +
 (1)
2
2
 X
×
 max{0, 1 − S(q|d) + S(t|d)}.
T (T − 1)q>t
The first term in this objective function is the usual
quadratic regularizer used in SVMs. The second term is
a hinge-loss soft-counting how many pairs q > t are incor-
rectly ranked by the scoring function. Note in particular that
a pair is considered correctly ranked only if scores are sep-
arated by at least a unit margin, i.e. S(q|d) > S(t|d) + 1.
Optimizing eq. (1) defines a function ρ(I1 , . . . , IT ; ψ)
that maps a sequence of T video frames to a single vector
d∗ . Since this vector contains enough information to rank
all the frames in the video, it aggregates information from
all of them and can be used as a video descriptor. In the rest
of the paper we refer to the process of constructing d∗ from
a sequence of video frames as rank pooling.

Computing a dynamic image entails solving the opti-
mization problem of eq. (1). While this is not particularly
slow with modern solvers, in this section we propose an ap-
proximation to rank pooling which is much faster and works
as well in practice. Later, this technique, which we call
approximate rank pooling, will be critical in incorporating
rank pooling in intermediate layers of a deep CNN and to
allow back-prop training through it.


% Although compute dynamic image is not difficult for moderns solvers a fast dynamic image computation is proposed in \cite{bilen2016dynamic}, which is This approximation of rank pooling will be used to generate the last RGB image of a dynamic image.


\begin{figure}[ht!]
 \centering
 \includegraphics[width=0.20\textwidth]{imgs/DI1.jpg}
 \hspace{-4pt}
 \includegraphics[width=0.20\textwidth]{imgs/DI2.jpg}\\
 \vspace{0pt}
 \includegraphics[width=0.20\textwidth]{imgs/DI3.jpg}
 \hspace{-4pt}
 \includegraphics[width=0.20\textwidth]{imgs/DI4.jpg}
 \caption{Samples of dynamic images that summarize videos from UCF101 dataset.}
 \label{fig:dis}
\end{figure}

\begin{equation}
 DI = \displaystyle \sum_{t+1}^T \alpha_t I_t
\end{equation}

\begin{equation}
 \alpha_t = 2t - T -1
\end{equation}
\subsection{Future Dynamic Image Generation}
\begin{equation}
 \mathcal{L}_{DL} = ||\hat{DI}_{t+1} - DI_{t+1}||_2
\end{equation}

\subsection{Future Frame Generation}
\begin{align}
 DI_{10} &=  \displaystyle \sum_{i=1}^{10} \alpha_i I_t \\
 DI_{10} &=  \alpha_{10}        I_{10} + \displaystyle \sum_{i=1}^{9} \alpha_i I_t \\
 I_{10} &= \frac{DI_{10} - \displaystyle \sum_{i=1}^{9} \alpha_i I_t}{\alpha_{10}}
\end{align}

\subsection{Multitask learning}
\begin{equation}
 \mathcal{L}_{SL} = ||\hat{I}_{t+1} - I_{t+1}||_2
\end{equation}

\begin{equation}
 \mathcal{L}_{CL} = - \sum_i y_i \log \hat{y}_i
\end{equation}

\begin{equation}
 \mathcal{L}_{M} = \max\{0, 1 - (d_{t}^g - d_{t+1}^g)\}
\end{equation}
Where, $d_{t}^g = || \hat{DI}_{t+1} - DI_{t} ||_2$ and $d_{t+1}^g = || \hat{DI}_{t+1} - DI_{t+1} ||_2$

\section{Experiments and Results}
\subsection{Datasets}

We perform our experiments using two popular datasets for action recognition UCF101 \cite{soomro2012ucf101} and JHMDB21 \cite{jhuang2013towards} which are briefly describe below.

\textbf{UCF-101} dataset consists of 13,320 videos (each contains a single action) of 101 action classes including a broad set of activities such as sports, playing musical instruments and human-object interaction, with an average length of 7.2 seconds. UCF-101 is one of the most challenging datasets due to its large diversity in terms of actions and to the presence of large variations in camera motion, cluttered background and illumination conditions. There are three standard training/test splits for this dataset. In our comparisons to the state-of-the-art for both action anticipation and recognition, we report the average accuracy over the three splits.

\textbf{JHMDB-21} is a subset of the HMDB51 dataset containing 928 videos and 21 action classes. Similar to UCF-101 dataset, each video contains one action starting from the beginning of the video.
\subsection{}
\begin{table}[ht!]
    \centering
  \begin{tabular}{lccc}
  \hline
  & \textbf{JHMDB21} & \textbf{UCF101} \\ \hline
Single DI & 45\% & \\ 
Multiple DI & 48\% &\\
Single RGB  & 39\%  & \\
Multiple RGB & 41\% & \\ 
MDI + MRGB  & 53\% &\\ 
\hline
  \end{tabular}
\end{table}

\subsection{Comparison}

The prediction of the action is evaluated using the accuracy over the percentage of interaction observed \cite{soomro2016online, aliakbarian2017encouraging}. Another standard practice to report the action anticipation is using the so-called earliest and latest prediction accuracies. However, there is no real agreement on the proportion of frames that the earliest setting corresponds to. Therefore we have to use for each dataset of the proportion that has been employed by the baselines (i.e., either 20\% or 50\%).

\begin{table}[ht!]
    \centering
  \begin{tabular}{lcc}
    \multicolumn{3}{c}{\textbf{JHMDB-21}}\\ \hline
    Method & Earliest & All video \\ \hline
    DP-SVM & 5\% & 46\% \\
S-SVM & 5\% & 43\% \\
Where/What & 10\% & 43\% \\
Ranking Loss & 29\% & 43\% \\
Context-Aware+Loss of & 28\% & 43\% \\
Context-Aware+Loss of & 33\% & 39\% \\
E-LSTM \cite{aliakbarian2017encouraging} & 55\% & 58\% \\ \hline
    Ours (1) & 42.9\% & 53.3\% \\ 
    Ours (2) & \% & 53\% \\ \hline
  \end{tabular}
\end{table}

\begin{table}[ht!]
    \centering
  \begin{tabular}{lcc}
    \multicolumn{3}{c}{\textbf{UCF-101}}\\ \hline
    Method & Earliest & All video \\ \hline
    Context-Aware+Loss of & 30.6\% & 71.1\% \\
    Context-Aware+Loss of & 22.6\% & 73.1\% \\
    E-LSTM \cite{aliakbarian2017encouraging} & 80.5\% & 83.4\% \\ \hline
    Ours (1) & - & - \% \\ 
    Ours (2) & - & - \% \\ \hline
  \end{tabular}
\end{table}

\section{Conclusions}
\endgroup

{\small
\bibliographystyle{ieee}
\bibliography{egbib}
}

\end{document}
