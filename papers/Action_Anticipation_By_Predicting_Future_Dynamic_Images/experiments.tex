In this section, we perform a series of experiments to evaluate our action anticipation method. First, we present results for action recognition using the \emph{static CNN} and the \emph{dynamic CNN} in section~\ref{sec.exp.action}. Then, we evaluate the impact of different loss functions for generating future dynamic images in section~\ref{sec.exp.loss}. After that in section~\ref{sec.act.eva}, we compare our method with state-of-the-art techniques for action anticipation. Finally, we present some other additional experiments to further analyse our model in sections~\ref{sec.exp.further}.


\textbf{Datasets} We test our method using three popular datasets for human action analysis JHMDB~\cite{jhuang2013towards}, UT-Interaction~\cite{UTInteractionData} and  UCF101-24~\cite{soomro2012ucf101}, which have been used for action anticipation in recent prior works~\cite{aliakbarian2017encouraging,soomro2016predicting,Singh2017}. % .

\textbf{JHMDB} dataset is a subset of the challenging HMDB51 dataset \cite{kuehne2011hmdb}. JHMDB is created by keeping action classes that involve a single person action. Videos have been collected from different sources such as movies and the world-wide-web. JHMDB dataset consists of 928 videos and 21 action classes. Each video contains one human action which usually starts at the beginning of the video. Following the recent literature for action anticipation~\cite{aliakbarian2017encouraging}, we report the average accuracy over the three splits and report results for so called \emph{earliest} setup. For earliest recognition, action recognition performance is measured only after observing 20\% of the video. To further understand our method, we also report recognition performance w.r.t. time (as a percentage).
\textbf{UT-Interaction} dataset (UTI) contains 20 video sequences where the average length of a video is around 1 minute. These videos contain complete executions of 6 human interaction classes: shake-hands, point, hug, push, kick and punch. Each video contains at least one execution of an interaction, and up to a maximum of 8 interactions. There are more than 15 different participants with different clothing. The videos are recorded with 30fps and with a resolution of 720 x 480 which we resize to 320 x 240. To evaluate all methods, we use recommended 10-fold leave-one-out cross-validation per set and report the mean performance over all sets. 
\textbf{UCF101-24} dataset is a subset of the challenging UCF101 dataset. This subset of 24 classes contains spatio-temporal localisation annotation. It has been constructed for THUMOS-2013 challenge\footnote{http://crcv.ucf.edu/ICCV13-Action-Workshop/download.html}. On average there are 1.5 action instances per video, each instance cover approximately 70\% of the duration of video. We report the action-anticipation accuracy for set 1, as has been done previously in \cite{Singh2017}. 

%\err{Moreover, we follow the standard practice of using the annotations provided with the dataset to split each video into sequence containing just individual actions.}

\subsection{Training of {\em Static} and {\em Dynamic} CNNs.}
\label{sec.exp.action}
\begin{table}[t]
  \centering
  \begin{tabular}{lc@{\hskip 0.1in}cc}
  \hline
  & \textbf{JHMDB} & \textbf{UT-Interaction} \\ \hline%%&  \textbf{UCF101-24} \\ \hline
Static CNN & 55.0\% & 70.9\%  \\%& 88.62\% \\ 
Dynamic CNN & 54.1\% & 71.8\% \\%& 87.09\% \\ 
\hline
  \end{tabular}
  \caption{Action recognition performance using dynamic and RGB images over JHMDB and UT-Interaction datasets. Action recognition performance is measured at frame level.}
  \label{tab:compl}
\end{table}
% %
In this section, we explain how we train our \emph{static} and \emph{dynamic} CNNs (see Fig.~\ref{fig:overview}).
Similar to~\cite{bilen2016dynamic,Bilen2017}, we train a \emph{Static CNN} for RGB frame-based video action recognition and a \emph{Dynamic CNN} for dynamic image-based action recognition. In all our experiments, each dynamic image is constructed using 10 RGB frames (T=10). We use different data augmentation techniques to reduce the effect of over-fitting. Images are randomly flipped horizontally, rotated by a random amount in a range of -20 to 20 degrees, horizontally shifted in a range of -64 to 64 pixels, vertically shifted in a range of -48 to 48 pixels, sheared in a range of 10 degrees counter-clockwise, zoomed in a range of 0.8 to 1.2 and shifted channels in a range of 0.3. We make use of pre-trained Inception Resnet V2 \cite{szegedy2017inception} to fine-tune both \emph{Static CNN} and the \emph{Dynamic CNN} using a learning rate of 0.0001. We use a batch size of 32 and a weight decay of 0.00004. We use ADAM \cite{kingma2015adam} optimizer to train these networks using epsilon of 0.1 and beta 0.5.
Action recognition performance using these CNNs for JHMDB and UTI datasets are reported in Table \ref{tab:compl}. Note that the action recognition performance in Table \ref{tab:compl} is only at frame level (not video level).
% 
We use these trained {\em Static} and {\em Dynamic} CNNs in the generation of future motion representation, dynamic images, and action anticipation tasks.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Impact of loss functions. %
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Impact of loss functions.}
\label{sec.exp.loss}
In this section we investigate the effectiveness of each loss function, explained in section~\ref{sec.digen}, in the generation process of future dynamic images. We evaluate the quality of the generated dynamic images in a {\em quantitative} evaluation. Using the dynamic CNN to report action recognition performance over generated dynamic images. %Then, we make a {\em qualitative} evaluation of the generated dynamic images using the most effective loss. %through a visual inspection, which is quite subjective (see Figure~\ref{fig:visual}).

%\textbf{Quantitative evaluation}.
We perform this experiment constructing a sequence of dynamic images using equation~\ref{eq:di} for each test video in the dataset. Then for each test dynamic image, we generate the future dynamic image using our convolutional autoencoder. Therefore, the number of generated dynamic images is almost equal to real testing dynamic images. Then we use our dynamic CNN (which has been pretrained in previous section) to evaluate the action recognition performance of generated dynamic images (\textbf{DIg}). Using this approach we can evaluate the impact of several loss functions in the generation of dynamic images. 

We use the first split of JHMDB and the first set of UTI to perform this experiment. We make use of the three proposed losses in section~\ref{sec.digen}: dynamic-loss ($\mathcal{L}_{DL}$), class-based loss ($\mathcal{L}_{CL}$) and static-loss ($\mathcal{L}_{SL}$) to train our autoencoder. We train the convolutional autoencoder using ADAM solver with a batch size of 32, a learning rate of 0.0001. We train our model for 30 epochs using the same augmentation process used in section~\ref{sec.exp.action}. 

We use the generalisation performance of {\em real dynamic images} from Table~\ref{tab:compl} as a reference to estimate the quality of generated dynamic images. Since, we measure the performance of generated dynamic images in the same way.

%From Table~\ref{tab:compl}, we know the generalisation performance of \emph{real dynamic image} and we measure the performance of generated dynamic images the same way so that we are able to estimate the quality of generated dynamic images for action anticipation.

% \vspace{-0.5cm}
\begin{table}[t]
    \centering
\begin{tabular}{lc@{\hskip 0.1in}c}
\hline                                                         & \textbf{JHMDB-21} &  \textbf{UT-Interaction}\\ \hline
$\mathcal{L}_{DL}$                                       & 42.8\%              &  64.3\%\\
$\mathcal{L}_{SL}$                                       & 49.5\%              &  64.2\%\\
$\mathcal{L}_{DL} + \mathcal{L}_{SL}$                    & 53.4\%              &  66.5\%\\
$\mathcal{L}_{DL} + \mathcal{L}_{CL}$                    & 52.5\%              &  64.5\%\\
$\mathcal{L}_{DL} + \mathcal{L}_{SL} + \mathcal{L}_{CL}$ & 54.0\%              &  68.4\%\\ \hline
\end{tabular}
  \caption{Results of using multitask learning to generate future dynamic images.}
  \label{tab:digen}
\end{table}
% \vspace{-0.5cm}

As can be seen in Table~\ref{tab:digen}, a combination of $\mathcal{L}_{DL}$, $\mathcal{L}_{CL}$ and $\mathcal{L}_{SL}$ gives excellent recognition performance of 54.0\% for the generated dynamic images which is very close to the model performance of single dynamic CNN 54.1\% in the case of JHMDB dataset. Indicating that our generative model along with loss functions are capable of generating representative and useful future dynamic images.
A similar trend can be seen for UTI dataset. 
Notice that the $\mathcal{L}_{DL}$ and $\mathcal{L}_{SL}$ already produce good recognition performance on JHMDB and UTI datasets, which suggest that those losses can generated images that understand the human motion. However, those generated images are not class specific.
%Surprisingly, the static loss $\mathcal{L}_{SL}$ alone gives good recognition performance on JHMDB dataset. 
%On JHMDB and UTI dataset, both static loss and the dynamic loss seems effective.
We conclude that convolutional autoencoder model trained with three losses is able to generate robust future dynamic images. These generated dynamic images are effective in action recognition.
%\textbf{Qualitative evaluation}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%	Action Anticipation
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Action anticipation}
\label{sec.act.eva}
Our action anticipation network consist of a {\em static} CNN and a {\em dynamic} CNN (see Fig~\ref{fig:overview}). Our action anticipation baseline uses observed multiple RGB frames and multiple dynamic images similar to~\cite{bilen2016dynamic}. In addition to that our method generates K number of future dynamic images and make use of them with dynamic CNN. Action anticipation performance is evaluated at different time steps after observing fraction of the video (\ie, 10$\%$, 20$\%$, $\cdots$, 100$\%$ of the video). Results are shown in Figure~\ref{fig:ratiovideo}. We can see that the most significant improvement for JHMDB is obtained at 20\% which is an enhancement of \textbf{5.1\%} with respect to the baseline. In the case of UTI dataset, the most significant improvement is obtained at 40\% of the video observed with a performance enhancement of \textbf{5.0\%} with respect to the baseline. Moreover, the less significant improvement are obtained when the video observation approaches the 100\% with a 0.62\% and 0.71\% of improvement with respect to the baseline on JHMDB and UTI dataset respectively.
%
\begin{figure}[t]
  \begin{subfigure}[b]{0.5\textwidth}
    \includegraphics[width=\textwidth]{imgs/JHMDB_videoobservation.pdf}
  \end{subfigure}
  \hfill
  \begin{subfigure}[b]{0.5\textwidth}
    \includegraphics[width=\textwidth]{imgs/UTI_videoobservation.pdf}
  \end{subfigure}
  \caption{Action anticipation performance with respect to portion of the video observed on JHMDB {\em(left)} and UTI {\em (right)} datasets.}
  \label{fig:ratiovideo}
\end{figure}

% \begin{figure}[t]
% 	\centering
%     \includegraphics[width=0.7\textwidth]{imgs/UCF101-24.pdf}
%   \caption{Action anticipation performance with respect to portion of the video observed on UCF101-24 dataset.}
%   \label{fig:ratiovideo}
% \end{figure}
% \vspace{-0.5cm}

% \begin{figure}[ht!]
% 	\centering
% 	\includegraphics[width=0.9\textwidth]{imgs/example6.pdf}
% 	\caption{Action anticipation performance with respect to the fraction of the video observed on JHMDB and UTI datasets.}
% 	\label{fig:ratiovideo}
% \end{figure}
% %
Another standard practice is to report the action anticipation performance using {\em earliest} and {\em latest} prediction accuracies as done in~\cite{ryoo2011human,aliakbarian2017encouraging}. Although, there is no agreement of what is the proportion of frames used in earliest configuration through different datasets. We make use of the proportion that has been employed by baselines (20\% and 50\% of the video for JHMDB and UTI respectively). Therefore, following~\cite{aliakbarian2017encouraging} we report results in Table \ref{tab:res:jhmdb21}, Table~\ref{tab:res:UTinter} for JHMDB and UTI datasets respectively.
We outperform other methods that rely on additional information, such as optical flow \cite{ma2016learning,soomro2016online,soomro2016predicting} and Fisher vector features based on improved Dense Trajectories \cite{soomro2016online}. Our approach outperforms the state-of-the-art by \textbf{4.0\%} on JHMDB and by \textbf{5.2\%} on UTI datasets in the  earliest configuration. Finally, we report results on UCF101-24 dataset for action anticipation. For this dataset, we use 10\% of the video to predict the action class in the earliest configuration. As we can see in Table \ref{tab:res:ucf101}, We outperform previous method \cite{Singh2017} by \textbf{5.1\%} on the earliest configuration. A more detailed analysis using UCF101-24 dataset is provided on the supplementary material.

\begin{table}[t]
    \centering
    \scalebox{0.9}{
    \begin{minipage}[b]{0.5\textwidth}
        \centering        
        \begin{tabular}{lcc}
%             \multicolumn{3}{c}{\textbf{JHMDB}} 
            \\ \hline
            Method                                           & Earliest & Latest \\ \hline
            DP-SVM \cite{soomro2016online}                 & 5\%      & 46\%      \\
            S-SVM \cite{soomro2016online}                  & 5\%      & 43\%      \\
            Where/What \cite{soomro2016predicting}         & 12\%     & 43\%      \\
            Context-Aware+Loss of \cite{jain2016recurrent} & 28\%     & 43\%      \\
            Ranking Loss \cite{ma2016learning}             & 29\%     & 43\%      \\
            Context-Aware+Loss of \cite{ma2016learning}    & 33\%     & 39\%      \\
            E-LSTM \cite{aliakbarian2017encouraging}       & 55\%     & 58\%      \\ 
            ROAD \cite{Singh2017} & 57\% & 68\% \\ \hline
            Ours                                          & \textbf{61\%}     & 63\%     \\
            \hline
        \end{tabular}
        \caption{Comparison of action anticipation methods on \textbf{JHMDB} dataset. 20\% of video is observed at \emph{Earliest}.}
        %   \vspace{0.2cm}
        \label{tab:res:jhmdb21}
%       \centering
      \begin{tabular}{lll}
      \hline
              & Earliest   & Latest   \\ \hline
        ROAD (RTF) \cite{Singh2017} & 81.7\% & 83.9\% \\
        ROAD (AF)  \cite{Singh2017} & 84.2\% & 85.5\% \\ \hline
        Ours  & 89.3\% & 90.2\% \\ \hline
      \end{tabular}
      \caption{Comparison of action anticipation methods on \textbf{UCF101-24} dataset. 10\% of video is observed at Earliest.}
      \label{tab:res:ucf101}
    \end{minipage}%
    }
    \qquad
    \scalebox{0.9}{
    \begin{minipage}[b]{0.5\textwidth}
        \centering
        \begin{tabular}{lll}
%             \multicolumn{3}{c}{\textbf{UT-Interaction}}
            \\ \hline
            Method                & Earliest & Latest \\ \hline
            S-SVN \cite{soomro2016online} & 11.0\%   & 13.4\% \\
            DP-SVM \cite{soomro2016online} & 13.0\%   & 14.6\% \\
            CuboidBayes \cite{ryoo2011human} & 25.0\%   & 71.7\% \\
            CuboidSVM \cite{ryoo2010overview} & 31.7\%   & 85.0\% \\
            Context-Aware+Loss of \cite{jain2016recurrent}& 45.0\%   & 65.0\% \\
            Context-Aware+Loss of \cite{ma2016learning}& 48.0\%   & 60.0\% \\ 
            BP\_SVM \cite{laviers2009improving} & 65.0\%   & 83.3\% \\
            I-BoW \cite{ryoo2011human} & 65.0\%   & 81.7\% \\
            D-BoW \cite{ryoo2011human} & 70.0\%   & 85.0\% \\
            E-LSTM \cite{aliakbarian2017encouraging} & 84.0\%   & 90.0\% \\             
%             \err{AAC \cite{xu2015activity}} & \textbf{91.7\%} & \textbf{96.7\%} \\ % We obtain better results in early configurations
            \hline
            Ours & 89.2\% & 91.9\% \\ \hline
        \end{tabular}
        \caption{Comparison of action anticipation methods using \textbf{UTI} dataset. 50\% of the video is observed at \emph{Earliest}.}
        \label{tab:res:UTinter}
    \end{minipage}
    }
\end{table}

These experiments evidence the benefits of generating future motion information using our framework for action anticipation.
\subsection{Further exploration}
\label{sec.exp.further}

In Fig.~\ref{tab:woworgbjhmdb} we observe the influence of generating dynamic images recursively for earliest configuration in JHMDB and UTI datasets. 
We generate $K$ number of future dynamic images recursively using the very last true dynamic image. 
As it can be seen in Fig.~\ref{tab:woworgbjhmdb}, as we generate more dynamic images into the future, the prediction performance degrades due to the error propagation.
We report action recognition performance for each generated future dynamic image (\ie for the generated future dynamic image at $K$).
If we do not generate any dynamic image for the future, we obtain an action recognition performance of 55.9\%.
If we include generated dynamic images, we obtain a best of 61.0\% on JHMDB. 
A similar trend can be seen for UTI dataset, where without future dynamic image we obtain 87.4\% and after generation we obtain an action recognition performance of 89.2\%.
The influence of generating more future dynamic images is shown in Fig~\ref{tab:woworgbjhmdb}.

\begin{figure}[t]
\centering
\caption{Impact of generating more future dynamic images recursively on JHMDB \textit{(left)} and UTI \textit{(right)} datasets. K is the number of generated dynamic images based on observed RGB frames. K=0 means no dynamic image is generated.}
\label{tab:woworgbjhmdb}
\includegraphics[width=0.45\textwidth]{imgs/jhmdb21MDIMRGB.pdf}
\includegraphics[width=0.45\textwidth]{imgs/utInterMDIMRGB.pdf}
\end{figure}
% \vspace{-0.5cm}

% \textbf{Visualisation}
% \label{sec.exp.visual}
\begin{figure*}[t]
 \centering
 \caption{Visual comparison between generated dynamic image {\em (bottom)} and ground truth {\em (top)}. $K$ refers to how many iterations we apply in the generation of dynamic image.}
 \label{fig:visual}
 \includegraphics[width=0.9\textwidth]{imgs/framestoFuture.png}
\end{figure*}
Finally, we visually inspect the recursively generated dynamic images for $K$ equal to 1, 4, 7 and 10 in Fig.~\ref{fig:visual}. 
Although, we can use our model to generate quite accurate dynamic images, as we predict into the further, the generated dynamic images might contain some artifacts.
