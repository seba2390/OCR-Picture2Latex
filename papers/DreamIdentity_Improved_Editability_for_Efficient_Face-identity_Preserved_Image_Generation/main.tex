\documentclass{article}


% if you need to pass options to natbib, use, e.g.:
%     \PassOptionsToPackage{numbers, compress}{natbib}
% before loading neurips_2023
\usepackage{natbib}
\setcitestyle{numbers,square}

% ready for submission
% \usepackage{neurips_2023}


% to compile a preprint version, e.g., for submission to arXiv, add add the
% [preprint] option:
\usepackage[preprint]{neurips_2023}


% to compile a camera-ready version, add the [final] option, e.g.:
%     \usepackage[final]{neurips_2023}


% to avoid loading the natbib package, add option nonatbib:
%    \usepackage[nonatbib]{neurips_2023}

\pdfoutput=1
\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage[hidelinks]{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{xcolor}         % colors
\usepackage{floatrow}
\floatsetup[table]{capposition=top}
\floatsetup[figure]{capposition=bottom}
\input{packages}
\input{macros}
\usepackage{color}


\title{\ours: 
Improved Editability for Efficient Face-identity Preserved Image Generation}

\author{
    Zhuowei Chen\textsuperscript{\rm 1,2}\quad 
    Shancheng Fang\textsuperscript{\rm 2}\quad  
    Wei Liu\textsuperscript{\rm 2} \quad Qian He\textsuperscript{\rm 2} \quad
    Mengqi Huang\textsuperscript{\rm 1}\\ 
    \textbf{Yongdong Zhang\textsuperscript{\rm 1}\quad
    Zhendong Mao\textsuperscript{\rm 1}\thanks{Corresponding authors} }\\
    \textsuperscript{\rm 1}~University of Science and Technology of China \quad 
    \textsuperscript{\rm 2}~ByteDance Inc.\quad  \\
    \{chenzw01, huangmq\}@mail.ustc.edu.cn \quad \{zdmao, zyd73\}@ustc.edu.cn \\
    \{fangshancheng.lh, liuwei.jikun, heqian\}@bytedance.com\\
\textcolor{red}{\url{https://dreamidentity.github.io/}}
}

\begin{document}


\maketitle
\input{sections/abs}
\input{sections/intro}
\vspace{-0.3cm}
\input{sections/related}
\input{sections/methods}
\input{sections/exps}
\section{Limitation}
\setParDis
While our method offers an efficient approach to recreate a human image given one face image, there are several limitations should be noticed. (1) Our model is trained on the high-quality realistic face image dataset, so when the input is a poor-quality face or out-of-domain image, such as a partially obstructed image, the edited image quality is often limited. (2) The \editb \  is undermined when we ask the model to generate a novel scene that may not be satiable for the gender.
\setParDef
% \input{sections/conclusion}
\input{sections/conclusion}

% \clearpage
\bibliographystyle{plain}
\bibliography{ref}
\clearpage


\section*{Supplementary}
% \renewcommand\thesection{\Alph{section}}

In this supplementary file, in Section.\ref{Self-Aug}, we will provide the details of constructing the self-augmented dataset. 
% In Section.\ref{multi_token}, we will show a qualitative ablation study on multiple word embeddings.
In Section.\ref{InstructPix2Pix}, we will compare our method with the recently proposed general editing method InstructPix2Pix \cite{brooks2022instructpix2pix}. In Section.\ref{scene}, we will compare our method with InstructPix2Pix and the baseline that doesn't use identity information on the scene switch application.

\input{figs/exps/self_aug}

\input{figs/exps/instruct-p2p}
\input{figs/app/anything_ablation}
\appendix
\renewcommand\thesection{\Alph{section}}

\section{Self-Augmented Dataset}\label{Self-Aug}
\myparagraph {Editing Prompts. } The editing prompt list:
\begin{itemize}
\item  Oil painting style, S* face
\item  Watercolor style, S* face
\item  Pencil art style, S* face
\item  Fauvism painting, S* face
\item  S* as a wizard, looking at the camera
\item  S* as a wizard, looking at the camera
\item  S* wearing a hat, looking at the camera
\item  S* as a chef, looking at the camera
\item  S* as a nurse, looking at the camera
\end{itemize}

\myparagraph {Celebrity List. } The celebrity list is in the additional supplementary file, celebrity\_list.txt 

\myparagraph {Training examples. } 
We show the representative training samples in Figure.\ref{fig:self_aug}. 

% \section{Qualitative Results on Multiple Word Embeddings}\label{multi_token}
% The results is shown in Figure.\ref{fig:multi_token}. It can be observed
% that as the number of embeddings increases to three, the generated images fail to depict the "police"
% concept, and using a single embedding results in lower face-similarity. To achieve better trade-off, we choose the number of embeddings as 2.


\section{Qualitative comparisons with InstructPix2Pix\cite{brooks2022instructpix2pix}} \label{InstructPix2Pix}
The results is demonstrated in Figure.\ref{fig:instruct-p2p}. In general, InstructPix2Pix faces challenges when the editing
prompt requries modification of the original image's layout.


\section{Scene Switch}\label{scene}


As depicted in Figure.\ref{fig:anything_ablation}, InstructPix2Pix\cite{brooks2022instructpix2pix} struggles to keep the original identity information in some editing prompts (\eg, "At the Great Wall"). When we only use gaze information, the output images fail to reflect the reference image identity. After adopting our ID encoder to provide ID information, the generated outputs show better identity similarity.

\end{document}
