\begin{figure}[tb]
  \centering
  \includegraphics[width=\linewidth]{figs/pipeline/pipeline_fig.pdf}
  % \vspace{-0.4cm}
  \caption{Overview of the proposed \ours: (a) The training and inference pipeline. The input face image is first encoded into multi-word embeddings (denoted by $S^*$) by our proposed $M^2$ ID encoder. Then $S^*$ are associated with the text input to generate face-identity preserved image in the text-aligned scene. (b) The architecture of $M^2$ ID encoder, where a ViT-based face identity encoder is adopted as the backbone and the extracted multi-scale features are projected to multi-word embedding. (c) The composition of the training data and its objectives. The training data consists of a public face dataset for reconstruction and a self-augmented dataset for  editability learning. }
  \label{fig:pipeline} 
\end{figure}