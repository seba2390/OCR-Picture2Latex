\section{Related Work}\label{sec:related}
\vspace{-0.2cm}
\subsection{Text-to-image Generation}
Text-to-Image generation aims to generate realistic and semantically consistent images with natural language descriptions. Early works mainly adopted GAN \cite{goodfellow2014generative} as the foundational generative model for this task. Various works have been proposed \cite{zhang2021cross, zhu2019dm, xu2018attngan, zhang2017stackgan, zhang2018photographic, liang2020cpgan, cheng2020rifegan, ruan2021dae, tao2020df, li2019controllable, huang2022dse} with well-designed text representation, elegant text-image interaction, and effective loss function. However, GAN-based models often suffer from training instability and model collapse, making it hard to be trained on large-scale datasets \cite{brock2018large, kang2023scaling, schuhmann2021laion}. Witnessed by the scalability of large language models \cite{radford2019language}, autoregressive methods like DALL-E and Parti \cite{yu2022scaling, ramesh2021zero} where the images are quantized into discrete tokens are scaled to learn more general text-to-image generation.
More recently, Diffusion Models, such as GLIDE \cite{nichol2021glide}, Imagen \cite{saharia2022photorealistic},  DALL-E 2 \cite{ramesh2022hierarchical}, LDM \cite{rombach2022high}  have demonstrated the ability on generating unprecedentedly high-quality and diverse images.  However, it remains infeasible to generate a specified identity within the context described by the text. However, generating a specified face/person identity within the context described by the text using the text-to-image model alone remains infeasible.




\input{figs/pipeline/pipeline}
\subsection{Personalized Image Synthesis for Face Identity Control}
% \vspace{-0.4cm}
Recently personalization methods \cite{gal2022image, ruiz2022dreambooth, kumari2022multi}  have shown promising results on customized concept generation. We can apply these methods to our tasks when the concept is a specified face identity. Textual Inversion \cite{gal2022image} optimizes a new word embedding to represent the given specific concept. \cite{ruiz2022dreambooth} \cite{kumari2022multi} associate the concept with a rare word embedding by fine-tuning part or all parameters in the generator.
However, the requirement for multiple images to specify a concept, coupled with the time-consuming optimization (requiring at least several minutes), limits wider application. Our work present an optimization-free identity encoder that directly encodes a face identity as the word embedding given only one image.

Similar to our goal, there are some concurrent works utilize an embedding encoder for efficient personalized image synthesis. Specifically, ELITE \cite{wei2023elite}, UMM-Diffusion \cite{ma2023unified} and InstantBooth \cite{shi2023instantbooth} encode a common object as a word embedding with the last layer feature from the CLIP encoder. Additionally, ELITE and InstantBooth augment finer details with a local mapping network. Our work differs in several aspects:  1) At the encoder level:  We design a dedicated ID encoder for accurate face encoding with multi-scale features along with multiple word embeddings mapping, whereas the concurrent works use a last layer feature to predict a single word embedding with a common object encoder (CLIP). 2) We propose a self-augmented editability learning method to improve the editability instead of training encoder solely under the reconstruction objective.
