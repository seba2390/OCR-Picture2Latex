\input{tables/main_result}


\section{Experiments}\label{sec:exp}


\subsection{Experiment Settings}
\myparagraph{Dataset.} Our experiments are conducted on the widely used FFHQ dataset \cite{karras2019style}, which contains $70000$ high-resolution human face images. We resize the images to 512x512 for training. The test set consists of 100 faces from \cite{liu2015faceattributes}. We make certain that there is no intersection between the test set and the self-augmented celebrity set to maintain the integrity of the experiment.

\myparagraph{Metrics.} We evaluate our method on Text-alignment and Face-similarity.  Text-alignment is used to indicate whether the generated image reflects editing prompts, which is calculated by the cosine distance in the CLIP text-image embedding space.  Face-similarity is used to measure whether the face ID is preserved. We use the ID feature from arcface \cite{deng2018arcface}, a model pre-trained on face recognition tasks, to represent the face identity. Then ID-similarity is measured by the cosine distance of ID features between the input face and the face cropped from the edited image. For each editing prompt and face identity, four images are generated.


\myparagraph{Implementation Details.} 
We choose Stable Diffusion 2.1-base as our base text-to-image model. The learning rate and batch size are set to $5e-5$ and $64$. The encoder is trained for 60,000 iterations. The embedding regularization weight $\lambda$ is set to $1e-4$. Our experiments are trained on a server with eight A100-80G GPUs, which takes about 1 day to complete each experiment. During inference, we use the DDIM \cite{song2020denoising} sampler with 30 steps. The guidance scale is set to 7.5.

\input{figs/exps/main_result}
\input{tables/id_feat_ablation}

\input{figs/exps/id_encoder}
\subsection{Comparison to SOTA Methods}
In this section, we compare our method with fine-tuning based methods: Textual Inversion \cite{gal2022image}, DreamBooth \cite{ruiz2022dreambooth} and concurrent works on efficient personalized model: E4T \cite{gal2023designing} which requires finetuning for around 15 iterations for each face, and ELITE \cite{wei2023elite}, a fine-tuning free work. We adopt the widely-used open-sourced Diffusers codebase for Textual Inversion, DreamBooth, and re-implemented  E4T and  ELITE. To ensure a fair comparison, all experiments are conducted with a single face image input. 

\myparagraph{Quantitative and Qualitative Results.} As demonstrated in Tab.\ref{tab:main_result}, our work \ours \ outperforms recent methods across all the metrics, demonstrating superior performance in terms of \editb, \Imetric, and encoding speed. 
We show that \ours \ improves the text-alignment by $7\%$ compared to the second-best E4T \cite{gal2023designing}. Meanwhile,  \ours \ surpasses the second-best model \cite{wei2023elite} on \Imetric \  by $3.7\%$, while enjoying better editability. Benefiting from the direct encoding rather than optimization for unique embeddings, the additional computation cost is only 0.04 s, which can be negligible compared to the time cost (seconds-level) for a standard diffusion-based text-to-image process. The conclusion is further validated by the qualitative results in Fig.\ref{fig:main_result}.

\input{tables/multi_token_ablation}
\input{figs/exps/gen}
\input{figs/exps/multi_token}


\subsection{Ablation Studies}
\vspace{-0.2cm}
In this section, we conduct ablation studies to verify the effectiveness of our proposed  $M^2$ ID feature and self-augmented editability learning.
 
\myparagraph{$M^{2}$ ID encoder.} 
We adopt CLIP encoder as our baseline, which is commonly used by concurrent encoder-based methods. Following \cite{wei2023elite, shi2023instantbooth}, we use the last layer CLS feature from CLIP encoder to predict a word embedding. As Fig.\ref{fig:id_encoder} shows, this baseline generally failed to capture the core identity information in the input image, and in some cases, it doesn't even capture the gender information.  
Upon switching from the CLIP encoder to the face-specific ID encoder, the \Imetric \ is improved from $0.266$ to $0.302$, as shown in Tab.\ref{tab:id_feat_ablation}. Integrating the multi-scale features further boosts the \Imetric \ to $0.412$.
Multi-word embeddings are  further utilized to enhance ID-preservation. As shown in Tab.\ref{tab:multi_token_ablation} and Fig.\ref{fig:multi_token}, when we increase the number of embedding  to 2, the Face-similarity is improved by $12\%$ with marginal change of $0.4\%$ on text-alignment. However, when we further increase the number of word embedding,  text-alignment is dropped by $17\%$. We argue that excessive word embeddings may include more information beyond the ID feature such that hinder the editability. Therefore, we choose the embedding number as $2$ to avoid degraded editability.  

\myparagraph{Self-Augmented Editability Learning.} Next, we study the effectiveness of self-augmented editability learning. Fig.\ref{fig:exp_self_aug} indicates that if the model is only trained under the reconstruction objective, the editability \ of embeddings will be limited. To be specific, the model trained without the editability learning objective fails to edit the input identity to a police. Besides, if we only use the limited generated editing dataset, face similarity will be degraded in that there are only around 1000 face IDs in the self-augmented dataset. Combining the reconstruction data (i.e., FFHQ) and generated self-augmented dataset is a better choice to preserve face similarity while following the textual instruction. The quantitative results in Tab.\ref{tab:gen_data_ablation} further confirm our conclusion.




\subsection{Application}
\myparagraph{ID-preserved Scene Switch.} As illustrated in Fig.\ref{fig:anything}, given the input face ID and its location in the canvas indicated by the gaze location, we can generate a series of different scene images which share the same identity information and head location with the help of ControlNet \cite{zhang2023adding}. The scene is specified by the text description and can encompass different accessories, hair style, backgrounds, and styles. With this method, we may achieve the effect of "everything and everywhere all at once". 

\input{figs/app/anything}







