\section{Methods}\label{sec:methods}

Given a single facial image of an individual, our objective is to endow the pre-trained T2I model with the ability to efficiently re-contextualize this unique identity under various textual prompts. These prompts may include variations in clothing, accessories, styles, or backgrounds.


The overall framework is shown in Fig.\ref{fig:pipeline}, given a pre-trained T2I model, 
to achieve fast and identity-preserved image generation, we first directly encode the target identity into the word embedding  space (represented as the pseudo word $S*$) with the proposed $M^2$ ID encoder. Afterward,
$S*$ is integrated with the input template prompt for 
generating the text-guided image. To empower the editability for the target identity, a novel \emph{self-augmented editability learning} is further introduced to train the $M^2$ ID encoder with the editability objective.


In the following parts, we first briefly introduce the pre-trained diffusion-based text-to-image model used in our work, then describe our proposed  $M^2$ ID encoder and self-augmented editability learning in detail, respectively.

\subsection{Preliminary}
In this work, we adopt the open-sourced Stable Diffusion 2.1-base (SD) as our text-to-image model, which has been trained on billions of images and shows amazing image generation quality and prompt understanding. 

SD is a kind of Latent Diffusion Model (LDM) \cite{rombach2022high}. LDM firstly represents the input image $x$ in a lower resolution latent space $z$ via a Variational Auto-Encoder (VAE) \cite{kingma2013auto}. Then a text-conditioned diffusion model is trained to generate the latent code of the target image from text input $c$. The loss function of this diffusion model can be formulated as:
\begin{equation}
    \mathcal{L}_{diffusion} = \mathbb{E}_{\epsilon,z,c,t}[\lVert{\epsilon - \epsilon_{\theta}(z_t,c,t)}\rVert_2^2],
\end{equation}
where $\epsilon_{\theta}$ is the noise predicted by the model with learnable parameters $\theta$, $\epsilon$ is noise sampled from standard normal distribution, $t$ is the time step, and $z_t$ is noisy latent at the time step $t$.

During inference, the image is generated by two stages: latent code is first generated by the diffusion model, then the decoder is employed to map the latent code to image space. 

\subsection{$M^{2}$ ID Encoder}

To accurately represent the input face identity, we propose a novel Multi-word Multi-scale embedding ID encoder ($M^2$ ID encoder) for an accurate mapping, which is achieved by the multi-scale ID features extracted from a dedicated backbone then followed by multiple word embedding projection.




\myparagraph{Backbone.} We argue that an accurate representation of the face identity is crucial, while common image encoder CLIP (which is adopted by \emph{all} existing works) fails for that purpose since it can not capture the identity feature as accurately as the face ID encoder which has been trained for face identification tasks on the large-scale face dataset. As \cite{bhat2023face} shows, the current best CLIP VIT-L/14 model is still much worse than the face recognition model on top-1 face identification accuracy ($80.95\%$ vs $87.61\%$). Therefore, we employ a ViT backbone \cite{dosovitskiy2020image} pre-trained on a large-scale face recognition dataset to faithfully extract ID-aware features for input face.


\myparagraph{Multi-scale Feature.}  However, naively mapping the final layer's output identity vector $v_{final}$ could only bring sub-optimal identity preservation. The reason lies in that $v_{final}$ mainly contains the high-level semantics which is suitable for discriminative tasks (\eg, face identification) rather than generative tasks. For example, the same identity with different expressions should share similar representation under the face recognition training loss, while the generation requests more detailed information like facial expressions. Hence, only mapping the last layer representation could become an information bottleneck for the image generation task. To deal with the above problem, we propose to utilize multi-scale features from the face encoder to represent an identity more faithfully. Specifically, the identity vector is augmented by four CLS embeddings ($v_3$, $v_6$, $v_{12}$, $v_{12}$) from the 3rd, 6th, 9th, and 12th layer, respectively. Formally, the multi-scale feature from the ID encoder is depicted as follows:
\begin{equation}
% \setlength\abovedisplayskip{1pt}
% \setlength\belowdisplayskip{1pt}
V = [v_3, v_6, v_9, v_{12}, v_{final}].
\end{equation}

\myparagraph{Multi-word Embeddings.} The multi-scale feature is further projected into the word embedding domain. To maintain the original large-scale T2I model's generalization and editability, we leave all its parameters and structure unchanged. As a result, it raises the problem that a single word embedding is hard to faithfully represent the face's identity. Therefore, we further propose a multi-word projection mechanism to represent a face with multi-word embedding:
\begin{equation}
\begin{aligned}
s_{i} = MLP_i(V), \text{for } i = 1, ..., k,
\end{aligned}
\end{equation}
where $k$ is the number of embeddings . Experimentally, we set $k=2$ as depicted in Fig.\ref{fig:pipeline}.  Following \cite{gal2023designing}, $l_2$ regularization is further adapted to constrain the output embedding:

\begin{equation}
    \mathcal{L}_{reg} = \sum_{i=1}^k{\lVert{s_{i}}\rVert}.
\end{equation}

Benefiting from the above-dedicated ID feature, we can facilitate highly identity-preservation control in the embedding space only, without sacrificing pre-trained T2I model's editability caused by feature injection. 

\subsection{Self-Augmented Editability  
 Learning}
Current efficient methods are trained under the reconstruction paradigm, which is given an input face image $I$, the objective to learn a unique word $S*$ such that the $S*$ can reconstruct $I$. However, in real-world applications, we wish to generate a set of new images, such as "watercolor style of $S*$ face", "$S*$ as a police". As a result, there exists a huge inconsistency between training and testing. We hope we can rely on the inductive bias in the word embedding space to achieve editability, but in reality, as Fig.\ref{fig:exp_self_aug} shows, the generated image doesn't always follow the text prompt if we only train encoder under the reconstruction objective. 

To deal with the inconsistency between training and testing, in this paper, we propose a novel \emph{self-augmented editability learning} to take the editing task into the training phase. However, collecting such pair data for the editing task is difficult. Experimentally,  we notice that the current state-of-the-art general text-to-image models can generate celebrity (\eg, Boris Johnson, Emma Watson) in different contexts with good identity preservation and text-coherence. With this insight, The \emph{self-augmented editability learning} utilizes the pre-trained model itself to construct a self-augmented dataset by generating various celebrity faces along with the target edited celebrity images, which will be used to train the $M^2$ ID encoder with the editability objective. Formally, the construction of the dataset includes the following four steps:


\myparagraph {Step 1: Celebrity List Generation.} Firstly we collect a candidate celebrity list. The large language model (\ie, ChatGPT) is used to generate the most famous 400 names in four fields (\ie, sports players, singers, actors, and politicians). After filtering duplicate ones, we finally get 1015 celebrity names.

\myparagraph {Step 2: Celebrity Face Generation.} We propose to use generated face images rather than real images because the model has its own understanding of celebrity. Specifically, the celebrities who appeared less frequently in the Stable Diffusion training dataset are not very similar to the real person while these generated faces maintain a high level of identity resemblance. We use the prompt template "<celebrity-name> face, looking at the camera" to produce the source images, then followed by face crop and alignment operation to get face-only images. A face-only image is kept if its short size is larger than 128 pixels. 

\myparagraph {Step 3: Edit Prompts and Edited Images Generation.} We manually design a variety of prompts that contain images of celebrities in different jobs, styles, and accessories (\eg, "<celebrity-name> as a chef", "oil painting style, <celebrity-name> face"). Then these prompts are transformed to images by the T2I model as edited images, and the <celebrity-name> in prompts is replaced by the pseudo word $S^*$ as Editing Prompts.

\myparagraph {Step 4: Data Cleaning.} After the above procedures, we can now get the initial self-augmented dataset consisting of a set of triplets, <identity face, editing prompt, edited images>. Due to the instability of the current diffusion model, the edited images don't always follow the edit instructions. 
Therefore, we need to filter out the noise data in the self-augmented dataset. We employ ID Loss and CLIP score which reflect identity similarity and text-image consistency as the metrics to rank the edited images for every prompt, then the top $25\%$ triplets at kept as the final training set. 

Finally, we construct a high-quality self-augmented dataset from the pre-trained T2I model itself, which is then used for edit-oriented training.


\subsection{Training}
We combine the FFHQ \cite{karras2019style} and the self-augmented dataset to train our proposed $M^2$ ID encoder. The total loss consists of noise prediction loss of diffusion and the embedding regularization loss:  
\begin{equation}
\mathcal{L}_{total} = \mathcal{L}_{diffusion} + \lambda \mathcal{L}_{reg} ,
\end{equation} 
where $\lambda$ is the embedding regularization weight.


