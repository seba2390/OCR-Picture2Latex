\section{Introduction}\label{sec:intro}

Diffusion-based large-scale text-to-image (T2I) models \cite{ramesh2022hierarchical, saharia2022photorealistic, rombach2022high} have revolutionized the field of visual content creation recently. With the help of these T2I models, it is now possible to create vivid and expressive human-centric images easily. An exciting application of these models is that, given a specific person's face in our personal life (our family members, friends, etc.), they potentially can create different scenes associated with this identity using natural language descriptions.

Deviated from the standard T2I task, as shown in Fig.\ref{fig:teaser}, the task \task \ requires the model to have the ability to preserve input face identity (\ie, ID-preservation) while adhering to textual prompts. A feasible solution is to personalize a pre-trained T2I model \cite{gal2022image, ruiz2022dreambooth, kumari2022multi} for each face identity, which involves learning to associate a unique word with the identity by optimizing its word embedding \cite{gal2022image} or tuning the model parameters \cite{ruiz2022dreambooth, kumari2022multi}. However, these optimization-based methods are highly inefficient due to the per-identity optimization. Afterwards, several optimization-free methods \cite{wei2023elite, shi2023instantbooth, ma2023unified} propose to directly map the image features extracted from a pre-trained image encoder (typically, CLIP) into a word embedding, eliminating the cumbersome per-identity optimization but at the cost of degraded ID-preservation. Consequently, these methods necessitate either fine-tuning the parameters of the pre-trained T2I model \cite{gal2023designing} or adjusting original structure for injecting additional grid image features, as a result of bringing the risk of compromising original T2I model's editing capabilities. In a word, all concurrent optimization-free works struggle to preserve identity while remaining the model's editability.



We argue that the above problem of existing optimization-free works arises from  two issues, \ie, (1) the inaccurate identity feature representation and (2) the inconsistency objective between the  training and testing. On one side, the common encoder (\ie, CLIP \cite{radford2021learning}) used by concurrent works \cite{wei2023elite, shi2023instantbooth, ma2023unified, gal2023designing} is unsuitable for identity re-contextualization task as evident by the fact that the current best CLIP model is still much worse than the face recognition model on top-1 face identification accuracy ($80.95\%$ vs $87.61\%$ \cite{bhat2023face}). Additionally, the last layer feature of CLIP struggles to preserve the identity information since it primarily contains high-level semantics, lacking detailed facial descriptions. On another side, all concurrent works solely adopt the vanilla reconstruction objective to learn the word embedding, which hurts the editability for the input face.

In this work, we introduce a novel optimization-free framework (dubbed as \emph{DreamIdentity}) with \emph{accurate identity representation} and \emph{consistent training/inference objective} to deal with the above challenge on identity-preservation and editability. To be specific, for the accurate identity representation,  we design a novel  Multi-word Multi-scale  ID encoder ($M^2$ ID encoder) in the architecture of Vision Transformer \cite{dosovitskiy2020image}, which is pre-trained on a large-scale face dataset and projects multi-scale features into multi-word embeddings.
For the consistent training/inference objective, we propose a novel Self-Augmented Editability Learning to take the editing task into the training phase, which utilizes the T2I model itself to construct a self-augmented dataset by generating  celebrity faces along with various target edited celebrity images. This dataset is then employed to train the $M^2$ ID encoder to enhance the model's editability.


Our contributions in this work are as follows:

\textbf{Conceptually,} we point out current optimization-free methods fail for both ID-preservation and high editability   since their inaccurate representation and inconsistency training/inference objective.

\textbf{Technically,} (1) for accurate representation, we propose $M^2$ ID Encoder, an ID-aware multi-scale feature with multi-embedding projection. (2) For consistent training/inference objective, we introduce self-augmented editability learning to generate a high-quality dataset by the base T2I model itself for editing.

\textbf{Experimentally,} extensive experiments demonstrate the superiority of our methods, which can efficiently achieve identity-preservation while enabling flexible text-guided editing, 
\ie, identity re-contextualization.
\input{figs/teaser/teaser}

