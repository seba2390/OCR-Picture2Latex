
In this section we will describe how the tiered vector data structure
from~\cite{Goodrich1999} works. 

\begin{figure}
	\includegraphics[width=\textwidth]{graphics/DSExample}
    \caption{An illustration of a tiered vector with $l = w = 3$. The elements are letters, and the tiered vector represents the sequence ABCDEFGHIJKLMNOPQRSTUVX. The elements in the leaves are the elements that are actually stored. The number above each node is its offset. The strings above an internal node $v$ with children $c_1, c_2, c_3$ is respectively $A(c_1) \cdot A(c_2) \cdot A(c_3)$ and $A(v)$, i.e.\ the elements $v$ represents before and after the circular shift. ? specifies an empty element.}
\label{fig:ds}
\end{figure}

%\paragraph{Data Structure} 
\subparagraph*{Data Structure} 
An $l$-tiered vector can be seen as a tree $T$ with root $r$, fixed
height $l - 1$ and out-degree $w$ for any $l \geq 2$.
A node $v \in T$ represents a sequence of elements $A(v)$ thus 
$A(r)$ is the sequence represented by the tiered vector. The capacity $\capacity(v)$ of a node $v$ is $w^{\height(v)+1}$. For a node $v$ with children $c_1, c_2, \ldots, c_w$, $A(v)$ is a circular shift of the
concatenation of the elements represented by its children, 
$A(c_1) \cdot A(c_2) \cdot \ldots \cdot A(c_w)$.
The circular shift is determined by an integer $\offset(v)
\in [\capacity(v)]$ that is explicitly stored for all nodes. Thus the sequence of
elements $A(v)$ of an internal node $v$ can be reconstructed by recursively
reconstructing the sequence for each of its children, concatenating these and
then circular shifting the sequence by $\offset(v)$. See Figure~\ref{fig:ds} for an illustration. A leaf $v$ of $T$
explicitly stores the sequence $A(v)$ in a circular array $\elements(v)$ with
size $w$ whereas internal nodes only store their respective offset.
 Call a node $v$ full if $|A(v)| = \capacity(v)$ and empty if $|A(v)| = 0$. In order to support fast $\aaccess$, for all nodes $v$ the elements of $A(v)$ are located in consecutive children of $v$ that
are all full, except the children containing the first
and last element of $A(v)$ which may be only partly full.

%\paragraph{Access \& Update}
\subparagraph*{Access \& Update}
To access an element $A(r)[i]$ at a given index $i$; one traverses a
path from the root down to a leaf in the tree. In each node the offset of the
node is added to the index to compensate for the cyclic shift, and the traversing is continued in the child corresponding to the newly calculated index. 
Finally when reaching a leaf, the desired element is
returned from the elements array of that leaf. The operation $\aaccess(v, i)$ returns the
element $A(v)[i]$ and is recursively computed as follows:

\begin{description} 
    \item[\quad v is internal:] Compute $i' = (i + \offset(v))
        \mod \capacity(v)$, let $v'$ be the $\lfloor i' / w \rfloor^{th}$ child of $v$ and return the element
    $\aaccess(v', i' \mod \capacity(v'))$. 
    
\item[\quad v is leaf:] Compute $i' = (i + \offset(v)) \mod w$
    and return the element $\elements(v)[i']$.
    \end{description}

The time complexity is $\Theta(l)$ as we visit all nodes on a root-to-leaf path in $T$. To navigate this path we must follow $l - 1$ child pointers, lookup $l$ offsets, and access the element itself. Therefore this requires $l - 1 + l + 1 = 2l$ memory probes.

The update operation is entirely similar to access, except
the element found is not returned but substituted with the new element. The
running time is therefore $\Theta(l)$ as well. For future use, let $\aupdate(v, i, e)$ be the operation that sets $A(v)[i] = e$ and returns the element that was
substituted. 

%\paragraph{Range Access}
\subparagraph*{Range Access}

Accessing a range of elements, can obviously be done by using the
$\aaccess$-operation multiple times, but this results in redundant traversing
of the tree, since consecutive elements of a leaf often
-- but not always due to circular shifts -- corresponds to consecutive elements of $A(r)$.
Let $\aaccess(v, i, m)$ report the
elements $A(v)[i \ldots i + m - 1]$ in order. The operation can recursively
be defined as:

\begin{description} \item[\quad v is internal:] 
    Let $i_l = (i + \offset(v)) \mod \capacity(v)$,
    and let $i_r = (i_l + m) \mod \capacity(v)$. The children of
    $v$ that contains the elements to be reported are in the range $[\lfloor i_l \cdot w / \capacity(v) \rfloor, \lfloor i_r \cdot w / \capacity(v) \rfloor] \mod w$,
    call these $c_l, c_{l+1},
    \ldots, c_r$. In order, call $\aaccess(c_l, i_l, \min(m, \capacity(c_l) -
    i_l))$, $\aaccess(c_i, 0, \capacity(c_i))$ for $c_i = c_{l+1}, \ldots,
    c_{r-1}$, and $\aaccess(c_r, e_{r-1}, 0, i_r \mod \capacity(c_r))$.
	
        \item[\quad v is leaf:] Report the elements $\elements(v)[i, i+m-1] \mod w$. \end{description}

The running time of this strategy is $O(lm)$, but saves a constant factor over the naive solution.


%\paragraph{Insert \& Delete}
\subparagraph*{Insert \& Delete}

Inserting an element in the end (or beginning) of the array can simply be
achieved using the $\aupdate$-operation. Thus the interesting part is fast insertion at an arbitrary position;
this is where we utilize the offsets.

Consider a node $v$, the key challenge is to shift a big chunk of elements $A(v)[i, i+m-1]$ one index right (or left) to $A(v)[i+1, i+m]$ to make room for a new element (without actually moving each element in the range). Look at the range of children $c_l, c_{l+1}, \ldots, c_r$ that covers the range of elements $A(v)[i, i+m-1]$ to be shifted. All elements in $c_{l+1}, \ldots, c_{r-1}$ must be shifted. These children are guaranteed to be full, so make a circular shift by decrementing each of their offsets by one. Afterwards take the element $A(c_{i-1})[0]$ and move it to $A(c_{i})[0]$ using the $\aupdate$ operation for $l < i \leq r$. In $c_l$ and $c_r$ only a subrange of the elements might need shifting, which we do recursively. In the base case of this recursion, namely when
$v$ is a leaf, shift the elements by actually moving the elements one-by-one in $\elements(v)$.

Formally we define the $\ashift(v, e, i, m)$ operation that (logically) shifts
all elements $A(v)[i, i+m-1]$ one place right to $A[i+1, i+m]$, sets $A[i] = e$ and returns the value that was previously on position $A[i+m]$ as:

\begin{description} \item[\quad v is internal:] Let $i_l = (i + \offset(v)) \mod
    \capacity(v)$, and let $i_r = (i_l + m) \mod \capacity(v)$. The children of
    $v$ that must be updated are in the range $[\lfloor i_l \cdot w / \capacity(v) \rfloor, \lfloor i_r \cdot w / \capacity(v) \rfloor] \mod w$ call these $c_l, c_{l+1}, \ldots, c_r$.
Let $e_l = \ashift(c_l, e, i_l, \min(m, \capacity(c_l) - i_l))$. Let $e_i =
\aupdate(c_i, size(c) - 1, e_{i-1})$ and set $\offset(c_i) = (\offset(c_i) - 1)
\mod \capacity(c)$ for $c_i = c_{l+1}, \ldots, c_{r-1}$. Finally call
$\ashift(c_r, e_{r-1}, 0, i_r \mod \capacity(c_r))$.
	
        \item[\quad v is leaf:] Let $e_o = \elements(v)[(i+m) \mod w]$. Move the
            elements $\elements(v)[i, (i+m-1) \mod w]$ to $\elements(v)[i+1,(i+m) \mod w]$, and set $\elements(v)[i] = e$. Return $e_o$.
    \end{description}

An insertion $\ainsert(i, e)$ can then be performed as $\ashift(root, e, i,
size(root) - i - 1)$. The running time of an insertion is $T(l) = 2T(l - 1) + w\cdot l \Rightarrow T(l) = O(2^l w)$.

%TODO: {Add illustration.}


A deletion of an element can basically be done as an inverted insertion, thus
deletion can be implemented using the $\ashift$-operation from before. A
$\adelete(i)$ can be performed as $\ashift(r, \bot, 0, i)$ followed by an
update of the root's offset to $(\offset(r) + 1) \mod \capacity(r)$.

%\paragraph{Space}
\subparagraph*{Space}

There are at most $O(w^{l-1})$ nodes in the tree and each takes up constant
space, thus the total space of the tree is $O(w^{l-1})$.
All leaves are either empty or full except the two leaves storing the first and
last element of the sequence which might contain less than $w$ elements.
Because the arrays of empty leaves are not allocated the space overhead of the arrays is $O(w)$.
Thus beyond the space required to store the $n$ elements themselves, tiered vectors
have a space overhead of $O(w^{l-1})$.

To obtain the desired bounds $w$ is maintained such that $w = \Theta(n^\epsilon)$ where $\epsilon = 1/l$ and $n$ is the number of elements in the tiered vector. This can be achieved by using global rebuilding to gradually increase/decrease the value of $w$ when elements are inserted/deleted without asymptotically changing the running times. We will not provide the details here. We sum up the original tiered vector data structure in the following theorem:

\begin{theorem}[\cite{Goodrich1999}] The original $l$-tiered vector solves the
    dynamic array problem for $l \geq 2$ using $\Theta(n^{1-1/l})$ extra space
    while supporting $\aaccess$ and $\aupdate$ in $\Theta(l)$ time and $2l$
    memory probes. The operations $\ainsert$ and $\adelete$ take $O(2^l n^{1/l})$ time.
    \label{thm:pointer}
\end{theorem}
