\section{Experiments}

\label{sec:experiments}
In this section we compare the tiered vector to some widely used C++ standard library containers. 
We also compare different variants of the tiered vector. 
We consider how the different representations of the data
structure listed in Section~\ref{sec:implementation}, 
and also how the height of tree and the capacity of the leaves affects the running time.
The following describes the test setup:

\subparagraph{Environment}

All experiments have been performed on a Intel Core i7-4770 CPU @ 3.40GHz with
32 GB RAM. The code has been compiled with GNU GCC version 5.4.0 with flags
``-O3''. The reported times are an average over 10 test runs.
 
 \subparagraph{Procedure}
%have been added to the data structure in
 
In all tests $10^8$ 32-bit integers 
are inserted in the data structure as a preliminary step
to simulate that it has already been
used\footnote{In order to minimize the overall running time of the experiments,
the elements were not added randomly, but we show this does not give our data
structure any benefits}.
For all the access and successor operations $10^9$ elements have been accessed
and the time reported is the average time per element.
For range access, 10.000 consecutive elements are accessed.
For insertion/deletion $10^6$ elements
have been (semi-)randomly\footnote{In order to not impact timing, a simple
access pattern has been used instead of a normal pseudo-random generator.}
added/deleted, though in the case of ``vector'' only 10.000 elements were
inserted/deleted to make the experiments terminate in reasonable time. 

\subsection{Comparison to C++ STL Data Structures}

In the following we have compared our best performing tiered vector (see the next sections) to the vector and
the multiset class from the C++ standard library.
The vector data structure directly supports the
operations of a dynamic array. The multiset class is implemented as a red-black
tree and is therefore interesting to compare with our data structure.
Unfortunately, multiset does not directly support the operations of a dynamic
array (in particular it has no notion of positions of elements). To simulate an
access operation we instead find the successor of an element in the multiset.
This requires a root-to-leaf traversal of the red-black tree, just as an access
operation in a dynamic array implemented as a red-black tree would. Insertion
is simulated as an insertion into the multiset, which again requires the same
computations as a dynamic array implemented as a red-black tree would.

Besides the random access, range access and insertion,
we have also tested the operations \textit{data dependent access},
insertion in the end, deletion, and \textit{successor} queries. In the
\textit{data dependent access} tests, the next index to lookup depends on the values of the prior
lookups. This ensures that the CPU cannot successfully pipeline
consecutive lookups, but must perform them in sequence. We test insertion in the end, since
this is a very common use case. Deletion is performed by deleting elements at
random positions. The $successor$ queries returns the successor of an element
and is not actually part of the
dynamic array problem, but is included since it is a commonly used operation on
a multiset in C++. It is simply implemented as a binary search over the elements in
both the vector and tiered vector tests where the elements are now inserted in sorted order. 

The results are summarized in Table~\ref{tab:test_comp} which shows that the vector performs slightly better than the tiered vector on all access and successor tests. As expected from the $\Theta(n)$ running time, it performs extremely poor on random insertion and deletion. For insertion in the end of the sequence, vector is also slightly faster than the tiered vector. The interesting part is that even though the tiered vector requires several extra memory lookups and computations, we have managed to get the running time down to less than the double of the vector for access, even less for data dependent access and only a few percent slowdown for range access. As discussed earlier,
this is most likely because the entire tree structure (without the elements)
fits within the CPU cache, and because the computations required has been minimized.

Comparing our tiered vector to multiset, we would expect access operations to be
faster since they run in $O(1)$ time compared to $O(\log n)$. On the other
hand, we would expect insertion/deletion to be significantly slower since it
runs in $O(n^{1/l})$ time compared to $O(\log n)$ (where $l = 4$ in these tests). We
see our expectations hold for the access operations where the tiered vector is faster by more than an order of magnitude.
In random insertions however,  the tiered vector is only $8\%$ slower -- even when operating on 100.000.000 elements. Both the tiered
vector and set requires $O(\log n)$ time for the successor operation. In our
experiments the tiered vector is 3 times faster for the successor operation.

Finally, we see that the memory usage of vector and tiered vector is almost identical.
This is expected since in both cases the space usage is dominated by the space taken by the actual elements.
The multiset uses more than 10 times as much space, so this is also a considerable drawback of the red-black tree behind this structure. 

To sum up, the tiered vectors performs better than multiset on all tests
but insertion, where it performs only slightly worse.

%\caption{Figures (a) through (e) show the performance of \textit{Tiered Arrays} (\protect\purple) compared
%to the \textit{set} (\protect\green) and \textit{vector} (\protect\blue) data structures from the C++ standard library.} \label{fig:animals}
\begin{table}
	\centering
	\begin{tabular}{|l|r|r|r|r|r|}
		\hline
		& \multicolumn{1}{l|}{\textit{tiered vector}} & \multicolumn{1}{l|}{\textit{set}} & \multicolumn{1}{l|}{\textit{set / tiered}} & \multicolumn{1}{l|}{\textit{vector}} & \multicolumn{1}{l|}{\textit{vector / tiered}} \\ \hline
		access     & $34.07$ ns                                  & $1432.05$ ns                      & 42.03                                      & $21.63$ ns                           & 0.63                                          \\ \hline
		dd-access    & $99.09$ ns                                  & $1436.67$ ns                      & 14.50                                      & $79.37$ ns                           & 0.80                                          \\ \hline
		range access   & $0.24$ ns                                   & $13.02$ ns                        & 53.53                                      & $0.23$ ns                            & 0.93                                          \\ \hline
		insert   & $1.79$ $\mu$s                               & $1.65$ $\mu$s                     & 0.92                                       & $21675.49$ $\mu$s                     & 12082.33                                      \\ \hline
		insertion in end     & $7.28$ ns                               & $242.90$ ns                     & 33.38                                       & $2.93$ ns                     & 0.40                                      \\ \hline
		successor & $0.55$ $\mu$s                               & $1.53$ $\mu$s                     & 2.75                                       & $0.36$ $\mu$s                        & 0.65                                          \\ \hline
		delete     & $1.92$ $\mu$s                               & $1.78$ $\mu$s                     & 0.93                                       & $21295.25$ $\mu$s                     & 11070.04                                      \\ \hline
		memory     & $408$ MB                               & $4802$ MB                     & 11.77                                       & $405$ MB                    & 0.99                                      \\ \hline
	\end{tabular}
	\caption{The table summarizes the performance of the implicit tiered vector
		compared to the performance of multiset and vector from the C++ standard library.\
		dd-access refers to data dependent access.}
\label{tab:test_comp}
\end{table}


\definecolor{cpurple}{RGB}{131,24,197}
\definecolor{cgreen}{RGB}{70,156,118}
\definecolor{cblue}{RGB}{11,178,228}
\definecolor{cdblue}{RGB}{11,112,173}
\definecolor{corange}{RGB}{219,162,55}
\definecolor{cyellow}{RGB}{238,228,98}
\definecolor{cred}{RGB}{110,55,38}
\newcommand{\purple}{\raisebox{2pt}{\tikz{\draw[cpurple,solid,line width=1.9pt](0,0) -- (3mm,0);}}}
\newcommand{\green}{\raisebox{2pt}{\tikz{\draw[cgreen,solid,line width=1.9pt](0,0) -- (3mm,0);}}}
\newcommand{\blue}{\raisebox{2pt}{\tikz{\draw[cblue,solid,line width=1.9pt](0,0) -- (3mm,0);}}}
\newcommand{\dblue}{\raisebox{2pt}{\tikz{\draw[cdblue,solid,line width=1.9pt](0,0) -- (3mm,0);}}}
\newcommand{\orange}{\raisebox{2pt}{\tikz{\draw[corange,solid,line width=1.9pt](0,0) -- (3mm,0);}}}
\newcommand{\yellow}{\raisebox{2pt}{\tikz{\draw[cyellow,solid,line width=1.9pt](0,0) -- (3mm,0);}}}
\newcommand{\red}{\raisebox{2pt}{\tikz{\draw[cred,solid,line width=1.9pt](0,0) -- (3mm,0);}}}


\begin{figure}[ht]
	\centering
	\begin{subfigure}[b]{0.3\textwidth}
		\includegraphics[width=\textwidth]{layout_test_get}
		\caption{\textit{access}}
	\end{subfigure}
	\begin{subfigure}[b]{0.3\textwidth}
		\includegraphics[width=\textwidth]{layout_test_random}
		\caption{\textit{insert}}
	\end{subfigure}
        \caption{Figures (a) and (b) show the performance of the
            \textit{original} (\protect\purple), \textit{optimized original}
            (\protect\green), \textit{lazy} (\protect\blue) \textit{packed
            lazy} (\protect\orange),
            \textit{implicit} (\protect\yellow)
            and \textit{packed implicit} (\protect\dblue) layouts.}
\label{fig:test_representation}
\end{figure}
\subsection{Tiered Vector Variants}

In this test we compare the performance
of the implementations listed in Section~\ref{sec:implementation} to that 
or the original data structure as described in~\ref{thm:pointer}.

%\paragraph{Optimized Original}
\subparagraph*{Optimized Original}
By co-locating the child offset and child pointer, the two memory lookups are at
adjacent memory locations. Due to the cache lines in modern processors,
the second memory lookup will then often be answered directly by the fast
L1-cache.
As can be seen on Figure~\ref{fig:test_representation}, this small change in the memory layout results in a significant improvement in performance for both access and insertion. In the latter case, the running time is more than halved.

%\paragraph{Lazy and Packed Lazy}
\subparagraph*{Lazy and Packed Lazy}

Figure~\ref{fig:test_representation} shows
how the fewer memory probes required by the
\textit{lazy} implementation in comparison to the \text{original}
and \text{optimized original} results in better performance.
Packing the offset and pointer in the leaves results in even better performance
for both access and insertion even though it requires a few extra instructions
to do the actual packing and unpacking.

%\paragraph{Implicit}
\subparagraph*{Implicit}
From Figure~\ref{fig:test_representation}, we see the implicit
data structure is the fastest.
This is as expected because it requires fewer
memory accesses than the other structures except
for the packed lazy which instead has a slight
computational overhead due to the packing and unpacking.

As shown in Theorem~\ref{thm:implicit} the implicit data structure has a
bigger memory overhead than the lazy data structure.
Therefore the packed lazy representation might be beneficial in some
settings.

%\paragraph{Packed Implicit}
\subparagraph*{Packed Implicit}

Packing the offsets array could lead to 
better cache performance due to the smaller memory footprint and therefore
yield better overall performance.
As can be seen on Figure~\ref{fig:test_representation},
the smaller memory footprint
did not improve the performance in practice.
The simple reason for this,
is that the strategy we used for packing the offsets required
extra computation. This clearly dominated the possible gain from the
hypothesized better cache performance. We tried a few strategies to minimize
the extra computations needed at the expense of slightly worse memory usage,
but none of these led to better results than when not packing the offsets at
all.

\subsection{Width Experiments}

\begin{figure}
	\centering
	\begin{subfigure}[b]{0.3\textwidth}
		\includegraphics[width=\textwidth]{width_test_get}
		\caption{\textit{access}}
	\end{subfigure}
	\begin{subfigure}[b]{0.3\textwidth}
		\includegraphics[width=\textwidth]{width_test_sum}
		\caption{\textit{range access}}
	\end{subfigure}
	\begin{subfigure}[b]{0.3\textwidth}
		\includegraphics[width=\textwidth]{width_test_random}
		\caption{\textit{insert}}
	\end{subfigure}
	\caption{Figures (a), (b) and (c) show the performance of the \textit{implicit} (\protect\purple) and
		the \textit{optimized original} tiered vector (\protect\green) for different tree widths.}
\label{fig:test_width}
\end{figure}

This experiment was performed to determine the best capacity ratio between the leaf nodes and the internal nodes.
The six different width configurations we have tested are: 32-32-32-4096, 32-32-64-2048, 32-64-64-1024, 64-64-64-512, 64-64-128-256, and 64-128-128-128.
All configurations have a constant height 4 and a capacity of approximately 130 mio.

We expect the performance of access operations to remain unchanged, since the
amount of work required only depends on the height of the tree,
and not the widths. We expect range access to perform better when the leaf size
is increased, since more elements will be located in consecutive memory
locations. For $insertion$ there is not a clearly expected behavior as the time
used to physically move elements in a leaf will increase with leaf size, but
then less operations on the internal nodes of the tree has to be performed.

On Figure~\ref{fig:test_width} we see access times are actually decreasing
slightly when leaves get bigger. This was not expected, but is most likely
due to small changes in the memory layout that results in slightly better cache
performance. The same is the case for range access, but this was expected. For
insertion, we see there is a tipping point. For our particular instance, the
best performance is achieved when the leaves have size 512.

%Based on this, we have performed the remaining tests with the 64-64-64-512 configuration (unless otherwise specified).

\subsection{Height Experiments}

\begin{figure}
	\centering
	\begin{subfigure}[b]{0.3\textwidth}
		\includegraphics[width=\textwidth]{height_get}
		\caption{\textit{access(i)}}
	\end{subfigure}
	\begin{subfigure}[b]{0.3\textwidth}
		\includegraphics[width=\textwidth]{height_sum}
		\caption{\textit{access(i, m)}}
	\end{subfigure}
	\begin{subfigure}[b]{0.3\textwidth}
		\includegraphics[width=\textwidth]{height_random}
		\caption{\textit{insert}}
	\end{subfigure}
	\caption{Figures (a),(b) and (c) show the performance of the \textit{implicit} (\protect\purple) and
		the \textit{optimized original} tiered vector (\protect\green) for different tree heights.}
\label{fig:test_height}
\end{figure}

In these tests we have studied how different heights affect the performance of
access and insertion operations. We have tested the configurations 8196-16384,
512-512-512, 64-64-64-512, 16-16-32-32-512, 8-8-16-16-16-512. All resulting in
the same capacity, but with heights in the range 2-6.

We expect the access operations to perform better for lower trees, since
the number of operations that must be performed is linear in the height. On the
other hand we expect insertion to perform significantly better with higher
trees, since its running time is $O(n^{1/l})$ where $l$ is the height plus one. 

On Figure~\ref{fig:test_height} we see the results follow our expectations. However, the access operations only perform slightly worse on higher trees.
This is most likely because all internal nodes fit within the L3-cache. Therefore the running time is dominated by the lookup of the element itself.
(It is highly unlikely that the element requested by an access 
to a random position would be among the small fraction of elements that
fit in the L3-cache).

Regarding insertion, we see significant improvements up until a height of 4. After that, increasing the height does not change the running time noticeably. This is most likely due to the hidden constant in $O(n^{1/l})$ increasing rapidly with the height.



\subsection{Configuration Experiments}

\begin{figure}
    \centering
    \begin{subfigure}[b]{0.3\textwidth}
        \includegraphics[width=\textwidth]{small_get}
        \caption{\textit{access}}
    \end{subfigure}
    \begin{subfigure}[b]{0.3\textwidth}
        \includegraphics[width=\textwidth]{small_sum}
        \caption{\textit{range access}}
    \end{subfigure}
    \begin{subfigure}[b]{0.3\textwidth}
        \includegraphics[width=\textwidth]{small_random}
        \caption{\textit{insert(i,x)}}
    \end{subfigure}
    \caption{Figures (a) and (b) show the performance of the
    \textit{base} (\protect\purple),
    \textit{rotated} (\protect\green), 
    \textit{non-aligned sizes} (\protect\blue),
    \textit{non-templated} (\protect\orange)
    layouts.}
\label{fig:test_minor}
\end{figure}

In these experiments, we test a few hypotheses about how different changes
impact the running time. The results are shown on
Figure~\ref{fig:test_minor}, the leftmost result (base) is
the implicit 64-64-64-512 configuration of the tiered vector 
to which we compare our hypotheses.
%our final and best

\textit{Rotated}: 
As already mentioned, the insertions performed as a
preliminary step to the tests are not done at random positions.
This means that all offsets are zero when our real operations
start. The purpose of this test is the ensure that
there are no significant performance gains in starting
from such a configuration which could otherwise
lead to misleading results.
To this end, we have randomized all
offsets (in a way such that the data structure is still valid, but the
order of elements change) after doing the preliminary insertions
but before timing the operations. As can be seen on
Figure~\ref{fig:test_minor}, the difference between this and the normal
procedure is insignificant, thus we find our approach gives a fair picture.


\textit{Non-Aligned Sizes}: In all our previous tests, we have ensured all
nodes had an out-degree that was a power of 2. This was chosen in order to let the
compiler simplify some calculations, i.e.\ replacing multiplication/division
instructions by shift/and instructions. As Figure~\ref{fig:test_minor} shows,
using sizes that are not powers of 2 results in significantly worse performance.
Besides showing that powers of 2 should always be used, this also indicates that not only
the number of memory accesses during an operation is critical for our
performance, but also the amount of computation we make.

\textit{Non-Templated}
The non-templated results 
in Figure~\ref{fig:test_representation} the
show that the change to templated recursion
has had a major impact on the running time. It should be noted that some
improvements have not been implemented in the non-templated version,
but it gives a good indication that this has been quite useful.
