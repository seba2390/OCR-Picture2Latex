\documentclass[10pt,twocolumn,article]{IEEEtran}
\usepackage{times}
\usepackage{amsbsy}
\usepackage{latexsym}
\usepackage{amssymb,bm}
\usepackage{mathtools}
\usepackage{amsmath,amssymb,amsfonts,epsfig,graphicx}
%\usepackage{times,amsbsy,latexsym,bm,xcolor,xparse}
%\usepackage[normalem]{ulem}
\usepackage[ansinew]{inputenc}
%\usepackage{tabularx}
%\usepackage{comment}
%\allowdisplaybreaks
%\usepackage[dvips ]{graphicx}
\DeclareMathOperator{\Tr}{Tr}
%\pagenumbering{gobble}% sem numeros paginas

\title{\huge Multi-Step Knowledge-Aided Iterative ESPRIT \\ for Direction Finding  \\
\author{Silvio F. B. Pinto $^1$ and Rodrigo C. de Lamare $^{1,2}$ \\
Center for Telecommunications Studies (CETUC) \\ $^1$ Pontifical
Catholic
University of Rio de Janeiro, RJ, Brazil.\\
$^2$ Department of Electronics, University of York, UK \\
Emails: silviof@cetuc.puc-rio.br, delamare@cetuc.puc-rio.br}}
\linespread{0.95}
\begin{document}
\maketitle
\begin{abstract}
In this work, we propose a subspace-based algorithm for DOA
estimation which iteratively reduces the disturbance factors of the
estimated data covariance matrix and incorporates prior knowledge
which is gradually obtained on line. An analysis of the MSE of the
reshaped data covariance matrix is carried out along with
comparisons between computational complexities of the proposed and
existing algorithms. Simulations focusing on closely-spaced sources,
where they are uncorrelated and correlated, illustrate the
improvements achieved.
\end{abstract}

%\begin{keywords}
%Large sensor arrays, knowledge-aided techniques, direction finding,
%high-resolution parameter estimation.
%\end{keywords}



\section{Introduction}
\label{introduction}

In array signal processing, direction-of-arrival (DOA) estimation is
a key task in a broad range of important applications including
radar and sonar systems, wireless communications and seismology
\cite{Vantrees}. Traditional high-resolution methods for DOA
estimation such as the multiple signal classification (MUSIC) method
\cite{schimdt}, the root-MUSIC algorithm \cite{Barabell}, the
estimation of signal parameters via rotational invariance techniques
(ESPRIT) \cite{Roy} and subspace techniques
\cite{scharf,bar-ness,pados99,reed98,hua,goldstein,santos,qian,delamarespl07,xutsa,delamaretsp,kwak,xu&liu,delamareccm,wcccm,delamareelb,jidf,delamarecl,delamaresp,delamaretvt,jioel,delamarespl07,delamare_ccmmswf,jidf_echo,delamaretvt10,delamaretvt2011ST,delamare10,fa10,lei09,ccmavf,lei10,jio_ccm,ccmavf,stap_jio,zhaocheng,zhaocheng2,arh_eusipco,arh_taes,dfjio,rdrab,dcg_conf,dcg,dce,drr_conf,dta_conf1,dta_conf2,dta_ls,song,wljio,barc,jiomber,saalt},\cite{Steinwandt,Wang,Qiu}
exploit the eigenstructure of the input data matrix. These
techniques may fail for reduced data sets or low signal-to-noise
ratio (SNR) levels where the expected estimation error is not
asymptotic to the Cramér-Rao bound (CRB) \cite{Thomas}. The accuracy
of the estimates of the covariance matrix is of fundamental
importance in parameter estimation. Low levels of SNR or short data
records can result in significant divergences between the true and
the sample data covariance matrices. In practice, only a modest
number of data snapshots is available and when the number of
snapshots is similar to the number of sensor array elements, the
estimated and the true subspaces can differ significantly. Several
approaches have been developed with the aim of enhancing the
computation of the covariance matrix \cite{Carlson}-\cite{Qian} and
for dealing with large sensor-array systems large
\cite{mmimo,wence,Costa,delamare_ieeproc,TDS_clarke,TDS_2,switch_int,switch_mc,smce,TongW,jpais_iet,TARMO,badstbc,baplnc,keke1,kekecl,keke2,wlbd,Tomlinson,dopeg_cl,peg_bf_iswcs,gqcpeg,peg_bf_cl,Harashima,mbthpc,zuthp,rmbthp,Hochwald,BDVP},\cite{delamare_mber,rontogiannis,delamare_itic,stspadf,choi,stbcccm,FL11,jio_mimo,peng_twc,spa,spa2,jio_mimo,P.Li,jingjing,memd,did,bfidd,mbdf,bfidd,mserrr,shaowcl08}.

Diagonal loading \cite{Carlson} and shrinkage
\cite{Chen,ruan1,ruan2} techniques can enhance the estimate of the
data covariance matrix by weighing and individually increasing its
diagonal by a real constant. Nevertheless, the eigenvectors remain
the same, which leads to unaltered estimates of the signal and noise
projection matrices obtained from the enhanced covariance matrix.
Additionally, an improvement of the estimates of the covariance
matrix can be achieved by employing forward/backward averaging and
spatial smoothing approaches \cite{Pillai,Evans}. The former leads
to twice the number of the original samples and its corresponding
enhancement. The latter extracts the array covariance matrix as the
average of all covariance matrices from its sub-arrays, resulting in
a greater number of samples. Both techniques are employed in signal
decorrelation. An approach to improve MUSIC  dealing with the
condition in which the number of snapshots and the sensor elements
approach infinity was presented in \cite{Mestre}. Nevertheless, this
technique is not that effective for reduced number of snapshots.
Other approaches to deal with reduced data sets or low SNR levels
\cite{Gershman,Qian} consist of reiterating the procedure of adding
pseudo-noise to the observations which results in new estimates of
the covariance matrix. Then, the set of solutions is computed from
previously stored DOA estimates. In \cite{Vorobyov2}, two aspects
resulting from the computation of DOAs for reduced data sets or low
SNR levels have been studied using the root-MUSIC technique. The
first aspect dealt with the probability of estimated signal roots
taking a smaller magnitude than the estimated noise roots, which is
an anomaly that leads to wrong choices of the closest roots to the
unit circle. To mitigate this problem, different groups of roots are
considered as potential solutions for the signal sources and the
most likely one is selected \cite{Stoica}. The second aspect
previously mentioned, shown in \cite{Johnson}, refers to the fact
that a reduced part of the true signal eigenvectors exists in the
sample noise subspace (and vice-versa). Such coexistence has been
expressed by a Frobenius norm of the related irregularity matrix and
introduced its mathematical foundation. An iterative technique to
enhance the efficacy of root-MUSIC by reducing this anomaly making
use of the gradual reshaping of the sample data covariance matrix
has been reported. Inspired by the work in \cite{Vorobyov2}, we have
developed an ESPRIT-based method known as Two-Step KAI-ESPRIT
(TS-ESPRIT) \cite{Pinto}, which combines that modifications of the
sample data covariance matrix with the use of prior knowledge
\cite{Guerci1}-\cite{Guerci2} about the covariance matrix of a set
of impinging signals to enhance the estimation accuracy in the
finite sample size region. In practice, this prior knowledge could
be from the signals coming from known base stations or from static
users in a system. TS-ESPRIT determines the value of a correction
factor that reduces the undesirable terms in the estimation of the
signal and noise subspaces in an iterative process, resulting in
better estimates.

In this work \cite{Pinto2,Pinto3}, we present the Multi-Step KAI
ESPRIT (MS-KAI-ESPRIT) approach that refines the covariance matrix
of the input data via multiple steps of reduction of its undesirable
terms. This work presents the MS-KAI-ESPRIT in further detail, an
analysis of the mean squared error (MSE) of the data covariance
matrix free of undesired terms (side effects), a more accurate study
of the computational complexity and a comprehensive study of
MS-KAI-ESPRIT and other competing techniques for scenarios with both
uncorrelated and correlated signals. Unlike TS-ESPRIT, which makes
use of only one iteration and available known DOAs, MS-KAI-ESPRIT
employs multiple iterations and obtains prior knowledge on line. At
each iteration of MS-KAI-ESPRIT, the initial Vandermonde matrix is
updated by replacing an increasing number of steering vectors of
initial estimates with their corresponding refined versions. In
other words, at each iteration, the knowledge obtained on line is
updated, allowing the direction finding algorithm to correct the
sample covariance matrix estimate, which yields more accurate
estimates.

In summary, this work has the following contributions:
\begin{itemize}
    \item The proposed MS-KAI-ESPRIT technique.
    \item An MSE analysis of the covariance matrix obtained with the proposed MS-KAI-ESPRIT algorithm.
    \item A comprehensive performance study of MS-KAI-ESPRIT and competing techniques.
\end{itemize}

This paper is organized as follows. Section II describes the system
model. Section III presents the proposed MS-KAI-ESPRIT algorithm. In
section IV, an analytical study of the MSE of the data covariance
matrix free of side-effects is carried out together with a study of
the computational complexity of the proposed and competing
algorithms. In Section V, we present and discuss the simulation
results. Section VI concludes the paper. %In the
%Appendix, we show that at the first of the $P$ iterations, the MSE
%of the estimated data covariance matrix free of side effects is less
%than or equal to the MSE of that of the standard one.}

\section{System Model }
\label{sysmodel}

Let us assume that \textit{P} narrowband signals from far-field
sources impinge on a uniform linear array (ULA) of $M\ (M
> \textit{P})$ sensor elements from  directions
${\boldsymbol \theta}=[\theta_{1},\theta_{2},\ldots, \theta_P]^T$.
We also consider that the sensors are spaced from each other by a
distance $ d\leq\frac{\lambda_{c}}{2}$, where $\lambda_{c}$ is the
signal wavelength, and that without loss of generality, we have
${\frac{-\pi}{2}\leq\theta_{1}\leq\theta_{2}\leq\ldots
\leq\theta_P\leq \frac{\pi}{2}}$.

The $i$th data snapshot of the $M$-dimensional array output vector
can be modeled as
\begin{equation}
\bm x(i)=\bm A\,s(i)+\bm n(i),\qquad i=1,2,\ldots,N,
\label{model}
\end{equation}
where $\bm s(i)=[s_{1}(i),\ldots,s_{P}(i)]^T
\in\mathbb{C}^{\mathit{P\times1}}$ represents the zero-mean source
data vector, $\bm n(i) \in\mathbb{C}^{\mathit{M \times 1}}$ is the
vector of white circular complex Gaussian noise with zero mean and
variance $\sigma_n^2$, and $N$ denotes the number of available
snapshots.

The Vandermonde matrix $\bm A(\bm \Theta)=[\bm
a(\theta_{1}),\ldots,\bm a(\theta_{P})] \in\mathbb
{C}^{\mathit{M\times P}}$, known as the array manifold, contains the
array steering vectors $\bm a(\theta_j)$ corresponding to the $n$th
source, which can be expressed as
\begin{equation}
\bm a(\theta_n)=[1,e^{j2\pi\frac{d}{\lambda_{c}}
\sin\theta_n},\ldots,e^{j2\pi(M-1)\frac{d}{\lambda_{c}}\sin\theta_n}]^T,
\label{steer}
\end{equation}
where $n=1,\ldots, P$. Using the fact that $\bm s(i)$ and $\bm n(i)$
are modeled as uncorrelated linearly independent variables, the
$M\times M$ signal covariance matrix is calculated by
\begin{equation}
\bm R=\mathbb E\left[\bm x(i)\bm x^H(i)
\right]=\bm A\,\bm R_{ss}\bm A^H+
\sigma_n^2\bm I_M,
\label{covariance}
\end{equation}
where the superscript \textit{H} and $\mathbb E[\cdot]$ in $\bm
R_{ss}=\mathbb E[\bm s(i)\bm s^H(i)]$ and in $\mathbb E[\bm n(i)\bm
n^H(i)]=\sigma_n^2\bm I_M^{}$ denote the Hermitian transposition and
the expectation operator and $\bm I_M$ stands for the
$M$-dimensional identity matrix. Since the true signal covariance
matrix is unknown, it must be estimated and a widely-adopted
approach is the sample average formula given by
\begin{equation}
 \bm {\hat{R}}=\frac{1}{N} \sum\limits^{N}_{i=1}\bm x(i)\bm x^H(i),
\label{covsample}
\end{equation}
whose estimation accuracy is dependent on $N$.

\section{Proposed MS-KAI-ESPRIT Algorithm }

In this section, we present the proposed MS-KAI-ESPRIT algorithm and
detail its main features. We start by expanding \eqref{covsample}
using \eqref{model} as derived in \cite{Vorobyov2}:
\begin{eqnarray}
\bm {\hat{R}}=\frac{1}{N} \sum\limits^{N}_{i=1}(\bm A\,s(i)+\bm
n(i))\:(\bm A\,s(i)+\bm n(i))^H \nonumber\\= \bm
A\left\lbrace\frac{1}{N} \sum\limits^{N}_{i=1}\bm s(i)\bm
s^H(i)\right\rbrace\bm A^H+\:\frac{1}{N} \sum\limits^{N}_{i=1}\bm
n(i)\bm n^H(i)\;+\nonumber\\\underbrace{\bm A\left\lbrace\frac{1}{N}
    \sum\limits^{N}_{i=1}\bm s(i)\bm n^H(i)\right\rbrace\:
+\:\left\lbrace\frac{1}{N} \sum\limits^{N}_{i=1}\bm n(i)\bm
s^H(i)\right\rbrace\bm{A}^{H}}_{{"undesirable terms"}}
\label{expandedcovsample}
\end{eqnarray}
The first two terms of \text{$\bm {\hat{R}}$} in
\eqref{expandedcovsample} can be considered as estimates of the two
summands of \text{$\bm R$} given in  \eqref{covariance}, which
represent the signal and the noise components, respectively.  The
last two terms in \eqref{expandedcovsample} are undesirable side
effects, which  can be seen as estimates for the correlation between
the signal and the noise vectors. The system model under study is
based on noise vectors which are zero-mean  and also independent of
the signal vectors. Thus, the signal and noise components are
uncorrelated to each other. As a consequence, for a large enough
number of samples $N$, the last two  terms of
\eqref{expandedcovsample} tend to zero. Nevertheless, in practice
the number of available samples can be limited. In such situations,
the last two terms in \eqref{expandedcovsample} may have significant
values, which causes the deviation of the estimates of the signal
and the noise subspaces from the true signal and noise subspaces.

The key point of the proposed MS-KAI-ESPRIT algorithm is to modify
the sample data covariance matrix estimate at each iteration by
gradually incorporating the knowledge provided by the newer
Vandermonde matrices which progressively embody the refined
estimates from the preceding iteration. Based on these updated
Vandermonde matrices, refined estimates of the projection matrices
of the signal and noise subspaces are calculated. These estimates of
projection matrices associated with the initial sample covariance
matrix estimate and the reliability factor are employed to reduce
its side effects and allow the algorithm to choose the set of
estimates that has the highest likelihood of being the set of the
true DOAs. The modified covariance matrix is computed by computing a
scaled version of the undesirable terms of $\bm {\hat{R}}$, as
pointed out in \eqref{expandedcovsample}.

The steps of the proposed algorithm are listed in Table
\ref{Multi_Step_KAI}. The algorithm starts by computing the sample
data covariance matrix \eqref{covsample}. Next, the DOAs are
estimated using the ESPRIT algorithm. The superscript
$(\cdot)^{(1)}$ refers to the estimation task performed in the first
step. Now, a procedure consisting of $n=1:P$ iterations starts by
forming the Vandermonde matrix using the DOA estimates. Then, the
amplitudes of the sources are estimated such that the square norm of
the differences between the observation vector and the vector
containing estimates and the available known DOAs is minimized. This
problem can be formulated \cite{Vorobyov2} as:
\begin{eqnarray}
    \hat{\bm{s}}(i)=\arg\min_{\substack{\bm
    s}}\parallel\bm{x}(i)-\hat{\bm{A}}\mathbf{s}\parallel^2_2.
    \label{minimization1}
\end{eqnarray}
The minimization of \eqref{minimization1} is achieved using the
least squares technique and the solution is described by
\begin{equation}
\hat{\bm{s}}(i)=(\mathbf{\hat{A}}^{H}\:\mathbf{\hat{A}})^{-1}\:\mathbf{\hat{A}}\:\bm{x}(i)
\label{minimization2}
\end{equation}
The noise component is then estimated as the difference between the
estimated signal and the observations made by the array, as given by
\begin{eqnarray}
 \hat{\bm n}(i)=\bm x(i)\:-\: \hat{\bm A}\:\hat{\bm s}(i).
\label{noise_component}
\end{eqnarray}
After estimating  the signal and noise vectors, the third term in
\eqref{expandedcovsample} can be computed as:
\begin{align}
\bm{V}&\triangleq \hat{\bm{A}}\left\lbrace\frac{1}{N}
\sum\limits^{N}_{i=1}\bm \hat{\mathbf{s}}(i)\bm
\hat{\mathbf{n}}^H(i)\right\rbrace\nonumber\\&=\hat{\bm{A}}\left\lbrace\frac{1}{N}
\sum\limits^{N}_{i=1}(\mathbf{\hat{A}}^{H}\:\mathbf{\hat{A}})^{-1}\mathbf{\hat{A}}^{H}\bm{x}(i)\right.\nonumber\\&\left.\times(\bm{x}^{H}(i)-\bm{x}^{H}(i)\hat{\mathbf{A}}(\hat{\mathbf{A}}^{H}\hat{\mathbf{A}})^{-1}\:\hat{\mathbf{A}}^{H})\right\rbrace\nonumber\\&=\mathbf{\hat{Q}}_{A}\left\lbrace\frac{1}{N}
\sum\limits^{N}_{i=1}
\bm{x}(i)\bm{x}^H(i)\:\left(\mathbf{I}_{M}\:-\:\hat{\mathbf{Q}}_{A}\right)
\right\rbrace\nonumber\\&=\mathbf{\hat{Q}}_{A}\:\mathbf{\hat{R}}\:\mathbf{\hat{Q}}_{A}^{\perp},
\label{terms_deducted}
\end{align}
where
\begin{equation}
\mathbf{\hat{Q}}_{A}\triangleq \mathbf{\hat{A}}\:(\mathbf{\hat{A}}^{H}\:\mathbf{\hat{A}})^{-1}\:\mathbf{\hat{A}}^{H}
\label{signal_projection}
\end{equation}
is an estimate of the projection matrix of the signal subspace, and
\begin{equation}
\mathbf{\hat{Q}}_{A}^{\perp}\triangleq\mathbf{I}_{M}\:-\:\mathbf{\hat{Q}}_{A}
\label{noise_projection}
\end{equation}
is an estimate of the projection matrix of the noise subspace.

Next, as part of the procedure consisting of $n=1:P $ iterations,
the modified data covariance matrix $\mathbf{\hat{R}}^{(n+1)}$ is
obtained by computing a scaled version of the estimated terms from
the initial sample data covariance matrix as given by
\begin{equation}
\label{modified_data_covariance}
\mathbf{\hat{R}}^{(n+1)} = \mathbf{\hat{R}}\:-\:\mathrm{\mu}\:(\mathbf{V}^{(n)}\:+\:\mathbf{V}^{(n)H}),
\end{equation}
where the superscript $(\cdot)^{(n)}$ refers to the $n^{th} $
iteration performed. The scaling or reliability factor \text{$\mu $}
increases from 0 to 1 incrementally, resulting in modified data
covariance matrices. Each of them gives origin to new estimated DOAs
also denoted by the superscript  $(\cdot)^{(n+1)}$ by using the
ESPRIT algorithm, as briefly described ahead.

%\textcolor{red}{Before continuing, it is important to address two
%questions that commonly arise in DOA estimation. The former is how
%to estimate the rank \textit{P}, i.e. the number of sources. The
%latter, which is specifically related to subspace-based methods, is
%what kind of approaches can be employed to estimate the signal and
%the orthogonal subspaces from the data records.}

In this work, the rank \textit{P} is assumed to be known, which is
an assumption frequently found in the literature. Alternatively, the
rank \textit{P} could be estimated by model-order selection schemes
such as Akaike´s Information Theoretic Criterion (AIC) \cite{Schell}
and the Minimum Descriptive Length (MDL) Criterion \cite{Rissanen}.

%\textcolor{red}{In the AIC and the MDL-based approaches, the number
%of $\mathit{\hat{P}}$ are computed as the value of
%$\mathit{p}\in[0,1,\ldots,M-1]$, which minimizes the following
%criteria:} \textcolor{red}{
%\begin{align}
%\mathit{AIC(d)}=-\log \mathit{\left\lbrace \frac{\prod_{i=p+1}^{M}\lambda_{i}^{\frac{1}{M-p}}}{\frac{1}{M-p}\sum_{i=p+1}^{M}\lambda_{i}}  \right\rbrace^{(M-p)N}+p(2M-p)}
%\label{AIC}
%\end{align}
%%
%\begin{align}
%\mathit{MDL(d)}=-\log \mathit{\left\lbrace \frac{\prod_{i=p+1}^{M}\lambda_{i}^{\frac{1}{M-p}}}{\frac{1}{M-p}\sum_{i=p+1}^{M}\lambda_{i}} \right\rbrace^{(M-p)N}}\nonumber\\+\mathit{\frac{p}{2}(2M-p)\log N}
%\label{MDL}
%\end{align}}
%\textcolor{red}{where $\mathit{\lambda}_{i}$ are the eigenvalues of
%the sample covariance matrix \eqref{covsample} and \textit{N} is the
%number of snapshots to compute it.\textit{M} is the number of
%elements in the array. The first terms in \eqref{AIC} and
%\eqref{MDL} are derived directly from the log-likelihood
%function.The second term in \eqref{AIC} is the penalty factor added
%by the AIC criterion, while in \eqref{MDL} is the penalty factor
%added by the MDL criterion. The MDL criterion produces  a consistent
%estimate of the number of the signals whereas the AIC criterion
%yields an inconsistent estimate that tends asymptoticaly to
%overestimate them.}
In order to estimate the signal and the orthogonal subspaces from
the data records, we may consider two approaches
\cite{Vaccaro,Haardt}: the direct data approach and the covariance
approach. The direct data approach makes use of singular value
decomposition(SVD) of the data matrix $\mathbf{X}$, composed of the
$i$th data snapshot \eqref{model} of the $M$-dimensional array data
vector:
\begin{align}
\bm X=&[\bm
{x}(1),\bm{x}(2),\ldots,\bm{x}(N)]\nonumber\\=&\bm A[\bm
{s}(1),\bm{s}(2),\ldots,\bm{s}(N)]+[\bm
{n}(1),\bm
{n}(2),\ldots,\bm{n}(N)]\nonumber\\=&\bm{A(\Theta)\;S}+\:\bm{N}\;\in\mathbb{C}^{\mathit{M \times N}}
\label{Data_matrix}
\end{align}

Since the number of the sources is assumed known or can be estimated
by AIC\cite{Schell} or MDL\cite{Rissanen} , as previously mentioned,
we can write $\mathbf{X}$ as:

\begin{align}
\begin{array}{ccc}
\bm{X}
\end{array}& =\left[ \begin{array}{ccc}
\mathbf{\hat{U}}_{s} & \mathbf{\hat{U}}_{n}  \\
\end{array} \right]\left[ \begin{array}{ccc}
\mathbf{\hat{\Gamma}}_{s} & 0  \\
0 & \mathbf{\hat{\Gamma}}_{n}
\end{array} \right]\left[ \begin{array}{ccc}
\mathbf{\hat{U}}_{s}^{H} \\
\mathbf{\hat{U}}_{n}^{H}
\end{array} \right],
\label{SVD}
\end{align}
%
where the diagonal matrices $\mathbf{\hat{\Gamma}}_{s}$ and
$\mathbf{\hat{\Gamma}}_{n}$ contain the $\mathit{P}$ largest
singular values and the $\mathit{M-P}$ smallest singular values,
respectively. The estimated signal subspace $\mathbf{\hat{U}}_{s}$
$\in\mathbb{C}^{\mathit{M \times P}}$ consists of the singular
vectors corresponding to $\mathbf{\hat{\Gamma}}_{s}$ and the
orthogonal subspace $\mathbf{\hat{U}}_{n}$ $\in\mathbb{C}^{\mathit{M
\times (M-P)}}$ is related to $\mathbf{\hat{\Gamma}}_{n}$. If the
signal subspace is estimated a rank-\textit{P} approximation of the
SVD can be applied.

The covariance approach applies the eigenvalue decomposition (EVD)
of the sample covariance matrix \eqref{covsample}, which is related
to the data matrix \eqref{Data_matrix}:
    \begin{equation}
    \bm {\hat{R}}=\frac{1}{N} \sum\limits^{N}_{i=1}\bm x(i)\bm x^H(i)=\frac{1}{N}\bm {X}\bm {X}^H\;\in\mathbb{C}^{\mathit{M \times M}
        \label{rel_datamatrix_covsample}}
    \end{equation}
Then, the EVD of \eqref{rel_datamatrix_covsample} can be carried out
as follows:
\begin{equation}
\begin{array}{ccc}
\mathbf{\hat{R}}
\end{array} =\left[ \begin{array}{ccc}
\mathbf{\hat{U}}_{s} & \mathbf{\hat{U}}_{n}  \\
\end{array} \right]\left[ \begin{array}{ccc}
\mathbf{\hat{\Lambda}}_{s} & 0  \\
0 & \mathbf{\hat{\Lambda}}_{n}
\end{array} \right]\left[ \begin{array}{ccc}
\mathbf{\hat{U}}_{s}^{H} \\
\mathbf{\hat{U}}_{n}^{H}
\end{array} \right],
\label{Detalha_ESPRIT}
\end{equation}
where the diagonal matrices $\mathbf{\hat{\Lambda}_{s}}$ and
$\mathbf{\hat{\Lambda}_{n}}$ contain the \textit{P} largest and the
    \textit{M-P} smallest eigenvalues, respectively.
    The estimated signal subspace $\mathbf{\hat{U}}_{s}$ $\in$
    $\mathbb{C}^{\mathit{M\times P}}$ corresponding to $\mathbf{\hat{\Gamma}_{s}}$
    and the orthogonal subspace $\mathbf{\hat{U}}_{n}$ $\in$
    $\mathbb{C}^{\mathit{M\times(M-P) }}$ complies with $\hat{\Gamma}_{n}$.
    If the signal subspace is estimated a rank-P approximation of the EVD can be applied.
With infinite precision arithmetic, both SVD and EVD can be
considered equivalent. However, as in practice, finite precision
arithmetic is employed, 'squaring' the data to obtain the Gramian
$\bm {X}\bm {X}^H$ \eqref{rel_datamatrix_covsample} can result in
round-off errors and overflow. These are potential problems to be
aware when using the covariance approach. %Despite this, we can
%employ the covariance approach  for the purpose of having the same
%basis for comparing the proposed algorithm to others which are based
%on the sample covariance matrix.}

Now, we can briefly review ESPRIT. We start by forming a twofold
subarray configuration, as each row of the array steering matrix
$\bm A(\bm \Theta)$ corresponds to one sensor element of the antenna
array. The subarrays are specified by two $\mathit{(s\times
M)}$-dimensional selection matrices $\mathbf{J_{1}}$  and
$\mathbf{J_{2}}$ which choose $\mathit{s}$ elements of the
$\mathit{M}$ existing sensors, respectively, where $\mathit{s}$ is
in the range  $\mathit{P\leq s < M}$. For maximum overlap, the
matrix $\mathbf{J_{1}}$ selects  the first $\mathit{s=M-1}$ elements
and the matrix $\mathbf{J_{2}}$ selects the last $\mathit{s=M-1}$
rows of $\bm A(\bm \Theta)$.

Since the matrices $\mathbf{J_{1}}$ and $\mathbf{J_{2}}$ have now
been computed, we can estimate the  operator $ \mathbf{\Psi} $ by
solving the approximation of the shift invariance equation
\eqref{shift_invariance_equation} given by
\begin{equation}
\mathbf{J}_{1}\:\mathbf{\hat{U}}_{s}\:\mathbf{\Psi}\:\approx\:\mathbf{J}_{2}\:\mathbf{\hat{U}}_{s}.
\label{shift_invariance_equation}
\end{equation}
where $\hat{U}_{s}$ is obtained in \eqref{Detalha_ESPRIT}.

Using  the least squares (LS) method, which yields
\begin{equation}
\hat{\mathbf{\Psi}}=\arg\min_{\substack{\mathbf
{\Psi}}}\parallel\mathbf{J}_{2}\:\mathbf{\hat{U}}_{s}\:-\:\mathbf{J}_{1}\:\mathbf{\hat{U}}_{s}\:\mathbf{\Psi}\parallel_{F}\:=\:\left(
\mathbf{J}_{1}\:\hat{\mathbf{U}}_{s}\right)
^{\dagger}\:\mathbf{J}_{2}\:\hat{\mathbf{U}}_{s},
\end{equation}
%
where $\parallel\cdot\parallel_{F}$ denotes the Frobenius norm and
$\left( \cdot\right)^{\dagger}$ stands for the pseudo-inverse.

Lastly, the eigenvalues $\lambda_{i}$ of $\hat{\mathbf{\Psi}}$
contain the estimates of the spatial frequencies $\gamma_{i}$
computed as:
\begin{equation}
\gamma_{i}\:=\:\arg\left(\lambda_{i} \right),
\label{spatial_frequencies}
\end{equation}
so that the DOAs can be calculated as:
\begin{equation}
\hat{\theta}_{i}\:=\:\arcsin\left(\frac{\gamma_{i}\:\lambda_{c}}{2\pi\:\mathit{d}} \right)
\label{doas_ESPRIT}
\end{equation}
where for \eqref{spatial_frequencies} and \eqref{doas_ESPRIT}
$\mathrm{i=1,\cdots,P}$.

Then, a new Vandermonde matrix $\mathbf{\hat{B}}^{(n+1)}$ is formed
by the steering vectors of those refined estimates of the DOAs. By
using this updated matrix, it is possible to compute the refined
estimates of the projection matrices of the signal \text{$
\mathbf{\hat{Q}}_{B}^{(n+1)} $} and the noise \text{$
\mathbf{\hat{Q}}_{B}^{(n+1)\perp} $} subspaces.

Next, employing the refined estimates of the projection matrices,
the initial sample data matrix, $\bm {\hat{R}}$, and the number of
sensors and sources, the stochastic maximum likelihood objective
function $\mathit{U^{(n+1)}(\mu)}$ \cite{Stoica} is computed for
each value of \text{$\mu $} at the $n^{th}$ iteration, as follows:
\begin{equation}
\begin{split}
\mathit{U^{(n+1)}(\mu)} & =\mathrm{ln\:det}
\Big(\mathbf{\hat{Q}}_{B}^{(n+1)}\:\mathbf{\hat{R}}\:\mathbf{\hat{Q}}_{B}^{(n+1)}
\\
&  \quad + \dfrac{{\rm
Trace}\{\mathbf{\hat{Q}}_{B}^{\perp\:(n+1)}\:\mathbf{\hat{R}}\}}
{\mathrm{M-P}}\:\mathbf{\hat{Q}}_{B}^{\:(n+1)\perp} \Big).
\end{split}
\label{SML_objective_function}
\end{equation}

The previous computation selects the set of unavailable DOA
estimates that have a higher likelihood at each iteration. Then, the
set of estimated DOAs corresponding to the optimum value of
\text{$\mu $} that minimizes \eqref{SML_objective_function} also at
each $n^{th}$ iteration is determined. Finally, the output of the
proposed MS-KAI-ESPRIT algorithm is formed by the set of the estimates
obtained at the $P^{th}$ iteration, as described in Table
\ref{Multi_Step_KAI}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{table}[htb!]
    \centering
    \small
    \caption{Proposed MS-KAI-ESPRIT Algorithm}\smallskip
    \scalebox{1.0}\medskip{
        \begin{tabular}{|r l|}
            \hline

            \multicolumn{2}{|l|}{\small $\textbf{\underline{Inputs}:}$}\\[0.7ex]
            \multicolumn{2}{|l|}{\small$\mathit{M}$,\hspace{2mm}$\mathit{d}$,\hspace{2mm}$\lambda$,\hspace{2mm}$\mathit{N}$,\hspace{2mm}$\mathit{P}$ }\\[0.6ex]
            \multicolumn{2}{|l|}{\small\text{Received vectors}  $\bm x(1)$,\hspace{2mm}$\bm x(2)$,$\cdots$, $\bm x(N)$}\\[0.6ex]



            \multicolumn{2}{|l|}{\small $\textbf{\underline{Outputs}:}$}\\[0.6ex]

            \multicolumn{2}{|l|}{\small\text{Estimates}\hspace{1mm}$\mathit{\hat{\theta}_{1}^{(n+1)}(\mu\,opt)}$,\hspace{2mm}$\mathit{\hat{\theta}_{2}^{(n+1)}(\mu\,opt)}$,$\cdots$,\hspace{2mm}$\mathit{\hat{\theta}_{P}^{(n+1)}(\mu\,opt)}$} \\[3.1ex]
            \hline


            \multicolumn{2}{|l|}{\small $\textbf{\underline{First step}:}$}\\[0.9ex]

            \multicolumn{2}{|l|}{\small $\mathbf{\hat{R}}=\frac{1}{N} \sum\limits^{N}_{i=1}\bm x(i)\bm x^H(i)$}\\[1.8ex]

            \multicolumn{2}{|l|}{\small $\{\mathit{\hat{\theta}_{1}}^{(1)},\:\mathit{\hat{\theta}_{2}}^{(1)},\cdots,\mathit{\hat{\theta}_{P}}^{(1)}\}\;\;\underleftarrow{ESPRIT}$ $\:(\mathbf{\hat{R}},P,d,\lambda)$}\\[1.2ex]


            \multicolumn{2}{|l|}{\small$\mathbf{\hat{A}}^{(1)}=\left[\mathbf{a}(\mathit{\hat{\theta}_{1}^{(1)}}),\mathbf{a}(\mathit{\hat{\theta}_{2}^{(1)}}),\cdots,\mathbf{a}(\mathit{\hat{\theta}_{P}^{(1)}})\right]$} \\ [1.1ex]

            \multicolumn{2}{|l|}{\small $\textbf{\underline{Second step}:}$}\\[0.9ex]

            \multicolumn{2}{|l|}{\small \textbf{for}  \textit{n}\hspace{1mm}=\hspace{2mm}\text{1}\hspace{1mm}:\hspace{1mm}\textit{P}}\\[0.6ex]

            \multicolumn{2}{|l|}{\small $\mathbf{\hat{Q}}_{A}^{(n)}= \mathbf{\hat{A}}^{(n)}\:(\mathbf{\hat{A}}^{(n)H}\:\mathbf{\hat{A}}^{(n)})^{-1}\:\mathbf{\hat{A}}^{(n)H}$ }\\[0.9ex]

            \multicolumn{2}{|l|}{\small$\mathbf{\hat{Q}}_{A}^{(n)\perp}=\mathbf{I}_{M}\:-\:\mathbf{\hat{Q}}_{A}^{(n)}$ }\\[1.0ex]

            \multicolumn{2}{|l|}{\small $\mathbf{V}^{(n)}=\mathbf{\hat{Q}}_{A}^{(n)}\:\mathbf{\hat{R}}\:\mathbf{\hat{Q}}_{A}^{(n)\perp}$}\\[1.8ex]



            \multicolumn{2}{|l|}{\small \textbf{for} $\mathbf{\mu=}\hspace{1mm}\text{0}:\text{$\iota$ \hspace{1mm}:\hspace{1mm}1} $}\\[0.9ex]

            \multicolumn{2}{|l|}{\small $ \mathbf{\hat{R}}^{(n+1)} = \mathbf{\hat{R}}\:-\:\mathrm{\mu}\:(\mathbf{V}^{(n)}\:+\:\mathbf{V}^{(n)H})$} \\[0.9ex]



            \multicolumn{2}{|l|}{\small $\{\mathit{\hat{\theta}_{1}}^{(n+1)},\:\mathit{\hat{\theta}_{2}}^{(n+1)},\cdots,\mathit{\hat{\theta}_{P}}^{(n+1)}\}\;\;\underleftarrow{ESPRIT}$ $\:(\mathbf{\hat{R}}^{(n+1)},\:P,d,\lambda)$}\\[1.4ex]

            \multicolumn{2}{|l|}{\small$\mathbf{\hat{B}}^{(n+1)}=\left[\mathbf{a}(\mathit{\hat{\theta}_{1}^{(n+1)}}),\mathbf{a}(\mathit{\hat{\theta}_{2}^{(n+1)}}),\cdots,\mathbf{a}(\mathit{\hat{\theta}_{P}^{(n+1)}})\right]$} \\ [1.4ex]

            \multicolumn{2}{|l|}{\small $\mathbf{\hat{Q}}_{B}^{(n+1)}= \mathbf{\hat{B}}^{(n+1)}\:(\mathbf{\hat{B}}^{(n+1)H}\:\mathbf{\hat{B}}^{(n+1)})^{-1}\:\mathbf{\hat{B}}^{(n+1)H}$}\\[1.2ex]

            \multicolumn{2}{|l|}{\small$\mathbf{\hat{Q}}_{B}^{(n+1)\perp}=\mathbf{I}_{M}\:-\:\mathbf{\hat{Q}}_{B}^{(n+1)}$}\\[1.2ex]

            \multicolumn{2}{|l|}{\small $\mathit{U^{(n+1)}(\mu)}=\mathrm{ln\:det}\left(\cdot\right),$}\\[1.1ex]

            \multicolumn{2}{|l|}{\small$\left(\cdot\right) =\left(\mathbf{\hat{Q}}_{B}^{(n+1)}\:\mathbf{\hat{R}}\:\mathbf{\hat{Q}}_{B}^{(n+1)}+\dfrac{{\rm Trace}\{\mathbf{\hat{Q}}_{B}^{\perp\:(n+1)}\:\mathbf{\hat{R}}\}} {\mathrm{M-P}}\:\mathbf{\hat{Q}}_{B}^{\:(n+1)\perp}\right)$}\\[1.1ex]



            \multicolumn{2}{|l|}{\small $\mathit{\mu}_{\mathrm{opt}}^{(n+1)}=\arg \min \hspace{1mm}\mathit{U^{(n+1)}(\mu)}$}\\[1.0ex]

            \multicolumn{2}{|l|}{\small $\mathrm{DOAs}^{(n+1)}= \left(\ast\right),$}\\[1.1ex]

            \multicolumn{2}{|l|}{\small$\left(\ast\right) =
                 \{\mathit{\hat{\theta}_{1}^{(n+1)}(\mu\,opt)}$,\hspace{2mm}$\mathit{\hat{\theta}_{2}^{(n+1)}(\mu\,opt)}$,$\cdots$,\hspace{2mm}$\mathit{\hat{\theta}_{P}^{(n+1)}(\mu\,opt)}\}$}\\[1.0ex]

            \multicolumn{2}{|l|}{\small $\mathbf{\hat{A}}^{(n+1)}=\left\{\mathbf{a}(\mathit{\hat{\theta}_{\{1,\cdots,n\}}^{(n+1)}})\right\}\bigcup\left\{\mathbf{a}(\mathit{\hat{\theta}_{\{1,\cdots,P\}\,-\,\{1,\cdots,n\}}^{(1)}})\right\}$}\\[1.0ex]

            \multicolumn{2}{|l|}{\small \textbf{end for}}\\[1.0ex]

            \multicolumn{2}{|l|}{\small \textbf{end for}}\\[1.0ex]

            \hline
        \end{tabular}
    }
    %\caption{Table to test captions and labels}
    \label{Multi_Step_KAI}

\end{table}
%
\section{ Analysis}
\label{Analysis}

In this section, we carry out an analysis of the MSE of the data
covariance matrix free of side effects along with a study of the
computational complexity of the proposed MS-KAI-ESPRIT and existing
direction finding algorithms.

\subsection{MSE Analysis}
\label{MSE_analysis}

In this subsection we show that at the first of the $P$ iterations,
the MSE of the data covariance matrix free of side effects
$\mathbf{\hat{R}}^{(n+1)}$  is less than or equal to the MSE of that
of the original one $\mathbf{\hat{R}}$. This can be formulated as:
%
\begin{align}
\mathrm{MSE}\left( \mathbf{\hat{R}}^{(n+1)}\right) \leq\mathrm{MSE}\left(\mathbf{\hat{R}}\right)
\label{problem_formulation1}
\end{align}
%
or, alternatively, as
\begin{align}
\mathrm{MSE}\left( \mathbf{\hat{R}}^{(n+1)}\right)- \mathrm{MSE}\left(\mathbf{\hat{R}}\right) \leq0
\label{problem_formulation2}
\end{align}
The proof of this inequality is provided in the Appendix.

%
\subsection{Computational Complexity Analysis}
\label{computational_analysis}

In this section, we evaluate the computational cost of the proposed
MS-KAI-ESPRIT algorithm which is compared to the following classical
subspace methods: ESPRIT \cite{Roy}, MUSIC \cite{schimdt},
Root-MUSIC \cite{Barabell}, Conjugate Gradient (CG) \cite{Semira},
Auxiliary Vector Filtering (AVF) \cite{Grover} and TS-ESPRIT
\cite{Pinto}. The ESPRIT and MUSIC-based methods use the Singular
Value Decomposition (SVD) of the sample covariance matrix
\eqref{covsample}. The computational complexity of MS-KAI-ESPRIT  in
terms of number of multiplications and additions is depicted in
Table \ref{Comput_Complexity1}, where $\mathrm{\tau}=\frac{1}{ \iota} +1$. The increment
${ \iota}$ is defined in Table \ref{Multi_Step_KAI}. As can be seen,
for this specific configuration used in the simulations
\ref{simulations} MS-KAI-ESPRIT shows a relatively high
computational burden with
$\mathcal{O}\mathit{(P\tau(3M^{3}+8MN^{2}))}$, where $\tau$ is
typically an integer that ranges from $ 1$  to $20 $. It can be
noticed that for the configuration used in the simulations $(P=4,
M=40, N=25)$ $ 3M^{3}$ and $8MN^{2}$ are comparable, resulting in
two dominant terms. It can also be seen that the number of multiplications required by the proposed algorithm is more significant than the number of additions. For this reason, in Table \ref{Comput_Complexity2},  we computed only the  computational burden of the previously mentioned algorithms in terms of multiplications for the purpose of comparisons. In that table, $\Delta$  stands for the search step.

Next, we will evaluate the influence of the number of sensor
elements on the number of multiplications based on the specific
configuration described in Table \ref{sysmodel}. Supposing
$\mathrm{P=4}$ narrowband signals impinging a ULA of $\mathrm{M}$
sensor elements and $\mathrm{N=25} $ available snapshots, we obtain
Fig. \ref{figura:Multiplications_reviewer2}. We can see the main trends in
terms of computational cost measured in multiplications of the
proposed and analyzed algorithms.
By examining Fig. \ref{figura:Multiplications_reviewer2}, it can be noticed
that in the range $M=\left[ 20\ 70\right] $ sensors, the curves
describing the exact number of multiplications in MS-KAI-ESPRIT and
AVF tend to merge. %In this region, $
%\frac{\mathrm{number\,of\,multiplications\, of\,MS-KAI-ESPRIT
%}}{number\,of\,multiplications\, of\,AVF} $ ranges from $\left[ 1.2\
%1.09\right] $.
For $M=40$, this ratio tends to $1$, i.e. the number
of multiplications are almost equivalent.
%
\begin{table}[!h]
    \caption{Computational complexity - MS-KAI-ESPRIT}
    \vspace{2mm}
    \centering
    \begin{tabular}{|l|l|p{6cm}|}
        \hline
        & \underline{Multiplications} \\

        \rule{0pt}{3ex}  & $P\:\mathrm{\tau [\frac{10}{3}M^{3}+M^{2}(3P+2)+M(\frac{5}{2}P^{2}+\frac{1}{2}P+8N^{2})}$  \\[1pt]

        MS-KAI&$\mathrm{+P^{2}(\frac{17}{2}P+\frac{1}{2})]}$ \\[1pt]
        -ESPRIT& \\[1pt]
        (Proposed)& $\mathrm{+P\:[2M^{3}+M^{2}(P)+M(\frac{3}{2}P^{2}+\frac{1}{2}P)+P^{2}(\frac{P}{2}+\frac{3}{2}) ]}$ \\[1pt]

        & $\mathrm{+2M^{2}(P)+M(P^{2}-P+8N^{2})+P^{2}(8P-1)}$\\[2pt]

        & \underline{Additions} \\[2pt]

        \rule{0pt}{3ex}  & $P\:\mathrm{\tau
            [\frac{10}{3}M^{3}+M^{2}(3P-1)+M(\frac{5}{2}P^{2}-\frac{9}{2}P+8N^{2})}$  \\[1pt]

        &$\mathrm{+P(8P^{2}-2P-\frac{5}{2})]}$ \\[1pt]

        & $\mathrm{+P\:[2M^{3}+M^{2}(P-2)+M(\frac{3}{2}P^{2}-\frac{1}{2}P)-P(P+\frac{1}{2}) ]}$ \\[1pt]

        & $\mathrm{+2M^{2}(P)+M(P^{2}-4P+8N^{2})+P(8P^{2}-P-2)}$\\[2pt]
        \hline

    \end{tabular}

    \label{Comput_Complexity1}
\end{table}
%
\begin{table}[!h]
    \caption{Computational complexity - other algorithms}
    \vspace{2mm}
    \centering
    \begin{tabular}{|l|l|p{6cm}|}
        \hline
        Algorithm & Multiplications \\[2pt]
        \hline
        MUSIC \cite{schimdt} & $\mathrm{\frac{180}{\Delta}[M^{2}+M(2-P)-P]+8MN^{2}}$  \\[2pt]
        \hline
        root-MUSIC\cite{Barabell} & $\mathrm{2M^{3}-M^{2}P+8MN^{2}}$  \\[2pt]
        \hline
        AVF \cite{Grover} & $\mathrm{\frac{180}{\Delta}[M^{2}(3P+1)+M(4P-2)+P+2]}$\\[1pt]
        &$\mathrm{+M^{2}N}$  \\[2pt]
        \hline
        CG \cite{Semira} & $\mathrm{\frac{180}{\Delta}[M^{2}(P+1)+M(6P+2)+P+1]+M^{2}N}$  \\[2pt]
        \hline
        ESPRIT\cite{Roy} & $\mathrm{2M^{2}P+M(P^{2}-2P+8N^{2})+8P^{3}-P^{2} }$  \\[2pt]
        \hline
        & $\mathrm{\tau[3M^{3}+M^{2}(3P+2)+M(\frac{5}{2}P^{2}-\frac{3}{2}P+8N^{2})}$  \\[1pt]

        &$\mathrm{+P^{2}(\frac{17}{2}P+\frac{1}{2})+1]}$ \\[1pt]

        TS-ESPRIT \cite{Pinto}*& $\mathrm{+[2M^{3}+M^{2}(3P)+M(\frac{5}{2}P^{2}-\frac{3}{2}P+8N^{2})}$ \\[1pt]

        & $\mathrm{+P^{2}(\frac{17}{2}P+\frac{1}{2})]}$\\[2pt]

        \hline
    \end{tabular}

    \label{Comput_Complexity2}
\end{table}
%
\begin{figure}[!h]

    \centering % para centralizarmos a figura
    \includegraphics[width=8cm,height=6cm]{fig1.eps} % leia abaixo
    \vspace{-1.0em}\caption{Number of multiplications as powers of 10 versus number of sensors  for $P=4$, $N=25$. }

    \label{figura:Multiplications_reviewer2}  %Multiplications_dB
\end{figure}


\section{Simulations}
\label{simulations}

In this section, we examine the performance  of the proposed
MS-KAI-ESPRIT in terms of probability of resolution and RMSE and
compare them to the standard ESPRIT \cite{Roy}, the Iterative ESPRIT
(IESPRIT), which is also developed here by combining the approach in
\cite{Vorobyov2} that exploits knowledge of the structure of the
covariance matrix and its perturbation terms, the Conjugate Gradient
(CG) \cite{Semira}, the Root-MUSIC \cite{Barabell}, and the MUSIC
\cite{schimdt} algorithms. Despite TS-ESPRIT is based on the
knowledge of available known DOAS and the proposed MS-KAI-ESPRIT
does not have access to prior knowledge, TS-ESPRIT is plotted with
the aim of illustrating the comparisons. For a fair comparison in
terms of RMSE and probability of resolution of all studied
algorithms, we suppose that we do not have prior knowledge, that is
to say that although we have available known DOAs, we compute
TS-ESPRIT as they were unavailable. We employ a ULA with \textit{
M=40} sensors, inter-element spacing $\Delta=\frac{\lambda_{c}}{2}$
and assume there are four uncorrelated complex Gaussian signals with
equal power impinging on the array. The closely-spaced sources are
separated by \textit{$2.4^{o}$}, at $\mathrm
(10.2^{o},12.6^{o},15^{o},17.4^{o})$,  and the number of available
snapshots is \textit{N}=25. For TS-ESPRIT, as previously mentioned,
we presume a priori knowledge of the last true DOAS
$\mathrm(15^{o},17.4^{o})$

In Fig. \ref{figura:DSP_PR_2ponto4deg_40sens_25snap_100runs}, we
show the probability of resolution versus SNR. We take into account
the criterion \cite{Stoica3}, in which two sources with DOA
$\theta_{1}$ and $\theta_{2}$  are said to be resolved if their
respective estimates $\hat{\theta}_{1}$ and $\hat{\theta}_{2}$ are
such that both $\left|\hat{\theta}_{1} -\theta_{1}\right|$ and
$\left|\hat{\theta}_{2} -\theta_{2}\right|$ are less than
$\left|\theta_{1} -\theta_{2}\right|/2$. The proposed MS-KAI-ESPRIT
algorithm outperforms IESPRIT developed here, based on
\cite{Vorobyov2}, and the standard ESPRIT \cite{Roy} in the range
between $-6$ and $ 5dB $ and MUSIC \cite{schimdt} from  $-6$ to $
8.5dB $. MS-KAI-ESPRIT also outperforms CG \cite{Semira} and Root-Music
\cite{Barabell} throughout the whole range of values. The poor
performance of the latter could be expected from the results for two
closed signals obtained in \cite{Vorobyov2}. When compared to
TS-ESPRIT, which as previously discussed, was supposed to have the
best performance, the proposed MS-KAI-ESPRIT algorithm is outperformed
by the former only in the range between $ -6 $ and $ -2dB $. From
this last point to $ 20dB $ its performance is superior or equal to
the other algorithms.

In Fig. \ref{figura:DSP_RMSE_CRB_2ponto4deg_40sens_25snap_100runs},
it is shown the RMSE in dB  versus SNR, where  the term CRB refers
to the square root of the deterministic Cramér-Rao bound
\cite{Stoica4}. The RMSE is defined as:
\begin{equation}
\centering \mathrm{RMSE}
=\sqrt{\frac{1}{L\:P}\sum\limits^{L}_{l=1}\sum\limits^{P}_{p=1}\bm(\theta_p
-\bm \hat{\theta}_p(l))^{2}}, \label{RMSE_run}
\end{equation}
where $L$ is the number of trials.

The results show the superior performance of MS-KAI-ESPRIT in the
range between $-2.5$ and $5$ dB. From this last point to $20$ dB,
MS-KAI-ESPRIT, IESPRIT, ESPRIT and TS-ESPRIT have similar
performance. The only range in which MS-KAI-ESPRIT is outperformed
lies in the range between $-6$ and $-2.5$ dB. From this last point
to $20$ dB its performance is better or similar to the others.


\begin{figure}[!h]
    \centering % para centralizarmos a figura
    \includegraphics[width=8cm,height=6cm]{fig2.eps} % leia abaixo
    \vspace{-1.0em}\caption{Probability of resolution versus SNR with $P=4$ uncorrelated sources, $M=40$, $N=25$, $L=100$ runs}
    \label{figura:DSP_PR_2ponto4deg_40sens_25snap_100runs}
\end{figure}
%

\begin{figure}[!h]
    \centering % para centralizarmos a figura
    \includegraphics[width=8cm, height=6cm]{fig3.eps} % leia abaixo
    \vspace{-1.0em}\caption{RMSE and the square root of CRB versus SNR with $P=4$ uncorrelated sources, $M=40$, $N=25$, $L=100$ runs}
    \label{figura:DSP_RMSE_CRB_2ponto4deg_40sens_25snap_100runs}
\end{figure}
%
Now, we focus on the performance of MS-KAI-ESPRIT under more severe
conditions, i.e., we analyze it in terms of RMSE when at least two
of the four equal-powered Gaussian signals are strongly correlated,
as shown in the following signal correlation matrix $\bm R_{ss}$
\eqref{signals_correlated_matrix}:
\begin{equation}
\bm R_{ss}=\sigma_{s}^{2}\begin{bmatrix}
\label{signals_correlated_matrix}
1   &  0.9   & 0.6  & 0   \\
0.9 &   1    & 0.4  & 0.5    \\
0.6 &  0.4   & 1    & 0     \\
0   &  0.5   & 0    & 1
\end{bmatrix}.
\end{equation}

The signal-to-noise ratio $\text(SNR)$ is defined as $SNR\triangleq 10\log_{10}\left(\frac{\sigma_{s}^{2}}{\sigma_{n}^{2}} \right)$.
\begin{figure}[!h]
    \centering % para centralizarmos a figura
    \includegraphics[width=8cm, height=6cm]{fig4.eps} % leia abaixo
    \vspace{-1.0em}\caption{RMSE and the square root of CRB versus SNR with $P=4$ correlated sources, $M=40$, $N=25$, $L=250$ runs}
    \label{figura:RMSE_dB_correlated_2ponto4deg_40sens_25snap_250runs}
\end{figure}
%
In Fig.
\ref{figura:RMSE_dB_correlated_2ponto4deg_40sens_25snap_250runs}, we
can see the performance of the same algorithms plotted in Fig.
\ref{figura:DSP_RMSE_CRB_2ponto4deg_40sens_25snap_100runs} in terms
of $\mathrm{RMSE(dB)}$ versus SNR computed after $\mathrm{250}$
runs, when  the signal correlation matrix is given by
\eqref{signals_correlated_matrix}. As can be seen, the superior
performance of MS-KAI-ESPRIT occurs in the whole range between $4.0$
and $ 12$ dB , which can be considered a small but consistent gain.
From $ 12$dB to $ 20$dB MS-KAI-ESPRIT, TS-ESPRIT, IESPRIT and ESPRIT
have similar performance.  The values for which MS-KAI-ESPRIT is
outperformed are in the range between $ -6.0 $ and $4.0$dB.
%\setstcolor{red}
%\st{ Nevertheless, it can  be noticed that
%in practice the interesting values of $\mathrm{RMSE(dB)}$ lay in the
%region where they are $\mathrm{RMSE< \frac{2.4^{o}}{2}=1.2 ^{o}}$
%i.e., $\mathrm{RMSE(dB)<0.79dB}$, which corresponds to the range $
%\left[ 2.7 \; 20\right]dB $, where in its subrange between $ 2.7 $
%and $4.0dB$, MS-KAI-ESPRIT is slightly outperformed by IESPRIT.
%Despite this, as previously mentioned, from  $ 4.0 dB $ to $ 20dB $,
%MS-KAI-ESPRIT is superior to the other algorithms.
%%
%In this specific case, the
%constraint $\mathrm{RMSE< 1.2 ^{o}}$  prevents the region bounded by
%the RMSE resulting from the estimate of one of the DOAs from
%overlapping  the region limited by  the RMSE arising from the
%estimate of the adjacent DOA. This case is outlined in Fig.}
%\sout{\ref{figura:RMSE_overlapping}}

In  Fig. \ref{RMSE_dB_corr_letter_steps_full_reviewer1}, we have
provided further  simulations to illustrate the performance of each
iteration of MS-KAI ESPRIT in terms of RMSE. The resulting
iterations can be compared to each other and to the the original
ESPRIT, which corresponds to the first step of MS-KAI ESPRIT. For
this purpose, we have considered the same scenario employed before,
except for the number of the trials, which is $L=200$ runs for  all
simulations. In particular, we have considered the case of
correlated sources. From Fig.
\ref{RMSE_dB_corr_letter_steps_reviewer1}, which is a magnified
detail of Fig. \ref{RMSE_dB_corr_letter_steps_full_reviewer1}, it
can be seen that the estimates become more accurate with the
increase of iterations.

\begin{figure}[!h]
    \centering % para centralizarmos a figura
    \includegraphics[width=8cm, height=6cm]{fig5.eps} % leia abaixo
    \vspace{-1.0em}\caption{RMSE for each iteration of MS-KAI ESPRIT,original ESPRIT and CRB versus SNR with $P=4$ correlated sources, $M=40$, $N=25$, $L=200$ runs}
    \label{RMSE_dB_corr_letter_steps_full_reviewer1}
\end{figure}
%
\begin{figure}[!h]
    \centering % para centralizarmos a figura
    \includegraphics[width=8cm, height=6cm]{fig6.eps} % leia abaixo
    \vspace{-1.0em}\caption{RMSE for each iteration of MS-KAI ESPRIT,original ESPRIT and CRB versus SNR with $P=4$ correlated sources, $M=40$, $N=25$, $L=200$ runs -magnification }
    \label{RMSE_dB_corr_letter_steps_reviewer1}
\end{figure}
%
%\excludecomment{figure}
%\let\endfigure\relax
%\begin{figure}[!h]
%
%    \centering % para centralizarmos a figura
%    \includegraphics[width=8cm, height=6cm]{RMSE_overlapping.eps} % leia abaixo
%    \vspace{-1.0em}\caption{Possibility of  overlapping of regions bounded by RMSEs resulting from estimates of adjacent DOAs}
%    \label{figura:RMSE_overlapping}
%\end{figure}

\section{Conclusions}

We have proposed the MS-KAI-ESPRIT algorithm which exploits the
knowledge of source signals obtained on line and the structure of
the covariance matrix and its perturbations. An analytical study of
the MSE of this matrix free of side effects has shown that it is
less or equal than the MSE of the original matrix, resulting in
better performance of MS-KAI-ESPRIT especially in scenarios where
limited number of samples are available. The proposed MS-KAI-ESPRIT
algorithm can obtain significant gains in RMSE or probability of
resolution performance over previously reported techniques, and has
excellent potential for applications with short data records in
large-scale antenna systems for wireless communications, radar and
other large sensor arrays. The relatively high computational burden
required, which is associated with extra matrix multiplications, the
increment $\iota$ applied to reduce the undesirable side effects and
the iterations needed to progressively incorporate the knowledge
obtained on line as newer estimates can be justified for the
superior performance achieved. Future work will consider approaches
to reducing the computational cost.

%\begin{itemize}
%    \color{blue}
%    \item[]\section*{Appendix}
%    \end{itemize}
%%\section*{Appendix}
%\label{Appendix}

\section*{{Appendix}}

{Here, we prove the inequality \eqref{problem_formulation2}
described in Section \ref{MSE_analysis}. We start by expressing the
MSE of the original data covariance matrix \eqref{covsample} as:}
%
\begin{align}
\mathrm{MSE}\left( \mathbf{\hat{R}}\right) =\mathbb E\left[\|\mathbf{\hat{R}}-
\mathbf{R}\|^2_F\right].
\label{problem_formulation3}
\end{align}
%
where $\mathbf{R} $ is the true covariance matrix .
%
Similarly, the  MSE of the data covariance matrix free of side effects $\mathbf{\hat{R}}^{(n+1)}$ can be expressed for the first iteration $n=1$ by making  use of  \eqref{modified_data_covariance}, as follows
%
\begin{align}
&\mathrm{MSE}\left( \mathbf{\hat{R}}^{(n+1)}\right)\big\rvert_{n=1}\nonumber\\&=\mathrm{MSE}\left( \mathbf{\hat{R}}^{(2)}\right)=\mathbb E\left[\|\mathbf{\hat{R}}^{(2)}-
\mathbf{R}\|^2_F\right]\nonumber\\
&=\mathbb E\left[\| \mathbf{\hat{R}}\:-\:\mathrm{\mu}\:(\mathbf{V}^{(1)}\:+\:\mathbf{V}^{(1)H})-
\mathbf{R}\|^2_F\right]\nonumber\\
&=\mathbb E\left[\|\left(  \mathbf{\hat{R}}-
\mathbf{R}\right) \:-\:\mathrm{\mu}\:(\mathbf{V}^{(1)}\:+\:\mathbf{V}^{(1)H})\|^2_F\right]
\label{MSE_eq1}
\end{align}
%
where for the sake of simplicity, from now on we omit the superscript $^{(1)}$, which refers to the first iteration.
In order to expand the result in \eqref{MSE_eq1}, we make use of the following proposition:

\medskip
\underline{Lemma 1}:
The squared Frobenius norm of the difference between any two matrices $\mathbf{A}$ $\in\mathbb{C}^{\mathit{m \times m}}$ and $\mathbf{B}$ $\in\mathbb{C}^{\mathit{m \times m}}$ is given by
\vspace{-0.5em}
\begin{align}
\|\mathbf{A}-
\mathbf{B}\|^2_F=\|\mathbf{A}\|^2_F +\|\mathbf{B}\|^2_F -\left( \Tr\mathbf{A}^{H}\mathbf{B} + \Tr\mathbf{A}\mathbf{B}^{H}\right)
\end{align}
%
\underline{Proof  of Lemma 1}:\\
The Frobenius norm of any $\mathbf{D}$ $\in$  $\mathbb{C}^{\mathit{m \times
        m}}$  matrix  is defined \cite{Vantrees} as \vspace{-1.00 em}
%
\begin{align}{
    \|\mathbf{D}\|_F=
    \left( \sum\limits_{i=1}^{m}\sum\limits_{j=1}^{m}\left| d_{ij}\right|^{2}\right )^\frac{1}{2}=\left[\Tr\left(\mathbf{ D}^{H}\mathbf{ D}\right)  \right]^{\frac{1}{2}}}
\label{Proof_Lemma_1}
\end{align}
%
We express $\mathbf{D} $ as a difference between  two  matrices $\mathbf{A}$ and  $\mathbf{B}$, both also $\in\mathbb{C}^{\mathit{m \times m}}$. Making use of Lemma1 and the properties of the trace, we obtain
%
{\begin{align}
    &\|\mathbf{A}-
    \mathbf{B}\|^2_F=  \Tr\left[\left( \mathbf{A}-\mathbf{B}\right)^{H} \left( \mathbf{A}-\mathbf{B}\right)\right]\nonumber\\
    &= \Tr\left[ \left( \mathbf{A}^{H}-\mathbf{B}^{H}\right) \left( \mathbf{A}-\mathbf{B}\right)\right]\nonumber\\
    &= \Tr\left[\left( \mathbf{A}^{H}\mathbf{A}\right)
    -\Tr\left( \mathbf{A}^{H}\mathbf{B}\right)- \Tr\left( \mathbf{B}^{H}\mathbf{A}\right)+\Tr\left( \mathbf{B}^{H}\mathbf{B}\right)\right]\nonumber\\
    &=\|\mathbf{A}\|^2_F +\|\mathbf{B}\|^2_F -\left( \Tr\mathbf{A}^{H}\mathbf{B} + \Tr\mathbf{A}\mathbf{B}^{H}\right),
    \end{align}}
which is the desired result.

Now, assuming that the
true $\mathbf{R}$ \cite{Haykin} and the  data covariance matrices  $\mathbf{\hat{R}} $ \cite{Haykin}  are Hermitian and  using \eqref{MSE_eq1}  combined with Lemma1, the cyclic \cite{Graybill} property of the trace  and the
linearity \cite{Karr} property of the expected value,  we get
%
\begin{align}
%\begin{split}
\mathrm{MSE}\left( \mathbf{\hat{R}}^{(2)}\right)& =\mathbb
E\left\lbrace \|\mathbf{\hat{R}}-
\mathbf{R}\|^2_F + \mathrm{\mu}^{2}\:\|\mathbf{V}\:+\:\mathbf{V}^{H}\|^2_F\right.\nonumber\\
&\left.-\Tr\left[ \left(\mathbf{\hat{R}}-
\mathbf{R}\right)^{H} \mu\left(\mathbf{V}\:+\:\mathbf{V}^{H} \right) \right]\right.\nonumber\\
& \left.-\Tr\left[\mu \left(\mathbf{V}+
\mathbf{V}^{H}\right)^{H} \left(\mathbf{\hat{R}}\:+\:\mathbf{R} \right)\right]\right\rbrace \nonumber\\
& = \mathbb E\left\lbrace \|\mathbf{\hat{R}}-
\mathbf{R}\|^2_F + \mathrm{\mu}^{2}\:\|\mathbf{V}\:+\:\mathbf{V}^{H}\|^2_F\right.\nonumber\\
&\left.-\mu\Tr\left[ \left(\mathbf{\hat{R}}-
\mathbf{R}\right)^{H} \left(\mathbf{V}\:+\:\mathbf{V}^{H} \right) \right]\right.\nonumber\\
& \left.-\mu\Tr\left[ \left(\mathbf{V}+
\mathbf{V}^{H}\right)^{H} \left(\mathbf{\hat{R}}\:+\:\mathbf{R} \right)\right]\right\rbrace \nonumber\\
&=\mathbb E\left\lbrace \|\mathbf{\hat{R}}-
\mathbf{R}\|^2_F + \mathrm{\mu}^{2}\:\|\mathbf{V}\:+\:\mathbf{V}^{H}\|^2_F\right.\nonumber\\
&\left.-\mu\Tr\left[ \left(\mathbf{\hat{R}}-
\mathbf{R}\right) \left(\mathbf{V}\:+\:\mathbf{V}^{H} \right) \right]\right.\nonumber\\
& \left.-\mu\Tr\left[  \left(\mathbf{V}^{H}+
\mathbf{V}\right)\left(\mathbf{\hat{R}}\:+\:\mathbf{R} \right)\right]\right\rbrace \nonumber\\
&=\mathbb E\left\lbrace \|\mathbf{\hat{R}}-
\mathbf{R}\|^2_F + \mathrm{\mu}^{2}\:\|\mathbf{V}\:+\:\mathbf{V}^{H}\|^2_F\right.\nonumber\\
&\left.-\mu\Tr\left[ \left(\mathbf{\hat{R}}-
\mathbf{R}\right) \left(\mathbf{V}\:+\:\mathbf{V}^{H} \right) \right]\right.\nonumber\\
& \left.-\mu\Tr\left[  \left(\mathbf{\hat{R}}\:+\:\mathbf{R} \right)\left(\mathbf{V}+
\mathbf{V}^{H}\right)\right]\right\rbrace\nonumber\\
&=\mathbb E\left\lbrace \|\mathbf{\hat{R}}-
\mathbf{R}\|^2_F\right\rbrace  +\mathrm{\mu}^{2} \mathbb E\left\lbrace\|\mathbf{V}\:+\:\mathbf{V}^{H}\|^2_F\right\rbrace \mathbb \nonumber\\
&-2\mu\mathbb E\left\lbrace \Tr\left[ \left(\mathbf{\hat{R}}-
\mathbf{R}\right) \left(\mathbf{V}\:+\:\mathbf{V}^{H} \right) \right]\right\rbrace\nonumber\\
&=\mathrm{MSE}\left( \mathbf{\hat{R}}\right) +\mathrm{\mu}^{2} \mathbb E\left\lbrace\|\mathbf{V}\:+\:\mathbf{V}^{H}\|^2_F\right\rbrace \mathbb \nonumber\\
&-2\mu\mathbb E\left\lbrace \Tr\left[ \left(\mathbf{\hat{R}}-
\mathbf{R}\right) \left(\mathbf{V}\:+\:\mathbf{V}^{H} \right) \right]\right\rbrace
\label{MSE_eq2}
%\end{split}
\end{align}
%
By moving the first summand of \eqref{MSE_eq2} to its first element,
we obtain the intended expression for the difference between the
$MSEs$ of the data covariance matrix free of perturbations and the
original one, i.e.:
%

\begin{align}
\mathrm{MSE}\left( \mathbf{\hat{R}}^{(n+1)}\right)\big\rvert_{n=1}-\mathrm{MSE}\left( \mathbf{\hat{R}}\right)=\mathrm{\mu}^{2} \mathbb E\left\lbrace\|\mathbf{V}+\mathbf{V}^{H}\|^2_F\right\rbrace \mathbb \nonumber\\
\quad -2\mu\mathbb E\left\lbrace \Tr\left[ \left(\mathbf{\hat{R}}-
\mathbf{R}\right) \left(\mathbf{V}\:+\:\mathbf{V}^{H} \right)
\right]\right\rbrace.
\label{Difference_MSEs}
\end{align}

%
Now, we  expand the expressions inside braces of the second member of \eqref{Difference_MSEs} individually. We start with the first summand
%

\begin{align}
\|\mathbf{V}+
\mathbf{V}^{H}\|^2_F& =\|\mathbf{V}\|^2_F +\|\mathbf{V}^{H}\|^2_F +\Tr\left(\mathbf{V}^{H}\mathbf{V}^{H} \right)+\nonumber\\
& \quad \Tr\left(\mathbf{(V^{\mathit{H}})}^{H}\mathbf{V} \right)\nonumber\\
&=\|\mathbf{V}\|^2_F +\|\mathbf{V}^{H}\|^2_F
+\Tr\left(\mathbf{V}^{H}\mathbf{V}^{H} \right)+
\Tr\left(\mathbf{V}\mathbf{V} \right).
\label{By_products}
\end{align}

%
The equation \eqref{By_products} can be computed by using the
projection matrices of the signal and the noise subspaces and the
data covariance matrix by using \eqref{terms_deducted},
\eqref{noise_projection}, the idempotence \cite{Vantrees} \cite{Graybill} of $\mathbf{\hat{Q}}_{A}$
and the cyclic property \cite{Graybill} of the trace. Starting with the computation
of its fourth summand, we have
%

\begin{align}
\Tr\left(\mathbf{V}\mathbf{V} \right)& = \Tr\left[\left( \mathbf{\hat{Q}}_{A}\:\mathbf{\hat{R}}\:\mathbf{\hat{Q}}_{A}^{\perp} \right)\left( \mathbf{\hat{Q}}_{A}\:\mathbf{\hat{R}}\:\mathbf{\hat{Q}}_{A}^{\perp} \right)\right] \nonumber\\
&= \Tr\left[  \mathbf{\hat{Q}}_{A}\:\mathbf{\hat{R}}\left(\mathbf{I_{M}}-\mathbf{\hat{Q}}_{A} \right)\: \mathbf{\hat{Q}}_{A}\:\mathbf{\hat{R}}\left(\mathbf{I_{M}}-\mathbf{\hat{Q}}_{A} \right) \right]\nonumber\\
&=\Tr\left[\left( \mathbf{\hat{Q}}_{A}\:\mathbf{\hat{R}}- \mathbf{\hat{Q}}_{A}\:\mathbf{\hat{R}}\:\mathbf{\hat{Q}}_{A}\right)\right.\nonumber\\
&\left.\left( \mathbf{\hat{Q}}_{A}\:\mathbf{\hat{R}}- \mathbf{\hat{Q}}_{A}\:\mathbf{\hat{R}}\:\mathbf{\hat{Q}}_{A}\right) \right]\nonumber\\
&= \Tr\left[\mathbf{\hat{Q}}_{A}\:\mathbf{\hat{R}}\:\mathbf{\hat{Q}}_{A}\:\mathbf{\hat{R}}  -\mathbf{\hat{Q}}_{A}\:\mathbf{\hat{R}}\:\mathbf{\hat{Q}}_{A}\:\mathbf{\hat{R}}\:\mathbf{\hat{Q}}_{A}\right.\nonumber\\
&\left.-\mathbf{\hat{Q}}_{A}\:\mathbf{\hat{R}}\:\mathbf{\hat{Q}}_{A}\mathbf{\hat{Q}}_{A}\:\mathbf{\hat{R}} +\mathbf{\hat{Q}}_{A}\:\mathbf{\hat{R}}\:\mathbf{\hat{Q}}_{A}\mathbf{\hat{Q}}_{A}\:\mathbf{\hat{R}}\:\mathbf{\hat{Q}}_{A}\right] \nonumber\\
&=\Tr\left(\mathbf{\hat{Q}}_{A}\:\mathbf{\hat{R}}\:\mathbf{\hat{Q}}_{A}\:\mathbf{\hat{R}}\right)   -\Tr\left(\mathbf{\hat{Q}}_{A}\:\mathbf{\hat{R}}\:\mathbf{\hat{Q}}_{A}\:\mathbf{\hat{R}}\:\mathbf{\hat{Q}}_{A}\right) \nonumber\\
&-\Tr\left(\mathbf{\hat{Q}}_{A}\:\mathbf{\hat{R}}\:\mathbf{\hat{Q}}_{A}\mathbf{\hat{Q}}_{A}\:\mathbf{\hat{R}}\right)\nonumber\\  & +\Tr\left(\mathbf{\hat{Q}}_{A}\:\mathbf{\hat{R}}\:\mathbf{\hat{Q}}_{A}\mathbf{\hat{Q}}_{A}\:\mathbf{\hat{R}}\:\mathbf{\hat{Q}}_{A}\right)\nonumber\\
&= \Tr\left(\mathbf{\hat{Q}}_{A}\:\mathbf{\hat{R}}\:\mathbf{\hat{Q}}_{A}\:\mathbf{\hat{R}}\right)-\Tr\left(\mathbf{\hat{Q}}_{A}\:\mathbf{\hat{R}}\:\mathbf{\hat{Q}}_{A}\:\mathbf{\hat{R}}\right)\nonumber\\
&-\Tr\left(\mathbf{\hat{Q}}_{A}\:\mathbf{\hat{R}}\:\mathbf{\hat{Q}}_{A}\:\mathbf{\hat{R}}\right)+\Tr\left(\mathbf{\hat{Q}}_{A}\:\mathbf{\hat{R}}\:\mathbf{\hat{Q}}_{A}\:\mathbf{\hat{R}}\right)=0.
\label{Trace_VV}
\end{align}
%
Taking into account that the data covariance matrix $ \mathbf{\hat{R}} $ and the estimate of the projection matrix of the noise subspace  $ \mathbf{\hat{Q}}_{A}^{\perp} $ are Hermitian, we can evaluate the third summand of \eqref{By_products}  as follows:
%
\begin{align}
&\Tr\left(\mathbf{V}^{H}\mathbf{V}^{H} \right)= \Tr\left[\left( \mathbf{\hat{Q}}_{A}\:\mathbf{\hat{R}}\:\mathbf{\hat{Q}}_{A}^{\perp} \right)^{H}\left( \mathbf{\hat{Q}}_{A}\:\mathbf{\hat{R}}\:\mathbf{\hat{Q}}_{A}^{\perp} \right)^{H}\right] \nonumber\\
&= \Tr\left\lbrace \left[ \left( \mathbf{\hat{Q}}_{A}^{\perp}\right)^{H}  \:\mathbf{\hat{R}}^{H}\:\mathbf{\hat{Q}}_{A}^{H} \right] \left[ \left( \mathbf{\hat{Q}}_{A}^{\perp}\right)^{H}  \:\mathbf{\hat{R}}^{H}\:\mathbf{\hat{Q}}_{A}^{H} \right] \right\rbrace \nonumber\\
&= \Tr\left\lbrace \left[ \mathbf{\hat{Q}}_{A}^{\perp} \:\mathbf{\hat{R}}\:\mathbf{\hat{Q}}_{A} \right] \left[ \mathbf{\hat{Q}}_{A}^{\perp} \:\mathbf{\hat{R}}\:\mathbf{\hat{Q}}_{A} \right] \right\rbrace  \nonumber\\
&= \Tr\left\lbrace \left[  \mathbf{\left(I_{M}-\mathbf{\hat{Q}}_{A} \right)\mathbf{\hat{R}}\:\hat{Q}}_{A}\right] \left[  \:\mathbf{\left(I_{M}-\mathbf{\hat{Q}}_{A} \right)\mathbf{\hat{R}}\:\hat{Q}}_{A} \right]\right\rbrace \nonumber\\
&=\Tr\left\lbrace \left[ \mathbf{\hat{R}}\:\mathbf{\hat{Q}}_{A}- \mathbf{\hat{Q}}_{A}\:\mathbf{\hat{R}}\:\mathbf{\hat{Q}}_{A}\right]\left[ \mathbf{\hat{R}}\:\mathbf{\hat{Q}}_{A}- \mathbf{\hat{Q}}_{A}\:\mathbf{\hat{R}}\:\mathbf{\hat{Q}}_{A}\right] \right\rbrace
\nonumber\\
&=\Tr\left\lbrace \mathbf{\hat{R}}\:\mathbf{\hat{Q}}_{A}\:\mathbf{\hat{R}}\:\mathbf{\hat{Q}}_{A}  -\mathbf{\hat{R}}\:\mathbf{\hat{Q}}_{A}\mathbf{\hat{Q}}_{A}\:\:\mathbf{\hat{R}}\:\mathbf{\hat{Q}}_{A}\right.\nonumber\\
&\left.-\mathbf{\hat{Q}}_{A}\:\mathbf{\hat{R}}\:\mathbf{\hat{Q}}_{A}\:\mathbf{\hat{R}}\:\mathbf{\hat{Q}}_{A} +\mathbf{\hat{Q}}_{A}\:\mathbf{\hat{R}}\:\mathbf{\hat{Q}}_{A}\mathbf{\hat{Q}}_{A}\:\mathbf{\hat{R}}\:\mathbf{\hat{Q}}_{A}\right\rbrace  \nonumber\\
&=\Tr\left(\mathbf{\hat{R}}\:\mathbf{\hat{Q}}_{A}\:\mathbf{\hat{R}}\:\mathbf{\hat{Q}}_{A}\:\right)   -\Tr\left(\mathbf{\hat{R}}\:\mathbf{\hat{Q}}_{A}\mathbf{\hat{Q}}_{A}\:\mathbf{\hat{R}}\:\mathbf{\hat{Q}}_{A}\right) \nonumber\\
&-\Tr\left(\mathbf{\hat{Q}}_{A}\:\mathbf{\hat{R}}\:\mathbf{\hat{Q}}_{A}\:\mathbf{\hat{R}}\:\mathbf{\hat{Q}}_{A}\right)  +\Tr\left(\mathbf{\hat{Q}}_{A}\:\mathbf{\hat{R}}\:\mathbf{\hat{Q}}_{A}\mathbf{\hat{Q}}_{A}\:\mathbf{\hat{R}}\:\mathbf{\hat{Q}}_{A}\right)\nonumber\\
&= \Tr\left(\mathbf{\hat{R}}\:\mathbf{\hat{Q}}_{A}\:\mathbf{\hat{R}}\:\mathbf{\hat{Q}}_{A}\right)-\Tr\left(\mathbf{\hat{R}}\:\mathbf{\hat{Q}}_{A}\:\mathbf{\hat{R}}\mathbf{\hat{Q}}_{A}\right)\nonumber\\
&-\Tr\left(\mathbf{\hat{Q}}_{A}\:\mathbf{\hat{R}}\:\mathbf{\hat{Q}}_{A}\:\mathbf{\hat{R}}\right)+\Tr\left(\mathbf{\hat{Q}}_{A}\:\mathbf{\hat{R}}\:\mathbf{\hat{Q}}_{A}\:\mathbf{\hat{R}}\right)=0.
\label{Trace_VHVH}
\end{align}
%
By using \eqref{Proof_Lemma_1}, we can expand the first and the second summands  of
\eqref{By_products} as follows:
%
\begin{align}
&\|\mathbf{V}\|^2_F +\|\mathbf{V}^{H}\|^2_F =\Tr\left(\mathbf{V}^{H}\mathbf{V} \right)+\Tr\left(\left( \mathbf{V}^{H}\right)^{H} \mathbf{V}^{H} \right)\nonumber\\
&= \Tr\left(\mathbf{V}^{H}\mathbf{V} \right)+\Tr\left(\mathbf{V}\mathbf{V}^{H} \right)\nonumber\\
&=\Tr\left(\mathbf{V}\mathbf{V}^{H}
\right)+\Tr\left(\mathbf{V}\mathbf{V}^{H}
\right)=2\Tr\left(\mathbf{V}\mathbf{V}^{H} \right).
\label{Sq_Frob_norm_of_VmaisVH}
\end{align}
%
Equation \eqref{Sq_Frob_norm_of_VmaisVH} can be  expressed in terms
of the projection matrices of the signal and the noise subspaces and
the data covariance, in a similar way as for the third and fourth
summands of \eqref{By_products}, as follows:
%
\begin{align}
& 2\Tr\left(\mathbf{V}\mathbf{V}^{H} \right)=2\Tr\left[ \left( \mathbf{\hat{Q}}_{A}\:\mathbf{\hat{R}}\:\mathbf{\hat{Q}}_{A}^{\perp} \right) \left( \mathbf{\hat{Q}}_{A}\:\mathbf{\hat{R}}\:\mathbf{\hat{Q}}_{A}^{\perp} \right)^{H}\right]\nonumber\\
&=2\Tr\left\lbrace\mathbf{\hat{Q}}_{A}\mathbf{\hat{R}} \left(\mathbf{ I_{M} }-\mathbf{\hat{Q}}_{A}  \right) \left[ \mathbf{\hat{Q}}_{A}\mathbf{\hat{R}} \left(\mathbf{ I_{M} }-\mathbf{\hat{Q}}_{A} \right)\right]^{H} \right\rbrace \nonumber\\
&=2\Tr\left\lbrace\left(\mathbf{\hat{Q}}_{A}\mathbf{\hat{R}}- \mathbf{\hat{Q}}_{A}\mathbf{\hat{R}}\mathbf{\hat{Q}}_{A}\right)\left(\mathbf{\hat{Q}}_{A}\mathbf{\hat{R}}- \mathbf{\hat{Q}}_{A}\mathbf{\hat{R}}\mathbf{\hat{Q}}_{A}\right)^{H}   \right\rbrace \nonumber\\
&=2\Tr\left\lbrace\mathbf{\hat{Q}}_{A}\mathbf{\hat{R}}\mathbf{\hat{R}}\mathbf{\hat{Q}}_{A}- \mathbf{\hat{Q}}_{A}\mathbf{\hat{R}}\mathbf{\hat{Q}}_{A}\mathbf{\hat{R}}\right.\nonumber\\
&\left.- \mathbf{\hat{Q}}_{A}\mathbf{\hat{R}}\mathbf{\hat{Q}}_{A}\mathbf{\hat{R}}\mathbf{\hat{Q}}_{A}+ \mathbf{\hat{Q}}_{A}\mathbf{\hat{R}}\mathbf{\hat{Q}}_{A}\mathbf{\hat{Q}}_{A}\mathbf{\hat{R}} \right\rbrace \nonumber\\
&=2\left\lbrace\Tr\left(\mathbf{\hat{Q}}_{A}\mathbf{\hat{R}}\mathbf{\hat{R}}\mathbf{\hat{Q}}_{A} \right) -\Tr\left(\mathbf{\hat{Q}}_{A}\mathbf{\hat{R}}\mathbf{\hat{Q}}_{A}\mathbf{\hat{R}} \right)  \right.\nonumber\\
&\left.-\Tr\left( \mathbf{\hat{Q}}_{A}\mathbf{\hat{R}}\mathbf{\hat{Q}}_{A}\mathbf{\hat{R}}\mathbf{\hat{Q}}_{A}\right)  +\Tr\left(\mathbf{\hat{Q}}_{A}\mathbf{\hat{R}}\mathbf{\hat{Q}}_{A}\mathbf{\hat{Q}}_{A}\mathbf{\hat{R}} \right)   \right\rbrace \nonumber\\
&=2\left\lbrace\Tr\left(\mathbf{\hat{Q}}_{A}\mathbf{\hat{Q}}_{A}\mathbf{\hat{R}}\mathbf{\hat{R}} \right) -\Tr\left(\mathbf{\hat{Q}}_{A}\mathbf{\hat{R}}\mathbf{\hat{Q}}_{A}\mathbf{\hat{R}} \right)  \right.\nonumber\\
&\left.-\Tr\left( \mathbf{\hat{Q}}_{A}\mathbf{\hat{R}}\mathbf{\hat{Q}}_{A}\mathbf{\hat{R}}\right)  +\Tr\left(\mathbf{\hat{Q}}_{A}\mathbf{\hat{R}}\mathbf{\hat{Q}}_{A}\mathbf{\hat{R}} \right)   \right\rbrace \nonumber\\
&=2\left\lbrace\Tr\left(\mathbf{\hat{Q}}_{A}\mathbf{\hat{Q}}_{A}\mathbf{\hat{R}}\mathbf{\hat{R}} \right) -\Tr\left(\mathbf{\hat{Q}}_{A}\mathbf{\hat{R}}\mathbf{\hat{Q}}_{A}\mathbf{\hat{R}} \right)\right\rbrace
\label{Double_tr_V_mais_VH}
\end{align}
%
From \eqref{By_products}, \eqref{Trace_VV}, \eqref{Trace_VHVH}, \eqref{Sq_Frob_norm_of_VmaisVH} and \eqref{Double_tr_V_mais_VH},  we obtain the first summand of \eqref{Difference_MSEs}, as follows:
%
\begin{align}
\mathrm{\mu}^{2} \mathbb {E}\left\lbrace\|\mathbf{V}+\mathbf{V}^{H}\|^2_F\right\rbrace& =\mathrm{2\mu}^{2} \mathbb{E}\left\lbrace\Tr\left(\mathbf{\hat{Q}}_{A}\mathbf{\hat{Q}}_{A}\mathbf{\hat{R}}\mathbf{\hat{R}} \right) \right.\nonumber\\
&\left.-\Tr\left(\mathbf{\hat{Q}}_{A}\mathbf{\hat{R}}\mathbf{\hat{Q}}_{A}\mathbf{\hat{R}} \right)\right\rbrace
\label{First_summ_Difference_MSEs_key}
\end{align}
%
In order to finish the expansion of the  expressions inside braces of the second member of \eqref{Difference_MSEs}, now  we deal with its second summand, in which we make use of the cyclic property \cite{Graybill} of the trace and the idempotence \cite{Vantrees} \cite{Graybill} of $\mathbf{\hat{Q}}_{A}$.

%
\begin{align}
& \Tr\left[\left( \mathbf{\hat{R}}-\mathbf{R}\right)  \left( \mathbf{V}+\mathbf{V}^{H}\right) \right] =\left\lbrace \Tr\left( \mathbf{\hat{R}}-\mathbf{R}\right)\right.\nonumber\\
&\left.\left[  \mathbf{\hat{Q}}_{A}\:\mathbf{\hat{R}}\:\mathbf{\hat{Q}}_{A}^{\perp}  + \left( \mathbf{\hat{Q}}_{A}\:\mathbf{\hat{R}}\:\mathbf{\hat{Q}}_{A}^{\perp} \right)^{H}\right]\right\rbrace \nonumber\\
&=\Tr\left\lbrace\left( \mathbf{\hat{R}}-\mathbf{R}\right)\left[ \mathbf{\hat{Q}}_{A}\mathbf{\hat{R}} \left(\mathbf{ I_{M} }-\mathbf{\hat{Q}}_{A} \right)\right.\right.\nonumber\\
&\left.\left.+ \left(  \mathbf{\hat{Q}}_{A}\mathbf{\hat{R}} \left(\mathbf{ I_{M} }-\mathbf{\hat{Q}}_{A}  \right)\right)^{H} \right] \right\rbrace \nonumber\\
&=\Tr\left\lbrace\left( \mathbf{\hat{R}}-\mathbf{R}\right)\left[\mathbf{\hat{Q}}_{A}\mathbf{\hat{R}}-\mathbf{\hat{Q}}_{A}\mathbf{\hat{R}}\mathbf{\hat{Q}}_{A}\right.\right.\nonumber\\
&\left.\left.+\left( \mathbf{\hat{Q}}_{A}\mathbf{\hat{R}}-\mathbf{\hat{Q}}_{A}\mathbf{\hat{R}}\mathbf{\hat{Q}}_{A}\right)^{H}  \right]  \right\rbrace \nonumber\\
&=\Tr\left\lbrace\left( \mathbf{\hat{R}}-\mathbf{R}\right)\left[\mathbf{\hat{Q}}_{A}\mathbf{\hat{R}}-\mathbf{\hat{Q}}_{A}\mathbf{\hat{R}}\mathbf{\hat{Q}}_{A}+\mathbf{\hat{R}\mathbf{\hat{Q}}_{A}}-\mathbf{\hat{Q}}_{A}\mathbf{\hat{R}}\mathbf{\hat{Q}}_{A} \right]  \right\rbrace \nonumber\\
&=\Tr\left\lbrace \mathbf{\hat{R}}\mathbf{\hat{Q}}_{A}\mathbf{\hat{R}}+\mathbf{\hat{R}}\mathbf{\hat{R}\mathbf{\hat{Q}}_{A}}-2\mathbf{\hat{R}}\mathbf{\hat{Q}}_{A}\mathbf{\hat{R}}\mathbf{\hat{Q}}_{A} \right.\nonumber\\
&\left.-\mathbf{R}\mathbf{\hat{Q}}_{A}\mathbf{\hat{R}}-\mathbf{R}\mathbf{\hat{R}\mathbf{\hat{Q}}_{A}}+2\mathbf{R}\mathbf{\hat{Q}}_{A}\mathbf{\hat{R}}\mathbf{\hat{Q}}_{A}\right\rbrace \nonumber\\
&=\Tr \mathbf{\hat{R}}\mathbf{\hat{Q}}_{A}\mathbf{\hat{R}}+\Tr\mathbf{\hat{R}}\mathbf{\hat{R}\mathbf{\hat{Q}}_{A}}-2\Tr\mathbf{\hat{R}}\mathbf{\hat{Q}}_{A}\mathbf{\hat{R}}\mathbf{\hat{Q}}_{A} \nonumber\\
&-\Tr\mathbf{R}\mathbf{\hat{Q}}_{A}\mathbf{\hat{R}}-\Tr\mathbf{R}\mathbf{\hat{R}\mathbf{\hat{Q}}_{A}}+2\Tr\mathbf{R}\mathbf{\hat{Q}}_{A}\mathbf{\hat{R}}\mathbf{\hat{Q}}_{A}\nonumber\\
&=\Tr\mathbf{\hat{Q}}_{A}\mathbf{\hat{R}}\mathbf{\hat{R}}+\Tr\mathbf{\hat{Q}}_{A}\mathbf{\hat{R}}\mathbf{\hat{R}}-2\Tr\mathbf{\hat{Q}}_{A}\mathbf{\hat{R}}\mathbf{\hat{Q}}_{A}\mathbf{\hat{R}} \nonumber\\
&-\Tr\mathbf{R}\mathbf{\hat{Q}}_{A}\mathbf{\hat{R}}-\Tr\mathbf{\hat{Q}}_{A}\mathbf{R}\mathbf{\hat{R}}+2\Tr\mathbf{\hat{Q}}_{A}\mathbf{R}\mathbf{\hat{Q}}_{A}\mathbf{\hat{R}}\nonumber\\
&=2\Tr\mathbf{\hat{Q}}_{A}\mathbf{\hat{R}}\mathbf{\hat{R}}-2\Tr\mathbf{\hat{Q}}_{A}\mathbf{\hat{R}}\mathbf{\hat{Q}}_{A}\mathbf{\hat{R}}-\Tr\mathbf{R}\mathbf{\hat{Q}}_{A}\mathbf{\hat{R}} \nonumber\\
&-\Tr\mathbf{\hat{Q}}_{A}\mathbf{R}\mathbf{\hat{R}}+2\Tr\mathbf{\hat{Q}}_{A}\mathbf{R}\mathbf{\hat{Q}}_{A}\mathbf{\hat{R}}\nonumber\\
&=2\Tr\mathbf{\hat{Q}}_{A}\mathbf{\hat{Q}}_{A}\mathbf{\hat{R}}\mathbf{\hat{R}}-2\Tr\mathbf{\hat{Q}}_{A}\mathbf{\hat{R}}\mathbf{\hat{Q}}_{A}\mathbf{\hat{R}}-\Tr\mathbf{R}\mathbf{\hat{Q}}_{A}\mathbf{\hat{Q}}_{A}\mathbf{\hat{R}} \nonumber\\
&-\Tr\mathbf{\hat{Q}}_{A}\mathbf{\hat{Q}}_{A}\mathbf{R}\mathbf{\hat{R}}+2\Tr\mathbf{\hat{Q}}_{A}\mathbf{R}\mathbf{\hat{Q}}_{A}\mathbf{\hat{R}}
\label{tr_Rsample_R_V_mais_VH}
\end{align}
%
By using \eqref{tr_Rsample_R_V_mais_VH}, we can straightforwardly write
the second summand of the second member of \eqref{Difference_MSEs} in terms of the projection matrices of the signal and the noise subspaces and the data covariance matrix as follows:
%
\begin{align}
&-2\mu\mathbb E\left\lbrace \Tr\left[ \left(\mathbf{\hat{R}}-
\mathbf{R}\right) \left(\mathbf{V}\:+\:\mathbf{V}^{H} \right) \right]\right\rbrace
\nonumber\\
&=-2\mu\mathbb E\left\lbrace 2\Tr\mathbf{\hat{Q}}_{A}\mathbf{\hat{Q}}_{A}\mathbf{\hat{R}}\mathbf{\hat{R}}-2\Tr\mathbf{\hat{Q}}_{A}\mathbf{\hat{R}}\mathbf{\hat{Q}}_{A}\mathbf{\hat{R}}-\Tr\mathbf{R}\mathbf{\hat{Q}}_{A}\mathbf{\hat{Q}}_{A}\mathbf{\hat{R}}\right. \nonumber\\
&\left.-\Tr\mathbf{\hat{Q}}_{A}\mathbf{\hat{Q}}_{A}\mathbf{R}\mathbf{\hat{R}}+2\Tr\mathbf{\hat{Q}}_{A}\mathbf{R}\mathbf{\hat{Q}}_{A}\mathbf{\hat{R}} \right\rbrace\nonumber\\
&= -4\mu\mathbb E\left\lbrace \Tr\mathbf{\hat{Q}}_{A}\mathbf{\hat{Q}}_{A}\mathbf{\hat{R}}\mathbf{\hat{R}}-\Tr\mathbf{\hat{Q}}_{A}\mathbf{\hat{R}}\mathbf{\hat{Q}}_{A}\mathbf{\hat{R}}\right\rbrace\nonumber\\
& -2\mu\left\lbrace-\Tr\mathbb E\left[ \mathbf{R}\mathbf{\hat{Q}}_{A}\mathbf{\hat{Q}}_{A}\mathbf{\hat{R}}\right]-\Tr\mathbb E\left[ \mathbf{\hat{Q}}_{A}\mathbf{\hat{Q}}_{A}\mathbf{R}\mathbf{\hat{R}}  \right]\right. \nonumber\\
&\left.+2\Tr\mathbb E\left[ \mathbf{\hat{Q}}_{A}\mathbf{R}\mathbf{\hat{Q}}_{A}\mathbf{\hat{R}}  \right]    \right\rbrace \nonumber\\
&= -4\mu\mathbb E\left\lbrace \Tr\mathbf{\hat{Q}}_{A}\mathbf{\hat{Q}}_{A}\mathbf{\hat{R}}\mathbf{\hat{R}}-\Tr\mathbf{\hat{Q}}_{A}\mathbf{\hat{R}}\mathbf{\hat{Q}}_{A}\mathbf{\hat{R}}\right\rbrace\nonumber\\
& -2\mu\left\lbrace-\Tr\mathbb \mathbf{R}\mathbf{\hat{Q}}_{A}\mathbf{\hat{Q}}_{A} \mathbb E\left[\mathbf{\hat{R}}\right]-\Tr \mathbf{\hat{Q}}_{A}\mathbf{\hat{Q}}_{A}\mathbf{R}\mathbb E\left[\mathbf{\hat{R}}  \right]\right. \nonumber\\
&\left.+2\Tr\mathbf{\hat{Q}}_{A}\mathbf{R}\mathbf{\hat{Q}}_{A} \mathbb E\left[\mathbf{\hat{R}}\right]    \right\rbrace
\label{Sec_summ_Difference_MSEs_key}
\end{align}
%
Now, by using \eqref{First_summ_Difference_MSEs_key} and
\eqref{Sec_summ_Difference_MSEs_key}, and assuming that $\mathbb
E\left[\mathbf{\hat{R}}\right] $ is an unbiased estimate of
$\mathbf{\hat{R}}$, i.e., $\mathbb E\left[\mathbf{\hat{R}}\right]
=\mathbf{R}$, we can rewrite \eqref{Difference_MSEs} as follows:
\begin{align}
&\mathrm{MSE}\left( \mathbf{\hat{R}}^{(n+1)}\right)\big\rvert_{n=1}-\mathrm{MSE}\left( \mathbf{\hat{R}}\right)=\mathrm{\mu}^{2} \mathbb E\left\lbrace\|\mathbf{V}+\mathbf{V}^{H}\|^2_F\right\rbrace \mathbb \nonumber\\
&-2\mu\mathbb E\left\lbrace \Tr\left[ \left(\mathbf{\hat{R}}-
\mathbf{R}\right) \left(\mathbf{V}\:+\:\mathbf{V}^{H} \right) \right]\right\rbrace \nonumber\\
&=\mathrm{2\mu}^{2} \mathbb{E}\left\lbrace\Tr\mathbf{\hat{Q}}_{A}\mathbf{\hat{Q}}_{A}\mathbf{\hat{R}}\mathbf{\hat{R}}  -\Tr\mathbf{\hat{Q}}_{A}\mathbf{\hat{R}}\mathbf{\hat{Q}}_{A}\mathbf{\hat{R}} \right\rbrace\nonumber\\
&-4\mu\mathbb E\left\lbrace \Tr\mathbf{\hat{Q}}_{A}\mathbf{\hat{Q}}_{A}\mathbf{\hat{R}}\mathbf{\hat{R}}-\Tr\mathbf{\hat{Q}}_{A}\mathbf{\hat{R}}\mathbf{\hat{Q}}_{A}\mathbf{\hat{R}}\right\rbrace\nonumber\\
& -2\mu\left\lbrace-\Tr \mathbf{R}\mathbf{\hat{Q}}_{A}\mathbf{\hat{Q}}_{A} \mathbf{R}-\Tr\mathbf{\hat{Q}}_{A}\mathbf{\hat{Q}}_{A}\mathbf{R}\mathbf{R}\right. \nonumber\\
&\left.+2\Tr\mathbf{\hat{Q}}_{A}\mathbf{R}\mathbf{\hat{Q}}_{A} \mathbf{R}    \right\rbrace\nonumber\\
&=\mathrm{2\mu}^{2} \mathbb{E}\left\lbrace\Tr\mathbf{\hat{Q}}_{A}\mathbf{\hat{Q}}_{A}\mathbf{\hat{R}}\mathbf{\hat{R}}  -\Tr\mathbf{\hat{Q}}_{A}\mathbf{\hat{R}}\mathbf{\hat{Q}}_{A}\mathbf{\hat{R}} \right\rbrace\nonumber\\
&-4\mu\mathbb E\left\lbrace \Tr\mathbf{\hat{Q}}_{A}\mathbf{\hat{Q}}_{A}\mathbf{\hat{R}}\mathbf{\hat{R}}-\Tr\mathbf{\hat{Q}}_{A}\mathbf{\hat{R}}\mathbf{\hat{Q}}_{A}\mathbf{\hat{R}}\right\rbrace\nonumber\\
& -2\mu\left\lbrace-2\Tr \mathbf{R}\mathbf{\hat{Q}}_{A}\mathbf{\hat{Q}}_{A} \mathbf{R} +2\Tr\mathbf{\hat{Q}}_{A}\mathbf{R}\mathbf{\hat{Q}}_{A} \mathbf{R}    \right\rbrace\nonumber\\
&=\mathrm{2\mu}^{2} \mathbb{E}\left\lbrace\Tr\mathbf{\hat{Q}}_{A}\mathbf{\hat{Q}}_{A}\mathbf{\hat{R}}\mathbf{\hat{R}}  -\Tr\mathbf{\hat{Q}}_{A}\mathbf{\hat{R}}\mathbf{\hat{Q}}_{A}\mathbf{\hat{R}} \right\rbrace\nonumber\\
&-4\mu\mathbb E\left\lbrace \Tr\mathbf{\hat{Q}}_{A}\mathbf{\hat{Q}}_{A}\mathbf{\hat{R}}\mathbf{\hat{R}}-\Tr\mathbf{\hat{Q}}_{A}\mathbf{\hat{R}}\mathbf{\hat{Q}}_{A}\mathbf{\hat{R}}\right\rbrace\nonumber\\
& -4\mu\left\lbrace\Tr \mathbf{\hat{Q}}_{A}\mathbf{\hat{Q}}_{A} \mathbf{R}\mathbf{R} -\Tr\mathbf{\hat{Q}}_{A}\mathbf{R}\mathbf{\hat{Q}}_{A} \mathbf{R} \right\rbrace\nonumber\\
&=\left( \mathrm{2\mu}^{2}-\mathrm{4\mu}\right)  \mathbb{E}\left\lbrace\Tr\mathbf{\hat{Q}}_{A}\mathbf{\hat{Q}}_{A}\mathbf{\hat{R}}\mathbf{\hat{R}}  -\Tr\mathbf{\hat{Q}}_{A}\mathbf{\hat{R}}\mathbf{\hat{Q}}_{A}\mathbf{\hat{R}} \right\rbrace\nonumber\\
& -4\mu\left\lbrace\Tr \mathbf{\hat{Q}}_{A}\mathbf{\hat{Q}}_{A} \mathbf{R}\mathbf{R} -\Tr\mathbf{\hat{Q}}_{A}\mathbf{R}\mathbf{\hat{Q}}_{A} \mathbf{R} \right\rbrace
\label{Difference_MSEs_final}
\end{align}
%
Next, we will discuss equation \eqref{Difference_MSEs_final}. For
this purpose, we assume that the estimate of the projection matrix
of the signal subspace  $\mathbf{\hat{Q}}_{A}$ \cite{Vantrees}, the
true $\mathbf{R}$ \cite{Haykin} and the  data covariance matrices  $
\mathbf{\hat{R}} $ \cite{Haykin}  are Hermitian. For the next steps
we will make use of the following Theorem which is proved in
\cite{Chang}:

\medskip

\underline{Theorem 1}:
For two Hermitian matrices $\mathbf{A}$ and $\mathbf{B}$ of the same order,
\vspace{-0.5em}
\begin{align}
\label{Theorem_1}
& \Tr\left(\mathbf{A}\mathbf{B} \right)^{2^{k}}\leq \Tr\left(\mathbf{A}^{2^{k}}\mathbf{B}^{2^{k}} \right),
\end{align}
%
where k is in integer.\\

By replacing $\mathbf{A}$ with $\mathbf{\hat{Q}}_{A}$ and $\mathbf{B}$ with $\mathbf{\hat{R}}$ in \eqref{Theorem_1} and also considering $ k=1 $ , we have
%
\begin{align}
& \Tr\left(\mathbf{\hat{Q}}_{A}\mathbf{\hat{R}} \right)^{2}\leq \Tr\left(\mathbf{\hat{Q}}_{A}^{2}\mathbf{\hat{R}}^{2} \right)\nonumber\\
&\therefore\Tr\mathbf{\hat{Q}}_{A}\mathbf{\hat{R}} \mathbf{\hat{Q}}_{A}\mathbf{\hat{R}}\leq \Tr\mathbf{\hat{Q}}_{A}\mathbf{\hat{Q}}_{A}\mathbf{\hat{R}}\mathbf{\hat{R}}\nonumber\\
&\Rightarrow \Tr\mathbf{\hat{Q}}_{A}\mathbf{\hat{Q}}_{A}\mathbf{\hat{R}}\mathbf{\hat{R}}-\Tr\mathbf{\hat{Q}}_{A}\mathbf{\hat{R}} \mathbf{\hat{Q}}_{A}\mathbf{\hat{R}}\geq0
\label{Inequality_2}
\end{align}

Similarly, making $\mathbf{A}=\mathbf{\hat{Q}}_{A}$ and $\mathbf{B}=\mathbf{R}$ for $ k=1 $,  we obtain
%
\begin{align}
& \Tr\left(\mathbf{\hat{Q}}_{A}\mathbf{R} \right)^{2}\leq \Tr\left(\mathbf{\hat{Q}}_{A}^{2}\mathbf{R}^{2} \right)\nonumber\\
&\therefore\Tr\mathbf{\hat{Q}}_{A}\mathbf{R} \mathbf{\hat{Q}}_{A}\mathbf{R}\leq \Tr\mathbf{\hat{Q}}_{A}\mathbf{\hat{Q}}_{A}\mathbf{R}\mathbf{R}\nonumber\\&\Rightarrow \Tr\mathbf{\hat{Q}}_{A}\mathbf{\hat{Q}}_{A}\mathbf{R}\mathbf{R}-\Tr\mathbf{\hat{Q}}_{A}\mathbf{R} \mathbf{\hat{Q}}_{A}\mathbf{R}\geq0
\label{Inequality_3}
\end{align}
%
Next, we analyze the behavior of the expressions $-4\mu$ and
$\left(\mathrm{2\mu}^{2}-\mathrm{4\mu}\right)$ based on the
reliability factor $\mu $  $\in$ $[0\;1]$, as defined in
\eqref{modified_data_covariance}. In order to illustrate the case
being studied, we assume that both expressions are continuous
functions as depicted in Fig. \ref{Behavior_reliability_factor4}.
%
\begin{figure}[!h]
    \centering % para centralizarmos a figura
    \includegraphics[width=8cm,height=6cm]{fig7.eps} % leia abaixo
    \vspace{-1.0em}\caption{Behavior of $\left(\mathrm{2\mu}^{2}-\mathrm{4\mu}\right)$ and $-4\mu$ for $\mu$ $\in$ $[0\;1]$}
    \label{Behavior_reliability_factor4}
\end{figure}
%
It can be seen in it that in the range $ [0\;1] $ both expressions assume values $\mathrm{f}(\mu)\leq0 $, i.e.:
%
\begin{align}
\mathrm{For}\:\mu\in[0\;1]: \left \{
\begin{aligned}
&\left(\mathrm{2\mu}^{2}-\mathrm{4\mu}\right)\leq0 \\
&-4\mu\leq0
\end{aligned} \right.
\label{Mu_conditions}
\end{align}
%
Now, we can consider the  traces which form the subtraction in  \eqref{Inequality_2} as different random variables $ \mathit{y}\left(\omega \right) $ and $ \mathit{x}\left(\omega \right) $, i.e.:
%
\begin{align}
\begin{aligned}
&\left.
\begin{aligned}
&\Tr\mathbf{\hat{Q}}_{A}\mathbf{\hat{Q}}_{A}\mathbf{\hat{R}}\mathbf{\hat{R}}=\mathit{y}\left(\omega \right)\\
&\Tr\mathbf{\hat{Q}}_{A}\mathbf{\hat{R}}\mathbf{\hat{Q}}_{A}\mathbf{\hat{R}}=\mathit{x}\left(\omega \right)
\end{aligned}
\right \},\:\forall\: \omega\:\in\Omega.
\end{aligned}
\label{Monotonicity_conditions_traces}
\end{align}
In addition, we can suppose that there is a random variable $
\mathit{z}\left(\omega \right) $ always greater than zero, i.e., $
\mathit{z}\left(\omega \right)\geq0 $, so that
\begin{align}
\mathit{z}\left(\omega \right) =\mathit{y}\left(\omega \right)-\mathit{x}\left(\omega \right)\geq0,\;\forall\: \omega\:\in\Omega
\label{Monotonicity_conditions_rv}
\end{align}
Taking the expectation of \eqref{Monotonicity_conditions_rv} and
applying its properties of linearity and monotonicity
\cite{Karr,JM}, we obtain
%
\begin{align}
&\mathbb{E}\left[  \mathit{z}\left(\omega \right)\right]
=\mathbb{E}\left[  \mathit{y}\left(\omega
\right)-\mathit{x}\left(\omega \right)\right] \geq0,
\label{Expect_Monotonicity_conditions_rv}
\end{align}
%
which, by making use of \eqref{Monotonicity_conditions_traces},
results in
\begin{align}
&\mathbb{E}\left[\mathit{z}\left(\omega \right)\right] =\mathbb{E}\left[  \mathit{y}\left(\omega \right)-\mathit{x}\left(\omega \right)\right]\nonumber\\
&=\mathbb{E}\left\lbrace\Tr\mathbf{\hat{Q}}_{A}\mathbf{\hat{Q}}_{A}\mathbf{\hat{R}}\mathbf{\hat{R}}  -\Tr\mathbf{\hat{Q}}_{A}\mathbf{\hat{R}}\mathbf{\hat{Q}}_{A}\mathbf{\hat{R}} \right\rbrace \geq0
\label{Expect_Monotonicity_conditions_trace}
\end{align}
%
Next, we can combine the inequalities \eqref{Mu_conditions} with
\eqref{Expect_Monotonicity_conditions_trace} to compute the  second
member of \eqref{Difference_MSEs_final}, for $  \mu\in[0\;1]$.

For its first summand, we combine \eqref{Mu_conditions} and
\eqref{Expect_Monotonicity_conditions_trace}, as follows:
%
\begin{align}
\left\lbrace
\begin{aligned}
&\mathbb{E}\left\lbrace\Tr\mathbf{\hat{Q}}_{A}\mathbf{\hat{Q}}_{A}\mathbf{\hat{R}}\mathbf{\hat{R}}  -\Tr\mathbf{\hat{Q}}_{A}\mathbf{\hat{R}}\mathbf{\hat{Q}}_{A}\mathbf{\hat{R}} \right\rbrace \geq0\\
&\left(\mathrm{2\mu}^{2}-\mathrm{4\mu}\right)\leq0,\;\mu\in[0\;1],
\end{aligned}
\right.
\label{Proof_MSE_final_firstsummand_1}
\end{align}
%
to obtain in a straightforward way
\begin{align}
&\left(\mathrm{2\mu}^{2}-\mathrm{4\mu}\right)\mathbb{E}\left\lbrace\Tr\mathbf{\hat{Q}}_{A}\mathbf{\hat{Q}}_{A}\mathbf{\hat{R}}\mathbf{\hat{R}}  -\Tr\mathbf{\hat{Q}}_{A}\mathbf{\hat{R}}\mathbf{\hat{Q}}_{A}\mathbf{\hat{R}} \right\rbrace \leq0
\label{Proof_MSE_final_first summand_2}
\end{align}
%
Similarly, we can compute its second member, by combining
\eqref{Mu_conditions} and \eqref{Inequality_3}, as described by
%
\begin{align}
\left\lbrace
\begin{aligned}
&\Tr\mathbf{\hat{Q}}_{A}\mathbf{\hat{Q}}_{A}\mathbf{R}\mathbf{R}  -\Tr\mathbf{\hat{Q}}_{A}\mathbf{\hat{R}}\mathbf{\hat{Q}}_{A}\mathbf{\hat{R}}  \geq0\\
&-\mathrm{4\mu}\leq0,\;\mu\in[0\;1],
\end{aligned}
\right.
\label{Proof_MSE_final_second_summand_1}
\end{align}
%
to obtain also straightforwardly the expression given by
\begin{align}
&-\mathrm{4\mu}\left\lbrace\Tr\mathbf{\hat{Q}}_{A}\mathbf{\hat{Q}}_{A}\mathbf{R}\mathbf{R}  -\Tr\mathbf{\hat{Q}}_{A}\mathbf{R}\mathbf{\hat{Q}}_{A}\mathbf{R} \right\rbrace \leq0
\label{Proof_MSE_final_second_summand_2}
\end{align}
%
By combining the inequalities \eqref{Proof_MSE_final_first
    summand_2} and \eqref{Proof_MSE_final_second_summand_2} with
\eqref{Difference_MSEs_final}, we have
%
\begin{align}
&\mathrm{MSE}\left( \mathbf{\hat{R}}^{(n+1)}\right)\big\rvert_{n=1}-\mathrm{MSE}\left( \mathbf{\hat{R}}\right)\nonumber\\
&=\underbrace{\left( \mathrm{2\mu}^{2}-\mathrm{4\mu}\right)  \mathbb{E}\left\lbrace\Tr\mathbf{\hat{Q}}_{A}\mathbf{\hat{Q}}_{A}\mathbf{\hat{R}}\mathbf{\hat{R}} -\Tr\mathbf{\hat{Q}}_{A}\mathbf{\hat{R}}\mathbf{\hat{Q}}_{A}\mathbf{\hat{R}} \right\rbrace}_{\leq\;0}\nonumber\\
&\underbrace{-4\mu\left\lbrace\Tr \mathbf{\hat{Q}}_{A}\mathbf{\hat{Q}}_{A} \mathbf{R}\mathbf{R} -\Tr\mathbf{\hat{Q}}_{A}\mathbf{R}\mathbf{\hat{Q}}_{A} \mathbf{R} \right\rbrace}_{\leq\;0}\\
& \therefore\;\mathrm{MSE}\left( \mathbf{\hat{R}}^{(n+1)}\right)\big\rvert_{n=1}-\mathrm{MSE}\left( \mathbf{\hat{R}}\right)\leq\:0
\label{Proof_final}
\end{align}
which is the desired result.

\begin{thebibliography}{100}

\bibitem{Vantrees}
H. L. Van Trees, \newblock {\em Detection, Estimation, and  Modulation, Part IV, Optimum Array Processing}, John Wiley \&Sons, 2002.

\bibitem{schimdt}
R. Schmidt, "Multiple emitter location and signal parameter estimation"
 \textit{IEEE Trans on Antennas and Propagation}, vol.34, No.3, Mar 1986, pp 276-280.

\bibitem{Barabell}
A. J. Barabell, Improving the resolution performance of eigenstructure-based
direction-finding algorithms, in Proc. ICASSP, Boston, MA, Apr.
1983, pp. 336339.

\bibitem{Roy}
 R. Roy and T. Kailath, "Estimation of signal parameters via
 rotational invariance techniques", \textit{IEEE Trans. Acoust., Speech., Signal Processing}, vol. 37, July 1989, pp 984-995.

 \bibitem{scharf}
L. L. Scharf and D. W. Tufts, ``Rank reduction for modeling
stationary signals," \textit{IEEE Transactions on Acoustics, Speech
and Signal Processing}, vol. ASSP-35, pp. 350-355, March 1987.



\bibitem{bar-ness} A. M. Haimovich
and Y. Bar-Ness, ``An eigenanalysis interference canceler," {\it
IEEE Trans. on Signal Processing}, vol. 39, pp. 76-84, Jan. 1991.

\bibitem{pados99} D. A. Pados and S. N. Batalama "Joint space-time
auxiliary vector filtering for DS/CDMA systems with antenna arrays"
\textit{ IEEE Transactions on Communications}, vol. 47, no. 9, pp.
1406 - 1415, 1999.



\bibitem{reed98} J. S. Goldstein, I. S. Reed and L. L. Scharf
"A multistage representation of the Wiener filter based on
orthogonal projections" \textit{IEEE Transactions on Information
Theory}, vol. 44, no. 7, 1998.

\bibitem{hua}
Y. Hua, M. Nikpour and P. Stoica, "Optimal reduced rank estimation
and filtering," IEEE Transactions on Signal Processing, pp. 457-469,
Vol. 49, No. 3, March 2001.


\bibitem{goldstein}
M. L. Honig and J. S. Goldstein, ``Adaptive reduced-rank
interference suppression based on the multistage Wiener filter,"
\textit{ IEEE Transactions on Communications}, vol. 50, no. 6, June
2002.

\bibitem{santos}
E. L. Santos and M. D. Zoltowski, ``On Low Rank MVDR Beamforming
using the Conjugate Gradient Algorithm", \textit{Proc. IEEE
International Conference on Acoustics, Speech and Signal
Processing}, 2004.

\bibitem{qian}
Q. Haoli and S.N. Batalama, ``Data record-based criteria for the
selection of an auxiliary vector estimator of the MMSE/MVDR filter",
\textit{IEEE Transactions on Communications}, vol. 51, no. 10, Oct.
2003, pp. 1700 - 1708.

\bibitem{delamarespl07}
R. C. de Lamare and R. Sampaio-Neto, ``Reduced-Rank Adaptive
Filtering Based on Joint Iterative Optimization of Adaptive
Filters", \textit{IEEE Signal Processing Letters}, Vol. 14, no. 12,
December 2007.

\bibitem{xutsa}
Z. Xu and M.K. Tsatsanis, ``Blind adaptive algorithms for minimum
variance CDMA receivers," \textit{IEEE Trans. Communications}, vol.
49, No. 1, January 2001.

\bibitem{delamaretsp}
R. C. de Lamare and R. Sampaio-Neto, ``Low-Complexity Variable
Step-Size Mechanisms for Stochastic Gradient Algorithms in Minimum
Variance CDMA Receivers", \textit{IEEE Trans. Signal Processing},
vol. 54, pp. 2302 - 2317, June 2006.

\bibitem{kwak}
C. Xu, G. Feng and K. S. Kwak, ``A Modified Constrained Constant
Modulus Approach to Blind Adaptive Multiuser Detection," \textit{
IEEE Trans. Communications}, vol. 49, No. 9, 2001.

\bibitem{xu&liu}
Z. Xu and P. Liu, ``Code-Constrained Blind Detection of CDMA Signals
in Multipath Channels," \textit{ IEEE Sig. Proc. Letters}, vol. 9,
No. 12, December 2002.

%\bibitem{BEACON_JIO_Clarke}
%P. Clarke and R. C. de Lamare,
%"Set-Membership Reduced-Rank BEACON Algorithm based on Joint Iterative Optimization of Adaptive Filters", \textit{IEEE International Symposium on Circuits and Systems}, Paris, June, 2010.
%
\bibitem{delamareccm}
R. C. de Lamare and R. Sampaio Neto, "Blind Adaptive
Code-Constrained Constant Modulus Algorithms for CDMA Interference
Suppression in Multipath Channels", \textit{ IEEE Communications
Letters}, vol 9. no. 4, April, 2005.

\bibitem{wcccm}
L. Landau, R. C. de Lamare and M. Haardt, ``Robust adaptive
beamforming algorithms using the constrained constant modulus
criterion," IET Signal Processing, vol.8, no.5, pp.447-457, July
2014.

\bibitem{delamareelb}
R. C. de Lamare, ``Adaptive Reduced-Rank LCMV Beamforming Algorithms
Based on Joint Iterative Optimisation of Filters",
\textit{Electronics Letters}, vol. 44, no. 9, 2008.


\bibitem{jidf}
R. C. de Lamare and R. Sampaio-Neto, ``Adaptive Reduced-Rank
Processing Based on Joint and Iterative Interpolation, Decimation
and Filtering", \textit{IEEE Transactions on Signal Processing},
vol. 57, no. 7, July 2009, pp. 2503 - 2514.

\bibitem{delamarecl}
R. C. de Lamare and Raimundo Sampaio-Neto, ``Reduced-rank
Interference Suppression for DS-CDMA based on Interpolated FIR
Filters", \textit{IEEE Communications Letters}, vol. 9, no. 3, March
2005.

\bibitem{delamaresp}
R. C. de Lamare and R. Sampaio-Neto, ``Adaptive Reduced-Rank MMSE
Filtering with Interpolated FIR Filters and Adaptive Interpolators",
\textit{IEEE Signal Processing Letters}, vol. 12, no. 3, March,
2005.

\bibitem{delamaretvt}
R. C. de Lamare and R. Sampaio-Neto, ``Adaptive Interference
Suppression for DS-CDMA Systems based on Interpolated FIR Filters
with Adaptive Interpolators in Multipath Channels", \textit{IEEE
Trans. Vehicular Technology}, Vol. 56, no. 6, September 2007.

\bibitem{jioel}
R. C. de Lamare, ``Adaptive Reduced-Rank LCMV Beamforming Algorithms
Based on Joint Iterative Optimisation of Filters," Electronics
Letters, 2008.


\bibitem{delamarespl07}
R. C. de Lamare and R. Sampaio-Neto, ``Reduced-rank adaptive
filtering based on joint iterative optimization of adaptive
filters",  \textit{IEEE Signal Process. Lett.}, vol. 14, no. 12, pp.
980-983, Dec. 2007.

\bibitem{delamare_ccmmswf}
R. C. de Lamare, M. Haardt, and R. Sampaio-Neto, ``Blind Adaptive
Constrained Reduced-Rank Parameter Estimation based on Constant
Modulus Design for CDMA Interference Suppression", \textit{IEEE
Transactions on Signal Processing}, June 2008.

\bibitem{jidf_echo}
M. Yukawa, R. C. de Lamare and R. Sampaio-Neto, ``Efficient Acoustic
Echo Cancellation With Reduced-Rank Adaptive Filtering Based on
Selective Decimation and Adaptive Interpolation," IEEE Transactions
on Audio, Speech, and Language Processing, vol.16, no. 4, pp.
696-710, May 2008.

\bibitem{delamaretvt10}
R. C. de Lamare and R. Sampaio-Neto, ``Reduced-rank space-time
adaptive interference suppression with joint iterative least squares
algorithms for spread-spectrum systems," \textit{IEEE Trans. Vehi.
Technol.}, vol. 59, no. 3, pp. 1217-1228, Mar. 2010.

\bibitem{delamaretvt2011ST}
R. C. de Lamare and R. Sampaio-Neto, ``Adaptive reduced-rank
equalization algorithms based on alternating optimization design
techniques for MIMO systems," \textit{IEEE Trans. Vehi. Technol.},
vol. 60, no. 6, pp. 2482-2494, Jul. 2011.


\bibitem{delamare10}
R. C. de Lamare, L. Wang, and R. Fa, ``Adaptive reduced-rank LCMV
beamforming algorithms based on joint iterative optimization of
filters: Design and analysis," Signal Processing, vol. 90, no. 2,
pp. 640-652, Feb. 2010.

\bibitem{fa10}
R. Fa, R. C. de Lamare, and L. Wang, ``Reduced-Rank STAP Schemes for
Airborne Radar Based on Switched Joint Interpolation, Decimation and
Filtering Algorithm," \textit{IEEE Transactions on Signal
Processing}, vol.58, no.8, Aug. 2010, pp.4182-4194.

\bibitem{lei09}
L. Wang and R. C. de Lamare, "Low-Complexity Adaptive Step Size
Constrained Constant Modulus SG Algorithms for Blind Adaptive
Beamforming", \textit{Signal Processing}, vol. 89, no. 12, December
2009, pp. 2503-2513.

\bibitem{ccmavf}
L. Wang and R. C. de Lamare, ``Adaptive Constrained Constant Modulus
Algorithm Based on Auxiliary Vector Filtering for Beamforming," IEEE
Transactions on Signal Processing, vol. 58, no. 10, pp. 5408-5413,
Oct. 2010.


\bibitem{lei10}
L. Wang, R. C. de Lamare, M. Yukawa, "Adaptive Reduced-Rank
Constrained Constant Modulus Algorithms Based on Joint Iterative
Optimization of Filters for Beamforming," \textit{IEEE Transactions
on Signal Processing}, vol.58, no.6, June 2010, pp.2983-2997.

\bibitem{jio_ccm}
L. Wang, R. C. de Lamare and M. Yukawa, ``Adaptive reduced-rank
constrained constant modulus algorithms based on joint iterative
optimization of filters for beamforming", IEEE Transactions on
Signal Processing, vol.58, no. 6, pp. 2983-2997, June 2010.

\bibitem{ccmavf}
L. Wang and R. C. de Lamare, ``Adaptive constrained constant modulus
algorithm based on auxiliary vector filtering for beamforming", IEEE
Transactions on Signal Processing, vol. 58, no. 10, pp. 5408-5413,
October 2010.

\bibitem{stap_jio}
R. Fa and R. C. de Lamare, ``Reduced-Rank STAP Algorithms using
Joint Iterative Optimization of Filters," IEEE Transactions on
Aerospace and Electronic Systems, vol.47, no.3, pp.1668-1684, July
2011.

\bibitem{zhaocheng}
Z. Yang, R. C. de Lamare and X. Li, ``L1-Regularized STAP Algorithms
With a Generalized Sidelobe Canceler Architecture for Airborne
Radar," IEEE Transactions on Signal Processing, vol.60, no.2,
pp.674-686, Feb. 2012.

\bibitem{zhaocheng2}
Z. Yang, R. C. de Lamare and X. Li, ``Sparsity-aware spacetime
adaptive processing algorithms with L1-norm regularisation for
airborne radar", IET signal processing, vol. 6, no. 5, pp. 413-423,
2012.

\bibitem{arh_eusipco}
Neto, F.G.A.; Nascimento, V.H.; Zakharov, Y.V.; de Lamare, R.C.,
"Adaptive re-weighting homotopy for sparse beamforming," in Signal
Processing Conference (EUSIPCO), 2014 Proceedings of the 22nd
European , vol., no., pp.1287-1291, 1-5 Sept. 2014

\bibitem{arh_taes}
Almeida Neto, F.G.; de Lamare, R.C.; Nascimento, V.H.; Zakharov,
Y.V.,``Adaptive reweighting homotopy algorithms applied to
beamforming," IEEE Transactions on Aerospace and Electronic Systems,
vol.51, no.3, pp.1902-1915, July 2015.

\bibitem{dfjio}
L. Wang, R. C. de Lamare and M. Haardt, ``Direction finding
algorithms based on joint iterative subspace optimization," IEEE
Transactions on Aerospace and Electronic Systems, vol.50, no.4,
pp.2541-2553, October 2014.

\bibitem{rdrab}
S. D. Somasundaram, N. H. Parsons, P. Li and R. C. de Lamare,
``Reduced-dimension robust capon beamforming using Krylov-subspace
techniques," IEEE Transactions on Aerospace and Electronic Systems,
vol.51, no.1, pp.270-289, January 2015.

\bibitem{dcg_conf}
S. Xu and R.C de Lamare, , \textit{Distributed conjugate gradient
strategies for distributed estimation over sensor networks}, Sensor
Signal Processing for Defense SSPD, September 2012.


\bibitem{dcg}
S. Xu, R. C. de Lamare, H. V. Poor, ``Distributed Estimation Over
Sensor Networks Based on Distributed Conjugate Gradient Strategies",
IET Signal Processing, 2016 (to appear).

\bibitem{dce}
S. Xu, R. C. de Lamare and H. V. Poor, \textit{Distributed
Compressed Estimation Based on Compressive Sensing}, IEEE Signal
Processing letters, vol. 22, no. 9, September 2014.

\bibitem{drr_conf}
S. Xu, R. C. de Lamare and H. V. Poor, ``Distributed reduced-rank
estimation based on joint iterative optimization in sensor
networks," in Proceedings of the 22nd European Signal Processing
Conference (EUSIPCO), pp.2360-2364, 1-5, Sept. 2014

\bibitem{dta_conf1}
S. Xu, R. C. de Lamare and H. V. Poor, ``Adaptive link selection
strategies for distributed estimation in diffusion wireless
networks," in Proc. IEEE International Conference onAcoustics,
Speech and Signal Processing (ICASSP),  , vol., no., pp.5402-5405,
26-31 May 2013.

\bibitem{dta_conf2}
S. Xu, R. C. de Lamare and H. V. Poor, ``Dynamic topology adaptation
for distributed estimation in smart grids," in Computational
Advances in Multi-Sensor Adaptive Processing (CAMSAP), 2013 IEEE 5th
International Workshop on , vol., no., pp.420-423, 15-18 Dec. 2013.

\bibitem{dta_ls}
S. Xu, R. C. de Lamare and H. V. Poor, ``Adaptive Link Selection
Algorithms for Distributed Estimation", EURASIP Journal on Advances
in Signal Processing, 2015.

\bibitem{song}
N. Song, R. C. de Lamare, M. Haardt, and M. Wolf, ``Adaptive Widely
Linear Reduced-Rank Interference Suppression based on the
Multi-Stage Wiener Filter," IEEE Transactions on Signal Processing,
vol. 60, no. 8, 2012.

\bibitem{wljio}
N. Song, W. U. Alokozai, R. C. de Lamare and M. Haardt, ``Adaptive
Widely Linear Reduced-Rank Beamforming Based on Joint Iterative
Optimization,"  IEEE Signal Processing Letters, vol.21, no.3, pp.
265-269, March 2014.

\bibitem{barc}
R.C. de Lamare, R. Sampaio-Neto and M. Haardt, "Blind Adaptive
Constrained Constant-Modulus Reduced-Rank Interference Suppression
Algorithms Based on Interpolation and Switched Decimation,"
\textit{IEEE Trans. on Signal Processing},  vol.59, no.2,
pp.681-695, Feb. 2011.

\bibitem{jiomber}
Y. Cai, R. C. de Lamare, ``Adaptive Linear Minimum BER Reduced-Rank
Interference Suppression Algorithms Based on Joint and Iterative
Optimization of Filters," IEEE Communications Letters, vol.17, no.4,
pp.633-636, April 2013.

\bibitem{saalt}
R. C. de Lamare and R. Sampaio-Neto, ``Sparsity-Aware Adaptive
Algorithms Based on Alternating Optimization and Shrinkage," IEEE
Signal Processing Letters, vol.21, no.2, pp.225,229, Feb. 2014.

\bibitem{Steinwandt}
J. Steinwandt, R. C. de Lamare and M. Haardt, "Beamspace direction
finding based on the conjugate gradient and the auxiliary vector
filtering algorithms", Signal Processing, vol. 93, no. 4, April
2013, pp. 641-651.

\bibitem{Wang}
L. Wang, R. C. de Lamare and M. Haardt, "Direction finding
algorithms based on joint iterative subspace optimization,"
\textit{IEEE Transactions on Aerospace and Electronic Systems}, vol.
50, no. 4, pp. 2541-2553, October 2014.

\bibitem{Qiu}
L. Qiu, Y. Cai, R. C. de Lamare and M. Zhao, "Reduced-Rank DOA
Estimation Algorithms Based on Alternating Low-Rank Decomposition,"
\textit{IEEE Signal Processing Letters}, vol. 23, no. 5, pp.
565-569, May 2016.

\bibitem{Thomas}
 J. Thomas, L. Scharf, and D. Tufts, The probability of a subspace swap
 in the SVD, IEEE Trans. Signal Process., vol. 43, no. 3, pp. 730736,
 Mar. 1995.

\bibitem{Carlson}
 B. D. Carlson, Covariance matrix estimation errors and diagonal
loading in adaptive arrays, IEEE Trans. Aerosp. Electron. Syst., vol.
24, no. 4, pp. 397401, Jul. 1988.

\bibitem{Chen}
 Y. Chen, A. Wiesel, Y. C. Eldar, and A. O. Hero, Shrinkage algorithms
for MMSE covariance estimation, IEEE Trans. Signal
Process., vol. 58, no. 10, pp. 50165028, Oct. 2010.

\bibitem{ruan1}
H. Ruan and R. C. de Lamare, ``Robust Adaptive Beamforming Using a
Low-Complexity Shrinkage-Based Mismatch Estimation Algorithm," IEEE
Signal Processing Letters, vol. 21, no. 1, pp. 60-64, Jan. 2014.

\bibitem{ruan2}
H. Ruan and R. C. de Lamare, ``Robust Adaptive Beamforming Based on
Low-Rank and Cross-Correlation Techniques," IEEE Transactions on
Signal Processing, vol. 64, no. 15, pp. 3919-3932, Aug.1, 1 2016.

\bibitem{Pillai}
S. U. Pillai and B. H. Known, Forward/backward spatial smoothing
techniques for coherent signal identification, IEEE Trans. Acoust.,
Speech, Signal Process., vol. 37, no. 1, pp. 815, Jan. 1989.

\bibitem{Evans}
J. E. Evans, J. R. Johnson, and D. F. Sun, Application of Advanced
Signal Processing Techniques to Angle of Arrival Estimation in ATC
Navigation and Surveillance Systems. Lexington, MA, USA: MIT
Lincoln Lab., June 1982.

\bibitem{Mestre}
 X. Mestre and M. A. Lagunas, Modified subspace algorithms for DOA
estimation with large arrays, IEEE Trans. Signal Process., vol. 56, no.
2, pp. 598614, Feb. 2008.

\bibitem{Gershman}
A. B. Gershman and J. F. Böhme, Improved DOA estimation via pseudorandom
resampling of spatial spectrum, IEEE Signal Process. Lett.,
vol. 4, no. 2, pp. 5457, Feb. 1997.

\bibitem{Vasylyshyn}
 V. Vasylyshyn, ``Removing the outliers in root-MUSIC via pseudonoise
resampling and conventional beamformer, Signal Process., vol.
93, no. 12, pp. 34233429, Dec. 2013.

\bibitem{Qian}
 C. Qian, L. Huang, and H. C. So, ``Improved unitary root-MUSIC for
DOA estimation based on pseudo-noise resampling, IEEE Signal
Process. Lett., vol. 21, no. 2, pp. 140144, Feb. 2014.

\bibitem{mmimo}
R. C. de Lamare, ``Massive MIMO Systems: Signal Processing
Challenges and Future Trends", Radio Science Bulletin, December
2013.

\bibitem{wence}
W. Zhang, H. Ren, C. Pan, M. Chen, R. C. de Lamare, B. Du and J.
Dai, ``Large-Scale Antenna Systems With UL/DL Hardware Mismatch:
Achievable Rates Analysis and Calibration", IEEE Trans. Commun.,
vol.63, no.4, pp. 1216-1229, April 2015.

\bibitem{Costa}
M. Costa, "Writing on dirty paper," \textit{IEEE Trans. Inform.
Theory}, vol. 29, no. 3, pp. 439-441, May 1983.

\bibitem{delamare_ieeproc}
R. C. de Lamare and A. Alcaim, "Strategies to improve the
performance of very low bit rate speech coders and application to a
1.2 kb/s codec" IEE Proceedings- Vision, image and signal
processing, vol. 152, no. 1, February, 2005.

\bibitem{TDS_clarke}
P. Clarke and R. C. de Lamare, "Joint Transmit Diversity
Optimization and Relay Selection for Multi-Relay Cooperative MIMO
Systems Using Discrete Stochastic Algorithms," \emph{IEEE
Communications Letters}, vol.15, no.10, pp.1035-1037, October 2011.

\bibitem{TDS_2}
P. Clarke and R. C. de Lamare, "Transmit Diversity and Relay
Selection Algorithms for Multirelay Cooperative MIMO Systems"
\emph{IEEE Transactions on Vehicular Technology}, vol.61, no. 3, pp.
1084-1098, October 2011.

\bibitem{switch_int}
Y. Cai, R. C. de Lamare, and R. Fa, ``Switched Interleaving
Techniques with Limited Feedback for Interference Mitigation in
DS-CDMA Systems," IEEE Transactions on Communications, vol.59, no.7,
pp.1946-1956, July 2011.

\bibitem{switch_mc}
Y. Cai, R. C. de Lamare, D. Le Ruyet, ``Transmit Processing
Techniques Based on Switched Interleaving and Limited Feedback for
Interference Mitigation in Multiantenna MC-CDMA Systems," IEEE
Transactions on Vehicular Technology, vol.60, no.4, pp.1559-1570,
May 2011.

\bibitem{smce}
T. Wang, R. C. de Lamare, and P. D. Mitchell, ``Low-Complexity
Set-Membership Channel Estimation for Cooperative Wireless Sensor
Networks," IEEE Transactions on Vehicular Technology, vol.60, no.6,
pp.2594-2607, July 2011.

\bibitem{TongW}
T. Wang, R. C. de Lamare and A. Schmeink, "Joint linear receiver
design and power allocation using alternating optimization
algorithms for wireless sensor networks," \textit{IEEE Trans. on
Vehi. Tech.}, vol. 61, pp. 4129-4141, 2012.

\bibitem{jpais_iet}
R. C. de Lamare, ``Joint iterative power allocation and linear
interference suppression algorithms for cooperative DS-CDMA
networks", IET Communications, vol. 6, no. 13 , 2012, pp. 1930-1942.

\bibitem{TARMO}
T. Peng, R. C. de Lamare and A. Schmeink, ``Adaptive Distributed
Space-Time Coding Based on Adjustable Code Matrices for Cooperative
MIMO Relaying Systems'', \emph{IEEE Transactions on Communications},
vol. 61, no. 7, July 2013.

\bibitem{badstbc}
T. Peng and R. C. de Lamare, ``Adaptive Buffer-Aided Distributed
Space-Time Coding for Cooperative Wireless Networks," IEEE
Transactions on Communications, vol. 64, no. 5, pp. 1888-1900, May
2016.

\bibitem{baplnc}
J. Gu, R. C. de Lamare and M. Huemer, ``Buffer-Aided Physical-Layer
Network Coding with Optimal Linear Code Designs for Cooperative
Networks," IEEE Transactions on Communications, 2018.

\bibitem{keke1}
K. Zu, R. C. de Lamare, ``Low-Complexity Lattice Reduction-Aided
Regularized Block Diagonalization for MU-MIMO Systems'', IEEE.
Communications Letters, Vol. 16, No. 6, June 2012, pp. 925-928.

\bibitem{kekecl}
K. Zu, R. C. de Lamare, ``Low-Complexity Lattice Reduction-Aided
Regularized Block Diagonalization for MU-MIMO Systems'', IEEE.
Communications Letters, Vol. 16, No. 6, June 2012.

\bibitem{keke2} K. Zu, R. C. de Lamare and M.
Haart, ``Generalized design of low-complexity block diagonalization
type precoding algorithms for multiuser MIMO systems", IEEE Trans.
Communications, 2013.

\bibitem{Tomlinson}
M. Tomlinson, "New automatic equaliser employing modulo arithmetic,"
\textit{Electronic Letters}, vol. 7, Mar. 1971.

\bibitem{dopeg_cl} C. T. Healy and R. C. de Lamare,
``Decoder-optimised progressive edge growth algorithms for the
design of LDPC codes with low error floors",  \textit{IEEE
Communications Letters}, vol. 16, no. 6, June 2012, pp. 889-892.

\bibitem{peg_bf_iswcs}
A. G. D. Uchoa, C. T. Healy, R. C. de Lamare, R. D. Souza, ``LDPC
codes based on progressive edge growth techniques for block fading
channels", \textit{Proc. 8th International Symposium on Wireless
Communication Systems (ISWCS)}, 2011, pp. 392-396.

\bibitem{gqcpeg}
A. G. D. Uchoa, C. T. Healy, R. C. de Lamare, R. D. Souza,
``Generalised Quasi-Cyclic LDPC codes based on progressive edge
growth techniques for block fading channels",  \textit{Proc.
International Symposium Wireless Communication Systems (ISWCS)},
2012, pp. 974-978.

\bibitem{peg_bf_cl}
A. G. D. Uchoa, C. T. Healy, R. C. de Lamare, R. D. Souza, ``Design
of LDPC Codes Based on Progressive Edge Growth Techniques for Block
Fading Channels", \textit{IEEE Communications Letters}, vol. 15, no.
11, November 2011, pp. 1221-1223.

\bibitem{Harashima}
H. Harashima and H. Miyakawa, "Matched-transmission technique for
channels with intersymbol interference," \textit{IEEE Trans.
Commun.}, vol. 20, Aug. 1972.

\bibitem{mbthpc}
K. Zu, R. C. de Lamare and M. Haardt, ``Multi-branch
tomlinson-harashima precoding for single-user MIMO systems," in
Smart Antennas (WSA), 2012 International ITG Workshop on , vol.,
no., pp.36-40, 7-8 March 2012.

\bibitem{zuthp}
K. Zu, R. C. de Lamare and M. Haardt, ``Multi-Branch
Tomlinson-Harashima Precoding Design for MU-MIMO Systems: Theory and
Algorithms," IEEE Transactions on Communications, vol.62, no.3,
pp.939,951, March 2014.


\bibitem{rmbthp}
L. Zhang, Y. Cai, R. C. de Lamare and M. Zhao,  ``Robust Multibranch
TomlinsonHarashima Precoding Design in Amplify-and-Forward MIMO
Relay Systems," IEEE Transactions on Communications, vol.62, no.10,
pp.3476,3490, Oct. 2014.

\bibitem{wlbd}
W. Zhang et al., ``Widely Linear Precoding for Large-Scale MIMO with
IQI: Algorithms and Performance Analysis," IEEE Transactions on
Wireless Communications, vol. 16, no. 5, pp. 3298-3312, May 2017.

\bibitem{Hochwald}
B. Hochwald, C. Peel and A. Swindlehurst, "A vector-perturbation
technique for near capacity multiantenna multiuser communication -
Part II: Perturbation," \textit{IEEE Trans. Commun.}, vol. 53, no.
3, Mar. 2005.

\bibitem{BDVP}
C. B. Chae, S. Shim and R. W. Heath, "Block diagonalized vector
perturbation for multiuser MIMO systems," \textit{IEEE Trans.
Wireless Commun.}, vol. 7, no. 11, pp. 4051 - 4057, Nov. 2008.

\bibitem{delamare_mber}
R. C. de Lamare, R. Sampaio-Neto, ``Adaptive MBER decision feedback
multiuser receivers in frequency selective fading channels",
\textit{ IEEE Communications Letters}, vol. 7, no. 2, Feb. 2003, pp.
73 - 75.

\bibitem{rontogiannis}
A. Rontogiannis, V. Kekatos, and K. Berberidis," A Square-Root
Adaptive V-BLAST Algorithm for Fast Time-Varying MIMO Channels,"
\textit{IEEE Signal Processing Letters}, Vol. 13, No. 5, pp.
265-268, May 2006.

\bibitem{delamare_itic} R. C.
de Lamare, R. Sampaio-Neto, A. Hjorungnes, ``Joint iterative
interference cancellation and parameter estimation for CDMA
systems", \textit{IEEE Communications Letters}, vol. 11, no. 12,
December 2007, pp. 916 - 918.

\bibitem{stspadf}
Y. Cai and R. C. de Lamare, "Adaptive Space-Time Decision Feedback
Detectors with Multiple Feedback Cancellation", \textit{IEEE
Transactions on Vehicular Technology}, vol. 58, no. 8,  October
2009, pp. 4129 - 4140.

\bibitem{choi}
J. W. Choi, A. C. Singer, J Lee, N. I. Cho, ``Improved linear
soft-input soft-output detection via soft feedback successive
interference cancellation," \textit{IEEE Trans. Commun.}, vol.58,
no.3, pp.986-996, March 2010.


\bibitem{stbcccm}
R. C. de Lamare and R. Sampaio-Neto, ``Blind adaptive MIMO receivers
for space-time block-coded DS-CDMA systems in multipath channels
using the constant modulus criterion," IEEE Transactions on
Communications, vol.58, no.1, pp.21-27, January 2010.


\bibitem{FL11}
R. Fa, R. C. de Lamare, ``Multi-Branch Successive Interference
Cancellation for MIMO Spatial Multiplexing Systems", \textit{ IET
Communications}, vol. 5, no. 4, pp. 484 - 494, March 2011.

\bibitem{jio_mimo}
R.C. de Lamare and R. Sampaio-Neto, ``Adaptive reduced-rank
equalization algorithms based on alternating optimization design
techniques for MIMO systems," IEEE Trans. Veh. Technol., vol. 60,
no. 6, pp. 2482-2494, July 2011.

\bibitem{peng_twc} P. Li, R. C. de Lamare and R. Fa, ``Multiple
Feedback Successive Interference Cancellation Detection for
Multiuser MIMO Systems," \textit{IEEE Transactions on Wireless
Communications}, vol. 10, no. 8, pp. 2434 - 2439, August 2011.

\bibitem{spa}
R.C. de Lamare, R. Sampaio-Neto, ``Minimum mean-squared error
iterative successive parallel arbitrated decision feedback detectors
for DS-CDMA systems," IEEE Trans. Commun., vol. 56, no. 5, May 2008,
pp. 778-789.

\bibitem{spa2}
R.C. de Lamare, R. Sampaio-Neto, ``Minimum mean-squared error
iterative successive parallel arbitrated decision feedback detectors
for DS-CDMA systems," IEEE Trans. Commun., vol. 56, no. 5, May 2008.

\bibitem{jio_mimo} R.C. de Lamare and R. Sampaio-Neto, ``Adaptive
reduced-rank equalization algorithms based on alternating
optimization design techniques for MIMO systems," IEEE Trans. Veh.
Technol., vol. 60, no. 6, pp. 2482-2494, July 2011.

\bibitem{P.Li}
P. Li, R. C. de Lamare and J. Liu, ``Adaptive Decision Feedback
Detection with Parallel Interference Cancellation and Constellation
Constraints for Multiuser MIMO systems'', IET Communications, vol.7,
2012, pp. 538-547.

\bibitem{jingjing}
J. Liu, R. C. de Lamare, ``Low-Latency Reweighted Belief Propagation
Decoding for LDPC Codes," IEEE Communications Letters, vol. 16, no.
10, pp. 1660-1663, October 2012.

\bibitem{memd}
C. T. Healy and R. C. de Lamare, ``Design of LDPC Codes Based on
Multipath EMD Strategies for Progressive Edge Growth," IEEE
Transactions on Communications, vol. 64, no. 8, pp. 3208-3219, Aug.
2016.

\bibitem{did}
P. Li and R. C. de Lamare, Distributed Iterative Detection With
Reduced Message Passing for Networked MIMO Cellular Systems, IEEE
Transactions on Vehicular Technology, vol.63, no.6, pp. 2947-2954,
July 2014.

\bibitem{bfidd}
A. G. D. Uchoa, C. T. Healy and R. C. de Lamare, ``Iterative
Detection and Decoding Algorithms For MIMO Systems in Block-Fading
Channels Using LDPC Codes," IEEE Transactions on Vehicular
Technology, 2015.

\bibitem{mbdf} R.
C. de Lamare, "Adaptive and Iterative Multi-Branch MMSE Decision
Feedback Detection Algorithms for Multi-Antenna Systems", \emph{IEEE
Trans. Wireless Commun.}, vol. 14, no. 10, October 2013.

\bibitem{bfidd}
A. G. D. Uchoa, C. T. Healy and R. C. de Lamare, ``Iterative
Detection and Decoding Algorithms for MIMO Systems in Block-Fading
Channels Using LDPC Codes," IEEE Transactions on Vehicular
Technology, vol. 65, no. 4, pp. 2735-2741, April 2016.

\bibitem{mserrr}
Y. Cai, R. C. de Lamare, B. Champagne, B. Qin and M. Zhao, "Adaptive
Reduced-Rank Receive Processing Based on Minimum Symbol-Error-Rate
Criterion for Large-Scale Multiple-Antenna Systems," in IEEE
Transactions on Communications, vol. 63, no. 11, pp. 4185-4201, Nov.
2015.

\bibitem{shaowcl08}
Z. Shao, R. C. de Lamare and L. T. N. Landau, ``Iterative Detection
and Decoding for Large-Scale Multiple-Antenna Systems with 1-Bit
ADCs," IEEE Wireless Communications Letters, 2018.


\bibitem{Vorobyov2}
M. Shaghaghi and S. A. Vorobyov, "Subspace leakage analysis and
improved DOA estimation with small sample size", IEEE Trans. Signal
Process., vol. 63, no.12, pp 3251-3265, Jun.2015.

\bibitem{Stoica}
P. Stoica and A. Nehorai, Performance study of conditional and
unconditional direction-of-arrival estimation, IEEE Trans. Acoust.,
Speech, Signal Process., vol. 38, no. 10, pp. 17831795, Oct. 1990.

 \bibitem{Johnson}
B. A. Johnson, Y. I. Abramovich, and X. Mestre, MUSIC, G-MUSIC,
and maximum-likelihood performance breakdown, IEEE Trans.
Signal Process., vol. 56, no. 8, pp. 39443958, Aug. 2008.

\bibitem{Pinto}
S. F. B. Pinto,  R. C. de Lamare, Two-Step Knowledge-aided Iterative
ESPRIT Algorithm, Twenty First ITG Workshop on Smart Antennas, 15-17
March 2017, Berlin, Germany.

\bibitem{Guerci1}
W. L. Melvin and J. R. Guerci, Knowledge-aided signal processing: a new paradigm for radar and other advanced sensors, IEEE Transactions on Aerospace and Electronic Systems, vol. 42, no. 3, pp. 983996, July 2006.

\bibitem{Showman}
W. L. Melvin and G. A. Showman, An approach to knowledge-aided
covariance estimation, IEEE Transactions on Aerospace and
Electronic Systems, vol. 42, no. 3, pp.10211042, July 2006.

\bibitem{Bergin}
J. S. Bergin, C. M. Teixeira, P. M. Techau, and J. R. Guerci,
``Improved clutter mitigation performance using knowledge-aided
space-time adaptive processing, IEEE Transactions on Aerospace and
Electronic Systems, vol. 42, no. 3, pp. 9971009, July 2006.

\bibitem{Steinwandt2}
J. Steinwandt, R. C. de Lamare and M. Haardt, "Knowledge-aided
direction finding based on Unitary ESPRIT," 2011 Conference Record
of the Forty Fifth Asilomar Conference on Signals, Systems and
Computers (ASILOMAR), Pacific Grove, CA, 2011, pp. 613-617.

\bibitem{Stoica2}
P. Stoica, J. Li, X. Zhu, and J. R. Guerci, "On using a priori
knowledge in space-time adaptive processing, IEEE Transactions on
Signal Processing, vol. 56, no. 6, pp. 2598-2602, June 2008.

\bibitem{Bouleux}
G. Bouleux, P. Stoica, and R. Boyer, "An optimal prior
knowledge-based DOA estimation method," in 17th European Signal
Processing Conference (EUSIPCO), Aug. 2009, pp. 869-873.

\bibitem{Guerci2}
P. Stoica, J. Li, X. Zhu, and J. R. Guerci, "On using a priori knowledge in spacetime adaptive processing," IEEE Transactions on Signal Processing, vol. 56, no. 6, pp.
25982602, June 2008.

\bibitem{Pinto2}
S. F. B. Pinto,  R. C. de Lamare, ``Multi-Step Knowledge-Aided
Iterative ESPRIT for Direction Finding", submitted to 22nd
International Conference on Digital Signal Processing, 23-25 August
2017 (DSP 2017), London, United Kingdom.

\bibitem{Pinto3}
S. F. B. Pinto and R. C. de Lamare, ``Multi-Step Knowledge-Aided
Iterative ESPRIT: Design and Analysis," IEEE Transactions on
Aerospace and Electronic Systems, 2018.

\bibitem{Haykin}
Simon Haykin, Adaptive Filter Theory, fourth edition, 2003.

\bibitem{Graybill}
F.A. Graybill, Matrices with Applications in Statistics, Wadsworth Publishing Company, Inc., Second Edition, 1983.

\bibitem{Karr}
Alan F. Karr, Probability, Springer-Verlag NY, Second Edition, 1993.

\bibitem{Chang}
Da-Wei Chang, "A Matrix Trace Inequality for Products of Hermitian Matrices", Journal of Mathematical Analysis and Applications 237, pp.721-725, 1999.

\bibitem{Semira}
H.Semira, H.Belkacemi, S.Marcos, "High-resolution source localization algorithm based on the conjugate gradient", EURASIP Journal on Advances in Signal Processing, 2007(2)(2007)19.

\bibitem{Grover}
R.Grover, D.A.Pados, M.J. Medley, "Subspace direction finding with an auxiliary-vector basis", IEEE Transactions on Signal Processing, 55
(2) (2007) pp.758763.

\bibitem{JM}
J.P.A. Almeida, J.M.P. Fortes, W.A. Finamore, Probability, Random Variables and Stochastic Processes, PUC-RIO/Interciencia, 2008.

 \bibitem{Stoica3}
P.Stoica and A.B.Gershman, Maximum-likelihood DOA estimation by
data-supported grid search, IEEE Signal Processing Letters, vol. 6,
no. 10, pp. 273- 275, Oct 1999.

\bibitem{Stoica4}
P.Stoica and Arye Nehorai, "MUSIC, maximum Likelihood, and Cramer-Rao Bound, IEEE Transactions on Acoustics, Speech and Signal Processing, vol. 37,no. 5, pp. 720- 741, May 1989.

\bibitem{Rappaport}
J.C.Liberti Jr, Theodore S. rappaport, "Smart antennas for Wireless
Communications: IS-95 and Third Generation CDMA Applications",
Chapter 9, pp 253-284, Prentice Hall, 1999.

\bibitem{Schell}
{S.V.Schell, W.A. Gardner, "High Resolution Direction Finding",
Chapter 17, K. Bose and C.R. Rao, pp 755-817,1993.}

\bibitem{Rissanen}
{J.Rissanen, "Modeling by the Shortest Data Description",
Automatica, Vol.14, pp 465-471,1978.}

\bibitem{Vaccaro}
{F. Li, R. J. Vaccaro, "Analysis of Min-Norm and MUSIC with
arbitrary array geometry", \textit{IEEE Transactions on Aerospace
and Electronic Systems}, vol.26, no. 6, pp 976-985, 1990.}

\bibitem{Haardt}
{M. Haardt, "Efficient one-, two-, and multidimensional
high-resolution  array signal processing", Ph.D. dissertation,
Munich University of Technology, Shaker Verlag, 1996.}


\end{thebibliography}
\end{document}
