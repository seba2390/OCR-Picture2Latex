\section{Experiments}\label{sec:applications}
We now consider the empirical impact of using NaMI compared with previous approaches.
In \S\ref{sec:vae-experiment}, we highlight the importance of using a faithful inverse in the VAE context, demonstrating that doing so results in a tighter variational bound and a higher log-likelihood.
In \S\ref{sec:binary-tree-experiment}, we use NaMI in the fixed-model setting.
Here our results demonstrate the importance of using both a faithful and minimal inverse on the efficiency of the learned inference network.
Low-level details on the experimental setups can be found in Appendix D and an implementation at {\small \url{https://git.io/fxVQu}}.

\subsection{Relaxed Bernoulli VAEs}\label{sec:vae-experiment}
\input{fig-relaxed-bernoulli-vae}
Prior work has shown that more expressive inference networks give an improvement in amortized SVI on sigmoid belief networks and standard VAEs, relative to using the mean-field approximation \citep{UriaEtAl2016, MaaloeEtAl2016, RezendeMohamed2015, KingmaEtAl2016}. \citet{KrishnanEtAl2017} report similar results when using more expressive inverses in deep linear-chain state-space models.
It is straightforward to see that any minimally faithful inverse for the standard VAE framework~\citep{KingmaWelling2013}
has a fully connected clique over the latent variables so that the inference network can take account of the explaining-away effects between the latent variables in the generative model.
As such, both forward-NaMI and backward-NaMI produce the same inverse.

The relaxed Bernoulli VAE \citep{MaddisonEtAl2016, JangEtAl2016} is a VAE variation that replaces both the prior on the latents and the distribution over the latents given the observations with the relaxed Bernoulli distribution (also known as the Concrete distribution). It can also be understood as a ``deep'' continuous relaxation of sigmoid belief networks.

We learn a relaxed Bernoulli VAE with 30 latent variables on MNIST, comparing a faithful inference network (parametrized with MADE~\citep{GermainEtAl2015}) to the mean-field approximation, after 1000 epochs of learning for
ten different sizes of inference network, keeping the size of the generative network fixed. We note that {the mean-field inference network has the same structure as the heuristic one that reverses the edges from the generative model}. A tight bound on the marginal likelihood is estimated with annealed importance sampling (AIS) \citep{Neal1998, WuEtAl2016b}.

The results shown in Figure \ref{fig:relaxed-bernoulli-vae} indicate that using a faithful inverse on this model produces a
significant improvement in learning over the mean-field inverse.
Note that the x-axis indicates the number of parameters in the inference network. We observe that for \emph{every} capacity level, the faithful inference network has a lower negative ELBO and AIS estimate than that of the mean-field inference network. In Figure \ref{fig:vae-gap}, the variational gap is observed to decrease (or rather, the variational bound tightens) for the faithful inverse as its capacity is increased, whereas it increases for the mean-field inverse.
This example illustrates the inadequacy of the mean-field approximation in certain classes of models, in that it can result in significantly underutilizing the capacity of the model.

\subsection{Binary-tree Gaussian BNs}\label{sec:binary-tree-experiment}
\input{fig-bn-graphs}
\input{fig-binary-tree-kl}

Gaussian BNs are a class of models in which the conditional distribution of each variable is normally distributed, with a fixed variance and a mean that is a fixed linear combination of its parents plus an offset. We consider here Gaussian BNs with a binary-tree structured graph and observed leaves (see Figure \ref{fig:binary-tree-bn} for the case of depth, $d=3$). In this class of models, the exact posterior can be calculated analytically \citep[\S7.2]{KollerFriedman2009} and so it forms a convenient test-bed for performance.

The heuristic inverses simply invert the edges of the graph (Figure \ref{fig:binary-tree-heuristic}), whereas a natural minimally faithful inverse requires extra edges between subtrees (e.g. Figure \ref{fig:binary-tree-inverse}) to account for the influence one node can have on others through its parent.
For this problem, it turns out that running reverse-NaMI (Figure \ref{fig:binary-tree-most-compact}) produces
a more compact inverse than forward-NaMI. This, in fact, turns out to be the most compact possible I-map for any $d>3$.
Nonetheless, all three inversion methods have significantly fewer edges than the fully connected inverse (Figure \ref{fig:binary-tree-fully-connected}).

The model is fixed and the inference network is learnt from samples from the generative model, minimizing the ``reverse'' KL-divergence, namely that from the posterior to the inference network $\textsc{KL}(p_{\theta}(\mathbf{z} | \mathbf{x}) || q_{\psi}(\mathbf{z} | \mathbf{x}))$, as per~\citep{PaigeWood2016}.
We compared learning across the inverses produced by using Stuhlm{\"u}ller's heuristic, forward-NaMI, reverse-NaMI, and taking the fully connected inverse.
The fully connected inference network was parametrized using MADE~\citep{GermainEtAl2015}, and the forward-NaMI one with a novel MADE variant that modifies the masking matrix to exactly capture the tree-structured dependencies (see Appendix E.2). As the same MADE approaches cannot be used for heuristic and reverse-NaMI inference networks, these were instead parametrized with a separate neural network for each variable's density function.
The inference network sizes were kept constant across approaches.

Results are given in Figure \ref{fig:binary-tree-kl} for depth $d=5$ averaging over 10 runs. Figures \ref{fig:binary-tree-kl-train} and \ref{fig:binary-tree-kl-test} show an estimate of $\textsc{KL}(p_{\theta}(\mathbf{z} | \mathbf{x}) || q_{\psi}(\mathbf{z} | \mathbf{x}))$ using the train and test sets respectively. From this, we observe that it is necessary to model at least the edges in an I-map for the inference network to be able to recover the posterior, and convergence is faster with fewer edges in the inference network.\input{fig-gmm}
Despite the more compact reverse-NaMI inverse converging faster than the forward-NaMI one, the latter seems to converges to a better final solution.  This may be because the MADE approach could not be used for the reverse-NaMI inverse, but this is a subject for future investigation nonetheless.

Figure \ref{fig:binary-tree-kl-posterior} shows the average negative log-likelihood of 200 samples from the inference networks evaluated on the analytical posterior, conditioning on five fixed datasets sampled from the generative model not seen during learning. It is thus a measure of how successful inference amortiziation has been. All three faithful inference networks have significantly lower variance over runs compared to the unfaithful inference network produced by Stuhlm{\"u}ller's algorithm.

We also observed during other experimentation that if one were to decrease the capacity of all methods, learning remains stable in the natural minimally faithful inverse at a threshold where it becomes unstable in the fully connected case and in Stuhlm{\"u}ller's inverse.

\subsection{Gaussian Mixture Models}\label{sec:gmm-experiment}

Gaussian mixture models (GMMs) are a clustering model where the data
$\mathbf{x}=\{x_1,x_2,\ldots,x_N\}$ is
assumed to have been generated from one of $K$ clusters, each of which
has a Gaussian distribution with parameters $\{\mu_j,\Sigma_j\}$, $j=1,2,\ldots,K$.   Each datum, $x_i$ is associated with a corresponding index, $z_i\in\{1,\ldots,K\}$ that gives the identity of that datum's cluster. The indices, $\mathbf{z}'=\{z_i\}$ are drawn i.i.d. from a categorical distribution with parameter $\phi$. Prior distributions are placed on $\theta=\{\mu_1,\Sigma_1,\ldots,\mu_K,\Sigma_K\}$ and $\phi$, so that the latent variables are $\mathbf{z}=\{\mathbf{z}',\theta,\phi\}$. The goal of inference is then to determine the posterior $p(\mathbf{z}\mid\mathbf{x})$, or some statistic of it.

As per the previous experiment, this falls into the fixed-model setting.
We factor the fully-connected inverse as, $q(\theta|x)q(\phi|\theta,\mathbf{x})q(\mathbf{z}'|\phi,\theta,\mathbf{x})$.
It turns out that applying reverse-NaMI decouples the dependence between the indices, $\mathbf{z}'$, and produces a much more compact factorization, $q(\theta|\mathbf{x},\phi)\prod^N_iq(z_i|x_i,\phi,\theta)q(\phi|\mathbf{x})$, than either the fully-connected or forward-NaMI inverses for this model. The inverse structure produced by Stuhlm\"uller's heuristic algorithm is very similar to the reverse-NaMI structure for this problem and is omitted.

We train our amortization artifact over datasets with $N=200$ samples and $K=3$ clusters. The inference network terms with distributions over vectors were parametrized by MADE, and we compare the results for the fully-connected and reverse-NaMI inverses.  We hold the neural network capacities constant across methods and average over 10 runs, the results for which are shown in Figure \ref{fig:gmm}. We see that learning is faster for the minimally faithful reverse-NaMI method, relative to the fully-connected inverse, and converges to a better solution, in agreement with the other experiments.

\subsection{Minimal and Non-minimal Faithful Inverses}\label{sec:skip-experiment}
\begin{wrapfigure}{r}{0.5\textwidth}
	\vspace{-8pt}
	\centering
	\subcaptionbox{12 skips edges\label{fig:skip1}}
	{\includegraphics[width=0.45\linewidth]{binary-tree-inverse-skip2.pdf}}\hspace{0.5cm}
	\subcaptionbox{16 skips edges\label{fig:skip2}}
	{\includegraphics[width=0.45\linewidth]{binary-tree-inverse-skip1.pdf}}
	\caption{Additional edges over forward-NaMI. \label{fig:skip-model}}\vspace{-10pt}
\end{wrapfigure}
To further examine the hypothesis that a non-minimal
faithful inverse has slower learning and converges to a worse
solution relative to a minimal one, we performed the setup of Experiment \ref{sec:binary-tree-experiment} with depth d = 4,  comparing the forward-NaMI
network to two additional networks that added 12 and 16 connections to forward-NaMI (holding the total capacity fixed).
\begin{wrapfigure}{r}{0.4\textwidth}
	\vspace{-15pt}
  \centering
  {\includegraphics[width=\linewidth,trim={0 0 0 0.6cm},clip]{binary_tree_d_4_skips_ll.pdf}}
  \vspace*{-3ex}
		\caption{Average NLL of inference network samples under analytical posterior. \label{fig:exp5-results}}
\end{wrapfigure}

\vspace{-2pt}
The additional edges are shown  in Figure \ref{fig:skip-model}.
Note the
regular forward-NaMI edges are omitted for visual clarity.

Figure \ref{fig:exp5-results} shows the average negative log likelihood (NLL) under the true posterior for samples generated by the inference network, based on 5 datasets not seen during training. It appears that the more edges are added beyond minimality, the slower is the initial learning and convergence is to a worse solution.

To further explain why minimality is crucial, we note that adding additional edges beyond minimality means that there will be factors that condition on variables whose probabilistic influence is blocked by the other variables. This effectively adds an input of random noise into these factors, which is why we then see slower learning and convergence to a worse solution.

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "main"
%%% End:
