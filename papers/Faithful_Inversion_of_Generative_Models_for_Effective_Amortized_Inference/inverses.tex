\section{Heuristic inverses}

Given a BN, $\mathcal{G}$, for a generative model, $p(\mathbf{x}, \mathbf{z})=p(\mathbf{x}\mid\mathbf{z})p(\mathbf{z})$ over observed variables $\mathbf{x}$ and latent variables $\mathbf{z}$, our goal is to construct a BN structure for the ``inverse model''. 
We define the inverse model structure, $\mathcal{H}$, as one that factors according to $q(\mathbf{x}, \mathbf{z})=q(\mathbf{x})q(\mathbf{z}\mid\mathbf{x})$, and preserves the conditional independencies in $p$, that is, $\mathcal{I}(\mathcal{H})\subseteq\mathcal{I}(\mathcal{G})$. 
The factorization will allow us to draw samples by ancestral sampling that are close to the posterior of $\mathbf{z}\mid\mathbf{x}$ in $p$.

In \citep{StuhlmullerEtAl2013}, an algorithm is given for forming an ``heuristic inverse'' as follows. 
First, we determine a reverse topological ordering on $\mathcal{G}$, which we will denote with $y_1,\ldots,y_n$ (where $y_i$ is equal to some $x_j$ or $z_k$ depending on the structure of the graph and the ordering). 
The graph $\mathcal{H}$ is then produced by setting the parents of $y_i$ to be the intersection of $y_1,\ldots,y_{i-1}$ and that node's Markov blanket in $\mathcal{G}$, $\textnormal{Markov}_\mathcal{G}(y_i)$, excluding latent parents for observed nodes. 
The procedure is equivalent to reversing the edges in $\mathcal{G}$, adding extra edges to fully connect all the parents of a node in $\mathcal{G}$, and removing edges from latent nodes into observed ones. 
It produces a factorization $q(\mathbf{x}\mid\mathbf{z})q(\mathbf{z})$.

\citep{PaigeWood2016} falsely claim that the inverse BN structure so produced does not introduce conditional independencies not present in $\mathcal{G}$, which is formalized in Proposition 1. 
That is, it is claimed that a heuristic inverse structure $\mathcal{H}$ is an I-map for $\mathcal{G}$, or equivalently, by the almost-everywhere soundness of d-separation, that $Y_1\rightleftharpoons\cdots\rightleftharpoons Y_m$ is active in $\mathcal{H}$ given $\mathbf{Z}$ implies that $Y_m\rightleftharpoons\cdots\rightleftharpoons Y_m$ is active in $\mathcal{G}$ given $\mathbf{Z}$, for an arbitrary trail. 

If this were true, then we could factor $p$ as,
\begin{align*}
p(\mathbf{y}) &= \prod_{i=1}^np(y_i\mid y_1,\ldots,y_{i-1})\\
	&= \prod_{i=1}^np(y_i\mid \{y_1,\ldots,y_{i-1}\}\cap\textnormal{Markov}_\mathcal{G}(y_i)\cap\mathbb{I}(y_i)),
\end{align*}
where,
\begin{align*}
	\mathbb{I}(y_i) &=
	\begin{cases}
		\mathbf{z}, &\text{if}\ y_i\in\mathbf{z}\\
		\mathbf{y}, &\text{otherwise}
	\end{cases}
\end{align*}
is defined to prevent edges from latent nodes into observed ones.

The problem is in going from the first to the second line. 
For example, consider the factor for an arbitrary latent node, $x_i$. 
We haven't conditioned on its \emph{complete} Markov blanket---only the children, and parents of children that occur previously in the ordering---and so we cannot assert that $x_i$ is independent from all the other previous variables.

It is easy to construct counterexamples, for which the influence of $y_i$ flows through one of its parents to effect another variable prior in the ordering that has not been conditioned on. 
For instance, see Figure \ref{fig:bn-examples}.

\input{fig-bn-examples}

Consider our first example in parts (a-b). 
Here, $B\rightarrow A\leftarrow C\leftarrow E$ is active in the heuristic inverse $\mathcal{H}$ given $A$, whereas the the trail is blocked by $A$ in $\mathcal{G}$ and there is no other active trail between $B$ and $E$ given $A$, a contradiction of Proposition 1. 
Likewise, in the second example of parts (d-e), we can follow a similar argument for the trail $B\rightarrow A\leftarrow C\leftarrow E$ in $\mathcal{H}$ given $A$. 
A correct inverse structure produced by our algorithm, to be explained in the sequel, is given in parts (c) and (f).
