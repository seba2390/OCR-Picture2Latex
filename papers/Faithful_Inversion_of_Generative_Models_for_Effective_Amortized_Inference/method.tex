\section{Method}
Our algorithm builds upon the tools of \emph{probabilistic graphical models}--- a summary for unfamiliar readers is given in Appendix A.

\subsection{General idea}\label{sec:stochastic-inverses}
Amortized inference algorithms make use of inference networks that approximate the posterior.
To be able to represent the posterior accurately, the distribution of the inference network should not encode independence assertions that are absent from the generative model.
An inference network that did encode additional independencies could not represent the true posterior, even in the non-parametric limit, with neural network factors whose capacity approaches infinity.

Let us define a \emph{stochastic inverse} for a generative model $p(\mathbf{x}|\mathbf{z})p(\mathbf{z})$ that factors according to a BN structure $\mathcal{G}$ to be a factorization of $q(\mathbf{z}|\mathbf{x})q(\mathbf{x})$ over $\mathcal{H}$ \citep{StuhlmullerEtAl2013,PaigeWood2016}.
The $q(\mathbf{z}|\mathbf{x})$ part of the stochastic inverse will define the factorization, or rather, coarse-grain structure, of the inference network.
Recall from \S\ref{sec:intro} that this involved two characteristics.
We first require $\mathcal{H}$ to be an \emph{I-map} for $\mathcal{G}$:
\begin{definition}
	Let $\mathcal{G}$ and $\mathcal{H}$ be two BN structures.
Denote the set of all conditional independence assertions made by a graph, $\mathcal{K}$, as $\mathcal{I}(\mathcal{K})$.
We say $\mathcal{H}$ is an \emph{I-map} for $\mathcal{G}$ if $\mathcal{I}(\mathcal{H})\subseteq\mathcal{I}(\mathcal{G})$.
\end{definition}
To be an I-map for $\mathcal{G}$, $\mathcal{H}$ may not encode all the independencies that $\mathcal{G}$ does, but it must not mislead us by encoding independencies not present in $\mathcal{G}$.
We term such inverses as being \emph{faithful}.
While the aforementioned heuristic methods \emph{do not} in general produce faithful inverses, using either a fully-connected inverse, or our method, does.

Second, since a fully-connected graph encodes no conditional independencies and is therefore suboptimal, we require in addition that $\mathcal{H}$ be a \emph{minimal I-map} for $\mathcal{G}$:
\begin{definition}
	A graph $\mathcal{K}$ is a \emph{minimal I-map} for a set of independencies $\mathcal{I}$ if it is an I-map for $\mathcal{I}$ and if removal of even a single edge from $\mathcal{K}$ renders it not an I-map.
\end{definition}
We call such inverses \emph{minimally faithful}, which roughly means that the inverse is a local optimum in the number of true independence assertions it encodes.

There will be many minimally faithful inverses for $\mathcal{G}$, each with a varying number of edges.
Our algorithm produces a \emph{natural inverse} in the sense that it either inverts the order of the random choices from that of the generative model (when it is run in the topological mode), or it preserves the ordering \input{fig-natural} of the random choices (when it is run in reverse topological mode):
\begin{definition}\label{def:naturalness}
	A stochastic inverse $\mathcal{H}$ for $\mathcal{G}$ over variables $\mathcal{X}$ is  a \emph{natural inverse} if either, for all $X\in\mathcal{X}$ there are no edges in $\mathcal{H}$ from $X$ to its descendants in $\mathcal{G}$, or, for all $X\in\mathcal{X}$ there are no edges in $\mathcal{H}$ from $X$ to its ancestors in $\mathcal{G}$.
\end{definition}
Essentially, a natural inverse is one for which if we were to perform ancestral sampling, the variables would be sampled in either a topological or reverse-topological ordering, relative to the original model. Consider the inverse networks
of $\mathcal{G}$ shown in Figure \ref{fig:natural-graphs}. $\mathcal{H}_1$ is not a natural inverse of $\mathcal{G}$, since there is both an edge $A\rightarrow C$ from a parent to a child, and an edge $C\rightarrow B$ from a child to a parent, relative to $\mathcal{G}$. However, $\mathcal{H}_2$ and $\mathcal{H}_3$ are natural, as they correspond respectively to the reverse-topological and topological orderings $C,B,A$ and $B,A,C$.

Most heuristic methods, including those of~\citep{StuhlmullerEtAl2013,PaigeWood2016}, produce (unfaithful) natural inverses that invert the order of the random choices, giving a reverse-topological ordering.

\subsection{Obtaining a natural minimally faithful inverse}
\label{sec:true-factorization}

We now present NaMI's graph inversion procedure that given an arbitrary BN structure, $\mathcal{G}$, produces a natural minimal I-map, $\mathcal{H}$.
We illustrate the procedure step-by-step on the example given in Figure \ref{fig:student-bn}. Here $H$ and $J$ are observed, as indicated by the shaded nodes.
Thus, our latent variables are $\mathbf{Z}=\{D,I,G,S,L\}$, our data is $\mathbf{X}=\{H,J\}$, and a factorization for $p(\mathbf{z}\mid\mathbf{x})$ is desired.

\begin{wrapfigure}[8]{r}{0.26\textwidth}
  \vspace*{-2ex}
	\centering
	\includegraphics[scale=0.34]{student-bn.pdf}
	\caption{Example BN}
  \label{fig:student-bn}
\end{wrapfigure}
The NaMI graph-inversion algorithm is traced in Table \ref{tab:inversion-example}.
Each step incrementally constructs two graphs:
an \emph{induced graph} $\mathcal{J}$ and a \emph{stochastic inverse} $\mathcal{H}$.
The induced graph is an undirected graph whose maximally connected subgraphs, or \emph{cliques}, correspond to the scopes of the intermediate factors produced by simulating variable elimination.
The stochastic inverse represents our eventual target which encodes the inverse dependency structure.  It is constructed using information from the partially-constructed induced graph.
Specifically, NaMI goes through the following steps for this example.

\textbf{{\scshape Step 0}}: The partial induced graph and stochastic inverse are initialized.
The initial induced graph is formed by taking the directed graph for the forward model, $\mathcal{G}$, removing the directionality of the edges, and adding additional edges between variables that share a child in $\mathcal{G}$---in this example, edges $D-I$, $S-L$ and $G-J$.
This process is known as \emph{moralization}.
The stochastic inverse begins as disconnected variables, and edges are added to it at each step.
\input{tab-inversion-example}

\textbf{{\scshape Step 1}}: The frontier set of variables to consider for elimination, $S$, is initialized to the latent variables having no latent parents in $\mathcal{G}$, that is, $D$, $I$.
To choose which variable to eliminate first, we apply the greedy min-fill heuristic, which is to choose the (possibly non-unique) variable that adds the fewest edges to the induced graph $\mathcal{J}$ in order to produce as compact an inverse as possible under the topological ordering.
Specifically, noting that the cliques of $\mathcal{J}$ correspond to the scopes of intermediate factors during variable elimination, we want to avoid producing intermediate factors which would require us to add additional edges to $\mathcal{J}$, as doing so will in turn induce additional edges in $\mathcal{H}$ at future steps.
For this example, if we were to eliminate $D$, that would produce an intermediate factor, $\psi_D(D,I,G)$, while if we were to eliminate $I$, that would produce an intermediate factor, $\psi_I(I,D,G,S)$.
Choosing to eliminate would $I$ thus requires adding an edge $G\hbox{--}S$ to the induced graph, as there is no clique $I,D,G,S$ in the current state of $\mathcal{J}$.
Conversely, eliminating $D$ does not require adding extra edges to $\mathcal{J}$ and so we choose to eliminate $D$.

 The elimination of $D$ is simulated by marking its node in $\mathcal{J}$.
The parents of $D$ in the inverse $\mathcal{H}$ are set to be its  nonmarked neighbours in $\mathcal{J}$, that is, $I$ and $G$.
$D$ is then removed from the frontier,  and any non-observed children in $\mathcal{G}$ of $D$ whose parents have all been marked added to it---in this case, there are none as the only child of $D$, $G$, still has an unmarked parent $I$.

\textbf{{\scshape Step 2}}: Variable $I$ is the sole member of the frontier and is chosen for elimination.
The elimination of $I$ is simulated by marking its node in $\mathcal{J}$ \emph{and} adding the additional edge $G\hbox{--}S$.
This is required because elimination of $I$ requires the addition of a factor, $\psi_I(I,G,S)$, that is not currently present in $\mathcal{J}$.
The parents of $I$ in the inverse $\mathcal{H}$ are set to be its nonmarked neighbours in $\mathcal{J}$, $G$ and $S$.
$I$ is then removed from the frontier.
Now, $G$ and $S$ are children of $I$, and both their parents $D$ and $I$	have been marked.
Therefore, they are added to the frontier.

\textbf{{\scshape Step 3-5}}: The process is continued until the end of the fifth step when all the latent variables, $D,I,S,G,L$, have been eliminated and the frontier is empty.
At this point, $\mathcal{H}$ represents a factorization $p(\mathbf{z}\mid\mathbf{x})$, and we stop here as only a factorization for the posterior is required for amortized inference. Note, however, that it is possible to continue simulating steps of variable elimination on the observed variables to complete the factorization as $p(\mathbf{z}\mid\mathbf{x})p(\mathbf{x})$.

An important point to note is that NaMI's graph inversion can be run in one of two modes. The ``topological mode,'' which we previously implicitly considered, simulates variable elimination in a topological ordering, producing an inverse that reverses the order of the random choices from the generative model.
Conversely, NaMI's graph inversion can also be run in ``reverse topological
\input{alg-invert-bn}
mode,'' which simulates variable elimination in a reverse topological ordering, producing an inverse that preserves the order of random choices in
the generative model.
We will refer to these approaches as \emph{forward-NaMI} and \emph{reverse-NaMI} respectively in the rest of the paper.
The rationale for these two modes is that, though they both produce minimally faithful inverses, one may be substantially more compact than the other, remembering that minimality only ensures a local optimum.
For an arbitrary graph, it cannot be said in advance which ordering will produce the more compact inverse.
However, as the cost of running the inversion algorithm is low, it is generally feasible to try and pick the one producing a better solution.

 The general NaMI graph-reversal procedure is given in Algorithm \ref{alg:invert-bn}.
It is further backed up by the following formal demonstration of correctness, the proof for which is given in Appendix F.
\vspace{-2pt}\begin{theorem}\label{theorem:correctness}
	The Natural Minimal I-Map Generator of Algorithm 1 produces inverse factorizations that are natural and minimally faithful.
\end{theorem}\vspace{-4pt}

We further note that NaMI's graph reversal has a running time of order $O(nc)$ where $n$ is the number of 
latent variables in the graph and $c<<n$ is the size of the largest clique in the induced graph.  We consequently see that it can be run
cheaply for practical problems: the computational cost of generating the
inverse is generally dominated by that of training the resulting
inference network itself.  See Appendix F for more details.

\subsection{Using the faithful inverse}
Once we have obtained the faithful inverse structure $\mathcal{H}$, the next step is to use it to learn an inference network, $q_\psi(\mathbf{z}\mid\mathbf{x})$. For this, we use the factorization given by $\mathcal{H}$.
Let $\tau$ denote the reverse of the order in which variables were selected for elimination by Line 9 in Algorithm 1, 
such that $\tau$ is a permutation of $1,\dots,n$ and $\tau(n)$ is the first variable eliminated.  
$\mathcal{H}$ encodes the factorization
\begin{align}
q_\psi(\mathbf{z}\mid\mathbf{x})=\prod\nolimits^n_{i=1} q_{i}(z_{\tau(i)}\mid\text{Pa}_\mathcal{H}(z_{\tau(i)}))
\end{align}
where $\text{Pa}_\mathcal{H}(z_{\tau(i)})\subseteq\left\{\mathbf{x},z_{\tau(1)},\dots,z_{\tau(i-1)}\right\}$ indicates the parents of $z_{\tau(i)}$ in $\mathcal{H}$.
For each factor $q_{i}$, we must decide both the class of distributions for $z_{\tau(i)}\mid\text{Pa}_\mathcal{H}(z_{\tau(i)})$, and how the parameters for that class are calculated.
Once learned, we can both sample from, and evaluate the density of, the inference network for a given dataset by considering each factor in turn.

The most natural choice for the class of distributions for each factor is to use
the same distribution family as the corresponding variable in the generative model, such that the supports of these distributions match.
For instance, continuing the example from Figure~\ref{fig:student-bn}, if $D\sim N(0,1)$ in the generative model, then
a normal distribution would also be used for
$D\mid I,G$ in the inference network. To establish the mapping from data to the parameters to this distribution,
we train neural networks using stochastic gradient ascent methods.  For instance, we could set
$D\mid\{I=i,G=g\}\sim N(\mu_\varphi(i,g),\sigma_\varphi(i,g))$, where $\mu_\varphi$ and $\sigma_\varphi$ are two densely connected
feedforward networks, with learnable parameters $\varphi$.
In general, it will be important to choose architectures which well match the problem at hand.
For example, when perceptual inputs such as images and language are present in the conditioning variables, it is
advantageous to first embed them to a lower-dimensional representation using, for example, convolutional neural networks.

Matching the distribution families in the inference network and generative model, whilst a simple and often adequate approximation, can be suboptimal. For example, suppose that for a normally distributed variable in the generative model, the true conditional distribution in the posterior for that variable is multimodal. In this case, using a (single mode) normal factor in the inference network would not suffice. One could straightforwardly instead use, for example, either a mixture of Gaussians, or, normalizing flows \citep{RezendeMohamed2015,KingmaEtAl2016}, to parametrize each inference network factor in order to improve expressivity, at the cost of additional implementational complexity.
In particular, if one were to use a provably universal density estimator to parameterize each
inference network factor, such as that introduced in~\citet{HuangEtAl2018}, the resulting NaMI inverse
would constitute a universal density estimator of the true posterior.

After the inference network has been parametrized, it can be trained in number of different ways, depending on the final use
case of the network.  For example, in the context of amortized stochastic variational inference (SVI) methods such as VAEs
\citep{KingmaWelling2013,RezendeEtAl2014}, the model $p_\theta(\mathbf{x},\mathbf{z})$ is learned along with the inference network $q_\psi(\mathbf{z}\mid\mathbf{x})$ by optimizing a lower bound on the marginal loglikelihood of the data, $\mathcal{L}_{\sc ELBO} = \mathbb{E}_{q_\psi(\mathbf{z}|\mathbf{x})}\left[\ln p_\theta(\mathbf{x},\mathbf{z}) - \ln q_\psi(\mathbf{z}\mid\mathbf{x})\right]$.
Stochastic gradient ascent can then be used to optimize $\mathcal{L}_{\sc ELBO}$ in the same way a standard VAE,
simulating from $q_\psi (z|x)$ by considering each factor in turn and using reparameterization~\citep{KingmaWelling2013}
when the individual factors permit doing so.

A distinct training approach is provided when the model $p(\mathbf{x},\mathbf{z})$ is fixed \citep{PapamakariosMurray2015}.
Here a proposal is learnt for either importance sampling~\citep{LeEtAl2016}
or sequential Monte Carlo~\citep{PaigeWood2016} by using stochastic gradient ascent to
minimize the reverse KL-divergence between the inference network $q_\psi(\mathbf{z}\mid\mathbf{x})$ and the true posterior
$p(\mathbf{z}\mid\mathbf{x})$. Up to a constant, the objective is given by
$\mathcal{L}_{\sc IC} = \mathbb{E}_{p(\mathbf{x},\mathbf{z})}\left[-\ln q_\psi(\mathbf{z}\mid\mathbf{x})\right].$

Using a minimally faithful inverse structure typically improves the best inference network attainable and the finite time training performance for both these settings, compared with previous naive approaches.
In the VAE setting, this can further have a knock-on effect on the quality of the learned model $p_\theta(\mathbf{x},\mathbf{z})$, both because a better inference network will give lower variance updates of the generative network~\citep{rainforth2018tighter} and because restrictions in the expressiveness of the inference network lead to similar restrictions in the generative network~\citep{cremer2017reinterpreting,cremer2018inference}.

In deep generative models, the BNs may be much larger than the examples shown here. However, typically at the macro-level, where we collapse each vector to a single node, they are quite simple. When we invert this type of collapsed graph, we must do so with the understanding that the distribution over a vector-valued node in the inverse must express dependencies between all its elements in order for the inference network to be faithful.

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "main"
%%% End:
