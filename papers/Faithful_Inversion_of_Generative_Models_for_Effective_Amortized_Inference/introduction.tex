\section{Introduction}%
\label{sec:intro}
Evidence from human cognition suggests that the brain reuses the results of past inferences to speed up subsequent related queries \citep{Gershman2014}.
In the context of Bayesian statistics, it is reasonable to expect that, given a generative model, $p(\mathbf{x},\mathbf{z})$, over data $\mathbf{x}$ and latent variables $\mathbf{z}$, inference on $p(\mathbf{z}\mid\mathbf{x}_1)$ is informative about inference on $p(\mathbf{z}\mid\mathbf{x}_2)$ for two related inputs, $\mathbf{x}_1$ and $\mathbf{x}_2$.
Several algorithms \citep{KingmaWelling2013, RezendeEtAl2014, StuhlmullerEtAl2013, PaigeWood2016, LeEtAl2016, LeEtAl2017, MaddisonEtAl2017, NaessethEtAl2017} have been developed with this insight to perform \emph{amortized inference} by learning an inference artefact $q(\mathbf{z}\mid\mathbf{x})$, which takes as input the values of the observed variables, and---typically with the use of neural network architectures---return a distribution over the latent variables approximating the posterior.
These inference artefacts are known variously as inference networks, recognition models, probabilistic encoders, and guide programs; we will adopt the term \emph{inference networks} throughout.


Along with conventional fixed-model settings~\citep{StuhlmullerEtAl2013,LeEtAl2016,Ritchie2016,PaigeWood2016},
a common application of inference amortization is in the training of variational auto-encoders (VAEs)~\citep{KingmaWelling2013}, for which the inference network is simultaneously learned alongside a generative model.
It is well documented that deficiencies in the expressiveness or training of the inference network can also have a knock-on effect on the learned generative model in such contexts~\citep{burda2015importance,cremer2017reinterpreting,cremer2018inference,rainforth2018tighter}, meaning that poorly chosen coarse-grained structures can be particularly damaging.

Implicit in the factorization of the generative model and inference network in both fixed and learned model settings are probabilistic graphical models, commonly Bayesian networks (BNs), encoding dependency structures. We refer to these as the \emph{coarse-grain} structure, in opposition to the \emph{fine-grain} structure of the neural networks that form each inference (and generative) network factor.
In this sense, amortized inference can be framed as the problem of graphical model inversion---how to invert the graphical model of the generative model to give a graphical model approximating the posterior. Many models from the deep generative modeling literature can be represented as BNs \citep{KrishnanEtAl2017, GanEtAl2015, Neal1990, KingmaWelling2013, GermainEtAl2015, VanDenOordEtAl2016a, VanDenOordEtAl2016b}, and fall within this framework.

In this paper, we borrow ideas from the probabilistic graphical models literature, to address the previously open problem of how best to automate the design of the coarse-grain structure of the inference network \citep{Ritchie2016}.
Typically, the inverse graphical model is formed heuristically.
At the simplest level, some methods just invert the edges in the BN for the generative model, removing edges between observed variables \citep{KingmaWelling2013, GanEtAl2015, RanganathEtAl2014}.
In a more principled, but still heuristic, approach, \citet{StuhlmullerEtAl2013,PaigeWood2016} construct the inference network by inverting the edges and additionally connecting the parents of children in the original graph (both of which are a subset of a variable's Markov blanket; see Appendix C).\input{fig-student-graphs} \vspace{-10pt}

In general, these heuristic methods introduce conditional independencies into the inference network that are not present in the original distribution.  Consequently, they cannot represent the true posterior
even in the limit of infinite neural network capacities.
Take the simple generative model with branching structure of Figure \ref{fig:example1-bn}.
The inference network formed by Stuhlm\"{u}ller's method inverts the edges of the model as in Figure \ref{fig:example1-brooks}.
However, an inference network that is able to represent the true posterior requires extra edges between the branches, as in Figure \ref{fig:example1-inverse}.

Another approach, taken by \citet{LeEtAl2016}, is to use a fully connected BN for the inverse graphical model, such that
every random choice made by the inference network depends on every previous one.
Though such a model is expressive enough to correctly represent the data given infinite capacity and training time, it ignores substantial available information from the forward model, inevitably leading to reduced performance for finite
training budgets and/or network capacities.

In this paper, we develop a tractable framework to remedy these deficiencies:
 the
\textbf{\emph{Na}}tural \textbf{\emph{M}}inimal \textbf{\emph{I}}-map generator (NaMI).
Given an arbitrary BN structure, NaMI can be used to construct an inverse BN structure that is provably both \emph{faithful} and \emph{minimal}.
It is faithful in that it contains sufficient edges to avoid encoding conditional independencies absent from the model.
It is minimal in that it does not contain any unnecessary edges; i.e., removing any edge would result in an unfaithful structure.

NaMI chiefly draws upon variable elimination \citep[Ch 9,10]{KollerFriedman2009}, a well-known algorithm from the graphical model literature for performing exact inference on discrete factor graphs.
The key idea in the operation of NaMI is to simulate variable elimination steps as a tool for successively determining a minimal, faithful, and natural inverse structure, which can then be used to parametrize an inference network.
NaMI further draws on ideas such as the min-fill heuristic \citep{FishelsonGeiger2004}, to choose the ordering in which variable elimination is simulated, which in turn influences the structure of the generated inverse.

To summarize, our key contributions are:
%
\begin{compactenum}[i)]
\item framing generative model learning through amortized variational inference as a graphical model inversion problem, and
\item using the simulation of exact inference algorithms to construct an algorithm for generating provably minimally faithful inverses.
\end{compactenum}
%
Our work thus highlights the importance of constructing both minimal and faithful inverses, while providing the first approach to produce inverses satisfying these properties.

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "main"
%%% End:
