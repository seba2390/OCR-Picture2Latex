\section{Related Work}
\label{section: related work}
Our work is inspired by the following three lines of literature:

\emph{Peer Prediction:} This line of work, addressing the incentive issues of eliciting high quality data without verification, starts roughly with the seminal ones \cite{prelec2004bayesian,gneiting2007strictly}. A series of follow-ups have relaxed various assumptions that have been made \cite{jurca2009mechanisms,witkowski2012peer,radanovic2013robust,dasgupta2013crowdsourced}. 

\emph{Inference method:} Recently, inference methods have been applied to crowdsourcing settings, aiming to uncover the true labels from multiple noisily copies. Notable successes include EM method \cite{dawid1979maximum,raykar2010learning,zhang2014spectral}, Variational Inference \cite{liu2012variational,chen2015statistical} and Minimax Entropy Inference~\cite{zhou2012learning,zhou2014aggregating}. Besides, Zheng \textit{et al.} \cite{zheng2017truth} provide a good survey for the existing ones.

\emph{Reinforcement Learning:} Over the past two decades, reinforcement learning (RL) algorithms have been proposed to iteratively improve the acting agent's learned policy \citep{Watkins92, Tesauro95, Sutton98, Gordon00, Szepesvari10}. More recently, with the help of advances in feature extraction and state representation, RL has made several breakthroughs in achieving human-level performance in challenging domains \citep{Mnih15,Liang16, Hasselt2016DeepRL, Silver17}. Meanwhile, many studies successfully deploy RL to address some societal problems \citep{Yu2013EmotionalMR,Leibo2017}. RL has also helped make progress in human-agent collaboration \citep{engel2005reinforcement, gasic2014gaussian,Sadhu2016ArgusSH,Wang2017}.

%In RL problems, an agent interacts with an unknown environment and attempts to maximize its cumulative utility \citep{Sutton98,Szepesvari10}. Recently, various RL aglorithms have been developed and sucessfully applied to the interaction with human beings~\cite{engel2005reinforcement,gasic2014gaussian}.

Our work differs from the above literature in the connection between incentive mechanisms and ML. There have been a very few recent studies that share a similar research taste with us.
For example, to improve the data requester's utility in crowdsourcing settings, Liu and Chen \cite{liu2017sequential} develop a multi-armed bandit algorithm to adjust the state-of-the-art peer prediction mechanism DG13~\cite{dasgupta2013crowdsourced} to a prior-free setting. Nonetheless, the results in above work require workers to follow a Nash Equilibrium at each step in a sequential setting, which is hard to achieve in practice. Instead of randomly choosing a reference worker as commonly done in peer prediction, Liu and Chen \cite{liu2017machine} propose to use supervised learning algorithms to generate the reference reports and derive the corresponding IC conditions. However, these reports need to be based on the contextual information of the tasks.
By contrast, in this paper, without assuming the contextual information about the tasks, we use Bayesian inference to learn workers' states and true labels, which leads to an unsupervised-learning solution.
%Our payments are determined based on the learned worker models, and we derive the incentive compatibility conditions for our unsupervised-learning-based mechanism.

%This idea has also been s\citet{RMDE,cai2018reinforcement} propose to build incentive mechanisms based on reinforcement learning.
%However, focusing on the empirical analysis, they never consider the theoretical incentive compatibility.
%In this paper, we also incorporate reinforcement learning to get rid of the assumption that workers are fully rational.
%When analyzing our incentive mechanism, we go one-step further by not only providing the empirical analysis but also present a novel proof for the incentive compatibility related with reinforcement learning.


%Peer prediction \cite{Prelec:2004,MRZ:2005,jurca2006minimum,jurca2009mechanisms,witkowski2012robust,witkowski2012peer,radanovic2013,dasgupta2013crowdsourced}, a class of mechanisms that have been developed extensively recently, are often adopted for incentivizing truthful or high-quality contributions from strategic sources when the quality of the contributions cannot be verified. %The Bayesian Truth Serum was proposed in \cite{Prelec:2004} to elicit truthful report when only minority of the crowd holds the true answer. 
%Particularly, the seminal work \cite{MRZ:2005} established that strictly proper scoring rule \cite{Gneiting:07} can be adopted in the peer prediction setting for eliciting truthful reports; following which a sequence of followed up works have been done on relaxing the assumptions that have been imposed therein \cite{witkowski2012robust,witkowski2012peer,radanovic2013}. More recently, \cite{Witkowski_hcomp13,dasgupta2013crowdsourced} formally introduced and studied an effort sensitive model for binary signal data elicitation. Particularly \cite{dasgupta2013crowdsourced} proposed a multi-task peer prediction mechanism that can help remove undesirable equilibria that lead to low quality reports. These results are further strengthened and extended to a non-binary signal setting in \cite{2016arXiv160303151S,kong2016framework}. One set of studies focuses on using models that aggregate tasks from different agents \cite{kamar2012incentives,faltings2014incentive,radanovic2016incentives}. These work do not consider a machine learning setting, and have not established a rigorous analysis. 


%Our work is inspired by the following three lines of literatures: (i) \emph{Peer Prediction} This line of work, addressing the incentive issues for reporting high quality data without verification, starts roughly with the seminal ones \cite{prelec2004bayesian,gneiting2007strictly}. A series of follow-ups have relaxed various assumptions that have been made \cite{jurca2009mechanisms,witkowski2012peer,radanovic2013robust,dasgupta2013crowdsourced}. (ii) \emph{Inference method} Recently inference methods have been applied to crowdsourcing settings, aiming to uncover the true labels from multiple noisily reported copies~\cite{zheng2017truth}. Notable successes include EM methods \cite{dawid1979maximum,raykar2010learning,zhang2014spectral}, Variational Inference \cite{liu2012variational,chen2015statistical} and Minimax Entropy Inference~\cite{zhou2012learning,zhou2014aggregating}. (iii) \emph{Reinforcement Learning} In the past few years, reinforcement learning has shown human-level performance in many real world application domains \citep{Mnih15,Hasselt2016DeepRL,Silver17,Sadhu2016ArgusSH,Wang2017}. Our work differs from the above literature in the mixture of two seemingly disconnected areas: ML studies rarely consider the effects of incentives, while incentive mechanism designers do not often leverage ML techniques. We demonstrate the benefits of using ML techniques for incentive mechanism design and show how incentives can be involved in ML studies. Besides technical contributions, we also made several interesting observations. For example, the design of RL algorithms should leave some gaps between two actions to prevent manipulations from strategical workers (Theorem 3).
% To summarize, our work is inspired by the following three lines of literatures: (i) \emph{Peer Prediction} \cite{prelec2004bayesian,gneiting2007strictly,jurca2009mechanisms,witkowski2012peer,radanovic2013robust,dasgupta2013crowdsourced} (ii) \emph{Inference method} \cite{dawid1979maximum,raykar2010learning,zhang2014spectral,liu2012variational,chen2015statistical,zhou2012learning,zhou2014aggregating} and (iii) \emph{Reinforcement Learning} \cite{Watkins92, Tesauro95,Baird95,Mnih15,Sadhu2016ArgusSH,Wang2017,cai2018reinforcement,cai2018reinforcement2}. Our work differs from the above literature in two seemingly disconnected aspects: ML studies rarely consider the effects of incentives, while incentive mechanism designers do not often leverage ML techniques. We demonstrate the benefits of using ML techniques for incentive mechanism design and show how incentives can be involved in ML studies. %Besides technical contributions, we also made several interesting observations. For example, the design of RL algorithms should leave some gaps between two actions to prevent manipulations from strategical workers (Theorem 3).


%analyze empirical properties, such as the robustness of incentives and the effects of different worker models.
%There have been a very few recent studies that share similar taste with ours.
%For example, to improve the utility of the data requester in crowdsourcing, a multi-armed bandit algorithm is developed in \cite{liu2017sequential} to adjust the state-of-the-art peer prediction mechanism DG13~\cite{dasgupta2013crowdsourced} to a prior-free setting.
%However, both above bandit algorithm and DG13 still need to assume that workers are fully rational.
%Instead of randomly choosing a reference worker as commonly done in peer prediction, Liu and Chen~\cite{liu2017machine} propose to use supervised learning algorithms to generate the reference reports based on the contextual information of tasks and derive the corresponding IC conditions.
%In this paper, without assuming the contextual information about tasks, we use Bayesian inference to learn workers' states and true labels, which is an unsupervised-learning algorithm.
