% !TEX root = main.tex

\section{Related Work}


As previously mentioned, there is a plethora of ML literature; this literature can be peer-reviewed (\ie white literature) or not (\eg technical reports and blog post)~\cite{soldani2019grey}. 

\subsection{Gray literature}
A quick  search on Google using the keywords  ``machine learning'' yields more than 647 million results; if we further specify  the query to include ``machine learning'' AND ``practices,'' the number of hits is reduced to more than 95 million hits, and it gets lower if we search for ``best practices'' instead, which yields about 40 million results instead.  Showing a significant amount of information that could be related to finding good guidelines when developing an ML system. 

Some of the gray literature that can be found when searching for ML practices or guidelines are published by recognized institutions such as SAS~\cite{wujek2016best}, Google~\cite{goole_pair, zinkevich_2021},  or Carnegie Mellon~\cite{horneman2020ai}. In particular, the aforementioned works present a list of practices and guidelines based on the authors' experience. The last two institutions, \cite{zinkevich_2021, horneman2020ai}, present the practices in a broad and general way.  By broad and general,  we mean that the practices are not specific for use cases but general recommendations, such as ``thinking about if the usage of ML/AI is beneficial and necessary or not''. 

Regarding the more detailed practices,  Google~\cite{goole_pair} presents a set of practices related  to designing with AI. The practices are presented in different ways. For example, they are associated with a series of case studies, organized by chapters that are milestones in product development flow, or they are retrieved by questions that guide the practice search. In addition, SAS~\cite{wujek2016best} presents a series of practices that are associated with different stages of the ML development process, \ie data preparation, training, and deployment.  For each of these stages, a brief description of what each stage encompasses is given, followed by a theory of possible approaches, \eg ways to deploy a model. However, this report is missing some  ML pipeline stages like model requirement (\ie the stage  in which  ``designers decide the functionalities that should be included in an ML system, their usefulness for new or existing products''), model evaluation,  data labeling, and model monitoring.


In addition, some of the gray literature covered are pre-prints or papers that are not peer-reviewed, \eg~\cite{MichaelLones2021, Clemmedsson2018}. Lones \etal~\cite{MichaelLones2021} present a series of challenges that they have encountered during their time in academia (\ie teaching and researching). For some of the pitfalls, they present a possible way to handle or avoid the error.  They mainly focus on research and on properties (\ie robustness, reliability) more than stages of the development of the ML system.  Clemmedsson \etal~\cite{Clemmedsson2018}  focus only on ML pitfalls and challenges in an industrial case study where, after a literature review on possible challenges, they surveyed employees in four companies. As a result, they  identify that some challenges are similar to the traditional SE ones, but some of them are only ML-specific challenges, showing that not all the ML challenges can be addressed in the same way as traditional SE challenges and problems. 


\subsection{White literature}

A couple of white literature encompassing some best practices in different fields of knowledge that are not targeted for relating ML and SE exists. For instance, \cite{halilaj2018machine} studies pitfalls and challenges in  biomechanics, followed by some practices to deal with them.  In addition, \cite{teschendorff2019avoiding} also focuses  on pitfalls, but in relation to the use of  ML use in omics data science, and they give guidelines to avoid those pitfalls, without giving details  on how those guidelines are extracted. 

Regarding the white literature that discusses ML and SE, there are a couple of studies~\cite{tantithamthavorn2018experience, breck2017ml, amershi2019software, serban2020adoption,ArpQuiPen_22} but with a different approach than the desired one (\ie ML for SE). Some areas of SE, such as defect modelling, use ML as a tool to achieve its purpose, which in this case would be to \textit{``understand actionable insights in order to make better decisions related to the different phases of software practice''}~\cite{tantithamthavorn2018experience}. And associated with ML use, research has been conducted to find and understand some problems and challenges, also giving active recommendations on how to handle them~\cite{tantithamthavorn2018experience}. However, those guidelines do not only focus on ML but also on different aspects that are associated with the specific ML applications without a clear separation of those two aspects. 
In addition,  those guides are not always associated with other possible SE applications in which they can be applied. This  association is also missing in the study by Arp~\etal~\cite{ArpQuiPen_22}, which identifies ten pitfalls in learning-based security systems and  evaluates their existence in the security literature using ML. They also give recommendations on how to avoid the pitfalls.

The other three aforementioned studies that relate SE  and ML, \cite{breck2017ml, amershi2019software, serban2020adoption}, are the ones that are more closely related to our approach. First, Breck \etal~\cite{breck2017ml}  list 28 practices for testing and monitoring different stages of the ML development process. Amershi \etal~\cite{amershi2019software} conducted a study that reports a broad range of practices and challenges observed  in software teams at Microsoft as they develop AI-based applications. Nevertheless, the set of challenges and practices are broad and often not actionable. They  also focus on a single enterprise (Microsoft).  Finally, Serban \etal~\cite{serban2020adoption} listed  a series of best practices for ML applications.  The presented practices are mined  from academic and gray literature with an engineering perspective, meaning that  the practices are from an engineering point of view and not ML, software engineering for ML (SE4ML).  Serban \etal~\cite{serban2020adoption}  also present a taxonomy of the SE4ML practices, and the taxonomy has  six categories: data, training, coding, deployment, team, and governance. This taxonomy was validated via a  survey in which they asked the respondents (i.e., researchers and practitioners) about the adoption of the  identified  practices on it. The authors also surveyed if adopting a set of practices would lead to a desired effect (\eg agility, software quality, and traceability).  As a result, of their study, they present the list of practices in their article and provide an online tool in which the practices are presented in more detail in the aforementioned taxonomy. 


Since our  goal is to help  reduce the gap of not having a clear handbook of ML practices applied to SE, we want to build on the strengths identified in the related work to accomplish that goal. This means that (i) we focus on the approach of ML for SE, trying to understand the perspective of researchers but also practitioners; (ii) give context to the practices and not only the practice itself; in this way, the practices are actionable and meaningful; (iii) related to the last point we want to provide not only case studies in which the practices are used but also help the interested person to identify what task they are trying to achieve/execute in an ML pipeline.


