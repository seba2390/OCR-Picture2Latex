\section{Current status}  
\label{sec:contr}

This section describes the results achieved so far for each research question in the context of the related work.


Regarding our first research question, in a paper currently under review~\cite{mojicapp}, we used as data source 14 StackExchange websites, including StackOverflow. We decided to use this family of Q\&A because of its popularity in the SE community, which can be seen in multiple studies that have used StackOverflow as a data source to analyze different topics in SE, \eg~\cite{zhang2021study, mondal2023automatic, chatterjee2020finding}. In addition, to its popularity in the SE community, StackOverflow has also been used to study ML-related topics such as expertise and challenges~\cite{Alshangiti_2019}, problems and challenges for ML libraries~\cite{Islam_2019}, and popular deep learning topics~\cite{Han_2020}.


%Regarding our first research question, and taking into account the popularity of communities of questions and answers (Q\&A) like StackOverflow, that has been used in previous studies to analyze different aspects in SE, \eg~\cite{zhang2021study, mondal2023automatic, chatterjee2020finding} and It has also been used to study ML-related topics such as expertise and challenges~\cite{Alshangiti_2019}, problems and challenges for ML libraries~\cite{Islam_2019}, and popular deep learning topics~\cite{Han_2020}.  In a paper currently under review~\cite{mojicapp}, we used not only the StackOverflow website as the main data source, but also 13 other StackExchange websites that were considered as potential data sources where ML practitioners ask and answer questions. 

As a result of selecting Q\&A Stack Exchange websites, filtering posts from them to extract the possible ML practices, and analyzing them, we obtained 157 ML best practices and their taxonomy.  The practices were obtained by executing an open-code procedure in which tags for the different practices were identified and assigned together with the identification of the ML pipeline stage(s) associated with each possible practice. For  stage identification, we used a predefined ML pipeline built by Amershi \etal \cite{amershi2019software}. As a result of the open-coding process, a list of 187 practices was identified, but only 157 were considered  best practices by ML experts after a validation process. 

Another outcome of the open-coding process is a four-level taxonomy. The first level of the taxonomy consists of the 10 ML-pipeline stages proposed by Amershi \etal \cite{amershi2019software}. The second level consists of categories  that encompass multiple tasks for each ML pipeline stage, \eg the learning category in the model training stage. The third level of the taxonomy is composed of an action/task that can be performed in each ML stage. The fourth level is the practice itself.

Upon further analysis of the practices, we identified which ML-pipeline stages had the highest number of practices and which ones had the lowest number of practices.  On the one side, the ML pipeline stages with the highest number of identified practices were model training and data cleaning, which could indicate an interest of practitioners in those two stages. The interest could be related to (i) model training being the core of the ML pipeline, as it enables the use of a model; (ii) data cleaning is a stage where  data scientists spend most of their time~\cite{anacondainc_2022}.   


On the other side, model deployment, model monitoring, and data labeling are the ones with the lowest number of identified practices. Regarding the low number of practices in model deployment and monitoring, this could be due to  these stages are more  related to the “operations” staff~\cite{LewisGrace2021WAIN} (\ie staff in charge of deploying, operating, and monitoring  ML-enabled systems).  Regarding the low number of practices identified for the data labeling stage, it could be related to the intrinsic nature of this stage.  By this, we mean that this stage is inherently  not mandatory in the ML pipeline, as it is only needed when  ground truth is required, \eg supervised and semi-supervised learning. In addition, sometimes, the data used to train models has already been labeled, which could lead to efforts being focused on other ML phases. 

Another  aspect that we noticed when analyzing the identified practices in the Q\&A websites is that they did not cover some specific topics.  Ethics is an example of a topic that was not discussed/covered by the identified practices.  This could indicate that there is a need to explore other sources of information to find ML-best practices, such as  technical blogs like the one presented by IBM~\cite{ibm_ethics}, which presents  an ethical framework for ML. 

When analyzing the validation of the ML experts, we noticed some  aspects to highlight. Firstly,  the majority of the practices considered good were validated by all the experts, which means that there was unanimous agreement. However, 30 practices were rejected by the experts, as only half or less of the experts considered them valid best practices. After inspecting the practices that the ML experts rejected, we noticed that, in most cases, the opinion was divided. This means that most of the time, half of the experts considered that the practices were not good ones, but the other half considered them good ones. This could mean that those practices, with divided opinions, were not well-known, or without a use case scenario, were not clear. In addition, some practices that were considered contradictory, \ie practices that indicate opposite actions, were agreed as good practices by the experts, which could also be an indicator of the need for more context to present an actionable practice. 

Concerning \textit{RQ2}, we are working on a study in which research-track  articles from top SE conferences are being analyzed in order to understand the reported and used ML practices. In this study, we are also taking as a reference the ML pipeline proposed by Amershi \etal \cite{amershi2019software}, for categorizing the identified practices. In preliminary results, we have found that the least mentioned stages, \ie stages in which few practices were identified, are related to model requirements, model deployment, and model monitoring. The last two were expected as it is not common to describe/execute those two stages in a research study. While the first mentioned stage, model requirements, could be considered the basis of a research study that uses ML, as it could define how the models should be built, and not defining it properly could cause disastrous consequences. 


Regarding  \textit{RQ3}, as part of the process of presenting a handbook of practices with both perspectives, practitioners, and researchers, we are currently designing an approach/tool, that will not only be able to be referenced but that will be useful in a practical way. With that, we mean that the practices will be associated and enriched with context, examples, and possible identified limitations.  In addition, this tool should present the aforementioned information in a friendly way, which will allow the users of the tool to find relevant information without going through an entire book, blog, or research article. For that, we are identifying ways  that  practices could be presented in a more interactive way,  like the appendix presented by Serban \etal \cite{serban2020adoption} for the SE practices for ML, the practices presented by Google in their  ``People + AI guided book'' \cite{goole_pair},  the ``Deep Learning Tuning Book''\cite{google_play_book} focused on the process of  model hyperparameter tuning addressed to engineers and researchers. We also take as a reference other white and gray literature aforementioned in the related work that, for each practice, present additional information, such as use cases, \eg~\cite{wujek2016best, ArpQuiPen_22}.



\iffalse


\fi