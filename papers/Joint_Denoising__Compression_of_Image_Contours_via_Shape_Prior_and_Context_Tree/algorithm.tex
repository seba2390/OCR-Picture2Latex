\begin{figure*}[!t]

% The spacer can be tweaked to stop underfull vboxes.

% ensure that we have normalsize text
\normalsize
% Store the current equation number.
\setcounter{mytempeqncnt}{\value{equation}}
% Set the equation number to one less than the one
% desired for the first equation here.
% The value here will have to changed if equations
% are added or removed prior to the place these
% equations are referenced in the main text.
\setcounter{equation}{25}

\begin{small}

\begin{equation}
G_i(\hat{\vec{x}}_{i-D}^{i-1},\vec{e},j-1) = \underset{\hat{x}_i \in \mathcal{A}}{\min}
\begin{cases}
f(\hat{\vec{x}}_{i-D}^{i}) ~+~ 
\mathbf{1}(j < L_{\vec{y}})\; G_{i+1}(\hat{\vec{x}}_{i-D+1}^{i}, v(\vec{e},\hat{x}_i),j) , & \text{if}\;\hat{x}_i = y_j \\
(c_0+c_2) + f(\hat{\vec{x}}_{i-D}^{i}) ~+~ B_{i+1}(\hat{\vec{x}}_{i-D+1}^{i}, v(\vec{e},\hat{x}_i),j), & \text{otherwise}
\end{cases}
\label{eq:good_recursion}
\end{equation}

\begin{equation}
B_i(\hat{\vec{x}}_{i-D}^{i-1},\vec{e},j-1)= \underset{\hat{x}_i \in \mathcal{A}}{\min}
\begin{cases}
c_2(k-j)~+~f(\hat{\vec{x}}_{i-D}^{i}) ~+~ G_{i+1}(\hat{\vec{x}}_{i-D+1}^{i}, v(\vec{e},\hat{x}_i),k), & \text{if}\;\exists k, v(\vec{e},\hat{x}_i) = \vec{e}_{\vec{y}}(k) \\
c_1 ~+~ f(\hat{\vec{x}}_{i-D}^{i}) ~+~ B_{i+1}(\hat{\vec{x}}_{i-D+1}^{i}, v(\vec{e},\hat{x}_i),j), & \text{otherwise}
\end{cases}
\label{eq:bad_recursion}
\end{equation}

\end{small}

% IEEE uses as a separator
\hrulefill

% Restore the current equation number.
\setcounter{equation}{\value{mytempeqncnt}}

\vspace*{4pt}

\end{figure*}

%We now present an optimization algorithm to minimize (\ref{eq:objective}).
Before processing the detected contours in the target image, a context tree $\mathcal{T}$ is first computed by PPM using a set of training images as discussed in Section\;\ref{sec:rate}.
The rate term $R(\hat{\vec{x}})$ in (\ref{eq:objective}) is computed using $\mathcal{T}$. 
We describe a dynamic programming (DP) algorithm to minimize (\ref{eq:objective}) optimally, then analyze its complexity. 
Finally, we design a total suffix tree (TST) to reduce the complexity.


\subsection{Dynamic Programming Algorithm}
\label{subsec:algorithm_dynamic}

To simplify the expression in the objective, we define a local cost term $f(\hat{\vec{x}}_{i-D}^{i})$ combining the prior and the rate term as:
\begin{equation}
f(\hat{\vec{x}}_{i-D}^{i}) = \beta s(\hat{\vec{x}}_{i-D_s}^i) - \lambda \log_2 P(\hat{x}_i|\hat{\vec{x}}_{i-D}^{i-1}).
\label{eq:local_cost}
\end{equation}
Note that the maximum depth $D=\ceil{\ln{L}/\ln{3}}$ of $\mathcal{T}$ is much larger than $D_s$.
We assume here that the first $D$ estimated symbols $\hat{\vec{x}}^D_1$ are the observed $\vec{y}^D_1$, and that the last estimated edge is correct, \textit{i.e.}, $\vec{e}_{\hat{\vec{x}}}(L_{\hat{\vec{x}}})=\vec{e}_{\vec{y}}(L_{\vec{y}})$. 


% The previous equation was number six.
% Account for the double column equations here.
\addtocounter{equation}{2}

In summary, our algorithm works as follows. 
As we examine each symbol in observed $\mathbf{y}$, we identify an ``optimal" state traversal through our 3-state Markov model---one that minimizes objective (\ref{eq:objective})---via two recursive functions. The optimal state traversal translates directly to an estimated DCC string $\hat{\vec{x}}$, which is the output of our algorithm.

Denote by $G_i(\hat{\vec{x}}_{i-D}^{i-1},\vec{e},j-1)$ the minimum cost for estimated $\hat{\vec{x}}$ from the $i$-th symbol onwards, given that we are in the good state with a set of $D$ previous symbols (context) $\hat{\vec{x}}^{i}_{i-D}$, and last edge is $\vec{e}$ which is the same as the $(j-1)$-th edge  in observed $\vec{y}$. 
If we select one additional symbol $\hat{x}_i = y_j$, then we remain in the good state, incurring a local cost $f(\hat{\vec{x}}^i_{i-D})$, plus a recursive cost $G_{i+1}(\,)$ for the remaining symbols in string $\hat{\vec{x}}$ due to new context $\hat{\vec{x}}^i_{i-D+1}$. 

If instead we choose one additional symbol $\hat{x}_i \neq y_j$, then we start a new burst error event, incurring a local cost $(c_0 + c_2)$ for the new event, in addition to $f(\hat{\vec{x}}^i_{i-D})$.
Entering bad states, we use $B_{i+1}(\,)$ for recursive cost instead. 

$B_i(\hat{\vec{x}}_{i-D}^{i-1},\vec{e},j-1)$ is similarly computed as $G_i(\hat{\vec{x}}_{i-D}^{i-1},\vec{e},j-1)$, except that if selected symbol $\hat{x}_i$ has no corresponding edge in $\vec{y}$, then we add $c_1$ to account for an additional error symbol. 
If selected symbol $\hat{x}_i$ has a corresponding edge in $\vec{y}$, then this is the end of the burst error event, and we return to good state (recursive call to $G_{i+1}(\,)$ instead). In this case, we must account for the change in length between $\hat{\vec{x}}$ and $\vec{y}$ due to this burst error event, weighted by $c_2$. 
$G_i(\hat{\vec{x}}_{i-D}^{i-1},\vec{e},j-1)$ and $B_i(\hat{\vec{x}}_{i-D}^{i-1},\vec{e},j-1)$ are defined in (\ref{eq:good_recursion}) and (\ref{eq:bad_recursion}), respectively. 
$\mathbf{1}(\mathbf{c})$ is an indicator function as defined earlier. 
$\vec{e}=\{(m,n),d\}$ denotes the $(i-1)$-th edge of $\hat{\vec{x}}$, and $v(\vec{e},\hat{x}_i)$ denotes the next edge given that the next symbol is $\hat{x}_i$. $j-1$ is the index of matched edge in $\vec{y}$.

Because we assume that the denoised DCC string $\hat{\vec{x}}$ is no longer than the observed $\vec{y}$, if $i>L_{\vec{y}}$ or $k-j < 0$, we stop the recursion and return infinity to signal an invalid solution.


\subsection{Complexity Analysis}
\label{subsec:algorithm_complexity}


The complexity of the DP algorithm is bounded by the size of the DP tables times the complexity of computing each table entry. 
Denote by $Q$ the number of possible edge endpoint locations.
Looking at the arguments of the two recursive funcions $G_i(\,)$ and $B_i(\,)$, we see that DP table size is $O(3^D \,  4Q \, L_{\vec{y}}^2)$. 
To compute each entry in $B_i(\,)$, for each $\hat{\vec{x}}_i \in \mathcal{A}$,  one must check for matching edge in $\vec{y}$, hence the complexity is $O(3 L_{\vec{y}})$. 
Hence the total complexity of the algorithm is $O(3^D Q L_{\vec{y}}^3)$, which is polynomial time with respect to $L_{\vec{y}}$.


\begin{figure}[tb]

\begin{minipage}[b]{1\linewidth}
  \centering
  \centerline{\includegraphics[width=8cm]{augment_context_tree-eps-converted-to.pdf}}
%  \vspace{1.5cm}
  \centerline{}\medskip
\end{minipage}

\vspace{-0.3cm}
\caption{An example of \textit{total suffix tree} (TST) derived from the context tree in Fig.\;\ref{fig:MCT_statespace}. End nodes in gray are added nodes based on the context tree. All the end nodes construct a TST: $\mathcal{T}_s^{\ast}=\{\texttt{l},\texttt{ls},\texttt{lr},\texttt{sl},\texttt{sls},\texttt{slr},\texttt{ss},\texttt{sr},\texttt{rl},\texttt{r},\texttt{rr}\}$.}
\label{fig:suffix_tree}
\end{figure}


\subsection{Total Suffix Tree}
\label{subsec:algorithm_suffix}

When the training data is large, $D$ is also large, resulting in a very large DP table size due to the exponential term $3^D$.
%Looking at (\ref{eq:local_cost}), the exponential complexity $3^D$ is decided by the rate term since $D_s$ is usually much smaller than $D$.
In (\ref{eq:good_recursion}) and (\ref{eq:bad_recursion}) when calculating local cost $f(\hat{\vec{x}}_{i-D}^{i})$, actually the context required to compute rate is $\mathbf{w}=\hat{\mathbf{x}}_{i-|\mathbf{w}|}^{i-1}$, where the context length $|\mathbf{w}|$ is typically smaller than $D$ because the context tree $\mathcal{T}$ of maximum depth $D$ is variable-length.
Thus, if we can, at appropriate recursive calls, reduce the ``history" from $\hat{\mathbf{x}}^{i}_{i+1-D}$ of length $D$ to $\hat{\mathbf{x}}^{i}_{i+1-k}$ of length $k$, $k < D$, for recursive call to $G_{i+1}()$ or $B_{i+1}$ in (\ref{eq:good_recursion}) or (\ref{eq:bad_recursion}), then we can reduce the DP table size and in turn the computation complexity of the DP algorithm.

The challenge is how to retain or ``remember" just enough previous symbols $\hat{x}_{i}, \hat{x}_{i-1}, \ldots$ during recursion so that the right context $\mathbf{w}$ can still be correctly identified to compute rate at a later recursive call.
The solution to this problem can be described simply.
Let $\mathbf{w}$ be a context (context node) in context tree $\mathcal{T}$.
A \textit{suffix} of length $k$ is the first $k$ symbols of $\mathbf{w}$, \textit{i.e.}, $\mathbf{w}^k_1$.
Context $\mathbf{w}$ at a recursive call must be a concatenation of a chosen $i$-th symbol $\hat{x}_i = w_{|\mathbf{w}|}$ during a previous recursive call $G_i()$ or $B_i()$ and suffix $\mathbf{w}^{|\mathbf{w}|-1}_1$.
It implies that suffix $\mathbf{w}^{|\mathbf{w}|-1}_1$ must be retained during the recursion in (\ref{eq:good_recursion}) or (\ref{eq:bad_recursion}) for this concatenation to $\mathbf{w}$ to take place at a later recursion.
To concatenate to suffix $\mathbf{w}^{|\mathbf{w}|-1}_1$ at a later recursive call, one must retain its suffix $\mathbf{w}^{|\mathbf{w}|-2}_1$ at an earlier call.
We can thus generalize this observation and state that \textit{a necessary and sufficient condition to preserve all contexts $\mathbf{w}$ in context tree $\mathcal{T}$ is to retain all suffixes of $\mathbf{w}$ during the recursion.}

All suffixes of contexts in $\mathcal{T}$ can be drawn collectively as a tree as follows. 
For each suffix $\mathbf{s}$, we trace $\mathbf{s}$ down from the root of a tree according to the symbols in $\mathbf{s}$, creating additional nodes if necessary. 
We call the resulting tree a \textit{total suffix tree} (TST)\footnote{An earlier version of TST was proposed in \cite{zheng17} for a different contour coding application.}, denoted as $\mathcal{T}_s$.
By definition, $\mathcal{T}$ is a sub-tree of $\mathcal{T}_s$.
Further, $\mathcal{T}_s$ is essentially a concatenation of all sub-trees of $\mathcal{T}$ at root. 
Assuming $\mathcal{T}$ has $K$ contexts, each of maximum length $D$. 
Each context can induce $O(D)$ additional context nodes in TST $\mathcal{T}_s$. 
Hence TST $\mathcal{T}_s$ has $O(K D)$ context nodes.

Fig.\;\ref{fig:suffix_tree} illustrates one example of TST derived from the context tree shown in Fig.\;\ref{fig:MCT_statespace}.
TST $\mathcal{T}_s$ can be used for compact DP table entry indexing during recursion (\ref{eq:good_recursion}) and (\ref{eq:bad_recursion}) as follows. 
When an updated history $\hat{\mathbf{x}}_{i+1-D}^{i}$ is created from a selection of symbol $\hat{x}_i$, we first truncate $\hat{\mathbf{x}}_{i+1-D}^{i}$ to $\hat{\mathbf{x}}_{i+1-k}^{i}$, where $\hat{\mathbf{x}}_{i+1-k}^{i}$ is the longest matching string in $\mathcal{T}_s$ from root node down. 
%Because TST $\mathcal{T}_s$ is a full tree, the longest matching string always corresponds to an end node. 
The shortened history $\hat{\mathbf{x}}_{i+1-k}^{i}$ is then used as the new argument for the recursive call.
Practically, it means that only DP table entries of arguments $\hat{\mathbf{x}}_{i+1-k}^i$ that are context nodes of TST $\mathcal{T}_s$ will be indexed.
Considering the complexity induced by the prior term, the complexity is thus reduced from original $O(3^D Q L_{\vec{y}}^3)$ to $O( (K D+ 3^{D_s}) Q L_{\vec{y}}^3)$, which is now polynomial in $D$.