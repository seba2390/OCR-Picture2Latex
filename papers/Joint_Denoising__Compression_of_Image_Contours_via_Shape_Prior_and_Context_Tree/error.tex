\begin{figure}[tb]

\begin{minipage}[b]{.48\linewidth}
  \centering
  \centerline{\includegraphics[width=4cm]{burst_example-eps-converted-to.pdf}}
%  \vspace{1.5cm}
  \centerline{(a)}\medskip
\end{minipage}
\hfill
\begin{minipage}[b]{0.48\linewidth}
  \centering
  \centerline{\includegraphics[width=3cm]{markov_process-eps-converted-to.pdf}}
%  \vspace{1.5cm}
  \centerline{(b)}\medskip
\end{minipage}

\vspace{-0.2cm}
\caption{{(a) The two red squares are the erred pixels. Observed $\vec{y}$ is composed of black solid edges (good states) and green solid edges (bad states). The ground truth $\vec{x}$ is composed of black solid edges and black dotted edges. (b) A three-state Markov model.}}
\label{fig:burst_example}
\end{figure}


We first rewrite the posterior $P(\vec{x} | \vec{y})$ in (\ref{eq:lagrangian_objective}) using Bayes' Rule: 
\begin{equation}
P(\vec{x}|\vec{y}) = \frac{P(\vec{y}|\vec{x}) P(\vec{x})}{P(\vec{y})}
\end{equation}
where $P(\vec{y}|\vec{x})$ is the likelihood of observing DCC string $\vec{y}$ given ground truth $\vec{x}$, and $P(\vec{x})$ is the prior which describes \textit{a priori} knowledge about the target DCC string.
We next describe an error model for DCC strings, then define likelihood $P(\vec{y} | \vec{x})$ and prior $P(\vec{x})$ in turn.


%\begin{figure*}[!t]
%
%
%% The spacer can be tweaked to stop underfull vboxes.
%
%% ensure that we have normalsize text
%\normalsize
%% Store the current equation number.
%\setcounter{mytempeqncnt}{\value{equation}}
%% Set the equation number to one less than the one
%% desired for the first equation here.
%% The value here will have to changed if equations
%% are added or removed prior to the place these
%% equations are referenced in the main text.
%\setcounter{equation}{8}
%
%\begin{equation}
%\label{eq:straight}
%s(\vec{w})=\underset{1 \leq k \leq L_{\vec{w}}}{\max}\left\{\frac{|(m_k-m_0)(n_{L_{\vec{w}}}-n_0)-(n_k - n_0)(m_{L_{\vec{w}}}-m_0) |}{\sqrt{(m_{L_{\vec{w}}}-m_0)^2+(n_{L_{\vec{w}}}-n_0)^2}}  \right\}
%\end{equation}
%
%% IEEE uses as a separator
%\hrulefill
%
%% Restore the current equation number.
%\setcounter{equation}{\value{mytempeqncnt}}
%
%\vspace*{4pt}
%
%\end{figure*}


\subsection{Error Model for DCC String}
\label{subsec:error_model}

Assuming that pixels in an image are corrupted by a small amount of independent and identically distributed (iid) noise, a detected contour will occasionally be shifted from the true contour by one or two pixels. 
However, the computed DCC string from the detected contour will experience a sequence of wrong symbols---a \textit{burst error}. 
This is illustrated in Fig.\;\ref{fig:burst_example}(a), where the left single erred pixel (in red) resulted in two erred symbols in the DCC string. The right single error pixel also resulted in a burst error in the observed string, which is \textit{longer} than the original string. Based on these observations, we propose our DCC string error model as follows.

We define a \textit{three-state Markov model} as illustrated in Fig.\;\ref{fig:burst_example}(b) to model the probability of observing DCC string $\vec{y}$ given original string $\vec{x}$. 
State \texttt{0} is the good state, and \textit{burst error state} \texttt{1} and \textit{burst length state} \texttt{2} are the bad states. 
$p$, $q_1$ and $q_2$ are the transition probabilities from state \texttt{0} to \texttt{1}, \texttt{1} to \texttt{2}, and \texttt{2} to \texttt{0}, respectively.
%, which can be estimated from training data.
Note that state \texttt{1} cannot transition directly to \texttt{0}, and likewise state \texttt{2} to \texttt{1} and \texttt{0} to \texttt{2}.

Starting at good state \texttt{0}, each journey to state \texttt{1} then to \texttt{2} then back to \texttt{0} is called a \textit{burst error event}.
From state \texttt{0}, each self-loop back to \texttt{0} with probability $1-p$ means that the next observed symbol $y_i$ is the same as $x_i$ in original $\vec{x}$. 
A transition to burst error state \texttt{1} with probability $p$, and each subsequent self-loop with probability $1-q_1$, mean observed $y_i$ is now different from $x_i$.
A transition to burst length state \texttt{2} then models the \textit{length increase} in observed $\vec{y}$ over original $\vec{x}$ due to this burst error event: the number of self-loops taken back to state \texttt{2} is the increase in number of symbols. 
A return to good state \texttt{0} signals the end of this burst error event. 


\subsection{Likelihood Term}
\label{subsec:error_likelihod}

Given the three-state Markov model, we can compute likelihood $P(\vec{y} | \vec{x})$ as follows. 
For simplicity, we assume that $\vec{y}$ starts and ends at good state \texttt{0}. 
Denote by $K$ the total number of burst error events in $\vec{y}$ given $\vec{x}$. 
Further, denote by $l_1(k)$ and $l_2(k)$ the number of visits to state \texttt{1} and \texttt{2} respectively during the $k$-th burst error event.
Similarly, denote by $l_0(k)$ the number of visits to state \texttt{0} \textit{after} the $k$-th burst error event. 
We can then write the likelihood $P(\vec{y}|\vec{x})$ as:

\vspace{-0.1in}
\begin{small}
\begin{equation}
\begin{split}
&(1-p)^{l_0(0)} \prod^{K}_{k=1} 
p(1-q_1)^{l_1(k)-1} q_1(1-q_2)^{l_2(k)-1}q_2 (1-p)^{l_0(k)-1} 
\end{split}
\label{eq:likelihood}
\end{equation}
\end{small}
%\red{missing the last series of good state at the end?} \blue{There are $K+1$ series of good state in total. $(1-p)^{l_0(1)}$ denotes the first series and $(1-p)^{l_0(k+1)-1}$ denotes the following $K$ series.}

\vspace{-0.15in}
For convenience, we define the total number of visits to state \texttt{0}, \texttt{1} and \texttt{2} as $\Gamma=\sum_{k=0}^{K} l_0(k)$, $\Lambda=\sum_{k=1}^K l_1(k)$ and $\Delta=\sum_{k=1}^K l_2(k)$, respectively.
We can then write the negative log of the likelihood as:
\begin{equation}
\label{eq:negaLogLikelihood}
\begin{split}
&-\log{P(\vec{y}|\vec{x})} =\\
& - K(\log{p}+\log{q_1}+\log{q_2}) -(\Gamma-K)\log{(1-p)} \\
&  - (\Lambda-K)\log{(1-q_1)} - (\Delta-K)\log{(1-q_2)}
\end{split}
\end{equation}

Assuming that burst errors are rare events, $p$ is small and $\log(1-p) \approx 0$. Hence:
\begin{equation}
\label{eq:negaLogLikelihoodAppro}
\begin{split}
& -\log{P(\vec{y}|\vec{x})} \\
& \approx  - K(\log{p}+\log{q_1}+\log{q_2}) \\ 
& - (\Lambda-K)\log{(1-q_1)} - (\Delta-K)\log{(1-q_2)} \\
& = - K(\underbrace{\log{p}+\log{\frac{q_1}{1-q_1}}+\log{\frac{q_2}{1-q_2}}}_{-c_0}) \\
& - \Lambda \underbrace{\log(1-q_1)}_{-c_1} -
\Delta \underbrace{\log(1-q_2)}_{-c_2}
\end{split}
\end{equation}
%Since $p$ is small, $-(\log{p}+\log{\frac{q_1}{1-q_1}}+\log{\frac{q_2}{1-q_2}})$ is a positive number. 
Thus $-\log P(\vec{y}|\vec{x})$ simplifies to:
\begin{equation}
\label{eq:negaLogLikelihoodApproSimple}
\begin{split}
& -\log{P(\vec{y}|\vec{x})}  \approx  (c_0 + c_2) K + c_1 \Lambda 
+ c_2 \Delta'
\end{split}
\end{equation}
%\red{the number of visits to state \texttt{2} is not exactly the same as length increase.} \blue{For each burst error, the number of visits to state \texttt{2} equals to 1 + the length increase. We always have state \texttt{2} even if the length doesn't increase.}
where $\Delta' = \Delta - K$ is the length increase in observed $\vec{y}$ compared to $\vec{x}$ due to the $K$ burst error events\footnote{Length increase of observed $\vec{y}$ due to $k$-th burst error event is $l_2(k) - 1$.}.
(\ref{eq:negaLogLikelihoodApproSimple}) states that the negative log of the likelihood is a linear sum of three terms: i) the number of burst error events $K$; ii) the number of error corrupted symbols $\Lambda$; and iii) the length increase $\Delta'$ in observed string $\vec{y}$.
This agrees with our intuition that more error events, more errors and more deviation in DCC length will result in a larger objective value in (\ref{eq:lagrangian_objective}).
We will validate this error model empirically in our experiments in Section \ref{sec:results}. 


\subsection{Prior Term}
\label{subsec:error_prior}

Similarly to \cite{daribo14,zheng17}, we propose a \textit{geometric shape prior} based on the assumption that contours in natural images tend to be more straight than curvy.
%The geometric prior models the underlying signal which is independent of the contour data.
Specifically, we write prior $P(\vec{x})$ as:
\begin{equation}
P(\vec{x}) = \exp\left\{- \beta \sum\limits_{i=D_s+1}^{L_{\vec{x}}} s(\vec{x}_{i-D_s}^{i})\right\}
\label{eq:geometric_prior}
\end{equation}
where $\beta$ and $D_s$ are parameters. 
$s(\vec{x}_{i-D_s}^i)$ measures the \textit{straightness} of DCC sub-string $\vec{x}_{i-D_s}^i$.
Let $\vec{w}$ be a DCC string of length $D_s+1$, \textit{i.e.}, $L_{\vec{w}}=D_s+1$. 
Then $s(\vec{w})$ is defined as the \textit{maximum Euclidean distance} between any coordinates of edge $\vec{e}_{\vec{w}}(k), 0\leq k \leq L_{\vec{w}}$ and the line connecting the first point $(m_0,n_0)$ and the last point $(m_{L_{\vec{w}}}, n_{L_{\vec{w}}})$ of $\vec{w}$ on the 2D grid.
We can write $s(\vec{w})$ as:

\vspace{-0.1in}
\begin{small}
\begin{equation}
\label{eq:straight}
\begin{split}
& \underset{0\! \leq k \leq L_{\vec{w}}}{\max}\left\{\!\frac{|(m_k\!-\!m_0)(n_{L_{\vec{w}}}\!-\!n_0)\!-\!(n_k\! -\! n_0)(m_{L_{\vec{w}}}\!-\!m_0) |}{\sqrt{(m_{L_{\vec{w}}}-m_0)^2+(n_{L_{\vec{w}}}-n_0)^2}} \! \right\}
\end{split}
\end{equation}
\end{small}

Since we compute the sum of a straightness measure for overlapped sub-strings, the length of each sub-string $\vec{x}_{i-D_s}^i$ should not be too large to capture the local contour behavior.
Thus, we choose $D_s$ to be a fixed small number in our implementation. 
Some examples of $s(\vec{w})$ are shown in Fig.\;\ref{fig:prior}. 


%% The previous equation was number six.
%% Account for the double column equations here.
%\addtocounter{equation}{1}

\begin{figure}[h]

\begin{minipage}[b]{.32\linewidth}
  \centering
  \centerline{\includegraphics[width=1.5cm]{prior_a-eps-converted-to.pdf}}
%  \vspace{1.5cm}
  \centerline{(a)}\medskip
\end{minipage}
\hfill
\begin{minipage}[b]{0.32\linewidth}
  \centering
  \centerline{\includegraphics[width=1.3cm]{prior_b-eps-converted-to.pdf}}
%  \vspace{1.5cm}
  \centerline{(b)}\medskip
\end{minipage}
\hfill
\begin{minipage}[b]{0.32\linewidth}
  \centering
  \centerline{\includegraphics[width=2.8cm]{prior_c-eps-converted-to.pdf}}
%  \vspace{1.5cm}
  \centerline{(c)}\medskip
\end{minipage}

\vspace{-0.3cm}
\caption{Three examples of the straightness of $s(\vec{w})$ with $L_{\vec{w}}=4$. (a) $\vec{w}=rrls$ and $s(\vec{w})=4\sqrt{5}/5$. (b) $\vec{w}=lrlr$ and $s(\vec{w})=6\sqrt{13}/13$. (c) $\vec{w}=ssss$ and $s(\vec{w})=0$.}
\label{fig:prior}
\end{figure}

Combining the likelihood and prior terms, we get the negative log of the posterior,
\begin{equation}
\label{eq:posterior}
\begin{split}
 -\log P(\hat{\vec{x}}|\vec{y})= & (c_0 + c_2) K + c_1 \Lambda + c_2 \Delta' \\
& +\beta \sum\limits_{i=D_s+1}^{L_{\hat{\vec{x}}}} s(\hat{\vec{x}}_{i-D_s}^i)
\end{split}
\end{equation}
