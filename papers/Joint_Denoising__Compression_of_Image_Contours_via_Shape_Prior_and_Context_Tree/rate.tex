We losslessly encode a chosen DCC string $\hat{\vec{x}}$ using arithmetic coding \cite{DCC1st1991}. 
Specifically, to implement arithmetic coding using local statistics, each DCC symbol $\hat{x}_i \in \mathcal{A}$ is assigned a conditional probability $P(\hat{x}_i|\hat{\vec{x}}_{1}^{i-1})$ given its all previous symbols $\hat{\vec{x}}_{1}^{i-1}$.
%The conditional probabilities are input to an arithmetic coder for entropy coding.
The rate term $R(\hat{\vec{x}})$ is thus approximated as the summation of negative log of conditional probabilities of all symbols in $\hat{\vec{x}}$:
\begin{equation}
\label{eq:rate}
R(\hat{\vec{x}})= - \sum\limits_{i=1}^N \log_2 P(\hat{x}_i|\hat{\vec{x}}_{1}^{i-1}) 
\end{equation}

To compute $R(\hat{\vec{x}})$, one must assign conditional probabilities $P(\hat{x}_i|\hat{\vec{x}}_{1}^{i-1})$ for all symbols $\hat{x}_i$.
In our implementation, we use the variable-length context tree model \cite{begleiter2004prediction} to compute the probabilities.
Specifically, to code contours in the target image, one context tree is trained using contours in a set of training images\footnote{For coding contours in the target frame of a video, the training images are the earlier coded frames.} which have correlated statistics with the target image.
Next we introduce the context tree model, then discuss our construction of a context tree using \textit{prediction by partial matching} (PPM)\footnote{While the computation of conditional probabilities are from PPM, our construction of a context tree is novel and stands as one key contribution in this paper.} \cite{moffat1990implementing}.

\begin{figure}[t]

\begin{minipage}[b]{1\linewidth}
  \centering
  \centerline{\includegraphics[width=8 cm]{MCT_statespace_simple-eps-converted-to.pdf}}
  %\vspace{0.1cm}
  \centerline{}
  %\medskip
\end{minipage}

\vspace{-0.2cm}
\caption{An example of context tree.
Each node is a sub-string and the root node is an empty sub-string.
The contexts are all the end nodes on $\mathcal{T}$: $\mathcal{T}=\{\texttt{l},\texttt{sl},\texttt{sls},\texttt{slr},\texttt{ss},\texttt{sr},\texttt{rl},\texttt{r},\texttt{rr}\}$.}
\label{fig:MCT_statespace}
\end{figure}


\subsection{Definition of Context Tree}
\label{subsec:contextTree}

We first define notations related to the contours in the set of training images.
Denote by $\vec{x}(m)$, $1 \leq m \leq M$, the $m$-th DCC string in the training set $\mathcal{X} = \{\vec{x}(1),\ldots,\vec{x}(M)\}$, where $M$ denotes the total number of DCC strings in $\mathcal{X}$. The total number of symbols in $\mathcal{X}$ is denoted by $L = \sum^{M}_{m=1}L_{\vec{x}(m)}$. 
Denote by $\vec{u}\vec{v}$ the concatenation of sub-strings $\vec{u}$ and $\vec{v}$.

We now define $N(\vec{u})$ as the number of occurrences of sub-string $\vec{u}$ in the training set $\mathcal{X}$. $N(\vec{u})$ can be computed as:
\begin{equation}
N(\vec{u})=\sum^{M}_{m=1}\sum_{i=1}^{L_{\vec{x}(m)} - |\vec{u}| + 1}
%1_{[\vec{x}(j)^{i+|\vec{u}|-1}_{i}=\vec{u}]} 
\vec{1} \left(
\vec{x}(m)_i^{i+|\vec{u}|-1} = \vec{u}
\right) 
\end{equation}
%where $\vec{u} \in \stackrel [k=1] {+\infty}{\cup} \mathcal{D}^{k}$. 
where $\vec{1}(\vec{c})$ is an indicator function that evaluates to $1$ if the specified binary clause $\vec{c}$ is true and $0$ otherwise.

Denote by $P(x|\vec{u})$ the conditional probability of symbol $x$ occurring given its previous sub-string is $\vec{u}$, where $x\in\mathcal{A}$. Given training data $\mathcal{X}$, $P(x|\vec{u})$ can be estimated using $N(\vec{u})$ as done in \cite{buhlmann1999variable},
\begin{equation}
\begin{array}{cc}
P(x|\vec{u})=\frac{N(x\vec{u})}{\delta +  N(\vec{u})}
\end{array}
\label{eq:cal_prob}
\end{equation}
where $\delta$ is a chosen parameter for different models.
%\red{u mean a parameter? what's the purpose of this parameter?}
%\blue{this parameter is used in PPM.}

Given $\mathcal{X}$, we learn a context model to assign a conditional probability to any symbol given its previous symbols in a DCC string. Specifically, to calculate the conditional probability $P(\hat{x}_i|\hat{\vec{x}}^{i-1}_{1})$, the model determines a \textit{context} $\vec{w}$ to calculate $P(\hat{x}_i|\vec{w})$, where $\vec{w}$ is a \textit{prefix} of the sub-string $\hat{\vec{x}}^{i-1}_{1}$, \textit{i.e.}, $\vec{w} = \hat{\vec{x}}^{i-1}_{i-l}$ for some context length $l$:
\begin{equation}
P(\hat{x}_i|\hat{\vec{x}}^{i-1}_{1}, \text{context model})=P(\hat{x}_i|\vec{w})
\label{eq:conditionalProbContextModel}
\end{equation}
$P(\hat{x}_i|\vec{w})$ is calculated using (\ref{eq:cal_prob}) given $\mathcal{X}$. 
The context model determines a unique context $\vec{w}$ of finite length for every possible past $\hat{\vec{x}}^{i-1}_{1}$. The set of all mappings from $\hat{\vec{x}}^{i-1}_{1}$ to $\vec{w}$ can be represented compactly as a context tree.

Denote by $\mathcal{T}$ the context tree, where $\mathcal{T}$ is a \textit{ternary tree}: each node has at most three children. 
The root node has an empty sub-string, and each child node has a sub-string $\vec{u}x$ that is a concatenation of: i) its parent's sub-string $\vec{u}$ if any, and ii) the symbol $x$ (one of \texttt{l}, \texttt{s} and \texttt{r}) representing the link connecting the parent node and itself in $\mathcal{T}$. 
An example is shown in Fig.\;\ref{fig:MCT_statespace}.
The contexts of the tree $\mathcal{T}$ are the sub-strings of the \textit{context nodes}---nodes that have at most two children, \textit{i.e.}, the end nodes and the intermediate nodes with fewer than three children. 
Note that $\mathcal{T}$ is completely specified by its set of context nodes and vice versa.
For each $\hat{\vec{x}}^{i-1}_{1}$, a context $\vec{w}$ is obtained by traversing $\mathcal{T}$ from the root node to the deepest context node, matching symbols $\hat{x}_{i-1}, \hat{x}_{i-2}, \ldots$ into the past.
We can then rewrite (\ref{eq:conditionalProbContextModel}) as follows:
\begin{equation}
P(\hat{x}_i|\hat{\vec{x}}^{i-1}_{1}, \mathcal{T})=P(\hat{x}_i|\vec{w}).
\label{eq:conditionalProbContextTree}
\end{equation}


\subsection{Construction of Context Tree by Prediction by Partial Matching (PPM)}
\label{subsec:PPM}

The PPM algorithm is considered to be one of the best lossless compression algorithms \cite{begleiter2004prediction}, which is based on the context tree model.
Using PPM, all the possible sub-strings $\vec{w}$ with non-zero occurrences in $\mathcal{X}$, \textit{i.e.}, $N(\vec{w}) > 0$, are contexts on the context tree. 
The key idea of of PPM is to deal with the \textit{zero frequency} problem when estimate $P(\hat{x}_i|\vec{w})$, where sub-string $\hat{x}_i\vec{w}$ does not occur in $\mathcal{X}$, \textit{i.e.}, $N(\hat{x}_i\vec{w})=0$.
In such case, using (\ref{eq:cal_prob}) to estimate $P(\hat{x}_i|\vec{w})$ would result in zero probability, which cannot be used for arithmetic coding.
When $N(\hat{x}_i\vec{w})=0$, $P(\hat{x}_i|\vec{w})$ is estimated instead by reducing the context length by one, \textit{i.e.}, $P(\hat{x}_i|\vec{w}_2^{|\vec{w}|})$.
If sub-string $\hat{x}_i \vec{w}_2^{|\vec{w}|}$ still does not occur in $\mathcal{X}$, the context length is further reduced until symbol $\hat{x}_i$ along with the shortened context occurs in $\mathcal{X}$. 
Let $\mathcal{A}_{\vec{w}}$ be an alphabet in which each symbol along with the context $\vec{w}$ occurs in $\mathcal{X}$, \textit{i.e.}, $\mathcal{A}_{\vec{w}}=\{x|N(x\vec{w}) > 0, x \in \mathcal{A}\}$.
Based on the PPM implemented in \cite{moffat1990implementing}, $P(\hat{x}_i|\vec{w})$ is computed using the following (recursive) equation:
\begin{equation}
P(\hat{x}_i|\vec{w}) = 
\begin{cases}
\frac{N(\hat{x}_i \vec{w})}{|\mathcal{A}_{\vec{w}}| + N(\vec{w})},&\text{if } \hat{x}_i \in \mathcal{A}_{\vec{w}} \\
\frac{|\mathcal{A}_{\vec{w}}|}{|\mathcal{A}_\vec{w}| + N(\vec{w})} \cdot P(\hat{x}_i|\vec{w}_2^{|\vec{w}|}),&\text{otherwise}
\end{cases}.
\label{eq:ppm_prob}
\end{equation}
%where $\frac{|\mathcal{A}_{\vec{w}}|}{|\mathcal{A}_\vec{w}| + N(\vec{w})}$ is a factor. 
%\red{what do u mean by factor?}

To construct $\mathcal{T}$, we traverse the training data $\mathcal{X}$ once to collect statistics for all potential contexts.
Each node in $\mathcal{T}$, \textit{i.e.}, sub-string $\mathbf{u}$, has three counters which store the number of occurrences of sub-strings $l\mathbf{u}$, $s\mathbf{u}$ and $r\mathbf{u}$, \textit{i.e.}, $N(l\mathbf{u})$, $N(s\mathbf{u})$ and $N(r\mathbf{u})$.
To reduce memory requirement, we set an upper bound $D$ on the maximum depth of $\mathcal{T}$.
As done in \cite{rissanen1983universal}, we choose the maximum depth of $\mathcal{T}$ as $D=\ceil{\ln{L}/\ln{3}}$, which ensures a large enough $D$ to capture natural statistics of the training data of length $L$.
$\mathcal{T}$ is constructed as described in Algorithm \ref{al:contextTree}.


\begin{algorithm}
\caption{Construction of the Context Tree}
\label{al:contextTree}
\begin{algorithmic}[1]

\State{Initialize $\mathcal{T}$ to an empty tree with only root node}

\For{each symbol $x(m)_i,\vec{x}(m)\in\mathcal{X}, \;i\geq D+1$, from $k=1$ to $k=D$ in order}

\If{there exist a node $\mathbf{u}=\vec{x}(m)_{i-k}^{i-1}$ on $\mathcal{T}$}
	\State{increase the counter $N(x(m)_i\mathbf{u})$ by $1$}
\Else
	\State{add node $\mathbf{u}=\vec{x}(m)_{i-k}^{i-1}$ to $\mathcal{T}$}
\EndIf

\EndFor


\end{algorithmic}
\end{algorithm}


The complexity of the algorithm is $O(D \, L)$.
To estimate the code rate of symbol $\hat{x}_i$, we first find the matched context $\vec{w}$ given past $\hat{\vec{x}}_{i-D}^{i-1}$ by traversing the context tree $\mathcal{T}$ from the root node to the deepest node, \textit{i.e.}, $\vec{w}=\hat{\vec{x}}^{i-1}_{i-|\vec{w}|}$, and then compute the corresponding conditional probability $P(\hat{x}_i|\vec{w})$ using (\ref{eq:ppm_prob}).
%The total rate of coding DCC string $\hat{\vec{x}}$ is computed as the summation of the negative log of conditional probability of all symbols in $\hat{\vec{x}}$.

In summary, having defined the likelihood, prior and rate terms, our Lagrangian objective (\ref{eq:lagrangian_objective}) can now be rewritten as:
\begin{equation}
\label{eq:objective}
\begin{split}
J(\hat{\vec{x}})=& -\log P(\vec{y}|\hat{\vec{x}})-\beta \log P(\hat{\vec{x}}) + \lambda R(\hat{\vec{x}})\\
\approx &(c_0 + c_2) K + c_1 \Lambda + c_2 \Delta' +\beta \sum\limits_{i=D_s+1}^{L_{\hat{\vec{x}}}} s(\hat{\vec{x}}_{i-D_s}^i)\\
 &- \lambda \sum\limits_{i=D+1}^{L_{\hat{\vec{x}}}} \log_2 P(\hat{x}_i|\hat{\vec{x}}_{i-D}^{i-1}). 
\end{split}
\end{equation}
We describe a dynamic programming algorithm to minimize the objective optimally in Section \ref{sec:algorithm}.