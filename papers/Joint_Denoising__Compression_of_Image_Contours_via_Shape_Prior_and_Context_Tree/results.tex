\subsection{Experimental Setup}
\label{subsec:experimentalSetup}


\begin{figure}[t]

\begin{minipage}[b]{.48\linewidth}
  \centering
  \centerline{\includegraphics[width=4.2cm]{original_seq1_view6-eps-converted-to.pdf}}
%  \vspace{1.5cm}
  \centerline{(a) \texttt{Model1}}\medskip
\end{minipage}
\hfill
\begin{minipage}[b]{0.48\linewidth}
  \centering
  \centerline{\includegraphics[width=4.2cm]{original_seq5_view5-eps-converted-to.pdf}}
%  \vspace{1.5cm}
  \centerline{(b) \texttt{Model2}}\medskip
\end{minipage}

\vspace{-0.2cm}
\caption{One view of the extracted multiview silhouette sequences.}
\label{fig:silhouette}
\end{figure}


We used two computer-generated (noiseless) depth sequences: \texttt{Dude} (800$\times$400) and \texttt{Tsukuba} (640$\times$480), and two natural (noisy) multiview silhouette sequences: \texttt{Model1} (768$\times$1024) and \texttt{Model2} (768$\times$1024) from Microsoft Research.
10 frames were tested for each sequence.
The multiview silhouette sequences are extracted using the equipment setup in \cite{loop2013real}, where eight views are taken for one frame of each sequence.
The extracted silhouettes are compressed and transmitted for further silhouette-based 3D model reconstruction \cite{mulayim2003silhouette,loop2013real}.
Fig.\;\ref{fig:silhouette} shows one view of \texttt{Model1} and \texttt{Model2}.

We used gradient-based edge detection \cite{daribo14} to detect contours.
For the silhouette sequences, the detected contours were noisy at acquisition.
Note that before detecting the contours for silhouette sequences, we first used a median filter\footnote{http://people.clarkson.edu/~hudsonb/courses/cs611/} to remove the noise inside and outside the silhouette.
For the computer-generated depth images, we first injected noise to the depth images assuming that the pixels along the contours were corrupted by iid noise: for each edge of the contour, the corruption probability was fixed at $\delta$.
If corrupted, the pixel along one side of this edge was replaced by the pixel from the other side (side was chosen with equal probability).
The noisy contours were then detected from the noisy depth images.
Note that for the computer-generated depth images, the ground truth contours were also detected from the original depth images.
We tested two different noise probabilities, $\delta=10\%$ and $\delta=30\%$.

To code contours in a given frame of a depth / silhouette sequence, previous two frames of the same sequence were used to train the context tree.
Unless specified otherwise, we set $D_s=4$ and $\beta = 3$ in (\ref{eq:geometric_prior}) for all experiments.

The quality of each decoded contour was evaluated using \textit{sum of squared distance distortion metric} as proposed in \cite{katsaggelos1998mpeg}: the sum of squared minimum distance from the points on each decoded contour $\vec{\hat{x}}$ to the ground truth contour $\vec{x}$, denoted as $D(\hat{\vec{x}}, \vec{x})$,
\begin{equation}
D(\hat{\vec{x}},\vec{x}) = \sum_{i=1}^{L_{\hat{\vec{x}}}} d^2(\vec{e}_{\hat{\vec{x}}}(i), \vec{x}).
\end{equation}
$d(\vec{e}_{\hat{\vec{x}}}(i), \vec{x})$ is the minimum absolute distance between coordinate $(\hat{m}, \hat{n})$ of the $i$-th edge $\vec{e}_{\hat{\vec{x}}}(i)$ of $\hat{\vec{x}}$ and the segment derived by all edges of $\vec{x}$,
\begin{equation}
d(\vec{e}_{\hat{\vec{x}}}(i), \vec{x}) = \underset{1\leq j \leq L_{\vec{x}}}{\min} |\hat{m}_i-m_j|+|\hat{n}_i-n_j|
\end{equation}
where $(m_j,n_j)$ is the coordinate of the $j$-th edge $\vec{e}_{\vec{x}}(j)$ of $\vec{x}$. 
The main motivation for considering this distortion metric stems from the popularity of the sum of squared distortion measure, which measures the aggregate errors.
Note that some other distortion metrics can also be adopted according to different applications, such as the absolute area between the decoded and ground truth contours, or the depth values change due to the decoded contour shifting compared to the ground truth contour.
For the silhouette sequences without ground truth contours, we set $\lambda=0$ in (\ref{eq:objective}) to get the best denoising result and use it as the ground truth.

We compared performance of four different schemes.
The first scheme, \texttt{Gaussian-ORD}, first denoised the contours using a Gaussian filter \cite{zhong2010convergence}, then encoded the denoised contours using a vertex based lossy contour coding method \cite{lai2010arbitrary}.
The second scheme, \texttt{Lossy-AEC}, first denoised the noisy contours using an irregularity-detection method \cite{daribo14}, then encoded the denoised contours using a DCC based lossy contour coding method \cite{yuan2015contour}.
The third scheme, \texttt{Separate}, first denoised the contours using our proposal by setting $\lambda=0$, then used PPM to encoded the denoised contours.
The fourth scheme, \texttt{Joint}, is our proposal that performed joint denoising / compression of contours.

\subsection{Transition Probability Estimation}
\label{subsec:paraneterEstimation}


\begin{figure}[t]

\begin{minipage}[b]{.48\linewidth}
  \centering
  \centerline{\includegraphics[width=4.5cm]{para_seq1_5-eps-converted-to.pdf}}
%  \vspace{1.5cm}
%  \centerline{(a)}\medskip
\end{minipage}
\hfill
\begin{minipage}[b]{0.48\linewidth}
  \centering
  \centerline{\includegraphics[width=4.5cm]{para_seq1_8-eps-converted-to.pdf}}
%  \vspace{1.5cm}
%  \centerline{(b)}\medskip
\end{minipage}
\vfill
\vspace{0.3cm}
\begin{minipage}[b]{.48\linewidth}
  \centering
  \centerline{\includegraphics[width=4.5cm]{para_seq2_5-eps-converted-to.pdf}}
%  \vspace{1.5cm}
%  \centerline{(c)}\medskip
\end{minipage}
\hfill
\begin{minipage}[b]{0.48\linewidth}
  \centering
  \centerline{\includegraphics[width=4.5cm]{para_seq2_8-eps-converted-to.pdf}}
%  \vspace{1.5cm}
%  \centerline{(d)}\medskip
\end{minipage}

%\vspace{-0.2cm}
\caption{Value changes of $p$, $q1$ and $q2$ over number of iterations. Left column: all initial values are setted to 0.5. Right column: all initial values are setted to 0.8.}
\label{fig:parameter_estiamtion}
\end{figure}


For each computer-generated depth sequence, the three transition probabilities in our three-state Markov model, \textit{i.e.}, $p$, $q_1$ and $q_2$, were estimated from average noise statistics by comparing the noisy contours and the ground truth contours.
As described in Section \ref{subsec:error_model}, we traversed the noisy DCC string $\vec{y}$ along with the corresponding ground truth DCC string $\vec{x}$ to count the number of transitions between the three states, \textit{i.e.}, \texttt{0}, \texttt{1} and \texttt{2}.
The transition probability from state\footnote{$j$ can be the same as $i$.} $i$ to state $j$ is thus computed as the number of transitions from state $i$ to state $j$ divided by the number of transitions started from state $i$.

For the silhouette sequences without ground truth contours, we estimated the transition probabilities via an alternating procedure, commonly used when deriving model parameters without ground truth data in machine learning \cite{mackay2003example,sundberg1976iterative}. 
Specifically, given a noisy DCC string $\vec{y}$, we first assigned initial values to the transition probabilities and used them to denoise $\vec{y}$ to get a MAP solution $\vec{x}_{MAP}^{\prime}$ ($\lambda=0$ in (\ref{eq:objective})).
Treating $\vec{x}_{MAP}^{\prime}$ as ground truth and comparing $\vec{y}$ and $\vec{x}_{MAP}^{\prime}$, we updated the transition probabilities.
Then, we used the updated transition probabilities to denoise $\vec{y}$ again to get a new MAP solution and computed the new transition probabilities by comparing $\vec{y}$ and the new MAP solution.
This process was repeated until the transition probabilities converged.

Fig.\;\ref{fig:parameter_estiamtion} shows the changes in model parameters $p$, $q_1$ and $q_2$ across the iterations of the alternating procedure using different initial values for \texttt{Model1} and \texttt{Model2}.
We see that for each silhouette sequence with different initial values, the parameters converged to the same values in only a few iterations.
%The reason of convergence is that, for every iteration with a set of input transition probabilities, the denoised contour tends to approximate the statistics reflected by the input transition probabilities.
Note that we can also use the alternating procedure to estimate model parameters for the computer-generated depth sequences, but the estimated results would not be as accurate as those  directly computed using the available ground truth contours and the noisy contours.

\subsection{Proposed Three-state Markov Model versus iid Model}
\label{subsec:vs_iid_model}

To validate our proposed three-state Markov model, we compared to a na\"ive iid generative model for computing the likelihood.
The model is as follows: for each symbol $x_i$ in the ground truth contour $\vec{x}$, we use an iid model (a coin toss) to see if it is erred in the observed contour $\vec{y}$.
If so, we use a Poisson distribution to model the length increase in $\vec{y}$ over $\vec{x}$.
Then we examine the next symbol $x_{i+1}$ and so on until the end of the DCC string.
This model assumes each symbol $x_i$ is independent and is capable of modelling the length increase.

We adopt the widely used Akaike information criterion (AIC) \cite{akaike1974new} to compare the two models.
AIC is a measure of the relative quality of statistical models for a given set of data,
\begin{equation}
AIC = 2k - 2\ln {\mathcal{L}}
\label{eq:AIC}
\end{equation}
where $\mathcal{L}=P(\vec{y}|\vec{x})$ is the likelihood and $k$ is the number of parameters to be estimated.
For the proposed three-state Markov model with three transition probabilities ($p$, $q_1$, $q_2$), $k = 3$; for the i.i.d model, $k=2$ which contains $p^{\prime}$---the probability of each symbol $x_i$ being an error and $\lambda^{\prime}$---the average increased length per erred symbol $x_i$ in Poisson distribution.
$p^{\prime}$ and $\lambda^{\prime}$ were estimated from average noise statistics as similar in Section \ref{subsec:paraneterEstimation}.
To accurately compute AIC, we only test sequence \texttt{Dude} and \texttt{Tsukuba} whose ground truth contours are available.

\begin{table}
\caption{Results of Model Validation}
\label{tab:model_validation}

\begin{tabular}{c|c|c|c|c|c|c}
\hline 
\hline
\multirow{2}{*}{Sequence} & \multirow{2}{*}{Noise} & \multicolumn{2}{c|}{-Log-likelihood} & \multicolumn{2}{c|}{AIC} & \multicolumn{1}{c}{Relative }\tabularnewline
\cline{3-6} 
 &  & i.i.d & Prop & i.i.d & Prop & likelihood\tabularnewline
\hline 
\multirow{2}{*}{Tsukuba} & 10\% & 67 & \textbf{64 } & 134  & \textbf{128 } & 3.93e-02 \tabularnewline
\cline{2-7} 
 & 30\% & 141  & \textbf{134 } & 283  & \textbf{267 } & 3.72e-04 \tabularnewline
\hline 
\multirow{2}{*}{Dude} & 10\% & 198  & \textbf{178 } & 397  & \textbf{356 } & 1.65e-09 \tabularnewline
\cline{2-7} 
 & 30\% & 378 & \textbf{348 } & 756  & \textbf{696 } & 8.93e-14 \tabularnewline
\hline 
\hline
\end{tabular}

\end{table}

Table\;\ref{tab:model_validation} shows the results of the negative log-likelihood and AIC of the two models.
All the values of negative log-likelihood and AIC are averaged to one contour for better comparison. 
A smaller value of AIC represents a better fit of the model.
We can see that the proposed three-state Markov model always achieved smaller values.
Relative likelihood \cite{burnham2003model}, computed as $exp((AIC_{prop}-AIC_{iid})/2)$, in the last column is interpreted as being proportional to the probability that the iid model is the proposed model.
For example, for $Tsukuba$ with 10\% noise, it means the iid model is 3.93e-2 times as probable as the proposed model to minimize the information loss.
All the results show that the proposed three-state Markov model fits the data better than the iid model.

\subsection{Performance of Rate Distortion}
\label{subsec:RD_performance}

\begin{figure}[t]

\begin{minipage}[b]{.48\linewidth}
  \centering
  \centerline{\includegraphics[width=4.5cm]{RDcurve_Dude_10-eps-converted-to.pdf}}
%  \vspace{1.5cm}
%  \centerline{(a)}\medskip
\end{minipage}
\hfill
\begin{minipage}[b]{0.48\linewidth}
  \centering
  \centerline{\includegraphics[width=4.5cm]{RDcurve_Dude_30-eps-converted-to.pdf}}
%  \vspace{1.5cm}
%  \centerline{(b)}\medskip
\end{minipage}
\vfill
\vspace{0.3cm}
\begin{minipage}[b]{.48\linewidth}
  \centering
  \centerline{\includegraphics[width=4.5cm]{RDcurve_Tsukuba_10-eps-converted-to.pdf}}
%  \vspace{1.5cm}
%  \centerline{(c)}\medskip
\end{minipage}
\hfill
\begin{minipage}[b]{0.48\linewidth}
  \centering
  \centerline{\includegraphics[width=4.5cm]{RDcurve_Tsukuba_30-eps-converted-to.pdf}}
%  \vspace{1.5cm}
%  \centerline{(d)}\medskip
\end{minipage}

%\vspace{-0.2cm}
\caption{Rate distortion curve of \texttt{Dude} and \texttt{Tsukuba}. Left column: noise ratio is 10\%. Right column: noise ratio is 30\%.}
\label{fig:RDcurve_synthetic}
\end{figure}

\begin{figure}[t]

\begin{minipage}[b]{.48\linewidth}
  \centering
  \centerline{\includegraphics[width=4.5cm]{RDcurve_Model1-eps-converted-to.pdf}}
%  \vspace{1.5cm}
%  \centerline{(a)}\medskip
\end{minipage}
\hfill
\begin{minipage}[b]{0.48\linewidth}
  \centering
  \centerline{\includegraphics[width=4.5cm]{RDcurve_Model2-eps-converted-to.pdf}}
%  \vspace{1.5cm}
%  \centerline{(b)}\medskip
\end{minipage}

\vspace{-0.2cm}
\caption{Rate distortion curve of \texttt{Model1} and \texttt{Model2}.}
\label{fig:RDcurve_silhouette}
\end{figure}


We show the rate-distortion (RD) performance of the four comparison schemes in Fig.\;\ref{fig:RDcurve_synthetic} and Fig.\;\ref{fig:RDcurve_silhouette}.
For \texttt{Gaussian-ORD}, RD curve was generated by adjusting the coding parameters of \texttt{ORD} \cite{lai2010arbitrary}. 
For \texttt{Lossy-AEC}, RD-curve was generated by adjusting the strength of contour approximation \cite{yuan2015contour}.
For \texttt{Joint}, RD curve was obtained by varying $\lambda$.
For \texttt{Separate}, we used the same fixed $\beta$ as in \texttt{Joint} to get the best denoising performance (MAP solution), then we increased $\beta$ to further smooth the contour to reduce the bit rate and obtained the RD curve.

We see that \texttt{Joint} achieved the best RD performance for both the depth sequences and the silhouette sequences, demonstrating the merit of our joint approach.
In particular, we save about 14.45\%, 36.2\% and 38.33\% bits on average against \texttt{Separate}, \texttt{Lossy-AEC} and \texttt{Gaussian-ORD} respectively for depth sequences with 10\% noise, and about 15.1\%, 28.27\% and 32.02\% for depth sequences with 30\% noise, and about 18.18\%, 54.17\% and 28.57\% respectively for silhouette sequences. 

Comparing the results with 10\% noise and the results with 30\% noise, we find that more noise generally results in larger bit rate for \texttt{Joint}, \texttt{Separate} and \texttt{Lossy-AEC}.
For \texttt{Gaussian-ORD}, the bit rate does not increase much with more noise, especially the results of \texttt{Tsukuba}.
This was because \texttt{Gaussian-ORD} used a vertex-based contour coding approach; \textit{i.e.}, \texttt{ORD} encoded the locations of some selected contour pixels (vertices).
After denoising using Gaussian filter, the number of selected vertices by \texttt{ORD} are similar for different noise probabilities, resulting in similar bit rates. 

Looking at the results by \texttt{Separate}, when the distortion becomes larger, the bit rate reduces slowly, in some points even increases slightly, especially compared to \texttt{Joint}.
Note that the RD curve of \texttt{Separate} was obtained by increasing $\beta$ to get over-smoothed denoised contour to reduce the bit rate.
It shows that smoother contour dose not always ensure smaller bit rate, which in other hand verifies the importance of the rate term in the problem formulation.
Thus, both the prior term and the rate term are necessary for our contour denoising / compression problem.

Compared to \texttt{Lossy-AEC} which encodes each DCC symbol using a limited number of candidates of conditional probabilities, our proposed \texttt{Joint} and \texttt{Separate} achieve much better coding performance by using PPM with variable length context tree model.
Compared to \texttt{Gaussian-ORD} which encodes the contour by coding some selected vertices, our proposed \texttt{Joint} encodes more contour pixels, but the accurately estimated conditional probabilities enable our contour pixels to be coded much more efficiently than \texttt{Gaussian-ORD}.
%We can also see that the results by \texttt{Gaussian-ORD} have larger distortion than other three approaches.
%This is because the interpolated contour pixels based on the selected vertices can be very different from the ground truth contour pixels.

\subsection{Performance of Denoising}
\label{subsec:Denoising_performance}


\begin{figure}[t]

\begin{minipage}[b]{.32\linewidth}
  \centering
  \centerline{\includegraphics[width=2.8cm]{subjective_ground_04-eps-converted-to.pdf}}
%  \vspace{1.5cm}
  \centerline{(a) Ground truth}\medskip
\end{minipage}
\hfill
\begin{minipage}[b]{0.32\linewidth}
  \centering
  \centerline{\includegraphics[width=2.8cm]{subjective_noise_10-eps-converted-to.pdf}}
%  \vspace{1.5cm}
  \centerline{(b) 10\% noise}\medskip
\end{minipage}
\hfill
\begin{minipage}[b]{0.32\linewidth}
  \centering
  \centerline{\includegraphics[width=2.8cm]{subjective_noise_30-eps-converted-to.pdf}}
%  \vspace{1.5cm}
  \centerline{(c) 30\% noise}\medskip
\end{minipage}
\vfill
%\vspace{0.3cm}

\begin{minipage}[b]{.32\linewidth}
  \centering
  \centerline{\includegraphics[width=2.8cm]{subjective_ord_10-eps-converted-to.pdf}}
%  \vspace{1.5cm}
  \centerline{(d) \texttt{Gaussian-ORD}}\medskip
\end{minipage}
\hfill
\begin{minipage}[b]{0.32\linewidth}
  \centering
  \centerline{\includegraphics[width=2.8cm]{subjective_AEC_10-eps-converted-to.pdf}}
%  \vspace{1.5cm}
  \centerline{(e) \texttt{Lossy-AEC}}\medskip
\end{minipage}
\hfill
\begin{minipage}[b]{0.32\linewidth}
  \centering
  \centerline{\includegraphics[width=2.8cm]{subjective_joint_10-eps-converted-to.pdf}}
%  \vspace{1.5cm}
  \centerline{(f) \texttt{Joint}}\medskip
\end{minipage}
\vfill
%\vspace{0.3cm}

\begin{minipage}[b]{.32\linewidth}
  \centering
  \centerline{\includegraphics[width=2.8cm]{subjective_ord_30-eps-converted-to.pdf}}
%  \vspace{1.5cm}
  \centerline{(g) \texttt{Gaussian-ORD}}\medskip
\end{minipage}
\hfill
\begin{minipage}[b]{0.32\linewidth}
  \centering
  \centerline{\includegraphics[width=2.8cm]{subjective_AEC_30-eps-converted-to.pdf}}
%  \vspace{1.5cm}
  \centerline{(h) \texttt{Lossy-AEC}}\medskip
\end{minipage}
\hfill
\begin{minipage}[b]{0.32\linewidth}
  \centering
  \centerline{\includegraphics[width=2.8cm]{subjective_joint_30-eps-converted-to.pdf}}
%  \vspace{1.5cm}
  \centerline{(i) \texttt{Joint}}\medskip
\end{minipage}

%\vspace{-0.2cm}
\caption{Visual denoising results of \texttt{Tsukuba}. Middle row: denoising results with 10\% noise. Last row: denoising results with 30\% noise. (d)\;$\sim$\;(f) at bit rate 1.38, 1.13 and 0.84 bits/symbol respectively. (g)\;$\sim$\;(i) at bit rate 1.39, 1.23 and 0.95 bits/symbol respectively.}
\label{fig:subjective_synthetic}
\end{figure}


\begin{figure}[t]

\begin{minipage}[b]{.48\linewidth}
  \centering
  \centerline{\includegraphics[width=2.8cm]{subjective_model1_f003_noise-eps-converted-to.pdf}}
%  \vspace{1.5cm}
  \centerline{(a) Observation}\medskip
\end{minipage}
\hfill
\begin{minipage}[b]{0.48\linewidth}
  \centering
  \centerline{\includegraphics[width=2.8cm]{subjective_model1_f003_max-eps-converted-to.pdf}}
%  \vspace{1.5cm}
  \centerline{(b) MAP solution}\medskip
\end{minipage}
\vfill
%\vspace{0.3cm}

\begin{minipage}[b]{.32\linewidth}
  \centering
  \centerline{\includegraphics[width=2.8cm]{subjective_model1_f003_ORD-eps-converted-to.pdf}}
%  \vspace{1.5cm}
  \centerline{(c) \texttt{Gaussian-ORD}}\medskip
\end{minipage}
\hfill
\begin{minipage}[b]{0.32\linewidth}
  \centering
  \centerline{\includegraphics[width=2.8cm]{subjective_model1_f003_AEC-eps-converted-to.pdf}}
%  \vspace{1.5cm}
  \centerline{(d) \texttt{Lossy-AEC}}\medskip
\end{minipage}
\hfill
\begin{minipage}[b]{0.32\linewidth}
  \centering
  \centerline{\includegraphics[width=2.8cm]{subjective_model1_f003_joint-eps-converted-to.pdf}}
%  \vspace{1.5cm}
  \centerline{(e) \texttt{Joint}}\medskip
\end{minipage}


%\vspace{-0.2cm}
\caption{Visual denoising results of \texttt{Model1}. (c)\;$\sim$\;(e) at bit rate 0.93, 1.30 and 0.72 bits/symbol respectively.}
\label{fig:subjective_silhouette}
\end{figure}

Fig.\;\ref{fig:subjective_synthetic} and Fig.\;\ref{fig:subjective_silhouette} illustrate the visual denoising results of \texttt{Tsukuba} and \texttt{Model1}.
We see that the denoised contours by \texttt{Joint} are most visually similar to the ground truth contours.
As shown inside the red circle of Fig.\;\ref{fig:subjective_synthetic}(d)\;(g) and Fig.\;\ref{fig:subjective_silhouette}(c), the results by \texttt{Gaussian-ORD} contain lots of staircase shapes.
This was because that the decoded contours were constructed by connecting the selected contour pixels in order, making the result unnatural with too many staircase lines.
\texttt{Lossy-AEC} used a pre-defined irregularity-detection approach to denoise the contours, which failed to remove the undefined noise, \textit{i.e.}, some noise along the diagonal directions of the contours or some noise with high irregularities, as illustrated inside the red circle of Fig.\;\ref{fig:subjective_synthetic}(e)\;(h) and Fig.\;\ref{fig:subjective_silhouette}(d).  
%\red{we will have room. can we have another visual demonstration?}