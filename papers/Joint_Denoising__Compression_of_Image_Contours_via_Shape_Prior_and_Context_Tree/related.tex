\subsection{Contour Coding}
\label{subsec:related_contour_coding}

\subsubsection{Lossless Contour Coding}
\label{subsubsec:related_lossless}

Most works in lossless contour coding \cite{daribo14,freeman1978application,liu2005efficient,DCC1st1991,chan1995highly,estes1995efficient,turner1996efficient,egger1996region,jordan1998shape} first convert an image contour into a \textit{chain code} \cite{freeman1961}: a sequence of symbols each representing one of four or eight possible \textit{absolute} directions on the pixel grid. 
Alternatively, a \textit{differential chain code} (DCC) \cite{freeman1978application} that specifies \textit{relative} directions instead can be used.
DCC symbols are entropy-coded using either Huffman \cite{liu2005efficient} or arithmetic coding \cite{DCC1st1991} given symbol probabilities. 
The challenge is to estimate conditional probabilities for DCC symbols given a set of training data; this is the main problem we discuss in Section \ref{sec:rate}. 

\cite{daribo12icip,daribo14} propose a linear geometric model to estimate conditional probabilities of the next DCC symbol. 
In summary, given a window of previous edges, a line-of-best-fit that minimizes the sum of distances to the edges' endpoints is first constructed. Then the probability of a candidate direction for the next symbol is assumed inversely proportional to the angle difference between the direction and the fitted line. 
This scheme is inferior in estimating symbol probabilities compared to context models, because there are only a few possible angle differences for a small number of previous edges, limiting the expressiveness of the model.

An alternative approach is \textit{context modeling}: given a window of $l$ previous symbols (context) $\mathbf{x}^{i-1}_{i-l}$, compute the conditional probability $P(x_{i} | \mathbf{x}^{i-1}_{i-l})$ of the next symbol $x_{i}$ by counting the number of occurrences of $\mathbf{x}^{i-1}_{i-l}$ followed by $x_{i}$ in the training data. 
In \cite{kaneko1985encoding,DCC1st1991,chan1995highly,estes1995efficient,turner1996efficient}, Markov models of fixed order up to eight are used for lossless coding. 
However, in applications where the training data is limited, there may not be enough occurrences of $\mathbf{x}^{i-1}_{i-l}$ to reliably estimate the conditional probabilities. 

\textit{Variable-length Context Tree} (VCT) \cite{rissanen1983universal,begleiter2004prediction} provides a more flexible approach for Markov context modeling by allowing each context to be of variable length. 
There are many ways to construct a VCT; \textit{e.g.}, Lempel-Ziv-78 (LZ78) \cite{ziv1977universal} and prediction by partial matching (PPM) \cite{cleary1984data,moffat1990implementing}.
LZ78 constructs a dictionary from scratch using the input data directly as training data. 
The probability estimation quality varies depending on the order of first appearing input symbols.
PPM considers all contexts restricted by a maximum length with non-zero occurrences in the training data when building a VCT.
PPM has an efficient way to deal with the zero frequency problem \cite{begleiter2004prediction}, where the input symbol along with the context does not occur in the training data, by reducing the context length. 
PPM is widely used for lossless contour coding \cite{egger1996region,jordan1998shape} because of its efficiency.
In this paper, we use PPM for contour coding, as described in Section \ref{sec:rate}.

\subsubsection{Lossy Contour Coding}
\label{subsubsec:related_lossy}

Lossy contour coding approaches can be classified into two categories: DCC-based \cite{zahir2007new,yuan2015contour} and vertex-based \cite{katsaggelos1998mpeg,sohel2007new,lai2010arbitrary} approaches. 
In \cite{zahir2007new,yuan2015contour}, the DCC strings are approximated by using line smoothing techniques. 
In contrast, vertex-based approaches select representative key points along the contour for coding at the encoder and interpolation at the decoder.
Because vertex-based approaches are not suitable for lossless contour coding and we consider joint denoising / compression of contours for a wide range of bitrates, 
we choose a DCC-based approach and compute the optimal rate-constrained MAP contour estimate for coding using PPM.

\subsection{Contour Denoising}
\label{subsec:related_contour_denoising}
Contour noise removal is considered in \cite{yu1997efficient,daribo14,hoang2011edge,reisert2008equivariant,lu2015rapid,zhong2010convergence}.
In \cite{yu1997efficient}, the noisy pixels along the chain code are first removed, then the processed chain code is decomposed into a set of connected straight lines.
In \cite{daribo14}, a chain code is simplified by removing ``irregularities", which are predefined non-smooth edge patterns.
Both \cite{yu1997efficient} and \cite{daribo14} are contour smoothing techniques, which does not guarantee the similarity between the original contour and the smoothed contour.
Sparse representation and non-linear filter are used in \cite{hoang2011edge} and \cite{reisert2008equivariant} respectively to enhance the contours in a binary document image, with the objective of removing the noise in the binary image.
\cite{lu2015rapid,zhong2010convergence} denoise the image contours using Gaussian filter by averaging the coordinates of the points on the contours.
Gaussian filter is widely used for removing noise from signal, but the Gaussian model does not specifically reflect the statistics of the noise on the image contours.
In contrast, we propose a burst error model to describe the errors on the image contours, as discussed in Section \ref{sec:error}.

Considering the problem of both denoising and compression of the image contours, the denoised contour by \cite{yu1997efficient,daribo14,hoang2011edge,reisert2008equivariant,lu2015rapid,zhong2010convergence} may require a large encoding overhead.
We perform a MAP estimate to denoise an observed contour subject to a rate constraint. We will show in Section \ref{sec:results} that we outperform a separate denoising / compression approach.