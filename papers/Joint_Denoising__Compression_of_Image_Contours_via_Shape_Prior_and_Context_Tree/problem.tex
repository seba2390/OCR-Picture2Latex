\begin{figure}[t]

\begin{minipage}[b]{1\linewidth}
  \centering
  \centerline{\includegraphics[width=4cm]{dude-eps-converted-to.pdf}}
%  \vspace{1.5cm}
%  \centerline{(a)}\medskip
\end{minipage}

\vspace{-0.3cm}
\caption{Depth image with three detected contours.
The detected contours are the edges between the green and the red pixels. 
Initial points of the contours are indicted by red arrows.}
\label{fig:contour_definition}
\end{figure}

We first describe our contour denoising / compression problem at a high level.
We assume that one or more object contours in a noise-corrupted image have first been detected, for example, using a method like gradient-based edge detection \cite{daribo14}.
Each contour is defined by an initial point and a following sequence of connected ``between-pixel" edges on a 2D grid that divide pixels in a local neighbourhood into two sides.
As an example, three contours in one video frame of \texttt{Dude} are drawn in Fig.\;\ref{fig:contour_definition}.
For each detected (and noise-corrupted) contour, the problem is to estimate a contour that is highly probable \textit{and} requires few bits for encoding.

We discuss two general approaches for this problem---joint approach and separate approach---and compare and contrast the two.
Adopting the joint approach, we formulate a rate-constrained MAP problem. 



\begin{figure}[t]

\begin{minipage}[b]{.62\linewidth}
  \centering
  \centerline{\includegraphics[width=5cm]{DCC-eps-converted-to.pdf}}
%  \vspace{1.5cm}
  \centerline{(a)}\medskip
\end{minipage}
\hfill
\begin{minipage}[b]{0.36\linewidth}
  \centering
  \centerline{\includegraphics[width=3cm]{direction_set-eps-converted-to.pdf}}
%  \vspace{1.5cm}
  \centerline{(b)}\medskip
\end{minipage}

\vspace{-0.2cm}
\caption{{(a) An example of an observed length-17 contour: $\!\texttt{E}\!-\!\texttt{s}\!-\!\texttt{s}\!-\!\texttt{r}-\!\texttt{l}\!-\!\texttt{s}\!-\!\texttt{r}-\!\texttt{s}\!-\!\texttt{l}\!-\!\texttt{s}\!-\!\texttt{s}\!-\!\texttt{s}\!-\!\texttt{l}\!-\!\texttt{r}\!-\!\texttt{r}\!-\!\texttt{l}\!-\!\texttt{s}$. $(m_0, n_0)$ is the coordinate of the initial point and $\vec{e}(2)=\{(m_0+2,n_0),E\}$. (b) Three relative directions.}}
\label{fig:DCC}
\end{figure}


\subsection{Joint and Separate Approaches}
\label{subsec:analysis_jointSeparate}

\subsubsection{Definitions}

We first convert each contour into a \textit{differential chain code} (DCC)\cite{freeman1978application}.
A length-$L_{\vec{z}}$ contour $\vec{z}$ can be compactly described as a symbol string denoted by $[z_1,z_2,\cdots,z_{L_{\vec{z}}}]$.
As shown in Fig.\;\ref{fig:DCC}, the first symbol $z_1$ is chosen from a size-four alphabet, $\mathcal{D}=\{\texttt{N},\texttt{E},\texttt{S},\texttt{W}\}$, specifying the four \textit{absolute} directions \texttt{north}, \texttt{east}, \texttt{south} and \texttt{west} with respect to a 2D-grid start point, \textit{i.e.}, $z_1\in \mathcal{D}$. 
Each subsequent DCC symbol $z_i$, $i\geq 2$, is chosen from a size-three alphabet, $\mathcal{A}=\{\texttt{l}, \texttt{s}, \texttt{r}\}$, specifying the three \textit{relative} directions \texttt{left}, \texttt{straight} and \texttt{right} with respect to the previous symbols $z_{i-1}$, \textit{i.e.}, $z_i\in \mathcal{A}$ for $i\geq 2$.
Denote by $\vec{z}^{j}_{i} = [{z}_{j},{z}_{j-1},\ldots,{z}_{i}]$, $i<j$ and $i,j \in \mathbb{Z}^+$, a \textit{sub-string} of length $j-i+1$ from the $i$-th symbol $z_i$ to the $j$-th symbol $z_j$ in reverse order. 

Alternatively, one can represent the $i$-th edge of contour $\vec{z}$ geometrically on the 2D grid as $\vec{e}_{\vec{z}}(i)=\{(m_i,n_i), d_i\}$, where $(m_i, n_i)$ is the 2D coordinate of the ending point of the edge and $d_i \in \mathcal{D}$ is the absolute direction, as shown in Fig.\;\ref{fig:DCC}(a).
This representation is used later when describing our optimization algorithm.
%\red{is this correct?} 


\subsubsection{Joint Problem Formulation}

Denote by $\vec{y} \in \mathcal{S}$ and $\vec{x} \in \mathcal{S}$ the observed and ground truth DCC strings respectively, where $\mathcal{S}$ is the space of all DCC strings of finite length. 
As done in \cite{sun13,sun14} for joint denoising / compression of multiview depth images\footnote{We note that though \cite{sun13,sun14} also formulated a rate-constrained MAP problem for the joint denoising / compresion problem, there was no theoretical analysis on why a joint approach is better in general.}, we follow a rate-constrained MAP formulation and define the objective as finding a string $\hat{\vec{x}} \in \mathcal{S}$ that maximizes the posterior probability $P(\hat{\vec{x}}|\vec{y})$, subject to a rate constraint on chosen $\hat{\vec{x}}$:
\begin{equation}
\label{eq:orig_objective}
\underset{\hat{\vec{x}}\in  \mathcal{S}}{\max}\ P(\hat{\vec{x}}|\vec{y}) ~~~ \mbox{s.t.} ~ 
R(\hat{\vec{x}})\leq  R_{\max}
\end{equation}
where $R(\hat{\vec{x}})$ is the bit count required to encode string $\hat{\vec{x}}$, and $R_{\max}$ is the bit budget. 

(\ref{eq:orig_objective}) essentially seeks one solution to \textit{both} the estimation problem \textit{and} the compression problem at the same time.
The estimated rate-constrained DCC string from (\ref{eq:orig_objective}) is then \textit{losslessly} compressed at the encoder.


\subsubsection{Separate Problem Formulation}

An alternative approach is to first solve an unconstrained MAP estimation sub-problem, then \textit{lossily} compress the estimated DCC string subject to the rate constraint as follows:
\begin{equation}
\vec{x}_{MAP} = \arg\,\underset{\hat{\vec{x}}\in  \mathcal{S}}{\max} \ P(\hat{\vec{x}}|\vec{y})
\label{eq:separate_estimation}
\end{equation} 
\begin{equation}
\underset{\hat{\vec{x}}\in  \mathcal{S}}{\max}\ P(\hat{\vec{x}}|\vec{x}_{MAP}) ~~~ \mbox{s.t.} ~ 
R(\hat{\vec{x}})\leq  R_{\max}
\label{eq:separate_compression}
\end{equation} 
where $\vec{x}_{MAP}$ is an optimal solution to the unconstrained estimation sub-problem (\ref{eq:separate_estimation}).
The lossy compression sub-problem (\ref{eq:separate_compression}) aims to find a DCC string $\vec{\hat{x}}$ which is closest to $\vec{x}_{MAP}$ in probability given bit budget $R_{\max}$.
In the sequel we call (\ref{eq:orig_objective}) the \textit{joint} approach, and (\ref{eq:separate_estimation}) and (\ref{eq:separate_compression}) the \textit{separate} approach.

Since the contour denoising / compression problem is inherently a joint estimation /  compression problem, the solution to the joint approach should not be worse than the solution to the separate approach.
The question is under what condition(s) the joint approach is strictly better than the separate approach.
%In other words, in what condition knowing $\vec{x}_{MAP}$ is equivalent to knowing $\vec{y}$.


\subsection{Comparison between Joint and Separate Approaches}
\label{subsec:analysis_comparison}

\subsubsection{Algebraic Comparison}

To understand the condition(s) under which the joint approach is strictly better than the separate approach, we first rewrite $P(\hat{\vec{x}}|\vec{x}_{MAP})$ in (\ref{eq:separate_compression}) using the law of total probability by introducing an observation $\hat{\vec{y}}$:
\begin{equation}
P(\hat{\vec{x}}|\vec{x}_{MAP}) = \sum_{\hat{\vec{y}}} P(\hat{\vec{x}}|\vec{x}_{MAP},\hat{\vec{y}})\,P(\hat{\vec{y}}|\vec{x}_{MAP}).
\label{eq:comparison_total}
\end{equation}
$P(\hat{\vec{y}}|\vec{x}_{MAP})$ can be further rewritten using Bayes' rule:
\begin{equation}
P(\hat{\vec{y}}|\vec{x}_{MAP}) = \frac{P(\vec{x}_{MAP}|\hat{\vec{y}})P(\hat{\vec{y}})}{P(\vec{x}_{MAP})}.
\label{eq:comparison_bayes}
\end{equation}
From (\ref{eq:separate_estimation}), $P(\vec{x}_{MAP}|\hat{\vec{y}}) = 1$ if $\vec{x}_{MAP}$ is the MAP solution given this observation $\hat{\vec{y}}$; otherwise, $P(\vec{x}_{MAP}|\hat{\vec{y}}) = 0$:
\begin{equation}
P(\vec{x}_{MAP}|\hat{\vec{y}}) = 
\begin{cases}
1, & \text{if } \vec{x}_{MAP} = \arg\,\underset{\tilde{\vec{x}}\in  \mathcal{S}}{\max} \ P(\tilde{\vec{x}}|\hat{\vec{y}}) \\
0, & \text{otherwise}
\end{cases}.
\label{eq:comparison_cases}
\end{equation}

Combining (\ref{eq:separate_estimation}) and (\ref{eq:comparison_total}) to (\ref{eq:comparison_cases}), $P(\hat{\vec{x}}|\vec{x}_{MAP})$ is rewritten as follows:
\begin{equation}
\begin{split}
&\sum_{\hat{\vec{y}}} P(\hat{\vec{x}}|\vec{x}_{MAP},\hat{\vec{y}}) P(\vec{x}_{MAP}|\hat{\vec{y}}) P(\hat{\vec{y}}) \\
&= \sum_{\hat{\vec{y}}|\vec{x}_{MAP} = \arg\,\underset{\tilde{\vec{x}}\in  \mathcal{S}}{\max} \ P(\tilde{\vec{x}}|\hat{\vec{y}})} P(\hat{\vec{x}}|\vec{x}_{MAP},\hat{\vec{y}}) P(\hat{\vec{y}}) \\
&= \sum_{\hat{\vec{y}}|\vec{x}_{MAP} = \arg\,\underset{\tilde{\vec{x}}\in  \mathcal{S}}{\max} \ P(\tilde{\vec{x}}|\hat{\vec{y}})} P(\hat{\vec{x}}|\hat{\vec{y}}) P(\hat{\vec{y}})
\end{split}
\end{equation}
where $P(\hat{\vec{x}}|\vec{x}_{MAP},\hat{\vec{y}}) = P(\hat{\vec{x}}|\hat{\vec{y}})$, since $\vec{x}_{MAP}$ is a function of $\hat{\vec{y}}$, \textit{i.e.}, $\vec{x}_{MAP} = \arg\,\underset{\tilde{\vec{x}}\in  \mathcal{S}}{\max} \ P(\tilde{\vec{x}}|\hat{\vec{y}})$. 
We ignore $P(\vec{x}_{MAP})$ in (\ref{eq:comparison_bayes}), because $\vec{x}_{MAP}$ (and hence $P(\vec{x}_{MAP})$) is fixed and does not affect the maximization.
%\red{not that clear; in (3) your variable is $\hat{\vec{x}}$ for fixed $\vec{x}_{MAP}$, so $P(\vec{x}_{MAP})$ is of course fixed. but now in your expansion $\vec{x}_{MAP}$ is a function of observation $\vec{y}$, which changes over the sum. I think we need to say $P(\vec{x}_{MAP}) = \sum_{\hat{\vec{y}}} P(\vec{x}_{MAP} | \hat{\vec{y}}) P(\hat{\vec{y}})$, so it's not a function of $\hat{\vec{y}}$. }

Thus, we can rewrite the separate approach as:
\begin{equation}
\underset{\hat{\vec{x}}\in  \mathcal{S}}{\max}\ \sum_{\hat{\vec{y}}|\vec{x}_{MAP} = \arg\,\underset{\tilde{\vec{x}}\in  \mathcal{S}}{\max} \ P(\tilde{\vec{x}}|\hat{\vec{y}})} P(\hat{\vec{x}}|\hat{\vec{y}}) P(\hat{\vec{y}}) ~~~ \mbox{s.t.} ~ 
R(\hat{\vec{x}})\leq  R_{\max}.
\label{eq:comprison_separate}
\end{equation}

Comparing (\ref{eq:comprison_separate}) to the joint approach (\ref{eq:orig_objective}), we can conclude the following: 
if there is only one observation $\hat{\vec{y}}$ that corresponds to $\vec{x}_{MAP}$, \textit{i.e.}, $\vec{x}_{MAP} = \arg\,\underset{\tilde{\vec{x}}\in  \mathcal{S}}{\max} \ P(\tilde{\vec{x}}|\hat{\vec{y}})$, then the solutions to the separate and joint approaches are the same---they both maximize $P(\hat{\vec{x}}|\vec{y})$ subject to $R(\hat{\vec{x}}) \leq R_{\max}$.
%Note that for a particular observation $\hat{\vec{y}}$, $P(\hat{\vec{y}})$ has a fixed value and can be ignored in (\ref{eq:comprison_separate}) during optimization.
This makes intuitive sense: if there is a one-to-one correspondence between $\vec{x}_{MAP}$ and observation $\vec{y}$, then knowing $\vec{x}_{MAP}$ is as good as knowing $\vec{y}$.
On the other hand, if there are more than one observation $\hat{\vec{y}}$ corresponding to $\vec{x}_{MAP}$, \textit{i.e.}, there are  more than one term in the sum in (\ref{eq:comprison_separate}), then the solutions between the two approaches can be far apart.
%then maximizing the rate constrained posterior $P(\hat{\vec{x}}|\vec{x}_{MAP})$ may lead to a solution which is far away from $\vec{y}$ but close to $\vec{y}^{\prime}$.

\begin{figure}[t]

\begin{minipage}[b]{.48\linewidth}
  \centering
  \centerline{\includegraphics[width=4.5cm]{jointBetter-eps-converted-to.pdf}}
%  \vspace{1.5cm}
  \centerline{(a)}\medskip
\end{minipage}
\hfill
\begin{minipage}[b]{0.48\linewidth}
  \centering
  \centerline{\includegraphics[width=2.7cm]{jointSame-eps-converted-to.pdf}}
%  \vspace{1.5cm}
  \centerline{(b)}\medskip
\end{minipage}

\vspace{-0.3cm}
\caption{(a) Scenario where the joint approach is better than the separate approach.
(b) Scenario where the joint approach is same as the separate approach.}
\label{fig:joint_separate}
\end{figure}


\subsubsection{Geometric Comparison}

To develop intuition on the difference between the joint and separate approaches, we examine the problem from a geometric viewpoint by representing each DCC string as a point in a high-dimensional space.
By Bayes' rule, maximization of the posterior $P(\hat{\vec{x}}|\vec{y})$ is the same as maximization of the product of the likelihood $P(\vec{y}|\hat{\vec{x}})$ and the prior $P(\hat{\vec{x}})$, or minimization of $-\log P(\vec{y}|\hat{\vec{x}}) -\log P(\hat{\vec{x}})$.
As shown in Fig.\;\ref{fig:joint_separate}, each point in the space denotes a DCC string\footnote{We can map a DCC string into a point in a $N$-dimensional space as follows. First divide the string on the 2D grid into $(N-1)/2$ segments of equal length. The $x$- and $y$-coordinates of the end point of each segment on the 2D grid becomes an index for each segment. The resulted $N-1$ index numbers, plus a number to denote the length of each segment, becomes an $N$-dimension coordinate in an $N$-dimensional space.}.
Specifically, $\vec{y}$ and $\vec{y}^{\prime}$ in Fig.\;\ref{fig:joint_separate}(a) denote two different observations that would yield the same unconstrained MAP solution $\vec{x}_{MAP}$, \textit{i.e.}, $\vec{x}_{MAP} = \arg \max_{\hat{\vec{x}}} P(\hat{\vec{x}}|\vec{y}) = \arg \max_{\hat{\vec{x}}} P(\hat{\vec{x}}|\vec{y}')$. 

Denote by $\vec{x}_0$ the DCC string with the largest prior $P(\vec{x}_0)$, \textit{e.g.}, the string corresponding to the ``straightest" contour.
For illustration, we draw \textit{isolines} centered at $\vec{y}$ (or $\vec{y}^{\prime}$), each denoting the set of strings that have the same metric distance $D_l(\vec{y},\vec{x})$---negative log likelihood $-\log P(\vec{y}|\hat{\vec{x}})$---given observation $\vec{y}$ (or $\vec{y}^{\prime}$).
Similarly, we draw isolines centered at $\vec{x}_0$ to denote strings with the same metric distance $D_p(\vec{x}_0,\vec{x})$---negative log prior $-\log P(\hat{\vec{x}})$. 
Thus, maximizing $P(\hat{\vec{x}}|\vec{y})$ is equivalent to identifying a point $\vec{x}$ in the $N$-dimensional space that minimizes the sum of distance $D_l(\vec{y},\vec{x})$ and $D_p(\vec{x}_0,\vec{x})$.

The joint approach is then equivalent to finding a point $\hat{\vec{x}}$ inside the isoline defined by the rate constraint $R(\vec{\hat{x}})=R_{\max}$---\textit{feasible rate region}---to minimize the sum of the two aforementioned distances.
On the other hand, the separate approach is equivalent to first finding the point $\vec{x}_{MAP}$ to minimize $D_l(\vec{y},\vec{x}_{MAP}) + D_p(\vec{x}_0,\vec{x}_{MAP})$, then find a point $\hat{\vec{x}}$ inside the feasible rate region to minimize $D_l(\vec{x}_{MAP}, \hat{\vec{x}})$, 
%$-\log P(\hat{\vec{x}}|\vec{x}_{MAP})$, 
\textit{i.e.}, the distance from $\hat{\vec{x}}$ to $\vec{x}_{MAP}$.

In Fig.\;\ref{fig:joint_separate}(a), $\vec{x}_c^{\prime}$ is the solution to the separate approach, which is the point bounded by the rate constraint with minimum distance $D_l(\vec{x}_{MAP}, \vec{x}_c^{\prime})$ to $\vec{x}_{MAP}$.
%\red{use red line to denote the rate constraint.}
Note that the solution to the joint approach denoted by $\vec{x}_c$ may be different from $\vec{x}_c^{\prime}$; \textit{i.e.}, the sum of the two distances $D_l(\vec{y},\vec{x}_c) + D_p(\vec{x}_0,\vec{x}_c)$ from $\vec{x}_c$ is smaller than the distance sum from $\vec{x}_c^{\prime}$. 
%\red{add blue lines to denote these distances.}

Geometrically, if the \textit{major} axes\footnote{Major axis of an ellipse is its longer diameter.} of the isolines centered at $\vec{y}$ and $\vec{x}_0$ are aligned in a straight line between $\vec{y}$ and $\vec{x}_0$ as shown in Fig.\;\ref{fig:joint_separate}(b), then: i) $\vec{x}_{MAP}$ \textit{must} reside on the line, and ii) there is only one corresponding observation $\vec{y}$ for a given $\vec{x}_{MAP}$.
The first claim is true because any point not on the straight line between $\vec{y}$ and $\vec{x}_0$ will have a distance sum strictly larger than a point on the straight line and thus cannot be the optimal $\vec{x}_{MAP}$. 
Given the first claim, the second claim is true because $\vec{y}$ must be located at a unique point on an extrapolated line from $\vec{x}_0$ to $\vec{x}_{MAP}$ such that $\vec{x}_{MAP} = \arg \min_{\vec{x}} D_l(\vec{y},\vec{x}) + D_p(\vec{x}_0,\vec{x})$.
%\footnote{If the axes of the two sets of isolines are not aligned and $\vec{x}_{MAP}$ is not on the straight line connecting $\vec{y}$ and $\vec{x}_0$, then there exists at least one other observation $\vec{y}^{\prime}$ in the $N$-dimensional space that can lead to MAP solution $\vec{x}_{MAP}$, and thus $\vec{y}$ is not unique, resulting in a contradiction.}.
In this special case, the solutions to the joint and separate approaches are the same.
Clearly, in the general case (as shown in Fig.\;\ref{fig:joint_separate}(a)) the axes of the isolines are not aligned, and thus in general the joint approach would lead to a better solution. 

%One specific example of Fig.\;\ref{fig:joint_separate}(b) can be obtained by assuming $D(\vec{y},\vec{x})=d^2(\vec{y},\vec{x})$, where $d(\vec{y},\vec{x})$ is the Euclidean distance between $\vec{y}$ and $\vec{x}$ in the space.
%Then we have,
%\begin{equation}
%\begin{split}
%-\log P(\vec{y}|\hat{\vec{x}})-\log P(\hat{\vec{x}}) 
%&=d^2(\vec{y},\hat{\vec{x}}) + d^2(\hat{\vec{x}},\vec{x}_0) \\
%&\geq \frac{1}{2}(d(\vec{y},\hat{\vec{x}}) + d(\hat{\vec{x}},\vec{x}_0))^2\\
%&\geq \frac{1}{2} d^2(\vec{y},\vec{x}_0)
%\end{split}
%\end{equation}
%When $\hat{\vec{x}}$ is located at the center of the line between $\vec{y}$ and $\vec{x}_0$, the equality hold up and we get the MAP solution $\vec{x}_{\max}$, \textit{i.e.}, the center.
%\red{why center?? I don't think this is correct, because notions of distance are not the same, and the isolines are not the same, right? }
%Given the rate constraint $d^2(\hat{\vec{x}},\vec{x}_0) \leq R_{\max}$, both the separate and the joint approach have the same solution $\vec{x}_c$, which is the intersection of the isoline determined by $R(\hat{\vec{x}})=R_{\max}$ and the line determined by $\vec{x}_{MAP}$ (or $\vec{y}$) and $\vec{x}_0$.


\subsection{Rate-Constrained Maximum a Posteriori (MAP) Problem}
\label{subsec:analysis_formulation}

Since the condition of one-to-one correspondence between $\vec{y}$ and $\vec{x}_{MAP}$ is not satisfied in general, we formulate our rate-constrained MAP problem based on the joint approach. 
Instead of (\ref{eq:orig_objective}), one can solve the corresponding Lagrangian relaxed version instead \cite{shoham88}:
\begin{equation}
\label{eq:lagrangian_objective}
\underset{\hat{\vec{x}}\in  \mathcal{S}}{\min}\ -\log P(\hat{\vec{x}}|\vec{y})+\lambda R(\hat{\vec{x}})
\end{equation}
where Lagrangian multiplier $\lambda$ is chosen so that the optimal solution $\hat{\vec{x}}$ to (\ref{eq:lagrangian_objective}) has rate $R(\hat{\vec{x}}) \leq R_{\max}$.
\cite{shoham88,abreu16arxiv} discussed how to select an appropriate $\lambda$.
We focus on how (\ref{eq:lagrangian_objective}) is solved for a given $\lambda > 0$.

We call the first term and the second term in (\ref{eq:lagrangian_objective}) error term and rate term, respectively.
The error term measures, in some sense, the distance between the estimated DCC string $\hat{\vec{x}}$ and the observed DCC string $\vec{y}$.
Note that this error term is not the distance between the estimated DCC string $\hat{\vec{x}}$ and the ground truth DCC string $\vec{x}$, since we have no access to ground truth $\vec{x}$ in practice.
Thus we seek an estimate $\hat{\vec{x}}$ which is somehow close to the observation $\hat{\vec{y}}$, while satisfying some priors of the ground truth signal.
The estimate $\hat{\vec{x}}$ is also required to have small rate term $R(\hat{\vec{x}})$.

Next, we first specify the error term by using a burst error model and a geometric prior in Section \ref{sec:error}.
Then we will specify the rate term using a general variable-length context tree (VCT) model \cite{begleiter2004prediction} in Section \ref{sec:rate}.