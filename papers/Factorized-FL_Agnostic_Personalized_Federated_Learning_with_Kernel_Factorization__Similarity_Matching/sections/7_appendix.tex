\clearpage
\appendix
\onecolumn

\paragraph{Organization} We provide in-depth descriptions for our algorithms, experimental setups, i.e. dataset configurations, implementation \& training details, and additional experimental results \& analysis that are not covered in the main document, as organized as follows:

\begin{itemize}
    \item \textbf{Section~\ref{appdx:algorithm}}: We provide our pseudo-code algorithms for \texttt{Factorized-FL} and \texttt{Factorized-FL $\beta$}. 
    \item \textbf{Section~\ref{appdx:dataset}}: We describe dataset configurations for label- and domain-heterogenous scenario.
    \item \textbf{Section~\ref{appdx:details}} - We elaborate on detailed implementation and training details for our methods and baselines.
    \item \textbf{Section~\ref{appdx:experiment}} - We provide additional experimental results and analysis. 
\end{itemize}

\section{\texttt{Factorized-FL} Algorithms}
\label{appdx:algorithm}

In this section, we describe our pseudo-code algorithms for \texttt{Factorized-FL} and \texttt{Factorized-FL $\beta$} in Algorithm~\ref{algo:factorized_fl} and~\ref{algo:factorized_fl_beta}. Our \texttt{Factorized-FL} has strength for not only reducing the dimensionality of model parameters by factorizing them into rank 1 vector spaces and the additional highly-sparse matrices, but also effectively learning client-general and task-specific knowledge. Particularly, \texttt{Factorized-FL} transmits a small portion of the models which are a set of $\textbf{u}$ ($\mathcal{U}$) and a single vector $\textbf{v}^{L-1}$ form the second last layer of neural networks, which significantly reduces communication costs while showing strong performance in label- and domain-heterogeneous scenarios, as shown in Section~\ref{sec:exp} in the main document.


\input{algorithms/algo}

\section{Dataset Configurations}
\label{appdx:dataset}
In this section, we describe detailed configurations for datasets that we used in label- and domain-heterogeneous scenarios.

\subsection{Label Heterogeneous Scenario}

We use CIFAR-10 and SVHN for the label-heterogeneous scenario. We first split each dataset into train, validation, and test sets for CIFAR-10 ($48,000$/$6,000$/$6,000$) and SVHN ($79,431$/$9,929$/$9,929$). We then split the train set into $K$ local partitions $\mathcal{P}_{1:20}$ ($K$=$20$) for iid partitions (all instances in each class are evenly distributed to all clients) or for the non-iid partitions (instances in each class are sampled from Dirichlet distribution with $\alpha$=$0.5$). We further permute the labels for each class per local partition $\mathcal{P}_k$ for permuted iid and permuted non-iid scenarios. We use different random seed per client, i.e. $\textit{fixed global seed} + \textit{client id}$, for example, $1234 + 0$ for Client 1 and $1234 + 19$ for Client 20. We provide permutations of labels that we used for each dataset in Table~\ref{tbl:dataset_1}.

\input{tables/tbl_dataset_1}

\vspace{-0.3in}
\subsection{Domain Heterogeneous Scenario}

We use CIFAR-100 datasets ($60,000$) and create five sub-datasets grouped by $10$ similar classes, such as Fruits\&Foods ($6,000$), Transport ($6,000$), Household Objects ($6,000$), Animals ($6,000$), Trees\&Flowers ($6,000$). We then split train ($4,800$), test ($600$), validation ($600$) sets for each sub-datset. To have $20$ clients in total, we assign four clients per subdataset, and split each train set into $4$ partitions, making a single partition contains $1,200$ instances. Additionally, we further permute the labels for those $20$ partitions to simulate more realistic scenarios where labeling schemes are not synchronized across all clients even in the same domain (sub-dataset). We provide class division and label permutation information in Table~\ref{tbl:dataset_2}. 


\section{Implementation \& Training Details} 
\label{appdx:details}
In this section, we provide detailed implementation and training details that are not described in the main document.

\subsection{ResNet-9 Architecture}

\input{tables/tbl_resnet_9}
We use ResNet-9 architecture consisting of eight convolutional layers and one fully connected layer as a classifier, as described in Table~\ref{tbl:resnet9}. We use max pooling with size $2$ after Conv $5$ and an adaptive max pooling after Conv $8$ to make output width 1 for the following FC layer. The total number of parameters of the model is $2.57 M$. As we use \texttt{PyTorch} framework for implementation and the default data type of tensor of the framework is 32-bits floating point, the model size can be calculated as $2.57 \times 4 = 10.28$ Mbytes. 


\input{tables/tbl_dataset_2}

\subsection{Calculation of Communication Cost}
We measure the communication cost by $\{(P_{S2C}+P_{C2S}) \times 4 \}_{byte} \times K \times R$, where $P_{S2C}$ is number of server-to-client transmitted parameters and $P_{C2S}$ is number of client-to-server transmitted parameters. Depending on the FL algorithms, $P_{S2C}$ and  $P_{C2S}$ are differently calculated. For example, \texttt{FedFOMO} downloads few random models from server ($10$ as default, reported in the paper) but sends only single local model to server. Our \texttt{Factorized-FL} only sends the  small portion of model parameters, $\mathcal{U}$ and $\textbf{v}^{L-1}$, to server, while receiving a single set of $\mathcal{U}$ from server. 

\subsection{Training Details}

As default, all training configurations are equally set across all models, unless otherwise stated to ensure stricter fairness. We use ResNet-9 architecture as local backbone networks and train them on $32\times32$ sized images with $256$ for batch size. We apply data augmentations, i.e. cropping, flipping, jittering, etc, during training. Optimizer that we used is Stochastic Gradient Descent (SGD). We set $1$e-$3$ for learning rate, $1$e-$6$ for weight decay, and  $0.9$ for momentum. For baseline models, we use the reported hyper-parameters as default, or we adjust hyper-parameters so that they show the best performance for fairness. For ours and \texttt{pFedPara}, the model capacity is adjusted to around $90\%$ - $99\%$ of the original size, as we fairly compare with other methods that use full capacity ($2.57M$ number of parameters). For ours, we use [$5$e-$4$, $1$e-$3$] for $\lambda_{\text{sparsity}}$, [$0$-$0.75$] for $\tau$, [$1$, $20$] for $\epsilon$. 

\section{Additional Experimental Results}
\label{appdx:experiment}

% \subsection{Comparison with the Hadamard Product Factorization}

% \input{figures/fig_pfedpara}

% \texttt{pFedPara}~\citep{anonymous2022fedpara} adopts the Hadamard product that uses two sets of low rank matrices, i.e. $(\textbf{u}_1\times\textbf{v}_1^\intercal)\odot(\textbf{u}_2\times\textbf{v}_2^\intercal)$, and share one of the sets $(\textbf{u}_1\times\textbf{v}_1^\intercal)$ globally for personalized federated learning scenario. Unlike the prior work, our approach utilizes rank-1 vectors to perform aggregation in the lowest subspace possible for compatibility, while effectively yet efficiently enhancing expressiveness with sparse bias matrices. 

% In this subsection, we analyze To see the effectiveness of factorization techniques, we 

\subsection{Sparsity Analysis on FL Scenarios}
\label{appdx:sparisty}

In the main document, we show the effect of model size and sparsity controlled by  $\lambda_{\text{sparsity}}$ for a single model. In this section, we analyze it under federated learning scenario. In Figure~\ref{fig:model_size} (a), we show the performance over model size in domain heterogeneous scenario. As shown, our method show superior performance even with around $65\%$ of the model size over the baseline model that achieves the best performance (\texttt{Per-FedAvg}) amongst other baseline models. With $50\%$ sparsity, ours still shows competitive performance compared to~\texttt{Clustered-FL} and \texttt{FedAvg}, while it starts being significantly degenerated when sparsity becomes over $50\%$. 

\input{figures/fig_model_size}

In Figure~\ref{fig:model_size} (b), we show accuracy over communication costs. Note that, in our method, the model size is not really related to the communication costs since we send very small portion of model parameters. For example, even though we use almost full model size ($\lambda_{\text{sparsity}}$=$3e$-$4$), our communication cost is significantly lesser than the other baseline models, as shown in the figure.


\subsection{Additional Results}

For label-heterogeneous FL scenario (Table~\ref{tbl:permuted} (Top), we provide test accuracy curves over communication rounds and transmission costs for results of CIFAR-10 and SVHN with stardard iid/non-iid and permuted iid/non-iid partitions in Figure~\ref{fig:all}. For domain-heterogeneous FL scenario (Table~\ref{tbl:permuted} (Bottom)), we provide performance of $20$ clients In Figure~~\ref{fig:bar}.

\input{figures/fig_all}
\input{figures/fig_all_cost}
\input{figures/fig_bar}