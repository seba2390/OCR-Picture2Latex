\vspace{-0.15in}
\section{Related Work}

\paragraph{Federated learning} A variety of algorithms have been proposed for federated learning since the introduction of \texttt{FedAvg}~\cite{McMahan2017CommunicationEfficientLO}, but, we specifically focus on works that aim to tackle the heterogeneity problems, e.g. Non-IID. Some studies focus on regularization methods~\citep{mohri2019agnostic,li2020federated}, correcting disparity between server and clients ~\cite{Wang2020Federated,karimireddy2021scaffold}, or contrastive learning~\citep{li2021modelcontrastive}. While we mostly consider the task-level heterogeneity problem in this paper, many existing works also tackle architecture-level heterogeneity~\citep{seo2020federated,zhu2021datafree,diao2021heterofl,shamsian2021personalized}.

%, where we need to aggregate the knowledge among networks with different architectures.

\textbf{Personalized federated learning} aims to improve the individual local clients instead of learning the universal global model via the mixture methods~\citep{mansour2020approaches,deng2020adaptive,hanzely2021federated},   meta-learning approaches~\citep{fallah2020personalized}, or partial network aggregation~\citep{arivazhagan2019federated,liang2020think}. Recent approaches avoid aggregating irrelevant other clients that is not helpful. \citet{zhang2021personalized} downloads and evaluate other clients locally to aggregate only beneficial clients. \citet{sattler2019clustered,duan2021fedgroup} measure client-wise similarity by using the gradient updates. Our method also measures client similarity but in a more efficient and effective way, simply utilizing a factorized vector.

\textbf{Re-parameterization for federated learning} \citet{jeong2021federated,yoon2021federated} decompose model parameters (use an additional set of parameters) to train them with different objectives, which do not reduce the dimensionality of model parameters. Some approaches factorize high dimensional model parameters into lower dimensional space, i.e. low rank matrices.~\citet{konevcny2016federated} introduces structured update which model directly learns 
factorized parameter space. ~\citet{anonymous2022fedpara} propose to use the Hadamard product of low rank matrices to enhance communication efficiency. Unlike prior works, we utilize rank-1 vectors which separately capture task-general and the client-specific knowledge for the extremely heterogeneous FL scenarios without losing expressiveness via sparse bias matrices. 

%However, their motivation is completely different from ours since they allow the rank to be relatively high while we set it to rank 1 vectors, since our motivation is to learn a low-dimensional embedding space.


%Our factorization method uses rank 1 vectors with sparse bias matrix and discuss difference in Figure~\ref{fig:factorize} and Section~\ref{subsec:facto}.

% heterogeneity by  ~\citep{li2020federated,karimireddy2021scaffold,Wang2020Federated,li2021modelcontrastive}. FedProx~\citep{} introduces proximal regularization, while  FedMA~\citep{Wang2020Federated} learns the layer-wise permutations to match differently ordered neurons and filters and before averaging the models. MOON~\citep{li2021modelcontrastive} performs model-level contrastive learning between global and local models. 

%  not only for communication efficiency but also effective knowledge transfer in extremely heterogeneous federated learning scenarios

% \citet{duan2021fedgroup,sattler2019clustered} measure client-wise similarity by using the gradient updates, however, our method uses factorized vector to measure client-wise relevance, which is way more efficient and effective than the prior works.
%None of the prior works reduces the dimensionality of parameter space directly.

% clustering-based approaches~\citep{sattler2019clustered,ghosh2021efficient}

%SCAFFOLD~\citep{karimireddy2021scaffold} utilizes control variates to adjust for the client-drift in the local updates.
%While we mostly consider the task-level heterogeneity problem in this paper, many existing works also tackle architecture-level heterogeneity~\citep{seo2020federated,zhu2021datafree,diao2021heterofl,shamsian2021personalized}, where we need to aggregate the knowledge among networks with different architectures.

% LG-FedAvg~\citep{liang2020think} instead shares the prediction layers.   
% \textbf{Learning from multiple domains} Several recent studies achieved the well-generalized global model on multiple domains via the mixture of the client target  distributions~\citep{mohri2019agnostic}, a distributed multi-task-learning framework~\citep{smith2018federated,multitask@20}, domain adaptation for unlabeled data~\citep{Peng2020Federated,yao2021federated}. Our work is, however, orthogonal to these methods, as we pursue personalized federated learning which improves individual local models.