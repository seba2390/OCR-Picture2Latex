\begin{abstract}

In real-world federated learning scenarios, participants could have their own personalized labels which are incompatible with those from other clients, due to using different label permutations or tackling completely different tasks or domains. However, most existing FL approaches cannot effectively tackle such extremely heterogeneous scenarios since they often assume that (1) all participants use a synchronized set of labels, and (2) they train on the same task from the same domain. In this work, to tackle these challenges, we introduce \texttt{Factorized-FL}, which allows to effectively tackle label- and task-heterogeneous federated learning settings by factorizing the model parameters into a pair of vectors, where one captures the common knowledge across different labels and tasks and the other captures knowledge specific to the task each local model tackles. Moreover, based on the distance in the client-specific vector space, \texttt{Factorized-FL} performs selective aggregation scheme to utilize only the knowledge from the relevant participants for each client. We extensively validate our method on both label- and domain-heterogeneous settings, on which it outperforms the state-of-the-art personalized federated learning methods.

\end{abstract}


% We then study two essential challenges of the agnostic personalized federated learning, which are (1) \textit{Label Heterogeneity} where local clients learn from the same single domain but labeling schemes are not synchronized with others and (2) \textit{Domain Heterogeneity} where the clients learn from the different datasets which can be semantically similar or dissimilar to each other. 