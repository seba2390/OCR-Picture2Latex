



\section{Problem Definition}
\label{sec:def}
We begin with the formal definition of the conventional federated learning scenario, and then introduce our novel Agnostic Personalized Federated Learning (APFL) problem. 

%We then describe the two scenarios for the APFL problem, namely Label and Domain Heterogeneous FL. 

\subsection{Preliminaries}
\label{subsec:fl}
Our main task is solving a given multi-class classification problem in an FL framework. Let $f_g$ be a global model (neural network) at the global server and $\mathcal{F}=\{f_k\}^{K}_{k=1}$ be a set of $K$ local neural networks, where $K$ is the number of local clients.  $\mathcal{D}=\{\textbf{x}_{i}, y_{i}\}^{N}_{i=1}$ be a given dataset, where $N$ is the number of instances, $\textbf{x}_i \in \mathbb{R}^{W \times H \times D}$ is the $i_{th}$ examples in a size of width $W$, height $H$, and depth $D$, with a corresponding target label $y_i \in \{1,\dots,C\}$ for the $C$-way multi-class classification problem. The given dataset $\mathcal{D}$ is then disjointly split into $K$ sub-partitions $\mathcal{P}_{k}=\{\textbf{x}_{k,i}, y_{k,i}\}^{N_{k}}_{i=1}$ s.t. $\mathcal{D} = \bigcup_{k=1}^{K} \mathcal{P}_{k}$, which are distributed to the corresponding local model $f_k$. Let $R$ be the total number of the communication rounds and $r$ denote the index of the $r_{th}$ communication round. At the first round $r$=$1$, the global model $f_g$ initialize the global weights $\theta^{(1)}_{f_g}$ and broadcasts $\theta^{(1)}_{f_g}$ to an arbitrary subset of local models that are available for training at round $r$, such that $\mathcal{F}^{(r)}\subset\mathcal{F}$, $|\mathcal{F}^{(r)}|=K^{(r)}$, and $K^{(r)} \leq K$, where $K^{(r)}$ is the number of available local models at round $r$. Then the active local models $f_{k}\in\mathcal{F}^{(r)}$ perform local training to minimize loss $\mathcal{L}( \theta^{(r)}_{k})$ on the corresponding sub-partition $\mathcal{P}_{k}$ and update their local weights $\theta^{(r+1)}_{k} \leftarrow \theta^{(r)}_{k}-\eta\nabla\mathcal{L}(\theta^{(r)}_{k})$, where $\theta^{(r)}_{k}$ is the set of weights for the local model $f_k$ at round $r$ and $\mathcal{L}(\cdot)$ is the loss function. When the local training is done, the global model $F$ collects and aggregates the learned weights $\theta^{(r+1)}_{f_g} \leftarrow \frac{N_{k}}{N}\sum_{i=1}^{K^{(r)}} \theta_{k}^{(r)}$ and then broadcasts newly updated weights to the local models available at the next round $r+1$. These learning procedures are repeated until the final round $R$. This is the standard setting for centralized federated learning, which aims to find a single global model that works well across all local data. On the other hand, Personalized Federated Learning aims to adapt the individual local models $f_{1:K}$ to their local data distribution $\mathcal{P}_{1:K}$, to obtain specialized solution for each task at the local client, while utilizing the knowledge from other clients. Thus merging the local knowledge for personalized FL is not necessarily done in the form of $\theta^{(r+1)}_{f_g} \leftarrow \frac{N_{k}}{N}\sum_{i=1}^{K^{(r)}} \theta_{k}^{(r)}$, and the specific ways to utilized the knowledge from others depends on the specific algorithm, i.e. $\theta^{(r+1)}_k\leftarrow \theta^{(r)}_k + \sum_{i\neq k}^{K^{(r)}} \omega_i(\theta_{k}^{(r)}-\theta_{i}^{(r)})$, wher $\omega(\cdot)$ is weighing function ~\citep{zhang2021personalized}.


\input{figures/fig_factorize}
\input{figures/fig_factorize_analysis}


\subsection{Agnostic Personalized Federated Learning}
\label{subsec:apfl}

Agnostic Personalized Federated Learning (APFL) is a scenario where any local participants from diverse domains with their own personalized labeling schemes can collaboratively learn, benefiting each other. There exist two critical challenges that need to be tackled to achieve this objective: (1) Label Heterogeneity and (2) Domain Heterogeneity.

\vspace{-0.125in}
\paragraph{Label Heterogeneity } This scenario assumes that the labeling schemes are not perfectly synchronized across all clients, as described in Section~\ref{sec:intro} and Figure~\ref{fig:overview} Left. Most underlying setting for this scenario is the same as the conventional single-domain setting with synchronized labels that is described in Section~\ref{subsec:fl}, except that labels are arbitrarily permuted amongst clients. The local data $\mathcal{P}_{k}$ for the local model $f_k$ is now defined as $\mathcal{P}_{k}=\{\textbf{x}_{k,i}, \varphi_{k}(y_{k,i})\}^{N_{k}}_{i=1}$, where $\varphi_{k}(\cdot)$ is a mapping function for the local model $f_k$ which maps a given class $y_{k,i}$ with a randomly permuted label $p_{k,i}$=$\varphi_{k}(y_{k,i})$. Let the $j$th layer out of $L$ layers in the neural networks of local model $f_k$ be $\ell_{k}^{j}$ and the last layer $\ell_{k}^L$ be the classifier layer. Since each client has differently permuted labels, the personalized classifiers $\ell_{1:K}^{L}$ are no longer compatible to each other. While we can merge the layers below the classifier in this setting, training with heterogeneous labels could still lead to large disparity in the local gradients even in the initial communication round, as described in Figure \ref{fig:concept}.


\vspace{-0.125in}
\paragraph{Domain Heterogeneity} This scenario presumes that local clients learn on their own dataset $\mathcal{D}$, that are completely different from the datasets that are used at other clients, as described in Section~\ref{sec:intro} and Figure~\ref{fig:overview} Right. In this setting, $K$ disjoint datasets $\mathcal{D}_{1:K}$ are assigned to the $K$ local clients $f_{1:K}$, where $\mathcal{D}_k=\{\textbf{x}_{k,i}, y_{k,i}\}^{N_k}_{i=1}$ is the dataset assigned to the local model $f_k$. The number of target classes may differ across clients, such that $y_{k,i} \in \{1,\dots,C_k\}$. We assume complete disjointness across clients, such that there is no instance-wise and class-wise overlap across the datasets: $\varnothing =\bigcap_{k=1}^K \mathcal{D}_k$. Similarly to the label-heterogeneous scenario described above, the personalized classifiers $\ell_{1:K}^{L}$ are no longer compatible to each other due to the heterogeneity in the data and the labels. Hence, the aggregation is done for the layers before the classifier, but they will be also incompatible as the learned model weights will be largely different across domains.


%$\varnothing =\bigcap_{k=1}^K \mathcal{P}_k$