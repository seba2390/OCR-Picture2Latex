
\section{Experiment}
\label{sec:exp}
We validate our method on label- and domain-heterogeneous FL scenarios, against relevant baselines.

\subsection{Experimental Setup}
\label{subsec:setup}

\paragraph{Models} We first consider well-known baseline FL methods, such as (1) \texttt{FedAvg}~\citep{McMahan2017CommunicationEfficientLO} and (2) \texttt{FedProx}~\citep{li2018federated}. We evaluate our factorization technique with (3) \texttt{pFedPara}~\citep{anonymous2022fedpara} which also uses kernel factorization technique for personalized FL scenario. Our similarity matching approach is compared to (4) \texttt{Clustered-FL}~\citep{sattler2019clustered} and (5) \texttt{FedFOMO}~\citep{zhang2021personalized}, which measuring client-wise similarity or helpfulness. (6) \texttt{Per-FedAvg}~\citep{fallah2020personalized} is also used for evaluation as it shows great performance on heterogeneous federated learning scenarios. We also show local training model, (7) \texttt{Stand-Alone}, for the lower bound performance. We introduce an additional variant of \texttt{Factorized-FL}, which aggregates not only the parameter bases across all client, but also coefficient and bias terms well. Particularly, for the standard FL scenarios, i.e. iid or non-iid, where label- and domain-heterogeneity does not exist, aggregation of separately learned knowledge can further effectively improve local performance. We name such model \texttt{Factorized-FL $\beta$}. Please see the Appendix~\ref{appdx:details} for detailed implementation and training details. 


\vspace{-0.1in}
\paragraph{Datasets} (1) \textit{Label Heterogeneous Scenario}: we use CIFAR-10 and SVHN datasets and we create four different partitions for each dataset, which are conventional iid and non-iid as well as permuted iid and permuted non-iid, which labels are permuted and incompatible to each other. We split the datsets into $20$ partitions and then simply permuted the labels on the same partition for label permuted settings. (2) \textit{Domain Heterogeneous Scenario}: we use CIFAR-100 datasets and create five sub-datasets grouped by $10$ similar classes, such as Household Objects, Fruits\&Foods, Trees\&Flowers, Transport, and Animals. We assign $4$ clients for each sub-dataset, i.e. Client 1-4 to Household Objects, Client 5-8 to Fruits\&Foods, Client 9-12 to Trees\&Flowers, Client 13-16 to Transport, and Client 17-20 to Animals. We then permute the labels for all partitions to simulate further realistic scenarios. Further descriptions are elaborated in Appendix~\ref{appdx:dataset}.


\subsection{Experimental Result}
\label{subsec:result}

\paragraph{Label-heterogeneous FL} 
As shown in Table~\ref{tbl:permuted} (Top), for the standard IID and Non-IID settings, all FL methods obtain higher performance than the local training baseline (\texttt{Stand-Alone}), which confirms that the locally learned knowledge is beneficial to others, when the data and label distributions are homogeneous across clients. However, when the labels are not synchronized across all clients (Permuted IID/Non-IID), all previous FL methods achieve significantly degenerated performance, even lower than that of the local training baseline. Again, note that we do not share the classifier layers to ensure fairness across all algorithms in this permuted settings. We conjecture that this is caused by the label permutation leading the local model to evolve a permuted set of features that are not coordinate-wise compatible to others when aggregated. Contrarily, our method \texttt{Factorized-FL} shows consistent performance regardless of whether labels are permuted or not. \texttt{Factorized-FL $\beta$} even largely outperforms all baseline models with significantly superior performance. Test accuracy curves over communication round and transmission cost are visualized in Appendix~\ref{appdx:experiment}.

\input{tables/tbl_single}


% \vspace{-0.1in}
\paragraph{Domain-heterogeneous FL}
Table~\ref{tbl:permuted} (Bottom) shows the experimental results for the domain and label heterogeneous scenarios. We observe that the conventional FL baselines, i.e. \texttt{FedAvg}, \texttt{FedProx}, fail to obtain better performance over purely local training baseline (\texttt{Stand-Alone}) due to the naive aggregation of extremely heterogeneous knowledge, which causes detrimental knowledge collapse. \texttt{FedFOMO} and \texttt{Clustered-FL} shows slightly higher performance ($1-2\%p$) over \texttt{Stand-Alone} model, as they can avoid irrelevant clients when aggregating local knowledge. The other personalized FL methods, i.e. \texttt{Per-FedAvg} and \texttt{pFedPara}, also show $1-2\%p$ higher performance over \texttt{Stand-Alone} model as they are specialized for personalized FL scenarios. 
However, on average, our method largely outperforms all baseline models even with the smallest communication costs, as shown in Figure~\ref{fig:acc_plot}. In the figure we plot the convergence rate of our Factorized-FL framework over communication round and transmission cost, compared to baseline models. Our method consistently obtain superior performance in the extremely heterogeneous scenarios, with significantly faster convergence and superior accuracy per transmission cost. Specifically, unlike \texttt{pFedPara} which uses low rank matrices for knowledge sharing, as we only communicate with factorized vectors, such as  $\textbf{u}$ for base knowledge sharing and $\textbf{v}$ for similarity matching, our method can largely reduce the communication costs while achieving superior performance over it, as shown in Figure~\ref{fig:acc_plot} (Right). 


\input{figures/fig_comm_cost}


\paragraph{Effect of kernel factorization} 
In Figure~\ref{fig:analysis} (g), we perform an ablation study of our factorization method in the domain-heterogeneous scenario. To clearly see the effectiveness of our factorization methods, we compare \texttt{Factorized-FedAvg}, a variant of Factorized-FL $\beta$ without similarity matching, against \texttt{FedAvg}. As shown, \texttt{Factorized-FedAvg} achieves higher performance over the original \texttt{FedAvg} model. As the only difference between the two is whether kernel is factorized or not, this clearly demonstrates that our factorization method alone improves the model performance by alleviating knowledge collapse.

We further analyze the effect of the sparse bias matrix $\mathcal{M}$. When we remove $\mathcal{M}$ from the \texttt{Factorized-FedAvg} model, w/o Mu in the figure, we observe large performance drop. This shows that that the bias term is essential in compensating for the loss of expressiveness from rather extreme factorization of the weight matrices into rank-1 vectors. With only $\mathcal{U}$ and $\mathcal{V}$, we use $90\%$ less model parameters ($0.27 M$) compared to the regular kernel model ($2.574 M$). 

\input{figures/fig_v_visualize}


We can control the sparsity of bias $\mathcal{M}$ by varying the hyper-parameter $\lambda_{\text{sparsity}}$ which gives intensity for the $L_1$ regularizer described in Eq.~\ref{eq:loss}. In Figure~\ref{fig:analysis} (i), we train Client 1 on CIFAR-10 IID Partition 1 for 20 epochs using a single factorized model and a regular model, respectively, and report their model size and performance. As shown, our factorized model still outperforms regular models $(2.574 M)$ even with $30\%$ less parameters $(1.784 M)$ (we further analyze on the effect of the sparsity in Appendix~\ref{appdx:sparisty}). Under the same experimental setup, we also experiment with ResNet-18 architecture to verify the scalability of our method (Figure~\ref{fig:analysis} (h)). Regardless of the model size, our factorized kernel models (the right most) consistently shows better performance. Interestingly, when we add more (factorized) fully-connected layer with or without batch normalization, our method obtains performance improvements, while regular kernel CNN models suffer from performance degeneration. These results demonstrate that our factorization method is scalable, consistent, and reliable in terms of model architectures consisting of convolutional, fully-connected layers, batch normalization, and skip connections. Additionally, we observe that model factorized by $\textbf{u} \in \mathbb{R}^{F \cdot F}$ and $\textbf{v} \in \mathbb{R}^{I \cdot O} $ perform better than the model factorized by $\textbf{u} \in \mathbb{R}^{I \cdot F}$ and $\textbf{v} \in \mathbb{R}^{O \cdot F} $. This is because in the former case, the factorization will separate the base filter knowledge from  the task-specific configurations of the filters, as described in Figure~\ref{fig:factorize_detail}, but the factorization does not have such a natural interpretation in the latter case.  


\vspace{-0.1in}
\paragraph{Effect of similarity matching} 
To verify the efficacy of our similarity matching method using the personalized factorized vector $\textbf{v}$, we visualize the inter-client similarity of $\textbf{u}^{L-1}_{f_k}$ and $\textbf{v}^{L-1}_{f_k}$ form the second last layer of $20$ clients  on domain heterogeneous setting. As shown in Figure~\ref{fig:analysis} from (a) round 1 to (e) round 100, we observe $\textbf{u}^{L-1}_{f_k}$ (upper row) are indeed highly correlated with other clients as the similarity scores are high (the darker color indicate higher values) while $\textbf{v}^{L-1}_{f_k}$ (bottom row) are relatively uncorrelated to each other, as expected as our assumption that $\mathcal{U}_{f_k}$ capture base knowledge across all clients and $\mathcal{V}_{f_k}$ capture personalized knowledge. Further, we also observe $\textbf{v}^{L-1}_{f_k}$ obtains higher similarity to that of parameters trained on the same domains (but with permuted labels), i.e. Client 1-4, Client 5-8, Client 9-12, Client 13-16, and Client 17-20, showing that they are effective in task- and domain-level similarities across models. We further visualize the frequency of the client matching across clients in Figure (f), after $100$ communication rounds. With only a single vector parameter $\textbf{v}^{L-1}_{f_k}$, we both efficiently and effectively find which clients will be helpful to certain other clients. 

For further analysis on our similarity matching, we compare it against Random and Worst Matching baselines under the multi domain scenario. The random matching baseline randomly selects three arbitrary models to be aggregated at each round, while the worst matching baseline selects three most dissimilar models. As shown in Table~\ref{fig:analysis} (g), both random and worst matching methods significantly suffer from the performance degeneration compared to our best matching strategy. This shows that our similarity matching algorithm is indeed effective, selecting beneficial knowledge from other clients. 





%We now provide the experimental results on the label- and domain-heterogeneous FL settings, and analyses of the results.



% We further analyze how heterogeneous the models have evolved in this label-permuted setting. We train 10 equally initialized stand-alone models on the exactly same dataset (CIFAR-10) but with differently permuted labels. Then, we compare euclidean distance of parameters from the models as shown Figure~\ref{fig:distance}. As shown in Figure~\ref{fig:distance} (a), the local models become largely different from each other, as the coordinate-wise correlation among the parameter across clients have very little correlation (the darker the color is, the farther the parameters are to each other). We try the same experiments using our factorization methods. Figure~\ref{fig:distance} (b) and (c) show that our factorized parameters remain more correlated across the clients. This is because the factorization allows the model to aggregate the commonalities across clients via $\textbf{u}$, while allowing $\textbf{v}$ to capture the variations resulting from different permutations, as described in Section~\ref{subsec:facto}.



% These results are from factorization technique that allows efficient similarity matching and effective knowledge sharing across heterogeneous tasks.
% We visualize the matching frequency between the participants in Figuer~\ref{}. As shown, we can see that clients from the same domains (but with permuted labels) are grouped and show the highest matching frequency. 

% They while ours show $3-4\%p$ higher performance. 

% critical challenges of federated learning. When we use Similarity Matching technique, one of our proposed algorithms of Factorized-FL, it achieves much higher scores over the existing federated learning algorithms, meaning that it properly matches and discriminate helpful knowledge amongst clients from different domains. However, it still shows slightly lower performance compared to the Stand-Alone model. This means that merging heterogeneous knowledge is further important even though we can choose the beneficial knowledge, which is an another critical challenges in this domain heterogeneous scenario.

% With our kernel factorization method, Factorized-FL can further improve performance over Stand-Alone model ($4\%p$ higher in average). It shows that our method can successfully alleviate knowledge collapse and information loss while effectively reflecting relevant knowledge to the given local domain. For few datasets (Fruits & Vege., Invertebrates, and Vehicles 1), ours shows marginal performance compared to the Stand-Alone method. We conjecture that those datasets are not really correlated to other local domains in the feature level. Even in this case, our algorithm still can guarantee that the performance can be improved if some additional clients from domains similar to those datasets newly participate in this learning procedure.

% In Figure~\ref{fig:analysis} (i), 


% We further analyze the efficacy of our factorized FL, without similarity matching (see Figure~\ref{fig:analysis} (b)). We conduct experiments on CIFAR-10 in Permuted IID and non-IID settings for 100 rounds. As shown, the combined model obtains $3-4\%p$ higher accuracy over pure federated learning algorithms. As the only difference between pure and combined models is whether kernel is factorized or not, this clearly demonstrates that our factorization method alone improves the model performance by alleviating knowledge cllapse.

% In Figure~\ref{fig:analysis} (g), we perform an ablation study of our factorization method in the same domain heterogeneous scenario. When not using the proposed factorization method, and only use similarity matching, it obtains approximately $4\%p$ lower performance. We observe large performance drop when we use factorized kernel but remove the sparse bias $\mu$. This shows that that the bias term is essential in compensating for the loss of expressive from rather extreme factorization of the weight matrices into rank-1 vectors. With only $\textbf{u}$ and $\textbf{v}$, we use $90\%$ less model parameters (0.27 M) compared to the regular kernel model (2.574 M). We can control the sparsity of bias $\mu$ by varying the hyper-parameter $\lambda_{sparsity}$ which gives intensity for the $L_1$ regularizer described in Eq.~\ref{eq:loss}. In Figure~\ref{fig:analysis} (i), we train a single factorized model and a regular model on CIFAR-10 for 20 epochs, respectively, and show their model size and performance. As shown, our factorized model still outperforms regular models (2.574 M) even with $30\%$ less parameters (1.784 M). 

% Under the same experimental setup, we also experiment with ResNet-18 architecture to verify the scalability of our method (Figure~\ref{fig:analysis} (h)). Regardless of the model size, our factorized kernel models (the right most) consistently shows better performance. Interestingly, when we add more (factorized) fully-connected layer with or without batch normalization, our method obtains performance improvements, while regular kernel CNN models suffer from performance degeneration. These results demonstrate that our factorization method is scalable, consistent, and reliable in terms of model architectures consisting of convolutional, fully-connected layers, batch normalization, and skip connections. Additionally, we observe that model factorized by $\textbf{u} \in \mathbb{R}^{F \cdot F}$ and $\textbf{v} \in \mathbb{R}^{I \cdot O} $ perform better than the model factorized by $\textbf{u} \in \mathbb{R}^{I \cdot F}$ and $\textbf{v} \in \mathbb{R}^{O \cdot F} $. This is because in the former case, the factorization will separate the base filter knowledge from  the task-specific configurations of the filters, as described in Figure~\ref{fig:factorize}, but the factorization does not have such a natural interpretation in the latter case.



% For personalized federated learning baselines, we use , . Both are relevant to our method since Clustered-FL leverages the client-level similarity to perform selective model aggregation as done with our method, and FedFOMO use the validation set to select models that are helpful for the local task, which aim to achieve the same goal as our similarity matching method. The local training baseline trains local models independently without any model aggregation. We compare against it since on the label- and domain-heterogeneous setting, they ironically outperformed the existing FL algorithms due to the large incompatibility in the parameters that are being aggregated. %We also consider Agnostic-FL~\citep{mohri2019agnostic} is since it tackles multi-domain FL scenario similar to our domain-heterogeneous settings.  FedPer~\citep{arivazhagan2019federated},  % \vspace{-0.2in}

% \paragraph{Training details} We use \texttt{ResNet-9} architecture for all algorithms and train on $32\times32$ sized images. We also utilize Stochastic Gradient Descent (SGD) optimizer. Training configurations are equally set  across all models, unless otherwise stated to ensure stricter fairness. We set $1$e-$3$ for learning rate, $1$e-$4$ for weight decay, and $256$ for batch size. For ours and \texttt{pFedPara}, the model capacity is fairly adjusted to $85$ - $99$ of the original size ($2.57M$ number of parameters). Please see Appendix~\ref{appnd:details} for further detailed configurations for each baseline.

%For our algorithm, we use $0.9$ for $\tau$ and $30$ for $\alpha$. $\lambda_{1}$ is set to $1$e-$4$. For all our experiment, please note that the classifier layers of all models are not aggregated in the label- and domain-heterogeneous settings for fairness. % Please see Appendix~\ref{append:config} for further detailed description.

%We use the following datasets for the label- and domain-heteogeneous FL experiments. 

% We first split each dataset into train, validation, and test sets for CIFAR-10 ($48,000$/$6,000$/$6,000$) and SVHN ($79,412$/$9,930$/$9,930$). We then split the train set into $K$ local partitions $\mathcal{P}_{1:20}$ ($K$=$20$) for iid partitions (all instances in each class are evenly distributed to all clients) or for the non-iid partitions (instances in each class are sampled from Dirichlet distribution with $\alpha$=$0.5$). We further permute the labels for each class per local partition $\mathcal{P}_k$ for permuted iid and permuted non-iid datasets. (2) \textit{Domain Heterogeneous Scenario}: we use CIFAR-100 datasets ($60,000$) and create five sub-datasets grouped by $10$ similar classes, such as Fruits\&Foods ($6,000$), Transport ($6,000$), Household Objects ($6,000$), Animals ($6,000$), Trees\&Flowers ($6,000$). We then split train ($4,800$), test ($1,200$), validation ($1,200$) sets for each sub-datset. Now We further partition each train set ($1,200$) for $4$ clients for each sub-dataset so that total number of clients can be $20$. Finally, we permute the labels for each $20$ partition to simulate further realistic scenarios.

% , such as MNIST, . We split each dataset into train, validation, and test sets, such that $55,990$/$7,000$/$7,000$ for MNIST, $47,985$/$6,000$/$6,000$ for CIFAR-10 and CIFAR-100. We then split the train set into $K$ local partitions $\mathcal{P}_{1:20}$ ($K$=$20$) in an identical manner (all instances in each class are evenly distributed to all clients) or a non-identical way (instances in each class are sampled from Dirichlet distribution with $\alpha$=$0.5$). We further permute the labels for each class per local partition $\mathcal{P}_k$ for Permuted IID and Permuted Non-IID setting to simulate the Label-Heterogeneity. (2) Domain Heterogeneous Scenario: we use CIFAR-100 datasets ($60,000$) and split it into 20 datasets $\mathcal{D}_{1:20}$ grouped by 20 superclasses (coarse labels), such as mammals, household furniture, vehicles, etc. In each domain $\mathcal{D}_{k}$ ($3,000$), non-overlapping 5 sub-classes are assigned to each client. Then we split each dataset into train ($2,400$), validation($150$), and test ($450$) sets. Please see further details for the dataset configuration in Section~\ref{subsec:apfl}.

%, and $79,412$/$9,930$/$9,930$ for SVHN. 



%The worst matching obtains the poorest performance (around $20\%p$ lower) as expected, and the random matching baseline also shows $15\%p$ lower performance compared to our similarity matching algorithm. 

% In another analysis, we compare two baseline models, which are ours with no factorization (w/o Factorization) and ours with no masks (w/o Masks $\phi$), in multi-domain scenario with the same training configurations. When we eliminate the factorization technique (which is equivalent to the Similarity Matching only), the model shows about $4\%p$ lower performance compared to our full model, demonstrating factorization has beneficial effect on improving performance in multi-domain scenario. When we remove the sparse masks $phi$, we observe significant performance drop, demonstrating it successfully captures the non-linearity supporting the low rank vectors.

% \paragraph{Alleviation of Information Loss} \blu{\cite{zhang2021personalized} discussed aggregating with unhelpful clients leads to significant performance degeneration. In this sense, we match only relevant clients amongst all clients based on our similarity matching algorithm, ensuring that irrelevant knowledge aggregation can be effectively prevented. This reduces information loss and knowledge collapse, as we can see Similarity Matching outperforms most federated learning algorithms in Table~\ref{tbl:multi}. However, the conventional knowledge aggregation is performed in extremely high dimensional parameter space in a coordinate-wise manner, which may have severe detrimental effects on the averaged model, as studied by~\cite{Wang2020Federated}. In this sense, we significantly reduce the dimensionality of model parameters into low rank vectors (with highly sparse masks) by our factorization method, which empirical results further improves performance achieved by similarity matching (+ Factorization in Table~\ref{tbl:multi}). The results demonstrate information loss is further largely reduced. }

% \input{figures/fig_bar}
% \input{figures/fig_similarity}

% \vspace{-0.1in}
% \input{tables/tbl_sim}


% \input{tables/tbl_fact_2}
% we conversely add our factorization technique to the existing federated learning methods, e.g. FedAvg, and see how the factorization affects the pure federated learning method. We conduct experiments on CIFAR-10 in permuted iid and non-idd settings for 100 rounds. As shown, the combined model shows $3-4\%p$ higher accuracy over pure federated learning algorithms. As the only difference between pure and combined models is the dimensionality of parameter space, which we demonstrate our factorization method alleviate the knowledge collapse and information loss caused by the coordinate-wise aggregation in high dimensional parameter space. Ours still outperforms the baseline model, meaning that our similarity matching algorithms further improves performance.