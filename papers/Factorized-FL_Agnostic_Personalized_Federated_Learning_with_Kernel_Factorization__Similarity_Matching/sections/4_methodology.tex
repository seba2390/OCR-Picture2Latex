


\section{Factorized Federated Learning}
\label{sec:method}

We now provide detailed descriptions of our novel algorithm \texttt{Factorized-FL}. 

\subsection{Kernel Factorization}
\label{subsec:facto}

\citet{Wang2020Federated} discussed that the conventional knowledge aggregation, that is often performed in a coordinate-wise manner, may have severe detrimental effects on the averaged model. This is because the deep neural networks have extremely high-dimensional parameters and thus meaningful element-wise neural matching is not guaranteed when aggregating the weights across different models trained under diverse settings. 

One naive solution to this problem is to factorize model parameters into lower dimensional space, i.e. low rank matrices, as shown in Figure~\ref{fig:factorize} (Left). Conventional approaches, such as SVD, Tucker, or Canonical Polyadic decomposition, however, factorize model parameters after training~\citep{lebedev2014speeding,phan2020stable} is done. Thus, the dimensionality at the time of knowledge aggregation will remain the same as the unfactorized model. ~\citet{konevcny2016federated,anonymous2022fedpara} pre-decompose model parameters to low rank matrices for FL scenarios. While~\citet{konevcny2016federated} use naive low rank matrices, ~\citet{anonymous2022fedpara} uses two sets of low rank matrices to improve expressiveness and utilize them as global and local weights (Figure~\ref{fig:factorize} (Middle)). Unlike prior works, our approach utilizes rank-1 vectors to perform aggregation in the lowest subspace possible for compatibility, while effectively yet efficiently enhancing expressiveness with sparse bias matrices, as shown in Figure~\ref{fig:factorize} (Right) and Figure~\ref{fig:factorize_detail}. Another crucial difference of our method from the previous factorization methods is that, our rank-1 vectors have distinct roles. Our factorization will separate the common knowledge from the task- or domain-specific knowledge, since $\textbf{u}$ could be thought as the bases  (the common knowledge across clients) and $\textbf{v}$ could be thought as the coefficients (client-specific information). 
%Specifically, $\textbf{u}$ captures , and $\textbf{v}$ as describing . 

In Figure~\ref{fig:f_analysis} (a) and (b), which shows the experimental results with the factorized model, we observe that $\textbf{u}$ trained on two datasets becomes closer to that of another dataset (MNIST Partition 1) while $\textbf{v}$ (personalized filter coefficient) remain largely different as federated learning goes on. With this observation, we further aggregate $\textbf{u}$ while allowing $\textbf{v}$ to be different across clients, to allow personalized FL. Further, we use the client specific $\textbf{v}$ for similarity matching, to identify relevant local models from other clients. In following paragraphs, we describe our factorization method in detail, for both fully-connected and convolutional layers. \vspace{-0.15in}


\input{figures/fig_factorize_detail}


\paragraph{Factorization of Fully-Connected Layers} We assume that each local model $f_k$ has a set of local weights $\theta_k$ across all layers; that is, $\theta_k=\{\textbf{W}^{i}_k\}^{L}_{i=1}$. The dimensionality of the dense weight $\textbf{W}_k^{i}$ for each fully connected layer is $\textbf{W}_k^{i} \in \mathbb{R}^{I \times O}$, where $I$ and $O$ indicate respective input and output dimensions. We can reduce the $I \times O$ complexity by factorizing the high order matrix into the outer product of two vectors as follows:
\begin{equation}
\begin{split}
\textbf{W}_k^{i} = \textbf{u}_k^{i} \times \textbf{v}_k^{i \intercal}, \text{where } \textbf{u}_k^{i} \in \mathbb{R}^{I}, \textbf{v}_k^{i} \in \mathbb{R}^{O}
\end{split}
\end{equation}
 However, such extreme factorization of the weight matrices may result in the loss of expressiveness in the parameter space. Thus, we additionally introduce a highly sparse bias matrix $\mu$ to further capture the information not captured by the outer product of the two vectors as follows:
\begin{equation}
\begin{split}
\textbf{W}_k^{i} = \textbf{u}_k^{i} \times \textbf{v}_k^{i \intercal} \oplus {\mu}_k^{i}, 
\text{where } \\ \textbf{u}_k^{i} \in \mathbb{R}^{I},  
\textbf{v}_k^{i} \in \mathbb{R}^{O}, {\mu}_k^{i} \in \mathbb{R}^{I \times O} 
\end{split}
\end{equation}
We initialize $\mu$ with zeros so that it can gradually capture the additional expressiveness that are not captured by $\textbf{u}$ and $\textbf{v}$ during training. We can control its sparsity by the hyper-parameter for the sparsity regularizer described in~\ref{subsec:algo}.

\vspace{-0.05in}
\paragraph{Factorization of Convolutional Layers} The difference between the fully-connected and convolutional layers is that the convolutional layers have multiple kernels (or filters) such that $\textbf{W}_k^{i} \in \mathbb{R}^{F \times F \times I \times O}$, where $F$ is a size of filters (we assume the filter size is equally paired for the simplicity). To induce $\textbf{u}$ to capture base filter knowledge and $\textbf{v}$ to learn filter coefficient, it is essential to design $\textbf{u}\in \mathbb{R}^{F \cdot F}$ and $\textbf{v}\in \mathbb{R}^{I \cdot O}$, but not in arbitrary ways, such as $\textbf{u}\in \mathbb{R}^{I \cdot F}$ and $\textbf{v}\in \mathbb{R}^{O \cdot F}$ or $\textbf{u}\in \mathbb{R}^{O}$ and $\textbf{v}\in \mathbb{R}^{I \cdot F \cdot F}$. We observe that performance is degenerated when the parameters are ambiguously factorized  (Figure~\ref{fig:analysis} (h)). Our proposed factorization method for convolutional layers are as follows:
\vspace{-0.1in}
\begin{equation}
\begin{split}
\textbf{W}_k^{i} = \pi(\textbf{u}_k^{i} \times \textbf{v}_k^{i \intercal} \oplus {\mu}_k^{i}), \text{where }  \textbf{u}_k^{i} \in \mathbb{R}^{F \cdot F }, \textbf{v}_k^{i} \in \mathbb{R}^{I \cdot O }, \\
{\mu}_k^{i} \in \mathbb{R}^{F \cdot F \times I \cdot O}, \pi(\cdot): \mathbb{R}^{F \cdot F \times I \cdot O} \rightarrow \mathbb{R}^{F \times F \times I \times O},
\end{split}
\end{equation}
% \vspace{-0.05in}
$\pi(\cdot)$ is the weight reshaping function. Note that we reparameterize our model \textit{at initialization time}. Then we reconstruct and train full weights of each layer $\textbf{W}_k^{1:L}$, while optimizing $\textbf{u}_k^{1:L}$, $\textbf{v}^{1:L}_k$, and ${\mu}^{1:L}_k$, respectively, during training phase. 






\subsection{Similarity Matching}
\label{subsec:sim}


\input{figures/fig_framework}


Since we assume task- and domain-heterogeneous FL scenarios, aggregating the parameter bases across all clients may not be optimal, since some of them could be highly irrelevant. \citet{yoon2021federated} and \citet{zhang2021personalized} also demonstrated that avoiding aggregation of irrelevant models from other clients improves local model performance. \citet{yoon2021federated} achieve this goal by taking the weighted combination of task-specific weights from other clients, and \citet{zhang2021personalized} suggest downloading the models from other clients and evaluating their performance on a local validation set, at each client. However, since they require additional communication and computing cost at the local clients, we provide a more efficient yet effective approach to find and match models that are beneficial to each other. 

\vspace{-0.1in}
\paragraph{Efficient similarity matching} Our method utilizes factorized vector $\textbf{v}_k$ for measuring similarity across different models, at the central server. Since $\textbf{v}$ are devised to learn personalized coefficient, we assume that clients trained on similar task or domain will have similar $\textbf{v}$. Specifically, we only use $\textbf{v}^{L-1}_{k}$ of the second last layer (before classifier layer) for similarity matching. The similarity matching function $\Omega(\cdot)$, is defined as the cosine similarity between target client $f_k$ and the other clients $\{f_i\}_{i\neq k}^K$, as follows:
\begin{equation}
\label{eq:sim}
\begin{split}
\Omega(\textbf{v}_{f_k}^{L-1}, \textbf{v}_{f_{i\neq k:K}}^{L-1}) = \{\sigma_i |  
\sigma_i = \frac{\textbf{v}_{f_k} \cdot \textbf{v}_{f_i}}{\|\textbf{v}_{f_k}\|\|\textbf{v}_{f_i}\|}
, \sigma_i \geq \tau \}_{i \neq k}^{K}
\end{split}
\end{equation}
The similarity scores for those with the cosine similarity scores lower than the given threshold $\tau$, are set to zero.
Our method is significantly more efficient than similarity matching approaches which use full gradient updates for clustering clients~\citep{sattler2019clustered,duan2021fedgroup}.

\vspace{-0.1in}
\paragraph{Personalized weighted averaging} We allow each local model to perform weighted aggregation of the model weights from other clients, utilizing their similarity scores:
\begin{equation}
\begin{split}
\textbf{u}_{k}^l \leftarrow \frac{\text{exp}({\epsilon \cdot \sigma_i})}{\sum_{i=1}^K \text{exp}(\epsilon \cdot \sigma_i)} \sum_{i=1}^{K} \textbf{u}_i^l, s.t. \forall  l \in \{1,2,\dots,L\}
\end{split}
\end{equation}
where $\epsilon$ is a hyperparameter for scaling the similarity score $\sigma_i$. We always set $\sigma_k$, the similarity score for itself, as $1.0$. 

\subsection{Learning Objective}
\label{subsec:algo}

Now we describe our final learning objective. Instead of utilizing the single term $\theta_k$ for local weights of neural network $f_k$, now let $\mathcal{U}_k$, $\mathcal{V}_k$, and $\mathcal{M}_k$ be sets of $\textbf{u}_k$, $\textbf{v}_k$, and $\mu_k$ of all layers in $f_k$, s.t. $\mathcal{U}_k=\{\textbf{u}^{i}_k\}^{L}_{i=1}$, $\mathcal{V}_k=\{\textbf{v}^{i}_k\}^{L}_{i=1}$, and $\mathcal{M}_k=\{{\mu}^{i}_k\}^{L}_{i=1}$, then our local objective function is,
\begin{equation}
\begin{split}
\min_{\mathcal{U}_k,\mathcal{V}_k,\mathcal{M}_k} \sum_{\mathcal{B} \in \mathcal{D}_k} \mathcal{L}(\mathcal{B}; \mathcal{U}_k, \mathcal{V}_k, \mathcal{M}_k) + \lambda_{\text{sparsity}} ||\mathcal{M}_k||_1,
\end{split}
\label{eq:loss}
\end{equation}
where $\mathcal{L}$ is the standard cross-entropy loss performed on all minibatch $\mathcal{B} \in \mathcal{D}_k$. We add the $L_1$ sparsity inducing regularization term to make the bias parameters highly sparse, controlling its effect with a hyperparameter $\lambda_{\text{sparsity}}$. Please see our pseudo-coded algorithm Algorithm 1 in Appendix~\ref{appdx:algorithm}.














%We also study such detrimental effects in Section~\ref{sec:intro} and Figure~\ref{fig:concept}. Even with the same initialization, models become extremely heterogeneous and incompatible to each other by label and domain heterogeneity in federated learning scenarios (Figure~\ref{fig:concept} (c)). 

%Reducing the dimensionality of model parameters may alleviate such issues caused by the coordinate-wise knowledge aggregation in the high dimensional space. 


%We also evolve each factorized vectors into one capturing the common knowledge and the other capturing the client-specific knowledge for personalized FL, where the latter is also used to measure the inter-model similarity, which is another crucial difference from previous factorization methods (Figure~\ref{fig:factorize} (Right)). 


% The prior work recommends to set proper rank $\gamma$ values for expressiveness, which increases dimensionality, our method simply uses rank 1 vectors, such as $\textbf{u}$ and $\textbf{v}$ (Figure~\ref{fig:factorize} Right). Our method can separately learn client-general (filter base) and client-specific (filter coefficient) knowledge and utilize them individually. 


%. In Figure~\ref{fig:f_analysis} (a) and (b), We train three (factorized) ResNet-9 networks on CIFAR-10 dataset and show their performance. Even with only $77\%$ of model capacity, our method show superior performance over the Hadamard product approach and even the full kernel model. 

%This results show that our factorization method can separate general and specific knowledge from the extremely high dimensional space while significantly reducing the dimensionality of parameter space, which has great applicability to extremely heterogeneous federated learning scenarios that we want to tackle.

%Such naive aggregation could be less dangerous if there exist some homogeneity across different clients, for example, if the private local data that the each model trains on have the same distribution, and if all models have the same initializations. However, when the local models learn on highly heterogeneous tasks from diverse domains, such simple aggregation scheme may lead to inter-client interference~ \citep{yoon2021federated} where the aggregated model achieves lower performance than those obtained by the local models without aggregation.

% To overcome such issues with simple parameter averaging, we aim to learn a low-dimensional subspace on which the projects of the parameters across different clients become more compatible to each other. To this end, we factorize the parameters of the local models into lower-rank vector components. 



%This efficient similarity matching is another clear advantage of our factorized-FL framework.

%Without additional training or evaluation process at each local client~\citep{yoon2021federated,zhang2021personalized}, 

%\paragraph{Effectiveness} Since $\textbf{v}$ are devised to learn personalized coefficient, we assume that clients trained on similar task or domain will have similar $\textbf{v}$. Thus, we can easily identify which clients are relevant to each other by measuring distance of $\textbf{v}$. We demonstrate its effectiveness by showing that clients are successfully clustered by simply matching $\textbf{v}_{f_k}^{L-1}$ in domain and label heterogeneous scenarios, in Figure~\ref{} (detailed explanation is described in Section~\ref{sec:exp}).



%We can further avoid aggregating irrelevant clients whose similarity scores are lower than the given threshold $\tau$.  

%Note that we omit the layer-wise notation for simplicity.
%where $\Theta$ is a set of $K$ local weights maintained at the server and $\Omega(\cdot)$ is a similarity function that returns a set of pairs of the similarity score $\sigma_i$ and the corresponding local weights $\theta_i$ for all $i \in \{1,2,\dots,K\}$, while satisfying the similarity scores are greater or equal to the given threshold $\tau$. 

% We propose a scheme to aggregate the parameters of only the most relevant local models, based on their task-level similarity. 
%\vspace{-0.05in}
% \input{algorithms/algo_factorized_fl}

% \input{figures/fig_distance}


% \paragraph{Personalized Knowledge Reflection} We avoid averaging the local knowledge directly. Instead, we individually reflect only difference between target local model and the other clients to preserve local reliability, inspired by~\cite{zhang2021personalized}. First, given local model $f_k$, we select the other beneficial knowledge that are relevant to $f_k$ via $\Omega(\cdot)$ described in Eq.~\ref{eq:sim}, returning a set of $J$ pairs of $(\sigma_j, \zeta_j,\xi_j,\phi_j)$. Second, we separately update each factorized parameters $\zeta_j,\xi_j,$ and $\phi_j$ while minimizing the collapse of information as follows:
% \begin{equation}
% \begin{split}
% \zeta_{k} \leftarrow \zeta_{k} + \frac{exp(\alpha \cdot \sigma_j)}{\sum_{1:J} exp(\alpha \cdot \sigma_j)} \cdot \sum_{j=1}^{J} (\zeta_{j}-\zeta_{k}),
% \end{split}
% \end{equation}
% where $\alpha$ is scalar value for scaling the reflection ratio of similarity score $\sigma_j$. This personalized knowledge reflection is also equally applied to $\xi_j$ and $\phi_j$, respectively (please see Algorithm~\ref{algo:simfed}).




% Our idea is to view the individual local models as respective encoders that can effectively transform their locally learned knowledge to their own personalized vector embeddings. If we equally inject the same identical input to all encoders, then they will interpret the criteria inputs depending on what they have learned in their own local environments and produce their own personalized transformations, which we can efficiently utilize them as representations of local knowledge to measure the relevance. Formally, the neural networks $f_{k}$ can be represented as:
% \begin{equation}
% \begin{split}
% f_{k}(\hat{\textbf{y}}|\textbf{x}) = P_k(\hat{\textbf{y}}|E_k(\textbf{z}|\textbf{x})),
% \end{split}
% \end{equation}
% where $f_{k}$ is a combination of an encoder  $E_{k}(\textbf{z}|\textbf{x})$ and a predictor $P_{k}(\hat{\textbf{y}}|\textbf{z})$, where $\textbf{z} \in \mathbb{R}^d$ is $d$-length transformed latent vector of input data \textbf{x} and $\hat{\textbf{y}} \in \mathbb{R}^C$ is a logit for the final prediction. We generate the unbiased criteria input $\textbf{x}_{\mathcal{N}} \in \mathbb{R}^{W \times H \times D}$ from the Gaussian normal distribution, such that ${x}_{i} \sim \mathcal{N}(0,1)$ where $x_i$ is its element, and assume that it is located at the server. Since we have $K$ local models $f_{1:K}$, we equally feed the single criteria input $\textbf{x}_{\mathcal{N}}$ to $K$ local encoders so that we can generate a set of $K$ multiple personalized transformations $\mathcal{Z}_\mathcal{N}$ as follows:
% \begin{equation}
% \begin{split}
% \mathcal{Z}_\mathcal{N} = \{\textbf{z}_{\mathcal{N},k} |  \textbf{z}_{\mathcal{N},k}=E_{k}(\textbf{z}|\textbf{x}_{\mathcal{N}})  \}^{K}_{k=1}
% \end{split}
% \end{equation}


% Now our remaining task is to compute the relevancy between the transformations $\mathcal{Z}_\mathcal{N}$ which each local knowledge is efficiently encoded. We use the cosine similarity to measure the relevance between target model $f_k$ with the rest of models in $\mathcal{F}$, as follows:
% \begin{equation}
% \label{eq:sim}
% \begin{split}
% \Omega(k, \tau, \mathcal{Z}_\mathcal{N}, \Theta) = \{(\sigma_i, \theta_i) | \sigma_i = \frac{\textbf{z}_k \cdot \textbf{z}_i}{\|\textbf{z}_k\|\|\textbf{z}_i\|}, \\ \sigma_i \geq \tau,\textbf{z}_i\in\mathcal{Z},\theta_i\in\Theta \}_{i=1}^{K},
% \end{split}
% \end{equation}
% where $\Theta$ is a set of $K$ local weights maintained at the server and $\Omega(\cdot)$ is a similarity function that returns a set of pairs of the similarity score $\sigma_i$ and the corresponding local weights $\theta_i$ for all $i \in \{1,2,\dots,K\}$, while satisfying the similarity scores are greater or equal to the given threshold $\tau$. 