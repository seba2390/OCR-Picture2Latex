\section{Evaluation}
\label{sec:eval}



\subsection{Experimental Setup}

\mypara{Data Collection}
We collected source code for seven open-source projects, including OpenSSL, OpenJPEG, FFmpeg, TCPDUMP, LibTIFF, cURL, and LibPNG. 
These projects are selected from diverse domains like communication protocols, image processing tools, and network traffic analyzers.
%There are 350 versions in these seven projects containing both vulnerable and patched functions.
After manual analysis, we extracted 906 CVEs corresponding to 1,281 vulnerable functions.
\autoref{tab:projects} lists the versions, application domains, CVE information, vulnerability, and code-related information.


\mypara{Baseline Tool Selection and Testbed}
We prepared two state-of-the-art baseline tools Asm2Vec \cite{asm2vec} and PalmTree \cite{Palmtree}, because of their popularity and excellent performance in vulnerability detection.  
We ran \name and Asm2vec on an Intel NUC kit (NUC8i5BEH) with an i5-8259U processor and 16 GB memory. 
Since Palmtree is a deep learning-based approach and requires intensive GPU power, we ran it on an accelerator cluster of high-performance computer (HPC) systems with 456 NVidia Tesla P100, 114 Dual Xeon 14-core E5-2690, and 256 GB memory.


\mypara{Project Compilation}
As mentioned in \autoref{sec:data preparation}, we compile all the versions relating to each vulnerability (i.e., the last version before patching and the first version after patching) of the project to generate binary code instances. 
Depending on the project, we use the projects' default compiling flags, either \texttt{-O2} or \texttt{-O3}. 
For each project, we use identical compiling flags for building. 
So when we diff the compiled binary code to generate vulnerability and patch signatures, the compiling options are the same. This minimizes the differences in binary codes and is the common practice as \cite{binxray, viva} to help find vulnerable instructions. 
At compile time, we set the debugging symbol option to acquire source-binary instructions mapping that will serve as ground truth.

\begin{figure}[!t]
\centering
\includegraphics[width=0.5\textwidth]{graphs/sig3.pdf}
\caption{Interpreting a \texttt{many-block-change signature} matching. The left-hand side is the generated vulnerable signature, and the right-hand side is the matched instructions in the query binary.}
\label{fig:sig3}
\end{figure}


\mypara{Research Questions}
In the first experiment, we compare \name with two state-of-the-art baseline tools to evaluate how well they find known binary code vulnerabilities.
In the second experiment, we test how \name interprets the found similar vulnerabilities and how \name assists humans in understanding the reason it considers the query binary vulnerable. 
In the third experiment, we match vulnerabilities in real-world firmware binaries to test how \name work in a real-world application.
In the fourth experiment, we investigate how diverse types of proposed vulnerability signatures (i.e., add, delete, and change) distribute.


\subsection{Performance Metrics}

\mypara{Top-1 Score}
%Each CVE in the signature database may involve one or multiple binary files (.o file). 
Each vulnerable function was patched after a certain version. 
And all the versions or a range of function versions are vulnerable before that specific version.
Therefore, we select a vulnerable binary function $f$ from binary code $\mathcal{B}$ to test how the tools discover similar vulnerabilities. We construct the vulnerable and patch signature of $f$ from the last pre-patch version and the first post-patch version and store the signature in the database.
To test how well the signature in the database can be matched, we prepare a binary version (denoted by $\mathcal{B}_{v}$) containing the vulnerable version of $f$ (denoted as $fv$), and a patched version binary (denoted by $\mathcal{B}_{p}$) containing the patched version of $f$ (denoted as $fp$) for testing purpose. 
%We randomly select $\mathcal{G}_{V}$ and $\mathcal{G}_{P}$ from our collected ground truth CVE information.  



$\mathcal{B}_{V}$ and $\mathcal{B}_{P}$ should differ from the versions that generate the binary signature. $\mathcal{B}_{v}$ and $\mathcal{B}_{p}$ contain many functions, including the vulnerable and patched version of $f$, and other functions. 
%We will evaluate how well the signatures generalize to differentiate other vulnerable/patched versions. 
 %test whether \name find the ground-truth vulnerable function $f$ and whether .  
%As we use the last vulnerable version and the first patched version to generate vulnerability signatures. 
For vulnerable function $f$, we inspect each function $fi$ in both $\mathcal{B}_{v}$ and $\mathcal{B}_{p}$ to derive a match score indicating the percentage that $fi$ is similar to $f$'s vulnerable signatures. 
$fv$ in $\mathcal{B}_{v}$ should have the highest score among all other functions; conversely, $fp$ and all other functions in $\mathcal{B}_{p}$ should have low match score. It is reasonable for $fp$ in $\mathcal{B}_{p}$ to have a higher score than other functions in $\mathcal{B}_{p}$ since $fp$ is patched from $f$. 
Nevertheless, $fp$ should be lower than $fv$'s score.
We use the top-1 score to measure the rate of ranking ground truth vulnerable function in the first place. 


We provide a simple example of how the top-1 score works in \name. 
Suppose there are ten vulnerable functions, each with a vulnerable and a patched binary version ${B}_{v}$ and ${B}_{p}$. ${B}_{v}$ contains $fv$. ${B}_{p}$ contains $fp$. Both ${B}_{v}$ and ${B}_{p}$ also contain many other functions.
We match the vulnerable signature of $f$ in the database with each function in both ${B}_{v}$ and ${B}_{p}$. 
If vulnerable function $fv$ has the highest score, we rank $fv$ at the top-1 place.  
If 8 out of 10 vulnerable functions rank their testing vulnerable version $fv$ in the top-1 place, then the top-1 score is 0.8. 
% Each has a testing ground-truth vulnerable version function and a testing ground-truth patched version function. 
%Iteratively, \name measures the top-1 score for each vulnerable function in testing vulnerable and patched binaries. 

\mypara{Mismatch Score}
Merely referring to the top-1 score partly reflects how accurately the tools distinguish ground truth vulnerable functions from other functions. 
However, the top-1 score cannot well demonstrate how the tools consider the non-ground-truth vulnerable function as non-vulnerable. 
A tool that identifies vulnerable functions well with a high top-1 score may not identify non-vulnerable functions as not vulnerable well.
If non-vulnerable functions have extremely close match scores to vulnerable functions, this leads to a high mismatch score.


For instance, some tools may output a similarity score of the ground truth vulnerable function as 0.98, while the score for the ground truth patched function or another random function is 0.97. 
In this case, even though the ground truth function is ranked first, the two scores are too close to reaching the final verdict. 
The ground truth vulnerable function should have a significantly higher score than any other function. 
If any non-ground-truth vulnerable function has a score close to or higher than the ground-truth vulnerable function, the function receives a non-zero \textbf{mismatch score}. 


The mismatch score indicates the reliability of the top-1 score.
To keep track of the mismatch score of each vulnerable function, the $\alpha$ parameter is a threshold to activate the mismatch score. 
We consider it a mismatch for any non-vulnerable function with a threshold above $S_{GV}-\alpha$, where $S_{GV}$ denotes the ground-truth vulnerable function score.  
%Since Asm2vec has a default threshold of 0.6 with output ranges from 0 to 1, and Palmtree has a default threshold of 0 with an output range from -1 to 1, any score below that threshold denotes the query function is dissimilar and filtered out directly. To align with Asm2vec and Palmtree, \name also has a threshold of xxx to firstly filter out dissimilar functions.
If $S_{GV}$ has an extremely low score (e.g., near zero), any non-vulnerable function having a score close to or above $S_{GV}$ is not considered a mismatch. 
Because the root failure occurs in detecting a vulnerability function rather than non-vulnerable functions, we set $S_{GV} < 0.6$ as an extremely low score.

\subsection{Vulnerability Detection}
Since \name aims to find replicated known vulnerabilities, and the two baseline tools Asm2vec and Palmtree find vulnerabilities based on binary code similarity, we compare \name with those two baseline tools to test their performance on the first objective --- finding replicate vulnerabilities. 
Moreover, the query binary is non-deterministic in real-world scenarios since it could be either a vulnerable or patched version. 
Thus, it is vital to the second objective --- differentiate vulnerable functions from other non-vulnerable functions. 
Therefore, we design this experiment to test these two goals concurrently. 



\mypara{Vulnerable Function Detection Accuracy}
\autoref{tab:top-1} lists top-1 scores of \name, Asm2vec, and Palmtree on the seven selected projects. 
Regarding the top-1 score, \name outperforms both baseline methods in six projects and is marginally lower than Palmtree on OpenSSL. 
\name ranks multiple testing vulnerable functions at the top after it extracts accurate vulnerability signatures and matches vulnerable and patched signatures. 
It is because \name matches the fine-grained vulnerability-related instruction (signature) rather than coarsely matches the whole function. 
Since the vulnerability signature tends to be small snippets of instructions, matching the whole function similarity fails to detect such fine-grained information.

\begin{table}[!t]
\centering
\caption{Top-1 scores of seven open-source projects. A higher score indicates a better performance.}
\label{tab:top-1}
\footnotesize
\begin{tabular}{l|c|c|c|c|c|c|c}
\hline
  Project & \rotatebox[origin=c]{270}{Openjpeg} & \rotatebox[origin=c]{270}{FFmpeg} & \rotatebox[origin=c]{270}{Tcpdump} & \rotatebox[origin=c]{270}{Libtiff} & \rotatebox[origin=c]{270}{curl} & \rotatebox[origin=c]{270}{LibPNG} & \rotatebox[origin=c]{270}{OpenSSL}\\
 \hline
 Asm2vec& 0.673& 0.643 &	0.702 & 0.675 & 0.821 & 0.815 & 0.536 \\
 Palmtree& 0.573 & 0.643 & 0.702 & 0.779 & 0.840 & 0.837 & \textbf{0.691}	\\
 \name& \textbf{0.791} & \textbf{0.714} & \textbf{0.786} & \textbf{0.825} & \textbf{0.872} & \textbf{0.859} & 0.673	\\
\hline
\end{tabular}
\end{table}




\mypara{Non-Vulnerable Function Detection Accuracy}
\autoref{tab:mismatch} lists mismatch measurements of \name, Asm2vec, and Palmtree with respect to different $\alpha$ values.  
In the mismatch score perspective, \name achieved the best result with the lowest mismatch score, indicating that \name differentiates the vulnerable version and patched versions with the highest confidence level. 
Conversely, Asm2vec and Palmtree had high mismatch scores, indicating that many decisions between vulnerable and patched versions were made with low confidence. 
Since $\alpha$ denotes the threshold distance to the vulnerable version and $S_{GV} < 0.6$,  we vary $\alpha$ between 0.1 and 0.4 to obtain positive mismatch scores. 
Our evaluation results empirically suggest that $alpha=0.1$ yields the best result. 


For FFmpeg, \name achieved mismatch scores as 0 compared with baseline methods' mismatch scores of approximately 1.
For other projects, there are huge contrasts between \name and baseline tools.
\name outperforms two baseline tools because  \name derives vulnerable signatures from the vulnerable and patched versions. 
Another reason is \name matches the fine-grained vulnerability signature rather than the coarse whole function similarity. 
Therefore, subtle vulnerability-related differences are accurately identified, which is superior to whole-function-level similarity matching.
%\name's signature captures the unique pattern in the vulnerable version while absent in the patched version. 
%Asm2vec and Palmtree use the similarity-based approach to measure similarity according to the complete function instructions. 
%Asm2vec and Palmtree might ignore subtle differences between vulnerable and patched versions.


\subsection{Interpretability}

% \begin{figure}
% \centering
% \includegraphics[width=0.5\textwidth]{graphs/sig2.pdf}
% \caption{Interpreting an add signature matching. The left-hand side is the generated vulnerable signature. The right-hand side is the matched instructions in the query binary.}
% \label{fig:sig2}
% \end{figure}



When finding vulnerable functions, a tool's interpretability is as important as its high accuracy. 
In practice, vulnerability-detecting tools assist human experts in making a final verdict. 
Therefore, a good tool should clearly explain why a query result is considered vulnerable.  
Unfortunately, the state-of-the-art baselines fail to provide good interpretation functionality. 
Palmtree outputs only the overall similarity score between the query function and the functions stored in its database. 
In addition to the overall similarity score, Asm2vec lists similar instructions for the query. 
Asm2vec fails to highlight the vulnerability-related instructions; instead, it highlights the whole function as different or similar.
%Moreover, Asm2vec suffers from poor accuracy.



\begin{table}[!t]
\centering
\caption{Mis-match scores of seven open-source projects. A stands for Asm2vec, P for Palmtree, and V for \name. A lower score indicates a better performance.}
\label{tab:mismatch}
\footnotesize
\begin{tabular}{c|c|c|c|c|c|c|c|c}
\hline
 $\alpha$&\rotatebox[origin=c]{270}{Project} & \rotatebox[origin=c]{270}{Openjpeg} & \rotatebox[origin=c]{270}{FFmpeg} & \rotatebox[origin=c]{270}{Tcpdump} & \rotatebox[origin=c]{270}{Libtiff} & \rotatebox[origin=c]{270}{curl} & \rotatebox[origin=c]{270}{LibPNG} & \rotatebox[origin=c]{270}{OpenSSL}\\
 \hline
%\multirow{3}{*}{Top-1 score}& Asm2vec& 0.673& 0.643 &	0.702 & 0.675 & 0.821 & 0.815 & 0.536 \\
% &Palmtree& 0.573 & 0.643 & 0.702 & 0.779 & 0.840 & 0.837 & \textbf{0.691}	\\
% &\name& \textbf{0.791} & \textbf{0.714} & \textbf{0.786} & \textbf{0.825} & \textbf{0.872} & \textbf{0.859} & 0.673	\\
%\hline

\multirow{3}{*}{0.1}&A&  0.700 & 0.929 & 0.881 & 0.968 & 0.949 & 0.924 & 0.945\\
&P&	0.909 & 0.821 & 0.905 & 0.955 & 0.750 & 0.946 & 0.936\\
&V&	\textbf{0.091} & \textbf{0.000} & \textbf{0.190} & \textbf{0.162} & \textbf{0.038} & \textbf{0.098} & \textbf{0.118}\\
\hline

\multirow{3}{*}{$0.2$}&A&0.836 & 0.964 & 0.988 & 1.000 & 1.000 & 1.000 & 1.000	\\
&P&0.982 & 1.000 & 0.940 & 1.000 & 0.885 & 1.000 & 0.955	\\
&V&\textbf{0.127} & \textbf{0.000} & \textbf{0.286} & \textbf{0.260} & \textbf{0.103} & \textbf{0.152} & \textbf{0.127}	\\
\hline

\multirow{3}{*}{$0.3$}&A& 0.900 & 1.000 & 1.000 & 1.000 & 1.000 & 1.000 & 1.000\\
&P&0.991 & 1.000 & 0.940 & 1.000 & 0.974 & 1.000 & 0.955\\
&V&	\textbf{0.155} & \textbf{0.000} & \textbf{0.429} & \textbf{0.312} & \textbf{0.147} & \textbf{0.163} & \textbf{0.182}\\
\hline

\multirow{3}{*}{$0.4$}&A&0.962 & 1.000 & 1.000 & 1.000 & 1.000 & 1.000 & 1.000\\
&P&1.000 & 1.000 & 0.976 & 1.000 & 1.000 & 1.000 & 0.964\\
&V&\textbf{0.173} & \textbf{0.000} & \textbf{0.500} & \textbf{0.383} & \textbf{0.224} & \textbf{0.196} & \textbf{0.218}\\
\hline
\end{tabular}
\end{table}





\autoref{fig:sig3} demonstrates an example of \name's interpretability.
%\autoref{fig:sig2} demonstrates the generated add signature in vulnerable function \texttt{j2k\_read\_SPCod\_SPCoc} on the left-hand side. The signature was extracted by using version 2.1.0 and version 2.1.1. The example was selected from CVE-2016-1681. On the right-hand side is the mostly matched binary instructions in the query binary code from version 2.0.1. And the matched instructions turn out to be in the same function \texttt{j2k\_read\_SPCod\_SPCoc}. The query binary is . function \texttt{j2k\_read\_SPCod\_SPCoc} has more than one signature but for the paper page limitation, we only show this signature here. The overall matching score for all the signatures is over 0.0.988. The total instructions in all structures in current vulnerable signature is 19 and 18 of them have found matching instructions in the query function. The only difference turn out to be the \texttt{lea rdx, [rip + 0xcac1]} instruction on left-hand side and \texttt{lea rdx, [rip + 0xc89f}. The difference is caused by the change of some structure members. 
This example is a \texttt{many-block-change} vulnerable signature matching selected from CVE-2016-9117. 
The signature (left-hand side) was extracted from the \texttt{imagetopnm} function with versions 2.1.2 and 2.2.0. 
The matched instructions (right-hand side) in the query binary are from version 2.1.1. 
For the selected signature, there are 23 instructions in all structures, and 19 of them are matched. 
The unmatched instructions \texttt{mov rcx, qword ptr [rip + 0xfc246]}, \texttt{mov rcx, qword ptr [rip + 0xfc22b]} on the left-hand side and the instructions \texttt{mov rcx, qword ptr [rip + 0xfc236]}, \texttt{mov rcx, qword ptr [rip + 0xfc21b]} on the right-hand side have different offsets due to structure fields are changed. 
Note that this vulnerable function has multiple signatures, and we omit others for clarity. 
The overall match score combining all signatures exceeds 0.867, indicating \name's high confidence level for the verdict.


\subsection{Real-world Vulnerability Detection}
Since IoT devices' firmware reuse open-source projects, they often contain 1-day vulnerabilities. 
In this experiment, we evaluate how effectively \name detects a real-world 1-day vulnerability in an IoT device's firmware. 
We select four IoT devices' firmware instances (i.e., DCS-3511, DCS-6517, DCS-7517, and DCS-6915) collected in the wild. 
We manually analyze the firmware and prepare 36 ground-truth 1-day vulnerabilities, including 52 vulnerable functions. 
We generate the vulnerability binary code signatures and store them in the database. 
For each vulnerable signature in the database, we detect it against each function $Fi$ in the firmware and assign a matching score for $Fi$. 
If the $Fi$ with the top score is the ground-truth vulnerable function, a vulnerable function is correctly detected. 
\name correctly detects 40 out of 52 (77\%) vulnerable functions. Again, the high accuracy in finding real-world replicate vulnerabilities is due to \name's concentration on the fine-grained vulnerable instructions along with the local control-flow information. We manually analyzed the failed case and found two main failure causes: 1) The binary code contains other function(s) with high similarity to the vulnerable one. 2) The testing binary code contains different structure fields thus at the binary level, the offsets of the structures are different from the signature in the database. For example, \texttt{[esi+0x40]} changed to \texttt{[esi+0x48]} where \texttt{esi} is the memory address of the structure. The same field changed from offset \texttt{0x40} to offset \texttt{0x48} because of adding or deleting other fields in the structure.


\subsection{Statistics of Signature Distributions}

\begin{table}[!t]
\centering
\caption{Vulnerability CWE types of the seven open source projects.}
\label{tab:cwe}
\footnotesize
\begin{tabular}{c|c|c|c|c|c|c|c}
\hline
 & \rotatebox[origin=c]{270}{openssl} & \rotatebox[origin=c]{270}{openjpeg} & \rotatebox[origin=c]{270}{libtiff} & \rotatebox[origin=c]{270}{libpng} & \rotatebox[origin=c]{270}{ffmpeg} & \rotatebox[origin=c]{270}{curl} & \rotatebox[origin=c]{270}{tcpdump}\\
 \hline
%Other 
NVD-CWE-Other&\textbf{0.15} & 0.01 & 0.04 & \textbf{0.15} & 0.03 & 0.00 & 0.00\\
 %Resource Management Errors 
 CWE-399&\textbf{0.12} & 0.00	&0.01&	0.12	&0.08	&0.00	&0.00\\
%Cryptographic Issues 
CWE-310&\textbf{0.12}	&0.00	&0.00&	0.00&	0.00	&0.00&	0.00\\
%Out-of-bounds Write 
CWE-787&0.02	&\textbf{0.15}	&\textbf{0.10}	&0.00&	0.02	&0.00	&0.01\\
%Improper Restriction of Operations within the Bounds of a Memory Buffer
CWE-119&0.11	&\textbf{0.33}	&\textbf{0.41}&	\textbf{0.31}&	\textbf{0.37}&	0.00	&\textbf{0.27}\\

%Integer Overflow or Wraparound 
CWE-190& 0.01	&\textbf{0.10}&	0.03&	0.02&	0.01&	0.00&	0.01\\
%Out-of-bounds Read 
CWE-125&0.03&	0.06&	\textbf{0.13}&	0.02&	0.03	&0.04	&\textbf{0.60}\\
%Numeric Errors 
CWE-189&0.04&	0.02&	0.06&	\textbf{0.19}&	\textbf{0.13}	&0.00	&\textbf{0.02}\\
%Insufficient Information 
NVD-CWE-noinfo&0.01	&0.01&	0.00	&0.08	&\textbf{0.14}&	0.00&	0.01\\
%Buffer Over-read 
CWE-126&0.00	&0.00&	0.00&	0.00&	0.00	&\textbf{0.13}&	0.01\\

%Heap-based Buffer Overflow 
CWE-122& 0.00&	0.05&	0.00&	0.00	&0.00	&\textbf{0.09}&	0.00\\
%Authentication Bypass by Primary Weakness 
CWE-305&0.00&	0.00	&0.00	&0.00&	0.00&	\textbf{0.09}&	0.00\\
\hline
\end{tabular}
\end{table}




In this experiment, we investigate the distribution of the vulnerability according to 1) the Common Weakness Enumeration (CWE) type and 2) our defined three types (i.e., add, delete, change). 
\autoref{tab:cwe} lists the vulnerability distribution according to different CWE types. 
Specifically, we select the three most popular CWE types for each project and concatenate them into the table. 
We observe that \texttt{Improper Restriction of Operations within the Bounds of a Memory Buffer (CWE-119)} is the most common vulnerability type in our experiment (5 in 7 projects). 
Curl contains the most CWE vulnerability types (43 types), while LibPNG contains the least CWE types (11 types).
%Note that each CVE can contain multiple vulnerable functions and each vulnerable function many contain one or multiple CWE types. Improper Restriction of Operations within the Bounds of a Memory Buffer (CWE-119) is the most vulnerability type among the total seven dataset (5 in 7 projects). In each project containing CWE-119, the proportion ranges from 0.272 to 0.405. At the meantime, CWE-119 is also the most occuring vulnerability in Openjpeg, Libtiff, LibPNG, and FFmpeg. The definition of CWE-119 is as follows: The application was designed to operates on one memory buffer, but can read or write a memory location which is outside of the intended buffer boundary. In the other three projects (i.e., OpenSSL, Curl, Tcpdump), the top most occuring CWE vulnerability are \texttt{Other (NVD-CWE-Other)}, \texttt{Buffer Over-read (CWE-126)}, and \texttt{Out-of-bounds Read (CWE-125)}. \texttt{Other (NVD-CWE-Other)} refers to the vulnerabilities that are not covered by NVD since NVD only uses a subset of CWE instead of the entire CWEs. \texttt{Out-of-bounds Read (CWE-125)} referes to the vulnerabilities that reads data after or prior to the intended buffer. \texttt{Buffer Over-read (CWE-126)} type uses buffer access mechanisms (e.g., indexes or pointers) that reference memory locations to read from a buffer after the targeted buffer. 


\autoref{tab:3type} shows the distribution of the four types of vulnerability signatures (i.e., add, delete, one-block-change, and many-block-change). Originally, there were three types (add, delete, change). 
We further split the change type into one-block-change and many-block-change for clarity.
Sig (\#) refers to the number of the signature type in the project. Avg. size refers to the average instruction amount of the specific signature in the project for each CVE.
Generally, \texttt{many-block-change} is the dominant type in all datasets. 
The \texttt{delete} type is the least common type in all datasets. 
The \texttt{add} type contains the most instruction size because the \texttt{add} type involves at least two complete basic blocks to form the signature.
Conversely, the \texttt{delete} type contains the least instruction size because the \texttt{delete} type does not contain control-flow information between multiple blocks that are made up of separate blocks. 
The change types may consist of parent-children structures or separate blocks.



\begin{table}[!t]
\centering
\caption{Statistics of signatures in the 7 open source projects. OBC for one-block-change, and MBC for many-block-change.}
\label{tab:3type}
\footnotesize
\begin{tabular}{cc|c|c|c|c|c|c|c}
\hline
 && \rotatebox[origin=c]{270}{openssl} & \rotatebox[origin=c]{270}{openjpeg} & \rotatebox[origin=c]{270}{libtiff} & \rotatebox[origin=c]{270}{libpng} & \rotatebox[origin=c]{270}{ffmpeg} & \rotatebox[origin=c]{270}{curl} & \rotatebox[origin=c]{270}{tcpdump}\\
 \hline
\multirow{2}{*}{add} &sig (\#) & 120 & 75 & 48 & 10 & 69 & 74 & 248\\
&Avg. size & 50 & 77 & 257 & 46 & 185 & 103 & 61\\
\hline
\multirow{2}{*}{delete} &sig (\#) & 37 & 17 & 9 & 1 & 9 & 20 & 64\\
&Avg.~size & 6 & 3 & 6 & 9 & 10 & 4 & 6\\
\hline
\multirow{2}{*}{OBC}&sig (\#) & 109 & 172 & 61 & 27 & 61 & 85 & 114\\
&Avg.~size & 6 & 11 & 10 & 7 & 14 & 6 & 7\\
\hline
\multirow{2}{*}{MBC}& sig (\#) & 146 & 258 & 91 & 27 & 61 & 109 & 269\\
&Avg.~size & 14 & 23 & 24 & 9 & 20 & 14 & 12\\
\hline
\end{tabular}
\end{table}