\section{Discussion}
\label{sec:dis}

\mypara{Require Source Code} Compared to three state-of-the-art works \cite{vmpbl,binxray,viva}, we require both source code and binary code to extract the signature.
All of the three tools \cite{vmpbl,binxray,viva} claim to only require binary code, but they require all the vulnerability-related versions of binary code, and the binary code must be compiled with the same optimization flag. 
This assumption is strong because one can not guarantee the binary versions (s)he collected from the wild are compiled with the same options.
Therefore, in their actual implementations, they still need the source code to generate different binary codes with the same optimization options from which a signature is extracted.


\mypara{Cross Architecture}  
\name only investigates the vulnerable and patched code on the same architecture.
However, the same source code could be compiled on different hardware architectures (e.g., ARM, x32, PowerPC, etc.) 
How to match cross-architecture vulnerable signatures remains an open research problem. 
Possible solutions include: 1) translating different architectures' instructions into an intermediate language, and 2) extracting vulnerable binary signatures on different architectures. 
However, this issue is beyond this paper's scope.

\mypara{Differences Introduced by Compilation}
An important challenge is mitigating instruction differences introduced by different compiling optimization settings, different compilers, and different compiler versions. 
This paper only considered the project's default optimization options and our testing system's default compiler. 
It is possible to observe the binaries compiled with different optimization levels or compilers in the wild. 
A plausible solution is to utilize symbolic execution to mitigate the impact of different optimization levels as \cite{fiber}. 
However, symbolic execution is time-consuming to execute. 
Another possible solution without changing our current methodology is to increase our training data. The training data refers to the binaries we extract signatures from.
Since we only extract vulnerability signatures from vulnerable and patched versions compiled by their default optimization level and the default compiler, the current training data are limited. 
To detect cross-optimization-level or cross-compiler signatures, a possible solution is to compile the project using multiple optimization levels or compilers and extract their corresponding signatures.


Patch and vulnerability detection genres of work directly extract assembly instructions and form signatures. 
The state-of-the-art whole-function similarity matching adopts many data-driven methods. 
Asm2vec \cite{asm2vec} and Palmtree \cite{Palmtree} convert the assembly instructions into vectors to mitigate subtle assembly differences introduced by compilations to some extent. 
Data-driven methods usually take less time than other methods. 
Merging these two methods by generating vectorized fine-grained signatures detects fine-grained signatures and mitigates assembly differences with less time and cost. 
Graph attributes-based vectors are generated in \cite{gemini,VULSEEKER}. 
Therefore, it is possible to extend \name by incorporating fine-grained graph-based embeddings as the signature.

