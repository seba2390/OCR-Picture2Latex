\section{Related Work}
\label{sec:related_work}


\subsection{User Behavior Modeling}
\label{subsec:rw:user}

User behavior modeling is an important topic in industrial ads, search, and recommendation system. A notable pioneering work that leverages the power of neural network is provided by Youtube Recommendation \cite{covington2016deep}. User historical interactions with the system are embedded first, and sum-pooled into fixed width input for downstream multi-layer perception. 

Follow-up work starts exploring the sequential nature of these interactions. Among these, earlier work exploits sequence models such as RNN \cite{hidasi2015session}, while later work starting with \cite{li2017neural} mostly adopts attention between the target example and user historical behavior sequence, notably DIN \cite{zhou2018deep} and KFAtt\cite{liu2020kalman}. 

More recently, self-attention \cite{kang2018self} and graph neural net \cite{wu2019session,pang2021heterogeneous} have been successfully applied in the sequential recommendation domain. 




% User Behavior Modeling Methods contains kinds of Methods:Pooling-based, Attention-based和Sequence-based。Pooling-based\cite{covington2016deep}和Attention-based\cite{zhou2018deep}, \cite{vaswani2017attention} Methods会将用户行为看成无序的集合，其中Pooling-based将所有用户行为看做是等价的。Sequence-based\cite{hidasi2015session}则可以根据用户行为序列提取信息，具有时间属性。

% ps:
% \cite{covington2016deep}: Deep Neural Networks for YouTube Recommendations.
% \cite{zhou2018deep}: Deep interest network for click-through rate prediction.
% \cite{vaswani2017attention}: Attention is all you need.
% \cite{hidasi2015session}: Session-based recommendations with recurrent neural networks.
% \label{subsec:rw:behavior}

\subsection{RNN in search and recommendation}
\label{subsec:rw:rnn}

While attention excels in training efficiency, RNN still plays a useful role in settings like incremental model training and updates. 
Compared to DNN, RNN is capable of taking the entire history of a user into account, effectively augmenting the input feature space. Furthermore, it harnesses the sequential nature of the input data efficiently, by constructing a training example at every event in the sequence, rather than only at the last event \cite{zhang2014sequential}. Since the introduction of Attention in \cite{vaswani2017attention}, however, RNN starts to lose its dominance in the sequential modeling field, mainly because of its high serving latency. 

We argue however RNN saves computation in online serving, since it propagates the user hidden state in a forward only manner, which is friendly to incremental update. In the case when user history can be as long as thousands of sessions, real time attention computation can be highly impractical, unless mitigated by some approximation strategies \cite{drachsler2008personal}. The latter introduces additional complexity and can easily lose accuracy. 

Most open-source implementations of reinforcement learning framework for search and recommendation system implicitly assume an underlying RNN backbone \cite{chen2019top}. The implementation however typically simplifies the design by only feeding a limited number of ID sequences into the RNN network \cite{zhao2019deep}. 

\cite{zhang2019deep} contains a good overview of existing RNN systems in Search / Recommendations. In particular, they are further divided into those with user identifier and those without. In the latter case, the largest unit of training example is a single session from which the user makes one or more related requests. While in the former category, a single user could come and go multiple times over a long period of time, thus providing much richer contexts to the ranker. It is the latter scenario that we focus on in this paper. To the best of our knowledge, such settings are virtually unexplored in the search ranking setting.




% RNN具备记忆能力，当涉及到连续的、与上下文相关的任务时，它比其他神经网络具有更大的优势。论文1\cite{zhang2019deep}Deep learning based recommender system: A survey and new perspectives里面将RNN在推荐系统中的应用分为三种：Session-based Recommendation without User Identifier, Sequential Recommendation with User Identifier和Feature Representation Learning with RNNs。其中不要求用户注册和登录的应用或网站没有用户标识，这些系统通常使用第一种方式通过the session或cookie获取用户的短期偏好。可以获取到用户标识的系统则使用第二种方式建模序列推荐任务，同时RNN也可作为一种特征表示的学习方式。RNN在搜索中的应用与在推荐场景中类似\cite{zhang2014sequential}。

\subsection{Deep Reinforcement Learning}
\label{subsec:rw:rl}

While the original reinforcement learning idea was proposed more than 3 decades ago, there has been a strong resurgence of interest in the past few years, thanks in part to its successful application in playing Atari games \cite{mnih2013playing}, DeepMind's AlphaGo \cite{silver2017mastering} and in text generation domains \cite{chen2019reinforcement,gong2019reinforcement}. Both lines of work achieve either super-human level or current state-of-the-art performance on a wide range of indisputable metrics.  

Several important technical milestones include Double DQN \cite{van2016deep} to mitigate over-estimation of Q value, and \cite{schaul2015prioritized}, which introduces experience replay. However, most of the work focuses on settings like gaming and robotics. We did not adopt experience replay in our work because of its large memory requirement, given the billion example scale at which we operate. 

The application in personalized search and recommendation has been more recent. Majority of the work in this area focuses on sequential recommendation such as \cite{zhao2018deep} as well as ads placement within search and recommendation results \cite{zhao2021dear}.

An interesting large scale off-policy recommendation work is presented in \cite{chen2019top} for youtube recommendation. They make heuristic correction of the policy gradient calculation to bridge the gap between on-policy and off-policy trajectory distributions. We tried it in our problem with moderate offline success, though online performance was weaker, likely because our changing user queries make the gradient adjustment less accurate.

Several notable works in search ranking include \cite{hu2018reinforcement} which takes an on-policy approach and \cite{xu2020reinforcement} which uses pairwise training examples similar to ours. However both works consider only a single query session, which is similar to the sequential recommendation setting, since the query being fixed can be treated as part of the user profile. In contrast, our work considers the user interactions on a search platform over an extended period of time, which typically consist of hundreds of different query sessions. 




% Several notable works include xyz (dawei yin, . Despite the dissimilarity between search ranking and recommendation tasks, namely the existence of a query, the problem setup is often quite similar. In particular, for learning to rank problems, typically only a single query session is considered. Many of the ranking related RL papers also focus on relevance learning, with a notable exception of xyz (Hu from alibaba). The latter however also considers a single query as the entire user history. Thus to the best of our knowledge, multi-query cross session reinforcement learniing has not been fully explored under RL.

% We also mention the work in the personalized Ads targeting domain, from ByteDance. Since it deals with the mixture of Ads and recommendation results, it does not directly address the homogeneous ranking problem that we face.


% \begin{itemize}
% \item Value-Based RL —DQN及其改进算法
% \begin{itemize}
% \item 论文1\cite{mnih2013playing}: Playing Atari with Deep Reinforcement Learning【深度强化学习开山之作】第一次提出了DQN算法，是深度强化学习真正意义的开山之作，算法用卷积神经网络构造Q网络，网络输入经过处理后的最近4帧游戏画面，输出在这种状态下执行各种动作的Q函数值。在绝大部分游戏上，DQN超过了之前最好的算法，在部分游戏上，甚至超过了人类玩家的水平；
% \item 论文2\cite{mnih2015human}: Human-level control through deep reinforcement learning 对论文1进行改进，构建目标Q网络，目标Q网络和Q网络之间周期性同步参数，提升了算法的收敛性；
% \item 论文3\cite{van2016deep}:  Deep reinforcement learning with double q-learning【Duoble DQN】 使用当前值网络的参数θ选择最优动作，用目标值网络的参数θ-评估该最优动作，将动作选择和策略评估分离，降低了过高估计Q值的风险；
% \item 论文4\cite{schaul2015prioritized}: Prioritized experience replay【Prioritized replay 样本采样方式优化】基于优先级采样的DQN，是对经验回放机制的改进，为经验池中的每个样本计算优先级，增大有价值的训练样本在采样时的概率。加快收敛速度和提升效果；
% \item 论文5\cite{wang2016dueling}: Dueling network architectures for deep reinforcement learning【Dueling networks】将CNN卷积层之后的全连接层替换为两个分支，其中一个分支拟合状态价值(state values)函数V(s)，另外一个分支拟合动作优势(action advantages)函数A(s,a)。最后将两个分支的输出值相加，形成Q函数值。这种改进能够更准确的估计Q值。
% \item 论文6\cite{hausknecht2015deep}:Deep recurrent q-learning for partially observable MDPs. 在CNN的卷积层之后加入LSTM单元，记住之前的信息。
% \item 论文7\cite{hessel2018rainbow}: Rainbow: Combining Improvements in Deep Reinforcement Learning 整合了DQN的诸多优化。包括Double Q-learning，Prioritized replay，Dueling networks，Multi-step learning，Distributional RL 和 Noisy Nets.
% \item DQN适用范围：DQN是求每个action的\(max_aQ(s,a)\)，适用于低维，离线的动作空间，在连续空间不适用。
% \end{itemize}
% \item Policy-Based RL算法
% \begin{itemize}
% \item  policy based RL 直接对策略建模，通过reward来直接对策略进行更新，使得累计回报最大。适用于连续的动作空间，但是无法衡量策略究竟是不是最优，策略评估高方差。
% \item  最开始的reinforcement算法\cite{williams1992simple} Simple Statistical Gradient-Following Algorithms for Connectionist Reinforcement Learning[1992, Williams] 
% \item  策略梯度算法\cite{sutton1999policy} Policy Gradient Methods for Reinforcement Learning with Function Approximation[2000, Sutton]
% \end{itemize}

% \item Actor-Critic算法
% \begin{itemize}
% \item 结合Policy-Based RL和Value-Based RL方法，基本上解决了高维状态与动作空间的问题，并使性能有明显的提升。但原始的Actor-Critic方法对于复杂问题可能会不稳定。
% \item DPG算法\cite{silver2014deterministic}：Deterministic Policy Gradient Algorithms【DPG】
% \item DDPG算法\cite{lillicrap2015continuous}：Continuous Control With Deep Reinforcement Learning【DDPG】
% \item TD3算法\cite{fujimoto2018addressing}：Addressing Function Approximation Error in Actor-Critic Methods【Twin Delayed Deep Deterministic policy gradient TD3】
% \end{itemize}

% \end{itemize}

% \label{subsec:rw:rl}

% \subsection{Reinforcement Learning in search and recommendation、}
% \begin{itemize}
% \item 传统的搜索/推荐算法
% \begin{itemize}
% \item 认为搜索/推荐是一个静态的过程；
% \item 建模即时reward，仅考虑当前的商品是否被点击/购买，忽略长期价值；
% \end{itemize}
% \item 强化学习在推荐中的应用
% \begin{itemize}
% \item  MDP推荐系统\cite{shani2005mdp}：An MDP-Based Recommender System
% \item  RL推荐系统（最大化长期价值）\cite{theocharous2015ad}：Personalized ad recommendation systems for life-time value optimization with guarantees
% \begin{itemize}
% \item 把个性化⼴告推荐系统定义为强化学习问题，最⼤化⽣命周期值(life-time value)
% \end{itemize}
% \item  DRN新闻推荐\cite{zheng2018drn}:DRN:A deep reinforcement learning framework for news recommendation
% \begin{itemize}
% \item 基于【DQN】的推荐算法
% \item 主要贡献：1.提出基于deep Q-learning的推荐框架，该框架可以明确建模长期reward（MAB-based方法不更清晰地给出future reward,MDP-based方法不适用于大规模数据）；2.引入用户活跃度（用户返回APP的情况），作为点击/不点击标签的补充，从而获取更多的的用户反馈信息；3.加入了探索策略（采用Dueling Bandit Gradient Descent方法挑选当前推荐环境下候选items），为用户寻找新的有吸引力的新闻；
% \end{itemize}
% \item Listwise推荐\cite{zhao2017deep}：Deep reinforcement learning for list-wise recommendations
% \begin{itemize}
% \item 基于【DDPG】的推荐算法
% \item 主要缺点：使用全联接网络表示用户状态，不能很好的建模用户和商品之间的关系
% \end{itemize}
% \item Pagewise推荐\cite{zhao2018deep}：Deep Reinforcement Learning for Page-wise Recommendations
% \begin{itemize}
% \item 基于【DDPG】的推荐算法
% \item 主要贡献：生成每个推荐物品的同时，也决定每个推荐维度在二维屏幕上的位置
% \end{itemize}
% \item 考虑正负反馈的推荐\cite{zhao2018recommendations}：Recommendations with Negative Feedback via
% Pairwise Deep Reinforcement Learning
% \begin{itemize}
% \item 基于【DQN】的推荐算法
% \item 主要贡献：考虑负反馈以及商品的偏序关系，并将这种偏序关系建模到DQN的loss函数中。用户跳过或者是没有任何行为的商品，不仅能够影响用户的行为，还可以让我们更好的了解用户的偏好。
% \end{itemize}
% \item User-Item Interactions Modeling\cite{liu2018deep} : Deep Reinforcement Learning based Recommendation with Explicit User-Item Interactions Modeling
% \begin{itemize}
% \item 基于【DDPG】的推荐算法
% \item 主要贡献：状态表征模块设计，文中强调了状态表征的重要性，并设计了三种状态表征模块；DRR-p：商品之间组pair，pair内item embedding相乘，和原始商品信息concat；DRR-u：用户embedding和item embedding相乘，同时concat商品pair相乘结果；DRR-ave：DRR-u基础上item处考虑position weight，并经过average pooling；
% \item 不足之处：虽然设计了状态表征模块，但是仅使用了用户最近的N个正反馈商品
% \end{itemize}
% \end{itemize}

% \item 强化学习在搜索中的应用
% \begin{itemize}
% \item 阿里RL搜索排序\cite{hu2018reinforcement}：Reinforcement learning to rank in e-commerce search engine: Formalization, analysis, and application
% \end{itemize}

% \end{itemize}

\label{subsec:rw:rl_search}

% mainly low rank approximation and pq based.
