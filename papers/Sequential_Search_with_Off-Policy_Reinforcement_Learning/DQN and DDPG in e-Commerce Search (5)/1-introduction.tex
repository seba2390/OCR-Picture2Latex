
\begin{CCSXML}
<ccs2012>
 <concept>
  <concept_id>10010520.10010553.10010562</concept_id>
  <concept_desc>Computer systems organization~Embedded systems</concept_desc>
  <concept_significance>500</concept_significance>
 </concept>
 <concept>
  <concept_id>10010520.10010575.10010755</concept_id>
  <concept_desc>Computer systems organization~Redundancy</concept_desc>
  <concept_significance>300</concept_significance>
 </concept>
 <concept>
  <concept_id>10010520.10010553.10010554</concept_id>
  <concept_desc>Computer systems organization~Robotics</concept_desc>
  <concept_significance>100</concept_significance>
 </concept>
 <concept>
  <concept_id>10003033.10003083.10003095</concept_id>
  <concept_desc>Networks~Network reliability</concept_desc>
  <concept_significance>100</concept_significance>
 </concept>
</ccs2012>
\end{CCSXML}

\ccsdesc[500]{Computer systems organization~Embedded systems}
\ccsdesc[300]{Computer systems organization~Redundancy}
\ccsdesc{Computer systems organization~Robotics}
\ccsdesc[100]{Networks~Network reliability}

%%
%% Keywords. The author(s) should pick words that accurately describe
%% the work being presented. Separate the keywords with commas.
\keywords{datasets, neural networks, gaze detection, text tagging}

%% A "teaser" image appears between the author and affiliation
%% information and the body of the document, and typically spans the
%% page.
\begin{teaserfigure}
  \includegraphics[width=\textwidth]{sampleteaser}
  \caption{Seattle Mariners at Spring Training, 2010.}
  \Description{Enjoying the baseball game from the third-base
  seats. Ichiro Suzuki preparing to bat.}
  \label{fig:teaser}
\end{teaserfigure}

\section{Introduction}
\label{sec:introduction}

Over the past decade, neural network has seen an exponential growth in industrial applications. One of its most successful applications is in the area of search and recommendation. For instance, in the e-commerce domain, users often log onto the e-commerce platform with either vague or clear ideas of things they want to purchase. A recommender system proposes items to the user based on the items' popularity or the users' preference, inferred through past user interactions or other user profile information, while a search engine also takes into account the user provided search query as an input.

To capture a user's likely preference among the millions of items, recommendation systems have sought to leverage the user's historical interactions with the system. This is known as the Sequential Recommendation problem and has been studied thoroughly, especially after the neural network revolution in 2012, thanks to its remarkable modeling flexibility. The end result is that recommendation systems nowadays such as TikTok can deliver users' preferred content (measured in terms of post-recommendation interactions) with freakish accuracy, especially for frequent users. 

The analogous problem in the search domain, surprisingly, has been largely unexplored, at least in the published literature. Just like recommenders, search engines host billions of users. Many of these users may have interacted with the ranked results hundreds of times, through clicks in general, and also adding to cart or purchase actions in the e-commerce domain. Furthermore, unlike with recommendation systems, search engine users also leave a trail of their search queries, each of which narrows down the space of potential result candidates and can help the search engine better understand specific areas of the user's interests.

To help close this gap in the search domain, we propose a new class of learning tasks called Sequential Search. To the best of our knowledge, this is the first paper to formally introduce this new problem domain. Despite the simlarity with the well-known Sequential Recommendation problems, Sequential Search has its own sets of opportunities and challenges. First and foremost, the provision of a query in the current session significantly restricts the candidate result pool, making it feasible to approach from a ranking perspective, instead of retrieval only. At the same time, the quality of the ranked results also depends heavily on the quality of the retrieval phase. Similarly, The presence of historical queries issued by the user in principle provides much more targeted information about his or her preferences with respect to different search intents. On the flip side, however, changing queries break the continuity of the result stream presented to the user, leading to sparsity of latent ranking signals, as well as rendering personalization secondary to semantic relevance.

The present paper is an attempt to take on the aforementioned opportunities and challenges at the same time. Similar to DIEN \cite{zhou2019deep} in the Sequential Recommendation literature, we experiment with a combination of attention and RNN network to capture users' historical interactions with the system. Besides the modeling consideration stated in \cite{zhou2019deep}, such as capturing users' interest evolution, our hybrid sequential approach is also motivated by the desire to update the model incrementally during online serving, for which RNN is naturally more efficient than attention. In addition, due to the difficulty of gradient propagation over long sequences of recurrent network,  we specialize the attention component (Figure~\ref{fig:din}) to deal with long term interactions (up to 1 year) with limited number of categorical feature sequences, while apply the RNN network only to near term interactions (within the past 30 days) with the full set of features. 

Because of the multiple queries involved in the user interaction sequence, our training data takes on a special nested format (Figure~\ref{fig:user_session_tsv}) which can be viewed as a 4-dimensional array. By contrast, users in sequential recommendation problems can be treated as having a single session, albeit extended over a long period of time potentially. To further deal with the unevenness of user sequence lengths within a training minibatch, we devise a novel knapsack packing procedure to merge several short user sequences into one, thereby significantly reducing computational cost during training.

Finally to optimize for long term user experience and core business metrics, we build a deep reinforcement learning model naturally on top of the RNN framework. As is typically done, the users are treated as a partially observable environment while the recommender or search engine itself plays the role of the agent, of which the model has full control. Following ideas similar to the Deep Deterministic Policy Gradient network \cite{lillicrap2015continuous}, we introduce \textbf{S}equential \textbf{S}ession \textbf{S}earch \textbf{D}eep \textbf{D}eterministic \textbf{P}olicy \textbf{G}radient network, or S3DDPG, that optimizes a policy gradient loss and a temporal difference loss at the same time, over a continuous action space represented by the RNN output embedding as well as the agent prediction score (See Figure~\ref{fig:ddpg}). 

One difficulty in applying reinforcement learning in the search context is that the environment is essentially static. Online model exploration is expensive in terms of core business metrics, especially if the model parameters need to be explored and adjusted continuously. Fortunately our model can be trained completely offline, based only on the logged user interactions with the search results. In particular, we do not introduce any (offline) simulator component in the reinforcement learning training cycle, including the popular experience replay technique \cite{mnih2015human}. This significantly reduces model complexity compared to similar work \cite{zhao2017deep} in the Sequential Recommendation domain. Thanks to the presence of the temporal difference loss, S3DDPG also does not seem to require an adjustment of the underlying trajectory distribution, unlike the policy gradient loss only approach taken by \cite{chen2019top}.




% Deep reinforcement learning has made great breakthroughs in a variety of machine learning settings, most notably gaming, robotics. 

% More recently, it has found applications in search and recommendation ranking problems. 
% The typical setup is as follows. The users are treated as the environment, whereas the model is the player/actor. By optimizing over a long term horizon, the model seeks to induce the highest amount of reward from the users, which are typically tied to clicks or other desirable behavior, such as price-weighted purchase.  

% Unlike web search, a user on a shopping site typically interacts with the search engine multiple times before finally deciding what to buy or add to cart. This makes sequential methods such as RNN and reinforcement learning very natural candidates for behavioral predictions. 

% Despite the apparent dissimilarity, reinforcement learning is in fact supervised. Unlike ordinary setup of supervised learning in search ranking, however, reinforcement learning constructs new labels that explicitly emphasize long term rewards. This is a holy grail of well-developed e-commerce businesses, since the latter has more or less fixed pool of users, whose long term engagement and satisfaction are crucial to the sustained profitability of the business.

% One difficulty in applying reinforcement learning in the search context is that the environment is essentially static. Businesses usually cannot afford to let the model poke around the environment by intentionally trying different ranking strategies against the users. Doing so could cause severe damage to user experience, as well as require highly flexible experimentation platform to train the model continuously. 

% Thus we take an offline approach, with a similar setup to the supervised baseline. As will be discussed in Section~\ref{sec:method}, the RNN backbone is crucial for subsequent experiments in RL. We took special care to build an RNN model that is fully compatible with an earlier MLP baseline, which already includes hundreds of features of various types, such as numeric, textual, and categorical ids. In particular the training data is arranged in a contiguous and chronological order with respect to each user, so that we can treat the entire history of a user as a single example. 



% For reinforcement learning, we initially tried to follow the approach of DQN \cite{mnih2015human}. Due to the combinatorial explosion of the ranking action space, however, it quickily became clear that maximizing the Q-function over all possible ranking actions is infeasible. Instead we drop the max operator in the Bellman equation, and replace it with another objective of maximizing the discounted cumulative reward. 

% Due to the combinatorial explosion of the ranking action space, however, it quickily became clear that maximizing the Q-function over all possible ranking actions is infeasible. Instead we drop the max operator in the Bellman equation, and replace it with another objective of maximizing the discounted cumulative reward. It turns out that this is almost exactly the approach taken by Deep Determinstic Policy Gradient \cite{lillicrap2015continuous}, with several crucial differences and specializations: i) we adopt a pairwise approach for our input data, where the action consists of the representation vectors of the two chosen items; ii) Unlike most applications of RL, our reward is differentiable with respect to the action. Indeed, it is simply given by the cross entropy loss between the pairwise prediction (action) and the pairwise label.

% It is also worth noting that even though several earlier works (\cite{xu2020reinforcement}, \cite{hu2018reinforcement}) have explored the use of reinforcement learning techniques in the search ranking context, the setup has always been for a single query session, where the ranking task is decomposed into an item-by-item retrieval task. 
% Not surprisingly, we found that offline gains are disproportionate with respect to online gains in the case of RL, since factors like position bias and temporal causal effects (e.g., user will less likely click on an item purchased earlier) are not accounted in offline static evaluations. To remedy the situation, we construct an offline model for the sole purpose of simulated evaluation. This is inspired by earlier works on RL in search / recommendation, most conspicuously VirtualTaobao \cite{shi2019virtual}, as well as several search related work with open sourced implementation such as \cite{}. However the latter simulates user/ranker interactive trajectory under a single query session, whereas we look at the entire history of a user within a given training window. The use of position is also somewhat implicit in the search / recommendation related RL work. Here we make it fully explicit, by incorporating it as part of the RNN hidden state update process as well as in the simulated evaluation.

In a word, we summarize our main contributions as follows:
\begin{itemize}
    \item We formally propose the highly practical problem class of Sequential Search, and contrast it with the well-studied Sequential Recommendation.
    \item We design a novel 4d user session data format, suitable for sequential learning with multiple query sessions, as well as a knapsack algorithm to reduce wasteful RNN computation due to padding.
    \item We present an efficient combination of RNN framework leveraging all features in near-term user interactions, and attention mechanism applied to selected categorical features in the long-term.
    \item We further propose a Sequential Session Search Deep Deterministic Policy Gradient (S3DDPG) model that outperforms other supervised baselines by a large margin.
    \item We demonstrate significant improvements, both offline and online, of RNN against DNN, as well as our proposed S3DDPG against RNN, in the sequential search problem setting.
\end{itemize}


All source code used in this paper will be released for the sake of reproducibility. 
\footnote{Source code will available at https://github.com/xxxx (released upon publication)}
