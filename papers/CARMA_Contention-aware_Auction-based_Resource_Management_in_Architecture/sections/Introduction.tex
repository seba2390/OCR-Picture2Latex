\IEEEraisesectionheading{\section{Introduction} \label{introduction}}
% no \IEEEPARstart
\IEEEPARstart{I}{t} is believed that only many-core architectures can handle the massive parallel applications since the number of cores on chip multiporcessors (\textit{CMP}) is increasing each year. 
%The number of cores on chip multiporcessors (\textit{CMP}) is increasing each year and it is believed that only many-core architectures can handle the massive parallel applications. 
Server-side \textit{CMP}s usually have more than 16 cores and potentially more than hundreds of applications can run on each server. These systems are going to be the future generation of the multi-core processor servers. Applications running on these systems share the same resources like last level cache (\textit{LLC}), interconnection network, memory controllers, off-chip memories, auxiliary processing capability like co-processors etc. Along with rapid growth of core integration, the performance of applications highly depend on the allocation of resources and specially the \textit{contention} for shared resources \cite{tang2011impact, zhuravlev2010addressing, hsu2006communist, kim2004fair, cho2006managing, tootaghajICCD, farhat2016stochastic, tootaghaj2016optimal, tootaghaj2015evaluating, farhat2016towardsStoc, tootaghaj2015thesis}. In particular, as the number of co-runners running on the shared resource increase, the magnitude of performance degradation increases. As a result, this new architectural paradigm introduces several new challenges in terms of scalability of resource management and assignment on these large-scale servers. Therefore, a scalable competition method between applications to reach the optimal assignment can significantly improve the performance of co-runners on a shared resource. Figure~\ref{fig:Slow_down} shows an example of performance degradation for 10 \textit{spec 2006} applications running on a shared 10MB \textit{LLC} and solo run on 1MB \textit{LLC}.  \\
%\indent Mohammad:\\
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure}[!tb]
\centering
%\includegraphics[height=3in, width=2.5in]{NodeArchs2.pdf}
\includegraphics[height=1.3in, width=3.3in]{Images/Perf.pdf} %Slow_down.pdf
%\epsfig{file=Dataset.eps, height=2.5in, width=3in}
\caption{\label{fig:Slow_down} Performance degradation of 10 different \textit{spec 2006} applications sharing \textit{LLC}.}
\end{figure}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\indent Among these shared resources, sharing \textit{CPU}s and \textit{LLC}s plays an important role in overall CMP utilization and performance. Modern \textit{CMP}s are moving towards heterogeneous architecture designs where one can get advantage of both small number of high performance \textit{CPU}s or higher number of low performance cores. The advent \textit{Intel Xeon Phi} co-processors is an example of such heterogeneous architectures that during run-time the programmer can decide to run any part of the code on small number of \textit{Xeon} processors or higher number of \textit{Xeon Phi} co-processors. Therefore, the burden of making decisions on getting the shared resources is moving towards the applications. In addition to the shared \textit{CPU}s, shared \textit{LLC} keeps data on chip and reduces off-chip communication costs \cite{liu2004organizing}. Sometimes an application may flood on a cache and occupy a large portion of available memory and hurt performance of another application which rarely loads on memory, but its accesses are usually latency-sensitive. Recently, many proposals target partitioning the cache space between applications such that (1) each application gets the minimum required space, so that per-application performance is guaranteed to be at an acceptable level, (2) system performance is improved by deciding how the remaining space should be allocated to each one. \\
\indent Prior schemes \cite{zhuravlev2010addressing, qureshi2006utility, lin2008gaining, iyer2004cqos, liu2004organizing, rafique2006architectural, jiang2008analysis, yekkehkhany2017gb, xie2016scheduling} are marching towards these two goals, usually by trading off the system complexity and maximum system utilization. It is shown that neither a pure private \textit{LLC}, nor a pure shared \textit{LLC}, can provide optimal performance for different workloads \cite{cho2006managing}. In general, cache partitioning techniques can be divided into way partitioning and co-scheduling techniques. In a set-associative cache, partitioning is done by per-way allocation. For example, in a 512KB 4-Way shared cache, allocating 128KB capacity to application A means to allow it storing data blocks in only one way per-set, without accessing remaining. Co-scheduling techniques try to co-schedule a set of applications with lowest interference together at the same time such that the magnitude of slow-down for each application is the same or a performance metric is optimized for all applications. However, it is shown that, depending on the objective function for the performance metric, cache allocation can result in totally different allocations \cite{hsu2006communist}. In general Prior schemes have the following three challanges:\\
%\begin{enumerate}
\indent \textbf{1. Scalability}: All of the prior schemes suffer from scalability; especially when the approach is tracking the application's dynamism \cite{qureshi2006utility, zhuravlev2010addressing, jiang2008analysis}. The reason is that algorithm complexity becomes higher in dynamic approaches. The root cause of this complexity is that all previous techniques make decisions (cache partitioning, co-scheduling) centralized using a central hardware or software. For example, main algorithm of \cite{qureshi2006utility} has exponential complexity $O( \binom{N+K-1}{K-1} )$ where $N$ is the number of applications sharing \textit{LLC} and $K$ is the number of ways. Table~\ref{table:complexity} shows the state of the art cache partitioning algorithms and their complexity of checking performance of different permutations.\\
\indent \textbf{2. Static-based}: Most of the prior works, use static co-scheduling to degrade slow-down of co-running applications on the same shared cache. However, static-based approaches can not catch dynamic behavior of applications. Figure~\ref{fig:Dynamic} shows an example of two applications' IPC (\textit{hmmer} and \textit{mcf}) from \textit{Spec 2006} under different \textit{LLC} sizes. It is shown that static-based approaches can not capture the dynamism in application's behavior and ultimately degrade the performance a lot. \\
\indent \textbf{3. Fairness}: Defining a single parameter for fairness is challenging for multiple applications, since applications have different performance benefits from each resource during each phase. In prior works fairness has been defined as a unique metric (eg. IPC, Power, Weighted Speed-up) for all applications. Therefore, in current approaches, the optimization goal of algorithms is the same for all applications. Consequently, we cannot sum up applications that desire different metrics in the same platform to decide on. However, if one application needs better IPC and another requires lower energy, the previous algorithms are not able to model it. The only way to address diversity of metrics (to be optimized) is to have an appropriate translation between different metrics (eg. IPC to Power) that is not trivial, while not addressed in any prior study.\\
\begin{table}[!tb] \scriptsize
\centering
\caption{\label{table:complexity} Complexity comparison of state-of-the-art \textit{LLC} partitioning/co-scheduling algorithms.}
\begin{tabular}{|c||c|} 
\hline Algorithm & Search Space\\
\hline Utility-based main algorithm \cite{qureshi2006utility} & $ \binom{N+K-1}{N-1}$  \\
\hline \pbox{20cm}{Greedy Co-scheduling \cite{jiang2008analysis}\\  $N$ applications and $N/K$ caches} & $ \binom{N}{K}  $ \\
\hline \pbox{20cm}{Hierarchical perfect matching \cite{jiang2008analysis} \\ $N$ applications} & $N^4 $ \\
\hline \pbox{20cm}{Local optimization \cite{jiang2008analysis} \\ $N$ applications and $N/K$ caches} & ${(N/K)}^2 \binom{2K}{K} $\\
\hline \pbox{20cm}{CAGE \\ $N$ applications and $K$ resources} & ${O(NK)}$\\
\hline
\end{tabular}
\end{table}
\begin{comment}
\begin{figure}%[!htb]
\centering
%\includegraphics[height=3in, width=2.5in]{NodeArchs2.pdf}
\includegraphics[height=2in, width=3.5in]{Images/Complexity.pdf}
%\epsfig{file=Dataset.eps, height=2.5in, width=3in}
\caption{\label{fig:complexity} Complexity comparison of state-of-the-art \textit{LLC} partitioning algorithms.}
\end{figure}
\end{comment}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure}[!b]
\centering
%\includegraphics[height=3in, width=2.5in]{NodeArchs2.pdf}
\includegraphics[height=1.5in, width=3.5in]{Images/Dynamic.pdf} %Slow_down.pdf
%\epsfig{file=Dataset.eps, height=2.5in, width=3in}
\caption{\label{fig:Dynamic} Performance comparison of static and dynamic scheduling of two applications (\textit{hmmer} and \textit{mcf} from \textit{Spec 2006}) under two different \textit{LLC} sizes.}
\end{figure}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\textcolor{red}{( Histogram charts on 128Kb cache for each and show how another 256Kb shared should be allocated to them on [Moin] and how it can be improved by dynamic decision (Figure 1 in STT-RAM paper).}\\
%\end{enumerate}
\indent In this paper, we present a distributed heterogeneous resource assignment method to address all the above shortcomings including scalability, dynamism and fairness, while applications can get their desired performance based on their utility functions:\\
\indent \textbf{1. Decentralized}: Dual of each centralized problem is decentralized, if optimization goal is broken into a smaller meaningful sub-problems. In the context of heterogeneous resource assignment this is straightforward; which decision on resource portion given to an application is done by itself while competes with others for best assignment. To achieve this, we introduce a novel market-based approach. Roughly speaking, the complexity of our approach in worst case scenario (for each application) is $O(NK)$ where $N$ is the number of applications and $K$ is the number of arcs in the resources available to each applications. However, on average the auction terminates in less than $N/2$ iterations.\\
\indent \textbf{2. Dynamic}: In order to confront the scalability problem of previous approaches we use a market-based approach to move the decision making to the individual applications. Iterative auctions have been designed to solve non-trivial resource allocation problems with low complexity cost in government sale of resources, \textit{eBay}, real estate sales and stock market. Similarly, decentralized computation complexity is definitely lower than centralized (for each application) which provides the opportunity to make the decision (revisiting the allocation) in small time quantum (or when a new applications leaves or comes into the system).\\
\indent \textbf{3. Fair}: The proposed method solves heterogeneous resource assignment problem in the context of marketing. Applications' demand regardless of the global optimization objective (IPC, Power, etc.) translates to virtual tokens (bids, coins, etc.) and these tokens are used for resource assignment; making it local optimization objectives. Hence, resource assignment can be performed for different applications with different objectives known as utility functions.\\
\indent Overall, the proposed approach for cache contention game on average brings in 33.6\% improvement in system performance (when running 16 applications) compared to shared \textit{LLC}; while reaching less than 11.1\% of the maximum achievable performance in the best dynamic scheme and for heterogeneous core case study, it brings in 106.6\% improvement (when running 16 applications at the same time). In addition, the performance improvement increases even more as the number of co-running applications increase in the system. \\
\indent \textbf{Other potentials}: The two case studies of cache partitioning and CPU sharing is an example for resource partition and the proposed method can be employed in any other resource partitioning. \\
\indent We introduce a distributed game-theoretic cache partitioning approach for different applications in large scale resource competition games.  In short, we pay for a high-end CMP system for servers and guarantee that each application/user takes its best from the system.\\ 
\indent The reminder of the paper is organized as follows. Section~\ref{Motivation} discusses the background and motivation behind this work. In section~\ref{Problem_definition} we discuss our auction based game model. Section~\ref{Case_Studies} discusses the case study of cache contention game and the case study of main processor and co-processor contention and simulation results. Section~\ref{Related_works} studies related works and Section~\ref{Conclusion} concludes the paper with a summary.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%The conventional resource management is usually done by the operating system, however this approach is not scalable for todays' large-scale servers and the operating system may not have enough information about application's demands. For example, one application may benefit more from more memory and others may benefit from having more CPU or network bandwidth and assigning the same amount of resources to different applications/users is not a good way for resource management. Furthermore, the traditional resource management systems rely on applications' utilization rather than applications' performance, but higher utility does not always lead to higher perfromance. For example, take the traditional LRU-based cache assignment strategy. The resource manager gives more cache space to the applications with higher cache utility. But there may be streaming applications which have high cache utility but their cache resue is very low and giving more cache capacity to them wouldn't affect the performance. \\ 
%%\indent Mohammad:\\
\begin{comment}
%\indent The conventional resource assignment approaches does not work properly for the new large-scale CMPs. 
In short, we pay for a high-end CMP system for servers and guarantee that each application/user takes its best from the system. Figure~\ref{fig:Slow_down} shows an example of performance degradation for 10 \textit{spec 2006} applications running on a shared 10MB \textit{LLC} and solo run on 1MB \textit{LLC}.   \\
\indent In this paper we present a distributed game-theoretic resource management approach for different applications and users in large scale resource competition games.   
The conventional resource management is usually done by the operating system, however this approach is not scalable for todays' large-scale servers and the operating system may not have enough information about application's demands. For example, one application may benefit more from more memory and others may benefit from having more CPU or network bandwidth and assigning the same amount of resources to different applications/users is not a good way for resource management. Furthermore, the traditional resource management systems rely on applications' utilization rather than applications' performance, but higher utility does not always lead to higher perfromance. For example, take the traditional LRU-based cache assignment strategy. The resource manager gives more cache space to the applications with higher cache utility. But there may be streaming applications which have high cache utility but their cache resue is very low and giving more cache capacity to them wouldn't affect the performance. \\ 
\indent Game theory has been used extensively in economics, political and mathematical decision making situations \cite{tootaghaj2011game, tootaghaj2011risk, kotobi2017spectrum, kotobi2015introduction}. A game is a situation, where the the output of each player not only depends on her own action in the game, but also on the action of other players \cite{osborne1994course}. Auction games are a class of games which has been used to formulate real world problems of assigning different resources between $n$ users. Auction game framework can model resource competition, where the payoff (cost) of each application in the system is a function of the contention level (number of applications) in the game.\\
\indent Inspired by nature predator-prey interactions in real life games, there exists a repeated interaction between competitors in a resource sharing game. We show that, assuming large number of applications the service rate of each application on each resource converges to each other. Furthermore, we show that the auction model is strategy-proof, such that no application can get more utilization by bidding more or less than the true value of the resource.   \\  
\indent The main questions we address in this paper are:
\begin{itemize}
  \item Does pure Nash equilibrium exist in a heterogeneous resource competition game? 
  \item How does different applications affect each other if they are given a private/shared cache.
  \item Having the dynamicity of different applications coming and leaving in the system and using shared resources, what is the convergence time of reaching the Nash equilibrium in the system with respect to the dynamic changes in the system.
  \item Defining a strategy for best response for each application and a learning phase to converge to the Nash equilibrium in the system. 
\end{itemize}  
\end{comment}



